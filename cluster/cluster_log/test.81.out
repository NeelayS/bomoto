Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=81, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 4536-4591
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01030317
Iteration 2/25 | Loss: 0.00249375
Iteration 3/25 | Loss: 0.00207346
Iteration 4/25 | Loss: 0.00153347
Iteration 5/25 | Loss: 0.00147632
Iteration 6/25 | Loss: 0.00139667
Iteration 7/25 | Loss: 0.00133194
Iteration 8/25 | Loss: 0.00128577
Iteration 9/25 | Loss: 0.00127669
Iteration 10/25 | Loss: 0.00121434
Iteration 11/25 | Loss: 0.00121971
Iteration 12/25 | Loss: 0.00121128
Iteration 13/25 | Loss: 0.00120397
Iteration 14/25 | Loss: 0.00119514
Iteration 15/25 | Loss: 0.00119185
Iteration 16/25 | Loss: 0.00119084
Iteration 17/25 | Loss: 0.00119077
Iteration 18/25 | Loss: 0.00119077
Iteration 19/25 | Loss: 0.00119077
Iteration 20/25 | Loss: 0.00119077
Iteration 21/25 | Loss: 0.00119077
Iteration 22/25 | Loss: 0.00119077
Iteration 23/25 | Loss: 0.00119077
Iteration 24/25 | Loss: 0.00119077
Iteration 25/25 | Loss: 0.00119077

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37589443
Iteration 2/25 | Loss: 0.00082779
Iteration 3/25 | Loss: 0.00082779
Iteration 4/25 | Loss: 0.00082779
Iteration 5/25 | Loss: 0.00082779
Iteration 6/25 | Loss: 0.00082779
Iteration 7/25 | Loss: 0.00082779
Iteration 8/25 | Loss: 0.00082778
Iteration 9/25 | Loss: 0.00082778
Iteration 10/25 | Loss: 0.00082778
Iteration 11/25 | Loss: 0.00082778
Iteration 12/25 | Loss: 0.00082778
Iteration 13/25 | Loss: 0.00082778
Iteration 14/25 | Loss: 0.00082778
Iteration 15/25 | Loss: 0.00082778
Iteration 16/25 | Loss: 0.00082778
Iteration 17/25 | Loss: 0.00082778
Iteration 18/25 | Loss: 0.00082778
Iteration 19/25 | Loss: 0.00082778
Iteration 20/25 | Loss: 0.00082778
Iteration 21/25 | Loss: 0.00082778
Iteration 22/25 | Loss: 0.00082778
Iteration 23/25 | Loss: 0.00082778
Iteration 24/25 | Loss: 0.00082778
Iteration 25/25 | Loss: 0.00082778

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082778
Iteration 2/1000 | Loss: 0.00125798
Iteration 3/1000 | Loss: 0.00077905
Iteration 4/1000 | Loss: 0.00169121
Iteration 5/1000 | Loss: 0.00029870
Iteration 6/1000 | Loss: 0.00004283
Iteration 7/1000 | Loss: 0.00016846
Iteration 8/1000 | Loss: 0.00034906
Iteration 9/1000 | Loss: 0.00116950
Iteration 10/1000 | Loss: 0.00013291
Iteration 11/1000 | Loss: 0.00005781
Iteration 12/1000 | Loss: 0.00004996
Iteration 13/1000 | Loss: 0.00002818
Iteration 14/1000 | Loss: 0.00006109
Iteration 15/1000 | Loss: 0.00002456
Iteration 16/1000 | Loss: 0.00002383
Iteration 17/1000 | Loss: 0.00002350
Iteration 18/1000 | Loss: 0.00002315
Iteration 19/1000 | Loss: 0.00002302
Iteration 20/1000 | Loss: 0.00002283
Iteration 21/1000 | Loss: 0.00002266
Iteration 22/1000 | Loss: 0.00002255
Iteration 23/1000 | Loss: 0.00002241
Iteration 24/1000 | Loss: 0.00002236
Iteration 25/1000 | Loss: 0.00002236
Iteration 26/1000 | Loss: 0.00002236
Iteration 27/1000 | Loss: 0.00002235
Iteration 28/1000 | Loss: 0.00002232
Iteration 29/1000 | Loss: 0.00002231
Iteration 30/1000 | Loss: 0.00002231
Iteration 31/1000 | Loss: 0.00002230
Iteration 32/1000 | Loss: 0.00002230
Iteration 33/1000 | Loss: 0.00002229
Iteration 34/1000 | Loss: 0.00002221
Iteration 35/1000 | Loss: 0.00002211
Iteration 36/1000 | Loss: 0.00002206
Iteration 37/1000 | Loss: 0.00002201
Iteration 38/1000 | Loss: 0.00002200
Iteration 39/1000 | Loss: 0.00002198
Iteration 40/1000 | Loss: 0.00002196
Iteration 41/1000 | Loss: 0.00002194
Iteration 42/1000 | Loss: 0.00002188
Iteration 43/1000 | Loss: 0.00002188
Iteration 44/1000 | Loss: 0.00002186
Iteration 45/1000 | Loss: 0.00002186
Iteration 46/1000 | Loss: 0.00002186
Iteration 47/1000 | Loss: 0.00002185
Iteration 48/1000 | Loss: 0.00002185
Iteration 49/1000 | Loss: 0.00002181
Iteration 50/1000 | Loss: 0.00002181
Iteration 51/1000 | Loss: 0.00002178
Iteration 52/1000 | Loss: 0.00002178
Iteration 53/1000 | Loss: 0.00002178
Iteration 54/1000 | Loss: 0.00002177
Iteration 55/1000 | Loss: 0.00002177
Iteration 56/1000 | Loss: 0.00002177
Iteration 57/1000 | Loss: 0.00002177
Iteration 58/1000 | Loss: 0.00002177
Iteration 59/1000 | Loss: 0.00002177
Iteration 60/1000 | Loss: 0.00002177
Iteration 61/1000 | Loss: 0.00002177
Iteration 62/1000 | Loss: 0.00002177
Iteration 63/1000 | Loss: 0.00002176
Iteration 64/1000 | Loss: 0.00002176
Iteration 65/1000 | Loss: 0.00002176
Iteration 66/1000 | Loss: 0.00002176
Iteration 67/1000 | Loss: 0.00002176
Iteration 68/1000 | Loss: 0.00002175
Iteration 69/1000 | Loss: 0.00002175
Iteration 70/1000 | Loss: 0.00002175
Iteration 71/1000 | Loss: 0.00002175
Iteration 72/1000 | Loss: 0.00002175
Iteration 73/1000 | Loss: 0.00002175
Iteration 74/1000 | Loss: 0.00002174
Iteration 75/1000 | Loss: 0.00002174
Iteration 76/1000 | Loss: 0.00002174
Iteration 77/1000 | Loss: 0.00002174
Iteration 78/1000 | Loss: 0.00002174
Iteration 79/1000 | Loss: 0.00002173
Iteration 80/1000 | Loss: 0.00002173
Iteration 81/1000 | Loss: 0.00002173
Iteration 82/1000 | Loss: 0.00002173
Iteration 83/1000 | Loss: 0.00002173
Iteration 84/1000 | Loss: 0.00002173
Iteration 85/1000 | Loss: 0.00002172
Iteration 86/1000 | Loss: 0.00002172
Iteration 87/1000 | Loss: 0.00002172
Iteration 88/1000 | Loss: 0.00002172
Iteration 89/1000 | Loss: 0.00002171
Iteration 90/1000 | Loss: 0.00002171
Iteration 91/1000 | Loss: 0.00002171
Iteration 92/1000 | Loss: 0.00002171
Iteration 93/1000 | Loss: 0.00002170
Iteration 94/1000 | Loss: 0.00002170
Iteration 95/1000 | Loss: 0.00002170
Iteration 96/1000 | Loss: 0.00002170
Iteration 97/1000 | Loss: 0.00002170
Iteration 98/1000 | Loss: 0.00002169
Iteration 99/1000 | Loss: 0.00002169
Iteration 100/1000 | Loss: 0.00002168
Iteration 101/1000 | Loss: 0.00002168
Iteration 102/1000 | Loss: 0.00002168
Iteration 103/1000 | Loss: 0.00002167
Iteration 104/1000 | Loss: 0.00002167
Iteration 105/1000 | Loss: 0.00002167
Iteration 106/1000 | Loss: 0.00002167
Iteration 107/1000 | Loss: 0.00002166
Iteration 108/1000 | Loss: 0.00002166
Iteration 109/1000 | Loss: 0.00002166
Iteration 110/1000 | Loss: 0.00002166
Iteration 111/1000 | Loss: 0.00002165
Iteration 112/1000 | Loss: 0.00002165
Iteration 113/1000 | Loss: 0.00002165
Iteration 114/1000 | Loss: 0.00002164
Iteration 115/1000 | Loss: 0.00002164
Iteration 116/1000 | Loss: 0.00002164
Iteration 117/1000 | Loss: 0.00002164
Iteration 118/1000 | Loss: 0.00002164
Iteration 119/1000 | Loss: 0.00002164
Iteration 120/1000 | Loss: 0.00002164
Iteration 121/1000 | Loss: 0.00002163
Iteration 122/1000 | Loss: 0.00002163
Iteration 123/1000 | Loss: 0.00002163
Iteration 124/1000 | Loss: 0.00002163
Iteration 125/1000 | Loss: 0.00002163
Iteration 126/1000 | Loss: 0.00002163
Iteration 127/1000 | Loss: 0.00002162
Iteration 128/1000 | Loss: 0.00002162
Iteration 129/1000 | Loss: 0.00002162
Iteration 130/1000 | Loss: 0.00002162
Iteration 131/1000 | Loss: 0.00002162
Iteration 132/1000 | Loss: 0.00002162
Iteration 133/1000 | Loss: 0.00002162
Iteration 134/1000 | Loss: 0.00002162
Iteration 135/1000 | Loss: 0.00002162
Iteration 136/1000 | Loss: 0.00002162
Iteration 137/1000 | Loss: 0.00002162
Iteration 138/1000 | Loss: 0.00002162
Iteration 139/1000 | Loss: 0.00002162
Iteration 140/1000 | Loss: 0.00002162
Iteration 141/1000 | Loss: 0.00002162
Iteration 142/1000 | Loss: 0.00002161
Iteration 143/1000 | Loss: 0.00002161
Iteration 144/1000 | Loss: 0.00002161
Iteration 145/1000 | Loss: 0.00002161
Iteration 146/1000 | Loss: 0.00002161
Iteration 147/1000 | Loss: 0.00002161
Iteration 148/1000 | Loss: 0.00002161
Iteration 149/1000 | Loss: 0.00002161
Iteration 150/1000 | Loss: 0.00002161
Iteration 151/1000 | Loss: 0.00002161
Iteration 152/1000 | Loss: 0.00002161
Iteration 153/1000 | Loss: 0.00002161
Iteration 154/1000 | Loss: 0.00002161
Iteration 155/1000 | Loss: 0.00002161
Iteration 156/1000 | Loss: 0.00002161
Iteration 157/1000 | Loss: 0.00002161
Iteration 158/1000 | Loss: 0.00002161
Iteration 159/1000 | Loss: 0.00002161
Iteration 160/1000 | Loss: 0.00002161
Iteration 161/1000 | Loss: 0.00002161
Iteration 162/1000 | Loss: 0.00002161
Iteration 163/1000 | Loss: 0.00002160
Iteration 164/1000 | Loss: 0.00002160
Iteration 165/1000 | Loss: 0.00002160
Iteration 166/1000 | Loss: 0.00002160
Iteration 167/1000 | Loss: 0.00002160
Iteration 168/1000 | Loss: 0.00002160
Iteration 169/1000 | Loss: 0.00002160
Iteration 170/1000 | Loss: 0.00002160
Iteration 171/1000 | Loss: 0.00002160
Iteration 172/1000 | Loss: 0.00002160
Iteration 173/1000 | Loss: 0.00002160
Iteration 174/1000 | Loss: 0.00002160
Iteration 175/1000 | Loss: 0.00002160
Iteration 176/1000 | Loss: 0.00002160
Iteration 177/1000 | Loss: 0.00002160
Iteration 178/1000 | Loss: 0.00002160
Iteration 179/1000 | Loss: 0.00002160
Iteration 180/1000 | Loss: 0.00002160
Iteration 181/1000 | Loss: 0.00002160
Iteration 182/1000 | Loss: 0.00002160
Iteration 183/1000 | Loss: 0.00002160
Iteration 184/1000 | Loss: 0.00002160
Iteration 185/1000 | Loss: 0.00002160
Iteration 186/1000 | Loss: 0.00002160
Iteration 187/1000 | Loss: 0.00002160
Iteration 188/1000 | Loss: 0.00002160
Iteration 189/1000 | Loss: 0.00002160
Iteration 190/1000 | Loss: 0.00002160
Iteration 191/1000 | Loss: 0.00002160
Iteration 192/1000 | Loss: 0.00002160
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [2.16021689993795e-05, 2.16021689993795e-05, 2.16021689993795e-05, 2.16021689993795e-05, 2.16021689993795e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.16021689993795e-05

Optimization complete. Final v2v error: 3.518864393234253 mm

Highest mean error: 11.490763664245605 mm for frame 146

Lowest mean error: 2.901747226715088 mm for frame 83

Saving results

Total time: 87.84583592414856
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00835233
Iteration 2/25 | Loss: 0.00126704
Iteration 3/25 | Loss: 0.00113476
Iteration 4/25 | Loss: 0.00112448
Iteration 5/25 | Loss: 0.00112210
Iteration 6/25 | Loss: 0.00112185
Iteration 7/25 | Loss: 0.00112185
Iteration 8/25 | Loss: 0.00112185
Iteration 9/25 | Loss: 0.00112185
Iteration 10/25 | Loss: 0.00112185
Iteration 11/25 | Loss: 0.00112185
Iteration 12/25 | Loss: 0.00112185
Iteration 13/25 | Loss: 0.00112185
Iteration 14/25 | Loss: 0.00112185
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0011218524305149913, 0.0011218524305149913, 0.0011218524305149913, 0.0011218524305149913, 0.0011218524305149913]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011218524305149913

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44797158
Iteration 2/25 | Loss: 0.00080894
Iteration 3/25 | Loss: 0.00080894
Iteration 4/25 | Loss: 0.00080894
Iteration 5/25 | Loss: 0.00080894
Iteration 6/25 | Loss: 0.00080894
Iteration 7/25 | Loss: 0.00080894
Iteration 8/25 | Loss: 0.00080894
Iteration 9/25 | Loss: 0.00080894
Iteration 10/25 | Loss: 0.00080894
Iteration 11/25 | Loss: 0.00080894
Iteration 12/25 | Loss: 0.00080894
Iteration 13/25 | Loss: 0.00080894
Iteration 14/25 | Loss: 0.00080894
Iteration 15/25 | Loss: 0.00080894
Iteration 16/25 | Loss: 0.00080894
Iteration 17/25 | Loss: 0.00080894
Iteration 18/25 | Loss: 0.00080894
Iteration 19/25 | Loss: 0.00080894
Iteration 20/25 | Loss: 0.00080894
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008089361945167184, 0.0008089361945167184, 0.0008089361945167184, 0.0008089361945167184, 0.0008089361945167184]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008089361945167184

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080894
Iteration 2/1000 | Loss: 0.00003100
Iteration 3/1000 | Loss: 0.00001656
Iteration 4/1000 | Loss: 0.00001449
Iteration 5/1000 | Loss: 0.00001347
Iteration 6/1000 | Loss: 0.00001286
Iteration 7/1000 | Loss: 0.00001246
Iteration 8/1000 | Loss: 0.00001232
Iteration 9/1000 | Loss: 0.00001227
Iteration 10/1000 | Loss: 0.00001219
Iteration 11/1000 | Loss: 0.00001197
Iteration 12/1000 | Loss: 0.00001180
Iteration 13/1000 | Loss: 0.00001175
Iteration 14/1000 | Loss: 0.00001174
Iteration 15/1000 | Loss: 0.00001172
Iteration 16/1000 | Loss: 0.00001172
Iteration 17/1000 | Loss: 0.00001168
Iteration 18/1000 | Loss: 0.00001166
Iteration 19/1000 | Loss: 0.00001165
Iteration 20/1000 | Loss: 0.00001164
Iteration 21/1000 | Loss: 0.00001161
Iteration 22/1000 | Loss: 0.00001160
Iteration 23/1000 | Loss: 0.00001160
Iteration 24/1000 | Loss: 0.00001159
Iteration 25/1000 | Loss: 0.00001159
Iteration 26/1000 | Loss: 0.00001159
Iteration 27/1000 | Loss: 0.00001159
Iteration 28/1000 | Loss: 0.00001159
Iteration 29/1000 | Loss: 0.00001158
Iteration 30/1000 | Loss: 0.00001157
Iteration 31/1000 | Loss: 0.00001155
Iteration 32/1000 | Loss: 0.00001154
Iteration 33/1000 | Loss: 0.00001154
Iteration 34/1000 | Loss: 0.00001154
Iteration 35/1000 | Loss: 0.00001153
Iteration 36/1000 | Loss: 0.00001147
Iteration 37/1000 | Loss: 0.00001144
Iteration 38/1000 | Loss: 0.00001144
Iteration 39/1000 | Loss: 0.00001143
Iteration 40/1000 | Loss: 0.00001143
Iteration 41/1000 | Loss: 0.00001143
Iteration 42/1000 | Loss: 0.00001142
Iteration 43/1000 | Loss: 0.00001142
Iteration 44/1000 | Loss: 0.00001142
Iteration 45/1000 | Loss: 0.00001141
Iteration 46/1000 | Loss: 0.00001140
Iteration 47/1000 | Loss: 0.00001138
Iteration 48/1000 | Loss: 0.00001138
Iteration 49/1000 | Loss: 0.00001138
Iteration 50/1000 | Loss: 0.00001138
Iteration 51/1000 | Loss: 0.00001138
Iteration 52/1000 | Loss: 0.00001138
Iteration 53/1000 | Loss: 0.00001138
Iteration 54/1000 | Loss: 0.00001137
Iteration 55/1000 | Loss: 0.00001136
Iteration 56/1000 | Loss: 0.00001136
Iteration 57/1000 | Loss: 0.00001136
Iteration 58/1000 | Loss: 0.00001135
Iteration 59/1000 | Loss: 0.00001135
Iteration 60/1000 | Loss: 0.00001135
Iteration 61/1000 | Loss: 0.00001135
Iteration 62/1000 | Loss: 0.00001135
Iteration 63/1000 | Loss: 0.00001134
Iteration 64/1000 | Loss: 0.00001134
Iteration 65/1000 | Loss: 0.00001134
Iteration 66/1000 | Loss: 0.00001133
Iteration 67/1000 | Loss: 0.00001133
Iteration 68/1000 | Loss: 0.00001133
Iteration 69/1000 | Loss: 0.00001133
Iteration 70/1000 | Loss: 0.00001132
Iteration 71/1000 | Loss: 0.00001132
Iteration 72/1000 | Loss: 0.00001132
Iteration 73/1000 | Loss: 0.00001131
Iteration 74/1000 | Loss: 0.00001131
Iteration 75/1000 | Loss: 0.00001131
Iteration 76/1000 | Loss: 0.00001131
Iteration 77/1000 | Loss: 0.00001130
Iteration 78/1000 | Loss: 0.00001130
Iteration 79/1000 | Loss: 0.00001130
Iteration 80/1000 | Loss: 0.00001129
Iteration 81/1000 | Loss: 0.00001129
Iteration 82/1000 | Loss: 0.00001129
Iteration 83/1000 | Loss: 0.00001129
Iteration 84/1000 | Loss: 0.00001129
Iteration 85/1000 | Loss: 0.00001128
Iteration 86/1000 | Loss: 0.00001128
Iteration 87/1000 | Loss: 0.00001128
Iteration 88/1000 | Loss: 0.00001128
Iteration 89/1000 | Loss: 0.00001128
Iteration 90/1000 | Loss: 0.00001127
Iteration 91/1000 | Loss: 0.00001127
Iteration 92/1000 | Loss: 0.00001127
Iteration 93/1000 | Loss: 0.00001127
Iteration 94/1000 | Loss: 0.00001127
Iteration 95/1000 | Loss: 0.00001127
Iteration 96/1000 | Loss: 0.00001127
Iteration 97/1000 | Loss: 0.00001126
Iteration 98/1000 | Loss: 0.00001126
Iteration 99/1000 | Loss: 0.00001126
Iteration 100/1000 | Loss: 0.00001126
Iteration 101/1000 | Loss: 0.00001126
Iteration 102/1000 | Loss: 0.00001126
Iteration 103/1000 | Loss: 0.00001126
Iteration 104/1000 | Loss: 0.00001126
Iteration 105/1000 | Loss: 0.00001126
Iteration 106/1000 | Loss: 0.00001125
Iteration 107/1000 | Loss: 0.00001125
Iteration 108/1000 | Loss: 0.00001125
Iteration 109/1000 | Loss: 0.00001125
Iteration 110/1000 | Loss: 0.00001125
Iteration 111/1000 | Loss: 0.00001125
Iteration 112/1000 | Loss: 0.00001125
Iteration 113/1000 | Loss: 0.00001125
Iteration 114/1000 | Loss: 0.00001125
Iteration 115/1000 | Loss: 0.00001125
Iteration 116/1000 | Loss: 0.00001125
Iteration 117/1000 | Loss: 0.00001125
Iteration 118/1000 | Loss: 0.00001125
Iteration 119/1000 | Loss: 0.00001125
Iteration 120/1000 | Loss: 0.00001125
Iteration 121/1000 | Loss: 0.00001125
Iteration 122/1000 | Loss: 0.00001125
Iteration 123/1000 | Loss: 0.00001124
Iteration 124/1000 | Loss: 0.00001124
Iteration 125/1000 | Loss: 0.00001124
Iteration 126/1000 | Loss: 0.00001124
Iteration 127/1000 | Loss: 0.00001124
Iteration 128/1000 | Loss: 0.00001123
Iteration 129/1000 | Loss: 0.00001123
Iteration 130/1000 | Loss: 0.00001123
Iteration 131/1000 | Loss: 0.00001123
Iteration 132/1000 | Loss: 0.00001122
Iteration 133/1000 | Loss: 0.00001122
Iteration 134/1000 | Loss: 0.00001122
Iteration 135/1000 | Loss: 0.00001122
Iteration 136/1000 | Loss: 0.00001122
Iteration 137/1000 | Loss: 0.00001122
Iteration 138/1000 | Loss: 0.00001122
Iteration 139/1000 | Loss: 0.00001122
Iteration 140/1000 | Loss: 0.00001121
Iteration 141/1000 | Loss: 0.00001121
Iteration 142/1000 | Loss: 0.00001121
Iteration 143/1000 | Loss: 0.00001121
Iteration 144/1000 | Loss: 0.00001121
Iteration 145/1000 | Loss: 0.00001121
Iteration 146/1000 | Loss: 0.00001120
Iteration 147/1000 | Loss: 0.00001120
Iteration 148/1000 | Loss: 0.00001120
Iteration 149/1000 | Loss: 0.00001120
Iteration 150/1000 | Loss: 0.00001119
Iteration 151/1000 | Loss: 0.00001119
Iteration 152/1000 | Loss: 0.00001119
Iteration 153/1000 | Loss: 0.00001119
Iteration 154/1000 | Loss: 0.00001119
Iteration 155/1000 | Loss: 0.00001119
Iteration 156/1000 | Loss: 0.00001119
Iteration 157/1000 | Loss: 0.00001118
Iteration 158/1000 | Loss: 0.00001118
Iteration 159/1000 | Loss: 0.00001118
Iteration 160/1000 | Loss: 0.00001118
Iteration 161/1000 | Loss: 0.00001117
Iteration 162/1000 | Loss: 0.00001117
Iteration 163/1000 | Loss: 0.00001117
Iteration 164/1000 | Loss: 0.00001117
Iteration 165/1000 | Loss: 0.00001117
Iteration 166/1000 | Loss: 0.00001117
Iteration 167/1000 | Loss: 0.00001117
Iteration 168/1000 | Loss: 0.00001117
Iteration 169/1000 | Loss: 0.00001117
Iteration 170/1000 | Loss: 0.00001117
Iteration 171/1000 | Loss: 0.00001117
Iteration 172/1000 | Loss: 0.00001116
Iteration 173/1000 | Loss: 0.00001116
Iteration 174/1000 | Loss: 0.00001116
Iteration 175/1000 | Loss: 0.00001116
Iteration 176/1000 | Loss: 0.00001116
Iteration 177/1000 | Loss: 0.00001116
Iteration 178/1000 | Loss: 0.00001116
Iteration 179/1000 | Loss: 0.00001116
Iteration 180/1000 | Loss: 0.00001116
Iteration 181/1000 | Loss: 0.00001116
Iteration 182/1000 | Loss: 0.00001116
Iteration 183/1000 | Loss: 0.00001116
Iteration 184/1000 | Loss: 0.00001116
Iteration 185/1000 | Loss: 0.00001116
Iteration 186/1000 | Loss: 0.00001115
Iteration 187/1000 | Loss: 0.00001115
Iteration 188/1000 | Loss: 0.00001115
Iteration 189/1000 | Loss: 0.00001115
Iteration 190/1000 | Loss: 0.00001115
Iteration 191/1000 | Loss: 0.00001115
Iteration 192/1000 | Loss: 0.00001115
Iteration 193/1000 | Loss: 0.00001114
Iteration 194/1000 | Loss: 0.00001114
Iteration 195/1000 | Loss: 0.00001114
Iteration 196/1000 | Loss: 0.00001114
Iteration 197/1000 | Loss: 0.00001114
Iteration 198/1000 | Loss: 0.00001114
Iteration 199/1000 | Loss: 0.00001114
Iteration 200/1000 | Loss: 0.00001114
Iteration 201/1000 | Loss: 0.00001114
Iteration 202/1000 | Loss: 0.00001114
Iteration 203/1000 | Loss: 0.00001114
Iteration 204/1000 | Loss: 0.00001114
Iteration 205/1000 | Loss: 0.00001114
Iteration 206/1000 | Loss: 0.00001114
Iteration 207/1000 | Loss: 0.00001114
Iteration 208/1000 | Loss: 0.00001114
Iteration 209/1000 | Loss: 0.00001114
Iteration 210/1000 | Loss: 0.00001114
Iteration 211/1000 | Loss: 0.00001114
Iteration 212/1000 | Loss: 0.00001114
Iteration 213/1000 | Loss: 0.00001114
Iteration 214/1000 | Loss: 0.00001114
Iteration 215/1000 | Loss: 0.00001114
Iteration 216/1000 | Loss: 0.00001114
Iteration 217/1000 | Loss: 0.00001114
Iteration 218/1000 | Loss: 0.00001114
Iteration 219/1000 | Loss: 0.00001114
Iteration 220/1000 | Loss: 0.00001114
Iteration 221/1000 | Loss: 0.00001114
Iteration 222/1000 | Loss: 0.00001114
Iteration 223/1000 | Loss: 0.00001114
Iteration 224/1000 | Loss: 0.00001114
Iteration 225/1000 | Loss: 0.00001114
Iteration 226/1000 | Loss: 0.00001114
Iteration 227/1000 | Loss: 0.00001114
Iteration 228/1000 | Loss: 0.00001114
Iteration 229/1000 | Loss: 0.00001114
Iteration 230/1000 | Loss: 0.00001114
Iteration 231/1000 | Loss: 0.00001114
Iteration 232/1000 | Loss: 0.00001114
Iteration 233/1000 | Loss: 0.00001114
Iteration 234/1000 | Loss: 0.00001114
Iteration 235/1000 | Loss: 0.00001114
Iteration 236/1000 | Loss: 0.00001114
Iteration 237/1000 | Loss: 0.00001114
Iteration 238/1000 | Loss: 0.00001114
Iteration 239/1000 | Loss: 0.00001114
Iteration 240/1000 | Loss: 0.00001114
Iteration 241/1000 | Loss: 0.00001114
Iteration 242/1000 | Loss: 0.00001114
Iteration 243/1000 | Loss: 0.00001114
Iteration 244/1000 | Loss: 0.00001114
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 244. Stopping optimization.
Last 5 losses: [1.1135149179608561e-05, 1.1135149179608561e-05, 1.1135149179608561e-05, 1.1135149179608561e-05, 1.1135149179608561e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1135149179608561e-05

Optimization complete. Final v2v error: 2.8088905811309814 mm

Highest mean error: 3.683121919631958 mm for frame 119

Lowest mean error: 2.5251128673553467 mm for frame 38

Saving results

Total time: 41.35452365875244
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00827680
Iteration 2/25 | Loss: 0.00137215
Iteration 3/25 | Loss: 0.00113116
Iteration 4/25 | Loss: 0.00109561
Iteration 5/25 | Loss: 0.00108855
Iteration 6/25 | Loss: 0.00108720
Iteration 7/25 | Loss: 0.00108711
Iteration 8/25 | Loss: 0.00108711
Iteration 9/25 | Loss: 0.00108711
Iteration 10/25 | Loss: 0.00108711
Iteration 11/25 | Loss: 0.00108711
Iteration 12/25 | Loss: 0.00108711
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010871092090383172, 0.0010871092090383172, 0.0010871092090383172, 0.0010871092090383172, 0.0010871092090383172]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010871092090383172

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.95992428
Iteration 2/25 | Loss: 0.00048332
Iteration 3/25 | Loss: 0.00048331
Iteration 4/25 | Loss: 0.00048331
Iteration 5/25 | Loss: 0.00048331
Iteration 6/25 | Loss: 0.00048331
Iteration 7/25 | Loss: 0.00048331
Iteration 8/25 | Loss: 0.00048331
Iteration 9/25 | Loss: 0.00048331
Iteration 10/25 | Loss: 0.00048331
Iteration 11/25 | Loss: 0.00048331
Iteration 12/25 | Loss: 0.00048331
Iteration 13/25 | Loss: 0.00048331
Iteration 14/25 | Loss: 0.00048331
Iteration 15/25 | Loss: 0.00048331
Iteration 16/25 | Loss: 0.00048331
Iteration 17/25 | Loss: 0.00048331
Iteration 18/25 | Loss: 0.00048331
Iteration 19/25 | Loss: 0.00048331
Iteration 20/25 | Loss: 0.00048331
Iteration 21/25 | Loss: 0.00048331
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0004833061248064041, 0.0004833061248064041, 0.0004833061248064041, 0.0004833061248064041, 0.0004833061248064041]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004833061248064041

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048331
Iteration 2/1000 | Loss: 0.00004528
Iteration 3/1000 | Loss: 0.00003226
Iteration 4/1000 | Loss: 0.00002654
Iteration 5/1000 | Loss: 0.00002444
Iteration 6/1000 | Loss: 0.00002312
Iteration 7/1000 | Loss: 0.00002214
Iteration 8/1000 | Loss: 0.00002134
Iteration 9/1000 | Loss: 0.00002086
Iteration 10/1000 | Loss: 0.00002052
Iteration 11/1000 | Loss: 0.00002032
Iteration 12/1000 | Loss: 0.00002012
Iteration 13/1000 | Loss: 0.00001987
Iteration 14/1000 | Loss: 0.00001971
Iteration 15/1000 | Loss: 0.00001962
Iteration 16/1000 | Loss: 0.00001959
Iteration 17/1000 | Loss: 0.00001948
Iteration 18/1000 | Loss: 0.00001946
Iteration 19/1000 | Loss: 0.00001946
Iteration 20/1000 | Loss: 0.00001945
Iteration 21/1000 | Loss: 0.00001945
Iteration 22/1000 | Loss: 0.00001945
Iteration 23/1000 | Loss: 0.00001945
Iteration 24/1000 | Loss: 0.00001945
Iteration 25/1000 | Loss: 0.00001945
Iteration 26/1000 | Loss: 0.00001945
Iteration 27/1000 | Loss: 0.00001945
Iteration 28/1000 | Loss: 0.00001945
Iteration 29/1000 | Loss: 0.00001945
Iteration 30/1000 | Loss: 0.00001944
Iteration 31/1000 | Loss: 0.00001944
Iteration 32/1000 | Loss: 0.00001944
Iteration 33/1000 | Loss: 0.00001943
Iteration 34/1000 | Loss: 0.00001942
Iteration 35/1000 | Loss: 0.00001942
Iteration 36/1000 | Loss: 0.00001941
Iteration 37/1000 | Loss: 0.00001941
Iteration 38/1000 | Loss: 0.00001941
Iteration 39/1000 | Loss: 0.00001941
Iteration 40/1000 | Loss: 0.00001941
Iteration 41/1000 | Loss: 0.00001941
Iteration 42/1000 | Loss: 0.00001941
Iteration 43/1000 | Loss: 0.00001941
Iteration 44/1000 | Loss: 0.00001941
Iteration 45/1000 | Loss: 0.00001941
Iteration 46/1000 | Loss: 0.00001941
Iteration 47/1000 | Loss: 0.00001941
Iteration 48/1000 | Loss: 0.00001940
Iteration 49/1000 | Loss: 0.00001940
Iteration 50/1000 | Loss: 0.00001939
Iteration 51/1000 | Loss: 0.00001938
Iteration 52/1000 | Loss: 0.00001937
Iteration 53/1000 | Loss: 0.00001937
Iteration 54/1000 | Loss: 0.00001937
Iteration 55/1000 | Loss: 0.00001937
Iteration 56/1000 | Loss: 0.00001937
Iteration 57/1000 | Loss: 0.00001937
Iteration 58/1000 | Loss: 0.00001937
Iteration 59/1000 | Loss: 0.00001937
Iteration 60/1000 | Loss: 0.00001937
Iteration 61/1000 | Loss: 0.00001937
Iteration 62/1000 | Loss: 0.00001937
Iteration 63/1000 | Loss: 0.00001936
Iteration 64/1000 | Loss: 0.00001936
Iteration 65/1000 | Loss: 0.00001935
Iteration 66/1000 | Loss: 0.00001935
Iteration 67/1000 | Loss: 0.00001935
Iteration 68/1000 | Loss: 0.00001934
Iteration 69/1000 | Loss: 0.00001934
Iteration 70/1000 | Loss: 0.00001934
Iteration 71/1000 | Loss: 0.00001934
Iteration 72/1000 | Loss: 0.00001934
Iteration 73/1000 | Loss: 0.00001934
Iteration 74/1000 | Loss: 0.00001934
Iteration 75/1000 | Loss: 0.00001933
Iteration 76/1000 | Loss: 0.00001933
Iteration 77/1000 | Loss: 0.00001933
Iteration 78/1000 | Loss: 0.00001933
Iteration 79/1000 | Loss: 0.00001933
Iteration 80/1000 | Loss: 0.00001933
Iteration 81/1000 | Loss: 0.00001932
Iteration 82/1000 | Loss: 0.00001932
Iteration 83/1000 | Loss: 0.00001932
Iteration 84/1000 | Loss: 0.00001932
Iteration 85/1000 | Loss: 0.00001932
Iteration 86/1000 | Loss: 0.00001932
Iteration 87/1000 | Loss: 0.00001932
Iteration 88/1000 | Loss: 0.00001932
Iteration 89/1000 | Loss: 0.00001932
Iteration 90/1000 | Loss: 0.00001932
Iteration 91/1000 | Loss: 0.00001932
Iteration 92/1000 | Loss: 0.00001932
Iteration 93/1000 | Loss: 0.00001932
Iteration 94/1000 | Loss: 0.00001932
Iteration 95/1000 | Loss: 0.00001932
Iteration 96/1000 | Loss: 0.00001932
Iteration 97/1000 | Loss: 0.00001932
Iteration 98/1000 | Loss: 0.00001932
Iteration 99/1000 | Loss: 0.00001932
Iteration 100/1000 | Loss: 0.00001932
Iteration 101/1000 | Loss: 0.00001932
Iteration 102/1000 | Loss: 0.00001932
Iteration 103/1000 | Loss: 0.00001932
Iteration 104/1000 | Loss: 0.00001932
Iteration 105/1000 | Loss: 0.00001932
Iteration 106/1000 | Loss: 0.00001932
Iteration 107/1000 | Loss: 0.00001932
Iteration 108/1000 | Loss: 0.00001932
Iteration 109/1000 | Loss: 0.00001932
Iteration 110/1000 | Loss: 0.00001932
Iteration 111/1000 | Loss: 0.00001932
Iteration 112/1000 | Loss: 0.00001932
Iteration 113/1000 | Loss: 0.00001932
Iteration 114/1000 | Loss: 0.00001932
Iteration 115/1000 | Loss: 0.00001932
Iteration 116/1000 | Loss: 0.00001932
Iteration 117/1000 | Loss: 0.00001932
Iteration 118/1000 | Loss: 0.00001932
Iteration 119/1000 | Loss: 0.00001932
Iteration 120/1000 | Loss: 0.00001932
Iteration 121/1000 | Loss: 0.00001932
Iteration 122/1000 | Loss: 0.00001932
Iteration 123/1000 | Loss: 0.00001932
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [1.9315732060931623e-05, 1.9315732060931623e-05, 1.9315732060931623e-05, 1.9315732060931623e-05, 1.9315732060931623e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9315732060931623e-05

Optimization complete. Final v2v error: 3.836596965789795 mm

Highest mean error: 4.105824947357178 mm for frame 131

Lowest mean error: 3.6304049491882324 mm for frame 109

Saving results

Total time: 35.168400287628174
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00402236
Iteration 2/25 | Loss: 0.00118739
Iteration 3/25 | Loss: 0.00108898
Iteration 4/25 | Loss: 0.00107655
Iteration 5/25 | Loss: 0.00107284
Iteration 6/25 | Loss: 0.00107180
Iteration 7/25 | Loss: 0.00107180
Iteration 8/25 | Loss: 0.00107180
Iteration 9/25 | Loss: 0.00107180
Iteration 10/25 | Loss: 0.00107180
Iteration 11/25 | Loss: 0.00107180
Iteration 12/25 | Loss: 0.00107180
Iteration 13/25 | Loss: 0.00107180
Iteration 14/25 | Loss: 0.00107180
Iteration 15/25 | Loss: 0.00107180
Iteration 16/25 | Loss: 0.00107180
Iteration 17/25 | Loss: 0.00107180
Iteration 18/25 | Loss: 0.00107180
Iteration 19/25 | Loss: 0.00107180
Iteration 20/25 | Loss: 0.00107180
Iteration 21/25 | Loss: 0.00107180
Iteration 22/25 | Loss: 0.00107180
Iteration 23/25 | Loss: 0.00107180
Iteration 24/25 | Loss: 0.00107180
Iteration 25/25 | Loss: 0.00107180

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44012129
Iteration 2/25 | Loss: 0.00086919
Iteration 3/25 | Loss: 0.00086919
Iteration 4/25 | Loss: 0.00086919
Iteration 5/25 | Loss: 0.00086919
Iteration 6/25 | Loss: 0.00086919
Iteration 7/25 | Loss: 0.00086919
Iteration 8/25 | Loss: 0.00086919
Iteration 9/25 | Loss: 0.00086919
Iteration 10/25 | Loss: 0.00086919
Iteration 11/25 | Loss: 0.00086919
Iteration 12/25 | Loss: 0.00086919
Iteration 13/25 | Loss: 0.00086918
Iteration 14/25 | Loss: 0.00086918
Iteration 15/25 | Loss: 0.00086918
Iteration 16/25 | Loss: 0.00086918
Iteration 17/25 | Loss: 0.00086918
Iteration 18/25 | Loss: 0.00086918
Iteration 19/25 | Loss: 0.00086918
Iteration 20/25 | Loss: 0.00086918
Iteration 21/25 | Loss: 0.00086918
Iteration 22/25 | Loss: 0.00086918
Iteration 23/25 | Loss: 0.00086918
Iteration 24/25 | Loss: 0.00086918
Iteration 25/25 | Loss: 0.00086918

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086918
Iteration 2/1000 | Loss: 0.00002026
Iteration 3/1000 | Loss: 0.00001215
Iteration 4/1000 | Loss: 0.00001061
Iteration 5/1000 | Loss: 0.00000995
Iteration 6/1000 | Loss: 0.00000964
Iteration 7/1000 | Loss: 0.00000936
Iteration 8/1000 | Loss: 0.00000935
Iteration 9/1000 | Loss: 0.00000933
Iteration 10/1000 | Loss: 0.00000927
Iteration 11/1000 | Loss: 0.00000927
Iteration 12/1000 | Loss: 0.00000926
Iteration 13/1000 | Loss: 0.00000910
Iteration 14/1000 | Loss: 0.00000903
Iteration 15/1000 | Loss: 0.00000894
Iteration 16/1000 | Loss: 0.00000893
Iteration 17/1000 | Loss: 0.00000892
Iteration 18/1000 | Loss: 0.00000890
Iteration 19/1000 | Loss: 0.00000890
Iteration 20/1000 | Loss: 0.00000887
Iteration 21/1000 | Loss: 0.00000884
Iteration 22/1000 | Loss: 0.00000883
Iteration 23/1000 | Loss: 0.00000883
Iteration 24/1000 | Loss: 0.00000882
Iteration 25/1000 | Loss: 0.00000880
Iteration 26/1000 | Loss: 0.00000870
Iteration 27/1000 | Loss: 0.00000869
Iteration 28/1000 | Loss: 0.00000869
Iteration 29/1000 | Loss: 0.00000867
Iteration 30/1000 | Loss: 0.00000865
Iteration 31/1000 | Loss: 0.00000864
Iteration 32/1000 | Loss: 0.00000864
Iteration 33/1000 | Loss: 0.00000863
Iteration 34/1000 | Loss: 0.00000862
Iteration 35/1000 | Loss: 0.00000862
Iteration 36/1000 | Loss: 0.00000862
Iteration 37/1000 | Loss: 0.00000861
Iteration 38/1000 | Loss: 0.00000860
Iteration 39/1000 | Loss: 0.00000858
Iteration 40/1000 | Loss: 0.00000858
Iteration 41/1000 | Loss: 0.00000857
Iteration 42/1000 | Loss: 0.00000857
Iteration 43/1000 | Loss: 0.00000857
Iteration 44/1000 | Loss: 0.00000857
Iteration 45/1000 | Loss: 0.00000857
Iteration 46/1000 | Loss: 0.00000856
Iteration 47/1000 | Loss: 0.00000855
Iteration 48/1000 | Loss: 0.00000854
Iteration 49/1000 | Loss: 0.00000852
Iteration 50/1000 | Loss: 0.00000852
Iteration 51/1000 | Loss: 0.00000852
Iteration 52/1000 | Loss: 0.00000852
Iteration 53/1000 | Loss: 0.00000851
Iteration 54/1000 | Loss: 0.00000851
Iteration 55/1000 | Loss: 0.00000851
Iteration 56/1000 | Loss: 0.00000851
Iteration 57/1000 | Loss: 0.00000850
Iteration 58/1000 | Loss: 0.00000850
Iteration 59/1000 | Loss: 0.00000849
Iteration 60/1000 | Loss: 0.00000849
Iteration 61/1000 | Loss: 0.00000848
Iteration 62/1000 | Loss: 0.00000848
Iteration 63/1000 | Loss: 0.00000848
Iteration 64/1000 | Loss: 0.00000848
Iteration 65/1000 | Loss: 0.00000847
Iteration 66/1000 | Loss: 0.00000847
Iteration 67/1000 | Loss: 0.00000847
Iteration 68/1000 | Loss: 0.00000847
Iteration 69/1000 | Loss: 0.00000847
Iteration 70/1000 | Loss: 0.00000847
Iteration 71/1000 | Loss: 0.00000847
Iteration 72/1000 | Loss: 0.00000847
Iteration 73/1000 | Loss: 0.00000846
Iteration 74/1000 | Loss: 0.00000846
Iteration 75/1000 | Loss: 0.00000846
Iteration 76/1000 | Loss: 0.00000845
Iteration 77/1000 | Loss: 0.00000845
Iteration 78/1000 | Loss: 0.00000845
Iteration 79/1000 | Loss: 0.00000845
Iteration 80/1000 | Loss: 0.00000845
Iteration 81/1000 | Loss: 0.00000845
Iteration 82/1000 | Loss: 0.00000845
Iteration 83/1000 | Loss: 0.00000845
Iteration 84/1000 | Loss: 0.00000845
Iteration 85/1000 | Loss: 0.00000845
Iteration 86/1000 | Loss: 0.00000844
Iteration 87/1000 | Loss: 0.00000844
Iteration 88/1000 | Loss: 0.00000844
Iteration 89/1000 | Loss: 0.00000844
Iteration 90/1000 | Loss: 0.00000844
Iteration 91/1000 | Loss: 0.00000844
Iteration 92/1000 | Loss: 0.00000844
Iteration 93/1000 | Loss: 0.00000843
Iteration 94/1000 | Loss: 0.00000843
Iteration 95/1000 | Loss: 0.00000843
Iteration 96/1000 | Loss: 0.00000842
Iteration 97/1000 | Loss: 0.00000842
Iteration 98/1000 | Loss: 0.00000842
Iteration 99/1000 | Loss: 0.00000842
Iteration 100/1000 | Loss: 0.00000842
Iteration 101/1000 | Loss: 0.00000841
Iteration 102/1000 | Loss: 0.00000841
Iteration 103/1000 | Loss: 0.00000841
Iteration 104/1000 | Loss: 0.00000841
Iteration 105/1000 | Loss: 0.00000841
Iteration 106/1000 | Loss: 0.00000841
Iteration 107/1000 | Loss: 0.00000841
Iteration 108/1000 | Loss: 0.00000841
Iteration 109/1000 | Loss: 0.00000841
Iteration 110/1000 | Loss: 0.00000841
Iteration 111/1000 | Loss: 0.00000841
Iteration 112/1000 | Loss: 0.00000841
Iteration 113/1000 | Loss: 0.00000840
Iteration 114/1000 | Loss: 0.00000840
Iteration 115/1000 | Loss: 0.00000840
Iteration 116/1000 | Loss: 0.00000840
Iteration 117/1000 | Loss: 0.00000840
Iteration 118/1000 | Loss: 0.00000839
Iteration 119/1000 | Loss: 0.00000839
Iteration 120/1000 | Loss: 0.00000838
Iteration 121/1000 | Loss: 0.00000838
Iteration 122/1000 | Loss: 0.00000838
Iteration 123/1000 | Loss: 0.00000838
Iteration 124/1000 | Loss: 0.00000838
Iteration 125/1000 | Loss: 0.00000837
Iteration 126/1000 | Loss: 0.00000837
Iteration 127/1000 | Loss: 0.00000837
Iteration 128/1000 | Loss: 0.00000837
Iteration 129/1000 | Loss: 0.00000836
Iteration 130/1000 | Loss: 0.00000836
Iteration 131/1000 | Loss: 0.00000835
Iteration 132/1000 | Loss: 0.00000835
Iteration 133/1000 | Loss: 0.00000834
Iteration 134/1000 | Loss: 0.00000833
Iteration 135/1000 | Loss: 0.00000833
Iteration 136/1000 | Loss: 0.00000833
Iteration 137/1000 | Loss: 0.00000833
Iteration 138/1000 | Loss: 0.00000833
Iteration 139/1000 | Loss: 0.00000833
Iteration 140/1000 | Loss: 0.00000833
Iteration 141/1000 | Loss: 0.00000832
Iteration 142/1000 | Loss: 0.00000832
Iteration 143/1000 | Loss: 0.00000832
Iteration 144/1000 | Loss: 0.00000832
Iteration 145/1000 | Loss: 0.00000832
Iteration 146/1000 | Loss: 0.00000832
Iteration 147/1000 | Loss: 0.00000831
Iteration 148/1000 | Loss: 0.00000831
Iteration 149/1000 | Loss: 0.00000831
Iteration 150/1000 | Loss: 0.00000831
Iteration 151/1000 | Loss: 0.00000831
Iteration 152/1000 | Loss: 0.00000831
Iteration 153/1000 | Loss: 0.00000831
Iteration 154/1000 | Loss: 0.00000831
Iteration 155/1000 | Loss: 0.00000831
Iteration 156/1000 | Loss: 0.00000830
Iteration 157/1000 | Loss: 0.00000830
Iteration 158/1000 | Loss: 0.00000830
Iteration 159/1000 | Loss: 0.00000829
Iteration 160/1000 | Loss: 0.00000829
Iteration 161/1000 | Loss: 0.00000829
Iteration 162/1000 | Loss: 0.00000829
Iteration 163/1000 | Loss: 0.00000829
Iteration 164/1000 | Loss: 0.00000829
Iteration 165/1000 | Loss: 0.00000829
Iteration 166/1000 | Loss: 0.00000828
Iteration 167/1000 | Loss: 0.00000828
Iteration 168/1000 | Loss: 0.00000828
Iteration 169/1000 | Loss: 0.00000828
Iteration 170/1000 | Loss: 0.00000828
Iteration 171/1000 | Loss: 0.00000827
Iteration 172/1000 | Loss: 0.00000827
Iteration 173/1000 | Loss: 0.00000827
Iteration 174/1000 | Loss: 0.00000827
Iteration 175/1000 | Loss: 0.00000827
Iteration 176/1000 | Loss: 0.00000827
Iteration 177/1000 | Loss: 0.00000827
Iteration 178/1000 | Loss: 0.00000827
Iteration 179/1000 | Loss: 0.00000827
Iteration 180/1000 | Loss: 0.00000827
Iteration 181/1000 | Loss: 0.00000827
Iteration 182/1000 | Loss: 0.00000826
Iteration 183/1000 | Loss: 0.00000826
Iteration 184/1000 | Loss: 0.00000826
Iteration 185/1000 | Loss: 0.00000826
Iteration 186/1000 | Loss: 0.00000826
Iteration 187/1000 | Loss: 0.00000826
Iteration 188/1000 | Loss: 0.00000826
Iteration 189/1000 | Loss: 0.00000826
Iteration 190/1000 | Loss: 0.00000825
Iteration 191/1000 | Loss: 0.00000825
Iteration 192/1000 | Loss: 0.00000825
Iteration 193/1000 | Loss: 0.00000825
Iteration 194/1000 | Loss: 0.00000825
Iteration 195/1000 | Loss: 0.00000825
Iteration 196/1000 | Loss: 0.00000825
Iteration 197/1000 | Loss: 0.00000824
Iteration 198/1000 | Loss: 0.00000824
Iteration 199/1000 | Loss: 0.00000824
Iteration 200/1000 | Loss: 0.00000824
Iteration 201/1000 | Loss: 0.00000824
Iteration 202/1000 | Loss: 0.00000824
Iteration 203/1000 | Loss: 0.00000824
Iteration 204/1000 | Loss: 0.00000824
Iteration 205/1000 | Loss: 0.00000824
Iteration 206/1000 | Loss: 0.00000824
Iteration 207/1000 | Loss: 0.00000823
Iteration 208/1000 | Loss: 0.00000823
Iteration 209/1000 | Loss: 0.00000823
Iteration 210/1000 | Loss: 0.00000823
Iteration 211/1000 | Loss: 0.00000823
Iteration 212/1000 | Loss: 0.00000823
Iteration 213/1000 | Loss: 0.00000823
Iteration 214/1000 | Loss: 0.00000823
Iteration 215/1000 | Loss: 0.00000823
Iteration 216/1000 | Loss: 0.00000823
Iteration 217/1000 | Loss: 0.00000823
Iteration 218/1000 | Loss: 0.00000823
Iteration 219/1000 | Loss: 0.00000823
Iteration 220/1000 | Loss: 0.00000823
Iteration 221/1000 | Loss: 0.00000823
Iteration 222/1000 | Loss: 0.00000823
Iteration 223/1000 | Loss: 0.00000823
Iteration 224/1000 | Loss: 0.00000823
Iteration 225/1000 | Loss: 0.00000823
Iteration 226/1000 | Loss: 0.00000823
Iteration 227/1000 | Loss: 0.00000823
Iteration 228/1000 | Loss: 0.00000822
Iteration 229/1000 | Loss: 0.00000822
Iteration 230/1000 | Loss: 0.00000822
Iteration 231/1000 | Loss: 0.00000822
Iteration 232/1000 | Loss: 0.00000822
Iteration 233/1000 | Loss: 0.00000822
Iteration 234/1000 | Loss: 0.00000822
Iteration 235/1000 | Loss: 0.00000822
Iteration 236/1000 | Loss: 0.00000822
Iteration 237/1000 | Loss: 0.00000822
Iteration 238/1000 | Loss: 0.00000822
Iteration 239/1000 | Loss: 0.00000822
Iteration 240/1000 | Loss: 0.00000822
Iteration 241/1000 | Loss: 0.00000822
Iteration 242/1000 | Loss: 0.00000822
Iteration 243/1000 | Loss: 0.00000822
Iteration 244/1000 | Loss: 0.00000822
Iteration 245/1000 | Loss: 0.00000822
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 245. Stopping optimization.
Last 5 losses: [8.223860277212225e-06, 8.223860277212225e-06, 8.223860277212225e-06, 8.223860277212225e-06, 8.223860277212225e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.223860277212225e-06

Optimization complete. Final v2v error: 2.472844362258911 mm

Highest mean error: 3.268724203109741 mm for frame 59

Lowest mean error: 2.147583484649658 mm for frame 84

Saving results

Total time: 40.20329022407532
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00216789
Iteration 2/25 | Loss: 0.00116543
Iteration 3/25 | Loss: 0.00107667
Iteration 4/25 | Loss: 0.00105390
Iteration 5/25 | Loss: 0.00104457
Iteration 6/25 | Loss: 0.00104216
Iteration 7/25 | Loss: 0.00104136
Iteration 8/25 | Loss: 0.00104124
Iteration 9/25 | Loss: 0.00104124
Iteration 10/25 | Loss: 0.00104124
Iteration 11/25 | Loss: 0.00104124
Iteration 12/25 | Loss: 0.00104124
Iteration 13/25 | Loss: 0.00104124
Iteration 14/25 | Loss: 0.00104124
Iteration 15/25 | Loss: 0.00104124
Iteration 16/25 | Loss: 0.00104124
Iteration 17/25 | Loss: 0.00104124
Iteration 18/25 | Loss: 0.00104124
Iteration 19/25 | Loss: 0.00104124
Iteration 20/25 | Loss: 0.00104124
Iteration 21/25 | Loss: 0.00104124
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0010412418050691485, 0.0010412418050691485, 0.0010412418050691485, 0.0010412418050691485, 0.0010412418050691485]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010412418050691485

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30997849
Iteration 2/25 | Loss: 0.00138894
Iteration 3/25 | Loss: 0.00138894
Iteration 4/25 | Loss: 0.00138893
Iteration 5/25 | Loss: 0.00138893
Iteration 6/25 | Loss: 0.00138893
Iteration 7/25 | Loss: 0.00138893
Iteration 8/25 | Loss: 0.00138893
Iteration 9/25 | Loss: 0.00138893
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.0013889346737414598, 0.0013889346737414598, 0.0013889346737414598, 0.0013889346737414598, 0.0013889346737414598]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013889346737414598

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00138893
Iteration 2/1000 | Loss: 0.00003631
Iteration 3/1000 | Loss: 0.00002345
Iteration 4/1000 | Loss: 0.00001689
Iteration 5/1000 | Loss: 0.00001475
Iteration 6/1000 | Loss: 0.00001386
Iteration 7/1000 | Loss: 0.00001317
Iteration 8/1000 | Loss: 0.00001282
Iteration 9/1000 | Loss: 0.00001253
Iteration 10/1000 | Loss: 0.00001245
Iteration 11/1000 | Loss: 0.00001232
Iteration 12/1000 | Loss: 0.00001226
Iteration 13/1000 | Loss: 0.00001223
Iteration 14/1000 | Loss: 0.00001220
Iteration 15/1000 | Loss: 0.00001220
Iteration 16/1000 | Loss: 0.00001219
Iteration 17/1000 | Loss: 0.00001219
Iteration 18/1000 | Loss: 0.00001219
Iteration 19/1000 | Loss: 0.00001218
Iteration 20/1000 | Loss: 0.00001218
Iteration 21/1000 | Loss: 0.00001212
Iteration 22/1000 | Loss: 0.00001212
Iteration 23/1000 | Loss: 0.00001211
Iteration 24/1000 | Loss: 0.00001211
Iteration 25/1000 | Loss: 0.00001204
Iteration 26/1000 | Loss: 0.00001202
Iteration 27/1000 | Loss: 0.00001202
Iteration 28/1000 | Loss: 0.00001201
Iteration 29/1000 | Loss: 0.00001201
Iteration 30/1000 | Loss: 0.00001200
Iteration 31/1000 | Loss: 0.00001198
Iteration 32/1000 | Loss: 0.00001197
Iteration 33/1000 | Loss: 0.00001196
Iteration 34/1000 | Loss: 0.00001196
Iteration 35/1000 | Loss: 0.00001195
Iteration 36/1000 | Loss: 0.00001195
Iteration 37/1000 | Loss: 0.00001194
Iteration 38/1000 | Loss: 0.00001194
Iteration 39/1000 | Loss: 0.00001194
Iteration 40/1000 | Loss: 0.00001194
Iteration 41/1000 | Loss: 0.00001193
Iteration 42/1000 | Loss: 0.00001193
Iteration 43/1000 | Loss: 0.00001193
Iteration 44/1000 | Loss: 0.00001192
Iteration 45/1000 | Loss: 0.00001191
Iteration 46/1000 | Loss: 0.00001189
Iteration 47/1000 | Loss: 0.00001189
Iteration 48/1000 | Loss: 0.00001189
Iteration 49/1000 | Loss: 0.00001189
Iteration 50/1000 | Loss: 0.00001188
Iteration 51/1000 | Loss: 0.00001188
Iteration 52/1000 | Loss: 0.00001186
Iteration 53/1000 | Loss: 0.00001185
Iteration 54/1000 | Loss: 0.00001185
Iteration 55/1000 | Loss: 0.00001185
Iteration 56/1000 | Loss: 0.00001184
Iteration 57/1000 | Loss: 0.00001184
Iteration 58/1000 | Loss: 0.00001184
Iteration 59/1000 | Loss: 0.00001183
Iteration 60/1000 | Loss: 0.00001180
Iteration 61/1000 | Loss: 0.00001180
Iteration 62/1000 | Loss: 0.00001178
Iteration 63/1000 | Loss: 0.00001178
Iteration 64/1000 | Loss: 0.00001178
Iteration 65/1000 | Loss: 0.00001177
Iteration 66/1000 | Loss: 0.00001177
Iteration 67/1000 | Loss: 0.00001177
Iteration 68/1000 | Loss: 0.00001177
Iteration 69/1000 | Loss: 0.00001176
Iteration 70/1000 | Loss: 0.00001176
Iteration 71/1000 | Loss: 0.00001176
Iteration 72/1000 | Loss: 0.00001176
Iteration 73/1000 | Loss: 0.00001175
Iteration 74/1000 | Loss: 0.00001175
Iteration 75/1000 | Loss: 0.00001175
Iteration 76/1000 | Loss: 0.00001174
Iteration 77/1000 | Loss: 0.00001174
Iteration 78/1000 | Loss: 0.00001174
Iteration 79/1000 | Loss: 0.00001173
Iteration 80/1000 | Loss: 0.00001173
Iteration 81/1000 | Loss: 0.00001172
Iteration 82/1000 | Loss: 0.00001172
Iteration 83/1000 | Loss: 0.00001172
Iteration 84/1000 | Loss: 0.00001172
Iteration 85/1000 | Loss: 0.00001172
Iteration 86/1000 | Loss: 0.00001172
Iteration 87/1000 | Loss: 0.00001172
Iteration 88/1000 | Loss: 0.00001172
Iteration 89/1000 | Loss: 0.00001171
Iteration 90/1000 | Loss: 0.00001171
Iteration 91/1000 | Loss: 0.00001171
Iteration 92/1000 | Loss: 0.00001170
Iteration 93/1000 | Loss: 0.00001169
Iteration 94/1000 | Loss: 0.00001169
Iteration 95/1000 | Loss: 0.00001169
Iteration 96/1000 | Loss: 0.00001168
Iteration 97/1000 | Loss: 0.00001168
Iteration 98/1000 | Loss: 0.00001168
Iteration 99/1000 | Loss: 0.00001168
Iteration 100/1000 | Loss: 0.00001168
Iteration 101/1000 | Loss: 0.00001168
Iteration 102/1000 | Loss: 0.00001168
Iteration 103/1000 | Loss: 0.00001168
Iteration 104/1000 | Loss: 0.00001168
Iteration 105/1000 | Loss: 0.00001168
Iteration 106/1000 | Loss: 0.00001168
Iteration 107/1000 | Loss: 0.00001168
Iteration 108/1000 | Loss: 0.00001168
Iteration 109/1000 | Loss: 0.00001168
Iteration 110/1000 | Loss: 0.00001168
Iteration 111/1000 | Loss: 0.00001168
Iteration 112/1000 | Loss: 0.00001168
Iteration 113/1000 | Loss: 0.00001167
Iteration 114/1000 | Loss: 0.00001167
Iteration 115/1000 | Loss: 0.00001166
Iteration 116/1000 | Loss: 0.00001166
Iteration 117/1000 | Loss: 0.00001165
Iteration 118/1000 | Loss: 0.00001165
Iteration 119/1000 | Loss: 0.00001165
Iteration 120/1000 | Loss: 0.00001165
Iteration 121/1000 | Loss: 0.00001165
Iteration 122/1000 | Loss: 0.00001165
Iteration 123/1000 | Loss: 0.00001165
Iteration 124/1000 | Loss: 0.00001164
Iteration 125/1000 | Loss: 0.00001164
Iteration 126/1000 | Loss: 0.00001164
Iteration 127/1000 | Loss: 0.00001164
Iteration 128/1000 | Loss: 0.00001164
Iteration 129/1000 | Loss: 0.00001164
Iteration 130/1000 | Loss: 0.00001164
Iteration 131/1000 | Loss: 0.00001164
Iteration 132/1000 | Loss: 0.00001164
Iteration 133/1000 | Loss: 0.00001164
Iteration 134/1000 | Loss: 0.00001164
Iteration 135/1000 | Loss: 0.00001164
Iteration 136/1000 | Loss: 0.00001164
Iteration 137/1000 | Loss: 0.00001164
Iteration 138/1000 | Loss: 0.00001163
Iteration 139/1000 | Loss: 0.00001163
Iteration 140/1000 | Loss: 0.00001163
Iteration 141/1000 | Loss: 0.00001163
Iteration 142/1000 | Loss: 0.00001163
Iteration 143/1000 | Loss: 0.00001163
Iteration 144/1000 | Loss: 0.00001163
Iteration 145/1000 | Loss: 0.00001163
Iteration 146/1000 | Loss: 0.00001162
Iteration 147/1000 | Loss: 0.00001162
Iteration 148/1000 | Loss: 0.00001162
Iteration 149/1000 | Loss: 0.00001162
Iteration 150/1000 | Loss: 0.00001162
Iteration 151/1000 | Loss: 0.00001162
Iteration 152/1000 | Loss: 0.00001162
Iteration 153/1000 | Loss: 0.00001162
Iteration 154/1000 | Loss: 0.00001162
Iteration 155/1000 | Loss: 0.00001162
Iteration 156/1000 | Loss: 0.00001162
Iteration 157/1000 | Loss: 0.00001162
Iteration 158/1000 | Loss: 0.00001161
Iteration 159/1000 | Loss: 0.00001161
Iteration 160/1000 | Loss: 0.00001161
Iteration 161/1000 | Loss: 0.00001161
Iteration 162/1000 | Loss: 0.00001161
Iteration 163/1000 | Loss: 0.00001161
Iteration 164/1000 | Loss: 0.00001161
Iteration 165/1000 | Loss: 0.00001161
Iteration 166/1000 | Loss: 0.00001161
Iteration 167/1000 | Loss: 0.00001161
Iteration 168/1000 | Loss: 0.00001161
Iteration 169/1000 | Loss: 0.00001161
Iteration 170/1000 | Loss: 0.00001161
Iteration 171/1000 | Loss: 0.00001161
Iteration 172/1000 | Loss: 0.00001160
Iteration 173/1000 | Loss: 0.00001160
Iteration 174/1000 | Loss: 0.00001160
Iteration 175/1000 | Loss: 0.00001160
Iteration 176/1000 | Loss: 0.00001160
Iteration 177/1000 | Loss: 0.00001160
Iteration 178/1000 | Loss: 0.00001160
Iteration 179/1000 | Loss: 0.00001160
Iteration 180/1000 | Loss: 0.00001160
Iteration 181/1000 | Loss: 0.00001160
Iteration 182/1000 | Loss: 0.00001160
Iteration 183/1000 | Loss: 0.00001159
Iteration 184/1000 | Loss: 0.00001159
Iteration 185/1000 | Loss: 0.00001159
Iteration 186/1000 | Loss: 0.00001159
Iteration 187/1000 | Loss: 0.00001159
Iteration 188/1000 | Loss: 0.00001159
Iteration 189/1000 | Loss: 0.00001159
Iteration 190/1000 | Loss: 0.00001159
Iteration 191/1000 | Loss: 0.00001159
Iteration 192/1000 | Loss: 0.00001159
Iteration 193/1000 | Loss: 0.00001158
Iteration 194/1000 | Loss: 0.00001158
Iteration 195/1000 | Loss: 0.00001158
Iteration 196/1000 | Loss: 0.00001158
Iteration 197/1000 | Loss: 0.00001158
Iteration 198/1000 | Loss: 0.00001158
Iteration 199/1000 | Loss: 0.00001158
Iteration 200/1000 | Loss: 0.00001158
Iteration 201/1000 | Loss: 0.00001158
Iteration 202/1000 | Loss: 0.00001158
Iteration 203/1000 | Loss: 0.00001158
Iteration 204/1000 | Loss: 0.00001158
Iteration 205/1000 | Loss: 0.00001158
Iteration 206/1000 | Loss: 0.00001158
Iteration 207/1000 | Loss: 0.00001158
Iteration 208/1000 | Loss: 0.00001158
Iteration 209/1000 | Loss: 0.00001158
Iteration 210/1000 | Loss: 0.00001158
Iteration 211/1000 | Loss: 0.00001158
Iteration 212/1000 | Loss: 0.00001158
Iteration 213/1000 | Loss: 0.00001158
Iteration 214/1000 | Loss: 0.00001157
Iteration 215/1000 | Loss: 0.00001157
Iteration 216/1000 | Loss: 0.00001157
Iteration 217/1000 | Loss: 0.00001157
Iteration 218/1000 | Loss: 0.00001157
Iteration 219/1000 | Loss: 0.00001157
Iteration 220/1000 | Loss: 0.00001157
Iteration 221/1000 | Loss: 0.00001157
Iteration 222/1000 | Loss: 0.00001157
Iteration 223/1000 | Loss: 0.00001157
Iteration 224/1000 | Loss: 0.00001157
Iteration 225/1000 | Loss: 0.00001157
Iteration 226/1000 | Loss: 0.00001157
Iteration 227/1000 | Loss: 0.00001157
Iteration 228/1000 | Loss: 0.00001157
Iteration 229/1000 | Loss: 0.00001157
Iteration 230/1000 | Loss: 0.00001157
Iteration 231/1000 | Loss: 0.00001157
Iteration 232/1000 | Loss: 0.00001157
Iteration 233/1000 | Loss: 0.00001157
Iteration 234/1000 | Loss: 0.00001157
Iteration 235/1000 | Loss: 0.00001157
Iteration 236/1000 | Loss: 0.00001157
Iteration 237/1000 | Loss: 0.00001157
Iteration 238/1000 | Loss: 0.00001157
Iteration 239/1000 | Loss: 0.00001157
Iteration 240/1000 | Loss: 0.00001157
Iteration 241/1000 | Loss: 0.00001157
Iteration 242/1000 | Loss: 0.00001157
Iteration 243/1000 | Loss: 0.00001157
Iteration 244/1000 | Loss: 0.00001157
Iteration 245/1000 | Loss: 0.00001157
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 245. Stopping optimization.
Last 5 losses: [1.156541929958621e-05, 1.156541929958621e-05, 1.156541929958621e-05, 1.156541929958621e-05, 1.156541929958621e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.156541929958621e-05

Optimization complete. Final v2v error: 2.910789966583252 mm

Highest mean error: 3.24699068069458 mm for frame 48

Lowest mean error: 2.5960428714752197 mm for frame 119

Saving results

Total time: 41.56782603263855
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00388268
Iteration 2/25 | Loss: 0.00119790
Iteration 3/25 | Loss: 0.00111456
Iteration 4/25 | Loss: 0.00110755
Iteration 5/25 | Loss: 0.00110508
Iteration 6/25 | Loss: 0.00110450
Iteration 7/25 | Loss: 0.00110450
Iteration 8/25 | Loss: 0.00110450
Iteration 9/25 | Loss: 0.00110450
Iteration 10/25 | Loss: 0.00110450
Iteration 11/25 | Loss: 0.00110450
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011045039864256978, 0.0011045039864256978, 0.0011045039864256978, 0.0011045039864256978, 0.0011045039864256978]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011045039864256978

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36632764
Iteration 2/25 | Loss: 0.00094897
Iteration 3/25 | Loss: 0.00094897
Iteration 4/25 | Loss: 0.00094897
Iteration 5/25 | Loss: 0.00094897
Iteration 6/25 | Loss: 0.00094897
Iteration 7/25 | Loss: 0.00094897
Iteration 8/25 | Loss: 0.00094897
Iteration 9/25 | Loss: 0.00094897
Iteration 10/25 | Loss: 0.00094897
Iteration 11/25 | Loss: 0.00094897
Iteration 12/25 | Loss: 0.00094897
Iteration 13/25 | Loss: 0.00094897
Iteration 14/25 | Loss: 0.00094897
Iteration 15/25 | Loss: 0.00094897
Iteration 16/25 | Loss: 0.00094897
Iteration 17/25 | Loss: 0.00094897
Iteration 18/25 | Loss: 0.00094897
Iteration 19/25 | Loss: 0.00094897
Iteration 20/25 | Loss: 0.00094897
Iteration 21/25 | Loss: 0.00094897
Iteration 22/25 | Loss: 0.00094897
Iteration 23/25 | Loss: 0.00094897
Iteration 24/25 | Loss: 0.00094897
Iteration 25/25 | Loss: 0.00094897

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094897
Iteration 2/1000 | Loss: 0.00002402
Iteration 3/1000 | Loss: 0.00001652
Iteration 4/1000 | Loss: 0.00001374
Iteration 5/1000 | Loss: 0.00001289
Iteration 6/1000 | Loss: 0.00001228
Iteration 7/1000 | Loss: 0.00001186
Iteration 8/1000 | Loss: 0.00001154
Iteration 9/1000 | Loss: 0.00001138
Iteration 10/1000 | Loss: 0.00001133
Iteration 11/1000 | Loss: 0.00001114
Iteration 12/1000 | Loss: 0.00001114
Iteration 13/1000 | Loss: 0.00001111
Iteration 14/1000 | Loss: 0.00001109
Iteration 15/1000 | Loss: 0.00001108
Iteration 16/1000 | Loss: 0.00001107
Iteration 17/1000 | Loss: 0.00001106
Iteration 18/1000 | Loss: 0.00001105
Iteration 19/1000 | Loss: 0.00001104
Iteration 20/1000 | Loss: 0.00001102
Iteration 21/1000 | Loss: 0.00001101
Iteration 22/1000 | Loss: 0.00001101
Iteration 23/1000 | Loss: 0.00001099
Iteration 24/1000 | Loss: 0.00001099
Iteration 25/1000 | Loss: 0.00001098
Iteration 26/1000 | Loss: 0.00001097
Iteration 27/1000 | Loss: 0.00001096
Iteration 28/1000 | Loss: 0.00001095
Iteration 29/1000 | Loss: 0.00001095
Iteration 30/1000 | Loss: 0.00001094
Iteration 31/1000 | Loss: 0.00001093
Iteration 32/1000 | Loss: 0.00001092
Iteration 33/1000 | Loss: 0.00001086
Iteration 34/1000 | Loss: 0.00001083
Iteration 35/1000 | Loss: 0.00001082
Iteration 36/1000 | Loss: 0.00001081
Iteration 37/1000 | Loss: 0.00001078
Iteration 38/1000 | Loss: 0.00001077
Iteration 39/1000 | Loss: 0.00001077
Iteration 40/1000 | Loss: 0.00001077
Iteration 41/1000 | Loss: 0.00001077
Iteration 42/1000 | Loss: 0.00001077
Iteration 43/1000 | Loss: 0.00001076
Iteration 44/1000 | Loss: 0.00001076
Iteration 45/1000 | Loss: 0.00001075
Iteration 46/1000 | Loss: 0.00001075
Iteration 47/1000 | Loss: 0.00001075
Iteration 48/1000 | Loss: 0.00001075
Iteration 49/1000 | Loss: 0.00001075
Iteration 50/1000 | Loss: 0.00001075
Iteration 51/1000 | Loss: 0.00001075
Iteration 52/1000 | Loss: 0.00001074
Iteration 53/1000 | Loss: 0.00001074
Iteration 54/1000 | Loss: 0.00001074
Iteration 55/1000 | Loss: 0.00001074
Iteration 56/1000 | Loss: 0.00001074
Iteration 57/1000 | Loss: 0.00001074
Iteration 58/1000 | Loss: 0.00001074
Iteration 59/1000 | Loss: 0.00001074
Iteration 60/1000 | Loss: 0.00001073
Iteration 61/1000 | Loss: 0.00001073
Iteration 62/1000 | Loss: 0.00001072
Iteration 63/1000 | Loss: 0.00001072
Iteration 64/1000 | Loss: 0.00001072
Iteration 65/1000 | Loss: 0.00001072
Iteration 66/1000 | Loss: 0.00001072
Iteration 67/1000 | Loss: 0.00001071
Iteration 68/1000 | Loss: 0.00001071
Iteration 69/1000 | Loss: 0.00001071
Iteration 70/1000 | Loss: 0.00001070
Iteration 71/1000 | Loss: 0.00001070
Iteration 72/1000 | Loss: 0.00001070
Iteration 73/1000 | Loss: 0.00001070
Iteration 74/1000 | Loss: 0.00001069
Iteration 75/1000 | Loss: 0.00001069
Iteration 76/1000 | Loss: 0.00001069
Iteration 77/1000 | Loss: 0.00001069
Iteration 78/1000 | Loss: 0.00001069
Iteration 79/1000 | Loss: 0.00001068
Iteration 80/1000 | Loss: 0.00001068
Iteration 81/1000 | Loss: 0.00001068
Iteration 82/1000 | Loss: 0.00001068
Iteration 83/1000 | Loss: 0.00001068
Iteration 84/1000 | Loss: 0.00001068
Iteration 85/1000 | Loss: 0.00001067
Iteration 86/1000 | Loss: 0.00001067
Iteration 87/1000 | Loss: 0.00001067
Iteration 88/1000 | Loss: 0.00001067
Iteration 89/1000 | Loss: 0.00001067
Iteration 90/1000 | Loss: 0.00001067
Iteration 91/1000 | Loss: 0.00001067
Iteration 92/1000 | Loss: 0.00001066
Iteration 93/1000 | Loss: 0.00001066
Iteration 94/1000 | Loss: 0.00001066
Iteration 95/1000 | Loss: 0.00001066
Iteration 96/1000 | Loss: 0.00001066
Iteration 97/1000 | Loss: 0.00001066
Iteration 98/1000 | Loss: 0.00001066
Iteration 99/1000 | Loss: 0.00001066
Iteration 100/1000 | Loss: 0.00001066
Iteration 101/1000 | Loss: 0.00001065
Iteration 102/1000 | Loss: 0.00001065
Iteration 103/1000 | Loss: 0.00001065
Iteration 104/1000 | Loss: 0.00001065
Iteration 105/1000 | Loss: 0.00001065
Iteration 106/1000 | Loss: 0.00001064
Iteration 107/1000 | Loss: 0.00001064
Iteration 108/1000 | Loss: 0.00001064
Iteration 109/1000 | Loss: 0.00001064
Iteration 110/1000 | Loss: 0.00001063
Iteration 111/1000 | Loss: 0.00001063
Iteration 112/1000 | Loss: 0.00001063
Iteration 113/1000 | Loss: 0.00001063
Iteration 114/1000 | Loss: 0.00001063
Iteration 115/1000 | Loss: 0.00001063
Iteration 116/1000 | Loss: 0.00001063
Iteration 117/1000 | Loss: 0.00001063
Iteration 118/1000 | Loss: 0.00001063
Iteration 119/1000 | Loss: 0.00001063
Iteration 120/1000 | Loss: 0.00001063
Iteration 121/1000 | Loss: 0.00001063
Iteration 122/1000 | Loss: 0.00001063
Iteration 123/1000 | Loss: 0.00001063
Iteration 124/1000 | Loss: 0.00001063
Iteration 125/1000 | Loss: 0.00001063
Iteration 126/1000 | Loss: 0.00001063
Iteration 127/1000 | Loss: 0.00001063
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.0629313692334108e-05, 1.0629313692334108e-05, 1.0629313692334108e-05, 1.0629313692334108e-05, 1.0629313692334108e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0629313692334108e-05

Optimization complete. Final v2v error: 2.758918046951294 mm

Highest mean error: 4.3286662101745605 mm for frame 39

Lowest mean error: 2.347240686416626 mm for frame 75

Saving results

Total time: 34.16488862037659
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01034759
Iteration 2/25 | Loss: 0.00215751
Iteration 3/25 | Loss: 0.00155903
Iteration 4/25 | Loss: 0.00126163
Iteration 5/25 | Loss: 0.00119649
Iteration 6/25 | Loss: 0.00117807
Iteration 7/25 | Loss: 0.00114399
Iteration 8/25 | Loss: 0.00113212
Iteration 9/25 | Loss: 0.00111036
Iteration 10/25 | Loss: 0.00109105
Iteration 11/25 | Loss: 0.00107999
Iteration 12/25 | Loss: 0.00107590
Iteration 13/25 | Loss: 0.00107313
Iteration 14/25 | Loss: 0.00107207
Iteration 15/25 | Loss: 0.00107132
Iteration 16/25 | Loss: 0.00107086
Iteration 17/25 | Loss: 0.00107102
Iteration 18/25 | Loss: 0.00107178
Iteration 19/25 | Loss: 0.00107057
Iteration 20/25 | Loss: 0.00106940
Iteration 21/25 | Loss: 0.00106940
Iteration 22/25 | Loss: 0.00106940
Iteration 23/25 | Loss: 0.00106940
Iteration 24/25 | Loss: 0.00106940
Iteration 25/25 | Loss: 0.00106940

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.85787773
Iteration 2/25 | Loss: 0.00086721
Iteration 3/25 | Loss: 0.00076832
Iteration 4/25 | Loss: 0.00076832
Iteration 5/25 | Loss: 0.00076832
Iteration 6/25 | Loss: 0.00076832
Iteration 7/25 | Loss: 0.00076832
Iteration 8/25 | Loss: 0.00076832
Iteration 9/25 | Loss: 0.00076832
Iteration 10/25 | Loss: 0.00076832
Iteration 11/25 | Loss: 0.00076832
Iteration 12/25 | Loss: 0.00076832
Iteration 13/25 | Loss: 0.00076832
Iteration 14/25 | Loss: 0.00076832
Iteration 15/25 | Loss: 0.00076832
Iteration 16/25 | Loss: 0.00076832
Iteration 17/25 | Loss: 0.00076832
Iteration 18/25 | Loss: 0.00076832
Iteration 19/25 | Loss: 0.00076832
Iteration 20/25 | Loss: 0.00076832
Iteration 21/25 | Loss: 0.00076832
Iteration 22/25 | Loss: 0.00076832
Iteration 23/25 | Loss: 0.00076832
Iteration 24/25 | Loss: 0.00076832
Iteration 25/25 | Loss: 0.00076832

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076832
Iteration 2/1000 | Loss: 0.00011345
Iteration 3/1000 | Loss: 0.00012803
Iteration 4/1000 | Loss: 0.00003546
Iteration 5/1000 | Loss: 0.00001447
Iteration 6/1000 | Loss: 0.00007484
Iteration 7/1000 | Loss: 0.00001342
Iteration 8/1000 | Loss: 0.00001296
Iteration 9/1000 | Loss: 0.00002953
Iteration 10/1000 | Loss: 0.00001250
Iteration 11/1000 | Loss: 0.00002306
Iteration 12/1000 | Loss: 0.00014093
Iteration 13/1000 | Loss: 0.00015924
Iteration 14/1000 | Loss: 0.00002400
Iteration 15/1000 | Loss: 0.00001204
Iteration 16/1000 | Loss: 0.00001197
Iteration 17/1000 | Loss: 0.00001380
Iteration 18/1000 | Loss: 0.00002045
Iteration 19/1000 | Loss: 0.00001360
Iteration 20/1000 | Loss: 0.00001640
Iteration 21/1000 | Loss: 0.00001181
Iteration 22/1000 | Loss: 0.00001186
Iteration 23/1000 | Loss: 0.00001186
Iteration 24/1000 | Loss: 0.00001186
Iteration 25/1000 | Loss: 0.00001186
Iteration 26/1000 | Loss: 0.00001185
Iteration 27/1000 | Loss: 0.00001914
Iteration 28/1000 | Loss: 0.00001169
Iteration 29/1000 | Loss: 0.00001350
Iteration 30/1000 | Loss: 0.00001164
Iteration 31/1000 | Loss: 0.00001163
Iteration 32/1000 | Loss: 0.00001163
Iteration 33/1000 | Loss: 0.00001163
Iteration 34/1000 | Loss: 0.00001163
Iteration 35/1000 | Loss: 0.00001163
Iteration 36/1000 | Loss: 0.00001163
Iteration 37/1000 | Loss: 0.00001163
Iteration 38/1000 | Loss: 0.00001163
Iteration 39/1000 | Loss: 0.00001162
Iteration 40/1000 | Loss: 0.00001162
Iteration 41/1000 | Loss: 0.00001161
Iteration 42/1000 | Loss: 0.00001161
Iteration 43/1000 | Loss: 0.00001160
Iteration 44/1000 | Loss: 0.00001160
Iteration 45/1000 | Loss: 0.00001159
Iteration 46/1000 | Loss: 0.00001158
Iteration 47/1000 | Loss: 0.00001158
Iteration 48/1000 | Loss: 0.00001158
Iteration 49/1000 | Loss: 0.00001157
Iteration 50/1000 | Loss: 0.00001157
Iteration 51/1000 | Loss: 0.00001157
Iteration 52/1000 | Loss: 0.00001157
Iteration 53/1000 | Loss: 0.00001156
Iteration 54/1000 | Loss: 0.00001156
Iteration 55/1000 | Loss: 0.00001155
Iteration 56/1000 | Loss: 0.00001155
Iteration 57/1000 | Loss: 0.00001155
Iteration 58/1000 | Loss: 0.00001155
Iteration 59/1000 | Loss: 0.00001154
Iteration 60/1000 | Loss: 0.00001153
Iteration 61/1000 | Loss: 0.00001152
Iteration 62/1000 | Loss: 0.00001151
Iteration 63/1000 | Loss: 0.00001150
Iteration 64/1000 | Loss: 0.00001150
Iteration 65/1000 | Loss: 0.00001150
Iteration 66/1000 | Loss: 0.00001149
Iteration 67/1000 | Loss: 0.00001149
Iteration 68/1000 | Loss: 0.00001149
Iteration 69/1000 | Loss: 0.00001148
Iteration 70/1000 | Loss: 0.00001148
Iteration 71/1000 | Loss: 0.00001148
Iteration 72/1000 | Loss: 0.00001148
Iteration 73/1000 | Loss: 0.00001148
Iteration 74/1000 | Loss: 0.00001148
Iteration 75/1000 | Loss: 0.00001147
Iteration 76/1000 | Loss: 0.00001147
Iteration 77/1000 | Loss: 0.00001147
Iteration 78/1000 | Loss: 0.00001147
Iteration 79/1000 | Loss: 0.00001147
Iteration 80/1000 | Loss: 0.00001147
Iteration 81/1000 | Loss: 0.00001146
Iteration 82/1000 | Loss: 0.00001146
Iteration 83/1000 | Loss: 0.00001145
Iteration 84/1000 | Loss: 0.00001145
Iteration 85/1000 | Loss: 0.00001145
Iteration 86/1000 | Loss: 0.00001145
Iteration 87/1000 | Loss: 0.00002915
Iteration 88/1000 | Loss: 0.00001382
Iteration 89/1000 | Loss: 0.00001142
Iteration 90/1000 | Loss: 0.00001142
Iteration 91/1000 | Loss: 0.00001142
Iteration 92/1000 | Loss: 0.00001199
Iteration 93/1000 | Loss: 0.00001494
Iteration 94/1000 | Loss: 0.00001184
Iteration 95/1000 | Loss: 0.00001177
Iteration 96/1000 | Loss: 0.00001139
Iteration 97/1000 | Loss: 0.00001139
Iteration 98/1000 | Loss: 0.00001139
Iteration 99/1000 | Loss: 0.00001138
Iteration 100/1000 | Loss: 0.00001138
Iteration 101/1000 | Loss: 0.00001138
Iteration 102/1000 | Loss: 0.00001138
Iteration 103/1000 | Loss: 0.00001137
Iteration 104/1000 | Loss: 0.00001137
Iteration 105/1000 | Loss: 0.00001137
Iteration 106/1000 | Loss: 0.00001137
Iteration 107/1000 | Loss: 0.00001137
Iteration 108/1000 | Loss: 0.00001137
Iteration 109/1000 | Loss: 0.00001137
Iteration 110/1000 | Loss: 0.00001137
Iteration 111/1000 | Loss: 0.00001137
Iteration 112/1000 | Loss: 0.00001136
Iteration 113/1000 | Loss: 0.00001136
Iteration 114/1000 | Loss: 0.00001136
Iteration 115/1000 | Loss: 0.00001136
Iteration 116/1000 | Loss: 0.00001569
Iteration 117/1000 | Loss: 0.00001136
Iteration 118/1000 | Loss: 0.00001130
Iteration 119/1000 | Loss: 0.00001130
Iteration 120/1000 | Loss: 0.00001130
Iteration 121/1000 | Loss: 0.00001130
Iteration 122/1000 | Loss: 0.00001130
Iteration 123/1000 | Loss: 0.00001130
Iteration 124/1000 | Loss: 0.00001130
Iteration 125/1000 | Loss: 0.00001130
Iteration 126/1000 | Loss: 0.00001130
Iteration 127/1000 | Loss: 0.00001130
Iteration 128/1000 | Loss: 0.00001130
Iteration 129/1000 | Loss: 0.00001130
Iteration 130/1000 | Loss: 0.00001130
Iteration 131/1000 | Loss: 0.00001130
Iteration 132/1000 | Loss: 0.00001129
Iteration 133/1000 | Loss: 0.00001129
Iteration 134/1000 | Loss: 0.00001129
Iteration 135/1000 | Loss: 0.00001129
Iteration 136/1000 | Loss: 0.00001129
Iteration 137/1000 | Loss: 0.00001129
Iteration 138/1000 | Loss: 0.00001129
Iteration 139/1000 | Loss: 0.00001129
Iteration 140/1000 | Loss: 0.00001129
Iteration 141/1000 | Loss: 0.00001129
Iteration 142/1000 | Loss: 0.00001129
Iteration 143/1000 | Loss: 0.00001129
Iteration 144/1000 | Loss: 0.00001129
Iteration 145/1000 | Loss: 0.00001129
Iteration 146/1000 | Loss: 0.00001129
Iteration 147/1000 | Loss: 0.00001129
Iteration 148/1000 | Loss: 0.00001129
Iteration 149/1000 | Loss: 0.00001129
Iteration 150/1000 | Loss: 0.00001129
Iteration 151/1000 | Loss: 0.00001129
Iteration 152/1000 | Loss: 0.00001129
Iteration 153/1000 | Loss: 0.00001129
Iteration 154/1000 | Loss: 0.00001129
Iteration 155/1000 | Loss: 0.00001129
Iteration 156/1000 | Loss: 0.00001129
Iteration 157/1000 | Loss: 0.00001129
Iteration 158/1000 | Loss: 0.00001129
Iteration 159/1000 | Loss: 0.00001129
Iteration 160/1000 | Loss: 0.00001129
Iteration 161/1000 | Loss: 0.00001129
Iteration 162/1000 | Loss: 0.00001129
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.1285170330666006e-05, 1.1285170330666006e-05, 1.1285170330666006e-05, 1.1285170330666006e-05, 1.1285170330666006e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1285170330666006e-05

Optimization complete. Final v2v error: 2.8661577701568604 mm

Highest mean error: 3.184735059738159 mm for frame 25

Lowest mean error: 2.5254671573638916 mm for frame 62

Saving results

Total time: 83.66976165771484
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00420333
Iteration 2/25 | Loss: 0.00124442
Iteration 3/25 | Loss: 0.00110576
Iteration 4/25 | Loss: 0.00109408
Iteration 5/25 | Loss: 0.00109018
Iteration 6/25 | Loss: 0.00108933
Iteration 7/25 | Loss: 0.00108933
Iteration 8/25 | Loss: 0.00108933
Iteration 9/25 | Loss: 0.00108933
Iteration 10/25 | Loss: 0.00108933
Iteration 11/25 | Loss: 0.00108933
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010893335565924644, 0.0010893335565924644, 0.0010893335565924644, 0.0010893335565924644, 0.0010893335565924644]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010893335565924644

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45164144
Iteration 2/25 | Loss: 0.00083397
Iteration 3/25 | Loss: 0.00083397
Iteration 4/25 | Loss: 0.00083397
Iteration 5/25 | Loss: 0.00083397
Iteration 6/25 | Loss: 0.00083397
Iteration 7/25 | Loss: 0.00083397
Iteration 8/25 | Loss: 0.00083397
Iteration 9/25 | Loss: 0.00083397
Iteration 10/25 | Loss: 0.00083397
Iteration 11/25 | Loss: 0.00083397
Iteration 12/25 | Loss: 0.00083397
Iteration 13/25 | Loss: 0.00083397
Iteration 14/25 | Loss: 0.00083397
Iteration 15/25 | Loss: 0.00083397
Iteration 16/25 | Loss: 0.00083397
Iteration 17/25 | Loss: 0.00083397
Iteration 18/25 | Loss: 0.00083397
Iteration 19/25 | Loss: 0.00083397
Iteration 20/25 | Loss: 0.00083397
Iteration 21/25 | Loss: 0.00083397
Iteration 22/25 | Loss: 0.00083397
Iteration 23/25 | Loss: 0.00083397
Iteration 24/25 | Loss: 0.00083397
Iteration 25/25 | Loss: 0.00083397

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083397
Iteration 2/1000 | Loss: 0.00002790
Iteration 3/1000 | Loss: 0.00001765
Iteration 4/1000 | Loss: 0.00001629
Iteration 5/1000 | Loss: 0.00001565
Iteration 6/1000 | Loss: 0.00001512
Iteration 7/1000 | Loss: 0.00001471
Iteration 8/1000 | Loss: 0.00001445
Iteration 9/1000 | Loss: 0.00001422
Iteration 10/1000 | Loss: 0.00001417
Iteration 11/1000 | Loss: 0.00001410
Iteration 12/1000 | Loss: 0.00001400
Iteration 13/1000 | Loss: 0.00001393
Iteration 14/1000 | Loss: 0.00001393
Iteration 15/1000 | Loss: 0.00001390
Iteration 16/1000 | Loss: 0.00001389
Iteration 17/1000 | Loss: 0.00001388
Iteration 18/1000 | Loss: 0.00001387
Iteration 19/1000 | Loss: 0.00001383
Iteration 20/1000 | Loss: 0.00001383
Iteration 21/1000 | Loss: 0.00001383
Iteration 22/1000 | Loss: 0.00001381
Iteration 23/1000 | Loss: 0.00001380
Iteration 24/1000 | Loss: 0.00001379
Iteration 25/1000 | Loss: 0.00001379
Iteration 26/1000 | Loss: 0.00001377
Iteration 27/1000 | Loss: 0.00001377
Iteration 28/1000 | Loss: 0.00001376
Iteration 29/1000 | Loss: 0.00001375
Iteration 30/1000 | Loss: 0.00001375
Iteration 31/1000 | Loss: 0.00001374
Iteration 32/1000 | Loss: 0.00001370
Iteration 33/1000 | Loss: 0.00001370
Iteration 34/1000 | Loss: 0.00001370
Iteration 35/1000 | Loss: 0.00001370
Iteration 36/1000 | Loss: 0.00001369
Iteration 37/1000 | Loss: 0.00001369
Iteration 38/1000 | Loss: 0.00001369
Iteration 39/1000 | Loss: 0.00001369
Iteration 40/1000 | Loss: 0.00001369
Iteration 41/1000 | Loss: 0.00001369
Iteration 42/1000 | Loss: 0.00001369
Iteration 43/1000 | Loss: 0.00001366
Iteration 44/1000 | Loss: 0.00001366
Iteration 45/1000 | Loss: 0.00001366
Iteration 46/1000 | Loss: 0.00001366
Iteration 47/1000 | Loss: 0.00001366
Iteration 48/1000 | Loss: 0.00001366
Iteration 49/1000 | Loss: 0.00001366
Iteration 50/1000 | Loss: 0.00001366
Iteration 51/1000 | Loss: 0.00001365
Iteration 52/1000 | Loss: 0.00001365
Iteration 53/1000 | Loss: 0.00001365
Iteration 54/1000 | Loss: 0.00001364
Iteration 55/1000 | Loss: 0.00001363
Iteration 56/1000 | Loss: 0.00001363
Iteration 57/1000 | Loss: 0.00001363
Iteration 58/1000 | Loss: 0.00001362
Iteration 59/1000 | Loss: 0.00001362
Iteration 60/1000 | Loss: 0.00001362
Iteration 61/1000 | Loss: 0.00001360
Iteration 62/1000 | Loss: 0.00001360
Iteration 63/1000 | Loss: 0.00001360
Iteration 64/1000 | Loss: 0.00001360
Iteration 65/1000 | Loss: 0.00001360
Iteration 66/1000 | Loss: 0.00001360
Iteration 67/1000 | Loss: 0.00001360
Iteration 68/1000 | Loss: 0.00001360
Iteration 69/1000 | Loss: 0.00001360
Iteration 70/1000 | Loss: 0.00001360
Iteration 71/1000 | Loss: 0.00001360
Iteration 72/1000 | Loss: 0.00001360
Iteration 73/1000 | Loss: 0.00001359
Iteration 74/1000 | Loss: 0.00001359
Iteration 75/1000 | Loss: 0.00001359
Iteration 76/1000 | Loss: 0.00001358
Iteration 77/1000 | Loss: 0.00001357
Iteration 78/1000 | Loss: 0.00001357
Iteration 79/1000 | Loss: 0.00001356
Iteration 80/1000 | Loss: 0.00001356
Iteration 81/1000 | Loss: 0.00001355
Iteration 82/1000 | Loss: 0.00001355
Iteration 83/1000 | Loss: 0.00001354
Iteration 84/1000 | Loss: 0.00001353
Iteration 85/1000 | Loss: 0.00001352
Iteration 86/1000 | Loss: 0.00001351
Iteration 87/1000 | Loss: 0.00001351
Iteration 88/1000 | Loss: 0.00001351
Iteration 89/1000 | Loss: 0.00001351
Iteration 90/1000 | Loss: 0.00001351
Iteration 91/1000 | Loss: 0.00001350
Iteration 92/1000 | Loss: 0.00001350
Iteration 93/1000 | Loss: 0.00001349
Iteration 94/1000 | Loss: 0.00001348
Iteration 95/1000 | Loss: 0.00001348
Iteration 96/1000 | Loss: 0.00001348
Iteration 97/1000 | Loss: 0.00001347
Iteration 98/1000 | Loss: 0.00001347
Iteration 99/1000 | Loss: 0.00001346
Iteration 100/1000 | Loss: 0.00001346
Iteration 101/1000 | Loss: 0.00001345
Iteration 102/1000 | Loss: 0.00001345
Iteration 103/1000 | Loss: 0.00001345
Iteration 104/1000 | Loss: 0.00001344
Iteration 105/1000 | Loss: 0.00001344
Iteration 106/1000 | Loss: 0.00001344
Iteration 107/1000 | Loss: 0.00001343
Iteration 108/1000 | Loss: 0.00001343
Iteration 109/1000 | Loss: 0.00001343
Iteration 110/1000 | Loss: 0.00001342
Iteration 111/1000 | Loss: 0.00001342
Iteration 112/1000 | Loss: 0.00001342
Iteration 113/1000 | Loss: 0.00001341
Iteration 114/1000 | Loss: 0.00001341
Iteration 115/1000 | Loss: 0.00001341
Iteration 116/1000 | Loss: 0.00001340
Iteration 117/1000 | Loss: 0.00001340
Iteration 118/1000 | Loss: 0.00001339
Iteration 119/1000 | Loss: 0.00001339
Iteration 120/1000 | Loss: 0.00001339
Iteration 121/1000 | Loss: 0.00001338
Iteration 122/1000 | Loss: 0.00001338
Iteration 123/1000 | Loss: 0.00001338
Iteration 124/1000 | Loss: 0.00001338
Iteration 125/1000 | Loss: 0.00001338
Iteration 126/1000 | Loss: 0.00001337
Iteration 127/1000 | Loss: 0.00001337
Iteration 128/1000 | Loss: 0.00001337
Iteration 129/1000 | Loss: 0.00001337
Iteration 130/1000 | Loss: 0.00001337
Iteration 131/1000 | Loss: 0.00001336
Iteration 132/1000 | Loss: 0.00001336
Iteration 133/1000 | Loss: 0.00001336
Iteration 134/1000 | Loss: 0.00001336
Iteration 135/1000 | Loss: 0.00001336
Iteration 136/1000 | Loss: 0.00001336
Iteration 137/1000 | Loss: 0.00001336
Iteration 138/1000 | Loss: 0.00001336
Iteration 139/1000 | Loss: 0.00001336
Iteration 140/1000 | Loss: 0.00001335
Iteration 141/1000 | Loss: 0.00001335
Iteration 142/1000 | Loss: 0.00001335
Iteration 143/1000 | Loss: 0.00001335
Iteration 144/1000 | Loss: 0.00001335
Iteration 145/1000 | Loss: 0.00001334
Iteration 146/1000 | Loss: 0.00001334
Iteration 147/1000 | Loss: 0.00001334
Iteration 148/1000 | Loss: 0.00001334
Iteration 149/1000 | Loss: 0.00001334
Iteration 150/1000 | Loss: 0.00001334
Iteration 151/1000 | Loss: 0.00001334
Iteration 152/1000 | Loss: 0.00001334
Iteration 153/1000 | Loss: 0.00001334
Iteration 154/1000 | Loss: 0.00001334
Iteration 155/1000 | Loss: 0.00001334
Iteration 156/1000 | Loss: 0.00001334
Iteration 157/1000 | Loss: 0.00001334
Iteration 158/1000 | Loss: 0.00001334
Iteration 159/1000 | Loss: 0.00001333
Iteration 160/1000 | Loss: 0.00001333
Iteration 161/1000 | Loss: 0.00001333
Iteration 162/1000 | Loss: 0.00001333
Iteration 163/1000 | Loss: 0.00001333
Iteration 164/1000 | Loss: 0.00001332
Iteration 165/1000 | Loss: 0.00001332
Iteration 166/1000 | Loss: 0.00001332
Iteration 167/1000 | Loss: 0.00001332
Iteration 168/1000 | Loss: 0.00001332
Iteration 169/1000 | Loss: 0.00001331
Iteration 170/1000 | Loss: 0.00001331
Iteration 171/1000 | Loss: 0.00001331
Iteration 172/1000 | Loss: 0.00001331
Iteration 173/1000 | Loss: 0.00001331
Iteration 174/1000 | Loss: 0.00001331
Iteration 175/1000 | Loss: 0.00001331
Iteration 176/1000 | Loss: 0.00001331
Iteration 177/1000 | Loss: 0.00001331
Iteration 178/1000 | Loss: 0.00001331
Iteration 179/1000 | Loss: 0.00001330
Iteration 180/1000 | Loss: 0.00001330
Iteration 181/1000 | Loss: 0.00001330
Iteration 182/1000 | Loss: 0.00001330
Iteration 183/1000 | Loss: 0.00001330
Iteration 184/1000 | Loss: 0.00001330
Iteration 185/1000 | Loss: 0.00001330
Iteration 186/1000 | Loss: 0.00001330
Iteration 187/1000 | Loss: 0.00001330
Iteration 188/1000 | Loss: 0.00001330
Iteration 189/1000 | Loss: 0.00001330
Iteration 190/1000 | Loss: 0.00001330
Iteration 191/1000 | Loss: 0.00001330
Iteration 192/1000 | Loss: 0.00001330
Iteration 193/1000 | Loss: 0.00001330
Iteration 194/1000 | Loss: 0.00001329
Iteration 195/1000 | Loss: 0.00001329
Iteration 196/1000 | Loss: 0.00001329
Iteration 197/1000 | Loss: 0.00001329
Iteration 198/1000 | Loss: 0.00001329
Iteration 199/1000 | Loss: 0.00001329
Iteration 200/1000 | Loss: 0.00001329
Iteration 201/1000 | Loss: 0.00001329
Iteration 202/1000 | Loss: 0.00001329
Iteration 203/1000 | Loss: 0.00001329
Iteration 204/1000 | Loss: 0.00001329
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [1.3294896234583575e-05, 1.3294896234583575e-05, 1.3294896234583575e-05, 1.3294896234583575e-05, 1.3294896234583575e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3294896234583575e-05

Optimization complete. Final v2v error: 3.0073587894439697 mm

Highest mean error: 3.7395553588867188 mm for frame 47

Lowest mean error: 2.279313564300537 mm for frame 158

Saving results

Total time: 38.77981352806091
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01065908
Iteration 2/25 | Loss: 0.01065908
Iteration 3/25 | Loss: 0.01065908
Iteration 4/25 | Loss: 0.01065908
Iteration 5/25 | Loss: 0.01065908
Iteration 6/25 | Loss: 0.01065908
Iteration 7/25 | Loss: 0.01065908
Iteration 8/25 | Loss: 0.01065908
Iteration 9/25 | Loss: 0.01065908
Iteration 10/25 | Loss: 0.01065908
Iteration 11/25 | Loss: 0.01065907
Iteration 12/25 | Loss: 0.01065907
Iteration 13/25 | Loss: 0.01065907
Iteration 14/25 | Loss: 0.01065907
Iteration 15/25 | Loss: 0.01065907
Iteration 16/25 | Loss: 0.01065907
Iteration 17/25 | Loss: 0.01065907
Iteration 18/25 | Loss: 0.01065907
Iteration 19/25 | Loss: 0.01065907
Iteration 20/25 | Loss: 0.01065907
Iteration 21/25 | Loss: 0.01065907
Iteration 22/25 | Loss: 0.01065907
Iteration 23/25 | Loss: 0.01065907
Iteration 24/25 | Loss: 0.01065906
Iteration 25/25 | Loss: 0.01065906

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56627834
Iteration 2/25 | Loss: 0.07005552
Iteration 3/25 | Loss: 0.06529776
Iteration 4/25 | Loss: 0.06504170
Iteration 5/25 | Loss: 0.06504168
Iteration 6/25 | Loss: 0.06504168
Iteration 7/25 | Loss: 0.06504168
Iteration 8/25 | Loss: 0.06504168
Iteration 9/25 | Loss: 0.06504168
Iteration 10/25 | Loss: 0.06504168
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.065041683614254, 0.065041683614254, 0.065041683614254, 0.065041683614254, 0.065041683614254]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.065041683614254

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.06504168
Iteration 2/1000 | Loss: 0.00378593
Iteration 3/1000 | Loss: 0.00170406
Iteration 4/1000 | Loss: 0.00059823
Iteration 5/1000 | Loss: 0.00261009
Iteration 6/1000 | Loss: 0.00041297
Iteration 7/1000 | Loss: 0.00132152
Iteration 8/1000 | Loss: 0.00041841
Iteration 9/1000 | Loss: 0.00092042
Iteration 10/1000 | Loss: 0.00026962
Iteration 11/1000 | Loss: 0.00030147
Iteration 12/1000 | Loss: 0.00020777
Iteration 13/1000 | Loss: 0.00054163
Iteration 14/1000 | Loss: 0.00099416
Iteration 15/1000 | Loss: 0.00141952
Iteration 16/1000 | Loss: 0.00106629
Iteration 17/1000 | Loss: 0.00172329
Iteration 18/1000 | Loss: 0.00012882
Iteration 19/1000 | Loss: 0.00031091
Iteration 20/1000 | Loss: 0.00039269
Iteration 21/1000 | Loss: 0.00004958
Iteration 22/1000 | Loss: 0.00004972
Iteration 23/1000 | Loss: 0.00039004
Iteration 24/1000 | Loss: 0.00004725
Iteration 25/1000 | Loss: 0.00054685
Iteration 26/1000 | Loss: 0.00007542
Iteration 27/1000 | Loss: 0.00003650
Iteration 28/1000 | Loss: 0.00012355
Iteration 29/1000 | Loss: 0.00003506
Iteration 30/1000 | Loss: 0.00003299
Iteration 31/1000 | Loss: 0.00030959
Iteration 32/1000 | Loss: 0.00068407
Iteration 33/1000 | Loss: 0.00004675
Iteration 34/1000 | Loss: 0.00003059
Iteration 35/1000 | Loss: 0.00030818
Iteration 36/1000 | Loss: 0.00045488
Iteration 37/1000 | Loss: 0.00005291
Iteration 38/1000 | Loss: 0.00016368
Iteration 39/1000 | Loss: 0.00002807
Iteration 40/1000 | Loss: 0.00002716
Iteration 41/1000 | Loss: 0.00002633
Iteration 42/1000 | Loss: 0.00002562
Iteration 43/1000 | Loss: 0.00002501
Iteration 44/1000 | Loss: 0.00002913
Iteration 45/1000 | Loss: 0.00002425
Iteration 46/1000 | Loss: 0.00002328
Iteration 47/1000 | Loss: 0.00002278
Iteration 48/1000 | Loss: 0.00002229
Iteration 49/1000 | Loss: 0.00002192
Iteration 50/1000 | Loss: 0.00002151
Iteration 51/1000 | Loss: 0.00002124
Iteration 52/1000 | Loss: 0.00002097
Iteration 53/1000 | Loss: 0.00002078
Iteration 54/1000 | Loss: 0.00002064
Iteration 55/1000 | Loss: 0.00002062
Iteration 56/1000 | Loss: 0.00002060
Iteration 57/1000 | Loss: 0.00002058
Iteration 58/1000 | Loss: 0.00002048
Iteration 59/1000 | Loss: 0.00002044
Iteration 60/1000 | Loss: 0.00002027
Iteration 61/1000 | Loss: 0.00002074
Iteration 62/1000 | Loss: 0.00002022
Iteration 63/1000 | Loss: 0.00002015
Iteration 64/1000 | Loss: 0.00002014
Iteration 65/1000 | Loss: 0.00002014
Iteration 66/1000 | Loss: 0.00002014
Iteration 67/1000 | Loss: 0.00002014
Iteration 68/1000 | Loss: 0.00002014
Iteration 69/1000 | Loss: 0.00002014
Iteration 70/1000 | Loss: 0.00002013
Iteration 71/1000 | Loss: 0.00002013
Iteration 72/1000 | Loss: 0.00002055
Iteration 73/1000 | Loss: 0.00002020
Iteration 74/1000 | Loss: 0.00002010
Iteration 75/1000 | Loss: 0.00002047
Iteration 76/1000 | Loss: 0.00002047
Iteration 77/1000 | Loss: 0.00002018
Iteration 78/1000 | Loss: 0.00002009
Iteration 79/1000 | Loss: 0.00002006
Iteration 80/1000 | Loss: 0.00002006
Iteration 81/1000 | Loss: 0.00002006
Iteration 82/1000 | Loss: 0.00002006
Iteration 83/1000 | Loss: 0.00002006
Iteration 84/1000 | Loss: 0.00002005
Iteration 85/1000 | Loss: 0.00002005
Iteration 86/1000 | Loss: 0.00002005
Iteration 87/1000 | Loss: 0.00002005
Iteration 88/1000 | Loss: 0.00002005
Iteration 89/1000 | Loss: 0.00002005
Iteration 90/1000 | Loss: 0.00002005
Iteration 91/1000 | Loss: 0.00002005
Iteration 92/1000 | Loss: 0.00002005
Iteration 93/1000 | Loss: 0.00002005
Iteration 94/1000 | Loss: 0.00002005
Iteration 95/1000 | Loss: 0.00002005
Iteration 96/1000 | Loss: 0.00002005
Iteration 97/1000 | Loss: 0.00002005
Iteration 98/1000 | Loss: 0.00002004
Iteration 99/1000 | Loss: 0.00002004
Iteration 100/1000 | Loss: 0.00002004
Iteration 101/1000 | Loss: 0.00002004
Iteration 102/1000 | Loss: 0.00002004
Iteration 103/1000 | Loss: 0.00002004
Iteration 104/1000 | Loss: 0.00002004
Iteration 105/1000 | Loss: 0.00002003
Iteration 106/1000 | Loss: 0.00002003
Iteration 107/1000 | Loss: 0.00002003
Iteration 108/1000 | Loss: 0.00002003
Iteration 109/1000 | Loss: 0.00002003
Iteration 110/1000 | Loss: 0.00002002
Iteration 111/1000 | Loss: 0.00002002
Iteration 112/1000 | Loss: 0.00002002
Iteration 113/1000 | Loss: 0.00002002
Iteration 114/1000 | Loss: 0.00001999
Iteration 115/1000 | Loss: 0.00001999
Iteration 116/1000 | Loss: 0.00001999
Iteration 117/1000 | Loss: 0.00001999
Iteration 118/1000 | Loss: 0.00001998
Iteration 119/1000 | Loss: 0.00001998
Iteration 120/1000 | Loss: 0.00001998
Iteration 121/1000 | Loss: 0.00001997
Iteration 122/1000 | Loss: 0.00001997
Iteration 123/1000 | Loss: 0.00001997
Iteration 124/1000 | Loss: 0.00001997
Iteration 125/1000 | Loss: 0.00001996
Iteration 126/1000 | Loss: 0.00001996
Iteration 127/1000 | Loss: 0.00001995
Iteration 128/1000 | Loss: 0.00001995
Iteration 129/1000 | Loss: 0.00001995
Iteration 130/1000 | Loss: 0.00001994
Iteration 131/1000 | Loss: 0.00001994
Iteration 132/1000 | Loss: 0.00001994
Iteration 133/1000 | Loss: 0.00001994
Iteration 134/1000 | Loss: 0.00001994
Iteration 135/1000 | Loss: 0.00001993
Iteration 136/1000 | Loss: 0.00001993
Iteration 137/1000 | Loss: 0.00001993
Iteration 138/1000 | Loss: 0.00001993
Iteration 139/1000 | Loss: 0.00001993
Iteration 140/1000 | Loss: 0.00001993
Iteration 141/1000 | Loss: 0.00001993
Iteration 142/1000 | Loss: 0.00001993
Iteration 143/1000 | Loss: 0.00001993
Iteration 144/1000 | Loss: 0.00001993
Iteration 145/1000 | Loss: 0.00001993
Iteration 146/1000 | Loss: 0.00001993
Iteration 147/1000 | Loss: 0.00001993
Iteration 148/1000 | Loss: 0.00001993
Iteration 149/1000 | Loss: 0.00001993
Iteration 150/1000 | Loss: 0.00001993
Iteration 151/1000 | Loss: 0.00001993
Iteration 152/1000 | Loss: 0.00001993
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.992893885471858e-05, 1.992893885471858e-05, 1.992893885471858e-05, 1.992893885471858e-05, 1.992893885471858e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.992893885471858e-05

Optimization complete. Final v2v error: 3.3984363079071045 mm

Highest mean error: 20.779747009277344 mm for frame 137

Lowest mean error: 2.773118734359741 mm for frame 77

Saving results

Total time: 109.4370789527893
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00473677
Iteration 2/25 | Loss: 0.00117612
Iteration 3/25 | Loss: 0.00109905
Iteration 4/25 | Loss: 0.00108497
Iteration 5/25 | Loss: 0.00107983
Iteration 6/25 | Loss: 0.00107845
Iteration 7/25 | Loss: 0.00107845
Iteration 8/25 | Loss: 0.00107845
Iteration 9/25 | Loss: 0.00107845
Iteration 10/25 | Loss: 0.00107845
Iteration 11/25 | Loss: 0.00107845
Iteration 12/25 | Loss: 0.00107845
Iteration 13/25 | Loss: 0.00107845
Iteration 14/25 | Loss: 0.00107845
Iteration 15/25 | Loss: 0.00107845
Iteration 16/25 | Loss: 0.00107845
Iteration 17/25 | Loss: 0.00107845
Iteration 18/25 | Loss: 0.00107845
Iteration 19/25 | Loss: 0.00107845
Iteration 20/25 | Loss: 0.00107845
Iteration 21/25 | Loss: 0.00107845
Iteration 22/25 | Loss: 0.00107845
Iteration 23/25 | Loss: 0.00107845
Iteration 24/25 | Loss: 0.00107845
Iteration 25/25 | Loss: 0.00107845

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.30074120
Iteration 2/25 | Loss: 0.00078570
Iteration 3/25 | Loss: 0.00078570
Iteration 4/25 | Loss: 0.00078570
Iteration 5/25 | Loss: 0.00078570
Iteration 6/25 | Loss: 0.00078570
Iteration 7/25 | Loss: 0.00078570
Iteration 8/25 | Loss: 0.00078570
Iteration 9/25 | Loss: 0.00078570
Iteration 10/25 | Loss: 0.00078570
Iteration 11/25 | Loss: 0.00078570
Iteration 12/25 | Loss: 0.00078570
Iteration 13/25 | Loss: 0.00078570
Iteration 14/25 | Loss: 0.00078570
Iteration 15/25 | Loss: 0.00078570
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0007857005693949759, 0.0007857005693949759, 0.0007857005693949759, 0.0007857005693949759, 0.0007857005693949759]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007857005693949759

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078570
Iteration 2/1000 | Loss: 0.00002556
Iteration 3/1000 | Loss: 0.00001642
Iteration 4/1000 | Loss: 0.00001437
Iteration 5/1000 | Loss: 0.00001353
Iteration 6/1000 | Loss: 0.00001311
Iteration 7/1000 | Loss: 0.00001264
Iteration 8/1000 | Loss: 0.00001236
Iteration 9/1000 | Loss: 0.00001206
Iteration 10/1000 | Loss: 0.00001182
Iteration 11/1000 | Loss: 0.00001177
Iteration 12/1000 | Loss: 0.00001176
Iteration 13/1000 | Loss: 0.00001160
Iteration 14/1000 | Loss: 0.00001155
Iteration 15/1000 | Loss: 0.00001150
Iteration 16/1000 | Loss: 0.00001149
Iteration 17/1000 | Loss: 0.00001149
Iteration 18/1000 | Loss: 0.00001148
Iteration 19/1000 | Loss: 0.00001148
Iteration 20/1000 | Loss: 0.00001147
Iteration 21/1000 | Loss: 0.00001146
Iteration 22/1000 | Loss: 0.00001144
Iteration 23/1000 | Loss: 0.00001144
Iteration 24/1000 | Loss: 0.00001141
Iteration 25/1000 | Loss: 0.00001140
Iteration 26/1000 | Loss: 0.00001136
Iteration 27/1000 | Loss: 0.00001133
Iteration 28/1000 | Loss: 0.00001131
Iteration 29/1000 | Loss: 0.00001131
Iteration 30/1000 | Loss: 0.00001130
Iteration 31/1000 | Loss: 0.00001129
Iteration 32/1000 | Loss: 0.00001125
Iteration 33/1000 | Loss: 0.00001125
Iteration 34/1000 | Loss: 0.00001124
Iteration 35/1000 | Loss: 0.00001124
Iteration 36/1000 | Loss: 0.00001124
Iteration 37/1000 | Loss: 0.00001124
Iteration 38/1000 | Loss: 0.00001124
Iteration 39/1000 | Loss: 0.00001124
Iteration 40/1000 | Loss: 0.00001124
Iteration 41/1000 | Loss: 0.00001123
Iteration 42/1000 | Loss: 0.00001123
Iteration 43/1000 | Loss: 0.00001123
Iteration 44/1000 | Loss: 0.00001121
Iteration 45/1000 | Loss: 0.00001121
Iteration 46/1000 | Loss: 0.00001121
Iteration 47/1000 | Loss: 0.00001121
Iteration 48/1000 | Loss: 0.00001121
Iteration 49/1000 | Loss: 0.00001121
Iteration 50/1000 | Loss: 0.00001121
Iteration 51/1000 | Loss: 0.00001121
Iteration 52/1000 | Loss: 0.00001121
Iteration 53/1000 | Loss: 0.00001120
Iteration 54/1000 | Loss: 0.00001120
Iteration 55/1000 | Loss: 0.00001120
Iteration 56/1000 | Loss: 0.00001119
Iteration 57/1000 | Loss: 0.00001119
Iteration 58/1000 | Loss: 0.00001119
Iteration 59/1000 | Loss: 0.00001119
Iteration 60/1000 | Loss: 0.00001119
Iteration 61/1000 | Loss: 0.00001118
Iteration 62/1000 | Loss: 0.00001118
Iteration 63/1000 | Loss: 0.00001118
Iteration 64/1000 | Loss: 0.00001118
Iteration 65/1000 | Loss: 0.00001117
Iteration 66/1000 | Loss: 0.00001117
Iteration 67/1000 | Loss: 0.00001117
Iteration 68/1000 | Loss: 0.00001117
Iteration 69/1000 | Loss: 0.00001117
Iteration 70/1000 | Loss: 0.00001116
Iteration 71/1000 | Loss: 0.00001116
Iteration 72/1000 | Loss: 0.00001116
Iteration 73/1000 | Loss: 0.00001115
Iteration 74/1000 | Loss: 0.00001115
Iteration 75/1000 | Loss: 0.00001115
Iteration 76/1000 | Loss: 0.00001115
Iteration 77/1000 | Loss: 0.00001114
Iteration 78/1000 | Loss: 0.00001114
Iteration 79/1000 | Loss: 0.00001114
Iteration 80/1000 | Loss: 0.00001113
Iteration 81/1000 | Loss: 0.00001113
Iteration 82/1000 | Loss: 0.00001112
Iteration 83/1000 | Loss: 0.00001111
Iteration 84/1000 | Loss: 0.00001111
Iteration 85/1000 | Loss: 0.00001111
Iteration 86/1000 | Loss: 0.00001111
Iteration 87/1000 | Loss: 0.00001111
Iteration 88/1000 | Loss: 0.00001111
Iteration 89/1000 | Loss: 0.00001111
Iteration 90/1000 | Loss: 0.00001110
Iteration 91/1000 | Loss: 0.00001110
Iteration 92/1000 | Loss: 0.00001110
Iteration 93/1000 | Loss: 0.00001109
Iteration 94/1000 | Loss: 0.00001109
Iteration 95/1000 | Loss: 0.00001109
Iteration 96/1000 | Loss: 0.00001108
Iteration 97/1000 | Loss: 0.00001108
Iteration 98/1000 | Loss: 0.00001108
Iteration 99/1000 | Loss: 0.00001107
Iteration 100/1000 | Loss: 0.00001107
Iteration 101/1000 | Loss: 0.00001107
Iteration 102/1000 | Loss: 0.00001107
Iteration 103/1000 | Loss: 0.00001107
Iteration 104/1000 | Loss: 0.00001107
Iteration 105/1000 | Loss: 0.00001106
Iteration 106/1000 | Loss: 0.00001106
Iteration 107/1000 | Loss: 0.00001106
Iteration 108/1000 | Loss: 0.00001106
Iteration 109/1000 | Loss: 0.00001106
Iteration 110/1000 | Loss: 0.00001106
Iteration 111/1000 | Loss: 0.00001106
Iteration 112/1000 | Loss: 0.00001106
Iteration 113/1000 | Loss: 0.00001106
Iteration 114/1000 | Loss: 0.00001106
Iteration 115/1000 | Loss: 0.00001106
Iteration 116/1000 | Loss: 0.00001106
Iteration 117/1000 | Loss: 0.00001105
Iteration 118/1000 | Loss: 0.00001105
Iteration 119/1000 | Loss: 0.00001105
Iteration 120/1000 | Loss: 0.00001105
Iteration 121/1000 | Loss: 0.00001105
Iteration 122/1000 | Loss: 0.00001105
Iteration 123/1000 | Loss: 0.00001105
Iteration 124/1000 | Loss: 0.00001104
Iteration 125/1000 | Loss: 0.00001104
Iteration 126/1000 | Loss: 0.00001104
Iteration 127/1000 | Loss: 0.00001104
Iteration 128/1000 | Loss: 0.00001104
Iteration 129/1000 | Loss: 0.00001103
Iteration 130/1000 | Loss: 0.00001103
Iteration 131/1000 | Loss: 0.00001103
Iteration 132/1000 | Loss: 0.00001103
Iteration 133/1000 | Loss: 0.00001103
Iteration 134/1000 | Loss: 0.00001103
Iteration 135/1000 | Loss: 0.00001103
Iteration 136/1000 | Loss: 0.00001102
Iteration 137/1000 | Loss: 0.00001102
Iteration 138/1000 | Loss: 0.00001101
Iteration 139/1000 | Loss: 0.00001101
Iteration 140/1000 | Loss: 0.00001101
Iteration 141/1000 | Loss: 0.00001101
Iteration 142/1000 | Loss: 0.00001101
Iteration 143/1000 | Loss: 0.00001101
Iteration 144/1000 | Loss: 0.00001101
Iteration 145/1000 | Loss: 0.00001101
Iteration 146/1000 | Loss: 0.00001101
Iteration 147/1000 | Loss: 0.00001101
Iteration 148/1000 | Loss: 0.00001101
Iteration 149/1000 | Loss: 0.00001100
Iteration 150/1000 | Loss: 0.00001100
Iteration 151/1000 | Loss: 0.00001100
Iteration 152/1000 | Loss: 0.00001099
Iteration 153/1000 | Loss: 0.00001099
Iteration 154/1000 | Loss: 0.00001099
Iteration 155/1000 | Loss: 0.00001099
Iteration 156/1000 | Loss: 0.00001099
Iteration 157/1000 | Loss: 0.00001098
Iteration 158/1000 | Loss: 0.00001098
Iteration 159/1000 | Loss: 0.00001098
Iteration 160/1000 | Loss: 0.00001098
Iteration 161/1000 | Loss: 0.00001097
Iteration 162/1000 | Loss: 0.00001097
Iteration 163/1000 | Loss: 0.00001097
Iteration 164/1000 | Loss: 0.00001097
Iteration 165/1000 | Loss: 0.00001097
Iteration 166/1000 | Loss: 0.00001097
Iteration 167/1000 | Loss: 0.00001097
Iteration 168/1000 | Loss: 0.00001097
Iteration 169/1000 | Loss: 0.00001097
Iteration 170/1000 | Loss: 0.00001097
Iteration 171/1000 | Loss: 0.00001097
Iteration 172/1000 | Loss: 0.00001097
Iteration 173/1000 | Loss: 0.00001097
Iteration 174/1000 | Loss: 0.00001097
Iteration 175/1000 | Loss: 0.00001097
Iteration 176/1000 | Loss: 0.00001097
Iteration 177/1000 | Loss: 0.00001097
Iteration 178/1000 | Loss: 0.00001097
Iteration 179/1000 | Loss: 0.00001097
Iteration 180/1000 | Loss: 0.00001097
Iteration 181/1000 | Loss: 0.00001097
Iteration 182/1000 | Loss: 0.00001097
Iteration 183/1000 | Loss: 0.00001096
Iteration 184/1000 | Loss: 0.00001096
Iteration 185/1000 | Loss: 0.00001096
Iteration 186/1000 | Loss: 0.00001096
Iteration 187/1000 | Loss: 0.00001096
Iteration 188/1000 | Loss: 0.00001096
Iteration 189/1000 | Loss: 0.00001096
Iteration 190/1000 | Loss: 0.00001096
Iteration 191/1000 | Loss: 0.00001096
Iteration 192/1000 | Loss: 0.00001096
Iteration 193/1000 | Loss: 0.00001096
Iteration 194/1000 | Loss: 0.00001096
Iteration 195/1000 | Loss: 0.00001096
Iteration 196/1000 | Loss: 0.00001096
Iteration 197/1000 | Loss: 0.00001096
Iteration 198/1000 | Loss: 0.00001096
Iteration 199/1000 | Loss: 0.00001096
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [1.0963932254526298e-05, 1.0963932254526298e-05, 1.0963932254526298e-05, 1.0963932254526298e-05, 1.0963932254526298e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0963932254526298e-05

Optimization complete. Final v2v error: 2.873316526412964 mm

Highest mean error: 3.192704439163208 mm for frame 108

Lowest mean error: 2.672637701034546 mm for frame 9

Saving results

Total time: 40.11574625968933
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01081616
Iteration 2/25 | Loss: 0.01081616
Iteration 3/25 | Loss: 0.00402971
Iteration 4/25 | Loss: 0.00230877
Iteration 5/25 | Loss: 0.00243234
Iteration 6/25 | Loss: 0.00270190
Iteration 7/25 | Loss: 0.00211855
Iteration 8/25 | Loss: 0.00153844
Iteration 9/25 | Loss: 0.00148406
Iteration 10/25 | Loss: 0.00147498
Iteration 11/25 | Loss: 0.00146046
Iteration 12/25 | Loss: 0.00145512
Iteration 13/25 | Loss: 0.00145190
Iteration 14/25 | Loss: 0.00144822
Iteration 15/25 | Loss: 0.00144714
Iteration 16/25 | Loss: 0.00144669
Iteration 17/25 | Loss: 0.00144507
Iteration 18/25 | Loss: 0.00144452
Iteration 19/25 | Loss: 0.00144434
Iteration 20/25 | Loss: 0.00144432
Iteration 21/25 | Loss: 0.00144432
Iteration 22/25 | Loss: 0.00144431
Iteration 23/25 | Loss: 0.00144431
Iteration 24/25 | Loss: 0.00144431
Iteration 25/25 | Loss: 0.00144431

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.68950397
Iteration 2/25 | Loss: 0.00118381
Iteration 3/25 | Loss: 0.00118381
Iteration 4/25 | Loss: 0.00118381
Iteration 5/25 | Loss: 0.00118380
Iteration 6/25 | Loss: 0.00118380
Iteration 7/25 | Loss: 0.00118380
Iteration 8/25 | Loss: 0.00118380
Iteration 9/25 | Loss: 0.00118380
Iteration 10/25 | Loss: 0.00118380
Iteration 11/25 | Loss: 0.00118380
Iteration 12/25 | Loss: 0.00118380
Iteration 13/25 | Loss: 0.00118380
Iteration 14/25 | Loss: 0.00118380
Iteration 15/25 | Loss: 0.00118380
Iteration 16/25 | Loss: 0.00118380
Iteration 17/25 | Loss: 0.00118380
Iteration 18/25 | Loss: 0.00118380
Iteration 19/25 | Loss: 0.00118380
Iteration 20/25 | Loss: 0.00118380
Iteration 21/25 | Loss: 0.00118380
Iteration 22/25 | Loss: 0.00118380
Iteration 23/25 | Loss: 0.00118380
Iteration 24/25 | Loss: 0.00118380
Iteration 25/25 | Loss: 0.00118380

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118380
Iteration 2/1000 | Loss: 0.00009969
Iteration 3/1000 | Loss: 0.00006349
Iteration 4/1000 | Loss: 0.00005689
Iteration 5/1000 | Loss: 0.00005476
Iteration 6/1000 | Loss: 0.00005284
Iteration 7/1000 | Loss: 0.00005110
Iteration 8/1000 | Loss: 0.00004950
Iteration 9/1000 | Loss: 0.00005401
Iteration 10/1000 | Loss: 0.00005138
Iteration 11/1000 | Loss: 0.00004896
Iteration 12/1000 | Loss: 0.00004724
Iteration 13/1000 | Loss: 0.00004633
Iteration 14/1000 | Loss: 0.00004591
Iteration 15/1000 | Loss: 0.00004543
Iteration 16/1000 | Loss: 0.00004500
Iteration 17/1000 | Loss: 0.00004475
Iteration 18/1000 | Loss: 0.00004467
Iteration 19/1000 | Loss: 0.00004451
Iteration 20/1000 | Loss: 0.00004446
Iteration 21/1000 | Loss: 0.00004428
Iteration 22/1000 | Loss: 0.00004412
Iteration 23/1000 | Loss: 0.00004407
Iteration 24/1000 | Loss: 0.00004406
Iteration 25/1000 | Loss: 0.00004402
Iteration 26/1000 | Loss: 0.00004393
Iteration 27/1000 | Loss: 0.00004393
Iteration 28/1000 | Loss: 0.00004382
Iteration 29/1000 | Loss: 0.00004382
Iteration 30/1000 | Loss: 0.00004381
Iteration 31/1000 | Loss: 0.00004381
Iteration 32/1000 | Loss: 0.00004381
Iteration 33/1000 | Loss: 0.00004381
Iteration 34/1000 | Loss: 0.00004381
Iteration 35/1000 | Loss: 0.00004381
Iteration 36/1000 | Loss: 0.00004381
Iteration 37/1000 | Loss: 0.00004381
Iteration 38/1000 | Loss: 0.00004381
Iteration 39/1000 | Loss: 0.00004381
Iteration 40/1000 | Loss: 0.00004381
Iteration 41/1000 | Loss: 0.00004381
Iteration 42/1000 | Loss: 0.00004381
Iteration 43/1000 | Loss: 0.00004381
Iteration 44/1000 | Loss: 0.00004381
Iteration 45/1000 | Loss: 0.00004381
Iteration 46/1000 | Loss: 0.00004381
Iteration 47/1000 | Loss: 0.00004381
Iteration 48/1000 | Loss: 0.00004381
Iteration 49/1000 | Loss: 0.00004381
Iteration 50/1000 | Loss: 0.00004381
Iteration 51/1000 | Loss: 0.00004381
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 51. Stopping optimization.
Last 5 losses: [4.380835889605805e-05, 4.380835889605805e-05, 4.380835889605805e-05, 4.380835889605805e-05, 4.380835889605805e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.380835889605805e-05

Optimization complete. Final v2v error: 5.290838718414307 mm

Highest mean error: 5.606213092803955 mm for frame 4

Lowest mean error: 4.920165061950684 mm for frame 249

Saving results

Total time: 76.40384817123413
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00390459
Iteration 2/25 | Loss: 0.00117969
Iteration 3/25 | Loss: 0.00107858
Iteration 4/25 | Loss: 0.00106621
Iteration 5/25 | Loss: 0.00106366
Iteration 6/25 | Loss: 0.00106366
Iteration 7/25 | Loss: 0.00106366
Iteration 8/25 | Loss: 0.00106366
Iteration 9/25 | Loss: 0.00106366
Iteration 10/25 | Loss: 0.00106366
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010636551305651665, 0.0010636551305651665, 0.0010636551305651665, 0.0010636551305651665, 0.0010636551305651665]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010636551305651665

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34039092
Iteration 2/25 | Loss: 0.00068909
Iteration 3/25 | Loss: 0.00068909
Iteration 4/25 | Loss: 0.00068909
Iteration 5/25 | Loss: 0.00068909
Iteration 6/25 | Loss: 0.00068909
Iteration 7/25 | Loss: 0.00068908
Iteration 8/25 | Loss: 0.00068908
Iteration 9/25 | Loss: 0.00068908
Iteration 10/25 | Loss: 0.00068908
Iteration 11/25 | Loss: 0.00068908
Iteration 12/25 | Loss: 0.00068908
Iteration 13/25 | Loss: 0.00068908
Iteration 14/25 | Loss: 0.00068908
Iteration 15/25 | Loss: 0.00068908
Iteration 16/25 | Loss: 0.00068908
Iteration 17/25 | Loss: 0.00068908
Iteration 18/25 | Loss: 0.00068908
Iteration 19/25 | Loss: 0.00068908
Iteration 20/25 | Loss: 0.00068908
Iteration 21/25 | Loss: 0.00068908
Iteration 22/25 | Loss: 0.00068908
Iteration 23/25 | Loss: 0.00068908
Iteration 24/25 | Loss: 0.00068908
Iteration 25/25 | Loss: 0.00068908

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068908
Iteration 2/1000 | Loss: 0.00002527
Iteration 3/1000 | Loss: 0.00001932
Iteration 4/1000 | Loss: 0.00001669
Iteration 5/1000 | Loss: 0.00001579
Iteration 6/1000 | Loss: 0.00001487
Iteration 7/1000 | Loss: 0.00001430
Iteration 8/1000 | Loss: 0.00001385
Iteration 9/1000 | Loss: 0.00001352
Iteration 10/1000 | Loss: 0.00001316
Iteration 11/1000 | Loss: 0.00001291
Iteration 12/1000 | Loss: 0.00001291
Iteration 13/1000 | Loss: 0.00001290
Iteration 14/1000 | Loss: 0.00001290
Iteration 15/1000 | Loss: 0.00001280
Iteration 16/1000 | Loss: 0.00001269
Iteration 17/1000 | Loss: 0.00001263
Iteration 18/1000 | Loss: 0.00001254
Iteration 19/1000 | Loss: 0.00001253
Iteration 20/1000 | Loss: 0.00001252
Iteration 21/1000 | Loss: 0.00001252
Iteration 22/1000 | Loss: 0.00001250
Iteration 23/1000 | Loss: 0.00001248
Iteration 24/1000 | Loss: 0.00001248
Iteration 25/1000 | Loss: 0.00001248
Iteration 26/1000 | Loss: 0.00001248
Iteration 27/1000 | Loss: 0.00001248
Iteration 28/1000 | Loss: 0.00001248
Iteration 29/1000 | Loss: 0.00001248
Iteration 30/1000 | Loss: 0.00001248
Iteration 31/1000 | Loss: 0.00001247
Iteration 32/1000 | Loss: 0.00001246
Iteration 33/1000 | Loss: 0.00001243
Iteration 34/1000 | Loss: 0.00001242
Iteration 35/1000 | Loss: 0.00001241
Iteration 36/1000 | Loss: 0.00001239
Iteration 37/1000 | Loss: 0.00001236
Iteration 38/1000 | Loss: 0.00001232
Iteration 39/1000 | Loss: 0.00001229
Iteration 40/1000 | Loss: 0.00001228
Iteration 41/1000 | Loss: 0.00001224
Iteration 42/1000 | Loss: 0.00001223
Iteration 43/1000 | Loss: 0.00001221
Iteration 44/1000 | Loss: 0.00001221
Iteration 45/1000 | Loss: 0.00001219
Iteration 46/1000 | Loss: 0.00001219
Iteration 47/1000 | Loss: 0.00001218
Iteration 48/1000 | Loss: 0.00001218
Iteration 49/1000 | Loss: 0.00001218
Iteration 50/1000 | Loss: 0.00001217
Iteration 51/1000 | Loss: 0.00001217
Iteration 52/1000 | Loss: 0.00001216
Iteration 53/1000 | Loss: 0.00001216
Iteration 54/1000 | Loss: 0.00001216
Iteration 55/1000 | Loss: 0.00001215
Iteration 56/1000 | Loss: 0.00001214
Iteration 57/1000 | Loss: 0.00001214
Iteration 58/1000 | Loss: 0.00001214
Iteration 59/1000 | Loss: 0.00001214
Iteration 60/1000 | Loss: 0.00001214
Iteration 61/1000 | Loss: 0.00001213
Iteration 62/1000 | Loss: 0.00001213
Iteration 63/1000 | Loss: 0.00001211
Iteration 64/1000 | Loss: 0.00001211
Iteration 65/1000 | Loss: 0.00001210
Iteration 66/1000 | Loss: 0.00001210
Iteration 67/1000 | Loss: 0.00001210
Iteration 68/1000 | Loss: 0.00001210
Iteration 69/1000 | Loss: 0.00001208
Iteration 70/1000 | Loss: 0.00001208
Iteration 71/1000 | Loss: 0.00001207
Iteration 72/1000 | Loss: 0.00001207
Iteration 73/1000 | Loss: 0.00001206
Iteration 74/1000 | Loss: 0.00001206
Iteration 75/1000 | Loss: 0.00001206
Iteration 76/1000 | Loss: 0.00001206
Iteration 77/1000 | Loss: 0.00001206
Iteration 78/1000 | Loss: 0.00001206
Iteration 79/1000 | Loss: 0.00001206
Iteration 80/1000 | Loss: 0.00001205
Iteration 81/1000 | Loss: 0.00001205
Iteration 82/1000 | Loss: 0.00001204
Iteration 83/1000 | Loss: 0.00001204
Iteration 84/1000 | Loss: 0.00001204
Iteration 85/1000 | Loss: 0.00001203
Iteration 86/1000 | Loss: 0.00001203
Iteration 87/1000 | Loss: 0.00001203
Iteration 88/1000 | Loss: 0.00001203
Iteration 89/1000 | Loss: 0.00001203
Iteration 90/1000 | Loss: 0.00001203
Iteration 91/1000 | Loss: 0.00001203
Iteration 92/1000 | Loss: 0.00001203
Iteration 93/1000 | Loss: 0.00001203
Iteration 94/1000 | Loss: 0.00001202
Iteration 95/1000 | Loss: 0.00001202
Iteration 96/1000 | Loss: 0.00001201
Iteration 97/1000 | Loss: 0.00001201
Iteration 98/1000 | Loss: 0.00001201
Iteration 99/1000 | Loss: 0.00001201
Iteration 100/1000 | Loss: 0.00001201
Iteration 101/1000 | Loss: 0.00001201
Iteration 102/1000 | Loss: 0.00001200
Iteration 103/1000 | Loss: 0.00001200
Iteration 104/1000 | Loss: 0.00001200
Iteration 105/1000 | Loss: 0.00001200
Iteration 106/1000 | Loss: 0.00001200
Iteration 107/1000 | Loss: 0.00001200
Iteration 108/1000 | Loss: 0.00001200
Iteration 109/1000 | Loss: 0.00001199
Iteration 110/1000 | Loss: 0.00001199
Iteration 111/1000 | Loss: 0.00001199
Iteration 112/1000 | Loss: 0.00001199
Iteration 113/1000 | Loss: 0.00001199
Iteration 114/1000 | Loss: 0.00001198
Iteration 115/1000 | Loss: 0.00001198
Iteration 116/1000 | Loss: 0.00001196
Iteration 117/1000 | Loss: 0.00001196
Iteration 118/1000 | Loss: 0.00001196
Iteration 119/1000 | Loss: 0.00001196
Iteration 120/1000 | Loss: 0.00001196
Iteration 121/1000 | Loss: 0.00001196
Iteration 122/1000 | Loss: 0.00001196
Iteration 123/1000 | Loss: 0.00001196
Iteration 124/1000 | Loss: 0.00001196
Iteration 125/1000 | Loss: 0.00001196
Iteration 126/1000 | Loss: 0.00001196
Iteration 127/1000 | Loss: 0.00001195
Iteration 128/1000 | Loss: 0.00001195
Iteration 129/1000 | Loss: 0.00001195
Iteration 130/1000 | Loss: 0.00001195
Iteration 131/1000 | Loss: 0.00001194
Iteration 132/1000 | Loss: 0.00001194
Iteration 133/1000 | Loss: 0.00001194
Iteration 134/1000 | Loss: 0.00001194
Iteration 135/1000 | Loss: 0.00001194
Iteration 136/1000 | Loss: 0.00001194
Iteration 137/1000 | Loss: 0.00001194
Iteration 138/1000 | Loss: 0.00001193
Iteration 139/1000 | Loss: 0.00001193
Iteration 140/1000 | Loss: 0.00001193
Iteration 141/1000 | Loss: 0.00001193
Iteration 142/1000 | Loss: 0.00001193
Iteration 143/1000 | Loss: 0.00001193
Iteration 144/1000 | Loss: 0.00001193
Iteration 145/1000 | Loss: 0.00001193
Iteration 146/1000 | Loss: 0.00001193
Iteration 147/1000 | Loss: 0.00001193
Iteration 148/1000 | Loss: 0.00001192
Iteration 149/1000 | Loss: 0.00001192
Iteration 150/1000 | Loss: 0.00001192
Iteration 151/1000 | Loss: 0.00001192
Iteration 152/1000 | Loss: 0.00001192
Iteration 153/1000 | Loss: 0.00001192
Iteration 154/1000 | Loss: 0.00001192
Iteration 155/1000 | Loss: 0.00001192
Iteration 156/1000 | Loss: 0.00001192
Iteration 157/1000 | Loss: 0.00001191
Iteration 158/1000 | Loss: 0.00001191
Iteration 159/1000 | Loss: 0.00001191
Iteration 160/1000 | Loss: 0.00001191
Iteration 161/1000 | Loss: 0.00001191
Iteration 162/1000 | Loss: 0.00001191
Iteration 163/1000 | Loss: 0.00001191
Iteration 164/1000 | Loss: 0.00001191
Iteration 165/1000 | Loss: 0.00001191
Iteration 166/1000 | Loss: 0.00001190
Iteration 167/1000 | Loss: 0.00001190
Iteration 168/1000 | Loss: 0.00001190
Iteration 169/1000 | Loss: 0.00001190
Iteration 170/1000 | Loss: 0.00001190
Iteration 171/1000 | Loss: 0.00001190
Iteration 172/1000 | Loss: 0.00001190
Iteration 173/1000 | Loss: 0.00001190
Iteration 174/1000 | Loss: 0.00001190
Iteration 175/1000 | Loss: 0.00001190
Iteration 176/1000 | Loss: 0.00001190
Iteration 177/1000 | Loss: 0.00001190
Iteration 178/1000 | Loss: 0.00001190
Iteration 179/1000 | Loss: 0.00001189
Iteration 180/1000 | Loss: 0.00001189
Iteration 181/1000 | Loss: 0.00001189
Iteration 182/1000 | Loss: 0.00001189
Iteration 183/1000 | Loss: 0.00001189
Iteration 184/1000 | Loss: 0.00001189
Iteration 185/1000 | Loss: 0.00001189
Iteration 186/1000 | Loss: 0.00001188
Iteration 187/1000 | Loss: 0.00001188
Iteration 188/1000 | Loss: 0.00001188
Iteration 189/1000 | Loss: 0.00001188
Iteration 190/1000 | Loss: 0.00001188
Iteration 191/1000 | Loss: 0.00001188
Iteration 192/1000 | Loss: 0.00001188
Iteration 193/1000 | Loss: 0.00001187
Iteration 194/1000 | Loss: 0.00001187
Iteration 195/1000 | Loss: 0.00001187
Iteration 196/1000 | Loss: 0.00001186
Iteration 197/1000 | Loss: 0.00001186
Iteration 198/1000 | Loss: 0.00001186
Iteration 199/1000 | Loss: 0.00001186
Iteration 200/1000 | Loss: 0.00001186
Iteration 201/1000 | Loss: 0.00001186
Iteration 202/1000 | Loss: 0.00001186
Iteration 203/1000 | Loss: 0.00001186
Iteration 204/1000 | Loss: 0.00001186
Iteration 205/1000 | Loss: 0.00001186
Iteration 206/1000 | Loss: 0.00001186
Iteration 207/1000 | Loss: 0.00001186
Iteration 208/1000 | Loss: 0.00001185
Iteration 209/1000 | Loss: 0.00001185
Iteration 210/1000 | Loss: 0.00001185
Iteration 211/1000 | Loss: 0.00001185
Iteration 212/1000 | Loss: 0.00001185
Iteration 213/1000 | Loss: 0.00001185
Iteration 214/1000 | Loss: 0.00001185
Iteration 215/1000 | Loss: 0.00001185
Iteration 216/1000 | Loss: 0.00001185
Iteration 217/1000 | Loss: 0.00001185
Iteration 218/1000 | Loss: 0.00001185
Iteration 219/1000 | Loss: 0.00001185
Iteration 220/1000 | Loss: 0.00001185
Iteration 221/1000 | Loss: 0.00001185
Iteration 222/1000 | Loss: 0.00001185
Iteration 223/1000 | Loss: 0.00001185
Iteration 224/1000 | Loss: 0.00001185
Iteration 225/1000 | Loss: 0.00001185
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [1.18543812277494e-05, 1.18543812277494e-05, 1.18543812277494e-05, 1.18543812277494e-05, 1.18543812277494e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.18543812277494e-05

Optimization complete. Final v2v error: 2.876246690750122 mm

Highest mean error: 3.4348111152648926 mm for frame 122

Lowest mean error: 2.4967148303985596 mm for frame 33

Saving results

Total time: 46.08646774291992
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00557650
Iteration 2/25 | Loss: 0.00113825
Iteration 3/25 | Loss: 0.00107114
Iteration 4/25 | Loss: 0.00106275
Iteration 5/25 | Loss: 0.00105978
Iteration 6/25 | Loss: 0.00105928
Iteration 7/25 | Loss: 0.00105928
Iteration 8/25 | Loss: 0.00105928
Iteration 9/25 | Loss: 0.00105928
Iteration 10/25 | Loss: 0.00105928
Iteration 11/25 | Loss: 0.00105928
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001059279078617692, 0.001059279078617692, 0.001059279078617692, 0.001059279078617692, 0.001059279078617692]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001059279078617692

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.71812963
Iteration 2/25 | Loss: 0.00080181
Iteration 3/25 | Loss: 0.00080180
Iteration 4/25 | Loss: 0.00080180
Iteration 5/25 | Loss: 0.00080180
Iteration 6/25 | Loss: 0.00080180
Iteration 7/25 | Loss: 0.00080180
Iteration 8/25 | Loss: 0.00080180
Iteration 9/25 | Loss: 0.00080180
Iteration 10/25 | Loss: 0.00080180
Iteration 11/25 | Loss: 0.00080180
Iteration 12/25 | Loss: 0.00080180
Iteration 13/25 | Loss: 0.00080180
Iteration 14/25 | Loss: 0.00080180
Iteration 15/25 | Loss: 0.00080180
Iteration 16/25 | Loss: 0.00080180
Iteration 17/25 | Loss: 0.00080180
Iteration 18/25 | Loss: 0.00080180
Iteration 19/25 | Loss: 0.00080180
Iteration 20/25 | Loss: 0.00080180
Iteration 21/25 | Loss: 0.00080180
Iteration 22/25 | Loss: 0.00080180
Iteration 23/25 | Loss: 0.00080180
Iteration 24/25 | Loss: 0.00080180
Iteration 25/25 | Loss: 0.00080180

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080180
Iteration 2/1000 | Loss: 0.00001807
Iteration 3/1000 | Loss: 0.00001205
Iteration 4/1000 | Loss: 0.00001070
Iteration 5/1000 | Loss: 0.00001017
Iteration 6/1000 | Loss: 0.00000973
Iteration 7/1000 | Loss: 0.00000969
Iteration 8/1000 | Loss: 0.00000948
Iteration 9/1000 | Loss: 0.00000928
Iteration 10/1000 | Loss: 0.00000922
Iteration 11/1000 | Loss: 0.00000918
Iteration 12/1000 | Loss: 0.00000912
Iteration 13/1000 | Loss: 0.00000910
Iteration 14/1000 | Loss: 0.00000908
Iteration 15/1000 | Loss: 0.00000907
Iteration 16/1000 | Loss: 0.00000906
Iteration 17/1000 | Loss: 0.00000906
Iteration 18/1000 | Loss: 0.00000906
Iteration 19/1000 | Loss: 0.00000905
Iteration 20/1000 | Loss: 0.00000905
Iteration 21/1000 | Loss: 0.00000905
Iteration 22/1000 | Loss: 0.00000894
Iteration 23/1000 | Loss: 0.00000893
Iteration 24/1000 | Loss: 0.00000892
Iteration 25/1000 | Loss: 0.00000891
Iteration 26/1000 | Loss: 0.00000891
Iteration 27/1000 | Loss: 0.00000890
Iteration 28/1000 | Loss: 0.00000889
Iteration 29/1000 | Loss: 0.00000889
Iteration 30/1000 | Loss: 0.00000887
Iteration 31/1000 | Loss: 0.00000887
Iteration 32/1000 | Loss: 0.00000886
Iteration 33/1000 | Loss: 0.00000885
Iteration 34/1000 | Loss: 0.00000885
Iteration 35/1000 | Loss: 0.00000884
Iteration 36/1000 | Loss: 0.00000884
Iteration 37/1000 | Loss: 0.00000884
Iteration 38/1000 | Loss: 0.00000883
Iteration 39/1000 | Loss: 0.00000883
Iteration 40/1000 | Loss: 0.00000882
Iteration 41/1000 | Loss: 0.00000882
Iteration 42/1000 | Loss: 0.00000881
Iteration 43/1000 | Loss: 0.00000880
Iteration 44/1000 | Loss: 0.00000878
Iteration 45/1000 | Loss: 0.00000878
Iteration 46/1000 | Loss: 0.00000877
Iteration 47/1000 | Loss: 0.00000877
Iteration 48/1000 | Loss: 0.00000876
Iteration 49/1000 | Loss: 0.00000876
Iteration 50/1000 | Loss: 0.00000875
Iteration 51/1000 | Loss: 0.00000875
Iteration 52/1000 | Loss: 0.00000875
Iteration 53/1000 | Loss: 0.00000874
Iteration 54/1000 | Loss: 0.00000874
Iteration 55/1000 | Loss: 0.00000873
Iteration 56/1000 | Loss: 0.00000873
Iteration 57/1000 | Loss: 0.00000873
Iteration 58/1000 | Loss: 0.00000873
Iteration 59/1000 | Loss: 0.00000873
Iteration 60/1000 | Loss: 0.00000872
Iteration 61/1000 | Loss: 0.00000872
Iteration 62/1000 | Loss: 0.00000872
Iteration 63/1000 | Loss: 0.00000872
Iteration 64/1000 | Loss: 0.00000872
Iteration 65/1000 | Loss: 0.00000871
Iteration 66/1000 | Loss: 0.00000871
Iteration 67/1000 | Loss: 0.00000871
Iteration 68/1000 | Loss: 0.00000871
Iteration 69/1000 | Loss: 0.00000871
Iteration 70/1000 | Loss: 0.00000871
Iteration 71/1000 | Loss: 0.00000871
Iteration 72/1000 | Loss: 0.00000870
Iteration 73/1000 | Loss: 0.00000870
Iteration 74/1000 | Loss: 0.00000870
Iteration 75/1000 | Loss: 0.00000869
Iteration 76/1000 | Loss: 0.00000868
Iteration 77/1000 | Loss: 0.00000868
Iteration 78/1000 | Loss: 0.00000867
Iteration 79/1000 | Loss: 0.00000867
Iteration 80/1000 | Loss: 0.00000867
Iteration 81/1000 | Loss: 0.00000866
Iteration 82/1000 | Loss: 0.00000866
Iteration 83/1000 | Loss: 0.00000866
Iteration 84/1000 | Loss: 0.00000865
Iteration 85/1000 | Loss: 0.00000865
Iteration 86/1000 | Loss: 0.00000864
Iteration 87/1000 | Loss: 0.00000864
Iteration 88/1000 | Loss: 0.00000864
Iteration 89/1000 | Loss: 0.00000864
Iteration 90/1000 | Loss: 0.00000863
Iteration 91/1000 | Loss: 0.00000863
Iteration 92/1000 | Loss: 0.00000863
Iteration 93/1000 | Loss: 0.00000863
Iteration 94/1000 | Loss: 0.00000863
Iteration 95/1000 | Loss: 0.00000863
Iteration 96/1000 | Loss: 0.00000863
Iteration 97/1000 | Loss: 0.00000863
Iteration 98/1000 | Loss: 0.00000862
Iteration 99/1000 | Loss: 0.00000862
Iteration 100/1000 | Loss: 0.00000861
Iteration 101/1000 | Loss: 0.00000861
Iteration 102/1000 | Loss: 0.00000861
Iteration 103/1000 | Loss: 0.00000859
Iteration 104/1000 | Loss: 0.00000859
Iteration 105/1000 | Loss: 0.00000859
Iteration 106/1000 | Loss: 0.00000859
Iteration 107/1000 | Loss: 0.00000859
Iteration 108/1000 | Loss: 0.00000859
Iteration 109/1000 | Loss: 0.00000859
Iteration 110/1000 | Loss: 0.00000859
Iteration 111/1000 | Loss: 0.00000859
Iteration 112/1000 | Loss: 0.00000858
Iteration 113/1000 | Loss: 0.00000858
Iteration 114/1000 | Loss: 0.00000858
Iteration 115/1000 | Loss: 0.00000857
Iteration 116/1000 | Loss: 0.00000857
Iteration 117/1000 | Loss: 0.00000857
Iteration 118/1000 | Loss: 0.00000857
Iteration 119/1000 | Loss: 0.00000857
Iteration 120/1000 | Loss: 0.00000857
Iteration 121/1000 | Loss: 0.00000856
Iteration 122/1000 | Loss: 0.00000856
Iteration 123/1000 | Loss: 0.00000856
Iteration 124/1000 | Loss: 0.00000856
Iteration 125/1000 | Loss: 0.00000856
Iteration 126/1000 | Loss: 0.00000856
Iteration 127/1000 | Loss: 0.00000856
Iteration 128/1000 | Loss: 0.00000856
Iteration 129/1000 | Loss: 0.00000856
Iteration 130/1000 | Loss: 0.00000856
Iteration 131/1000 | Loss: 0.00000856
Iteration 132/1000 | Loss: 0.00000856
Iteration 133/1000 | Loss: 0.00000855
Iteration 134/1000 | Loss: 0.00000855
Iteration 135/1000 | Loss: 0.00000855
Iteration 136/1000 | Loss: 0.00000855
Iteration 137/1000 | Loss: 0.00000855
Iteration 138/1000 | Loss: 0.00000855
Iteration 139/1000 | Loss: 0.00000855
Iteration 140/1000 | Loss: 0.00000855
Iteration 141/1000 | Loss: 0.00000855
Iteration 142/1000 | Loss: 0.00000854
Iteration 143/1000 | Loss: 0.00000854
Iteration 144/1000 | Loss: 0.00000854
Iteration 145/1000 | Loss: 0.00000854
Iteration 146/1000 | Loss: 0.00000854
Iteration 147/1000 | Loss: 0.00000854
Iteration 148/1000 | Loss: 0.00000854
Iteration 149/1000 | Loss: 0.00000853
Iteration 150/1000 | Loss: 0.00000853
Iteration 151/1000 | Loss: 0.00000853
Iteration 152/1000 | Loss: 0.00000853
Iteration 153/1000 | Loss: 0.00000853
Iteration 154/1000 | Loss: 0.00000853
Iteration 155/1000 | Loss: 0.00000853
Iteration 156/1000 | Loss: 0.00000853
Iteration 157/1000 | Loss: 0.00000852
Iteration 158/1000 | Loss: 0.00000852
Iteration 159/1000 | Loss: 0.00000852
Iteration 160/1000 | Loss: 0.00000852
Iteration 161/1000 | Loss: 0.00000852
Iteration 162/1000 | Loss: 0.00000852
Iteration 163/1000 | Loss: 0.00000852
Iteration 164/1000 | Loss: 0.00000851
Iteration 165/1000 | Loss: 0.00000851
Iteration 166/1000 | Loss: 0.00000851
Iteration 167/1000 | Loss: 0.00000851
Iteration 168/1000 | Loss: 0.00000850
Iteration 169/1000 | Loss: 0.00000850
Iteration 170/1000 | Loss: 0.00000850
Iteration 171/1000 | Loss: 0.00000850
Iteration 172/1000 | Loss: 0.00000850
Iteration 173/1000 | Loss: 0.00000850
Iteration 174/1000 | Loss: 0.00000850
Iteration 175/1000 | Loss: 0.00000849
Iteration 176/1000 | Loss: 0.00000849
Iteration 177/1000 | Loss: 0.00000849
Iteration 178/1000 | Loss: 0.00000849
Iteration 179/1000 | Loss: 0.00000849
Iteration 180/1000 | Loss: 0.00000849
Iteration 181/1000 | Loss: 0.00000849
Iteration 182/1000 | Loss: 0.00000849
Iteration 183/1000 | Loss: 0.00000849
Iteration 184/1000 | Loss: 0.00000849
Iteration 185/1000 | Loss: 0.00000849
Iteration 186/1000 | Loss: 0.00000849
Iteration 187/1000 | Loss: 0.00000849
Iteration 188/1000 | Loss: 0.00000849
Iteration 189/1000 | Loss: 0.00000848
Iteration 190/1000 | Loss: 0.00000848
Iteration 191/1000 | Loss: 0.00000848
Iteration 192/1000 | Loss: 0.00000848
Iteration 193/1000 | Loss: 0.00000848
Iteration 194/1000 | Loss: 0.00000848
Iteration 195/1000 | Loss: 0.00000848
Iteration 196/1000 | Loss: 0.00000848
Iteration 197/1000 | Loss: 0.00000848
Iteration 198/1000 | Loss: 0.00000848
Iteration 199/1000 | Loss: 0.00000848
Iteration 200/1000 | Loss: 0.00000848
Iteration 201/1000 | Loss: 0.00000848
Iteration 202/1000 | Loss: 0.00000848
Iteration 203/1000 | Loss: 0.00000848
Iteration 204/1000 | Loss: 0.00000848
Iteration 205/1000 | Loss: 0.00000848
Iteration 206/1000 | Loss: 0.00000848
Iteration 207/1000 | Loss: 0.00000848
Iteration 208/1000 | Loss: 0.00000848
Iteration 209/1000 | Loss: 0.00000848
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [8.481571967422497e-06, 8.481571967422497e-06, 8.481571967422497e-06, 8.481571967422497e-06, 8.481571967422497e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.481571967422497e-06

Optimization complete. Final v2v error: 2.520393133163452 mm

Highest mean error: 2.9002766609191895 mm for frame 111

Lowest mean error: 2.3748176097869873 mm for frame 38

Saving results

Total time: 36.874804973602295
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_019/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_019/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00285423
Iteration 2/25 | Loss: 0.00123196
Iteration 3/25 | Loss: 0.00106839
Iteration 4/25 | Loss: 0.00104916
Iteration 5/25 | Loss: 0.00104398
Iteration 6/25 | Loss: 0.00104236
Iteration 7/25 | Loss: 0.00104233
Iteration 8/25 | Loss: 0.00104233
Iteration 9/25 | Loss: 0.00104233
Iteration 10/25 | Loss: 0.00104233
Iteration 11/25 | Loss: 0.00104233
Iteration 12/25 | Loss: 0.00104233
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001042332500219345, 0.001042332500219345, 0.001042332500219345, 0.001042332500219345, 0.001042332500219345]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001042332500219345

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31962836
Iteration 2/25 | Loss: 0.00089270
Iteration 3/25 | Loss: 0.00089269
Iteration 4/25 | Loss: 0.00089269
Iteration 5/25 | Loss: 0.00089269
Iteration 6/25 | Loss: 0.00089269
Iteration 7/25 | Loss: 0.00089269
Iteration 8/25 | Loss: 0.00089269
Iteration 9/25 | Loss: 0.00089269
Iteration 10/25 | Loss: 0.00089269
Iteration 11/25 | Loss: 0.00089269
Iteration 12/25 | Loss: 0.00089269
Iteration 13/25 | Loss: 0.00089269
Iteration 14/25 | Loss: 0.00089269
Iteration 15/25 | Loss: 0.00089269
Iteration 16/25 | Loss: 0.00089269
Iteration 17/25 | Loss: 0.00089269
Iteration 18/25 | Loss: 0.00089269
Iteration 19/25 | Loss: 0.00089269
Iteration 20/25 | Loss: 0.00089269
Iteration 21/25 | Loss: 0.00089269
Iteration 22/25 | Loss: 0.00089269
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008926915470510721, 0.0008926915470510721, 0.0008926915470510721, 0.0008926915470510721, 0.0008926915470510721]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008926915470510721

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089269
Iteration 2/1000 | Loss: 0.00003665
Iteration 3/1000 | Loss: 0.00001905
Iteration 4/1000 | Loss: 0.00001612
Iteration 5/1000 | Loss: 0.00001498
Iteration 6/1000 | Loss: 0.00001406
Iteration 7/1000 | Loss: 0.00001347
Iteration 8/1000 | Loss: 0.00001302
Iteration 9/1000 | Loss: 0.00001258
Iteration 10/1000 | Loss: 0.00001217
Iteration 11/1000 | Loss: 0.00001201
Iteration 12/1000 | Loss: 0.00001192
Iteration 13/1000 | Loss: 0.00001186
Iteration 14/1000 | Loss: 0.00001185
Iteration 15/1000 | Loss: 0.00001184
Iteration 16/1000 | Loss: 0.00001178
Iteration 17/1000 | Loss: 0.00001176
Iteration 18/1000 | Loss: 0.00001176
Iteration 19/1000 | Loss: 0.00001175
Iteration 20/1000 | Loss: 0.00001174
Iteration 21/1000 | Loss: 0.00001174
Iteration 22/1000 | Loss: 0.00001174
Iteration 23/1000 | Loss: 0.00001173
Iteration 24/1000 | Loss: 0.00001171
Iteration 25/1000 | Loss: 0.00001170
Iteration 26/1000 | Loss: 0.00001169
Iteration 27/1000 | Loss: 0.00001168
Iteration 28/1000 | Loss: 0.00001167
Iteration 29/1000 | Loss: 0.00001167
Iteration 30/1000 | Loss: 0.00001166
Iteration 31/1000 | Loss: 0.00001164
Iteration 32/1000 | Loss: 0.00001164
Iteration 33/1000 | Loss: 0.00001163
Iteration 34/1000 | Loss: 0.00001163
Iteration 35/1000 | Loss: 0.00001161
Iteration 36/1000 | Loss: 0.00001161
Iteration 37/1000 | Loss: 0.00001159
Iteration 38/1000 | Loss: 0.00001159
Iteration 39/1000 | Loss: 0.00001159
Iteration 40/1000 | Loss: 0.00001159
Iteration 41/1000 | Loss: 0.00001159
Iteration 42/1000 | Loss: 0.00001159
Iteration 43/1000 | Loss: 0.00001159
Iteration 44/1000 | Loss: 0.00001158
Iteration 45/1000 | Loss: 0.00001158
Iteration 46/1000 | Loss: 0.00001158
Iteration 47/1000 | Loss: 0.00001157
Iteration 48/1000 | Loss: 0.00001157
Iteration 49/1000 | Loss: 0.00001156
Iteration 50/1000 | Loss: 0.00001156
Iteration 51/1000 | Loss: 0.00001156
Iteration 52/1000 | Loss: 0.00001155
Iteration 53/1000 | Loss: 0.00001155
Iteration 54/1000 | Loss: 0.00001155
Iteration 55/1000 | Loss: 0.00001155
Iteration 56/1000 | Loss: 0.00001154
Iteration 57/1000 | Loss: 0.00001154
Iteration 58/1000 | Loss: 0.00001153
Iteration 59/1000 | Loss: 0.00001153
Iteration 60/1000 | Loss: 0.00001152
Iteration 61/1000 | Loss: 0.00001152
Iteration 62/1000 | Loss: 0.00001152
Iteration 63/1000 | Loss: 0.00001152
Iteration 64/1000 | Loss: 0.00001151
Iteration 65/1000 | Loss: 0.00001151
Iteration 66/1000 | Loss: 0.00001151
Iteration 67/1000 | Loss: 0.00001151
Iteration 68/1000 | Loss: 0.00001151
Iteration 69/1000 | Loss: 0.00001151
Iteration 70/1000 | Loss: 0.00001150
Iteration 71/1000 | Loss: 0.00001150
Iteration 72/1000 | Loss: 0.00001150
Iteration 73/1000 | Loss: 0.00001150
Iteration 74/1000 | Loss: 0.00001150
Iteration 75/1000 | Loss: 0.00001149
Iteration 76/1000 | Loss: 0.00001149
Iteration 77/1000 | Loss: 0.00001148
Iteration 78/1000 | Loss: 0.00001148
Iteration 79/1000 | Loss: 0.00001148
Iteration 80/1000 | Loss: 0.00001147
Iteration 81/1000 | Loss: 0.00001147
Iteration 82/1000 | Loss: 0.00001147
Iteration 83/1000 | Loss: 0.00001146
Iteration 84/1000 | Loss: 0.00001146
Iteration 85/1000 | Loss: 0.00001145
Iteration 86/1000 | Loss: 0.00001145
Iteration 87/1000 | Loss: 0.00001145
Iteration 88/1000 | Loss: 0.00001144
Iteration 89/1000 | Loss: 0.00001144
Iteration 90/1000 | Loss: 0.00001144
Iteration 91/1000 | Loss: 0.00001143
Iteration 92/1000 | Loss: 0.00001143
Iteration 93/1000 | Loss: 0.00001143
Iteration 94/1000 | Loss: 0.00001143
Iteration 95/1000 | Loss: 0.00001142
Iteration 96/1000 | Loss: 0.00001142
Iteration 97/1000 | Loss: 0.00001142
Iteration 98/1000 | Loss: 0.00001142
Iteration 99/1000 | Loss: 0.00001142
Iteration 100/1000 | Loss: 0.00001141
Iteration 101/1000 | Loss: 0.00001141
Iteration 102/1000 | Loss: 0.00001141
Iteration 103/1000 | Loss: 0.00001141
Iteration 104/1000 | Loss: 0.00001141
Iteration 105/1000 | Loss: 0.00001141
Iteration 106/1000 | Loss: 0.00001140
Iteration 107/1000 | Loss: 0.00001140
Iteration 108/1000 | Loss: 0.00001140
Iteration 109/1000 | Loss: 0.00001140
Iteration 110/1000 | Loss: 0.00001140
Iteration 111/1000 | Loss: 0.00001140
Iteration 112/1000 | Loss: 0.00001140
Iteration 113/1000 | Loss: 0.00001140
Iteration 114/1000 | Loss: 0.00001140
Iteration 115/1000 | Loss: 0.00001139
Iteration 116/1000 | Loss: 0.00001139
Iteration 117/1000 | Loss: 0.00001139
Iteration 118/1000 | Loss: 0.00001139
Iteration 119/1000 | Loss: 0.00001139
Iteration 120/1000 | Loss: 0.00001139
Iteration 121/1000 | Loss: 0.00001139
Iteration 122/1000 | Loss: 0.00001139
Iteration 123/1000 | Loss: 0.00001139
Iteration 124/1000 | Loss: 0.00001139
Iteration 125/1000 | Loss: 0.00001139
Iteration 126/1000 | Loss: 0.00001139
Iteration 127/1000 | Loss: 0.00001139
Iteration 128/1000 | Loss: 0.00001139
Iteration 129/1000 | Loss: 0.00001139
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [1.1387868653400801e-05, 1.1387868653400801e-05, 1.1387868653400801e-05, 1.1387868653400801e-05, 1.1387868653400801e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1387868653400801e-05

Optimization complete. Final v2v error: 2.8852572441101074 mm

Highest mean error: 3.0649964809417725 mm for frame 18

Lowest mean error: 2.6473727226257324 mm for frame 41

Saving results

Total time: 39.49707078933716
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00816351
Iteration 2/25 | Loss: 0.00161999
Iteration 3/25 | Loss: 0.00143373
Iteration 4/25 | Loss: 0.00141490
Iteration 5/25 | Loss: 0.00141249
Iteration 6/25 | Loss: 0.00141249
Iteration 7/25 | Loss: 0.00141249
Iteration 8/25 | Loss: 0.00141249
Iteration 9/25 | Loss: 0.00141249
Iteration 10/25 | Loss: 0.00141249
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0014124892186373472, 0.0014124892186373472, 0.0014124892186373472, 0.0014124892186373472, 0.0014124892186373472]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014124892186373472

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.01791990
Iteration 2/25 | Loss: 0.00070035
Iteration 3/25 | Loss: 0.00070035
Iteration 4/25 | Loss: 0.00070035
Iteration 5/25 | Loss: 0.00070035
Iteration 6/25 | Loss: 0.00070035
Iteration 7/25 | Loss: 0.00070035
Iteration 8/25 | Loss: 0.00070035
Iteration 9/25 | Loss: 0.00070035
Iteration 10/25 | Loss: 0.00070035
Iteration 11/25 | Loss: 0.00070035
Iteration 12/25 | Loss: 0.00070034
Iteration 13/25 | Loss: 0.00070034
Iteration 14/25 | Loss: 0.00070034
Iteration 15/25 | Loss: 0.00070034
Iteration 16/25 | Loss: 0.00070034
Iteration 17/25 | Loss: 0.00070034
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007003449718467891, 0.0007003449718467891, 0.0007003449718467891, 0.0007003449718467891, 0.0007003449718467891]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007003449718467891

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070034
Iteration 2/1000 | Loss: 0.00004626
Iteration 3/1000 | Loss: 0.00003527
Iteration 4/1000 | Loss: 0.00003154
Iteration 5/1000 | Loss: 0.00003001
Iteration 6/1000 | Loss: 0.00002897
Iteration 7/1000 | Loss: 0.00002826
Iteration 8/1000 | Loss: 0.00002779
Iteration 9/1000 | Loss: 0.00002745
Iteration 10/1000 | Loss: 0.00002710
Iteration 11/1000 | Loss: 0.00002667
Iteration 12/1000 | Loss: 0.00002619
Iteration 13/1000 | Loss: 0.00002593
Iteration 14/1000 | Loss: 0.00002578
Iteration 15/1000 | Loss: 0.00002561
Iteration 16/1000 | Loss: 0.00002560
Iteration 17/1000 | Loss: 0.00002556
Iteration 18/1000 | Loss: 0.00002556
Iteration 19/1000 | Loss: 0.00002551
Iteration 20/1000 | Loss: 0.00002536
Iteration 21/1000 | Loss: 0.00002535
Iteration 22/1000 | Loss: 0.00002526
Iteration 23/1000 | Loss: 0.00002526
Iteration 24/1000 | Loss: 0.00002526
Iteration 25/1000 | Loss: 0.00002526
Iteration 26/1000 | Loss: 0.00002526
Iteration 27/1000 | Loss: 0.00002524
Iteration 28/1000 | Loss: 0.00002524
Iteration 29/1000 | Loss: 0.00002523
Iteration 30/1000 | Loss: 0.00002523
Iteration 31/1000 | Loss: 0.00002513
Iteration 32/1000 | Loss: 0.00002513
Iteration 33/1000 | Loss: 0.00002512
Iteration 34/1000 | Loss: 0.00002512
Iteration 35/1000 | Loss: 0.00002511
Iteration 36/1000 | Loss: 0.00002511
Iteration 37/1000 | Loss: 0.00002510
Iteration 38/1000 | Loss: 0.00002510
Iteration 39/1000 | Loss: 0.00002506
Iteration 40/1000 | Loss: 0.00002506
Iteration 41/1000 | Loss: 0.00002506
Iteration 42/1000 | Loss: 0.00002506
Iteration 43/1000 | Loss: 0.00002506
Iteration 44/1000 | Loss: 0.00002506
Iteration 45/1000 | Loss: 0.00002506
Iteration 46/1000 | Loss: 0.00002506
Iteration 47/1000 | Loss: 0.00002506
Iteration 48/1000 | Loss: 0.00002506
Iteration 49/1000 | Loss: 0.00002506
Iteration 50/1000 | Loss: 0.00002506
Iteration 51/1000 | Loss: 0.00002506
Iteration 52/1000 | Loss: 0.00002506
Iteration 53/1000 | Loss: 0.00002506
Iteration 54/1000 | Loss: 0.00002506
Iteration 55/1000 | Loss: 0.00002506
Iteration 56/1000 | Loss: 0.00002506
Iteration 57/1000 | Loss: 0.00002506
Iteration 58/1000 | Loss: 0.00002506
Iteration 59/1000 | Loss: 0.00002506
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 59. Stopping optimization.
Last 5 losses: [2.5059969630092382e-05, 2.5059969630092382e-05, 2.5059969630092382e-05, 2.5059969630092382e-05, 2.5059969630092382e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5059969630092382e-05

Optimization complete. Final v2v error: 4.193796157836914 mm

Highest mean error: 4.224351406097412 mm for frame 0

Lowest mean error: 4.170884609222412 mm for frame 58

Saving results

Total time: 33.112109422683716
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00937288
Iteration 2/25 | Loss: 0.00228538
Iteration 3/25 | Loss: 0.00174451
Iteration 4/25 | Loss: 0.00164830
Iteration 5/25 | Loss: 0.00165886
Iteration 6/25 | Loss: 0.00155881
Iteration 7/25 | Loss: 0.00148723
Iteration 8/25 | Loss: 0.00145872
Iteration 9/25 | Loss: 0.00146004
Iteration 10/25 | Loss: 0.00144272
Iteration 11/25 | Loss: 0.00143861
Iteration 12/25 | Loss: 0.00143701
Iteration 13/25 | Loss: 0.00143533
Iteration 14/25 | Loss: 0.00143505
Iteration 15/25 | Loss: 0.00143482
Iteration 16/25 | Loss: 0.00144064
Iteration 17/25 | Loss: 0.00144058
Iteration 18/25 | Loss: 0.00143510
Iteration 19/25 | Loss: 0.00143474
Iteration 20/25 | Loss: 0.00144085
Iteration 21/25 | Loss: 0.00143812
Iteration 22/25 | Loss: 0.00144081
Iteration 23/25 | Loss: 0.00143786
Iteration 24/25 | Loss: 0.00143989
Iteration 25/25 | Loss: 0.00144266

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39308977
Iteration 2/25 | Loss: 0.00209101
Iteration 3/25 | Loss: 0.00209101
Iteration 4/25 | Loss: 0.00209101
Iteration 5/25 | Loss: 0.00209101
Iteration 6/25 | Loss: 0.00209101
Iteration 7/25 | Loss: 0.00209101
Iteration 8/25 | Loss: 0.00209101
Iteration 9/25 | Loss: 0.00209101
Iteration 10/25 | Loss: 0.00209101
Iteration 11/25 | Loss: 0.00209101
Iteration 12/25 | Loss: 0.00209101
Iteration 13/25 | Loss: 0.00209101
Iteration 14/25 | Loss: 0.00209101
Iteration 15/25 | Loss: 0.00209101
Iteration 16/25 | Loss: 0.00209101
Iteration 17/25 | Loss: 0.00209101
Iteration 18/25 | Loss: 0.00209101
Iteration 19/25 | Loss: 0.00209101
Iteration 20/25 | Loss: 0.00209101
Iteration 21/25 | Loss: 0.00209101
Iteration 22/25 | Loss: 0.00209101
Iteration 23/25 | Loss: 0.00209101
Iteration 24/25 | Loss: 0.00209101
Iteration 25/25 | Loss: 0.00209101

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00209101
Iteration 2/1000 | Loss: 0.00032314
Iteration 3/1000 | Loss: 0.00011756
Iteration 4/1000 | Loss: 0.00009905
Iteration 5/1000 | Loss: 0.00008957
Iteration 6/1000 | Loss: 0.00035456
Iteration 7/1000 | Loss: 0.00030119
Iteration 8/1000 | Loss: 0.00013383
Iteration 9/1000 | Loss: 0.00029489
Iteration 10/1000 | Loss: 0.00010356
Iteration 11/1000 | Loss: 0.00010925
Iteration 12/1000 | Loss: 0.00010693
Iteration 13/1000 | Loss: 0.00010514
Iteration 14/1000 | Loss: 0.00012551
Iteration 15/1000 | Loss: 0.00008580
Iteration 16/1000 | Loss: 0.00050497
Iteration 17/1000 | Loss: 0.00038975
Iteration 18/1000 | Loss: 0.00009870
Iteration 19/1000 | Loss: 0.00053029
Iteration 20/1000 | Loss: 0.00043082
Iteration 21/1000 | Loss: 0.00026352
Iteration 22/1000 | Loss: 0.00010128
Iteration 23/1000 | Loss: 0.00050459
Iteration 24/1000 | Loss: 0.00009442
Iteration 25/1000 | Loss: 0.00007824
Iteration 26/1000 | Loss: 0.00007600
Iteration 27/1000 | Loss: 0.00030636
Iteration 28/1000 | Loss: 0.00007602
Iteration 29/1000 | Loss: 0.00007306
Iteration 30/1000 | Loss: 0.00025051
Iteration 31/1000 | Loss: 0.00007441
Iteration 32/1000 | Loss: 0.00006997
Iteration 33/1000 | Loss: 0.00045952
Iteration 34/1000 | Loss: 0.00023774
Iteration 35/1000 | Loss: 0.00008910
Iteration 36/1000 | Loss: 0.00007864
Iteration 37/1000 | Loss: 0.00006871
Iteration 38/1000 | Loss: 0.00006685
Iteration 39/1000 | Loss: 0.00046841
Iteration 40/1000 | Loss: 0.00008983
Iteration 41/1000 | Loss: 0.00007576
Iteration 42/1000 | Loss: 0.00007049
Iteration 43/1000 | Loss: 0.00006736
Iteration 44/1000 | Loss: 0.00006529
Iteration 45/1000 | Loss: 0.00006412
Iteration 46/1000 | Loss: 0.00006295
Iteration 47/1000 | Loss: 0.00006172
Iteration 48/1000 | Loss: 0.00006128
Iteration 49/1000 | Loss: 0.00006094
Iteration 50/1000 | Loss: 0.00006060
Iteration 51/1000 | Loss: 0.00006035
Iteration 52/1000 | Loss: 0.00006013
Iteration 53/1000 | Loss: 0.00006005
Iteration 54/1000 | Loss: 0.00006002
Iteration 55/1000 | Loss: 0.00005998
Iteration 56/1000 | Loss: 0.00005993
Iteration 57/1000 | Loss: 0.00005993
Iteration 58/1000 | Loss: 0.00005993
Iteration 59/1000 | Loss: 0.00005993
Iteration 60/1000 | Loss: 0.00005992
Iteration 61/1000 | Loss: 0.00005982
Iteration 62/1000 | Loss: 0.00092738
Iteration 63/1000 | Loss: 0.00217518
Iteration 64/1000 | Loss: 0.00286787
Iteration 65/1000 | Loss: 0.00026349
Iteration 66/1000 | Loss: 0.00010330
Iteration 67/1000 | Loss: 0.00007685
Iteration 68/1000 | Loss: 0.00006480
Iteration 69/1000 | Loss: 0.00005195
Iteration 70/1000 | Loss: 0.00004527
Iteration 71/1000 | Loss: 0.00003848
Iteration 72/1000 | Loss: 0.00003525
Iteration 73/1000 | Loss: 0.00003342
Iteration 74/1000 | Loss: 0.00003201
Iteration 75/1000 | Loss: 0.00003091
Iteration 76/1000 | Loss: 0.00003002
Iteration 77/1000 | Loss: 0.00002926
Iteration 78/1000 | Loss: 0.00002863
Iteration 79/1000 | Loss: 0.00002820
Iteration 80/1000 | Loss: 0.00002786
Iteration 81/1000 | Loss: 0.00002757
Iteration 82/1000 | Loss: 0.00002738
Iteration 83/1000 | Loss: 0.00002715
Iteration 84/1000 | Loss: 0.00002706
Iteration 85/1000 | Loss: 0.00002694
Iteration 86/1000 | Loss: 0.00002693
Iteration 87/1000 | Loss: 0.00002692
Iteration 88/1000 | Loss: 0.00002691
Iteration 89/1000 | Loss: 0.00002688
Iteration 90/1000 | Loss: 0.00002685
Iteration 91/1000 | Loss: 0.00002684
Iteration 92/1000 | Loss: 0.00002684
Iteration 93/1000 | Loss: 0.00002680
Iteration 94/1000 | Loss: 0.00002679
Iteration 95/1000 | Loss: 0.00002678
Iteration 96/1000 | Loss: 0.00002675
Iteration 97/1000 | Loss: 0.00002674
Iteration 98/1000 | Loss: 0.00002673
Iteration 99/1000 | Loss: 0.00002672
Iteration 100/1000 | Loss: 0.00002672
Iteration 101/1000 | Loss: 0.00002672
Iteration 102/1000 | Loss: 0.00002671
Iteration 103/1000 | Loss: 0.00002671
Iteration 104/1000 | Loss: 0.00002670
Iteration 105/1000 | Loss: 0.00002670
Iteration 106/1000 | Loss: 0.00002669
Iteration 107/1000 | Loss: 0.00002669
Iteration 108/1000 | Loss: 0.00002668
Iteration 109/1000 | Loss: 0.00002668
Iteration 110/1000 | Loss: 0.00002668
Iteration 111/1000 | Loss: 0.00002667
Iteration 112/1000 | Loss: 0.00002667
Iteration 113/1000 | Loss: 0.00002667
Iteration 114/1000 | Loss: 0.00002667
Iteration 115/1000 | Loss: 0.00002666
Iteration 116/1000 | Loss: 0.00002666
Iteration 117/1000 | Loss: 0.00002666
Iteration 118/1000 | Loss: 0.00002665
Iteration 119/1000 | Loss: 0.00002665
Iteration 120/1000 | Loss: 0.00002665
Iteration 121/1000 | Loss: 0.00002664
Iteration 122/1000 | Loss: 0.00002664
Iteration 123/1000 | Loss: 0.00002663
Iteration 124/1000 | Loss: 0.00002663
Iteration 125/1000 | Loss: 0.00002662
Iteration 126/1000 | Loss: 0.00002662
Iteration 127/1000 | Loss: 0.00002662
Iteration 128/1000 | Loss: 0.00002662
Iteration 129/1000 | Loss: 0.00002662
Iteration 130/1000 | Loss: 0.00002661
Iteration 131/1000 | Loss: 0.00002661
Iteration 132/1000 | Loss: 0.00002661
Iteration 133/1000 | Loss: 0.00002661
Iteration 134/1000 | Loss: 0.00002661
Iteration 135/1000 | Loss: 0.00002661
Iteration 136/1000 | Loss: 0.00002661
Iteration 137/1000 | Loss: 0.00002660
Iteration 138/1000 | Loss: 0.00002660
Iteration 139/1000 | Loss: 0.00002660
Iteration 140/1000 | Loss: 0.00002660
Iteration 141/1000 | Loss: 0.00002660
Iteration 142/1000 | Loss: 0.00002660
Iteration 143/1000 | Loss: 0.00002659
Iteration 144/1000 | Loss: 0.00002659
Iteration 145/1000 | Loss: 0.00002659
Iteration 146/1000 | Loss: 0.00002659
Iteration 147/1000 | Loss: 0.00002659
Iteration 148/1000 | Loss: 0.00002659
Iteration 149/1000 | Loss: 0.00002659
Iteration 150/1000 | Loss: 0.00002658
Iteration 151/1000 | Loss: 0.00002658
Iteration 152/1000 | Loss: 0.00002658
Iteration 153/1000 | Loss: 0.00002658
Iteration 154/1000 | Loss: 0.00002658
Iteration 155/1000 | Loss: 0.00002658
Iteration 156/1000 | Loss: 0.00002657
Iteration 157/1000 | Loss: 0.00002657
Iteration 158/1000 | Loss: 0.00002657
Iteration 159/1000 | Loss: 0.00002657
Iteration 160/1000 | Loss: 0.00002657
Iteration 161/1000 | Loss: 0.00002657
Iteration 162/1000 | Loss: 0.00002657
Iteration 163/1000 | Loss: 0.00002657
Iteration 164/1000 | Loss: 0.00002657
Iteration 165/1000 | Loss: 0.00002657
Iteration 166/1000 | Loss: 0.00002657
Iteration 167/1000 | Loss: 0.00002657
Iteration 168/1000 | Loss: 0.00002657
Iteration 169/1000 | Loss: 0.00002657
Iteration 170/1000 | Loss: 0.00002657
Iteration 171/1000 | Loss: 0.00002657
Iteration 172/1000 | Loss: 0.00002657
Iteration 173/1000 | Loss: 0.00002657
Iteration 174/1000 | Loss: 0.00002657
Iteration 175/1000 | Loss: 0.00002657
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [2.6571415219223127e-05, 2.6571415219223127e-05, 2.6571415219223127e-05, 2.6571415219223127e-05, 2.6571415219223127e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6571415219223127e-05

Optimization complete. Final v2v error: 4.167909622192383 mm

Highest mean error: 6.020351886749268 mm for frame 47

Lowest mean error: 2.9002139568328857 mm for frame 0

Saving results

Total time: 156.28449130058289
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00751944
Iteration 2/25 | Loss: 0.00150022
Iteration 3/25 | Loss: 0.00137876
Iteration 4/25 | Loss: 0.00136102
Iteration 5/25 | Loss: 0.00135501
Iteration 6/25 | Loss: 0.00135362
Iteration 7/25 | Loss: 0.00135356
Iteration 8/25 | Loss: 0.00135356
Iteration 9/25 | Loss: 0.00135356
Iteration 10/25 | Loss: 0.00135356
Iteration 11/25 | Loss: 0.00135356
Iteration 12/25 | Loss: 0.00135356
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013535615289583802, 0.0013535615289583802, 0.0013535615289583802, 0.0013535615289583802, 0.0013535615289583802]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013535615289583802

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.67454529
Iteration 2/25 | Loss: 0.00093399
Iteration 3/25 | Loss: 0.00093396
Iteration 4/25 | Loss: 0.00093396
Iteration 5/25 | Loss: 0.00093396
Iteration 6/25 | Loss: 0.00093396
Iteration 7/25 | Loss: 0.00093396
Iteration 8/25 | Loss: 0.00093396
Iteration 9/25 | Loss: 0.00093396
Iteration 10/25 | Loss: 0.00093396
Iteration 11/25 | Loss: 0.00093396
Iteration 12/25 | Loss: 0.00093396
Iteration 13/25 | Loss: 0.00093396
Iteration 14/25 | Loss: 0.00093396
Iteration 15/25 | Loss: 0.00093396
Iteration 16/25 | Loss: 0.00093396
Iteration 17/25 | Loss: 0.00093396
Iteration 18/25 | Loss: 0.00093396
Iteration 19/25 | Loss: 0.00093396
Iteration 20/25 | Loss: 0.00093396
Iteration 21/25 | Loss: 0.00093396
Iteration 22/25 | Loss: 0.00093396
Iteration 23/25 | Loss: 0.00093396
Iteration 24/25 | Loss: 0.00093396
Iteration 25/25 | Loss: 0.00093396

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093396
Iteration 2/1000 | Loss: 0.00004767
Iteration 3/1000 | Loss: 0.00002705
Iteration 4/1000 | Loss: 0.00002343
Iteration 5/1000 | Loss: 0.00002196
Iteration 6/1000 | Loss: 0.00002127
Iteration 7/1000 | Loss: 0.00002081
Iteration 8/1000 | Loss: 0.00002033
Iteration 9/1000 | Loss: 0.00001999
Iteration 10/1000 | Loss: 0.00001977
Iteration 11/1000 | Loss: 0.00001953
Iteration 12/1000 | Loss: 0.00001948
Iteration 13/1000 | Loss: 0.00001932
Iteration 14/1000 | Loss: 0.00001931
Iteration 15/1000 | Loss: 0.00001918
Iteration 16/1000 | Loss: 0.00001914
Iteration 17/1000 | Loss: 0.00001913
Iteration 18/1000 | Loss: 0.00001912
Iteration 19/1000 | Loss: 0.00001910
Iteration 20/1000 | Loss: 0.00001909
Iteration 21/1000 | Loss: 0.00001908
Iteration 22/1000 | Loss: 0.00001906
Iteration 23/1000 | Loss: 0.00001899
Iteration 24/1000 | Loss: 0.00001899
Iteration 25/1000 | Loss: 0.00001898
Iteration 26/1000 | Loss: 0.00001898
Iteration 27/1000 | Loss: 0.00001897
Iteration 28/1000 | Loss: 0.00001896
Iteration 29/1000 | Loss: 0.00001896
Iteration 30/1000 | Loss: 0.00001895
Iteration 31/1000 | Loss: 0.00001894
Iteration 32/1000 | Loss: 0.00001894
Iteration 33/1000 | Loss: 0.00001894
Iteration 34/1000 | Loss: 0.00001894
Iteration 35/1000 | Loss: 0.00001893
Iteration 36/1000 | Loss: 0.00001893
Iteration 37/1000 | Loss: 0.00001893
Iteration 38/1000 | Loss: 0.00001893
Iteration 39/1000 | Loss: 0.00001893
Iteration 40/1000 | Loss: 0.00001893
Iteration 41/1000 | Loss: 0.00001893
Iteration 42/1000 | Loss: 0.00001893
Iteration 43/1000 | Loss: 0.00001893
Iteration 44/1000 | Loss: 0.00001892
Iteration 45/1000 | Loss: 0.00001892
Iteration 46/1000 | Loss: 0.00001892
Iteration 47/1000 | Loss: 0.00001892
Iteration 48/1000 | Loss: 0.00001891
Iteration 49/1000 | Loss: 0.00001890
Iteration 50/1000 | Loss: 0.00001890
Iteration 51/1000 | Loss: 0.00001890
Iteration 52/1000 | Loss: 0.00001890
Iteration 53/1000 | Loss: 0.00001890
Iteration 54/1000 | Loss: 0.00001890
Iteration 55/1000 | Loss: 0.00001890
Iteration 56/1000 | Loss: 0.00001889
Iteration 57/1000 | Loss: 0.00001889
Iteration 58/1000 | Loss: 0.00001889
Iteration 59/1000 | Loss: 0.00001889
Iteration 60/1000 | Loss: 0.00001889
Iteration 61/1000 | Loss: 0.00001888
Iteration 62/1000 | Loss: 0.00001888
Iteration 63/1000 | Loss: 0.00001888
Iteration 64/1000 | Loss: 0.00001888
Iteration 65/1000 | Loss: 0.00001888
Iteration 66/1000 | Loss: 0.00001887
Iteration 67/1000 | Loss: 0.00001887
Iteration 68/1000 | Loss: 0.00001887
Iteration 69/1000 | Loss: 0.00001886
Iteration 70/1000 | Loss: 0.00001886
Iteration 71/1000 | Loss: 0.00001886
Iteration 72/1000 | Loss: 0.00001886
Iteration 73/1000 | Loss: 0.00001885
Iteration 74/1000 | Loss: 0.00001885
Iteration 75/1000 | Loss: 0.00001885
Iteration 76/1000 | Loss: 0.00001884
Iteration 77/1000 | Loss: 0.00001884
Iteration 78/1000 | Loss: 0.00001884
Iteration 79/1000 | Loss: 0.00001884
Iteration 80/1000 | Loss: 0.00001884
Iteration 81/1000 | Loss: 0.00001883
Iteration 82/1000 | Loss: 0.00001883
Iteration 83/1000 | Loss: 0.00001883
Iteration 84/1000 | Loss: 0.00001883
Iteration 85/1000 | Loss: 0.00001883
Iteration 86/1000 | Loss: 0.00001882
Iteration 87/1000 | Loss: 0.00001882
Iteration 88/1000 | Loss: 0.00001882
Iteration 89/1000 | Loss: 0.00001881
Iteration 90/1000 | Loss: 0.00001881
Iteration 91/1000 | Loss: 0.00001881
Iteration 92/1000 | Loss: 0.00001881
Iteration 93/1000 | Loss: 0.00001881
Iteration 94/1000 | Loss: 0.00001881
Iteration 95/1000 | Loss: 0.00001881
Iteration 96/1000 | Loss: 0.00001881
Iteration 97/1000 | Loss: 0.00001880
Iteration 98/1000 | Loss: 0.00001880
Iteration 99/1000 | Loss: 0.00001880
Iteration 100/1000 | Loss: 0.00001880
Iteration 101/1000 | Loss: 0.00001880
Iteration 102/1000 | Loss: 0.00001880
Iteration 103/1000 | Loss: 0.00001880
Iteration 104/1000 | Loss: 0.00001880
Iteration 105/1000 | Loss: 0.00001880
Iteration 106/1000 | Loss: 0.00001879
Iteration 107/1000 | Loss: 0.00001879
Iteration 108/1000 | Loss: 0.00001878
Iteration 109/1000 | Loss: 0.00001878
Iteration 110/1000 | Loss: 0.00001878
Iteration 111/1000 | Loss: 0.00001878
Iteration 112/1000 | Loss: 0.00001878
Iteration 113/1000 | Loss: 0.00001878
Iteration 114/1000 | Loss: 0.00001878
Iteration 115/1000 | Loss: 0.00001878
Iteration 116/1000 | Loss: 0.00001877
Iteration 117/1000 | Loss: 0.00001877
Iteration 118/1000 | Loss: 0.00001877
Iteration 119/1000 | Loss: 0.00001877
Iteration 120/1000 | Loss: 0.00001877
Iteration 121/1000 | Loss: 0.00001877
Iteration 122/1000 | Loss: 0.00001877
Iteration 123/1000 | Loss: 0.00001877
Iteration 124/1000 | Loss: 0.00001877
Iteration 125/1000 | Loss: 0.00001877
Iteration 126/1000 | Loss: 0.00001877
Iteration 127/1000 | Loss: 0.00001877
Iteration 128/1000 | Loss: 0.00001876
Iteration 129/1000 | Loss: 0.00001876
Iteration 130/1000 | Loss: 0.00001875
Iteration 131/1000 | Loss: 0.00001875
Iteration 132/1000 | Loss: 0.00001875
Iteration 133/1000 | Loss: 0.00001875
Iteration 134/1000 | Loss: 0.00001875
Iteration 135/1000 | Loss: 0.00001875
Iteration 136/1000 | Loss: 0.00001875
Iteration 137/1000 | Loss: 0.00001875
Iteration 138/1000 | Loss: 0.00001875
Iteration 139/1000 | Loss: 0.00001875
Iteration 140/1000 | Loss: 0.00001875
Iteration 141/1000 | Loss: 0.00001875
Iteration 142/1000 | Loss: 0.00001875
Iteration 143/1000 | Loss: 0.00001875
Iteration 144/1000 | Loss: 0.00001875
Iteration 145/1000 | Loss: 0.00001875
Iteration 146/1000 | Loss: 0.00001875
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.8748631191556342e-05, 1.8748631191556342e-05, 1.8748631191556342e-05, 1.8748631191556342e-05, 1.8748631191556342e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8748631191556342e-05

Optimization complete. Final v2v error: 3.6842005252838135 mm

Highest mean error: 4.085790157318115 mm for frame 48

Lowest mean error: 3.21561598777771 mm for frame 6

Saving results

Total time: 38.473713397979736
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00787289
Iteration 2/25 | Loss: 0.00137750
Iteration 3/25 | Loss: 0.00127801
Iteration 4/25 | Loss: 0.00126707
Iteration 5/25 | Loss: 0.00126482
Iteration 6/25 | Loss: 0.00126482
Iteration 7/25 | Loss: 0.00126482
Iteration 8/25 | Loss: 0.00126482
Iteration 9/25 | Loss: 0.00126482
Iteration 10/25 | Loss: 0.00126482
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012648237170651555, 0.0012648237170651555, 0.0012648237170651555, 0.0012648237170651555, 0.0012648237170651555]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012648237170651555

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39731503
Iteration 2/25 | Loss: 0.00081878
Iteration 3/25 | Loss: 0.00081877
Iteration 4/25 | Loss: 0.00081877
Iteration 5/25 | Loss: 0.00081877
Iteration 6/25 | Loss: 0.00081877
Iteration 7/25 | Loss: 0.00081877
Iteration 8/25 | Loss: 0.00081877
Iteration 9/25 | Loss: 0.00081877
Iteration 10/25 | Loss: 0.00081877
Iteration 11/25 | Loss: 0.00081877
Iteration 12/25 | Loss: 0.00081877
Iteration 13/25 | Loss: 0.00081877
Iteration 14/25 | Loss: 0.00081877
Iteration 15/25 | Loss: 0.00081877
Iteration 16/25 | Loss: 0.00081877
Iteration 17/25 | Loss: 0.00081877
Iteration 18/25 | Loss: 0.00081877
Iteration 19/25 | Loss: 0.00081877
Iteration 20/25 | Loss: 0.00081877
Iteration 21/25 | Loss: 0.00081877
Iteration 22/25 | Loss: 0.00081877
Iteration 23/25 | Loss: 0.00081877
Iteration 24/25 | Loss: 0.00081877
Iteration 25/25 | Loss: 0.00081877

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081877
Iteration 2/1000 | Loss: 0.00002566
Iteration 3/1000 | Loss: 0.00001765
Iteration 4/1000 | Loss: 0.00001593
Iteration 5/1000 | Loss: 0.00001478
Iteration 6/1000 | Loss: 0.00001407
Iteration 7/1000 | Loss: 0.00001358
Iteration 8/1000 | Loss: 0.00001326
Iteration 9/1000 | Loss: 0.00001300
Iteration 10/1000 | Loss: 0.00001268
Iteration 11/1000 | Loss: 0.00001256
Iteration 12/1000 | Loss: 0.00001251
Iteration 13/1000 | Loss: 0.00001244
Iteration 14/1000 | Loss: 0.00001239
Iteration 15/1000 | Loss: 0.00001234
Iteration 16/1000 | Loss: 0.00001230
Iteration 17/1000 | Loss: 0.00001227
Iteration 18/1000 | Loss: 0.00001227
Iteration 19/1000 | Loss: 0.00001226
Iteration 20/1000 | Loss: 0.00001223
Iteration 21/1000 | Loss: 0.00001223
Iteration 22/1000 | Loss: 0.00001219
Iteration 23/1000 | Loss: 0.00001218
Iteration 24/1000 | Loss: 0.00001218
Iteration 25/1000 | Loss: 0.00001217
Iteration 26/1000 | Loss: 0.00001217
Iteration 27/1000 | Loss: 0.00001216
Iteration 28/1000 | Loss: 0.00001216
Iteration 29/1000 | Loss: 0.00001215
Iteration 30/1000 | Loss: 0.00001214
Iteration 31/1000 | Loss: 0.00001214
Iteration 32/1000 | Loss: 0.00001213
Iteration 33/1000 | Loss: 0.00001213
Iteration 34/1000 | Loss: 0.00001213
Iteration 35/1000 | Loss: 0.00001213
Iteration 36/1000 | Loss: 0.00001212
Iteration 37/1000 | Loss: 0.00001212
Iteration 38/1000 | Loss: 0.00001212
Iteration 39/1000 | Loss: 0.00001211
Iteration 40/1000 | Loss: 0.00001211
Iteration 41/1000 | Loss: 0.00001210
Iteration 42/1000 | Loss: 0.00001210
Iteration 43/1000 | Loss: 0.00001209
Iteration 44/1000 | Loss: 0.00001209
Iteration 45/1000 | Loss: 0.00001209
Iteration 46/1000 | Loss: 0.00001209
Iteration 47/1000 | Loss: 0.00001208
Iteration 48/1000 | Loss: 0.00001207
Iteration 49/1000 | Loss: 0.00001207
Iteration 50/1000 | Loss: 0.00001206
Iteration 51/1000 | Loss: 0.00001206
Iteration 52/1000 | Loss: 0.00001205
Iteration 53/1000 | Loss: 0.00001205
Iteration 54/1000 | Loss: 0.00001205
Iteration 55/1000 | Loss: 0.00001204
Iteration 56/1000 | Loss: 0.00001204
Iteration 57/1000 | Loss: 0.00001204
Iteration 58/1000 | Loss: 0.00001204
Iteration 59/1000 | Loss: 0.00001204
Iteration 60/1000 | Loss: 0.00001204
Iteration 61/1000 | Loss: 0.00001204
Iteration 62/1000 | Loss: 0.00001204
Iteration 63/1000 | Loss: 0.00001204
Iteration 64/1000 | Loss: 0.00001203
Iteration 65/1000 | Loss: 0.00001199
Iteration 66/1000 | Loss: 0.00001199
Iteration 67/1000 | Loss: 0.00001198
Iteration 68/1000 | Loss: 0.00001198
Iteration 69/1000 | Loss: 0.00001198
Iteration 70/1000 | Loss: 0.00001198
Iteration 71/1000 | Loss: 0.00001197
Iteration 72/1000 | Loss: 0.00001196
Iteration 73/1000 | Loss: 0.00001195
Iteration 74/1000 | Loss: 0.00001195
Iteration 75/1000 | Loss: 0.00001195
Iteration 76/1000 | Loss: 0.00001195
Iteration 77/1000 | Loss: 0.00001195
Iteration 78/1000 | Loss: 0.00001195
Iteration 79/1000 | Loss: 0.00001195
Iteration 80/1000 | Loss: 0.00001195
Iteration 81/1000 | Loss: 0.00001195
Iteration 82/1000 | Loss: 0.00001195
Iteration 83/1000 | Loss: 0.00001195
Iteration 84/1000 | Loss: 0.00001195
Iteration 85/1000 | Loss: 0.00001194
Iteration 86/1000 | Loss: 0.00001194
Iteration 87/1000 | Loss: 0.00001194
Iteration 88/1000 | Loss: 0.00001193
Iteration 89/1000 | Loss: 0.00001193
Iteration 90/1000 | Loss: 0.00001192
Iteration 91/1000 | Loss: 0.00001192
Iteration 92/1000 | Loss: 0.00001192
Iteration 93/1000 | Loss: 0.00001191
Iteration 94/1000 | Loss: 0.00001191
Iteration 95/1000 | Loss: 0.00001191
Iteration 96/1000 | Loss: 0.00001190
Iteration 97/1000 | Loss: 0.00001190
Iteration 98/1000 | Loss: 0.00001190
Iteration 99/1000 | Loss: 0.00001189
Iteration 100/1000 | Loss: 0.00001189
Iteration 101/1000 | Loss: 0.00001188
Iteration 102/1000 | Loss: 0.00001188
Iteration 103/1000 | Loss: 0.00001188
Iteration 104/1000 | Loss: 0.00001188
Iteration 105/1000 | Loss: 0.00001188
Iteration 106/1000 | Loss: 0.00001187
Iteration 107/1000 | Loss: 0.00001187
Iteration 108/1000 | Loss: 0.00001187
Iteration 109/1000 | Loss: 0.00001187
Iteration 110/1000 | Loss: 0.00001187
Iteration 111/1000 | Loss: 0.00001187
Iteration 112/1000 | Loss: 0.00001187
Iteration 113/1000 | Loss: 0.00001186
Iteration 114/1000 | Loss: 0.00001186
Iteration 115/1000 | Loss: 0.00001186
Iteration 116/1000 | Loss: 0.00001186
Iteration 117/1000 | Loss: 0.00001185
Iteration 118/1000 | Loss: 0.00001185
Iteration 119/1000 | Loss: 0.00001185
Iteration 120/1000 | Loss: 0.00001185
Iteration 121/1000 | Loss: 0.00001185
Iteration 122/1000 | Loss: 0.00001185
Iteration 123/1000 | Loss: 0.00001184
Iteration 124/1000 | Loss: 0.00001184
Iteration 125/1000 | Loss: 0.00001184
Iteration 126/1000 | Loss: 0.00001184
Iteration 127/1000 | Loss: 0.00001183
Iteration 128/1000 | Loss: 0.00001183
Iteration 129/1000 | Loss: 0.00001183
Iteration 130/1000 | Loss: 0.00001183
Iteration 131/1000 | Loss: 0.00001183
Iteration 132/1000 | Loss: 0.00001183
Iteration 133/1000 | Loss: 0.00001183
Iteration 134/1000 | Loss: 0.00001183
Iteration 135/1000 | Loss: 0.00001182
Iteration 136/1000 | Loss: 0.00001182
Iteration 137/1000 | Loss: 0.00001182
Iteration 138/1000 | Loss: 0.00001182
Iteration 139/1000 | Loss: 0.00001182
Iteration 140/1000 | Loss: 0.00001182
Iteration 141/1000 | Loss: 0.00001181
Iteration 142/1000 | Loss: 0.00001181
Iteration 143/1000 | Loss: 0.00001181
Iteration 144/1000 | Loss: 0.00001181
Iteration 145/1000 | Loss: 0.00001181
Iteration 146/1000 | Loss: 0.00001181
Iteration 147/1000 | Loss: 0.00001180
Iteration 148/1000 | Loss: 0.00001180
Iteration 149/1000 | Loss: 0.00001180
Iteration 150/1000 | Loss: 0.00001180
Iteration 151/1000 | Loss: 0.00001179
Iteration 152/1000 | Loss: 0.00001179
Iteration 153/1000 | Loss: 0.00001178
Iteration 154/1000 | Loss: 0.00001178
Iteration 155/1000 | Loss: 0.00001178
Iteration 156/1000 | Loss: 0.00001178
Iteration 157/1000 | Loss: 0.00001178
Iteration 158/1000 | Loss: 0.00001178
Iteration 159/1000 | Loss: 0.00001178
Iteration 160/1000 | Loss: 0.00001177
Iteration 161/1000 | Loss: 0.00001177
Iteration 162/1000 | Loss: 0.00001177
Iteration 163/1000 | Loss: 0.00001176
Iteration 164/1000 | Loss: 0.00001176
Iteration 165/1000 | Loss: 0.00001176
Iteration 166/1000 | Loss: 0.00001176
Iteration 167/1000 | Loss: 0.00001176
Iteration 168/1000 | Loss: 0.00001176
Iteration 169/1000 | Loss: 0.00001175
Iteration 170/1000 | Loss: 0.00001175
Iteration 171/1000 | Loss: 0.00001175
Iteration 172/1000 | Loss: 0.00001175
Iteration 173/1000 | Loss: 0.00001175
Iteration 174/1000 | Loss: 0.00001175
Iteration 175/1000 | Loss: 0.00001175
Iteration 176/1000 | Loss: 0.00001175
Iteration 177/1000 | Loss: 0.00001175
Iteration 178/1000 | Loss: 0.00001175
Iteration 179/1000 | Loss: 0.00001175
Iteration 180/1000 | Loss: 0.00001175
Iteration 181/1000 | Loss: 0.00001175
Iteration 182/1000 | Loss: 0.00001175
Iteration 183/1000 | Loss: 0.00001175
Iteration 184/1000 | Loss: 0.00001174
Iteration 185/1000 | Loss: 0.00001174
Iteration 186/1000 | Loss: 0.00001174
Iteration 187/1000 | Loss: 0.00001174
Iteration 188/1000 | Loss: 0.00001174
Iteration 189/1000 | Loss: 0.00001174
Iteration 190/1000 | Loss: 0.00001174
Iteration 191/1000 | Loss: 0.00001174
Iteration 192/1000 | Loss: 0.00001174
Iteration 193/1000 | Loss: 0.00001174
Iteration 194/1000 | Loss: 0.00001174
Iteration 195/1000 | Loss: 0.00001174
Iteration 196/1000 | Loss: 0.00001174
Iteration 197/1000 | Loss: 0.00001173
Iteration 198/1000 | Loss: 0.00001173
Iteration 199/1000 | Loss: 0.00001173
Iteration 200/1000 | Loss: 0.00001173
Iteration 201/1000 | Loss: 0.00001173
Iteration 202/1000 | Loss: 0.00001173
Iteration 203/1000 | Loss: 0.00001173
Iteration 204/1000 | Loss: 0.00001172
Iteration 205/1000 | Loss: 0.00001172
Iteration 206/1000 | Loss: 0.00001172
Iteration 207/1000 | Loss: 0.00001172
Iteration 208/1000 | Loss: 0.00001172
Iteration 209/1000 | Loss: 0.00001172
Iteration 210/1000 | Loss: 0.00001171
Iteration 211/1000 | Loss: 0.00001171
Iteration 212/1000 | Loss: 0.00001171
Iteration 213/1000 | Loss: 0.00001171
Iteration 214/1000 | Loss: 0.00001171
Iteration 215/1000 | Loss: 0.00001171
Iteration 216/1000 | Loss: 0.00001171
Iteration 217/1000 | Loss: 0.00001171
Iteration 218/1000 | Loss: 0.00001171
Iteration 219/1000 | Loss: 0.00001171
Iteration 220/1000 | Loss: 0.00001171
Iteration 221/1000 | Loss: 0.00001171
Iteration 222/1000 | Loss: 0.00001170
Iteration 223/1000 | Loss: 0.00001170
Iteration 224/1000 | Loss: 0.00001170
Iteration 225/1000 | Loss: 0.00001170
Iteration 226/1000 | Loss: 0.00001170
Iteration 227/1000 | Loss: 0.00001170
Iteration 228/1000 | Loss: 0.00001170
Iteration 229/1000 | Loss: 0.00001170
Iteration 230/1000 | Loss: 0.00001170
Iteration 231/1000 | Loss: 0.00001170
Iteration 232/1000 | Loss: 0.00001170
Iteration 233/1000 | Loss: 0.00001170
Iteration 234/1000 | Loss: 0.00001170
Iteration 235/1000 | Loss: 0.00001170
Iteration 236/1000 | Loss: 0.00001170
Iteration 237/1000 | Loss: 0.00001170
Iteration 238/1000 | Loss: 0.00001170
Iteration 239/1000 | Loss: 0.00001170
Iteration 240/1000 | Loss: 0.00001170
Iteration 241/1000 | Loss: 0.00001170
Iteration 242/1000 | Loss: 0.00001170
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 242. Stopping optimization.
Last 5 losses: [1.1700632057909388e-05, 1.1700632057909388e-05, 1.1700632057909388e-05, 1.1700632057909388e-05, 1.1700632057909388e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1700632057909388e-05

Optimization complete. Final v2v error: 2.9185900688171387 mm

Highest mean error: 3.100275993347168 mm for frame 41

Lowest mean error: 2.774054765701294 mm for frame 173

Saving results

Total time: 42.96864700317383
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00431595
Iteration 2/25 | Loss: 0.00137013
Iteration 3/25 | Loss: 0.00129896
Iteration 4/25 | Loss: 0.00128370
Iteration 5/25 | Loss: 0.00127917
Iteration 6/25 | Loss: 0.00127848
Iteration 7/25 | Loss: 0.00127848
Iteration 8/25 | Loss: 0.00127848
Iteration 9/25 | Loss: 0.00127848
Iteration 10/25 | Loss: 0.00127848
Iteration 11/25 | Loss: 0.00127848
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001278475858271122, 0.001278475858271122, 0.001278475858271122, 0.001278475858271122, 0.001278475858271122]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001278475858271122

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39604557
Iteration 2/25 | Loss: 0.00081458
Iteration 3/25 | Loss: 0.00081458
Iteration 4/25 | Loss: 0.00081458
Iteration 5/25 | Loss: 0.00081458
Iteration 6/25 | Loss: 0.00081458
Iteration 7/25 | Loss: 0.00081458
Iteration 8/25 | Loss: 0.00081458
Iteration 9/25 | Loss: 0.00081458
Iteration 10/25 | Loss: 0.00081458
Iteration 11/25 | Loss: 0.00081458
Iteration 12/25 | Loss: 0.00081458
Iteration 13/25 | Loss: 0.00081458
Iteration 14/25 | Loss: 0.00081458
Iteration 15/25 | Loss: 0.00081458
Iteration 16/25 | Loss: 0.00081458
Iteration 17/25 | Loss: 0.00081458
Iteration 18/25 | Loss: 0.00081458
Iteration 19/25 | Loss: 0.00081458
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008145800675265491, 0.0008145800675265491, 0.0008145800675265491, 0.0008145800675265491, 0.0008145800675265491]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008145800675265491

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081458
Iteration 2/1000 | Loss: 0.00003224
Iteration 3/1000 | Loss: 0.00002195
Iteration 4/1000 | Loss: 0.00001930
Iteration 5/1000 | Loss: 0.00001807
Iteration 6/1000 | Loss: 0.00001750
Iteration 7/1000 | Loss: 0.00001707
Iteration 8/1000 | Loss: 0.00001662
Iteration 9/1000 | Loss: 0.00001635
Iteration 10/1000 | Loss: 0.00001601
Iteration 11/1000 | Loss: 0.00001578
Iteration 12/1000 | Loss: 0.00001578
Iteration 13/1000 | Loss: 0.00001577
Iteration 14/1000 | Loss: 0.00001575
Iteration 15/1000 | Loss: 0.00001567
Iteration 16/1000 | Loss: 0.00001563
Iteration 17/1000 | Loss: 0.00001562
Iteration 18/1000 | Loss: 0.00001560
Iteration 19/1000 | Loss: 0.00001559
Iteration 20/1000 | Loss: 0.00001555
Iteration 21/1000 | Loss: 0.00001554
Iteration 22/1000 | Loss: 0.00001552
Iteration 23/1000 | Loss: 0.00001551
Iteration 24/1000 | Loss: 0.00001550
Iteration 25/1000 | Loss: 0.00001549
Iteration 26/1000 | Loss: 0.00001547
Iteration 27/1000 | Loss: 0.00001547
Iteration 28/1000 | Loss: 0.00001544
Iteration 29/1000 | Loss: 0.00001543
Iteration 30/1000 | Loss: 0.00001543
Iteration 31/1000 | Loss: 0.00001538
Iteration 32/1000 | Loss: 0.00001536
Iteration 33/1000 | Loss: 0.00001532
Iteration 34/1000 | Loss: 0.00001530
Iteration 35/1000 | Loss: 0.00001529
Iteration 36/1000 | Loss: 0.00001529
Iteration 37/1000 | Loss: 0.00001528
Iteration 38/1000 | Loss: 0.00001528
Iteration 39/1000 | Loss: 0.00001528
Iteration 40/1000 | Loss: 0.00001526
Iteration 41/1000 | Loss: 0.00001526
Iteration 42/1000 | Loss: 0.00001525
Iteration 43/1000 | Loss: 0.00001525
Iteration 44/1000 | Loss: 0.00001524
Iteration 45/1000 | Loss: 0.00001524
Iteration 46/1000 | Loss: 0.00001523
Iteration 47/1000 | Loss: 0.00001523
Iteration 48/1000 | Loss: 0.00001523
Iteration 49/1000 | Loss: 0.00001522
Iteration 50/1000 | Loss: 0.00001522
Iteration 51/1000 | Loss: 0.00001522
Iteration 52/1000 | Loss: 0.00001521
Iteration 53/1000 | Loss: 0.00001521
Iteration 54/1000 | Loss: 0.00001520
Iteration 55/1000 | Loss: 0.00001518
Iteration 56/1000 | Loss: 0.00001517
Iteration 57/1000 | Loss: 0.00001516
Iteration 58/1000 | Loss: 0.00001515
Iteration 59/1000 | Loss: 0.00001515
Iteration 60/1000 | Loss: 0.00001515
Iteration 61/1000 | Loss: 0.00001514
Iteration 62/1000 | Loss: 0.00001514
Iteration 63/1000 | Loss: 0.00001513
Iteration 64/1000 | Loss: 0.00001513
Iteration 65/1000 | Loss: 0.00001513
Iteration 66/1000 | Loss: 0.00001509
Iteration 67/1000 | Loss: 0.00001507
Iteration 68/1000 | Loss: 0.00001507
Iteration 69/1000 | Loss: 0.00001506
Iteration 70/1000 | Loss: 0.00001506
Iteration 71/1000 | Loss: 0.00001506
Iteration 72/1000 | Loss: 0.00001505
Iteration 73/1000 | Loss: 0.00001505
Iteration 74/1000 | Loss: 0.00001504
Iteration 75/1000 | Loss: 0.00001504
Iteration 76/1000 | Loss: 0.00001504
Iteration 77/1000 | Loss: 0.00001502
Iteration 78/1000 | Loss: 0.00001502
Iteration 79/1000 | Loss: 0.00001501
Iteration 80/1000 | Loss: 0.00001501
Iteration 81/1000 | Loss: 0.00001501
Iteration 82/1000 | Loss: 0.00001500
Iteration 83/1000 | Loss: 0.00001499
Iteration 84/1000 | Loss: 0.00001499
Iteration 85/1000 | Loss: 0.00001498
Iteration 86/1000 | Loss: 0.00001498
Iteration 87/1000 | Loss: 0.00001498
Iteration 88/1000 | Loss: 0.00001498
Iteration 89/1000 | Loss: 0.00001497
Iteration 90/1000 | Loss: 0.00001497
Iteration 91/1000 | Loss: 0.00001497
Iteration 92/1000 | Loss: 0.00001497
Iteration 93/1000 | Loss: 0.00001497
Iteration 94/1000 | Loss: 0.00001497
Iteration 95/1000 | Loss: 0.00001497
Iteration 96/1000 | Loss: 0.00001497
Iteration 97/1000 | Loss: 0.00001497
Iteration 98/1000 | Loss: 0.00001496
Iteration 99/1000 | Loss: 0.00001495
Iteration 100/1000 | Loss: 0.00001495
Iteration 101/1000 | Loss: 0.00001493
Iteration 102/1000 | Loss: 0.00001493
Iteration 103/1000 | Loss: 0.00001493
Iteration 104/1000 | Loss: 0.00001493
Iteration 105/1000 | Loss: 0.00001493
Iteration 106/1000 | Loss: 0.00001492
Iteration 107/1000 | Loss: 0.00001492
Iteration 108/1000 | Loss: 0.00001492
Iteration 109/1000 | Loss: 0.00001492
Iteration 110/1000 | Loss: 0.00001492
Iteration 111/1000 | Loss: 0.00001491
Iteration 112/1000 | Loss: 0.00001491
Iteration 113/1000 | Loss: 0.00001490
Iteration 114/1000 | Loss: 0.00001490
Iteration 115/1000 | Loss: 0.00001490
Iteration 116/1000 | Loss: 0.00001490
Iteration 117/1000 | Loss: 0.00001490
Iteration 118/1000 | Loss: 0.00001490
Iteration 119/1000 | Loss: 0.00001490
Iteration 120/1000 | Loss: 0.00001490
Iteration 121/1000 | Loss: 0.00001490
Iteration 122/1000 | Loss: 0.00001490
Iteration 123/1000 | Loss: 0.00001490
Iteration 124/1000 | Loss: 0.00001490
Iteration 125/1000 | Loss: 0.00001490
Iteration 126/1000 | Loss: 0.00001490
Iteration 127/1000 | Loss: 0.00001490
Iteration 128/1000 | Loss: 0.00001490
Iteration 129/1000 | Loss: 0.00001490
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [1.4897032087901607e-05, 1.4897032087901607e-05, 1.4897032087901607e-05, 1.4897032087901607e-05, 1.4897032087901607e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4897032087901607e-05

Optimization complete. Final v2v error: 3.311974287033081 mm

Highest mean error: 3.559866428375244 mm for frame 104

Lowest mean error: 3.1557517051696777 mm for frame 157

Saving results

Total time: 37.78398513793945
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00957557
Iteration 2/25 | Loss: 0.00957557
Iteration 3/25 | Loss: 0.00280765
Iteration 4/25 | Loss: 0.00222556
Iteration 5/25 | Loss: 0.00213209
Iteration 6/25 | Loss: 0.00208309
Iteration 7/25 | Loss: 0.00206517
Iteration 8/25 | Loss: 0.00209064
Iteration 9/25 | Loss: 0.00202571
Iteration 10/25 | Loss: 0.00197128
Iteration 11/25 | Loss: 0.00193511
Iteration 12/25 | Loss: 0.00189704
Iteration 13/25 | Loss: 0.00188300
Iteration 14/25 | Loss: 0.00187219
Iteration 15/25 | Loss: 0.00186333
Iteration 16/25 | Loss: 0.00186872
Iteration 17/25 | Loss: 0.00185723
Iteration 18/25 | Loss: 0.00186015
Iteration 19/25 | Loss: 0.00185435
Iteration 20/25 | Loss: 0.00185368
Iteration 21/25 | Loss: 0.00185353
Iteration 22/25 | Loss: 0.00185344
Iteration 23/25 | Loss: 0.00185342
Iteration 24/25 | Loss: 0.00185342
Iteration 25/25 | Loss: 0.00185342

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39077079
Iteration 2/25 | Loss: 0.00443190
Iteration 3/25 | Loss: 0.00443190
Iteration 4/25 | Loss: 0.00443190
Iteration 5/25 | Loss: 0.00443190
Iteration 6/25 | Loss: 0.00443190
Iteration 7/25 | Loss: 0.00443190
Iteration 8/25 | Loss: 0.00443190
Iteration 9/25 | Loss: 0.00443190
Iteration 10/25 | Loss: 0.00443190
Iteration 11/25 | Loss: 0.00443190
Iteration 12/25 | Loss: 0.00443190
Iteration 13/25 | Loss: 0.00443190
Iteration 14/25 | Loss: 0.00443190
Iteration 15/25 | Loss: 0.00443190
Iteration 16/25 | Loss: 0.00443190
Iteration 17/25 | Loss: 0.00443190
Iteration 18/25 | Loss: 0.00443190
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.004431896843016148, 0.004431896843016148, 0.004431896843016148, 0.004431896843016148, 0.004431896843016148]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004431896843016148

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00443190
Iteration 2/1000 | Loss: 0.00140557
Iteration 3/1000 | Loss: 0.00527429
Iteration 4/1000 | Loss: 0.00047871
Iteration 5/1000 | Loss: 0.00047712
Iteration 6/1000 | Loss: 0.00085152
Iteration 7/1000 | Loss: 0.00100572
Iteration 8/1000 | Loss: 0.00146234
Iteration 9/1000 | Loss: 0.00825535
Iteration 10/1000 | Loss: 0.00063373
Iteration 11/1000 | Loss: 0.00032714
Iteration 12/1000 | Loss: 0.00058096
Iteration 13/1000 | Loss: 0.00035651
Iteration 14/1000 | Loss: 0.00125551
Iteration 15/1000 | Loss: 0.00109613
Iteration 16/1000 | Loss: 0.00200296
Iteration 17/1000 | Loss: 0.00036583
Iteration 18/1000 | Loss: 0.00027783
Iteration 19/1000 | Loss: 0.00058877
Iteration 20/1000 | Loss: 0.00218301
Iteration 21/1000 | Loss: 0.00055431
Iteration 22/1000 | Loss: 0.00029866
Iteration 23/1000 | Loss: 0.00030831
Iteration 24/1000 | Loss: 0.00040287
Iteration 25/1000 | Loss: 0.00096981
Iteration 26/1000 | Loss: 0.01630277
Iteration 27/1000 | Loss: 0.00965783
Iteration 28/1000 | Loss: 0.00130733
Iteration 29/1000 | Loss: 0.00063931
Iteration 30/1000 | Loss: 0.00034576
Iteration 31/1000 | Loss: 0.00048588
Iteration 32/1000 | Loss: 0.00024779
Iteration 33/1000 | Loss: 0.00023688
Iteration 34/1000 | Loss: 0.00027245
Iteration 35/1000 | Loss: 0.00025523
Iteration 36/1000 | Loss: 0.00019010
Iteration 37/1000 | Loss: 0.00009020
Iteration 38/1000 | Loss: 0.00026327
Iteration 39/1000 | Loss: 0.00006258
Iteration 40/1000 | Loss: 0.00019323
Iteration 41/1000 | Loss: 0.00006104
Iteration 42/1000 | Loss: 0.00004369
Iteration 43/1000 | Loss: 0.00008381
Iteration 44/1000 | Loss: 0.00003764
Iteration 45/1000 | Loss: 0.00003403
Iteration 46/1000 | Loss: 0.00003056
Iteration 47/1000 | Loss: 0.00006291
Iteration 48/1000 | Loss: 0.00002673
Iteration 49/1000 | Loss: 0.00018193
Iteration 50/1000 | Loss: 0.00092190
Iteration 51/1000 | Loss: 0.00007312
Iteration 52/1000 | Loss: 0.00009119
Iteration 53/1000 | Loss: 0.00011047
Iteration 54/1000 | Loss: 0.00002754
Iteration 55/1000 | Loss: 0.00002534
Iteration 56/1000 | Loss: 0.00002454
Iteration 57/1000 | Loss: 0.00002372
Iteration 58/1000 | Loss: 0.00026401
Iteration 59/1000 | Loss: 0.00002306
Iteration 60/1000 | Loss: 0.00002252
Iteration 61/1000 | Loss: 0.00002191
Iteration 62/1000 | Loss: 0.00017101
Iteration 63/1000 | Loss: 0.00006319
Iteration 64/1000 | Loss: 0.00005988
Iteration 65/1000 | Loss: 0.00002138
Iteration 66/1000 | Loss: 0.00002112
Iteration 67/1000 | Loss: 0.00012847
Iteration 68/1000 | Loss: 0.00008279
Iteration 69/1000 | Loss: 0.00002141
Iteration 70/1000 | Loss: 0.00002656
Iteration 71/1000 | Loss: 0.00002656
Iteration 72/1000 | Loss: 0.00020678
Iteration 73/1000 | Loss: 0.00002275
Iteration 74/1000 | Loss: 0.00002111
Iteration 75/1000 | Loss: 0.00002411
Iteration 76/1000 | Loss: 0.00002098
Iteration 77/1000 | Loss: 0.00002097
Iteration 78/1000 | Loss: 0.00002097
Iteration 79/1000 | Loss: 0.00002089
Iteration 80/1000 | Loss: 0.00002082
Iteration 81/1000 | Loss: 0.00002077
Iteration 82/1000 | Loss: 0.00002075
Iteration 83/1000 | Loss: 0.00002075
Iteration 84/1000 | Loss: 0.00002074
Iteration 85/1000 | Loss: 0.00002074
Iteration 86/1000 | Loss: 0.00002074
Iteration 87/1000 | Loss: 0.00002073
Iteration 88/1000 | Loss: 0.00002073
Iteration 89/1000 | Loss: 0.00002072
Iteration 90/1000 | Loss: 0.00002071
Iteration 91/1000 | Loss: 0.00002071
Iteration 92/1000 | Loss: 0.00002070
Iteration 93/1000 | Loss: 0.00002069
Iteration 94/1000 | Loss: 0.00002068
Iteration 95/1000 | Loss: 0.00002067
Iteration 96/1000 | Loss: 0.00019476
Iteration 97/1000 | Loss: 0.00002099
Iteration 98/1000 | Loss: 0.00008760
Iteration 99/1000 | Loss: 0.00002377
Iteration 100/1000 | Loss: 0.00002583
Iteration 101/1000 | Loss: 0.00002076
Iteration 102/1000 | Loss: 0.00002068
Iteration 103/1000 | Loss: 0.00002068
Iteration 104/1000 | Loss: 0.00016235
Iteration 105/1000 | Loss: 0.00007127
Iteration 106/1000 | Loss: 0.00002201
Iteration 107/1000 | Loss: 0.00003425
Iteration 108/1000 | Loss: 0.00002072
Iteration 109/1000 | Loss: 0.00002067
Iteration 110/1000 | Loss: 0.00002067
Iteration 111/1000 | Loss: 0.00002066
Iteration 112/1000 | Loss: 0.00002066
Iteration 113/1000 | Loss: 0.00002063
Iteration 114/1000 | Loss: 0.00002061
Iteration 115/1000 | Loss: 0.00002060
Iteration 116/1000 | Loss: 0.00002060
Iteration 117/1000 | Loss: 0.00002060
Iteration 118/1000 | Loss: 0.00002055
Iteration 119/1000 | Loss: 0.00002055
Iteration 120/1000 | Loss: 0.00002053
Iteration 121/1000 | Loss: 0.00002053
Iteration 122/1000 | Loss: 0.00002052
Iteration 123/1000 | Loss: 0.00002052
Iteration 124/1000 | Loss: 0.00002052
Iteration 125/1000 | Loss: 0.00002052
Iteration 126/1000 | Loss: 0.00002051
Iteration 127/1000 | Loss: 0.00002051
Iteration 128/1000 | Loss: 0.00002051
Iteration 129/1000 | Loss: 0.00002051
Iteration 130/1000 | Loss: 0.00002051
Iteration 131/1000 | Loss: 0.00002051
Iteration 132/1000 | Loss: 0.00002050
Iteration 133/1000 | Loss: 0.00002050
Iteration 134/1000 | Loss: 0.00002050
Iteration 135/1000 | Loss: 0.00002050
Iteration 136/1000 | Loss: 0.00002049
Iteration 137/1000 | Loss: 0.00002049
Iteration 138/1000 | Loss: 0.00002049
Iteration 139/1000 | Loss: 0.00002049
Iteration 140/1000 | Loss: 0.00002049
Iteration 141/1000 | Loss: 0.00002049
Iteration 142/1000 | Loss: 0.00002049
Iteration 143/1000 | Loss: 0.00002049
Iteration 144/1000 | Loss: 0.00002049
Iteration 145/1000 | Loss: 0.00002049
Iteration 146/1000 | Loss: 0.00002048
Iteration 147/1000 | Loss: 0.00002048
Iteration 148/1000 | Loss: 0.00002048
Iteration 149/1000 | Loss: 0.00002048
Iteration 150/1000 | Loss: 0.00002048
Iteration 151/1000 | Loss: 0.00002048
Iteration 152/1000 | Loss: 0.00002048
Iteration 153/1000 | Loss: 0.00002048
Iteration 154/1000 | Loss: 0.00002048
Iteration 155/1000 | Loss: 0.00002048
Iteration 156/1000 | Loss: 0.00002048
Iteration 157/1000 | Loss: 0.00002048
Iteration 158/1000 | Loss: 0.00002048
Iteration 159/1000 | Loss: 0.00002048
Iteration 160/1000 | Loss: 0.00002048
Iteration 161/1000 | Loss: 0.00002048
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [2.0475234123296104e-05, 2.0475234123296104e-05, 2.0475234123296104e-05, 2.0475234123296104e-05, 2.0475234123296104e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0475234123296104e-05

Optimization complete. Final v2v error: 3.8177595138549805 mm

Highest mean error: 7.469569683074951 mm for frame 145

Lowest mean error: 3.6340813636779785 mm for frame 112

Saving results

Total time: 164.87439560890198
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00431645
Iteration 2/25 | Loss: 0.00137283
Iteration 3/25 | Loss: 0.00130217
Iteration 4/25 | Loss: 0.00128491
Iteration 5/25 | Loss: 0.00127932
Iteration 6/25 | Loss: 0.00127837
Iteration 7/25 | Loss: 0.00127837
Iteration 8/25 | Loss: 0.00127837
Iteration 9/25 | Loss: 0.00127837
Iteration 10/25 | Loss: 0.00127837
Iteration 11/25 | Loss: 0.00127837
Iteration 12/25 | Loss: 0.00127837
Iteration 13/25 | Loss: 0.00127837
Iteration 14/25 | Loss: 0.00127837
Iteration 15/25 | Loss: 0.00127837
Iteration 16/25 | Loss: 0.00127837
Iteration 17/25 | Loss: 0.00127837
Iteration 18/25 | Loss: 0.00127837
Iteration 19/25 | Loss: 0.00127837
Iteration 20/25 | Loss: 0.00127837
Iteration 21/25 | Loss: 0.00127837
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012783660786226392, 0.0012783660786226392, 0.0012783660786226392, 0.0012783660786226392, 0.0012783660786226392]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012783660786226392

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39617884
Iteration 2/25 | Loss: 0.00081137
Iteration 3/25 | Loss: 0.00081137
Iteration 4/25 | Loss: 0.00081137
Iteration 5/25 | Loss: 0.00081137
Iteration 6/25 | Loss: 0.00081137
Iteration 7/25 | Loss: 0.00081137
Iteration 8/25 | Loss: 0.00081137
Iteration 9/25 | Loss: 0.00081137
Iteration 10/25 | Loss: 0.00081137
Iteration 11/25 | Loss: 0.00081136
Iteration 12/25 | Loss: 0.00081136
Iteration 13/25 | Loss: 0.00081136
Iteration 14/25 | Loss: 0.00081136
Iteration 15/25 | Loss: 0.00081136
Iteration 16/25 | Loss: 0.00081136
Iteration 17/25 | Loss: 0.00081136
Iteration 18/25 | Loss: 0.00081136
Iteration 19/25 | Loss: 0.00081136
Iteration 20/25 | Loss: 0.00081136
Iteration 21/25 | Loss: 0.00081136
Iteration 22/25 | Loss: 0.00081136
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.000811364792753011, 0.000811364792753011, 0.000811364792753011, 0.000811364792753011, 0.000811364792753011]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000811364792753011

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081136
Iteration 2/1000 | Loss: 0.00002847
Iteration 3/1000 | Loss: 0.00002081
Iteration 4/1000 | Loss: 0.00001844
Iteration 5/1000 | Loss: 0.00001766
Iteration 6/1000 | Loss: 0.00001717
Iteration 7/1000 | Loss: 0.00001668
Iteration 8/1000 | Loss: 0.00001649
Iteration 9/1000 | Loss: 0.00001632
Iteration 10/1000 | Loss: 0.00001607
Iteration 11/1000 | Loss: 0.00001592
Iteration 12/1000 | Loss: 0.00001577
Iteration 13/1000 | Loss: 0.00001570
Iteration 14/1000 | Loss: 0.00001567
Iteration 15/1000 | Loss: 0.00001557
Iteration 16/1000 | Loss: 0.00001556
Iteration 17/1000 | Loss: 0.00001554
Iteration 18/1000 | Loss: 0.00001552
Iteration 19/1000 | Loss: 0.00001551
Iteration 20/1000 | Loss: 0.00001546
Iteration 21/1000 | Loss: 0.00001543
Iteration 22/1000 | Loss: 0.00001542
Iteration 23/1000 | Loss: 0.00001539
Iteration 24/1000 | Loss: 0.00001533
Iteration 25/1000 | Loss: 0.00001532
Iteration 26/1000 | Loss: 0.00001529
Iteration 27/1000 | Loss: 0.00001529
Iteration 28/1000 | Loss: 0.00001528
Iteration 29/1000 | Loss: 0.00001525
Iteration 30/1000 | Loss: 0.00001525
Iteration 31/1000 | Loss: 0.00001524
Iteration 32/1000 | Loss: 0.00001524
Iteration 33/1000 | Loss: 0.00001524
Iteration 34/1000 | Loss: 0.00001524
Iteration 35/1000 | Loss: 0.00001524
Iteration 36/1000 | Loss: 0.00001524
Iteration 37/1000 | Loss: 0.00001524
Iteration 38/1000 | Loss: 0.00001524
Iteration 39/1000 | Loss: 0.00001524
Iteration 40/1000 | Loss: 0.00001523
Iteration 41/1000 | Loss: 0.00001522
Iteration 42/1000 | Loss: 0.00001522
Iteration 43/1000 | Loss: 0.00001521
Iteration 44/1000 | Loss: 0.00001518
Iteration 45/1000 | Loss: 0.00001511
Iteration 46/1000 | Loss: 0.00001510
Iteration 47/1000 | Loss: 0.00001509
Iteration 48/1000 | Loss: 0.00001508
Iteration 49/1000 | Loss: 0.00001508
Iteration 50/1000 | Loss: 0.00001506
Iteration 51/1000 | Loss: 0.00001506
Iteration 52/1000 | Loss: 0.00001505
Iteration 53/1000 | Loss: 0.00001504
Iteration 54/1000 | Loss: 0.00001504
Iteration 55/1000 | Loss: 0.00001504
Iteration 56/1000 | Loss: 0.00001503
Iteration 57/1000 | Loss: 0.00001503
Iteration 58/1000 | Loss: 0.00001502
Iteration 59/1000 | Loss: 0.00001502
Iteration 60/1000 | Loss: 0.00001501
Iteration 61/1000 | Loss: 0.00001501
Iteration 62/1000 | Loss: 0.00001501
Iteration 63/1000 | Loss: 0.00001501
Iteration 64/1000 | Loss: 0.00001500
Iteration 65/1000 | Loss: 0.00001500
Iteration 66/1000 | Loss: 0.00001500
Iteration 67/1000 | Loss: 0.00001500
Iteration 68/1000 | Loss: 0.00001500
Iteration 69/1000 | Loss: 0.00001499
Iteration 70/1000 | Loss: 0.00001499
Iteration 71/1000 | Loss: 0.00001498
Iteration 72/1000 | Loss: 0.00001497
Iteration 73/1000 | Loss: 0.00001497
Iteration 74/1000 | Loss: 0.00001496
Iteration 75/1000 | Loss: 0.00001495
Iteration 76/1000 | Loss: 0.00001495
Iteration 77/1000 | Loss: 0.00001495
Iteration 78/1000 | Loss: 0.00001494
Iteration 79/1000 | Loss: 0.00001493
Iteration 80/1000 | Loss: 0.00001493
Iteration 81/1000 | Loss: 0.00001493
Iteration 82/1000 | Loss: 0.00001492
Iteration 83/1000 | Loss: 0.00001492
Iteration 84/1000 | Loss: 0.00001491
Iteration 85/1000 | Loss: 0.00001491
Iteration 86/1000 | Loss: 0.00001491
Iteration 87/1000 | Loss: 0.00001491
Iteration 88/1000 | Loss: 0.00001490
Iteration 89/1000 | Loss: 0.00001490
Iteration 90/1000 | Loss: 0.00001490
Iteration 91/1000 | Loss: 0.00001490
Iteration 92/1000 | Loss: 0.00001489
Iteration 93/1000 | Loss: 0.00001489
Iteration 94/1000 | Loss: 0.00001489
Iteration 95/1000 | Loss: 0.00001488
Iteration 96/1000 | Loss: 0.00001488
Iteration 97/1000 | Loss: 0.00001488
Iteration 98/1000 | Loss: 0.00001488
Iteration 99/1000 | Loss: 0.00001488
Iteration 100/1000 | Loss: 0.00001488
Iteration 101/1000 | Loss: 0.00001488
Iteration 102/1000 | Loss: 0.00001488
Iteration 103/1000 | Loss: 0.00001488
Iteration 104/1000 | Loss: 0.00001488
Iteration 105/1000 | Loss: 0.00001488
Iteration 106/1000 | Loss: 0.00001488
Iteration 107/1000 | Loss: 0.00001488
Iteration 108/1000 | Loss: 0.00001488
Iteration 109/1000 | Loss: 0.00001487
Iteration 110/1000 | Loss: 0.00001487
Iteration 111/1000 | Loss: 0.00001487
Iteration 112/1000 | Loss: 0.00001487
Iteration 113/1000 | Loss: 0.00001487
Iteration 114/1000 | Loss: 0.00001487
Iteration 115/1000 | Loss: 0.00001487
Iteration 116/1000 | Loss: 0.00001487
Iteration 117/1000 | Loss: 0.00001487
Iteration 118/1000 | Loss: 0.00001487
Iteration 119/1000 | Loss: 0.00001487
Iteration 120/1000 | Loss: 0.00001487
Iteration 121/1000 | Loss: 0.00001487
Iteration 122/1000 | Loss: 0.00001487
Iteration 123/1000 | Loss: 0.00001487
Iteration 124/1000 | Loss: 0.00001487
Iteration 125/1000 | Loss: 0.00001486
Iteration 126/1000 | Loss: 0.00001486
Iteration 127/1000 | Loss: 0.00001486
Iteration 128/1000 | Loss: 0.00001486
Iteration 129/1000 | Loss: 0.00001486
Iteration 130/1000 | Loss: 0.00001486
Iteration 131/1000 | Loss: 0.00001486
Iteration 132/1000 | Loss: 0.00001486
Iteration 133/1000 | Loss: 0.00001486
Iteration 134/1000 | Loss: 0.00001486
Iteration 135/1000 | Loss: 0.00001486
Iteration 136/1000 | Loss: 0.00001486
Iteration 137/1000 | Loss: 0.00001486
Iteration 138/1000 | Loss: 0.00001485
Iteration 139/1000 | Loss: 0.00001485
Iteration 140/1000 | Loss: 0.00001485
Iteration 141/1000 | Loss: 0.00001485
Iteration 142/1000 | Loss: 0.00001485
Iteration 143/1000 | Loss: 0.00001485
Iteration 144/1000 | Loss: 0.00001485
Iteration 145/1000 | Loss: 0.00001485
Iteration 146/1000 | Loss: 0.00001485
Iteration 147/1000 | Loss: 0.00001485
Iteration 148/1000 | Loss: 0.00001485
Iteration 149/1000 | Loss: 0.00001485
Iteration 150/1000 | Loss: 0.00001485
Iteration 151/1000 | Loss: 0.00001485
Iteration 152/1000 | Loss: 0.00001485
Iteration 153/1000 | Loss: 0.00001485
Iteration 154/1000 | Loss: 0.00001485
Iteration 155/1000 | Loss: 0.00001485
Iteration 156/1000 | Loss: 0.00001485
Iteration 157/1000 | Loss: 0.00001485
Iteration 158/1000 | Loss: 0.00001485
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [1.4848077626083978e-05, 1.4848077626083978e-05, 1.4848077626083978e-05, 1.4848077626083978e-05, 1.4848077626083978e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4848077626083978e-05

Optimization complete. Final v2v error: 3.3037142753601074 mm

Highest mean error: 3.5234830379486084 mm for frame 103

Lowest mean error: 3.146698236465454 mm for frame 157

Saving results

Total time: 39.81720018386841
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00979932
Iteration 2/25 | Loss: 0.00278239
Iteration 3/25 | Loss: 0.00202755
Iteration 4/25 | Loss: 0.00194807
Iteration 5/25 | Loss: 0.00213158
Iteration 6/25 | Loss: 0.00178561
Iteration 7/25 | Loss: 0.00150931
Iteration 8/25 | Loss: 0.00143842
Iteration 9/25 | Loss: 0.00142421
Iteration 10/25 | Loss: 0.00141263
Iteration 11/25 | Loss: 0.00140144
Iteration 12/25 | Loss: 0.00139844
Iteration 13/25 | Loss: 0.00139192
Iteration 14/25 | Loss: 0.00138873
Iteration 15/25 | Loss: 0.00138709
Iteration 16/25 | Loss: 0.00138626
Iteration 17/25 | Loss: 0.00138528
Iteration 18/25 | Loss: 0.00138512
Iteration 19/25 | Loss: 0.00138609
Iteration 20/25 | Loss: 0.00138365
Iteration 21/25 | Loss: 0.00138186
Iteration 22/25 | Loss: 0.00138208
Iteration 23/25 | Loss: 0.00138163
Iteration 24/25 | Loss: 0.00138108
Iteration 25/25 | Loss: 0.00138116

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37442088
Iteration 2/25 | Loss: 0.00076477
Iteration 3/25 | Loss: 0.00076477
Iteration 4/25 | Loss: 0.00076477
Iteration 5/25 | Loss: 0.00076476
Iteration 6/25 | Loss: 0.00076476
Iteration 7/25 | Loss: 0.00076476
Iteration 8/25 | Loss: 0.00076476
Iteration 9/25 | Loss: 0.00076476
Iteration 10/25 | Loss: 0.00076476
Iteration 11/25 | Loss: 0.00076476
Iteration 12/25 | Loss: 0.00076476
Iteration 13/25 | Loss: 0.00076476
Iteration 14/25 | Loss: 0.00076476
Iteration 15/25 | Loss: 0.00076476
Iteration 16/25 | Loss: 0.00076476
Iteration 17/25 | Loss: 0.00076476
Iteration 18/25 | Loss: 0.00076476
Iteration 19/25 | Loss: 0.00076476
Iteration 20/25 | Loss: 0.00076476
Iteration 21/25 | Loss: 0.00076476
Iteration 22/25 | Loss: 0.00076476
Iteration 23/25 | Loss: 0.00076476
Iteration 24/25 | Loss: 0.00076476
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.000764762400649488, 0.000764762400649488, 0.000764762400649488, 0.000764762400649488, 0.000764762400649488]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000764762400649488

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076476
Iteration 2/1000 | Loss: 0.00004829
Iteration 3/1000 | Loss: 0.00003797
Iteration 4/1000 | Loss: 0.00003641
Iteration 5/1000 | Loss: 0.00003215
Iteration 6/1000 | Loss: 0.00002892
Iteration 7/1000 | Loss: 0.00003723
Iteration 8/1000 | Loss: 0.00003810
Iteration 9/1000 | Loss: 0.00003748
Iteration 10/1000 | Loss: 0.00003601
Iteration 11/1000 | Loss: 0.00004374
Iteration 12/1000 | Loss: 0.00003583
Iteration 13/1000 | Loss: 0.00004507
Iteration 14/1000 | Loss: 0.00002909
Iteration 15/1000 | Loss: 0.00002741
Iteration 16/1000 | Loss: 0.00002616
Iteration 17/1000 | Loss: 0.00002558
Iteration 18/1000 | Loss: 0.00002498
Iteration 19/1000 | Loss: 0.00002467
Iteration 20/1000 | Loss: 0.00002458
Iteration 21/1000 | Loss: 0.00002458
Iteration 22/1000 | Loss: 0.00002458
Iteration 23/1000 | Loss: 0.00002458
Iteration 24/1000 | Loss: 0.00002458
Iteration 25/1000 | Loss: 0.00002458
Iteration 26/1000 | Loss: 0.00002457
Iteration 27/1000 | Loss: 0.00002457
Iteration 28/1000 | Loss: 0.00002457
Iteration 29/1000 | Loss: 0.00002457
Iteration 30/1000 | Loss: 0.00002457
Iteration 31/1000 | Loss: 0.00002457
Iteration 32/1000 | Loss: 0.00002457
Iteration 33/1000 | Loss: 0.00002457
Iteration 34/1000 | Loss: 0.00002457
Iteration 35/1000 | Loss: 0.00002457
Iteration 36/1000 | Loss: 0.00002457
Iteration 37/1000 | Loss: 0.00002457
Iteration 38/1000 | Loss: 0.00002457
Iteration 39/1000 | Loss: 0.00002457
Iteration 40/1000 | Loss: 0.00002457
Iteration 41/1000 | Loss: 0.00002457
Iteration 42/1000 | Loss: 0.00002457
Iteration 43/1000 | Loss: 0.00002457
Iteration 44/1000 | Loss: 0.00002457
Iteration 45/1000 | Loss: 0.00002457
Iteration 46/1000 | Loss: 0.00002457
Iteration 47/1000 | Loss: 0.00002457
Iteration 48/1000 | Loss: 0.00002457
Iteration 49/1000 | Loss: 0.00002457
Iteration 50/1000 | Loss: 0.00002457
Iteration 51/1000 | Loss: 0.00002457
Iteration 52/1000 | Loss: 0.00002457
Iteration 53/1000 | Loss: 0.00002457
Iteration 54/1000 | Loss: 0.00002457
Iteration 55/1000 | Loss: 0.00002457
Iteration 56/1000 | Loss: 0.00002457
Iteration 57/1000 | Loss: 0.00002457
Iteration 58/1000 | Loss: 0.00002457
Iteration 59/1000 | Loss: 0.00002457
Iteration 60/1000 | Loss: 0.00002457
Iteration 61/1000 | Loss: 0.00002457
Iteration 62/1000 | Loss: 0.00002457
Iteration 63/1000 | Loss: 0.00002457
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 63. Stopping optimization.
Last 5 losses: [2.457220762153156e-05, 2.457220762153156e-05, 2.457220762153156e-05, 2.457220762153156e-05, 2.457220762153156e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.457220762153156e-05

Optimization complete. Final v2v error: 4.214456081390381 mm

Highest mean error: 5.342921257019043 mm for frame 96

Lowest mean error: 3.9986636638641357 mm for frame 67

Saving results

Total time: 83.472665309906
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00409723
Iteration 2/25 | Loss: 0.00135224
Iteration 3/25 | Loss: 0.00127781
Iteration 4/25 | Loss: 0.00126382
Iteration 5/25 | Loss: 0.00125922
Iteration 6/25 | Loss: 0.00125810
Iteration 7/25 | Loss: 0.00125799
Iteration 8/25 | Loss: 0.00125799
Iteration 9/25 | Loss: 0.00125799
Iteration 10/25 | Loss: 0.00125799
Iteration 11/25 | Loss: 0.00125799
Iteration 12/25 | Loss: 0.00125799
Iteration 13/25 | Loss: 0.00125799
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012579908361658454, 0.0012579908361658454, 0.0012579908361658454, 0.0012579908361658454, 0.0012579908361658454]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012579908361658454

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.99169433
Iteration 2/25 | Loss: 0.00083027
Iteration 3/25 | Loss: 0.00083026
Iteration 4/25 | Loss: 0.00083026
Iteration 5/25 | Loss: 0.00083026
Iteration 6/25 | Loss: 0.00083026
Iteration 7/25 | Loss: 0.00083026
Iteration 8/25 | Loss: 0.00083026
Iteration 9/25 | Loss: 0.00083026
Iteration 10/25 | Loss: 0.00083026
Iteration 11/25 | Loss: 0.00083026
Iteration 12/25 | Loss: 0.00083026
Iteration 13/25 | Loss: 0.00083026
Iteration 14/25 | Loss: 0.00083026
Iteration 15/25 | Loss: 0.00083026
Iteration 16/25 | Loss: 0.00083026
Iteration 17/25 | Loss: 0.00083026
Iteration 18/25 | Loss: 0.00083026
Iteration 19/25 | Loss: 0.00083026
Iteration 20/25 | Loss: 0.00083026
Iteration 21/25 | Loss: 0.00083026
Iteration 22/25 | Loss: 0.00083026
Iteration 23/25 | Loss: 0.00083026
Iteration 24/25 | Loss: 0.00083026
Iteration 25/25 | Loss: 0.00083026

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083026
Iteration 2/1000 | Loss: 0.00003250
Iteration 3/1000 | Loss: 0.00002311
Iteration 4/1000 | Loss: 0.00002013
Iteration 5/1000 | Loss: 0.00001902
Iteration 6/1000 | Loss: 0.00001822
Iteration 7/1000 | Loss: 0.00001773
Iteration 8/1000 | Loss: 0.00001724
Iteration 9/1000 | Loss: 0.00001690
Iteration 10/1000 | Loss: 0.00001668
Iteration 11/1000 | Loss: 0.00001645
Iteration 12/1000 | Loss: 0.00001625
Iteration 13/1000 | Loss: 0.00001621
Iteration 14/1000 | Loss: 0.00001604
Iteration 15/1000 | Loss: 0.00001600
Iteration 16/1000 | Loss: 0.00001584
Iteration 17/1000 | Loss: 0.00001582
Iteration 18/1000 | Loss: 0.00001564
Iteration 19/1000 | Loss: 0.00001560
Iteration 20/1000 | Loss: 0.00001556
Iteration 21/1000 | Loss: 0.00001554
Iteration 22/1000 | Loss: 0.00001552
Iteration 23/1000 | Loss: 0.00001552
Iteration 24/1000 | Loss: 0.00001549
Iteration 25/1000 | Loss: 0.00001547
Iteration 26/1000 | Loss: 0.00001546
Iteration 27/1000 | Loss: 0.00001546
Iteration 28/1000 | Loss: 0.00001545
Iteration 29/1000 | Loss: 0.00001544
Iteration 30/1000 | Loss: 0.00001543
Iteration 31/1000 | Loss: 0.00001539
Iteration 32/1000 | Loss: 0.00001537
Iteration 33/1000 | Loss: 0.00001535
Iteration 34/1000 | Loss: 0.00001535
Iteration 35/1000 | Loss: 0.00001534
Iteration 36/1000 | Loss: 0.00001529
Iteration 37/1000 | Loss: 0.00001525
Iteration 38/1000 | Loss: 0.00001525
Iteration 39/1000 | Loss: 0.00001524
Iteration 40/1000 | Loss: 0.00001524
Iteration 41/1000 | Loss: 0.00001523
Iteration 42/1000 | Loss: 0.00001523
Iteration 43/1000 | Loss: 0.00001522
Iteration 44/1000 | Loss: 0.00001521
Iteration 45/1000 | Loss: 0.00001518
Iteration 46/1000 | Loss: 0.00001518
Iteration 47/1000 | Loss: 0.00001518
Iteration 48/1000 | Loss: 0.00001518
Iteration 49/1000 | Loss: 0.00001517
Iteration 50/1000 | Loss: 0.00001517
Iteration 51/1000 | Loss: 0.00001517
Iteration 52/1000 | Loss: 0.00001515
Iteration 53/1000 | Loss: 0.00001515
Iteration 54/1000 | Loss: 0.00001514
Iteration 55/1000 | Loss: 0.00001514
Iteration 56/1000 | Loss: 0.00001514
Iteration 57/1000 | Loss: 0.00001513
Iteration 58/1000 | Loss: 0.00001512
Iteration 59/1000 | Loss: 0.00001512
Iteration 60/1000 | Loss: 0.00001512
Iteration 61/1000 | Loss: 0.00001512
Iteration 62/1000 | Loss: 0.00001511
Iteration 63/1000 | Loss: 0.00001511
Iteration 64/1000 | Loss: 0.00001510
Iteration 65/1000 | Loss: 0.00001509
Iteration 66/1000 | Loss: 0.00001509
Iteration 67/1000 | Loss: 0.00001508
Iteration 68/1000 | Loss: 0.00001508
Iteration 69/1000 | Loss: 0.00001508
Iteration 70/1000 | Loss: 0.00001507
Iteration 71/1000 | Loss: 0.00001507
Iteration 72/1000 | Loss: 0.00001507
Iteration 73/1000 | Loss: 0.00001506
Iteration 74/1000 | Loss: 0.00001505
Iteration 75/1000 | Loss: 0.00001505
Iteration 76/1000 | Loss: 0.00001505
Iteration 77/1000 | Loss: 0.00001504
Iteration 78/1000 | Loss: 0.00001504
Iteration 79/1000 | Loss: 0.00001503
Iteration 80/1000 | Loss: 0.00001503
Iteration 81/1000 | Loss: 0.00001503
Iteration 82/1000 | Loss: 0.00001503
Iteration 83/1000 | Loss: 0.00001502
Iteration 84/1000 | Loss: 0.00001502
Iteration 85/1000 | Loss: 0.00001502
Iteration 86/1000 | Loss: 0.00001502
Iteration 87/1000 | Loss: 0.00001501
Iteration 88/1000 | Loss: 0.00001501
Iteration 89/1000 | Loss: 0.00001501
Iteration 90/1000 | Loss: 0.00001501
Iteration 91/1000 | Loss: 0.00001500
Iteration 92/1000 | Loss: 0.00001500
Iteration 93/1000 | Loss: 0.00001500
Iteration 94/1000 | Loss: 0.00001500
Iteration 95/1000 | Loss: 0.00001500
Iteration 96/1000 | Loss: 0.00001500
Iteration 97/1000 | Loss: 0.00001500
Iteration 98/1000 | Loss: 0.00001500
Iteration 99/1000 | Loss: 0.00001500
Iteration 100/1000 | Loss: 0.00001500
Iteration 101/1000 | Loss: 0.00001500
Iteration 102/1000 | Loss: 0.00001500
Iteration 103/1000 | Loss: 0.00001500
Iteration 104/1000 | Loss: 0.00001500
Iteration 105/1000 | Loss: 0.00001500
Iteration 106/1000 | Loss: 0.00001500
Iteration 107/1000 | Loss: 0.00001499
Iteration 108/1000 | Loss: 0.00001499
Iteration 109/1000 | Loss: 0.00001499
Iteration 110/1000 | Loss: 0.00001499
Iteration 111/1000 | Loss: 0.00001499
Iteration 112/1000 | Loss: 0.00001499
Iteration 113/1000 | Loss: 0.00001499
Iteration 114/1000 | Loss: 0.00001499
Iteration 115/1000 | Loss: 0.00001499
Iteration 116/1000 | Loss: 0.00001499
Iteration 117/1000 | Loss: 0.00001499
Iteration 118/1000 | Loss: 0.00001499
Iteration 119/1000 | Loss: 0.00001499
Iteration 120/1000 | Loss: 0.00001499
Iteration 121/1000 | Loss: 0.00001498
Iteration 122/1000 | Loss: 0.00001498
Iteration 123/1000 | Loss: 0.00001498
Iteration 124/1000 | Loss: 0.00001498
Iteration 125/1000 | Loss: 0.00001498
Iteration 126/1000 | Loss: 0.00001498
Iteration 127/1000 | Loss: 0.00001498
Iteration 128/1000 | Loss: 0.00001498
Iteration 129/1000 | Loss: 0.00001498
Iteration 130/1000 | Loss: 0.00001498
Iteration 131/1000 | Loss: 0.00001498
Iteration 132/1000 | Loss: 0.00001498
Iteration 133/1000 | Loss: 0.00001498
Iteration 134/1000 | Loss: 0.00001497
Iteration 135/1000 | Loss: 0.00001497
Iteration 136/1000 | Loss: 0.00001497
Iteration 137/1000 | Loss: 0.00001497
Iteration 138/1000 | Loss: 0.00001497
Iteration 139/1000 | Loss: 0.00001497
Iteration 140/1000 | Loss: 0.00001497
Iteration 141/1000 | Loss: 0.00001497
Iteration 142/1000 | Loss: 0.00001497
Iteration 143/1000 | Loss: 0.00001497
Iteration 144/1000 | Loss: 0.00001497
Iteration 145/1000 | Loss: 0.00001497
Iteration 146/1000 | Loss: 0.00001497
Iteration 147/1000 | Loss: 0.00001497
Iteration 148/1000 | Loss: 0.00001497
Iteration 149/1000 | Loss: 0.00001497
Iteration 150/1000 | Loss: 0.00001497
Iteration 151/1000 | Loss: 0.00001497
Iteration 152/1000 | Loss: 0.00001497
Iteration 153/1000 | Loss: 0.00001496
Iteration 154/1000 | Loss: 0.00001496
Iteration 155/1000 | Loss: 0.00001496
Iteration 156/1000 | Loss: 0.00001496
Iteration 157/1000 | Loss: 0.00001496
Iteration 158/1000 | Loss: 0.00001496
Iteration 159/1000 | Loss: 0.00001496
Iteration 160/1000 | Loss: 0.00001496
Iteration 161/1000 | Loss: 0.00001496
Iteration 162/1000 | Loss: 0.00001496
Iteration 163/1000 | Loss: 0.00001496
Iteration 164/1000 | Loss: 0.00001496
Iteration 165/1000 | Loss: 0.00001496
Iteration 166/1000 | Loss: 0.00001496
Iteration 167/1000 | Loss: 0.00001496
Iteration 168/1000 | Loss: 0.00001496
Iteration 169/1000 | Loss: 0.00001496
Iteration 170/1000 | Loss: 0.00001495
Iteration 171/1000 | Loss: 0.00001495
Iteration 172/1000 | Loss: 0.00001495
Iteration 173/1000 | Loss: 0.00001495
Iteration 174/1000 | Loss: 0.00001495
Iteration 175/1000 | Loss: 0.00001495
Iteration 176/1000 | Loss: 0.00001495
Iteration 177/1000 | Loss: 0.00001495
Iteration 178/1000 | Loss: 0.00001495
Iteration 179/1000 | Loss: 0.00001495
Iteration 180/1000 | Loss: 0.00001495
Iteration 181/1000 | Loss: 0.00001495
Iteration 182/1000 | Loss: 0.00001495
Iteration 183/1000 | Loss: 0.00001495
Iteration 184/1000 | Loss: 0.00001495
Iteration 185/1000 | Loss: 0.00001495
Iteration 186/1000 | Loss: 0.00001495
Iteration 187/1000 | Loss: 0.00001495
Iteration 188/1000 | Loss: 0.00001495
Iteration 189/1000 | Loss: 0.00001495
Iteration 190/1000 | Loss: 0.00001494
Iteration 191/1000 | Loss: 0.00001494
Iteration 192/1000 | Loss: 0.00001494
Iteration 193/1000 | Loss: 0.00001494
Iteration 194/1000 | Loss: 0.00001494
Iteration 195/1000 | Loss: 0.00001494
Iteration 196/1000 | Loss: 0.00001494
Iteration 197/1000 | Loss: 0.00001494
Iteration 198/1000 | Loss: 0.00001494
Iteration 199/1000 | Loss: 0.00001494
Iteration 200/1000 | Loss: 0.00001494
Iteration 201/1000 | Loss: 0.00001494
Iteration 202/1000 | Loss: 0.00001494
Iteration 203/1000 | Loss: 0.00001494
Iteration 204/1000 | Loss: 0.00001494
Iteration 205/1000 | Loss: 0.00001494
Iteration 206/1000 | Loss: 0.00001494
Iteration 207/1000 | Loss: 0.00001494
Iteration 208/1000 | Loss: 0.00001494
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [1.4939731045160443e-05, 1.4939731045160443e-05, 1.4939731045160443e-05, 1.4939731045160443e-05, 1.4939731045160443e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4939731045160443e-05

Optimization complete. Final v2v error: 3.3006582260131836 mm

Highest mean error: 4.299848556518555 mm for frame 56

Lowest mean error: 3.0428218841552734 mm for frame 92

Saving results

Total time: 46.747190713882446
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00432340
Iteration 2/25 | Loss: 0.00141108
Iteration 3/25 | Loss: 0.00134894
Iteration 4/25 | Loss: 0.00134402
Iteration 5/25 | Loss: 0.00134266
Iteration 6/25 | Loss: 0.00134227
Iteration 7/25 | Loss: 0.00134223
Iteration 8/25 | Loss: 0.00134223
Iteration 9/25 | Loss: 0.00134223
Iteration 10/25 | Loss: 0.00134223
Iteration 11/25 | Loss: 0.00134223
Iteration 12/25 | Loss: 0.00134223
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013422326883301139, 0.0013422326883301139, 0.0013422326883301139, 0.0013422326883301139, 0.0013422326883301139]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013422326883301139

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42750466
Iteration 2/25 | Loss: 0.00096664
Iteration 3/25 | Loss: 0.00096664
Iteration 4/25 | Loss: 0.00096664
Iteration 5/25 | Loss: 0.00096664
Iteration 6/25 | Loss: 0.00096664
Iteration 7/25 | Loss: 0.00096664
Iteration 8/25 | Loss: 0.00096664
Iteration 9/25 | Loss: 0.00096664
Iteration 10/25 | Loss: 0.00096663
Iteration 11/25 | Loss: 0.00096663
Iteration 12/25 | Loss: 0.00096663
Iteration 13/25 | Loss: 0.00096663
Iteration 14/25 | Loss: 0.00096663
Iteration 15/25 | Loss: 0.00096663
Iteration 16/25 | Loss: 0.00096663
Iteration 17/25 | Loss: 0.00096663
Iteration 18/25 | Loss: 0.00096663
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009666344849392772, 0.0009666344849392772, 0.0009666344849392772, 0.0009666344849392772, 0.0009666344849392772]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009666344849392772

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096663
Iteration 2/1000 | Loss: 0.00004469
Iteration 3/1000 | Loss: 0.00002587
Iteration 4/1000 | Loss: 0.00002255
Iteration 5/1000 | Loss: 0.00002147
Iteration 6/1000 | Loss: 0.00002067
Iteration 7/1000 | Loss: 0.00002015
Iteration 8/1000 | Loss: 0.00001977
Iteration 9/1000 | Loss: 0.00001956
Iteration 10/1000 | Loss: 0.00001942
Iteration 11/1000 | Loss: 0.00001928
Iteration 12/1000 | Loss: 0.00001919
Iteration 13/1000 | Loss: 0.00001905
Iteration 14/1000 | Loss: 0.00001892
Iteration 15/1000 | Loss: 0.00001886
Iteration 16/1000 | Loss: 0.00001883
Iteration 17/1000 | Loss: 0.00001878
Iteration 18/1000 | Loss: 0.00001875
Iteration 19/1000 | Loss: 0.00001874
Iteration 20/1000 | Loss: 0.00001872
Iteration 21/1000 | Loss: 0.00001867
Iteration 22/1000 | Loss: 0.00001867
Iteration 23/1000 | Loss: 0.00001865
Iteration 24/1000 | Loss: 0.00001865
Iteration 25/1000 | Loss: 0.00001864
Iteration 26/1000 | Loss: 0.00001863
Iteration 27/1000 | Loss: 0.00001863
Iteration 28/1000 | Loss: 0.00001863
Iteration 29/1000 | Loss: 0.00001863
Iteration 30/1000 | Loss: 0.00001863
Iteration 31/1000 | Loss: 0.00001863
Iteration 32/1000 | Loss: 0.00001863
Iteration 33/1000 | Loss: 0.00001863
Iteration 34/1000 | Loss: 0.00001863
Iteration 35/1000 | Loss: 0.00001863
Iteration 36/1000 | Loss: 0.00001863
Iteration 37/1000 | Loss: 0.00001863
Iteration 38/1000 | Loss: 0.00001862
Iteration 39/1000 | Loss: 0.00001862
Iteration 40/1000 | Loss: 0.00001862
Iteration 41/1000 | Loss: 0.00001861
Iteration 42/1000 | Loss: 0.00001861
Iteration 43/1000 | Loss: 0.00001861
Iteration 44/1000 | Loss: 0.00001860
Iteration 45/1000 | Loss: 0.00001860
Iteration 46/1000 | Loss: 0.00001860
Iteration 47/1000 | Loss: 0.00001860
Iteration 48/1000 | Loss: 0.00001860
Iteration 49/1000 | Loss: 0.00001860
Iteration 50/1000 | Loss: 0.00001860
Iteration 51/1000 | Loss: 0.00001860
Iteration 52/1000 | Loss: 0.00001859
Iteration 53/1000 | Loss: 0.00001859
Iteration 54/1000 | Loss: 0.00001859
Iteration 55/1000 | Loss: 0.00001858
Iteration 56/1000 | Loss: 0.00001858
Iteration 57/1000 | Loss: 0.00001858
Iteration 58/1000 | Loss: 0.00001858
Iteration 59/1000 | Loss: 0.00001857
Iteration 60/1000 | Loss: 0.00001857
Iteration 61/1000 | Loss: 0.00001857
Iteration 62/1000 | Loss: 0.00001856
Iteration 63/1000 | Loss: 0.00001856
Iteration 64/1000 | Loss: 0.00001856
Iteration 65/1000 | Loss: 0.00001856
Iteration 66/1000 | Loss: 0.00001856
Iteration 67/1000 | Loss: 0.00001855
Iteration 68/1000 | Loss: 0.00001855
Iteration 69/1000 | Loss: 0.00001855
Iteration 70/1000 | Loss: 0.00001854
Iteration 71/1000 | Loss: 0.00001854
Iteration 72/1000 | Loss: 0.00001854
Iteration 73/1000 | Loss: 0.00001854
Iteration 74/1000 | Loss: 0.00001854
Iteration 75/1000 | Loss: 0.00001854
Iteration 76/1000 | Loss: 0.00001853
Iteration 77/1000 | Loss: 0.00001853
Iteration 78/1000 | Loss: 0.00001852
Iteration 79/1000 | Loss: 0.00001852
Iteration 80/1000 | Loss: 0.00001852
Iteration 81/1000 | Loss: 0.00001852
Iteration 82/1000 | Loss: 0.00001852
Iteration 83/1000 | Loss: 0.00001852
Iteration 84/1000 | Loss: 0.00001852
Iteration 85/1000 | Loss: 0.00001852
Iteration 86/1000 | Loss: 0.00001852
Iteration 87/1000 | Loss: 0.00001852
Iteration 88/1000 | Loss: 0.00001852
Iteration 89/1000 | Loss: 0.00001851
Iteration 90/1000 | Loss: 0.00001851
Iteration 91/1000 | Loss: 0.00001851
Iteration 92/1000 | Loss: 0.00001851
Iteration 93/1000 | Loss: 0.00001851
Iteration 94/1000 | Loss: 0.00001851
Iteration 95/1000 | Loss: 0.00001851
Iteration 96/1000 | Loss: 0.00001850
Iteration 97/1000 | Loss: 0.00001850
Iteration 98/1000 | Loss: 0.00001850
Iteration 99/1000 | Loss: 0.00001850
Iteration 100/1000 | Loss: 0.00001850
Iteration 101/1000 | Loss: 0.00001850
Iteration 102/1000 | Loss: 0.00001850
Iteration 103/1000 | Loss: 0.00001850
Iteration 104/1000 | Loss: 0.00001849
Iteration 105/1000 | Loss: 0.00001849
Iteration 106/1000 | Loss: 0.00001849
Iteration 107/1000 | Loss: 0.00001849
Iteration 108/1000 | Loss: 0.00001849
Iteration 109/1000 | Loss: 0.00001849
Iteration 110/1000 | Loss: 0.00001849
Iteration 111/1000 | Loss: 0.00001849
Iteration 112/1000 | Loss: 0.00001849
Iteration 113/1000 | Loss: 0.00001849
Iteration 114/1000 | Loss: 0.00001849
Iteration 115/1000 | Loss: 0.00001849
Iteration 116/1000 | Loss: 0.00001849
Iteration 117/1000 | Loss: 0.00001849
Iteration 118/1000 | Loss: 0.00001849
Iteration 119/1000 | Loss: 0.00001849
Iteration 120/1000 | Loss: 0.00001849
Iteration 121/1000 | Loss: 0.00001849
Iteration 122/1000 | Loss: 0.00001848
Iteration 123/1000 | Loss: 0.00001848
Iteration 124/1000 | Loss: 0.00001848
Iteration 125/1000 | Loss: 0.00001848
Iteration 126/1000 | Loss: 0.00001848
Iteration 127/1000 | Loss: 0.00001848
Iteration 128/1000 | Loss: 0.00001848
Iteration 129/1000 | Loss: 0.00001848
Iteration 130/1000 | Loss: 0.00001847
Iteration 131/1000 | Loss: 0.00001847
Iteration 132/1000 | Loss: 0.00001847
Iteration 133/1000 | Loss: 0.00001847
Iteration 134/1000 | Loss: 0.00001847
Iteration 135/1000 | Loss: 0.00001847
Iteration 136/1000 | Loss: 0.00001846
Iteration 137/1000 | Loss: 0.00001846
Iteration 138/1000 | Loss: 0.00001846
Iteration 139/1000 | Loss: 0.00001846
Iteration 140/1000 | Loss: 0.00001846
Iteration 141/1000 | Loss: 0.00001846
Iteration 142/1000 | Loss: 0.00001846
Iteration 143/1000 | Loss: 0.00001846
Iteration 144/1000 | Loss: 0.00001846
Iteration 145/1000 | Loss: 0.00001846
Iteration 146/1000 | Loss: 0.00001846
Iteration 147/1000 | Loss: 0.00001846
Iteration 148/1000 | Loss: 0.00001845
Iteration 149/1000 | Loss: 0.00001845
Iteration 150/1000 | Loss: 0.00001845
Iteration 151/1000 | Loss: 0.00001845
Iteration 152/1000 | Loss: 0.00001845
Iteration 153/1000 | Loss: 0.00001845
Iteration 154/1000 | Loss: 0.00001845
Iteration 155/1000 | Loss: 0.00001845
Iteration 156/1000 | Loss: 0.00001844
Iteration 157/1000 | Loss: 0.00001844
Iteration 158/1000 | Loss: 0.00001844
Iteration 159/1000 | Loss: 0.00001844
Iteration 160/1000 | Loss: 0.00001844
Iteration 161/1000 | Loss: 0.00001844
Iteration 162/1000 | Loss: 0.00001844
Iteration 163/1000 | Loss: 0.00001844
Iteration 164/1000 | Loss: 0.00001843
Iteration 165/1000 | Loss: 0.00001843
Iteration 166/1000 | Loss: 0.00001843
Iteration 167/1000 | Loss: 0.00001843
Iteration 168/1000 | Loss: 0.00001843
Iteration 169/1000 | Loss: 0.00001843
Iteration 170/1000 | Loss: 0.00001843
Iteration 171/1000 | Loss: 0.00001843
Iteration 172/1000 | Loss: 0.00001843
Iteration 173/1000 | Loss: 0.00001842
Iteration 174/1000 | Loss: 0.00001842
Iteration 175/1000 | Loss: 0.00001842
Iteration 176/1000 | Loss: 0.00001842
Iteration 177/1000 | Loss: 0.00001842
Iteration 178/1000 | Loss: 0.00001842
Iteration 179/1000 | Loss: 0.00001842
Iteration 180/1000 | Loss: 0.00001841
Iteration 181/1000 | Loss: 0.00001841
Iteration 182/1000 | Loss: 0.00001841
Iteration 183/1000 | Loss: 0.00001841
Iteration 184/1000 | Loss: 0.00001841
Iteration 185/1000 | Loss: 0.00001841
Iteration 186/1000 | Loss: 0.00001841
Iteration 187/1000 | Loss: 0.00001841
Iteration 188/1000 | Loss: 0.00001841
Iteration 189/1000 | Loss: 0.00001841
Iteration 190/1000 | Loss: 0.00001841
Iteration 191/1000 | Loss: 0.00001841
Iteration 192/1000 | Loss: 0.00001841
Iteration 193/1000 | Loss: 0.00001840
Iteration 194/1000 | Loss: 0.00001840
Iteration 195/1000 | Loss: 0.00001840
Iteration 196/1000 | Loss: 0.00001840
Iteration 197/1000 | Loss: 0.00001840
Iteration 198/1000 | Loss: 0.00001840
Iteration 199/1000 | Loss: 0.00001840
Iteration 200/1000 | Loss: 0.00001840
Iteration 201/1000 | Loss: 0.00001840
Iteration 202/1000 | Loss: 0.00001840
Iteration 203/1000 | Loss: 0.00001840
Iteration 204/1000 | Loss: 0.00001840
Iteration 205/1000 | Loss: 0.00001840
Iteration 206/1000 | Loss: 0.00001840
Iteration 207/1000 | Loss: 0.00001840
Iteration 208/1000 | Loss: 0.00001840
Iteration 209/1000 | Loss: 0.00001840
Iteration 210/1000 | Loss: 0.00001840
Iteration 211/1000 | Loss: 0.00001839
Iteration 212/1000 | Loss: 0.00001839
Iteration 213/1000 | Loss: 0.00001839
Iteration 214/1000 | Loss: 0.00001839
Iteration 215/1000 | Loss: 0.00001839
Iteration 216/1000 | Loss: 0.00001839
Iteration 217/1000 | Loss: 0.00001839
Iteration 218/1000 | Loss: 0.00001839
Iteration 219/1000 | Loss: 0.00001839
Iteration 220/1000 | Loss: 0.00001839
Iteration 221/1000 | Loss: 0.00001839
Iteration 222/1000 | Loss: 0.00001839
Iteration 223/1000 | Loss: 0.00001839
Iteration 224/1000 | Loss: 0.00001839
Iteration 225/1000 | Loss: 0.00001839
Iteration 226/1000 | Loss: 0.00001839
Iteration 227/1000 | Loss: 0.00001839
Iteration 228/1000 | Loss: 0.00001839
Iteration 229/1000 | Loss: 0.00001839
Iteration 230/1000 | Loss: 0.00001839
Iteration 231/1000 | Loss: 0.00001839
Iteration 232/1000 | Loss: 0.00001838
Iteration 233/1000 | Loss: 0.00001838
Iteration 234/1000 | Loss: 0.00001838
Iteration 235/1000 | Loss: 0.00001838
Iteration 236/1000 | Loss: 0.00001838
Iteration 237/1000 | Loss: 0.00001838
Iteration 238/1000 | Loss: 0.00001838
Iteration 239/1000 | Loss: 0.00001838
Iteration 240/1000 | Loss: 0.00001838
Iteration 241/1000 | Loss: 0.00001838
Iteration 242/1000 | Loss: 0.00001838
Iteration 243/1000 | Loss: 0.00001838
Iteration 244/1000 | Loss: 0.00001838
Iteration 245/1000 | Loss: 0.00001838
Iteration 246/1000 | Loss: 0.00001838
Iteration 247/1000 | Loss: 0.00001838
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 247. Stopping optimization.
Last 5 losses: [1.8383414499112405e-05, 1.8383414499112405e-05, 1.8383414499112405e-05, 1.8383414499112405e-05, 1.8383414499112405e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8383414499112405e-05

Optimization complete. Final v2v error: 3.501446485519409 mm

Highest mean error: 4.735833644866943 mm for frame 38

Lowest mean error: 3.0841379165649414 mm for frame 93

Saving results

Total time: 41.884400606155396
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00552209
Iteration 2/25 | Loss: 0.00146697
Iteration 3/25 | Loss: 0.00138371
Iteration 4/25 | Loss: 0.00136332
Iteration 5/25 | Loss: 0.00135557
Iteration 6/25 | Loss: 0.00135414
Iteration 7/25 | Loss: 0.00135414
Iteration 8/25 | Loss: 0.00135414
Iteration 9/25 | Loss: 0.00135414
Iteration 10/25 | Loss: 0.00135414
Iteration 11/25 | Loss: 0.00135414
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013541383668780327, 0.0013541383668780327, 0.0013541383668780327, 0.0013541383668780327, 0.0013541383668780327]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013541383668780327

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36343431
Iteration 2/25 | Loss: 0.00110193
Iteration 3/25 | Loss: 0.00110193
Iteration 4/25 | Loss: 0.00110193
Iteration 5/25 | Loss: 0.00110193
Iteration 6/25 | Loss: 0.00110193
Iteration 7/25 | Loss: 0.00110193
Iteration 8/25 | Loss: 0.00110193
Iteration 9/25 | Loss: 0.00110193
Iteration 10/25 | Loss: 0.00110193
Iteration 11/25 | Loss: 0.00110193
Iteration 12/25 | Loss: 0.00110193
Iteration 13/25 | Loss: 0.00110193
Iteration 14/25 | Loss: 0.00110193
Iteration 15/25 | Loss: 0.00110193
Iteration 16/25 | Loss: 0.00110193
Iteration 17/25 | Loss: 0.00110193
Iteration 18/25 | Loss: 0.00110193
Iteration 19/25 | Loss: 0.00110193
Iteration 20/25 | Loss: 0.00110193
Iteration 21/25 | Loss: 0.00110193
Iteration 22/25 | Loss: 0.00110193
Iteration 23/25 | Loss: 0.00110193
Iteration 24/25 | Loss: 0.00110193
Iteration 25/25 | Loss: 0.00110193

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110193
Iteration 2/1000 | Loss: 0.00004226
Iteration 3/1000 | Loss: 0.00002943
Iteration 4/1000 | Loss: 0.00002645
Iteration 5/1000 | Loss: 0.00002511
Iteration 6/1000 | Loss: 0.00002399
Iteration 7/1000 | Loss: 0.00002310
Iteration 8/1000 | Loss: 0.00002267
Iteration 9/1000 | Loss: 0.00002219
Iteration 10/1000 | Loss: 0.00002175
Iteration 11/1000 | Loss: 0.00002153
Iteration 12/1000 | Loss: 0.00002131
Iteration 13/1000 | Loss: 0.00002116
Iteration 14/1000 | Loss: 0.00002114
Iteration 15/1000 | Loss: 0.00002107
Iteration 16/1000 | Loss: 0.00002102
Iteration 17/1000 | Loss: 0.00002102
Iteration 18/1000 | Loss: 0.00002100
Iteration 19/1000 | Loss: 0.00002095
Iteration 20/1000 | Loss: 0.00002095
Iteration 21/1000 | Loss: 0.00002095
Iteration 22/1000 | Loss: 0.00002095
Iteration 23/1000 | Loss: 0.00002094
Iteration 24/1000 | Loss: 0.00002091
Iteration 25/1000 | Loss: 0.00002091
Iteration 26/1000 | Loss: 0.00002090
Iteration 27/1000 | Loss: 0.00002090
Iteration 28/1000 | Loss: 0.00002090
Iteration 29/1000 | Loss: 0.00002090
Iteration 30/1000 | Loss: 0.00002089
Iteration 31/1000 | Loss: 0.00002086
Iteration 32/1000 | Loss: 0.00002086
Iteration 33/1000 | Loss: 0.00002086
Iteration 34/1000 | Loss: 0.00002085
Iteration 35/1000 | Loss: 0.00002085
Iteration 36/1000 | Loss: 0.00002082
Iteration 37/1000 | Loss: 0.00002082
Iteration 38/1000 | Loss: 0.00002082
Iteration 39/1000 | Loss: 0.00002082
Iteration 40/1000 | Loss: 0.00002082
Iteration 41/1000 | Loss: 0.00002082
Iteration 42/1000 | Loss: 0.00002081
Iteration 43/1000 | Loss: 0.00002081
Iteration 44/1000 | Loss: 0.00002081
Iteration 45/1000 | Loss: 0.00002081
Iteration 46/1000 | Loss: 0.00002081
Iteration 47/1000 | Loss: 0.00002081
Iteration 48/1000 | Loss: 0.00002081
Iteration 49/1000 | Loss: 0.00002081
Iteration 50/1000 | Loss: 0.00002079
Iteration 51/1000 | Loss: 0.00002079
Iteration 52/1000 | Loss: 0.00002078
Iteration 53/1000 | Loss: 0.00002078
Iteration 54/1000 | Loss: 0.00002077
Iteration 55/1000 | Loss: 0.00002077
Iteration 56/1000 | Loss: 0.00002077
Iteration 57/1000 | Loss: 0.00002076
Iteration 58/1000 | Loss: 0.00002076
Iteration 59/1000 | Loss: 0.00002075
Iteration 60/1000 | Loss: 0.00002074
Iteration 61/1000 | Loss: 0.00002074
Iteration 62/1000 | Loss: 0.00002074
Iteration 63/1000 | Loss: 0.00002074
Iteration 64/1000 | Loss: 0.00002073
Iteration 65/1000 | Loss: 0.00002073
Iteration 66/1000 | Loss: 0.00002073
Iteration 67/1000 | Loss: 0.00002072
Iteration 68/1000 | Loss: 0.00002071
Iteration 69/1000 | Loss: 0.00002071
Iteration 70/1000 | Loss: 0.00002070
Iteration 71/1000 | Loss: 0.00002070
Iteration 72/1000 | Loss: 0.00002070
Iteration 73/1000 | Loss: 0.00002069
Iteration 74/1000 | Loss: 0.00002069
Iteration 75/1000 | Loss: 0.00002069
Iteration 76/1000 | Loss: 0.00002069
Iteration 77/1000 | Loss: 0.00002069
Iteration 78/1000 | Loss: 0.00002069
Iteration 79/1000 | Loss: 0.00002069
Iteration 80/1000 | Loss: 0.00002069
Iteration 81/1000 | Loss: 0.00002069
Iteration 82/1000 | Loss: 0.00002068
Iteration 83/1000 | Loss: 0.00002068
Iteration 84/1000 | Loss: 0.00002068
Iteration 85/1000 | Loss: 0.00002068
Iteration 86/1000 | Loss: 0.00002068
Iteration 87/1000 | Loss: 0.00002068
Iteration 88/1000 | Loss: 0.00002068
Iteration 89/1000 | Loss: 0.00002068
Iteration 90/1000 | Loss: 0.00002067
Iteration 91/1000 | Loss: 0.00002067
Iteration 92/1000 | Loss: 0.00002066
Iteration 93/1000 | Loss: 0.00002066
Iteration 94/1000 | Loss: 0.00002066
Iteration 95/1000 | Loss: 0.00002065
Iteration 96/1000 | Loss: 0.00002065
Iteration 97/1000 | Loss: 0.00002064
Iteration 98/1000 | Loss: 0.00002064
Iteration 99/1000 | Loss: 0.00002063
Iteration 100/1000 | Loss: 0.00002063
Iteration 101/1000 | Loss: 0.00002063
Iteration 102/1000 | Loss: 0.00002063
Iteration 103/1000 | Loss: 0.00002062
Iteration 104/1000 | Loss: 0.00002062
Iteration 105/1000 | Loss: 0.00002062
Iteration 106/1000 | Loss: 0.00002061
Iteration 107/1000 | Loss: 0.00002061
Iteration 108/1000 | Loss: 0.00002060
Iteration 109/1000 | Loss: 0.00002060
Iteration 110/1000 | Loss: 0.00002060
Iteration 111/1000 | Loss: 0.00002060
Iteration 112/1000 | Loss: 0.00002060
Iteration 113/1000 | Loss: 0.00002060
Iteration 114/1000 | Loss: 0.00002060
Iteration 115/1000 | Loss: 0.00002060
Iteration 116/1000 | Loss: 0.00002060
Iteration 117/1000 | Loss: 0.00002060
Iteration 118/1000 | Loss: 0.00002060
Iteration 119/1000 | Loss: 0.00002059
Iteration 120/1000 | Loss: 0.00002059
Iteration 121/1000 | Loss: 0.00002059
Iteration 122/1000 | Loss: 0.00002059
Iteration 123/1000 | Loss: 0.00002059
Iteration 124/1000 | Loss: 0.00002059
Iteration 125/1000 | Loss: 0.00002059
Iteration 126/1000 | Loss: 0.00002059
Iteration 127/1000 | Loss: 0.00002058
Iteration 128/1000 | Loss: 0.00002058
Iteration 129/1000 | Loss: 0.00002058
Iteration 130/1000 | Loss: 0.00002058
Iteration 131/1000 | Loss: 0.00002058
Iteration 132/1000 | Loss: 0.00002058
Iteration 133/1000 | Loss: 0.00002058
Iteration 134/1000 | Loss: 0.00002058
Iteration 135/1000 | Loss: 0.00002057
Iteration 136/1000 | Loss: 0.00002057
Iteration 137/1000 | Loss: 0.00002057
Iteration 138/1000 | Loss: 0.00002057
Iteration 139/1000 | Loss: 0.00002057
Iteration 140/1000 | Loss: 0.00002057
Iteration 141/1000 | Loss: 0.00002057
Iteration 142/1000 | Loss: 0.00002057
Iteration 143/1000 | Loss: 0.00002057
Iteration 144/1000 | Loss: 0.00002057
Iteration 145/1000 | Loss: 0.00002057
Iteration 146/1000 | Loss: 0.00002057
Iteration 147/1000 | Loss: 0.00002057
Iteration 148/1000 | Loss: 0.00002057
Iteration 149/1000 | Loss: 0.00002057
Iteration 150/1000 | Loss: 0.00002057
Iteration 151/1000 | Loss: 0.00002057
Iteration 152/1000 | Loss: 0.00002057
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [2.0571091226884164e-05, 2.0571091226884164e-05, 2.0571091226884164e-05, 2.0571091226884164e-05, 2.0571091226884164e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0571091226884164e-05

Optimization complete. Final v2v error: 3.8243813514709473 mm

Highest mean error: 4.396047115325928 mm for frame 66

Lowest mean error: 3.3709089756011963 mm for frame 224

Saving results

Total time: 44.07162165641785
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00842418
Iteration 2/25 | Loss: 0.00194558
Iteration 3/25 | Loss: 0.00150217
Iteration 4/25 | Loss: 0.00146751
Iteration 5/25 | Loss: 0.00146398
Iteration 6/25 | Loss: 0.00146350
Iteration 7/25 | Loss: 0.00146350
Iteration 8/25 | Loss: 0.00146350
Iteration 9/25 | Loss: 0.00146350
Iteration 10/25 | Loss: 0.00146350
Iteration 11/25 | Loss: 0.00146350
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014635039260610938, 0.0014635039260610938, 0.0014635039260610938, 0.0014635039260610938, 0.0014635039260610938]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014635039260610938

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22116899
Iteration 2/25 | Loss: 0.00085077
Iteration 3/25 | Loss: 0.00085077
Iteration 4/25 | Loss: 0.00085077
Iteration 5/25 | Loss: 0.00085077
Iteration 6/25 | Loss: 0.00085077
Iteration 7/25 | Loss: 0.00085077
Iteration 8/25 | Loss: 0.00085077
Iteration 9/25 | Loss: 0.00085076
Iteration 10/25 | Loss: 0.00085076
Iteration 11/25 | Loss: 0.00085076
Iteration 12/25 | Loss: 0.00085076
Iteration 13/25 | Loss: 0.00085076
Iteration 14/25 | Loss: 0.00085076
Iteration 15/25 | Loss: 0.00085076
Iteration 16/25 | Loss: 0.00085076
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008507646271027625, 0.0008507646271027625, 0.0008507646271027625, 0.0008507646271027625, 0.0008507646271027625]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008507646271027625

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085076
Iteration 2/1000 | Loss: 0.00006214
Iteration 3/1000 | Loss: 0.00003532
Iteration 4/1000 | Loss: 0.00003010
Iteration 5/1000 | Loss: 0.00002746
Iteration 6/1000 | Loss: 0.00002629
Iteration 7/1000 | Loss: 0.00002556
Iteration 8/1000 | Loss: 0.00002508
Iteration 9/1000 | Loss: 0.00002459
Iteration 10/1000 | Loss: 0.00002434
Iteration 11/1000 | Loss: 0.00002412
Iteration 12/1000 | Loss: 0.00002390
Iteration 13/1000 | Loss: 0.00002367
Iteration 14/1000 | Loss: 0.00002364
Iteration 15/1000 | Loss: 0.00002364
Iteration 16/1000 | Loss: 0.00002349
Iteration 17/1000 | Loss: 0.00002343
Iteration 18/1000 | Loss: 0.00002342
Iteration 19/1000 | Loss: 0.00002341
Iteration 20/1000 | Loss: 0.00002337
Iteration 21/1000 | Loss: 0.00002337
Iteration 22/1000 | Loss: 0.00002337
Iteration 23/1000 | Loss: 0.00002336
Iteration 24/1000 | Loss: 0.00002335
Iteration 25/1000 | Loss: 0.00002334
Iteration 26/1000 | Loss: 0.00002332
Iteration 27/1000 | Loss: 0.00002328
Iteration 28/1000 | Loss: 0.00002328
Iteration 29/1000 | Loss: 0.00002328
Iteration 30/1000 | Loss: 0.00002328
Iteration 31/1000 | Loss: 0.00002327
Iteration 32/1000 | Loss: 0.00002327
Iteration 33/1000 | Loss: 0.00002327
Iteration 34/1000 | Loss: 0.00002327
Iteration 35/1000 | Loss: 0.00002327
Iteration 36/1000 | Loss: 0.00002327
Iteration 37/1000 | Loss: 0.00002327
Iteration 38/1000 | Loss: 0.00002327
Iteration 39/1000 | Loss: 0.00002325
Iteration 40/1000 | Loss: 0.00002325
Iteration 41/1000 | Loss: 0.00002324
Iteration 42/1000 | Loss: 0.00002324
Iteration 43/1000 | Loss: 0.00002324
Iteration 44/1000 | Loss: 0.00002324
Iteration 45/1000 | Loss: 0.00002324
Iteration 46/1000 | Loss: 0.00002323
Iteration 47/1000 | Loss: 0.00002321
Iteration 48/1000 | Loss: 0.00002321
Iteration 49/1000 | Loss: 0.00002321
Iteration 50/1000 | Loss: 0.00002321
Iteration 51/1000 | Loss: 0.00002320
Iteration 52/1000 | Loss: 0.00002320
Iteration 53/1000 | Loss: 0.00002320
Iteration 54/1000 | Loss: 0.00002319
Iteration 55/1000 | Loss: 0.00002319
Iteration 56/1000 | Loss: 0.00002319
Iteration 57/1000 | Loss: 0.00002319
Iteration 58/1000 | Loss: 0.00002319
Iteration 59/1000 | Loss: 0.00002319
Iteration 60/1000 | Loss: 0.00002319
Iteration 61/1000 | Loss: 0.00002318
Iteration 62/1000 | Loss: 0.00002318
Iteration 63/1000 | Loss: 0.00002318
Iteration 64/1000 | Loss: 0.00002318
Iteration 65/1000 | Loss: 0.00002318
Iteration 66/1000 | Loss: 0.00002318
Iteration 67/1000 | Loss: 0.00002318
Iteration 68/1000 | Loss: 0.00002318
Iteration 69/1000 | Loss: 0.00002318
Iteration 70/1000 | Loss: 0.00002318
Iteration 71/1000 | Loss: 0.00002318
Iteration 72/1000 | Loss: 0.00002317
Iteration 73/1000 | Loss: 0.00002317
Iteration 74/1000 | Loss: 0.00002317
Iteration 75/1000 | Loss: 0.00002317
Iteration 76/1000 | Loss: 0.00002317
Iteration 77/1000 | Loss: 0.00002317
Iteration 78/1000 | Loss: 0.00002317
Iteration 79/1000 | Loss: 0.00002316
Iteration 80/1000 | Loss: 0.00002316
Iteration 81/1000 | Loss: 0.00002316
Iteration 82/1000 | Loss: 0.00002315
Iteration 83/1000 | Loss: 0.00002315
Iteration 84/1000 | Loss: 0.00002315
Iteration 85/1000 | Loss: 0.00002315
Iteration 86/1000 | Loss: 0.00002315
Iteration 87/1000 | Loss: 0.00002315
Iteration 88/1000 | Loss: 0.00002314
Iteration 89/1000 | Loss: 0.00002314
Iteration 90/1000 | Loss: 0.00002314
Iteration 91/1000 | Loss: 0.00002314
Iteration 92/1000 | Loss: 0.00002314
Iteration 93/1000 | Loss: 0.00002314
Iteration 94/1000 | Loss: 0.00002314
Iteration 95/1000 | Loss: 0.00002314
Iteration 96/1000 | Loss: 0.00002314
Iteration 97/1000 | Loss: 0.00002314
Iteration 98/1000 | Loss: 0.00002313
Iteration 99/1000 | Loss: 0.00002313
Iteration 100/1000 | Loss: 0.00002313
Iteration 101/1000 | Loss: 0.00002313
Iteration 102/1000 | Loss: 0.00002313
Iteration 103/1000 | Loss: 0.00002313
Iteration 104/1000 | Loss: 0.00002313
Iteration 105/1000 | Loss: 0.00002313
Iteration 106/1000 | Loss: 0.00002313
Iteration 107/1000 | Loss: 0.00002313
Iteration 108/1000 | Loss: 0.00002313
Iteration 109/1000 | Loss: 0.00002313
Iteration 110/1000 | Loss: 0.00002313
Iteration 111/1000 | Loss: 0.00002313
Iteration 112/1000 | Loss: 0.00002313
Iteration 113/1000 | Loss: 0.00002313
Iteration 114/1000 | Loss: 0.00002313
Iteration 115/1000 | Loss: 0.00002313
Iteration 116/1000 | Loss: 0.00002313
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [2.3131273337639868e-05, 2.3131273337639868e-05, 2.3131273337639868e-05, 2.3131273337639868e-05, 2.3131273337639868e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3131273337639868e-05

Optimization complete. Final v2v error: 4.0166215896606445 mm

Highest mean error: 4.208961486816406 mm for frame 72

Lowest mean error: 3.7900984287261963 mm for frame 32

Saving results

Total time: 34.909055948257446
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00802483
Iteration 2/25 | Loss: 0.00139322
Iteration 3/25 | Loss: 0.00128177
Iteration 4/25 | Loss: 0.00127064
Iteration 5/25 | Loss: 0.00126812
Iteration 6/25 | Loss: 0.00126796
Iteration 7/25 | Loss: 0.00126796
Iteration 8/25 | Loss: 0.00126796
Iteration 9/25 | Loss: 0.00126796
Iteration 10/25 | Loss: 0.00126796
Iteration 11/25 | Loss: 0.00126796
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012679569190368056, 0.0012679569190368056, 0.0012679569190368056, 0.0012679569190368056, 0.0012679569190368056]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012679569190368056

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39577484
Iteration 2/25 | Loss: 0.00082786
Iteration 3/25 | Loss: 0.00082786
Iteration 4/25 | Loss: 0.00082786
Iteration 5/25 | Loss: 0.00082786
Iteration 6/25 | Loss: 0.00082785
Iteration 7/25 | Loss: 0.00082785
Iteration 8/25 | Loss: 0.00082785
Iteration 9/25 | Loss: 0.00082785
Iteration 10/25 | Loss: 0.00082785
Iteration 11/25 | Loss: 0.00082785
Iteration 12/25 | Loss: 0.00082785
Iteration 13/25 | Loss: 0.00082785
Iteration 14/25 | Loss: 0.00082785
Iteration 15/25 | Loss: 0.00082785
Iteration 16/25 | Loss: 0.00082785
Iteration 17/25 | Loss: 0.00082785
Iteration 18/25 | Loss: 0.00082785
Iteration 19/25 | Loss: 0.00082785
Iteration 20/25 | Loss: 0.00082785
Iteration 21/25 | Loss: 0.00082785
Iteration 22/25 | Loss: 0.00082785
Iteration 23/25 | Loss: 0.00082785
Iteration 24/25 | Loss: 0.00082785
Iteration 25/25 | Loss: 0.00082785

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082785
Iteration 2/1000 | Loss: 0.00002772
Iteration 3/1000 | Loss: 0.00001879
Iteration 4/1000 | Loss: 0.00001674
Iteration 5/1000 | Loss: 0.00001562
Iteration 6/1000 | Loss: 0.00001481
Iteration 7/1000 | Loss: 0.00001413
Iteration 8/1000 | Loss: 0.00001380
Iteration 9/1000 | Loss: 0.00001368
Iteration 10/1000 | Loss: 0.00001349
Iteration 11/1000 | Loss: 0.00001323
Iteration 12/1000 | Loss: 0.00001317
Iteration 13/1000 | Loss: 0.00001309
Iteration 14/1000 | Loss: 0.00001309
Iteration 15/1000 | Loss: 0.00001307
Iteration 16/1000 | Loss: 0.00001307
Iteration 17/1000 | Loss: 0.00001306
Iteration 18/1000 | Loss: 0.00001305
Iteration 19/1000 | Loss: 0.00001304
Iteration 20/1000 | Loss: 0.00001303
Iteration 21/1000 | Loss: 0.00001302
Iteration 22/1000 | Loss: 0.00001295
Iteration 23/1000 | Loss: 0.00001294
Iteration 24/1000 | Loss: 0.00001293
Iteration 25/1000 | Loss: 0.00001292
Iteration 26/1000 | Loss: 0.00001291
Iteration 27/1000 | Loss: 0.00001286
Iteration 28/1000 | Loss: 0.00001282
Iteration 29/1000 | Loss: 0.00001281
Iteration 30/1000 | Loss: 0.00001280
Iteration 31/1000 | Loss: 0.00001280
Iteration 32/1000 | Loss: 0.00001279
Iteration 33/1000 | Loss: 0.00001278
Iteration 34/1000 | Loss: 0.00001278
Iteration 35/1000 | Loss: 0.00001277
Iteration 36/1000 | Loss: 0.00001275
Iteration 37/1000 | Loss: 0.00001275
Iteration 38/1000 | Loss: 0.00001274
Iteration 39/1000 | Loss: 0.00001274
Iteration 40/1000 | Loss: 0.00001273
Iteration 41/1000 | Loss: 0.00001272
Iteration 42/1000 | Loss: 0.00001271
Iteration 43/1000 | Loss: 0.00001269
Iteration 44/1000 | Loss: 0.00001269
Iteration 45/1000 | Loss: 0.00001269
Iteration 46/1000 | Loss: 0.00001269
Iteration 47/1000 | Loss: 0.00001269
Iteration 48/1000 | Loss: 0.00001269
Iteration 49/1000 | Loss: 0.00001269
Iteration 50/1000 | Loss: 0.00001269
Iteration 51/1000 | Loss: 0.00001269
Iteration 52/1000 | Loss: 0.00001268
Iteration 53/1000 | Loss: 0.00001268
Iteration 54/1000 | Loss: 0.00001264
Iteration 55/1000 | Loss: 0.00001264
Iteration 56/1000 | Loss: 0.00001263
Iteration 57/1000 | Loss: 0.00001262
Iteration 58/1000 | Loss: 0.00001262
Iteration 59/1000 | Loss: 0.00001261
Iteration 60/1000 | Loss: 0.00001258
Iteration 61/1000 | Loss: 0.00001258
Iteration 62/1000 | Loss: 0.00001255
Iteration 63/1000 | Loss: 0.00001252
Iteration 64/1000 | Loss: 0.00001251
Iteration 65/1000 | Loss: 0.00001251
Iteration 66/1000 | Loss: 0.00001251
Iteration 67/1000 | Loss: 0.00001249
Iteration 68/1000 | Loss: 0.00001247
Iteration 69/1000 | Loss: 0.00001247
Iteration 70/1000 | Loss: 0.00001247
Iteration 71/1000 | Loss: 0.00001246
Iteration 72/1000 | Loss: 0.00001245
Iteration 73/1000 | Loss: 0.00001245
Iteration 74/1000 | Loss: 0.00001244
Iteration 75/1000 | Loss: 0.00001244
Iteration 76/1000 | Loss: 0.00001244
Iteration 77/1000 | Loss: 0.00001243
Iteration 78/1000 | Loss: 0.00001243
Iteration 79/1000 | Loss: 0.00001243
Iteration 80/1000 | Loss: 0.00001243
Iteration 81/1000 | Loss: 0.00001243
Iteration 82/1000 | Loss: 0.00001243
Iteration 83/1000 | Loss: 0.00001242
Iteration 84/1000 | Loss: 0.00001242
Iteration 85/1000 | Loss: 0.00001242
Iteration 86/1000 | Loss: 0.00001241
Iteration 87/1000 | Loss: 0.00001241
Iteration 88/1000 | Loss: 0.00001241
Iteration 89/1000 | Loss: 0.00001241
Iteration 90/1000 | Loss: 0.00001241
Iteration 91/1000 | Loss: 0.00001241
Iteration 92/1000 | Loss: 0.00001241
Iteration 93/1000 | Loss: 0.00001241
Iteration 94/1000 | Loss: 0.00001241
Iteration 95/1000 | Loss: 0.00001241
Iteration 96/1000 | Loss: 0.00001241
Iteration 97/1000 | Loss: 0.00001241
Iteration 98/1000 | Loss: 0.00001241
Iteration 99/1000 | Loss: 0.00001241
Iteration 100/1000 | Loss: 0.00001241
Iteration 101/1000 | Loss: 0.00001241
Iteration 102/1000 | Loss: 0.00001241
Iteration 103/1000 | Loss: 0.00001241
Iteration 104/1000 | Loss: 0.00001241
Iteration 105/1000 | Loss: 0.00001241
Iteration 106/1000 | Loss: 0.00001241
Iteration 107/1000 | Loss: 0.00001241
Iteration 108/1000 | Loss: 0.00001241
Iteration 109/1000 | Loss: 0.00001241
Iteration 110/1000 | Loss: 0.00001241
Iteration 111/1000 | Loss: 0.00001241
Iteration 112/1000 | Loss: 0.00001241
Iteration 113/1000 | Loss: 0.00001241
Iteration 114/1000 | Loss: 0.00001241
Iteration 115/1000 | Loss: 0.00001241
Iteration 116/1000 | Loss: 0.00001241
Iteration 117/1000 | Loss: 0.00001241
Iteration 118/1000 | Loss: 0.00001241
Iteration 119/1000 | Loss: 0.00001241
Iteration 120/1000 | Loss: 0.00001241
Iteration 121/1000 | Loss: 0.00001241
Iteration 122/1000 | Loss: 0.00001241
Iteration 123/1000 | Loss: 0.00001241
Iteration 124/1000 | Loss: 0.00001241
Iteration 125/1000 | Loss: 0.00001241
Iteration 126/1000 | Loss: 0.00001241
Iteration 127/1000 | Loss: 0.00001241
Iteration 128/1000 | Loss: 0.00001241
Iteration 129/1000 | Loss: 0.00001241
Iteration 130/1000 | Loss: 0.00001241
Iteration 131/1000 | Loss: 0.00001241
Iteration 132/1000 | Loss: 0.00001241
Iteration 133/1000 | Loss: 0.00001241
Iteration 134/1000 | Loss: 0.00001241
Iteration 135/1000 | Loss: 0.00001241
Iteration 136/1000 | Loss: 0.00001241
Iteration 137/1000 | Loss: 0.00001241
Iteration 138/1000 | Loss: 0.00001241
Iteration 139/1000 | Loss: 0.00001241
Iteration 140/1000 | Loss: 0.00001241
Iteration 141/1000 | Loss: 0.00001241
Iteration 142/1000 | Loss: 0.00001241
Iteration 143/1000 | Loss: 0.00001241
Iteration 144/1000 | Loss: 0.00001241
Iteration 145/1000 | Loss: 0.00001241
Iteration 146/1000 | Loss: 0.00001241
Iteration 147/1000 | Loss: 0.00001241
Iteration 148/1000 | Loss: 0.00001241
Iteration 149/1000 | Loss: 0.00001241
Iteration 150/1000 | Loss: 0.00001241
Iteration 151/1000 | Loss: 0.00001241
Iteration 152/1000 | Loss: 0.00001241
Iteration 153/1000 | Loss: 0.00001241
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.2406673704390414e-05, 1.2406673704390414e-05, 1.2406673704390414e-05, 1.2406673704390414e-05, 1.2406673704390414e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2406673704390414e-05

Optimization complete. Final v2v error: 3.006899118423462 mm

Highest mean error: 3.1980817317962646 mm for frame 57

Lowest mean error: 2.8354530334472656 mm for frame 172

Saving results

Total time: 36.020395278930664
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00857107
Iteration 2/25 | Loss: 0.00190130
Iteration 3/25 | Loss: 0.00147026
Iteration 4/25 | Loss: 0.00140478
Iteration 5/25 | Loss: 0.00143653
Iteration 6/25 | Loss: 0.00140554
Iteration 7/25 | Loss: 0.00135972
Iteration 8/25 | Loss: 0.00133945
Iteration 9/25 | Loss: 0.00132381
Iteration 10/25 | Loss: 0.00131883
Iteration 11/25 | Loss: 0.00131769
Iteration 12/25 | Loss: 0.00131734
Iteration 13/25 | Loss: 0.00132106
Iteration 14/25 | Loss: 0.00131726
Iteration 15/25 | Loss: 0.00131726
Iteration 16/25 | Loss: 0.00131726
Iteration 17/25 | Loss: 0.00131726
Iteration 18/25 | Loss: 0.00131726
Iteration 19/25 | Loss: 0.00131726
Iteration 20/25 | Loss: 0.00131726
Iteration 21/25 | Loss: 0.00131726
Iteration 22/25 | Loss: 0.00131726
Iteration 23/25 | Loss: 0.00131725
Iteration 24/25 | Loss: 0.00131725
Iteration 25/25 | Loss: 0.00131725

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47681963
Iteration 2/25 | Loss: 0.00092768
Iteration 3/25 | Loss: 0.00092767
Iteration 4/25 | Loss: 0.00092767
Iteration 5/25 | Loss: 0.00092767
Iteration 6/25 | Loss: 0.00092767
Iteration 7/25 | Loss: 0.00092767
Iteration 8/25 | Loss: 0.00092767
Iteration 9/25 | Loss: 0.00092767
Iteration 10/25 | Loss: 0.00092767
Iteration 11/25 | Loss: 0.00092767
Iteration 12/25 | Loss: 0.00092767
Iteration 13/25 | Loss: 0.00092767
Iteration 14/25 | Loss: 0.00092767
Iteration 15/25 | Loss: 0.00092767
Iteration 16/25 | Loss: 0.00092767
Iteration 17/25 | Loss: 0.00092767
Iteration 18/25 | Loss: 0.00092767
Iteration 19/25 | Loss: 0.00092767
Iteration 20/25 | Loss: 0.00092767
Iteration 21/25 | Loss: 0.00092767
Iteration 22/25 | Loss: 0.00092767
Iteration 23/25 | Loss: 0.00092767
Iteration 24/25 | Loss: 0.00092767
Iteration 25/25 | Loss: 0.00092767

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092767
Iteration 2/1000 | Loss: 0.00004371
Iteration 3/1000 | Loss: 0.00011612
Iteration 4/1000 | Loss: 0.00002382
Iteration 5/1000 | Loss: 0.00002254
Iteration 6/1000 | Loss: 0.00002139
Iteration 7/1000 | Loss: 0.00007912
Iteration 8/1000 | Loss: 0.00002067
Iteration 9/1000 | Loss: 0.00002006
Iteration 10/1000 | Loss: 0.00001952
Iteration 11/1000 | Loss: 0.00001926
Iteration 12/1000 | Loss: 0.00001900
Iteration 13/1000 | Loss: 0.00001875
Iteration 14/1000 | Loss: 0.00001854
Iteration 15/1000 | Loss: 0.00009438
Iteration 16/1000 | Loss: 0.00001845
Iteration 17/1000 | Loss: 0.00001833
Iteration 18/1000 | Loss: 0.00001831
Iteration 19/1000 | Loss: 0.00001828
Iteration 20/1000 | Loss: 0.00001824
Iteration 21/1000 | Loss: 0.00001824
Iteration 22/1000 | Loss: 0.00001819
Iteration 23/1000 | Loss: 0.00001818
Iteration 24/1000 | Loss: 0.00001818
Iteration 25/1000 | Loss: 0.00001815
Iteration 26/1000 | Loss: 0.00008242
Iteration 27/1000 | Loss: 0.00001864
Iteration 28/1000 | Loss: 0.00001797
Iteration 29/1000 | Loss: 0.00001796
Iteration 30/1000 | Loss: 0.00001796
Iteration 31/1000 | Loss: 0.00001796
Iteration 32/1000 | Loss: 0.00001795
Iteration 33/1000 | Loss: 0.00001795
Iteration 34/1000 | Loss: 0.00001795
Iteration 35/1000 | Loss: 0.00001795
Iteration 36/1000 | Loss: 0.00001794
Iteration 37/1000 | Loss: 0.00001794
Iteration 38/1000 | Loss: 0.00001794
Iteration 39/1000 | Loss: 0.00001794
Iteration 40/1000 | Loss: 0.00001794
Iteration 41/1000 | Loss: 0.00001794
Iteration 42/1000 | Loss: 0.00001794
Iteration 43/1000 | Loss: 0.00001794
Iteration 44/1000 | Loss: 0.00001793
Iteration 45/1000 | Loss: 0.00001793
Iteration 46/1000 | Loss: 0.00001793
Iteration 47/1000 | Loss: 0.00001793
Iteration 48/1000 | Loss: 0.00001793
Iteration 49/1000 | Loss: 0.00001793
Iteration 50/1000 | Loss: 0.00001793
Iteration 51/1000 | Loss: 0.00001793
Iteration 52/1000 | Loss: 0.00001792
Iteration 53/1000 | Loss: 0.00001792
Iteration 54/1000 | Loss: 0.00001792
Iteration 55/1000 | Loss: 0.00001792
Iteration 56/1000 | Loss: 0.00001792
Iteration 57/1000 | Loss: 0.00001791
Iteration 58/1000 | Loss: 0.00001791
Iteration 59/1000 | Loss: 0.00001791
Iteration 60/1000 | Loss: 0.00001791
Iteration 61/1000 | Loss: 0.00001791
Iteration 62/1000 | Loss: 0.00001791
Iteration 63/1000 | Loss: 0.00001790
Iteration 64/1000 | Loss: 0.00001790
Iteration 65/1000 | Loss: 0.00001790
Iteration 66/1000 | Loss: 0.00001790
Iteration 67/1000 | Loss: 0.00001789
Iteration 68/1000 | Loss: 0.00001789
Iteration 69/1000 | Loss: 0.00001789
Iteration 70/1000 | Loss: 0.00001789
Iteration 71/1000 | Loss: 0.00001788
Iteration 72/1000 | Loss: 0.00001788
Iteration 73/1000 | Loss: 0.00001787
Iteration 74/1000 | Loss: 0.00001787
Iteration 75/1000 | Loss: 0.00001787
Iteration 76/1000 | Loss: 0.00008555
Iteration 77/1000 | Loss: 0.00006703
Iteration 78/1000 | Loss: 0.00002921
Iteration 79/1000 | Loss: 0.00001791
Iteration 80/1000 | Loss: 0.00001788
Iteration 81/1000 | Loss: 0.00001787
Iteration 82/1000 | Loss: 0.00003058
Iteration 83/1000 | Loss: 0.00001783
Iteration 84/1000 | Loss: 0.00001782
Iteration 85/1000 | Loss: 0.00001782
Iteration 86/1000 | Loss: 0.00001782
Iteration 87/1000 | Loss: 0.00001782
Iteration 88/1000 | Loss: 0.00001782
Iteration 89/1000 | Loss: 0.00001781
Iteration 90/1000 | Loss: 0.00001781
Iteration 91/1000 | Loss: 0.00001781
Iteration 92/1000 | Loss: 0.00001781
Iteration 93/1000 | Loss: 0.00001781
Iteration 94/1000 | Loss: 0.00001781
Iteration 95/1000 | Loss: 0.00001781
Iteration 96/1000 | Loss: 0.00001781
Iteration 97/1000 | Loss: 0.00001781
Iteration 98/1000 | Loss: 0.00001781
Iteration 99/1000 | Loss: 0.00001781
Iteration 100/1000 | Loss: 0.00001781
Iteration 101/1000 | Loss: 0.00001781
Iteration 102/1000 | Loss: 0.00001781
Iteration 103/1000 | Loss: 0.00001781
Iteration 104/1000 | Loss: 0.00001781
Iteration 105/1000 | Loss: 0.00001781
Iteration 106/1000 | Loss: 0.00001781
Iteration 107/1000 | Loss: 0.00001781
Iteration 108/1000 | Loss: 0.00001781
Iteration 109/1000 | Loss: 0.00001781
Iteration 110/1000 | Loss: 0.00001781
Iteration 111/1000 | Loss: 0.00001781
Iteration 112/1000 | Loss: 0.00001781
Iteration 113/1000 | Loss: 0.00001781
Iteration 114/1000 | Loss: 0.00001781
Iteration 115/1000 | Loss: 0.00001781
Iteration 116/1000 | Loss: 0.00001781
Iteration 117/1000 | Loss: 0.00001781
Iteration 118/1000 | Loss: 0.00001781
Iteration 119/1000 | Loss: 0.00001781
Iteration 120/1000 | Loss: 0.00001781
Iteration 121/1000 | Loss: 0.00001781
Iteration 122/1000 | Loss: 0.00001781
Iteration 123/1000 | Loss: 0.00001781
Iteration 124/1000 | Loss: 0.00001781
Iteration 125/1000 | Loss: 0.00001781
Iteration 126/1000 | Loss: 0.00001781
Iteration 127/1000 | Loss: 0.00001781
Iteration 128/1000 | Loss: 0.00001781
Iteration 129/1000 | Loss: 0.00001781
Iteration 130/1000 | Loss: 0.00001781
Iteration 131/1000 | Loss: 0.00001781
Iteration 132/1000 | Loss: 0.00001781
Iteration 133/1000 | Loss: 0.00001781
Iteration 134/1000 | Loss: 0.00001781
Iteration 135/1000 | Loss: 0.00001781
Iteration 136/1000 | Loss: 0.00001781
Iteration 137/1000 | Loss: 0.00001781
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.7809998098528013e-05, 1.7809998098528013e-05, 1.7809998098528013e-05, 1.7809998098528013e-05, 1.7809998098528013e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7809998098528013e-05

Optimization complete. Final v2v error: 3.612569808959961 mm

Highest mean error: 4.3591156005859375 mm for frame 73

Lowest mean error: 3.308532238006592 mm for frame 113

Saving results

Total time: 62.11521339416504
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00850373
Iteration 2/25 | Loss: 0.00192830
Iteration 3/25 | Loss: 0.00168458
Iteration 4/25 | Loss: 0.00164496
Iteration 5/25 | Loss: 0.00164934
Iteration 6/25 | Loss: 0.00165132
Iteration 7/25 | Loss: 0.00163678
Iteration 8/25 | Loss: 0.00163268
Iteration 9/25 | Loss: 0.00162727
Iteration 10/25 | Loss: 0.00162669
Iteration 11/25 | Loss: 0.00161090
Iteration 12/25 | Loss: 0.00160761
Iteration 13/25 | Loss: 0.00160499
Iteration 14/25 | Loss: 0.00159870
Iteration 15/25 | Loss: 0.00159485
Iteration 16/25 | Loss: 0.00159491
Iteration 17/25 | Loss: 0.00159476
Iteration 18/25 | Loss: 0.00159267
Iteration 19/25 | Loss: 0.00159432
Iteration 20/25 | Loss: 0.00159372
Iteration 21/25 | Loss: 0.00159395
Iteration 22/25 | Loss: 0.00159621
Iteration 23/25 | Loss: 0.00159365
Iteration 24/25 | Loss: 0.00159394
Iteration 25/25 | Loss: 0.00159679

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26575899
Iteration 2/25 | Loss: 0.00268542
Iteration 3/25 | Loss: 0.00268535
Iteration 4/25 | Loss: 0.00268535
Iteration 5/25 | Loss: 0.00268535
Iteration 6/25 | Loss: 0.00268535
Iteration 7/25 | Loss: 0.00268535
Iteration 8/25 | Loss: 0.00268535
Iteration 9/25 | Loss: 0.00268535
Iteration 10/25 | Loss: 0.00268535
Iteration 11/25 | Loss: 0.00268535
Iteration 12/25 | Loss: 0.00268535
Iteration 13/25 | Loss: 0.00268535
Iteration 14/25 | Loss: 0.00268535
Iteration 15/25 | Loss: 0.00268535
Iteration 16/25 | Loss: 0.00268535
Iteration 17/25 | Loss: 0.00268535
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0026853461749851704, 0.0026853461749851704, 0.0026853461749851704, 0.0026853461749851704, 0.0026853461749851704]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026853461749851704

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00268535
Iteration 2/1000 | Loss: 0.00042167
Iteration 3/1000 | Loss: 0.00027761
Iteration 4/1000 | Loss: 0.00023478
Iteration 5/1000 | Loss: 0.00020770
Iteration 6/1000 | Loss: 0.00019169
Iteration 7/1000 | Loss: 0.00018189
Iteration 8/1000 | Loss: 0.00017398
Iteration 9/1000 | Loss: 0.00016664
Iteration 10/1000 | Loss: 0.00016269
Iteration 11/1000 | Loss: 0.00015746
Iteration 12/1000 | Loss: 0.00015342
Iteration 13/1000 | Loss: 0.00015039
Iteration 14/1000 | Loss: 0.00014823
Iteration 15/1000 | Loss: 0.00014684
Iteration 16/1000 | Loss: 0.00014566
Iteration 17/1000 | Loss: 0.00014481
Iteration 18/1000 | Loss: 0.00014428
Iteration 19/1000 | Loss: 0.00014365
Iteration 20/1000 | Loss: 0.00014296
Iteration 21/1000 | Loss: 0.00014242
Iteration 22/1000 | Loss: 0.00014192
Iteration 23/1000 | Loss: 0.00014151
Iteration 24/1000 | Loss: 0.00014117
Iteration 25/1000 | Loss: 0.00014086
Iteration 26/1000 | Loss: 0.00014059
Iteration 27/1000 | Loss: 0.00014038
Iteration 28/1000 | Loss: 0.00014022
Iteration 29/1000 | Loss: 0.00014019
Iteration 30/1000 | Loss: 0.00014017
Iteration 31/1000 | Loss: 0.00014017
Iteration 32/1000 | Loss: 0.00014016
Iteration 33/1000 | Loss: 0.00014014
Iteration 34/1000 | Loss: 0.00014012
Iteration 35/1000 | Loss: 0.00014012
Iteration 36/1000 | Loss: 0.00014006
Iteration 37/1000 | Loss: 0.00014005
Iteration 38/1000 | Loss: 0.00014005
Iteration 39/1000 | Loss: 0.00014002
Iteration 40/1000 | Loss: 0.00013999
Iteration 41/1000 | Loss: 0.00013998
Iteration 42/1000 | Loss: 0.00013995
Iteration 43/1000 | Loss: 0.00013994
Iteration 44/1000 | Loss: 0.00013994
Iteration 45/1000 | Loss: 0.00013993
Iteration 46/1000 | Loss: 0.00013993
Iteration 47/1000 | Loss: 0.00013993
Iteration 48/1000 | Loss: 0.00013993
Iteration 49/1000 | Loss: 0.00013993
Iteration 50/1000 | Loss: 0.00013993
Iteration 51/1000 | Loss: 0.00013993
Iteration 52/1000 | Loss: 0.00013993
Iteration 53/1000 | Loss: 0.00013993
Iteration 54/1000 | Loss: 0.00013993
Iteration 55/1000 | Loss: 0.00013992
Iteration 56/1000 | Loss: 0.00013992
Iteration 57/1000 | Loss: 0.00013992
Iteration 58/1000 | Loss: 0.00013992
Iteration 59/1000 | Loss: 0.00013992
Iteration 60/1000 | Loss: 0.00013991
Iteration 61/1000 | Loss: 0.00013991
Iteration 62/1000 | Loss: 0.00013991
Iteration 63/1000 | Loss: 0.00013991
Iteration 64/1000 | Loss: 0.00013991
Iteration 65/1000 | Loss: 0.00013991
Iteration 66/1000 | Loss: 0.00013990
Iteration 67/1000 | Loss: 0.00013990
Iteration 68/1000 | Loss: 0.00013990
Iteration 69/1000 | Loss: 0.00013990
Iteration 70/1000 | Loss: 0.00013990
Iteration 71/1000 | Loss: 0.00013990
Iteration 72/1000 | Loss: 0.00013990
Iteration 73/1000 | Loss: 0.00013990
Iteration 74/1000 | Loss: 0.00013990
Iteration 75/1000 | Loss: 0.00013990
Iteration 76/1000 | Loss: 0.00013989
Iteration 77/1000 | Loss: 0.00013989
Iteration 78/1000 | Loss: 0.00013989
Iteration 79/1000 | Loss: 0.00013989
Iteration 80/1000 | Loss: 0.00013989
Iteration 81/1000 | Loss: 0.00013989
Iteration 82/1000 | Loss: 0.00013989
Iteration 83/1000 | Loss: 0.00013989
Iteration 84/1000 | Loss: 0.00013989
Iteration 85/1000 | Loss: 0.00013989
Iteration 86/1000 | Loss: 0.00013989
Iteration 87/1000 | Loss: 0.00013988
Iteration 88/1000 | Loss: 0.00013988
Iteration 89/1000 | Loss: 0.00013988
Iteration 90/1000 | Loss: 0.00013987
Iteration 91/1000 | Loss: 0.00013987
Iteration 92/1000 | Loss: 0.00013987
Iteration 93/1000 | Loss: 0.00013987
Iteration 94/1000 | Loss: 0.00013987
Iteration 95/1000 | Loss: 0.00013986
Iteration 96/1000 | Loss: 0.00013986
Iteration 97/1000 | Loss: 0.00013986
Iteration 98/1000 | Loss: 0.00013985
Iteration 99/1000 | Loss: 0.00013985
Iteration 100/1000 | Loss: 0.00013985
Iteration 101/1000 | Loss: 0.00013985
Iteration 102/1000 | Loss: 0.00013985
Iteration 103/1000 | Loss: 0.00013985
Iteration 104/1000 | Loss: 0.00013985
Iteration 105/1000 | Loss: 0.00013984
Iteration 106/1000 | Loss: 0.00013984
Iteration 107/1000 | Loss: 0.00013984
Iteration 108/1000 | Loss: 0.00013984
Iteration 109/1000 | Loss: 0.00013984
Iteration 110/1000 | Loss: 0.00013984
Iteration 111/1000 | Loss: 0.00013984
Iteration 112/1000 | Loss: 0.00013984
Iteration 113/1000 | Loss: 0.00013984
Iteration 114/1000 | Loss: 0.00013984
Iteration 115/1000 | Loss: 0.00013984
Iteration 116/1000 | Loss: 0.00013984
Iteration 117/1000 | Loss: 0.00013983
Iteration 118/1000 | Loss: 0.00013983
Iteration 119/1000 | Loss: 0.00013983
Iteration 120/1000 | Loss: 0.00013983
Iteration 121/1000 | Loss: 0.00013983
Iteration 122/1000 | Loss: 0.00013983
Iteration 123/1000 | Loss: 0.00013983
Iteration 124/1000 | Loss: 0.00013982
Iteration 125/1000 | Loss: 0.00013982
Iteration 126/1000 | Loss: 0.00013982
Iteration 127/1000 | Loss: 0.00013982
Iteration 128/1000 | Loss: 0.00013982
Iteration 129/1000 | Loss: 0.00013982
Iteration 130/1000 | Loss: 0.00013982
Iteration 131/1000 | Loss: 0.00013982
Iteration 132/1000 | Loss: 0.00013982
Iteration 133/1000 | Loss: 0.00013982
Iteration 134/1000 | Loss: 0.00013982
Iteration 135/1000 | Loss: 0.00013982
Iteration 136/1000 | Loss: 0.00013981
Iteration 137/1000 | Loss: 0.00013981
Iteration 138/1000 | Loss: 0.00013981
Iteration 139/1000 | Loss: 0.00013981
Iteration 140/1000 | Loss: 0.00013981
Iteration 141/1000 | Loss: 0.00013981
Iteration 142/1000 | Loss: 0.00013980
Iteration 143/1000 | Loss: 0.00013980
Iteration 144/1000 | Loss: 0.00013980
Iteration 145/1000 | Loss: 0.00013980
Iteration 146/1000 | Loss: 0.00013980
Iteration 147/1000 | Loss: 0.00013980
Iteration 148/1000 | Loss: 0.00013980
Iteration 149/1000 | Loss: 0.00013980
Iteration 150/1000 | Loss: 0.00013980
Iteration 151/1000 | Loss: 0.00013980
Iteration 152/1000 | Loss: 0.00013980
Iteration 153/1000 | Loss: 0.00013980
Iteration 154/1000 | Loss: 0.00013980
Iteration 155/1000 | Loss: 0.00013980
Iteration 156/1000 | Loss: 0.00013980
Iteration 157/1000 | Loss: 0.00013980
Iteration 158/1000 | Loss: 0.00013980
Iteration 159/1000 | Loss: 0.00013980
Iteration 160/1000 | Loss: 0.00013980
Iteration 161/1000 | Loss: 0.00013980
Iteration 162/1000 | Loss: 0.00013980
Iteration 163/1000 | Loss: 0.00013980
Iteration 164/1000 | Loss: 0.00013980
Iteration 165/1000 | Loss: 0.00013980
Iteration 166/1000 | Loss: 0.00013980
Iteration 167/1000 | Loss: 0.00013980
Iteration 168/1000 | Loss: 0.00013980
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [0.00013979738287162036, 0.00013979738287162036, 0.00013979738287162036, 0.00013979738287162036, 0.00013979738287162036]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00013979738287162036

Optimization complete. Final v2v error: 6.427784442901611 mm

Highest mean error: 10.987878799438477 mm for frame 45

Lowest mean error: 4.523597717285156 mm for frame 10

Saving results

Total time: 92.86358380317688
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00767558
Iteration 2/25 | Loss: 0.00149077
Iteration 3/25 | Loss: 0.00132982
Iteration 4/25 | Loss: 0.00131254
Iteration 5/25 | Loss: 0.00131016
Iteration 6/25 | Loss: 0.00131016
Iteration 7/25 | Loss: 0.00131016
Iteration 8/25 | Loss: 0.00131016
Iteration 9/25 | Loss: 0.00131016
Iteration 10/25 | Loss: 0.00131016
Iteration 11/25 | Loss: 0.00131016
Iteration 12/25 | Loss: 0.00131016
Iteration 13/25 | Loss: 0.00131013
Iteration 14/25 | Loss: 0.00131013
Iteration 15/25 | Loss: 0.00131013
Iteration 16/25 | Loss: 0.00131013
Iteration 17/25 | Loss: 0.00131013
Iteration 18/25 | Loss: 0.00131013
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001310126157477498, 0.001310126157477498, 0.001310126157477498, 0.001310126157477498, 0.001310126157477498]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001310126157477498

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38674784
Iteration 2/25 | Loss: 0.00085519
Iteration 3/25 | Loss: 0.00085519
Iteration 4/25 | Loss: 0.00085519
Iteration 5/25 | Loss: 0.00085519
Iteration 6/25 | Loss: 0.00085519
Iteration 7/25 | Loss: 0.00085519
Iteration 8/25 | Loss: 0.00085519
Iteration 9/25 | Loss: 0.00085519
Iteration 10/25 | Loss: 0.00085519
Iteration 11/25 | Loss: 0.00085519
Iteration 12/25 | Loss: 0.00085519
Iteration 13/25 | Loss: 0.00085519
Iteration 14/25 | Loss: 0.00085519
Iteration 15/25 | Loss: 0.00085519
Iteration 16/25 | Loss: 0.00085519
Iteration 17/25 | Loss: 0.00085519
Iteration 18/25 | Loss: 0.00085519
Iteration 19/25 | Loss: 0.00085519
Iteration 20/25 | Loss: 0.00085519
Iteration 21/25 | Loss: 0.00085519
Iteration 22/25 | Loss: 0.00085519
Iteration 23/25 | Loss: 0.00085519
Iteration 24/25 | Loss: 0.00085519
Iteration 25/25 | Loss: 0.00085519

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085519
Iteration 2/1000 | Loss: 0.00003400
Iteration 3/1000 | Loss: 0.00002080
Iteration 4/1000 | Loss: 0.00001801
Iteration 5/1000 | Loss: 0.00001687
Iteration 6/1000 | Loss: 0.00001618
Iteration 7/1000 | Loss: 0.00001562
Iteration 8/1000 | Loss: 0.00001527
Iteration 9/1000 | Loss: 0.00001507
Iteration 10/1000 | Loss: 0.00001474
Iteration 11/1000 | Loss: 0.00001453
Iteration 12/1000 | Loss: 0.00001439
Iteration 13/1000 | Loss: 0.00001435
Iteration 14/1000 | Loss: 0.00001434
Iteration 15/1000 | Loss: 0.00001431
Iteration 16/1000 | Loss: 0.00001428
Iteration 17/1000 | Loss: 0.00001426
Iteration 18/1000 | Loss: 0.00001425
Iteration 19/1000 | Loss: 0.00001423
Iteration 20/1000 | Loss: 0.00001423
Iteration 21/1000 | Loss: 0.00001422
Iteration 22/1000 | Loss: 0.00001422
Iteration 23/1000 | Loss: 0.00001422
Iteration 24/1000 | Loss: 0.00001421
Iteration 25/1000 | Loss: 0.00001421
Iteration 26/1000 | Loss: 0.00001420
Iteration 27/1000 | Loss: 0.00001420
Iteration 28/1000 | Loss: 0.00001420
Iteration 29/1000 | Loss: 0.00001417
Iteration 30/1000 | Loss: 0.00001416
Iteration 31/1000 | Loss: 0.00001414
Iteration 32/1000 | Loss: 0.00001414
Iteration 33/1000 | Loss: 0.00001411
Iteration 34/1000 | Loss: 0.00001408
Iteration 35/1000 | Loss: 0.00001407
Iteration 36/1000 | Loss: 0.00001406
Iteration 37/1000 | Loss: 0.00001405
Iteration 38/1000 | Loss: 0.00001404
Iteration 39/1000 | Loss: 0.00001404
Iteration 40/1000 | Loss: 0.00001403
Iteration 41/1000 | Loss: 0.00001402
Iteration 42/1000 | Loss: 0.00001402
Iteration 43/1000 | Loss: 0.00001402
Iteration 44/1000 | Loss: 0.00001400
Iteration 45/1000 | Loss: 0.00001400
Iteration 46/1000 | Loss: 0.00001399
Iteration 47/1000 | Loss: 0.00001398
Iteration 48/1000 | Loss: 0.00001398
Iteration 49/1000 | Loss: 0.00001396
Iteration 50/1000 | Loss: 0.00001393
Iteration 51/1000 | Loss: 0.00001393
Iteration 52/1000 | Loss: 0.00001393
Iteration 53/1000 | Loss: 0.00001392
Iteration 54/1000 | Loss: 0.00001391
Iteration 55/1000 | Loss: 0.00001391
Iteration 56/1000 | Loss: 0.00001390
Iteration 57/1000 | Loss: 0.00001390
Iteration 58/1000 | Loss: 0.00001389
Iteration 59/1000 | Loss: 0.00001389
Iteration 60/1000 | Loss: 0.00001389
Iteration 61/1000 | Loss: 0.00001388
Iteration 62/1000 | Loss: 0.00001387
Iteration 63/1000 | Loss: 0.00001386
Iteration 64/1000 | Loss: 0.00001385
Iteration 65/1000 | Loss: 0.00001385
Iteration 66/1000 | Loss: 0.00001384
Iteration 67/1000 | Loss: 0.00001383
Iteration 68/1000 | Loss: 0.00001383
Iteration 69/1000 | Loss: 0.00001382
Iteration 70/1000 | Loss: 0.00001382
Iteration 71/1000 | Loss: 0.00001382
Iteration 72/1000 | Loss: 0.00001381
Iteration 73/1000 | Loss: 0.00001381
Iteration 74/1000 | Loss: 0.00001381
Iteration 75/1000 | Loss: 0.00001381
Iteration 76/1000 | Loss: 0.00001380
Iteration 77/1000 | Loss: 0.00001380
Iteration 78/1000 | Loss: 0.00001380
Iteration 79/1000 | Loss: 0.00001380
Iteration 80/1000 | Loss: 0.00001379
Iteration 81/1000 | Loss: 0.00001379
Iteration 82/1000 | Loss: 0.00001379
Iteration 83/1000 | Loss: 0.00001378
Iteration 84/1000 | Loss: 0.00001378
Iteration 85/1000 | Loss: 0.00001377
Iteration 86/1000 | Loss: 0.00001377
Iteration 87/1000 | Loss: 0.00001377
Iteration 88/1000 | Loss: 0.00001377
Iteration 89/1000 | Loss: 0.00001377
Iteration 90/1000 | Loss: 0.00001377
Iteration 91/1000 | Loss: 0.00001377
Iteration 92/1000 | Loss: 0.00001377
Iteration 93/1000 | Loss: 0.00001377
Iteration 94/1000 | Loss: 0.00001377
Iteration 95/1000 | Loss: 0.00001377
Iteration 96/1000 | Loss: 0.00001377
Iteration 97/1000 | Loss: 0.00001377
Iteration 98/1000 | Loss: 0.00001377
Iteration 99/1000 | Loss: 0.00001377
Iteration 100/1000 | Loss: 0.00001377
Iteration 101/1000 | Loss: 0.00001377
Iteration 102/1000 | Loss: 0.00001377
Iteration 103/1000 | Loss: 0.00001377
Iteration 104/1000 | Loss: 0.00001377
Iteration 105/1000 | Loss: 0.00001377
Iteration 106/1000 | Loss: 0.00001377
Iteration 107/1000 | Loss: 0.00001377
Iteration 108/1000 | Loss: 0.00001377
Iteration 109/1000 | Loss: 0.00001377
Iteration 110/1000 | Loss: 0.00001377
Iteration 111/1000 | Loss: 0.00001377
Iteration 112/1000 | Loss: 0.00001377
Iteration 113/1000 | Loss: 0.00001377
Iteration 114/1000 | Loss: 0.00001377
Iteration 115/1000 | Loss: 0.00001377
Iteration 116/1000 | Loss: 0.00001377
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [1.3766184565611184e-05, 1.3766184565611184e-05, 1.3766184565611184e-05, 1.3766184565611184e-05, 1.3766184565611184e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3766184565611184e-05

Optimization complete. Final v2v error: 3.172527551651001 mm

Highest mean error: 3.489563226699829 mm for frame 121

Lowest mean error: 2.9314591884613037 mm for frame 62

Saving results

Total time: 38.967965841293335
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00832204
Iteration 2/25 | Loss: 0.00163082
Iteration 3/25 | Loss: 0.00145847
Iteration 4/25 | Loss: 0.00144497
Iteration 5/25 | Loss: 0.00144301
Iteration 6/25 | Loss: 0.00144301
Iteration 7/25 | Loss: 0.00144301
Iteration 8/25 | Loss: 0.00144301
Iteration 9/25 | Loss: 0.00144301
Iteration 10/25 | Loss: 0.00144301
Iteration 11/25 | Loss: 0.00144301
Iteration 12/25 | Loss: 0.00144301
Iteration 13/25 | Loss: 0.00144301
Iteration 14/25 | Loss: 0.00144301
Iteration 15/25 | Loss: 0.00144301
Iteration 16/25 | Loss: 0.00144301
Iteration 17/25 | Loss: 0.00144301
Iteration 18/25 | Loss: 0.00144301
Iteration 19/25 | Loss: 0.00144301
Iteration 20/25 | Loss: 0.00144301
Iteration 21/25 | Loss: 0.00144301
Iteration 22/25 | Loss: 0.00144301
Iteration 23/25 | Loss: 0.00144301
Iteration 24/25 | Loss: 0.00144301
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0014430065639317036, 0.0014430065639317036, 0.0014430065639317036, 0.0014430065639317036, 0.0014430065639317036]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014430065639317036

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.00963604
Iteration 2/25 | Loss: 0.00075929
Iteration 3/25 | Loss: 0.00075928
Iteration 4/25 | Loss: 0.00075928
Iteration 5/25 | Loss: 0.00075928
Iteration 6/25 | Loss: 0.00075928
Iteration 7/25 | Loss: 0.00075928
Iteration 8/25 | Loss: 0.00075928
Iteration 9/25 | Loss: 0.00075928
Iteration 10/25 | Loss: 0.00075928
Iteration 11/25 | Loss: 0.00075928
Iteration 12/25 | Loss: 0.00075928
Iteration 13/25 | Loss: 0.00075928
Iteration 14/25 | Loss: 0.00075928
Iteration 15/25 | Loss: 0.00075928
Iteration 16/25 | Loss: 0.00075928
Iteration 17/25 | Loss: 0.00075928
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000759278773330152, 0.000759278773330152, 0.000759278773330152, 0.000759278773330152, 0.000759278773330152]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000759278773330152

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075928
Iteration 2/1000 | Loss: 0.00004582
Iteration 3/1000 | Loss: 0.00003410
Iteration 4/1000 | Loss: 0.00003065
Iteration 5/1000 | Loss: 0.00002925
Iteration 6/1000 | Loss: 0.00002850
Iteration 7/1000 | Loss: 0.00002786
Iteration 8/1000 | Loss: 0.00002749
Iteration 9/1000 | Loss: 0.00002720
Iteration 10/1000 | Loss: 0.00002688
Iteration 11/1000 | Loss: 0.00002658
Iteration 12/1000 | Loss: 0.00002627
Iteration 13/1000 | Loss: 0.00002598
Iteration 14/1000 | Loss: 0.00002584
Iteration 15/1000 | Loss: 0.00002579
Iteration 16/1000 | Loss: 0.00002563
Iteration 17/1000 | Loss: 0.00002558
Iteration 18/1000 | Loss: 0.00002558
Iteration 19/1000 | Loss: 0.00002557
Iteration 20/1000 | Loss: 0.00002557
Iteration 21/1000 | Loss: 0.00002547
Iteration 22/1000 | Loss: 0.00002546
Iteration 23/1000 | Loss: 0.00002545
Iteration 24/1000 | Loss: 0.00002545
Iteration 25/1000 | Loss: 0.00002545
Iteration 26/1000 | Loss: 0.00002544
Iteration 27/1000 | Loss: 0.00002544
Iteration 28/1000 | Loss: 0.00002543
Iteration 29/1000 | Loss: 0.00002543
Iteration 30/1000 | Loss: 0.00002543
Iteration 31/1000 | Loss: 0.00002542
Iteration 32/1000 | Loss: 0.00002542
Iteration 33/1000 | Loss: 0.00002542
Iteration 34/1000 | Loss: 0.00002541
Iteration 35/1000 | Loss: 0.00002541
Iteration 36/1000 | Loss: 0.00002541
Iteration 37/1000 | Loss: 0.00002540
Iteration 38/1000 | Loss: 0.00002538
Iteration 39/1000 | Loss: 0.00002538
Iteration 40/1000 | Loss: 0.00002537
Iteration 41/1000 | Loss: 0.00002537
Iteration 42/1000 | Loss: 0.00002536
Iteration 43/1000 | Loss: 0.00002536
Iteration 44/1000 | Loss: 0.00002532
Iteration 45/1000 | Loss: 0.00002532
Iteration 46/1000 | Loss: 0.00002531
Iteration 47/1000 | Loss: 0.00002531
Iteration 48/1000 | Loss: 0.00002531
Iteration 49/1000 | Loss: 0.00002530
Iteration 50/1000 | Loss: 0.00002530
Iteration 51/1000 | Loss: 0.00002530
Iteration 52/1000 | Loss: 0.00002530
Iteration 53/1000 | Loss: 0.00002530
Iteration 54/1000 | Loss: 0.00002530
Iteration 55/1000 | Loss: 0.00002530
Iteration 56/1000 | Loss: 0.00002530
Iteration 57/1000 | Loss: 0.00002529
Iteration 58/1000 | Loss: 0.00002529
Iteration 59/1000 | Loss: 0.00002529
Iteration 60/1000 | Loss: 0.00002529
Iteration 61/1000 | Loss: 0.00002529
Iteration 62/1000 | Loss: 0.00002528
Iteration 63/1000 | Loss: 0.00002528
Iteration 64/1000 | Loss: 0.00002528
Iteration 65/1000 | Loss: 0.00002528
Iteration 66/1000 | Loss: 0.00002528
Iteration 67/1000 | Loss: 0.00002528
Iteration 68/1000 | Loss: 0.00002528
Iteration 69/1000 | Loss: 0.00002528
Iteration 70/1000 | Loss: 0.00002528
Iteration 71/1000 | Loss: 0.00002528
Iteration 72/1000 | Loss: 0.00002528
Iteration 73/1000 | Loss: 0.00002528
Iteration 74/1000 | Loss: 0.00002527
Iteration 75/1000 | Loss: 0.00002527
Iteration 76/1000 | Loss: 0.00002527
Iteration 77/1000 | Loss: 0.00002527
Iteration 78/1000 | Loss: 0.00002527
Iteration 79/1000 | Loss: 0.00002527
Iteration 80/1000 | Loss: 0.00002527
Iteration 81/1000 | Loss: 0.00002527
Iteration 82/1000 | Loss: 0.00002527
Iteration 83/1000 | Loss: 0.00002527
Iteration 84/1000 | Loss: 0.00002527
Iteration 85/1000 | Loss: 0.00002527
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 85. Stopping optimization.
Last 5 losses: [2.5273888240917586e-05, 2.5273888240917586e-05, 2.5273888240917586e-05, 2.5273888240917586e-05, 2.5273888240917586e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5273888240917586e-05

Optimization complete. Final v2v error: 4.231968879699707 mm

Highest mean error: 4.280755996704102 mm for frame 128

Lowest mean error: 4.186119079589844 mm for frame 145

Saving results

Total time: 34.538798332214355
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01056824
Iteration 2/25 | Loss: 0.01056824
Iteration 3/25 | Loss: 0.00211557
Iteration 4/25 | Loss: 0.00160575
Iteration 5/25 | Loss: 0.00145465
Iteration 6/25 | Loss: 0.00142445
Iteration 7/25 | Loss: 0.00136366
Iteration 8/25 | Loss: 0.00134875
Iteration 9/25 | Loss: 0.00132695
Iteration 10/25 | Loss: 0.00131132
Iteration 11/25 | Loss: 0.00130536
Iteration 12/25 | Loss: 0.00130253
Iteration 13/25 | Loss: 0.00130148
Iteration 14/25 | Loss: 0.00130056
Iteration 15/25 | Loss: 0.00130023
Iteration 16/25 | Loss: 0.00130006
Iteration 17/25 | Loss: 0.00129992
Iteration 18/25 | Loss: 0.00129981
Iteration 19/25 | Loss: 0.00129980
Iteration 20/25 | Loss: 0.00129979
Iteration 21/25 | Loss: 0.00129978
Iteration 22/25 | Loss: 0.00129967
Iteration 23/25 | Loss: 0.00129927
Iteration 24/25 | Loss: 0.00129911
Iteration 25/25 | Loss: 0.00129906

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.54943180
Iteration 2/25 | Loss: 0.00092511
Iteration 3/25 | Loss: 0.00091280
Iteration 4/25 | Loss: 0.00086640
Iteration 5/25 | Loss: 0.00085391
Iteration 6/25 | Loss: 0.00085391
Iteration 7/25 | Loss: 0.00085391
Iteration 8/25 | Loss: 0.00085391
Iteration 9/25 | Loss: 0.00085391
Iteration 10/25 | Loss: 0.00085391
Iteration 11/25 | Loss: 0.00085391
Iteration 12/25 | Loss: 0.00085391
Iteration 13/25 | Loss: 0.00085391
Iteration 14/25 | Loss: 0.00085391
Iteration 15/25 | Loss: 0.00085391
Iteration 16/25 | Loss: 0.00085391
Iteration 17/25 | Loss: 0.00085391
Iteration 18/25 | Loss: 0.00085391
Iteration 19/25 | Loss: 0.00085391
Iteration 20/25 | Loss: 0.00085391
Iteration 21/25 | Loss: 0.00085391
Iteration 22/25 | Loss: 0.00085391
Iteration 23/25 | Loss: 0.00085391
Iteration 24/25 | Loss: 0.00085391
Iteration 25/25 | Loss: 0.00085391

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085391
Iteration 2/1000 | Loss: 0.00007737
Iteration 3/1000 | Loss: 0.00002258
Iteration 4/1000 | Loss: 0.00007151
Iteration 5/1000 | Loss: 0.00002020
Iteration 6/1000 | Loss: 0.00001944
Iteration 7/1000 | Loss: 0.00007929
Iteration 8/1000 | Loss: 0.00003499
Iteration 9/1000 | Loss: 0.00001898
Iteration 10/1000 | Loss: 0.00001815
Iteration 11/1000 | Loss: 0.00002712
Iteration 12/1000 | Loss: 0.00002014
Iteration 13/1000 | Loss: 0.00001754
Iteration 14/1000 | Loss: 0.00001844
Iteration 15/1000 | Loss: 0.00001729
Iteration 16/1000 | Loss: 0.00001720
Iteration 17/1000 | Loss: 0.00001704
Iteration 18/1000 | Loss: 0.00001704
Iteration 19/1000 | Loss: 0.00001696
Iteration 20/1000 | Loss: 0.00001694
Iteration 21/1000 | Loss: 0.00001693
Iteration 22/1000 | Loss: 0.00001692
Iteration 23/1000 | Loss: 0.00001691
Iteration 24/1000 | Loss: 0.00001690
Iteration 25/1000 | Loss: 0.00001690
Iteration 26/1000 | Loss: 0.00001690
Iteration 27/1000 | Loss: 0.00001690
Iteration 28/1000 | Loss: 0.00001690
Iteration 29/1000 | Loss: 0.00001690
Iteration 30/1000 | Loss: 0.00001690
Iteration 31/1000 | Loss: 0.00001690
Iteration 32/1000 | Loss: 0.00001689
Iteration 33/1000 | Loss: 0.00001689
Iteration 34/1000 | Loss: 0.00001689
Iteration 35/1000 | Loss: 0.00001686
Iteration 36/1000 | Loss: 0.00001686
Iteration 37/1000 | Loss: 0.00001685
Iteration 38/1000 | Loss: 0.00001685
Iteration 39/1000 | Loss: 0.00001684
Iteration 40/1000 | Loss: 0.00001680
Iteration 41/1000 | Loss: 0.00001680
Iteration 42/1000 | Loss: 0.00001679
Iteration 43/1000 | Loss: 0.00001678
Iteration 44/1000 | Loss: 0.00001678
Iteration 45/1000 | Loss: 0.00001677
Iteration 46/1000 | Loss: 0.00001676
Iteration 47/1000 | Loss: 0.00001676
Iteration 48/1000 | Loss: 0.00001675
Iteration 49/1000 | Loss: 0.00001675
Iteration 50/1000 | Loss: 0.00001673
Iteration 51/1000 | Loss: 0.00007607
Iteration 52/1000 | Loss: 0.00002564
Iteration 53/1000 | Loss: 0.00003323
Iteration 54/1000 | Loss: 0.00001665
Iteration 55/1000 | Loss: 0.00001664
Iteration 56/1000 | Loss: 0.00001664
Iteration 57/1000 | Loss: 0.00001664
Iteration 58/1000 | Loss: 0.00001664
Iteration 59/1000 | Loss: 0.00001663
Iteration 60/1000 | Loss: 0.00001662
Iteration 61/1000 | Loss: 0.00001662
Iteration 62/1000 | Loss: 0.00001662
Iteration 63/1000 | Loss: 0.00001662
Iteration 64/1000 | Loss: 0.00001662
Iteration 65/1000 | Loss: 0.00001662
Iteration 66/1000 | Loss: 0.00001662
Iteration 67/1000 | Loss: 0.00001662
Iteration 68/1000 | Loss: 0.00001661
Iteration 69/1000 | Loss: 0.00001661
Iteration 70/1000 | Loss: 0.00001661
Iteration 71/1000 | Loss: 0.00001659
Iteration 72/1000 | Loss: 0.00001659
Iteration 73/1000 | Loss: 0.00001659
Iteration 74/1000 | Loss: 0.00001658
Iteration 75/1000 | Loss: 0.00001658
Iteration 76/1000 | Loss: 0.00001658
Iteration 77/1000 | Loss: 0.00001658
Iteration 78/1000 | Loss: 0.00001657
Iteration 79/1000 | Loss: 0.00001657
Iteration 80/1000 | Loss: 0.00001657
Iteration 81/1000 | Loss: 0.00001656
Iteration 82/1000 | Loss: 0.00001656
Iteration 83/1000 | Loss: 0.00001656
Iteration 84/1000 | Loss: 0.00001656
Iteration 85/1000 | Loss: 0.00001655
Iteration 86/1000 | Loss: 0.00001655
Iteration 87/1000 | Loss: 0.00001655
Iteration 88/1000 | Loss: 0.00001655
Iteration 89/1000 | Loss: 0.00001655
Iteration 90/1000 | Loss: 0.00001655
Iteration 91/1000 | Loss: 0.00001655
Iteration 92/1000 | Loss: 0.00001654
Iteration 93/1000 | Loss: 0.00001654
Iteration 94/1000 | Loss: 0.00001654
Iteration 95/1000 | Loss: 0.00006814
Iteration 96/1000 | Loss: 0.00001779
Iteration 97/1000 | Loss: 0.00001662
Iteration 98/1000 | Loss: 0.00003108
Iteration 99/1000 | Loss: 0.00001662
Iteration 100/1000 | Loss: 0.00001657
Iteration 101/1000 | Loss: 0.00001655
Iteration 102/1000 | Loss: 0.00001654
Iteration 103/1000 | Loss: 0.00001654
Iteration 104/1000 | Loss: 0.00001654
Iteration 105/1000 | Loss: 0.00001654
Iteration 106/1000 | Loss: 0.00001654
Iteration 107/1000 | Loss: 0.00001654
Iteration 108/1000 | Loss: 0.00001653
Iteration 109/1000 | Loss: 0.00001653
Iteration 110/1000 | Loss: 0.00001653
Iteration 111/1000 | Loss: 0.00001653
Iteration 112/1000 | Loss: 0.00001652
Iteration 113/1000 | Loss: 0.00001652
Iteration 114/1000 | Loss: 0.00001652
Iteration 115/1000 | Loss: 0.00001652
Iteration 116/1000 | Loss: 0.00001651
Iteration 117/1000 | Loss: 0.00001651
Iteration 118/1000 | Loss: 0.00001651
Iteration 119/1000 | Loss: 0.00001651
Iteration 120/1000 | Loss: 0.00001651
Iteration 121/1000 | Loss: 0.00001651
Iteration 122/1000 | Loss: 0.00001651
Iteration 123/1000 | Loss: 0.00001650
Iteration 124/1000 | Loss: 0.00001650
Iteration 125/1000 | Loss: 0.00001650
Iteration 126/1000 | Loss: 0.00001650
Iteration 127/1000 | Loss: 0.00001650
Iteration 128/1000 | Loss: 0.00001650
Iteration 129/1000 | Loss: 0.00001650
Iteration 130/1000 | Loss: 0.00001650
Iteration 131/1000 | Loss: 0.00001649
Iteration 132/1000 | Loss: 0.00001649
Iteration 133/1000 | Loss: 0.00001649
Iteration 134/1000 | Loss: 0.00001649
Iteration 135/1000 | Loss: 0.00001649
Iteration 136/1000 | Loss: 0.00001649
Iteration 137/1000 | Loss: 0.00001649
Iteration 138/1000 | Loss: 0.00001649
Iteration 139/1000 | Loss: 0.00001648
Iteration 140/1000 | Loss: 0.00001648
Iteration 141/1000 | Loss: 0.00001648
Iteration 142/1000 | Loss: 0.00001648
Iteration 143/1000 | Loss: 0.00001648
Iteration 144/1000 | Loss: 0.00001648
Iteration 145/1000 | Loss: 0.00001648
Iteration 146/1000 | Loss: 0.00001648
Iteration 147/1000 | Loss: 0.00001648
Iteration 148/1000 | Loss: 0.00001648
Iteration 149/1000 | Loss: 0.00001648
Iteration 150/1000 | Loss: 0.00001648
Iteration 151/1000 | Loss: 0.00001647
Iteration 152/1000 | Loss: 0.00001647
Iteration 153/1000 | Loss: 0.00001647
Iteration 154/1000 | Loss: 0.00001647
Iteration 155/1000 | Loss: 0.00001647
Iteration 156/1000 | Loss: 0.00001647
Iteration 157/1000 | Loss: 0.00001647
Iteration 158/1000 | Loss: 0.00001647
Iteration 159/1000 | Loss: 0.00001647
Iteration 160/1000 | Loss: 0.00001647
Iteration 161/1000 | Loss: 0.00001647
Iteration 162/1000 | Loss: 0.00001647
Iteration 163/1000 | Loss: 0.00001647
Iteration 164/1000 | Loss: 0.00001647
Iteration 165/1000 | Loss: 0.00001647
Iteration 166/1000 | Loss: 0.00001647
Iteration 167/1000 | Loss: 0.00001647
Iteration 168/1000 | Loss: 0.00001647
Iteration 169/1000 | Loss: 0.00001647
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.6465712178614922e-05, 1.6465712178614922e-05, 1.6465712178614922e-05, 1.6465712178614922e-05, 1.6465712178614922e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6465712178614922e-05

Optimization complete. Final v2v error: 3.4369471073150635 mm

Highest mean error: 4.119321823120117 mm for frame 184

Lowest mean error: 3.1252686977386475 mm for frame 93

Saving results

Total time: 96.98592853546143
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00854469
Iteration 2/25 | Loss: 0.00153884
Iteration 3/25 | Loss: 0.00138176
Iteration 4/25 | Loss: 0.00136826
Iteration 5/25 | Loss: 0.00136587
Iteration 6/25 | Loss: 0.00136587
Iteration 7/25 | Loss: 0.00136587
Iteration 8/25 | Loss: 0.00136587
Iteration 9/25 | Loss: 0.00136587
Iteration 10/25 | Loss: 0.00136587
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013658691896125674, 0.0013658691896125674, 0.0013658691896125674, 0.0013658691896125674, 0.0013658691896125674]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013658691896125674

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40129566
Iteration 2/25 | Loss: 0.00088583
Iteration 3/25 | Loss: 0.00088582
Iteration 4/25 | Loss: 0.00088582
Iteration 5/25 | Loss: 0.00088582
Iteration 6/25 | Loss: 0.00088582
Iteration 7/25 | Loss: 0.00088582
Iteration 8/25 | Loss: 0.00088582
Iteration 9/25 | Loss: 0.00088582
Iteration 10/25 | Loss: 0.00088582
Iteration 11/25 | Loss: 0.00088582
Iteration 12/25 | Loss: 0.00088582
Iteration 13/25 | Loss: 0.00088582
Iteration 14/25 | Loss: 0.00088582
Iteration 15/25 | Loss: 0.00088582
Iteration 16/25 | Loss: 0.00088582
Iteration 17/25 | Loss: 0.00088582
Iteration 18/25 | Loss: 0.00088582
Iteration 19/25 | Loss: 0.00088582
Iteration 20/25 | Loss: 0.00088582
Iteration 21/25 | Loss: 0.00088582
Iteration 22/25 | Loss: 0.00088582
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008858164655975997, 0.0008858164655975997, 0.0008858164655975997, 0.0008858164655975997, 0.0008858164655975997]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008858164655975997

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088582
Iteration 2/1000 | Loss: 0.00003052
Iteration 3/1000 | Loss: 0.00002339
Iteration 4/1000 | Loss: 0.00002171
Iteration 5/1000 | Loss: 0.00002095
Iteration 6/1000 | Loss: 0.00002039
Iteration 7/1000 | Loss: 0.00002000
Iteration 8/1000 | Loss: 0.00001954
Iteration 9/1000 | Loss: 0.00001925
Iteration 10/1000 | Loss: 0.00001907
Iteration 11/1000 | Loss: 0.00001901
Iteration 12/1000 | Loss: 0.00001899
Iteration 13/1000 | Loss: 0.00001890
Iteration 14/1000 | Loss: 0.00001889
Iteration 15/1000 | Loss: 0.00001877
Iteration 16/1000 | Loss: 0.00001873
Iteration 17/1000 | Loss: 0.00001865
Iteration 18/1000 | Loss: 0.00001864
Iteration 19/1000 | Loss: 0.00001864
Iteration 20/1000 | Loss: 0.00001862
Iteration 21/1000 | Loss: 0.00001859
Iteration 22/1000 | Loss: 0.00001858
Iteration 23/1000 | Loss: 0.00001855
Iteration 24/1000 | Loss: 0.00001851
Iteration 25/1000 | Loss: 0.00001851
Iteration 26/1000 | Loss: 0.00001851
Iteration 27/1000 | Loss: 0.00001849
Iteration 28/1000 | Loss: 0.00001849
Iteration 29/1000 | Loss: 0.00001845
Iteration 30/1000 | Loss: 0.00001844
Iteration 31/1000 | Loss: 0.00001842
Iteration 32/1000 | Loss: 0.00001842
Iteration 33/1000 | Loss: 0.00001842
Iteration 34/1000 | Loss: 0.00001841
Iteration 35/1000 | Loss: 0.00001841
Iteration 36/1000 | Loss: 0.00001841
Iteration 37/1000 | Loss: 0.00001838
Iteration 38/1000 | Loss: 0.00001837
Iteration 39/1000 | Loss: 0.00001837
Iteration 40/1000 | Loss: 0.00001837
Iteration 41/1000 | Loss: 0.00001837
Iteration 42/1000 | Loss: 0.00001837
Iteration 43/1000 | Loss: 0.00001837
Iteration 44/1000 | Loss: 0.00001836
Iteration 45/1000 | Loss: 0.00001835
Iteration 46/1000 | Loss: 0.00001835
Iteration 47/1000 | Loss: 0.00001833
Iteration 48/1000 | Loss: 0.00001832
Iteration 49/1000 | Loss: 0.00001832
Iteration 50/1000 | Loss: 0.00001831
Iteration 51/1000 | Loss: 0.00001829
Iteration 52/1000 | Loss: 0.00001828
Iteration 53/1000 | Loss: 0.00001828
Iteration 54/1000 | Loss: 0.00001828
Iteration 55/1000 | Loss: 0.00001828
Iteration 56/1000 | Loss: 0.00001828
Iteration 57/1000 | Loss: 0.00001828
Iteration 58/1000 | Loss: 0.00001828
Iteration 59/1000 | Loss: 0.00001828
Iteration 60/1000 | Loss: 0.00001828
Iteration 61/1000 | Loss: 0.00001827
Iteration 62/1000 | Loss: 0.00001827
Iteration 63/1000 | Loss: 0.00001826
Iteration 64/1000 | Loss: 0.00001825
Iteration 65/1000 | Loss: 0.00001824
Iteration 66/1000 | Loss: 0.00001824
Iteration 67/1000 | Loss: 0.00001824
Iteration 68/1000 | Loss: 0.00001824
Iteration 69/1000 | Loss: 0.00001824
Iteration 70/1000 | Loss: 0.00001824
Iteration 71/1000 | Loss: 0.00001824
Iteration 72/1000 | Loss: 0.00001824
Iteration 73/1000 | Loss: 0.00001824
Iteration 74/1000 | Loss: 0.00001823
Iteration 75/1000 | Loss: 0.00001823
Iteration 76/1000 | Loss: 0.00001823
Iteration 77/1000 | Loss: 0.00001823
Iteration 78/1000 | Loss: 0.00001822
Iteration 79/1000 | Loss: 0.00001822
Iteration 80/1000 | Loss: 0.00001822
Iteration 81/1000 | Loss: 0.00001821
Iteration 82/1000 | Loss: 0.00001821
Iteration 83/1000 | Loss: 0.00001821
Iteration 84/1000 | Loss: 0.00001821
Iteration 85/1000 | Loss: 0.00001821
Iteration 86/1000 | Loss: 0.00001821
Iteration 87/1000 | Loss: 0.00001821
Iteration 88/1000 | Loss: 0.00001821
Iteration 89/1000 | Loss: 0.00001821
Iteration 90/1000 | Loss: 0.00001821
Iteration 91/1000 | Loss: 0.00001820
Iteration 92/1000 | Loss: 0.00001820
Iteration 93/1000 | Loss: 0.00001820
Iteration 94/1000 | Loss: 0.00001819
Iteration 95/1000 | Loss: 0.00001819
Iteration 96/1000 | Loss: 0.00001819
Iteration 97/1000 | Loss: 0.00001819
Iteration 98/1000 | Loss: 0.00001818
Iteration 99/1000 | Loss: 0.00001818
Iteration 100/1000 | Loss: 0.00001818
Iteration 101/1000 | Loss: 0.00001818
Iteration 102/1000 | Loss: 0.00001818
Iteration 103/1000 | Loss: 0.00001818
Iteration 104/1000 | Loss: 0.00001818
Iteration 105/1000 | Loss: 0.00001818
Iteration 106/1000 | Loss: 0.00001818
Iteration 107/1000 | Loss: 0.00001817
Iteration 108/1000 | Loss: 0.00001817
Iteration 109/1000 | Loss: 0.00001817
Iteration 110/1000 | Loss: 0.00001817
Iteration 111/1000 | Loss: 0.00001817
Iteration 112/1000 | Loss: 0.00001817
Iteration 113/1000 | Loss: 0.00001817
Iteration 114/1000 | Loss: 0.00001817
Iteration 115/1000 | Loss: 0.00001816
Iteration 116/1000 | Loss: 0.00001816
Iteration 117/1000 | Loss: 0.00001816
Iteration 118/1000 | Loss: 0.00001816
Iteration 119/1000 | Loss: 0.00001816
Iteration 120/1000 | Loss: 0.00001816
Iteration 121/1000 | Loss: 0.00001816
Iteration 122/1000 | Loss: 0.00001816
Iteration 123/1000 | Loss: 0.00001815
Iteration 124/1000 | Loss: 0.00001815
Iteration 125/1000 | Loss: 0.00001815
Iteration 126/1000 | Loss: 0.00001815
Iteration 127/1000 | Loss: 0.00001815
Iteration 128/1000 | Loss: 0.00001815
Iteration 129/1000 | Loss: 0.00001815
Iteration 130/1000 | Loss: 0.00001815
Iteration 131/1000 | Loss: 0.00001815
Iteration 132/1000 | Loss: 0.00001815
Iteration 133/1000 | Loss: 0.00001815
Iteration 134/1000 | Loss: 0.00001815
Iteration 135/1000 | Loss: 0.00001815
Iteration 136/1000 | Loss: 0.00001814
Iteration 137/1000 | Loss: 0.00001814
Iteration 138/1000 | Loss: 0.00001814
Iteration 139/1000 | Loss: 0.00001814
Iteration 140/1000 | Loss: 0.00001814
Iteration 141/1000 | Loss: 0.00001814
Iteration 142/1000 | Loss: 0.00001814
Iteration 143/1000 | Loss: 0.00001814
Iteration 144/1000 | Loss: 0.00001814
Iteration 145/1000 | Loss: 0.00001814
Iteration 146/1000 | Loss: 0.00001814
Iteration 147/1000 | Loss: 0.00001814
Iteration 148/1000 | Loss: 0.00001814
Iteration 149/1000 | Loss: 0.00001814
Iteration 150/1000 | Loss: 0.00001814
Iteration 151/1000 | Loss: 0.00001814
Iteration 152/1000 | Loss: 0.00001814
Iteration 153/1000 | Loss: 0.00001814
Iteration 154/1000 | Loss: 0.00001814
Iteration 155/1000 | Loss: 0.00001814
Iteration 156/1000 | Loss: 0.00001814
Iteration 157/1000 | Loss: 0.00001813
Iteration 158/1000 | Loss: 0.00001813
Iteration 159/1000 | Loss: 0.00001813
Iteration 160/1000 | Loss: 0.00001813
Iteration 161/1000 | Loss: 0.00001813
Iteration 162/1000 | Loss: 0.00001813
Iteration 163/1000 | Loss: 0.00001813
Iteration 164/1000 | Loss: 0.00001813
Iteration 165/1000 | Loss: 0.00001813
Iteration 166/1000 | Loss: 0.00001813
Iteration 167/1000 | Loss: 0.00001813
Iteration 168/1000 | Loss: 0.00001813
Iteration 169/1000 | Loss: 0.00001813
Iteration 170/1000 | Loss: 0.00001813
Iteration 171/1000 | Loss: 0.00001813
Iteration 172/1000 | Loss: 0.00001813
Iteration 173/1000 | Loss: 0.00001813
Iteration 174/1000 | Loss: 0.00001813
Iteration 175/1000 | Loss: 0.00001813
Iteration 176/1000 | Loss: 0.00001813
Iteration 177/1000 | Loss: 0.00001813
Iteration 178/1000 | Loss: 0.00001813
Iteration 179/1000 | Loss: 0.00001813
Iteration 180/1000 | Loss: 0.00001813
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.8126389477401972e-05, 1.8126389477401972e-05, 1.8126389477401972e-05, 1.8126389477401972e-05, 1.8126389477401972e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8126389477401972e-05

Optimization complete. Final v2v error: 3.5846734046936035 mm

Highest mean error: 4.006824493408203 mm for frame 151

Lowest mean error: 3.3380584716796875 mm for frame 103

Saving results

Total time: 44.08563852310181
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00739744
Iteration 2/25 | Loss: 0.00229856
Iteration 3/25 | Loss: 0.00181839
Iteration 4/25 | Loss: 0.00174642
Iteration 5/25 | Loss: 0.00166909
Iteration 6/25 | Loss: 0.00162878
Iteration 7/25 | Loss: 0.00160878
Iteration 8/25 | Loss: 0.00160811
Iteration 9/25 | Loss: 0.00163523
Iteration 10/25 | Loss: 0.00165603
Iteration 11/25 | Loss: 0.00172645
Iteration 12/25 | Loss: 0.00166185
Iteration 13/25 | Loss: 0.00177404
Iteration 14/25 | Loss: 0.00149039
Iteration 15/25 | Loss: 0.00141392
Iteration 16/25 | Loss: 0.00134097
Iteration 17/25 | Loss: 0.00134009
Iteration 18/25 | Loss: 0.00133998
Iteration 19/25 | Loss: 0.00133995
Iteration 20/25 | Loss: 0.00133995
Iteration 21/25 | Loss: 0.00133995
Iteration 22/25 | Loss: 0.00133995
Iteration 23/25 | Loss: 0.00133995
Iteration 24/25 | Loss: 0.00133994
Iteration 25/25 | Loss: 0.00133994

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.43635464
Iteration 2/25 | Loss: 0.00142678
Iteration 3/25 | Loss: 0.00116717
Iteration 4/25 | Loss: 0.00116708
Iteration 5/25 | Loss: 0.00116707
Iteration 6/25 | Loss: 0.00116707
Iteration 7/25 | Loss: 0.00116707
Iteration 8/25 | Loss: 0.00116707
Iteration 9/25 | Loss: 0.00116707
Iteration 10/25 | Loss: 0.00116707
Iteration 11/25 | Loss: 0.00116707
Iteration 12/25 | Loss: 0.00116707
Iteration 13/25 | Loss: 0.00116707
Iteration 14/25 | Loss: 0.00116707
Iteration 15/25 | Loss: 0.00116707
Iteration 16/25 | Loss: 0.00116707
Iteration 17/25 | Loss: 0.00116707
Iteration 18/25 | Loss: 0.00116707
Iteration 19/25 | Loss: 0.00116707
Iteration 20/25 | Loss: 0.00116707
Iteration 21/25 | Loss: 0.00116707
Iteration 22/25 | Loss: 0.00116707
Iteration 23/25 | Loss: 0.00116707
Iteration 24/25 | Loss: 0.00116707
Iteration 25/25 | Loss: 0.00116707

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116707
Iteration 2/1000 | Loss: 0.00051523
Iteration 3/1000 | Loss: 0.00006706
Iteration 4/1000 | Loss: 0.00004461
Iteration 5/1000 | Loss: 0.00003911
Iteration 6/1000 | Loss: 0.00003532
Iteration 7/1000 | Loss: 0.00003310
Iteration 8/1000 | Loss: 0.00003189
Iteration 9/1000 | Loss: 0.00003093
Iteration 10/1000 | Loss: 0.00003025
Iteration 11/1000 | Loss: 0.00002961
Iteration 12/1000 | Loss: 0.00002920
Iteration 13/1000 | Loss: 0.00002875
Iteration 14/1000 | Loss: 0.00002842
Iteration 15/1000 | Loss: 0.00002816
Iteration 16/1000 | Loss: 0.00002795
Iteration 17/1000 | Loss: 0.00002786
Iteration 18/1000 | Loss: 0.00002768
Iteration 19/1000 | Loss: 0.00002744
Iteration 20/1000 | Loss: 0.00002882
Iteration 21/1000 | Loss: 0.00002801
Iteration 22/1000 | Loss: 0.00002708
Iteration 23/1000 | Loss: 0.00002624
Iteration 24/1000 | Loss: 0.00002596
Iteration 25/1000 | Loss: 0.00002567
Iteration 26/1000 | Loss: 0.00002560
Iteration 27/1000 | Loss: 0.00002556
Iteration 28/1000 | Loss: 0.00002556
Iteration 29/1000 | Loss: 0.00002554
Iteration 30/1000 | Loss: 0.00002554
Iteration 31/1000 | Loss: 0.00002553
Iteration 32/1000 | Loss: 0.00002553
Iteration 33/1000 | Loss: 0.00002552
Iteration 34/1000 | Loss: 0.00002552
Iteration 35/1000 | Loss: 0.00002551
Iteration 36/1000 | Loss: 0.00002551
Iteration 37/1000 | Loss: 0.00002551
Iteration 38/1000 | Loss: 0.00002550
Iteration 39/1000 | Loss: 0.00002550
Iteration 40/1000 | Loss: 0.00002549
Iteration 41/1000 | Loss: 0.00002549
Iteration 42/1000 | Loss: 0.00002549
Iteration 43/1000 | Loss: 0.00002548
Iteration 44/1000 | Loss: 0.00002548
Iteration 45/1000 | Loss: 0.00002547
Iteration 46/1000 | Loss: 0.00002547
Iteration 47/1000 | Loss: 0.00002547
Iteration 48/1000 | Loss: 0.00002547
Iteration 49/1000 | Loss: 0.00002547
Iteration 50/1000 | Loss: 0.00002546
Iteration 51/1000 | Loss: 0.00002546
Iteration 52/1000 | Loss: 0.00002546
Iteration 53/1000 | Loss: 0.00002545
Iteration 54/1000 | Loss: 0.00002545
Iteration 55/1000 | Loss: 0.00002545
Iteration 56/1000 | Loss: 0.00002544
Iteration 57/1000 | Loss: 0.00002544
Iteration 58/1000 | Loss: 0.00002544
Iteration 59/1000 | Loss: 0.00002543
Iteration 60/1000 | Loss: 0.00002543
Iteration 61/1000 | Loss: 0.00002543
Iteration 62/1000 | Loss: 0.00002543
Iteration 63/1000 | Loss: 0.00002543
Iteration 64/1000 | Loss: 0.00002542
Iteration 65/1000 | Loss: 0.00002542
Iteration 66/1000 | Loss: 0.00002542
Iteration 67/1000 | Loss: 0.00002542
Iteration 68/1000 | Loss: 0.00002542
Iteration 69/1000 | Loss: 0.00002542
Iteration 70/1000 | Loss: 0.00002542
Iteration 71/1000 | Loss: 0.00002542
Iteration 72/1000 | Loss: 0.00002542
Iteration 73/1000 | Loss: 0.00002542
Iteration 74/1000 | Loss: 0.00002541
Iteration 75/1000 | Loss: 0.00002541
Iteration 76/1000 | Loss: 0.00002541
Iteration 77/1000 | Loss: 0.00002541
Iteration 78/1000 | Loss: 0.00002541
Iteration 79/1000 | Loss: 0.00002540
Iteration 80/1000 | Loss: 0.00002540
Iteration 81/1000 | Loss: 0.00002540
Iteration 82/1000 | Loss: 0.00002540
Iteration 83/1000 | Loss: 0.00002539
Iteration 84/1000 | Loss: 0.00002539
Iteration 85/1000 | Loss: 0.00002539
Iteration 86/1000 | Loss: 0.00002539
Iteration 87/1000 | Loss: 0.00002539
Iteration 88/1000 | Loss: 0.00002538
Iteration 89/1000 | Loss: 0.00002538
Iteration 90/1000 | Loss: 0.00002538
Iteration 91/1000 | Loss: 0.00002538
Iteration 92/1000 | Loss: 0.00002538
Iteration 93/1000 | Loss: 0.00002538
Iteration 94/1000 | Loss: 0.00002538
Iteration 95/1000 | Loss: 0.00002538
Iteration 96/1000 | Loss: 0.00002538
Iteration 97/1000 | Loss: 0.00002537
Iteration 98/1000 | Loss: 0.00002537
Iteration 99/1000 | Loss: 0.00002537
Iteration 100/1000 | Loss: 0.00002537
Iteration 101/1000 | Loss: 0.00002537
Iteration 102/1000 | Loss: 0.00002537
Iteration 103/1000 | Loss: 0.00002537
Iteration 104/1000 | Loss: 0.00002536
Iteration 105/1000 | Loss: 0.00002536
Iteration 106/1000 | Loss: 0.00002536
Iteration 107/1000 | Loss: 0.00002536
Iteration 108/1000 | Loss: 0.00002536
Iteration 109/1000 | Loss: 0.00002536
Iteration 110/1000 | Loss: 0.00002536
Iteration 111/1000 | Loss: 0.00002536
Iteration 112/1000 | Loss: 0.00002535
Iteration 113/1000 | Loss: 0.00002535
Iteration 114/1000 | Loss: 0.00002535
Iteration 115/1000 | Loss: 0.00002535
Iteration 116/1000 | Loss: 0.00002535
Iteration 117/1000 | Loss: 0.00002534
Iteration 118/1000 | Loss: 0.00002534
Iteration 119/1000 | Loss: 0.00002534
Iteration 120/1000 | Loss: 0.00002534
Iteration 121/1000 | Loss: 0.00002534
Iteration 122/1000 | Loss: 0.00002534
Iteration 123/1000 | Loss: 0.00002534
Iteration 124/1000 | Loss: 0.00002534
Iteration 125/1000 | Loss: 0.00002534
Iteration 126/1000 | Loss: 0.00002534
Iteration 127/1000 | Loss: 0.00002534
Iteration 128/1000 | Loss: 0.00002533
Iteration 129/1000 | Loss: 0.00002533
Iteration 130/1000 | Loss: 0.00002533
Iteration 131/1000 | Loss: 0.00002533
Iteration 132/1000 | Loss: 0.00002533
Iteration 133/1000 | Loss: 0.00002533
Iteration 134/1000 | Loss: 0.00002533
Iteration 135/1000 | Loss: 0.00002533
Iteration 136/1000 | Loss: 0.00002533
Iteration 137/1000 | Loss: 0.00002533
Iteration 138/1000 | Loss: 0.00002533
Iteration 139/1000 | Loss: 0.00002532
Iteration 140/1000 | Loss: 0.00002532
Iteration 141/1000 | Loss: 0.00002532
Iteration 142/1000 | Loss: 0.00002532
Iteration 143/1000 | Loss: 0.00002532
Iteration 144/1000 | Loss: 0.00002532
Iteration 145/1000 | Loss: 0.00002532
Iteration 146/1000 | Loss: 0.00002532
Iteration 147/1000 | Loss: 0.00002532
Iteration 148/1000 | Loss: 0.00002532
Iteration 149/1000 | Loss: 0.00002531
Iteration 150/1000 | Loss: 0.00002531
Iteration 151/1000 | Loss: 0.00002531
Iteration 152/1000 | Loss: 0.00002531
Iteration 153/1000 | Loss: 0.00002531
Iteration 154/1000 | Loss: 0.00002531
Iteration 155/1000 | Loss: 0.00002531
Iteration 156/1000 | Loss: 0.00002531
Iteration 157/1000 | Loss: 0.00002531
Iteration 158/1000 | Loss: 0.00002531
Iteration 159/1000 | Loss: 0.00002530
Iteration 160/1000 | Loss: 0.00002530
Iteration 161/1000 | Loss: 0.00002530
Iteration 162/1000 | Loss: 0.00002530
Iteration 163/1000 | Loss: 0.00002530
Iteration 164/1000 | Loss: 0.00002530
Iteration 165/1000 | Loss: 0.00002530
Iteration 166/1000 | Loss: 0.00002530
Iteration 167/1000 | Loss: 0.00002530
Iteration 168/1000 | Loss: 0.00002530
Iteration 169/1000 | Loss: 0.00002529
Iteration 170/1000 | Loss: 0.00002529
Iteration 171/1000 | Loss: 0.00002529
Iteration 172/1000 | Loss: 0.00002529
Iteration 173/1000 | Loss: 0.00002529
Iteration 174/1000 | Loss: 0.00002529
Iteration 175/1000 | Loss: 0.00002528
Iteration 176/1000 | Loss: 0.00002528
Iteration 177/1000 | Loss: 0.00002528
Iteration 178/1000 | Loss: 0.00002528
Iteration 179/1000 | Loss: 0.00002527
Iteration 180/1000 | Loss: 0.00002527
Iteration 181/1000 | Loss: 0.00002527
Iteration 182/1000 | Loss: 0.00002527
Iteration 183/1000 | Loss: 0.00002527
Iteration 184/1000 | Loss: 0.00002527
Iteration 185/1000 | Loss: 0.00002527
Iteration 186/1000 | Loss: 0.00002527
Iteration 187/1000 | Loss: 0.00002527
Iteration 188/1000 | Loss: 0.00002527
Iteration 189/1000 | Loss: 0.00002527
Iteration 190/1000 | Loss: 0.00002527
Iteration 191/1000 | Loss: 0.00002527
Iteration 192/1000 | Loss: 0.00002527
Iteration 193/1000 | Loss: 0.00002527
Iteration 194/1000 | Loss: 0.00002527
Iteration 195/1000 | Loss: 0.00002527
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [2.5267383534810506e-05, 2.5267383534810506e-05, 2.5267383534810506e-05, 2.5267383534810506e-05, 2.5267383534810506e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5267383534810506e-05

Optimization complete. Final v2v error: 4.0692572593688965 mm

Highest mean error: 6.748291015625 mm for frame 154

Lowest mean error: 3.09660267829895 mm for frame 17

Saving results

Total time: 95.44535255432129
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00985774
Iteration 2/25 | Loss: 0.00426517
Iteration 3/25 | Loss: 0.00311604
Iteration 4/25 | Loss: 0.00220710
Iteration 5/25 | Loss: 0.00223363
Iteration 6/25 | Loss: 0.00186447
Iteration 7/25 | Loss: 0.00146909
Iteration 8/25 | Loss: 0.00134193
Iteration 9/25 | Loss: 0.00130343
Iteration 10/25 | Loss: 0.00129728
Iteration 11/25 | Loss: 0.00129784
Iteration 12/25 | Loss: 0.00129514
Iteration 13/25 | Loss: 0.00128699
Iteration 14/25 | Loss: 0.00127486
Iteration 15/25 | Loss: 0.00127036
Iteration 16/25 | Loss: 0.00126910
Iteration 17/25 | Loss: 0.00126876
Iteration 18/25 | Loss: 0.00126866
Iteration 19/25 | Loss: 0.00126866
Iteration 20/25 | Loss: 0.00126865
Iteration 21/25 | Loss: 0.00126865
Iteration 22/25 | Loss: 0.00126865
Iteration 23/25 | Loss: 0.00126865
Iteration 24/25 | Loss: 0.00126865
Iteration 25/25 | Loss: 0.00126865

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39545918
Iteration 2/25 | Loss: 0.00081646
Iteration 3/25 | Loss: 0.00081646
Iteration 4/25 | Loss: 0.00081646
Iteration 5/25 | Loss: 0.00081646
Iteration 6/25 | Loss: 0.00081646
Iteration 7/25 | Loss: 0.00081646
Iteration 8/25 | Loss: 0.00081646
Iteration 9/25 | Loss: 0.00081646
Iteration 10/25 | Loss: 0.00081646
Iteration 11/25 | Loss: 0.00081646
Iteration 12/25 | Loss: 0.00081646
Iteration 13/25 | Loss: 0.00081646
Iteration 14/25 | Loss: 0.00081646
Iteration 15/25 | Loss: 0.00081646
Iteration 16/25 | Loss: 0.00081646
Iteration 17/25 | Loss: 0.00081646
Iteration 18/25 | Loss: 0.00081646
Iteration 19/25 | Loss: 0.00081646
Iteration 20/25 | Loss: 0.00081646
Iteration 21/25 | Loss: 0.00081646
Iteration 22/25 | Loss: 0.00081646
Iteration 23/25 | Loss: 0.00081646
Iteration 24/25 | Loss: 0.00081646
Iteration 25/25 | Loss: 0.00081646

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081646
Iteration 2/1000 | Loss: 0.00003455
Iteration 3/1000 | Loss: 0.00002639
Iteration 4/1000 | Loss: 0.00002425
Iteration 5/1000 | Loss: 0.00002302
Iteration 6/1000 | Loss: 0.00002225
Iteration 7/1000 | Loss: 0.00002159
Iteration 8/1000 | Loss: 0.00002129
Iteration 9/1000 | Loss: 0.00002085
Iteration 10/1000 | Loss: 0.00002055
Iteration 11/1000 | Loss: 0.00188259
Iteration 12/1000 | Loss: 0.00002431
Iteration 13/1000 | Loss: 0.00002033
Iteration 14/1000 | Loss: 0.00001810
Iteration 15/1000 | Loss: 0.00001629
Iteration 16/1000 | Loss: 0.00009617
Iteration 17/1000 | Loss: 0.00003203
Iteration 18/1000 | Loss: 0.00001538
Iteration 19/1000 | Loss: 0.00001440
Iteration 20/1000 | Loss: 0.00001408
Iteration 21/1000 | Loss: 0.00001405
Iteration 22/1000 | Loss: 0.00001383
Iteration 23/1000 | Loss: 0.00001359
Iteration 24/1000 | Loss: 0.00001336
Iteration 25/1000 | Loss: 0.00001317
Iteration 26/1000 | Loss: 0.00001315
Iteration 27/1000 | Loss: 0.00001315
Iteration 28/1000 | Loss: 0.00001314
Iteration 29/1000 | Loss: 0.00001313
Iteration 30/1000 | Loss: 0.00001313
Iteration 31/1000 | Loss: 0.00001313
Iteration 32/1000 | Loss: 0.00001313
Iteration 33/1000 | Loss: 0.00001312
Iteration 34/1000 | Loss: 0.00001310
Iteration 35/1000 | Loss: 0.00001309
Iteration 36/1000 | Loss: 0.00001304
Iteration 37/1000 | Loss: 0.00001303
Iteration 38/1000 | Loss: 0.00001303
Iteration 39/1000 | Loss: 0.00001303
Iteration 40/1000 | Loss: 0.00001302
Iteration 41/1000 | Loss: 0.00001302
Iteration 42/1000 | Loss: 0.00001301
Iteration 43/1000 | Loss: 0.00001301
Iteration 44/1000 | Loss: 0.00001301
Iteration 45/1000 | Loss: 0.00001301
Iteration 46/1000 | Loss: 0.00001301
Iteration 47/1000 | Loss: 0.00001300
Iteration 48/1000 | Loss: 0.00001300
Iteration 49/1000 | Loss: 0.00001300
Iteration 50/1000 | Loss: 0.00001299
Iteration 51/1000 | Loss: 0.00001299
Iteration 52/1000 | Loss: 0.00001299
Iteration 53/1000 | Loss: 0.00001298
Iteration 54/1000 | Loss: 0.00001298
Iteration 55/1000 | Loss: 0.00001298
Iteration 56/1000 | Loss: 0.00001298
Iteration 57/1000 | Loss: 0.00001298
Iteration 58/1000 | Loss: 0.00001298
Iteration 59/1000 | Loss: 0.00001298
Iteration 60/1000 | Loss: 0.00001297
Iteration 61/1000 | Loss: 0.00001297
Iteration 62/1000 | Loss: 0.00001297
Iteration 63/1000 | Loss: 0.00001297
Iteration 64/1000 | Loss: 0.00001297
Iteration 65/1000 | Loss: 0.00001297
Iteration 66/1000 | Loss: 0.00001297
Iteration 67/1000 | Loss: 0.00001297
Iteration 68/1000 | Loss: 0.00001296
Iteration 69/1000 | Loss: 0.00001296
Iteration 70/1000 | Loss: 0.00001296
Iteration 71/1000 | Loss: 0.00001296
Iteration 72/1000 | Loss: 0.00001296
Iteration 73/1000 | Loss: 0.00001296
Iteration 74/1000 | Loss: 0.00001296
Iteration 75/1000 | Loss: 0.00001296
Iteration 76/1000 | Loss: 0.00001296
Iteration 77/1000 | Loss: 0.00001296
Iteration 78/1000 | Loss: 0.00001296
Iteration 79/1000 | Loss: 0.00001296
Iteration 80/1000 | Loss: 0.00001296
Iteration 81/1000 | Loss: 0.00001296
Iteration 82/1000 | Loss: 0.00001295
Iteration 83/1000 | Loss: 0.00001295
Iteration 84/1000 | Loss: 0.00001295
Iteration 85/1000 | Loss: 0.00001295
Iteration 86/1000 | Loss: 0.00001295
Iteration 87/1000 | Loss: 0.00001295
Iteration 88/1000 | Loss: 0.00001295
Iteration 89/1000 | Loss: 0.00001295
Iteration 90/1000 | Loss: 0.00001295
Iteration 91/1000 | Loss: 0.00001295
Iteration 92/1000 | Loss: 0.00001295
Iteration 93/1000 | Loss: 0.00001295
Iteration 94/1000 | Loss: 0.00001295
Iteration 95/1000 | Loss: 0.00001295
Iteration 96/1000 | Loss: 0.00001294
Iteration 97/1000 | Loss: 0.00001294
Iteration 98/1000 | Loss: 0.00001294
Iteration 99/1000 | Loss: 0.00001294
Iteration 100/1000 | Loss: 0.00001294
Iteration 101/1000 | Loss: 0.00001294
Iteration 102/1000 | Loss: 0.00001294
Iteration 103/1000 | Loss: 0.00001294
Iteration 104/1000 | Loss: 0.00001294
Iteration 105/1000 | Loss: 0.00001294
Iteration 106/1000 | Loss: 0.00001294
Iteration 107/1000 | Loss: 0.00001294
Iteration 108/1000 | Loss: 0.00001294
Iteration 109/1000 | Loss: 0.00001294
Iteration 110/1000 | Loss: 0.00001294
Iteration 111/1000 | Loss: 0.00001294
Iteration 112/1000 | Loss: 0.00001294
Iteration 113/1000 | Loss: 0.00001294
Iteration 114/1000 | Loss: 0.00001294
Iteration 115/1000 | Loss: 0.00001294
Iteration 116/1000 | Loss: 0.00001294
Iteration 117/1000 | Loss: 0.00001294
Iteration 118/1000 | Loss: 0.00001294
Iteration 119/1000 | Loss: 0.00001294
Iteration 120/1000 | Loss: 0.00001294
Iteration 121/1000 | Loss: 0.00001294
Iteration 122/1000 | Loss: 0.00001294
Iteration 123/1000 | Loss: 0.00001294
Iteration 124/1000 | Loss: 0.00001294
Iteration 125/1000 | Loss: 0.00001294
Iteration 126/1000 | Loss: 0.00001294
Iteration 127/1000 | Loss: 0.00001294
Iteration 128/1000 | Loss: 0.00001294
Iteration 129/1000 | Loss: 0.00001294
Iteration 130/1000 | Loss: 0.00001294
Iteration 131/1000 | Loss: 0.00001294
Iteration 132/1000 | Loss: 0.00001294
Iteration 133/1000 | Loss: 0.00001294
Iteration 134/1000 | Loss: 0.00001294
Iteration 135/1000 | Loss: 0.00001294
Iteration 136/1000 | Loss: 0.00001294
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [1.2936310668010265e-05, 1.2936310668010265e-05, 1.2936310668010265e-05, 1.2936310668010265e-05, 1.2936310668010265e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2936310668010265e-05

Optimization complete. Final v2v error: 3.0888352394104004 mm

Highest mean error: 3.4762966632843018 mm for frame 138

Lowest mean error: 3.0051822662353516 mm for frame 80

Saving results

Total time: 70.99242329597473
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00832185
Iteration 2/25 | Loss: 0.00163729
Iteration 3/25 | Loss: 0.00146010
Iteration 4/25 | Loss: 0.00144485
Iteration 5/25 | Loss: 0.00144319
Iteration 6/25 | Loss: 0.00144319
Iteration 7/25 | Loss: 0.00144319
Iteration 8/25 | Loss: 0.00144319
Iteration 9/25 | Loss: 0.00144319
Iteration 10/25 | Loss: 0.00144319
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0014431942254304886, 0.0014431942254304886, 0.0014431942254304886, 0.0014431942254304886, 0.0014431942254304886]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014431942254304886

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.00910652
Iteration 2/25 | Loss: 0.00074631
Iteration 3/25 | Loss: 0.00074630
Iteration 4/25 | Loss: 0.00074630
Iteration 5/25 | Loss: 0.00074630
Iteration 6/25 | Loss: 0.00074630
Iteration 7/25 | Loss: 0.00074630
Iteration 8/25 | Loss: 0.00074630
Iteration 9/25 | Loss: 0.00074630
Iteration 10/25 | Loss: 0.00074630
Iteration 11/25 | Loss: 0.00074630
Iteration 12/25 | Loss: 0.00074630
Iteration 13/25 | Loss: 0.00074630
Iteration 14/25 | Loss: 0.00074630
Iteration 15/25 | Loss: 0.00074630
Iteration 16/25 | Loss: 0.00074630
Iteration 17/25 | Loss: 0.00074630
Iteration 18/25 | Loss: 0.00074630
Iteration 19/25 | Loss: 0.00074630
Iteration 20/25 | Loss: 0.00074630
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007463018991984427, 0.0007463018991984427, 0.0007463018991984427, 0.0007463018991984427, 0.0007463018991984427]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007463018991984427

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074630
Iteration 2/1000 | Loss: 0.00004736
Iteration 3/1000 | Loss: 0.00003464
Iteration 4/1000 | Loss: 0.00003078
Iteration 5/1000 | Loss: 0.00002956
Iteration 6/1000 | Loss: 0.00002872
Iteration 7/1000 | Loss: 0.00002796
Iteration 8/1000 | Loss: 0.00002754
Iteration 9/1000 | Loss: 0.00002721
Iteration 10/1000 | Loss: 0.00002687
Iteration 11/1000 | Loss: 0.00002667
Iteration 12/1000 | Loss: 0.00002665
Iteration 13/1000 | Loss: 0.00002639
Iteration 14/1000 | Loss: 0.00002609
Iteration 15/1000 | Loss: 0.00002589
Iteration 16/1000 | Loss: 0.00002588
Iteration 17/1000 | Loss: 0.00002587
Iteration 18/1000 | Loss: 0.00002583
Iteration 19/1000 | Loss: 0.00002583
Iteration 20/1000 | Loss: 0.00002576
Iteration 21/1000 | Loss: 0.00002560
Iteration 22/1000 | Loss: 0.00002555
Iteration 23/1000 | Loss: 0.00002554
Iteration 24/1000 | Loss: 0.00002554
Iteration 25/1000 | Loss: 0.00002553
Iteration 26/1000 | Loss: 0.00002553
Iteration 27/1000 | Loss: 0.00002547
Iteration 28/1000 | Loss: 0.00002547
Iteration 29/1000 | Loss: 0.00002547
Iteration 30/1000 | Loss: 0.00002546
Iteration 31/1000 | Loss: 0.00002546
Iteration 32/1000 | Loss: 0.00002546
Iteration 33/1000 | Loss: 0.00002546
Iteration 34/1000 | Loss: 0.00002546
Iteration 35/1000 | Loss: 0.00002546
Iteration 36/1000 | Loss: 0.00002546
Iteration 37/1000 | Loss: 0.00002546
Iteration 38/1000 | Loss: 0.00002546
Iteration 39/1000 | Loss: 0.00002546
Iteration 40/1000 | Loss: 0.00002546
Iteration 41/1000 | Loss: 0.00002546
Iteration 42/1000 | Loss: 0.00002546
Iteration 43/1000 | Loss: 0.00002546
Iteration 44/1000 | Loss: 0.00002546
Iteration 45/1000 | Loss: 0.00002545
Iteration 46/1000 | Loss: 0.00002545
Iteration 47/1000 | Loss: 0.00002544
Iteration 48/1000 | Loss: 0.00002543
Iteration 49/1000 | Loss: 0.00002539
Iteration 50/1000 | Loss: 0.00002539
Iteration 51/1000 | Loss: 0.00002539
Iteration 52/1000 | Loss: 0.00002539
Iteration 53/1000 | Loss: 0.00002539
Iteration 54/1000 | Loss: 0.00002538
Iteration 55/1000 | Loss: 0.00002538
Iteration 56/1000 | Loss: 0.00002538
Iteration 57/1000 | Loss: 0.00002537
Iteration 58/1000 | Loss: 0.00002537
Iteration 59/1000 | Loss: 0.00002536
Iteration 60/1000 | Loss: 0.00002536
Iteration 61/1000 | Loss: 0.00002536
Iteration 62/1000 | Loss: 0.00002536
Iteration 63/1000 | Loss: 0.00002536
Iteration 64/1000 | Loss: 0.00002536
Iteration 65/1000 | Loss: 0.00002535
Iteration 66/1000 | Loss: 0.00002535
Iteration 67/1000 | Loss: 0.00002535
Iteration 68/1000 | Loss: 0.00002534
Iteration 69/1000 | Loss: 0.00002534
Iteration 70/1000 | Loss: 0.00002529
Iteration 71/1000 | Loss: 0.00002529
Iteration 72/1000 | Loss: 0.00002529
Iteration 73/1000 | Loss: 0.00002529
Iteration 74/1000 | Loss: 0.00002528
Iteration 75/1000 | Loss: 0.00002528
Iteration 76/1000 | Loss: 0.00002528
Iteration 77/1000 | Loss: 0.00002528
Iteration 78/1000 | Loss: 0.00002528
Iteration 79/1000 | Loss: 0.00002528
Iteration 80/1000 | Loss: 0.00002528
Iteration 81/1000 | Loss: 0.00002528
Iteration 82/1000 | Loss: 0.00002527
Iteration 83/1000 | Loss: 0.00002527
Iteration 84/1000 | Loss: 0.00002527
Iteration 85/1000 | Loss: 0.00002527
Iteration 86/1000 | Loss: 0.00002527
Iteration 87/1000 | Loss: 0.00002527
Iteration 88/1000 | Loss: 0.00002527
Iteration 89/1000 | Loss: 0.00002527
Iteration 90/1000 | Loss: 0.00002527
Iteration 91/1000 | Loss: 0.00002527
Iteration 92/1000 | Loss: 0.00002526
Iteration 93/1000 | Loss: 0.00002526
Iteration 94/1000 | Loss: 0.00002526
Iteration 95/1000 | Loss: 0.00002526
Iteration 96/1000 | Loss: 0.00002526
Iteration 97/1000 | Loss: 0.00002526
Iteration 98/1000 | Loss: 0.00002526
Iteration 99/1000 | Loss: 0.00002526
Iteration 100/1000 | Loss: 0.00002526
Iteration 101/1000 | Loss: 0.00002526
Iteration 102/1000 | Loss: 0.00002526
Iteration 103/1000 | Loss: 0.00002526
Iteration 104/1000 | Loss: 0.00002526
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [2.526480966480449e-05, 2.526480966480449e-05, 2.526480966480449e-05, 2.526480966480449e-05, 2.526480966480449e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.526480966480449e-05

Optimization complete. Final v2v error: 4.233530044555664 mm

Highest mean error: 4.286515712738037 mm for frame 128

Lowest mean error: 4.191482067108154 mm for frame 149

Saving results

Total time: 35.996007204055786
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00414663
Iteration 2/25 | Loss: 0.00138537
Iteration 3/25 | Loss: 0.00132422
Iteration 4/25 | Loss: 0.00132061
Iteration 5/25 | Loss: 0.00132028
Iteration 6/25 | Loss: 0.00132028
Iteration 7/25 | Loss: 0.00132028
Iteration 8/25 | Loss: 0.00132028
Iteration 9/25 | Loss: 0.00132028
Iteration 10/25 | Loss: 0.00132028
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001320276758633554, 0.001320276758633554, 0.001320276758633554, 0.001320276758633554, 0.001320276758633554]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001320276758633554

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38882375
Iteration 2/25 | Loss: 0.00090670
Iteration 3/25 | Loss: 0.00090670
Iteration 4/25 | Loss: 0.00090670
Iteration 5/25 | Loss: 0.00090670
Iteration 6/25 | Loss: 0.00090670
Iteration 7/25 | Loss: 0.00090670
Iteration 8/25 | Loss: 0.00090670
Iteration 9/25 | Loss: 0.00090670
Iteration 10/25 | Loss: 0.00090670
Iteration 11/25 | Loss: 0.00090670
Iteration 12/25 | Loss: 0.00090670
Iteration 13/25 | Loss: 0.00090670
Iteration 14/25 | Loss: 0.00090670
Iteration 15/25 | Loss: 0.00090670
Iteration 16/25 | Loss: 0.00090670
Iteration 17/25 | Loss: 0.00090670
Iteration 18/25 | Loss: 0.00090670
Iteration 19/25 | Loss: 0.00090670
Iteration 20/25 | Loss: 0.00090670
Iteration 21/25 | Loss: 0.00090670
Iteration 22/25 | Loss: 0.00090670
Iteration 23/25 | Loss: 0.00090670
Iteration 24/25 | Loss: 0.00090670
Iteration 25/25 | Loss: 0.00090670

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090670
Iteration 2/1000 | Loss: 0.00003216
Iteration 3/1000 | Loss: 0.00002133
Iteration 4/1000 | Loss: 0.00001891
Iteration 5/1000 | Loss: 0.00001759
Iteration 6/1000 | Loss: 0.00001684
Iteration 7/1000 | Loss: 0.00001657
Iteration 8/1000 | Loss: 0.00001624
Iteration 9/1000 | Loss: 0.00001604
Iteration 10/1000 | Loss: 0.00001589
Iteration 11/1000 | Loss: 0.00001588
Iteration 12/1000 | Loss: 0.00001582
Iteration 13/1000 | Loss: 0.00001581
Iteration 14/1000 | Loss: 0.00001580
Iteration 15/1000 | Loss: 0.00001577
Iteration 16/1000 | Loss: 0.00001576
Iteration 17/1000 | Loss: 0.00001574
Iteration 18/1000 | Loss: 0.00001573
Iteration 19/1000 | Loss: 0.00001573
Iteration 20/1000 | Loss: 0.00001568
Iteration 21/1000 | Loss: 0.00001557
Iteration 22/1000 | Loss: 0.00001552
Iteration 23/1000 | Loss: 0.00001550
Iteration 24/1000 | Loss: 0.00001546
Iteration 25/1000 | Loss: 0.00001546
Iteration 26/1000 | Loss: 0.00001546
Iteration 27/1000 | Loss: 0.00001546
Iteration 28/1000 | Loss: 0.00001546
Iteration 29/1000 | Loss: 0.00001546
Iteration 30/1000 | Loss: 0.00001545
Iteration 31/1000 | Loss: 0.00001545
Iteration 32/1000 | Loss: 0.00001545
Iteration 33/1000 | Loss: 0.00001545
Iteration 34/1000 | Loss: 0.00001545
Iteration 35/1000 | Loss: 0.00001545
Iteration 36/1000 | Loss: 0.00001545
Iteration 37/1000 | Loss: 0.00001541
Iteration 38/1000 | Loss: 0.00001540
Iteration 39/1000 | Loss: 0.00001540
Iteration 40/1000 | Loss: 0.00001536
Iteration 41/1000 | Loss: 0.00001536
Iteration 42/1000 | Loss: 0.00001536
Iteration 43/1000 | Loss: 0.00001535
Iteration 44/1000 | Loss: 0.00001535
Iteration 45/1000 | Loss: 0.00001535
Iteration 46/1000 | Loss: 0.00001535
Iteration 47/1000 | Loss: 0.00001535
Iteration 48/1000 | Loss: 0.00001535
Iteration 49/1000 | Loss: 0.00001535
Iteration 50/1000 | Loss: 0.00001535
Iteration 51/1000 | Loss: 0.00001534
Iteration 52/1000 | Loss: 0.00001534
Iteration 53/1000 | Loss: 0.00001533
Iteration 54/1000 | Loss: 0.00001532
Iteration 55/1000 | Loss: 0.00001531
Iteration 56/1000 | Loss: 0.00001531
Iteration 57/1000 | Loss: 0.00001531
Iteration 58/1000 | Loss: 0.00001531
Iteration 59/1000 | Loss: 0.00001529
Iteration 60/1000 | Loss: 0.00001529
Iteration 61/1000 | Loss: 0.00001529
Iteration 62/1000 | Loss: 0.00001528
Iteration 63/1000 | Loss: 0.00001527
Iteration 64/1000 | Loss: 0.00001527
Iteration 65/1000 | Loss: 0.00001527
Iteration 66/1000 | Loss: 0.00001527
Iteration 67/1000 | Loss: 0.00001527
Iteration 68/1000 | Loss: 0.00001527
Iteration 69/1000 | Loss: 0.00001527
Iteration 70/1000 | Loss: 0.00001526
Iteration 71/1000 | Loss: 0.00001526
Iteration 72/1000 | Loss: 0.00001526
Iteration 73/1000 | Loss: 0.00001525
Iteration 74/1000 | Loss: 0.00001524
Iteration 75/1000 | Loss: 0.00001524
Iteration 76/1000 | Loss: 0.00001524
Iteration 77/1000 | Loss: 0.00001524
Iteration 78/1000 | Loss: 0.00001524
Iteration 79/1000 | Loss: 0.00001524
Iteration 80/1000 | Loss: 0.00001524
Iteration 81/1000 | Loss: 0.00001523
Iteration 82/1000 | Loss: 0.00001523
Iteration 83/1000 | Loss: 0.00001523
Iteration 84/1000 | Loss: 0.00001523
Iteration 85/1000 | Loss: 0.00001522
Iteration 86/1000 | Loss: 0.00001522
Iteration 87/1000 | Loss: 0.00001521
Iteration 88/1000 | Loss: 0.00001521
Iteration 89/1000 | Loss: 0.00001521
Iteration 90/1000 | Loss: 0.00001521
Iteration 91/1000 | Loss: 0.00001520
Iteration 92/1000 | Loss: 0.00001520
Iteration 93/1000 | Loss: 0.00001520
Iteration 94/1000 | Loss: 0.00001520
Iteration 95/1000 | Loss: 0.00001520
Iteration 96/1000 | Loss: 0.00001520
Iteration 97/1000 | Loss: 0.00001520
Iteration 98/1000 | Loss: 0.00001519
Iteration 99/1000 | Loss: 0.00001519
Iteration 100/1000 | Loss: 0.00001518
Iteration 101/1000 | Loss: 0.00001518
Iteration 102/1000 | Loss: 0.00001518
Iteration 103/1000 | Loss: 0.00001518
Iteration 104/1000 | Loss: 0.00001518
Iteration 105/1000 | Loss: 0.00001518
Iteration 106/1000 | Loss: 0.00001518
Iteration 107/1000 | Loss: 0.00001518
Iteration 108/1000 | Loss: 0.00001518
Iteration 109/1000 | Loss: 0.00001517
Iteration 110/1000 | Loss: 0.00001517
Iteration 111/1000 | Loss: 0.00001517
Iteration 112/1000 | Loss: 0.00001517
Iteration 113/1000 | Loss: 0.00001517
Iteration 114/1000 | Loss: 0.00001517
Iteration 115/1000 | Loss: 0.00001517
Iteration 116/1000 | Loss: 0.00001517
Iteration 117/1000 | Loss: 0.00001517
Iteration 118/1000 | Loss: 0.00001517
Iteration 119/1000 | Loss: 0.00001517
Iteration 120/1000 | Loss: 0.00001517
Iteration 121/1000 | Loss: 0.00001516
Iteration 122/1000 | Loss: 0.00001516
Iteration 123/1000 | Loss: 0.00001516
Iteration 124/1000 | Loss: 0.00001515
Iteration 125/1000 | Loss: 0.00001515
Iteration 126/1000 | Loss: 0.00001515
Iteration 127/1000 | Loss: 0.00001515
Iteration 128/1000 | Loss: 0.00001514
Iteration 129/1000 | Loss: 0.00001514
Iteration 130/1000 | Loss: 0.00001514
Iteration 131/1000 | Loss: 0.00001513
Iteration 132/1000 | Loss: 0.00001513
Iteration 133/1000 | Loss: 0.00001513
Iteration 134/1000 | Loss: 0.00001513
Iteration 135/1000 | Loss: 0.00001512
Iteration 136/1000 | Loss: 0.00001512
Iteration 137/1000 | Loss: 0.00001511
Iteration 138/1000 | Loss: 0.00001511
Iteration 139/1000 | Loss: 0.00001511
Iteration 140/1000 | Loss: 0.00001511
Iteration 141/1000 | Loss: 0.00001511
Iteration 142/1000 | Loss: 0.00001511
Iteration 143/1000 | Loss: 0.00001511
Iteration 144/1000 | Loss: 0.00001510
Iteration 145/1000 | Loss: 0.00001510
Iteration 146/1000 | Loss: 0.00001510
Iteration 147/1000 | Loss: 0.00001510
Iteration 148/1000 | Loss: 0.00001510
Iteration 149/1000 | Loss: 0.00001509
Iteration 150/1000 | Loss: 0.00001509
Iteration 151/1000 | Loss: 0.00001509
Iteration 152/1000 | Loss: 0.00001509
Iteration 153/1000 | Loss: 0.00001508
Iteration 154/1000 | Loss: 0.00001508
Iteration 155/1000 | Loss: 0.00001508
Iteration 156/1000 | Loss: 0.00001508
Iteration 157/1000 | Loss: 0.00001508
Iteration 158/1000 | Loss: 0.00001508
Iteration 159/1000 | Loss: 0.00001508
Iteration 160/1000 | Loss: 0.00001508
Iteration 161/1000 | Loss: 0.00001508
Iteration 162/1000 | Loss: 0.00001508
Iteration 163/1000 | Loss: 0.00001508
Iteration 164/1000 | Loss: 0.00001508
Iteration 165/1000 | Loss: 0.00001508
Iteration 166/1000 | Loss: 0.00001508
Iteration 167/1000 | Loss: 0.00001508
Iteration 168/1000 | Loss: 0.00001508
Iteration 169/1000 | Loss: 0.00001508
Iteration 170/1000 | Loss: 0.00001508
Iteration 171/1000 | Loss: 0.00001508
Iteration 172/1000 | Loss: 0.00001508
Iteration 173/1000 | Loss: 0.00001508
Iteration 174/1000 | Loss: 0.00001508
Iteration 175/1000 | Loss: 0.00001508
Iteration 176/1000 | Loss: 0.00001508
Iteration 177/1000 | Loss: 0.00001508
Iteration 178/1000 | Loss: 0.00001508
Iteration 179/1000 | Loss: 0.00001508
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.507783690612996e-05, 1.507783690612996e-05, 1.507783690612996e-05, 1.507783690612996e-05, 1.507783690612996e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.507783690612996e-05

Optimization complete. Final v2v error: 3.30246639251709 mm

Highest mean error: 3.6269664764404297 mm for frame 51

Lowest mean error: 3.1814961433410645 mm for frame 135

Saving results

Total time: 37.967806577682495
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00465547
Iteration 2/25 | Loss: 0.00139319
Iteration 3/25 | Loss: 0.00131208
Iteration 4/25 | Loss: 0.00130273
Iteration 5/25 | Loss: 0.00129965
Iteration 6/25 | Loss: 0.00129915
Iteration 7/25 | Loss: 0.00129915
Iteration 8/25 | Loss: 0.00129915
Iteration 9/25 | Loss: 0.00129915
Iteration 10/25 | Loss: 0.00129915
Iteration 11/25 | Loss: 0.00129915
Iteration 12/25 | Loss: 0.00129915
Iteration 13/25 | Loss: 0.00129915
Iteration 14/25 | Loss: 0.00129915
Iteration 15/25 | Loss: 0.00129915
Iteration 16/25 | Loss: 0.00129915
Iteration 17/25 | Loss: 0.00129915
Iteration 18/25 | Loss: 0.00129915
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012991452822461724, 0.0012991452822461724, 0.0012991452822461724, 0.0012991452822461724, 0.0012991452822461724]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012991452822461724

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.06496048
Iteration 2/25 | Loss: 0.00091681
Iteration 3/25 | Loss: 0.00091681
Iteration 4/25 | Loss: 0.00091680
Iteration 5/25 | Loss: 0.00091680
Iteration 6/25 | Loss: 0.00091680
Iteration 7/25 | Loss: 0.00091680
Iteration 8/25 | Loss: 0.00091680
Iteration 9/25 | Loss: 0.00091680
Iteration 10/25 | Loss: 0.00091680
Iteration 11/25 | Loss: 0.00091680
Iteration 12/25 | Loss: 0.00091680
Iteration 13/25 | Loss: 0.00091680
Iteration 14/25 | Loss: 0.00091680
Iteration 15/25 | Loss: 0.00091680
Iteration 16/25 | Loss: 0.00091680
Iteration 17/25 | Loss: 0.00091680
Iteration 18/25 | Loss: 0.00091680
Iteration 19/25 | Loss: 0.00091680
Iteration 20/25 | Loss: 0.00091680
Iteration 21/25 | Loss: 0.00091680
Iteration 22/25 | Loss: 0.00091680
Iteration 23/25 | Loss: 0.00091680
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0009168018004857004, 0.0009168018004857004, 0.0009168018004857004, 0.0009168018004857004, 0.0009168018004857004]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009168018004857004

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091680
Iteration 2/1000 | Loss: 0.00002741
Iteration 3/1000 | Loss: 0.00002041
Iteration 4/1000 | Loss: 0.00001857
Iteration 5/1000 | Loss: 0.00001785
Iteration 6/1000 | Loss: 0.00001730
Iteration 7/1000 | Loss: 0.00001682
Iteration 8/1000 | Loss: 0.00001641
Iteration 9/1000 | Loss: 0.00001626
Iteration 10/1000 | Loss: 0.00001602
Iteration 11/1000 | Loss: 0.00001573
Iteration 12/1000 | Loss: 0.00001566
Iteration 13/1000 | Loss: 0.00001562
Iteration 14/1000 | Loss: 0.00001562
Iteration 15/1000 | Loss: 0.00001561
Iteration 16/1000 | Loss: 0.00001560
Iteration 17/1000 | Loss: 0.00001559
Iteration 18/1000 | Loss: 0.00001559
Iteration 19/1000 | Loss: 0.00001558
Iteration 20/1000 | Loss: 0.00001550
Iteration 21/1000 | Loss: 0.00001542
Iteration 22/1000 | Loss: 0.00001542
Iteration 23/1000 | Loss: 0.00001542
Iteration 24/1000 | Loss: 0.00001541
Iteration 25/1000 | Loss: 0.00001541
Iteration 26/1000 | Loss: 0.00001539
Iteration 27/1000 | Loss: 0.00001539
Iteration 28/1000 | Loss: 0.00001538
Iteration 29/1000 | Loss: 0.00001538
Iteration 30/1000 | Loss: 0.00001537
Iteration 31/1000 | Loss: 0.00001537
Iteration 32/1000 | Loss: 0.00001536
Iteration 33/1000 | Loss: 0.00001536
Iteration 34/1000 | Loss: 0.00001534
Iteration 35/1000 | Loss: 0.00001533
Iteration 36/1000 | Loss: 0.00001532
Iteration 37/1000 | Loss: 0.00001532
Iteration 38/1000 | Loss: 0.00001532
Iteration 39/1000 | Loss: 0.00001532
Iteration 40/1000 | Loss: 0.00001531
Iteration 41/1000 | Loss: 0.00001531
Iteration 42/1000 | Loss: 0.00001531
Iteration 43/1000 | Loss: 0.00001531
Iteration 44/1000 | Loss: 0.00001531
Iteration 45/1000 | Loss: 0.00001531
Iteration 46/1000 | Loss: 0.00001527
Iteration 47/1000 | Loss: 0.00001527
Iteration 48/1000 | Loss: 0.00001527
Iteration 49/1000 | Loss: 0.00001527
Iteration 50/1000 | Loss: 0.00001527
Iteration 51/1000 | Loss: 0.00001526
Iteration 52/1000 | Loss: 0.00001525
Iteration 53/1000 | Loss: 0.00001525
Iteration 54/1000 | Loss: 0.00001525
Iteration 55/1000 | Loss: 0.00001524
Iteration 56/1000 | Loss: 0.00001524
Iteration 57/1000 | Loss: 0.00001524
Iteration 58/1000 | Loss: 0.00001523
Iteration 59/1000 | Loss: 0.00001523
Iteration 60/1000 | Loss: 0.00001521
Iteration 61/1000 | Loss: 0.00001520
Iteration 62/1000 | Loss: 0.00001520
Iteration 63/1000 | Loss: 0.00001519
Iteration 64/1000 | Loss: 0.00001517
Iteration 65/1000 | Loss: 0.00001517
Iteration 66/1000 | Loss: 0.00001517
Iteration 67/1000 | Loss: 0.00001516
Iteration 68/1000 | Loss: 0.00001516
Iteration 69/1000 | Loss: 0.00001516
Iteration 70/1000 | Loss: 0.00001515
Iteration 71/1000 | Loss: 0.00001514
Iteration 72/1000 | Loss: 0.00001514
Iteration 73/1000 | Loss: 0.00001513
Iteration 74/1000 | Loss: 0.00001513
Iteration 75/1000 | Loss: 0.00001512
Iteration 76/1000 | Loss: 0.00001512
Iteration 77/1000 | Loss: 0.00001510
Iteration 78/1000 | Loss: 0.00001505
Iteration 79/1000 | Loss: 0.00001505
Iteration 80/1000 | Loss: 0.00001504
Iteration 81/1000 | Loss: 0.00001504
Iteration 82/1000 | Loss: 0.00001504
Iteration 83/1000 | Loss: 0.00001504
Iteration 84/1000 | Loss: 0.00001503
Iteration 85/1000 | Loss: 0.00001503
Iteration 86/1000 | Loss: 0.00001502
Iteration 87/1000 | Loss: 0.00001502
Iteration 88/1000 | Loss: 0.00001502
Iteration 89/1000 | Loss: 0.00001501
Iteration 90/1000 | Loss: 0.00001501
Iteration 91/1000 | Loss: 0.00001501
Iteration 92/1000 | Loss: 0.00001501
Iteration 93/1000 | Loss: 0.00001500
Iteration 94/1000 | Loss: 0.00001500
Iteration 95/1000 | Loss: 0.00001500
Iteration 96/1000 | Loss: 0.00001499
Iteration 97/1000 | Loss: 0.00001499
Iteration 98/1000 | Loss: 0.00001499
Iteration 99/1000 | Loss: 0.00001499
Iteration 100/1000 | Loss: 0.00001498
Iteration 101/1000 | Loss: 0.00001498
Iteration 102/1000 | Loss: 0.00001498
Iteration 103/1000 | Loss: 0.00001498
Iteration 104/1000 | Loss: 0.00001498
Iteration 105/1000 | Loss: 0.00001498
Iteration 106/1000 | Loss: 0.00001498
Iteration 107/1000 | Loss: 0.00001497
Iteration 108/1000 | Loss: 0.00001497
Iteration 109/1000 | Loss: 0.00001497
Iteration 110/1000 | Loss: 0.00001497
Iteration 111/1000 | Loss: 0.00001496
Iteration 112/1000 | Loss: 0.00001496
Iteration 113/1000 | Loss: 0.00001496
Iteration 114/1000 | Loss: 0.00001495
Iteration 115/1000 | Loss: 0.00001495
Iteration 116/1000 | Loss: 0.00001495
Iteration 117/1000 | Loss: 0.00001495
Iteration 118/1000 | Loss: 0.00001495
Iteration 119/1000 | Loss: 0.00001495
Iteration 120/1000 | Loss: 0.00001495
Iteration 121/1000 | Loss: 0.00001495
Iteration 122/1000 | Loss: 0.00001495
Iteration 123/1000 | Loss: 0.00001495
Iteration 124/1000 | Loss: 0.00001495
Iteration 125/1000 | Loss: 0.00001495
Iteration 126/1000 | Loss: 0.00001494
Iteration 127/1000 | Loss: 0.00001494
Iteration 128/1000 | Loss: 0.00001494
Iteration 129/1000 | Loss: 0.00001494
Iteration 130/1000 | Loss: 0.00001493
Iteration 131/1000 | Loss: 0.00001493
Iteration 132/1000 | Loss: 0.00001493
Iteration 133/1000 | Loss: 0.00001493
Iteration 134/1000 | Loss: 0.00001493
Iteration 135/1000 | Loss: 0.00001493
Iteration 136/1000 | Loss: 0.00001493
Iteration 137/1000 | Loss: 0.00001493
Iteration 138/1000 | Loss: 0.00001492
Iteration 139/1000 | Loss: 0.00001492
Iteration 140/1000 | Loss: 0.00001492
Iteration 141/1000 | Loss: 0.00001492
Iteration 142/1000 | Loss: 0.00001491
Iteration 143/1000 | Loss: 0.00001491
Iteration 144/1000 | Loss: 0.00001491
Iteration 145/1000 | Loss: 0.00001491
Iteration 146/1000 | Loss: 0.00001491
Iteration 147/1000 | Loss: 0.00001491
Iteration 148/1000 | Loss: 0.00001491
Iteration 149/1000 | Loss: 0.00001491
Iteration 150/1000 | Loss: 0.00001491
Iteration 151/1000 | Loss: 0.00001491
Iteration 152/1000 | Loss: 0.00001491
Iteration 153/1000 | Loss: 0.00001491
Iteration 154/1000 | Loss: 0.00001491
Iteration 155/1000 | Loss: 0.00001491
Iteration 156/1000 | Loss: 0.00001491
Iteration 157/1000 | Loss: 0.00001491
Iteration 158/1000 | Loss: 0.00001491
Iteration 159/1000 | Loss: 0.00001491
Iteration 160/1000 | Loss: 0.00001491
Iteration 161/1000 | Loss: 0.00001491
Iteration 162/1000 | Loss: 0.00001491
Iteration 163/1000 | Loss: 0.00001491
Iteration 164/1000 | Loss: 0.00001491
Iteration 165/1000 | Loss: 0.00001491
Iteration 166/1000 | Loss: 0.00001491
Iteration 167/1000 | Loss: 0.00001491
Iteration 168/1000 | Loss: 0.00001491
Iteration 169/1000 | Loss: 0.00001491
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.4909835044818465e-05, 1.4909835044818465e-05, 1.4909835044818465e-05, 1.4909835044818465e-05, 1.4909835044818465e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4909835044818465e-05

Optimization complete. Final v2v error: 3.2614355087280273 mm

Highest mean error: 3.6191625595092773 mm for frame 114

Lowest mean error: 2.989952564239502 mm for frame 124

Saving results

Total time: 37.62686085700989
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00808254
Iteration 2/25 | Loss: 0.00165996
Iteration 3/25 | Loss: 0.00141185
Iteration 4/25 | Loss: 0.00136597
Iteration 5/25 | Loss: 0.00135473
Iteration 6/25 | Loss: 0.00135111
Iteration 7/25 | Loss: 0.00134996
Iteration 8/25 | Loss: 0.00134960
Iteration 9/25 | Loss: 0.00134942
Iteration 10/25 | Loss: 0.00134931
Iteration 11/25 | Loss: 0.00135206
Iteration 12/25 | Loss: 0.00135041
Iteration 13/25 | Loss: 0.00134937
Iteration 14/25 | Loss: 0.00134883
Iteration 15/25 | Loss: 0.00134696
Iteration 16/25 | Loss: 0.00134629
Iteration 17/25 | Loss: 0.00134612
Iteration 18/25 | Loss: 0.00134609
Iteration 19/25 | Loss: 0.00134609
Iteration 20/25 | Loss: 0.00134609
Iteration 21/25 | Loss: 0.00134609
Iteration 22/25 | Loss: 0.00134609
Iteration 23/25 | Loss: 0.00134608
Iteration 24/25 | Loss: 0.00134608
Iteration 25/25 | Loss: 0.00134608

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.67452335
Iteration 2/25 | Loss: 0.00106601
Iteration 3/25 | Loss: 0.00106601
Iteration 4/25 | Loss: 0.00106601
Iteration 5/25 | Loss: 0.00106601
Iteration 6/25 | Loss: 0.00106601
Iteration 7/25 | Loss: 0.00106601
Iteration 8/25 | Loss: 0.00106601
Iteration 9/25 | Loss: 0.00106601
Iteration 10/25 | Loss: 0.00106601
Iteration 11/25 | Loss: 0.00106601
Iteration 12/25 | Loss: 0.00106601
Iteration 13/25 | Loss: 0.00106601
Iteration 14/25 | Loss: 0.00106601
Iteration 15/25 | Loss: 0.00106601
Iteration 16/25 | Loss: 0.00106601
Iteration 17/25 | Loss: 0.00106601
Iteration 18/25 | Loss: 0.00106601
Iteration 19/25 | Loss: 0.00106601
Iteration 20/25 | Loss: 0.00106601
Iteration 21/25 | Loss: 0.00106601
Iteration 22/25 | Loss: 0.00106601
Iteration 23/25 | Loss: 0.00106601
Iteration 24/25 | Loss: 0.00106601
Iteration 25/25 | Loss: 0.00106601

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106601
Iteration 2/1000 | Loss: 0.00006971
Iteration 3/1000 | Loss: 0.00004508
Iteration 4/1000 | Loss: 0.00003713
Iteration 5/1000 | Loss: 0.00003423
Iteration 6/1000 | Loss: 0.00003271
Iteration 7/1000 | Loss: 0.00003113
Iteration 8/1000 | Loss: 0.00003020
Iteration 9/1000 | Loss: 0.00002928
Iteration 10/1000 | Loss: 0.00002872
Iteration 11/1000 | Loss: 0.00002839
Iteration 12/1000 | Loss: 0.00002813
Iteration 13/1000 | Loss: 0.00002786
Iteration 14/1000 | Loss: 0.00002768
Iteration 15/1000 | Loss: 0.00002751
Iteration 16/1000 | Loss: 0.00002732
Iteration 17/1000 | Loss: 0.00002726
Iteration 18/1000 | Loss: 0.00002717
Iteration 19/1000 | Loss: 0.00002712
Iteration 20/1000 | Loss: 0.00002711
Iteration 21/1000 | Loss: 0.00002706
Iteration 22/1000 | Loss: 0.00002705
Iteration 23/1000 | Loss: 0.00002703
Iteration 24/1000 | Loss: 0.00002702
Iteration 25/1000 | Loss: 0.00002695
Iteration 26/1000 | Loss: 0.00002693
Iteration 27/1000 | Loss: 0.00002692
Iteration 28/1000 | Loss: 0.00002691
Iteration 29/1000 | Loss: 0.00002691
Iteration 30/1000 | Loss: 0.00002691
Iteration 31/1000 | Loss: 0.00002690
Iteration 32/1000 | Loss: 0.00002690
Iteration 33/1000 | Loss: 0.00002689
Iteration 34/1000 | Loss: 0.00002689
Iteration 35/1000 | Loss: 0.00002689
Iteration 36/1000 | Loss: 0.00002689
Iteration 37/1000 | Loss: 0.00002689
Iteration 38/1000 | Loss: 0.00002689
Iteration 39/1000 | Loss: 0.00002689
Iteration 40/1000 | Loss: 0.00002688
Iteration 41/1000 | Loss: 0.00002688
Iteration 42/1000 | Loss: 0.00002688
Iteration 43/1000 | Loss: 0.00002688
Iteration 44/1000 | Loss: 0.00002687
Iteration 45/1000 | Loss: 0.00002687
Iteration 46/1000 | Loss: 0.00002686
Iteration 47/1000 | Loss: 0.00002686
Iteration 48/1000 | Loss: 0.00002686
Iteration 49/1000 | Loss: 0.00002685
Iteration 50/1000 | Loss: 0.00002685
Iteration 51/1000 | Loss: 0.00002685
Iteration 52/1000 | Loss: 0.00002684
Iteration 53/1000 | Loss: 0.00002683
Iteration 54/1000 | Loss: 0.00002683
Iteration 55/1000 | Loss: 0.00002683
Iteration 56/1000 | Loss: 0.00002683
Iteration 57/1000 | Loss: 0.00002683
Iteration 58/1000 | Loss: 0.00002683
Iteration 59/1000 | Loss: 0.00002682
Iteration 60/1000 | Loss: 0.00002682
Iteration 61/1000 | Loss: 0.00002681
Iteration 62/1000 | Loss: 0.00002681
Iteration 63/1000 | Loss: 0.00002681
Iteration 64/1000 | Loss: 0.00002680
Iteration 65/1000 | Loss: 0.00002680
Iteration 66/1000 | Loss: 0.00002680
Iteration 67/1000 | Loss: 0.00002679
Iteration 68/1000 | Loss: 0.00002679
Iteration 69/1000 | Loss: 0.00002678
Iteration 70/1000 | Loss: 0.00002678
Iteration 71/1000 | Loss: 0.00002678
Iteration 72/1000 | Loss: 0.00002678
Iteration 73/1000 | Loss: 0.00002677
Iteration 74/1000 | Loss: 0.00002677
Iteration 75/1000 | Loss: 0.00002677
Iteration 76/1000 | Loss: 0.00002677
Iteration 77/1000 | Loss: 0.00002677
Iteration 78/1000 | Loss: 0.00002677
Iteration 79/1000 | Loss: 0.00002676
Iteration 80/1000 | Loss: 0.00002676
Iteration 81/1000 | Loss: 0.00002676
Iteration 82/1000 | Loss: 0.00002675
Iteration 83/1000 | Loss: 0.00002675
Iteration 84/1000 | Loss: 0.00002675
Iteration 85/1000 | Loss: 0.00002675
Iteration 86/1000 | Loss: 0.00002675
Iteration 87/1000 | Loss: 0.00002675
Iteration 88/1000 | Loss: 0.00002675
Iteration 89/1000 | Loss: 0.00002675
Iteration 90/1000 | Loss: 0.00002674
Iteration 91/1000 | Loss: 0.00002674
Iteration 92/1000 | Loss: 0.00002674
Iteration 93/1000 | Loss: 0.00002674
Iteration 94/1000 | Loss: 0.00002674
Iteration 95/1000 | Loss: 0.00002674
Iteration 96/1000 | Loss: 0.00002673
Iteration 97/1000 | Loss: 0.00002673
Iteration 98/1000 | Loss: 0.00002673
Iteration 99/1000 | Loss: 0.00002672
Iteration 100/1000 | Loss: 0.00002672
Iteration 101/1000 | Loss: 0.00002672
Iteration 102/1000 | Loss: 0.00002672
Iteration 103/1000 | Loss: 0.00002672
Iteration 104/1000 | Loss: 0.00002672
Iteration 105/1000 | Loss: 0.00002672
Iteration 106/1000 | Loss: 0.00002671
Iteration 107/1000 | Loss: 0.00002671
Iteration 108/1000 | Loss: 0.00002671
Iteration 109/1000 | Loss: 0.00002671
Iteration 110/1000 | Loss: 0.00002671
Iteration 111/1000 | Loss: 0.00002670
Iteration 112/1000 | Loss: 0.00002670
Iteration 113/1000 | Loss: 0.00002670
Iteration 114/1000 | Loss: 0.00002670
Iteration 115/1000 | Loss: 0.00002670
Iteration 116/1000 | Loss: 0.00002670
Iteration 117/1000 | Loss: 0.00002670
Iteration 118/1000 | Loss: 0.00002670
Iteration 119/1000 | Loss: 0.00002670
Iteration 120/1000 | Loss: 0.00002670
Iteration 121/1000 | Loss: 0.00002670
Iteration 122/1000 | Loss: 0.00002669
Iteration 123/1000 | Loss: 0.00002669
Iteration 124/1000 | Loss: 0.00002669
Iteration 125/1000 | Loss: 0.00002668
Iteration 126/1000 | Loss: 0.00002668
Iteration 127/1000 | Loss: 0.00002668
Iteration 128/1000 | Loss: 0.00002668
Iteration 129/1000 | Loss: 0.00002667
Iteration 130/1000 | Loss: 0.00002667
Iteration 131/1000 | Loss: 0.00002667
Iteration 132/1000 | Loss: 0.00002667
Iteration 133/1000 | Loss: 0.00002666
Iteration 134/1000 | Loss: 0.00002666
Iteration 135/1000 | Loss: 0.00002666
Iteration 136/1000 | Loss: 0.00002666
Iteration 137/1000 | Loss: 0.00002666
Iteration 138/1000 | Loss: 0.00002666
Iteration 139/1000 | Loss: 0.00002666
Iteration 140/1000 | Loss: 0.00002666
Iteration 141/1000 | Loss: 0.00002666
Iteration 142/1000 | Loss: 0.00002666
Iteration 143/1000 | Loss: 0.00002666
Iteration 144/1000 | Loss: 0.00002666
Iteration 145/1000 | Loss: 0.00002666
Iteration 146/1000 | Loss: 0.00002665
Iteration 147/1000 | Loss: 0.00002665
Iteration 148/1000 | Loss: 0.00002665
Iteration 149/1000 | Loss: 0.00002665
Iteration 150/1000 | Loss: 0.00002665
Iteration 151/1000 | Loss: 0.00002664
Iteration 152/1000 | Loss: 0.00002664
Iteration 153/1000 | Loss: 0.00002664
Iteration 154/1000 | Loss: 0.00002664
Iteration 155/1000 | Loss: 0.00002664
Iteration 156/1000 | Loss: 0.00002663
Iteration 157/1000 | Loss: 0.00002663
Iteration 158/1000 | Loss: 0.00002663
Iteration 159/1000 | Loss: 0.00002663
Iteration 160/1000 | Loss: 0.00002663
Iteration 161/1000 | Loss: 0.00002663
Iteration 162/1000 | Loss: 0.00002663
Iteration 163/1000 | Loss: 0.00002663
Iteration 164/1000 | Loss: 0.00002663
Iteration 165/1000 | Loss: 0.00002663
Iteration 166/1000 | Loss: 0.00002663
Iteration 167/1000 | Loss: 0.00002663
Iteration 168/1000 | Loss: 0.00002662
Iteration 169/1000 | Loss: 0.00002662
Iteration 170/1000 | Loss: 0.00002662
Iteration 171/1000 | Loss: 0.00002662
Iteration 172/1000 | Loss: 0.00002662
Iteration 173/1000 | Loss: 0.00002662
Iteration 174/1000 | Loss: 0.00002661
Iteration 175/1000 | Loss: 0.00002661
Iteration 176/1000 | Loss: 0.00002661
Iteration 177/1000 | Loss: 0.00002661
Iteration 178/1000 | Loss: 0.00002661
Iteration 179/1000 | Loss: 0.00002660
Iteration 180/1000 | Loss: 0.00002660
Iteration 181/1000 | Loss: 0.00002660
Iteration 182/1000 | Loss: 0.00002660
Iteration 183/1000 | Loss: 0.00002660
Iteration 184/1000 | Loss: 0.00002660
Iteration 185/1000 | Loss: 0.00002659
Iteration 186/1000 | Loss: 0.00002659
Iteration 187/1000 | Loss: 0.00002659
Iteration 188/1000 | Loss: 0.00002659
Iteration 189/1000 | Loss: 0.00002659
Iteration 190/1000 | Loss: 0.00002659
Iteration 191/1000 | Loss: 0.00002659
Iteration 192/1000 | Loss: 0.00002659
Iteration 193/1000 | Loss: 0.00002659
Iteration 194/1000 | Loss: 0.00002659
Iteration 195/1000 | Loss: 0.00002659
Iteration 196/1000 | Loss: 0.00002659
Iteration 197/1000 | Loss: 0.00002659
Iteration 198/1000 | Loss: 0.00002659
Iteration 199/1000 | Loss: 0.00002658
Iteration 200/1000 | Loss: 0.00002658
Iteration 201/1000 | Loss: 0.00002658
Iteration 202/1000 | Loss: 0.00002658
Iteration 203/1000 | Loss: 0.00002658
Iteration 204/1000 | Loss: 0.00002658
Iteration 205/1000 | Loss: 0.00002658
Iteration 206/1000 | Loss: 0.00002658
Iteration 207/1000 | Loss: 0.00002658
Iteration 208/1000 | Loss: 0.00002658
Iteration 209/1000 | Loss: 0.00002658
Iteration 210/1000 | Loss: 0.00002658
Iteration 211/1000 | Loss: 0.00002658
Iteration 212/1000 | Loss: 0.00002658
Iteration 213/1000 | Loss: 0.00002658
Iteration 214/1000 | Loss: 0.00002658
Iteration 215/1000 | Loss: 0.00002657
Iteration 216/1000 | Loss: 0.00002657
Iteration 217/1000 | Loss: 0.00002657
Iteration 218/1000 | Loss: 0.00002657
Iteration 219/1000 | Loss: 0.00002657
Iteration 220/1000 | Loss: 0.00002657
Iteration 221/1000 | Loss: 0.00002657
Iteration 222/1000 | Loss: 0.00002657
Iteration 223/1000 | Loss: 0.00002657
Iteration 224/1000 | Loss: 0.00002657
Iteration 225/1000 | Loss: 0.00002657
Iteration 226/1000 | Loss: 0.00002657
Iteration 227/1000 | Loss: 0.00002657
Iteration 228/1000 | Loss: 0.00002657
Iteration 229/1000 | Loss: 0.00002657
Iteration 230/1000 | Loss: 0.00002657
Iteration 231/1000 | Loss: 0.00002657
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 231. Stopping optimization.
Last 5 losses: [2.657475124578923e-05, 2.657475124578923e-05, 2.657475124578923e-05, 2.657475124578923e-05, 2.657475124578923e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.657475124578923e-05

Optimization complete. Final v2v error: 4.330686569213867 mm

Highest mean error: 5.8860883712768555 mm for frame 50

Lowest mean error: 3.3685808181762695 mm for frame 27

Saving results

Total time: 68.57683491706848
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00460280
Iteration 2/25 | Loss: 0.00142071
Iteration 3/25 | Loss: 0.00130262
Iteration 4/25 | Loss: 0.00128884
Iteration 5/25 | Loss: 0.00128405
Iteration 6/25 | Loss: 0.00128392
Iteration 7/25 | Loss: 0.00128392
Iteration 8/25 | Loss: 0.00128392
Iteration 9/25 | Loss: 0.00128392
Iteration 10/25 | Loss: 0.00128392
Iteration 11/25 | Loss: 0.00128392
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012839208357036114, 0.0012839208357036114, 0.0012839208357036114, 0.0012839208357036114, 0.0012839208357036114]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012839208357036114

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46767628
Iteration 2/25 | Loss: 0.00089797
Iteration 3/25 | Loss: 0.00089797
Iteration 4/25 | Loss: 0.00089797
Iteration 5/25 | Loss: 0.00089797
Iteration 6/25 | Loss: 0.00089797
Iteration 7/25 | Loss: 0.00089797
Iteration 8/25 | Loss: 0.00089797
Iteration 9/25 | Loss: 0.00089797
Iteration 10/25 | Loss: 0.00089797
Iteration 11/25 | Loss: 0.00089797
Iteration 12/25 | Loss: 0.00089797
Iteration 13/25 | Loss: 0.00089797
Iteration 14/25 | Loss: 0.00089797
Iteration 15/25 | Loss: 0.00089797
Iteration 16/25 | Loss: 0.00089797
Iteration 17/25 | Loss: 0.00089797
Iteration 18/25 | Loss: 0.00089797
Iteration 19/25 | Loss: 0.00089797
Iteration 20/25 | Loss: 0.00089797
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008979656849987805, 0.0008979656849987805, 0.0008979656849987805, 0.0008979656849987805, 0.0008979656849987805]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008979656849987805

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089797
Iteration 2/1000 | Loss: 0.00002825
Iteration 3/1000 | Loss: 0.00002074
Iteration 4/1000 | Loss: 0.00001915
Iteration 5/1000 | Loss: 0.00001809
Iteration 6/1000 | Loss: 0.00001743
Iteration 7/1000 | Loss: 0.00001707
Iteration 8/1000 | Loss: 0.00001660
Iteration 9/1000 | Loss: 0.00001626
Iteration 10/1000 | Loss: 0.00001609
Iteration 11/1000 | Loss: 0.00001604
Iteration 12/1000 | Loss: 0.00001586
Iteration 13/1000 | Loss: 0.00001581
Iteration 14/1000 | Loss: 0.00001581
Iteration 15/1000 | Loss: 0.00001578
Iteration 16/1000 | Loss: 0.00001576
Iteration 17/1000 | Loss: 0.00001567
Iteration 18/1000 | Loss: 0.00001567
Iteration 19/1000 | Loss: 0.00001562
Iteration 20/1000 | Loss: 0.00001551
Iteration 21/1000 | Loss: 0.00001545
Iteration 22/1000 | Loss: 0.00001545
Iteration 23/1000 | Loss: 0.00001542
Iteration 24/1000 | Loss: 0.00001541
Iteration 25/1000 | Loss: 0.00001540
Iteration 26/1000 | Loss: 0.00001539
Iteration 27/1000 | Loss: 0.00001537
Iteration 28/1000 | Loss: 0.00001537
Iteration 29/1000 | Loss: 0.00001537
Iteration 30/1000 | Loss: 0.00001537
Iteration 31/1000 | Loss: 0.00001537
Iteration 32/1000 | Loss: 0.00001537
Iteration 33/1000 | Loss: 0.00001537
Iteration 34/1000 | Loss: 0.00001537
Iteration 35/1000 | Loss: 0.00001537
Iteration 36/1000 | Loss: 0.00001537
Iteration 37/1000 | Loss: 0.00001537
Iteration 38/1000 | Loss: 0.00001537
Iteration 39/1000 | Loss: 0.00001537
Iteration 40/1000 | Loss: 0.00001537
Iteration 41/1000 | Loss: 0.00001537
Iteration 42/1000 | Loss: 0.00001537
Iteration 43/1000 | Loss: 0.00001537
Iteration 44/1000 | Loss: 0.00001537
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 44. Stopping optimization.
Last 5 losses: [1.5374562281067483e-05, 1.5374562281067483e-05, 1.5374562281067483e-05, 1.5374562281067483e-05, 1.5374562281067483e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5374562281067483e-05

Optimization complete. Final v2v error: 3.3323752880096436 mm

Highest mean error: 4.197198390960693 mm for frame 159

Lowest mean error: 2.9405274391174316 mm for frame 10

Saving results

Total time: 32.730077266693115
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00990101
Iteration 2/25 | Loss: 0.00240892
Iteration 3/25 | Loss: 0.00185373
Iteration 4/25 | Loss: 0.00169427
Iteration 5/25 | Loss: 0.00160057
Iteration 6/25 | Loss: 0.00160436
Iteration 7/25 | Loss: 0.00159530
Iteration 8/25 | Loss: 0.00158154
Iteration 9/25 | Loss: 0.00151604
Iteration 10/25 | Loss: 0.00149783
Iteration 11/25 | Loss: 0.00146318
Iteration 12/25 | Loss: 0.00142691
Iteration 13/25 | Loss: 0.00141605
Iteration 14/25 | Loss: 0.00139714
Iteration 15/25 | Loss: 0.00139935
Iteration 16/25 | Loss: 0.00139794
Iteration 17/25 | Loss: 0.00139341
Iteration 18/25 | Loss: 0.00138550
Iteration 19/25 | Loss: 0.00138182
Iteration 20/25 | Loss: 0.00138315
Iteration 21/25 | Loss: 0.00138292
Iteration 22/25 | Loss: 0.00138144
Iteration 23/25 | Loss: 0.00137980
Iteration 24/25 | Loss: 0.00137930
Iteration 25/25 | Loss: 0.00137888

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.12145066
Iteration 2/25 | Loss: 0.00143050
Iteration 3/25 | Loss: 0.00128185
Iteration 4/25 | Loss: 0.00128185
Iteration 5/25 | Loss: 0.00128185
Iteration 6/25 | Loss: 0.00128185
Iteration 7/25 | Loss: 0.00128185
Iteration 8/25 | Loss: 0.00128184
Iteration 9/25 | Loss: 0.00128184
Iteration 10/25 | Loss: 0.00128184
Iteration 11/25 | Loss: 0.00128184
Iteration 12/25 | Loss: 0.00128184
Iteration 13/25 | Loss: 0.00128184
Iteration 14/25 | Loss: 0.00128184
Iteration 15/25 | Loss: 0.00128184
Iteration 16/25 | Loss: 0.00128184
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001281843869946897, 0.001281843869946897, 0.001281843869946897, 0.001281843869946897, 0.001281843869946897]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001281843869946897

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128184
Iteration 2/1000 | Loss: 0.00034845
Iteration 3/1000 | Loss: 0.00007268
Iteration 4/1000 | Loss: 0.00004594
Iteration 5/1000 | Loss: 0.00002985
Iteration 6/1000 | Loss: 0.00002747
Iteration 7/1000 | Loss: 0.00002971
Iteration 8/1000 | Loss: 0.00031228
Iteration 9/1000 | Loss: 0.00003395
Iteration 10/1000 | Loss: 0.00003484
Iteration 11/1000 | Loss: 0.00002233
Iteration 12/1000 | Loss: 0.00002166
Iteration 13/1000 | Loss: 0.00002120
Iteration 14/1000 | Loss: 0.00002074
Iteration 15/1000 | Loss: 0.00002027
Iteration 16/1000 | Loss: 0.00001992
Iteration 17/1000 | Loss: 0.00001973
Iteration 18/1000 | Loss: 0.00011254
Iteration 19/1000 | Loss: 0.00001985
Iteration 20/1000 | Loss: 0.00001959
Iteration 21/1000 | Loss: 0.00001955
Iteration 22/1000 | Loss: 0.00001954
Iteration 23/1000 | Loss: 0.00001952
Iteration 24/1000 | Loss: 0.00001952
Iteration 25/1000 | Loss: 0.00001941
Iteration 26/1000 | Loss: 0.00001940
Iteration 27/1000 | Loss: 0.00001940
Iteration 28/1000 | Loss: 0.00001939
Iteration 29/1000 | Loss: 0.00001938
Iteration 30/1000 | Loss: 0.00001937
Iteration 31/1000 | Loss: 0.00001936
Iteration 32/1000 | Loss: 0.00012696
Iteration 33/1000 | Loss: 0.00002444
Iteration 34/1000 | Loss: 0.00006543
Iteration 35/1000 | Loss: 0.00001938
Iteration 36/1000 | Loss: 0.00001926
Iteration 37/1000 | Loss: 0.00001926
Iteration 38/1000 | Loss: 0.00001926
Iteration 39/1000 | Loss: 0.00001926
Iteration 40/1000 | Loss: 0.00001925
Iteration 41/1000 | Loss: 0.00001925
Iteration 42/1000 | Loss: 0.00001925
Iteration 43/1000 | Loss: 0.00001925
Iteration 44/1000 | Loss: 0.00001924
Iteration 45/1000 | Loss: 0.00001924
Iteration 46/1000 | Loss: 0.00001924
Iteration 47/1000 | Loss: 0.00001923
Iteration 48/1000 | Loss: 0.00001923
Iteration 49/1000 | Loss: 0.00001923
Iteration 50/1000 | Loss: 0.00001923
Iteration 51/1000 | Loss: 0.00001923
Iteration 52/1000 | Loss: 0.00001923
Iteration 53/1000 | Loss: 0.00001923
Iteration 54/1000 | Loss: 0.00001923
Iteration 55/1000 | Loss: 0.00001922
Iteration 56/1000 | Loss: 0.00001922
Iteration 57/1000 | Loss: 0.00001922
Iteration 58/1000 | Loss: 0.00001920
Iteration 59/1000 | Loss: 0.00001920
Iteration 60/1000 | Loss: 0.00001920
Iteration 61/1000 | Loss: 0.00001920
Iteration 62/1000 | Loss: 0.00001920
Iteration 63/1000 | Loss: 0.00001920
Iteration 64/1000 | Loss: 0.00001920
Iteration 65/1000 | Loss: 0.00001920
Iteration 66/1000 | Loss: 0.00001920
Iteration 67/1000 | Loss: 0.00001919
Iteration 68/1000 | Loss: 0.00001919
Iteration 69/1000 | Loss: 0.00001919
Iteration 70/1000 | Loss: 0.00001919
Iteration 71/1000 | Loss: 0.00001919
Iteration 72/1000 | Loss: 0.00001919
Iteration 73/1000 | Loss: 0.00001918
Iteration 74/1000 | Loss: 0.00001917
Iteration 75/1000 | Loss: 0.00001917
Iteration 76/1000 | Loss: 0.00001917
Iteration 77/1000 | Loss: 0.00001917
Iteration 78/1000 | Loss: 0.00001917
Iteration 79/1000 | Loss: 0.00001917
Iteration 80/1000 | Loss: 0.00001917
Iteration 81/1000 | Loss: 0.00001917
Iteration 82/1000 | Loss: 0.00001917
Iteration 83/1000 | Loss: 0.00001917
Iteration 84/1000 | Loss: 0.00001917
Iteration 85/1000 | Loss: 0.00001916
Iteration 86/1000 | Loss: 0.00001916
Iteration 87/1000 | Loss: 0.00001916
Iteration 88/1000 | Loss: 0.00001916
Iteration 89/1000 | Loss: 0.00001916
Iteration 90/1000 | Loss: 0.00001916
Iteration 91/1000 | Loss: 0.00001916
Iteration 92/1000 | Loss: 0.00001916
Iteration 93/1000 | Loss: 0.00001916
Iteration 94/1000 | Loss: 0.00001915
Iteration 95/1000 | Loss: 0.00001915
Iteration 96/1000 | Loss: 0.00001915
Iteration 97/1000 | Loss: 0.00001915
Iteration 98/1000 | Loss: 0.00001915
Iteration 99/1000 | Loss: 0.00001914
Iteration 100/1000 | Loss: 0.00001914
Iteration 101/1000 | Loss: 0.00001914
Iteration 102/1000 | Loss: 0.00001913
Iteration 103/1000 | Loss: 0.00001913
Iteration 104/1000 | Loss: 0.00001913
Iteration 105/1000 | Loss: 0.00001913
Iteration 106/1000 | Loss: 0.00001913
Iteration 107/1000 | Loss: 0.00001913
Iteration 108/1000 | Loss: 0.00001912
Iteration 109/1000 | Loss: 0.00001912
Iteration 110/1000 | Loss: 0.00001912
Iteration 111/1000 | Loss: 0.00001912
Iteration 112/1000 | Loss: 0.00001911
Iteration 113/1000 | Loss: 0.00001911
Iteration 114/1000 | Loss: 0.00001911
Iteration 115/1000 | Loss: 0.00001911
Iteration 116/1000 | Loss: 0.00001911
Iteration 117/1000 | Loss: 0.00001910
Iteration 118/1000 | Loss: 0.00001910
Iteration 119/1000 | Loss: 0.00001910
Iteration 120/1000 | Loss: 0.00001910
Iteration 121/1000 | Loss: 0.00001910
Iteration 122/1000 | Loss: 0.00001910
Iteration 123/1000 | Loss: 0.00001910
Iteration 124/1000 | Loss: 0.00001910
Iteration 125/1000 | Loss: 0.00001910
Iteration 126/1000 | Loss: 0.00001910
Iteration 127/1000 | Loss: 0.00001910
Iteration 128/1000 | Loss: 0.00001910
Iteration 129/1000 | Loss: 0.00001910
Iteration 130/1000 | Loss: 0.00001909
Iteration 131/1000 | Loss: 0.00001909
Iteration 132/1000 | Loss: 0.00001909
Iteration 133/1000 | Loss: 0.00001909
Iteration 134/1000 | Loss: 0.00001909
Iteration 135/1000 | Loss: 0.00001909
Iteration 136/1000 | Loss: 0.00001908
Iteration 137/1000 | Loss: 0.00001908
Iteration 138/1000 | Loss: 0.00001908
Iteration 139/1000 | Loss: 0.00001908
Iteration 140/1000 | Loss: 0.00001908
Iteration 141/1000 | Loss: 0.00001908
Iteration 142/1000 | Loss: 0.00001908
Iteration 143/1000 | Loss: 0.00001907
Iteration 144/1000 | Loss: 0.00001907
Iteration 145/1000 | Loss: 0.00001907
Iteration 146/1000 | Loss: 0.00001907
Iteration 147/1000 | Loss: 0.00001907
Iteration 148/1000 | Loss: 0.00001907
Iteration 149/1000 | Loss: 0.00001907
Iteration 150/1000 | Loss: 0.00001907
Iteration 151/1000 | Loss: 0.00001907
Iteration 152/1000 | Loss: 0.00001907
Iteration 153/1000 | Loss: 0.00001907
Iteration 154/1000 | Loss: 0.00001907
Iteration 155/1000 | Loss: 0.00001906
Iteration 156/1000 | Loss: 0.00001906
Iteration 157/1000 | Loss: 0.00001906
Iteration 158/1000 | Loss: 0.00001906
Iteration 159/1000 | Loss: 0.00001906
Iteration 160/1000 | Loss: 0.00001906
Iteration 161/1000 | Loss: 0.00001906
Iteration 162/1000 | Loss: 0.00001906
Iteration 163/1000 | Loss: 0.00001906
Iteration 164/1000 | Loss: 0.00001906
Iteration 165/1000 | Loss: 0.00001906
Iteration 166/1000 | Loss: 0.00001906
Iteration 167/1000 | Loss: 0.00001905
Iteration 168/1000 | Loss: 0.00001905
Iteration 169/1000 | Loss: 0.00001905
Iteration 170/1000 | Loss: 0.00001905
Iteration 171/1000 | Loss: 0.00001905
Iteration 172/1000 | Loss: 0.00001905
Iteration 173/1000 | Loss: 0.00001905
Iteration 174/1000 | Loss: 0.00001904
Iteration 175/1000 | Loss: 0.00001904
Iteration 176/1000 | Loss: 0.00001904
Iteration 177/1000 | Loss: 0.00001904
Iteration 178/1000 | Loss: 0.00001904
Iteration 179/1000 | Loss: 0.00001904
Iteration 180/1000 | Loss: 0.00001904
Iteration 181/1000 | Loss: 0.00001904
Iteration 182/1000 | Loss: 0.00001904
Iteration 183/1000 | Loss: 0.00001904
Iteration 184/1000 | Loss: 0.00013063
Iteration 185/1000 | Loss: 0.00001925
Iteration 186/1000 | Loss: 0.00001906
Iteration 187/1000 | Loss: 0.00001905
Iteration 188/1000 | Loss: 0.00001905
Iteration 189/1000 | Loss: 0.00001905
Iteration 190/1000 | Loss: 0.00001905
Iteration 191/1000 | Loss: 0.00001904
Iteration 192/1000 | Loss: 0.00001904
Iteration 193/1000 | Loss: 0.00001904
Iteration 194/1000 | Loss: 0.00001904
Iteration 195/1000 | Loss: 0.00001904
Iteration 196/1000 | Loss: 0.00001904
Iteration 197/1000 | Loss: 0.00001904
Iteration 198/1000 | Loss: 0.00001904
Iteration 199/1000 | Loss: 0.00001904
Iteration 200/1000 | Loss: 0.00001903
Iteration 201/1000 | Loss: 0.00001903
Iteration 202/1000 | Loss: 0.00001903
Iteration 203/1000 | Loss: 0.00001903
Iteration 204/1000 | Loss: 0.00001903
Iteration 205/1000 | Loss: 0.00001903
Iteration 206/1000 | Loss: 0.00001902
Iteration 207/1000 | Loss: 0.00001902
Iteration 208/1000 | Loss: 0.00001902
Iteration 209/1000 | Loss: 0.00001902
Iteration 210/1000 | Loss: 0.00001901
Iteration 211/1000 | Loss: 0.00001901
Iteration 212/1000 | Loss: 0.00001901
Iteration 213/1000 | Loss: 0.00001901
Iteration 214/1000 | Loss: 0.00001901
Iteration 215/1000 | Loss: 0.00001901
Iteration 216/1000 | Loss: 0.00001901
Iteration 217/1000 | Loss: 0.00001901
Iteration 218/1000 | Loss: 0.00001901
Iteration 219/1000 | Loss: 0.00001900
Iteration 220/1000 | Loss: 0.00001900
Iteration 221/1000 | Loss: 0.00001900
Iteration 222/1000 | Loss: 0.00001900
Iteration 223/1000 | Loss: 0.00001900
Iteration 224/1000 | Loss: 0.00001900
Iteration 225/1000 | Loss: 0.00001900
Iteration 226/1000 | Loss: 0.00001900
Iteration 227/1000 | Loss: 0.00001900
Iteration 228/1000 | Loss: 0.00001900
Iteration 229/1000 | Loss: 0.00001900
Iteration 230/1000 | Loss: 0.00001900
Iteration 231/1000 | Loss: 0.00001900
Iteration 232/1000 | Loss: 0.00001900
Iteration 233/1000 | Loss: 0.00001900
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 233. Stopping optimization.
Last 5 losses: [1.9002727640327066e-05, 1.9002727640327066e-05, 1.9002727640327066e-05, 1.9002727640327066e-05, 1.9002727640327066e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9002727640327066e-05

Optimization complete. Final v2v error: 3.655344009399414 mm

Highest mean error: 3.993145704269409 mm for frame 51

Lowest mean error: 3.2720251083374023 mm for frame 15

Saving results

Total time: 93.46005845069885
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00420004
Iteration 2/25 | Loss: 0.00140207
Iteration 3/25 | Loss: 0.00131887
Iteration 4/25 | Loss: 0.00130622
Iteration 5/25 | Loss: 0.00130209
Iteration 6/25 | Loss: 0.00130188
Iteration 7/25 | Loss: 0.00130188
Iteration 8/25 | Loss: 0.00130188
Iteration 9/25 | Loss: 0.00130188
Iteration 10/25 | Loss: 0.00130188
Iteration 11/25 | Loss: 0.00130188
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013018844183534384, 0.0013018844183534384, 0.0013018844183534384, 0.0013018844183534384, 0.0013018844183534384]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013018844183534384

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40085232
Iteration 2/25 | Loss: 0.00103249
Iteration 3/25 | Loss: 0.00103249
Iteration 4/25 | Loss: 0.00103249
Iteration 5/25 | Loss: 0.00103248
Iteration 6/25 | Loss: 0.00103248
Iteration 7/25 | Loss: 0.00103248
Iteration 8/25 | Loss: 0.00103248
Iteration 9/25 | Loss: 0.00103248
Iteration 10/25 | Loss: 0.00103248
Iteration 11/25 | Loss: 0.00103248
Iteration 12/25 | Loss: 0.00103248
Iteration 13/25 | Loss: 0.00103248
Iteration 14/25 | Loss: 0.00103248
Iteration 15/25 | Loss: 0.00103248
Iteration 16/25 | Loss: 0.00103248
Iteration 17/25 | Loss: 0.00103248
Iteration 18/25 | Loss: 0.00103248
Iteration 19/25 | Loss: 0.00103248
Iteration 20/25 | Loss: 0.00103248
Iteration 21/25 | Loss: 0.00103248
Iteration 22/25 | Loss: 0.00103248
Iteration 23/25 | Loss: 0.00103248
Iteration 24/25 | Loss: 0.00103248
Iteration 25/25 | Loss: 0.00103248
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0010324828326702118, 0.0010324828326702118, 0.0010324828326702118, 0.0010324828326702118, 0.0010324828326702118]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010324828326702118

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103248
Iteration 2/1000 | Loss: 0.00002798
Iteration 3/1000 | Loss: 0.00002092
Iteration 4/1000 | Loss: 0.00001874
Iteration 5/1000 | Loss: 0.00001750
Iteration 6/1000 | Loss: 0.00001667
Iteration 7/1000 | Loss: 0.00001622
Iteration 8/1000 | Loss: 0.00001610
Iteration 9/1000 | Loss: 0.00001574
Iteration 10/1000 | Loss: 0.00001543
Iteration 11/1000 | Loss: 0.00001521
Iteration 12/1000 | Loss: 0.00001515
Iteration 13/1000 | Loss: 0.00001512
Iteration 14/1000 | Loss: 0.00001506
Iteration 15/1000 | Loss: 0.00001503
Iteration 16/1000 | Loss: 0.00001494
Iteration 17/1000 | Loss: 0.00001494
Iteration 18/1000 | Loss: 0.00001494
Iteration 19/1000 | Loss: 0.00001486
Iteration 20/1000 | Loss: 0.00001485
Iteration 21/1000 | Loss: 0.00001484
Iteration 22/1000 | Loss: 0.00001483
Iteration 23/1000 | Loss: 0.00001483
Iteration 24/1000 | Loss: 0.00001482
Iteration 25/1000 | Loss: 0.00001477
Iteration 26/1000 | Loss: 0.00001476
Iteration 27/1000 | Loss: 0.00001475
Iteration 28/1000 | Loss: 0.00001467
Iteration 29/1000 | Loss: 0.00001467
Iteration 30/1000 | Loss: 0.00001460
Iteration 31/1000 | Loss: 0.00001454
Iteration 32/1000 | Loss: 0.00001450
Iteration 33/1000 | Loss: 0.00001450
Iteration 34/1000 | Loss: 0.00001444
Iteration 35/1000 | Loss: 0.00001442
Iteration 36/1000 | Loss: 0.00001442
Iteration 37/1000 | Loss: 0.00001442
Iteration 38/1000 | Loss: 0.00001441
Iteration 39/1000 | Loss: 0.00001441
Iteration 40/1000 | Loss: 0.00001439
Iteration 41/1000 | Loss: 0.00001439
Iteration 42/1000 | Loss: 0.00001439
Iteration 43/1000 | Loss: 0.00001439
Iteration 44/1000 | Loss: 0.00001438
Iteration 45/1000 | Loss: 0.00001438
Iteration 46/1000 | Loss: 0.00001437
Iteration 47/1000 | Loss: 0.00001437
Iteration 48/1000 | Loss: 0.00001437
Iteration 49/1000 | Loss: 0.00001436
Iteration 50/1000 | Loss: 0.00001436
Iteration 51/1000 | Loss: 0.00001436
Iteration 52/1000 | Loss: 0.00001435
Iteration 53/1000 | Loss: 0.00001434
Iteration 54/1000 | Loss: 0.00001433
Iteration 55/1000 | Loss: 0.00001433
Iteration 56/1000 | Loss: 0.00001432
Iteration 57/1000 | Loss: 0.00001428
Iteration 58/1000 | Loss: 0.00001428
Iteration 59/1000 | Loss: 0.00001425
Iteration 60/1000 | Loss: 0.00001425
Iteration 61/1000 | Loss: 0.00001424
Iteration 62/1000 | Loss: 0.00001424
Iteration 63/1000 | Loss: 0.00001424
Iteration 64/1000 | Loss: 0.00001424
Iteration 65/1000 | Loss: 0.00001424
Iteration 66/1000 | Loss: 0.00001424
Iteration 67/1000 | Loss: 0.00001424
Iteration 68/1000 | Loss: 0.00001424
Iteration 69/1000 | Loss: 0.00001424
Iteration 70/1000 | Loss: 0.00001424
Iteration 71/1000 | Loss: 0.00001423
Iteration 72/1000 | Loss: 0.00001423
Iteration 73/1000 | Loss: 0.00001423
Iteration 74/1000 | Loss: 0.00001422
Iteration 75/1000 | Loss: 0.00001422
Iteration 76/1000 | Loss: 0.00001422
Iteration 77/1000 | Loss: 0.00001422
Iteration 78/1000 | Loss: 0.00001421
Iteration 79/1000 | Loss: 0.00001421
Iteration 80/1000 | Loss: 0.00001421
Iteration 81/1000 | Loss: 0.00001421
Iteration 82/1000 | Loss: 0.00001421
Iteration 83/1000 | Loss: 0.00001421
Iteration 84/1000 | Loss: 0.00001420
Iteration 85/1000 | Loss: 0.00001420
Iteration 86/1000 | Loss: 0.00001420
Iteration 87/1000 | Loss: 0.00001420
Iteration 88/1000 | Loss: 0.00001420
Iteration 89/1000 | Loss: 0.00001420
Iteration 90/1000 | Loss: 0.00001420
Iteration 91/1000 | Loss: 0.00001420
Iteration 92/1000 | Loss: 0.00001420
Iteration 93/1000 | Loss: 0.00001420
Iteration 94/1000 | Loss: 0.00001419
Iteration 95/1000 | Loss: 0.00001419
Iteration 96/1000 | Loss: 0.00001419
Iteration 97/1000 | Loss: 0.00001419
Iteration 98/1000 | Loss: 0.00001419
Iteration 99/1000 | Loss: 0.00001418
Iteration 100/1000 | Loss: 0.00001418
Iteration 101/1000 | Loss: 0.00001418
Iteration 102/1000 | Loss: 0.00001418
Iteration 103/1000 | Loss: 0.00001418
Iteration 104/1000 | Loss: 0.00001418
Iteration 105/1000 | Loss: 0.00001417
Iteration 106/1000 | Loss: 0.00001417
Iteration 107/1000 | Loss: 0.00001417
Iteration 108/1000 | Loss: 0.00001417
Iteration 109/1000 | Loss: 0.00001416
Iteration 110/1000 | Loss: 0.00001416
Iteration 111/1000 | Loss: 0.00001416
Iteration 112/1000 | Loss: 0.00001416
Iteration 113/1000 | Loss: 0.00001415
Iteration 114/1000 | Loss: 0.00001415
Iteration 115/1000 | Loss: 0.00001415
Iteration 116/1000 | Loss: 0.00001415
Iteration 117/1000 | Loss: 0.00001415
Iteration 118/1000 | Loss: 0.00001415
Iteration 119/1000 | Loss: 0.00001415
Iteration 120/1000 | Loss: 0.00001415
Iteration 121/1000 | Loss: 0.00001415
Iteration 122/1000 | Loss: 0.00001414
Iteration 123/1000 | Loss: 0.00001414
Iteration 124/1000 | Loss: 0.00001414
Iteration 125/1000 | Loss: 0.00001414
Iteration 126/1000 | Loss: 0.00001414
Iteration 127/1000 | Loss: 0.00001414
Iteration 128/1000 | Loss: 0.00001414
Iteration 129/1000 | Loss: 0.00001414
Iteration 130/1000 | Loss: 0.00001414
Iteration 131/1000 | Loss: 0.00001414
Iteration 132/1000 | Loss: 0.00001414
Iteration 133/1000 | Loss: 0.00001414
Iteration 134/1000 | Loss: 0.00001413
Iteration 135/1000 | Loss: 0.00001413
Iteration 136/1000 | Loss: 0.00001413
Iteration 137/1000 | Loss: 0.00001413
Iteration 138/1000 | Loss: 0.00001413
Iteration 139/1000 | Loss: 0.00001413
Iteration 140/1000 | Loss: 0.00001413
Iteration 141/1000 | Loss: 0.00001413
Iteration 142/1000 | Loss: 0.00001413
Iteration 143/1000 | Loss: 0.00001413
Iteration 144/1000 | Loss: 0.00001413
Iteration 145/1000 | Loss: 0.00001413
Iteration 146/1000 | Loss: 0.00001413
Iteration 147/1000 | Loss: 0.00001412
Iteration 148/1000 | Loss: 0.00001412
Iteration 149/1000 | Loss: 0.00001412
Iteration 150/1000 | Loss: 0.00001412
Iteration 151/1000 | Loss: 0.00001412
Iteration 152/1000 | Loss: 0.00001412
Iteration 153/1000 | Loss: 0.00001412
Iteration 154/1000 | Loss: 0.00001412
Iteration 155/1000 | Loss: 0.00001412
Iteration 156/1000 | Loss: 0.00001412
Iteration 157/1000 | Loss: 0.00001412
Iteration 158/1000 | Loss: 0.00001412
Iteration 159/1000 | Loss: 0.00001412
Iteration 160/1000 | Loss: 0.00001412
Iteration 161/1000 | Loss: 0.00001412
Iteration 162/1000 | Loss: 0.00001412
Iteration 163/1000 | Loss: 0.00001412
Iteration 164/1000 | Loss: 0.00001412
Iteration 165/1000 | Loss: 0.00001412
Iteration 166/1000 | Loss: 0.00001411
Iteration 167/1000 | Loss: 0.00001411
Iteration 168/1000 | Loss: 0.00001411
Iteration 169/1000 | Loss: 0.00001411
Iteration 170/1000 | Loss: 0.00001411
Iteration 171/1000 | Loss: 0.00001411
Iteration 172/1000 | Loss: 0.00001411
Iteration 173/1000 | Loss: 0.00001411
Iteration 174/1000 | Loss: 0.00001411
Iteration 175/1000 | Loss: 0.00001410
Iteration 176/1000 | Loss: 0.00001410
Iteration 177/1000 | Loss: 0.00001410
Iteration 178/1000 | Loss: 0.00001410
Iteration 179/1000 | Loss: 0.00001410
Iteration 180/1000 | Loss: 0.00001410
Iteration 181/1000 | Loss: 0.00001410
Iteration 182/1000 | Loss: 0.00001410
Iteration 183/1000 | Loss: 0.00001410
Iteration 184/1000 | Loss: 0.00001410
Iteration 185/1000 | Loss: 0.00001410
Iteration 186/1000 | Loss: 0.00001410
Iteration 187/1000 | Loss: 0.00001410
Iteration 188/1000 | Loss: 0.00001410
Iteration 189/1000 | Loss: 0.00001410
Iteration 190/1000 | Loss: 0.00001410
Iteration 191/1000 | Loss: 0.00001410
Iteration 192/1000 | Loss: 0.00001410
Iteration 193/1000 | Loss: 0.00001410
Iteration 194/1000 | Loss: 0.00001410
Iteration 195/1000 | Loss: 0.00001410
Iteration 196/1000 | Loss: 0.00001410
Iteration 197/1000 | Loss: 0.00001410
Iteration 198/1000 | Loss: 0.00001410
Iteration 199/1000 | Loss: 0.00001410
Iteration 200/1000 | Loss: 0.00001410
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [1.41017908390495e-05, 1.41017908390495e-05, 1.41017908390495e-05, 1.41017908390495e-05, 1.41017908390495e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.41017908390495e-05

Optimization complete. Final v2v error: 3.2003796100616455 mm

Highest mean error: 3.5803866386413574 mm for frame 232

Lowest mean error: 2.9679033756256104 mm for frame 165

Saving results

Total time: 46.98468828201294
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00990322
Iteration 2/25 | Loss: 0.00223471
Iteration 3/25 | Loss: 0.00163382
Iteration 4/25 | Loss: 0.00153032
Iteration 5/25 | Loss: 0.00154229
Iteration 6/25 | Loss: 0.00151005
Iteration 7/25 | Loss: 0.00148909
Iteration 8/25 | Loss: 0.00143753
Iteration 9/25 | Loss: 0.00139498
Iteration 10/25 | Loss: 0.00139135
Iteration 11/25 | Loss: 0.00137160
Iteration 12/25 | Loss: 0.00136948
Iteration 13/25 | Loss: 0.00135753
Iteration 14/25 | Loss: 0.00135070
Iteration 15/25 | Loss: 0.00134350
Iteration 16/25 | Loss: 0.00134235
Iteration 17/25 | Loss: 0.00134897
Iteration 18/25 | Loss: 0.00134248
Iteration 19/25 | Loss: 0.00133857
Iteration 20/25 | Loss: 0.00134352
Iteration 21/25 | Loss: 0.00133618
Iteration 22/25 | Loss: 0.00133124
Iteration 23/25 | Loss: 0.00132804
Iteration 24/25 | Loss: 0.00132581
Iteration 25/25 | Loss: 0.00132475

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41040444
Iteration 2/25 | Loss: 0.00097035
Iteration 3/25 | Loss: 0.00097034
Iteration 4/25 | Loss: 0.00094192
Iteration 5/25 | Loss: 0.00094192
Iteration 6/25 | Loss: 0.00094192
Iteration 7/25 | Loss: 0.00094192
Iteration 8/25 | Loss: 0.00094192
Iteration 9/25 | Loss: 0.00094192
Iteration 10/25 | Loss: 0.00094192
Iteration 11/25 | Loss: 0.00094192
Iteration 12/25 | Loss: 0.00094192
Iteration 13/25 | Loss: 0.00094192
Iteration 14/25 | Loss: 0.00094192
Iteration 15/25 | Loss: 0.00094192
Iteration 16/25 | Loss: 0.00094192
Iteration 17/25 | Loss: 0.00094192
Iteration 18/25 | Loss: 0.00094192
Iteration 19/25 | Loss: 0.00094192
Iteration 20/25 | Loss: 0.00094192
Iteration 21/25 | Loss: 0.00094192
Iteration 22/25 | Loss: 0.00094192
Iteration 23/25 | Loss: 0.00094192
Iteration 24/25 | Loss: 0.00094192
Iteration 25/25 | Loss: 0.00094192

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094192
Iteration 2/1000 | Loss: 0.00005228
Iteration 3/1000 | Loss: 0.00006791
Iteration 4/1000 | Loss: 0.00003281
Iteration 5/1000 | Loss: 0.00018020
Iteration 6/1000 | Loss: 0.00016130
Iteration 7/1000 | Loss: 0.00003471
Iteration 8/1000 | Loss: 0.00004134
Iteration 9/1000 | Loss: 0.00004298
Iteration 10/1000 | Loss: 0.00005291
Iteration 11/1000 | Loss: 0.00004372
Iteration 12/1000 | Loss: 0.00003993
Iteration 13/1000 | Loss: 0.00008835
Iteration 14/1000 | Loss: 0.00006746
Iteration 15/1000 | Loss: 0.00004190
Iteration 16/1000 | Loss: 0.00006065
Iteration 17/1000 | Loss: 0.00006660
Iteration 18/1000 | Loss: 0.00006278
Iteration 19/1000 | Loss: 0.00005922
Iteration 20/1000 | Loss: 0.00002477
Iteration 21/1000 | Loss: 0.00004377
Iteration 22/1000 | Loss: 0.00004666
Iteration 23/1000 | Loss: 0.00005419
Iteration 24/1000 | Loss: 0.00006214
Iteration 25/1000 | Loss: 0.00009738
Iteration 26/1000 | Loss: 0.00020632
Iteration 27/1000 | Loss: 0.00003517
Iteration 28/1000 | Loss: 0.00004681
Iteration 29/1000 | Loss: 0.00010003
Iteration 30/1000 | Loss: 0.00004455
Iteration 31/1000 | Loss: 0.00006881
Iteration 32/1000 | Loss: 0.00005220
Iteration 33/1000 | Loss: 0.00006028
Iteration 34/1000 | Loss: 0.00005039
Iteration 35/1000 | Loss: 0.00003648
Iteration 36/1000 | Loss: 0.00003767
Iteration 37/1000 | Loss: 0.00009620
Iteration 38/1000 | Loss: 0.00007132
Iteration 39/1000 | Loss: 0.00004611
Iteration 40/1000 | Loss: 0.00006467
Iteration 41/1000 | Loss: 0.00004238
Iteration 42/1000 | Loss: 0.00006229
Iteration 43/1000 | Loss: 0.00027923
Iteration 44/1000 | Loss: 0.00007718
Iteration 45/1000 | Loss: 0.00005281
Iteration 46/1000 | Loss: 0.00013322
Iteration 47/1000 | Loss: 0.00007106
Iteration 48/1000 | Loss: 0.00003050
Iteration 49/1000 | Loss: 0.00004525
Iteration 50/1000 | Loss: 0.00006789
Iteration 51/1000 | Loss: 0.00004096
Iteration 52/1000 | Loss: 0.00002153
Iteration 53/1000 | Loss: 0.00006189
Iteration 54/1000 | Loss: 0.00002086
Iteration 55/1000 | Loss: 0.00002034
Iteration 56/1000 | Loss: 0.00001962
Iteration 57/1000 | Loss: 0.00016469
Iteration 58/1000 | Loss: 0.00004269
Iteration 59/1000 | Loss: 0.00024230
Iteration 60/1000 | Loss: 0.00034517
Iteration 61/1000 | Loss: 0.00002447
Iteration 62/1000 | Loss: 0.00019516
Iteration 63/1000 | Loss: 0.00002381
Iteration 64/1000 | Loss: 0.00001882
Iteration 65/1000 | Loss: 0.00004737
Iteration 66/1000 | Loss: 0.00001877
Iteration 67/1000 | Loss: 0.00001859
Iteration 68/1000 | Loss: 0.00001857
Iteration 69/1000 | Loss: 0.00001857
Iteration 70/1000 | Loss: 0.00001856
Iteration 71/1000 | Loss: 0.00001856
Iteration 72/1000 | Loss: 0.00001855
Iteration 73/1000 | Loss: 0.00001855
Iteration 74/1000 | Loss: 0.00001855
Iteration 75/1000 | Loss: 0.00001854
Iteration 76/1000 | Loss: 0.00001849
Iteration 77/1000 | Loss: 0.00013401
Iteration 78/1000 | Loss: 0.00002283
Iteration 79/1000 | Loss: 0.00013769
Iteration 80/1000 | Loss: 0.00015795
Iteration 81/1000 | Loss: 0.00018037
Iteration 82/1000 | Loss: 0.00011438
Iteration 83/1000 | Loss: 0.00001881
Iteration 84/1000 | Loss: 0.00001852
Iteration 85/1000 | Loss: 0.00005352
Iteration 86/1000 | Loss: 0.00001850
Iteration 87/1000 | Loss: 0.00002752
Iteration 88/1000 | Loss: 0.00001834
Iteration 89/1000 | Loss: 0.00001834
Iteration 90/1000 | Loss: 0.00001834
Iteration 91/1000 | Loss: 0.00001834
Iteration 92/1000 | Loss: 0.00001834
Iteration 93/1000 | Loss: 0.00001834
Iteration 94/1000 | Loss: 0.00001834
Iteration 95/1000 | Loss: 0.00001833
Iteration 96/1000 | Loss: 0.00001833
Iteration 97/1000 | Loss: 0.00001833
Iteration 98/1000 | Loss: 0.00001833
Iteration 99/1000 | Loss: 0.00001833
Iteration 100/1000 | Loss: 0.00001833
Iteration 101/1000 | Loss: 0.00001832
Iteration 102/1000 | Loss: 0.00001829
Iteration 103/1000 | Loss: 0.00001829
Iteration 104/1000 | Loss: 0.00001829
Iteration 105/1000 | Loss: 0.00001829
Iteration 106/1000 | Loss: 0.00001829
Iteration 107/1000 | Loss: 0.00001829
Iteration 108/1000 | Loss: 0.00001829
Iteration 109/1000 | Loss: 0.00001829
Iteration 110/1000 | Loss: 0.00001829
Iteration 111/1000 | Loss: 0.00001828
Iteration 112/1000 | Loss: 0.00001828
Iteration 113/1000 | Loss: 0.00001828
Iteration 114/1000 | Loss: 0.00001828
Iteration 115/1000 | Loss: 0.00001826
Iteration 116/1000 | Loss: 0.00001826
Iteration 117/1000 | Loss: 0.00001826
Iteration 118/1000 | Loss: 0.00001826
Iteration 119/1000 | Loss: 0.00001825
Iteration 120/1000 | Loss: 0.00001825
Iteration 121/1000 | Loss: 0.00001824
Iteration 122/1000 | Loss: 0.00001824
Iteration 123/1000 | Loss: 0.00001824
Iteration 124/1000 | Loss: 0.00001824
Iteration 125/1000 | Loss: 0.00001823
Iteration 126/1000 | Loss: 0.00001823
Iteration 127/1000 | Loss: 0.00005949
Iteration 128/1000 | Loss: 0.00001841
Iteration 129/1000 | Loss: 0.00002700
Iteration 130/1000 | Loss: 0.00001825
Iteration 131/1000 | Loss: 0.00001824
Iteration 132/1000 | Loss: 0.00002786
Iteration 133/1000 | Loss: 0.00001824
Iteration 134/1000 | Loss: 0.00001823
Iteration 135/1000 | Loss: 0.00001823
Iteration 136/1000 | Loss: 0.00001823
Iteration 137/1000 | Loss: 0.00001822
Iteration 138/1000 | Loss: 0.00001822
Iteration 139/1000 | Loss: 0.00001822
Iteration 140/1000 | Loss: 0.00001822
Iteration 141/1000 | Loss: 0.00001822
Iteration 142/1000 | Loss: 0.00001822
Iteration 143/1000 | Loss: 0.00001822
Iteration 144/1000 | Loss: 0.00001822
Iteration 145/1000 | Loss: 0.00001821
Iteration 146/1000 | Loss: 0.00001821
Iteration 147/1000 | Loss: 0.00001821
Iteration 148/1000 | Loss: 0.00001821
Iteration 149/1000 | Loss: 0.00001821
Iteration 150/1000 | Loss: 0.00001821
Iteration 151/1000 | Loss: 0.00001821
Iteration 152/1000 | Loss: 0.00001821
Iteration 153/1000 | Loss: 0.00001821
Iteration 154/1000 | Loss: 0.00001821
Iteration 155/1000 | Loss: 0.00001821
Iteration 156/1000 | Loss: 0.00001821
Iteration 157/1000 | Loss: 0.00001821
Iteration 158/1000 | Loss: 0.00001821
Iteration 159/1000 | Loss: 0.00001821
Iteration 160/1000 | Loss: 0.00001821
Iteration 161/1000 | Loss: 0.00001821
Iteration 162/1000 | Loss: 0.00001821
Iteration 163/1000 | Loss: 0.00001821
Iteration 164/1000 | Loss: 0.00001821
Iteration 165/1000 | Loss: 0.00001821
Iteration 166/1000 | Loss: 0.00001820
Iteration 167/1000 | Loss: 0.00001820
Iteration 168/1000 | Loss: 0.00001820
Iteration 169/1000 | Loss: 0.00001820
Iteration 170/1000 | Loss: 0.00001820
Iteration 171/1000 | Loss: 0.00001820
Iteration 172/1000 | Loss: 0.00001820
Iteration 173/1000 | Loss: 0.00001820
Iteration 174/1000 | Loss: 0.00001820
Iteration 175/1000 | Loss: 0.00001820
Iteration 176/1000 | Loss: 0.00001820
Iteration 177/1000 | Loss: 0.00001820
Iteration 178/1000 | Loss: 0.00001820
Iteration 179/1000 | Loss: 0.00001820
Iteration 180/1000 | Loss: 0.00001819
Iteration 181/1000 | Loss: 0.00001819
Iteration 182/1000 | Loss: 0.00001819
Iteration 183/1000 | Loss: 0.00001819
Iteration 184/1000 | Loss: 0.00001819
Iteration 185/1000 | Loss: 0.00001819
Iteration 186/1000 | Loss: 0.00001819
Iteration 187/1000 | Loss: 0.00001818
Iteration 188/1000 | Loss: 0.00001818
Iteration 189/1000 | Loss: 0.00001818
Iteration 190/1000 | Loss: 0.00001818
Iteration 191/1000 | Loss: 0.00001818
Iteration 192/1000 | Loss: 0.00001817
Iteration 193/1000 | Loss: 0.00001817
Iteration 194/1000 | Loss: 0.00001817
Iteration 195/1000 | Loss: 0.00001817
Iteration 196/1000 | Loss: 0.00001817
Iteration 197/1000 | Loss: 0.00001817
Iteration 198/1000 | Loss: 0.00001816
Iteration 199/1000 | Loss: 0.00001816
Iteration 200/1000 | Loss: 0.00001816
Iteration 201/1000 | Loss: 0.00001816
Iteration 202/1000 | Loss: 0.00001815
Iteration 203/1000 | Loss: 0.00001815
Iteration 204/1000 | Loss: 0.00001815
Iteration 205/1000 | Loss: 0.00001815
Iteration 206/1000 | Loss: 0.00001815
Iteration 207/1000 | Loss: 0.00001815
Iteration 208/1000 | Loss: 0.00001815
Iteration 209/1000 | Loss: 0.00001815
Iteration 210/1000 | Loss: 0.00001815
Iteration 211/1000 | Loss: 0.00001815
Iteration 212/1000 | Loss: 0.00001815
Iteration 213/1000 | Loss: 0.00001815
Iteration 214/1000 | Loss: 0.00001814
Iteration 215/1000 | Loss: 0.00001814
Iteration 216/1000 | Loss: 0.00001814
Iteration 217/1000 | Loss: 0.00001814
Iteration 218/1000 | Loss: 0.00001814
Iteration 219/1000 | Loss: 0.00001814
Iteration 220/1000 | Loss: 0.00001814
Iteration 221/1000 | Loss: 0.00001814
Iteration 222/1000 | Loss: 0.00001814
Iteration 223/1000 | Loss: 0.00001814
Iteration 224/1000 | Loss: 0.00001814
Iteration 225/1000 | Loss: 0.00001814
Iteration 226/1000 | Loss: 0.00001813
Iteration 227/1000 | Loss: 0.00001813
Iteration 228/1000 | Loss: 0.00001813
Iteration 229/1000 | Loss: 0.00001813
Iteration 230/1000 | Loss: 0.00001813
Iteration 231/1000 | Loss: 0.00001813
Iteration 232/1000 | Loss: 0.00001813
Iteration 233/1000 | Loss: 0.00001813
Iteration 234/1000 | Loss: 0.00001813
Iteration 235/1000 | Loss: 0.00001813
Iteration 236/1000 | Loss: 0.00001813
Iteration 237/1000 | Loss: 0.00001813
Iteration 238/1000 | Loss: 0.00001813
Iteration 239/1000 | Loss: 0.00001812
Iteration 240/1000 | Loss: 0.00001812
Iteration 241/1000 | Loss: 0.00001812
Iteration 242/1000 | Loss: 0.00001812
Iteration 243/1000 | Loss: 0.00001812
Iteration 244/1000 | Loss: 0.00001812
Iteration 245/1000 | Loss: 0.00001812
Iteration 246/1000 | Loss: 0.00001812
Iteration 247/1000 | Loss: 0.00001812
Iteration 248/1000 | Loss: 0.00001812
Iteration 249/1000 | Loss: 0.00001811
Iteration 250/1000 | Loss: 0.00001811
Iteration 251/1000 | Loss: 0.00001811
Iteration 252/1000 | Loss: 0.00001811
Iteration 253/1000 | Loss: 0.00001811
Iteration 254/1000 | Loss: 0.00001811
Iteration 255/1000 | Loss: 0.00001811
Iteration 256/1000 | Loss: 0.00001811
Iteration 257/1000 | Loss: 0.00001811
Iteration 258/1000 | Loss: 0.00001811
Iteration 259/1000 | Loss: 0.00001811
Iteration 260/1000 | Loss: 0.00001811
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 260. Stopping optimization.
Last 5 losses: [1.811037327570375e-05, 1.811037327570375e-05, 1.811037327570375e-05, 1.811037327570375e-05, 1.811037327570375e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.811037327570375e-05

Optimization complete. Final v2v error: 3.4118270874023438 mm

Highest mean error: 5.8365936279296875 mm for frame 42

Lowest mean error: 2.9746932983398438 mm for frame 13

Saving results

Total time: 165.55782318115234
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01001493
Iteration 2/25 | Loss: 0.00256623
Iteration 3/25 | Loss: 0.00178038
Iteration 4/25 | Loss: 0.00171536
Iteration 5/25 | Loss: 0.00167666
Iteration 6/25 | Loss: 0.00158222
Iteration 7/25 | Loss: 0.00150762
Iteration 8/25 | Loss: 0.00157170
Iteration 9/25 | Loss: 0.00156392
Iteration 10/25 | Loss: 0.00148024
Iteration 11/25 | Loss: 0.00147628
Iteration 12/25 | Loss: 0.00149085
Iteration 13/25 | Loss: 0.00144459
Iteration 14/25 | Loss: 0.00138028
Iteration 15/25 | Loss: 0.00137035
Iteration 16/25 | Loss: 0.00137624
Iteration 17/25 | Loss: 0.00135842
Iteration 18/25 | Loss: 0.00135491
Iteration 19/25 | Loss: 0.00135139
Iteration 20/25 | Loss: 0.00134652
Iteration 21/25 | Loss: 0.00133936
Iteration 22/25 | Loss: 0.00133720
Iteration 23/25 | Loss: 0.00133629
Iteration 24/25 | Loss: 0.00133324
Iteration 25/25 | Loss: 0.00133225

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63969052
Iteration 2/25 | Loss: 0.00096455
Iteration 3/25 | Loss: 0.00095980
Iteration 4/25 | Loss: 0.00095980
Iteration 5/25 | Loss: 0.00095980
Iteration 6/25 | Loss: 0.00095980
Iteration 7/25 | Loss: 0.00095980
Iteration 8/25 | Loss: 0.00095980
Iteration 9/25 | Loss: 0.00095980
Iteration 10/25 | Loss: 0.00095980
Iteration 11/25 | Loss: 0.00095980
Iteration 12/25 | Loss: 0.00095980
Iteration 13/25 | Loss: 0.00095980
Iteration 14/25 | Loss: 0.00095980
Iteration 15/25 | Loss: 0.00095980
Iteration 16/25 | Loss: 0.00095980
Iteration 17/25 | Loss: 0.00095980
Iteration 18/25 | Loss: 0.00095980
Iteration 19/25 | Loss: 0.00095980
Iteration 20/25 | Loss: 0.00095980
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.000959798926487565, 0.000959798926487565, 0.000959798926487565, 0.000959798926487565, 0.000959798926487565]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000959798926487565

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095980
Iteration 2/1000 | Loss: 0.00004198
Iteration 3/1000 | Loss: 0.00002727
Iteration 4/1000 | Loss: 0.00002565
Iteration 5/1000 | Loss: 0.00019659
Iteration 6/1000 | Loss: 0.00003026
Iteration 7/1000 | Loss: 0.00001830
Iteration 8/1000 | Loss: 0.00001729
Iteration 9/1000 | Loss: 0.00001706
Iteration 10/1000 | Loss: 0.00002620
Iteration 11/1000 | Loss: 0.00001653
Iteration 12/1000 | Loss: 0.00001754
Iteration 13/1000 | Loss: 0.00001642
Iteration 14/1000 | Loss: 0.00001641
Iteration 15/1000 | Loss: 0.00001641
Iteration 16/1000 | Loss: 0.00001639
Iteration 17/1000 | Loss: 0.00001638
Iteration 18/1000 | Loss: 0.00001638
Iteration 19/1000 | Loss: 0.00001637
Iteration 20/1000 | Loss: 0.00001842
Iteration 21/1000 | Loss: 0.00001632
Iteration 22/1000 | Loss: 0.00001630
Iteration 23/1000 | Loss: 0.00001628
Iteration 24/1000 | Loss: 0.00001627
Iteration 25/1000 | Loss: 0.00001608
Iteration 26/1000 | Loss: 0.00001608
Iteration 27/1000 | Loss: 0.00001604
Iteration 28/1000 | Loss: 0.00001597
Iteration 29/1000 | Loss: 0.00001593
Iteration 30/1000 | Loss: 0.00001591
Iteration 31/1000 | Loss: 0.00001586
Iteration 32/1000 | Loss: 0.00001579
Iteration 33/1000 | Loss: 0.00001576
Iteration 34/1000 | Loss: 0.00001576
Iteration 35/1000 | Loss: 0.00001576
Iteration 36/1000 | Loss: 0.00001576
Iteration 37/1000 | Loss: 0.00001576
Iteration 38/1000 | Loss: 0.00001576
Iteration 39/1000 | Loss: 0.00001576
Iteration 40/1000 | Loss: 0.00002031
Iteration 41/1000 | Loss: 0.00001572
Iteration 42/1000 | Loss: 0.00001571
Iteration 43/1000 | Loss: 0.00001571
Iteration 44/1000 | Loss: 0.00001570
Iteration 45/1000 | Loss: 0.00001570
Iteration 46/1000 | Loss: 0.00001570
Iteration 47/1000 | Loss: 0.00001570
Iteration 48/1000 | Loss: 0.00001570
Iteration 49/1000 | Loss: 0.00001570
Iteration 50/1000 | Loss: 0.00001570
Iteration 51/1000 | Loss: 0.00001570
Iteration 52/1000 | Loss: 0.00001570
Iteration 53/1000 | Loss: 0.00001569
Iteration 54/1000 | Loss: 0.00001569
Iteration 55/1000 | Loss: 0.00001568
Iteration 56/1000 | Loss: 0.00001861
Iteration 57/1000 | Loss: 0.00001565
Iteration 58/1000 | Loss: 0.00001565
Iteration 59/1000 | Loss: 0.00001565
Iteration 60/1000 | Loss: 0.00001564
Iteration 61/1000 | Loss: 0.00001564
Iteration 62/1000 | Loss: 0.00001564
Iteration 63/1000 | Loss: 0.00001564
Iteration 64/1000 | Loss: 0.00001564
Iteration 65/1000 | Loss: 0.00001564
Iteration 66/1000 | Loss: 0.00001564
Iteration 67/1000 | Loss: 0.00001564
Iteration 68/1000 | Loss: 0.00001564
Iteration 69/1000 | Loss: 0.00001742
Iteration 70/1000 | Loss: 0.00001563
Iteration 71/1000 | Loss: 0.00001563
Iteration 72/1000 | Loss: 0.00001563
Iteration 73/1000 | Loss: 0.00001563
Iteration 74/1000 | Loss: 0.00001563
Iteration 75/1000 | Loss: 0.00001563
Iteration 76/1000 | Loss: 0.00001562
Iteration 77/1000 | Loss: 0.00001562
Iteration 78/1000 | Loss: 0.00001562
Iteration 79/1000 | Loss: 0.00001562
Iteration 80/1000 | Loss: 0.00001561
Iteration 81/1000 | Loss: 0.00001561
Iteration 82/1000 | Loss: 0.00001561
Iteration 83/1000 | Loss: 0.00001561
Iteration 84/1000 | Loss: 0.00001561
Iteration 85/1000 | Loss: 0.00001561
Iteration 86/1000 | Loss: 0.00001561
Iteration 87/1000 | Loss: 0.00001560
Iteration 88/1000 | Loss: 0.00001560
Iteration 89/1000 | Loss: 0.00001560
Iteration 90/1000 | Loss: 0.00001559
Iteration 91/1000 | Loss: 0.00001559
Iteration 92/1000 | Loss: 0.00001559
Iteration 93/1000 | Loss: 0.00001558
Iteration 94/1000 | Loss: 0.00001558
Iteration 95/1000 | Loss: 0.00001558
Iteration 96/1000 | Loss: 0.00001558
Iteration 97/1000 | Loss: 0.00001557
Iteration 98/1000 | Loss: 0.00001557
Iteration 99/1000 | Loss: 0.00001557
Iteration 100/1000 | Loss: 0.00001556
Iteration 101/1000 | Loss: 0.00001555
Iteration 102/1000 | Loss: 0.00001554
Iteration 103/1000 | Loss: 0.00001554
Iteration 104/1000 | Loss: 0.00001554
Iteration 105/1000 | Loss: 0.00001554
Iteration 106/1000 | Loss: 0.00001554
Iteration 107/1000 | Loss: 0.00001554
Iteration 108/1000 | Loss: 0.00001553
Iteration 109/1000 | Loss: 0.00001553
Iteration 110/1000 | Loss: 0.00001553
Iteration 111/1000 | Loss: 0.00001553
Iteration 112/1000 | Loss: 0.00001552
Iteration 113/1000 | Loss: 0.00001552
Iteration 114/1000 | Loss: 0.00001551
Iteration 115/1000 | Loss: 0.00001551
Iteration 116/1000 | Loss: 0.00001550
Iteration 117/1000 | Loss: 0.00001550
Iteration 118/1000 | Loss: 0.00001550
Iteration 119/1000 | Loss: 0.00001549
Iteration 120/1000 | Loss: 0.00001549
Iteration 121/1000 | Loss: 0.00001549
Iteration 122/1000 | Loss: 0.00001548
Iteration 123/1000 | Loss: 0.00001548
Iteration 124/1000 | Loss: 0.00001548
Iteration 125/1000 | Loss: 0.00001548
Iteration 126/1000 | Loss: 0.00001548
Iteration 127/1000 | Loss: 0.00001548
Iteration 128/1000 | Loss: 0.00001548
Iteration 129/1000 | Loss: 0.00001547
Iteration 130/1000 | Loss: 0.00001547
Iteration 131/1000 | Loss: 0.00001547
Iteration 132/1000 | Loss: 0.00001547
Iteration 133/1000 | Loss: 0.00001547
Iteration 134/1000 | Loss: 0.00001547
Iteration 135/1000 | Loss: 0.00001547
Iteration 136/1000 | Loss: 0.00001547
Iteration 137/1000 | Loss: 0.00001547
Iteration 138/1000 | Loss: 0.00001546
Iteration 139/1000 | Loss: 0.00001546
Iteration 140/1000 | Loss: 0.00001546
Iteration 141/1000 | Loss: 0.00001546
Iteration 142/1000 | Loss: 0.00001545
Iteration 143/1000 | Loss: 0.00001545
Iteration 144/1000 | Loss: 0.00001545
Iteration 145/1000 | Loss: 0.00001545
Iteration 146/1000 | Loss: 0.00001545
Iteration 147/1000 | Loss: 0.00001545
Iteration 148/1000 | Loss: 0.00001545
Iteration 149/1000 | Loss: 0.00001545
Iteration 150/1000 | Loss: 0.00001545
Iteration 151/1000 | Loss: 0.00001544
Iteration 152/1000 | Loss: 0.00001544
Iteration 153/1000 | Loss: 0.00001544
Iteration 154/1000 | Loss: 0.00001544
Iteration 155/1000 | Loss: 0.00001544
Iteration 156/1000 | Loss: 0.00001544
Iteration 157/1000 | Loss: 0.00001544
Iteration 158/1000 | Loss: 0.00001544
Iteration 159/1000 | Loss: 0.00001544
Iteration 160/1000 | Loss: 0.00001544
Iteration 161/1000 | Loss: 0.00001544
Iteration 162/1000 | Loss: 0.00001544
Iteration 163/1000 | Loss: 0.00001544
Iteration 164/1000 | Loss: 0.00001544
Iteration 165/1000 | Loss: 0.00001544
Iteration 166/1000 | Loss: 0.00001544
Iteration 167/1000 | Loss: 0.00001544
Iteration 168/1000 | Loss: 0.00001544
Iteration 169/1000 | Loss: 0.00001544
Iteration 170/1000 | Loss: 0.00001544
Iteration 171/1000 | Loss: 0.00001544
Iteration 172/1000 | Loss: 0.00001544
Iteration 173/1000 | Loss: 0.00001544
Iteration 174/1000 | Loss: 0.00001544
Iteration 175/1000 | Loss: 0.00001544
Iteration 176/1000 | Loss: 0.00001544
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.5443663869518787e-05, 1.5443663869518787e-05, 1.5443663869518787e-05, 1.5443663869518787e-05, 1.5443663869518787e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5443663869518787e-05

Optimization complete. Final v2v error: 3.3378090858459473 mm

Highest mean error: 3.8787682056427 mm for frame 71

Lowest mean error: 2.978026866912842 mm for frame 41

Saving results

Total time: 79.93626666069031
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01031124
Iteration 2/25 | Loss: 0.00255175
Iteration 3/25 | Loss: 0.00175397
Iteration 4/25 | Loss: 0.00153153
Iteration 5/25 | Loss: 0.00147415
Iteration 6/25 | Loss: 0.00149574
Iteration 7/25 | Loss: 0.00140990
Iteration 8/25 | Loss: 0.00139536
Iteration 9/25 | Loss: 0.00135257
Iteration 10/25 | Loss: 0.00134090
Iteration 11/25 | Loss: 0.00135819
Iteration 12/25 | Loss: 0.00134584
Iteration 13/25 | Loss: 0.00133694
Iteration 14/25 | Loss: 0.00132820
Iteration 15/25 | Loss: 0.00132455
Iteration 16/25 | Loss: 0.00132629
Iteration 17/25 | Loss: 0.00132749
Iteration 18/25 | Loss: 0.00131786
Iteration 19/25 | Loss: 0.00130721
Iteration 20/25 | Loss: 0.00130976
Iteration 21/25 | Loss: 0.00130355
Iteration 22/25 | Loss: 0.00129616
Iteration 23/25 | Loss: 0.00129607
Iteration 24/25 | Loss: 0.00129410
Iteration 25/25 | Loss: 0.00129396

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.81744599
Iteration 2/25 | Loss: 0.00091839
Iteration 3/25 | Loss: 0.00092252
Iteration 4/25 | Loss: 0.00091838
Iteration 5/25 | Loss: 0.00091837
Iteration 6/25 | Loss: 0.00091837
Iteration 7/25 | Loss: 0.00091837
Iteration 8/25 | Loss: 0.00091837
Iteration 9/25 | Loss: 0.00091837
Iteration 10/25 | Loss: 0.00091837
Iteration 11/25 | Loss: 0.00091837
Iteration 12/25 | Loss: 0.00091837
Iteration 13/25 | Loss: 0.00091837
Iteration 14/25 | Loss: 0.00091837
Iteration 15/25 | Loss: 0.00091837
Iteration 16/25 | Loss: 0.00091837
Iteration 17/25 | Loss: 0.00091837
Iteration 18/25 | Loss: 0.00091837
Iteration 19/25 | Loss: 0.00091837
Iteration 20/25 | Loss: 0.00091837
Iteration 21/25 | Loss: 0.00091837
Iteration 22/25 | Loss: 0.00091837
Iteration 23/25 | Loss: 0.00091837
Iteration 24/25 | Loss: 0.00091837
Iteration 25/25 | Loss: 0.00091837

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091837
Iteration 2/1000 | Loss: 0.00005000
Iteration 3/1000 | Loss: 0.00007153
Iteration 4/1000 | Loss: 0.00004573
Iteration 5/1000 | Loss: 0.00004733
Iteration 6/1000 | Loss: 0.00003403
Iteration 7/1000 | Loss: 0.00005135
Iteration 8/1000 | Loss: 0.00004895
Iteration 9/1000 | Loss: 0.00005216
Iteration 10/1000 | Loss: 0.00012547
Iteration 11/1000 | Loss: 0.00005399
Iteration 12/1000 | Loss: 0.00005487
Iteration 13/1000 | Loss: 0.00005578
Iteration 14/1000 | Loss: 0.00005256
Iteration 15/1000 | Loss: 0.00005356
Iteration 16/1000 | Loss: 0.00005319
Iteration 17/1000 | Loss: 0.00013115
Iteration 18/1000 | Loss: 0.00005161
Iteration 19/1000 | Loss: 0.00005732
Iteration 20/1000 | Loss: 0.00005657
Iteration 21/1000 | Loss: 0.00004948
Iteration 22/1000 | Loss: 0.00006198
Iteration 23/1000 | Loss: 0.00003319
Iteration 24/1000 | Loss: 0.00006784
Iteration 25/1000 | Loss: 0.00006892
Iteration 26/1000 | Loss: 0.00009442
Iteration 27/1000 | Loss: 0.00008881
Iteration 28/1000 | Loss: 0.00006592
Iteration 29/1000 | Loss: 0.00005269
Iteration 30/1000 | Loss: 0.00006172
Iteration 31/1000 | Loss: 0.00005730
Iteration 32/1000 | Loss: 0.00008479
Iteration 33/1000 | Loss: 0.00005741
Iteration 34/1000 | Loss: 0.00008239
Iteration 35/1000 | Loss: 0.00005882
Iteration 36/1000 | Loss: 0.00011081
Iteration 37/1000 | Loss: 0.00003246
Iteration 38/1000 | Loss: 0.00004552
Iteration 39/1000 | Loss: 0.00003944
Iteration 40/1000 | Loss: 0.00005818
Iteration 41/1000 | Loss: 0.00004688
Iteration 42/1000 | Loss: 0.00005725
Iteration 43/1000 | Loss: 0.00005807
Iteration 44/1000 | Loss: 0.00004313
Iteration 45/1000 | Loss: 0.00003551
Iteration 46/1000 | Loss: 0.00005249
Iteration 47/1000 | Loss: 0.00006079
Iteration 48/1000 | Loss: 0.00005306
Iteration 49/1000 | Loss: 0.00006152
Iteration 50/1000 | Loss: 0.00006685
Iteration 51/1000 | Loss: 0.00005407
Iteration 52/1000 | Loss: 0.00005168
Iteration 53/1000 | Loss: 0.00005034
Iteration 54/1000 | Loss: 0.00005468
Iteration 55/1000 | Loss: 0.00009536
Iteration 56/1000 | Loss: 0.00005055
Iteration 57/1000 | Loss: 0.00003200
Iteration 58/1000 | Loss: 0.00003729
Iteration 59/1000 | Loss: 0.00002626
Iteration 60/1000 | Loss: 0.00003141
Iteration 61/1000 | Loss: 0.00002286
Iteration 62/1000 | Loss: 0.00002093
Iteration 63/1000 | Loss: 0.00001947
Iteration 64/1000 | Loss: 0.00005544
Iteration 65/1000 | Loss: 0.00022066
Iteration 66/1000 | Loss: 0.00002521
Iteration 67/1000 | Loss: 0.00001741
Iteration 68/1000 | Loss: 0.00004263
Iteration 69/1000 | Loss: 0.00001831
Iteration 70/1000 | Loss: 0.00002073
Iteration 71/1000 | Loss: 0.00012349
Iteration 72/1000 | Loss: 0.00001713
Iteration 73/1000 | Loss: 0.00002372
Iteration 74/1000 | Loss: 0.00004296
Iteration 75/1000 | Loss: 0.00001641
Iteration 76/1000 | Loss: 0.00001628
Iteration 77/1000 | Loss: 0.00001625
Iteration 78/1000 | Loss: 0.00001617
Iteration 79/1000 | Loss: 0.00001609
Iteration 80/1000 | Loss: 0.00001608
Iteration 81/1000 | Loss: 0.00001607
Iteration 82/1000 | Loss: 0.00001606
Iteration 83/1000 | Loss: 0.00001602
Iteration 84/1000 | Loss: 0.00001600
Iteration 85/1000 | Loss: 0.00001599
Iteration 86/1000 | Loss: 0.00001596
Iteration 87/1000 | Loss: 0.00001595
Iteration 88/1000 | Loss: 0.00001591
Iteration 89/1000 | Loss: 0.00001591
Iteration 90/1000 | Loss: 0.00001590
Iteration 91/1000 | Loss: 0.00001589
Iteration 92/1000 | Loss: 0.00001588
Iteration 93/1000 | Loss: 0.00001588
Iteration 94/1000 | Loss: 0.00001587
Iteration 95/1000 | Loss: 0.00001586
Iteration 96/1000 | Loss: 0.00001586
Iteration 97/1000 | Loss: 0.00001586
Iteration 98/1000 | Loss: 0.00001585
Iteration 99/1000 | Loss: 0.00001585
Iteration 100/1000 | Loss: 0.00001585
Iteration 101/1000 | Loss: 0.00001585
Iteration 102/1000 | Loss: 0.00001585
Iteration 103/1000 | Loss: 0.00001585
Iteration 104/1000 | Loss: 0.00001585
Iteration 105/1000 | Loss: 0.00001584
Iteration 106/1000 | Loss: 0.00001584
Iteration 107/1000 | Loss: 0.00001583
Iteration 108/1000 | Loss: 0.00001583
Iteration 109/1000 | Loss: 0.00001582
Iteration 110/1000 | Loss: 0.00001582
Iteration 111/1000 | Loss: 0.00001582
Iteration 112/1000 | Loss: 0.00001581
Iteration 113/1000 | Loss: 0.00001581
Iteration 114/1000 | Loss: 0.00001581
Iteration 115/1000 | Loss: 0.00001581
Iteration 116/1000 | Loss: 0.00001580
Iteration 117/1000 | Loss: 0.00001580
Iteration 118/1000 | Loss: 0.00001580
Iteration 119/1000 | Loss: 0.00001580
Iteration 120/1000 | Loss: 0.00001579
Iteration 121/1000 | Loss: 0.00001579
Iteration 122/1000 | Loss: 0.00001579
Iteration 123/1000 | Loss: 0.00001579
Iteration 124/1000 | Loss: 0.00001578
Iteration 125/1000 | Loss: 0.00001578
Iteration 126/1000 | Loss: 0.00001578
Iteration 127/1000 | Loss: 0.00001578
Iteration 128/1000 | Loss: 0.00001578
Iteration 129/1000 | Loss: 0.00001577
Iteration 130/1000 | Loss: 0.00001577
Iteration 131/1000 | Loss: 0.00001577
Iteration 132/1000 | Loss: 0.00001576
Iteration 133/1000 | Loss: 0.00001576
Iteration 134/1000 | Loss: 0.00001576
Iteration 135/1000 | Loss: 0.00001576
Iteration 136/1000 | Loss: 0.00001575
Iteration 137/1000 | Loss: 0.00001575
Iteration 138/1000 | Loss: 0.00001575
Iteration 139/1000 | Loss: 0.00001575
Iteration 140/1000 | Loss: 0.00001575
Iteration 141/1000 | Loss: 0.00001575
Iteration 142/1000 | Loss: 0.00001575
Iteration 143/1000 | Loss: 0.00001574
Iteration 144/1000 | Loss: 0.00001574
Iteration 145/1000 | Loss: 0.00001574
Iteration 146/1000 | Loss: 0.00001574
Iteration 147/1000 | Loss: 0.00001574
Iteration 148/1000 | Loss: 0.00001574
Iteration 149/1000 | Loss: 0.00001574
Iteration 150/1000 | Loss: 0.00001574
Iteration 151/1000 | Loss: 0.00001574
Iteration 152/1000 | Loss: 0.00001574
Iteration 153/1000 | Loss: 0.00001574
Iteration 154/1000 | Loss: 0.00001573
Iteration 155/1000 | Loss: 0.00001573
Iteration 156/1000 | Loss: 0.00001573
Iteration 157/1000 | Loss: 0.00001573
Iteration 158/1000 | Loss: 0.00001572
Iteration 159/1000 | Loss: 0.00001572
Iteration 160/1000 | Loss: 0.00001572
Iteration 161/1000 | Loss: 0.00001572
Iteration 162/1000 | Loss: 0.00001571
Iteration 163/1000 | Loss: 0.00001571
Iteration 164/1000 | Loss: 0.00001571
Iteration 165/1000 | Loss: 0.00001571
Iteration 166/1000 | Loss: 0.00001571
Iteration 167/1000 | Loss: 0.00001571
Iteration 168/1000 | Loss: 0.00001570
Iteration 169/1000 | Loss: 0.00001570
Iteration 170/1000 | Loss: 0.00001570
Iteration 171/1000 | Loss: 0.00001570
Iteration 172/1000 | Loss: 0.00001570
Iteration 173/1000 | Loss: 0.00001570
Iteration 174/1000 | Loss: 0.00001570
Iteration 175/1000 | Loss: 0.00001570
Iteration 176/1000 | Loss: 0.00001570
Iteration 177/1000 | Loss: 0.00001570
Iteration 178/1000 | Loss: 0.00001570
Iteration 179/1000 | Loss: 0.00001570
Iteration 180/1000 | Loss: 0.00001570
Iteration 181/1000 | Loss: 0.00001570
Iteration 182/1000 | Loss: 0.00001570
Iteration 183/1000 | Loss: 0.00001570
Iteration 184/1000 | Loss: 0.00001570
Iteration 185/1000 | Loss: 0.00001570
Iteration 186/1000 | Loss: 0.00001570
Iteration 187/1000 | Loss: 0.00001570
Iteration 188/1000 | Loss: 0.00001570
Iteration 189/1000 | Loss: 0.00001570
Iteration 190/1000 | Loss: 0.00001570
Iteration 191/1000 | Loss: 0.00001570
Iteration 192/1000 | Loss: 0.00001570
Iteration 193/1000 | Loss: 0.00001570
Iteration 194/1000 | Loss: 0.00001570
Iteration 195/1000 | Loss: 0.00001570
Iteration 196/1000 | Loss: 0.00001570
Iteration 197/1000 | Loss: 0.00001570
Iteration 198/1000 | Loss: 0.00001570
Iteration 199/1000 | Loss: 0.00001570
Iteration 200/1000 | Loss: 0.00001570
Iteration 201/1000 | Loss: 0.00001570
Iteration 202/1000 | Loss: 0.00001570
Iteration 203/1000 | Loss: 0.00001570
Iteration 204/1000 | Loss: 0.00001570
Iteration 205/1000 | Loss: 0.00001570
Iteration 206/1000 | Loss: 0.00001570
Iteration 207/1000 | Loss: 0.00001570
Iteration 208/1000 | Loss: 0.00001570
Iteration 209/1000 | Loss: 0.00001570
Iteration 210/1000 | Loss: 0.00001570
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [1.5697147318860516e-05, 1.5697147318860516e-05, 1.5697147318860516e-05, 1.5697147318860516e-05, 1.5697147318860516e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5697147318860516e-05

Optimization complete. Final v2v error: 3.355604410171509 mm

Highest mean error: 3.6754980087280273 mm for frame 98

Lowest mean error: 3.1642627716064453 mm for frame 39

Saving results

Total time: 161.47221779823303
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00835664
Iteration 2/25 | Loss: 0.00146053
Iteration 3/25 | Loss: 0.00131408
Iteration 4/25 | Loss: 0.00130015
Iteration 5/25 | Loss: 0.00129638
Iteration 6/25 | Loss: 0.00129638
Iteration 7/25 | Loss: 0.00129638
Iteration 8/25 | Loss: 0.00129638
Iteration 9/25 | Loss: 0.00129638
Iteration 10/25 | Loss: 0.00129638
Iteration 11/25 | Loss: 0.00129638
Iteration 12/25 | Loss: 0.00129638
Iteration 13/25 | Loss: 0.00129638
Iteration 14/25 | Loss: 0.00129638
Iteration 15/25 | Loss: 0.00129638
Iteration 16/25 | Loss: 0.00129638
Iteration 17/25 | Loss: 0.00129638
Iteration 18/25 | Loss: 0.00129638
Iteration 19/25 | Loss: 0.00129638
Iteration 20/25 | Loss: 0.00129638
Iteration 21/25 | Loss: 0.00129638
Iteration 22/25 | Loss: 0.00129638
Iteration 23/25 | Loss: 0.00129638
Iteration 24/25 | Loss: 0.00129638
Iteration 25/25 | Loss: 0.00129638

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34085870
Iteration 2/25 | Loss: 0.00097522
Iteration 3/25 | Loss: 0.00097522
Iteration 4/25 | Loss: 0.00097522
Iteration 5/25 | Loss: 0.00097522
Iteration 6/25 | Loss: 0.00097522
Iteration 7/25 | Loss: 0.00097522
Iteration 8/25 | Loss: 0.00097522
Iteration 9/25 | Loss: 0.00097522
Iteration 10/25 | Loss: 0.00097522
Iteration 11/25 | Loss: 0.00097522
Iteration 12/25 | Loss: 0.00097522
Iteration 13/25 | Loss: 0.00097522
Iteration 14/25 | Loss: 0.00097522
Iteration 15/25 | Loss: 0.00097522
Iteration 16/25 | Loss: 0.00097522
Iteration 17/25 | Loss: 0.00097522
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009752162732183933, 0.0009752162732183933, 0.0009752162732183933, 0.0009752162732183933, 0.0009752162732183933]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009752162732183933

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097522
Iteration 2/1000 | Loss: 0.00003101
Iteration 3/1000 | Loss: 0.00002005
Iteration 4/1000 | Loss: 0.00001727
Iteration 5/1000 | Loss: 0.00001629
Iteration 6/1000 | Loss: 0.00001561
Iteration 7/1000 | Loss: 0.00001521
Iteration 8/1000 | Loss: 0.00001495
Iteration 9/1000 | Loss: 0.00001465
Iteration 10/1000 | Loss: 0.00001444
Iteration 11/1000 | Loss: 0.00001436
Iteration 12/1000 | Loss: 0.00001424
Iteration 13/1000 | Loss: 0.00001410
Iteration 14/1000 | Loss: 0.00001405
Iteration 15/1000 | Loss: 0.00001403
Iteration 16/1000 | Loss: 0.00001403
Iteration 17/1000 | Loss: 0.00001402
Iteration 18/1000 | Loss: 0.00001402
Iteration 19/1000 | Loss: 0.00001402
Iteration 20/1000 | Loss: 0.00001401
Iteration 21/1000 | Loss: 0.00001401
Iteration 22/1000 | Loss: 0.00001400
Iteration 23/1000 | Loss: 0.00001399
Iteration 24/1000 | Loss: 0.00001397
Iteration 25/1000 | Loss: 0.00001395
Iteration 26/1000 | Loss: 0.00001394
Iteration 27/1000 | Loss: 0.00001393
Iteration 28/1000 | Loss: 0.00001392
Iteration 29/1000 | Loss: 0.00001392
Iteration 30/1000 | Loss: 0.00001391
Iteration 31/1000 | Loss: 0.00001390
Iteration 32/1000 | Loss: 0.00001389
Iteration 33/1000 | Loss: 0.00001389
Iteration 34/1000 | Loss: 0.00001388
Iteration 35/1000 | Loss: 0.00001388
Iteration 36/1000 | Loss: 0.00001385
Iteration 37/1000 | Loss: 0.00001385
Iteration 38/1000 | Loss: 0.00001383
Iteration 39/1000 | Loss: 0.00001383
Iteration 40/1000 | Loss: 0.00001382
Iteration 41/1000 | Loss: 0.00001381
Iteration 42/1000 | Loss: 0.00001379
Iteration 43/1000 | Loss: 0.00001378
Iteration 44/1000 | Loss: 0.00001378
Iteration 45/1000 | Loss: 0.00001378
Iteration 46/1000 | Loss: 0.00001377
Iteration 47/1000 | Loss: 0.00001377
Iteration 48/1000 | Loss: 0.00001376
Iteration 49/1000 | Loss: 0.00001376
Iteration 50/1000 | Loss: 0.00001376
Iteration 51/1000 | Loss: 0.00001376
Iteration 52/1000 | Loss: 0.00001375
Iteration 53/1000 | Loss: 0.00001375
Iteration 54/1000 | Loss: 0.00001375
Iteration 55/1000 | Loss: 0.00001374
Iteration 56/1000 | Loss: 0.00001374
Iteration 57/1000 | Loss: 0.00001374
Iteration 58/1000 | Loss: 0.00001374
Iteration 59/1000 | Loss: 0.00001374
Iteration 60/1000 | Loss: 0.00001374
Iteration 61/1000 | Loss: 0.00001373
Iteration 62/1000 | Loss: 0.00001373
Iteration 63/1000 | Loss: 0.00001373
Iteration 64/1000 | Loss: 0.00001373
Iteration 65/1000 | Loss: 0.00001372
Iteration 66/1000 | Loss: 0.00001372
Iteration 67/1000 | Loss: 0.00001371
Iteration 68/1000 | Loss: 0.00001371
Iteration 69/1000 | Loss: 0.00001371
Iteration 70/1000 | Loss: 0.00001370
Iteration 71/1000 | Loss: 0.00001370
Iteration 72/1000 | Loss: 0.00001370
Iteration 73/1000 | Loss: 0.00001370
Iteration 74/1000 | Loss: 0.00001370
Iteration 75/1000 | Loss: 0.00001370
Iteration 76/1000 | Loss: 0.00001369
Iteration 77/1000 | Loss: 0.00001369
Iteration 78/1000 | Loss: 0.00001369
Iteration 79/1000 | Loss: 0.00001369
Iteration 80/1000 | Loss: 0.00001369
Iteration 81/1000 | Loss: 0.00001369
Iteration 82/1000 | Loss: 0.00001369
Iteration 83/1000 | Loss: 0.00001368
Iteration 84/1000 | Loss: 0.00001368
Iteration 85/1000 | Loss: 0.00001368
Iteration 86/1000 | Loss: 0.00001368
Iteration 87/1000 | Loss: 0.00001368
Iteration 88/1000 | Loss: 0.00001368
Iteration 89/1000 | Loss: 0.00001367
Iteration 90/1000 | Loss: 0.00001367
Iteration 91/1000 | Loss: 0.00001367
Iteration 92/1000 | Loss: 0.00001367
Iteration 93/1000 | Loss: 0.00001367
Iteration 94/1000 | Loss: 0.00001367
Iteration 95/1000 | Loss: 0.00001367
Iteration 96/1000 | Loss: 0.00001367
Iteration 97/1000 | Loss: 0.00001367
Iteration 98/1000 | Loss: 0.00001366
Iteration 99/1000 | Loss: 0.00001366
Iteration 100/1000 | Loss: 0.00001366
Iteration 101/1000 | Loss: 0.00001366
Iteration 102/1000 | Loss: 0.00001366
Iteration 103/1000 | Loss: 0.00001365
Iteration 104/1000 | Loss: 0.00001365
Iteration 105/1000 | Loss: 0.00001365
Iteration 106/1000 | Loss: 0.00001365
Iteration 107/1000 | Loss: 0.00001365
Iteration 108/1000 | Loss: 0.00001364
Iteration 109/1000 | Loss: 0.00001364
Iteration 110/1000 | Loss: 0.00001364
Iteration 111/1000 | Loss: 0.00001364
Iteration 112/1000 | Loss: 0.00001364
Iteration 113/1000 | Loss: 0.00001364
Iteration 114/1000 | Loss: 0.00001363
Iteration 115/1000 | Loss: 0.00001363
Iteration 116/1000 | Loss: 0.00001363
Iteration 117/1000 | Loss: 0.00001363
Iteration 118/1000 | Loss: 0.00001363
Iteration 119/1000 | Loss: 0.00001362
Iteration 120/1000 | Loss: 0.00001362
Iteration 121/1000 | Loss: 0.00001362
Iteration 122/1000 | Loss: 0.00001362
Iteration 123/1000 | Loss: 0.00001362
Iteration 124/1000 | Loss: 0.00001362
Iteration 125/1000 | Loss: 0.00001362
Iteration 126/1000 | Loss: 0.00001362
Iteration 127/1000 | Loss: 0.00001362
Iteration 128/1000 | Loss: 0.00001362
Iteration 129/1000 | Loss: 0.00001362
Iteration 130/1000 | Loss: 0.00001362
Iteration 131/1000 | Loss: 0.00001362
Iteration 132/1000 | Loss: 0.00001362
Iteration 133/1000 | Loss: 0.00001362
Iteration 134/1000 | Loss: 0.00001362
Iteration 135/1000 | Loss: 0.00001362
Iteration 136/1000 | Loss: 0.00001362
Iteration 137/1000 | Loss: 0.00001362
Iteration 138/1000 | Loss: 0.00001362
Iteration 139/1000 | Loss: 0.00001362
Iteration 140/1000 | Loss: 0.00001362
Iteration 141/1000 | Loss: 0.00001362
Iteration 142/1000 | Loss: 0.00001362
Iteration 143/1000 | Loss: 0.00001362
Iteration 144/1000 | Loss: 0.00001362
Iteration 145/1000 | Loss: 0.00001362
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [1.361990871373564e-05, 1.361990871373564e-05, 1.361990871373564e-05, 1.361990871373564e-05, 1.361990871373564e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.361990871373564e-05

Optimization complete. Final v2v error: 3.046577215194702 mm

Highest mean error: 3.8993310928344727 mm for frame 216

Lowest mean error: 2.6961095333099365 mm for frame 150

Saving results

Total time: 41.136008739471436
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00506729
Iteration 2/25 | Loss: 0.00144791
Iteration 3/25 | Loss: 0.00133509
Iteration 4/25 | Loss: 0.00129628
Iteration 5/25 | Loss: 0.00129233
Iteration 6/25 | Loss: 0.00129129
Iteration 7/25 | Loss: 0.00129129
Iteration 8/25 | Loss: 0.00129129
Iteration 9/25 | Loss: 0.00129129
Iteration 10/25 | Loss: 0.00129129
Iteration 11/25 | Loss: 0.00129129
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001291294815018773, 0.001291294815018773, 0.001291294815018773, 0.001291294815018773, 0.001291294815018773]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001291294815018773

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.95177555
Iteration 2/25 | Loss: 0.00087696
Iteration 3/25 | Loss: 0.00087693
Iteration 4/25 | Loss: 0.00087693
Iteration 5/25 | Loss: 0.00087693
Iteration 6/25 | Loss: 0.00087693
Iteration 7/25 | Loss: 0.00087693
Iteration 8/25 | Loss: 0.00087693
Iteration 9/25 | Loss: 0.00087693
Iteration 10/25 | Loss: 0.00087693
Iteration 11/25 | Loss: 0.00087693
Iteration 12/25 | Loss: 0.00087693
Iteration 13/25 | Loss: 0.00087693
Iteration 14/25 | Loss: 0.00087693
Iteration 15/25 | Loss: 0.00087693
Iteration 16/25 | Loss: 0.00087693
Iteration 17/25 | Loss: 0.00087693
Iteration 18/25 | Loss: 0.00087693
Iteration 19/25 | Loss: 0.00087693
Iteration 20/25 | Loss: 0.00087693
Iteration 21/25 | Loss: 0.00087693
Iteration 22/25 | Loss: 0.00087693
Iteration 23/25 | Loss: 0.00087693
Iteration 24/25 | Loss: 0.00087693
Iteration 25/25 | Loss: 0.00087693

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087693
Iteration 2/1000 | Loss: 0.00003125
Iteration 3/1000 | Loss: 0.00002169
Iteration 4/1000 | Loss: 0.00001927
Iteration 5/1000 | Loss: 0.00001799
Iteration 6/1000 | Loss: 0.00001700
Iteration 7/1000 | Loss: 0.00001654
Iteration 8/1000 | Loss: 0.00001625
Iteration 9/1000 | Loss: 0.00001590
Iteration 10/1000 | Loss: 0.00001566
Iteration 11/1000 | Loss: 0.00001548
Iteration 12/1000 | Loss: 0.00001534
Iteration 13/1000 | Loss: 0.00001533
Iteration 14/1000 | Loss: 0.00001528
Iteration 15/1000 | Loss: 0.00001524
Iteration 16/1000 | Loss: 0.00001523
Iteration 17/1000 | Loss: 0.00001523
Iteration 18/1000 | Loss: 0.00001521
Iteration 19/1000 | Loss: 0.00001520
Iteration 20/1000 | Loss: 0.00001519
Iteration 21/1000 | Loss: 0.00001515
Iteration 22/1000 | Loss: 0.00001515
Iteration 23/1000 | Loss: 0.00001515
Iteration 24/1000 | Loss: 0.00001513
Iteration 25/1000 | Loss: 0.00001513
Iteration 26/1000 | Loss: 0.00001512
Iteration 27/1000 | Loss: 0.00001512
Iteration 28/1000 | Loss: 0.00001512
Iteration 29/1000 | Loss: 0.00001509
Iteration 30/1000 | Loss: 0.00001509
Iteration 31/1000 | Loss: 0.00001508
Iteration 32/1000 | Loss: 0.00001508
Iteration 33/1000 | Loss: 0.00001508
Iteration 34/1000 | Loss: 0.00001497
Iteration 35/1000 | Loss: 0.00001497
Iteration 36/1000 | Loss: 0.00001496
Iteration 37/1000 | Loss: 0.00001494
Iteration 38/1000 | Loss: 0.00001494
Iteration 39/1000 | Loss: 0.00001493
Iteration 40/1000 | Loss: 0.00001493
Iteration 41/1000 | Loss: 0.00001493
Iteration 42/1000 | Loss: 0.00001492
Iteration 43/1000 | Loss: 0.00001492
Iteration 44/1000 | Loss: 0.00001492
Iteration 45/1000 | Loss: 0.00001492
Iteration 46/1000 | Loss: 0.00001491
Iteration 47/1000 | Loss: 0.00001491
Iteration 48/1000 | Loss: 0.00001491
Iteration 49/1000 | Loss: 0.00001491
Iteration 50/1000 | Loss: 0.00001490
Iteration 51/1000 | Loss: 0.00001490
Iteration 52/1000 | Loss: 0.00001490
Iteration 53/1000 | Loss: 0.00001490
Iteration 54/1000 | Loss: 0.00001490
Iteration 55/1000 | Loss: 0.00001490
Iteration 56/1000 | Loss: 0.00001489
Iteration 57/1000 | Loss: 0.00001489
Iteration 58/1000 | Loss: 0.00001489
Iteration 59/1000 | Loss: 0.00001489
Iteration 60/1000 | Loss: 0.00001489
Iteration 61/1000 | Loss: 0.00001488
Iteration 62/1000 | Loss: 0.00001488
Iteration 63/1000 | Loss: 0.00001487
Iteration 64/1000 | Loss: 0.00001487
Iteration 65/1000 | Loss: 0.00001487
Iteration 66/1000 | Loss: 0.00001487
Iteration 67/1000 | Loss: 0.00001487
Iteration 68/1000 | Loss: 0.00001487
Iteration 69/1000 | Loss: 0.00001486
Iteration 70/1000 | Loss: 0.00001486
Iteration 71/1000 | Loss: 0.00001486
Iteration 72/1000 | Loss: 0.00001486
Iteration 73/1000 | Loss: 0.00001486
Iteration 74/1000 | Loss: 0.00001486
Iteration 75/1000 | Loss: 0.00001485
Iteration 76/1000 | Loss: 0.00001485
Iteration 77/1000 | Loss: 0.00001485
Iteration 78/1000 | Loss: 0.00001485
Iteration 79/1000 | Loss: 0.00001485
Iteration 80/1000 | Loss: 0.00001484
Iteration 81/1000 | Loss: 0.00001484
Iteration 82/1000 | Loss: 0.00001484
Iteration 83/1000 | Loss: 0.00001483
Iteration 84/1000 | Loss: 0.00001483
Iteration 85/1000 | Loss: 0.00001483
Iteration 86/1000 | Loss: 0.00001482
Iteration 87/1000 | Loss: 0.00001482
Iteration 88/1000 | Loss: 0.00001482
Iteration 89/1000 | Loss: 0.00001481
Iteration 90/1000 | Loss: 0.00001481
Iteration 91/1000 | Loss: 0.00001481
Iteration 92/1000 | Loss: 0.00001480
Iteration 93/1000 | Loss: 0.00001480
Iteration 94/1000 | Loss: 0.00001480
Iteration 95/1000 | Loss: 0.00001479
Iteration 96/1000 | Loss: 0.00001479
Iteration 97/1000 | Loss: 0.00001479
Iteration 98/1000 | Loss: 0.00001479
Iteration 99/1000 | Loss: 0.00001479
Iteration 100/1000 | Loss: 0.00001478
Iteration 101/1000 | Loss: 0.00001478
Iteration 102/1000 | Loss: 0.00001478
Iteration 103/1000 | Loss: 0.00001478
Iteration 104/1000 | Loss: 0.00001478
Iteration 105/1000 | Loss: 0.00001477
Iteration 106/1000 | Loss: 0.00001477
Iteration 107/1000 | Loss: 0.00001477
Iteration 108/1000 | Loss: 0.00001477
Iteration 109/1000 | Loss: 0.00001477
Iteration 110/1000 | Loss: 0.00001477
Iteration 111/1000 | Loss: 0.00001476
Iteration 112/1000 | Loss: 0.00001476
Iteration 113/1000 | Loss: 0.00001476
Iteration 114/1000 | Loss: 0.00001476
Iteration 115/1000 | Loss: 0.00001476
Iteration 116/1000 | Loss: 0.00001476
Iteration 117/1000 | Loss: 0.00001475
Iteration 118/1000 | Loss: 0.00001475
Iteration 119/1000 | Loss: 0.00001475
Iteration 120/1000 | Loss: 0.00001475
Iteration 121/1000 | Loss: 0.00001475
Iteration 122/1000 | Loss: 0.00001475
Iteration 123/1000 | Loss: 0.00001475
Iteration 124/1000 | Loss: 0.00001474
Iteration 125/1000 | Loss: 0.00001474
Iteration 126/1000 | Loss: 0.00001474
Iteration 127/1000 | Loss: 0.00001474
Iteration 128/1000 | Loss: 0.00001474
Iteration 129/1000 | Loss: 0.00001474
Iteration 130/1000 | Loss: 0.00001473
Iteration 131/1000 | Loss: 0.00001473
Iteration 132/1000 | Loss: 0.00001473
Iteration 133/1000 | Loss: 0.00001473
Iteration 134/1000 | Loss: 0.00001473
Iteration 135/1000 | Loss: 0.00001473
Iteration 136/1000 | Loss: 0.00001473
Iteration 137/1000 | Loss: 0.00001473
Iteration 138/1000 | Loss: 0.00001473
Iteration 139/1000 | Loss: 0.00001473
Iteration 140/1000 | Loss: 0.00001472
Iteration 141/1000 | Loss: 0.00001472
Iteration 142/1000 | Loss: 0.00001472
Iteration 143/1000 | Loss: 0.00001472
Iteration 144/1000 | Loss: 0.00001472
Iteration 145/1000 | Loss: 0.00001472
Iteration 146/1000 | Loss: 0.00001472
Iteration 147/1000 | Loss: 0.00001472
Iteration 148/1000 | Loss: 0.00001472
Iteration 149/1000 | Loss: 0.00001472
Iteration 150/1000 | Loss: 0.00001472
Iteration 151/1000 | Loss: 0.00001472
Iteration 152/1000 | Loss: 0.00001471
Iteration 153/1000 | Loss: 0.00001471
Iteration 154/1000 | Loss: 0.00001471
Iteration 155/1000 | Loss: 0.00001471
Iteration 156/1000 | Loss: 0.00001471
Iteration 157/1000 | Loss: 0.00001471
Iteration 158/1000 | Loss: 0.00001471
Iteration 159/1000 | Loss: 0.00001471
Iteration 160/1000 | Loss: 0.00001471
Iteration 161/1000 | Loss: 0.00001471
Iteration 162/1000 | Loss: 0.00001471
Iteration 163/1000 | Loss: 0.00001470
Iteration 164/1000 | Loss: 0.00001470
Iteration 165/1000 | Loss: 0.00001470
Iteration 166/1000 | Loss: 0.00001470
Iteration 167/1000 | Loss: 0.00001470
Iteration 168/1000 | Loss: 0.00001470
Iteration 169/1000 | Loss: 0.00001470
Iteration 170/1000 | Loss: 0.00001470
Iteration 171/1000 | Loss: 0.00001470
Iteration 172/1000 | Loss: 0.00001470
Iteration 173/1000 | Loss: 0.00001470
Iteration 174/1000 | Loss: 0.00001470
Iteration 175/1000 | Loss: 0.00001470
Iteration 176/1000 | Loss: 0.00001470
Iteration 177/1000 | Loss: 0.00001470
Iteration 178/1000 | Loss: 0.00001470
Iteration 179/1000 | Loss: 0.00001470
Iteration 180/1000 | Loss: 0.00001470
Iteration 181/1000 | Loss: 0.00001470
Iteration 182/1000 | Loss: 0.00001470
Iteration 183/1000 | Loss: 0.00001470
Iteration 184/1000 | Loss: 0.00001470
Iteration 185/1000 | Loss: 0.00001470
Iteration 186/1000 | Loss: 0.00001470
Iteration 187/1000 | Loss: 0.00001470
Iteration 188/1000 | Loss: 0.00001470
Iteration 189/1000 | Loss: 0.00001470
Iteration 190/1000 | Loss: 0.00001470
Iteration 191/1000 | Loss: 0.00001470
Iteration 192/1000 | Loss: 0.00001470
Iteration 193/1000 | Loss: 0.00001470
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [1.4698774975840934e-05, 1.4698774975840934e-05, 1.4698774975840934e-05, 1.4698774975840934e-05, 1.4698774975840934e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4698774975840934e-05

Optimization complete. Final v2v error: 3.227191686630249 mm

Highest mean error: 3.6576340198516846 mm for frame 199

Lowest mean error: 2.910553216934204 mm for frame 132

Saving results

Total time: 47.905056953430176
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00709730
Iteration 2/25 | Loss: 0.00142434
Iteration 3/25 | Loss: 0.00130943
Iteration 4/25 | Loss: 0.00127543
Iteration 5/25 | Loss: 0.00127563
Iteration 6/25 | Loss: 0.00126913
Iteration 7/25 | Loss: 0.00126817
Iteration 8/25 | Loss: 0.00126760
Iteration 9/25 | Loss: 0.00126718
Iteration 10/25 | Loss: 0.00126703
Iteration 11/25 | Loss: 0.00126698
Iteration 12/25 | Loss: 0.00126697
Iteration 13/25 | Loss: 0.00126697
Iteration 14/25 | Loss: 0.00126695
Iteration 15/25 | Loss: 0.00126695
Iteration 16/25 | Loss: 0.00126694
Iteration 17/25 | Loss: 0.00126694
Iteration 18/25 | Loss: 0.00126694
Iteration 19/25 | Loss: 0.00126694
Iteration 20/25 | Loss: 0.00126694
Iteration 21/25 | Loss: 0.00126694
Iteration 22/25 | Loss: 0.00126693
Iteration 23/25 | Loss: 0.00126693
Iteration 24/25 | Loss: 0.00126693
Iteration 25/25 | Loss: 0.00126693

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.85909200
Iteration 2/25 | Loss: 0.00091703
Iteration 3/25 | Loss: 0.00091703
Iteration 4/25 | Loss: 0.00091703
Iteration 5/25 | Loss: 0.00091703
Iteration 6/25 | Loss: 0.00091702
Iteration 7/25 | Loss: 0.00091702
Iteration 8/25 | Loss: 0.00091702
Iteration 9/25 | Loss: 0.00091702
Iteration 10/25 | Loss: 0.00091702
Iteration 11/25 | Loss: 0.00091702
Iteration 12/25 | Loss: 0.00091702
Iteration 13/25 | Loss: 0.00091702
Iteration 14/25 | Loss: 0.00091702
Iteration 15/25 | Loss: 0.00091702
Iteration 16/25 | Loss: 0.00091702
Iteration 17/25 | Loss: 0.00091702
Iteration 18/25 | Loss: 0.00091702
Iteration 19/25 | Loss: 0.00091702
Iteration 20/25 | Loss: 0.00091702
Iteration 21/25 | Loss: 0.00091702
Iteration 22/25 | Loss: 0.00091702
Iteration 23/25 | Loss: 0.00091702
Iteration 24/25 | Loss: 0.00091702
Iteration 25/25 | Loss: 0.00091702

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091702
Iteration 2/1000 | Loss: 0.00002065
Iteration 3/1000 | Loss: 0.00001566
Iteration 4/1000 | Loss: 0.00001458
Iteration 5/1000 | Loss: 0.00006461
Iteration 6/1000 | Loss: 0.00064520
Iteration 7/1000 | Loss: 0.00022016
Iteration 8/1000 | Loss: 0.00031696
Iteration 9/1000 | Loss: 0.00004051
Iteration 10/1000 | Loss: 0.00004794
Iteration 11/1000 | Loss: 0.00001356
Iteration 12/1000 | Loss: 0.00001336
Iteration 13/1000 | Loss: 0.00001335
Iteration 14/1000 | Loss: 0.00005776
Iteration 15/1000 | Loss: 0.00006673
Iteration 16/1000 | Loss: 0.00001700
Iteration 17/1000 | Loss: 0.00001447
Iteration 18/1000 | Loss: 0.00002132
Iteration 19/1000 | Loss: 0.00001285
Iteration 20/1000 | Loss: 0.00001282
Iteration 21/1000 | Loss: 0.00001268
Iteration 22/1000 | Loss: 0.00001264
Iteration 23/1000 | Loss: 0.00001260
Iteration 24/1000 | Loss: 0.00001254
Iteration 25/1000 | Loss: 0.00001254
Iteration 26/1000 | Loss: 0.00001253
Iteration 27/1000 | Loss: 0.00001252
Iteration 28/1000 | Loss: 0.00001251
Iteration 29/1000 | Loss: 0.00001251
Iteration 30/1000 | Loss: 0.00001250
Iteration 31/1000 | Loss: 0.00001249
Iteration 32/1000 | Loss: 0.00001246
Iteration 33/1000 | Loss: 0.00001246
Iteration 34/1000 | Loss: 0.00001245
Iteration 35/1000 | Loss: 0.00001240
Iteration 36/1000 | Loss: 0.00001237
Iteration 37/1000 | Loss: 0.00001236
Iteration 38/1000 | Loss: 0.00001235
Iteration 39/1000 | Loss: 0.00001234
Iteration 40/1000 | Loss: 0.00001233
Iteration 41/1000 | Loss: 0.00001232
Iteration 42/1000 | Loss: 0.00001232
Iteration 43/1000 | Loss: 0.00001231
Iteration 44/1000 | Loss: 0.00001228
Iteration 45/1000 | Loss: 0.00001225
Iteration 46/1000 | Loss: 0.00001224
Iteration 47/1000 | Loss: 0.00009300
Iteration 48/1000 | Loss: 0.00001372
Iteration 49/1000 | Loss: 0.00001367
Iteration 50/1000 | Loss: 0.00001216
Iteration 51/1000 | Loss: 0.00001212
Iteration 52/1000 | Loss: 0.00001211
Iteration 53/1000 | Loss: 0.00001208
Iteration 54/1000 | Loss: 0.00001207
Iteration 55/1000 | Loss: 0.00001207
Iteration 56/1000 | Loss: 0.00001205
Iteration 57/1000 | Loss: 0.00001204
Iteration 58/1000 | Loss: 0.00001204
Iteration 59/1000 | Loss: 0.00004250
Iteration 60/1000 | Loss: 0.00001399
Iteration 61/1000 | Loss: 0.00002150
Iteration 62/1000 | Loss: 0.00001204
Iteration 63/1000 | Loss: 0.00001203
Iteration 64/1000 | Loss: 0.00001203
Iteration 65/1000 | Loss: 0.00001203
Iteration 66/1000 | Loss: 0.00001203
Iteration 67/1000 | Loss: 0.00001203
Iteration 68/1000 | Loss: 0.00001202
Iteration 69/1000 | Loss: 0.00001202
Iteration 70/1000 | Loss: 0.00001202
Iteration 71/1000 | Loss: 0.00001202
Iteration 72/1000 | Loss: 0.00001202
Iteration 73/1000 | Loss: 0.00001202
Iteration 74/1000 | Loss: 0.00001201
Iteration 75/1000 | Loss: 0.00001201
Iteration 76/1000 | Loss: 0.00001201
Iteration 77/1000 | Loss: 0.00001200
Iteration 78/1000 | Loss: 0.00001200
Iteration 79/1000 | Loss: 0.00001200
Iteration 80/1000 | Loss: 0.00001199
Iteration 81/1000 | Loss: 0.00001199
Iteration 82/1000 | Loss: 0.00001199
Iteration 83/1000 | Loss: 0.00001199
Iteration 84/1000 | Loss: 0.00001199
Iteration 85/1000 | Loss: 0.00001199
Iteration 86/1000 | Loss: 0.00001199
Iteration 87/1000 | Loss: 0.00001199
Iteration 88/1000 | Loss: 0.00001199
Iteration 89/1000 | Loss: 0.00001199
Iteration 90/1000 | Loss: 0.00001198
Iteration 91/1000 | Loss: 0.00001198
Iteration 92/1000 | Loss: 0.00001198
Iteration 93/1000 | Loss: 0.00001198
Iteration 94/1000 | Loss: 0.00001198
Iteration 95/1000 | Loss: 0.00001198
Iteration 96/1000 | Loss: 0.00001197
Iteration 97/1000 | Loss: 0.00001197
Iteration 98/1000 | Loss: 0.00001197
Iteration 99/1000 | Loss: 0.00001197
Iteration 100/1000 | Loss: 0.00001197
Iteration 101/1000 | Loss: 0.00001196
Iteration 102/1000 | Loss: 0.00001196
Iteration 103/1000 | Loss: 0.00001196
Iteration 104/1000 | Loss: 0.00001196
Iteration 105/1000 | Loss: 0.00001196
Iteration 106/1000 | Loss: 0.00001196
Iteration 107/1000 | Loss: 0.00001196
Iteration 108/1000 | Loss: 0.00001196
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [1.1961805284954607e-05, 1.1961805284954607e-05, 1.1961805284954607e-05, 1.1961805284954607e-05, 1.1961805284954607e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1961805284954607e-05

Optimization complete. Final v2v error: 2.9715027809143066 mm

Highest mean error: 3.690060615539551 mm for frame 145

Lowest mean error: 2.797100305557251 mm for frame 23

Saving results

Total time: 72.123361825943
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00836229
Iteration 2/25 | Loss: 0.00149657
Iteration 3/25 | Loss: 0.00129700
Iteration 4/25 | Loss: 0.00127634
Iteration 5/25 | Loss: 0.00127075
Iteration 6/25 | Loss: 0.00127007
Iteration 7/25 | Loss: 0.00127007
Iteration 8/25 | Loss: 0.00127007
Iteration 9/25 | Loss: 0.00127007
Iteration 10/25 | Loss: 0.00127007
Iteration 11/25 | Loss: 0.00127007
Iteration 12/25 | Loss: 0.00127007
Iteration 13/25 | Loss: 0.00127007
Iteration 14/25 | Loss: 0.00127007
Iteration 15/25 | Loss: 0.00127007
Iteration 16/25 | Loss: 0.00127007
Iteration 17/25 | Loss: 0.00127007
Iteration 18/25 | Loss: 0.00127007
Iteration 19/25 | Loss: 0.00127007
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012700679944828153, 0.0012700679944828153, 0.0012700679944828153, 0.0012700679944828153, 0.0012700679944828153]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012700679944828153

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.72688818
Iteration 2/25 | Loss: 0.00089617
Iteration 3/25 | Loss: 0.00089617
Iteration 4/25 | Loss: 0.00089617
Iteration 5/25 | Loss: 0.00089617
Iteration 6/25 | Loss: 0.00089617
Iteration 7/25 | Loss: 0.00089617
Iteration 8/25 | Loss: 0.00089617
Iteration 9/25 | Loss: 0.00089617
Iteration 10/25 | Loss: 0.00089617
Iteration 11/25 | Loss: 0.00089617
Iteration 12/25 | Loss: 0.00089617
Iteration 13/25 | Loss: 0.00089617
Iteration 14/25 | Loss: 0.00089616
Iteration 15/25 | Loss: 0.00089616
Iteration 16/25 | Loss: 0.00089616
Iteration 17/25 | Loss: 0.00089616
Iteration 18/25 | Loss: 0.00089616
Iteration 19/25 | Loss: 0.00089616
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008961649728007615, 0.0008961649728007615, 0.0008961649728007615, 0.0008961649728007615, 0.0008961649728007615]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008961649728007615

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089616
Iteration 2/1000 | Loss: 0.00002451
Iteration 3/1000 | Loss: 0.00001842
Iteration 4/1000 | Loss: 0.00001689
Iteration 5/1000 | Loss: 0.00001605
Iteration 6/1000 | Loss: 0.00001540
Iteration 7/1000 | Loss: 0.00001508
Iteration 8/1000 | Loss: 0.00001470
Iteration 9/1000 | Loss: 0.00001443
Iteration 10/1000 | Loss: 0.00001421
Iteration 11/1000 | Loss: 0.00001414
Iteration 12/1000 | Loss: 0.00001400
Iteration 13/1000 | Loss: 0.00001394
Iteration 14/1000 | Loss: 0.00001392
Iteration 15/1000 | Loss: 0.00001387
Iteration 16/1000 | Loss: 0.00001385
Iteration 17/1000 | Loss: 0.00001384
Iteration 18/1000 | Loss: 0.00001384
Iteration 19/1000 | Loss: 0.00001383
Iteration 20/1000 | Loss: 0.00001382
Iteration 21/1000 | Loss: 0.00001379
Iteration 22/1000 | Loss: 0.00001379
Iteration 23/1000 | Loss: 0.00001378
Iteration 24/1000 | Loss: 0.00001378
Iteration 25/1000 | Loss: 0.00001378
Iteration 26/1000 | Loss: 0.00001378
Iteration 27/1000 | Loss: 0.00001377
Iteration 28/1000 | Loss: 0.00001377
Iteration 29/1000 | Loss: 0.00001376
Iteration 30/1000 | Loss: 0.00001371
Iteration 31/1000 | Loss: 0.00001370
Iteration 32/1000 | Loss: 0.00001368
Iteration 33/1000 | Loss: 0.00001368
Iteration 34/1000 | Loss: 0.00001367
Iteration 35/1000 | Loss: 0.00001365
Iteration 36/1000 | Loss: 0.00001364
Iteration 37/1000 | Loss: 0.00001364
Iteration 38/1000 | Loss: 0.00001362
Iteration 39/1000 | Loss: 0.00001361
Iteration 40/1000 | Loss: 0.00001359
Iteration 41/1000 | Loss: 0.00001359
Iteration 42/1000 | Loss: 0.00001358
Iteration 43/1000 | Loss: 0.00001357
Iteration 44/1000 | Loss: 0.00001357
Iteration 45/1000 | Loss: 0.00001357
Iteration 46/1000 | Loss: 0.00001356
Iteration 47/1000 | Loss: 0.00001356
Iteration 48/1000 | Loss: 0.00001356
Iteration 49/1000 | Loss: 0.00001355
Iteration 50/1000 | Loss: 0.00001355
Iteration 51/1000 | Loss: 0.00001355
Iteration 52/1000 | Loss: 0.00001354
Iteration 53/1000 | Loss: 0.00001354
Iteration 54/1000 | Loss: 0.00001353
Iteration 55/1000 | Loss: 0.00001352
Iteration 56/1000 | Loss: 0.00001352
Iteration 57/1000 | Loss: 0.00001352
Iteration 58/1000 | Loss: 0.00001352
Iteration 59/1000 | Loss: 0.00001352
Iteration 60/1000 | Loss: 0.00001352
Iteration 61/1000 | Loss: 0.00001352
Iteration 62/1000 | Loss: 0.00001352
Iteration 63/1000 | Loss: 0.00001352
Iteration 64/1000 | Loss: 0.00001352
Iteration 65/1000 | Loss: 0.00001351
Iteration 66/1000 | Loss: 0.00001348
Iteration 67/1000 | Loss: 0.00001346
Iteration 68/1000 | Loss: 0.00001343
Iteration 69/1000 | Loss: 0.00001343
Iteration 70/1000 | Loss: 0.00001342
Iteration 71/1000 | Loss: 0.00001341
Iteration 72/1000 | Loss: 0.00001341
Iteration 73/1000 | Loss: 0.00001341
Iteration 74/1000 | Loss: 0.00001340
Iteration 75/1000 | Loss: 0.00001339
Iteration 76/1000 | Loss: 0.00001339
Iteration 77/1000 | Loss: 0.00001338
Iteration 78/1000 | Loss: 0.00001338
Iteration 79/1000 | Loss: 0.00001337
Iteration 80/1000 | Loss: 0.00001337
Iteration 81/1000 | Loss: 0.00001337
Iteration 82/1000 | Loss: 0.00001336
Iteration 83/1000 | Loss: 0.00001336
Iteration 84/1000 | Loss: 0.00001336
Iteration 85/1000 | Loss: 0.00001335
Iteration 86/1000 | Loss: 0.00001335
Iteration 87/1000 | Loss: 0.00001335
Iteration 88/1000 | Loss: 0.00001334
Iteration 89/1000 | Loss: 0.00001334
Iteration 90/1000 | Loss: 0.00001334
Iteration 91/1000 | Loss: 0.00001334
Iteration 92/1000 | Loss: 0.00001333
Iteration 93/1000 | Loss: 0.00001333
Iteration 94/1000 | Loss: 0.00001333
Iteration 95/1000 | Loss: 0.00001332
Iteration 96/1000 | Loss: 0.00001332
Iteration 97/1000 | Loss: 0.00001332
Iteration 98/1000 | Loss: 0.00001331
Iteration 99/1000 | Loss: 0.00001331
Iteration 100/1000 | Loss: 0.00001331
Iteration 101/1000 | Loss: 0.00001331
Iteration 102/1000 | Loss: 0.00001331
Iteration 103/1000 | Loss: 0.00001331
Iteration 104/1000 | Loss: 0.00001331
Iteration 105/1000 | Loss: 0.00001331
Iteration 106/1000 | Loss: 0.00001330
Iteration 107/1000 | Loss: 0.00001329
Iteration 108/1000 | Loss: 0.00001329
Iteration 109/1000 | Loss: 0.00001329
Iteration 110/1000 | Loss: 0.00001329
Iteration 111/1000 | Loss: 0.00001328
Iteration 112/1000 | Loss: 0.00001328
Iteration 113/1000 | Loss: 0.00001328
Iteration 114/1000 | Loss: 0.00001328
Iteration 115/1000 | Loss: 0.00001328
Iteration 116/1000 | Loss: 0.00001328
Iteration 117/1000 | Loss: 0.00001328
Iteration 118/1000 | Loss: 0.00001328
Iteration 119/1000 | Loss: 0.00001327
Iteration 120/1000 | Loss: 0.00001327
Iteration 121/1000 | Loss: 0.00001327
Iteration 122/1000 | Loss: 0.00001327
Iteration 123/1000 | Loss: 0.00001327
Iteration 124/1000 | Loss: 0.00001327
Iteration 125/1000 | Loss: 0.00001327
Iteration 126/1000 | Loss: 0.00001327
Iteration 127/1000 | Loss: 0.00001326
Iteration 128/1000 | Loss: 0.00001326
Iteration 129/1000 | Loss: 0.00001326
Iteration 130/1000 | Loss: 0.00001326
Iteration 131/1000 | Loss: 0.00001326
Iteration 132/1000 | Loss: 0.00001326
Iteration 133/1000 | Loss: 0.00001326
Iteration 134/1000 | Loss: 0.00001326
Iteration 135/1000 | Loss: 0.00001325
Iteration 136/1000 | Loss: 0.00001325
Iteration 137/1000 | Loss: 0.00001325
Iteration 138/1000 | Loss: 0.00001325
Iteration 139/1000 | Loss: 0.00001325
Iteration 140/1000 | Loss: 0.00001325
Iteration 141/1000 | Loss: 0.00001325
Iteration 142/1000 | Loss: 0.00001325
Iteration 143/1000 | Loss: 0.00001325
Iteration 144/1000 | Loss: 0.00001325
Iteration 145/1000 | Loss: 0.00001325
Iteration 146/1000 | Loss: 0.00001325
Iteration 147/1000 | Loss: 0.00001325
Iteration 148/1000 | Loss: 0.00001325
Iteration 149/1000 | Loss: 0.00001325
Iteration 150/1000 | Loss: 0.00001325
Iteration 151/1000 | Loss: 0.00001325
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.324719050899148e-05, 1.324719050899148e-05, 1.324719050899148e-05, 1.324719050899148e-05, 1.324719050899148e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.324719050899148e-05

Optimization complete. Final v2v error: 3.1153879165649414 mm

Highest mean error: 3.4257824420928955 mm for frame 191

Lowest mean error: 2.855599880218506 mm for frame 2

Saving results

Total time: 43.30812859535217
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01002988
Iteration 2/25 | Loss: 0.00242470
Iteration 3/25 | Loss: 0.00311679
Iteration 4/25 | Loss: 0.00216515
Iteration 5/25 | Loss: 0.00179127
Iteration 6/25 | Loss: 0.00175189
Iteration 7/25 | Loss: 0.00148216
Iteration 8/25 | Loss: 0.00146826
Iteration 9/25 | Loss: 0.00142008
Iteration 10/25 | Loss: 0.00138857
Iteration 11/25 | Loss: 0.00137251
Iteration 12/25 | Loss: 0.00137306
Iteration 13/25 | Loss: 0.00135889
Iteration 14/25 | Loss: 0.00136284
Iteration 15/25 | Loss: 0.00135079
Iteration 16/25 | Loss: 0.00136350
Iteration 17/25 | Loss: 0.00135456
Iteration 18/25 | Loss: 0.00134820
Iteration 19/25 | Loss: 0.00134627
Iteration 20/25 | Loss: 0.00134598
Iteration 21/25 | Loss: 0.00134596
Iteration 22/25 | Loss: 0.00134594
Iteration 23/25 | Loss: 0.00134594
Iteration 24/25 | Loss: 0.00134594
Iteration 25/25 | Loss: 0.00134594

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46742713
Iteration 2/25 | Loss: 0.00098613
Iteration 3/25 | Loss: 0.00098613
Iteration 4/25 | Loss: 0.00098612
Iteration 5/25 | Loss: 0.00098612
Iteration 6/25 | Loss: 0.00098612
Iteration 7/25 | Loss: 0.00098612
Iteration 8/25 | Loss: 0.00098612
Iteration 9/25 | Loss: 0.00098612
Iteration 10/25 | Loss: 0.00098612
Iteration 11/25 | Loss: 0.00098612
Iteration 12/25 | Loss: 0.00098612
Iteration 13/25 | Loss: 0.00098612
Iteration 14/25 | Loss: 0.00098612
Iteration 15/25 | Loss: 0.00098612
Iteration 16/25 | Loss: 0.00098612
Iteration 17/25 | Loss: 0.00098612
Iteration 18/25 | Loss: 0.00098612
Iteration 19/25 | Loss: 0.00098612
Iteration 20/25 | Loss: 0.00098612
Iteration 21/25 | Loss: 0.00098612
Iteration 22/25 | Loss: 0.00098612
Iteration 23/25 | Loss: 0.00098612
Iteration 24/25 | Loss: 0.00098612
Iteration 25/25 | Loss: 0.00098612

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098612
Iteration 2/1000 | Loss: 0.00190123
Iteration 3/1000 | Loss: 0.00260345
Iteration 4/1000 | Loss: 0.00029537
Iteration 5/1000 | Loss: 0.00118818
Iteration 6/1000 | Loss: 0.00008846
Iteration 7/1000 | Loss: 0.00004256
Iteration 8/1000 | Loss: 0.00002731
Iteration 9/1000 | Loss: 0.00002527
Iteration 10/1000 | Loss: 0.00002416
Iteration 11/1000 | Loss: 0.00002341
Iteration 12/1000 | Loss: 0.00002289
Iteration 13/1000 | Loss: 0.00002256
Iteration 14/1000 | Loss: 0.00002220
Iteration 15/1000 | Loss: 0.00002172
Iteration 16/1000 | Loss: 0.00014962
Iteration 17/1000 | Loss: 0.00016311
Iteration 18/1000 | Loss: 0.00004315
Iteration 19/1000 | Loss: 0.00002453
Iteration 20/1000 | Loss: 0.00012228
Iteration 21/1000 | Loss: 0.00002958
Iteration 22/1000 | Loss: 0.00002310
Iteration 23/1000 | Loss: 0.00002215
Iteration 24/1000 | Loss: 0.00002137
Iteration 25/1000 | Loss: 0.00002104
Iteration 26/1000 | Loss: 0.00002088
Iteration 27/1000 | Loss: 0.00002087
Iteration 28/1000 | Loss: 0.00002086
Iteration 29/1000 | Loss: 0.00002085
Iteration 30/1000 | Loss: 0.00002085
Iteration 31/1000 | Loss: 0.00002085
Iteration 32/1000 | Loss: 0.00002084
Iteration 33/1000 | Loss: 0.00002084
Iteration 34/1000 | Loss: 0.00002083
Iteration 35/1000 | Loss: 0.00002082
Iteration 36/1000 | Loss: 0.00002081
Iteration 37/1000 | Loss: 0.00002081
Iteration 38/1000 | Loss: 0.00002081
Iteration 39/1000 | Loss: 0.00002077
Iteration 40/1000 | Loss: 0.00002069
Iteration 41/1000 | Loss: 0.00002068
Iteration 42/1000 | Loss: 0.00002068
Iteration 43/1000 | Loss: 0.00002067
Iteration 44/1000 | Loss: 0.00002067
Iteration 45/1000 | Loss: 0.00002066
Iteration 46/1000 | Loss: 0.00002055
Iteration 47/1000 | Loss: 0.00002053
Iteration 48/1000 | Loss: 0.00002053
Iteration 49/1000 | Loss: 0.00002052
Iteration 50/1000 | Loss: 0.00002052
Iteration 51/1000 | Loss: 0.00002052
Iteration 52/1000 | Loss: 0.00002051
Iteration 53/1000 | Loss: 0.00002051
Iteration 54/1000 | Loss: 0.00002051
Iteration 55/1000 | Loss: 0.00002051
Iteration 56/1000 | Loss: 0.00002051
Iteration 57/1000 | Loss: 0.00002051
Iteration 58/1000 | Loss: 0.00002051
Iteration 59/1000 | Loss: 0.00002051
Iteration 60/1000 | Loss: 0.00002051
Iteration 61/1000 | Loss: 0.00002051
Iteration 62/1000 | Loss: 0.00002051
Iteration 63/1000 | Loss: 0.00002051
Iteration 64/1000 | Loss: 0.00002050
Iteration 65/1000 | Loss: 0.00002050
Iteration 66/1000 | Loss: 0.00002050
Iteration 67/1000 | Loss: 0.00002050
Iteration 68/1000 | Loss: 0.00002050
Iteration 69/1000 | Loss: 0.00002049
Iteration 70/1000 | Loss: 0.00002049
Iteration 71/1000 | Loss: 0.00002049
Iteration 72/1000 | Loss: 0.00002049
Iteration 73/1000 | Loss: 0.00002049
Iteration 74/1000 | Loss: 0.00002049
Iteration 75/1000 | Loss: 0.00002048
Iteration 76/1000 | Loss: 0.00002048
Iteration 77/1000 | Loss: 0.00002048
Iteration 78/1000 | Loss: 0.00002048
Iteration 79/1000 | Loss: 0.00002048
Iteration 80/1000 | Loss: 0.00002048
Iteration 81/1000 | Loss: 0.00002048
Iteration 82/1000 | Loss: 0.00002048
Iteration 83/1000 | Loss: 0.00002047
Iteration 84/1000 | Loss: 0.00002047
Iteration 85/1000 | Loss: 0.00002045
Iteration 86/1000 | Loss: 0.00002045
Iteration 87/1000 | Loss: 0.00002044
Iteration 88/1000 | Loss: 0.00002044
Iteration 89/1000 | Loss: 0.00002044
Iteration 90/1000 | Loss: 0.00002044
Iteration 91/1000 | Loss: 0.00002044
Iteration 92/1000 | Loss: 0.00002043
Iteration 93/1000 | Loss: 0.00002043
Iteration 94/1000 | Loss: 0.00002043
Iteration 95/1000 | Loss: 0.00002043
Iteration 96/1000 | Loss: 0.00002043
Iteration 97/1000 | Loss: 0.00002042
Iteration 98/1000 | Loss: 0.00002042
Iteration 99/1000 | Loss: 0.00002042
Iteration 100/1000 | Loss: 0.00002042
Iteration 101/1000 | Loss: 0.00002042
Iteration 102/1000 | Loss: 0.00002042
Iteration 103/1000 | Loss: 0.00002041
Iteration 104/1000 | Loss: 0.00002041
Iteration 105/1000 | Loss: 0.00002041
Iteration 106/1000 | Loss: 0.00002041
Iteration 107/1000 | Loss: 0.00002041
Iteration 108/1000 | Loss: 0.00002041
Iteration 109/1000 | Loss: 0.00002041
Iteration 110/1000 | Loss: 0.00002041
Iteration 111/1000 | Loss: 0.00002040
Iteration 112/1000 | Loss: 0.00002040
Iteration 113/1000 | Loss: 0.00002039
Iteration 114/1000 | Loss: 0.00002039
Iteration 115/1000 | Loss: 0.00002039
Iteration 116/1000 | Loss: 0.00002038
Iteration 117/1000 | Loss: 0.00002038
Iteration 118/1000 | Loss: 0.00002037
Iteration 119/1000 | Loss: 0.00002037
Iteration 120/1000 | Loss: 0.00002037
Iteration 121/1000 | Loss: 0.00002036
Iteration 122/1000 | Loss: 0.00002036
Iteration 123/1000 | Loss: 0.00002036
Iteration 124/1000 | Loss: 0.00002035
Iteration 125/1000 | Loss: 0.00002035
Iteration 126/1000 | Loss: 0.00002035
Iteration 127/1000 | Loss: 0.00002035
Iteration 128/1000 | Loss: 0.00002035
Iteration 129/1000 | Loss: 0.00002035
Iteration 130/1000 | Loss: 0.00002034
Iteration 131/1000 | Loss: 0.00002034
Iteration 132/1000 | Loss: 0.00002034
Iteration 133/1000 | Loss: 0.00002034
Iteration 134/1000 | Loss: 0.00002034
Iteration 135/1000 | Loss: 0.00002034
Iteration 136/1000 | Loss: 0.00002034
Iteration 137/1000 | Loss: 0.00002034
Iteration 138/1000 | Loss: 0.00002034
Iteration 139/1000 | Loss: 0.00002033
Iteration 140/1000 | Loss: 0.00002033
Iteration 141/1000 | Loss: 0.00002033
Iteration 142/1000 | Loss: 0.00002033
Iteration 143/1000 | Loss: 0.00002033
Iteration 144/1000 | Loss: 0.00002033
Iteration 145/1000 | Loss: 0.00002032
Iteration 146/1000 | Loss: 0.00002032
Iteration 147/1000 | Loss: 0.00002032
Iteration 148/1000 | Loss: 0.00002032
Iteration 149/1000 | Loss: 0.00002031
Iteration 150/1000 | Loss: 0.00002031
Iteration 151/1000 | Loss: 0.00002031
Iteration 152/1000 | Loss: 0.00002030
Iteration 153/1000 | Loss: 0.00002030
Iteration 154/1000 | Loss: 0.00002030
Iteration 155/1000 | Loss: 0.00002030
Iteration 156/1000 | Loss: 0.00002030
Iteration 157/1000 | Loss: 0.00002030
Iteration 158/1000 | Loss: 0.00002029
Iteration 159/1000 | Loss: 0.00002029
Iteration 160/1000 | Loss: 0.00002029
Iteration 161/1000 | Loss: 0.00002028
Iteration 162/1000 | Loss: 0.00002028
Iteration 163/1000 | Loss: 0.00002028
Iteration 164/1000 | Loss: 0.00002028
Iteration 165/1000 | Loss: 0.00002028
Iteration 166/1000 | Loss: 0.00002028
Iteration 167/1000 | Loss: 0.00002028
Iteration 168/1000 | Loss: 0.00002027
Iteration 169/1000 | Loss: 0.00002027
Iteration 170/1000 | Loss: 0.00002027
Iteration 171/1000 | Loss: 0.00002027
Iteration 172/1000 | Loss: 0.00002027
Iteration 173/1000 | Loss: 0.00002026
Iteration 174/1000 | Loss: 0.00002026
Iteration 175/1000 | Loss: 0.00002026
Iteration 176/1000 | Loss: 0.00002026
Iteration 177/1000 | Loss: 0.00002026
Iteration 178/1000 | Loss: 0.00002026
Iteration 179/1000 | Loss: 0.00002026
Iteration 180/1000 | Loss: 0.00002026
Iteration 181/1000 | Loss: 0.00002025
Iteration 182/1000 | Loss: 0.00002025
Iteration 183/1000 | Loss: 0.00010869
Iteration 184/1000 | Loss: 0.00002307
Iteration 185/1000 | Loss: 0.00002192
Iteration 186/1000 | Loss: 0.00002150
Iteration 187/1000 | Loss: 0.00002112
Iteration 188/1000 | Loss: 0.00002078
Iteration 189/1000 | Loss: 0.00002066
Iteration 190/1000 | Loss: 0.00002047
Iteration 191/1000 | Loss: 0.00002036
Iteration 192/1000 | Loss: 0.00002015
Iteration 193/1000 | Loss: 0.00002006
Iteration 194/1000 | Loss: 0.00002006
Iteration 195/1000 | Loss: 0.00002005
Iteration 196/1000 | Loss: 0.00002000
Iteration 197/1000 | Loss: 0.00002000
Iteration 198/1000 | Loss: 0.00001998
Iteration 199/1000 | Loss: 0.00001995
Iteration 200/1000 | Loss: 0.00001995
Iteration 201/1000 | Loss: 0.00001995
Iteration 202/1000 | Loss: 0.00001995
Iteration 203/1000 | Loss: 0.00001995
Iteration 204/1000 | Loss: 0.00001994
Iteration 205/1000 | Loss: 0.00001994
Iteration 206/1000 | Loss: 0.00001994
Iteration 207/1000 | Loss: 0.00001994
Iteration 208/1000 | Loss: 0.00001994
Iteration 209/1000 | Loss: 0.00001993
Iteration 210/1000 | Loss: 0.00001993
Iteration 211/1000 | Loss: 0.00001993
Iteration 212/1000 | Loss: 0.00001992
Iteration 213/1000 | Loss: 0.00001992
Iteration 214/1000 | Loss: 0.00001992
Iteration 215/1000 | Loss: 0.00001992
Iteration 216/1000 | Loss: 0.00001991
Iteration 217/1000 | Loss: 0.00001991
Iteration 218/1000 | Loss: 0.00001991
Iteration 219/1000 | Loss: 0.00001991
Iteration 220/1000 | Loss: 0.00001991
Iteration 221/1000 | Loss: 0.00001990
Iteration 222/1000 | Loss: 0.00001990
Iteration 223/1000 | Loss: 0.00001990
Iteration 224/1000 | Loss: 0.00001990
Iteration 225/1000 | Loss: 0.00001989
Iteration 226/1000 | Loss: 0.00001989
Iteration 227/1000 | Loss: 0.00001989
Iteration 228/1000 | Loss: 0.00001989
Iteration 229/1000 | Loss: 0.00001989
Iteration 230/1000 | Loss: 0.00001988
Iteration 231/1000 | Loss: 0.00001988
Iteration 232/1000 | Loss: 0.00001988
Iteration 233/1000 | Loss: 0.00001987
Iteration 234/1000 | Loss: 0.00001987
Iteration 235/1000 | Loss: 0.00001987
Iteration 236/1000 | Loss: 0.00001986
Iteration 237/1000 | Loss: 0.00001986
Iteration 238/1000 | Loss: 0.00001986
Iteration 239/1000 | Loss: 0.00001986
Iteration 240/1000 | Loss: 0.00001985
Iteration 241/1000 | Loss: 0.00001985
Iteration 242/1000 | Loss: 0.00001985
Iteration 243/1000 | Loss: 0.00001985
Iteration 244/1000 | Loss: 0.00001984
Iteration 245/1000 | Loss: 0.00001984
Iteration 246/1000 | Loss: 0.00001984
Iteration 247/1000 | Loss: 0.00001984
Iteration 248/1000 | Loss: 0.00001983
Iteration 249/1000 | Loss: 0.00001983
Iteration 250/1000 | Loss: 0.00001983
Iteration 251/1000 | Loss: 0.00001982
Iteration 252/1000 | Loss: 0.00001982
Iteration 253/1000 | Loss: 0.00001982
Iteration 254/1000 | Loss: 0.00001982
Iteration 255/1000 | Loss: 0.00001982
Iteration 256/1000 | Loss: 0.00001982
Iteration 257/1000 | Loss: 0.00001981
Iteration 258/1000 | Loss: 0.00001981
Iteration 259/1000 | Loss: 0.00001981
Iteration 260/1000 | Loss: 0.00001981
Iteration 261/1000 | Loss: 0.00001981
Iteration 262/1000 | Loss: 0.00001981
Iteration 263/1000 | Loss: 0.00001981
Iteration 264/1000 | Loss: 0.00001981
Iteration 265/1000 | Loss: 0.00001981
Iteration 266/1000 | Loss: 0.00001981
Iteration 267/1000 | Loss: 0.00001980
Iteration 268/1000 | Loss: 0.00001980
Iteration 269/1000 | Loss: 0.00001980
Iteration 270/1000 | Loss: 0.00001980
Iteration 271/1000 | Loss: 0.00001979
Iteration 272/1000 | Loss: 0.00001979
Iteration 273/1000 | Loss: 0.00001978
Iteration 274/1000 | Loss: 0.00001978
Iteration 275/1000 | Loss: 0.00001978
Iteration 276/1000 | Loss: 0.00001978
Iteration 277/1000 | Loss: 0.00001978
Iteration 278/1000 | Loss: 0.00001978
Iteration 279/1000 | Loss: 0.00001978
Iteration 280/1000 | Loss: 0.00001978
Iteration 281/1000 | Loss: 0.00001978
Iteration 282/1000 | Loss: 0.00001978
Iteration 283/1000 | Loss: 0.00001977
Iteration 284/1000 | Loss: 0.00001977
Iteration 285/1000 | Loss: 0.00001977
Iteration 286/1000 | Loss: 0.00001977
Iteration 287/1000 | Loss: 0.00001977
Iteration 288/1000 | Loss: 0.00001976
Iteration 289/1000 | Loss: 0.00001976
Iteration 290/1000 | Loss: 0.00001976
Iteration 291/1000 | Loss: 0.00001976
Iteration 292/1000 | Loss: 0.00001976
Iteration 293/1000 | Loss: 0.00001976
Iteration 294/1000 | Loss: 0.00001975
Iteration 295/1000 | Loss: 0.00001975
Iteration 296/1000 | Loss: 0.00001975
Iteration 297/1000 | Loss: 0.00001975
Iteration 298/1000 | Loss: 0.00001975
Iteration 299/1000 | Loss: 0.00001975
Iteration 300/1000 | Loss: 0.00001975
Iteration 301/1000 | Loss: 0.00001975
Iteration 302/1000 | Loss: 0.00001975
Iteration 303/1000 | Loss: 0.00001975
Iteration 304/1000 | Loss: 0.00001975
Iteration 305/1000 | Loss: 0.00001975
Iteration 306/1000 | Loss: 0.00001975
Iteration 307/1000 | Loss: 0.00001975
Iteration 308/1000 | Loss: 0.00001975
Iteration 309/1000 | Loss: 0.00001975
Iteration 310/1000 | Loss: 0.00001975
Iteration 311/1000 | Loss: 0.00001975
Iteration 312/1000 | Loss: 0.00001975
Iteration 313/1000 | Loss: 0.00001974
Iteration 314/1000 | Loss: 0.00001974
Iteration 315/1000 | Loss: 0.00001974
Iteration 316/1000 | Loss: 0.00001974
Iteration 317/1000 | Loss: 0.00001974
Iteration 318/1000 | Loss: 0.00001974
Iteration 319/1000 | Loss: 0.00001974
Iteration 320/1000 | Loss: 0.00001974
Iteration 321/1000 | Loss: 0.00001974
Iteration 322/1000 | Loss: 0.00001974
Iteration 323/1000 | Loss: 0.00001974
Iteration 324/1000 | Loss: 0.00001974
Iteration 325/1000 | Loss: 0.00001974
Iteration 326/1000 | Loss: 0.00001974
Iteration 327/1000 | Loss: 0.00001974
Iteration 328/1000 | Loss: 0.00001974
Iteration 329/1000 | Loss: 0.00001974
Iteration 330/1000 | Loss: 0.00001974
Iteration 331/1000 | Loss: 0.00001974
Iteration 332/1000 | Loss: 0.00001974
Iteration 333/1000 | Loss: 0.00001974
Iteration 334/1000 | Loss: 0.00001974
Iteration 335/1000 | Loss: 0.00001974
Iteration 336/1000 | Loss: 0.00001974
Iteration 337/1000 | Loss: 0.00001974
Iteration 338/1000 | Loss: 0.00001974
Iteration 339/1000 | Loss: 0.00001974
Iteration 340/1000 | Loss: 0.00001974
Iteration 341/1000 | Loss: 0.00001974
Iteration 342/1000 | Loss: 0.00001974
Iteration 343/1000 | Loss: 0.00001974
Iteration 344/1000 | Loss: 0.00001974
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 344. Stopping optimization.
Last 5 losses: [1.9743119992199354e-05, 1.9743119992199354e-05, 1.9743119992199354e-05, 1.9743119992199354e-05, 1.9743119992199354e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9743119992199354e-05

Optimization complete. Final v2v error: 3.742684841156006 mm

Highest mean error: 5.3380045890808105 mm for frame 41

Lowest mean error: 3.15523624420166 mm for frame 168

Saving results

Total time: 126.16906642913818
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00970511
Iteration 2/25 | Loss: 0.00302677
Iteration 3/25 | Loss: 0.00207095
Iteration 4/25 | Loss: 0.00180660
Iteration 5/25 | Loss: 0.00185070
Iteration 6/25 | Loss: 0.00183099
Iteration 7/25 | Loss: 0.00169977
Iteration 8/25 | Loss: 0.00160147
Iteration 9/25 | Loss: 0.00161100
Iteration 10/25 | Loss: 0.00158069
Iteration 11/25 | Loss: 0.00150978
Iteration 12/25 | Loss: 0.00147782
Iteration 13/25 | Loss: 0.00145561
Iteration 14/25 | Loss: 0.00145199
Iteration 15/25 | Loss: 0.00142048
Iteration 16/25 | Loss: 0.00140073
Iteration 17/25 | Loss: 0.00138102
Iteration 18/25 | Loss: 0.00137268
Iteration 19/25 | Loss: 0.00137411
Iteration 20/25 | Loss: 0.00137709
Iteration 21/25 | Loss: 0.00137278
Iteration 22/25 | Loss: 0.00136932
Iteration 23/25 | Loss: 0.00136136
Iteration 24/25 | Loss: 0.00135884
Iteration 25/25 | Loss: 0.00135691

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37564313
Iteration 2/25 | Loss: 0.00116023
Iteration 3/25 | Loss: 0.00111223
Iteration 4/25 | Loss: 0.00111223
Iteration 5/25 | Loss: 0.00111223
Iteration 6/25 | Loss: 0.00111223
Iteration 7/25 | Loss: 0.00111223
Iteration 8/25 | Loss: 0.00111223
Iteration 9/25 | Loss: 0.00111223
Iteration 10/25 | Loss: 0.00111223
Iteration 11/25 | Loss: 0.00111223
Iteration 12/25 | Loss: 0.00111223
Iteration 13/25 | Loss: 0.00111223
Iteration 14/25 | Loss: 0.00111223
Iteration 15/25 | Loss: 0.00111223
Iteration 16/25 | Loss: 0.00111223
Iteration 17/25 | Loss: 0.00111223
Iteration 18/25 | Loss: 0.00111223
Iteration 19/25 | Loss: 0.00111223
Iteration 20/25 | Loss: 0.00111223
Iteration 21/25 | Loss: 0.00111223
Iteration 22/25 | Loss: 0.00111223
Iteration 23/25 | Loss: 0.00111223
Iteration 24/25 | Loss: 0.00111223
Iteration 25/25 | Loss: 0.00111223

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111223
Iteration 2/1000 | Loss: 0.00105781
Iteration 3/1000 | Loss: 0.00046166
Iteration 4/1000 | Loss: 0.00030687
Iteration 5/1000 | Loss: 0.00008505
Iteration 6/1000 | Loss: 0.00009010
Iteration 7/1000 | Loss: 0.00007639
Iteration 8/1000 | Loss: 0.00028730
Iteration 9/1000 | Loss: 0.00038658
Iteration 10/1000 | Loss: 0.00007670
Iteration 11/1000 | Loss: 0.00028396
Iteration 12/1000 | Loss: 0.00011125
Iteration 13/1000 | Loss: 0.00009365
Iteration 14/1000 | Loss: 0.00011498
Iteration 15/1000 | Loss: 0.00032603
Iteration 16/1000 | Loss: 0.00040286
Iteration 17/1000 | Loss: 0.00007543
Iteration 18/1000 | Loss: 0.00025942
Iteration 19/1000 | Loss: 0.00015964
Iteration 20/1000 | Loss: 0.00014191
Iteration 21/1000 | Loss: 0.00005910
Iteration 22/1000 | Loss: 0.00051277
Iteration 23/1000 | Loss: 0.00031102
Iteration 24/1000 | Loss: 0.00040322
Iteration 25/1000 | Loss: 0.00006509
Iteration 26/1000 | Loss: 0.00045985
Iteration 27/1000 | Loss: 0.00016222
Iteration 28/1000 | Loss: 0.00016496
Iteration 29/1000 | Loss: 0.00026537
Iteration 30/1000 | Loss: 0.00007925
Iteration 31/1000 | Loss: 0.00018532
Iteration 32/1000 | Loss: 0.00019171
Iteration 33/1000 | Loss: 0.00011860
Iteration 34/1000 | Loss: 0.00037497
Iteration 35/1000 | Loss: 0.00033450
Iteration 36/1000 | Loss: 0.00027206
Iteration 37/1000 | Loss: 0.00015421
Iteration 38/1000 | Loss: 0.00008485
Iteration 39/1000 | Loss: 0.00020469
Iteration 40/1000 | Loss: 0.00015492
Iteration 41/1000 | Loss: 0.00031311
Iteration 42/1000 | Loss: 0.00028375
Iteration 43/1000 | Loss: 0.00015953
Iteration 44/1000 | Loss: 0.00016377
Iteration 45/1000 | Loss: 0.00022362
Iteration 46/1000 | Loss: 0.00012808
Iteration 47/1000 | Loss: 0.00020598
Iteration 48/1000 | Loss: 0.00016413
Iteration 49/1000 | Loss: 0.00007684
Iteration 50/1000 | Loss: 0.00017297
Iteration 51/1000 | Loss: 0.00018956
Iteration 52/1000 | Loss: 0.00051466
Iteration 53/1000 | Loss: 0.00034133
Iteration 54/1000 | Loss: 0.00007120
Iteration 55/1000 | Loss: 0.00020819
Iteration 56/1000 | Loss: 0.00026735
Iteration 57/1000 | Loss: 0.00011122
Iteration 58/1000 | Loss: 0.00013076
Iteration 59/1000 | Loss: 0.00005741
Iteration 60/1000 | Loss: 0.00005227
Iteration 61/1000 | Loss: 0.00018845
Iteration 62/1000 | Loss: 0.00005911
Iteration 63/1000 | Loss: 0.00042634
Iteration 64/1000 | Loss: 0.00008026
Iteration 65/1000 | Loss: 0.00015877
Iteration 66/1000 | Loss: 0.00013113
Iteration 67/1000 | Loss: 0.00015457
Iteration 68/1000 | Loss: 0.00022597
Iteration 69/1000 | Loss: 0.00021017
Iteration 70/1000 | Loss: 0.00024175
Iteration 71/1000 | Loss: 0.00009716
Iteration 72/1000 | Loss: 0.00005983
Iteration 73/1000 | Loss: 0.00009666
Iteration 74/1000 | Loss: 0.00012827
Iteration 75/1000 | Loss: 0.00005639
Iteration 76/1000 | Loss: 0.00005650
Iteration 77/1000 | Loss: 0.00004228
Iteration 78/1000 | Loss: 0.00020176
Iteration 79/1000 | Loss: 0.00004633
Iteration 80/1000 | Loss: 0.00019889
Iteration 81/1000 | Loss: 0.00005156
Iteration 82/1000 | Loss: 0.00004795
Iteration 83/1000 | Loss: 0.00004056
Iteration 84/1000 | Loss: 0.00003215
Iteration 85/1000 | Loss: 0.00002990
Iteration 86/1000 | Loss: 0.00004261
Iteration 87/1000 | Loss: 0.00043229
Iteration 88/1000 | Loss: 0.00017919
Iteration 89/1000 | Loss: 0.00022177
Iteration 90/1000 | Loss: 0.00007731
Iteration 91/1000 | Loss: 0.00004818
Iteration 92/1000 | Loss: 0.00004428
Iteration 93/1000 | Loss: 0.00003767
Iteration 94/1000 | Loss: 0.00003026
Iteration 95/1000 | Loss: 0.00003040
Iteration 96/1000 | Loss: 0.00002761
Iteration 97/1000 | Loss: 0.00003392
Iteration 98/1000 | Loss: 0.00003399
Iteration 99/1000 | Loss: 0.00002140
Iteration 100/1000 | Loss: 0.00002704
Iteration 101/1000 | Loss: 0.00002850
Iteration 102/1000 | Loss: 0.00003385
Iteration 103/1000 | Loss: 0.00003588
Iteration 104/1000 | Loss: 0.00003100
Iteration 105/1000 | Loss: 0.00003953
Iteration 106/1000 | Loss: 0.00002701
Iteration 107/1000 | Loss: 0.00002963
Iteration 108/1000 | Loss: 0.00002979
Iteration 109/1000 | Loss: 0.00001969
Iteration 110/1000 | Loss: 0.00002133
Iteration 111/1000 | Loss: 0.00003012
Iteration 112/1000 | Loss: 0.00002853
Iteration 113/1000 | Loss: 0.00003703
Iteration 114/1000 | Loss: 0.00002143
Iteration 115/1000 | Loss: 0.00002065
Iteration 116/1000 | Loss: 0.00002014
Iteration 117/1000 | Loss: 0.00001941
Iteration 118/1000 | Loss: 0.00001885
Iteration 119/1000 | Loss: 0.00001858
Iteration 120/1000 | Loss: 0.00001832
Iteration 121/1000 | Loss: 0.00001811
Iteration 122/1000 | Loss: 0.00001791
Iteration 123/1000 | Loss: 0.00001788
Iteration 124/1000 | Loss: 0.00001780
Iteration 125/1000 | Loss: 0.00001770
Iteration 126/1000 | Loss: 0.00001762
Iteration 127/1000 | Loss: 0.00001762
Iteration 128/1000 | Loss: 0.00001762
Iteration 129/1000 | Loss: 0.00001762
Iteration 130/1000 | Loss: 0.00001762
Iteration 131/1000 | Loss: 0.00001762
Iteration 132/1000 | Loss: 0.00001762
Iteration 133/1000 | Loss: 0.00001762
Iteration 134/1000 | Loss: 0.00001762
Iteration 135/1000 | Loss: 0.00001762
Iteration 136/1000 | Loss: 0.00001759
Iteration 137/1000 | Loss: 0.00001759
Iteration 138/1000 | Loss: 0.00001755
Iteration 139/1000 | Loss: 0.00001754
Iteration 140/1000 | Loss: 0.00001752
Iteration 141/1000 | Loss: 0.00001752
Iteration 142/1000 | Loss: 0.00001752
Iteration 143/1000 | Loss: 0.00001751
Iteration 144/1000 | Loss: 0.00001750
Iteration 145/1000 | Loss: 0.00001750
Iteration 146/1000 | Loss: 0.00001750
Iteration 147/1000 | Loss: 0.00001749
Iteration 148/1000 | Loss: 0.00001749
Iteration 149/1000 | Loss: 0.00001749
Iteration 150/1000 | Loss: 0.00001749
Iteration 151/1000 | Loss: 0.00001749
Iteration 152/1000 | Loss: 0.00001749
Iteration 153/1000 | Loss: 0.00001749
Iteration 154/1000 | Loss: 0.00001749
Iteration 155/1000 | Loss: 0.00001749
Iteration 156/1000 | Loss: 0.00001749
Iteration 157/1000 | Loss: 0.00001749
Iteration 158/1000 | Loss: 0.00001748
Iteration 159/1000 | Loss: 0.00001748
Iteration 160/1000 | Loss: 0.00001748
Iteration 161/1000 | Loss: 0.00001748
Iteration 162/1000 | Loss: 0.00001747
Iteration 163/1000 | Loss: 0.00001747
Iteration 164/1000 | Loss: 0.00001747
Iteration 165/1000 | Loss: 0.00001747
Iteration 166/1000 | Loss: 0.00001746
Iteration 167/1000 | Loss: 0.00001746
Iteration 168/1000 | Loss: 0.00001746
Iteration 169/1000 | Loss: 0.00001746
Iteration 170/1000 | Loss: 0.00001746
Iteration 171/1000 | Loss: 0.00001746
Iteration 172/1000 | Loss: 0.00001746
Iteration 173/1000 | Loss: 0.00001745
Iteration 174/1000 | Loss: 0.00001745
Iteration 175/1000 | Loss: 0.00001744
Iteration 176/1000 | Loss: 0.00001743
Iteration 177/1000 | Loss: 0.00001743
Iteration 178/1000 | Loss: 0.00001743
Iteration 179/1000 | Loss: 0.00001742
Iteration 180/1000 | Loss: 0.00001742
Iteration 181/1000 | Loss: 0.00001742
Iteration 182/1000 | Loss: 0.00001742
Iteration 183/1000 | Loss: 0.00001742
Iteration 184/1000 | Loss: 0.00001742
Iteration 185/1000 | Loss: 0.00001742
Iteration 186/1000 | Loss: 0.00001742
Iteration 187/1000 | Loss: 0.00001742
Iteration 188/1000 | Loss: 0.00001742
Iteration 189/1000 | Loss: 0.00001742
Iteration 190/1000 | Loss: 0.00001742
Iteration 191/1000 | Loss: 0.00001741
Iteration 192/1000 | Loss: 0.00001741
Iteration 193/1000 | Loss: 0.00001741
Iteration 194/1000 | Loss: 0.00001741
Iteration 195/1000 | Loss: 0.00001741
Iteration 196/1000 | Loss: 0.00001741
Iteration 197/1000 | Loss: 0.00001741
Iteration 198/1000 | Loss: 0.00001740
Iteration 199/1000 | Loss: 0.00001740
Iteration 200/1000 | Loss: 0.00001740
Iteration 201/1000 | Loss: 0.00001740
Iteration 202/1000 | Loss: 0.00001740
Iteration 203/1000 | Loss: 0.00001740
Iteration 204/1000 | Loss: 0.00001740
Iteration 205/1000 | Loss: 0.00001740
Iteration 206/1000 | Loss: 0.00001740
Iteration 207/1000 | Loss: 0.00001740
Iteration 208/1000 | Loss: 0.00001739
Iteration 209/1000 | Loss: 0.00001739
Iteration 210/1000 | Loss: 0.00001739
Iteration 211/1000 | Loss: 0.00001739
Iteration 212/1000 | Loss: 0.00001739
Iteration 213/1000 | Loss: 0.00001739
Iteration 214/1000 | Loss: 0.00001738
Iteration 215/1000 | Loss: 0.00001738
Iteration 216/1000 | Loss: 0.00001738
Iteration 217/1000 | Loss: 0.00001738
Iteration 218/1000 | Loss: 0.00001738
Iteration 219/1000 | Loss: 0.00001738
Iteration 220/1000 | Loss: 0.00001738
Iteration 221/1000 | Loss: 0.00001738
Iteration 222/1000 | Loss: 0.00001738
Iteration 223/1000 | Loss: 0.00001738
Iteration 224/1000 | Loss: 0.00001738
Iteration 225/1000 | Loss: 0.00001737
Iteration 226/1000 | Loss: 0.00001737
Iteration 227/1000 | Loss: 0.00001737
Iteration 228/1000 | Loss: 0.00001737
Iteration 229/1000 | Loss: 0.00001737
Iteration 230/1000 | Loss: 0.00001737
Iteration 231/1000 | Loss: 0.00001737
Iteration 232/1000 | Loss: 0.00001737
Iteration 233/1000 | Loss: 0.00001737
Iteration 234/1000 | Loss: 0.00001737
Iteration 235/1000 | Loss: 0.00001737
Iteration 236/1000 | Loss: 0.00001737
Iteration 237/1000 | Loss: 0.00001737
Iteration 238/1000 | Loss: 0.00001736
Iteration 239/1000 | Loss: 0.00001736
Iteration 240/1000 | Loss: 0.00001736
Iteration 241/1000 | Loss: 0.00001736
Iteration 242/1000 | Loss: 0.00001735
Iteration 243/1000 | Loss: 0.00001735
Iteration 244/1000 | Loss: 0.00001735
Iteration 245/1000 | Loss: 0.00001735
Iteration 246/1000 | Loss: 0.00001735
Iteration 247/1000 | Loss: 0.00001735
Iteration 248/1000 | Loss: 0.00001735
Iteration 249/1000 | Loss: 0.00001735
Iteration 250/1000 | Loss: 0.00001735
Iteration 251/1000 | Loss: 0.00001735
Iteration 252/1000 | Loss: 0.00001735
Iteration 253/1000 | Loss: 0.00001735
Iteration 254/1000 | Loss: 0.00001735
Iteration 255/1000 | Loss: 0.00001735
Iteration 256/1000 | Loss: 0.00001735
Iteration 257/1000 | Loss: 0.00001735
Iteration 258/1000 | Loss: 0.00001735
Iteration 259/1000 | Loss: 0.00001734
Iteration 260/1000 | Loss: 0.00001734
Iteration 261/1000 | Loss: 0.00001734
Iteration 262/1000 | Loss: 0.00001734
Iteration 263/1000 | Loss: 0.00001734
Iteration 264/1000 | Loss: 0.00001734
Iteration 265/1000 | Loss: 0.00001734
Iteration 266/1000 | Loss: 0.00001734
Iteration 267/1000 | Loss: 0.00001734
Iteration 268/1000 | Loss: 0.00001734
Iteration 269/1000 | Loss: 0.00001734
Iteration 270/1000 | Loss: 0.00001734
Iteration 271/1000 | Loss: 0.00001734
Iteration 272/1000 | Loss: 0.00001734
Iteration 273/1000 | Loss: 0.00001734
Iteration 274/1000 | Loss: 0.00001734
Iteration 275/1000 | Loss: 0.00001734
Iteration 276/1000 | Loss: 0.00001734
Iteration 277/1000 | Loss: 0.00001734
Iteration 278/1000 | Loss: 0.00001733
Iteration 279/1000 | Loss: 0.00001733
Iteration 280/1000 | Loss: 0.00001733
Iteration 281/1000 | Loss: 0.00001733
Iteration 282/1000 | Loss: 0.00001733
Iteration 283/1000 | Loss: 0.00001733
Iteration 284/1000 | Loss: 0.00001733
Iteration 285/1000 | Loss: 0.00001733
Iteration 286/1000 | Loss: 0.00001733
Iteration 287/1000 | Loss: 0.00001733
Iteration 288/1000 | Loss: 0.00001733
Iteration 289/1000 | Loss: 0.00001733
Iteration 290/1000 | Loss: 0.00001733
Iteration 291/1000 | Loss: 0.00001733
Iteration 292/1000 | Loss: 0.00001733
Iteration 293/1000 | Loss: 0.00001733
Iteration 294/1000 | Loss: 0.00001733
Iteration 295/1000 | Loss: 0.00001733
Iteration 296/1000 | Loss: 0.00001733
Iteration 297/1000 | Loss: 0.00001733
Iteration 298/1000 | Loss: 0.00001733
Iteration 299/1000 | Loss: 0.00001733
Iteration 300/1000 | Loss: 0.00001733
Iteration 301/1000 | Loss: 0.00001733
Iteration 302/1000 | Loss: 0.00001733
Iteration 303/1000 | Loss: 0.00001733
Iteration 304/1000 | Loss: 0.00001733
Iteration 305/1000 | Loss: 0.00001733
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 305. Stopping optimization.
Last 5 losses: [1.7326094166492112e-05, 1.7326094166492112e-05, 1.7326094166492112e-05, 1.7326094166492112e-05, 1.7326094166492112e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7326094166492112e-05

Optimization complete. Final v2v error: 3.3902792930603027 mm

Highest mean error: 5.201460838317871 mm for frame 95

Lowest mean error: 3.155717134475708 mm for frame 87

Saving results

Total time: 261.30812644958496
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00355418
Iteration 2/25 | Loss: 0.00143123
Iteration 3/25 | Loss: 0.00128817
Iteration 4/25 | Loss: 0.00127572
Iteration 5/25 | Loss: 0.00127102
Iteration 6/25 | Loss: 0.00126964
Iteration 7/25 | Loss: 0.00126964
Iteration 8/25 | Loss: 0.00126964
Iteration 9/25 | Loss: 0.00126964
Iteration 10/25 | Loss: 0.00126964
Iteration 11/25 | Loss: 0.00126964
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001269641099497676, 0.001269641099497676, 0.001269641099497676, 0.001269641099497676, 0.001269641099497676]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001269641099497676

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36001074
Iteration 2/25 | Loss: 0.00091903
Iteration 3/25 | Loss: 0.00091903
Iteration 4/25 | Loss: 0.00091903
Iteration 5/25 | Loss: 0.00091903
Iteration 6/25 | Loss: 0.00091903
Iteration 7/25 | Loss: 0.00091903
Iteration 8/25 | Loss: 0.00091903
Iteration 9/25 | Loss: 0.00091903
Iteration 10/25 | Loss: 0.00091902
Iteration 11/25 | Loss: 0.00091902
Iteration 12/25 | Loss: 0.00091902
Iteration 13/25 | Loss: 0.00091902
Iteration 14/25 | Loss: 0.00091902
Iteration 15/25 | Loss: 0.00091902
Iteration 16/25 | Loss: 0.00091902
Iteration 17/25 | Loss: 0.00091902
Iteration 18/25 | Loss: 0.00091902
Iteration 19/25 | Loss: 0.00091902
Iteration 20/25 | Loss: 0.00091902
Iteration 21/25 | Loss: 0.00091902
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009190243436023593, 0.0009190243436023593, 0.0009190243436023593, 0.0009190243436023593, 0.0009190243436023593]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009190243436023593

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091902
Iteration 2/1000 | Loss: 0.00003455
Iteration 3/1000 | Loss: 0.00002363
Iteration 4/1000 | Loss: 0.00002052
Iteration 5/1000 | Loss: 0.00001953
Iteration 6/1000 | Loss: 0.00001857
Iteration 7/1000 | Loss: 0.00001794
Iteration 8/1000 | Loss: 0.00001745
Iteration 9/1000 | Loss: 0.00001719
Iteration 10/1000 | Loss: 0.00001710
Iteration 11/1000 | Loss: 0.00001682
Iteration 12/1000 | Loss: 0.00001674
Iteration 13/1000 | Loss: 0.00001661
Iteration 14/1000 | Loss: 0.00001658
Iteration 15/1000 | Loss: 0.00001653
Iteration 16/1000 | Loss: 0.00001649
Iteration 17/1000 | Loss: 0.00001642
Iteration 18/1000 | Loss: 0.00001642
Iteration 19/1000 | Loss: 0.00001635
Iteration 20/1000 | Loss: 0.00001635
Iteration 21/1000 | Loss: 0.00001634
Iteration 22/1000 | Loss: 0.00001633
Iteration 23/1000 | Loss: 0.00001631
Iteration 24/1000 | Loss: 0.00001629
Iteration 25/1000 | Loss: 0.00001629
Iteration 26/1000 | Loss: 0.00001625
Iteration 27/1000 | Loss: 0.00001625
Iteration 28/1000 | Loss: 0.00001622
Iteration 29/1000 | Loss: 0.00001618
Iteration 30/1000 | Loss: 0.00001618
Iteration 31/1000 | Loss: 0.00001618
Iteration 32/1000 | Loss: 0.00001617
Iteration 33/1000 | Loss: 0.00001615
Iteration 34/1000 | Loss: 0.00001615
Iteration 35/1000 | Loss: 0.00001614
Iteration 36/1000 | Loss: 0.00001614
Iteration 37/1000 | Loss: 0.00001613
Iteration 38/1000 | Loss: 0.00001613
Iteration 39/1000 | Loss: 0.00001612
Iteration 40/1000 | Loss: 0.00001612
Iteration 41/1000 | Loss: 0.00001612
Iteration 42/1000 | Loss: 0.00001611
Iteration 43/1000 | Loss: 0.00001611
Iteration 44/1000 | Loss: 0.00001610
Iteration 45/1000 | Loss: 0.00001610
Iteration 46/1000 | Loss: 0.00001609
Iteration 47/1000 | Loss: 0.00001608
Iteration 48/1000 | Loss: 0.00001607
Iteration 49/1000 | Loss: 0.00001606
Iteration 50/1000 | Loss: 0.00001606
Iteration 51/1000 | Loss: 0.00001605
Iteration 52/1000 | Loss: 0.00001604
Iteration 53/1000 | Loss: 0.00001603
Iteration 54/1000 | Loss: 0.00001603
Iteration 55/1000 | Loss: 0.00001602
Iteration 56/1000 | Loss: 0.00001601
Iteration 57/1000 | Loss: 0.00001601
Iteration 58/1000 | Loss: 0.00001600
Iteration 59/1000 | Loss: 0.00001599
Iteration 60/1000 | Loss: 0.00001599
Iteration 61/1000 | Loss: 0.00001599
Iteration 62/1000 | Loss: 0.00001598
Iteration 63/1000 | Loss: 0.00001598
Iteration 64/1000 | Loss: 0.00001598
Iteration 65/1000 | Loss: 0.00001597
Iteration 66/1000 | Loss: 0.00001597
Iteration 67/1000 | Loss: 0.00001597
Iteration 68/1000 | Loss: 0.00001597
Iteration 69/1000 | Loss: 0.00001596
Iteration 70/1000 | Loss: 0.00001596
Iteration 71/1000 | Loss: 0.00001596
Iteration 72/1000 | Loss: 0.00001595
Iteration 73/1000 | Loss: 0.00001595
Iteration 74/1000 | Loss: 0.00001595
Iteration 75/1000 | Loss: 0.00001595
Iteration 76/1000 | Loss: 0.00001594
Iteration 77/1000 | Loss: 0.00001594
Iteration 78/1000 | Loss: 0.00001594
Iteration 79/1000 | Loss: 0.00001594
Iteration 80/1000 | Loss: 0.00001594
Iteration 81/1000 | Loss: 0.00001594
Iteration 82/1000 | Loss: 0.00001594
Iteration 83/1000 | Loss: 0.00001594
Iteration 84/1000 | Loss: 0.00001593
Iteration 85/1000 | Loss: 0.00001593
Iteration 86/1000 | Loss: 0.00001593
Iteration 87/1000 | Loss: 0.00001593
Iteration 88/1000 | Loss: 0.00001593
Iteration 89/1000 | Loss: 0.00001593
Iteration 90/1000 | Loss: 0.00001592
Iteration 91/1000 | Loss: 0.00001592
Iteration 92/1000 | Loss: 0.00001592
Iteration 93/1000 | Loss: 0.00001592
Iteration 94/1000 | Loss: 0.00001592
Iteration 95/1000 | Loss: 0.00001591
Iteration 96/1000 | Loss: 0.00001591
Iteration 97/1000 | Loss: 0.00001591
Iteration 98/1000 | Loss: 0.00001591
Iteration 99/1000 | Loss: 0.00001591
Iteration 100/1000 | Loss: 0.00001590
Iteration 101/1000 | Loss: 0.00001590
Iteration 102/1000 | Loss: 0.00001590
Iteration 103/1000 | Loss: 0.00001590
Iteration 104/1000 | Loss: 0.00001590
Iteration 105/1000 | Loss: 0.00001590
Iteration 106/1000 | Loss: 0.00001590
Iteration 107/1000 | Loss: 0.00001590
Iteration 108/1000 | Loss: 0.00001590
Iteration 109/1000 | Loss: 0.00001590
Iteration 110/1000 | Loss: 0.00001590
Iteration 111/1000 | Loss: 0.00001590
Iteration 112/1000 | Loss: 0.00001590
Iteration 113/1000 | Loss: 0.00001590
Iteration 114/1000 | Loss: 0.00001590
Iteration 115/1000 | Loss: 0.00001590
Iteration 116/1000 | Loss: 0.00001590
Iteration 117/1000 | Loss: 0.00001590
Iteration 118/1000 | Loss: 0.00001589
Iteration 119/1000 | Loss: 0.00001589
Iteration 120/1000 | Loss: 0.00001589
Iteration 121/1000 | Loss: 0.00001589
Iteration 122/1000 | Loss: 0.00001589
Iteration 123/1000 | Loss: 0.00001589
Iteration 124/1000 | Loss: 0.00001589
Iteration 125/1000 | Loss: 0.00001589
Iteration 126/1000 | Loss: 0.00001589
Iteration 127/1000 | Loss: 0.00001588
Iteration 128/1000 | Loss: 0.00001588
Iteration 129/1000 | Loss: 0.00001588
Iteration 130/1000 | Loss: 0.00001588
Iteration 131/1000 | Loss: 0.00001588
Iteration 132/1000 | Loss: 0.00001588
Iteration 133/1000 | Loss: 0.00001588
Iteration 134/1000 | Loss: 0.00001588
Iteration 135/1000 | Loss: 0.00001588
Iteration 136/1000 | Loss: 0.00001588
Iteration 137/1000 | Loss: 0.00001588
Iteration 138/1000 | Loss: 0.00001587
Iteration 139/1000 | Loss: 0.00001587
Iteration 140/1000 | Loss: 0.00001587
Iteration 141/1000 | Loss: 0.00001587
Iteration 142/1000 | Loss: 0.00001587
Iteration 143/1000 | Loss: 0.00001587
Iteration 144/1000 | Loss: 0.00001587
Iteration 145/1000 | Loss: 0.00001587
Iteration 146/1000 | Loss: 0.00001587
Iteration 147/1000 | Loss: 0.00001587
Iteration 148/1000 | Loss: 0.00001587
Iteration 149/1000 | Loss: 0.00001587
Iteration 150/1000 | Loss: 0.00001587
Iteration 151/1000 | Loss: 0.00001587
Iteration 152/1000 | Loss: 0.00001587
Iteration 153/1000 | Loss: 0.00001587
Iteration 154/1000 | Loss: 0.00001587
Iteration 155/1000 | Loss: 0.00001587
Iteration 156/1000 | Loss: 0.00001587
Iteration 157/1000 | Loss: 0.00001587
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.5869412891333923e-05, 1.5869412891333923e-05, 1.5869412891333923e-05, 1.5869412891333923e-05, 1.5869412891333923e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5869412891333923e-05

Optimization complete. Final v2v error: 3.4240570068359375 mm

Highest mean error: 3.813176155090332 mm for frame 18

Lowest mean error: 3.1339774131774902 mm for frame 205

Saving results

Total time: 41.64992427825928
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01034955
Iteration 2/25 | Loss: 0.00223884
Iteration 3/25 | Loss: 0.00179456
Iteration 4/25 | Loss: 0.00169657
Iteration 5/25 | Loss: 0.00171516
Iteration 6/25 | Loss: 0.00166103
Iteration 7/25 | Loss: 0.00158970
Iteration 8/25 | Loss: 0.00153386
Iteration 9/25 | Loss: 0.00143366
Iteration 10/25 | Loss: 0.00141515
Iteration 11/25 | Loss: 0.00140318
Iteration 12/25 | Loss: 0.00140441
Iteration 13/25 | Loss: 0.00139985
Iteration 14/25 | Loss: 0.00138919
Iteration 15/25 | Loss: 0.00138587
Iteration 16/25 | Loss: 0.00138372
Iteration 17/25 | Loss: 0.00137824
Iteration 18/25 | Loss: 0.00137447
Iteration 19/25 | Loss: 0.00136899
Iteration 20/25 | Loss: 0.00137115
Iteration 21/25 | Loss: 0.00136899
Iteration 22/25 | Loss: 0.00136649
Iteration 23/25 | Loss: 0.00136963
Iteration 24/25 | Loss: 0.00136513
Iteration 25/25 | Loss: 0.00136204

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39178824
Iteration 2/25 | Loss: 0.00148672
Iteration 3/25 | Loss: 0.00110780
Iteration 4/25 | Loss: 0.00110779
Iteration 5/25 | Loss: 0.00110779
Iteration 6/25 | Loss: 0.00110779
Iteration 7/25 | Loss: 0.00110779
Iteration 8/25 | Loss: 0.00110779
Iteration 9/25 | Loss: 0.00110779
Iteration 10/25 | Loss: 0.00110779
Iteration 11/25 | Loss: 0.00110779
Iteration 12/25 | Loss: 0.00110779
Iteration 13/25 | Loss: 0.00110779
Iteration 14/25 | Loss: 0.00110779
Iteration 15/25 | Loss: 0.00110779
Iteration 16/25 | Loss: 0.00110779
Iteration 17/25 | Loss: 0.00110779
Iteration 18/25 | Loss: 0.00110779
Iteration 19/25 | Loss: 0.00110779
Iteration 20/25 | Loss: 0.00110779
Iteration 21/25 | Loss: 0.00110779
Iteration 22/25 | Loss: 0.00110779
Iteration 23/25 | Loss: 0.00110779
Iteration 24/25 | Loss: 0.00110779
Iteration 25/25 | Loss: 0.00110779

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110779
Iteration 2/1000 | Loss: 0.00030660
Iteration 3/1000 | Loss: 0.00030763
Iteration 4/1000 | Loss: 0.00030465
Iteration 5/1000 | Loss: 0.00011101
Iteration 6/1000 | Loss: 0.00023801
Iteration 7/1000 | Loss: 0.00016890
Iteration 8/1000 | Loss: 0.00018947
Iteration 9/1000 | Loss: 0.00024146
Iteration 10/1000 | Loss: 0.00025122
Iteration 11/1000 | Loss: 0.00024138
Iteration 12/1000 | Loss: 0.00022850
Iteration 13/1000 | Loss: 0.00024069
Iteration 14/1000 | Loss: 0.00021357
Iteration 15/1000 | Loss: 0.00020158
Iteration 16/1000 | Loss: 0.00015158
Iteration 17/1000 | Loss: 0.00024701
Iteration 18/1000 | Loss: 0.00004958
Iteration 19/1000 | Loss: 0.00013505
Iteration 20/1000 | Loss: 0.00011570
Iteration 21/1000 | Loss: 0.00005391
Iteration 22/1000 | Loss: 0.00005332
Iteration 23/1000 | Loss: 0.00004261
Iteration 24/1000 | Loss: 0.00005756
Iteration 25/1000 | Loss: 0.00011828
Iteration 26/1000 | Loss: 0.00006919
Iteration 27/1000 | Loss: 0.00016746
Iteration 28/1000 | Loss: 0.00015867
Iteration 29/1000 | Loss: 0.00016925
Iteration 30/1000 | Loss: 0.00011221
Iteration 31/1000 | Loss: 0.00006461
Iteration 32/1000 | Loss: 0.00010993
Iteration 33/1000 | Loss: 0.00010334
Iteration 34/1000 | Loss: 0.00010679
Iteration 35/1000 | Loss: 0.00011753
Iteration 36/1000 | Loss: 0.00006230
Iteration 37/1000 | Loss: 0.00011127
Iteration 38/1000 | Loss: 0.00011177
Iteration 39/1000 | Loss: 0.00009919
Iteration 40/1000 | Loss: 0.00013627
Iteration 41/1000 | Loss: 0.00009974
Iteration 42/1000 | Loss: 0.00009807
Iteration 43/1000 | Loss: 0.00010013
Iteration 44/1000 | Loss: 0.00008184
Iteration 45/1000 | Loss: 0.00011363
Iteration 46/1000 | Loss: 0.00013462
Iteration 47/1000 | Loss: 0.00011959
Iteration 48/1000 | Loss: 0.00013881
Iteration 49/1000 | Loss: 0.00009886
Iteration 50/1000 | Loss: 0.00012548
Iteration 51/1000 | Loss: 0.00012497
Iteration 52/1000 | Loss: 0.00014375
Iteration 53/1000 | Loss: 0.00013315
Iteration 54/1000 | Loss: 0.00014982
Iteration 55/1000 | Loss: 0.00018220
Iteration 56/1000 | Loss: 0.00015038
Iteration 57/1000 | Loss: 0.00017375
Iteration 58/1000 | Loss: 0.00026789
Iteration 59/1000 | Loss: 0.00004412
Iteration 60/1000 | Loss: 0.00004312
Iteration 61/1000 | Loss: 0.00038989
Iteration 62/1000 | Loss: 0.00005566
Iteration 63/1000 | Loss: 0.00004791
Iteration 64/1000 | Loss: 0.00003966
Iteration 65/1000 | Loss: 0.00005213
Iteration 66/1000 | Loss: 0.00004995
Iteration 67/1000 | Loss: 0.00003151
Iteration 68/1000 | Loss: 0.00005448
Iteration 69/1000 | Loss: 0.00017358
Iteration 70/1000 | Loss: 0.00015414
Iteration 71/1000 | Loss: 0.00004493
Iteration 72/1000 | Loss: 0.00004806
Iteration 73/1000 | Loss: 0.00004529
Iteration 74/1000 | Loss: 0.00004949
Iteration 75/1000 | Loss: 0.00007816
Iteration 76/1000 | Loss: 0.00003736
Iteration 77/1000 | Loss: 0.00003431
Iteration 78/1000 | Loss: 0.00003562
Iteration 79/1000 | Loss: 0.00003797
Iteration 80/1000 | Loss: 0.00004266
Iteration 81/1000 | Loss: 0.00005084
Iteration 82/1000 | Loss: 0.00007594
Iteration 83/1000 | Loss: 0.00005133
Iteration 84/1000 | Loss: 0.00005049
Iteration 85/1000 | Loss: 0.00006891
Iteration 86/1000 | Loss: 0.00004113
Iteration 87/1000 | Loss: 0.00005883
Iteration 88/1000 | Loss: 0.00004270
Iteration 89/1000 | Loss: 0.00005260
Iteration 90/1000 | Loss: 0.00003959
Iteration 91/1000 | Loss: 0.00004245
Iteration 92/1000 | Loss: 0.00004059
Iteration 93/1000 | Loss: 0.00005159
Iteration 94/1000 | Loss: 0.00004047
Iteration 95/1000 | Loss: 0.00004407
Iteration 96/1000 | Loss: 0.00005003
Iteration 97/1000 | Loss: 0.00004353
Iteration 98/1000 | Loss: 0.00002837
Iteration 99/1000 | Loss: 0.00002963
Iteration 100/1000 | Loss: 0.00003881
Iteration 101/1000 | Loss: 0.00004765
Iteration 102/1000 | Loss: 0.00003950
Iteration 103/1000 | Loss: 0.00004665
Iteration 104/1000 | Loss: 0.00004118
Iteration 105/1000 | Loss: 0.00004742
Iteration 106/1000 | Loss: 0.00003720
Iteration 107/1000 | Loss: 0.00004280
Iteration 108/1000 | Loss: 0.00003566
Iteration 109/1000 | Loss: 0.00003553
Iteration 110/1000 | Loss: 0.00003785
Iteration 111/1000 | Loss: 0.00004892
Iteration 112/1000 | Loss: 0.00004139
Iteration 113/1000 | Loss: 0.00004709
Iteration 114/1000 | Loss: 0.00004109
Iteration 115/1000 | Loss: 0.00004170
Iteration 116/1000 | Loss: 0.00004467
Iteration 117/1000 | Loss: 0.00004608
Iteration 118/1000 | Loss: 0.00003563
Iteration 119/1000 | Loss: 0.00003070
Iteration 120/1000 | Loss: 0.00002795
Iteration 121/1000 | Loss: 0.00004983
Iteration 122/1000 | Loss: 0.00004519
Iteration 123/1000 | Loss: 0.00004053
Iteration 124/1000 | Loss: 0.00004812
Iteration 125/1000 | Loss: 0.00004138
Iteration 126/1000 | Loss: 0.00004288
Iteration 127/1000 | Loss: 0.00003809
Iteration 128/1000 | Loss: 0.00005097
Iteration 129/1000 | Loss: 0.00003941
Iteration 130/1000 | Loss: 0.00003950
Iteration 131/1000 | Loss: 0.00004496
Iteration 132/1000 | Loss: 0.00003039
Iteration 133/1000 | Loss: 0.00003624
Iteration 134/1000 | Loss: 0.00003508
Iteration 135/1000 | Loss: 0.00004176
Iteration 136/1000 | Loss: 0.00004535
Iteration 137/1000 | Loss: 0.00003498
Iteration 138/1000 | Loss: 0.00004692
Iteration 139/1000 | Loss: 0.00004860
Iteration 140/1000 | Loss: 0.00004735
Iteration 141/1000 | Loss: 0.00004131
Iteration 142/1000 | Loss: 0.00004575
Iteration 143/1000 | Loss: 0.00004939
Iteration 144/1000 | Loss: 0.00006374
Iteration 145/1000 | Loss: 0.00003605
Iteration 146/1000 | Loss: 0.00002734
Iteration 147/1000 | Loss: 0.00003292
Iteration 148/1000 | Loss: 0.00003562
Iteration 149/1000 | Loss: 0.00003166
Iteration 150/1000 | Loss: 0.00003248
Iteration 151/1000 | Loss: 0.00003727
Iteration 152/1000 | Loss: 0.00003657
Iteration 153/1000 | Loss: 0.00003170
Iteration 154/1000 | Loss: 0.00003370
Iteration 155/1000 | Loss: 0.00002961
Iteration 156/1000 | Loss: 0.00003525
Iteration 157/1000 | Loss: 0.00003229
Iteration 158/1000 | Loss: 0.00003170
Iteration 159/1000 | Loss: 0.00003479
Iteration 160/1000 | Loss: 0.00003275
Iteration 161/1000 | Loss: 0.00003446
Iteration 162/1000 | Loss: 0.00003408
Iteration 163/1000 | Loss: 0.00003563
Iteration 164/1000 | Loss: 0.00003546
Iteration 165/1000 | Loss: 0.00003401
Iteration 166/1000 | Loss: 0.00003257
Iteration 167/1000 | Loss: 0.00003390
Iteration 168/1000 | Loss: 0.00003551
Iteration 169/1000 | Loss: 0.00003197
Iteration 170/1000 | Loss: 0.00003509
Iteration 171/1000 | Loss: 0.00003591
Iteration 172/1000 | Loss: 0.00003451
Iteration 173/1000 | Loss: 0.00003416
Iteration 174/1000 | Loss: 0.00003579
Iteration 175/1000 | Loss: 0.00003533
Iteration 176/1000 | Loss: 0.00003651
Iteration 177/1000 | Loss: 0.00003332
Iteration 178/1000 | Loss: 0.00003474
Iteration 179/1000 | Loss: 0.00003017
Iteration 180/1000 | Loss: 0.00003623
Iteration 181/1000 | Loss: 0.00003427
Iteration 182/1000 | Loss: 0.00003533
Iteration 183/1000 | Loss: 0.00003473
Iteration 184/1000 | Loss: 0.00003112
Iteration 185/1000 | Loss: 0.00003395
Iteration 186/1000 | Loss: 0.00003590
Iteration 187/1000 | Loss: 0.00004291
Iteration 188/1000 | Loss: 0.00002239
Iteration 189/1000 | Loss: 0.00002223
Iteration 190/1000 | Loss: 0.00002223
Iteration 191/1000 | Loss: 0.00002222
Iteration 192/1000 | Loss: 0.00002222
Iteration 193/1000 | Loss: 0.00002221
Iteration 194/1000 | Loss: 0.00002221
Iteration 195/1000 | Loss: 0.00002220
Iteration 196/1000 | Loss: 0.00002214
Iteration 197/1000 | Loss: 0.00003616
Iteration 198/1000 | Loss: 0.00010188
Iteration 199/1000 | Loss: 0.00011340
Iteration 200/1000 | Loss: 0.00004900
Iteration 201/1000 | Loss: 0.00003355
Iteration 202/1000 | Loss: 0.00003695
Iteration 203/1000 | Loss: 0.00004032
Iteration 204/1000 | Loss: 0.00004116
Iteration 205/1000 | Loss: 0.00004307
Iteration 206/1000 | Loss: 0.00004526
Iteration 207/1000 | Loss: 0.00003634
Iteration 208/1000 | Loss: 0.00003448
Iteration 209/1000 | Loss: 0.00005301
Iteration 210/1000 | Loss: 0.00002344
Iteration 211/1000 | Loss: 0.00003916
Iteration 212/1000 | Loss: 0.00004845
Iteration 213/1000 | Loss: 0.00003561
Iteration 214/1000 | Loss: 0.00004816
Iteration 215/1000 | Loss: 0.00002658
Iteration 216/1000 | Loss: 0.00002385
Iteration 217/1000 | Loss: 0.00002252
Iteration 218/1000 | Loss: 0.00002215
Iteration 219/1000 | Loss: 0.00002194
Iteration 220/1000 | Loss: 0.00002181
Iteration 221/1000 | Loss: 0.00005914
Iteration 222/1000 | Loss: 0.00006136
Iteration 223/1000 | Loss: 0.00002177
Iteration 224/1000 | Loss: 0.00002163
Iteration 225/1000 | Loss: 0.00002157
Iteration 226/1000 | Loss: 0.00002156
Iteration 227/1000 | Loss: 0.00002155
Iteration 228/1000 | Loss: 0.00002155
Iteration 229/1000 | Loss: 0.00002155
Iteration 230/1000 | Loss: 0.00002154
Iteration 231/1000 | Loss: 0.00002153
Iteration 232/1000 | Loss: 0.00002153
Iteration 233/1000 | Loss: 0.00002153
Iteration 234/1000 | Loss: 0.00002153
Iteration 235/1000 | Loss: 0.00002153
Iteration 236/1000 | Loss: 0.00002153
Iteration 237/1000 | Loss: 0.00002153
Iteration 238/1000 | Loss: 0.00002153
Iteration 239/1000 | Loss: 0.00002153
Iteration 240/1000 | Loss: 0.00002153
Iteration 241/1000 | Loss: 0.00002153
Iteration 242/1000 | Loss: 0.00002153
Iteration 243/1000 | Loss: 0.00002152
Iteration 244/1000 | Loss: 0.00002152
Iteration 245/1000 | Loss: 0.00002151
Iteration 246/1000 | Loss: 0.00002151
Iteration 247/1000 | Loss: 0.00002150
Iteration 248/1000 | Loss: 0.00002149
Iteration 249/1000 | Loss: 0.00002148
Iteration 250/1000 | Loss: 0.00002147
Iteration 251/1000 | Loss: 0.00002146
Iteration 252/1000 | Loss: 0.00002146
Iteration 253/1000 | Loss: 0.00002146
Iteration 254/1000 | Loss: 0.00002145
Iteration 255/1000 | Loss: 0.00002145
Iteration 256/1000 | Loss: 0.00002145
Iteration 257/1000 | Loss: 0.00002145
Iteration 258/1000 | Loss: 0.00002145
Iteration 259/1000 | Loss: 0.00002145
Iteration 260/1000 | Loss: 0.00002145
Iteration 261/1000 | Loss: 0.00002145
Iteration 262/1000 | Loss: 0.00002145
Iteration 263/1000 | Loss: 0.00002145
Iteration 264/1000 | Loss: 0.00002145
Iteration 265/1000 | Loss: 0.00002145
Iteration 266/1000 | Loss: 0.00002144
Iteration 267/1000 | Loss: 0.00002144
Iteration 268/1000 | Loss: 0.00002144
Iteration 269/1000 | Loss: 0.00002144
Iteration 270/1000 | Loss: 0.00002143
Iteration 271/1000 | Loss: 0.00002143
Iteration 272/1000 | Loss: 0.00002143
Iteration 273/1000 | Loss: 0.00002143
Iteration 274/1000 | Loss: 0.00002143
Iteration 275/1000 | Loss: 0.00002143
Iteration 276/1000 | Loss: 0.00002143
Iteration 277/1000 | Loss: 0.00002143
Iteration 278/1000 | Loss: 0.00002143
Iteration 279/1000 | Loss: 0.00002143
Iteration 280/1000 | Loss: 0.00002142
Iteration 281/1000 | Loss: 0.00002142
Iteration 282/1000 | Loss: 0.00002142
Iteration 283/1000 | Loss: 0.00002142
Iteration 284/1000 | Loss: 0.00002141
Iteration 285/1000 | Loss: 0.00002141
Iteration 286/1000 | Loss: 0.00002141
Iteration 287/1000 | Loss: 0.00002141
Iteration 288/1000 | Loss: 0.00002140
Iteration 289/1000 | Loss: 0.00002140
Iteration 290/1000 | Loss: 0.00002140
Iteration 291/1000 | Loss: 0.00002140
Iteration 292/1000 | Loss: 0.00002140
Iteration 293/1000 | Loss: 0.00002140
Iteration 294/1000 | Loss: 0.00002139
Iteration 295/1000 | Loss: 0.00002139
Iteration 296/1000 | Loss: 0.00002139
Iteration 297/1000 | Loss: 0.00002139
Iteration 298/1000 | Loss: 0.00002139
Iteration 299/1000 | Loss: 0.00002138
Iteration 300/1000 | Loss: 0.00002138
Iteration 301/1000 | Loss: 0.00002137
Iteration 302/1000 | Loss: 0.00002137
Iteration 303/1000 | Loss: 0.00002137
Iteration 304/1000 | Loss: 0.00002137
Iteration 305/1000 | Loss: 0.00002137
Iteration 306/1000 | Loss: 0.00002137
Iteration 307/1000 | Loss: 0.00002137
Iteration 308/1000 | Loss: 0.00002136
Iteration 309/1000 | Loss: 0.00002136
Iteration 310/1000 | Loss: 0.00002136
Iteration 311/1000 | Loss: 0.00002136
Iteration 312/1000 | Loss: 0.00002136
Iteration 313/1000 | Loss: 0.00002136
Iteration 314/1000 | Loss: 0.00002136
Iteration 315/1000 | Loss: 0.00002136
Iteration 316/1000 | Loss: 0.00002136
Iteration 317/1000 | Loss: 0.00002136
Iteration 318/1000 | Loss: 0.00002135
Iteration 319/1000 | Loss: 0.00002135
Iteration 320/1000 | Loss: 0.00002135
Iteration 321/1000 | Loss: 0.00002135
Iteration 322/1000 | Loss: 0.00002134
Iteration 323/1000 | Loss: 0.00002133
Iteration 324/1000 | Loss: 0.00005872
Iteration 325/1000 | Loss: 0.00016036
Iteration 326/1000 | Loss: 0.00009447
Iteration 327/1000 | Loss: 0.00002138
Iteration 328/1000 | Loss: 0.00002129
Iteration 329/1000 | Loss: 0.00002128
Iteration 330/1000 | Loss: 0.00002128
Iteration 331/1000 | Loss: 0.00002128
Iteration 332/1000 | Loss: 0.00002128
Iteration 333/1000 | Loss: 0.00002128
Iteration 334/1000 | Loss: 0.00002128
Iteration 335/1000 | Loss: 0.00002128
Iteration 336/1000 | Loss: 0.00002128
Iteration 337/1000 | Loss: 0.00002128
Iteration 338/1000 | Loss: 0.00002128
Iteration 339/1000 | Loss: 0.00002128
Iteration 340/1000 | Loss: 0.00002128
Iteration 341/1000 | Loss: 0.00002128
Iteration 342/1000 | Loss: 0.00002127
Iteration 343/1000 | Loss: 0.00002127
Iteration 344/1000 | Loss: 0.00002127
Iteration 345/1000 | Loss: 0.00002127
Iteration 346/1000 | Loss: 0.00002127
Iteration 347/1000 | Loss: 0.00002127
Iteration 348/1000 | Loss: 0.00002127
Iteration 349/1000 | Loss: 0.00002127
Iteration 350/1000 | Loss: 0.00002127
Iteration 351/1000 | Loss: 0.00002127
Iteration 352/1000 | Loss: 0.00002126
Iteration 353/1000 | Loss: 0.00002125
Iteration 354/1000 | Loss: 0.00002124
Iteration 355/1000 | Loss: 0.00002123
Iteration 356/1000 | Loss: 0.00002122
Iteration 357/1000 | Loss: 0.00002122
Iteration 358/1000 | Loss: 0.00002122
Iteration 359/1000 | Loss: 0.00002122
Iteration 360/1000 | Loss: 0.00002122
Iteration 361/1000 | Loss: 0.00002121
Iteration 362/1000 | Loss: 0.00002121
Iteration 363/1000 | Loss: 0.00002121
Iteration 364/1000 | Loss: 0.00002121
Iteration 365/1000 | Loss: 0.00002121
Iteration 366/1000 | Loss: 0.00002121
Iteration 367/1000 | Loss: 0.00002121
Iteration 368/1000 | Loss: 0.00002121
Iteration 369/1000 | Loss: 0.00002121
Iteration 370/1000 | Loss: 0.00002121
Iteration 371/1000 | Loss: 0.00002120
Iteration 372/1000 | Loss: 0.00002120
Iteration 373/1000 | Loss: 0.00002120
Iteration 374/1000 | Loss: 0.00002120
Iteration 375/1000 | Loss: 0.00002120
Iteration 376/1000 | Loss: 0.00002120
Iteration 377/1000 | Loss: 0.00002119
Iteration 378/1000 | Loss: 0.00002119
Iteration 379/1000 | Loss: 0.00002119
Iteration 380/1000 | Loss: 0.00002119
Iteration 381/1000 | Loss: 0.00002119
Iteration 382/1000 | Loss: 0.00002119
Iteration 383/1000 | Loss: 0.00002119
Iteration 384/1000 | Loss: 0.00002119
Iteration 385/1000 | Loss: 0.00002119
Iteration 386/1000 | Loss: 0.00002118
Iteration 387/1000 | Loss: 0.00002118
Iteration 388/1000 | Loss: 0.00002118
Iteration 389/1000 | Loss: 0.00002118
Iteration 390/1000 | Loss: 0.00002118
Iteration 391/1000 | Loss: 0.00002118
Iteration 392/1000 | Loss: 0.00002118
Iteration 393/1000 | Loss: 0.00002118
Iteration 394/1000 | Loss: 0.00002118
Iteration 395/1000 | Loss: 0.00002118
Iteration 396/1000 | Loss: 0.00002118
Iteration 397/1000 | Loss: 0.00002118
Iteration 398/1000 | Loss: 0.00002118
Iteration 399/1000 | Loss: 0.00002118
Iteration 400/1000 | Loss: 0.00002118
Iteration 401/1000 | Loss: 0.00002118
Iteration 402/1000 | Loss: 0.00002118
Iteration 403/1000 | Loss: 0.00002118
Iteration 404/1000 | Loss: 0.00002118
Iteration 405/1000 | Loss: 0.00002118
Iteration 406/1000 | Loss: 0.00002118
Iteration 407/1000 | Loss: 0.00002118
Iteration 408/1000 | Loss: 0.00002118
Iteration 409/1000 | Loss: 0.00002118
Iteration 410/1000 | Loss: 0.00002118
Iteration 411/1000 | Loss: 0.00002118
Iteration 412/1000 | Loss: 0.00002118
Iteration 413/1000 | Loss: 0.00002118
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 413. Stopping optimization.
Last 5 losses: [2.1182941054576077e-05, 2.1182941054576077e-05, 2.1182941054576077e-05, 2.1182941054576077e-05, 2.1182941054576077e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1182941054576077e-05

Optimization complete. Final v2v error: 3.6708319187164307 mm

Highest mean error: 7.226933479309082 mm for frame 63

Lowest mean error: 3.21710467338562 mm for frame 133

Saving results

Total time: 412.559939622879
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00460274
Iteration 2/25 | Loss: 0.00141036
Iteration 3/25 | Loss: 0.00132677
Iteration 4/25 | Loss: 0.00131532
Iteration 5/25 | Loss: 0.00131226
Iteration 6/25 | Loss: 0.00131226
Iteration 7/25 | Loss: 0.00131226
Iteration 8/25 | Loss: 0.00131226
Iteration 9/25 | Loss: 0.00131226
Iteration 10/25 | Loss: 0.00131226
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013122641248628497, 0.0013122641248628497, 0.0013122641248628497, 0.0013122641248628497, 0.0013122641248628497]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013122641248628497

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.70684004
Iteration 2/25 | Loss: 0.00092180
Iteration 3/25 | Loss: 0.00092180
Iteration 4/25 | Loss: 0.00092180
Iteration 5/25 | Loss: 0.00092179
Iteration 6/25 | Loss: 0.00092179
Iteration 7/25 | Loss: 0.00092179
Iteration 8/25 | Loss: 0.00092179
Iteration 9/25 | Loss: 0.00092179
Iteration 10/25 | Loss: 0.00092179
Iteration 11/25 | Loss: 0.00092179
Iteration 12/25 | Loss: 0.00092179
Iteration 13/25 | Loss: 0.00092179
Iteration 14/25 | Loss: 0.00092179
Iteration 15/25 | Loss: 0.00092179
Iteration 16/25 | Loss: 0.00092179
Iteration 17/25 | Loss: 0.00092179
Iteration 18/25 | Loss: 0.00092179
Iteration 19/25 | Loss: 0.00092179
Iteration 20/25 | Loss: 0.00092179
Iteration 21/25 | Loss: 0.00092179
Iteration 22/25 | Loss: 0.00092179
Iteration 23/25 | Loss: 0.00092179
Iteration 24/25 | Loss: 0.00092179
Iteration 25/25 | Loss: 0.00092179

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092179
Iteration 2/1000 | Loss: 0.00002601
Iteration 3/1000 | Loss: 0.00002024
Iteration 4/1000 | Loss: 0.00001905
Iteration 5/1000 | Loss: 0.00001847
Iteration 6/1000 | Loss: 0.00001770
Iteration 7/1000 | Loss: 0.00001717
Iteration 8/1000 | Loss: 0.00001686
Iteration 9/1000 | Loss: 0.00001649
Iteration 10/1000 | Loss: 0.00001625
Iteration 11/1000 | Loss: 0.00001605
Iteration 12/1000 | Loss: 0.00001588
Iteration 13/1000 | Loss: 0.00001575
Iteration 14/1000 | Loss: 0.00001571
Iteration 15/1000 | Loss: 0.00001571
Iteration 16/1000 | Loss: 0.00001570
Iteration 17/1000 | Loss: 0.00001570
Iteration 18/1000 | Loss: 0.00001569
Iteration 19/1000 | Loss: 0.00001569
Iteration 20/1000 | Loss: 0.00001569
Iteration 21/1000 | Loss: 0.00001568
Iteration 22/1000 | Loss: 0.00001568
Iteration 23/1000 | Loss: 0.00001567
Iteration 24/1000 | Loss: 0.00001565
Iteration 25/1000 | Loss: 0.00001561
Iteration 26/1000 | Loss: 0.00001554
Iteration 27/1000 | Loss: 0.00001551
Iteration 28/1000 | Loss: 0.00001551
Iteration 29/1000 | Loss: 0.00001546
Iteration 30/1000 | Loss: 0.00001544
Iteration 31/1000 | Loss: 0.00001543
Iteration 32/1000 | Loss: 0.00001543
Iteration 33/1000 | Loss: 0.00001543
Iteration 34/1000 | Loss: 0.00001542
Iteration 35/1000 | Loss: 0.00001542
Iteration 36/1000 | Loss: 0.00001541
Iteration 37/1000 | Loss: 0.00001541
Iteration 38/1000 | Loss: 0.00001540
Iteration 39/1000 | Loss: 0.00001540
Iteration 40/1000 | Loss: 0.00001539
Iteration 41/1000 | Loss: 0.00001539
Iteration 42/1000 | Loss: 0.00001539
Iteration 43/1000 | Loss: 0.00001539
Iteration 44/1000 | Loss: 0.00001537
Iteration 45/1000 | Loss: 0.00001537
Iteration 46/1000 | Loss: 0.00001536
Iteration 47/1000 | Loss: 0.00001536
Iteration 48/1000 | Loss: 0.00001536
Iteration 49/1000 | Loss: 0.00001536
Iteration 50/1000 | Loss: 0.00001536
Iteration 51/1000 | Loss: 0.00001535
Iteration 52/1000 | Loss: 0.00001535
Iteration 53/1000 | Loss: 0.00001534
Iteration 54/1000 | Loss: 0.00001534
Iteration 55/1000 | Loss: 0.00001532
Iteration 56/1000 | Loss: 0.00001532
Iteration 57/1000 | Loss: 0.00001532
Iteration 58/1000 | Loss: 0.00001532
Iteration 59/1000 | Loss: 0.00001531
Iteration 60/1000 | Loss: 0.00001531
Iteration 61/1000 | Loss: 0.00001531
Iteration 62/1000 | Loss: 0.00001531
Iteration 63/1000 | Loss: 0.00001531
Iteration 64/1000 | Loss: 0.00001531
Iteration 65/1000 | Loss: 0.00001530
Iteration 66/1000 | Loss: 0.00001530
Iteration 67/1000 | Loss: 0.00001528
Iteration 68/1000 | Loss: 0.00001528
Iteration 69/1000 | Loss: 0.00001528
Iteration 70/1000 | Loss: 0.00001528
Iteration 71/1000 | Loss: 0.00001527
Iteration 72/1000 | Loss: 0.00001527
Iteration 73/1000 | Loss: 0.00001526
Iteration 74/1000 | Loss: 0.00001525
Iteration 75/1000 | Loss: 0.00001525
Iteration 76/1000 | Loss: 0.00001524
Iteration 77/1000 | Loss: 0.00001524
Iteration 78/1000 | Loss: 0.00001523
Iteration 79/1000 | Loss: 0.00001523
Iteration 80/1000 | Loss: 0.00001523
Iteration 81/1000 | Loss: 0.00001522
Iteration 82/1000 | Loss: 0.00001522
Iteration 83/1000 | Loss: 0.00001522
Iteration 84/1000 | Loss: 0.00001522
Iteration 85/1000 | Loss: 0.00001522
Iteration 86/1000 | Loss: 0.00001522
Iteration 87/1000 | Loss: 0.00001521
Iteration 88/1000 | Loss: 0.00001521
Iteration 89/1000 | Loss: 0.00001521
Iteration 90/1000 | Loss: 0.00001521
Iteration 91/1000 | Loss: 0.00001521
Iteration 92/1000 | Loss: 0.00001521
Iteration 93/1000 | Loss: 0.00001521
Iteration 94/1000 | Loss: 0.00001521
Iteration 95/1000 | Loss: 0.00001520
Iteration 96/1000 | Loss: 0.00001520
Iteration 97/1000 | Loss: 0.00001520
Iteration 98/1000 | Loss: 0.00001520
Iteration 99/1000 | Loss: 0.00001520
Iteration 100/1000 | Loss: 0.00001520
Iteration 101/1000 | Loss: 0.00001520
Iteration 102/1000 | Loss: 0.00001519
Iteration 103/1000 | Loss: 0.00001519
Iteration 104/1000 | Loss: 0.00001518
Iteration 105/1000 | Loss: 0.00001518
Iteration 106/1000 | Loss: 0.00001518
Iteration 107/1000 | Loss: 0.00001518
Iteration 108/1000 | Loss: 0.00001518
Iteration 109/1000 | Loss: 0.00001518
Iteration 110/1000 | Loss: 0.00001518
Iteration 111/1000 | Loss: 0.00001518
Iteration 112/1000 | Loss: 0.00001518
Iteration 113/1000 | Loss: 0.00001518
Iteration 114/1000 | Loss: 0.00001518
Iteration 115/1000 | Loss: 0.00001517
Iteration 116/1000 | Loss: 0.00001517
Iteration 117/1000 | Loss: 0.00001517
Iteration 118/1000 | Loss: 0.00001517
Iteration 119/1000 | Loss: 0.00001517
Iteration 120/1000 | Loss: 0.00001517
Iteration 121/1000 | Loss: 0.00001517
Iteration 122/1000 | Loss: 0.00001516
Iteration 123/1000 | Loss: 0.00001516
Iteration 124/1000 | Loss: 0.00001516
Iteration 125/1000 | Loss: 0.00001516
Iteration 126/1000 | Loss: 0.00001516
Iteration 127/1000 | Loss: 0.00001516
Iteration 128/1000 | Loss: 0.00001516
Iteration 129/1000 | Loss: 0.00001516
Iteration 130/1000 | Loss: 0.00001515
Iteration 131/1000 | Loss: 0.00001515
Iteration 132/1000 | Loss: 0.00001515
Iteration 133/1000 | Loss: 0.00001515
Iteration 134/1000 | Loss: 0.00001515
Iteration 135/1000 | Loss: 0.00001515
Iteration 136/1000 | Loss: 0.00001515
Iteration 137/1000 | Loss: 0.00001515
Iteration 138/1000 | Loss: 0.00001515
Iteration 139/1000 | Loss: 0.00001515
Iteration 140/1000 | Loss: 0.00001515
Iteration 141/1000 | Loss: 0.00001515
Iteration 142/1000 | Loss: 0.00001515
Iteration 143/1000 | Loss: 0.00001514
Iteration 144/1000 | Loss: 0.00001514
Iteration 145/1000 | Loss: 0.00001514
Iteration 146/1000 | Loss: 0.00001514
Iteration 147/1000 | Loss: 0.00001513
Iteration 148/1000 | Loss: 0.00001513
Iteration 149/1000 | Loss: 0.00001513
Iteration 150/1000 | Loss: 0.00001513
Iteration 151/1000 | Loss: 0.00001513
Iteration 152/1000 | Loss: 0.00001513
Iteration 153/1000 | Loss: 0.00001513
Iteration 154/1000 | Loss: 0.00001513
Iteration 155/1000 | Loss: 0.00001513
Iteration 156/1000 | Loss: 0.00001513
Iteration 157/1000 | Loss: 0.00001513
Iteration 158/1000 | Loss: 0.00001512
Iteration 159/1000 | Loss: 0.00001512
Iteration 160/1000 | Loss: 0.00001512
Iteration 161/1000 | Loss: 0.00001512
Iteration 162/1000 | Loss: 0.00001512
Iteration 163/1000 | Loss: 0.00001512
Iteration 164/1000 | Loss: 0.00001512
Iteration 165/1000 | Loss: 0.00001512
Iteration 166/1000 | Loss: 0.00001511
Iteration 167/1000 | Loss: 0.00001511
Iteration 168/1000 | Loss: 0.00001511
Iteration 169/1000 | Loss: 0.00001511
Iteration 170/1000 | Loss: 0.00001511
Iteration 171/1000 | Loss: 0.00001511
Iteration 172/1000 | Loss: 0.00001511
Iteration 173/1000 | Loss: 0.00001511
Iteration 174/1000 | Loss: 0.00001511
Iteration 175/1000 | Loss: 0.00001511
Iteration 176/1000 | Loss: 0.00001511
Iteration 177/1000 | Loss: 0.00001511
Iteration 178/1000 | Loss: 0.00001511
Iteration 179/1000 | Loss: 0.00001511
Iteration 180/1000 | Loss: 0.00001511
Iteration 181/1000 | Loss: 0.00001511
Iteration 182/1000 | Loss: 0.00001511
Iteration 183/1000 | Loss: 0.00001511
Iteration 184/1000 | Loss: 0.00001511
Iteration 185/1000 | Loss: 0.00001511
Iteration 186/1000 | Loss: 0.00001511
Iteration 187/1000 | Loss: 0.00001511
Iteration 188/1000 | Loss: 0.00001511
Iteration 189/1000 | Loss: 0.00001511
Iteration 190/1000 | Loss: 0.00001511
Iteration 191/1000 | Loss: 0.00001511
Iteration 192/1000 | Loss: 0.00001511
Iteration 193/1000 | Loss: 0.00001511
Iteration 194/1000 | Loss: 0.00001511
Iteration 195/1000 | Loss: 0.00001511
Iteration 196/1000 | Loss: 0.00001511
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [1.5113270819711033e-05, 1.5113270819711033e-05, 1.5113270819711033e-05, 1.5113270819711033e-05, 1.5113270819711033e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5113270819711033e-05

Optimization complete. Final v2v error: 3.293625831604004 mm

Highest mean error: 3.6353495121002197 mm for frame 86

Lowest mean error: 3.0081117153167725 mm for frame 3

Saving results

Total time: 43.6651725769043
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00418097
Iteration 2/25 | Loss: 0.00138311
Iteration 3/25 | Loss: 0.00130347
Iteration 4/25 | Loss: 0.00128669
Iteration 5/25 | Loss: 0.00128147
Iteration 6/25 | Loss: 0.00128105
Iteration 7/25 | Loss: 0.00128105
Iteration 8/25 | Loss: 0.00128105
Iteration 9/25 | Loss: 0.00128105
Iteration 10/25 | Loss: 0.00128105
Iteration 11/25 | Loss: 0.00128105
Iteration 12/25 | Loss: 0.00128105
Iteration 13/25 | Loss: 0.00128105
Iteration 14/25 | Loss: 0.00128105
Iteration 15/25 | Loss: 0.00128105
Iteration 16/25 | Loss: 0.00128105
Iteration 17/25 | Loss: 0.00128105
Iteration 18/25 | Loss: 0.00128105
Iteration 19/25 | Loss: 0.00128105
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012810459593310952, 0.0012810459593310952, 0.0012810459593310952, 0.0012810459593310952, 0.0012810459593310952]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012810459593310952

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40433025
Iteration 2/25 | Loss: 0.00082233
Iteration 3/25 | Loss: 0.00082233
Iteration 4/25 | Loss: 0.00082233
Iteration 5/25 | Loss: 0.00082233
Iteration 6/25 | Loss: 0.00082233
Iteration 7/25 | Loss: 0.00082232
Iteration 8/25 | Loss: 0.00082232
Iteration 9/25 | Loss: 0.00082232
Iteration 10/25 | Loss: 0.00082232
Iteration 11/25 | Loss: 0.00082232
Iteration 12/25 | Loss: 0.00082232
Iteration 13/25 | Loss: 0.00082232
Iteration 14/25 | Loss: 0.00082232
Iteration 15/25 | Loss: 0.00082232
Iteration 16/25 | Loss: 0.00082232
Iteration 17/25 | Loss: 0.00082232
Iteration 18/25 | Loss: 0.00082232
Iteration 19/25 | Loss: 0.00082232
Iteration 20/25 | Loss: 0.00082232
Iteration 21/25 | Loss: 0.00082232
Iteration 22/25 | Loss: 0.00082232
Iteration 23/25 | Loss: 0.00082232
Iteration 24/25 | Loss: 0.00082232
Iteration 25/25 | Loss: 0.00082232

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082232
Iteration 2/1000 | Loss: 0.00002715
Iteration 3/1000 | Loss: 0.00002015
Iteration 4/1000 | Loss: 0.00001863
Iteration 5/1000 | Loss: 0.00001783
Iteration 6/1000 | Loss: 0.00001725
Iteration 7/1000 | Loss: 0.00001683
Iteration 8/1000 | Loss: 0.00001657
Iteration 9/1000 | Loss: 0.00001623
Iteration 10/1000 | Loss: 0.00001601
Iteration 11/1000 | Loss: 0.00001584
Iteration 12/1000 | Loss: 0.00001581
Iteration 13/1000 | Loss: 0.00001576
Iteration 14/1000 | Loss: 0.00001575
Iteration 15/1000 | Loss: 0.00001575
Iteration 16/1000 | Loss: 0.00001562
Iteration 17/1000 | Loss: 0.00001562
Iteration 18/1000 | Loss: 0.00001554
Iteration 19/1000 | Loss: 0.00001554
Iteration 20/1000 | Loss: 0.00001547
Iteration 21/1000 | Loss: 0.00001547
Iteration 22/1000 | Loss: 0.00001547
Iteration 23/1000 | Loss: 0.00001547
Iteration 24/1000 | Loss: 0.00001547
Iteration 25/1000 | Loss: 0.00001547
Iteration 26/1000 | Loss: 0.00001547
Iteration 27/1000 | Loss: 0.00001547
Iteration 28/1000 | Loss: 0.00001547
Iteration 29/1000 | Loss: 0.00001547
Iteration 30/1000 | Loss: 0.00001546
Iteration 31/1000 | Loss: 0.00001546
Iteration 32/1000 | Loss: 0.00001546
Iteration 33/1000 | Loss: 0.00001542
Iteration 34/1000 | Loss: 0.00001542
Iteration 35/1000 | Loss: 0.00001540
Iteration 36/1000 | Loss: 0.00001539
Iteration 37/1000 | Loss: 0.00001538
Iteration 38/1000 | Loss: 0.00001537
Iteration 39/1000 | Loss: 0.00001535
Iteration 40/1000 | Loss: 0.00001535
Iteration 41/1000 | Loss: 0.00001531
Iteration 42/1000 | Loss: 0.00001530
Iteration 43/1000 | Loss: 0.00001529
Iteration 44/1000 | Loss: 0.00001529
Iteration 45/1000 | Loss: 0.00001528
Iteration 46/1000 | Loss: 0.00001528
Iteration 47/1000 | Loss: 0.00001528
Iteration 48/1000 | Loss: 0.00001527
Iteration 49/1000 | Loss: 0.00001527
Iteration 50/1000 | Loss: 0.00001526
Iteration 51/1000 | Loss: 0.00001524
Iteration 52/1000 | Loss: 0.00001524
Iteration 53/1000 | Loss: 0.00001524
Iteration 54/1000 | Loss: 0.00001524
Iteration 55/1000 | Loss: 0.00001523
Iteration 56/1000 | Loss: 0.00001523
Iteration 57/1000 | Loss: 0.00001522
Iteration 58/1000 | Loss: 0.00001522
Iteration 59/1000 | Loss: 0.00001522
Iteration 60/1000 | Loss: 0.00001522
Iteration 61/1000 | Loss: 0.00001522
Iteration 62/1000 | Loss: 0.00001522
Iteration 63/1000 | Loss: 0.00001522
Iteration 64/1000 | Loss: 0.00001522
Iteration 65/1000 | Loss: 0.00001521
Iteration 66/1000 | Loss: 0.00001521
Iteration 67/1000 | Loss: 0.00001521
Iteration 68/1000 | Loss: 0.00001521
Iteration 69/1000 | Loss: 0.00001520
Iteration 70/1000 | Loss: 0.00001520
Iteration 71/1000 | Loss: 0.00001519
Iteration 72/1000 | Loss: 0.00001518
Iteration 73/1000 | Loss: 0.00001517
Iteration 74/1000 | Loss: 0.00001517
Iteration 75/1000 | Loss: 0.00001517
Iteration 76/1000 | Loss: 0.00001517
Iteration 77/1000 | Loss: 0.00001517
Iteration 78/1000 | Loss: 0.00001517
Iteration 79/1000 | Loss: 0.00001516
Iteration 80/1000 | Loss: 0.00001516
Iteration 81/1000 | Loss: 0.00001514
Iteration 82/1000 | Loss: 0.00001514
Iteration 83/1000 | Loss: 0.00001514
Iteration 84/1000 | Loss: 0.00001514
Iteration 85/1000 | Loss: 0.00001513
Iteration 86/1000 | Loss: 0.00001513
Iteration 87/1000 | Loss: 0.00001513
Iteration 88/1000 | Loss: 0.00001513
Iteration 89/1000 | Loss: 0.00001513
Iteration 90/1000 | Loss: 0.00001513
Iteration 91/1000 | Loss: 0.00001513
Iteration 92/1000 | Loss: 0.00001513
Iteration 93/1000 | Loss: 0.00001512
Iteration 94/1000 | Loss: 0.00001511
Iteration 95/1000 | Loss: 0.00001510
Iteration 96/1000 | Loss: 0.00001510
Iteration 97/1000 | Loss: 0.00001510
Iteration 98/1000 | Loss: 0.00001510
Iteration 99/1000 | Loss: 0.00001509
Iteration 100/1000 | Loss: 0.00001509
Iteration 101/1000 | Loss: 0.00001509
Iteration 102/1000 | Loss: 0.00001509
Iteration 103/1000 | Loss: 0.00001509
Iteration 104/1000 | Loss: 0.00001509
Iteration 105/1000 | Loss: 0.00001509
Iteration 106/1000 | Loss: 0.00001509
Iteration 107/1000 | Loss: 0.00001509
Iteration 108/1000 | Loss: 0.00001509
Iteration 109/1000 | Loss: 0.00001509
Iteration 110/1000 | Loss: 0.00001509
Iteration 111/1000 | Loss: 0.00001509
Iteration 112/1000 | Loss: 0.00001509
Iteration 113/1000 | Loss: 0.00001509
Iteration 114/1000 | Loss: 0.00001509
Iteration 115/1000 | Loss: 0.00001509
Iteration 116/1000 | Loss: 0.00001509
Iteration 117/1000 | Loss: 0.00001509
Iteration 118/1000 | Loss: 0.00001508
Iteration 119/1000 | Loss: 0.00001508
Iteration 120/1000 | Loss: 0.00001508
Iteration 121/1000 | Loss: 0.00001508
Iteration 122/1000 | Loss: 0.00001508
Iteration 123/1000 | Loss: 0.00001508
Iteration 124/1000 | Loss: 0.00001508
Iteration 125/1000 | Loss: 0.00001508
Iteration 126/1000 | Loss: 0.00001508
Iteration 127/1000 | Loss: 0.00001508
Iteration 128/1000 | Loss: 0.00001508
Iteration 129/1000 | Loss: 0.00001508
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [1.5080486264196225e-05, 1.5080486264196225e-05, 1.5080486264196225e-05, 1.5080486264196225e-05, 1.5080486264196225e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5080486264196225e-05

Optimization complete. Final v2v error: 3.3442959785461426 mm

Highest mean error: 3.486985921859741 mm for frame 120

Lowest mean error: 3.269082546234131 mm for frame 109

Saving results

Total time: 38.14583230018616
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_021/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_021/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00872540
Iteration 2/25 | Loss: 0.00175206
Iteration 3/25 | Loss: 0.00153738
Iteration 4/25 | Loss: 0.00135705
Iteration 5/25 | Loss: 0.00134978
Iteration 6/25 | Loss: 0.00131530
Iteration 7/25 | Loss: 0.00130796
Iteration 8/25 | Loss: 0.00130631
Iteration 9/25 | Loss: 0.00130527
Iteration 10/25 | Loss: 0.00130480
Iteration 11/25 | Loss: 0.00130468
Iteration 12/25 | Loss: 0.00130465
Iteration 13/25 | Loss: 0.00130465
Iteration 14/25 | Loss: 0.00130465
Iteration 15/25 | Loss: 0.00130465
Iteration 16/25 | Loss: 0.00130465
Iteration 17/25 | Loss: 0.00130465
Iteration 18/25 | Loss: 0.00130465
Iteration 19/25 | Loss: 0.00130465
Iteration 20/25 | Loss: 0.00130465
Iteration 21/25 | Loss: 0.00130465
Iteration 22/25 | Loss: 0.00130465
Iteration 23/25 | Loss: 0.00130465
Iteration 24/25 | Loss: 0.00130465
Iteration 25/25 | Loss: 0.00130464

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.49763370
Iteration 2/25 | Loss: 0.00093209
Iteration 3/25 | Loss: 0.00093208
Iteration 4/25 | Loss: 0.00093208
Iteration 5/25 | Loss: 0.00093208
Iteration 6/25 | Loss: 0.00093208
Iteration 7/25 | Loss: 0.00093208
Iteration 8/25 | Loss: 0.00093208
Iteration 9/25 | Loss: 0.00093208
Iteration 10/25 | Loss: 0.00093208
Iteration 11/25 | Loss: 0.00093208
Iteration 12/25 | Loss: 0.00093208
Iteration 13/25 | Loss: 0.00093208
Iteration 14/25 | Loss: 0.00093208
Iteration 15/25 | Loss: 0.00093208
Iteration 16/25 | Loss: 0.00093208
Iteration 17/25 | Loss: 0.00093208
Iteration 18/25 | Loss: 0.00093208
Iteration 19/25 | Loss: 0.00093208
Iteration 20/25 | Loss: 0.00093208
Iteration 21/25 | Loss: 0.00093208
Iteration 22/25 | Loss: 0.00093208
Iteration 23/25 | Loss: 0.00093208
Iteration 24/25 | Loss: 0.00093208
Iteration 25/25 | Loss: 0.00093208

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093208
Iteration 2/1000 | Loss: 0.00020699
Iteration 3/1000 | Loss: 0.00002630
Iteration 4/1000 | Loss: 0.00002213
Iteration 5/1000 | Loss: 0.00002030
Iteration 6/1000 | Loss: 0.00001920
Iteration 7/1000 | Loss: 0.00001857
Iteration 8/1000 | Loss: 0.00001815
Iteration 9/1000 | Loss: 0.00001790
Iteration 10/1000 | Loss: 0.00001757
Iteration 11/1000 | Loss: 0.00026225
Iteration 12/1000 | Loss: 0.00001771
Iteration 13/1000 | Loss: 0.00001717
Iteration 14/1000 | Loss: 0.00001711
Iteration 15/1000 | Loss: 0.00001711
Iteration 16/1000 | Loss: 0.00001708
Iteration 17/1000 | Loss: 0.00001708
Iteration 18/1000 | Loss: 0.00001706
Iteration 19/1000 | Loss: 0.00001704
Iteration 20/1000 | Loss: 0.00001704
Iteration 21/1000 | Loss: 0.00001703
Iteration 22/1000 | Loss: 0.00001703
Iteration 23/1000 | Loss: 0.00001702
Iteration 24/1000 | Loss: 0.00001689
Iteration 25/1000 | Loss: 0.00001685
Iteration 26/1000 | Loss: 0.00001685
Iteration 27/1000 | Loss: 0.00001683
Iteration 28/1000 | Loss: 0.00001683
Iteration 29/1000 | Loss: 0.00001681
Iteration 30/1000 | Loss: 0.00001680
Iteration 31/1000 | Loss: 0.00001679
Iteration 32/1000 | Loss: 0.00025399
Iteration 33/1000 | Loss: 0.00004498
Iteration 34/1000 | Loss: 0.00003188
Iteration 35/1000 | Loss: 0.00001696
Iteration 36/1000 | Loss: 0.00001679
Iteration 37/1000 | Loss: 0.00001679
Iteration 38/1000 | Loss: 0.00001677
Iteration 39/1000 | Loss: 0.00001672
Iteration 40/1000 | Loss: 0.00001665
Iteration 41/1000 | Loss: 0.00001661
Iteration 42/1000 | Loss: 0.00001657
Iteration 43/1000 | Loss: 0.00001656
Iteration 44/1000 | Loss: 0.00001654
Iteration 45/1000 | Loss: 0.00001654
Iteration 46/1000 | Loss: 0.00001654
Iteration 47/1000 | Loss: 0.00001653
Iteration 48/1000 | Loss: 0.00001653
Iteration 49/1000 | Loss: 0.00001652
Iteration 50/1000 | Loss: 0.00001652
Iteration 51/1000 | Loss: 0.00001652
Iteration 52/1000 | Loss: 0.00001651
Iteration 53/1000 | Loss: 0.00001651
Iteration 54/1000 | Loss: 0.00001651
Iteration 55/1000 | Loss: 0.00001651
Iteration 56/1000 | Loss: 0.00001650
Iteration 57/1000 | Loss: 0.00001650
Iteration 58/1000 | Loss: 0.00001650
Iteration 59/1000 | Loss: 0.00001650
Iteration 60/1000 | Loss: 0.00001650
Iteration 61/1000 | Loss: 0.00001650
Iteration 62/1000 | Loss: 0.00001650
Iteration 63/1000 | Loss: 0.00001649
Iteration 64/1000 | Loss: 0.00001649
Iteration 65/1000 | Loss: 0.00001649
Iteration 66/1000 | Loss: 0.00001648
Iteration 67/1000 | Loss: 0.00001648
Iteration 68/1000 | Loss: 0.00001648
Iteration 69/1000 | Loss: 0.00001647
Iteration 70/1000 | Loss: 0.00001647
Iteration 71/1000 | Loss: 0.00001647
Iteration 72/1000 | Loss: 0.00001647
Iteration 73/1000 | Loss: 0.00001647
Iteration 74/1000 | Loss: 0.00001647
Iteration 75/1000 | Loss: 0.00001646
Iteration 76/1000 | Loss: 0.00001646
Iteration 77/1000 | Loss: 0.00001646
Iteration 78/1000 | Loss: 0.00001646
Iteration 79/1000 | Loss: 0.00001646
Iteration 80/1000 | Loss: 0.00001645
Iteration 81/1000 | Loss: 0.00001645
Iteration 82/1000 | Loss: 0.00001645
Iteration 83/1000 | Loss: 0.00001645
Iteration 84/1000 | Loss: 0.00001645
Iteration 85/1000 | Loss: 0.00001645
Iteration 86/1000 | Loss: 0.00001645
Iteration 87/1000 | Loss: 0.00001645
Iteration 88/1000 | Loss: 0.00001645
Iteration 89/1000 | Loss: 0.00001645
Iteration 90/1000 | Loss: 0.00001645
Iteration 91/1000 | Loss: 0.00001645
Iteration 92/1000 | Loss: 0.00001645
Iteration 93/1000 | Loss: 0.00001645
Iteration 94/1000 | Loss: 0.00001645
Iteration 95/1000 | Loss: 0.00001645
Iteration 96/1000 | Loss: 0.00001645
Iteration 97/1000 | Loss: 0.00001645
Iteration 98/1000 | Loss: 0.00001644
Iteration 99/1000 | Loss: 0.00001644
Iteration 100/1000 | Loss: 0.00001644
Iteration 101/1000 | Loss: 0.00001644
Iteration 102/1000 | Loss: 0.00001644
Iteration 103/1000 | Loss: 0.00001644
Iteration 104/1000 | Loss: 0.00001644
Iteration 105/1000 | Loss: 0.00001644
Iteration 106/1000 | Loss: 0.00001644
Iteration 107/1000 | Loss: 0.00001644
Iteration 108/1000 | Loss: 0.00001643
Iteration 109/1000 | Loss: 0.00001643
Iteration 110/1000 | Loss: 0.00001643
Iteration 111/1000 | Loss: 0.00001643
Iteration 112/1000 | Loss: 0.00001642
Iteration 113/1000 | Loss: 0.00001642
Iteration 114/1000 | Loss: 0.00001639
Iteration 115/1000 | Loss: 0.00001638
Iteration 116/1000 | Loss: 0.00001638
Iteration 117/1000 | Loss: 0.00001636
Iteration 118/1000 | Loss: 0.00001636
Iteration 119/1000 | Loss: 0.00001636
Iteration 120/1000 | Loss: 0.00001636
Iteration 121/1000 | Loss: 0.00001635
Iteration 122/1000 | Loss: 0.00001635
Iteration 123/1000 | Loss: 0.00001635
Iteration 124/1000 | Loss: 0.00001635
Iteration 125/1000 | Loss: 0.00001635
Iteration 126/1000 | Loss: 0.00001635
Iteration 127/1000 | Loss: 0.00001635
Iteration 128/1000 | Loss: 0.00001635
Iteration 129/1000 | Loss: 0.00001635
Iteration 130/1000 | Loss: 0.00001635
Iteration 131/1000 | Loss: 0.00001635
Iteration 132/1000 | Loss: 0.00001634
Iteration 133/1000 | Loss: 0.00001634
Iteration 134/1000 | Loss: 0.00001634
Iteration 135/1000 | Loss: 0.00001634
Iteration 136/1000 | Loss: 0.00001634
Iteration 137/1000 | Loss: 0.00001634
Iteration 138/1000 | Loss: 0.00001634
Iteration 139/1000 | Loss: 0.00001634
Iteration 140/1000 | Loss: 0.00001633
Iteration 141/1000 | Loss: 0.00001633
Iteration 142/1000 | Loss: 0.00001633
Iteration 143/1000 | Loss: 0.00001633
Iteration 144/1000 | Loss: 0.00001633
Iteration 145/1000 | Loss: 0.00001633
Iteration 146/1000 | Loss: 0.00001633
Iteration 147/1000 | Loss: 0.00001633
Iteration 148/1000 | Loss: 0.00001633
Iteration 149/1000 | Loss: 0.00001633
Iteration 150/1000 | Loss: 0.00001633
Iteration 151/1000 | Loss: 0.00001632
Iteration 152/1000 | Loss: 0.00001632
Iteration 153/1000 | Loss: 0.00001632
Iteration 154/1000 | Loss: 0.00001632
Iteration 155/1000 | Loss: 0.00001631
Iteration 156/1000 | Loss: 0.00001631
Iteration 157/1000 | Loss: 0.00001631
Iteration 158/1000 | Loss: 0.00001631
Iteration 159/1000 | Loss: 0.00001631
Iteration 160/1000 | Loss: 0.00001631
Iteration 161/1000 | Loss: 0.00001631
Iteration 162/1000 | Loss: 0.00001631
Iteration 163/1000 | Loss: 0.00001631
Iteration 164/1000 | Loss: 0.00001631
Iteration 165/1000 | Loss: 0.00001631
Iteration 166/1000 | Loss: 0.00001630
Iteration 167/1000 | Loss: 0.00001630
Iteration 168/1000 | Loss: 0.00001630
Iteration 169/1000 | Loss: 0.00001630
Iteration 170/1000 | Loss: 0.00001629
Iteration 171/1000 | Loss: 0.00001629
Iteration 172/1000 | Loss: 0.00001629
Iteration 173/1000 | Loss: 0.00001629
Iteration 174/1000 | Loss: 0.00001629
Iteration 175/1000 | Loss: 0.00001629
Iteration 176/1000 | Loss: 0.00001629
Iteration 177/1000 | Loss: 0.00001629
Iteration 178/1000 | Loss: 0.00001628
Iteration 179/1000 | Loss: 0.00001628
Iteration 180/1000 | Loss: 0.00001628
Iteration 181/1000 | Loss: 0.00001628
Iteration 182/1000 | Loss: 0.00001628
Iteration 183/1000 | Loss: 0.00001628
Iteration 184/1000 | Loss: 0.00001628
Iteration 185/1000 | Loss: 0.00001628
Iteration 186/1000 | Loss: 0.00001628
Iteration 187/1000 | Loss: 0.00001628
Iteration 188/1000 | Loss: 0.00001628
Iteration 189/1000 | Loss: 0.00001628
Iteration 190/1000 | Loss: 0.00001628
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [1.628096288186498e-05, 1.628096288186498e-05, 1.628096288186498e-05, 1.628096288186498e-05, 1.628096288186498e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.628096288186498e-05

Optimization complete. Final v2v error: 3.377890110015869 mm

Highest mean error: 3.936863899230957 mm for frame 200

Lowest mean error: 2.8807365894317627 mm for frame 0

Saving results

Total time: 69.30997371673584
