Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=115, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 6440-6495
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carina_posed_010/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00910078
Iteration 2/25 | Loss: 0.00229588
Iteration 3/25 | Loss: 0.00162467
Iteration 4/25 | Loss: 0.00152729
Iteration 5/25 | Loss: 0.00150294
Iteration 6/25 | Loss: 0.00149231
Iteration 7/25 | Loss: 0.00148740
Iteration 8/25 | Loss: 0.00147681
Iteration 9/25 | Loss: 0.00147540
Iteration 10/25 | Loss: 0.00147648
Iteration 11/25 | Loss: 0.00146641
Iteration 12/25 | Loss: 0.00146618
Iteration 13/25 | Loss: 0.00147083
Iteration 14/25 | Loss: 0.00147006
Iteration 15/25 | Loss: 0.00147568
Iteration 16/25 | Loss: 0.00148581
Iteration 17/25 | Loss: 0.00147301
Iteration 18/25 | Loss: 0.00146218
Iteration 19/25 | Loss: 0.00145636
Iteration 20/25 | Loss: 0.00145331
Iteration 21/25 | Loss: 0.00145648
Iteration 22/25 | Loss: 0.00145441
Iteration 23/25 | Loss: 0.00145092
Iteration 24/25 | Loss: 0.00145044
Iteration 25/25 | Loss: 0.00145037

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18457139
Iteration 2/25 | Loss: 0.00187966
Iteration 3/25 | Loss: 0.00187966
Iteration 4/25 | Loss: 0.00187966
Iteration 5/25 | Loss: 0.00187966
Iteration 6/25 | Loss: 0.00187966
Iteration 7/25 | Loss: 0.00187966
Iteration 8/25 | Loss: 0.00187966
Iteration 9/25 | Loss: 0.00187966
Iteration 10/25 | Loss: 0.00187966
Iteration 11/25 | Loss: 0.00187966
Iteration 12/25 | Loss: 0.00187966
Iteration 13/25 | Loss: 0.00187966
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.00187965901568532, 0.00187965901568532, 0.00187965901568532, 0.00187965901568532, 0.00187965901568532]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00187965901568532

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00187966
Iteration 2/1000 | Loss: 0.00004889
Iteration 3/1000 | Loss: 0.00003016
Iteration 4/1000 | Loss: 0.00002592
Iteration 5/1000 | Loss: 0.00002457
Iteration 6/1000 | Loss: 0.00002325
Iteration 7/1000 | Loss: 0.00002251
Iteration 8/1000 | Loss: 0.00002198
Iteration 9/1000 | Loss: 0.00002152
Iteration 10/1000 | Loss: 0.00002111
Iteration 11/1000 | Loss: 0.00002083
Iteration 12/1000 | Loss: 0.00002078
Iteration 13/1000 | Loss: 0.00002058
Iteration 14/1000 | Loss: 0.00002050
Iteration 15/1000 | Loss: 0.00002039
Iteration 16/1000 | Loss: 0.00002034
Iteration 17/1000 | Loss: 0.00002033
Iteration 18/1000 | Loss: 0.00002031
Iteration 19/1000 | Loss: 0.00002030
Iteration 20/1000 | Loss: 0.00002029
Iteration 21/1000 | Loss: 0.00002027
Iteration 22/1000 | Loss: 0.00002026
Iteration 23/1000 | Loss: 0.00002025
Iteration 24/1000 | Loss: 0.00002025
Iteration 25/1000 | Loss: 0.00002024
Iteration 26/1000 | Loss: 0.00002019
Iteration 27/1000 | Loss: 0.00002018
Iteration 28/1000 | Loss: 0.00002013
Iteration 29/1000 | Loss: 0.00002012
Iteration 30/1000 | Loss: 0.00002012
Iteration 31/1000 | Loss: 0.00002011
Iteration 32/1000 | Loss: 0.00002011
Iteration 33/1000 | Loss: 0.00002010
Iteration 34/1000 | Loss: 0.00002010
Iteration 35/1000 | Loss: 0.00002010
Iteration 36/1000 | Loss: 0.00002009
Iteration 37/1000 | Loss: 0.00002008
Iteration 38/1000 | Loss: 0.00002008
Iteration 39/1000 | Loss: 0.00002008
Iteration 40/1000 | Loss: 0.00002008
Iteration 41/1000 | Loss: 0.00002008
Iteration 42/1000 | Loss: 0.00002007
Iteration 43/1000 | Loss: 0.00002007
Iteration 44/1000 | Loss: 0.00002007
Iteration 45/1000 | Loss: 0.00002007
Iteration 46/1000 | Loss: 0.00002006
Iteration 47/1000 | Loss: 0.00002006
Iteration 48/1000 | Loss: 0.00002006
Iteration 49/1000 | Loss: 0.00002006
Iteration 50/1000 | Loss: 0.00002006
Iteration 51/1000 | Loss: 0.00002005
Iteration 52/1000 | Loss: 0.00002005
Iteration 53/1000 | Loss: 0.00002004
Iteration 54/1000 | Loss: 0.00002004
Iteration 55/1000 | Loss: 0.00002004
Iteration 56/1000 | Loss: 0.00002004
Iteration 57/1000 | Loss: 0.00002003
Iteration 58/1000 | Loss: 0.00002003
Iteration 59/1000 | Loss: 0.00002003
Iteration 60/1000 | Loss: 0.00002002
Iteration 61/1000 | Loss: 0.00002002
Iteration 62/1000 | Loss: 0.00002002
Iteration 63/1000 | Loss: 0.00002001
Iteration 64/1000 | Loss: 0.00002001
Iteration 65/1000 | Loss: 0.00002001
Iteration 66/1000 | Loss: 0.00002001
Iteration 67/1000 | Loss: 0.00002001
Iteration 68/1000 | Loss: 0.00002000
Iteration 69/1000 | Loss: 0.00002000
Iteration 70/1000 | Loss: 0.00002000
Iteration 71/1000 | Loss: 0.00002000
Iteration 72/1000 | Loss: 0.00002000
Iteration 73/1000 | Loss: 0.00001999
Iteration 74/1000 | Loss: 0.00001999
Iteration 75/1000 | Loss: 0.00001999
Iteration 76/1000 | Loss: 0.00001999
Iteration 77/1000 | Loss: 0.00001999
Iteration 78/1000 | Loss: 0.00001999
Iteration 79/1000 | Loss: 0.00001998
Iteration 80/1000 | Loss: 0.00001998
Iteration 81/1000 | Loss: 0.00001998
Iteration 82/1000 | Loss: 0.00001998
Iteration 83/1000 | Loss: 0.00001997
Iteration 84/1000 | Loss: 0.00001997
Iteration 85/1000 | Loss: 0.00001996
Iteration 86/1000 | Loss: 0.00001996
Iteration 87/1000 | Loss: 0.00001995
Iteration 88/1000 | Loss: 0.00001995
Iteration 89/1000 | Loss: 0.00001995
Iteration 90/1000 | Loss: 0.00001995
Iteration 91/1000 | Loss: 0.00001995
Iteration 92/1000 | Loss: 0.00001994
Iteration 93/1000 | Loss: 0.00001993
Iteration 94/1000 | Loss: 0.00001993
Iteration 95/1000 | Loss: 0.00001993
Iteration 96/1000 | Loss: 0.00001993
Iteration 97/1000 | Loss: 0.00001992
Iteration 98/1000 | Loss: 0.00001992
Iteration 99/1000 | Loss: 0.00001991
Iteration 100/1000 | Loss: 0.00001990
Iteration 101/1000 | Loss: 0.00001990
Iteration 102/1000 | Loss: 0.00001990
Iteration 103/1000 | Loss: 0.00001990
Iteration 104/1000 | Loss: 0.00001989
Iteration 105/1000 | Loss: 0.00001989
Iteration 106/1000 | Loss: 0.00001989
Iteration 107/1000 | Loss: 0.00001988
Iteration 108/1000 | Loss: 0.00001988
Iteration 109/1000 | Loss: 0.00001988
Iteration 110/1000 | Loss: 0.00001988
Iteration 111/1000 | Loss: 0.00001987
Iteration 112/1000 | Loss: 0.00001987
Iteration 113/1000 | Loss: 0.00001987
Iteration 114/1000 | Loss: 0.00001986
Iteration 115/1000 | Loss: 0.00001986
Iteration 116/1000 | Loss: 0.00001986
Iteration 117/1000 | Loss: 0.00001986
Iteration 118/1000 | Loss: 0.00001985
Iteration 119/1000 | Loss: 0.00001985
Iteration 120/1000 | Loss: 0.00001985
Iteration 121/1000 | Loss: 0.00001985
Iteration 122/1000 | Loss: 0.00001984
Iteration 123/1000 | Loss: 0.00001984
Iteration 124/1000 | Loss: 0.00001984
Iteration 125/1000 | Loss: 0.00001984
Iteration 126/1000 | Loss: 0.00001984
Iteration 127/1000 | Loss: 0.00001983
Iteration 128/1000 | Loss: 0.00001983
Iteration 129/1000 | Loss: 0.00001983
Iteration 130/1000 | Loss: 0.00001982
Iteration 131/1000 | Loss: 0.00001982
Iteration 132/1000 | Loss: 0.00001982
Iteration 133/1000 | Loss: 0.00001981
Iteration 134/1000 | Loss: 0.00001981
Iteration 135/1000 | Loss: 0.00001981
Iteration 136/1000 | Loss: 0.00001981
Iteration 137/1000 | Loss: 0.00001981
Iteration 138/1000 | Loss: 0.00001981
Iteration 139/1000 | Loss: 0.00001981
Iteration 140/1000 | Loss: 0.00001981
Iteration 141/1000 | Loss: 0.00001981
Iteration 142/1000 | Loss: 0.00001980
Iteration 143/1000 | Loss: 0.00001980
Iteration 144/1000 | Loss: 0.00001980
Iteration 145/1000 | Loss: 0.00001979
Iteration 146/1000 | Loss: 0.00001979
Iteration 147/1000 | Loss: 0.00001979
Iteration 148/1000 | Loss: 0.00001978
Iteration 149/1000 | Loss: 0.00001978
Iteration 150/1000 | Loss: 0.00001978
Iteration 151/1000 | Loss: 0.00001978
Iteration 152/1000 | Loss: 0.00001978
Iteration 153/1000 | Loss: 0.00001978
Iteration 154/1000 | Loss: 0.00001978
Iteration 155/1000 | Loss: 0.00001978
Iteration 156/1000 | Loss: 0.00001977
Iteration 157/1000 | Loss: 0.00001977
Iteration 158/1000 | Loss: 0.00001977
Iteration 159/1000 | Loss: 0.00001977
Iteration 160/1000 | Loss: 0.00001977
Iteration 161/1000 | Loss: 0.00001977
Iteration 162/1000 | Loss: 0.00001977
Iteration 163/1000 | Loss: 0.00001977
Iteration 164/1000 | Loss: 0.00001977
Iteration 165/1000 | Loss: 0.00001977
Iteration 166/1000 | Loss: 0.00001977
Iteration 167/1000 | Loss: 0.00001977
Iteration 168/1000 | Loss: 0.00001977
Iteration 169/1000 | Loss: 0.00001977
Iteration 170/1000 | Loss: 0.00001977
Iteration 171/1000 | Loss: 0.00001977
Iteration 172/1000 | Loss: 0.00001977
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.9770654034800828e-05, 1.9770654034800828e-05, 1.9770654034800828e-05, 1.9770654034800828e-05, 1.9770654034800828e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9770654034800828e-05

Optimization complete. Final v2v error: 3.6830554008483887 mm

Highest mean error: 6.990780830383301 mm for frame 38

Lowest mean error: 3.1808109283447266 mm for frame 103

Saving results

Total time: 82.81948256492615
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carina_posed_010/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00409418
Iteration 2/25 | Loss: 0.00142361
Iteration 3/25 | Loss: 0.00136140
Iteration 4/25 | Loss: 0.00135576
Iteration 5/25 | Loss: 0.00135432
Iteration 6/25 | Loss: 0.00135411
Iteration 7/25 | Loss: 0.00135411
Iteration 8/25 | Loss: 0.00135411
Iteration 9/25 | Loss: 0.00135411
Iteration 10/25 | Loss: 0.00135411
Iteration 11/25 | Loss: 0.00135411
Iteration 12/25 | Loss: 0.00135411
Iteration 13/25 | Loss: 0.00135411
Iteration 14/25 | Loss: 0.00135411
Iteration 15/25 | Loss: 0.00135411
Iteration 16/25 | Loss: 0.00135411
Iteration 17/25 | Loss: 0.00135411
Iteration 18/25 | Loss: 0.00135411
Iteration 19/25 | Loss: 0.00135411
Iteration 20/25 | Loss: 0.00135411
Iteration 21/25 | Loss: 0.00135411
Iteration 22/25 | Loss: 0.00135411
Iteration 23/25 | Loss: 0.00135411
Iteration 24/25 | Loss: 0.00135411
Iteration 25/25 | Loss: 0.00135411

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23820245
Iteration 2/25 | Loss: 0.00230375
Iteration 3/25 | Loss: 0.00230372
Iteration 4/25 | Loss: 0.00230372
Iteration 5/25 | Loss: 0.00230372
Iteration 6/25 | Loss: 0.00230372
Iteration 7/25 | Loss: 0.00230372
Iteration 8/25 | Loss: 0.00230372
Iteration 9/25 | Loss: 0.00230372
Iteration 10/25 | Loss: 0.00230372
Iteration 11/25 | Loss: 0.00230372
Iteration 12/25 | Loss: 0.00230372
Iteration 13/25 | Loss: 0.00230372
Iteration 14/25 | Loss: 0.00230372
Iteration 15/25 | Loss: 0.00230372
Iteration 16/25 | Loss: 0.00230372
Iteration 17/25 | Loss: 0.00230372
Iteration 18/25 | Loss: 0.00230372
Iteration 19/25 | Loss: 0.00230372
Iteration 20/25 | Loss: 0.00230372
Iteration 21/25 | Loss: 0.00230372
Iteration 22/25 | Loss: 0.00230372
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0023037169594317675, 0.0023037169594317675, 0.0023037169594317675, 0.0023037169594317675, 0.0023037169594317675]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023037169594317675

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00230372
Iteration 2/1000 | Loss: 0.00002915
Iteration 3/1000 | Loss: 0.00001831
Iteration 4/1000 | Loss: 0.00001592
Iteration 5/1000 | Loss: 0.00001483
Iteration 6/1000 | Loss: 0.00001420
Iteration 7/1000 | Loss: 0.00001361
Iteration 8/1000 | Loss: 0.00001328
Iteration 9/1000 | Loss: 0.00001298
Iteration 10/1000 | Loss: 0.00001274
Iteration 11/1000 | Loss: 0.00001255
Iteration 12/1000 | Loss: 0.00001247
Iteration 13/1000 | Loss: 0.00001232
Iteration 14/1000 | Loss: 0.00001223
Iteration 15/1000 | Loss: 0.00001205
Iteration 16/1000 | Loss: 0.00001198
Iteration 17/1000 | Loss: 0.00001198
Iteration 18/1000 | Loss: 0.00001197
Iteration 19/1000 | Loss: 0.00001196
Iteration 20/1000 | Loss: 0.00001195
Iteration 21/1000 | Loss: 0.00001195
Iteration 22/1000 | Loss: 0.00001194
Iteration 23/1000 | Loss: 0.00001193
Iteration 24/1000 | Loss: 0.00001192
Iteration 25/1000 | Loss: 0.00001190
Iteration 26/1000 | Loss: 0.00001190
Iteration 27/1000 | Loss: 0.00001188
Iteration 28/1000 | Loss: 0.00001188
Iteration 29/1000 | Loss: 0.00001185
Iteration 30/1000 | Loss: 0.00001183
Iteration 31/1000 | Loss: 0.00001182
Iteration 32/1000 | Loss: 0.00001181
Iteration 33/1000 | Loss: 0.00001181
Iteration 34/1000 | Loss: 0.00001181
Iteration 35/1000 | Loss: 0.00001180
Iteration 36/1000 | Loss: 0.00001177
Iteration 37/1000 | Loss: 0.00001177
Iteration 38/1000 | Loss: 0.00001176
Iteration 39/1000 | Loss: 0.00001176
Iteration 40/1000 | Loss: 0.00001175
Iteration 41/1000 | Loss: 0.00001172
Iteration 42/1000 | Loss: 0.00001170
Iteration 43/1000 | Loss: 0.00001170
Iteration 44/1000 | Loss: 0.00001169
Iteration 45/1000 | Loss: 0.00001164
Iteration 46/1000 | Loss: 0.00001162
Iteration 47/1000 | Loss: 0.00001162
Iteration 48/1000 | Loss: 0.00001161
Iteration 49/1000 | Loss: 0.00001160
Iteration 50/1000 | Loss: 0.00001159
Iteration 51/1000 | Loss: 0.00001154
Iteration 52/1000 | Loss: 0.00001148
Iteration 53/1000 | Loss: 0.00001148
Iteration 54/1000 | Loss: 0.00001147
Iteration 55/1000 | Loss: 0.00001147
Iteration 56/1000 | Loss: 0.00001146
Iteration 57/1000 | Loss: 0.00001146
Iteration 58/1000 | Loss: 0.00001146
Iteration 59/1000 | Loss: 0.00001146
Iteration 60/1000 | Loss: 0.00001146
Iteration 61/1000 | Loss: 0.00001146
Iteration 62/1000 | Loss: 0.00001146
Iteration 63/1000 | Loss: 0.00001146
Iteration 64/1000 | Loss: 0.00001146
Iteration 65/1000 | Loss: 0.00001145
Iteration 66/1000 | Loss: 0.00001145
Iteration 67/1000 | Loss: 0.00001144
Iteration 68/1000 | Loss: 0.00001144
Iteration 69/1000 | Loss: 0.00001144
Iteration 70/1000 | Loss: 0.00001143
Iteration 71/1000 | Loss: 0.00001143
Iteration 72/1000 | Loss: 0.00001143
Iteration 73/1000 | Loss: 0.00001143
Iteration 74/1000 | Loss: 0.00001143
Iteration 75/1000 | Loss: 0.00001143
Iteration 76/1000 | Loss: 0.00001143
Iteration 77/1000 | Loss: 0.00001143
Iteration 78/1000 | Loss: 0.00001143
Iteration 79/1000 | Loss: 0.00001142
Iteration 80/1000 | Loss: 0.00001142
Iteration 81/1000 | Loss: 0.00001141
Iteration 82/1000 | Loss: 0.00001141
Iteration 83/1000 | Loss: 0.00001141
Iteration 84/1000 | Loss: 0.00001140
Iteration 85/1000 | Loss: 0.00001140
Iteration 86/1000 | Loss: 0.00001140
Iteration 87/1000 | Loss: 0.00001140
Iteration 88/1000 | Loss: 0.00001140
Iteration 89/1000 | Loss: 0.00001140
Iteration 90/1000 | Loss: 0.00001140
Iteration 91/1000 | Loss: 0.00001140
Iteration 92/1000 | Loss: 0.00001140
Iteration 93/1000 | Loss: 0.00001139
Iteration 94/1000 | Loss: 0.00001139
Iteration 95/1000 | Loss: 0.00001139
Iteration 96/1000 | Loss: 0.00001138
Iteration 97/1000 | Loss: 0.00001138
Iteration 98/1000 | Loss: 0.00001138
Iteration 99/1000 | Loss: 0.00001138
Iteration 100/1000 | Loss: 0.00001138
Iteration 101/1000 | Loss: 0.00001138
Iteration 102/1000 | Loss: 0.00001138
Iteration 103/1000 | Loss: 0.00001137
Iteration 104/1000 | Loss: 0.00001137
Iteration 105/1000 | Loss: 0.00001137
Iteration 106/1000 | Loss: 0.00001137
Iteration 107/1000 | Loss: 0.00001137
Iteration 108/1000 | Loss: 0.00001136
Iteration 109/1000 | Loss: 0.00001136
Iteration 110/1000 | Loss: 0.00001136
Iteration 111/1000 | Loss: 0.00001136
Iteration 112/1000 | Loss: 0.00001136
Iteration 113/1000 | Loss: 0.00001135
Iteration 114/1000 | Loss: 0.00001135
Iteration 115/1000 | Loss: 0.00001135
Iteration 116/1000 | Loss: 0.00001135
Iteration 117/1000 | Loss: 0.00001134
Iteration 118/1000 | Loss: 0.00001134
Iteration 119/1000 | Loss: 0.00001134
Iteration 120/1000 | Loss: 0.00001133
Iteration 121/1000 | Loss: 0.00001133
Iteration 122/1000 | Loss: 0.00001133
Iteration 123/1000 | Loss: 0.00001133
Iteration 124/1000 | Loss: 0.00001132
Iteration 125/1000 | Loss: 0.00001132
Iteration 126/1000 | Loss: 0.00001132
Iteration 127/1000 | Loss: 0.00001132
Iteration 128/1000 | Loss: 0.00001132
Iteration 129/1000 | Loss: 0.00001132
Iteration 130/1000 | Loss: 0.00001132
Iteration 131/1000 | Loss: 0.00001132
Iteration 132/1000 | Loss: 0.00001131
Iteration 133/1000 | Loss: 0.00001131
Iteration 134/1000 | Loss: 0.00001131
Iteration 135/1000 | Loss: 0.00001131
Iteration 136/1000 | Loss: 0.00001131
Iteration 137/1000 | Loss: 0.00001130
Iteration 138/1000 | Loss: 0.00001130
Iteration 139/1000 | Loss: 0.00001130
Iteration 140/1000 | Loss: 0.00001130
Iteration 141/1000 | Loss: 0.00001129
Iteration 142/1000 | Loss: 0.00001129
Iteration 143/1000 | Loss: 0.00001129
Iteration 144/1000 | Loss: 0.00001129
Iteration 145/1000 | Loss: 0.00001128
Iteration 146/1000 | Loss: 0.00001128
Iteration 147/1000 | Loss: 0.00001128
Iteration 148/1000 | Loss: 0.00001128
Iteration 149/1000 | Loss: 0.00001128
Iteration 150/1000 | Loss: 0.00001128
Iteration 151/1000 | Loss: 0.00001128
Iteration 152/1000 | Loss: 0.00001128
Iteration 153/1000 | Loss: 0.00001127
Iteration 154/1000 | Loss: 0.00001127
Iteration 155/1000 | Loss: 0.00001127
Iteration 156/1000 | Loss: 0.00001127
Iteration 157/1000 | Loss: 0.00001127
Iteration 158/1000 | Loss: 0.00001127
Iteration 159/1000 | Loss: 0.00001126
Iteration 160/1000 | Loss: 0.00001126
Iteration 161/1000 | Loss: 0.00001126
Iteration 162/1000 | Loss: 0.00001126
Iteration 163/1000 | Loss: 0.00001126
Iteration 164/1000 | Loss: 0.00001126
Iteration 165/1000 | Loss: 0.00001126
Iteration 166/1000 | Loss: 0.00001125
Iteration 167/1000 | Loss: 0.00001125
Iteration 168/1000 | Loss: 0.00001125
Iteration 169/1000 | Loss: 0.00001125
Iteration 170/1000 | Loss: 0.00001125
Iteration 171/1000 | Loss: 0.00001125
Iteration 172/1000 | Loss: 0.00001125
Iteration 173/1000 | Loss: 0.00001125
Iteration 174/1000 | Loss: 0.00001125
Iteration 175/1000 | Loss: 0.00001124
Iteration 176/1000 | Loss: 0.00001124
Iteration 177/1000 | Loss: 0.00001124
Iteration 178/1000 | Loss: 0.00001124
Iteration 179/1000 | Loss: 0.00001124
Iteration 180/1000 | Loss: 0.00001124
Iteration 181/1000 | Loss: 0.00001124
Iteration 182/1000 | Loss: 0.00001124
Iteration 183/1000 | Loss: 0.00001124
Iteration 184/1000 | Loss: 0.00001124
Iteration 185/1000 | Loss: 0.00001124
Iteration 186/1000 | Loss: 0.00001124
Iteration 187/1000 | Loss: 0.00001124
Iteration 188/1000 | Loss: 0.00001124
Iteration 189/1000 | Loss: 0.00001124
Iteration 190/1000 | Loss: 0.00001124
Iteration 191/1000 | Loss: 0.00001123
Iteration 192/1000 | Loss: 0.00001123
Iteration 193/1000 | Loss: 0.00001123
Iteration 194/1000 | Loss: 0.00001123
Iteration 195/1000 | Loss: 0.00001123
Iteration 196/1000 | Loss: 0.00001123
Iteration 197/1000 | Loss: 0.00001123
Iteration 198/1000 | Loss: 0.00001123
Iteration 199/1000 | Loss: 0.00001123
Iteration 200/1000 | Loss: 0.00001123
Iteration 201/1000 | Loss: 0.00001123
Iteration 202/1000 | Loss: 0.00001122
Iteration 203/1000 | Loss: 0.00001122
Iteration 204/1000 | Loss: 0.00001122
Iteration 205/1000 | Loss: 0.00001122
Iteration 206/1000 | Loss: 0.00001122
Iteration 207/1000 | Loss: 0.00001122
Iteration 208/1000 | Loss: 0.00001122
Iteration 209/1000 | Loss: 0.00001122
Iteration 210/1000 | Loss: 0.00001122
Iteration 211/1000 | Loss: 0.00001122
Iteration 212/1000 | Loss: 0.00001122
Iteration 213/1000 | Loss: 0.00001122
Iteration 214/1000 | Loss: 0.00001122
Iteration 215/1000 | Loss: 0.00001122
Iteration 216/1000 | Loss: 0.00001122
Iteration 217/1000 | Loss: 0.00001122
Iteration 218/1000 | Loss: 0.00001122
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 218. Stopping optimization.
Last 5 losses: [1.1223596629861277e-05, 1.1223596629861277e-05, 1.1223596629861277e-05, 1.1223596629861277e-05, 1.1223596629861277e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1223596629861277e-05

Optimization complete. Final v2v error: 2.8861899375915527 mm

Highest mean error: 3.164473056793213 mm for frame 84

Lowest mean error: 2.724717617034912 mm for frame 4

Saving results

Total time: 44.471840620040894
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carina_posed_010/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00642355
Iteration 2/25 | Loss: 0.00166841
Iteration 3/25 | Loss: 0.00144213
Iteration 4/25 | Loss: 0.00142150
Iteration 5/25 | Loss: 0.00141820
Iteration 6/25 | Loss: 0.00140801
Iteration 7/25 | Loss: 0.00140688
Iteration 8/25 | Loss: 0.00140495
Iteration 9/25 | Loss: 0.00140358
Iteration 10/25 | Loss: 0.00140492
Iteration 11/25 | Loss: 0.00140292
Iteration 12/25 | Loss: 0.00140195
Iteration 13/25 | Loss: 0.00140128
Iteration 14/25 | Loss: 0.00140114
Iteration 15/25 | Loss: 0.00140114
Iteration 16/25 | Loss: 0.00140114
Iteration 17/25 | Loss: 0.00140113
Iteration 18/25 | Loss: 0.00140113
Iteration 19/25 | Loss: 0.00140113
Iteration 20/25 | Loss: 0.00140113
Iteration 21/25 | Loss: 0.00140113
Iteration 22/25 | Loss: 0.00140113
Iteration 23/25 | Loss: 0.00140113
Iteration 24/25 | Loss: 0.00140112
Iteration 25/25 | Loss: 0.00140112

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.73173916
Iteration 2/25 | Loss: 0.00272350
Iteration 3/25 | Loss: 0.00272348
Iteration 4/25 | Loss: 0.00272348
Iteration 5/25 | Loss: 0.00272348
Iteration 6/25 | Loss: 0.00272348
Iteration 7/25 | Loss: 0.00272348
Iteration 8/25 | Loss: 0.00272348
Iteration 9/25 | Loss: 0.00272348
Iteration 10/25 | Loss: 0.00272348
Iteration 11/25 | Loss: 0.00272348
Iteration 12/25 | Loss: 0.00272348
Iteration 13/25 | Loss: 0.00272348
Iteration 14/25 | Loss: 0.00272348
Iteration 15/25 | Loss: 0.00272348
Iteration 16/25 | Loss: 0.00272348
Iteration 17/25 | Loss: 0.00272348
Iteration 18/25 | Loss: 0.00272348
Iteration 19/25 | Loss: 0.00272348
Iteration 20/25 | Loss: 0.00272348
Iteration 21/25 | Loss: 0.00272348
Iteration 22/25 | Loss: 0.00272348
Iteration 23/25 | Loss: 0.00272348
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00272347591817379, 0.00272347591817379, 0.00272347591817379, 0.00272347591817379, 0.00272347591817379]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00272347591817379

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00272348
Iteration 2/1000 | Loss: 0.00008379
Iteration 3/1000 | Loss: 0.00005219
Iteration 4/1000 | Loss: 0.00004059
Iteration 5/1000 | Loss: 0.00003557
Iteration 6/1000 | Loss: 0.00024858
Iteration 7/1000 | Loss: 0.00003518
Iteration 8/1000 | Loss: 0.00006526
Iteration 9/1000 | Loss: 0.00003489
Iteration 10/1000 | Loss: 0.00002452
Iteration 11/1000 | Loss: 0.00004075
Iteration 12/1000 | Loss: 0.00002261
Iteration 13/1000 | Loss: 0.00002197
Iteration 14/1000 | Loss: 0.00002155
Iteration 15/1000 | Loss: 0.00002119
Iteration 16/1000 | Loss: 0.00007459
Iteration 17/1000 | Loss: 0.00002186
Iteration 18/1000 | Loss: 0.00002775
Iteration 19/1000 | Loss: 0.00027757
Iteration 20/1000 | Loss: 0.00002372
Iteration 21/1000 | Loss: 0.00002016
Iteration 22/1000 | Loss: 0.00001920
Iteration 23/1000 | Loss: 0.00004693
Iteration 24/1000 | Loss: 0.00001853
Iteration 25/1000 | Loss: 0.00001814
Iteration 26/1000 | Loss: 0.00001786
Iteration 27/1000 | Loss: 0.00001775
Iteration 28/1000 | Loss: 0.00001774
Iteration 29/1000 | Loss: 0.00001773
Iteration 30/1000 | Loss: 0.00001772
Iteration 31/1000 | Loss: 0.00001771
Iteration 32/1000 | Loss: 0.00001771
Iteration 33/1000 | Loss: 0.00001770
Iteration 34/1000 | Loss: 0.00001769
Iteration 35/1000 | Loss: 0.00001762
Iteration 36/1000 | Loss: 0.00001755
Iteration 37/1000 | Loss: 0.00006176
Iteration 38/1000 | Loss: 0.00001887
Iteration 39/1000 | Loss: 0.00001923
Iteration 40/1000 | Loss: 0.00001743
Iteration 41/1000 | Loss: 0.00001738
Iteration 42/1000 | Loss: 0.00002248
Iteration 43/1000 | Loss: 0.00001732
Iteration 44/1000 | Loss: 0.00001732
Iteration 45/1000 | Loss: 0.00001732
Iteration 46/1000 | Loss: 0.00001732
Iteration 47/1000 | Loss: 0.00001732
Iteration 48/1000 | Loss: 0.00001732
Iteration 49/1000 | Loss: 0.00001732
Iteration 50/1000 | Loss: 0.00001732
Iteration 51/1000 | Loss: 0.00001732
Iteration 52/1000 | Loss: 0.00001731
Iteration 53/1000 | Loss: 0.00001731
Iteration 54/1000 | Loss: 0.00001731
Iteration 55/1000 | Loss: 0.00001730
Iteration 56/1000 | Loss: 0.00001727
Iteration 57/1000 | Loss: 0.00001726
Iteration 58/1000 | Loss: 0.00001726
Iteration 59/1000 | Loss: 0.00001726
Iteration 60/1000 | Loss: 0.00001726
Iteration 61/1000 | Loss: 0.00001726
Iteration 62/1000 | Loss: 0.00002032
Iteration 63/1000 | Loss: 0.00001721
Iteration 64/1000 | Loss: 0.00001721
Iteration 65/1000 | Loss: 0.00001721
Iteration 66/1000 | Loss: 0.00001721
Iteration 67/1000 | Loss: 0.00001721
Iteration 68/1000 | Loss: 0.00001721
Iteration 69/1000 | Loss: 0.00001720
Iteration 70/1000 | Loss: 0.00001720
Iteration 71/1000 | Loss: 0.00001720
Iteration 72/1000 | Loss: 0.00001719
Iteration 73/1000 | Loss: 0.00001718
Iteration 74/1000 | Loss: 0.00001711
Iteration 75/1000 | Loss: 0.00001711
Iteration 76/1000 | Loss: 0.00001711
Iteration 77/1000 | Loss: 0.00001711
Iteration 78/1000 | Loss: 0.00001711
Iteration 79/1000 | Loss: 0.00001709
Iteration 80/1000 | Loss: 0.00001707
Iteration 81/1000 | Loss: 0.00001706
Iteration 82/1000 | Loss: 0.00001705
Iteration 83/1000 | Loss: 0.00001705
Iteration 84/1000 | Loss: 0.00001705
Iteration 85/1000 | Loss: 0.00001705
Iteration 86/1000 | Loss: 0.00001705
Iteration 87/1000 | Loss: 0.00001704
Iteration 88/1000 | Loss: 0.00001704
Iteration 89/1000 | Loss: 0.00001703
Iteration 90/1000 | Loss: 0.00001703
Iteration 91/1000 | Loss: 0.00001702
Iteration 92/1000 | Loss: 0.00001702
Iteration 93/1000 | Loss: 0.00001701
Iteration 94/1000 | Loss: 0.00001700
Iteration 95/1000 | Loss: 0.00001700
Iteration 96/1000 | Loss: 0.00001700
Iteration 97/1000 | Loss: 0.00001699
Iteration 98/1000 | Loss: 0.00001699
Iteration 99/1000 | Loss: 0.00001698
Iteration 100/1000 | Loss: 0.00001698
Iteration 101/1000 | Loss: 0.00001697
Iteration 102/1000 | Loss: 0.00001697
Iteration 103/1000 | Loss: 0.00001697
Iteration 104/1000 | Loss: 0.00001697
Iteration 105/1000 | Loss: 0.00001697
Iteration 106/1000 | Loss: 0.00001697
Iteration 107/1000 | Loss: 0.00001697
Iteration 108/1000 | Loss: 0.00001697
Iteration 109/1000 | Loss: 0.00006238
Iteration 110/1000 | Loss: 0.00001881
Iteration 111/1000 | Loss: 0.00001704
Iteration 112/1000 | Loss: 0.00002261
Iteration 113/1000 | Loss: 0.00001696
Iteration 114/1000 | Loss: 0.00001696
Iteration 115/1000 | Loss: 0.00001696
Iteration 116/1000 | Loss: 0.00001696
Iteration 117/1000 | Loss: 0.00001696
Iteration 118/1000 | Loss: 0.00001696
Iteration 119/1000 | Loss: 0.00001696
Iteration 120/1000 | Loss: 0.00001696
Iteration 121/1000 | Loss: 0.00001696
Iteration 122/1000 | Loss: 0.00001696
Iteration 123/1000 | Loss: 0.00001696
Iteration 124/1000 | Loss: 0.00001696
Iteration 125/1000 | Loss: 0.00001696
Iteration 126/1000 | Loss: 0.00001695
Iteration 127/1000 | Loss: 0.00001695
Iteration 128/1000 | Loss: 0.00001695
Iteration 129/1000 | Loss: 0.00001695
Iteration 130/1000 | Loss: 0.00001695
Iteration 131/1000 | Loss: 0.00001695
Iteration 132/1000 | Loss: 0.00001694
Iteration 133/1000 | Loss: 0.00001694
Iteration 134/1000 | Loss: 0.00001694
Iteration 135/1000 | Loss: 0.00001694
Iteration 136/1000 | Loss: 0.00001693
Iteration 137/1000 | Loss: 0.00001693
Iteration 138/1000 | Loss: 0.00001693
Iteration 139/1000 | Loss: 0.00001693
Iteration 140/1000 | Loss: 0.00001693
Iteration 141/1000 | Loss: 0.00001692
Iteration 142/1000 | Loss: 0.00001692
Iteration 143/1000 | Loss: 0.00001692
Iteration 144/1000 | Loss: 0.00001692
Iteration 145/1000 | Loss: 0.00001692
Iteration 146/1000 | Loss: 0.00001691
Iteration 147/1000 | Loss: 0.00001691
Iteration 148/1000 | Loss: 0.00001691
Iteration 149/1000 | Loss: 0.00001691
Iteration 150/1000 | Loss: 0.00001691
Iteration 151/1000 | Loss: 0.00001691
Iteration 152/1000 | Loss: 0.00001690
Iteration 153/1000 | Loss: 0.00001690
Iteration 154/1000 | Loss: 0.00001690
Iteration 155/1000 | Loss: 0.00001690
Iteration 156/1000 | Loss: 0.00001690
Iteration 157/1000 | Loss: 0.00001690
Iteration 158/1000 | Loss: 0.00001690
Iteration 159/1000 | Loss: 0.00001690
Iteration 160/1000 | Loss: 0.00001690
Iteration 161/1000 | Loss: 0.00001690
Iteration 162/1000 | Loss: 0.00001689
Iteration 163/1000 | Loss: 0.00001689
Iteration 164/1000 | Loss: 0.00001689
Iteration 165/1000 | Loss: 0.00001689
Iteration 166/1000 | Loss: 0.00001689
Iteration 167/1000 | Loss: 0.00001689
Iteration 168/1000 | Loss: 0.00001689
Iteration 169/1000 | Loss: 0.00001689
Iteration 170/1000 | Loss: 0.00001689
Iteration 171/1000 | Loss: 0.00001689
Iteration 172/1000 | Loss: 0.00001689
Iteration 173/1000 | Loss: 0.00001689
Iteration 174/1000 | Loss: 0.00001689
Iteration 175/1000 | Loss: 0.00001688
Iteration 176/1000 | Loss: 0.00001688
Iteration 177/1000 | Loss: 0.00001688
Iteration 178/1000 | Loss: 0.00001688
Iteration 179/1000 | Loss: 0.00001688
Iteration 180/1000 | Loss: 0.00001688
Iteration 181/1000 | Loss: 0.00001688
Iteration 182/1000 | Loss: 0.00001688
Iteration 183/1000 | Loss: 0.00001688
Iteration 184/1000 | Loss: 0.00001687
Iteration 185/1000 | Loss: 0.00001687
Iteration 186/1000 | Loss: 0.00001687
Iteration 187/1000 | Loss: 0.00001687
Iteration 188/1000 | Loss: 0.00001687
Iteration 189/1000 | Loss: 0.00001687
Iteration 190/1000 | Loss: 0.00001687
Iteration 191/1000 | Loss: 0.00001687
Iteration 192/1000 | Loss: 0.00001687
Iteration 193/1000 | Loss: 0.00001687
Iteration 194/1000 | Loss: 0.00001687
Iteration 195/1000 | Loss: 0.00001687
Iteration 196/1000 | Loss: 0.00001687
Iteration 197/1000 | Loss: 0.00001687
Iteration 198/1000 | Loss: 0.00001687
Iteration 199/1000 | Loss: 0.00001687
Iteration 200/1000 | Loss: 0.00001687
Iteration 201/1000 | Loss: 0.00001687
Iteration 202/1000 | Loss: 0.00004618
Iteration 203/1000 | Loss: 0.00001750
Iteration 204/1000 | Loss: 0.00001689
Iteration 205/1000 | Loss: 0.00001689
Iteration 206/1000 | Loss: 0.00001689
Iteration 207/1000 | Loss: 0.00001688
Iteration 208/1000 | Loss: 0.00001688
Iteration 209/1000 | Loss: 0.00001688
Iteration 210/1000 | Loss: 0.00001926
Iteration 211/1000 | Loss: 0.00001688
Iteration 212/1000 | Loss: 0.00001688
Iteration 213/1000 | Loss: 0.00001687
Iteration 214/1000 | Loss: 0.00001687
Iteration 215/1000 | Loss: 0.00001687
Iteration 216/1000 | Loss: 0.00001687
Iteration 217/1000 | Loss: 0.00001687
Iteration 218/1000 | Loss: 0.00001687
Iteration 219/1000 | Loss: 0.00001687
Iteration 220/1000 | Loss: 0.00001687
Iteration 221/1000 | Loss: 0.00001687
Iteration 222/1000 | Loss: 0.00001687
Iteration 223/1000 | Loss: 0.00001687
Iteration 224/1000 | Loss: 0.00001687
Iteration 225/1000 | Loss: 0.00001687
Iteration 226/1000 | Loss: 0.00001687
Iteration 227/1000 | Loss: 0.00001687
Iteration 228/1000 | Loss: 0.00001687
Iteration 229/1000 | Loss: 0.00001687
Iteration 230/1000 | Loss: 0.00001687
Iteration 231/1000 | Loss: 0.00001687
Iteration 232/1000 | Loss: 0.00001687
Iteration 233/1000 | Loss: 0.00001687
Iteration 234/1000 | Loss: 0.00001687
Iteration 235/1000 | Loss: 0.00001687
Iteration 236/1000 | Loss: 0.00001687
Iteration 237/1000 | Loss: 0.00001687
Iteration 238/1000 | Loss: 0.00001687
Iteration 239/1000 | Loss: 0.00001687
Iteration 240/1000 | Loss: 0.00001687
Iteration 241/1000 | Loss: 0.00001687
Iteration 242/1000 | Loss: 0.00001687
Iteration 243/1000 | Loss: 0.00001687
Iteration 244/1000 | Loss: 0.00001687
Iteration 245/1000 | Loss: 0.00001687
Iteration 246/1000 | Loss: 0.00001687
Iteration 247/1000 | Loss: 0.00001687
Iteration 248/1000 | Loss: 0.00001687
Iteration 249/1000 | Loss: 0.00001687
Iteration 250/1000 | Loss: 0.00001687
Iteration 251/1000 | Loss: 0.00001687
Iteration 252/1000 | Loss: 0.00001687
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 252. Stopping optimization.
Last 5 losses: [1.686556606728118e-05, 1.686556606728118e-05, 1.686556606728118e-05, 1.686556606728118e-05, 1.686556606728118e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.686556606728118e-05

Optimization complete. Final v2v error: 3.462456703186035 mm

Highest mean error: 4.280311107635498 mm for frame 176

Lowest mean error: 2.948923110961914 mm for frame 156

Saving results

Total time: 107.61948204040527
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carina_posed_010/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00829780
Iteration 2/25 | Loss: 0.00153720
Iteration 3/25 | Loss: 0.00139886
Iteration 4/25 | Loss: 0.00138376
Iteration 5/25 | Loss: 0.00137919
Iteration 6/25 | Loss: 0.00137892
Iteration 7/25 | Loss: 0.00137892
Iteration 8/25 | Loss: 0.00137892
Iteration 9/25 | Loss: 0.00137892
Iteration 10/25 | Loss: 0.00137892
Iteration 11/25 | Loss: 0.00137892
Iteration 12/25 | Loss: 0.00137892
Iteration 13/25 | Loss: 0.00137892
Iteration 14/25 | Loss: 0.00137892
Iteration 15/25 | Loss: 0.00137892
Iteration 16/25 | Loss: 0.00137892
Iteration 17/25 | Loss: 0.00137892
Iteration 18/25 | Loss: 0.00137892
Iteration 19/25 | Loss: 0.00137892
Iteration 20/25 | Loss: 0.00137892
Iteration 21/25 | Loss: 0.00137892
Iteration 22/25 | Loss: 0.00137892
Iteration 23/25 | Loss: 0.00137892
Iteration 24/25 | Loss: 0.00137892
Iteration 25/25 | Loss: 0.00137892

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.84417313
Iteration 2/25 | Loss: 0.00114392
Iteration 3/25 | Loss: 0.00114392
Iteration 4/25 | Loss: 0.00114392
Iteration 5/25 | Loss: 0.00114392
Iteration 6/25 | Loss: 0.00114392
Iteration 7/25 | Loss: 0.00114392
Iteration 8/25 | Loss: 0.00114392
Iteration 9/25 | Loss: 0.00114392
Iteration 10/25 | Loss: 0.00114392
Iteration 11/25 | Loss: 0.00114392
Iteration 12/25 | Loss: 0.00114392
Iteration 13/25 | Loss: 0.00114392
Iteration 14/25 | Loss: 0.00114392
Iteration 15/25 | Loss: 0.00114392
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011439162772148848, 0.0011439162772148848, 0.0011439162772148848, 0.0011439162772148848, 0.0011439162772148848]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011439162772148848

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114392
Iteration 2/1000 | Loss: 0.00004117
Iteration 3/1000 | Loss: 0.00003255
Iteration 4/1000 | Loss: 0.00003002
Iteration 5/1000 | Loss: 0.00002878
Iteration 6/1000 | Loss: 0.00002788
Iteration 7/1000 | Loss: 0.00002738
Iteration 8/1000 | Loss: 0.00002709
Iteration 9/1000 | Loss: 0.00002681
Iteration 10/1000 | Loss: 0.00002667
Iteration 11/1000 | Loss: 0.00002652
Iteration 12/1000 | Loss: 0.00002634
Iteration 13/1000 | Loss: 0.00002634
Iteration 14/1000 | Loss: 0.00002634
Iteration 15/1000 | Loss: 0.00002634
Iteration 16/1000 | Loss: 0.00002633
Iteration 17/1000 | Loss: 0.00002632
Iteration 18/1000 | Loss: 0.00002632
Iteration 19/1000 | Loss: 0.00002632
Iteration 20/1000 | Loss: 0.00002632
Iteration 21/1000 | Loss: 0.00002631
Iteration 22/1000 | Loss: 0.00002631
Iteration 23/1000 | Loss: 0.00002628
Iteration 24/1000 | Loss: 0.00002628
Iteration 25/1000 | Loss: 0.00002628
Iteration 26/1000 | Loss: 0.00002627
Iteration 27/1000 | Loss: 0.00002627
Iteration 28/1000 | Loss: 0.00002626
Iteration 29/1000 | Loss: 0.00002626
Iteration 30/1000 | Loss: 0.00002625
Iteration 31/1000 | Loss: 0.00002624
Iteration 32/1000 | Loss: 0.00002618
Iteration 33/1000 | Loss: 0.00002618
Iteration 34/1000 | Loss: 0.00002618
Iteration 35/1000 | Loss: 0.00002617
Iteration 36/1000 | Loss: 0.00002617
Iteration 37/1000 | Loss: 0.00002617
Iteration 38/1000 | Loss: 0.00002617
Iteration 39/1000 | Loss: 0.00002617
Iteration 40/1000 | Loss: 0.00002617
Iteration 41/1000 | Loss: 0.00002616
Iteration 42/1000 | Loss: 0.00002616
Iteration 43/1000 | Loss: 0.00002616
Iteration 44/1000 | Loss: 0.00002616
Iteration 45/1000 | Loss: 0.00002616
Iteration 46/1000 | Loss: 0.00002616
Iteration 47/1000 | Loss: 0.00002616
Iteration 48/1000 | Loss: 0.00002616
Iteration 49/1000 | Loss: 0.00002616
Iteration 50/1000 | Loss: 0.00002616
Iteration 51/1000 | Loss: 0.00002616
Iteration 52/1000 | Loss: 0.00002615
Iteration 53/1000 | Loss: 0.00002615
Iteration 54/1000 | Loss: 0.00002615
Iteration 55/1000 | Loss: 0.00002615
Iteration 56/1000 | Loss: 0.00002614
Iteration 57/1000 | Loss: 0.00002614
Iteration 58/1000 | Loss: 0.00002614
Iteration 59/1000 | Loss: 0.00002614
Iteration 60/1000 | Loss: 0.00002614
Iteration 61/1000 | Loss: 0.00002614
Iteration 62/1000 | Loss: 0.00002614
Iteration 63/1000 | Loss: 0.00002614
Iteration 64/1000 | Loss: 0.00002614
Iteration 65/1000 | Loss: 0.00002614
Iteration 66/1000 | Loss: 0.00002614
Iteration 67/1000 | Loss: 0.00002614
Iteration 68/1000 | Loss: 0.00002614
Iteration 69/1000 | Loss: 0.00002614
Iteration 70/1000 | Loss: 0.00002614
Iteration 71/1000 | Loss: 0.00002614
Iteration 72/1000 | Loss: 0.00002614
Iteration 73/1000 | Loss: 0.00002614
Iteration 74/1000 | Loss: 0.00002614
Iteration 75/1000 | Loss: 0.00002614
Iteration 76/1000 | Loss: 0.00002614
Iteration 77/1000 | Loss: 0.00002614
Iteration 78/1000 | Loss: 0.00002614
Iteration 79/1000 | Loss: 0.00002614
Iteration 80/1000 | Loss: 0.00002614
Iteration 81/1000 | Loss: 0.00002614
Iteration 82/1000 | Loss: 0.00002614
Iteration 83/1000 | Loss: 0.00002614
Iteration 84/1000 | Loss: 0.00002614
Iteration 85/1000 | Loss: 0.00002614
Iteration 86/1000 | Loss: 0.00002614
Iteration 87/1000 | Loss: 0.00002614
Iteration 88/1000 | Loss: 0.00002614
Iteration 89/1000 | Loss: 0.00002614
Iteration 90/1000 | Loss: 0.00002614
Iteration 91/1000 | Loss: 0.00002614
Iteration 92/1000 | Loss: 0.00002614
Iteration 93/1000 | Loss: 0.00002614
Iteration 94/1000 | Loss: 0.00002614
Iteration 95/1000 | Loss: 0.00002614
Iteration 96/1000 | Loss: 0.00002614
Iteration 97/1000 | Loss: 0.00002614
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [2.6140221962123178e-05, 2.6140221962123178e-05, 2.6140221962123178e-05, 2.6140221962123178e-05, 2.6140221962123178e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6140221962123178e-05

Optimization complete. Final v2v error: 4.372032642364502 mm

Highest mean error: 4.4815778732299805 mm for frame 45

Lowest mean error: 4.288038730621338 mm for frame 110

Saving results

Total time: 28.99856162071228
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carina_posed_010/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01039795
Iteration 2/25 | Loss: 0.00174645
Iteration 3/25 | Loss: 0.00152794
Iteration 4/25 | Loss: 0.00150482
Iteration 5/25 | Loss: 0.00150240
Iteration 6/25 | Loss: 0.00150250
Iteration 7/25 | Loss: 0.00150113
Iteration 8/25 | Loss: 0.00147273
Iteration 9/25 | Loss: 0.00146649
Iteration 10/25 | Loss: 0.00145076
Iteration 11/25 | Loss: 0.00144936
Iteration 12/25 | Loss: 0.00144583
Iteration 13/25 | Loss: 0.00144552
Iteration 14/25 | Loss: 0.00144535
Iteration 15/25 | Loss: 0.00144529
Iteration 16/25 | Loss: 0.00144520
Iteration 17/25 | Loss: 0.00144508
Iteration 18/25 | Loss: 0.00144499
Iteration 19/25 | Loss: 0.00144496
Iteration 20/25 | Loss: 0.00144496
Iteration 21/25 | Loss: 0.00144496
Iteration 22/25 | Loss: 0.00144495
Iteration 23/25 | Loss: 0.00144495
Iteration 24/25 | Loss: 0.00144495
Iteration 25/25 | Loss: 0.00144495

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44019616
Iteration 2/25 | Loss: 0.00174182
Iteration 3/25 | Loss: 0.00174180
Iteration 4/25 | Loss: 0.00174179
Iteration 5/25 | Loss: 0.00174179
Iteration 6/25 | Loss: 0.00174179
Iteration 7/25 | Loss: 0.00174179
Iteration 8/25 | Loss: 0.00174179
Iteration 9/25 | Loss: 0.00174179
Iteration 10/25 | Loss: 0.00174179
Iteration 11/25 | Loss: 0.00174179
Iteration 12/25 | Loss: 0.00174179
Iteration 13/25 | Loss: 0.00174179
Iteration 14/25 | Loss: 0.00174179
Iteration 15/25 | Loss: 0.00174179
Iteration 16/25 | Loss: 0.00174179
Iteration 17/25 | Loss: 0.00174179
Iteration 18/25 | Loss: 0.00174179
Iteration 19/25 | Loss: 0.00174179
Iteration 20/25 | Loss: 0.00174179
Iteration 21/25 | Loss: 0.00174179
Iteration 22/25 | Loss: 0.00174179
Iteration 23/25 | Loss: 0.00174179
Iteration 24/25 | Loss: 0.00174179
Iteration 25/25 | Loss: 0.00174179

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00174179
Iteration 2/1000 | Loss: 0.00005210
Iteration 3/1000 | Loss: 0.00003601
Iteration 4/1000 | Loss: 0.00003186
Iteration 5/1000 | Loss: 0.00002988
Iteration 6/1000 | Loss: 0.00002893
Iteration 7/1000 | Loss: 0.00002823
Iteration 8/1000 | Loss: 0.00002758
Iteration 9/1000 | Loss: 0.00002712
Iteration 10/1000 | Loss: 0.00045775
Iteration 11/1000 | Loss: 0.00003169
Iteration 12/1000 | Loss: 0.00002612
Iteration 13/1000 | Loss: 0.00002387
Iteration 14/1000 | Loss: 0.00002268
Iteration 15/1000 | Loss: 0.00002191
Iteration 16/1000 | Loss: 0.00002155
Iteration 17/1000 | Loss: 0.00002118
Iteration 18/1000 | Loss: 0.00002085
Iteration 19/1000 | Loss: 0.00002063
Iteration 20/1000 | Loss: 0.00002058
Iteration 21/1000 | Loss: 0.00002058
Iteration 22/1000 | Loss: 0.00002057
Iteration 23/1000 | Loss: 0.00002055
Iteration 24/1000 | Loss: 0.00002045
Iteration 25/1000 | Loss: 0.00002039
Iteration 26/1000 | Loss: 0.00002038
Iteration 27/1000 | Loss: 0.00002037
Iteration 28/1000 | Loss: 0.00002036
Iteration 29/1000 | Loss: 0.00002035
Iteration 30/1000 | Loss: 0.00002035
Iteration 31/1000 | Loss: 0.00002034
Iteration 32/1000 | Loss: 0.00002031
Iteration 33/1000 | Loss: 0.00002030
Iteration 34/1000 | Loss: 0.00002030
Iteration 35/1000 | Loss: 0.00002030
Iteration 36/1000 | Loss: 0.00002029
Iteration 37/1000 | Loss: 0.00002029
Iteration 38/1000 | Loss: 0.00002029
Iteration 39/1000 | Loss: 0.00002029
Iteration 40/1000 | Loss: 0.00002028
Iteration 41/1000 | Loss: 0.00002028
Iteration 42/1000 | Loss: 0.00002028
Iteration 43/1000 | Loss: 0.00002027
Iteration 44/1000 | Loss: 0.00002025
Iteration 45/1000 | Loss: 0.00002025
Iteration 46/1000 | Loss: 0.00002025
Iteration 47/1000 | Loss: 0.00002025
Iteration 48/1000 | Loss: 0.00002025
Iteration 49/1000 | Loss: 0.00002024
Iteration 50/1000 | Loss: 0.00002024
Iteration 51/1000 | Loss: 0.00002024
Iteration 52/1000 | Loss: 0.00002023
Iteration 53/1000 | Loss: 0.00002021
Iteration 54/1000 | Loss: 0.00002021
Iteration 55/1000 | Loss: 0.00002021
Iteration 56/1000 | Loss: 0.00002021
Iteration 57/1000 | Loss: 0.00002021
Iteration 58/1000 | Loss: 0.00002020
Iteration 59/1000 | Loss: 0.00002020
Iteration 60/1000 | Loss: 0.00002020
Iteration 61/1000 | Loss: 0.00002020
Iteration 62/1000 | Loss: 0.00002020
Iteration 63/1000 | Loss: 0.00002020
Iteration 64/1000 | Loss: 0.00002020
Iteration 65/1000 | Loss: 0.00002020
Iteration 66/1000 | Loss: 0.00002020
Iteration 67/1000 | Loss: 0.00002019
Iteration 68/1000 | Loss: 0.00002019
Iteration 69/1000 | Loss: 0.00002019
Iteration 70/1000 | Loss: 0.00002018
Iteration 71/1000 | Loss: 0.00002018
Iteration 72/1000 | Loss: 0.00002018
Iteration 73/1000 | Loss: 0.00002017
Iteration 74/1000 | Loss: 0.00002017
Iteration 75/1000 | Loss: 0.00002017
Iteration 76/1000 | Loss: 0.00002017
Iteration 77/1000 | Loss: 0.00002017
Iteration 78/1000 | Loss: 0.00002017
Iteration 79/1000 | Loss: 0.00002017
Iteration 80/1000 | Loss: 0.00002016
Iteration 81/1000 | Loss: 0.00002016
Iteration 82/1000 | Loss: 0.00002016
Iteration 83/1000 | Loss: 0.00002016
Iteration 84/1000 | Loss: 0.00002016
Iteration 85/1000 | Loss: 0.00002016
Iteration 86/1000 | Loss: 0.00002016
Iteration 87/1000 | Loss: 0.00002016
Iteration 88/1000 | Loss: 0.00002016
Iteration 89/1000 | Loss: 0.00002016
Iteration 90/1000 | Loss: 0.00002016
Iteration 91/1000 | Loss: 0.00002016
Iteration 92/1000 | Loss: 0.00002016
Iteration 93/1000 | Loss: 0.00002016
Iteration 94/1000 | Loss: 0.00002015
Iteration 95/1000 | Loss: 0.00002015
Iteration 96/1000 | Loss: 0.00002015
Iteration 97/1000 | Loss: 0.00002015
Iteration 98/1000 | Loss: 0.00002015
Iteration 99/1000 | Loss: 0.00002015
Iteration 100/1000 | Loss: 0.00002014
Iteration 101/1000 | Loss: 0.00002014
Iteration 102/1000 | Loss: 0.00002014
Iteration 103/1000 | Loss: 0.00002014
Iteration 104/1000 | Loss: 0.00002014
Iteration 105/1000 | Loss: 0.00002014
Iteration 106/1000 | Loss: 0.00002014
Iteration 107/1000 | Loss: 0.00002014
Iteration 108/1000 | Loss: 0.00002014
Iteration 109/1000 | Loss: 0.00002014
Iteration 110/1000 | Loss: 0.00002013
Iteration 111/1000 | Loss: 0.00002013
Iteration 112/1000 | Loss: 0.00002013
Iteration 113/1000 | Loss: 0.00002013
Iteration 114/1000 | Loss: 0.00002013
Iteration 115/1000 | Loss: 0.00002013
Iteration 116/1000 | Loss: 0.00002013
Iteration 117/1000 | Loss: 0.00002013
Iteration 118/1000 | Loss: 0.00002013
Iteration 119/1000 | Loss: 0.00002013
Iteration 120/1000 | Loss: 0.00002012
Iteration 121/1000 | Loss: 0.00002012
Iteration 122/1000 | Loss: 0.00002012
Iteration 123/1000 | Loss: 0.00002012
Iteration 124/1000 | Loss: 0.00002012
Iteration 125/1000 | Loss: 0.00002012
Iteration 126/1000 | Loss: 0.00002011
Iteration 127/1000 | Loss: 0.00002011
Iteration 128/1000 | Loss: 0.00002011
Iteration 129/1000 | Loss: 0.00002011
Iteration 130/1000 | Loss: 0.00002011
Iteration 131/1000 | Loss: 0.00002011
Iteration 132/1000 | Loss: 0.00002011
Iteration 133/1000 | Loss: 0.00002010
Iteration 134/1000 | Loss: 0.00002010
Iteration 135/1000 | Loss: 0.00002010
Iteration 136/1000 | Loss: 0.00002010
Iteration 137/1000 | Loss: 0.00002010
Iteration 138/1000 | Loss: 0.00002010
Iteration 139/1000 | Loss: 0.00002010
Iteration 140/1000 | Loss: 0.00002010
Iteration 141/1000 | Loss: 0.00002010
Iteration 142/1000 | Loss: 0.00002010
Iteration 143/1000 | Loss: 0.00002009
Iteration 144/1000 | Loss: 0.00002009
Iteration 145/1000 | Loss: 0.00002009
Iteration 146/1000 | Loss: 0.00002009
Iteration 147/1000 | Loss: 0.00002009
Iteration 148/1000 | Loss: 0.00002009
Iteration 149/1000 | Loss: 0.00002009
Iteration 150/1000 | Loss: 0.00002008
Iteration 151/1000 | Loss: 0.00002008
Iteration 152/1000 | Loss: 0.00002008
Iteration 153/1000 | Loss: 0.00002008
Iteration 154/1000 | Loss: 0.00002008
Iteration 155/1000 | Loss: 0.00002008
Iteration 156/1000 | Loss: 0.00002008
Iteration 157/1000 | Loss: 0.00002008
Iteration 158/1000 | Loss: 0.00002008
Iteration 159/1000 | Loss: 0.00002008
Iteration 160/1000 | Loss: 0.00002007
Iteration 161/1000 | Loss: 0.00002007
Iteration 162/1000 | Loss: 0.00002007
Iteration 163/1000 | Loss: 0.00002007
Iteration 164/1000 | Loss: 0.00002007
Iteration 165/1000 | Loss: 0.00002007
Iteration 166/1000 | Loss: 0.00002007
Iteration 167/1000 | Loss: 0.00002006
Iteration 168/1000 | Loss: 0.00002006
Iteration 169/1000 | Loss: 0.00002006
Iteration 170/1000 | Loss: 0.00002006
Iteration 171/1000 | Loss: 0.00002006
Iteration 172/1000 | Loss: 0.00002006
Iteration 173/1000 | Loss: 0.00002006
Iteration 174/1000 | Loss: 0.00002006
Iteration 175/1000 | Loss: 0.00002006
Iteration 176/1000 | Loss: 0.00002006
Iteration 177/1000 | Loss: 0.00002006
Iteration 178/1000 | Loss: 0.00002006
Iteration 179/1000 | Loss: 0.00002006
Iteration 180/1000 | Loss: 0.00002006
Iteration 181/1000 | Loss: 0.00002006
Iteration 182/1000 | Loss: 0.00002006
Iteration 183/1000 | Loss: 0.00002006
Iteration 184/1000 | Loss: 0.00002005
Iteration 185/1000 | Loss: 0.00002005
Iteration 186/1000 | Loss: 0.00002005
Iteration 187/1000 | Loss: 0.00002005
Iteration 188/1000 | Loss: 0.00002005
Iteration 189/1000 | Loss: 0.00002005
Iteration 190/1000 | Loss: 0.00002005
Iteration 191/1000 | Loss: 0.00002005
Iteration 192/1000 | Loss: 0.00002005
Iteration 193/1000 | Loss: 0.00002005
Iteration 194/1000 | Loss: 0.00002004
Iteration 195/1000 | Loss: 0.00002004
Iteration 196/1000 | Loss: 0.00002004
Iteration 197/1000 | Loss: 0.00002004
Iteration 198/1000 | Loss: 0.00002004
Iteration 199/1000 | Loss: 0.00002004
Iteration 200/1000 | Loss: 0.00002004
Iteration 201/1000 | Loss: 0.00002004
Iteration 202/1000 | Loss: 0.00002004
Iteration 203/1000 | Loss: 0.00002004
Iteration 204/1000 | Loss: 0.00002004
Iteration 205/1000 | Loss: 0.00002004
Iteration 206/1000 | Loss: 0.00002004
Iteration 207/1000 | Loss: 0.00002004
Iteration 208/1000 | Loss: 0.00002004
Iteration 209/1000 | Loss: 0.00002004
Iteration 210/1000 | Loss: 0.00002004
Iteration 211/1000 | Loss: 0.00002004
Iteration 212/1000 | Loss: 0.00002003
Iteration 213/1000 | Loss: 0.00002003
Iteration 214/1000 | Loss: 0.00002003
Iteration 215/1000 | Loss: 0.00002003
Iteration 216/1000 | Loss: 0.00002003
Iteration 217/1000 | Loss: 0.00002003
Iteration 218/1000 | Loss: 0.00002003
Iteration 219/1000 | Loss: 0.00002003
Iteration 220/1000 | Loss: 0.00002003
Iteration 221/1000 | Loss: 0.00002003
Iteration 222/1000 | Loss: 0.00002003
Iteration 223/1000 | Loss: 0.00002003
Iteration 224/1000 | Loss: 0.00002003
Iteration 225/1000 | Loss: 0.00002003
Iteration 226/1000 | Loss: 0.00002003
Iteration 227/1000 | Loss: 0.00002003
Iteration 228/1000 | Loss: 0.00002003
Iteration 229/1000 | Loss: 0.00002003
Iteration 230/1000 | Loss: 0.00002003
Iteration 231/1000 | Loss: 0.00002003
Iteration 232/1000 | Loss: 0.00002003
Iteration 233/1000 | Loss: 0.00002003
Iteration 234/1000 | Loss: 0.00002003
Iteration 235/1000 | Loss: 0.00002003
Iteration 236/1000 | Loss: 0.00002003
Iteration 237/1000 | Loss: 0.00002003
Iteration 238/1000 | Loss: 0.00002003
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 238. Stopping optimization.
Last 5 losses: [2.002583642024547e-05, 2.002583642024547e-05, 2.002583642024547e-05, 2.002583642024547e-05, 2.002583642024547e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.002583642024547e-05

Optimization complete. Final v2v error: 3.7269327640533447 mm

Highest mean error: 4.031297206878662 mm for frame 12

Lowest mean error: 3.446035623550415 mm for frame 52

Saving results

Total time: 72.49845004081726
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carina_posed_010/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00997268
Iteration 2/25 | Loss: 0.00326724
Iteration 3/25 | Loss: 0.00272374
Iteration 4/25 | Loss: 0.00232391
Iteration 5/25 | Loss: 0.00217426
Iteration 6/25 | Loss: 0.00204862
Iteration 7/25 | Loss: 0.00192751
Iteration 8/25 | Loss: 0.00188345
Iteration 9/25 | Loss: 0.00181231
Iteration 10/25 | Loss: 0.00178393
Iteration 11/25 | Loss: 0.00178466
Iteration 12/25 | Loss: 0.00175589
Iteration 13/25 | Loss: 0.00171679
Iteration 14/25 | Loss: 0.00170529
Iteration 15/25 | Loss: 0.00169954
Iteration 16/25 | Loss: 0.00169652
Iteration 17/25 | Loss: 0.00168188
Iteration 18/25 | Loss: 0.00167780
Iteration 19/25 | Loss: 0.00167757
Iteration 20/25 | Loss: 0.00167210
Iteration 21/25 | Loss: 0.00167466
Iteration 22/25 | Loss: 0.00166928
Iteration 23/25 | Loss: 0.00166902
Iteration 24/25 | Loss: 0.00166891
Iteration 25/25 | Loss: 0.00166878

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24478853
Iteration 2/25 | Loss: 0.00653949
Iteration 3/25 | Loss: 0.00513056
Iteration 4/25 | Loss: 0.00513056
Iteration 5/25 | Loss: 0.00513056
Iteration 6/25 | Loss: 0.00513056
Iteration 7/25 | Loss: 0.00513055
Iteration 8/25 | Loss: 0.00513055
Iteration 9/25 | Loss: 0.00513055
Iteration 10/25 | Loss: 0.00513055
Iteration 11/25 | Loss: 0.00513055
Iteration 12/25 | Loss: 0.00513055
Iteration 13/25 | Loss: 0.00513055
Iteration 14/25 | Loss: 0.00513055
Iteration 15/25 | Loss: 0.00513055
Iteration 16/25 | Loss: 0.00513055
Iteration 17/25 | Loss: 0.00513055
Iteration 18/25 | Loss: 0.00513055
Iteration 19/25 | Loss: 0.00513055
Iteration 20/25 | Loss: 0.00513055
Iteration 21/25 | Loss: 0.00513055
Iteration 22/25 | Loss: 0.00513055
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.005130551755428314, 0.005130551755428314, 0.005130551755428314, 0.005130551755428314, 0.005130551755428314]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005130551755428314

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00513055
Iteration 2/1000 | Loss: 0.00185676
Iteration 3/1000 | Loss: 0.00313663
Iteration 4/1000 | Loss: 0.00032773
Iteration 5/1000 | Loss: 0.00028863
Iteration 6/1000 | Loss: 0.00025865
Iteration 7/1000 | Loss: 0.00024572
Iteration 8/1000 | Loss: 0.00120616
Iteration 9/1000 | Loss: 0.00160257
Iteration 10/1000 | Loss: 0.00024879
Iteration 11/1000 | Loss: 0.00023268
Iteration 12/1000 | Loss: 0.00022513
Iteration 13/1000 | Loss: 0.00021994
Iteration 14/1000 | Loss: 0.00032923
Iteration 15/1000 | Loss: 0.00021326
Iteration 16/1000 | Loss: 0.00037109
Iteration 17/1000 | Loss: 0.00032415
Iteration 18/1000 | Loss: 0.00020737
Iteration 19/1000 | Loss: 0.00054292
Iteration 20/1000 | Loss: 0.00115039
Iteration 21/1000 | Loss: 0.00229691
Iteration 22/1000 | Loss: 0.00067476
Iteration 23/1000 | Loss: 0.00068858
Iteration 24/1000 | Loss: 0.00069269
Iteration 25/1000 | Loss: 0.00020690
Iteration 26/1000 | Loss: 0.00020214
Iteration 27/1000 | Loss: 0.00020071
Iteration 28/1000 | Loss: 0.00029613
Iteration 29/1000 | Loss: 0.00087378
Iteration 30/1000 | Loss: 0.00019892
Iteration 31/1000 | Loss: 0.00163635
Iteration 32/1000 | Loss: 0.00074502
Iteration 33/1000 | Loss: 0.00271160
Iteration 34/1000 | Loss: 0.00369681
Iteration 35/1000 | Loss: 0.00243071
Iteration 36/1000 | Loss: 0.00123958
Iteration 37/1000 | Loss: 0.00051136
Iteration 38/1000 | Loss: 0.00101131
Iteration 39/1000 | Loss: 0.00051637
Iteration 40/1000 | Loss: 0.00040064
Iteration 41/1000 | Loss: 0.00054430
Iteration 42/1000 | Loss: 0.00038928
Iteration 43/1000 | Loss: 0.00034496
Iteration 44/1000 | Loss: 0.00086869
Iteration 45/1000 | Loss: 0.00043398
Iteration 46/1000 | Loss: 0.00034546
Iteration 47/1000 | Loss: 0.00030139
Iteration 48/1000 | Loss: 0.00033180
Iteration 49/1000 | Loss: 0.00020378
Iteration 50/1000 | Loss: 0.00034790
Iteration 51/1000 | Loss: 0.00018578
Iteration 52/1000 | Loss: 0.00018319
Iteration 53/1000 | Loss: 0.00021751
Iteration 54/1000 | Loss: 0.00053247
Iteration 55/1000 | Loss: 0.00019438
Iteration 56/1000 | Loss: 0.00023791
Iteration 57/1000 | Loss: 0.00017403
Iteration 58/1000 | Loss: 0.00046374
Iteration 59/1000 | Loss: 0.00017765
Iteration 60/1000 | Loss: 0.00038752
Iteration 61/1000 | Loss: 0.00128162
Iteration 62/1000 | Loss: 0.00026816
Iteration 63/1000 | Loss: 0.00034473
Iteration 64/1000 | Loss: 0.00021180
Iteration 65/1000 | Loss: 0.00028113
Iteration 66/1000 | Loss: 0.00039138
Iteration 67/1000 | Loss: 0.00035047
Iteration 68/1000 | Loss: 0.00024087
Iteration 69/1000 | Loss: 0.00044017
Iteration 70/1000 | Loss: 0.00040334
Iteration 71/1000 | Loss: 0.00040776
Iteration 72/1000 | Loss: 0.00025057
Iteration 73/1000 | Loss: 0.00017174
Iteration 74/1000 | Loss: 0.00017767
Iteration 75/1000 | Loss: 0.00034603
Iteration 76/1000 | Loss: 0.00016584
Iteration 77/1000 | Loss: 0.00016090
Iteration 78/1000 | Loss: 0.00048588
Iteration 79/1000 | Loss: 0.00032069
Iteration 80/1000 | Loss: 0.00039793
Iteration 81/1000 | Loss: 0.00029173
Iteration 82/1000 | Loss: 0.00037055
Iteration 83/1000 | Loss: 0.00039106
Iteration 84/1000 | Loss: 0.00026410
Iteration 85/1000 | Loss: 0.00029287
Iteration 86/1000 | Loss: 0.00040831
Iteration 87/1000 | Loss: 0.00028342
Iteration 88/1000 | Loss: 0.00022447
Iteration 89/1000 | Loss: 0.00018017
Iteration 90/1000 | Loss: 0.00017156
Iteration 91/1000 | Loss: 0.00026232
Iteration 92/1000 | Loss: 0.00015450
Iteration 93/1000 | Loss: 0.00015253
Iteration 94/1000 | Loss: 0.00018263
Iteration 95/1000 | Loss: 0.00014986
Iteration 96/1000 | Loss: 0.00014819
Iteration 97/1000 | Loss: 0.00026434
Iteration 98/1000 | Loss: 0.00014754
Iteration 99/1000 | Loss: 0.00014599
Iteration 100/1000 | Loss: 0.00014513
Iteration 101/1000 | Loss: 0.00014446
Iteration 102/1000 | Loss: 0.00014406
Iteration 103/1000 | Loss: 0.00014371
Iteration 104/1000 | Loss: 0.00014340
Iteration 105/1000 | Loss: 0.00014322
Iteration 106/1000 | Loss: 0.00014315
Iteration 107/1000 | Loss: 0.00014312
Iteration 108/1000 | Loss: 0.00014311
Iteration 109/1000 | Loss: 0.00014311
Iteration 110/1000 | Loss: 0.00014310
Iteration 111/1000 | Loss: 0.00014309
Iteration 112/1000 | Loss: 0.00014309
Iteration 113/1000 | Loss: 0.00014308
Iteration 114/1000 | Loss: 0.00014308
Iteration 115/1000 | Loss: 0.00014308
Iteration 116/1000 | Loss: 0.00014307
Iteration 117/1000 | Loss: 0.00014307
Iteration 118/1000 | Loss: 0.00014305
Iteration 119/1000 | Loss: 0.00014304
Iteration 120/1000 | Loss: 0.00014300
Iteration 121/1000 | Loss: 0.00014300
Iteration 122/1000 | Loss: 0.00014300
Iteration 123/1000 | Loss: 0.00014299
Iteration 124/1000 | Loss: 0.00014298
Iteration 125/1000 | Loss: 0.00014298
Iteration 126/1000 | Loss: 0.00014298
Iteration 127/1000 | Loss: 0.00014298
Iteration 128/1000 | Loss: 0.00014297
Iteration 129/1000 | Loss: 0.00014296
Iteration 130/1000 | Loss: 0.00014295
Iteration 131/1000 | Loss: 0.00014295
Iteration 132/1000 | Loss: 0.00014295
Iteration 133/1000 | Loss: 0.00014294
Iteration 134/1000 | Loss: 0.00014294
Iteration 135/1000 | Loss: 0.00014294
Iteration 136/1000 | Loss: 0.00014293
Iteration 137/1000 | Loss: 0.00014292
Iteration 138/1000 | Loss: 0.00014292
Iteration 139/1000 | Loss: 0.00014290
Iteration 140/1000 | Loss: 0.00014290
Iteration 141/1000 | Loss: 0.00014290
Iteration 142/1000 | Loss: 0.00014290
Iteration 143/1000 | Loss: 0.00014290
Iteration 144/1000 | Loss: 0.00014290
Iteration 145/1000 | Loss: 0.00014289
Iteration 146/1000 | Loss: 0.00014289
Iteration 147/1000 | Loss: 0.00014289
Iteration 148/1000 | Loss: 0.00014289
Iteration 149/1000 | Loss: 0.00014288
Iteration 150/1000 | Loss: 0.00014288
Iteration 151/1000 | Loss: 0.00014288
Iteration 152/1000 | Loss: 0.00014288
Iteration 153/1000 | Loss: 0.00014287
Iteration 154/1000 | Loss: 0.00014287
Iteration 155/1000 | Loss: 0.00014287
Iteration 156/1000 | Loss: 0.00014287
Iteration 157/1000 | Loss: 0.00014287
Iteration 158/1000 | Loss: 0.00014287
Iteration 159/1000 | Loss: 0.00014287
Iteration 160/1000 | Loss: 0.00014287
Iteration 161/1000 | Loss: 0.00014287
Iteration 162/1000 | Loss: 0.00014287
Iteration 163/1000 | Loss: 0.00014287
Iteration 164/1000 | Loss: 0.00014287
Iteration 165/1000 | Loss: 0.00014287
Iteration 166/1000 | Loss: 0.00014286
Iteration 167/1000 | Loss: 0.00014286
Iteration 168/1000 | Loss: 0.00014286
Iteration 169/1000 | Loss: 0.00014285
Iteration 170/1000 | Loss: 0.00014285
Iteration 171/1000 | Loss: 0.00014285
Iteration 172/1000 | Loss: 0.00014285
Iteration 173/1000 | Loss: 0.00027294
Iteration 174/1000 | Loss: 0.00014303
Iteration 175/1000 | Loss: 0.00014285
Iteration 176/1000 | Loss: 0.00014283
Iteration 177/1000 | Loss: 0.00014283
Iteration 178/1000 | Loss: 0.00014283
Iteration 179/1000 | Loss: 0.00014283
Iteration 180/1000 | Loss: 0.00014283
Iteration 181/1000 | Loss: 0.00014283
Iteration 182/1000 | Loss: 0.00014283
Iteration 183/1000 | Loss: 0.00014283
Iteration 184/1000 | Loss: 0.00014283
Iteration 185/1000 | Loss: 0.00014282
Iteration 186/1000 | Loss: 0.00014282
Iteration 187/1000 | Loss: 0.00014282
Iteration 188/1000 | Loss: 0.00014282
Iteration 189/1000 | Loss: 0.00014282
Iteration 190/1000 | Loss: 0.00014281
Iteration 191/1000 | Loss: 0.00014281
Iteration 192/1000 | Loss: 0.00014281
Iteration 193/1000 | Loss: 0.00014281
Iteration 194/1000 | Loss: 0.00014281
Iteration 195/1000 | Loss: 0.00014281
Iteration 196/1000 | Loss: 0.00014280
Iteration 197/1000 | Loss: 0.00014280
Iteration 198/1000 | Loss: 0.00014280
Iteration 199/1000 | Loss: 0.00014280
Iteration 200/1000 | Loss: 0.00014280
Iteration 201/1000 | Loss: 0.00014280
Iteration 202/1000 | Loss: 0.00014280
Iteration 203/1000 | Loss: 0.00014280
Iteration 204/1000 | Loss: 0.00014280
Iteration 205/1000 | Loss: 0.00014279
Iteration 206/1000 | Loss: 0.00014279
Iteration 207/1000 | Loss: 0.00014279
Iteration 208/1000 | Loss: 0.00014279
Iteration 209/1000 | Loss: 0.00014279
Iteration 210/1000 | Loss: 0.00014279
Iteration 211/1000 | Loss: 0.00014279
Iteration 212/1000 | Loss: 0.00014279
Iteration 213/1000 | Loss: 0.00014279
Iteration 214/1000 | Loss: 0.00014279
Iteration 215/1000 | Loss: 0.00014279
Iteration 216/1000 | Loss: 0.00014279
Iteration 217/1000 | Loss: 0.00014279
Iteration 218/1000 | Loss: 0.00014279
Iteration 219/1000 | Loss: 0.00014278
Iteration 220/1000 | Loss: 0.00014278
Iteration 221/1000 | Loss: 0.00014278
Iteration 222/1000 | Loss: 0.00014278
Iteration 223/1000 | Loss: 0.00014278
Iteration 224/1000 | Loss: 0.00014278
Iteration 225/1000 | Loss: 0.00014278
Iteration 226/1000 | Loss: 0.00014278
Iteration 227/1000 | Loss: 0.00014278
Iteration 228/1000 | Loss: 0.00014278
Iteration 229/1000 | Loss: 0.00014278
Iteration 230/1000 | Loss: 0.00014278
Iteration 231/1000 | Loss: 0.00014278
Iteration 232/1000 | Loss: 0.00014278
Iteration 233/1000 | Loss: 0.00014278
Iteration 234/1000 | Loss: 0.00014277
Iteration 235/1000 | Loss: 0.00014277
Iteration 236/1000 | Loss: 0.00014277
Iteration 237/1000 | Loss: 0.00014277
Iteration 238/1000 | Loss: 0.00014277
Iteration 239/1000 | Loss: 0.00014277
Iteration 240/1000 | Loss: 0.00014277
Iteration 241/1000 | Loss: 0.00014277
Iteration 242/1000 | Loss: 0.00014277
Iteration 243/1000 | Loss: 0.00014277
Iteration 244/1000 | Loss: 0.00014277
Iteration 245/1000 | Loss: 0.00014276
Iteration 246/1000 | Loss: 0.00014276
Iteration 247/1000 | Loss: 0.00014276
Iteration 248/1000 | Loss: 0.00014276
Iteration 249/1000 | Loss: 0.00014276
Iteration 250/1000 | Loss: 0.00014276
Iteration 251/1000 | Loss: 0.00014276
Iteration 252/1000 | Loss: 0.00014276
Iteration 253/1000 | Loss: 0.00014276
Iteration 254/1000 | Loss: 0.00014276
Iteration 255/1000 | Loss: 0.00014276
Iteration 256/1000 | Loss: 0.00014276
Iteration 257/1000 | Loss: 0.00014276
Iteration 258/1000 | Loss: 0.00014276
Iteration 259/1000 | Loss: 0.00014276
Iteration 260/1000 | Loss: 0.00014276
Iteration 261/1000 | Loss: 0.00014276
Iteration 262/1000 | Loss: 0.00014276
Iteration 263/1000 | Loss: 0.00014276
Iteration 264/1000 | Loss: 0.00014276
Iteration 265/1000 | Loss: 0.00014276
Iteration 266/1000 | Loss: 0.00014276
Iteration 267/1000 | Loss: 0.00014276
Iteration 268/1000 | Loss: 0.00014276
Iteration 269/1000 | Loss: 0.00014276
Iteration 270/1000 | Loss: 0.00014276
Iteration 271/1000 | Loss: 0.00014276
Iteration 272/1000 | Loss: 0.00014276
Iteration 273/1000 | Loss: 0.00014276
Iteration 274/1000 | Loss: 0.00014276
Iteration 275/1000 | Loss: 0.00014276
Iteration 276/1000 | Loss: 0.00014276
Iteration 277/1000 | Loss: 0.00014276
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 277. Stopping optimization.
Last 5 losses: [0.00014275559806264937, 0.00014275559806264937, 0.00014275559806264937, 0.00014275559806264937, 0.00014275559806264937]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00014275559806264937

Optimization complete. Final v2v error: 6.413909435272217 mm

Highest mean error: 11.33544635772705 mm for frame 44

Lowest mean error: 3.80759596824646 mm for frame 150

Saving results

Total time: 207.77905440330505
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carina_posed_010/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00471730
Iteration 2/25 | Loss: 0.00144979
Iteration 3/25 | Loss: 0.00137010
Iteration 4/25 | Loss: 0.00136059
Iteration 5/25 | Loss: 0.00135915
Iteration 6/25 | Loss: 0.00135915
Iteration 7/25 | Loss: 0.00135915
Iteration 8/25 | Loss: 0.00135915
Iteration 9/25 | Loss: 0.00135915
Iteration 10/25 | Loss: 0.00135915
Iteration 11/25 | Loss: 0.00135915
Iteration 12/25 | Loss: 0.00135915
Iteration 13/25 | Loss: 0.00135915
Iteration 14/25 | Loss: 0.00135915
Iteration 15/25 | Loss: 0.00135915
Iteration 16/25 | Loss: 0.00135915
Iteration 17/25 | Loss: 0.00135915
Iteration 18/25 | Loss: 0.00135915
Iteration 19/25 | Loss: 0.00135915
Iteration 20/25 | Loss: 0.00135915
Iteration 21/25 | Loss: 0.00135915
Iteration 22/25 | Loss: 0.00135915
Iteration 23/25 | Loss: 0.00135915
Iteration 24/25 | Loss: 0.00135915
Iteration 25/25 | Loss: 0.00135915

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.74424839
Iteration 2/25 | Loss: 0.00164152
Iteration 3/25 | Loss: 0.00164151
Iteration 4/25 | Loss: 0.00164151
Iteration 5/25 | Loss: 0.00164151
Iteration 6/25 | Loss: 0.00164151
Iteration 7/25 | Loss: 0.00164151
Iteration 8/25 | Loss: 0.00164151
Iteration 9/25 | Loss: 0.00164151
Iteration 10/25 | Loss: 0.00164151
Iteration 11/25 | Loss: 0.00164151
Iteration 12/25 | Loss: 0.00164151
Iteration 13/25 | Loss: 0.00164151
Iteration 14/25 | Loss: 0.00164151
Iteration 15/25 | Loss: 0.00164151
Iteration 16/25 | Loss: 0.00164151
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0016415119171142578, 0.0016415119171142578, 0.0016415119171142578, 0.0016415119171142578, 0.0016415119171142578]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016415119171142578

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00164151
Iteration 2/1000 | Loss: 0.00002551
Iteration 3/1000 | Loss: 0.00002010
Iteration 4/1000 | Loss: 0.00001863
Iteration 5/1000 | Loss: 0.00001745
Iteration 6/1000 | Loss: 0.00001675
Iteration 7/1000 | Loss: 0.00001631
Iteration 8/1000 | Loss: 0.00001576
Iteration 9/1000 | Loss: 0.00001532
Iteration 10/1000 | Loss: 0.00001500
Iteration 11/1000 | Loss: 0.00001483
Iteration 12/1000 | Loss: 0.00001460
Iteration 13/1000 | Loss: 0.00001449
Iteration 14/1000 | Loss: 0.00001447
Iteration 15/1000 | Loss: 0.00001422
Iteration 16/1000 | Loss: 0.00001405
Iteration 17/1000 | Loss: 0.00001399
Iteration 18/1000 | Loss: 0.00001395
Iteration 19/1000 | Loss: 0.00001386
Iteration 20/1000 | Loss: 0.00001386
Iteration 21/1000 | Loss: 0.00001384
Iteration 22/1000 | Loss: 0.00001383
Iteration 23/1000 | Loss: 0.00001378
Iteration 24/1000 | Loss: 0.00001373
Iteration 25/1000 | Loss: 0.00001371
Iteration 26/1000 | Loss: 0.00001370
Iteration 27/1000 | Loss: 0.00001369
Iteration 28/1000 | Loss: 0.00001362
Iteration 29/1000 | Loss: 0.00001362
Iteration 30/1000 | Loss: 0.00001362
Iteration 31/1000 | Loss: 0.00001362
Iteration 32/1000 | Loss: 0.00001361
Iteration 33/1000 | Loss: 0.00001359
Iteration 34/1000 | Loss: 0.00001359
Iteration 35/1000 | Loss: 0.00001358
Iteration 36/1000 | Loss: 0.00001356
Iteration 37/1000 | Loss: 0.00001356
Iteration 38/1000 | Loss: 0.00001355
Iteration 39/1000 | Loss: 0.00001355
Iteration 40/1000 | Loss: 0.00001354
Iteration 41/1000 | Loss: 0.00001354
Iteration 42/1000 | Loss: 0.00001354
Iteration 43/1000 | Loss: 0.00001353
Iteration 44/1000 | Loss: 0.00001353
Iteration 45/1000 | Loss: 0.00001353
Iteration 46/1000 | Loss: 0.00001351
Iteration 47/1000 | Loss: 0.00001351
Iteration 48/1000 | Loss: 0.00001351
Iteration 49/1000 | Loss: 0.00001351
Iteration 50/1000 | Loss: 0.00001351
Iteration 51/1000 | Loss: 0.00001351
Iteration 52/1000 | Loss: 0.00001351
Iteration 53/1000 | Loss: 0.00001350
Iteration 54/1000 | Loss: 0.00001350
Iteration 55/1000 | Loss: 0.00001350
Iteration 56/1000 | Loss: 0.00001349
Iteration 57/1000 | Loss: 0.00001349
Iteration 58/1000 | Loss: 0.00001349
Iteration 59/1000 | Loss: 0.00001348
Iteration 60/1000 | Loss: 0.00001348
Iteration 61/1000 | Loss: 0.00001348
Iteration 62/1000 | Loss: 0.00001348
Iteration 63/1000 | Loss: 0.00001348
Iteration 64/1000 | Loss: 0.00001348
Iteration 65/1000 | Loss: 0.00001348
Iteration 66/1000 | Loss: 0.00001347
Iteration 67/1000 | Loss: 0.00001347
Iteration 68/1000 | Loss: 0.00001347
Iteration 69/1000 | Loss: 0.00001347
Iteration 70/1000 | Loss: 0.00001346
Iteration 71/1000 | Loss: 0.00001346
Iteration 72/1000 | Loss: 0.00001346
Iteration 73/1000 | Loss: 0.00001346
Iteration 74/1000 | Loss: 0.00001346
Iteration 75/1000 | Loss: 0.00001346
Iteration 76/1000 | Loss: 0.00001346
Iteration 77/1000 | Loss: 0.00001346
Iteration 78/1000 | Loss: 0.00001346
Iteration 79/1000 | Loss: 0.00001346
Iteration 80/1000 | Loss: 0.00001346
Iteration 81/1000 | Loss: 0.00001345
Iteration 82/1000 | Loss: 0.00001345
Iteration 83/1000 | Loss: 0.00001345
Iteration 84/1000 | Loss: 0.00001345
Iteration 85/1000 | Loss: 0.00001345
Iteration 86/1000 | Loss: 0.00001345
Iteration 87/1000 | Loss: 0.00001345
Iteration 88/1000 | Loss: 0.00001345
Iteration 89/1000 | Loss: 0.00001345
Iteration 90/1000 | Loss: 0.00001345
Iteration 91/1000 | Loss: 0.00001344
Iteration 92/1000 | Loss: 0.00001344
Iteration 93/1000 | Loss: 0.00001344
Iteration 94/1000 | Loss: 0.00001344
Iteration 95/1000 | Loss: 0.00001344
Iteration 96/1000 | Loss: 0.00001344
Iteration 97/1000 | Loss: 0.00001344
Iteration 98/1000 | Loss: 0.00001344
Iteration 99/1000 | Loss: 0.00001344
Iteration 100/1000 | Loss: 0.00001344
Iteration 101/1000 | Loss: 0.00001344
Iteration 102/1000 | Loss: 0.00001344
Iteration 103/1000 | Loss: 0.00001344
Iteration 104/1000 | Loss: 0.00001344
Iteration 105/1000 | Loss: 0.00001344
Iteration 106/1000 | Loss: 0.00001344
Iteration 107/1000 | Loss: 0.00001344
Iteration 108/1000 | Loss: 0.00001344
Iteration 109/1000 | Loss: 0.00001344
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [1.3435073924483731e-05, 1.3435073924483731e-05, 1.3435073924483731e-05, 1.3435073924483731e-05, 1.3435073924483731e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3435073924483731e-05

Optimization complete. Final v2v error: 3.1651406288146973 mm

Highest mean error: 3.3425986766815186 mm for frame 246

Lowest mean error: 2.9856019020080566 mm for frame 207

Saving results

Total time: 44.06774282455444
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carina_posed_010/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00353136
Iteration 2/25 | Loss: 0.00145832
Iteration 3/25 | Loss: 0.00135763
Iteration 4/25 | Loss: 0.00133835
Iteration 5/25 | Loss: 0.00133158
Iteration 6/25 | Loss: 0.00133093
Iteration 7/25 | Loss: 0.00133093
Iteration 8/25 | Loss: 0.00133093
Iteration 9/25 | Loss: 0.00133093
Iteration 10/25 | Loss: 0.00133093
Iteration 11/25 | Loss: 0.00133093
Iteration 12/25 | Loss: 0.00133093
Iteration 13/25 | Loss: 0.00133093
Iteration 14/25 | Loss: 0.00133093
Iteration 15/25 | Loss: 0.00133093
Iteration 16/25 | Loss: 0.00133093
Iteration 17/25 | Loss: 0.00133093
Iteration 18/25 | Loss: 0.00133093
Iteration 19/25 | Loss: 0.00133093
Iteration 20/25 | Loss: 0.00133093
Iteration 21/25 | Loss: 0.00133093
Iteration 22/25 | Loss: 0.00133093
Iteration 23/25 | Loss: 0.00133093
Iteration 24/25 | Loss: 0.00133093
Iteration 25/25 | Loss: 0.00133093

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20928800
Iteration 2/25 | Loss: 0.00250814
Iteration 3/25 | Loss: 0.00250814
Iteration 4/25 | Loss: 0.00250814
Iteration 5/25 | Loss: 0.00250814
Iteration 6/25 | Loss: 0.00250814
Iteration 7/25 | Loss: 0.00250814
Iteration 8/25 | Loss: 0.00250814
Iteration 9/25 | Loss: 0.00250814
Iteration 10/25 | Loss: 0.00250814
Iteration 11/25 | Loss: 0.00250814
Iteration 12/25 | Loss: 0.00250814
Iteration 13/25 | Loss: 0.00250814
Iteration 14/25 | Loss: 0.00250814
Iteration 15/25 | Loss: 0.00250814
Iteration 16/25 | Loss: 0.00250814
Iteration 17/25 | Loss: 0.00250814
Iteration 18/25 | Loss: 0.00250814
Iteration 19/25 | Loss: 0.00250814
Iteration 20/25 | Loss: 0.00250814
Iteration 21/25 | Loss: 0.00250814
Iteration 22/25 | Loss: 0.00250814
Iteration 23/25 | Loss: 0.00250814
Iteration 24/25 | Loss: 0.00250814
Iteration 25/25 | Loss: 0.00250814

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00250814
Iteration 2/1000 | Loss: 0.00004982
Iteration 3/1000 | Loss: 0.00003557
Iteration 4/1000 | Loss: 0.00002855
Iteration 5/1000 | Loss: 0.00002589
Iteration 6/1000 | Loss: 0.00002381
Iteration 7/1000 | Loss: 0.00002229
Iteration 8/1000 | Loss: 0.00002143
Iteration 9/1000 | Loss: 0.00002079
Iteration 10/1000 | Loss: 0.00002035
Iteration 11/1000 | Loss: 0.00001989
Iteration 12/1000 | Loss: 0.00001957
Iteration 13/1000 | Loss: 0.00001936
Iteration 14/1000 | Loss: 0.00001934
Iteration 15/1000 | Loss: 0.00001927
Iteration 16/1000 | Loss: 0.00001906
Iteration 17/1000 | Loss: 0.00001889
Iteration 18/1000 | Loss: 0.00001888
Iteration 19/1000 | Loss: 0.00001877
Iteration 20/1000 | Loss: 0.00001870
Iteration 21/1000 | Loss: 0.00001868
Iteration 22/1000 | Loss: 0.00001866
Iteration 23/1000 | Loss: 0.00001865
Iteration 24/1000 | Loss: 0.00001865
Iteration 25/1000 | Loss: 0.00001863
Iteration 26/1000 | Loss: 0.00001863
Iteration 27/1000 | Loss: 0.00001863
Iteration 28/1000 | Loss: 0.00001858
Iteration 29/1000 | Loss: 0.00001858
Iteration 30/1000 | Loss: 0.00001855
Iteration 31/1000 | Loss: 0.00001854
Iteration 32/1000 | Loss: 0.00001854
Iteration 33/1000 | Loss: 0.00001852
Iteration 34/1000 | Loss: 0.00001852
Iteration 35/1000 | Loss: 0.00001851
Iteration 36/1000 | Loss: 0.00001850
Iteration 37/1000 | Loss: 0.00001849
Iteration 38/1000 | Loss: 0.00001849
Iteration 39/1000 | Loss: 0.00001848
Iteration 40/1000 | Loss: 0.00001847
Iteration 41/1000 | Loss: 0.00001847
Iteration 42/1000 | Loss: 0.00001846
Iteration 43/1000 | Loss: 0.00001846
Iteration 44/1000 | Loss: 0.00001845
Iteration 45/1000 | Loss: 0.00001845
Iteration 46/1000 | Loss: 0.00001844
Iteration 47/1000 | Loss: 0.00001844
Iteration 48/1000 | Loss: 0.00001843
Iteration 49/1000 | Loss: 0.00001842
Iteration 50/1000 | Loss: 0.00001842
Iteration 51/1000 | Loss: 0.00001841
Iteration 52/1000 | Loss: 0.00001840
Iteration 53/1000 | Loss: 0.00001840
Iteration 54/1000 | Loss: 0.00001840
Iteration 55/1000 | Loss: 0.00001839
Iteration 56/1000 | Loss: 0.00001838
Iteration 57/1000 | Loss: 0.00001837
Iteration 58/1000 | Loss: 0.00001836
Iteration 59/1000 | Loss: 0.00001836
Iteration 60/1000 | Loss: 0.00001835
Iteration 61/1000 | Loss: 0.00001835
Iteration 62/1000 | Loss: 0.00001834
Iteration 63/1000 | Loss: 0.00001834
Iteration 64/1000 | Loss: 0.00001834
Iteration 65/1000 | Loss: 0.00001834
Iteration 66/1000 | Loss: 0.00001834
Iteration 67/1000 | Loss: 0.00001833
Iteration 68/1000 | Loss: 0.00001833
Iteration 69/1000 | Loss: 0.00001833
Iteration 70/1000 | Loss: 0.00001833
Iteration 71/1000 | Loss: 0.00001833
Iteration 72/1000 | Loss: 0.00001833
Iteration 73/1000 | Loss: 0.00001833
Iteration 74/1000 | Loss: 0.00001832
Iteration 75/1000 | Loss: 0.00001832
Iteration 76/1000 | Loss: 0.00001832
Iteration 77/1000 | Loss: 0.00001831
Iteration 78/1000 | Loss: 0.00001831
Iteration 79/1000 | Loss: 0.00001831
Iteration 80/1000 | Loss: 0.00001831
Iteration 81/1000 | Loss: 0.00001831
Iteration 82/1000 | Loss: 0.00001831
Iteration 83/1000 | Loss: 0.00001831
Iteration 84/1000 | Loss: 0.00001831
Iteration 85/1000 | Loss: 0.00001831
Iteration 86/1000 | Loss: 0.00001830
Iteration 87/1000 | Loss: 0.00001830
Iteration 88/1000 | Loss: 0.00001830
Iteration 89/1000 | Loss: 0.00001830
Iteration 90/1000 | Loss: 0.00001830
Iteration 91/1000 | Loss: 0.00001830
Iteration 92/1000 | Loss: 0.00001829
Iteration 93/1000 | Loss: 0.00001829
Iteration 94/1000 | Loss: 0.00001829
Iteration 95/1000 | Loss: 0.00001829
Iteration 96/1000 | Loss: 0.00001829
Iteration 97/1000 | Loss: 0.00001828
Iteration 98/1000 | Loss: 0.00001828
Iteration 99/1000 | Loss: 0.00001828
Iteration 100/1000 | Loss: 0.00001828
Iteration 101/1000 | Loss: 0.00001828
Iteration 102/1000 | Loss: 0.00001827
Iteration 103/1000 | Loss: 0.00001827
Iteration 104/1000 | Loss: 0.00001827
Iteration 105/1000 | Loss: 0.00001827
Iteration 106/1000 | Loss: 0.00001827
Iteration 107/1000 | Loss: 0.00001827
Iteration 108/1000 | Loss: 0.00001827
Iteration 109/1000 | Loss: 0.00001826
Iteration 110/1000 | Loss: 0.00001826
Iteration 111/1000 | Loss: 0.00001826
Iteration 112/1000 | Loss: 0.00001826
Iteration 113/1000 | Loss: 0.00001826
Iteration 114/1000 | Loss: 0.00001825
Iteration 115/1000 | Loss: 0.00001825
Iteration 116/1000 | Loss: 0.00001825
Iteration 117/1000 | Loss: 0.00001825
Iteration 118/1000 | Loss: 0.00001824
Iteration 119/1000 | Loss: 0.00001824
Iteration 120/1000 | Loss: 0.00001824
Iteration 121/1000 | Loss: 0.00001824
Iteration 122/1000 | Loss: 0.00001823
Iteration 123/1000 | Loss: 0.00001823
Iteration 124/1000 | Loss: 0.00001823
Iteration 125/1000 | Loss: 0.00001823
Iteration 126/1000 | Loss: 0.00001823
Iteration 127/1000 | Loss: 0.00001822
Iteration 128/1000 | Loss: 0.00001822
Iteration 129/1000 | Loss: 0.00001822
Iteration 130/1000 | Loss: 0.00001821
Iteration 131/1000 | Loss: 0.00001821
Iteration 132/1000 | Loss: 0.00001820
Iteration 133/1000 | Loss: 0.00001820
Iteration 134/1000 | Loss: 0.00001820
Iteration 135/1000 | Loss: 0.00001819
Iteration 136/1000 | Loss: 0.00001819
Iteration 137/1000 | Loss: 0.00001819
Iteration 138/1000 | Loss: 0.00001818
Iteration 139/1000 | Loss: 0.00001818
Iteration 140/1000 | Loss: 0.00001818
Iteration 141/1000 | Loss: 0.00001818
Iteration 142/1000 | Loss: 0.00001818
Iteration 143/1000 | Loss: 0.00001818
Iteration 144/1000 | Loss: 0.00001818
Iteration 145/1000 | Loss: 0.00001817
Iteration 146/1000 | Loss: 0.00001816
Iteration 147/1000 | Loss: 0.00001816
Iteration 148/1000 | Loss: 0.00001816
Iteration 149/1000 | Loss: 0.00001815
Iteration 150/1000 | Loss: 0.00001815
Iteration 151/1000 | Loss: 0.00001815
Iteration 152/1000 | Loss: 0.00001814
Iteration 153/1000 | Loss: 0.00001814
Iteration 154/1000 | Loss: 0.00001814
Iteration 155/1000 | Loss: 0.00001813
Iteration 156/1000 | Loss: 0.00001813
Iteration 157/1000 | Loss: 0.00001812
Iteration 158/1000 | Loss: 0.00001812
Iteration 159/1000 | Loss: 0.00001812
Iteration 160/1000 | Loss: 0.00001812
Iteration 161/1000 | Loss: 0.00001812
Iteration 162/1000 | Loss: 0.00001811
Iteration 163/1000 | Loss: 0.00001811
Iteration 164/1000 | Loss: 0.00001810
Iteration 165/1000 | Loss: 0.00001810
Iteration 166/1000 | Loss: 0.00001810
Iteration 167/1000 | Loss: 0.00001810
Iteration 168/1000 | Loss: 0.00001809
Iteration 169/1000 | Loss: 0.00001809
Iteration 170/1000 | Loss: 0.00001809
Iteration 171/1000 | Loss: 0.00001808
Iteration 172/1000 | Loss: 0.00001808
Iteration 173/1000 | Loss: 0.00001808
Iteration 174/1000 | Loss: 0.00001808
Iteration 175/1000 | Loss: 0.00001807
Iteration 176/1000 | Loss: 0.00001807
Iteration 177/1000 | Loss: 0.00001807
Iteration 178/1000 | Loss: 0.00001806
Iteration 179/1000 | Loss: 0.00001806
Iteration 180/1000 | Loss: 0.00001806
Iteration 181/1000 | Loss: 0.00001806
Iteration 182/1000 | Loss: 0.00001806
Iteration 183/1000 | Loss: 0.00001806
Iteration 184/1000 | Loss: 0.00001806
Iteration 185/1000 | Loss: 0.00001806
Iteration 186/1000 | Loss: 0.00001806
Iteration 187/1000 | Loss: 0.00001806
Iteration 188/1000 | Loss: 0.00001805
Iteration 189/1000 | Loss: 0.00001805
Iteration 190/1000 | Loss: 0.00001805
Iteration 191/1000 | Loss: 0.00001805
Iteration 192/1000 | Loss: 0.00001805
Iteration 193/1000 | Loss: 0.00001805
Iteration 194/1000 | Loss: 0.00001805
Iteration 195/1000 | Loss: 0.00001805
Iteration 196/1000 | Loss: 0.00001805
Iteration 197/1000 | Loss: 0.00001805
Iteration 198/1000 | Loss: 0.00001805
Iteration 199/1000 | Loss: 0.00001805
Iteration 200/1000 | Loss: 0.00001805
Iteration 201/1000 | Loss: 0.00001805
Iteration 202/1000 | Loss: 0.00001805
Iteration 203/1000 | Loss: 0.00001805
Iteration 204/1000 | Loss: 0.00001805
Iteration 205/1000 | Loss: 0.00001805
Iteration 206/1000 | Loss: 0.00001805
Iteration 207/1000 | Loss: 0.00001805
Iteration 208/1000 | Loss: 0.00001805
Iteration 209/1000 | Loss: 0.00001805
Iteration 210/1000 | Loss: 0.00001805
Iteration 211/1000 | Loss: 0.00001805
Iteration 212/1000 | Loss: 0.00001805
Iteration 213/1000 | Loss: 0.00001805
Iteration 214/1000 | Loss: 0.00001805
Iteration 215/1000 | Loss: 0.00001805
Iteration 216/1000 | Loss: 0.00001805
Iteration 217/1000 | Loss: 0.00001805
Iteration 218/1000 | Loss: 0.00001805
Iteration 219/1000 | Loss: 0.00001805
Iteration 220/1000 | Loss: 0.00001805
Iteration 221/1000 | Loss: 0.00001805
Iteration 222/1000 | Loss: 0.00001805
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 222. Stopping optimization.
Last 5 losses: [1.8047130652121268e-05, 1.8047130652121268e-05, 1.8047130652121268e-05, 1.8047130652121268e-05, 1.8047130652121268e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8047130652121268e-05

Optimization complete. Final v2v error: 3.6129539012908936 mm

Highest mean error: 4.541792392730713 mm for frame 143

Lowest mean error: 3.023090362548828 mm for frame 179

Saving results

Total time: 55.681281328201294
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carina_posed_010/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00842666
Iteration 2/25 | Loss: 0.00169728
Iteration 3/25 | Loss: 0.00144942
Iteration 4/25 | Loss: 0.00141233
Iteration 5/25 | Loss: 0.00140741
Iteration 6/25 | Loss: 0.00140656
Iteration 7/25 | Loss: 0.00140656
Iteration 8/25 | Loss: 0.00140656
Iteration 9/25 | Loss: 0.00140656
Iteration 10/25 | Loss: 0.00140656
Iteration 11/25 | Loss: 0.00140656
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014065628638491035, 0.0014065628638491035, 0.0014065628638491035, 0.0014065628638491035, 0.0014065628638491035]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014065628638491035

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.84810740
Iteration 2/25 | Loss: 0.00137381
Iteration 3/25 | Loss: 0.00137381
Iteration 4/25 | Loss: 0.00137381
Iteration 5/25 | Loss: 0.00137381
Iteration 6/25 | Loss: 0.00137381
Iteration 7/25 | Loss: 0.00137381
Iteration 8/25 | Loss: 0.00137381
Iteration 9/25 | Loss: 0.00137381
Iteration 10/25 | Loss: 0.00137381
Iteration 11/25 | Loss: 0.00137381
Iteration 12/25 | Loss: 0.00137381
Iteration 13/25 | Loss: 0.00137381
Iteration 14/25 | Loss: 0.00137381
Iteration 15/25 | Loss: 0.00137381
Iteration 16/25 | Loss: 0.00137381
Iteration 17/25 | Loss: 0.00137381
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001373810926452279, 0.001373810926452279, 0.001373810926452279, 0.001373810926452279, 0.001373810926452279]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001373810926452279

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137381
Iteration 2/1000 | Loss: 0.00005654
Iteration 3/1000 | Loss: 0.00004346
Iteration 4/1000 | Loss: 0.00003542
Iteration 5/1000 | Loss: 0.00003291
Iteration 6/1000 | Loss: 0.00003133
Iteration 7/1000 | Loss: 0.00003021
Iteration 8/1000 | Loss: 0.00002951
Iteration 9/1000 | Loss: 0.00002889
Iteration 10/1000 | Loss: 0.00002843
Iteration 11/1000 | Loss: 0.00002807
Iteration 12/1000 | Loss: 0.00002775
Iteration 13/1000 | Loss: 0.00002747
Iteration 14/1000 | Loss: 0.00002723
Iteration 15/1000 | Loss: 0.00002702
Iteration 16/1000 | Loss: 0.00002685
Iteration 17/1000 | Loss: 0.00002684
Iteration 18/1000 | Loss: 0.00002681
Iteration 19/1000 | Loss: 0.00002681
Iteration 20/1000 | Loss: 0.00002681
Iteration 21/1000 | Loss: 0.00002681
Iteration 22/1000 | Loss: 0.00002681
Iteration 23/1000 | Loss: 0.00002680
Iteration 24/1000 | Loss: 0.00002675
Iteration 25/1000 | Loss: 0.00002670
Iteration 26/1000 | Loss: 0.00002670
Iteration 27/1000 | Loss: 0.00002670
Iteration 28/1000 | Loss: 0.00002670
Iteration 29/1000 | Loss: 0.00002670
Iteration 30/1000 | Loss: 0.00002670
Iteration 31/1000 | Loss: 0.00002670
Iteration 32/1000 | Loss: 0.00002670
Iteration 33/1000 | Loss: 0.00002670
Iteration 34/1000 | Loss: 0.00002670
Iteration 35/1000 | Loss: 0.00002669
Iteration 36/1000 | Loss: 0.00002668
Iteration 37/1000 | Loss: 0.00002668
Iteration 38/1000 | Loss: 0.00002668
Iteration 39/1000 | Loss: 0.00002667
Iteration 40/1000 | Loss: 0.00002667
Iteration 41/1000 | Loss: 0.00002667
Iteration 42/1000 | Loss: 0.00002667
Iteration 43/1000 | Loss: 0.00002666
Iteration 44/1000 | Loss: 0.00002666
Iteration 45/1000 | Loss: 0.00002665
Iteration 46/1000 | Loss: 0.00002661
Iteration 47/1000 | Loss: 0.00002661
Iteration 48/1000 | Loss: 0.00002661
Iteration 49/1000 | Loss: 0.00002661
Iteration 50/1000 | Loss: 0.00002661
Iteration 51/1000 | Loss: 0.00002661
Iteration 52/1000 | Loss: 0.00002660
Iteration 53/1000 | Loss: 0.00002660
Iteration 54/1000 | Loss: 0.00002660
Iteration 55/1000 | Loss: 0.00002660
Iteration 56/1000 | Loss: 0.00002660
Iteration 57/1000 | Loss: 0.00002659
Iteration 58/1000 | Loss: 0.00002659
Iteration 59/1000 | Loss: 0.00002659
Iteration 60/1000 | Loss: 0.00002659
Iteration 61/1000 | Loss: 0.00002659
Iteration 62/1000 | Loss: 0.00002659
Iteration 63/1000 | Loss: 0.00002659
Iteration 64/1000 | Loss: 0.00002659
Iteration 65/1000 | Loss: 0.00002659
Iteration 66/1000 | Loss: 0.00002659
Iteration 67/1000 | Loss: 0.00002658
Iteration 68/1000 | Loss: 0.00002658
Iteration 69/1000 | Loss: 0.00002658
Iteration 70/1000 | Loss: 0.00002658
Iteration 71/1000 | Loss: 0.00002658
Iteration 72/1000 | Loss: 0.00002657
Iteration 73/1000 | Loss: 0.00002657
Iteration 74/1000 | Loss: 0.00002657
Iteration 75/1000 | Loss: 0.00002651
Iteration 76/1000 | Loss: 0.00002651
Iteration 77/1000 | Loss: 0.00002650
Iteration 78/1000 | Loss: 0.00002650
Iteration 79/1000 | Loss: 0.00002649
Iteration 80/1000 | Loss: 0.00002648
Iteration 81/1000 | Loss: 0.00002647
Iteration 82/1000 | Loss: 0.00002646
Iteration 83/1000 | Loss: 0.00002646
Iteration 84/1000 | Loss: 0.00002646
Iteration 85/1000 | Loss: 0.00002645
Iteration 86/1000 | Loss: 0.00002644
Iteration 87/1000 | Loss: 0.00002644
Iteration 88/1000 | Loss: 0.00002644
Iteration 89/1000 | Loss: 0.00002643
Iteration 90/1000 | Loss: 0.00002640
Iteration 91/1000 | Loss: 0.00002640
Iteration 92/1000 | Loss: 0.00002640
Iteration 93/1000 | Loss: 0.00002640
Iteration 94/1000 | Loss: 0.00002640
Iteration 95/1000 | Loss: 0.00002640
Iteration 96/1000 | Loss: 0.00002640
Iteration 97/1000 | Loss: 0.00002639
Iteration 98/1000 | Loss: 0.00002639
Iteration 99/1000 | Loss: 0.00002638
Iteration 100/1000 | Loss: 0.00002638
Iteration 101/1000 | Loss: 0.00002638
Iteration 102/1000 | Loss: 0.00002638
Iteration 103/1000 | Loss: 0.00002638
Iteration 104/1000 | Loss: 0.00002638
Iteration 105/1000 | Loss: 0.00002638
Iteration 106/1000 | Loss: 0.00002638
Iteration 107/1000 | Loss: 0.00002638
Iteration 108/1000 | Loss: 0.00002637
Iteration 109/1000 | Loss: 0.00002637
Iteration 110/1000 | Loss: 0.00002637
Iteration 111/1000 | Loss: 0.00002637
Iteration 112/1000 | Loss: 0.00002637
Iteration 113/1000 | Loss: 0.00002637
Iteration 114/1000 | Loss: 0.00002637
Iteration 115/1000 | Loss: 0.00002637
Iteration 116/1000 | Loss: 0.00002636
Iteration 117/1000 | Loss: 0.00002636
Iteration 118/1000 | Loss: 0.00002636
Iteration 119/1000 | Loss: 0.00002635
Iteration 120/1000 | Loss: 0.00002635
Iteration 121/1000 | Loss: 0.00002635
Iteration 122/1000 | Loss: 0.00002635
Iteration 123/1000 | Loss: 0.00002635
Iteration 124/1000 | Loss: 0.00002634
Iteration 125/1000 | Loss: 0.00002634
Iteration 126/1000 | Loss: 0.00002634
Iteration 127/1000 | Loss: 0.00002634
Iteration 128/1000 | Loss: 0.00002634
Iteration 129/1000 | Loss: 0.00002634
Iteration 130/1000 | Loss: 0.00002634
Iteration 131/1000 | Loss: 0.00002634
Iteration 132/1000 | Loss: 0.00002634
Iteration 133/1000 | Loss: 0.00002634
Iteration 134/1000 | Loss: 0.00002634
Iteration 135/1000 | Loss: 0.00002633
Iteration 136/1000 | Loss: 0.00002633
Iteration 137/1000 | Loss: 0.00002633
Iteration 138/1000 | Loss: 0.00002632
Iteration 139/1000 | Loss: 0.00002632
Iteration 140/1000 | Loss: 0.00002632
Iteration 141/1000 | Loss: 0.00002632
Iteration 142/1000 | Loss: 0.00002632
Iteration 143/1000 | Loss: 0.00002632
Iteration 144/1000 | Loss: 0.00002631
Iteration 145/1000 | Loss: 0.00002631
Iteration 146/1000 | Loss: 0.00002631
Iteration 147/1000 | Loss: 0.00002631
Iteration 148/1000 | Loss: 0.00002631
Iteration 149/1000 | Loss: 0.00002630
Iteration 150/1000 | Loss: 0.00002630
Iteration 151/1000 | Loss: 0.00002630
Iteration 152/1000 | Loss: 0.00002630
Iteration 153/1000 | Loss: 0.00002630
Iteration 154/1000 | Loss: 0.00002630
Iteration 155/1000 | Loss: 0.00002630
Iteration 156/1000 | Loss: 0.00002630
Iteration 157/1000 | Loss: 0.00002630
Iteration 158/1000 | Loss: 0.00002630
Iteration 159/1000 | Loss: 0.00002630
Iteration 160/1000 | Loss: 0.00002630
Iteration 161/1000 | Loss: 0.00002630
Iteration 162/1000 | Loss: 0.00002630
Iteration 163/1000 | Loss: 0.00002630
Iteration 164/1000 | Loss: 0.00002630
Iteration 165/1000 | Loss: 0.00002630
Iteration 166/1000 | Loss: 0.00002630
Iteration 167/1000 | Loss: 0.00002630
Iteration 168/1000 | Loss: 0.00002630
Iteration 169/1000 | Loss: 0.00002630
Iteration 170/1000 | Loss: 0.00002630
Iteration 171/1000 | Loss: 0.00002630
Iteration 172/1000 | Loss: 0.00002630
Iteration 173/1000 | Loss: 0.00002630
Iteration 174/1000 | Loss: 0.00002630
Iteration 175/1000 | Loss: 0.00002630
Iteration 176/1000 | Loss: 0.00002630
Iteration 177/1000 | Loss: 0.00002630
Iteration 178/1000 | Loss: 0.00002630
Iteration 179/1000 | Loss: 0.00002630
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [2.629898881423287e-05, 2.629898881423287e-05, 2.629898881423287e-05, 2.629898881423287e-05, 2.629898881423287e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.629898881423287e-05

Optimization complete. Final v2v error: 4.43683385848999 mm

Highest mean error: 4.682477951049805 mm for frame 100

Lowest mean error: 3.9034550189971924 mm for frame 0

Saving results

Total time: 44.22295093536377
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carina_posed_010/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00836068
Iteration 2/25 | Loss: 0.00185320
Iteration 3/25 | Loss: 0.00169156
Iteration 4/25 | Loss: 0.00168447
Iteration 5/25 | Loss: 0.00168228
Iteration 6/25 | Loss: 0.00168194
Iteration 7/25 | Loss: 0.00168194
Iteration 8/25 | Loss: 0.00168194
Iteration 9/25 | Loss: 0.00168194
Iteration 10/25 | Loss: 0.00168194
Iteration 11/25 | Loss: 0.00168194
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0016819401644170284, 0.0016819401644170284, 0.0016819401644170284, 0.0016819401644170284, 0.0016819401644170284]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016819401644170284

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.46142215
Iteration 2/25 | Loss: 0.00161446
Iteration 3/25 | Loss: 0.00161446
Iteration 4/25 | Loss: 0.00161446
Iteration 5/25 | Loss: 0.00161446
Iteration 6/25 | Loss: 0.00161446
Iteration 7/25 | Loss: 0.00161446
Iteration 8/25 | Loss: 0.00161445
Iteration 9/25 | Loss: 0.00161446
Iteration 10/25 | Loss: 0.00161446
Iteration 11/25 | Loss: 0.00161446
Iteration 12/25 | Loss: 0.00161446
Iteration 13/25 | Loss: 0.00161446
Iteration 14/25 | Loss: 0.00161446
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001614455017261207, 0.001614455017261207, 0.001614455017261207, 0.001614455017261207, 0.001614455017261207]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001614455017261207

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00161446
Iteration 2/1000 | Loss: 0.00011234
Iteration 3/1000 | Loss: 0.00006799
Iteration 4/1000 | Loss: 0.00005192
Iteration 5/1000 | Loss: 0.00004819
Iteration 6/1000 | Loss: 0.00004591
Iteration 7/1000 | Loss: 0.00004492
Iteration 8/1000 | Loss: 0.00004422
Iteration 9/1000 | Loss: 0.00004337
Iteration 10/1000 | Loss: 0.00004252
Iteration 11/1000 | Loss: 0.00004184
Iteration 12/1000 | Loss: 0.00004122
Iteration 13/1000 | Loss: 0.00004077
Iteration 14/1000 | Loss: 0.00004033
Iteration 15/1000 | Loss: 0.00003995
Iteration 16/1000 | Loss: 0.00003956
Iteration 17/1000 | Loss: 0.00003931
Iteration 18/1000 | Loss: 0.00003910
Iteration 19/1000 | Loss: 0.00003893
Iteration 20/1000 | Loss: 0.00003886
Iteration 21/1000 | Loss: 0.00003883
Iteration 22/1000 | Loss: 0.00003882
Iteration 23/1000 | Loss: 0.00003882
Iteration 24/1000 | Loss: 0.00003882
Iteration 25/1000 | Loss: 0.00003881
Iteration 26/1000 | Loss: 0.00003881
Iteration 27/1000 | Loss: 0.00003881
Iteration 28/1000 | Loss: 0.00003878
Iteration 29/1000 | Loss: 0.00003878
Iteration 30/1000 | Loss: 0.00003877
Iteration 31/1000 | Loss: 0.00003877
Iteration 32/1000 | Loss: 0.00003877
Iteration 33/1000 | Loss: 0.00003877
Iteration 34/1000 | Loss: 0.00003877
Iteration 35/1000 | Loss: 0.00003877
Iteration 36/1000 | Loss: 0.00003877
Iteration 37/1000 | Loss: 0.00003876
Iteration 38/1000 | Loss: 0.00003876
Iteration 39/1000 | Loss: 0.00003874
Iteration 40/1000 | Loss: 0.00003874
Iteration 41/1000 | Loss: 0.00003874
Iteration 42/1000 | Loss: 0.00003874
Iteration 43/1000 | Loss: 0.00003874
Iteration 44/1000 | Loss: 0.00003873
Iteration 45/1000 | Loss: 0.00003873
Iteration 46/1000 | Loss: 0.00003873
Iteration 47/1000 | Loss: 0.00003873
Iteration 48/1000 | Loss: 0.00003873
Iteration 49/1000 | Loss: 0.00003873
Iteration 50/1000 | Loss: 0.00003873
Iteration 51/1000 | Loss: 0.00003870
Iteration 52/1000 | Loss: 0.00003870
Iteration 53/1000 | Loss: 0.00003870
Iteration 54/1000 | Loss: 0.00003870
Iteration 55/1000 | Loss: 0.00003870
Iteration 56/1000 | Loss: 0.00003869
Iteration 57/1000 | Loss: 0.00003869
Iteration 58/1000 | Loss: 0.00003869
Iteration 59/1000 | Loss: 0.00003869
Iteration 60/1000 | Loss: 0.00003869
Iteration 61/1000 | Loss: 0.00003869
Iteration 62/1000 | Loss: 0.00003869
Iteration 63/1000 | Loss: 0.00003869
Iteration 64/1000 | Loss: 0.00003869
Iteration 65/1000 | Loss: 0.00003869
Iteration 66/1000 | Loss: 0.00003866
Iteration 67/1000 | Loss: 0.00003861
Iteration 68/1000 | Loss: 0.00003861
Iteration 69/1000 | Loss: 0.00003859
Iteration 70/1000 | Loss: 0.00003858
Iteration 71/1000 | Loss: 0.00003857
Iteration 72/1000 | Loss: 0.00003857
Iteration 73/1000 | Loss: 0.00003857
Iteration 74/1000 | Loss: 0.00003857
Iteration 75/1000 | Loss: 0.00003857
Iteration 76/1000 | Loss: 0.00003857
Iteration 77/1000 | Loss: 0.00003857
Iteration 78/1000 | Loss: 0.00003857
Iteration 79/1000 | Loss: 0.00003857
Iteration 80/1000 | Loss: 0.00003856
Iteration 81/1000 | Loss: 0.00003856
Iteration 82/1000 | Loss: 0.00003856
Iteration 83/1000 | Loss: 0.00003854
Iteration 84/1000 | Loss: 0.00003854
Iteration 85/1000 | Loss: 0.00003854
Iteration 86/1000 | Loss: 0.00003854
Iteration 87/1000 | Loss: 0.00003852
Iteration 88/1000 | Loss: 0.00003849
Iteration 89/1000 | Loss: 0.00003847
Iteration 90/1000 | Loss: 0.00003847
Iteration 91/1000 | Loss: 0.00003847
Iteration 92/1000 | Loss: 0.00003847
Iteration 93/1000 | Loss: 0.00003847
Iteration 94/1000 | Loss: 0.00003847
Iteration 95/1000 | Loss: 0.00003847
Iteration 96/1000 | Loss: 0.00003847
Iteration 97/1000 | Loss: 0.00003846
Iteration 98/1000 | Loss: 0.00003846
Iteration 99/1000 | Loss: 0.00003846
Iteration 100/1000 | Loss: 0.00003846
Iteration 101/1000 | Loss: 0.00003846
Iteration 102/1000 | Loss: 0.00003846
Iteration 103/1000 | Loss: 0.00003846
Iteration 104/1000 | Loss: 0.00003846
Iteration 105/1000 | Loss: 0.00003846
Iteration 106/1000 | Loss: 0.00003846
Iteration 107/1000 | Loss: 0.00003845
Iteration 108/1000 | Loss: 0.00003845
Iteration 109/1000 | Loss: 0.00003845
Iteration 110/1000 | Loss: 0.00003844
Iteration 111/1000 | Loss: 0.00003843
Iteration 112/1000 | Loss: 0.00003843
Iteration 113/1000 | Loss: 0.00003842
Iteration 114/1000 | Loss: 0.00003842
Iteration 115/1000 | Loss: 0.00003842
Iteration 116/1000 | Loss: 0.00003842
Iteration 117/1000 | Loss: 0.00003842
Iteration 118/1000 | Loss: 0.00003842
Iteration 119/1000 | Loss: 0.00003842
Iteration 120/1000 | Loss: 0.00003842
Iteration 121/1000 | Loss: 0.00003842
Iteration 122/1000 | Loss: 0.00003841
Iteration 123/1000 | Loss: 0.00003841
Iteration 124/1000 | Loss: 0.00003841
Iteration 125/1000 | Loss: 0.00003841
Iteration 126/1000 | Loss: 0.00003841
Iteration 127/1000 | Loss: 0.00003841
Iteration 128/1000 | Loss: 0.00003841
Iteration 129/1000 | Loss: 0.00003840
Iteration 130/1000 | Loss: 0.00003840
Iteration 131/1000 | Loss: 0.00003840
Iteration 132/1000 | Loss: 0.00003840
Iteration 133/1000 | Loss: 0.00003840
Iteration 134/1000 | Loss: 0.00003840
Iteration 135/1000 | Loss: 0.00003840
Iteration 136/1000 | Loss: 0.00003840
Iteration 137/1000 | Loss: 0.00003840
Iteration 138/1000 | Loss: 0.00003840
Iteration 139/1000 | Loss: 0.00003840
Iteration 140/1000 | Loss: 0.00003839
Iteration 141/1000 | Loss: 0.00003839
Iteration 142/1000 | Loss: 0.00003839
Iteration 143/1000 | Loss: 0.00003839
Iteration 144/1000 | Loss: 0.00003839
Iteration 145/1000 | Loss: 0.00003839
Iteration 146/1000 | Loss: 0.00003839
Iteration 147/1000 | Loss: 0.00003839
Iteration 148/1000 | Loss: 0.00003839
Iteration 149/1000 | Loss: 0.00003839
Iteration 150/1000 | Loss: 0.00003838
Iteration 151/1000 | Loss: 0.00003838
Iteration 152/1000 | Loss: 0.00003838
Iteration 153/1000 | Loss: 0.00003838
Iteration 154/1000 | Loss: 0.00003838
Iteration 155/1000 | Loss: 0.00003838
Iteration 156/1000 | Loss: 0.00003838
Iteration 157/1000 | Loss: 0.00003838
Iteration 158/1000 | Loss: 0.00003838
Iteration 159/1000 | Loss: 0.00003838
Iteration 160/1000 | Loss: 0.00003838
Iteration 161/1000 | Loss: 0.00003838
Iteration 162/1000 | Loss: 0.00003837
Iteration 163/1000 | Loss: 0.00003837
Iteration 164/1000 | Loss: 0.00003837
Iteration 165/1000 | Loss: 0.00003837
Iteration 166/1000 | Loss: 0.00003836
Iteration 167/1000 | Loss: 0.00003836
Iteration 168/1000 | Loss: 0.00003836
Iteration 169/1000 | Loss: 0.00003836
Iteration 170/1000 | Loss: 0.00003836
Iteration 171/1000 | Loss: 0.00003835
Iteration 172/1000 | Loss: 0.00003835
Iteration 173/1000 | Loss: 0.00003835
Iteration 174/1000 | Loss: 0.00003835
Iteration 175/1000 | Loss: 0.00003835
Iteration 176/1000 | Loss: 0.00003835
Iteration 177/1000 | Loss: 0.00003835
Iteration 178/1000 | Loss: 0.00003834
Iteration 179/1000 | Loss: 0.00003834
Iteration 180/1000 | Loss: 0.00003834
Iteration 181/1000 | Loss: 0.00003834
Iteration 182/1000 | Loss: 0.00003834
Iteration 183/1000 | Loss: 0.00003834
Iteration 184/1000 | Loss: 0.00003833
Iteration 185/1000 | Loss: 0.00003833
Iteration 186/1000 | Loss: 0.00003833
Iteration 187/1000 | Loss: 0.00003833
Iteration 188/1000 | Loss: 0.00003833
Iteration 189/1000 | Loss: 0.00003833
Iteration 190/1000 | Loss: 0.00003833
Iteration 191/1000 | Loss: 0.00003833
Iteration 192/1000 | Loss: 0.00003833
Iteration 193/1000 | Loss: 0.00003833
Iteration 194/1000 | Loss: 0.00003833
Iteration 195/1000 | Loss: 0.00003833
Iteration 196/1000 | Loss: 0.00003833
Iteration 197/1000 | Loss: 0.00003833
Iteration 198/1000 | Loss: 0.00003833
Iteration 199/1000 | Loss: 0.00003833
Iteration 200/1000 | Loss: 0.00003833
Iteration 201/1000 | Loss: 0.00003833
Iteration 202/1000 | Loss: 0.00003833
Iteration 203/1000 | Loss: 0.00003833
Iteration 204/1000 | Loss: 0.00003833
Iteration 205/1000 | Loss: 0.00003833
Iteration 206/1000 | Loss: 0.00003833
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [3.832665606751107e-05, 3.832665606751107e-05, 3.832665606751107e-05, 3.832665606751107e-05, 3.832665606751107e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.832665606751107e-05

Optimization complete. Final v2v error: 5.136475086212158 mm

Highest mean error: 6.156833648681641 mm for frame 143

Lowest mean error: 4.860105991363525 mm for frame 36

Saving results

Total time: 50.39118504524231
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carina_posed_010/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00465402
Iteration 2/25 | Loss: 0.00145254
Iteration 3/25 | Loss: 0.00136949
Iteration 4/25 | Loss: 0.00135866
Iteration 5/25 | Loss: 0.00135650
Iteration 6/25 | Loss: 0.00135650
Iteration 7/25 | Loss: 0.00135650
Iteration 8/25 | Loss: 0.00135650
Iteration 9/25 | Loss: 0.00135650
Iteration 10/25 | Loss: 0.00135650
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013564956607297063, 0.0013564956607297063, 0.0013564956607297063, 0.0013564956607297063, 0.0013564956607297063]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013564956607297063

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.32612562
Iteration 2/25 | Loss: 0.00163148
Iteration 3/25 | Loss: 0.00163148
Iteration 4/25 | Loss: 0.00163147
Iteration 5/25 | Loss: 0.00163147
Iteration 6/25 | Loss: 0.00163147
Iteration 7/25 | Loss: 0.00163147
Iteration 8/25 | Loss: 0.00163147
Iteration 9/25 | Loss: 0.00163147
Iteration 10/25 | Loss: 0.00163147
Iteration 11/25 | Loss: 0.00163147
Iteration 12/25 | Loss: 0.00163147
Iteration 13/25 | Loss: 0.00163147
Iteration 14/25 | Loss: 0.00163147
Iteration 15/25 | Loss: 0.00163147
Iteration 16/25 | Loss: 0.00163147
Iteration 17/25 | Loss: 0.00163147
Iteration 18/25 | Loss: 0.00163147
Iteration 19/25 | Loss: 0.00163147
Iteration 20/25 | Loss: 0.00163147
Iteration 21/25 | Loss: 0.00163147
Iteration 22/25 | Loss: 0.00163147
Iteration 23/25 | Loss: 0.00163147
Iteration 24/25 | Loss: 0.00163147
Iteration 25/25 | Loss: 0.00163147

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00163147
Iteration 2/1000 | Loss: 0.00002383
Iteration 3/1000 | Loss: 0.00001975
Iteration 4/1000 | Loss: 0.00001812
Iteration 5/1000 | Loss: 0.00001704
Iteration 6/1000 | Loss: 0.00001642
Iteration 7/1000 | Loss: 0.00001585
Iteration 8/1000 | Loss: 0.00001541
Iteration 9/1000 | Loss: 0.00001506
Iteration 10/1000 | Loss: 0.00001475
Iteration 11/1000 | Loss: 0.00001455
Iteration 12/1000 | Loss: 0.00001438
Iteration 13/1000 | Loss: 0.00001416
Iteration 14/1000 | Loss: 0.00001394
Iteration 15/1000 | Loss: 0.00001389
Iteration 16/1000 | Loss: 0.00001387
Iteration 17/1000 | Loss: 0.00001386
Iteration 18/1000 | Loss: 0.00001385
Iteration 19/1000 | Loss: 0.00001384
Iteration 20/1000 | Loss: 0.00001382
Iteration 21/1000 | Loss: 0.00001382
Iteration 22/1000 | Loss: 0.00001381
Iteration 23/1000 | Loss: 0.00001377
Iteration 24/1000 | Loss: 0.00001373
Iteration 25/1000 | Loss: 0.00001372
Iteration 26/1000 | Loss: 0.00001371
Iteration 27/1000 | Loss: 0.00001362
Iteration 28/1000 | Loss: 0.00001358
Iteration 29/1000 | Loss: 0.00001357
Iteration 30/1000 | Loss: 0.00001357
Iteration 31/1000 | Loss: 0.00001356
Iteration 32/1000 | Loss: 0.00001351
Iteration 33/1000 | Loss: 0.00001350
Iteration 34/1000 | Loss: 0.00001350
Iteration 35/1000 | Loss: 0.00001349
Iteration 36/1000 | Loss: 0.00001348
Iteration 37/1000 | Loss: 0.00001347
Iteration 38/1000 | Loss: 0.00001346
Iteration 39/1000 | Loss: 0.00001346
Iteration 40/1000 | Loss: 0.00001345
Iteration 41/1000 | Loss: 0.00001345
Iteration 42/1000 | Loss: 0.00001345
Iteration 43/1000 | Loss: 0.00001343
Iteration 44/1000 | Loss: 0.00001343
Iteration 45/1000 | Loss: 0.00001342
Iteration 46/1000 | Loss: 0.00001340
Iteration 47/1000 | Loss: 0.00001339
Iteration 48/1000 | Loss: 0.00001339
Iteration 49/1000 | Loss: 0.00001339
Iteration 50/1000 | Loss: 0.00001339
Iteration 51/1000 | Loss: 0.00001339
Iteration 52/1000 | Loss: 0.00001339
Iteration 53/1000 | Loss: 0.00001339
Iteration 54/1000 | Loss: 0.00001339
Iteration 55/1000 | Loss: 0.00001339
Iteration 56/1000 | Loss: 0.00001339
Iteration 57/1000 | Loss: 0.00001338
Iteration 58/1000 | Loss: 0.00001338
Iteration 59/1000 | Loss: 0.00001336
Iteration 60/1000 | Loss: 0.00001336
Iteration 61/1000 | Loss: 0.00001336
Iteration 62/1000 | Loss: 0.00001336
Iteration 63/1000 | Loss: 0.00001336
Iteration 64/1000 | Loss: 0.00001336
Iteration 65/1000 | Loss: 0.00001336
Iteration 66/1000 | Loss: 0.00001336
Iteration 67/1000 | Loss: 0.00001335
Iteration 68/1000 | Loss: 0.00001335
Iteration 69/1000 | Loss: 0.00001335
Iteration 70/1000 | Loss: 0.00001335
Iteration 71/1000 | Loss: 0.00001335
Iteration 72/1000 | Loss: 0.00001335
Iteration 73/1000 | Loss: 0.00001335
Iteration 74/1000 | Loss: 0.00001335
Iteration 75/1000 | Loss: 0.00001335
Iteration 76/1000 | Loss: 0.00001335
Iteration 77/1000 | Loss: 0.00001334
Iteration 78/1000 | Loss: 0.00001333
Iteration 79/1000 | Loss: 0.00001332
Iteration 80/1000 | Loss: 0.00001332
Iteration 81/1000 | Loss: 0.00001332
Iteration 82/1000 | Loss: 0.00001332
Iteration 83/1000 | Loss: 0.00001332
Iteration 84/1000 | Loss: 0.00001332
Iteration 85/1000 | Loss: 0.00001332
Iteration 86/1000 | Loss: 0.00001332
Iteration 87/1000 | Loss: 0.00001332
Iteration 88/1000 | Loss: 0.00001332
Iteration 89/1000 | Loss: 0.00001332
Iteration 90/1000 | Loss: 0.00001331
Iteration 91/1000 | Loss: 0.00001331
Iteration 92/1000 | Loss: 0.00001331
Iteration 93/1000 | Loss: 0.00001331
Iteration 94/1000 | Loss: 0.00001331
Iteration 95/1000 | Loss: 0.00001331
Iteration 96/1000 | Loss: 0.00001331
Iteration 97/1000 | Loss: 0.00001331
Iteration 98/1000 | Loss: 0.00001331
Iteration 99/1000 | Loss: 0.00001330
Iteration 100/1000 | Loss: 0.00001330
Iteration 101/1000 | Loss: 0.00001330
Iteration 102/1000 | Loss: 0.00001330
Iteration 103/1000 | Loss: 0.00001329
Iteration 104/1000 | Loss: 0.00001329
Iteration 105/1000 | Loss: 0.00001329
Iteration 106/1000 | Loss: 0.00001329
Iteration 107/1000 | Loss: 0.00001329
Iteration 108/1000 | Loss: 0.00001328
Iteration 109/1000 | Loss: 0.00001328
Iteration 110/1000 | Loss: 0.00001328
Iteration 111/1000 | Loss: 0.00001328
Iteration 112/1000 | Loss: 0.00001328
Iteration 113/1000 | Loss: 0.00001328
Iteration 114/1000 | Loss: 0.00001328
Iteration 115/1000 | Loss: 0.00001328
Iteration 116/1000 | Loss: 0.00001328
Iteration 117/1000 | Loss: 0.00001327
Iteration 118/1000 | Loss: 0.00001327
Iteration 119/1000 | Loss: 0.00001327
Iteration 120/1000 | Loss: 0.00001327
Iteration 121/1000 | Loss: 0.00001327
Iteration 122/1000 | Loss: 0.00001326
Iteration 123/1000 | Loss: 0.00001326
Iteration 124/1000 | Loss: 0.00001326
Iteration 125/1000 | Loss: 0.00001326
Iteration 126/1000 | Loss: 0.00001326
Iteration 127/1000 | Loss: 0.00001326
Iteration 128/1000 | Loss: 0.00001326
Iteration 129/1000 | Loss: 0.00001326
Iteration 130/1000 | Loss: 0.00001326
Iteration 131/1000 | Loss: 0.00001326
Iteration 132/1000 | Loss: 0.00001326
Iteration 133/1000 | Loss: 0.00001326
Iteration 134/1000 | Loss: 0.00001326
Iteration 135/1000 | Loss: 0.00001326
Iteration 136/1000 | Loss: 0.00001326
Iteration 137/1000 | Loss: 0.00001326
Iteration 138/1000 | Loss: 0.00001326
Iteration 139/1000 | Loss: 0.00001326
Iteration 140/1000 | Loss: 0.00001326
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.325641915173037e-05, 1.325641915173037e-05, 1.325641915173037e-05, 1.325641915173037e-05, 1.325641915173037e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.325641915173037e-05

Optimization complete. Final v2v error: 3.143763780593872 mm

Highest mean error: 3.3067760467529297 mm for frame 247

Lowest mean error: 2.9972386360168457 mm for frame 224

Saving results

Total time: 44.1393940448761
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carina_posed_010/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00750073
Iteration 2/25 | Loss: 0.00144146
Iteration 3/25 | Loss: 0.00135061
Iteration 4/25 | Loss: 0.00133819
Iteration 5/25 | Loss: 0.00133488
Iteration 6/25 | Loss: 0.00133436
Iteration 7/25 | Loss: 0.00133436
Iteration 8/25 | Loss: 0.00133436
Iteration 9/25 | Loss: 0.00133436
Iteration 10/25 | Loss: 0.00133436
Iteration 11/25 | Loss: 0.00133436
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013343575410544872, 0.0013343575410544872, 0.0013343575410544872, 0.0013343575410544872, 0.0013343575410544872]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013343575410544872

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.81995189
Iteration 2/25 | Loss: 0.00189770
Iteration 3/25 | Loss: 0.00189770
Iteration 4/25 | Loss: 0.00189770
Iteration 5/25 | Loss: 0.00189770
Iteration 6/25 | Loss: 0.00189770
Iteration 7/25 | Loss: 0.00189770
Iteration 8/25 | Loss: 0.00189770
Iteration 9/25 | Loss: 0.00189770
Iteration 10/25 | Loss: 0.00189770
Iteration 11/25 | Loss: 0.00189770
Iteration 12/25 | Loss: 0.00189770
Iteration 13/25 | Loss: 0.00189770
Iteration 14/25 | Loss: 0.00189770
Iteration 15/25 | Loss: 0.00189770
Iteration 16/25 | Loss: 0.00189770
Iteration 17/25 | Loss: 0.00189770
Iteration 18/25 | Loss: 0.00189770
Iteration 19/25 | Loss: 0.00189770
Iteration 20/25 | Loss: 0.00189770
Iteration 21/25 | Loss: 0.00189770
Iteration 22/25 | Loss: 0.00189770
Iteration 23/25 | Loss: 0.00189770
Iteration 24/25 | Loss: 0.00189770
Iteration 25/25 | Loss: 0.00189770

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00189770
Iteration 2/1000 | Loss: 0.00002441
Iteration 3/1000 | Loss: 0.00001789
Iteration 4/1000 | Loss: 0.00001622
Iteration 5/1000 | Loss: 0.00001524
Iteration 6/1000 | Loss: 0.00001443
Iteration 7/1000 | Loss: 0.00001380
Iteration 8/1000 | Loss: 0.00001344
Iteration 9/1000 | Loss: 0.00001302
Iteration 10/1000 | Loss: 0.00001274
Iteration 11/1000 | Loss: 0.00001262
Iteration 12/1000 | Loss: 0.00001243
Iteration 13/1000 | Loss: 0.00001233
Iteration 14/1000 | Loss: 0.00001225
Iteration 15/1000 | Loss: 0.00001222
Iteration 16/1000 | Loss: 0.00001221
Iteration 17/1000 | Loss: 0.00001217
Iteration 18/1000 | Loss: 0.00001211
Iteration 19/1000 | Loss: 0.00001210
Iteration 20/1000 | Loss: 0.00001203
Iteration 21/1000 | Loss: 0.00001201
Iteration 22/1000 | Loss: 0.00001201
Iteration 23/1000 | Loss: 0.00001192
Iteration 24/1000 | Loss: 0.00001189
Iteration 25/1000 | Loss: 0.00001188
Iteration 26/1000 | Loss: 0.00001186
Iteration 27/1000 | Loss: 0.00001181
Iteration 28/1000 | Loss: 0.00001181
Iteration 29/1000 | Loss: 0.00001181
Iteration 30/1000 | Loss: 0.00001180
Iteration 31/1000 | Loss: 0.00001179
Iteration 32/1000 | Loss: 0.00001179
Iteration 33/1000 | Loss: 0.00001178
Iteration 34/1000 | Loss: 0.00001176
Iteration 35/1000 | Loss: 0.00001174
Iteration 36/1000 | Loss: 0.00001174
Iteration 37/1000 | Loss: 0.00001174
Iteration 38/1000 | Loss: 0.00001174
Iteration 39/1000 | Loss: 0.00001174
Iteration 40/1000 | Loss: 0.00001174
Iteration 41/1000 | Loss: 0.00001174
Iteration 42/1000 | Loss: 0.00001172
Iteration 43/1000 | Loss: 0.00001172
Iteration 44/1000 | Loss: 0.00001171
Iteration 45/1000 | Loss: 0.00001170
Iteration 46/1000 | Loss: 0.00001169
Iteration 47/1000 | Loss: 0.00001169
Iteration 48/1000 | Loss: 0.00001169
Iteration 49/1000 | Loss: 0.00001168
Iteration 50/1000 | Loss: 0.00001168
Iteration 51/1000 | Loss: 0.00001162
Iteration 52/1000 | Loss: 0.00001162
Iteration 53/1000 | Loss: 0.00001161
Iteration 54/1000 | Loss: 0.00001161
Iteration 55/1000 | Loss: 0.00001160
Iteration 56/1000 | Loss: 0.00001160
Iteration 57/1000 | Loss: 0.00001159
Iteration 58/1000 | Loss: 0.00001159
Iteration 59/1000 | Loss: 0.00001158
Iteration 60/1000 | Loss: 0.00001158
Iteration 61/1000 | Loss: 0.00001158
Iteration 62/1000 | Loss: 0.00001158
Iteration 63/1000 | Loss: 0.00001157
Iteration 64/1000 | Loss: 0.00001157
Iteration 65/1000 | Loss: 0.00001157
Iteration 66/1000 | Loss: 0.00001154
Iteration 67/1000 | Loss: 0.00001154
Iteration 68/1000 | Loss: 0.00001154
Iteration 69/1000 | Loss: 0.00001154
Iteration 70/1000 | Loss: 0.00001153
Iteration 71/1000 | Loss: 0.00001153
Iteration 72/1000 | Loss: 0.00001153
Iteration 73/1000 | Loss: 0.00001153
Iteration 74/1000 | Loss: 0.00001153
Iteration 75/1000 | Loss: 0.00001153
Iteration 76/1000 | Loss: 0.00001153
Iteration 77/1000 | Loss: 0.00001153
Iteration 78/1000 | Loss: 0.00001153
Iteration 79/1000 | Loss: 0.00001153
Iteration 80/1000 | Loss: 0.00001153
Iteration 81/1000 | Loss: 0.00001153
Iteration 82/1000 | Loss: 0.00001153
Iteration 83/1000 | Loss: 0.00001153
Iteration 84/1000 | Loss: 0.00001153
Iteration 85/1000 | Loss: 0.00001153
Iteration 86/1000 | Loss: 0.00001153
Iteration 87/1000 | Loss: 0.00001153
Iteration 88/1000 | Loss: 0.00001153
Iteration 89/1000 | Loss: 0.00001153
Iteration 90/1000 | Loss: 0.00001153
Iteration 91/1000 | Loss: 0.00001153
Iteration 92/1000 | Loss: 0.00001153
Iteration 93/1000 | Loss: 0.00001153
Iteration 94/1000 | Loss: 0.00001153
Iteration 95/1000 | Loss: 0.00001153
Iteration 96/1000 | Loss: 0.00001153
Iteration 97/1000 | Loss: 0.00001153
Iteration 98/1000 | Loss: 0.00001153
Iteration 99/1000 | Loss: 0.00001153
Iteration 100/1000 | Loss: 0.00001153
Iteration 101/1000 | Loss: 0.00001153
Iteration 102/1000 | Loss: 0.00001153
Iteration 103/1000 | Loss: 0.00001153
Iteration 104/1000 | Loss: 0.00001153
Iteration 105/1000 | Loss: 0.00001153
Iteration 106/1000 | Loss: 0.00001153
Iteration 107/1000 | Loss: 0.00001153
Iteration 108/1000 | Loss: 0.00001153
Iteration 109/1000 | Loss: 0.00001153
Iteration 110/1000 | Loss: 0.00001153
Iteration 111/1000 | Loss: 0.00001153
Iteration 112/1000 | Loss: 0.00001153
Iteration 113/1000 | Loss: 0.00001153
Iteration 114/1000 | Loss: 0.00001153
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [1.1525509762577713e-05, 1.1525509762577713e-05, 1.1525509762577713e-05, 1.1525509762577713e-05, 1.1525509762577713e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1525509762577713e-05

Optimization complete. Final v2v error: 2.973759412765503 mm

Highest mean error: 3.140258550643921 mm for frame 39

Lowest mean error: 2.8639655113220215 mm for frame 1

Saving results

Total time: 36.97590661048889
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carina_posed_010/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00944342
Iteration 2/25 | Loss: 0.00340279
Iteration 3/25 | Loss: 0.00230378
Iteration 4/25 | Loss: 0.00207600
Iteration 5/25 | Loss: 0.00196999
Iteration 6/25 | Loss: 0.00191004
Iteration 7/25 | Loss: 0.00200893
Iteration 8/25 | Loss: 0.00180950
Iteration 9/25 | Loss: 0.00173199
Iteration 10/25 | Loss: 0.00169269
Iteration 11/25 | Loss: 0.00167377
Iteration 12/25 | Loss: 0.00164929
Iteration 13/25 | Loss: 0.00164811
Iteration 14/25 | Loss: 0.00161013
Iteration 15/25 | Loss: 0.00160987
Iteration 16/25 | Loss: 0.00160540
Iteration 17/25 | Loss: 0.00160381
Iteration 18/25 | Loss: 0.00160032
Iteration 19/25 | Loss: 0.00159810
Iteration 20/25 | Loss: 0.00159638
Iteration 21/25 | Loss: 0.00159576
Iteration 22/25 | Loss: 0.00159556
Iteration 23/25 | Loss: 0.00159652
Iteration 24/25 | Loss: 0.00159536
Iteration 25/25 | Loss: 0.00159536

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25243175
Iteration 2/25 | Loss: 0.00508495
Iteration 3/25 | Loss: 0.00474149
Iteration 4/25 | Loss: 0.00474149
Iteration 5/25 | Loss: 0.00474148
Iteration 6/25 | Loss: 0.00474148
Iteration 7/25 | Loss: 0.00474148
Iteration 8/25 | Loss: 0.00474148
Iteration 9/25 | Loss: 0.00474148
Iteration 10/25 | Loss: 0.00474148
Iteration 11/25 | Loss: 0.00474148
Iteration 12/25 | Loss: 0.00474148
Iteration 13/25 | Loss: 0.00474148
Iteration 14/25 | Loss: 0.00474148
Iteration 15/25 | Loss: 0.00474148
Iteration 16/25 | Loss: 0.00474148
Iteration 17/25 | Loss: 0.00474148
Iteration 18/25 | Loss: 0.00474148
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.004741481505334377, 0.004741481505334377, 0.004741481505334377, 0.004741481505334377, 0.004741481505334377]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004741481505334377

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00474148
Iteration 2/1000 | Loss: 0.00175215
Iteration 3/1000 | Loss: 0.00077976
Iteration 4/1000 | Loss: 0.00262061
Iteration 5/1000 | Loss: 0.00030524
Iteration 6/1000 | Loss: 0.00026280
Iteration 7/1000 | Loss: 0.00057091
Iteration 8/1000 | Loss: 0.00029886
Iteration 9/1000 | Loss: 0.00019552
Iteration 10/1000 | Loss: 0.00020003
Iteration 11/1000 | Loss: 0.00011715
Iteration 12/1000 | Loss: 0.00040480
Iteration 13/1000 | Loss: 0.00037190
Iteration 14/1000 | Loss: 0.00012542
Iteration 15/1000 | Loss: 0.00027353
Iteration 16/1000 | Loss: 0.00004791
Iteration 17/1000 | Loss: 0.00008063
Iteration 18/1000 | Loss: 0.00007213
Iteration 19/1000 | Loss: 0.00064252
Iteration 20/1000 | Loss: 0.00003759
Iteration 21/1000 | Loss: 0.00004392
Iteration 22/1000 | Loss: 0.00004056
Iteration 23/1000 | Loss: 0.00004059
Iteration 24/1000 | Loss: 0.00005538
Iteration 25/1000 | Loss: 0.00003002
Iteration 26/1000 | Loss: 0.00003181
Iteration 27/1000 | Loss: 0.00003839
Iteration 28/1000 | Loss: 0.00008902
Iteration 29/1000 | Loss: 0.00004686
Iteration 30/1000 | Loss: 0.00011654
Iteration 31/1000 | Loss: 0.00003321
Iteration 32/1000 | Loss: 0.00002723
Iteration 33/1000 | Loss: 0.00004358
Iteration 34/1000 | Loss: 0.00002945
Iteration 35/1000 | Loss: 0.00003593
Iteration 36/1000 | Loss: 0.00003961
Iteration 37/1000 | Loss: 0.00004234
Iteration 38/1000 | Loss: 0.00002791
Iteration 39/1000 | Loss: 0.00003946
Iteration 40/1000 | Loss: 0.00002517
Iteration 41/1000 | Loss: 0.00002658
Iteration 42/1000 | Loss: 0.00003489
Iteration 43/1000 | Loss: 0.00004311
Iteration 44/1000 | Loss: 0.00002520
Iteration 45/1000 | Loss: 0.00006733
Iteration 46/1000 | Loss: 0.00004438
Iteration 47/1000 | Loss: 0.00007781
Iteration 48/1000 | Loss: 0.00002669
Iteration 49/1000 | Loss: 0.00003017
Iteration 50/1000 | Loss: 0.00004207
Iteration 51/1000 | Loss: 0.00002587
Iteration 52/1000 | Loss: 0.00002382
Iteration 53/1000 | Loss: 0.00002381
Iteration 54/1000 | Loss: 0.00002380
Iteration 55/1000 | Loss: 0.00004698
Iteration 56/1000 | Loss: 0.00003483
Iteration 57/1000 | Loss: 0.00002988
Iteration 58/1000 | Loss: 0.00003783
Iteration 59/1000 | Loss: 0.00002586
Iteration 60/1000 | Loss: 0.00002396
Iteration 61/1000 | Loss: 0.00002332
Iteration 62/1000 | Loss: 0.00004030
Iteration 63/1000 | Loss: 0.00004039
Iteration 64/1000 | Loss: 0.00002426
Iteration 65/1000 | Loss: 0.00002412
Iteration 66/1000 | Loss: 0.00002301
Iteration 67/1000 | Loss: 0.00002301
Iteration 68/1000 | Loss: 0.00002301
Iteration 69/1000 | Loss: 0.00002300
Iteration 70/1000 | Loss: 0.00002299
Iteration 71/1000 | Loss: 0.00002299
Iteration 72/1000 | Loss: 0.00002299
Iteration 73/1000 | Loss: 0.00002299
Iteration 74/1000 | Loss: 0.00002299
Iteration 75/1000 | Loss: 0.00002299
Iteration 76/1000 | Loss: 0.00002299
Iteration 77/1000 | Loss: 0.00002298
Iteration 78/1000 | Loss: 0.00002298
Iteration 79/1000 | Loss: 0.00002297
Iteration 80/1000 | Loss: 0.00002296
Iteration 81/1000 | Loss: 0.00002296
Iteration 82/1000 | Loss: 0.00002296
Iteration 83/1000 | Loss: 0.00002296
Iteration 84/1000 | Loss: 0.00002296
Iteration 85/1000 | Loss: 0.00002296
Iteration 86/1000 | Loss: 0.00002295
Iteration 87/1000 | Loss: 0.00002295
Iteration 88/1000 | Loss: 0.00002295
Iteration 89/1000 | Loss: 0.00002295
Iteration 90/1000 | Loss: 0.00002294
Iteration 91/1000 | Loss: 0.00002294
Iteration 92/1000 | Loss: 0.00002293
Iteration 93/1000 | Loss: 0.00002293
Iteration 94/1000 | Loss: 0.00002293
Iteration 95/1000 | Loss: 0.00002292
Iteration 96/1000 | Loss: 0.00002292
Iteration 97/1000 | Loss: 0.00002292
Iteration 98/1000 | Loss: 0.00002292
Iteration 99/1000 | Loss: 0.00002292
Iteration 100/1000 | Loss: 0.00002292
Iteration 101/1000 | Loss: 0.00002291
Iteration 102/1000 | Loss: 0.00002291
Iteration 103/1000 | Loss: 0.00002291
Iteration 104/1000 | Loss: 0.00002291
Iteration 105/1000 | Loss: 0.00002290
Iteration 106/1000 | Loss: 0.00002290
Iteration 107/1000 | Loss: 0.00002290
Iteration 108/1000 | Loss: 0.00002289
Iteration 109/1000 | Loss: 0.00002289
Iteration 110/1000 | Loss: 0.00002289
Iteration 111/1000 | Loss: 0.00002288
Iteration 112/1000 | Loss: 0.00002288
Iteration 113/1000 | Loss: 0.00002287
Iteration 114/1000 | Loss: 0.00002287
Iteration 115/1000 | Loss: 0.00002287
Iteration 116/1000 | Loss: 0.00002286
Iteration 117/1000 | Loss: 0.00002286
Iteration 118/1000 | Loss: 0.00002286
Iteration 119/1000 | Loss: 0.00002286
Iteration 120/1000 | Loss: 0.00002286
Iteration 121/1000 | Loss: 0.00002285
Iteration 122/1000 | Loss: 0.00002285
Iteration 123/1000 | Loss: 0.00002285
Iteration 124/1000 | Loss: 0.00002285
Iteration 125/1000 | Loss: 0.00002284
Iteration 126/1000 | Loss: 0.00002284
Iteration 127/1000 | Loss: 0.00002283
Iteration 128/1000 | Loss: 0.00002283
Iteration 129/1000 | Loss: 0.00002707
Iteration 130/1000 | Loss: 0.00003618
Iteration 131/1000 | Loss: 0.00003089
Iteration 132/1000 | Loss: 0.00004096
Iteration 133/1000 | Loss: 0.00003076
Iteration 134/1000 | Loss: 0.00002325
Iteration 135/1000 | Loss: 0.00002288
Iteration 136/1000 | Loss: 0.00002277
Iteration 137/1000 | Loss: 0.00002277
Iteration 138/1000 | Loss: 0.00002276
Iteration 139/1000 | Loss: 0.00002276
Iteration 140/1000 | Loss: 0.00002276
Iteration 141/1000 | Loss: 0.00002276
Iteration 142/1000 | Loss: 0.00002276
Iteration 143/1000 | Loss: 0.00002512
Iteration 144/1000 | Loss: 0.00002275
Iteration 145/1000 | Loss: 0.00002275
Iteration 146/1000 | Loss: 0.00002275
Iteration 147/1000 | Loss: 0.00002275
Iteration 148/1000 | Loss: 0.00002275
Iteration 149/1000 | Loss: 0.00002275
Iteration 150/1000 | Loss: 0.00002275
Iteration 151/1000 | Loss: 0.00002275
Iteration 152/1000 | Loss: 0.00002275
Iteration 153/1000 | Loss: 0.00002275
Iteration 154/1000 | Loss: 0.00002274
Iteration 155/1000 | Loss: 0.00002274
Iteration 156/1000 | Loss: 0.00002274
Iteration 157/1000 | Loss: 0.00002274
Iteration 158/1000 | Loss: 0.00002274
Iteration 159/1000 | Loss: 0.00002274
Iteration 160/1000 | Loss: 0.00002274
Iteration 161/1000 | Loss: 0.00002274
Iteration 162/1000 | Loss: 0.00002274
Iteration 163/1000 | Loss: 0.00002274
Iteration 164/1000 | Loss: 0.00002274
Iteration 165/1000 | Loss: 0.00002274
Iteration 166/1000 | Loss: 0.00002274
Iteration 167/1000 | Loss: 0.00002274
Iteration 168/1000 | Loss: 0.00002274
Iteration 169/1000 | Loss: 0.00002274
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [2.274053440487478e-05, 2.274053440487478e-05, 2.274053440487478e-05, 2.274053440487478e-05, 2.274053440487478e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.274053440487478e-05

Optimization complete. Final v2v error: 3.4029502868652344 mm

Highest mean error: 10.898950576782227 mm for frame 10

Lowest mean error: 2.8768136501312256 mm for frame 95

Saving results

Total time: 160.2889063358307
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carina_posed_010/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00404649
Iteration 2/25 | Loss: 0.00148814
Iteration 3/25 | Loss: 0.00138751
Iteration 4/25 | Loss: 0.00137554
Iteration 5/25 | Loss: 0.00137304
Iteration 6/25 | Loss: 0.00137293
Iteration 7/25 | Loss: 0.00137293
Iteration 8/25 | Loss: 0.00137293
Iteration 9/25 | Loss: 0.00137293
Iteration 10/25 | Loss: 0.00137293
Iteration 11/25 | Loss: 0.00137293
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013729281490668654, 0.0013729281490668654, 0.0013729281490668654, 0.0013729281490668654, 0.0013729281490668654]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013729281490668654

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27223134
Iteration 2/25 | Loss: 0.00182898
Iteration 3/25 | Loss: 0.00182897
Iteration 4/25 | Loss: 0.00182897
Iteration 5/25 | Loss: 0.00182897
Iteration 6/25 | Loss: 0.00182897
Iteration 7/25 | Loss: 0.00182897
Iteration 8/25 | Loss: 0.00182897
Iteration 9/25 | Loss: 0.00182897
Iteration 10/25 | Loss: 0.00182897
Iteration 11/25 | Loss: 0.00182897
Iteration 12/25 | Loss: 0.00182897
Iteration 13/25 | Loss: 0.00182897
Iteration 14/25 | Loss: 0.00182897
Iteration 15/25 | Loss: 0.00182897
Iteration 16/25 | Loss: 0.00182897
Iteration 17/25 | Loss: 0.00182897
Iteration 18/25 | Loss: 0.00182897
Iteration 19/25 | Loss: 0.00182897
Iteration 20/25 | Loss: 0.00182897
Iteration 21/25 | Loss: 0.00182897
Iteration 22/25 | Loss: 0.00182897
Iteration 23/25 | Loss: 0.00182897
Iteration 24/25 | Loss: 0.00182897
Iteration 25/25 | Loss: 0.00182897

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00182897
Iteration 2/1000 | Loss: 0.00003281
Iteration 3/1000 | Loss: 0.00002257
Iteration 4/1000 | Loss: 0.00002020
Iteration 5/1000 | Loss: 0.00001920
Iteration 6/1000 | Loss: 0.00001845
Iteration 7/1000 | Loss: 0.00001802
Iteration 8/1000 | Loss: 0.00001765
Iteration 9/1000 | Loss: 0.00001731
Iteration 10/1000 | Loss: 0.00001696
Iteration 11/1000 | Loss: 0.00001670
Iteration 12/1000 | Loss: 0.00001643
Iteration 13/1000 | Loss: 0.00001622
Iteration 14/1000 | Loss: 0.00001611
Iteration 15/1000 | Loss: 0.00001609
Iteration 16/1000 | Loss: 0.00001605
Iteration 17/1000 | Loss: 0.00001604
Iteration 18/1000 | Loss: 0.00001602
Iteration 19/1000 | Loss: 0.00001601
Iteration 20/1000 | Loss: 0.00001600
Iteration 21/1000 | Loss: 0.00001595
Iteration 22/1000 | Loss: 0.00001594
Iteration 23/1000 | Loss: 0.00001585
Iteration 24/1000 | Loss: 0.00001585
Iteration 25/1000 | Loss: 0.00001582
Iteration 26/1000 | Loss: 0.00001582
Iteration 27/1000 | Loss: 0.00001580
Iteration 28/1000 | Loss: 0.00001579
Iteration 29/1000 | Loss: 0.00001579
Iteration 30/1000 | Loss: 0.00001579
Iteration 31/1000 | Loss: 0.00001578
Iteration 32/1000 | Loss: 0.00001577
Iteration 33/1000 | Loss: 0.00001577
Iteration 34/1000 | Loss: 0.00001577
Iteration 35/1000 | Loss: 0.00001576
Iteration 36/1000 | Loss: 0.00001576
Iteration 37/1000 | Loss: 0.00001575
Iteration 38/1000 | Loss: 0.00001574
Iteration 39/1000 | Loss: 0.00001570
Iteration 40/1000 | Loss: 0.00001570
Iteration 41/1000 | Loss: 0.00001569
Iteration 42/1000 | Loss: 0.00001567
Iteration 43/1000 | Loss: 0.00001566
Iteration 44/1000 | Loss: 0.00001565
Iteration 45/1000 | Loss: 0.00001564
Iteration 46/1000 | Loss: 0.00001564
Iteration 47/1000 | Loss: 0.00001563
Iteration 48/1000 | Loss: 0.00001562
Iteration 49/1000 | Loss: 0.00001561
Iteration 50/1000 | Loss: 0.00001560
Iteration 51/1000 | Loss: 0.00001560
Iteration 52/1000 | Loss: 0.00001560
Iteration 53/1000 | Loss: 0.00001559
Iteration 54/1000 | Loss: 0.00001559
Iteration 55/1000 | Loss: 0.00001558
Iteration 56/1000 | Loss: 0.00001558
Iteration 57/1000 | Loss: 0.00001556
Iteration 58/1000 | Loss: 0.00001555
Iteration 59/1000 | Loss: 0.00001548
Iteration 60/1000 | Loss: 0.00001548
Iteration 61/1000 | Loss: 0.00001547
Iteration 62/1000 | Loss: 0.00001547
Iteration 63/1000 | Loss: 0.00001546
Iteration 64/1000 | Loss: 0.00001546
Iteration 65/1000 | Loss: 0.00001546
Iteration 66/1000 | Loss: 0.00001546
Iteration 67/1000 | Loss: 0.00001545
Iteration 68/1000 | Loss: 0.00001545
Iteration 69/1000 | Loss: 0.00001545
Iteration 70/1000 | Loss: 0.00001545
Iteration 71/1000 | Loss: 0.00001545
Iteration 72/1000 | Loss: 0.00001545
Iteration 73/1000 | Loss: 0.00001545
Iteration 74/1000 | Loss: 0.00001545
Iteration 75/1000 | Loss: 0.00001545
Iteration 76/1000 | Loss: 0.00001545
Iteration 77/1000 | Loss: 0.00001544
Iteration 78/1000 | Loss: 0.00001544
Iteration 79/1000 | Loss: 0.00001544
Iteration 80/1000 | Loss: 0.00001544
Iteration 81/1000 | Loss: 0.00001543
Iteration 82/1000 | Loss: 0.00001543
Iteration 83/1000 | Loss: 0.00001543
Iteration 84/1000 | Loss: 0.00001543
Iteration 85/1000 | Loss: 0.00001543
Iteration 86/1000 | Loss: 0.00001542
Iteration 87/1000 | Loss: 0.00001542
Iteration 88/1000 | Loss: 0.00001542
Iteration 89/1000 | Loss: 0.00001542
Iteration 90/1000 | Loss: 0.00001542
Iteration 91/1000 | Loss: 0.00001541
Iteration 92/1000 | Loss: 0.00001541
Iteration 93/1000 | Loss: 0.00001541
Iteration 94/1000 | Loss: 0.00001541
Iteration 95/1000 | Loss: 0.00001541
Iteration 96/1000 | Loss: 0.00001541
Iteration 97/1000 | Loss: 0.00001541
Iteration 98/1000 | Loss: 0.00001541
Iteration 99/1000 | Loss: 0.00001541
Iteration 100/1000 | Loss: 0.00001541
Iteration 101/1000 | Loss: 0.00001540
Iteration 102/1000 | Loss: 0.00001540
Iteration 103/1000 | Loss: 0.00001540
Iteration 104/1000 | Loss: 0.00001540
Iteration 105/1000 | Loss: 0.00001540
Iteration 106/1000 | Loss: 0.00001540
Iteration 107/1000 | Loss: 0.00001540
Iteration 108/1000 | Loss: 0.00001539
Iteration 109/1000 | Loss: 0.00001539
Iteration 110/1000 | Loss: 0.00001539
Iteration 111/1000 | Loss: 0.00001539
Iteration 112/1000 | Loss: 0.00001539
Iteration 113/1000 | Loss: 0.00001539
Iteration 114/1000 | Loss: 0.00001539
Iteration 115/1000 | Loss: 0.00001539
Iteration 116/1000 | Loss: 0.00001539
Iteration 117/1000 | Loss: 0.00001539
Iteration 118/1000 | Loss: 0.00001539
Iteration 119/1000 | Loss: 0.00001539
Iteration 120/1000 | Loss: 0.00001539
Iteration 121/1000 | Loss: 0.00001539
Iteration 122/1000 | Loss: 0.00001539
Iteration 123/1000 | Loss: 0.00001539
Iteration 124/1000 | Loss: 0.00001539
Iteration 125/1000 | Loss: 0.00001539
Iteration 126/1000 | Loss: 0.00001539
Iteration 127/1000 | Loss: 0.00001539
Iteration 128/1000 | Loss: 0.00001539
Iteration 129/1000 | Loss: 0.00001539
Iteration 130/1000 | Loss: 0.00001539
Iteration 131/1000 | Loss: 0.00001539
Iteration 132/1000 | Loss: 0.00001539
Iteration 133/1000 | Loss: 0.00001539
Iteration 134/1000 | Loss: 0.00001539
Iteration 135/1000 | Loss: 0.00001539
Iteration 136/1000 | Loss: 0.00001539
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [1.5388315659947693e-05, 1.5388315659947693e-05, 1.5388315659947693e-05, 1.5388315659947693e-05, 1.5388315659947693e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5388315659947693e-05

Optimization complete. Final v2v error: 3.288893938064575 mm

Highest mean error: 3.5032036304473877 mm for frame 21

Lowest mean error: 2.959045171737671 mm for frame 65

Saving results

Total time: 38.45190644264221
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carina_posed_010/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00855417
Iteration 2/25 | Loss: 0.00176527
Iteration 3/25 | Loss: 0.00151027
Iteration 4/25 | Loss: 0.00148466
Iteration 5/25 | Loss: 0.00147906
Iteration 6/25 | Loss: 0.00148264
Iteration 7/25 | Loss: 0.00146800
Iteration 8/25 | Loss: 0.00145882
Iteration 9/25 | Loss: 0.00145264
Iteration 10/25 | Loss: 0.00144523
Iteration 11/25 | Loss: 0.00144290
Iteration 12/25 | Loss: 0.00143140
Iteration 13/25 | Loss: 0.00142825
Iteration 14/25 | Loss: 0.00142774
Iteration 15/25 | Loss: 0.00142231
Iteration 16/25 | Loss: 0.00142141
Iteration 17/25 | Loss: 0.00142173
Iteration 18/25 | Loss: 0.00142009
Iteration 19/25 | Loss: 0.00141896
Iteration 20/25 | Loss: 0.00141873
Iteration 21/25 | Loss: 0.00141872
Iteration 22/25 | Loss: 0.00141872
Iteration 23/25 | Loss: 0.00141872
Iteration 24/25 | Loss: 0.00141869
Iteration 25/25 | Loss: 0.00141868

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.84768736
Iteration 2/25 | Loss: 0.00111953
Iteration 3/25 | Loss: 0.00111952
Iteration 4/25 | Loss: 0.00111952
Iteration 5/25 | Loss: 0.00111952
Iteration 6/25 | Loss: 0.00111952
Iteration 7/25 | Loss: 0.00111952
Iteration 8/25 | Loss: 0.00111952
Iteration 9/25 | Loss: 0.00111952
Iteration 10/25 | Loss: 0.00111952
Iteration 11/25 | Loss: 0.00111952
Iteration 12/25 | Loss: 0.00111952
Iteration 13/25 | Loss: 0.00111952
Iteration 14/25 | Loss: 0.00111952
Iteration 15/25 | Loss: 0.00111952
Iteration 16/25 | Loss: 0.00111952
Iteration 17/25 | Loss: 0.00111952
Iteration 18/25 | Loss: 0.00111952
Iteration 19/25 | Loss: 0.00111952
Iteration 20/25 | Loss: 0.00111952
Iteration 21/25 | Loss: 0.00111952
Iteration 22/25 | Loss: 0.00111952
Iteration 23/25 | Loss: 0.00111952
Iteration 24/25 | Loss: 0.00111952
Iteration 25/25 | Loss: 0.00111952

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111952
Iteration 2/1000 | Loss: 0.00004585
Iteration 3/1000 | Loss: 0.00003669
Iteration 4/1000 | Loss: 0.00003383
Iteration 5/1000 | Loss: 0.00003259
Iteration 6/1000 | Loss: 0.00003143
Iteration 7/1000 | Loss: 0.00003046
Iteration 8/1000 | Loss: 0.00002996
Iteration 9/1000 | Loss: 0.00002948
Iteration 10/1000 | Loss: 0.00002911
Iteration 11/1000 | Loss: 0.00002884
Iteration 12/1000 | Loss: 0.00002864
Iteration 13/1000 | Loss: 0.00002857
Iteration 14/1000 | Loss: 0.00002842
Iteration 15/1000 | Loss: 0.00002841
Iteration 16/1000 | Loss: 0.00002828
Iteration 17/1000 | Loss: 0.00002825
Iteration 18/1000 | Loss: 0.00002823
Iteration 19/1000 | Loss: 0.00002823
Iteration 20/1000 | Loss: 0.00002823
Iteration 21/1000 | Loss: 0.00002822
Iteration 22/1000 | Loss: 0.00002822
Iteration 23/1000 | Loss: 0.00002822
Iteration 24/1000 | Loss: 0.00002822
Iteration 25/1000 | Loss: 0.00002822
Iteration 26/1000 | Loss: 0.00002822
Iteration 27/1000 | Loss: 0.00002822
Iteration 28/1000 | Loss: 0.00002822
Iteration 29/1000 | Loss: 0.00002822
Iteration 30/1000 | Loss: 0.00002821
Iteration 31/1000 | Loss: 0.00002818
Iteration 32/1000 | Loss: 0.00002818
Iteration 33/1000 | Loss: 0.00002818
Iteration 34/1000 | Loss: 0.00002818
Iteration 35/1000 | Loss: 0.00002817
Iteration 36/1000 | Loss: 0.00002817
Iteration 37/1000 | Loss: 0.00002817
Iteration 38/1000 | Loss: 0.00002817
Iteration 39/1000 | Loss: 0.00002817
Iteration 40/1000 | Loss: 0.00002817
Iteration 41/1000 | Loss: 0.00002817
Iteration 42/1000 | Loss: 0.00002817
Iteration 43/1000 | Loss: 0.00002816
Iteration 44/1000 | Loss: 0.00002815
Iteration 45/1000 | Loss: 0.00002815
Iteration 46/1000 | Loss: 0.00002815
Iteration 47/1000 | Loss: 0.00002815
Iteration 48/1000 | Loss: 0.00002815
Iteration 49/1000 | Loss: 0.00002814
Iteration 50/1000 | Loss: 0.00002814
Iteration 51/1000 | Loss: 0.00002814
Iteration 52/1000 | Loss: 0.00002814
Iteration 53/1000 | Loss: 0.00002814
Iteration 54/1000 | Loss: 0.00002814
Iteration 55/1000 | Loss: 0.00002813
Iteration 56/1000 | Loss: 0.00002812
Iteration 57/1000 | Loss: 0.00002812
Iteration 58/1000 | Loss: 0.00002812
Iteration 59/1000 | Loss: 0.00002812
Iteration 60/1000 | Loss: 0.00002812
Iteration 61/1000 | Loss: 0.00002812
Iteration 62/1000 | Loss: 0.00002812
Iteration 63/1000 | Loss: 0.00002811
Iteration 64/1000 | Loss: 0.00002811
Iteration 65/1000 | Loss: 0.00002811
Iteration 66/1000 | Loss: 0.00002811
Iteration 67/1000 | Loss: 0.00002810
Iteration 68/1000 | Loss: 0.00002810
Iteration 69/1000 | Loss: 0.00002810
Iteration 70/1000 | Loss: 0.00002810
Iteration 71/1000 | Loss: 0.00002810
Iteration 72/1000 | Loss: 0.00002810
Iteration 73/1000 | Loss: 0.00002810
Iteration 74/1000 | Loss: 0.00002809
Iteration 75/1000 | Loss: 0.00002805
Iteration 76/1000 | Loss: 0.00002805
Iteration 77/1000 | Loss: 0.00002802
Iteration 78/1000 | Loss: 0.00002801
Iteration 79/1000 | Loss: 0.00002801
Iteration 80/1000 | Loss: 0.00002801
Iteration 81/1000 | Loss: 0.00002800
Iteration 82/1000 | Loss: 0.00002799
Iteration 83/1000 | Loss: 0.00002799
Iteration 84/1000 | Loss: 0.00002797
Iteration 85/1000 | Loss: 0.00002797
Iteration 86/1000 | Loss: 0.00002797
Iteration 87/1000 | Loss: 0.00002797
Iteration 88/1000 | Loss: 0.00002796
Iteration 89/1000 | Loss: 0.00002796
Iteration 90/1000 | Loss: 0.00002796
Iteration 91/1000 | Loss: 0.00002796
Iteration 92/1000 | Loss: 0.00002796
Iteration 93/1000 | Loss: 0.00002795
Iteration 94/1000 | Loss: 0.00002795
Iteration 95/1000 | Loss: 0.00002795
Iteration 96/1000 | Loss: 0.00002795
Iteration 97/1000 | Loss: 0.00002795
Iteration 98/1000 | Loss: 0.00002795
Iteration 99/1000 | Loss: 0.00002795
Iteration 100/1000 | Loss: 0.00002795
Iteration 101/1000 | Loss: 0.00002794
Iteration 102/1000 | Loss: 0.00002794
Iteration 103/1000 | Loss: 0.00002794
Iteration 104/1000 | Loss: 0.00002794
Iteration 105/1000 | Loss: 0.00002794
Iteration 106/1000 | Loss: 0.00002794
Iteration 107/1000 | Loss: 0.00002794
Iteration 108/1000 | Loss: 0.00002793
Iteration 109/1000 | Loss: 0.00002793
Iteration 110/1000 | Loss: 0.00002793
Iteration 111/1000 | Loss: 0.00002793
Iteration 112/1000 | Loss: 0.00002793
Iteration 113/1000 | Loss: 0.00002793
Iteration 114/1000 | Loss: 0.00002793
Iteration 115/1000 | Loss: 0.00002793
Iteration 116/1000 | Loss: 0.00002793
Iteration 117/1000 | Loss: 0.00002793
Iteration 118/1000 | Loss: 0.00002793
Iteration 119/1000 | Loss: 0.00002793
Iteration 120/1000 | Loss: 0.00002793
Iteration 121/1000 | Loss: 0.00002793
Iteration 122/1000 | Loss: 0.00002793
Iteration 123/1000 | Loss: 0.00002793
Iteration 124/1000 | Loss: 0.00002793
Iteration 125/1000 | Loss: 0.00002793
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [2.79324158327654e-05, 2.79324158327654e-05, 2.79324158327654e-05, 2.79324158327654e-05, 2.79324158327654e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.79324158327654e-05

Optimization complete. Final v2v error: 4.492367267608643 mm

Highest mean error: 4.766611576080322 mm for frame 146

Lowest mean error: 4.36785888671875 mm for frame 39

Saving results

Total time: 64.23344016075134
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carina_posed_010/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00460628
Iteration 2/25 | Loss: 0.00147450
Iteration 3/25 | Loss: 0.00139477
Iteration 4/25 | Loss: 0.00138168
Iteration 5/25 | Loss: 0.00137766
Iteration 6/25 | Loss: 0.00137743
Iteration 7/25 | Loss: 0.00137743
Iteration 8/25 | Loss: 0.00137743
Iteration 9/25 | Loss: 0.00137743
Iteration 10/25 | Loss: 0.00137743
Iteration 11/25 | Loss: 0.00137743
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013774281833320856, 0.0013774281833320856, 0.0013774281833320856, 0.0013774281833320856, 0.0013774281833320856]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013774281833320856

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32843328
Iteration 2/25 | Loss: 0.00182792
Iteration 3/25 | Loss: 0.00182792
Iteration 4/25 | Loss: 0.00182792
Iteration 5/25 | Loss: 0.00182792
Iteration 6/25 | Loss: 0.00182792
Iteration 7/25 | Loss: 0.00182792
Iteration 8/25 | Loss: 0.00182792
Iteration 9/25 | Loss: 0.00182792
Iteration 10/25 | Loss: 0.00182792
Iteration 11/25 | Loss: 0.00182792
Iteration 12/25 | Loss: 0.00182792
Iteration 13/25 | Loss: 0.00182792
Iteration 14/25 | Loss: 0.00182792
Iteration 15/25 | Loss: 0.00182792
Iteration 16/25 | Loss: 0.00182792
Iteration 17/25 | Loss: 0.00182792
Iteration 18/25 | Loss: 0.00182792
Iteration 19/25 | Loss: 0.00182792
Iteration 20/25 | Loss: 0.00182792
Iteration 21/25 | Loss: 0.00182792
Iteration 22/25 | Loss: 0.00182792
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0018279199721291661, 0.0018279199721291661, 0.0018279199721291661, 0.0018279199721291661, 0.0018279199721291661]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018279199721291661

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00182792
Iteration 2/1000 | Loss: 0.00003547
Iteration 3/1000 | Loss: 0.00002780
Iteration 4/1000 | Loss: 0.00002515
Iteration 5/1000 | Loss: 0.00002385
Iteration 6/1000 | Loss: 0.00002297
Iteration 7/1000 | Loss: 0.00002217
Iteration 8/1000 | Loss: 0.00002172
Iteration 9/1000 | Loss: 0.00002123
Iteration 10/1000 | Loss: 0.00002074
Iteration 11/1000 | Loss: 0.00002038
Iteration 12/1000 | Loss: 0.00002004
Iteration 13/1000 | Loss: 0.00001974
Iteration 14/1000 | Loss: 0.00001951
Iteration 15/1000 | Loss: 0.00001946
Iteration 16/1000 | Loss: 0.00001937
Iteration 17/1000 | Loss: 0.00001919
Iteration 18/1000 | Loss: 0.00001915
Iteration 19/1000 | Loss: 0.00001914
Iteration 20/1000 | Loss: 0.00001913
Iteration 21/1000 | Loss: 0.00001913
Iteration 22/1000 | Loss: 0.00001912
Iteration 23/1000 | Loss: 0.00001903
Iteration 24/1000 | Loss: 0.00001903
Iteration 25/1000 | Loss: 0.00001897
Iteration 26/1000 | Loss: 0.00001896
Iteration 27/1000 | Loss: 0.00001895
Iteration 28/1000 | Loss: 0.00001895
Iteration 29/1000 | Loss: 0.00001894
Iteration 30/1000 | Loss: 0.00001889
Iteration 31/1000 | Loss: 0.00001888
Iteration 32/1000 | Loss: 0.00001887
Iteration 33/1000 | Loss: 0.00001886
Iteration 34/1000 | Loss: 0.00001885
Iteration 35/1000 | Loss: 0.00001885
Iteration 36/1000 | Loss: 0.00001885
Iteration 37/1000 | Loss: 0.00001884
Iteration 38/1000 | Loss: 0.00001884
Iteration 39/1000 | Loss: 0.00001884
Iteration 40/1000 | Loss: 0.00001884
Iteration 41/1000 | Loss: 0.00001883
Iteration 42/1000 | Loss: 0.00001883
Iteration 43/1000 | Loss: 0.00001882
Iteration 44/1000 | Loss: 0.00001881
Iteration 45/1000 | Loss: 0.00001881
Iteration 46/1000 | Loss: 0.00001881
Iteration 47/1000 | Loss: 0.00001880
Iteration 48/1000 | Loss: 0.00001880
Iteration 49/1000 | Loss: 0.00001879
Iteration 50/1000 | Loss: 0.00001879
Iteration 51/1000 | Loss: 0.00001879
Iteration 52/1000 | Loss: 0.00001879
Iteration 53/1000 | Loss: 0.00001879
Iteration 54/1000 | Loss: 0.00001879
Iteration 55/1000 | Loss: 0.00001879
Iteration 56/1000 | Loss: 0.00001879
Iteration 57/1000 | Loss: 0.00001879
Iteration 58/1000 | Loss: 0.00001879
Iteration 59/1000 | Loss: 0.00001878
Iteration 60/1000 | Loss: 0.00001878
Iteration 61/1000 | Loss: 0.00001878
Iteration 62/1000 | Loss: 0.00001878
Iteration 63/1000 | Loss: 0.00001878
Iteration 64/1000 | Loss: 0.00001878
Iteration 65/1000 | Loss: 0.00001878
Iteration 66/1000 | Loss: 0.00001878
Iteration 67/1000 | Loss: 0.00001877
Iteration 68/1000 | Loss: 0.00001877
Iteration 69/1000 | Loss: 0.00001877
Iteration 70/1000 | Loss: 0.00001877
Iteration 71/1000 | Loss: 0.00001876
Iteration 72/1000 | Loss: 0.00001876
Iteration 73/1000 | Loss: 0.00001876
Iteration 74/1000 | Loss: 0.00001876
Iteration 75/1000 | Loss: 0.00001876
Iteration 76/1000 | Loss: 0.00001876
Iteration 77/1000 | Loss: 0.00001876
Iteration 78/1000 | Loss: 0.00001876
Iteration 79/1000 | Loss: 0.00001876
Iteration 80/1000 | Loss: 0.00001876
Iteration 81/1000 | Loss: 0.00001876
Iteration 82/1000 | Loss: 0.00001875
Iteration 83/1000 | Loss: 0.00001875
Iteration 84/1000 | Loss: 0.00001875
Iteration 85/1000 | Loss: 0.00001875
Iteration 86/1000 | Loss: 0.00001875
Iteration 87/1000 | Loss: 0.00001875
Iteration 88/1000 | Loss: 0.00001875
Iteration 89/1000 | Loss: 0.00001875
Iteration 90/1000 | Loss: 0.00001875
Iteration 91/1000 | Loss: 0.00001874
Iteration 92/1000 | Loss: 0.00001874
Iteration 93/1000 | Loss: 0.00001874
Iteration 94/1000 | Loss: 0.00001874
Iteration 95/1000 | Loss: 0.00001873
Iteration 96/1000 | Loss: 0.00001873
Iteration 97/1000 | Loss: 0.00001873
Iteration 98/1000 | Loss: 0.00001873
Iteration 99/1000 | Loss: 0.00001873
Iteration 100/1000 | Loss: 0.00001873
Iteration 101/1000 | Loss: 0.00001873
Iteration 102/1000 | Loss: 0.00001873
Iteration 103/1000 | Loss: 0.00001872
Iteration 104/1000 | Loss: 0.00001872
Iteration 105/1000 | Loss: 0.00001872
Iteration 106/1000 | Loss: 0.00001872
Iteration 107/1000 | Loss: 0.00001872
Iteration 108/1000 | Loss: 0.00001872
Iteration 109/1000 | Loss: 0.00001872
Iteration 110/1000 | Loss: 0.00001872
Iteration 111/1000 | Loss: 0.00001872
Iteration 112/1000 | Loss: 0.00001872
Iteration 113/1000 | Loss: 0.00001872
Iteration 114/1000 | Loss: 0.00001872
Iteration 115/1000 | Loss: 0.00001872
Iteration 116/1000 | Loss: 0.00001872
Iteration 117/1000 | Loss: 0.00001872
Iteration 118/1000 | Loss: 0.00001872
Iteration 119/1000 | Loss: 0.00001872
Iteration 120/1000 | Loss: 0.00001872
Iteration 121/1000 | Loss: 0.00001872
Iteration 122/1000 | Loss: 0.00001872
Iteration 123/1000 | Loss: 0.00001872
Iteration 124/1000 | Loss: 0.00001872
Iteration 125/1000 | Loss: 0.00001872
Iteration 126/1000 | Loss: 0.00001872
Iteration 127/1000 | Loss: 0.00001872
Iteration 128/1000 | Loss: 0.00001872
Iteration 129/1000 | Loss: 0.00001872
Iteration 130/1000 | Loss: 0.00001872
Iteration 131/1000 | Loss: 0.00001872
Iteration 132/1000 | Loss: 0.00001872
Iteration 133/1000 | Loss: 0.00001872
Iteration 134/1000 | Loss: 0.00001872
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.8724702385952696e-05, 1.8724702385952696e-05, 1.8724702385952696e-05, 1.8724702385952696e-05, 1.8724702385952696e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8724702385952696e-05

Optimization complete. Final v2v error: 3.7309305667877197 mm

Highest mean error: 4.057053565979004 mm for frame 31

Lowest mean error: 3.4865212440490723 mm for frame 89

Saving results

Total time: 39.75188851356506
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carina_posed_010/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00829769
Iteration 2/25 | Loss: 0.00158950
Iteration 3/25 | Loss: 0.00145303
Iteration 4/25 | Loss: 0.00143153
Iteration 5/25 | Loss: 0.00142470
Iteration 6/25 | Loss: 0.00142316
Iteration 7/25 | Loss: 0.00142316
Iteration 8/25 | Loss: 0.00142316
Iteration 9/25 | Loss: 0.00142316
Iteration 10/25 | Loss: 0.00142316
Iteration 11/25 | Loss: 0.00142316
Iteration 12/25 | Loss: 0.00142316
Iteration 13/25 | Loss: 0.00142316
Iteration 14/25 | Loss: 0.00142316
Iteration 15/25 | Loss: 0.00142316
Iteration 16/25 | Loss: 0.00142316
Iteration 17/25 | Loss: 0.00142316
Iteration 18/25 | Loss: 0.00142316
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0014231643872335553, 0.0014231643872335553, 0.0014231643872335553, 0.0014231643872335553, 0.0014231643872335553]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014231643872335553

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.86732817
Iteration 2/25 | Loss: 0.00237474
Iteration 3/25 | Loss: 0.00237458
Iteration 4/25 | Loss: 0.00237458
Iteration 5/25 | Loss: 0.00237458
Iteration 6/25 | Loss: 0.00237458
Iteration 7/25 | Loss: 0.00237458
Iteration 8/25 | Loss: 0.00237458
Iteration 9/25 | Loss: 0.00237458
Iteration 10/25 | Loss: 0.00237458
Iteration 11/25 | Loss: 0.00237458
Iteration 12/25 | Loss: 0.00237458
Iteration 13/25 | Loss: 0.00237458
Iteration 14/25 | Loss: 0.00237458
Iteration 15/25 | Loss: 0.00237458
Iteration 16/25 | Loss: 0.00237458
Iteration 17/25 | Loss: 0.00237458
Iteration 18/25 | Loss: 0.00237458
Iteration 19/25 | Loss: 0.00237458
Iteration 20/25 | Loss: 0.00237458
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0023745798971503973, 0.0023745798971503973, 0.0023745798971503973, 0.0023745798971503973, 0.0023745798971503973]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023745798971503973

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00237458
Iteration 2/1000 | Loss: 0.00006476
Iteration 3/1000 | Loss: 0.00004484
Iteration 4/1000 | Loss: 0.00003876
Iteration 5/1000 | Loss: 0.00003562
Iteration 6/1000 | Loss: 0.00003367
Iteration 7/1000 | Loss: 0.00003233
Iteration 8/1000 | Loss: 0.00003134
Iteration 9/1000 | Loss: 0.00003056
Iteration 10/1000 | Loss: 0.00003007
Iteration 11/1000 | Loss: 0.00002959
Iteration 12/1000 | Loss: 0.00002916
Iteration 13/1000 | Loss: 0.00002883
Iteration 14/1000 | Loss: 0.00002857
Iteration 15/1000 | Loss: 0.00002837
Iteration 16/1000 | Loss: 0.00002819
Iteration 17/1000 | Loss: 0.00002813
Iteration 18/1000 | Loss: 0.00002796
Iteration 19/1000 | Loss: 0.00002793
Iteration 20/1000 | Loss: 0.00002793
Iteration 21/1000 | Loss: 0.00002789
Iteration 22/1000 | Loss: 0.00002784
Iteration 23/1000 | Loss: 0.00002780
Iteration 24/1000 | Loss: 0.00002779
Iteration 25/1000 | Loss: 0.00002771
Iteration 26/1000 | Loss: 0.00002770
Iteration 27/1000 | Loss: 0.00002766
Iteration 28/1000 | Loss: 0.00002765
Iteration 29/1000 | Loss: 0.00002765
Iteration 30/1000 | Loss: 0.00002764
Iteration 31/1000 | Loss: 0.00002764
Iteration 32/1000 | Loss: 0.00002763
Iteration 33/1000 | Loss: 0.00002757
Iteration 34/1000 | Loss: 0.00002751
Iteration 35/1000 | Loss: 0.00002751
Iteration 36/1000 | Loss: 0.00002747
Iteration 37/1000 | Loss: 0.00002747
Iteration 38/1000 | Loss: 0.00002747
Iteration 39/1000 | Loss: 0.00002747
Iteration 40/1000 | Loss: 0.00002747
Iteration 41/1000 | Loss: 0.00002746
Iteration 42/1000 | Loss: 0.00002746
Iteration 43/1000 | Loss: 0.00002746
Iteration 44/1000 | Loss: 0.00002745
Iteration 45/1000 | Loss: 0.00002745
Iteration 46/1000 | Loss: 0.00002744
Iteration 47/1000 | Loss: 0.00002743
Iteration 48/1000 | Loss: 0.00002743
Iteration 49/1000 | Loss: 0.00002743
Iteration 50/1000 | Loss: 0.00002743
Iteration 51/1000 | Loss: 0.00002743
Iteration 52/1000 | Loss: 0.00002743
Iteration 53/1000 | Loss: 0.00002743
Iteration 54/1000 | Loss: 0.00002743
Iteration 55/1000 | Loss: 0.00002743
Iteration 56/1000 | Loss: 0.00002742
Iteration 57/1000 | Loss: 0.00002742
Iteration 58/1000 | Loss: 0.00002742
Iteration 59/1000 | Loss: 0.00002741
Iteration 60/1000 | Loss: 0.00002740
Iteration 61/1000 | Loss: 0.00002740
Iteration 62/1000 | Loss: 0.00002739
Iteration 63/1000 | Loss: 0.00002739
Iteration 64/1000 | Loss: 0.00002739
Iteration 65/1000 | Loss: 0.00002738
Iteration 66/1000 | Loss: 0.00002738
Iteration 67/1000 | Loss: 0.00002737
Iteration 68/1000 | Loss: 0.00002737
Iteration 69/1000 | Loss: 0.00002737
Iteration 70/1000 | Loss: 0.00002736
Iteration 71/1000 | Loss: 0.00002736
Iteration 72/1000 | Loss: 0.00002736
Iteration 73/1000 | Loss: 0.00002735
Iteration 74/1000 | Loss: 0.00002735
Iteration 75/1000 | Loss: 0.00002734
Iteration 76/1000 | Loss: 0.00002734
Iteration 77/1000 | Loss: 0.00002733
Iteration 78/1000 | Loss: 0.00002733
Iteration 79/1000 | Loss: 0.00002733
Iteration 80/1000 | Loss: 0.00002732
Iteration 81/1000 | Loss: 0.00002732
Iteration 82/1000 | Loss: 0.00002731
Iteration 83/1000 | Loss: 0.00002731
Iteration 84/1000 | Loss: 0.00002731
Iteration 85/1000 | Loss: 0.00002730
Iteration 86/1000 | Loss: 0.00002730
Iteration 87/1000 | Loss: 0.00002730
Iteration 88/1000 | Loss: 0.00002729
Iteration 89/1000 | Loss: 0.00002729
Iteration 90/1000 | Loss: 0.00002728
Iteration 91/1000 | Loss: 0.00002728
Iteration 92/1000 | Loss: 0.00002728
Iteration 93/1000 | Loss: 0.00002728
Iteration 94/1000 | Loss: 0.00002727
Iteration 95/1000 | Loss: 0.00002727
Iteration 96/1000 | Loss: 0.00002727
Iteration 97/1000 | Loss: 0.00002727
Iteration 98/1000 | Loss: 0.00002726
Iteration 99/1000 | Loss: 0.00002726
Iteration 100/1000 | Loss: 0.00002726
Iteration 101/1000 | Loss: 0.00002726
Iteration 102/1000 | Loss: 0.00002725
Iteration 103/1000 | Loss: 0.00002725
Iteration 104/1000 | Loss: 0.00002724
Iteration 105/1000 | Loss: 0.00002724
Iteration 106/1000 | Loss: 0.00002724
Iteration 107/1000 | Loss: 0.00002724
Iteration 108/1000 | Loss: 0.00002724
Iteration 109/1000 | Loss: 0.00002724
Iteration 110/1000 | Loss: 0.00002724
Iteration 111/1000 | Loss: 0.00002724
Iteration 112/1000 | Loss: 0.00002724
Iteration 113/1000 | Loss: 0.00002724
Iteration 114/1000 | Loss: 0.00002724
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [2.723963734752033e-05, 2.723963734752033e-05, 2.723963734752033e-05, 2.723963734752033e-05, 2.723963734752033e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.723963734752033e-05

Optimization complete. Final v2v error: 4.347233295440674 mm

Highest mean error: 6.4419264793396 mm for frame 130

Lowest mean error: 3.2336783409118652 mm for frame 153

Saving results

Total time: 52.90017032623291
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carina_posed_010/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01006397
Iteration 2/25 | Loss: 0.00227842
Iteration 3/25 | Loss: 0.00184547
Iteration 4/25 | Loss: 0.00186833
Iteration 5/25 | Loss: 0.00180536
Iteration 6/25 | Loss: 0.00166896
Iteration 7/25 | Loss: 0.00157542
Iteration 8/25 | Loss: 0.00159053
Iteration 9/25 | Loss: 0.00158391
Iteration 10/25 | Loss: 0.00149985
Iteration 11/25 | Loss: 0.00144615
Iteration 12/25 | Loss: 0.00152938
Iteration 13/25 | Loss: 0.00148985
Iteration 14/25 | Loss: 0.00145846
Iteration 15/25 | Loss: 0.00142183
Iteration 16/25 | Loss: 0.00140854
Iteration 17/25 | Loss: 0.00140162
Iteration 18/25 | Loss: 0.00139916
Iteration 19/25 | Loss: 0.00140586
Iteration 20/25 | Loss: 0.00139582
Iteration 21/25 | Loss: 0.00138575
Iteration 22/25 | Loss: 0.00138655
Iteration 23/25 | Loss: 0.00138548
Iteration 24/25 | Loss: 0.00138338
Iteration 25/25 | Loss: 0.00138969

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35980844
Iteration 2/25 | Loss: 0.00235306
Iteration 3/25 | Loss: 0.00234827
Iteration 4/25 | Loss: 0.00234830
Iteration 5/25 | Loss: 0.00234830
Iteration 6/25 | Loss: 0.00234829
Iteration 7/25 | Loss: 0.00234829
Iteration 8/25 | Loss: 0.00234829
Iteration 9/25 | Loss: 0.00234829
Iteration 10/25 | Loss: 0.00233373
Iteration 11/25 | Loss: 0.00233373
Iteration 12/25 | Loss: 0.00233373
Iteration 13/25 | Loss: 0.00233373
Iteration 14/25 | Loss: 0.00233373
Iteration 15/25 | Loss: 0.00233373
Iteration 16/25 | Loss: 0.00233373
Iteration 17/25 | Loss: 0.00233373
Iteration 18/25 | Loss: 0.00233373
Iteration 19/25 | Loss: 0.00233373
Iteration 20/25 | Loss: 0.00233373
Iteration 21/25 | Loss: 0.00233373
Iteration 22/25 | Loss: 0.00233373
Iteration 23/25 | Loss: 0.00233373
Iteration 24/25 | Loss: 0.00233373
Iteration 25/25 | Loss: 0.00233373

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00233373
Iteration 2/1000 | Loss: 0.00019554
Iteration 3/1000 | Loss: 0.00023999
Iteration 4/1000 | Loss: 0.00036134
Iteration 5/1000 | Loss: 0.00026316
Iteration 6/1000 | Loss: 0.00020886
Iteration 7/1000 | Loss: 0.00014308
Iteration 8/1000 | Loss: 0.00017785
Iteration 9/1000 | Loss: 0.00040299
Iteration 10/1000 | Loss: 0.00024875
Iteration 11/1000 | Loss: 0.00015219
Iteration 12/1000 | Loss: 0.00013031
Iteration 13/1000 | Loss: 0.00016523
Iteration 14/1000 | Loss: 0.00020167
Iteration 15/1000 | Loss: 0.00028080
Iteration 16/1000 | Loss: 0.00034617
Iteration 17/1000 | Loss: 0.00016015
Iteration 18/1000 | Loss: 0.00023811
Iteration 19/1000 | Loss: 0.00014946
Iteration 20/1000 | Loss: 0.00027024
Iteration 21/1000 | Loss: 0.00019588
Iteration 22/1000 | Loss: 0.00017784
Iteration 23/1000 | Loss: 0.00028079
Iteration 24/1000 | Loss: 0.00025306
Iteration 25/1000 | Loss: 0.00030630
Iteration 26/1000 | Loss: 0.00023950
Iteration 27/1000 | Loss: 0.00023209
Iteration 28/1000 | Loss: 0.00029036
Iteration 29/1000 | Loss: 0.00031926
Iteration 30/1000 | Loss: 0.00038083
Iteration 31/1000 | Loss: 0.00030967
Iteration 32/1000 | Loss: 0.00025508
Iteration 33/1000 | Loss: 0.00021433
Iteration 34/1000 | Loss: 0.00024222
Iteration 35/1000 | Loss: 0.00021480
Iteration 36/1000 | Loss: 0.00025051
Iteration 37/1000 | Loss: 0.00020162
Iteration 38/1000 | Loss: 0.00021403
Iteration 39/1000 | Loss: 0.00030418
Iteration 40/1000 | Loss: 0.00016376
Iteration 41/1000 | Loss: 0.00021623
Iteration 42/1000 | Loss: 0.00033043
Iteration 43/1000 | Loss: 0.00016090
Iteration 44/1000 | Loss: 0.00021449
Iteration 45/1000 | Loss: 0.00035066
Iteration 46/1000 | Loss: 0.00019784
Iteration 47/1000 | Loss: 0.00015550
Iteration 48/1000 | Loss: 0.00016856
Iteration 49/1000 | Loss: 0.00019442
Iteration 50/1000 | Loss: 0.00026438
Iteration 51/1000 | Loss: 0.00030303
Iteration 52/1000 | Loss: 0.00033849
Iteration 53/1000 | Loss: 0.00020359
Iteration 54/1000 | Loss: 0.00027733
Iteration 55/1000 | Loss: 0.00020863
Iteration 56/1000 | Loss: 0.00034004
Iteration 57/1000 | Loss: 0.00015358
Iteration 58/1000 | Loss: 0.00015323
Iteration 59/1000 | Loss: 0.00017876
Iteration 60/1000 | Loss: 0.00015693
Iteration 61/1000 | Loss: 0.00016652
Iteration 62/1000 | Loss: 0.00012624
Iteration 63/1000 | Loss: 0.00029712
Iteration 64/1000 | Loss: 0.00052658
Iteration 65/1000 | Loss: 0.00038778
Iteration 66/1000 | Loss: 0.00036153
Iteration 67/1000 | Loss: 0.00032663
Iteration 68/1000 | Loss: 0.00023457
Iteration 69/1000 | Loss: 0.00020530
Iteration 70/1000 | Loss: 0.00043705
Iteration 71/1000 | Loss: 0.00032278
Iteration 72/1000 | Loss: 0.00026370
Iteration 73/1000 | Loss: 0.00032543
Iteration 74/1000 | Loss: 0.00035909
Iteration 75/1000 | Loss: 0.00033319
Iteration 76/1000 | Loss: 0.00032544
Iteration 77/1000 | Loss: 0.00033666
Iteration 78/1000 | Loss: 0.00028732
Iteration 79/1000 | Loss: 0.00037615
Iteration 80/1000 | Loss: 0.00037943
Iteration 81/1000 | Loss: 0.00038846
Iteration 82/1000 | Loss: 0.00042788
Iteration 83/1000 | Loss: 0.00041393
Iteration 84/1000 | Loss: 0.00039378
Iteration 85/1000 | Loss: 0.00033494
Iteration 86/1000 | Loss: 0.00038428
Iteration 87/1000 | Loss: 0.00034044
Iteration 88/1000 | Loss: 0.00048172
Iteration 89/1000 | Loss: 0.00073282
Iteration 90/1000 | Loss: 0.00079648
Iteration 91/1000 | Loss: 0.00047555
Iteration 92/1000 | Loss: 0.00060076
Iteration 93/1000 | Loss: 0.00036834
Iteration 94/1000 | Loss: 0.00037876
Iteration 95/1000 | Loss: 0.00042504
Iteration 96/1000 | Loss: 0.00036489
Iteration 97/1000 | Loss: 0.00024614
Iteration 98/1000 | Loss: 0.00029936
Iteration 99/1000 | Loss: 0.00009150
Iteration 100/1000 | Loss: 0.00023530
Iteration 101/1000 | Loss: 0.00025936
Iteration 102/1000 | Loss: 0.00038032
Iteration 103/1000 | Loss: 0.00044023
Iteration 104/1000 | Loss: 0.00051047
Iteration 105/1000 | Loss: 0.00020354
Iteration 106/1000 | Loss: 0.00013378
Iteration 107/1000 | Loss: 0.00017369
Iteration 108/1000 | Loss: 0.00018540
Iteration 109/1000 | Loss: 0.00009901
Iteration 110/1000 | Loss: 0.00019411
Iteration 111/1000 | Loss: 0.00019639
Iteration 112/1000 | Loss: 0.00020096
Iteration 113/1000 | Loss: 0.00016265
Iteration 114/1000 | Loss: 0.00013102
Iteration 115/1000 | Loss: 0.00010306
Iteration 116/1000 | Loss: 0.00018532
Iteration 117/1000 | Loss: 0.00018206
Iteration 118/1000 | Loss: 0.00022124
Iteration 119/1000 | Loss: 0.00018582
Iteration 120/1000 | Loss: 0.00017682
Iteration 121/1000 | Loss: 0.00023303
Iteration 122/1000 | Loss: 0.00007952
Iteration 123/1000 | Loss: 0.00007628
Iteration 124/1000 | Loss: 0.00005365
Iteration 125/1000 | Loss: 0.00010212
Iteration 126/1000 | Loss: 0.00011637
Iteration 127/1000 | Loss: 0.00012978
Iteration 128/1000 | Loss: 0.00012292
Iteration 129/1000 | Loss: 0.00017735
Iteration 130/1000 | Loss: 0.00012436
Iteration 131/1000 | Loss: 0.00018434
Iteration 132/1000 | Loss: 0.00016130
Iteration 133/1000 | Loss: 0.00017745
Iteration 134/1000 | Loss: 0.00015798
Iteration 135/1000 | Loss: 0.00148679
Iteration 136/1000 | Loss: 0.00125957
Iteration 137/1000 | Loss: 0.00041994
Iteration 138/1000 | Loss: 0.00019320
Iteration 139/1000 | Loss: 0.00032352
Iteration 140/1000 | Loss: 0.00021452
Iteration 141/1000 | Loss: 0.00008819
Iteration 142/1000 | Loss: 0.00007371
Iteration 143/1000 | Loss: 0.00013935
Iteration 144/1000 | Loss: 0.00015142
Iteration 145/1000 | Loss: 0.00013889
Iteration 146/1000 | Loss: 0.00054292
Iteration 147/1000 | Loss: 0.00021125
Iteration 148/1000 | Loss: 0.00005817
Iteration 149/1000 | Loss: 0.00007347
Iteration 150/1000 | Loss: 0.00003180
Iteration 151/1000 | Loss: 0.00020572
Iteration 152/1000 | Loss: 0.00005741
Iteration 153/1000 | Loss: 0.00011735
Iteration 154/1000 | Loss: 0.00014795
Iteration 155/1000 | Loss: 0.00004550
Iteration 156/1000 | Loss: 0.00004123
Iteration 157/1000 | Loss: 0.00006654
Iteration 158/1000 | Loss: 0.00004463
Iteration 159/1000 | Loss: 0.00004122
Iteration 160/1000 | Loss: 0.00003679
Iteration 161/1000 | Loss: 0.00004779
Iteration 162/1000 | Loss: 0.00004281
Iteration 163/1000 | Loss: 0.00004329
Iteration 164/1000 | Loss: 0.00004399
Iteration 165/1000 | Loss: 0.00003157
Iteration 166/1000 | Loss: 0.00003676
Iteration 167/1000 | Loss: 0.00004070
Iteration 168/1000 | Loss: 0.00004148
Iteration 169/1000 | Loss: 0.00003540
Iteration 170/1000 | Loss: 0.00004191
Iteration 171/1000 | Loss: 0.00004033
Iteration 172/1000 | Loss: 0.00004125
Iteration 173/1000 | Loss: 0.00003964
Iteration 174/1000 | Loss: 0.00004700
Iteration 175/1000 | Loss: 0.00003824
Iteration 176/1000 | Loss: 0.00004579
Iteration 177/1000 | Loss: 0.00003879
Iteration 178/1000 | Loss: 0.00004510
Iteration 179/1000 | Loss: 0.00004024
Iteration 180/1000 | Loss: 0.00004046
Iteration 181/1000 | Loss: 0.00003872
Iteration 182/1000 | Loss: 0.00004005
Iteration 183/1000 | Loss: 0.00003749
Iteration 184/1000 | Loss: 0.00003916
Iteration 185/1000 | Loss: 0.00003446
Iteration 186/1000 | Loss: 0.00003836
Iteration 187/1000 | Loss: 0.00003312
Iteration 188/1000 | Loss: 0.00002866
Iteration 189/1000 | Loss: 0.00003216
Iteration 190/1000 | Loss: 0.00005015
Iteration 191/1000 | Loss: 0.00004171
Iteration 192/1000 | Loss: 0.00006042
Iteration 193/1000 | Loss: 0.00005032
Iteration 194/1000 | Loss: 0.00005196
Iteration 195/1000 | Loss: 0.00003832
Iteration 196/1000 | Loss: 0.00004584
Iteration 197/1000 | Loss: 0.00003794
Iteration 198/1000 | Loss: 0.00004442
Iteration 199/1000 | Loss: 0.00003654
Iteration 200/1000 | Loss: 0.00004691
Iteration 201/1000 | Loss: 0.00004405
Iteration 202/1000 | Loss: 0.00004879
Iteration 203/1000 | Loss: 0.00003866
Iteration 204/1000 | Loss: 0.00002547
Iteration 205/1000 | Loss: 0.00002440
Iteration 206/1000 | Loss: 0.00003510
Iteration 207/1000 | Loss: 0.00002252
Iteration 208/1000 | Loss: 0.00028925
Iteration 209/1000 | Loss: 0.00017538
Iteration 210/1000 | Loss: 0.00002247
Iteration 211/1000 | Loss: 0.00002103
Iteration 212/1000 | Loss: 0.00013674
Iteration 213/1000 | Loss: 0.00003313
Iteration 214/1000 | Loss: 0.00002183
Iteration 215/1000 | Loss: 0.00001977
Iteration 216/1000 | Loss: 0.00001857
Iteration 217/1000 | Loss: 0.00001931
Iteration 218/1000 | Loss: 0.00002641
Iteration 219/1000 | Loss: 0.00001715
Iteration 220/1000 | Loss: 0.00002268
Iteration 221/1000 | Loss: 0.00001695
Iteration 222/1000 | Loss: 0.00002118
Iteration 223/1000 | Loss: 0.00001863
Iteration 224/1000 | Loss: 0.00001706
Iteration 225/1000 | Loss: 0.00001624
Iteration 226/1000 | Loss: 0.00001623
Iteration 227/1000 | Loss: 0.00001623
Iteration 228/1000 | Loss: 0.00002033
Iteration 229/1000 | Loss: 0.00002297
Iteration 230/1000 | Loss: 0.00001603
Iteration 231/1000 | Loss: 0.00001603
Iteration 232/1000 | Loss: 0.00001603
Iteration 233/1000 | Loss: 0.00001603
Iteration 234/1000 | Loss: 0.00001610
Iteration 235/1000 | Loss: 0.00001610
Iteration 236/1000 | Loss: 0.00001609
Iteration 237/1000 | Loss: 0.00001609
Iteration 238/1000 | Loss: 0.00001777
Iteration 239/1000 | Loss: 0.00002269
Iteration 240/1000 | Loss: 0.00001643
Iteration 241/1000 | Loss: 0.00001775
Iteration 242/1000 | Loss: 0.00001600
Iteration 243/1000 | Loss: 0.00001718
Iteration 244/1000 | Loss: 0.00001585
Iteration 245/1000 | Loss: 0.00001585
Iteration 246/1000 | Loss: 0.00001585
Iteration 247/1000 | Loss: 0.00001585
Iteration 248/1000 | Loss: 0.00001585
Iteration 249/1000 | Loss: 0.00001585
Iteration 250/1000 | Loss: 0.00001584
Iteration 251/1000 | Loss: 0.00001584
Iteration 252/1000 | Loss: 0.00001584
Iteration 253/1000 | Loss: 0.00001584
Iteration 254/1000 | Loss: 0.00001584
Iteration 255/1000 | Loss: 0.00001584
Iteration 256/1000 | Loss: 0.00001584
Iteration 257/1000 | Loss: 0.00001584
Iteration 258/1000 | Loss: 0.00001584
Iteration 259/1000 | Loss: 0.00001584
Iteration 260/1000 | Loss: 0.00001584
Iteration 261/1000 | Loss: 0.00001584
Iteration 262/1000 | Loss: 0.00001584
Iteration 263/1000 | Loss: 0.00001584
Iteration 264/1000 | Loss: 0.00001584
Iteration 265/1000 | Loss: 0.00001584
Iteration 266/1000 | Loss: 0.00001584
Iteration 267/1000 | Loss: 0.00001584
Iteration 268/1000 | Loss: 0.00001584
Iteration 269/1000 | Loss: 0.00001584
Iteration 270/1000 | Loss: 0.00001584
Iteration 271/1000 | Loss: 0.00001584
Iteration 272/1000 | Loss: 0.00001584
Iteration 273/1000 | Loss: 0.00001584
Iteration 274/1000 | Loss: 0.00001584
Iteration 275/1000 | Loss: 0.00001584
Iteration 276/1000 | Loss: 0.00001584
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 276. Stopping optimization.
Last 5 losses: [1.58419570652768e-05, 1.58419570652768e-05, 1.58419570652768e-05, 1.58419570652768e-05, 1.58419570652768e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.58419570652768e-05

Optimization complete. Final v2v error: 3.3850724697113037 mm

Highest mean error: 4.82601261138916 mm for frame 67

Lowest mean error: 2.8512680530548096 mm for frame 113

Saving results

Total time: 364.48183393478394
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carina_posed_010/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01032248
Iteration 2/25 | Loss: 0.01032248
Iteration 3/25 | Loss: 0.01032248
Iteration 4/25 | Loss: 0.01032247
Iteration 5/25 | Loss: 0.01032247
Iteration 6/25 | Loss: 0.01032247
Iteration 7/25 | Loss: 0.01032247
Iteration 8/25 | Loss: 0.01032247
Iteration 9/25 | Loss: 0.01032247
Iteration 10/25 | Loss: 0.01032247
Iteration 11/25 | Loss: 0.01032247
Iteration 12/25 | Loss: 0.01032246
Iteration 13/25 | Loss: 0.01032246
Iteration 14/25 | Loss: 0.01032246
Iteration 15/25 | Loss: 0.01032246
Iteration 16/25 | Loss: 0.01032246
Iteration 17/25 | Loss: 0.01032246
Iteration 18/25 | Loss: 0.01032246
Iteration 19/25 | Loss: 0.01032246
Iteration 20/25 | Loss: 0.01032246
Iteration 21/25 | Loss: 0.01032246
Iteration 22/25 | Loss: 0.01032245
Iteration 23/25 | Loss: 0.01032245
Iteration 24/25 | Loss: 0.01032245
Iteration 25/25 | Loss: 0.01032245

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59251976
Iteration 2/25 | Loss: 0.08820165
Iteration 3/25 | Loss: 0.08817240
Iteration 4/25 | Loss: 0.08817237
Iteration 5/25 | Loss: 0.08817236
Iteration 6/25 | Loss: 0.08817236
Iteration 7/25 | Loss: 0.08817236
Iteration 8/25 | Loss: 0.08817236
Iteration 9/25 | Loss: 0.08817235
Iteration 10/25 | Loss: 0.08817235
Iteration 11/25 | Loss: 0.08817235
Iteration 12/25 | Loss: 0.08817235
Iteration 13/25 | Loss: 0.08817235
Iteration 14/25 | Loss: 0.08817235
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.08817234635353088, 0.08817234635353088, 0.08817234635353088, 0.08817234635353088, 0.08817234635353088]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08817234635353088

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08817235
Iteration 2/1000 | Loss: 0.00169906
Iteration 3/1000 | Loss: 0.00053562
Iteration 4/1000 | Loss: 0.00065856
Iteration 5/1000 | Loss: 0.00115887
Iteration 6/1000 | Loss: 0.00025132
Iteration 7/1000 | Loss: 0.00006770
Iteration 8/1000 | Loss: 0.00005485
Iteration 9/1000 | Loss: 0.00008917
Iteration 10/1000 | Loss: 0.00052050
Iteration 11/1000 | Loss: 0.00004053
Iteration 12/1000 | Loss: 0.00003215
Iteration 13/1000 | Loss: 0.00010241
Iteration 14/1000 | Loss: 0.00005462
Iteration 15/1000 | Loss: 0.00005709
Iteration 16/1000 | Loss: 0.00002631
Iteration 17/1000 | Loss: 0.00002611
Iteration 18/1000 | Loss: 0.00002451
Iteration 19/1000 | Loss: 0.00002367
Iteration 20/1000 | Loss: 0.00002273
Iteration 21/1000 | Loss: 0.00004014
Iteration 22/1000 | Loss: 0.00036857
Iteration 23/1000 | Loss: 0.00003246
Iteration 24/1000 | Loss: 0.00003644
Iteration 25/1000 | Loss: 0.00006214
Iteration 26/1000 | Loss: 0.00002047
Iteration 27/1000 | Loss: 0.00002081
Iteration 28/1000 | Loss: 0.00002422
Iteration 29/1000 | Loss: 0.00001939
Iteration 30/1000 | Loss: 0.00002031
Iteration 31/1000 | Loss: 0.00001886
Iteration 32/1000 | Loss: 0.00001852
Iteration 33/1000 | Loss: 0.00001827
Iteration 34/1000 | Loss: 0.00001816
Iteration 35/1000 | Loss: 0.00001816
Iteration 36/1000 | Loss: 0.00003143
Iteration 37/1000 | Loss: 0.00001801
Iteration 38/1000 | Loss: 0.00001800
Iteration 39/1000 | Loss: 0.00001789
Iteration 40/1000 | Loss: 0.00001783
Iteration 41/1000 | Loss: 0.00001780
Iteration 42/1000 | Loss: 0.00001777
Iteration 43/1000 | Loss: 0.00001775
Iteration 44/1000 | Loss: 0.00001775
Iteration 45/1000 | Loss: 0.00001773
Iteration 46/1000 | Loss: 0.00001773
Iteration 47/1000 | Loss: 0.00001772
Iteration 48/1000 | Loss: 0.00001772
Iteration 49/1000 | Loss: 0.00001772
Iteration 50/1000 | Loss: 0.00001771
Iteration 51/1000 | Loss: 0.00001771
Iteration 52/1000 | Loss: 0.00001771
Iteration 53/1000 | Loss: 0.00001770
Iteration 54/1000 | Loss: 0.00001770
Iteration 55/1000 | Loss: 0.00001770
Iteration 56/1000 | Loss: 0.00001770
Iteration 57/1000 | Loss: 0.00001770
Iteration 58/1000 | Loss: 0.00001770
Iteration 59/1000 | Loss: 0.00001769
Iteration 60/1000 | Loss: 0.00001768
Iteration 61/1000 | Loss: 0.00001767
Iteration 62/1000 | Loss: 0.00001767
Iteration 63/1000 | Loss: 0.00001767
Iteration 64/1000 | Loss: 0.00001766
Iteration 65/1000 | Loss: 0.00001766
Iteration 66/1000 | Loss: 0.00001765
Iteration 67/1000 | Loss: 0.00001765
Iteration 68/1000 | Loss: 0.00001764
Iteration 69/1000 | Loss: 0.00001764
Iteration 70/1000 | Loss: 0.00001764
Iteration 71/1000 | Loss: 0.00001764
Iteration 72/1000 | Loss: 0.00001764
Iteration 73/1000 | Loss: 0.00001764
Iteration 74/1000 | Loss: 0.00001764
Iteration 75/1000 | Loss: 0.00001764
Iteration 76/1000 | Loss: 0.00001764
Iteration 77/1000 | Loss: 0.00001764
Iteration 78/1000 | Loss: 0.00001764
Iteration 79/1000 | Loss: 0.00001764
Iteration 80/1000 | Loss: 0.00001764
Iteration 81/1000 | Loss: 0.00001763
Iteration 82/1000 | Loss: 0.00001763
Iteration 83/1000 | Loss: 0.00001763
Iteration 84/1000 | Loss: 0.00001763
Iteration 85/1000 | Loss: 0.00001763
Iteration 86/1000 | Loss: 0.00001762
Iteration 87/1000 | Loss: 0.00001762
Iteration 88/1000 | Loss: 0.00001762
Iteration 89/1000 | Loss: 0.00001761
Iteration 90/1000 | Loss: 0.00001761
Iteration 91/1000 | Loss: 0.00001761
Iteration 92/1000 | Loss: 0.00001761
Iteration 93/1000 | Loss: 0.00001760
Iteration 94/1000 | Loss: 0.00001760
Iteration 95/1000 | Loss: 0.00001760
Iteration 96/1000 | Loss: 0.00001759
Iteration 97/1000 | Loss: 0.00001759
Iteration 98/1000 | Loss: 0.00001759
Iteration 99/1000 | Loss: 0.00001759
Iteration 100/1000 | Loss: 0.00001759
Iteration 101/1000 | Loss: 0.00001759
Iteration 102/1000 | Loss: 0.00001758
Iteration 103/1000 | Loss: 0.00001758
Iteration 104/1000 | Loss: 0.00001758
Iteration 105/1000 | Loss: 0.00001758
Iteration 106/1000 | Loss: 0.00001758
Iteration 107/1000 | Loss: 0.00001758
Iteration 108/1000 | Loss: 0.00001758
Iteration 109/1000 | Loss: 0.00001758
Iteration 110/1000 | Loss: 0.00001758
Iteration 111/1000 | Loss: 0.00001757
Iteration 112/1000 | Loss: 0.00001757
Iteration 113/1000 | Loss: 0.00001757
Iteration 114/1000 | Loss: 0.00001757
Iteration 115/1000 | Loss: 0.00001757
Iteration 116/1000 | Loss: 0.00001757
Iteration 117/1000 | Loss: 0.00001757
Iteration 118/1000 | Loss: 0.00001757
Iteration 119/1000 | Loss: 0.00001757
Iteration 120/1000 | Loss: 0.00001757
Iteration 121/1000 | Loss: 0.00001757
Iteration 122/1000 | Loss: 0.00001756
Iteration 123/1000 | Loss: 0.00001756
Iteration 124/1000 | Loss: 0.00001756
Iteration 125/1000 | Loss: 0.00001756
Iteration 126/1000 | Loss: 0.00001756
Iteration 127/1000 | Loss: 0.00001756
Iteration 128/1000 | Loss: 0.00001756
Iteration 129/1000 | Loss: 0.00001756
Iteration 130/1000 | Loss: 0.00001756
Iteration 131/1000 | Loss: 0.00001755
Iteration 132/1000 | Loss: 0.00001755
Iteration 133/1000 | Loss: 0.00001755
Iteration 134/1000 | Loss: 0.00001755
Iteration 135/1000 | Loss: 0.00001754
Iteration 136/1000 | Loss: 0.00001754
Iteration 137/1000 | Loss: 0.00001754
Iteration 138/1000 | Loss: 0.00001754
Iteration 139/1000 | Loss: 0.00001754
Iteration 140/1000 | Loss: 0.00001754
Iteration 141/1000 | Loss: 0.00001753
Iteration 142/1000 | Loss: 0.00003872
Iteration 143/1000 | Loss: 0.00001756
Iteration 144/1000 | Loss: 0.00001753
Iteration 145/1000 | Loss: 0.00001753
Iteration 146/1000 | Loss: 0.00001753
Iteration 147/1000 | Loss: 0.00001752
Iteration 148/1000 | Loss: 0.00001752
Iteration 149/1000 | Loss: 0.00002217
Iteration 150/1000 | Loss: 0.00001851
Iteration 151/1000 | Loss: 0.00002050
Iteration 152/1000 | Loss: 0.00001751
Iteration 153/1000 | Loss: 0.00001751
Iteration 154/1000 | Loss: 0.00001751
Iteration 155/1000 | Loss: 0.00001751
Iteration 156/1000 | Loss: 0.00001751
Iteration 157/1000 | Loss: 0.00001751
Iteration 158/1000 | Loss: 0.00001751
Iteration 159/1000 | Loss: 0.00001751
Iteration 160/1000 | Loss: 0.00001751
Iteration 161/1000 | Loss: 0.00001751
Iteration 162/1000 | Loss: 0.00001751
Iteration 163/1000 | Loss: 0.00001751
Iteration 164/1000 | Loss: 0.00001751
Iteration 165/1000 | Loss: 0.00001751
Iteration 166/1000 | Loss: 0.00001751
Iteration 167/1000 | Loss: 0.00001751
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.750627598084975e-05, 1.750627598084975e-05, 1.750627598084975e-05, 1.750627598084975e-05, 1.750627598084975e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.750627598084975e-05

Optimization complete. Final v2v error: 3.6347508430480957 mm

Highest mean error: 4.2698445320129395 mm for frame 8

Lowest mean error: 3.3472700119018555 mm for frame 106

Saving results

Total time: 71.64463639259338
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carina_posed_010/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00664268
Iteration 2/25 | Loss: 0.00177922
Iteration 3/25 | Loss: 0.00150843
Iteration 4/25 | Loss: 0.00140623
Iteration 5/25 | Loss: 0.00139017
Iteration 6/25 | Loss: 0.00139358
Iteration 7/25 | Loss: 0.00138217
Iteration 8/25 | Loss: 0.00138221
Iteration 9/25 | Loss: 0.00137695
Iteration 10/25 | Loss: 0.00138035
Iteration 11/25 | Loss: 0.00137828
Iteration 12/25 | Loss: 0.00137577
Iteration 13/25 | Loss: 0.00137870
Iteration 14/25 | Loss: 0.00137571
Iteration 15/25 | Loss: 0.00137567
Iteration 16/25 | Loss: 0.00137567
Iteration 17/25 | Loss: 0.00137566
Iteration 18/25 | Loss: 0.00137566
Iteration 19/25 | Loss: 0.00137566
Iteration 20/25 | Loss: 0.00137566
Iteration 21/25 | Loss: 0.00137566
Iteration 22/25 | Loss: 0.00137566
Iteration 23/25 | Loss: 0.00137566
Iteration 24/25 | Loss: 0.00137566
Iteration 25/25 | Loss: 0.00137566

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.50408745
Iteration 2/25 | Loss: 0.00160503
Iteration 3/25 | Loss: 0.00160488
Iteration 4/25 | Loss: 0.00160488
Iteration 5/25 | Loss: 0.00160488
Iteration 6/25 | Loss: 0.00160488
Iteration 7/25 | Loss: 0.00160488
Iteration 8/25 | Loss: 0.00160488
Iteration 9/25 | Loss: 0.00160488
Iteration 10/25 | Loss: 0.00160488
Iteration 11/25 | Loss: 0.00160488
Iteration 12/25 | Loss: 0.00160488
Iteration 13/25 | Loss: 0.00160488
Iteration 14/25 | Loss: 0.00160488
Iteration 15/25 | Loss: 0.00160488
Iteration 16/25 | Loss: 0.00160488
Iteration 17/25 | Loss: 0.00160488
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0016048784600570798, 0.0016048784600570798, 0.0016048784600570798, 0.0016048784600570798, 0.0016048784600570798]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016048784600570798

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00160488
Iteration 2/1000 | Loss: 0.00003145
Iteration 3/1000 | Loss: 0.00007455
Iteration 4/1000 | Loss: 0.00002141
Iteration 5/1000 | Loss: 0.00002012
Iteration 6/1000 | Loss: 0.00001917
Iteration 7/1000 | Loss: 0.00001837
Iteration 8/1000 | Loss: 0.00001786
Iteration 9/1000 | Loss: 0.00001744
Iteration 10/1000 | Loss: 0.00001712
Iteration 11/1000 | Loss: 0.00001687
Iteration 12/1000 | Loss: 0.00001661
Iteration 13/1000 | Loss: 0.00001636
Iteration 14/1000 | Loss: 0.00001612
Iteration 15/1000 | Loss: 0.00001594
Iteration 16/1000 | Loss: 0.00001589
Iteration 17/1000 | Loss: 0.00001585
Iteration 18/1000 | Loss: 0.00001585
Iteration 19/1000 | Loss: 0.00001583
Iteration 20/1000 | Loss: 0.00001582
Iteration 21/1000 | Loss: 0.00001578
Iteration 22/1000 | Loss: 0.00001574
Iteration 23/1000 | Loss: 0.00001572
Iteration 24/1000 | Loss: 0.00001571
Iteration 25/1000 | Loss: 0.00001570
Iteration 26/1000 | Loss: 0.00001568
Iteration 27/1000 | Loss: 0.00001567
Iteration 28/1000 | Loss: 0.00001566
Iteration 29/1000 | Loss: 0.00001565
Iteration 30/1000 | Loss: 0.00001563
Iteration 31/1000 | Loss: 0.00001561
Iteration 32/1000 | Loss: 0.00001557
Iteration 33/1000 | Loss: 0.00001557
Iteration 34/1000 | Loss: 0.00001553
Iteration 35/1000 | Loss: 0.00001551
Iteration 36/1000 | Loss: 0.00001551
Iteration 37/1000 | Loss: 0.00001547
Iteration 38/1000 | Loss: 0.00001547
Iteration 39/1000 | Loss: 0.00001544
Iteration 40/1000 | Loss: 0.00001544
Iteration 41/1000 | Loss: 0.00001543
Iteration 42/1000 | Loss: 0.00001543
Iteration 43/1000 | Loss: 0.00001542
Iteration 44/1000 | Loss: 0.00001542
Iteration 45/1000 | Loss: 0.00001542
Iteration 46/1000 | Loss: 0.00001541
Iteration 47/1000 | Loss: 0.00001540
Iteration 48/1000 | Loss: 0.00001540
Iteration 49/1000 | Loss: 0.00001540
Iteration 50/1000 | Loss: 0.00001540
Iteration 51/1000 | Loss: 0.00001539
Iteration 52/1000 | Loss: 0.00001539
Iteration 53/1000 | Loss: 0.00001539
Iteration 54/1000 | Loss: 0.00001538
Iteration 55/1000 | Loss: 0.00001538
Iteration 56/1000 | Loss: 0.00001538
Iteration 57/1000 | Loss: 0.00001538
Iteration 58/1000 | Loss: 0.00001538
Iteration 59/1000 | Loss: 0.00001538
Iteration 60/1000 | Loss: 0.00001538
Iteration 61/1000 | Loss: 0.00001538
Iteration 62/1000 | Loss: 0.00001538
Iteration 63/1000 | Loss: 0.00001538
Iteration 64/1000 | Loss: 0.00001538
Iteration 65/1000 | Loss: 0.00001538
Iteration 66/1000 | Loss: 0.00001538
Iteration 67/1000 | Loss: 0.00001538
Iteration 68/1000 | Loss: 0.00001538
Iteration 69/1000 | Loss: 0.00001538
Iteration 70/1000 | Loss: 0.00001538
Iteration 71/1000 | Loss: 0.00001538
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 71. Stopping optimization.
Last 5 losses: [1.5377270756289363e-05, 1.5377270756289363e-05, 1.5377270756289363e-05, 1.5377270756289363e-05, 1.5377270756289363e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5377270756289363e-05

Optimization complete. Final v2v error: 3.3634586334228516 mm

Highest mean error: 3.7114522457122803 mm for frame 90

Lowest mean error: 3.0303802490234375 mm for frame 57

Saving results

Total time: 63.66884446144104
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carina_posed_010/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00726834
Iteration 2/25 | Loss: 0.00171789
Iteration 3/25 | Loss: 0.00146743
Iteration 4/25 | Loss: 0.00143923
Iteration 5/25 | Loss: 0.00143788
Iteration 6/25 | Loss: 0.00143681
Iteration 7/25 | Loss: 0.00143587
Iteration 8/25 | Loss: 0.00143337
Iteration 9/25 | Loss: 0.00143128
Iteration 10/25 | Loss: 0.00143085
Iteration 11/25 | Loss: 0.00143059
Iteration 12/25 | Loss: 0.00143013
Iteration 13/25 | Loss: 0.00143269
Iteration 14/25 | Loss: 0.00142922
Iteration 15/25 | Loss: 0.00142881
Iteration 16/25 | Loss: 0.00142874
Iteration 17/25 | Loss: 0.00142874
Iteration 18/25 | Loss: 0.00142874
Iteration 19/25 | Loss: 0.00142874
Iteration 20/25 | Loss: 0.00142874
Iteration 21/25 | Loss: 0.00142874
Iteration 22/25 | Loss: 0.00142874
Iteration 23/25 | Loss: 0.00142874
Iteration 24/25 | Loss: 0.00142874
Iteration 25/25 | Loss: 0.00142873

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.84669876
Iteration 2/25 | Loss: 0.00143516
Iteration 3/25 | Loss: 0.00143514
Iteration 4/25 | Loss: 0.00143514
Iteration 5/25 | Loss: 0.00143514
Iteration 6/25 | Loss: 0.00143514
Iteration 7/25 | Loss: 0.00143514
Iteration 8/25 | Loss: 0.00143514
Iteration 9/25 | Loss: 0.00143514
Iteration 10/25 | Loss: 0.00143514
Iteration 11/25 | Loss: 0.00143514
Iteration 12/25 | Loss: 0.00143514
Iteration 13/25 | Loss: 0.00143514
Iteration 14/25 | Loss: 0.00143514
Iteration 15/25 | Loss: 0.00143514
Iteration 16/25 | Loss: 0.00143514
Iteration 17/25 | Loss: 0.00143514
Iteration 18/25 | Loss: 0.00143514
Iteration 19/25 | Loss: 0.00143514
Iteration 20/25 | Loss: 0.00143514
Iteration 21/25 | Loss: 0.00143514
Iteration 22/25 | Loss: 0.00143514
Iteration 23/25 | Loss: 0.00143514
Iteration 24/25 | Loss: 0.00143514
Iteration 25/25 | Loss: 0.00143514

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00143514
Iteration 2/1000 | Loss: 0.00003428
Iteration 3/1000 | Loss: 0.00002675
Iteration 4/1000 | Loss: 0.00002516
Iteration 5/1000 | Loss: 0.00002385
Iteration 6/1000 | Loss: 0.00002309
Iteration 7/1000 | Loss: 0.00002240
Iteration 8/1000 | Loss: 0.00002180
Iteration 9/1000 | Loss: 0.00002143
Iteration 10/1000 | Loss: 0.00002118
Iteration 11/1000 | Loss: 0.00002099
Iteration 12/1000 | Loss: 0.00002095
Iteration 13/1000 | Loss: 0.00002094
Iteration 14/1000 | Loss: 0.00002094
Iteration 15/1000 | Loss: 0.00002085
Iteration 16/1000 | Loss: 0.00002078
Iteration 17/1000 | Loss: 0.00002075
Iteration 18/1000 | Loss: 0.00002075
Iteration 19/1000 | Loss: 0.00002075
Iteration 20/1000 | Loss: 0.00002074
Iteration 21/1000 | Loss: 0.00002073
Iteration 22/1000 | Loss: 0.00002073
Iteration 23/1000 | Loss: 0.00002072
Iteration 24/1000 | Loss: 0.00002072
Iteration 25/1000 | Loss: 0.00002071
Iteration 26/1000 | Loss: 0.00002071
Iteration 27/1000 | Loss: 0.00002071
Iteration 28/1000 | Loss: 0.00002070
Iteration 29/1000 | Loss: 0.00002070
Iteration 30/1000 | Loss: 0.00002070
Iteration 31/1000 | Loss: 0.00002070
Iteration 32/1000 | Loss: 0.00002070
Iteration 33/1000 | Loss: 0.00002070
Iteration 34/1000 | Loss: 0.00002070
Iteration 35/1000 | Loss: 0.00002070
Iteration 36/1000 | Loss: 0.00002070
Iteration 37/1000 | Loss: 0.00002070
Iteration 38/1000 | Loss: 0.00002070
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 38. Stopping optimization.
Last 5 losses: [2.0699379092548043e-05, 2.0699379092548043e-05, 2.0699379092548043e-05, 2.0699379092548043e-05, 2.0699379092548043e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0699379092548043e-05

Optimization complete. Final v2v error: 3.8316292762756348 mm

Highest mean error: 5.3095879554748535 mm for frame 202

Lowest mean error: 3.436286211013794 mm for frame 214

Saving results

Total time: 53.05732202529907
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carina_posed_010/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00401878
Iteration 2/25 | Loss: 0.00145952
Iteration 3/25 | Loss: 0.00138798
Iteration 4/25 | Loss: 0.00137370
Iteration 5/25 | Loss: 0.00136884
Iteration 6/25 | Loss: 0.00136693
Iteration 7/25 | Loss: 0.00136621
Iteration 8/25 | Loss: 0.00136621
Iteration 9/25 | Loss: 0.00136621
Iteration 10/25 | Loss: 0.00136621
Iteration 11/25 | Loss: 0.00136621
Iteration 12/25 | Loss: 0.00136621
Iteration 13/25 | Loss: 0.00136621
Iteration 14/25 | Loss: 0.00136621
Iteration 15/25 | Loss: 0.00136621
Iteration 16/25 | Loss: 0.00136621
Iteration 17/25 | Loss: 0.00136621
Iteration 18/25 | Loss: 0.00136621
Iteration 19/25 | Loss: 0.00136621
Iteration 20/25 | Loss: 0.00136621
Iteration 21/25 | Loss: 0.00136621
Iteration 22/25 | Loss: 0.00136621
Iteration 23/25 | Loss: 0.00136621
Iteration 24/25 | Loss: 0.00136621
Iteration 25/25 | Loss: 0.00136621

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26594973
Iteration 2/25 | Loss: 0.00285382
Iteration 3/25 | Loss: 0.00285382
Iteration 4/25 | Loss: 0.00285382
Iteration 5/25 | Loss: 0.00285382
Iteration 6/25 | Loss: 0.00285382
Iteration 7/25 | Loss: 0.00285382
Iteration 8/25 | Loss: 0.00285382
Iteration 9/25 | Loss: 0.00285382
Iteration 10/25 | Loss: 0.00285382
Iteration 11/25 | Loss: 0.00285382
Iteration 12/25 | Loss: 0.00285382
Iteration 13/25 | Loss: 0.00285382
Iteration 14/25 | Loss: 0.00285382
Iteration 15/25 | Loss: 0.00285382
Iteration 16/25 | Loss: 0.00285382
Iteration 17/25 | Loss: 0.00285382
Iteration 18/25 | Loss: 0.00285382
Iteration 19/25 | Loss: 0.00285382
Iteration 20/25 | Loss: 0.00285382
Iteration 21/25 | Loss: 0.00285382
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0028538168407976627, 0.0028538168407976627, 0.0028538168407976627, 0.0028538168407976627, 0.0028538168407976627]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0028538168407976627

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00285382
Iteration 2/1000 | Loss: 0.00004452
Iteration 3/1000 | Loss: 0.00002706
Iteration 4/1000 | Loss: 0.00002142
Iteration 5/1000 | Loss: 0.00001977
Iteration 6/1000 | Loss: 0.00001892
Iteration 7/1000 | Loss: 0.00001830
Iteration 8/1000 | Loss: 0.00001783
Iteration 9/1000 | Loss: 0.00001745
Iteration 10/1000 | Loss: 0.00001718
Iteration 11/1000 | Loss: 0.00001697
Iteration 12/1000 | Loss: 0.00001696
Iteration 13/1000 | Loss: 0.00001677
Iteration 14/1000 | Loss: 0.00001669
Iteration 15/1000 | Loss: 0.00001659
Iteration 16/1000 | Loss: 0.00001656
Iteration 17/1000 | Loss: 0.00001639
Iteration 18/1000 | Loss: 0.00001630
Iteration 19/1000 | Loss: 0.00001628
Iteration 20/1000 | Loss: 0.00001628
Iteration 21/1000 | Loss: 0.00001627
Iteration 22/1000 | Loss: 0.00001627
Iteration 23/1000 | Loss: 0.00001626
Iteration 24/1000 | Loss: 0.00001625
Iteration 25/1000 | Loss: 0.00001625
Iteration 26/1000 | Loss: 0.00001623
Iteration 27/1000 | Loss: 0.00001621
Iteration 28/1000 | Loss: 0.00001621
Iteration 29/1000 | Loss: 0.00001621
Iteration 30/1000 | Loss: 0.00001621
Iteration 31/1000 | Loss: 0.00001621
Iteration 32/1000 | Loss: 0.00001620
Iteration 33/1000 | Loss: 0.00001620
Iteration 34/1000 | Loss: 0.00001620
Iteration 35/1000 | Loss: 0.00001620
Iteration 36/1000 | Loss: 0.00001620
Iteration 37/1000 | Loss: 0.00001620
Iteration 38/1000 | Loss: 0.00001619
Iteration 39/1000 | Loss: 0.00001619
Iteration 40/1000 | Loss: 0.00001619
Iteration 41/1000 | Loss: 0.00001618
Iteration 42/1000 | Loss: 0.00001617
Iteration 43/1000 | Loss: 0.00001616
Iteration 44/1000 | Loss: 0.00001615
Iteration 45/1000 | Loss: 0.00001614
Iteration 46/1000 | Loss: 0.00001613
Iteration 47/1000 | Loss: 0.00001613
Iteration 48/1000 | Loss: 0.00001613
Iteration 49/1000 | Loss: 0.00001613
Iteration 50/1000 | Loss: 0.00001613
Iteration 51/1000 | Loss: 0.00001613
Iteration 52/1000 | Loss: 0.00001612
Iteration 53/1000 | Loss: 0.00001612
Iteration 54/1000 | Loss: 0.00001612
Iteration 55/1000 | Loss: 0.00001612
Iteration 56/1000 | Loss: 0.00001612
Iteration 57/1000 | Loss: 0.00001612
Iteration 58/1000 | Loss: 0.00001612
Iteration 59/1000 | Loss: 0.00001612
Iteration 60/1000 | Loss: 0.00001612
Iteration 61/1000 | Loss: 0.00001612
Iteration 62/1000 | Loss: 0.00001612
Iteration 63/1000 | Loss: 0.00001612
Iteration 64/1000 | Loss: 0.00001611
Iteration 65/1000 | Loss: 0.00001611
Iteration 66/1000 | Loss: 0.00001611
Iteration 67/1000 | Loss: 0.00001610
Iteration 68/1000 | Loss: 0.00001610
Iteration 69/1000 | Loss: 0.00001610
Iteration 70/1000 | Loss: 0.00001609
Iteration 71/1000 | Loss: 0.00001609
Iteration 72/1000 | Loss: 0.00001609
Iteration 73/1000 | Loss: 0.00001609
Iteration 74/1000 | Loss: 0.00001609
Iteration 75/1000 | Loss: 0.00001608
Iteration 76/1000 | Loss: 0.00001608
Iteration 77/1000 | Loss: 0.00001608
Iteration 78/1000 | Loss: 0.00001607
Iteration 79/1000 | Loss: 0.00001607
Iteration 80/1000 | Loss: 0.00001607
Iteration 81/1000 | Loss: 0.00001606
Iteration 82/1000 | Loss: 0.00001606
Iteration 83/1000 | Loss: 0.00001605
Iteration 84/1000 | Loss: 0.00001605
Iteration 85/1000 | Loss: 0.00001605
Iteration 86/1000 | Loss: 0.00001605
Iteration 87/1000 | Loss: 0.00001604
Iteration 88/1000 | Loss: 0.00001604
Iteration 89/1000 | Loss: 0.00001604
Iteration 90/1000 | Loss: 0.00001604
Iteration 91/1000 | Loss: 0.00001604
Iteration 92/1000 | Loss: 0.00001603
Iteration 93/1000 | Loss: 0.00001603
Iteration 94/1000 | Loss: 0.00001602
Iteration 95/1000 | Loss: 0.00001601
Iteration 96/1000 | Loss: 0.00001601
Iteration 97/1000 | Loss: 0.00001601
Iteration 98/1000 | Loss: 0.00001601
Iteration 99/1000 | Loss: 0.00001601
Iteration 100/1000 | Loss: 0.00001601
Iteration 101/1000 | Loss: 0.00001600
Iteration 102/1000 | Loss: 0.00001600
Iteration 103/1000 | Loss: 0.00001600
Iteration 104/1000 | Loss: 0.00001600
Iteration 105/1000 | Loss: 0.00001600
Iteration 106/1000 | Loss: 0.00001600
Iteration 107/1000 | Loss: 0.00001600
Iteration 108/1000 | Loss: 0.00001599
Iteration 109/1000 | Loss: 0.00001599
Iteration 110/1000 | Loss: 0.00001599
Iteration 111/1000 | Loss: 0.00001599
Iteration 112/1000 | Loss: 0.00001598
Iteration 113/1000 | Loss: 0.00001598
Iteration 114/1000 | Loss: 0.00001598
Iteration 115/1000 | Loss: 0.00001598
Iteration 116/1000 | Loss: 0.00001597
Iteration 117/1000 | Loss: 0.00001597
Iteration 118/1000 | Loss: 0.00001597
Iteration 119/1000 | Loss: 0.00001597
Iteration 120/1000 | Loss: 0.00001597
Iteration 121/1000 | Loss: 0.00001596
Iteration 122/1000 | Loss: 0.00001596
Iteration 123/1000 | Loss: 0.00001596
Iteration 124/1000 | Loss: 0.00001596
Iteration 125/1000 | Loss: 0.00001596
Iteration 126/1000 | Loss: 0.00001596
Iteration 127/1000 | Loss: 0.00001596
Iteration 128/1000 | Loss: 0.00001596
Iteration 129/1000 | Loss: 0.00001596
Iteration 130/1000 | Loss: 0.00001596
Iteration 131/1000 | Loss: 0.00001595
Iteration 132/1000 | Loss: 0.00001595
Iteration 133/1000 | Loss: 0.00001594
Iteration 134/1000 | Loss: 0.00001594
Iteration 135/1000 | Loss: 0.00001593
Iteration 136/1000 | Loss: 0.00001593
Iteration 137/1000 | Loss: 0.00001593
Iteration 138/1000 | Loss: 0.00001593
Iteration 139/1000 | Loss: 0.00001593
Iteration 140/1000 | Loss: 0.00001593
Iteration 141/1000 | Loss: 0.00001593
Iteration 142/1000 | Loss: 0.00001593
Iteration 143/1000 | Loss: 0.00001592
Iteration 144/1000 | Loss: 0.00001592
Iteration 145/1000 | Loss: 0.00001591
Iteration 146/1000 | Loss: 0.00001591
Iteration 147/1000 | Loss: 0.00001591
Iteration 148/1000 | Loss: 0.00001591
Iteration 149/1000 | Loss: 0.00001590
Iteration 150/1000 | Loss: 0.00001589
Iteration 151/1000 | Loss: 0.00001589
Iteration 152/1000 | Loss: 0.00001589
Iteration 153/1000 | Loss: 0.00001589
Iteration 154/1000 | Loss: 0.00001588
Iteration 155/1000 | Loss: 0.00001588
Iteration 156/1000 | Loss: 0.00001588
Iteration 157/1000 | Loss: 0.00001588
Iteration 158/1000 | Loss: 0.00001588
Iteration 159/1000 | Loss: 0.00001587
Iteration 160/1000 | Loss: 0.00001587
Iteration 161/1000 | Loss: 0.00001586
Iteration 162/1000 | Loss: 0.00001585
Iteration 163/1000 | Loss: 0.00001585
Iteration 164/1000 | Loss: 0.00001585
Iteration 165/1000 | Loss: 0.00001584
Iteration 166/1000 | Loss: 0.00001584
Iteration 167/1000 | Loss: 0.00001584
Iteration 168/1000 | Loss: 0.00001584
Iteration 169/1000 | Loss: 0.00001583
Iteration 170/1000 | Loss: 0.00001583
Iteration 171/1000 | Loss: 0.00001582
Iteration 172/1000 | Loss: 0.00001582
Iteration 173/1000 | Loss: 0.00001582
Iteration 174/1000 | Loss: 0.00001582
Iteration 175/1000 | Loss: 0.00001582
Iteration 176/1000 | Loss: 0.00001582
Iteration 177/1000 | Loss: 0.00001582
Iteration 178/1000 | Loss: 0.00001582
Iteration 179/1000 | Loss: 0.00001582
Iteration 180/1000 | Loss: 0.00001581
Iteration 181/1000 | Loss: 0.00001581
Iteration 182/1000 | Loss: 0.00001581
Iteration 183/1000 | Loss: 0.00001581
Iteration 184/1000 | Loss: 0.00001581
Iteration 185/1000 | Loss: 0.00001581
Iteration 186/1000 | Loss: 0.00001581
Iteration 187/1000 | Loss: 0.00001581
Iteration 188/1000 | Loss: 0.00001580
Iteration 189/1000 | Loss: 0.00001580
Iteration 190/1000 | Loss: 0.00001580
Iteration 191/1000 | Loss: 0.00001580
Iteration 192/1000 | Loss: 0.00001580
Iteration 193/1000 | Loss: 0.00001580
Iteration 194/1000 | Loss: 0.00001580
Iteration 195/1000 | Loss: 0.00001579
Iteration 196/1000 | Loss: 0.00001579
Iteration 197/1000 | Loss: 0.00001579
Iteration 198/1000 | Loss: 0.00001579
Iteration 199/1000 | Loss: 0.00001579
Iteration 200/1000 | Loss: 0.00001579
Iteration 201/1000 | Loss: 0.00001579
Iteration 202/1000 | Loss: 0.00001578
Iteration 203/1000 | Loss: 0.00001578
Iteration 204/1000 | Loss: 0.00001578
Iteration 205/1000 | Loss: 0.00001578
Iteration 206/1000 | Loss: 0.00001578
Iteration 207/1000 | Loss: 0.00001578
Iteration 208/1000 | Loss: 0.00001578
Iteration 209/1000 | Loss: 0.00001578
Iteration 210/1000 | Loss: 0.00001578
Iteration 211/1000 | Loss: 0.00001578
Iteration 212/1000 | Loss: 0.00001578
Iteration 213/1000 | Loss: 0.00001578
Iteration 214/1000 | Loss: 0.00001578
Iteration 215/1000 | Loss: 0.00001578
Iteration 216/1000 | Loss: 0.00001578
Iteration 217/1000 | Loss: 0.00001577
Iteration 218/1000 | Loss: 0.00001577
Iteration 219/1000 | Loss: 0.00001577
Iteration 220/1000 | Loss: 0.00001577
Iteration 221/1000 | Loss: 0.00001577
Iteration 222/1000 | Loss: 0.00001577
Iteration 223/1000 | Loss: 0.00001577
Iteration 224/1000 | Loss: 0.00001576
Iteration 225/1000 | Loss: 0.00001576
Iteration 226/1000 | Loss: 0.00001576
Iteration 227/1000 | Loss: 0.00001576
Iteration 228/1000 | Loss: 0.00001576
Iteration 229/1000 | Loss: 0.00001575
Iteration 230/1000 | Loss: 0.00001575
Iteration 231/1000 | Loss: 0.00001575
Iteration 232/1000 | Loss: 0.00001575
Iteration 233/1000 | Loss: 0.00001575
Iteration 234/1000 | Loss: 0.00001575
Iteration 235/1000 | Loss: 0.00001575
Iteration 236/1000 | Loss: 0.00001575
Iteration 237/1000 | Loss: 0.00001575
Iteration 238/1000 | Loss: 0.00001575
Iteration 239/1000 | Loss: 0.00001575
Iteration 240/1000 | Loss: 0.00001575
Iteration 241/1000 | Loss: 0.00001575
Iteration 242/1000 | Loss: 0.00001575
Iteration 243/1000 | Loss: 0.00001574
Iteration 244/1000 | Loss: 0.00001574
Iteration 245/1000 | Loss: 0.00001574
Iteration 246/1000 | Loss: 0.00001574
Iteration 247/1000 | Loss: 0.00001573
Iteration 248/1000 | Loss: 0.00001573
Iteration 249/1000 | Loss: 0.00001573
Iteration 250/1000 | Loss: 0.00001573
Iteration 251/1000 | Loss: 0.00001573
Iteration 252/1000 | Loss: 0.00001573
Iteration 253/1000 | Loss: 0.00001572
Iteration 254/1000 | Loss: 0.00001572
Iteration 255/1000 | Loss: 0.00001572
Iteration 256/1000 | Loss: 0.00001572
Iteration 257/1000 | Loss: 0.00001572
Iteration 258/1000 | Loss: 0.00001572
Iteration 259/1000 | Loss: 0.00001572
Iteration 260/1000 | Loss: 0.00001572
Iteration 261/1000 | Loss: 0.00001572
Iteration 262/1000 | Loss: 0.00001572
Iteration 263/1000 | Loss: 0.00001572
Iteration 264/1000 | Loss: 0.00001572
Iteration 265/1000 | Loss: 0.00001572
Iteration 266/1000 | Loss: 0.00001572
Iteration 267/1000 | Loss: 0.00001571
Iteration 268/1000 | Loss: 0.00001571
Iteration 269/1000 | Loss: 0.00001571
Iteration 270/1000 | Loss: 0.00001571
Iteration 271/1000 | Loss: 0.00001571
Iteration 272/1000 | Loss: 0.00001571
Iteration 273/1000 | Loss: 0.00001570
Iteration 274/1000 | Loss: 0.00001570
Iteration 275/1000 | Loss: 0.00001570
Iteration 276/1000 | Loss: 0.00001570
Iteration 277/1000 | Loss: 0.00001570
Iteration 278/1000 | Loss: 0.00001570
Iteration 279/1000 | Loss: 0.00001570
Iteration 280/1000 | Loss: 0.00001570
Iteration 281/1000 | Loss: 0.00001570
Iteration 282/1000 | Loss: 0.00001570
Iteration 283/1000 | Loss: 0.00001569
Iteration 284/1000 | Loss: 0.00001569
Iteration 285/1000 | Loss: 0.00001569
Iteration 286/1000 | Loss: 0.00001569
Iteration 287/1000 | Loss: 0.00001569
Iteration 288/1000 | Loss: 0.00001568
Iteration 289/1000 | Loss: 0.00001568
Iteration 290/1000 | Loss: 0.00001568
Iteration 291/1000 | Loss: 0.00001568
Iteration 292/1000 | Loss: 0.00001568
Iteration 293/1000 | Loss: 0.00001568
Iteration 294/1000 | Loss: 0.00001568
Iteration 295/1000 | Loss: 0.00001568
Iteration 296/1000 | Loss: 0.00001567
Iteration 297/1000 | Loss: 0.00001567
Iteration 298/1000 | Loss: 0.00001567
Iteration 299/1000 | Loss: 0.00001567
Iteration 300/1000 | Loss: 0.00001567
Iteration 301/1000 | Loss: 0.00001567
Iteration 302/1000 | Loss: 0.00001566
Iteration 303/1000 | Loss: 0.00001566
Iteration 304/1000 | Loss: 0.00001566
Iteration 305/1000 | Loss: 0.00001566
Iteration 306/1000 | Loss: 0.00001566
Iteration 307/1000 | Loss: 0.00001566
Iteration 308/1000 | Loss: 0.00001566
Iteration 309/1000 | Loss: 0.00001566
Iteration 310/1000 | Loss: 0.00001566
Iteration 311/1000 | Loss: 0.00001566
Iteration 312/1000 | Loss: 0.00001566
Iteration 313/1000 | Loss: 0.00001566
Iteration 314/1000 | Loss: 0.00001566
Iteration 315/1000 | Loss: 0.00001566
Iteration 316/1000 | Loss: 0.00001566
Iteration 317/1000 | Loss: 0.00001565
Iteration 318/1000 | Loss: 0.00001565
Iteration 319/1000 | Loss: 0.00001565
Iteration 320/1000 | Loss: 0.00001565
Iteration 321/1000 | Loss: 0.00001565
Iteration 322/1000 | Loss: 0.00001565
Iteration 323/1000 | Loss: 0.00001565
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 323. Stopping optimization.
Last 5 losses: [1.565460115671158e-05, 1.565460115671158e-05, 1.565460115671158e-05, 1.565460115671158e-05, 1.565460115671158e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.565460115671158e-05

Optimization complete. Final v2v error: 3.2971136569976807 mm

Highest mean error: 3.7255594730377197 mm for frame 2

Lowest mean error: 3.033365249633789 mm for frame 41

Saving results

Total time: 52.359562397003174
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carina_posed_010/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00874864
Iteration 2/25 | Loss: 0.00203844
Iteration 3/25 | Loss: 0.00172573
Iteration 4/25 | Loss: 0.00158683
Iteration 5/25 | Loss: 0.00153858
Iteration 6/25 | Loss: 0.00152430
Iteration 7/25 | Loss: 0.00149406
Iteration 8/25 | Loss: 0.00146157
Iteration 9/25 | Loss: 0.00145024
Iteration 10/25 | Loss: 0.00143936
Iteration 11/25 | Loss: 0.00142711
Iteration 12/25 | Loss: 0.00142434
Iteration 13/25 | Loss: 0.00142888
Iteration 14/25 | Loss: 0.00142126
Iteration 15/25 | Loss: 0.00141805
Iteration 16/25 | Loss: 0.00142268
Iteration 17/25 | Loss: 0.00142428
Iteration 18/25 | Loss: 0.00142355
Iteration 19/25 | Loss: 0.00141760
Iteration 20/25 | Loss: 0.00141378
Iteration 21/25 | Loss: 0.00141465
Iteration 22/25 | Loss: 0.00141484
Iteration 23/25 | Loss: 0.00141237
Iteration 24/25 | Loss: 0.00141034
Iteration 25/25 | Loss: 0.00140872

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39077055
Iteration 2/25 | Loss: 0.00172778
Iteration 3/25 | Loss: 0.00172776
Iteration 4/25 | Loss: 0.00172776
Iteration 5/25 | Loss: 0.00172776
Iteration 6/25 | Loss: 0.00172776
Iteration 7/25 | Loss: 0.00172776
Iteration 8/25 | Loss: 0.00172776
Iteration 9/25 | Loss: 0.00172776
Iteration 10/25 | Loss: 0.00172776
Iteration 11/25 | Loss: 0.00172776
Iteration 12/25 | Loss: 0.00172776
Iteration 13/25 | Loss: 0.00172776
Iteration 14/25 | Loss: 0.00172776
Iteration 15/25 | Loss: 0.00172776
Iteration 16/25 | Loss: 0.00172776
Iteration 17/25 | Loss: 0.00172776
Iteration 18/25 | Loss: 0.00172776
Iteration 19/25 | Loss: 0.00172776
Iteration 20/25 | Loss: 0.00172776
Iteration 21/25 | Loss: 0.00172776
Iteration 22/25 | Loss: 0.00172776
Iteration 23/25 | Loss: 0.00172776
Iteration 24/25 | Loss: 0.00172776
Iteration 25/25 | Loss: 0.00172776

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00172776
Iteration 2/1000 | Loss: 0.00005582
Iteration 3/1000 | Loss: 0.00003758
Iteration 4/1000 | Loss: 0.00003343
Iteration 5/1000 | Loss: 0.00003133
Iteration 6/1000 | Loss: 0.00002963
Iteration 7/1000 | Loss: 0.00002850
Iteration 8/1000 | Loss: 0.00002776
Iteration 9/1000 | Loss: 0.00021525
Iteration 10/1000 | Loss: 0.00023759
Iteration 11/1000 | Loss: 0.00035380
Iteration 12/1000 | Loss: 0.00004440
Iteration 13/1000 | Loss: 0.00003809
Iteration 14/1000 | Loss: 0.00003335
Iteration 15/1000 | Loss: 0.00041258
Iteration 16/1000 | Loss: 0.00002809
Iteration 17/1000 | Loss: 0.00002292
Iteration 18/1000 | Loss: 0.00002204
Iteration 19/1000 | Loss: 0.00002120
Iteration 20/1000 | Loss: 0.00002054
Iteration 21/1000 | Loss: 0.00002010
Iteration 22/1000 | Loss: 0.00001968
Iteration 23/1000 | Loss: 0.00001939
Iteration 24/1000 | Loss: 0.00001914
Iteration 25/1000 | Loss: 0.00001910
Iteration 26/1000 | Loss: 0.00001900
Iteration 27/1000 | Loss: 0.00001896
Iteration 28/1000 | Loss: 0.00001895
Iteration 29/1000 | Loss: 0.00001895
Iteration 30/1000 | Loss: 0.00001894
Iteration 31/1000 | Loss: 0.00001893
Iteration 32/1000 | Loss: 0.00001893
Iteration 33/1000 | Loss: 0.00001893
Iteration 34/1000 | Loss: 0.00001892
Iteration 35/1000 | Loss: 0.00001891
Iteration 36/1000 | Loss: 0.00001891
Iteration 37/1000 | Loss: 0.00001890
Iteration 38/1000 | Loss: 0.00001889
Iteration 39/1000 | Loss: 0.00001889
Iteration 40/1000 | Loss: 0.00001887
Iteration 41/1000 | Loss: 0.00001884
Iteration 42/1000 | Loss: 0.00001884
Iteration 43/1000 | Loss: 0.00001883
Iteration 44/1000 | Loss: 0.00001883
Iteration 45/1000 | Loss: 0.00001883
Iteration 46/1000 | Loss: 0.00001883
Iteration 47/1000 | Loss: 0.00001882
Iteration 48/1000 | Loss: 0.00001882
Iteration 49/1000 | Loss: 0.00001882
Iteration 50/1000 | Loss: 0.00001882
Iteration 51/1000 | Loss: 0.00001881
Iteration 52/1000 | Loss: 0.00001881
Iteration 53/1000 | Loss: 0.00001881
Iteration 54/1000 | Loss: 0.00001881
Iteration 55/1000 | Loss: 0.00001881
Iteration 56/1000 | Loss: 0.00001881
Iteration 57/1000 | Loss: 0.00001881
Iteration 58/1000 | Loss: 0.00001881
Iteration 59/1000 | Loss: 0.00001881
Iteration 60/1000 | Loss: 0.00001881
Iteration 61/1000 | Loss: 0.00001880
Iteration 62/1000 | Loss: 0.00001880
Iteration 63/1000 | Loss: 0.00001879
Iteration 64/1000 | Loss: 0.00001879
Iteration 65/1000 | Loss: 0.00001879
Iteration 66/1000 | Loss: 0.00001879
Iteration 67/1000 | Loss: 0.00001879
Iteration 68/1000 | Loss: 0.00001878
Iteration 69/1000 | Loss: 0.00001878
Iteration 70/1000 | Loss: 0.00001878
Iteration 71/1000 | Loss: 0.00001878
Iteration 72/1000 | Loss: 0.00001878
Iteration 73/1000 | Loss: 0.00001878
Iteration 74/1000 | Loss: 0.00001877
Iteration 75/1000 | Loss: 0.00001877
Iteration 76/1000 | Loss: 0.00001877
Iteration 77/1000 | Loss: 0.00001877
Iteration 78/1000 | Loss: 0.00001877
Iteration 79/1000 | Loss: 0.00001877
Iteration 80/1000 | Loss: 0.00001877
Iteration 81/1000 | Loss: 0.00001876
Iteration 82/1000 | Loss: 0.00001876
Iteration 83/1000 | Loss: 0.00001876
Iteration 84/1000 | Loss: 0.00001876
Iteration 85/1000 | Loss: 0.00001876
Iteration 86/1000 | Loss: 0.00001876
Iteration 87/1000 | Loss: 0.00001876
Iteration 88/1000 | Loss: 0.00001875
Iteration 89/1000 | Loss: 0.00001875
Iteration 90/1000 | Loss: 0.00001874
Iteration 91/1000 | Loss: 0.00001874
Iteration 92/1000 | Loss: 0.00001873
Iteration 93/1000 | Loss: 0.00001873
Iteration 94/1000 | Loss: 0.00001873
Iteration 95/1000 | Loss: 0.00001872
Iteration 96/1000 | Loss: 0.00001872
Iteration 97/1000 | Loss: 0.00001872
Iteration 98/1000 | Loss: 0.00001872
Iteration 99/1000 | Loss: 0.00001872
Iteration 100/1000 | Loss: 0.00001872
Iteration 101/1000 | Loss: 0.00001872
Iteration 102/1000 | Loss: 0.00001871
Iteration 103/1000 | Loss: 0.00001871
Iteration 104/1000 | Loss: 0.00001871
Iteration 105/1000 | Loss: 0.00001871
Iteration 106/1000 | Loss: 0.00001871
Iteration 107/1000 | Loss: 0.00001871
Iteration 108/1000 | Loss: 0.00001871
Iteration 109/1000 | Loss: 0.00001871
Iteration 110/1000 | Loss: 0.00001871
Iteration 111/1000 | Loss: 0.00001871
Iteration 112/1000 | Loss: 0.00001871
Iteration 113/1000 | Loss: 0.00001871
Iteration 114/1000 | Loss: 0.00001871
Iteration 115/1000 | Loss: 0.00001871
Iteration 116/1000 | Loss: 0.00001871
Iteration 117/1000 | Loss: 0.00001870
Iteration 118/1000 | Loss: 0.00001870
Iteration 119/1000 | Loss: 0.00001870
Iteration 120/1000 | Loss: 0.00001870
Iteration 121/1000 | Loss: 0.00001870
Iteration 122/1000 | Loss: 0.00001870
Iteration 123/1000 | Loss: 0.00001870
Iteration 124/1000 | Loss: 0.00001870
Iteration 125/1000 | Loss: 0.00001870
Iteration 126/1000 | Loss: 0.00001870
Iteration 127/1000 | Loss: 0.00001869
Iteration 128/1000 | Loss: 0.00001869
Iteration 129/1000 | Loss: 0.00001869
Iteration 130/1000 | Loss: 0.00001869
Iteration 131/1000 | Loss: 0.00001869
Iteration 132/1000 | Loss: 0.00001869
Iteration 133/1000 | Loss: 0.00001869
Iteration 134/1000 | Loss: 0.00001869
Iteration 135/1000 | Loss: 0.00001869
Iteration 136/1000 | Loss: 0.00001869
Iteration 137/1000 | Loss: 0.00001869
Iteration 138/1000 | Loss: 0.00001869
Iteration 139/1000 | Loss: 0.00001869
Iteration 140/1000 | Loss: 0.00001869
Iteration 141/1000 | Loss: 0.00001869
Iteration 142/1000 | Loss: 0.00001869
Iteration 143/1000 | Loss: 0.00001869
Iteration 144/1000 | Loss: 0.00001869
Iteration 145/1000 | Loss: 0.00001869
Iteration 146/1000 | Loss: 0.00001869
Iteration 147/1000 | Loss: 0.00001868
Iteration 148/1000 | Loss: 0.00001868
Iteration 149/1000 | Loss: 0.00001868
Iteration 150/1000 | Loss: 0.00001868
Iteration 151/1000 | Loss: 0.00001868
Iteration 152/1000 | Loss: 0.00001868
Iteration 153/1000 | Loss: 0.00001868
Iteration 154/1000 | Loss: 0.00001868
Iteration 155/1000 | Loss: 0.00001868
Iteration 156/1000 | Loss: 0.00001868
Iteration 157/1000 | Loss: 0.00001868
Iteration 158/1000 | Loss: 0.00001868
Iteration 159/1000 | Loss: 0.00001868
Iteration 160/1000 | Loss: 0.00001868
Iteration 161/1000 | Loss: 0.00001868
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.867849459813442e-05, 1.867849459813442e-05, 1.867849459813442e-05, 1.867849459813442e-05, 1.867849459813442e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.867849459813442e-05

Optimization complete. Final v2v error: 3.6460189819335938 mm

Highest mean error: 5.952846527099609 mm for frame 69

Lowest mean error: 3.068819761276245 mm for frame 141

Saving results

Total time: 89.09052181243896
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carina_posed_010/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00410187
Iteration 2/25 | Loss: 0.00148211
Iteration 3/25 | Loss: 0.00137422
Iteration 4/25 | Loss: 0.00135772
Iteration 5/25 | Loss: 0.00135299
Iteration 6/25 | Loss: 0.00135172
Iteration 7/25 | Loss: 0.00135172
Iteration 8/25 | Loss: 0.00135172
Iteration 9/25 | Loss: 0.00135172
Iteration 10/25 | Loss: 0.00135172
Iteration 11/25 | Loss: 0.00135172
Iteration 12/25 | Loss: 0.00135172
Iteration 13/25 | Loss: 0.00135172
Iteration 14/25 | Loss: 0.00135172
Iteration 15/25 | Loss: 0.00135172
Iteration 16/25 | Loss: 0.00135172
Iteration 17/25 | Loss: 0.00135172
Iteration 18/25 | Loss: 0.00135172
Iteration 19/25 | Loss: 0.00135172
Iteration 20/25 | Loss: 0.00135172
Iteration 21/25 | Loss: 0.00135172
Iteration 22/25 | Loss: 0.00135172
Iteration 23/25 | Loss: 0.00135172
Iteration 24/25 | Loss: 0.00135172
Iteration 25/25 | Loss: 0.00135172
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0013517248444259167, 0.0013517248444259167, 0.0013517248444259167, 0.0013517248444259167, 0.0013517248444259167]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013517248444259167

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27781951
Iteration 2/25 | Loss: 0.00208147
Iteration 3/25 | Loss: 0.00208145
Iteration 4/25 | Loss: 0.00208145
Iteration 5/25 | Loss: 0.00208145
Iteration 6/25 | Loss: 0.00208145
Iteration 7/25 | Loss: 0.00208145
Iteration 8/25 | Loss: 0.00208145
Iteration 9/25 | Loss: 0.00208145
Iteration 10/25 | Loss: 0.00208145
Iteration 11/25 | Loss: 0.00208145
Iteration 12/25 | Loss: 0.00208145
Iteration 13/25 | Loss: 0.00208145
Iteration 14/25 | Loss: 0.00208145
Iteration 15/25 | Loss: 0.00208145
Iteration 16/25 | Loss: 0.00208145
Iteration 17/25 | Loss: 0.00208145
Iteration 18/25 | Loss: 0.00208145
Iteration 19/25 | Loss: 0.00208145
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0020814454182982445, 0.0020814454182982445, 0.0020814454182982445, 0.0020814454182982445, 0.0020814454182982445]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020814454182982445

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00208145
Iteration 2/1000 | Loss: 0.00005267
Iteration 3/1000 | Loss: 0.00003475
Iteration 4/1000 | Loss: 0.00002560
Iteration 5/1000 | Loss: 0.00002303
Iteration 6/1000 | Loss: 0.00002115
Iteration 7/1000 | Loss: 0.00001981
Iteration 8/1000 | Loss: 0.00001900
Iteration 9/1000 | Loss: 0.00001834
Iteration 10/1000 | Loss: 0.00001789
Iteration 11/1000 | Loss: 0.00001756
Iteration 12/1000 | Loss: 0.00001725
Iteration 13/1000 | Loss: 0.00001698
Iteration 14/1000 | Loss: 0.00001698
Iteration 15/1000 | Loss: 0.00001696
Iteration 16/1000 | Loss: 0.00001695
Iteration 17/1000 | Loss: 0.00001689
Iteration 18/1000 | Loss: 0.00001689
Iteration 19/1000 | Loss: 0.00001688
Iteration 20/1000 | Loss: 0.00001682
Iteration 21/1000 | Loss: 0.00001676
Iteration 22/1000 | Loss: 0.00001676
Iteration 23/1000 | Loss: 0.00001675
Iteration 24/1000 | Loss: 0.00001675
Iteration 25/1000 | Loss: 0.00001674
Iteration 26/1000 | Loss: 0.00001674
Iteration 27/1000 | Loss: 0.00001673
Iteration 28/1000 | Loss: 0.00001671
Iteration 29/1000 | Loss: 0.00001666
Iteration 30/1000 | Loss: 0.00001665
Iteration 31/1000 | Loss: 0.00001662
Iteration 32/1000 | Loss: 0.00001657
Iteration 33/1000 | Loss: 0.00001653
Iteration 34/1000 | Loss: 0.00001649
Iteration 35/1000 | Loss: 0.00001641
Iteration 36/1000 | Loss: 0.00001638
Iteration 37/1000 | Loss: 0.00001638
Iteration 38/1000 | Loss: 0.00001636
Iteration 39/1000 | Loss: 0.00001631
Iteration 40/1000 | Loss: 0.00001631
Iteration 41/1000 | Loss: 0.00001630
Iteration 42/1000 | Loss: 0.00001629
Iteration 43/1000 | Loss: 0.00001629
Iteration 44/1000 | Loss: 0.00001627
Iteration 45/1000 | Loss: 0.00001626
Iteration 46/1000 | Loss: 0.00001622
Iteration 47/1000 | Loss: 0.00001621
Iteration 48/1000 | Loss: 0.00001621
Iteration 49/1000 | Loss: 0.00001620
Iteration 50/1000 | Loss: 0.00001619
Iteration 51/1000 | Loss: 0.00001618
Iteration 52/1000 | Loss: 0.00001618
Iteration 53/1000 | Loss: 0.00001617
Iteration 54/1000 | Loss: 0.00001614
Iteration 55/1000 | Loss: 0.00001614
Iteration 56/1000 | Loss: 0.00001614
Iteration 57/1000 | Loss: 0.00001613
Iteration 58/1000 | Loss: 0.00001613
Iteration 59/1000 | Loss: 0.00001612
Iteration 60/1000 | Loss: 0.00001611
Iteration 61/1000 | Loss: 0.00001611
Iteration 62/1000 | Loss: 0.00001610
Iteration 63/1000 | Loss: 0.00001610
Iteration 64/1000 | Loss: 0.00001609
Iteration 65/1000 | Loss: 0.00001609
Iteration 66/1000 | Loss: 0.00001609
Iteration 67/1000 | Loss: 0.00001609
Iteration 68/1000 | Loss: 0.00001609
Iteration 69/1000 | Loss: 0.00001609
Iteration 70/1000 | Loss: 0.00001608
Iteration 71/1000 | Loss: 0.00001608
Iteration 72/1000 | Loss: 0.00001608
Iteration 73/1000 | Loss: 0.00001608
Iteration 74/1000 | Loss: 0.00001607
Iteration 75/1000 | Loss: 0.00001607
Iteration 76/1000 | Loss: 0.00001607
Iteration 77/1000 | Loss: 0.00001607
Iteration 78/1000 | Loss: 0.00001606
Iteration 79/1000 | Loss: 0.00001606
Iteration 80/1000 | Loss: 0.00001606
Iteration 81/1000 | Loss: 0.00001606
Iteration 82/1000 | Loss: 0.00001605
Iteration 83/1000 | Loss: 0.00001605
Iteration 84/1000 | Loss: 0.00001605
Iteration 85/1000 | Loss: 0.00001605
Iteration 86/1000 | Loss: 0.00001604
Iteration 87/1000 | Loss: 0.00001604
Iteration 88/1000 | Loss: 0.00001604
Iteration 89/1000 | Loss: 0.00001603
Iteration 90/1000 | Loss: 0.00001603
Iteration 91/1000 | Loss: 0.00001603
Iteration 92/1000 | Loss: 0.00001602
Iteration 93/1000 | Loss: 0.00001600
Iteration 94/1000 | Loss: 0.00001599
Iteration 95/1000 | Loss: 0.00001599
Iteration 96/1000 | Loss: 0.00001599
Iteration 97/1000 | Loss: 0.00001598
Iteration 98/1000 | Loss: 0.00001597
Iteration 99/1000 | Loss: 0.00001597
Iteration 100/1000 | Loss: 0.00001597
Iteration 101/1000 | Loss: 0.00001596
Iteration 102/1000 | Loss: 0.00001596
Iteration 103/1000 | Loss: 0.00001596
Iteration 104/1000 | Loss: 0.00001596
Iteration 105/1000 | Loss: 0.00001595
Iteration 106/1000 | Loss: 0.00001595
Iteration 107/1000 | Loss: 0.00001595
Iteration 108/1000 | Loss: 0.00001595
Iteration 109/1000 | Loss: 0.00001595
Iteration 110/1000 | Loss: 0.00001594
Iteration 111/1000 | Loss: 0.00001594
Iteration 112/1000 | Loss: 0.00001594
Iteration 113/1000 | Loss: 0.00001593
Iteration 114/1000 | Loss: 0.00001593
Iteration 115/1000 | Loss: 0.00001593
Iteration 116/1000 | Loss: 0.00001592
Iteration 117/1000 | Loss: 0.00001592
Iteration 118/1000 | Loss: 0.00001592
Iteration 119/1000 | Loss: 0.00001592
Iteration 120/1000 | Loss: 0.00001591
Iteration 121/1000 | Loss: 0.00001591
Iteration 122/1000 | Loss: 0.00001591
Iteration 123/1000 | Loss: 0.00001590
Iteration 124/1000 | Loss: 0.00001590
Iteration 125/1000 | Loss: 0.00001590
Iteration 126/1000 | Loss: 0.00001589
Iteration 127/1000 | Loss: 0.00001589
Iteration 128/1000 | Loss: 0.00001589
Iteration 129/1000 | Loss: 0.00001588
Iteration 130/1000 | Loss: 0.00001588
Iteration 131/1000 | Loss: 0.00001588
Iteration 132/1000 | Loss: 0.00001588
Iteration 133/1000 | Loss: 0.00001587
Iteration 134/1000 | Loss: 0.00001587
Iteration 135/1000 | Loss: 0.00001587
Iteration 136/1000 | Loss: 0.00001587
Iteration 137/1000 | Loss: 0.00001587
Iteration 138/1000 | Loss: 0.00001587
Iteration 139/1000 | Loss: 0.00001586
Iteration 140/1000 | Loss: 0.00001586
Iteration 141/1000 | Loss: 0.00001586
Iteration 142/1000 | Loss: 0.00001586
Iteration 143/1000 | Loss: 0.00001586
Iteration 144/1000 | Loss: 0.00001585
Iteration 145/1000 | Loss: 0.00001585
Iteration 146/1000 | Loss: 0.00001585
Iteration 147/1000 | Loss: 0.00001585
Iteration 148/1000 | Loss: 0.00001584
Iteration 149/1000 | Loss: 0.00001584
Iteration 150/1000 | Loss: 0.00001584
Iteration 151/1000 | Loss: 0.00001584
Iteration 152/1000 | Loss: 0.00001584
Iteration 153/1000 | Loss: 0.00001584
Iteration 154/1000 | Loss: 0.00001583
Iteration 155/1000 | Loss: 0.00001583
Iteration 156/1000 | Loss: 0.00001583
Iteration 157/1000 | Loss: 0.00001583
Iteration 158/1000 | Loss: 0.00001583
Iteration 159/1000 | Loss: 0.00001582
Iteration 160/1000 | Loss: 0.00001582
Iteration 161/1000 | Loss: 0.00001582
Iteration 162/1000 | Loss: 0.00001582
Iteration 163/1000 | Loss: 0.00001582
Iteration 164/1000 | Loss: 0.00001582
Iteration 165/1000 | Loss: 0.00001581
Iteration 166/1000 | Loss: 0.00001581
Iteration 167/1000 | Loss: 0.00001581
Iteration 168/1000 | Loss: 0.00001581
Iteration 169/1000 | Loss: 0.00001581
Iteration 170/1000 | Loss: 0.00001581
Iteration 171/1000 | Loss: 0.00001581
Iteration 172/1000 | Loss: 0.00001581
Iteration 173/1000 | Loss: 0.00001581
Iteration 174/1000 | Loss: 0.00001581
Iteration 175/1000 | Loss: 0.00001581
Iteration 176/1000 | Loss: 0.00001581
Iteration 177/1000 | Loss: 0.00001581
Iteration 178/1000 | Loss: 0.00001581
Iteration 179/1000 | Loss: 0.00001581
Iteration 180/1000 | Loss: 0.00001581
Iteration 181/1000 | Loss: 0.00001581
Iteration 182/1000 | Loss: 0.00001581
Iteration 183/1000 | Loss: 0.00001581
Iteration 184/1000 | Loss: 0.00001580
Iteration 185/1000 | Loss: 0.00001580
Iteration 186/1000 | Loss: 0.00001580
Iteration 187/1000 | Loss: 0.00001580
Iteration 188/1000 | Loss: 0.00001580
Iteration 189/1000 | Loss: 0.00001580
Iteration 190/1000 | Loss: 0.00001580
Iteration 191/1000 | Loss: 0.00001580
Iteration 192/1000 | Loss: 0.00001580
Iteration 193/1000 | Loss: 0.00001580
Iteration 194/1000 | Loss: 0.00001580
Iteration 195/1000 | Loss: 0.00001580
Iteration 196/1000 | Loss: 0.00001579
Iteration 197/1000 | Loss: 0.00001579
Iteration 198/1000 | Loss: 0.00001579
Iteration 199/1000 | Loss: 0.00001579
Iteration 200/1000 | Loss: 0.00001579
Iteration 201/1000 | Loss: 0.00001579
Iteration 202/1000 | Loss: 0.00001579
Iteration 203/1000 | Loss: 0.00001579
Iteration 204/1000 | Loss: 0.00001579
Iteration 205/1000 | Loss: 0.00001579
Iteration 206/1000 | Loss: 0.00001578
Iteration 207/1000 | Loss: 0.00001578
Iteration 208/1000 | Loss: 0.00001578
Iteration 209/1000 | Loss: 0.00001578
Iteration 210/1000 | Loss: 0.00001578
Iteration 211/1000 | Loss: 0.00001578
Iteration 212/1000 | Loss: 0.00001578
Iteration 213/1000 | Loss: 0.00001578
Iteration 214/1000 | Loss: 0.00001578
Iteration 215/1000 | Loss: 0.00001577
Iteration 216/1000 | Loss: 0.00001577
Iteration 217/1000 | Loss: 0.00001577
Iteration 218/1000 | Loss: 0.00001577
Iteration 219/1000 | Loss: 0.00001577
Iteration 220/1000 | Loss: 0.00001577
Iteration 221/1000 | Loss: 0.00001576
Iteration 222/1000 | Loss: 0.00001576
Iteration 223/1000 | Loss: 0.00001576
Iteration 224/1000 | Loss: 0.00001576
Iteration 225/1000 | Loss: 0.00001576
Iteration 226/1000 | Loss: 0.00001575
Iteration 227/1000 | Loss: 0.00001575
Iteration 228/1000 | Loss: 0.00001575
Iteration 229/1000 | Loss: 0.00001575
Iteration 230/1000 | Loss: 0.00001574
Iteration 231/1000 | Loss: 0.00001574
Iteration 232/1000 | Loss: 0.00001574
Iteration 233/1000 | Loss: 0.00001574
Iteration 234/1000 | Loss: 0.00001574
Iteration 235/1000 | Loss: 0.00001574
Iteration 236/1000 | Loss: 0.00001574
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 236. Stopping optimization.
Last 5 losses: [1.574390262248926e-05, 1.574390262248926e-05, 1.574390262248926e-05, 1.574390262248926e-05, 1.574390262248926e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.574390262248926e-05

Optimization complete. Final v2v error: 3.291557550430298 mm

Highest mean error: 5.299740314483643 mm for frame 73

Lowest mean error: 2.7342326641082764 mm for frame 132

Saving results

Total time: 51.04267406463623
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carina_posed_010/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00798479
Iteration 2/25 | Loss: 0.00202442
Iteration 3/25 | Loss: 0.00159076
Iteration 4/25 | Loss: 0.00153424
Iteration 5/25 | Loss: 0.00153157
Iteration 6/25 | Loss: 0.00149755
Iteration 7/25 | Loss: 0.00151259
Iteration 8/25 | Loss: 0.00146098
Iteration 9/25 | Loss: 0.00144475
Iteration 10/25 | Loss: 0.00144093
Iteration 11/25 | Loss: 0.00144023
Iteration 12/25 | Loss: 0.00144016
Iteration 13/25 | Loss: 0.00144016
Iteration 14/25 | Loss: 0.00144016
Iteration 15/25 | Loss: 0.00144016
Iteration 16/25 | Loss: 0.00144016
Iteration 17/25 | Loss: 0.00144016
Iteration 18/25 | Loss: 0.00144016
Iteration 19/25 | Loss: 0.00144016
Iteration 20/25 | Loss: 0.00144016
Iteration 21/25 | Loss: 0.00144016
Iteration 22/25 | Loss: 0.00144016
Iteration 23/25 | Loss: 0.00144016
Iteration 24/25 | Loss: 0.00144016
Iteration 25/25 | Loss: 0.00144016

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21537471
Iteration 2/25 | Loss: 0.00193688
Iteration 3/25 | Loss: 0.00193687
Iteration 4/25 | Loss: 0.00193687
Iteration 5/25 | Loss: 0.00193687
Iteration 6/25 | Loss: 0.00193687
Iteration 7/25 | Loss: 0.00193687
Iteration 8/25 | Loss: 0.00193687
Iteration 9/25 | Loss: 0.00193687
Iteration 10/25 | Loss: 0.00193687
Iteration 11/25 | Loss: 0.00193687
Iteration 12/25 | Loss: 0.00193687
Iteration 13/25 | Loss: 0.00193687
Iteration 14/25 | Loss: 0.00193687
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0019368695793673396, 0.0019368695793673396, 0.0019368695793673396, 0.0019368695793673396, 0.0019368695793673396]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019368695793673396

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00193687
Iteration 2/1000 | Loss: 0.00300228
Iteration 3/1000 | Loss: 0.00065825
Iteration 4/1000 | Loss: 0.00064049
Iteration 5/1000 | Loss: 0.00065794
Iteration 6/1000 | Loss: 0.00032976
Iteration 7/1000 | Loss: 0.00020345
Iteration 8/1000 | Loss: 0.00007637
Iteration 9/1000 | Loss: 0.00005797
Iteration 10/1000 | Loss: 0.00005036
Iteration 11/1000 | Loss: 0.00131270
Iteration 12/1000 | Loss: 0.00023020
Iteration 13/1000 | Loss: 0.00009430
Iteration 14/1000 | Loss: 0.00007806
Iteration 15/1000 | Loss: 0.00022925
Iteration 16/1000 | Loss: 0.00008266
Iteration 17/1000 | Loss: 0.00020643
Iteration 18/1000 | Loss: 0.00016118
Iteration 19/1000 | Loss: 0.00019757
Iteration 20/1000 | Loss: 0.00131196
Iteration 21/1000 | Loss: 0.00050573
Iteration 22/1000 | Loss: 0.00038349
Iteration 23/1000 | Loss: 0.00058139
Iteration 24/1000 | Loss: 0.00021031
Iteration 25/1000 | Loss: 0.00009597
Iteration 26/1000 | Loss: 0.00006510
Iteration 27/1000 | Loss: 0.00007320
Iteration 28/1000 | Loss: 0.00005114
Iteration 29/1000 | Loss: 0.00004589
Iteration 30/1000 | Loss: 0.00004358
Iteration 31/1000 | Loss: 0.00004190
Iteration 32/1000 | Loss: 0.00004007
Iteration 33/1000 | Loss: 0.00003871
Iteration 34/1000 | Loss: 0.00003729
Iteration 35/1000 | Loss: 0.00004542
Iteration 36/1000 | Loss: 0.00003735
Iteration 37/1000 | Loss: 0.00004082
Iteration 38/1000 | Loss: 0.00003595
Iteration 39/1000 | Loss: 0.00003447
Iteration 40/1000 | Loss: 0.00003426
Iteration 41/1000 | Loss: 0.00003220
Iteration 42/1000 | Loss: 0.00003123
Iteration 43/1000 | Loss: 0.00018678
Iteration 44/1000 | Loss: 0.00004706
Iteration 45/1000 | Loss: 0.00003717
Iteration 46/1000 | Loss: 0.00003277
Iteration 47/1000 | Loss: 0.00019675
Iteration 48/1000 | Loss: 0.00013597
Iteration 49/1000 | Loss: 0.00019387
Iteration 50/1000 | Loss: 0.00020407
Iteration 51/1000 | Loss: 0.00005699
Iteration 52/1000 | Loss: 0.00004400
Iteration 53/1000 | Loss: 0.00003952
Iteration 54/1000 | Loss: 0.00003637
Iteration 55/1000 | Loss: 0.00003816
Iteration 56/1000 | Loss: 0.00004228
Iteration 57/1000 | Loss: 0.00003911
Iteration 58/1000 | Loss: 0.00003085
Iteration 59/1000 | Loss: 0.00003895
Iteration 60/1000 | Loss: 0.00003166
Iteration 61/1000 | Loss: 0.00002849
Iteration 62/1000 | Loss: 0.00004437
Iteration 63/1000 | Loss: 0.00003367
Iteration 64/1000 | Loss: 0.00003094
Iteration 65/1000 | Loss: 0.00002920
Iteration 66/1000 | Loss: 0.00004622
Iteration 67/1000 | Loss: 0.00002952
Iteration 68/1000 | Loss: 0.00018782
Iteration 69/1000 | Loss: 0.00004838
Iteration 70/1000 | Loss: 0.00003174
Iteration 71/1000 | Loss: 0.00002948
Iteration 72/1000 | Loss: 0.00002845
Iteration 73/1000 | Loss: 0.00002797
Iteration 74/1000 | Loss: 0.00002758
Iteration 75/1000 | Loss: 0.00002720
Iteration 76/1000 | Loss: 0.00002693
Iteration 77/1000 | Loss: 0.00016339
Iteration 78/1000 | Loss: 0.00003753
Iteration 79/1000 | Loss: 0.00003188
Iteration 80/1000 | Loss: 0.00002954
Iteration 81/1000 | Loss: 0.00002843
Iteration 82/1000 | Loss: 0.00002817
Iteration 83/1000 | Loss: 0.00002793
Iteration 84/1000 | Loss: 0.00002781
Iteration 85/1000 | Loss: 0.00002763
Iteration 86/1000 | Loss: 0.00002754
Iteration 87/1000 | Loss: 0.00002749
Iteration 88/1000 | Loss: 0.00009867
Iteration 89/1000 | Loss: 0.00003761
Iteration 90/1000 | Loss: 0.00002971
Iteration 91/1000 | Loss: 0.00002712
Iteration 92/1000 | Loss: 0.00002651
Iteration 93/1000 | Loss: 0.00002620
Iteration 94/1000 | Loss: 0.00002593
Iteration 95/1000 | Loss: 0.00002579
Iteration 96/1000 | Loss: 0.00002559
Iteration 97/1000 | Loss: 0.00002550
Iteration 98/1000 | Loss: 0.00002540
Iteration 99/1000 | Loss: 0.00002539
Iteration 100/1000 | Loss: 0.00002539
Iteration 101/1000 | Loss: 0.00002538
Iteration 102/1000 | Loss: 0.00002538
Iteration 103/1000 | Loss: 0.00002536
Iteration 104/1000 | Loss: 0.00002536
Iteration 105/1000 | Loss: 0.00002535
Iteration 106/1000 | Loss: 0.00002535
Iteration 107/1000 | Loss: 0.00002535
Iteration 108/1000 | Loss: 0.00002535
Iteration 109/1000 | Loss: 0.00002535
Iteration 110/1000 | Loss: 0.00002534
Iteration 111/1000 | Loss: 0.00002534
Iteration 112/1000 | Loss: 0.00002533
Iteration 113/1000 | Loss: 0.00002533
Iteration 114/1000 | Loss: 0.00002531
Iteration 115/1000 | Loss: 0.00002530
Iteration 116/1000 | Loss: 0.00002530
Iteration 117/1000 | Loss: 0.00002529
Iteration 118/1000 | Loss: 0.00002529
Iteration 119/1000 | Loss: 0.00002529
Iteration 120/1000 | Loss: 0.00002529
Iteration 121/1000 | Loss: 0.00002528
Iteration 122/1000 | Loss: 0.00002528
Iteration 123/1000 | Loss: 0.00002528
Iteration 124/1000 | Loss: 0.00002528
Iteration 125/1000 | Loss: 0.00002528
Iteration 126/1000 | Loss: 0.00002528
Iteration 127/1000 | Loss: 0.00002528
Iteration 128/1000 | Loss: 0.00002527
Iteration 129/1000 | Loss: 0.00002527
Iteration 130/1000 | Loss: 0.00002527
Iteration 131/1000 | Loss: 0.00002527
Iteration 132/1000 | Loss: 0.00002527
Iteration 133/1000 | Loss: 0.00002527
Iteration 134/1000 | Loss: 0.00002527
Iteration 135/1000 | Loss: 0.00002527
Iteration 136/1000 | Loss: 0.00002527
Iteration 137/1000 | Loss: 0.00002527
Iteration 138/1000 | Loss: 0.00002527
Iteration 139/1000 | Loss: 0.00002527
Iteration 140/1000 | Loss: 0.00002527
Iteration 141/1000 | Loss: 0.00002526
Iteration 142/1000 | Loss: 0.00002526
Iteration 143/1000 | Loss: 0.00002526
Iteration 144/1000 | Loss: 0.00002526
Iteration 145/1000 | Loss: 0.00002526
Iteration 146/1000 | Loss: 0.00002526
Iteration 147/1000 | Loss: 0.00002526
Iteration 148/1000 | Loss: 0.00002526
Iteration 149/1000 | Loss: 0.00002526
Iteration 150/1000 | Loss: 0.00002526
Iteration 151/1000 | Loss: 0.00002526
Iteration 152/1000 | Loss: 0.00002526
Iteration 153/1000 | Loss: 0.00002525
Iteration 154/1000 | Loss: 0.00002525
Iteration 155/1000 | Loss: 0.00002525
Iteration 156/1000 | Loss: 0.00002525
Iteration 157/1000 | Loss: 0.00002525
Iteration 158/1000 | Loss: 0.00002525
Iteration 159/1000 | Loss: 0.00002525
Iteration 160/1000 | Loss: 0.00002525
Iteration 161/1000 | Loss: 0.00002525
Iteration 162/1000 | Loss: 0.00002525
Iteration 163/1000 | Loss: 0.00002525
Iteration 164/1000 | Loss: 0.00002525
Iteration 165/1000 | Loss: 0.00002525
Iteration 166/1000 | Loss: 0.00002525
Iteration 167/1000 | Loss: 0.00002525
Iteration 168/1000 | Loss: 0.00002525
Iteration 169/1000 | Loss: 0.00002525
Iteration 170/1000 | Loss: 0.00002525
Iteration 171/1000 | Loss: 0.00002525
Iteration 172/1000 | Loss: 0.00002525
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [2.5248275051126257e-05, 2.5248275051126257e-05, 2.5248275051126257e-05, 2.5248275051126257e-05, 2.5248275051126257e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5248275051126257e-05

Optimization complete. Final v2v error: 4.099379539489746 mm

Highest mean error: 6.081609725952148 mm for frame 147

Lowest mean error: 3.4808309078216553 mm for frame 151

Saving results

Total time: 181.09058046340942
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carina_posed_010/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00363304
Iteration 2/25 | Loss: 0.00147167
Iteration 3/25 | Loss: 0.00136775
Iteration 4/25 | Loss: 0.00135115
Iteration 5/25 | Loss: 0.00134511
Iteration 6/25 | Loss: 0.00134433
Iteration 7/25 | Loss: 0.00134433
Iteration 8/25 | Loss: 0.00134433
Iteration 9/25 | Loss: 0.00134433
Iteration 10/25 | Loss: 0.00134433
Iteration 11/25 | Loss: 0.00134433
Iteration 12/25 | Loss: 0.00134433
Iteration 13/25 | Loss: 0.00134433
Iteration 14/25 | Loss: 0.00134433
Iteration 15/25 | Loss: 0.00134433
Iteration 16/25 | Loss: 0.00134433
Iteration 17/25 | Loss: 0.00134433
Iteration 18/25 | Loss: 0.00134433
Iteration 19/25 | Loss: 0.00134433
Iteration 20/25 | Loss: 0.00134433
Iteration 21/25 | Loss: 0.00134433
Iteration 22/25 | Loss: 0.00134433
Iteration 23/25 | Loss: 0.00134433
Iteration 24/25 | Loss: 0.00134433
Iteration 25/25 | Loss: 0.00134433

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24578834
Iteration 2/25 | Loss: 0.00269211
Iteration 3/25 | Loss: 0.00269211
Iteration 4/25 | Loss: 0.00269211
Iteration 5/25 | Loss: 0.00269211
Iteration 6/25 | Loss: 0.00269211
Iteration 7/25 | Loss: 0.00269211
Iteration 8/25 | Loss: 0.00269211
Iteration 9/25 | Loss: 0.00269211
Iteration 10/25 | Loss: 0.00269211
Iteration 11/25 | Loss: 0.00269211
Iteration 12/25 | Loss: 0.00269211
Iteration 13/25 | Loss: 0.00269211
Iteration 14/25 | Loss: 0.00269211
Iteration 15/25 | Loss: 0.00269211
Iteration 16/25 | Loss: 0.00269211
Iteration 17/25 | Loss: 0.00269211
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0026921085081994534, 0.0026921085081994534, 0.0026921085081994534, 0.0026921085081994534, 0.0026921085081994534]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026921085081994534

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00269211
Iteration 2/1000 | Loss: 0.00005849
Iteration 3/1000 | Loss: 0.00004052
Iteration 4/1000 | Loss: 0.00003143
Iteration 5/1000 | Loss: 0.00002816
Iteration 6/1000 | Loss: 0.00002684
Iteration 7/1000 | Loss: 0.00002542
Iteration 8/1000 | Loss: 0.00002455
Iteration 9/1000 | Loss: 0.00002403
Iteration 10/1000 | Loss: 0.00002358
Iteration 11/1000 | Loss: 0.00002321
Iteration 12/1000 | Loss: 0.00002280
Iteration 13/1000 | Loss: 0.00002256
Iteration 14/1000 | Loss: 0.00002253
Iteration 15/1000 | Loss: 0.00002237
Iteration 16/1000 | Loss: 0.00002219
Iteration 17/1000 | Loss: 0.00002218
Iteration 18/1000 | Loss: 0.00002211
Iteration 19/1000 | Loss: 0.00002199
Iteration 20/1000 | Loss: 0.00002194
Iteration 21/1000 | Loss: 0.00002193
Iteration 22/1000 | Loss: 0.00002192
Iteration 23/1000 | Loss: 0.00002191
Iteration 24/1000 | Loss: 0.00002189
Iteration 25/1000 | Loss: 0.00002182
Iteration 26/1000 | Loss: 0.00002182
Iteration 27/1000 | Loss: 0.00002181
Iteration 28/1000 | Loss: 0.00002177
Iteration 29/1000 | Loss: 0.00002177
Iteration 30/1000 | Loss: 0.00002173
Iteration 31/1000 | Loss: 0.00002167
Iteration 32/1000 | Loss: 0.00002167
Iteration 33/1000 | Loss: 0.00002162
Iteration 34/1000 | Loss: 0.00002156
Iteration 35/1000 | Loss: 0.00002153
Iteration 36/1000 | Loss: 0.00002153
Iteration 37/1000 | Loss: 0.00002152
Iteration 38/1000 | Loss: 0.00002152
Iteration 39/1000 | Loss: 0.00002151
Iteration 40/1000 | Loss: 0.00002150
Iteration 41/1000 | Loss: 0.00002150
Iteration 42/1000 | Loss: 0.00002149
Iteration 43/1000 | Loss: 0.00002148
Iteration 44/1000 | Loss: 0.00002147
Iteration 45/1000 | Loss: 0.00002147
Iteration 46/1000 | Loss: 0.00002147
Iteration 47/1000 | Loss: 0.00002147
Iteration 48/1000 | Loss: 0.00002146
Iteration 49/1000 | Loss: 0.00002146
Iteration 50/1000 | Loss: 0.00002146
Iteration 51/1000 | Loss: 0.00002146
Iteration 52/1000 | Loss: 0.00002146
Iteration 53/1000 | Loss: 0.00002145
Iteration 54/1000 | Loss: 0.00002145
Iteration 55/1000 | Loss: 0.00002144
Iteration 56/1000 | Loss: 0.00002144
Iteration 57/1000 | Loss: 0.00002144
Iteration 58/1000 | Loss: 0.00002143
Iteration 59/1000 | Loss: 0.00002143
Iteration 60/1000 | Loss: 0.00002143
Iteration 61/1000 | Loss: 0.00002142
Iteration 62/1000 | Loss: 0.00002142
Iteration 63/1000 | Loss: 0.00002142
Iteration 64/1000 | Loss: 0.00002142
Iteration 65/1000 | Loss: 0.00002142
Iteration 66/1000 | Loss: 0.00002142
Iteration 67/1000 | Loss: 0.00002141
Iteration 68/1000 | Loss: 0.00002141
Iteration 69/1000 | Loss: 0.00002141
Iteration 70/1000 | Loss: 0.00002141
Iteration 71/1000 | Loss: 0.00002140
Iteration 72/1000 | Loss: 0.00002140
Iteration 73/1000 | Loss: 0.00002140
Iteration 74/1000 | Loss: 0.00002140
Iteration 75/1000 | Loss: 0.00002139
Iteration 76/1000 | Loss: 0.00002139
Iteration 77/1000 | Loss: 0.00002139
Iteration 78/1000 | Loss: 0.00002139
Iteration 79/1000 | Loss: 0.00002139
Iteration 80/1000 | Loss: 0.00002138
Iteration 81/1000 | Loss: 0.00002138
Iteration 82/1000 | Loss: 0.00002138
Iteration 83/1000 | Loss: 0.00002138
Iteration 84/1000 | Loss: 0.00002138
Iteration 85/1000 | Loss: 0.00002138
Iteration 86/1000 | Loss: 0.00002137
Iteration 87/1000 | Loss: 0.00002137
Iteration 88/1000 | Loss: 0.00002137
Iteration 89/1000 | Loss: 0.00002137
Iteration 90/1000 | Loss: 0.00002137
Iteration 91/1000 | Loss: 0.00002137
Iteration 92/1000 | Loss: 0.00002137
Iteration 93/1000 | Loss: 0.00002136
Iteration 94/1000 | Loss: 0.00002136
Iteration 95/1000 | Loss: 0.00002136
Iteration 96/1000 | Loss: 0.00002136
Iteration 97/1000 | Loss: 0.00002136
Iteration 98/1000 | Loss: 0.00002136
Iteration 99/1000 | Loss: 0.00002135
Iteration 100/1000 | Loss: 0.00002135
Iteration 101/1000 | Loss: 0.00002135
Iteration 102/1000 | Loss: 0.00002135
Iteration 103/1000 | Loss: 0.00002135
Iteration 104/1000 | Loss: 0.00002135
Iteration 105/1000 | Loss: 0.00002135
Iteration 106/1000 | Loss: 0.00002134
Iteration 107/1000 | Loss: 0.00002134
Iteration 108/1000 | Loss: 0.00002134
Iteration 109/1000 | Loss: 0.00002134
Iteration 110/1000 | Loss: 0.00002134
Iteration 111/1000 | Loss: 0.00002134
Iteration 112/1000 | Loss: 0.00002134
Iteration 113/1000 | Loss: 0.00002133
Iteration 114/1000 | Loss: 0.00002133
Iteration 115/1000 | Loss: 0.00002133
Iteration 116/1000 | Loss: 0.00002133
Iteration 117/1000 | Loss: 0.00002133
Iteration 118/1000 | Loss: 0.00002132
Iteration 119/1000 | Loss: 0.00002132
Iteration 120/1000 | Loss: 0.00002132
Iteration 121/1000 | Loss: 0.00002132
Iteration 122/1000 | Loss: 0.00002132
Iteration 123/1000 | Loss: 0.00002132
Iteration 124/1000 | Loss: 0.00002132
Iteration 125/1000 | Loss: 0.00002131
Iteration 126/1000 | Loss: 0.00002131
Iteration 127/1000 | Loss: 0.00002131
Iteration 128/1000 | Loss: 0.00002131
Iteration 129/1000 | Loss: 0.00002131
Iteration 130/1000 | Loss: 0.00002131
Iteration 131/1000 | Loss: 0.00002131
Iteration 132/1000 | Loss: 0.00002131
Iteration 133/1000 | Loss: 0.00002130
Iteration 134/1000 | Loss: 0.00002130
Iteration 135/1000 | Loss: 0.00002130
Iteration 136/1000 | Loss: 0.00002130
Iteration 137/1000 | Loss: 0.00002129
Iteration 138/1000 | Loss: 0.00002129
Iteration 139/1000 | Loss: 0.00002129
Iteration 140/1000 | Loss: 0.00002129
Iteration 141/1000 | Loss: 0.00002129
Iteration 142/1000 | Loss: 0.00002128
Iteration 143/1000 | Loss: 0.00002128
Iteration 144/1000 | Loss: 0.00002128
Iteration 145/1000 | Loss: 0.00002128
Iteration 146/1000 | Loss: 0.00002128
Iteration 147/1000 | Loss: 0.00002128
Iteration 148/1000 | Loss: 0.00002127
Iteration 149/1000 | Loss: 0.00002127
Iteration 150/1000 | Loss: 0.00002127
Iteration 151/1000 | Loss: 0.00002126
Iteration 152/1000 | Loss: 0.00002126
Iteration 153/1000 | Loss: 0.00002125
Iteration 154/1000 | Loss: 0.00002125
Iteration 155/1000 | Loss: 0.00002125
Iteration 156/1000 | Loss: 0.00002125
Iteration 157/1000 | Loss: 0.00002125
Iteration 158/1000 | Loss: 0.00002125
Iteration 159/1000 | Loss: 0.00002125
Iteration 160/1000 | Loss: 0.00002124
Iteration 161/1000 | Loss: 0.00002124
Iteration 162/1000 | Loss: 0.00002124
Iteration 163/1000 | Loss: 0.00002124
Iteration 164/1000 | Loss: 0.00002124
Iteration 165/1000 | Loss: 0.00002123
Iteration 166/1000 | Loss: 0.00002123
Iteration 167/1000 | Loss: 0.00002123
Iteration 168/1000 | Loss: 0.00002123
Iteration 169/1000 | Loss: 0.00002123
Iteration 170/1000 | Loss: 0.00002123
Iteration 171/1000 | Loss: 0.00002123
Iteration 172/1000 | Loss: 0.00002122
Iteration 173/1000 | Loss: 0.00002122
Iteration 174/1000 | Loss: 0.00002122
Iteration 175/1000 | Loss: 0.00002122
Iteration 176/1000 | Loss: 0.00002122
Iteration 177/1000 | Loss: 0.00002122
Iteration 178/1000 | Loss: 0.00002122
Iteration 179/1000 | Loss: 0.00002122
Iteration 180/1000 | Loss: 0.00002121
Iteration 181/1000 | Loss: 0.00002121
Iteration 182/1000 | Loss: 0.00002121
Iteration 183/1000 | Loss: 0.00002121
Iteration 184/1000 | Loss: 0.00002121
Iteration 185/1000 | Loss: 0.00002121
Iteration 186/1000 | Loss: 0.00002121
Iteration 187/1000 | Loss: 0.00002121
Iteration 188/1000 | Loss: 0.00002121
Iteration 189/1000 | Loss: 0.00002121
Iteration 190/1000 | Loss: 0.00002121
Iteration 191/1000 | Loss: 0.00002121
Iteration 192/1000 | Loss: 0.00002121
Iteration 193/1000 | Loss: 0.00002121
Iteration 194/1000 | Loss: 0.00002120
Iteration 195/1000 | Loss: 0.00002120
Iteration 196/1000 | Loss: 0.00002120
Iteration 197/1000 | Loss: 0.00002120
Iteration 198/1000 | Loss: 0.00002120
Iteration 199/1000 | Loss: 0.00002120
Iteration 200/1000 | Loss: 0.00002120
Iteration 201/1000 | Loss: 0.00002120
Iteration 202/1000 | Loss: 0.00002120
Iteration 203/1000 | Loss: 0.00002120
Iteration 204/1000 | Loss: 0.00002120
Iteration 205/1000 | Loss: 0.00002120
Iteration 206/1000 | Loss: 0.00002120
Iteration 207/1000 | Loss: 0.00002119
Iteration 208/1000 | Loss: 0.00002119
Iteration 209/1000 | Loss: 0.00002119
Iteration 210/1000 | Loss: 0.00002119
Iteration 211/1000 | Loss: 0.00002119
Iteration 212/1000 | Loss: 0.00002119
Iteration 213/1000 | Loss: 0.00002119
Iteration 214/1000 | Loss: 0.00002118
Iteration 215/1000 | Loss: 0.00002118
Iteration 216/1000 | Loss: 0.00002118
Iteration 217/1000 | Loss: 0.00002118
Iteration 218/1000 | Loss: 0.00002118
Iteration 219/1000 | Loss: 0.00002118
Iteration 220/1000 | Loss: 0.00002118
Iteration 221/1000 | Loss: 0.00002118
Iteration 222/1000 | Loss: 0.00002118
Iteration 223/1000 | Loss: 0.00002118
Iteration 224/1000 | Loss: 0.00002118
Iteration 225/1000 | Loss: 0.00002117
Iteration 226/1000 | Loss: 0.00002117
Iteration 227/1000 | Loss: 0.00002117
Iteration 228/1000 | Loss: 0.00002117
Iteration 229/1000 | Loss: 0.00002117
Iteration 230/1000 | Loss: 0.00002117
Iteration 231/1000 | Loss: 0.00002117
Iteration 232/1000 | Loss: 0.00002117
Iteration 233/1000 | Loss: 0.00002117
Iteration 234/1000 | Loss: 0.00002117
Iteration 235/1000 | Loss: 0.00002117
Iteration 236/1000 | Loss: 0.00002116
Iteration 237/1000 | Loss: 0.00002116
Iteration 238/1000 | Loss: 0.00002116
Iteration 239/1000 | Loss: 0.00002116
Iteration 240/1000 | Loss: 0.00002116
Iteration 241/1000 | Loss: 0.00002116
Iteration 242/1000 | Loss: 0.00002116
Iteration 243/1000 | Loss: 0.00002115
Iteration 244/1000 | Loss: 0.00002115
Iteration 245/1000 | Loss: 0.00002115
Iteration 246/1000 | Loss: 0.00002115
Iteration 247/1000 | Loss: 0.00002115
Iteration 248/1000 | Loss: 0.00002115
Iteration 249/1000 | Loss: 0.00002115
Iteration 250/1000 | Loss: 0.00002115
Iteration 251/1000 | Loss: 0.00002115
Iteration 252/1000 | Loss: 0.00002115
Iteration 253/1000 | Loss: 0.00002115
Iteration 254/1000 | Loss: 0.00002115
Iteration 255/1000 | Loss: 0.00002115
Iteration 256/1000 | Loss: 0.00002115
Iteration 257/1000 | Loss: 0.00002115
Iteration 258/1000 | Loss: 0.00002115
Iteration 259/1000 | Loss: 0.00002114
Iteration 260/1000 | Loss: 0.00002114
Iteration 261/1000 | Loss: 0.00002114
Iteration 262/1000 | Loss: 0.00002114
Iteration 263/1000 | Loss: 0.00002114
Iteration 264/1000 | Loss: 0.00002114
Iteration 265/1000 | Loss: 0.00002114
Iteration 266/1000 | Loss: 0.00002114
Iteration 267/1000 | Loss: 0.00002113
Iteration 268/1000 | Loss: 0.00002113
Iteration 269/1000 | Loss: 0.00002113
Iteration 270/1000 | Loss: 0.00002113
Iteration 271/1000 | Loss: 0.00002113
Iteration 272/1000 | Loss: 0.00002113
Iteration 273/1000 | Loss: 0.00002113
Iteration 274/1000 | Loss: 0.00002113
Iteration 275/1000 | Loss: 0.00002113
Iteration 276/1000 | Loss: 0.00002113
Iteration 277/1000 | Loss: 0.00002113
Iteration 278/1000 | Loss: 0.00002113
Iteration 279/1000 | Loss: 0.00002113
Iteration 280/1000 | Loss: 0.00002113
Iteration 281/1000 | Loss: 0.00002112
Iteration 282/1000 | Loss: 0.00002112
Iteration 283/1000 | Loss: 0.00002112
Iteration 284/1000 | Loss: 0.00002112
Iteration 285/1000 | Loss: 0.00002112
Iteration 286/1000 | Loss: 0.00002112
Iteration 287/1000 | Loss: 0.00002112
Iteration 288/1000 | Loss: 0.00002112
Iteration 289/1000 | Loss: 0.00002112
Iteration 290/1000 | Loss: 0.00002112
Iteration 291/1000 | Loss: 0.00002112
Iteration 292/1000 | Loss: 0.00002112
Iteration 293/1000 | Loss: 0.00002112
Iteration 294/1000 | Loss: 0.00002112
Iteration 295/1000 | Loss: 0.00002112
Iteration 296/1000 | Loss: 0.00002112
Iteration 297/1000 | Loss: 0.00002112
Iteration 298/1000 | Loss: 0.00002112
Iteration 299/1000 | Loss: 0.00002112
Iteration 300/1000 | Loss: 0.00002112
Iteration 301/1000 | Loss: 0.00002112
Iteration 302/1000 | Loss: 0.00002112
Iteration 303/1000 | Loss: 0.00002112
Iteration 304/1000 | Loss: 0.00002112
Iteration 305/1000 | Loss: 0.00002112
Iteration 306/1000 | Loss: 0.00002112
Iteration 307/1000 | Loss: 0.00002112
Iteration 308/1000 | Loss: 0.00002112
Iteration 309/1000 | Loss: 0.00002112
Iteration 310/1000 | Loss: 0.00002112
Iteration 311/1000 | Loss: 0.00002112
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 311. Stopping optimization.
Last 5 losses: [2.1123863916727714e-05, 2.1123863916727714e-05, 2.1123863916727714e-05, 2.1123863916727714e-05, 2.1123863916727714e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1123863916727714e-05

Optimization complete. Final v2v error: 3.8549859523773193 mm

Highest mean error: 5.108239650726318 mm for frame 188

Lowest mean error: 2.796344518661499 mm for frame 220

Saving results

Total time: 62.707218647003174
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carina_posed_010/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00442826
Iteration 2/25 | Loss: 0.00148842
Iteration 3/25 | Loss: 0.00140152
Iteration 4/25 | Loss: 0.00139913
Iteration 5/25 | Loss: 0.00139845
Iteration 6/25 | Loss: 0.00139845
Iteration 7/25 | Loss: 0.00139845
Iteration 8/25 | Loss: 0.00139845
Iteration 9/25 | Loss: 0.00139845
Iteration 10/25 | Loss: 0.00139845
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013984538381919265, 0.0013984538381919265, 0.0013984538381919265, 0.0013984538381919265, 0.0013984538381919265]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013984538381919265

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39502847
Iteration 2/25 | Loss: 0.00143393
Iteration 3/25 | Loss: 0.00143392
Iteration 4/25 | Loss: 0.00143392
Iteration 5/25 | Loss: 0.00143392
Iteration 6/25 | Loss: 0.00143392
Iteration 7/25 | Loss: 0.00143392
Iteration 8/25 | Loss: 0.00143392
Iteration 9/25 | Loss: 0.00143392
Iteration 10/25 | Loss: 0.00143392
Iteration 11/25 | Loss: 0.00143392
Iteration 12/25 | Loss: 0.00143392
Iteration 13/25 | Loss: 0.00143392
Iteration 14/25 | Loss: 0.00143392
Iteration 15/25 | Loss: 0.00143392
Iteration 16/25 | Loss: 0.00143392
Iteration 17/25 | Loss: 0.00143392
Iteration 18/25 | Loss: 0.00143392
Iteration 19/25 | Loss: 0.00143392
Iteration 20/25 | Loss: 0.00143392
Iteration 21/25 | Loss: 0.00143392
Iteration 22/25 | Loss: 0.00143392
Iteration 23/25 | Loss: 0.00143392
Iteration 24/25 | Loss: 0.00143392
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0014339167391881347, 0.0014339167391881347, 0.0014339167391881347, 0.0014339167391881347, 0.0014339167391881347]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014339167391881347

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00143392
Iteration 2/1000 | Loss: 0.00003231
Iteration 3/1000 | Loss: 0.00002285
Iteration 4/1000 | Loss: 0.00002106
Iteration 5/1000 | Loss: 0.00002012
Iteration 6/1000 | Loss: 0.00001945
Iteration 7/1000 | Loss: 0.00001891
Iteration 8/1000 | Loss: 0.00001840
Iteration 9/1000 | Loss: 0.00001805
Iteration 10/1000 | Loss: 0.00001781
Iteration 11/1000 | Loss: 0.00001754
Iteration 12/1000 | Loss: 0.00001748
Iteration 13/1000 | Loss: 0.00001738
Iteration 14/1000 | Loss: 0.00001724
Iteration 15/1000 | Loss: 0.00001720
Iteration 16/1000 | Loss: 0.00001718
Iteration 17/1000 | Loss: 0.00001717
Iteration 18/1000 | Loss: 0.00001717
Iteration 19/1000 | Loss: 0.00001713
Iteration 20/1000 | Loss: 0.00001708
Iteration 21/1000 | Loss: 0.00001707
Iteration 22/1000 | Loss: 0.00001697
Iteration 23/1000 | Loss: 0.00001695
Iteration 24/1000 | Loss: 0.00001691
Iteration 25/1000 | Loss: 0.00001690
Iteration 26/1000 | Loss: 0.00001690
Iteration 27/1000 | Loss: 0.00001689
Iteration 28/1000 | Loss: 0.00001689
Iteration 29/1000 | Loss: 0.00001682
Iteration 30/1000 | Loss: 0.00001682
Iteration 31/1000 | Loss: 0.00001679
Iteration 32/1000 | Loss: 0.00001679
Iteration 33/1000 | Loss: 0.00001678
Iteration 34/1000 | Loss: 0.00001676
Iteration 35/1000 | Loss: 0.00001673
Iteration 36/1000 | Loss: 0.00001669
Iteration 37/1000 | Loss: 0.00001669
Iteration 38/1000 | Loss: 0.00001669
Iteration 39/1000 | Loss: 0.00001669
Iteration 40/1000 | Loss: 0.00001669
Iteration 41/1000 | Loss: 0.00001668
Iteration 42/1000 | Loss: 0.00001668
Iteration 43/1000 | Loss: 0.00001668
Iteration 44/1000 | Loss: 0.00001668
Iteration 45/1000 | Loss: 0.00001668
Iteration 46/1000 | Loss: 0.00001668
Iteration 47/1000 | Loss: 0.00001668
Iteration 48/1000 | Loss: 0.00001668
Iteration 49/1000 | Loss: 0.00001668
Iteration 50/1000 | Loss: 0.00001667
Iteration 51/1000 | Loss: 0.00001665
Iteration 52/1000 | Loss: 0.00001665
Iteration 53/1000 | Loss: 0.00001664
Iteration 54/1000 | Loss: 0.00001664
Iteration 55/1000 | Loss: 0.00001663
Iteration 56/1000 | Loss: 0.00001663
Iteration 57/1000 | Loss: 0.00001663
Iteration 58/1000 | Loss: 0.00001663
Iteration 59/1000 | Loss: 0.00001663
Iteration 60/1000 | Loss: 0.00001663
Iteration 61/1000 | Loss: 0.00001663
Iteration 62/1000 | Loss: 0.00001662
Iteration 63/1000 | Loss: 0.00001661
Iteration 64/1000 | Loss: 0.00001661
Iteration 65/1000 | Loss: 0.00001660
Iteration 66/1000 | Loss: 0.00001660
Iteration 67/1000 | Loss: 0.00001660
Iteration 68/1000 | Loss: 0.00001660
Iteration 69/1000 | Loss: 0.00001660
Iteration 70/1000 | Loss: 0.00001659
Iteration 71/1000 | Loss: 0.00001659
Iteration 72/1000 | Loss: 0.00001659
Iteration 73/1000 | Loss: 0.00001658
Iteration 74/1000 | Loss: 0.00001658
Iteration 75/1000 | Loss: 0.00001658
Iteration 76/1000 | Loss: 0.00001658
Iteration 77/1000 | Loss: 0.00001658
Iteration 78/1000 | Loss: 0.00001658
Iteration 79/1000 | Loss: 0.00001657
Iteration 80/1000 | Loss: 0.00001657
Iteration 81/1000 | Loss: 0.00001657
Iteration 82/1000 | Loss: 0.00001656
Iteration 83/1000 | Loss: 0.00001656
Iteration 84/1000 | Loss: 0.00001656
Iteration 85/1000 | Loss: 0.00001656
Iteration 86/1000 | Loss: 0.00001656
Iteration 87/1000 | Loss: 0.00001656
Iteration 88/1000 | Loss: 0.00001656
Iteration 89/1000 | Loss: 0.00001656
Iteration 90/1000 | Loss: 0.00001656
Iteration 91/1000 | Loss: 0.00001656
Iteration 92/1000 | Loss: 0.00001656
Iteration 93/1000 | Loss: 0.00001655
Iteration 94/1000 | Loss: 0.00001655
Iteration 95/1000 | Loss: 0.00001655
Iteration 96/1000 | Loss: 0.00001655
Iteration 97/1000 | Loss: 0.00001655
Iteration 98/1000 | Loss: 0.00001655
Iteration 99/1000 | Loss: 0.00001655
Iteration 100/1000 | Loss: 0.00001655
Iteration 101/1000 | Loss: 0.00001655
Iteration 102/1000 | Loss: 0.00001655
Iteration 103/1000 | Loss: 0.00001654
Iteration 104/1000 | Loss: 0.00001654
Iteration 105/1000 | Loss: 0.00001654
Iteration 106/1000 | Loss: 0.00001654
Iteration 107/1000 | Loss: 0.00001654
Iteration 108/1000 | Loss: 0.00001654
Iteration 109/1000 | Loss: 0.00001654
Iteration 110/1000 | Loss: 0.00001654
Iteration 111/1000 | Loss: 0.00001654
Iteration 112/1000 | Loss: 0.00001654
Iteration 113/1000 | Loss: 0.00001654
Iteration 114/1000 | Loss: 0.00001654
Iteration 115/1000 | Loss: 0.00001653
Iteration 116/1000 | Loss: 0.00001653
Iteration 117/1000 | Loss: 0.00001653
Iteration 118/1000 | Loss: 0.00001653
Iteration 119/1000 | Loss: 0.00001653
Iteration 120/1000 | Loss: 0.00001653
Iteration 121/1000 | Loss: 0.00001653
Iteration 122/1000 | Loss: 0.00001653
Iteration 123/1000 | Loss: 0.00001653
Iteration 124/1000 | Loss: 0.00001653
Iteration 125/1000 | Loss: 0.00001652
Iteration 126/1000 | Loss: 0.00001652
Iteration 127/1000 | Loss: 0.00001652
Iteration 128/1000 | Loss: 0.00001652
Iteration 129/1000 | Loss: 0.00001652
Iteration 130/1000 | Loss: 0.00001652
Iteration 131/1000 | Loss: 0.00001652
Iteration 132/1000 | Loss: 0.00001652
Iteration 133/1000 | Loss: 0.00001652
Iteration 134/1000 | Loss: 0.00001652
Iteration 135/1000 | Loss: 0.00001652
Iteration 136/1000 | Loss: 0.00001652
Iteration 137/1000 | Loss: 0.00001652
Iteration 138/1000 | Loss: 0.00001652
Iteration 139/1000 | Loss: 0.00001651
Iteration 140/1000 | Loss: 0.00001651
Iteration 141/1000 | Loss: 0.00001651
Iteration 142/1000 | Loss: 0.00001650
Iteration 143/1000 | Loss: 0.00001650
Iteration 144/1000 | Loss: 0.00001650
Iteration 145/1000 | Loss: 0.00001650
Iteration 146/1000 | Loss: 0.00001649
Iteration 147/1000 | Loss: 0.00001649
Iteration 148/1000 | Loss: 0.00001649
Iteration 149/1000 | Loss: 0.00001649
Iteration 150/1000 | Loss: 0.00001649
Iteration 151/1000 | Loss: 0.00001649
Iteration 152/1000 | Loss: 0.00001649
Iteration 153/1000 | Loss: 0.00001649
Iteration 154/1000 | Loss: 0.00001648
Iteration 155/1000 | Loss: 0.00001648
Iteration 156/1000 | Loss: 0.00001648
Iteration 157/1000 | Loss: 0.00001648
Iteration 158/1000 | Loss: 0.00001647
Iteration 159/1000 | Loss: 0.00001647
Iteration 160/1000 | Loss: 0.00001647
Iteration 161/1000 | Loss: 0.00001647
Iteration 162/1000 | Loss: 0.00001647
Iteration 163/1000 | Loss: 0.00001647
Iteration 164/1000 | Loss: 0.00001647
Iteration 165/1000 | Loss: 0.00001647
Iteration 166/1000 | Loss: 0.00001646
Iteration 167/1000 | Loss: 0.00001646
Iteration 168/1000 | Loss: 0.00001646
Iteration 169/1000 | Loss: 0.00001646
Iteration 170/1000 | Loss: 0.00001646
Iteration 171/1000 | Loss: 0.00001646
Iteration 172/1000 | Loss: 0.00001646
Iteration 173/1000 | Loss: 0.00001646
Iteration 174/1000 | Loss: 0.00001646
Iteration 175/1000 | Loss: 0.00001646
Iteration 176/1000 | Loss: 0.00001645
Iteration 177/1000 | Loss: 0.00001645
Iteration 178/1000 | Loss: 0.00001645
Iteration 179/1000 | Loss: 0.00001645
Iteration 180/1000 | Loss: 0.00001645
Iteration 181/1000 | Loss: 0.00001645
Iteration 182/1000 | Loss: 0.00001645
Iteration 183/1000 | Loss: 0.00001645
Iteration 184/1000 | Loss: 0.00001645
Iteration 185/1000 | Loss: 0.00001645
Iteration 186/1000 | Loss: 0.00001645
Iteration 187/1000 | Loss: 0.00001644
Iteration 188/1000 | Loss: 0.00001644
Iteration 189/1000 | Loss: 0.00001644
Iteration 190/1000 | Loss: 0.00001644
Iteration 191/1000 | Loss: 0.00001644
Iteration 192/1000 | Loss: 0.00001644
Iteration 193/1000 | Loss: 0.00001644
Iteration 194/1000 | Loss: 0.00001644
Iteration 195/1000 | Loss: 0.00001644
Iteration 196/1000 | Loss: 0.00001644
Iteration 197/1000 | Loss: 0.00001644
Iteration 198/1000 | Loss: 0.00001644
Iteration 199/1000 | Loss: 0.00001643
Iteration 200/1000 | Loss: 0.00001643
Iteration 201/1000 | Loss: 0.00001643
Iteration 202/1000 | Loss: 0.00001643
Iteration 203/1000 | Loss: 0.00001643
Iteration 204/1000 | Loss: 0.00001643
Iteration 205/1000 | Loss: 0.00001643
Iteration 206/1000 | Loss: 0.00001643
Iteration 207/1000 | Loss: 0.00001643
Iteration 208/1000 | Loss: 0.00001643
Iteration 209/1000 | Loss: 0.00001642
Iteration 210/1000 | Loss: 0.00001642
Iteration 211/1000 | Loss: 0.00001642
Iteration 212/1000 | Loss: 0.00001642
Iteration 213/1000 | Loss: 0.00001641
Iteration 214/1000 | Loss: 0.00001641
Iteration 215/1000 | Loss: 0.00001641
Iteration 216/1000 | Loss: 0.00001641
Iteration 217/1000 | Loss: 0.00001641
Iteration 218/1000 | Loss: 0.00001641
Iteration 219/1000 | Loss: 0.00001641
Iteration 220/1000 | Loss: 0.00001641
Iteration 221/1000 | Loss: 0.00001641
Iteration 222/1000 | Loss: 0.00001641
Iteration 223/1000 | Loss: 0.00001641
Iteration 224/1000 | Loss: 0.00001641
Iteration 225/1000 | Loss: 0.00001641
Iteration 226/1000 | Loss: 0.00001641
Iteration 227/1000 | Loss: 0.00001641
Iteration 228/1000 | Loss: 0.00001641
Iteration 229/1000 | Loss: 0.00001641
Iteration 230/1000 | Loss: 0.00001641
Iteration 231/1000 | Loss: 0.00001641
Iteration 232/1000 | Loss: 0.00001641
Iteration 233/1000 | Loss: 0.00001641
Iteration 234/1000 | Loss: 0.00001641
Iteration 235/1000 | Loss: 0.00001641
Iteration 236/1000 | Loss: 0.00001641
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 236. Stopping optimization.
Last 5 losses: [1.6410745956818573e-05, 1.6410745956818573e-05, 1.6410745956818573e-05, 1.6410745956818573e-05, 1.6410745956818573e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6410745956818573e-05

Optimization complete. Final v2v error: 3.43972110748291 mm

Highest mean error: 3.7050116062164307 mm for frame 106

Lowest mean error: 3.27523136138916 mm for frame 78

Saving results

Total time: 43.29562592506409
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carina_posed_010/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00532283
Iteration 2/25 | Loss: 0.00171597
Iteration 3/25 | Loss: 0.00146362
Iteration 4/25 | Loss: 0.00143773
Iteration 5/25 | Loss: 0.00143106
Iteration 6/25 | Loss: 0.00142945
Iteration 7/25 | Loss: 0.00142945
Iteration 8/25 | Loss: 0.00142945
Iteration 9/25 | Loss: 0.00142945
Iteration 10/25 | Loss: 0.00142945
Iteration 11/25 | Loss: 0.00142945
Iteration 12/25 | Loss: 0.00142945
Iteration 13/25 | Loss: 0.00142945
Iteration 14/25 | Loss: 0.00142945
Iteration 15/25 | Loss: 0.00142945
Iteration 16/25 | Loss: 0.00142945
Iteration 17/25 | Loss: 0.00142945
Iteration 18/25 | Loss: 0.00142945
Iteration 19/25 | Loss: 0.00142945
Iteration 20/25 | Loss: 0.00142945
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0014294509310275316, 0.0014294509310275316, 0.0014294509310275316, 0.0014294509310275316, 0.0014294509310275316]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014294509310275316

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.84439290
Iteration 2/25 | Loss: 0.00190847
Iteration 3/25 | Loss: 0.00190843
Iteration 4/25 | Loss: 0.00190842
Iteration 5/25 | Loss: 0.00190842
Iteration 6/25 | Loss: 0.00190842
Iteration 7/25 | Loss: 0.00190842
Iteration 8/25 | Loss: 0.00190842
Iteration 9/25 | Loss: 0.00190842
Iteration 10/25 | Loss: 0.00190842
Iteration 11/25 | Loss: 0.00190842
Iteration 12/25 | Loss: 0.00190842
Iteration 13/25 | Loss: 0.00190842
Iteration 14/25 | Loss: 0.00190842
Iteration 15/25 | Loss: 0.00190842
Iteration 16/25 | Loss: 0.00190842
Iteration 17/25 | Loss: 0.00190842
Iteration 18/25 | Loss: 0.00190842
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0019084223313257098, 0.0019084223313257098, 0.0019084223313257098, 0.0019084223313257098, 0.0019084223313257098]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019084223313257098

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00190842
Iteration 2/1000 | Loss: 0.00008002
Iteration 3/1000 | Loss: 0.00004256
Iteration 4/1000 | Loss: 0.00003611
Iteration 5/1000 | Loss: 0.00003376
Iteration 6/1000 | Loss: 0.00003193
Iteration 7/1000 | Loss: 0.00003083
Iteration 8/1000 | Loss: 0.00002992
Iteration 9/1000 | Loss: 0.00002925
Iteration 10/1000 | Loss: 0.00002876
Iteration 11/1000 | Loss: 0.00002824
Iteration 12/1000 | Loss: 0.00002783
Iteration 13/1000 | Loss: 0.00002749
Iteration 14/1000 | Loss: 0.00002716
Iteration 15/1000 | Loss: 0.00002686
Iteration 16/1000 | Loss: 0.00002675
Iteration 17/1000 | Loss: 0.00002660
Iteration 18/1000 | Loss: 0.00002640
Iteration 19/1000 | Loss: 0.00002623
Iteration 20/1000 | Loss: 0.00002620
Iteration 21/1000 | Loss: 0.00002620
Iteration 22/1000 | Loss: 0.00002613
Iteration 23/1000 | Loss: 0.00002605
Iteration 24/1000 | Loss: 0.00002596
Iteration 25/1000 | Loss: 0.00002592
Iteration 26/1000 | Loss: 0.00002592
Iteration 27/1000 | Loss: 0.00002589
Iteration 28/1000 | Loss: 0.00002589
Iteration 29/1000 | Loss: 0.00002589
Iteration 30/1000 | Loss: 0.00002589
Iteration 31/1000 | Loss: 0.00002589
Iteration 32/1000 | Loss: 0.00002589
Iteration 33/1000 | Loss: 0.00002587
Iteration 34/1000 | Loss: 0.00002587
Iteration 35/1000 | Loss: 0.00002587
Iteration 36/1000 | Loss: 0.00002587
Iteration 37/1000 | Loss: 0.00002587
Iteration 38/1000 | Loss: 0.00002587
Iteration 39/1000 | Loss: 0.00002586
Iteration 40/1000 | Loss: 0.00002586
Iteration 41/1000 | Loss: 0.00002585
Iteration 42/1000 | Loss: 0.00002584
Iteration 43/1000 | Loss: 0.00002584
Iteration 44/1000 | Loss: 0.00002584
Iteration 45/1000 | Loss: 0.00002583
Iteration 46/1000 | Loss: 0.00002583
Iteration 47/1000 | Loss: 0.00002583
Iteration 48/1000 | Loss: 0.00002583
Iteration 49/1000 | Loss: 0.00002582
Iteration 50/1000 | Loss: 0.00002582
Iteration 51/1000 | Loss: 0.00002582
Iteration 52/1000 | Loss: 0.00002582
Iteration 53/1000 | Loss: 0.00002582
Iteration 54/1000 | Loss: 0.00002582
Iteration 55/1000 | Loss: 0.00002582
Iteration 56/1000 | Loss: 0.00002581
Iteration 57/1000 | Loss: 0.00002581
Iteration 58/1000 | Loss: 0.00002580
Iteration 59/1000 | Loss: 0.00002580
Iteration 60/1000 | Loss: 0.00002580
Iteration 61/1000 | Loss: 0.00002580
Iteration 62/1000 | Loss: 0.00002580
Iteration 63/1000 | Loss: 0.00002580
Iteration 64/1000 | Loss: 0.00002580
Iteration 65/1000 | Loss: 0.00002579
Iteration 66/1000 | Loss: 0.00002579
Iteration 67/1000 | Loss: 0.00002579
Iteration 68/1000 | Loss: 0.00002579
Iteration 69/1000 | Loss: 0.00002579
Iteration 70/1000 | Loss: 0.00002579
Iteration 71/1000 | Loss: 0.00002578
Iteration 72/1000 | Loss: 0.00002578
Iteration 73/1000 | Loss: 0.00002578
Iteration 74/1000 | Loss: 0.00002578
Iteration 75/1000 | Loss: 0.00002578
Iteration 76/1000 | Loss: 0.00002578
Iteration 77/1000 | Loss: 0.00002578
Iteration 78/1000 | Loss: 0.00002578
Iteration 79/1000 | Loss: 0.00002578
Iteration 80/1000 | Loss: 0.00002578
Iteration 81/1000 | Loss: 0.00002578
Iteration 82/1000 | Loss: 0.00002578
Iteration 83/1000 | Loss: 0.00002578
Iteration 84/1000 | Loss: 0.00002577
Iteration 85/1000 | Loss: 0.00002577
Iteration 86/1000 | Loss: 0.00002576
Iteration 87/1000 | Loss: 0.00002576
Iteration 88/1000 | Loss: 0.00002576
Iteration 89/1000 | Loss: 0.00002576
Iteration 90/1000 | Loss: 0.00002576
Iteration 91/1000 | Loss: 0.00002576
Iteration 92/1000 | Loss: 0.00002576
Iteration 93/1000 | Loss: 0.00002576
Iteration 94/1000 | Loss: 0.00002576
Iteration 95/1000 | Loss: 0.00002576
Iteration 96/1000 | Loss: 0.00002576
Iteration 97/1000 | Loss: 0.00002576
Iteration 98/1000 | Loss: 0.00002575
Iteration 99/1000 | Loss: 0.00002575
Iteration 100/1000 | Loss: 0.00002575
Iteration 101/1000 | Loss: 0.00002575
Iteration 102/1000 | Loss: 0.00002575
Iteration 103/1000 | Loss: 0.00002575
Iteration 104/1000 | Loss: 0.00002574
Iteration 105/1000 | Loss: 0.00002574
Iteration 106/1000 | Loss: 0.00002574
Iteration 107/1000 | Loss: 0.00002574
Iteration 108/1000 | Loss: 0.00002574
Iteration 109/1000 | Loss: 0.00002574
Iteration 110/1000 | Loss: 0.00002574
Iteration 111/1000 | Loss: 0.00002574
Iteration 112/1000 | Loss: 0.00002574
Iteration 113/1000 | Loss: 0.00002574
Iteration 114/1000 | Loss: 0.00002573
Iteration 115/1000 | Loss: 0.00002573
Iteration 116/1000 | Loss: 0.00002573
Iteration 117/1000 | Loss: 0.00002573
Iteration 118/1000 | Loss: 0.00002573
Iteration 119/1000 | Loss: 0.00002573
Iteration 120/1000 | Loss: 0.00002573
Iteration 121/1000 | Loss: 0.00002573
Iteration 122/1000 | Loss: 0.00002573
Iteration 123/1000 | Loss: 0.00002573
Iteration 124/1000 | Loss: 0.00002573
Iteration 125/1000 | Loss: 0.00002573
Iteration 126/1000 | Loss: 0.00002572
Iteration 127/1000 | Loss: 0.00002572
Iteration 128/1000 | Loss: 0.00002572
Iteration 129/1000 | Loss: 0.00002572
Iteration 130/1000 | Loss: 0.00002572
Iteration 131/1000 | Loss: 0.00002572
Iteration 132/1000 | Loss: 0.00002572
Iteration 133/1000 | Loss: 0.00002571
Iteration 134/1000 | Loss: 0.00002571
Iteration 135/1000 | Loss: 0.00002571
Iteration 136/1000 | Loss: 0.00002571
Iteration 137/1000 | Loss: 0.00002571
Iteration 138/1000 | Loss: 0.00002571
Iteration 139/1000 | Loss: 0.00002571
Iteration 140/1000 | Loss: 0.00002571
Iteration 141/1000 | Loss: 0.00002571
Iteration 142/1000 | Loss: 0.00002571
Iteration 143/1000 | Loss: 0.00002571
Iteration 144/1000 | Loss: 0.00002571
Iteration 145/1000 | Loss: 0.00002571
Iteration 146/1000 | Loss: 0.00002570
Iteration 147/1000 | Loss: 0.00002570
Iteration 148/1000 | Loss: 0.00002570
Iteration 149/1000 | Loss: 0.00002570
Iteration 150/1000 | Loss: 0.00002570
Iteration 151/1000 | Loss: 0.00002570
Iteration 152/1000 | Loss: 0.00002570
Iteration 153/1000 | Loss: 0.00002570
Iteration 154/1000 | Loss: 0.00002570
Iteration 155/1000 | Loss: 0.00002570
Iteration 156/1000 | Loss: 0.00002570
Iteration 157/1000 | Loss: 0.00002570
Iteration 158/1000 | Loss: 0.00002570
Iteration 159/1000 | Loss: 0.00002570
Iteration 160/1000 | Loss: 0.00002570
Iteration 161/1000 | Loss: 0.00002570
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [2.570265860413201e-05, 2.570265860413201e-05, 2.570265860413201e-05, 2.570265860413201e-05, 2.570265860413201e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.570265860413201e-05

Optimization complete. Final v2v error: 4.246330261230469 mm

Highest mean error: 5.227964401245117 mm for frame 19

Lowest mean error: 3.754014730453491 mm for frame 90

Saving results

Total time: 48.88684153556824
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carina_posed_010/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00453206
Iteration 2/25 | Loss: 0.00159070
Iteration 3/25 | Loss: 0.00142060
Iteration 4/25 | Loss: 0.00139078
Iteration 5/25 | Loss: 0.00138442
Iteration 6/25 | Loss: 0.00138294
Iteration 7/25 | Loss: 0.00138294
Iteration 8/25 | Loss: 0.00138294
Iteration 9/25 | Loss: 0.00138294
Iteration 10/25 | Loss: 0.00138294
Iteration 11/25 | Loss: 0.00138294
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013829373056069016, 0.0013829373056069016, 0.0013829373056069016, 0.0013829373056069016, 0.0013829373056069016]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013829373056069016

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16888762
Iteration 2/25 | Loss: 0.00267499
Iteration 3/25 | Loss: 0.00267499
Iteration 4/25 | Loss: 0.00267499
Iteration 5/25 | Loss: 0.00267498
Iteration 6/25 | Loss: 0.00267498
Iteration 7/25 | Loss: 0.00267498
Iteration 8/25 | Loss: 0.00267498
Iteration 9/25 | Loss: 0.00267498
Iteration 10/25 | Loss: 0.00267498
Iteration 11/25 | Loss: 0.00267498
Iteration 12/25 | Loss: 0.00267498
Iteration 13/25 | Loss: 0.00267498
Iteration 14/25 | Loss: 0.00267498
Iteration 15/25 | Loss: 0.00267498
Iteration 16/25 | Loss: 0.00267498
Iteration 17/25 | Loss: 0.00267498
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0026749828830361366, 0.0026749828830361366, 0.0026749828830361366, 0.0026749828830361366, 0.0026749828830361366]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026749828830361366

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00267498
Iteration 2/1000 | Loss: 0.00005366
Iteration 3/1000 | Loss: 0.00003284
Iteration 4/1000 | Loss: 0.00002596
Iteration 5/1000 | Loss: 0.00002362
Iteration 6/1000 | Loss: 0.00002168
Iteration 7/1000 | Loss: 0.00002068
Iteration 8/1000 | Loss: 0.00001995
Iteration 9/1000 | Loss: 0.00001938
Iteration 10/1000 | Loss: 0.00001875
Iteration 11/1000 | Loss: 0.00001835
Iteration 12/1000 | Loss: 0.00001828
Iteration 13/1000 | Loss: 0.00001803
Iteration 14/1000 | Loss: 0.00001781
Iteration 15/1000 | Loss: 0.00001778
Iteration 16/1000 | Loss: 0.00001775
Iteration 17/1000 | Loss: 0.00001767
Iteration 18/1000 | Loss: 0.00001761
Iteration 19/1000 | Loss: 0.00001759
Iteration 20/1000 | Loss: 0.00001748
Iteration 21/1000 | Loss: 0.00001744
Iteration 22/1000 | Loss: 0.00001743
Iteration 23/1000 | Loss: 0.00001733
Iteration 24/1000 | Loss: 0.00001732
Iteration 25/1000 | Loss: 0.00001731
Iteration 26/1000 | Loss: 0.00001730
Iteration 27/1000 | Loss: 0.00001730
Iteration 28/1000 | Loss: 0.00001729
Iteration 29/1000 | Loss: 0.00001729
Iteration 30/1000 | Loss: 0.00001728
Iteration 31/1000 | Loss: 0.00001728
Iteration 32/1000 | Loss: 0.00001727
Iteration 33/1000 | Loss: 0.00001727
Iteration 34/1000 | Loss: 0.00001727
Iteration 35/1000 | Loss: 0.00001726
Iteration 36/1000 | Loss: 0.00001726
Iteration 37/1000 | Loss: 0.00001726
Iteration 38/1000 | Loss: 0.00001722
Iteration 39/1000 | Loss: 0.00001720
Iteration 40/1000 | Loss: 0.00001720
Iteration 41/1000 | Loss: 0.00001715
Iteration 42/1000 | Loss: 0.00001714
Iteration 43/1000 | Loss: 0.00001714
Iteration 44/1000 | Loss: 0.00001714
Iteration 45/1000 | Loss: 0.00001714
Iteration 46/1000 | Loss: 0.00001712
Iteration 47/1000 | Loss: 0.00001712
Iteration 48/1000 | Loss: 0.00001712
Iteration 49/1000 | Loss: 0.00001711
Iteration 50/1000 | Loss: 0.00001710
Iteration 51/1000 | Loss: 0.00001710
Iteration 52/1000 | Loss: 0.00001709
Iteration 53/1000 | Loss: 0.00001709
Iteration 54/1000 | Loss: 0.00001706
Iteration 55/1000 | Loss: 0.00001705
Iteration 56/1000 | Loss: 0.00001704
Iteration 57/1000 | Loss: 0.00001701
Iteration 58/1000 | Loss: 0.00001701
Iteration 59/1000 | Loss: 0.00001699
Iteration 60/1000 | Loss: 0.00001698
Iteration 61/1000 | Loss: 0.00001698
Iteration 62/1000 | Loss: 0.00001697
Iteration 63/1000 | Loss: 0.00001697
Iteration 64/1000 | Loss: 0.00001696
Iteration 65/1000 | Loss: 0.00001696
Iteration 66/1000 | Loss: 0.00001696
Iteration 67/1000 | Loss: 0.00001695
Iteration 68/1000 | Loss: 0.00001695
Iteration 69/1000 | Loss: 0.00001695
Iteration 70/1000 | Loss: 0.00001694
Iteration 71/1000 | Loss: 0.00001694
Iteration 72/1000 | Loss: 0.00001694
Iteration 73/1000 | Loss: 0.00001694
Iteration 74/1000 | Loss: 0.00001693
Iteration 75/1000 | Loss: 0.00001693
Iteration 76/1000 | Loss: 0.00001693
Iteration 77/1000 | Loss: 0.00001692
Iteration 78/1000 | Loss: 0.00001692
Iteration 79/1000 | Loss: 0.00001692
Iteration 80/1000 | Loss: 0.00001692
Iteration 81/1000 | Loss: 0.00001692
Iteration 82/1000 | Loss: 0.00001691
Iteration 83/1000 | Loss: 0.00001691
Iteration 84/1000 | Loss: 0.00001691
Iteration 85/1000 | Loss: 0.00001691
Iteration 86/1000 | Loss: 0.00001691
Iteration 87/1000 | Loss: 0.00001691
Iteration 88/1000 | Loss: 0.00001691
Iteration 89/1000 | Loss: 0.00001691
Iteration 90/1000 | Loss: 0.00001691
Iteration 91/1000 | Loss: 0.00001691
Iteration 92/1000 | Loss: 0.00001691
Iteration 93/1000 | Loss: 0.00001691
Iteration 94/1000 | Loss: 0.00001690
Iteration 95/1000 | Loss: 0.00001690
Iteration 96/1000 | Loss: 0.00001690
Iteration 97/1000 | Loss: 0.00001690
Iteration 98/1000 | Loss: 0.00001690
Iteration 99/1000 | Loss: 0.00001688
Iteration 100/1000 | Loss: 0.00001688
Iteration 101/1000 | Loss: 0.00001687
Iteration 102/1000 | Loss: 0.00001687
Iteration 103/1000 | Loss: 0.00001687
Iteration 104/1000 | Loss: 0.00001687
Iteration 105/1000 | Loss: 0.00001686
Iteration 106/1000 | Loss: 0.00001686
Iteration 107/1000 | Loss: 0.00001686
Iteration 108/1000 | Loss: 0.00001686
Iteration 109/1000 | Loss: 0.00001686
Iteration 110/1000 | Loss: 0.00001685
Iteration 111/1000 | Loss: 0.00001685
Iteration 112/1000 | Loss: 0.00001685
Iteration 113/1000 | Loss: 0.00001685
Iteration 114/1000 | Loss: 0.00001685
Iteration 115/1000 | Loss: 0.00001684
Iteration 116/1000 | Loss: 0.00001684
Iteration 117/1000 | Loss: 0.00001684
Iteration 118/1000 | Loss: 0.00001684
Iteration 119/1000 | Loss: 0.00001683
Iteration 120/1000 | Loss: 0.00001683
Iteration 121/1000 | Loss: 0.00001682
Iteration 122/1000 | Loss: 0.00001682
Iteration 123/1000 | Loss: 0.00001682
Iteration 124/1000 | Loss: 0.00001682
Iteration 125/1000 | Loss: 0.00001681
Iteration 126/1000 | Loss: 0.00001681
Iteration 127/1000 | Loss: 0.00001681
Iteration 128/1000 | Loss: 0.00001681
Iteration 129/1000 | Loss: 0.00001680
Iteration 130/1000 | Loss: 0.00001680
Iteration 131/1000 | Loss: 0.00001680
Iteration 132/1000 | Loss: 0.00001680
Iteration 133/1000 | Loss: 0.00001680
Iteration 134/1000 | Loss: 0.00001679
Iteration 135/1000 | Loss: 0.00001679
Iteration 136/1000 | Loss: 0.00001679
Iteration 137/1000 | Loss: 0.00001678
Iteration 138/1000 | Loss: 0.00001678
Iteration 139/1000 | Loss: 0.00001678
Iteration 140/1000 | Loss: 0.00001678
Iteration 141/1000 | Loss: 0.00001677
Iteration 142/1000 | Loss: 0.00001677
Iteration 143/1000 | Loss: 0.00001677
Iteration 144/1000 | Loss: 0.00001677
Iteration 145/1000 | Loss: 0.00001677
Iteration 146/1000 | Loss: 0.00001677
Iteration 147/1000 | Loss: 0.00001677
Iteration 148/1000 | Loss: 0.00001677
Iteration 149/1000 | Loss: 0.00001677
Iteration 150/1000 | Loss: 0.00001676
Iteration 151/1000 | Loss: 0.00001676
Iteration 152/1000 | Loss: 0.00001676
Iteration 153/1000 | Loss: 0.00001676
Iteration 154/1000 | Loss: 0.00001676
Iteration 155/1000 | Loss: 0.00001676
Iteration 156/1000 | Loss: 0.00001676
Iteration 157/1000 | Loss: 0.00001676
Iteration 158/1000 | Loss: 0.00001676
Iteration 159/1000 | Loss: 0.00001675
Iteration 160/1000 | Loss: 0.00001675
Iteration 161/1000 | Loss: 0.00001675
Iteration 162/1000 | Loss: 0.00001675
Iteration 163/1000 | Loss: 0.00001675
Iteration 164/1000 | Loss: 0.00001675
Iteration 165/1000 | Loss: 0.00001675
Iteration 166/1000 | Loss: 0.00001675
Iteration 167/1000 | Loss: 0.00001675
Iteration 168/1000 | Loss: 0.00001675
Iteration 169/1000 | Loss: 0.00001675
Iteration 170/1000 | Loss: 0.00001675
Iteration 171/1000 | Loss: 0.00001675
Iteration 172/1000 | Loss: 0.00001675
Iteration 173/1000 | Loss: 0.00001675
Iteration 174/1000 | Loss: 0.00001675
Iteration 175/1000 | Loss: 0.00001674
Iteration 176/1000 | Loss: 0.00001674
Iteration 177/1000 | Loss: 0.00001674
Iteration 178/1000 | Loss: 0.00001674
Iteration 179/1000 | Loss: 0.00001674
Iteration 180/1000 | Loss: 0.00001674
Iteration 181/1000 | Loss: 0.00001674
Iteration 182/1000 | Loss: 0.00001674
Iteration 183/1000 | Loss: 0.00001674
Iteration 184/1000 | Loss: 0.00001674
Iteration 185/1000 | Loss: 0.00001674
Iteration 186/1000 | Loss: 0.00001674
Iteration 187/1000 | Loss: 0.00001674
Iteration 188/1000 | Loss: 0.00001674
Iteration 189/1000 | Loss: 0.00001674
Iteration 190/1000 | Loss: 0.00001674
Iteration 191/1000 | Loss: 0.00001674
Iteration 192/1000 | Loss: 0.00001674
Iteration 193/1000 | Loss: 0.00001674
Iteration 194/1000 | Loss: 0.00001674
Iteration 195/1000 | Loss: 0.00001674
Iteration 196/1000 | Loss: 0.00001674
Iteration 197/1000 | Loss: 0.00001674
Iteration 198/1000 | Loss: 0.00001674
Iteration 199/1000 | Loss: 0.00001674
Iteration 200/1000 | Loss: 0.00001674
Iteration 201/1000 | Loss: 0.00001674
Iteration 202/1000 | Loss: 0.00001674
Iteration 203/1000 | Loss: 0.00001674
Iteration 204/1000 | Loss: 0.00001674
Iteration 205/1000 | Loss: 0.00001674
Iteration 206/1000 | Loss: 0.00001674
Iteration 207/1000 | Loss: 0.00001674
Iteration 208/1000 | Loss: 0.00001674
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [1.6735957615310326e-05, 1.6735957615310326e-05, 1.6735957615310326e-05, 1.6735957615310326e-05, 1.6735957615310326e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6735957615310326e-05

Optimization complete. Final v2v error: 3.4643166065216064 mm

Highest mean error: 4.048893451690674 mm for frame 247

Lowest mean error: 3.0633933544158936 mm for frame 56

Saving results

Total time: 55.173784494400024
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carina_posed_010/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00804124
Iteration 2/25 | Loss: 0.00140994
Iteration 3/25 | Loss: 0.00133971
Iteration 4/25 | Loss: 0.00133437
Iteration 5/25 | Loss: 0.00133339
Iteration 6/25 | Loss: 0.00133339
Iteration 7/25 | Loss: 0.00133339
Iteration 8/25 | Loss: 0.00133339
Iteration 9/25 | Loss: 0.00133339
Iteration 10/25 | Loss: 0.00133339
Iteration 11/25 | Loss: 0.00133339
Iteration 12/25 | Loss: 0.00133339
Iteration 13/25 | Loss: 0.00133339
Iteration 14/25 | Loss: 0.00133339
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0013333866372704506, 0.0013333866372704506, 0.0013333866372704506, 0.0013333866372704506, 0.0013333866372704506]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013333866372704506

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26484561
Iteration 2/25 | Loss: 0.00184123
Iteration 3/25 | Loss: 0.00184123
Iteration 4/25 | Loss: 0.00184122
Iteration 5/25 | Loss: 0.00184122
Iteration 6/25 | Loss: 0.00184122
Iteration 7/25 | Loss: 0.00184122
Iteration 8/25 | Loss: 0.00184122
Iteration 9/25 | Loss: 0.00184122
Iteration 10/25 | Loss: 0.00184122
Iteration 11/25 | Loss: 0.00184122
Iteration 12/25 | Loss: 0.00184122
Iteration 13/25 | Loss: 0.00184122
Iteration 14/25 | Loss: 0.00184122
Iteration 15/25 | Loss: 0.00184122
Iteration 16/25 | Loss: 0.00184122
Iteration 17/25 | Loss: 0.00184122
Iteration 18/25 | Loss: 0.00184122
Iteration 19/25 | Loss: 0.00184122
Iteration 20/25 | Loss: 0.00184122
Iteration 21/25 | Loss: 0.00184122
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0018412219360470772, 0.0018412219360470772, 0.0018412219360470772, 0.0018412219360470772, 0.0018412219360470772]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018412219360470772

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00184122
Iteration 2/1000 | Loss: 0.00002225
Iteration 3/1000 | Loss: 0.00001639
Iteration 4/1000 | Loss: 0.00001511
Iteration 5/1000 | Loss: 0.00001405
Iteration 6/1000 | Loss: 0.00001333
Iteration 7/1000 | Loss: 0.00001270
Iteration 8/1000 | Loss: 0.00001240
Iteration 9/1000 | Loss: 0.00001195
Iteration 10/1000 | Loss: 0.00001172
Iteration 11/1000 | Loss: 0.00001159
Iteration 12/1000 | Loss: 0.00001149
Iteration 13/1000 | Loss: 0.00001146
Iteration 14/1000 | Loss: 0.00001145
Iteration 15/1000 | Loss: 0.00001135
Iteration 16/1000 | Loss: 0.00001127
Iteration 17/1000 | Loss: 0.00001117
Iteration 18/1000 | Loss: 0.00001116
Iteration 19/1000 | Loss: 0.00001109
Iteration 20/1000 | Loss: 0.00001108
Iteration 21/1000 | Loss: 0.00001107
Iteration 22/1000 | Loss: 0.00001105
Iteration 23/1000 | Loss: 0.00001097
Iteration 24/1000 | Loss: 0.00001085
Iteration 25/1000 | Loss: 0.00001084
Iteration 26/1000 | Loss: 0.00001082
Iteration 27/1000 | Loss: 0.00001081
Iteration 28/1000 | Loss: 0.00001080
Iteration 29/1000 | Loss: 0.00001079
Iteration 30/1000 | Loss: 0.00001079
Iteration 31/1000 | Loss: 0.00001078
Iteration 32/1000 | Loss: 0.00001078
Iteration 33/1000 | Loss: 0.00001077
Iteration 34/1000 | Loss: 0.00001077
Iteration 35/1000 | Loss: 0.00001076
Iteration 36/1000 | Loss: 0.00001075
Iteration 37/1000 | Loss: 0.00001075
Iteration 38/1000 | Loss: 0.00001075
Iteration 39/1000 | Loss: 0.00001074
Iteration 40/1000 | Loss: 0.00001074
Iteration 41/1000 | Loss: 0.00001074
Iteration 42/1000 | Loss: 0.00001073
Iteration 43/1000 | Loss: 0.00001072
Iteration 44/1000 | Loss: 0.00001071
Iteration 45/1000 | Loss: 0.00001071
Iteration 46/1000 | Loss: 0.00001070
Iteration 47/1000 | Loss: 0.00001070
Iteration 48/1000 | Loss: 0.00001069
Iteration 49/1000 | Loss: 0.00001068
Iteration 50/1000 | Loss: 0.00001068
Iteration 51/1000 | Loss: 0.00001066
Iteration 52/1000 | Loss: 0.00001065
Iteration 53/1000 | Loss: 0.00001065
Iteration 54/1000 | Loss: 0.00001065
Iteration 55/1000 | Loss: 0.00001065
Iteration 56/1000 | Loss: 0.00001065
Iteration 57/1000 | Loss: 0.00001064
Iteration 58/1000 | Loss: 0.00001064
Iteration 59/1000 | Loss: 0.00001064
Iteration 60/1000 | Loss: 0.00001062
Iteration 61/1000 | Loss: 0.00001062
Iteration 62/1000 | Loss: 0.00001061
Iteration 63/1000 | Loss: 0.00001061
Iteration 64/1000 | Loss: 0.00001061
Iteration 65/1000 | Loss: 0.00001061
Iteration 66/1000 | Loss: 0.00001060
Iteration 67/1000 | Loss: 0.00001060
Iteration 68/1000 | Loss: 0.00001059
Iteration 69/1000 | Loss: 0.00001059
Iteration 70/1000 | Loss: 0.00001058
Iteration 71/1000 | Loss: 0.00001058
Iteration 72/1000 | Loss: 0.00001057
Iteration 73/1000 | Loss: 0.00001057
Iteration 74/1000 | Loss: 0.00001053
Iteration 75/1000 | Loss: 0.00001053
Iteration 76/1000 | Loss: 0.00001053
Iteration 77/1000 | Loss: 0.00001053
Iteration 78/1000 | Loss: 0.00001053
Iteration 79/1000 | Loss: 0.00001053
Iteration 80/1000 | Loss: 0.00001053
Iteration 81/1000 | Loss: 0.00001053
Iteration 82/1000 | Loss: 0.00001053
Iteration 83/1000 | Loss: 0.00001053
Iteration 84/1000 | Loss: 0.00001053
Iteration 85/1000 | Loss: 0.00001053
Iteration 86/1000 | Loss: 0.00001053
Iteration 87/1000 | Loss: 0.00001052
Iteration 88/1000 | Loss: 0.00001052
Iteration 89/1000 | Loss: 0.00001052
Iteration 90/1000 | Loss: 0.00001052
Iteration 91/1000 | Loss: 0.00001052
Iteration 92/1000 | Loss: 0.00001052
Iteration 93/1000 | Loss: 0.00001052
Iteration 94/1000 | Loss: 0.00001052
Iteration 95/1000 | Loss: 0.00001052
Iteration 96/1000 | Loss: 0.00001052
Iteration 97/1000 | Loss: 0.00001052
Iteration 98/1000 | Loss: 0.00001052
Iteration 99/1000 | Loss: 0.00001052
Iteration 100/1000 | Loss: 0.00001052
Iteration 101/1000 | Loss: 0.00001052
Iteration 102/1000 | Loss: 0.00001052
Iteration 103/1000 | Loss: 0.00001052
Iteration 104/1000 | Loss: 0.00001052
Iteration 105/1000 | Loss: 0.00001052
Iteration 106/1000 | Loss: 0.00001052
Iteration 107/1000 | Loss: 0.00001052
Iteration 108/1000 | Loss: 0.00001052
Iteration 109/1000 | Loss: 0.00001052
Iteration 110/1000 | Loss: 0.00001052
Iteration 111/1000 | Loss: 0.00001052
Iteration 112/1000 | Loss: 0.00001052
Iteration 113/1000 | Loss: 0.00001052
Iteration 114/1000 | Loss: 0.00001052
Iteration 115/1000 | Loss: 0.00001052
Iteration 116/1000 | Loss: 0.00001052
Iteration 117/1000 | Loss: 0.00001052
Iteration 118/1000 | Loss: 0.00001052
Iteration 119/1000 | Loss: 0.00001052
Iteration 120/1000 | Loss: 0.00001052
Iteration 121/1000 | Loss: 0.00001052
Iteration 122/1000 | Loss: 0.00001052
Iteration 123/1000 | Loss: 0.00001052
Iteration 124/1000 | Loss: 0.00001052
Iteration 125/1000 | Loss: 0.00001052
Iteration 126/1000 | Loss: 0.00001052
Iteration 127/1000 | Loss: 0.00001052
Iteration 128/1000 | Loss: 0.00001052
Iteration 129/1000 | Loss: 0.00001052
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [1.05227099993499e-05, 1.05227099993499e-05, 1.05227099993499e-05, 1.05227099993499e-05, 1.05227099993499e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.05227099993499e-05

Optimization complete. Final v2v error: 2.7978484630584717 mm

Highest mean error: 2.9762229919433594 mm for frame 126

Lowest mean error: 2.6772658824920654 mm for frame 208

Saving results

Total time: 38.95244336128235
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carina_posed_010/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00396018
Iteration 2/25 | Loss: 0.00147655
Iteration 3/25 | Loss: 0.00136710
Iteration 4/25 | Loss: 0.00135351
Iteration 5/25 | Loss: 0.00135128
Iteration 6/25 | Loss: 0.00135088
Iteration 7/25 | Loss: 0.00135088
Iteration 8/25 | Loss: 0.00135088
Iteration 9/25 | Loss: 0.00135088
Iteration 10/25 | Loss: 0.00135088
Iteration 11/25 | Loss: 0.00135088
Iteration 12/25 | Loss: 0.00135088
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013508799020200968, 0.0013508799020200968, 0.0013508799020200968, 0.0013508799020200968, 0.0013508799020200968]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013508799020200968

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24747956
Iteration 2/25 | Loss: 0.00178217
Iteration 3/25 | Loss: 0.00178216
Iteration 4/25 | Loss: 0.00178216
Iteration 5/25 | Loss: 0.00178216
Iteration 6/25 | Loss: 0.00178216
Iteration 7/25 | Loss: 0.00178216
Iteration 8/25 | Loss: 0.00178216
Iteration 9/25 | Loss: 0.00178216
Iteration 10/25 | Loss: 0.00178216
Iteration 11/25 | Loss: 0.00178216
Iteration 12/25 | Loss: 0.00178216
Iteration 13/25 | Loss: 0.00178216
Iteration 14/25 | Loss: 0.00178216
Iteration 15/25 | Loss: 0.00178216
Iteration 16/25 | Loss: 0.00178216
Iteration 17/25 | Loss: 0.00178216
Iteration 18/25 | Loss: 0.00178216
Iteration 19/25 | Loss: 0.00178216
Iteration 20/25 | Loss: 0.00178216
Iteration 21/25 | Loss: 0.00178216
Iteration 22/25 | Loss: 0.00178216
Iteration 23/25 | Loss: 0.00178216
Iteration 24/25 | Loss: 0.00178216
Iteration 25/25 | Loss: 0.00178216
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0017821602523326874, 0.0017821602523326874, 0.0017821602523326874, 0.0017821602523326874, 0.0017821602523326874]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017821602523326874

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00178216
Iteration 2/1000 | Loss: 0.00003501
Iteration 3/1000 | Loss: 0.00002541
Iteration 4/1000 | Loss: 0.00002144
Iteration 5/1000 | Loss: 0.00002015
Iteration 6/1000 | Loss: 0.00001906
Iteration 7/1000 | Loss: 0.00001835
Iteration 8/1000 | Loss: 0.00001779
Iteration 9/1000 | Loss: 0.00001743
Iteration 10/1000 | Loss: 0.00001704
Iteration 11/1000 | Loss: 0.00001670
Iteration 12/1000 | Loss: 0.00001663
Iteration 13/1000 | Loss: 0.00001641
Iteration 14/1000 | Loss: 0.00001623
Iteration 15/1000 | Loss: 0.00001617
Iteration 16/1000 | Loss: 0.00001604
Iteration 17/1000 | Loss: 0.00001604
Iteration 18/1000 | Loss: 0.00001603
Iteration 19/1000 | Loss: 0.00001602
Iteration 20/1000 | Loss: 0.00001601
Iteration 21/1000 | Loss: 0.00001600
Iteration 22/1000 | Loss: 0.00001599
Iteration 23/1000 | Loss: 0.00001598
Iteration 24/1000 | Loss: 0.00001598
Iteration 25/1000 | Loss: 0.00001596
Iteration 26/1000 | Loss: 0.00001592
Iteration 27/1000 | Loss: 0.00001592
Iteration 28/1000 | Loss: 0.00001584
Iteration 29/1000 | Loss: 0.00001580
Iteration 30/1000 | Loss: 0.00001579
Iteration 31/1000 | Loss: 0.00001577
Iteration 32/1000 | Loss: 0.00001576
Iteration 33/1000 | Loss: 0.00001575
Iteration 34/1000 | Loss: 0.00001575
Iteration 35/1000 | Loss: 0.00001574
Iteration 36/1000 | Loss: 0.00001574
Iteration 37/1000 | Loss: 0.00001571
Iteration 38/1000 | Loss: 0.00001568
Iteration 39/1000 | Loss: 0.00001564
Iteration 40/1000 | Loss: 0.00001562
Iteration 41/1000 | Loss: 0.00001555
Iteration 42/1000 | Loss: 0.00001555
Iteration 43/1000 | Loss: 0.00001553
Iteration 44/1000 | Loss: 0.00001553
Iteration 45/1000 | Loss: 0.00001553
Iteration 46/1000 | Loss: 0.00001553
Iteration 47/1000 | Loss: 0.00001552
Iteration 48/1000 | Loss: 0.00001552
Iteration 49/1000 | Loss: 0.00001552
Iteration 50/1000 | Loss: 0.00001552
Iteration 51/1000 | Loss: 0.00001552
Iteration 52/1000 | Loss: 0.00001552
Iteration 53/1000 | Loss: 0.00001552
Iteration 54/1000 | Loss: 0.00001552
Iteration 55/1000 | Loss: 0.00001550
Iteration 56/1000 | Loss: 0.00001548
Iteration 57/1000 | Loss: 0.00001548
Iteration 58/1000 | Loss: 0.00001548
Iteration 59/1000 | Loss: 0.00001548
Iteration 60/1000 | Loss: 0.00001547
Iteration 61/1000 | Loss: 0.00001547
Iteration 62/1000 | Loss: 0.00001547
Iteration 63/1000 | Loss: 0.00001547
Iteration 64/1000 | Loss: 0.00001546
Iteration 65/1000 | Loss: 0.00001546
Iteration 66/1000 | Loss: 0.00001546
Iteration 67/1000 | Loss: 0.00001545
Iteration 68/1000 | Loss: 0.00001545
Iteration 69/1000 | Loss: 0.00001545
Iteration 70/1000 | Loss: 0.00001544
Iteration 71/1000 | Loss: 0.00001544
Iteration 72/1000 | Loss: 0.00001543
Iteration 73/1000 | Loss: 0.00001543
Iteration 74/1000 | Loss: 0.00001543
Iteration 75/1000 | Loss: 0.00001543
Iteration 76/1000 | Loss: 0.00001542
Iteration 77/1000 | Loss: 0.00001541
Iteration 78/1000 | Loss: 0.00001541
Iteration 79/1000 | Loss: 0.00001541
Iteration 80/1000 | Loss: 0.00001541
Iteration 81/1000 | Loss: 0.00001541
Iteration 82/1000 | Loss: 0.00001540
Iteration 83/1000 | Loss: 0.00001540
Iteration 84/1000 | Loss: 0.00001539
Iteration 85/1000 | Loss: 0.00001539
Iteration 86/1000 | Loss: 0.00001538
Iteration 87/1000 | Loss: 0.00001538
Iteration 88/1000 | Loss: 0.00001538
Iteration 89/1000 | Loss: 0.00001537
Iteration 90/1000 | Loss: 0.00001537
Iteration 91/1000 | Loss: 0.00001537
Iteration 92/1000 | Loss: 0.00001537
Iteration 93/1000 | Loss: 0.00001537
Iteration 94/1000 | Loss: 0.00001536
Iteration 95/1000 | Loss: 0.00001536
Iteration 96/1000 | Loss: 0.00001536
Iteration 97/1000 | Loss: 0.00001536
Iteration 98/1000 | Loss: 0.00001536
Iteration 99/1000 | Loss: 0.00001535
Iteration 100/1000 | Loss: 0.00001535
Iteration 101/1000 | Loss: 0.00001535
Iteration 102/1000 | Loss: 0.00001534
Iteration 103/1000 | Loss: 0.00001534
Iteration 104/1000 | Loss: 0.00001534
Iteration 105/1000 | Loss: 0.00001534
Iteration 106/1000 | Loss: 0.00001534
Iteration 107/1000 | Loss: 0.00001534
Iteration 108/1000 | Loss: 0.00001533
Iteration 109/1000 | Loss: 0.00001533
Iteration 110/1000 | Loss: 0.00001532
Iteration 111/1000 | Loss: 0.00001532
Iteration 112/1000 | Loss: 0.00001531
Iteration 113/1000 | Loss: 0.00001531
Iteration 114/1000 | Loss: 0.00001531
Iteration 115/1000 | Loss: 0.00001530
Iteration 116/1000 | Loss: 0.00001530
Iteration 117/1000 | Loss: 0.00001529
Iteration 118/1000 | Loss: 0.00001529
Iteration 119/1000 | Loss: 0.00001529
Iteration 120/1000 | Loss: 0.00001528
Iteration 121/1000 | Loss: 0.00001528
Iteration 122/1000 | Loss: 0.00001528
Iteration 123/1000 | Loss: 0.00001527
Iteration 124/1000 | Loss: 0.00001527
Iteration 125/1000 | Loss: 0.00001527
Iteration 126/1000 | Loss: 0.00001526
Iteration 127/1000 | Loss: 0.00001526
Iteration 128/1000 | Loss: 0.00001525
Iteration 129/1000 | Loss: 0.00001525
Iteration 130/1000 | Loss: 0.00001525
Iteration 131/1000 | Loss: 0.00001525
Iteration 132/1000 | Loss: 0.00001525
Iteration 133/1000 | Loss: 0.00001525
Iteration 134/1000 | Loss: 0.00001525
Iteration 135/1000 | Loss: 0.00001525
Iteration 136/1000 | Loss: 0.00001525
Iteration 137/1000 | Loss: 0.00001524
Iteration 138/1000 | Loss: 0.00001524
Iteration 139/1000 | Loss: 0.00001524
Iteration 140/1000 | Loss: 0.00001524
Iteration 141/1000 | Loss: 0.00001524
Iteration 142/1000 | Loss: 0.00001524
Iteration 143/1000 | Loss: 0.00001524
Iteration 144/1000 | Loss: 0.00001524
Iteration 145/1000 | Loss: 0.00001524
Iteration 146/1000 | Loss: 0.00001524
Iteration 147/1000 | Loss: 0.00001524
Iteration 148/1000 | Loss: 0.00001524
Iteration 149/1000 | Loss: 0.00001524
Iteration 150/1000 | Loss: 0.00001524
Iteration 151/1000 | Loss: 0.00001524
Iteration 152/1000 | Loss: 0.00001523
Iteration 153/1000 | Loss: 0.00001523
Iteration 154/1000 | Loss: 0.00001523
Iteration 155/1000 | Loss: 0.00001523
Iteration 156/1000 | Loss: 0.00001523
Iteration 157/1000 | Loss: 0.00001523
Iteration 158/1000 | Loss: 0.00001523
Iteration 159/1000 | Loss: 0.00001523
Iteration 160/1000 | Loss: 0.00001523
Iteration 161/1000 | Loss: 0.00001523
Iteration 162/1000 | Loss: 0.00001523
Iteration 163/1000 | Loss: 0.00001523
Iteration 164/1000 | Loss: 0.00001523
Iteration 165/1000 | Loss: 0.00001523
Iteration 166/1000 | Loss: 0.00001522
Iteration 167/1000 | Loss: 0.00001522
Iteration 168/1000 | Loss: 0.00001522
Iteration 169/1000 | Loss: 0.00001522
Iteration 170/1000 | Loss: 0.00001522
Iteration 171/1000 | Loss: 0.00001522
Iteration 172/1000 | Loss: 0.00001522
Iteration 173/1000 | Loss: 0.00001522
Iteration 174/1000 | Loss: 0.00001522
Iteration 175/1000 | Loss: 0.00001522
Iteration 176/1000 | Loss: 0.00001522
Iteration 177/1000 | Loss: 0.00001521
Iteration 178/1000 | Loss: 0.00001521
Iteration 179/1000 | Loss: 0.00001521
Iteration 180/1000 | Loss: 0.00001521
Iteration 181/1000 | Loss: 0.00001521
Iteration 182/1000 | Loss: 0.00001521
Iteration 183/1000 | Loss: 0.00001521
Iteration 184/1000 | Loss: 0.00001520
Iteration 185/1000 | Loss: 0.00001520
Iteration 186/1000 | Loss: 0.00001520
Iteration 187/1000 | Loss: 0.00001520
Iteration 188/1000 | Loss: 0.00001520
Iteration 189/1000 | Loss: 0.00001520
Iteration 190/1000 | Loss: 0.00001519
Iteration 191/1000 | Loss: 0.00001519
Iteration 192/1000 | Loss: 0.00001519
Iteration 193/1000 | Loss: 0.00001519
Iteration 194/1000 | Loss: 0.00001519
Iteration 195/1000 | Loss: 0.00001519
Iteration 196/1000 | Loss: 0.00001519
Iteration 197/1000 | Loss: 0.00001518
Iteration 198/1000 | Loss: 0.00001518
Iteration 199/1000 | Loss: 0.00001518
Iteration 200/1000 | Loss: 0.00001518
Iteration 201/1000 | Loss: 0.00001518
Iteration 202/1000 | Loss: 0.00001518
Iteration 203/1000 | Loss: 0.00001518
Iteration 204/1000 | Loss: 0.00001518
Iteration 205/1000 | Loss: 0.00001518
Iteration 206/1000 | Loss: 0.00001518
Iteration 207/1000 | Loss: 0.00001518
Iteration 208/1000 | Loss: 0.00001518
Iteration 209/1000 | Loss: 0.00001518
Iteration 210/1000 | Loss: 0.00001518
Iteration 211/1000 | Loss: 0.00001518
Iteration 212/1000 | Loss: 0.00001518
Iteration 213/1000 | Loss: 0.00001517
Iteration 214/1000 | Loss: 0.00001517
Iteration 215/1000 | Loss: 0.00001517
Iteration 216/1000 | Loss: 0.00001517
Iteration 217/1000 | Loss: 0.00001517
Iteration 218/1000 | Loss: 0.00001517
Iteration 219/1000 | Loss: 0.00001517
Iteration 220/1000 | Loss: 0.00001517
Iteration 221/1000 | Loss: 0.00001517
Iteration 222/1000 | Loss: 0.00001517
Iteration 223/1000 | Loss: 0.00001517
Iteration 224/1000 | Loss: 0.00001517
Iteration 225/1000 | Loss: 0.00001516
Iteration 226/1000 | Loss: 0.00001516
Iteration 227/1000 | Loss: 0.00001516
Iteration 228/1000 | Loss: 0.00001516
Iteration 229/1000 | Loss: 0.00001516
Iteration 230/1000 | Loss: 0.00001516
Iteration 231/1000 | Loss: 0.00001516
Iteration 232/1000 | Loss: 0.00001516
Iteration 233/1000 | Loss: 0.00001516
Iteration 234/1000 | Loss: 0.00001516
Iteration 235/1000 | Loss: 0.00001515
Iteration 236/1000 | Loss: 0.00001515
Iteration 237/1000 | Loss: 0.00001515
Iteration 238/1000 | Loss: 0.00001515
Iteration 239/1000 | Loss: 0.00001515
Iteration 240/1000 | Loss: 0.00001515
Iteration 241/1000 | Loss: 0.00001515
Iteration 242/1000 | Loss: 0.00001515
Iteration 243/1000 | Loss: 0.00001515
Iteration 244/1000 | Loss: 0.00001515
Iteration 245/1000 | Loss: 0.00001515
Iteration 246/1000 | Loss: 0.00001515
Iteration 247/1000 | Loss: 0.00001514
Iteration 248/1000 | Loss: 0.00001514
Iteration 249/1000 | Loss: 0.00001514
Iteration 250/1000 | Loss: 0.00001514
Iteration 251/1000 | Loss: 0.00001514
Iteration 252/1000 | Loss: 0.00001514
Iteration 253/1000 | Loss: 0.00001514
Iteration 254/1000 | Loss: 0.00001514
Iteration 255/1000 | Loss: 0.00001514
Iteration 256/1000 | Loss: 0.00001514
Iteration 257/1000 | Loss: 0.00001514
Iteration 258/1000 | Loss: 0.00001514
Iteration 259/1000 | Loss: 0.00001514
Iteration 260/1000 | Loss: 0.00001514
Iteration 261/1000 | Loss: 0.00001513
Iteration 262/1000 | Loss: 0.00001513
Iteration 263/1000 | Loss: 0.00001513
Iteration 264/1000 | Loss: 0.00001513
Iteration 265/1000 | Loss: 0.00001513
Iteration 266/1000 | Loss: 0.00001513
Iteration 267/1000 | Loss: 0.00001513
Iteration 268/1000 | Loss: 0.00001512
Iteration 269/1000 | Loss: 0.00001512
Iteration 270/1000 | Loss: 0.00001512
Iteration 271/1000 | Loss: 0.00001512
Iteration 272/1000 | Loss: 0.00001512
Iteration 273/1000 | Loss: 0.00001512
Iteration 274/1000 | Loss: 0.00001512
Iteration 275/1000 | Loss: 0.00001512
Iteration 276/1000 | Loss: 0.00001511
Iteration 277/1000 | Loss: 0.00001511
Iteration 278/1000 | Loss: 0.00001511
Iteration 279/1000 | Loss: 0.00001511
Iteration 280/1000 | Loss: 0.00001511
Iteration 281/1000 | Loss: 0.00001511
Iteration 282/1000 | Loss: 0.00001511
Iteration 283/1000 | Loss: 0.00001511
Iteration 284/1000 | Loss: 0.00001510
Iteration 285/1000 | Loss: 0.00001510
Iteration 286/1000 | Loss: 0.00001510
Iteration 287/1000 | Loss: 0.00001510
Iteration 288/1000 | Loss: 0.00001510
Iteration 289/1000 | Loss: 0.00001510
Iteration 290/1000 | Loss: 0.00001510
Iteration 291/1000 | Loss: 0.00001510
Iteration 292/1000 | Loss: 0.00001510
Iteration 293/1000 | Loss: 0.00001509
Iteration 294/1000 | Loss: 0.00001509
Iteration 295/1000 | Loss: 0.00001509
Iteration 296/1000 | Loss: 0.00001509
Iteration 297/1000 | Loss: 0.00001509
Iteration 298/1000 | Loss: 0.00001509
Iteration 299/1000 | Loss: 0.00001509
Iteration 300/1000 | Loss: 0.00001509
Iteration 301/1000 | Loss: 0.00001509
Iteration 302/1000 | Loss: 0.00001509
Iteration 303/1000 | Loss: 0.00001509
Iteration 304/1000 | Loss: 0.00001509
Iteration 305/1000 | Loss: 0.00001508
Iteration 306/1000 | Loss: 0.00001508
Iteration 307/1000 | Loss: 0.00001508
Iteration 308/1000 | Loss: 0.00001508
Iteration 309/1000 | Loss: 0.00001508
Iteration 310/1000 | Loss: 0.00001508
Iteration 311/1000 | Loss: 0.00001508
Iteration 312/1000 | Loss: 0.00001508
Iteration 313/1000 | Loss: 0.00001508
Iteration 314/1000 | Loss: 0.00001507
Iteration 315/1000 | Loss: 0.00001507
Iteration 316/1000 | Loss: 0.00001507
Iteration 317/1000 | Loss: 0.00001507
Iteration 318/1000 | Loss: 0.00001507
Iteration 319/1000 | Loss: 0.00001507
Iteration 320/1000 | Loss: 0.00001507
Iteration 321/1000 | Loss: 0.00001507
Iteration 322/1000 | Loss: 0.00001506
Iteration 323/1000 | Loss: 0.00001506
Iteration 324/1000 | Loss: 0.00001506
Iteration 325/1000 | Loss: 0.00001506
Iteration 326/1000 | Loss: 0.00001506
Iteration 327/1000 | Loss: 0.00001506
Iteration 328/1000 | Loss: 0.00001506
Iteration 329/1000 | Loss: 0.00001506
Iteration 330/1000 | Loss: 0.00001505
Iteration 331/1000 | Loss: 0.00001505
Iteration 332/1000 | Loss: 0.00001505
Iteration 333/1000 | Loss: 0.00001505
Iteration 334/1000 | Loss: 0.00001505
Iteration 335/1000 | Loss: 0.00001505
Iteration 336/1000 | Loss: 0.00001505
Iteration 337/1000 | Loss: 0.00001505
Iteration 338/1000 | Loss: 0.00001505
Iteration 339/1000 | Loss: 0.00001505
Iteration 340/1000 | Loss: 0.00001505
Iteration 341/1000 | Loss: 0.00001505
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 341. Stopping optimization.
Last 5 losses: [1.5049520698084962e-05, 1.5049520698084962e-05, 1.5049520698084962e-05, 1.5049520698084962e-05, 1.5049520698084962e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5049520698084962e-05

Optimization complete. Final v2v error: 3.2567555904388428 mm

Highest mean error: 3.5113115310668945 mm for frame 20

Lowest mean error: 3.0494422912597656 mm for frame 43

Saving results

Total time: 56.04477119445801
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carina_posed_010/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01017102
Iteration 2/25 | Loss: 0.00288239
Iteration 3/25 | Loss: 0.00186289
Iteration 4/25 | Loss: 0.00173834
Iteration 5/25 | Loss: 0.00176655
Iteration 6/25 | Loss: 0.00182788
Iteration 7/25 | Loss: 0.00161486
Iteration 8/25 | Loss: 0.00152631
Iteration 9/25 | Loss: 0.00147290
Iteration 10/25 | Loss: 0.00145016
Iteration 11/25 | Loss: 0.00142469
Iteration 12/25 | Loss: 0.00142083
Iteration 13/25 | Loss: 0.00141198
Iteration 14/25 | Loss: 0.00140935
Iteration 15/25 | Loss: 0.00140965
Iteration 16/25 | Loss: 0.00140757
Iteration 17/25 | Loss: 0.00140687
Iteration 18/25 | Loss: 0.00140669
Iteration 19/25 | Loss: 0.00140666
Iteration 20/25 | Loss: 0.00140666
Iteration 21/25 | Loss: 0.00140666
Iteration 22/25 | Loss: 0.00140666
Iteration 23/25 | Loss: 0.00140666
Iteration 24/25 | Loss: 0.00140666
Iteration 25/25 | Loss: 0.00140666

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62717843
Iteration 2/25 | Loss: 0.00191685
Iteration 3/25 | Loss: 0.00190472
Iteration 4/25 | Loss: 0.00190472
Iteration 5/25 | Loss: 0.00190472
Iteration 6/25 | Loss: 0.00190472
Iteration 7/25 | Loss: 0.00190472
Iteration 8/25 | Loss: 0.00190472
Iteration 9/25 | Loss: 0.00190472
Iteration 10/25 | Loss: 0.00190472
Iteration 11/25 | Loss: 0.00190472
Iteration 12/25 | Loss: 0.00190472
Iteration 13/25 | Loss: 0.00190472
Iteration 14/25 | Loss: 0.00190472
Iteration 15/25 | Loss: 0.00190472
Iteration 16/25 | Loss: 0.00190472
Iteration 17/25 | Loss: 0.00190472
Iteration 18/25 | Loss: 0.00190472
Iteration 19/25 | Loss: 0.00190472
Iteration 20/25 | Loss: 0.00190472
Iteration 21/25 | Loss: 0.00190472
Iteration 22/25 | Loss: 0.00190472
Iteration 23/25 | Loss: 0.00190472
Iteration 24/25 | Loss: 0.00190472
Iteration 25/25 | Loss: 0.00190472

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00190472
Iteration 2/1000 | Loss: 0.00007427
Iteration 3/1000 | Loss: 0.00005163
Iteration 4/1000 | Loss: 0.00005214
Iteration 5/1000 | Loss: 0.00004311
Iteration 6/1000 | Loss: 0.00004066
Iteration 7/1000 | Loss: 0.00004473
Iteration 8/1000 | Loss: 0.00003839
Iteration 9/1000 | Loss: 0.00003736
Iteration 10/1000 | Loss: 0.00003640
Iteration 11/1000 | Loss: 0.00005373
Iteration 12/1000 | Loss: 0.00232378
Iteration 13/1000 | Loss: 0.00057564
Iteration 14/1000 | Loss: 0.00004752
Iteration 15/1000 | Loss: 0.00005643
Iteration 16/1000 | Loss: 0.00003196
Iteration 17/1000 | Loss: 0.00010782
Iteration 18/1000 | Loss: 0.00002246
Iteration 19/1000 | Loss: 0.00008244
Iteration 20/1000 | Loss: 0.00001982
Iteration 21/1000 | Loss: 0.00001833
Iteration 22/1000 | Loss: 0.00001732
Iteration 23/1000 | Loss: 0.00003215
Iteration 24/1000 | Loss: 0.00002674
Iteration 25/1000 | Loss: 0.00007510
Iteration 26/1000 | Loss: 0.00001832
Iteration 27/1000 | Loss: 0.00001582
Iteration 28/1000 | Loss: 0.00001528
Iteration 29/1000 | Loss: 0.00001505
Iteration 30/1000 | Loss: 0.00001478
Iteration 31/1000 | Loss: 0.00001476
Iteration 32/1000 | Loss: 0.00002503
Iteration 33/1000 | Loss: 0.00001466
Iteration 34/1000 | Loss: 0.00001464
Iteration 35/1000 | Loss: 0.00001464
Iteration 36/1000 | Loss: 0.00001463
Iteration 37/1000 | Loss: 0.00001458
Iteration 38/1000 | Loss: 0.00001458
Iteration 39/1000 | Loss: 0.00001453
Iteration 40/1000 | Loss: 0.00001453
Iteration 41/1000 | Loss: 0.00001452
Iteration 42/1000 | Loss: 0.00001451
Iteration 43/1000 | Loss: 0.00001450
Iteration 44/1000 | Loss: 0.00001450
Iteration 45/1000 | Loss: 0.00001449
Iteration 46/1000 | Loss: 0.00001449
Iteration 47/1000 | Loss: 0.00001449
Iteration 48/1000 | Loss: 0.00001449
Iteration 49/1000 | Loss: 0.00001449
Iteration 50/1000 | Loss: 0.00001448
Iteration 51/1000 | Loss: 0.00001448
Iteration 52/1000 | Loss: 0.00001448
Iteration 53/1000 | Loss: 0.00001448
Iteration 54/1000 | Loss: 0.00001448
Iteration 55/1000 | Loss: 0.00001447
Iteration 56/1000 | Loss: 0.00001447
Iteration 57/1000 | Loss: 0.00001447
Iteration 58/1000 | Loss: 0.00001447
Iteration 59/1000 | Loss: 0.00001447
Iteration 60/1000 | Loss: 0.00001446
Iteration 61/1000 | Loss: 0.00001446
Iteration 62/1000 | Loss: 0.00001446
Iteration 63/1000 | Loss: 0.00001446
Iteration 64/1000 | Loss: 0.00001446
Iteration 65/1000 | Loss: 0.00001446
Iteration 66/1000 | Loss: 0.00001446
Iteration 67/1000 | Loss: 0.00001446
Iteration 68/1000 | Loss: 0.00001446
Iteration 69/1000 | Loss: 0.00001446
Iteration 70/1000 | Loss: 0.00001446
Iteration 71/1000 | Loss: 0.00001445
Iteration 72/1000 | Loss: 0.00001445
Iteration 73/1000 | Loss: 0.00001445
Iteration 74/1000 | Loss: 0.00001445
Iteration 75/1000 | Loss: 0.00001445
Iteration 76/1000 | Loss: 0.00001444
Iteration 77/1000 | Loss: 0.00001444
Iteration 78/1000 | Loss: 0.00001444
Iteration 79/1000 | Loss: 0.00001444
Iteration 80/1000 | Loss: 0.00001444
Iteration 81/1000 | Loss: 0.00001444
Iteration 82/1000 | Loss: 0.00001444
Iteration 83/1000 | Loss: 0.00001444
Iteration 84/1000 | Loss: 0.00001444
Iteration 85/1000 | Loss: 0.00001444
Iteration 86/1000 | Loss: 0.00001443
Iteration 87/1000 | Loss: 0.00001443
Iteration 88/1000 | Loss: 0.00001443
Iteration 89/1000 | Loss: 0.00001443
Iteration 90/1000 | Loss: 0.00001443
Iteration 91/1000 | Loss: 0.00001443
Iteration 92/1000 | Loss: 0.00002476
Iteration 93/1000 | Loss: 0.00001566
Iteration 94/1000 | Loss: 0.00001443
Iteration 95/1000 | Loss: 0.00001443
Iteration 96/1000 | Loss: 0.00001443
Iteration 97/1000 | Loss: 0.00001456
Iteration 98/1000 | Loss: 0.00001456
Iteration 99/1000 | Loss: 0.00001456
Iteration 100/1000 | Loss: 0.00001456
Iteration 101/1000 | Loss: 0.00001441
Iteration 102/1000 | Loss: 0.00001567
Iteration 103/1000 | Loss: 0.00001440
Iteration 104/1000 | Loss: 0.00001440
Iteration 105/1000 | Loss: 0.00001440
Iteration 106/1000 | Loss: 0.00001439
Iteration 107/1000 | Loss: 0.00001439
Iteration 108/1000 | Loss: 0.00001439
Iteration 109/1000 | Loss: 0.00001439
Iteration 110/1000 | Loss: 0.00001439
Iteration 111/1000 | Loss: 0.00001439
Iteration 112/1000 | Loss: 0.00001439
Iteration 113/1000 | Loss: 0.00001439
Iteration 114/1000 | Loss: 0.00001439
Iteration 115/1000 | Loss: 0.00001439
Iteration 116/1000 | Loss: 0.00001439
Iteration 117/1000 | Loss: 0.00001439
Iteration 118/1000 | Loss: 0.00001439
Iteration 119/1000 | Loss: 0.00001439
Iteration 120/1000 | Loss: 0.00001439
Iteration 121/1000 | Loss: 0.00001439
Iteration 122/1000 | Loss: 0.00001439
Iteration 123/1000 | Loss: 0.00001439
Iteration 124/1000 | Loss: 0.00001439
Iteration 125/1000 | Loss: 0.00001439
Iteration 126/1000 | Loss: 0.00001439
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.438912204321241e-05, 1.438912204321241e-05, 1.438912204321241e-05, 1.438912204321241e-05, 1.438912204321241e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.438912204321241e-05

Optimization complete. Final v2v error: 3.243292808532715 mm

Highest mean error: 4.345102787017822 mm for frame 78

Lowest mean error: 2.905524730682373 mm for frame 25

Saving results

Total time: 85.84972143173218
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carina_posed_010/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01005333
Iteration 2/25 | Loss: 0.00207561
Iteration 3/25 | Loss: 0.00188725
Iteration 4/25 | Loss: 0.00140862
Iteration 5/25 | Loss: 0.00138609
Iteration 6/25 | Loss: 0.00138194
Iteration 7/25 | Loss: 0.00138536
Iteration 8/25 | Loss: 0.00138067
Iteration 9/25 | Loss: 0.00137728
Iteration 10/25 | Loss: 0.00137624
Iteration 11/25 | Loss: 0.00137593
Iteration 12/25 | Loss: 0.00137575
Iteration 13/25 | Loss: 0.00137564
Iteration 14/25 | Loss: 0.00137554
Iteration 15/25 | Loss: 0.00137538
Iteration 16/25 | Loss: 0.00137521
Iteration 17/25 | Loss: 0.00137513
Iteration 18/25 | Loss: 0.00137510
Iteration 19/25 | Loss: 0.00137510
Iteration 20/25 | Loss: 0.00137510
Iteration 21/25 | Loss: 0.00137508
Iteration 22/25 | Loss: 0.00137500
Iteration 23/25 | Loss: 0.00137485
Iteration 24/25 | Loss: 0.00137474
Iteration 25/25 | Loss: 0.00137473

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20713365
Iteration 2/25 | Loss: 0.00221045
Iteration 3/25 | Loss: 0.00221044
Iteration 4/25 | Loss: 0.00221044
Iteration 5/25 | Loss: 0.00221044
Iteration 6/25 | Loss: 0.00221044
Iteration 7/25 | Loss: 0.00221044
Iteration 8/25 | Loss: 0.00221044
Iteration 9/25 | Loss: 0.00221044
Iteration 10/25 | Loss: 0.00221044
Iteration 11/25 | Loss: 0.00221044
Iteration 12/25 | Loss: 0.00221044
Iteration 13/25 | Loss: 0.00221044
Iteration 14/25 | Loss: 0.00221044
Iteration 15/25 | Loss: 0.00221044
Iteration 16/25 | Loss: 0.00221044
Iteration 17/25 | Loss: 0.00221044
Iteration 18/25 | Loss: 0.00221044
Iteration 19/25 | Loss: 0.00221044
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.002210441045463085, 0.002210441045463085, 0.002210441045463085, 0.002210441045463085, 0.002210441045463085]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002210441045463085

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00221044
Iteration 2/1000 | Loss: 0.00024098
Iteration 3/1000 | Loss: 0.00012387
Iteration 4/1000 | Loss: 0.00008566
Iteration 5/1000 | Loss: 0.00002698
Iteration 6/1000 | Loss: 0.00002273
Iteration 7/1000 | Loss: 0.00002051
Iteration 8/1000 | Loss: 0.00001866
Iteration 9/1000 | Loss: 0.00001716
Iteration 10/1000 | Loss: 0.00001630
Iteration 11/1000 | Loss: 0.00001568
Iteration 12/1000 | Loss: 0.00001525
Iteration 13/1000 | Loss: 0.00001504
Iteration 14/1000 | Loss: 0.00001488
Iteration 15/1000 | Loss: 0.00001468
Iteration 16/1000 | Loss: 0.00001465
Iteration 17/1000 | Loss: 0.00001465
Iteration 18/1000 | Loss: 0.00001465
Iteration 19/1000 | Loss: 0.00001465
Iteration 20/1000 | Loss: 0.00001464
Iteration 21/1000 | Loss: 0.00001464
Iteration 22/1000 | Loss: 0.00001464
Iteration 23/1000 | Loss: 0.00001463
Iteration 24/1000 | Loss: 0.00001456
Iteration 25/1000 | Loss: 0.00001454
Iteration 26/1000 | Loss: 0.00001452
Iteration 27/1000 | Loss: 0.00001452
Iteration 28/1000 | Loss: 0.00001452
Iteration 29/1000 | Loss: 0.00001452
Iteration 30/1000 | Loss: 0.00001452
Iteration 31/1000 | Loss: 0.00001451
Iteration 32/1000 | Loss: 0.00001451
Iteration 33/1000 | Loss: 0.00001450
Iteration 34/1000 | Loss: 0.00001449
Iteration 35/1000 | Loss: 0.00001448
Iteration 36/1000 | Loss: 0.00001447
Iteration 37/1000 | Loss: 0.00001441
Iteration 38/1000 | Loss: 0.00001440
Iteration 39/1000 | Loss: 0.00001440
Iteration 40/1000 | Loss: 0.00001439
Iteration 41/1000 | Loss: 0.00001439
Iteration 42/1000 | Loss: 0.00001439
Iteration 43/1000 | Loss: 0.00001439
Iteration 44/1000 | Loss: 0.00001439
Iteration 45/1000 | Loss: 0.00001436
Iteration 46/1000 | Loss: 0.00001436
Iteration 47/1000 | Loss: 0.00001435
Iteration 48/1000 | Loss: 0.00001435
Iteration 49/1000 | Loss: 0.00001435
Iteration 50/1000 | Loss: 0.00001434
Iteration 51/1000 | Loss: 0.00001434
Iteration 52/1000 | Loss: 0.00001434
Iteration 53/1000 | Loss: 0.00001434
Iteration 54/1000 | Loss: 0.00001434
Iteration 55/1000 | Loss: 0.00001434
Iteration 56/1000 | Loss: 0.00001434
Iteration 57/1000 | Loss: 0.00001433
Iteration 58/1000 | Loss: 0.00001433
Iteration 59/1000 | Loss: 0.00001433
Iteration 60/1000 | Loss: 0.00001433
Iteration 61/1000 | Loss: 0.00001433
Iteration 62/1000 | Loss: 0.00001433
Iteration 63/1000 | Loss: 0.00001432
Iteration 64/1000 | Loss: 0.00001432
Iteration 65/1000 | Loss: 0.00001432
Iteration 66/1000 | Loss: 0.00001432
Iteration 67/1000 | Loss: 0.00001431
Iteration 68/1000 | Loss: 0.00001431
Iteration 69/1000 | Loss: 0.00001430
Iteration 70/1000 | Loss: 0.00001430
Iteration 71/1000 | Loss: 0.00001430
Iteration 72/1000 | Loss: 0.00001430
Iteration 73/1000 | Loss: 0.00001430
Iteration 74/1000 | Loss: 0.00001430
Iteration 75/1000 | Loss: 0.00001429
Iteration 76/1000 | Loss: 0.00001429
Iteration 77/1000 | Loss: 0.00001429
Iteration 78/1000 | Loss: 0.00001429
Iteration 79/1000 | Loss: 0.00001429
Iteration 80/1000 | Loss: 0.00001429
Iteration 81/1000 | Loss: 0.00001429
Iteration 82/1000 | Loss: 0.00001428
Iteration 83/1000 | Loss: 0.00001428
Iteration 84/1000 | Loss: 0.00001428
Iteration 85/1000 | Loss: 0.00001428
Iteration 86/1000 | Loss: 0.00001428
Iteration 87/1000 | Loss: 0.00001427
Iteration 88/1000 | Loss: 0.00001427
Iteration 89/1000 | Loss: 0.00001426
Iteration 90/1000 | Loss: 0.00001426
Iteration 91/1000 | Loss: 0.00001426
Iteration 92/1000 | Loss: 0.00001426
Iteration 93/1000 | Loss: 0.00001426
Iteration 94/1000 | Loss: 0.00001425
Iteration 95/1000 | Loss: 0.00001425
Iteration 96/1000 | Loss: 0.00001425
Iteration 97/1000 | Loss: 0.00001425
Iteration 98/1000 | Loss: 0.00001425
Iteration 99/1000 | Loss: 0.00001424
Iteration 100/1000 | Loss: 0.00001424
Iteration 101/1000 | Loss: 0.00001424
Iteration 102/1000 | Loss: 0.00001424
Iteration 103/1000 | Loss: 0.00001424
Iteration 104/1000 | Loss: 0.00001424
Iteration 105/1000 | Loss: 0.00001423
Iteration 106/1000 | Loss: 0.00001423
Iteration 107/1000 | Loss: 0.00001423
Iteration 108/1000 | Loss: 0.00001423
Iteration 109/1000 | Loss: 0.00001423
Iteration 110/1000 | Loss: 0.00001423
Iteration 111/1000 | Loss: 0.00001423
Iteration 112/1000 | Loss: 0.00001423
Iteration 113/1000 | Loss: 0.00001423
Iteration 114/1000 | Loss: 0.00001422
Iteration 115/1000 | Loss: 0.00001422
Iteration 116/1000 | Loss: 0.00001422
Iteration 117/1000 | Loss: 0.00001422
Iteration 118/1000 | Loss: 0.00001422
Iteration 119/1000 | Loss: 0.00001422
Iteration 120/1000 | Loss: 0.00001422
Iteration 121/1000 | Loss: 0.00001421
Iteration 122/1000 | Loss: 0.00001421
Iteration 123/1000 | Loss: 0.00001421
Iteration 124/1000 | Loss: 0.00001421
Iteration 125/1000 | Loss: 0.00001421
Iteration 126/1000 | Loss: 0.00001421
Iteration 127/1000 | Loss: 0.00001421
Iteration 128/1000 | Loss: 0.00001421
Iteration 129/1000 | Loss: 0.00001420
Iteration 130/1000 | Loss: 0.00001420
Iteration 131/1000 | Loss: 0.00001420
Iteration 132/1000 | Loss: 0.00001420
Iteration 133/1000 | Loss: 0.00001419
Iteration 134/1000 | Loss: 0.00001419
Iteration 135/1000 | Loss: 0.00001419
Iteration 136/1000 | Loss: 0.00001419
Iteration 137/1000 | Loss: 0.00001419
Iteration 138/1000 | Loss: 0.00001419
Iteration 139/1000 | Loss: 0.00001419
Iteration 140/1000 | Loss: 0.00001419
Iteration 141/1000 | Loss: 0.00001419
Iteration 142/1000 | Loss: 0.00001419
Iteration 143/1000 | Loss: 0.00001419
Iteration 144/1000 | Loss: 0.00001419
Iteration 145/1000 | Loss: 0.00001419
Iteration 146/1000 | Loss: 0.00001419
Iteration 147/1000 | Loss: 0.00001419
Iteration 148/1000 | Loss: 0.00001419
Iteration 149/1000 | Loss: 0.00001419
Iteration 150/1000 | Loss: 0.00001419
Iteration 151/1000 | Loss: 0.00001419
Iteration 152/1000 | Loss: 0.00001419
Iteration 153/1000 | Loss: 0.00001419
Iteration 154/1000 | Loss: 0.00001419
Iteration 155/1000 | Loss: 0.00001419
Iteration 156/1000 | Loss: 0.00001419
Iteration 157/1000 | Loss: 0.00001419
Iteration 158/1000 | Loss: 0.00001419
Iteration 159/1000 | Loss: 0.00001419
Iteration 160/1000 | Loss: 0.00001419
Iteration 161/1000 | Loss: 0.00001419
Iteration 162/1000 | Loss: 0.00001419
Iteration 163/1000 | Loss: 0.00001419
Iteration 164/1000 | Loss: 0.00001419
Iteration 165/1000 | Loss: 0.00001419
Iteration 166/1000 | Loss: 0.00001419
Iteration 167/1000 | Loss: 0.00001419
Iteration 168/1000 | Loss: 0.00001419
Iteration 169/1000 | Loss: 0.00001419
Iteration 170/1000 | Loss: 0.00001419
Iteration 171/1000 | Loss: 0.00001419
Iteration 172/1000 | Loss: 0.00001419
Iteration 173/1000 | Loss: 0.00001419
Iteration 174/1000 | Loss: 0.00001419
Iteration 175/1000 | Loss: 0.00001419
Iteration 176/1000 | Loss: 0.00001419
Iteration 177/1000 | Loss: 0.00001419
Iteration 178/1000 | Loss: 0.00001419
Iteration 179/1000 | Loss: 0.00001419
Iteration 180/1000 | Loss: 0.00001419
Iteration 181/1000 | Loss: 0.00001419
Iteration 182/1000 | Loss: 0.00001419
Iteration 183/1000 | Loss: 0.00001419
Iteration 184/1000 | Loss: 0.00001419
Iteration 185/1000 | Loss: 0.00001419
Iteration 186/1000 | Loss: 0.00001419
Iteration 187/1000 | Loss: 0.00001419
Iteration 188/1000 | Loss: 0.00001419
Iteration 189/1000 | Loss: 0.00001419
Iteration 190/1000 | Loss: 0.00001419
Iteration 191/1000 | Loss: 0.00001419
Iteration 192/1000 | Loss: 0.00001419
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [1.4189396097208373e-05, 1.4189396097208373e-05, 1.4189396097208373e-05, 1.4189396097208373e-05, 1.4189396097208373e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4189396097208373e-05

Optimization complete. Final v2v error: 3.2318241596221924 mm

Highest mean error: 3.5727646350860596 mm for frame 58

Lowest mean error: 3.0042667388916016 mm for frame 1

Saving results

Total time: 68.04690027236938
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carina_posed_010/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00989962
Iteration 2/25 | Loss: 0.00281895
Iteration 3/25 | Loss: 0.00191438
Iteration 4/25 | Loss: 0.00172119
Iteration 5/25 | Loss: 0.00171468
Iteration 6/25 | Loss: 0.00168827
Iteration 7/25 | Loss: 0.00159889
Iteration 8/25 | Loss: 0.00152796
Iteration 9/25 | Loss: 0.00147132
Iteration 10/25 | Loss: 0.00145278
Iteration 11/25 | Loss: 0.00146692
Iteration 12/25 | Loss: 0.00145995
Iteration 13/25 | Loss: 0.00144838
Iteration 14/25 | Loss: 0.00144508
Iteration 15/25 | Loss: 0.00145825
Iteration 16/25 | Loss: 0.00143646
Iteration 17/25 | Loss: 0.00142460
Iteration 18/25 | Loss: 0.00142148
Iteration 19/25 | Loss: 0.00141993
Iteration 20/25 | Loss: 0.00142397
Iteration 21/25 | Loss: 0.00142005
Iteration 22/25 | Loss: 0.00141808
Iteration 23/25 | Loss: 0.00141624
Iteration 24/25 | Loss: 0.00141582
Iteration 25/25 | Loss: 0.00141518

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24903750
Iteration 2/25 | Loss: 0.00236442
Iteration 3/25 | Loss: 0.00223051
Iteration 4/25 | Loss: 0.00223050
Iteration 5/25 | Loss: 0.00223050
Iteration 6/25 | Loss: 0.00223050
Iteration 7/25 | Loss: 0.00223050
Iteration 8/25 | Loss: 0.00223050
Iteration 9/25 | Loss: 0.00223050
Iteration 10/25 | Loss: 0.00223050
Iteration 11/25 | Loss: 0.00223050
Iteration 12/25 | Loss: 0.00223050
Iteration 13/25 | Loss: 0.00223050
Iteration 14/25 | Loss: 0.00223050
Iteration 15/25 | Loss: 0.00223050
Iteration 16/25 | Loss: 0.00223050
Iteration 17/25 | Loss: 0.00223050
Iteration 18/25 | Loss: 0.00223050
Iteration 19/25 | Loss: 0.00223050
Iteration 20/25 | Loss: 0.00223050
Iteration 21/25 | Loss: 0.00223050
Iteration 22/25 | Loss: 0.00223050
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0022305005695670843, 0.0022305005695670843, 0.0022305005695670843, 0.0022305005695670843, 0.0022305005695670843]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022305005695670843

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00223050
Iteration 2/1000 | Loss: 0.00026716
Iteration 3/1000 | Loss: 0.00010545
Iteration 4/1000 | Loss: 0.00005452
Iteration 5/1000 | Loss: 0.00019653
Iteration 6/1000 | Loss: 0.00006709
Iteration 7/1000 | Loss: 0.00004566
Iteration 8/1000 | Loss: 0.00004392
Iteration 9/1000 | Loss: 0.00009407
Iteration 10/1000 | Loss: 0.00004182
Iteration 11/1000 | Loss: 0.00012453
Iteration 12/1000 | Loss: 0.00014519
Iteration 13/1000 | Loss: 0.00005153
Iteration 14/1000 | Loss: 0.00003971
Iteration 15/1000 | Loss: 0.00012957
Iteration 16/1000 | Loss: 0.00007928
Iteration 17/1000 | Loss: 0.00013392
Iteration 18/1000 | Loss: 0.00010476
Iteration 19/1000 | Loss: 0.00006369
Iteration 20/1000 | Loss: 0.00007815
Iteration 21/1000 | Loss: 0.00004421
Iteration 22/1000 | Loss: 0.00004234
Iteration 23/1000 | Loss: 0.00004172
Iteration 24/1000 | Loss: 0.00004064
Iteration 25/1000 | Loss: 0.00060798
Iteration 26/1000 | Loss: 0.00022548
Iteration 27/1000 | Loss: 0.00018903
Iteration 28/1000 | Loss: 0.00004125
Iteration 29/1000 | Loss: 0.00003872
Iteration 30/1000 | Loss: 0.00003715
Iteration 31/1000 | Loss: 0.00003598
Iteration 32/1000 | Loss: 0.00003522
Iteration 33/1000 | Loss: 0.00003455
Iteration 34/1000 | Loss: 0.00003384
Iteration 35/1000 | Loss: 0.00003315
Iteration 36/1000 | Loss: 0.00003257
Iteration 37/1000 | Loss: 0.00003230
Iteration 38/1000 | Loss: 0.00003207
Iteration 39/1000 | Loss: 0.00003197
Iteration 40/1000 | Loss: 0.00003190
Iteration 41/1000 | Loss: 0.00003186
Iteration 42/1000 | Loss: 0.00003171
Iteration 43/1000 | Loss: 0.00003166
Iteration 44/1000 | Loss: 0.00003165
Iteration 45/1000 | Loss: 0.00029740
Iteration 46/1000 | Loss: 0.00036632
Iteration 47/1000 | Loss: 0.00127813
Iteration 48/1000 | Loss: 0.00076249
Iteration 49/1000 | Loss: 0.00063950
Iteration 50/1000 | Loss: 0.00036032
Iteration 51/1000 | Loss: 0.00042260
Iteration 52/1000 | Loss: 0.00008692
Iteration 53/1000 | Loss: 0.00003134
Iteration 54/1000 | Loss: 0.00002851
Iteration 55/1000 | Loss: 0.00005813
Iteration 56/1000 | Loss: 0.00002553
Iteration 57/1000 | Loss: 0.00002449
Iteration 58/1000 | Loss: 0.00002388
Iteration 59/1000 | Loss: 0.00002349
Iteration 60/1000 | Loss: 0.00002307
Iteration 61/1000 | Loss: 0.00002273
Iteration 62/1000 | Loss: 0.00002240
Iteration 63/1000 | Loss: 0.00002226
Iteration 64/1000 | Loss: 0.00002215
Iteration 65/1000 | Loss: 0.00002207
Iteration 66/1000 | Loss: 0.00002206
Iteration 67/1000 | Loss: 0.00002200
Iteration 68/1000 | Loss: 0.00002200
Iteration 69/1000 | Loss: 0.00002198
Iteration 70/1000 | Loss: 0.00002195
Iteration 71/1000 | Loss: 0.00002195
Iteration 72/1000 | Loss: 0.00002194
Iteration 73/1000 | Loss: 0.00002194
Iteration 74/1000 | Loss: 0.00002194
Iteration 75/1000 | Loss: 0.00002194
Iteration 76/1000 | Loss: 0.00002193
Iteration 77/1000 | Loss: 0.00002193
Iteration 78/1000 | Loss: 0.00002193
Iteration 79/1000 | Loss: 0.00002193
Iteration 80/1000 | Loss: 0.00002193
Iteration 81/1000 | Loss: 0.00002192
Iteration 82/1000 | Loss: 0.00002191
Iteration 83/1000 | Loss: 0.00002190
Iteration 84/1000 | Loss: 0.00002190
Iteration 85/1000 | Loss: 0.00002190
Iteration 86/1000 | Loss: 0.00002190
Iteration 87/1000 | Loss: 0.00002190
Iteration 88/1000 | Loss: 0.00002190
Iteration 89/1000 | Loss: 0.00002190
Iteration 90/1000 | Loss: 0.00002190
Iteration 91/1000 | Loss: 0.00002190
Iteration 92/1000 | Loss: 0.00002190
Iteration 93/1000 | Loss: 0.00002190
Iteration 94/1000 | Loss: 0.00002190
Iteration 95/1000 | Loss: 0.00002189
Iteration 96/1000 | Loss: 0.00002189
Iteration 97/1000 | Loss: 0.00002189
Iteration 98/1000 | Loss: 0.00002188
Iteration 99/1000 | Loss: 0.00002188
Iteration 100/1000 | Loss: 0.00002188
Iteration 101/1000 | Loss: 0.00002188
Iteration 102/1000 | Loss: 0.00002187
Iteration 103/1000 | Loss: 0.00002187
Iteration 104/1000 | Loss: 0.00002187
Iteration 105/1000 | Loss: 0.00002187
Iteration 106/1000 | Loss: 0.00002187
Iteration 107/1000 | Loss: 0.00002187
Iteration 108/1000 | Loss: 0.00002186
Iteration 109/1000 | Loss: 0.00002186
Iteration 110/1000 | Loss: 0.00002186
Iteration 111/1000 | Loss: 0.00002185
Iteration 112/1000 | Loss: 0.00002185
Iteration 113/1000 | Loss: 0.00002185
Iteration 114/1000 | Loss: 0.00002184
Iteration 115/1000 | Loss: 0.00002184
Iteration 116/1000 | Loss: 0.00002184
Iteration 117/1000 | Loss: 0.00002184
Iteration 118/1000 | Loss: 0.00002184
Iteration 119/1000 | Loss: 0.00002184
Iteration 120/1000 | Loss: 0.00002184
Iteration 121/1000 | Loss: 0.00002184
Iteration 122/1000 | Loss: 0.00002184
Iteration 123/1000 | Loss: 0.00002184
Iteration 124/1000 | Loss: 0.00002183
Iteration 125/1000 | Loss: 0.00002183
Iteration 126/1000 | Loss: 0.00002183
Iteration 127/1000 | Loss: 0.00002183
Iteration 128/1000 | Loss: 0.00002183
Iteration 129/1000 | Loss: 0.00002183
Iteration 130/1000 | Loss: 0.00002183
Iteration 131/1000 | Loss: 0.00002182
Iteration 132/1000 | Loss: 0.00002182
Iteration 133/1000 | Loss: 0.00002182
Iteration 134/1000 | Loss: 0.00002182
Iteration 135/1000 | Loss: 0.00002182
Iteration 136/1000 | Loss: 0.00002182
Iteration 137/1000 | Loss: 0.00002182
Iteration 138/1000 | Loss: 0.00002182
Iteration 139/1000 | Loss: 0.00002182
Iteration 140/1000 | Loss: 0.00002182
Iteration 141/1000 | Loss: 0.00002182
Iteration 142/1000 | Loss: 0.00002182
Iteration 143/1000 | Loss: 0.00002182
Iteration 144/1000 | Loss: 0.00002182
Iteration 145/1000 | Loss: 0.00002182
Iteration 146/1000 | Loss: 0.00002182
Iteration 147/1000 | Loss: 0.00002182
Iteration 148/1000 | Loss: 0.00002182
Iteration 149/1000 | Loss: 0.00002182
Iteration 150/1000 | Loss: 0.00002182
Iteration 151/1000 | Loss: 0.00002182
Iteration 152/1000 | Loss: 0.00002182
Iteration 153/1000 | Loss: 0.00002182
Iteration 154/1000 | Loss: 0.00002182
Iteration 155/1000 | Loss: 0.00002182
Iteration 156/1000 | Loss: 0.00002182
Iteration 157/1000 | Loss: 0.00002182
Iteration 158/1000 | Loss: 0.00002182
Iteration 159/1000 | Loss: 0.00002182
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [2.181614217988681e-05, 2.181614217988681e-05, 2.181614217988681e-05, 2.181614217988681e-05, 2.181614217988681e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.181614217988681e-05

Optimization complete. Final v2v error: 4.0013427734375 mm

Highest mean error: 4.448582649230957 mm for frame 69

Lowest mean error: 3.379000425338745 mm for frame 5

Saving results

Total time: 137.5052134990692
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carina_posed_010/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carina_posed_010/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00494270
Iteration 2/25 | Loss: 0.00144614
Iteration 3/25 | Loss: 0.00137170
Iteration 4/25 | Loss: 0.00135732
Iteration 5/25 | Loss: 0.00135363
Iteration 6/25 | Loss: 0.00135330
Iteration 7/25 | Loss: 0.00135330
Iteration 8/25 | Loss: 0.00135330
Iteration 9/25 | Loss: 0.00135330
Iteration 10/25 | Loss: 0.00135330
Iteration 11/25 | Loss: 0.00135330
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013532985467463732, 0.0013532985467463732, 0.0013532985467463732, 0.0013532985467463732, 0.0013532985467463732]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013532985467463732

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.90592420
Iteration 2/25 | Loss: 0.00176226
Iteration 3/25 | Loss: 0.00176221
Iteration 4/25 | Loss: 0.00176221
Iteration 5/25 | Loss: 0.00176221
Iteration 6/25 | Loss: 0.00176221
Iteration 7/25 | Loss: 0.00176221
Iteration 8/25 | Loss: 0.00176221
Iteration 9/25 | Loss: 0.00176221
Iteration 10/25 | Loss: 0.00176221
Iteration 11/25 | Loss: 0.00176221
Iteration 12/25 | Loss: 0.00176221
Iteration 13/25 | Loss: 0.00176221
Iteration 14/25 | Loss: 0.00176221
Iteration 15/25 | Loss: 0.00176221
Iteration 16/25 | Loss: 0.00176221
Iteration 17/25 | Loss: 0.00176221
Iteration 18/25 | Loss: 0.00176221
Iteration 19/25 | Loss: 0.00176221
Iteration 20/25 | Loss: 0.00176221
Iteration 21/25 | Loss: 0.00176221
Iteration 22/25 | Loss: 0.00176221
Iteration 23/25 | Loss: 0.00176221
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0017622108571231365, 0.0017622108571231365, 0.0017622108571231365, 0.0017622108571231365, 0.0017622108571231365]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017622108571231365

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00176221
Iteration 2/1000 | Loss: 0.00004963
Iteration 3/1000 | Loss: 0.00003255
Iteration 4/1000 | Loss: 0.00002703
Iteration 5/1000 | Loss: 0.00002569
Iteration 6/1000 | Loss: 0.00002417
Iteration 7/1000 | Loss: 0.00002329
Iteration 8/1000 | Loss: 0.00002246
Iteration 9/1000 | Loss: 0.00002196
Iteration 10/1000 | Loss: 0.00002141
Iteration 11/1000 | Loss: 0.00002107
Iteration 12/1000 | Loss: 0.00002061
Iteration 13/1000 | Loss: 0.00002024
Iteration 14/1000 | Loss: 0.00001987
Iteration 15/1000 | Loss: 0.00001957
Iteration 16/1000 | Loss: 0.00001915
Iteration 17/1000 | Loss: 0.00001911
Iteration 18/1000 | Loss: 0.00001899
Iteration 19/1000 | Loss: 0.00001893
Iteration 20/1000 | Loss: 0.00001884
Iteration 21/1000 | Loss: 0.00001884
Iteration 22/1000 | Loss: 0.00001883
Iteration 23/1000 | Loss: 0.00001883
Iteration 24/1000 | Loss: 0.00001882
Iteration 25/1000 | Loss: 0.00001873
Iteration 26/1000 | Loss: 0.00001871
Iteration 27/1000 | Loss: 0.00001866
Iteration 28/1000 | Loss: 0.00001863
Iteration 29/1000 | Loss: 0.00001861
Iteration 30/1000 | Loss: 0.00001861
Iteration 31/1000 | Loss: 0.00001861
Iteration 32/1000 | Loss: 0.00001861
Iteration 33/1000 | Loss: 0.00001861
Iteration 34/1000 | Loss: 0.00001861
Iteration 35/1000 | Loss: 0.00001861
Iteration 36/1000 | Loss: 0.00001861
Iteration 37/1000 | Loss: 0.00001861
Iteration 38/1000 | Loss: 0.00001860
Iteration 39/1000 | Loss: 0.00001860
Iteration 40/1000 | Loss: 0.00001860
Iteration 41/1000 | Loss: 0.00001860
Iteration 42/1000 | Loss: 0.00001860
Iteration 43/1000 | Loss: 0.00001860
Iteration 44/1000 | Loss: 0.00001860
Iteration 45/1000 | Loss: 0.00001860
Iteration 46/1000 | Loss: 0.00001860
Iteration 47/1000 | Loss: 0.00001860
Iteration 48/1000 | Loss: 0.00001860
Iteration 49/1000 | Loss: 0.00001860
Iteration 50/1000 | Loss: 0.00001860
Iteration 51/1000 | Loss: 0.00001860
Iteration 52/1000 | Loss: 0.00001860
Iteration 53/1000 | Loss: 0.00001860
Iteration 54/1000 | Loss: 0.00001860
Iteration 55/1000 | Loss: 0.00001860
Iteration 56/1000 | Loss: 0.00001859
Iteration 57/1000 | Loss: 0.00001858
Iteration 58/1000 | Loss: 0.00001858
Iteration 59/1000 | Loss: 0.00001858
Iteration 60/1000 | Loss: 0.00001858
Iteration 61/1000 | Loss: 0.00001858
Iteration 62/1000 | Loss: 0.00001858
Iteration 63/1000 | Loss: 0.00001857
Iteration 64/1000 | Loss: 0.00001857
Iteration 65/1000 | Loss: 0.00001856
Iteration 66/1000 | Loss: 0.00001856
Iteration 67/1000 | Loss: 0.00001856
Iteration 68/1000 | Loss: 0.00001855
Iteration 69/1000 | Loss: 0.00001855
Iteration 70/1000 | Loss: 0.00001855
Iteration 71/1000 | Loss: 0.00001855
Iteration 72/1000 | Loss: 0.00001855
Iteration 73/1000 | Loss: 0.00001855
Iteration 74/1000 | Loss: 0.00001855
Iteration 75/1000 | Loss: 0.00001855
Iteration 76/1000 | Loss: 0.00001855
Iteration 77/1000 | Loss: 0.00001854
Iteration 78/1000 | Loss: 0.00001854
Iteration 79/1000 | Loss: 0.00001854
Iteration 80/1000 | Loss: 0.00001853
Iteration 81/1000 | Loss: 0.00001853
Iteration 82/1000 | Loss: 0.00001853
Iteration 83/1000 | Loss: 0.00001853
Iteration 84/1000 | Loss: 0.00001853
Iteration 85/1000 | Loss: 0.00001853
Iteration 86/1000 | Loss: 0.00001852
Iteration 87/1000 | Loss: 0.00001852
Iteration 88/1000 | Loss: 0.00001852
Iteration 89/1000 | Loss: 0.00001851
Iteration 90/1000 | Loss: 0.00001851
Iteration 91/1000 | Loss: 0.00001851
Iteration 92/1000 | Loss: 0.00001851
Iteration 93/1000 | Loss: 0.00001851
Iteration 94/1000 | Loss: 0.00001850
Iteration 95/1000 | Loss: 0.00001850
Iteration 96/1000 | Loss: 0.00001850
Iteration 97/1000 | Loss: 0.00001850
Iteration 98/1000 | Loss: 0.00001850
Iteration 99/1000 | Loss: 0.00001850
Iteration 100/1000 | Loss: 0.00001850
Iteration 101/1000 | Loss: 0.00001850
Iteration 102/1000 | Loss: 0.00001849
Iteration 103/1000 | Loss: 0.00001849
Iteration 104/1000 | Loss: 0.00001849
Iteration 105/1000 | Loss: 0.00001849
Iteration 106/1000 | Loss: 0.00001849
Iteration 107/1000 | Loss: 0.00001849
Iteration 108/1000 | Loss: 0.00001849
Iteration 109/1000 | Loss: 0.00001849
Iteration 110/1000 | Loss: 0.00001849
Iteration 111/1000 | Loss: 0.00001848
Iteration 112/1000 | Loss: 0.00001848
Iteration 113/1000 | Loss: 0.00001848
Iteration 114/1000 | Loss: 0.00001848
Iteration 115/1000 | Loss: 0.00001848
Iteration 116/1000 | Loss: 0.00001848
Iteration 117/1000 | Loss: 0.00001848
Iteration 118/1000 | Loss: 0.00001848
Iteration 119/1000 | Loss: 0.00001848
Iteration 120/1000 | Loss: 0.00001847
Iteration 121/1000 | Loss: 0.00001847
Iteration 122/1000 | Loss: 0.00001847
Iteration 123/1000 | Loss: 0.00001847
Iteration 124/1000 | Loss: 0.00001847
Iteration 125/1000 | Loss: 0.00001847
Iteration 126/1000 | Loss: 0.00001847
Iteration 127/1000 | Loss: 0.00001847
Iteration 128/1000 | Loss: 0.00001847
Iteration 129/1000 | Loss: 0.00001847
Iteration 130/1000 | Loss: 0.00001847
Iteration 131/1000 | Loss: 0.00001846
Iteration 132/1000 | Loss: 0.00001846
Iteration 133/1000 | Loss: 0.00001846
Iteration 134/1000 | Loss: 0.00001846
Iteration 135/1000 | Loss: 0.00001846
Iteration 136/1000 | Loss: 0.00001845
Iteration 137/1000 | Loss: 0.00001845
Iteration 138/1000 | Loss: 0.00001845
Iteration 139/1000 | Loss: 0.00001844
Iteration 140/1000 | Loss: 0.00001844
Iteration 141/1000 | Loss: 0.00001844
Iteration 142/1000 | Loss: 0.00001844
Iteration 143/1000 | Loss: 0.00001844
Iteration 144/1000 | Loss: 0.00001844
Iteration 145/1000 | Loss: 0.00001844
Iteration 146/1000 | Loss: 0.00001844
Iteration 147/1000 | Loss: 0.00001844
Iteration 148/1000 | Loss: 0.00001844
Iteration 149/1000 | Loss: 0.00001844
Iteration 150/1000 | Loss: 0.00001844
Iteration 151/1000 | Loss: 0.00001844
Iteration 152/1000 | Loss: 0.00001844
Iteration 153/1000 | Loss: 0.00001844
Iteration 154/1000 | Loss: 0.00001844
Iteration 155/1000 | Loss: 0.00001844
Iteration 156/1000 | Loss: 0.00001844
Iteration 157/1000 | Loss: 0.00001844
Iteration 158/1000 | Loss: 0.00001844
Iteration 159/1000 | Loss: 0.00001844
Iteration 160/1000 | Loss: 0.00001844
Iteration 161/1000 | Loss: 0.00001844
Iteration 162/1000 | Loss: 0.00001844
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.8435253878124058e-05, 1.8435253878124058e-05, 1.8435253878124058e-05, 1.8435253878124058e-05, 1.8435253878124058e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8435253878124058e-05

Optimization complete. Final v2v error: 3.6651861667633057 mm

Highest mean error: 3.6982462406158447 mm for frame 4

Lowest mean error: 3.6389405727386475 mm for frame 55

Saving results

Total time: 42.25317692756653
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_5533/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5533/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5533/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01104184
Iteration 2/25 | Loss: 0.00393318
Iteration 3/25 | Loss: 0.00236555
Iteration 4/25 | Loss: 0.00185910
Iteration 5/25 | Loss: 0.00194009
Iteration 6/25 | Loss: 0.00191544
Iteration 7/25 | Loss: 0.00182471
Iteration 8/25 | Loss: 0.00175102
Iteration 9/25 | Loss: 0.00162714
Iteration 10/25 | Loss: 0.00152597
Iteration 11/25 | Loss: 0.00143277
Iteration 12/25 | Loss: 0.00139782
Iteration 13/25 | Loss: 0.00137804
Iteration 14/25 | Loss: 0.00135356
Iteration 15/25 | Loss: 0.00133072
Iteration 16/25 | Loss: 0.00131325
Iteration 17/25 | Loss: 0.00130438
Iteration 18/25 | Loss: 0.00129397
Iteration 19/25 | Loss: 0.00127874
Iteration 20/25 | Loss: 0.00127114
Iteration 21/25 | Loss: 0.00126350
Iteration 22/25 | Loss: 0.00126321
Iteration 23/25 | Loss: 0.00125747
Iteration 24/25 | Loss: 0.00125503
Iteration 25/25 | Loss: 0.00125502

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.96328175
Iteration 2/25 | Loss: 0.00170332
Iteration 3/25 | Loss: 0.00170331
Iteration 4/25 | Loss: 0.00170331
Iteration 5/25 | Loss: 0.00170331
Iteration 6/25 | Loss: 0.00170331
Iteration 7/25 | Loss: 0.00170331
Iteration 8/25 | Loss: 0.00170331
Iteration 9/25 | Loss: 0.00170331
Iteration 10/25 | Loss: 0.00170331
Iteration 11/25 | Loss: 0.00170331
Iteration 12/25 | Loss: 0.00170331
Iteration 13/25 | Loss: 0.00170331
Iteration 14/25 | Loss: 0.00170331
Iteration 15/25 | Loss: 0.00170331
Iteration 16/25 | Loss: 0.00170331
Iteration 17/25 | Loss: 0.00170331
Iteration 18/25 | Loss: 0.00170331
Iteration 19/25 | Loss: 0.00170331
Iteration 20/25 | Loss: 0.00170331
Iteration 21/25 | Loss: 0.00170331
Iteration 22/25 | Loss: 0.00170331
Iteration 23/25 | Loss: 0.00170331
Iteration 24/25 | Loss: 0.00170331
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0017033106414601207, 0.0017033106414601207, 0.0017033106414601207, 0.0017033106414601207, 0.0017033106414601207]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017033106414601207

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00170331
Iteration 2/1000 | Loss: 0.00018030
Iteration 3/1000 | Loss: 0.00023560
Iteration 4/1000 | Loss: 0.00009567
Iteration 5/1000 | Loss: 0.00007535
Iteration 6/1000 | Loss: 0.00015672
Iteration 7/1000 | Loss: 0.00006435
Iteration 8/1000 | Loss: 0.00006317
Iteration 9/1000 | Loss: 0.00012326
Iteration 10/1000 | Loss: 0.00015166
Iteration 11/1000 | Loss: 0.00009067
Iteration 12/1000 | Loss: 0.00007392
Iteration 13/1000 | Loss: 0.00007426
Iteration 14/1000 | Loss: 0.00007948
Iteration 15/1000 | Loss: 0.00006631
Iteration 16/1000 | Loss: 0.00006295
Iteration 17/1000 | Loss: 0.00006987
Iteration 18/1000 | Loss: 0.00007403
Iteration 19/1000 | Loss: 0.00007855
Iteration 20/1000 | Loss: 0.00014555
Iteration 21/1000 | Loss: 0.00011515
Iteration 22/1000 | Loss: 0.00005979
Iteration 23/1000 | Loss: 0.00009505
Iteration 24/1000 | Loss: 0.00004987
Iteration 25/1000 | Loss: 0.00006890
Iteration 26/1000 | Loss: 0.00004663
Iteration 27/1000 | Loss: 0.00006861
Iteration 28/1000 | Loss: 0.00004721
Iteration 29/1000 | Loss: 0.00004545
Iteration 30/1000 | Loss: 0.00004500
Iteration 31/1000 | Loss: 0.00004456
Iteration 32/1000 | Loss: 0.00004420
Iteration 33/1000 | Loss: 0.00005792
Iteration 34/1000 | Loss: 0.00004382
Iteration 35/1000 | Loss: 0.00004353
Iteration 36/1000 | Loss: 0.00004330
Iteration 37/1000 | Loss: 0.00004314
Iteration 38/1000 | Loss: 0.00004300
Iteration 39/1000 | Loss: 0.00007222
Iteration 40/1000 | Loss: 0.00004293
Iteration 41/1000 | Loss: 0.00004282
Iteration 42/1000 | Loss: 0.00004281
Iteration 43/1000 | Loss: 0.00004279
Iteration 44/1000 | Loss: 0.00004278
Iteration 45/1000 | Loss: 0.00004278
Iteration 46/1000 | Loss: 0.00004278
Iteration 47/1000 | Loss: 0.00004278
Iteration 48/1000 | Loss: 0.00004278
Iteration 49/1000 | Loss: 0.00004278
Iteration 50/1000 | Loss: 0.00004278
Iteration 51/1000 | Loss: 0.00004278
Iteration 52/1000 | Loss: 0.00004278
Iteration 53/1000 | Loss: 0.00004278
Iteration 54/1000 | Loss: 0.00004278
Iteration 55/1000 | Loss: 0.00004278
Iteration 56/1000 | Loss: 0.00004277
Iteration 57/1000 | Loss: 0.00004277
Iteration 58/1000 | Loss: 0.00004277
Iteration 59/1000 | Loss: 0.00004277
Iteration 60/1000 | Loss: 0.00004277
Iteration 61/1000 | Loss: 0.00004269
Iteration 62/1000 | Loss: 0.00015842
Iteration 63/1000 | Loss: 0.00025173
Iteration 64/1000 | Loss: 0.00052156
Iteration 65/1000 | Loss: 0.00042684
Iteration 66/1000 | Loss: 0.00043088
Iteration 67/1000 | Loss: 0.00062023
Iteration 68/1000 | Loss: 0.00027165
Iteration 69/1000 | Loss: 0.00008294
Iteration 70/1000 | Loss: 0.00058247
Iteration 71/1000 | Loss: 0.00009861
Iteration 72/1000 | Loss: 0.00006750
Iteration 73/1000 | Loss: 0.00007924
Iteration 74/1000 | Loss: 0.00005786
Iteration 75/1000 | Loss: 0.00048387
Iteration 76/1000 | Loss: 0.00009636
Iteration 77/1000 | Loss: 0.00074077
Iteration 78/1000 | Loss: 0.00014008
Iteration 79/1000 | Loss: 0.00009203
Iteration 80/1000 | Loss: 0.00028052
Iteration 81/1000 | Loss: 0.00006829
Iteration 82/1000 | Loss: 0.00026037
Iteration 83/1000 | Loss: 0.00029146
Iteration 84/1000 | Loss: 0.00058871
Iteration 85/1000 | Loss: 0.00021377
Iteration 86/1000 | Loss: 0.00061453
Iteration 87/1000 | Loss: 0.00020779
Iteration 88/1000 | Loss: 0.00045582
Iteration 89/1000 | Loss: 0.00019919
Iteration 90/1000 | Loss: 0.00029152
Iteration 91/1000 | Loss: 0.00028077
Iteration 92/1000 | Loss: 0.00036840
Iteration 93/1000 | Loss: 0.00028964
Iteration 94/1000 | Loss: 0.00027043
Iteration 95/1000 | Loss: 0.00006254
Iteration 96/1000 | Loss: 0.00005461
Iteration 97/1000 | Loss: 0.00023858
Iteration 98/1000 | Loss: 0.00028460
Iteration 99/1000 | Loss: 0.00027364
Iteration 100/1000 | Loss: 0.00015868
Iteration 101/1000 | Loss: 0.00031361
Iteration 102/1000 | Loss: 0.00029239
Iteration 103/1000 | Loss: 0.00041770
Iteration 104/1000 | Loss: 0.00036558
Iteration 105/1000 | Loss: 0.00039537
Iteration 106/1000 | Loss: 0.00028509
Iteration 107/1000 | Loss: 0.00040959
Iteration 108/1000 | Loss: 0.00047740
Iteration 109/1000 | Loss: 0.00013232
Iteration 110/1000 | Loss: 0.00006053
Iteration 111/1000 | Loss: 0.00005470
Iteration 112/1000 | Loss: 0.00043229
Iteration 113/1000 | Loss: 0.00014888
Iteration 114/1000 | Loss: 0.00013257
Iteration 115/1000 | Loss: 0.00028690
Iteration 116/1000 | Loss: 0.00027121
Iteration 117/1000 | Loss: 0.00051869
Iteration 118/1000 | Loss: 0.00037923
Iteration 119/1000 | Loss: 0.00007461
Iteration 120/1000 | Loss: 0.00005314
Iteration 121/1000 | Loss: 0.00004963
Iteration 122/1000 | Loss: 0.00006658
Iteration 123/1000 | Loss: 0.00028187
Iteration 124/1000 | Loss: 0.00041087
Iteration 125/1000 | Loss: 0.00006041
Iteration 126/1000 | Loss: 0.00004975
Iteration 127/1000 | Loss: 0.00006312
Iteration 128/1000 | Loss: 0.00004558
Iteration 129/1000 | Loss: 0.00004491
Iteration 130/1000 | Loss: 0.00004453
Iteration 131/1000 | Loss: 0.00004429
Iteration 132/1000 | Loss: 0.00004399
Iteration 133/1000 | Loss: 0.00012689
Iteration 134/1000 | Loss: 0.00006715
Iteration 135/1000 | Loss: 0.00005302
Iteration 136/1000 | Loss: 0.00004514
Iteration 137/1000 | Loss: 0.00010590
Iteration 138/1000 | Loss: 0.00004494
Iteration 139/1000 | Loss: 0.00009390
Iteration 140/1000 | Loss: 0.00004447
Iteration 141/1000 | Loss: 0.00012280
Iteration 142/1000 | Loss: 0.00008744
Iteration 143/1000 | Loss: 0.00009643
Iteration 144/1000 | Loss: 0.00011884
Iteration 145/1000 | Loss: 0.00007619
Iteration 146/1000 | Loss: 0.00012581
Iteration 147/1000 | Loss: 0.00004854
Iteration 148/1000 | Loss: 0.00015270
Iteration 149/1000 | Loss: 0.00019935
Iteration 150/1000 | Loss: 0.00009484
Iteration 151/1000 | Loss: 0.00012719
Iteration 152/1000 | Loss: 0.00015385
Iteration 153/1000 | Loss: 0.00008436
Iteration 154/1000 | Loss: 0.00009443
Iteration 155/1000 | Loss: 0.00008287
Iteration 156/1000 | Loss: 0.00015173
Iteration 157/1000 | Loss: 0.00008240
Iteration 158/1000 | Loss: 0.00011868
Iteration 159/1000 | Loss: 0.00008340
Iteration 160/1000 | Loss: 0.00012753
Iteration 161/1000 | Loss: 0.00028698
Iteration 162/1000 | Loss: 0.00012731
Iteration 163/1000 | Loss: 0.00018226
Iteration 164/1000 | Loss: 0.00020298
Iteration 165/1000 | Loss: 0.00012452
Iteration 166/1000 | Loss: 0.00015517
Iteration 167/1000 | Loss: 0.00019410
Iteration 168/1000 | Loss: 0.00015671
Iteration 169/1000 | Loss: 0.00016401
Iteration 170/1000 | Loss: 0.00007791
Iteration 171/1000 | Loss: 0.00010327
Iteration 172/1000 | Loss: 0.00011893
Iteration 173/1000 | Loss: 0.00019360
Iteration 174/1000 | Loss: 0.00018137
Iteration 175/1000 | Loss: 0.00011307
Iteration 176/1000 | Loss: 0.00013601
Iteration 177/1000 | Loss: 0.00007171
Iteration 178/1000 | Loss: 0.00009318
Iteration 179/1000 | Loss: 0.00011946
Iteration 180/1000 | Loss: 0.00021926
Iteration 181/1000 | Loss: 0.00010334
Iteration 182/1000 | Loss: 0.00017858
Iteration 183/1000 | Loss: 0.00010671
Iteration 184/1000 | Loss: 0.00018609
Iteration 185/1000 | Loss: 0.00014836
Iteration 186/1000 | Loss: 0.00022205
Iteration 187/1000 | Loss: 0.00016136
Iteration 188/1000 | Loss: 0.00017464
Iteration 189/1000 | Loss: 0.00027976
Iteration 190/1000 | Loss: 0.00012671
Iteration 191/1000 | Loss: 0.00019335
Iteration 192/1000 | Loss: 0.00009268
Iteration 193/1000 | Loss: 0.00010672
Iteration 194/1000 | Loss: 0.00019612
Iteration 195/1000 | Loss: 0.00013881
Iteration 196/1000 | Loss: 0.00004556
Iteration 197/1000 | Loss: 0.00013780
Iteration 198/1000 | Loss: 0.00012389
Iteration 199/1000 | Loss: 0.00011069
Iteration 200/1000 | Loss: 0.00021611
Iteration 201/1000 | Loss: 0.00019368
Iteration 202/1000 | Loss: 0.00017794
Iteration 203/1000 | Loss: 0.00020461
Iteration 204/1000 | Loss: 0.00015991
Iteration 205/1000 | Loss: 0.00019404
Iteration 206/1000 | Loss: 0.00020458
Iteration 207/1000 | Loss: 0.00004819
Iteration 208/1000 | Loss: 0.00004533
Iteration 209/1000 | Loss: 0.00013360
Iteration 210/1000 | Loss: 0.00006174
Iteration 211/1000 | Loss: 0.00010642
Iteration 212/1000 | Loss: 0.00022279
Iteration 213/1000 | Loss: 0.00013403
Iteration 214/1000 | Loss: 0.00006375
Iteration 215/1000 | Loss: 0.00005584
Iteration 216/1000 | Loss: 0.00012992
Iteration 217/1000 | Loss: 0.00010717
Iteration 218/1000 | Loss: 0.00016324
Iteration 219/1000 | Loss: 0.00010061
Iteration 220/1000 | Loss: 0.00006833
Iteration 221/1000 | Loss: 0.00019102
Iteration 222/1000 | Loss: 0.00009510
Iteration 223/1000 | Loss: 0.00016749
Iteration 224/1000 | Loss: 0.00004883
Iteration 225/1000 | Loss: 0.00004492
Iteration 226/1000 | Loss: 0.00004337
Iteration 227/1000 | Loss: 0.00004255
Iteration 228/1000 | Loss: 0.00004226
Iteration 229/1000 | Loss: 0.00004190
Iteration 230/1000 | Loss: 0.00006376
Iteration 231/1000 | Loss: 0.00005262
Iteration 232/1000 | Loss: 0.00004140
Iteration 233/1000 | Loss: 0.00004139
Iteration 234/1000 | Loss: 0.00004138
Iteration 235/1000 | Loss: 0.00004138
Iteration 236/1000 | Loss: 0.00004138
Iteration 237/1000 | Loss: 0.00004138
Iteration 238/1000 | Loss: 0.00004137
Iteration 239/1000 | Loss: 0.00004137
Iteration 240/1000 | Loss: 0.00004137
Iteration 241/1000 | Loss: 0.00004137
Iteration 242/1000 | Loss: 0.00004137
Iteration 243/1000 | Loss: 0.00004137
Iteration 244/1000 | Loss: 0.00004133
Iteration 245/1000 | Loss: 0.00004129
Iteration 246/1000 | Loss: 0.00004127
Iteration 247/1000 | Loss: 0.00004127
Iteration 248/1000 | Loss: 0.00004127
Iteration 249/1000 | Loss: 0.00004127
Iteration 250/1000 | Loss: 0.00004127
Iteration 251/1000 | Loss: 0.00004126
Iteration 252/1000 | Loss: 0.00004126
Iteration 253/1000 | Loss: 0.00004126
Iteration 254/1000 | Loss: 0.00004126
Iteration 255/1000 | Loss: 0.00004126
Iteration 256/1000 | Loss: 0.00004126
Iteration 257/1000 | Loss: 0.00004126
Iteration 258/1000 | Loss: 0.00004126
Iteration 259/1000 | Loss: 0.00004126
Iteration 260/1000 | Loss: 0.00004126
Iteration 261/1000 | Loss: 0.00004124
Iteration 262/1000 | Loss: 0.00004123
Iteration 263/1000 | Loss: 0.00004123
Iteration 264/1000 | Loss: 0.00004123
Iteration 265/1000 | Loss: 0.00004123
Iteration 266/1000 | Loss: 0.00004122
Iteration 267/1000 | Loss: 0.00004122
Iteration 268/1000 | Loss: 0.00004122
Iteration 269/1000 | Loss: 0.00004121
Iteration 270/1000 | Loss: 0.00004121
Iteration 271/1000 | Loss: 0.00004121
Iteration 272/1000 | Loss: 0.00004121
Iteration 273/1000 | Loss: 0.00004121
Iteration 274/1000 | Loss: 0.00004121
Iteration 275/1000 | Loss: 0.00004120
Iteration 276/1000 | Loss: 0.00004120
Iteration 277/1000 | Loss: 0.00004120
Iteration 278/1000 | Loss: 0.00004120
Iteration 279/1000 | Loss: 0.00004120
Iteration 280/1000 | Loss: 0.00004120
Iteration 281/1000 | Loss: 0.00004120
Iteration 282/1000 | Loss: 0.00004120
Iteration 283/1000 | Loss: 0.00004120
Iteration 284/1000 | Loss: 0.00004120
Iteration 285/1000 | Loss: 0.00004120
Iteration 286/1000 | Loss: 0.00004120
Iteration 287/1000 | Loss: 0.00004120
Iteration 288/1000 | Loss: 0.00004120
Iteration 289/1000 | Loss: 0.00004120
Iteration 290/1000 | Loss: 0.00004119
Iteration 291/1000 | Loss: 0.00004119
Iteration 292/1000 | Loss: 0.00004119
Iteration 293/1000 | Loss: 0.00004119
Iteration 294/1000 | Loss: 0.00004119
Iteration 295/1000 | Loss: 0.00004119
Iteration 296/1000 | Loss: 0.00004118
Iteration 297/1000 | Loss: 0.00004118
Iteration 298/1000 | Loss: 0.00004118
Iteration 299/1000 | Loss: 0.00004118
Iteration 300/1000 | Loss: 0.00004118
Iteration 301/1000 | Loss: 0.00004118
Iteration 302/1000 | Loss: 0.00004118
Iteration 303/1000 | Loss: 0.00004117
Iteration 304/1000 | Loss: 0.00004117
Iteration 305/1000 | Loss: 0.00004117
Iteration 306/1000 | Loss: 0.00004117
Iteration 307/1000 | Loss: 0.00004117
Iteration 308/1000 | Loss: 0.00004116
Iteration 309/1000 | Loss: 0.00004116
Iteration 310/1000 | Loss: 0.00004116
Iteration 311/1000 | Loss: 0.00004116
Iteration 312/1000 | Loss: 0.00004116
Iteration 313/1000 | Loss: 0.00004116
Iteration 314/1000 | Loss: 0.00004116
Iteration 315/1000 | Loss: 0.00004116
Iteration 316/1000 | Loss: 0.00004116
Iteration 317/1000 | Loss: 0.00004116
Iteration 318/1000 | Loss: 0.00004116
Iteration 319/1000 | Loss: 0.00004116
Iteration 320/1000 | Loss: 0.00004115
Iteration 321/1000 | Loss: 0.00004115
Iteration 322/1000 | Loss: 0.00004115
Iteration 323/1000 | Loss: 0.00004115
Iteration 324/1000 | Loss: 0.00004115
Iteration 325/1000 | Loss: 0.00004115
Iteration 326/1000 | Loss: 0.00004115
Iteration 327/1000 | Loss: 0.00004115
Iteration 328/1000 | Loss: 0.00004114
Iteration 329/1000 | Loss: 0.00004114
Iteration 330/1000 | Loss: 0.00004114
Iteration 331/1000 | Loss: 0.00004114
Iteration 332/1000 | Loss: 0.00004114
Iteration 333/1000 | Loss: 0.00004114
Iteration 334/1000 | Loss: 0.00004113
Iteration 335/1000 | Loss: 0.00004113
Iteration 336/1000 | Loss: 0.00004113
Iteration 337/1000 | Loss: 0.00004113
Iteration 338/1000 | Loss: 0.00004112
Iteration 339/1000 | Loss: 0.00004111
Iteration 340/1000 | Loss: 0.00004111
Iteration 341/1000 | Loss: 0.00004111
Iteration 342/1000 | Loss: 0.00004111
Iteration 343/1000 | Loss: 0.00004111
Iteration 344/1000 | Loss: 0.00004111
Iteration 345/1000 | Loss: 0.00004110
Iteration 346/1000 | Loss: 0.00004110
Iteration 347/1000 | Loss: 0.00004110
Iteration 348/1000 | Loss: 0.00004110
Iteration 349/1000 | Loss: 0.00004110
Iteration 350/1000 | Loss: 0.00004110
Iteration 351/1000 | Loss: 0.00004110
Iteration 352/1000 | Loss: 0.00004110
Iteration 353/1000 | Loss: 0.00004110
Iteration 354/1000 | Loss: 0.00004109
Iteration 355/1000 | Loss: 0.00004109
Iteration 356/1000 | Loss: 0.00004109
Iteration 357/1000 | Loss: 0.00004109
Iteration 358/1000 | Loss: 0.00004109
Iteration 359/1000 | Loss: 0.00004109
Iteration 360/1000 | Loss: 0.00004109
Iteration 361/1000 | Loss: 0.00004109
Iteration 362/1000 | Loss: 0.00004109
Iteration 363/1000 | Loss: 0.00004109
Iteration 364/1000 | Loss: 0.00004109
Iteration 365/1000 | Loss: 0.00004108
Iteration 366/1000 | Loss: 0.00004108
Iteration 367/1000 | Loss: 0.00004108
Iteration 368/1000 | Loss: 0.00004108
Iteration 369/1000 | Loss: 0.00004108
Iteration 370/1000 | Loss: 0.00004108
Iteration 371/1000 | Loss: 0.00004108
Iteration 372/1000 | Loss: 0.00004108
Iteration 373/1000 | Loss: 0.00004108
Iteration 374/1000 | Loss: 0.00004108
Iteration 375/1000 | Loss: 0.00004108
Iteration 376/1000 | Loss: 0.00004108
Iteration 377/1000 | Loss: 0.00004108
Iteration 378/1000 | Loss: 0.00004107
Iteration 379/1000 | Loss: 0.00004107
Iteration 380/1000 | Loss: 0.00004107
Iteration 381/1000 | Loss: 0.00004107
Iteration 382/1000 | Loss: 0.00004107
Iteration 383/1000 | Loss: 0.00004107
Iteration 384/1000 | Loss: 0.00004107
Iteration 385/1000 | Loss: 0.00004107
Iteration 386/1000 | Loss: 0.00004107
Iteration 387/1000 | Loss: 0.00004107
Iteration 388/1000 | Loss: 0.00004106
Iteration 389/1000 | Loss: 0.00004106
Iteration 390/1000 | Loss: 0.00004106
Iteration 391/1000 | Loss: 0.00004106
Iteration 392/1000 | Loss: 0.00004106
Iteration 393/1000 | Loss: 0.00004106
Iteration 394/1000 | Loss: 0.00004106
Iteration 395/1000 | Loss: 0.00004106
Iteration 396/1000 | Loss: 0.00004106
Iteration 397/1000 | Loss: 0.00004106
Iteration 398/1000 | Loss: 0.00004106
Iteration 399/1000 | Loss: 0.00004106
Iteration 400/1000 | Loss: 0.00004106
Iteration 401/1000 | Loss: 0.00004106
Iteration 402/1000 | Loss: 0.00004106
Iteration 403/1000 | Loss: 0.00004105
Iteration 404/1000 | Loss: 0.00004105
Iteration 405/1000 | Loss: 0.00004105
Iteration 406/1000 | Loss: 0.00004105
Iteration 407/1000 | Loss: 0.00004105
Iteration 408/1000 | Loss: 0.00004105
Iteration 409/1000 | Loss: 0.00004105
Iteration 410/1000 | Loss: 0.00004105
Iteration 411/1000 | Loss: 0.00004105
Iteration 412/1000 | Loss: 0.00004105
Iteration 413/1000 | Loss: 0.00004105
Iteration 414/1000 | Loss: 0.00004105
Iteration 415/1000 | Loss: 0.00004105
Iteration 416/1000 | Loss: 0.00004105
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 416. Stopping optimization.
Last 5 losses: [4.105332845938392e-05, 4.105332845938392e-05, 4.105332845938392e-05, 4.105332845938392e-05, 4.105332845938392e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.105332845938392e-05

Optimization complete. Final v2v error: 4.8147292137146 mm

Highest mean error: 19.49286651611328 mm for frame 37

Lowest mean error: 3.7167563438415527 mm for frame 20

Saving results

Total time: 393.92135787010193
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_5533/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5533/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5533/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01042765
Iteration 2/25 | Loss: 0.00208921
Iteration 3/25 | Loss: 0.00161224
Iteration 4/25 | Loss: 0.00156561
Iteration 5/25 | Loss: 0.00158904
Iteration 6/25 | Loss: 0.00147910
Iteration 7/25 | Loss: 0.00133862
Iteration 8/25 | Loss: 0.00124442
Iteration 9/25 | Loss: 0.00118379
Iteration 10/25 | Loss: 0.00115859
Iteration 11/25 | Loss: 0.00114076
Iteration 12/25 | Loss: 0.00112845
Iteration 13/25 | Loss: 0.00112480
Iteration 14/25 | Loss: 0.00112403
Iteration 15/25 | Loss: 0.00112384
Iteration 16/25 | Loss: 0.00112378
Iteration 17/25 | Loss: 0.00112378
Iteration 18/25 | Loss: 0.00112378
Iteration 19/25 | Loss: 0.00112378
Iteration 20/25 | Loss: 0.00112378
Iteration 21/25 | Loss: 0.00112378
Iteration 22/25 | Loss: 0.00112378
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0011237799189984798, 0.0011237799189984798, 0.0011237799189984798, 0.0011237799189984798, 0.0011237799189984798]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011237799189984798

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18527770
Iteration 2/25 | Loss: 0.00220637
Iteration 3/25 | Loss: 0.00220637
Iteration 4/25 | Loss: 0.00220637
Iteration 5/25 | Loss: 0.00220637
Iteration 6/25 | Loss: 0.00220637
Iteration 7/25 | Loss: 0.00220637
Iteration 8/25 | Loss: 0.00220637
Iteration 9/25 | Loss: 0.00220637
Iteration 10/25 | Loss: 0.00220637
Iteration 11/25 | Loss: 0.00220637
Iteration 12/25 | Loss: 0.00220637
Iteration 13/25 | Loss: 0.00220637
Iteration 14/25 | Loss: 0.00220637
Iteration 15/25 | Loss: 0.00220637
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0022063704673200846, 0.0022063704673200846, 0.0022063704673200846, 0.0022063704673200846, 0.0022063704673200846]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022063704673200846

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00220637
Iteration 2/1000 | Loss: 0.00003563
Iteration 3/1000 | Loss: 0.00002343
Iteration 4/1000 | Loss: 0.00002159
Iteration 5/1000 | Loss: 0.00002064
Iteration 6/1000 | Loss: 0.00001999
Iteration 7/1000 | Loss: 0.00001948
Iteration 8/1000 | Loss: 0.00001903
Iteration 9/1000 | Loss: 0.00001874
Iteration 10/1000 | Loss: 0.00001871
Iteration 11/1000 | Loss: 0.00001846
Iteration 12/1000 | Loss: 0.00001838
Iteration 13/1000 | Loss: 0.00001838
Iteration 14/1000 | Loss: 0.00001829
Iteration 15/1000 | Loss: 0.00001828
Iteration 16/1000 | Loss: 0.00001825
Iteration 17/1000 | Loss: 0.00001825
Iteration 18/1000 | Loss: 0.00001824
Iteration 19/1000 | Loss: 0.00001824
Iteration 20/1000 | Loss: 0.00001824
Iteration 21/1000 | Loss: 0.00001823
Iteration 22/1000 | Loss: 0.00001823
Iteration 23/1000 | Loss: 0.00001823
Iteration 24/1000 | Loss: 0.00001823
Iteration 25/1000 | Loss: 0.00001823
Iteration 26/1000 | Loss: 0.00001823
Iteration 27/1000 | Loss: 0.00001822
Iteration 28/1000 | Loss: 0.00001822
Iteration 29/1000 | Loss: 0.00001822
Iteration 30/1000 | Loss: 0.00001822
Iteration 31/1000 | Loss: 0.00001822
Iteration 32/1000 | Loss: 0.00001821
Iteration 33/1000 | Loss: 0.00001821
Iteration 34/1000 | Loss: 0.00001821
Iteration 35/1000 | Loss: 0.00001820
Iteration 36/1000 | Loss: 0.00001820
Iteration 37/1000 | Loss: 0.00001820
Iteration 38/1000 | Loss: 0.00001819
Iteration 39/1000 | Loss: 0.00001819
Iteration 40/1000 | Loss: 0.00001818
Iteration 41/1000 | Loss: 0.00001818
Iteration 42/1000 | Loss: 0.00001818
Iteration 43/1000 | Loss: 0.00001818
Iteration 44/1000 | Loss: 0.00001817
Iteration 45/1000 | Loss: 0.00001817
Iteration 46/1000 | Loss: 0.00001816
Iteration 47/1000 | Loss: 0.00001816
Iteration 48/1000 | Loss: 0.00001816
Iteration 49/1000 | Loss: 0.00001816
Iteration 50/1000 | Loss: 0.00001815
Iteration 51/1000 | Loss: 0.00001815
Iteration 52/1000 | Loss: 0.00001815
Iteration 53/1000 | Loss: 0.00001815
Iteration 54/1000 | Loss: 0.00001814
Iteration 55/1000 | Loss: 0.00001814
Iteration 56/1000 | Loss: 0.00001814
Iteration 57/1000 | Loss: 0.00001814
Iteration 58/1000 | Loss: 0.00001814
Iteration 59/1000 | Loss: 0.00001814
Iteration 60/1000 | Loss: 0.00001814
Iteration 61/1000 | Loss: 0.00001814
Iteration 62/1000 | Loss: 0.00001814
Iteration 63/1000 | Loss: 0.00001814
Iteration 64/1000 | Loss: 0.00001814
Iteration 65/1000 | Loss: 0.00001814
Iteration 66/1000 | Loss: 0.00001814
Iteration 67/1000 | Loss: 0.00001814
Iteration 68/1000 | Loss: 0.00001814
Iteration 69/1000 | Loss: 0.00001814
Iteration 70/1000 | Loss: 0.00001814
Iteration 71/1000 | Loss: 0.00001814
Iteration 72/1000 | Loss: 0.00001814
Iteration 73/1000 | Loss: 0.00001814
Iteration 74/1000 | Loss: 0.00001814
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 74. Stopping optimization.
Last 5 losses: [1.8137778170057572e-05, 1.8137778170057572e-05, 1.8137778170057572e-05, 1.8137778170057572e-05, 1.8137778170057572e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8137778170057572e-05

Optimization complete. Final v2v error: 3.5689916610717773 mm

Highest mean error: 3.7798619270324707 mm for frame 135

Lowest mean error: 3.449965715408325 mm for frame 203

Saving results

Total time: 53.381380796432495
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_5533/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5533/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5533/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00770010
Iteration 2/25 | Loss: 0.00165784
Iteration 3/25 | Loss: 0.00120471
Iteration 4/25 | Loss: 0.00114659
Iteration 5/25 | Loss: 0.00114929
Iteration 6/25 | Loss: 0.00112804
Iteration 7/25 | Loss: 0.00112205
Iteration 8/25 | Loss: 0.00112007
Iteration 9/25 | Loss: 0.00111822
Iteration 10/25 | Loss: 0.00111735
Iteration 11/25 | Loss: 0.00111687
Iteration 12/25 | Loss: 0.00111622
Iteration 13/25 | Loss: 0.00111369
Iteration 14/25 | Loss: 0.00111274
Iteration 15/25 | Loss: 0.00111218
Iteration 16/25 | Loss: 0.00111213
Iteration 17/25 | Loss: 0.00111212
Iteration 18/25 | Loss: 0.00111212
Iteration 19/25 | Loss: 0.00111212
Iteration 20/25 | Loss: 0.00111212
Iteration 21/25 | Loss: 0.00111212
Iteration 22/25 | Loss: 0.00111212
Iteration 23/25 | Loss: 0.00111211
Iteration 24/25 | Loss: 0.00111211
Iteration 25/25 | Loss: 0.00111211

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.54717684
Iteration 2/25 | Loss: 0.00195560
Iteration 3/25 | Loss: 0.00195555
Iteration 4/25 | Loss: 0.00195554
Iteration 5/25 | Loss: 0.00195554
Iteration 6/25 | Loss: 0.00195554
Iteration 7/25 | Loss: 0.00195554
Iteration 8/25 | Loss: 0.00195554
Iteration 9/25 | Loss: 0.00195554
Iteration 10/25 | Loss: 0.00195554
Iteration 11/25 | Loss: 0.00195554
Iteration 12/25 | Loss: 0.00195554
Iteration 13/25 | Loss: 0.00195554
Iteration 14/25 | Loss: 0.00195554
Iteration 15/25 | Loss: 0.00195554
Iteration 16/25 | Loss: 0.00195554
Iteration 17/25 | Loss: 0.00195554
Iteration 18/25 | Loss: 0.00195554
Iteration 19/25 | Loss: 0.00195554
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0019555396866053343, 0.0019555396866053343, 0.0019555396866053343, 0.0019555396866053343, 0.0019555396866053343]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019555396866053343

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00195554
Iteration 2/1000 | Loss: 0.00009161
Iteration 3/1000 | Loss: 0.00004948
Iteration 4/1000 | Loss: 0.00009638
Iteration 5/1000 | Loss: 0.00005239
Iteration 6/1000 | Loss: 0.00003058
Iteration 7/1000 | Loss: 0.00051241
Iteration 8/1000 | Loss: 0.00013304
Iteration 9/1000 | Loss: 0.00019834
Iteration 10/1000 | Loss: 0.00021159
Iteration 11/1000 | Loss: 0.00011280
Iteration 12/1000 | Loss: 0.00005865
Iteration 13/1000 | Loss: 0.00009744
Iteration 14/1000 | Loss: 0.00002439
Iteration 15/1000 | Loss: 0.00003168
Iteration 16/1000 | Loss: 0.00002349
Iteration 17/1000 | Loss: 0.00002237
Iteration 18/1000 | Loss: 0.00002153
Iteration 19/1000 | Loss: 0.00002110
Iteration 20/1000 | Loss: 0.00002075
Iteration 21/1000 | Loss: 0.00002040
Iteration 22/1000 | Loss: 0.00002017
Iteration 23/1000 | Loss: 0.00002016
Iteration 24/1000 | Loss: 0.00002013
Iteration 25/1000 | Loss: 0.00002005
Iteration 26/1000 | Loss: 0.00002004
Iteration 27/1000 | Loss: 0.00002004
Iteration 28/1000 | Loss: 0.00002002
Iteration 29/1000 | Loss: 0.00002000
Iteration 30/1000 | Loss: 0.00001999
Iteration 31/1000 | Loss: 0.00001993
Iteration 32/1000 | Loss: 0.00001988
Iteration 33/1000 | Loss: 0.00001981
Iteration 34/1000 | Loss: 0.00001969
Iteration 35/1000 | Loss: 0.00012220
Iteration 36/1000 | Loss: 0.00002372
Iteration 37/1000 | Loss: 0.00002164
Iteration 38/1000 | Loss: 0.00001991
Iteration 39/1000 | Loss: 0.00001941
Iteration 40/1000 | Loss: 0.00001921
Iteration 41/1000 | Loss: 0.00001914
Iteration 42/1000 | Loss: 0.00001909
Iteration 43/1000 | Loss: 0.00001907
Iteration 44/1000 | Loss: 0.00001907
Iteration 45/1000 | Loss: 0.00001907
Iteration 46/1000 | Loss: 0.00001907
Iteration 47/1000 | Loss: 0.00001907
Iteration 48/1000 | Loss: 0.00001907
Iteration 49/1000 | Loss: 0.00001907
Iteration 50/1000 | Loss: 0.00001907
Iteration 51/1000 | Loss: 0.00001907
Iteration 52/1000 | Loss: 0.00001907
Iteration 53/1000 | Loss: 0.00001907
Iteration 54/1000 | Loss: 0.00001906
Iteration 55/1000 | Loss: 0.00001906
Iteration 56/1000 | Loss: 0.00001906
Iteration 57/1000 | Loss: 0.00001906
Iteration 58/1000 | Loss: 0.00001906
Iteration 59/1000 | Loss: 0.00001905
Iteration 60/1000 | Loss: 0.00001905
Iteration 61/1000 | Loss: 0.00001905
Iteration 62/1000 | Loss: 0.00001905
Iteration 63/1000 | Loss: 0.00001905
Iteration 64/1000 | Loss: 0.00001905
Iteration 65/1000 | Loss: 0.00001905
Iteration 66/1000 | Loss: 0.00001905
Iteration 67/1000 | Loss: 0.00001905
Iteration 68/1000 | Loss: 0.00001905
Iteration 69/1000 | Loss: 0.00001905
Iteration 70/1000 | Loss: 0.00001905
Iteration 71/1000 | Loss: 0.00001904
Iteration 72/1000 | Loss: 0.00001904
Iteration 73/1000 | Loss: 0.00001904
Iteration 74/1000 | Loss: 0.00001904
Iteration 75/1000 | Loss: 0.00001904
Iteration 76/1000 | Loss: 0.00001904
Iteration 77/1000 | Loss: 0.00001904
Iteration 78/1000 | Loss: 0.00001904
Iteration 79/1000 | Loss: 0.00001904
Iteration 80/1000 | Loss: 0.00001904
Iteration 81/1000 | Loss: 0.00001904
Iteration 82/1000 | Loss: 0.00001904
Iteration 83/1000 | Loss: 0.00001904
Iteration 84/1000 | Loss: 0.00001904
Iteration 85/1000 | Loss: 0.00001904
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 85. Stopping optimization.
Last 5 losses: [1.9040522602153942e-05, 1.9040522602153942e-05, 1.9040522602153942e-05, 1.9040522602153942e-05, 1.9040522602153942e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9040522602153942e-05

Optimization complete. Final v2v error: 3.4301071166992188 mm

Highest mean error: 14.460469245910645 mm for frame 211

Lowest mean error: 2.6537158489227295 mm for frame 229

Saving results

Total time: 84.42024397850037
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_5533/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5533/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5533/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01092517
Iteration 2/25 | Loss: 0.01092517
Iteration 3/25 | Loss: 0.01092516
Iteration 4/25 | Loss: 0.01092516
Iteration 5/25 | Loss: 0.00380439
Iteration 6/25 | Loss: 0.00289986
Iteration 7/25 | Loss: 0.00237279
Iteration 8/25 | Loss: 0.00190460
Iteration 9/25 | Loss: 0.00176238
Iteration 10/25 | Loss: 0.00173294
Iteration 11/25 | Loss: 0.00156207
Iteration 12/25 | Loss: 0.00145951
Iteration 13/25 | Loss: 0.00143064
Iteration 14/25 | Loss: 0.00141634
Iteration 15/25 | Loss: 0.00139532
Iteration 16/25 | Loss: 0.00137918
Iteration 17/25 | Loss: 0.00134076
Iteration 18/25 | Loss: 0.00131304
Iteration 19/25 | Loss: 0.00130365
Iteration 20/25 | Loss: 0.00130258
Iteration 21/25 | Loss: 0.00129033
Iteration 22/25 | Loss: 0.00129165
Iteration 23/25 | Loss: 0.00127392
Iteration 24/25 | Loss: 0.00126506
Iteration 25/25 | Loss: 0.00125730

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.70391041
Iteration 2/25 | Loss: 0.00282459
Iteration 3/25 | Loss: 0.00190231
Iteration 4/25 | Loss: 0.00190231
Iteration 5/25 | Loss: 0.00190231
Iteration 6/25 | Loss: 0.00190231
Iteration 7/25 | Loss: 0.00190231
Iteration 8/25 | Loss: 0.00190231
Iteration 9/25 | Loss: 0.00190231
Iteration 10/25 | Loss: 0.00190231
Iteration 11/25 | Loss: 0.00190231
Iteration 12/25 | Loss: 0.00190231
Iteration 13/25 | Loss: 0.00190231
Iteration 14/25 | Loss: 0.00190231
Iteration 15/25 | Loss: 0.00190231
Iteration 16/25 | Loss: 0.00190231
Iteration 17/25 | Loss: 0.00190231
Iteration 18/25 | Loss: 0.00190231
Iteration 19/25 | Loss: 0.00190231
Iteration 20/25 | Loss: 0.00190231
Iteration 21/25 | Loss: 0.00190231
Iteration 22/25 | Loss: 0.00190231
Iteration 23/25 | Loss: 0.00190231
Iteration 24/25 | Loss: 0.00190231
Iteration 25/25 | Loss: 0.00190231

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00190231
Iteration 2/1000 | Loss: 0.00016380
Iteration 3/1000 | Loss: 0.00008535
Iteration 4/1000 | Loss: 0.00005977
Iteration 5/1000 | Loss: 0.00043707
Iteration 6/1000 | Loss: 0.00059314
Iteration 7/1000 | Loss: 0.00060939
Iteration 8/1000 | Loss: 0.00005849
Iteration 9/1000 | Loss: 0.00007319
Iteration 10/1000 | Loss: 0.00006474
Iteration 11/1000 | Loss: 0.00005394
Iteration 12/1000 | Loss: 0.00007704
Iteration 13/1000 | Loss: 0.00005832
Iteration 14/1000 | Loss: 0.00006083
Iteration 15/1000 | Loss: 0.00004939
Iteration 16/1000 | Loss: 0.00005082
Iteration 17/1000 | Loss: 0.00005937
Iteration 18/1000 | Loss: 0.00007796
Iteration 19/1000 | Loss: 0.00038141
Iteration 20/1000 | Loss: 0.00044232
Iteration 21/1000 | Loss: 0.00047207
Iteration 22/1000 | Loss: 0.00006858
Iteration 23/1000 | Loss: 0.00006050
Iteration 24/1000 | Loss: 0.00006681
Iteration 25/1000 | Loss: 0.00006659
Iteration 26/1000 | Loss: 0.00007261
Iteration 27/1000 | Loss: 0.00004999
Iteration 28/1000 | Loss: 0.00007801
Iteration 29/1000 | Loss: 0.00006875
Iteration 30/1000 | Loss: 0.00007032
Iteration 31/1000 | Loss: 0.00006443
Iteration 32/1000 | Loss: 0.00005464
Iteration 33/1000 | Loss: 0.00005906
Iteration 34/1000 | Loss: 0.00005793
Iteration 35/1000 | Loss: 0.00005332
Iteration 36/1000 | Loss: 0.00006894
Iteration 37/1000 | Loss: 0.00006452
Iteration 38/1000 | Loss: 0.00007970
Iteration 39/1000 | Loss: 0.00007393
Iteration 40/1000 | Loss: 0.00008086
Iteration 41/1000 | Loss: 0.00007168
Iteration 42/1000 | Loss: 0.00006154
Iteration 43/1000 | Loss: 0.00007154
Iteration 44/1000 | Loss: 0.00007659
Iteration 45/1000 | Loss: 0.00032872
Iteration 46/1000 | Loss: 0.00020685
Iteration 47/1000 | Loss: 0.00006521
Iteration 48/1000 | Loss: 0.00005028
Iteration 49/1000 | Loss: 0.00005114
Iteration 50/1000 | Loss: 0.00004762
Iteration 51/1000 | Loss: 0.00012741
Iteration 52/1000 | Loss: 0.00007172
Iteration 53/1000 | Loss: 0.00012759
Iteration 54/1000 | Loss: 0.00006423
Iteration 55/1000 | Loss: 0.00012077
Iteration 56/1000 | Loss: 0.00027519
Iteration 57/1000 | Loss: 0.00021742
Iteration 58/1000 | Loss: 0.00005471
Iteration 59/1000 | Loss: 0.00004979
Iteration 60/1000 | Loss: 0.00004616
Iteration 61/1000 | Loss: 0.00030430
Iteration 62/1000 | Loss: 0.00007011
Iteration 63/1000 | Loss: 0.00006006
Iteration 64/1000 | Loss: 0.00005744
Iteration 65/1000 | Loss: 0.00004509
Iteration 66/1000 | Loss: 0.00005484
Iteration 67/1000 | Loss: 0.00005073
Iteration 68/1000 | Loss: 0.00018471
Iteration 69/1000 | Loss: 0.00030310
Iteration 70/1000 | Loss: 0.00004984
Iteration 71/1000 | Loss: 0.00005745
Iteration 72/1000 | Loss: 0.00003959
Iteration 73/1000 | Loss: 0.00003604
Iteration 74/1000 | Loss: 0.00003431
Iteration 75/1000 | Loss: 0.00003330
Iteration 76/1000 | Loss: 0.00003244
Iteration 77/1000 | Loss: 0.00003201
Iteration 78/1000 | Loss: 0.00020290
Iteration 79/1000 | Loss: 0.00013363
Iteration 80/1000 | Loss: 0.00003549
Iteration 81/1000 | Loss: 0.00003292
Iteration 82/1000 | Loss: 0.00003154
Iteration 83/1000 | Loss: 0.00016683
Iteration 84/1000 | Loss: 0.00035817
Iteration 85/1000 | Loss: 0.00021650
Iteration 86/1000 | Loss: 0.00003988
Iteration 87/1000 | Loss: 0.00003493
Iteration 88/1000 | Loss: 0.00003290
Iteration 89/1000 | Loss: 0.00003124
Iteration 90/1000 | Loss: 0.00003056
Iteration 91/1000 | Loss: 0.00003003
Iteration 92/1000 | Loss: 0.00002966
Iteration 93/1000 | Loss: 0.00002939
Iteration 94/1000 | Loss: 0.00002926
Iteration 95/1000 | Loss: 0.00002919
Iteration 96/1000 | Loss: 0.00002912
Iteration 97/1000 | Loss: 0.00002909
Iteration 98/1000 | Loss: 0.00002908
Iteration 99/1000 | Loss: 0.00002908
Iteration 100/1000 | Loss: 0.00002907
Iteration 101/1000 | Loss: 0.00002907
Iteration 102/1000 | Loss: 0.00002907
Iteration 103/1000 | Loss: 0.00002907
Iteration 104/1000 | Loss: 0.00002907
Iteration 105/1000 | Loss: 0.00002907
Iteration 106/1000 | Loss: 0.00002907
Iteration 107/1000 | Loss: 0.00002907
Iteration 108/1000 | Loss: 0.00002907
Iteration 109/1000 | Loss: 0.00002907
Iteration 110/1000 | Loss: 0.00002907
Iteration 111/1000 | Loss: 0.00002907
Iteration 112/1000 | Loss: 0.00002906
Iteration 113/1000 | Loss: 0.00002906
Iteration 114/1000 | Loss: 0.00002906
Iteration 115/1000 | Loss: 0.00002906
Iteration 116/1000 | Loss: 0.00002905
Iteration 117/1000 | Loss: 0.00002904
Iteration 118/1000 | Loss: 0.00002902
Iteration 119/1000 | Loss: 0.00002901
Iteration 120/1000 | Loss: 0.00002901
Iteration 121/1000 | Loss: 0.00002900
Iteration 122/1000 | Loss: 0.00002899
Iteration 123/1000 | Loss: 0.00002899
Iteration 124/1000 | Loss: 0.00002899
Iteration 125/1000 | Loss: 0.00002899
Iteration 126/1000 | Loss: 0.00002899
Iteration 127/1000 | Loss: 0.00002899
Iteration 128/1000 | Loss: 0.00002898
Iteration 129/1000 | Loss: 0.00002898
Iteration 130/1000 | Loss: 0.00002897
Iteration 131/1000 | Loss: 0.00002897
Iteration 132/1000 | Loss: 0.00002897
Iteration 133/1000 | Loss: 0.00002897
Iteration 134/1000 | Loss: 0.00002897
Iteration 135/1000 | Loss: 0.00002897
Iteration 136/1000 | Loss: 0.00002897
Iteration 137/1000 | Loss: 0.00002897
Iteration 138/1000 | Loss: 0.00002897
Iteration 139/1000 | Loss: 0.00002897
Iteration 140/1000 | Loss: 0.00002897
Iteration 141/1000 | Loss: 0.00002897
Iteration 142/1000 | Loss: 0.00002897
Iteration 143/1000 | Loss: 0.00002897
Iteration 144/1000 | Loss: 0.00002897
Iteration 145/1000 | Loss: 0.00002896
Iteration 146/1000 | Loss: 0.00002896
Iteration 147/1000 | Loss: 0.00002896
Iteration 148/1000 | Loss: 0.00002896
Iteration 149/1000 | Loss: 0.00002896
Iteration 150/1000 | Loss: 0.00002896
Iteration 151/1000 | Loss: 0.00002896
Iteration 152/1000 | Loss: 0.00002895
Iteration 153/1000 | Loss: 0.00002895
Iteration 154/1000 | Loss: 0.00002895
Iteration 155/1000 | Loss: 0.00002895
Iteration 156/1000 | Loss: 0.00002895
Iteration 157/1000 | Loss: 0.00002895
Iteration 158/1000 | Loss: 0.00002894
Iteration 159/1000 | Loss: 0.00002894
Iteration 160/1000 | Loss: 0.00002894
Iteration 161/1000 | Loss: 0.00002894
Iteration 162/1000 | Loss: 0.00002894
Iteration 163/1000 | Loss: 0.00002894
Iteration 164/1000 | Loss: 0.00002893
Iteration 165/1000 | Loss: 0.00002893
Iteration 166/1000 | Loss: 0.00002893
Iteration 167/1000 | Loss: 0.00002893
Iteration 168/1000 | Loss: 0.00002892
Iteration 169/1000 | Loss: 0.00002891
Iteration 170/1000 | Loss: 0.00002891
Iteration 171/1000 | Loss: 0.00002891
Iteration 172/1000 | Loss: 0.00002890
Iteration 173/1000 | Loss: 0.00002890
Iteration 174/1000 | Loss: 0.00002889
Iteration 175/1000 | Loss: 0.00002888
Iteration 176/1000 | Loss: 0.00002888
Iteration 177/1000 | Loss: 0.00002888
Iteration 178/1000 | Loss: 0.00002888
Iteration 179/1000 | Loss: 0.00002887
Iteration 180/1000 | Loss: 0.00002887
Iteration 181/1000 | Loss: 0.00002887
Iteration 182/1000 | Loss: 0.00002887
Iteration 183/1000 | Loss: 0.00002887
Iteration 184/1000 | Loss: 0.00002886
Iteration 185/1000 | Loss: 0.00002886
Iteration 186/1000 | Loss: 0.00002886
Iteration 187/1000 | Loss: 0.00002886
Iteration 188/1000 | Loss: 0.00002886
Iteration 189/1000 | Loss: 0.00002885
Iteration 190/1000 | Loss: 0.00002885
Iteration 191/1000 | Loss: 0.00002885
Iteration 192/1000 | Loss: 0.00002885
Iteration 193/1000 | Loss: 0.00002885
Iteration 194/1000 | Loss: 0.00002885
Iteration 195/1000 | Loss: 0.00002885
Iteration 196/1000 | Loss: 0.00002885
Iteration 197/1000 | Loss: 0.00002885
Iteration 198/1000 | Loss: 0.00002885
Iteration 199/1000 | Loss: 0.00002885
Iteration 200/1000 | Loss: 0.00002885
Iteration 201/1000 | Loss: 0.00002885
Iteration 202/1000 | Loss: 0.00002885
Iteration 203/1000 | Loss: 0.00002885
Iteration 204/1000 | Loss: 0.00002885
Iteration 205/1000 | Loss: 0.00002885
Iteration 206/1000 | Loss: 0.00002885
Iteration 207/1000 | Loss: 0.00002885
Iteration 208/1000 | Loss: 0.00002885
Iteration 209/1000 | Loss: 0.00002885
Iteration 210/1000 | Loss: 0.00002885
Iteration 211/1000 | Loss: 0.00002885
Iteration 212/1000 | Loss: 0.00002885
Iteration 213/1000 | Loss: 0.00002885
Iteration 214/1000 | Loss: 0.00002885
Iteration 215/1000 | Loss: 0.00002885
Iteration 216/1000 | Loss: 0.00002885
Iteration 217/1000 | Loss: 0.00002885
Iteration 218/1000 | Loss: 0.00002885
Iteration 219/1000 | Loss: 0.00002885
Iteration 220/1000 | Loss: 0.00002885
Iteration 221/1000 | Loss: 0.00002885
Iteration 222/1000 | Loss: 0.00002885
Iteration 223/1000 | Loss: 0.00002885
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [2.8848147849203087e-05, 2.8848147849203087e-05, 2.8848147849203087e-05, 2.8848147849203087e-05, 2.8848147849203087e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8848147849203087e-05

Optimization complete. Final v2v error: 4.00406551361084 mm

Highest mean error: 20.576641082763672 mm for frame 191

Lowest mean error: 2.879291296005249 mm for frame 78

Saving results

Total time: 208.23339414596558
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_5533/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5533/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5533/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01082694
Iteration 2/25 | Loss: 0.01082694
Iteration 3/25 | Loss: 0.01082694
Iteration 4/25 | Loss: 0.00210246
Iteration 5/25 | Loss: 0.00164572
Iteration 6/25 | Loss: 0.00151381
Iteration 7/25 | Loss: 0.00131649
Iteration 8/25 | Loss: 0.00126789
Iteration 9/25 | Loss: 0.00122448
Iteration 10/25 | Loss: 0.00120852
Iteration 11/25 | Loss: 0.00115901
Iteration 12/25 | Loss: 0.00114049
Iteration 13/25 | Loss: 0.00112646
Iteration 14/25 | Loss: 0.00112125
Iteration 15/25 | Loss: 0.00111263
Iteration 16/25 | Loss: 0.00111059
Iteration 17/25 | Loss: 0.00111073
Iteration 18/25 | Loss: 0.00110927
Iteration 19/25 | Loss: 0.00110935
Iteration 20/25 | Loss: 0.00110878
Iteration 21/25 | Loss: 0.00110744
Iteration 22/25 | Loss: 0.00110741
Iteration 23/25 | Loss: 0.00110726
Iteration 24/25 | Loss: 0.00110854
Iteration 25/25 | Loss: 0.00110775

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28267515
Iteration 2/25 | Loss: 0.00226268
Iteration 3/25 | Loss: 0.00226267
Iteration 4/25 | Loss: 0.00226267
Iteration 5/25 | Loss: 0.00226267
Iteration 6/25 | Loss: 0.00226267
Iteration 7/25 | Loss: 0.00226267
Iteration 8/25 | Loss: 0.00226267
Iteration 9/25 | Loss: 0.00226267
Iteration 10/25 | Loss: 0.00226267
Iteration 11/25 | Loss: 0.00226267
Iteration 12/25 | Loss: 0.00226267
Iteration 13/25 | Loss: 0.00226267
Iteration 14/25 | Loss: 0.00226267
Iteration 15/25 | Loss: 0.00226267
Iteration 16/25 | Loss: 0.00226267
Iteration 17/25 | Loss: 0.00226267
Iteration 18/25 | Loss: 0.00226267
Iteration 19/25 | Loss: 0.00226267
Iteration 20/25 | Loss: 0.00226267
Iteration 21/25 | Loss: 0.00226267
Iteration 22/25 | Loss: 0.00226267
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0022626714780926704, 0.0022626714780926704, 0.0022626714780926704, 0.0022626714780926704, 0.0022626714780926704]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022626714780926704

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00226267
Iteration 2/1000 | Loss: 0.00041427
Iteration 3/1000 | Loss: 0.00040583
Iteration 4/1000 | Loss: 0.00055822
Iteration 5/1000 | Loss: 0.00033238
Iteration 6/1000 | Loss: 0.00028389
Iteration 7/1000 | Loss: 0.00033019
Iteration 8/1000 | Loss: 0.00064079
Iteration 9/1000 | Loss: 0.00048524
Iteration 10/1000 | Loss: 0.00035122
Iteration 11/1000 | Loss: 0.00024135
Iteration 12/1000 | Loss: 0.00044539
Iteration 13/1000 | Loss: 0.00028620
Iteration 14/1000 | Loss: 0.00066266
Iteration 15/1000 | Loss: 0.00034566
Iteration 16/1000 | Loss: 0.00038264
Iteration 17/1000 | Loss: 0.00034802
Iteration 18/1000 | Loss: 0.00034227
Iteration 19/1000 | Loss: 0.00038936
Iteration 20/1000 | Loss: 0.00041211
Iteration 21/1000 | Loss: 0.00016206
Iteration 22/1000 | Loss: 0.00031509
Iteration 23/1000 | Loss: 0.00025569
Iteration 24/1000 | Loss: 0.00029619
Iteration 25/1000 | Loss: 0.00040585
Iteration 26/1000 | Loss: 0.00035399
Iteration 27/1000 | Loss: 0.00025894
Iteration 28/1000 | Loss: 0.00031006
Iteration 29/1000 | Loss: 0.00030561
Iteration 30/1000 | Loss: 0.00020557
Iteration 31/1000 | Loss: 0.00028695
Iteration 32/1000 | Loss: 0.00032269
Iteration 33/1000 | Loss: 0.00040027
Iteration 34/1000 | Loss: 0.00041329
Iteration 35/1000 | Loss: 0.00040688
Iteration 36/1000 | Loss: 0.00034957
Iteration 37/1000 | Loss: 0.00035349
Iteration 38/1000 | Loss: 0.00027493
Iteration 39/1000 | Loss: 0.00030451
Iteration 40/1000 | Loss: 0.00023077
Iteration 41/1000 | Loss: 0.00026815
Iteration 42/1000 | Loss: 0.00027025
Iteration 43/1000 | Loss: 0.00023729
Iteration 44/1000 | Loss: 0.00024634
Iteration 45/1000 | Loss: 0.00023670
Iteration 46/1000 | Loss: 0.00027778
Iteration 47/1000 | Loss: 0.00033020
Iteration 48/1000 | Loss: 0.00017491
Iteration 49/1000 | Loss: 0.00021260
Iteration 50/1000 | Loss: 0.00016249
Iteration 51/1000 | Loss: 0.00031127
Iteration 52/1000 | Loss: 0.00029565
Iteration 53/1000 | Loss: 0.00029342
Iteration 54/1000 | Loss: 0.00020960
Iteration 55/1000 | Loss: 0.00029164
Iteration 56/1000 | Loss: 0.00017749
Iteration 57/1000 | Loss: 0.00022866
Iteration 58/1000 | Loss: 0.00019371
Iteration 59/1000 | Loss: 0.00019366
Iteration 60/1000 | Loss: 0.00025622
Iteration 61/1000 | Loss: 0.00027759
Iteration 62/1000 | Loss: 0.00027598
Iteration 63/1000 | Loss: 0.00028698
Iteration 64/1000 | Loss: 0.00029136
Iteration 65/1000 | Loss: 0.00031325
Iteration 66/1000 | Loss: 0.00038689
Iteration 67/1000 | Loss: 0.00025636
Iteration 68/1000 | Loss: 0.00008664
Iteration 69/1000 | Loss: 0.00034459
Iteration 70/1000 | Loss: 0.00015675
Iteration 71/1000 | Loss: 0.00012466
Iteration 72/1000 | Loss: 0.00020354
Iteration 73/1000 | Loss: 0.00014544
Iteration 74/1000 | Loss: 0.00011604
Iteration 75/1000 | Loss: 0.00018292
Iteration 76/1000 | Loss: 0.00019296
Iteration 77/1000 | Loss: 0.00019339
Iteration 78/1000 | Loss: 0.00018594
Iteration 79/1000 | Loss: 0.00051355
Iteration 80/1000 | Loss: 0.00039645
Iteration 81/1000 | Loss: 0.00049791
Iteration 82/1000 | Loss: 0.00029815
Iteration 83/1000 | Loss: 0.00011215
Iteration 84/1000 | Loss: 0.00020343
Iteration 85/1000 | Loss: 0.00022097
Iteration 86/1000 | Loss: 0.00014922
Iteration 87/1000 | Loss: 0.00015849
Iteration 88/1000 | Loss: 0.00017456
Iteration 89/1000 | Loss: 0.00019817
Iteration 90/1000 | Loss: 0.00033911
Iteration 91/1000 | Loss: 0.00012486
Iteration 92/1000 | Loss: 0.00015026
Iteration 93/1000 | Loss: 0.00011458
Iteration 94/1000 | Loss: 0.00010448
Iteration 95/1000 | Loss: 0.00028385
Iteration 96/1000 | Loss: 0.00011644
Iteration 97/1000 | Loss: 0.00007339
Iteration 98/1000 | Loss: 0.00008274
Iteration 99/1000 | Loss: 0.00004621
Iteration 100/1000 | Loss: 0.00004472
Iteration 101/1000 | Loss: 0.00011366
Iteration 102/1000 | Loss: 0.00004826
Iteration 103/1000 | Loss: 0.00004529
Iteration 104/1000 | Loss: 0.00005744
Iteration 105/1000 | Loss: 0.00012414
Iteration 106/1000 | Loss: 0.00019733
Iteration 107/1000 | Loss: 0.00060182
Iteration 108/1000 | Loss: 0.00056451
Iteration 109/1000 | Loss: 0.00034065
Iteration 110/1000 | Loss: 0.00005438
Iteration 111/1000 | Loss: 0.00029308
Iteration 112/1000 | Loss: 0.00010600
Iteration 113/1000 | Loss: 0.00014247
Iteration 114/1000 | Loss: 0.00010227
Iteration 115/1000 | Loss: 0.00019245
Iteration 116/1000 | Loss: 0.00016986
Iteration 117/1000 | Loss: 0.00017946
Iteration 118/1000 | Loss: 0.00016882
Iteration 119/1000 | Loss: 0.00068470
Iteration 120/1000 | Loss: 0.00025059
Iteration 121/1000 | Loss: 0.00018961
Iteration 122/1000 | Loss: 0.00010075
Iteration 123/1000 | Loss: 0.00014182
Iteration 124/1000 | Loss: 0.00014266
Iteration 125/1000 | Loss: 0.00014944
Iteration 126/1000 | Loss: 0.00016796
Iteration 127/1000 | Loss: 0.00017206
Iteration 128/1000 | Loss: 0.00016659
Iteration 129/1000 | Loss: 0.00019533
Iteration 130/1000 | Loss: 0.00018944
Iteration 131/1000 | Loss: 0.00018812
Iteration 132/1000 | Loss: 0.00019012
Iteration 133/1000 | Loss: 0.00013906
Iteration 134/1000 | Loss: 0.00025113
Iteration 135/1000 | Loss: 0.00020885
Iteration 136/1000 | Loss: 0.00020184
Iteration 137/1000 | Loss: 0.00020964
Iteration 138/1000 | Loss: 0.00020555
Iteration 139/1000 | Loss: 0.00022546
Iteration 140/1000 | Loss: 0.00019176
Iteration 141/1000 | Loss: 0.00009621
Iteration 142/1000 | Loss: 0.00023762
Iteration 143/1000 | Loss: 0.00013638
Iteration 144/1000 | Loss: 0.00055994
Iteration 145/1000 | Loss: 0.00024172
Iteration 146/1000 | Loss: 0.00012181
Iteration 147/1000 | Loss: 0.00013272
Iteration 148/1000 | Loss: 0.00020164
Iteration 149/1000 | Loss: 0.00024684
Iteration 150/1000 | Loss: 0.00006539
Iteration 151/1000 | Loss: 0.00014127
Iteration 152/1000 | Loss: 0.00019022
Iteration 153/1000 | Loss: 0.00058994
Iteration 154/1000 | Loss: 0.00025147
Iteration 155/1000 | Loss: 0.00016071
Iteration 156/1000 | Loss: 0.00014115
Iteration 157/1000 | Loss: 0.00013459
Iteration 158/1000 | Loss: 0.00018866
Iteration 159/1000 | Loss: 0.00017805
Iteration 160/1000 | Loss: 0.00007477
Iteration 161/1000 | Loss: 0.00015323
Iteration 162/1000 | Loss: 0.00011890
Iteration 163/1000 | Loss: 0.00005597
Iteration 164/1000 | Loss: 0.00008595
Iteration 165/1000 | Loss: 0.00005948
Iteration 166/1000 | Loss: 0.00012484
Iteration 167/1000 | Loss: 0.00044541
Iteration 168/1000 | Loss: 0.00025007
Iteration 169/1000 | Loss: 0.00008384
Iteration 170/1000 | Loss: 0.00014067
Iteration 171/1000 | Loss: 0.00016208
Iteration 172/1000 | Loss: 0.00014874
Iteration 173/1000 | Loss: 0.00014788
Iteration 174/1000 | Loss: 0.00013030
Iteration 175/1000 | Loss: 0.00013593
Iteration 176/1000 | Loss: 0.00045992
Iteration 177/1000 | Loss: 0.00015953
Iteration 178/1000 | Loss: 0.00023511
Iteration 179/1000 | Loss: 0.00018800
Iteration 180/1000 | Loss: 0.00017041
Iteration 181/1000 | Loss: 0.00011635
Iteration 182/1000 | Loss: 0.00025829
Iteration 183/1000 | Loss: 0.00016374
Iteration 184/1000 | Loss: 0.00010369
Iteration 185/1000 | Loss: 0.00014826
Iteration 186/1000 | Loss: 0.00013015
Iteration 187/1000 | Loss: 0.00012587
Iteration 188/1000 | Loss: 0.00006884
Iteration 189/1000 | Loss: 0.00021228
Iteration 190/1000 | Loss: 0.00008165
Iteration 191/1000 | Loss: 0.00019954
Iteration 192/1000 | Loss: 0.00013545
Iteration 193/1000 | Loss: 0.00010652
Iteration 194/1000 | Loss: 0.00013637
Iteration 195/1000 | Loss: 0.00011265
Iteration 196/1000 | Loss: 0.00015035
Iteration 197/1000 | Loss: 0.00012353
Iteration 198/1000 | Loss: 0.00013482
Iteration 199/1000 | Loss: 0.00009321
Iteration 200/1000 | Loss: 0.00011223
Iteration 201/1000 | Loss: 0.00009835
Iteration 202/1000 | Loss: 0.00015069
Iteration 203/1000 | Loss: 0.00013932
Iteration 204/1000 | Loss: 0.00015138
Iteration 205/1000 | Loss: 0.00012854
Iteration 206/1000 | Loss: 0.00015250
Iteration 207/1000 | Loss: 0.00014463
Iteration 208/1000 | Loss: 0.00018882
Iteration 209/1000 | Loss: 0.00009588
Iteration 210/1000 | Loss: 0.00004720
Iteration 211/1000 | Loss: 0.00047147
Iteration 212/1000 | Loss: 0.00026665
Iteration 213/1000 | Loss: 0.00035649
Iteration 214/1000 | Loss: 0.00008831
Iteration 215/1000 | Loss: 0.00014519
Iteration 216/1000 | Loss: 0.00059056
Iteration 217/1000 | Loss: 0.00038072
Iteration 218/1000 | Loss: 0.00032585
Iteration 219/1000 | Loss: 0.00005083
Iteration 220/1000 | Loss: 0.00012608
Iteration 221/1000 | Loss: 0.00016565
Iteration 222/1000 | Loss: 0.00023047
Iteration 223/1000 | Loss: 0.00013255
Iteration 224/1000 | Loss: 0.00013755
Iteration 225/1000 | Loss: 0.00011253
Iteration 226/1000 | Loss: 0.00013231
Iteration 227/1000 | Loss: 0.00012344
Iteration 228/1000 | Loss: 0.00012924
Iteration 229/1000 | Loss: 0.00013084
Iteration 230/1000 | Loss: 0.00026868
Iteration 231/1000 | Loss: 0.00091620
Iteration 232/1000 | Loss: 0.00051516
Iteration 233/1000 | Loss: 0.00006930
Iteration 234/1000 | Loss: 0.00004398
Iteration 235/1000 | Loss: 0.00045138
Iteration 236/1000 | Loss: 0.00021422
Iteration 237/1000 | Loss: 0.00076288
Iteration 238/1000 | Loss: 0.00050844
Iteration 239/1000 | Loss: 0.00012744
Iteration 240/1000 | Loss: 0.00010275
Iteration 241/1000 | Loss: 0.00004328
Iteration 242/1000 | Loss: 0.00003953
Iteration 243/1000 | Loss: 0.00043874
Iteration 244/1000 | Loss: 0.00021200
Iteration 245/1000 | Loss: 0.00029559
Iteration 246/1000 | Loss: 0.00003581
Iteration 247/1000 | Loss: 0.00014585
Iteration 248/1000 | Loss: 0.00009149
Iteration 249/1000 | Loss: 0.00019368
Iteration 250/1000 | Loss: 0.00004477
Iteration 251/1000 | Loss: 0.00003214
Iteration 252/1000 | Loss: 0.00003062
Iteration 253/1000 | Loss: 0.00002823
Iteration 254/1000 | Loss: 0.00002676
Iteration 255/1000 | Loss: 0.00002618
Iteration 256/1000 | Loss: 0.00002559
Iteration 257/1000 | Loss: 0.00002528
Iteration 258/1000 | Loss: 0.00002506
Iteration 259/1000 | Loss: 0.00002503
Iteration 260/1000 | Loss: 0.00002486
Iteration 261/1000 | Loss: 0.00002483
Iteration 262/1000 | Loss: 0.00002480
Iteration 263/1000 | Loss: 0.00002478
Iteration 264/1000 | Loss: 0.00002477
Iteration 265/1000 | Loss: 0.00002475
Iteration 266/1000 | Loss: 0.00002474
Iteration 267/1000 | Loss: 0.00002474
Iteration 268/1000 | Loss: 0.00002473
Iteration 269/1000 | Loss: 0.00002472
Iteration 270/1000 | Loss: 0.00002472
Iteration 271/1000 | Loss: 0.00002472
Iteration 272/1000 | Loss: 0.00002472
Iteration 273/1000 | Loss: 0.00002471
Iteration 274/1000 | Loss: 0.00002471
Iteration 275/1000 | Loss: 0.00002471
Iteration 276/1000 | Loss: 0.00002470
Iteration 277/1000 | Loss: 0.00002470
Iteration 278/1000 | Loss: 0.00002469
Iteration 279/1000 | Loss: 0.00002464
Iteration 280/1000 | Loss: 0.00002463
Iteration 281/1000 | Loss: 0.00002463
Iteration 282/1000 | Loss: 0.00002463
Iteration 283/1000 | Loss: 0.00002462
Iteration 284/1000 | Loss: 0.00002462
Iteration 285/1000 | Loss: 0.00002462
Iteration 286/1000 | Loss: 0.00002461
Iteration 287/1000 | Loss: 0.00002461
Iteration 288/1000 | Loss: 0.00002460
Iteration 289/1000 | Loss: 0.00002460
Iteration 290/1000 | Loss: 0.00002460
Iteration 291/1000 | Loss: 0.00002460
Iteration 292/1000 | Loss: 0.00002459
Iteration 293/1000 | Loss: 0.00002459
Iteration 294/1000 | Loss: 0.00002458
Iteration 295/1000 | Loss: 0.00002458
Iteration 296/1000 | Loss: 0.00002458
Iteration 297/1000 | Loss: 0.00002457
Iteration 298/1000 | Loss: 0.00002457
Iteration 299/1000 | Loss: 0.00002457
Iteration 300/1000 | Loss: 0.00002457
Iteration 301/1000 | Loss: 0.00002456
Iteration 302/1000 | Loss: 0.00002456
Iteration 303/1000 | Loss: 0.00002455
Iteration 304/1000 | Loss: 0.00002455
Iteration 305/1000 | Loss: 0.00002455
Iteration 306/1000 | Loss: 0.00002455
Iteration 307/1000 | Loss: 0.00002454
Iteration 308/1000 | Loss: 0.00002454
Iteration 309/1000 | Loss: 0.00002454
Iteration 310/1000 | Loss: 0.00002454
Iteration 311/1000 | Loss: 0.00002454
Iteration 312/1000 | Loss: 0.00002454
Iteration 313/1000 | Loss: 0.00002454
Iteration 314/1000 | Loss: 0.00002454
Iteration 315/1000 | Loss: 0.00002454
Iteration 316/1000 | Loss: 0.00002453
Iteration 317/1000 | Loss: 0.00002453
Iteration 318/1000 | Loss: 0.00002452
Iteration 319/1000 | Loss: 0.00002452
Iteration 320/1000 | Loss: 0.00002452
Iteration 321/1000 | Loss: 0.00002452
Iteration 322/1000 | Loss: 0.00002451
Iteration 323/1000 | Loss: 0.00002451
Iteration 324/1000 | Loss: 0.00002451
Iteration 325/1000 | Loss: 0.00002451
Iteration 326/1000 | Loss: 0.00002450
Iteration 327/1000 | Loss: 0.00002450
Iteration 328/1000 | Loss: 0.00002450
Iteration 329/1000 | Loss: 0.00002449
Iteration 330/1000 | Loss: 0.00002449
Iteration 331/1000 | Loss: 0.00002448
Iteration 332/1000 | Loss: 0.00002448
Iteration 333/1000 | Loss: 0.00002447
Iteration 334/1000 | Loss: 0.00002447
Iteration 335/1000 | Loss: 0.00002447
Iteration 336/1000 | Loss: 0.00002447
Iteration 337/1000 | Loss: 0.00002447
Iteration 338/1000 | Loss: 0.00002447
Iteration 339/1000 | Loss: 0.00002447
Iteration 340/1000 | Loss: 0.00002447
Iteration 341/1000 | Loss: 0.00002446
Iteration 342/1000 | Loss: 0.00002446
Iteration 343/1000 | Loss: 0.00002445
Iteration 344/1000 | Loss: 0.00002445
Iteration 345/1000 | Loss: 0.00002444
Iteration 346/1000 | Loss: 0.00002444
Iteration 347/1000 | Loss: 0.00002444
Iteration 348/1000 | Loss: 0.00002443
Iteration 349/1000 | Loss: 0.00002443
Iteration 350/1000 | Loss: 0.00002443
Iteration 351/1000 | Loss: 0.00002443
Iteration 352/1000 | Loss: 0.00002443
Iteration 353/1000 | Loss: 0.00002443
Iteration 354/1000 | Loss: 0.00002443
Iteration 355/1000 | Loss: 0.00002443
Iteration 356/1000 | Loss: 0.00002443
Iteration 357/1000 | Loss: 0.00002443
Iteration 358/1000 | Loss: 0.00002443
Iteration 359/1000 | Loss: 0.00002443
Iteration 360/1000 | Loss: 0.00002443
Iteration 361/1000 | Loss: 0.00002443
Iteration 362/1000 | Loss: 0.00002443
Iteration 363/1000 | Loss: 0.00002442
Iteration 364/1000 | Loss: 0.00002442
Iteration 365/1000 | Loss: 0.00002442
Iteration 366/1000 | Loss: 0.00002442
Iteration 367/1000 | Loss: 0.00002442
Iteration 368/1000 | Loss: 0.00002442
Iteration 369/1000 | Loss: 0.00002442
Iteration 370/1000 | Loss: 0.00002442
Iteration 371/1000 | Loss: 0.00002442
Iteration 372/1000 | Loss: 0.00002442
Iteration 373/1000 | Loss: 0.00002442
Iteration 374/1000 | Loss: 0.00002441
Iteration 375/1000 | Loss: 0.00002441
Iteration 376/1000 | Loss: 0.00002441
Iteration 377/1000 | Loss: 0.00002440
Iteration 378/1000 | Loss: 0.00002440
Iteration 379/1000 | Loss: 0.00002440
Iteration 380/1000 | Loss: 0.00002440
Iteration 381/1000 | Loss: 0.00002440
Iteration 382/1000 | Loss: 0.00002440
Iteration 383/1000 | Loss: 0.00002439
Iteration 384/1000 | Loss: 0.00002439
Iteration 385/1000 | Loss: 0.00002438
Iteration 386/1000 | Loss: 0.00002438
Iteration 387/1000 | Loss: 0.00002438
Iteration 388/1000 | Loss: 0.00002437
Iteration 389/1000 | Loss: 0.00002437
Iteration 390/1000 | Loss: 0.00002437
Iteration 391/1000 | Loss: 0.00002436
Iteration 392/1000 | Loss: 0.00002436
Iteration 393/1000 | Loss: 0.00002435
Iteration 394/1000 | Loss: 0.00002435
Iteration 395/1000 | Loss: 0.00002431
Iteration 396/1000 | Loss: 0.00002430
Iteration 397/1000 | Loss: 0.00002429
Iteration 398/1000 | Loss: 0.00002428
Iteration 399/1000 | Loss: 0.00002428
Iteration 400/1000 | Loss: 0.00002427
Iteration 401/1000 | Loss: 0.00002427
Iteration 402/1000 | Loss: 0.00002423
Iteration 403/1000 | Loss: 0.00002423
Iteration 404/1000 | Loss: 0.00002421
Iteration 405/1000 | Loss: 0.00002421
Iteration 406/1000 | Loss: 0.00002421
Iteration 407/1000 | Loss: 0.00002420
Iteration 408/1000 | Loss: 0.00002420
Iteration 409/1000 | Loss: 0.00002417
Iteration 410/1000 | Loss: 0.00002416
Iteration 411/1000 | Loss: 0.00002416
Iteration 412/1000 | Loss: 0.00002415
Iteration 413/1000 | Loss: 0.00002414
Iteration 414/1000 | Loss: 0.00002414
Iteration 415/1000 | Loss: 0.00002413
Iteration 416/1000 | Loss: 0.00002412
Iteration 417/1000 | Loss: 0.00002411
Iteration 418/1000 | Loss: 0.00002411
Iteration 419/1000 | Loss: 0.00002410
Iteration 420/1000 | Loss: 0.00002409
Iteration 421/1000 | Loss: 0.00002408
Iteration 422/1000 | Loss: 0.00002407
Iteration 423/1000 | Loss: 0.00002407
Iteration 424/1000 | Loss: 0.00002406
Iteration 425/1000 | Loss: 0.00002405
Iteration 426/1000 | Loss: 0.00002405
Iteration 427/1000 | Loss: 0.00002404
Iteration 428/1000 | Loss: 0.00002404
Iteration 429/1000 | Loss: 0.00002401
Iteration 430/1000 | Loss: 0.00002399
Iteration 431/1000 | Loss: 0.00002398
Iteration 432/1000 | Loss: 0.00002397
Iteration 433/1000 | Loss: 0.00002397
Iteration 434/1000 | Loss: 0.00002396
Iteration 435/1000 | Loss: 0.00002396
Iteration 436/1000 | Loss: 0.00002395
Iteration 437/1000 | Loss: 0.00048877
Iteration 438/1000 | Loss: 0.00003173
Iteration 439/1000 | Loss: 0.00002646
Iteration 440/1000 | Loss: 0.00002774
Iteration 441/1000 | Loss: 0.00002341
Iteration 442/1000 | Loss: 0.00003310
Iteration 443/1000 | Loss: 0.00002474
Iteration 444/1000 | Loss: 0.00003026
Iteration 445/1000 | Loss: 0.00002514
Iteration 446/1000 | Loss: 0.00002930
Iteration 447/1000 | Loss: 0.00002382
Iteration 448/1000 | Loss: 0.00002785
Iteration 449/1000 | Loss: 0.00002344
Iteration 450/1000 | Loss: 0.00002742
Iteration 451/1000 | Loss: 0.00002344
Iteration 452/1000 | Loss: 0.00002672
Iteration 453/1000 | Loss: 0.00002306
Iteration 454/1000 | Loss: 0.00002466
Iteration 455/1000 | Loss: 0.00002286
Iteration 456/1000 | Loss: 0.00002583
Iteration 457/1000 | Loss: 0.00002239
Iteration 458/1000 | Loss: 0.00002342
Iteration 459/1000 | Loss: 0.00002189
Iteration 460/1000 | Loss: 0.00002186
Iteration 461/1000 | Loss: 0.00002185
Iteration 462/1000 | Loss: 0.00002178
Iteration 463/1000 | Loss: 0.00002174
Iteration 464/1000 | Loss: 0.00002168
Iteration 465/1000 | Loss: 0.00002167
Iteration 466/1000 | Loss: 0.00002164
Iteration 467/1000 | Loss: 0.00002163
Iteration 468/1000 | Loss: 0.00002163
Iteration 469/1000 | Loss: 0.00002162
Iteration 470/1000 | Loss: 0.00002162
Iteration 471/1000 | Loss: 0.00002161
Iteration 472/1000 | Loss: 0.00002161
Iteration 473/1000 | Loss: 0.00002159
Iteration 474/1000 | Loss: 0.00002159
Iteration 475/1000 | Loss: 0.00002158
Iteration 476/1000 | Loss: 0.00002157
Iteration 477/1000 | Loss: 0.00002154
Iteration 478/1000 | Loss: 0.00002151
Iteration 479/1000 | Loss: 0.00002151
Iteration 480/1000 | Loss: 0.00002150
Iteration 481/1000 | Loss: 0.00002150
Iteration 482/1000 | Loss: 0.00002150
Iteration 483/1000 | Loss: 0.00002149
Iteration 484/1000 | Loss: 0.00002149
Iteration 485/1000 | Loss: 0.00002148
Iteration 486/1000 | Loss: 0.00002148
Iteration 487/1000 | Loss: 0.00002148
Iteration 488/1000 | Loss: 0.00002147
Iteration 489/1000 | Loss: 0.00002147
Iteration 490/1000 | Loss: 0.00002147
Iteration 491/1000 | Loss: 0.00002147
Iteration 492/1000 | Loss: 0.00002147
Iteration 493/1000 | Loss: 0.00002147
Iteration 494/1000 | Loss: 0.00002147
Iteration 495/1000 | Loss: 0.00002147
Iteration 496/1000 | Loss: 0.00002147
Iteration 497/1000 | Loss: 0.00002147
Iteration 498/1000 | Loss: 0.00002147
Iteration 499/1000 | Loss: 0.00002147
Iteration 500/1000 | Loss: 0.00002147
Iteration 501/1000 | Loss: 0.00002147
Iteration 502/1000 | Loss: 0.00002147
Iteration 503/1000 | Loss: 0.00002147
Iteration 504/1000 | Loss: 0.00002147
Iteration 505/1000 | Loss: 0.00002147
Iteration 506/1000 | Loss: 0.00002147
Iteration 507/1000 | Loss: 0.00002147
Iteration 508/1000 | Loss: 0.00002147
Iteration 509/1000 | Loss: 0.00002147
Iteration 510/1000 | Loss: 0.00002147
Iteration 511/1000 | Loss: 0.00002147
Iteration 512/1000 | Loss: 0.00002147
Iteration 513/1000 | Loss: 0.00002147
Iteration 514/1000 | Loss: 0.00002147
Iteration 515/1000 | Loss: 0.00002147
Iteration 516/1000 | Loss: 0.00002147
Iteration 517/1000 | Loss: 0.00002147
Iteration 518/1000 | Loss: 0.00002147
Iteration 519/1000 | Loss: 0.00002147
Iteration 520/1000 | Loss: 0.00002147
Iteration 521/1000 | Loss: 0.00002147
Iteration 522/1000 | Loss: 0.00002147
Iteration 523/1000 | Loss: 0.00002147
Iteration 524/1000 | Loss: 0.00002147
Iteration 525/1000 | Loss: 0.00002147
Iteration 526/1000 | Loss: 0.00002147
Iteration 527/1000 | Loss: 0.00002147
Iteration 528/1000 | Loss: 0.00002147
Iteration 529/1000 | Loss: 0.00002147
Iteration 530/1000 | Loss: 0.00002147
Iteration 531/1000 | Loss: 0.00002147
Iteration 532/1000 | Loss: 0.00002147
Iteration 533/1000 | Loss: 0.00002147
Iteration 534/1000 | Loss: 0.00002147
Iteration 535/1000 | Loss: 0.00002147
Iteration 536/1000 | Loss: 0.00002147
Iteration 537/1000 | Loss: 0.00002147
Iteration 538/1000 | Loss: 0.00002147
Iteration 539/1000 | Loss: 0.00002147
Iteration 540/1000 | Loss: 0.00002147
Iteration 541/1000 | Loss: 0.00002147
Iteration 542/1000 | Loss: 0.00002147
Iteration 543/1000 | Loss: 0.00002147
Iteration 544/1000 | Loss: 0.00002147
Iteration 545/1000 | Loss: 0.00002147
Iteration 546/1000 | Loss: 0.00002147
Iteration 547/1000 | Loss: 0.00002147
Iteration 548/1000 | Loss: 0.00002147
Iteration 549/1000 | Loss: 0.00002147
Iteration 550/1000 | Loss: 0.00002147
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 550. Stopping optimization.
Last 5 losses: [2.1467578335432336e-05, 2.1467578335432336e-05, 2.1467578335432336e-05, 2.1467578335432336e-05, 2.1467578335432336e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1467578335432336e-05

Optimization complete. Final v2v error: 3.420668125152588 mm

Highest mean error: 20.84100341796875 mm for frame 223

Lowest mean error: 2.7485554218292236 mm for frame 169

Saving results

Total time: 518.5108847618103
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_5533/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5533/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5533/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01123905
Iteration 2/25 | Loss: 0.00358353
Iteration 3/25 | Loss: 0.00237052
Iteration 4/25 | Loss: 0.00210217
Iteration 5/25 | Loss: 0.00136477
Iteration 6/25 | Loss: 0.00129743
Iteration 7/25 | Loss: 0.00124840
Iteration 8/25 | Loss: 0.00123213
Iteration 9/25 | Loss: 0.00122990
Iteration 10/25 | Loss: 0.00121738
Iteration 11/25 | Loss: 0.00121481
Iteration 12/25 | Loss: 0.00121440
Iteration 13/25 | Loss: 0.00121417
Iteration 14/25 | Loss: 0.00121410
Iteration 15/25 | Loss: 0.00121410
Iteration 16/25 | Loss: 0.00121410
Iteration 17/25 | Loss: 0.00121409
Iteration 18/25 | Loss: 0.00121409
Iteration 19/25 | Loss: 0.00121409
Iteration 20/25 | Loss: 0.00121401
Iteration 21/25 | Loss: 0.00121397
Iteration 22/25 | Loss: 0.00121397
Iteration 23/25 | Loss: 0.00121396
Iteration 24/25 | Loss: 0.00121396
Iteration 25/25 | Loss: 0.00121396

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.58872163
Iteration 2/25 | Loss: 0.00192949
Iteration 3/25 | Loss: 0.00192949
Iteration 4/25 | Loss: 0.00192949
Iteration 5/25 | Loss: 0.00192949
Iteration 6/25 | Loss: 0.00192949
Iteration 7/25 | Loss: 0.00192949
Iteration 8/25 | Loss: 0.00192949
Iteration 9/25 | Loss: 0.00192949
Iteration 10/25 | Loss: 0.00192949
Iteration 11/25 | Loss: 0.00192949
Iteration 12/25 | Loss: 0.00192949
Iteration 13/25 | Loss: 0.00192949
Iteration 14/25 | Loss: 0.00192949
Iteration 15/25 | Loss: 0.00192949
Iteration 16/25 | Loss: 0.00192949
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0019294851226732135, 0.0019294851226732135, 0.0019294851226732135, 0.0019294851226732135, 0.0019294851226732135]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019294851226732135

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00192949
Iteration 2/1000 | Loss: 0.00235099
Iteration 3/1000 | Loss: 0.00067890
Iteration 4/1000 | Loss: 0.00004318
Iteration 5/1000 | Loss: 0.00018641
Iteration 6/1000 | Loss: 0.00003521
Iteration 7/1000 | Loss: 0.00003331
Iteration 8/1000 | Loss: 0.00003211
Iteration 9/1000 | Loss: 0.00003122
Iteration 10/1000 | Loss: 0.00003074
Iteration 11/1000 | Loss: 0.00003023
Iteration 12/1000 | Loss: 0.00002954
Iteration 13/1000 | Loss: 0.00002871
Iteration 14/1000 | Loss: 0.00003634
Iteration 15/1000 | Loss: 0.00003905
Iteration 16/1000 | Loss: 0.00003490
Iteration 17/1000 | Loss: 0.00003217
Iteration 18/1000 | Loss: 0.00003826
Iteration 19/1000 | Loss: 0.00003174
Iteration 20/1000 | Loss: 0.00002847
Iteration 21/1000 | Loss: 0.00002690
Iteration 22/1000 | Loss: 0.00002613
Iteration 23/1000 | Loss: 0.00002579
Iteration 24/1000 | Loss: 0.00002554
Iteration 25/1000 | Loss: 0.00002538
Iteration 26/1000 | Loss: 0.00002537
Iteration 27/1000 | Loss: 0.00002532
Iteration 28/1000 | Loss: 0.00002532
Iteration 29/1000 | Loss: 0.00002531
Iteration 30/1000 | Loss: 0.00002531
Iteration 31/1000 | Loss: 0.00002531
Iteration 32/1000 | Loss: 0.00002530
Iteration 33/1000 | Loss: 0.00002530
Iteration 34/1000 | Loss: 0.00002530
Iteration 35/1000 | Loss: 0.00002524
Iteration 36/1000 | Loss: 0.00002524
Iteration 37/1000 | Loss: 0.00002523
Iteration 38/1000 | Loss: 0.00002523
Iteration 39/1000 | Loss: 0.00002522
Iteration 40/1000 | Loss: 0.00002522
Iteration 41/1000 | Loss: 0.00002521
Iteration 42/1000 | Loss: 0.00002520
Iteration 43/1000 | Loss: 0.00002518
Iteration 44/1000 | Loss: 0.00002517
Iteration 45/1000 | Loss: 0.00002517
Iteration 46/1000 | Loss: 0.00002517
Iteration 47/1000 | Loss: 0.00002517
Iteration 48/1000 | Loss: 0.00002516
Iteration 49/1000 | Loss: 0.00002514
Iteration 50/1000 | Loss: 0.00002514
Iteration 51/1000 | Loss: 0.00002514
Iteration 52/1000 | Loss: 0.00002514
Iteration 53/1000 | Loss: 0.00002513
Iteration 54/1000 | Loss: 0.00002513
Iteration 55/1000 | Loss: 0.00002511
Iteration 56/1000 | Loss: 0.00002510
Iteration 57/1000 | Loss: 0.00002510
Iteration 58/1000 | Loss: 0.00002510
Iteration 59/1000 | Loss: 0.00002510
Iteration 60/1000 | Loss: 0.00002510
Iteration 61/1000 | Loss: 0.00002510
Iteration 62/1000 | Loss: 0.00002510
Iteration 63/1000 | Loss: 0.00002510
Iteration 64/1000 | Loss: 0.00002510
Iteration 65/1000 | Loss: 0.00002510
Iteration 66/1000 | Loss: 0.00002510
Iteration 67/1000 | Loss: 0.00002509
Iteration 68/1000 | Loss: 0.00002509
Iteration 69/1000 | Loss: 0.00002509
Iteration 70/1000 | Loss: 0.00002509
Iteration 71/1000 | Loss: 0.00002509
Iteration 72/1000 | Loss: 0.00002509
Iteration 73/1000 | Loss: 0.00002509
Iteration 74/1000 | Loss: 0.00002508
Iteration 75/1000 | Loss: 0.00002508
Iteration 76/1000 | Loss: 0.00002503
Iteration 77/1000 | Loss: 0.00002503
Iteration 78/1000 | Loss: 0.00002503
Iteration 79/1000 | Loss: 0.00002503
Iteration 80/1000 | Loss: 0.00002503
Iteration 81/1000 | Loss: 0.00002502
Iteration 82/1000 | Loss: 0.00002502
Iteration 83/1000 | Loss: 0.00002502
Iteration 84/1000 | Loss: 0.00002502
Iteration 85/1000 | Loss: 0.00002502
Iteration 86/1000 | Loss: 0.00002502
Iteration 87/1000 | Loss: 0.00002502
Iteration 88/1000 | Loss: 0.00002501
Iteration 89/1000 | Loss: 0.00002500
Iteration 90/1000 | Loss: 0.00002500
Iteration 91/1000 | Loss: 0.00002500
Iteration 92/1000 | Loss: 0.00002500
Iteration 93/1000 | Loss: 0.00002500
Iteration 94/1000 | Loss: 0.00002500
Iteration 95/1000 | Loss: 0.00002500
Iteration 96/1000 | Loss: 0.00002500
Iteration 97/1000 | Loss: 0.00002499
Iteration 98/1000 | Loss: 0.00002499
Iteration 99/1000 | Loss: 0.00002499
Iteration 100/1000 | Loss: 0.00002499
Iteration 101/1000 | Loss: 0.00002499
Iteration 102/1000 | Loss: 0.00002499
Iteration 103/1000 | Loss: 0.00002499
Iteration 104/1000 | Loss: 0.00002498
Iteration 105/1000 | Loss: 0.00002498
Iteration 106/1000 | Loss: 0.00002498
Iteration 107/1000 | Loss: 0.00002498
Iteration 108/1000 | Loss: 0.00002498
Iteration 109/1000 | Loss: 0.00002498
Iteration 110/1000 | Loss: 0.00002498
Iteration 111/1000 | Loss: 0.00002498
Iteration 112/1000 | Loss: 0.00002498
Iteration 113/1000 | Loss: 0.00002498
Iteration 114/1000 | Loss: 0.00002498
Iteration 115/1000 | Loss: 0.00002498
Iteration 116/1000 | Loss: 0.00002498
Iteration 117/1000 | Loss: 0.00002497
Iteration 118/1000 | Loss: 0.00002497
Iteration 119/1000 | Loss: 0.00002497
Iteration 120/1000 | Loss: 0.00002497
Iteration 121/1000 | Loss: 0.00002497
Iteration 122/1000 | Loss: 0.00002497
Iteration 123/1000 | Loss: 0.00002497
Iteration 124/1000 | Loss: 0.00002497
Iteration 125/1000 | Loss: 0.00002497
Iteration 126/1000 | Loss: 0.00002497
Iteration 127/1000 | Loss: 0.00002497
Iteration 128/1000 | Loss: 0.00002497
Iteration 129/1000 | Loss: 0.00002497
Iteration 130/1000 | Loss: 0.00002496
Iteration 131/1000 | Loss: 0.00002496
Iteration 132/1000 | Loss: 0.00002496
Iteration 133/1000 | Loss: 0.00002496
Iteration 134/1000 | Loss: 0.00002496
Iteration 135/1000 | Loss: 0.00002496
Iteration 136/1000 | Loss: 0.00002496
Iteration 137/1000 | Loss: 0.00002496
Iteration 138/1000 | Loss: 0.00002496
Iteration 139/1000 | Loss: 0.00002496
Iteration 140/1000 | Loss: 0.00002496
Iteration 141/1000 | Loss: 0.00002496
Iteration 142/1000 | Loss: 0.00002496
Iteration 143/1000 | Loss: 0.00002496
Iteration 144/1000 | Loss: 0.00002496
Iteration 145/1000 | Loss: 0.00002496
Iteration 146/1000 | Loss: 0.00002496
Iteration 147/1000 | Loss: 0.00002496
Iteration 148/1000 | Loss: 0.00002495
Iteration 149/1000 | Loss: 0.00002495
Iteration 150/1000 | Loss: 0.00002495
Iteration 151/1000 | Loss: 0.00002495
Iteration 152/1000 | Loss: 0.00002495
Iteration 153/1000 | Loss: 0.00002495
Iteration 154/1000 | Loss: 0.00002495
Iteration 155/1000 | Loss: 0.00002495
Iteration 156/1000 | Loss: 0.00002495
Iteration 157/1000 | Loss: 0.00002495
Iteration 158/1000 | Loss: 0.00002494
Iteration 159/1000 | Loss: 0.00002494
Iteration 160/1000 | Loss: 0.00002494
Iteration 161/1000 | Loss: 0.00002494
Iteration 162/1000 | Loss: 0.00002494
Iteration 163/1000 | Loss: 0.00002494
Iteration 164/1000 | Loss: 0.00002494
Iteration 165/1000 | Loss: 0.00002494
Iteration 166/1000 | Loss: 0.00002494
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [2.4942291929619387e-05, 2.4942291929619387e-05, 2.4942291929619387e-05, 2.4942291929619387e-05, 2.4942291929619387e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4942291929619387e-05

Optimization complete. Final v2v error: 3.753166437149048 mm

Highest mean error: 10.033052444458008 mm for frame 75

Lowest mean error: 3.1445870399475098 mm for frame 11

Saving results

Total time: 72.8883638381958
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_5533/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5533/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5533/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00663857
Iteration 2/25 | Loss: 0.00131493
Iteration 3/25 | Loss: 0.00114352
Iteration 4/25 | Loss: 0.00112065
Iteration 5/25 | Loss: 0.00111684
Iteration 6/25 | Loss: 0.00111471
Iteration 7/25 | Loss: 0.00111404
Iteration 8/25 | Loss: 0.00111372
Iteration 9/25 | Loss: 0.00111342
Iteration 10/25 | Loss: 0.00111304
Iteration 11/25 | Loss: 0.00111281
Iteration 12/25 | Loss: 0.00111270
Iteration 13/25 | Loss: 0.00111270
Iteration 14/25 | Loss: 0.00111270
Iteration 15/25 | Loss: 0.00111270
Iteration 16/25 | Loss: 0.00111270
Iteration 17/25 | Loss: 0.00111270
Iteration 18/25 | Loss: 0.00111270
Iteration 19/25 | Loss: 0.00111270
Iteration 20/25 | Loss: 0.00111270
Iteration 21/25 | Loss: 0.00111270
Iteration 22/25 | Loss: 0.00111269
Iteration 23/25 | Loss: 0.00111269
Iteration 24/25 | Loss: 0.00111269
Iteration 25/25 | Loss: 0.00111269

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66774201
Iteration 2/25 | Loss: 0.00280603
Iteration 3/25 | Loss: 0.00280602
Iteration 4/25 | Loss: 0.00280601
Iteration 5/25 | Loss: 0.00280601
Iteration 6/25 | Loss: 0.00280601
Iteration 7/25 | Loss: 0.00280601
Iteration 8/25 | Loss: 0.00280601
Iteration 9/25 | Loss: 0.00280601
Iteration 10/25 | Loss: 0.00280601
Iteration 11/25 | Loss: 0.00280601
Iteration 12/25 | Loss: 0.00280601
Iteration 13/25 | Loss: 0.00280601
Iteration 14/25 | Loss: 0.00280601
Iteration 15/25 | Loss: 0.00280601
Iteration 16/25 | Loss: 0.00280601
Iteration 17/25 | Loss: 0.00280601
Iteration 18/25 | Loss: 0.00280601
Iteration 19/25 | Loss: 0.00280601
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0028060120530426502, 0.0028060120530426502, 0.0028060120530426502, 0.0028060120530426502, 0.0028060120530426502]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0028060120530426502

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00280601
Iteration 2/1000 | Loss: 0.00008939
Iteration 3/1000 | Loss: 0.00005654
Iteration 4/1000 | Loss: 0.00005463
Iteration 5/1000 | Loss: 0.00003710
Iteration 6/1000 | Loss: 0.00026474
Iteration 7/1000 | Loss: 0.00017779
Iteration 8/1000 | Loss: 0.00025112
Iteration 9/1000 | Loss: 0.00032301
Iteration 10/1000 | Loss: 0.00016325
Iteration 11/1000 | Loss: 0.00022490
Iteration 12/1000 | Loss: 0.00003350
Iteration 13/1000 | Loss: 0.00002439
Iteration 14/1000 | Loss: 0.00002080
Iteration 15/1000 | Loss: 0.00001918
Iteration 16/1000 | Loss: 0.00001807
Iteration 17/1000 | Loss: 0.00001758
Iteration 18/1000 | Loss: 0.00001727
Iteration 19/1000 | Loss: 0.00001696
Iteration 20/1000 | Loss: 0.00001670
Iteration 21/1000 | Loss: 0.00001650
Iteration 22/1000 | Loss: 0.00001634
Iteration 23/1000 | Loss: 0.00001621
Iteration 24/1000 | Loss: 0.00001613
Iteration 25/1000 | Loss: 0.00001612
Iteration 26/1000 | Loss: 0.00001607
Iteration 27/1000 | Loss: 0.00001603
Iteration 28/1000 | Loss: 0.00001603
Iteration 29/1000 | Loss: 0.00001595
Iteration 30/1000 | Loss: 0.00001594
Iteration 31/1000 | Loss: 0.00001594
Iteration 32/1000 | Loss: 0.00001594
Iteration 33/1000 | Loss: 0.00001594
Iteration 34/1000 | Loss: 0.00001594
Iteration 35/1000 | Loss: 0.00001594
Iteration 36/1000 | Loss: 0.00001593
Iteration 37/1000 | Loss: 0.00001593
Iteration 38/1000 | Loss: 0.00001592
Iteration 39/1000 | Loss: 0.00001592
Iteration 40/1000 | Loss: 0.00001591
Iteration 41/1000 | Loss: 0.00001588
Iteration 42/1000 | Loss: 0.00001587
Iteration 43/1000 | Loss: 0.00001585
Iteration 44/1000 | Loss: 0.00001585
Iteration 45/1000 | Loss: 0.00001585
Iteration 46/1000 | Loss: 0.00001585
Iteration 47/1000 | Loss: 0.00001585
Iteration 48/1000 | Loss: 0.00001585
Iteration 49/1000 | Loss: 0.00001585
Iteration 50/1000 | Loss: 0.00001584
Iteration 51/1000 | Loss: 0.00001584
Iteration 52/1000 | Loss: 0.00001584
Iteration 53/1000 | Loss: 0.00001584
Iteration 54/1000 | Loss: 0.00001584
Iteration 55/1000 | Loss: 0.00001584
Iteration 56/1000 | Loss: 0.00001584
Iteration 57/1000 | Loss: 0.00001584
Iteration 58/1000 | Loss: 0.00001583
Iteration 59/1000 | Loss: 0.00001583
Iteration 60/1000 | Loss: 0.00001583
Iteration 61/1000 | Loss: 0.00001582
Iteration 62/1000 | Loss: 0.00001582
Iteration 63/1000 | Loss: 0.00001582
Iteration 64/1000 | Loss: 0.00001582
Iteration 65/1000 | Loss: 0.00001582
Iteration 66/1000 | Loss: 0.00001582
Iteration 67/1000 | Loss: 0.00001582
Iteration 68/1000 | Loss: 0.00001582
Iteration 69/1000 | Loss: 0.00001582
Iteration 70/1000 | Loss: 0.00001581
Iteration 71/1000 | Loss: 0.00001581
Iteration 72/1000 | Loss: 0.00001581
Iteration 73/1000 | Loss: 0.00001581
Iteration 74/1000 | Loss: 0.00001581
Iteration 75/1000 | Loss: 0.00001581
Iteration 76/1000 | Loss: 0.00001580
Iteration 77/1000 | Loss: 0.00001580
Iteration 78/1000 | Loss: 0.00001580
Iteration 79/1000 | Loss: 0.00001580
Iteration 80/1000 | Loss: 0.00001580
Iteration 81/1000 | Loss: 0.00001580
Iteration 82/1000 | Loss: 0.00001580
Iteration 83/1000 | Loss: 0.00001580
Iteration 84/1000 | Loss: 0.00001580
Iteration 85/1000 | Loss: 0.00001580
Iteration 86/1000 | Loss: 0.00001580
Iteration 87/1000 | Loss: 0.00001580
Iteration 88/1000 | Loss: 0.00001580
Iteration 89/1000 | Loss: 0.00001579
Iteration 90/1000 | Loss: 0.00001579
Iteration 91/1000 | Loss: 0.00001579
Iteration 92/1000 | Loss: 0.00001579
Iteration 93/1000 | Loss: 0.00001579
Iteration 94/1000 | Loss: 0.00001579
Iteration 95/1000 | Loss: 0.00001579
Iteration 96/1000 | Loss: 0.00001579
Iteration 97/1000 | Loss: 0.00001579
Iteration 98/1000 | Loss: 0.00001579
Iteration 99/1000 | Loss: 0.00001579
Iteration 100/1000 | Loss: 0.00001579
Iteration 101/1000 | Loss: 0.00001579
Iteration 102/1000 | Loss: 0.00001578
Iteration 103/1000 | Loss: 0.00001578
Iteration 104/1000 | Loss: 0.00001578
Iteration 105/1000 | Loss: 0.00001578
Iteration 106/1000 | Loss: 0.00001578
Iteration 107/1000 | Loss: 0.00001578
Iteration 108/1000 | Loss: 0.00001578
Iteration 109/1000 | Loss: 0.00001578
Iteration 110/1000 | Loss: 0.00001578
Iteration 111/1000 | Loss: 0.00001578
Iteration 112/1000 | Loss: 0.00001578
Iteration 113/1000 | Loss: 0.00001577
Iteration 114/1000 | Loss: 0.00001577
Iteration 115/1000 | Loss: 0.00001577
Iteration 116/1000 | Loss: 0.00001577
Iteration 117/1000 | Loss: 0.00001577
Iteration 118/1000 | Loss: 0.00001577
Iteration 119/1000 | Loss: 0.00001576
Iteration 120/1000 | Loss: 0.00001576
Iteration 121/1000 | Loss: 0.00001576
Iteration 122/1000 | Loss: 0.00001576
Iteration 123/1000 | Loss: 0.00001576
Iteration 124/1000 | Loss: 0.00001576
Iteration 125/1000 | Loss: 0.00001576
Iteration 126/1000 | Loss: 0.00001576
Iteration 127/1000 | Loss: 0.00001575
Iteration 128/1000 | Loss: 0.00001575
Iteration 129/1000 | Loss: 0.00001575
Iteration 130/1000 | Loss: 0.00001575
Iteration 131/1000 | Loss: 0.00001575
Iteration 132/1000 | Loss: 0.00001575
Iteration 133/1000 | Loss: 0.00001575
Iteration 134/1000 | Loss: 0.00001575
Iteration 135/1000 | Loss: 0.00001575
Iteration 136/1000 | Loss: 0.00001575
Iteration 137/1000 | Loss: 0.00001575
Iteration 138/1000 | Loss: 0.00001575
Iteration 139/1000 | Loss: 0.00001575
Iteration 140/1000 | Loss: 0.00001575
Iteration 141/1000 | Loss: 0.00001575
Iteration 142/1000 | Loss: 0.00001575
Iteration 143/1000 | Loss: 0.00001575
Iteration 144/1000 | Loss: 0.00001575
Iteration 145/1000 | Loss: 0.00001575
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [1.5750343663967215e-05, 1.5750343663967215e-05, 1.5750343663967215e-05, 1.5750343663967215e-05, 1.5750343663967215e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5750343663967215e-05

Optimization complete. Final v2v error: 3.235276460647583 mm

Highest mean error: 4.004648208618164 mm for frame 177

Lowest mean error: 2.701075553894043 mm for frame 143

Saving results

Total time: 71.58099961280823
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_5533/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5533/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5533/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00838736
Iteration 2/25 | Loss: 0.00112953
Iteration 3/25 | Loss: 0.00104074
Iteration 4/25 | Loss: 0.00103200
Iteration 5/25 | Loss: 0.00102930
Iteration 6/25 | Loss: 0.00102826
Iteration 7/25 | Loss: 0.00102826
Iteration 8/25 | Loss: 0.00102826
Iteration 9/25 | Loss: 0.00102826
Iteration 10/25 | Loss: 0.00102826
Iteration 11/25 | Loss: 0.00102826
Iteration 12/25 | Loss: 0.00102826
Iteration 13/25 | Loss: 0.00102826
Iteration 14/25 | Loss: 0.00102826
Iteration 15/25 | Loss: 0.00102826
Iteration 16/25 | Loss: 0.00102826
Iteration 17/25 | Loss: 0.00102826
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010282570729032159, 0.0010282570729032159, 0.0010282570729032159, 0.0010282570729032159, 0.0010282570729032159]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010282570729032159

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21968555
Iteration 2/25 | Loss: 0.00206937
Iteration 3/25 | Loss: 0.00206937
Iteration 4/25 | Loss: 0.00206937
Iteration 5/25 | Loss: 0.00206937
Iteration 6/25 | Loss: 0.00206937
Iteration 7/25 | Loss: 0.00206937
Iteration 8/25 | Loss: 0.00206937
Iteration 9/25 | Loss: 0.00206937
Iteration 10/25 | Loss: 0.00206937
Iteration 11/25 | Loss: 0.00206937
Iteration 12/25 | Loss: 0.00206937
Iteration 13/25 | Loss: 0.00206937
Iteration 14/25 | Loss: 0.00206937
Iteration 15/25 | Loss: 0.00206937
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.002069370588287711, 0.002069370588287711, 0.002069370588287711, 0.002069370588287711, 0.002069370588287711]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002069370588287711

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00206937
Iteration 2/1000 | Loss: 0.00003406
Iteration 3/1000 | Loss: 0.00001827
Iteration 4/1000 | Loss: 0.00001598
Iteration 5/1000 | Loss: 0.00001465
Iteration 6/1000 | Loss: 0.00001400
Iteration 7/1000 | Loss: 0.00001327
Iteration 8/1000 | Loss: 0.00001277
Iteration 9/1000 | Loss: 0.00001261
Iteration 10/1000 | Loss: 0.00001249
Iteration 11/1000 | Loss: 0.00001249
Iteration 12/1000 | Loss: 0.00001230
Iteration 13/1000 | Loss: 0.00001223
Iteration 14/1000 | Loss: 0.00001221
Iteration 15/1000 | Loss: 0.00001219
Iteration 16/1000 | Loss: 0.00001218
Iteration 17/1000 | Loss: 0.00001215
Iteration 18/1000 | Loss: 0.00001213
Iteration 19/1000 | Loss: 0.00001213
Iteration 20/1000 | Loss: 0.00001212
Iteration 21/1000 | Loss: 0.00001211
Iteration 22/1000 | Loss: 0.00001210
Iteration 23/1000 | Loss: 0.00001210
Iteration 24/1000 | Loss: 0.00001210
Iteration 25/1000 | Loss: 0.00001209
Iteration 26/1000 | Loss: 0.00001209
Iteration 27/1000 | Loss: 0.00001208
Iteration 28/1000 | Loss: 0.00001207
Iteration 29/1000 | Loss: 0.00001202
Iteration 30/1000 | Loss: 0.00001201
Iteration 31/1000 | Loss: 0.00001197
Iteration 32/1000 | Loss: 0.00001196
Iteration 33/1000 | Loss: 0.00001196
Iteration 34/1000 | Loss: 0.00001196
Iteration 35/1000 | Loss: 0.00001196
Iteration 36/1000 | Loss: 0.00001196
Iteration 37/1000 | Loss: 0.00001196
Iteration 38/1000 | Loss: 0.00001195
Iteration 39/1000 | Loss: 0.00001195
Iteration 40/1000 | Loss: 0.00001195
Iteration 41/1000 | Loss: 0.00001194
Iteration 42/1000 | Loss: 0.00001194
Iteration 43/1000 | Loss: 0.00001193
Iteration 44/1000 | Loss: 0.00001188
Iteration 45/1000 | Loss: 0.00001187
Iteration 46/1000 | Loss: 0.00001187
Iteration 47/1000 | Loss: 0.00001187
Iteration 48/1000 | Loss: 0.00001186
Iteration 49/1000 | Loss: 0.00001186
Iteration 50/1000 | Loss: 0.00001185
Iteration 51/1000 | Loss: 0.00001185
Iteration 52/1000 | Loss: 0.00001185
Iteration 53/1000 | Loss: 0.00001185
Iteration 54/1000 | Loss: 0.00001184
Iteration 55/1000 | Loss: 0.00001183
Iteration 56/1000 | Loss: 0.00001183
Iteration 57/1000 | Loss: 0.00001182
Iteration 58/1000 | Loss: 0.00001182
Iteration 59/1000 | Loss: 0.00001182
Iteration 60/1000 | Loss: 0.00001182
Iteration 61/1000 | Loss: 0.00001182
Iteration 62/1000 | Loss: 0.00001182
Iteration 63/1000 | Loss: 0.00001182
Iteration 64/1000 | Loss: 0.00001182
Iteration 65/1000 | Loss: 0.00001182
Iteration 66/1000 | Loss: 0.00001182
Iteration 67/1000 | Loss: 0.00001182
Iteration 68/1000 | Loss: 0.00001182
Iteration 69/1000 | Loss: 0.00001182
Iteration 70/1000 | Loss: 0.00001182
Iteration 71/1000 | Loss: 0.00001182
Iteration 72/1000 | Loss: 0.00001182
Iteration 73/1000 | Loss: 0.00001182
Iteration 74/1000 | Loss: 0.00001182
Iteration 75/1000 | Loss: 0.00001182
Iteration 76/1000 | Loss: 0.00001182
Iteration 77/1000 | Loss: 0.00001182
Iteration 78/1000 | Loss: 0.00001182
Iteration 79/1000 | Loss: 0.00001182
Iteration 80/1000 | Loss: 0.00001182
Iteration 81/1000 | Loss: 0.00001182
Iteration 82/1000 | Loss: 0.00001182
Iteration 83/1000 | Loss: 0.00001182
Iteration 84/1000 | Loss: 0.00001182
Iteration 85/1000 | Loss: 0.00001182
Iteration 86/1000 | Loss: 0.00001182
Iteration 87/1000 | Loss: 0.00001182
Iteration 88/1000 | Loss: 0.00001182
Iteration 89/1000 | Loss: 0.00001182
Iteration 90/1000 | Loss: 0.00001182
Iteration 91/1000 | Loss: 0.00001182
Iteration 92/1000 | Loss: 0.00001182
Iteration 93/1000 | Loss: 0.00001182
Iteration 94/1000 | Loss: 0.00001182
Iteration 95/1000 | Loss: 0.00001182
Iteration 96/1000 | Loss: 0.00001182
Iteration 97/1000 | Loss: 0.00001182
Iteration 98/1000 | Loss: 0.00001182
Iteration 99/1000 | Loss: 0.00001182
Iteration 100/1000 | Loss: 0.00001182
Iteration 101/1000 | Loss: 0.00001182
Iteration 102/1000 | Loss: 0.00001182
Iteration 103/1000 | Loss: 0.00001182
Iteration 104/1000 | Loss: 0.00001182
Iteration 105/1000 | Loss: 0.00001182
Iteration 106/1000 | Loss: 0.00001182
Iteration 107/1000 | Loss: 0.00001182
Iteration 108/1000 | Loss: 0.00001182
Iteration 109/1000 | Loss: 0.00001182
Iteration 110/1000 | Loss: 0.00001182
Iteration 111/1000 | Loss: 0.00001182
Iteration 112/1000 | Loss: 0.00001182
Iteration 113/1000 | Loss: 0.00001182
Iteration 114/1000 | Loss: 0.00001182
Iteration 115/1000 | Loss: 0.00001182
Iteration 116/1000 | Loss: 0.00001182
Iteration 117/1000 | Loss: 0.00001182
Iteration 118/1000 | Loss: 0.00001182
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [1.1818207894975785e-05, 1.1818207894975785e-05, 1.1818207894975785e-05, 1.1818207894975785e-05, 1.1818207894975785e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1818207894975785e-05

Optimization complete. Final v2v error: 2.9156928062438965 mm

Highest mean error: 3.1244423389434814 mm for frame 79

Lowest mean error: 2.70163893699646 mm for frame 117

Saving results

Total time: 30.130518913269043
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_5533/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5533/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5533/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00371143
Iteration 2/25 | Loss: 0.00110017
Iteration 3/25 | Loss: 0.00100965
Iteration 4/25 | Loss: 0.00100078
Iteration 5/25 | Loss: 0.00099843
Iteration 6/25 | Loss: 0.00099785
Iteration 7/25 | Loss: 0.00099785
Iteration 8/25 | Loss: 0.00099785
Iteration 9/25 | Loss: 0.00099785
Iteration 10/25 | Loss: 0.00099785
Iteration 11/25 | Loss: 0.00099785
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009978526504710317, 0.0009978526504710317, 0.0009978526504710317, 0.0009978526504710317, 0.0009978526504710317]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009978526504710317

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32174790
Iteration 2/25 | Loss: 0.00198868
Iteration 3/25 | Loss: 0.00198868
Iteration 4/25 | Loss: 0.00198868
Iteration 5/25 | Loss: 0.00198868
Iteration 6/25 | Loss: 0.00198868
Iteration 7/25 | Loss: 0.00198868
Iteration 8/25 | Loss: 0.00198868
Iteration 9/25 | Loss: 0.00198868
Iteration 10/25 | Loss: 0.00198868
Iteration 11/25 | Loss: 0.00198868
Iteration 12/25 | Loss: 0.00198868
Iteration 13/25 | Loss: 0.00198868
Iteration 14/25 | Loss: 0.00198868
Iteration 15/25 | Loss: 0.00198868
Iteration 16/25 | Loss: 0.00198868
Iteration 17/25 | Loss: 0.00198868
Iteration 18/25 | Loss: 0.00198868
Iteration 19/25 | Loss: 0.00198868
Iteration 20/25 | Loss: 0.00198868
Iteration 21/25 | Loss: 0.00198868
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0019886777736246586, 0.0019886777736246586, 0.0019886777736246586, 0.0019886777736246586, 0.0019886777736246586]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019886777736246586

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00198868
Iteration 2/1000 | Loss: 0.00002331
Iteration 3/1000 | Loss: 0.00001244
Iteration 4/1000 | Loss: 0.00001107
Iteration 5/1000 | Loss: 0.00001017
Iteration 6/1000 | Loss: 0.00000964
Iteration 7/1000 | Loss: 0.00000925
Iteration 8/1000 | Loss: 0.00000913
Iteration 9/1000 | Loss: 0.00000889
Iteration 10/1000 | Loss: 0.00000877
Iteration 11/1000 | Loss: 0.00000873
Iteration 12/1000 | Loss: 0.00000872
Iteration 13/1000 | Loss: 0.00000871
Iteration 14/1000 | Loss: 0.00000871
Iteration 15/1000 | Loss: 0.00000870
Iteration 16/1000 | Loss: 0.00000869
Iteration 17/1000 | Loss: 0.00000868
Iteration 18/1000 | Loss: 0.00000868
Iteration 19/1000 | Loss: 0.00000867
Iteration 20/1000 | Loss: 0.00000867
Iteration 21/1000 | Loss: 0.00000867
Iteration 22/1000 | Loss: 0.00000867
Iteration 23/1000 | Loss: 0.00000866
Iteration 24/1000 | Loss: 0.00000866
Iteration 25/1000 | Loss: 0.00000866
Iteration 26/1000 | Loss: 0.00000864
Iteration 27/1000 | Loss: 0.00000864
Iteration 28/1000 | Loss: 0.00000864
Iteration 29/1000 | Loss: 0.00000863
Iteration 30/1000 | Loss: 0.00000863
Iteration 31/1000 | Loss: 0.00000863
Iteration 32/1000 | Loss: 0.00000862
Iteration 33/1000 | Loss: 0.00000862
Iteration 34/1000 | Loss: 0.00000862
Iteration 35/1000 | Loss: 0.00000862
Iteration 36/1000 | Loss: 0.00000861
Iteration 37/1000 | Loss: 0.00000861
Iteration 38/1000 | Loss: 0.00000861
Iteration 39/1000 | Loss: 0.00000861
Iteration 40/1000 | Loss: 0.00000860
Iteration 41/1000 | Loss: 0.00000859
Iteration 42/1000 | Loss: 0.00000859
Iteration 43/1000 | Loss: 0.00000859
Iteration 44/1000 | Loss: 0.00000859
Iteration 45/1000 | Loss: 0.00000858
Iteration 46/1000 | Loss: 0.00000858
Iteration 47/1000 | Loss: 0.00000858
Iteration 48/1000 | Loss: 0.00000858
Iteration 49/1000 | Loss: 0.00000858
Iteration 50/1000 | Loss: 0.00000857
Iteration 51/1000 | Loss: 0.00000857
Iteration 52/1000 | Loss: 0.00000857
Iteration 53/1000 | Loss: 0.00000856
Iteration 54/1000 | Loss: 0.00000856
Iteration 55/1000 | Loss: 0.00000855
Iteration 56/1000 | Loss: 0.00000855
Iteration 57/1000 | Loss: 0.00000854
Iteration 58/1000 | Loss: 0.00000854
Iteration 59/1000 | Loss: 0.00000852
Iteration 60/1000 | Loss: 0.00000852
Iteration 61/1000 | Loss: 0.00000852
Iteration 62/1000 | Loss: 0.00000852
Iteration 63/1000 | Loss: 0.00000852
Iteration 64/1000 | Loss: 0.00000851
Iteration 65/1000 | Loss: 0.00000851
Iteration 66/1000 | Loss: 0.00000851
Iteration 67/1000 | Loss: 0.00000851
Iteration 68/1000 | Loss: 0.00000851
Iteration 69/1000 | Loss: 0.00000851
Iteration 70/1000 | Loss: 0.00000851
Iteration 71/1000 | Loss: 0.00000851
Iteration 72/1000 | Loss: 0.00000851
Iteration 73/1000 | Loss: 0.00000851
Iteration 74/1000 | Loss: 0.00000851
Iteration 75/1000 | Loss: 0.00000851
Iteration 76/1000 | Loss: 0.00000851
Iteration 77/1000 | Loss: 0.00000850
Iteration 78/1000 | Loss: 0.00000850
Iteration 79/1000 | Loss: 0.00000850
Iteration 80/1000 | Loss: 0.00000849
Iteration 81/1000 | Loss: 0.00000849
Iteration 82/1000 | Loss: 0.00000849
Iteration 83/1000 | Loss: 0.00000848
Iteration 84/1000 | Loss: 0.00000848
Iteration 85/1000 | Loss: 0.00000848
Iteration 86/1000 | Loss: 0.00000848
Iteration 87/1000 | Loss: 0.00000848
Iteration 88/1000 | Loss: 0.00000848
Iteration 89/1000 | Loss: 0.00000848
Iteration 90/1000 | Loss: 0.00000847
Iteration 91/1000 | Loss: 0.00000847
Iteration 92/1000 | Loss: 0.00000846
Iteration 93/1000 | Loss: 0.00000846
Iteration 94/1000 | Loss: 0.00000846
Iteration 95/1000 | Loss: 0.00000845
Iteration 96/1000 | Loss: 0.00000845
Iteration 97/1000 | Loss: 0.00000845
Iteration 98/1000 | Loss: 0.00000845
Iteration 99/1000 | Loss: 0.00000845
Iteration 100/1000 | Loss: 0.00000844
Iteration 101/1000 | Loss: 0.00000844
Iteration 102/1000 | Loss: 0.00000844
Iteration 103/1000 | Loss: 0.00000844
Iteration 104/1000 | Loss: 0.00000844
Iteration 105/1000 | Loss: 0.00000844
Iteration 106/1000 | Loss: 0.00000844
Iteration 107/1000 | Loss: 0.00000844
Iteration 108/1000 | Loss: 0.00000844
Iteration 109/1000 | Loss: 0.00000844
Iteration 110/1000 | Loss: 0.00000844
Iteration 111/1000 | Loss: 0.00000844
Iteration 112/1000 | Loss: 0.00000844
Iteration 113/1000 | Loss: 0.00000844
Iteration 114/1000 | Loss: 0.00000843
Iteration 115/1000 | Loss: 0.00000843
Iteration 116/1000 | Loss: 0.00000843
Iteration 117/1000 | Loss: 0.00000843
Iteration 118/1000 | Loss: 0.00000843
Iteration 119/1000 | Loss: 0.00000843
Iteration 120/1000 | Loss: 0.00000842
Iteration 121/1000 | Loss: 0.00000842
Iteration 122/1000 | Loss: 0.00000842
Iteration 123/1000 | Loss: 0.00000842
Iteration 124/1000 | Loss: 0.00000842
Iteration 125/1000 | Loss: 0.00000842
Iteration 126/1000 | Loss: 0.00000842
Iteration 127/1000 | Loss: 0.00000842
Iteration 128/1000 | Loss: 0.00000842
Iteration 129/1000 | Loss: 0.00000842
Iteration 130/1000 | Loss: 0.00000842
Iteration 131/1000 | Loss: 0.00000842
Iteration 132/1000 | Loss: 0.00000842
Iteration 133/1000 | Loss: 0.00000842
Iteration 134/1000 | Loss: 0.00000842
Iteration 135/1000 | Loss: 0.00000842
Iteration 136/1000 | Loss: 0.00000842
Iteration 137/1000 | Loss: 0.00000842
Iteration 138/1000 | Loss: 0.00000841
Iteration 139/1000 | Loss: 0.00000841
Iteration 140/1000 | Loss: 0.00000841
Iteration 141/1000 | Loss: 0.00000841
Iteration 142/1000 | Loss: 0.00000841
Iteration 143/1000 | Loss: 0.00000841
Iteration 144/1000 | Loss: 0.00000841
Iteration 145/1000 | Loss: 0.00000841
Iteration 146/1000 | Loss: 0.00000841
Iteration 147/1000 | Loss: 0.00000841
Iteration 148/1000 | Loss: 0.00000841
Iteration 149/1000 | Loss: 0.00000841
Iteration 150/1000 | Loss: 0.00000841
Iteration 151/1000 | Loss: 0.00000841
Iteration 152/1000 | Loss: 0.00000841
Iteration 153/1000 | Loss: 0.00000841
Iteration 154/1000 | Loss: 0.00000841
Iteration 155/1000 | Loss: 0.00000841
Iteration 156/1000 | Loss: 0.00000841
Iteration 157/1000 | Loss: 0.00000841
Iteration 158/1000 | Loss: 0.00000841
Iteration 159/1000 | Loss: 0.00000841
Iteration 160/1000 | Loss: 0.00000841
Iteration 161/1000 | Loss: 0.00000841
Iteration 162/1000 | Loss: 0.00000841
Iteration 163/1000 | Loss: 0.00000841
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [8.408725079789292e-06, 8.408725079789292e-06, 8.408725079789292e-06, 8.408725079789292e-06, 8.408725079789292e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.408725079789292e-06

Optimization complete. Final v2v error: 2.4122772216796875 mm

Highest mean error: 2.8174777030944824 mm for frame 97

Lowest mean error: 2.249882936477661 mm for frame 6

Saving results

Total time: 34.24038314819336
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_5533/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5533/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5533/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01063818
Iteration 2/25 | Loss: 0.00171237
Iteration 3/25 | Loss: 0.00158062
Iteration 4/25 | Loss: 0.00129853
Iteration 5/25 | Loss: 0.00123721
Iteration 6/25 | Loss: 0.00121878
Iteration 7/25 | Loss: 0.00112003
Iteration 8/25 | Loss: 0.00109678
Iteration 9/25 | Loss: 0.00108522
Iteration 10/25 | Loss: 0.00107205
Iteration 11/25 | Loss: 0.00107656
Iteration 12/25 | Loss: 0.00107783
Iteration 13/25 | Loss: 0.00107162
Iteration 14/25 | Loss: 0.00106803
Iteration 15/25 | Loss: 0.00107408
Iteration 16/25 | Loss: 0.00107297
Iteration 17/25 | Loss: 0.00107052
Iteration 18/25 | Loss: 0.00106794
Iteration 19/25 | Loss: 0.00106490
Iteration 20/25 | Loss: 0.00106232
Iteration 21/25 | Loss: 0.00106045
Iteration 22/25 | Loss: 0.00106016
Iteration 23/25 | Loss: 0.00105636
Iteration 24/25 | Loss: 0.00105977
Iteration 25/25 | Loss: 0.00105020

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29930413
Iteration 2/25 | Loss: 0.00221675
Iteration 3/25 | Loss: 0.00221675
Iteration 4/25 | Loss: 0.00221675
Iteration 5/25 | Loss: 0.00221675
Iteration 6/25 | Loss: 0.00221675
Iteration 7/25 | Loss: 0.00221675
Iteration 8/25 | Loss: 0.00221675
Iteration 9/25 | Loss: 0.00221675
Iteration 10/25 | Loss: 0.00221675
Iteration 11/25 | Loss: 0.00221675
Iteration 12/25 | Loss: 0.00221675
Iteration 13/25 | Loss: 0.00221675
Iteration 14/25 | Loss: 0.00221675
Iteration 15/25 | Loss: 0.00221675
Iteration 16/25 | Loss: 0.00221675
Iteration 17/25 | Loss: 0.00221675
Iteration 18/25 | Loss: 0.00221675
Iteration 19/25 | Loss: 0.00221675
Iteration 20/25 | Loss: 0.00221675
Iteration 21/25 | Loss: 0.00221675
Iteration 22/25 | Loss: 0.00221675
Iteration 23/25 | Loss: 0.00221675
Iteration 24/25 | Loss: 0.00221675
Iteration 25/25 | Loss: 0.00221675

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00221675
Iteration 2/1000 | Loss: 0.00064501
Iteration 3/1000 | Loss: 0.00006080
Iteration 4/1000 | Loss: 0.00020293
Iteration 5/1000 | Loss: 0.00004943
Iteration 6/1000 | Loss: 0.00012728
Iteration 7/1000 | Loss: 0.00013909
Iteration 8/1000 | Loss: 0.00009225
Iteration 9/1000 | Loss: 0.00004562
Iteration 10/1000 | Loss: 0.00008044
Iteration 11/1000 | Loss: 0.00002380
Iteration 12/1000 | Loss: 0.00002738
Iteration 13/1000 | Loss: 0.00003046
Iteration 14/1000 | Loss: 0.00008874
Iteration 15/1000 | Loss: 0.00007722
Iteration 16/1000 | Loss: 0.00010339
Iteration 17/1000 | Loss: 0.00006699
Iteration 18/1000 | Loss: 0.00002037
Iteration 19/1000 | Loss: 0.00001899
Iteration 20/1000 | Loss: 0.00015457
Iteration 21/1000 | Loss: 0.00004140
Iteration 22/1000 | Loss: 0.00003416
Iteration 23/1000 | Loss: 0.00013736
Iteration 24/1000 | Loss: 0.00013549
Iteration 25/1000 | Loss: 0.00008828
Iteration 26/1000 | Loss: 0.00007434
Iteration 27/1000 | Loss: 0.00018935
Iteration 28/1000 | Loss: 0.00002289
Iteration 29/1000 | Loss: 0.00002079
Iteration 30/1000 | Loss: 0.00003573
Iteration 31/1000 | Loss: 0.00034594
Iteration 32/1000 | Loss: 0.00018894
Iteration 33/1000 | Loss: 0.00027683
Iteration 34/1000 | Loss: 0.00015757
Iteration 35/1000 | Loss: 0.00007450
Iteration 36/1000 | Loss: 0.00005432
Iteration 37/1000 | Loss: 0.00006216
Iteration 38/1000 | Loss: 0.00036953
Iteration 39/1000 | Loss: 0.00030214
Iteration 40/1000 | Loss: 0.00002028
Iteration 41/1000 | Loss: 0.00001859
Iteration 42/1000 | Loss: 0.00001766
Iteration 43/1000 | Loss: 0.00001673
Iteration 44/1000 | Loss: 0.00001565
Iteration 45/1000 | Loss: 0.00001454
Iteration 46/1000 | Loss: 0.00001381
Iteration 47/1000 | Loss: 0.00001331
Iteration 48/1000 | Loss: 0.00024141
Iteration 49/1000 | Loss: 0.00035688
Iteration 50/1000 | Loss: 0.00025908
Iteration 51/1000 | Loss: 0.00010246
Iteration 52/1000 | Loss: 0.00002555
Iteration 53/1000 | Loss: 0.00002123
Iteration 54/1000 | Loss: 0.00001971
Iteration 55/1000 | Loss: 0.00001886
Iteration 56/1000 | Loss: 0.00001837
Iteration 57/1000 | Loss: 0.00020656
Iteration 58/1000 | Loss: 0.00010519
Iteration 59/1000 | Loss: 0.00002626
Iteration 60/1000 | Loss: 0.00015857
Iteration 61/1000 | Loss: 0.00001837
Iteration 62/1000 | Loss: 0.00046377
Iteration 63/1000 | Loss: 0.00014290
Iteration 64/1000 | Loss: 0.00005730
Iteration 65/1000 | Loss: 0.00013469
Iteration 66/1000 | Loss: 0.00032778
Iteration 67/1000 | Loss: 0.00043582
Iteration 68/1000 | Loss: 0.00014422
Iteration 69/1000 | Loss: 0.00032419
Iteration 70/1000 | Loss: 0.00029820
Iteration 71/1000 | Loss: 0.00014016
Iteration 72/1000 | Loss: 0.00006648
Iteration 73/1000 | Loss: 0.00003049
Iteration 74/1000 | Loss: 0.00001620
Iteration 75/1000 | Loss: 0.00001318
Iteration 76/1000 | Loss: 0.00001254
Iteration 77/1000 | Loss: 0.00001222
Iteration 78/1000 | Loss: 0.00001198
Iteration 79/1000 | Loss: 0.00001198
Iteration 80/1000 | Loss: 0.00001197
Iteration 81/1000 | Loss: 0.00001196
Iteration 82/1000 | Loss: 0.00001192
Iteration 83/1000 | Loss: 0.00001190
Iteration 84/1000 | Loss: 0.00001177
Iteration 85/1000 | Loss: 0.00001170
Iteration 86/1000 | Loss: 0.00001166
Iteration 87/1000 | Loss: 0.00001165
Iteration 88/1000 | Loss: 0.00001165
Iteration 89/1000 | Loss: 0.00001165
Iteration 90/1000 | Loss: 0.00001165
Iteration 91/1000 | Loss: 0.00001164
Iteration 92/1000 | Loss: 0.00001164
Iteration 93/1000 | Loss: 0.00001163
Iteration 94/1000 | Loss: 0.00001163
Iteration 95/1000 | Loss: 0.00001162
Iteration 96/1000 | Loss: 0.00001162
Iteration 97/1000 | Loss: 0.00001162
Iteration 98/1000 | Loss: 0.00001162
Iteration 99/1000 | Loss: 0.00001162
Iteration 100/1000 | Loss: 0.00001162
Iteration 101/1000 | Loss: 0.00001162
Iteration 102/1000 | Loss: 0.00001162
Iteration 103/1000 | Loss: 0.00001161
Iteration 104/1000 | Loss: 0.00001161
Iteration 105/1000 | Loss: 0.00001161
Iteration 106/1000 | Loss: 0.00001161
Iteration 107/1000 | Loss: 0.00001161
Iteration 108/1000 | Loss: 0.00001161
Iteration 109/1000 | Loss: 0.00001161
Iteration 110/1000 | Loss: 0.00001161
Iteration 111/1000 | Loss: 0.00001161
Iteration 112/1000 | Loss: 0.00001161
Iteration 113/1000 | Loss: 0.00001161
Iteration 114/1000 | Loss: 0.00001161
Iteration 115/1000 | Loss: 0.00001160
Iteration 116/1000 | Loss: 0.00001160
Iteration 117/1000 | Loss: 0.00001160
Iteration 118/1000 | Loss: 0.00001160
Iteration 119/1000 | Loss: 0.00001160
Iteration 120/1000 | Loss: 0.00001160
Iteration 121/1000 | Loss: 0.00001160
Iteration 122/1000 | Loss: 0.00001160
Iteration 123/1000 | Loss: 0.00001160
Iteration 124/1000 | Loss: 0.00001160
Iteration 125/1000 | Loss: 0.00001160
Iteration 126/1000 | Loss: 0.00001159
Iteration 127/1000 | Loss: 0.00001159
Iteration 128/1000 | Loss: 0.00001159
Iteration 129/1000 | Loss: 0.00001159
Iteration 130/1000 | Loss: 0.00001159
Iteration 131/1000 | Loss: 0.00001159
Iteration 132/1000 | Loss: 0.00001158
Iteration 133/1000 | Loss: 0.00001158
Iteration 134/1000 | Loss: 0.00001158
Iteration 135/1000 | Loss: 0.00001158
Iteration 136/1000 | Loss: 0.00001158
Iteration 137/1000 | Loss: 0.00001158
Iteration 138/1000 | Loss: 0.00001158
Iteration 139/1000 | Loss: 0.00001158
Iteration 140/1000 | Loss: 0.00001157
Iteration 141/1000 | Loss: 0.00001157
Iteration 142/1000 | Loss: 0.00001157
Iteration 143/1000 | Loss: 0.00001157
Iteration 144/1000 | Loss: 0.00001157
Iteration 145/1000 | Loss: 0.00001156
Iteration 146/1000 | Loss: 0.00001156
Iteration 147/1000 | Loss: 0.00001156
Iteration 148/1000 | Loss: 0.00001156
Iteration 149/1000 | Loss: 0.00001156
Iteration 150/1000 | Loss: 0.00001156
Iteration 151/1000 | Loss: 0.00001156
Iteration 152/1000 | Loss: 0.00001155
Iteration 153/1000 | Loss: 0.00001155
Iteration 154/1000 | Loss: 0.00001155
Iteration 155/1000 | Loss: 0.00001155
Iteration 156/1000 | Loss: 0.00001155
Iteration 157/1000 | Loss: 0.00001155
Iteration 158/1000 | Loss: 0.00001155
Iteration 159/1000 | Loss: 0.00001154
Iteration 160/1000 | Loss: 0.00001154
Iteration 161/1000 | Loss: 0.00001154
Iteration 162/1000 | Loss: 0.00001153
Iteration 163/1000 | Loss: 0.00001153
Iteration 164/1000 | Loss: 0.00001153
Iteration 165/1000 | Loss: 0.00001153
Iteration 166/1000 | Loss: 0.00001153
Iteration 167/1000 | Loss: 0.00001153
Iteration 168/1000 | Loss: 0.00001153
Iteration 169/1000 | Loss: 0.00001152
Iteration 170/1000 | Loss: 0.00001152
Iteration 171/1000 | Loss: 0.00001152
Iteration 172/1000 | Loss: 0.00001152
Iteration 173/1000 | Loss: 0.00001152
Iteration 174/1000 | Loss: 0.00001152
Iteration 175/1000 | Loss: 0.00001152
Iteration 176/1000 | Loss: 0.00001152
Iteration 177/1000 | Loss: 0.00001151
Iteration 178/1000 | Loss: 0.00001151
Iteration 179/1000 | Loss: 0.00001151
Iteration 180/1000 | Loss: 0.00001151
Iteration 181/1000 | Loss: 0.00001151
Iteration 182/1000 | Loss: 0.00001151
Iteration 183/1000 | Loss: 0.00001151
Iteration 184/1000 | Loss: 0.00001151
Iteration 185/1000 | Loss: 0.00001151
Iteration 186/1000 | Loss: 0.00001151
Iteration 187/1000 | Loss: 0.00001151
Iteration 188/1000 | Loss: 0.00001151
Iteration 189/1000 | Loss: 0.00001151
Iteration 190/1000 | Loss: 0.00001151
Iteration 191/1000 | Loss: 0.00001151
Iteration 192/1000 | Loss: 0.00001151
Iteration 193/1000 | Loss: 0.00001151
Iteration 194/1000 | Loss: 0.00001151
Iteration 195/1000 | Loss: 0.00001151
Iteration 196/1000 | Loss: 0.00001151
Iteration 197/1000 | Loss: 0.00001151
Iteration 198/1000 | Loss: 0.00001151
Iteration 199/1000 | Loss: 0.00001151
Iteration 200/1000 | Loss: 0.00001151
Iteration 201/1000 | Loss: 0.00001151
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [1.1508358511491679e-05, 1.1508358511491679e-05, 1.1508358511491679e-05, 1.1508358511491679e-05, 1.1508358511491679e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1508358511491679e-05

Optimization complete. Final v2v error: 2.7279839515686035 mm

Highest mean error: 4.939637660980225 mm for frame 102

Lowest mean error: 2.304194927215576 mm for frame 127

Saving results

Total time: 161.77997756004333
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_5533/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5533/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5533/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00992488
Iteration 2/25 | Loss: 0.00195785
Iteration 3/25 | Loss: 0.00142013
Iteration 4/25 | Loss: 0.00130806
Iteration 5/25 | Loss: 0.00127546
Iteration 6/25 | Loss: 0.00124956
Iteration 7/25 | Loss: 0.00122688
Iteration 8/25 | Loss: 0.00118086
Iteration 9/25 | Loss: 0.00115313
Iteration 10/25 | Loss: 0.00114688
Iteration 11/25 | Loss: 0.00113866
Iteration 12/25 | Loss: 0.00113837
Iteration 13/25 | Loss: 0.00113730
Iteration 14/25 | Loss: 0.00113269
Iteration 15/25 | Loss: 0.00113139
Iteration 16/25 | Loss: 0.00113093
Iteration 17/25 | Loss: 0.00113073
Iteration 18/25 | Loss: 0.00113061
Iteration 19/25 | Loss: 0.00113048
Iteration 20/25 | Loss: 0.00113030
Iteration 21/25 | Loss: 0.00113017
Iteration 22/25 | Loss: 0.00113008
Iteration 23/25 | Loss: 0.00112998
Iteration 24/25 | Loss: 0.00112984
Iteration 25/25 | Loss: 0.00112969

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25419438
Iteration 2/25 | Loss: 0.00290759
Iteration 3/25 | Loss: 0.00238010
Iteration 4/25 | Loss: 0.00238010
Iteration 5/25 | Loss: 0.00238010
Iteration 6/25 | Loss: 0.00238010
Iteration 7/25 | Loss: 0.00238010
Iteration 8/25 | Loss: 0.00238010
Iteration 9/25 | Loss: 0.00238010
Iteration 10/25 | Loss: 0.00238010
Iteration 11/25 | Loss: 0.00238010
Iteration 12/25 | Loss: 0.00238010
Iteration 13/25 | Loss: 0.00238010
Iteration 14/25 | Loss: 0.00238010
Iteration 15/25 | Loss: 0.00238010
Iteration 16/25 | Loss: 0.00238010
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.002380100078880787, 0.002380100078880787, 0.002380100078880787, 0.002380100078880787, 0.002380100078880787]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002380100078880787

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00238010
Iteration 2/1000 | Loss: 0.00008493
Iteration 3/1000 | Loss: 0.00048085
Iteration 4/1000 | Loss: 0.00022463
Iteration 5/1000 | Loss: 0.00003916
Iteration 6/1000 | Loss: 0.00015461
Iteration 7/1000 | Loss: 0.00009614
Iteration 8/1000 | Loss: 0.00003088
Iteration 9/1000 | Loss: 0.00002864
Iteration 10/1000 | Loss: 0.00002750
Iteration 11/1000 | Loss: 0.00002674
Iteration 12/1000 | Loss: 0.00002604
Iteration 13/1000 | Loss: 0.00002558
Iteration 14/1000 | Loss: 0.00002526
Iteration 15/1000 | Loss: 0.00044345
Iteration 16/1000 | Loss: 0.00065413
Iteration 17/1000 | Loss: 0.00044660
Iteration 18/1000 | Loss: 0.00004259
Iteration 19/1000 | Loss: 0.00003195
Iteration 20/1000 | Loss: 0.00026937
Iteration 21/1000 | Loss: 0.00032918
Iteration 22/1000 | Loss: 0.00002250
Iteration 23/1000 | Loss: 0.00018870
Iteration 24/1000 | Loss: 0.00002326
Iteration 25/1000 | Loss: 0.00001938
Iteration 26/1000 | Loss: 0.00001851
Iteration 27/1000 | Loss: 0.00001802
Iteration 28/1000 | Loss: 0.00001763
Iteration 29/1000 | Loss: 0.00001736
Iteration 30/1000 | Loss: 0.00001732
Iteration 31/1000 | Loss: 0.00033318
Iteration 32/1000 | Loss: 0.00001751
Iteration 33/1000 | Loss: 0.00001717
Iteration 34/1000 | Loss: 0.00001705
Iteration 35/1000 | Loss: 0.00001699
Iteration 36/1000 | Loss: 0.00001696
Iteration 37/1000 | Loss: 0.00001695
Iteration 38/1000 | Loss: 0.00001694
Iteration 39/1000 | Loss: 0.00001694
Iteration 40/1000 | Loss: 0.00001693
Iteration 41/1000 | Loss: 0.00001693
Iteration 42/1000 | Loss: 0.00001692
Iteration 43/1000 | Loss: 0.00001691
Iteration 44/1000 | Loss: 0.00001691
Iteration 45/1000 | Loss: 0.00001691
Iteration 46/1000 | Loss: 0.00001691
Iteration 47/1000 | Loss: 0.00001691
Iteration 48/1000 | Loss: 0.00001691
Iteration 49/1000 | Loss: 0.00001691
Iteration 50/1000 | Loss: 0.00001691
Iteration 51/1000 | Loss: 0.00001690
Iteration 52/1000 | Loss: 0.00001690
Iteration 53/1000 | Loss: 0.00001690
Iteration 54/1000 | Loss: 0.00001690
Iteration 55/1000 | Loss: 0.00001690
Iteration 56/1000 | Loss: 0.00001690
Iteration 57/1000 | Loss: 0.00001690
Iteration 58/1000 | Loss: 0.00001690
Iteration 59/1000 | Loss: 0.00001690
Iteration 60/1000 | Loss: 0.00001690
Iteration 61/1000 | Loss: 0.00001689
Iteration 62/1000 | Loss: 0.00001689
Iteration 63/1000 | Loss: 0.00001689
Iteration 64/1000 | Loss: 0.00001689
Iteration 65/1000 | Loss: 0.00001689
Iteration 66/1000 | Loss: 0.00001689
Iteration 67/1000 | Loss: 0.00001689
Iteration 68/1000 | Loss: 0.00001689
Iteration 69/1000 | Loss: 0.00001689
Iteration 70/1000 | Loss: 0.00001689
Iteration 71/1000 | Loss: 0.00001689
Iteration 72/1000 | Loss: 0.00001689
Iteration 73/1000 | Loss: 0.00001689
Iteration 74/1000 | Loss: 0.00001689
Iteration 75/1000 | Loss: 0.00001689
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 75. Stopping optimization.
Last 5 losses: [1.6885907825781032e-05, 1.6885907825781032e-05, 1.6885907825781032e-05, 1.6885907825781032e-05, 1.6885907825781032e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6885907825781032e-05

Optimization complete. Final v2v error: 3.486731767654419 mm

Highest mean error: 4.10774564743042 mm for frame 133

Lowest mean error: 3.171363592147827 mm for frame 205

Saving results

Total time: 108.51610064506531
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_5533/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5533/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5533/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01060081
Iteration 2/25 | Loss: 0.01060081
Iteration 3/25 | Loss: 0.01060080
Iteration 4/25 | Loss: 0.01060079
Iteration 5/25 | Loss: 0.00327279
Iteration 6/25 | Loss: 0.00225024
Iteration 7/25 | Loss: 0.00187538
Iteration 8/25 | Loss: 0.00168580
Iteration 9/25 | Loss: 0.00147595
Iteration 10/25 | Loss: 0.00136661
Iteration 11/25 | Loss: 0.00120618
Iteration 12/25 | Loss: 0.00114169
Iteration 13/25 | Loss: 0.00111177
Iteration 14/25 | Loss: 0.00110048
Iteration 15/25 | Loss: 0.00109371
Iteration 16/25 | Loss: 0.00109037
Iteration 17/25 | Loss: 0.00108901
Iteration 18/25 | Loss: 0.00108748
Iteration 19/25 | Loss: 0.00108713
Iteration 20/25 | Loss: 0.00109029
Iteration 21/25 | Loss: 0.00108853
Iteration 22/25 | Loss: 0.00108618
Iteration 23/25 | Loss: 0.00108545
Iteration 24/25 | Loss: 0.00108408
Iteration 25/25 | Loss: 0.00108362

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21824336
Iteration 2/25 | Loss: 0.00213726
Iteration 3/25 | Loss: 0.00213726
Iteration 4/25 | Loss: 0.00213726
Iteration 5/25 | Loss: 0.00213726
Iteration 6/25 | Loss: 0.00213726
Iteration 7/25 | Loss: 0.00213725
Iteration 8/25 | Loss: 0.00213725
Iteration 9/25 | Loss: 0.00213725
Iteration 10/25 | Loss: 0.00213725
Iteration 11/25 | Loss: 0.00213725
Iteration 12/25 | Loss: 0.00213725
Iteration 13/25 | Loss: 0.00213725
Iteration 14/25 | Loss: 0.00213725
Iteration 15/25 | Loss: 0.00213725
Iteration 16/25 | Loss: 0.00213725
Iteration 17/25 | Loss: 0.00213725
Iteration 18/25 | Loss: 0.00213725
Iteration 19/25 | Loss: 0.00213725
Iteration 20/25 | Loss: 0.00213725
Iteration 21/25 | Loss: 0.00213725
Iteration 22/25 | Loss: 0.00213725
Iteration 23/25 | Loss: 0.00213725
Iteration 24/25 | Loss: 0.00213725
Iteration 25/25 | Loss: 0.00213725

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00213725
Iteration 2/1000 | Loss: 0.00008188
Iteration 3/1000 | Loss: 0.00005927
Iteration 4/1000 | Loss: 0.00005016
Iteration 5/1000 | Loss: 0.00004568
Iteration 6/1000 | Loss: 0.00004367
Iteration 7/1000 | Loss: 0.00004157
Iteration 8/1000 | Loss: 0.00004018
Iteration 9/1000 | Loss: 0.00003905
Iteration 10/1000 | Loss: 0.00127887
Iteration 11/1000 | Loss: 0.00107544
Iteration 12/1000 | Loss: 0.00015383
Iteration 13/1000 | Loss: 0.00005877
Iteration 14/1000 | Loss: 0.00003852
Iteration 15/1000 | Loss: 0.00003930
Iteration 16/1000 | Loss: 0.00002870
Iteration 17/1000 | Loss: 0.00002577
Iteration 18/1000 | Loss: 0.00002384
Iteration 19/1000 | Loss: 0.00002401
Iteration 20/1000 | Loss: 0.00002236
Iteration 21/1000 | Loss: 0.00002150
Iteration 22/1000 | Loss: 0.00002095
Iteration 23/1000 | Loss: 0.00002037
Iteration 24/1000 | Loss: 0.00001994
Iteration 25/1000 | Loss: 0.00002116
Iteration 26/1000 | Loss: 0.00001990
Iteration 27/1000 | Loss: 0.00001934
Iteration 28/1000 | Loss: 0.00001922
Iteration 29/1000 | Loss: 0.00001921
Iteration 30/1000 | Loss: 0.00001914
Iteration 31/1000 | Loss: 0.00001911
Iteration 32/1000 | Loss: 0.00001910
Iteration 33/1000 | Loss: 0.00001910
Iteration 34/1000 | Loss: 0.00001909
Iteration 35/1000 | Loss: 0.00001908
Iteration 36/1000 | Loss: 0.00001908
Iteration 37/1000 | Loss: 0.00001907
Iteration 38/1000 | Loss: 0.00001907
Iteration 39/1000 | Loss: 0.00001906
Iteration 40/1000 | Loss: 0.00001905
Iteration 41/1000 | Loss: 0.00001904
Iteration 42/1000 | Loss: 0.00001904
Iteration 43/1000 | Loss: 0.00001904
Iteration 44/1000 | Loss: 0.00001903
Iteration 45/1000 | Loss: 0.00001903
Iteration 46/1000 | Loss: 0.00001902
Iteration 47/1000 | Loss: 0.00001902
Iteration 48/1000 | Loss: 0.00001902
Iteration 49/1000 | Loss: 0.00001901
Iteration 50/1000 | Loss: 0.00001900
Iteration 51/1000 | Loss: 0.00001900
Iteration 52/1000 | Loss: 0.00001900
Iteration 53/1000 | Loss: 0.00001900
Iteration 54/1000 | Loss: 0.00001900
Iteration 55/1000 | Loss: 0.00001900
Iteration 56/1000 | Loss: 0.00001900
Iteration 57/1000 | Loss: 0.00001899
Iteration 58/1000 | Loss: 0.00001899
Iteration 59/1000 | Loss: 0.00001899
Iteration 60/1000 | Loss: 0.00001898
Iteration 61/1000 | Loss: 0.00001896
Iteration 62/1000 | Loss: 0.00001896
Iteration 63/1000 | Loss: 0.00001896
Iteration 64/1000 | Loss: 0.00001895
Iteration 65/1000 | Loss: 0.00001895
Iteration 66/1000 | Loss: 0.00001895
Iteration 67/1000 | Loss: 0.00001895
Iteration 68/1000 | Loss: 0.00001895
Iteration 69/1000 | Loss: 0.00001895
Iteration 70/1000 | Loss: 0.00001894
Iteration 71/1000 | Loss: 0.00001894
Iteration 72/1000 | Loss: 0.00001894
Iteration 73/1000 | Loss: 0.00001894
Iteration 74/1000 | Loss: 0.00001894
Iteration 75/1000 | Loss: 0.00001894
Iteration 76/1000 | Loss: 0.00001894
Iteration 77/1000 | Loss: 0.00001894
Iteration 78/1000 | Loss: 0.00001894
Iteration 79/1000 | Loss: 0.00001893
Iteration 80/1000 | Loss: 0.00001893
Iteration 81/1000 | Loss: 0.00001893
Iteration 82/1000 | Loss: 0.00001893
Iteration 83/1000 | Loss: 0.00001893
Iteration 84/1000 | Loss: 0.00001893
Iteration 85/1000 | Loss: 0.00001893
Iteration 86/1000 | Loss: 0.00001893
Iteration 87/1000 | Loss: 0.00001893
Iteration 88/1000 | Loss: 0.00001892
Iteration 89/1000 | Loss: 0.00001892
Iteration 90/1000 | Loss: 0.00001892
Iteration 91/1000 | Loss: 0.00001892
Iteration 92/1000 | Loss: 0.00001892
Iteration 93/1000 | Loss: 0.00001892
Iteration 94/1000 | Loss: 0.00001892
Iteration 95/1000 | Loss: 0.00001892
Iteration 96/1000 | Loss: 0.00001891
Iteration 97/1000 | Loss: 0.00001891
Iteration 98/1000 | Loss: 0.00001891
Iteration 99/1000 | Loss: 0.00001891
Iteration 100/1000 | Loss: 0.00001891
Iteration 101/1000 | Loss: 0.00001891
Iteration 102/1000 | Loss: 0.00001891
Iteration 103/1000 | Loss: 0.00001891
Iteration 104/1000 | Loss: 0.00001891
Iteration 105/1000 | Loss: 0.00001891
Iteration 106/1000 | Loss: 0.00001891
Iteration 107/1000 | Loss: 0.00001891
Iteration 108/1000 | Loss: 0.00001891
Iteration 109/1000 | Loss: 0.00001891
Iteration 110/1000 | Loss: 0.00001891
Iteration 111/1000 | Loss: 0.00001890
Iteration 112/1000 | Loss: 0.00001890
Iteration 113/1000 | Loss: 0.00001890
Iteration 114/1000 | Loss: 0.00001890
Iteration 115/1000 | Loss: 0.00001890
Iteration 116/1000 | Loss: 0.00001890
Iteration 117/1000 | Loss: 0.00001890
Iteration 118/1000 | Loss: 0.00001890
Iteration 119/1000 | Loss: 0.00001890
Iteration 120/1000 | Loss: 0.00001890
Iteration 121/1000 | Loss: 0.00001890
Iteration 122/1000 | Loss: 0.00001890
Iteration 123/1000 | Loss: 0.00001890
Iteration 124/1000 | Loss: 0.00001890
Iteration 125/1000 | Loss: 0.00001890
Iteration 126/1000 | Loss: 0.00001890
Iteration 127/1000 | Loss: 0.00001890
Iteration 128/1000 | Loss: 0.00001890
Iteration 129/1000 | Loss: 0.00001890
Iteration 130/1000 | Loss: 0.00001890
Iteration 131/1000 | Loss: 0.00001890
Iteration 132/1000 | Loss: 0.00001890
Iteration 133/1000 | Loss: 0.00001890
Iteration 134/1000 | Loss: 0.00001890
Iteration 135/1000 | Loss: 0.00001890
Iteration 136/1000 | Loss: 0.00001890
Iteration 137/1000 | Loss: 0.00001890
Iteration 138/1000 | Loss: 0.00001890
Iteration 139/1000 | Loss: 0.00001890
Iteration 140/1000 | Loss: 0.00001890
Iteration 141/1000 | Loss: 0.00001890
Iteration 142/1000 | Loss: 0.00001890
Iteration 143/1000 | Loss: 0.00001890
Iteration 144/1000 | Loss: 0.00001890
Iteration 145/1000 | Loss: 0.00001890
Iteration 146/1000 | Loss: 0.00001890
Iteration 147/1000 | Loss: 0.00001890
Iteration 148/1000 | Loss: 0.00001890
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.890190651465673e-05, 1.890190651465673e-05, 1.890190651465673e-05, 1.890190651465673e-05, 1.890190651465673e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.890190651465673e-05

Optimization complete. Final v2v error: 3.1767473220825195 mm

Highest mean error: 20.477312088012695 mm for frame 215

Lowest mean error: 2.807785987854004 mm for frame 239

Saving results

Total time: 100.6683988571167
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_5533/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5533/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5533/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00838690
Iteration 2/25 | Loss: 0.00133669
Iteration 3/25 | Loss: 0.00117690
Iteration 4/25 | Loss: 0.00115322
Iteration 5/25 | Loss: 0.00114589
Iteration 6/25 | Loss: 0.00114400
Iteration 7/25 | Loss: 0.00114395
Iteration 8/25 | Loss: 0.00114395
Iteration 9/25 | Loss: 0.00114395
Iteration 10/25 | Loss: 0.00114395
Iteration 11/25 | Loss: 0.00114395
Iteration 12/25 | Loss: 0.00114395
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011439479421824217, 0.0011439479421824217, 0.0011439479421824217, 0.0011439479421824217, 0.0011439479421824217]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011439479421824217

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.30703902
Iteration 2/25 | Loss: 0.00290581
Iteration 3/25 | Loss: 0.00290581
Iteration 4/25 | Loss: 0.00290580
Iteration 5/25 | Loss: 0.00290580
Iteration 6/25 | Loss: 0.00290580
Iteration 7/25 | Loss: 0.00290580
Iteration 8/25 | Loss: 0.00290580
Iteration 9/25 | Loss: 0.00290580
Iteration 10/25 | Loss: 0.00290580
Iteration 11/25 | Loss: 0.00290580
Iteration 12/25 | Loss: 0.00290580
Iteration 13/25 | Loss: 0.00290580
Iteration 14/25 | Loss: 0.00290580
Iteration 15/25 | Loss: 0.00290580
Iteration 16/25 | Loss: 0.00290580
Iteration 17/25 | Loss: 0.00290580
Iteration 18/25 | Loss: 0.00290580
Iteration 19/25 | Loss: 0.00290580
Iteration 20/25 | Loss: 0.00290580
Iteration 21/25 | Loss: 0.00290580
Iteration 22/25 | Loss: 0.00290580
Iteration 23/25 | Loss: 0.00290580
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.002905802568420768, 0.002905802568420768, 0.002905802568420768, 0.002905802568420768, 0.002905802568420768]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002905802568420768

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00290580
Iteration 2/1000 | Loss: 0.00008294
Iteration 3/1000 | Loss: 0.00004227
Iteration 4/1000 | Loss: 0.00002742
Iteration 5/1000 | Loss: 0.00002334
Iteration 6/1000 | Loss: 0.00002150
Iteration 7/1000 | Loss: 0.00002046
Iteration 8/1000 | Loss: 0.00001995
Iteration 9/1000 | Loss: 0.00001947
Iteration 10/1000 | Loss: 0.00001903
Iteration 11/1000 | Loss: 0.00001873
Iteration 12/1000 | Loss: 0.00001852
Iteration 13/1000 | Loss: 0.00001831
Iteration 14/1000 | Loss: 0.00001829
Iteration 15/1000 | Loss: 0.00001825
Iteration 16/1000 | Loss: 0.00001823
Iteration 17/1000 | Loss: 0.00001818
Iteration 18/1000 | Loss: 0.00001814
Iteration 19/1000 | Loss: 0.00001812
Iteration 20/1000 | Loss: 0.00001809
Iteration 21/1000 | Loss: 0.00001807
Iteration 22/1000 | Loss: 0.00001806
Iteration 23/1000 | Loss: 0.00001804
Iteration 24/1000 | Loss: 0.00001801
Iteration 25/1000 | Loss: 0.00001795
Iteration 26/1000 | Loss: 0.00001794
Iteration 27/1000 | Loss: 0.00001793
Iteration 28/1000 | Loss: 0.00001793
Iteration 29/1000 | Loss: 0.00001792
Iteration 30/1000 | Loss: 0.00001791
Iteration 31/1000 | Loss: 0.00001791
Iteration 32/1000 | Loss: 0.00001791
Iteration 33/1000 | Loss: 0.00001791
Iteration 34/1000 | Loss: 0.00001790
Iteration 35/1000 | Loss: 0.00001790
Iteration 36/1000 | Loss: 0.00001790
Iteration 37/1000 | Loss: 0.00001789
Iteration 38/1000 | Loss: 0.00001787
Iteration 39/1000 | Loss: 0.00001787
Iteration 40/1000 | Loss: 0.00001786
Iteration 41/1000 | Loss: 0.00001785
Iteration 42/1000 | Loss: 0.00001785
Iteration 43/1000 | Loss: 0.00001784
Iteration 44/1000 | Loss: 0.00001784
Iteration 45/1000 | Loss: 0.00001784
Iteration 46/1000 | Loss: 0.00001783
Iteration 47/1000 | Loss: 0.00001783
Iteration 48/1000 | Loss: 0.00001782
Iteration 49/1000 | Loss: 0.00001782
Iteration 50/1000 | Loss: 0.00001782
Iteration 51/1000 | Loss: 0.00001781
Iteration 52/1000 | Loss: 0.00001781
Iteration 53/1000 | Loss: 0.00001781
Iteration 54/1000 | Loss: 0.00001781
Iteration 55/1000 | Loss: 0.00001781
Iteration 56/1000 | Loss: 0.00001781
Iteration 57/1000 | Loss: 0.00001780
Iteration 58/1000 | Loss: 0.00001780
Iteration 59/1000 | Loss: 0.00001779
Iteration 60/1000 | Loss: 0.00001779
Iteration 61/1000 | Loss: 0.00001779
Iteration 62/1000 | Loss: 0.00001779
Iteration 63/1000 | Loss: 0.00001779
Iteration 64/1000 | Loss: 0.00001779
Iteration 65/1000 | Loss: 0.00001778
Iteration 66/1000 | Loss: 0.00001778
Iteration 67/1000 | Loss: 0.00001778
Iteration 68/1000 | Loss: 0.00001778
Iteration 69/1000 | Loss: 0.00001778
Iteration 70/1000 | Loss: 0.00001777
Iteration 71/1000 | Loss: 0.00001777
Iteration 72/1000 | Loss: 0.00001777
Iteration 73/1000 | Loss: 0.00001777
Iteration 74/1000 | Loss: 0.00001777
Iteration 75/1000 | Loss: 0.00001777
Iteration 76/1000 | Loss: 0.00001776
Iteration 77/1000 | Loss: 0.00001776
Iteration 78/1000 | Loss: 0.00001776
Iteration 79/1000 | Loss: 0.00001776
Iteration 80/1000 | Loss: 0.00001776
Iteration 81/1000 | Loss: 0.00001775
Iteration 82/1000 | Loss: 0.00001775
Iteration 83/1000 | Loss: 0.00001775
Iteration 84/1000 | Loss: 0.00001774
Iteration 85/1000 | Loss: 0.00001774
Iteration 86/1000 | Loss: 0.00001774
Iteration 87/1000 | Loss: 0.00001773
Iteration 88/1000 | Loss: 0.00001773
Iteration 89/1000 | Loss: 0.00001773
Iteration 90/1000 | Loss: 0.00001773
Iteration 91/1000 | Loss: 0.00001772
Iteration 92/1000 | Loss: 0.00001772
Iteration 93/1000 | Loss: 0.00001772
Iteration 94/1000 | Loss: 0.00001772
Iteration 95/1000 | Loss: 0.00001771
Iteration 96/1000 | Loss: 0.00001771
Iteration 97/1000 | Loss: 0.00001771
Iteration 98/1000 | Loss: 0.00001771
Iteration 99/1000 | Loss: 0.00001771
Iteration 100/1000 | Loss: 0.00001771
Iteration 101/1000 | Loss: 0.00001770
Iteration 102/1000 | Loss: 0.00001770
Iteration 103/1000 | Loss: 0.00001770
Iteration 104/1000 | Loss: 0.00001770
Iteration 105/1000 | Loss: 0.00001769
Iteration 106/1000 | Loss: 0.00001769
Iteration 107/1000 | Loss: 0.00001769
Iteration 108/1000 | Loss: 0.00001768
Iteration 109/1000 | Loss: 0.00001768
Iteration 110/1000 | Loss: 0.00001768
Iteration 111/1000 | Loss: 0.00001768
Iteration 112/1000 | Loss: 0.00001768
Iteration 113/1000 | Loss: 0.00001768
Iteration 114/1000 | Loss: 0.00001767
Iteration 115/1000 | Loss: 0.00001767
Iteration 116/1000 | Loss: 0.00001767
Iteration 117/1000 | Loss: 0.00001767
Iteration 118/1000 | Loss: 0.00001767
Iteration 119/1000 | Loss: 0.00001766
Iteration 120/1000 | Loss: 0.00001766
Iteration 121/1000 | Loss: 0.00001766
Iteration 122/1000 | Loss: 0.00001766
Iteration 123/1000 | Loss: 0.00001766
Iteration 124/1000 | Loss: 0.00001766
Iteration 125/1000 | Loss: 0.00001766
Iteration 126/1000 | Loss: 0.00001766
Iteration 127/1000 | Loss: 0.00001766
Iteration 128/1000 | Loss: 0.00001766
Iteration 129/1000 | Loss: 0.00001766
Iteration 130/1000 | Loss: 0.00001766
Iteration 131/1000 | Loss: 0.00001766
Iteration 132/1000 | Loss: 0.00001766
Iteration 133/1000 | Loss: 0.00001766
Iteration 134/1000 | Loss: 0.00001766
Iteration 135/1000 | Loss: 0.00001766
Iteration 136/1000 | Loss: 0.00001766
Iteration 137/1000 | Loss: 0.00001766
Iteration 138/1000 | Loss: 0.00001766
Iteration 139/1000 | Loss: 0.00001766
Iteration 140/1000 | Loss: 0.00001766
Iteration 141/1000 | Loss: 0.00001766
Iteration 142/1000 | Loss: 0.00001766
Iteration 143/1000 | Loss: 0.00001766
Iteration 144/1000 | Loss: 0.00001766
Iteration 145/1000 | Loss: 0.00001766
Iteration 146/1000 | Loss: 0.00001766
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.7656568161328323e-05, 1.7656568161328323e-05, 1.7656568161328323e-05, 1.7656568161328323e-05, 1.7656568161328323e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7656568161328323e-05

Optimization complete. Final v2v error: 3.5200889110565186 mm

Highest mean error: 3.9340438842773438 mm for frame 53

Lowest mean error: 3.1970229148864746 mm for frame 88

Saving results

Total time: 39.552966356277466
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_5533/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5533/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5533/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00500339
Iteration 2/25 | Loss: 0.00138205
Iteration 3/25 | Loss: 0.00111370
Iteration 4/25 | Loss: 0.00108555
Iteration 5/25 | Loss: 0.00108268
Iteration 6/25 | Loss: 0.00108182
Iteration 7/25 | Loss: 0.00108182
Iteration 8/25 | Loss: 0.00108182
Iteration 9/25 | Loss: 0.00108182
Iteration 10/25 | Loss: 0.00108182
Iteration 11/25 | Loss: 0.00108182
Iteration 12/25 | Loss: 0.00108182
Iteration 13/25 | Loss: 0.00108182
Iteration 14/25 | Loss: 0.00108182
Iteration 15/25 | Loss: 0.00108182
Iteration 16/25 | Loss: 0.00108182
Iteration 17/25 | Loss: 0.00108182
Iteration 18/25 | Loss: 0.00108182
Iteration 19/25 | Loss: 0.00108182
Iteration 20/25 | Loss: 0.00108182
Iteration 21/25 | Loss: 0.00108182
Iteration 22/25 | Loss: 0.00108182
Iteration 23/25 | Loss: 0.00108182
Iteration 24/25 | Loss: 0.00108182
Iteration 25/25 | Loss: 0.00108182

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27171445
Iteration 2/25 | Loss: 0.00166092
Iteration 3/25 | Loss: 0.00166092
Iteration 4/25 | Loss: 0.00166092
Iteration 5/25 | Loss: 0.00166092
Iteration 6/25 | Loss: 0.00166092
Iteration 7/25 | Loss: 0.00166092
Iteration 8/25 | Loss: 0.00166092
Iteration 9/25 | Loss: 0.00166092
Iteration 10/25 | Loss: 0.00166092
Iteration 11/25 | Loss: 0.00166092
Iteration 12/25 | Loss: 0.00166092
Iteration 13/25 | Loss: 0.00166092
Iteration 14/25 | Loss: 0.00166092
Iteration 15/25 | Loss: 0.00166092
Iteration 16/25 | Loss: 0.00166092
Iteration 17/25 | Loss: 0.00166092
Iteration 18/25 | Loss: 0.00166092
Iteration 19/25 | Loss: 0.00166092
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0016609180020168424, 0.0016609180020168424, 0.0016609180020168424, 0.0016609180020168424, 0.0016609180020168424]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016609180020168424

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00166092
Iteration 2/1000 | Loss: 0.00005355
Iteration 3/1000 | Loss: 0.00003207
Iteration 4/1000 | Loss: 0.00002031
Iteration 5/1000 | Loss: 0.00001847
Iteration 6/1000 | Loss: 0.00001711
Iteration 7/1000 | Loss: 0.00001647
Iteration 8/1000 | Loss: 0.00001601
Iteration 9/1000 | Loss: 0.00001545
Iteration 10/1000 | Loss: 0.00001510
Iteration 11/1000 | Loss: 0.00001482
Iteration 12/1000 | Loss: 0.00001466
Iteration 13/1000 | Loss: 0.00001455
Iteration 14/1000 | Loss: 0.00001452
Iteration 15/1000 | Loss: 0.00001444
Iteration 16/1000 | Loss: 0.00001433
Iteration 17/1000 | Loss: 0.00001432
Iteration 18/1000 | Loss: 0.00001431
Iteration 19/1000 | Loss: 0.00001430
Iteration 20/1000 | Loss: 0.00001429
Iteration 21/1000 | Loss: 0.00001429
Iteration 22/1000 | Loss: 0.00001429
Iteration 23/1000 | Loss: 0.00001428
Iteration 24/1000 | Loss: 0.00001428
Iteration 25/1000 | Loss: 0.00001428
Iteration 26/1000 | Loss: 0.00001428
Iteration 27/1000 | Loss: 0.00001428
Iteration 28/1000 | Loss: 0.00001428
Iteration 29/1000 | Loss: 0.00001428
Iteration 30/1000 | Loss: 0.00001428
Iteration 31/1000 | Loss: 0.00001428
Iteration 32/1000 | Loss: 0.00001428
Iteration 33/1000 | Loss: 0.00001427
Iteration 34/1000 | Loss: 0.00001427
Iteration 35/1000 | Loss: 0.00001427
Iteration 36/1000 | Loss: 0.00001427
Iteration 37/1000 | Loss: 0.00001426
Iteration 38/1000 | Loss: 0.00001425
Iteration 39/1000 | Loss: 0.00001425
Iteration 40/1000 | Loss: 0.00001425
Iteration 41/1000 | Loss: 0.00001425
Iteration 42/1000 | Loss: 0.00001424
Iteration 43/1000 | Loss: 0.00001424
Iteration 44/1000 | Loss: 0.00001424
Iteration 45/1000 | Loss: 0.00001423
Iteration 46/1000 | Loss: 0.00001422
Iteration 47/1000 | Loss: 0.00001421
Iteration 48/1000 | Loss: 0.00001421
Iteration 49/1000 | Loss: 0.00001421
Iteration 50/1000 | Loss: 0.00001421
Iteration 51/1000 | Loss: 0.00001420
Iteration 52/1000 | Loss: 0.00001420
Iteration 53/1000 | Loss: 0.00001420
Iteration 54/1000 | Loss: 0.00001420
Iteration 55/1000 | Loss: 0.00001420
Iteration 56/1000 | Loss: 0.00001420
Iteration 57/1000 | Loss: 0.00001420
Iteration 58/1000 | Loss: 0.00001420
Iteration 59/1000 | Loss: 0.00001420
Iteration 60/1000 | Loss: 0.00001420
Iteration 61/1000 | Loss: 0.00001420
Iteration 62/1000 | Loss: 0.00001420
Iteration 63/1000 | Loss: 0.00001420
Iteration 64/1000 | Loss: 0.00001420
Iteration 65/1000 | Loss: 0.00001420
Iteration 66/1000 | Loss: 0.00001419
Iteration 67/1000 | Loss: 0.00001419
Iteration 68/1000 | Loss: 0.00001418
Iteration 69/1000 | Loss: 0.00001418
Iteration 70/1000 | Loss: 0.00001418
Iteration 71/1000 | Loss: 0.00001417
Iteration 72/1000 | Loss: 0.00001417
Iteration 73/1000 | Loss: 0.00001417
Iteration 74/1000 | Loss: 0.00001417
Iteration 75/1000 | Loss: 0.00001417
Iteration 76/1000 | Loss: 0.00001416
Iteration 77/1000 | Loss: 0.00001416
Iteration 78/1000 | Loss: 0.00001415
Iteration 79/1000 | Loss: 0.00001415
Iteration 80/1000 | Loss: 0.00001415
Iteration 81/1000 | Loss: 0.00001415
Iteration 82/1000 | Loss: 0.00001415
Iteration 83/1000 | Loss: 0.00001415
Iteration 84/1000 | Loss: 0.00001415
Iteration 85/1000 | Loss: 0.00001414
Iteration 86/1000 | Loss: 0.00001414
Iteration 87/1000 | Loss: 0.00001414
Iteration 88/1000 | Loss: 0.00001414
Iteration 89/1000 | Loss: 0.00001414
Iteration 90/1000 | Loss: 0.00001414
Iteration 91/1000 | Loss: 0.00001414
Iteration 92/1000 | Loss: 0.00001414
Iteration 93/1000 | Loss: 0.00001413
Iteration 94/1000 | Loss: 0.00001413
Iteration 95/1000 | Loss: 0.00001412
Iteration 96/1000 | Loss: 0.00001412
Iteration 97/1000 | Loss: 0.00001412
Iteration 98/1000 | Loss: 0.00001412
Iteration 99/1000 | Loss: 0.00001411
Iteration 100/1000 | Loss: 0.00001411
Iteration 101/1000 | Loss: 0.00001411
Iteration 102/1000 | Loss: 0.00001411
Iteration 103/1000 | Loss: 0.00001411
Iteration 104/1000 | Loss: 0.00001411
Iteration 105/1000 | Loss: 0.00001410
Iteration 106/1000 | Loss: 0.00001410
Iteration 107/1000 | Loss: 0.00001410
Iteration 108/1000 | Loss: 0.00001410
Iteration 109/1000 | Loss: 0.00001409
Iteration 110/1000 | Loss: 0.00001409
Iteration 111/1000 | Loss: 0.00001409
Iteration 112/1000 | Loss: 0.00001408
Iteration 113/1000 | Loss: 0.00001408
Iteration 114/1000 | Loss: 0.00001408
Iteration 115/1000 | Loss: 0.00001408
Iteration 116/1000 | Loss: 0.00001408
Iteration 117/1000 | Loss: 0.00001407
Iteration 118/1000 | Loss: 0.00001407
Iteration 119/1000 | Loss: 0.00001407
Iteration 120/1000 | Loss: 0.00001407
Iteration 121/1000 | Loss: 0.00001406
Iteration 122/1000 | Loss: 0.00001406
Iteration 123/1000 | Loss: 0.00001406
Iteration 124/1000 | Loss: 0.00001406
Iteration 125/1000 | Loss: 0.00001405
Iteration 126/1000 | Loss: 0.00001405
Iteration 127/1000 | Loss: 0.00001405
Iteration 128/1000 | Loss: 0.00001405
Iteration 129/1000 | Loss: 0.00001405
Iteration 130/1000 | Loss: 0.00001405
Iteration 131/1000 | Loss: 0.00001405
Iteration 132/1000 | Loss: 0.00001405
Iteration 133/1000 | Loss: 0.00001404
Iteration 134/1000 | Loss: 0.00001404
Iteration 135/1000 | Loss: 0.00001404
Iteration 136/1000 | Loss: 0.00001404
Iteration 137/1000 | Loss: 0.00001404
Iteration 138/1000 | Loss: 0.00001404
Iteration 139/1000 | Loss: 0.00001404
Iteration 140/1000 | Loss: 0.00001404
Iteration 141/1000 | Loss: 0.00001403
Iteration 142/1000 | Loss: 0.00001403
Iteration 143/1000 | Loss: 0.00001403
Iteration 144/1000 | Loss: 0.00001403
Iteration 145/1000 | Loss: 0.00001402
Iteration 146/1000 | Loss: 0.00001402
Iteration 147/1000 | Loss: 0.00001402
Iteration 148/1000 | Loss: 0.00001401
Iteration 149/1000 | Loss: 0.00001401
Iteration 150/1000 | Loss: 0.00001401
Iteration 151/1000 | Loss: 0.00001401
Iteration 152/1000 | Loss: 0.00001401
Iteration 153/1000 | Loss: 0.00001400
Iteration 154/1000 | Loss: 0.00001400
Iteration 155/1000 | Loss: 0.00001400
Iteration 156/1000 | Loss: 0.00001400
Iteration 157/1000 | Loss: 0.00001400
Iteration 158/1000 | Loss: 0.00001400
Iteration 159/1000 | Loss: 0.00001400
Iteration 160/1000 | Loss: 0.00001400
Iteration 161/1000 | Loss: 0.00001399
Iteration 162/1000 | Loss: 0.00001399
Iteration 163/1000 | Loss: 0.00001399
Iteration 164/1000 | Loss: 0.00001399
Iteration 165/1000 | Loss: 0.00001399
Iteration 166/1000 | Loss: 0.00001399
Iteration 167/1000 | Loss: 0.00001399
Iteration 168/1000 | Loss: 0.00001399
Iteration 169/1000 | Loss: 0.00001398
Iteration 170/1000 | Loss: 0.00001398
Iteration 171/1000 | Loss: 0.00001398
Iteration 172/1000 | Loss: 0.00001398
Iteration 173/1000 | Loss: 0.00001398
Iteration 174/1000 | Loss: 0.00001398
Iteration 175/1000 | Loss: 0.00001398
Iteration 176/1000 | Loss: 0.00001398
Iteration 177/1000 | Loss: 0.00001398
Iteration 178/1000 | Loss: 0.00001398
Iteration 179/1000 | Loss: 0.00001397
Iteration 180/1000 | Loss: 0.00001397
Iteration 181/1000 | Loss: 0.00001397
Iteration 182/1000 | Loss: 0.00001397
Iteration 183/1000 | Loss: 0.00001397
Iteration 184/1000 | Loss: 0.00001397
Iteration 185/1000 | Loss: 0.00001397
Iteration 186/1000 | Loss: 0.00001397
Iteration 187/1000 | Loss: 0.00001397
Iteration 188/1000 | Loss: 0.00001397
Iteration 189/1000 | Loss: 0.00001397
Iteration 190/1000 | Loss: 0.00001397
Iteration 191/1000 | Loss: 0.00001397
Iteration 192/1000 | Loss: 0.00001397
Iteration 193/1000 | Loss: 0.00001397
Iteration 194/1000 | Loss: 0.00001397
Iteration 195/1000 | Loss: 0.00001397
Iteration 196/1000 | Loss: 0.00001397
Iteration 197/1000 | Loss: 0.00001397
Iteration 198/1000 | Loss: 0.00001397
Iteration 199/1000 | Loss: 0.00001397
Iteration 200/1000 | Loss: 0.00001397
Iteration 201/1000 | Loss: 0.00001397
Iteration 202/1000 | Loss: 0.00001397
Iteration 203/1000 | Loss: 0.00001397
Iteration 204/1000 | Loss: 0.00001397
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [1.3966713595436886e-05, 1.3966713595436886e-05, 1.3966713595436886e-05, 1.3966713595436886e-05, 1.3966713595436886e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3966713595436886e-05

Optimization complete. Final v2v error: 3.1072845458984375 mm

Highest mean error: 4.105271816253662 mm for frame 76

Lowest mean error: 2.498929977416992 mm for frame 130

Saving results

Total time: 41.67683410644531
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_5533/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5533/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5533/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01010889
Iteration 2/25 | Loss: 0.00306463
Iteration 3/25 | Loss: 0.00221884
Iteration 4/25 | Loss: 0.00216063
Iteration 5/25 | Loss: 0.00217403
Iteration 6/25 | Loss: 0.00212729
Iteration 7/25 | Loss: 0.00212344
Iteration 8/25 | Loss: 0.00205429
Iteration 9/25 | Loss: 0.00199560
Iteration 10/25 | Loss: 0.00197108
Iteration 11/25 | Loss: 0.00194542
Iteration 12/25 | Loss: 0.00194396
Iteration 13/25 | Loss: 0.00193813
Iteration 14/25 | Loss: 0.00193382
Iteration 15/25 | Loss: 0.00192731
Iteration 16/25 | Loss: 0.00192324
Iteration 17/25 | Loss: 0.00192396
Iteration 18/25 | Loss: 0.00192306
Iteration 19/25 | Loss: 0.00192370
Iteration 20/25 | Loss: 0.00192239
Iteration 21/25 | Loss: 0.00192244
Iteration 22/25 | Loss: 0.00193292
Iteration 23/25 | Loss: 0.00191994
Iteration 24/25 | Loss: 0.00191608
Iteration 25/25 | Loss: 0.00190772

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14241707
Iteration 2/25 | Loss: 0.00896940
Iteration 3/25 | Loss: 0.00896940
Iteration 4/25 | Loss: 0.00896940
Iteration 5/25 | Loss: 0.00896940
Iteration 6/25 | Loss: 0.00896940
Iteration 7/25 | Loss: 0.00896940
Iteration 8/25 | Loss: 0.00896940
Iteration 9/25 | Loss: 0.00896940
Iteration 10/25 | Loss: 0.00896940
Iteration 11/25 | Loss: 0.00896940
Iteration 12/25 | Loss: 0.00896940
Iteration 13/25 | Loss: 0.00896940
Iteration 14/25 | Loss: 0.00896940
Iteration 15/25 | Loss: 0.00896940
Iteration 16/25 | Loss: 0.00896940
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00896939542144537, 0.00896939542144537, 0.00896939542144537, 0.00896939542144537, 0.00896939542144537]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00896939542144537

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00896940
Iteration 2/1000 | Loss: 0.00104295
Iteration 3/1000 | Loss: 0.00079440
Iteration 4/1000 | Loss: 0.00070302
Iteration 5/1000 | Loss: 0.00066689
Iteration 6/1000 | Loss: 0.00055022
Iteration 7/1000 | Loss: 0.00056175
Iteration 8/1000 | Loss: 0.00062825
Iteration 9/1000 | Loss: 0.00056810
Iteration 10/1000 | Loss: 0.00056367
Iteration 11/1000 | Loss: 0.00046065
Iteration 12/1000 | Loss: 0.00044444
Iteration 13/1000 | Loss: 0.00042790
Iteration 14/1000 | Loss: 0.00041262
Iteration 15/1000 | Loss: 0.00051834
Iteration 16/1000 | Loss: 0.00040265
Iteration 17/1000 | Loss: 0.00065203
Iteration 18/1000 | Loss: 0.00124856
Iteration 19/1000 | Loss: 0.02736810
Iteration 20/1000 | Loss: 0.00809740
Iteration 21/1000 | Loss: 0.00243480
Iteration 22/1000 | Loss: 0.00176961
Iteration 23/1000 | Loss: 0.00188689
Iteration 24/1000 | Loss: 0.00081698
Iteration 25/1000 | Loss: 0.00064937
Iteration 26/1000 | Loss: 0.00121902
Iteration 27/1000 | Loss: 0.00132501
Iteration 28/1000 | Loss: 0.00046627
Iteration 29/1000 | Loss: 0.00036982
Iteration 30/1000 | Loss: 0.00026326
Iteration 31/1000 | Loss: 0.00050580
Iteration 32/1000 | Loss: 0.00055212
Iteration 33/1000 | Loss: 0.00022336
Iteration 34/1000 | Loss: 0.00029093
Iteration 35/1000 | Loss: 0.00036410
Iteration 36/1000 | Loss: 0.00015285
Iteration 37/1000 | Loss: 0.00012817
Iteration 38/1000 | Loss: 0.00008139
Iteration 39/1000 | Loss: 0.00018374
Iteration 40/1000 | Loss: 0.00006359
Iteration 41/1000 | Loss: 0.00005361
Iteration 42/1000 | Loss: 0.00005911
Iteration 43/1000 | Loss: 0.00010973
Iteration 44/1000 | Loss: 0.00004656
Iteration 45/1000 | Loss: 0.00019307
Iteration 46/1000 | Loss: 0.00021472
Iteration 47/1000 | Loss: 0.00017323
Iteration 48/1000 | Loss: 0.00013974
Iteration 49/1000 | Loss: 0.00004875
Iteration 50/1000 | Loss: 0.00009453
Iteration 51/1000 | Loss: 0.00004509
Iteration 52/1000 | Loss: 0.00008225
Iteration 53/1000 | Loss: 0.00008878
Iteration 54/1000 | Loss: 0.00008482
Iteration 55/1000 | Loss: 0.00012070
Iteration 56/1000 | Loss: 0.00011608
Iteration 57/1000 | Loss: 0.00003133
Iteration 58/1000 | Loss: 0.00002565
Iteration 59/1000 | Loss: 0.00008933
Iteration 60/1000 | Loss: 0.00004131
Iteration 61/1000 | Loss: 0.00009188
Iteration 62/1000 | Loss: 0.00005457
Iteration 63/1000 | Loss: 0.00007673
Iteration 64/1000 | Loss: 0.00005922
Iteration 65/1000 | Loss: 0.00006204
Iteration 66/1000 | Loss: 0.00010843
Iteration 67/1000 | Loss: 0.00018649
Iteration 68/1000 | Loss: 0.00002585
Iteration 69/1000 | Loss: 0.00009936
Iteration 70/1000 | Loss: 0.00002323
Iteration 71/1000 | Loss: 0.00002139
Iteration 72/1000 | Loss: 0.00001916
Iteration 73/1000 | Loss: 0.00001762
Iteration 74/1000 | Loss: 0.00001654
Iteration 75/1000 | Loss: 0.00001570
Iteration 76/1000 | Loss: 0.00001540
Iteration 77/1000 | Loss: 0.00001532
Iteration 78/1000 | Loss: 0.00001522
Iteration 79/1000 | Loss: 0.00001506
Iteration 80/1000 | Loss: 0.00001485
Iteration 81/1000 | Loss: 0.00001481
Iteration 82/1000 | Loss: 0.00001476
Iteration 83/1000 | Loss: 0.00001476
Iteration 84/1000 | Loss: 0.00001475
Iteration 85/1000 | Loss: 0.00001475
Iteration 86/1000 | Loss: 0.00001473
Iteration 87/1000 | Loss: 0.00001472
Iteration 88/1000 | Loss: 0.00001470
Iteration 89/1000 | Loss: 0.00001469
Iteration 90/1000 | Loss: 0.00001469
Iteration 91/1000 | Loss: 0.00001467
Iteration 92/1000 | Loss: 0.00001467
Iteration 93/1000 | Loss: 0.00001467
Iteration 94/1000 | Loss: 0.00001466
Iteration 95/1000 | Loss: 0.00001466
Iteration 96/1000 | Loss: 0.00001466
Iteration 97/1000 | Loss: 0.00001466
Iteration 98/1000 | Loss: 0.00001466
Iteration 99/1000 | Loss: 0.00001466
Iteration 100/1000 | Loss: 0.00001466
Iteration 101/1000 | Loss: 0.00001466
Iteration 102/1000 | Loss: 0.00001466
Iteration 103/1000 | Loss: 0.00001466
Iteration 104/1000 | Loss: 0.00001466
Iteration 105/1000 | Loss: 0.00001465
Iteration 106/1000 | Loss: 0.00001465
Iteration 107/1000 | Loss: 0.00001465
Iteration 108/1000 | Loss: 0.00001465
Iteration 109/1000 | Loss: 0.00001465
Iteration 110/1000 | Loss: 0.00001464
Iteration 111/1000 | Loss: 0.00001464
Iteration 112/1000 | Loss: 0.00001464
Iteration 113/1000 | Loss: 0.00001464
Iteration 114/1000 | Loss: 0.00001464
Iteration 115/1000 | Loss: 0.00001464
Iteration 116/1000 | Loss: 0.00001463
Iteration 117/1000 | Loss: 0.00001463
Iteration 118/1000 | Loss: 0.00001462
Iteration 119/1000 | Loss: 0.00001462
Iteration 120/1000 | Loss: 0.00001461
Iteration 121/1000 | Loss: 0.00001461
Iteration 122/1000 | Loss: 0.00001460
Iteration 123/1000 | Loss: 0.00001460
Iteration 124/1000 | Loss: 0.00001460
Iteration 125/1000 | Loss: 0.00001459
Iteration 126/1000 | Loss: 0.00001459
Iteration 127/1000 | Loss: 0.00001459
Iteration 128/1000 | Loss: 0.00001459
Iteration 129/1000 | Loss: 0.00001459
Iteration 130/1000 | Loss: 0.00001459
Iteration 131/1000 | Loss: 0.00001459
Iteration 132/1000 | Loss: 0.00001459
Iteration 133/1000 | Loss: 0.00001458
Iteration 134/1000 | Loss: 0.00001458
Iteration 135/1000 | Loss: 0.00001458
Iteration 136/1000 | Loss: 0.00001458
Iteration 137/1000 | Loss: 0.00001458
Iteration 138/1000 | Loss: 0.00001458
Iteration 139/1000 | Loss: 0.00001458
Iteration 140/1000 | Loss: 0.00001458
Iteration 141/1000 | Loss: 0.00001458
Iteration 142/1000 | Loss: 0.00001458
Iteration 143/1000 | Loss: 0.00001457
Iteration 144/1000 | Loss: 0.00001457
Iteration 145/1000 | Loss: 0.00001457
Iteration 146/1000 | Loss: 0.00001457
Iteration 147/1000 | Loss: 0.00001457
Iteration 148/1000 | Loss: 0.00001457
Iteration 149/1000 | Loss: 0.00001457
Iteration 150/1000 | Loss: 0.00001457
Iteration 151/1000 | Loss: 0.00001457
Iteration 152/1000 | Loss: 0.00001457
Iteration 153/1000 | Loss: 0.00001457
Iteration 154/1000 | Loss: 0.00001457
Iteration 155/1000 | Loss: 0.00001457
Iteration 156/1000 | Loss: 0.00001457
Iteration 157/1000 | Loss: 0.00001457
Iteration 158/1000 | Loss: 0.00001457
Iteration 159/1000 | Loss: 0.00001457
Iteration 160/1000 | Loss: 0.00001457
Iteration 161/1000 | Loss: 0.00001456
Iteration 162/1000 | Loss: 0.00001456
Iteration 163/1000 | Loss: 0.00001456
Iteration 164/1000 | Loss: 0.00001456
Iteration 165/1000 | Loss: 0.00001456
Iteration 166/1000 | Loss: 0.00001456
Iteration 167/1000 | Loss: 0.00001456
Iteration 168/1000 | Loss: 0.00001456
Iteration 169/1000 | Loss: 0.00001456
Iteration 170/1000 | Loss: 0.00001456
Iteration 171/1000 | Loss: 0.00001456
Iteration 172/1000 | Loss: 0.00001456
Iteration 173/1000 | Loss: 0.00001456
Iteration 174/1000 | Loss: 0.00001456
Iteration 175/1000 | Loss: 0.00001456
Iteration 176/1000 | Loss: 0.00001456
Iteration 177/1000 | Loss: 0.00001455
Iteration 178/1000 | Loss: 0.00001455
Iteration 179/1000 | Loss: 0.00001455
Iteration 180/1000 | Loss: 0.00001455
Iteration 181/1000 | Loss: 0.00001455
Iteration 182/1000 | Loss: 0.00001455
Iteration 183/1000 | Loss: 0.00001455
Iteration 184/1000 | Loss: 0.00001455
Iteration 185/1000 | Loss: 0.00001455
Iteration 186/1000 | Loss: 0.00001455
Iteration 187/1000 | Loss: 0.00001455
Iteration 188/1000 | Loss: 0.00001455
Iteration 189/1000 | Loss: 0.00001455
Iteration 190/1000 | Loss: 0.00001455
Iteration 191/1000 | Loss: 0.00001455
Iteration 192/1000 | Loss: 0.00001455
Iteration 193/1000 | Loss: 0.00001455
Iteration 194/1000 | Loss: 0.00001455
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [1.4547495993610937e-05, 1.4547495993610937e-05, 1.4547495993610937e-05, 1.4547495993610937e-05, 1.4547495993610937e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4547495993610937e-05

Optimization complete. Final v2v error: 3.2186052799224854 mm

Highest mean error: 10.10970401763916 mm for frame 70

Lowest mean error: 3.037430763244629 mm for frame 21

Saving results

Total time: 186.58631563186646
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_5533/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5533/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5533/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00879250
Iteration 2/25 | Loss: 0.00119050
Iteration 3/25 | Loss: 0.00108014
Iteration 4/25 | Loss: 0.00105482
Iteration 5/25 | Loss: 0.00104624
Iteration 6/25 | Loss: 0.00104404
Iteration 7/25 | Loss: 0.00104404
Iteration 8/25 | Loss: 0.00104404
Iteration 9/25 | Loss: 0.00104404
Iteration 10/25 | Loss: 0.00104404
Iteration 11/25 | Loss: 0.00104404
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001044039148837328, 0.001044039148837328, 0.001044039148837328, 0.001044039148837328, 0.001044039148837328]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001044039148837328

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14930344
Iteration 2/25 | Loss: 0.00284396
Iteration 3/25 | Loss: 0.00284396
Iteration 4/25 | Loss: 0.00284396
Iteration 5/25 | Loss: 0.00284396
Iteration 6/25 | Loss: 0.00284396
Iteration 7/25 | Loss: 0.00284396
Iteration 8/25 | Loss: 0.00284396
Iteration 9/25 | Loss: 0.00284396
Iteration 10/25 | Loss: 0.00284396
Iteration 11/25 | Loss: 0.00284396
Iteration 12/25 | Loss: 0.00284396
Iteration 13/25 | Loss: 0.00284396
Iteration 14/25 | Loss: 0.00284396
Iteration 15/25 | Loss: 0.00284396
Iteration 16/25 | Loss: 0.00284396
Iteration 17/25 | Loss: 0.00284396
Iteration 18/25 | Loss: 0.00284395
Iteration 19/25 | Loss: 0.00284396
Iteration 20/25 | Loss: 0.00284395
Iteration 21/25 | Loss: 0.00284396
Iteration 22/25 | Loss: 0.00284396
Iteration 23/25 | Loss: 0.00284396
Iteration 24/25 | Loss: 0.00284396
Iteration 25/25 | Loss: 0.00284396

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00284396
Iteration 2/1000 | Loss: 0.00005002
Iteration 3/1000 | Loss: 0.00002774
Iteration 4/1000 | Loss: 0.00002064
Iteration 5/1000 | Loss: 0.00001923
Iteration 6/1000 | Loss: 0.00001802
Iteration 7/1000 | Loss: 0.00001738
Iteration 8/1000 | Loss: 0.00001676
Iteration 9/1000 | Loss: 0.00001629
Iteration 10/1000 | Loss: 0.00001593
Iteration 11/1000 | Loss: 0.00001592
Iteration 12/1000 | Loss: 0.00001577
Iteration 13/1000 | Loss: 0.00001565
Iteration 14/1000 | Loss: 0.00001561
Iteration 15/1000 | Loss: 0.00001561
Iteration 16/1000 | Loss: 0.00001560
Iteration 17/1000 | Loss: 0.00001557
Iteration 18/1000 | Loss: 0.00001557
Iteration 19/1000 | Loss: 0.00001556
Iteration 20/1000 | Loss: 0.00001553
Iteration 21/1000 | Loss: 0.00001549
Iteration 22/1000 | Loss: 0.00001548
Iteration 23/1000 | Loss: 0.00001547
Iteration 24/1000 | Loss: 0.00001546
Iteration 25/1000 | Loss: 0.00001546
Iteration 26/1000 | Loss: 0.00001545
Iteration 27/1000 | Loss: 0.00001545
Iteration 28/1000 | Loss: 0.00001544
Iteration 29/1000 | Loss: 0.00001544
Iteration 30/1000 | Loss: 0.00001543
Iteration 31/1000 | Loss: 0.00001542
Iteration 32/1000 | Loss: 0.00001541
Iteration 33/1000 | Loss: 0.00001541
Iteration 34/1000 | Loss: 0.00001540
Iteration 35/1000 | Loss: 0.00001540
Iteration 36/1000 | Loss: 0.00001540
Iteration 37/1000 | Loss: 0.00001540
Iteration 38/1000 | Loss: 0.00001539
Iteration 39/1000 | Loss: 0.00001538
Iteration 40/1000 | Loss: 0.00001537
Iteration 41/1000 | Loss: 0.00001537
Iteration 42/1000 | Loss: 0.00001537
Iteration 43/1000 | Loss: 0.00001537
Iteration 44/1000 | Loss: 0.00001536
Iteration 45/1000 | Loss: 0.00001536
Iteration 46/1000 | Loss: 0.00001536
Iteration 47/1000 | Loss: 0.00001535
Iteration 48/1000 | Loss: 0.00001535
Iteration 49/1000 | Loss: 0.00001534
Iteration 50/1000 | Loss: 0.00001534
Iteration 51/1000 | Loss: 0.00001533
Iteration 52/1000 | Loss: 0.00001533
Iteration 53/1000 | Loss: 0.00001533
Iteration 54/1000 | Loss: 0.00001533
Iteration 55/1000 | Loss: 0.00001533
Iteration 56/1000 | Loss: 0.00001532
Iteration 57/1000 | Loss: 0.00001532
Iteration 58/1000 | Loss: 0.00001532
Iteration 59/1000 | Loss: 0.00001532
Iteration 60/1000 | Loss: 0.00001532
Iteration 61/1000 | Loss: 0.00001532
Iteration 62/1000 | Loss: 0.00001531
Iteration 63/1000 | Loss: 0.00001531
Iteration 64/1000 | Loss: 0.00001531
Iteration 65/1000 | Loss: 0.00001530
Iteration 66/1000 | Loss: 0.00001529
Iteration 67/1000 | Loss: 0.00001529
Iteration 68/1000 | Loss: 0.00001529
Iteration 69/1000 | Loss: 0.00001529
Iteration 70/1000 | Loss: 0.00001528
Iteration 71/1000 | Loss: 0.00001528
Iteration 72/1000 | Loss: 0.00001528
Iteration 73/1000 | Loss: 0.00001527
Iteration 74/1000 | Loss: 0.00001527
Iteration 75/1000 | Loss: 0.00001526
Iteration 76/1000 | Loss: 0.00001526
Iteration 77/1000 | Loss: 0.00001526
Iteration 78/1000 | Loss: 0.00001526
Iteration 79/1000 | Loss: 0.00001526
Iteration 80/1000 | Loss: 0.00001526
Iteration 81/1000 | Loss: 0.00001526
Iteration 82/1000 | Loss: 0.00001526
Iteration 83/1000 | Loss: 0.00001526
Iteration 84/1000 | Loss: 0.00001526
Iteration 85/1000 | Loss: 0.00001526
Iteration 86/1000 | Loss: 0.00001526
Iteration 87/1000 | Loss: 0.00001526
Iteration 88/1000 | Loss: 0.00001526
Iteration 89/1000 | Loss: 0.00001526
Iteration 90/1000 | Loss: 0.00001526
Iteration 91/1000 | Loss: 0.00001526
Iteration 92/1000 | Loss: 0.00001526
Iteration 93/1000 | Loss: 0.00001526
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [1.5262321539921686e-05, 1.5262321539921686e-05, 1.5262321539921686e-05, 1.5262321539921686e-05, 1.5262321539921686e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5262321539921686e-05

Optimization complete. Final v2v error: 3.3007261753082275 mm

Highest mean error: 3.8742194175720215 mm for frame 178

Lowest mean error: 2.870920419692993 mm for frame 133

Saving results

Total time: 37.16100788116455
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_5533/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5533/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5533/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00994920
Iteration 2/25 | Loss: 0.00169566
Iteration 3/25 | Loss: 0.00128210
Iteration 4/25 | Loss: 0.00125805
Iteration 5/25 | Loss: 0.00125139
Iteration 6/25 | Loss: 0.00124969
Iteration 7/25 | Loss: 0.00124969
Iteration 8/25 | Loss: 0.00124969
Iteration 9/25 | Loss: 0.00124969
Iteration 10/25 | Loss: 0.00124969
Iteration 11/25 | Loss: 0.00124969
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012496933341026306, 0.0012496933341026306, 0.0012496933341026306, 0.0012496933341026306, 0.0012496933341026306]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012496933341026306

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.49619365
Iteration 2/25 | Loss: 0.00166111
Iteration 3/25 | Loss: 0.00166111
Iteration 4/25 | Loss: 0.00166111
Iteration 5/25 | Loss: 0.00166111
Iteration 6/25 | Loss: 0.00166111
Iteration 7/25 | Loss: 0.00166111
Iteration 8/25 | Loss: 0.00166111
Iteration 9/25 | Loss: 0.00166111
Iteration 10/25 | Loss: 0.00166111
Iteration 11/25 | Loss: 0.00166111
Iteration 12/25 | Loss: 0.00166111
Iteration 13/25 | Loss: 0.00166111
Iteration 14/25 | Loss: 0.00166111
Iteration 15/25 | Loss: 0.00166111
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0016611055471003056, 0.0016611055471003056, 0.0016611055471003056, 0.0016611055471003056, 0.0016611055471003056]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016611055471003056

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00166111
Iteration 2/1000 | Loss: 0.00007615
Iteration 3/1000 | Loss: 0.00005263
Iteration 4/1000 | Loss: 0.00004611
Iteration 5/1000 | Loss: 0.00004322
Iteration 6/1000 | Loss: 0.00004202
Iteration 7/1000 | Loss: 0.00004071
Iteration 8/1000 | Loss: 0.00004004
Iteration 9/1000 | Loss: 0.00003967
Iteration 10/1000 | Loss: 0.00003933
Iteration 11/1000 | Loss: 0.00003903
Iteration 12/1000 | Loss: 0.00003883
Iteration 13/1000 | Loss: 0.00003864
Iteration 14/1000 | Loss: 0.00003844
Iteration 15/1000 | Loss: 0.00003828
Iteration 16/1000 | Loss: 0.00003817
Iteration 17/1000 | Loss: 0.00003809
Iteration 18/1000 | Loss: 0.00003808
Iteration 19/1000 | Loss: 0.00003808
Iteration 20/1000 | Loss: 0.00003802
Iteration 21/1000 | Loss: 0.00003799
Iteration 22/1000 | Loss: 0.00003786
Iteration 23/1000 | Loss: 0.00003773
Iteration 24/1000 | Loss: 0.00003768
Iteration 25/1000 | Loss: 0.00003764
Iteration 26/1000 | Loss: 0.00003764
Iteration 27/1000 | Loss: 0.00003761
Iteration 28/1000 | Loss: 0.00003759
Iteration 29/1000 | Loss: 0.00003759
Iteration 30/1000 | Loss: 0.00003756
Iteration 31/1000 | Loss: 0.00003752
Iteration 32/1000 | Loss: 0.00003752
Iteration 33/1000 | Loss: 0.00003752
Iteration 34/1000 | Loss: 0.00003752
Iteration 35/1000 | Loss: 0.00003751
Iteration 36/1000 | Loss: 0.00003751
Iteration 37/1000 | Loss: 0.00003751
Iteration 38/1000 | Loss: 0.00003750
Iteration 39/1000 | Loss: 0.00003750
Iteration 40/1000 | Loss: 0.00003748
Iteration 41/1000 | Loss: 0.00003748
Iteration 42/1000 | Loss: 0.00003748
Iteration 43/1000 | Loss: 0.00003748
Iteration 44/1000 | Loss: 0.00003747
Iteration 45/1000 | Loss: 0.00003747
Iteration 46/1000 | Loss: 0.00003747
Iteration 47/1000 | Loss: 0.00003747
Iteration 48/1000 | Loss: 0.00003747
Iteration 49/1000 | Loss: 0.00003746
Iteration 50/1000 | Loss: 0.00003746
Iteration 51/1000 | Loss: 0.00003745
Iteration 52/1000 | Loss: 0.00003745
Iteration 53/1000 | Loss: 0.00003745
Iteration 54/1000 | Loss: 0.00003745
Iteration 55/1000 | Loss: 0.00003745
Iteration 56/1000 | Loss: 0.00003745
Iteration 57/1000 | Loss: 0.00003745
Iteration 58/1000 | Loss: 0.00003745
Iteration 59/1000 | Loss: 0.00003745
Iteration 60/1000 | Loss: 0.00003745
Iteration 61/1000 | Loss: 0.00003745
Iteration 62/1000 | Loss: 0.00003745
Iteration 63/1000 | Loss: 0.00003745
Iteration 64/1000 | Loss: 0.00003745
Iteration 65/1000 | Loss: 0.00003745
Iteration 66/1000 | Loss: 0.00003745
Iteration 67/1000 | Loss: 0.00003745
Iteration 68/1000 | Loss: 0.00003745
Iteration 69/1000 | Loss: 0.00003744
Iteration 70/1000 | Loss: 0.00003744
Iteration 71/1000 | Loss: 0.00003743
Iteration 72/1000 | Loss: 0.00003743
Iteration 73/1000 | Loss: 0.00003742
Iteration 74/1000 | Loss: 0.00003742
Iteration 75/1000 | Loss: 0.00003742
Iteration 76/1000 | Loss: 0.00003742
Iteration 77/1000 | Loss: 0.00003742
Iteration 78/1000 | Loss: 0.00003742
Iteration 79/1000 | Loss: 0.00003741
Iteration 80/1000 | Loss: 0.00003741
Iteration 81/1000 | Loss: 0.00003741
Iteration 82/1000 | Loss: 0.00003741
Iteration 83/1000 | Loss: 0.00003741
Iteration 84/1000 | Loss: 0.00003741
Iteration 85/1000 | Loss: 0.00003741
Iteration 86/1000 | Loss: 0.00003740
Iteration 87/1000 | Loss: 0.00003740
Iteration 88/1000 | Loss: 0.00003740
Iteration 89/1000 | Loss: 0.00003740
Iteration 90/1000 | Loss: 0.00003740
Iteration 91/1000 | Loss: 0.00003740
Iteration 92/1000 | Loss: 0.00003740
Iteration 93/1000 | Loss: 0.00003739
Iteration 94/1000 | Loss: 0.00003739
Iteration 95/1000 | Loss: 0.00003739
Iteration 96/1000 | Loss: 0.00003738
Iteration 97/1000 | Loss: 0.00003738
Iteration 98/1000 | Loss: 0.00003738
Iteration 99/1000 | Loss: 0.00003738
Iteration 100/1000 | Loss: 0.00003738
Iteration 101/1000 | Loss: 0.00003738
Iteration 102/1000 | Loss: 0.00003738
Iteration 103/1000 | Loss: 0.00003738
Iteration 104/1000 | Loss: 0.00003738
Iteration 105/1000 | Loss: 0.00003738
Iteration 106/1000 | Loss: 0.00003738
Iteration 107/1000 | Loss: 0.00003737
Iteration 108/1000 | Loss: 0.00003737
Iteration 109/1000 | Loss: 0.00003737
Iteration 110/1000 | Loss: 0.00003737
Iteration 111/1000 | Loss: 0.00003737
Iteration 112/1000 | Loss: 0.00003737
Iteration 113/1000 | Loss: 0.00003737
Iteration 114/1000 | Loss: 0.00003737
Iteration 115/1000 | Loss: 0.00003737
Iteration 116/1000 | Loss: 0.00003737
Iteration 117/1000 | Loss: 0.00003737
Iteration 118/1000 | Loss: 0.00003737
Iteration 119/1000 | Loss: 0.00003736
Iteration 120/1000 | Loss: 0.00003736
Iteration 121/1000 | Loss: 0.00003736
Iteration 122/1000 | Loss: 0.00003736
Iteration 123/1000 | Loss: 0.00003736
Iteration 124/1000 | Loss: 0.00003736
Iteration 125/1000 | Loss: 0.00003736
Iteration 126/1000 | Loss: 0.00003736
Iteration 127/1000 | Loss: 0.00003736
Iteration 128/1000 | Loss: 0.00003736
Iteration 129/1000 | Loss: 0.00003735
Iteration 130/1000 | Loss: 0.00003735
Iteration 131/1000 | Loss: 0.00003735
Iteration 132/1000 | Loss: 0.00003735
Iteration 133/1000 | Loss: 0.00003735
Iteration 134/1000 | Loss: 0.00003735
Iteration 135/1000 | Loss: 0.00003735
Iteration 136/1000 | Loss: 0.00003735
Iteration 137/1000 | Loss: 0.00003735
Iteration 138/1000 | Loss: 0.00003735
Iteration 139/1000 | Loss: 0.00003735
Iteration 140/1000 | Loss: 0.00003735
Iteration 141/1000 | Loss: 0.00003735
Iteration 142/1000 | Loss: 0.00003735
Iteration 143/1000 | Loss: 0.00003735
Iteration 144/1000 | Loss: 0.00003735
Iteration 145/1000 | Loss: 0.00003734
Iteration 146/1000 | Loss: 0.00003734
Iteration 147/1000 | Loss: 0.00003734
Iteration 148/1000 | Loss: 0.00003734
Iteration 149/1000 | Loss: 0.00003734
Iteration 150/1000 | Loss: 0.00003734
Iteration 151/1000 | Loss: 0.00003734
Iteration 152/1000 | Loss: 0.00003734
Iteration 153/1000 | Loss: 0.00003733
Iteration 154/1000 | Loss: 0.00003733
Iteration 155/1000 | Loss: 0.00003733
Iteration 156/1000 | Loss: 0.00003733
Iteration 157/1000 | Loss: 0.00003733
Iteration 158/1000 | Loss: 0.00003733
Iteration 159/1000 | Loss: 0.00003733
Iteration 160/1000 | Loss: 0.00003733
Iteration 161/1000 | Loss: 0.00003733
Iteration 162/1000 | Loss: 0.00003733
Iteration 163/1000 | Loss: 0.00003733
Iteration 164/1000 | Loss: 0.00003733
Iteration 165/1000 | Loss: 0.00003733
Iteration 166/1000 | Loss: 0.00003733
Iteration 167/1000 | Loss: 0.00003733
Iteration 168/1000 | Loss: 0.00003732
Iteration 169/1000 | Loss: 0.00003732
Iteration 170/1000 | Loss: 0.00003732
Iteration 171/1000 | Loss: 0.00003732
Iteration 172/1000 | Loss: 0.00003732
Iteration 173/1000 | Loss: 0.00003732
Iteration 174/1000 | Loss: 0.00003732
Iteration 175/1000 | Loss: 0.00003732
Iteration 176/1000 | Loss: 0.00003732
Iteration 177/1000 | Loss: 0.00003732
Iteration 178/1000 | Loss: 0.00003731
Iteration 179/1000 | Loss: 0.00003731
Iteration 180/1000 | Loss: 0.00003731
Iteration 181/1000 | Loss: 0.00003731
Iteration 182/1000 | Loss: 0.00003731
Iteration 183/1000 | Loss: 0.00003731
Iteration 184/1000 | Loss: 0.00003731
Iteration 185/1000 | Loss: 0.00003731
Iteration 186/1000 | Loss: 0.00003731
Iteration 187/1000 | Loss: 0.00003731
Iteration 188/1000 | Loss: 0.00003731
Iteration 189/1000 | Loss: 0.00003731
Iteration 190/1000 | Loss: 0.00003731
Iteration 191/1000 | Loss: 0.00003731
Iteration 192/1000 | Loss: 0.00003731
Iteration 193/1000 | Loss: 0.00003731
Iteration 194/1000 | Loss: 0.00003730
Iteration 195/1000 | Loss: 0.00003730
Iteration 196/1000 | Loss: 0.00003730
Iteration 197/1000 | Loss: 0.00003730
Iteration 198/1000 | Loss: 0.00003730
Iteration 199/1000 | Loss: 0.00003730
Iteration 200/1000 | Loss: 0.00003730
Iteration 201/1000 | Loss: 0.00003730
Iteration 202/1000 | Loss: 0.00003730
Iteration 203/1000 | Loss: 0.00003730
Iteration 204/1000 | Loss: 0.00003730
Iteration 205/1000 | Loss: 0.00003730
Iteration 206/1000 | Loss: 0.00003729
Iteration 207/1000 | Loss: 0.00003729
Iteration 208/1000 | Loss: 0.00003729
Iteration 209/1000 | Loss: 0.00003729
Iteration 210/1000 | Loss: 0.00003729
Iteration 211/1000 | Loss: 0.00003729
Iteration 212/1000 | Loss: 0.00003729
Iteration 213/1000 | Loss: 0.00003729
Iteration 214/1000 | Loss: 0.00003729
Iteration 215/1000 | Loss: 0.00003729
Iteration 216/1000 | Loss: 0.00003728
Iteration 217/1000 | Loss: 0.00003728
Iteration 218/1000 | Loss: 0.00003728
Iteration 219/1000 | Loss: 0.00003728
Iteration 220/1000 | Loss: 0.00003728
Iteration 221/1000 | Loss: 0.00003728
Iteration 222/1000 | Loss: 0.00003728
Iteration 223/1000 | Loss: 0.00003728
Iteration 224/1000 | Loss: 0.00003727
Iteration 225/1000 | Loss: 0.00003727
Iteration 226/1000 | Loss: 0.00003727
Iteration 227/1000 | Loss: 0.00003727
Iteration 228/1000 | Loss: 0.00003727
Iteration 229/1000 | Loss: 0.00003727
Iteration 230/1000 | Loss: 0.00003727
Iteration 231/1000 | Loss: 0.00003727
Iteration 232/1000 | Loss: 0.00003727
Iteration 233/1000 | Loss: 0.00003727
Iteration 234/1000 | Loss: 0.00003726
Iteration 235/1000 | Loss: 0.00003726
Iteration 236/1000 | Loss: 0.00003726
Iteration 237/1000 | Loss: 0.00003726
Iteration 238/1000 | Loss: 0.00003726
Iteration 239/1000 | Loss: 0.00003726
Iteration 240/1000 | Loss: 0.00003726
Iteration 241/1000 | Loss: 0.00003726
Iteration 242/1000 | Loss: 0.00003726
Iteration 243/1000 | Loss: 0.00003726
Iteration 244/1000 | Loss: 0.00003726
Iteration 245/1000 | Loss: 0.00003726
Iteration 246/1000 | Loss: 0.00003726
Iteration 247/1000 | Loss: 0.00003726
Iteration 248/1000 | Loss: 0.00003726
Iteration 249/1000 | Loss: 0.00003726
Iteration 250/1000 | Loss: 0.00003726
Iteration 251/1000 | Loss: 0.00003726
Iteration 252/1000 | Loss: 0.00003726
Iteration 253/1000 | Loss: 0.00003726
Iteration 254/1000 | Loss: 0.00003726
Iteration 255/1000 | Loss: 0.00003726
Iteration 256/1000 | Loss: 0.00003726
Iteration 257/1000 | Loss: 0.00003726
Iteration 258/1000 | Loss: 0.00003726
Iteration 259/1000 | Loss: 0.00003726
Iteration 260/1000 | Loss: 0.00003726
Iteration 261/1000 | Loss: 0.00003726
Iteration 262/1000 | Loss: 0.00003726
Iteration 263/1000 | Loss: 0.00003726
Iteration 264/1000 | Loss: 0.00003726
Iteration 265/1000 | Loss: 0.00003726
Iteration 266/1000 | Loss: 0.00003726
Iteration 267/1000 | Loss: 0.00003726
Iteration 268/1000 | Loss: 0.00003726
Iteration 269/1000 | Loss: 0.00003726
Iteration 270/1000 | Loss: 0.00003726
Iteration 271/1000 | Loss: 0.00003726
Iteration 272/1000 | Loss: 0.00003726
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 272. Stopping optimization.
Last 5 losses: [3.7256188079481944e-05, 3.7256188079481944e-05, 3.7256188079481944e-05, 3.7256188079481944e-05, 3.7256188079481944e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.7256188079481944e-05

Optimization complete. Final v2v error: 4.8844685554504395 mm

Highest mean error: 5.878016948699951 mm for frame 6

Lowest mean error: 4.494316577911377 mm for frame 66

Saving results

Total time: 52.986260652542114
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_5533/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5533/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5533/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01156593
Iteration 2/25 | Loss: 0.00186496
Iteration 3/25 | Loss: 0.00136159
Iteration 4/25 | Loss: 0.00131877
Iteration 5/25 | Loss: 0.00131094
Iteration 6/25 | Loss: 0.00130767
Iteration 7/25 | Loss: 0.00130565
Iteration 8/25 | Loss: 0.00131107
Iteration 9/25 | Loss: 0.00130966
Iteration 10/25 | Loss: 0.00130651
Iteration 11/25 | Loss: 0.00130415
Iteration 12/25 | Loss: 0.00130362
Iteration 13/25 | Loss: 0.00130279
Iteration 14/25 | Loss: 0.00129939
Iteration 15/25 | Loss: 0.00130376
Iteration 16/25 | Loss: 0.00130396
Iteration 17/25 | Loss: 0.00130289
Iteration 18/25 | Loss: 0.00130166
Iteration 19/25 | Loss: 0.00130633
Iteration 20/25 | Loss: 0.00130265
Iteration 21/25 | Loss: 0.00129978
Iteration 22/25 | Loss: 0.00130189
Iteration 23/25 | Loss: 0.00130000
Iteration 24/25 | Loss: 0.00130102
Iteration 25/25 | Loss: 0.00129857

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.94662499
Iteration 2/25 | Loss: 0.00222519
Iteration 3/25 | Loss: 0.00222517
Iteration 4/25 | Loss: 0.00222517
Iteration 5/25 | Loss: 0.00222517
Iteration 6/25 | Loss: 0.00222516
Iteration 7/25 | Loss: 0.00222516
Iteration 8/25 | Loss: 0.00222516
Iteration 9/25 | Loss: 0.00222516
Iteration 10/25 | Loss: 0.00222516
Iteration 11/25 | Loss: 0.00222516
Iteration 12/25 | Loss: 0.00222516
Iteration 13/25 | Loss: 0.00222516
Iteration 14/25 | Loss: 0.00222516
Iteration 15/25 | Loss: 0.00222516
Iteration 16/25 | Loss: 0.00222516
Iteration 17/25 | Loss: 0.00222516
Iteration 18/25 | Loss: 0.00222516
Iteration 19/25 | Loss: 0.00222516
Iteration 20/25 | Loss: 0.00222516
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.002225162461400032, 0.002225162461400032, 0.002225162461400032, 0.002225162461400032, 0.002225162461400032]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002225162461400032

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00222516
Iteration 2/1000 | Loss: 0.00041564
Iteration 3/1000 | Loss: 0.00043146
Iteration 4/1000 | Loss: 0.00043650
Iteration 5/1000 | Loss: 0.00045243
Iteration 6/1000 | Loss: 0.00031810
Iteration 7/1000 | Loss: 0.00028333
Iteration 8/1000 | Loss: 0.00033378
Iteration 9/1000 | Loss: 0.00044728
Iteration 10/1000 | Loss: 0.00062327
Iteration 11/1000 | Loss: 0.00050879
Iteration 12/1000 | Loss: 0.00046320
Iteration 13/1000 | Loss: 0.00048722
Iteration 14/1000 | Loss: 0.00052771
Iteration 15/1000 | Loss: 0.00051924
Iteration 16/1000 | Loss: 0.00051006
Iteration 17/1000 | Loss: 0.00045623
Iteration 18/1000 | Loss: 0.00042636
Iteration 19/1000 | Loss: 0.00036969
Iteration 20/1000 | Loss: 0.00030368
Iteration 21/1000 | Loss: 0.00025309
Iteration 22/1000 | Loss: 0.00018127
Iteration 23/1000 | Loss: 0.00027488
Iteration 24/1000 | Loss: 0.00020035
Iteration 25/1000 | Loss: 0.00017795
Iteration 26/1000 | Loss: 0.00019446
Iteration 27/1000 | Loss: 0.00014728
Iteration 28/1000 | Loss: 0.00021252
Iteration 29/1000 | Loss: 0.00009215
Iteration 30/1000 | Loss: 0.00007705
Iteration 31/1000 | Loss: 0.00007851
Iteration 32/1000 | Loss: 0.00004009
Iteration 33/1000 | Loss: 0.00014615
Iteration 34/1000 | Loss: 0.00004713
Iteration 35/1000 | Loss: 0.00004094
Iteration 36/1000 | Loss: 0.00003911
Iteration 37/1000 | Loss: 0.00003807
Iteration 38/1000 | Loss: 0.00003650
Iteration 39/1000 | Loss: 0.00003517
Iteration 40/1000 | Loss: 0.00003476
Iteration 41/1000 | Loss: 0.00003439
Iteration 42/1000 | Loss: 0.00003403
Iteration 43/1000 | Loss: 0.00003360
Iteration 44/1000 | Loss: 0.00003329
Iteration 45/1000 | Loss: 0.00003306
Iteration 46/1000 | Loss: 0.00003301
Iteration 47/1000 | Loss: 0.00003292
Iteration 48/1000 | Loss: 0.00003284
Iteration 49/1000 | Loss: 0.00003277
Iteration 50/1000 | Loss: 0.00003272
Iteration 51/1000 | Loss: 0.00003254
Iteration 52/1000 | Loss: 0.00003237
Iteration 53/1000 | Loss: 0.00003236
Iteration 54/1000 | Loss: 0.00003229
Iteration 55/1000 | Loss: 0.00003229
Iteration 56/1000 | Loss: 0.00003229
Iteration 57/1000 | Loss: 0.00003228
Iteration 58/1000 | Loss: 0.00003227
Iteration 59/1000 | Loss: 0.00003227
Iteration 60/1000 | Loss: 0.00003227
Iteration 61/1000 | Loss: 0.00003226
Iteration 62/1000 | Loss: 0.00003226
Iteration 63/1000 | Loss: 0.00003225
Iteration 64/1000 | Loss: 0.00003225
Iteration 65/1000 | Loss: 0.00003225
Iteration 66/1000 | Loss: 0.00003225
Iteration 67/1000 | Loss: 0.00003225
Iteration 68/1000 | Loss: 0.00003225
Iteration 69/1000 | Loss: 0.00003225
Iteration 70/1000 | Loss: 0.00003225
Iteration 71/1000 | Loss: 0.00003225
Iteration 72/1000 | Loss: 0.00003225
Iteration 73/1000 | Loss: 0.00003224
Iteration 74/1000 | Loss: 0.00003224
Iteration 75/1000 | Loss: 0.00003224
Iteration 76/1000 | Loss: 0.00003224
Iteration 77/1000 | Loss: 0.00003224
Iteration 78/1000 | Loss: 0.00003224
Iteration 79/1000 | Loss: 0.00003224
Iteration 80/1000 | Loss: 0.00003224
Iteration 81/1000 | Loss: 0.00003224
Iteration 82/1000 | Loss: 0.00003224
Iteration 83/1000 | Loss: 0.00003224
Iteration 84/1000 | Loss: 0.00003223
Iteration 85/1000 | Loss: 0.00003223
Iteration 86/1000 | Loss: 0.00003223
Iteration 87/1000 | Loss: 0.00003223
Iteration 88/1000 | Loss: 0.00003222
Iteration 89/1000 | Loss: 0.00003222
Iteration 90/1000 | Loss: 0.00003222
Iteration 91/1000 | Loss: 0.00003222
Iteration 92/1000 | Loss: 0.00003222
Iteration 93/1000 | Loss: 0.00003222
Iteration 94/1000 | Loss: 0.00003222
Iteration 95/1000 | Loss: 0.00003222
Iteration 96/1000 | Loss: 0.00003222
Iteration 97/1000 | Loss: 0.00003222
Iteration 98/1000 | Loss: 0.00003222
Iteration 99/1000 | Loss: 0.00003222
Iteration 100/1000 | Loss: 0.00003221
Iteration 101/1000 | Loss: 0.00003221
Iteration 102/1000 | Loss: 0.00003221
Iteration 103/1000 | Loss: 0.00003220
Iteration 104/1000 | Loss: 0.00003220
Iteration 105/1000 | Loss: 0.00003220
Iteration 106/1000 | Loss: 0.00003220
Iteration 107/1000 | Loss: 0.00003220
Iteration 108/1000 | Loss: 0.00003220
Iteration 109/1000 | Loss: 0.00003219
Iteration 110/1000 | Loss: 0.00003219
Iteration 111/1000 | Loss: 0.00003218
Iteration 112/1000 | Loss: 0.00003218
Iteration 113/1000 | Loss: 0.00003218
Iteration 114/1000 | Loss: 0.00003218
Iteration 115/1000 | Loss: 0.00003218
Iteration 116/1000 | Loss: 0.00003218
Iteration 117/1000 | Loss: 0.00003217
Iteration 118/1000 | Loss: 0.00003217
Iteration 119/1000 | Loss: 0.00003217
Iteration 120/1000 | Loss: 0.00003217
Iteration 121/1000 | Loss: 0.00003217
Iteration 122/1000 | Loss: 0.00003217
Iteration 123/1000 | Loss: 0.00003217
Iteration 124/1000 | Loss: 0.00003217
Iteration 125/1000 | Loss: 0.00003217
Iteration 126/1000 | Loss: 0.00003216
Iteration 127/1000 | Loss: 0.00003216
Iteration 128/1000 | Loss: 0.00003216
Iteration 129/1000 | Loss: 0.00003216
Iteration 130/1000 | Loss: 0.00003216
Iteration 131/1000 | Loss: 0.00003216
Iteration 132/1000 | Loss: 0.00003216
Iteration 133/1000 | Loss: 0.00003215
Iteration 134/1000 | Loss: 0.00003215
Iteration 135/1000 | Loss: 0.00003215
Iteration 136/1000 | Loss: 0.00003214
Iteration 137/1000 | Loss: 0.00003214
Iteration 138/1000 | Loss: 0.00003214
Iteration 139/1000 | Loss: 0.00003214
Iteration 140/1000 | Loss: 0.00003214
Iteration 141/1000 | Loss: 0.00003214
Iteration 142/1000 | Loss: 0.00003213
Iteration 143/1000 | Loss: 0.00003213
Iteration 144/1000 | Loss: 0.00003213
Iteration 145/1000 | Loss: 0.00003213
Iteration 146/1000 | Loss: 0.00003213
Iteration 147/1000 | Loss: 0.00003213
Iteration 148/1000 | Loss: 0.00003213
Iteration 149/1000 | Loss: 0.00003212
Iteration 150/1000 | Loss: 0.00003212
Iteration 151/1000 | Loss: 0.00003212
Iteration 152/1000 | Loss: 0.00003211
Iteration 153/1000 | Loss: 0.00003210
Iteration 154/1000 | Loss: 0.00003209
Iteration 155/1000 | Loss: 0.00003209
Iteration 156/1000 | Loss: 0.00003209
Iteration 157/1000 | Loss: 0.00003208
Iteration 158/1000 | Loss: 0.00003207
Iteration 159/1000 | Loss: 0.00003207
Iteration 160/1000 | Loss: 0.00003207
Iteration 161/1000 | Loss: 0.00003206
Iteration 162/1000 | Loss: 0.00003206
Iteration 163/1000 | Loss: 0.00003206
Iteration 164/1000 | Loss: 0.00003206
Iteration 165/1000 | Loss: 0.00003206
Iteration 166/1000 | Loss: 0.00003205
Iteration 167/1000 | Loss: 0.00003205
Iteration 168/1000 | Loss: 0.00003205
Iteration 169/1000 | Loss: 0.00003204
Iteration 170/1000 | Loss: 0.00003204
Iteration 171/1000 | Loss: 0.00003204
Iteration 172/1000 | Loss: 0.00003203
Iteration 173/1000 | Loss: 0.00003203
Iteration 174/1000 | Loss: 0.00003203
Iteration 175/1000 | Loss: 0.00003203
Iteration 176/1000 | Loss: 0.00003203
Iteration 177/1000 | Loss: 0.00003202
Iteration 178/1000 | Loss: 0.00003202
Iteration 179/1000 | Loss: 0.00003202
Iteration 180/1000 | Loss: 0.00003202
Iteration 181/1000 | Loss: 0.00003202
Iteration 182/1000 | Loss: 0.00003202
Iteration 183/1000 | Loss: 0.00003202
Iteration 184/1000 | Loss: 0.00003202
Iteration 185/1000 | Loss: 0.00003202
Iteration 186/1000 | Loss: 0.00003202
Iteration 187/1000 | Loss: 0.00003201
Iteration 188/1000 | Loss: 0.00003201
Iteration 189/1000 | Loss: 0.00003201
Iteration 190/1000 | Loss: 0.00003201
Iteration 191/1000 | Loss: 0.00003201
Iteration 192/1000 | Loss: 0.00003201
Iteration 193/1000 | Loss: 0.00003201
Iteration 194/1000 | Loss: 0.00003201
Iteration 195/1000 | Loss: 0.00003201
Iteration 196/1000 | Loss: 0.00003201
Iteration 197/1000 | Loss: 0.00003201
Iteration 198/1000 | Loss: 0.00003201
Iteration 199/1000 | Loss: 0.00003201
Iteration 200/1000 | Loss: 0.00003201
Iteration 201/1000 | Loss: 0.00003201
Iteration 202/1000 | Loss: 0.00003201
Iteration 203/1000 | Loss: 0.00003201
Iteration 204/1000 | Loss: 0.00003201
Iteration 205/1000 | Loss: 0.00003201
Iteration 206/1000 | Loss: 0.00003201
Iteration 207/1000 | Loss: 0.00003201
Iteration 208/1000 | Loss: 0.00003201
Iteration 209/1000 | Loss: 0.00003201
Iteration 210/1000 | Loss: 0.00003201
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [3.201175786671229e-05, 3.201175786671229e-05, 3.201175786671229e-05, 3.201175786671229e-05, 3.201175786671229e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.201175786671229e-05

Optimization complete. Final v2v error: 4.52782678604126 mm

Highest mean error: 5.965003490447998 mm for frame 70

Lowest mean error: 3.704408645629883 mm for frame 28

Saving results

Total time: 149.60118317604065
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_5533/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5533/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5533/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00971000
Iteration 2/25 | Loss: 0.00121788
Iteration 3/25 | Loss: 0.00105544
Iteration 4/25 | Loss: 0.00103850
Iteration 5/25 | Loss: 0.00103434
Iteration 6/25 | Loss: 0.00103345
Iteration 7/25 | Loss: 0.00103345
Iteration 8/25 | Loss: 0.00103345
Iteration 9/25 | Loss: 0.00103345
Iteration 10/25 | Loss: 0.00103345
Iteration 11/25 | Loss: 0.00103345
Iteration 12/25 | Loss: 0.00103345
Iteration 13/25 | Loss: 0.00103345
Iteration 14/25 | Loss: 0.00103345
Iteration 15/25 | Loss: 0.00103345
Iteration 16/25 | Loss: 0.00103345
Iteration 17/25 | Loss: 0.00103345
Iteration 18/25 | Loss: 0.00103345
Iteration 19/25 | Loss: 0.00103345
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001033450593240559, 0.001033450593240559, 0.001033450593240559, 0.001033450593240559, 0.001033450593240559]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001033450593240559

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22164452
Iteration 2/25 | Loss: 0.00192072
Iteration 3/25 | Loss: 0.00192072
Iteration 4/25 | Loss: 0.00192072
Iteration 5/25 | Loss: 0.00192072
Iteration 6/25 | Loss: 0.00192072
Iteration 7/25 | Loss: 0.00192072
Iteration 8/25 | Loss: 0.00192072
Iteration 9/25 | Loss: 0.00192072
Iteration 10/25 | Loss: 0.00192072
Iteration 11/25 | Loss: 0.00192071
Iteration 12/25 | Loss: 0.00192071
Iteration 13/25 | Loss: 0.00192071
Iteration 14/25 | Loss: 0.00192071
Iteration 15/25 | Loss: 0.00192071
Iteration 16/25 | Loss: 0.00192071
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0019207147415727377, 0.0019207147415727377, 0.0019207147415727377, 0.0019207147415727377, 0.0019207147415727377]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019207147415727377

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00192071
Iteration 2/1000 | Loss: 0.00003089
Iteration 3/1000 | Loss: 0.00001745
Iteration 4/1000 | Loss: 0.00001537
Iteration 5/1000 | Loss: 0.00001425
Iteration 6/1000 | Loss: 0.00001342
Iteration 7/1000 | Loss: 0.00001279
Iteration 8/1000 | Loss: 0.00001245
Iteration 9/1000 | Loss: 0.00001222
Iteration 10/1000 | Loss: 0.00001200
Iteration 11/1000 | Loss: 0.00001193
Iteration 12/1000 | Loss: 0.00001179
Iteration 13/1000 | Loss: 0.00001175
Iteration 14/1000 | Loss: 0.00001175
Iteration 15/1000 | Loss: 0.00001174
Iteration 16/1000 | Loss: 0.00001174
Iteration 17/1000 | Loss: 0.00001173
Iteration 18/1000 | Loss: 0.00001172
Iteration 19/1000 | Loss: 0.00001168
Iteration 20/1000 | Loss: 0.00001168
Iteration 21/1000 | Loss: 0.00001167
Iteration 22/1000 | Loss: 0.00001167
Iteration 23/1000 | Loss: 0.00001167
Iteration 24/1000 | Loss: 0.00001166
Iteration 25/1000 | Loss: 0.00001166
Iteration 26/1000 | Loss: 0.00001166
Iteration 27/1000 | Loss: 0.00001165
Iteration 28/1000 | Loss: 0.00001165
Iteration 29/1000 | Loss: 0.00001163
Iteration 30/1000 | Loss: 0.00001162
Iteration 31/1000 | Loss: 0.00001161
Iteration 32/1000 | Loss: 0.00001161
Iteration 33/1000 | Loss: 0.00001160
Iteration 34/1000 | Loss: 0.00001160
Iteration 35/1000 | Loss: 0.00001159
Iteration 36/1000 | Loss: 0.00001159
Iteration 37/1000 | Loss: 0.00001158
Iteration 38/1000 | Loss: 0.00001157
Iteration 39/1000 | Loss: 0.00001157
Iteration 40/1000 | Loss: 0.00001157
Iteration 41/1000 | Loss: 0.00001157
Iteration 42/1000 | Loss: 0.00001157
Iteration 43/1000 | Loss: 0.00001156
Iteration 44/1000 | Loss: 0.00001156
Iteration 45/1000 | Loss: 0.00001155
Iteration 46/1000 | Loss: 0.00001155
Iteration 47/1000 | Loss: 0.00001154
Iteration 48/1000 | Loss: 0.00001154
Iteration 49/1000 | Loss: 0.00001154
Iteration 50/1000 | Loss: 0.00001154
Iteration 51/1000 | Loss: 0.00001154
Iteration 52/1000 | Loss: 0.00001153
Iteration 53/1000 | Loss: 0.00001153
Iteration 54/1000 | Loss: 0.00001153
Iteration 55/1000 | Loss: 0.00001152
Iteration 56/1000 | Loss: 0.00001151
Iteration 57/1000 | Loss: 0.00001151
Iteration 58/1000 | Loss: 0.00001151
Iteration 59/1000 | Loss: 0.00001150
Iteration 60/1000 | Loss: 0.00001150
Iteration 61/1000 | Loss: 0.00001150
Iteration 62/1000 | Loss: 0.00001150
Iteration 63/1000 | Loss: 0.00001150
Iteration 64/1000 | Loss: 0.00001150
Iteration 65/1000 | Loss: 0.00001150
Iteration 66/1000 | Loss: 0.00001150
Iteration 67/1000 | Loss: 0.00001149
Iteration 68/1000 | Loss: 0.00001149
Iteration 69/1000 | Loss: 0.00001149
Iteration 70/1000 | Loss: 0.00001149
Iteration 71/1000 | Loss: 0.00001149
Iteration 72/1000 | Loss: 0.00001149
Iteration 73/1000 | Loss: 0.00001148
Iteration 74/1000 | Loss: 0.00001148
Iteration 75/1000 | Loss: 0.00001148
Iteration 76/1000 | Loss: 0.00001148
Iteration 77/1000 | Loss: 0.00001148
Iteration 78/1000 | Loss: 0.00001147
Iteration 79/1000 | Loss: 0.00001147
Iteration 80/1000 | Loss: 0.00001147
Iteration 81/1000 | Loss: 0.00001147
Iteration 82/1000 | Loss: 0.00001147
Iteration 83/1000 | Loss: 0.00001146
Iteration 84/1000 | Loss: 0.00001146
Iteration 85/1000 | Loss: 0.00001146
Iteration 86/1000 | Loss: 0.00001146
Iteration 87/1000 | Loss: 0.00001146
Iteration 88/1000 | Loss: 0.00001145
Iteration 89/1000 | Loss: 0.00001145
Iteration 90/1000 | Loss: 0.00001145
Iteration 91/1000 | Loss: 0.00001145
Iteration 92/1000 | Loss: 0.00001145
Iteration 93/1000 | Loss: 0.00001144
Iteration 94/1000 | Loss: 0.00001144
Iteration 95/1000 | Loss: 0.00001144
Iteration 96/1000 | Loss: 0.00001144
Iteration 97/1000 | Loss: 0.00001144
Iteration 98/1000 | Loss: 0.00001144
Iteration 99/1000 | Loss: 0.00001144
Iteration 100/1000 | Loss: 0.00001144
Iteration 101/1000 | Loss: 0.00001144
Iteration 102/1000 | Loss: 0.00001144
Iteration 103/1000 | Loss: 0.00001144
Iteration 104/1000 | Loss: 0.00001143
Iteration 105/1000 | Loss: 0.00001143
Iteration 106/1000 | Loss: 0.00001143
Iteration 107/1000 | Loss: 0.00001143
Iteration 108/1000 | Loss: 0.00001143
Iteration 109/1000 | Loss: 0.00001143
Iteration 110/1000 | Loss: 0.00001143
Iteration 111/1000 | Loss: 0.00001143
Iteration 112/1000 | Loss: 0.00001143
Iteration 113/1000 | Loss: 0.00001143
Iteration 114/1000 | Loss: 0.00001142
Iteration 115/1000 | Loss: 0.00001142
Iteration 116/1000 | Loss: 0.00001142
Iteration 117/1000 | Loss: 0.00001142
Iteration 118/1000 | Loss: 0.00001142
Iteration 119/1000 | Loss: 0.00001141
Iteration 120/1000 | Loss: 0.00001141
Iteration 121/1000 | Loss: 0.00001141
Iteration 122/1000 | Loss: 0.00001141
Iteration 123/1000 | Loss: 0.00001141
Iteration 124/1000 | Loss: 0.00001141
Iteration 125/1000 | Loss: 0.00001141
Iteration 126/1000 | Loss: 0.00001141
Iteration 127/1000 | Loss: 0.00001141
Iteration 128/1000 | Loss: 0.00001141
Iteration 129/1000 | Loss: 0.00001141
Iteration 130/1000 | Loss: 0.00001141
Iteration 131/1000 | Loss: 0.00001141
Iteration 132/1000 | Loss: 0.00001141
Iteration 133/1000 | Loss: 0.00001140
Iteration 134/1000 | Loss: 0.00001140
Iteration 135/1000 | Loss: 0.00001140
Iteration 136/1000 | Loss: 0.00001140
Iteration 137/1000 | Loss: 0.00001140
Iteration 138/1000 | Loss: 0.00001140
Iteration 139/1000 | Loss: 0.00001140
Iteration 140/1000 | Loss: 0.00001140
Iteration 141/1000 | Loss: 0.00001139
Iteration 142/1000 | Loss: 0.00001139
Iteration 143/1000 | Loss: 0.00001139
Iteration 144/1000 | Loss: 0.00001139
Iteration 145/1000 | Loss: 0.00001139
Iteration 146/1000 | Loss: 0.00001139
Iteration 147/1000 | Loss: 0.00001139
Iteration 148/1000 | Loss: 0.00001139
Iteration 149/1000 | Loss: 0.00001139
Iteration 150/1000 | Loss: 0.00001139
Iteration 151/1000 | Loss: 0.00001139
Iteration 152/1000 | Loss: 0.00001139
Iteration 153/1000 | Loss: 0.00001139
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.139438791142311e-05, 1.139438791142311e-05, 1.139438791142311e-05, 1.139438791142311e-05, 1.139438791142311e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.139438791142311e-05

Optimization complete. Final v2v error: 2.8319172859191895 mm

Highest mean error: 3.386793851852417 mm for frame 56

Lowest mean error: 2.506939172744751 mm for frame 34

Saving results

Total time: 34.38568449020386
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_5533/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5533/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5533/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01057935
Iteration 2/25 | Loss: 0.00315097
Iteration 3/25 | Loss: 0.00184972
Iteration 4/25 | Loss: 0.00154407
Iteration 5/25 | Loss: 0.00150403
Iteration 6/25 | Loss: 0.00149586
Iteration 7/25 | Loss: 0.00146542
Iteration 8/25 | Loss: 0.00139650
Iteration 9/25 | Loss: 0.00125282
Iteration 10/25 | Loss: 0.00124575
Iteration 11/25 | Loss: 0.00120892
Iteration 12/25 | Loss: 0.00119261
Iteration 13/25 | Loss: 0.00119691
Iteration 14/25 | Loss: 0.00119086
Iteration 15/25 | Loss: 0.00118532
Iteration 16/25 | Loss: 0.00118140
Iteration 17/25 | Loss: 0.00118063
Iteration 18/25 | Loss: 0.00118147
Iteration 19/25 | Loss: 0.00117946
Iteration 20/25 | Loss: 0.00117617
Iteration 21/25 | Loss: 0.00117935
Iteration 22/25 | Loss: 0.00117585
Iteration 23/25 | Loss: 0.00117397
Iteration 24/25 | Loss: 0.00117372
Iteration 25/25 | Loss: 0.00117320

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23050249
Iteration 2/25 | Loss: 0.00245598
Iteration 3/25 | Loss: 0.00243656
Iteration 4/25 | Loss: 0.00243656
Iteration 5/25 | Loss: 0.00243656
Iteration 6/25 | Loss: 0.00243656
Iteration 7/25 | Loss: 0.00243656
Iteration 8/25 | Loss: 0.00243656
Iteration 9/25 | Loss: 0.00243656
Iteration 10/25 | Loss: 0.00243656
Iteration 11/25 | Loss: 0.00243656
Iteration 12/25 | Loss: 0.00243656
Iteration 13/25 | Loss: 0.00243656
Iteration 14/25 | Loss: 0.00243656
Iteration 15/25 | Loss: 0.00243656
Iteration 16/25 | Loss: 0.00243656
Iteration 17/25 | Loss: 0.00243656
Iteration 18/25 | Loss: 0.00243656
Iteration 19/25 | Loss: 0.00243656
Iteration 20/25 | Loss: 0.00243656
Iteration 21/25 | Loss: 0.00243656
Iteration 22/25 | Loss: 0.00243656
Iteration 23/25 | Loss: 0.00243656
Iteration 24/25 | Loss: 0.00243656
Iteration 25/25 | Loss: 0.00243656

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00243656
Iteration 2/1000 | Loss: 0.00020091
Iteration 3/1000 | Loss: 0.00013219
Iteration 4/1000 | Loss: 0.00011419
Iteration 5/1000 | Loss: 0.00012925
Iteration 6/1000 | Loss: 0.00006460
Iteration 7/1000 | Loss: 0.00006096
Iteration 8/1000 | Loss: 0.00048219
Iteration 9/1000 | Loss: 0.00054157
Iteration 10/1000 | Loss: 0.00070536
Iteration 11/1000 | Loss: 0.00019770
Iteration 12/1000 | Loss: 0.00043374
Iteration 13/1000 | Loss: 0.00032288
Iteration 14/1000 | Loss: 0.00054689
Iteration 15/1000 | Loss: 0.00072173
Iteration 16/1000 | Loss: 0.00064050
Iteration 17/1000 | Loss: 0.00008146
Iteration 18/1000 | Loss: 0.00005500
Iteration 19/1000 | Loss: 0.00020499
Iteration 20/1000 | Loss: 0.00017460
Iteration 21/1000 | Loss: 0.00008419
Iteration 22/1000 | Loss: 0.00009109
Iteration 23/1000 | Loss: 0.00006568
Iteration 24/1000 | Loss: 0.00005999
Iteration 25/1000 | Loss: 0.00005122
Iteration 26/1000 | Loss: 0.00006398
Iteration 27/1000 | Loss: 0.00005470
Iteration 28/1000 | Loss: 0.00026157
Iteration 29/1000 | Loss: 0.00008916
Iteration 30/1000 | Loss: 0.00006020
Iteration 31/1000 | Loss: 0.00004998
Iteration 32/1000 | Loss: 0.00005193
Iteration 33/1000 | Loss: 0.00004949
Iteration 34/1000 | Loss: 0.00054494
Iteration 35/1000 | Loss: 0.00027204
Iteration 36/1000 | Loss: 0.00056810
Iteration 37/1000 | Loss: 0.00015018
Iteration 38/1000 | Loss: 0.00005225
Iteration 39/1000 | Loss: 0.00012567
Iteration 40/1000 | Loss: 0.00010016
Iteration 41/1000 | Loss: 0.00010694
Iteration 42/1000 | Loss: 0.00015241
Iteration 43/1000 | Loss: 0.00015100
Iteration 44/1000 | Loss: 0.00011681
Iteration 45/1000 | Loss: 0.00033758
Iteration 46/1000 | Loss: 0.00048645
Iteration 47/1000 | Loss: 0.00035320
Iteration 48/1000 | Loss: 0.00010427
Iteration 49/1000 | Loss: 0.00005132
Iteration 50/1000 | Loss: 0.00009099
Iteration 51/1000 | Loss: 0.00005495
Iteration 52/1000 | Loss: 0.00004657
Iteration 53/1000 | Loss: 0.00019368
Iteration 54/1000 | Loss: 0.00004591
Iteration 55/1000 | Loss: 0.00005175
Iteration 56/1000 | Loss: 0.00004346
Iteration 57/1000 | Loss: 0.00003434
Iteration 58/1000 | Loss: 0.00023374
Iteration 59/1000 | Loss: 0.00010652
Iteration 60/1000 | Loss: 0.00002853
Iteration 61/1000 | Loss: 0.00019885
Iteration 62/1000 | Loss: 0.00020012
Iteration 63/1000 | Loss: 0.00002632
Iteration 64/1000 | Loss: 0.00002552
Iteration 65/1000 | Loss: 0.00014100
Iteration 66/1000 | Loss: 0.00004553
Iteration 67/1000 | Loss: 0.00003504
Iteration 68/1000 | Loss: 0.00005996
Iteration 69/1000 | Loss: 0.00011040
Iteration 70/1000 | Loss: 0.00005489
Iteration 71/1000 | Loss: 0.00009984
Iteration 72/1000 | Loss: 0.00007478
Iteration 73/1000 | Loss: 0.00003069
Iteration 74/1000 | Loss: 0.00023486
Iteration 75/1000 | Loss: 0.00015784
Iteration 76/1000 | Loss: 0.00021778
Iteration 77/1000 | Loss: 0.00007951
Iteration 78/1000 | Loss: 0.00003433
Iteration 79/1000 | Loss: 0.00007435
Iteration 80/1000 | Loss: 0.00007789
Iteration 81/1000 | Loss: 0.00008972
Iteration 82/1000 | Loss: 0.00007314
Iteration 83/1000 | Loss: 0.00013355
Iteration 84/1000 | Loss: 0.00008562
Iteration 85/1000 | Loss: 0.00004486
Iteration 86/1000 | Loss: 0.00009239
Iteration 87/1000 | Loss: 0.00007899
Iteration 88/1000 | Loss: 0.00003310
Iteration 89/1000 | Loss: 0.00012195
Iteration 90/1000 | Loss: 0.00006289
Iteration 91/1000 | Loss: 0.00009878
Iteration 92/1000 | Loss: 0.00010031
Iteration 93/1000 | Loss: 0.00009621
Iteration 94/1000 | Loss: 0.00006224
Iteration 95/1000 | Loss: 0.00017085
Iteration 96/1000 | Loss: 0.00006086
Iteration 97/1000 | Loss: 0.00010195
Iteration 98/1000 | Loss: 0.00002879
Iteration 99/1000 | Loss: 0.00002395
Iteration 100/1000 | Loss: 0.00012266
Iteration 101/1000 | Loss: 0.00005439
Iteration 102/1000 | Loss: 0.00006271
Iteration 103/1000 | Loss: 0.00002385
Iteration 104/1000 | Loss: 0.00002158
Iteration 105/1000 | Loss: 0.00004719
Iteration 106/1000 | Loss: 0.00007750
Iteration 107/1000 | Loss: 0.00002111
Iteration 108/1000 | Loss: 0.00002083
Iteration 109/1000 | Loss: 0.00002063
Iteration 110/1000 | Loss: 0.00002061
Iteration 111/1000 | Loss: 0.00002057
Iteration 112/1000 | Loss: 0.00002056
Iteration 113/1000 | Loss: 0.00002055
Iteration 114/1000 | Loss: 0.00002054
Iteration 115/1000 | Loss: 0.00002053
Iteration 116/1000 | Loss: 0.00002052
Iteration 117/1000 | Loss: 0.00002051
Iteration 118/1000 | Loss: 0.00002047
Iteration 119/1000 | Loss: 0.00002044
Iteration 120/1000 | Loss: 0.00002043
Iteration 121/1000 | Loss: 0.00002040
Iteration 122/1000 | Loss: 0.00002039
Iteration 123/1000 | Loss: 0.00002038
Iteration 124/1000 | Loss: 0.00002038
Iteration 125/1000 | Loss: 0.00002038
Iteration 126/1000 | Loss: 0.00002037
Iteration 127/1000 | Loss: 0.00002036
Iteration 128/1000 | Loss: 0.00002036
Iteration 129/1000 | Loss: 0.00002035
Iteration 130/1000 | Loss: 0.00002035
Iteration 131/1000 | Loss: 0.00002035
Iteration 132/1000 | Loss: 0.00002035
Iteration 133/1000 | Loss: 0.00002035
Iteration 134/1000 | Loss: 0.00002034
Iteration 135/1000 | Loss: 0.00002034
Iteration 136/1000 | Loss: 0.00002034
Iteration 137/1000 | Loss: 0.00002033
Iteration 138/1000 | Loss: 0.00002033
Iteration 139/1000 | Loss: 0.00002033
Iteration 140/1000 | Loss: 0.00002032
Iteration 141/1000 | Loss: 0.00002032
Iteration 142/1000 | Loss: 0.00002032
Iteration 143/1000 | Loss: 0.00002031
Iteration 144/1000 | Loss: 0.00002031
Iteration 145/1000 | Loss: 0.00002031
Iteration 146/1000 | Loss: 0.00002030
Iteration 147/1000 | Loss: 0.00002030
Iteration 148/1000 | Loss: 0.00002029
Iteration 149/1000 | Loss: 0.00002028
Iteration 150/1000 | Loss: 0.00002028
Iteration 151/1000 | Loss: 0.00002028
Iteration 152/1000 | Loss: 0.00002027
Iteration 153/1000 | Loss: 0.00002027
Iteration 154/1000 | Loss: 0.00002027
Iteration 155/1000 | Loss: 0.00002027
Iteration 156/1000 | Loss: 0.00002027
Iteration 157/1000 | Loss: 0.00002027
Iteration 158/1000 | Loss: 0.00002026
Iteration 159/1000 | Loss: 0.00002026
Iteration 160/1000 | Loss: 0.00002026
Iteration 161/1000 | Loss: 0.00002026
Iteration 162/1000 | Loss: 0.00002026
Iteration 163/1000 | Loss: 0.00002026
Iteration 164/1000 | Loss: 0.00002026
Iteration 165/1000 | Loss: 0.00002025
Iteration 166/1000 | Loss: 0.00002025
Iteration 167/1000 | Loss: 0.00002025
Iteration 168/1000 | Loss: 0.00002025
Iteration 169/1000 | Loss: 0.00002025
Iteration 170/1000 | Loss: 0.00002024
Iteration 171/1000 | Loss: 0.00002024
Iteration 172/1000 | Loss: 0.00002024
Iteration 173/1000 | Loss: 0.00002024
Iteration 174/1000 | Loss: 0.00002024
Iteration 175/1000 | Loss: 0.00002023
Iteration 176/1000 | Loss: 0.00002023
Iteration 177/1000 | Loss: 0.00002023
Iteration 178/1000 | Loss: 0.00002023
Iteration 179/1000 | Loss: 0.00002023
Iteration 180/1000 | Loss: 0.00005805
Iteration 181/1000 | Loss: 0.00010581
Iteration 182/1000 | Loss: 0.00009749
Iteration 183/1000 | Loss: 0.00005714
Iteration 184/1000 | Loss: 0.00008219
Iteration 185/1000 | Loss: 0.00010327
Iteration 186/1000 | Loss: 0.00008443
Iteration 187/1000 | Loss: 0.00009900
Iteration 188/1000 | Loss: 0.00010087
Iteration 189/1000 | Loss: 0.00004429
Iteration 190/1000 | Loss: 0.00003171
Iteration 191/1000 | Loss: 0.00002418
Iteration 192/1000 | Loss: 0.00002142
Iteration 193/1000 | Loss: 0.00001999
Iteration 194/1000 | Loss: 0.00001945
Iteration 195/1000 | Loss: 0.00001886
Iteration 196/1000 | Loss: 0.00001863
Iteration 197/1000 | Loss: 0.00001834
Iteration 198/1000 | Loss: 0.00001831
Iteration 199/1000 | Loss: 0.00001826
Iteration 200/1000 | Loss: 0.00001824
Iteration 201/1000 | Loss: 0.00001821
Iteration 202/1000 | Loss: 0.00001817
Iteration 203/1000 | Loss: 0.00001817
Iteration 204/1000 | Loss: 0.00001816
Iteration 205/1000 | Loss: 0.00001816
Iteration 206/1000 | Loss: 0.00001816
Iteration 207/1000 | Loss: 0.00001816
Iteration 208/1000 | Loss: 0.00001816
Iteration 209/1000 | Loss: 0.00001816
Iteration 210/1000 | Loss: 0.00001815
Iteration 211/1000 | Loss: 0.00001815
Iteration 212/1000 | Loss: 0.00001814
Iteration 213/1000 | Loss: 0.00001814
Iteration 214/1000 | Loss: 0.00001813
Iteration 215/1000 | Loss: 0.00001813
Iteration 216/1000 | Loss: 0.00001813
Iteration 217/1000 | Loss: 0.00001813
Iteration 218/1000 | Loss: 0.00001813
Iteration 219/1000 | Loss: 0.00001813
Iteration 220/1000 | Loss: 0.00001813
Iteration 221/1000 | Loss: 0.00001812
Iteration 222/1000 | Loss: 0.00001812
Iteration 223/1000 | Loss: 0.00001812
Iteration 224/1000 | Loss: 0.00001812
Iteration 225/1000 | Loss: 0.00001811
Iteration 226/1000 | Loss: 0.00001811
Iteration 227/1000 | Loss: 0.00001811
Iteration 228/1000 | Loss: 0.00001811
Iteration 229/1000 | Loss: 0.00001811
Iteration 230/1000 | Loss: 0.00001811
Iteration 231/1000 | Loss: 0.00001811
Iteration 232/1000 | Loss: 0.00001811
Iteration 233/1000 | Loss: 0.00001810
Iteration 234/1000 | Loss: 0.00001810
Iteration 235/1000 | Loss: 0.00001810
Iteration 236/1000 | Loss: 0.00001810
Iteration 237/1000 | Loss: 0.00001810
Iteration 238/1000 | Loss: 0.00001810
Iteration 239/1000 | Loss: 0.00001810
Iteration 240/1000 | Loss: 0.00001809
Iteration 241/1000 | Loss: 0.00001809
Iteration 242/1000 | Loss: 0.00001809
Iteration 243/1000 | Loss: 0.00001809
Iteration 244/1000 | Loss: 0.00001809
Iteration 245/1000 | Loss: 0.00001809
Iteration 246/1000 | Loss: 0.00001808
Iteration 247/1000 | Loss: 0.00001808
Iteration 248/1000 | Loss: 0.00001808
Iteration 249/1000 | Loss: 0.00001808
Iteration 250/1000 | Loss: 0.00001808
Iteration 251/1000 | Loss: 0.00001808
Iteration 252/1000 | Loss: 0.00001808
Iteration 253/1000 | Loss: 0.00001808
Iteration 254/1000 | Loss: 0.00001807
Iteration 255/1000 | Loss: 0.00001807
Iteration 256/1000 | Loss: 0.00001807
Iteration 257/1000 | Loss: 0.00001807
Iteration 258/1000 | Loss: 0.00001807
Iteration 259/1000 | Loss: 0.00001807
Iteration 260/1000 | Loss: 0.00001807
Iteration 261/1000 | Loss: 0.00001807
Iteration 262/1000 | Loss: 0.00001807
Iteration 263/1000 | Loss: 0.00001807
Iteration 264/1000 | Loss: 0.00001807
Iteration 265/1000 | Loss: 0.00001807
Iteration 266/1000 | Loss: 0.00001807
Iteration 267/1000 | Loss: 0.00001806
Iteration 268/1000 | Loss: 0.00001806
Iteration 269/1000 | Loss: 0.00001806
Iteration 270/1000 | Loss: 0.00001806
Iteration 271/1000 | Loss: 0.00001806
Iteration 272/1000 | Loss: 0.00001806
Iteration 273/1000 | Loss: 0.00001806
Iteration 274/1000 | Loss: 0.00001806
Iteration 275/1000 | Loss: 0.00001806
Iteration 276/1000 | Loss: 0.00001805
Iteration 277/1000 | Loss: 0.00001805
Iteration 278/1000 | Loss: 0.00001805
Iteration 279/1000 | Loss: 0.00001805
Iteration 280/1000 | Loss: 0.00001805
Iteration 281/1000 | Loss: 0.00001805
Iteration 282/1000 | Loss: 0.00001805
Iteration 283/1000 | Loss: 0.00001805
Iteration 284/1000 | Loss: 0.00001805
Iteration 285/1000 | Loss: 0.00001805
Iteration 286/1000 | Loss: 0.00001805
Iteration 287/1000 | Loss: 0.00001805
Iteration 288/1000 | Loss: 0.00001805
Iteration 289/1000 | Loss: 0.00001805
Iteration 290/1000 | Loss: 0.00001805
Iteration 291/1000 | Loss: 0.00001805
Iteration 292/1000 | Loss: 0.00001805
Iteration 293/1000 | Loss: 0.00001805
Iteration 294/1000 | Loss: 0.00001805
Iteration 295/1000 | Loss: 0.00001805
Iteration 296/1000 | Loss: 0.00001805
Iteration 297/1000 | Loss: 0.00001805
Iteration 298/1000 | Loss: 0.00001805
Iteration 299/1000 | Loss: 0.00001805
Iteration 300/1000 | Loss: 0.00001805
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 300. Stopping optimization.
Last 5 losses: [1.8051234292215668e-05, 1.8051234292215668e-05, 1.8051234292215668e-05, 1.8051234292215668e-05, 1.8051234292215668e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8051234292215668e-05

Optimization complete. Final v2v error: 3.547436475753784 mm

Highest mean error: 9.456438064575195 mm for frame 89

Lowest mean error: 3.144845485687256 mm for frame 63

Saving results

Total time: 268.29034519195557
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_32_nl_5533/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5533/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_32_nl_5533/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00973957
Iteration 2/25 | Loss: 0.00331091
Iteration 3/25 | Loss: 0.00222839
Iteration 4/25 | Loss: 0.00163686
Iteration 5/25 | Loss: 0.00152961
Iteration 6/25 | Loss: 0.00151095
Iteration 7/25 | Loss: 0.00151007
Iteration 8/25 | Loss: 0.00147258
Iteration 9/25 | Loss: 0.00142797
Iteration 10/25 | Loss: 0.00141232
Iteration 11/25 | Loss: 0.00139768
Iteration 12/25 | Loss: 0.00138629
Iteration 13/25 | Loss: 0.00137190
Iteration 14/25 | Loss: 0.00136428
Iteration 15/25 | Loss: 0.00135939
Iteration 16/25 | Loss: 0.00135748
Iteration 17/25 | Loss: 0.00136216
Iteration 18/25 | Loss: 0.00135426
Iteration 19/25 | Loss: 0.00134790
Iteration 20/25 | Loss: 0.00134532
Iteration 21/25 | Loss: 0.00134767
Iteration 22/25 | Loss: 0.00134870
Iteration 23/25 | Loss: 0.00134605
Iteration 24/25 | Loss: 0.00134225
Iteration 25/25 | Loss: 0.00134513

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.14572096
Iteration 2/25 | Loss: 0.00479380
Iteration 3/25 | Loss: 0.00421260
Iteration 4/25 | Loss: 0.00421260
Iteration 5/25 | Loss: 0.00421259
Iteration 6/25 | Loss: 0.00421259
Iteration 7/25 | Loss: 0.00421259
Iteration 8/25 | Loss: 0.00421259
Iteration 9/25 | Loss: 0.00421259
Iteration 10/25 | Loss: 0.00421259
Iteration 11/25 | Loss: 0.00421259
Iteration 12/25 | Loss: 0.00421259
Iteration 13/25 | Loss: 0.00421259
Iteration 14/25 | Loss: 0.00421259
Iteration 15/25 | Loss: 0.00421259
Iteration 16/25 | Loss: 0.00421259
Iteration 17/25 | Loss: 0.00421259
Iteration 18/25 | Loss: 0.00421259
Iteration 19/25 | Loss: 0.00421259
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.004212592728435993, 0.004212592728435993, 0.004212592728435993, 0.004212592728435993, 0.004212592728435993]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004212592728435993

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00421259
Iteration 2/1000 | Loss: 0.00128995
Iteration 3/1000 | Loss: 0.00051532
Iteration 4/1000 | Loss: 0.00038780
Iteration 5/1000 | Loss: 0.00283560
Iteration 6/1000 | Loss: 0.00330708
Iteration 7/1000 | Loss: 0.00035992
Iteration 8/1000 | Loss: 0.00081358
Iteration 9/1000 | Loss: 0.00154375
Iteration 10/1000 | Loss: 0.00035567
Iteration 11/1000 | Loss: 0.00034655
Iteration 12/1000 | Loss: 0.00017856
Iteration 13/1000 | Loss: 0.00011541
Iteration 14/1000 | Loss: 0.00025900
Iteration 15/1000 | Loss: 0.00010779
Iteration 16/1000 | Loss: 0.00055303
Iteration 17/1000 | Loss: 0.00225276
Iteration 18/1000 | Loss: 0.00070289
Iteration 19/1000 | Loss: 0.00212724
Iteration 20/1000 | Loss: 0.00079167
Iteration 21/1000 | Loss: 0.00046564
Iteration 22/1000 | Loss: 0.00011430
Iteration 23/1000 | Loss: 0.00071742
Iteration 24/1000 | Loss: 0.00009678
Iteration 25/1000 | Loss: 0.00011401
Iteration 26/1000 | Loss: 0.00069321
Iteration 27/1000 | Loss: 0.00026513
Iteration 28/1000 | Loss: 0.00012257
Iteration 29/1000 | Loss: 0.00013549
Iteration 30/1000 | Loss: 0.00009156
Iteration 31/1000 | Loss: 0.00004668
Iteration 32/1000 | Loss: 0.00015209
Iteration 33/1000 | Loss: 0.00017850
Iteration 34/1000 | Loss: 0.00004041
Iteration 35/1000 | Loss: 0.00055087
Iteration 36/1000 | Loss: 0.00005431
Iteration 37/1000 | Loss: 0.00004209
Iteration 38/1000 | Loss: 0.00003818
Iteration 39/1000 | Loss: 0.00010364
Iteration 40/1000 | Loss: 0.00003637
Iteration 41/1000 | Loss: 0.00004381
Iteration 42/1000 | Loss: 0.00004471
Iteration 43/1000 | Loss: 0.00004699
Iteration 44/1000 | Loss: 0.00004517
Iteration 45/1000 | Loss: 0.00004287
Iteration 46/1000 | Loss: 0.00003065
Iteration 47/1000 | Loss: 0.00002967
Iteration 48/1000 | Loss: 0.00019115
Iteration 49/1000 | Loss: 0.00014490
Iteration 50/1000 | Loss: 0.00003480
Iteration 51/1000 | Loss: 0.00014440
Iteration 52/1000 | Loss: 0.00021484
Iteration 53/1000 | Loss: 0.00061286
Iteration 54/1000 | Loss: 0.00015816
Iteration 55/1000 | Loss: 0.00009693
Iteration 56/1000 | Loss: 0.00009542
Iteration 57/1000 | Loss: 0.00007225
Iteration 58/1000 | Loss: 0.00004112
Iteration 59/1000 | Loss: 0.00004552
Iteration 60/1000 | Loss: 0.00004192
Iteration 61/1000 | Loss: 0.00003989
Iteration 62/1000 | Loss: 0.00003481
Iteration 63/1000 | Loss: 0.00003360
Iteration 64/1000 | Loss: 0.00003373
Iteration 65/1000 | Loss: 0.00030224
Iteration 66/1000 | Loss: 0.00025916
Iteration 67/1000 | Loss: 0.00005090
Iteration 68/1000 | Loss: 0.00004090
Iteration 69/1000 | Loss: 0.00004314
Iteration 70/1000 | Loss: 0.00003265
Iteration 71/1000 | Loss: 0.00007447
Iteration 72/1000 | Loss: 0.00004196
Iteration 73/1000 | Loss: 0.00008658
Iteration 74/1000 | Loss: 0.00006319
Iteration 75/1000 | Loss: 0.00004478
Iteration 76/1000 | Loss: 0.00003561
Iteration 77/1000 | Loss: 0.00006764
Iteration 78/1000 | Loss: 0.00005454
Iteration 79/1000 | Loss: 0.00006535
Iteration 80/1000 | Loss: 0.00006708
Iteration 81/1000 | Loss: 0.00006831
Iteration 82/1000 | Loss: 0.00003582
Iteration 83/1000 | Loss: 0.00003312
Iteration 84/1000 | Loss: 0.00003106
Iteration 85/1000 | Loss: 0.00003219
Iteration 86/1000 | Loss: 0.00002575
Iteration 87/1000 | Loss: 0.00003676
Iteration 88/1000 | Loss: 0.00003231
Iteration 89/1000 | Loss: 0.00003060
Iteration 90/1000 | Loss: 0.00003172
Iteration 91/1000 | Loss: 0.00002936
Iteration 92/1000 | Loss: 0.00003116
Iteration 93/1000 | Loss: 0.00002994
Iteration 94/1000 | Loss: 0.00003710
Iteration 95/1000 | Loss: 0.00003256
Iteration 96/1000 | Loss: 0.00003742
Iteration 97/1000 | Loss: 0.00003463
Iteration 98/1000 | Loss: 0.00003210
Iteration 99/1000 | Loss: 0.00003665
Iteration 100/1000 | Loss: 0.00003118
Iteration 101/1000 | Loss: 0.00003725
Iteration 102/1000 | Loss: 0.00003805
Iteration 103/1000 | Loss: 0.00003216
Iteration 104/1000 | Loss: 0.00003404
Iteration 105/1000 | Loss: 0.00002759
Iteration 106/1000 | Loss: 0.00002749
Iteration 107/1000 | Loss: 0.00004416
Iteration 108/1000 | Loss: 0.00002892
Iteration 109/1000 | Loss: 0.00003124
Iteration 110/1000 | Loss: 0.00003226
Iteration 111/1000 | Loss: 0.00003279
Iteration 112/1000 | Loss: 0.00003593
Iteration 113/1000 | Loss: 0.00003356
Iteration 114/1000 | Loss: 0.00003812
Iteration 115/1000 | Loss: 0.00003319
Iteration 116/1000 | Loss: 0.00003789
Iteration 117/1000 | Loss: 0.00003282
Iteration 118/1000 | Loss: 0.00003768
Iteration 119/1000 | Loss: 0.00003147
Iteration 120/1000 | Loss: 0.00003342
Iteration 121/1000 | Loss: 0.00003027
Iteration 122/1000 | Loss: 0.00003309
Iteration 123/1000 | Loss: 0.00003661
Iteration 124/1000 | Loss: 0.00003275
Iteration 125/1000 | Loss: 0.00003658
Iteration 126/1000 | Loss: 0.00003290
Iteration 127/1000 | Loss: 0.00003483
Iteration 128/1000 | Loss: 0.00003402
Iteration 129/1000 | Loss: 0.00002825
Iteration 130/1000 | Loss: 0.00003990
Iteration 131/1000 | Loss: 0.00003461
Iteration 132/1000 | Loss: 0.00003761
Iteration 133/1000 | Loss: 0.00003358
Iteration 134/1000 | Loss: 0.00002805
Iteration 135/1000 | Loss: 0.00002966
Iteration 136/1000 | Loss: 0.00002797
Iteration 137/1000 | Loss: 0.00003159
Iteration 138/1000 | Loss: 0.00002767
Iteration 139/1000 | Loss: 0.00004289
Iteration 140/1000 | Loss: 0.00003313
Iteration 141/1000 | Loss: 0.00003480
Iteration 142/1000 | Loss: 0.00003297
Iteration 143/1000 | Loss: 0.00003119
Iteration 144/1000 | Loss: 0.00003428
Iteration 145/1000 | Loss: 0.00002993
Iteration 146/1000 | Loss: 0.00003234
Iteration 147/1000 | Loss: 0.00003084
Iteration 148/1000 | Loss: 0.00003051
Iteration 149/1000 | Loss: 0.00003365
Iteration 150/1000 | Loss: 0.00003480
Iteration 151/1000 | Loss: 0.00003440
Iteration 152/1000 | Loss: 0.00003485
Iteration 153/1000 | Loss: 0.00003591
Iteration 154/1000 | Loss: 0.00003930
Iteration 155/1000 | Loss: 0.00003721
Iteration 156/1000 | Loss: 0.00003421
Iteration 157/1000 | Loss: 0.00003515
Iteration 158/1000 | Loss: 0.00003371
Iteration 159/1000 | Loss: 0.00003501
Iteration 160/1000 | Loss: 0.00003337
Iteration 161/1000 | Loss: 0.00003080
Iteration 162/1000 | Loss: 0.00003292
Iteration 163/1000 | Loss: 0.00003077
Iteration 164/1000 | Loss: 0.00003208
Iteration 165/1000 | Loss: 0.00003467
Iteration 166/1000 | Loss: 0.00003592
Iteration 167/1000 | Loss: 0.00002864
Iteration 168/1000 | Loss: 0.00004249
Iteration 169/1000 | Loss: 0.00003431
Iteration 170/1000 | Loss: 0.00004079
Iteration 171/1000 | Loss: 0.00003391
Iteration 172/1000 | Loss: 0.00004677
Iteration 173/1000 | Loss: 0.00002897
Iteration 174/1000 | Loss: 0.00002598
Iteration 175/1000 | Loss: 0.00002452
Iteration 176/1000 | Loss: 0.00002375
Iteration 177/1000 | Loss: 0.00002344
Iteration 178/1000 | Loss: 0.00002320
Iteration 179/1000 | Loss: 0.00002318
Iteration 180/1000 | Loss: 0.00002311
Iteration 181/1000 | Loss: 0.00002305
Iteration 182/1000 | Loss: 0.00002305
Iteration 183/1000 | Loss: 0.00002305
Iteration 184/1000 | Loss: 0.00002304
Iteration 185/1000 | Loss: 0.00002304
Iteration 186/1000 | Loss: 0.00002303
Iteration 187/1000 | Loss: 0.00002302
Iteration 188/1000 | Loss: 0.00002300
Iteration 189/1000 | Loss: 0.00002300
Iteration 190/1000 | Loss: 0.00002296
Iteration 191/1000 | Loss: 0.00002293
Iteration 192/1000 | Loss: 0.00002292
Iteration 193/1000 | Loss: 0.00002292
Iteration 194/1000 | Loss: 0.00002292
Iteration 195/1000 | Loss: 0.00002291
Iteration 196/1000 | Loss: 0.00002291
Iteration 197/1000 | Loss: 0.00002290
Iteration 198/1000 | Loss: 0.00002289
Iteration 199/1000 | Loss: 0.00002289
Iteration 200/1000 | Loss: 0.00002288
Iteration 201/1000 | Loss: 0.00002286
Iteration 202/1000 | Loss: 0.00002285
Iteration 203/1000 | Loss: 0.00002284
Iteration 204/1000 | Loss: 0.00002281
Iteration 205/1000 | Loss: 0.00002275
Iteration 206/1000 | Loss: 0.00002266
Iteration 207/1000 | Loss: 0.00002266
Iteration 208/1000 | Loss: 0.00002266
Iteration 209/1000 | Loss: 0.00002265
Iteration 210/1000 | Loss: 0.00002264
Iteration 211/1000 | Loss: 0.00002264
Iteration 212/1000 | Loss: 0.00002264
Iteration 213/1000 | Loss: 0.00002263
Iteration 214/1000 | Loss: 0.00002263
Iteration 215/1000 | Loss: 0.00002263
Iteration 216/1000 | Loss: 0.00002263
Iteration 217/1000 | Loss: 0.00002263
Iteration 218/1000 | Loss: 0.00002263
Iteration 219/1000 | Loss: 0.00002262
Iteration 220/1000 | Loss: 0.00002262
Iteration 221/1000 | Loss: 0.00002262
Iteration 222/1000 | Loss: 0.00002262
Iteration 223/1000 | Loss: 0.00002258
Iteration 224/1000 | Loss: 0.00002255
Iteration 225/1000 | Loss: 0.00002254
Iteration 226/1000 | Loss: 0.00002254
Iteration 227/1000 | Loss: 0.00002254
Iteration 228/1000 | Loss: 0.00002253
Iteration 229/1000 | Loss: 0.00002253
Iteration 230/1000 | Loss: 0.00002253
Iteration 231/1000 | Loss: 0.00002252
Iteration 232/1000 | Loss: 0.00002251
Iteration 233/1000 | Loss: 0.00002251
Iteration 234/1000 | Loss: 0.00002251
Iteration 235/1000 | Loss: 0.00002250
Iteration 236/1000 | Loss: 0.00002250
Iteration 237/1000 | Loss: 0.00002250
Iteration 238/1000 | Loss: 0.00002250
Iteration 239/1000 | Loss: 0.00002250
Iteration 240/1000 | Loss: 0.00002249
Iteration 241/1000 | Loss: 0.00002249
Iteration 242/1000 | Loss: 0.00002249
Iteration 243/1000 | Loss: 0.00002249
Iteration 244/1000 | Loss: 0.00002248
Iteration 245/1000 | Loss: 0.00002248
Iteration 246/1000 | Loss: 0.00002248
Iteration 247/1000 | Loss: 0.00002248
Iteration 248/1000 | Loss: 0.00002248
Iteration 249/1000 | Loss: 0.00002248
Iteration 250/1000 | Loss: 0.00002247
Iteration 251/1000 | Loss: 0.00002247
Iteration 252/1000 | Loss: 0.00002247
Iteration 253/1000 | Loss: 0.00002246
Iteration 254/1000 | Loss: 0.00002246
Iteration 255/1000 | Loss: 0.00002246
Iteration 256/1000 | Loss: 0.00002246
Iteration 257/1000 | Loss: 0.00002246
Iteration 258/1000 | Loss: 0.00002245
Iteration 259/1000 | Loss: 0.00002245
Iteration 260/1000 | Loss: 0.00002245
Iteration 261/1000 | Loss: 0.00002245
Iteration 262/1000 | Loss: 0.00002245
Iteration 263/1000 | Loss: 0.00002245
Iteration 264/1000 | Loss: 0.00002244
Iteration 265/1000 | Loss: 0.00002244
Iteration 266/1000 | Loss: 0.00002244
Iteration 267/1000 | Loss: 0.00002244
Iteration 268/1000 | Loss: 0.00002244
Iteration 269/1000 | Loss: 0.00002244
Iteration 270/1000 | Loss: 0.00002244
Iteration 271/1000 | Loss: 0.00002244
Iteration 272/1000 | Loss: 0.00002244
Iteration 273/1000 | Loss: 0.00002244
Iteration 274/1000 | Loss: 0.00002243
Iteration 275/1000 | Loss: 0.00002243
Iteration 276/1000 | Loss: 0.00002243
Iteration 277/1000 | Loss: 0.00002242
Iteration 278/1000 | Loss: 0.00002242
Iteration 279/1000 | Loss: 0.00002242
Iteration 280/1000 | Loss: 0.00002242
Iteration 281/1000 | Loss: 0.00002242
Iteration 282/1000 | Loss: 0.00002242
Iteration 283/1000 | Loss: 0.00002241
Iteration 284/1000 | Loss: 0.00002241
Iteration 285/1000 | Loss: 0.00002241
Iteration 286/1000 | Loss: 0.00002241
Iteration 287/1000 | Loss: 0.00002241
Iteration 288/1000 | Loss: 0.00002241
Iteration 289/1000 | Loss: 0.00002241
Iteration 290/1000 | Loss: 0.00002241
Iteration 291/1000 | Loss: 0.00002240
Iteration 292/1000 | Loss: 0.00002239
Iteration 293/1000 | Loss: 0.00002239
Iteration 294/1000 | Loss: 0.00002239
Iteration 295/1000 | Loss: 0.00002238
Iteration 296/1000 | Loss: 0.00002238
Iteration 297/1000 | Loss: 0.00002238
Iteration 298/1000 | Loss: 0.00002237
Iteration 299/1000 | Loss: 0.00002237
Iteration 300/1000 | Loss: 0.00002237
Iteration 301/1000 | Loss: 0.00002237
Iteration 302/1000 | Loss: 0.00002237
Iteration 303/1000 | Loss: 0.00002237
Iteration 304/1000 | Loss: 0.00002236
Iteration 305/1000 | Loss: 0.00002236
Iteration 306/1000 | Loss: 0.00002236
Iteration 307/1000 | Loss: 0.00002236
Iteration 308/1000 | Loss: 0.00002236
Iteration 309/1000 | Loss: 0.00002236
Iteration 310/1000 | Loss: 0.00002236
Iteration 311/1000 | Loss: 0.00002236
Iteration 312/1000 | Loss: 0.00002236
Iteration 313/1000 | Loss: 0.00002236
Iteration 314/1000 | Loss: 0.00002236
Iteration 315/1000 | Loss: 0.00002236
Iteration 316/1000 | Loss: 0.00002236
Iteration 317/1000 | Loss: 0.00002235
Iteration 318/1000 | Loss: 0.00002235
Iteration 319/1000 | Loss: 0.00002235
Iteration 320/1000 | Loss: 0.00002235
Iteration 321/1000 | Loss: 0.00002235
Iteration 322/1000 | Loss: 0.00002235
Iteration 323/1000 | Loss: 0.00002235
Iteration 324/1000 | Loss: 0.00002235
Iteration 325/1000 | Loss: 0.00002234
Iteration 326/1000 | Loss: 0.00002234
Iteration 327/1000 | Loss: 0.00002234
Iteration 328/1000 | Loss: 0.00002234
Iteration 329/1000 | Loss: 0.00002234
Iteration 330/1000 | Loss: 0.00002234
Iteration 331/1000 | Loss: 0.00002234
Iteration 332/1000 | Loss: 0.00002234
Iteration 333/1000 | Loss: 0.00002234
Iteration 334/1000 | Loss: 0.00002234
Iteration 335/1000 | Loss: 0.00002234
Iteration 336/1000 | Loss: 0.00002234
Iteration 337/1000 | Loss: 0.00002234
Iteration 338/1000 | Loss: 0.00002234
Iteration 339/1000 | Loss: 0.00002234
Iteration 340/1000 | Loss: 0.00002234
Iteration 341/1000 | Loss: 0.00002234
Iteration 342/1000 | Loss: 0.00002234
Iteration 343/1000 | Loss: 0.00002234
Iteration 344/1000 | Loss: 0.00002234
Iteration 345/1000 | Loss: 0.00002234
Iteration 346/1000 | Loss: 0.00002234
Iteration 347/1000 | Loss: 0.00002234
Iteration 348/1000 | Loss: 0.00002234
Iteration 349/1000 | Loss: 0.00002234
Iteration 350/1000 | Loss: 0.00002234
Iteration 351/1000 | Loss: 0.00002234
Iteration 352/1000 | Loss: 0.00002234
Iteration 353/1000 | Loss: 0.00002234
Iteration 354/1000 | Loss: 0.00002234
Iteration 355/1000 | Loss: 0.00002234
Iteration 356/1000 | Loss: 0.00002234
Iteration 357/1000 | Loss: 0.00002234
Iteration 358/1000 | Loss: 0.00002234
Iteration 359/1000 | Loss: 0.00002234
Iteration 360/1000 | Loss: 0.00002234
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 360. Stopping optimization.
Last 5 losses: [2.2339696442941204e-05, 2.2339696442941204e-05, 2.2339696442941204e-05, 2.2339696442941204e-05, 2.2339696442941204e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2339696442941204e-05

Optimization complete. Final v2v error: 3.7080116271972656 mm

Highest mean error: 8.99680233001709 mm for frame 48

Lowest mean error: 2.6733343601226807 mm for frame 154

Saving results

Total time: 332.44319915771484
