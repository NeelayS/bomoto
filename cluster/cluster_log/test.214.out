Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=214, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 11984-12039
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00679746
Iteration 2/25 | Loss: 0.00141216
Iteration 3/25 | Loss: 0.00131329
Iteration 4/25 | Loss: 0.00127461
Iteration 5/25 | Loss: 0.00127286
Iteration 6/25 | Loss: 0.00126356
Iteration 7/25 | Loss: 0.00127703
Iteration 8/25 | Loss: 0.00125790
Iteration 9/25 | Loss: 0.00125843
Iteration 10/25 | Loss: 0.00125756
Iteration 11/25 | Loss: 0.00125427
Iteration 12/25 | Loss: 0.00125348
Iteration 13/25 | Loss: 0.00125318
Iteration 14/25 | Loss: 0.00125317
Iteration 15/25 | Loss: 0.00125317
Iteration 16/25 | Loss: 0.00125317
Iteration 17/25 | Loss: 0.00125317
Iteration 18/25 | Loss: 0.00125317
Iteration 19/25 | Loss: 0.00125317
Iteration 20/25 | Loss: 0.00125317
Iteration 21/25 | Loss: 0.00125317
Iteration 22/25 | Loss: 0.00125316
Iteration 23/25 | Loss: 0.00125316
Iteration 24/25 | Loss: 0.00125316
Iteration 25/25 | Loss: 0.00125316

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.36777115
Iteration 2/25 | Loss: 0.00091793
Iteration 3/25 | Loss: 0.00090230
Iteration 4/25 | Loss: 0.00090230
Iteration 5/25 | Loss: 0.00090230
Iteration 6/25 | Loss: 0.00090230
Iteration 7/25 | Loss: 0.00090229
Iteration 8/25 | Loss: 0.00090229
Iteration 9/25 | Loss: 0.00090229
Iteration 10/25 | Loss: 0.00090229
Iteration 11/25 | Loss: 0.00090229
Iteration 12/25 | Loss: 0.00090229
Iteration 13/25 | Loss: 0.00090229
Iteration 14/25 | Loss: 0.00090229
Iteration 15/25 | Loss: 0.00090229
Iteration 16/25 | Loss: 0.00090229
Iteration 17/25 | Loss: 0.00090229
Iteration 18/25 | Loss: 0.00090229
Iteration 19/25 | Loss: 0.00090229
Iteration 20/25 | Loss: 0.00090229
Iteration 21/25 | Loss: 0.00090229
Iteration 22/25 | Loss: 0.00090229
Iteration 23/25 | Loss: 0.00090229
Iteration 24/25 | Loss: 0.00090229
Iteration 25/25 | Loss: 0.00090229

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090229
Iteration 2/1000 | Loss: 0.00002402
Iteration 3/1000 | Loss: 0.00003163
Iteration 4/1000 | Loss: 0.00001567
Iteration 5/1000 | Loss: 0.00001469
Iteration 6/1000 | Loss: 0.00002666
Iteration 7/1000 | Loss: 0.00001384
Iteration 8/1000 | Loss: 0.00001945
Iteration 9/1000 | Loss: 0.00001336
Iteration 10/1000 | Loss: 0.00002437
Iteration 11/1000 | Loss: 0.00001322
Iteration 12/1000 | Loss: 0.00001298
Iteration 13/1000 | Loss: 0.00001297
Iteration 14/1000 | Loss: 0.00001290
Iteration 15/1000 | Loss: 0.00001289
Iteration 16/1000 | Loss: 0.00001288
Iteration 17/1000 | Loss: 0.00001287
Iteration 18/1000 | Loss: 0.00001286
Iteration 19/1000 | Loss: 0.00001275
Iteration 20/1000 | Loss: 0.00001272
Iteration 21/1000 | Loss: 0.00001271
Iteration 22/1000 | Loss: 0.00001271
Iteration 23/1000 | Loss: 0.00001269
Iteration 24/1000 | Loss: 0.00001269
Iteration 25/1000 | Loss: 0.00001267
Iteration 26/1000 | Loss: 0.00001267
Iteration 27/1000 | Loss: 0.00001266
Iteration 28/1000 | Loss: 0.00001266
Iteration 29/1000 | Loss: 0.00001266
Iteration 30/1000 | Loss: 0.00001263
Iteration 31/1000 | Loss: 0.00001262
Iteration 32/1000 | Loss: 0.00001262
Iteration 33/1000 | Loss: 0.00001260
Iteration 34/1000 | Loss: 0.00001260
Iteration 35/1000 | Loss: 0.00001257
Iteration 36/1000 | Loss: 0.00001254
Iteration 37/1000 | Loss: 0.00001254
Iteration 38/1000 | Loss: 0.00001252
Iteration 39/1000 | Loss: 0.00001251
Iteration 40/1000 | Loss: 0.00001919
Iteration 41/1000 | Loss: 0.00001919
Iteration 42/1000 | Loss: 0.00001247
Iteration 43/1000 | Loss: 0.00001241
Iteration 44/1000 | Loss: 0.00001241
Iteration 45/1000 | Loss: 0.00001241
Iteration 46/1000 | Loss: 0.00001241
Iteration 47/1000 | Loss: 0.00001241
Iteration 48/1000 | Loss: 0.00001241
Iteration 49/1000 | Loss: 0.00001241
Iteration 50/1000 | Loss: 0.00001241
Iteration 51/1000 | Loss: 0.00001241
Iteration 52/1000 | Loss: 0.00001241
Iteration 53/1000 | Loss: 0.00001241
Iteration 54/1000 | Loss: 0.00001241
Iteration 55/1000 | Loss: 0.00001241
Iteration 56/1000 | Loss: 0.00001241
Iteration 57/1000 | Loss: 0.00001241
Iteration 58/1000 | Loss: 0.00001241
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 58. Stopping optimization.
Last 5 losses: [1.2406773748807609e-05, 1.2406773748807609e-05, 1.2406773748807609e-05, 1.2406773748807609e-05, 1.2406773748807609e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2406773748807609e-05

Optimization complete. Final v2v error: 3.027374505996704 mm

Highest mean error: 3.523128032684326 mm for frame 127

Lowest mean error: 2.7932746410369873 mm for frame 4

Saving results

Total time: 57.11299395561218
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00765831
Iteration 2/25 | Loss: 0.00174411
Iteration 3/25 | Loss: 0.00159767
Iteration 4/25 | Loss: 0.00157163
Iteration 5/25 | Loss: 0.00156453
Iteration 6/25 | Loss: 0.00156315
Iteration 7/25 | Loss: 0.00156315
Iteration 8/25 | Loss: 0.00156315
Iteration 9/25 | Loss: 0.00156315
Iteration 10/25 | Loss: 0.00156315
Iteration 11/25 | Loss: 0.00156315
Iteration 12/25 | Loss: 0.00156315
Iteration 13/25 | Loss: 0.00156315
Iteration 14/25 | Loss: 0.00156315
Iteration 15/25 | Loss: 0.00156315
Iteration 16/25 | Loss: 0.00156315
Iteration 17/25 | Loss: 0.00156315
Iteration 18/25 | Loss: 0.00156315
Iteration 19/25 | Loss: 0.00156315
Iteration 20/25 | Loss: 0.00156315
Iteration 21/25 | Loss: 0.00156315
Iteration 22/25 | Loss: 0.00156315
Iteration 23/25 | Loss: 0.00156315
Iteration 24/25 | Loss: 0.00156315
Iteration 25/25 | Loss: 0.00156315

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.96018457
Iteration 2/25 | Loss: 0.00151723
Iteration 3/25 | Loss: 0.00151723
Iteration 4/25 | Loss: 0.00151723
Iteration 5/25 | Loss: 0.00151723
Iteration 6/25 | Loss: 0.00151723
Iteration 7/25 | Loss: 0.00151723
Iteration 8/25 | Loss: 0.00151723
Iteration 9/25 | Loss: 0.00151723
Iteration 10/25 | Loss: 0.00151723
Iteration 11/25 | Loss: 0.00151723
Iteration 12/25 | Loss: 0.00151723
Iteration 13/25 | Loss: 0.00151723
Iteration 14/25 | Loss: 0.00151723
Iteration 15/25 | Loss: 0.00151723
Iteration 16/25 | Loss: 0.00151723
Iteration 17/25 | Loss: 0.00151723
Iteration 18/25 | Loss: 0.00151723
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0015172252897173166, 0.0015172252897173166, 0.0015172252897173166, 0.0015172252897173166, 0.0015172252897173166]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015172252897173166

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00151723
Iteration 2/1000 | Loss: 0.00009805
Iteration 3/1000 | Loss: 0.00007081
Iteration 4/1000 | Loss: 0.00005905
Iteration 5/1000 | Loss: 0.00005544
Iteration 6/1000 | Loss: 0.00005352
Iteration 7/1000 | Loss: 0.00005228
Iteration 8/1000 | Loss: 0.00005134
Iteration 9/1000 | Loss: 0.00005069
Iteration 10/1000 | Loss: 0.00005008
Iteration 11/1000 | Loss: 0.00004964
Iteration 12/1000 | Loss: 0.00004929
Iteration 13/1000 | Loss: 0.00004896
Iteration 14/1000 | Loss: 0.00004857
Iteration 15/1000 | Loss: 0.00004828
Iteration 16/1000 | Loss: 0.00004817
Iteration 17/1000 | Loss: 0.00004810
Iteration 18/1000 | Loss: 0.00004803
Iteration 19/1000 | Loss: 0.00004803
Iteration 20/1000 | Loss: 0.00004794
Iteration 21/1000 | Loss: 0.00004788
Iteration 22/1000 | Loss: 0.00004775
Iteration 23/1000 | Loss: 0.00004773
Iteration 24/1000 | Loss: 0.00004767
Iteration 25/1000 | Loss: 0.00004761
Iteration 26/1000 | Loss: 0.00004761
Iteration 27/1000 | Loss: 0.00004757
Iteration 28/1000 | Loss: 0.00004757
Iteration 29/1000 | Loss: 0.00004756
Iteration 30/1000 | Loss: 0.00004755
Iteration 31/1000 | Loss: 0.00004755
Iteration 32/1000 | Loss: 0.00004754
Iteration 33/1000 | Loss: 0.00004751
Iteration 34/1000 | Loss: 0.00004748
Iteration 35/1000 | Loss: 0.00004744
Iteration 36/1000 | Loss: 0.00004743
Iteration 37/1000 | Loss: 0.00004743
Iteration 38/1000 | Loss: 0.00004742
Iteration 39/1000 | Loss: 0.00004741
Iteration 40/1000 | Loss: 0.00004741
Iteration 41/1000 | Loss: 0.00004741
Iteration 42/1000 | Loss: 0.00004741
Iteration 43/1000 | Loss: 0.00004740
Iteration 44/1000 | Loss: 0.00004740
Iteration 45/1000 | Loss: 0.00004740
Iteration 46/1000 | Loss: 0.00004740
Iteration 47/1000 | Loss: 0.00004740
Iteration 48/1000 | Loss: 0.00004739
Iteration 49/1000 | Loss: 0.00004737
Iteration 50/1000 | Loss: 0.00004737
Iteration 51/1000 | Loss: 0.00004737
Iteration 52/1000 | Loss: 0.00004737
Iteration 53/1000 | Loss: 0.00004737
Iteration 54/1000 | Loss: 0.00004737
Iteration 55/1000 | Loss: 0.00004736
Iteration 56/1000 | Loss: 0.00004736
Iteration 57/1000 | Loss: 0.00004736
Iteration 58/1000 | Loss: 0.00004736
Iteration 59/1000 | Loss: 0.00004736
Iteration 60/1000 | Loss: 0.00004736
Iteration 61/1000 | Loss: 0.00004733
Iteration 62/1000 | Loss: 0.00004733
Iteration 63/1000 | Loss: 0.00004733
Iteration 64/1000 | Loss: 0.00004733
Iteration 65/1000 | Loss: 0.00004733
Iteration 66/1000 | Loss: 0.00004733
Iteration 67/1000 | Loss: 0.00004733
Iteration 68/1000 | Loss: 0.00004733
Iteration 69/1000 | Loss: 0.00004732
Iteration 70/1000 | Loss: 0.00004732
Iteration 71/1000 | Loss: 0.00004732
Iteration 72/1000 | Loss: 0.00004732
Iteration 73/1000 | Loss: 0.00004732
Iteration 74/1000 | Loss: 0.00004732
Iteration 75/1000 | Loss: 0.00004731
Iteration 76/1000 | Loss: 0.00004731
Iteration 77/1000 | Loss: 0.00004730
Iteration 78/1000 | Loss: 0.00004730
Iteration 79/1000 | Loss: 0.00004730
Iteration 80/1000 | Loss: 0.00004730
Iteration 81/1000 | Loss: 0.00004730
Iteration 82/1000 | Loss: 0.00004730
Iteration 83/1000 | Loss: 0.00004730
Iteration 84/1000 | Loss: 0.00004730
Iteration 85/1000 | Loss: 0.00004729
Iteration 86/1000 | Loss: 0.00004729
Iteration 87/1000 | Loss: 0.00004729
Iteration 88/1000 | Loss: 0.00004729
Iteration 89/1000 | Loss: 0.00004729
Iteration 90/1000 | Loss: 0.00004729
Iteration 91/1000 | Loss: 0.00004728
Iteration 92/1000 | Loss: 0.00004728
Iteration 93/1000 | Loss: 0.00004728
Iteration 94/1000 | Loss: 0.00004728
Iteration 95/1000 | Loss: 0.00004728
Iteration 96/1000 | Loss: 0.00004728
Iteration 97/1000 | Loss: 0.00004728
Iteration 98/1000 | Loss: 0.00004728
Iteration 99/1000 | Loss: 0.00004728
Iteration 100/1000 | Loss: 0.00004728
Iteration 101/1000 | Loss: 0.00004728
Iteration 102/1000 | Loss: 0.00004728
Iteration 103/1000 | Loss: 0.00004728
Iteration 104/1000 | Loss: 0.00004727
Iteration 105/1000 | Loss: 0.00004727
Iteration 106/1000 | Loss: 0.00004727
Iteration 107/1000 | Loss: 0.00004727
Iteration 108/1000 | Loss: 0.00004726
Iteration 109/1000 | Loss: 0.00004726
Iteration 110/1000 | Loss: 0.00004726
Iteration 111/1000 | Loss: 0.00004726
Iteration 112/1000 | Loss: 0.00004726
Iteration 113/1000 | Loss: 0.00004726
Iteration 114/1000 | Loss: 0.00004726
Iteration 115/1000 | Loss: 0.00004726
Iteration 116/1000 | Loss: 0.00004726
Iteration 117/1000 | Loss: 0.00004725
Iteration 118/1000 | Loss: 0.00004725
Iteration 119/1000 | Loss: 0.00004725
Iteration 120/1000 | Loss: 0.00004725
Iteration 121/1000 | Loss: 0.00004724
Iteration 122/1000 | Loss: 0.00004724
Iteration 123/1000 | Loss: 0.00004724
Iteration 124/1000 | Loss: 0.00004724
Iteration 125/1000 | Loss: 0.00004724
Iteration 126/1000 | Loss: 0.00004724
Iteration 127/1000 | Loss: 0.00004724
Iteration 128/1000 | Loss: 0.00004724
Iteration 129/1000 | Loss: 0.00004724
Iteration 130/1000 | Loss: 0.00004724
Iteration 131/1000 | Loss: 0.00004724
Iteration 132/1000 | Loss: 0.00004723
Iteration 133/1000 | Loss: 0.00004723
Iteration 134/1000 | Loss: 0.00004723
Iteration 135/1000 | Loss: 0.00004723
Iteration 136/1000 | Loss: 0.00004723
Iteration 137/1000 | Loss: 0.00004723
Iteration 138/1000 | Loss: 0.00004722
Iteration 139/1000 | Loss: 0.00004722
Iteration 140/1000 | Loss: 0.00004722
Iteration 141/1000 | Loss: 0.00004722
Iteration 142/1000 | Loss: 0.00004722
Iteration 143/1000 | Loss: 0.00004722
Iteration 144/1000 | Loss: 0.00004722
Iteration 145/1000 | Loss: 0.00004722
Iteration 146/1000 | Loss: 0.00004722
Iteration 147/1000 | Loss: 0.00004721
Iteration 148/1000 | Loss: 0.00004721
Iteration 149/1000 | Loss: 0.00004721
Iteration 150/1000 | Loss: 0.00004721
Iteration 151/1000 | Loss: 0.00004721
Iteration 152/1000 | Loss: 0.00004721
Iteration 153/1000 | Loss: 0.00004721
Iteration 154/1000 | Loss: 0.00004721
Iteration 155/1000 | Loss: 0.00004721
Iteration 156/1000 | Loss: 0.00004721
Iteration 157/1000 | Loss: 0.00004721
Iteration 158/1000 | Loss: 0.00004721
Iteration 159/1000 | Loss: 0.00004721
Iteration 160/1000 | Loss: 0.00004721
Iteration 161/1000 | Loss: 0.00004721
Iteration 162/1000 | Loss: 0.00004721
Iteration 163/1000 | Loss: 0.00004720
Iteration 164/1000 | Loss: 0.00004720
Iteration 165/1000 | Loss: 0.00004720
Iteration 166/1000 | Loss: 0.00004720
Iteration 167/1000 | Loss: 0.00004720
Iteration 168/1000 | Loss: 0.00004720
Iteration 169/1000 | Loss: 0.00004720
Iteration 170/1000 | Loss: 0.00004720
Iteration 171/1000 | Loss: 0.00004720
Iteration 172/1000 | Loss: 0.00004719
Iteration 173/1000 | Loss: 0.00004719
Iteration 174/1000 | Loss: 0.00004719
Iteration 175/1000 | Loss: 0.00004719
Iteration 176/1000 | Loss: 0.00004719
Iteration 177/1000 | Loss: 0.00004719
Iteration 178/1000 | Loss: 0.00004719
Iteration 179/1000 | Loss: 0.00004719
Iteration 180/1000 | Loss: 0.00004719
Iteration 181/1000 | Loss: 0.00004719
Iteration 182/1000 | Loss: 0.00004719
Iteration 183/1000 | Loss: 0.00004719
Iteration 184/1000 | Loss: 0.00004719
Iteration 185/1000 | Loss: 0.00004719
Iteration 186/1000 | Loss: 0.00004719
Iteration 187/1000 | Loss: 0.00004719
Iteration 188/1000 | Loss: 0.00004719
Iteration 189/1000 | Loss: 0.00004719
Iteration 190/1000 | Loss: 0.00004719
Iteration 191/1000 | Loss: 0.00004719
Iteration 192/1000 | Loss: 0.00004719
Iteration 193/1000 | Loss: 0.00004719
Iteration 194/1000 | Loss: 0.00004719
Iteration 195/1000 | Loss: 0.00004719
Iteration 196/1000 | Loss: 0.00004719
Iteration 197/1000 | Loss: 0.00004719
Iteration 198/1000 | Loss: 0.00004719
Iteration 199/1000 | Loss: 0.00004719
Iteration 200/1000 | Loss: 0.00004719
Iteration 201/1000 | Loss: 0.00004719
Iteration 202/1000 | Loss: 0.00004719
Iteration 203/1000 | Loss: 0.00004719
Iteration 204/1000 | Loss: 0.00004719
Iteration 205/1000 | Loss: 0.00004719
Iteration 206/1000 | Loss: 0.00004719
Iteration 207/1000 | Loss: 0.00004719
Iteration 208/1000 | Loss: 0.00004719
Iteration 209/1000 | Loss: 0.00004719
Iteration 210/1000 | Loss: 0.00004719
Iteration 211/1000 | Loss: 0.00004719
Iteration 212/1000 | Loss: 0.00004719
Iteration 213/1000 | Loss: 0.00004719
Iteration 214/1000 | Loss: 0.00004719
Iteration 215/1000 | Loss: 0.00004719
Iteration 216/1000 | Loss: 0.00004719
Iteration 217/1000 | Loss: 0.00004719
Iteration 218/1000 | Loss: 0.00004719
Iteration 219/1000 | Loss: 0.00004719
Iteration 220/1000 | Loss: 0.00004719
Iteration 221/1000 | Loss: 0.00004719
Iteration 222/1000 | Loss: 0.00004719
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 222. Stopping optimization.
Last 5 losses: [4.719008211395703e-05, 4.719008211395703e-05, 4.719008211395703e-05, 4.719008211395703e-05, 4.719008211395703e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.719008211395703e-05

Optimization complete. Final v2v error: 5.632068157196045 mm

Highest mean error: 7.252782821655273 mm for frame 66

Lowest mean error: 4.7641096115112305 mm for frame 145

Saving results

Total time: 58.833935499191284
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00401376
Iteration 2/25 | Loss: 0.00135959
Iteration 3/25 | Loss: 0.00126162
Iteration 4/25 | Loss: 0.00125116
Iteration 5/25 | Loss: 0.00124885
Iteration 6/25 | Loss: 0.00124834
Iteration 7/25 | Loss: 0.00124834
Iteration 8/25 | Loss: 0.00124834
Iteration 9/25 | Loss: 0.00124834
Iteration 10/25 | Loss: 0.00124834
Iteration 11/25 | Loss: 0.00124834
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012483352329581976, 0.0012483352329581976, 0.0012483352329581976, 0.0012483352329581976, 0.0012483352329581976]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012483352329581976

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42673922
Iteration 2/25 | Loss: 0.00086934
Iteration 3/25 | Loss: 0.00086934
Iteration 4/25 | Loss: 0.00086934
Iteration 5/25 | Loss: 0.00086934
Iteration 6/25 | Loss: 0.00086934
Iteration 7/25 | Loss: 0.00086934
Iteration 8/25 | Loss: 0.00086934
Iteration 9/25 | Loss: 0.00086933
Iteration 10/25 | Loss: 0.00086933
Iteration 11/25 | Loss: 0.00086933
Iteration 12/25 | Loss: 0.00086933
Iteration 13/25 | Loss: 0.00086933
Iteration 14/25 | Loss: 0.00086933
Iteration 15/25 | Loss: 0.00086933
Iteration 16/25 | Loss: 0.00086933
Iteration 17/25 | Loss: 0.00086933
Iteration 18/25 | Loss: 0.00086933
Iteration 19/25 | Loss: 0.00086933
Iteration 20/25 | Loss: 0.00086933
Iteration 21/25 | Loss: 0.00086933
Iteration 22/25 | Loss: 0.00086933
Iteration 23/25 | Loss: 0.00086933
Iteration 24/25 | Loss: 0.00086933
Iteration 25/25 | Loss: 0.00086933

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086933
Iteration 2/1000 | Loss: 0.00002444
Iteration 3/1000 | Loss: 0.00001714
Iteration 4/1000 | Loss: 0.00001528
Iteration 5/1000 | Loss: 0.00001430
Iteration 6/1000 | Loss: 0.00001354
Iteration 7/1000 | Loss: 0.00001313
Iteration 8/1000 | Loss: 0.00001292
Iteration 9/1000 | Loss: 0.00001270
Iteration 10/1000 | Loss: 0.00001261
Iteration 11/1000 | Loss: 0.00001260
Iteration 12/1000 | Loss: 0.00001260
Iteration 13/1000 | Loss: 0.00001260
Iteration 14/1000 | Loss: 0.00001260
Iteration 15/1000 | Loss: 0.00001254
Iteration 16/1000 | Loss: 0.00001252
Iteration 17/1000 | Loss: 0.00001252
Iteration 18/1000 | Loss: 0.00001251
Iteration 19/1000 | Loss: 0.00001250
Iteration 20/1000 | Loss: 0.00001249
Iteration 21/1000 | Loss: 0.00001248
Iteration 22/1000 | Loss: 0.00001248
Iteration 23/1000 | Loss: 0.00001247
Iteration 24/1000 | Loss: 0.00001247
Iteration 25/1000 | Loss: 0.00001246
Iteration 26/1000 | Loss: 0.00001246
Iteration 27/1000 | Loss: 0.00001245
Iteration 28/1000 | Loss: 0.00001245
Iteration 29/1000 | Loss: 0.00001242
Iteration 30/1000 | Loss: 0.00001241
Iteration 31/1000 | Loss: 0.00001241
Iteration 32/1000 | Loss: 0.00001241
Iteration 33/1000 | Loss: 0.00001240
Iteration 34/1000 | Loss: 0.00001240
Iteration 35/1000 | Loss: 0.00001239
Iteration 36/1000 | Loss: 0.00001239
Iteration 37/1000 | Loss: 0.00001239
Iteration 38/1000 | Loss: 0.00001238
Iteration 39/1000 | Loss: 0.00001238
Iteration 40/1000 | Loss: 0.00001238
Iteration 41/1000 | Loss: 0.00001237
Iteration 42/1000 | Loss: 0.00001237
Iteration 43/1000 | Loss: 0.00001236
Iteration 44/1000 | Loss: 0.00001236
Iteration 45/1000 | Loss: 0.00001236
Iteration 46/1000 | Loss: 0.00001235
Iteration 47/1000 | Loss: 0.00001235
Iteration 48/1000 | Loss: 0.00001235
Iteration 49/1000 | Loss: 0.00001232
Iteration 50/1000 | Loss: 0.00001231
Iteration 51/1000 | Loss: 0.00001231
Iteration 52/1000 | Loss: 0.00001231
Iteration 53/1000 | Loss: 0.00001230
Iteration 54/1000 | Loss: 0.00001229
Iteration 55/1000 | Loss: 0.00001229
Iteration 56/1000 | Loss: 0.00001229
Iteration 57/1000 | Loss: 0.00001228
Iteration 58/1000 | Loss: 0.00001228
Iteration 59/1000 | Loss: 0.00001228
Iteration 60/1000 | Loss: 0.00001228
Iteration 61/1000 | Loss: 0.00001227
Iteration 62/1000 | Loss: 0.00001227
Iteration 63/1000 | Loss: 0.00001227
Iteration 64/1000 | Loss: 0.00001226
Iteration 65/1000 | Loss: 0.00001226
Iteration 66/1000 | Loss: 0.00001226
Iteration 67/1000 | Loss: 0.00001225
Iteration 68/1000 | Loss: 0.00001225
Iteration 69/1000 | Loss: 0.00001224
Iteration 70/1000 | Loss: 0.00001224
Iteration 71/1000 | Loss: 0.00001224
Iteration 72/1000 | Loss: 0.00001223
Iteration 73/1000 | Loss: 0.00001223
Iteration 74/1000 | Loss: 0.00001223
Iteration 75/1000 | Loss: 0.00001223
Iteration 76/1000 | Loss: 0.00001222
Iteration 77/1000 | Loss: 0.00001222
Iteration 78/1000 | Loss: 0.00001222
Iteration 79/1000 | Loss: 0.00001222
Iteration 80/1000 | Loss: 0.00001222
Iteration 81/1000 | Loss: 0.00001221
Iteration 82/1000 | Loss: 0.00001216
Iteration 83/1000 | Loss: 0.00001216
Iteration 84/1000 | Loss: 0.00001216
Iteration 85/1000 | Loss: 0.00001215
Iteration 86/1000 | Loss: 0.00001215
Iteration 87/1000 | Loss: 0.00001214
Iteration 88/1000 | Loss: 0.00001214
Iteration 89/1000 | Loss: 0.00001213
Iteration 90/1000 | Loss: 0.00001213
Iteration 91/1000 | Loss: 0.00001213
Iteration 92/1000 | Loss: 0.00001213
Iteration 93/1000 | Loss: 0.00001212
Iteration 94/1000 | Loss: 0.00001212
Iteration 95/1000 | Loss: 0.00001212
Iteration 96/1000 | Loss: 0.00001212
Iteration 97/1000 | Loss: 0.00001212
Iteration 98/1000 | Loss: 0.00001212
Iteration 99/1000 | Loss: 0.00001211
Iteration 100/1000 | Loss: 0.00001211
Iteration 101/1000 | Loss: 0.00001211
Iteration 102/1000 | Loss: 0.00001211
Iteration 103/1000 | Loss: 0.00001211
Iteration 104/1000 | Loss: 0.00001211
Iteration 105/1000 | Loss: 0.00001211
Iteration 106/1000 | Loss: 0.00001210
Iteration 107/1000 | Loss: 0.00001210
Iteration 108/1000 | Loss: 0.00001210
Iteration 109/1000 | Loss: 0.00001209
Iteration 110/1000 | Loss: 0.00001209
Iteration 111/1000 | Loss: 0.00001207
Iteration 112/1000 | Loss: 0.00001207
Iteration 113/1000 | Loss: 0.00001207
Iteration 114/1000 | Loss: 0.00001207
Iteration 115/1000 | Loss: 0.00001206
Iteration 116/1000 | Loss: 0.00001206
Iteration 117/1000 | Loss: 0.00001206
Iteration 118/1000 | Loss: 0.00001206
Iteration 119/1000 | Loss: 0.00001205
Iteration 120/1000 | Loss: 0.00001205
Iteration 121/1000 | Loss: 0.00001205
Iteration 122/1000 | Loss: 0.00001205
Iteration 123/1000 | Loss: 0.00001204
Iteration 124/1000 | Loss: 0.00001204
Iteration 125/1000 | Loss: 0.00001204
Iteration 126/1000 | Loss: 0.00001204
Iteration 127/1000 | Loss: 0.00001204
Iteration 128/1000 | Loss: 0.00001204
Iteration 129/1000 | Loss: 0.00001204
Iteration 130/1000 | Loss: 0.00001204
Iteration 131/1000 | Loss: 0.00001204
Iteration 132/1000 | Loss: 0.00001203
Iteration 133/1000 | Loss: 0.00001203
Iteration 134/1000 | Loss: 0.00001203
Iteration 135/1000 | Loss: 0.00001203
Iteration 136/1000 | Loss: 0.00001202
Iteration 137/1000 | Loss: 0.00001202
Iteration 138/1000 | Loss: 0.00001202
Iteration 139/1000 | Loss: 0.00001202
Iteration 140/1000 | Loss: 0.00001202
Iteration 141/1000 | Loss: 0.00001201
Iteration 142/1000 | Loss: 0.00001201
Iteration 143/1000 | Loss: 0.00001201
Iteration 144/1000 | Loss: 0.00001201
Iteration 145/1000 | Loss: 0.00001200
Iteration 146/1000 | Loss: 0.00001200
Iteration 147/1000 | Loss: 0.00001200
Iteration 148/1000 | Loss: 0.00001200
Iteration 149/1000 | Loss: 0.00001200
Iteration 150/1000 | Loss: 0.00001200
Iteration 151/1000 | Loss: 0.00001200
Iteration 152/1000 | Loss: 0.00001200
Iteration 153/1000 | Loss: 0.00001200
Iteration 154/1000 | Loss: 0.00001199
Iteration 155/1000 | Loss: 0.00001199
Iteration 156/1000 | Loss: 0.00001199
Iteration 157/1000 | Loss: 0.00001199
Iteration 158/1000 | Loss: 0.00001199
Iteration 159/1000 | Loss: 0.00001199
Iteration 160/1000 | Loss: 0.00001199
Iteration 161/1000 | Loss: 0.00001199
Iteration 162/1000 | Loss: 0.00001199
Iteration 163/1000 | Loss: 0.00001198
Iteration 164/1000 | Loss: 0.00001198
Iteration 165/1000 | Loss: 0.00001198
Iteration 166/1000 | Loss: 0.00001198
Iteration 167/1000 | Loss: 0.00001198
Iteration 168/1000 | Loss: 0.00001198
Iteration 169/1000 | Loss: 0.00001198
Iteration 170/1000 | Loss: 0.00001197
Iteration 171/1000 | Loss: 0.00001197
Iteration 172/1000 | Loss: 0.00001197
Iteration 173/1000 | Loss: 0.00001197
Iteration 174/1000 | Loss: 0.00001197
Iteration 175/1000 | Loss: 0.00001197
Iteration 176/1000 | Loss: 0.00001197
Iteration 177/1000 | Loss: 0.00001197
Iteration 178/1000 | Loss: 0.00001197
Iteration 179/1000 | Loss: 0.00001197
Iteration 180/1000 | Loss: 0.00001196
Iteration 181/1000 | Loss: 0.00001196
Iteration 182/1000 | Loss: 0.00001196
Iteration 183/1000 | Loss: 0.00001196
Iteration 184/1000 | Loss: 0.00001196
Iteration 185/1000 | Loss: 0.00001195
Iteration 186/1000 | Loss: 0.00001195
Iteration 187/1000 | Loss: 0.00001195
Iteration 188/1000 | Loss: 0.00001195
Iteration 189/1000 | Loss: 0.00001195
Iteration 190/1000 | Loss: 0.00001195
Iteration 191/1000 | Loss: 0.00001195
Iteration 192/1000 | Loss: 0.00001195
Iteration 193/1000 | Loss: 0.00001195
Iteration 194/1000 | Loss: 0.00001194
Iteration 195/1000 | Loss: 0.00001194
Iteration 196/1000 | Loss: 0.00001194
Iteration 197/1000 | Loss: 0.00001194
Iteration 198/1000 | Loss: 0.00001194
Iteration 199/1000 | Loss: 0.00001194
Iteration 200/1000 | Loss: 0.00001194
Iteration 201/1000 | Loss: 0.00001194
Iteration 202/1000 | Loss: 0.00001194
Iteration 203/1000 | Loss: 0.00001194
Iteration 204/1000 | Loss: 0.00001194
Iteration 205/1000 | Loss: 0.00001194
Iteration 206/1000 | Loss: 0.00001193
Iteration 207/1000 | Loss: 0.00001193
Iteration 208/1000 | Loss: 0.00001193
Iteration 209/1000 | Loss: 0.00001193
Iteration 210/1000 | Loss: 0.00001193
Iteration 211/1000 | Loss: 0.00001193
Iteration 212/1000 | Loss: 0.00001193
Iteration 213/1000 | Loss: 0.00001193
Iteration 214/1000 | Loss: 0.00001193
Iteration 215/1000 | Loss: 0.00001193
Iteration 216/1000 | Loss: 0.00001193
Iteration 217/1000 | Loss: 0.00001193
Iteration 218/1000 | Loss: 0.00001192
Iteration 219/1000 | Loss: 0.00001192
Iteration 220/1000 | Loss: 0.00001192
Iteration 221/1000 | Loss: 0.00001192
Iteration 222/1000 | Loss: 0.00001192
Iteration 223/1000 | Loss: 0.00001192
Iteration 224/1000 | Loss: 0.00001192
Iteration 225/1000 | Loss: 0.00001192
Iteration 226/1000 | Loss: 0.00001192
Iteration 227/1000 | Loss: 0.00001192
Iteration 228/1000 | Loss: 0.00001192
Iteration 229/1000 | Loss: 0.00001192
Iteration 230/1000 | Loss: 0.00001192
Iteration 231/1000 | Loss: 0.00001192
Iteration 232/1000 | Loss: 0.00001192
Iteration 233/1000 | Loss: 0.00001192
Iteration 234/1000 | Loss: 0.00001192
Iteration 235/1000 | Loss: 0.00001192
Iteration 236/1000 | Loss: 0.00001192
Iteration 237/1000 | Loss: 0.00001192
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 237. Stopping optimization.
Last 5 losses: [1.1918181371584069e-05, 1.1918181371584069e-05, 1.1918181371584069e-05, 1.1918181371584069e-05, 1.1918181371584069e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1918181371584069e-05

Optimization complete. Final v2v error: 2.940361738204956 mm

Highest mean error: 3.8680202960968018 mm for frame 70

Lowest mean error: 2.7996366024017334 mm for frame 106

Saving results

Total time: 42.89914560317993
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00746883
Iteration 2/25 | Loss: 0.00165249
Iteration 3/25 | Loss: 0.00145345
Iteration 4/25 | Loss: 0.00134477
Iteration 5/25 | Loss: 0.00133999
Iteration 6/25 | Loss: 0.00132618
Iteration 7/25 | Loss: 0.00131075
Iteration 8/25 | Loss: 0.00130286
Iteration 9/25 | Loss: 0.00129393
Iteration 10/25 | Loss: 0.00129399
Iteration 11/25 | Loss: 0.00129007
Iteration 12/25 | Loss: 0.00128979
Iteration 13/25 | Loss: 0.00128966
Iteration 14/25 | Loss: 0.00128964
Iteration 15/25 | Loss: 0.00128964
Iteration 16/25 | Loss: 0.00128964
Iteration 17/25 | Loss: 0.00128964
Iteration 18/25 | Loss: 0.00128964
Iteration 19/25 | Loss: 0.00128963
Iteration 20/25 | Loss: 0.00128963
Iteration 21/25 | Loss: 0.00128963
Iteration 22/25 | Loss: 0.00128963
Iteration 23/25 | Loss: 0.00128963
Iteration 24/25 | Loss: 0.00128963
Iteration 25/25 | Loss: 0.00128963

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.75553465
Iteration 2/25 | Loss: 0.00089751
Iteration 3/25 | Loss: 0.00089751
Iteration 4/25 | Loss: 0.00089751
Iteration 5/25 | Loss: 0.00089751
Iteration 6/25 | Loss: 0.00089751
Iteration 7/25 | Loss: 0.00089751
Iteration 8/25 | Loss: 0.00089751
Iteration 9/25 | Loss: 0.00089751
Iteration 10/25 | Loss: 0.00089751
Iteration 11/25 | Loss: 0.00089751
Iteration 12/25 | Loss: 0.00089751
Iteration 13/25 | Loss: 0.00089751
Iteration 14/25 | Loss: 0.00089751
Iteration 15/25 | Loss: 0.00089751
Iteration 16/25 | Loss: 0.00089751
Iteration 17/25 | Loss: 0.00089751
Iteration 18/25 | Loss: 0.00089751
Iteration 19/25 | Loss: 0.00089751
Iteration 20/25 | Loss: 0.00089751
Iteration 21/25 | Loss: 0.00089751
Iteration 22/25 | Loss: 0.00089751
Iteration 23/25 | Loss: 0.00089751
Iteration 24/25 | Loss: 0.00089751
Iteration 25/25 | Loss: 0.00089751

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089751
Iteration 2/1000 | Loss: 0.00003008
Iteration 3/1000 | Loss: 0.00002327
Iteration 4/1000 | Loss: 0.00005695
Iteration 5/1000 | Loss: 0.00002092
Iteration 6/1000 | Loss: 0.00001993
Iteration 7/1000 | Loss: 0.00001927
Iteration 8/1000 | Loss: 0.00001880
Iteration 9/1000 | Loss: 0.00001843
Iteration 10/1000 | Loss: 0.00027372
Iteration 11/1000 | Loss: 0.00002016
Iteration 12/1000 | Loss: 0.00001806
Iteration 13/1000 | Loss: 0.00001729
Iteration 14/1000 | Loss: 0.00001678
Iteration 15/1000 | Loss: 0.00001641
Iteration 16/1000 | Loss: 0.00001625
Iteration 17/1000 | Loss: 0.00001625
Iteration 18/1000 | Loss: 0.00001621
Iteration 19/1000 | Loss: 0.00001618
Iteration 20/1000 | Loss: 0.00001615
Iteration 21/1000 | Loss: 0.00001613
Iteration 22/1000 | Loss: 0.00001613
Iteration 23/1000 | Loss: 0.00001609
Iteration 24/1000 | Loss: 0.00001598
Iteration 25/1000 | Loss: 0.00001582
Iteration 26/1000 | Loss: 0.00001578
Iteration 27/1000 | Loss: 0.00001578
Iteration 28/1000 | Loss: 0.00001578
Iteration 29/1000 | Loss: 0.00001576
Iteration 30/1000 | Loss: 0.00001575
Iteration 31/1000 | Loss: 0.00001575
Iteration 32/1000 | Loss: 0.00001574
Iteration 33/1000 | Loss: 0.00001573
Iteration 34/1000 | Loss: 0.00001572
Iteration 35/1000 | Loss: 0.00001571
Iteration 36/1000 | Loss: 0.00001571
Iteration 37/1000 | Loss: 0.00001570
Iteration 38/1000 | Loss: 0.00001570
Iteration 39/1000 | Loss: 0.00001570
Iteration 40/1000 | Loss: 0.00001569
Iteration 41/1000 | Loss: 0.00001569
Iteration 42/1000 | Loss: 0.00001569
Iteration 43/1000 | Loss: 0.00001569
Iteration 44/1000 | Loss: 0.00001567
Iteration 45/1000 | Loss: 0.00001567
Iteration 46/1000 | Loss: 0.00001565
Iteration 47/1000 | Loss: 0.00001565
Iteration 48/1000 | Loss: 0.00001564
Iteration 49/1000 | Loss: 0.00001564
Iteration 50/1000 | Loss: 0.00001560
Iteration 51/1000 | Loss: 0.00001559
Iteration 52/1000 | Loss: 0.00001559
Iteration 53/1000 | Loss: 0.00001559
Iteration 54/1000 | Loss: 0.00001558
Iteration 55/1000 | Loss: 0.00001558
Iteration 56/1000 | Loss: 0.00001558
Iteration 57/1000 | Loss: 0.00001558
Iteration 58/1000 | Loss: 0.00001558
Iteration 59/1000 | Loss: 0.00001558
Iteration 60/1000 | Loss: 0.00001557
Iteration 61/1000 | Loss: 0.00001557
Iteration 62/1000 | Loss: 0.00001557
Iteration 63/1000 | Loss: 0.00001557
Iteration 64/1000 | Loss: 0.00001556
Iteration 65/1000 | Loss: 0.00001556
Iteration 66/1000 | Loss: 0.00001556
Iteration 67/1000 | Loss: 0.00001555
Iteration 68/1000 | Loss: 0.00001555
Iteration 69/1000 | Loss: 0.00001555
Iteration 70/1000 | Loss: 0.00001555
Iteration 71/1000 | Loss: 0.00001555
Iteration 72/1000 | Loss: 0.00001554
Iteration 73/1000 | Loss: 0.00001554
Iteration 74/1000 | Loss: 0.00001554
Iteration 75/1000 | Loss: 0.00001554
Iteration 76/1000 | Loss: 0.00001554
Iteration 77/1000 | Loss: 0.00001554
Iteration 78/1000 | Loss: 0.00001554
Iteration 79/1000 | Loss: 0.00001554
Iteration 80/1000 | Loss: 0.00001553
Iteration 81/1000 | Loss: 0.00001553
Iteration 82/1000 | Loss: 0.00001553
Iteration 83/1000 | Loss: 0.00001553
Iteration 84/1000 | Loss: 0.00001553
Iteration 85/1000 | Loss: 0.00001553
Iteration 86/1000 | Loss: 0.00001553
Iteration 87/1000 | Loss: 0.00001553
Iteration 88/1000 | Loss: 0.00001553
Iteration 89/1000 | Loss: 0.00001553
Iteration 90/1000 | Loss: 0.00001553
Iteration 91/1000 | Loss: 0.00001552
Iteration 92/1000 | Loss: 0.00001552
Iteration 93/1000 | Loss: 0.00001552
Iteration 94/1000 | Loss: 0.00001552
Iteration 95/1000 | Loss: 0.00001552
Iteration 96/1000 | Loss: 0.00001552
Iteration 97/1000 | Loss: 0.00001552
Iteration 98/1000 | Loss: 0.00001551
Iteration 99/1000 | Loss: 0.00001551
Iteration 100/1000 | Loss: 0.00001551
Iteration 101/1000 | Loss: 0.00001551
Iteration 102/1000 | Loss: 0.00001551
Iteration 103/1000 | Loss: 0.00001551
Iteration 104/1000 | Loss: 0.00001551
Iteration 105/1000 | Loss: 0.00001551
Iteration 106/1000 | Loss: 0.00001551
Iteration 107/1000 | Loss: 0.00001551
Iteration 108/1000 | Loss: 0.00001550
Iteration 109/1000 | Loss: 0.00001550
Iteration 110/1000 | Loss: 0.00001550
Iteration 111/1000 | Loss: 0.00001550
Iteration 112/1000 | Loss: 0.00001550
Iteration 113/1000 | Loss: 0.00001550
Iteration 114/1000 | Loss: 0.00001550
Iteration 115/1000 | Loss: 0.00001550
Iteration 116/1000 | Loss: 0.00001550
Iteration 117/1000 | Loss: 0.00001550
Iteration 118/1000 | Loss: 0.00001549
Iteration 119/1000 | Loss: 0.00001549
Iteration 120/1000 | Loss: 0.00001549
Iteration 121/1000 | Loss: 0.00001549
Iteration 122/1000 | Loss: 0.00001549
Iteration 123/1000 | Loss: 0.00001549
Iteration 124/1000 | Loss: 0.00001549
Iteration 125/1000 | Loss: 0.00001549
Iteration 126/1000 | Loss: 0.00001549
Iteration 127/1000 | Loss: 0.00001549
Iteration 128/1000 | Loss: 0.00001549
Iteration 129/1000 | Loss: 0.00001549
Iteration 130/1000 | Loss: 0.00001549
Iteration 131/1000 | Loss: 0.00001549
Iteration 132/1000 | Loss: 0.00001549
Iteration 133/1000 | Loss: 0.00001549
Iteration 134/1000 | Loss: 0.00001549
Iteration 135/1000 | Loss: 0.00001548
Iteration 136/1000 | Loss: 0.00001548
Iteration 137/1000 | Loss: 0.00001548
Iteration 138/1000 | Loss: 0.00001548
Iteration 139/1000 | Loss: 0.00001548
Iteration 140/1000 | Loss: 0.00001548
Iteration 141/1000 | Loss: 0.00001548
Iteration 142/1000 | Loss: 0.00001548
Iteration 143/1000 | Loss: 0.00001548
Iteration 144/1000 | Loss: 0.00001548
Iteration 145/1000 | Loss: 0.00001548
Iteration 146/1000 | Loss: 0.00001548
Iteration 147/1000 | Loss: 0.00001548
Iteration 148/1000 | Loss: 0.00001548
Iteration 149/1000 | Loss: 0.00001547
Iteration 150/1000 | Loss: 0.00001547
Iteration 151/1000 | Loss: 0.00001547
Iteration 152/1000 | Loss: 0.00001547
Iteration 153/1000 | Loss: 0.00001547
Iteration 154/1000 | Loss: 0.00001547
Iteration 155/1000 | Loss: 0.00001547
Iteration 156/1000 | Loss: 0.00001547
Iteration 157/1000 | Loss: 0.00001547
Iteration 158/1000 | Loss: 0.00001547
Iteration 159/1000 | Loss: 0.00001547
Iteration 160/1000 | Loss: 0.00001547
Iteration 161/1000 | Loss: 0.00001547
Iteration 162/1000 | Loss: 0.00001547
Iteration 163/1000 | Loss: 0.00001547
Iteration 164/1000 | Loss: 0.00001547
Iteration 165/1000 | Loss: 0.00001547
Iteration 166/1000 | Loss: 0.00001547
Iteration 167/1000 | Loss: 0.00001547
Iteration 168/1000 | Loss: 0.00001547
Iteration 169/1000 | Loss: 0.00001547
Iteration 170/1000 | Loss: 0.00001547
Iteration 171/1000 | Loss: 0.00001547
Iteration 172/1000 | Loss: 0.00001547
Iteration 173/1000 | Loss: 0.00001546
Iteration 174/1000 | Loss: 0.00001546
Iteration 175/1000 | Loss: 0.00001546
Iteration 176/1000 | Loss: 0.00001546
Iteration 177/1000 | Loss: 0.00001546
Iteration 178/1000 | Loss: 0.00001546
Iteration 179/1000 | Loss: 0.00001546
Iteration 180/1000 | Loss: 0.00001546
Iteration 181/1000 | Loss: 0.00001546
Iteration 182/1000 | Loss: 0.00001546
Iteration 183/1000 | Loss: 0.00001546
Iteration 184/1000 | Loss: 0.00001546
Iteration 185/1000 | Loss: 0.00001546
Iteration 186/1000 | Loss: 0.00001545
Iteration 187/1000 | Loss: 0.00001545
Iteration 188/1000 | Loss: 0.00001545
Iteration 189/1000 | Loss: 0.00001545
Iteration 190/1000 | Loss: 0.00001545
Iteration 191/1000 | Loss: 0.00001545
Iteration 192/1000 | Loss: 0.00001545
Iteration 193/1000 | Loss: 0.00001545
Iteration 194/1000 | Loss: 0.00001545
Iteration 195/1000 | Loss: 0.00001545
Iteration 196/1000 | Loss: 0.00001545
Iteration 197/1000 | Loss: 0.00001545
Iteration 198/1000 | Loss: 0.00001545
Iteration 199/1000 | Loss: 0.00001545
Iteration 200/1000 | Loss: 0.00001545
Iteration 201/1000 | Loss: 0.00001545
Iteration 202/1000 | Loss: 0.00001545
Iteration 203/1000 | Loss: 0.00001545
Iteration 204/1000 | Loss: 0.00001545
Iteration 205/1000 | Loss: 0.00001545
Iteration 206/1000 | Loss: 0.00001545
Iteration 207/1000 | Loss: 0.00001545
Iteration 208/1000 | Loss: 0.00001545
Iteration 209/1000 | Loss: 0.00001545
Iteration 210/1000 | Loss: 0.00001544
Iteration 211/1000 | Loss: 0.00001544
Iteration 212/1000 | Loss: 0.00001544
Iteration 213/1000 | Loss: 0.00001544
Iteration 214/1000 | Loss: 0.00001544
Iteration 215/1000 | Loss: 0.00001544
Iteration 216/1000 | Loss: 0.00001544
Iteration 217/1000 | Loss: 0.00001544
Iteration 218/1000 | Loss: 0.00001544
Iteration 219/1000 | Loss: 0.00001544
Iteration 220/1000 | Loss: 0.00001544
Iteration 221/1000 | Loss: 0.00001544
Iteration 222/1000 | Loss: 0.00001544
Iteration 223/1000 | Loss: 0.00001544
Iteration 224/1000 | Loss: 0.00001544
Iteration 225/1000 | Loss: 0.00001544
Iteration 226/1000 | Loss: 0.00001544
Iteration 227/1000 | Loss: 0.00001544
Iteration 228/1000 | Loss: 0.00001544
Iteration 229/1000 | Loss: 0.00001544
Iteration 230/1000 | Loss: 0.00001544
Iteration 231/1000 | Loss: 0.00001544
Iteration 232/1000 | Loss: 0.00001544
Iteration 233/1000 | Loss: 0.00001544
Iteration 234/1000 | Loss: 0.00001544
Iteration 235/1000 | Loss: 0.00001544
Iteration 236/1000 | Loss: 0.00001544
Iteration 237/1000 | Loss: 0.00001544
Iteration 238/1000 | Loss: 0.00001544
Iteration 239/1000 | Loss: 0.00001544
Iteration 240/1000 | Loss: 0.00001544
Iteration 241/1000 | Loss: 0.00001544
Iteration 242/1000 | Loss: 0.00001544
Iteration 243/1000 | Loss: 0.00001544
Iteration 244/1000 | Loss: 0.00001544
Iteration 245/1000 | Loss: 0.00001544
Iteration 246/1000 | Loss: 0.00001544
Iteration 247/1000 | Loss: 0.00001544
Iteration 248/1000 | Loss: 0.00001544
Iteration 249/1000 | Loss: 0.00001544
Iteration 250/1000 | Loss: 0.00001544
Iteration 251/1000 | Loss: 0.00001544
Iteration 252/1000 | Loss: 0.00001544
Iteration 253/1000 | Loss: 0.00001544
Iteration 254/1000 | Loss: 0.00001544
Iteration 255/1000 | Loss: 0.00001544
Iteration 256/1000 | Loss: 0.00001544
Iteration 257/1000 | Loss: 0.00001544
Iteration 258/1000 | Loss: 0.00001544
Iteration 259/1000 | Loss: 0.00001544
Iteration 260/1000 | Loss: 0.00001544
Iteration 261/1000 | Loss: 0.00001544
Iteration 262/1000 | Loss: 0.00001544
Iteration 263/1000 | Loss: 0.00001544
Iteration 264/1000 | Loss: 0.00001544
Iteration 265/1000 | Loss: 0.00001544
Iteration 266/1000 | Loss: 0.00001544
Iteration 267/1000 | Loss: 0.00001544
Iteration 268/1000 | Loss: 0.00001544
Iteration 269/1000 | Loss: 0.00001544
Iteration 270/1000 | Loss: 0.00001544
Iteration 271/1000 | Loss: 0.00001544
Iteration 272/1000 | Loss: 0.00001544
Iteration 273/1000 | Loss: 0.00001544
Iteration 274/1000 | Loss: 0.00001544
Iteration 275/1000 | Loss: 0.00001544
Iteration 276/1000 | Loss: 0.00001544
Iteration 277/1000 | Loss: 0.00001544
Iteration 278/1000 | Loss: 0.00001544
Iteration 279/1000 | Loss: 0.00001544
Iteration 280/1000 | Loss: 0.00001544
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 280. Stopping optimization.
Last 5 losses: [1.5440784409292974e-05, 1.5440784409292974e-05, 1.5440784409292974e-05, 1.5440784409292974e-05, 1.5440784409292974e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5440784409292974e-05

Optimization complete. Final v2v error: 3.3400588035583496 mm

Highest mean error: 3.895009994506836 mm for frame 195

Lowest mean error: 2.9529335498809814 mm for frame 199

Saving results

Total time: 75.62456130981445
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00712385
Iteration 2/25 | Loss: 0.00144566
Iteration 3/25 | Loss: 0.00133763
Iteration 4/25 | Loss: 0.00132392
Iteration 5/25 | Loss: 0.00131906
Iteration 6/25 | Loss: 0.00131846
Iteration 7/25 | Loss: 0.00131846
Iteration 8/25 | Loss: 0.00131846
Iteration 9/25 | Loss: 0.00131846
Iteration 10/25 | Loss: 0.00131846
Iteration 11/25 | Loss: 0.00131846
Iteration 12/25 | Loss: 0.00131846
Iteration 13/25 | Loss: 0.00131846
Iteration 14/25 | Loss: 0.00131846
Iteration 15/25 | Loss: 0.00131846
Iteration 16/25 | Loss: 0.00131846
Iteration 17/25 | Loss: 0.00131846
Iteration 18/25 | Loss: 0.00131846
Iteration 19/25 | Loss: 0.00131846
Iteration 20/25 | Loss: 0.00131846
Iteration 21/25 | Loss: 0.00131846
Iteration 22/25 | Loss: 0.00131846
Iteration 23/25 | Loss: 0.00131846
Iteration 24/25 | Loss: 0.00131846
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0013184644049033523, 0.0013184644049033523, 0.0013184644049033523, 0.0013184644049033523, 0.0013184644049033523]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013184644049033523

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.14669490
Iteration 2/25 | Loss: 0.00099382
Iteration 3/25 | Loss: 0.00099382
Iteration 4/25 | Loss: 0.00099382
Iteration 5/25 | Loss: 0.00099381
Iteration 6/25 | Loss: 0.00099381
Iteration 7/25 | Loss: 0.00099381
Iteration 8/25 | Loss: 0.00099381
Iteration 9/25 | Loss: 0.00099381
Iteration 10/25 | Loss: 0.00099381
Iteration 11/25 | Loss: 0.00099381
Iteration 12/25 | Loss: 0.00099381
Iteration 13/25 | Loss: 0.00099381
Iteration 14/25 | Loss: 0.00099381
Iteration 15/25 | Loss: 0.00099381
Iteration 16/25 | Loss: 0.00099381
Iteration 17/25 | Loss: 0.00099381
Iteration 18/25 | Loss: 0.00099381
Iteration 19/25 | Loss: 0.00099381
Iteration 20/25 | Loss: 0.00099381
Iteration 21/25 | Loss: 0.00099381
Iteration 22/25 | Loss: 0.00099381
Iteration 23/25 | Loss: 0.00099381
Iteration 24/25 | Loss: 0.00099381
Iteration 25/25 | Loss: 0.00099381

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099381
Iteration 2/1000 | Loss: 0.00004060
Iteration 3/1000 | Loss: 0.00002753
Iteration 4/1000 | Loss: 0.00002367
Iteration 5/1000 | Loss: 0.00002232
Iteration 6/1000 | Loss: 0.00002133
Iteration 7/1000 | Loss: 0.00002065
Iteration 8/1000 | Loss: 0.00002007
Iteration 9/1000 | Loss: 0.00001975
Iteration 10/1000 | Loss: 0.00001950
Iteration 11/1000 | Loss: 0.00001927
Iteration 12/1000 | Loss: 0.00001917
Iteration 13/1000 | Loss: 0.00001908
Iteration 14/1000 | Loss: 0.00001904
Iteration 15/1000 | Loss: 0.00001901
Iteration 16/1000 | Loss: 0.00001896
Iteration 17/1000 | Loss: 0.00001879
Iteration 18/1000 | Loss: 0.00001876
Iteration 19/1000 | Loss: 0.00001871
Iteration 20/1000 | Loss: 0.00001863
Iteration 21/1000 | Loss: 0.00001863
Iteration 22/1000 | Loss: 0.00001862
Iteration 23/1000 | Loss: 0.00001856
Iteration 24/1000 | Loss: 0.00001855
Iteration 25/1000 | Loss: 0.00001853
Iteration 26/1000 | Loss: 0.00001852
Iteration 27/1000 | Loss: 0.00001852
Iteration 28/1000 | Loss: 0.00001851
Iteration 29/1000 | Loss: 0.00001847
Iteration 30/1000 | Loss: 0.00001847
Iteration 31/1000 | Loss: 0.00001845
Iteration 32/1000 | Loss: 0.00001844
Iteration 33/1000 | Loss: 0.00001843
Iteration 34/1000 | Loss: 0.00001843
Iteration 35/1000 | Loss: 0.00001842
Iteration 36/1000 | Loss: 0.00001842
Iteration 37/1000 | Loss: 0.00001841
Iteration 38/1000 | Loss: 0.00001840
Iteration 39/1000 | Loss: 0.00001840
Iteration 40/1000 | Loss: 0.00001840
Iteration 41/1000 | Loss: 0.00001839
Iteration 42/1000 | Loss: 0.00001839
Iteration 43/1000 | Loss: 0.00001839
Iteration 44/1000 | Loss: 0.00001839
Iteration 45/1000 | Loss: 0.00001839
Iteration 46/1000 | Loss: 0.00001839
Iteration 47/1000 | Loss: 0.00001837
Iteration 48/1000 | Loss: 0.00001836
Iteration 49/1000 | Loss: 0.00001835
Iteration 50/1000 | Loss: 0.00001835
Iteration 51/1000 | Loss: 0.00001834
Iteration 52/1000 | Loss: 0.00001834
Iteration 53/1000 | Loss: 0.00001833
Iteration 54/1000 | Loss: 0.00001832
Iteration 55/1000 | Loss: 0.00001831
Iteration 56/1000 | Loss: 0.00001831
Iteration 57/1000 | Loss: 0.00001831
Iteration 58/1000 | Loss: 0.00001830
Iteration 59/1000 | Loss: 0.00001830
Iteration 60/1000 | Loss: 0.00001830
Iteration 61/1000 | Loss: 0.00001829
Iteration 62/1000 | Loss: 0.00001829
Iteration 63/1000 | Loss: 0.00001829
Iteration 64/1000 | Loss: 0.00001828
Iteration 65/1000 | Loss: 0.00001828
Iteration 66/1000 | Loss: 0.00001828
Iteration 67/1000 | Loss: 0.00001828
Iteration 68/1000 | Loss: 0.00001827
Iteration 69/1000 | Loss: 0.00001827
Iteration 70/1000 | Loss: 0.00001826
Iteration 71/1000 | Loss: 0.00001826
Iteration 72/1000 | Loss: 0.00001825
Iteration 73/1000 | Loss: 0.00001825
Iteration 74/1000 | Loss: 0.00001825
Iteration 75/1000 | Loss: 0.00001825
Iteration 76/1000 | Loss: 0.00001825
Iteration 77/1000 | Loss: 0.00001825
Iteration 78/1000 | Loss: 0.00001825
Iteration 79/1000 | Loss: 0.00001824
Iteration 80/1000 | Loss: 0.00001824
Iteration 81/1000 | Loss: 0.00001824
Iteration 82/1000 | Loss: 0.00001823
Iteration 83/1000 | Loss: 0.00001823
Iteration 84/1000 | Loss: 0.00001823
Iteration 85/1000 | Loss: 0.00001822
Iteration 86/1000 | Loss: 0.00001822
Iteration 87/1000 | Loss: 0.00001822
Iteration 88/1000 | Loss: 0.00001821
Iteration 89/1000 | Loss: 0.00001821
Iteration 90/1000 | Loss: 0.00001821
Iteration 91/1000 | Loss: 0.00001821
Iteration 92/1000 | Loss: 0.00001820
Iteration 93/1000 | Loss: 0.00001820
Iteration 94/1000 | Loss: 0.00001820
Iteration 95/1000 | Loss: 0.00001819
Iteration 96/1000 | Loss: 0.00001819
Iteration 97/1000 | Loss: 0.00001819
Iteration 98/1000 | Loss: 0.00001819
Iteration 99/1000 | Loss: 0.00001819
Iteration 100/1000 | Loss: 0.00001819
Iteration 101/1000 | Loss: 0.00001819
Iteration 102/1000 | Loss: 0.00001819
Iteration 103/1000 | Loss: 0.00001818
Iteration 104/1000 | Loss: 0.00001818
Iteration 105/1000 | Loss: 0.00001818
Iteration 106/1000 | Loss: 0.00001818
Iteration 107/1000 | Loss: 0.00001818
Iteration 108/1000 | Loss: 0.00001817
Iteration 109/1000 | Loss: 0.00001817
Iteration 110/1000 | Loss: 0.00001817
Iteration 111/1000 | Loss: 0.00001817
Iteration 112/1000 | Loss: 0.00001817
Iteration 113/1000 | Loss: 0.00001817
Iteration 114/1000 | Loss: 0.00001817
Iteration 115/1000 | Loss: 0.00001817
Iteration 116/1000 | Loss: 0.00001817
Iteration 117/1000 | Loss: 0.00001816
Iteration 118/1000 | Loss: 0.00001816
Iteration 119/1000 | Loss: 0.00001816
Iteration 120/1000 | Loss: 0.00001816
Iteration 121/1000 | Loss: 0.00001816
Iteration 122/1000 | Loss: 0.00001815
Iteration 123/1000 | Loss: 0.00001815
Iteration 124/1000 | Loss: 0.00001815
Iteration 125/1000 | Loss: 0.00001815
Iteration 126/1000 | Loss: 0.00001814
Iteration 127/1000 | Loss: 0.00001814
Iteration 128/1000 | Loss: 0.00001814
Iteration 129/1000 | Loss: 0.00001814
Iteration 130/1000 | Loss: 0.00001814
Iteration 131/1000 | Loss: 0.00001814
Iteration 132/1000 | Loss: 0.00001814
Iteration 133/1000 | Loss: 0.00001814
Iteration 134/1000 | Loss: 0.00001814
Iteration 135/1000 | Loss: 0.00001814
Iteration 136/1000 | Loss: 0.00001814
Iteration 137/1000 | Loss: 0.00001814
Iteration 138/1000 | Loss: 0.00001813
Iteration 139/1000 | Loss: 0.00001813
Iteration 140/1000 | Loss: 0.00001813
Iteration 141/1000 | Loss: 0.00001813
Iteration 142/1000 | Loss: 0.00001813
Iteration 143/1000 | Loss: 0.00001813
Iteration 144/1000 | Loss: 0.00001813
Iteration 145/1000 | Loss: 0.00001813
Iteration 146/1000 | Loss: 0.00001813
Iteration 147/1000 | Loss: 0.00001813
Iteration 148/1000 | Loss: 0.00001813
Iteration 149/1000 | Loss: 0.00001813
Iteration 150/1000 | Loss: 0.00001813
Iteration 151/1000 | Loss: 0.00001813
Iteration 152/1000 | Loss: 0.00001812
Iteration 153/1000 | Loss: 0.00001812
Iteration 154/1000 | Loss: 0.00001812
Iteration 155/1000 | Loss: 0.00001812
Iteration 156/1000 | Loss: 0.00001812
Iteration 157/1000 | Loss: 0.00001812
Iteration 158/1000 | Loss: 0.00001812
Iteration 159/1000 | Loss: 0.00001812
Iteration 160/1000 | Loss: 0.00001812
Iteration 161/1000 | Loss: 0.00001812
Iteration 162/1000 | Loss: 0.00001812
Iteration 163/1000 | Loss: 0.00001812
Iteration 164/1000 | Loss: 0.00001811
Iteration 165/1000 | Loss: 0.00001811
Iteration 166/1000 | Loss: 0.00001811
Iteration 167/1000 | Loss: 0.00001811
Iteration 168/1000 | Loss: 0.00001811
Iteration 169/1000 | Loss: 0.00001811
Iteration 170/1000 | Loss: 0.00001811
Iteration 171/1000 | Loss: 0.00001811
Iteration 172/1000 | Loss: 0.00001811
Iteration 173/1000 | Loss: 0.00001811
Iteration 174/1000 | Loss: 0.00001811
Iteration 175/1000 | Loss: 0.00001811
Iteration 176/1000 | Loss: 0.00001811
Iteration 177/1000 | Loss: 0.00001811
Iteration 178/1000 | Loss: 0.00001811
Iteration 179/1000 | Loss: 0.00001810
Iteration 180/1000 | Loss: 0.00001810
Iteration 181/1000 | Loss: 0.00001810
Iteration 182/1000 | Loss: 0.00001810
Iteration 183/1000 | Loss: 0.00001810
Iteration 184/1000 | Loss: 0.00001810
Iteration 185/1000 | Loss: 0.00001810
Iteration 186/1000 | Loss: 0.00001810
Iteration 187/1000 | Loss: 0.00001809
Iteration 188/1000 | Loss: 0.00001809
Iteration 189/1000 | Loss: 0.00001809
Iteration 190/1000 | Loss: 0.00001809
Iteration 191/1000 | Loss: 0.00001809
Iteration 192/1000 | Loss: 0.00001809
Iteration 193/1000 | Loss: 0.00001809
Iteration 194/1000 | Loss: 0.00001809
Iteration 195/1000 | Loss: 0.00001809
Iteration 196/1000 | Loss: 0.00001809
Iteration 197/1000 | Loss: 0.00001809
Iteration 198/1000 | Loss: 0.00001809
Iteration 199/1000 | Loss: 0.00001809
Iteration 200/1000 | Loss: 0.00001809
Iteration 201/1000 | Loss: 0.00001809
Iteration 202/1000 | Loss: 0.00001809
Iteration 203/1000 | Loss: 0.00001809
Iteration 204/1000 | Loss: 0.00001809
Iteration 205/1000 | Loss: 0.00001808
Iteration 206/1000 | Loss: 0.00001808
Iteration 207/1000 | Loss: 0.00001808
Iteration 208/1000 | Loss: 0.00001808
Iteration 209/1000 | Loss: 0.00001808
Iteration 210/1000 | Loss: 0.00001808
Iteration 211/1000 | Loss: 0.00001808
Iteration 212/1000 | Loss: 0.00001808
Iteration 213/1000 | Loss: 0.00001808
Iteration 214/1000 | Loss: 0.00001808
Iteration 215/1000 | Loss: 0.00001807
Iteration 216/1000 | Loss: 0.00001807
Iteration 217/1000 | Loss: 0.00001807
Iteration 218/1000 | Loss: 0.00001807
Iteration 219/1000 | Loss: 0.00001807
Iteration 220/1000 | Loss: 0.00001807
Iteration 221/1000 | Loss: 0.00001807
Iteration 222/1000 | Loss: 0.00001807
Iteration 223/1000 | Loss: 0.00001807
Iteration 224/1000 | Loss: 0.00001807
Iteration 225/1000 | Loss: 0.00001807
Iteration 226/1000 | Loss: 0.00001807
Iteration 227/1000 | Loss: 0.00001807
Iteration 228/1000 | Loss: 0.00001807
Iteration 229/1000 | Loss: 0.00001807
Iteration 230/1000 | Loss: 0.00001807
Iteration 231/1000 | Loss: 0.00001807
Iteration 232/1000 | Loss: 0.00001807
Iteration 233/1000 | Loss: 0.00001807
Iteration 234/1000 | Loss: 0.00001806
Iteration 235/1000 | Loss: 0.00001806
Iteration 236/1000 | Loss: 0.00001806
Iteration 237/1000 | Loss: 0.00001806
Iteration 238/1000 | Loss: 0.00001806
Iteration 239/1000 | Loss: 0.00001806
Iteration 240/1000 | Loss: 0.00001806
Iteration 241/1000 | Loss: 0.00001806
Iteration 242/1000 | Loss: 0.00001806
Iteration 243/1000 | Loss: 0.00001806
Iteration 244/1000 | Loss: 0.00001806
Iteration 245/1000 | Loss: 0.00001806
Iteration 246/1000 | Loss: 0.00001806
Iteration 247/1000 | Loss: 0.00001806
Iteration 248/1000 | Loss: 0.00001806
Iteration 249/1000 | Loss: 0.00001806
Iteration 250/1000 | Loss: 0.00001806
Iteration 251/1000 | Loss: 0.00001806
Iteration 252/1000 | Loss: 0.00001806
Iteration 253/1000 | Loss: 0.00001806
Iteration 254/1000 | Loss: 0.00001806
Iteration 255/1000 | Loss: 0.00001806
Iteration 256/1000 | Loss: 0.00001806
Iteration 257/1000 | Loss: 0.00001806
Iteration 258/1000 | Loss: 0.00001806
Iteration 259/1000 | Loss: 0.00001806
Iteration 260/1000 | Loss: 0.00001806
Iteration 261/1000 | Loss: 0.00001806
Iteration 262/1000 | Loss: 0.00001806
Iteration 263/1000 | Loss: 0.00001806
Iteration 264/1000 | Loss: 0.00001806
Iteration 265/1000 | Loss: 0.00001806
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 265. Stopping optimization.
Last 5 losses: [1.8058495697914623e-05, 1.8058495697914623e-05, 1.8058495697914623e-05, 1.8058495697914623e-05, 1.8058495697914623e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8058495697914623e-05

Optimization complete. Final v2v error: 3.5504631996154785 mm

Highest mean error: 5.215217590332031 mm for frame 98

Lowest mean error: 3.153442144393921 mm for frame 140

Saving results

Total time: 48.8171284198761
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01009115
Iteration 2/25 | Loss: 0.00249534
Iteration 3/25 | Loss: 0.00221083
Iteration 4/25 | Loss: 0.00214960
Iteration 5/25 | Loss: 0.00210185
Iteration 6/25 | Loss: 0.00207663
Iteration 7/25 | Loss: 0.00204486
Iteration 8/25 | Loss: 0.00201955
Iteration 9/25 | Loss: 0.00200667
Iteration 10/25 | Loss: 0.00200207
Iteration 11/25 | Loss: 0.00200076
Iteration 12/25 | Loss: 0.00200010
Iteration 13/25 | Loss: 0.00199998
Iteration 14/25 | Loss: 0.00199998
Iteration 15/25 | Loss: 0.00199998
Iteration 16/25 | Loss: 0.00199998
Iteration 17/25 | Loss: 0.00199998
Iteration 18/25 | Loss: 0.00199998
Iteration 19/25 | Loss: 0.00199998
Iteration 20/25 | Loss: 0.00199998
Iteration 21/25 | Loss: 0.00199998
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0019999828655272722, 0.0019999828655272722, 0.0019999828655272722, 0.0019999828655272722, 0.0019999828655272722]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019999828655272722

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29670334
Iteration 2/25 | Loss: 0.00980267
Iteration 3/25 | Loss: 0.00980267
Iteration 4/25 | Loss: 0.00980267
Iteration 5/25 | Loss: 0.00980267
Iteration 6/25 | Loss: 0.00980267
Iteration 7/25 | Loss: 0.00980267
Iteration 8/25 | Loss: 0.00980267
Iteration 9/25 | Loss: 0.00980267
Iteration 10/25 | Loss: 0.00980267
Iteration 11/25 | Loss: 0.00980267
Iteration 12/25 | Loss: 0.00980267
Iteration 13/25 | Loss: 0.00980267
Iteration 14/25 | Loss: 0.00980267
Iteration 15/25 | Loss: 0.00980267
Iteration 16/25 | Loss: 0.00980267
Iteration 17/25 | Loss: 0.00980267
Iteration 18/25 | Loss: 0.00980267
Iteration 19/25 | Loss: 0.00980267
Iteration 20/25 | Loss: 0.00980267
Iteration 21/25 | Loss: 0.00980267
Iteration 22/25 | Loss: 0.00980267
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.009802665561437607, 0.009802665561437607, 0.009802665561437607, 0.009802665561437607, 0.009802665561437607]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.009802665561437607

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00980267
Iteration 2/1000 | Loss: 0.00121473
Iteration 3/1000 | Loss: 0.00097471
Iteration 4/1000 | Loss: 0.00062430
Iteration 5/1000 | Loss: 0.00057133
Iteration 6/1000 | Loss: 0.00051649
Iteration 7/1000 | Loss: 0.00046974
Iteration 8/1000 | Loss: 0.00043228
Iteration 9/1000 | Loss: 0.00041774
Iteration 10/1000 | Loss: 0.00040184
Iteration 11/1000 | Loss: 0.00039279
Iteration 12/1000 | Loss: 0.00038574
Iteration 13/1000 | Loss: 0.00038234
Iteration 14/1000 | Loss: 0.00037830
Iteration 15/1000 | Loss: 0.00037585
Iteration 16/1000 | Loss: 0.00037353
Iteration 17/1000 | Loss: 0.00037171
Iteration 18/1000 | Loss: 0.00037014
Iteration 19/1000 | Loss: 0.00036900
Iteration 20/1000 | Loss: 0.00036806
Iteration 21/1000 | Loss: 0.00036754
Iteration 22/1000 | Loss: 0.00036696
Iteration 23/1000 | Loss: 0.00036666
Iteration 24/1000 | Loss: 0.00036626
Iteration 25/1000 | Loss: 0.00036590
Iteration 26/1000 | Loss: 0.00036561
Iteration 27/1000 | Loss: 0.00036533
Iteration 28/1000 | Loss: 0.00036509
Iteration 29/1000 | Loss: 0.00036488
Iteration 30/1000 | Loss: 0.00036466
Iteration 31/1000 | Loss: 0.00036448
Iteration 32/1000 | Loss: 0.00036402
Iteration 33/1000 | Loss: 0.00036321
Iteration 34/1000 | Loss: 0.00036085
Iteration 35/1000 | Loss: 0.00035508
Iteration 36/1000 | Loss: 0.00034604
Iteration 37/1000 | Loss: 0.00033734
Iteration 38/1000 | Loss: 0.00032924
Iteration 39/1000 | Loss: 0.00031961
Iteration 40/1000 | Loss: 0.00031402
Iteration 41/1000 | Loss: 0.00030784
Iteration 42/1000 | Loss: 0.00030258
Iteration 43/1000 | Loss: 0.00029793
Iteration 44/1000 | Loss: 0.00029362
Iteration 45/1000 | Loss: 0.00028925
Iteration 46/1000 | Loss: 0.00028649
Iteration 47/1000 | Loss: 0.00028403
Iteration 48/1000 | Loss: 0.00028248
Iteration 49/1000 | Loss: 0.00028113
Iteration 50/1000 | Loss: 0.00027997
Iteration 51/1000 | Loss: 0.00027905
Iteration 52/1000 | Loss: 0.00027824
Iteration 53/1000 | Loss: 0.00027732
Iteration 54/1000 | Loss: 0.00027685
Iteration 55/1000 | Loss: 0.00027626
Iteration 56/1000 | Loss: 0.00027593
Iteration 57/1000 | Loss: 0.00027563
Iteration 58/1000 | Loss: 0.00027536
Iteration 59/1000 | Loss: 0.00027515
Iteration 60/1000 | Loss: 0.00027482
Iteration 61/1000 | Loss: 0.00027452
Iteration 62/1000 | Loss: 0.00027414
Iteration 63/1000 | Loss: 0.00027369
Iteration 64/1000 | Loss: 0.00027317
Iteration 65/1000 | Loss: 0.00027259
Iteration 66/1000 | Loss: 0.00027202
Iteration 67/1000 | Loss: 0.00027152
Iteration 68/1000 | Loss: 0.00027093
Iteration 69/1000 | Loss: 0.00027020
Iteration 70/1000 | Loss: 0.00026863
Iteration 71/1000 | Loss: 0.00037499
Iteration 72/1000 | Loss: 0.00032025
Iteration 73/1000 | Loss: 0.00954501
Iteration 74/1000 | Loss: 0.00216664
Iteration 75/1000 | Loss: 0.00029394
Iteration 76/1000 | Loss: 0.00027530
Iteration 77/1000 | Loss: 0.00027219
Iteration 78/1000 | Loss: 0.00027021
Iteration 79/1000 | Loss: 0.00026697
Iteration 80/1000 | Loss: 0.00658601
Iteration 81/1000 | Loss: 0.00118021
Iteration 82/1000 | Loss: 0.00026924
Iteration 83/1000 | Loss: 0.01433940
Iteration 84/1000 | Loss: 0.00169027
Iteration 85/1000 | Loss: 0.00027709
Iteration 86/1000 | Loss: 0.00026349
Iteration 87/1000 | Loss: 0.01518027
Iteration 88/1000 | Loss: 0.00098197
Iteration 89/1000 | Loss: 0.00027326
Iteration 90/1000 | Loss: 0.00706843
Iteration 91/1000 | Loss: 0.00052625
Iteration 92/1000 | Loss: 0.00028272
Iteration 93/1000 | Loss: 0.00026668
Iteration 94/1000 | Loss: 0.01610995
Iteration 95/1000 | Loss: 0.00490283
Iteration 96/1000 | Loss: 0.00108250
Iteration 97/1000 | Loss: 0.00116214
Iteration 98/1000 | Loss: 0.00046262
Iteration 99/1000 | Loss: 0.00030708
Iteration 100/1000 | Loss: 0.00025121
Iteration 101/1000 | Loss: 0.00014262
Iteration 102/1000 | Loss: 0.00012606
Iteration 103/1000 | Loss: 0.00009820
Iteration 104/1000 | Loss: 0.00008058
Iteration 105/1000 | Loss: 0.00006629
Iteration 106/1000 | Loss: 0.00005662
Iteration 107/1000 | Loss: 0.00004839
Iteration 108/1000 | Loss: 0.00004176
Iteration 109/1000 | Loss: 0.00003747
Iteration 110/1000 | Loss: 0.00003381
Iteration 111/1000 | Loss: 0.00003086
Iteration 112/1000 | Loss: 0.00002856
Iteration 113/1000 | Loss: 0.00002578
Iteration 114/1000 | Loss: 0.00002403
Iteration 115/1000 | Loss: 0.00002301
Iteration 116/1000 | Loss: 0.00002226
Iteration 117/1000 | Loss: 0.00002163
Iteration 118/1000 | Loss: 0.00002112
Iteration 119/1000 | Loss: 0.00002076
Iteration 120/1000 | Loss: 0.00002034
Iteration 121/1000 | Loss: 0.00002002
Iteration 122/1000 | Loss: 0.00001973
Iteration 123/1000 | Loss: 0.00001963
Iteration 124/1000 | Loss: 0.00001943
Iteration 125/1000 | Loss: 0.00001934
Iteration 126/1000 | Loss: 0.00001920
Iteration 127/1000 | Loss: 0.00001915
Iteration 128/1000 | Loss: 0.00001907
Iteration 129/1000 | Loss: 0.00001892
Iteration 130/1000 | Loss: 0.00001890
Iteration 131/1000 | Loss: 0.00001888
Iteration 132/1000 | Loss: 0.00001887
Iteration 133/1000 | Loss: 0.00001886
Iteration 134/1000 | Loss: 0.00001882
Iteration 135/1000 | Loss: 0.00001881
Iteration 136/1000 | Loss: 0.00001881
Iteration 137/1000 | Loss: 0.00001877
Iteration 138/1000 | Loss: 0.00001877
Iteration 139/1000 | Loss: 0.00001876
Iteration 140/1000 | Loss: 0.00001875
Iteration 141/1000 | Loss: 0.00001874
Iteration 142/1000 | Loss: 0.00001874
Iteration 143/1000 | Loss: 0.00001872
Iteration 144/1000 | Loss: 0.00001866
Iteration 145/1000 | Loss: 0.00001865
Iteration 146/1000 | Loss: 0.00001863
Iteration 147/1000 | Loss: 0.00001859
Iteration 148/1000 | Loss: 0.00001859
Iteration 149/1000 | Loss: 0.00001857
Iteration 150/1000 | Loss: 0.00001856
Iteration 151/1000 | Loss: 0.00001853
Iteration 152/1000 | Loss: 0.00001852
Iteration 153/1000 | Loss: 0.00001852
Iteration 154/1000 | Loss: 0.00001850
Iteration 155/1000 | Loss: 0.00001850
Iteration 156/1000 | Loss: 0.00001846
Iteration 157/1000 | Loss: 0.00001845
Iteration 158/1000 | Loss: 0.00001845
Iteration 159/1000 | Loss: 0.00001844
Iteration 160/1000 | Loss: 0.00001844
Iteration 161/1000 | Loss: 0.00001840
Iteration 162/1000 | Loss: 0.00001840
Iteration 163/1000 | Loss: 0.00001835
Iteration 164/1000 | Loss: 0.00001834
Iteration 165/1000 | Loss: 0.00001834
Iteration 166/1000 | Loss: 0.00001834
Iteration 167/1000 | Loss: 0.00001834
Iteration 168/1000 | Loss: 0.00001833
Iteration 169/1000 | Loss: 0.00001832
Iteration 170/1000 | Loss: 0.00001830
Iteration 171/1000 | Loss: 0.00001830
Iteration 172/1000 | Loss: 0.00001829
Iteration 173/1000 | Loss: 0.00001828
Iteration 174/1000 | Loss: 0.00001828
Iteration 175/1000 | Loss: 0.00001827
Iteration 176/1000 | Loss: 0.00001827
Iteration 177/1000 | Loss: 0.00001827
Iteration 178/1000 | Loss: 0.00001827
Iteration 179/1000 | Loss: 0.00001827
Iteration 180/1000 | Loss: 0.00001827
Iteration 181/1000 | Loss: 0.00001824
Iteration 182/1000 | Loss: 0.00001824
Iteration 183/1000 | Loss: 0.00001822
Iteration 184/1000 | Loss: 0.00001822
Iteration 185/1000 | Loss: 0.00001822
Iteration 186/1000 | Loss: 0.00001822
Iteration 187/1000 | Loss: 0.00001822
Iteration 188/1000 | Loss: 0.00001822
Iteration 189/1000 | Loss: 0.00001822
Iteration 190/1000 | Loss: 0.00001822
Iteration 191/1000 | Loss: 0.00001822
Iteration 192/1000 | Loss: 0.00001821
Iteration 193/1000 | Loss: 0.00001821
Iteration 194/1000 | Loss: 0.00001821
Iteration 195/1000 | Loss: 0.00001821
Iteration 196/1000 | Loss: 0.00001821
Iteration 197/1000 | Loss: 0.00001820
Iteration 198/1000 | Loss: 0.00001820
Iteration 199/1000 | Loss: 0.00001820
Iteration 200/1000 | Loss: 0.00001820
Iteration 201/1000 | Loss: 0.00001819
Iteration 202/1000 | Loss: 0.00001819
Iteration 203/1000 | Loss: 0.00001819
Iteration 204/1000 | Loss: 0.00001819
Iteration 205/1000 | Loss: 0.00001818
Iteration 206/1000 | Loss: 0.00001818
Iteration 207/1000 | Loss: 0.00001818
Iteration 208/1000 | Loss: 0.00001818
Iteration 209/1000 | Loss: 0.00001818
Iteration 210/1000 | Loss: 0.00001818
Iteration 211/1000 | Loss: 0.00001815
Iteration 212/1000 | Loss: 0.00001815
Iteration 213/1000 | Loss: 0.00001815
Iteration 214/1000 | Loss: 0.00001815
Iteration 215/1000 | Loss: 0.00001815
Iteration 216/1000 | Loss: 0.00001815
Iteration 217/1000 | Loss: 0.00001815
Iteration 218/1000 | Loss: 0.00001814
Iteration 219/1000 | Loss: 0.00001814
Iteration 220/1000 | Loss: 0.00001814
Iteration 221/1000 | Loss: 0.00001814
Iteration 222/1000 | Loss: 0.00001814
Iteration 223/1000 | Loss: 0.00001814
Iteration 224/1000 | Loss: 0.00001813
Iteration 225/1000 | Loss: 0.00001813
Iteration 226/1000 | Loss: 0.00001813
Iteration 227/1000 | Loss: 0.00001812
Iteration 228/1000 | Loss: 0.00001812
Iteration 229/1000 | Loss: 0.00001812
Iteration 230/1000 | Loss: 0.00001812
Iteration 231/1000 | Loss: 0.00001812
Iteration 232/1000 | Loss: 0.00001812
Iteration 233/1000 | Loss: 0.00001811
Iteration 234/1000 | Loss: 0.00001811
Iteration 235/1000 | Loss: 0.00001811
Iteration 236/1000 | Loss: 0.00001811
Iteration 237/1000 | Loss: 0.00001811
Iteration 238/1000 | Loss: 0.00001811
Iteration 239/1000 | Loss: 0.00001811
Iteration 240/1000 | Loss: 0.00001811
Iteration 241/1000 | Loss: 0.00001811
Iteration 242/1000 | Loss: 0.00001811
Iteration 243/1000 | Loss: 0.00001811
Iteration 244/1000 | Loss: 0.00001811
Iteration 245/1000 | Loss: 0.00001811
Iteration 246/1000 | Loss: 0.00001811
Iteration 247/1000 | Loss: 0.00001811
Iteration 248/1000 | Loss: 0.00001811
Iteration 249/1000 | Loss: 0.00001811
Iteration 250/1000 | Loss: 0.00001811
Iteration 251/1000 | Loss: 0.00001811
Iteration 252/1000 | Loss: 0.00001811
Iteration 253/1000 | Loss: 0.00001811
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 253. Stopping optimization.
Last 5 losses: [1.811175752663985e-05, 1.811175752663985e-05, 1.811175752663985e-05, 1.811175752663985e-05, 1.811175752663985e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.811175752663985e-05

Optimization complete. Final v2v error: 3.4058685302734375 mm

Highest mean error: 11.279955863952637 mm for frame 178

Lowest mean error: 3.2892115116119385 mm for frame 239

Saving results

Total time: 250.16621446609497
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00471374
Iteration 2/25 | Loss: 0.00170407
Iteration 3/25 | Loss: 0.00148683
Iteration 4/25 | Loss: 0.00142023
Iteration 5/25 | Loss: 0.00140774
Iteration 6/25 | Loss: 0.00138377
Iteration 7/25 | Loss: 0.00137058
Iteration 8/25 | Loss: 0.00136941
Iteration 9/25 | Loss: 0.00136572
Iteration 10/25 | Loss: 0.00136536
Iteration 11/25 | Loss: 0.00136519
Iteration 12/25 | Loss: 0.00136454
Iteration 13/25 | Loss: 0.00136614
Iteration 14/25 | Loss: 0.00136323
Iteration 15/25 | Loss: 0.00136220
Iteration 16/25 | Loss: 0.00136202
Iteration 17/25 | Loss: 0.00136191
Iteration 18/25 | Loss: 0.00136052
Iteration 19/25 | Loss: 0.00135902
Iteration 20/25 | Loss: 0.00135878
Iteration 21/25 | Loss: 0.00135877
Iteration 22/25 | Loss: 0.00135877
Iteration 23/25 | Loss: 0.00135877
Iteration 24/25 | Loss: 0.00135877
Iteration 25/25 | Loss: 0.00135877

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40359390
Iteration 2/25 | Loss: 0.00109737
Iteration 3/25 | Loss: 0.00109735
Iteration 4/25 | Loss: 0.00109734
Iteration 5/25 | Loss: 0.00109734
Iteration 6/25 | Loss: 0.00109734
Iteration 7/25 | Loss: 0.00109734
Iteration 8/25 | Loss: 0.00109734
Iteration 9/25 | Loss: 0.00109734
Iteration 10/25 | Loss: 0.00109734
Iteration 11/25 | Loss: 0.00109734
Iteration 12/25 | Loss: 0.00109734
Iteration 13/25 | Loss: 0.00109734
Iteration 14/25 | Loss: 0.00109734
Iteration 15/25 | Loss: 0.00109734
Iteration 16/25 | Loss: 0.00109734
Iteration 17/25 | Loss: 0.00109734
Iteration 18/25 | Loss: 0.00109734
Iteration 19/25 | Loss: 0.00109734
Iteration 20/25 | Loss: 0.00109734
Iteration 21/25 | Loss: 0.00109734
Iteration 22/25 | Loss: 0.00109734
Iteration 23/25 | Loss: 0.00109734
Iteration 24/25 | Loss: 0.00109734
Iteration 25/25 | Loss: 0.00109734

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109734
Iteration 2/1000 | Loss: 0.00009749
Iteration 3/1000 | Loss: 0.00005279
Iteration 4/1000 | Loss: 0.00003562
Iteration 5/1000 | Loss: 0.00003818
Iteration 6/1000 | Loss: 0.00003479
Iteration 7/1000 | Loss: 0.00004017
Iteration 8/1000 | Loss: 0.00003722
Iteration 9/1000 | Loss: 0.00004342
Iteration 10/1000 | Loss: 0.00004437
Iteration 11/1000 | Loss: 0.00003610
Iteration 12/1000 | Loss: 0.00003214
Iteration 13/1000 | Loss: 0.00003862
Iteration 14/1000 | Loss: 0.00004737
Iteration 15/1000 | Loss: 0.00003715
Iteration 16/1000 | Loss: 0.00003668
Iteration 17/1000 | Loss: 0.00002963
Iteration 18/1000 | Loss: 0.00003483
Iteration 19/1000 | Loss: 0.00002963
Iteration 20/1000 | Loss: 0.00002718
Iteration 21/1000 | Loss: 0.00002712
Iteration 22/1000 | Loss: 0.00003850
Iteration 23/1000 | Loss: 0.00003822
Iteration 24/1000 | Loss: 0.00003757
Iteration 25/1000 | Loss: 0.00003059
Iteration 26/1000 | Loss: 0.00003031
Iteration 27/1000 | Loss: 0.00002961
Iteration 28/1000 | Loss: 0.00002869
Iteration 29/1000 | Loss: 0.00002844
Iteration 30/1000 | Loss: 0.00002829
Iteration 31/1000 | Loss: 0.00002939
Iteration 32/1000 | Loss: 0.00002829
Iteration 33/1000 | Loss: 0.00002610
Iteration 34/1000 | Loss: 0.00003125
Iteration 35/1000 | Loss: 0.00002962
Iteration 36/1000 | Loss: 0.00002635
Iteration 37/1000 | Loss: 0.00002974
Iteration 38/1000 | Loss: 0.00002908
Iteration 39/1000 | Loss: 0.00003003
Iteration 40/1000 | Loss: 0.00002776
Iteration 41/1000 | Loss: 0.00002858
Iteration 42/1000 | Loss: 0.00002887
Iteration 43/1000 | Loss: 0.00002727
Iteration 44/1000 | Loss: 0.00002734
Iteration 45/1000 | Loss: 0.00002638
Iteration 46/1000 | Loss: 0.00002892
Iteration 47/1000 | Loss: 0.00003338
Iteration 48/1000 | Loss: 0.00003262
Iteration 49/1000 | Loss: 0.00003620
Iteration 50/1000 | Loss: 0.00003533
Iteration 51/1000 | Loss: 0.00003974
Iteration 52/1000 | Loss: 0.00003708
Iteration 53/1000 | Loss: 0.00004057
Iteration 54/1000 | Loss: 0.00004040
Iteration 55/1000 | Loss: 0.00004257
Iteration 56/1000 | Loss: 0.00004169
Iteration 57/1000 | Loss: 0.00004980
Iteration 58/1000 | Loss: 0.00004462
Iteration 59/1000 | Loss: 0.00004853
Iteration 60/1000 | Loss: 0.00004682
Iteration 61/1000 | Loss: 0.00005082
Iteration 62/1000 | Loss: 0.00004375
Iteration 63/1000 | Loss: 0.00004301
Iteration 64/1000 | Loss: 0.00004536
Iteration 65/1000 | Loss: 0.00004613
Iteration 66/1000 | Loss: 0.00003105
Iteration 67/1000 | Loss: 0.00002706
Iteration 68/1000 | Loss: 0.00003070
Iteration 69/1000 | Loss: 0.00002848
Iteration 70/1000 | Loss: 0.00002962
Iteration 71/1000 | Loss: 0.00004441
Iteration 72/1000 | Loss: 0.00004429
Iteration 73/1000 | Loss: 0.00004943
Iteration 74/1000 | Loss: 0.00005025
Iteration 75/1000 | Loss: 0.00004823
Iteration 76/1000 | Loss: 0.00005074
Iteration 77/1000 | Loss: 0.00003145
Iteration 78/1000 | Loss: 0.00003100
Iteration 79/1000 | Loss: 0.00002628
Iteration 80/1000 | Loss: 0.00003831
Iteration 81/1000 | Loss: 0.00003421
Iteration 82/1000 | Loss: 0.00004212
Iteration 83/1000 | Loss: 0.00004045
Iteration 84/1000 | Loss: 0.00004222
Iteration 85/1000 | Loss: 0.00003630
Iteration 86/1000 | Loss: 0.00003950
Iteration 87/1000 | Loss: 0.00004087
Iteration 88/1000 | Loss: 0.00004394
Iteration 89/1000 | Loss: 0.00004736
Iteration 90/1000 | Loss: 0.00005444
Iteration 91/1000 | Loss: 0.00005545
Iteration 92/1000 | Loss: 0.00006033
Iteration 93/1000 | Loss: 0.00004401
Iteration 94/1000 | Loss: 0.00005788
Iteration 95/1000 | Loss: 0.00005437
Iteration 96/1000 | Loss: 0.00005058
Iteration 97/1000 | Loss: 0.00004918
Iteration 98/1000 | Loss: 0.00006905
Iteration 99/1000 | Loss: 0.00006065
Iteration 100/1000 | Loss: 0.00003101
Iteration 101/1000 | Loss: 0.00002853
Iteration 102/1000 | Loss: 0.00002691
Iteration 103/1000 | Loss: 0.00002906
Iteration 104/1000 | Loss: 0.00002814
Iteration 105/1000 | Loss: 0.00003309
Iteration 106/1000 | Loss: 0.00003355
Iteration 107/1000 | Loss: 0.00004138
Iteration 108/1000 | Loss: 0.00003272
Iteration 109/1000 | Loss: 0.00002683
Iteration 110/1000 | Loss: 0.00002664
Iteration 111/1000 | Loss: 0.00002634
Iteration 112/1000 | Loss: 0.00002736
Iteration 113/1000 | Loss: 0.00002756
Iteration 114/1000 | Loss: 0.00002908
Iteration 115/1000 | Loss: 0.00002762
Iteration 116/1000 | Loss: 0.00002732
Iteration 117/1000 | Loss: 0.00002797
Iteration 118/1000 | Loss: 0.00002901
Iteration 119/1000 | Loss: 0.00002701
Iteration 120/1000 | Loss: 0.00002664
Iteration 121/1000 | Loss: 0.00002778
Iteration 122/1000 | Loss: 0.00002950
Iteration 123/1000 | Loss: 0.00003063
Iteration 124/1000 | Loss: 0.00003135
Iteration 125/1000 | Loss: 0.00003623
Iteration 126/1000 | Loss: 0.00003688
Iteration 127/1000 | Loss: 0.00003706
Iteration 128/1000 | Loss: 0.00003549
Iteration 129/1000 | Loss: 0.00003603
Iteration 130/1000 | Loss: 0.00004135
Iteration 131/1000 | Loss: 0.00004158
Iteration 132/1000 | Loss: 0.00004677
Iteration 133/1000 | Loss: 0.00004397
Iteration 134/1000 | Loss: 0.00004539
Iteration 135/1000 | Loss: 0.00004526
Iteration 136/1000 | Loss: 0.00003628
Iteration 137/1000 | Loss: 0.00003823
Iteration 138/1000 | Loss: 0.00004219
Iteration 139/1000 | Loss: 0.00004208
Iteration 140/1000 | Loss: 0.00005240
Iteration 141/1000 | Loss: 0.00004663
Iteration 142/1000 | Loss: 0.00005589
Iteration 143/1000 | Loss: 0.00004869
Iteration 144/1000 | Loss: 0.00005032
Iteration 145/1000 | Loss: 0.00005060
Iteration 146/1000 | Loss: 0.00004850
Iteration 147/1000 | Loss: 0.00005097
Iteration 148/1000 | Loss: 0.00005795
Iteration 149/1000 | Loss: 0.00005599
Iteration 150/1000 | Loss: 0.00004692
Iteration 151/1000 | Loss: 0.00004971
Iteration 152/1000 | Loss: 0.00006466
Iteration 153/1000 | Loss: 0.00006829
Iteration 154/1000 | Loss: 0.00005420
Iteration 155/1000 | Loss: 0.00006246
Iteration 156/1000 | Loss: 0.00004579
Iteration 157/1000 | Loss: 0.00004893
Iteration 158/1000 | Loss: 0.00002796
Iteration 159/1000 | Loss: 0.00002502
Iteration 160/1000 | Loss: 0.00004169
Iteration 161/1000 | Loss: 0.00004571
Iteration 162/1000 | Loss: 0.00003210
Iteration 163/1000 | Loss: 0.00003049
Iteration 164/1000 | Loss: 0.00005029
Iteration 165/1000 | Loss: 0.00004902
Iteration 166/1000 | Loss: 0.00004642
Iteration 167/1000 | Loss: 0.00005072
Iteration 168/1000 | Loss: 0.00003730
Iteration 169/1000 | Loss: 0.00002690
Iteration 170/1000 | Loss: 0.00003217
Iteration 171/1000 | Loss: 0.00007264
Iteration 172/1000 | Loss: 0.00005037
Iteration 173/1000 | Loss: 0.00003891
Iteration 174/1000 | Loss: 0.00003060
Iteration 175/1000 | Loss: 0.00006075
Iteration 176/1000 | Loss: 0.00004500
Iteration 177/1000 | Loss: 0.00008444
Iteration 178/1000 | Loss: 0.00006067
Iteration 179/1000 | Loss: 0.00008738
Iteration 180/1000 | Loss: 0.00006396
Iteration 181/1000 | Loss: 0.00006748
Iteration 182/1000 | Loss: 0.00005574
Iteration 183/1000 | Loss: 0.00007316
Iteration 184/1000 | Loss: 0.00005909
Iteration 185/1000 | Loss: 0.00008767
Iteration 186/1000 | Loss: 0.00006388
Iteration 187/1000 | Loss: 0.00006474
Iteration 188/1000 | Loss: 0.00005864
Iteration 189/1000 | Loss: 0.00008748
Iteration 190/1000 | Loss: 0.00005912
Iteration 191/1000 | Loss: 0.00005738
Iteration 192/1000 | Loss: 0.00005218
Iteration 193/1000 | Loss: 0.00008409
Iteration 194/1000 | Loss: 0.00006484
Iteration 195/1000 | Loss: 0.00004384
Iteration 196/1000 | Loss: 0.00003256
Iteration 197/1000 | Loss: 0.00004026
Iteration 198/1000 | Loss: 0.00002624
Iteration 199/1000 | Loss: 0.00004253
Iteration 200/1000 | Loss: 0.00004187
Iteration 201/1000 | Loss: 0.00002933
Iteration 202/1000 | Loss: 0.00003209
Iteration 203/1000 | Loss: 0.00002826
Iteration 204/1000 | Loss: 0.00004462
Iteration 205/1000 | Loss: 0.00004795
Iteration 206/1000 | Loss: 0.00005045
Iteration 207/1000 | Loss: 0.00005056
Iteration 208/1000 | Loss: 0.00003157
Iteration 209/1000 | Loss: 0.00003159
Iteration 210/1000 | Loss: 0.00003218
Iteration 211/1000 | Loss: 0.00005489
Iteration 212/1000 | Loss: 0.00005082
Iteration 213/1000 | Loss: 0.00006532
Iteration 214/1000 | Loss: 0.00005327
Iteration 215/1000 | Loss: 0.00005799
Iteration 216/1000 | Loss: 0.00005579
Iteration 217/1000 | Loss: 0.00003235
Iteration 218/1000 | Loss: 0.00002761
Iteration 219/1000 | Loss: 0.00005295
Iteration 220/1000 | Loss: 0.00005305
Iteration 221/1000 | Loss: 0.00006175
Iteration 222/1000 | Loss: 0.00005990
Iteration 223/1000 | Loss: 0.00005784
Iteration 224/1000 | Loss: 0.00006114
Iteration 225/1000 | Loss: 0.00006920
Iteration 226/1000 | Loss: 0.00007158
Iteration 227/1000 | Loss: 0.00007269
Iteration 228/1000 | Loss: 0.00007156
Iteration 229/1000 | Loss: 0.00007295
Iteration 230/1000 | Loss: 0.00006973
Iteration 231/1000 | Loss: 0.00007070
Iteration 232/1000 | Loss: 0.00007207
Iteration 233/1000 | Loss: 0.00007076
Iteration 234/1000 | Loss: 0.00008214
Iteration 235/1000 | Loss: 0.00007293
Iteration 236/1000 | Loss: 0.00006353
Iteration 237/1000 | Loss: 0.00005669
Iteration 238/1000 | Loss: 0.00007232
Iteration 239/1000 | Loss: 0.00003271
Iteration 240/1000 | Loss: 0.00005705
Iteration 241/1000 | Loss: 0.00003417
Iteration 242/1000 | Loss: 0.00003489
Iteration 243/1000 | Loss: 0.00002774
Iteration 244/1000 | Loss: 0.00003742
Iteration 245/1000 | Loss: 0.00003042
Iteration 246/1000 | Loss: 0.00005522
Iteration 247/1000 | Loss: 0.00003867
Iteration 248/1000 | Loss: 0.00005927
Iteration 249/1000 | Loss: 0.00004136
Iteration 250/1000 | Loss: 0.00003101
Iteration 251/1000 | Loss: 0.00002631
Iteration 252/1000 | Loss: 0.00003081
Iteration 253/1000 | Loss: 0.00002675
Iteration 254/1000 | Loss: 0.00005911
Iteration 255/1000 | Loss: 0.00004152
Iteration 256/1000 | Loss: 0.00003027
Iteration 257/1000 | Loss: 0.00002579
Iteration 258/1000 | Loss: 0.00004648
Iteration 259/1000 | Loss: 0.00004018
Iteration 260/1000 | Loss: 0.00007169
Iteration 261/1000 | Loss: 0.00005083
Iteration 262/1000 | Loss: 0.00007525
Iteration 263/1000 | Loss: 0.00006370
Iteration 264/1000 | Loss: 0.00002912
Iteration 265/1000 | Loss: 0.00002743
Iteration 266/1000 | Loss: 0.00005549
Iteration 267/1000 | Loss: 0.00005405
Iteration 268/1000 | Loss: 0.00004667
Iteration 269/1000 | Loss: 0.00003340
Iteration 270/1000 | Loss: 0.00005564
Iteration 271/1000 | Loss: 0.00005392
Iteration 272/1000 | Loss: 0.00002676
Iteration 273/1000 | Loss: 0.00004378
Iteration 274/1000 | Loss: 0.00003258
Iteration 275/1000 | Loss: 0.00003678
Iteration 276/1000 | Loss: 0.00002944
Iteration 277/1000 | Loss: 0.00006352
Iteration 278/1000 | Loss: 0.00005550
Iteration 279/1000 | Loss: 0.00007386
Iteration 280/1000 | Loss: 0.00005815
Iteration 281/1000 | Loss: 0.00007006
Iteration 282/1000 | Loss: 0.00006103
Iteration 283/1000 | Loss: 0.00007222
Iteration 284/1000 | Loss: 0.00005447
Iteration 285/1000 | Loss: 0.00006239
Iteration 286/1000 | Loss: 0.00005626
Iteration 287/1000 | Loss: 0.00006832
Iteration 288/1000 | Loss: 0.00005932
Iteration 289/1000 | Loss: 0.00007347
Iteration 290/1000 | Loss: 0.00005723
Iteration 291/1000 | Loss: 0.00007739
Iteration 292/1000 | Loss: 0.00006124
Iteration 293/1000 | Loss: 0.00007867
Iteration 294/1000 | Loss: 0.00005152
Iteration 295/1000 | Loss: 0.00003282
Iteration 296/1000 | Loss: 0.00004907
Iteration 297/1000 | Loss: 0.00003399
Iteration 298/1000 | Loss: 0.00003804
Iteration 299/1000 | Loss: 0.00004577
Iteration 300/1000 | Loss: 0.00005584
Iteration 301/1000 | Loss: 0.00005909
Iteration 302/1000 | Loss: 0.00005749
Iteration 303/1000 | Loss: 0.00007267
Iteration 304/1000 | Loss: 0.00006365
Iteration 305/1000 | Loss: 0.00020728
Iteration 306/1000 | Loss: 0.00004029
Iteration 307/1000 | Loss: 0.00006193
Iteration 308/1000 | Loss: 0.00006000
Iteration 309/1000 | Loss: 0.00006404
Iteration 310/1000 | Loss: 0.00006084
Iteration 311/1000 | Loss: 0.00003563
Iteration 312/1000 | Loss: 0.00003226
Iteration 313/1000 | Loss: 0.00003560
Iteration 314/1000 | Loss: 0.00003780
Iteration 315/1000 | Loss: 0.00002795
Iteration 316/1000 | Loss: 0.00002821
Iteration 317/1000 | Loss: 0.00002900
Iteration 318/1000 | Loss: 0.00002871
Iteration 319/1000 | Loss: 0.00003072
Iteration 320/1000 | Loss: 0.00003053
Iteration 321/1000 | Loss: 0.00003150
Iteration 322/1000 | Loss: 0.00003369
Iteration 323/1000 | Loss: 0.00003031
Iteration 324/1000 | Loss: 0.00003136
Iteration 325/1000 | Loss: 0.00003100
Iteration 326/1000 | Loss: 0.00002871
Iteration 327/1000 | Loss: 0.00003783
Iteration 328/1000 | Loss: 0.00003601
Iteration 329/1000 | Loss: 0.00002855
Iteration 330/1000 | Loss: 0.00003043
Iteration 331/1000 | Loss: 0.00003149
Iteration 332/1000 | Loss: 0.00003931
Iteration 333/1000 | Loss: 0.00003843
Iteration 334/1000 | Loss: 0.00002702
Iteration 335/1000 | Loss: 0.00004355
Iteration 336/1000 | Loss: 0.00003712
Iteration 337/1000 | Loss: 0.00003344
Iteration 338/1000 | Loss: 0.00003135
Iteration 339/1000 | Loss: 0.00004240
Iteration 340/1000 | Loss: 0.00003464
Iteration 341/1000 | Loss: 0.00003890
Iteration 342/1000 | Loss: 0.00003532
Iteration 343/1000 | Loss: 0.00003122
Iteration 344/1000 | Loss: 0.00003427
Iteration 345/1000 | Loss: 0.00002819
Iteration 346/1000 | Loss: 0.00004454
Iteration 347/1000 | Loss: 0.00003758
Iteration 348/1000 | Loss: 0.00004176
Iteration 349/1000 | Loss: 0.00003578
Iteration 350/1000 | Loss: 0.00003139
Iteration 351/1000 | Loss: 0.00002680
Iteration 352/1000 | Loss: 0.00003874
Iteration 353/1000 | Loss: 0.00003168
Iteration 354/1000 | Loss: 0.00004992
Iteration 355/1000 | Loss: 0.00003892
Iteration 356/1000 | Loss: 0.00005050
Iteration 357/1000 | Loss: 0.00003709
Iteration 358/1000 | Loss: 0.00004923
Iteration 359/1000 | Loss: 0.00003677
Iteration 360/1000 | Loss: 0.00004543
Iteration 361/1000 | Loss: 0.00003891
Iteration 362/1000 | Loss: 0.00005377
Iteration 363/1000 | Loss: 0.00004022
Iteration 364/1000 | Loss: 0.00005647
Iteration 365/1000 | Loss: 0.00004520
Iteration 366/1000 | Loss: 0.00005837
Iteration 367/1000 | Loss: 0.00003816
Iteration 368/1000 | Loss: 0.00003846
Iteration 369/1000 | Loss: 0.00002842
Iteration 370/1000 | Loss: 0.00002748
Iteration 371/1000 | Loss: 0.00002722
Iteration 372/1000 | Loss: 0.00003130
Iteration 373/1000 | Loss: 0.00003174
Iteration 374/1000 | Loss: 0.00003266
Iteration 375/1000 | Loss: 0.00003708
Iteration 376/1000 | Loss: 0.00003080
Iteration 377/1000 | Loss: 0.00002870
Iteration 378/1000 | Loss: 0.00003693
Iteration 379/1000 | Loss: 0.00003198
Iteration 380/1000 | Loss: 0.00002912
Iteration 381/1000 | Loss: 0.00002592
Iteration 382/1000 | Loss: 0.00003062
Iteration 383/1000 | Loss: 0.00004308
Iteration 384/1000 | Loss: 0.00002829
Iteration 385/1000 | Loss: 0.00003259
Iteration 386/1000 | Loss: 0.00005109
Iteration 387/1000 | Loss: 0.00003995
Iteration 388/1000 | Loss: 0.00004526
Iteration 389/1000 | Loss: 0.00003593
Iteration 390/1000 | Loss: 0.00005250
Iteration 391/1000 | Loss: 0.00004011
Iteration 392/1000 | Loss: 0.00006637
Iteration 393/1000 | Loss: 0.00004979
Iteration 394/1000 | Loss: 0.00004014
Iteration 395/1000 | Loss: 0.00003288
Iteration 396/1000 | Loss: 0.00002875
Iteration 397/1000 | Loss: 0.00003079
Iteration 398/1000 | Loss: 0.00003935
Iteration 399/1000 | Loss: 0.00003759
Iteration 400/1000 | Loss: 0.00004824
Iteration 401/1000 | Loss: 0.00004514
Iteration 402/1000 | Loss: 0.00005082
Iteration 403/1000 | Loss: 0.00004903
Iteration 404/1000 | Loss: 0.00004866
Iteration 405/1000 | Loss: 0.00005052
Iteration 406/1000 | Loss: 0.00005159
Iteration 407/1000 | Loss: 0.00003517
Iteration 408/1000 | Loss: 0.00003339
Iteration 409/1000 | Loss: 0.00002718
Iteration 410/1000 | Loss: 0.00002586
Iteration 411/1000 | Loss: 0.00002771
Iteration 412/1000 | Loss: 0.00002704
Iteration 413/1000 | Loss: 0.00003107
Iteration 414/1000 | Loss: 0.00003319
Iteration 415/1000 | Loss: 0.00003919
Iteration 416/1000 | Loss: 0.00003487
Iteration 417/1000 | Loss: 0.00003790
Iteration 418/1000 | Loss: 0.00003640
Iteration 419/1000 | Loss: 0.00004606
Iteration 420/1000 | Loss: 0.00004410
Iteration 421/1000 | Loss: 0.00004354
Iteration 422/1000 | Loss: 0.00004444
Iteration 423/1000 | Loss: 0.00005159
Iteration 424/1000 | Loss: 0.00005118
Iteration 425/1000 | Loss: 0.00005442
Iteration 426/1000 | Loss: 0.00005719
Iteration 427/1000 | Loss: 0.00005089
Iteration 428/1000 | Loss: 0.00005420
Iteration 429/1000 | Loss: 0.00005138
Iteration 430/1000 | Loss: 0.00005414
Iteration 431/1000 | Loss: 0.00004679
Iteration 432/1000 | Loss: 0.00003399
Iteration 433/1000 | Loss: 0.00002507
Iteration 434/1000 | Loss: 0.00005123
Iteration 435/1000 | Loss: 0.00003711
Iteration 436/1000 | Loss: 0.00002624
Iteration 437/1000 | Loss: 0.00002948
Iteration 438/1000 | Loss: 0.00003960
Iteration 439/1000 | Loss: 0.00004586
Iteration 440/1000 | Loss: 0.00006230
Iteration 441/1000 | Loss: 0.00006932
Iteration 442/1000 | Loss: 0.00006354
Iteration 443/1000 | Loss: 0.00006498
Iteration 444/1000 | Loss: 0.00006621
Iteration 445/1000 | Loss: 0.00008031
Iteration 446/1000 | Loss: 0.00005970
Iteration 447/1000 | Loss: 0.00006867
Iteration 448/1000 | Loss: 0.00006605
Iteration 449/1000 | Loss: 0.00008399
Iteration 450/1000 | Loss: 0.00006389
Iteration 451/1000 | Loss: 0.00007341
Iteration 452/1000 | Loss: 0.00006659
Iteration 453/1000 | Loss: 0.00008061
Iteration 454/1000 | Loss: 0.00006640
Iteration 455/1000 | Loss: 0.00008949
Iteration 456/1000 | Loss: 0.00006809
Iteration 457/1000 | Loss: 0.00008915
Iteration 458/1000 | Loss: 0.00006389
Iteration 459/1000 | Loss: 0.00008305
Iteration 460/1000 | Loss: 0.00006117
Iteration 461/1000 | Loss: 0.00009993
Iteration 462/1000 | Loss: 0.00007288
Iteration 463/1000 | Loss: 0.00009783
Iteration 464/1000 | Loss: 0.00006752
Iteration 465/1000 | Loss: 0.00008227
Iteration 466/1000 | Loss: 0.00007706
Iteration 467/1000 | Loss: 0.00008933
Iteration 468/1000 | Loss: 0.00006824
Iteration 469/1000 | Loss: 0.00009147
Iteration 470/1000 | Loss: 0.00003376
Iteration 471/1000 | Loss: 0.00003081
Iteration 472/1000 | Loss: 0.00003873
Iteration 473/1000 | Loss: 0.00003602
Iteration 474/1000 | Loss: 0.00003647
Iteration 475/1000 | Loss: 0.00002622
Iteration 476/1000 | Loss: 0.00003800
Iteration 477/1000 | Loss: 0.00005215
Iteration 478/1000 | Loss: 0.00006120
Iteration 479/1000 | Loss: 0.00006339
Iteration 480/1000 | Loss: 0.00006786
Iteration 481/1000 | Loss: 0.00007954
Iteration 482/1000 | Loss: 0.00007758
Iteration 483/1000 | Loss: 0.00006533
Iteration 484/1000 | Loss: 0.00006712
Iteration 485/1000 | Loss: 0.00007734
Iteration 486/1000 | Loss: 0.00007455
Iteration 487/1000 | Loss: 0.00006717
Iteration 488/1000 | Loss: 0.00007023
Iteration 489/1000 | Loss: 0.00004946
Iteration 490/1000 | Loss: 0.00003943
Iteration 491/1000 | Loss: 0.00003391
Iteration 492/1000 | Loss: 0.00006160
Iteration 493/1000 | Loss: 0.00004588
Iteration 494/1000 | Loss: 0.00006958
Iteration 495/1000 | Loss: 0.00006407
Iteration 496/1000 | Loss: 0.00003289
Iteration 497/1000 | Loss: 0.00004764
Iteration 498/1000 | Loss: 0.00007151
Iteration 499/1000 | Loss: 0.00004563
Iteration 500/1000 | Loss: 0.00002979
Iteration 501/1000 | Loss: 0.00004076
Iteration 502/1000 | Loss: 0.00002679
Iteration 503/1000 | Loss: 0.00006013
Iteration 504/1000 | Loss: 0.00004294
Iteration 505/1000 | Loss: 0.00005985
Iteration 506/1000 | Loss: 0.00005140
Iteration 507/1000 | Loss: 0.00006530
Iteration 508/1000 | Loss: 0.00005304
Iteration 509/1000 | Loss: 0.00007099
Iteration 510/1000 | Loss: 0.00006051
Iteration 511/1000 | Loss: 0.00007440
Iteration 512/1000 | Loss: 0.00005072
Iteration 513/1000 | Loss: 0.00007187
Iteration 514/1000 | Loss: 0.00006053
Iteration 515/1000 | Loss: 0.00002841
Iteration 516/1000 | Loss: 0.00005024
Iteration 517/1000 | Loss: 0.00003488
Iteration 518/1000 | Loss: 0.00002611
Iteration 519/1000 | Loss: 0.00004117
Iteration 520/1000 | Loss: 0.00003379
Iteration 521/1000 | Loss: 0.00002873
Iteration 522/1000 | Loss: 0.00003467
Iteration 523/1000 | Loss: 0.00002761
Iteration 524/1000 | Loss: 0.00003045
Iteration 525/1000 | Loss: 0.00003900
Iteration 526/1000 | Loss: 0.00002992
Iteration 527/1000 | Loss: 0.00002819
Iteration 528/1000 | Loss: 0.00005451
Iteration 529/1000 | Loss: 0.00004028
Iteration 530/1000 | Loss: 0.00003219
Iteration 531/1000 | Loss: 0.00003392
Iteration 532/1000 | Loss: 0.00004490
Iteration 533/1000 | Loss: 0.00004623
Iteration 534/1000 | Loss: 0.00005050
Iteration 535/1000 | Loss: 0.00005357
Iteration 536/1000 | Loss: 0.00002881
Iteration 537/1000 | Loss: 0.00002732
Iteration 538/1000 | Loss: 0.00004909
Iteration 539/1000 | Loss: 0.00003975
Iteration 540/1000 | Loss: 0.00004926
Iteration 541/1000 | Loss: 0.00004690
Iteration 542/1000 | Loss: 0.00006217
Iteration 543/1000 | Loss: 0.00005116
Iteration 544/1000 | Loss: 0.00006671
Iteration 545/1000 | Loss: 0.00006232
Iteration 546/1000 | Loss: 0.00002822
Iteration 547/1000 | Loss: 0.00003326
Iteration 548/1000 | Loss: 0.00004841
Iteration 549/1000 | Loss: 0.00005031
Iteration 550/1000 | Loss: 0.00002870
Iteration 551/1000 | Loss: 0.00005661
Iteration 552/1000 | Loss: 0.00003520
Iteration 553/1000 | Loss: 0.00005555
Iteration 554/1000 | Loss: 0.00004685
Iteration 555/1000 | Loss: 0.00004864
Iteration 556/1000 | Loss: 0.00004541
Iteration 557/1000 | Loss: 0.00005949
Iteration 558/1000 | Loss: 0.00004794
Iteration 559/1000 | Loss: 0.00006330
Iteration 560/1000 | Loss: 0.00004619
Iteration 561/1000 | Loss: 0.00003335
Iteration 562/1000 | Loss: 0.00004689
Iteration 563/1000 | Loss: 0.00003001
Iteration 564/1000 | Loss: 0.00005907
Iteration 565/1000 | Loss: 0.00003633
Iteration 566/1000 | Loss: 0.00003410
Iteration 567/1000 | Loss: 0.00002773
Iteration 568/1000 | Loss: 0.00003031
Iteration 569/1000 | Loss: 0.00003353
Iteration 570/1000 | Loss: 0.00005118
Iteration 571/1000 | Loss: 0.00004252
Iteration 572/1000 | Loss: 0.00005275
Iteration 573/1000 | Loss: 0.00004421
Iteration 574/1000 | Loss: 0.00005167
Iteration 575/1000 | Loss: 0.00004026
Iteration 576/1000 | Loss: 0.00005433
Iteration 577/1000 | Loss: 0.00003456
Iteration 578/1000 | Loss: 0.00004238
Iteration 579/1000 | Loss: 0.00003653
Iteration 580/1000 | Loss: 0.00004674
Iteration 581/1000 | Loss: 0.00004198
Iteration 582/1000 | Loss: 0.00005380
Iteration 583/1000 | Loss: 0.00004050
Iteration 584/1000 | Loss: 0.00003147
Iteration 585/1000 | Loss: 0.00002663
Iteration 586/1000 | Loss: 0.00002719
Iteration 587/1000 | Loss: 0.00004248
Iteration 588/1000 | Loss: 0.00003971
Iteration 589/1000 | Loss: 0.00005082
Iteration 590/1000 | Loss: 0.00004252
Iteration 591/1000 | Loss: 0.00005069
Iteration 592/1000 | Loss: 0.00003806
Iteration 593/1000 | Loss: 0.00005464
Iteration 594/1000 | Loss: 0.00004340
Iteration 595/1000 | Loss: 0.00005321
Iteration 596/1000 | Loss: 0.00003428
Iteration 597/1000 | Loss: 0.00005394
Iteration 598/1000 | Loss: 0.00006266
Iteration 599/1000 | Loss: 0.00005122
Iteration 600/1000 | Loss: 0.00005704
Iteration 601/1000 | Loss: 0.00005730
Iteration 602/1000 | Loss: 0.00004966
Iteration 603/1000 | Loss: 0.00006545
Iteration 604/1000 | Loss: 0.00003899
Iteration 605/1000 | Loss: 0.00006076
Iteration 606/1000 | Loss: 0.00004631
Iteration 607/1000 | Loss: 0.00006222
Iteration 608/1000 | Loss: 0.00004769
Iteration 609/1000 | Loss: 0.00004938
Iteration 610/1000 | Loss: 0.00004512
Iteration 611/1000 | Loss: 0.00004513
Iteration 612/1000 | Loss: 0.00004531
Iteration 613/1000 | Loss: 0.00003131
Iteration 614/1000 | Loss: 0.00003323
Iteration 615/1000 | Loss: 0.00002931
Iteration 616/1000 | Loss: 0.00003416
Iteration 617/1000 | Loss: 0.00003481
Iteration 618/1000 | Loss: 0.00004548
Iteration 619/1000 | Loss: 0.00003947
Iteration 620/1000 | Loss: 0.00004029
Iteration 621/1000 | Loss: 0.00004068
Iteration 622/1000 | Loss: 0.00004850
Iteration 623/1000 | Loss: 0.00004254
Iteration 624/1000 | Loss: 0.00003439
Iteration 625/1000 | Loss: 0.00002977
Iteration 626/1000 | Loss: 0.00004163
Iteration 627/1000 | Loss: 0.00004446
Iteration 628/1000 | Loss: 0.00003432
Iteration 629/1000 | Loss: 0.00003978
Iteration 630/1000 | Loss: 0.00004776
Iteration 631/1000 | Loss: 0.00004705
Iteration 632/1000 | Loss: 0.00005513
Iteration 633/1000 | Loss: 0.00004868
Iteration 634/1000 | Loss: 0.00005565
Iteration 635/1000 | Loss: 0.00004011
Iteration 636/1000 | Loss: 0.00005841
Iteration 637/1000 | Loss: 0.00003659
Iteration 638/1000 | Loss: 0.00003782
Iteration 639/1000 | Loss: 0.00003821
Iteration 640/1000 | Loss: 0.00005845
Iteration 641/1000 | Loss: 0.00004733
Iteration 642/1000 | Loss: 0.00004575
Iteration 643/1000 | Loss: 0.00004496
Iteration 644/1000 | Loss: 0.00005017
Iteration 645/1000 | Loss: 0.00004164
Iteration 646/1000 | Loss: 0.00007168
Iteration 647/1000 | Loss: 0.00004504
Iteration 648/1000 | Loss: 0.00004700
Iteration 649/1000 | Loss: 0.00004405
Iteration 650/1000 | Loss: 0.00004961
Iteration 651/1000 | Loss: 0.00004555
Iteration 652/1000 | Loss: 0.00005789
Iteration 653/1000 | Loss: 0.00004422
Iteration 654/1000 | Loss: 0.00006003
Iteration 655/1000 | Loss: 0.00005505
Iteration 656/1000 | Loss: 0.00004322
Iteration 657/1000 | Loss: 0.00004448
Iteration 658/1000 | Loss: 0.00008065
Iteration 659/1000 | Loss: 0.00004061
Iteration 660/1000 | Loss: 0.00003820
Iteration 661/1000 | Loss: 0.00004364
Iteration 662/1000 | Loss: 0.00004375
Iteration 663/1000 | Loss: 0.00003958
Iteration 664/1000 | Loss: 0.00005977
Iteration 665/1000 | Loss: 0.00004631
Iteration 666/1000 | Loss: 0.00006637
Iteration 667/1000 | Loss: 0.00004217
Iteration 668/1000 | Loss: 0.00004910
Iteration 669/1000 | Loss: 0.00003603
Iteration 670/1000 | Loss: 0.00005634
Iteration 671/1000 | Loss: 0.00004140
Iteration 672/1000 | Loss: 0.00006174
Iteration 673/1000 | Loss: 0.00003828
Iteration 674/1000 | Loss: 0.00005871
Iteration 675/1000 | Loss: 0.00004323
Iteration 676/1000 | Loss: 0.00003264
Iteration 677/1000 | Loss: 0.00003021
Iteration 678/1000 | Loss: 0.00005652
Iteration 679/1000 | Loss: 0.00004925
Iteration 680/1000 | Loss: 0.00006535
Iteration 681/1000 | Loss: 0.00005195
Iteration 682/1000 | Loss: 0.00007079
Iteration 683/1000 | Loss: 0.00004266
Iteration 684/1000 | Loss: 0.00007224
Iteration 685/1000 | Loss: 0.00004817
Iteration 686/1000 | Loss: 0.00007264
Iteration 687/1000 | Loss: 0.00003514
Iteration 688/1000 | Loss: 0.00003379
Iteration 689/1000 | Loss: 0.00002825
Iteration 690/1000 | Loss: 0.00002493
Iteration 691/1000 | Loss: 0.00002425
Iteration 692/1000 | Loss: 0.00002506
Iteration 693/1000 | Loss: 0.00002809
Iteration 694/1000 | Loss: 0.00002757
Iteration 695/1000 | Loss: 0.00002602
Iteration 696/1000 | Loss: 0.00002993
Iteration 697/1000 | Loss: 0.00003254
Iteration 698/1000 | Loss: 0.00003931
Iteration 699/1000 | Loss: 0.00003722
Iteration 700/1000 | Loss: 0.00004836
Iteration 701/1000 | Loss: 0.00004530
Iteration 702/1000 | Loss: 0.00005537
Iteration 703/1000 | Loss: 0.00005202
Iteration 704/1000 | Loss: 0.00006425
Iteration 705/1000 | Loss: 0.00005671
Iteration 706/1000 | Loss: 0.00006687
Iteration 707/1000 | Loss: 0.00006016
Iteration 708/1000 | Loss: 0.00004393
Iteration 709/1000 | Loss: 0.00004592
Iteration 710/1000 | Loss: 0.00005419
Iteration 711/1000 | Loss: 0.00005254
Iteration 712/1000 | Loss: 0.00005638
Iteration 713/1000 | Loss: 0.00005771
Iteration 714/1000 | Loss: 0.00006211
Iteration 715/1000 | Loss: 0.00006190
Iteration 716/1000 | Loss: 0.00007657
Iteration 717/1000 | Loss: 0.00005332
Iteration 718/1000 | Loss: 0.00006208
Iteration 719/1000 | Loss: 0.00006132
Iteration 720/1000 | Loss: 0.00006736
Iteration 721/1000 | Loss: 0.00005503
Iteration 722/1000 | Loss: 0.00004679
Iteration 723/1000 | Loss: 0.00005796
Iteration 724/1000 | Loss: 0.00007718
Iteration 725/1000 | Loss: 0.00006019
Iteration 726/1000 | Loss: 0.00008262
Iteration 727/1000 | Loss: 0.00006535
Iteration 728/1000 | Loss: 0.00008378
Iteration 729/1000 | Loss: 0.00006460
Iteration 730/1000 | Loss: 0.00009202
Iteration 731/1000 | Loss: 0.00006768
Iteration 732/1000 | Loss: 0.00009377
Iteration 733/1000 | Loss: 0.00006433
Iteration 734/1000 | Loss: 0.00005996
Iteration 735/1000 | Loss: 0.00004843
Iteration 736/1000 | Loss: 0.00006378
Iteration 737/1000 | Loss: 0.00005642
Iteration 738/1000 | Loss: 0.00007326
Iteration 739/1000 | Loss: 0.00007711
Iteration 740/1000 | Loss: 0.00007452
Iteration 741/1000 | Loss: 0.00006504
Iteration 742/1000 | Loss: 0.00007798
Iteration 743/1000 | Loss: 0.00006407
Iteration 744/1000 | Loss: 0.00007437
Iteration 745/1000 | Loss: 0.00007036
Iteration 746/1000 | Loss: 0.00010988
Iteration 747/1000 | Loss: 0.00007077
Iteration 748/1000 | Loss: 0.00005377
Iteration 749/1000 | Loss: 0.00006274
Iteration 750/1000 | Loss: 0.00007482
Iteration 751/1000 | Loss: 0.00005980
Iteration 752/1000 | Loss: 0.00008669
Iteration 753/1000 | Loss: 0.00005145
Iteration 754/1000 | Loss: 0.00003585
Iteration 755/1000 | Loss: 0.00004033
Iteration 756/1000 | Loss: 0.00006680
Iteration 757/1000 | Loss: 0.00005725
Iteration 758/1000 | Loss: 0.00006061
Iteration 759/1000 | Loss: 0.00005303
Iteration 760/1000 | Loss: 0.00008030
Iteration 761/1000 | Loss: 0.00007061
Iteration 762/1000 | Loss: 0.00006061
Iteration 763/1000 | Loss: 0.00005388
Iteration 764/1000 | Loss: 0.00004140
Iteration 765/1000 | Loss: 0.00004212
Iteration 766/1000 | Loss: 0.00006139
Iteration 767/1000 | Loss: 0.00005857
Iteration 768/1000 | Loss: 0.00005331
Iteration 769/1000 | Loss: 0.00004647
Iteration 770/1000 | Loss: 0.00005670
Iteration 771/1000 | Loss: 0.00004367
Iteration 772/1000 | Loss: 0.00003544
Iteration 773/1000 | Loss: 0.00002752
Iteration 774/1000 | Loss: 0.00003242
Iteration 775/1000 | Loss: 0.00003674
Iteration 776/1000 | Loss: 0.00003468
Iteration 777/1000 | Loss: 0.00003354
Iteration 778/1000 | Loss: 0.00003634
Iteration 779/1000 | Loss: 0.00003416
Iteration 780/1000 | Loss: 0.00003848
Iteration 781/1000 | Loss: 0.00004337
Iteration 782/1000 | Loss: 0.00006557
Iteration 783/1000 | Loss: 0.00004616
Iteration 784/1000 | Loss: 0.00004036
Iteration 785/1000 | Loss: 0.00004256
Iteration 786/1000 | Loss: 0.00004426
Iteration 787/1000 | Loss: 0.00004968
Iteration 788/1000 | Loss: 0.00007166
Iteration 789/1000 | Loss: 0.00004399
Iteration 790/1000 | Loss: 0.00005676
Iteration 791/1000 | Loss: 0.00005016
Iteration 792/1000 | Loss: 0.00006446
Iteration 793/1000 | Loss: 0.00005899
Iteration 794/1000 | Loss: 0.00006334
Iteration 795/1000 | Loss: 0.00005731
Iteration 796/1000 | Loss: 0.00008101
Iteration 797/1000 | Loss: 0.00003691
Iteration 798/1000 | Loss: 0.00003813
Iteration 799/1000 | Loss: 0.00004345
Iteration 800/1000 | Loss: 0.00005874
Iteration 801/1000 | Loss: 0.00005680
Iteration 802/1000 | Loss: 0.00004721
Iteration 803/1000 | Loss: 0.00004506
Iteration 804/1000 | Loss: 0.00005996
Iteration 805/1000 | Loss: 0.00004809
Iteration 806/1000 | Loss: 0.00003698
Iteration 807/1000 | Loss: 0.00003145
Iteration 808/1000 | Loss: 0.00002630
Iteration 809/1000 | Loss: 0.00002490
Iteration 810/1000 | Loss: 0.00002405
Iteration 811/1000 | Loss: 0.00002796
Iteration 812/1000 | Loss: 0.00003143
Iteration 813/1000 | Loss: 0.00003549
Iteration 814/1000 | Loss: 0.00002887
Iteration 815/1000 | Loss: 0.00002569
Iteration 816/1000 | Loss: 0.00002461
Iteration 817/1000 | Loss: 0.00002914
Iteration 818/1000 | Loss: 0.00002726
Iteration 819/1000 | Loss: 0.00002766
Iteration 820/1000 | Loss: 0.00003201
Iteration 821/1000 | Loss: 0.00003192
Iteration 822/1000 | Loss: 0.00002640
Iteration 823/1000 | Loss: 0.00002671
Iteration 824/1000 | Loss: 0.00003010
Iteration 825/1000 | Loss: 0.00003384
Iteration 826/1000 | Loss: 0.00003813
Iteration 827/1000 | Loss: 0.00003992
Iteration 828/1000 | Loss: 0.00004288
Iteration 829/1000 | Loss: 0.00003686
Iteration 830/1000 | Loss: 0.00004115
Iteration 831/1000 | Loss: 0.00004936
Iteration 832/1000 | Loss: 0.00005251
Iteration 833/1000 | Loss: 0.00007696
Iteration 834/1000 | Loss: 0.00004756
Iteration 835/1000 | Loss: 0.00004703
Iteration 836/1000 | Loss: 0.00005888
Iteration 837/1000 | Loss: 0.00004751
Iteration 838/1000 | Loss: 0.00005463
Iteration 839/1000 | Loss: 0.00003832
Iteration 840/1000 | Loss: 0.00004095
Iteration 841/1000 | Loss: 0.00004002
Iteration 842/1000 | Loss: 0.00004500
Iteration 843/1000 | Loss: 0.00004523
Iteration 844/1000 | Loss: 0.00006018
Iteration 845/1000 | Loss: 0.00004406
Iteration 846/1000 | Loss: 0.00003316
Iteration 847/1000 | Loss: 0.00003804
Iteration 848/1000 | Loss: 0.00003437
Iteration 849/1000 | Loss: 0.00002610
Iteration 850/1000 | Loss: 0.00002828
Iteration 851/1000 | Loss: 0.00002731
Iteration 852/1000 | Loss: 0.00002676
Iteration 853/1000 | Loss: 0.00002907
Iteration 854/1000 | Loss: 0.00003115
Iteration 855/1000 | Loss: 0.00003429
Iteration 856/1000 | Loss: 0.00004379
Iteration 857/1000 | Loss: 0.00003672
Iteration 858/1000 | Loss: 0.00004407
Iteration 859/1000 | Loss: 0.00003478
Iteration 860/1000 | Loss: 0.00004580
Iteration 861/1000 | Loss: 0.00004191
Iteration 862/1000 | Loss: 0.00004011
Iteration 863/1000 | Loss: 0.00002824
Iteration 864/1000 | Loss: 0.00002709
Iteration 865/1000 | Loss: 0.00003294
Iteration 866/1000 | Loss: 0.00003998
Iteration 867/1000 | Loss: 0.00003815
Iteration 868/1000 | Loss: 0.00004488
Iteration 869/1000 | Loss: 0.00004665
Iteration 870/1000 | Loss: 0.00005442
Iteration 871/1000 | Loss: 0.00005149
Iteration 872/1000 | Loss: 0.00005436
Iteration 873/1000 | Loss: 0.00004043
Iteration 874/1000 | Loss: 0.00003140
Iteration 875/1000 | Loss: 0.00002534
Iteration 876/1000 | Loss: 0.00003061
Iteration 877/1000 | Loss: 0.00004146
Iteration 878/1000 | Loss: 0.00005376
Iteration 879/1000 | Loss: 0.00005148
Iteration 880/1000 | Loss: 0.00003869
Iteration 881/1000 | Loss: 0.00004458
Iteration 882/1000 | Loss: 0.00004573
Iteration 883/1000 | Loss: 0.00005151
Iteration 884/1000 | Loss: 0.00005406
Iteration 885/1000 | Loss: 0.00003776
Iteration 886/1000 | Loss: 0.00003930
Iteration 887/1000 | Loss: 0.00005700
Iteration 888/1000 | Loss: 0.00005638
Iteration 889/1000 | Loss: 0.00005677
Iteration 890/1000 | Loss: 0.00005457
Iteration 891/1000 | Loss: 0.00005318
Iteration 892/1000 | Loss: 0.00005107
Iteration 893/1000 | Loss: 0.00006165
Iteration 894/1000 | Loss: 0.00004904
Iteration 895/1000 | Loss: 0.00005207
Iteration 896/1000 | Loss: 0.00003979
Iteration 897/1000 | Loss: 0.00003438
Iteration 898/1000 | Loss: 0.00003422
Iteration 899/1000 | Loss: 0.00002908
Iteration 900/1000 | Loss: 0.00002681
Iteration 901/1000 | Loss: 0.00004649
Iteration 902/1000 | Loss: 0.00004122
Iteration 903/1000 | Loss: 0.00003440
Iteration 904/1000 | Loss: 0.00003661
Iteration 905/1000 | Loss: 0.00004099
Iteration 906/1000 | Loss: 0.00004332
Iteration 907/1000 | Loss: 0.00004533
Iteration 908/1000 | Loss: 0.00005179
Iteration 909/1000 | Loss: 0.00004881
Iteration 910/1000 | Loss: 0.00004776
Iteration 911/1000 | Loss: 0.00004896
Iteration 912/1000 | Loss: 0.00006000
Iteration 913/1000 | Loss: 0.00004546
Iteration 914/1000 | Loss: 0.00004391
Iteration 915/1000 | Loss: 0.00004599
Iteration 916/1000 | Loss: 0.00006924
Iteration 917/1000 | Loss: 0.00004315
Iteration 918/1000 | Loss: 0.00004543
Iteration 919/1000 | Loss: 0.00004315
Iteration 920/1000 | Loss: 0.00005790
Iteration 921/1000 | Loss: 0.00006040
Iteration 922/1000 | Loss: 0.00004008
Iteration 923/1000 | Loss: 0.00003557
Iteration 924/1000 | Loss: 0.00004817
Iteration 925/1000 | Loss: 0.00005144
Iteration 926/1000 | Loss: 0.00004992
Iteration 927/1000 | Loss: 0.00006225
Iteration 928/1000 | Loss: 0.00005033
Iteration 929/1000 | Loss: 0.00004245
Iteration 930/1000 | Loss: 0.00005568
Iteration 931/1000 | Loss: 0.00003955
Iteration 932/1000 | Loss: 0.00005491
Iteration 933/1000 | Loss: 0.00003622
Iteration 934/1000 | Loss: 0.00007092
Iteration 935/1000 | Loss: 0.00004620
Iteration 936/1000 | Loss: 0.00006187
Iteration 937/1000 | Loss: 0.00005504
Iteration 938/1000 | Loss: 0.00004805
Iteration 939/1000 | Loss: 0.00004899
Iteration 940/1000 | Loss: 0.00006319
Iteration 941/1000 | Loss: 0.00004316
Iteration 942/1000 | Loss: 0.00003356
Iteration 943/1000 | Loss: 0.00003920
Iteration 944/1000 | Loss: 0.00003695
Iteration 945/1000 | Loss: 0.00003302
Iteration 946/1000 | Loss: 0.00005157
Iteration 947/1000 | Loss: 0.00005550
Iteration 948/1000 | Loss: 0.00005411
Iteration 949/1000 | Loss: 0.00004144
Iteration 950/1000 | Loss: 0.00005128
Iteration 951/1000 | Loss: 0.00005771
Iteration 952/1000 | Loss: 0.00007003
Iteration 953/1000 | Loss: 0.00003621
Iteration 954/1000 | Loss: 0.00004967
Iteration 955/1000 | Loss: 0.00004622
Iteration 956/1000 | Loss: 0.00007464
Iteration 957/1000 | Loss: 0.00004183
Iteration 958/1000 | Loss: 0.00004969
Iteration 959/1000 | Loss: 0.00004062
Iteration 960/1000 | Loss: 0.00007426
Iteration 961/1000 | Loss: 0.00004811
Iteration 962/1000 | Loss: 0.00005781
Iteration 963/1000 | Loss: 0.00005541
Iteration 964/1000 | Loss: 0.00007265
Iteration 965/1000 | Loss: 0.00007560
Iteration 966/1000 | Loss: 0.00007004
Iteration 967/1000 | Loss: 0.00004783
Iteration 968/1000 | Loss: 0.00005264
Iteration 969/1000 | Loss: 0.00006273
Iteration 970/1000 | Loss: 0.00005757
Iteration 971/1000 | Loss: 0.00005863
Iteration 972/1000 | Loss: 0.00006597
Iteration 973/1000 | Loss: 0.00006659
Iteration 974/1000 | Loss: 0.00007058
Iteration 975/1000 | Loss: 0.00008315
Iteration 976/1000 | Loss: 0.00005905
Iteration 977/1000 | Loss: 0.00007259
Iteration 978/1000 | Loss: 0.00006889
Iteration 979/1000 | Loss: 0.00007664
Iteration 980/1000 | Loss: 0.00006359
Iteration 981/1000 | Loss: 0.00008408
Iteration 982/1000 | Loss: 0.00007063
Iteration 983/1000 | Loss: 0.00008108
Iteration 984/1000 | Loss: 0.00006862
Iteration 985/1000 | Loss: 0.00008451
Iteration 986/1000 | Loss: 0.00007647
Iteration 987/1000 | Loss: 0.00008245
Iteration 988/1000 | Loss: 0.00009879
Iteration 989/1000 | Loss: 0.00006799
Iteration 990/1000 | Loss: 0.00009943
Iteration 991/1000 | Loss: 0.00004253
Iteration 992/1000 | Loss: 0.00004851
Iteration 993/1000 | Loss: 0.00004556
Iteration 994/1000 | Loss: 0.00006231
Iteration 995/1000 | Loss: 0.00003745
Iteration 996/1000 | Loss: 0.00006279
Iteration 997/1000 | Loss: 0.00004227
Iteration 998/1000 | Loss: 0.00005305
Iteration 999/1000 | Loss: 0.00002918
Iteration 1000/1000 | Loss: 0.00002506

Optimization complete. Final v2v error: 3.883605718612671 mm

Highest mean error: 5.710231304168701 mm for frame 127

Lowest mean error: 2.8983919620513916 mm for frame 30

Saving results

Total time: 1442.5516002178192
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00415044
Iteration 2/25 | Loss: 0.00132614
Iteration 3/25 | Loss: 0.00125639
Iteration 4/25 | Loss: 0.00124586
Iteration 5/25 | Loss: 0.00124161
Iteration 6/25 | Loss: 0.00124111
Iteration 7/25 | Loss: 0.00124111
Iteration 8/25 | Loss: 0.00124111
Iteration 9/25 | Loss: 0.00124111
Iteration 10/25 | Loss: 0.00124111
Iteration 11/25 | Loss: 0.00124111
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012411077041178942, 0.0012411077041178942, 0.0012411077041178942, 0.0012411077041178942, 0.0012411077041178942]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012411077041178942

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.90334749
Iteration 2/25 | Loss: 0.00087964
Iteration 3/25 | Loss: 0.00087964
Iteration 4/25 | Loss: 0.00087964
Iteration 5/25 | Loss: 0.00087964
Iteration 6/25 | Loss: 0.00087964
Iteration 7/25 | Loss: 0.00087964
Iteration 8/25 | Loss: 0.00087964
Iteration 9/25 | Loss: 0.00087964
Iteration 10/25 | Loss: 0.00087964
Iteration 11/25 | Loss: 0.00087964
Iteration 12/25 | Loss: 0.00087964
Iteration 13/25 | Loss: 0.00087964
Iteration 14/25 | Loss: 0.00087964
Iteration 15/25 | Loss: 0.00087964
Iteration 16/25 | Loss: 0.00087964
Iteration 17/25 | Loss: 0.00087964
Iteration 18/25 | Loss: 0.00087964
Iteration 19/25 | Loss: 0.00087964
Iteration 20/25 | Loss: 0.00087964
Iteration 21/25 | Loss: 0.00087964
Iteration 22/25 | Loss: 0.00087964
Iteration 23/25 | Loss: 0.00087964
Iteration 24/25 | Loss: 0.00087964
Iteration 25/25 | Loss: 0.00087964

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087964
Iteration 2/1000 | Loss: 0.00002247
Iteration 3/1000 | Loss: 0.00001576
Iteration 4/1000 | Loss: 0.00001422
Iteration 5/1000 | Loss: 0.00001359
Iteration 6/1000 | Loss: 0.00001300
Iteration 7/1000 | Loss: 0.00001266
Iteration 8/1000 | Loss: 0.00001253
Iteration 9/1000 | Loss: 0.00001228
Iteration 10/1000 | Loss: 0.00001204
Iteration 11/1000 | Loss: 0.00001197
Iteration 12/1000 | Loss: 0.00001196
Iteration 13/1000 | Loss: 0.00001195
Iteration 14/1000 | Loss: 0.00001193
Iteration 15/1000 | Loss: 0.00001192
Iteration 16/1000 | Loss: 0.00001188
Iteration 17/1000 | Loss: 0.00001185
Iteration 18/1000 | Loss: 0.00001177
Iteration 19/1000 | Loss: 0.00001174
Iteration 20/1000 | Loss: 0.00001171
Iteration 21/1000 | Loss: 0.00001170
Iteration 22/1000 | Loss: 0.00001169
Iteration 23/1000 | Loss: 0.00001164
Iteration 24/1000 | Loss: 0.00001164
Iteration 25/1000 | Loss: 0.00001161
Iteration 26/1000 | Loss: 0.00001160
Iteration 27/1000 | Loss: 0.00001158
Iteration 28/1000 | Loss: 0.00001156
Iteration 29/1000 | Loss: 0.00001156
Iteration 30/1000 | Loss: 0.00001156
Iteration 31/1000 | Loss: 0.00001156
Iteration 32/1000 | Loss: 0.00001155
Iteration 33/1000 | Loss: 0.00001155
Iteration 34/1000 | Loss: 0.00001155
Iteration 35/1000 | Loss: 0.00001154
Iteration 36/1000 | Loss: 0.00001154
Iteration 37/1000 | Loss: 0.00001153
Iteration 38/1000 | Loss: 0.00001152
Iteration 39/1000 | Loss: 0.00001151
Iteration 40/1000 | Loss: 0.00001151
Iteration 41/1000 | Loss: 0.00001151
Iteration 42/1000 | Loss: 0.00001151
Iteration 43/1000 | Loss: 0.00001150
Iteration 44/1000 | Loss: 0.00001150
Iteration 45/1000 | Loss: 0.00001150
Iteration 46/1000 | Loss: 0.00001148
Iteration 47/1000 | Loss: 0.00001148
Iteration 48/1000 | Loss: 0.00001146
Iteration 49/1000 | Loss: 0.00001145
Iteration 50/1000 | Loss: 0.00001145
Iteration 51/1000 | Loss: 0.00001144
Iteration 52/1000 | Loss: 0.00001141
Iteration 53/1000 | Loss: 0.00001140
Iteration 54/1000 | Loss: 0.00001139
Iteration 55/1000 | Loss: 0.00001138
Iteration 56/1000 | Loss: 0.00001138
Iteration 57/1000 | Loss: 0.00001138
Iteration 58/1000 | Loss: 0.00001138
Iteration 59/1000 | Loss: 0.00001138
Iteration 60/1000 | Loss: 0.00001137
Iteration 61/1000 | Loss: 0.00001136
Iteration 62/1000 | Loss: 0.00001136
Iteration 63/1000 | Loss: 0.00001135
Iteration 64/1000 | Loss: 0.00001135
Iteration 65/1000 | Loss: 0.00001134
Iteration 66/1000 | Loss: 0.00001134
Iteration 67/1000 | Loss: 0.00001133
Iteration 68/1000 | Loss: 0.00001133
Iteration 69/1000 | Loss: 0.00001132
Iteration 70/1000 | Loss: 0.00001132
Iteration 71/1000 | Loss: 0.00001132
Iteration 72/1000 | Loss: 0.00001131
Iteration 73/1000 | Loss: 0.00001131
Iteration 74/1000 | Loss: 0.00001131
Iteration 75/1000 | Loss: 0.00001130
Iteration 76/1000 | Loss: 0.00001130
Iteration 77/1000 | Loss: 0.00001130
Iteration 78/1000 | Loss: 0.00001130
Iteration 79/1000 | Loss: 0.00001129
Iteration 80/1000 | Loss: 0.00001129
Iteration 81/1000 | Loss: 0.00001129
Iteration 82/1000 | Loss: 0.00001128
Iteration 83/1000 | Loss: 0.00001128
Iteration 84/1000 | Loss: 0.00001128
Iteration 85/1000 | Loss: 0.00001127
Iteration 86/1000 | Loss: 0.00001127
Iteration 87/1000 | Loss: 0.00001126
Iteration 88/1000 | Loss: 0.00001126
Iteration 89/1000 | Loss: 0.00001126
Iteration 90/1000 | Loss: 0.00001126
Iteration 91/1000 | Loss: 0.00001125
Iteration 92/1000 | Loss: 0.00001125
Iteration 93/1000 | Loss: 0.00001125
Iteration 94/1000 | Loss: 0.00001125
Iteration 95/1000 | Loss: 0.00001124
Iteration 96/1000 | Loss: 0.00001122
Iteration 97/1000 | Loss: 0.00001121
Iteration 98/1000 | Loss: 0.00001121
Iteration 99/1000 | Loss: 0.00001119
Iteration 100/1000 | Loss: 0.00001119
Iteration 101/1000 | Loss: 0.00001118
Iteration 102/1000 | Loss: 0.00001118
Iteration 103/1000 | Loss: 0.00001118
Iteration 104/1000 | Loss: 0.00001118
Iteration 105/1000 | Loss: 0.00001118
Iteration 106/1000 | Loss: 0.00001117
Iteration 107/1000 | Loss: 0.00001117
Iteration 108/1000 | Loss: 0.00001117
Iteration 109/1000 | Loss: 0.00001116
Iteration 110/1000 | Loss: 0.00001116
Iteration 111/1000 | Loss: 0.00001116
Iteration 112/1000 | Loss: 0.00001116
Iteration 113/1000 | Loss: 0.00001116
Iteration 114/1000 | Loss: 0.00001115
Iteration 115/1000 | Loss: 0.00001115
Iteration 116/1000 | Loss: 0.00001115
Iteration 117/1000 | Loss: 0.00001114
Iteration 118/1000 | Loss: 0.00001114
Iteration 119/1000 | Loss: 0.00001114
Iteration 120/1000 | Loss: 0.00001114
Iteration 121/1000 | Loss: 0.00001113
Iteration 122/1000 | Loss: 0.00001113
Iteration 123/1000 | Loss: 0.00001113
Iteration 124/1000 | Loss: 0.00001113
Iteration 125/1000 | Loss: 0.00001113
Iteration 126/1000 | Loss: 0.00001113
Iteration 127/1000 | Loss: 0.00001113
Iteration 128/1000 | Loss: 0.00001113
Iteration 129/1000 | Loss: 0.00001112
Iteration 130/1000 | Loss: 0.00001112
Iteration 131/1000 | Loss: 0.00001112
Iteration 132/1000 | Loss: 0.00001112
Iteration 133/1000 | Loss: 0.00001112
Iteration 134/1000 | Loss: 0.00001112
Iteration 135/1000 | Loss: 0.00001112
Iteration 136/1000 | Loss: 0.00001112
Iteration 137/1000 | Loss: 0.00001112
Iteration 138/1000 | Loss: 0.00001112
Iteration 139/1000 | Loss: 0.00001112
Iteration 140/1000 | Loss: 0.00001112
Iteration 141/1000 | Loss: 0.00001112
Iteration 142/1000 | Loss: 0.00001112
Iteration 143/1000 | Loss: 0.00001112
Iteration 144/1000 | Loss: 0.00001112
Iteration 145/1000 | Loss: 0.00001112
Iteration 146/1000 | Loss: 0.00001112
Iteration 147/1000 | Loss: 0.00001112
Iteration 148/1000 | Loss: 0.00001112
Iteration 149/1000 | Loss: 0.00001112
Iteration 150/1000 | Loss: 0.00001112
Iteration 151/1000 | Loss: 0.00001112
Iteration 152/1000 | Loss: 0.00001112
Iteration 153/1000 | Loss: 0.00001112
Iteration 154/1000 | Loss: 0.00001112
Iteration 155/1000 | Loss: 0.00001112
Iteration 156/1000 | Loss: 0.00001112
Iteration 157/1000 | Loss: 0.00001112
Iteration 158/1000 | Loss: 0.00001112
Iteration 159/1000 | Loss: 0.00001112
Iteration 160/1000 | Loss: 0.00001112
Iteration 161/1000 | Loss: 0.00001112
Iteration 162/1000 | Loss: 0.00001112
Iteration 163/1000 | Loss: 0.00001112
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.1120979252154939e-05, 1.1120979252154939e-05, 1.1120979252154939e-05, 1.1120979252154939e-05, 1.1120979252154939e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1120979252154939e-05

Optimization complete. Final v2v error: 2.8752076625823975 mm

Highest mean error: 3.2430150508880615 mm for frame 121

Lowest mean error: 2.6775355339050293 mm for frame 155

Saving results

Total time: 40.18546485900879
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00401427
Iteration 2/25 | Loss: 0.00135229
Iteration 3/25 | Loss: 0.00126879
Iteration 4/25 | Loss: 0.00125779
Iteration 5/25 | Loss: 0.00125437
Iteration 6/25 | Loss: 0.00125346
Iteration 7/25 | Loss: 0.00125346
Iteration 8/25 | Loss: 0.00125346
Iteration 9/25 | Loss: 0.00125346
Iteration 10/25 | Loss: 0.00125346
Iteration 11/25 | Loss: 0.00125346
Iteration 12/25 | Loss: 0.00125346
Iteration 13/25 | Loss: 0.00125346
Iteration 14/25 | Loss: 0.00125346
Iteration 15/25 | Loss: 0.00125346
Iteration 16/25 | Loss: 0.00125346
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012534577399492264, 0.0012534577399492264, 0.0012534577399492264, 0.0012534577399492264, 0.0012534577399492264]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012534577399492264

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42777872
Iteration 2/25 | Loss: 0.00086357
Iteration 3/25 | Loss: 0.00086357
Iteration 4/25 | Loss: 0.00086357
Iteration 5/25 | Loss: 0.00086357
Iteration 6/25 | Loss: 0.00086357
Iteration 7/25 | Loss: 0.00086357
Iteration 8/25 | Loss: 0.00086357
Iteration 9/25 | Loss: 0.00086357
Iteration 10/25 | Loss: 0.00086357
Iteration 11/25 | Loss: 0.00086357
Iteration 12/25 | Loss: 0.00086357
Iteration 13/25 | Loss: 0.00086357
Iteration 14/25 | Loss: 0.00086357
Iteration 15/25 | Loss: 0.00086357
Iteration 16/25 | Loss: 0.00086357
Iteration 17/25 | Loss: 0.00086357
Iteration 18/25 | Loss: 0.00086357
Iteration 19/25 | Loss: 0.00086357
Iteration 20/25 | Loss: 0.00086357
Iteration 21/25 | Loss: 0.00086357
Iteration 22/25 | Loss: 0.00086357
Iteration 23/25 | Loss: 0.00086357
Iteration 24/25 | Loss: 0.00086357
Iteration 25/25 | Loss: 0.00086357

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086357
Iteration 2/1000 | Loss: 0.00003315
Iteration 3/1000 | Loss: 0.00002136
Iteration 4/1000 | Loss: 0.00001758
Iteration 5/1000 | Loss: 0.00001616
Iteration 6/1000 | Loss: 0.00001519
Iteration 7/1000 | Loss: 0.00001455
Iteration 8/1000 | Loss: 0.00001411
Iteration 9/1000 | Loss: 0.00001391
Iteration 10/1000 | Loss: 0.00001367
Iteration 11/1000 | Loss: 0.00001359
Iteration 12/1000 | Loss: 0.00001356
Iteration 13/1000 | Loss: 0.00001352
Iteration 14/1000 | Loss: 0.00001351
Iteration 15/1000 | Loss: 0.00001351
Iteration 16/1000 | Loss: 0.00001350
Iteration 17/1000 | Loss: 0.00001350
Iteration 18/1000 | Loss: 0.00001349
Iteration 19/1000 | Loss: 0.00001349
Iteration 20/1000 | Loss: 0.00001341
Iteration 21/1000 | Loss: 0.00001340
Iteration 22/1000 | Loss: 0.00001339
Iteration 23/1000 | Loss: 0.00001339
Iteration 24/1000 | Loss: 0.00001338
Iteration 25/1000 | Loss: 0.00001337
Iteration 26/1000 | Loss: 0.00001336
Iteration 27/1000 | Loss: 0.00001336
Iteration 28/1000 | Loss: 0.00001336
Iteration 29/1000 | Loss: 0.00001336
Iteration 30/1000 | Loss: 0.00001336
Iteration 31/1000 | Loss: 0.00001336
Iteration 32/1000 | Loss: 0.00001336
Iteration 33/1000 | Loss: 0.00001335
Iteration 34/1000 | Loss: 0.00001335
Iteration 35/1000 | Loss: 0.00001335
Iteration 36/1000 | Loss: 0.00001335
Iteration 37/1000 | Loss: 0.00001335
Iteration 38/1000 | Loss: 0.00001335
Iteration 39/1000 | Loss: 0.00001335
Iteration 40/1000 | Loss: 0.00001335
Iteration 41/1000 | Loss: 0.00001335
Iteration 42/1000 | Loss: 0.00001335
Iteration 43/1000 | Loss: 0.00001335
Iteration 44/1000 | Loss: 0.00001335
Iteration 45/1000 | Loss: 0.00001334
Iteration 46/1000 | Loss: 0.00001334
Iteration 47/1000 | Loss: 0.00001334
Iteration 48/1000 | Loss: 0.00001334
Iteration 49/1000 | Loss: 0.00001331
Iteration 50/1000 | Loss: 0.00001330
Iteration 51/1000 | Loss: 0.00001330
Iteration 52/1000 | Loss: 0.00001330
Iteration 53/1000 | Loss: 0.00001329
Iteration 54/1000 | Loss: 0.00001329
Iteration 55/1000 | Loss: 0.00001328
Iteration 56/1000 | Loss: 0.00001328
Iteration 57/1000 | Loss: 0.00001325
Iteration 58/1000 | Loss: 0.00001325
Iteration 59/1000 | Loss: 0.00001324
Iteration 60/1000 | Loss: 0.00001324
Iteration 61/1000 | Loss: 0.00001324
Iteration 62/1000 | Loss: 0.00001323
Iteration 63/1000 | Loss: 0.00001323
Iteration 64/1000 | Loss: 0.00001323
Iteration 65/1000 | Loss: 0.00001323
Iteration 66/1000 | Loss: 0.00001323
Iteration 67/1000 | Loss: 0.00001323
Iteration 68/1000 | Loss: 0.00001322
Iteration 69/1000 | Loss: 0.00001322
Iteration 70/1000 | Loss: 0.00001321
Iteration 71/1000 | Loss: 0.00001321
Iteration 72/1000 | Loss: 0.00001321
Iteration 73/1000 | Loss: 0.00001320
Iteration 74/1000 | Loss: 0.00001320
Iteration 75/1000 | Loss: 0.00001320
Iteration 76/1000 | Loss: 0.00001319
Iteration 77/1000 | Loss: 0.00001319
Iteration 78/1000 | Loss: 0.00001319
Iteration 79/1000 | Loss: 0.00001318
Iteration 80/1000 | Loss: 0.00001318
Iteration 81/1000 | Loss: 0.00001318
Iteration 82/1000 | Loss: 0.00001318
Iteration 83/1000 | Loss: 0.00001318
Iteration 84/1000 | Loss: 0.00001318
Iteration 85/1000 | Loss: 0.00001318
Iteration 86/1000 | Loss: 0.00001317
Iteration 87/1000 | Loss: 0.00001317
Iteration 88/1000 | Loss: 0.00001317
Iteration 89/1000 | Loss: 0.00001317
Iteration 90/1000 | Loss: 0.00001317
Iteration 91/1000 | Loss: 0.00001317
Iteration 92/1000 | Loss: 0.00001317
Iteration 93/1000 | Loss: 0.00001316
Iteration 94/1000 | Loss: 0.00001316
Iteration 95/1000 | Loss: 0.00001316
Iteration 96/1000 | Loss: 0.00001316
Iteration 97/1000 | Loss: 0.00001316
Iteration 98/1000 | Loss: 0.00001315
Iteration 99/1000 | Loss: 0.00001315
Iteration 100/1000 | Loss: 0.00001315
Iteration 101/1000 | Loss: 0.00001315
Iteration 102/1000 | Loss: 0.00001314
Iteration 103/1000 | Loss: 0.00001314
Iteration 104/1000 | Loss: 0.00001314
Iteration 105/1000 | Loss: 0.00001313
Iteration 106/1000 | Loss: 0.00001313
Iteration 107/1000 | Loss: 0.00001312
Iteration 108/1000 | Loss: 0.00001312
Iteration 109/1000 | Loss: 0.00001312
Iteration 110/1000 | Loss: 0.00001312
Iteration 111/1000 | Loss: 0.00001312
Iteration 112/1000 | Loss: 0.00001312
Iteration 113/1000 | Loss: 0.00001312
Iteration 114/1000 | Loss: 0.00001311
Iteration 115/1000 | Loss: 0.00001311
Iteration 116/1000 | Loss: 0.00001311
Iteration 117/1000 | Loss: 0.00001310
Iteration 118/1000 | Loss: 0.00001310
Iteration 119/1000 | Loss: 0.00001309
Iteration 120/1000 | Loss: 0.00001309
Iteration 121/1000 | Loss: 0.00001308
Iteration 122/1000 | Loss: 0.00001308
Iteration 123/1000 | Loss: 0.00001307
Iteration 124/1000 | Loss: 0.00001307
Iteration 125/1000 | Loss: 0.00001307
Iteration 126/1000 | Loss: 0.00001307
Iteration 127/1000 | Loss: 0.00001307
Iteration 128/1000 | Loss: 0.00001306
Iteration 129/1000 | Loss: 0.00001306
Iteration 130/1000 | Loss: 0.00001306
Iteration 131/1000 | Loss: 0.00001306
Iteration 132/1000 | Loss: 0.00001305
Iteration 133/1000 | Loss: 0.00001305
Iteration 134/1000 | Loss: 0.00001305
Iteration 135/1000 | Loss: 0.00001304
Iteration 136/1000 | Loss: 0.00001304
Iteration 137/1000 | Loss: 0.00001303
Iteration 138/1000 | Loss: 0.00001303
Iteration 139/1000 | Loss: 0.00001303
Iteration 140/1000 | Loss: 0.00001303
Iteration 141/1000 | Loss: 0.00001303
Iteration 142/1000 | Loss: 0.00001302
Iteration 143/1000 | Loss: 0.00001302
Iteration 144/1000 | Loss: 0.00001302
Iteration 145/1000 | Loss: 0.00001302
Iteration 146/1000 | Loss: 0.00001301
Iteration 147/1000 | Loss: 0.00001301
Iteration 148/1000 | Loss: 0.00001301
Iteration 149/1000 | Loss: 0.00001301
Iteration 150/1000 | Loss: 0.00001301
Iteration 151/1000 | Loss: 0.00001300
Iteration 152/1000 | Loss: 0.00001300
Iteration 153/1000 | Loss: 0.00001300
Iteration 154/1000 | Loss: 0.00001300
Iteration 155/1000 | Loss: 0.00001300
Iteration 156/1000 | Loss: 0.00001300
Iteration 157/1000 | Loss: 0.00001300
Iteration 158/1000 | Loss: 0.00001300
Iteration 159/1000 | Loss: 0.00001300
Iteration 160/1000 | Loss: 0.00001299
Iteration 161/1000 | Loss: 0.00001299
Iteration 162/1000 | Loss: 0.00001299
Iteration 163/1000 | Loss: 0.00001299
Iteration 164/1000 | Loss: 0.00001299
Iteration 165/1000 | Loss: 0.00001299
Iteration 166/1000 | Loss: 0.00001299
Iteration 167/1000 | Loss: 0.00001298
Iteration 168/1000 | Loss: 0.00001298
Iteration 169/1000 | Loss: 0.00001298
Iteration 170/1000 | Loss: 0.00001298
Iteration 171/1000 | Loss: 0.00001298
Iteration 172/1000 | Loss: 0.00001298
Iteration 173/1000 | Loss: 0.00001298
Iteration 174/1000 | Loss: 0.00001298
Iteration 175/1000 | Loss: 0.00001298
Iteration 176/1000 | Loss: 0.00001298
Iteration 177/1000 | Loss: 0.00001297
Iteration 178/1000 | Loss: 0.00001297
Iteration 179/1000 | Loss: 0.00001297
Iteration 180/1000 | Loss: 0.00001297
Iteration 181/1000 | Loss: 0.00001297
Iteration 182/1000 | Loss: 0.00001297
Iteration 183/1000 | Loss: 0.00001297
Iteration 184/1000 | Loss: 0.00001297
Iteration 185/1000 | Loss: 0.00001297
Iteration 186/1000 | Loss: 0.00001297
Iteration 187/1000 | Loss: 0.00001297
Iteration 188/1000 | Loss: 0.00001296
Iteration 189/1000 | Loss: 0.00001296
Iteration 190/1000 | Loss: 0.00001296
Iteration 191/1000 | Loss: 0.00001296
Iteration 192/1000 | Loss: 0.00001296
Iteration 193/1000 | Loss: 0.00001296
Iteration 194/1000 | Loss: 0.00001295
Iteration 195/1000 | Loss: 0.00001295
Iteration 196/1000 | Loss: 0.00001295
Iteration 197/1000 | Loss: 0.00001295
Iteration 198/1000 | Loss: 0.00001295
Iteration 199/1000 | Loss: 0.00001294
Iteration 200/1000 | Loss: 0.00001294
Iteration 201/1000 | Loss: 0.00001294
Iteration 202/1000 | Loss: 0.00001294
Iteration 203/1000 | Loss: 0.00001293
Iteration 204/1000 | Loss: 0.00001293
Iteration 205/1000 | Loss: 0.00001293
Iteration 206/1000 | Loss: 0.00001293
Iteration 207/1000 | Loss: 0.00001293
Iteration 208/1000 | Loss: 0.00001293
Iteration 209/1000 | Loss: 0.00001293
Iteration 210/1000 | Loss: 0.00001293
Iteration 211/1000 | Loss: 0.00001293
Iteration 212/1000 | Loss: 0.00001293
Iteration 213/1000 | Loss: 0.00001293
Iteration 214/1000 | Loss: 0.00001293
Iteration 215/1000 | Loss: 0.00001293
Iteration 216/1000 | Loss: 0.00001293
Iteration 217/1000 | Loss: 0.00001293
Iteration 218/1000 | Loss: 0.00001293
Iteration 219/1000 | Loss: 0.00001293
Iteration 220/1000 | Loss: 0.00001293
Iteration 221/1000 | Loss: 0.00001293
Iteration 222/1000 | Loss: 0.00001293
Iteration 223/1000 | Loss: 0.00001293
Iteration 224/1000 | Loss: 0.00001293
Iteration 225/1000 | Loss: 0.00001293
Iteration 226/1000 | Loss: 0.00001293
Iteration 227/1000 | Loss: 0.00001293
Iteration 228/1000 | Loss: 0.00001293
Iteration 229/1000 | Loss: 0.00001293
Iteration 230/1000 | Loss: 0.00001293
Iteration 231/1000 | Loss: 0.00001293
Iteration 232/1000 | Loss: 0.00001293
Iteration 233/1000 | Loss: 0.00001293
Iteration 234/1000 | Loss: 0.00001293
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 234. Stopping optimization.
Last 5 losses: [1.2928519936394878e-05, 1.2928519936394878e-05, 1.2928519936394878e-05, 1.2928519936394878e-05, 1.2928519936394878e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2928519936394878e-05

Optimization complete. Final v2v error: 3.0617268085479736 mm

Highest mean error: 4.061732292175293 mm for frame 89

Lowest mean error: 2.8295795917510986 mm for frame 13

Saving results

Total time: 45.46210503578186
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00797342
Iteration 2/25 | Loss: 0.00150970
Iteration 3/25 | Loss: 0.00130878
Iteration 4/25 | Loss: 0.00128205
Iteration 5/25 | Loss: 0.00127483
Iteration 6/25 | Loss: 0.00127305
Iteration 7/25 | Loss: 0.00127304
Iteration 8/25 | Loss: 0.00127304
Iteration 9/25 | Loss: 0.00127304
Iteration 10/25 | Loss: 0.00127304
Iteration 11/25 | Loss: 0.00127304
Iteration 12/25 | Loss: 0.00127304
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001273036585189402, 0.001273036585189402, 0.001273036585189402, 0.001273036585189402, 0.001273036585189402]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001273036585189402

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45436573
Iteration 2/25 | Loss: 0.00088842
Iteration 3/25 | Loss: 0.00088842
Iteration 4/25 | Loss: 0.00088842
Iteration 5/25 | Loss: 0.00088841
Iteration 6/25 | Loss: 0.00088841
Iteration 7/25 | Loss: 0.00088841
Iteration 8/25 | Loss: 0.00088841
Iteration 9/25 | Loss: 0.00088841
Iteration 10/25 | Loss: 0.00088841
Iteration 11/25 | Loss: 0.00088841
Iteration 12/25 | Loss: 0.00088841
Iteration 13/25 | Loss: 0.00088841
Iteration 14/25 | Loss: 0.00088841
Iteration 15/25 | Loss: 0.00088841
Iteration 16/25 | Loss: 0.00088841
Iteration 17/25 | Loss: 0.00088841
Iteration 18/25 | Loss: 0.00088841
Iteration 19/25 | Loss: 0.00088841
Iteration 20/25 | Loss: 0.00088841
Iteration 21/25 | Loss: 0.00088841
Iteration 22/25 | Loss: 0.00088841
Iteration 23/25 | Loss: 0.00088841
Iteration 24/25 | Loss: 0.00088841
Iteration 25/25 | Loss: 0.00088841

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088841
Iteration 2/1000 | Loss: 0.00004023
Iteration 3/1000 | Loss: 0.00002738
Iteration 4/1000 | Loss: 0.00002306
Iteration 5/1000 | Loss: 0.00002086
Iteration 6/1000 | Loss: 0.00001951
Iteration 7/1000 | Loss: 0.00001847
Iteration 8/1000 | Loss: 0.00001780
Iteration 9/1000 | Loss: 0.00001730
Iteration 10/1000 | Loss: 0.00001700
Iteration 11/1000 | Loss: 0.00001671
Iteration 12/1000 | Loss: 0.00001645
Iteration 13/1000 | Loss: 0.00001644
Iteration 14/1000 | Loss: 0.00001629
Iteration 15/1000 | Loss: 0.00001627
Iteration 16/1000 | Loss: 0.00001624
Iteration 17/1000 | Loss: 0.00001616
Iteration 18/1000 | Loss: 0.00001615
Iteration 19/1000 | Loss: 0.00001613
Iteration 20/1000 | Loss: 0.00001611
Iteration 21/1000 | Loss: 0.00001607
Iteration 22/1000 | Loss: 0.00001605
Iteration 23/1000 | Loss: 0.00001601
Iteration 24/1000 | Loss: 0.00001599
Iteration 25/1000 | Loss: 0.00001599
Iteration 26/1000 | Loss: 0.00001598
Iteration 27/1000 | Loss: 0.00001598
Iteration 28/1000 | Loss: 0.00001597
Iteration 29/1000 | Loss: 0.00001597
Iteration 30/1000 | Loss: 0.00001597
Iteration 31/1000 | Loss: 0.00001596
Iteration 32/1000 | Loss: 0.00001596
Iteration 33/1000 | Loss: 0.00001596
Iteration 34/1000 | Loss: 0.00001595
Iteration 35/1000 | Loss: 0.00001595
Iteration 36/1000 | Loss: 0.00001594
Iteration 37/1000 | Loss: 0.00001594
Iteration 38/1000 | Loss: 0.00001593
Iteration 39/1000 | Loss: 0.00001593
Iteration 40/1000 | Loss: 0.00001593
Iteration 41/1000 | Loss: 0.00001592
Iteration 42/1000 | Loss: 0.00001592
Iteration 43/1000 | Loss: 0.00001592
Iteration 44/1000 | Loss: 0.00001591
Iteration 45/1000 | Loss: 0.00001591
Iteration 46/1000 | Loss: 0.00001591
Iteration 47/1000 | Loss: 0.00001590
Iteration 48/1000 | Loss: 0.00001590
Iteration 49/1000 | Loss: 0.00001590
Iteration 50/1000 | Loss: 0.00001589
Iteration 51/1000 | Loss: 0.00001589
Iteration 52/1000 | Loss: 0.00001589
Iteration 53/1000 | Loss: 0.00001588
Iteration 54/1000 | Loss: 0.00001588
Iteration 55/1000 | Loss: 0.00001588
Iteration 56/1000 | Loss: 0.00001587
Iteration 57/1000 | Loss: 0.00001587
Iteration 58/1000 | Loss: 0.00001587
Iteration 59/1000 | Loss: 0.00001587
Iteration 60/1000 | Loss: 0.00001587
Iteration 61/1000 | Loss: 0.00001586
Iteration 62/1000 | Loss: 0.00001586
Iteration 63/1000 | Loss: 0.00001586
Iteration 64/1000 | Loss: 0.00001586
Iteration 65/1000 | Loss: 0.00001586
Iteration 66/1000 | Loss: 0.00001586
Iteration 67/1000 | Loss: 0.00001586
Iteration 68/1000 | Loss: 0.00001586
Iteration 69/1000 | Loss: 0.00001586
Iteration 70/1000 | Loss: 0.00001585
Iteration 71/1000 | Loss: 0.00001585
Iteration 72/1000 | Loss: 0.00001585
Iteration 73/1000 | Loss: 0.00001585
Iteration 74/1000 | Loss: 0.00001585
Iteration 75/1000 | Loss: 0.00001585
Iteration 76/1000 | Loss: 0.00001584
Iteration 77/1000 | Loss: 0.00001584
Iteration 78/1000 | Loss: 0.00001584
Iteration 79/1000 | Loss: 0.00001584
Iteration 80/1000 | Loss: 0.00001583
Iteration 81/1000 | Loss: 0.00001583
Iteration 82/1000 | Loss: 0.00001583
Iteration 83/1000 | Loss: 0.00001582
Iteration 84/1000 | Loss: 0.00001582
Iteration 85/1000 | Loss: 0.00001582
Iteration 86/1000 | Loss: 0.00001581
Iteration 87/1000 | Loss: 0.00001581
Iteration 88/1000 | Loss: 0.00001581
Iteration 89/1000 | Loss: 0.00001581
Iteration 90/1000 | Loss: 0.00001580
Iteration 91/1000 | Loss: 0.00001580
Iteration 92/1000 | Loss: 0.00001580
Iteration 93/1000 | Loss: 0.00001580
Iteration 94/1000 | Loss: 0.00001580
Iteration 95/1000 | Loss: 0.00001580
Iteration 96/1000 | Loss: 0.00001580
Iteration 97/1000 | Loss: 0.00001580
Iteration 98/1000 | Loss: 0.00001580
Iteration 99/1000 | Loss: 0.00001580
Iteration 100/1000 | Loss: 0.00001580
Iteration 101/1000 | Loss: 0.00001579
Iteration 102/1000 | Loss: 0.00001579
Iteration 103/1000 | Loss: 0.00001579
Iteration 104/1000 | Loss: 0.00001579
Iteration 105/1000 | Loss: 0.00001579
Iteration 106/1000 | Loss: 0.00001579
Iteration 107/1000 | Loss: 0.00001579
Iteration 108/1000 | Loss: 0.00001579
Iteration 109/1000 | Loss: 0.00001579
Iteration 110/1000 | Loss: 0.00001579
Iteration 111/1000 | Loss: 0.00001579
Iteration 112/1000 | Loss: 0.00001579
Iteration 113/1000 | Loss: 0.00001579
Iteration 114/1000 | Loss: 0.00001579
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [1.579470517754089e-05, 1.579470517754089e-05, 1.579470517754089e-05, 1.579470517754089e-05, 1.579470517754089e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.579470517754089e-05

Optimization complete. Final v2v error: 3.394080638885498 mm

Highest mean error: 3.829624891281128 mm for frame 79

Lowest mean error: 3.0439248085021973 mm for frame 154

Saving results

Total time: 39.38461923599243
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00410295
Iteration 2/25 | Loss: 0.00134017
Iteration 3/25 | Loss: 0.00126792
Iteration 4/25 | Loss: 0.00125611
Iteration 5/25 | Loss: 0.00125219
Iteration 6/25 | Loss: 0.00125144
Iteration 7/25 | Loss: 0.00125141
Iteration 8/25 | Loss: 0.00125141
Iteration 9/25 | Loss: 0.00125141
Iteration 10/25 | Loss: 0.00125141
Iteration 11/25 | Loss: 0.00125141
Iteration 12/25 | Loss: 0.00125141
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012514084810391068, 0.0012514084810391068, 0.0012514084810391068, 0.0012514084810391068, 0.0012514084810391068]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012514084810391068

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.93170261
Iteration 2/25 | Loss: 0.00085687
Iteration 3/25 | Loss: 0.00085687
Iteration 4/25 | Loss: 0.00085687
Iteration 5/25 | Loss: 0.00085686
Iteration 6/25 | Loss: 0.00085686
Iteration 7/25 | Loss: 0.00085686
Iteration 8/25 | Loss: 0.00085686
Iteration 9/25 | Loss: 0.00085686
Iteration 10/25 | Loss: 0.00085686
Iteration 11/25 | Loss: 0.00085686
Iteration 12/25 | Loss: 0.00085686
Iteration 13/25 | Loss: 0.00085686
Iteration 14/25 | Loss: 0.00085686
Iteration 15/25 | Loss: 0.00085686
Iteration 16/25 | Loss: 0.00085686
Iteration 17/25 | Loss: 0.00085686
Iteration 18/25 | Loss: 0.00085686
Iteration 19/25 | Loss: 0.00085686
Iteration 20/25 | Loss: 0.00085686
Iteration 21/25 | Loss: 0.00085686
Iteration 22/25 | Loss: 0.00085686
Iteration 23/25 | Loss: 0.00085686
Iteration 24/25 | Loss: 0.00085686
Iteration 25/25 | Loss: 0.00085686

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085686
Iteration 2/1000 | Loss: 0.00002599
Iteration 3/1000 | Loss: 0.00001794
Iteration 4/1000 | Loss: 0.00001627
Iteration 5/1000 | Loss: 0.00001538
Iteration 6/1000 | Loss: 0.00001483
Iteration 7/1000 | Loss: 0.00001433
Iteration 8/1000 | Loss: 0.00001401
Iteration 9/1000 | Loss: 0.00001396
Iteration 10/1000 | Loss: 0.00001393
Iteration 11/1000 | Loss: 0.00001393
Iteration 12/1000 | Loss: 0.00001376
Iteration 13/1000 | Loss: 0.00001354
Iteration 14/1000 | Loss: 0.00001348
Iteration 15/1000 | Loss: 0.00001340
Iteration 16/1000 | Loss: 0.00001336
Iteration 17/1000 | Loss: 0.00001328
Iteration 18/1000 | Loss: 0.00001326
Iteration 19/1000 | Loss: 0.00001322
Iteration 20/1000 | Loss: 0.00001322
Iteration 21/1000 | Loss: 0.00001321
Iteration 22/1000 | Loss: 0.00001317
Iteration 23/1000 | Loss: 0.00001314
Iteration 24/1000 | Loss: 0.00001313
Iteration 25/1000 | Loss: 0.00001312
Iteration 26/1000 | Loss: 0.00001311
Iteration 27/1000 | Loss: 0.00001309
Iteration 28/1000 | Loss: 0.00001309
Iteration 29/1000 | Loss: 0.00001308
Iteration 30/1000 | Loss: 0.00001307
Iteration 31/1000 | Loss: 0.00001306
Iteration 32/1000 | Loss: 0.00001305
Iteration 33/1000 | Loss: 0.00001305
Iteration 34/1000 | Loss: 0.00001305
Iteration 35/1000 | Loss: 0.00001305
Iteration 36/1000 | Loss: 0.00001304
Iteration 37/1000 | Loss: 0.00001304
Iteration 38/1000 | Loss: 0.00001304
Iteration 39/1000 | Loss: 0.00001303
Iteration 40/1000 | Loss: 0.00001300
Iteration 41/1000 | Loss: 0.00001299
Iteration 42/1000 | Loss: 0.00001299
Iteration 43/1000 | Loss: 0.00001299
Iteration 44/1000 | Loss: 0.00001299
Iteration 45/1000 | Loss: 0.00001299
Iteration 46/1000 | Loss: 0.00001299
Iteration 47/1000 | Loss: 0.00001299
Iteration 48/1000 | Loss: 0.00001299
Iteration 49/1000 | Loss: 0.00001299
Iteration 50/1000 | Loss: 0.00001299
Iteration 51/1000 | Loss: 0.00001299
Iteration 52/1000 | Loss: 0.00001298
Iteration 53/1000 | Loss: 0.00001298
Iteration 54/1000 | Loss: 0.00001297
Iteration 55/1000 | Loss: 0.00001297
Iteration 56/1000 | Loss: 0.00001297
Iteration 57/1000 | Loss: 0.00001296
Iteration 58/1000 | Loss: 0.00001295
Iteration 59/1000 | Loss: 0.00001295
Iteration 60/1000 | Loss: 0.00001294
Iteration 61/1000 | Loss: 0.00001294
Iteration 62/1000 | Loss: 0.00001294
Iteration 63/1000 | Loss: 0.00001294
Iteration 64/1000 | Loss: 0.00001294
Iteration 65/1000 | Loss: 0.00001293
Iteration 66/1000 | Loss: 0.00001293
Iteration 67/1000 | Loss: 0.00001292
Iteration 68/1000 | Loss: 0.00001292
Iteration 69/1000 | Loss: 0.00001292
Iteration 70/1000 | Loss: 0.00001288
Iteration 71/1000 | Loss: 0.00001288
Iteration 72/1000 | Loss: 0.00001288
Iteration 73/1000 | Loss: 0.00001286
Iteration 74/1000 | Loss: 0.00001286
Iteration 75/1000 | Loss: 0.00001285
Iteration 76/1000 | Loss: 0.00001285
Iteration 77/1000 | Loss: 0.00001285
Iteration 78/1000 | Loss: 0.00001284
Iteration 79/1000 | Loss: 0.00001284
Iteration 80/1000 | Loss: 0.00001284
Iteration 81/1000 | Loss: 0.00001283
Iteration 82/1000 | Loss: 0.00001283
Iteration 83/1000 | Loss: 0.00001283
Iteration 84/1000 | Loss: 0.00001283
Iteration 85/1000 | Loss: 0.00001283
Iteration 86/1000 | Loss: 0.00001282
Iteration 87/1000 | Loss: 0.00001282
Iteration 88/1000 | Loss: 0.00001282
Iteration 89/1000 | Loss: 0.00001282
Iteration 90/1000 | Loss: 0.00001281
Iteration 91/1000 | Loss: 0.00001281
Iteration 92/1000 | Loss: 0.00001281
Iteration 93/1000 | Loss: 0.00001281
Iteration 94/1000 | Loss: 0.00001280
Iteration 95/1000 | Loss: 0.00001280
Iteration 96/1000 | Loss: 0.00001280
Iteration 97/1000 | Loss: 0.00001280
Iteration 98/1000 | Loss: 0.00001280
Iteration 99/1000 | Loss: 0.00001280
Iteration 100/1000 | Loss: 0.00001280
Iteration 101/1000 | Loss: 0.00001280
Iteration 102/1000 | Loss: 0.00001280
Iteration 103/1000 | Loss: 0.00001279
Iteration 104/1000 | Loss: 0.00001279
Iteration 105/1000 | Loss: 0.00001278
Iteration 106/1000 | Loss: 0.00001278
Iteration 107/1000 | Loss: 0.00001278
Iteration 108/1000 | Loss: 0.00001278
Iteration 109/1000 | Loss: 0.00001277
Iteration 110/1000 | Loss: 0.00001277
Iteration 111/1000 | Loss: 0.00001277
Iteration 112/1000 | Loss: 0.00001277
Iteration 113/1000 | Loss: 0.00001276
Iteration 114/1000 | Loss: 0.00001276
Iteration 115/1000 | Loss: 0.00001276
Iteration 116/1000 | Loss: 0.00001276
Iteration 117/1000 | Loss: 0.00001276
Iteration 118/1000 | Loss: 0.00001276
Iteration 119/1000 | Loss: 0.00001275
Iteration 120/1000 | Loss: 0.00001275
Iteration 121/1000 | Loss: 0.00001275
Iteration 122/1000 | Loss: 0.00001275
Iteration 123/1000 | Loss: 0.00001275
Iteration 124/1000 | Loss: 0.00001275
Iteration 125/1000 | Loss: 0.00001275
Iteration 126/1000 | Loss: 0.00001275
Iteration 127/1000 | Loss: 0.00001275
Iteration 128/1000 | Loss: 0.00001275
Iteration 129/1000 | Loss: 0.00001275
Iteration 130/1000 | Loss: 0.00001275
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.2745384992740583e-05, 1.2745384992740583e-05, 1.2745384992740583e-05, 1.2745384992740583e-05, 1.2745384992740583e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2745384992740583e-05

Optimization complete. Final v2v error: 3.053420066833496 mm

Highest mean error: 3.5207581520080566 mm for frame 55

Lowest mean error: 2.8723998069763184 mm for frame 86

Saving results

Total time: 38.40739178657532
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00507213
Iteration 2/25 | Loss: 0.00148278
Iteration 3/25 | Loss: 0.00137329
Iteration 4/25 | Loss: 0.00136279
Iteration 5/25 | Loss: 0.00135957
Iteration 6/25 | Loss: 0.00135957
Iteration 7/25 | Loss: 0.00135957
Iteration 8/25 | Loss: 0.00135957
Iteration 9/25 | Loss: 0.00135957
Iteration 10/25 | Loss: 0.00135957
Iteration 11/25 | Loss: 0.00135957
Iteration 12/25 | Loss: 0.00135957
Iteration 13/25 | Loss: 0.00135957
Iteration 14/25 | Loss: 0.00135957
Iteration 15/25 | Loss: 0.00135957
Iteration 16/25 | Loss: 0.00135957
Iteration 17/25 | Loss: 0.00135957
Iteration 18/25 | Loss: 0.00135957
Iteration 19/25 | Loss: 0.00135957
Iteration 20/25 | Loss: 0.00135957
Iteration 21/25 | Loss: 0.00135957
Iteration 22/25 | Loss: 0.00135957
Iteration 23/25 | Loss: 0.00135957
Iteration 24/25 | Loss: 0.00135957
Iteration 25/25 | Loss: 0.00135957

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.66268170
Iteration 2/25 | Loss: 0.00096428
Iteration 3/25 | Loss: 0.00096427
Iteration 4/25 | Loss: 0.00096427
Iteration 5/25 | Loss: 0.00096427
Iteration 6/25 | Loss: 0.00096427
Iteration 7/25 | Loss: 0.00096427
Iteration 8/25 | Loss: 0.00096427
Iteration 9/25 | Loss: 0.00096427
Iteration 10/25 | Loss: 0.00096427
Iteration 11/25 | Loss: 0.00096427
Iteration 12/25 | Loss: 0.00096427
Iteration 13/25 | Loss: 0.00096427
Iteration 14/25 | Loss: 0.00096427
Iteration 15/25 | Loss: 0.00096427
Iteration 16/25 | Loss: 0.00096427
Iteration 17/25 | Loss: 0.00096427
Iteration 18/25 | Loss: 0.00096427
Iteration 19/25 | Loss: 0.00096427
Iteration 20/25 | Loss: 0.00096427
Iteration 21/25 | Loss: 0.00096427
Iteration 22/25 | Loss: 0.00096427
Iteration 23/25 | Loss: 0.00096427
Iteration 24/25 | Loss: 0.00096427
Iteration 25/25 | Loss: 0.00096427
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0009642703807912767, 0.0009642703807912767, 0.0009642703807912767, 0.0009642703807912767, 0.0009642703807912767]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009642703807912767

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096427
Iteration 2/1000 | Loss: 0.00005686
Iteration 3/1000 | Loss: 0.00003435
Iteration 4/1000 | Loss: 0.00003080
Iteration 5/1000 | Loss: 0.00002952
Iteration 6/1000 | Loss: 0.00002759
Iteration 7/1000 | Loss: 0.00002650
Iteration 8/1000 | Loss: 0.00002547
Iteration 9/1000 | Loss: 0.00002494
Iteration 10/1000 | Loss: 0.00002436
Iteration 11/1000 | Loss: 0.00002399
Iteration 12/1000 | Loss: 0.00002369
Iteration 13/1000 | Loss: 0.00002334
Iteration 14/1000 | Loss: 0.00002311
Iteration 15/1000 | Loss: 0.00002288
Iteration 16/1000 | Loss: 0.00002266
Iteration 17/1000 | Loss: 0.00002258
Iteration 18/1000 | Loss: 0.00002243
Iteration 19/1000 | Loss: 0.00002224
Iteration 20/1000 | Loss: 0.00002217
Iteration 21/1000 | Loss: 0.00002213
Iteration 22/1000 | Loss: 0.00002203
Iteration 23/1000 | Loss: 0.00002202
Iteration 24/1000 | Loss: 0.00002199
Iteration 25/1000 | Loss: 0.00002199
Iteration 26/1000 | Loss: 0.00002195
Iteration 27/1000 | Loss: 0.00002194
Iteration 28/1000 | Loss: 0.00002194
Iteration 29/1000 | Loss: 0.00002192
Iteration 30/1000 | Loss: 0.00002192
Iteration 31/1000 | Loss: 0.00002192
Iteration 32/1000 | Loss: 0.00002192
Iteration 33/1000 | Loss: 0.00002192
Iteration 34/1000 | Loss: 0.00002192
Iteration 35/1000 | Loss: 0.00002192
Iteration 36/1000 | Loss: 0.00002191
Iteration 37/1000 | Loss: 0.00002191
Iteration 38/1000 | Loss: 0.00002191
Iteration 39/1000 | Loss: 0.00002191
Iteration 40/1000 | Loss: 0.00002191
Iteration 41/1000 | Loss: 0.00002191
Iteration 42/1000 | Loss: 0.00002191
Iteration 43/1000 | Loss: 0.00002191
Iteration 44/1000 | Loss: 0.00002191
Iteration 45/1000 | Loss: 0.00002190
Iteration 46/1000 | Loss: 0.00002187
Iteration 47/1000 | Loss: 0.00002187
Iteration 48/1000 | Loss: 0.00002186
Iteration 49/1000 | Loss: 0.00002186
Iteration 50/1000 | Loss: 0.00002186
Iteration 51/1000 | Loss: 0.00002185
Iteration 52/1000 | Loss: 0.00002184
Iteration 53/1000 | Loss: 0.00002182
Iteration 54/1000 | Loss: 0.00002181
Iteration 55/1000 | Loss: 0.00002180
Iteration 56/1000 | Loss: 0.00002179
Iteration 57/1000 | Loss: 0.00002179
Iteration 58/1000 | Loss: 0.00002179
Iteration 59/1000 | Loss: 0.00002179
Iteration 60/1000 | Loss: 0.00002179
Iteration 61/1000 | Loss: 0.00002179
Iteration 62/1000 | Loss: 0.00002179
Iteration 63/1000 | Loss: 0.00002178
Iteration 64/1000 | Loss: 0.00002178
Iteration 65/1000 | Loss: 0.00002178
Iteration 66/1000 | Loss: 0.00002178
Iteration 67/1000 | Loss: 0.00002178
Iteration 68/1000 | Loss: 0.00002178
Iteration 69/1000 | Loss: 0.00002178
Iteration 70/1000 | Loss: 0.00002178
Iteration 71/1000 | Loss: 0.00002178
Iteration 72/1000 | Loss: 0.00002178
Iteration 73/1000 | Loss: 0.00002178
Iteration 74/1000 | Loss: 0.00002178
Iteration 75/1000 | Loss: 0.00002178
Iteration 76/1000 | Loss: 0.00002177
Iteration 77/1000 | Loss: 0.00002177
Iteration 78/1000 | Loss: 0.00002176
Iteration 79/1000 | Loss: 0.00002176
Iteration 80/1000 | Loss: 0.00002176
Iteration 81/1000 | Loss: 0.00002175
Iteration 82/1000 | Loss: 0.00002175
Iteration 83/1000 | Loss: 0.00002175
Iteration 84/1000 | Loss: 0.00002175
Iteration 85/1000 | Loss: 0.00002175
Iteration 86/1000 | Loss: 0.00002175
Iteration 87/1000 | Loss: 0.00002175
Iteration 88/1000 | Loss: 0.00002175
Iteration 89/1000 | Loss: 0.00002174
Iteration 90/1000 | Loss: 0.00002174
Iteration 91/1000 | Loss: 0.00002174
Iteration 92/1000 | Loss: 0.00002173
Iteration 93/1000 | Loss: 0.00002173
Iteration 94/1000 | Loss: 0.00002173
Iteration 95/1000 | Loss: 0.00002172
Iteration 96/1000 | Loss: 0.00002172
Iteration 97/1000 | Loss: 0.00002172
Iteration 98/1000 | Loss: 0.00002172
Iteration 99/1000 | Loss: 0.00002172
Iteration 100/1000 | Loss: 0.00002172
Iteration 101/1000 | Loss: 0.00002172
Iteration 102/1000 | Loss: 0.00002172
Iteration 103/1000 | Loss: 0.00002172
Iteration 104/1000 | Loss: 0.00002172
Iteration 105/1000 | Loss: 0.00002172
Iteration 106/1000 | Loss: 0.00002172
Iteration 107/1000 | Loss: 0.00002172
Iteration 108/1000 | Loss: 0.00002171
Iteration 109/1000 | Loss: 0.00002171
Iteration 110/1000 | Loss: 0.00002171
Iteration 111/1000 | Loss: 0.00002171
Iteration 112/1000 | Loss: 0.00002171
Iteration 113/1000 | Loss: 0.00002171
Iteration 114/1000 | Loss: 0.00002171
Iteration 115/1000 | Loss: 0.00002171
Iteration 116/1000 | Loss: 0.00002171
Iteration 117/1000 | Loss: 0.00002171
Iteration 118/1000 | Loss: 0.00002171
Iteration 119/1000 | Loss: 0.00002170
Iteration 120/1000 | Loss: 0.00002170
Iteration 121/1000 | Loss: 0.00002170
Iteration 122/1000 | Loss: 0.00002170
Iteration 123/1000 | Loss: 0.00002170
Iteration 124/1000 | Loss: 0.00002170
Iteration 125/1000 | Loss: 0.00002170
Iteration 126/1000 | Loss: 0.00002169
Iteration 127/1000 | Loss: 0.00002169
Iteration 128/1000 | Loss: 0.00002169
Iteration 129/1000 | Loss: 0.00002169
Iteration 130/1000 | Loss: 0.00002169
Iteration 131/1000 | Loss: 0.00002168
Iteration 132/1000 | Loss: 0.00002168
Iteration 133/1000 | Loss: 0.00002168
Iteration 134/1000 | Loss: 0.00002168
Iteration 135/1000 | Loss: 0.00002168
Iteration 136/1000 | Loss: 0.00002168
Iteration 137/1000 | Loss: 0.00002167
Iteration 138/1000 | Loss: 0.00002167
Iteration 139/1000 | Loss: 0.00002167
Iteration 140/1000 | Loss: 0.00002167
Iteration 141/1000 | Loss: 0.00002166
Iteration 142/1000 | Loss: 0.00002166
Iteration 143/1000 | Loss: 0.00002166
Iteration 144/1000 | Loss: 0.00002165
Iteration 145/1000 | Loss: 0.00002165
Iteration 146/1000 | Loss: 0.00002165
Iteration 147/1000 | Loss: 0.00002164
Iteration 148/1000 | Loss: 0.00002164
Iteration 149/1000 | Loss: 0.00002164
Iteration 150/1000 | Loss: 0.00002164
Iteration 151/1000 | Loss: 0.00002164
Iteration 152/1000 | Loss: 0.00002164
Iteration 153/1000 | Loss: 0.00002163
Iteration 154/1000 | Loss: 0.00002163
Iteration 155/1000 | Loss: 0.00002163
Iteration 156/1000 | Loss: 0.00002162
Iteration 157/1000 | Loss: 0.00002162
Iteration 158/1000 | Loss: 0.00002162
Iteration 159/1000 | Loss: 0.00002162
Iteration 160/1000 | Loss: 0.00002162
Iteration 161/1000 | Loss: 0.00002161
Iteration 162/1000 | Loss: 0.00002161
Iteration 163/1000 | Loss: 0.00002161
Iteration 164/1000 | Loss: 0.00002161
Iteration 165/1000 | Loss: 0.00002160
Iteration 166/1000 | Loss: 0.00002160
Iteration 167/1000 | Loss: 0.00002160
Iteration 168/1000 | Loss: 0.00002160
Iteration 169/1000 | Loss: 0.00002159
Iteration 170/1000 | Loss: 0.00002159
Iteration 171/1000 | Loss: 0.00002159
Iteration 172/1000 | Loss: 0.00002159
Iteration 173/1000 | Loss: 0.00002158
Iteration 174/1000 | Loss: 0.00002158
Iteration 175/1000 | Loss: 0.00002158
Iteration 176/1000 | Loss: 0.00002158
Iteration 177/1000 | Loss: 0.00002157
Iteration 178/1000 | Loss: 0.00002157
Iteration 179/1000 | Loss: 0.00002157
Iteration 180/1000 | Loss: 0.00002156
Iteration 181/1000 | Loss: 0.00002156
Iteration 182/1000 | Loss: 0.00002156
Iteration 183/1000 | Loss: 0.00002156
Iteration 184/1000 | Loss: 0.00002156
Iteration 185/1000 | Loss: 0.00002155
Iteration 186/1000 | Loss: 0.00002155
Iteration 187/1000 | Loss: 0.00002155
Iteration 188/1000 | Loss: 0.00002155
Iteration 189/1000 | Loss: 0.00002155
Iteration 190/1000 | Loss: 0.00002155
Iteration 191/1000 | Loss: 0.00002155
Iteration 192/1000 | Loss: 0.00002155
Iteration 193/1000 | Loss: 0.00002155
Iteration 194/1000 | Loss: 0.00002155
Iteration 195/1000 | Loss: 0.00002155
Iteration 196/1000 | Loss: 0.00002155
Iteration 197/1000 | Loss: 0.00002154
Iteration 198/1000 | Loss: 0.00002154
Iteration 199/1000 | Loss: 0.00002154
Iteration 200/1000 | Loss: 0.00002154
Iteration 201/1000 | Loss: 0.00002154
Iteration 202/1000 | Loss: 0.00002154
Iteration 203/1000 | Loss: 0.00002154
Iteration 204/1000 | Loss: 0.00002154
Iteration 205/1000 | Loss: 0.00002154
Iteration 206/1000 | Loss: 0.00002153
Iteration 207/1000 | Loss: 0.00002153
Iteration 208/1000 | Loss: 0.00002153
Iteration 209/1000 | Loss: 0.00002153
Iteration 210/1000 | Loss: 0.00002153
Iteration 211/1000 | Loss: 0.00002153
Iteration 212/1000 | Loss: 0.00002153
Iteration 213/1000 | Loss: 0.00002153
Iteration 214/1000 | Loss: 0.00002153
Iteration 215/1000 | Loss: 0.00002153
Iteration 216/1000 | Loss: 0.00002153
Iteration 217/1000 | Loss: 0.00002153
Iteration 218/1000 | Loss: 0.00002153
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 218. Stopping optimization.
Last 5 losses: [2.152933484467212e-05, 2.152933484467212e-05, 2.152933484467212e-05, 2.152933484467212e-05, 2.152933484467212e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.152933484467212e-05

Optimization complete. Final v2v error: 3.885880947113037 mm

Highest mean error: 4.283668518066406 mm for frame 245

Lowest mean error: 3.7515928745269775 mm for frame 209

Saving results

Total time: 61.156816720962524
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00635606
Iteration 2/25 | Loss: 0.00172528
Iteration 3/25 | Loss: 0.00146988
Iteration 4/25 | Loss: 0.00145035
Iteration 5/25 | Loss: 0.00144549
Iteration 6/25 | Loss: 0.00144487
Iteration 7/25 | Loss: 0.00144487
Iteration 8/25 | Loss: 0.00144487
Iteration 9/25 | Loss: 0.00144487
Iteration 10/25 | Loss: 0.00144487
Iteration 11/25 | Loss: 0.00144487
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014448695583269, 0.0014448695583269, 0.0014448695583269, 0.0014448695583269, 0.0014448695583269]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014448695583269

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39622724
Iteration 2/25 | Loss: 0.00118880
Iteration 3/25 | Loss: 0.00118877
Iteration 4/25 | Loss: 0.00118877
Iteration 5/25 | Loss: 0.00118877
Iteration 6/25 | Loss: 0.00118877
Iteration 7/25 | Loss: 0.00118877
Iteration 8/25 | Loss: 0.00118877
Iteration 9/25 | Loss: 0.00118877
Iteration 10/25 | Loss: 0.00118877
Iteration 11/25 | Loss: 0.00118877
Iteration 12/25 | Loss: 0.00118877
Iteration 13/25 | Loss: 0.00118877
Iteration 14/25 | Loss: 0.00118877
Iteration 15/25 | Loss: 0.00118877
Iteration 16/25 | Loss: 0.00118877
Iteration 17/25 | Loss: 0.00118877
Iteration 18/25 | Loss: 0.00118877
Iteration 19/25 | Loss: 0.00118877
Iteration 20/25 | Loss: 0.00118877
Iteration 21/25 | Loss: 0.00118877
Iteration 22/25 | Loss: 0.00118877
Iteration 23/25 | Loss: 0.00118877
Iteration 24/25 | Loss: 0.00118877
Iteration 25/25 | Loss: 0.00118877

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118877
Iteration 2/1000 | Loss: 0.00004415
Iteration 3/1000 | Loss: 0.00003311
Iteration 4/1000 | Loss: 0.00003056
Iteration 5/1000 | Loss: 0.00002924
Iteration 6/1000 | Loss: 0.00002844
Iteration 7/1000 | Loss: 0.00002794
Iteration 8/1000 | Loss: 0.00002761
Iteration 9/1000 | Loss: 0.00002703
Iteration 10/1000 | Loss: 0.00002673
Iteration 11/1000 | Loss: 0.00002648
Iteration 12/1000 | Loss: 0.00002624
Iteration 13/1000 | Loss: 0.00002596
Iteration 14/1000 | Loss: 0.00002575
Iteration 15/1000 | Loss: 0.00002571
Iteration 16/1000 | Loss: 0.00002557
Iteration 17/1000 | Loss: 0.00002551
Iteration 18/1000 | Loss: 0.00002538
Iteration 19/1000 | Loss: 0.00002530
Iteration 20/1000 | Loss: 0.00002530
Iteration 21/1000 | Loss: 0.00002527
Iteration 22/1000 | Loss: 0.00002516
Iteration 23/1000 | Loss: 0.00002511
Iteration 24/1000 | Loss: 0.00002508
Iteration 25/1000 | Loss: 0.00002508
Iteration 26/1000 | Loss: 0.00002508
Iteration 27/1000 | Loss: 0.00002508
Iteration 28/1000 | Loss: 0.00002508
Iteration 29/1000 | Loss: 0.00002507
Iteration 30/1000 | Loss: 0.00002507
Iteration 31/1000 | Loss: 0.00002507
Iteration 32/1000 | Loss: 0.00002507
Iteration 33/1000 | Loss: 0.00002507
Iteration 34/1000 | Loss: 0.00002507
Iteration 35/1000 | Loss: 0.00002507
Iteration 36/1000 | Loss: 0.00002507
Iteration 37/1000 | Loss: 0.00002506
Iteration 38/1000 | Loss: 0.00002506
Iteration 39/1000 | Loss: 0.00002506
Iteration 40/1000 | Loss: 0.00002506
Iteration 41/1000 | Loss: 0.00002505
Iteration 42/1000 | Loss: 0.00002505
Iteration 43/1000 | Loss: 0.00002505
Iteration 44/1000 | Loss: 0.00002504
Iteration 45/1000 | Loss: 0.00002504
Iteration 46/1000 | Loss: 0.00002504
Iteration 47/1000 | Loss: 0.00002503
Iteration 48/1000 | Loss: 0.00002503
Iteration 49/1000 | Loss: 0.00002503
Iteration 50/1000 | Loss: 0.00002503
Iteration 51/1000 | Loss: 0.00002503
Iteration 52/1000 | Loss: 0.00002503
Iteration 53/1000 | Loss: 0.00002503
Iteration 54/1000 | Loss: 0.00002503
Iteration 55/1000 | Loss: 0.00002503
Iteration 56/1000 | Loss: 0.00002502
Iteration 57/1000 | Loss: 0.00002502
Iteration 58/1000 | Loss: 0.00002502
Iteration 59/1000 | Loss: 0.00002501
Iteration 60/1000 | Loss: 0.00002501
Iteration 61/1000 | Loss: 0.00002501
Iteration 62/1000 | Loss: 0.00002500
Iteration 63/1000 | Loss: 0.00002500
Iteration 64/1000 | Loss: 0.00002500
Iteration 65/1000 | Loss: 0.00002499
Iteration 66/1000 | Loss: 0.00002499
Iteration 67/1000 | Loss: 0.00002499
Iteration 68/1000 | Loss: 0.00002499
Iteration 69/1000 | Loss: 0.00002499
Iteration 70/1000 | Loss: 0.00002499
Iteration 71/1000 | Loss: 0.00002499
Iteration 72/1000 | Loss: 0.00002498
Iteration 73/1000 | Loss: 0.00002498
Iteration 74/1000 | Loss: 0.00002498
Iteration 75/1000 | Loss: 0.00002498
Iteration 76/1000 | Loss: 0.00002497
Iteration 77/1000 | Loss: 0.00002497
Iteration 78/1000 | Loss: 0.00002497
Iteration 79/1000 | Loss: 0.00002496
Iteration 80/1000 | Loss: 0.00002496
Iteration 81/1000 | Loss: 0.00002496
Iteration 82/1000 | Loss: 0.00002496
Iteration 83/1000 | Loss: 0.00002496
Iteration 84/1000 | Loss: 0.00002495
Iteration 85/1000 | Loss: 0.00002495
Iteration 86/1000 | Loss: 0.00002495
Iteration 87/1000 | Loss: 0.00002494
Iteration 88/1000 | Loss: 0.00002494
Iteration 89/1000 | Loss: 0.00002494
Iteration 90/1000 | Loss: 0.00002494
Iteration 91/1000 | Loss: 0.00002494
Iteration 92/1000 | Loss: 0.00002494
Iteration 93/1000 | Loss: 0.00002494
Iteration 94/1000 | Loss: 0.00002494
Iteration 95/1000 | Loss: 0.00002493
Iteration 96/1000 | Loss: 0.00002493
Iteration 97/1000 | Loss: 0.00002493
Iteration 98/1000 | Loss: 0.00002493
Iteration 99/1000 | Loss: 0.00002493
Iteration 100/1000 | Loss: 0.00002493
Iteration 101/1000 | Loss: 0.00002493
Iteration 102/1000 | Loss: 0.00002492
Iteration 103/1000 | Loss: 0.00002492
Iteration 104/1000 | Loss: 0.00002492
Iteration 105/1000 | Loss: 0.00002492
Iteration 106/1000 | Loss: 0.00002492
Iteration 107/1000 | Loss: 0.00002492
Iteration 108/1000 | Loss: 0.00002491
Iteration 109/1000 | Loss: 0.00002491
Iteration 110/1000 | Loss: 0.00002491
Iteration 111/1000 | Loss: 0.00002491
Iteration 112/1000 | Loss: 0.00002490
Iteration 113/1000 | Loss: 0.00002490
Iteration 114/1000 | Loss: 0.00002490
Iteration 115/1000 | Loss: 0.00002490
Iteration 116/1000 | Loss: 0.00002490
Iteration 117/1000 | Loss: 0.00002490
Iteration 118/1000 | Loss: 0.00002490
Iteration 119/1000 | Loss: 0.00002490
Iteration 120/1000 | Loss: 0.00002490
Iteration 121/1000 | Loss: 0.00002490
Iteration 122/1000 | Loss: 0.00002490
Iteration 123/1000 | Loss: 0.00002490
Iteration 124/1000 | Loss: 0.00002490
Iteration 125/1000 | Loss: 0.00002490
Iteration 126/1000 | Loss: 0.00002490
Iteration 127/1000 | Loss: 0.00002489
Iteration 128/1000 | Loss: 0.00002489
Iteration 129/1000 | Loss: 0.00002489
Iteration 130/1000 | Loss: 0.00002489
Iteration 131/1000 | Loss: 0.00002489
Iteration 132/1000 | Loss: 0.00002489
Iteration 133/1000 | Loss: 0.00002489
Iteration 134/1000 | Loss: 0.00002489
Iteration 135/1000 | Loss: 0.00002489
Iteration 136/1000 | Loss: 0.00002489
Iteration 137/1000 | Loss: 0.00002489
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [2.4890441636671312e-05, 2.4890441636671312e-05, 2.4890441636671312e-05, 2.4890441636671312e-05, 2.4890441636671312e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4890441636671312e-05

Optimization complete. Final v2v error: 4.1842041015625 mm

Highest mean error: 4.5057878494262695 mm for frame 204

Lowest mean error: 3.8557827472686768 mm for frame 238

Saving results

Total time: 50.03191137313843
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00503648
Iteration 2/25 | Loss: 0.00130157
Iteration 3/25 | Loss: 0.00124800
Iteration 4/25 | Loss: 0.00123967
Iteration 5/25 | Loss: 0.00123748
Iteration 6/25 | Loss: 0.00123730
Iteration 7/25 | Loss: 0.00123730
Iteration 8/25 | Loss: 0.00123730
Iteration 9/25 | Loss: 0.00123730
Iteration 10/25 | Loss: 0.00123730
Iteration 11/25 | Loss: 0.00123730
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012373031349852681, 0.0012373031349852681, 0.0012373031349852681, 0.0012373031349852681, 0.0012373031349852681]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012373031349852681

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56034219
Iteration 2/25 | Loss: 0.00079504
Iteration 3/25 | Loss: 0.00079504
Iteration 4/25 | Loss: 0.00079504
Iteration 5/25 | Loss: 0.00079503
Iteration 6/25 | Loss: 0.00079503
Iteration 7/25 | Loss: 0.00079503
Iteration 8/25 | Loss: 0.00079503
Iteration 9/25 | Loss: 0.00079503
Iteration 10/25 | Loss: 0.00079503
Iteration 11/25 | Loss: 0.00079503
Iteration 12/25 | Loss: 0.00079503
Iteration 13/25 | Loss: 0.00079503
Iteration 14/25 | Loss: 0.00079503
Iteration 15/25 | Loss: 0.00079503
Iteration 16/25 | Loss: 0.00079503
Iteration 17/25 | Loss: 0.00079503
Iteration 18/25 | Loss: 0.00079503
Iteration 19/25 | Loss: 0.00079503
Iteration 20/25 | Loss: 0.00079503
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007950327126309276, 0.0007950327126309276, 0.0007950327126309276, 0.0007950327126309276, 0.0007950327126309276]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007950327126309276

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079503
Iteration 2/1000 | Loss: 0.00002421
Iteration 3/1000 | Loss: 0.00001836
Iteration 4/1000 | Loss: 0.00001607
Iteration 5/1000 | Loss: 0.00001510
Iteration 6/1000 | Loss: 0.00001448
Iteration 7/1000 | Loss: 0.00001401
Iteration 8/1000 | Loss: 0.00001371
Iteration 9/1000 | Loss: 0.00001370
Iteration 10/1000 | Loss: 0.00001355
Iteration 11/1000 | Loss: 0.00001330
Iteration 12/1000 | Loss: 0.00001318
Iteration 13/1000 | Loss: 0.00001310
Iteration 14/1000 | Loss: 0.00001309
Iteration 15/1000 | Loss: 0.00001292
Iteration 16/1000 | Loss: 0.00001289
Iteration 17/1000 | Loss: 0.00001284
Iteration 18/1000 | Loss: 0.00001283
Iteration 19/1000 | Loss: 0.00001279
Iteration 20/1000 | Loss: 0.00001276
Iteration 21/1000 | Loss: 0.00001275
Iteration 22/1000 | Loss: 0.00001274
Iteration 23/1000 | Loss: 0.00001274
Iteration 24/1000 | Loss: 0.00001273
Iteration 25/1000 | Loss: 0.00001272
Iteration 26/1000 | Loss: 0.00001272
Iteration 27/1000 | Loss: 0.00001271
Iteration 28/1000 | Loss: 0.00001271
Iteration 29/1000 | Loss: 0.00001270
Iteration 30/1000 | Loss: 0.00001270
Iteration 31/1000 | Loss: 0.00001270
Iteration 32/1000 | Loss: 0.00001268
Iteration 33/1000 | Loss: 0.00001268
Iteration 34/1000 | Loss: 0.00001267
Iteration 35/1000 | Loss: 0.00001267
Iteration 36/1000 | Loss: 0.00001267
Iteration 37/1000 | Loss: 0.00001266
Iteration 38/1000 | Loss: 0.00001265
Iteration 39/1000 | Loss: 0.00001265
Iteration 40/1000 | Loss: 0.00001265
Iteration 41/1000 | Loss: 0.00001264
Iteration 42/1000 | Loss: 0.00001264
Iteration 43/1000 | Loss: 0.00001264
Iteration 44/1000 | Loss: 0.00001264
Iteration 45/1000 | Loss: 0.00001263
Iteration 46/1000 | Loss: 0.00001263
Iteration 47/1000 | Loss: 0.00001263
Iteration 48/1000 | Loss: 0.00001263
Iteration 49/1000 | Loss: 0.00001263
Iteration 50/1000 | Loss: 0.00001262
Iteration 51/1000 | Loss: 0.00001261
Iteration 52/1000 | Loss: 0.00001258
Iteration 53/1000 | Loss: 0.00001258
Iteration 54/1000 | Loss: 0.00001258
Iteration 55/1000 | Loss: 0.00001257
Iteration 56/1000 | Loss: 0.00001257
Iteration 57/1000 | Loss: 0.00001257
Iteration 58/1000 | Loss: 0.00001257
Iteration 59/1000 | Loss: 0.00001257
Iteration 60/1000 | Loss: 0.00001257
Iteration 61/1000 | Loss: 0.00001257
Iteration 62/1000 | Loss: 0.00001257
Iteration 63/1000 | Loss: 0.00001256
Iteration 64/1000 | Loss: 0.00001256
Iteration 65/1000 | Loss: 0.00001255
Iteration 66/1000 | Loss: 0.00001255
Iteration 67/1000 | Loss: 0.00001255
Iteration 68/1000 | Loss: 0.00001254
Iteration 69/1000 | Loss: 0.00001254
Iteration 70/1000 | Loss: 0.00001254
Iteration 71/1000 | Loss: 0.00001253
Iteration 72/1000 | Loss: 0.00001253
Iteration 73/1000 | Loss: 0.00001253
Iteration 74/1000 | Loss: 0.00001253
Iteration 75/1000 | Loss: 0.00001252
Iteration 76/1000 | Loss: 0.00001252
Iteration 77/1000 | Loss: 0.00001252
Iteration 78/1000 | Loss: 0.00001251
Iteration 79/1000 | Loss: 0.00001251
Iteration 80/1000 | Loss: 0.00001251
Iteration 81/1000 | Loss: 0.00001250
Iteration 82/1000 | Loss: 0.00001250
Iteration 83/1000 | Loss: 0.00001250
Iteration 84/1000 | Loss: 0.00001250
Iteration 85/1000 | Loss: 0.00001250
Iteration 86/1000 | Loss: 0.00001250
Iteration 87/1000 | Loss: 0.00001249
Iteration 88/1000 | Loss: 0.00001249
Iteration 89/1000 | Loss: 0.00001248
Iteration 90/1000 | Loss: 0.00001248
Iteration 91/1000 | Loss: 0.00001248
Iteration 92/1000 | Loss: 0.00001248
Iteration 93/1000 | Loss: 0.00001248
Iteration 94/1000 | Loss: 0.00001247
Iteration 95/1000 | Loss: 0.00001247
Iteration 96/1000 | Loss: 0.00001247
Iteration 97/1000 | Loss: 0.00001247
Iteration 98/1000 | Loss: 0.00001247
Iteration 99/1000 | Loss: 0.00001247
Iteration 100/1000 | Loss: 0.00001247
Iteration 101/1000 | Loss: 0.00001246
Iteration 102/1000 | Loss: 0.00001246
Iteration 103/1000 | Loss: 0.00001246
Iteration 104/1000 | Loss: 0.00001246
Iteration 105/1000 | Loss: 0.00001245
Iteration 106/1000 | Loss: 0.00001245
Iteration 107/1000 | Loss: 0.00001245
Iteration 108/1000 | Loss: 0.00001245
Iteration 109/1000 | Loss: 0.00001245
Iteration 110/1000 | Loss: 0.00001245
Iteration 111/1000 | Loss: 0.00001244
Iteration 112/1000 | Loss: 0.00001244
Iteration 113/1000 | Loss: 0.00001244
Iteration 114/1000 | Loss: 0.00001244
Iteration 115/1000 | Loss: 0.00001243
Iteration 116/1000 | Loss: 0.00001243
Iteration 117/1000 | Loss: 0.00001242
Iteration 118/1000 | Loss: 0.00001242
Iteration 119/1000 | Loss: 0.00001242
Iteration 120/1000 | Loss: 0.00001242
Iteration 121/1000 | Loss: 0.00001242
Iteration 122/1000 | Loss: 0.00001242
Iteration 123/1000 | Loss: 0.00001241
Iteration 124/1000 | Loss: 0.00001241
Iteration 125/1000 | Loss: 0.00001241
Iteration 126/1000 | Loss: 0.00001241
Iteration 127/1000 | Loss: 0.00001241
Iteration 128/1000 | Loss: 0.00001241
Iteration 129/1000 | Loss: 0.00001241
Iteration 130/1000 | Loss: 0.00001241
Iteration 131/1000 | Loss: 0.00001241
Iteration 132/1000 | Loss: 0.00001240
Iteration 133/1000 | Loss: 0.00001240
Iteration 134/1000 | Loss: 0.00001240
Iteration 135/1000 | Loss: 0.00001240
Iteration 136/1000 | Loss: 0.00001239
Iteration 137/1000 | Loss: 0.00001239
Iteration 138/1000 | Loss: 0.00001239
Iteration 139/1000 | Loss: 0.00001239
Iteration 140/1000 | Loss: 0.00001239
Iteration 141/1000 | Loss: 0.00001238
Iteration 142/1000 | Loss: 0.00001238
Iteration 143/1000 | Loss: 0.00001238
Iteration 144/1000 | Loss: 0.00001238
Iteration 145/1000 | Loss: 0.00001238
Iteration 146/1000 | Loss: 0.00001238
Iteration 147/1000 | Loss: 0.00001238
Iteration 148/1000 | Loss: 0.00001238
Iteration 149/1000 | Loss: 0.00001238
Iteration 150/1000 | Loss: 0.00001238
Iteration 151/1000 | Loss: 0.00001238
Iteration 152/1000 | Loss: 0.00001238
Iteration 153/1000 | Loss: 0.00001238
Iteration 154/1000 | Loss: 0.00001238
Iteration 155/1000 | Loss: 0.00001238
Iteration 156/1000 | Loss: 0.00001238
Iteration 157/1000 | Loss: 0.00001238
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.2379678992147092e-05, 1.2379678992147092e-05, 1.2379678992147092e-05, 1.2379678992147092e-05, 1.2379678992147092e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2379678992147092e-05

Optimization complete. Final v2v error: 3.0340216159820557 mm

Highest mean error: 3.2842013835906982 mm for frame 79

Lowest mean error: 2.8598098754882812 mm for frame 97

Saving results

Total time: 38.979002952575684
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00422359
Iteration 2/25 | Loss: 0.00141201
Iteration 3/25 | Loss: 0.00132060
Iteration 4/25 | Loss: 0.00130460
Iteration 5/25 | Loss: 0.00130060
Iteration 6/25 | Loss: 0.00129952
Iteration 7/25 | Loss: 0.00129943
Iteration 8/25 | Loss: 0.00129943
Iteration 9/25 | Loss: 0.00129943
Iteration 10/25 | Loss: 0.00129943
Iteration 11/25 | Loss: 0.00129943
Iteration 12/25 | Loss: 0.00129943
Iteration 13/25 | Loss: 0.00129943
Iteration 14/25 | Loss: 0.00129943
Iteration 15/25 | Loss: 0.00129943
Iteration 16/25 | Loss: 0.00129943
Iteration 17/25 | Loss: 0.00129943
Iteration 18/25 | Loss: 0.00129943
Iteration 19/25 | Loss: 0.00129943
Iteration 20/25 | Loss: 0.00129943
Iteration 21/25 | Loss: 0.00129943
Iteration 22/25 | Loss: 0.00129943
Iteration 23/25 | Loss: 0.00129943
Iteration 24/25 | Loss: 0.00129943
Iteration 25/25 | Loss: 0.00129943

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44702852
Iteration 2/25 | Loss: 0.00083805
Iteration 3/25 | Loss: 0.00083805
Iteration 4/25 | Loss: 0.00083805
Iteration 5/25 | Loss: 0.00083804
Iteration 6/25 | Loss: 0.00083804
Iteration 7/25 | Loss: 0.00083804
Iteration 8/25 | Loss: 0.00083804
Iteration 9/25 | Loss: 0.00083804
Iteration 10/25 | Loss: 0.00083804
Iteration 11/25 | Loss: 0.00083804
Iteration 12/25 | Loss: 0.00083804
Iteration 13/25 | Loss: 0.00083804
Iteration 14/25 | Loss: 0.00083804
Iteration 15/25 | Loss: 0.00083804
Iteration 16/25 | Loss: 0.00083804
Iteration 17/25 | Loss: 0.00083804
Iteration 18/25 | Loss: 0.00083804
Iteration 19/25 | Loss: 0.00083804
Iteration 20/25 | Loss: 0.00083804
Iteration 21/25 | Loss: 0.00083804
Iteration 22/25 | Loss: 0.00083804
Iteration 23/25 | Loss: 0.00083804
Iteration 24/25 | Loss: 0.00083804
Iteration 25/25 | Loss: 0.00083804

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083804
Iteration 2/1000 | Loss: 0.00004551
Iteration 3/1000 | Loss: 0.00002773
Iteration 4/1000 | Loss: 0.00002254
Iteration 5/1000 | Loss: 0.00002100
Iteration 6/1000 | Loss: 0.00001998
Iteration 7/1000 | Loss: 0.00001929
Iteration 8/1000 | Loss: 0.00001878
Iteration 9/1000 | Loss: 0.00001837
Iteration 10/1000 | Loss: 0.00001807
Iteration 11/1000 | Loss: 0.00001795
Iteration 12/1000 | Loss: 0.00001778
Iteration 13/1000 | Loss: 0.00001761
Iteration 14/1000 | Loss: 0.00001757
Iteration 15/1000 | Loss: 0.00001755
Iteration 16/1000 | Loss: 0.00001752
Iteration 17/1000 | Loss: 0.00001751
Iteration 18/1000 | Loss: 0.00001748
Iteration 19/1000 | Loss: 0.00001744
Iteration 20/1000 | Loss: 0.00001744
Iteration 21/1000 | Loss: 0.00001740
Iteration 22/1000 | Loss: 0.00001736
Iteration 23/1000 | Loss: 0.00001733
Iteration 24/1000 | Loss: 0.00001732
Iteration 25/1000 | Loss: 0.00001731
Iteration 26/1000 | Loss: 0.00001730
Iteration 27/1000 | Loss: 0.00001724
Iteration 28/1000 | Loss: 0.00001724
Iteration 29/1000 | Loss: 0.00001723
Iteration 30/1000 | Loss: 0.00001723
Iteration 31/1000 | Loss: 0.00001723
Iteration 32/1000 | Loss: 0.00001722
Iteration 33/1000 | Loss: 0.00001722
Iteration 34/1000 | Loss: 0.00001720
Iteration 35/1000 | Loss: 0.00001720
Iteration 36/1000 | Loss: 0.00001719
Iteration 37/1000 | Loss: 0.00001719
Iteration 38/1000 | Loss: 0.00001718
Iteration 39/1000 | Loss: 0.00001718
Iteration 40/1000 | Loss: 0.00001718
Iteration 41/1000 | Loss: 0.00001718
Iteration 42/1000 | Loss: 0.00001718
Iteration 43/1000 | Loss: 0.00001718
Iteration 44/1000 | Loss: 0.00001718
Iteration 45/1000 | Loss: 0.00001718
Iteration 46/1000 | Loss: 0.00001718
Iteration 47/1000 | Loss: 0.00001717
Iteration 48/1000 | Loss: 0.00001717
Iteration 49/1000 | Loss: 0.00001717
Iteration 50/1000 | Loss: 0.00001717
Iteration 51/1000 | Loss: 0.00001717
Iteration 52/1000 | Loss: 0.00001717
Iteration 53/1000 | Loss: 0.00001717
Iteration 54/1000 | Loss: 0.00001717
Iteration 55/1000 | Loss: 0.00001716
Iteration 56/1000 | Loss: 0.00001716
Iteration 57/1000 | Loss: 0.00001716
Iteration 58/1000 | Loss: 0.00001716
Iteration 59/1000 | Loss: 0.00001716
Iteration 60/1000 | Loss: 0.00001716
Iteration 61/1000 | Loss: 0.00001715
Iteration 62/1000 | Loss: 0.00001715
Iteration 63/1000 | Loss: 0.00001715
Iteration 64/1000 | Loss: 0.00001715
Iteration 65/1000 | Loss: 0.00001715
Iteration 66/1000 | Loss: 0.00001715
Iteration 67/1000 | Loss: 0.00001715
Iteration 68/1000 | Loss: 0.00001715
Iteration 69/1000 | Loss: 0.00001714
Iteration 70/1000 | Loss: 0.00001714
Iteration 71/1000 | Loss: 0.00001714
Iteration 72/1000 | Loss: 0.00001713
Iteration 73/1000 | Loss: 0.00001713
Iteration 74/1000 | Loss: 0.00001713
Iteration 75/1000 | Loss: 0.00001713
Iteration 76/1000 | Loss: 0.00001713
Iteration 77/1000 | Loss: 0.00001712
Iteration 78/1000 | Loss: 0.00001712
Iteration 79/1000 | Loss: 0.00001712
Iteration 80/1000 | Loss: 0.00001712
Iteration 81/1000 | Loss: 0.00001711
Iteration 82/1000 | Loss: 0.00001711
Iteration 83/1000 | Loss: 0.00001711
Iteration 84/1000 | Loss: 0.00001710
Iteration 85/1000 | Loss: 0.00001710
Iteration 86/1000 | Loss: 0.00001710
Iteration 87/1000 | Loss: 0.00001710
Iteration 88/1000 | Loss: 0.00001710
Iteration 89/1000 | Loss: 0.00001709
Iteration 90/1000 | Loss: 0.00001709
Iteration 91/1000 | Loss: 0.00001709
Iteration 92/1000 | Loss: 0.00001709
Iteration 93/1000 | Loss: 0.00001709
Iteration 94/1000 | Loss: 0.00001708
Iteration 95/1000 | Loss: 0.00001708
Iteration 96/1000 | Loss: 0.00001707
Iteration 97/1000 | Loss: 0.00001707
Iteration 98/1000 | Loss: 0.00001706
Iteration 99/1000 | Loss: 0.00001706
Iteration 100/1000 | Loss: 0.00001706
Iteration 101/1000 | Loss: 0.00001706
Iteration 102/1000 | Loss: 0.00001705
Iteration 103/1000 | Loss: 0.00001705
Iteration 104/1000 | Loss: 0.00001705
Iteration 105/1000 | Loss: 0.00001705
Iteration 106/1000 | Loss: 0.00001704
Iteration 107/1000 | Loss: 0.00001704
Iteration 108/1000 | Loss: 0.00001704
Iteration 109/1000 | Loss: 0.00001704
Iteration 110/1000 | Loss: 0.00001704
Iteration 111/1000 | Loss: 0.00001704
Iteration 112/1000 | Loss: 0.00001704
Iteration 113/1000 | Loss: 0.00001703
Iteration 114/1000 | Loss: 0.00001703
Iteration 115/1000 | Loss: 0.00001703
Iteration 116/1000 | Loss: 0.00001703
Iteration 117/1000 | Loss: 0.00001703
Iteration 118/1000 | Loss: 0.00001703
Iteration 119/1000 | Loss: 0.00001703
Iteration 120/1000 | Loss: 0.00001703
Iteration 121/1000 | Loss: 0.00001702
Iteration 122/1000 | Loss: 0.00001702
Iteration 123/1000 | Loss: 0.00001702
Iteration 124/1000 | Loss: 0.00001702
Iteration 125/1000 | Loss: 0.00001702
Iteration 126/1000 | Loss: 0.00001702
Iteration 127/1000 | Loss: 0.00001701
Iteration 128/1000 | Loss: 0.00001701
Iteration 129/1000 | Loss: 0.00001701
Iteration 130/1000 | Loss: 0.00001701
Iteration 131/1000 | Loss: 0.00001701
Iteration 132/1000 | Loss: 0.00001701
Iteration 133/1000 | Loss: 0.00001701
Iteration 134/1000 | Loss: 0.00001701
Iteration 135/1000 | Loss: 0.00001701
Iteration 136/1000 | Loss: 0.00001701
Iteration 137/1000 | Loss: 0.00001700
Iteration 138/1000 | Loss: 0.00001700
Iteration 139/1000 | Loss: 0.00001700
Iteration 140/1000 | Loss: 0.00001700
Iteration 141/1000 | Loss: 0.00001700
Iteration 142/1000 | Loss: 0.00001700
Iteration 143/1000 | Loss: 0.00001700
Iteration 144/1000 | Loss: 0.00001700
Iteration 145/1000 | Loss: 0.00001700
Iteration 146/1000 | Loss: 0.00001700
Iteration 147/1000 | Loss: 0.00001700
Iteration 148/1000 | Loss: 0.00001700
Iteration 149/1000 | Loss: 0.00001700
Iteration 150/1000 | Loss: 0.00001700
Iteration 151/1000 | Loss: 0.00001700
Iteration 152/1000 | Loss: 0.00001700
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.700300163065549e-05, 1.700300163065549e-05, 1.700300163065549e-05, 1.700300163065549e-05, 1.700300163065549e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.700300163065549e-05

Optimization complete. Final v2v error: 3.4515469074249268 mm

Highest mean error: 4.75008487701416 mm for frame 54

Lowest mean error: 2.9046096801757812 mm for frame 114

Saving results

Total time: 41.756402254104614
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00376014
Iteration 2/25 | Loss: 0.00127131
Iteration 3/25 | Loss: 0.00123183
Iteration 4/25 | Loss: 0.00122613
Iteration 5/25 | Loss: 0.00122479
Iteration 6/25 | Loss: 0.00122479
Iteration 7/25 | Loss: 0.00122479
Iteration 8/25 | Loss: 0.00122479
Iteration 9/25 | Loss: 0.00122479
Iteration 10/25 | Loss: 0.00122479
Iteration 11/25 | Loss: 0.00122479
Iteration 12/25 | Loss: 0.00122479
Iteration 13/25 | Loss: 0.00122479
Iteration 14/25 | Loss: 0.00122479
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012247900012880564, 0.0012247900012880564, 0.0012247900012880564, 0.0012247900012880564, 0.0012247900012880564]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012247900012880564

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43680549
Iteration 2/25 | Loss: 0.00080870
Iteration 3/25 | Loss: 0.00080870
Iteration 4/25 | Loss: 0.00080870
Iteration 5/25 | Loss: 0.00080870
Iteration 6/25 | Loss: 0.00080870
Iteration 7/25 | Loss: 0.00080870
Iteration 8/25 | Loss: 0.00080870
Iteration 9/25 | Loss: 0.00080870
Iteration 10/25 | Loss: 0.00080870
Iteration 11/25 | Loss: 0.00080870
Iteration 12/25 | Loss: 0.00080870
Iteration 13/25 | Loss: 0.00080870
Iteration 14/25 | Loss: 0.00080870
Iteration 15/25 | Loss: 0.00080870
Iteration 16/25 | Loss: 0.00080870
Iteration 17/25 | Loss: 0.00080870
Iteration 18/25 | Loss: 0.00080870
Iteration 19/25 | Loss: 0.00080870
Iteration 20/25 | Loss: 0.00080870
Iteration 21/25 | Loss: 0.00080870
Iteration 22/25 | Loss: 0.00080870
Iteration 23/25 | Loss: 0.00080870
Iteration 24/25 | Loss: 0.00080870
Iteration 25/25 | Loss: 0.00080870

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080870
Iteration 2/1000 | Loss: 0.00002306
Iteration 3/1000 | Loss: 0.00001556
Iteration 4/1000 | Loss: 0.00001409
Iteration 5/1000 | Loss: 0.00001329
Iteration 6/1000 | Loss: 0.00001280
Iteration 7/1000 | Loss: 0.00001243
Iteration 8/1000 | Loss: 0.00001243
Iteration 9/1000 | Loss: 0.00001232
Iteration 10/1000 | Loss: 0.00001214
Iteration 11/1000 | Loss: 0.00001192
Iteration 12/1000 | Loss: 0.00001182
Iteration 13/1000 | Loss: 0.00001179
Iteration 14/1000 | Loss: 0.00001179
Iteration 15/1000 | Loss: 0.00001174
Iteration 16/1000 | Loss: 0.00001173
Iteration 17/1000 | Loss: 0.00001173
Iteration 18/1000 | Loss: 0.00001166
Iteration 19/1000 | Loss: 0.00001165
Iteration 20/1000 | Loss: 0.00001164
Iteration 21/1000 | Loss: 0.00001161
Iteration 22/1000 | Loss: 0.00001157
Iteration 23/1000 | Loss: 0.00001156
Iteration 24/1000 | Loss: 0.00001155
Iteration 25/1000 | Loss: 0.00001155
Iteration 26/1000 | Loss: 0.00001155
Iteration 27/1000 | Loss: 0.00001154
Iteration 28/1000 | Loss: 0.00001151
Iteration 29/1000 | Loss: 0.00001150
Iteration 30/1000 | Loss: 0.00001150
Iteration 31/1000 | Loss: 0.00001149
Iteration 32/1000 | Loss: 0.00001149
Iteration 33/1000 | Loss: 0.00001149
Iteration 34/1000 | Loss: 0.00001148
Iteration 35/1000 | Loss: 0.00001147
Iteration 36/1000 | Loss: 0.00001147
Iteration 37/1000 | Loss: 0.00001147
Iteration 38/1000 | Loss: 0.00001147
Iteration 39/1000 | Loss: 0.00001147
Iteration 40/1000 | Loss: 0.00001147
Iteration 41/1000 | Loss: 0.00001147
Iteration 42/1000 | Loss: 0.00001146
Iteration 43/1000 | Loss: 0.00001146
Iteration 44/1000 | Loss: 0.00001146
Iteration 45/1000 | Loss: 0.00001146
Iteration 46/1000 | Loss: 0.00001146
Iteration 47/1000 | Loss: 0.00001145
Iteration 48/1000 | Loss: 0.00001145
Iteration 49/1000 | Loss: 0.00001145
Iteration 50/1000 | Loss: 0.00001145
Iteration 51/1000 | Loss: 0.00001144
Iteration 52/1000 | Loss: 0.00001144
Iteration 53/1000 | Loss: 0.00001143
Iteration 54/1000 | Loss: 0.00001143
Iteration 55/1000 | Loss: 0.00001143
Iteration 56/1000 | Loss: 0.00001143
Iteration 57/1000 | Loss: 0.00001143
Iteration 58/1000 | Loss: 0.00001143
Iteration 59/1000 | Loss: 0.00001143
Iteration 60/1000 | Loss: 0.00001142
Iteration 61/1000 | Loss: 0.00001142
Iteration 62/1000 | Loss: 0.00001142
Iteration 63/1000 | Loss: 0.00001142
Iteration 64/1000 | Loss: 0.00001142
Iteration 65/1000 | Loss: 0.00001142
Iteration 66/1000 | Loss: 0.00001141
Iteration 67/1000 | Loss: 0.00001141
Iteration 68/1000 | Loss: 0.00001141
Iteration 69/1000 | Loss: 0.00001140
Iteration 70/1000 | Loss: 0.00001140
Iteration 71/1000 | Loss: 0.00001139
Iteration 72/1000 | Loss: 0.00001138
Iteration 73/1000 | Loss: 0.00001137
Iteration 74/1000 | Loss: 0.00001137
Iteration 75/1000 | Loss: 0.00001136
Iteration 76/1000 | Loss: 0.00001136
Iteration 77/1000 | Loss: 0.00001135
Iteration 78/1000 | Loss: 0.00001135
Iteration 79/1000 | Loss: 0.00001134
Iteration 80/1000 | Loss: 0.00001133
Iteration 81/1000 | Loss: 0.00001133
Iteration 82/1000 | Loss: 0.00001132
Iteration 83/1000 | Loss: 0.00001132
Iteration 84/1000 | Loss: 0.00001132
Iteration 85/1000 | Loss: 0.00001131
Iteration 86/1000 | Loss: 0.00001131
Iteration 87/1000 | Loss: 0.00001131
Iteration 88/1000 | Loss: 0.00001131
Iteration 89/1000 | Loss: 0.00001131
Iteration 90/1000 | Loss: 0.00001131
Iteration 91/1000 | Loss: 0.00001131
Iteration 92/1000 | Loss: 0.00001131
Iteration 93/1000 | Loss: 0.00001131
Iteration 94/1000 | Loss: 0.00001131
Iteration 95/1000 | Loss: 0.00001131
Iteration 96/1000 | Loss: 0.00001130
Iteration 97/1000 | Loss: 0.00001130
Iteration 98/1000 | Loss: 0.00001130
Iteration 99/1000 | Loss: 0.00001129
Iteration 100/1000 | Loss: 0.00001129
Iteration 101/1000 | Loss: 0.00001129
Iteration 102/1000 | Loss: 0.00001128
Iteration 103/1000 | Loss: 0.00001128
Iteration 104/1000 | Loss: 0.00001128
Iteration 105/1000 | Loss: 0.00001127
Iteration 106/1000 | Loss: 0.00001127
Iteration 107/1000 | Loss: 0.00001127
Iteration 108/1000 | Loss: 0.00001127
Iteration 109/1000 | Loss: 0.00001127
Iteration 110/1000 | Loss: 0.00001127
Iteration 111/1000 | Loss: 0.00001127
Iteration 112/1000 | Loss: 0.00001127
Iteration 113/1000 | Loss: 0.00001127
Iteration 114/1000 | Loss: 0.00001127
Iteration 115/1000 | Loss: 0.00001126
Iteration 116/1000 | Loss: 0.00001126
Iteration 117/1000 | Loss: 0.00001126
Iteration 118/1000 | Loss: 0.00001125
Iteration 119/1000 | Loss: 0.00001125
Iteration 120/1000 | Loss: 0.00001125
Iteration 121/1000 | Loss: 0.00001125
Iteration 122/1000 | Loss: 0.00001124
Iteration 123/1000 | Loss: 0.00001124
Iteration 124/1000 | Loss: 0.00001124
Iteration 125/1000 | Loss: 0.00001124
Iteration 126/1000 | Loss: 0.00001123
Iteration 127/1000 | Loss: 0.00001123
Iteration 128/1000 | Loss: 0.00001122
Iteration 129/1000 | Loss: 0.00001121
Iteration 130/1000 | Loss: 0.00001121
Iteration 131/1000 | Loss: 0.00001121
Iteration 132/1000 | Loss: 0.00001120
Iteration 133/1000 | Loss: 0.00001120
Iteration 134/1000 | Loss: 0.00001120
Iteration 135/1000 | Loss: 0.00001120
Iteration 136/1000 | Loss: 0.00001120
Iteration 137/1000 | Loss: 0.00001120
Iteration 138/1000 | Loss: 0.00001120
Iteration 139/1000 | Loss: 0.00001120
Iteration 140/1000 | Loss: 0.00001120
Iteration 141/1000 | Loss: 0.00001120
Iteration 142/1000 | Loss: 0.00001119
Iteration 143/1000 | Loss: 0.00001119
Iteration 144/1000 | Loss: 0.00001119
Iteration 145/1000 | Loss: 0.00001119
Iteration 146/1000 | Loss: 0.00001119
Iteration 147/1000 | Loss: 0.00001119
Iteration 148/1000 | Loss: 0.00001118
Iteration 149/1000 | Loss: 0.00001118
Iteration 150/1000 | Loss: 0.00001118
Iteration 151/1000 | Loss: 0.00001117
Iteration 152/1000 | Loss: 0.00001117
Iteration 153/1000 | Loss: 0.00001117
Iteration 154/1000 | Loss: 0.00001116
Iteration 155/1000 | Loss: 0.00001116
Iteration 156/1000 | Loss: 0.00001116
Iteration 157/1000 | Loss: 0.00001116
Iteration 158/1000 | Loss: 0.00001116
Iteration 159/1000 | Loss: 0.00001116
Iteration 160/1000 | Loss: 0.00001116
Iteration 161/1000 | Loss: 0.00001116
Iteration 162/1000 | Loss: 0.00001116
Iteration 163/1000 | Loss: 0.00001116
Iteration 164/1000 | Loss: 0.00001116
Iteration 165/1000 | Loss: 0.00001116
Iteration 166/1000 | Loss: 0.00001116
Iteration 167/1000 | Loss: 0.00001116
Iteration 168/1000 | Loss: 0.00001116
Iteration 169/1000 | Loss: 0.00001116
Iteration 170/1000 | Loss: 0.00001116
Iteration 171/1000 | Loss: 0.00001116
Iteration 172/1000 | Loss: 0.00001116
Iteration 173/1000 | Loss: 0.00001116
Iteration 174/1000 | Loss: 0.00001116
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [1.11598437797511e-05, 1.11598437797511e-05, 1.11598437797511e-05, 1.11598437797511e-05, 1.11598437797511e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.11598437797511e-05

Optimization complete. Final v2v error: 2.8777077198028564 mm

Highest mean error: 2.964733839035034 mm for frame 139

Lowest mean error: 2.7941837310791016 mm for frame 151

Saving results

Total time: 38.0274612903595
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00804335
Iteration 2/25 | Loss: 0.00151887
Iteration 3/25 | Loss: 0.00131598
Iteration 4/25 | Loss: 0.00130043
Iteration 5/25 | Loss: 0.00129762
Iteration 6/25 | Loss: 0.00129675
Iteration 7/25 | Loss: 0.00129633
Iteration 8/25 | Loss: 0.00129603
Iteration 9/25 | Loss: 0.00129814
Iteration 10/25 | Loss: 0.00129814
Iteration 11/25 | Loss: 0.00129982
Iteration 12/25 | Loss: 0.00129831
Iteration 13/25 | Loss: 0.00129182
Iteration 14/25 | Loss: 0.00129079
Iteration 15/25 | Loss: 0.00128966
Iteration 16/25 | Loss: 0.00129044
Iteration 17/25 | Loss: 0.00128991
Iteration 18/25 | Loss: 0.00129036
Iteration 19/25 | Loss: 0.00128965
Iteration 20/25 | Loss: 0.00128958
Iteration 21/25 | Loss: 0.00128942
Iteration 22/25 | Loss: 0.00128841
Iteration 23/25 | Loss: 0.00128827
Iteration 24/25 | Loss: 0.00128881
Iteration 25/25 | Loss: 0.00128802

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42717409
Iteration 2/25 | Loss: 0.00086773
Iteration 3/25 | Loss: 0.00086773
Iteration 4/25 | Loss: 0.00086773
Iteration 5/25 | Loss: 0.00086773
Iteration 6/25 | Loss: 0.00086773
Iteration 7/25 | Loss: 0.00086773
Iteration 8/25 | Loss: 0.00086773
Iteration 9/25 | Loss: 0.00086773
Iteration 10/25 | Loss: 0.00086773
Iteration 11/25 | Loss: 0.00086773
Iteration 12/25 | Loss: 0.00086773
Iteration 13/25 | Loss: 0.00086773
Iteration 14/25 | Loss: 0.00086773
Iteration 15/25 | Loss: 0.00086773
Iteration 16/25 | Loss: 0.00086773
Iteration 17/25 | Loss: 0.00086773
Iteration 18/25 | Loss: 0.00086773
Iteration 19/25 | Loss: 0.00086773
Iteration 20/25 | Loss: 0.00086773
Iteration 21/25 | Loss: 0.00086773
Iteration 22/25 | Loss: 0.00086773
Iteration 23/25 | Loss: 0.00086773
Iteration 24/25 | Loss: 0.00086773
Iteration 25/25 | Loss: 0.00086773

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086773
Iteration 2/1000 | Loss: 0.00003738
Iteration 3/1000 | Loss: 0.00003218
Iteration 4/1000 | Loss: 0.00003766
Iteration 5/1000 | Loss: 0.00002988
Iteration 6/1000 | Loss: 0.00002636
Iteration 7/1000 | Loss: 0.00002664
Iteration 8/1000 | Loss: 0.00003623
Iteration 9/1000 | Loss: 0.00002473
Iteration 10/1000 | Loss: 0.00002070
Iteration 11/1000 | Loss: 0.00002016
Iteration 12/1000 | Loss: 0.00003685
Iteration 13/1000 | Loss: 0.00002844
Iteration 14/1000 | Loss: 0.00004596
Iteration 15/1000 | Loss: 0.00003285
Iteration 16/1000 | Loss: 0.00002703
Iteration 17/1000 | Loss: 0.00003258
Iteration 18/1000 | Loss: 0.00003138
Iteration 19/1000 | Loss: 0.00002563
Iteration 20/1000 | Loss: 0.00002768
Iteration 21/1000 | Loss: 0.00002408
Iteration 22/1000 | Loss: 0.00002925
Iteration 23/1000 | Loss: 0.00002896
Iteration 24/1000 | Loss: 0.00002808
Iteration 25/1000 | Loss: 0.00003479
Iteration 26/1000 | Loss: 0.00003244
Iteration 27/1000 | Loss: 0.00003199
Iteration 28/1000 | Loss: 0.00003299
Iteration 29/1000 | Loss: 0.00002012
Iteration 30/1000 | Loss: 0.00001837
Iteration 31/1000 | Loss: 0.00001572
Iteration 32/1000 | Loss: 0.00001487
Iteration 33/1000 | Loss: 0.00001424
Iteration 34/1000 | Loss: 0.00001420
Iteration 35/1000 | Loss: 0.00001411
Iteration 36/1000 | Loss: 0.00001391
Iteration 37/1000 | Loss: 0.00001370
Iteration 38/1000 | Loss: 0.00001369
Iteration 39/1000 | Loss: 0.00001362
Iteration 40/1000 | Loss: 0.00001362
Iteration 41/1000 | Loss: 0.00001361
Iteration 42/1000 | Loss: 0.00001361
Iteration 43/1000 | Loss: 0.00001359
Iteration 44/1000 | Loss: 0.00001359
Iteration 45/1000 | Loss: 0.00001358
Iteration 46/1000 | Loss: 0.00001347
Iteration 47/1000 | Loss: 0.00001343
Iteration 48/1000 | Loss: 0.00001338
Iteration 49/1000 | Loss: 0.00001336
Iteration 50/1000 | Loss: 0.00001336
Iteration 51/1000 | Loss: 0.00001336
Iteration 52/1000 | Loss: 0.00001336
Iteration 53/1000 | Loss: 0.00001335
Iteration 54/1000 | Loss: 0.00001335
Iteration 55/1000 | Loss: 0.00001335
Iteration 56/1000 | Loss: 0.00001335
Iteration 57/1000 | Loss: 0.00001335
Iteration 58/1000 | Loss: 0.00001335
Iteration 59/1000 | Loss: 0.00001335
Iteration 60/1000 | Loss: 0.00001335
Iteration 61/1000 | Loss: 0.00001335
Iteration 62/1000 | Loss: 0.00001335
Iteration 63/1000 | Loss: 0.00001334
Iteration 64/1000 | Loss: 0.00001334
Iteration 65/1000 | Loss: 0.00001334
Iteration 66/1000 | Loss: 0.00001332
Iteration 67/1000 | Loss: 0.00001332
Iteration 68/1000 | Loss: 0.00001332
Iteration 69/1000 | Loss: 0.00001332
Iteration 70/1000 | Loss: 0.00001332
Iteration 71/1000 | Loss: 0.00001331
Iteration 72/1000 | Loss: 0.00001331
Iteration 73/1000 | Loss: 0.00001331
Iteration 74/1000 | Loss: 0.00001331
Iteration 75/1000 | Loss: 0.00001331
Iteration 76/1000 | Loss: 0.00001330
Iteration 77/1000 | Loss: 0.00001329
Iteration 78/1000 | Loss: 0.00001329
Iteration 79/1000 | Loss: 0.00001328
Iteration 80/1000 | Loss: 0.00001328
Iteration 81/1000 | Loss: 0.00001328
Iteration 82/1000 | Loss: 0.00001328
Iteration 83/1000 | Loss: 0.00001328
Iteration 84/1000 | Loss: 0.00001327
Iteration 85/1000 | Loss: 0.00001327
Iteration 86/1000 | Loss: 0.00001327
Iteration 87/1000 | Loss: 0.00001326
Iteration 88/1000 | Loss: 0.00001325
Iteration 89/1000 | Loss: 0.00001325
Iteration 90/1000 | Loss: 0.00001325
Iteration 91/1000 | Loss: 0.00001325
Iteration 92/1000 | Loss: 0.00001324
Iteration 93/1000 | Loss: 0.00001324
Iteration 94/1000 | Loss: 0.00001324
Iteration 95/1000 | Loss: 0.00001324
Iteration 96/1000 | Loss: 0.00001324
Iteration 97/1000 | Loss: 0.00001324
Iteration 98/1000 | Loss: 0.00001324
Iteration 99/1000 | Loss: 0.00001324
Iteration 100/1000 | Loss: 0.00001324
Iteration 101/1000 | Loss: 0.00001323
Iteration 102/1000 | Loss: 0.00001323
Iteration 103/1000 | Loss: 0.00001323
Iteration 104/1000 | Loss: 0.00001323
Iteration 105/1000 | Loss: 0.00001323
Iteration 106/1000 | Loss: 0.00001323
Iteration 107/1000 | Loss: 0.00001323
Iteration 108/1000 | Loss: 0.00001323
Iteration 109/1000 | Loss: 0.00001322
Iteration 110/1000 | Loss: 0.00001322
Iteration 111/1000 | Loss: 0.00001322
Iteration 112/1000 | Loss: 0.00001322
Iteration 113/1000 | Loss: 0.00001322
Iteration 114/1000 | Loss: 0.00001322
Iteration 115/1000 | Loss: 0.00001322
Iteration 116/1000 | Loss: 0.00001321
Iteration 117/1000 | Loss: 0.00001321
Iteration 118/1000 | Loss: 0.00001321
Iteration 119/1000 | Loss: 0.00001321
Iteration 120/1000 | Loss: 0.00001321
Iteration 121/1000 | Loss: 0.00001321
Iteration 122/1000 | Loss: 0.00001321
Iteration 123/1000 | Loss: 0.00001320
Iteration 124/1000 | Loss: 0.00001320
Iteration 125/1000 | Loss: 0.00001320
Iteration 126/1000 | Loss: 0.00001320
Iteration 127/1000 | Loss: 0.00001319
Iteration 128/1000 | Loss: 0.00001319
Iteration 129/1000 | Loss: 0.00001319
Iteration 130/1000 | Loss: 0.00001319
Iteration 131/1000 | Loss: 0.00001319
Iteration 132/1000 | Loss: 0.00001318
Iteration 133/1000 | Loss: 0.00001318
Iteration 134/1000 | Loss: 0.00001318
Iteration 135/1000 | Loss: 0.00001318
Iteration 136/1000 | Loss: 0.00001318
Iteration 137/1000 | Loss: 0.00001318
Iteration 138/1000 | Loss: 0.00001318
Iteration 139/1000 | Loss: 0.00001318
Iteration 140/1000 | Loss: 0.00001318
Iteration 141/1000 | Loss: 0.00001318
Iteration 142/1000 | Loss: 0.00001318
Iteration 143/1000 | Loss: 0.00001318
Iteration 144/1000 | Loss: 0.00001317
Iteration 145/1000 | Loss: 0.00001317
Iteration 146/1000 | Loss: 0.00001317
Iteration 147/1000 | Loss: 0.00001316
Iteration 148/1000 | Loss: 0.00001316
Iteration 149/1000 | Loss: 0.00001316
Iteration 150/1000 | Loss: 0.00001316
Iteration 151/1000 | Loss: 0.00001315
Iteration 152/1000 | Loss: 0.00001315
Iteration 153/1000 | Loss: 0.00001314
Iteration 154/1000 | Loss: 0.00001314
Iteration 155/1000 | Loss: 0.00001314
Iteration 156/1000 | Loss: 0.00001313
Iteration 157/1000 | Loss: 0.00001313
Iteration 158/1000 | Loss: 0.00001313
Iteration 159/1000 | Loss: 0.00001313
Iteration 160/1000 | Loss: 0.00001313
Iteration 161/1000 | Loss: 0.00001312
Iteration 162/1000 | Loss: 0.00001312
Iteration 163/1000 | Loss: 0.00001312
Iteration 164/1000 | Loss: 0.00001311
Iteration 165/1000 | Loss: 0.00001311
Iteration 166/1000 | Loss: 0.00001311
Iteration 167/1000 | Loss: 0.00001311
Iteration 168/1000 | Loss: 0.00001310
Iteration 169/1000 | Loss: 0.00001310
Iteration 170/1000 | Loss: 0.00001310
Iteration 171/1000 | Loss: 0.00001310
Iteration 172/1000 | Loss: 0.00001310
Iteration 173/1000 | Loss: 0.00001309
Iteration 174/1000 | Loss: 0.00001309
Iteration 175/1000 | Loss: 0.00001309
Iteration 176/1000 | Loss: 0.00001309
Iteration 177/1000 | Loss: 0.00001308
Iteration 178/1000 | Loss: 0.00001308
Iteration 179/1000 | Loss: 0.00001308
Iteration 180/1000 | Loss: 0.00001308
Iteration 181/1000 | Loss: 0.00001308
Iteration 182/1000 | Loss: 0.00001308
Iteration 183/1000 | Loss: 0.00001308
Iteration 184/1000 | Loss: 0.00001308
Iteration 185/1000 | Loss: 0.00001307
Iteration 186/1000 | Loss: 0.00001307
Iteration 187/1000 | Loss: 0.00001307
Iteration 188/1000 | Loss: 0.00001307
Iteration 189/1000 | Loss: 0.00001307
Iteration 190/1000 | Loss: 0.00001307
Iteration 191/1000 | Loss: 0.00001307
Iteration 192/1000 | Loss: 0.00001307
Iteration 193/1000 | Loss: 0.00001306
Iteration 194/1000 | Loss: 0.00001306
Iteration 195/1000 | Loss: 0.00001306
Iteration 196/1000 | Loss: 0.00001306
Iteration 197/1000 | Loss: 0.00001306
Iteration 198/1000 | Loss: 0.00001306
Iteration 199/1000 | Loss: 0.00001306
Iteration 200/1000 | Loss: 0.00001306
Iteration 201/1000 | Loss: 0.00001305
Iteration 202/1000 | Loss: 0.00001305
Iteration 203/1000 | Loss: 0.00001305
Iteration 204/1000 | Loss: 0.00001305
Iteration 205/1000 | Loss: 0.00001305
Iteration 206/1000 | Loss: 0.00001305
Iteration 207/1000 | Loss: 0.00001305
Iteration 208/1000 | Loss: 0.00001305
Iteration 209/1000 | Loss: 0.00001305
Iteration 210/1000 | Loss: 0.00001304
Iteration 211/1000 | Loss: 0.00001304
Iteration 212/1000 | Loss: 0.00001304
Iteration 213/1000 | Loss: 0.00001304
Iteration 214/1000 | Loss: 0.00001304
Iteration 215/1000 | Loss: 0.00001303
Iteration 216/1000 | Loss: 0.00001303
Iteration 217/1000 | Loss: 0.00001303
Iteration 218/1000 | Loss: 0.00001303
Iteration 219/1000 | Loss: 0.00001303
Iteration 220/1000 | Loss: 0.00001303
Iteration 221/1000 | Loss: 0.00001303
Iteration 222/1000 | Loss: 0.00001303
Iteration 223/1000 | Loss: 0.00001303
Iteration 224/1000 | Loss: 0.00001303
Iteration 225/1000 | Loss: 0.00001302
Iteration 226/1000 | Loss: 0.00001302
Iteration 227/1000 | Loss: 0.00001302
Iteration 228/1000 | Loss: 0.00001302
Iteration 229/1000 | Loss: 0.00001302
Iteration 230/1000 | Loss: 0.00001302
Iteration 231/1000 | Loss: 0.00001302
Iteration 232/1000 | Loss: 0.00001301
Iteration 233/1000 | Loss: 0.00001301
Iteration 234/1000 | Loss: 0.00001301
Iteration 235/1000 | Loss: 0.00001301
Iteration 236/1000 | Loss: 0.00001301
Iteration 237/1000 | Loss: 0.00001301
Iteration 238/1000 | Loss: 0.00001301
Iteration 239/1000 | Loss: 0.00001301
Iteration 240/1000 | Loss: 0.00001301
Iteration 241/1000 | Loss: 0.00001301
Iteration 242/1000 | Loss: 0.00001301
Iteration 243/1000 | Loss: 0.00001301
Iteration 244/1000 | Loss: 0.00001301
Iteration 245/1000 | Loss: 0.00001301
Iteration 246/1000 | Loss: 0.00001301
Iteration 247/1000 | Loss: 0.00001301
Iteration 248/1000 | Loss: 0.00001301
Iteration 249/1000 | Loss: 0.00001301
Iteration 250/1000 | Loss: 0.00001300
Iteration 251/1000 | Loss: 0.00001300
Iteration 252/1000 | Loss: 0.00001300
Iteration 253/1000 | Loss: 0.00001300
Iteration 254/1000 | Loss: 0.00001300
Iteration 255/1000 | Loss: 0.00001300
Iteration 256/1000 | Loss: 0.00001300
Iteration 257/1000 | Loss: 0.00001300
Iteration 258/1000 | Loss: 0.00001300
Iteration 259/1000 | Loss: 0.00001300
Iteration 260/1000 | Loss: 0.00001300
Iteration 261/1000 | Loss: 0.00001300
Iteration 262/1000 | Loss: 0.00001300
Iteration 263/1000 | Loss: 0.00001300
Iteration 264/1000 | Loss: 0.00001300
Iteration 265/1000 | Loss: 0.00001300
Iteration 266/1000 | Loss: 0.00001300
Iteration 267/1000 | Loss: 0.00001300
Iteration 268/1000 | Loss: 0.00001300
Iteration 269/1000 | Loss: 0.00001300
Iteration 270/1000 | Loss: 0.00001299
Iteration 271/1000 | Loss: 0.00001299
Iteration 272/1000 | Loss: 0.00001299
Iteration 273/1000 | Loss: 0.00001299
Iteration 274/1000 | Loss: 0.00001299
Iteration 275/1000 | Loss: 0.00001299
Iteration 276/1000 | Loss: 0.00001299
Iteration 277/1000 | Loss: 0.00001299
Iteration 278/1000 | Loss: 0.00001299
Iteration 279/1000 | Loss: 0.00001299
Iteration 280/1000 | Loss: 0.00001299
Iteration 281/1000 | Loss: 0.00001299
Iteration 282/1000 | Loss: 0.00001299
Iteration 283/1000 | Loss: 0.00001299
Iteration 284/1000 | Loss: 0.00001299
Iteration 285/1000 | Loss: 0.00001299
Iteration 286/1000 | Loss: 0.00001299
Iteration 287/1000 | Loss: 0.00001298
Iteration 288/1000 | Loss: 0.00001298
Iteration 289/1000 | Loss: 0.00001298
Iteration 290/1000 | Loss: 0.00001298
Iteration 291/1000 | Loss: 0.00001298
Iteration 292/1000 | Loss: 0.00001298
Iteration 293/1000 | Loss: 0.00001297
Iteration 294/1000 | Loss: 0.00001297
Iteration 295/1000 | Loss: 0.00001297
Iteration 296/1000 | Loss: 0.00001297
Iteration 297/1000 | Loss: 0.00001297
Iteration 298/1000 | Loss: 0.00001297
Iteration 299/1000 | Loss: 0.00001297
Iteration 300/1000 | Loss: 0.00001297
Iteration 301/1000 | Loss: 0.00001297
Iteration 302/1000 | Loss: 0.00001297
Iteration 303/1000 | Loss: 0.00001297
Iteration 304/1000 | Loss: 0.00001297
Iteration 305/1000 | Loss: 0.00001297
Iteration 306/1000 | Loss: 0.00001297
Iteration 307/1000 | Loss: 0.00001297
Iteration 308/1000 | Loss: 0.00001297
Iteration 309/1000 | Loss: 0.00001297
Iteration 310/1000 | Loss: 0.00001297
Iteration 311/1000 | Loss: 0.00001297
Iteration 312/1000 | Loss: 0.00001296
Iteration 313/1000 | Loss: 0.00001296
Iteration 314/1000 | Loss: 0.00001296
Iteration 315/1000 | Loss: 0.00001296
Iteration 316/1000 | Loss: 0.00001296
Iteration 317/1000 | Loss: 0.00001296
Iteration 318/1000 | Loss: 0.00001296
Iteration 319/1000 | Loss: 0.00001296
Iteration 320/1000 | Loss: 0.00001296
Iteration 321/1000 | Loss: 0.00001296
Iteration 322/1000 | Loss: 0.00001296
Iteration 323/1000 | Loss: 0.00001296
Iteration 324/1000 | Loss: 0.00001296
Iteration 325/1000 | Loss: 0.00001296
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 325. Stopping optimization.
Last 5 losses: [1.2964013876626268e-05, 1.2964013876626268e-05, 1.2964013876626268e-05, 1.2964013876626268e-05, 1.2964013876626268e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2964013876626268e-05

Optimization complete. Final v2v error: 3.0825438499450684 mm

Highest mean error: 4.176571369171143 mm for frame 64

Lowest mean error: 2.9513983726501465 mm for frame 7

Saving results

Total time: 121.97645926475525
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00796199
Iteration 2/25 | Loss: 0.00145524
Iteration 3/25 | Loss: 0.00132745
Iteration 4/25 | Loss: 0.00130464
Iteration 5/25 | Loss: 0.00129942
Iteration 6/25 | Loss: 0.00129829
Iteration 7/25 | Loss: 0.00129811
Iteration 8/25 | Loss: 0.00129811
Iteration 9/25 | Loss: 0.00129811
Iteration 10/25 | Loss: 0.00129811
Iteration 11/25 | Loss: 0.00129811
Iteration 12/25 | Loss: 0.00129811
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012981080217286944, 0.0012981080217286944, 0.0012981080217286944, 0.0012981080217286944, 0.0012981080217286944]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012981080217286944

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.53984714
Iteration 2/25 | Loss: 0.00092606
Iteration 3/25 | Loss: 0.00092604
Iteration 4/25 | Loss: 0.00092603
Iteration 5/25 | Loss: 0.00092603
Iteration 6/25 | Loss: 0.00092603
Iteration 7/25 | Loss: 0.00092603
Iteration 8/25 | Loss: 0.00092603
Iteration 9/25 | Loss: 0.00092603
Iteration 10/25 | Loss: 0.00092603
Iteration 11/25 | Loss: 0.00092603
Iteration 12/25 | Loss: 0.00092603
Iteration 13/25 | Loss: 0.00092603
Iteration 14/25 | Loss: 0.00092603
Iteration 15/25 | Loss: 0.00092603
Iteration 16/25 | Loss: 0.00092603
Iteration 17/25 | Loss: 0.00092603
Iteration 18/25 | Loss: 0.00092603
Iteration 19/25 | Loss: 0.00092603
Iteration 20/25 | Loss: 0.00092603
Iteration 21/25 | Loss: 0.00092603
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009260319056920707, 0.0009260319056920707, 0.0009260319056920707, 0.0009260319056920707, 0.0009260319056920707]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009260319056920707

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092603
Iteration 2/1000 | Loss: 0.00006442
Iteration 3/1000 | Loss: 0.00003545
Iteration 4/1000 | Loss: 0.00002688
Iteration 5/1000 | Loss: 0.00002418
Iteration 6/1000 | Loss: 0.00002281
Iteration 7/1000 | Loss: 0.00002185
Iteration 8/1000 | Loss: 0.00002119
Iteration 9/1000 | Loss: 0.00002062
Iteration 10/1000 | Loss: 0.00002016
Iteration 11/1000 | Loss: 0.00001989
Iteration 12/1000 | Loss: 0.00001983
Iteration 13/1000 | Loss: 0.00001965
Iteration 14/1000 | Loss: 0.00001946
Iteration 15/1000 | Loss: 0.00001942
Iteration 16/1000 | Loss: 0.00001930
Iteration 17/1000 | Loss: 0.00001921
Iteration 18/1000 | Loss: 0.00001920
Iteration 19/1000 | Loss: 0.00001919
Iteration 20/1000 | Loss: 0.00001917
Iteration 21/1000 | Loss: 0.00001916
Iteration 22/1000 | Loss: 0.00001916
Iteration 23/1000 | Loss: 0.00001915
Iteration 24/1000 | Loss: 0.00001915
Iteration 25/1000 | Loss: 0.00001914
Iteration 26/1000 | Loss: 0.00001913
Iteration 27/1000 | Loss: 0.00001911
Iteration 28/1000 | Loss: 0.00001909
Iteration 29/1000 | Loss: 0.00001909
Iteration 30/1000 | Loss: 0.00001904
Iteration 31/1000 | Loss: 0.00001904
Iteration 32/1000 | Loss: 0.00001903
Iteration 33/1000 | Loss: 0.00001902
Iteration 34/1000 | Loss: 0.00001902
Iteration 35/1000 | Loss: 0.00001901
Iteration 36/1000 | Loss: 0.00001898
Iteration 37/1000 | Loss: 0.00001898
Iteration 38/1000 | Loss: 0.00001897
Iteration 39/1000 | Loss: 0.00001897
Iteration 40/1000 | Loss: 0.00001896
Iteration 41/1000 | Loss: 0.00001896
Iteration 42/1000 | Loss: 0.00001896
Iteration 43/1000 | Loss: 0.00001895
Iteration 44/1000 | Loss: 0.00001895
Iteration 45/1000 | Loss: 0.00001895
Iteration 46/1000 | Loss: 0.00001894
Iteration 47/1000 | Loss: 0.00001894
Iteration 48/1000 | Loss: 0.00001894
Iteration 49/1000 | Loss: 0.00001894
Iteration 50/1000 | Loss: 0.00001894
Iteration 51/1000 | Loss: 0.00001894
Iteration 52/1000 | Loss: 0.00001894
Iteration 53/1000 | Loss: 0.00001894
Iteration 54/1000 | Loss: 0.00001894
Iteration 55/1000 | Loss: 0.00001893
Iteration 56/1000 | Loss: 0.00001893
Iteration 57/1000 | Loss: 0.00001893
Iteration 58/1000 | Loss: 0.00001893
Iteration 59/1000 | Loss: 0.00001893
Iteration 60/1000 | Loss: 0.00001893
Iteration 61/1000 | Loss: 0.00001892
Iteration 62/1000 | Loss: 0.00001892
Iteration 63/1000 | Loss: 0.00001892
Iteration 64/1000 | Loss: 0.00001892
Iteration 65/1000 | Loss: 0.00001892
Iteration 66/1000 | Loss: 0.00001891
Iteration 67/1000 | Loss: 0.00001891
Iteration 68/1000 | Loss: 0.00001891
Iteration 69/1000 | Loss: 0.00001891
Iteration 70/1000 | Loss: 0.00001891
Iteration 71/1000 | Loss: 0.00001890
Iteration 72/1000 | Loss: 0.00001890
Iteration 73/1000 | Loss: 0.00001890
Iteration 74/1000 | Loss: 0.00001890
Iteration 75/1000 | Loss: 0.00001889
Iteration 76/1000 | Loss: 0.00001889
Iteration 77/1000 | Loss: 0.00001889
Iteration 78/1000 | Loss: 0.00001888
Iteration 79/1000 | Loss: 0.00001888
Iteration 80/1000 | Loss: 0.00001888
Iteration 81/1000 | Loss: 0.00001887
Iteration 82/1000 | Loss: 0.00001887
Iteration 83/1000 | Loss: 0.00001887
Iteration 84/1000 | Loss: 0.00001886
Iteration 85/1000 | Loss: 0.00001886
Iteration 86/1000 | Loss: 0.00001886
Iteration 87/1000 | Loss: 0.00001886
Iteration 88/1000 | Loss: 0.00001886
Iteration 89/1000 | Loss: 0.00001886
Iteration 90/1000 | Loss: 0.00001886
Iteration 91/1000 | Loss: 0.00001885
Iteration 92/1000 | Loss: 0.00001885
Iteration 93/1000 | Loss: 0.00001885
Iteration 94/1000 | Loss: 0.00001885
Iteration 95/1000 | Loss: 0.00001885
Iteration 96/1000 | Loss: 0.00001884
Iteration 97/1000 | Loss: 0.00001884
Iteration 98/1000 | Loss: 0.00001884
Iteration 99/1000 | Loss: 0.00001884
Iteration 100/1000 | Loss: 0.00001884
Iteration 101/1000 | Loss: 0.00001883
Iteration 102/1000 | Loss: 0.00001883
Iteration 103/1000 | Loss: 0.00001883
Iteration 104/1000 | Loss: 0.00001883
Iteration 105/1000 | Loss: 0.00001883
Iteration 106/1000 | Loss: 0.00001883
Iteration 107/1000 | Loss: 0.00001883
Iteration 108/1000 | Loss: 0.00001882
Iteration 109/1000 | Loss: 0.00001882
Iteration 110/1000 | Loss: 0.00001882
Iteration 111/1000 | Loss: 0.00001882
Iteration 112/1000 | Loss: 0.00001882
Iteration 113/1000 | Loss: 0.00001882
Iteration 114/1000 | Loss: 0.00001882
Iteration 115/1000 | Loss: 0.00001882
Iteration 116/1000 | Loss: 0.00001882
Iteration 117/1000 | Loss: 0.00001882
Iteration 118/1000 | Loss: 0.00001881
Iteration 119/1000 | Loss: 0.00001881
Iteration 120/1000 | Loss: 0.00001881
Iteration 121/1000 | Loss: 0.00001881
Iteration 122/1000 | Loss: 0.00001881
Iteration 123/1000 | Loss: 0.00001881
Iteration 124/1000 | Loss: 0.00001881
Iteration 125/1000 | Loss: 0.00001881
Iteration 126/1000 | Loss: 0.00001881
Iteration 127/1000 | Loss: 0.00001880
Iteration 128/1000 | Loss: 0.00001880
Iteration 129/1000 | Loss: 0.00001880
Iteration 130/1000 | Loss: 0.00001880
Iteration 131/1000 | Loss: 0.00001880
Iteration 132/1000 | Loss: 0.00001880
Iteration 133/1000 | Loss: 0.00001880
Iteration 134/1000 | Loss: 0.00001880
Iteration 135/1000 | Loss: 0.00001880
Iteration 136/1000 | Loss: 0.00001880
Iteration 137/1000 | Loss: 0.00001880
Iteration 138/1000 | Loss: 0.00001879
Iteration 139/1000 | Loss: 0.00001879
Iteration 140/1000 | Loss: 0.00001879
Iteration 141/1000 | Loss: 0.00001879
Iteration 142/1000 | Loss: 0.00001879
Iteration 143/1000 | Loss: 0.00001879
Iteration 144/1000 | Loss: 0.00001879
Iteration 145/1000 | Loss: 0.00001879
Iteration 146/1000 | Loss: 0.00001879
Iteration 147/1000 | Loss: 0.00001879
Iteration 148/1000 | Loss: 0.00001879
Iteration 149/1000 | Loss: 0.00001879
Iteration 150/1000 | Loss: 0.00001879
Iteration 151/1000 | Loss: 0.00001879
Iteration 152/1000 | Loss: 0.00001879
Iteration 153/1000 | Loss: 0.00001879
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.8785045540425926e-05, 1.8785045540425926e-05, 1.8785045540425926e-05, 1.8785045540425926e-05, 1.8785045540425926e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8785045540425926e-05

Optimization complete. Final v2v error: 3.6820404529571533 mm

Highest mean error: 4.490783214569092 mm for frame 11

Lowest mean error: 3.063896656036377 mm for frame 148

Saving results

Total time: 43.625335454940796
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00818706
Iteration 2/25 | Loss: 0.00145377
Iteration 3/25 | Loss: 0.00131402
Iteration 4/25 | Loss: 0.00128599
Iteration 5/25 | Loss: 0.00127499
Iteration 6/25 | Loss: 0.00127279
Iteration 7/25 | Loss: 0.00127238
Iteration 8/25 | Loss: 0.00127238
Iteration 9/25 | Loss: 0.00127238
Iteration 10/25 | Loss: 0.00127238
Iteration 11/25 | Loss: 0.00127238
Iteration 12/25 | Loss: 0.00127238
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012723817490041256, 0.0012723817490041256, 0.0012723817490041256, 0.0012723817490041256, 0.0012723817490041256]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012723817490041256

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58116186
Iteration 2/25 | Loss: 0.00085981
Iteration 3/25 | Loss: 0.00085981
Iteration 4/25 | Loss: 0.00085981
Iteration 5/25 | Loss: 0.00085981
Iteration 6/25 | Loss: 0.00085980
Iteration 7/25 | Loss: 0.00085980
Iteration 8/25 | Loss: 0.00085980
Iteration 9/25 | Loss: 0.00085980
Iteration 10/25 | Loss: 0.00085980
Iteration 11/25 | Loss: 0.00085980
Iteration 12/25 | Loss: 0.00085980
Iteration 13/25 | Loss: 0.00085980
Iteration 14/25 | Loss: 0.00085980
Iteration 15/25 | Loss: 0.00085980
Iteration 16/25 | Loss: 0.00085980
Iteration 17/25 | Loss: 0.00085980
Iteration 18/25 | Loss: 0.00085980
Iteration 19/25 | Loss: 0.00085980
Iteration 20/25 | Loss: 0.00085980
Iteration 21/25 | Loss: 0.00085980
Iteration 22/25 | Loss: 0.00085980
Iteration 23/25 | Loss: 0.00085980
Iteration 24/25 | Loss: 0.00085980
Iteration 25/25 | Loss: 0.00085980
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0008598032291047275, 0.0008598032291047275, 0.0008598032291047275, 0.0008598032291047275, 0.0008598032291047275]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008598032291047275

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085980
Iteration 2/1000 | Loss: 0.00004952
Iteration 3/1000 | Loss: 0.00003696
Iteration 4/1000 | Loss: 0.00003219
Iteration 5/1000 | Loss: 0.00003018
Iteration 6/1000 | Loss: 0.00002903
Iteration 7/1000 | Loss: 0.00002806
Iteration 8/1000 | Loss: 0.00002742
Iteration 9/1000 | Loss: 0.00002686
Iteration 10/1000 | Loss: 0.00002646
Iteration 11/1000 | Loss: 0.00002613
Iteration 12/1000 | Loss: 0.00002586
Iteration 13/1000 | Loss: 0.00002563
Iteration 14/1000 | Loss: 0.00002546
Iteration 15/1000 | Loss: 0.00002537
Iteration 16/1000 | Loss: 0.00002529
Iteration 17/1000 | Loss: 0.00002527
Iteration 18/1000 | Loss: 0.00002525
Iteration 19/1000 | Loss: 0.00002516
Iteration 20/1000 | Loss: 0.00002513
Iteration 21/1000 | Loss: 0.00002510
Iteration 22/1000 | Loss: 0.00002508
Iteration 23/1000 | Loss: 0.00002507
Iteration 24/1000 | Loss: 0.00002506
Iteration 25/1000 | Loss: 0.00002506
Iteration 26/1000 | Loss: 0.00002506
Iteration 27/1000 | Loss: 0.00002504
Iteration 28/1000 | Loss: 0.00002504
Iteration 29/1000 | Loss: 0.00002503
Iteration 30/1000 | Loss: 0.00002503
Iteration 31/1000 | Loss: 0.00002503
Iteration 32/1000 | Loss: 0.00002503
Iteration 33/1000 | Loss: 0.00002502
Iteration 34/1000 | Loss: 0.00002502
Iteration 35/1000 | Loss: 0.00002502
Iteration 36/1000 | Loss: 0.00002502
Iteration 37/1000 | Loss: 0.00002502
Iteration 38/1000 | Loss: 0.00002502
Iteration 39/1000 | Loss: 0.00002501
Iteration 40/1000 | Loss: 0.00002500
Iteration 41/1000 | Loss: 0.00002500
Iteration 42/1000 | Loss: 0.00002499
Iteration 43/1000 | Loss: 0.00002499
Iteration 44/1000 | Loss: 0.00002498
Iteration 45/1000 | Loss: 0.00002498
Iteration 46/1000 | Loss: 0.00002498
Iteration 47/1000 | Loss: 0.00002498
Iteration 48/1000 | Loss: 0.00002498
Iteration 49/1000 | Loss: 0.00002498
Iteration 50/1000 | Loss: 0.00002498
Iteration 51/1000 | Loss: 0.00002498
Iteration 52/1000 | Loss: 0.00002498
Iteration 53/1000 | Loss: 0.00002498
Iteration 54/1000 | Loss: 0.00002498
Iteration 55/1000 | Loss: 0.00002497
Iteration 56/1000 | Loss: 0.00002497
Iteration 57/1000 | Loss: 0.00002497
Iteration 58/1000 | Loss: 0.00002497
Iteration 59/1000 | Loss: 0.00002497
Iteration 60/1000 | Loss: 0.00002497
Iteration 61/1000 | Loss: 0.00002497
Iteration 62/1000 | Loss: 0.00002496
Iteration 63/1000 | Loss: 0.00002495
Iteration 64/1000 | Loss: 0.00002494
Iteration 65/1000 | Loss: 0.00002494
Iteration 66/1000 | Loss: 0.00002494
Iteration 67/1000 | Loss: 0.00002493
Iteration 68/1000 | Loss: 0.00002493
Iteration 69/1000 | Loss: 0.00002493
Iteration 70/1000 | Loss: 0.00002493
Iteration 71/1000 | Loss: 0.00002493
Iteration 72/1000 | Loss: 0.00002492
Iteration 73/1000 | Loss: 0.00002492
Iteration 74/1000 | Loss: 0.00002492
Iteration 75/1000 | Loss: 0.00002491
Iteration 76/1000 | Loss: 0.00002491
Iteration 77/1000 | Loss: 0.00002491
Iteration 78/1000 | Loss: 0.00002490
Iteration 79/1000 | Loss: 0.00002490
Iteration 80/1000 | Loss: 0.00002490
Iteration 81/1000 | Loss: 0.00002489
Iteration 82/1000 | Loss: 0.00002489
Iteration 83/1000 | Loss: 0.00002489
Iteration 84/1000 | Loss: 0.00002489
Iteration 85/1000 | Loss: 0.00002488
Iteration 86/1000 | Loss: 0.00002488
Iteration 87/1000 | Loss: 0.00002488
Iteration 88/1000 | Loss: 0.00002488
Iteration 89/1000 | Loss: 0.00002488
Iteration 90/1000 | Loss: 0.00002487
Iteration 91/1000 | Loss: 0.00002487
Iteration 92/1000 | Loss: 0.00002487
Iteration 93/1000 | Loss: 0.00002487
Iteration 94/1000 | Loss: 0.00002487
Iteration 95/1000 | Loss: 0.00002487
Iteration 96/1000 | Loss: 0.00002487
Iteration 97/1000 | Loss: 0.00002487
Iteration 98/1000 | Loss: 0.00002487
Iteration 99/1000 | Loss: 0.00002486
Iteration 100/1000 | Loss: 0.00002486
Iteration 101/1000 | Loss: 0.00002486
Iteration 102/1000 | Loss: 0.00002486
Iteration 103/1000 | Loss: 0.00002486
Iteration 104/1000 | Loss: 0.00002486
Iteration 105/1000 | Loss: 0.00002486
Iteration 106/1000 | Loss: 0.00002486
Iteration 107/1000 | Loss: 0.00002486
Iteration 108/1000 | Loss: 0.00002485
Iteration 109/1000 | Loss: 0.00002485
Iteration 110/1000 | Loss: 0.00002485
Iteration 111/1000 | Loss: 0.00002485
Iteration 112/1000 | Loss: 0.00002485
Iteration 113/1000 | Loss: 0.00002485
Iteration 114/1000 | Loss: 0.00002485
Iteration 115/1000 | Loss: 0.00002485
Iteration 116/1000 | Loss: 0.00002485
Iteration 117/1000 | Loss: 0.00002484
Iteration 118/1000 | Loss: 0.00002484
Iteration 119/1000 | Loss: 0.00002484
Iteration 120/1000 | Loss: 0.00002483
Iteration 121/1000 | Loss: 0.00002483
Iteration 122/1000 | Loss: 0.00002483
Iteration 123/1000 | Loss: 0.00002483
Iteration 124/1000 | Loss: 0.00002482
Iteration 125/1000 | Loss: 0.00002482
Iteration 126/1000 | Loss: 0.00002482
Iteration 127/1000 | Loss: 0.00002482
Iteration 128/1000 | Loss: 0.00002482
Iteration 129/1000 | Loss: 0.00002482
Iteration 130/1000 | Loss: 0.00002481
Iteration 131/1000 | Loss: 0.00002481
Iteration 132/1000 | Loss: 0.00002481
Iteration 133/1000 | Loss: 0.00002481
Iteration 134/1000 | Loss: 0.00002480
Iteration 135/1000 | Loss: 0.00002480
Iteration 136/1000 | Loss: 0.00002480
Iteration 137/1000 | Loss: 0.00002480
Iteration 138/1000 | Loss: 0.00002479
Iteration 139/1000 | Loss: 0.00002479
Iteration 140/1000 | Loss: 0.00002479
Iteration 141/1000 | Loss: 0.00002479
Iteration 142/1000 | Loss: 0.00002478
Iteration 143/1000 | Loss: 0.00002478
Iteration 144/1000 | Loss: 0.00002478
Iteration 145/1000 | Loss: 0.00002478
Iteration 146/1000 | Loss: 0.00002478
Iteration 147/1000 | Loss: 0.00002478
Iteration 148/1000 | Loss: 0.00002477
Iteration 149/1000 | Loss: 0.00002477
Iteration 150/1000 | Loss: 0.00002477
Iteration 151/1000 | Loss: 0.00002477
Iteration 152/1000 | Loss: 0.00002476
Iteration 153/1000 | Loss: 0.00002476
Iteration 154/1000 | Loss: 0.00002476
Iteration 155/1000 | Loss: 0.00002476
Iteration 156/1000 | Loss: 0.00002476
Iteration 157/1000 | Loss: 0.00002476
Iteration 158/1000 | Loss: 0.00002475
Iteration 159/1000 | Loss: 0.00002475
Iteration 160/1000 | Loss: 0.00002475
Iteration 161/1000 | Loss: 0.00002475
Iteration 162/1000 | Loss: 0.00002475
Iteration 163/1000 | Loss: 0.00002475
Iteration 164/1000 | Loss: 0.00002475
Iteration 165/1000 | Loss: 0.00002475
Iteration 166/1000 | Loss: 0.00002475
Iteration 167/1000 | Loss: 0.00002475
Iteration 168/1000 | Loss: 0.00002474
Iteration 169/1000 | Loss: 0.00002474
Iteration 170/1000 | Loss: 0.00002474
Iteration 171/1000 | Loss: 0.00002474
Iteration 172/1000 | Loss: 0.00002474
Iteration 173/1000 | Loss: 0.00002474
Iteration 174/1000 | Loss: 0.00002474
Iteration 175/1000 | Loss: 0.00002473
Iteration 176/1000 | Loss: 0.00002473
Iteration 177/1000 | Loss: 0.00002473
Iteration 178/1000 | Loss: 0.00002473
Iteration 179/1000 | Loss: 0.00002473
Iteration 180/1000 | Loss: 0.00002473
Iteration 181/1000 | Loss: 0.00002473
Iteration 182/1000 | Loss: 0.00002473
Iteration 183/1000 | Loss: 0.00002473
Iteration 184/1000 | Loss: 0.00002473
Iteration 185/1000 | Loss: 0.00002472
Iteration 186/1000 | Loss: 0.00002472
Iteration 187/1000 | Loss: 0.00002472
Iteration 188/1000 | Loss: 0.00002472
Iteration 189/1000 | Loss: 0.00002472
Iteration 190/1000 | Loss: 0.00002472
Iteration 191/1000 | Loss: 0.00002472
Iteration 192/1000 | Loss: 0.00002472
Iteration 193/1000 | Loss: 0.00002472
Iteration 194/1000 | Loss: 0.00002472
Iteration 195/1000 | Loss: 0.00002472
Iteration 196/1000 | Loss: 0.00002472
Iteration 197/1000 | Loss: 0.00002472
Iteration 198/1000 | Loss: 0.00002471
Iteration 199/1000 | Loss: 0.00002471
Iteration 200/1000 | Loss: 0.00002471
Iteration 201/1000 | Loss: 0.00002471
Iteration 202/1000 | Loss: 0.00002471
Iteration 203/1000 | Loss: 0.00002471
Iteration 204/1000 | Loss: 0.00002471
Iteration 205/1000 | Loss: 0.00002471
Iteration 206/1000 | Loss: 0.00002471
Iteration 207/1000 | Loss: 0.00002471
Iteration 208/1000 | Loss: 0.00002470
Iteration 209/1000 | Loss: 0.00002470
Iteration 210/1000 | Loss: 0.00002470
Iteration 211/1000 | Loss: 0.00002470
Iteration 212/1000 | Loss: 0.00002470
Iteration 213/1000 | Loss: 0.00002470
Iteration 214/1000 | Loss: 0.00002470
Iteration 215/1000 | Loss: 0.00002470
Iteration 216/1000 | Loss: 0.00002470
Iteration 217/1000 | Loss: 0.00002470
Iteration 218/1000 | Loss: 0.00002470
Iteration 219/1000 | Loss: 0.00002470
Iteration 220/1000 | Loss: 0.00002470
Iteration 221/1000 | Loss: 0.00002470
Iteration 222/1000 | Loss: 0.00002470
Iteration 223/1000 | Loss: 0.00002470
Iteration 224/1000 | Loss: 0.00002470
Iteration 225/1000 | Loss: 0.00002470
Iteration 226/1000 | Loss: 0.00002470
Iteration 227/1000 | Loss: 0.00002470
Iteration 228/1000 | Loss: 0.00002470
Iteration 229/1000 | Loss: 0.00002470
Iteration 230/1000 | Loss: 0.00002470
Iteration 231/1000 | Loss: 0.00002470
Iteration 232/1000 | Loss: 0.00002470
Iteration 233/1000 | Loss: 0.00002470
Iteration 234/1000 | Loss: 0.00002470
Iteration 235/1000 | Loss: 0.00002470
Iteration 236/1000 | Loss: 0.00002470
Iteration 237/1000 | Loss: 0.00002470
Iteration 238/1000 | Loss: 0.00002470
Iteration 239/1000 | Loss: 0.00002470
Iteration 240/1000 | Loss: 0.00002470
Iteration 241/1000 | Loss: 0.00002470
Iteration 242/1000 | Loss: 0.00002470
Iteration 243/1000 | Loss: 0.00002470
Iteration 244/1000 | Loss: 0.00002470
Iteration 245/1000 | Loss: 0.00002470
Iteration 246/1000 | Loss: 0.00002470
Iteration 247/1000 | Loss: 0.00002470
Iteration 248/1000 | Loss: 0.00002470
Iteration 249/1000 | Loss: 0.00002470
Iteration 250/1000 | Loss: 0.00002470
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 250. Stopping optimization.
Last 5 losses: [2.4703274903004058e-05, 2.4703274903004058e-05, 2.4703274903004058e-05, 2.4703274903004058e-05, 2.4703274903004058e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4703274903004058e-05

Optimization complete. Final v2v error: 4.069288730621338 mm

Highest mean error: 6.1943745613098145 mm for frame 59

Lowest mean error: 3.0022783279418945 mm for frame 46

Saving results

Total time: 49.98340630531311
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00766325
Iteration 2/25 | Loss: 0.00146879
Iteration 3/25 | Loss: 0.00130404
Iteration 4/25 | Loss: 0.00128489
Iteration 5/25 | Loss: 0.00127970
Iteration 6/25 | Loss: 0.00127859
Iteration 7/25 | Loss: 0.00127859
Iteration 8/25 | Loss: 0.00127859
Iteration 9/25 | Loss: 0.00127859
Iteration 10/25 | Loss: 0.00127859
Iteration 11/25 | Loss: 0.00127859
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012785891303792596, 0.0012785891303792596, 0.0012785891303792596, 0.0012785891303792596, 0.0012785891303792596]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012785891303792596

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33964694
Iteration 2/25 | Loss: 0.00071276
Iteration 3/25 | Loss: 0.00071276
Iteration 4/25 | Loss: 0.00071276
Iteration 5/25 | Loss: 0.00071276
Iteration 6/25 | Loss: 0.00071276
Iteration 7/25 | Loss: 0.00071276
Iteration 8/25 | Loss: 0.00071276
Iteration 9/25 | Loss: 0.00071276
Iteration 10/25 | Loss: 0.00071276
Iteration 11/25 | Loss: 0.00071276
Iteration 12/25 | Loss: 0.00071276
Iteration 13/25 | Loss: 0.00071276
Iteration 14/25 | Loss: 0.00071276
Iteration 15/25 | Loss: 0.00071276
Iteration 16/25 | Loss: 0.00071276
Iteration 17/25 | Loss: 0.00071276
Iteration 18/25 | Loss: 0.00071276
Iteration 19/25 | Loss: 0.00071276
Iteration 20/25 | Loss: 0.00071276
Iteration 21/25 | Loss: 0.00071276
Iteration 22/25 | Loss: 0.00071276
Iteration 23/25 | Loss: 0.00071276
Iteration 24/25 | Loss: 0.00071276
Iteration 25/25 | Loss: 0.00071276
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007127609569579363, 0.0007127609569579363, 0.0007127609569579363, 0.0007127609569579363, 0.0007127609569579363]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007127609569579363

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071276
Iteration 2/1000 | Loss: 0.00004965
Iteration 3/1000 | Loss: 0.00003217
Iteration 4/1000 | Loss: 0.00002642
Iteration 5/1000 | Loss: 0.00002384
Iteration 6/1000 | Loss: 0.00002256
Iteration 7/1000 | Loss: 0.00002177
Iteration 8/1000 | Loss: 0.00002110
Iteration 9/1000 | Loss: 0.00002058
Iteration 10/1000 | Loss: 0.00002022
Iteration 11/1000 | Loss: 0.00001994
Iteration 12/1000 | Loss: 0.00001973
Iteration 13/1000 | Loss: 0.00001960
Iteration 14/1000 | Loss: 0.00001958
Iteration 15/1000 | Loss: 0.00001944
Iteration 16/1000 | Loss: 0.00001941
Iteration 17/1000 | Loss: 0.00001930
Iteration 18/1000 | Loss: 0.00001927
Iteration 19/1000 | Loss: 0.00001926
Iteration 20/1000 | Loss: 0.00001926
Iteration 21/1000 | Loss: 0.00001918
Iteration 22/1000 | Loss: 0.00001918
Iteration 23/1000 | Loss: 0.00001914
Iteration 24/1000 | Loss: 0.00001913
Iteration 25/1000 | Loss: 0.00001912
Iteration 26/1000 | Loss: 0.00001912
Iteration 27/1000 | Loss: 0.00001911
Iteration 28/1000 | Loss: 0.00001911
Iteration 29/1000 | Loss: 0.00001911
Iteration 30/1000 | Loss: 0.00001911
Iteration 31/1000 | Loss: 0.00001910
Iteration 32/1000 | Loss: 0.00001910
Iteration 33/1000 | Loss: 0.00001910
Iteration 34/1000 | Loss: 0.00001910
Iteration 35/1000 | Loss: 0.00001909
Iteration 36/1000 | Loss: 0.00001909
Iteration 37/1000 | Loss: 0.00001909
Iteration 38/1000 | Loss: 0.00001908
Iteration 39/1000 | Loss: 0.00001908
Iteration 40/1000 | Loss: 0.00001908
Iteration 41/1000 | Loss: 0.00001908
Iteration 42/1000 | Loss: 0.00001908
Iteration 43/1000 | Loss: 0.00001908
Iteration 44/1000 | Loss: 0.00001907
Iteration 45/1000 | Loss: 0.00001907
Iteration 46/1000 | Loss: 0.00001907
Iteration 47/1000 | Loss: 0.00001905
Iteration 48/1000 | Loss: 0.00001905
Iteration 49/1000 | Loss: 0.00001905
Iteration 50/1000 | Loss: 0.00001904
Iteration 51/1000 | Loss: 0.00001904
Iteration 52/1000 | Loss: 0.00001904
Iteration 53/1000 | Loss: 0.00001904
Iteration 54/1000 | Loss: 0.00001904
Iteration 55/1000 | Loss: 0.00001903
Iteration 56/1000 | Loss: 0.00001903
Iteration 57/1000 | Loss: 0.00001902
Iteration 58/1000 | Loss: 0.00001902
Iteration 59/1000 | Loss: 0.00001902
Iteration 60/1000 | Loss: 0.00001902
Iteration 61/1000 | Loss: 0.00001902
Iteration 62/1000 | Loss: 0.00001901
Iteration 63/1000 | Loss: 0.00001901
Iteration 64/1000 | Loss: 0.00001901
Iteration 65/1000 | Loss: 0.00001900
Iteration 66/1000 | Loss: 0.00001900
Iteration 67/1000 | Loss: 0.00001900
Iteration 68/1000 | Loss: 0.00001900
Iteration 69/1000 | Loss: 0.00001900
Iteration 70/1000 | Loss: 0.00001900
Iteration 71/1000 | Loss: 0.00001899
Iteration 72/1000 | Loss: 0.00001899
Iteration 73/1000 | Loss: 0.00001899
Iteration 74/1000 | Loss: 0.00001899
Iteration 75/1000 | Loss: 0.00001899
Iteration 76/1000 | Loss: 0.00001899
Iteration 77/1000 | Loss: 0.00001899
Iteration 78/1000 | Loss: 0.00001899
Iteration 79/1000 | Loss: 0.00001899
Iteration 80/1000 | Loss: 0.00001899
Iteration 81/1000 | Loss: 0.00001899
Iteration 82/1000 | Loss: 0.00001899
Iteration 83/1000 | Loss: 0.00001899
Iteration 84/1000 | Loss: 0.00001899
Iteration 85/1000 | Loss: 0.00001899
Iteration 86/1000 | Loss: 0.00001899
Iteration 87/1000 | Loss: 0.00001899
Iteration 88/1000 | Loss: 0.00001899
Iteration 89/1000 | Loss: 0.00001898
Iteration 90/1000 | Loss: 0.00001898
Iteration 91/1000 | Loss: 0.00001898
Iteration 92/1000 | Loss: 0.00001898
Iteration 93/1000 | Loss: 0.00001897
Iteration 94/1000 | Loss: 0.00001897
Iteration 95/1000 | Loss: 0.00001897
Iteration 96/1000 | Loss: 0.00001897
Iteration 97/1000 | Loss: 0.00001897
Iteration 98/1000 | Loss: 0.00001897
Iteration 99/1000 | Loss: 0.00001897
Iteration 100/1000 | Loss: 0.00001897
Iteration 101/1000 | Loss: 0.00001897
Iteration 102/1000 | Loss: 0.00001896
Iteration 103/1000 | Loss: 0.00001896
Iteration 104/1000 | Loss: 0.00001896
Iteration 105/1000 | Loss: 0.00001896
Iteration 106/1000 | Loss: 0.00001896
Iteration 107/1000 | Loss: 0.00001896
Iteration 108/1000 | Loss: 0.00001895
Iteration 109/1000 | Loss: 0.00001895
Iteration 110/1000 | Loss: 0.00001895
Iteration 111/1000 | Loss: 0.00001895
Iteration 112/1000 | Loss: 0.00001895
Iteration 113/1000 | Loss: 0.00001895
Iteration 114/1000 | Loss: 0.00001895
Iteration 115/1000 | Loss: 0.00001894
Iteration 116/1000 | Loss: 0.00001894
Iteration 117/1000 | Loss: 0.00001894
Iteration 118/1000 | Loss: 0.00001894
Iteration 119/1000 | Loss: 0.00001894
Iteration 120/1000 | Loss: 0.00001894
Iteration 121/1000 | Loss: 0.00001894
Iteration 122/1000 | Loss: 0.00001894
Iteration 123/1000 | Loss: 0.00001894
Iteration 124/1000 | Loss: 0.00001894
Iteration 125/1000 | Loss: 0.00001894
Iteration 126/1000 | Loss: 0.00001894
Iteration 127/1000 | Loss: 0.00001893
Iteration 128/1000 | Loss: 0.00001893
Iteration 129/1000 | Loss: 0.00001893
Iteration 130/1000 | Loss: 0.00001893
Iteration 131/1000 | Loss: 0.00001893
Iteration 132/1000 | Loss: 0.00001893
Iteration 133/1000 | Loss: 0.00001893
Iteration 134/1000 | Loss: 0.00001893
Iteration 135/1000 | Loss: 0.00001893
Iteration 136/1000 | Loss: 0.00001893
Iteration 137/1000 | Loss: 0.00001893
Iteration 138/1000 | Loss: 0.00001893
Iteration 139/1000 | Loss: 0.00001893
Iteration 140/1000 | Loss: 0.00001893
Iteration 141/1000 | Loss: 0.00001893
Iteration 142/1000 | Loss: 0.00001893
Iteration 143/1000 | Loss: 0.00001892
Iteration 144/1000 | Loss: 0.00001892
Iteration 145/1000 | Loss: 0.00001892
Iteration 146/1000 | Loss: 0.00001892
Iteration 147/1000 | Loss: 0.00001892
Iteration 148/1000 | Loss: 0.00001892
Iteration 149/1000 | Loss: 0.00001892
Iteration 150/1000 | Loss: 0.00001892
Iteration 151/1000 | Loss: 0.00001892
Iteration 152/1000 | Loss: 0.00001892
Iteration 153/1000 | Loss: 0.00001892
Iteration 154/1000 | Loss: 0.00001892
Iteration 155/1000 | Loss: 0.00001892
Iteration 156/1000 | Loss: 0.00001892
Iteration 157/1000 | Loss: 0.00001892
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.891997999337036e-05, 1.891997999337036e-05, 1.891997999337036e-05, 1.891997999337036e-05, 1.891997999337036e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.891997999337036e-05

Optimization complete. Final v2v error: 3.6730029582977295 mm

Highest mean error: 4.036131381988525 mm for frame 105

Lowest mean error: 3.0706212520599365 mm for frame 0

Saving results

Total time: 41.366777658462524
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00822250
Iteration 2/25 | Loss: 0.00145074
Iteration 3/25 | Loss: 0.00131887
Iteration 4/25 | Loss: 0.00129428
Iteration 5/25 | Loss: 0.00128774
Iteration 6/25 | Loss: 0.00128723
Iteration 7/25 | Loss: 0.00128723
Iteration 8/25 | Loss: 0.00128723
Iteration 9/25 | Loss: 0.00128723
Iteration 10/25 | Loss: 0.00128723
Iteration 11/25 | Loss: 0.00128723
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012872298248112202, 0.0012872298248112202, 0.0012872298248112202, 0.0012872298248112202, 0.0012872298248112202]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012872298248112202

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.02033520
Iteration 2/25 | Loss: 0.00053541
Iteration 3/25 | Loss: 0.00053540
Iteration 4/25 | Loss: 0.00053540
Iteration 5/25 | Loss: 0.00053540
Iteration 6/25 | Loss: 0.00053540
Iteration 7/25 | Loss: 0.00053540
Iteration 8/25 | Loss: 0.00053540
Iteration 9/25 | Loss: 0.00053540
Iteration 10/25 | Loss: 0.00053540
Iteration 11/25 | Loss: 0.00053540
Iteration 12/25 | Loss: 0.00053540
Iteration 13/25 | Loss: 0.00053540
Iteration 14/25 | Loss: 0.00053540
Iteration 15/25 | Loss: 0.00053540
Iteration 16/25 | Loss: 0.00053540
Iteration 17/25 | Loss: 0.00053540
Iteration 18/25 | Loss: 0.00053540
Iteration 19/25 | Loss: 0.00053540
Iteration 20/25 | Loss: 0.00053540
Iteration 21/25 | Loss: 0.00053540
Iteration 22/25 | Loss: 0.00053540
Iteration 23/25 | Loss: 0.00053540
Iteration 24/25 | Loss: 0.00053540
Iteration 25/25 | Loss: 0.00053540

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053540
Iteration 2/1000 | Loss: 0.00003683
Iteration 3/1000 | Loss: 0.00002954
Iteration 4/1000 | Loss: 0.00002743
Iteration 5/1000 | Loss: 0.00002614
Iteration 6/1000 | Loss: 0.00002507
Iteration 7/1000 | Loss: 0.00002439
Iteration 8/1000 | Loss: 0.00002407
Iteration 9/1000 | Loss: 0.00002367
Iteration 10/1000 | Loss: 0.00002344
Iteration 11/1000 | Loss: 0.00002322
Iteration 12/1000 | Loss: 0.00002302
Iteration 13/1000 | Loss: 0.00002286
Iteration 14/1000 | Loss: 0.00002285
Iteration 15/1000 | Loss: 0.00002285
Iteration 16/1000 | Loss: 0.00002283
Iteration 17/1000 | Loss: 0.00002281
Iteration 18/1000 | Loss: 0.00002276
Iteration 19/1000 | Loss: 0.00002271
Iteration 20/1000 | Loss: 0.00002271
Iteration 21/1000 | Loss: 0.00002271
Iteration 22/1000 | Loss: 0.00002270
Iteration 23/1000 | Loss: 0.00002270
Iteration 24/1000 | Loss: 0.00002269
Iteration 25/1000 | Loss: 0.00002269
Iteration 26/1000 | Loss: 0.00002268
Iteration 27/1000 | Loss: 0.00002268
Iteration 28/1000 | Loss: 0.00002268
Iteration 29/1000 | Loss: 0.00002268
Iteration 30/1000 | Loss: 0.00002268
Iteration 31/1000 | Loss: 0.00002268
Iteration 32/1000 | Loss: 0.00002268
Iteration 33/1000 | Loss: 0.00002268
Iteration 34/1000 | Loss: 0.00002268
Iteration 35/1000 | Loss: 0.00002267
Iteration 36/1000 | Loss: 0.00002266
Iteration 37/1000 | Loss: 0.00002266
Iteration 38/1000 | Loss: 0.00002265
Iteration 39/1000 | Loss: 0.00002265
Iteration 40/1000 | Loss: 0.00002265
Iteration 41/1000 | Loss: 0.00002265
Iteration 42/1000 | Loss: 0.00002265
Iteration 43/1000 | Loss: 0.00002265
Iteration 44/1000 | Loss: 0.00002265
Iteration 45/1000 | Loss: 0.00002263
Iteration 46/1000 | Loss: 0.00002263
Iteration 47/1000 | Loss: 0.00002263
Iteration 48/1000 | Loss: 0.00002263
Iteration 49/1000 | Loss: 0.00002263
Iteration 50/1000 | Loss: 0.00002263
Iteration 51/1000 | Loss: 0.00002263
Iteration 52/1000 | Loss: 0.00002263
Iteration 53/1000 | Loss: 0.00002263
Iteration 54/1000 | Loss: 0.00002263
Iteration 55/1000 | Loss: 0.00002263
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 55. Stopping optimization.
Last 5 losses: [2.2626616555498913e-05, 2.2626616555498913e-05, 2.2626616555498913e-05, 2.2626616555498913e-05, 2.2626616555498913e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2626616555498913e-05

Optimization complete. Final v2v error: 4.068851947784424 mm

Highest mean error: 4.704000949859619 mm for frame 239

Lowest mean error: 3.6687724590301514 mm for frame 3

Saving results

Total time: 35.78868532180786
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00872965
Iteration 2/25 | Loss: 0.00185425
Iteration 3/25 | Loss: 0.00148501
Iteration 4/25 | Loss: 0.00143235
Iteration 5/25 | Loss: 0.00141735
Iteration 6/25 | Loss: 0.00138608
Iteration 7/25 | Loss: 0.00138730
Iteration 8/25 | Loss: 0.00139050
Iteration 9/25 | Loss: 0.00138577
Iteration 10/25 | Loss: 0.00137584
Iteration 11/25 | Loss: 0.00137115
Iteration 12/25 | Loss: 0.00136969
Iteration 13/25 | Loss: 0.00137496
Iteration 14/25 | Loss: 0.00137480
Iteration 15/25 | Loss: 0.00137468
Iteration 16/25 | Loss: 0.00137093
Iteration 17/25 | Loss: 0.00136806
Iteration 18/25 | Loss: 0.00136751
Iteration 19/25 | Loss: 0.00136737
Iteration 20/25 | Loss: 0.00136734
Iteration 21/25 | Loss: 0.00136734
Iteration 22/25 | Loss: 0.00136734
Iteration 23/25 | Loss: 0.00136733
Iteration 24/25 | Loss: 0.00136733
Iteration 25/25 | Loss: 0.00136733

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.77120399
Iteration 2/25 | Loss: 0.00084686
Iteration 3/25 | Loss: 0.00084663
Iteration 4/25 | Loss: 0.00084663
Iteration 5/25 | Loss: 0.00084663
Iteration 6/25 | Loss: 0.00084663
Iteration 7/25 | Loss: 0.00084663
Iteration 8/25 | Loss: 0.00084663
Iteration 9/25 | Loss: 0.00084663
Iteration 10/25 | Loss: 0.00084663
Iteration 11/25 | Loss: 0.00084663
Iteration 12/25 | Loss: 0.00084663
Iteration 13/25 | Loss: 0.00084663
Iteration 14/25 | Loss: 0.00084663
Iteration 15/25 | Loss: 0.00084663
Iteration 16/25 | Loss: 0.00084663
Iteration 17/25 | Loss: 0.00084663
Iteration 18/25 | Loss: 0.00084663
Iteration 19/25 | Loss: 0.00084663
Iteration 20/25 | Loss: 0.00084663
Iteration 21/25 | Loss: 0.00084663
Iteration 22/25 | Loss: 0.00084663
Iteration 23/25 | Loss: 0.00084663
Iteration 24/25 | Loss: 0.00084663
Iteration 25/25 | Loss: 0.00084663
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0008466278668493032, 0.0008466278668493032, 0.0008466278668493032, 0.0008466278668493032, 0.0008466278668493032]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008466278668493032

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084663
Iteration 2/1000 | Loss: 0.00032814
Iteration 3/1000 | Loss: 0.00004505
Iteration 4/1000 | Loss: 0.00003314
Iteration 5/1000 | Loss: 0.00002953
Iteration 6/1000 | Loss: 0.00002769
Iteration 7/1000 | Loss: 0.00002670
Iteration 8/1000 | Loss: 0.00025778
Iteration 9/1000 | Loss: 0.00003558
Iteration 10/1000 | Loss: 0.00003009
Iteration 11/1000 | Loss: 0.00002862
Iteration 12/1000 | Loss: 0.00002767
Iteration 13/1000 | Loss: 0.00002730
Iteration 14/1000 | Loss: 0.00002672
Iteration 15/1000 | Loss: 0.00002617
Iteration 16/1000 | Loss: 0.00014630
Iteration 17/1000 | Loss: 0.00009652
Iteration 18/1000 | Loss: 0.00039871
Iteration 19/1000 | Loss: 0.00002450
Iteration 20/1000 | Loss: 0.00002384
Iteration 21/1000 | Loss: 0.00002338
Iteration 22/1000 | Loss: 0.00007709
Iteration 23/1000 | Loss: 0.00002312
Iteration 24/1000 | Loss: 0.00002284
Iteration 25/1000 | Loss: 0.00002246
Iteration 26/1000 | Loss: 0.00002240
Iteration 27/1000 | Loss: 0.00002221
Iteration 28/1000 | Loss: 0.00002211
Iteration 29/1000 | Loss: 0.00002205
Iteration 30/1000 | Loss: 0.00002201
Iteration 31/1000 | Loss: 0.00002193
Iteration 32/1000 | Loss: 0.00002192
Iteration 33/1000 | Loss: 0.00002189
Iteration 34/1000 | Loss: 0.00002186
Iteration 35/1000 | Loss: 0.00002186
Iteration 36/1000 | Loss: 0.00002184
Iteration 37/1000 | Loss: 0.00002183
Iteration 38/1000 | Loss: 0.00002182
Iteration 39/1000 | Loss: 0.00002182
Iteration 40/1000 | Loss: 0.00002182
Iteration 41/1000 | Loss: 0.00002182
Iteration 42/1000 | Loss: 0.00002182
Iteration 43/1000 | Loss: 0.00002182
Iteration 44/1000 | Loss: 0.00002182
Iteration 45/1000 | Loss: 0.00002182
Iteration 46/1000 | Loss: 0.00002182
Iteration 47/1000 | Loss: 0.00002181
Iteration 48/1000 | Loss: 0.00002181
Iteration 49/1000 | Loss: 0.00002181
Iteration 50/1000 | Loss: 0.00002181
Iteration 51/1000 | Loss: 0.00002180
Iteration 52/1000 | Loss: 0.00002180
Iteration 53/1000 | Loss: 0.00002180
Iteration 54/1000 | Loss: 0.00002179
Iteration 55/1000 | Loss: 0.00002179
Iteration 56/1000 | Loss: 0.00002178
Iteration 57/1000 | Loss: 0.00002178
Iteration 58/1000 | Loss: 0.00002178
Iteration 59/1000 | Loss: 0.00002177
Iteration 60/1000 | Loss: 0.00002177
Iteration 61/1000 | Loss: 0.00002176
Iteration 62/1000 | Loss: 0.00002176
Iteration 63/1000 | Loss: 0.00002176
Iteration 64/1000 | Loss: 0.00002175
Iteration 65/1000 | Loss: 0.00002175
Iteration 66/1000 | Loss: 0.00002175
Iteration 67/1000 | Loss: 0.00002174
Iteration 68/1000 | Loss: 0.00002174
Iteration 69/1000 | Loss: 0.00002173
Iteration 70/1000 | Loss: 0.00002173
Iteration 71/1000 | Loss: 0.00002172
Iteration 72/1000 | Loss: 0.00002171
Iteration 73/1000 | Loss: 0.00002171
Iteration 74/1000 | Loss: 0.00002171
Iteration 75/1000 | Loss: 0.00002171
Iteration 76/1000 | Loss: 0.00002171
Iteration 77/1000 | Loss: 0.00002170
Iteration 78/1000 | Loss: 0.00002170
Iteration 79/1000 | Loss: 0.00002170
Iteration 80/1000 | Loss: 0.00002170
Iteration 81/1000 | Loss: 0.00002170
Iteration 82/1000 | Loss: 0.00002170
Iteration 83/1000 | Loss: 0.00002170
Iteration 84/1000 | Loss: 0.00002170
Iteration 85/1000 | Loss: 0.00002170
Iteration 86/1000 | Loss: 0.00002170
Iteration 87/1000 | Loss: 0.00002169
Iteration 88/1000 | Loss: 0.00002169
Iteration 89/1000 | Loss: 0.00002169
Iteration 90/1000 | Loss: 0.00002169
Iteration 91/1000 | Loss: 0.00002169
Iteration 92/1000 | Loss: 0.00002169
Iteration 93/1000 | Loss: 0.00002169
Iteration 94/1000 | Loss: 0.00002169
Iteration 95/1000 | Loss: 0.00002169
Iteration 96/1000 | Loss: 0.00002169
Iteration 97/1000 | Loss: 0.00002169
Iteration 98/1000 | Loss: 0.00002169
Iteration 99/1000 | Loss: 0.00002169
Iteration 100/1000 | Loss: 0.00002169
Iteration 101/1000 | Loss: 0.00002169
Iteration 102/1000 | Loss: 0.00002169
Iteration 103/1000 | Loss: 0.00002169
Iteration 104/1000 | Loss: 0.00002169
Iteration 105/1000 | Loss: 0.00002168
Iteration 106/1000 | Loss: 0.00002168
Iteration 107/1000 | Loss: 0.00002168
Iteration 108/1000 | Loss: 0.00002168
Iteration 109/1000 | Loss: 0.00002167
Iteration 110/1000 | Loss: 0.00002167
Iteration 111/1000 | Loss: 0.00002167
Iteration 112/1000 | Loss: 0.00002167
Iteration 113/1000 | Loss: 0.00002167
Iteration 114/1000 | Loss: 0.00002167
Iteration 115/1000 | Loss: 0.00002167
Iteration 116/1000 | Loss: 0.00002166
Iteration 117/1000 | Loss: 0.00002166
Iteration 118/1000 | Loss: 0.00002166
Iteration 119/1000 | Loss: 0.00002166
Iteration 120/1000 | Loss: 0.00002166
Iteration 121/1000 | Loss: 0.00002166
Iteration 122/1000 | Loss: 0.00002166
Iteration 123/1000 | Loss: 0.00002166
Iteration 124/1000 | Loss: 0.00002166
Iteration 125/1000 | Loss: 0.00002166
Iteration 126/1000 | Loss: 0.00002166
Iteration 127/1000 | Loss: 0.00002166
Iteration 128/1000 | Loss: 0.00002166
Iteration 129/1000 | Loss: 0.00002166
Iteration 130/1000 | Loss: 0.00002166
Iteration 131/1000 | Loss: 0.00002166
Iteration 132/1000 | Loss: 0.00002166
Iteration 133/1000 | Loss: 0.00002166
Iteration 134/1000 | Loss: 0.00002166
Iteration 135/1000 | Loss: 0.00002166
Iteration 136/1000 | Loss: 0.00002166
Iteration 137/1000 | Loss: 0.00002166
Iteration 138/1000 | Loss: 0.00002166
Iteration 139/1000 | Loss: 0.00002166
Iteration 140/1000 | Loss: 0.00002166
Iteration 141/1000 | Loss: 0.00002166
Iteration 142/1000 | Loss: 0.00002166
Iteration 143/1000 | Loss: 0.00002166
Iteration 144/1000 | Loss: 0.00002166
Iteration 145/1000 | Loss: 0.00002166
Iteration 146/1000 | Loss: 0.00002166
Iteration 147/1000 | Loss: 0.00002166
Iteration 148/1000 | Loss: 0.00002166
Iteration 149/1000 | Loss: 0.00002166
Iteration 150/1000 | Loss: 0.00002166
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [2.1657451725332066e-05, 2.1657451725332066e-05, 2.1657451725332066e-05, 2.1657451725332066e-05, 2.1657451725332066e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1657451725332066e-05

Optimization complete. Final v2v error: 3.8757131099700928 mm

Highest mean error: 5.878029823303223 mm for frame 98

Lowest mean error: 3.2548165321350098 mm for frame 19

Saving results

Total time: 84.63268733024597
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00586382
Iteration 2/25 | Loss: 0.00136782
Iteration 3/25 | Loss: 0.00130482
Iteration 4/25 | Loss: 0.00129196
Iteration 5/25 | Loss: 0.00128919
Iteration 6/25 | Loss: 0.00128910
Iteration 7/25 | Loss: 0.00128910
Iteration 8/25 | Loss: 0.00128910
Iteration 9/25 | Loss: 0.00128910
Iteration 10/25 | Loss: 0.00128910
Iteration 11/25 | Loss: 0.00128910
Iteration 12/25 | Loss: 0.00128910
Iteration 13/25 | Loss: 0.00128910
Iteration 14/25 | Loss: 0.00128910
Iteration 15/25 | Loss: 0.00128910
Iteration 16/25 | Loss: 0.00128910
Iteration 17/25 | Loss: 0.00128910
Iteration 18/25 | Loss: 0.00128910
Iteration 19/25 | Loss: 0.00128910
Iteration 20/25 | Loss: 0.00128910
Iteration 21/25 | Loss: 0.00128910
Iteration 22/25 | Loss: 0.00128910
Iteration 23/25 | Loss: 0.00128910
Iteration 24/25 | Loss: 0.00128910
Iteration 25/25 | Loss: 0.00128910

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.21778488
Iteration 2/25 | Loss: 0.00088847
Iteration 3/25 | Loss: 0.00088846
Iteration 4/25 | Loss: 0.00088846
Iteration 5/25 | Loss: 0.00088846
Iteration 6/25 | Loss: 0.00088846
Iteration 7/25 | Loss: 0.00088846
Iteration 8/25 | Loss: 0.00088846
Iteration 9/25 | Loss: 0.00088846
Iteration 10/25 | Loss: 0.00088846
Iteration 11/25 | Loss: 0.00088846
Iteration 12/25 | Loss: 0.00088846
Iteration 13/25 | Loss: 0.00088846
Iteration 14/25 | Loss: 0.00088846
Iteration 15/25 | Loss: 0.00088846
Iteration 16/25 | Loss: 0.00088846
Iteration 17/25 | Loss: 0.00088846
Iteration 18/25 | Loss: 0.00088846
Iteration 19/25 | Loss: 0.00088846
Iteration 20/25 | Loss: 0.00088846
Iteration 21/25 | Loss: 0.00088846
Iteration 22/25 | Loss: 0.00088846
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008884625276550651, 0.0008884625276550651, 0.0008884625276550651, 0.0008884625276550651, 0.0008884625276550651]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008884625276550651

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088846
Iteration 2/1000 | Loss: 0.00003218
Iteration 3/1000 | Loss: 0.00002501
Iteration 4/1000 | Loss: 0.00002282
Iteration 5/1000 | Loss: 0.00002218
Iteration 6/1000 | Loss: 0.00002152
Iteration 7/1000 | Loss: 0.00002099
Iteration 8/1000 | Loss: 0.00002073
Iteration 9/1000 | Loss: 0.00002039
Iteration 10/1000 | Loss: 0.00002017
Iteration 11/1000 | Loss: 0.00002000
Iteration 12/1000 | Loss: 0.00001980
Iteration 13/1000 | Loss: 0.00001971
Iteration 14/1000 | Loss: 0.00001970
Iteration 15/1000 | Loss: 0.00001970
Iteration 16/1000 | Loss: 0.00001966
Iteration 17/1000 | Loss: 0.00001965
Iteration 18/1000 | Loss: 0.00001964
Iteration 19/1000 | Loss: 0.00001964
Iteration 20/1000 | Loss: 0.00001962
Iteration 21/1000 | Loss: 0.00001962
Iteration 22/1000 | Loss: 0.00001961
Iteration 23/1000 | Loss: 0.00001959
Iteration 24/1000 | Loss: 0.00001959
Iteration 25/1000 | Loss: 0.00001959
Iteration 26/1000 | Loss: 0.00001959
Iteration 27/1000 | Loss: 0.00001959
Iteration 28/1000 | Loss: 0.00001959
Iteration 29/1000 | Loss: 0.00001959
Iteration 30/1000 | Loss: 0.00001955
Iteration 31/1000 | Loss: 0.00001955
Iteration 32/1000 | Loss: 0.00001955
Iteration 33/1000 | Loss: 0.00001955
Iteration 34/1000 | Loss: 0.00001954
Iteration 35/1000 | Loss: 0.00001954
Iteration 36/1000 | Loss: 0.00001953
Iteration 37/1000 | Loss: 0.00001953
Iteration 38/1000 | Loss: 0.00001947
Iteration 39/1000 | Loss: 0.00001943
Iteration 40/1000 | Loss: 0.00001943
Iteration 41/1000 | Loss: 0.00001942
Iteration 42/1000 | Loss: 0.00001942
Iteration 43/1000 | Loss: 0.00001942
Iteration 44/1000 | Loss: 0.00001942
Iteration 45/1000 | Loss: 0.00001942
Iteration 46/1000 | Loss: 0.00001942
Iteration 47/1000 | Loss: 0.00001942
Iteration 48/1000 | Loss: 0.00001942
Iteration 49/1000 | Loss: 0.00001942
Iteration 50/1000 | Loss: 0.00001941
Iteration 51/1000 | Loss: 0.00001941
Iteration 52/1000 | Loss: 0.00001941
Iteration 53/1000 | Loss: 0.00001941
Iteration 54/1000 | Loss: 0.00001941
Iteration 55/1000 | Loss: 0.00001940
Iteration 56/1000 | Loss: 0.00001939
Iteration 57/1000 | Loss: 0.00001939
Iteration 58/1000 | Loss: 0.00001939
Iteration 59/1000 | Loss: 0.00001938
Iteration 60/1000 | Loss: 0.00001938
Iteration 61/1000 | Loss: 0.00001938
Iteration 62/1000 | Loss: 0.00001938
Iteration 63/1000 | Loss: 0.00001937
Iteration 64/1000 | Loss: 0.00001937
Iteration 65/1000 | Loss: 0.00001937
Iteration 66/1000 | Loss: 0.00001936
Iteration 67/1000 | Loss: 0.00001936
Iteration 68/1000 | Loss: 0.00001936
Iteration 69/1000 | Loss: 0.00001936
Iteration 70/1000 | Loss: 0.00001935
Iteration 71/1000 | Loss: 0.00001935
Iteration 72/1000 | Loss: 0.00001935
Iteration 73/1000 | Loss: 0.00001934
Iteration 74/1000 | Loss: 0.00001934
Iteration 75/1000 | Loss: 0.00001934
Iteration 76/1000 | Loss: 0.00001933
Iteration 77/1000 | Loss: 0.00001933
Iteration 78/1000 | Loss: 0.00001933
Iteration 79/1000 | Loss: 0.00001932
Iteration 80/1000 | Loss: 0.00001931
Iteration 81/1000 | Loss: 0.00001931
Iteration 82/1000 | Loss: 0.00001931
Iteration 83/1000 | Loss: 0.00001931
Iteration 84/1000 | Loss: 0.00001931
Iteration 85/1000 | Loss: 0.00001931
Iteration 86/1000 | Loss: 0.00001930
Iteration 87/1000 | Loss: 0.00001930
Iteration 88/1000 | Loss: 0.00001930
Iteration 89/1000 | Loss: 0.00001930
Iteration 90/1000 | Loss: 0.00001930
Iteration 91/1000 | Loss: 0.00001930
Iteration 92/1000 | Loss: 0.00001930
Iteration 93/1000 | Loss: 0.00001930
Iteration 94/1000 | Loss: 0.00001929
Iteration 95/1000 | Loss: 0.00001929
Iteration 96/1000 | Loss: 0.00001929
Iteration 97/1000 | Loss: 0.00001928
Iteration 98/1000 | Loss: 0.00001928
Iteration 99/1000 | Loss: 0.00001927
Iteration 100/1000 | Loss: 0.00001926
Iteration 101/1000 | Loss: 0.00001926
Iteration 102/1000 | Loss: 0.00001926
Iteration 103/1000 | Loss: 0.00001926
Iteration 104/1000 | Loss: 0.00001926
Iteration 105/1000 | Loss: 0.00001926
Iteration 106/1000 | Loss: 0.00001926
Iteration 107/1000 | Loss: 0.00001926
Iteration 108/1000 | Loss: 0.00001926
Iteration 109/1000 | Loss: 0.00001926
Iteration 110/1000 | Loss: 0.00001926
Iteration 111/1000 | Loss: 0.00001926
Iteration 112/1000 | Loss: 0.00001925
Iteration 113/1000 | Loss: 0.00001925
Iteration 114/1000 | Loss: 0.00001925
Iteration 115/1000 | Loss: 0.00001925
Iteration 116/1000 | Loss: 0.00001924
Iteration 117/1000 | Loss: 0.00001924
Iteration 118/1000 | Loss: 0.00001924
Iteration 119/1000 | Loss: 0.00001924
Iteration 120/1000 | Loss: 0.00001924
Iteration 121/1000 | Loss: 0.00001923
Iteration 122/1000 | Loss: 0.00001923
Iteration 123/1000 | Loss: 0.00001923
Iteration 124/1000 | Loss: 0.00001923
Iteration 125/1000 | Loss: 0.00001923
Iteration 126/1000 | Loss: 0.00001923
Iteration 127/1000 | Loss: 0.00001923
Iteration 128/1000 | Loss: 0.00001923
Iteration 129/1000 | Loss: 0.00001923
Iteration 130/1000 | Loss: 0.00001923
Iteration 131/1000 | Loss: 0.00001923
Iteration 132/1000 | Loss: 0.00001923
Iteration 133/1000 | Loss: 0.00001923
Iteration 134/1000 | Loss: 0.00001922
Iteration 135/1000 | Loss: 0.00001922
Iteration 136/1000 | Loss: 0.00001922
Iteration 137/1000 | Loss: 0.00001922
Iteration 138/1000 | Loss: 0.00001922
Iteration 139/1000 | Loss: 0.00001922
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.922481351357419e-05, 1.922481351357419e-05, 1.922481351357419e-05, 1.922481351357419e-05, 1.922481351357419e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.922481351357419e-05

Optimization complete. Final v2v error: 3.7318525314331055 mm

Highest mean error: 4.048251628875732 mm for frame 95

Lowest mean error: 3.5165274143218994 mm for frame 82

Saving results

Total time: 38.16596031188965
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00813853
Iteration 2/25 | Loss: 0.00155425
Iteration 3/25 | Loss: 0.00138852
Iteration 4/25 | Loss: 0.00133652
Iteration 5/25 | Loss: 0.00133263
Iteration 6/25 | Loss: 0.00132685
Iteration 7/25 | Loss: 0.00132583
Iteration 8/25 | Loss: 0.00132943
Iteration 9/25 | Loss: 0.00132519
Iteration 10/25 | Loss: 0.00132503
Iteration 11/25 | Loss: 0.00132438
Iteration 12/25 | Loss: 0.00132355
Iteration 13/25 | Loss: 0.00132324
Iteration 14/25 | Loss: 0.00132317
Iteration 15/25 | Loss: 0.00132317
Iteration 16/25 | Loss: 0.00132317
Iteration 17/25 | Loss: 0.00132317
Iteration 18/25 | Loss: 0.00132316
Iteration 19/25 | Loss: 0.00132315
Iteration 20/25 | Loss: 0.00132315
Iteration 21/25 | Loss: 0.00132315
Iteration 22/25 | Loss: 0.00132315
Iteration 23/25 | Loss: 0.00132315
Iteration 24/25 | Loss: 0.00132315
Iteration 25/25 | Loss: 0.00132315

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.97985125
Iteration 2/25 | Loss: 0.00100424
Iteration 3/25 | Loss: 0.00100420
Iteration 4/25 | Loss: 0.00100419
Iteration 5/25 | Loss: 0.00100419
Iteration 6/25 | Loss: 0.00100419
Iteration 7/25 | Loss: 0.00100419
Iteration 8/25 | Loss: 0.00100419
Iteration 9/25 | Loss: 0.00100419
Iteration 10/25 | Loss: 0.00100419
Iteration 11/25 | Loss: 0.00100419
Iteration 12/25 | Loss: 0.00100419
Iteration 13/25 | Loss: 0.00100419
Iteration 14/25 | Loss: 0.00100419
Iteration 15/25 | Loss: 0.00100419
Iteration 16/25 | Loss: 0.00100419
Iteration 17/25 | Loss: 0.00100419
Iteration 18/25 | Loss: 0.00100419
Iteration 19/25 | Loss: 0.00100419
Iteration 20/25 | Loss: 0.00100419
Iteration 21/25 | Loss: 0.00100419
Iteration 22/25 | Loss: 0.00100419
Iteration 23/25 | Loss: 0.00100419
Iteration 24/25 | Loss: 0.00100419
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0010041908826678991, 0.0010041908826678991, 0.0010041908826678991, 0.0010041908826678991, 0.0010041908826678991]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010041908826678991

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100419
Iteration 2/1000 | Loss: 0.00004323
Iteration 3/1000 | Loss: 0.00019635
Iteration 4/1000 | Loss: 0.00002664
Iteration 5/1000 | Loss: 0.00002497
Iteration 6/1000 | Loss: 0.00002368
Iteration 7/1000 | Loss: 0.00002270
Iteration 8/1000 | Loss: 0.00002208
Iteration 9/1000 | Loss: 0.00002159
Iteration 10/1000 | Loss: 0.00002124
Iteration 11/1000 | Loss: 0.00002095
Iteration 12/1000 | Loss: 0.00002069
Iteration 13/1000 | Loss: 0.00002056
Iteration 14/1000 | Loss: 0.00002047
Iteration 15/1000 | Loss: 0.00002039
Iteration 16/1000 | Loss: 0.00002023
Iteration 17/1000 | Loss: 0.00002020
Iteration 18/1000 | Loss: 0.00002019
Iteration 19/1000 | Loss: 0.00002018
Iteration 20/1000 | Loss: 0.00002017
Iteration 21/1000 | Loss: 0.00002017
Iteration 22/1000 | Loss: 0.00002015
Iteration 23/1000 | Loss: 0.00002015
Iteration 24/1000 | Loss: 0.00002014
Iteration 25/1000 | Loss: 0.00002014
Iteration 26/1000 | Loss: 0.00002013
Iteration 27/1000 | Loss: 0.00002011
Iteration 28/1000 | Loss: 0.00002011
Iteration 29/1000 | Loss: 0.00002010
Iteration 30/1000 | Loss: 0.00002010
Iteration 31/1000 | Loss: 0.00002010
Iteration 32/1000 | Loss: 0.00002009
Iteration 33/1000 | Loss: 0.00002007
Iteration 34/1000 | Loss: 0.00002007
Iteration 35/1000 | Loss: 0.00002007
Iteration 36/1000 | Loss: 0.00002007
Iteration 37/1000 | Loss: 0.00002006
Iteration 38/1000 | Loss: 0.00002005
Iteration 39/1000 | Loss: 0.00002005
Iteration 40/1000 | Loss: 0.00002004
Iteration 41/1000 | Loss: 0.00002004
Iteration 42/1000 | Loss: 0.00002004
Iteration 43/1000 | Loss: 0.00002004
Iteration 44/1000 | Loss: 0.00002004
Iteration 45/1000 | Loss: 0.00002004
Iteration 46/1000 | Loss: 0.00002004
Iteration 47/1000 | Loss: 0.00002003
Iteration 48/1000 | Loss: 0.00002003
Iteration 49/1000 | Loss: 0.00002003
Iteration 50/1000 | Loss: 0.00002002
Iteration 51/1000 | Loss: 0.00002002
Iteration 52/1000 | Loss: 0.00002002
Iteration 53/1000 | Loss: 0.00002001
Iteration 54/1000 | Loss: 0.00002001
Iteration 55/1000 | Loss: 0.00002001
Iteration 56/1000 | Loss: 0.00002001
Iteration 57/1000 | Loss: 0.00002000
Iteration 58/1000 | Loss: 0.00002000
Iteration 59/1000 | Loss: 0.00002000
Iteration 60/1000 | Loss: 0.00001999
Iteration 61/1000 | Loss: 0.00001998
Iteration 62/1000 | Loss: 0.00001998
Iteration 63/1000 | Loss: 0.00001998
Iteration 64/1000 | Loss: 0.00001997
Iteration 65/1000 | Loss: 0.00001997
Iteration 66/1000 | Loss: 0.00001997
Iteration 67/1000 | Loss: 0.00001996
Iteration 68/1000 | Loss: 0.00001996
Iteration 69/1000 | Loss: 0.00001995
Iteration 70/1000 | Loss: 0.00001995
Iteration 71/1000 | Loss: 0.00001995
Iteration 72/1000 | Loss: 0.00001994
Iteration 73/1000 | Loss: 0.00001994
Iteration 74/1000 | Loss: 0.00001994
Iteration 75/1000 | Loss: 0.00001993
Iteration 76/1000 | Loss: 0.00001993
Iteration 77/1000 | Loss: 0.00001993
Iteration 78/1000 | Loss: 0.00001992
Iteration 79/1000 | Loss: 0.00001992
Iteration 80/1000 | Loss: 0.00001992
Iteration 81/1000 | Loss: 0.00001991
Iteration 82/1000 | Loss: 0.00001991
Iteration 83/1000 | Loss: 0.00001991
Iteration 84/1000 | Loss: 0.00001990
Iteration 85/1000 | Loss: 0.00001989
Iteration 86/1000 | Loss: 0.00001989
Iteration 87/1000 | Loss: 0.00001989
Iteration 88/1000 | Loss: 0.00001989
Iteration 89/1000 | Loss: 0.00001989
Iteration 90/1000 | Loss: 0.00001989
Iteration 91/1000 | Loss: 0.00001988
Iteration 92/1000 | Loss: 0.00001988
Iteration 93/1000 | Loss: 0.00001988
Iteration 94/1000 | Loss: 0.00001987
Iteration 95/1000 | Loss: 0.00001987
Iteration 96/1000 | Loss: 0.00001987
Iteration 97/1000 | Loss: 0.00001986
Iteration 98/1000 | Loss: 0.00001986
Iteration 99/1000 | Loss: 0.00001986
Iteration 100/1000 | Loss: 0.00001986
Iteration 101/1000 | Loss: 0.00001986
Iteration 102/1000 | Loss: 0.00001985
Iteration 103/1000 | Loss: 0.00001985
Iteration 104/1000 | Loss: 0.00001985
Iteration 105/1000 | Loss: 0.00001984
Iteration 106/1000 | Loss: 0.00001984
Iteration 107/1000 | Loss: 0.00001984
Iteration 108/1000 | Loss: 0.00001983
Iteration 109/1000 | Loss: 0.00001983
Iteration 110/1000 | Loss: 0.00001983
Iteration 111/1000 | Loss: 0.00001982
Iteration 112/1000 | Loss: 0.00001982
Iteration 113/1000 | Loss: 0.00001982
Iteration 114/1000 | Loss: 0.00001982
Iteration 115/1000 | Loss: 0.00001981
Iteration 116/1000 | Loss: 0.00001981
Iteration 117/1000 | Loss: 0.00001981
Iteration 118/1000 | Loss: 0.00001980
Iteration 119/1000 | Loss: 0.00001980
Iteration 120/1000 | Loss: 0.00001980
Iteration 121/1000 | Loss: 0.00001980
Iteration 122/1000 | Loss: 0.00001980
Iteration 123/1000 | Loss: 0.00001980
Iteration 124/1000 | Loss: 0.00001979
Iteration 125/1000 | Loss: 0.00001979
Iteration 126/1000 | Loss: 0.00001979
Iteration 127/1000 | Loss: 0.00001979
Iteration 128/1000 | Loss: 0.00001978
Iteration 129/1000 | Loss: 0.00001978
Iteration 130/1000 | Loss: 0.00001978
Iteration 131/1000 | Loss: 0.00001978
Iteration 132/1000 | Loss: 0.00001978
Iteration 133/1000 | Loss: 0.00001978
Iteration 134/1000 | Loss: 0.00001978
Iteration 135/1000 | Loss: 0.00001978
Iteration 136/1000 | Loss: 0.00001977
Iteration 137/1000 | Loss: 0.00001977
Iteration 138/1000 | Loss: 0.00001977
Iteration 139/1000 | Loss: 0.00001977
Iteration 140/1000 | Loss: 0.00001977
Iteration 141/1000 | Loss: 0.00001977
Iteration 142/1000 | Loss: 0.00001977
Iteration 143/1000 | Loss: 0.00001977
Iteration 144/1000 | Loss: 0.00001977
Iteration 145/1000 | Loss: 0.00001977
Iteration 146/1000 | Loss: 0.00001976
Iteration 147/1000 | Loss: 0.00001976
Iteration 148/1000 | Loss: 0.00001976
Iteration 149/1000 | Loss: 0.00001976
Iteration 150/1000 | Loss: 0.00001976
Iteration 151/1000 | Loss: 0.00001976
Iteration 152/1000 | Loss: 0.00001976
Iteration 153/1000 | Loss: 0.00001976
Iteration 154/1000 | Loss: 0.00001976
Iteration 155/1000 | Loss: 0.00001976
Iteration 156/1000 | Loss: 0.00001976
Iteration 157/1000 | Loss: 0.00001976
Iteration 158/1000 | Loss: 0.00001976
Iteration 159/1000 | Loss: 0.00001976
Iteration 160/1000 | Loss: 0.00001976
Iteration 161/1000 | Loss: 0.00001976
Iteration 162/1000 | Loss: 0.00001976
Iteration 163/1000 | Loss: 0.00001976
Iteration 164/1000 | Loss: 0.00001976
Iteration 165/1000 | Loss: 0.00001976
Iteration 166/1000 | Loss: 0.00001976
Iteration 167/1000 | Loss: 0.00001976
Iteration 168/1000 | Loss: 0.00001975
Iteration 169/1000 | Loss: 0.00001975
Iteration 170/1000 | Loss: 0.00001975
Iteration 171/1000 | Loss: 0.00001975
Iteration 172/1000 | Loss: 0.00001975
Iteration 173/1000 | Loss: 0.00001975
Iteration 174/1000 | Loss: 0.00001975
Iteration 175/1000 | Loss: 0.00001975
Iteration 176/1000 | Loss: 0.00001975
Iteration 177/1000 | Loss: 0.00001975
Iteration 178/1000 | Loss: 0.00001975
Iteration 179/1000 | Loss: 0.00001975
Iteration 180/1000 | Loss: 0.00001975
Iteration 181/1000 | Loss: 0.00001975
Iteration 182/1000 | Loss: 0.00001975
Iteration 183/1000 | Loss: 0.00001975
Iteration 184/1000 | Loss: 0.00001975
Iteration 185/1000 | Loss: 0.00001975
Iteration 186/1000 | Loss: 0.00001975
Iteration 187/1000 | Loss: 0.00001975
Iteration 188/1000 | Loss: 0.00001975
Iteration 189/1000 | Loss: 0.00001975
Iteration 190/1000 | Loss: 0.00001975
Iteration 191/1000 | Loss: 0.00001975
Iteration 192/1000 | Loss: 0.00001975
Iteration 193/1000 | Loss: 0.00001975
Iteration 194/1000 | Loss: 0.00001975
Iteration 195/1000 | Loss: 0.00001975
Iteration 196/1000 | Loss: 0.00001975
Iteration 197/1000 | Loss: 0.00001975
Iteration 198/1000 | Loss: 0.00001975
Iteration 199/1000 | Loss: 0.00001975
Iteration 200/1000 | Loss: 0.00001975
Iteration 201/1000 | Loss: 0.00001975
Iteration 202/1000 | Loss: 0.00001975
Iteration 203/1000 | Loss: 0.00001975
Iteration 204/1000 | Loss: 0.00001975
Iteration 205/1000 | Loss: 0.00001975
Iteration 206/1000 | Loss: 0.00001975
Iteration 207/1000 | Loss: 0.00001975
Iteration 208/1000 | Loss: 0.00001975
Iteration 209/1000 | Loss: 0.00001975
Iteration 210/1000 | Loss: 0.00001975
Iteration 211/1000 | Loss: 0.00001975
Iteration 212/1000 | Loss: 0.00001975
Iteration 213/1000 | Loss: 0.00001975
Iteration 214/1000 | Loss: 0.00001975
Iteration 215/1000 | Loss: 0.00001975
Iteration 216/1000 | Loss: 0.00001975
Iteration 217/1000 | Loss: 0.00001975
Iteration 218/1000 | Loss: 0.00001975
Iteration 219/1000 | Loss: 0.00001975
Iteration 220/1000 | Loss: 0.00001975
Iteration 221/1000 | Loss: 0.00001975
Iteration 222/1000 | Loss: 0.00001975
Iteration 223/1000 | Loss: 0.00001975
Iteration 224/1000 | Loss: 0.00001975
Iteration 225/1000 | Loss: 0.00001975
Iteration 226/1000 | Loss: 0.00001975
Iteration 227/1000 | Loss: 0.00001975
Iteration 228/1000 | Loss: 0.00001975
Iteration 229/1000 | Loss: 0.00001975
Iteration 230/1000 | Loss: 0.00001975
Iteration 231/1000 | Loss: 0.00001975
Iteration 232/1000 | Loss: 0.00001975
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 232. Stopping optimization.
Last 5 losses: [1.9748516933759674e-05, 1.9748516933759674e-05, 1.9748516933759674e-05, 1.9748516933759674e-05, 1.9748516933759674e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9748516933759674e-05

Optimization complete. Final v2v error: 3.6848347187042236 mm

Highest mean error: 4.496159076690674 mm for frame 56

Lowest mean error: 3.0734405517578125 mm for frame 108

Saving results

Total time: 60.231666564941406
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00947153
Iteration 2/25 | Loss: 0.00355454
Iteration 3/25 | Loss: 0.00239574
Iteration 4/25 | Loss: 0.00222308
Iteration 5/25 | Loss: 0.00203482
Iteration 6/25 | Loss: 0.00189822
Iteration 7/25 | Loss: 0.00185797
Iteration 8/25 | Loss: 0.00179284
Iteration 9/25 | Loss: 0.00175402
Iteration 10/25 | Loss: 0.00169546
Iteration 11/25 | Loss: 0.00167529
Iteration 12/25 | Loss: 0.00165845
Iteration 13/25 | Loss: 0.00165285
Iteration 14/25 | Loss: 0.00163634
Iteration 15/25 | Loss: 0.00162226
Iteration 16/25 | Loss: 0.00161584
Iteration 17/25 | Loss: 0.00160888
Iteration 18/25 | Loss: 0.00161602
Iteration 19/25 | Loss: 0.00160086
Iteration 20/25 | Loss: 0.00157175
Iteration 21/25 | Loss: 0.00156422
Iteration 22/25 | Loss: 0.00156580
Iteration 23/25 | Loss: 0.00155805
Iteration 24/25 | Loss: 0.00154375
Iteration 25/25 | Loss: 0.00152881

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41969562
Iteration 2/25 | Loss: 0.00308704
Iteration 3/25 | Loss: 0.00296653
Iteration 4/25 | Loss: 0.00296638
Iteration 5/25 | Loss: 0.00296637
Iteration 6/25 | Loss: 0.00296637
Iteration 7/25 | Loss: 0.00296637
Iteration 8/25 | Loss: 0.00296637
Iteration 9/25 | Loss: 0.00296637
Iteration 10/25 | Loss: 0.00296637
Iteration 11/25 | Loss: 0.00296637
Iteration 12/25 | Loss: 0.00296637
Iteration 13/25 | Loss: 0.00296637
Iteration 14/25 | Loss: 0.00296637
Iteration 15/25 | Loss: 0.00296637
Iteration 16/25 | Loss: 0.00296637
Iteration 17/25 | Loss: 0.00296637
Iteration 18/25 | Loss: 0.00296637
Iteration 19/25 | Loss: 0.00296637
Iteration 20/25 | Loss: 0.00296637
Iteration 21/25 | Loss: 0.00296637
Iteration 22/25 | Loss: 0.00296637
Iteration 23/25 | Loss: 0.00296637
Iteration 24/25 | Loss: 0.00296637
Iteration 25/25 | Loss: 0.00296637

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00296637
Iteration 2/1000 | Loss: 0.00109894
Iteration 3/1000 | Loss: 0.00101318
Iteration 4/1000 | Loss: 0.00229173
Iteration 5/1000 | Loss: 0.00457085
Iteration 6/1000 | Loss: 0.00142977
Iteration 7/1000 | Loss: 0.00115010
Iteration 8/1000 | Loss: 0.00082047
Iteration 9/1000 | Loss: 0.00026228
Iteration 10/1000 | Loss: 0.00122364
Iteration 11/1000 | Loss: 0.00024143
Iteration 12/1000 | Loss: 0.00013101
Iteration 13/1000 | Loss: 0.00033286
Iteration 14/1000 | Loss: 0.00017520
Iteration 15/1000 | Loss: 0.00050460
Iteration 16/1000 | Loss: 0.00126206
Iteration 17/1000 | Loss: 0.00046168
Iteration 18/1000 | Loss: 0.00032570
Iteration 19/1000 | Loss: 0.00053250
Iteration 20/1000 | Loss: 0.00124604
Iteration 21/1000 | Loss: 0.00012014
Iteration 22/1000 | Loss: 0.00024257
Iteration 23/1000 | Loss: 0.00009847
Iteration 24/1000 | Loss: 0.00075985
Iteration 25/1000 | Loss: 0.00150470
Iteration 26/1000 | Loss: 0.00220199
Iteration 27/1000 | Loss: 0.00312809
Iteration 28/1000 | Loss: 0.00013093
Iteration 29/1000 | Loss: 0.00035612
Iteration 30/1000 | Loss: 0.00029231
Iteration 31/1000 | Loss: 0.00009431
Iteration 32/1000 | Loss: 0.00009506
Iteration 33/1000 | Loss: 0.00033548
Iteration 34/1000 | Loss: 0.00096077
Iteration 35/1000 | Loss: 0.00077013
Iteration 36/1000 | Loss: 0.00035400
Iteration 37/1000 | Loss: 0.00059444
Iteration 38/1000 | Loss: 0.00009514
Iteration 39/1000 | Loss: 0.00042510
Iteration 40/1000 | Loss: 0.00027590
Iteration 41/1000 | Loss: 0.00030355
Iteration 42/1000 | Loss: 0.00011093
Iteration 43/1000 | Loss: 0.00042650
Iteration 44/1000 | Loss: 0.00041696
Iteration 45/1000 | Loss: 0.00029703
Iteration 46/1000 | Loss: 0.00015066
Iteration 47/1000 | Loss: 0.00025128
Iteration 48/1000 | Loss: 0.00015744
Iteration 49/1000 | Loss: 0.00012857
Iteration 50/1000 | Loss: 0.00016661
Iteration 51/1000 | Loss: 0.00058842
Iteration 52/1000 | Loss: 0.00035965
Iteration 53/1000 | Loss: 0.00027691
Iteration 54/1000 | Loss: 0.00029424
Iteration 55/1000 | Loss: 0.00014053
Iteration 56/1000 | Loss: 0.00012385
Iteration 57/1000 | Loss: 0.00008932
Iteration 58/1000 | Loss: 0.00021846
Iteration 59/1000 | Loss: 0.00030504
Iteration 60/1000 | Loss: 0.00008577
Iteration 61/1000 | Loss: 0.00008004
Iteration 62/1000 | Loss: 0.00007936
Iteration 63/1000 | Loss: 0.00006999
Iteration 64/1000 | Loss: 0.00007535
Iteration 65/1000 | Loss: 0.00008960
Iteration 66/1000 | Loss: 0.00007732
Iteration 67/1000 | Loss: 0.00006402
Iteration 68/1000 | Loss: 0.00007744
Iteration 69/1000 | Loss: 0.00026202
Iteration 70/1000 | Loss: 0.00058682
Iteration 71/1000 | Loss: 0.00044954
Iteration 72/1000 | Loss: 0.00015258
Iteration 73/1000 | Loss: 0.00007509
Iteration 74/1000 | Loss: 0.00008255
Iteration 75/1000 | Loss: 0.00006463
Iteration 76/1000 | Loss: 0.00005951
Iteration 77/1000 | Loss: 0.00014397
Iteration 78/1000 | Loss: 0.00016275
Iteration 79/1000 | Loss: 0.00008671
Iteration 80/1000 | Loss: 0.00006725
Iteration 81/1000 | Loss: 0.00006933
Iteration 82/1000 | Loss: 0.00017771
Iteration 83/1000 | Loss: 0.00005253
Iteration 84/1000 | Loss: 0.00011743
Iteration 85/1000 | Loss: 0.00007533
Iteration 86/1000 | Loss: 0.00004824
Iteration 87/1000 | Loss: 0.00004641
Iteration 88/1000 | Loss: 0.00006689
Iteration 89/1000 | Loss: 0.00008999
Iteration 90/1000 | Loss: 0.00006595
Iteration 91/1000 | Loss: 0.00004513
Iteration 92/1000 | Loss: 0.00004485
Iteration 93/1000 | Loss: 0.00004429
Iteration 94/1000 | Loss: 0.00004394
Iteration 95/1000 | Loss: 0.00004367
Iteration 96/1000 | Loss: 0.00004346
Iteration 97/1000 | Loss: 0.00004342
Iteration 98/1000 | Loss: 0.00004320
Iteration 99/1000 | Loss: 0.00004310
Iteration 100/1000 | Loss: 0.00004294
Iteration 101/1000 | Loss: 0.00004285
Iteration 102/1000 | Loss: 0.00004283
Iteration 103/1000 | Loss: 0.00004282
Iteration 104/1000 | Loss: 0.00004281
Iteration 105/1000 | Loss: 0.00004280
Iteration 106/1000 | Loss: 0.00004279
Iteration 107/1000 | Loss: 0.00004279
Iteration 108/1000 | Loss: 0.00004279
Iteration 109/1000 | Loss: 0.00004279
Iteration 110/1000 | Loss: 0.00004279
Iteration 111/1000 | Loss: 0.00004279
Iteration 112/1000 | Loss: 0.00004279
Iteration 113/1000 | Loss: 0.00004279
Iteration 114/1000 | Loss: 0.00004278
Iteration 115/1000 | Loss: 0.00004278
Iteration 116/1000 | Loss: 0.00004278
Iteration 117/1000 | Loss: 0.00004278
Iteration 118/1000 | Loss: 0.00004278
Iteration 119/1000 | Loss: 0.00004278
Iteration 120/1000 | Loss: 0.00004278
Iteration 121/1000 | Loss: 0.00004277
Iteration 122/1000 | Loss: 0.00004277
Iteration 123/1000 | Loss: 0.00004277
Iteration 124/1000 | Loss: 0.00004276
Iteration 125/1000 | Loss: 0.00004276
Iteration 126/1000 | Loss: 0.00004276
Iteration 127/1000 | Loss: 0.00004276
Iteration 128/1000 | Loss: 0.00004276
Iteration 129/1000 | Loss: 0.00004276
Iteration 130/1000 | Loss: 0.00004276
Iteration 131/1000 | Loss: 0.00004275
Iteration 132/1000 | Loss: 0.00004275
Iteration 133/1000 | Loss: 0.00004275
Iteration 134/1000 | Loss: 0.00004275
Iteration 135/1000 | Loss: 0.00004275
Iteration 136/1000 | Loss: 0.00004275
Iteration 137/1000 | Loss: 0.00004275
Iteration 138/1000 | Loss: 0.00004275
Iteration 139/1000 | Loss: 0.00004274
Iteration 140/1000 | Loss: 0.00004274
Iteration 141/1000 | Loss: 0.00004274
Iteration 142/1000 | Loss: 0.00004274
Iteration 143/1000 | Loss: 0.00004274
Iteration 144/1000 | Loss: 0.00004274
Iteration 145/1000 | Loss: 0.00004274
Iteration 146/1000 | Loss: 0.00004274
Iteration 147/1000 | Loss: 0.00004274
Iteration 148/1000 | Loss: 0.00004274
Iteration 149/1000 | Loss: 0.00004274
Iteration 150/1000 | Loss: 0.00004274
Iteration 151/1000 | Loss: 0.00004274
Iteration 152/1000 | Loss: 0.00004273
Iteration 153/1000 | Loss: 0.00004273
Iteration 154/1000 | Loss: 0.00004273
Iteration 155/1000 | Loss: 0.00004273
Iteration 156/1000 | Loss: 0.00004273
Iteration 157/1000 | Loss: 0.00004273
Iteration 158/1000 | Loss: 0.00004273
Iteration 159/1000 | Loss: 0.00004273
Iteration 160/1000 | Loss: 0.00004273
Iteration 161/1000 | Loss: 0.00004273
Iteration 162/1000 | Loss: 0.00004273
Iteration 163/1000 | Loss: 0.00004273
Iteration 164/1000 | Loss: 0.00004273
Iteration 165/1000 | Loss: 0.00004273
Iteration 166/1000 | Loss: 0.00004273
Iteration 167/1000 | Loss: 0.00004273
Iteration 168/1000 | Loss: 0.00004272
Iteration 169/1000 | Loss: 0.00004272
Iteration 170/1000 | Loss: 0.00004272
Iteration 171/1000 | Loss: 0.00004272
Iteration 172/1000 | Loss: 0.00004272
Iteration 173/1000 | Loss: 0.00004272
Iteration 174/1000 | Loss: 0.00004272
Iteration 175/1000 | Loss: 0.00004272
Iteration 176/1000 | Loss: 0.00004272
Iteration 177/1000 | Loss: 0.00004272
Iteration 178/1000 | Loss: 0.00004272
Iteration 179/1000 | Loss: 0.00004272
Iteration 180/1000 | Loss: 0.00004271
Iteration 181/1000 | Loss: 0.00004271
Iteration 182/1000 | Loss: 0.00004271
Iteration 183/1000 | Loss: 0.00004271
Iteration 184/1000 | Loss: 0.00004271
Iteration 185/1000 | Loss: 0.00004271
Iteration 186/1000 | Loss: 0.00004271
Iteration 187/1000 | Loss: 0.00004271
Iteration 188/1000 | Loss: 0.00004271
Iteration 189/1000 | Loss: 0.00004271
Iteration 190/1000 | Loss: 0.00004271
Iteration 191/1000 | Loss: 0.00004271
Iteration 192/1000 | Loss: 0.00004271
Iteration 193/1000 | Loss: 0.00004270
Iteration 194/1000 | Loss: 0.00004270
Iteration 195/1000 | Loss: 0.00004270
Iteration 196/1000 | Loss: 0.00004270
Iteration 197/1000 | Loss: 0.00004270
Iteration 198/1000 | Loss: 0.00004270
Iteration 199/1000 | Loss: 0.00004270
Iteration 200/1000 | Loss: 0.00004270
Iteration 201/1000 | Loss: 0.00004270
Iteration 202/1000 | Loss: 0.00004270
Iteration 203/1000 | Loss: 0.00004270
Iteration 204/1000 | Loss: 0.00004270
Iteration 205/1000 | Loss: 0.00004270
Iteration 206/1000 | Loss: 0.00004270
Iteration 207/1000 | Loss: 0.00004270
Iteration 208/1000 | Loss: 0.00004270
Iteration 209/1000 | Loss: 0.00004269
Iteration 210/1000 | Loss: 0.00004269
Iteration 211/1000 | Loss: 0.00004269
Iteration 212/1000 | Loss: 0.00004269
Iteration 213/1000 | Loss: 0.00004269
Iteration 214/1000 | Loss: 0.00004269
Iteration 215/1000 | Loss: 0.00004268
Iteration 216/1000 | Loss: 0.00004268
Iteration 217/1000 | Loss: 0.00004268
Iteration 218/1000 | Loss: 0.00004268
Iteration 219/1000 | Loss: 0.00004268
Iteration 220/1000 | Loss: 0.00004267
Iteration 221/1000 | Loss: 0.00004267
Iteration 222/1000 | Loss: 0.00004267
Iteration 223/1000 | Loss: 0.00004267
Iteration 224/1000 | Loss: 0.00004266
Iteration 225/1000 | Loss: 0.00004266
Iteration 226/1000 | Loss: 0.00004266
Iteration 227/1000 | Loss: 0.00004265
Iteration 228/1000 | Loss: 0.00004265
Iteration 229/1000 | Loss: 0.00004265
Iteration 230/1000 | Loss: 0.00004265
Iteration 231/1000 | Loss: 0.00004265
Iteration 232/1000 | Loss: 0.00004264
Iteration 233/1000 | Loss: 0.00004264
Iteration 234/1000 | Loss: 0.00004264
Iteration 235/1000 | Loss: 0.00004264
Iteration 236/1000 | Loss: 0.00004264
Iteration 237/1000 | Loss: 0.00004263
Iteration 238/1000 | Loss: 0.00004263
Iteration 239/1000 | Loss: 0.00004263
Iteration 240/1000 | Loss: 0.00004263
Iteration 241/1000 | Loss: 0.00004263
Iteration 242/1000 | Loss: 0.00004263
Iteration 243/1000 | Loss: 0.00004263
Iteration 244/1000 | Loss: 0.00004262
Iteration 245/1000 | Loss: 0.00004262
Iteration 246/1000 | Loss: 0.00004262
Iteration 247/1000 | Loss: 0.00004262
Iteration 248/1000 | Loss: 0.00004262
Iteration 249/1000 | Loss: 0.00004262
Iteration 250/1000 | Loss: 0.00004262
Iteration 251/1000 | Loss: 0.00004262
Iteration 252/1000 | Loss: 0.00004262
Iteration 253/1000 | Loss: 0.00004262
Iteration 254/1000 | Loss: 0.00004262
Iteration 255/1000 | Loss: 0.00004262
Iteration 256/1000 | Loss: 0.00004262
Iteration 257/1000 | Loss: 0.00004262
Iteration 258/1000 | Loss: 0.00004262
Iteration 259/1000 | Loss: 0.00004262
Iteration 260/1000 | Loss: 0.00004262
Iteration 261/1000 | Loss: 0.00004262
Iteration 262/1000 | Loss: 0.00004262
Iteration 263/1000 | Loss: 0.00004262
Iteration 264/1000 | Loss: 0.00004262
Iteration 265/1000 | Loss: 0.00004262
Iteration 266/1000 | Loss: 0.00004262
Iteration 267/1000 | Loss: 0.00004261
Iteration 268/1000 | Loss: 0.00004261
Iteration 269/1000 | Loss: 0.00004261
Iteration 270/1000 | Loss: 0.00004261
Iteration 271/1000 | Loss: 0.00004261
Iteration 272/1000 | Loss: 0.00004261
Iteration 273/1000 | Loss: 0.00004261
Iteration 274/1000 | Loss: 0.00004261
Iteration 275/1000 | Loss: 0.00004261
Iteration 276/1000 | Loss: 0.00004261
Iteration 277/1000 | Loss: 0.00004261
Iteration 278/1000 | Loss: 0.00004261
Iteration 279/1000 | Loss: 0.00004261
Iteration 280/1000 | Loss: 0.00004261
Iteration 281/1000 | Loss: 0.00004261
Iteration 282/1000 | Loss: 0.00004261
Iteration 283/1000 | Loss: 0.00004261
Iteration 284/1000 | Loss: 0.00004261
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 284. Stopping optimization.
Last 5 losses: [4.2613221012288705e-05, 4.2613221012288705e-05, 4.2613221012288705e-05, 4.2613221012288705e-05, 4.2613221012288705e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.2613221012288705e-05

Optimization complete. Final v2v error: 4.245637893676758 mm

Highest mean error: 10.94131088256836 mm for frame 171

Lowest mean error: 3.4125609397888184 mm for frame 5

Saving results

Total time: 205.03558683395386
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00948373
Iteration 2/25 | Loss: 0.00325650
Iteration 3/25 | Loss: 0.00233873
Iteration 4/25 | Loss: 0.00197854
Iteration 5/25 | Loss: 0.00188606
Iteration 6/25 | Loss: 0.00168823
Iteration 7/25 | Loss: 0.00166139
Iteration 8/25 | Loss: 0.00159623
Iteration 9/25 | Loss: 0.00157924
Iteration 10/25 | Loss: 0.00158061
Iteration 11/25 | Loss: 0.00157600
Iteration 12/25 | Loss: 0.00156514
Iteration 13/25 | Loss: 0.00156406
Iteration 14/25 | Loss: 0.00156365
Iteration 15/25 | Loss: 0.00156577
Iteration 16/25 | Loss: 0.00156377
Iteration 17/25 | Loss: 0.00156245
Iteration 18/25 | Loss: 0.00156025
Iteration 19/25 | Loss: 0.00155962
Iteration 20/25 | Loss: 0.00155937
Iteration 21/25 | Loss: 0.00155934
Iteration 22/25 | Loss: 0.00155933
Iteration 23/25 | Loss: 0.00155933
Iteration 24/25 | Loss: 0.00155930
Iteration 25/25 | Loss: 0.00155930

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39338589
Iteration 2/25 | Loss: 0.00152259
Iteration 3/25 | Loss: 0.00139034
Iteration 4/25 | Loss: 0.00139034
Iteration 5/25 | Loss: 0.00139034
Iteration 6/25 | Loss: 0.00139033
Iteration 7/25 | Loss: 0.00139033
Iteration 8/25 | Loss: 0.00139033
Iteration 9/25 | Loss: 0.00139033
Iteration 10/25 | Loss: 0.00139033
Iteration 11/25 | Loss: 0.00139033
Iteration 12/25 | Loss: 0.00139033
Iteration 13/25 | Loss: 0.00139033
Iteration 14/25 | Loss: 0.00139033
Iteration 15/25 | Loss: 0.00139033
Iteration 16/25 | Loss: 0.00139033
Iteration 17/25 | Loss: 0.00139033
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013903332874178886, 0.0013903332874178886, 0.0013903332874178886, 0.0013903332874178886, 0.0013903332874178886]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013903332874178886

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00139033
Iteration 2/1000 | Loss: 0.00020196
Iteration 3/1000 | Loss: 0.00014020
Iteration 4/1000 | Loss: 0.00040703
Iteration 5/1000 | Loss: 0.00007692
Iteration 6/1000 | Loss: 0.00005623
Iteration 7/1000 | Loss: 0.00005187
Iteration 8/1000 | Loss: 0.00004930
Iteration 9/1000 | Loss: 0.00004731
Iteration 10/1000 | Loss: 0.00004634
Iteration 11/1000 | Loss: 0.00004526
Iteration 12/1000 | Loss: 0.00004425
Iteration 13/1000 | Loss: 0.00004346
Iteration 14/1000 | Loss: 0.00016169
Iteration 15/1000 | Loss: 0.00019112
Iteration 16/1000 | Loss: 0.00008444
Iteration 17/1000 | Loss: 0.00004306
Iteration 18/1000 | Loss: 0.00004237
Iteration 19/1000 | Loss: 0.00004182
Iteration 20/1000 | Loss: 0.00018810
Iteration 21/1000 | Loss: 0.00009321
Iteration 22/1000 | Loss: 0.00004182
Iteration 23/1000 | Loss: 0.00004086
Iteration 24/1000 | Loss: 0.00019472
Iteration 25/1000 | Loss: 0.00004145
Iteration 26/1000 | Loss: 0.00004008
Iteration 27/1000 | Loss: 0.00003963
Iteration 28/1000 | Loss: 0.00003943
Iteration 29/1000 | Loss: 0.00003938
Iteration 30/1000 | Loss: 0.00003929
Iteration 31/1000 | Loss: 0.00003922
Iteration 32/1000 | Loss: 0.00003916
Iteration 33/1000 | Loss: 0.00003916
Iteration 34/1000 | Loss: 0.00003907
Iteration 35/1000 | Loss: 0.00003907
Iteration 36/1000 | Loss: 0.00003907
Iteration 37/1000 | Loss: 0.00003907
Iteration 38/1000 | Loss: 0.00003906
Iteration 39/1000 | Loss: 0.00003906
Iteration 40/1000 | Loss: 0.00003906
Iteration 41/1000 | Loss: 0.00003906
Iteration 42/1000 | Loss: 0.00003905
Iteration 43/1000 | Loss: 0.00003905
Iteration 44/1000 | Loss: 0.00003904
Iteration 45/1000 | Loss: 0.00003902
Iteration 46/1000 | Loss: 0.00003901
Iteration 47/1000 | Loss: 0.00003901
Iteration 48/1000 | Loss: 0.00003901
Iteration 49/1000 | Loss: 0.00003900
Iteration 50/1000 | Loss: 0.00003900
Iteration 51/1000 | Loss: 0.00003900
Iteration 52/1000 | Loss: 0.00003899
Iteration 53/1000 | Loss: 0.00003899
Iteration 54/1000 | Loss: 0.00003899
Iteration 55/1000 | Loss: 0.00003898
Iteration 56/1000 | Loss: 0.00003898
Iteration 57/1000 | Loss: 0.00003897
Iteration 58/1000 | Loss: 0.00003894
Iteration 59/1000 | Loss: 0.00003894
Iteration 60/1000 | Loss: 0.00003893
Iteration 61/1000 | Loss: 0.00003892
Iteration 62/1000 | Loss: 0.00003892
Iteration 63/1000 | Loss: 0.00003892
Iteration 64/1000 | Loss: 0.00003892
Iteration 65/1000 | Loss: 0.00003891
Iteration 66/1000 | Loss: 0.00003890
Iteration 67/1000 | Loss: 0.00003890
Iteration 68/1000 | Loss: 0.00003890
Iteration 69/1000 | Loss: 0.00003890
Iteration 70/1000 | Loss: 0.00003890
Iteration 71/1000 | Loss: 0.00003890
Iteration 72/1000 | Loss: 0.00003890
Iteration 73/1000 | Loss: 0.00003890
Iteration 74/1000 | Loss: 0.00003890
Iteration 75/1000 | Loss: 0.00003889
Iteration 76/1000 | Loss: 0.00003889
Iteration 77/1000 | Loss: 0.00003889
Iteration 78/1000 | Loss: 0.00003889
Iteration 79/1000 | Loss: 0.00003889
Iteration 80/1000 | Loss: 0.00003889
Iteration 81/1000 | Loss: 0.00003889
Iteration 82/1000 | Loss: 0.00003889
Iteration 83/1000 | Loss: 0.00003889
Iteration 84/1000 | Loss: 0.00003889
Iteration 85/1000 | Loss: 0.00003889
Iteration 86/1000 | Loss: 0.00003889
Iteration 87/1000 | Loss: 0.00003889
Iteration 88/1000 | Loss: 0.00003889
Iteration 89/1000 | Loss: 0.00003889
Iteration 90/1000 | Loss: 0.00003889
Iteration 91/1000 | Loss: 0.00003889
Iteration 92/1000 | Loss: 0.00003889
Iteration 93/1000 | Loss: 0.00003889
Iteration 94/1000 | Loss: 0.00003889
Iteration 95/1000 | Loss: 0.00003889
Iteration 96/1000 | Loss: 0.00003889
Iteration 97/1000 | Loss: 0.00003889
Iteration 98/1000 | Loss: 0.00003889
Iteration 99/1000 | Loss: 0.00003889
Iteration 100/1000 | Loss: 0.00003889
Iteration 101/1000 | Loss: 0.00003889
Iteration 102/1000 | Loss: 0.00003889
Iteration 103/1000 | Loss: 0.00003889
Iteration 104/1000 | Loss: 0.00003889
Iteration 105/1000 | Loss: 0.00003889
Iteration 106/1000 | Loss: 0.00003889
Iteration 107/1000 | Loss: 0.00003889
Iteration 108/1000 | Loss: 0.00003889
Iteration 109/1000 | Loss: 0.00003889
Iteration 110/1000 | Loss: 0.00003889
Iteration 111/1000 | Loss: 0.00003889
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [3.889192521455698e-05, 3.889192521455698e-05, 3.889192521455698e-05, 3.889192521455698e-05, 3.889192521455698e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.889192521455698e-05

Optimization complete. Final v2v error: 5.134781837463379 mm

Highest mean error: 11.45862865447998 mm for frame 232

Lowest mean error: 4.0155792236328125 mm for frame 155

Saving results

Total time: 98.88449883460999
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00814891
Iteration 2/25 | Loss: 0.00157826
Iteration 3/25 | Loss: 0.00133470
Iteration 4/25 | Loss: 0.00132224
Iteration 5/25 | Loss: 0.00132008
Iteration 6/25 | Loss: 0.00132003
Iteration 7/25 | Loss: 0.00132003
Iteration 8/25 | Loss: 0.00132003
Iteration 9/25 | Loss: 0.00132003
Iteration 10/25 | Loss: 0.00132003
Iteration 11/25 | Loss: 0.00132003
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013200263492763042, 0.0013200263492763042, 0.0013200263492763042, 0.0013200263492763042, 0.0013200263492763042]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013200263492763042

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.02117884
Iteration 2/25 | Loss: 0.00060095
Iteration 3/25 | Loss: 0.00060093
Iteration 4/25 | Loss: 0.00060093
Iteration 5/25 | Loss: 0.00060093
Iteration 6/25 | Loss: 0.00060093
Iteration 7/25 | Loss: 0.00060093
Iteration 8/25 | Loss: 0.00060093
Iteration 9/25 | Loss: 0.00060093
Iteration 10/25 | Loss: 0.00060093
Iteration 11/25 | Loss: 0.00060093
Iteration 12/25 | Loss: 0.00060093
Iteration 13/25 | Loss: 0.00060093
Iteration 14/25 | Loss: 0.00060093
Iteration 15/25 | Loss: 0.00060093
Iteration 16/25 | Loss: 0.00060093
Iteration 17/25 | Loss: 0.00060093
Iteration 18/25 | Loss: 0.00060093
Iteration 19/25 | Loss: 0.00060093
Iteration 20/25 | Loss: 0.00060093
Iteration 21/25 | Loss: 0.00060093
Iteration 22/25 | Loss: 0.00060093
Iteration 23/25 | Loss: 0.00060093
Iteration 24/25 | Loss: 0.00060093
Iteration 25/25 | Loss: 0.00060093

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060093
Iteration 2/1000 | Loss: 0.00003735
Iteration 3/1000 | Loss: 0.00002548
Iteration 4/1000 | Loss: 0.00002312
Iteration 5/1000 | Loss: 0.00002206
Iteration 6/1000 | Loss: 0.00002143
Iteration 7/1000 | Loss: 0.00002095
Iteration 8/1000 | Loss: 0.00002061
Iteration 9/1000 | Loss: 0.00002025
Iteration 10/1000 | Loss: 0.00002014
Iteration 11/1000 | Loss: 0.00001990
Iteration 12/1000 | Loss: 0.00001971
Iteration 13/1000 | Loss: 0.00001958
Iteration 14/1000 | Loss: 0.00001956
Iteration 15/1000 | Loss: 0.00001953
Iteration 16/1000 | Loss: 0.00001944
Iteration 17/1000 | Loss: 0.00001941
Iteration 18/1000 | Loss: 0.00001941
Iteration 19/1000 | Loss: 0.00001939
Iteration 20/1000 | Loss: 0.00001936
Iteration 21/1000 | Loss: 0.00001936
Iteration 22/1000 | Loss: 0.00001936
Iteration 23/1000 | Loss: 0.00001936
Iteration 24/1000 | Loss: 0.00001934
Iteration 25/1000 | Loss: 0.00001928
Iteration 26/1000 | Loss: 0.00001928
Iteration 27/1000 | Loss: 0.00001927
Iteration 28/1000 | Loss: 0.00001927
Iteration 29/1000 | Loss: 0.00001925
Iteration 30/1000 | Loss: 0.00001925
Iteration 31/1000 | Loss: 0.00001925
Iteration 32/1000 | Loss: 0.00001925
Iteration 33/1000 | Loss: 0.00001925
Iteration 34/1000 | Loss: 0.00001924
Iteration 35/1000 | Loss: 0.00001924
Iteration 36/1000 | Loss: 0.00001924
Iteration 37/1000 | Loss: 0.00001924
Iteration 38/1000 | Loss: 0.00001924
Iteration 39/1000 | Loss: 0.00001923
Iteration 40/1000 | Loss: 0.00001923
Iteration 41/1000 | Loss: 0.00001923
Iteration 42/1000 | Loss: 0.00001923
Iteration 43/1000 | Loss: 0.00001922
Iteration 44/1000 | Loss: 0.00001922
Iteration 45/1000 | Loss: 0.00001922
Iteration 46/1000 | Loss: 0.00001921
Iteration 47/1000 | Loss: 0.00001918
Iteration 48/1000 | Loss: 0.00001918
Iteration 49/1000 | Loss: 0.00001917
Iteration 50/1000 | Loss: 0.00001917
Iteration 51/1000 | Loss: 0.00001916
Iteration 52/1000 | Loss: 0.00001916
Iteration 53/1000 | Loss: 0.00001916
Iteration 54/1000 | Loss: 0.00001915
Iteration 55/1000 | Loss: 0.00001915
Iteration 56/1000 | Loss: 0.00001915
Iteration 57/1000 | Loss: 0.00001915
Iteration 58/1000 | Loss: 0.00001915
Iteration 59/1000 | Loss: 0.00001915
Iteration 60/1000 | Loss: 0.00001915
Iteration 61/1000 | Loss: 0.00001915
Iteration 62/1000 | Loss: 0.00001915
Iteration 63/1000 | Loss: 0.00001915
Iteration 64/1000 | Loss: 0.00001914
Iteration 65/1000 | Loss: 0.00001914
Iteration 66/1000 | Loss: 0.00001914
Iteration 67/1000 | Loss: 0.00001913
Iteration 68/1000 | Loss: 0.00001913
Iteration 69/1000 | Loss: 0.00001912
Iteration 70/1000 | Loss: 0.00001912
Iteration 71/1000 | Loss: 0.00001911
Iteration 72/1000 | Loss: 0.00001911
Iteration 73/1000 | Loss: 0.00001911
Iteration 74/1000 | Loss: 0.00001910
Iteration 75/1000 | Loss: 0.00001910
Iteration 76/1000 | Loss: 0.00001910
Iteration 77/1000 | Loss: 0.00001910
Iteration 78/1000 | Loss: 0.00001910
Iteration 79/1000 | Loss: 0.00001910
Iteration 80/1000 | Loss: 0.00001910
Iteration 81/1000 | Loss: 0.00001910
Iteration 82/1000 | Loss: 0.00001910
Iteration 83/1000 | Loss: 0.00001909
Iteration 84/1000 | Loss: 0.00001908
Iteration 85/1000 | Loss: 0.00001908
Iteration 86/1000 | Loss: 0.00001908
Iteration 87/1000 | Loss: 0.00001908
Iteration 88/1000 | Loss: 0.00001908
Iteration 89/1000 | Loss: 0.00001908
Iteration 90/1000 | Loss: 0.00001908
Iteration 91/1000 | Loss: 0.00001908
Iteration 92/1000 | Loss: 0.00001908
Iteration 93/1000 | Loss: 0.00001908
Iteration 94/1000 | Loss: 0.00001908
Iteration 95/1000 | Loss: 0.00001908
Iteration 96/1000 | Loss: 0.00001907
Iteration 97/1000 | Loss: 0.00001907
Iteration 98/1000 | Loss: 0.00001907
Iteration 99/1000 | Loss: 0.00001907
Iteration 100/1000 | Loss: 0.00001907
Iteration 101/1000 | Loss: 0.00001907
Iteration 102/1000 | Loss: 0.00001906
Iteration 103/1000 | Loss: 0.00001906
Iteration 104/1000 | Loss: 0.00001906
Iteration 105/1000 | Loss: 0.00001905
Iteration 106/1000 | Loss: 0.00001905
Iteration 107/1000 | Loss: 0.00001905
Iteration 108/1000 | Loss: 0.00001905
Iteration 109/1000 | Loss: 0.00001905
Iteration 110/1000 | Loss: 0.00001905
Iteration 111/1000 | Loss: 0.00001905
Iteration 112/1000 | Loss: 0.00001905
Iteration 113/1000 | Loss: 0.00001905
Iteration 114/1000 | Loss: 0.00001905
Iteration 115/1000 | Loss: 0.00001905
Iteration 116/1000 | Loss: 0.00001904
Iteration 117/1000 | Loss: 0.00001904
Iteration 118/1000 | Loss: 0.00001904
Iteration 119/1000 | Loss: 0.00001904
Iteration 120/1000 | Loss: 0.00001903
Iteration 121/1000 | Loss: 0.00001903
Iteration 122/1000 | Loss: 0.00001903
Iteration 123/1000 | Loss: 0.00001903
Iteration 124/1000 | Loss: 0.00001903
Iteration 125/1000 | Loss: 0.00001903
Iteration 126/1000 | Loss: 0.00001903
Iteration 127/1000 | Loss: 0.00001903
Iteration 128/1000 | Loss: 0.00001903
Iteration 129/1000 | Loss: 0.00001903
Iteration 130/1000 | Loss: 0.00001903
Iteration 131/1000 | Loss: 0.00001903
Iteration 132/1000 | Loss: 0.00001902
Iteration 133/1000 | Loss: 0.00001902
Iteration 134/1000 | Loss: 0.00001902
Iteration 135/1000 | Loss: 0.00001902
Iteration 136/1000 | Loss: 0.00001902
Iteration 137/1000 | Loss: 0.00001902
Iteration 138/1000 | Loss: 0.00001902
Iteration 139/1000 | Loss: 0.00001902
Iteration 140/1000 | Loss: 0.00001902
Iteration 141/1000 | Loss: 0.00001902
Iteration 142/1000 | Loss: 0.00001902
Iteration 143/1000 | Loss: 0.00001902
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.9024837456527166e-05, 1.9024837456527166e-05, 1.9024837456527166e-05, 1.9024837456527166e-05, 1.9024837456527166e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9024837456527166e-05

Optimization complete. Final v2v error: 3.631422281265259 mm

Highest mean error: 3.9627625942230225 mm for frame 25

Lowest mean error: 3.4479973316192627 mm for frame 138

Saving results

Total time: 37.164103507995605
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00386599
Iteration 2/25 | Loss: 0.00142680
Iteration 3/25 | Loss: 0.00130614
Iteration 4/25 | Loss: 0.00128701
Iteration 5/25 | Loss: 0.00128042
Iteration 6/25 | Loss: 0.00127849
Iteration 7/25 | Loss: 0.00127754
Iteration 8/25 | Loss: 0.00127754
Iteration 9/25 | Loss: 0.00127754
Iteration 10/25 | Loss: 0.00127754
Iteration 11/25 | Loss: 0.00127754
Iteration 12/25 | Loss: 0.00127754
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012775411596521735, 0.0012775411596521735, 0.0012775411596521735, 0.0012775411596521735, 0.0012775411596521735]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012775411596521735

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50402474
Iteration 2/25 | Loss: 0.00104783
Iteration 3/25 | Loss: 0.00104782
Iteration 4/25 | Loss: 0.00104782
Iteration 5/25 | Loss: 0.00104782
Iteration 6/25 | Loss: 0.00104782
Iteration 7/25 | Loss: 0.00104782
Iteration 8/25 | Loss: 0.00104782
Iteration 9/25 | Loss: 0.00104782
Iteration 10/25 | Loss: 0.00104782
Iteration 11/25 | Loss: 0.00104782
Iteration 12/25 | Loss: 0.00104782
Iteration 13/25 | Loss: 0.00104782
Iteration 14/25 | Loss: 0.00104782
Iteration 15/25 | Loss: 0.00104782
Iteration 16/25 | Loss: 0.00104782
Iteration 17/25 | Loss: 0.00104782
Iteration 18/25 | Loss: 0.00104782
Iteration 19/25 | Loss: 0.00104782
Iteration 20/25 | Loss: 0.00104782
Iteration 21/25 | Loss: 0.00104782
Iteration 22/25 | Loss: 0.00104782
Iteration 23/25 | Loss: 0.00104782
Iteration 24/25 | Loss: 0.00104782
Iteration 25/25 | Loss: 0.00104782

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104782
Iteration 2/1000 | Loss: 0.00003433
Iteration 3/1000 | Loss: 0.00002239
Iteration 4/1000 | Loss: 0.00002019
Iteration 5/1000 | Loss: 0.00001913
Iteration 6/1000 | Loss: 0.00001841
Iteration 7/1000 | Loss: 0.00001782
Iteration 8/1000 | Loss: 0.00001751
Iteration 9/1000 | Loss: 0.00001726
Iteration 10/1000 | Loss: 0.00001704
Iteration 11/1000 | Loss: 0.00001693
Iteration 12/1000 | Loss: 0.00001687
Iteration 13/1000 | Loss: 0.00001685
Iteration 14/1000 | Loss: 0.00001681
Iteration 15/1000 | Loss: 0.00001681
Iteration 16/1000 | Loss: 0.00001680
Iteration 17/1000 | Loss: 0.00001671
Iteration 18/1000 | Loss: 0.00001671
Iteration 19/1000 | Loss: 0.00001671
Iteration 20/1000 | Loss: 0.00001668
Iteration 21/1000 | Loss: 0.00001666
Iteration 22/1000 | Loss: 0.00001664
Iteration 23/1000 | Loss: 0.00001663
Iteration 24/1000 | Loss: 0.00001660
Iteration 25/1000 | Loss: 0.00001656
Iteration 26/1000 | Loss: 0.00001656
Iteration 27/1000 | Loss: 0.00001654
Iteration 28/1000 | Loss: 0.00001653
Iteration 29/1000 | Loss: 0.00001652
Iteration 30/1000 | Loss: 0.00001652
Iteration 31/1000 | Loss: 0.00001651
Iteration 32/1000 | Loss: 0.00001651
Iteration 33/1000 | Loss: 0.00001651
Iteration 34/1000 | Loss: 0.00001651
Iteration 35/1000 | Loss: 0.00001650
Iteration 36/1000 | Loss: 0.00001650
Iteration 37/1000 | Loss: 0.00001649
Iteration 38/1000 | Loss: 0.00001648
Iteration 39/1000 | Loss: 0.00001648
Iteration 40/1000 | Loss: 0.00001648
Iteration 41/1000 | Loss: 0.00001647
Iteration 42/1000 | Loss: 0.00001647
Iteration 43/1000 | Loss: 0.00001646
Iteration 44/1000 | Loss: 0.00001645
Iteration 45/1000 | Loss: 0.00001645
Iteration 46/1000 | Loss: 0.00001644
Iteration 47/1000 | Loss: 0.00001644
Iteration 48/1000 | Loss: 0.00001644
Iteration 49/1000 | Loss: 0.00001644
Iteration 50/1000 | Loss: 0.00001644
Iteration 51/1000 | Loss: 0.00001644
Iteration 52/1000 | Loss: 0.00001644
Iteration 53/1000 | Loss: 0.00001644
Iteration 54/1000 | Loss: 0.00001643
Iteration 55/1000 | Loss: 0.00001643
Iteration 56/1000 | Loss: 0.00001643
Iteration 57/1000 | Loss: 0.00001642
Iteration 58/1000 | Loss: 0.00001642
Iteration 59/1000 | Loss: 0.00001642
Iteration 60/1000 | Loss: 0.00001641
Iteration 61/1000 | Loss: 0.00001641
Iteration 62/1000 | Loss: 0.00001641
Iteration 63/1000 | Loss: 0.00001640
Iteration 64/1000 | Loss: 0.00001639
Iteration 65/1000 | Loss: 0.00001639
Iteration 66/1000 | Loss: 0.00001639
Iteration 67/1000 | Loss: 0.00001639
Iteration 68/1000 | Loss: 0.00001639
Iteration 69/1000 | Loss: 0.00001639
Iteration 70/1000 | Loss: 0.00001639
Iteration 71/1000 | Loss: 0.00001639
Iteration 72/1000 | Loss: 0.00001639
Iteration 73/1000 | Loss: 0.00001638
Iteration 74/1000 | Loss: 0.00001638
Iteration 75/1000 | Loss: 0.00001638
Iteration 76/1000 | Loss: 0.00001638
Iteration 77/1000 | Loss: 0.00001638
Iteration 78/1000 | Loss: 0.00001638
Iteration 79/1000 | Loss: 0.00001638
Iteration 80/1000 | Loss: 0.00001637
Iteration 81/1000 | Loss: 0.00001636
Iteration 82/1000 | Loss: 0.00001635
Iteration 83/1000 | Loss: 0.00001635
Iteration 84/1000 | Loss: 0.00001634
Iteration 85/1000 | Loss: 0.00001634
Iteration 86/1000 | Loss: 0.00001633
Iteration 87/1000 | Loss: 0.00001633
Iteration 88/1000 | Loss: 0.00001633
Iteration 89/1000 | Loss: 0.00001632
Iteration 90/1000 | Loss: 0.00001632
Iteration 91/1000 | Loss: 0.00001631
Iteration 92/1000 | Loss: 0.00001631
Iteration 93/1000 | Loss: 0.00001631
Iteration 94/1000 | Loss: 0.00001630
Iteration 95/1000 | Loss: 0.00001630
Iteration 96/1000 | Loss: 0.00001630
Iteration 97/1000 | Loss: 0.00001629
Iteration 98/1000 | Loss: 0.00001629
Iteration 99/1000 | Loss: 0.00001629
Iteration 100/1000 | Loss: 0.00001628
Iteration 101/1000 | Loss: 0.00001628
Iteration 102/1000 | Loss: 0.00001628
Iteration 103/1000 | Loss: 0.00001627
Iteration 104/1000 | Loss: 0.00001627
Iteration 105/1000 | Loss: 0.00001627
Iteration 106/1000 | Loss: 0.00001626
Iteration 107/1000 | Loss: 0.00001626
Iteration 108/1000 | Loss: 0.00001626
Iteration 109/1000 | Loss: 0.00001626
Iteration 110/1000 | Loss: 0.00001626
Iteration 111/1000 | Loss: 0.00001626
Iteration 112/1000 | Loss: 0.00001626
Iteration 113/1000 | Loss: 0.00001626
Iteration 114/1000 | Loss: 0.00001626
Iteration 115/1000 | Loss: 0.00001625
Iteration 116/1000 | Loss: 0.00001625
Iteration 117/1000 | Loss: 0.00001625
Iteration 118/1000 | Loss: 0.00001625
Iteration 119/1000 | Loss: 0.00001625
Iteration 120/1000 | Loss: 0.00001624
Iteration 121/1000 | Loss: 0.00001624
Iteration 122/1000 | Loss: 0.00001624
Iteration 123/1000 | Loss: 0.00001624
Iteration 124/1000 | Loss: 0.00001624
Iteration 125/1000 | Loss: 0.00001623
Iteration 126/1000 | Loss: 0.00001623
Iteration 127/1000 | Loss: 0.00001623
Iteration 128/1000 | Loss: 0.00001623
Iteration 129/1000 | Loss: 0.00001623
Iteration 130/1000 | Loss: 0.00001623
Iteration 131/1000 | Loss: 0.00001623
Iteration 132/1000 | Loss: 0.00001623
Iteration 133/1000 | Loss: 0.00001622
Iteration 134/1000 | Loss: 0.00001622
Iteration 135/1000 | Loss: 0.00001622
Iteration 136/1000 | Loss: 0.00001622
Iteration 137/1000 | Loss: 0.00001622
Iteration 138/1000 | Loss: 0.00001621
Iteration 139/1000 | Loss: 0.00001621
Iteration 140/1000 | Loss: 0.00001621
Iteration 141/1000 | Loss: 0.00001621
Iteration 142/1000 | Loss: 0.00001621
Iteration 143/1000 | Loss: 0.00001621
Iteration 144/1000 | Loss: 0.00001621
Iteration 145/1000 | Loss: 0.00001621
Iteration 146/1000 | Loss: 0.00001620
Iteration 147/1000 | Loss: 0.00001620
Iteration 148/1000 | Loss: 0.00001620
Iteration 149/1000 | Loss: 0.00001620
Iteration 150/1000 | Loss: 0.00001620
Iteration 151/1000 | Loss: 0.00001620
Iteration 152/1000 | Loss: 0.00001620
Iteration 153/1000 | Loss: 0.00001620
Iteration 154/1000 | Loss: 0.00001620
Iteration 155/1000 | Loss: 0.00001620
Iteration 156/1000 | Loss: 0.00001620
Iteration 157/1000 | Loss: 0.00001620
Iteration 158/1000 | Loss: 0.00001620
Iteration 159/1000 | Loss: 0.00001620
Iteration 160/1000 | Loss: 0.00001619
Iteration 161/1000 | Loss: 0.00001619
Iteration 162/1000 | Loss: 0.00001619
Iteration 163/1000 | Loss: 0.00001619
Iteration 164/1000 | Loss: 0.00001619
Iteration 165/1000 | Loss: 0.00001619
Iteration 166/1000 | Loss: 0.00001619
Iteration 167/1000 | Loss: 0.00001619
Iteration 168/1000 | Loss: 0.00001619
Iteration 169/1000 | Loss: 0.00001619
Iteration 170/1000 | Loss: 0.00001619
Iteration 171/1000 | Loss: 0.00001619
Iteration 172/1000 | Loss: 0.00001619
Iteration 173/1000 | Loss: 0.00001619
Iteration 174/1000 | Loss: 0.00001619
Iteration 175/1000 | Loss: 0.00001619
Iteration 176/1000 | Loss: 0.00001619
Iteration 177/1000 | Loss: 0.00001619
Iteration 178/1000 | Loss: 0.00001619
Iteration 179/1000 | Loss: 0.00001619
Iteration 180/1000 | Loss: 0.00001619
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.618715214135591e-05, 1.618715214135591e-05, 1.618715214135591e-05, 1.618715214135591e-05, 1.618715214135591e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.618715214135591e-05

Optimization complete. Final v2v error: 3.3954074382781982 mm

Highest mean error: 3.992603063583374 mm for frame 144

Lowest mean error: 2.805307388305664 mm for frame 6

Saving results

Total time: 41.572813987731934
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01059152
Iteration 2/25 | Loss: 0.00303795
Iteration 3/25 | Loss: 0.00202624
Iteration 4/25 | Loss: 0.00203695
Iteration 5/25 | Loss: 0.00157865
Iteration 6/25 | Loss: 0.00154559
Iteration 7/25 | Loss: 0.00154372
Iteration 8/25 | Loss: 0.00154372
Iteration 9/25 | Loss: 0.00154372
Iteration 10/25 | Loss: 0.00154372
Iteration 11/25 | Loss: 0.00154372
Iteration 12/25 | Loss: 0.00154372
Iteration 13/25 | Loss: 0.00154372
Iteration 14/25 | Loss: 0.00154372
Iteration 15/25 | Loss: 0.00154372
Iteration 16/25 | Loss: 0.00154372
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0015437210677191615, 0.0015437210677191615, 0.0015437210677191615, 0.0015437210677191615, 0.0015437210677191615]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015437210677191615

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24904716
Iteration 2/25 | Loss: 0.00114142
Iteration 3/25 | Loss: 0.00114141
Iteration 4/25 | Loss: 0.00114141
Iteration 5/25 | Loss: 0.00114141
Iteration 6/25 | Loss: 0.00114141
Iteration 7/25 | Loss: 0.00114141
Iteration 8/25 | Loss: 0.00114141
Iteration 9/25 | Loss: 0.00114141
Iteration 10/25 | Loss: 0.00114141
Iteration 11/25 | Loss: 0.00114141
Iteration 12/25 | Loss: 0.00114141
Iteration 13/25 | Loss: 0.00114141
Iteration 14/25 | Loss: 0.00114141
Iteration 15/25 | Loss: 0.00114141
Iteration 16/25 | Loss: 0.00114141
Iteration 17/25 | Loss: 0.00114141
Iteration 18/25 | Loss: 0.00114141
Iteration 19/25 | Loss: 0.00114141
Iteration 20/25 | Loss: 0.00114141
Iteration 21/25 | Loss: 0.00114141
Iteration 22/25 | Loss: 0.00114141
Iteration 23/25 | Loss: 0.00114141
Iteration 24/25 | Loss: 0.00114141
Iteration 25/25 | Loss: 0.00114141

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114141
Iteration 2/1000 | Loss: 0.00005809
Iteration 3/1000 | Loss: 0.00004038
Iteration 4/1000 | Loss: 0.00003588
Iteration 5/1000 | Loss: 0.00003447
Iteration 6/1000 | Loss: 0.00003333
Iteration 7/1000 | Loss: 0.00003282
Iteration 8/1000 | Loss: 0.00003241
Iteration 9/1000 | Loss: 0.00003217
Iteration 10/1000 | Loss: 0.00003197
Iteration 11/1000 | Loss: 0.00003195
Iteration 12/1000 | Loss: 0.00003189
Iteration 13/1000 | Loss: 0.00003187
Iteration 14/1000 | Loss: 0.00003186
Iteration 15/1000 | Loss: 0.00003176
Iteration 16/1000 | Loss: 0.00003176
Iteration 17/1000 | Loss: 0.00003176
Iteration 18/1000 | Loss: 0.00003176
Iteration 19/1000 | Loss: 0.00003176
Iteration 20/1000 | Loss: 0.00003175
Iteration 21/1000 | Loss: 0.00003175
Iteration 22/1000 | Loss: 0.00003174
Iteration 23/1000 | Loss: 0.00003174
Iteration 24/1000 | Loss: 0.00003174
Iteration 25/1000 | Loss: 0.00003173
Iteration 26/1000 | Loss: 0.00003173
Iteration 27/1000 | Loss: 0.00003172
Iteration 28/1000 | Loss: 0.00003170
Iteration 29/1000 | Loss: 0.00003169
Iteration 30/1000 | Loss: 0.00003165
Iteration 31/1000 | Loss: 0.00003165
Iteration 32/1000 | Loss: 0.00003163
Iteration 33/1000 | Loss: 0.00003163
Iteration 34/1000 | Loss: 0.00003163
Iteration 35/1000 | Loss: 0.00003162
Iteration 36/1000 | Loss: 0.00003162
Iteration 37/1000 | Loss: 0.00003162
Iteration 38/1000 | Loss: 0.00003162
Iteration 39/1000 | Loss: 0.00003160
Iteration 40/1000 | Loss: 0.00003160
Iteration 41/1000 | Loss: 0.00003158
Iteration 42/1000 | Loss: 0.00003158
Iteration 43/1000 | Loss: 0.00003157
Iteration 44/1000 | Loss: 0.00003157
Iteration 45/1000 | Loss: 0.00003157
Iteration 46/1000 | Loss: 0.00003157
Iteration 47/1000 | Loss: 0.00003157
Iteration 48/1000 | Loss: 0.00003157
Iteration 49/1000 | Loss: 0.00003157
Iteration 50/1000 | Loss: 0.00003157
Iteration 51/1000 | Loss: 0.00003157
Iteration 52/1000 | Loss: 0.00003157
Iteration 53/1000 | Loss: 0.00003157
Iteration 54/1000 | Loss: 0.00003156
Iteration 55/1000 | Loss: 0.00003156
Iteration 56/1000 | Loss: 0.00003156
Iteration 57/1000 | Loss: 0.00003156
Iteration 58/1000 | Loss: 0.00003156
Iteration 59/1000 | Loss: 0.00003155
Iteration 60/1000 | Loss: 0.00003155
Iteration 61/1000 | Loss: 0.00003155
Iteration 62/1000 | Loss: 0.00003154
Iteration 63/1000 | Loss: 0.00003154
Iteration 64/1000 | Loss: 0.00003154
Iteration 65/1000 | Loss: 0.00003154
Iteration 66/1000 | Loss: 0.00003154
Iteration 67/1000 | Loss: 0.00003154
Iteration 68/1000 | Loss: 0.00003153
Iteration 69/1000 | Loss: 0.00003153
Iteration 70/1000 | Loss: 0.00003152
Iteration 71/1000 | Loss: 0.00003152
Iteration 72/1000 | Loss: 0.00003152
Iteration 73/1000 | Loss: 0.00003151
Iteration 74/1000 | Loss: 0.00003151
Iteration 75/1000 | Loss: 0.00003151
Iteration 76/1000 | Loss: 0.00003151
Iteration 77/1000 | Loss: 0.00003151
Iteration 78/1000 | Loss: 0.00003151
Iteration 79/1000 | Loss: 0.00003151
Iteration 80/1000 | Loss: 0.00003151
Iteration 81/1000 | Loss: 0.00003151
Iteration 82/1000 | Loss: 0.00003150
Iteration 83/1000 | Loss: 0.00003150
Iteration 84/1000 | Loss: 0.00003150
Iteration 85/1000 | Loss: 0.00003150
Iteration 86/1000 | Loss: 0.00003150
Iteration 87/1000 | Loss: 0.00003149
Iteration 88/1000 | Loss: 0.00003149
Iteration 89/1000 | Loss: 0.00003149
Iteration 90/1000 | Loss: 0.00003149
Iteration 91/1000 | Loss: 0.00003149
Iteration 92/1000 | Loss: 0.00003149
Iteration 93/1000 | Loss: 0.00003149
Iteration 94/1000 | Loss: 0.00003149
Iteration 95/1000 | Loss: 0.00003149
Iteration 96/1000 | Loss: 0.00003149
Iteration 97/1000 | Loss: 0.00003149
Iteration 98/1000 | Loss: 0.00003149
Iteration 99/1000 | Loss: 0.00003148
Iteration 100/1000 | Loss: 0.00003148
Iteration 101/1000 | Loss: 0.00003148
Iteration 102/1000 | Loss: 0.00003148
Iteration 103/1000 | Loss: 0.00003148
Iteration 104/1000 | Loss: 0.00003147
Iteration 105/1000 | Loss: 0.00003147
Iteration 106/1000 | Loss: 0.00003147
Iteration 107/1000 | Loss: 0.00003147
Iteration 108/1000 | Loss: 0.00003147
Iteration 109/1000 | Loss: 0.00003147
Iteration 110/1000 | Loss: 0.00003147
Iteration 111/1000 | Loss: 0.00003146
Iteration 112/1000 | Loss: 0.00003146
Iteration 113/1000 | Loss: 0.00003146
Iteration 114/1000 | Loss: 0.00003146
Iteration 115/1000 | Loss: 0.00003146
Iteration 116/1000 | Loss: 0.00003146
Iteration 117/1000 | Loss: 0.00003146
Iteration 118/1000 | Loss: 0.00003146
Iteration 119/1000 | Loss: 0.00003146
Iteration 120/1000 | Loss: 0.00003145
Iteration 121/1000 | Loss: 0.00003145
Iteration 122/1000 | Loss: 0.00003145
Iteration 123/1000 | Loss: 0.00003145
Iteration 124/1000 | Loss: 0.00003145
Iteration 125/1000 | Loss: 0.00003145
Iteration 126/1000 | Loss: 0.00003144
Iteration 127/1000 | Loss: 0.00003144
Iteration 128/1000 | Loss: 0.00003144
Iteration 129/1000 | Loss: 0.00003144
Iteration 130/1000 | Loss: 0.00003144
Iteration 131/1000 | Loss: 0.00003144
Iteration 132/1000 | Loss: 0.00003144
Iteration 133/1000 | Loss: 0.00003144
Iteration 134/1000 | Loss: 0.00003144
Iteration 135/1000 | Loss: 0.00003144
Iteration 136/1000 | Loss: 0.00003144
Iteration 137/1000 | Loss: 0.00003144
Iteration 138/1000 | Loss: 0.00003144
Iteration 139/1000 | Loss: 0.00003144
Iteration 140/1000 | Loss: 0.00003144
Iteration 141/1000 | Loss: 0.00003144
Iteration 142/1000 | Loss: 0.00003143
Iteration 143/1000 | Loss: 0.00003143
Iteration 144/1000 | Loss: 0.00003143
Iteration 145/1000 | Loss: 0.00003143
Iteration 146/1000 | Loss: 0.00003143
Iteration 147/1000 | Loss: 0.00003143
Iteration 148/1000 | Loss: 0.00003143
Iteration 149/1000 | Loss: 0.00003143
Iteration 150/1000 | Loss: 0.00003143
Iteration 151/1000 | Loss: 0.00003143
Iteration 152/1000 | Loss: 0.00003143
Iteration 153/1000 | Loss: 0.00003142
Iteration 154/1000 | Loss: 0.00003142
Iteration 155/1000 | Loss: 0.00003142
Iteration 156/1000 | Loss: 0.00003142
Iteration 157/1000 | Loss: 0.00003142
Iteration 158/1000 | Loss: 0.00003142
Iteration 159/1000 | Loss: 0.00003142
Iteration 160/1000 | Loss: 0.00003142
Iteration 161/1000 | Loss: 0.00003142
Iteration 162/1000 | Loss: 0.00003142
Iteration 163/1000 | Loss: 0.00003142
Iteration 164/1000 | Loss: 0.00003142
Iteration 165/1000 | Loss: 0.00003142
Iteration 166/1000 | Loss: 0.00003142
Iteration 167/1000 | Loss: 0.00003142
Iteration 168/1000 | Loss: 0.00003142
Iteration 169/1000 | Loss: 0.00003142
Iteration 170/1000 | Loss: 0.00003142
Iteration 171/1000 | Loss: 0.00003142
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [3.1416682759299874e-05, 3.1416682759299874e-05, 3.1416682759299874e-05, 3.1416682759299874e-05, 3.1416682759299874e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.1416682759299874e-05

Optimization complete. Final v2v error: 4.608622074127197 mm

Highest mean error: 4.909382343292236 mm for frame 127

Lowest mean error: 4.40023136138916 mm for frame 166

Saving results

Total time: 43.6711859703064
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00430761
Iteration 2/25 | Loss: 0.00144665
Iteration 3/25 | Loss: 0.00130751
Iteration 4/25 | Loss: 0.00129722
Iteration 5/25 | Loss: 0.00129573
Iteration 6/25 | Loss: 0.00129560
Iteration 7/25 | Loss: 0.00129560
Iteration 8/25 | Loss: 0.00129560
Iteration 9/25 | Loss: 0.00129560
Iteration 10/25 | Loss: 0.00129560
Iteration 11/25 | Loss: 0.00129560
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012956035789102316, 0.0012956035789102316, 0.0012956035789102316, 0.0012956035789102316, 0.0012956035789102316]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012956035789102316

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.11612701
Iteration 2/25 | Loss: 0.00077972
Iteration 3/25 | Loss: 0.00077970
Iteration 4/25 | Loss: 0.00077970
Iteration 5/25 | Loss: 0.00077970
Iteration 6/25 | Loss: 0.00077970
Iteration 7/25 | Loss: 0.00077970
Iteration 8/25 | Loss: 0.00077970
Iteration 9/25 | Loss: 0.00077970
Iteration 10/25 | Loss: 0.00077970
Iteration 11/25 | Loss: 0.00077970
Iteration 12/25 | Loss: 0.00077970
Iteration 13/25 | Loss: 0.00077970
Iteration 14/25 | Loss: 0.00077970
Iteration 15/25 | Loss: 0.00077970
Iteration 16/25 | Loss: 0.00077970
Iteration 17/25 | Loss: 0.00077970
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007796997670084238, 0.0007796997670084238, 0.0007796997670084238, 0.0007796997670084238, 0.0007796997670084238]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007796997670084238

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077970
Iteration 2/1000 | Loss: 0.00003265
Iteration 3/1000 | Loss: 0.00002438
Iteration 4/1000 | Loss: 0.00002116
Iteration 5/1000 | Loss: 0.00001985
Iteration 6/1000 | Loss: 0.00001884
Iteration 7/1000 | Loss: 0.00001820
Iteration 8/1000 | Loss: 0.00001772
Iteration 9/1000 | Loss: 0.00001738
Iteration 10/1000 | Loss: 0.00001707
Iteration 11/1000 | Loss: 0.00001685
Iteration 12/1000 | Loss: 0.00001680
Iteration 13/1000 | Loss: 0.00001677
Iteration 14/1000 | Loss: 0.00001671
Iteration 15/1000 | Loss: 0.00001657
Iteration 16/1000 | Loss: 0.00001651
Iteration 17/1000 | Loss: 0.00001647
Iteration 18/1000 | Loss: 0.00001647
Iteration 19/1000 | Loss: 0.00001646
Iteration 20/1000 | Loss: 0.00001642
Iteration 21/1000 | Loss: 0.00001641
Iteration 22/1000 | Loss: 0.00001639
Iteration 23/1000 | Loss: 0.00001639
Iteration 24/1000 | Loss: 0.00001637
Iteration 25/1000 | Loss: 0.00001634
Iteration 26/1000 | Loss: 0.00001634
Iteration 27/1000 | Loss: 0.00001633
Iteration 28/1000 | Loss: 0.00001632
Iteration 29/1000 | Loss: 0.00001632
Iteration 30/1000 | Loss: 0.00001631
Iteration 31/1000 | Loss: 0.00001630
Iteration 32/1000 | Loss: 0.00001627
Iteration 33/1000 | Loss: 0.00001624
Iteration 34/1000 | Loss: 0.00001624
Iteration 35/1000 | Loss: 0.00001623
Iteration 36/1000 | Loss: 0.00001623
Iteration 37/1000 | Loss: 0.00001619
Iteration 38/1000 | Loss: 0.00001617
Iteration 39/1000 | Loss: 0.00001615
Iteration 40/1000 | Loss: 0.00001614
Iteration 41/1000 | Loss: 0.00001614
Iteration 42/1000 | Loss: 0.00001614
Iteration 43/1000 | Loss: 0.00001614
Iteration 44/1000 | Loss: 0.00001614
Iteration 45/1000 | Loss: 0.00001614
Iteration 46/1000 | Loss: 0.00001614
Iteration 47/1000 | Loss: 0.00001614
Iteration 48/1000 | Loss: 0.00001614
Iteration 49/1000 | Loss: 0.00001614
Iteration 50/1000 | Loss: 0.00001614
Iteration 51/1000 | Loss: 0.00001612
Iteration 52/1000 | Loss: 0.00001612
Iteration 53/1000 | Loss: 0.00001612
Iteration 54/1000 | Loss: 0.00001611
Iteration 55/1000 | Loss: 0.00001611
Iteration 56/1000 | Loss: 0.00001611
Iteration 57/1000 | Loss: 0.00001610
Iteration 58/1000 | Loss: 0.00001610
Iteration 59/1000 | Loss: 0.00001610
Iteration 60/1000 | Loss: 0.00001610
Iteration 61/1000 | Loss: 0.00001610
Iteration 62/1000 | Loss: 0.00001610
Iteration 63/1000 | Loss: 0.00001609
Iteration 64/1000 | Loss: 0.00001608
Iteration 65/1000 | Loss: 0.00001608
Iteration 66/1000 | Loss: 0.00001608
Iteration 67/1000 | Loss: 0.00001608
Iteration 68/1000 | Loss: 0.00001608
Iteration 69/1000 | Loss: 0.00001608
Iteration 70/1000 | Loss: 0.00001607
Iteration 71/1000 | Loss: 0.00001607
Iteration 72/1000 | Loss: 0.00001607
Iteration 73/1000 | Loss: 0.00001607
Iteration 74/1000 | Loss: 0.00001607
Iteration 75/1000 | Loss: 0.00001607
Iteration 76/1000 | Loss: 0.00001605
Iteration 77/1000 | Loss: 0.00001605
Iteration 78/1000 | Loss: 0.00001605
Iteration 79/1000 | Loss: 0.00001605
Iteration 80/1000 | Loss: 0.00001605
Iteration 81/1000 | Loss: 0.00001605
Iteration 82/1000 | Loss: 0.00001604
Iteration 83/1000 | Loss: 0.00001604
Iteration 84/1000 | Loss: 0.00001604
Iteration 85/1000 | Loss: 0.00001604
Iteration 86/1000 | Loss: 0.00001604
Iteration 87/1000 | Loss: 0.00001603
Iteration 88/1000 | Loss: 0.00001603
Iteration 89/1000 | Loss: 0.00001603
Iteration 90/1000 | Loss: 0.00001603
Iteration 91/1000 | Loss: 0.00001602
Iteration 92/1000 | Loss: 0.00001602
Iteration 93/1000 | Loss: 0.00001602
Iteration 94/1000 | Loss: 0.00001602
Iteration 95/1000 | Loss: 0.00001602
Iteration 96/1000 | Loss: 0.00001602
Iteration 97/1000 | Loss: 0.00001601
Iteration 98/1000 | Loss: 0.00001601
Iteration 99/1000 | Loss: 0.00001601
Iteration 100/1000 | Loss: 0.00001601
Iteration 101/1000 | Loss: 0.00001601
Iteration 102/1000 | Loss: 0.00001601
Iteration 103/1000 | Loss: 0.00001601
Iteration 104/1000 | Loss: 0.00001601
Iteration 105/1000 | Loss: 0.00001601
Iteration 106/1000 | Loss: 0.00001600
Iteration 107/1000 | Loss: 0.00001600
Iteration 108/1000 | Loss: 0.00001600
Iteration 109/1000 | Loss: 0.00001600
Iteration 110/1000 | Loss: 0.00001600
Iteration 111/1000 | Loss: 0.00001600
Iteration 112/1000 | Loss: 0.00001600
Iteration 113/1000 | Loss: 0.00001599
Iteration 114/1000 | Loss: 0.00001599
Iteration 115/1000 | Loss: 0.00001599
Iteration 116/1000 | Loss: 0.00001599
Iteration 117/1000 | Loss: 0.00001599
Iteration 118/1000 | Loss: 0.00001599
Iteration 119/1000 | Loss: 0.00001599
Iteration 120/1000 | Loss: 0.00001599
Iteration 121/1000 | Loss: 0.00001599
Iteration 122/1000 | Loss: 0.00001599
Iteration 123/1000 | Loss: 0.00001598
Iteration 124/1000 | Loss: 0.00001598
Iteration 125/1000 | Loss: 0.00001598
Iteration 126/1000 | Loss: 0.00001598
Iteration 127/1000 | Loss: 0.00001598
Iteration 128/1000 | Loss: 0.00001598
Iteration 129/1000 | Loss: 0.00001598
Iteration 130/1000 | Loss: 0.00001598
Iteration 131/1000 | Loss: 0.00001598
Iteration 132/1000 | Loss: 0.00001598
Iteration 133/1000 | Loss: 0.00001598
Iteration 134/1000 | Loss: 0.00001598
Iteration 135/1000 | Loss: 0.00001598
Iteration 136/1000 | Loss: 0.00001598
Iteration 137/1000 | Loss: 0.00001598
Iteration 138/1000 | Loss: 0.00001598
Iteration 139/1000 | Loss: 0.00001598
Iteration 140/1000 | Loss: 0.00001598
Iteration 141/1000 | Loss: 0.00001598
Iteration 142/1000 | Loss: 0.00001598
Iteration 143/1000 | Loss: 0.00001598
Iteration 144/1000 | Loss: 0.00001598
Iteration 145/1000 | Loss: 0.00001598
Iteration 146/1000 | Loss: 0.00001598
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.597502159711439e-05, 1.597502159711439e-05, 1.597502159711439e-05, 1.597502159711439e-05, 1.597502159711439e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.597502159711439e-05

Optimization complete. Final v2v error: 3.385742425918579 mm

Highest mean error: 3.971281051635742 mm for frame 103

Lowest mean error: 3.0640201568603516 mm for frame 0

Saving results

Total time: 38.257354974746704
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00837487
Iteration 2/25 | Loss: 0.00145037
Iteration 3/25 | Loss: 0.00132928
Iteration 4/25 | Loss: 0.00131886
Iteration 5/25 | Loss: 0.00131532
Iteration 6/25 | Loss: 0.00131516
Iteration 7/25 | Loss: 0.00131516
Iteration 8/25 | Loss: 0.00131516
Iteration 9/25 | Loss: 0.00131516
Iteration 10/25 | Loss: 0.00131516
Iteration 11/25 | Loss: 0.00131516
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013151621678844094, 0.0013151621678844094, 0.0013151621678844094, 0.0013151621678844094, 0.0013151621678844094]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013151621678844094

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.95006013
Iteration 2/25 | Loss: 0.00058671
Iteration 3/25 | Loss: 0.00058670
Iteration 4/25 | Loss: 0.00058670
Iteration 5/25 | Loss: 0.00058670
Iteration 6/25 | Loss: 0.00058670
Iteration 7/25 | Loss: 0.00058670
Iteration 8/25 | Loss: 0.00058670
Iteration 9/25 | Loss: 0.00058670
Iteration 10/25 | Loss: 0.00058670
Iteration 11/25 | Loss: 0.00058670
Iteration 12/25 | Loss: 0.00058670
Iteration 13/25 | Loss: 0.00058670
Iteration 14/25 | Loss: 0.00058670
Iteration 15/25 | Loss: 0.00058670
Iteration 16/25 | Loss: 0.00058670
Iteration 17/25 | Loss: 0.00058670
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005866987048648298, 0.0005866987048648298, 0.0005866987048648298, 0.0005866987048648298, 0.0005866987048648298]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005866987048648298

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058670
Iteration 2/1000 | Loss: 0.00004241
Iteration 3/1000 | Loss: 0.00003430
Iteration 4/1000 | Loss: 0.00003147
Iteration 5/1000 | Loss: 0.00003028
Iteration 6/1000 | Loss: 0.00002942
Iteration 7/1000 | Loss: 0.00002877
Iteration 8/1000 | Loss: 0.00002859
Iteration 9/1000 | Loss: 0.00002826
Iteration 10/1000 | Loss: 0.00002813
Iteration 11/1000 | Loss: 0.00002796
Iteration 12/1000 | Loss: 0.00002793
Iteration 13/1000 | Loss: 0.00002788
Iteration 14/1000 | Loss: 0.00002782
Iteration 15/1000 | Loss: 0.00002782
Iteration 16/1000 | Loss: 0.00002781
Iteration 17/1000 | Loss: 0.00002781
Iteration 18/1000 | Loss: 0.00002779
Iteration 19/1000 | Loss: 0.00002775
Iteration 20/1000 | Loss: 0.00002774
Iteration 21/1000 | Loss: 0.00002774
Iteration 22/1000 | Loss: 0.00002774
Iteration 23/1000 | Loss: 0.00002774
Iteration 24/1000 | Loss: 0.00002774
Iteration 25/1000 | Loss: 0.00002774
Iteration 26/1000 | Loss: 0.00002773
Iteration 27/1000 | Loss: 0.00002773
Iteration 28/1000 | Loss: 0.00002773
Iteration 29/1000 | Loss: 0.00002773
Iteration 30/1000 | Loss: 0.00002772
Iteration 31/1000 | Loss: 0.00002772
Iteration 32/1000 | Loss: 0.00002772
Iteration 33/1000 | Loss: 0.00002772
Iteration 34/1000 | Loss: 0.00002772
Iteration 35/1000 | Loss: 0.00002772
Iteration 36/1000 | Loss: 0.00002768
Iteration 37/1000 | Loss: 0.00002768
Iteration 38/1000 | Loss: 0.00002768
Iteration 39/1000 | Loss: 0.00002768
Iteration 40/1000 | Loss: 0.00002767
Iteration 41/1000 | Loss: 0.00002767
Iteration 42/1000 | Loss: 0.00002767
Iteration 43/1000 | Loss: 0.00002767
Iteration 44/1000 | Loss: 0.00002767
Iteration 45/1000 | Loss: 0.00002767
Iteration 46/1000 | Loss: 0.00002767
Iteration 47/1000 | Loss: 0.00002767
Iteration 48/1000 | Loss: 0.00002767
Iteration 49/1000 | Loss: 0.00002767
Iteration 50/1000 | Loss: 0.00002766
Iteration 51/1000 | Loss: 0.00002766
Iteration 52/1000 | Loss: 0.00002766
Iteration 53/1000 | Loss: 0.00002766
Iteration 54/1000 | Loss: 0.00002766
Iteration 55/1000 | Loss: 0.00002766
Iteration 56/1000 | Loss: 0.00002766
Iteration 57/1000 | Loss: 0.00002766
Iteration 58/1000 | Loss: 0.00002766
Iteration 59/1000 | Loss: 0.00002766
Iteration 60/1000 | Loss: 0.00002766
Iteration 61/1000 | Loss: 0.00002766
Iteration 62/1000 | Loss: 0.00002765
Iteration 63/1000 | Loss: 0.00002765
Iteration 64/1000 | Loss: 0.00002765
Iteration 65/1000 | Loss: 0.00002765
Iteration 66/1000 | Loss: 0.00002765
Iteration 67/1000 | Loss: 0.00002765
Iteration 68/1000 | Loss: 0.00002765
Iteration 69/1000 | Loss: 0.00002765
Iteration 70/1000 | Loss: 0.00002765
Iteration 71/1000 | Loss: 0.00002765
Iteration 72/1000 | Loss: 0.00002765
Iteration 73/1000 | Loss: 0.00002765
Iteration 74/1000 | Loss: 0.00002765
Iteration 75/1000 | Loss: 0.00002765
Iteration 76/1000 | Loss: 0.00002764
Iteration 77/1000 | Loss: 0.00002764
Iteration 78/1000 | Loss: 0.00002764
Iteration 79/1000 | Loss: 0.00002764
Iteration 80/1000 | Loss: 0.00002764
Iteration 81/1000 | Loss: 0.00002764
Iteration 82/1000 | Loss: 0.00002764
Iteration 83/1000 | Loss: 0.00002764
Iteration 84/1000 | Loss: 0.00002764
Iteration 85/1000 | Loss: 0.00002764
Iteration 86/1000 | Loss: 0.00002764
Iteration 87/1000 | Loss: 0.00002764
Iteration 88/1000 | Loss: 0.00002764
Iteration 89/1000 | Loss: 0.00002764
Iteration 90/1000 | Loss: 0.00002763
Iteration 91/1000 | Loss: 0.00002763
Iteration 92/1000 | Loss: 0.00002763
Iteration 93/1000 | Loss: 0.00002763
Iteration 94/1000 | Loss: 0.00002763
Iteration 95/1000 | Loss: 0.00002763
Iteration 96/1000 | Loss: 0.00002763
Iteration 97/1000 | Loss: 0.00002762
Iteration 98/1000 | Loss: 0.00002762
Iteration 99/1000 | Loss: 0.00002762
Iteration 100/1000 | Loss: 0.00002762
Iteration 101/1000 | Loss: 0.00002762
Iteration 102/1000 | Loss: 0.00002762
Iteration 103/1000 | Loss: 0.00002762
Iteration 104/1000 | Loss: 0.00002762
Iteration 105/1000 | Loss: 0.00002761
Iteration 106/1000 | Loss: 0.00002761
Iteration 107/1000 | Loss: 0.00002761
Iteration 108/1000 | Loss: 0.00002761
Iteration 109/1000 | Loss: 0.00002761
Iteration 110/1000 | Loss: 0.00002761
Iteration 111/1000 | Loss: 0.00002761
Iteration 112/1000 | Loss: 0.00002761
Iteration 113/1000 | Loss: 0.00002761
Iteration 114/1000 | Loss: 0.00002761
Iteration 115/1000 | Loss: 0.00002761
Iteration 116/1000 | Loss: 0.00002761
Iteration 117/1000 | Loss: 0.00002760
Iteration 118/1000 | Loss: 0.00002760
Iteration 119/1000 | Loss: 0.00002760
Iteration 120/1000 | Loss: 0.00002760
Iteration 121/1000 | Loss: 0.00002760
Iteration 122/1000 | Loss: 0.00002760
Iteration 123/1000 | Loss: 0.00002760
Iteration 124/1000 | Loss: 0.00002760
Iteration 125/1000 | Loss: 0.00002760
Iteration 126/1000 | Loss: 0.00002760
Iteration 127/1000 | Loss: 0.00002760
Iteration 128/1000 | Loss: 0.00002760
Iteration 129/1000 | Loss: 0.00002760
Iteration 130/1000 | Loss: 0.00002760
Iteration 131/1000 | Loss: 0.00002760
Iteration 132/1000 | Loss: 0.00002760
Iteration 133/1000 | Loss: 0.00002760
Iteration 134/1000 | Loss: 0.00002760
Iteration 135/1000 | Loss: 0.00002760
Iteration 136/1000 | Loss: 0.00002760
Iteration 137/1000 | Loss: 0.00002760
Iteration 138/1000 | Loss: 0.00002760
Iteration 139/1000 | Loss: 0.00002760
Iteration 140/1000 | Loss: 0.00002760
Iteration 141/1000 | Loss: 0.00002760
Iteration 142/1000 | Loss: 0.00002760
Iteration 143/1000 | Loss: 0.00002760
Iteration 144/1000 | Loss: 0.00002760
Iteration 145/1000 | Loss: 0.00002760
Iteration 146/1000 | Loss: 0.00002760
Iteration 147/1000 | Loss: 0.00002760
Iteration 148/1000 | Loss: 0.00002760
Iteration 149/1000 | Loss: 0.00002760
Iteration 150/1000 | Loss: 0.00002760
Iteration 151/1000 | Loss: 0.00002760
Iteration 152/1000 | Loss: 0.00002760
Iteration 153/1000 | Loss: 0.00002760
Iteration 154/1000 | Loss: 0.00002760
Iteration 155/1000 | Loss: 0.00002760
Iteration 156/1000 | Loss: 0.00002760
Iteration 157/1000 | Loss: 0.00002760
Iteration 158/1000 | Loss: 0.00002760
Iteration 159/1000 | Loss: 0.00002760
Iteration 160/1000 | Loss: 0.00002760
Iteration 161/1000 | Loss: 0.00002760
Iteration 162/1000 | Loss: 0.00002760
Iteration 163/1000 | Loss: 0.00002760
Iteration 164/1000 | Loss: 0.00002760
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [2.7599866371019743e-05, 2.7599866371019743e-05, 2.7599866371019743e-05, 2.7599866371019743e-05, 2.7599866371019743e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7599866371019743e-05

Optimization complete. Final v2v error: 4.4108099937438965 mm

Highest mean error: 4.49945592880249 mm for frame 48

Lowest mean error: 4.317085266113281 mm for frame 111

Saving results

Total time: 33.730912923812866
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00615842
Iteration 2/25 | Loss: 0.00156413
Iteration 3/25 | Loss: 0.00141944
Iteration 4/25 | Loss: 0.00141165
Iteration 5/25 | Loss: 0.00141028
Iteration 6/25 | Loss: 0.00141028
Iteration 7/25 | Loss: 0.00141028
Iteration 8/25 | Loss: 0.00141028
Iteration 9/25 | Loss: 0.00141028
Iteration 10/25 | Loss: 0.00141028
Iteration 11/25 | Loss: 0.00141028
Iteration 12/25 | Loss: 0.00141028
Iteration 13/25 | Loss: 0.00141028
Iteration 14/25 | Loss: 0.00141028
Iteration 15/25 | Loss: 0.00141028
Iteration 16/25 | Loss: 0.00141028
Iteration 17/25 | Loss: 0.00141028
Iteration 18/25 | Loss: 0.00141028
Iteration 19/25 | Loss: 0.00141028
Iteration 20/25 | Loss: 0.00141028
Iteration 21/25 | Loss: 0.00141028
Iteration 22/25 | Loss: 0.00141028
Iteration 23/25 | Loss: 0.00141028
Iteration 24/25 | Loss: 0.00141028
Iteration 25/25 | Loss: 0.00141028

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.63834000
Iteration 2/25 | Loss: 0.00089941
Iteration 3/25 | Loss: 0.00089923
Iteration 4/25 | Loss: 0.00089923
Iteration 5/25 | Loss: 0.00089923
Iteration 6/25 | Loss: 0.00089923
Iteration 7/25 | Loss: 0.00089923
Iteration 8/25 | Loss: 0.00089923
Iteration 9/25 | Loss: 0.00089923
Iteration 10/25 | Loss: 0.00089923
Iteration 11/25 | Loss: 0.00089923
Iteration 12/25 | Loss: 0.00089923
Iteration 13/25 | Loss: 0.00089923
Iteration 14/25 | Loss: 0.00089923
Iteration 15/25 | Loss: 0.00089923
Iteration 16/25 | Loss: 0.00089923
Iteration 17/25 | Loss: 0.00089923
Iteration 18/25 | Loss: 0.00089923
Iteration 19/25 | Loss: 0.00089923
Iteration 20/25 | Loss: 0.00089923
Iteration 21/25 | Loss: 0.00089923
Iteration 22/25 | Loss: 0.00089923
Iteration 23/25 | Loss: 0.00089923
Iteration 24/25 | Loss: 0.00089923
Iteration 25/25 | Loss: 0.00089923

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089923
Iteration 2/1000 | Loss: 0.00003995
Iteration 3/1000 | Loss: 0.00002652
Iteration 4/1000 | Loss: 0.00002282
Iteration 5/1000 | Loss: 0.00002178
Iteration 6/1000 | Loss: 0.00002091
Iteration 7/1000 | Loss: 0.00002027
Iteration 8/1000 | Loss: 0.00001996
Iteration 9/1000 | Loss: 0.00001969
Iteration 10/1000 | Loss: 0.00001938
Iteration 11/1000 | Loss: 0.00001910
Iteration 12/1000 | Loss: 0.00001893
Iteration 13/1000 | Loss: 0.00001877
Iteration 14/1000 | Loss: 0.00001865
Iteration 15/1000 | Loss: 0.00001864
Iteration 16/1000 | Loss: 0.00001863
Iteration 17/1000 | Loss: 0.00001863
Iteration 18/1000 | Loss: 0.00001863
Iteration 19/1000 | Loss: 0.00001859
Iteration 20/1000 | Loss: 0.00001857
Iteration 21/1000 | Loss: 0.00001857
Iteration 22/1000 | Loss: 0.00001857
Iteration 23/1000 | Loss: 0.00001856
Iteration 24/1000 | Loss: 0.00001856
Iteration 25/1000 | Loss: 0.00001854
Iteration 26/1000 | Loss: 0.00001853
Iteration 27/1000 | Loss: 0.00001853
Iteration 28/1000 | Loss: 0.00001853
Iteration 29/1000 | Loss: 0.00001852
Iteration 30/1000 | Loss: 0.00001852
Iteration 31/1000 | Loss: 0.00001852
Iteration 32/1000 | Loss: 0.00001851
Iteration 33/1000 | Loss: 0.00001851
Iteration 34/1000 | Loss: 0.00001851
Iteration 35/1000 | Loss: 0.00001850
Iteration 36/1000 | Loss: 0.00001850
Iteration 37/1000 | Loss: 0.00001850
Iteration 38/1000 | Loss: 0.00001850
Iteration 39/1000 | Loss: 0.00001850
Iteration 40/1000 | Loss: 0.00001850
Iteration 41/1000 | Loss: 0.00001850
Iteration 42/1000 | Loss: 0.00001849
Iteration 43/1000 | Loss: 0.00001849
Iteration 44/1000 | Loss: 0.00001849
Iteration 45/1000 | Loss: 0.00001849
Iteration 46/1000 | Loss: 0.00001849
Iteration 47/1000 | Loss: 0.00001849
Iteration 48/1000 | Loss: 0.00001849
Iteration 49/1000 | Loss: 0.00001848
Iteration 50/1000 | Loss: 0.00001848
Iteration 51/1000 | Loss: 0.00001847
Iteration 52/1000 | Loss: 0.00001847
Iteration 53/1000 | Loss: 0.00001847
Iteration 54/1000 | Loss: 0.00001847
Iteration 55/1000 | Loss: 0.00001847
Iteration 56/1000 | Loss: 0.00001847
Iteration 57/1000 | Loss: 0.00001847
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 57. Stopping optimization.
Last 5 losses: [1.8472988813300617e-05, 1.8472988813300617e-05, 1.8472988813300617e-05, 1.8472988813300617e-05, 1.8472988813300617e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8472988813300617e-05

Optimization complete. Final v2v error: 3.5647289752960205 mm

Highest mean error: 3.990302324295044 mm for frame 38

Lowest mean error: 3.0861942768096924 mm for frame 208

Saving results

Total time: 35.06542778015137
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00856247
Iteration 2/25 | Loss: 0.00153136
Iteration 3/25 | Loss: 0.00137130
Iteration 4/25 | Loss: 0.00135716
Iteration 5/25 | Loss: 0.00135488
Iteration 6/25 | Loss: 0.00135488
Iteration 7/25 | Loss: 0.00135488
Iteration 8/25 | Loss: 0.00135488
Iteration 9/25 | Loss: 0.00135488
Iteration 10/25 | Loss: 0.00135488
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001354883541353047, 0.001354883541353047, 0.001354883541353047, 0.001354883541353047, 0.001354883541353047]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001354883541353047

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42649138
Iteration 2/25 | Loss: 0.00088572
Iteration 3/25 | Loss: 0.00088572
Iteration 4/25 | Loss: 0.00088572
Iteration 5/25 | Loss: 0.00088572
Iteration 6/25 | Loss: 0.00088572
Iteration 7/25 | Loss: 0.00088572
Iteration 8/25 | Loss: 0.00088572
Iteration 9/25 | Loss: 0.00088572
Iteration 10/25 | Loss: 0.00088572
Iteration 11/25 | Loss: 0.00088572
Iteration 12/25 | Loss: 0.00088572
Iteration 13/25 | Loss: 0.00088572
Iteration 14/25 | Loss: 0.00088572
Iteration 15/25 | Loss: 0.00088572
Iteration 16/25 | Loss: 0.00088572
Iteration 17/25 | Loss: 0.00088572
Iteration 18/25 | Loss: 0.00088572
Iteration 19/25 | Loss: 0.00088572
Iteration 20/25 | Loss: 0.00088572
Iteration 21/25 | Loss: 0.00088572
Iteration 22/25 | Loss: 0.00088572
Iteration 23/25 | Loss: 0.00088572
Iteration 24/25 | Loss: 0.00088572
Iteration 25/25 | Loss: 0.00088572

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088572
Iteration 2/1000 | Loss: 0.00003003
Iteration 3/1000 | Loss: 0.00002311
Iteration 4/1000 | Loss: 0.00002160
Iteration 5/1000 | Loss: 0.00002077
Iteration 6/1000 | Loss: 0.00002036
Iteration 7/1000 | Loss: 0.00001996
Iteration 8/1000 | Loss: 0.00001967
Iteration 9/1000 | Loss: 0.00001948
Iteration 10/1000 | Loss: 0.00001934
Iteration 11/1000 | Loss: 0.00001923
Iteration 12/1000 | Loss: 0.00001913
Iteration 13/1000 | Loss: 0.00001910
Iteration 14/1000 | Loss: 0.00001901
Iteration 15/1000 | Loss: 0.00001894
Iteration 16/1000 | Loss: 0.00001891
Iteration 17/1000 | Loss: 0.00001884
Iteration 18/1000 | Loss: 0.00001882
Iteration 19/1000 | Loss: 0.00001881
Iteration 20/1000 | Loss: 0.00001875
Iteration 21/1000 | Loss: 0.00001873
Iteration 22/1000 | Loss: 0.00001872
Iteration 23/1000 | Loss: 0.00001867
Iteration 24/1000 | Loss: 0.00001865
Iteration 25/1000 | Loss: 0.00001864
Iteration 26/1000 | Loss: 0.00001863
Iteration 27/1000 | Loss: 0.00001862
Iteration 28/1000 | Loss: 0.00001862
Iteration 29/1000 | Loss: 0.00001862
Iteration 30/1000 | Loss: 0.00001862
Iteration 31/1000 | Loss: 0.00001861
Iteration 32/1000 | Loss: 0.00001861
Iteration 33/1000 | Loss: 0.00001861
Iteration 34/1000 | Loss: 0.00001861
Iteration 35/1000 | Loss: 0.00001861
Iteration 36/1000 | Loss: 0.00001861
Iteration 37/1000 | Loss: 0.00001861
Iteration 38/1000 | Loss: 0.00001861
Iteration 39/1000 | Loss: 0.00001861
Iteration 40/1000 | Loss: 0.00001861
Iteration 41/1000 | Loss: 0.00001861
Iteration 42/1000 | Loss: 0.00001861
Iteration 43/1000 | Loss: 0.00001860
Iteration 44/1000 | Loss: 0.00001860
Iteration 45/1000 | Loss: 0.00001859
Iteration 46/1000 | Loss: 0.00001858
Iteration 47/1000 | Loss: 0.00001857
Iteration 48/1000 | Loss: 0.00001857
Iteration 49/1000 | Loss: 0.00001857
Iteration 50/1000 | Loss: 0.00001857
Iteration 51/1000 | Loss: 0.00001857
Iteration 52/1000 | Loss: 0.00001856
Iteration 53/1000 | Loss: 0.00001856
Iteration 54/1000 | Loss: 0.00001856
Iteration 55/1000 | Loss: 0.00001856
Iteration 56/1000 | Loss: 0.00001856
Iteration 57/1000 | Loss: 0.00001855
Iteration 58/1000 | Loss: 0.00001855
Iteration 59/1000 | Loss: 0.00001855
Iteration 60/1000 | Loss: 0.00001854
Iteration 61/1000 | Loss: 0.00001854
Iteration 62/1000 | Loss: 0.00001851
Iteration 63/1000 | Loss: 0.00001851
Iteration 64/1000 | Loss: 0.00001851
Iteration 65/1000 | Loss: 0.00001851
Iteration 66/1000 | Loss: 0.00001851
Iteration 67/1000 | Loss: 0.00001851
Iteration 68/1000 | Loss: 0.00001851
Iteration 69/1000 | Loss: 0.00001851
Iteration 70/1000 | Loss: 0.00001851
Iteration 71/1000 | Loss: 0.00001851
Iteration 72/1000 | Loss: 0.00001851
Iteration 73/1000 | Loss: 0.00001850
Iteration 74/1000 | Loss: 0.00001849
Iteration 75/1000 | Loss: 0.00001849
Iteration 76/1000 | Loss: 0.00001849
Iteration 77/1000 | Loss: 0.00001849
Iteration 78/1000 | Loss: 0.00001848
Iteration 79/1000 | Loss: 0.00001847
Iteration 80/1000 | Loss: 0.00001847
Iteration 81/1000 | Loss: 0.00001847
Iteration 82/1000 | Loss: 0.00001847
Iteration 83/1000 | Loss: 0.00001847
Iteration 84/1000 | Loss: 0.00001847
Iteration 85/1000 | Loss: 0.00001847
Iteration 86/1000 | Loss: 0.00001846
Iteration 87/1000 | Loss: 0.00001846
Iteration 88/1000 | Loss: 0.00001846
Iteration 89/1000 | Loss: 0.00001846
Iteration 90/1000 | Loss: 0.00001845
Iteration 91/1000 | Loss: 0.00001844
Iteration 92/1000 | Loss: 0.00001844
Iteration 93/1000 | Loss: 0.00001844
Iteration 94/1000 | Loss: 0.00001843
Iteration 95/1000 | Loss: 0.00001843
Iteration 96/1000 | Loss: 0.00001843
Iteration 97/1000 | Loss: 0.00001843
Iteration 98/1000 | Loss: 0.00001843
Iteration 99/1000 | Loss: 0.00001843
Iteration 100/1000 | Loss: 0.00001843
Iteration 101/1000 | Loss: 0.00001843
Iteration 102/1000 | Loss: 0.00001843
Iteration 103/1000 | Loss: 0.00001842
Iteration 104/1000 | Loss: 0.00001842
Iteration 105/1000 | Loss: 0.00001842
Iteration 106/1000 | Loss: 0.00001841
Iteration 107/1000 | Loss: 0.00001841
Iteration 108/1000 | Loss: 0.00001841
Iteration 109/1000 | Loss: 0.00001841
Iteration 110/1000 | Loss: 0.00001840
Iteration 111/1000 | Loss: 0.00001840
Iteration 112/1000 | Loss: 0.00001840
Iteration 113/1000 | Loss: 0.00001840
Iteration 114/1000 | Loss: 0.00001840
Iteration 115/1000 | Loss: 0.00001839
Iteration 116/1000 | Loss: 0.00001839
Iteration 117/1000 | Loss: 0.00001839
Iteration 118/1000 | Loss: 0.00001839
Iteration 119/1000 | Loss: 0.00001839
Iteration 120/1000 | Loss: 0.00001838
Iteration 121/1000 | Loss: 0.00001838
Iteration 122/1000 | Loss: 0.00001838
Iteration 123/1000 | Loss: 0.00001838
Iteration 124/1000 | Loss: 0.00001838
Iteration 125/1000 | Loss: 0.00001838
Iteration 126/1000 | Loss: 0.00001838
Iteration 127/1000 | Loss: 0.00001837
Iteration 128/1000 | Loss: 0.00001837
Iteration 129/1000 | Loss: 0.00001837
Iteration 130/1000 | Loss: 0.00001837
Iteration 131/1000 | Loss: 0.00001837
Iteration 132/1000 | Loss: 0.00001836
Iteration 133/1000 | Loss: 0.00001836
Iteration 134/1000 | Loss: 0.00001836
Iteration 135/1000 | Loss: 0.00001836
Iteration 136/1000 | Loss: 0.00001836
Iteration 137/1000 | Loss: 0.00001836
Iteration 138/1000 | Loss: 0.00001836
Iteration 139/1000 | Loss: 0.00001836
Iteration 140/1000 | Loss: 0.00001835
Iteration 141/1000 | Loss: 0.00001835
Iteration 142/1000 | Loss: 0.00001835
Iteration 143/1000 | Loss: 0.00001835
Iteration 144/1000 | Loss: 0.00001835
Iteration 145/1000 | Loss: 0.00001835
Iteration 146/1000 | Loss: 0.00001835
Iteration 147/1000 | Loss: 0.00001835
Iteration 148/1000 | Loss: 0.00001835
Iteration 149/1000 | Loss: 0.00001834
Iteration 150/1000 | Loss: 0.00001834
Iteration 151/1000 | Loss: 0.00001834
Iteration 152/1000 | Loss: 0.00001834
Iteration 153/1000 | Loss: 0.00001834
Iteration 154/1000 | Loss: 0.00001834
Iteration 155/1000 | Loss: 0.00001834
Iteration 156/1000 | Loss: 0.00001834
Iteration 157/1000 | Loss: 0.00001834
Iteration 158/1000 | Loss: 0.00001833
Iteration 159/1000 | Loss: 0.00001833
Iteration 160/1000 | Loss: 0.00001833
Iteration 161/1000 | Loss: 0.00001833
Iteration 162/1000 | Loss: 0.00001833
Iteration 163/1000 | Loss: 0.00001833
Iteration 164/1000 | Loss: 0.00001833
Iteration 165/1000 | Loss: 0.00001833
Iteration 166/1000 | Loss: 0.00001833
Iteration 167/1000 | Loss: 0.00001833
Iteration 168/1000 | Loss: 0.00001833
Iteration 169/1000 | Loss: 0.00001833
Iteration 170/1000 | Loss: 0.00001833
Iteration 171/1000 | Loss: 0.00001833
Iteration 172/1000 | Loss: 0.00001833
Iteration 173/1000 | Loss: 0.00001833
Iteration 174/1000 | Loss: 0.00001833
Iteration 175/1000 | Loss: 0.00001833
Iteration 176/1000 | Loss: 0.00001833
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.8326678400626406e-05, 1.8326678400626406e-05, 1.8326678400626406e-05, 1.8326678400626406e-05, 1.8326678400626406e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8326678400626406e-05

Optimization complete. Final v2v error: 3.6064727306365967 mm

Highest mean error: 4.035029411315918 mm for frame 151

Lowest mean error: 3.374844789505005 mm for frame 106

Saving results

Total time: 43.116679430007935
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01015914
Iteration 2/25 | Loss: 0.00243461
Iteration 3/25 | Loss: 0.00198721
Iteration 4/25 | Loss: 0.00171229
Iteration 5/25 | Loss: 0.00169827
Iteration 6/25 | Loss: 0.00165934
Iteration 7/25 | Loss: 0.00160165
Iteration 8/25 | Loss: 0.00149730
Iteration 9/25 | Loss: 0.00145427
Iteration 10/25 | Loss: 0.00143639
Iteration 11/25 | Loss: 0.00143278
Iteration 12/25 | Loss: 0.00143084
Iteration 13/25 | Loss: 0.00143761
Iteration 14/25 | Loss: 0.00142966
Iteration 15/25 | Loss: 0.00143125
Iteration 16/25 | Loss: 0.00143676
Iteration 17/25 | Loss: 0.00142684
Iteration 18/25 | Loss: 0.00142037
Iteration 19/25 | Loss: 0.00141986
Iteration 20/25 | Loss: 0.00141979
Iteration 21/25 | Loss: 0.00141979
Iteration 22/25 | Loss: 0.00141978
Iteration 23/25 | Loss: 0.00141978
Iteration 24/25 | Loss: 0.00141978
Iteration 25/25 | Loss: 0.00141978

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43957508
Iteration 2/25 | Loss: 0.00314605
Iteration 3/25 | Loss: 0.00175208
Iteration 4/25 | Loss: 0.00175010
Iteration 5/25 | Loss: 0.00175010
Iteration 6/25 | Loss: 0.00175010
Iteration 7/25 | Loss: 0.00175010
Iteration 8/25 | Loss: 0.00175010
Iteration 9/25 | Loss: 0.00175010
Iteration 10/25 | Loss: 0.00175010
Iteration 11/25 | Loss: 0.00175010
Iteration 12/25 | Loss: 0.00175010
Iteration 13/25 | Loss: 0.00175010
Iteration 14/25 | Loss: 0.00175010
Iteration 15/25 | Loss: 0.00175010
Iteration 16/25 | Loss: 0.00175010
Iteration 17/25 | Loss: 0.00175010
Iteration 18/25 | Loss: 0.00175010
Iteration 19/25 | Loss: 0.00175010
Iteration 20/25 | Loss: 0.00175010
Iteration 21/25 | Loss: 0.00175010
Iteration 22/25 | Loss: 0.00175010
Iteration 23/25 | Loss: 0.00175010
Iteration 24/25 | Loss: 0.00175010
Iteration 25/25 | Loss: 0.00175010

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00175010
Iteration 2/1000 | Loss: 0.00081461
Iteration 3/1000 | Loss: 0.00064434
Iteration 4/1000 | Loss: 0.00011671
Iteration 5/1000 | Loss: 0.00026028
Iteration 6/1000 | Loss: 0.00136507
Iteration 7/1000 | Loss: 0.00171792
Iteration 8/1000 | Loss: 0.00067686
Iteration 9/1000 | Loss: 0.00072172
Iteration 10/1000 | Loss: 0.00010594
Iteration 11/1000 | Loss: 0.00016549
Iteration 12/1000 | Loss: 0.00031605
Iteration 13/1000 | Loss: 0.00523363
Iteration 14/1000 | Loss: 0.00977207
Iteration 15/1000 | Loss: 0.01063703
Iteration 16/1000 | Loss: 0.00747261
Iteration 17/1000 | Loss: 0.00024680
Iteration 18/1000 | Loss: 0.00354352
Iteration 19/1000 | Loss: 0.00362430
Iteration 20/1000 | Loss: 0.00017714
Iteration 21/1000 | Loss: 0.00097527
Iteration 22/1000 | Loss: 0.00078628
Iteration 23/1000 | Loss: 0.00122960
Iteration 24/1000 | Loss: 0.00031008
Iteration 25/1000 | Loss: 0.00013045
Iteration 26/1000 | Loss: 0.00006553
Iteration 27/1000 | Loss: 0.00005972
Iteration 28/1000 | Loss: 0.00030117
Iteration 29/1000 | Loss: 0.00023817
Iteration 30/1000 | Loss: 0.00029546
Iteration 31/1000 | Loss: 0.00019381
Iteration 32/1000 | Loss: 0.00014281
Iteration 33/1000 | Loss: 0.00011365
Iteration 34/1000 | Loss: 0.00005669
Iteration 35/1000 | Loss: 0.00005335
Iteration 36/1000 | Loss: 0.00032510
Iteration 37/1000 | Loss: 0.00005053
Iteration 38/1000 | Loss: 0.00004935
Iteration 39/1000 | Loss: 0.00004837
Iteration 40/1000 | Loss: 0.00018515
Iteration 41/1000 | Loss: 0.00010337
Iteration 42/1000 | Loss: 0.00004786
Iteration 43/1000 | Loss: 0.00012241
Iteration 44/1000 | Loss: 0.00007408
Iteration 45/1000 | Loss: 0.00008515
Iteration 46/1000 | Loss: 0.00004701
Iteration 47/1000 | Loss: 0.00004668
Iteration 48/1000 | Loss: 0.00004629
Iteration 49/1000 | Loss: 0.00004589
Iteration 50/1000 | Loss: 0.00062943
Iteration 51/1000 | Loss: 0.00404406
Iteration 52/1000 | Loss: 0.00040088
Iteration 53/1000 | Loss: 0.00004480
Iteration 54/1000 | Loss: 0.00004491
Iteration 55/1000 | Loss: 0.00003449
Iteration 56/1000 | Loss: 0.00003038
Iteration 57/1000 | Loss: 0.00002614
Iteration 58/1000 | Loss: 0.00002311
Iteration 59/1000 | Loss: 0.00002125
Iteration 60/1000 | Loss: 0.00002023
Iteration 61/1000 | Loss: 0.00001960
Iteration 62/1000 | Loss: 0.00001919
Iteration 63/1000 | Loss: 0.00031217
Iteration 64/1000 | Loss: 0.00004324
Iteration 65/1000 | Loss: 0.00003028
Iteration 66/1000 | Loss: 0.00001872
Iteration 67/1000 | Loss: 0.00001848
Iteration 68/1000 | Loss: 0.00001832
Iteration 69/1000 | Loss: 0.00001831
Iteration 70/1000 | Loss: 0.00001830
Iteration 71/1000 | Loss: 0.00001823
Iteration 72/1000 | Loss: 0.00001817
Iteration 73/1000 | Loss: 0.00001813
Iteration 74/1000 | Loss: 0.00001810
Iteration 75/1000 | Loss: 0.00001809
Iteration 76/1000 | Loss: 0.00001809
Iteration 77/1000 | Loss: 0.00001808
Iteration 78/1000 | Loss: 0.00001808
Iteration 79/1000 | Loss: 0.00001807
Iteration 80/1000 | Loss: 0.00001807
Iteration 81/1000 | Loss: 0.00001806
Iteration 82/1000 | Loss: 0.00001806
Iteration 83/1000 | Loss: 0.00001805
Iteration 84/1000 | Loss: 0.00001805
Iteration 85/1000 | Loss: 0.00001803
Iteration 86/1000 | Loss: 0.00001794
Iteration 87/1000 | Loss: 0.00001791
Iteration 88/1000 | Loss: 0.00001790
Iteration 89/1000 | Loss: 0.00001789
Iteration 90/1000 | Loss: 0.00001789
Iteration 91/1000 | Loss: 0.00001788
Iteration 92/1000 | Loss: 0.00001788
Iteration 93/1000 | Loss: 0.00001787
Iteration 94/1000 | Loss: 0.00001787
Iteration 95/1000 | Loss: 0.00001786
Iteration 96/1000 | Loss: 0.00001786
Iteration 97/1000 | Loss: 0.00001786
Iteration 98/1000 | Loss: 0.00001786
Iteration 99/1000 | Loss: 0.00001785
Iteration 100/1000 | Loss: 0.00001785
Iteration 101/1000 | Loss: 0.00001785
Iteration 102/1000 | Loss: 0.00001785
Iteration 103/1000 | Loss: 0.00001785
Iteration 104/1000 | Loss: 0.00001784
Iteration 105/1000 | Loss: 0.00001784
Iteration 106/1000 | Loss: 0.00001784
Iteration 107/1000 | Loss: 0.00001784
Iteration 108/1000 | Loss: 0.00001783
Iteration 109/1000 | Loss: 0.00001783
Iteration 110/1000 | Loss: 0.00001783
Iteration 111/1000 | Loss: 0.00001783
Iteration 112/1000 | Loss: 0.00001783
Iteration 113/1000 | Loss: 0.00001783
Iteration 114/1000 | Loss: 0.00001783
Iteration 115/1000 | Loss: 0.00001783
Iteration 116/1000 | Loss: 0.00001782
Iteration 117/1000 | Loss: 0.00001782
Iteration 118/1000 | Loss: 0.00001782
Iteration 119/1000 | Loss: 0.00001782
Iteration 120/1000 | Loss: 0.00001782
Iteration 121/1000 | Loss: 0.00001782
Iteration 122/1000 | Loss: 0.00001781
Iteration 123/1000 | Loss: 0.00001781
Iteration 124/1000 | Loss: 0.00001781
Iteration 125/1000 | Loss: 0.00001781
Iteration 126/1000 | Loss: 0.00001781
Iteration 127/1000 | Loss: 0.00001781
Iteration 128/1000 | Loss: 0.00001780
Iteration 129/1000 | Loss: 0.00001780
Iteration 130/1000 | Loss: 0.00001780
Iteration 131/1000 | Loss: 0.00001780
Iteration 132/1000 | Loss: 0.00001780
Iteration 133/1000 | Loss: 0.00001780
Iteration 134/1000 | Loss: 0.00001780
Iteration 135/1000 | Loss: 0.00001780
Iteration 136/1000 | Loss: 0.00001780
Iteration 137/1000 | Loss: 0.00001780
Iteration 138/1000 | Loss: 0.00001780
Iteration 139/1000 | Loss: 0.00001780
Iteration 140/1000 | Loss: 0.00001780
Iteration 141/1000 | Loss: 0.00001780
Iteration 142/1000 | Loss: 0.00001780
Iteration 143/1000 | Loss: 0.00001780
Iteration 144/1000 | Loss: 0.00001780
Iteration 145/1000 | Loss: 0.00001780
Iteration 146/1000 | Loss: 0.00001780
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.7796863176045008e-05, 1.7796863176045008e-05, 1.7796863176045008e-05, 1.7796863176045008e-05, 1.7796863176045008e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7796863176045008e-05

Optimization complete. Final v2v error: 3.550915002822876 mm

Highest mean error: 4.632796287536621 mm for frame 61

Lowest mean error: 3.1831095218658447 mm for frame 2

Saving results

Total time: 137.43141412734985
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00958624
Iteration 2/25 | Loss: 0.00958624
Iteration 3/25 | Loss: 0.00958624
Iteration 4/25 | Loss: 0.00958624
Iteration 5/25 | Loss: 0.00958623
Iteration 6/25 | Loss: 0.00958623
Iteration 7/25 | Loss: 0.00958623
Iteration 8/25 | Loss: 0.00958623
Iteration 9/25 | Loss: 0.00958623
Iteration 10/25 | Loss: 0.00958623
Iteration 11/25 | Loss: 0.00958622
Iteration 12/25 | Loss: 0.00958622
Iteration 13/25 | Loss: 0.00958622
Iteration 14/25 | Loss: 0.00958622
Iteration 15/25 | Loss: 0.00958622
Iteration 16/25 | Loss: 0.00958622
Iteration 17/25 | Loss: 0.00958621
Iteration 18/25 | Loss: 0.00958621
Iteration 19/25 | Loss: 0.00958621
Iteration 20/25 | Loss: 0.00958621
Iteration 21/25 | Loss: 0.00958621
Iteration 22/25 | Loss: 0.00958620
Iteration 23/25 | Loss: 0.00958620
Iteration 24/25 | Loss: 0.00958620
Iteration 25/25 | Loss: 0.00958620

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50767052
Iteration 2/25 | Loss: 0.19354001
Iteration 3/25 | Loss: 0.19044967
Iteration 4/25 | Loss: 0.18956715
Iteration 5/25 | Loss: 0.18949114
Iteration 6/25 | Loss: 0.19054146
Iteration 7/25 | Loss: 0.19061264
Iteration 8/25 | Loss: 0.19013923
Iteration 9/25 | Loss: 0.18842332
Iteration 10/25 | Loss: 0.18850441
Iteration 11/25 | Loss: 0.18850170
Iteration 12/25 | Loss: 0.18848597
Iteration 13/25 | Loss: 0.18857768
Iteration 14/25 | Loss: 0.18857764
Iteration 15/25 | Loss: 0.18863283
Iteration 16/25 | Loss: 0.18854223
Iteration 17/25 | Loss: 0.18850322
Iteration 18/25 | Loss: 0.18841255
Iteration 19/25 | Loss: 0.18841252
Iteration 20/25 | Loss: 0.18841247
Iteration 21/25 | Loss: 0.18841247
Iteration 22/25 | Loss: 0.18841244
Iteration 23/25 | Loss: 0.18841246
Iteration 24/25 | Loss: 0.18841246
Iteration 25/25 | Loss: 0.18841246

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.18841244
Iteration 2/1000 | Loss: 0.01140181
Iteration 3/1000 | Loss: 0.00339464
Iteration 4/1000 | Loss: 0.00085251
Iteration 5/1000 | Loss: 0.00035398
Iteration 6/1000 | Loss: 0.00018492
Iteration 7/1000 | Loss: 0.00010555
Iteration 8/1000 | Loss: 0.00006733
Iteration 9/1000 | Loss: 0.00005109
Iteration 10/1000 | Loss: 0.00004468
Iteration 11/1000 | Loss: 0.00003763
Iteration 12/1000 | Loss: 0.00003284
Iteration 13/1000 | Loss: 0.00003047
Iteration 14/1000 | Loss: 0.00002832
Iteration 15/1000 | Loss: 0.00002696
Iteration 16/1000 | Loss: 0.00002596
Iteration 17/1000 | Loss: 0.00002522
Iteration 18/1000 | Loss: 0.00002484
Iteration 19/1000 | Loss: 0.00002448
Iteration 20/1000 | Loss: 0.00002427
Iteration 21/1000 | Loss: 0.00002418
Iteration 22/1000 | Loss: 0.00002406
Iteration 23/1000 | Loss: 0.00002399
Iteration 24/1000 | Loss: 0.00002391
Iteration 25/1000 | Loss: 0.00002389
Iteration 26/1000 | Loss: 0.00002389
Iteration 27/1000 | Loss: 0.00002389
Iteration 28/1000 | Loss: 0.00002389
Iteration 29/1000 | Loss: 0.00002387
Iteration 30/1000 | Loss: 0.00002386
Iteration 31/1000 | Loss: 0.00002382
Iteration 32/1000 | Loss: 0.00002382
Iteration 33/1000 | Loss: 0.00002382
Iteration 34/1000 | Loss: 0.00002381
Iteration 35/1000 | Loss: 0.00002381
Iteration 36/1000 | Loss: 0.00002381
Iteration 37/1000 | Loss: 0.00002381
Iteration 38/1000 | Loss: 0.00002381
Iteration 39/1000 | Loss: 0.00002381
Iteration 40/1000 | Loss: 0.00002381
Iteration 41/1000 | Loss: 0.00002378
Iteration 42/1000 | Loss: 0.00002377
Iteration 43/1000 | Loss: 0.00002375
Iteration 44/1000 | Loss: 0.00002375
Iteration 45/1000 | Loss: 0.00002375
Iteration 46/1000 | Loss: 0.00002374
Iteration 47/1000 | Loss: 0.00002373
Iteration 48/1000 | Loss: 0.00002372
Iteration 49/1000 | Loss: 0.00002372
Iteration 50/1000 | Loss: 0.00002372
Iteration 51/1000 | Loss: 0.00002371
Iteration 52/1000 | Loss: 0.00002371
Iteration 53/1000 | Loss: 0.00002371
Iteration 54/1000 | Loss: 0.00002371
Iteration 55/1000 | Loss: 0.00002370
Iteration 56/1000 | Loss: 0.00002370
Iteration 57/1000 | Loss: 0.00002370
Iteration 58/1000 | Loss: 0.00002366
Iteration 59/1000 | Loss: 0.00002364
Iteration 60/1000 | Loss: 0.00002364
Iteration 61/1000 | Loss: 0.00002364
Iteration 62/1000 | Loss: 0.00002364
Iteration 63/1000 | Loss: 0.00002364
Iteration 64/1000 | Loss: 0.00002364
Iteration 65/1000 | Loss: 0.00002364
Iteration 66/1000 | Loss: 0.00002363
Iteration 67/1000 | Loss: 0.00002363
Iteration 68/1000 | Loss: 0.00002363
Iteration 69/1000 | Loss: 0.00002363
Iteration 70/1000 | Loss: 0.00002363
Iteration 71/1000 | Loss: 0.00002363
Iteration 72/1000 | Loss: 0.00002363
Iteration 73/1000 | Loss: 0.00002363
Iteration 74/1000 | Loss: 0.00002362
Iteration 75/1000 | Loss: 0.00002362
Iteration 76/1000 | Loss: 0.00002362
Iteration 77/1000 | Loss: 0.00002362
Iteration 78/1000 | Loss: 0.00002362
Iteration 79/1000 | Loss: 0.00002362
Iteration 80/1000 | Loss: 0.00002362
Iteration 81/1000 | Loss: 0.00002362
Iteration 82/1000 | Loss: 0.00002362
Iteration 83/1000 | Loss: 0.00002362
Iteration 84/1000 | Loss: 0.00002362
Iteration 85/1000 | Loss: 0.00002362
Iteration 86/1000 | Loss: 0.00002362
Iteration 87/1000 | Loss: 0.00002361
Iteration 88/1000 | Loss: 0.00002359
Iteration 89/1000 | Loss: 0.00002359
Iteration 90/1000 | Loss: 0.00002358
Iteration 91/1000 | Loss: 0.00002358
Iteration 92/1000 | Loss: 0.00002358
Iteration 93/1000 | Loss: 0.00002357
Iteration 94/1000 | Loss: 0.00002357
Iteration 95/1000 | Loss: 0.00002357
Iteration 96/1000 | Loss: 0.00002357
Iteration 97/1000 | Loss: 0.00002356
Iteration 98/1000 | Loss: 0.00002356
Iteration 99/1000 | Loss: 0.00002356
Iteration 100/1000 | Loss: 0.00002356
Iteration 101/1000 | Loss: 0.00002356
Iteration 102/1000 | Loss: 0.00002356
Iteration 103/1000 | Loss: 0.00002356
Iteration 104/1000 | Loss: 0.00002356
Iteration 105/1000 | Loss: 0.00002356
Iteration 106/1000 | Loss: 0.00002356
Iteration 107/1000 | Loss: 0.00002355
Iteration 108/1000 | Loss: 0.00002355
Iteration 109/1000 | Loss: 0.00002354
Iteration 110/1000 | Loss: 0.00002354
Iteration 111/1000 | Loss: 0.00002354
Iteration 112/1000 | Loss: 0.00002354
Iteration 113/1000 | Loss: 0.00002354
Iteration 114/1000 | Loss: 0.00002354
Iteration 115/1000 | Loss: 0.00002354
Iteration 116/1000 | Loss: 0.00002354
Iteration 117/1000 | Loss: 0.00002354
Iteration 118/1000 | Loss: 0.00002354
Iteration 119/1000 | Loss: 0.00002354
Iteration 120/1000 | Loss: 0.00002353
Iteration 121/1000 | Loss: 0.00002353
Iteration 122/1000 | Loss: 0.00002353
Iteration 123/1000 | Loss: 0.00002353
Iteration 124/1000 | Loss: 0.00002353
Iteration 125/1000 | Loss: 0.00002353
Iteration 126/1000 | Loss: 0.00002353
Iteration 127/1000 | Loss: 0.00002353
Iteration 128/1000 | Loss: 0.00002353
Iteration 129/1000 | Loss: 0.00002353
Iteration 130/1000 | Loss: 0.00002353
Iteration 131/1000 | Loss: 0.00002353
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [2.3533784769824706e-05, 2.3533784769824706e-05, 2.3533784769824706e-05, 2.3533784769824706e-05, 2.3533784769824706e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3533784769824706e-05

Optimization complete. Final v2v error: 4.0756354331970215 mm

Highest mean error: 4.411950588226318 mm for frame 8

Lowest mean error: 3.8899221420288086 mm for frame 60

Saving results

Total time: 70.83716487884521
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00417401
Iteration 2/25 | Loss: 0.00146899
Iteration 3/25 | Loss: 0.00131094
Iteration 4/25 | Loss: 0.00128938
Iteration 5/25 | Loss: 0.00128677
Iteration 6/25 | Loss: 0.00128668
Iteration 7/25 | Loss: 0.00128668
Iteration 8/25 | Loss: 0.00128668
Iteration 9/25 | Loss: 0.00128668
Iteration 10/25 | Loss: 0.00128668
Iteration 11/25 | Loss: 0.00128668
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001286681042984128, 0.001286681042984128, 0.001286681042984128, 0.001286681042984128, 0.001286681042984128]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001286681042984128

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42619538
Iteration 2/25 | Loss: 0.00086764
Iteration 3/25 | Loss: 0.00086764
Iteration 4/25 | Loss: 0.00086764
Iteration 5/25 | Loss: 0.00086764
Iteration 6/25 | Loss: 0.00086764
Iteration 7/25 | Loss: 0.00086764
Iteration 8/25 | Loss: 0.00086764
Iteration 9/25 | Loss: 0.00086764
Iteration 10/25 | Loss: 0.00086764
Iteration 11/25 | Loss: 0.00086764
Iteration 12/25 | Loss: 0.00086764
Iteration 13/25 | Loss: 0.00086764
Iteration 14/25 | Loss: 0.00086764
Iteration 15/25 | Loss: 0.00086764
Iteration 16/25 | Loss: 0.00086764
Iteration 17/25 | Loss: 0.00086764
Iteration 18/25 | Loss: 0.00086764
Iteration 19/25 | Loss: 0.00086764
Iteration 20/25 | Loss: 0.00086764
Iteration 21/25 | Loss: 0.00086764
Iteration 22/25 | Loss: 0.00086764
Iteration 23/25 | Loss: 0.00086764
Iteration 24/25 | Loss: 0.00086764
Iteration 25/25 | Loss: 0.00086764

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086764
Iteration 2/1000 | Loss: 0.00003498
Iteration 3/1000 | Loss: 0.00002109
Iteration 4/1000 | Loss: 0.00001866
Iteration 5/1000 | Loss: 0.00001748
Iteration 6/1000 | Loss: 0.00001645
Iteration 7/1000 | Loss: 0.00001578
Iteration 8/1000 | Loss: 0.00001531
Iteration 9/1000 | Loss: 0.00001504
Iteration 10/1000 | Loss: 0.00001479
Iteration 11/1000 | Loss: 0.00001455
Iteration 12/1000 | Loss: 0.00001448
Iteration 13/1000 | Loss: 0.00001443
Iteration 14/1000 | Loss: 0.00001442
Iteration 15/1000 | Loss: 0.00001441
Iteration 16/1000 | Loss: 0.00001435
Iteration 17/1000 | Loss: 0.00001432
Iteration 18/1000 | Loss: 0.00001430
Iteration 19/1000 | Loss: 0.00001428
Iteration 20/1000 | Loss: 0.00001425
Iteration 21/1000 | Loss: 0.00001424
Iteration 22/1000 | Loss: 0.00001423
Iteration 23/1000 | Loss: 0.00001423
Iteration 24/1000 | Loss: 0.00001417
Iteration 25/1000 | Loss: 0.00001416
Iteration 26/1000 | Loss: 0.00001414
Iteration 27/1000 | Loss: 0.00001413
Iteration 28/1000 | Loss: 0.00001413
Iteration 29/1000 | Loss: 0.00001412
Iteration 30/1000 | Loss: 0.00001412
Iteration 31/1000 | Loss: 0.00001411
Iteration 32/1000 | Loss: 0.00001411
Iteration 33/1000 | Loss: 0.00001411
Iteration 34/1000 | Loss: 0.00001409
Iteration 35/1000 | Loss: 0.00001409
Iteration 36/1000 | Loss: 0.00001409
Iteration 37/1000 | Loss: 0.00001409
Iteration 38/1000 | Loss: 0.00001409
Iteration 39/1000 | Loss: 0.00001409
Iteration 40/1000 | Loss: 0.00001409
Iteration 41/1000 | Loss: 0.00001408
Iteration 42/1000 | Loss: 0.00001408
Iteration 43/1000 | Loss: 0.00001408
Iteration 44/1000 | Loss: 0.00001408
Iteration 45/1000 | Loss: 0.00001407
Iteration 46/1000 | Loss: 0.00001406
Iteration 47/1000 | Loss: 0.00001406
Iteration 48/1000 | Loss: 0.00001406
Iteration 49/1000 | Loss: 0.00001406
Iteration 50/1000 | Loss: 0.00001405
Iteration 51/1000 | Loss: 0.00001404
Iteration 52/1000 | Loss: 0.00001404
Iteration 53/1000 | Loss: 0.00001404
Iteration 54/1000 | Loss: 0.00001404
Iteration 55/1000 | Loss: 0.00001404
Iteration 56/1000 | Loss: 0.00001403
Iteration 57/1000 | Loss: 0.00001403
Iteration 58/1000 | Loss: 0.00001403
Iteration 59/1000 | Loss: 0.00001403
Iteration 60/1000 | Loss: 0.00001403
Iteration 61/1000 | Loss: 0.00001403
Iteration 62/1000 | Loss: 0.00001403
Iteration 63/1000 | Loss: 0.00001403
Iteration 64/1000 | Loss: 0.00001403
Iteration 65/1000 | Loss: 0.00001403
Iteration 66/1000 | Loss: 0.00001402
Iteration 67/1000 | Loss: 0.00001402
Iteration 68/1000 | Loss: 0.00001402
Iteration 69/1000 | Loss: 0.00001401
Iteration 70/1000 | Loss: 0.00001401
Iteration 71/1000 | Loss: 0.00001401
Iteration 72/1000 | Loss: 0.00001401
Iteration 73/1000 | Loss: 0.00001400
Iteration 74/1000 | Loss: 0.00001399
Iteration 75/1000 | Loss: 0.00001399
Iteration 76/1000 | Loss: 0.00001399
Iteration 77/1000 | Loss: 0.00001399
Iteration 78/1000 | Loss: 0.00001398
Iteration 79/1000 | Loss: 0.00001397
Iteration 80/1000 | Loss: 0.00001397
Iteration 81/1000 | Loss: 0.00001396
Iteration 82/1000 | Loss: 0.00001396
Iteration 83/1000 | Loss: 0.00001396
Iteration 84/1000 | Loss: 0.00001395
Iteration 85/1000 | Loss: 0.00001395
Iteration 86/1000 | Loss: 0.00001394
Iteration 87/1000 | Loss: 0.00001394
Iteration 88/1000 | Loss: 0.00001394
Iteration 89/1000 | Loss: 0.00001393
Iteration 90/1000 | Loss: 0.00001393
Iteration 91/1000 | Loss: 0.00001393
Iteration 92/1000 | Loss: 0.00001393
Iteration 93/1000 | Loss: 0.00001392
Iteration 94/1000 | Loss: 0.00001392
Iteration 95/1000 | Loss: 0.00001392
Iteration 96/1000 | Loss: 0.00001392
Iteration 97/1000 | Loss: 0.00001392
Iteration 98/1000 | Loss: 0.00001392
Iteration 99/1000 | Loss: 0.00001392
Iteration 100/1000 | Loss: 0.00001392
Iteration 101/1000 | Loss: 0.00001392
Iteration 102/1000 | Loss: 0.00001392
Iteration 103/1000 | Loss: 0.00001391
Iteration 104/1000 | Loss: 0.00001391
Iteration 105/1000 | Loss: 0.00001391
Iteration 106/1000 | Loss: 0.00001390
Iteration 107/1000 | Loss: 0.00001390
Iteration 108/1000 | Loss: 0.00001390
Iteration 109/1000 | Loss: 0.00001390
Iteration 110/1000 | Loss: 0.00001390
Iteration 111/1000 | Loss: 0.00001390
Iteration 112/1000 | Loss: 0.00001389
Iteration 113/1000 | Loss: 0.00001389
Iteration 114/1000 | Loss: 0.00001389
Iteration 115/1000 | Loss: 0.00001389
Iteration 116/1000 | Loss: 0.00001389
Iteration 117/1000 | Loss: 0.00001389
Iteration 118/1000 | Loss: 0.00001389
Iteration 119/1000 | Loss: 0.00001388
Iteration 120/1000 | Loss: 0.00001388
Iteration 121/1000 | Loss: 0.00001388
Iteration 122/1000 | Loss: 0.00001388
Iteration 123/1000 | Loss: 0.00001388
Iteration 124/1000 | Loss: 0.00001388
Iteration 125/1000 | Loss: 0.00001388
Iteration 126/1000 | Loss: 0.00001388
Iteration 127/1000 | Loss: 0.00001388
Iteration 128/1000 | Loss: 0.00001388
Iteration 129/1000 | Loss: 0.00001388
Iteration 130/1000 | Loss: 0.00001388
Iteration 131/1000 | Loss: 0.00001388
Iteration 132/1000 | Loss: 0.00001387
Iteration 133/1000 | Loss: 0.00001387
Iteration 134/1000 | Loss: 0.00001387
Iteration 135/1000 | Loss: 0.00001387
Iteration 136/1000 | Loss: 0.00001386
Iteration 137/1000 | Loss: 0.00001386
Iteration 138/1000 | Loss: 0.00001386
Iteration 139/1000 | Loss: 0.00001385
Iteration 140/1000 | Loss: 0.00001385
Iteration 141/1000 | Loss: 0.00001385
Iteration 142/1000 | Loss: 0.00001385
Iteration 143/1000 | Loss: 0.00001385
Iteration 144/1000 | Loss: 0.00001385
Iteration 145/1000 | Loss: 0.00001385
Iteration 146/1000 | Loss: 0.00001385
Iteration 147/1000 | Loss: 0.00001385
Iteration 148/1000 | Loss: 0.00001385
Iteration 149/1000 | Loss: 0.00001385
Iteration 150/1000 | Loss: 0.00001385
Iteration 151/1000 | Loss: 0.00001385
Iteration 152/1000 | Loss: 0.00001385
Iteration 153/1000 | Loss: 0.00001385
Iteration 154/1000 | Loss: 0.00001384
Iteration 155/1000 | Loss: 0.00001384
Iteration 156/1000 | Loss: 0.00001384
Iteration 157/1000 | Loss: 0.00001384
Iteration 158/1000 | Loss: 0.00001384
Iteration 159/1000 | Loss: 0.00001384
Iteration 160/1000 | Loss: 0.00001383
Iteration 161/1000 | Loss: 0.00001383
Iteration 162/1000 | Loss: 0.00001383
Iteration 163/1000 | Loss: 0.00001383
Iteration 164/1000 | Loss: 0.00001383
Iteration 165/1000 | Loss: 0.00001383
Iteration 166/1000 | Loss: 0.00001382
Iteration 167/1000 | Loss: 0.00001382
Iteration 168/1000 | Loss: 0.00001382
Iteration 169/1000 | Loss: 0.00001382
Iteration 170/1000 | Loss: 0.00001381
Iteration 171/1000 | Loss: 0.00001381
Iteration 172/1000 | Loss: 0.00001381
Iteration 173/1000 | Loss: 0.00001381
Iteration 174/1000 | Loss: 0.00001381
Iteration 175/1000 | Loss: 0.00001381
Iteration 176/1000 | Loss: 0.00001381
Iteration 177/1000 | Loss: 0.00001381
Iteration 178/1000 | Loss: 0.00001381
Iteration 179/1000 | Loss: 0.00001381
Iteration 180/1000 | Loss: 0.00001380
Iteration 181/1000 | Loss: 0.00001380
Iteration 182/1000 | Loss: 0.00001380
Iteration 183/1000 | Loss: 0.00001380
Iteration 184/1000 | Loss: 0.00001380
Iteration 185/1000 | Loss: 0.00001380
Iteration 186/1000 | Loss: 0.00001379
Iteration 187/1000 | Loss: 0.00001379
Iteration 188/1000 | Loss: 0.00001379
Iteration 189/1000 | Loss: 0.00001379
Iteration 190/1000 | Loss: 0.00001379
Iteration 191/1000 | Loss: 0.00001378
Iteration 192/1000 | Loss: 0.00001378
Iteration 193/1000 | Loss: 0.00001378
Iteration 194/1000 | Loss: 0.00001378
Iteration 195/1000 | Loss: 0.00001378
Iteration 196/1000 | Loss: 0.00001378
Iteration 197/1000 | Loss: 0.00001377
Iteration 198/1000 | Loss: 0.00001377
Iteration 199/1000 | Loss: 0.00001377
Iteration 200/1000 | Loss: 0.00001377
Iteration 201/1000 | Loss: 0.00001376
Iteration 202/1000 | Loss: 0.00001376
Iteration 203/1000 | Loss: 0.00001376
Iteration 204/1000 | Loss: 0.00001376
Iteration 205/1000 | Loss: 0.00001376
Iteration 206/1000 | Loss: 0.00001376
Iteration 207/1000 | Loss: 0.00001375
Iteration 208/1000 | Loss: 0.00001375
Iteration 209/1000 | Loss: 0.00001375
Iteration 210/1000 | Loss: 0.00001375
Iteration 211/1000 | Loss: 0.00001374
Iteration 212/1000 | Loss: 0.00001374
Iteration 213/1000 | Loss: 0.00001374
Iteration 214/1000 | Loss: 0.00001374
Iteration 215/1000 | Loss: 0.00001374
Iteration 216/1000 | Loss: 0.00001374
Iteration 217/1000 | Loss: 0.00001374
Iteration 218/1000 | Loss: 0.00001374
Iteration 219/1000 | Loss: 0.00001373
Iteration 220/1000 | Loss: 0.00001373
Iteration 221/1000 | Loss: 0.00001373
Iteration 222/1000 | Loss: 0.00001373
Iteration 223/1000 | Loss: 0.00001372
Iteration 224/1000 | Loss: 0.00001372
Iteration 225/1000 | Loss: 0.00001372
Iteration 226/1000 | Loss: 0.00001372
Iteration 227/1000 | Loss: 0.00001372
Iteration 228/1000 | Loss: 0.00001372
Iteration 229/1000 | Loss: 0.00001372
Iteration 230/1000 | Loss: 0.00001372
Iteration 231/1000 | Loss: 0.00001372
Iteration 232/1000 | Loss: 0.00001372
Iteration 233/1000 | Loss: 0.00001371
Iteration 234/1000 | Loss: 0.00001371
Iteration 235/1000 | Loss: 0.00001371
Iteration 236/1000 | Loss: 0.00001371
Iteration 237/1000 | Loss: 0.00001371
Iteration 238/1000 | Loss: 0.00001371
Iteration 239/1000 | Loss: 0.00001371
Iteration 240/1000 | Loss: 0.00001371
Iteration 241/1000 | Loss: 0.00001370
Iteration 242/1000 | Loss: 0.00001370
Iteration 243/1000 | Loss: 0.00001370
Iteration 244/1000 | Loss: 0.00001370
Iteration 245/1000 | Loss: 0.00001370
Iteration 246/1000 | Loss: 0.00001370
Iteration 247/1000 | Loss: 0.00001370
Iteration 248/1000 | Loss: 0.00001370
Iteration 249/1000 | Loss: 0.00001370
Iteration 250/1000 | Loss: 0.00001370
Iteration 251/1000 | Loss: 0.00001370
Iteration 252/1000 | Loss: 0.00001369
Iteration 253/1000 | Loss: 0.00001369
Iteration 254/1000 | Loss: 0.00001369
Iteration 255/1000 | Loss: 0.00001369
Iteration 256/1000 | Loss: 0.00001369
Iteration 257/1000 | Loss: 0.00001369
Iteration 258/1000 | Loss: 0.00001369
Iteration 259/1000 | Loss: 0.00001369
Iteration 260/1000 | Loss: 0.00001368
Iteration 261/1000 | Loss: 0.00001368
Iteration 262/1000 | Loss: 0.00001368
Iteration 263/1000 | Loss: 0.00001368
Iteration 264/1000 | Loss: 0.00001368
Iteration 265/1000 | Loss: 0.00001367
Iteration 266/1000 | Loss: 0.00001367
Iteration 267/1000 | Loss: 0.00001367
Iteration 268/1000 | Loss: 0.00001367
Iteration 269/1000 | Loss: 0.00001367
Iteration 270/1000 | Loss: 0.00001367
Iteration 271/1000 | Loss: 0.00001367
Iteration 272/1000 | Loss: 0.00001367
Iteration 273/1000 | Loss: 0.00001367
Iteration 274/1000 | Loss: 0.00001367
Iteration 275/1000 | Loss: 0.00001367
Iteration 276/1000 | Loss: 0.00001367
Iteration 277/1000 | Loss: 0.00001367
Iteration 278/1000 | Loss: 0.00001366
Iteration 279/1000 | Loss: 0.00001366
Iteration 280/1000 | Loss: 0.00001366
Iteration 281/1000 | Loss: 0.00001366
Iteration 282/1000 | Loss: 0.00001366
Iteration 283/1000 | Loss: 0.00001366
Iteration 284/1000 | Loss: 0.00001366
Iteration 285/1000 | Loss: 0.00001366
Iteration 286/1000 | Loss: 0.00001366
Iteration 287/1000 | Loss: 0.00001366
Iteration 288/1000 | Loss: 0.00001366
Iteration 289/1000 | Loss: 0.00001366
Iteration 290/1000 | Loss: 0.00001366
Iteration 291/1000 | Loss: 0.00001366
Iteration 292/1000 | Loss: 0.00001366
Iteration 293/1000 | Loss: 0.00001365
Iteration 294/1000 | Loss: 0.00001365
Iteration 295/1000 | Loss: 0.00001365
Iteration 296/1000 | Loss: 0.00001365
Iteration 297/1000 | Loss: 0.00001365
Iteration 298/1000 | Loss: 0.00001365
Iteration 299/1000 | Loss: 0.00001365
Iteration 300/1000 | Loss: 0.00001365
Iteration 301/1000 | Loss: 0.00001365
Iteration 302/1000 | Loss: 0.00001365
Iteration 303/1000 | Loss: 0.00001365
Iteration 304/1000 | Loss: 0.00001364
Iteration 305/1000 | Loss: 0.00001364
Iteration 306/1000 | Loss: 0.00001364
Iteration 307/1000 | Loss: 0.00001364
Iteration 308/1000 | Loss: 0.00001364
Iteration 309/1000 | Loss: 0.00001364
Iteration 310/1000 | Loss: 0.00001364
Iteration 311/1000 | Loss: 0.00001364
Iteration 312/1000 | Loss: 0.00001364
Iteration 313/1000 | Loss: 0.00001364
Iteration 314/1000 | Loss: 0.00001364
Iteration 315/1000 | Loss: 0.00001364
Iteration 316/1000 | Loss: 0.00001364
Iteration 317/1000 | Loss: 0.00001363
Iteration 318/1000 | Loss: 0.00001363
Iteration 319/1000 | Loss: 0.00001363
Iteration 320/1000 | Loss: 0.00001363
Iteration 321/1000 | Loss: 0.00001363
Iteration 322/1000 | Loss: 0.00001363
Iteration 323/1000 | Loss: 0.00001363
Iteration 324/1000 | Loss: 0.00001363
Iteration 325/1000 | Loss: 0.00001363
Iteration 326/1000 | Loss: 0.00001363
Iteration 327/1000 | Loss: 0.00001363
Iteration 328/1000 | Loss: 0.00001363
Iteration 329/1000 | Loss: 0.00001363
Iteration 330/1000 | Loss: 0.00001363
Iteration 331/1000 | Loss: 0.00001363
Iteration 332/1000 | Loss: 0.00001363
Iteration 333/1000 | Loss: 0.00001363
Iteration 334/1000 | Loss: 0.00001363
Iteration 335/1000 | Loss: 0.00001363
Iteration 336/1000 | Loss: 0.00001363
Iteration 337/1000 | Loss: 0.00001363
Iteration 338/1000 | Loss: 0.00001363
Iteration 339/1000 | Loss: 0.00001363
Iteration 340/1000 | Loss: 0.00001363
Iteration 341/1000 | Loss: 0.00001363
Iteration 342/1000 | Loss: 0.00001363
Iteration 343/1000 | Loss: 0.00001363
Iteration 344/1000 | Loss: 0.00001363
Iteration 345/1000 | Loss: 0.00001363
Iteration 346/1000 | Loss: 0.00001363
Iteration 347/1000 | Loss: 0.00001363
Iteration 348/1000 | Loss: 0.00001363
Iteration 349/1000 | Loss: 0.00001363
Iteration 350/1000 | Loss: 0.00001363
Iteration 351/1000 | Loss: 0.00001363
Iteration 352/1000 | Loss: 0.00001363
Iteration 353/1000 | Loss: 0.00001363
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 353. Stopping optimization.
Last 5 losses: [1.3632859918288887e-05, 1.3632859918288887e-05, 1.3632859918288887e-05, 1.3632859918288887e-05, 1.3632859918288887e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3632859918288887e-05

Optimization complete. Final v2v error: 3.1518383026123047 mm

Highest mean error: 3.6545286178588867 mm for frame 97

Lowest mean error: 2.911839246749878 mm for frame 53

Saving results

Total time: 52.13276696205139
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00837566
Iteration 2/25 | Loss: 0.00134923
Iteration 3/25 | Loss: 0.00126820
Iteration 4/25 | Loss: 0.00126029
Iteration 5/25 | Loss: 0.00125712
Iteration 6/25 | Loss: 0.00125668
Iteration 7/25 | Loss: 0.00125668
Iteration 8/25 | Loss: 0.00125668
Iteration 9/25 | Loss: 0.00125668
Iteration 10/25 | Loss: 0.00125668
Iteration 11/25 | Loss: 0.00125668
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012566783698275685, 0.0012566783698275685, 0.0012566783698275685, 0.0012566783698275685, 0.0012566783698275685]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012566783698275685

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.81770992
Iteration 2/25 | Loss: 0.00092372
Iteration 3/25 | Loss: 0.00092372
Iteration 4/25 | Loss: 0.00092372
Iteration 5/25 | Loss: 0.00092372
Iteration 6/25 | Loss: 0.00092372
Iteration 7/25 | Loss: 0.00092372
Iteration 8/25 | Loss: 0.00092372
Iteration 9/25 | Loss: 0.00092372
Iteration 10/25 | Loss: 0.00092372
Iteration 11/25 | Loss: 0.00092372
Iteration 12/25 | Loss: 0.00092372
Iteration 13/25 | Loss: 0.00092372
Iteration 14/25 | Loss: 0.00092372
Iteration 15/25 | Loss: 0.00092372
Iteration 16/25 | Loss: 0.00092372
Iteration 17/25 | Loss: 0.00092372
Iteration 18/25 | Loss: 0.00092372
Iteration 19/25 | Loss: 0.00092372
Iteration 20/25 | Loss: 0.00092372
Iteration 21/25 | Loss: 0.00092372
Iteration 22/25 | Loss: 0.00092372
Iteration 23/25 | Loss: 0.00092372
Iteration 24/25 | Loss: 0.00092372
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0009237188496626914, 0.0009237188496626914, 0.0009237188496626914, 0.0009237188496626914, 0.0009237188496626914]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009237188496626914

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092372
Iteration 2/1000 | Loss: 0.00002293
Iteration 3/1000 | Loss: 0.00001673
Iteration 4/1000 | Loss: 0.00001481
Iteration 5/1000 | Loss: 0.00001415
Iteration 6/1000 | Loss: 0.00001360
Iteration 7/1000 | Loss: 0.00001305
Iteration 8/1000 | Loss: 0.00001275
Iteration 9/1000 | Loss: 0.00001254
Iteration 10/1000 | Loss: 0.00001235
Iteration 11/1000 | Loss: 0.00001215
Iteration 12/1000 | Loss: 0.00001213
Iteration 13/1000 | Loss: 0.00001213
Iteration 14/1000 | Loss: 0.00001209
Iteration 15/1000 | Loss: 0.00001208
Iteration 16/1000 | Loss: 0.00001207
Iteration 17/1000 | Loss: 0.00001207
Iteration 18/1000 | Loss: 0.00001206
Iteration 19/1000 | Loss: 0.00001205
Iteration 20/1000 | Loss: 0.00001204
Iteration 21/1000 | Loss: 0.00001204
Iteration 22/1000 | Loss: 0.00001203
Iteration 23/1000 | Loss: 0.00001200
Iteration 24/1000 | Loss: 0.00001199
Iteration 25/1000 | Loss: 0.00001197
Iteration 26/1000 | Loss: 0.00001192
Iteration 27/1000 | Loss: 0.00001189
Iteration 28/1000 | Loss: 0.00001188
Iteration 29/1000 | Loss: 0.00001188
Iteration 30/1000 | Loss: 0.00001187
Iteration 31/1000 | Loss: 0.00001185
Iteration 32/1000 | Loss: 0.00001185
Iteration 33/1000 | Loss: 0.00001184
Iteration 34/1000 | Loss: 0.00001183
Iteration 35/1000 | Loss: 0.00001183
Iteration 36/1000 | Loss: 0.00001182
Iteration 37/1000 | Loss: 0.00001182
Iteration 38/1000 | Loss: 0.00001181
Iteration 39/1000 | Loss: 0.00001181
Iteration 40/1000 | Loss: 0.00001181
Iteration 41/1000 | Loss: 0.00001181
Iteration 42/1000 | Loss: 0.00001181
Iteration 43/1000 | Loss: 0.00001181
Iteration 44/1000 | Loss: 0.00001180
Iteration 45/1000 | Loss: 0.00001180
Iteration 46/1000 | Loss: 0.00001179
Iteration 47/1000 | Loss: 0.00001179
Iteration 48/1000 | Loss: 0.00001179
Iteration 49/1000 | Loss: 0.00001179
Iteration 50/1000 | Loss: 0.00001179
Iteration 51/1000 | Loss: 0.00001179
Iteration 52/1000 | Loss: 0.00001179
Iteration 53/1000 | Loss: 0.00001178
Iteration 54/1000 | Loss: 0.00001178
Iteration 55/1000 | Loss: 0.00001178
Iteration 56/1000 | Loss: 0.00001177
Iteration 57/1000 | Loss: 0.00001177
Iteration 58/1000 | Loss: 0.00001176
Iteration 59/1000 | Loss: 0.00001176
Iteration 60/1000 | Loss: 0.00001176
Iteration 61/1000 | Loss: 0.00001175
Iteration 62/1000 | Loss: 0.00001175
Iteration 63/1000 | Loss: 0.00001175
Iteration 64/1000 | Loss: 0.00001174
Iteration 65/1000 | Loss: 0.00001174
Iteration 66/1000 | Loss: 0.00001174
Iteration 67/1000 | Loss: 0.00001173
Iteration 68/1000 | Loss: 0.00001173
Iteration 69/1000 | Loss: 0.00001172
Iteration 70/1000 | Loss: 0.00001172
Iteration 71/1000 | Loss: 0.00001172
Iteration 72/1000 | Loss: 0.00001172
Iteration 73/1000 | Loss: 0.00001171
Iteration 74/1000 | Loss: 0.00001171
Iteration 75/1000 | Loss: 0.00001171
Iteration 76/1000 | Loss: 0.00001171
Iteration 77/1000 | Loss: 0.00001171
Iteration 78/1000 | Loss: 0.00001171
Iteration 79/1000 | Loss: 0.00001171
Iteration 80/1000 | Loss: 0.00001171
Iteration 81/1000 | Loss: 0.00001170
Iteration 82/1000 | Loss: 0.00001170
Iteration 83/1000 | Loss: 0.00001170
Iteration 84/1000 | Loss: 0.00001170
Iteration 85/1000 | Loss: 0.00001170
Iteration 86/1000 | Loss: 0.00001170
Iteration 87/1000 | Loss: 0.00001169
Iteration 88/1000 | Loss: 0.00001169
Iteration 89/1000 | Loss: 0.00001169
Iteration 90/1000 | Loss: 0.00001169
Iteration 91/1000 | Loss: 0.00001169
Iteration 92/1000 | Loss: 0.00001169
Iteration 93/1000 | Loss: 0.00001169
Iteration 94/1000 | Loss: 0.00001169
Iteration 95/1000 | Loss: 0.00001168
Iteration 96/1000 | Loss: 0.00001168
Iteration 97/1000 | Loss: 0.00001168
Iteration 98/1000 | Loss: 0.00001168
Iteration 99/1000 | Loss: 0.00001168
Iteration 100/1000 | Loss: 0.00001168
Iteration 101/1000 | Loss: 0.00001167
Iteration 102/1000 | Loss: 0.00001167
Iteration 103/1000 | Loss: 0.00001167
Iteration 104/1000 | Loss: 0.00001167
Iteration 105/1000 | Loss: 0.00001167
Iteration 106/1000 | Loss: 0.00001167
Iteration 107/1000 | Loss: 0.00001167
Iteration 108/1000 | Loss: 0.00001167
Iteration 109/1000 | Loss: 0.00001166
Iteration 110/1000 | Loss: 0.00001166
Iteration 111/1000 | Loss: 0.00001166
Iteration 112/1000 | Loss: 0.00001166
Iteration 113/1000 | Loss: 0.00001166
Iteration 114/1000 | Loss: 0.00001166
Iteration 115/1000 | Loss: 0.00001165
Iteration 116/1000 | Loss: 0.00001165
Iteration 117/1000 | Loss: 0.00001164
Iteration 118/1000 | Loss: 0.00001164
Iteration 119/1000 | Loss: 0.00001163
Iteration 120/1000 | Loss: 0.00001163
Iteration 121/1000 | Loss: 0.00001163
Iteration 122/1000 | Loss: 0.00001162
Iteration 123/1000 | Loss: 0.00001162
Iteration 124/1000 | Loss: 0.00001162
Iteration 125/1000 | Loss: 0.00001162
Iteration 126/1000 | Loss: 0.00001162
Iteration 127/1000 | Loss: 0.00001161
Iteration 128/1000 | Loss: 0.00001161
Iteration 129/1000 | Loss: 0.00001161
Iteration 130/1000 | Loss: 0.00001161
Iteration 131/1000 | Loss: 0.00001160
Iteration 132/1000 | Loss: 0.00001160
Iteration 133/1000 | Loss: 0.00001160
Iteration 134/1000 | Loss: 0.00001159
Iteration 135/1000 | Loss: 0.00001159
Iteration 136/1000 | Loss: 0.00001159
Iteration 137/1000 | Loss: 0.00001159
Iteration 138/1000 | Loss: 0.00001159
Iteration 139/1000 | Loss: 0.00001158
Iteration 140/1000 | Loss: 0.00001158
Iteration 141/1000 | Loss: 0.00001158
Iteration 142/1000 | Loss: 0.00001158
Iteration 143/1000 | Loss: 0.00001158
Iteration 144/1000 | Loss: 0.00001158
Iteration 145/1000 | Loss: 0.00001158
Iteration 146/1000 | Loss: 0.00001157
Iteration 147/1000 | Loss: 0.00001157
Iteration 148/1000 | Loss: 0.00001157
Iteration 149/1000 | Loss: 0.00001157
Iteration 150/1000 | Loss: 0.00001157
Iteration 151/1000 | Loss: 0.00001157
Iteration 152/1000 | Loss: 0.00001157
Iteration 153/1000 | Loss: 0.00001157
Iteration 154/1000 | Loss: 0.00001157
Iteration 155/1000 | Loss: 0.00001157
Iteration 156/1000 | Loss: 0.00001157
Iteration 157/1000 | Loss: 0.00001157
Iteration 158/1000 | Loss: 0.00001157
Iteration 159/1000 | Loss: 0.00001157
Iteration 160/1000 | Loss: 0.00001157
Iteration 161/1000 | Loss: 0.00001157
Iteration 162/1000 | Loss: 0.00001157
Iteration 163/1000 | Loss: 0.00001157
Iteration 164/1000 | Loss: 0.00001157
Iteration 165/1000 | Loss: 0.00001157
Iteration 166/1000 | Loss: 0.00001157
Iteration 167/1000 | Loss: 0.00001156
Iteration 168/1000 | Loss: 0.00001156
Iteration 169/1000 | Loss: 0.00001156
Iteration 170/1000 | Loss: 0.00001156
Iteration 171/1000 | Loss: 0.00001156
Iteration 172/1000 | Loss: 0.00001156
Iteration 173/1000 | Loss: 0.00001156
Iteration 174/1000 | Loss: 0.00001156
Iteration 175/1000 | Loss: 0.00001156
Iteration 176/1000 | Loss: 0.00001156
Iteration 177/1000 | Loss: 0.00001155
Iteration 178/1000 | Loss: 0.00001155
Iteration 179/1000 | Loss: 0.00001155
Iteration 180/1000 | Loss: 0.00001155
Iteration 181/1000 | Loss: 0.00001155
Iteration 182/1000 | Loss: 0.00001155
Iteration 183/1000 | Loss: 0.00001155
Iteration 184/1000 | Loss: 0.00001155
Iteration 185/1000 | Loss: 0.00001155
Iteration 186/1000 | Loss: 0.00001155
Iteration 187/1000 | Loss: 0.00001155
Iteration 188/1000 | Loss: 0.00001155
Iteration 189/1000 | Loss: 0.00001155
Iteration 190/1000 | Loss: 0.00001155
Iteration 191/1000 | Loss: 0.00001155
Iteration 192/1000 | Loss: 0.00001155
Iteration 193/1000 | Loss: 0.00001155
Iteration 194/1000 | Loss: 0.00001155
Iteration 195/1000 | Loss: 0.00001155
Iteration 196/1000 | Loss: 0.00001155
Iteration 197/1000 | Loss: 0.00001155
Iteration 198/1000 | Loss: 0.00001155
Iteration 199/1000 | Loss: 0.00001155
Iteration 200/1000 | Loss: 0.00001155
Iteration 201/1000 | Loss: 0.00001155
Iteration 202/1000 | Loss: 0.00001155
Iteration 203/1000 | Loss: 0.00001155
Iteration 204/1000 | Loss: 0.00001155
Iteration 205/1000 | Loss: 0.00001155
Iteration 206/1000 | Loss: 0.00001155
Iteration 207/1000 | Loss: 0.00001155
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [1.1550764611456543e-05, 1.1550764611456543e-05, 1.1550764611456543e-05, 1.1550764611456543e-05, 1.1550764611456543e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1550764611456543e-05

Optimization complete. Final v2v error: 2.9182982444763184 mm

Highest mean error: 3.2739410400390625 mm for frame 63

Lowest mean error: 2.772273540496826 mm for frame 121

Saving results

Total time: 37.69378900527954
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00737942
Iteration 2/25 | Loss: 0.00171007
Iteration 3/25 | Loss: 0.00144104
Iteration 4/25 | Loss: 0.00139344
Iteration 5/25 | Loss: 0.00138136
Iteration 6/25 | Loss: 0.00138050
Iteration 7/25 | Loss: 0.00137813
Iteration 8/25 | Loss: 0.00137680
Iteration 9/25 | Loss: 0.00137657
Iteration 10/25 | Loss: 0.00137657
Iteration 11/25 | Loss: 0.00137657
Iteration 12/25 | Loss: 0.00137657
Iteration 13/25 | Loss: 0.00137657
Iteration 14/25 | Loss: 0.00137657
Iteration 15/25 | Loss: 0.00137657
Iteration 16/25 | Loss: 0.00137657
Iteration 17/25 | Loss: 0.00137657
Iteration 18/25 | Loss: 0.00137656
Iteration 19/25 | Loss: 0.00137656
Iteration 20/25 | Loss: 0.00137656
Iteration 21/25 | Loss: 0.00137656
Iteration 22/25 | Loss: 0.00137656
Iteration 23/25 | Loss: 0.00137656
Iteration 24/25 | Loss: 0.00137656
Iteration 25/25 | Loss: 0.00137656

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.35512066
Iteration 2/25 | Loss: 0.00121159
Iteration 3/25 | Loss: 0.00121158
Iteration 4/25 | Loss: 0.00121158
Iteration 5/25 | Loss: 0.00119742
Iteration 6/25 | Loss: 0.00119742
Iteration 7/25 | Loss: 0.00119742
Iteration 8/25 | Loss: 0.00119742
Iteration 9/25 | Loss: 0.00119742
Iteration 10/25 | Loss: 0.00119742
Iteration 11/25 | Loss: 0.00119742
Iteration 12/25 | Loss: 0.00119742
Iteration 13/25 | Loss: 0.00119742
Iteration 14/25 | Loss: 0.00119742
Iteration 15/25 | Loss: 0.00119742
Iteration 16/25 | Loss: 0.00119742
Iteration 17/25 | Loss: 0.00119742
Iteration 18/25 | Loss: 0.00119742
Iteration 19/25 | Loss: 0.00119742
Iteration 20/25 | Loss: 0.00119742
Iteration 21/25 | Loss: 0.00119742
Iteration 22/25 | Loss: 0.00119742
Iteration 23/25 | Loss: 0.00119742
Iteration 24/25 | Loss: 0.00119742
Iteration 25/25 | Loss: 0.00119742

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119742
Iteration 2/1000 | Loss: 0.00006145
Iteration 3/1000 | Loss: 0.00003854
Iteration 4/1000 | Loss: 0.00002692
Iteration 5/1000 | Loss: 0.00002582
Iteration 6/1000 | Loss: 0.00002529
Iteration 7/1000 | Loss: 0.00002496
Iteration 8/1000 | Loss: 0.00002471
Iteration 9/1000 | Loss: 0.00002443
Iteration 10/1000 | Loss: 0.00002422
Iteration 11/1000 | Loss: 0.00002419
Iteration 12/1000 | Loss: 0.00002405
Iteration 13/1000 | Loss: 0.00002399
Iteration 14/1000 | Loss: 0.00002392
Iteration 15/1000 | Loss: 0.00002391
Iteration 16/1000 | Loss: 0.00002391
Iteration 17/1000 | Loss: 0.00002390
Iteration 18/1000 | Loss: 0.00002386
Iteration 19/1000 | Loss: 0.00002386
Iteration 20/1000 | Loss: 0.00002385
Iteration 21/1000 | Loss: 0.00002381
Iteration 22/1000 | Loss: 0.00002379
Iteration 23/1000 | Loss: 0.00002379
Iteration 24/1000 | Loss: 0.00002373
Iteration 25/1000 | Loss: 0.00002365
Iteration 26/1000 | Loss: 0.00002358
Iteration 27/1000 | Loss: 0.00002358
Iteration 28/1000 | Loss: 0.00002358
Iteration 29/1000 | Loss: 0.00002352
Iteration 30/1000 | Loss: 0.00002352
Iteration 31/1000 | Loss: 0.00002352
Iteration 32/1000 | Loss: 0.00002352
Iteration 33/1000 | Loss: 0.00002352
Iteration 34/1000 | Loss: 0.00002352
Iteration 35/1000 | Loss: 0.00002352
Iteration 36/1000 | Loss: 0.00002349
Iteration 37/1000 | Loss: 0.00002348
Iteration 38/1000 | Loss: 0.00002347
Iteration 39/1000 | Loss: 0.00002347
Iteration 40/1000 | Loss: 0.00002347
Iteration 41/1000 | Loss: 0.00002346
Iteration 42/1000 | Loss: 0.00002346
Iteration 43/1000 | Loss: 0.00002346
Iteration 44/1000 | Loss: 0.00002346
Iteration 45/1000 | Loss: 0.00002346
Iteration 46/1000 | Loss: 0.00002346
Iteration 47/1000 | Loss: 0.00002344
Iteration 48/1000 | Loss: 0.00002342
Iteration 49/1000 | Loss: 0.00002342
Iteration 50/1000 | Loss: 0.00002342
Iteration 51/1000 | Loss: 0.00002342
Iteration 52/1000 | Loss: 0.00002342
Iteration 53/1000 | Loss: 0.00002342
Iteration 54/1000 | Loss: 0.00002342
Iteration 55/1000 | Loss: 0.00002342
Iteration 56/1000 | Loss: 0.00002341
Iteration 57/1000 | Loss: 0.00002341
Iteration 58/1000 | Loss: 0.00002341
Iteration 59/1000 | Loss: 0.00002341
Iteration 60/1000 | Loss: 0.00002341
Iteration 61/1000 | Loss: 0.00002339
Iteration 62/1000 | Loss: 0.00002338
Iteration 63/1000 | Loss: 0.00002338
Iteration 64/1000 | Loss: 0.00002338
Iteration 65/1000 | Loss: 0.00002337
Iteration 66/1000 | Loss: 0.00002337
Iteration 67/1000 | Loss: 0.00002337
Iteration 68/1000 | Loss: 0.00002337
Iteration 69/1000 | Loss: 0.00002336
Iteration 70/1000 | Loss: 0.00002335
Iteration 71/1000 | Loss: 0.00002335
Iteration 72/1000 | Loss: 0.00002335
Iteration 73/1000 | Loss: 0.00002334
Iteration 74/1000 | Loss: 0.00002334
Iteration 75/1000 | Loss: 0.00002333
Iteration 76/1000 | Loss: 0.00002332
Iteration 77/1000 | Loss: 0.00002332
Iteration 78/1000 | Loss: 0.00002332
Iteration 79/1000 | Loss: 0.00002331
Iteration 80/1000 | Loss: 0.00002331
Iteration 81/1000 | Loss: 0.00002331
Iteration 82/1000 | Loss: 0.00002331
Iteration 83/1000 | Loss: 0.00002331
Iteration 84/1000 | Loss: 0.00002331
Iteration 85/1000 | Loss: 0.00002331
Iteration 86/1000 | Loss: 0.00002330
Iteration 87/1000 | Loss: 0.00002330
Iteration 88/1000 | Loss: 0.00002330
Iteration 89/1000 | Loss: 0.00002330
Iteration 90/1000 | Loss: 0.00002330
Iteration 91/1000 | Loss: 0.00002329
Iteration 92/1000 | Loss: 0.00002329
Iteration 93/1000 | Loss: 0.00002329
Iteration 94/1000 | Loss: 0.00002329
Iteration 95/1000 | Loss: 0.00002329
Iteration 96/1000 | Loss: 0.00002329
Iteration 97/1000 | Loss: 0.00002329
Iteration 98/1000 | Loss: 0.00002328
Iteration 99/1000 | Loss: 0.00002328
Iteration 100/1000 | Loss: 0.00002328
Iteration 101/1000 | Loss: 0.00002328
Iteration 102/1000 | Loss: 0.00002328
Iteration 103/1000 | Loss: 0.00002327
Iteration 104/1000 | Loss: 0.00002327
Iteration 105/1000 | Loss: 0.00002327
Iteration 106/1000 | Loss: 0.00002327
Iteration 107/1000 | Loss: 0.00002327
Iteration 108/1000 | Loss: 0.00002327
Iteration 109/1000 | Loss: 0.00002327
Iteration 110/1000 | Loss: 0.00002327
Iteration 111/1000 | Loss: 0.00002327
Iteration 112/1000 | Loss: 0.00002327
Iteration 113/1000 | Loss: 0.00002327
Iteration 114/1000 | Loss: 0.00002326
Iteration 115/1000 | Loss: 0.00002326
Iteration 116/1000 | Loss: 0.00002326
Iteration 117/1000 | Loss: 0.00002326
Iteration 118/1000 | Loss: 0.00002326
Iteration 119/1000 | Loss: 0.00002326
Iteration 120/1000 | Loss: 0.00002326
Iteration 121/1000 | Loss: 0.00002326
Iteration 122/1000 | Loss: 0.00002326
Iteration 123/1000 | Loss: 0.00002326
Iteration 124/1000 | Loss: 0.00002326
Iteration 125/1000 | Loss: 0.00002326
Iteration 126/1000 | Loss: 0.00002326
Iteration 127/1000 | Loss: 0.00002326
Iteration 128/1000 | Loss: 0.00002326
Iteration 129/1000 | Loss: 0.00002326
Iteration 130/1000 | Loss: 0.00002326
Iteration 131/1000 | Loss: 0.00002326
Iteration 132/1000 | Loss: 0.00002326
Iteration 133/1000 | Loss: 0.00002326
Iteration 134/1000 | Loss: 0.00002326
Iteration 135/1000 | Loss: 0.00002326
Iteration 136/1000 | Loss: 0.00002326
Iteration 137/1000 | Loss: 0.00002326
Iteration 138/1000 | Loss: 0.00002326
Iteration 139/1000 | Loss: 0.00002326
Iteration 140/1000 | Loss: 0.00002326
Iteration 141/1000 | Loss: 0.00002326
Iteration 142/1000 | Loss: 0.00002326
Iteration 143/1000 | Loss: 0.00002326
Iteration 144/1000 | Loss: 0.00002326
Iteration 145/1000 | Loss: 0.00002326
Iteration 146/1000 | Loss: 0.00002326
Iteration 147/1000 | Loss: 0.00002326
Iteration 148/1000 | Loss: 0.00002326
Iteration 149/1000 | Loss: 0.00002326
Iteration 150/1000 | Loss: 0.00002326
Iteration 151/1000 | Loss: 0.00002326
Iteration 152/1000 | Loss: 0.00002326
Iteration 153/1000 | Loss: 0.00002326
Iteration 154/1000 | Loss: 0.00002326
Iteration 155/1000 | Loss: 0.00002326
Iteration 156/1000 | Loss: 0.00002326
Iteration 157/1000 | Loss: 0.00002326
Iteration 158/1000 | Loss: 0.00002326
Iteration 159/1000 | Loss: 0.00002326
Iteration 160/1000 | Loss: 0.00002326
Iteration 161/1000 | Loss: 0.00002326
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [2.325822970306035e-05, 2.325822970306035e-05, 2.325822970306035e-05, 2.325822970306035e-05, 2.325822970306035e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.325822970306035e-05

Optimization complete. Final v2v error: 4.032537937164307 mm

Highest mean error: 4.434258460998535 mm for frame 212

Lowest mean error: 3.3991715908050537 mm for frame 144

Saving results

Total time: 54.484639406204224
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00789673
Iteration 2/25 | Loss: 0.00131149
Iteration 3/25 | Loss: 0.00125447
Iteration 4/25 | Loss: 0.00125058
Iteration 5/25 | Loss: 0.00124950
Iteration 6/25 | Loss: 0.00124950
Iteration 7/25 | Loss: 0.00124950
Iteration 8/25 | Loss: 0.00124950
Iteration 9/25 | Loss: 0.00124950
Iteration 10/25 | Loss: 0.00124950
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012495012488216162, 0.0012495012488216162, 0.0012495012488216162, 0.0012495012488216162, 0.0012495012488216162]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012495012488216162

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42489207
Iteration 2/25 | Loss: 0.00080823
Iteration 3/25 | Loss: 0.00080823
Iteration 4/25 | Loss: 0.00080823
Iteration 5/25 | Loss: 0.00080823
Iteration 6/25 | Loss: 0.00080823
Iteration 7/25 | Loss: 0.00080823
Iteration 8/25 | Loss: 0.00080823
Iteration 9/25 | Loss: 0.00080823
Iteration 10/25 | Loss: 0.00080823
Iteration 11/25 | Loss: 0.00080823
Iteration 12/25 | Loss: 0.00080823
Iteration 13/25 | Loss: 0.00080823
Iteration 14/25 | Loss: 0.00080823
Iteration 15/25 | Loss: 0.00080823
Iteration 16/25 | Loss: 0.00080823
Iteration 17/25 | Loss: 0.00080823
Iteration 18/25 | Loss: 0.00080823
Iteration 19/25 | Loss: 0.00080823
Iteration 20/25 | Loss: 0.00080823
Iteration 21/25 | Loss: 0.00080823
Iteration 22/25 | Loss: 0.00080823
Iteration 23/25 | Loss: 0.00080823
Iteration 24/25 | Loss: 0.00080823
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008082252461463213, 0.0008082252461463213, 0.0008082252461463213, 0.0008082252461463213, 0.0008082252461463213]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008082252461463213

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080823
Iteration 2/1000 | Loss: 0.00002739
Iteration 3/1000 | Loss: 0.00001942
Iteration 4/1000 | Loss: 0.00001662
Iteration 5/1000 | Loss: 0.00001552
Iteration 6/1000 | Loss: 0.00001461
Iteration 7/1000 | Loss: 0.00001401
Iteration 8/1000 | Loss: 0.00001350
Iteration 9/1000 | Loss: 0.00001324
Iteration 10/1000 | Loss: 0.00001297
Iteration 11/1000 | Loss: 0.00001286
Iteration 12/1000 | Loss: 0.00001280
Iteration 13/1000 | Loss: 0.00001276
Iteration 14/1000 | Loss: 0.00001270
Iteration 15/1000 | Loss: 0.00001270
Iteration 16/1000 | Loss: 0.00001269
Iteration 17/1000 | Loss: 0.00001269
Iteration 18/1000 | Loss: 0.00001266
Iteration 19/1000 | Loss: 0.00001263
Iteration 20/1000 | Loss: 0.00001262
Iteration 21/1000 | Loss: 0.00001255
Iteration 22/1000 | Loss: 0.00001250
Iteration 23/1000 | Loss: 0.00001250
Iteration 24/1000 | Loss: 0.00001249
Iteration 25/1000 | Loss: 0.00001248
Iteration 26/1000 | Loss: 0.00001248
Iteration 27/1000 | Loss: 0.00001247
Iteration 28/1000 | Loss: 0.00001247
Iteration 29/1000 | Loss: 0.00001247
Iteration 30/1000 | Loss: 0.00001246
Iteration 31/1000 | Loss: 0.00001246
Iteration 32/1000 | Loss: 0.00001246
Iteration 33/1000 | Loss: 0.00001245
Iteration 34/1000 | Loss: 0.00001245
Iteration 35/1000 | Loss: 0.00001245
Iteration 36/1000 | Loss: 0.00001245
Iteration 37/1000 | Loss: 0.00001245
Iteration 38/1000 | Loss: 0.00001245
Iteration 39/1000 | Loss: 0.00001244
Iteration 40/1000 | Loss: 0.00001244
Iteration 41/1000 | Loss: 0.00001244
Iteration 42/1000 | Loss: 0.00001243
Iteration 43/1000 | Loss: 0.00001243
Iteration 44/1000 | Loss: 0.00001242
Iteration 45/1000 | Loss: 0.00001242
Iteration 46/1000 | Loss: 0.00001241
Iteration 47/1000 | Loss: 0.00001241
Iteration 48/1000 | Loss: 0.00001241
Iteration 49/1000 | Loss: 0.00001241
Iteration 50/1000 | Loss: 0.00001240
Iteration 51/1000 | Loss: 0.00001240
Iteration 52/1000 | Loss: 0.00001240
Iteration 53/1000 | Loss: 0.00001239
Iteration 54/1000 | Loss: 0.00001239
Iteration 55/1000 | Loss: 0.00001239
Iteration 56/1000 | Loss: 0.00001239
Iteration 57/1000 | Loss: 0.00001238
Iteration 58/1000 | Loss: 0.00001238
Iteration 59/1000 | Loss: 0.00001238
Iteration 60/1000 | Loss: 0.00001237
Iteration 61/1000 | Loss: 0.00001237
Iteration 62/1000 | Loss: 0.00001236
Iteration 63/1000 | Loss: 0.00001236
Iteration 64/1000 | Loss: 0.00001236
Iteration 65/1000 | Loss: 0.00001235
Iteration 66/1000 | Loss: 0.00001235
Iteration 67/1000 | Loss: 0.00001235
Iteration 68/1000 | Loss: 0.00001234
Iteration 69/1000 | Loss: 0.00001234
Iteration 70/1000 | Loss: 0.00001233
Iteration 71/1000 | Loss: 0.00001232
Iteration 72/1000 | Loss: 0.00001232
Iteration 73/1000 | Loss: 0.00001232
Iteration 74/1000 | Loss: 0.00001230
Iteration 75/1000 | Loss: 0.00001230
Iteration 76/1000 | Loss: 0.00001230
Iteration 77/1000 | Loss: 0.00001229
Iteration 78/1000 | Loss: 0.00001229
Iteration 79/1000 | Loss: 0.00001229
Iteration 80/1000 | Loss: 0.00001228
Iteration 81/1000 | Loss: 0.00001228
Iteration 82/1000 | Loss: 0.00001227
Iteration 83/1000 | Loss: 0.00001226
Iteration 84/1000 | Loss: 0.00001225
Iteration 85/1000 | Loss: 0.00001225
Iteration 86/1000 | Loss: 0.00001225
Iteration 87/1000 | Loss: 0.00001225
Iteration 88/1000 | Loss: 0.00001225
Iteration 89/1000 | Loss: 0.00001225
Iteration 90/1000 | Loss: 0.00001225
Iteration 91/1000 | Loss: 0.00001225
Iteration 92/1000 | Loss: 0.00001225
Iteration 93/1000 | Loss: 0.00001225
Iteration 94/1000 | Loss: 0.00001224
Iteration 95/1000 | Loss: 0.00001224
Iteration 96/1000 | Loss: 0.00001224
Iteration 97/1000 | Loss: 0.00001223
Iteration 98/1000 | Loss: 0.00001223
Iteration 99/1000 | Loss: 0.00001223
Iteration 100/1000 | Loss: 0.00001223
Iteration 101/1000 | Loss: 0.00001222
Iteration 102/1000 | Loss: 0.00001222
Iteration 103/1000 | Loss: 0.00001222
Iteration 104/1000 | Loss: 0.00001221
Iteration 105/1000 | Loss: 0.00001221
Iteration 106/1000 | Loss: 0.00001220
Iteration 107/1000 | Loss: 0.00001220
Iteration 108/1000 | Loss: 0.00001219
Iteration 109/1000 | Loss: 0.00001218
Iteration 110/1000 | Loss: 0.00001218
Iteration 111/1000 | Loss: 0.00001218
Iteration 112/1000 | Loss: 0.00001218
Iteration 113/1000 | Loss: 0.00001218
Iteration 114/1000 | Loss: 0.00001218
Iteration 115/1000 | Loss: 0.00001218
Iteration 116/1000 | Loss: 0.00001218
Iteration 117/1000 | Loss: 0.00001217
Iteration 118/1000 | Loss: 0.00001217
Iteration 119/1000 | Loss: 0.00001217
Iteration 120/1000 | Loss: 0.00001216
Iteration 121/1000 | Loss: 0.00001216
Iteration 122/1000 | Loss: 0.00001216
Iteration 123/1000 | Loss: 0.00001216
Iteration 124/1000 | Loss: 0.00001216
Iteration 125/1000 | Loss: 0.00001216
Iteration 126/1000 | Loss: 0.00001216
Iteration 127/1000 | Loss: 0.00001216
Iteration 128/1000 | Loss: 0.00001216
Iteration 129/1000 | Loss: 0.00001215
Iteration 130/1000 | Loss: 0.00001215
Iteration 131/1000 | Loss: 0.00001215
Iteration 132/1000 | Loss: 0.00001215
Iteration 133/1000 | Loss: 0.00001215
Iteration 134/1000 | Loss: 0.00001215
Iteration 135/1000 | Loss: 0.00001215
Iteration 136/1000 | Loss: 0.00001215
Iteration 137/1000 | Loss: 0.00001215
Iteration 138/1000 | Loss: 0.00001215
Iteration 139/1000 | Loss: 0.00001215
Iteration 140/1000 | Loss: 0.00001215
Iteration 141/1000 | Loss: 0.00001215
Iteration 142/1000 | Loss: 0.00001215
Iteration 143/1000 | Loss: 0.00001215
Iteration 144/1000 | Loss: 0.00001215
Iteration 145/1000 | Loss: 0.00001215
Iteration 146/1000 | Loss: 0.00001215
Iteration 147/1000 | Loss: 0.00001215
Iteration 148/1000 | Loss: 0.00001215
Iteration 149/1000 | Loss: 0.00001215
Iteration 150/1000 | Loss: 0.00001215
Iteration 151/1000 | Loss: 0.00001215
Iteration 152/1000 | Loss: 0.00001215
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.2150891052442603e-05, 1.2150891052442603e-05, 1.2150891052442603e-05, 1.2150891052442603e-05, 1.2150891052442603e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2150891052442603e-05

Optimization complete. Final v2v error: 2.979731798171997 mm

Highest mean error: 3.1490302085876465 mm for frame 86

Lowest mean error: 2.824659585952759 mm for frame 20

Saving results

Total time: 36.17729830741882
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01044348
Iteration 2/25 | Loss: 0.00226747
Iteration 3/25 | Loss: 0.00200050
Iteration 4/25 | Loss: 0.00167441
Iteration 5/25 | Loss: 0.00153252
Iteration 6/25 | Loss: 0.00145887
Iteration 7/25 | Loss: 0.00144149
Iteration 8/25 | Loss: 0.00143302
Iteration 9/25 | Loss: 0.00142727
Iteration 10/25 | Loss: 0.00141760
Iteration 11/25 | Loss: 0.00141458
Iteration 12/25 | Loss: 0.00141410
Iteration 13/25 | Loss: 0.00141369
Iteration 14/25 | Loss: 0.00141323
Iteration 15/25 | Loss: 0.00141308
Iteration 16/25 | Loss: 0.00141444
Iteration 17/25 | Loss: 0.00141451
Iteration 18/25 | Loss: 0.00141468
Iteration 19/25 | Loss: 0.00141454
Iteration 20/25 | Loss: 0.00141241
Iteration 21/25 | Loss: 0.00141193
Iteration 22/25 | Loss: 0.00141188
Iteration 23/25 | Loss: 0.00141188
Iteration 24/25 | Loss: 0.00141188
Iteration 25/25 | Loss: 0.00141188

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.80906045
Iteration 2/25 | Loss: 0.00116911
Iteration 3/25 | Loss: 0.00116885
Iteration 4/25 | Loss: 0.00116885
Iteration 5/25 | Loss: 0.00116885
Iteration 6/25 | Loss: 0.00116885
Iteration 7/25 | Loss: 0.00116885
Iteration 8/25 | Loss: 0.00116885
Iteration 9/25 | Loss: 0.00116885
Iteration 10/25 | Loss: 0.00116885
Iteration 11/25 | Loss: 0.00116885
Iteration 12/25 | Loss: 0.00116885
Iteration 13/25 | Loss: 0.00116885
Iteration 14/25 | Loss: 0.00116885
Iteration 15/25 | Loss: 0.00116885
Iteration 16/25 | Loss: 0.00116885
Iteration 17/25 | Loss: 0.00116885
Iteration 18/25 | Loss: 0.00116885
Iteration 19/25 | Loss: 0.00116885
Iteration 20/25 | Loss: 0.00116885
Iteration 21/25 | Loss: 0.00116885
Iteration 22/25 | Loss: 0.00116885
Iteration 23/25 | Loss: 0.00116885
Iteration 24/25 | Loss: 0.00116885
Iteration 25/25 | Loss: 0.00116885

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116885
Iteration 2/1000 | Loss: 0.00010478
Iteration 3/1000 | Loss: 0.00007159
Iteration 4/1000 | Loss: 0.00005203
Iteration 5/1000 | Loss: 0.00004684
Iteration 6/1000 | Loss: 0.00004466
Iteration 7/1000 | Loss: 0.00004319
Iteration 8/1000 | Loss: 0.00004221
Iteration 9/1000 | Loss: 0.00004141
Iteration 10/1000 | Loss: 0.00004078
Iteration 11/1000 | Loss: 0.00004022
Iteration 12/1000 | Loss: 0.00003970
Iteration 13/1000 | Loss: 0.00003939
Iteration 14/1000 | Loss: 0.00003914
Iteration 15/1000 | Loss: 0.00003927
Iteration 16/1000 | Loss: 0.00003893
Iteration 17/1000 | Loss: 0.00003873
Iteration 18/1000 | Loss: 0.00003852
Iteration 19/1000 | Loss: 0.00003846
Iteration 20/1000 | Loss: 0.00003842
Iteration 21/1000 | Loss: 0.00003826
Iteration 22/1000 | Loss: 0.00003824
Iteration 23/1000 | Loss: 0.00003818
Iteration 24/1000 | Loss: 0.00003817
Iteration 25/1000 | Loss: 0.00003824
Iteration 26/1000 | Loss: 0.00003823
Iteration 27/1000 | Loss: 0.00003801
Iteration 28/1000 | Loss: 0.00003788
Iteration 29/1000 | Loss: 0.00003785
Iteration 30/1000 | Loss: 0.00003784
Iteration 31/1000 | Loss: 0.00003784
Iteration 32/1000 | Loss: 0.00003782
Iteration 33/1000 | Loss: 0.00003781
Iteration 34/1000 | Loss: 0.00003781
Iteration 35/1000 | Loss: 0.00003781
Iteration 36/1000 | Loss: 0.00003780
Iteration 37/1000 | Loss: 0.00003780
Iteration 38/1000 | Loss: 0.00003780
Iteration 39/1000 | Loss: 0.00003779
Iteration 40/1000 | Loss: 0.00003779
Iteration 41/1000 | Loss: 0.00003778
Iteration 42/1000 | Loss: 0.00003778
Iteration 43/1000 | Loss: 0.00003778
Iteration 44/1000 | Loss: 0.00003777
Iteration 45/1000 | Loss: 0.00003776
Iteration 46/1000 | Loss: 0.00003776
Iteration 47/1000 | Loss: 0.00003776
Iteration 48/1000 | Loss: 0.00003774
Iteration 49/1000 | Loss: 0.00003774
Iteration 50/1000 | Loss: 0.00003773
Iteration 51/1000 | Loss: 0.00003773
Iteration 52/1000 | Loss: 0.00003773
Iteration 53/1000 | Loss: 0.00003771
Iteration 54/1000 | Loss: 0.00003771
Iteration 55/1000 | Loss: 0.00003771
Iteration 56/1000 | Loss: 0.00003770
Iteration 57/1000 | Loss: 0.00003770
Iteration 58/1000 | Loss: 0.00003770
Iteration 59/1000 | Loss: 0.00003770
Iteration 60/1000 | Loss: 0.00003770
Iteration 61/1000 | Loss: 0.00003769
Iteration 62/1000 | Loss: 0.00003769
Iteration 63/1000 | Loss: 0.00003769
Iteration 64/1000 | Loss: 0.00003769
Iteration 65/1000 | Loss: 0.00003769
Iteration 66/1000 | Loss: 0.00003769
Iteration 67/1000 | Loss: 0.00003769
Iteration 68/1000 | Loss: 0.00003769
Iteration 69/1000 | Loss: 0.00003769
Iteration 70/1000 | Loss: 0.00003769
Iteration 71/1000 | Loss: 0.00003768
Iteration 72/1000 | Loss: 0.00003768
Iteration 73/1000 | Loss: 0.00003768
Iteration 74/1000 | Loss: 0.00003768
Iteration 75/1000 | Loss: 0.00003768
Iteration 76/1000 | Loss: 0.00003767
Iteration 77/1000 | Loss: 0.00003767
Iteration 78/1000 | Loss: 0.00003767
Iteration 79/1000 | Loss: 0.00003767
Iteration 80/1000 | Loss: 0.00003767
Iteration 81/1000 | Loss: 0.00003766
Iteration 82/1000 | Loss: 0.00003766
Iteration 83/1000 | Loss: 0.00003765
Iteration 84/1000 | Loss: 0.00003764
Iteration 85/1000 | Loss: 0.00003764
Iteration 86/1000 | Loss: 0.00003764
Iteration 87/1000 | Loss: 0.00003764
Iteration 88/1000 | Loss: 0.00003764
Iteration 89/1000 | Loss: 0.00003764
Iteration 90/1000 | Loss: 0.00003764
Iteration 91/1000 | Loss: 0.00003764
Iteration 92/1000 | Loss: 0.00003764
Iteration 93/1000 | Loss: 0.00003763
Iteration 94/1000 | Loss: 0.00003762
Iteration 95/1000 | Loss: 0.00003762
Iteration 96/1000 | Loss: 0.00003762
Iteration 97/1000 | Loss: 0.00003761
Iteration 98/1000 | Loss: 0.00003761
Iteration 99/1000 | Loss: 0.00003761
Iteration 100/1000 | Loss: 0.00003761
Iteration 101/1000 | Loss: 0.00003761
Iteration 102/1000 | Loss: 0.00003761
Iteration 103/1000 | Loss: 0.00003761
Iteration 104/1000 | Loss: 0.00003761
Iteration 105/1000 | Loss: 0.00003761
Iteration 106/1000 | Loss: 0.00003760
Iteration 107/1000 | Loss: 0.00003760
Iteration 108/1000 | Loss: 0.00003760
Iteration 109/1000 | Loss: 0.00003760
Iteration 110/1000 | Loss: 0.00003795
Iteration 111/1000 | Loss: 0.00003795
Iteration 112/1000 | Loss: 0.00003777
Iteration 113/1000 | Loss: 0.00003768
Iteration 114/1000 | Loss: 0.00003768
Iteration 115/1000 | Loss: 0.00003768
Iteration 116/1000 | Loss: 0.00003768
Iteration 117/1000 | Loss: 0.00003768
Iteration 118/1000 | Loss: 0.00003768
Iteration 119/1000 | Loss: 0.00003767
Iteration 120/1000 | Loss: 0.00003767
Iteration 121/1000 | Loss: 0.00003767
Iteration 122/1000 | Loss: 0.00003767
Iteration 123/1000 | Loss: 0.00003767
Iteration 124/1000 | Loss: 0.00003766
Iteration 125/1000 | Loss: 0.00003766
Iteration 126/1000 | Loss: 0.00003766
Iteration 127/1000 | Loss: 0.00003766
Iteration 128/1000 | Loss: 0.00003766
Iteration 129/1000 | Loss: 0.00003766
Iteration 130/1000 | Loss: 0.00003766
Iteration 131/1000 | Loss: 0.00003766
Iteration 132/1000 | Loss: 0.00003766
Iteration 133/1000 | Loss: 0.00003766
Iteration 134/1000 | Loss: 0.00003766
Iteration 135/1000 | Loss: 0.00003766
Iteration 136/1000 | Loss: 0.00003766
Iteration 137/1000 | Loss: 0.00003766
Iteration 138/1000 | Loss: 0.00003766
Iteration 139/1000 | Loss: 0.00003766
Iteration 140/1000 | Loss: 0.00003766
Iteration 141/1000 | Loss: 0.00003766
Iteration 142/1000 | Loss: 0.00003766
Iteration 143/1000 | Loss: 0.00003766
Iteration 144/1000 | Loss: 0.00003766
Iteration 145/1000 | Loss: 0.00003766
Iteration 146/1000 | Loss: 0.00003766
Iteration 147/1000 | Loss: 0.00003766
Iteration 148/1000 | Loss: 0.00003766
Iteration 149/1000 | Loss: 0.00003766
Iteration 150/1000 | Loss: 0.00003766
Iteration 151/1000 | Loss: 0.00003766
Iteration 152/1000 | Loss: 0.00003766
Iteration 153/1000 | Loss: 0.00003766
Iteration 154/1000 | Loss: 0.00003766
Iteration 155/1000 | Loss: 0.00003766
Iteration 156/1000 | Loss: 0.00003766
Iteration 157/1000 | Loss: 0.00003766
Iteration 158/1000 | Loss: 0.00003766
Iteration 159/1000 | Loss: 0.00003766
Iteration 160/1000 | Loss: 0.00003766
Iteration 161/1000 | Loss: 0.00003766
Iteration 162/1000 | Loss: 0.00003766
Iteration 163/1000 | Loss: 0.00003766
Iteration 164/1000 | Loss: 0.00003766
Iteration 165/1000 | Loss: 0.00003766
Iteration 166/1000 | Loss: 0.00003766
Iteration 167/1000 | Loss: 0.00003766
Iteration 168/1000 | Loss: 0.00003766
Iteration 169/1000 | Loss: 0.00003766
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [3.765664223465137e-05, 3.765664223465137e-05, 3.765664223465137e-05, 3.765664223465137e-05, 3.765664223465137e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.765664223465137e-05

Optimization complete. Final v2v error: 4.917476177215576 mm

Highest mean error: 6.712522983551025 mm for frame 63

Lowest mean error: 3.4860100746154785 mm for frame 0

Saving results

Total time: 81.94968056678772
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00724623
Iteration 2/25 | Loss: 0.00146863
Iteration 3/25 | Loss: 0.00133910
Iteration 4/25 | Loss: 0.00127786
Iteration 5/25 | Loss: 0.00126960
Iteration 6/25 | Loss: 0.00126219
Iteration 7/25 | Loss: 0.00125780
Iteration 8/25 | Loss: 0.00125467
Iteration 9/25 | Loss: 0.00125355
Iteration 10/25 | Loss: 0.00125308
Iteration 11/25 | Loss: 0.00125288
Iteration 12/25 | Loss: 0.00125277
Iteration 13/25 | Loss: 0.00125277
Iteration 14/25 | Loss: 0.00125277
Iteration 15/25 | Loss: 0.00125276
Iteration 16/25 | Loss: 0.00125276
Iteration 17/25 | Loss: 0.00125276
Iteration 18/25 | Loss: 0.00125276
Iteration 19/25 | Loss: 0.00125276
Iteration 20/25 | Loss: 0.00125276
Iteration 21/25 | Loss: 0.00125276
Iteration 22/25 | Loss: 0.00125276
Iteration 23/25 | Loss: 0.00125276
Iteration 24/25 | Loss: 0.00125276
Iteration 25/25 | Loss: 0.00125276

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.77298355
Iteration 2/25 | Loss: 0.00101527
Iteration 3/25 | Loss: 0.00101523
Iteration 4/25 | Loss: 0.00101523
Iteration 5/25 | Loss: 0.00101522
Iteration 6/25 | Loss: 0.00101522
Iteration 7/25 | Loss: 0.00101522
Iteration 8/25 | Loss: 0.00101522
Iteration 9/25 | Loss: 0.00101522
Iteration 10/25 | Loss: 0.00101522
Iteration 11/25 | Loss: 0.00101522
Iteration 12/25 | Loss: 0.00101522
Iteration 13/25 | Loss: 0.00101522
Iteration 14/25 | Loss: 0.00101522
Iteration 15/25 | Loss: 0.00101522
Iteration 16/25 | Loss: 0.00101522
Iteration 17/25 | Loss: 0.00101522
Iteration 18/25 | Loss: 0.00101522
Iteration 19/25 | Loss: 0.00101522
Iteration 20/25 | Loss: 0.00101522
Iteration 21/25 | Loss: 0.00101522
Iteration 22/25 | Loss: 0.00101522
Iteration 23/25 | Loss: 0.00101522
Iteration 24/25 | Loss: 0.00101522
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0010152229806408286, 0.0010152229806408286, 0.0010152229806408286, 0.0010152229806408286, 0.0010152229806408286]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010152229806408286

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101522
Iteration 2/1000 | Loss: 0.00004201
Iteration 3/1000 | Loss: 0.00002674
Iteration 4/1000 | Loss: 0.00002147
Iteration 5/1000 | Loss: 0.00002018
Iteration 6/1000 | Loss: 0.00001901
Iteration 7/1000 | Loss: 0.00001806
Iteration 8/1000 | Loss: 0.00001749
Iteration 9/1000 | Loss: 0.00001708
Iteration 10/1000 | Loss: 0.00001704
Iteration 11/1000 | Loss: 0.00001681
Iteration 12/1000 | Loss: 0.00001656
Iteration 13/1000 | Loss: 0.00001648
Iteration 14/1000 | Loss: 0.00001631
Iteration 15/1000 | Loss: 0.00001624
Iteration 16/1000 | Loss: 0.00001624
Iteration 17/1000 | Loss: 0.00001619
Iteration 18/1000 | Loss: 0.00001613
Iteration 19/1000 | Loss: 0.00001611
Iteration 20/1000 | Loss: 0.00001605
Iteration 21/1000 | Loss: 0.00001605
Iteration 22/1000 | Loss: 0.00001603
Iteration 23/1000 | Loss: 0.00001601
Iteration 24/1000 | Loss: 0.00001600
Iteration 25/1000 | Loss: 0.00001600
Iteration 26/1000 | Loss: 0.00001599
Iteration 27/1000 | Loss: 0.00001599
Iteration 28/1000 | Loss: 0.00001599
Iteration 29/1000 | Loss: 0.00001598
Iteration 30/1000 | Loss: 0.00001598
Iteration 31/1000 | Loss: 0.00001597
Iteration 32/1000 | Loss: 0.00001597
Iteration 33/1000 | Loss: 0.00001597
Iteration 34/1000 | Loss: 0.00001596
Iteration 35/1000 | Loss: 0.00001596
Iteration 36/1000 | Loss: 0.00001596
Iteration 37/1000 | Loss: 0.00001595
Iteration 38/1000 | Loss: 0.00001595
Iteration 39/1000 | Loss: 0.00001595
Iteration 40/1000 | Loss: 0.00001595
Iteration 41/1000 | Loss: 0.00001594
Iteration 42/1000 | Loss: 0.00001594
Iteration 43/1000 | Loss: 0.00001594
Iteration 44/1000 | Loss: 0.00001594
Iteration 45/1000 | Loss: 0.00001594
Iteration 46/1000 | Loss: 0.00001594
Iteration 47/1000 | Loss: 0.00001594
Iteration 48/1000 | Loss: 0.00001593
Iteration 49/1000 | Loss: 0.00001593
Iteration 50/1000 | Loss: 0.00001593
Iteration 51/1000 | Loss: 0.00001592
Iteration 52/1000 | Loss: 0.00001592
Iteration 53/1000 | Loss: 0.00001592
Iteration 54/1000 | Loss: 0.00001592
Iteration 55/1000 | Loss: 0.00001592
Iteration 56/1000 | Loss: 0.00001592
Iteration 57/1000 | Loss: 0.00001591
Iteration 58/1000 | Loss: 0.00001591
Iteration 59/1000 | Loss: 0.00001591
Iteration 60/1000 | Loss: 0.00001591
Iteration 61/1000 | Loss: 0.00001591
Iteration 62/1000 | Loss: 0.00001591
Iteration 63/1000 | Loss: 0.00001591
Iteration 64/1000 | Loss: 0.00001590
Iteration 65/1000 | Loss: 0.00001590
Iteration 66/1000 | Loss: 0.00001590
Iteration 67/1000 | Loss: 0.00001590
Iteration 68/1000 | Loss: 0.00001590
Iteration 69/1000 | Loss: 0.00001590
Iteration 70/1000 | Loss: 0.00001590
Iteration 71/1000 | Loss: 0.00001589
Iteration 72/1000 | Loss: 0.00001589
Iteration 73/1000 | Loss: 0.00001589
Iteration 74/1000 | Loss: 0.00001589
Iteration 75/1000 | Loss: 0.00001588
Iteration 76/1000 | Loss: 0.00001588
Iteration 77/1000 | Loss: 0.00001588
Iteration 78/1000 | Loss: 0.00001588
Iteration 79/1000 | Loss: 0.00001588
Iteration 80/1000 | Loss: 0.00001588
Iteration 81/1000 | Loss: 0.00001587
Iteration 82/1000 | Loss: 0.00001587
Iteration 83/1000 | Loss: 0.00001587
Iteration 84/1000 | Loss: 0.00001587
Iteration 85/1000 | Loss: 0.00001587
Iteration 86/1000 | Loss: 0.00001587
Iteration 87/1000 | Loss: 0.00001586
Iteration 88/1000 | Loss: 0.00001586
Iteration 89/1000 | Loss: 0.00001586
Iteration 90/1000 | Loss: 0.00001586
Iteration 91/1000 | Loss: 0.00001586
Iteration 92/1000 | Loss: 0.00001585
Iteration 93/1000 | Loss: 0.00001585
Iteration 94/1000 | Loss: 0.00001585
Iteration 95/1000 | Loss: 0.00001585
Iteration 96/1000 | Loss: 0.00001585
Iteration 97/1000 | Loss: 0.00001585
Iteration 98/1000 | Loss: 0.00001585
Iteration 99/1000 | Loss: 0.00001585
Iteration 100/1000 | Loss: 0.00001585
Iteration 101/1000 | Loss: 0.00001585
Iteration 102/1000 | Loss: 0.00001585
Iteration 103/1000 | Loss: 0.00001585
Iteration 104/1000 | Loss: 0.00001585
Iteration 105/1000 | Loss: 0.00001584
Iteration 106/1000 | Loss: 0.00001584
Iteration 107/1000 | Loss: 0.00001584
Iteration 108/1000 | Loss: 0.00001584
Iteration 109/1000 | Loss: 0.00001583
Iteration 110/1000 | Loss: 0.00001583
Iteration 111/1000 | Loss: 0.00001583
Iteration 112/1000 | Loss: 0.00001583
Iteration 113/1000 | Loss: 0.00001583
Iteration 114/1000 | Loss: 0.00001582
Iteration 115/1000 | Loss: 0.00001582
Iteration 116/1000 | Loss: 0.00001582
Iteration 117/1000 | Loss: 0.00001582
Iteration 118/1000 | Loss: 0.00001582
Iteration 119/1000 | Loss: 0.00001581
Iteration 120/1000 | Loss: 0.00001581
Iteration 121/1000 | Loss: 0.00001581
Iteration 122/1000 | Loss: 0.00001581
Iteration 123/1000 | Loss: 0.00001581
Iteration 124/1000 | Loss: 0.00001581
Iteration 125/1000 | Loss: 0.00001581
Iteration 126/1000 | Loss: 0.00001581
Iteration 127/1000 | Loss: 0.00001581
Iteration 128/1000 | Loss: 0.00001580
Iteration 129/1000 | Loss: 0.00001580
Iteration 130/1000 | Loss: 0.00001580
Iteration 131/1000 | Loss: 0.00001580
Iteration 132/1000 | Loss: 0.00001580
Iteration 133/1000 | Loss: 0.00001579
Iteration 134/1000 | Loss: 0.00001579
Iteration 135/1000 | Loss: 0.00001579
Iteration 136/1000 | Loss: 0.00001579
Iteration 137/1000 | Loss: 0.00001579
Iteration 138/1000 | Loss: 0.00001579
Iteration 139/1000 | Loss: 0.00001579
Iteration 140/1000 | Loss: 0.00001579
Iteration 141/1000 | Loss: 0.00001579
Iteration 142/1000 | Loss: 0.00001579
Iteration 143/1000 | Loss: 0.00001579
Iteration 144/1000 | Loss: 0.00001579
Iteration 145/1000 | Loss: 0.00001579
Iteration 146/1000 | Loss: 0.00001579
Iteration 147/1000 | Loss: 0.00001579
Iteration 148/1000 | Loss: 0.00001579
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.5788762539159507e-05, 1.5788762539159507e-05, 1.5788762539159507e-05, 1.5788762539159507e-05, 1.5788762539159507e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5788762539159507e-05

Optimization complete. Final v2v error: 3.399649143218994 mm

Highest mean error: 3.879669427871704 mm for frame 10

Lowest mean error: 3.0991370677948 mm for frame 127

Saving results

Total time: 50.282979249954224
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00916337
Iteration 2/25 | Loss: 0.00179921
Iteration 3/25 | Loss: 0.00146929
Iteration 4/25 | Loss: 0.00144187
Iteration 5/25 | Loss: 0.00143293
Iteration 6/25 | Loss: 0.00143198
Iteration 7/25 | Loss: 0.00143198
Iteration 8/25 | Loss: 0.00143198
Iteration 9/25 | Loss: 0.00143198
Iteration 10/25 | Loss: 0.00143198
Iteration 11/25 | Loss: 0.00143198
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014319820329546928, 0.0014319820329546928, 0.0014319820329546928, 0.0014319820329546928, 0.0014319820329546928]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014319820329546928

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.98657870
Iteration 2/25 | Loss: 0.00099798
Iteration 3/25 | Loss: 0.00099798
Iteration 4/25 | Loss: 0.00099798
Iteration 5/25 | Loss: 0.00099798
Iteration 6/25 | Loss: 0.00099798
Iteration 7/25 | Loss: 0.00099798
Iteration 8/25 | Loss: 0.00099798
Iteration 9/25 | Loss: 0.00099798
Iteration 10/25 | Loss: 0.00099798
Iteration 11/25 | Loss: 0.00099798
Iteration 12/25 | Loss: 0.00099798
Iteration 13/25 | Loss: 0.00099798
Iteration 14/25 | Loss: 0.00099798
Iteration 15/25 | Loss: 0.00099798
Iteration 16/25 | Loss: 0.00099798
Iteration 17/25 | Loss: 0.00099798
Iteration 18/25 | Loss: 0.00099798
Iteration 19/25 | Loss: 0.00099798
Iteration 20/25 | Loss: 0.00099798
Iteration 21/25 | Loss: 0.00099798
Iteration 22/25 | Loss: 0.00099798
Iteration 23/25 | Loss: 0.00099798
Iteration 24/25 | Loss: 0.00099798
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0009979763999581337, 0.0009979763999581337, 0.0009979763999581337, 0.0009979763999581337, 0.0009979763999581337]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009979763999581337

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099798
Iteration 2/1000 | Loss: 0.00006743
Iteration 3/1000 | Loss: 0.00004632
Iteration 4/1000 | Loss: 0.00004225
Iteration 5/1000 | Loss: 0.00004007
Iteration 6/1000 | Loss: 0.00003888
Iteration 7/1000 | Loss: 0.00003773
Iteration 8/1000 | Loss: 0.00003719
Iteration 9/1000 | Loss: 0.00003659
Iteration 10/1000 | Loss: 0.00003616
Iteration 11/1000 | Loss: 0.00003579
Iteration 12/1000 | Loss: 0.00003548
Iteration 13/1000 | Loss: 0.00003512
Iteration 14/1000 | Loss: 0.00003484
Iteration 15/1000 | Loss: 0.00003456
Iteration 16/1000 | Loss: 0.00003428
Iteration 17/1000 | Loss: 0.00003404
Iteration 18/1000 | Loss: 0.00003385
Iteration 19/1000 | Loss: 0.00003383
Iteration 20/1000 | Loss: 0.00003368
Iteration 21/1000 | Loss: 0.00003359
Iteration 22/1000 | Loss: 0.00003359
Iteration 23/1000 | Loss: 0.00003350
Iteration 24/1000 | Loss: 0.00003340
Iteration 25/1000 | Loss: 0.00003339
Iteration 26/1000 | Loss: 0.00003338
Iteration 27/1000 | Loss: 0.00003338
Iteration 28/1000 | Loss: 0.00003337
Iteration 29/1000 | Loss: 0.00003337
Iteration 30/1000 | Loss: 0.00003335
Iteration 31/1000 | Loss: 0.00003333
Iteration 32/1000 | Loss: 0.00003332
Iteration 33/1000 | Loss: 0.00003332
Iteration 34/1000 | Loss: 0.00003330
Iteration 35/1000 | Loss: 0.00003330
Iteration 36/1000 | Loss: 0.00003330
Iteration 37/1000 | Loss: 0.00003329
Iteration 38/1000 | Loss: 0.00003329
Iteration 39/1000 | Loss: 0.00003329
Iteration 40/1000 | Loss: 0.00003329
Iteration 41/1000 | Loss: 0.00003329
Iteration 42/1000 | Loss: 0.00003329
Iteration 43/1000 | Loss: 0.00003328
Iteration 44/1000 | Loss: 0.00003328
Iteration 45/1000 | Loss: 0.00003328
Iteration 46/1000 | Loss: 0.00003328
Iteration 47/1000 | Loss: 0.00003328
Iteration 48/1000 | Loss: 0.00003328
Iteration 49/1000 | Loss: 0.00003327
Iteration 50/1000 | Loss: 0.00003327
Iteration 51/1000 | Loss: 0.00003327
Iteration 52/1000 | Loss: 0.00003326
Iteration 53/1000 | Loss: 0.00003326
Iteration 54/1000 | Loss: 0.00003326
Iteration 55/1000 | Loss: 0.00003326
Iteration 56/1000 | Loss: 0.00003326
Iteration 57/1000 | Loss: 0.00003326
Iteration 58/1000 | Loss: 0.00003326
Iteration 59/1000 | Loss: 0.00003326
Iteration 60/1000 | Loss: 0.00003326
Iteration 61/1000 | Loss: 0.00003326
Iteration 62/1000 | Loss: 0.00003326
Iteration 63/1000 | Loss: 0.00003326
Iteration 64/1000 | Loss: 0.00003326
Iteration 65/1000 | Loss: 0.00003326
Iteration 66/1000 | Loss: 0.00003326
Iteration 67/1000 | Loss: 0.00003326
Iteration 68/1000 | Loss: 0.00003326
Iteration 69/1000 | Loss: 0.00003325
Iteration 70/1000 | Loss: 0.00003325
Iteration 71/1000 | Loss: 0.00003325
Iteration 72/1000 | Loss: 0.00003325
Iteration 73/1000 | Loss: 0.00003325
Iteration 74/1000 | Loss: 0.00003325
Iteration 75/1000 | Loss: 0.00003325
Iteration 76/1000 | Loss: 0.00003325
Iteration 77/1000 | Loss: 0.00003325
Iteration 78/1000 | Loss: 0.00003324
Iteration 79/1000 | Loss: 0.00003324
Iteration 80/1000 | Loss: 0.00003324
Iteration 81/1000 | Loss: 0.00003324
Iteration 82/1000 | Loss: 0.00003324
Iteration 83/1000 | Loss: 0.00003324
Iteration 84/1000 | Loss: 0.00003323
Iteration 85/1000 | Loss: 0.00003323
Iteration 86/1000 | Loss: 0.00003323
Iteration 87/1000 | Loss: 0.00003322
Iteration 88/1000 | Loss: 0.00003322
Iteration 89/1000 | Loss: 0.00003322
Iteration 90/1000 | Loss: 0.00003322
Iteration 91/1000 | Loss: 0.00003322
Iteration 92/1000 | Loss: 0.00003321
Iteration 93/1000 | Loss: 0.00003321
Iteration 94/1000 | Loss: 0.00003321
Iteration 95/1000 | Loss: 0.00003320
Iteration 96/1000 | Loss: 0.00003320
Iteration 97/1000 | Loss: 0.00003320
Iteration 98/1000 | Loss: 0.00003320
Iteration 99/1000 | Loss: 0.00003320
Iteration 100/1000 | Loss: 0.00003320
Iteration 101/1000 | Loss: 0.00003319
Iteration 102/1000 | Loss: 0.00003319
Iteration 103/1000 | Loss: 0.00003319
Iteration 104/1000 | Loss: 0.00003319
Iteration 105/1000 | Loss: 0.00003319
Iteration 106/1000 | Loss: 0.00003319
Iteration 107/1000 | Loss: 0.00003319
Iteration 108/1000 | Loss: 0.00003319
Iteration 109/1000 | Loss: 0.00003319
Iteration 110/1000 | Loss: 0.00003319
Iteration 111/1000 | Loss: 0.00003319
Iteration 112/1000 | Loss: 0.00003318
Iteration 113/1000 | Loss: 0.00003318
Iteration 114/1000 | Loss: 0.00003318
Iteration 115/1000 | Loss: 0.00003318
Iteration 116/1000 | Loss: 0.00003317
Iteration 117/1000 | Loss: 0.00003317
Iteration 118/1000 | Loss: 0.00003317
Iteration 119/1000 | Loss: 0.00003317
Iteration 120/1000 | Loss: 0.00003317
Iteration 121/1000 | Loss: 0.00003317
Iteration 122/1000 | Loss: 0.00003317
Iteration 123/1000 | Loss: 0.00003317
Iteration 124/1000 | Loss: 0.00003316
Iteration 125/1000 | Loss: 0.00003316
Iteration 126/1000 | Loss: 0.00003316
Iteration 127/1000 | Loss: 0.00003316
Iteration 128/1000 | Loss: 0.00003316
Iteration 129/1000 | Loss: 0.00003316
Iteration 130/1000 | Loss: 0.00003316
Iteration 131/1000 | Loss: 0.00003316
Iteration 132/1000 | Loss: 0.00003315
Iteration 133/1000 | Loss: 0.00003315
Iteration 134/1000 | Loss: 0.00003315
Iteration 135/1000 | Loss: 0.00003315
Iteration 136/1000 | Loss: 0.00003315
Iteration 137/1000 | Loss: 0.00003315
Iteration 138/1000 | Loss: 0.00003315
Iteration 139/1000 | Loss: 0.00003315
Iteration 140/1000 | Loss: 0.00003315
Iteration 141/1000 | Loss: 0.00003315
Iteration 142/1000 | Loss: 0.00003314
Iteration 143/1000 | Loss: 0.00003314
Iteration 144/1000 | Loss: 0.00003314
Iteration 145/1000 | Loss: 0.00003314
Iteration 146/1000 | Loss: 0.00003314
Iteration 147/1000 | Loss: 0.00003314
Iteration 148/1000 | Loss: 0.00003314
Iteration 149/1000 | Loss: 0.00003314
Iteration 150/1000 | Loss: 0.00003314
Iteration 151/1000 | Loss: 0.00003313
Iteration 152/1000 | Loss: 0.00003313
Iteration 153/1000 | Loss: 0.00003313
Iteration 154/1000 | Loss: 0.00003313
Iteration 155/1000 | Loss: 0.00003313
Iteration 156/1000 | Loss: 0.00003313
Iteration 157/1000 | Loss: 0.00003313
Iteration 158/1000 | Loss: 0.00003313
Iteration 159/1000 | Loss: 0.00003313
Iteration 160/1000 | Loss: 0.00003313
Iteration 161/1000 | Loss: 0.00003313
Iteration 162/1000 | Loss: 0.00003313
Iteration 163/1000 | Loss: 0.00003313
Iteration 164/1000 | Loss: 0.00003313
Iteration 165/1000 | Loss: 0.00003313
Iteration 166/1000 | Loss: 0.00003312
Iteration 167/1000 | Loss: 0.00003312
Iteration 168/1000 | Loss: 0.00003312
Iteration 169/1000 | Loss: 0.00003312
Iteration 170/1000 | Loss: 0.00003312
Iteration 171/1000 | Loss: 0.00003312
Iteration 172/1000 | Loss: 0.00003312
Iteration 173/1000 | Loss: 0.00003312
Iteration 174/1000 | Loss: 0.00003312
Iteration 175/1000 | Loss: 0.00003312
Iteration 176/1000 | Loss: 0.00003312
Iteration 177/1000 | Loss: 0.00003312
Iteration 178/1000 | Loss: 0.00003312
Iteration 179/1000 | Loss: 0.00003312
Iteration 180/1000 | Loss: 0.00003312
Iteration 181/1000 | Loss: 0.00003312
Iteration 182/1000 | Loss: 0.00003312
Iteration 183/1000 | Loss: 0.00003312
Iteration 184/1000 | Loss: 0.00003312
Iteration 185/1000 | Loss: 0.00003312
Iteration 186/1000 | Loss: 0.00003312
Iteration 187/1000 | Loss: 0.00003311
Iteration 188/1000 | Loss: 0.00003311
Iteration 189/1000 | Loss: 0.00003311
Iteration 190/1000 | Loss: 0.00003311
Iteration 191/1000 | Loss: 0.00003311
Iteration 192/1000 | Loss: 0.00003311
Iteration 193/1000 | Loss: 0.00003311
Iteration 194/1000 | Loss: 0.00003311
Iteration 195/1000 | Loss: 0.00003311
Iteration 196/1000 | Loss: 0.00003311
Iteration 197/1000 | Loss: 0.00003311
Iteration 198/1000 | Loss: 0.00003311
Iteration 199/1000 | Loss: 0.00003311
Iteration 200/1000 | Loss: 0.00003310
Iteration 201/1000 | Loss: 0.00003310
Iteration 202/1000 | Loss: 0.00003310
Iteration 203/1000 | Loss: 0.00003310
Iteration 204/1000 | Loss: 0.00003310
Iteration 205/1000 | Loss: 0.00003310
Iteration 206/1000 | Loss: 0.00003310
Iteration 207/1000 | Loss: 0.00003310
Iteration 208/1000 | Loss: 0.00003310
Iteration 209/1000 | Loss: 0.00003310
Iteration 210/1000 | Loss: 0.00003310
Iteration 211/1000 | Loss: 0.00003310
Iteration 212/1000 | Loss: 0.00003310
Iteration 213/1000 | Loss: 0.00003310
Iteration 214/1000 | Loss: 0.00003310
Iteration 215/1000 | Loss: 0.00003310
Iteration 216/1000 | Loss: 0.00003310
Iteration 217/1000 | Loss: 0.00003310
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [3.309805106255226e-05, 3.309805106255226e-05, 3.309805106255226e-05, 3.309805106255226e-05, 3.309805106255226e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.309805106255226e-05

Optimization complete. Final v2v error: 4.743507385253906 mm

Highest mean error: 5.8525495529174805 mm for frame 118

Lowest mean error: 3.963747978210449 mm for frame 18

Saving results

Total time: 60.34306335449219
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00754674
Iteration 2/25 | Loss: 0.00155395
Iteration 3/25 | Loss: 0.00137314
Iteration 4/25 | Loss: 0.00135877
Iteration 5/25 | Loss: 0.00135590
Iteration 6/25 | Loss: 0.00135590
Iteration 7/25 | Loss: 0.00135590
Iteration 8/25 | Loss: 0.00135590
Iteration 9/25 | Loss: 0.00135590
Iteration 10/25 | Loss: 0.00135590
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001355899847112596, 0.001355899847112596, 0.001355899847112596, 0.001355899847112596, 0.001355899847112596]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001355899847112596

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.13518715
Iteration 2/25 | Loss: 0.00089170
Iteration 3/25 | Loss: 0.00089169
Iteration 4/25 | Loss: 0.00089169
Iteration 5/25 | Loss: 0.00089169
Iteration 6/25 | Loss: 0.00089169
Iteration 7/25 | Loss: 0.00089169
Iteration 8/25 | Loss: 0.00089169
Iteration 9/25 | Loss: 0.00089169
Iteration 10/25 | Loss: 0.00089169
Iteration 11/25 | Loss: 0.00089169
Iteration 12/25 | Loss: 0.00089169
Iteration 13/25 | Loss: 0.00089169
Iteration 14/25 | Loss: 0.00089169
Iteration 15/25 | Loss: 0.00089169
Iteration 16/25 | Loss: 0.00089169
Iteration 17/25 | Loss: 0.00089169
Iteration 18/25 | Loss: 0.00089169
Iteration 19/25 | Loss: 0.00089169
Iteration 20/25 | Loss: 0.00089169
Iteration 21/25 | Loss: 0.00089169
Iteration 22/25 | Loss: 0.00089169
Iteration 23/25 | Loss: 0.00089169
Iteration 24/25 | Loss: 0.00089169
Iteration 25/25 | Loss: 0.00089169

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089169
Iteration 2/1000 | Loss: 0.00004603
Iteration 3/1000 | Loss: 0.00003088
Iteration 4/1000 | Loss: 0.00002625
Iteration 5/1000 | Loss: 0.00002450
Iteration 6/1000 | Loss: 0.00002342
Iteration 7/1000 | Loss: 0.00002258
Iteration 8/1000 | Loss: 0.00002205
Iteration 9/1000 | Loss: 0.00002163
Iteration 10/1000 | Loss: 0.00002140
Iteration 11/1000 | Loss: 0.00002110
Iteration 12/1000 | Loss: 0.00002086
Iteration 13/1000 | Loss: 0.00002067
Iteration 14/1000 | Loss: 0.00002066
Iteration 15/1000 | Loss: 0.00002063
Iteration 16/1000 | Loss: 0.00002056
Iteration 17/1000 | Loss: 0.00002048
Iteration 18/1000 | Loss: 0.00002042
Iteration 19/1000 | Loss: 0.00002040
Iteration 20/1000 | Loss: 0.00002033
Iteration 21/1000 | Loss: 0.00002031
Iteration 22/1000 | Loss: 0.00002030
Iteration 23/1000 | Loss: 0.00002030
Iteration 24/1000 | Loss: 0.00002029
Iteration 25/1000 | Loss: 0.00002029
Iteration 26/1000 | Loss: 0.00002029
Iteration 27/1000 | Loss: 0.00002026
Iteration 28/1000 | Loss: 0.00002025
Iteration 29/1000 | Loss: 0.00002023
Iteration 30/1000 | Loss: 0.00002022
Iteration 31/1000 | Loss: 0.00002021
Iteration 32/1000 | Loss: 0.00002021
Iteration 33/1000 | Loss: 0.00002021
Iteration 34/1000 | Loss: 0.00002019
Iteration 35/1000 | Loss: 0.00002019
Iteration 36/1000 | Loss: 0.00002019
Iteration 37/1000 | Loss: 0.00002018
Iteration 38/1000 | Loss: 0.00002017
Iteration 39/1000 | Loss: 0.00002017
Iteration 40/1000 | Loss: 0.00002017
Iteration 41/1000 | Loss: 0.00002016
Iteration 42/1000 | Loss: 0.00002016
Iteration 43/1000 | Loss: 0.00002015
Iteration 44/1000 | Loss: 0.00002015
Iteration 45/1000 | Loss: 0.00002015
Iteration 46/1000 | Loss: 0.00002014
Iteration 47/1000 | Loss: 0.00002014
Iteration 48/1000 | Loss: 0.00002014
Iteration 49/1000 | Loss: 0.00002013
Iteration 50/1000 | Loss: 0.00002013
Iteration 51/1000 | Loss: 0.00002013
Iteration 52/1000 | Loss: 0.00002012
Iteration 53/1000 | Loss: 0.00002012
Iteration 54/1000 | Loss: 0.00002011
Iteration 55/1000 | Loss: 0.00002011
Iteration 56/1000 | Loss: 0.00002011
Iteration 57/1000 | Loss: 0.00002011
Iteration 58/1000 | Loss: 0.00002010
Iteration 59/1000 | Loss: 0.00002010
Iteration 60/1000 | Loss: 0.00002010
Iteration 61/1000 | Loss: 0.00002009
Iteration 62/1000 | Loss: 0.00002009
Iteration 63/1000 | Loss: 0.00002009
Iteration 64/1000 | Loss: 0.00002009
Iteration 65/1000 | Loss: 0.00002008
Iteration 66/1000 | Loss: 0.00002008
Iteration 67/1000 | Loss: 0.00002008
Iteration 68/1000 | Loss: 0.00002007
Iteration 69/1000 | Loss: 0.00002007
Iteration 70/1000 | Loss: 0.00002007
Iteration 71/1000 | Loss: 0.00002007
Iteration 72/1000 | Loss: 0.00002007
Iteration 73/1000 | Loss: 0.00002006
Iteration 74/1000 | Loss: 0.00002006
Iteration 75/1000 | Loss: 0.00002006
Iteration 76/1000 | Loss: 0.00002006
Iteration 77/1000 | Loss: 0.00002006
Iteration 78/1000 | Loss: 0.00002005
Iteration 79/1000 | Loss: 0.00002005
Iteration 80/1000 | Loss: 0.00002005
Iteration 81/1000 | Loss: 0.00002005
Iteration 82/1000 | Loss: 0.00002005
Iteration 83/1000 | Loss: 0.00002005
Iteration 84/1000 | Loss: 0.00002004
Iteration 85/1000 | Loss: 0.00002004
Iteration 86/1000 | Loss: 0.00002004
Iteration 87/1000 | Loss: 0.00002004
Iteration 88/1000 | Loss: 0.00002004
Iteration 89/1000 | Loss: 0.00002003
Iteration 90/1000 | Loss: 0.00002003
Iteration 91/1000 | Loss: 0.00002003
Iteration 92/1000 | Loss: 0.00002003
Iteration 93/1000 | Loss: 0.00002003
Iteration 94/1000 | Loss: 0.00002003
Iteration 95/1000 | Loss: 0.00002003
Iteration 96/1000 | Loss: 0.00002003
Iteration 97/1000 | Loss: 0.00002003
Iteration 98/1000 | Loss: 0.00002003
Iteration 99/1000 | Loss: 0.00002002
Iteration 100/1000 | Loss: 0.00002002
Iteration 101/1000 | Loss: 0.00002002
Iteration 102/1000 | Loss: 0.00002002
Iteration 103/1000 | Loss: 0.00002002
Iteration 104/1000 | Loss: 0.00002002
Iteration 105/1000 | Loss: 0.00002002
Iteration 106/1000 | Loss: 0.00002002
Iteration 107/1000 | Loss: 0.00002002
Iteration 108/1000 | Loss: 0.00002002
Iteration 109/1000 | Loss: 0.00002001
Iteration 110/1000 | Loss: 0.00002001
Iteration 111/1000 | Loss: 0.00002001
Iteration 112/1000 | Loss: 0.00002001
Iteration 113/1000 | Loss: 0.00002001
Iteration 114/1000 | Loss: 0.00002001
Iteration 115/1000 | Loss: 0.00002001
Iteration 116/1000 | Loss: 0.00002001
Iteration 117/1000 | Loss: 0.00002001
Iteration 118/1000 | Loss: 0.00002001
Iteration 119/1000 | Loss: 0.00002001
Iteration 120/1000 | Loss: 0.00002001
Iteration 121/1000 | Loss: 0.00002001
Iteration 122/1000 | Loss: 0.00002001
Iteration 123/1000 | Loss: 0.00002001
Iteration 124/1000 | Loss: 0.00002001
Iteration 125/1000 | Loss: 0.00002001
Iteration 126/1000 | Loss: 0.00002001
Iteration 127/1000 | Loss: 0.00002001
Iteration 128/1000 | Loss: 0.00002001
Iteration 129/1000 | Loss: 0.00002001
Iteration 130/1000 | Loss: 0.00002001
Iteration 131/1000 | Loss: 0.00002001
Iteration 132/1000 | Loss: 0.00002001
Iteration 133/1000 | Loss: 0.00002001
Iteration 134/1000 | Loss: 0.00002001
Iteration 135/1000 | Loss: 0.00002001
Iteration 136/1000 | Loss: 0.00002001
Iteration 137/1000 | Loss: 0.00002001
Iteration 138/1000 | Loss: 0.00002001
Iteration 139/1000 | Loss: 0.00002001
Iteration 140/1000 | Loss: 0.00002001
Iteration 141/1000 | Loss: 0.00002001
Iteration 142/1000 | Loss: 0.00002001
Iteration 143/1000 | Loss: 0.00002001
Iteration 144/1000 | Loss: 0.00002001
Iteration 145/1000 | Loss: 0.00002001
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [2.00124122784473e-05, 2.00124122784473e-05, 2.00124122784473e-05, 2.00124122784473e-05, 2.00124122784473e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.00124122784473e-05

Optimization complete. Final v2v error: 3.6953201293945312 mm

Highest mean error: 5.161664962768555 mm for frame 161

Lowest mean error: 3.0500168800354004 mm for frame 189

Saving results

Total time: 42.00548815727234
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00526125
Iteration 2/25 | Loss: 0.00133813
Iteration 3/25 | Loss: 0.00128607
Iteration 4/25 | Loss: 0.00127999
Iteration 5/25 | Loss: 0.00127842
Iteration 6/25 | Loss: 0.00127842
Iteration 7/25 | Loss: 0.00127842
Iteration 8/25 | Loss: 0.00127842
Iteration 9/25 | Loss: 0.00127842
Iteration 10/25 | Loss: 0.00127842
Iteration 11/25 | Loss: 0.00127842
Iteration 12/25 | Loss: 0.00127842
Iteration 13/25 | Loss: 0.00127842
Iteration 14/25 | Loss: 0.00127842
Iteration 15/25 | Loss: 0.00127842
Iteration 16/25 | Loss: 0.00127842
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001278419396840036, 0.001278419396840036, 0.001278419396840036, 0.001278419396840036, 0.001278419396840036]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001278419396840036

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 18.71850014
Iteration 2/25 | Loss: 0.00087174
Iteration 3/25 | Loss: 0.00087166
Iteration 4/25 | Loss: 0.00087166
Iteration 5/25 | Loss: 0.00087166
Iteration 6/25 | Loss: 0.00087166
Iteration 7/25 | Loss: 0.00087166
Iteration 8/25 | Loss: 0.00087166
Iteration 9/25 | Loss: 0.00087166
Iteration 10/25 | Loss: 0.00087166
Iteration 11/25 | Loss: 0.00087166
Iteration 12/25 | Loss: 0.00087166
Iteration 13/25 | Loss: 0.00087166
Iteration 14/25 | Loss: 0.00087166
Iteration 15/25 | Loss: 0.00087166
Iteration 16/25 | Loss: 0.00087166
Iteration 17/25 | Loss: 0.00087166
Iteration 18/25 | Loss: 0.00087166
Iteration 19/25 | Loss: 0.00087166
Iteration 20/25 | Loss: 0.00087166
Iteration 21/25 | Loss: 0.00087166
Iteration 22/25 | Loss: 0.00087166
Iteration 23/25 | Loss: 0.00087166
Iteration 24/25 | Loss: 0.00087166
Iteration 25/25 | Loss: 0.00087166

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087166
Iteration 2/1000 | Loss: 0.00002820
Iteration 3/1000 | Loss: 0.00001827
Iteration 4/1000 | Loss: 0.00001668
Iteration 5/1000 | Loss: 0.00001591
Iteration 6/1000 | Loss: 0.00001530
Iteration 7/1000 | Loss: 0.00001496
Iteration 8/1000 | Loss: 0.00001462
Iteration 9/1000 | Loss: 0.00001438
Iteration 10/1000 | Loss: 0.00001425
Iteration 11/1000 | Loss: 0.00001415
Iteration 12/1000 | Loss: 0.00001397
Iteration 13/1000 | Loss: 0.00001383
Iteration 14/1000 | Loss: 0.00001382
Iteration 15/1000 | Loss: 0.00001374
Iteration 16/1000 | Loss: 0.00001374
Iteration 17/1000 | Loss: 0.00001372
Iteration 18/1000 | Loss: 0.00001371
Iteration 19/1000 | Loss: 0.00001371
Iteration 20/1000 | Loss: 0.00001370
Iteration 21/1000 | Loss: 0.00001370
Iteration 22/1000 | Loss: 0.00001367
Iteration 23/1000 | Loss: 0.00001363
Iteration 24/1000 | Loss: 0.00001359
Iteration 25/1000 | Loss: 0.00001359
Iteration 26/1000 | Loss: 0.00001358
Iteration 27/1000 | Loss: 0.00001358
Iteration 28/1000 | Loss: 0.00001350
Iteration 29/1000 | Loss: 0.00001346
Iteration 30/1000 | Loss: 0.00001346
Iteration 31/1000 | Loss: 0.00001346
Iteration 32/1000 | Loss: 0.00001346
Iteration 33/1000 | Loss: 0.00001346
Iteration 34/1000 | Loss: 0.00001346
Iteration 35/1000 | Loss: 0.00001346
Iteration 36/1000 | Loss: 0.00001346
Iteration 37/1000 | Loss: 0.00001346
Iteration 38/1000 | Loss: 0.00001346
Iteration 39/1000 | Loss: 0.00001346
Iteration 40/1000 | Loss: 0.00001346
Iteration 41/1000 | Loss: 0.00001346
Iteration 42/1000 | Loss: 0.00001346
Iteration 43/1000 | Loss: 0.00001346
Iteration 44/1000 | Loss: 0.00001346
Iteration 45/1000 | Loss: 0.00001346
Iteration 46/1000 | Loss: 0.00001346
Iteration 47/1000 | Loss: 0.00001346
Iteration 48/1000 | Loss: 0.00001346
Iteration 49/1000 | Loss: 0.00001346
Iteration 50/1000 | Loss: 0.00001346
Iteration 51/1000 | Loss: 0.00001346
Iteration 52/1000 | Loss: 0.00001346
Iteration 53/1000 | Loss: 0.00001346
Iteration 54/1000 | Loss: 0.00001346
Iteration 55/1000 | Loss: 0.00001346
Iteration 56/1000 | Loss: 0.00001346
Iteration 57/1000 | Loss: 0.00001346
Iteration 58/1000 | Loss: 0.00001346
Iteration 59/1000 | Loss: 0.00001346
Iteration 60/1000 | Loss: 0.00001346
Iteration 61/1000 | Loss: 0.00001346
Iteration 62/1000 | Loss: 0.00001346
Iteration 63/1000 | Loss: 0.00001346
Iteration 64/1000 | Loss: 0.00001346
Iteration 65/1000 | Loss: 0.00001346
Iteration 66/1000 | Loss: 0.00001346
Iteration 67/1000 | Loss: 0.00001346
Iteration 68/1000 | Loss: 0.00001346
Iteration 69/1000 | Loss: 0.00001346
Iteration 70/1000 | Loss: 0.00001346
Iteration 71/1000 | Loss: 0.00001346
Iteration 72/1000 | Loss: 0.00001346
Iteration 73/1000 | Loss: 0.00001346
Iteration 74/1000 | Loss: 0.00001346
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 74. Stopping optimization.
Last 5 losses: [1.3457718523568474e-05, 1.3457718523568474e-05, 1.3457718523568474e-05, 1.3457718523568474e-05, 1.3457718523568474e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3457718523568474e-05

Optimization complete. Final v2v error: 3.132969617843628 mm

Highest mean error: 3.4504806995391846 mm for frame 137

Lowest mean error: 2.8615407943725586 mm for frame 1

Saving results

Total time: 34.321781396865845
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00667170
Iteration 2/25 | Loss: 0.00152970
Iteration 3/25 | Loss: 0.00140365
Iteration 4/25 | Loss: 0.00139366
Iteration 5/25 | Loss: 0.00139212
Iteration 6/25 | Loss: 0.00139212
Iteration 7/25 | Loss: 0.00139212
Iteration 8/25 | Loss: 0.00139212
Iteration 9/25 | Loss: 0.00139212
Iteration 10/25 | Loss: 0.00139212
Iteration 11/25 | Loss: 0.00139212
Iteration 12/25 | Loss: 0.00139212
Iteration 13/25 | Loss: 0.00139212
Iteration 14/25 | Loss: 0.00139212
Iteration 15/25 | Loss: 0.00139212
Iteration 16/25 | Loss: 0.00139212
Iteration 17/25 | Loss: 0.00139212
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013921178178861737, 0.0013921178178861737, 0.0013921178178861737, 0.0013921178178861737, 0.0013921178178861737]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013921178178861737

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.74298406
Iteration 2/25 | Loss: 0.00096608
Iteration 3/25 | Loss: 0.00096605
Iteration 4/25 | Loss: 0.00096605
Iteration 5/25 | Loss: 0.00096605
Iteration 6/25 | Loss: 0.00096605
Iteration 7/25 | Loss: 0.00096605
Iteration 8/25 | Loss: 0.00096605
Iteration 9/25 | Loss: 0.00096605
Iteration 10/25 | Loss: 0.00096605
Iteration 11/25 | Loss: 0.00096605
Iteration 12/25 | Loss: 0.00096605
Iteration 13/25 | Loss: 0.00096605
Iteration 14/25 | Loss: 0.00096605
Iteration 15/25 | Loss: 0.00096605
Iteration 16/25 | Loss: 0.00096605
Iteration 17/25 | Loss: 0.00096605
Iteration 18/25 | Loss: 0.00096605
Iteration 19/25 | Loss: 0.00096605
Iteration 20/25 | Loss: 0.00096605
Iteration 21/25 | Loss: 0.00096605
Iteration 22/25 | Loss: 0.00096605
Iteration 23/25 | Loss: 0.00096605
Iteration 24/25 | Loss: 0.00096605
Iteration 25/25 | Loss: 0.00096605

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096605
Iteration 2/1000 | Loss: 0.00003726
Iteration 3/1000 | Loss: 0.00002707
Iteration 4/1000 | Loss: 0.00002487
Iteration 5/1000 | Loss: 0.00002363
Iteration 6/1000 | Loss: 0.00002300
Iteration 7/1000 | Loss: 0.00002240
Iteration 8/1000 | Loss: 0.00002203
Iteration 9/1000 | Loss: 0.00002168
Iteration 10/1000 | Loss: 0.00002139
Iteration 11/1000 | Loss: 0.00002118
Iteration 12/1000 | Loss: 0.00002101
Iteration 13/1000 | Loss: 0.00002098
Iteration 14/1000 | Loss: 0.00002091
Iteration 15/1000 | Loss: 0.00002087
Iteration 16/1000 | Loss: 0.00002086
Iteration 17/1000 | Loss: 0.00002085
Iteration 18/1000 | Loss: 0.00002077
Iteration 19/1000 | Loss: 0.00002073
Iteration 20/1000 | Loss: 0.00002072
Iteration 21/1000 | Loss: 0.00002061
Iteration 22/1000 | Loss: 0.00002056
Iteration 23/1000 | Loss: 0.00002056
Iteration 24/1000 | Loss: 0.00002055
Iteration 25/1000 | Loss: 0.00002055
Iteration 26/1000 | Loss: 0.00002052
Iteration 27/1000 | Loss: 0.00002051
Iteration 28/1000 | Loss: 0.00002051
Iteration 29/1000 | Loss: 0.00002050
Iteration 30/1000 | Loss: 0.00002049
Iteration 31/1000 | Loss: 0.00002049
Iteration 32/1000 | Loss: 0.00002045
Iteration 33/1000 | Loss: 0.00002044
Iteration 34/1000 | Loss: 0.00002042
Iteration 35/1000 | Loss: 0.00002041
Iteration 36/1000 | Loss: 0.00002041
Iteration 37/1000 | Loss: 0.00002041
Iteration 38/1000 | Loss: 0.00002041
Iteration 39/1000 | Loss: 0.00002041
Iteration 40/1000 | Loss: 0.00002041
Iteration 41/1000 | Loss: 0.00002041
Iteration 42/1000 | Loss: 0.00002040
Iteration 43/1000 | Loss: 0.00002040
Iteration 44/1000 | Loss: 0.00002040
Iteration 45/1000 | Loss: 0.00002039
Iteration 46/1000 | Loss: 0.00002039
Iteration 47/1000 | Loss: 0.00002038
Iteration 48/1000 | Loss: 0.00002038
Iteration 49/1000 | Loss: 0.00002037
Iteration 50/1000 | Loss: 0.00002037
Iteration 51/1000 | Loss: 0.00002036
Iteration 52/1000 | Loss: 0.00002035
Iteration 53/1000 | Loss: 0.00002034
Iteration 54/1000 | Loss: 0.00002034
Iteration 55/1000 | Loss: 0.00002034
Iteration 56/1000 | Loss: 0.00002034
Iteration 57/1000 | Loss: 0.00002034
Iteration 58/1000 | Loss: 0.00002034
Iteration 59/1000 | Loss: 0.00002033
Iteration 60/1000 | Loss: 0.00002032
Iteration 61/1000 | Loss: 0.00002032
Iteration 62/1000 | Loss: 0.00002031
Iteration 63/1000 | Loss: 0.00002031
Iteration 64/1000 | Loss: 0.00002030
Iteration 65/1000 | Loss: 0.00002030
Iteration 66/1000 | Loss: 0.00002029
Iteration 67/1000 | Loss: 0.00002029
Iteration 68/1000 | Loss: 0.00002029
Iteration 69/1000 | Loss: 0.00002029
Iteration 70/1000 | Loss: 0.00002029
Iteration 71/1000 | Loss: 0.00002029
Iteration 72/1000 | Loss: 0.00002029
Iteration 73/1000 | Loss: 0.00002029
Iteration 74/1000 | Loss: 0.00002029
Iteration 75/1000 | Loss: 0.00002029
Iteration 76/1000 | Loss: 0.00002028
Iteration 77/1000 | Loss: 0.00002028
Iteration 78/1000 | Loss: 0.00002027
Iteration 79/1000 | Loss: 0.00002027
Iteration 80/1000 | Loss: 0.00002026
Iteration 81/1000 | Loss: 0.00002026
Iteration 82/1000 | Loss: 0.00002026
Iteration 83/1000 | Loss: 0.00002026
Iteration 84/1000 | Loss: 0.00002026
Iteration 85/1000 | Loss: 0.00002026
Iteration 86/1000 | Loss: 0.00002025
Iteration 87/1000 | Loss: 0.00002025
Iteration 88/1000 | Loss: 0.00002024
Iteration 89/1000 | Loss: 0.00002024
Iteration 90/1000 | Loss: 0.00002024
Iteration 91/1000 | Loss: 0.00002024
Iteration 92/1000 | Loss: 0.00002024
Iteration 93/1000 | Loss: 0.00002023
Iteration 94/1000 | Loss: 0.00002023
Iteration 95/1000 | Loss: 0.00002023
Iteration 96/1000 | Loss: 0.00002023
Iteration 97/1000 | Loss: 0.00002023
Iteration 98/1000 | Loss: 0.00002023
Iteration 99/1000 | Loss: 0.00002022
Iteration 100/1000 | Loss: 0.00002022
Iteration 101/1000 | Loss: 0.00002022
Iteration 102/1000 | Loss: 0.00002022
Iteration 103/1000 | Loss: 0.00002022
Iteration 104/1000 | Loss: 0.00002021
Iteration 105/1000 | Loss: 0.00002021
Iteration 106/1000 | Loss: 0.00002020
Iteration 107/1000 | Loss: 0.00002020
Iteration 108/1000 | Loss: 0.00002020
Iteration 109/1000 | Loss: 0.00002020
Iteration 110/1000 | Loss: 0.00002020
Iteration 111/1000 | Loss: 0.00002020
Iteration 112/1000 | Loss: 0.00002020
Iteration 113/1000 | Loss: 0.00002019
Iteration 114/1000 | Loss: 0.00002019
Iteration 115/1000 | Loss: 0.00002019
Iteration 116/1000 | Loss: 0.00002019
Iteration 117/1000 | Loss: 0.00002018
Iteration 118/1000 | Loss: 0.00002018
Iteration 119/1000 | Loss: 0.00002018
Iteration 120/1000 | Loss: 0.00002018
Iteration 121/1000 | Loss: 0.00002018
Iteration 122/1000 | Loss: 0.00002018
Iteration 123/1000 | Loss: 0.00002017
Iteration 124/1000 | Loss: 0.00002017
Iteration 125/1000 | Loss: 0.00002017
Iteration 126/1000 | Loss: 0.00002017
Iteration 127/1000 | Loss: 0.00002017
Iteration 128/1000 | Loss: 0.00002016
Iteration 129/1000 | Loss: 0.00002016
Iteration 130/1000 | Loss: 0.00002016
Iteration 131/1000 | Loss: 0.00002016
Iteration 132/1000 | Loss: 0.00002015
Iteration 133/1000 | Loss: 0.00002015
Iteration 134/1000 | Loss: 0.00002015
Iteration 135/1000 | Loss: 0.00002015
Iteration 136/1000 | Loss: 0.00002015
Iteration 137/1000 | Loss: 0.00002014
Iteration 138/1000 | Loss: 0.00002014
Iteration 139/1000 | Loss: 0.00002013
Iteration 140/1000 | Loss: 0.00002013
Iteration 141/1000 | Loss: 0.00002013
Iteration 142/1000 | Loss: 0.00002013
Iteration 143/1000 | Loss: 0.00002012
Iteration 144/1000 | Loss: 0.00002012
Iteration 145/1000 | Loss: 0.00002012
Iteration 146/1000 | Loss: 0.00002011
Iteration 147/1000 | Loss: 0.00002011
Iteration 148/1000 | Loss: 0.00002011
Iteration 149/1000 | Loss: 0.00002011
Iteration 150/1000 | Loss: 0.00002011
Iteration 151/1000 | Loss: 0.00002011
Iteration 152/1000 | Loss: 0.00002011
Iteration 153/1000 | Loss: 0.00002011
Iteration 154/1000 | Loss: 0.00002010
Iteration 155/1000 | Loss: 0.00002010
Iteration 156/1000 | Loss: 0.00002010
Iteration 157/1000 | Loss: 0.00002010
Iteration 158/1000 | Loss: 0.00002009
Iteration 159/1000 | Loss: 0.00002009
Iteration 160/1000 | Loss: 0.00002009
Iteration 161/1000 | Loss: 0.00002009
Iteration 162/1000 | Loss: 0.00002009
Iteration 163/1000 | Loss: 0.00002009
Iteration 164/1000 | Loss: 0.00002009
Iteration 165/1000 | Loss: 0.00002009
Iteration 166/1000 | Loss: 0.00002009
Iteration 167/1000 | Loss: 0.00002009
Iteration 168/1000 | Loss: 0.00002009
Iteration 169/1000 | Loss: 0.00002009
Iteration 170/1000 | Loss: 0.00002009
Iteration 171/1000 | Loss: 0.00002009
Iteration 172/1000 | Loss: 0.00002009
Iteration 173/1000 | Loss: 0.00002009
Iteration 174/1000 | Loss: 0.00002008
Iteration 175/1000 | Loss: 0.00002008
Iteration 176/1000 | Loss: 0.00002008
Iteration 177/1000 | Loss: 0.00002008
Iteration 178/1000 | Loss: 0.00002008
Iteration 179/1000 | Loss: 0.00002008
Iteration 180/1000 | Loss: 0.00002008
Iteration 181/1000 | Loss: 0.00002008
Iteration 182/1000 | Loss: 0.00002007
Iteration 183/1000 | Loss: 0.00002007
Iteration 184/1000 | Loss: 0.00002007
Iteration 185/1000 | Loss: 0.00002007
Iteration 186/1000 | Loss: 0.00002007
Iteration 187/1000 | Loss: 0.00002007
Iteration 188/1000 | Loss: 0.00002007
Iteration 189/1000 | Loss: 0.00002007
Iteration 190/1000 | Loss: 0.00002007
Iteration 191/1000 | Loss: 0.00002007
Iteration 192/1000 | Loss: 0.00002007
Iteration 193/1000 | Loss: 0.00002007
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [2.0069310266990215e-05, 2.0069310266990215e-05, 2.0069310266990215e-05, 2.0069310266990215e-05, 2.0069310266990215e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0069310266990215e-05

Optimization complete. Final v2v error: 3.6764490604400635 mm

Highest mean error: 4.55155086517334 mm for frame 176

Lowest mean error: 3.120683193206787 mm for frame 62

Saving results

Total time: 45.62445878982544
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00484042
Iteration 2/25 | Loss: 0.00137484
Iteration 3/25 | Loss: 0.00130907
Iteration 4/25 | Loss: 0.00129818
Iteration 5/25 | Loss: 0.00129520
Iteration 6/25 | Loss: 0.00129520
Iteration 7/25 | Loss: 0.00129520
Iteration 8/25 | Loss: 0.00129520
Iteration 9/25 | Loss: 0.00129520
Iteration 10/25 | Loss: 0.00129520
Iteration 11/25 | Loss: 0.00129520
Iteration 12/25 | Loss: 0.00129520
Iteration 13/25 | Loss: 0.00129520
Iteration 14/25 | Loss: 0.00129520
Iteration 15/25 | Loss: 0.00129520
Iteration 16/25 | Loss: 0.00129520
Iteration 17/25 | Loss: 0.00129520
Iteration 18/25 | Loss: 0.00129520
Iteration 19/25 | Loss: 0.00129520
Iteration 20/25 | Loss: 0.00129520
Iteration 21/25 | Loss: 0.00129520
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012951998505741358, 0.0012951998505741358, 0.0012951998505741358, 0.0012951998505741358, 0.0012951998505741358]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012951998505741358

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45472026
Iteration 2/25 | Loss: 0.00091939
Iteration 3/25 | Loss: 0.00091939
Iteration 4/25 | Loss: 0.00091939
Iteration 5/25 | Loss: 0.00091939
Iteration 6/25 | Loss: 0.00091939
Iteration 7/25 | Loss: 0.00091939
Iteration 8/25 | Loss: 0.00091939
Iteration 9/25 | Loss: 0.00091939
Iteration 10/25 | Loss: 0.00091939
Iteration 11/25 | Loss: 0.00091939
Iteration 12/25 | Loss: 0.00091939
Iteration 13/25 | Loss: 0.00091939
Iteration 14/25 | Loss: 0.00091939
Iteration 15/25 | Loss: 0.00091939
Iteration 16/25 | Loss: 0.00091939
Iteration 17/25 | Loss: 0.00091939
Iteration 18/25 | Loss: 0.00091939
Iteration 19/25 | Loss: 0.00091939
Iteration 20/25 | Loss: 0.00091939
Iteration 21/25 | Loss: 0.00091939
Iteration 22/25 | Loss: 0.00091939
Iteration 23/25 | Loss: 0.00091939
Iteration 24/25 | Loss: 0.00091939
Iteration 25/25 | Loss: 0.00091939

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091939
Iteration 2/1000 | Loss: 0.00002731
Iteration 3/1000 | Loss: 0.00002122
Iteration 4/1000 | Loss: 0.00001987
Iteration 5/1000 | Loss: 0.00001894
Iteration 6/1000 | Loss: 0.00001849
Iteration 7/1000 | Loss: 0.00001823
Iteration 8/1000 | Loss: 0.00001793
Iteration 9/1000 | Loss: 0.00001767
Iteration 10/1000 | Loss: 0.00001766
Iteration 11/1000 | Loss: 0.00001762
Iteration 12/1000 | Loss: 0.00001750
Iteration 13/1000 | Loss: 0.00001739
Iteration 14/1000 | Loss: 0.00001736
Iteration 15/1000 | Loss: 0.00001736
Iteration 16/1000 | Loss: 0.00001736
Iteration 17/1000 | Loss: 0.00001734
Iteration 18/1000 | Loss: 0.00001722
Iteration 19/1000 | Loss: 0.00001718
Iteration 20/1000 | Loss: 0.00001705
Iteration 21/1000 | Loss: 0.00001701
Iteration 22/1000 | Loss: 0.00001701
Iteration 23/1000 | Loss: 0.00001699
Iteration 24/1000 | Loss: 0.00001699
Iteration 25/1000 | Loss: 0.00001698
Iteration 26/1000 | Loss: 0.00001697
Iteration 27/1000 | Loss: 0.00001693
Iteration 28/1000 | Loss: 0.00001693
Iteration 29/1000 | Loss: 0.00001692
Iteration 30/1000 | Loss: 0.00001692
Iteration 31/1000 | Loss: 0.00001691
Iteration 32/1000 | Loss: 0.00001690
Iteration 33/1000 | Loss: 0.00001687
Iteration 34/1000 | Loss: 0.00001686
Iteration 35/1000 | Loss: 0.00001686
Iteration 36/1000 | Loss: 0.00001685
Iteration 37/1000 | Loss: 0.00001684
Iteration 38/1000 | Loss: 0.00001683
Iteration 39/1000 | Loss: 0.00001680
Iteration 40/1000 | Loss: 0.00001680
Iteration 41/1000 | Loss: 0.00001680
Iteration 42/1000 | Loss: 0.00001679
Iteration 43/1000 | Loss: 0.00001676
Iteration 44/1000 | Loss: 0.00001676
Iteration 45/1000 | Loss: 0.00001676
Iteration 46/1000 | Loss: 0.00001675
Iteration 47/1000 | Loss: 0.00001675
Iteration 48/1000 | Loss: 0.00001674
Iteration 49/1000 | Loss: 0.00001674
Iteration 50/1000 | Loss: 0.00001673
Iteration 51/1000 | Loss: 0.00001673
Iteration 52/1000 | Loss: 0.00001672
Iteration 53/1000 | Loss: 0.00001671
Iteration 54/1000 | Loss: 0.00001671
Iteration 55/1000 | Loss: 0.00001671
Iteration 56/1000 | Loss: 0.00001670
Iteration 57/1000 | Loss: 0.00001669
Iteration 58/1000 | Loss: 0.00001668
Iteration 59/1000 | Loss: 0.00001668
Iteration 60/1000 | Loss: 0.00001667
Iteration 61/1000 | Loss: 0.00001667
Iteration 62/1000 | Loss: 0.00001667
Iteration 63/1000 | Loss: 0.00001667
Iteration 64/1000 | Loss: 0.00001667
Iteration 65/1000 | Loss: 0.00001666
Iteration 66/1000 | Loss: 0.00001666
Iteration 67/1000 | Loss: 0.00001665
Iteration 68/1000 | Loss: 0.00001665
Iteration 69/1000 | Loss: 0.00001665
Iteration 70/1000 | Loss: 0.00001665
Iteration 71/1000 | Loss: 0.00001665
Iteration 72/1000 | Loss: 0.00001665
Iteration 73/1000 | Loss: 0.00001664
Iteration 74/1000 | Loss: 0.00001664
Iteration 75/1000 | Loss: 0.00001664
Iteration 76/1000 | Loss: 0.00001663
Iteration 77/1000 | Loss: 0.00001663
Iteration 78/1000 | Loss: 0.00001662
Iteration 79/1000 | Loss: 0.00001662
Iteration 80/1000 | Loss: 0.00001662
Iteration 81/1000 | Loss: 0.00001662
Iteration 82/1000 | Loss: 0.00001661
Iteration 83/1000 | Loss: 0.00001661
Iteration 84/1000 | Loss: 0.00001660
Iteration 85/1000 | Loss: 0.00001660
Iteration 86/1000 | Loss: 0.00001660
Iteration 87/1000 | Loss: 0.00001660
Iteration 88/1000 | Loss: 0.00001660
Iteration 89/1000 | Loss: 0.00001660
Iteration 90/1000 | Loss: 0.00001660
Iteration 91/1000 | Loss: 0.00001658
Iteration 92/1000 | Loss: 0.00001657
Iteration 93/1000 | Loss: 0.00001657
Iteration 94/1000 | Loss: 0.00001657
Iteration 95/1000 | Loss: 0.00001657
Iteration 96/1000 | Loss: 0.00001657
Iteration 97/1000 | Loss: 0.00001656
Iteration 98/1000 | Loss: 0.00001656
Iteration 99/1000 | Loss: 0.00001656
Iteration 100/1000 | Loss: 0.00001656
Iteration 101/1000 | Loss: 0.00001656
Iteration 102/1000 | Loss: 0.00001656
Iteration 103/1000 | Loss: 0.00001656
Iteration 104/1000 | Loss: 0.00001656
Iteration 105/1000 | Loss: 0.00001656
Iteration 106/1000 | Loss: 0.00001656
Iteration 107/1000 | Loss: 0.00001655
Iteration 108/1000 | Loss: 0.00001655
Iteration 109/1000 | Loss: 0.00001655
Iteration 110/1000 | Loss: 0.00001655
Iteration 111/1000 | Loss: 0.00001654
Iteration 112/1000 | Loss: 0.00001654
Iteration 113/1000 | Loss: 0.00001654
Iteration 114/1000 | Loss: 0.00001654
Iteration 115/1000 | Loss: 0.00001654
Iteration 116/1000 | Loss: 0.00001654
Iteration 117/1000 | Loss: 0.00001654
Iteration 118/1000 | Loss: 0.00001654
Iteration 119/1000 | Loss: 0.00001654
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.6543966921744868e-05, 1.6543966921744868e-05, 1.6543966921744868e-05, 1.6543966921744868e-05, 1.6543966921744868e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6543966921744868e-05

Optimization complete. Final v2v error: 3.3797552585601807 mm

Highest mean error: 3.5726377964019775 mm for frame 193

Lowest mean error: 3.1567909717559814 mm for frame 15

Saving results

Total time: 41.495550870895386
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00533631
Iteration 2/25 | Loss: 0.00148496
Iteration 3/25 | Loss: 0.00139599
Iteration 4/25 | Loss: 0.00138606
Iteration 5/25 | Loss: 0.00138288
Iteration 6/25 | Loss: 0.00138288
Iteration 7/25 | Loss: 0.00138288
Iteration 8/25 | Loss: 0.00138288
Iteration 9/25 | Loss: 0.00138288
Iteration 10/25 | Loss: 0.00138288
Iteration 11/25 | Loss: 0.00138288
Iteration 12/25 | Loss: 0.00138288
Iteration 13/25 | Loss: 0.00138288
Iteration 14/25 | Loss: 0.00138288
Iteration 15/25 | Loss: 0.00138288
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0013828841038048267, 0.0013828841038048267, 0.0013828841038048267, 0.0013828841038048267, 0.0013828841038048267]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013828841038048267

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.28096867
Iteration 2/25 | Loss: 0.00088108
Iteration 3/25 | Loss: 0.00088108
Iteration 4/25 | Loss: 0.00088108
Iteration 5/25 | Loss: 0.00088108
Iteration 6/25 | Loss: 0.00088108
Iteration 7/25 | Loss: 0.00088108
Iteration 8/25 | Loss: 0.00088108
Iteration 9/25 | Loss: 0.00088108
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.0008810819126665592, 0.0008810819126665592, 0.0008810819126665592, 0.0008810819126665592, 0.0008810819126665592]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008810819126665592

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088108
Iteration 2/1000 | Loss: 0.00005250
Iteration 3/1000 | Loss: 0.00002943
Iteration 4/1000 | Loss: 0.00002594
Iteration 5/1000 | Loss: 0.00002459
Iteration 6/1000 | Loss: 0.00002345
Iteration 7/1000 | Loss: 0.00002283
Iteration 8/1000 | Loss: 0.00002245
Iteration 9/1000 | Loss: 0.00002204
Iteration 10/1000 | Loss: 0.00002171
Iteration 11/1000 | Loss: 0.00002146
Iteration 12/1000 | Loss: 0.00002131
Iteration 13/1000 | Loss: 0.00002115
Iteration 14/1000 | Loss: 0.00002108
Iteration 15/1000 | Loss: 0.00002103
Iteration 16/1000 | Loss: 0.00002099
Iteration 17/1000 | Loss: 0.00002098
Iteration 18/1000 | Loss: 0.00002096
Iteration 19/1000 | Loss: 0.00002092
Iteration 20/1000 | Loss: 0.00002091
Iteration 21/1000 | Loss: 0.00002087
Iteration 22/1000 | Loss: 0.00002086
Iteration 23/1000 | Loss: 0.00002082
Iteration 24/1000 | Loss: 0.00002082
Iteration 25/1000 | Loss: 0.00002082
Iteration 26/1000 | Loss: 0.00002082
Iteration 27/1000 | Loss: 0.00002081
Iteration 28/1000 | Loss: 0.00002081
Iteration 29/1000 | Loss: 0.00002081
Iteration 30/1000 | Loss: 0.00002081
Iteration 31/1000 | Loss: 0.00002081
Iteration 32/1000 | Loss: 0.00002081
Iteration 33/1000 | Loss: 0.00002081
Iteration 34/1000 | Loss: 0.00002081
Iteration 35/1000 | Loss: 0.00002080
Iteration 36/1000 | Loss: 0.00002080
Iteration 37/1000 | Loss: 0.00002078
Iteration 38/1000 | Loss: 0.00002078
Iteration 39/1000 | Loss: 0.00002078
Iteration 40/1000 | Loss: 0.00002077
Iteration 41/1000 | Loss: 0.00002077
Iteration 42/1000 | Loss: 0.00002077
Iteration 43/1000 | Loss: 0.00002077
Iteration 44/1000 | Loss: 0.00002077
Iteration 45/1000 | Loss: 0.00002077
Iteration 46/1000 | Loss: 0.00002077
Iteration 47/1000 | Loss: 0.00002077
Iteration 48/1000 | Loss: 0.00002076
Iteration 49/1000 | Loss: 0.00002076
Iteration 50/1000 | Loss: 0.00002076
Iteration 51/1000 | Loss: 0.00002075
Iteration 52/1000 | Loss: 0.00002073
Iteration 53/1000 | Loss: 0.00002073
Iteration 54/1000 | Loss: 0.00002073
Iteration 55/1000 | Loss: 0.00002072
Iteration 56/1000 | Loss: 0.00002072
Iteration 57/1000 | Loss: 0.00002071
Iteration 58/1000 | Loss: 0.00002071
Iteration 59/1000 | Loss: 0.00002070
Iteration 60/1000 | Loss: 0.00002069
Iteration 61/1000 | Loss: 0.00002069
Iteration 62/1000 | Loss: 0.00002069
Iteration 63/1000 | Loss: 0.00002069
Iteration 64/1000 | Loss: 0.00002069
Iteration 65/1000 | Loss: 0.00002069
Iteration 66/1000 | Loss: 0.00002069
Iteration 67/1000 | Loss: 0.00002068
Iteration 68/1000 | Loss: 0.00002068
Iteration 69/1000 | Loss: 0.00002068
Iteration 70/1000 | Loss: 0.00002068
Iteration 71/1000 | Loss: 0.00002068
Iteration 72/1000 | Loss: 0.00002067
Iteration 73/1000 | Loss: 0.00002067
Iteration 74/1000 | Loss: 0.00002067
Iteration 75/1000 | Loss: 0.00002065
Iteration 76/1000 | Loss: 0.00002065
Iteration 77/1000 | Loss: 0.00002065
Iteration 78/1000 | Loss: 0.00002065
Iteration 79/1000 | Loss: 0.00002065
Iteration 80/1000 | Loss: 0.00002065
Iteration 81/1000 | Loss: 0.00002065
Iteration 82/1000 | Loss: 0.00002065
Iteration 83/1000 | Loss: 0.00002065
Iteration 84/1000 | Loss: 0.00002064
Iteration 85/1000 | Loss: 0.00002064
Iteration 86/1000 | Loss: 0.00002064
Iteration 87/1000 | Loss: 0.00002064
Iteration 88/1000 | Loss: 0.00002062
Iteration 89/1000 | Loss: 0.00002061
Iteration 90/1000 | Loss: 0.00002061
Iteration 91/1000 | Loss: 0.00002061
Iteration 92/1000 | Loss: 0.00002061
Iteration 93/1000 | Loss: 0.00002061
Iteration 94/1000 | Loss: 0.00002061
Iteration 95/1000 | Loss: 0.00002061
Iteration 96/1000 | Loss: 0.00002061
Iteration 97/1000 | Loss: 0.00002061
Iteration 98/1000 | Loss: 0.00002061
Iteration 99/1000 | Loss: 0.00002060
Iteration 100/1000 | Loss: 0.00002060
Iteration 101/1000 | Loss: 0.00002060
Iteration 102/1000 | Loss: 0.00002060
Iteration 103/1000 | Loss: 0.00002059
Iteration 104/1000 | Loss: 0.00002058
Iteration 105/1000 | Loss: 0.00002058
Iteration 106/1000 | Loss: 0.00002058
Iteration 107/1000 | Loss: 0.00002058
Iteration 108/1000 | Loss: 0.00002058
Iteration 109/1000 | Loss: 0.00002057
Iteration 110/1000 | Loss: 0.00002057
Iteration 111/1000 | Loss: 0.00002057
Iteration 112/1000 | Loss: 0.00002057
Iteration 113/1000 | Loss: 0.00002057
Iteration 114/1000 | Loss: 0.00002057
Iteration 115/1000 | Loss: 0.00002057
Iteration 116/1000 | Loss: 0.00002057
Iteration 117/1000 | Loss: 0.00002057
Iteration 118/1000 | Loss: 0.00002057
Iteration 119/1000 | Loss: 0.00002057
Iteration 120/1000 | Loss: 0.00002057
Iteration 121/1000 | Loss: 0.00002056
Iteration 122/1000 | Loss: 0.00002056
Iteration 123/1000 | Loss: 0.00002056
Iteration 124/1000 | Loss: 0.00002056
Iteration 125/1000 | Loss: 0.00002056
Iteration 126/1000 | Loss: 0.00002056
Iteration 127/1000 | Loss: 0.00002055
Iteration 128/1000 | Loss: 0.00002055
Iteration 129/1000 | Loss: 0.00002055
Iteration 130/1000 | Loss: 0.00002055
Iteration 131/1000 | Loss: 0.00002055
Iteration 132/1000 | Loss: 0.00002055
Iteration 133/1000 | Loss: 0.00002055
Iteration 134/1000 | Loss: 0.00002055
Iteration 135/1000 | Loss: 0.00002055
Iteration 136/1000 | Loss: 0.00002055
Iteration 137/1000 | Loss: 0.00002055
Iteration 138/1000 | Loss: 0.00002055
Iteration 139/1000 | Loss: 0.00002055
Iteration 140/1000 | Loss: 0.00002055
Iteration 141/1000 | Loss: 0.00002055
Iteration 142/1000 | Loss: 0.00002054
Iteration 143/1000 | Loss: 0.00002054
Iteration 144/1000 | Loss: 0.00002054
Iteration 145/1000 | Loss: 0.00002054
Iteration 146/1000 | Loss: 0.00002054
Iteration 147/1000 | Loss: 0.00002054
Iteration 148/1000 | Loss: 0.00002054
Iteration 149/1000 | Loss: 0.00002054
Iteration 150/1000 | Loss: 0.00002054
Iteration 151/1000 | Loss: 0.00002053
Iteration 152/1000 | Loss: 0.00002053
Iteration 153/1000 | Loss: 0.00002053
Iteration 154/1000 | Loss: 0.00002053
Iteration 155/1000 | Loss: 0.00002053
Iteration 156/1000 | Loss: 0.00002053
Iteration 157/1000 | Loss: 0.00002053
Iteration 158/1000 | Loss: 0.00002053
Iteration 159/1000 | Loss: 0.00002053
Iteration 160/1000 | Loss: 0.00002053
Iteration 161/1000 | Loss: 0.00002053
Iteration 162/1000 | Loss: 0.00002052
Iteration 163/1000 | Loss: 0.00002052
Iteration 164/1000 | Loss: 0.00002052
Iteration 165/1000 | Loss: 0.00002052
Iteration 166/1000 | Loss: 0.00002052
Iteration 167/1000 | Loss: 0.00002052
Iteration 168/1000 | Loss: 0.00002052
Iteration 169/1000 | Loss: 0.00002052
Iteration 170/1000 | Loss: 0.00002052
Iteration 171/1000 | Loss: 0.00002052
Iteration 172/1000 | Loss: 0.00002052
Iteration 173/1000 | Loss: 0.00002052
Iteration 174/1000 | Loss: 0.00002052
Iteration 175/1000 | Loss: 0.00002052
Iteration 176/1000 | Loss: 0.00002052
Iteration 177/1000 | Loss: 0.00002051
Iteration 178/1000 | Loss: 0.00002051
Iteration 179/1000 | Loss: 0.00002051
Iteration 180/1000 | Loss: 0.00002051
Iteration 181/1000 | Loss: 0.00002051
Iteration 182/1000 | Loss: 0.00002051
Iteration 183/1000 | Loss: 0.00002051
Iteration 184/1000 | Loss: 0.00002051
Iteration 185/1000 | Loss: 0.00002051
Iteration 186/1000 | Loss: 0.00002051
Iteration 187/1000 | Loss: 0.00002051
Iteration 188/1000 | Loss: 0.00002051
Iteration 189/1000 | Loss: 0.00002051
Iteration 190/1000 | Loss: 0.00002051
Iteration 191/1000 | Loss: 0.00002050
Iteration 192/1000 | Loss: 0.00002050
Iteration 193/1000 | Loss: 0.00002050
Iteration 194/1000 | Loss: 0.00002050
Iteration 195/1000 | Loss: 0.00002050
Iteration 196/1000 | Loss: 0.00002050
Iteration 197/1000 | Loss: 0.00002050
Iteration 198/1000 | Loss: 0.00002050
Iteration 199/1000 | Loss: 0.00002050
Iteration 200/1000 | Loss: 0.00002050
Iteration 201/1000 | Loss: 0.00002050
Iteration 202/1000 | Loss: 0.00002050
Iteration 203/1000 | Loss: 0.00002050
Iteration 204/1000 | Loss: 0.00002050
Iteration 205/1000 | Loss: 0.00002050
Iteration 206/1000 | Loss: 0.00002050
Iteration 207/1000 | Loss: 0.00002050
Iteration 208/1000 | Loss: 0.00002050
Iteration 209/1000 | Loss: 0.00002050
Iteration 210/1000 | Loss: 0.00002050
Iteration 211/1000 | Loss: 0.00002050
Iteration 212/1000 | Loss: 0.00002050
Iteration 213/1000 | Loss: 0.00002050
Iteration 214/1000 | Loss: 0.00002050
Iteration 215/1000 | Loss: 0.00002050
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [2.0502142433542758e-05, 2.0502142433542758e-05, 2.0502142433542758e-05, 2.0502142433542758e-05, 2.0502142433542758e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0502142433542758e-05

Optimization complete. Final v2v error: 3.791781425476074 mm

Highest mean error: 4.141380310058594 mm for frame 123

Lowest mean error: 3.555054187774658 mm for frame 150

Saving results

Total time: 46.92642164230347
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00805181
Iteration 2/25 | Loss: 0.00172401
Iteration 3/25 | Loss: 0.00146252
Iteration 4/25 | Loss: 0.00145097
Iteration 5/25 | Loss: 0.00145097
Iteration 6/25 | Loss: 0.00145097
Iteration 7/25 | Loss: 0.00145097
Iteration 8/25 | Loss: 0.00145097
Iteration 9/25 | Loss: 0.00145097
Iteration 10/25 | Loss: 0.00145097
Iteration 11/25 | Loss: 0.00145097
Iteration 12/25 | Loss: 0.00145097
Iteration 13/25 | Loss: 0.00145097
Iteration 14/25 | Loss: 0.00145097
Iteration 15/25 | Loss: 0.00145097
Iteration 16/25 | Loss: 0.00145097
Iteration 17/25 | Loss: 0.00145097
Iteration 18/25 | Loss: 0.00145097
Iteration 19/25 | Loss: 0.00145097
Iteration 20/25 | Loss: 0.00145097
Iteration 21/25 | Loss: 0.00145097
Iteration 22/25 | Loss: 0.00145097
Iteration 23/25 | Loss: 0.00145097
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0014509651809930801, 0.0014509651809930801, 0.0014509651809930801, 0.0014509651809930801, 0.0014509651809930801]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014509651809930801

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30428112
Iteration 2/25 | Loss: 0.00092903
Iteration 3/25 | Loss: 0.00092900
Iteration 4/25 | Loss: 0.00092900
Iteration 5/25 | Loss: 0.00092899
Iteration 6/25 | Loss: 0.00092899
Iteration 7/25 | Loss: 0.00092899
Iteration 8/25 | Loss: 0.00092899
Iteration 9/25 | Loss: 0.00092899
Iteration 10/25 | Loss: 0.00092899
Iteration 11/25 | Loss: 0.00092899
Iteration 12/25 | Loss: 0.00092899
Iteration 13/25 | Loss: 0.00092899
Iteration 14/25 | Loss: 0.00092899
Iteration 15/25 | Loss: 0.00092899
Iteration 16/25 | Loss: 0.00092899
Iteration 17/25 | Loss: 0.00092899
Iteration 18/25 | Loss: 0.00092899
Iteration 19/25 | Loss: 0.00092899
Iteration 20/25 | Loss: 0.00092899
Iteration 21/25 | Loss: 0.00092899
Iteration 22/25 | Loss: 0.00092899
Iteration 23/25 | Loss: 0.00092899
Iteration 24/25 | Loss: 0.00092899
Iteration 25/25 | Loss: 0.00092899

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092899
Iteration 2/1000 | Loss: 0.00004981
Iteration 3/1000 | Loss: 0.00003062
Iteration 4/1000 | Loss: 0.00002763
Iteration 5/1000 | Loss: 0.00002671
Iteration 6/1000 | Loss: 0.00002605
Iteration 7/1000 | Loss: 0.00002571
Iteration 8/1000 | Loss: 0.00002532
Iteration 9/1000 | Loss: 0.00002511
Iteration 10/1000 | Loss: 0.00002509
Iteration 11/1000 | Loss: 0.00002497
Iteration 12/1000 | Loss: 0.00002484
Iteration 13/1000 | Loss: 0.00002482
Iteration 14/1000 | Loss: 0.00002482
Iteration 15/1000 | Loss: 0.00002476
Iteration 16/1000 | Loss: 0.00002476
Iteration 17/1000 | Loss: 0.00002476
Iteration 18/1000 | Loss: 0.00002475
Iteration 19/1000 | Loss: 0.00002472
Iteration 20/1000 | Loss: 0.00002470
Iteration 21/1000 | Loss: 0.00002469
Iteration 22/1000 | Loss: 0.00002469
Iteration 23/1000 | Loss: 0.00002468
Iteration 24/1000 | Loss: 0.00002467
Iteration 25/1000 | Loss: 0.00002467
Iteration 26/1000 | Loss: 0.00002466
Iteration 27/1000 | Loss: 0.00002465
Iteration 28/1000 | Loss: 0.00002464
Iteration 29/1000 | Loss: 0.00002464
Iteration 30/1000 | Loss: 0.00002464
Iteration 31/1000 | Loss: 0.00002464
Iteration 32/1000 | Loss: 0.00002464
Iteration 33/1000 | Loss: 0.00002463
Iteration 34/1000 | Loss: 0.00002463
Iteration 35/1000 | Loss: 0.00002463
Iteration 36/1000 | Loss: 0.00002463
Iteration 37/1000 | Loss: 0.00002463
Iteration 38/1000 | Loss: 0.00002463
Iteration 39/1000 | Loss: 0.00002463
Iteration 40/1000 | Loss: 0.00002462
Iteration 41/1000 | Loss: 0.00002462
Iteration 42/1000 | Loss: 0.00002461
Iteration 43/1000 | Loss: 0.00002461
Iteration 44/1000 | Loss: 0.00002460
Iteration 45/1000 | Loss: 0.00002460
Iteration 46/1000 | Loss: 0.00002459
Iteration 47/1000 | Loss: 0.00002459
Iteration 48/1000 | Loss: 0.00002459
Iteration 49/1000 | Loss: 0.00002458
Iteration 50/1000 | Loss: 0.00002458
Iteration 51/1000 | Loss: 0.00002457
Iteration 52/1000 | Loss: 0.00002457
Iteration 53/1000 | Loss: 0.00002456
Iteration 54/1000 | Loss: 0.00002456
Iteration 55/1000 | Loss: 0.00002456
Iteration 56/1000 | Loss: 0.00002455
Iteration 57/1000 | Loss: 0.00002455
Iteration 58/1000 | Loss: 0.00002454
Iteration 59/1000 | Loss: 0.00002454
Iteration 60/1000 | Loss: 0.00002452
Iteration 61/1000 | Loss: 0.00002452
Iteration 62/1000 | Loss: 0.00002451
Iteration 63/1000 | Loss: 0.00002451
Iteration 64/1000 | Loss: 0.00002451
Iteration 65/1000 | Loss: 0.00002451
Iteration 66/1000 | Loss: 0.00002450
Iteration 67/1000 | Loss: 0.00002450
Iteration 68/1000 | Loss: 0.00002447
Iteration 69/1000 | Loss: 0.00002447
Iteration 70/1000 | Loss: 0.00002447
Iteration 71/1000 | Loss: 0.00002447
Iteration 72/1000 | Loss: 0.00002447
Iteration 73/1000 | Loss: 0.00002446
Iteration 74/1000 | Loss: 0.00002446
Iteration 75/1000 | Loss: 0.00002446
Iteration 76/1000 | Loss: 0.00002446
Iteration 77/1000 | Loss: 0.00002446
Iteration 78/1000 | Loss: 0.00002445
Iteration 79/1000 | Loss: 0.00002445
Iteration 80/1000 | Loss: 0.00002443
Iteration 81/1000 | Loss: 0.00002442
Iteration 82/1000 | Loss: 0.00002441
Iteration 83/1000 | Loss: 0.00002441
Iteration 84/1000 | Loss: 0.00002440
Iteration 85/1000 | Loss: 0.00002440
Iteration 86/1000 | Loss: 0.00002440
Iteration 87/1000 | Loss: 0.00002440
Iteration 88/1000 | Loss: 0.00002439
Iteration 89/1000 | Loss: 0.00002439
Iteration 90/1000 | Loss: 0.00002439
Iteration 91/1000 | Loss: 0.00002439
Iteration 92/1000 | Loss: 0.00002439
Iteration 93/1000 | Loss: 0.00002439
Iteration 94/1000 | Loss: 0.00002439
Iteration 95/1000 | Loss: 0.00002439
Iteration 96/1000 | Loss: 0.00002439
Iteration 97/1000 | Loss: 0.00002438
Iteration 98/1000 | Loss: 0.00002438
Iteration 99/1000 | Loss: 0.00002438
Iteration 100/1000 | Loss: 0.00002438
Iteration 101/1000 | Loss: 0.00002437
Iteration 102/1000 | Loss: 0.00002437
Iteration 103/1000 | Loss: 0.00002437
Iteration 104/1000 | Loss: 0.00002437
Iteration 105/1000 | Loss: 0.00002436
Iteration 106/1000 | Loss: 0.00002436
Iteration 107/1000 | Loss: 0.00002436
Iteration 108/1000 | Loss: 0.00002436
Iteration 109/1000 | Loss: 0.00002436
Iteration 110/1000 | Loss: 0.00002436
Iteration 111/1000 | Loss: 0.00002435
Iteration 112/1000 | Loss: 0.00002435
Iteration 113/1000 | Loss: 0.00002435
Iteration 114/1000 | Loss: 0.00002435
Iteration 115/1000 | Loss: 0.00002435
Iteration 116/1000 | Loss: 0.00002435
Iteration 117/1000 | Loss: 0.00002434
Iteration 118/1000 | Loss: 0.00002434
Iteration 119/1000 | Loss: 0.00002434
Iteration 120/1000 | Loss: 0.00002434
Iteration 121/1000 | Loss: 0.00002434
Iteration 122/1000 | Loss: 0.00002434
Iteration 123/1000 | Loss: 0.00002434
Iteration 124/1000 | Loss: 0.00002434
Iteration 125/1000 | Loss: 0.00002433
Iteration 126/1000 | Loss: 0.00002433
Iteration 127/1000 | Loss: 0.00002433
Iteration 128/1000 | Loss: 0.00002433
Iteration 129/1000 | Loss: 0.00002433
Iteration 130/1000 | Loss: 0.00002433
Iteration 131/1000 | Loss: 0.00002433
Iteration 132/1000 | Loss: 0.00002433
Iteration 133/1000 | Loss: 0.00002433
Iteration 134/1000 | Loss: 0.00002433
Iteration 135/1000 | Loss: 0.00002433
Iteration 136/1000 | Loss: 0.00002433
Iteration 137/1000 | Loss: 0.00002433
Iteration 138/1000 | Loss: 0.00002432
Iteration 139/1000 | Loss: 0.00002432
Iteration 140/1000 | Loss: 0.00002432
Iteration 141/1000 | Loss: 0.00002432
Iteration 142/1000 | Loss: 0.00002432
Iteration 143/1000 | Loss: 0.00002432
Iteration 144/1000 | Loss: 0.00002432
Iteration 145/1000 | Loss: 0.00002432
Iteration 146/1000 | Loss: 0.00002432
Iteration 147/1000 | Loss: 0.00002432
Iteration 148/1000 | Loss: 0.00002432
Iteration 149/1000 | Loss: 0.00002431
Iteration 150/1000 | Loss: 0.00002431
Iteration 151/1000 | Loss: 0.00002431
Iteration 152/1000 | Loss: 0.00002431
Iteration 153/1000 | Loss: 0.00002431
Iteration 154/1000 | Loss: 0.00002431
Iteration 155/1000 | Loss: 0.00002431
Iteration 156/1000 | Loss: 0.00002431
Iteration 157/1000 | Loss: 0.00002431
Iteration 158/1000 | Loss: 0.00002431
Iteration 159/1000 | Loss: 0.00002431
Iteration 160/1000 | Loss: 0.00002431
Iteration 161/1000 | Loss: 0.00002431
Iteration 162/1000 | Loss: 0.00002431
Iteration 163/1000 | Loss: 0.00002430
Iteration 164/1000 | Loss: 0.00002430
Iteration 165/1000 | Loss: 0.00002430
Iteration 166/1000 | Loss: 0.00002430
Iteration 167/1000 | Loss: 0.00002430
Iteration 168/1000 | Loss: 0.00002430
Iteration 169/1000 | Loss: 0.00002430
Iteration 170/1000 | Loss: 0.00002430
Iteration 171/1000 | Loss: 0.00002430
Iteration 172/1000 | Loss: 0.00002430
Iteration 173/1000 | Loss: 0.00002430
Iteration 174/1000 | Loss: 0.00002430
Iteration 175/1000 | Loss: 0.00002430
Iteration 176/1000 | Loss: 0.00002430
Iteration 177/1000 | Loss: 0.00002430
Iteration 178/1000 | Loss: 0.00002430
Iteration 179/1000 | Loss: 0.00002430
Iteration 180/1000 | Loss: 0.00002430
Iteration 181/1000 | Loss: 0.00002430
Iteration 182/1000 | Loss: 0.00002430
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [2.429793676128611e-05, 2.429793676128611e-05, 2.429793676128611e-05, 2.429793676128611e-05, 2.429793676128611e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.429793676128611e-05

Optimization complete. Final v2v error: 4.070819854736328 mm

Highest mean error: 4.298009872436523 mm for frame 157

Lowest mean error: 3.8492591381073 mm for frame 93

Saving results

Total time: 41.4840362071991
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00793179
Iteration 2/25 | Loss: 0.00147455
Iteration 3/25 | Loss: 0.00137084
Iteration 4/25 | Loss: 0.00135610
Iteration 5/25 | Loss: 0.00135153
Iteration 6/25 | Loss: 0.00135153
Iteration 7/25 | Loss: 0.00135153
Iteration 8/25 | Loss: 0.00135153
Iteration 9/25 | Loss: 0.00135153
Iteration 10/25 | Loss: 0.00135153
Iteration 11/25 | Loss: 0.00135153
Iteration 12/25 | Loss: 0.00135153
Iteration 13/25 | Loss: 0.00135153
Iteration 14/25 | Loss: 0.00135153
Iteration 15/25 | Loss: 0.00135153
Iteration 16/25 | Loss: 0.00135153
Iteration 17/25 | Loss: 0.00135153
Iteration 18/25 | Loss: 0.00135153
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0013515286846086383, 0.0013515286846086383, 0.0013515286846086383, 0.0013515286846086383, 0.0013515286846086383]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013515286846086383

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32108808
Iteration 2/25 | Loss: 0.00071968
Iteration 3/25 | Loss: 0.00071963
Iteration 4/25 | Loss: 0.00071963
Iteration 5/25 | Loss: 0.00071963
Iteration 6/25 | Loss: 0.00071962
Iteration 7/25 | Loss: 0.00071962
Iteration 8/25 | Loss: 0.00071962
Iteration 9/25 | Loss: 0.00071962
Iteration 10/25 | Loss: 0.00071962
Iteration 11/25 | Loss: 0.00071962
Iteration 12/25 | Loss: 0.00071962
Iteration 13/25 | Loss: 0.00071962
Iteration 14/25 | Loss: 0.00071962
Iteration 15/25 | Loss: 0.00071962
Iteration 16/25 | Loss: 0.00071962
Iteration 17/25 | Loss: 0.00071962
Iteration 18/25 | Loss: 0.00071962
Iteration 19/25 | Loss: 0.00071962
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007196235237643123, 0.0007196235237643123, 0.0007196235237643123, 0.0007196235237643123, 0.0007196235237643123]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007196235237643123

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071962
Iteration 2/1000 | Loss: 0.00005047
Iteration 3/1000 | Loss: 0.00003702
Iteration 4/1000 | Loss: 0.00003186
Iteration 5/1000 | Loss: 0.00002968
Iteration 6/1000 | Loss: 0.00002824
Iteration 7/1000 | Loss: 0.00002713
Iteration 8/1000 | Loss: 0.00002642
Iteration 9/1000 | Loss: 0.00002607
Iteration 10/1000 | Loss: 0.00002576
Iteration 11/1000 | Loss: 0.00002555
Iteration 12/1000 | Loss: 0.00002536
Iteration 13/1000 | Loss: 0.00002517
Iteration 14/1000 | Loss: 0.00002514
Iteration 15/1000 | Loss: 0.00002500
Iteration 16/1000 | Loss: 0.00002496
Iteration 17/1000 | Loss: 0.00002491
Iteration 18/1000 | Loss: 0.00002485
Iteration 19/1000 | Loss: 0.00002483
Iteration 20/1000 | Loss: 0.00002480
Iteration 21/1000 | Loss: 0.00002475
Iteration 22/1000 | Loss: 0.00002466
Iteration 23/1000 | Loss: 0.00002460
Iteration 24/1000 | Loss: 0.00002460
Iteration 25/1000 | Loss: 0.00002459
Iteration 26/1000 | Loss: 0.00002459
Iteration 27/1000 | Loss: 0.00002459
Iteration 28/1000 | Loss: 0.00002459
Iteration 29/1000 | Loss: 0.00002458
Iteration 30/1000 | Loss: 0.00002458
Iteration 31/1000 | Loss: 0.00002457
Iteration 32/1000 | Loss: 0.00002457
Iteration 33/1000 | Loss: 0.00002456
Iteration 34/1000 | Loss: 0.00002456
Iteration 35/1000 | Loss: 0.00002456
Iteration 36/1000 | Loss: 0.00002456
Iteration 37/1000 | Loss: 0.00002455
Iteration 38/1000 | Loss: 0.00002455
Iteration 39/1000 | Loss: 0.00002455
Iteration 40/1000 | Loss: 0.00002455
Iteration 41/1000 | Loss: 0.00002455
Iteration 42/1000 | Loss: 0.00002455
Iteration 43/1000 | Loss: 0.00002455
Iteration 44/1000 | Loss: 0.00002455
Iteration 45/1000 | Loss: 0.00002455
Iteration 46/1000 | Loss: 0.00002455
Iteration 47/1000 | Loss: 0.00002454
Iteration 48/1000 | Loss: 0.00002454
Iteration 49/1000 | Loss: 0.00002454
Iteration 50/1000 | Loss: 0.00002454
Iteration 51/1000 | Loss: 0.00002454
Iteration 52/1000 | Loss: 0.00002453
Iteration 53/1000 | Loss: 0.00002453
Iteration 54/1000 | Loss: 0.00002453
Iteration 55/1000 | Loss: 0.00002452
Iteration 56/1000 | Loss: 0.00002452
Iteration 57/1000 | Loss: 0.00002452
Iteration 58/1000 | Loss: 0.00002451
Iteration 59/1000 | Loss: 0.00002451
Iteration 60/1000 | Loss: 0.00002451
Iteration 61/1000 | Loss: 0.00002451
Iteration 62/1000 | Loss: 0.00002451
Iteration 63/1000 | Loss: 0.00002451
Iteration 64/1000 | Loss: 0.00002451
Iteration 65/1000 | Loss: 0.00002451
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 65. Stopping optimization.
Last 5 losses: [2.4514738470315933e-05, 2.4514738470315933e-05, 2.4514738470315933e-05, 2.4514738470315933e-05, 2.4514738470315933e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4514738470315933e-05

Optimization complete. Final v2v error: 4.154638290405273 mm

Highest mean error: 4.346160888671875 mm for frame 75

Lowest mean error: 3.8603012561798096 mm for frame 210

Saving results

Total time: 38.29008960723877
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00991611
Iteration 2/25 | Loss: 0.00206686
Iteration 3/25 | Loss: 0.00158775
Iteration 4/25 | Loss: 0.00152951
Iteration 5/25 | Loss: 0.00146031
Iteration 6/25 | Loss: 0.00147321
Iteration 7/25 | Loss: 0.00144631
Iteration 8/25 | Loss: 0.00143388
Iteration 9/25 | Loss: 0.00142846
Iteration 10/25 | Loss: 0.00139311
Iteration 11/25 | Loss: 0.00137887
Iteration 12/25 | Loss: 0.00137144
Iteration 13/25 | Loss: 0.00135890
Iteration 14/25 | Loss: 0.00135861
Iteration 15/25 | Loss: 0.00135703
Iteration 16/25 | Loss: 0.00135739
Iteration 17/25 | Loss: 0.00135705
Iteration 18/25 | Loss: 0.00135607
Iteration 19/25 | Loss: 0.00135892
Iteration 20/25 | Loss: 0.00135735
Iteration 21/25 | Loss: 0.00135714
Iteration 22/25 | Loss: 0.00135937
Iteration 23/25 | Loss: 0.00135585
Iteration 24/25 | Loss: 0.00135933
Iteration 25/25 | Loss: 0.00135793

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41533256
Iteration 2/25 | Loss: 0.00154792
Iteration 3/25 | Loss: 0.00094571
Iteration 4/25 | Loss: 0.00088351
Iteration 5/25 | Loss: 0.00088351
Iteration 6/25 | Loss: 0.00088351
Iteration 7/25 | Loss: 0.00088351
Iteration 8/25 | Loss: 0.00088350
Iteration 9/25 | Loss: 0.00088350
Iteration 10/25 | Loss: 0.00088350
Iteration 11/25 | Loss: 0.00088350
Iteration 12/25 | Loss: 0.00088350
Iteration 13/25 | Loss: 0.00088350
Iteration 14/25 | Loss: 0.00088350
Iteration 15/25 | Loss: 0.00088350
Iteration 16/25 | Loss: 0.00088350
Iteration 17/25 | Loss: 0.00088350
Iteration 18/25 | Loss: 0.00088350
Iteration 19/25 | Loss: 0.00088350
Iteration 20/25 | Loss: 0.00088350
Iteration 21/25 | Loss: 0.00088350
Iteration 22/25 | Loss: 0.00088350
Iteration 23/25 | Loss: 0.00088350
Iteration 24/25 | Loss: 0.00088350
Iteration 25/25 | Loss: 0.00088350

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088350
Iteration 2/1000 | Loss: 0.00073597
Iteration 3/1000 | Loss: 0.00015612
Iteration 4/1000 | Loss: 0.00007341
Iteration 5/1000 | Loss: 0.00002680
Iteration 6/1000 | Loss: 0.00004562
Iteration 7/1000 | Loss: 0.00007185
Iteration 8/1000 | Loss: 0.00010397
Iteration 9/1000 | Loss: 0.00002417
Iteration 10/1000 | Loss: 0.00003255
Iteration 11/1000 | Loss: 0.00002419
Iteration 12/1000 | Loss: 0.00018835
Iteration 13/1000 | Loss: 0.00002420
Iteration 14/1000 | Loss: 0.00006991
Iteration 15/1000 | Loss: 0.00010767
Iteration 16/1000 | Loss: 0.00003153
Iteration 17/1000 | Loss: 0.00003305
Iteration 18/1000 | Loss: 0.00005411
Iteration 19/1000 | Loss: 0.00002765
Iteration 20/1000 | Loss: 0.00004234
Iteration 21/1000 | Loss: 0.00003658
Iteration 22/1000 | Loss: 0.00009616
Iteration 23/1000 | Loss: 0.00072179
Iteration 24/1000 | Loss: 0.00003940
Iteration 25/1000 | Loss: 0.00002787
Iteration 26/1000 | Loss: 0.00008208
Iteration 27/1000 | Loss: 0.00002280
Iteration 28/1000 | Loss: 0.00004280
Iteration 29/1000 | Loss: 0.00002202
Iteration 30/1000 | Loss: 0.00002195
Iteration 31/1000 | Loss: 0.00002194
Iteration 32/1000 | Loss: 0.00006871
Iteration 33/1000 | Loss: 0.00010452
Iteration 34/1000 | Loss: 0.00006585
Iteration 35/1000 | Loss: 0.00005380
Iteration 36/1000 | Loss: 0.00026562
Iteration 37/1000 | Loss: 0.00009029
Iteration 38/1000 | Loss: 0.00002993
Iteration 39/1000 | Loss: 0.00003839
Iteration 40/1000 | Loss: 0.00002298
Iteration 41/1000 | Loss: 0.00003197
Iteration 42/1000 | Loss: 0.00019010
Iteration 43/1000 | Loss: 0.00002584
Iteration 44/1000 | Loss: 0.00002179
Iteration 45/1000 | Loss: 0.00002179
Iteration 46/1000 | Loss: 0.00002178
Iteration 47/1000 | Loss: 0.00002178
Iteration 48/1000 | Loss: 0.00002178
Iteration 49/1000 | Loss: 0.00002177
Iteration 50/1000 | Loss: 0.00002174
Iteration 51/1000 | Loss: 0.00002173
Iteration 52/1000 | Loss: 0.00002173
Iteration 53/1000 | Loss: 0.00002173
Iteration 54/1000 | Loss: 0.00002172
Iteration 55/1000 | Loss: 0.00002172
Iteration 56/1000 | Loss: 0.00003866
Iteration 57/1000 | Loss: 0.00002722
Iteration 58/1000 | Loss: 0.00002167
Iteration 59/1000 | Loss: 0.00002167
Iteration 60/1000 | Loss: 0.00002613
Iteration 61/1000 | Loss: 0.00002171
Iteration 62/1000 | Loss: 0.00002166
Iteration 63/1000 | Loss: 0.00003157
Iteration 64/1000 | Loss: 0.00002310
Iteration 65/1000 | Loss: 0.00002169
Iteration 66/1000 | Loss: 0.00002169
Iteration 67/1000 | Loss: 0.00002169
Iteration 68/1000 | Loss: 0.00002165
Iteration 69/1000 | Loss: 0.00002164
Iteration 70/1000 | Loss: 0.00002164
Iteration 71/1000 | Loss: 0.00002164
Iteration 72/1000 | Loss: 0.00002164
Iteration 73/1000 | Loss: 0.00002163
Iteration 74/1000 | Loss: 0.00002163
Iteration 75/1000 | Loss: 0.00002163
Iteration 76/1000 | Loss: 0.00002163
Iteration 77/1000 | Loss: 0.00002163
Iteration 78/1000 | Loss: 0.00002163
Iteration 79/1000 | Loss: 0.00002163
Iteration 80/1000 | Loss: 0.00002162
Iteration 81/1000 | Loss: 0.00002162
Iteration 82/1000 | Loss: 0.00006340
Iteration 83/1000 | Loss: 0.00002187
Iteration 84/1000 | Loss: 0.00002158
Iteration 85/1000 | Loss: 0.00002158
Iteration 86/1000 | Loss: 0.00002158
Iteration 87/1000 | Loss: 0.00002158
Iteration 88/1000 | Loss: 0.00002158
Iteration 89/1000 | Loss: 0.00002158
Iteration 90/1000 | Loss: 0.00002158
Iteration 91/1000 | Loss: 0.00002158
Iteration 92/1000 | Loss: 0.00002157
Iteration 93/1000 | Loss: 0.00002157
Iteration 94/1000 | Loss: 0.00002157
Iteration 95/1000 | Loss: 0.00002156
Iteration 96/1000 | Loss: 0.00002156
Iteration 97/1000 | Loss: 0.00002156
Iteration 98/1000 | Loss: 0.00002156
Iteration 99/1000 | Loss: 0.00002156
Iteration 100/1000 | Loss: 0.00002156
Iteration 101/1000 | Loss: 0.00002156
Iteration 102/1000 | Loss: 0.00002156
Iteration 103/1000 | Loss: 0.00002155
Iteration 104/1000 | Loss: 0.00002155
Iteration 105/1000 | Loss: 0.00002155
Iteration 106/1000 | Loss: 0.00002155
Iteration 107/1000 | Loss: 0.00002155
Iteration 108/1000 | Loss: 0.00002155
Iteration 109/1000 | Loss: 0.00002155
Iteration 110/1000 | Loss: 0.00002155
Iteration 111/1000 | Loss: 0.00002155
Iteration 112/1000 | Loss: 0.00002155
Iteration 113/1000 | Loss: 0.00002155
Iteration 114/1000 | Loss: 0.00002155
Iteration 115/1000 | Loss: 0.00002155
Iteration 116/1000 | Loss: 0.00002155
Iteration 117/1000 | Loss: 0.00002155
Iteration 118/1000 | Loss: 0.00002155
Iteration 119/1000 | Loss: 0.00002155
Iteration 120/1000 | Loss: 0.00002155
Iteration 121/1000 | Loss: 0.00002155
Iteration 122/1000 | Loss: 0.00002155
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [2.1551561076194048e-05, 2.1551561076194048e-05, 2.1551561076194048e-05, 2.1551561076194048e-05, 2.1551561076194048e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1551561076194048e-05

Optimization complete. Final v2v error: 3.972287178039551 mm

Highest mean error: 4.1927490234375 mm for frame 67

Lowest mean error: 3.690871000289917 mm for frame 40

Saving results

Total time: 139.36433601379395
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00790609
Iteration 2/25 | Loss: 0.00134611
Iteration 3/25 | Loss: 0.00127472
Iteration 4/25 | Loss: 0.00126021
Iteration 5/25 | Loss: 0.00125605
Iteration 6/25 | Loss: 0.00125598
Iteration 7/25 | Loss: 0.00125598
Iteration 8/25 | Loss: 0.00125598
Iteration 9/25 | Loss: 0.00125598
Iteration 10/25 | Loss: 0.00125598
Iteration 11/25 | Loss: 0.00125598
Iteration 12/25 | Loss: 0.00125598
Iteration 13/25 | Loss: 0.00125598
Iteration 14/25 | Loss: 0.00125598
Iteration 15/25 | Loss: 0.00125598
Iteration 16/25 | Loss: 0.00125598
Iteration 17/25 | Loss: 0.00125598
Iteration 18/25 | Loss: 0.00125598
Iteration 19/25 | Loss: 0.00125572
Iteration 20/25 | Loss: 0.00125572
Iteration 21/25 | Loss: 0.00125572
Iteration 22/25 | Loss: 0.00125572
Iteration 23/25 | Loss: 0.00125572
Iteration 24/25 | Loss: 0.00125572
Iteration 25/25 | Loss: 0.00125572

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47010815
Iteration 2/25 | Loss: 0.00079768
Iteration 3/25 | Loss: 0.00079767
Iteration 4/25 | Loss: 0.00079767
Iteration 5/25 | Loss: 0.00079767
Iteration 6/25 | Loss: 0.00079766
Iteration 7/25 | Loss: 0.00079766
Iteration 8/25 | Loss: 0.00079766
Iteration 9/25 | Loss: 0.00079766
Iteration 10/25 | Loss: 0.00079766
Iteration 11/25 | Loss: 0.00079766
Iteration 12/25 | Loss: 0.00079766
Iteration 13/25 | Loss: 0.00079766
Iteration 14/25 | Loss: 0.00079766
Iteration 15/25 | Loss: 0.00079766
Iteration 16/25 | Loss: 0.00079766
Iteration 17/25 | Loss: 0.00079766
Iteration 18/25 | Loss: 0.00079766
Iteration 19/25 | Loss: 0.00079766
Iteration 20/25 | Loss: 0.00079766
Iteration 21/25 | Loss: 0.00079766
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007976638153195381, 0.0007976638153195381, 0.0007976638153195381, 0.0007976638153195381, 0.0007976638153195381]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007976638153195381

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079766
Iteration 2/1000 | Loss: 0.00003182
Iteration 3/1000 | Loss: 0.00002147
Iteration 4/1000 | Loss: 0.00001971
Iteration 5/1000 | Loss: 0.00001864
Iteration 6/1000 | Loss: 0.00001785
Iteration 7/1000 | Loss: 0.00001737
Iteration 8/1000 | Loss: 0.00001708
Iteration 9/1000 | Loss: 0.00001676
Iteration 10/1000 | Loss: 0.00001648
Iteration 11/1000 | Loss: 0.00001642
Iteration 12/1000 | Loss: 0.00001628
Iteration 13/1000 | Loss: 0.00001626
Iteration 14/1000 | Loss: 0.00001616
Iteration 15/1000 | Loss: 0.00001611
Iteration 16/1000 | Loss: 0.00001601
Iteration 17/1000 | Loss: 0.00001598
Iteration 18/1000 | Loss: 0.00001597
Iteration 19/1000 | Loss: 0.00001597
Iteration 20/1000 | Loss: 0.00001593
Iteration 21/1000 | Loss: 0.00001593
Iteration 22/1000 | Loss: 0.00001592
Iteration 23/1000 | Loss: 0.00001592
Iteration 24/1000 | Loss: 0.00001592
Iteration 25/1000 | Loss: 0.00001591
Iteration 26/1000 | Loss: 0.00001590
Iteration 27/1000 | Loss: 0.00001588
Iteration 28/1000 | Loss: 0.00001587
Iteration 29/1000 | Loss: 0.00001587
Iteration 30/1000 | Loss: 0.00001587
Iteration 31/1000 | Loss: 0.00001587
Iteration 32/1000 | Loss: 0.00001586
Iteration 33/1000 | Loss: 0.00001586
Iteration 34/1000 | Loss: 0.00001585
Iteration 35/1000 | Loss: 0.00001585
Iteration 36/1000 | Loss: 0.00001583
Iteration 37/1000 | Loss: 0.00001581
Iteration 38/1000 | Loss: 0.00001581
Iteration 39/1000 | Loss: 0.00001581
Iteration 40/1000 | Loss: 0.00001581
Iteration 41/1000 | Loss: 0.00001580
Iteration 42/1000 | Loss: 0.00001580
Iteration 43/1000 | Loss: 0.00001579
Iteration 44/1000 | Loss: 0.00001577
Iteration 45/1000 | Loss: 0.00001576
Iteration 46/1000 | Loss: 0.00001576
Iteration 47/1000 | Loss: 0.00001576
Iteration 48/1000 | Loss: 0.00001576
Iteration 49/1000 | Loss: 0.00001575
Iteration 50/1000 | Loss: 0.00001575
Iteration 51/1000 | Loss: 0.00001575
Iteration 52/1000 | Loss: 0.00001575
Iteration 53/1000 | Loss: 0.00001575
Iteration 54/1000 | Loss: 0.00001574
Iteration 55/1000 | Loss: 0.00001574
Iteration 56/1000 | Loss: 0.00001573
Iteration 57/1000 | Loss: 0.00001573
Iteration 58/1000 | Loss: 0.00001572
Iteration 59/1000 | Loss: 0.00001572
Iteration 60/1000 | Loss: 0.00001572
Iteration 61/1000 | Loss: 0.00001572
Iteration 62/1000 | Loss: 0.00001572
Iteration 63/1000 | Loss: 0.00001571
Iteration 64/1000 | Loss: 0.00001571
Iteration 65/1000 | Loss: 0.00001571
Iteration 66/1000 | Loss: 0.00001570
Iteration 67/1000 | Loss: 0.00001570
Iteration 68/1000 | Loss: 0.00001570
Iteration 69/1000 | Loss: 0.00001570
Iteration 70/1000 | Loss: 0.00001569
Iteration 71/1000 | Loss: 0.00001569
Iteration 72/1000 | Loss: 0.00001569
Iteration 73/1000 | Loss: 0.00001569
Iteration 74/1000 | Loss: 0.00001568
Iteration 75/1000 | Loss: 0.00001568
Iteration 76/1000 | Loss: 0.00001568
Iteration 77/1000 | Loss: 0.00001568
Iteration 78/1000 | Loss: 0.00001568
Iteration 79/1000 | Loss: 0.00001568
Iteration 80/1000 | Loss: 0.00001568
Iteration 81/1000 | Loss: 0.00001568
Iteration 82/1000 | Loss: 0.00001568
Iteration 83/1000 | Loss: 0.00001567
Iteration 84/1000 | Loss: 0.00001567
Iteration 85/1000 | Loss: 0.00001567
Iteration 86/1000 | Loss: 0.00001566
Iteration 87/1000 | Loss: 0.00001566
Iteration 88/1000 | Loss: 0.00001565
Iteration 89/1000 | Loss: 0.00001565
Iteration 90/1000 | Loss: 0.00001565
Iteration 91/1000 | Loss: 0.00001565
Iteration 92/1000 | Loss: 0.00001565
Iteration 93/1000 | Loss: 0.00001565
Iteration 94/1000 | Loss: 0.00001565
Iteration 95/1000 | Loss: 0.00001564
Iteration 96/1000 | Loss: 0.00001564
Iteration 97/1000 | Loss: 0.00001564
Iteration 98/1000 | Loss: 0.00001562
Iteration 99/1000 | Loss: 0.00001562
Iteration 100/1000 | Loss: 0.00001561
Iteration 101/1000 | Loss: 0.00001561
Iteration 102/1000 | Loss: 0.00001560
Iteration 103/1000 | Loss: 0.00001560
Iteration 104/1000 | Loss: 0.00001560
Iteration 105/1000 | Loss: 0.00001560
Iteration 106/1000 | Loss: 0.00001559
Iteration 107/1000 | Loss: 0.00001559
Iteration 108/1000 | Loss: 0.00001559
Iteration 109/1000 | Loss: 0.00001559
Iteration 110/1000 | Loss: 0.00001559
Iteration 111/1000 | Loss: 0.00001558
Iteration 112/1000 | Loss: 0.00001558
Iteration 113/1000 | Loss: 0.00001558
Iteration 114/1000 | Loss: 0.00001558
Iteration 115/1000 | Loss: 0.00001557
Iteration 116/1000 | Loss: 0.00001557
Iteration 117/1000 | Loss: 0.00001557
Iteration 118/1000 | Loss: 0.00001557
Iteration 119/1000 | Loss: 0.00001557
Iteration 120/1000 | Loss: 0.00001557
Iteration 121/1000 | Loss: 0.00001557
Iteration 122/1000 | Loss: 0.00001557
Iteration 123/1000 | Loss: 0.00001557
Iteration 124/1000 | Loss: 0.00001557
Iteration 125/1000 | Loss: 0.00001557
Iteration 126/1000 | Loss: 0.00001556
Iteration 127/1000 | Loss: 0.00001556
Iteration 128/1000 | Loss: 0.00001556
Iteration 129/1000 | Loss: 0.00001555
Iteration 130/1000 | Loss: 0.00001555
Iteration 131/1000 | Loss: 0.00001555
Iteration 132/1000 | Loss: 0.00001555
Iteration 133/1000 | Loss: 0.00001554
Iteration 134/1000 | Loss: 0.00001554
Iteration 135/1000 | Loss: 0.00001554
Iteration 136/1000 | Loss: 0.00001554
Iteration 137/1000 | Loss: 0.00001554
Iteration 138/1000 | Loss: 0.00001554
Iteration 139/1000 | Loss: 0.00001554
Iteration 140/1000 | Loss: 0.00001554
Iteration 141/1000 | Loss: 0.00001554
Iteration 142/1000 | Loss: 0.00001553
Iteration 143/1000 | Loss: 0.00001553
Iteration 144/1000 | Loss: 0.00001553
Iteration 145/1000 | Loss: 0.00001553
Iteration 146/1000 | Loss: 0.00001553
Iteration 147/1000 | Loss: 0.00001553
Iteration 148/1000 | Loss: 0.00001553
Iteration 149/1000 | Loss: 0.00001552
Iteration 150/1000 | Loss: 0.00001552
Iteration 151/1000 | Loss: 0.00001552
Iteration 152/1000 | Loss: 0.00001551
Iteration 153/1000 | Loss: 0.00001551
Iteration 154/1000 | Loss: 0.00001551
Iteration 155/1000 | Loss: 0.00001551
Iteration 156/1000 | Loss: 0.00001551
Iteration 157/1000 | Loss: 0.00001551
Iteration 158/1000 | Loss: 0.00001551
Iteration 159/1000 | Loss: 0.00001551
Iteration 160/1000 | Loss: 0.00001551
Iteration 161/1000 | Loss: 0.00001551
Iteration 162/1000 | Loss: 0.00001551
Iteration 163/1000 | Loss: 0.00001551
Iteration 164/1000 | Loss: 0.00001551
Iteration 165/1000 | Loss: 0.00001551
Iteration 166/1000 | Loss: 0.00001551
Iteration 167/1000 | Loss: 0.00001550
Iteration 168/1000 | Loss: 0.00001550
Iteration 169/1000 | Loss: 0.00001550
Iteration 170/1000 | Loss: 0.00001550
Iteration 171/1000 | Loss: 0.00001550
Iteration 172/1000 | Loss: 0.00001550
Iteration 173/1000 | Loss: 0.00001550
Iteration 174/1000 | Loss: 0.00001550
Iteration 175/1000 | Loss: 0.00001550
Iteration 176/1000 | Loss: 0.00001550
Iteration 177/1000 | Loss: 0.00001550
Iteration 178/1000 | Loss: 0.00001550
Iteration 179/1000 | Loss: 0.00001549
Iteration 180/1000 | Loss: 0.00001549
Iteration 181/1000 | Loss: 0.00001549
Iteration 182/1000 | Loss: 0.00001549
Iteration 183/1000 | Loss: 0.00001549
Iteration 184/1000 | Loss: 0.00001549
Iteration 185/1000 | Loss: 0.00001549
Iteration 186/1000 | Loss: 0.00001549
Iteration 187/1000 | Loss: 0.00001549
Iteration 188/1000 | Loss: 0.00001549
Iteration 189/1000 | Loss: 0.00001549
Iteration 190/1000 | Loss: 0.00001549
Iteration 191/1000 | Loss: 0.00001549
Iteration 192/1000 | Loss: 0.00001549
Iteration 193/1000 | Loss: 0.00001549
Iteration 194/1000 | Loss: 0.00001549
Iteration 195/1000 | Loss: 0.00001549
Iteration 196/1000 | Loss: 0.00001549
Iteration 197/1000 | Loss: 0.00001549
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [1.5487328710150905e-05, 1.5487328710150905e-05, 1.5487328710150905e-05, 1.5487328710150905e-05, 1.5487328710150905e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5487328710150905e-05

Optimization complete. Final v2v error: 3.386885166168213 mm

Highest mean error: 3.676130533218384 mm for frame 154

Lowest mean error: 3.1873116493225098 mm for frame 65

Saving results

Total time: 48.988908529281616
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01105173
Iteration 2/25 | Loss: 0.01105173
Iteration 3/25 | Loss: 0.01105172
Iteration 4/25 | Loss: 0.01105172
Iteration 5/25 | Loss: 0.01105172
Iteration 6/25 | Loss: 0.01105172
Iteration 7/25 | Loss: 0.01105172
Iteration 8/25 | Loss: 0.01105171
Iteration 9/25 | Loss: 0.01105171
Iteration 10/25 | Loss: 0.01105171
Iteration 11/25 | Loss: 0.01105171
Iteration 12/25 | Loss: 0.01105171
Iteration 13/25 | Loss: 0.01105170
Iteration 14/25 | Loss: 0.01105170
Iteration 15/25 | Loss: 0.01105169
Iteration 16/25 | Loss: 0.01105169
Iteration 17/25 | Loss: 0.01105169
Iteration 18/25 | Loss: 0.01105169
Iteration 19/25 | Loss: 0.01105168
Iteration 20/25 | Loss: 0.01105168
Iteration 21/25 | Loss: 0.01105168
Iteration 22/25 | Loss: 0.01105168
Iteration 23/25 | Loss: 0.01105168
Iteration 24/25 | Loss: 0.01105168
Iteration 25/25 | Loss: 0.01105168

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30964112
Iteration 2/25 | Loss: 0.17776547
Iteration 3/25 | Loss: 0.17568763
Iteration 4/25 | Loss: 0.17568758
Iteration 5/25 | Loss: 0.17568757
Iteration 6/25 | Loss: 0.17568757
Iteration 7/25 | Loss: 0.17568758
Iteration 8/25 | Loss: 0.17568758
Iteration 9/25 | Loss: 0.17568758
Iteration 10/25 | Loss: 0.17568757
Iteration 11/25 | Loss: 0.17568757
Iteration 12/25 | Loss: 0.17568757
Iteration 13/25 | Loss: 0.17568757
Iteration 14/25 | Loss: 0.17568757
Iteration 15/25 | Loss: 0.17568757
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.17568756639957428, 0.17568756639957428, 0.17568756639957428, 0.17568756639957428, 0.17568756639957428]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.17568756639957428

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17568757
Iteration 2/1000 | Loss: 0.00351895
Iteration 3/1000 | Loss: 0.00128824
Iteration 4/1000 | Loss: 0.00180607
Iteration 5/1000 | Loss: 0.00202277
Iteration 6/1000 | Loss: 0.00032487
Iteration 7/1000 | Loss: 0.00018530
Iteration 8/1000 | Loss: 0.00012656
Iteration 9/1000 | Loss: 0.00105538
Iteration 10/1000 | Loss: 0.00196499
Iteration 11/1000 | Loss: 0.00012143
Iteration 12/1000 | Loss: 0.00027307
Iteration 13/1000 | Loss: 0.00037465
Iteration 14/1000 | Loss: 0.00006798
Iteration 15/1000 | Loss: 0.00063060
Iteration 16/1000 | Loss: 0.00010098
Iteration 17/1000 | Loss: 0.00008476
Iteration 18/1000 | Loss: 0.00004014
Iteration 19/1000 | Loss: 0.00003372
Iteration 20/1000 | Loss: 0.00072175
Iteration 21/1000 | Loss: 0.00004422
Iteration 22/1000 | Loss: 0.00003575
Iteration 23/1000 | Loss: 0.00002985
Iteration 24/1000 | Loss: 0.00002809
Iteration 25/1000 | Loss: 0.00002684
Iteration 26/1000 | Loss: 0.00002594
Iteration 27/1000 | Loss: 0.00002548
Iteration 28/1000 | Loss: 0.00002513
Iteration 29/1000 | Loss: 0.00002483
Iteration 30/1000 | Loss: 0.00002451
Iteration 31/1000 | Loss: 0.00002444
Iteration 32/1000 | Loss: 0.00002435
Iteration 33/1000 | Loss: 0.00002418
Iteration 34/1000 | Loss: 0.00002416
Iteration 35/1000 | Loss: 0.00002413
Iteration 36/1000 | Loss: 0.00002412
Iteration 37/1000 | Loss: 0.00002411
Iteration 38/1000 | Loss: 0.00002411
Iteration 39/1000 | Loss: 0.00002411
Iteration 40/1000 | Loss: 0.00002409
Iteration 41/1000 | Loss: 0.00002409
Iteration 42/1000 | Loss: 0.00002409
Iteration 43/1000 | Loss: 0.00002407
Iteration 44/1000 | Loss: 0.00002407
Iteration 45/1000 | Loss: 0.00002407
Iteration 46/1000 | Loss: 0.00002406
Iteration 47/1000 | Loss: 0.00002406
Iteration 48/1000 | Loss: 0.00002403
Iteration 49/1000 | Loss: 0.00002403
Iteration 50/1000 | Loss: 0.00002403
Iteration 51/1000 | Loss: 0.00002403
Iteration 52/1000 | Loss: 0.00002403
Iteration 53/1000 | Loss: 0.00002402
Iteration 54/1000 | Loss: 0.00002401
Iteration 55/1000 | Loss: 0.00002396
Iteration 56/1000 | Loss: 0.00002396
Iteration 57/1000 | Loss: 0.00002396
Iteration 58/1000 | Loss: 0.00002396
Iteration 59/1000 | Loss: 0.00002396
Iteration 60/1000 | Loss: 0.00002396
Iteration 61/1000 | Loss: 0.00002395
Iteration 62/1000 | Loss: 0.00002395
Iteration 63/1000 | Loss: 0.00002395
Iteration 64/1000 | Loss: 0.00002395
Iteration 65/1000 | Loss: 0.00002395
Iteration 66/1000 | Loss: 0.00002395
Iteration 67/1000 | Loss: 0.00002395
Iteration 68/1000 | Loss: 0.00002394
Iteration 69/1000 | Loss: 0.00002393
Iteration 70/1000 | Loss: 0.00002393
Iteration 71/1000 | Loss: 0.00002392
Iteration 72/1000 | Loss: 0.00002390
Iteration 73/1000 | Loss: 0.00002390
Iteration 74/1000 | Loss: 0.00002386
Iteration 75/1000 | Loss: 0.00002386
Iteration 76/1000 | Loss: 0.00002386
Iteration 77/1000 | Loss: 0.00002385
Iteration 78/1000 | Loss: 0.00002385
Iteration 79/1000 | Loss: 0.00002385
Iteration 80/1000 | Loss: 0.00002385
Iteration 81/1000 | Loss: 0.00002385
Iteration 82/1000 | Loss: 0.00002385
Iteration 83/1000 | Loss: 0.00002384
Iteration 84/1000 | Loss: 0.00002384
Iteration 85/1000 | Loss: 0.00002384
Iteration 86/1000 | Loss: 0.00002384
Iteration 87/1000 | Loss: 0.00002383
Iteration 88/1000 | Loss: 0.00002383
Iteration 89/1000 | Loss: 0.00002382
Iteration 90/1000 | Loss: 0.00002382
Iteration 91/1000 | Loss: 0.00002380
Iteration 92/1000 | Loss: 0.00002380
Iteration 93/1000 | Loss: 0.00002379
Iteration 94/1000 | Loss: 0.00002379
Iteration 95/1000 | Loss: 0.00002379
Iteration 96/1000 | Loss: 0.00002378
Iteration 97/1000 | Loss: 0.00002378
Iteration 98/1000 | Loss: 0.00002378
Iteration 99/1000 | Loss: 0.00002378
Iteration 100/1000 | Loss: 0.00002378
Iteration 101/1000 | Loss: 0.00002378
Iteration 102/1000 | Loss: 0.00002377
Iteration 103/1000 | Loss: 0.00002377
Iteration 104/1000 | Loss: 0.00002377
Iteration 105/1000 | Loss: 0.00002377
Iteration 106/1000 | Loss: 0.00002377
Iteration 107/1000 | Loss: 0.00002377
Iteration 108/1000 | Loss: 0.00002377
Iteration 109/1000 | Loss: 0.00002376
Iteration 110/1000 | Loss: 0.00002376
Iteration 111/1000 | Loss: 0.00002376
Iteration 112/1000 | Loss: 0.00002376
Iteration 113/1000 | Loss: 0.00002375
Iteration 114/1000 | Loss: 0.00002375
Iteration 115/1000 | Loss: 0.00002375
Iteration 116/1000 | Loss: 0.00002375
Iteration 117/1000 | Loss: 0.00002375
Iteration 118/1000 | Loss: 0.00002374
Iteration 119/1000 | Loss: 0.00002374
Iteration 120/1000 | Loss: 0.00002374
Iteration 121/1000 | Loss: 0.00002374
Iteration 122/1000 | Loss: 0.00002374
Iteration 123/1000 | Loss: 0.00002374
Iteration 124/1000 | Loss: 0.00002374
Iteration 125/1000 | Loss: 0.00002374
Iteration 126/1000 | Loss: 0.00002374
Iteration 127/1000 | Loss: 0.00002374
Iteration 128/1000 | Loss: 0.00002374
Iteration 129/1000 | Loss: 0.00002373
Iteration 130/1000 | Loss: 0.00002373
Iteration 131/1000 | Loss: 0.00002373
Iteration 132/1000 | Loss: 0.00002373
Iteration 133/1000 | Loss: 0.00002373
Iteration 134/1000 | Loss: 0.00002373
Iteration 135/1000 | Loss: 0.00002373
Iteration 136/1000 | Loss: 0.00002373
Iteration 137/1000 | Loss: 0.00002373
Iteration 138/1000 | Loss: 0.00002373
Iteration 139/1000 | Loss: 0.00002373
Iteration 140/1000 | Loss: 0.00002373
Iteration 141/1000 | Loss: 0.00002373
Iteration 142/1000 | Loss: 0.00002373
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [2.3730655811959878e-05, 2.3730655811959878e-05, 2.3730655811959878e-05, 2.3730655811959878e-05, 2.3730655811959878e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3730655811959878e-05

Optimization complete. Final v2v error: 3.9384942054748535 mm

Highest mean error: 4.80927038192749 mm for frame 76

Lowest mean error: 3.4574828147888184 mm for frame 23

Saving results

Total time: 72.98836731910706
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00805433
Iteration 2/25 | Loss: 0.00143030
Iteration 3/25 | Loss: 0.00131741
Iteration 4/25 | Loss: 0.00130653
Iteration 5/25 | Loss: 0.00130348
Iteration 6/25 | Loss: 0.00130348
Iteration 7/25 | Loss: 0.00130348
Iteration 8/25 | Loss: 0.00130348
Iteration 9/25 | Loss: 0.00130348
Iteration 10/25 | Loss: 0.00130348
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013034803560003638, 0.0013034803560003638, 0.0013034803560003638, 0.0013034803560003638, 0.0013034803560003638]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013034803560003638

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 11.52409267
Iteration 2/25 | Loss: 0.00079418
Iteration 3/25 | Loss: 0.00079408
Iteration 4/25 | Loss: 0.00079407
Iteration 5/25 | Loss: 0.00079407
Iteration 6/25 | Loss: 0.00079407
Iteration 7/25 | Loss: 0.00079407
Iteration 8/25 | Loss: 0.00079407
Iteration 9/25 | Loss: 0.00079407
Iteration 10/25 | Loss: 0.00079407
Iteration 11/25 | Loss: 0.00079407
Iteration 12/25 | Loss: 0.00079407
Iteration 13/25 | Loss: 0.00079407
Iteration 14/25 | Loss: 0.00079407
Iteration 15/25 | Loss: 0.00079407
Iteration 16/25 | Loss: 0.00079407
Iteration 17/25 | Loss: 0.00079407
Iteration 18/25 | Loss: 0.00079407
Iteration 19/25 | Loss: 0.00079407
Iteration 20/25 | Loss: 0.00079407
Iteration 21/25 | Loss: 0.00079407
Iteration 22/25 | Loss: 0.00079407
Iteration 23/25 | Loss: 0.00079407
Iteration 24/25 | Loss: 0.00079407
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.000794071820564568, 0.000794071820564568, 0.000794071820564568, 0.000794071820564568, 0.000794071820564568]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000794071820564568

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079407
Iteration 2/1000 | Loss: 0.00003818
Iteration 3/1000 | Loss: 0.00002639
Iteration 4/1000 | Loss: 0.00002427
Iteration 5/1000 | Loss: 0.00002316
Iteration 6/1000 | Loss: 0.00002223
Iteration 7/1000 | Loss: 0.00002161
Iteration 8/1000 | Loss: 0.00002117
Iteration 9/1000 | Loss: 0.00002069
Iteration 10/1000 | Loss: 0.00002040
Iteration 11/1000 | Loss: 0.00002016
Iteration 12/1000 | Loss: 0.00001996
Iteration 13/1000 | Loss: 0.00001992
Iteration 14/1000 | Loss: 0.00001984
Iteration 15/1000 | Loss: 0.00001974
Iteration 16/1000 | Loss: 0.00001973
Iteration 17/1000 | Loss: 0.00001971
Iteration 18/1000 | Loss: 0.00001965
Iteration 19/1000 | Loss: 0.00001959
Iteration 20/1000 | Loss: 0.00001955
Iteration 21/1000 | Loss: 0.00001954
Iteration 22/1000 | Loss: 0.00001954
Iteration 23/1000 | Loss: 0.00001953
Iteration 24/1000 | Loss: 0.00001953
Iteration 25/1000 | Loss: 0.00001952
Iteration 26/1000 | Loss: 0.00001949
Iteration 27/1000 | Loss: 0.00001949
Iteration 28/1000 | Loss: 0.00001947
Iteration 29/1000 | Loss: 0.00001941
Iteration 30/1000 | Loss: 0.00001939
Iteration 31/1000 | Loss: 0.00001938
Iteration 32/1000 | Loss: 0.00001937
Iteration 33/1000 | Loss: 0.00001937
Iteration 34/1000 | Loss: 0.00001933
Iteration 35/1000 | Loss: 0.00001933
Iteration 36/1000 | Loss: 0.00001932
Iteration 37/1000 | Loss: 0.00001930
Iteration 38/1000 | Loss: 0.00001929
Iteration 39/1000 | Loss: 0.00001929
Iteration 40/1000 | Loss: 0.00001928
Iteration 41/1000 | Loss: 0.00001928
Iteration 42/1000 | Loss: 0.00001927
Iteration 43/1000 | Loss: 0.00001927
Iteration 44/1000 | Loss: 0.00001926
Iteration 45/1000 | Loss: 0.00001926
Iteration 46/1000 | Loss: 0.00001924
Iteration 47/1000 | Loss: 0.00001923
Iteration 48/1000 | Loss: 0.00001923
Iteration 49/1000 | Loss: 0.00001923
Iteration 50/1000 | Loss: 0.00001922
Iteration 51/1000 | Loss: 0.00001921
Iteration 52/1000 | Loss: 0.00001921
Iteration 53/1000 | Loss: 0.00001921
Iteration 54/1000 | Loss: 0.00001921
Iteration 55/1000 | Loss: 0.00001921
Iteration 56/1000 | Loss: 0.00001919
Iteration 57/1000 | Loss: 0.00001918
Iteration 58/1000 | Loss: 0.00001918
Iteration 59/1000 | Loss: 0.00001917
Iteration 60/1000 | Loss: 0.00001917
Iteration 61/1000 | Loss: 0.00001917
Iteration 62/1000 | Loss: 0.00001917
Iteration 63/1000 | Loss: 0.00001916
Iteration 64/1000 | Loss: 0.00001916
Iteration 65/1000 | Loss: 0.00001915
Iteration 66/1000 | Loss: 0.00001915
Iteration 67/1000 | Loss: 0.00001914
Iteration 68/1000 | Loss: 0.00001914
Iteration 69/1000 | Loss: 0.00001914
Iteration 70/1000 | Loss: 0.00001914
Iteration 71/1000 | Loss: 0.00001914
Iteration 72/1000 | Loss: 0.00001914
Iteration 73/1000 | Loss: 0.00001914
Iteration 74/1000 | Loss: 0.00001914
Iteration 75/1000 | Loss: 0.00001914
Iteration 76/1000 | Loss: 0.00001914
Iteration 77/1000 | Loss: 0.00001914
Iteration 78/1000 | Loss: 0.00001914
Iteration 79/1000 | Loss: 0.00001913
Iteration 80/1000 | Loss: 0.00001913
Iteration 81/1000 | Loss: 0.00001913
Iteration 82/1000 | Loss: 0.00001913
Iteration 83/1000 | Loss: 0.00001912
Iteration 84/1000 | Loss: 0.00001912
Iteration 85/1000 | Loss: 0.00001912
Iteration 86/1000 | Loss: 0.00001912
Iteration 87/1000 | Loss: 0.00001912
Iteration 88/1000 | Loss: 0.00001912
Iteration 89/1000 | Loss: 0.00001912
Iteration 90/1000 | Loss: 0.00001912
Iteration 91/1000 | Loss: 0.00001911
Iteration 92/1000 | Loss: 0.00001911
Iteration 93/1000 | Loss: 0.00001911
Iteration 94/1000 | Loss: 0.00001911
Iteration 95/1000 | Loss: 0.00001911
Iteration 96/1000 | Loss: 0.00001911
Iteration 97/1000 | Loss: 0.00001911
Iteration 98/1000 | Loss: 0.00001911
Iteration 99/1000 | Loss: 0.00001911
Iteration 100/1000 | Loss: 0.00001911
Iteration 101/1000 | Loss: 0.00001911
Iteration 102/1000 | Loss: 0.00001911
Iteration 103/1000 | Loss: 0.00001911
Iteration 104/1000 | Loss: 0.00001911
Iteration 105/1000 | Loss: 0.00001910
Iteration 106/1000 | Loss: 0.00001910
Iteration 107/1000 | Loss: 0.00001910
Iteration 108/1000 | Loss: 0.00001910
Iteration 109/1000 | Loss: 0.00001910
Iteration 110/1000 | Loss: 0.00001910
Iteration 111/1000 | Loss: 0.00001910
Iteration 112/1000 | Loss: 0.00001910
Iteration 113/1000 | Loss: 0.00001910
Iteration 114/1000 | Loss: 0.00001910
Iteration 115/1000 | Loss: 0.00001910
Iteration 116/1000 | Loss: 0.00001910
Iteration 117/1000 | Loss: 0.00001910
Iteration 118/1000 | Loss: 0.00001910
Iteration 119/1000 | Loss: 0.00001910
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.9097389667876996e-05, 1.9097389667876996e-05, 1.9097389667876996e-05, 1.9097389667876996e-05, 1.9097389667876996e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9097389667876996e-05

Optimization complete. Final v2v error: 3.697566509246826 mm

Highest mean error: 4.6781907081604 mm for frame 109

Lowest mean error: 3.227428674697876 mm for frame 25

Saving results

Total time: 44.641759634017944
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00991701
Iteration 2/25 | Loss: 0.00991701
Iteration 3/25 | Loss: 0.00991701
Iteration 4/25 | Loss: 0.00991700
Iteration 5/25 | Loss: 0.00991700
Iteration 6/25 | Loss: 0.00991700
Iteration 7/25 | Loss: 0.00991699
Iteration 8/25 | Loss: 0.00991699
Iteration 9/25 | Loss: 0.00991699
Iteration 10/25 | Loss: 0.00991699
Iteration 11/25 | Loss: 0.00991699
Iteration 12/25 | Loss: 0.00991698
Iteration 13/25 | Loss: 0.00991698
Iteration 14/25 | Loss: 0.00991698
Iteration 15/25 | Loss: 0.00991698
Iteration 16/25 | Loss: 0.00991698
Iteration 17/25 | Loss: 0.00991698
Iteration 18/25 | Loss: 0.00991698
Iteration 19/25 | Loss: 0.00991697
Iteration 20/25 | Loss: 0.00991697
Iteration 21/25 | Loss: 0.00991697
Iteration 22/25 | Loss: 0.00991697
Iteration 23/25 | Loss: 0.00991697
Iteration 24/25 | Loss: 0.00991697
Iteration 25/25 | Loss: 0.00991696

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.75168443
Iteration 2/25 | Loss: 0.16552316
Iteration 3/25 | Loss: 0.16186407
Iteration 4/25 | Loss: 0.16150706
Iteration 5/25 | Loss: 0.16150677
Iteration 6/25 | Loss: 0.16150671
Iteration 7/25 | Loss: 0.16150671
Iteration 8/25 | Loss: 0.16150671
Iteration 9/25 | Loss: 0.16150670
Iteration 10/25 | Loss: 0.16150670
Iteration 11/25 | Loss: 0.16150670
Iteration 12/25 | Loss: 0.16150667
Iteration 13/25 | Loss: 0.16150670
Iteration 14/25 | Loss: 0.16150670
Iteration 15/25 | Loss: 0.16150667
Iteration 16/25 | Loss: 0.16150667
Iteration 17/25 | Loss: 0.16150667
Iteration 18/25 | Loss: 0.16150667
Iteration 19/25 | Loss: 0.16150667
Iteration 20/25 | Loss: 0.16150667
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.16150666773319244, 0.16150666773319244, 0.16150666773319244, 0.16150666773319244, 0.16150666773319244]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.16150666773319244

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.16150667
Iteration 2/1000 | Loss: 0.00607348
Iteration 3/1000 | Loss: 0.00291965
Iteration 4/1000 | Loss: 0.00073095
Iteration 5/1000 | Loss: 0.00037675
Iteration 6/1000 | Loss: 0.00047145
Iteration 7/1000 | Loss: 0.00044752
Iteration 8/1000 | Loss: 0.00043088
Iteration 9/1000 | Loss: 0.00016742
Iteration 10/1000 | Loss: 0.00007344
Iteration 11/1000 | Loss: 0.00006368
Iteration 12/1000 | Loss: 0.00019455
Iteration 13/1000 | Loss: 0.00005674
Iteration 14/1000 | Loss: 0.00004817
Iteration 15/1000 | Loss: 0.00049552
Iteration 16/1000 | Loss: 0.00004392
Iteration 17/1000 | Loss: 0.00004040
Iteration 18/1000 | Loss: 0.00003812
Iteration 19/1000 | Loss: 0.00003633
Iteration 20/1000 | Loss: 0.00003442
Iteration 21/1000 | Loss: 0.00042620
Iteration 22/1000 | Loss: 0.00060859
Iteration 23/1000 | Loss: 0.00028759
Iteration 24/1000 | Loss: 0.00004388
Iteration 25/1000 | Loss: 0.00003725
Iteration 26/1000 | Loss: 0.00003201
Iteration 27/1000 | Loss: 0.00009744
Iteration 28/1000 | Loss: 0.00002970
Iteration 29/1000 | Loss: 0.00002908
Iteration 30/1000 | Loss: 0.00002843
Iteration 31/1000 | Loss: 0.00015112
Iteration 32/1000 | Loss: 0.00057471
Iteration 33/1000 | Loss: 0.00050110
Iteration 34/1000 | Loss: 0.00077911
Iteration 35/1000 | Loss: 0.00017771
Iteration 36/1000 | Loss: 0.00003041
Iteration 37/1000 | Loss: 0.00021342
Iteration 38/1000 | Loss: 0.00002734
Iteration 39/1000 | Loss: 0.00002659
Iteration 40/1000 | Loss: 0.00020013
Iteration 41/1000 | Loss: 0.00011139
Iteration 42/1000 | Loss: 0.00002608
Iteration 43/1000 | Loss: 0.00015615
Iteration 44/1000 | Loss: 0.00002613
Iteration 45/1000 | Loss: 0.00002543
Iteration 46/1000 | Loss: 0.00002520
Iteration 47/1000 | Loss: 0.00002497
Iteration 48/1000 | Loss: 0.00002486
Iteration 49/1000 | Loss: 0.00002486
Iteration 50/1000 | Loss: 0.00002485
Iteration 51/1000 | Loss: 0.00002485
Iteration 52/1000 | Loss: 0.00002484
Iteration 53/1000 | Loss: 0.00002479
Iteration 54/1000 | Loss: 0.00002479
Iteration 55/1000 | Loss: 0.00002477
Iteration 56/1000 | Loss: 0.00002476
Iteration 57/1000 | Loss: 0.00002471
Iteration 58/1000 | Loss: 0.00002465
Iteration 59/1000 | Loss: 0.00002458
Iteration 60/1000 | Loss: 0.00002457
Iteration 61/1000 | Loss: 0.00002457
Iteration 62/1000 | Loss: 0.00002457
Iteration 63/1000 | Loss: 0.00002456
Iteration 64/1000 | Loss: 0.00002456
Iteration 65/1000 | Loss: 0.00002455
Iteration 66/1000 | Loss: 0.00002454
Iteration 67/1000 | Loss: 0.00002453
Iteration 68/1000 | Loss: 0.00002451
Iteration 69/1000 | Loss: 0.00002451
Iteration 70/1000 | Loss: 0.00002451
Iteration 71/1000 | Loss: 0.00002450
Iteration 72/1000 | Loss: 0.00002449
Iteration 73/1000 | Loss: 0.00002449
Iteration 74/1000 | Loss: 0.00002449
Iteration 75/1000 | Loss: 0.00002449
Iteration 76/1000 | Loss: 0.00002449
Iteration 77/1000 | Loss: 0.00002448
Iteration 78/1000 | Loss: 0.00002448
Iteration 79/1000 | Loss: 0.00002448
Iteration 80/1000 | Loss: 0.00002448
Iteration 81/1000 | Loss: 0.00002448
Iteration 82/1000 | Loss: 0.00002448
Iteration 83/1000 | Loss: 0.00002448
Iteration 84/1000 | Loss: 0.00002448
Iteration 85/1000 | Loss: 0.00002447
Iteration 86/1000 | Loss: 0.00002447
Iteration 87/1000 | Loss: 0.00002447
Iteration 88/1000 | Loss: 0.00002446
Iteration 89/1000 | Loss: 0.00002446
Iteration 90/1000 | Loss: 0.00002446
Iteration 91/1000 | Loss: 0.00002445
Iteration 92/1000 | Loss: 0.00002445
Iteration 93/1000 | Loss: 0.00002441
Iteration 94/1000 | Loss: 0.00002441
Iteration 95/1000 | Loss: 0.00002440
Iteration 96/1000 | Loss: 0.00002437
Iteration 97/1000 | Loss: 0.00002437
Iteration 98/1000 | Loss: 0.00002437
Iteration 99/1000 | Loss: 0.00002437
Iteration 100/1000 | Loss: 0.00002437
Iteration 101/1000 | Loss: 0.00002437
Iteration 102/1000 | Loss: 0.00002437
Iteration 103/1000 | Loss: 0.00002436
Iteration 104/1000 | Loss: 0.00002436
Iteration 105/1000 | Loss: 0.00002436
Iteration 106/1000 | Loss: 0.00002436
Iteration 107/1000 | Loss: 0.00002436
Iteration 108/1000 | Loss: 0.00002436
Iteration 109/1000 | Loss: 0.00002436
Iteration 110/1000 | Loss: 0.00002436
Iteration 111/1000 | Loss: 0.00002435
Iteration 112/1000 | Loss: 0.00002435
Iteration 113/1000 | Loss: 0.00002435
Iteration 114/1000 | Loss: 0.00002434
Iteration 115/1000 | Loss: 0.00002433
Iteration 116/1000 | Loss: 0.00002433
Iteration 117/1000 | Loss: 0.00002433
Iteration 118/1000 | Loss: 0.00002433
Iteration 119/1000 | Loss: 0.00002433
Iteration 120/1000 | Loss: 0.00002433
Iteration 121/1000 | Loss: 0.00002433
Iteration 122/1000 | Loss: 0.00002433
Iteration 123/1000 | Loss: 0.00002433
Iteration 124/1000 | Loss: 0.00002433
Iteration 125/1000 | Loss: 0.00002432
Iteration 126/1000 | Loss: 0.00002432
Iteration 127/1000 | Loss: 0.00002432
Iteration 128/1000 | Loss: 0.00002432
Iteration 129/1000 | Loss: 0.00002432
Iteration 130/1000 | Loss: 0.00002432
Iteration 131/1000 | Loss: 0.00002432
Iteration 132/1000 | Loss: 0.00002431
Iteration 133/1000 | Loss: 0.00002431
Iteration 134/1000 | Loss: 0.00002431
Iteration 135/1000 | Loss: 0.00002431
Iteration 136/1000 | Loss: 0.00002430
Iteration 137/1000 | Loss: 0.00002430
Iteration 138/1000 | Loss: 0.00002430
Iteration 139/1000 | Loss: 0.00002429
Iteration 140/1000 | Loss: 0.00002429
Iteration 141/1000 | Loss: 0.00002429
Iteration 142/1000 | Loss: 0.00002429
Iteration 143/1000 | Loss: 0.00002429
Iteration 144/1000 | Loss: 0.00002428
Iteration 145/1000 | Loss: 0.00002428
Iteration 146/1000 | Loss: 0.00002428
Iteration 147/1000 | Loss: 0.00002428
Iteration 148/1000 | Loss: 0.00002428
Iteration 149/1000 | Loss: 0.00002428
Iteration 150/1000 | Loss: 0.00002427
Iteration 151/1000 | Loss: 0.00002427
Iteration 152/1000 | Loss: 0.00002427
Iteration 153/1000 | Loss: 0.00002427
Iteration 154/1000 | Loss: 0.00002427
Iteration 155/1000 | Loss: 0.00002427
Iteration 156/1000 | Loss: 0.00002427
Iteration 157/1000 | Loss: 0.00002427
Iteration 158/1000 | Loss: 0.00002427
Iteration 159/1000 | Loss: 0.00002427
Iteration 160/1000 | Loss: 0.00002427
Iteration 161/1000 | Loss: 0.00002427
Iteration 162/1000 | Loss: 0.00002426
Iteration 163/1000 | Loss: 0.00002426
Iteration 164/1000 | Loss: 0.00002426
Iteration 165/1000 | Loss: 0.00002426
Iteration 166/1000 | Loss: 0.00002426
Iteration 167/1000 | Loss: 0.00002426
Iteration 168/1000 | Loss: 0.00002426
Iteration 169/1000 | Loss: 0.00002426
Iteration 170/1000 | Loss: 0.00002426
Iteration 171/1000 | Loss: 0.00002426
Iteration 172/1000 | Loss: 0.00002426
Iteration 173/1000 | Loss: 0.00002426
Iteration 174/1000 | Loss: 0.00002426
Iteration 175/1000 | Loss: 0.00002426
Iteration 176/1000 | Loss: 0.00002426
Iteration 177/1000 | Loss: 0.00002426
Iteration 178/1000 | Loss: 0.00002426
Iteration 179/1000 | Loss: 0.00002426
Iteration 180/1000 | Loss: 0.00002426
Iteration 181/1000 | Loss: 0.00002426
Iteration 182/1000 | Loss: 0.00002426
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [2.425817001494579e-05, 2.425817001494579e-05, 2.425817001494579e-05, 2.425817001494579e-05, 2.425817001494579e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.425817001494579e-05

Optimization complete. Final v2v error: 4.209557056427002 mm

Highest mean error: 4.790494918823242 mm for frame 230

Lowest mean error: 3.89089035987854 mm for frame 145

Saving results

Total time: 101.69736576080322
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00811618
Iteration 2/25 | Loss: 0.00154667
Iteration 3/25 | Loss: 0.00134950
Iteration 4/25 | Loss: 0.00133498
Iteration 5/25 | Loss: 0.00133209
Iteration 6/25 | Loss: 0.00133209
Iteration 7/25 | Loss: 0.00133209
Iteration 8/25 | Loss: 0.00133209
Iteration 9/25 | Loss: 0.00133209
Iteration 10/25 | Loss: 0.00133209
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013320944271981716, 0.0013320944271981716, 0.0013320944271981716, 0.0013320944271981716, 0.0013320944271981716]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013320944271981716

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.95542264
Iteration 2/25 | Loss: 0.00065715
Iteration 3/25 | Loss: 0.00065715
Iteration 4/25 | Loss: 0.00065715
Iteration 5/25 | Loss: 0.00065715
Iteration 6/25 | Loss: 0.00065715
Iteration 7/25 | Loss: 0.00065715
Iteration 8/25 | Loss: 0.00065715
Iteration 9/25 | Loss: 0.00065714
Iteration 10/25 | Loss: 0.00065714
Iteration 11/25 | Loss: 0.00065714
Iteration 12/25 | Loss: 0.00065714
Iteration 13/25 | Loss: 0.00065714
Iteration 14/25 | Loss: 0.00065714
Iteration 15/25 | Loss: 0.00065714
Iteration 16/25 | Loss: 0.00065714
Iteration 17/25 | Loss: 0.00065714
Iteration 18/25 | Loss: 0.00065714
Iteration 19/25 | Loss: 0.00065714
Iteration 20/25 | Loss: 0.00065714
Iteration 21/25 | Loss: 0.00065714
Iteration 22/25 | Loss: 0.00065714
Iteration 23/25 | Loss: 0.00065714
Iteration 24/25 | Loss: 0.00065714
Iteration 25/25 | Loss: 0.00065714

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065714
Iteration 2/1000 | Loss: 0.00003540
Iteration 3/1000 | Loss: 0.00002817
Iteration 4/1000 | Loss: 0.00002616
Iteration 5/1000 | Loss: 0.00002517
Iteration 6/1000 | Loss: 0.00002456
Iteration 7/1000 | Loss: 0.00002429
Iteration 8/1000 | Loss: 0.00002387
Iteration 9/1000 | Loss: 0.00002362
Iteration 10/1000 | Loss: 0.00002354
Iteration 11/1000 | Loss: 0.00002344
Iteration 12/1000 | Loss: 0.00002343
Iteration 13/1000 | Loss: 0.00002328
Iteration 14/1000 | Loss: 0.00002327
Iteration 15/1000 | Loss: 0.00002326
Iteration 16/1000 | Loss: 0.00002323
Iteration 17/1000 | Loss: 0.00002323
Iteration 18/1000 | Loss: 0.00002323
Iteration 19/1000 | Loss: 0.00002323
Iteration 20/1000 | Loss: 0.00002322
Iteration 21/1000 | Loss: 0.00002322
Iteration 22/1000 | Loss: 0.00002321
Iteration 23/1000 | Loss: 0.00002315
Iteration 24/1000 | Loss: 0.00002313
Iteration 25/1000 | Loss: 0.00002311
Iteration 26/1000 | Loss: 0.00002311
Iteration 27/1000 | Loss: 0.00002311
Iteration 28/1000 | Loss: 0.00002311
Iteration 29/1000 | Loss: 0.00002311
Iteration 30/1000 | Loss: 0.00002311
Iteration 31/1000 | Loss: 0.00002310
Iteration 32/1000 | Loss: 0.00002310
Iteration 33/1000 | Loss: 0.00002309
Iteration 34/1000 | Loss: 0.00002309
Iteration 35/1000 | Loss: 0.00002309
Iteration 36/1000 | Loss: 0.00002309
Iteration 37/1000 | Loss: 0.00002309
Iteration 38/1000 | Loss: 0.00002307
Iteration 39/1000 | Loss: 0.00002307
Iteration 40/1000 | Loss: 0.00002307
Iteration 41/1000 | Loss: 0.00002307
Iteration 42/1000 | Loss: 0.00002307
Iteration 43/1000 | Loss: 0.00002306
Iteration 44/1000 | Loss: 0.00002306
Iteration 45/1000 | Loss: 0.00002306
Iteration 46/1000 | Loss: 0.00002306
Iteration 47/1000 | Loss: 0.00002306
Iteration 48/1000 | Loss: 0.00002306
Iteration 49/1000 | Loss: 0.00002306
Iteration 50/1000 | Loss: 0.00002306
Iteration 51/1000 | Loss: 0.00002306
Iteration 52/1000 | Loss: 0.00002306
Iteration 53/1000 | Loss: 0.00002306
Iteration 54/1000 | Loss: 0.00002306
Iteration 55/1000 | Loss: 0.00002305
Iteration 56/1000 | Loss: 0.00002305
Iteration 57/1000 | Loss: 0.00002305
Iteration 58/1000 | Loss: 0.00002305
Iteration 59/1000 | Loss: 0.00002305
Iteration 60/1000 | Loss: 0.00002304
Iteration 61/1000 | Loss: 0.00002303
Iteration 62/1000 | Loss: 0.00002302
Iteration 63/1000 | Loss: 0.00002302
Iteration 64/1000 | Loss: 0.00002302
Iteration 65/1000 | Loss: 0.00002301
Iteration 66/1000 | Loss: 0.00002301
Iteration 67/1000 | Loss: 0.00002301
Iteration 68/1000 | Loss: 0.00002301
Iteration 69/1000 | Loss: 0.00002300
Iteration 70/1000 | Loss: 0.00002300
Iteration 71/1000 | Loss: 0.00002300
Iteration 72/1000 | Loss: 0.00002300
Iteration 73/1000 | Loss: 0.00002300
Iteration 74/1000 | Loss: 0.00002300
Iteration 75/1000 | Loss: 0.00002300
Iteration 76/1000 | Loss: 0.00002300
Iteration 77/1000 | Loss: 0.00002300
Iteration 78/1000 | Loss: 0.00002300
Iteration 79/1000 | Loss: 0.00002300
Iteration 80/1000 | Loss: 0.00002300
Iteration 81/1000 | Loss: 0.00002299
Iteration 82/1000 | Loss: 0.00002299
Iteration 83/1000 | Loss: 0.00002298
Iteration 84/1000 | Loss: 0.00002298
Iteration 85/1000 | Loss: 0.00002298
Iteration 86/1000 | Loss: 0.00002297
Iteration 87/1000 | Loss: 0.00002297
Iteration 88/1000 | Loss: 0.00002297
Iteration 89/1000 | Loss: 0.00002297
Iteration 90/1000 | Loss: 0.00002297
Iteration 91/1000 | Loss: 0.00002297
Iteration 92/1000 | Loss: 0.00002297
Iteration 93/1000 | Loss: 0.00002297
Iteration 94/1000 | Loss: 0.00002297
Iteration 95/1000 | Loss: 0.00002297
Iteration 96/1000 | Loss: 0.00002297
Iteration 97/1000 | Loss: 0.00002296
Iteration 98/1000 | Loss: 0.00002295
Iteration 99/1000 | Loss: 0.00002295
Iteration 100/1000 | Loss: 0.00002294
Iteration 101/1000 | Loss: 0.00002294
Iteration 102/1000 | Loss: 0.00002294
Iteration 103/1000 | Loss: 0.00002293
Iteration 104/1000 | Loss: 0.00002293
Iteration 105/1000 | Loss: 0.00002293
Iteration 106/1000 | Loss: 0.00002293
Iteration 107/1000 | Loss: 0.00002293
Iteration 108/1000 | Loss: 0.00002293
Iteration 109/1000 | Loss: 0.00002292
Iteration 110/1000 | Loss: 0.00002292
Iteration 111/1000 | Loss: 0.00002292
Iteration 112/1000 | Loss: 0.00002292
Iteration 113/1000 | Loss: 0.00002291
Iteration 114/1000 | Loss: 0.00002291
Iteration 115/1000 | Loss: 0.00002291
Iteration 116/1000 | Loss: 0.00002291
Iteration 117/1000 | Loss: 0.00002291
Iteration 118/1000 | Loss: 0.00002290
Iteration 119/1000 | Loss: 0.00002290
Iteration 120/1000 | Loss: 0.00002289
Iteration 121/1000 | Loss: 0.00002288
Iteration 122/1000 | Loss: 0.00002288
Iteration 123/1000 | Loss: 0.00002288
Iteration 124/1000 | Loss: 0.00002288
Iteration 125/1000 | Loss: 0.00002287
Iteration 126/1000 | Loss: 0.00002287
Iteration 127/1000 | Loss: 0.00002287
Iteration 128/1000 | Loss: 0.00002287
Iteration 129/1000 | Loss: 0.00002287
Iteration 130/1000 | Loss: 0.00002287
Iteration 131/1000 | Loss: 0.00002287
Iteration 132/1000 | Loss: 0.00002286
Iteration 133/1000 | Loss: 0.00002286
Iteration 134/1000 | Loss: 0.00002286
Iteration 135/1000 | Loss: 0.00002285
Iteration 136/1000 | Loss: 0.00002285
Iteration 137/1000 | Loss: 0.00002285
Iteration 138/1000 | Loss: 0.00002285
Iteration 139/1000 | Loss: 0.00002285
Iteration 140/1000 | Loss: 0.00002285
Iteration 141/1000 | Loss: 0.00002285
Iteration 142/1000 | Loss: 0.00002285
Iteration 143/1000 | Loss: 0.00002284
Iteration 144/1000 | Loss: 0.00002284
Iteration 145/1000 | Loss: 0.00002284
Iteration 146/1000 | Loss: 0.00002284
Iteration 147/1000 | Loss: 0.00002284
Iteration 148/1000 | Loss: 0.00002284
Iteration 149/1000 | Loss: 0.00002283
Iteration 150/1000 | Loss: 0.00002283
Iteration 151/1000 | Loss: 0.00002283
Iteration 152/1000 | Loss: 0.00002283
Iteration 153/1000 | Loss: 0.00002283
Iteration 154/1000 | Loss: 0.00002283
Iteration 155/1000 | Loss: 0.00002283
Iteration 156/1000 | Loss: 0.00002283
Iteration 157/1000 | Loss: 0.00002283
Iteration 158/1000 | Loss: 0.00002283
Iteration 159/1000 | Loss: 0.00002283
Iteration 160/1000 | Loss: 0.00002283
Iteration 161/1000 | Loss: 0.00002283
Iteration 162/1000 | Loss: 0.00002283
Iteration 163/1000 | Loss: 0.00002283
Iteration 164/1000 | Loss: 0.00002282
Iteration 165/1000 | Loss: 0.00002282
Iteration 166/1000 | Loss: 0.00002282
Iteration 167/1000 | Loss: 0.00002282
Iteration 168/1000 | Loss: 0.00002282
Iteration 169/1000 | Loss: 0.00002282
Iteration 170/1000 | Loss: 0.00002282
Iteration 171/1000 | Loss: 0.00002282
Iteration 172/1000 | Loss: 0.00002282
Iteration 173/1000 | Loss: 0.00002282
Iteration 174/1000 | Loss: 0.00002282
Iteration 175/1000 | Loss: 0.00002282
Iteration 176/1000 | Loss: 0.00002282
Iteration 177/1000 | Loss: 0.00002282
Iteration 178/1000 | Loss: 0.00002282
Iteration 179/1000 | Loss: 0.00002282
Iteration 180/1000 | Loss: 0.00002282
Iteration 181/1000 | Loss: 0.00002282
Iteration 182/1000 | Loss: 0.00002282
Iteration 183/1000 | Loss: 0.00002282
Iteration 184/1000 | Loss: 0.00002282
Iteration 185/1000 | Loss: 0.00002282
Iteration 186/1000 | Loss: 0.00002282
Iteration 187/1000 | Loss: 0.00002282
Iteration 188/1000 | Loss: 0.00002282
Iteration 189/1000 | Loss: 0.00002282
Iteration 190/1000 | Loss: 0.00002282
Iteration 191/1000 | Loss: 0.00002282
Iteration 192/1000 | Loss: 0.00002282
Iteration 193/1000 | Loss: 0.00002282
Iteration 194/1000 | Loss: 0.00002282
Iteration 195/1000 | Loss: 0.00002282
Iteration 196/1000 | Loss: 0.00002282
Iteration 197/1000 | Loss: 0.00002282
Iteration 198/1000 | Loss: 0.00002282
Iteration 199/1000 | Loss: 0.00002282
Iteration 200/1000 | Loss: 0.00002282
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [2.2818154320702888e-05, 2.2818154320702888e-05, 2.2818154320702888e-05, 2.2818154320702888e-05, 2.2818154320702888e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2818154320702888e-05

Optimization complete. Final v2v error: 3.966676950454712 mm

Highest mean error: 4.174919128417969 mm for frame 153

Lowest mean error: 3.791886568069458 mm for frame 185

Saving results

Total time: 41.32855582237244
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_022/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_022/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00998528
Iteration 2/25 | Loss: 0.00244966
Iteration 3/25 | Loss: 0.00173060
Iteration 4/25 | Loss: 0.00189100
Iteration 5/25 | Loss: 0.00149597
Iteration 6/25 | Loss: 0.00142681
Iteration 7/25 | Loss: 0.00140893
Iteration 8/25 | Loss: 0.00140666
Iteration 9/25 | Loss: 0.00140587
Iteration 10/25 | Loss: 0.00140132
Iteration 11/25 | Loss: 0.00139045
Iteration 12/25 | Loss: 0.00139241
Iteration 13/25 | Loss: 0.00138848
Iteration 14/25 | Loss: 0.00138684
Iteration 15/25 | Loss: 0.00138640
Iteration 16/25 | Loss: 0.00138634
Iteration 17/25 | Loss: 0.00138634
Iteration 18/25 | Loss: 0.00138634
Iteration 19/25 | Loss: 0.00138633
Iteration 20/25 | Loss: 0.00138633
Iteration 21/25 | Loss: 0.00138633
Iteration 22/25 | Loss: 0.00138633
Iteration 23/25 | Loss: 0.00138633
Iteration 24/25 | Loss: 0.00138633
Iteration 25/25 | Loss: 0.00138633

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39357853
Iteration 2/25 | Loss: 0.00088165
Iteration 3/25 | Loss: 0.00088164
Iteration 4/25 | Loss: 0.00086129
Iteration 5/25 | Loss: 0.00086129
Iteration 6/25 | Loss: 0.00086129
Iteration 7/25 | Loss: 0.00086129
Iteration 8/25 | Loss: 0.00086129
Iteration 9/25 | Loss: 0.00086129
Iteration 10/25 | Loss: 0.00086129
Iteration 11/25 | Loss: 0.00086129
Iteration 12/25 | Loss: 0.00086129
Iteration 13/25 | Loss: 0.00086129
Iteration 14/25 | Loss: 0.00086129
Iteration 15/25 | Loss: 0.00086129
Iteration 16/25 | Loss: 0.00086129
Iteration 17/25 | Loss: 0.00086129
Iteration 18/25 | Loss: 0.00086129
Iteration 19/25 | Loss: 0.00086129
Iteration 20/25 | Loss: 0.00086129
Iteration 21/25 | Loss: 0.00086129
Iteration 22/25 | Loss: 0.00086129
Iteration 23/25 | Loss: 0.00086129
Iteration 24/25 | Loss: 0.00086129
Iteration 25/25 | Loss: 0.00086129
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0008612911333329976, 0.0008612911333329976, 0.0008612911333329976, 0.0008612911333329976, 0.0008612911333329976]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008612911333329976

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086129
Iteration 2/1000 | Loss: 0.00012146
Iteration 3/1000 | Loss: 0.00004383
Iteration 4/1000 | Loss: 0.00003659
Iteration 5/1000 | Loss: 0.00003399
Iteration 6/1000 | Loss: 0.00003281
Iteration 7/1000 | Loss: 0.00003733
Iteration 8/1000 | Loss: 0.00006003
Iteration 9/1000 | Loss: 0.00004517
Iteration 10/1000 | Loss: 0.00009964
Iteration 11/1000 | Loss: 0.00004803
Iteration 12/1000 | Loss: 0.00003042
Iteration 13/1000 | Loss: 0.00002991
Iteration 14/1000 | Loss: 0.00006026
Iteration 15/1000 | Loss: 0.00004782
Iteration 16/1000 | Loss: 0.00008073
Iteration 17/1000 | Loss: 0.00002977
Iteration 18/1000 | Loss: 0.00002955
Iteration 19/1000 | Loss: 0.00010987
Iteration 20/1000 | Loss: 0.00006795
Iteration 21/1000 | Loss: 0.00003029
Iteration 22/1000 | Loss: 0.00005120
Iteration 23/1000 | Loss: 0.00002914
Iteration 24/1000 | Loss: 0.00003860
Iteration 25/1000 | Loss: 0.00002816
Iteration 26/1000 | Loss: 0.00002786
Iteration 27/1000 | Loss: 0.00002770
Iteration 28/1000 | Loss: 0.00002753
Iteration 29/1000 | Loss: 0.00002753
Iteration 30/1000 | Loss: 0.00002751
Iteration 31/1000 | Loss: 0.00002742
Iteration 32/1000 | Loss: 0.00002732
Iteration 33/1000 | Loss: 0.00002731
Iteration 34/1000 | Loss: 0.00002730
Iteration 35/1000 | Loss: 0.00002730
Iteration 36/1000 | Loss: 0.00002730
Iteration 37/1000 | Loss: 0.00002730
Iteration 38/1000 | Loss: 0.00002730
Iteration 39/1000 | Loss: 0.00002730
Iteration 40/1000 | Loss: 0.00002730
Iteration 41/1000 | Loss: 0.00002729
Iteration 42/1000 | Loss: 0.00002729
Iteration 43/1000 | Loss: 0.00002728
Iteration 44/1000 | Loss: 0.00002728
Iteration 45/1000 | Loss: 0.00002727
Iteration 46/1000 | Loss: 0.00002727
Iteration 47/1000 | Loss: 0.00002727
Iteration 48/1000 | Loss: 0.00002727
Iteration 49/1000 | Loss: 0.00002727
Iteration 50/1000 | Loss: 0.00002727
Iteration 51/1000 | Loss: 0.00002726
Iteration 52/1000 | Loss: 0.00002726
Iteration 53/1000 | Loss: 0.00002726
Iteration 54/1000 | Loss: 0.00002726
Iteration 55/1000 | Loss: 0.00002726
Iteration 56/1000 | Loss: 0.00002725
Iteration 57/1000 | Loss: 0.00002724
Iteration 58/1000 | Loss: 0.00002724
Iteration 59/1000 | Loss: 0.00002724
Iteration 60/1000 | Loss: 0.00002723
Iteration 61/1000 | Loss: 0.00002723
Iteration 62/1000 | Loss: 0.00002723
Iteration 63/1000 | Loss: 0.00002723
Iteration 64/1000 | Loss: 0.00002723
Iteration 65/1000 | Loss: 0.00002723
Iteration 66/1000 | Loss: 0.00002723
Iteration 67/1000 | Loss: 0.00002723
Iteration 68/1000 | Loss: 0.00002723
Iteration 69/1000 | Loss: 0.00002723
Iteration 70/1000 | Loss: 0.00002723
Iteration 71/1000 | Loss: 0.00002722
Iteration 72/1000 | Loss: 0.00002722
Iteration 73/1000 | Loss: 0.00002722
Iteration 74/1000 | Loss: 0.00002722
Iteration 75/1000 | Loss: 0.00002722
Iteration 76/1000 | Loss: 0.00002722
Iteration 77/1000 | Loss: 0.00002722
Iteration 78/1000 | Loss: 0.00002722
Iteration 79/1000 | Loss: 0.00002722
Iteration 80/1000 | Loss: 0.00002721
Iteration 81/1000 | Loss: 0.00002721
Iteration 82/1000 | Loss: 0.00002721
Iteration 83/1000 | Loss: 0.00002721
Iteration 84/1000 | Loss: 0.00002720
Iteration 85/1000 | Loss: 0.00002720
Iteration 86/1000 | Loss: 0.00002720
Iteration 87/1000 | Loss: 0.00002720
Iteration 88/1000 | Loss: 0.00002720
Iteration 89/1000 | Loss: 0.00002720
Iteration 90/1000 | Loss: 0.00002720
Iteration 91/1000 | Loss: 0.00002720
Iteration 92/1000 | Loss: 0.00002720
Iteration 93/1000 | Loss: 0.00002720
Iteration 94/1000 | Loss: 0.00002720
Iteration 95/1000 | Loss: 0.00002720
Iteration 96/1000 | Loss: 0.00002720
Iteration 97/1000 | Loss: 0.00002720
Iteration 98/1000 | Loss: 0.00002720
Iteration 99/1000 | Loss: 0.00002720
Iteration 100/1000 | Loss: 0.00002719
Iteration 101/1000 | Loss: 0.00002719
Iteration 102/1000 | Loss: 0.00002719
Iteration 103/1000 | Loss: 0.00002719
Iteration 104/1000 | Loss: 0.00002719
Iteration 105/1000 | Loss: 0.00002719
Iteration 106/1000 | Loss: 0.00002719
Iteration 107/1000 | Loss: 0.00002719
Iteration 108/1000 | Loss: 0.00002719
Iteration 109/1000 | Loss: 0.00002719
Iteration 110/1000 | Loss: 0.00002719
Iteration 111/1000 | Loss: 0.00002719
Iteration 112/1000 | Loss: 0.00002719
Iteration 113/1000 | Loss: 0.00002719
Iteration 114/1000 | Loss: 0.00002719
Iteration 115/1000 | Loss: 0.00002719
Iteration 116/1000 | Loss: 0.00002718
Iteration 117/1000 | Loss: 0.00002718
Iteration 118/1000 | Loss: 0.00002718
Iteration 119/1000 | Loss: 0.00002718
Iteration 120/1000 | Loss: 0.00002718
Iteration 121/1000 | Loss: 0.00002718
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [2.7184749342268333e-05, 2.7184749342268333e-05, 2.7184749342268333e-05, 2.7184749342268333e-05, 2.7184749342268333e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7184749342268333e-05

Optimization complete. Final v2v error: 4.386429786682129 mm

Highest mean error: 4.974515914916992 mm for frame 156

Lowest mean error: 3.9037926197052 mm for frame 0

Saving results

Total time: 76.28939700126648
