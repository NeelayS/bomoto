Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=31, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 1736-1791
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00375475
Iteration 2/25 | Loss: 0.00116504
Iteration 3/25 | Loss: 0.00097231
Iteration 4/25 | Loss: 0.00095119
Iteration 5/25 | Loss: 0.00094556
Iteration 6/25 | Loss: 0.00094451
Iteration 7/25 | Loss: 0.00094451
Iteration 8/25 | Loss: 0.00094451
Iteration 9/25 | Loss: 0.00094451
Iteration 10/25 | Loss: 0.00094451
Iteration 11/25 | Loss: 0.00094451
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009445109171792865, 0.0009445109171792865, 0.0009445109171792865, 0.0009445109171792865, 0.0009445109171792865]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009445109171792865

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.75082326
Iteration 2/25 | Loss: 0.00063671
Iteration 3/25 | Loss: 0.00063670
Iteration 4/25 | Loss: 0.00063670
Iteration 5/25 | Loss: 0.00063670
Iteration 6/25 | Loss: 0.00063670
Iteration 7/25 | Loss: 0.00063670
Iteration 8/25 | Loss: 0.00063670
Iteration 9/25 | Loss: 0.00063670
Iteration 10/25 | Loss: 0.00063670
Iteration 11/25 | Loss: 0.00063670
Iteration 12/25 | Loss: 0.00063670
Iteration 13/25 | Loss: 0.00063670
Iteration 14/25 | Loss: 0.00063670
Iteration 15/25 | Loss: 0.00063670
Iteration 16/25 | Loss: 0.00063670
Iteration 17/25 | Loss: 0.00063670
Iteration 18/25 | Loss: 0.00063670
Iteration 19/25 | Loss: 0.00063670
Iteration 20/25 | Loss: 0.00063670
Iteration 21/25 | Loss: 0.00063670
Iteration 22/25 | Loss: 0.00063670
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0006366989691741765, 0.0006366989691741765, 0.0006366989691741765, 0.0006366989691741765, 0.0006366989691741765]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006366989691741765

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063670
Iteration 2/1000 | Loss: 0.00002863
Iteration 3/1000 | Loss: 0.00001413
Iteration 4/1000 | Loss: 0.00001216
Iteration 5/1000 | Loss: 0.00001119
Iteration 6/1000 | Loss: 0.00001071
Iteration 7/1000 | Loss: 0.00001042
Iteration 8/1000 | Loss: 0.00001009
Iteration 9/1000 | Loss: 0.00000995
Iteration 10/1000 | Loss: 0.00000992
Iteration 11/1000 | Loss: 0.00000992
Iteration 12/1000 | Loss: 0.00000986
Iteration 13/1000 | Loss: 0.00000972
Iteration 14/1000 | Loss: 0.00000970
Iteration 15/1000 | Loss: 0.00000968
Iteration 16/1000 | Loss: 0.00000967
Iteration 17/1000 | Loss: 0.00000966
Iteration 18/1000 | Loss: 0.00000965
Iteration 19/1000 | Loss: 0.00000965
Iteration 20/1000 | Loss: 0.00000964
Iteration 21/1000 | Loss: 0.00000964
Iteration 22/1000 | Loss: 0.00000963
Iteration 23/1000 | Loss: 0.00000963
Iteration 24/1000 | Loss: 0.00000962
Iteration 25/1000 | Loss: 0.00000962
Iteration 26/1000 | Loss: 0.00000962
Iteration 27/1000 | Loss: 0.00000962
Iteration 28/1000 | Loss: 0.00000961
Iteration 29/1000 | Loss: 0.00000961
Iteration 30/1000 | Loss: 0.00000961
Iteration 31/1000 | Loss: 0.00000961
Iteration 32/1000 | Loss: 0.00000961
Iteration 33/1000 | Loss: 0.00000961
Iteration 34/1000 | Loss: 0.00000960
Iteration 35/1000 | Loss: 0.00000960
Iteration 36/1000 | Loss: 0.00000960
Iteration 37/1000 | Loss: 0.00000960
Iteration 38/1000 | Loss: 0.00000960
Iteration 39/1000 | Loss: 0.00000960
Iteration 40/1000 | Loss: 0.00000960
Iteration 41/1000 | Loss: 0.00000960
Iteration 42/1000 | Loss: 0.00000960
Iteration 43/1000 | Loss: 0.00000960
Iteration 44/1000 | Loss: 0.00000959
Iteration 45/1000 | Loss: 0.00000959
Iteration 46/1000 | Loss: 0.00000959
Iteration 47/1000 | Loss: 0.00000958
Iteration 48/1000 | Loss: 0.00000958
Iteration 49/1000 | Loss: 0.00000958
Iteration 50/1000 | Loss: 0.00000957
Iteration 51/1000 | Loss: 0.00000957
Iteration 52/1000 | Loss: 0.00000957
Iteration 53/1000 | Loss: 0.00000957
Iteration 54/1000 | Loss: 0.00000957
Iteration 55/1000 | Loss: 0.00000957
Iteration 56/1000 | Loss: 0.00000957
Iteration 57/1000 | Loss: 0.00000956
Iteration 58/1000 | Loss: 0.00000956
Iteration 59/1000 | Loss: 0.00000956
Iteration 60/1000 | Loss: 0.00000956
Iteration 61/1000 | Loss: 0.00000956
Iteration 62/1000 | Loss: 0.00000956
Iteration 63/1000 | Loss: 0.00000956
Iteration 64/1000 | Loss: 0.00000955
Iteration 65/1000 | Loss: 0.00000955
Iteration 66/1000 | Loss: 0.00000955
Iteration 67/1000 | Loss: 0.00000955
Iteration 68/1000 | Loss: 0.00000955
Iteration 69/1000 | Loss: 0.00000955
Iteration 70/1000 | Loss: 0.00000955
Iteration 71/1000 | Loss: 0.00000955
Iteration 72/1000 | Loss: 0.00000955
Iteration 73/1000 | Loss: 0.00000955
Iteration 74/1000 | Loss: 0.00000955
Iteration 75/1000 | Loss: 0.00000955
Iteration 76/1000 | Loss: 0.00000955
Iteration 77/1000 | Loss: 0.00000955
Iteration 78/1000 | Loss: 0.00000955
Iteration 79/1000 | Loss: 0.00000955
Iteration 80/1000 | Loss: 0.00000955
Iteration 81/1000 | Loss: 0.00000955
Iteration 82/1000 | Loss: 0.00000954
Iteration 83/1000 | Loss: 0.00000954
Iteration 84/1000 | Loss: 0.00000954
Iteration 85/1000 | Loss: 0.00000954
Iteration 86/1000 | Loss: 0.00000954
Iteration 87/1000 | Loss: 0.00000954
Iteration 88/1000 | Loss: 0.00000954
Iteration 89/1000 | Loss: 0.00000954
Iteration 90/1000 | Loss: 0.00000954
Iteration 91/1000 | Loss: 0.00000954
Iteration 92/1000 | Loss: 0.00000954
Iteration 93/1000 | Loss: 0.00000954
Iteration 94/1000 | Loss: 0.00000954
Iteration 95/1000 | Loss: 0.00000954
Iteration 96/1000 | Loss: 0.00000954
Iteration 97/1000 | Loss: 0.00000954
Iteration 98/1000 | Loss: 0.00000954
Iteration 99/1000 | Loss: 0.00000954
Iteration 100/1000 | Loss: 0.00000954
Iteration 101/1000 | Loss: 0.00000954
Iteration 102/1000 | Loss: 0.00000954
Iteration 103/1000 | Loss: 0.00000954
Iteration 104/1000 | Loss: 0.00000954
Iteration 105/1000 | Loss: 0.00000954
Iteration 106/1000 | Loss: 0.00000954
Iteration 107/1000 | Loss: 0.00000954
Iteration 108/1000 | Loss: 0.00000954
Iteration 109/1000 | Loss: 0.00000954
Iteration 110/1000 | Loss: 0.00000954
Iteration 111/1000 | Loss: 0.00000954
Iteration 112/1000 | Loss: 0.00000954
Iteration 113/1000 | Loss: 0.00000954
Iteration 114/1000 | Loss: 0.00000954
Iteration 115/1000 | Loss: 0.00000954
Iteration 116/1000 | Loss: 0.00000954
Iteration 117/1000 | Loss: 0.00000954
Iteration 118/1000 | Loss: 0.00000954
Iteration 119/1000 | Loss: 0.00000954
Iteration 120/1000 | Loss: 0.00000954
Iteration 121/1000 | Loss: 0.00000954
Iteration 122/1000 | Loss: 0.00000954
Iteration 123/1000 | Loss: 0.00000954
Iteration 124/1000 | Loss: 0.00000954
Iteration 125/1000 | Loss: 0.00000954
Iteration 126/1000 | Loss: 0.00000954
Iteration 127/1000 | Loss: 0.00000954
Iteration 128/1000 | Loss: 0.00000954
Iteration 129/1000 | Loss: 0.00000954
Iteration 130/1000 | Loss: 0.00000954
Iteration 131/1000 | Loss: 0.00000954
Iteration 132/1000 | Loss: 0.00000954
Iteration 133/1000 | Loss: 0.00000954
Iteration 134/1000 | Loss: 0.00000954
Iteration 135/1000 | Loss: 0.00000954
Iteration 136/1000 | Loss: 0.00000954
Iteration 137/1000 | Loss: 0.00000954
Iteration 138/1000 | Loss: 0.00000954
Iteration 139/1000 | Loss: 0.00000954
Iteration 140/1000 | Loss: 0.00000954
Iteration 141/1000 | Loss: 0.00000954
Iteration 142/1000 | Loss: 0.00000954
Iteration 143/1000 | Loss: 0.00000954
Iteration 144/1000 | Loss: 0.00000954
Iteration 145/1000 | Loss: 0.00000954
Iteration 146/1000 | Loss: 0.00000954
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [9.54173992795404e-06, 9.54173992795404e-06, 9.54173992795404e-06, 9.54173992795404e-06, 9.54173992795404e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.54173992795404e-06

Optimization complete. Final v2v error: 2.6499826908111572 mm

Highest mean error: 3.035374164581299 mm for frame 75

Lowest mean error: 2.3999061584472656 mm for frame 229

Saving results

Total time: 34.78734993934631
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01004907
Iteration 2/25 | Loss: 0.01004907
Iteration 3/25 | Loss: 0.01004907
Iteration 4/25 | Loss: 0.01004906
Iteration 5/25 | Loss: 0.01004906
Iteration 6/25 | Loss: 0.01004906
Iteration 7/25 | Loss: 0.01004906
Iteration 8/25 | Loss: 0.01004906
Iteration 9/25 | Loss: 0.01004906
Iteration 10/25 | Loss: 0.01004906
Iteration 11/25 | Loss: 0.01004905
Iteration 12/25 | Loss: 0.01004905
Iteration 13/25 | Loss: 0.01004905
Iteration 14/25 | Loss: 0.01004905
Iteration 15/25 | Loss: 0.01004905
Iteration 16/25 | Loss: 0.01004905
Iteration 17/25 | Loss: 0.01004904
Iteration 18/25 | Loss: 0.01004904
Iteration 19/25 | Loss: 0.01004904
Iteration 20/25 | Loss: 0.01004904
Iteration 21/25 | Loss: 0.01004904
Iteration 22/25 | Loss: 0.01004904
Iteration 23/25 | Loss: 0.01004903
Iteration 24/25 | Loss: 0.01004903
Iteration 25/25 | Loss: 0.01004903

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41533375
Iteration 2/25 | Loss: 0.18466876
Iteration 3/25 | Loss: 0.17667523
Iteration 4/25 | Loss: 0.17667520
Iteration 5/25 | Loss: 0.17667517
Iteration 6/25 | Loss: 0.17667517
Iteration 7/25 | Loss: 0.17667517
Iteration 8/25 | Loss: 0.17667517
Iteration 9/25 | Loss: 0.17667516
Iteration 10/25 | Loss: 0.17667516
Iteration 11/25 | Loss: 0.17667516
Iteration 12/25 | Loss: 0.17667516
Iteration 13/25 | Loss: 0.17667516
Iteration 14/25 | Loss: 0.17667516
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.17667515575885773, 0.17667515575885773, 0.17667515575885773, 0.17667515575885773, 0.17667515575885773]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.17667515575885773

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17667516
Iteration 2/1000 | Loss: 0.00657830
Iteration 3/1000 | Loss: 0.00300862
Iteration 4/1000 | Loss: 0.00165916
Iteration 5/1000 | Loss: 0.00134334
Iteration 6/1000 | Loss: 0.00071484
Iteration 7/1000 | Loss: 0.00229730
Iteration 8/1000 | Loss: 0.00048685
Iteration 9/1000 | Loss: 0.00028273
Iteration 10/1000 | Loss: 0.00034481
Iteration 11/1000 | Loss: 0.00009709
Iteration 12/1000 | Loss: 0.00008160
Iteration 13/1000 | Loss: 0.00085012
Iteration 14/1000 | Loss: 0.00013564
Iteration 15/1000 | Loss: 0.00013610
Iteration 16/1000 | Loss: 0.00005603
Iteration 17/1000 | Loss: 0.00024581
Iteration 18/1000 | Loss: 0.00008057
Iteration 19/1000 | Loss: 0.00007635
Iteration 20/1000 | Loss: 0.00018655
Iteration 21/1000 | Loss: 0.00005123
Iteration 22/1000 | Loss: 0.00008071
Iteration 23/1000 | Loss: 0.00004172
Iteration 24/1000 | Loss: 0.00006373
Iteration 25/1000 | Loss: 0.00003644
Iteration 26/1000 | Loss: 0.00023601
Iteration 27/1000 | Loss: 0.00003434
Iteration 28/1000 | Loss: 0.00006942
Iteration 29/1000 | Loss: 0.00003396
Iteration 30/1000 | Loss: 0.00009095
Iteration 31/1000 | Loss: 0.00009049
Iteration 32/1000 | Loss: 0.00011234
Iteration 33/1000 | Loss: 0.00004360
Iteration 34/1000 | Loss: 0.00008713
Iteration 35/1000 | Loss: 0.00014162
Iteration 36/1000 | Loss: 0.00026767
Iteration 37/1000 | Loss: 0.00005886
Iteration 38/1000 | Loss: 0.00002893
Iteration 39/1000 | Loss: 0.00002788
Iteration 40/1000 | Loss: 0.00011871
Iteration 41/1000 | Loss: 0.00013830
Iteration 42/1000 | Loss: 0.00003138
Iteration 43/1000 | Loss: 0.00004089
Iteration 44/1000 | Loss: 0.00002706
Iteration 45/1000 | Loss: 0.00009985
Iteration 46/1000 | Loss: 0.00003276
Iteration 47/1000 | Loss: 0.00010621
Iteration 48/1000 | Loss: 0.00003140
Iteration 49/1000 | Loss: 0.00002902
Iteration 50/1000 | Loss: 0.00002620
Iteration 51/1000 | Loss: 0.00008960
Iteration 52/1000 | Loss: 0.00046420
Iteration 53/1000 | Loss: 0.00003134
Iteration 54/1000 | Loss: 0.00003048
Iteration 55/1000 | Loss: 0.00002567
Iteration 56/1000 | Loss: 0.00008411
Iteration 57/1000 | Loss: 0.00018170
Iteration 58/1000 | Loss: 0.00003372
Iteration 59/1000 | Loss: 0.00003716
Iteration 60/1000 | Loss: 0.00006417
Iteration 61/1000 | Loss: 0.00002530
Iteration 62/1000 | Loss: 0.00002520
Iteration 63/1000 | Loss: 0.00002520
Iteration 64/1000 | Loss: 0.00002518
Iteration 65/1000 | Loss: 0.00002518
Iteration 66/1000 | Loss: 0.00002517
Iteration 67/1000 | Loss: 0.00002517
Iteration 68/1000 | Loss: 0.00002517
Iteration 69/1000 | Loss: 0.00002517
Iteration 70/1000 | Loss: 0.00002516
Iteration 71/1000 | Loss: 0.00002516
Iteration 72/1000 | Loss: 0.00002516
Iteration 73/1000 | Loss: 0.00002515
Iteration 74/1000 | Loss: 0.00002515
Iteration 75/1000 | Loss: 0.00002514
Iteration 76/1000 | Loss: 0.00002514
Iteration 77/1000 | Loss: 0.00002514
Iteration 78/1000 | Loss: 0.00002514
Iteration 79/1000 | Loss: 0.00002514
Iteration 80/1000 | Loss: 0.00002513
Iteration 81/1000 | Loss: 0.00002513
Iteration 82/1000 | Loss: 0.00002513
Iteration 83/1000 | Loss: 0.00002513
Iteration 84/1000 | Loss: 0.00002512
Iteration 85/1000 | Loss: 0.00002511
Iteration 86/1000 | Loss: 0.00002511
Iteration 87/1000 | Loss: 0.00002510
Iteration 88/1000 | Loss: 0.00002507
Iteration 89/1000 | Loss: 0.00002506
Iteration 90/1000 | Loss: 0.00009861
Iteration 91/1000 | Loss: 0.00011894
Iteration 92/1000 | Loss: 0.00033584
Iteration 93/1000 | Loss: 0.00042838
Iteration 94/1000 | Loss: 0.00002706
Iteration 95/1000 | Loss: 0.00004489
Iteration 96/1000 | Loss: 0.00003641
Iteration 97/1000 | Loss: 0.00002501
Iteration 98/1000 | Loss: 0.00002495
Iteration 99/1000 | Loss: 0.00002494
Iteration 100/1000 | Loss: 0.00002494
Iteration 101/1000 | Loss: 0.00002493
Iteration 102/1000 | Loss: 0.00006239
Iteration 103/1000 | Loss: 0.00002486
Iteration 104/1000 | Loss: 0.00002486
Iteration 105/1000 | Loss: 0.00002486
Iteration 106/1000 | Loss: 0.00002486
Iteration 107/1000 | Loss: 0.00008669
Iteration 108/1000 | Loss: 0.00003131
Iteration 109/1000 | Loss: 0.00003784
Iteration 110/1000 | Loss: 0.00002486
Iteration 111/1000 | Loss: 0.00002486
Iteration 112/1000 | Loss: 0.00002486
Iteration 113/1000 | Loss: 0.00002485
Iteration 114/1000 | Loss: 0.00002485
Iteration 115/1000 | Loss: 0.00002485
Iteration 116/1000 | Loss: 0.00002485
Iteration 117/1000 | Loss: 0.00002485
Iteration 118/1000 | Loss: 0.00002485
Iteration 119/1000 | Loss: 0.00002484
Iteration 120/1000 | Loss: 0.00002484
Iteration 121/1000 | Loss: 0.00002484
Iteration 122/1000 | Loss: 0.00002484
Iteration 123/1000 | Loss: 0.00002484
Iteration 124/1000 | Loss: 0.00002484
Iteration 125/1000 | Loss: 0.00002484
Iteration 126/1000 | Loss: 0.00002484
Iteration 127/1000 | Loss: 0.00002484
Iteration 128/1000 | Loss: 0.00002484
Iteration 129/1000 | Loss: 0.00002484
Iteration 130/1000 | Loss: 0.00002484
Iteration 131/1000 | Loss: 0.00002483
Iteration 132/1000 | Loss: 0.00002483
Iteration 133/1000 | Loss: 0.00002483
Iteration 134/1000 | Loss: 0.00002483
Iteration 135/1000 | Loss: 0.00002483
Iteration 136/1000 | Loss: 0.00002483
Iteration 137/1000 | Loss: 0.00002483
Iteration 138/1000 | Loss: 0.00002483
Iteration 139/1000 | Loss: 0.00002483
Iteration 140/1000 | Loss: 0.00002483
Iteration 141/1000 | Loss: 0.00002483
Iteration 142/1000 | Loss: 0.00002483
Iteration 143/1000 | Loss: 0.00002483
Iteration 144/1000 | Loss: 0.00002483
Iteration 145/1000 | Loss: 0.00002482
Iteration 146/1000 | Loss: 0.00002482
Iteration 147/1000 | Loss: 0.00002482
Iteration 148/1000 | Loss: 0.00002482
Iteration 149/1000 | Loss: 0.00002482
Iteration 150/1000 | Loss: 0.00002482
Iteration 151/1000 | Loss: 0.00002482
Iteration 152/1000 | Loss: 0.00002481
Iteration 153/1000 | Loss: 0.00002481
Iteration 154/1000 | Loss: 0.00002481
Iteration 155/1000 | Loss: 0.00002481
Iteration 156/1000 | Loss: 0.00002481
Iteration 157/1000 | Loss: 0.00002481
Iteration 158/1000 | Loss: 0.00002481
Iteration 159/1000 | Loss: 0.00002481
Iteration 160/1000 | Loss: 0.00002481
Iteration 161/1000 | Loss: 0.00009395
Iteration 162/1000 | Loss: 0.00005707
Iteration 163/1000 | Loss: 0.00003035
Iteration 164/1000 | Loss: 0.00003066
Iteration 165/1000 | Loss: 0.00002693
Iteration 166/1000 | Loss: 0.00002485
Iteration 167/1000 | Loss: 0.00002485
Iteration 168/1000 | Loss: 0.00002485
Iteration 169/1000 | Loss: 0.00002485
Iteration 170/1000 | Loss: 0.00002485
Iteration 171/1000 | Loss: 0.00002485
Iteration 172/1000 | Loss: 0.00002484
Iteration 173/1000 | Loss: 0.00002484
Iteration 174/1000 | Loss: 0.00002484
Iteration 175/1000 | Loss: 0.00002484
Iteration 176/1000 | Loss: 0.00002484
Iteration 177/1000 | Loss: 0.00002484
Iteration 178/1000 | Loss: 0.00002484
Iteration 179/1000 | Loss: 0.00002484
Iteration 180/1000 | Loss: 0.00002484
Iteration 181/1000 | Loss: 0.00002484
Iteration 182/1000 | Loss: 0.00002484
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [2.484257856849581e-05, 2.484257856849581e-05, 2.484257856849581e-05, 2.484257856849581e-05, 2.484257856849581e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.484257856849581e-05

Optimization complete. Final v2v error: 3.440633773803711 mm

Highest mean error: 23.242036819458008 mm for frame 99

Lowest mean error: 2.753950595855713 mm for frame 128

Saving results

Total time: 136.8709831237793
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00389035
Iteration 2/25 | Loss: 0.00114577
Iteration 3/25 | Loss: 0.00098015
Iteration 4/25 | Loss: 0.00096218
Iteration 5/25 | Loss: 0.00095791
Iteration 6/25 | Loss: 0.00095752
Iteration 7/25 | Loss: 0.00095752
Iteration 8/25 | Loss: 0.00095752
Iteration 9/25 | Loss: 0.00095752
Iteration 10/25 | Loss: 0.00095752
Iteration 11/25 | Loss: 0.00095752
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009575187577866018, 0.0009575187577866018, 0.0009575187577866018, 0.0009575187577866018, 0.0009575187577866018]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009575187577866018

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32902420
Iteration 2/25 | Loss: 0.00076922
Iteration 3/25 | Loss: 0.00076922
Iteration 4/25 | Loss: 0.00076922
Iteration 5/25 | Loss: 0.00076922
Iteration 6/25 | Loss: 0.00076922
Iteration 7/25 | Loss: 0.00076922
Iteration 8/25 | Loss: 0.00076922
Iteration 9/25 | Loss: 0.00076922
Iteration 10/25 | Loss: 0.00076922
Iteration 11/25 | Loss: 0.00076922
Iteration 12/25 | Loss: 0.00076922
Iteration 13/25 | Loss: 0.00076922
Iteration 14/25 | Loss: 0.00076922
Iteration 15/25 | Loss: 0.00076922
Iteration 16/25 | Loss: 0.00076922
Iteration 17/25 | Loss: 0.00076922
Iteration 18/25 | Loss: 0.00076922
Iteration 19/25 | Loss: 0.00076922
Iteration 20/25 | Loss: 0.00076922
Iteration 21/25 | Loss: 0.00076922
Iteration 22/25 | Loss: 0.00076922
Iteration 23/25 | Loss: 0.00076922
Iteration 24/25 | Loss: 0.00076922
Iteration 25/25 | Loss: 0.00076922

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076922
Iteration 2/1000 | Loss: 0.00003989
Iteration 3/1000 | Loss: 0.00002597
Iteration 4/1000 | Loss: 0.00001953
Iteration 5/1000 | Loss: 0.00001753
Iteration 6/1000 | Loss: 0.00001666
Iteration 7/1000 | Loss: 0.00001605
Iteration 8/1000 | Loss: 0.00001565
Iteration 9/1000 | Loss: 0.00001531
Iteration 10/1000 | Loss: 0.00001497
Iteration 11/1000 | Loss: 0.00001496
Iteration 12/1000 | Loss: 0.00001487
Iteration 13/1000 | Loss: 0.00001465
Iteration 14/1000 | Loss: 0.00001442
Iteration 15/1000 | Loss: 0.00001440
Iteration 16/1000 | Loss: 0.00001437
Iteration 17/1000 | Loss: 0.00001432
Iteration 18/1000 | Loss: 0.00001426
Iteration 19/1000 | Loss: 0.00001423
Iteration 20/1000 | Loss: 0.00001422
Iteration 21/1000 | Loss: 0.00001422
Iteration 22/1000 | Loss: 0.00001422
Iteration 23/1000 | Loss: 0.00001421
Iteration 24/1000 | Loss: 0.00001421
Iteration 25/1000 | Loss: 0.00001420
Iteration 26/1000 | Loss: 0.00001419
Iteration 27/1000 | Loss: 0.00001419
Iteration 28/1000 | Loss: 0.00001419
Iteration 29/1000 | Loss: 0.00001418
Iteration 30/1000 | Loss: 0.00001418
Iteration 31/1000 | Loss: 0.00001418
Iteration 32/1000 | Loss: 0.00001418
Iteration 33/1000 | Loss: 0.00001417
Iteration 34/1000 | Loss: 0.00001417
Iteration 35/1000 | Loss: 0.00001417
Iteration 36/1000 | Loss: 0.00001417
Iteration 37/1000 | Loss: 0.00001416
Iteration 38/1000 | Loss: 0.00001416
Iteration 39/1000 | Loss: 0.00001416
Iteration 40/1000 | Loss: 0.00001415
Iteration 41/1000 | Loss: 0.00001415
Iteration 42/1000 | Loss: 0.00001414
Iteration 43/1000 | Loss: 0.00001414
Iteration 44/1000 | Loss: 0.00001414
Iteration 45/1000 | Loss: 0.00001413
Iteration 46/1000 | Loss: 0.00001413
Iteration 47/1000 | Loss: 0.00001412
Iteration 48/1000 | Loss: 0.00001412
Iteration 49/1000 | Loss: 0.00001412
Iteration 50/1000 | Loss: 0.00001411
Iteration 51/1000 | Loss: 0.00001411
Iteration 52/1000 | Loss: 0.00001410
Iteration 53/1000 | Loss: 0.00001410
Iteration 54/1000 | Loss: 0.00001409
Iteration 55/1000 | Loss: 0.00001409
Iteration 56/1000 | Loss: 0.00001408
Iteration 57/1000 | Loss: 0.00001408
Iteration 58/1000 | Loss: 0.00001408
Iteration 59/1000 | Loss: 0.00001408
Iteration 60/1000 | Loss: 0.00001408
Iteration 61/1000 | Loss: 0.00001408
Iteration 62/1000 | Loss: 0.00001408
Iteration 63/1000 | Loss: 0.00001408
Iteration 64/1000 | Loss: 0.00001407
Iteration 65/1000 | Loss: 0.00001407
Iteration 66/1000 | Loss: 0.00001407
Iteration 67/1000 | Loss: 0.00001406
Iteration 68/1000 | Loss: 0.00001406
Iteration 69/1000 | Loss: 0.00001406
Iteration 70/1000 | Loss: 0.00001406
Iteration 71/1000 | Loss: 0.00001406
Iteration 72/1000 | Loss: 0.00001406
Iteration 73/1000 | Loss: 0.00001406
Iteration 74/1000 | Loss: 0.00001406
Iteration 75/1000 | Loss: 0.00001406
Iteration 76/1000 | Loss: 0.00001405
Iteration 77/1000 | Loss: 0.00001405
Iteration 78/1000 | Loss: 0.00001405
Iteration 79/1000 | Loss: 0.00001405
Iteration 80/1000 | Loss: 0.00001405
Iteration 81/1000 | Loss: 0.00001405
Iteration 82/1000 | Loss: 0.00001405
Iteration 83/1000 | Loss: 0.00001405
Iteration 84/1000 | Loss: 0.00001405
Iteration 85/1000 | Loss: 0.00001404
Iteration 86/1000 | Loss: 0.00001404
Iteration 87/1000 | Loss: 0.00001404
Iteration 88/1000 | Loss: 0.00001404
Iteration 89/1000 | Loss: 0.00001404
Iteration 90/1000 | Loss: 0.00001404
Iteration 91/1000 | Loss: 0.00001404
Iteration 92/1000 | Loss: 0.00001404
Iteration 93/1000 | Loss: 0.00001404
Iteration 94/1000 | Loss: 0.00001404
Iteration 95/1000 | Loss: 0.00001404
Iteration 96/1000 | Loss: 0.00001404
Iteration 97/1000 | Loss: 0.00001403
Iteration 98/1000 | Loss: 0.00001403
Iteration 99/1000 | Loss: 0.00001403
Iteration 100/1000 | Loss: 0.00001403
Iteration 101/1000 | Loss: 0.00001403
Iteration 102/1000 | Loss: 0.00001403
Iteration 103/1000 | Loss: 0.00001403
Iteration 104/1000 | Loss: 0.00001403
Iteration 105/1000 | Loss: 0.00001403
Iteration 106/1000 | Loss: 0.00001403
Iteration 107/1000 | Loss: 0.00001403
Iteration 108/1000 | Loss: 0.00001403
Iteration 109/1000 | Loss: 0.00001403
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [1.4027543329575565e-05, 1.4027543329575565e-05, 1.4027543329575565e-05, 1.4027543329575565e-05, 1.4027543329575565e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4027543329575565e-05

Optimization complete. Final v2v error: 3.0764334201812744 mm

Highest mean error: 3.8009955883026123 mm for frame 204

Lowest mean error: 2.4280874729156494 mm for frame 5

Saving results

Total time: 38.537400007247925
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00794404
Iteration 2/25 | Loss: 0.00119245
Iteration 3/25 | Loss: 0.00101817
Iteration 4/25 | Loss: 0.00100093
Iteration 5/25 | Loss: 0.00099566
Iteration 6/25 | Loss: 0.00099498
Iteration 7/25 | Loss: 0.00099498
Iteration 8/25 | Loss: 0.00099498
Iteration 9/25 | Loss: 0.00099498
Iteration 10/25 | Loss: 0.00099498
Iteration 11/25 | Loss: 0.00099498
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009949773084372282, 0.0009949773084372282, 0.0009949773084372282, 0.0009949773084372282, 0.0009949773084372282]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009949773084372282

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26439571
Iteration 2/25 | Loss: 0.00072824
Iteration 3/25 | Loss: 0.00072822
Iteration 4/25 | Loss: 0.00072822
Iteration 5/25 | Loss: 0.00072822
Iteration 6/25 | Loss: 0.00072822
Iteration 7/25 | Loss: 0.00072822
Iteration 8/25 | Loss: 0.00072822
Iteration 9/25 | Loss: 0.00072822
Iteration 10/25 | Loss: 0.00072822
Iteration 11/25 | Loss: 0.00072822
Iteration 12/25 | Loss: 0.00072822
Iteration 13/25 | Loss: 0.00072822
Iteration 14/25 | Loss: 0.00072822
Iteration 15/25 | Loss: 0.00072822
Iteration 16/25 | Loss: 0.00072822
Iteration 17/25 | Loss: 0.00072822
Iteration 18/25 | Loss: 0.00072822
Iteration 19/25 | Loss: 0.00072822
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007282185251824558, 0.0007282185251824558, 0.0007282185251824558, 0.0007282185251824558, 0.0007282185251824558]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007282185251824558

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072822
Iteration 2/1000 | Loss: 0.00002355
Iteration 3/1000 | Loss: 0.00001741
Iteration 4/1000 | Loss: 0.00001461
Iteration 5/1000 | Loss: 0.00001370
Iteration 6/1000 | Loss: 0.00001312
Iteration 7/1000 | Loss: 0.00001285
Iteration 8/1000 | Loss: 0.00001261
Iteration 9/1000 | Loss: 0.00001258
Iteration 10/1000 | Loss: 0.00001248
Iteration 11/1000 | Loss: 0.00001241
Iteration 12/1000 | Loss: 0.00001240
Iteration 13/1000 | Loss: 0.00001239
Iteration 14/1000 | Loss: 0.00001239
Iteration 15/1000 | Loss: 0.00001230
Iteration 16/1000 | Loss: 0.00001229
Iteration 17/1000 | Loss: 0.00001225
Iteration 18/1000 | Loss: 0.00001220
Iteration 19/1000 | Loss: 0.00001220
Iteration 20/1000 | Loss: 0.00001220
Iteration 21/1000 | Loss: 0.00001219
Iteration 22/1000 | Loss: 0.00001218
Iteration 23/1000 | Loss: 0.00001218
Iteration 24/1000 | Loss: 0.00001218
Iteration 25/1000 | Loss: 0.00001218
Iteration 26/1000 | Loss: 0.00001218
Iteration 27/1000 | Loss: 0.00001218
Iteration 28/1000 | Loss: 0.00001218
Iteration 29/1000 | Loss: 0.00001217
Iteration 30/1000 | Loss: 0.00001215
Iteration 31/1000 | Loss: 0.00001215
Iteration 32/1000 | Loss: 0.00001215
Iteration 33/1000 | Loss: 0.00001215
Iteration 34/1000 | Loss: 0.00001215
Iteration 35/1000 | Loss: 0.00001215
Iteration 36/1000 | Loss: 0.00001215
Iteration 37/1000 | Loss: 0.00001215
Iteration 38/1000 | Loss: 0.00001215
Iteration 39/1000 | Loss: 0.00001215
Iteration 40/1000 | Loss: 0.00001214
Iteration 41/1000 | Loss: 0.00001214
Iteration 42/1000 | Loss: 0.00001212
Iteration 43/1000 | Loss: 0.00001211
Iteration 44/1000 | Loss: 0.00001209
Iteration 45/1000 | Loss: 0.00001209
Iteration 46/1000 | Loss: 0.00001208
Iteration 47/1000 | Loss: 0.00001208
Iteration 48/1000 | Loss: 0.00001208
Iteration 49/1000 | Loss: 0.00001208
Iteration 50/1000 | Loss: 0.00001207
Iteration 51/1000 | Loss: 0.00001205
Iteration 52/1000 | Loss: 0.00001205
Iteration 53/1000 | Loss: 0.00001205
Iteration 54/1000 | Loss: 0.00001205
Iteration 55/1000 | Loss: 0.00001205
Iteration 56/1000 | Loss: 0.00001205
Iteration 57/1000 | Loss: 0.00001205
Iteration 58/1000 | Loss: 0.00001205
Iteration 59/1000 | Loss: 0.00001204
Iteration 60/1000 | Loss: 0.00001204
Iteration 61/1000 | Loss: 0.00001204
Iteration 62/1000 | Loss: 0.00001204
Iteration 63/1000 | Loss: 0.00001204
Iteration 64/1000 | Loss: 0.00001204
Iteration 65/1000 | Loss: 0.00001204
Iteration 66/1000 | Loss: 0.00001204
Iteration 67/1000 | Loss: 0.00001204
Iteration 68/1000 | Loss: 0.00001204
Iteration 69/1000 | Loss: 0.00001204
Iteration 70/1000 | Loss: 0.00001204
Iteration 71/1000 | Loss: 0.00001204
Iteration 72/1000 | Loss: 0.00001204
Iteration 73/1000 | Loss: 0.00001204
Iteration 74/1000 | Loss: 0.00001204
Iteration 75/1000 | Loss: 0.00001204
Iteration 76/1000 | Loss: 0.00001204
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 76. Stopping optimization.
Last 5 losses: [1.2037247870466672e-05, 1.2037247870466672e-05, 1.2037247870466672e-05, 1.2037247870466672e-05, 1.2037247870466672e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2037247870466672e-05

Optimization complete. Final v2v error: 3.000547170639038 mm

Highest mean error: 3.376887321472168 mm for frame 4

Lowest mean error: 2.7842867374420166 mm for frame 239

Saving results

Total time: 32.113051652908325
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01065228
Iteration 2/25 | Loss: 0.00261464
Iteration 3/25 | Loss: 0.00163730
Iteration 4/25 | Loss: 0.00134538
Iteration 5/25 | Loss: 0.00135002
Iteration 6/25 | Loss: 0.00131551
Iteration 7/25 | Loss: 0.00129682
Iteration 8/25 | Loss: 0.00118801
Iteration 9/25 | Loss: 0.00113282
Iteration 10/25 | Loss: 0.00109963
Iteration 11/25 | Loss: 0.00107198
Iteration 12/25 | Loss: 0.00105147
Iteration 13/25 | Loss: 0.00103429
Iteration 14/25 | Loss: 0.00104129
Iteration 15/25 | Loss: 0.00102533
Iteration 16/25 | Loss: 0.00101549
Iteration 17/25 | Loss: 0.00100675
Iteration 18/25 | Loss: 0.00100021
Iteration 19/25 | Loss: 0.00099147
Iteration 20/25 | Loss: 0.00099020
Iteration 21/25 | Loss: 0.00098920
Iteration 22/25 | Loss: 0.00099470
Iteration 23/25 | Loss: 0.00099149
Iteration 24/25 | Loss: 0.00099002
Iteration 25/25 | Loss: 0.00098910

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.02498341
Iteration 2/25 | Loss: 0.00057673
Iteration 3/25 | Loss: 0.00057673
Iteration 4/25 | Loss: 0.00057672
Iteration 5/25 | Loss: 0.00057672
Iteration 6/25 | Loss: 0.00057672
Iteration 7/25 | Loss: 0.00057672
Iteration 8/25 | Loss: 0.00057672
Iteration 9/25 | Loss: 0.00057672
Iteration 10/25 | Loss: 0.00057672
Iteration 11/25 | Loss: 0.00057672
Iteration 12/25 | Loss: 0.00057672
Iteration 13/25 | Loss: 0.00057672
Iteration 14/25 | Loss: 0.00057672
Iteration 15/25 | Loss: 0.00057672
Iteration 16/25 | Loss: 0.00057672
Iteration 17/25 | Loss: 0.00057672
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005767226684838533, 0.0005767226684838533, 0.0005767226684838533, 0.0005767226684838533, 0.0005767226684838533]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005767226684838533

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057672
Iteration 2/1000 | Loss: 0.00013575
Iteration 3/1000 | Loss: 0.00022654
Iteration 4/1000 | Loss: 0.00003395
Iteration 5/1000 | Loss: 0.00017008
Iteration 6/1000 | Loss: 0.00009320
Iteration 7/1000 | Loss: 0.00002716
Iteration 8/1000 | Loss: 0.00008908
Iteration 9/1000 | Loss: 0.00025564
Iteration 10/1000 | Loss: 0.00002437
Iteration 11/1000 | Loss: 0.00002326
Iteration 12/1000 | Loss: 0.00002266
Iteration 13/1000 | Loss: 0.00002202
Iteration 14/1000 | Loss: 0.00002144
Iteration 15/1000 | Loss: 0.00002160
Iteration 16/1000 | Loss: 0.00002100
Iteration 17/1000 | Loss: 0.00002072
Iteration 18/1000 | Loss: 0.00002068
Iteration 19/1000 | Loss: 0.00002104
Iteration 20/1000 | Loss: 0.00002061
Iteration 21/1000 | Loss: 0.00002048
Iteration 22/1000 | Loss: 0.00002036
Iteration 23/1000 | Loss: 0.00002028
Iteration 24/1000 | Loss: 0.00002028
Iteration 25/1000 | Loss: 0.00002028
Iteration 26/1000 | Loss: 0.00002028
Iteration 27/1000 | Loss: 0.00002027
Iteration 28/1000 | Loss: 0.00002027
Iteration 29/1000 | Loss: 0.00002027
Iteration 30/1000 | Loss: 0.00002027
Iteration 31/1000 | Loss: 0.00002027
Iteration 32/1000 | Loss: 0.00002027
Iteration 33/1000 | Loss: 0.00002027
Iteration 34/1000 | Loss: 0.00002027
Iteration 35/1000 | Loss: 0.00002027
Iteration 36/1000 | Loss: 0.00002027
Iteration 37/1000 | Loss: 0.00002027
Iteration 38/1000 | Loss: 0.00002027
Iteration 39/1000 | Loss: 0.00002027
Iteration 40/1000 | Loss: 0.00002026
Iteration 41/1000 | Loss: 0.00002026
Iteration 42/1000 | Loss: 0.00002026
Iteration 43/1000 | Loss: 0.00002026
Iteration 44/1000 | Loss: 0.00002026
Iteration 45/1000 | Loss: 0.00002026
Iteration 46/1000 | Loss: 0.00002026
Iteration 47/1000 | Loss: 0.00002026
Iteration 48/1000 | Loss: 0.00002025
Iteration 49/1000 | Loss: 0.00002025
Iteration 50/1000 | Loss: 0.00002025
Iteration 51/1000 | Loss: 0.00002025
Iteration 52/1000 | Loss: 0.00002025
Iteration 53/1000 | Loss: 0.00002025
Iteration 54/1000 | Loss: 0.00002025
Iteration 55/1000 | Loss: 0.00002025
Iteration 56/1000 | Loss: 0.00002025
Iteration 57/1000 | Loss: 0.00002024
Iteration 58/1000 | Loss: 0.00002024
Iteration 59/1000 | Loss: 0.00002024
Iteration 60/1000 | Loss: 0.00002024
Iteration 61/1000 | Loss: 0.00002024
Iteration 62/1000 | Loss: 0.00002024
Iteration 63/1000 | Loss: 0.00002024
Iteration 64/1000 | Loss: 0.00002024
Iteration 65/1000 | Loss: 0.00002024
Iteration 66/1000 | Loss: 0.00002024
Iteration 67/1000 | Loss: 0.00002024
Iteration 68/1000 | Loss: 0.00002024
Iteration 69/1000 | Loss: 0.00002024
Iteration 70/1000 | Loss: 0.00002024
Iteration 71/1000 | Loss: 0.00002024
Iteration 72/1000 | Loss: 0.00002024
Iteration 73/1000 | Loss: 0.00002023
Iteration 74/1000 | Loss: 0.00002023
Iteration 75/1000 | Loss: 0.00002023
Iteration 76/1000 | Loss: 0.00002023
Iteration 77/1000 | Loss: 0.00002023
Iteration 78/1000 | Loss: 0.00002023
Iteration 79/1000 | Loss: 0.00002023
Iteration 80/1000 | Loss: 0.00002023
Iteration 81/1000 | Loss: 0.00002023
Iteration 82/1000 | Loss: 0.00002023
Iteration 83/1000 | Loss: 0.00002023
Iteration 84/1000 | Loss: 0.00002023
Iteration 85/1000 | Loss: 0.00002023
Iteration 86/1000 | Loss: 0.00002023
Iteration 87/1000 | Loss: 0.00002023
Iteration 88/1000 | Loss: 0.00002023
Iteration 89/1000 | Loss: 0.00002023
Iteration 90/1000 | Loss: 0.00002022
Iteration 91/1000 | Loss: 0.00002022
Iteration 92/1000 | Loss: 0.00002022
Iteration 93/1000 | Loss: 0.00002022
Iteration 94/1000 | Loss: 0.00002022
Iteration 95/1000 | Loss: 0.00002022
Iteration 96/1000 | Loss: 0.00002022
Iteration 97/1000 | Loss: 0.00002022
Iteration 98/1000 | Loss: 0.00002022
Iteration 99/1000 | Loss: 0.00002022
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [2.0223680621711537e-05, 2.0223680621711537e-05, 2.0223680621711537e-05, 2.0223680621711537e-05, 2.0223680621711537e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0223680621711537e-05

Optimization complete. Final v2v error: 3.080923557281494 mm

Highest mean error: 20.280582427978516 mm for frame 56

Lowest mean error: 2.662196397781372 mm for frame 17

Saving results

Total time: 74.19425392150879
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00981697
Iteration 2/25 | Loss: 0.00138813
Iteration 3/25 | Loss: 0.00108944
Iteration 4/25 | Loss: 0.00107197
Iteration 5/25 | Loss: 0.00106883
Iteration 6/25 | Loss: 0.00106846
Iteration 7/25 | Loss: 0.00106846
Iteration 8/25 | Loss: 0.00106846
Iteration 9/25 | Loss: 0.00106846
Iteration 10/25 | Loss: 0.00106846
Iteration 11/25 | Loss: 0.00106846
Iteration 12/25 | Loss: 0.00106846
Iteration 13/25 | Loss: 0.00106846
Iteration 14/25 | Loss: 0.00106846
Iteration 15/25 | Loss: 0.00106846
Iteration 16/25 | Loss: 0.00106846
Iteration 17/25 | Loss: 0.00106846
Iteration 18/25 | Loss: 0.00106846
Iteration 19/25 | Loss: 0.00106846
Iteration 20/25 | Loss: 0.00106846
Iteration 21/25 | Loss: 0.00106846
Iteration 22/25 | Loss: 0.00106846
Iteration 23/25 | Loss: 0.00106846
Iteration 24/25 | Loss: 0.00106846
Iteration 25/25 | Loss: 0.00106846

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63462591
Iteration 2/25 | Loss: 0.00053566
Iteration 3/25 | Loss: 0.00053565
Iteration 4/25 | Loss: 0.00053565
Iteration 5/25 | Loss: 0.00053565
Iteration 6/25 | Loss: 0.00053565
Iteration 7/25 | Loss: 0.00053565
Iteration 8/25 | Loss: 0.00053565
Iteration 9/25 | Loss: 0.00053565
Iteration 10/25 | Loss: 0.00053565
Iteration 11/25 | Loss: 0.00053565
Iteration 12/25 | Loss: 0.00053565
Iteration 13/25 | Loss: 0.00053565
Iteration 14/25 | Loss: 0.00053565
Iteration 15/25 | Loss: 0.00053565
Iteration 16/25 | Loss: 0.00053565
Iteration 17/25 | Loss: 0.00053565
Iteration 18/25 | Loss: 0.00053565
Iteration 19/25 | Loss: 0.00053565
Iteration 20/25 | Loss: 0.00053565
Iteration 21/25 | Loss: 0.00053565
Iteration 22/25 | Loss: 0.00053565
Iteration 23/25 | Loss: 0.00053565
Iteration 24/25 | Loss: 0.00053565
Iteration 25/25 | Loss: 0.00053565

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053565
Iteration 2/1000 | Loss: 0.00004429
Iteration 3/1000 | Loss: 0.00002749
Iteration 4/1000 | Loss: 0.00002350
Iteration 5/1000 | Loss: 0.00002149
Iteration 6/1000 | Loss: 0.00002074
Iteration 7/1000 | Loss: 0.00002027
Iteration 8/1000 | Loss: 0.00001992
Iteration 9/1000 | Loss: 0.00001965
Iteration 10/1000 | Loss: 0.00001940
Iteration 11/1000 | Loss: 0.00001920
Iteration 12/1000 | Loss: 0.00001906
Iteration 13/1000 | Loss: 0.00001893
Iteration 14/1000 | Loss: 0.00001884
Iteration 15/1000 | Loss: 0.00001882
Iteration 16/1000 | Loss: 0.00001879
Iteration 17/1000 | Loss: 0.00001875
Iteration 18/1000 | Loss: 0.00001868
Iteration 19/1000 | Loss: 0.00001863
Iteration 20/1000 | Loss: 0.00001862
Iteration 21/1000 | Loss: 0.00001856
Iteration 22/1000 | Loss: 0.00001850
Iteration 23/1000 | Loss: 0.00001850
Iteration 24/1000 | Loss: 0.00001848
Iteration 25/1000 | Loss: 0.00001848
Iteration 26/1000 | Loss: 0.00001847
Iteration 27/1000 | Loss: 0.00001847
Iteration 28/1000 | Loss: 0.00001847
Iteration 29/1000 | Loss: 0.00001847
Iteration 30/1000 | Loss: 0.00001847
Iteration 31/1000 | Loss: 0.00001847
Iteration 32/1000 | Loss: 0.00001847
Iteration 33/1000 | Loss: 0.00001846
Iteration 34/1000 | Loss: 0.00001846
Iteration 35/1000 | Loss: 0.00001846
Iteration 36/1000 | Loss: 0.00001846
Iteration 37/1000 | Loss: 0.00001846
Iteration 38/1000 | Loss: 0.00001846
Iteration 39/1000 | Loss: 0.00001846
Iteration 40/1000 | Loss: 0.00001846
Iteration 41/1000 | Loss: 0.00001846
Iteration 42/1000 | Loss: 0.00001846
Iteration 43/1000 | Loss: 0.00001846
Iteration 44/1000 | Loss: 0.00001845
Iteration 45/1000 | Loss: 0.00001845
Iteration 46/1000 | Loss: 0.00001845
Iteration 47/1000 | Loss: 0.00001845
Iteration 48/1000 | Loss: 0.00001845
Iteration 49/1000 | Loss: 0.00001845
Iteration 50/1000 | Loss: 0.00001845
Iteration 51/1000 | Loss: 0.00001844
Iteration 52/1000 | Loss: 0.00001844
Iteration 53/1000 | Loss: 0.00001844
Iteration 54/1000 | Loss: 0.00001843
Iteration 55/1000 | Loss: 0.00001843
Iteration 56/1000 | Loss: 0.00001842
Iteration 57/1000 | Loss: 0.00001842
Iteration 58/1000 | Loss: 0.00001842
Iteration 59/1000 | Loss: 0.00001842
Iteration 60/1000 | Loss: 0.00001842
Iteration 61/1000 | Loss: 0.00001842
Iteration 62/1000 | Loss: 0.00001841
Iteration 63/1000 | Loss: 0.00001841
Iteration 64/1000 | Loss: 0.00001841
Iteration 65/1000 | Loss: 0.00001841
Iteration 66/1000 | Loss: 0.00001841
Iteration 67/1000 | Loss: 0.00001841
Iteration 68/1000 | Loss: 0.00001841
Iteration 69/1000 | Loss: 0.00001841
Iteration 70/1000 | Loss: 0.00001841
Iteration 71/1000 | Loss: 0.00001841
Iteration 72/1000 | Loss: 0.00001841
Iteration 73/1000 | Loss: 0.00001840
Iteration 74/1000 | Loss: 0.00001840
Iteration 75/1000 | Loss: 0.00001840
Iteration 76/1000 | Loss: 0.00001840
Iteration 77/1000 | Loss: 0.00001840
Iteration 78/1000 | Loss: 0.00001840
Iteration 79/1000 | Loss: 0.00001840
Iteration 80/1000 | Loss: 0.00001840
Iteration 81/1000 | Loss: 0.00001840
Iteration 82/1000 | Loss: 0.00001840
Iteration 83/1000 | Loss: 0.00001840
Iteration 84/1000 | Loss: 0.00001839
Iteration 85/1000 | Loss: 0.00001839
Iteration 86/1000 | Loss: 0.00001839
Iteration 87/1000 | Loss: 0.00001838
Iteration 88/1000 | Loss: 0.00001838
Iteration 89/1000 | Loss: 0.00001838
Iteration 90/1000 | Loss: 0.00001838
Iteration 91/1000 | Loss: 0.00001838
Iteration 92/1000 | Loss: 0.00001838
Iteration 93/1000 | Loss: 0.00001838
Iteration 94/1000 | Loss: 0.00001838
Iteration 95/1000 | Loss: 0.00001838
Iteration 96/1000 | Loss: 0.00001838
Iteration 97/1000 | Loss: 0.00001838
Iteration 98/1000 | Loss: 0.00001837
Iteration 99/1000 | Loss: 0.00001837
Iteration 100/1000 | Loss: 0.00001837
Iteration 101/1000 | Loss: 0.00001837
Iteration 102/1000 | Loss: 0.00001837
Iteration 103/1000 | Loss: 0.00001836
Iteration 104/1000 | Loss: 0.00001836
Iteration 105/1000 | Loss: 0.00001836
Iteration 106/1000 | Loss: 0.00001836
Iteration 107/1000 | Loss: 0.00001836
Iteration 108/1000 | Loss: 0.00001836
Iteration 109/1000 | Loss: 0.00001836
Iteration 110/1000 | Loss: 0.00001835
Iteration 111/1000 | Loss: 0.00001835
Iteration 112/1000 | Loss: 0.00001835
Iteration 113/1000 | Loss: 0.00001835
Iteration 114/1000 | Loss: 0.00001835
Iteration 115/1000 | Loss: 0.00001835
Iteration 116/1000 | Loss: 0.00001835
Iteration 117/1000 | Loss: 0.00001834
Iteration 118/1000 | Loss: 0.00001834
Iteration 119/1000 | Loss: 0.00001834
Iteration 120/1000 | Loss: 0.00001834
Iteration 121/1000 | Loss: 0.00001834
Iteration 122/1000 | Loss: 0.00001834
Iteration 123/1000 | Loss: 0.00001834
Iteration 124/1000 | Loss: 0.00001834
Iteration 125/1000 | Loss: 0.00001834
Iteration 126/1000 | Loss: 0.00001834
Iteration 127/1000 | Loss: 0.00001834
Iteration 128/1000 | Loss: 0.00001834
Iteration 129/1000 | Loss: 0.00001834
Iteration 130/1000 | Loss: 0.00001833
Iteration 131/1000 | Loss: 0.00001833
Iteration 132/1000 | Loss: 0.00001833
Iteration 133/1000 | Loss: 0.00001833
Iteration 134/1000 | Loss: 0.00001833
Iteration 135/1000 | Loss: 0.00001833
Iteration 136/1000 | Loss: 0.00001833
Iteration 137/1000 | Loss: 0.00001833
Iteration 138/1000 | Loss: 0.00001832
Iteration 139/1000 | Loss: 0.00001832
Iteration 140/1000 | Loss: 0.00001832
Iteration 141/1000 | Loss: 0.00001832
Iteration 142/1000 | Loss: 0.00001831
Iteration 143/1000 | Loss: 0.00001831
Iteration 144/1000 | Loss: 0.00001831
Iteration 145/1000 | Loss: 0.00001831
Iteration 146/1000 | Loss: 0.00001831
Iteration 147/1000 | Loss: 0.00001831
Iteration 148/1000 | Loss: 0.00001831
Iteration 149/1000 | Loss: 0.00001831
Iteration 150/1000 | Loss: 0.00001831
Iteration 151/1000 | Loss: 0.00001831
Iteration 152/1000 | Loss: 0.00001831
Iteration 153/1000 | Loss: 0.00001831
Iteration 154/1000 | Loss: 0.00001830
Iteration 155/1000 | Loss: 0.00001830
Iteration 156/1000 | Loss: 0.00001830
Iteration 157/1000 | Loss: 0.00001830
Iteration 158/1000 | Loss: 0.00001830
Iteration 159/1000 | Loss: 0.00001830
Iteration 160/1000 | Loss: 0.00001830
Iteration 161/1000 | Loss: 0.00001829
Iteration 162/1000 | Loss: 0.00001829
Iteration 163/1000 | Loss: 0.00001829
Iteration 164/1000 | Loss: 0.00001829
Iteration 165/1000 | Loss: 0.00001829
Iteration 166/1000 | Loss: 0.00001829
Iteration 167/1000 | Loss: 0.00001829
Iteration 168/1000 | Loss: 0.00001828
Iteration 169/1000 | Loss: 0.00001828
Iteration 170/1000 | Loss: 0.00001828
Iteration 171/1000 | Loss: 0.00001828
Iteration 172/1000 | Loss: 0.00001828
Iteration 173/1000 | Loss: 0.00001828
Iteration 174/1000 | Loss: 0.00001828
Iteration 175/1000 | Loss: 0.00001828
Iteration 176/1000 | Loss: 0.00001828
Iteration 177/1000 | Loss: 0.00001828
Iteration 178/1000 | Loss: 0.00001828
Iteration 179/1000 | Loss: 0.00001828
Iteration 180/1000 | Loss: 0.00001827
Iteration 181/1000 | Loss: 0.00001827
Iteration 182/1000 | Loss: 0.00001827
Iteration 183/1000 | Loss: 0.00001827
Iteration 184/1000 | Loss: 0.00001827
Iteration 185/1000 | Loss: 0.00001827
Iteration 186/1000 | Loss: 0.00001827
Iteration 187/1000 | Loss: 0.00001827
Iteration 188/1000 | Loss: 0.00001827
Iteration 189/1000 | Loss: 0.00001827
Iteration 190/1000 | Loss: 0.00001827
Iteration 191/1000 | Loss: 0.00001827
Iteration 192/1000 | Loss: 0.00001827
Iteration 193/1000 | Loss: 0.00001827
Iteration 194/1000 | Loss: 0.00001827
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [1.827118467190303e-05, 1.827118467190303e-05, 1.827118467190303e-05, 1.827118467190303e-05, 1.827118467190303e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.827118467190303e-05

Optimization complete. Final v2v error: 3.503570795059204 mm

Highest mean error: 5.205142498016357 mm for frame 5

Lowest mean error: 2.715899705886841 mm for frame 125

Saving results

Total time: 48.190826177597046
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00858284
Iteration 2/25 | Loss: 0.00142106
Iteration 3/25 | Loss: 0.00100124
Iteration 4/25 | Loss: 0.00095614
Iteration 5/25 | Loss: 0.00094851
Iteration 6/25 | Loss: 0.00094660
Iteration 7/25 | Loss: 0.00094596
Iteration 8/25 | Loss: 0.00094565
Iteration 9/25 | Loss: 0.00094531
Iteration 10/25 | Loss: 0.00094795
Iteration 11/25 | Loss: 0.00094746
Iteration 12/25 | Loss: 0.00094618
Iteration 13/25 | Loss: 0.00094487
Iteration 14/25 | Loss: 0.00094316
Iteration 15/25 | Loss: 0.00094202
Iteration 16/25 | Loss: 0.00094234
Iteration 17/25 | Loss: 0.00094209
Iteration 18/25 | Loss: 0.00094159
Iteration 19/25 | Loss: 0.00094182
Iteration 20/25 | Loss: 0.00094189
Iteration 21/25 | Loss: 0.00094221
Iteration 22/25 | Loss: 0.00094124
Iteration 23/25 | Loss: 0.00094176
Iteration 24/25 | Loss: 0.00094201
Iteration 25/25 | Loss: 0.00094215

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33452094
Iteration 2/25 | Loss: 0.00057604
Iteration 3/25 | Loss: 0.00057604
Iteration 4/25 | Loss: 0.00057604
Iteration 5/25 | Loss: 0.00057604
Iteration 6/25 | Loss: 0.00057603
Iteration 7/25 | Loss: 0.00057603
Iteration 8/25 | Loss: 0.00057603
Iteration 9/25 | Loss: 0.00057603
Iteration 10/25 | Loss: 0.00057603
Iteration 11/25 | Loss: 0.00057603
Iteration 12/25 | Loss: 0.00057603
Iteration 13/25 | Loss: 0.00057603
Iteration 14/25 | Loss: 0.00057603
Iteration 15/25 | Loss: 0.00057603
Iteration 16/25 | Loss: 0.00057603
Iteration 17/25 | Loss: 0.00057603
Iteration 18/25 | Loss: 0.00057603
Iteration 19/25 | Loss: 0.00057603
Iteration 20/25 | Loss: 0.00057603
Iteration 21/25 | Loss: 0.00057603
Iteration 22/25 | Loss: 0.00057603
Iteration 23/25 | Loss: 0.00057603
Iteration 24/25 | Loss: 0.00057603
Iteration 25/25 | Loss: 0.00057603

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057603
Iteration 2/1000 | Loss: 0.00002006
Iteration 3/1000 | Loss: 0.00001239
Iteration 4/1000 | Loss: 0.00001069
Iteration 5/1000 | Loss: 0.00001017
Iteration 6/1000 | Loss: 0.00000982
Iteration 7/1000 | Loss: 0.00001206
Iteration 8/1000 | Loss: 0.00001013
Iteration 9/1000 | Loss: 0.00001091
Iteration 10/1000 | Loss: 0.00002605
Iteration 11/1000 | Loss: 0.00002572
Iteration 12/1000 | Loss: 0.00002792
Iteration 13/1000 | Loss: 0.00002686
Iteration 14/1000 | Loss: 0.00002750
Iteration 15/1000 | Loss: 0.00002624
Iteration 16/1000 | Loss: 0.00002964
Iteration 17/1000 | Loss: 0.00002889
Iteration 18/1000 | Loss: 0.00002762
Iteration 19/1000 | Loss: 0.00001237
Iteration 20/1000 | Loss: 0.00002598
Iteration 21/1000 | Loss: 0.00002490
Iteration 22/1000 | Loss: 0.00001103
Iteration 23/1000 | Loss: 0.00003010
Iteration 24/1000 | Loss: 0.00001583
Iteration 25/1000 | Loss: 0.00001235
Iteration 26/1000 | Loss: 0.00001054
Iteration 27/1000 | Loss: 0.00001004
Iteration 28/1000 | Loss: 0.00000967
Iteration 29/1000 | Loss: 0.00000962
Iteration 30/1000 | Loss: 0.00000937
Iteration 31/1000 | Loss: 0.00000913
Iteration 32/1000 | Loss: 0.00000908
Iteration 33/1000 | Loss: 0.00000886
Iteration 34/1000 | Loss: 0.00000885
Iteration 35/1000 | Loss: 0.00000884
Iteration 36/1000 | Loss: 0.00000884
Iteration 37/1000 | Loss: 0.00000883
Iteration 38/1000 | Loss: 0.00000883
Iteration 39/1000 | Loss: 0.00000882
Iteration 40/1000 | Loss: 0.00000882
Iteration 41/1000 | Loss: 0.00000882
Iteration 42/1000 | Loss: 0.00000881
Iteration 43/1000 | Loss: 0.00000881
Iteration 44/1000 | Loss: 0.00000880
Iteration 45/1000 | Loss: 0.00000880
Iteration 46/1000 | Loss: 0.00000880
Iteration 47/1000 | Loss: 0.00000879
Iteration 48/1000 | Loss: 0.00000879
Iteration 49/1000 | Loss: 0.00000879
Iteration 50/1000 | Loss: 0.00000878
Iteration 51/1000 | Loss: 0.00000877
Iteration 52/1000 | Loss: 0.00000875
Iteration 53/1000 | Loss: 0.00000875
Iteration 54/1000 | Loss: 0.00000874
Iteration 55/1000 | Loss: 0.00000874
Iteration 56/1000 | Loss: 0.00000873
Iteration 57/1000 | Loss: 0.00000873
Iteration 58/1000 | Loss: 0.00000872
Iteration 59/1000 | Loss: 0.00000871
Iteration 60/1000 | Loss: 0.00000871
Iteration 61/1000 | Loss: 0.00000871
Iteration 62/1000 | Loss: 0.00000870
Iteration 63/1000 | Loss: 0.00000870
Iteration 64/1000 | Loss: 0.00000870
Iteration 65/1000 | Loss: 0.00000870
Iteration 66/1000 | Loss: 0.00000870
Iteration 67/1000 | Loss: 0.00000870
Iteration 68/1000 | Loss: 0.00000869
Iteration 69/1000 | Loss: 0.00000869
Iteration 70/1000 | Loss: 0.00000869
Iteration 71/1000 | Loss: 0.00000868
Iteration 72/1000 | Loss: 0.00000867
Iteration 73/1000 | Loss: 0.00000867
Iteration 74/1000 | Loss: 0.00000867
Iteration 75/1000 | Loss: 0.00000866
Iteration 76/1000 | Loss: 0.00000866
Iteration 77/1000 | Loss: 0.00000866
Iteration 78/1000 | Loss: 0.00000865
Iteration 79/1000 | Loss: 0.00000865
Iteration 80/1000 | Loss: 0.00000864
Iteration 81/1000 | Loss: 0.00000863
Iteration 82/1000 | Loss: 0.00000862
Iteration 83/1000 | Loss: 0.00000861
Iteration 84/1000 | Loss: 0.00000860
Iteration 85/1000 | Loss: 0.00000859
Iteration 86/1000 | Loss: 0.00000859
Iteration 87/1000 | Loss: 0.00000854
Iteration 88/1000 | Loss: 0.00000853
Iteration 89/1000 | Loss: 0.00000852
Iteration 90/1000 | Loss: 0.00000852
Iteration 91/1000 | Loss: 0.00000852
Iteration 92/1000 | Loss: 0.00000852
Iteration 93/1000 | Loss: 0.00000851
Iteration 94/1000 | Loss: 0.00000851
Iteration 95/1000 | Loss: 0.00000850
Iteration 96/1000 | Loss: 0.00000850
Iteration 97/1000 | Loss: 0.00000849
Iteration 98/1000 | Loss: 0.00000849
Iteration 99/1000 | Loss: 0.00000849
Iteration 100/1000 | Loss: 0.00000848
Iteration 101/1000 | Loss: 0.00000846
Iteration 102/1000 | Loss: 0.00000846
Iteration 103/1000 | Loss: 0.00000846
Iteration 104/1000 | Loss: 0.00000845
Iteration 105/1000 | Loss: 0.00000844
Iteration 106/1000 | Loss: 0.00000844
Iteration 107/1000 | Loss: 0.00000843
Iteration 108/1000 | Loss: 0.00000843
Iteration 109/1000 | Loss: 0.00000843
Iteration 110/1000 | Loss: 0.00000843
Iteration 111/1000 | Loss: 0.00000842
Iteration 112/1000 | Loss: 0.00000842
Iteration 113/1000 | Loss: 0.00000842
Iteration 114/1000 | Loss: 0.00000841
Iteration 115/1000 | Loss: 0.00000841
Iteration 116/1000 | Loss: 0.00000841
Iteration 117/1000 | Loss: 0.00000841
Iteration 118/1000 | Loss: 0.00000841
Iteration 119/1000 | Loss: 0.00000841
Iteration 120/1000 | Loss: 0.00000841
Iteration 121/1000 | Loss: 0.00000841
Iteration 122/1000 | Loss: 0.00000841
Iteration 123/1000 | Loss: 0.00000840
Iteration 124/1000 | Loss: 0.00000840
Iteration 125/1000 | Loss: 0.00000840
Iteration 126/1000 | Loss: 0.00000840
Iteration 127/1000 | Loss: 0.00000840
Iteration 128/1000 | Loss: 0.00000840
Iteration 129/1000 | Loss: 0.00000840
Iteration 130/1000 | Loss: 0.00000840
Iteration 131/1000 | Loss: 0.00000840
Iteration 132/1000 | Loss: 0.00000839
Iteration 133/1000 | Loss: 0.00000839
Iteration 134/1000 | Loss: 0.00000839
Iteration 135/1000 | Loss: 0.00000839
Iteration 136/1000 | Loss: 0.00000839
Iteration 137/1000 | Loss: 0.00000839
Iteration 138/1000 | Loss: 0.00000839
Iteration 139/1000 | Loss: 0.00000839
Iteration 140/1000 | Loss: 0.00000839
Iteration 141/1000 | Loss: 0.00000839
Iteration 142/1000 | Loss: 0.00000839
Iteration 143/1000 | Loss: 0.00000839
Iteration 144/1000 | Loss: 0.00000839
Iteration 145/1000 | Loss: 0.00000838
Iteration 146/1000 | Loss: 0.00000838
Iteration 147/1000 | Loss: 0.00000838
Iteration 148/1000 | Loss: 0.00000838
Iteration 149/1000 | Loss: 0.00000838
Iteration 150/1000 | Loss: 0.00000838
Iteration 151/1000 | Loss: 0.00000838
Iteration 152/1000 | Loss: 0.00000838
Iteration 153/1000 | Loss: 0.00000838
Iteration 154/1000 | Loss: 0.00000838
Iteration 155/1000 | Loss: 0.00000838
Iteration 156/1000 | Loss: 0.00000838
Iteration 157/1000 | Loss: 0.00000838
Iteration 158/1000 | Loss: 0.00000838
Iteration 159/1000 | Loss: 0.00000838
Iteration 160/1000 | Loss: 0.00000838
Iteration 161/1000 | Loss: 0.00000838
Iteration 162/1000 | Loss: 0.00000838
Iteration 163/1000 | Loss: 0.00000838
Iteration 164/1000 | Loss: 0.00000838
Iteration 165/1000 | Loss: 0.00000838
Iteration 166/1000 | Loss: 0.00000838
Iteration 167/1000 | Loss: 0.00000838
Iteration 168/1000 | Loss: 0.00000837
Iteration 169/1000 | Loss: 0.00000837
Iteration 170/1000 | Loss: 0.00000837
Iteration 171/1000 | Loss: 0.00000837
Iteration 172/1000 | Loss: 0.00000837
Iteration 173/1000 | Loss: 0.00000837
Iteration 174/1000 | Loss: 0.00000837
Iteration 175/1000 | Loss: 0.00000837
Iteration 176/1000 | Loss: 0.00000837
Iteration 177/1000 | Loss: 0.00000837
Iteration 178/1000 | Loss: 0.00000837
Iteration 179/1000 | Loss: 0.00000837
Iteration 180/1000 | Loss: 0.00000837
Iteration 181/1000 | Loss: 0.00000837
Iteration 182/1000 | Loss: 0.00000837
Iteration 183/1000 | Loss: 0.00000837
Iteration 184/1000 | Loss: 0.00000837
Iteration 185/1000 | Loss: 0.00000837
Iteration 186/1000 | Loss: 0.00000837
Iteration 187/1000 | Loss: 0.00000837
Iteration 188/1000 | Loss: 0.00000837
Iteration 189/1000 | Loss: 0.00000837
Iteration 190/1000 | Loss: 0.00000837
Iteration 191/1000 | Loss: 0.00000837
Iteration 192/1000 | Loss: 0.00000837
Iteration 193/1000 | Loss: 0.00000837
Iteration 194/1000 | Loss: 0.00000836
Iteration 195/1000 | Loss: 0.00000836
Iteration 196/1000 | Loss: 0.00000836
Iteration 197/1000 | Loss: 0.00000836
Iteration 198/1000 | Loss: 0.00000836
Iteration 199/1000 | Loss: 0.00000836
Iteration 200/1000 | Loss: 0.00000836
Iteration 201/1000 | Loss: 0.00000836
Iteration 202/1000 | Loss: 0.00000836
Iteration 203/1000 | Loss: 0.00000836
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [8.364185305254068e-06, 8.364185305254068e-06, 8.364185305254068e-06, 8.364185305254068e-06, 8.364185305254068e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.364185305254068e-06

Optimization complete. Final v2v error: 2.487440824508667 mm

Highest mean error: 4.176580429077148 mm for frame 139

Lowest mean error: 2.3216466903686523 mm for frame 173

Saving results

Total time: 116.3900465965271
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00711600
Iteration 2/25 | Loss: 0.00106928
Iteration 3/25 | Loss: 0.00094060
Iteration 4/25 | Loss: 0.00092713
Iteration 5/25 | Loss: 0.00092300
Iteration 6/25 | Loss: 0.00092165
Iteration 7/25 | Loss: 0.00092156
Iteration 8/25 | Loss: 0.00092156
Iteration 9/25 | Loss: 0.00092156
Iteration 10/25 | Loss: 0.00092156
Iteration 11/25 | Loss: 0.00092156
Iteration 12/25 | Loss: 0.00092156
Iteration 13/25 | Loss: 0.00092156
Iteration 14/25 | Loss: 0.00092156
Iteration 15/25 | Loss: 0.00092156
Iteration 16/25 | Loss: 0.00092156
Iteration 17/25 | Loss: 0.00092156
Iteration 18/25 | Loss: 0.00092156
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009215603349730372, 0.0009215603349730372, 0.0009215603349730372, 0.0009215603349730372, 0.0009215603349730372]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009215603349730372

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39835429
Iteration 2/25 | Loss: 0.00061322
Iteration 3/25 | Loss: 0.00061322
Iteration 4/25 | Loss: 0.00061322
Iteration 5/25 | Loss: 0.00061322
Iteration 6/25 | Loss: 0.00061322
Iteration 7/25 | Loss: 0.00061322
Iteration 8/25 | Loss: 0.00061322
Iteration 9/25 | Loss: 0.00061322
Iteration 10/25 | Loss: 0.00061322
Iteration 11/25 | Loss: 0.00061322
Iteration 12/25 | Loss: 0.00061322
Iteration 13/25 | Loss: 0.00061322
Iteration 14/25 | Loss: 0.00061322
Iteration 15/25 | Loss: 0.00061322
Iteration 16/25 | Loss: 0.00061322
Iteration 17/25 | Loss: 0.00061322
Iteration 18/25 | Loss: 0.00061322
Iteration 19/25 | Loss: 0.00061322
Iteration 20/25 | Loss: 0.00061322
Iteration 21/25 | Loss: 0.00061322
Iteration 22/25 | Loss: 0.00061322
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0006132155540399253, 0.0006132155540399253, 0.0006132155540399253, 0.0006132155540399253, 0.0006132155540399253]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006132155540399253

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061322
Iteration 2/1000 | Loss: 0.00002684
Iteration 3/1000 | Loss: 0.00001513
Iteration 4/1000 | Loss: 0.00001125
Iteration 5/1000 | Loss: 0.00001030
Iteration 6/1000 | Loss: 0.00000969
Iteration 7/1000 | Loss: 0.00000945
Iteration 8/1000 | Loss: 0.00000926
Iteration 9/1000 | Loss: 0.00000916
Iteration 10/1000 | Loss: 0.00000915
Iteration 11/1000 | Loss: 0.00000912
Iteration 12/1000 | Loss: 0.00000912
Iteration 13/1000 | Loss: 0.00000911
Iteration 14/1000 | Loss: 0.00000911
Iteration 15/1000 | Loss: 0.00000911
Iteration 16/1000 | Loss: 0.00000910
Iteration 17/1000 | Loss: 0.00000910
Iteration 18/1000 | Loss: 0.00000910
Iteration 19/1000 | Loss: 0.00000909
Iteration 20/1000 | Loss: 0.00000909
Iteration 21/1000 | Loss: 0.00000908
Iteration 22/1000 | Loss: 0.00000908
Iteration 23/1000 | Loss: 0.00000908
Iteration 24/1000 | Loss: 0.00000907
Iteration 25/1000 | Loss: 0.00000907
Iteration 26/1000 | Loss: 0.00000907
Iteration 27/1000 | Loss: 0.00000906
Iteration 28/1000 | Loss: 0.00000906
Iteration 29/1000 | Loss: 0.00000905
Iteration 30/1000 | Loss: 0.00000905
Iteration 31/1000 | Loss: 0.00000904
Iteration 32/1000 | Loss: 0.00000904
Iteration 33/1000 | Loss: 0.00000904
Iteration 34/1000 | Loss: 0.00000904
Iteration 35/1000 | Loss: 0.00000904
Iteration 36/1000 | Loss: 0.00000904
Iteration 37/1000 | Loss: 0.00000904
Iteration 38/1000 | Loss: 0.00000903
Iteration 39/1000 | Loss: 0.00000903
Iteration 40/1000 | Loss: 0.00000903
Iteration 41/1000 | Loss: 0.00000902
Iteration 42/1000 | Loss: 0.00000901
Iteration 43/1000 | Loss: 0.00000900
Iteration 44/1000 | Loss: 0.00000900
Iteration 45/1000 | Loss: 0.00000899
Iteration 46/1000 | Loss: 0.00000899
Iteration 47/1000 | Loss: 0.00000899
Iteration 48/1000 | Loss: 0.00000899
Iteration 49/1000 | Loss: 0.00000899
Iteration 50/1000 | Loss: 0.00000899
Iteration 51/1000 | Loss: 0.00000898
Iteration 52/1000 | Loss: 0.00000898
Iteration 53/1000 | Loss: 0.00000898
Iteration 54/1000 | Loss: 0.00000898
Iteration 55/1000 | Loss: 0.00000897
Iteration 56/1000 | Loss: 0.00000897
Iteration 57/1000 | Loss: 0.00000896
Iteration 58/1000 | Loss: 0.00000896
Iteration 59/1000 | Loss: 0.00000896
Iteration 60/1000 | Loss: 0.00000896
Iteration 61/1000 | Loss: 0.00000896
Iteration 62/1000 | Loss: 0.00000896
Iteration 63/1000 | Loss: 0.00000896
Iteration 64/1000 | Loss: 0.00000896
Iteration 65/1000 | Loss: 0.00000896
Iteration 66/1000 | Loss: 0.00000895
Iteration 67/1000 | Loss: 0.00000895
Iteration 68/1000 | Loss: 0.00000895
Iteration 69/1000 | Loss: 0.00000894
Iteration 70/1000 | Loss: 0.00000894
Iteration 71/1000 | Loss: 0.00000894
Iteration 72/1000 | Loss: 0.00000894
Iteration 73/1000 | Loss: 0.00000893
Iteration 74/1000 | Loss: 0.00000893
Iteration 75/1000 | Loss: 0.00000893
Iteration 76/1000 | Loss: 0.00000893
Iteration 77/1000 | Loss: 0.00000893
Iteration 78/1000 | Loss: 0.00000893
Iteration 79/1000 | Loss: 0.00000893
Iteration 80/1000 | Loss: 0.00000893
Iteration 81/1000 | Loss: 0.00000893
Iteration 82/1000 | Loss: 0.00000892
Iteration 83/1000 | Loss: 0.00000892
Iteration 84/1000 | Loss: 0.00000892
Iteration 85/1000 | Loss: 0.00000892
Iteration 86/1000 | Loss: 0.00000891
Iteration 87/1000 | Loss: 0.00000891
Iteration 88/1000 | Loss: 0.00000891
Iteration 89/1000 | Loss: 0.00000891
Iteration 90/1000 | Loss: 0.00000890
Iteration 91/1000 | Loss: 0.00000890
Iteration 92/1000 | Loss: 0.00000890
Iteration 93/1000 | Loss: 0.00000890
Iteration 94/1000 | Loss: 0.00000890
Iteration 95/1000 | Loss: 0.00000890
Iteration 96/1000 | Loss: 0.00000889
Iteration 97/1000 | Loss: 0.00000889
Iteration 98/1000 | Loss: 0.00000889
Iteration 99/1000 | Loss: 0.00000889
Iteration 100/1000 | Loss: 0.00000889
Iteration 101/1000 | Loss: 0.00000889
Iteration 102/1000 | Loss: 0.00000888
Iteration 103/1000 | Loss: 0.00000888
Iteration 104/1000 | Loss: 0.00000888
Iteration 105/1000 | Loss: 0.00000888
Iteration 106/1000 | Loss: 0.00000888
Iteration 107/1000 | Loss: 0.00000888
Iteration 108/1000 | Loss: 0.00000888
Iteration 109/1000 | Loss: 0.00000888
Iteration 110/1000 | Loss: 0.00000887
Iteration 111/1000 | Loss: 0.00000887
Iteration 112/1000 | Loss: 0.00000887
Iteration 113/1000 | Loss: 0.00000887
Iteration 114/1000 | Loss: 0.00000887
Iteration 115/1000 | Loss: 0.00000886
Iteration 116/1000 | Loss: 0.00000886
Iteration 117/1000 | Loss: 0.00000886
Iteration 118/1000 | Loss: 0.00000885
Iteration 119/1000 | Loss: 0.00000885
Iteration 120/1000 | Loss: 0.00000885
Iteration 121/1000 | Loss: 0.00000885
Iteration 122/1000 | Loss: 0.00000885
Iteration 123/1000 | Loss: 0.00000884
Iteration 124/1000 | Loss: 0.00000884
Iteration 125/1000 | Loss: 0.00000884
Iteration 126/1000 | Loss: 0.00000884
Iteration 127/1000 | Loss: 0.00000884
Iteration 128/1000 | Loss: 0.00000884
Iteration 129/1000 | Loss: 0.00000884
Iteration 130/1000 | Loss: 0.00000884
Iteration 131/1000 | Loss: 0.00000883
Iteration 132/1000 | Loss: 0.00000883
Iteration 133/1000 | Loss: 0.00000883
Iteration 134/1000 | Loss: 0.00000883
Iteration 135/1000 | Loss: 0.00000883
Iteration 136/1000 | Loss: 0.00000883
Iteration 137/1000 | Loss: 0.00000883
Iteration 138/1000 | Loss: 0.00000883
Iteration 139/1000 | Loss: 0.00000883
Iteration 140/1000 | Loss: 0.00000883
Iteration 141/1000 | Loss: 0.00000883
Iteration 142/1000 | Loss: 0.00000883
Iteration 143/1000 | Loss: 0.00000883
Iteration 144/1000 | Loss: 0.00000883
Iteration 145/1000 | Loss: 0.00000883
Iteration 146/1000 | Loss: 0.00000883
Iteration 147/1000 | Loss: 0.00000883
Iteration 148/1000 | Loss: 0.00000882
Iteration 149/1000 | Loss: 0.00000882
Iteration 150/1000 | Loss: 0.00000882
Iteration 151/1000 | Loss: 0.00000882
Iteration 152/1000 | Loss: 0.00000882
Iteration 153/1000 | Loss: 0.00000882
Iteration 154/1000 | Loss: 0.00000882
Iteration 155/1000 | Loss: 0.00000882
Iteration 156/1000 | Loss: 0.00000882
Iteration 157/1000 | Loss: 0.00000882
Iteration 158/1000 | Loss: 0.00000882
Iteration 159/1000 | Loss: 0.00000882
Iteration 160/1000 | Loss: 0.00000882
Iteration 161/1000 | Loss: 0.00000882
Iteration 162/1000 | Loss: 0.00000882
Iteration 163/1000 | Loss: 0.00000882
Iteration 164/1000 | Loss: 0.00000882
Iteration 165/1000 | Loss: 0.00000882
Iteration 166/1000 | Loss: 0.00000882
Iteration 167/1000 | Loss: 0.00000882
Iteration 168/1000 | Loss: 0.00000882
Iteration 169/1000 | Loss: 0.00000882
Iteration 170/1000 | Loss: 0.00000882
Iteration 171/1000 | Loss: 0.00000882
Iteration 172/1000 | Loss: 0.00000882
Iteration 173/1000 | Loss: 0.00000882
Iteration 174/1000 | Loss: 0.00000882
Iteration 175/1000 | Loss: 0.00000882
Iteration 176/1000 | Loss: 0.00000882
Iteration 177/1000 | Loss: 0.00000882
Iteration 178/1000 | Loss: 0.00000882
Iteration 179/1000 | Loss: 0.00000882
Iteration 180/1000 | Loss: 0.00000882
Iteration 181/1000 | Loss: 0.00000882
Iteration 182/1000 | Loss: 0.00000882
Iteration 183/1000 | Loss: 0.00000882
Iteration 184/1000 | Loss: 0.00000882
Iteration 185/1000 | Loss: 0.00000882
Iteration 186/1000 | Loss: 0.00000882
Iteration 187/1000 | Loss: 0.00000882
Iteration 188/1000 | Loss: 0.00000882
Iteration 189/1000 | Loss: 0.00000882
Iteration 190/1000 | Loss: 0.00000882
Iteration 191/1000 | Loss: 0.00000882
Iteration 192/1000 | Loss: 0.00000882
Iteration 193/1000 | Loss: 0.00000882
Iteration 194/1000 | Loss: 0.00000882
Iteration 195/1000 | Loss: 0.00000882
Iteration 196/1000 | Loss: 0.00000882
Iteration 197/1000 | Loss: 0.00000882
Iteration 198/1000 | Loss: 0.00000882
Iteration 199/1000 | Loss: 0.00000882
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [8.816271474643145e-06, 8.816271474643145e-06, 8.816271474643145e-06, 8.816271474643145e-06, 8.816271474643145e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.816271474643145e-06

Optimization complete. Final v2v error: 2.5027432441711426 mm

Highest mean error: 3.5363640785217285 mm for frame 73

Lowest mean error: 2.2521164417266846 mm for frame 108

Saving results

Total time: 32.2777841091156
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00816240
Iteration 2/25 | Loss: 0.00106417
Iteration 3/25 | Loss: 0.00092512
Iteration 4/25 | Loss: 0.00091786
Iteration 5/25 | Loss: 0.00091725
Iteration 6/25 | Loss: 0.00091725
Iteration 7/25 | Loss: 0.00091725
Iteration 8/25 | Loss: 0.00091725
Iteration 9/25 | Loss: 0.00091725
Iteration 10/25 | Loss: 0.00091725
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0009172523277811706, 0.0009172523277811706, 0.0009172523277811706, 0.0009172523277811706, 0.0009172523277811706]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009172523277811706

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33921123
Iteration 2/25 | Loss: 0.00056038
Iteration 3/25 | Loss: 0.00056038
Iteration 4/25 | Loss: 0.00056038
Iteration 5/25 | Loss: 0.00056038
Iteration 6/25 | Loss: 0.00056038
Iteration 7/25 | Loss: 0.00056038
Iteration 8/25 | Loss: 0.00056037
Iteration 9/25 | Loss: 0.00056037
Iteration 10/25 | Loss: 0.00056037
Iteration 11/25 | Loss: 0.00056037
Iteration 12/25 | Loss: 0.00056037
Iteration 13/25 | Loss: 0.00056037
Iteration 14/25 | Loss: 0.00056037
Iteration 15/25 | Loss: 0.00056037
Iteration 16/25 | Loss: 0.00056037
Iteration 17/25 | Loss: 0.00056037
Iteration 18/25 | Loss: 0.00056037
Iteration 19/25 | Loss: 0.00056037
Iteration 20/25 | Loss: 0.00056037
Iteration 21/25 | Loss: 0.00056037
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005603737663477659, 0.0005603737663477659, 0.0005603737663477659, 0.0005603737663477659, 0.0005603737663477659]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005603737663477659

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056037
Iteration 2/1000 | Loss: 0.00001816
Iteration 3/1000 | Loss: 0.00001144
Iteration 4/1000 | Loss: 0.00001040
Iteration 5/1000 | Loss: 0.00000970
Iteration 6/1000 | Loss: 0.00000940
Iteration 7/1000 | Loss: 0.00000902
Iteration 8/1000 | Loss: 0.00000895
Iteration 9/1000 | Loss: 0.00000878
Iteration 10/1000 | Loss: 0.00000875
Iteration 11/1000 | Loss: 0.00000867
Iteration 12/1000 | Loss: 0.00000866
Iteration 13/1000 | Loss: 0.00000866
Iteration 14/1000 | Loss: 0.00000866
Iteration 15/1000 | Loss: 0.00000863
Iteration 16/1000 | Loss: 0.00000861
Iteration 17/1000 | Loss: 0.00000861
Iteration 18/1000 | Loss: 0.00000860
Iteration 19/1000 | Loss: 0.00000860
Iteration 20/1000 | Loss: 0.00000856
Iteration 21/1000 | Loss: 0.00000853
Iteration 22/1000 | Loss: 0.00000852
Iteration 23/1000 | Loss: 0.00000852
Iteration 24/1000 | Loss: 0.00000852
Iteration 25/1000 | Loss: 0.00000850
Iteration 26/1000 | Loss: 0.00000850
Iteration 27/1000 | Loss: 0.00000847
Iteration 28/1000 | Loss: 0.00000847
Iteration 29/1000 | Loss: 0.00000846
Iteration 30/1000 | Loss: 0.00000846
Iteration 31/1000 | Loss: 0.00000843
Iteration 32/1000 | Loss: 0.00000843
Iteration 33/1000 | Loss: 0.00000843
Iteration 34/1000 | Loss: 0.00000842
Iteration 35/1000 | Loss: 0.00000842
Iteration 36/1000 | Loss: 0.00000842
Iteration 37/1000 | Loss: 0.00000842
Iteration 38/1000 | Loss: 0.00000841
Iteration 39/1000 | Loss: 0.00000841
Iteration 40/1000 | Loss: 0.00000841
Iteration 41/1000 | Loss: 0.00000840
Iteration 42/1000 | Loss: 0.00000840
Iteration 43/1000 | Loss: 0.00000839
Iteration 44/1000 | Loss: 0.00000839
Iteration 45/1000 | Loss: 0.00000839
Iteration 46/1000 | Loss: 0.00000839
Iteration 47/1000 | Loss: 0.00000839
Iteration 48/1000 | Loss: 0.00000839
Iteration 49/1000 | Loss: 0.00000839
Iteration 50/1000 | Loss: 0.00000839
Iteration 51/1000 | Loss: 0.00000838
Iteration 52/1000 | Loss: 0.00000838
Iteration 53/1000 | Loss: 0.00000838
Iteration 54/1000 | Loss: 0.00000834
Iteration 55/1000 | Loss: 0.00000833
Iteration 56/1000 | Loss: 0.00000833
Iteration 57/1000 | Loss: 0.00000833
Iteration 58/1000 | Loss: 0.00000833
Iteration 59/1000 | Loss: 0.00000833
Iteration 60/1000 | Loss: 0.00000833
Iteration 61/1000 | Loss: 0.00000832
Iteration 62/1000 | Loss: 0.00000832
Iteration 63/1000 | Loss: 0.00000831
Iteration 64/1000 | Loss: 0.00000831
Iteration 65/1000 | Loss: 0.00000831
Iteration 66/1000 | Loss: 0.00000831
Iteration 67/1000 | Loss: 0.00000831
Iteration 68/1000 | Loss: 0.00000831
Iteration 69/1000 | Loss: 0.00000831
Iteration 70/1000 | Loss: 0.00000830
Iteration 71/1000 | Loss: 0.00000830
Iteration 72/1000 | Loss: 0.00000830
Iteration 73/1000 | Loss: 0.00000830
Iteration 74/1000 | Loss: 0.00000830
Iteration 75/1000 | Loss: 0.00000829
Iteration 76/1000 | Loss: 0.00000829
Iteration 77/1000 | Loss: 0.00000828
Iteration 78/1000 | Loss: 0.00000828
Iteration 79/1000 | Loss: 0.00000828
Iteration 80/1000 | Loss: 0.00000828
Iteration 81/1000 | Loss: 0.00000827
Iteration 82/1000 | Loss: 0.00000827
Iteration 83/1000 | Loss: 0.00000826
Iteration 84/1000 | Loss: 0.00000826
Iteration 85/1000 | Loss: 0.00000826
Iteration 86/1000 | Loss: 0.00000826
Iteration 87/1000 | Loss: 0.00000826
Iteration 88/1000 | Loss: 0.00000826
Iteration 89/1000 | Loss: 0.00000826
Iteration 90/1000 | Loss: 0.00000826
Iteration 91/1000 | Loss: 0.00000825
Iteration 92/1000 | Loss: 0.00000825
Iteration 93/1000 | Loss: 0.00000825
Iteration 94/1000 | Loss: 0.00000825
Iteration 95/1000 | Loss: 0.00000825
Iteration 96/1000 | Loss: 0.00000824
Iteration 97/1000 | Loss: 0.00000824
Iteration 98/1000 | Loss: 0.00000824
Iteration 99/1000 | Loss: 0.00000824
Iteration 100/1000 | Loss: 0.00000822
Iteration 101/1000 | Loss: 0.00000822
Iteration 102/1000 | Loss: 0.00000822
Iteration 103/1000 | Loss: 0.00000822
Iteration 104/1000 | Loss: 0.00000822
Iteration 105/1000 | Loss: 0.00000822
Iteration 106/1000 | Loss: 0.00000821
Iteration 107/1000 | Loss: 0.00000821
Iteration 108/1000 | Loss: 0.00000821
Iteration 109/1000 | Loss: 0.00000820
Iteration 110/1000 | Loss: 0.00000820
Iteration 111/1000 | Loss: 0.00000820
Iteration 112/1000 | Loss: 0.00000819
Iteration 113/1000 | Loss: 0.00000819
Iteration 114/1000 | Loss: 0.00000819
Iteration 115/1000 | Loss: 0.00000819
Iteration 116/1000 | Loss: 0.00000819
Iteration 117/1000 | Loss: 0.00000819
Iteration 118/1000 | Loss: 0.00000819
Iteration 119/1000 | Loss: 0.00000819
Iteration 120/1000 | Loss: 0.00000819
Iteration 121/1000 | Loss: 0.00000819
Iteration 122/1000 | Loss: 0.00000819
Iteration 123/1000 | Loss: 0.00000819
Iteration 124/1000 | Loss: 0.00000819
Iteration 125/1000 | Loss: 0.00000819
Iteration 126/1000 | Loss: 0.00000819
Iteration 127/1000 | Loss: 0.00000819
Iteration 128/1000 | Loss: 0.00000819
Iteration 129/1000 | Loss: 0.00000819
Iteration 130/1000 | Loss: 0.00000819
Iteration 131/1000 | Loss: 0.00000819
Iteration 132/1000 | Loss: 0.00000819
Iteration 133/1000 | Loss: 0.00000819
Iteration 134/1000 | Loss: 0.00000819
Iteration 135/1000 | Loss: 0.00000819
Iteration 136/1000 | Loss: 0.00000819
Iteration 137/1000 | Loss: 0.00000819
Iteration 138/1000 | Loss: 0.00000819
Iteration 139/1000 | Loss: 0.00000819
Iteration 140/1000 | Loss: 0.00000819
Iteration 141/1000 | Loss: 0.00000819
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [8.187995263142511e-06, 8.187995263142511e-06, 8.187995263142511e-06, 8.187995263142511e-06, 8.187995263142511e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.187995263142511e-06

Optimization complete. Final v2v error: 2.4304697513580322 mm

Highest mean error: 2.5734503269195557 mm for frame 43

Lowest mean error: 2.3326456546783447 mm for frame 251

Saving results

Total time: 35.036452770233154
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00440836
Iteration 2/25 | Loss: 0.00113500
Iteration 3/25 | Loss: 0.00099431
Iteration 4/25 | Loss: 0.00098183
Iteration 5/25 | Loss: 0.00097793
Iteration 6/25 | Loss: 0.00097717
Iteration 7/25 | Loss: 0.00097717
Iteration 8/25 | Loss: 0.00097717
Iteration 9/25 | Loss: 0.00097717
Iteration 10/25 | Loss: 0.00097717
Iteration 11/25 | Loss: 0.00097717
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00097716657910496, 0.00097716657910496, 0.00097716657910496, 0.00097716657910496, 0.00097716657910496]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00097716657910496

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.97436213
Iteration 2/25 | Loss: 0.00051137
Iteration 3/25 | Loss: 0.00051137
Iteration 4/25 | Loss: 0.00051137
Iteration 5/25 | Loss: 0.00051137
Iteration 6/25 | Loss: 0.00051137
Iteration 7/25 | Loss: 0.00051137
Iteration 8/25 | Loss: 0.00051137
Iteration 9/25 | Loss: 0.00051137
Iteration 10/25 | Loss: 0.00051137
Iteration 11/25 | Loss: 0.00051137
Iteration 12/25 | Loss: 0.00051137
Iteration 13/25 | Loss: 0.00051137
Iteration 14/25 | Loss: 0.00051137
Iteration 15/25 | Loss: 0.00051137
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0005113680963404477, 0.0005113680963404477, 0.0005113680963404477, 0.0005113680963404477, 0.0005113680963404477]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005113680963404477

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051137
Iteration 2/1000 | Loss: 0.00003167
Iteration 3/1000 | Loss: 0.00001977
Iteration 4/1000 | Loss: 0.00001512
Iteration 5/1000 | Loss: 0.00001411
Iteration 6/1000 | Loss: 0.00001354
Iteration 7/1000 | Loss: 0.00001329
Iteration 8/1000 | Loss: 0.00001302
Iteration 9/1000 | Loss: 0.00001277
Iteration 10/1000 | Loss: 0.00001276
Iteration 11/1000 | Loss: 0.00001275
Iteration 12/1000 | Loss: 0.00001275
Iteration 13/1000 | Loss: 0.00001275
Iteration 14/1000 | Loss: 0.00001275
Iteration 15/1000 | Loss: 0.00001275
Iteration 16/1000 | Loss: 0.00001272
Iteration 17/1000 | Loss: 0.00001272
Iteration 18/1000 | Loss: 0.00001272
Iteration 19/1000 | Loss: 0.00001272
Iteration 20/1000 | Loss: 0.00001272
Iteration 21/1000 | Loss: 0.00001272
Iteration 22/1000 | Loss: 0.00001272
Iteration 23/1000 | Loss: 0.00001272
Iteration 24/1000 | Loss: 0.00001271
Iteration 25/1000 | Loss: 0.00001271
Iteration 26/1000 | Loss: 0.00001268
Iteration 27/1000 | Loss: 0.00001263
Iteration 28/1000 | Loss: 0.00001263
Iteration 29/1000 | Loss: 0.00001263
Iteration 30/1000 | Loss: 0.00001263
Iteration 31/1000 | Loss: 0.00001262
Iteration 32/1000 | Loss: 0.00001262
Iteration 33/1000 | Loss: 0.00001262
Iteration 34/1000 | Loss: 0.00001262
Iteration 35/1000 | Loss: 0.00001262
Iteration 36/1000 | Loss: 0.00001262
Iteration 37/1000 | Loss: 0.00001262
Iteration 38/1000 | Loss: 0.00001262
Iteration 39/1000 | Loss: 0.00001262
Iteration 40/1000 | Loss: 0.00001261
Iteration 41/1000 | Loss: 0.00001261
Iteration 42/1000 | Loss: 0.00001259
Iteration 43/1000 | Loss: 0.00001259
Iteration 44/1000 | Loss: 0.00001258
Iteration 45/1000 | Loss: 0.00001258
Iteration 46/1000 | Loss: 0.00001258
Iteration 47/1000 | Loss: 0.00001258
Iteration 48/1000 | Loss: 0.00001258
Iteration 49/1000 | Loss: 0.00001258
Iteration 50/1000 | Loss: 0.00001258
Iteration 51/1000 | Loss: 0.00001258
Iteration 52/1000 | Loss: 0.00001258
Iteration 53/1000 | Loss: 0.00001258
Iteration 54/1000 | Loss: 0.00001257
Iteration 55/1000 | Loss: 0.00001255
Iteration 56/1000 | Loss: 0.00001255
Iteration 57/1000 | Loss: 0.00001252
Iteration 58/1000 | Loss: 0.00001252
Iteration 59/1000 | Loss: 0.00001252
Iteration 60/1000 | Loss: 0.00001251
Iteration 61/1000 | Loss: 0.00001251
Iteration 62/1000 | Loss: 0.00001251
Iteration 63/1000 | Loss: 0.00001251
Iteration 64/1000 | Loss: 0.00001251
Iteration 65/1000 | Loss: 0.00001251
Iteration 66/1000 | Loss: 0.00001250
Iteration 67/1000 | Loss: 0.00001250
Iteration 68/1000 | Loss: 0.00001250
Iteration 69/1000 | Loss: 0.00001250
Iteration 70/1000 | Loss: 0.00001250
Iteration 71/1000 | Loss: 0.00001250
Iteration 72/1000 | Loss: 0.00001249
Iteration 73/1000 | Loss: 0.00001249
Iteration 74/1000 | Loss: 0.00001248
Iteration 75/1000 | Loss: 0.00001247
Iteration 76/1000 | Loss: 0.00001247
Iteration 77/1000 | Loss: 0.00001246
Iteration 78/1000 | Loss: 0.00001246
Iteration 79/1000 | Loss: 0.00001246
Iteration 80/1000 | Loss: 0.00001246
Iteration 81/1000 | Loss: 0.00001245
Iteration 82/1000 | Loss: 0.00001245
Iteration 83/1000 | Loss: 0.00001245
Iteration 84/1000 | Loss: 0.00001245
Iteration 85/1000 | Loss: 0.00001244
Iteration 86/1000 | Loss: 0.00001244
Iteration 87/1000 | Loss: 0.00001244
Iteration 88/1000 | Loss: 0.00001244
Iteration 89/1000 | Loss: 0.00001244
Iteration 90/1000 | Loss: 0.00001244
Iteration 91/1000 | Loss: 0.00001244
Iteration 92/1000 | Loss: 0.00001243
Iteration 93/1000 | Loss: 0.00001243
Iteration 94/1000 | Loss: 0.00001243
Iteration 95/1000 | Loss: 0.00001243
Iteration 96/1000 | Loss: 0.00001243
Iteration 97/1000 | Loss: 0.00001243
Iteration 98/1000 | Loss: 0.00001243
Iteration 99/1000 | Loss: 0.00001243
Iteration 100/1000 | Loss: 0.00001243
Iteration 101/1000 | Loss: 0.00001243
Iteration 102/1000 | Loss: 0.00001243
Iteration 103/1000 | Loss: 0.00001242
Iteration 104/1000 | Loss: 0.00001242
Iteration 105/1000 | Loss: 0.00001242
Iteration 106/1000 | Loss: 0.00001242
Iteration 107/1000 | Loss: 0.00001242
Iteration 108/1000 | Loss: 0.00001242
Iteration 109/1000 | Loss: 0.00001241
Iteration 110/1000 | Loss: 0.00001241
Iteration 111/1000 | Loss: 0.00001241
Iteration 112/1000 | Loss: 0.00001241
Iteration 113/1000 | Loss: 0.00001241
Iteration 114/1000 | Loss: 0.00001241
Iteration 115/1000 | Loss: 0.00001241
Iteration 116/1000 | Loss: 0.00001241
Iteration 117/1000 | Loss: 0.00001241
Iteration 118/1000 | Loss: 0.00001241
Iteration 119/1000 | Loss: 0.00001241
Iteration 120/1000 | Loss: 0.00001241
Iteration 121/1000 | Loss: 0.00001241
Iteration 122/1000 | Loss: 0.00001240
Iteration 123/1000 | Loss: 0.00001240
Iteration 124/1000 | Loss: 0.00001240
Iteration 125/1000 | Loss: 0.00001240
Iteration 126/1000 | Loss: 0.00001240
Iteration 127/1000 | Loss: 0.00001240
Iteration 128/1000 | Loss: 0.00001240
Iteration 129/1000 | Loss: 0.00001240
Iteration 130/1000 | Loss: 0.00001239
Iteration 131/1000 | Loss: 0.00001239
Iteration 132/1000 | Loss: 0.00001239
Iteration 133/1000 | Loss: 0.00001239
Iteration 134/1000 | Loss: 0.00001239
Iteration 135/1000 | Loss: 0.00001239
Iteration 136/1000 | Loss: 0.00001239
Iteration 137/1000 | Loss: 0.00001239
Iteration 138/1000 | Loss: 0.00001239
Iteration 139/1000 | Loss: 0.00001239
Iteration 140/1000 | Loss: 0.00001239
Iteration 141/1000 | Loss: 0.00001239
Iteration 142/1000 | Loss: 0.00001239
Iteration 143/1000 | Loss: 0.00001238
Iteration 144/1000 | Loss: 0.00001238
Iteration 145/1000 | Loss: 0.00001238
Iteration 146/1000 | Loss: 0.00001238
Iteration 147/1000 | Loss: 0.00001238
Iteration 148/1000 | Loss: 0.00001238
Iteration 149/1000 | Loss: 0.00001238
Iteration 150/1000 | Loss: 0.00001238
Iteration 151/1000 | Loss: 0.00001238
Iteration 152/1000 | Loss: 0.00001238
Iteration 153/1000 | Loss: 0.00001238
Iteration 154/1000 | Loss: 0.00001238
Iteration 155/1000 | Loss: 0.00001238
Iteration 156/1000 | Loss: 0.00001238
Iteration 157/1000 | Loss: 0.00001238
Iteration 158/1000 | Loss: 0.00001238
Iteration 159/1000 | Loss: 0.00001238
Iteration 160/1000 | Loss: 0.00001238
Iteration 161/1000 | Loss: 0.00001238
Iteration 162/1000 | Loss: 0.00001238
Iteration 163/1000 | Loss: 0.00001238
Iteration 164/1000 | Loss: 0.00001238
Iteration 165/1000 | Loss: 0.00001238
Iteration 166/1000 | Loss: 0.00001238
Iteration 167/1000 | Loss: 0.00001238
Iteration 168/1000 | Loss: 0.00001238
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.2382243767206091e-05, 1.2382243767206091e-05, 1.2382243767206091e-05, 1.2382243767206091e-05, 1.2382243767206091e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2382243767206091e-05

Optimization complete. Final v2v error: 2.947697401046753 mm

Highest mean error: 2.9702320098876953 mm for frame 67

Lowest mean error: 2.892916202545166 mm for frame 96

Saving results

Total time: 29.602675437927246
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00925479
Iteration 2/25 | Loss: 0.00136050
Iteration 3/25 | Loss: 0.00106333
Iteration 4/25 | Loss: 0.00104343
Iteration 5/25 | Loss: 0.00103918
Iteration 6/25 | Loss: 0.00103844
Iteration 7/25 | Loss: 0.00103844
Iteration 8/25 | Loss: 0.00103844
Iteration 9/25 | Loss: 0.00103844
Iteration 10/25 | Loss: 0.00103844
Iteration 11/25 | Loss: 0.00103844
Iteration 12/25 | Loss: 0.00103844
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010384400375187397, 0.0010384400375187397, 0.0010384400375187397, 0.0010384400375187397, 0.0010384400375187397]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010384400375187397

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.88322842
Iteration 2/25 | Loss: 0.00025381
Iteration 3/25 | Loss: 0.00025381
Iteration 4/25 | Loss: 0.00025381
Iteration 5/25 | Loss: 0.00025381
Iteration 6/25 | Loss: 0.00025381
Iteration 7/25 | Loss: 0.00025381
Iteration 8/25 | Loss: 0.00025381
Iteration 9/25 | Loss: 0.00025381
Iteration 10/25 | Loss: 0.00025381
Iteration 11/25 | Loss: 0.00025381
Iteration 12/25 | Loss: 0.00025381
Iteration 13/25 | Loss: 0.00025381
Iteration 14/25 | Loss: 0.00025381
Iteration 15/25 | Loss: 0.00025381
Iteration 16/25 | Loss: 0.00025381
Iteration 17/25 | Loss: 0.00025381
Iteration 18/25 | Loss: 0.00025381
Iteration 19/25 | Loss: 0.00025381
Iteration 20/25 | Loss: 0.00025381
Iteration 21/25 | Loss: 0.00025381
Iteration 22/25 | Loss: 0.00025381
Iteration 23/25 | Loss: 0.00025381
Iteration 24/25 | Loss: 0.00025381
Iteration 25/25 | Loss: 0.00025381

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025381
Iteration 2/1000 | Loss: 0.00004072
Iteration 3/1000 | Loss: 0.00003343
Iteration 4/1000 | Loss: 0.00003028
Iteration 5/1000 | Loss: 0.00002910
Iteration 6/1000 | Loss: 0.00002804
Iteration 7/1000 | Loss: 0.00002750
Iteration 8/1000 | Loss: 0.00002707
Iteration 9/1000 | Loss: 0.00002680
Iteration 10/1000 | Loss: 0.00002662
Iteration 11/1000 | Loss: 0.00002639
Iteration 12/1000 | Loss: 0.00002631
Iteration 13/1000 | Loss: 0.00002630
Iteration 14/1000 | Loss: 0.00002630
Iteration 15/1000 | Loss: 0.00002628
Iteration 16/1000 | Loss: 0.00002628
Iteration 17/1000 | Loss: 0.00002628
Iteration 18/1000 | Loss: 0.00002628
Iteration 19/1000 | Loss: 0.00002628
Iteration 20/1000 | Loss: 0.00002628
Iteration 21/1000 | Loss: 0.00002628
Iteration 22/1000 | Loss: 0.00002628
Iteration 23/1000 | Loss: 0.00002628
Iteration 24/1000 | Loss: 0.00002628
Iteration 25/1000 | Loss: 0.00002627
Iteration 26/1000 | Loss: 0.00002627
Iteration 27/1000 | Loss: 0.00002627
Iteration 28/1000 | Loss: 0.00002627
Iteration 29/1000 | Loss: 0.00002627
Iteration 30/1000 | Loss: 0.00002627
Iteration 31/1000 | Loss: 0.00002627
Iteration 32/1000 | Loss: 0.00002627
Iteration 33/1000 | Loss: 0.00002627
Iteration 34/1000 | Loss: 0.00002626
Iteration 35/1000 | Loss: 0.00002625
Iteration 36/1000 | Loss: 0.00002625
Iteration 37/1000 | Loss: 0.00002624
Iteration 38/1000 | Loss: 0.00002623
Iteration 39/1000 | Loss: 0.00002623
Iteration 40/1000 | Loss: 0.00002623
Iteration 41/1000 | Loss: 0.00002623
Iteration 42/1000 | Loss: 0.00002623
Iteration 43/1000 | Loss: 0.00002622
Iteration 44/1000 | Loss: 0.00002622
Iteration 45/1000 | Loss: 0.00002622
Iteration 46/1000 | Loss: 0.00002622
Iteration 47/1000 | Loss: 0.00002621
Iteration 48/1000 | Loss: 0.00002621
Iteration 49/1000 | Loss: 0.00002621
Iteration 50/1000 | Loss: 0.00002621
Iteration 51/1000 | Loss: 0.00002621
Iteration 52/1000 | Loss: 0.00002621
Iteration 53/1000 | Loss: 0.00002621
Iteration 54/1000 | Loss: 0.00002621
Iteration 55/1000 | Loss: 0.00002621
Iteration 56/1000 | Loss: 0.00002621
Iteration 57/1000 | Loss: 0.00002621
Iteration 58/1000 | Loss: 0.00002621
Iteration 59/1000 | Loss: 0.00002621
Iteration 60/1000 | Loss: 0.00002621
Iteration 61/1000 | Loss: 0.00002621
Iteration 62/1000 | Loss: 0.00002621
Iteration 63/1000 | Loss: 0.00002621
Iteration 64/1000 | Loss: 0.00002621
Iteration 65/1000 | Loss: 0.00002621
Iteration 66/1000 | Loss: 0.00002621
Iteration 67/1000 | Loss: 0.00002621
Iteration 68/1000 | Loss: 0.00002621
Iteration 69/1000 | Loss: 0.00002621
Iteration 70/1000 | Loss: 0.00002621
Iteration 71/1000 | Loss: 0.00002621
Iteration 72/1000 | Loss: 0.00002621
Iteration 73/1000 | Loss: 0.00002621
Iteration 74/1000 | Loss: 0.00002621
Iteration 75/1000 | Loss: 0.00002621
Iteration 76/1000 | Loss: 0.00002621
Iteration 77/1000 | Loss: 0.00002621
Iteration 78/1000 | Loss: 0.00002621
Iteration 79/1000 | Loss: 0.00002621
Iteration 80/1000 | Loss: 0.00002621
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 80. Stopping optimization.
Last 5 losses: [2.6210029318463057e-05, 2.6210029318463057e-05, 2.6210029318463057e-05, 2.6210029318463057e-05, 2.6210029318463057e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6210029318463057e-05

Optimization complete. Final v2v error: 4.286459445953369 mm

Highest mean error: 4.548648357391357 mm for frame 113

Lowest mean error: 4.0304131507873535 mm for frame 81

Saving results

Total time: 27.476688623428345
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00840248
Iteration 2/25 | Loss: 0.00123011
Iteration 3/25 | Loss: 0.00102180
Iteration 4/25 | Loss: 0.00099626
Iteration 5/25 | Loss: 0.00098832
Iteration 6/25 | Loss: 0.00098685
Iteration 7/25 | Loss: 0.00098667
Iteration 8/25 | Loss: 0.00098667
Iteration 9/25 | Loss: 0.00098667
Iteration 10/25 | Loss: 0.00098667
Iteration 11/25 | Loss: 0.00098667
Iteration 12/25 | Loss: 0.00098667
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000986670609563589, 0.000986670609563589, 0.000986670609563589, 0.000986670609563589, 0.000986670609563589]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000986670609563589

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24592793
Iteration 2/25 | Loss: 0.00050259
Iteration 3/25 | Loss: 0.00050255
Iteration 4/25 | Loss: 0.00050255
Iteration 5/25 | Loss: 0.00050254
Iteration 6/25 | Loss: 0.00050254
Iteration 7/25 | Loss: 0.00050254
Iteration 8/25 | Loss: 0.00050254
Iteration 9/25 | Loss: 0.00050254
Iteration 10/25 | Loss: 0.00050254
Iteration 11/25 | Loss: 0.00050254
Iteration 12/25 | Loss: 0.00050254
Iteration 13/25 | Loss: 0.00050254
Iteration 14/25 | Loss: 0.00050254
Iteration 15/25 | Loss: 0.00050254
Iteration 16/25 | Loss: 0.00050254
Iteration 17/25 | Loss: 0.00050254
Iteration 18/25 | Loss: 0.00050254
Iteration 19/25 | Loss: 0.00050254
Iteration 20/25 | Loss: 0.00050254
Iteration 21/25 | Loss: 0.00050254
Iteration 22/25 | Loss: 0.00050254
Iteration 23/25 | Loss: 0.00050254
Iteration 24/25 | Loss: 0.00050254
Iteration 25/25 | Loss: 0.00050254

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050254
Iteration 2/1000 | Loss: 0.00005360
Iteration 3/1000 | Loss: 0.00002723
Iteration 4/1000 | Loss: 0.00002249
Iteration 5/1000 | Loss: 0.00002093
Iteration 6/1000 | Loss: 0.00002005
Iteration 7/1000 | Loss: 0.00001934
Iteration 8/1000 | Loss: 0.00001898
Iteration 9/1000 | Loss: 0.00001858
Iteration 10/1000 | Loss: 0.00001832
Iteration 11/1000 | Loss: 0.00001826
Iteration 12/1000 | Loss: 0.00001817
Iteration 13/1000 | Loss: 0.00001813
Iteration 14/1000 | Loss: 0.00001811
Iteration 15/1000 | Loss: 0.00001810
Iteration 16/1000 | Loss: 0.00001805
Iteration 17/1000 | Loss: 0.00001795
Iteration 18/1000 | Loss: 0.00001786
Iteration 19/1000 | Loss: 0.00001777
Iteration 20/1000 | Loss: 0.00001775
Iteration 21/1000 | Loss: 0.00001774
Iteration 22/1000 | Loss: 0.00001774
Iteration 23/1000 | Loss: 0.00001773
Iteration 24/1000 | Loss: 0.00001773
Iteration 25/1000 | Loss: 0.00001773
Iteration 26/1000 | Loss: 0.00001772
Iteration 27/1000 | Loss: 0.00001772
Iteration 28/1000 | Loss: 0.00001771
Iteration 29/1000 | Loss: 0.00001770
Iteration 30/1000 | Loss: 0.00001770
Iteration 31/1000 | Loss: 0.00001770
Iteration 32/1000 | Loss: 0.00001769
Iteration 33/1000 | Loss: 0.00001769
Iteration 34/1000 | Loss: 0.00001768
Iteration 35/1000 | Loss: 0.00001768
Iteration 36/1000 | Loss: 0.00001767
Iteration 37/1000 | Loss: 0.00001767
Iteration 38/1000 | Loss: 0.00001767
Iteration 39/1000 | Loss: 0.00001766
Iteration 40/1000 | Loss: 0.00001766
Iteration 41/1000 | Loss: 0.00001765
Iteration 42/1000 | Loss: 0.00001765
Iteration 43/1000 | Loss: 0.00001765
Iteration 44/1000 | Loss: 0.00001764
Iteration 45/1000 | Loss: 0.00001764
Iteration 46/1000 | Loss: 0.00001764
Iteration 47/1000 | Loss: 0.00001764
Iteration 48/1000 | Loss: 0.00001764
Iteration 49/1000 | Loss: 0.00001763
Iteration 50/1000 | Loss: 0.00001763
Iteration 51/1000 | Loss: 0.00001763
Iteration 52/1000 | Loss: 0.00001763
Iteration 53/1000 | Loss: 0.00001763
Iteration 54/1000 | Loss: 0.00001762
Iteration 55/1000 | Loss: 0.00001762
Iteration 56/1000 | Loss: 0.00001762
Iteration 57/1000 | Loss: 0.00001762
Iteration 58/1000 | Loss: 0.00001761
Iteration 59/1000 | Loss: 0.00001761
Iteration 60/1000 | Loss: 0.00001761
Iteration 61/1000 | Loss: 0.00001761
Iteration 62/1000 | Loss: 0.00001761
Iteration 63/1000 | Loss: 0.00001761
Iteration 64/1000 | Loss: 0.00001761
Iteration 65/1000 | Loss: 0.00001761
Iteration 66/1000 | Loss: 0.00001761
Iteration 67/1000 | Loss: 0.00001761
Iteration 68/1000 | Loss: 0.00001760
Iteration 69/1000 | Loss: 0.00001760
Iteration 70/1000 | Loss: 0.00001760
Iteration 71/1000 | Loss: 0.00001760
Iteration 72/1000 | Loss: 0.00001759
Iteration 73/1000 | Loss: 0.00001759
Iteration 74/1000 | Loss: 0.00001759
Iteration 75/1000 | Loss: 0.00001759
Iteration 76/1000 | Loss: 0.00001758
Iteration 77/1000 | Loss: 0.00001758
Iteration 78/1000 | Loss: 0.00001758
Iteration 79/1000 | Loss: 0.00001758
Iteration 80/1000 | Loss: 0.00001758
Iteration 81/1000 | Loss: 0.00001758
Iteration 82/1000 | Loss: 0.00001758
Iteration 83/1000 | Loss: 0.00001758
Iteration 84/1000 | Loss: 0.00001758
Iteration 85/1000 | Loss: 0.00001758
Iteration 86/1000 | Loss: 0.00001758
Iteration 87/1000 | Loss: 0.00001758
Iteration 88/1000 | Loss: 0.00001758
Iteration 89/1000 | Loss: 0.00001758
Iteration 90/1000 | Loss: 0.00001758
Iteration 91/1000 | Loss: 0.00001758
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [1.7578415281604975e-05, 1.7578415281604975e-05, 1.7578415281604975e-05, 1.7578415281604975e-05, 1.7578415281604975e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7578415281604975e-05

Optimization complete. Final v2v error: 3.4726552963256836 mm

Highest mean error: 4.744470119476318 mm for frame 53

Lowest mean error: 2.771899700164795 mm for frame 85

Saving results

Total time: 34.31517577171326
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00858819
Iteration 2/25 | Loss: 0.00131821
Iteration 3/25 | Loss: 0.00108741
Iteration 4/25 | Loss: 0.00105898
Iteration 5/25 | Loss: 0.00106096
Iteration 6/25 | Loss: 0.00105467
Iteration 7/25 | Loss: 0.00104032
Iteration 8/25 | Loss: 0.00103179
Iteration 9/25 | Loss: 0.00103064
Iteration 10/25 | Loss: 0.00103030
Iteration 11/25 | Loss: 0.00103029
Iteration 12/25 | Loss: 0.00103029
Iteration 13/25 | Loss: 0.00103029
Iteration 14/25 | Loss: 0.00103029
Iteration 15/25 | Loss: 0.00103028
Iteration 16/25 | Loss: 0.00103028
Iteration 17/25 | Loss: 0.00103028
Iteration 18/25 | Loss: 0.00103028
Iteration 19/25 | Loss: 0.00103028
Iteration 20/25 | Loss: 0.00103028
Iteration 21/25 | Loss: 0.00103028
Iteration 22/25 | Loss: 0.00103028
Iteration 23/25 | Loss: 0.00103028
Iteration 24/25 | Loss: 0.00103028
Iteration 25/25 | Loss: 0.00103028

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.30107021
Iteration 2/25 | Loss: 0.00068664
Iteration 3/25 | Loss: 0.00068650
Iteration 4/25 | Loss: 0.00068650
Iteration 5/25 | Loss: 0.00068650
Iteration 6/25 | Loss: 0.00068650
Iteration 7/25 | Loss: 0.00068650
Iteration 8/25 | Loss: 0.00068650
Iteration 9/25 | Loss: 0.00068650
Iteration 10/25 | Loss: 0.00068650
Iteration 11/25 | Loss: 0.00068650
Iteration 12/25 | Loss: 0.00068650
Iteration 13/25 | Loss: 0.00068650
Iteration 14/25 | Loss: 0.00068650
Iteration 15/25 | Loss: 0.00068650
Iteration 16/25 | Loss: 0.00068650
Iteration 17/25 | Loss: 0.00068650
Iteration 18/25 | Loss: 0.00068650
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006864971364848316, 0.0006864971364848316, 0.0006864971364848316, 0.0006864971364848316, 0.0006864971364848316]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006864971364848316

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068650
Iteration 2/1000 | Loss: 0.00004383
Iteration 3/1000 | Loss: 0.00002489
Iteration 4/1000 | Loss: 0.00002013
Iteration 5/1000 | Loss: 0.00001866
Iteration 6/1000 | Loss: 0.00001788
Iteration 7/1000 | Loss: 0.00001731
Iteration 8/1000 | Loss: 0.00001703
Iteration 9/1000 | Loss: 0.00001670
Iteration 10/1000 | Loss: 0.00001648
Iteration 11/1000 | Loss: 0.00001631
Iteration 12/1000 | Loss: 0.00001621
Iteration 13/1000 | Loss: 0.00001620
Iteration 14/1000 | Loss: 0.00001615
Iteration 15/1000 | Loss: 0.00001610
Iteration 16/1000 | Loss: 0.00001606
Iteration 17/1000 | Loss: 0.00001604
Iteration 18/1000 | Loss: 0.00001604
Iteration 19/1000 | Loss: 0.00001603
Iteration 20/1000 | Loss: 0.00001602
Iteration 21/1000 | Loss: 0.00001602
Iteration 22/1000 | Loss: 0.00001602
Iteration 23/1000 | Loss: 0.00001601
Iteration 24/1000 | Loss: 0.00001601
Iteration 25/1000 | Loss: 0.00001598
Iteration 26/1000 | Loss: 0.00001597
Iteration 27/1000 | Loss: 0.00001596
Iteration 28/1000 | Loss: 0.00001595
Iteration 29/1000 | Loss: 0.00001595
Iteration 30/1000 | Loss: 0.00001594
Iteration 31/1000 | Loss: 0.00001594
Iteration 32/1000 | Loss: 0.00001593
Iteration 33/1000 | Loss: 0.00001592
Iteration 34/1000 | Loss: 0.00001592
Iteration 35/1000 | Loss: 0.00001592
Iteration 36/1000 | Loss: 0.00001591
Iteration 37/1000 | Loss: 0.00001591
Iteration 38/1000 | Loss: 0.00001590
Iteration 39/1000 | Loss: 0.00001590
Iteration 40/1000 | Loss: 0.00001589
Iteration 41/1000 | Loss: 0.00001589
Iteration 42/1000 | Loss: 0.00001589
Iteration 43/1000 | Loss: 0.00001589
Iteration 44/1000 | Loss: 0.00001588
Iteration 45/1000 | Loss: 0.00001588
Iteration 46/1000 | Loss: 0.00001587
Iteration 47/1000 | Loss: 0.00001586
Iteration 48/1000 | Loss: 0.00001585
Iteration 49/1000 | Loss: 0.00001585
Iteration 50/1000 | Loss: 0.00001585
Iteration 51/1000 | Loss: 0.00001584
Iteration 52/1000 | Loss: 0.00001584
Iteration 53/1000 | Loss: 0.00001583
Iteration 54/1000 | Loss: 0.00001583
Iteration 55/1000 | Loss: 0.00001582
Iteration 56/1000 | Loss: 0.00001582
Iteration 57/1000 | Loss: 0.00001581
Iteration 58/1000 | Loss: 0.00001581
Iteration 59/1000 | Loss: 0.00001581
Iteration 60/1000 | Loss: 0.00001581
Iteration 61/1000 | Loss: 0.00001581
Iteration 62/1000 | Loss: 0.00001581
Iteration 63/1000 | Loss: 0.00001580
Iteration 64/1000 | Loss: 0.00001580
Iteration 65/1000 | Loss: 0.00001580
Iteration 66/1000 | Loss: 0.00001580
Iteration 67/1000 | Loss: 0.00001579
Iteration 68/1000 | Loss: 0.00001579
Iteration 69/1000 | Loss: 0.00001579
Iteration 70/1000 | Loss: 0.00001579
Iteration 71/1000 | Loss: 0.00001578
Iteration 72/1000 | Loss: 0.00001578
Iteration 73/1000 | Loss: 0.00001578
Iteration 74/1000 | Loss: 0.00001578
Iteration 75/1000 | Loss: 0.00001578
Iteration 76/1000 | Loss: 0.00001577
Iteration 77/1000 | Loss: 0.00001577
Iteration 78/1000 | Loss: 0.00001577
Iteration 79/1000 | Loss: 0.00001577
Iteration 80/1000 | Loss: 0.00001577
Iteration 81/1000 | Loss: 0.00001577
Iteration 82/1000 | Loss: 0.00001577
Iteration 83/1000 | Loss: 0.00001577
Iteration 84/1000 | Loss: 0.00001577
Iteration 85/1000 | Loss: 0.00001576
Iteration 86/1000 | Loss: 0.00001576
Iteration 87/1000 | Loss: 0.00001576
Iteration 88/1000 | Loss: 0.00001576
Iteration 89/1000 | Loss: 0.00001576
Iteration 90/1000 | Loss: 0.00001576
Iteration 91/1000 | Loss: 0.00001576
Iteration 92/1000 | Loss: 0.00001576
Iteration 93/1000 | Loss: 0.00001576
Iteration 94/1000 | Loss: 0.00001576
Iteration 95/1000 | Loss: 0.00001576
Iteration 96/1000 | Loss: 0.00001575
Iteration 97/1000 | Loss: 0.00001575
Iteration 98/1000 | Loss: 0.00001575
Iteration 99/1000 | Loss: 0.00001575
Iteration 100/1000 | Loss: 0.00001575
Iteration 101/1000 | Loss: 0.00001575
Iteration 102/1000 | Loss: 0.00001575
Iteration 103/1000 | Loss: 0.00001575
Iteration 104/1000 | Loss: 0.00001575
Iteration 105/1000 | Loss: 0.00001574
Iteration 106/1000 | Loss: 0.00001574
Iteration 107/1000 | Loss: 0.00001574
Iteration 108/1000 | Loss: 0.00001574
Iteration 109/1000 | Loss: 0.00001574
Iteration 110/1000 | Loss: 0.00001574
Iteration 111/1000 | Loss: 0.00001574
Iteration 112/1000 | Loss: 0.00001573
Iteration 113/1000 | Loss: 0.00001573
Iteration 114/1000 | Loss: 0.00001573
Iteration 115/1000 | Loss: 0.00001573
Iteration 116/1000 | Loss: 0.00001573
Iteration 117/1000 | Loss: 0.00001573
Iteration 118/1000 | Loss: 0.00001573
Iteration 119/1000 | Loss: 0.00001573
Iteration 120/1000 | Loss: 0.00001573
Iteration 121/1000 | Loss: 0.00001573
Iteration 122/1000 | Loss: 0.00001573
Iteration 123/1000 | Loss: 0.00001573
Iteration 124/1000 | Loss: 0.00001573
Iteration 125/1000 | Loss: 0.00001573
Iteration 126/1000 | Loss: 0.00001573
Iteration 127/1000 | Loss: 0.00001573
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.5731418898212723e-05, 1.5731418898212723e-05, 1.5731418898212723e-05, 1.5731418898212723e-05, 1.5731418898212723e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5731418898212723e-05

Optimization complete. Final v2v error: 3.2520344257354736 mm

Highest mean error: 4.129571914672852 mm for frame 21

Lowest mean error: 2.630075454711914 mm for frame 56

Saving results

Total time: 50.686781883239746
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_020/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_020/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00866765
Iteration 2/25 | Loss: 0.00180656
Iteration 3/25 | Loss: 0.00126146
Iteration 4/25 | Loss: 0.00116854
Iteration 5/25 | Loss: 0.00126799
Iteration 6/25 | Loss: 0.00119258
Iteration 7/25 | Loss: 0.00111815
Iteration 8/25 | Loss: 0.00106885
Iteration 9/25 | Loss: 0.00105505
Iteration 10/25 | Loss: 0.00105244
Iteration 11/25 | Loss: 0.00105166
Iteration 12/25 | Loss: 0.00105366
Iteration 13/25 | Loss: 0.00105439
Iteration 14/25 | Loss: 0.00105264
Iteration 15/25 | Loss: 0.00104909
Iteration 16/25 | Loss: 0.00104809
Iteration 17/25 | Loss: 0.00104769
Iteration 18/25 | Loss: 0.00104741
Iteration 19/25 | Loss: 0.00104732
Iteration 20/25 | Loss: 0.00104731
Iteration 21/25 | Loss: 0.00104731
Iteration 22/25 | Loss: 0.00104730
Iteration 23/25 | Loss: 0.00104730
Iteration 24/25 | Loss: 0.00104730
Iteration 25/25 | Loss: 0.00104730

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29364610
Iteration 2/25 | Loss: 0.00058930
Iteration 3/25 | Loss: 0.00058930
Iteration 4/25 | Loss: 0.00058930
Iteration 5/25 | Loss: 0.00058930
Iteration 6/25 | Loss: 0.00058930
Iteration 7/25 | Loss: 0.00058930
Iteration 8/25 | Loss: 0.00058930
Iteration 9/25 | Loss: 0.00058930
Iteration 10/25 | Loss: 0.00058930
Iteration 11/25 | Loss: 0.00058930
Iteration 12/25 | Loss: 0.00058930
Iteration 13/25 | Loss: 0.00058930
Iteration 14/25 | Loss: 0.00058930
Iteration 15/25 | Loss: 0.00058930
Iteration 16/25 | Loss: 0.00058930
Iteration 17/25 | Loss: 0.00058930
Iteration 18/25 | Loss: 0.00058930
Iteration 19/25 | Loss: 0.00058930
Iteration 20/25 | Loss: 0.00058930
Iteration 21/25 | Loss: 0.00058930
Iteration 22/25 | Loss: 0.00058930
Iteration 23/25 | Loss: 0.00058930
Iteration 24/25 | Loss: 0.00058930
Iteration 25/25 | Loss: 0.00058930

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058930
Iteration 2/1000 | Loss: 0.00047928
Iteration 3/1000 | Loss: 0.00017678
Iteration 4/1000 | Loss: 0.00059945
Iteration 5/1000 | Loss: 0.00052900
Iteration 6/1000 | Loss: 0.00023951
Iteration 7/1000 | Loss: 0.00011945
Iteration 8/1000 | Loss: 0.00004357
Iteration 9/1000 | Loss: 0.00003591
Iteration 10/1000 | Loss: 0.00042916
Iteration 11/1000 | Loss: 0.00026587
Iteration 12/1000 | Loss: 0.00033843
Iteration 13/1000 | Loss: 0.00035863
Iteration 14/1000 | Loss: 0.00032435
Iteration 15/1000 | Loss: 0.00010136
Iteration 16/1000 | Loss: 0.00005500
Iteration 17/1000 | Loss: 0.00030836
Iteration 18/1000 | Loss: 0.00004481
Iteration 19/1000 | Loss: 0.00006605
Iteration 20/1000 | Loss: 0.00017139
Iteration 21/1000 | Loss: 0.00023746
Iteration 22/1000 | Loss: 0.00010444
Iteration 23/1000 | Loss: 0.00023035
Iteration 24/1000 | Loss: 0.00010250
Iteration 25/1000 | Loss: 0.00007855
Iteration 26/1000 | Loss: 0.00061184
Iteration 27/1000 | Loss: 0.00009227
Iteration 28/1000 | Loss: 0.00004770
Iteration 29/1000 | Loss: 0.00004940
Iteration 30/1000 | Loss: 0.00004941
Iteration 31/1000 | Loss: 0.00010792
Iteration 32/1000 | Loss: 0.00005155
Iteration 33/1000 | Loss: 0.00015204
Iteration 34/1000 | Loss: 0.00006376
Iteration 35/1000 | Loss: 0.00004573
Iteration 36/1000 | Loss: 0.00011223
Iteration 37/1000 | Loss: 0.00016251
Iteration 38/1000 | Loss: 0.00005157
Iteration 39/1000 | Loss: 0.00018205
Iteration 40/1000 | Loss: 0.00034055
Iteration 41/1000 | Loss: 0.00014042
Iteration 42/1000 | Loss: 0.00004972
Iteration 43/1000 | Loss: 0.00040177
Iteration 44/1000 | Loss: 0.00035339
Iteration 45/1000 | Loss: 0.00022251
Iteration 46/1000 | Loss: 0.00020609
Iteration 47/1000 | Loss: 0.00014511
Iteration 48/1000 | Loss: 0.00036622
Iteration 49/1000 | Loss: 0.00017599
Iteration 50/1000 | Loss: 0.00014613
Iteration 51/1000 | Loss: 0.00004601
Iteration 52/1000 | Loss: 0.00020153
Iteration 53/1000 | Loss: 0.00010831
Iteration 54/1000 | Loss: 0.00014285
Iteration 55/1000 | Loss: 0.00022724
Iteration 56/1000 | Loss: 0.00014634
Iteration 57/1000 | Loss: 0.00021473
Iteration 58/1000 | Loss: 0.00015529
Iteration 59/1000 | Loss: 0.00017768
Iteration 60/1000 | Loss: 0.00019470
Iteration 61/1000 | Loss: 0.00003279
Iteration 62/1000 | Loss: 0.00003031
Iteration 63/1000 | Loss: 0.00004080
Iteration 64/1000 | Loss: 0.00008315
Iteration 65/1000 | Loss: 0.00004360
Iteration 66/1000 | Loss: 0.00003187
Iteration 67/1000 | Loss: 0.00002633
Iteration 68/1000 | Loss: 0.00004007
Iteration 69/1000 | Loss: 0.00010128
Iteration 70/1000 | Loss: 0.00009577
Iteration 71/1000 | Loss: 0.00002812
Iteration 72/1000 | Loss: 0.00003457
Iteration 73/1000 | Loss: 0.00003847
Iteration 74/1000 | Loss: 0.00003683
Iteration 75/1000 | Loss: 0.00004297
Iteration 76/1000 | Loss: 0.00020753
Iteration 77/1000 | Loss: 0.00014269
Iteration 78/1000 | Loss: 0.00019411
Iteration 79/1000 | Loss: 0.00003870
Iteration 80/1000 | Loss: 0.00003038
Iteration 81/1000 | Loss: 0.00002700
Iteration 82/1000 | Loss: 0.00002422
Iteration 83/1000 | Loss: 0.00002874
Iteration 84/1000 | Loss: 0.00002233
Iteration 85/1000 | Loss: 0.00002152
Iteration 86/1000 | Loss: 0.00002031
Iteration 87/1000 | Loss: 0.00001963
Iteration 88/1000 | Loss: 0.00001898
Iteration 89/1000 | Loss: 0.00001858
Iteration 90/1000 | Loss: 0.00001804
Iteration 91/1000 | Loss: 0.00001761
Iteration 92/1000 | Loss: 0.00001727
Iteration 93/1000 | Loss: 0.00001698
Iteration 94/1000 | Loss: 0.00001678
Iteration 95/1000 | Loss: 0.00001672
Iteration 96/1000 | Loss: 0.00001650
Iteration 97/1000 | Loss: 0.00001644
Iteration 98/1000 | Loss: 0.00001643
Iteration 99/1000 | Loss: 0.00001643
Iteration 100/1000 | Loss: 0.00001642
Iteration 101/1000 | Loss: 0.00001641
Iteration 102/1000 | Loss: 0.00001636
Iteration 103/1000 | Loss: 0.00001636
Iteration 104/1000 | Loss: 0.00001634
Iteration 105/1000 | Loss: 0.00001629
Iteration 106/1000 | Loss: 0.00001625
Iteration 107/1000 | Loss: 0.00001625
Iteration 108/1000 | Loss: 0.00001625
Iteration 109/1000 | Loss: 0.00001620
Iteration 110/1000 | Loss: 0.00001619
Iteration 111/1000 | Loss: 0.00001619
Iteration 112/1000 | Loss: 0.00001618
Iteration 113/1000 | Loss: 0.00001618
Iteration 114/1000 | Loss: 0.00001618
Iteration 115/1000 | Loss: 0.00001617
Iteration 116/1000 | Loss: 0.00001617
Iteration 117/1000 | Loss: 0.00001617
Iteration 118/1000 | Loss: 0.00001617
Iteration 119/1000 | Loss: 0.00001617
Iteration 120/1000 | Loss: 0.00001617
Iteration 121/1000 | Loss: 0.00001617
Iteration 122/1000 | Loss: 0.00001617
Iteration 123/1000 | Loss: 0.00001617
Iteration 124/1000 | Loss: 0.00001617
Iteration 125/1000 | Loss: 0.00001616
Iteration 126/1000 | Loss: 0.00001616
Iteration 127/1000 | Loss: 0.00001616
Iteration 128/1000 | Loss: 0.00001616
Iteration 129/1000 | Loss: 0.00001616
Iteration 130/1000 | Loss: 0.00001616
Iteration 131/1000 | Loss: 0.00001616
Iteration 132/1000 | Loss: 0.00001616
Iteration 133/1000 | Loss: 0.00001615
Iteration 134/1000 | Loss: 0.00001615
Iteration 135/1000 | Loss: 0.00001615
Iteration 136/1000 | Loss: 0.00001615
Iteration 137/1000 | Loss: 0.00001615
Iteration 138/1000 | Loss: 0.00001615
Iteration 139/1000 | Loss: 0.00001615
Iteration 140/1000 | Loss: 0.00001614
Iteration 141/1000 | Loss: 0.00001614
Iteration 142/1000 | Loss: 0.00001614
Iteration 143/1000 | Loss: 0.00001614
Iteration 144/1000 | Loss: 0.00001614
Iteration 145/1000 | Loss: 0.00001614
Iteration 146/1000 | Loss: 0.00001614
Iteration 147/1000 | Loss: 0.00001614
Iteration 148/1000 | Loss: 0.00001614
Iteration 149/1000 | Loss: 0.00001614
Iteration 150/1000 | Loss: 0.00001614
Iteration 151/1000 | Loss: 0.00001614
Iteration 152/1000 | Loss: 0.00001614
Iteration 153/1000 | Loss: 0.00001613
Iteration 154/1000 | Loss: 0.00001613
Iteration 155/1000 | Loss: 0.00001613
Iteration 156/1000 | Loss: 0.00001613
Iteration 157/1000 | Loss: 0.00001613
Iteration 158/1000 | Loss: 0.00001613
Iteration 159/1000 | Loss: 0.00001612
Iteration 160/1000 | Loss: 0.00001612
Iteration 161/1000 | Loss: 0.00001612
Iteration 162/1000 | Loss: 0.00001612
Iteration 163/1000 | Loss: 0.00001612
Iteration 164/1000 | Loss: 0.00001612
Iteration 165/1000 | Loss: 0.00001612
Iteration 166/1000 | Loss: 0.00001612
Iteration 167/1000 | Loss: 0.00001611
Iteration 168/1000 | Loss: 0.00001611
Iteration 169/1000 | Loss: 0.00001611
Iteration 170/1000 | Loss: 0.00001611
Iteration 171/1000 | Loss: 0.00001611
Iteration 172/1000 | Loss: 0.00001611
Iteration 173/1000 | Loss: 0.00001610
Iteration 174/1000 | Loss: 0.00001610
Iteration 175/1000 | Loss: 0.00001610
Iteration 176/1000 | Loss: 0.00001610
Iteration 177/1000 | Loss: 0.00001610
Iteration 178/1000 | Loss: 0.00001610
Iteration 179/1000 | Loss: 0.00001610
Iteration 180/1000 | Loss: 0.00001610
Iteration 181/1000 | Loss: 0.00001610
Iteration 182/1000 | Loss: 0.00001610
Iteration 183/1000 | Loss: 0.00001610
Iteration 184/1000 | Loss: 0.00001610
Iteration 185/1000 | Loss: 0.00001610
Iteration 186/1000 | Loss: 0.00001610
Iteration 187/1000 | Loss: 0.00001610
Iteration 188/1000 | Loss: 0.00001610
Iteration 189/1000 | Loss: 0.00001610
Iteration 190/1000 | Loss: 0.00001610
Iteration 191/1000 | Loss: 0.00001609
Iteration 192/1000 | Loss: 0.00001609
Iteration 193/1000 | Loss: 0.00001609
Iteration 194/1000 | Loss: 0.00001609
Iteration 195/1000 | Loss: 0.00001609
Iteration 196/1000 | Loss: 0.00001609
Iteration 197/1000 | Loss: 0.00001609
Iteration 198/1000 | Loss: 0.00001609
Iteration 199/1000 | Loss: 0.00001609
Iteration 200/1000 | Loss: 0.00001609
Iteration 201/1000 | Loss: 0.00001609
Iteration 202/1000 | Loss: 0.00001609
Iteration 203/1000 | Loss: 0.00001609
Iteration 204/1000 | Loss: 0.00001609
Iteration 205/1000 | Loss: 0.00001609
Iteration 206/1000 | Loss: 0.00001609
Iteration 207/1000 | Loss: 0.00001609
Iteration 208/1000 | Loss: 0.00001609
Iteration 209/1000 | Loss: 0.00001609
Iteration 210/1000 | Loss: 0.00001609
Iteration 211/1000 | Loss: 0.00001608
Iteration 212/1000 | Loss: 0.00001608
Iteration 213/1000 | Loss: 0.00001608
Iteration 214/1000 | Loss: 0.00001608
Iteration 215/1000 | Loss: 0.00001608
Iteration 216/1000 | Loss: 0.00001608
Iteration 217/1000 | Loss: 0.00001608
Iteration 218/1000 | Loss: 0.00001608
Iteration 219/1000 | Loss: 0.00001608
Iteration 220/1000 | Loss: 0.00001608
Iteration 221/1000 | Loss: 0.00001608
Iteration 222/1000 | Loss: 0.00001608
Iteration 223/1000 | Loss: 0.00001608
Iteration 224/1000 | Loss: 0.00001608
Iteration 225/1000 | Loss: 0.00001608
Iteration 226/1000 | Loss: 0.00001608
Iteration 227/1000 | Loss: 0.00001608
Iteration 228/1000 | Loss: 0.00001608
Iteration 229/1000 | Loss: 0.00001608
Iteration 230/1000 | Loss: 0.00001608
Iteration 231/1000 | Loss: 0.00001608
Iteration 232/1000 | Loss: 0.00001608
Iteration 233/1000 | Loss: 0.00001608
Iteration 234/1000 | Loss: 0.00001608
Iteration 235/1000 | Loss: 0.00001608
Iteration 236/1000 | Loss: 0.00001608
Iteration 237/1000 | Loss: 0.00001608
Iteration 238/1000 | Loss: 0.00001608
Iteration 239/1000 | Loss: 0.00001608
Iteration 240/1000 | Loss: 0.00001608
Iteration 241/1000 | Loss: 0.00001608
Iteration 242/1000 | Loss: 0.00001608
Iteration 243/1000 | Loss: 0.00001608
Iteration 244/1000 | Loss: 0.00001608
Iteration 245/1000 | Loss: 0.00001608
Iteration 246/1000 | Loss: 0.00001608
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 246. Stopping optimization.
Last 5 losses: [1.6083582522696815e-05, 1.6083582522696815e-05, 1.6083582522696815e-05, 1.6083582522696815e-05, 1.6083582522696815e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6083582522696815e-05

Optimization complete. Final v2v error: 3.3035888671875 mm

Highest mean error: 4.857574462890625 mm for frame 59

Lowest mean error: 2.874415397644043 mm for frame 151

Saving results

Total time: 198.30631351470947
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00835355
Iteration 2/25 | Loss: 0.00128999
Iteration 3/25 | Loss: 0.00122214
Iteration 4/25 | Loss: 0.00120934
Iteration 5/25 | Loss: 0.00120731
Iteration 6/25 | Loss: 0.00120731
Iteration 7/25 | Loss: 0.00120731
Iteration 8/25 | Loss: 0.00120731
Iteration 9/25 | Loss: 0.00120731
Iteration 10/25 | Loss: 0.00120731
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012073054676875472, 0.0012073054676875472, 0.0012073054676875472, 0.0012073054676875472, 0.0012073054676875472]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012073054676875472

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36649370
Iteration 2/25 | Loss: 0.00093466
Iteration 3/25 | Loss: 0.00093465
Iteration 4/25 | Loss: 0.00093465
Iteration 5/25 | Loss: 0.00093465
Iteration 6/25 | Loss: 0.00093464
Iteration 7/25 | Loss: 0.00093464
Iteration 8/25 | Loss: 0.00093464
Iteration 9/25 | Loss: 0.00093464
Iteration 10/25 | Loss: 0.00093464
Iteration 11/25 | Loss: 0.00093464
Iteration 12/25 | Loss: 0.00093464
Iteration 13/25 | Loss: 0.00093464
Iteration 14/25 | Loss: 0.00093464
Iteration 15/25 | Loss: 0.00093464
Iteration 16/25 | Loss: 0.00093464
Iteration 17/25 | Loss: 0.00093464
Iteration 18/25 | Loss: 0.00093464
Iteration 19/25 | Loss: 0.00093464
Iteration 20/25 | Loss: 0.00093464
Iteration 21/25 | Loss: 0.00093464
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009346433216705918, 0.0009346433216705918, 0.0009346433216705918, 0.0009346433216705918, 0.0009346433216705918]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009346433216705918

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093464
Iteration 2/1000 | Loss: 0.00002462
Iteration 3/1000 | Loss: 0.00001795
Iteration 4/1000 | Loss: 0.00001664
Iteration 5/1000 | Loss: 0.00001571
Iteration 6/1000 | Loss: 0.00001500
Iteration 7/1000 | Loss: 0.00001459
Iteration 8/1000 | Loss: 0.00001419
Iteration 9/1000 | Loss: 0.00001381
Iteration 10/1000 | Loss: 0.00001370
Iteration 11/1000 | Loss: 0.00001353
Iteration 12/1000 | Loss: 0.00001337
Iteration 13/1000 | Loss: 0.00001337
Iteration 14/1000 | Loss: 0.00001319
Iteration 15/1000 | Loss: 0.00001315
Iteration 16/1000 | Loss: 0.00001308
Iteration 17/1000 | Loss: 0.00001301
Iteration 18/1000 | Loss: 0.00001300
Iteration 19/1000 | Loss: 0.00001295
Iteration 20/1000 | Loss: 0.00001295
Iteration 21/1000 | Loss: 0.00001286
Iteration 22/1000 | Loss: 0.00001281
Iteration 23/1000 | Loss: 0.00001280
Iteration 24/1000 | Loss: 0.00001280
Iteration 25/1000 | Loss: 0.00001279
Iteration 26/1000 | Loss: 0.00001279
Iteration 27/1000 | Loss: 0.00001278
Iteration 28/1000 | Loss: 0.00001277
Iteration 29/1000 | Loss: 0.00001277
Iteration 30/1000 | Loss: 0.00001277
Iteration 31/1000 | Loss: 0.00001276
Iteration 32/1000 | Loss: 0.00001276
Iteration 33/1000 | Loss: 0.00001275
Iteration 34/1000 | Loss: 0.00001274
Iteration 35/1000 | Loss: 0.00001272
Iteration 36/1000 | Loss: 0.00001272
Iteration 37/1000 | Loss: 0.00001271
Iteration 38/1000 | Loss: 0.00001271
Iteration 39/1000 | Loss: 0.00001271
Iteration 40/1000 | Loss: 0.00001270
Iteration 41/1000 | Loss: 0.00001270
Iteration 42/1000 | Loss: 0.00001270
Iteration 43/1000 | Loss: 0.00001270
Iteration 44/1000 | Loss: 0.00001269
Iteration 45/1000 | Loss: 0.00001269
Iteration 46/1000 | Loss: 0.00001268
Iteration 47/1000 | Loss: 0.00001268
Iteration 48/1000 | Loss: 0.00001266
Iteration 49/1000 | Loss: 0.00001266
Iteration 50/1000 | Loss: 0.00001266
Iteration 51/1000 | Loss: 0.00001266
Iteration 52/1000 | Loss: 0.00001266
Iteration 53/1000 | Loss: 0.00001266
Iteration 54/1000 | Loss: 0.00001266
Iteration 55/1000 | Loss: 0.00001264
Iteration 56/1000 | Loss: 0.00001263
Iteration 57/1000 | Loss: 0.00001262
Iteration 58/1000 | Loss: 0.00001261
Iteration 59/1000 | Loss: 0.00001261
Iteration 60/1000 | Loss: 0.00001260
Iteration 61/1000 | Loss: 0.00001260
Iteration 62/1000 | Loss: 0.00001259
Iteration 63/1000 | Loss: 0.00001259
Iteration 64/1000 | Loss: 0.00001259
Iteration 65/1000 | Loss: 0.00001258
Iteration 66/1000 | Loss: 0.00001258
Iteration 67/1000 | Loss: 0.00001258
Iteration 68/1000 | Loss: 0.00001257
Iteration 69/1000 | Loss: 0.00001257
Iteration 70/1000 | Loss: 0.00001257
Iteration 71/1000 | Loss: 0.00001257
Iteration 72/1000 | Loss: 0.00001257
Iteration 73/1000 | Loss: 0.00001257
Iteration 74/1000 | Loss: 0.00001257
Iteration 75/1000 | Loss: 0.00001256
Iteration 76/1000 | Loss: 0.00001255
Iteration 77/1000 | Loss: 0.00001255
Iteration 78/1000 | Loss: 0.00001255
Iteration 79/1000 | Loss: 0.00001254
Iteration 80/1000 | Loss: 0.00001254
Iteration 81/1000 | Loss: 0.00001253
Iteration 82/1000 | Loss: 0.00001253
Iteration 83/1000 | Loss: 0.00001252
Iteration 84/1000 | Loss: 0.00001251
Iteration 85/1000 | Loss: 0.00001251
Iteration 86/1000 | Loss: 0.00001251
Iteration 87/1000 | Loss: 0.00001248
Iteration 88/1000 | Loss: 0.00001248
Iteration 89/1000 | Loss: 0.00001248
Iteration 90/1000 | Loss: 0.00001248
Iteration 91/1000 | Loss: 0.00001248
Iteration 92/1000 | Loss: 0.00001248
Iteration 93/1000 | Loss: 0.00001247
Iteration 94/1000 | Loss: 0.00001247
Iteration 95/1000 | Loss: 0.00001246
Iteration 96/1000 | Loss: 0.00001246
Iteration 97/1000 | Loss: 0.00001245
Iteration 98/1000 | Loss: 0.00001245
Iteration 99/1000 | Loss: 0.00001245
Iteration 100/1000 | Loss: 0.00001245
Iteration 101/1000 | Loss: 0.00001244
Iteration 102/1000 | Loss: 0.00001244
Iteration 103/1000 | Loss: 0.00001244
Iteration 104/1000 | Loss: 0.00001244
Iteration 105/1000 | Loss: 0.00001244
Iteration 106/1000 | Loss: 0.00001244
Iteration 107/1000 | Loss: 0.00001244
Iteration 108/1000 | Loss: 0.00001244
Iteration 109/1000 | Loss: 0.00001244
Iteration 110/1000 | Loss: 0.00001244
Iteration 111/1000 | Loss: 0.00001244
Iteration 112/1000 | Loss: 0.00001244
Iteration 113/1000 | Loss: 0.00001244
Iteration 114/1000 | Loss: 0.00001244
Iteration 115/1000 | Loss: 0.00001244
Iteration 116/1000 | Loss: 0.00001244
Iteration 117/1000 | Loss: 0.00001244
Iteration 118/1000 | Loss: 0.00001244
Iteration 119/1000 | Loss: 0.00001244
Iteration 120/1000 | Loss: 0.00001244
Iteration 121/1000 | Loss: 0.00001244
Iteration 122/1000 | Loss: 0.00001244
Iteration 123/1000 | Loss: 0.00001244
Iteration 124/1000 | Loss: 0.00001244
Iteration 125/1000 | Loss: 0.00001244
Iteration 126/1000 | Loss: 0.00001244
Iteration 127/1000 | Loss: 0.00001244
Iteration 128/1000 | Loss: 0.00001244
Iteration 129/1000 | Loss: 0.00001244
Iteration 130/1000 | Loss: 0.00001244
Iteration 131/1000 | Loss: 0.00001244
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [1.2438494195521343e-05, 1.2438494195521343e-05, 1.2438494195521343e-05, 1.2438494195521343e-05, 1.2438494195521343e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2438494195521343e-05

Optimization complete. Final v2v error: 3.032301902770996 mm

Highest mean error: 3.364673137664795 mm for frame 116

Lowest mean error: 2.8884356021881104 mm for frame 206

Saving results

Total time: 42.50488901138306
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01100933
Iteration 2/25 | Loss: 0.00216064
Iteration 3/25 | Loss: 0.00156565
Iteration 4/25 | Loss: 0.00152277
Iteration 5/25 | Loss: 0.00153961
Iteration 6/25 | Loss: 0.00149771
Iteration 7/25 | Loss: 0.00147040
Iteration 8/25 | Loss: 0.00144376
Iteration 9/25 | Loss: 0.00143186
Iteration 10/25 | Loss: 0.00142461
Iteration 11/25 | Loss: 0.00140921
Iteration 12/25 | Loss: 0.00139455
Iteration 13/25 | Loss: 0.00139785
Iteration 14/25 | Loss: 0.00139368
Iteration 15/25 | Loss: 0.00140280
Iteration 16/25 | Loss: 0.00139980
Iteration 17/25 | Loss: 0.00139689
Iteration 18/25 | Loss: 0.00138436
Iteration 19/25 | Loss: 0.00138378
Iteration 20/25 | Loss: 0.00138441
Iteration 21/25 | Loss: 0.00137926
Iteration 22/25 | Loss: 0.00138144
Iteration 23/25 | Loss: 0.00137964
Iteration 24/25 | Loss: 0.00137864
Iteration 25/25 | Loss: 0.00137443

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.95237452
Iteration 2/25 | Loss: 0.00188795
Iteration 3/25 | Loss: 0.00157956
Iteration 4/25 | Loss: 0.00157956
Iteration 5/25 | Loss: 0.00157956
Iteration 6/25 | Loss: 0.00157956
Iteration 7/25 | Loss: 0.00157956
Iteration 8/25 | Loss: 0.00157956
Iteration 9/25 | Loss: 0.00157956
Iteration 10/25 | Loss: 0.00157956
Iteration 11/25 | Loss: 0.00157956
Iteration 12/25 | Loss: 0.00157956
Iteration 13/25 | Loss: 0.00157956
Iteration 14/25 | Loss: 0.00157956
Iteration 15/25 | Loss: 0.00157956
Iteration 16/25 | Loss: 0.00157956
Iteration 17/25 | Loss: 0.00157956
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0015795594081282616, 0.0015795594081282616, 0.0015795594081282616, 0.0015795594081282616, 0.0015795594081282616]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015795594081282616

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00157956
Iteration 2/1000 | Loss: 0.00072670
Iteration 3/1000 | Loss: 0.00050750
Iteration 4/1000 | Loss: 0.00054117
Iteration 5/1000 | Loss: 0.00041792
Iteration 6/1000 | Loss: 0.00016602
Iteration 7/1000 | Loss: 0.00034389
Iteration 8/1000 | Loss: 0.00013723
Iteration 9/1000 | Loss: 0.00015301
Iteration 10/1000 | Loss: 0.00011874
Iteration 11/1000 | Loss: 0.00014029
Iteration 12/1000 | Loss: 0.00007298
Iteration 13/1000 | Loss: 0.00006577
Iteration 14/1000 | Loss: 0.00004898
Iteration 15/1000 | Loss: 0.00004627
Iteration 16/1000 | Loss: 0.00047158
Iteration 17/1000 | Loss: 0.00022766
Iteration 18/1000 | Loss: 0.00035343
Iteration 19/1000 | Loss: 0.00066894
Iteration 20/1000 | Loss: 0.00033395
Iteration 21/1000 | Loss: 0.00030967
Iteration 22/1000 | Loss: 0.00025751
Iteration 23/1000 | Loss: 0.00022230
Iteration 24/1000 | Loss: 0.00080322
Iteration 25/1000 | Loss: 0.00052572
Iteration 26/1000 | Loss: 0.00033156
Iteration 27/1000 | Loss: 0.00027072
Iteration 28/1000 | Loss: 0.00010047
Iteration 29/1000 | Loss: 0.00006159
Iteration 30/1000 | Loss: 0.00010404
Iteration 31/1000 | Loss: 0.00005841
Iteration 32/1000 | Loss: 0.00007878
Iteration 33/1000 | Loss: 0.00021321
Iteration 34/1000 | Loss: 0.00028700
Iteration 35/1000 | Loss: 0.00019775
Iteration 36/1000 | Loss: 0.00008721
Iteration 37/1000 | Loss: 0.00010163
Iteration 38/1000 | Loss: 0.00008735
Iteration 39/1000 | Loss: 0.00005049
Iteration 40/1000 | Loss: 0.00015263
Iteration 41/1000 | Loss: 0.00009443
Iteration 42/1000 | Loss: 0.00010790
Iteration 43/1000 | Loss: 0.00009215
Iteration 44/1000 | Loss: 0.00014134
Iteration 45/1000 | Loss: 0.00010032
Iteration 46/1000 | Loss: 0.00005579
Iteration 47/1000 | Loss: 0.00003269
Iteration 48/1000 | Loss: 0.00017344
Iteration 49/1000 | Loss: 0.00024115
Iteration 50/1000 | Loss: 0.00007984
Iteration 51/1000 | Loss: 0.00003101
Iteration 52/1000 | Loss: 0.00003273
Iteration 53/1000 | Loss: 0.00021016
Iteration 54/1000 | Loss: 0.00016114
Iteration 55/1000 | Loss: 0.00002940
Iteration 56/1000 | Loss: 0.00003121
Iteration 57/1000 | Loss: 0.00002802
Iteration 58/1000 | Loss: 0.00002678
Iteration 59/1000 | Loss: 0.00002624
Iteration 60/1000 | Loss: 0.00015115
Iteration 61/1000 | Loss: 0.00004695
Iteration 62/1000 | Loss: 0.00003975
Iteration 63/1000 | Loss: 0.00002547
Iteration 64/1000 | Loss: 0.00003531
Iteration 65/1000 | Loss: 0.00005695
Iteration 66/1000 | Loss: 0.00002587
Iteration 67/1000 | Loss: 0.00003299
Iteration 68/1000 | Loss: 0.00002494
Iteration 69/1000 | Loss: 0.00002460
Iteration 70/1000 | Loss: 0.00002444
Iteration 71/1000 | Loss: 0.00002444
Iteration 72/1000 | Loss: 0.00002443
Iteration 73/1000 | Loss: 0.00002442
Iteration 74/1000 | Loss: 0.00002442
Iteration 75/1000 | Loss: 0.00003547
Iteration 76/1000 | Loss: 0.00002427
Iteration 77/1000 | Loss: 0.00002424
Iteration 78/1000 | Loss: 0.00002423
Iteration 79/1000 | Loss: 0.00002418
Iteration 80/1000 | Loss: 0.00002417
Iteration 81/1000 | Loss: 0.00002409
Iteration 82/1000 | Loss: 0.00002409
Iteration 83/1000 | Loss: 0.00002408
Iteration 84/1000 | Loss: 0.00002408
Iteration 85/1000 | Loss: 0.00002408
Iteration 86/1000 | Loss: 0.00002407
Iteration 87/1000 | Loss: 0.00002407
Iteration 88/1000 | Loss: 0.00002407
Iteration 89/1000 | Loss: 0.00002406
Iteration 90/1000 | Loss: 0.00002406
Iteration 91/1000 | Loss: 0.00002406
Iteration 92/1000 | Loss: 0.00002406
Iteration 93/1000 | Loss: 0.00002406
Iteration 94/1000 | Loss: 0.00002405
Iteration 95/1000 | Loss: 0.00002401
Iteration 96/1000 | Loss: 0.00002401
Iteration 97/1000 | Loss: 0.00002400
Iteration 98/1000 | Loss: 0.00002399
Iteration 99/1000 | Loss: 0.00002399
Iteration 100/1000 | Loss: 0.00002398
Iteration 101/1000 | Loss: 0.00002398
Iteration 102/1000 | Loss: 0.00002398
Iteration 103/1000 | Loss: 0.00002398
Iteration 104/1000 | Loss: 0.00002398
Iteration 105/1000 | Loss: 0.00002398
Iteration 106/1000 | Loss: 0.00002398
Iteration 107/1000 | Loss: 0.00002398
Iteration 108/1000 | Loss: 0.00002397
Iteration 109/1000 | Loss: 0.00002397
Iteration 110/1000 | Loss: 0.00002397
Iteration 111/1000 | Loss: 0.00002397
Iteration 112/1000 | Loss: 0.00002397
Iteration 113/1000 | Loss: 0.00002397
Iteration 114/1000 | Loss: 0.00002397
Iteration 115/1000 | Loss: 0.00002397
Iteration 116/1000 | Loss: 0.00002397
Iteration 117/1000 | Loss: 0.00002397
Iteration 118/1000 | Loss: 0.00002396
Iteration 119/1000 | Loss: 0.00002396
Iteration 120/1000 | Loss: 0.00002396
Iteration 121/1000 | Loss: 0.00002396
Iteration 122/1000 | Loss: 0.00002395
Iteration 123/1000 | Loss: 0.00002395
Iteration 124/1000 | Loss: 0.00002395
Iteration 125/1000 | Loss: 0.00002394
Iteration 126/1000 | Loss: 0.00002394
Iteration 127/1000 | Loss: 0.00002394
Iteration 128/1000 | Loss: 0.00002394
Iteration 129/1000 | Loss: 0.00002393
Iteration 130/1000 | Loss: 0.00002393
Iteration 131/1000 | Loss: 0.00002392
Iteration 132/1000 | Loss: 0.00002390
Iteration 133/1000 | Loss: 0.00002389
Iteration 134/1000 | Loss: 0.00002389
Iteration 135/1000 | Loss: 0.00002389
Iteration 136/1000 | Loss: 0.00002388
Iteration 137/1000 | Loss: 0.00002388
Iteration 138/1000 | Loss: 0.00002388
Iteration 139/1000 | Loss: 0.00002387
Iteration 140/1000 | Loss: 0.00002387
Iteration 141/1000 | Loss: 0.00002387
Iteration 142/1000 | Loss: 0.00002387
Iteration 143/1000 | Loss: 0.00002386
Iteration 144/1000 | Loss: 0.00002386
Iteration 145/1000 | Loss: 0.00002385
Iteration 146/1000 | Loss: 0.00002385
Iteration 147/1000 | Loss: 0.00002385
Iteration 148/1000 | Loss: 0.00002385
Iteration 149/1000 | Loss: 0.00002385
Iteration 150/1000 | Loss: 0.00002385
Iteration 151/1000 | Loss: 0.00002385
Iteration 152/1000 | Loss: 0.00002385
Iteration 153/1000 | Loss: 0.00002385
Iteration 154/1000 | Loss: 0.00002385
Iteration 155/1000 | Loss: 0.00002385
Iteration 156/1000 | Loss: 0.00002385
Iteration 157/1000 | Loss: 0.00002385
Iteration 158/1000 | Loss: 0.00002385
Iteration 159/1000 | Loss: 0.00002385
Iteration 160/1000 | Loss: 0.00002385
Iteration 161/1000 | Loss: 0.00002385
Iteration 162/1000 | Loss: 0.00002385
Iteration 163/1000 | Loss: 0.00002385
Iteration 164/1000 | Loss: 0.00002385
Iteration 165/1000 | Loss: 0.00002385
Iteration 166/1000 | Loss: 0.00002385
Iteration 167/1000 | Loss: 0.00002385
Iteration 168/1000 | Loss: 0.00002385
Iteration 169/1000 | Loss: 0.00002385
Iteration 170/1000 | Loss: 0.00002385
Iteration 171/1000 | Loss: 0.00002385
Iteration 172/1000 | Loss: 0.00002385
Iteration 173/1000 | Loss: 0.00002385
Iteration 174/1000 | Loss: 0.00002385
Iteration 175/1000 | Loss: 0.00002385
Iteration 176/1000 | Loss: 0.00002385
Iteration 177/1000 | Loss: 0.00002385
Iteration 178/1000 | Loss: 0.00002385
Iteration 179/1000 | Loss: 0.00002385
Iteration 180/1000 | Loss: 0.00002385
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [2.3846088879508898e-05, 2.3846088879508898e-05, 2.3846088879508898e-05, 2.3846088879508898e-05, 2.3846088879508898e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3846088879508898e-05

Optimization complete. Final v2v error: 3.9700045585632324 mm

Highest mean error: 4.96946907043457 mm for frame 211

Lowest mean error: 3.5737197399139404 mm for frame 234

Saving results

Total time: 174.00733184814453
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00747185
Iteration 2/25 | Loss: 0.00192256
Iteration 3/25 | Loss: 0.00136737
Iteration 4/25 | Loss: 0.00130865
Iteration 5/25 | Loss: 0.00131131
Iteration 6/25 | Loss: 0.00130429
Iteration 7/25 | Loss: 0.00128871
Iteration 8/25 | Loss: 0.00128919
Iteration 9/25 | Loss: 0.00127687
Iteration 10/25 | Loss: 0.00127452
Iteration 11/25 | Loss: 0.00127524
Iteration 12/25 | Loss: 0.00127376
Iteration 13/25 | Loss: 0.00127049
Iteration 14/25 | Loss: 0.00127050
Iteration 15/25 | Loss: 0.00126927
Iteration 16/25 | Loss: 0.00126823
Iteration 17/25 | Loss: 0.00126797
Iteration 18/25 | Loss: 0.00127377
Iteration 19/25 | Loss: 0.00127175
Iteration 20/25 | Loss: 0.00126947
Iteration 21/25 | Loss: 0.00126621
Iteration 22/25 | Loss: 0.00126612
Iteration 23/25 | Loss: 0.00126572
Iteration 24/25 | Loss: 0.00126612
Iteration 25/25 | Loss: 0.00126585

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.58289456
Iteration 2/25 | Loss: 0.00104272
Iteration 3/25 | Loss: 0.00104268
Iteration 4/25 | Loss: 0.00104268
Iteration 5/25 | Loss: 0.00104268
Iteration 6/25 | Loss: 0.00104268
Iteration 7/25 | Loss: 0.00104268
Iteration 8/25 | Loss: 0.00104268
Iteration 9/25 | Loss: 0.00104268
Iteration 10/25 | Loss: 0.00104268
Iteration 11/25 | Loss: 0.00104268
Iteration 12/25 | Loss: 0.00104268
Iteration 13/25 | Loss: 0.00104268
Iteration 14/25 | Loss: 0.00104268
Iteration 15/25 | Loss: 0.00104268
Iteration 16/25 | Loss: 0.00104268
Iteration 17/25 | Loss: 0.00104268
Iteration 18/25 | Loss: 0.00104268
Iteration 19/25 | Loss: 0.00104268
Iteration 20/25 | Loss: 0.00104268
Iteration 21/25 | Loss: 0.00104268
Iteration 22/25 | Loss: 0.00104268
Iteration 23/25 | Loss: 0.00104268
Iteration 24/25 | Loss: 0.00104268
Iteration 25/25 | Loss: 0.00104268

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104268
Iteration 2/1000 | Loss: 0.00004360
Iteration 3/1000 | Loss: 0.00003494
Iteration 4/1000 | Loss: 0.00002980
Iteration 5/1000 | Loss: 0.00002290
Iteration 6/1000 | Loss: 0.00003330
Iteration 7/1000 | Loss: 0.00002892
Iteration 8/1000 | Loss: 0.00002697
Iteration 9/1000 | Loss: 0.00002717
Iteration 10/1000 | Loss: 0.00002450
Iteration 11/1000 | Loss: 0.00003286
Iteration 12/1000 | Loss: 0.00002839
Iteration 13/1000 | Loss: 0.00003836
Iteration 14/1000 | Loss: 0.00003250
Iteration 15/1000 | Loss: 0.00005773
Iteration 16/1000 | Loss: 0.00002513
Iteration 17/1000 | Loss: 0.00001828
Iteration 18/1000 | Loss: 0.00001679
Iteration 19/1000 | Loss: 0.00001642
Iteration 20/1000 | Loss: 0.00001638
Iteration 21/1000 | Loss: 0.00004924
Iteration 22/1000 | Loss: 0.00003700
Iteration 23/1000 | Loss: 0.00001611
Iteration 24/1000 | Loss: 0.00004543
Iteration 25/1000 | Loss: 0.00001595
Iteration 26/1000 | Loss: 0.00001588
Iteration 27/1000 | Loss: 0.00001580
Iteration 28/1000 | Loss: 0.00001578
Iteration 29/1000 | Loss: 0.00001577
Iteration 30/1000 | Loss: 0.00001572
Iteration 31/1000 | Loss: 0.00001560
Iteration 32/1000 | Loss: 0.00001559
Iteration 33/1000 | Loss: 0.00001559
Iteration 34/1000 | Loss: 0.00001555
Iteration 35/1000 | Loss: 0.00001555
Iteration 36/1000 | Loss: 0.00001549
Iteration 37/1000 | Loss: 0.00001546
Iteration 38/1000 | Loss: 0.00001544
Iteration 39/1000 | Loss: 0.00001544
Iteration 40/1000 | Loss: 0.00001543
Iteration 41/1000 | Loss: 0.00001543
Iteration 42/1000 | Loss: 0.00001542
Iteration 43/1000 | Loss: 0.00001541
Iteration 44/1000 | Loss: 0.00001541
Iteration 45/1000 | Loss: 0.00001540
Iteration 46/1000 | Loss: 0.00001540
Iteration 47/1000 | Loss: 0.00001539
Iteration 48/1000 | Loss: 0.00001539
Iteration 49/1000 | Loss: 0.00001538
Iteration 50/1000 | Loss: 0.00001538
Iteration 51/1000 | Loss: 0.00001537
Iteration 52/1000 | Loss: 0.00001537
Iteration 53/1000 | Loss: 0.00001537
Iteration 54/1000 | Loss: 0.00001536
Iteration 55/1000 | Loss: 0.00001535
Iteration 56/1000 | Loss: 0.00001535
Iteration 57/1000 | Loss: 0.00001535
Iteration 58/1000 | Loss: 0.00001535
Iteration 59/1000 | Loss: 0.00001535
Iteration 60/1000 | Loss: 0.00001534
Iteration 61/1000 | Loss: 0.00001534
Iteration 62/1000 | Loss: 0.00001533
Iteration 63/1000 | Loss: 0.00001532
Iteration 64/1000 | Loss: 0.00001532
Iteration 65/1000 | Loss: 0.00001532
Iteration 66/1000 | Loss: 0.00001532
Iteration 67/1000 | Loss: 0.00001532
Iteration 68/1000 | Loss: 0.00001532
Iteration 69/1000 | Loss: 0.00001532
Iteration 70/1000 | Loss: 0.00001532
Iteration 71/1000 | Loss: 0.00001532
Iteration 72/1000 | Loss: 0.00001531
Iteration 73/1000 | Loss: 0.00001531
Iteration 74/1000 | Loss: 0.00001530
Iteration 75/1000 | Loss: 0.00001530
Iteration 76/1000 | Loss: 0.00001530
Iteration 77/1000 | Loss: 0.00001530
Iteration 78/1000 | Loss: 0.00001530
Iteration 79/1000 | Loss: 0.00001530
Iteration 80/1000 | Loss: 0.00001530
Iteration 81/1000 | Loss: 0.00001530
Iteration 82/1000 | Loss: 0.00001530
Iteration 83/1000 | Loss: 0.00001529
Iteration 84/1000 | Loss: 0.00001529
Iteration 85/1000 | Loss: 0.00001529
Iteration 86/1000 | Loss: 0.00001529
Iteration 87/1000 | Loss: 0.00001529
Iteration 88/1000 | Loss: 0.00001528
Iteration 89/1000 | Loss: 0.00001528
Iteration 90/1000 | Loss: 0.00001528
Iteration 91/1000 | Loss: 0.00001528
Iteration 92/1000 | Loss: 0.00001527
Iteration 93/1000 | Loss: 0.00001527
Iteration 94/1000 | Loss: 0.00001527
Iteration 95/1000 | Loss: 0.00001527
Iteration 96/1000 | Loss: 0.00001527
Iteration 97/1000 | Loss: 0.00001527
Iteration 98/1000 | Loss: 0.00001527
Iteration 99/1000 | Loss: 0.00001526
Iteration 100/1000 | Loss: 0.00001526
Iteration 101/1000 | Loss: 0.00001526
Iteration 102/1000 | Loss: 0.00001526
Iteration 103/1000 | Loss: 0.00001526
Iteration 104/1000 | Loss: 0.00001525
Iteration 105/1000 | Loss: 0.00001525
Iteration 106/1000 | Loss: 0.00001525
Iteration 107/1000 | Loss: 0.00001525
Iteration 108/1000 | Loss: 0.00001524
Iteration 109/1000 | Loss: 0.00001524
Iteration 110/1000 | Loss: 0.00001524
Iteration 111/1000 | Loss: 0.00001524
Iteration 112/1000 | Loss: 0.00001524
Iteration 113/1000 | Loss: 0.00001524
Iteration 114/1000 | Loss: 0.00001523
Iteration 115/1000 | Loss: 0.00001523
Iteration 116/1000 | Loss: 0.00001522
Iteration 117/1000 | Loss: 0.00001522
Iteration 118/1000 | Loss: 0.00001522
Iteration 119/1000 | Loss: 0.00001522
Iteration 120/1000 | Loss: 0.00001521
Iteration 121/1000 | Loss: 0.00001521
Iteration 122/1000 | Loss: 0.00001521
Iteration 123/1000 | Loss: 0.00001521
Iteration 124/1000 | Loss: 0.00001521
Iteration 125/1000 | Loss: 0.00001521
Iteration 126/1000 | Loss: 0.00001521
Iteration 127/1000 | Loss: 0.00001521
Iteration 128/1000 | Loss: 0.00001521
Iteration 129/1000 | Loss: 0.00001520
Iteration 130/1000 | Loss: 0.00001520
Iteration 131/1000 | Loss: 0.00001520
Iteration 132/1000 | Loss: 0.00001520
Iteration 133/1000 | Loss: 0.00001520
Iteration 134/1000 | Loss: 0.00001519
Iteration 135/1000 | Loss: 0.00001519
Iteration 136/1000 | Loss: 0.00001519
Iteration 137/1000 | Loss: 0.00001519
Iteration 138/1000 | Loss: 0.00001519
Iteration 139/1000 | Loss: 0.00001519
Iteration 140/1000 | Loss: 0.00001518
Iteration 141/1000 | Loss: 0.00001518
Iteration 142/1000 | Loss: 0.00001518
Iteration 143/1000 | Loss: 0.00001518
Iteration 144/1000 | Loss: 0.00001518
Iteration 145/1000 | Loss: 0.00001518
Iteration 146/1000 | Loss: 0.00001518
Iteration 147/1000 | Loss: 0.00001518
Iteration 148/1000 | Loss: 0.00001518
Iteration 149/1000 | Loss: 0.00001518
Iteration 150/1000 | Loss: 0.00001518
Iteration 151/1000 | Loss: 0.00001517
Iteration 152/1000 | Loss: 0.00001517
Iteration 153/1000 | Loss: 0.00001517
Iteration 154/1000 | Loss: 0.00001517
Iteration 155/1000 | Loss: 0.00001517
Iteration 156/1000 | Loss: 0.00001517
Iteration 157/1000 | Loss: 0.00001517
Iteration 158/1000 | Loss: 0.00001517
Iteration 159/1000 | Loss: 0.00001517
Iteration 160/1000 | Loss: 0.00001517
Iteration 161/1000 | Loss: 0.00001516
Iteration 162/1000 | Loss: 0.00001516
Iteration 163/1000 | Loss: 0.00001516
Iteration 164/1000 | Loss: 0.00001516
Iteration 165/1000 | Loss: 0.00001516
Iteration 166/1000 | Loss: 0.00001516
Iteration 167/1000 | Loss: 0.00001516
Iteration 168/1000 | Loss: 0.00001516
Iteration 169/1000 | Loss: 0.00001516
Iteration 170/1000 | Loss: 0.00001516
Iteration 171/1000 | Loss: 0.00001515
Iteration 172/1000 | Loss: 0.00001515
Iteration 173/1000 | Loss: 0.00001515
Iteration 174/1000 | Loss: 0.00001514
Iteration 175/1000 | Loss: 0.00001514
Iteration 176/1000 | Loss: 0.00005512
Iteration 177/1000 | Loss: 0.00001788
Iteration 178/1000 | Loss: 0.00002745
Iteration 179/1000 | Loss: 0.00001515
Iteration 180/1000 | Loss: 0.00003281
Iteration 181/1000 | Loss: 0.00001516
Iteration 182/1000 | Loss: 0.00001512
Iteration 183/1000 | Loss: 0.00001511
Iteration 184/1000 | Loss: 0.00001511
Iteration 185/1000 | Loss: 0.00001511
Iteration 186/1000 | Loss: 0.00001511
Iteration 187/1000 | Loss: 0.00001510
Iteration 188/1000 | Loss: 0.00001510
Iteration 189/1000 | Loss: 0.00001509
Iteration 190/1000 | Loss: 0.00001509
Iteration 191/1000 | Loss: 0.00001509
Iteration 192/1000 | Loss: 0.00001508
Iteration 193/1000 | Loss: 0.00001508
Iteration 194/1000 | Loss: 0.00001508
Iteration 195/1000 | Loss: 0.00001508
Iteration 196/1000 | Loss: 0.00001508
Iteration 197/1000 | Loss: 0.00001507
Iteration 198/1000 | Loss: 0.00001507
Iteration 199/1000 | Loss: 0.00001507
Iteration 200/1000 | Loss: 0.00001507
Iteration 201/1000 | Loss: 0.00001507
Iteration 202/1000 | Loss: 0.00001507
Iteration 203/1000 | Loss: 0.00001507
Iteration 204/1000 | Loss: 0.00001507
Iteration 205/1000 | Loss: 0.00001507
Iteration 206/1000 | Loss: 0.00001507
Iteration 207/1000 | Loss: 0.00001506
Iteration 208/1000 | Loss: 0.00001506
Iteration 209/1000 | Loss: 0.00001506
Iteration 210/1000 | Loss: 0.00001506
Iteration 211/1000 | Loss: 0.00001506
Iteration 212/1000 | Loss: 0.00001506
Iteration 213/1000 | Loss: 0.00001506
Iteration 214/1000 | Loss: 0.00001506
Iteration 215/1000 | Loss: 0.00001506
Iteration 216/1000 | Loss: 0.00001506
Iteration 217/1000 | Loss: 0.00001506
Iteration 218/1000 | Loss: 0.00001506
Iteration 219/1000 | Loss: 0.00001506
Iteration 220/1000 | Loss: 0.00001505
Iteration 221/1000 | Loss: 0.00001505
Iteration 222/1000 | Loss: 0.00001505
Iteration 223/1000 | Loss: 0.00001505
Iteration 224/1000 | Loss: 0.00001505
Iteration 225/1000 | Loss: 0.00001505
Iteration 226/1000 | Loss: 0.00001505
Iteration 227/1000 | Loss: 0.00001505
Iteration 228/1000 | Loss: 0.00001505
Iteration 229/1000 | Loss: 0.00001505
Iteration 230/1000 | Loss: 0.00001505
Iteration 231/1000 | Loss: 0.00001505
Iteration 232/1000 | Loss: 0.00001505
Iteration 233/1000 | Loss: 0.00001505
Iteration 234/1000 | Loss: 0.00001505
Iteration 235/1000 | Loss: 0.00001504
Iteration 236/1000 | Loss: 0.00001504
Iteration 237/1000 | Loss: 0.00001504
Iteration 238/1000 | Loss: 0.00001504
Iteration 239/1000 | Loss: 0.00001504
Iteration 240/1000 | Loss: 0.00001504
Iteration 241/1000 | Loss: 0.00001504
Iteration 242/1000 | Loss: 0.00001504
Iteration 243/1000 | Loss: 0.00001504
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 243. Stopping optimization.
Last 5 losses: [1.5044853171275463e-05, 1.5044853171275463e-05, 1.5044853171275463e-05, 1.5044853171275463e-05, 1.5044853171275463e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5044853171275463e-05

Optimization complete. Final v2v error: 3.2624197006225586 mm

Highest mean error: 3.7051584720611572 mm for frame 49

Lowest mean error: 2.8312535285949707 mm for frame 207

Saving results

Total time: 118.15440726280212
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00416997
Iteration 2/25 | Loss: 0.00130504
Iteration 3/25 | Loss: 0.00122558
Iteration 4/25 | Loss: 0.00121111
Iteration 5/25 | Loss: 0.00120654
Iteration 6/25 | Loss: 0.00120607
Iteration 7/25 | Loss: 0.00120607
Iteration 8/25 | Loss: 0.00120607
Iteration 9/25 | Loss: 0.00120607
Iteration 10/25 | Loss: 0.00120607
Iteration 11/25 | Loss: 0.00120607
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012060735607519746, 0.0012060735607519746, 0.0012060735607519746, 0.0012060735607519746, 0.0012060735607519746]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012060735607519746

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.48546028
Iteration 2/25 | Loss: 0.00094405
Iteration 3/25 | Loss: 0.00094404
Iteration 4/25 | Loss: 0.00094404
Iteration 5/25 | Loss: 0.00094404
Iteration 6/25 | Loss: 0.00094404
Iteration 7/25 | Loss: 0.00094404
Iteration 8/25 | Loss: 0.00094404
Iteration 9/25 | Loss: 0.00094404
Iteration 10/25 | Loss: 0.00094404
Iteration 11/25 | Loss: 0.00094404
Iteration 12/25 | Loss: 0.00094404
Iteration 13/25 | Loss: 0.00094404
Iteration 14/25 | Loss: 0.00094404
Iteration 15/25 | Loss: 0.00094404
Iteration 16/25 | Loss: 0.00094404
Iteration 17/25 | Loss: 0.00094404
Iteration 18/25 | Loss: 0.00094404
Iteration 19/25 | Loss: 0.00094404
Iteration 20/25 | Loss: 0.00094404
Iteration 21/25 | Loss: 0.00094404
Iteration 22/25 | Loss: 0.00094404
Iteration 23/25 | Loss: 0.00094404
Iteration 24/25 | Loss: 0.00094404
Iteration 25/25 | Loss: 0.00094404

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094404
Iteration 2/1000 | Loss: 0.00002902
Iteration 3/1000 | Loss: 0.00001861
Iteration 4/1000 | Loss: 0.00001629
Iteration 5/1000 | Loss: 0.00001568
Iteration 6/1000 | Loss: 0.00001514
Iteration 7/1000 | Loss: 0.00001450
Iteration 8/1000 | Loss: 0.00001412
Iteration 9/1000 | Loss: 0.00001375
Iteration 10/1000 | Loss: 0.00001343
Iteration 11/1000 | Loss: 0.00001329
Iteration 12/1000 | Loss: 0.00001310
Iteration 13/1000 | Loss: 0.00001300
Iteration 14/1000 | Loss: 0.00001297
Iteration 15/1000 | Loss: 0.00001296
Iteration 16/1000 | Loss: 0.00001295
Iteration 17/1000 | Loss: 0.00001280
Iteration 18/1000 | Loss: 0.00001268
Iteration 19/1000 | Loss: 0.00001265
Iteration 20/1000 | Loss: 0.00001265
Iteration 21/1000 | Loss: 0.00001264
Iteration 22/1000 | Loss: 0.00001263
Iteration 23/1000 | Loss: 0.00001262
Iteration 24/1000 | Loss: 0.00001261
Iteration 25/1000 | Loss: 0.00001260
Iteration 26/1000 | Loss: 0.00001259
Iteration 27/1000 | Loss: 0.00001259
Iteration 28/1000 | Loss: 0.00001258
Iteration 29/1000 | Loss: 0.00001257
Iteration 30/1000 | Loss: 0.00001257
Iteration 31/1000 | Loss: 0.00001256
Iteration 32/1000 | Loss: 0.00001256
Iteration 33/1000 | Loss: 0.00001256
Iteration 34/1000 | Loss: 0.00001256
Iteration 35/1000 | Loss: 0.00001255
Iteration 36/1000 | Loss: 0.00001255
Iteration 37/1000 | Loss: 0.00001254
Iteration 38/1000 | Loss: 0.00001253
Iteration 39/1000 | Loss: 0.00001253
Iteration 40/1000 | Loss: 0.00001253
Iteration 41/1000 | Loss: 0.00001252
Iteration 42/1000 | Loss: 0.00001252
Iteration 43/1000 | Loss: 0.00001251
Iteration 44/1000 | Loss: 0.00001251
Iteration 45/1000 | Loss: 0.00001250
Iteration 46/1000 | Loss: 0.00001250
Iteration 47/1000 | Loss: 0.00001250
Iteration 48/1000 | Loss: 0.00001249
Iteration 49/1000 | Loss: 0.00001248
Iteration 50/1000 | Loss: 0.00001247
Iteration 51/1000 | Loss: 0.00001247
Iteration 52/1000 | Loss: 0.00001246
Iteration 53/1000 | Loss: 0.00001246
Iteration 54/1000 | Loss: 0.00001246
Iteration 55/1000 | Loss: 0.00001245
Iteration 56/1000 | Loss: 0.00001245
Iteration 57/1000 | Loss: 0.00001244
Iteration 58/1000 | Loss: 0.00001244
Iteration 59/1000 | Loss: 0.00001243
Iteration 60/1000 | Loss: 0.00001243
Iteration 61/1000 | Loss: 0.00001243
Iteration 62/1000 | Loss: 0.00001243
Iteration 63/1000 | Loss: 0.00001242
Iteration 64/1000 | Loss: 0.00001242
Iteration 65/1000 | Loss: 0.00001242
Iteration 66/1000 | Loss: 0.00001242
Iteration 67/1000 | Loss: 0.00001242
Iteration 68/1000 | Loss: 0.00001242
Iteration 69/1000 | Loss: 0.00001242
Iteration 70/1000 | Loss: 0.00001241
Iteration 71/1000 | Loss: 0.00001241
Iteration 72/1000 | Loss: 0.00001240
Iteration 73/1000 | Loss: 0.00001239
Iteration 74/1000 | Loss: 0.00001238
Iteration 75/1000 | Loss: 0.00001238
Iteration 76/1000 | Loss: 0.00001238
Iteration 77/1000 | Loss: 0.00001238
Iteration 78/1000 | Loss: 0.00001237
Iteration 79/1000 | Loss: 0.00001237
Iteration 80/1000 | Loss: 0.00001237
Iteration 81/1000 | Loss: 0.00001235
Iteration 82/1000 | Loss: 0.00001235
Iteration 83/1000 | Loss: 0.00001235
Iteration 84/1000 | Loss: 0.00001235
Iteration 85/1000 | Loss: 0.00001235
Iteration 86/1000 | Loss: 0.00001235
Iteration 87/1000 | Loss: 0.00001235
Iteration 88/1000 | Loss: 0.00001235
Iteration 89/1000 | Loss: 0.00001235
Iteration 90/1000 | Loss: 0.00001235
Iteration 91/1000 | Loss: 0.00001234
Iteration 92/1000 | Loss: 0.00001234
Iteration 93/1000 | Loss: 0.00001233
Iteration 94/1000 | Loss: 0.00001232
Iteration 95/1000 | Loss: 0.00001232
Iteration 96/1000 | Loss: 0.00001232
Iteration 97/1000 | Loss: 0.00001232
Iteration 98/1000 | Loss: 0.00001232
Iteration 99/1000 | Loss: 0.00001232
Iteration 100/1000 | Loss: 0.00001232
Iteration 101/1000 | Loss: 0.00001232
Iteration 102/1000 | Loss: 0.00001231
Iteration 103/1000 | Loss: 0.00001231
Iteration 104/1000 | Loss: 0.00001231
Iteration 105/1000 | Loss: 0.00001231
Iteration 106/1000 | Loss: 0.00001231
Iteration 107/1000 | Loss: 0.00001231
Iteration 108/1000 | Loss: 0.00001231
Iteration 109/1000 | Loss: 0.00001231
Iteration 110/1000 | Loss: 0.00001231
Iteration 111/1000 | Loss: 0.00001230
Iteration 112/1000 | Loss: 0.00001230
Iteration 113/1000 | Loss: 0.00001230
Iteration 114/1000 | Loss: 0.00001230
Iteration 115/1000 | Loss: 0.00001229
Iteration 116/1000 | Loss: 0.00001229
Iteration 117/1000 | Loss: 0.00001229
Iteration 118/1000 | Loss: 0.00001229
Iteration 119/1000 | Loss: 0.00001229
Iteration 120/1000 | Loss: 0.00001228
Iteration 121/1000 | Loss: 0.00001228
Iteration 122/1000 | Loss: 0.00001228
Iteration 123/1000 | Loss: 0.00001228
Iteration 124/1000 | Loss: 0.00001228
Iteration 125/1000 | Loss: 0.00001228
Iteration 126/1000 | Loss: 0.00001227
Iteration 127/1000 | Loss: 0.00001227
Iteration 128/1000 | Loss: 0.00001227
Iteration 129/1000 | Loss: 0.00001226
Iteration 130/1000 | Loss: 0.00001226
Iteration 131/1000 | Loss: 0.00001226
Iteration 132/1000 | Loss: 0.00001226
Iteration 133/1000 | Loss: 0.00001225
Iteration 134/1000 | Loss: 0.00001225
Iteration 135/1000 | Loss: 0.00001225
Iteration 136/1000 | Loss: 0.00001225
Iteration 137/1000 | Loss: 0.00001225
Iteration 138/1000 | Loss: 0.00001225
Iteration 139/1000 | Loss: 0.00001224
Iteration 140/1000 | Loss: 0.00001224
Iteration 141/1000 | Loss: 0.00001224
Iteration 142/1000 | Loss: 0.00001224
Iteration 143/1000 | Loss: 0.00001224
Iteration 144/1000 | Loss: 0.00001224
Iteration 145/1000 | Loss: 0.00001223
Iteration 146/1000 | Loss: 0.00001223
Iteration 147/1000 | Loss: 0.00001223
Iteration 148/1000 | Loss: 0.00001223
Iteration 149/1000 | Loss: 0.00001222
Iteration 150/1000 | Loss: 0.00001222
Iteration 151/1000 | Loss: 0.00001222
Iteration 152/1000 | Loss: 0.00001222
Iteration 153/1000 | Loss: 0.00001222
Iteration 154/1000 | Loss: 0.00001222
Iteration 155/1000 | Loss: 0.00001222
Iteration 156/1000 | Loss: 0.00001222
Iteration 157/1000 | Loss: 0.00001222
Iteration 158/1000 | Loss: 0.00001222
Iteration 159/1000 | Loss: 0.00001222
Iteration 160/1000 | Loss: 0.00001222
Iteration 161/1000 | Loss: 0.00001221
Iteration 162/1000 | Loss: 0.00001221
Iteration 163/1000 | Loss: 0.00001221
Iteration 164/1000 | Loss: 0.00001221
Iteration 165/1000 | Loss: 0.00001221
Iteration 166/1000 | Loss: 0.00001221
Iteration 167/1000 | Loss: 0.00001221
Iteration 168/1000 | Loss: 0.00001221
Iteration 169/1000 | Loss: 0.00001221
Iteration 170/1000 | Loss: 0.00001221
Iteration 171/1000 | Loss: 0.00001221
Iteration 172/1000 | Loss: 0.00001221
Iteration 173/1000 | Loss: 0.00001220
Iteration 174/1000 | Loss: 0.00001220
Iteration 175/1000 | Loss: 0.00001220
Iteration 176/1000 | Loss: 0.00001220
Iteration 177/1000 | Loss: 0.00001220
Iteration 178/1000 | Loss: 0.00001220
Iteration 179/1000 | Loss: 0.00001220
Iteration 180/1000 | Loss: 0.00001220
Iteration 181/1000 | Loss: 0.00001220
Iteration 182/1000 | Loss: 0.00001220
Iteration 183/1000 | Loss: 0.00001220
Iteration 184/1000 | Loss: 0.00001220
Iteration 185/1000 | Loss: 0.00001220
Iteration 186/1000 | Loss: 0.00001220
Iteration 187/1000 | Loss: 0.00001220
Iteration 188/1000 | Loss: 0.00001220
Iteration 189/1000 | Loss: 0.00001220
Iteration 190/1000 | Loss: 0.00001220
Iteration 191/1000 | Loss: 0.00001220
Iteration 192/1000 | Loss: 0.00001220
Iteration 193/1000 | Loss: 0.00001220
Iteration 194/1000 | Loss: 0.00001220
Iteration 195/1000 | Loss: 0.00001220
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [1.2203362530271988e-05, 1.2203362530271988e-05, 1.2203362530271988e-05, 1.2203362530271988e-05, 1.2203362530271988e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2203362530271988e-05

Optimization complete. Final v2v error: 3.0015347003936768 mm

Highest mean error: 3.393299102783203 mm for frame 90

Lowest mean error: 2.844358444213867 mm for frame 130

Saving results

Total time: 43.336771726608276
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00978190
Iteration 2/25 | Loss: 0.00278222
Iteration 3/25 | Loss: 0.00191539
Iteration 4/25 | Loss: 0.00183733
Iteration 5/25 | Loss: 0.00173051
Iteration 6/25 | Loss: 0.00161470
Iteration 7/25 | Loss: 0.00155651
Iteration 8/25 | Loss: 0.00153074
Iteration 9/25 | Loss: 0.00155334
Iteration 10/25 | Loss: 0.00148344
Iteration 11/25 | Loss: 0.00147835
Iteration 12/25 | Loss: 0.00146703
Iteration 13/25 | Loss: 0.00145855
Iteration 14/25 | Loss: 0.00147660
Iteration 15/25 | Loss: 0.00145788
Iteration 16/25 | Loss: 0.00143910
Iteration 17/25 | Loss: 0.00143351
Iteration 18/25 | Loss: 0.00143206
Iteration 19/25 | Loss: 0.00143054
Iteration 20/25 | Loss: 0.00142910
Iteration 21/25 | Loss: 0.00143171
Iteration 22/25 | Loss: 0.00142840
Iteration 23/25 | Loss: 0.00143022
Iteration 24/25 | Loss: 0.00142815
Iteration 25/25 | Loss: 0.00142999

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37470090
Iteration 2/25 | Loss: 0.00208857
Iteration 3/25 | Loss: 0.00198497
Iteration 4/25 | Loss: 0.00200318
Iteration 5/25 | Loss: 0.00198820
Iteration 6/25 | Loss: 0.00199112
Iteration 7/25 | Loss: 0.00198517
Iteration 8/25 | Loss: 0.00197126
Iteration 9/25 | Loss: 0.00197126
Iteration 10/25 | Loss: 0.00197126
Iteration 11/25 | Loss: 0.00197126
Iteration 12/25 | Loss: 0.00197126
Iteration 13/25 | Loss: 0.00197126
Iteration 14/25 | Loss: 0.00197126
Iteration 15/25 | Loss: 0.00197126
Iteration 16/25 | Loss: 0.00197126
Iteration 17/25 | Loss: 0.00197126
Iteration 18/25 | Loss: 0.00197126
Iteration 19/25 | Loss: 0.00197126
Iteration 20/25 | Loss: 0.00197126
Iteration 21/25 | Loss: 0.00197126
Iteration 22/25 | Loss: 0.00197126
Iteration 23/25 | Loss: 0.00197126
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001971257384866476, 0.001971257384866476, 0.001971257384866476, 0.001971257384866476, 0.001971257384866476]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001971257384866476

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00197126
Iteration 2/1000 | Loss: 0.00030328
Iteration 3/1000 | Loss: 0.00028083
Iteration 4/1000 | Loss: 0.00015248
Iteration 5/1000 | Loss: 0.00012891
Iteration 6/1000 | Loss: 0.00017537
Iteration 7/1000 | Loss: 0.00009202
Iteration 8/1000 | Loss: 0.00078716
Iteration 9/1000 | Loss: 0.00085053
Iteration 10/1000 | Loss: 0.00027604
Iteration 11/1000 | Loss: 0.00026830
Iteration 12/1000 | Loss: 0.00009573
Iteration 13/1000 | Loss: 0.00020581
Iteration 14/1000 | Loss: 0.00058093
Iteration 15/1000 | Loss: 0.00045806
Iteration 16/1000 | Loss: 0.00043643
Iteration 17/1000 | Loss: 0.00008985
Iteration 18/1000 | Loss: 0.00007102
Iteration 19/1000 | Loss: 0.00022794
Iteration 20/1000 | Loss: 0.00020331
Iteration 21/1000 | Loss: 0.00006645
Iteration 22/1000 | Loss: 0.00007457
Iteration 23/1000 | Loss: 0.00016975
Iteration 24/1000 | Loss: 0.00016796
Iteration 25/1000 | Loss: 0.00017130
Iteration 26/1000 | Loss: 0.00016936
Iteration 27/1000 | Loss: 0.00016665
Iteration 28/1000 | Loss: 0.00019752
Iteration 29/1000 | Loss: 0.00021266
Iteration 30/1000 | Loss: 0.00015744
Iteration 31/1000 | Loss: 0.00007737
Iteration 32/1000 | Loss: 0.00006485
Iteration 33/1000 | Loss: 0.00007002
Iteration 34/1000 | Loss: 0.00006365
Iteration 35/1000 | Loss: 0.00006551
Iteration 36/1000 | Loss: 0.00006023
Iteration 37/1000 | Loss: 0.00021075
Iteration 38/1000 | Loss: 0.00039457
Iteration 39/1000 | Loss: 0.00026225
Iteration 40/1000 | Loss: 0.00012213
Iteration 41/1000 | Loss: 0.00018933
Iteration 42/1000 | Loss: 0.00006574
Iteration 43/1000 | Loss: 0.00017569
Iteration 44/1000 | Loss: 0.00005793
Iteration 45/1000 | Loss: 0.00005401
Iteration 46/1000 | Loss: 0.00012553
Iteration 47/1000 | Loss: 0.00009594
Iteration 48/1000 | Loss: 0.00026755
Iteration 49/1000 | Loss: 0.00006684
Iteration 50/1000 | Loss: 0.00005363
Iteration 51/1000 | Loss: 0.00016122
Iteration 52/1000 | Loss: 0.00006990
Iteration 53/1000 | Loss: 0.00004800
Iteration 54/1000 | Loss: 0.00005950
Iteration 55/1000 | Loss: 0.00004624
Iteration 56/1000 | Loss: 0.00009577
Iteration 57/1000 | Loss: 0.00012429
Iteration 58/1000 | Loss: 0.00006732
Iteration 59/1000 | Loss: 0.00004547
Iteration 60/1000 | Loss: 0.00004474
Iteration 61/1000 | Loss: 0.00007578
Iteration 62/1000 | Loss: 0.00004712
Iteration 63/1000 | Loss: 0.00006061
Iteration 64/1000 | Loss: 0.00004382
Iteration 65/1000 | Loss: 0.00004371
Iteration 66/1000 | Loss: 0.00014293
Iteration 67/1000 | Loss: 0.00007767
Iteration 68/1000 | Loss: 0.00004591
Iteration 69/1000 | Loss: 0.00005113
Iteration 70/1000 | Loss: 0.00004367
Iteration 71/1000 | Loss: 0.00005695
Iteration 72/1000 | Loss: 0.00004276
Iteration 73/1000 | Loss: 0.00007888
Iteration 74/1000 | Loss: 0.00009075
Iteration 75/1000 | Loss: 0.00006201
Iteration 76/1000 | Loss: 0.00004199
Iteration 77/1000 | Loss: 0.00004832
Iteration 78/1000 | Loss: 0.00004187
Iteration 79/1000 | Loss: 0.00004180
Iteration 80/1000 | Loss: 0.00004180
Iteration 81/1000 | Loss: 0.00004179
Iteration 82/1000 | Loss: 0.00004179
Iteration 83/1000 | Loss: 0.00004179
Iteration 84/1000 | Loss: 0.00004179
Iteration 85/1000 | Loss: 0.00004179
Iteration 86/1000 | Loss: 0.00004179
Iteration 87/1000 | Loss: 0.00004179
Iteration 88/1000 | Loss: 0.00004179
Iteration 89/1000 | Loss: 0.00004179
Iteration 90/1000 | Loss: 0.00004179
Iteration 91/1000 | Loss: 0.00004179
Iteration 92/1000 | Loss: 0.00004179
Iteration 93/1000 | Loss: 0.00004178
Iteration 94/1000 | Loss: 0.00004178
Iteration 95/1000 | Loss: 0.00004177
Iteration 96/1000 | Loss: 0.00004176
Iteration 97/1000 | Loss: 0.00004176
Iteration 98/1000 | Loss: 0.00004176
Iteration 99/1000 | Loss: 0.00004176
Iteration 100/1000 | Loss: 0.00004176
Iteration 101/1000 | Loss: 0.00004176
Iteration 102/1000 | Loss: 0.00004176
Iteration 103/1000 | Loss: 0.00004176
Iteration 104/1000 | Loss: 0.00004175
Iteration 105/1000 | Loss: 0.00005003
Iteration 106/1000 | Loss: 0.00005022
Iteration 107/1000 | Loss: 0.00004158
Iteration 108/1000 | Loss: 0.00010275
Iteration 109/1000 | Loss: 0.00005864
Iteration 110/1000 | Loss: 0.00004751
Iteration 111/1000 | Loss: 0.00004296
Iteration 112/1000 | Loss: 0.00005746
Iteration 113/1000 | Loss: 0.00010025
Iteration 114/1000 | Loss: 0.00004356
Iteration 115/1000 | Loss: 0.00004813
Iteration 116/1000 | Loss: 0.00006989
Iteration 117/1000 | Loss: 0.00005748
Iteration 118/1000 | Loss: 0.00013873
Iteration 119/1000 | Loss: 0.00004828
Iteration 120/1000 | Loss: 0.00005016
Iteration 121/1000 | Loss: 0.00004195
Iteration 122/1000 | Loss: 0.00012933
Iteration 123/1000 | Loss: 0.00004731
Iteration 124/1000 | Loss: 0.00010478
Iteration 125/1000 | Loss: 0.00012624
Iteration 126/1000 | Loss: 0.00048271
Iteration 127/1000 | Loss: 0.00009766
Iteration 128/1000 | Loss: 0.00004509
Iteration 129/1000 | Loss: 0.00011569
Iteration 130/1000 | Loss: 0.00010914
Iteration 131/1000 | Loss: 0.00016263
Iteration 132/1000 | Loss: 0.00004032
Iteration 133/1000 | Loss: 0.00004582
Iteration 134/1000 | Loss: 0.00011223
Iteration 135/1000 | Loss: 0.00094954
Iteration 136/1000 | Loss: 0.00003735
Iteration 137/1000 | Loss: 0.00011491
Iteration 138/1000 | Loss: 0.00005394
Iteration 139/1000 | Loss: 0.00004084
Iteration 140/1000 | Loss: 0.00004208
Iteration 141/1000 | Loss: 0.00005603
Iteration 142/1000 | Loss: 0.00008798
Iteration 143/1000 | Loss: 0.00006676
Iteration 144/1000 | Loss: 0.00008260
Iteration 145/1000 | Loss: 0.00003421
Iteration 146/1000 | Loss: 0.00003379
Iteration 147/1000 | Loss: 0.00007222
Iteration 148/1000 | Loss: 0.00003868
Iteration 149/1000 | Loss: 0.00003477
Iteration 150/1000 | Loss: 0.00003322
Iteration 151/1000 | Loss: 0.00003318
Iteration 152/1000 | Loss: 0.00003315
Iteration 153/1000 | Loss: 0.00003314
Iteration 154/1000 | Loss: 0.00003314
Iteration 155/1000 | Loss: 0.00003313
Iteration 156/1000 | Loss: 0.00004039
Iteration 157/1000 | Loss: 0.00006457
Iteration 158/1000 | Loss: 0.00004556
Iteration 159/1000 | Loss: 0.00003381
Iteration 160/1000 | Loss: 0.00003562
Iteration 161/1000 | Loss: 0.00003405
Iteration 162/1000 | Loss: 0.00003299
Iteration 163/1000 | Loss: 0.00003299
Iteration 164/1000 | Loss: 0.00003299
Iteration 165/1000 | Loss: 0.00003298
Iteration 166/1000 | Loss: 0.00003298
Iteration 167/1000 | Loss: 0.00003298
Iteration 168/1000 | Loss: 0.00003297
Iteration 169/1000 | Loss: 0.00003594
Iteration 170/1000 | Loss: 0.00003292
Iteration 171/1000 | Loss: 0.00003292
Iteration 172/1000 | Loss: 0.00003292
Iteration 173/1000 | Loss: 0.00003292
Iteration 174/1000 | Loss: 0.00003292
Iteration 175/1000 | Loss: 0.00003292
Iteration 176/1000 | Loss: 0.00003292
Iteration 177/1000 | Loss: 0.00003292
Iteration 178/1000 | Loss: 0.00003291
Iteration 179/1000 | Loss: 0.00003291
Iteration 180/1000 | Loss: 0.00003291
Iteration 181/1000 | Loss: 0.00003291
Iteration 182/1000 | Loss: 0.00003291
Iteration 183/1000 | Loss: 0.00003291
Iteration 184/1000 | Loss: 0.00003291
Iteration 185/1000 | Loss: 0.00003291
Iteration 186/1000 | Loss: 0.00003291
Iteration 187/1000 | Loss: 0.00003291
Iteration 188/1000 | Loss: 0.00003291
Iteration 189/1000 | Loss: 0.00003291
Iteration 190/1000 | Loss: 0.00003291
Iteration 191/1000 | Loss: 0.00003291
Iteration 192/1000 | Loss: 0.00003291
Iteration 193/1000 | Loss: 0.00003291
Iteration 194/1000 | Loss: 0.00003291
Iteration 195/1000 | Loss: 0.00003291
Iteration 196/1000 | Loss: 0.00003291
Iteration 197/1000 | Loss: 0.00003291
Iteration 198/1000 | Loss: 0.00003291
Iteration 199/1000 | Loss: 0.00003291
Iteration 200/1000 | Loss: 0.00003291
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [3.291172833996825e-05, 3.291172833996825e-05, 3.291172833996825e-05, 3.291172833996825e-05, 3.291172833996825e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.291172833996825e-05

Optimization complete. Final v2v error: 3.9287562370300293 mm

Highest mean error: 10.627052307128906 mm for frame 1

Lowest mean error: 2.69897198677063 mm for frame 169

Saving results

Total time: 262.19009709358215
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00825248
Iteration 2/25 | Loss: 0.00143962
Iteration 3/25 | Loss: 0.00135299
Iteration 4/25 | Loss: 0.00133754
Iteration 5/25 | Loss: 0.00133209
Iteration 6/25 | Loss: 0.00133158
Iteration 7/25 | Loss: 0.00133158
Iteration 8/25 | Loss: 0.00133158
Iteration 9/25 | Loss: 0.00133158
Iteration 10/25 | Loss: 0.00133158
Iteration 11/25 | Loss: 0.00133158
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013315753312781453, 0.0013315753312781453, 0.0013315753312781453, 0.0013315753312781453, 0.0013315753312781453]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013315753312781453

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17061210
Iteration 2/25 | Loss: 0.00082308
Iteration 3/25 | Loss: 0.00082307
Iteration 4/25 | Loss: 0.00082307
Iteration 5/25 | Loss: 0.00082307
Iteration 6/25 | Loss: 0.00082307
Iteration 7/25 | Loss: 0.00082307
Iteration 8/25 | Loss: 0.00082307
Iteration 9/25 | Loss: 0.00082307
Iteration 10/25 | Loss: 0.00082307
Iteration 11/25 | Loss: 0.00082307
Iteration 12/25 | Loss: 0.00082307
Iteration 13/25 | Loss: 0.00082307
Iteration 14/25 | Loss: 0.00082307
Iteration 15/25 | Loss: 0.00082307
Iteration 16/25 | Loss: 0.00082307
Iteration 17/25 | Loss: 0.00082307
Iteration 18/25 | Loss: 0.00082307
Iteration 19/25 | Loss: 0.00082307
Iteration 20/25 | Loss: 0.00082307
Iteration 21/25 | Loss: 0.00082307
Iteration 22/25 | Loss: 0.00082307
Iteration 23/25 | Loss: 0.00082307
Iteration 24/25 | Loss: 0.00082307
Iteration 25/25 | Loss: 0.00082307

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082307
Iteration 2/1000 | Loss: 0.00005177
Iteration 3/1000 | Loss: 0.00003572
Iteration 4/1000 | Loss: 0.00003239
Iteration 5/1000 | Loss: 0.00003036
Iteration 6/1000 | Loss: 0.00002918
Iteration 7/1000 | Loss: 0.00002830
Iteration 8/1000 | Loss: 0.00002779
Iteration 9/1000 | Loss: 0.00002727
Iteration 10/1000 | Loss: 0.00002690
Iteration 11/1000 | Loss: 0.00002666
Iteration 12/1000 | Loss: 0.00002639
Iteration 13/1000 | Loss: 0.00002621
Iteration 14/1000 | Loss: 0.00002609
Iteration 15/1000 | Loss: 0.00002609
Iteration 16/1000 | Loss: 0.00002608
Iteration 17/1000 | Loss: 0.00002607
Iteration 18/1000 | Loss: 0.00002602
Iteration 19/1000 | Loss: 0.00002601
Iteration 20/1000 | Loss: 0.00002598
Iteration 21/1000 | Loss: 0.00002594
Iteration 22/1000 | Loss: 0.00002591
Iteration 23/1000 | Loss: 0.00002591
Iteration 24/1000 | Loss: 0.00002591
Iteration 25/1000 | Loss: 0.00002590
Iteration 26/1000 | Loss: 0.00002590
Iteration 27/1000 | Loss: 0.00002586
Iteration 28/1000 | Loss: 0.00002585
Iteration 29/1000 | Loss: 0.00002584
Iteration 30/1000 | Loss: 0.00002584
Iteration 31/1000 | Loss: 0.00002580
Iteration 32/1000 | Loss: 0.00002580
Iteration 33/1000 | Loss: 0.00002580
Iteration 34/1000 | Loss: 0.00002580
Iteration 35/1000 | Loss: 0.00002579
Iteration 36/1000 | Loss: 0.00002578
Iteration 37/1000 | Loss: 0.00002576
Iteration 38/1000 | Loss: 0.00002576
Iteration 39/1000 | Loss: 0.00002576
Iteration 40/1000 | Loss: 0.00002575
Iteration 41/1000 | Loss: 0.00002575
Iteration 42/1000 | Loss: 0.00002575
Iteration 43/1000 | Loss: 0.00002572
Iteration 44/1000 | Loss: 0.00002572
Iteration 45/1000 | Loss: 0.00002572
Iteration 46/1000 | Loss: 0.00002572
Iteration 47/1000 | Loss: 0.00002571
Iteration 48/1000 | Loss: 0.00002571
Iteration 49/1000 | Loss: 0.00002570
Iteration 50/1000 | Loss: 0.00002570
Iteration 51/1000 | Loss: 0.00002570
Iteration 52/1000 | Loss: 0.00002569
Iteration 53/1000 | Loss: 0.00002568
Iteration 54/1000 | Loss: 0.00002568
Iteration 55/1000 | Loss: 0.00002568
Iteration 56/1000 | Loss: 0.00002568
Iteration 57/1000 | Loss: 0.00002567
Iteration 58/1000 | Loss: 0.00002567
Iteration 59/1000 | Loss: 0.00002567
Iteration 60/1000 | Loss: 0.00002567
Iteration 61/1000 | Loss: 0.00002566
Iteration 62/1000 | Loss: 0.00002566
Iteration 63/1000 | Loss: 0.00002565
Iteration 64/1000 | Loss: 0.00002565
Iteration 65/1000 | Loss: 0.00002565
Iteration 66/1000 | Loss: 0.00002565
Iteration 67/1000 | Loss: 0.00002564
Iteration 68/1000 | Loss: 0.00002564
Iteration 69/1000 | Loss: 0.00002564
Iteration 70/1000 | Loss: 0.00002564
Iteration 71/1000 | Loss: 0.00002564
Iteration 72/1000 | Loss: 0.00002563
Iteration 73/1000 | Loss: 0.00002563
Iteration 74/1000 | Loss: 0.00002563
Iteration 75/1000 | Loss: 0.00002563
Iteration 76/1000 | Loss: 0.00002563
Iteration 77/1000 | Loss: 0.00002562
Iteration 78/1000 | Loss: 0.00002562
Iteration 79/1000 | Loss: 0.00002562
Iteration 80/1000 | Loss: 0.00002562
Iteration 81/1000 | Loss: 0.00002562
Iteration 82/1000 | Loss: 0.00002561
Iteration 83/1000 | Loss: 0.00002561
Iteration 84/1000 | Loss: 0.00002561
Iteration 85/1000 | Loss: 0.00002561
Iteration 86/1000 | Loss: 0.00002561
Iteration 87/1000 | Loss: 0.00002561
Iteration 88/1000 | Loss: 0.00002561
Iteration 89/1000 | Loss: 0.00002561
Iteration 90/1000 | Loss: 0.00002560
Iteration 91/1000 | Loss: 0.00002560
Iteration 92/1000 | Loss: 0.00002560
Iteration 93/1000 | Loss: 0.00002560
Iteration 94/1000 | Loss: 0.00002560
Iteration 95/1000 | Loss: 0.00002559
Iteration 96/1000 | Loss: 0.00002559
Iteration 97/1000 | Loss: 0.00002559
Iteration 98/1000 | Loss: 0.00002558
Iteration 99/1000 | Loss: 0.00002558
Iteration 100/1000 | Loss: 0.00002558
Iteration 101/1000 | Loss: 0.00002558
Iteration 102/1000 | Loss: 0.00002558
Iteration 103/1000 | Loss: 0.00002558
Iteration 104/1000 | Loss: 0.00002557
Iteration 105/1000 | Loss: 0.00002557
Iteration 106/1000 | Loss: 0.00002557
Iteration 107/1000 | Loss: 0.00002556
Iteration 108/1000 | Loss: 0.00002556
Iteration 109/1000 | Loss: 0.00002556
Iteration 110/1000 | Loss: 0.00002556
Iteration 111/1000 | Loss: 0.00002556
Iteration 112/1000 | Loss: 0.00002556
Iteration 113/1000 | Loss: 0.00002556
Iteration 114/1000 | Loss: 0.00002556
Iteration 115/1000 | Loss: 0.00002556
Iteration 116/1000 | Loss: 0.00002556
Iteration 117/1000 | Loss: 0.00002556
Iteration 118/1000 | Loss: 0.00002556
Iteration 119/1000 | Loss: 0.00002556
Iteration 120/1000 | Loss: 0.00002556
Iteration 121/1000 | Loss: 0.00002556
Iteration 122/1000 | Loss: 0.00002555
Iteration 123/1000 | Loss: 0.00002555
Iteration 124/1000 | Loss: 0.00002555
Iteration 125/1000 | Loss: 0.00002555
Iteration 126/1000 | Loss: 0.00002555
Iteration 127/1000 | Loss: 0.00002555
Iteration 128/1000 | Loss: 0.00002555
Iteration 129/1000 | Loss: 0.00002555
Iteration 130/1000 | Loss: 0.00002555
Iteration 131/1000 | Loss: 0.00002555
Iteration 132/1000 | Loss: 0.00002554
Iteration 133/1000 | Loss: 0.00002554
Iteration 134/1000 | Loss: 0.00002554
Iteration 135/1000 | Loss: 0.00002554
Iteration 136/1000 | Loss: 0.00002554
Iteration 137/1000 | Loss: 0.00002554
Iteration 138/1000 | Loss: 0.00002554
Iteration 139/1000 | Loss: 0.00002554
Iteration 140/1000 | Loss: 0.00002554
Iteration 141/1000 | Loss: 0.00002554
Iteration 142/1000 | Loss: 0.00002554
Iteration 143/1000 | Loss: 0.00002554
Iteration 144/1000 | Loss: 0.00002554
Iteration 145/1000 | Loss: 0.00002554
Iteration 146/1000 | Loss: 0.00002554
Iteration 147/1000 | Loss: 0.00002554
Iteration 148/1000 | Loss: 0.00002554
Iteration 149/1000 | Loss: 0.00002554
Iteration 150/1000 | Loss: 0.00002554
Iteration 151/1000 | Loss: 0.00002554
Iteration 152/1000 | Loss: 0.00002554
Iteration 153/1000 | Loss: 0.00002554
Iteration 154/1000 | Loss: 0.00002554
Iteration 155/1000 | Loss: 0.00002554
Iteration 156/1000 | Loss: 0.00002554
Iteration 157/1000 | Loss: 0.00002554
Iteration 158/1000 | Loss: 0.00002554
Iteration 159/1000 | Loss: 0.00002554
Iteration 160/1000 | Loss: 0.00002554
Iteration 161/1000 | Loss: 0.00002554
Iteration 162/1000 | Loss: 0.00002554
Iteration 163/1000 | Loss: 0.00002554
Iteration 164/1000 | Loss: 0.00002554
Iteration 165/1000 | Loss: 0.00002554
Iteration 166/1000 | Loss: 0.00002554
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [2.5538673071423545e-05, 2.5538673071423545e-05, 2.5538673071423545e-05, 2.5538673071423545e-05, 2.5538673071423545e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5538673071423545e-05

Optimization complete. Final v2v error: 4.14703369140625 mm

Highest mean error: 5.4501423835754395 mm for frame 127

Lowest mean error: 3.202512264251709 mm for frame 51

Saving results

Total time: 45.92236590385437
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00494960
Iteration 2/25 | Loss: 0.00149359
Iteration 3/25 | Loss: 0.00127994
Iteration 4/25 | Loss: 0.00125997
Iteration 5/25 | Loss: 0.00125635
Iteration 6/25 | Loss: 0.00125563
Iteration 7/25 | Loss: 0.00125563
Iteration 8/25 | Loss: 0.00125563
Iteration 9/25 | Loss: 0.00125563
Iteration 10/25 | Loss: 0.00125563
Iteration 11/25 | Loss: 0.00125563
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012556344736367464, 0.0012556344736367464, 0.0012556344736367464, 0.0012556344736367464, 0.0012556344736367464]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012556344736367464

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37237144
Iteration 2/25 | Loss: 0.00092026
Iteration 3/25 | Loss: 0.00092025
Iteration 4/25 | Loss: 0.00092025
Iteration 5/25 | Loss: 0.00092025
Iteration 6/25 | Loss: 0.00092025
Iteration 7/25 | Loss: 0.00092025
Iteration 8/25 | Loss: 0.00092025
Iteration 9/25 | Loss: 0.00092025
Iteration 10/25 | Loss: 0.00092025
Iteration 11/25 | Loss: 0.00092025
Iteration 12/25 | Loss: 0.00092025
Iteration 13/25 | Loss: 0.00092025
Iteration 14/25 | Loss: 0.00092025
Iteration 15/25 | Loss: 0.00092025
Iteration 16/25 | Loss: 0.00092025
Iteration 17/25 | Loss: 0.00092025
Iteration 18/25 | Loss: 0.00092025
Iteration 19/25 | Loss: 0.00092025
Iteration 20/25 | Loss: 0.00092025
Iteration 21/25 | Loss: 0.00092025
Iteration 22/25 | Loss: 0.00092025
Iteration 23/25 | Loss: 0.00092025
Iteration 24/25 | Loss: 0.00092025
Iteration 25/25 | Loss: 0.00092025
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0009202503715641797, 0.0009202503715641797, 0.0009202503715641797, 0.0009202503715641797, 0.0009202503715641797]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009202503715641797

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092025
Iteration 2/1000 | Loss: 0.00003044
Iteration 3/1000 | Loss: 0.00002156
Iteration 4/1000 | Loss: 0.00001937
Iteration 5/1000 | Loss: 0.00001828
Iteration 6/1000 | Loss: 0.00001756
Iteration 7/1000 | Loss: 0.00001706
Iteration 8/1000 | Loss: 0.00001672
Iteration 9/1000 | Loss: 0.00001649
Iteration 10/1000 | Loss: 0.00001623
Iteration 11/1000 | Loss: 0.00001608
Iteration 12/1000 | Loss: 0.00001606
Iteration 13/1000 | Loss: 0.00001601
Iteration 14/1000 | Loss: 0.00001592
Iteration 15/1000 | Loss: 0.00001586
Iteration 16/1000 | Loss: 0.00001586
Iteration 17/1000 | Loss: 0.00001582
Iteration 18/1000 | Loss: 0.00001579
Iteration 19/1000 | Loss: 0.00001579
Iteration 20/1000 | Loss: 0.00001578
Iteration 21/1000 | Loss: 0.00001578
Iteration 22/1000 | Loss: 0.00001578
Iteration 23/1000 | Loss: 0.00001577
Iteration 24/1000 | Loss: 0.00001577
Iteration 25/1000 | Loss: 0.00001576
Iteration 26/1000 | Loss: 0.00001576
Iteration 27/1000 | Loss: 0.00001575
Iteration 28/1000 | Loss: 0.00001575
Iteration 29/1000 | Loss: 0.00001574
Iteration 30/1000 | Loss: 0.00001574
Iteration 31/1000 | Loss: 0.00001574
Iteration 32/1000 | Loss: 0.00001573
Iteration 33/1000 | Loss: 0.00001573
Iteration 34/1000 | Loss: 0.00001573
Iteration 35/1000 | Loss: 0.00001572
Iteration 36/1000 | Loss: 0.00001572
Iteration 37/1000 | Loss: 0.00001572
Iteration 38/1000 | Loss: 0.00001571
Iteration 39/1000 | Loss: 0.00001571
Iteration 40/1000 | Loss: 0.00001570
Iteration 41/1000 | Loss: 0.00001570
Iteration 42/1000 | Loss: 0.00001569
Iteration 43/1000 | Loss: 0.00001569
Iteration 44/1000 | Loss: 0.00001568
Iteration 45/1000 | Loss: 0.00001568
Iteration 46/1000 | Loss: 0.00001568
Iteration 47/1000 | Loss: 0.00001567
Iteration 48/1000 | Loss: 0.00001567
Iteration 49/1000 | Loss: 0.00001567
Iteration 50/1000 | Loss: 0.00001566
Iteration 51/1000 | Loss: 0.00001565
Iteration 52/1000 | Loss: 0.00001565
Iteration 53/1000 | Loss: 0.00001565
Iteration 54/1000 | Loss: 0.00001564
Iteration 55/1000 | Loss: 0.00001563
Iteration 56/1000 | Loss: 0.00001563
Iteration 57/1000 | Loss: 0.00001563
Iteration 58/1000 | Loss: 0.00001563
Iteration 59/1000 | Loss: 0.00001563
Iteration 60/1000 | Loss: 0.00001563
Iteration 61/1000 | Loss: 0.00001563
Iteration 62/1000 | Loss: 0.00001563
Iteration 63/1000 | Loss: 0.00001562
Iteration 64/1000 | Loss: 0.00001562
Iteration 65/1000 | Loss: 0.00001562
Iteration 66/1000 | Loss: 0.00001561
Iteration 67/1000 | Loss: 0.00001561
Iteration 68/1000 | Loss: 0.00001560
Iteration 69/1000 | Loss: 0.00001560
Iteration 70/1000 | Loss: 0.00001560
Iteration 71/1000 | Loss: 0.00001560
Iteration 72/1000 | Loss: 0.00001560
Iteration 73/1000 | Loss: 0.00001560
Iteration 74/1000 | Loss: 0.00001559
Iteration 75/1000 | Loss: 0.00001559
Iteration 76/1000 | Loss: 0.00001559
Iteration 77/1000 | Loss: 0.00001558
Iteration 78/1000 | Loss: 0.00001558
Iteration 79/1000 | Loss: 0.00001558
Iteration 80/1000 | Loss: 0.00001557
Iteration 81/1000 | Loss: 0.00001557
Iteration 82/1000 | Loss: 0.00001557
Iteration 83/1000 | Loss: 0.00001557
Iteration 84/1000 | Loss: 0.00001557
Iteration 85/1000 | Loss: 0.00001557
Iteration 86/1000 | Loss: 0.00001557
Iteration 87/1000 | Loss: 0.00001556
Iteration 88/1000 | Loss: 0.00001556
Iteration 89/1000 | Loss: 0.00001556
Iteration 90/1000 | Loss: 0.00001556
Iteration 91/1000 | Loss: 0.00001556
Iteration 92/1000 | Loss: 0.00001556
Iteration 93/1000 | Loss: 0.00001555
Iteration 94/1000 | Loss: 0.00001555
Iteration 95/1000 | Loss: 0.00001555
Iteration 96/1000 | Loss: 0.00001555
Iteration 97/1000 | Loss: 0.00001554
Iteration 98/1000 | Loss: 0.00001554
Iteration 99/1000 | Loss: 0.00001554
Iteration 100/1000 | Loss: 0.00001554
Iteration 101/1000 | Loss: 0.00001554
Iteration 102/1000 | Loss: 0.00001554
Iteration 103/1000 | Loss: 0.00001554
Iteration 104/1000 | Loss: 0.00001554
Iteration 105/1000 | Loss: 0.00001554
Iteration 106/1000 | Loss: 0.00001554
Iteration 107/1000 | Loss: 0.00001554
Iteration 108/1000 | Loss: 0.00001554
Iteration 109/1000 | Loss: 0.00001554
Iteration 110/1000 | Loss: 0.00001553
Iteration 111/1000 | Loss: 0.00001553
Iteration 112/1000 | Loss: 0.00001553
Iteration 113/1000 | Loss: 0.00001552
Iteration 114/1000 | Loss: 0.00001552
Iteration 115/1000 | Loss: 0.00001552
Iteration 116/1000 | Loss: 0.00001551
Iteration 117/1000 | Loss: 0.00001551
Iteration 118/1000 | Loss: 0.00001551
Iteration 119/1000 | Loss: 0.00001551
Iteration 120/1000 | Loss: 0.00001550
Iteration 121/1000 | Loss: 0.00001550
Iteration 122/1000 | Loss: 0.00001550
Iteration 123/1000 | Loss: 0.00001550
Iteration 124/1000 | Loss: 0.00001550
Iteration 125/1000 | Loss: 0.00001549
Iteration 126/1000 | Loss: 0.00001549
Iteration 127/1000 | Loss: 0.00001548
Iteration 128/1000 | Loss: 0.00001548
Iteration 129/1000 | Loss: 0.00001548
Iteration 130/1000 | Loss: 0.00001548
Iteration 131/1000 | Loss: 0.00001547
Iteration 132/1000 | Loss: 0.00001547
Iteration 133/1000 | Loss: 0.00001546
Iteration 134/1000 | Loss: 0.00001546
Iteration 135/1000 | Loss: 0.00001546
Iteration 136/1000 | Loss: 0.00001546
Iteration 137/1000 | Loss: 0.00001546
Iteration 138/1000 | Loss: 0.00001546
Iteration 139/1000 | Loss: 0.00001546
Iteration 140/1000 | Loss: 0.00001545
Iteration 141/1000 | Loss: 0.00001545
Iteration 142/1000 | Loss: 0.00001545
Iteration 143/1000 | Loss: 0.00001545
Iteration 144/1000 | Loss: 0.00001545
Iteration 145/1000 | Loss: 0.00001545
Iteration 146/1000 | Loss: 0.00001545
Iteration 147/1000 | Loss: 0.00001545
Iteration 148/1000 | Loss: 0.00001545
Iteration 149/1000 | Loss: 0.00001544
Iteration 150/1000 | Loss: 0.00001544
Iteration 151/1000 | Loss: 0.00001544
Iteration 152/1000 | Loss: 0.00001544
Iteration 153/1000 | Loss: 0.00001544
Iteration 154/1000 | Loss: 0.00001544
Iteration 155/1000 | Loss: 0.00001543
Iteration 156/1000 | Loss: 0.00001543
Iteration 157/1000 | Loss: 0.00001543
Iteration 158/1000 | Loss: 0.00001542
Iteration 159/1000 | Loss: 0.00001542
Iteration 160/1000 | Loss: 0.00001542
Iteration 161/1000 | Loss: 0.00001542
Iteration 162/1000 | Loss: 0.00001542
Iteration 163/1000 | Loss: 0.00001542
Iteration 164/1000 | Loss: 0.00001542
Iteration 165/1000 | Loss: 0.00001542
Iteration 166/1000 | Loss: 0.00001542
Iteration 167/1000 | Loss: 0.00001542
Iteration 168/1000 | Loss: 0.00001541
Iteration 169/1000 | Loss: 0.00001541
Iteration 170/1000 | Loss: 0.00001541
Iteration 171/1000 | Loss: 0.00001540
Iteration 172/1000 | Loss: 0.00001540
Iteration 173/1000 | Loss: 0.00001540
Iteration 174/1000 | Loss: 0.00001540
Iteration 175/1000 | Loss: 0.00001540
Iteration 176/1000 | Loss: 0.00001540
Iteration 177/1000 | Loss: 0.00001540
Iteration 178/1000 | Loss: 0.00001540
Iteration 179/1000 | Loss: 0.00001540
Iteration 180/1000 | Loss: 0.00001540
Iteration 181/1000 | Loss: 0.00001540
Iteration 182/1000 | Loss: 0.00001540
Iteration 183/1000 | Loss: 0.00001540
Iteration 184/1000 | Loss: 0.00001540
Iteration 185/1000 | Loss: 0.00001540
Iteration 186/1000 | Loss: 0.00001540
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [1.540257471788209e-05, 1.540257471788209e-05, 1.540257471788209e-05, 1.540257471788209e-05, 1.540257471788209e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.540257471788209e-05

Optimization complete. Final v2v error: 3.2903122901916504 mm

Highest mean error: 4.385044097900391 mm for frame 92

Lowest mean error: 2.9396743774414062 mm for frame 19

Saving results

Total time: 40.87836003303528
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00705213
Iteration 2/25 | Loss: 0.00147590
Iteration 3/25 | Loss: 0.00135947
Iteration 4/25 | Loss: 0.00135053
Iteration 5/25 | Loss: 0.00134877
Iteration 6/25 | Loss: 0.00134877
Iteration 7/25 | Loss: 0.00134877
Iteration 8/25 | Loss: 0.00134877
Iteration 9/25 | Loss: 0.00134877
Iteration 10/25 | Loss: 0.00134877
Iteration 11/25 | Loss: 0.00134877
Iteration 12/25 | Loss: 0.00134877
Iteration 13/25 | Loss: 0.00134877
Iteration 14/25 | Loss: 0.00134877
Iteration 15/25 | Loss: 0.00134877
Iteration 16/25 | Loss: 0.00134877
Iteration 17/25 | Loss: 0.00134877
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001348766265437007, 0.001348766265437007, 0.001348766265437007, 0.001348766265437007, 0.001348766265437007]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001348766265437007

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.79633808
Iteration 2/25 | Loss: 0.00097128
Iteration 3/25 | Loss: 0.00097128
Iteration 4/25 | Loss: 0.00097128
Iteration 5/25 | Loss: 0.00097127
Iteration 6/25 | Loss: 0.00097127
Iteration 7/25 | Loss: 0.00097127
Iteration 8/25 | Loss: 0.00097127
Iteration 9/25 | Loss: 0.00097127
Iteration 10/25 | Loss: 0.00097127
Iteration 11/25 | Loss: 0.00097127
Iteration 12/25 | Loss: 0.00097127
Iteration 13/25 | Loss: 0.00097127
Iteration 14/25 | Loss: 0.00097127
Iteration 15/25 | Loss: 0.00097127
Iteration 16/25 | Loss: 0.00097127
Iteration 17/25 | Loss: 0.00097127
Iteration 18/25 | Loss: 0.00097127
Iteration 19/25 | Loss: 0.00097127
Iteration 20/25 | Loss: 0.00097127
Iteration 21/25 | Loss: 0.00097127
Iteration 22/25 | Loss: 0.00097127
Iteration 23/25 | Loss: 0.00097127
Iteration 24/25 | Loss: 0.00097127
Iteration 25/25 | Loss: 0.00097127

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097127
Iteration 2/1000 | Loss: 0.00004258
Iteration 3/1000 | Loss: 0.00002908
Iteration 4/1000 | Loss: 0.00002525
Iteration 5/1000 | Loss: 0.00002353
Iteration 6/1000 | Loss: 0.00002267
Iteration 7/1000 | Loss: 0.00002220
Iteration 8/1000 | Loss: 0.00002177
Iteration 9/1000 | Loss: 0.00002143
Iteration 10/1000 | Loss: 0.00002110
Iteration 11/1000 | Loss: 0.00002087
Iteration 12/1000 | Loss: 0.00002080
Iteration 13/1000 | Loss: 0.00002056
Iteration 14/1000 | Loss: 0.00002052
Iteration 15/1000 | Loss: 0.00002037
Iteration 16/1000 | Loss: 0.00002022
Iteration 17/1000 | Loss: 0.00002021
Iteration 18/1000 | Loss: 0.00002020
Iteration 19/1000 | Loss: 0.00002009
Iteration 20/1000 | Loss: 0.00002006
Iteration 21/1000 | Loss: 0.00002002
Iteration 22/1000 | Loss: 0.00002002
Iteration 23/1000 | Loss: 0.00002002
Iteration 24/1000 | Loss: 0.00002000
Iteration 25/1000 | Loss: 0.00002000
Iteration 26/1000 | Loss: 0.00001998
Iteration 27/1000 | Loss: 0.00001992
Iteration 28/1000 | Loss: 0.00001983
Iteration 29/1000 | Loss: 0.00001981
Iteration 30/1000 | Loss: 0.00001980
Iteration 31/1000 | Loss: 0.00001979
Iteration 32/1000 | Loss: 0.00001978
Iteration 33/1000 | Loss: 0.00001978
Iteration 34/1000 | Loss: 0.00001978
Iteration 35/1000 | Loss: 0.00001978
Iteration 36/1000 | Loss: 0.00001977
Iteration 37/1000 | Loss: 0.00001976
Iteration 38/1000 | Loss: 0.00001976
Iteration 39/1000 | Loss: 0.00001976
Iteration 40/1000 | Loss: 0.00001975
Iteration 41/1000 | Loss: 0.00001975
Iteration 42/1000 | Loss: 0.00001975
Iteration 43/1000 | Loss: 0.00001974
Iteration 44/1000 | Loss: 0.00001973
Iteration 45/1000 | Loss: 0.00001973
Iteration 46/1000 | Loss: 0.00001973
Iteration 47/1000 | Loss: 0.00001973
Iteration 48/1000 | Loss: 0.00001972
Iteration 49/1000 | Loss: 0.00001972
Iteration 50/1000 | Loss: 0.00001972
Iteration 51/1000 | Loss: 0.00001971
Iteration 52/1000 | Loss: 0.00001971
Iteration 53/1000 | Loss: 0.00001970
Iteration 54/1000 | Loss: 0.00001970
Iteration 55/1000 | Loss: 0.00001969
Iteration 56/1000 | Loss: 0.00001969
Iteration 57/1000 | Loss: 0.00001969
Iteration 58/1000 | Loss: 0.00001969
Iteration 59/1000 | Loss: 0.00001968
Iteration 60/1000 | Loss: 0.00001968
Iteration 61/1000 | Loss: 0.00001968
Iteration 62/1000 | Loss: 0.00001968
Iteration 63/1000 | Loss: 0.00001968
Iteration 64/1000 | Loss: 0.00001967
Iteration 65/1000 | Loss: 0.00001967
Iteration 66/1000 | Loss: 0.00001967
Iteration 67/1000 | Loss: 0.00001966
Iteration 68/1000 | Loss: 0.00001966
Iteration 69/1000 | Loss: 0.00001966
Iteration 70/1000 | Loss: 0.00001966
Iteration 71/1000 | Loss: 0.00001966
Iteration 72/1000 | Loss: 0.00001966
Iteration 73/1000 | Loss: 0.00001966
Iteration 74/1000 | Loss: 0.00001966
Iteration 75/1000 | Loss: 0.00001966
Iteration 76/1000 | Loss: 0.00001966
Iteration 77/1000 | Loss: 0.00001966
Iteration 78/1000 | Loss: 0.00001966
Iteration 79/1000 | Loss: 0.00001966
Iteration 80/1000 | Loss: 0.00001966
Iteration 81/1000 | Loss: 0.00001966
Iteration 82/1000 | Loss: 0.00001965
Iteration 83/1000 | Loss: 0.00001965
Iteration 84/1000 | Loss: 0.00001965
Iteration 85/1000 | Loss: 0.00001965
Iteration 86/1000 | Loss: 0.00001965
Iteration 87/1000 | Loss: 0.00001965
Iteration 88/1000 | Loss: 0.00001965
Iteration 89/1000 | Loss: 0.00001965
Iteration 90/1000 | Loss: 0.00001965
Iteration 91/1000 | Loss: 0.00001965
Iteration 92/1000 | Loss: 0.00001965
Iteration 93/1000 | Loss: 0.00001965
Iteration 94/1000 | Loss: 0.00001965
Iteration 95/1000 | Loss: 0.00001964
Iteration 96/1000 | Loss: 0.00001964
Iteration 97/1000 | Loss: 0.00001964
Iteration 98/1000 | Loss: 0.00001964
Iteration 99/1000 | Loss: 0.00001964
Iteration 100/1000 | Loss: 0.00001964
Iteration 101/1000 | Loss: 0.00001964
Iteration 102/1000 | Loss: 0.00001964
Iteration 103/1000 | Loss: 0.00001964
Iteration 104/1000 | Loss: 0.00001964
Iteration 105/1000 | Loss: 0.00001964
Iteration 106/1000 | Loss: 0.00001964
Iteration 107/1000 | Loss: 0.00001964
Iteration 108/1000 | Loss: 0.00001964
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [1.9635655917227268e-05, 1.9635655917227268e-05, 1.9635655917227268e-05, 1.9635655917227268e-05, 1.9635655917227268e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9635655917227268e-05

Optimization complete. Final v2v error: 3.692517042160034 mm

Highest mean error: 4.175040245056152 mm for frame 198

Lowest mean error: 3.3537981510162354 mm for frame 176

Saving results

Total time: 43.1412935256958
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00544225
Iteration 2/25 | Loss: 0.00141680
Iteration 3/25 | Loss: 0.00129137
Iteration 4/25 | Loss: 0.00129142
Iteration 5/25 | Loss: 0.00125703
Iteration 6/25 | Loss: 0.00125945
Iteration 7/25 | Loss: 0.00123723
Iteration 8/25 | Loss: 0.00123675
Iteration 9/25 | Loss: 0.00123666
Iteration 10/25 | Loss: 0.00123664
Iteration 11/25 | Loss: 0.00123664
Iteration 12/25 | Loss: 0.00123663
Iteration 13/25 | Loss: 0.00123663
Iteration 14/25 | Loss: 0.00123663
Iteration 15/25 | Loss: 0.00123662
Iteration 16/25 | Loss: 0.00123662
Iteration 17/25 | Loss: 0.00123662
Iteration 18/25 | Loss: 0.00123662
Iteration 19/25 | Loss: 0.00123662
Iteration 20/25 | Loss: 0.00123662
Iteration 21/25 | Loss: 0.00123662
Iteration 22/25 | Loss: 0.00123662
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012366165174171329, 0.0012366165174171329, 0.0012366165174171329, 0.0012366165174171329, 0.0012366165174171329]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012366165174171329

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.21690607
Iteration 2/25 | Loss: 0.00110917
Iteration 3/25 | Loss: 0.00110915
Iteration 4/25 | Loss: 0.00110915
Iteration 5/25 | Loss: 0.00110915
Iteration 6/25 | Loss: 0.00110915
Iteration 7/25 | Loss: 0.00110915
Iteration 8/25 | Loss: 0.00110915
Iteration 9/25 | Loss: 0.00110915
Iteration 10/25 | Loss: 0.00110915
Iteration 11/25 | Loss: 0.00110915
Iteration 12/25 | Loss: 0.00110915
Iteration 13/25 | Loss: 0.00110915
Iteration 14/25 | Loss: 0.00110915
Iteration 15/25 | Loss: 0.00110915
Iteration 16/25 | Loss: 0.00110915
Iteration 17/25 | Loss: 0.00110915
Iteration 18/25 | Loss: 0.00110915
Iteration 19/25 | Loss: 0.00110915
Iteration 20/25 | Loss: 0.00110915
Iteration 21/25 | Loss: 0.00110915
Iteration 22/25 | Loss: 0.00110915
Iteration 23/25 | Loss: 0.00110915
Iteration 24/25 | Loss: 0.00110915
Iteration 25/25 | Loss: 0.00110915

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110915
Iteration 2/1000 | Loss: 0.00002221
Iteration 3/1000 | Loss: 0.00001618
Iteration 4/1000 | Loss: 0.00002218
Iteration 5/1000 | Loss: 0.00001426
Iteration 6/1000 | Loss: 0.00001362
Iteration 7/1000 | Loss: 0.00001319
Iteration 8/1000 | Loss: 0.00001303
Iteration 9/1000 | Loss: 0.00001295
Iteration 10/1000 | Loss: 0.00001274
Iteration 11/1000 | Loss: 0.00001255
Iteration 12/1000 | Loss: 0.00001245
Iteration 13/1000 | Loss: 0.00001239
Iteration 14/1000 | Loss: 0.00001238
Iteration 15/1000 | Loss: 0.00001236
Iteration 16/1000 | Loss: 0.00001234
Iteration 17/1000 | Loss: 0.00001228
Iteration 18/1000 | Loss: 0.00001226
Iteration 19/1000 | Loss: 0.00001221
Iteration 20/1000 | Loss: 0.00001221
Iteration 21/1000 | Loss: 0.00001221
Iteration 22/1000 | Loss: 0.00001220
Iteration 23/1000 | Loss: 0.00001219
Iteration 24/1000 | Loss: 0.00001219
Iteration 25/1000 | Loss: 0.00001218
Iteration 26/1000 | Loss: 0.00001218
Iteration 27/1000 | Loss: 0.00001218
Iteration 28/1000 | Loss: 0.00001218
Iteration 29/1000 | Loss: 0.00001218
Iteration 30/1000 | Loss: 0.00001218
Iteration 31/1000 | Loss: 0.00001218
Iteration 32/1000 | Loss: 0.00001218
Iteration 33/1000 | Loss: 0.00001218
Iteration 34/1000 | Loss: 0.00001218
Iteration 35/1000 | Loss: 0.00001217
Iteration 36/1000 | Loss: 0.00001217
Iteration 37/1000 | Loss: 0.00001217
Iteration 38/1000 | Loss: 0.00001217
Iteration 39/1000 | Loss: 0.00001217
Iteration 40/1000 | Loss: 0.00001216
Iteration 41/1000 | Loss: 0.00001216
Iteration 42/1000 | Loss: 0.00001216
Iteration 43/1000 | Loss: 0.00001215
Iteration 44/1000 | Loss: 0.00001215
Iteration 45/1000 | Loss: 0.00001214
Iteration 46/1000 | Loss: 0.00001214
Iteration 47/1000 | Loss: 0.00001214
Iteration 48/1000 | Loss: 0.00001213
Iteration 49/1000 | Loss: 0.00001213
Iteration 50/1000 | Loss: 0.00001212
Iteration 51/1000 | Loss: 0.00001212
Iteration 52/1000 | Loss: 0.00001212
Iteration 53/1000 | Loss: 0.00001211
Iteration 54/1000 | Loss: 0.00001211
Iteration 55/1000 | Loss: 0.00001210
Iteration 56/1000 | Loss: 0.00001210
Iteration 57/1000 | Loss: 0.00001210
Iteration 58/1000 | Loss: 0.00001209
Iteration 59/1000 | Loss: 0.00001209
Iteration 60/1000 | Loss: 0.00001208
Iteration 61/1000 | Loss: 0.00001208
Iteration 62/1000 | Loss: 0.00001208
Iteration 63/1000 | Loss: 0.00001208
Iteration 64/1000 | Loss: 0.00001208
Iteration 65/1000 | Loss: 0.00001208
Iteration 66/1000 | Loss: 0.00001208
Iteration 67/1000 | Loss: 0.00001207
Iteration 68/1000 | Loss: 0.00001207
Iteration 69/1000 | Loss: 0.00001207
Iteration 70/1000 | Loss: 0.00001205
Iteration 71/1000 | Loss: 0.00001205
Iteration 72/1000 | Loss: 0.00001204
Iteration 73/1000 | Loss: 0.00001204
Iteration 74/1000 | Loss: 0.00001203
Iteration 75/1000 | Loss: 0.00001203
Iteration 76/1000 | Loss: 0.00001202
Iteration 77/1000 | Loss: 0.00001202
Iteration 78/1000 | Loss: 0.00001202
Iteration 79/1000 | Loss: 0.00001202
Iteration 80/1000 | Loss: 0.00001201
Iteration 81/1000 | Loss: 0.00001201
Iteration 82/1000 | Loss: 0.00001201
Iteration 83/1000 | Loss: 0.00001201
Iteration 84/1000 | Loss: 0.00001201
Iteration 85/1000 | Loss: 0.00001200
Iteration 86/1000 | Loss: 0.00001200
Iteration 87/1000 | Loss: 0.00001200
Iteration 88/1000 | Loss: 0.00001200
Iteration 89/1000 | Loss: 0.00001199
Iteration 90/1000 | Loss: 0.00001199
Iteration 91/1000 | Loss: 0.00001199
Iteration 92/1000 | Loss: 0.00001199
Iteration 93/1000 | Loss: 0.00001199
Iteration 94/1000 | Loss: 0.00001198
Iteration 95/1000 | Loss: 0.00001198
Iteration 96/1000 | Loss: 0.00001198
Iteration 97/1000 | Loss: 0.00001198
Iteration 98/1000 | Loss: 0.00001197
Iteration 99/1000 | Loss: 0.00001197
Iteration 100/1000 | Loss: 0.00001197
Iteration 101/1000 | Loss: 0.00001197
Iteration 102/1000 | Loss: 0.00001196
Iteration 103/1000 | Loss: 0.00001196
Iteration 104/1000 | Loss: 0.00001196
Iteration 105/1000 | Loss: 0.00001196
Iteration 106/1000 | Loss: 0.00001196
Iteration 107/1000 | Loss: 0.00001196
Iteration 108/1000 | Loss: 0.00001195
Iteration 109/1000 | Loss: 0.00001195
Iteration 110/1000 | Loss: 0.00001195
Iteration 111/1000 | Loss: 0.00001195
Iteration 112/1000 | Loss: 0.00001195
Iteration 113/1000 | Loss: 0.00001195
Iteration 114/1000 | Loss: 0.00001195
Iteration 115/1000 | Loss: 0.00001195
Iteration 116/1000 | Loss: 0.00001195
Iteration 117/1000 | Loss: 0.00001195
Iteration 118/1000 | Loss: 0.00001195
Iteration 119/1000 | Loss: 0.00001195
Iteration 120/1000 | Loss: 0.00001195
Iteration 121/1000 | Loss: 0.00001195
Iteration 122/1000 | Loss: 0.00001195
Iteration 123/1000 | Loss: 0.00001195
Iteration 124/1000 | Loss: 0.00001195
Iteration 125/1000 | Loss: 0.00001195
Iteration 126/1000 | Loss: 0.00001195
Iteration 127/1000 | Loss: 0.00001195
Iteration 128/1000 | Loss: 0.00001195
Iteration 129/1000 | Loss: 0.00001195
Iteration 130/1000 | Loss: 0.00001195
Iteration 131/1000 | Loss: 0.00001195
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [1.1945761798415333e-05, 1.1945761798415333e-05, 1.1945761798415333e-05, 1.1945761798415333e-05, 1.1945761798415333e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1945761798415333e-05

Optimization complete. Final v2v error: 2.915973424911499 mm

Highest mean error: 3.5862298011779785 mm for frame 165

Lowest mean error: 2.5637784004211426 mm for frame 90

Saving results

Total time: 47.26672387123108
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00566834
Iteration 2/25 | Loss: 0.00131048
Iteration 3/25 | Loss: 0.00123713
Iteration 4/25 | Loss: 0.00122423
Iteration 5/25 | Loss: 0.00122043
Iteration 6/25 | Loss: 0.00121982
Iteration 7/25 | Loss: 0.00121982
Iteration 8/25 | Loss: 0.00121982
Iteration 9/25 | Loss: 0.00121982
Iteration 10/25 | Loss: 0.00121982
Iteration 11/25 | Loss: 0.00121982
Iteration 12/25 | Loss: 0.00121982
Iteration 13/25 | Loss: 0.00121982
Iteration 14/25 | Loss: 0.00121982
Iteration 15/25 | Loss: 0.00121982
Iteration 16/25 | Loss: 0.00121982
Iteration 17/25 | Loss: 0.00121982
Iteration 18/25 | Loss: 0.00121982
Iteration 19/25 | Loss: 0.00121982
Iteration 20/25 | Loss: 0.00121982
Iteration 21/25 | Loss: 0.00121982
Iteration 22/25 | Loss: 0.00121982
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012198197655379772, 0.0012198197655379772, 0.0012198197655379772, 0.0012198197655379772, 0.0012198197655379772]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012198197655379772

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.98906177
Iteration 2/25 | Loss: 0.00068914
Iteration 3/25 | Loss: 0.00068914
Iteration 4/25 | Loss: 0.00068914
Iteration 5/25 | Loss: 0.00068914
Iteration 6/25 | Loss: 0.00068914
Iteration 7/25 | Loss: 0.00068914
Iteration 8/25 | Loss: 0.00068914
Iteration 9/25 | Loss: 0.00068914
Iteration 10/25 | Loss: 0.00068914
Iteration 11/25 | Loss: 0.00068914
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0006891423836350441, 0.0006891423836350441, 0.0006891423836350441, 0.0006891423836350441, 0.0006891423836350441]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006891423836350441

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068914
Iteration 2/1000 | Loss: 0.00003921
Iteration 3/1000 | Loss: 0.00003398
Iteration 4/1000 | Loss: 0.00003170
Iteration 5/1000 | Loss: 0.00002979
Iteration 6/1000 | Loss: 0.00002873
Iteration 7/1000 | Loss: 0.00002823
Iteration 8/1000 | Loss: 0.00002784
Iteration 9/1000 | Loss: 0.00002745
Iteration 10/1000 | Loss: 0.00002712
Iteration 11/1000 | Loss: 0.00002697
Iteration 12/1000 | Loss: 0.00002675
Iteration 13/1000 | Loss: 0.00002654
Iteration 14/1000 | Loss: 0.00002639
Iteration 15/1000 | Loss: 0.00002626
Iteration 16/1000 | Loss: 0.00002626
Iteration 17/1000 | Loss: 0.00002622
Iteration 18/1000 | Loss: 0.00002621
Iteration 19/1000 | Loss: 0.00002621
Iteration 20/1000 | Loss: 0.00002621
Iteration 21/1000 | Loss: 0.00002621
Iteration 22/1000 | Loss: 0.00002621
Iteration 23/1000 | Loss: 0.00002621
Iteration 24/1000 | Loss: 0.00002621
Iteration 25/1000 | Loss: 0.00002621
Iteration 26/1000 | Loss: 0.00002621
Iteration 27/1000 | Loss: 0.00002621
Iteration 28/1000 | Loss: 0.00002621
Iteration 29/1000 | Loss: 0.00002620
Iteration 30/1000 | Loss: 0.00002620
Iteration 31/1000 | Loss: 0.00002620
Iteration 32/1000 | Loss: 0.00002620
Iteration 33/1000 | Loss: 0.00002620
Iteration 34/1000 | Loss: 0.00002620
Iteration 35/1000 | Loss: 0.00002620
Iteration 36/1000 | Loss: 0.00002619
Iteration 37/1000 | Loss: 0.00002619
Iteration 38/1000 | Loss: 0.00002619
Iteration 39/1000 | Loss: 0.00002619
Iteration 40/1000 | Loss: 0.00002619
Iteration 41/1000 | Loss: 0.00002619
Iteration 42/1000 | Loss: 0.00002619
Iteration 43/1000 | Loss: 0.00002619
Iteration 44/1000 | Loss: 0.00002619
Iteration 45/1000 | Loss: 0.00002619
Iteration 46/1000 | Loss: 0.00002619
Iteration 47/1000 | Loss: 0.00002618
Iteration 48/1000 | Loss: 0.00002618
Iteration 49/1000 | Loss: 0.00002618
Iteration 50/1000 | Loss: 0.00002617
Iteration 51/1000 | Loss: 0.00002617
Iteration 52/1000 | Loss: 0.00002617
Iteration 53/1000 | Loss: 0.00002617
Iteration 54/1000 | Loss: 0.00002616
Iteration 55/1000 | Loss: 0.00002616
Iteration 56/1000 | Loss: 0.00002616
Iteration 57/1000 | Loss: 0.00002616
Iteration 58/1000 | Loss: 0.00002616
Iteration 59/1000 | Loss: 0.00002616
Iteration 60/1000 | Loss: 0.00002616
Iteration 61/1000 | Loss: 0.00002615
Iteration 62/1000 | Loss: 0.00002615
Iteration 63/1000 | Loss: 0.00002615
Iteration 64/1000 | Loss: 0.00002615
Iteration 65/1000 | Loss: 0.00002614
Iteration 66/1000 | Loss: 0.00002613
Iteration 67/1000 | Loss: 0.00002613
Iteration 68/1000 | Loss: 0.00002613
Iteration 69/1000 | Loss: 0.00002613
Iteration 70/1000 | Loss: 0.00002613
Iteration 71/1000 | Loss: 0.00002613
Iteration 72/1000 | Loss: 0.00002613
Iteration 73/1000 | Loss: 0.00002613
Iteration 74/1000 | Loss: 0.00002613
Iteration 75/1000 | Loss: 0.00002612
Iteration 76/1000 | Loss: 0.00002612
Iteration 77/1000 | Loss: 0.00002612
Iteration 78/1000 | Loss: 0.00002612
Iteration 79/1000 | Loss: 0.00002612
Iteration 80/1000 | Loss: 0.00002611
Iteration 81/1000 | Loss: 0.00002611
Iteration 82/1000 | Loss: 0.00002611
Iteration 83/1000 | Loss: 0.00002611
Iteration 84/1000 | Loss: 0.00002611
Iteration 85/1000 | Loss: 0.00002611
Iteration 86/1000 | Loss: 0.00002611
Iteration 87/1000 | Loss: 0.00002611
Iteration 88/1000 | Loss: 0.00002611
Iteration 89/1000 | Loss: 0.00002611
Iteration 90/1000 | Loss: 0.00002610
Iteration 91/1000 | Loss: 0.00002610
Iteration 92/1000 | Loss: 0.00002610
Iteration 93/1000 | Loss: 0.00002610
Iteration 94/1000 | Loss: 0.00002610
Iteration 95/1000 | Loss: 0.00002610
Iteration 96/1000 | Loss: 0.00002609
Iteration 97/1000 | Loss: 0.00002609
Iteration 98/1000 | Loss: 0.00002609
Iteration 99/1000 | Loss: 0.00002609
Iteration 100/1000 | Loss: 0.00002608
Iteration 101/1000 | Loss: 0.00002608
Iteration 102/1000 | Loss: 0.00002607
Iteration 103/1000 | Loss: 0.00002607
Iteration 104/1000 | Loss: 0.00002607
Iteration 105/1000 | Loss: 0.00002603
Iteration 106/1000 | Loss: 0.00002602
Iteration 107/1000 | Loss: 0.00002602
Iteration 108/1000 | Loss: 0.00002602
Iteration 109/1000 | Loss: 0.00002602
Iteration 110/1000 | Loss: 0.00002602
Iteration 111/1000 | Loss: 0.00002602
Iteration 112/1000 | Loss: 0.00002602
Iteration 113/1000 | Loss: 0.00002602
Iteration 114/1000 | Loss: 0.00002602
Iteration 115/1000 | Loss: 0.00002602
Iteration 116/1000 | Loss: 0.00002601
Iteration 117/1000 | Loss: 0.00002600
Iteration 118/1000 | Loss: 0.00002597
Iteration 119/1000 | Loss: 0.00002597
Iteration 120/1000 | Loss: 0.00002597
Iteration 121/1000 | Loss: 0.00002597
Iteration 122/1000 | Loss: 0.00002597
Iteration 123/1000 | Loss: 0.00002597
Iteration 124/1000 | Loss: 0.00002597
Iteration 125/1000 | Loss: 0.00002597
Iteration 126/1000 | Loss: 0.00002596
Iteration 127/1000 | Loss: 0.00002596
Iteration 128/1000 | Loss: 0.00002596
Iteration 129/1000 | Loss: 0.00002596
Iteration 130/1000 | Loss: 0.00002596
Iteration 131/1000 | Loss: 0.00002595
Iteration 132/1000 | Loss: 0.00002595
Iteration 133/1000 | Loss: 0.00002595
Iteration 134/1000 | Loss: 0.00002595
Iteration 135/1000 | Loss: 0.00002595
Iteration 136/1000 | Loss: 0.00002595
Iteration 137/1000 | Loss: 0.00002594
Iteration 138/1000 | Loss: 0.00002594
Iteration 139/1000 | Loss: 0.00002594
Iteration 140/1000 | Loss: 0.00002594
Iteration 141/1000 | Loss: 0.00002594
Iteration 142/1000 | Loss: 0.00002594
Iteration 143/1000 | Loss: 0.00002594
Iteration 144/1000 | Loss: 0.00002594
Iteration 145/1000 | Loss: 0.00002594
Iteration 146/1000 | Loss: 0.00002594
Iteration 147/1000 | Loss: 0.00002594
Iteration 148/1000 | Loss: 0.00002594
Iteration 149/1000 | Loss: 0.00002594
Iteration 150/1000 | Loss: 0.00002594
Iteration 151/1000 | Loss: 0.00002594
Iteration 152/1000 | Loss: 0.00002594
Iteration 153/1000 | Loss: 0.00002594
Iteration 154/1000 | Loss: 0.00002594
Iteration 155/1000 | Loss: 0.00002594
Iteration 156/1000 | Loss: 0.00002593
Iteration 157/1000 | Loss: 0.00002593
Iteration 158/1000 | Loss: 0.00002593
Iteration 159/1000 | Loss: 0.00002593
Iteration 160/1000 | Loss: 0.00002593
Iteration 161/1000 | Loss: 0.00002593
Iteration 162/1000 | Loss: 0.00002593
Iteration 163/1000 | Loss: 0.00002593
Iteration 164/1000 | Loss: 0.00002593
Iteration 165/1000 | Loss: 0.00002593
Iteration 166/1000 | Loss: 0.00002593
Iteration 167/1000 | Loss: 0.00002593
Iteration 168/1000 | Loss: 0.00002593
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [2.5934048608178273e-05, 2.5934048608178273e-05, 2.5934048608178273e-05, 2.5934048608178273e-05, 2.5934048608178273e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5934048608178273e-05

Optimization complete. Final v2v error: 4.386769771575928 mm

Highest mean error: 4.498492240905762 mm for frame 15

Lowest mean error: 4.2171525955200195 mm for frame 142

Saving results

Total time: 37.10231304168701
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00767365
Iteration 2/25 | Loss: 0.00173662
Iteration 3/25 | Loss: 0.00144478
Iteration 4/25 | Loss: 0.00141076
Iteration 5/25 | Loss: 0.00138847
Iteration 6/25 | Loss: 0.00139543
Iteration 7/25 | Loss: 0.00139494
Iteration 8/25 | Loss: 0.00140080
Iteration 9/25 | Loss: 0.00138152
Iteration 10/25 | Loss: 0.00137570
Iteration 11/25 | Loss: 0.00137333
Iteration 12/25 | Loss: 0.00136383
Iteration 13/25 | Loss: 0.00135573
Iteration 14/25 | Loss: 0.00134990
Iteration 15/25 | Loss: 0.00135185
Iteration 16/25 | Loss: 0.00134711
Iteration 17/25 | Loss: 0.00134587
Iteration 18/25 | Loss: 0.00135070
Iteration 19/25 | Loss: 0.00134652
Iteration 20/25 | Loss: 0.00134287
Iteration 21/25 | Loss: 0.00134235
Iteration 22/25 | Loss: 0.00134320
Iteration 23/25 | Loss: 0.00134299
Iteration 24/25 | Loss: 0.00134233
Iteration 25/25 | Loss: 0.00134204

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33273602
Iteration 2/25 | Loss: 0.00091539
Iteration 3/25 | Loss: 0.00091539
Iteration 4/25 | Loss: 0.00091539
Iteration 5/25 | Loss: 0.00091539
Iteration 6/25 | Loss: 0.00091539
Iteration 7/25 | Loss: 0.00091539
Iteration 8/25 | Loss: 0.00091539
Iteration 9/25 | Loss: 0.00091539
Iteration 10/25 | Loss: 0.00091538
Iteration 11/25 | Loss: 0.00091538
Iteration 12/25 | Loss: 0.00091538
Iteration 13/25 | Loss: 0.00091538
Iteration 14/25 | Loss: 0.00091538
Iteration 15/25 | Loss: 0.00091538
Iteration 16/25 | Loss: 0.00091538
Iteration 17/25 | Loss: 0.00091538
Iteration 18/25 | Loss: 0.00091538
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009153846767731011, 0.0009153846767731011, 0.0009153846767731011, 0.0009153846767731011, 0.0009153846767731011]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009153846767731011

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091538
Iteration 2/1000 | Loss: 0.00004888
Iteration 3/1000 | Loss: 0.00003537
Iteration 4/1000 | Loss: 0.00004371
Iteration 5/1000 | Loss: 0.00003877
Iteration 6/1000 | Loss: 0.00003801
Iteration 7/1000 | Loss: 0.00004663
Iteration 8/1000 | Loss: 0.00004505
Iteration 9/1000 | Loss: 0.00003924
Iteration 10/1000 | Loss: 0.00003345
Iteration 11/1000 | Loss: 0.00003568
Iteration 12/1000 | Loss: 0.00004125
Iteration 13/1000 | Loss: 0.00003733
Iteration 14/1000 | Loss: 0.00003508
Iteration 15/1000 | Loss: 0.00003003
Iteration 16/1000 | Loss: 0.00003613
Iteration 17/1000 | Loss: 0.00004155
Iteration 18/1000 | Loss: 0.00022617
Iteration 19/1000 | Loss: 0.00022220
Iteration 20/1000 | Loss: 0.00008728
Iteration 21/1000 | Loss: 0.00008504
Iteration 22/1000 | Loss: 0.00003955
Iteration 23/1000 | Loss: 0.00003328
Iteration 24/1000 | Loss: 0.00003161
Iteration 25/1000 | Loss: 0.00003002
Iteration 26/1000 | Loss: 0.00004794
Iteration 27/1000 | Loss: 0.00003123
Iteration 28/1000 | Loss: 0.00002875
Iteration 29/1000 | Loss: 0.00002787
Iteration 30/1000 | Loss: 0.00002751
Iteration 31/1000 | Loss: 0.00002737
Iteration 32/1000 | Loss: 0.00002711
Iteration 33/1000 | Loss: 0.00002693
Iteration 34/1000 | Loss: 0.00002690
Iteration 35/1000 | Loss: 0.00002688
Iteration 36/1000 | Loss: 0.00002688
Iteration 37/1000 | Loss: 0.00002687
Iteration 38/1000 | Loss: 0.00002687
Iteration 39/1000 | Loss: 0.00002671
Iteration 40/1000 | Loss: 0.00002667
Iteration 41/1000 | Loss: 0.00002661
Iteration 42/1000 | Loss: 0.00002657
Iteration 43/1000 | Loss: 0.00002656
Iteration 44/1000 | Loss: 0.00002649
Iteration 45/1000 | Loss: 0.00002646
Iteration 46/1000 | Loss: 0.00002641
Iteration 47/1000 | Loss: 0.00002640
Iteration 48/1000 | Loss: 0.00002639
Iteration 49/1000 | Loss: 0.00002639
Iteration 50/1000 | Loss: 0.00002639
Iteration 51/1000 | Loss: 0.00002636
Iteration 52/1000 | Loss: 0.00002635
Iteration 53/1000 | Loss: 0.00002635
Iteration 54/1000 | Loss: 0.00002634
Iteration 55/1000 | Loss: 0.00002634
Iteration 56/1000 | Loss: 0.00002634
Iteration 57/1000 | Loss: 0.00002634
Iteration 58/1000 | Loss: 0.00002633
Iteration 59/1000 | Loss: 0.00002633
Iteration 60/1000 | Loss: 0.00002633
Iteration 61/1000 | Loss: 0.00002632
Iteration 62/1000 | Loss: 0.00002632
Iteration 63/1000 | Loss: 0.00002632
Iteration 64/1000 | Loss: 0.00002632
Iteration 65/1000 | Loss: 0.00002631
Iteration 66/1000 | Loss: 0.00002631
Iteration 67/1000 | Loss: 0.00002631
Iteration 68/1000 | Loss: 0.00002631
Iteration 69/1000 | Loss: 0.00002630
Iteration 70/1000 | Loss: 0.00002630
Iteration 71/1000 | Loss: 0.00002630
Iteration 72/1000 | Loss: 0.00002630
Iteration 73/1000 | Loss: 0.00002630
Iteration 74/1000 | Loss: 0.00002630
Iteration 75/1000 | Loss: 0.00002630
Iteration 76/1000 | Loss: 0.00002630
Iteration 77/1000 | Loss: 0.00002629
Iteration 78/1000 | Loss: 0.00002629
Iteration 79/1000 | Loss: 0.00002629
Iteration 80/1000 | Loss: 0.00002629
Iteration 81/1000 | Loss: 0.00002626
Iteration 82/1000 | Loss: 0.00002625
Iteration 83/1000 | Loss: 0.00002625
Iteration 84/1000 | Loss: 0.00002624
Iteration 85/1000 | Loss: 0.00002623
Iteration 86/1000 | Loss: 0.00002623
Iteration 87/1000 | Loss: 0.00002622
Iteration 88/1000 | Loss: 0.00002622
Iteration 89/1000 | Loss: 0.00002622
Iteration 90/1000 | Loss: 0.00002621
Iteration 91/1000 | Loss: 0.00002621
Iteration 92/1000 | Loss: 0.00002620
Iteration 93/1000 | Loss: 0.00002620
Iteration 94/1000 | Loss: 0.00002619
Iteration 95/1000 | Loss: 0.00002619
Iteration 96/1000 | Loss: 0.00002619
Iteration 97/1000 | Loss: 0.00002619
Iteration 98/1000 | Loss: 0.00002618
Iteration 99/1000 | Loss: 0.00002618
Iteration 100/1000 | Loss: 0.00002617
Iteration 101/1000 | Loss: 0.00002617
Iteration 102/1000 | Loss: 0.00002617
Iteration 103/1000 | Loss: 0.00002616
Iteration 104/1000 | Loss: 0.00002616
Iteration 105/1000 | Loss: 0.00002616
Iteration 106/1000 | Loss: 0.00002616
Iteration 107/1000 | Loss: 0.00002615
Iteration 108/1000 | Loss: 0.00002615
Iteration 109/1000 | Loss: 0.00002615
Iteration 110/1000 | Loss: 0.00002614
Iteration 111/1000 | Loss: 0.00002614
Iteration 112/1000 | Loss: 0.00002614
Iteration 113/1000 | Loss: 0.00002614
Iteration 114/1000 | Loss: 0.00002614
Iteration 115/1000 | Loss: 0.00002614
Iteration 116/1000 | Loss: 0.00002614
Iteration 117/1000 | Loss: 0.00002614
Iteration 118/1000 | Loss: 0.00002614
Iteration 119/1000 | Loss: 0.00002614
Iteration 120/1000 | Loss: 0.00002614
Iteration 121/1000 | Loss: 0.00002613
Iteration 122/1000 | Loss: 0.00002613
Iteration 123/1000 | Loss: 0.00002613
Iteration 124/1000 | Loss: 0.00002610
Iteration 125/1000 | Loss: 0.00002609
Iteration 126/1000 | Loss: 0.00002609
Iteration 127/1000 | Loss: 0.00002609
Iteration 128/1000 | Loss: 0.00002608
Iteration 129/1000 | Loss: 0.00002608
Iteration 130/1000 | Loss: 0.00002607
Iteration 131/1000 | Loss: 0.00002607
Iteration 132/1000 | Loss: 0.00002606
Iteration 133/1000 | Loss: 0.00002606
Iteration 134/1000 | Loss: 0.00002606
Iteration 135/1000 | Loss: 0.00002606
Iteration 136/1000 | Loss: 0.00002606
Iteration 137/1000 | Loss: 0.00002605
Iteration 138/1000 | Loss: 0.00002605
Iteration 139/1000 | Loss: 0.00002605
Iteration 140/1000 | Loss: 0.00002605
Iteration 141/1000 | Loss: 0.00002605
Iteration 142/1000 | Loss: 0.00002605
Iteration 143/1000 | Loss: 0.00002605
Iteration 144/1000 | Loss: 0.00002605
Iteration 145/1000 | Loss: 0.00002605
Iteration 146/1000 | Loss: 0.00002604
Iteration 147/1000 | Loss: 0.00002604
Iteration 148/1000 | Loss: 0.00002603
Iteration 149/1000 | Loss: 0.00002603
Iteration 150/1000 | Loss: 0.00002602
Iteration 151/1000 | Loss: 0.00002601
Iteration 152/1000 | Loss: 0.00002601
Iteration 153/1000 | Loss: 0.00002600
Iteration 154/1000 | Loss: 0.00017615
Iteration 155/1000 | Loss: 0.00002918
Iteration 156/1000 | Loss: 0.00002803
Iteration 157/1000 | Loss: 0.00002754
Iteration 158/1000 | Loss: 0.00002707
Iteration 159/1000 | Loss: 0.00003554
Iteration 160/1000 | Loss: 0.00003040
Iteration 161/1000 | Loss: 0.00002729
Iteration 162/1000 | Loss: 0.00002623
Iteration 163/1000 | Loss: 0.00002589
Iteration 164/1000 | Loss: 0.00002570
Iteration 165/1000 | Loss: 0.00002560
Iteration 166/1000 | Loss: 0.00002558
Iteration 167/1000 | Loss: 0.00002542
Iteration 168/1000 | Loss: 0.00002531
Iteration 169/1000 | Loss: 0.00002526
Iteration 170/1000 | Loss: 0.00002520
Iteration 171/1000 | Loss: 0.00002520
Iteration 172/1000 | Loss: 0.00002519
Iteration 173/1000 | Loss: 0.00002519
Iteration 174/1000 | Loss: 0.00002518
Iteration 175/1000 | Loss: 0.00002517
Iteration 176/1000 | Loss: 0.00002517
Iteration 177/1000 | Loss: 0.00002516
Iteration 178/1000 | Loss: 0.00002516
Iteration 179/1000 | Loss: 0.00002515
Iteration 180/1000 | Loss: 0.00002515
Iteration 181/1000 | Loss: 0.00002512
Iteration 182/1000 | Loss: 0.00002510
Iteration 183/1000 | Loss: 0.00002509
Iteration 184/1000 | Loss: 0.00002503
Iteration 185/1000 | Loss: 0.00002500
Iteration 186/1000 | Loss: 0.00002499
Iteration 187/1000 | Loss: 0.00002498
Iteration 188/1000 | Loss: 0.00002497
Iteration 189/1000 | Loss: 0.00002484
Iteration 190/1000 | Loss: 0.00002481
Iteration 191/1000 | Loss: 0.00002479
Iteration 192/1000 | Loss: 0.00002479
Iteration 193/1000 | Loss: 0.00002472
Iteration 194/1000 | Loss: 0.00002469
Iteration 195/1000 | Loss: 0.00019256
Iteration 196/1000 | Loss: 0.00010388
Iteration 197/1000 | Loss: 0.00002494
Iteration 198/1000 | Loss: 0.00002472
Iteration 199/1000 | Loss: 0.00002467
Iteration 200/1000 | Loss: 0.00020292
Iteration 201/1000 | Loss: 0.00011307
Iteration 202/1000 | Loss: 0.00003211
Iteration 203/1000 | Loss: 0.00017712
Iteration 204/1000 | Loss: 0.00007193
Iteration 205/1000 | Loss: 0.00002485
Iteration 206/1000 | Loss: 0.00002473
Iteration 207/1000 | Loss: 0.00002473
Iteration 208/1000 | Loss: 0.00002473
Iteration 209/1000 | Loss: 0.00002473
Iteration 210/1000 | Loss: 0.00002472
Iteration 211/1000 | Loss: 0.00002472
Iteration 212/1000 | Loss: 0.00002472
Iteration 213/1000 | Loss: 0.00002471
Iteration 214/1000 | Loss: 0.00002470
Iteration 215/1000 | Loss: 0.00002470
Iteration 216/1000 | Loss: 0.00002469
Iteration 217/1000 | Loss: 0.00002469
Iteration 218/1000 | Loss: 0.00002469
Iteration 219/1000 | Loss: 0.00002468
Iteration 220/1000 | Loss: 0.00002468
Iteration 221/1000 | Loss: 0.00002468
Iteration 222/1000 | Loss: 0.00002468
Iteration 223/1000 | Loss: 0.00002467
Iteration 224/1000 | Loss: 0.00002467
Iteration 225/1000 | Loss: 0.00002467
Iteration 226/1000 | Loss: 0.00002466
Iteration 227/1000 | Loss: 0.00002466
Iteration 228/1000 | Loss: 0.00002466
Iteration 229/1000 | Loss: 0.00002466
Iteration 230/1000 | Loss: 0.00002466
Iteration 231/1000 | Loss: 0.00002466
Iteration 232/1000 | Loss: 0.00002466
Iteration 233/1000 | Loss: 0.00002466
Iteration 234/1000 | Loss: 0.00002466
Iteration 235/1000 | Loss: 0.00002466
Iteration 236/1000 | Loss: 0.00002466
Iteration 237/1000 | Loss: 0.00002465
Iteration 238/1000 | Loss: 0.00002465
Iteration 239/1000 | Loss: 0.00002465
Iteration 240/1000 | Loss: 0.00002465
Iteration 241/1000 | Loss: 0.00002465
Iteration 242/1000 | Loss: 0.00002465
Iteration 243/1000 | Loss: 0.00002465
Iteration 244/1000 | Loss: 0.00002465
Iteration 245/1000 | Loss: 0.00002465
Iteration 246/1000 | Loss: 0.00002465
Iteration 247/1000 | Loss: 0.00002465
Iteration 248/1000 | Loss: 0.00002465
Iteration 249/1000 | Loss: 0.00002465
Iteration 250/1000 | Loss: 0.00002465
Iteration 251/1000 | Loss: 0.00002465
Iteration 252/1000 | Loss: 0.00002465
Iteration 253/1000 | Loss: 0.00002465
Iteration 254/1000 | Loss: 0.00002465
Iteration 255/1000 | Loss: 0.00002465
Iteration 256/1000 | Loss: 0.00002465
Iteration 257/1000 | Loss: 0.00002465
Iteration 258/1000 | Loss: 0.00002465
Iteration 259/1000 | Loss: 0.00002465
Iteration 260/1000 | Loss: 0.00002465
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 260. Stopping optimization.
Last 5 losses: [2.46523559326306e-05, 2.46523559326306e-05, 2.46523559326306e-05, 2.46523559326306e-05, 2.46523559326306e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.46523559326306e-05

Optimization complete. Final v2v error: 4.007988452911377 mm

Highest mean error: 8.066293716430664 mm for frame 93

Lowest mean error: 3.3135576248168945 mm for frame 187

Saving results

Total time: 167.97937989234924
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01002578
Iteration 2/25 | Loss: 0.00655089
Iteration 3/25 | Loss: 0.00266717
Iteration 4/25 | Loss: 0.00228770
Iteration 5/25 | Loss: 0.00198813
Iteration 6/25 | Loss: 0.00189319
Iteration 7/25 | Loss: 0.00174900
Iteration 8/25 | Loss: 0.00163523
Iteration 9/25 | Loss: 0.00150627
Iteration 10/25 | Loss: 0.00150767
Iteration 11/25 | Loss: 0.00148380
Iteration 12/25 | Loss: 0.00142745
Iteration 13/25 | Loss: 0.00139516
Iteration 14/25 | Loss: 0.00141522
Iteration 15/25 | Loss: 0.00138239
Iteration 16/25 | Loss: 0.00136728
Iteration 17/25 | Loss: 0.00136981
Iteration 18/25 | Loss: 0.00136398
Iteration 19/25 | Loss: 0.00135932
Iteration 20/25 | Loss: 0.00135562
Iteration 21/25 | Loss: 0.00136034
Iteration 22/25 | Loss: 0.00135817
Iteration 23/25 | Loss: 0.00135204
Iteration 24/25 | Loss: 0.00135577
Iteration 25/25 | Loss: 0.00134188

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42376351
Iteration 2/25 | Loss: 0.00185181
Iteration 3/25 | Loss: 0.00122575
Iteration 4/25 | Loss: 0.00122575
Iteration 5/25 | Loss: 0.00122575
Iteration 6/25 | Loss: 0.00122575
Iteration 7/25 | Loss: 0.00122575
Iteration 8/25 | Loss: 0.00122575
Iteration 9/25 | Loss: 0.00122575
Iteration 10/25 | Loss: 0.00122575
Iteration 11/25 | Loss: 0.00122575
Iteration 12/25 | Loss: 0.00122575
Iteration 13/25 | Loss: 0.00122575
Iteration 14/25 | Loss: 0.00122575
Iteration 15/25 | Loss: 0.00122575
Iteration 16/25 | Loss: 0.00122575
Iteration 17/25 | Loss: 0.00122575
Iteration 18/25 | Loss: 0.00122575
Iteration 19/25 | Loss: 0.00122575
Iteration 20/25 | Loss: 0.00122575
Iteration 21/25 | Loss: 0.00122575
Iteration 22/25 | Loss: 0.00122575
Iteration 23/25 | Loss: 0.00122575
Iteration 24/25 | Loss: 0.00122575
Iteration 25/25 | Loss: 0.00122575
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0012257458874955773, 0.0012257458874955773, 0.0012257458874955773, 0.0012257458874955773, 0.0012257458874955773]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012257458874955773

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122575
Iteration 2/1000 | Loss: 0.00086279
Iteration 3/1000 | Loss: 0.00094695
Iteration 4/1000 | Loss: 0.00236418
Iteration 5/1000 | Loss: 0.00041899
Iteration 6/1000 | Loss: 0.00089075
Iteration 7/1000 | Loss: 0.00039448
Iteration 8/1000 | Loss: 0.00102860
Iteration 9/1000 | Loss: 0.00064811
Iteration 10/1000 | Loss: 0.00065998
Iteration 11/1000 | Loss: 0.00014550
Iteration 12/1000 | Loss: 0.00039975
Iteration 13/1000 | Loss: 0.00098575
Iteration 14/1000 | Loss: 0.00027804
Iteration 15/1000 | Loss: 0.00015911
Iteration 16/1000 | Loss: 0.00006921
Iteration 17/1000 | Loss: 0.00019486
Iteration 18/1000 | Loss: 0.00035684
Iteration 19/1000 | Loss: 0.00062771
Iteration 20/1000 | Loss: 0.00018372
Iteration 21/1000 | Loss: 0.00024564
Iteration 22/1000 | Loss: 0.00020695
Iteration 23/1000 | Loss: 0.00020395
Iteration 24/1000 | Loss: 0.00037393
Iteration 25/1000 | Loss: 0.00059648
Iteration 26/1000 | Loss: 0.00052655
Iteration 27/1000 | Loss: 0.00011187
Iteration 28/1000 | Loss: 0.00022676
Iteration 29/1000 | Loss: 0.00009350
Iteration 30/1000 | Loss: 0.00032818
Iteration 31/1000 | Loss: 0.00007749
Iteration 32/1000 | Loss: 0.00032120
Iteration 33/1000 | Loss: 0.00070985
Iteration 34/1000 | Loss: 0.00041179
Iteration 35/1000 | Loss: 0.00014883
Iteration 36/1000 | Loss: 0.00048589
Iteration 37/1000 | Loss: 0.00022186
Iteration 38/1000 | Loss: 0.00027860
Iteration 39/1000 | Loss: 0.00007466
Iteration 40/1000 | Loss: 0.00020870
Iteration 41/1000 | Loss: 0.00005675
Iteration 42/1000 | Loss: 0.00017408
Iteration 43/1000 | Loss: 0.00005661
Iteration 44/1000 | Loss: 0.00008883
Iteration 45/1000 | Loss: 0.00023447
Iteration 46/1000 | Loss: 0.00018717
Iteration 47/1000 | Loss: 0.00020726
Iteration 48/1000 | Loss: 0.00011118
Iteration 49/1000 | Loss: 0.00004962
Iteration 50/1000 | Loss: 0.00062989
Iteration 51/1000 | Loss: 0.00067850
Iteration 52/1000 | Loss: 0.00029906
Iteration 53/1000 | Loss: 0.00007508
Iteration 54/1000 | Loss: 0.00005496
Iteration 55/1000 | Loss: 0.00004784
Iteration 56/1000 | Loss: 0.00007374
Iteration 57/1000 | Loss: 0.00064196
Iteration 58/1000 | Loss: 0.00006964
Iteration 59/1000 | Loss: 0.00005412
Iteration 60/1000 | Loss: 0.00003974
Iteration 61/1000 | Loss: 0.00004565
Iteration 62/1000 | Loss: 0.00022499
Iteration 63/1000 | Loss: 0.00030865
Iteration 64/1000 | Loss: 0.00017203
Iteration 65/1000 | Loss: 0.00017782
Iteration 66/1000 | Loss: 0.00024134
Iteration 67/1000 | Loss: 0.00009919
Iteration 68/1000 | Loss: 0.00010896
Iteration 69/1000 | Loss: 0.00003492
Iteration 70/1000 | Loss: 0.00024218
Iteration 71/1000 | Loss: 0.00004419
Iteration 72/1000 | Loss: 0.00020591
Iteration 73/1000 | Loss: 0.00031728
Iteration 74/1000 | Loss: 0.00026475
Iteration 75/1000 | Loss: 0.00013931
Iteration 76/1000 | Loss: 0.00016156
Iteration 77/1000 | Loss: 0.00020125
Iteration 78/1000 | Loss: 0.00005383
Iteration 79/1000 | Loss: 0.00022367
Iteration 80/1000 | Loss: 0.00005654
Iteration 81/1000 | Loss: 0.00008678
Iteration 82/1000 | Loss: 0.00003355
Iteration 83/1000 | Loss: 0.00003226
Iteration 84/1000 | Loss: 0.00011763
Iteration 85/1000 | Loss: 0.00018977
Iteration 86/1000 | Loss: 0.00004064
Iteration 87/1000 | Loss: 0.00011077
Iteration 88/1000 | Loss: 0.00009712
Iteration 89/1000 | Loss: 0.00040448
Iteration 90/1000 | Loss: 0.00003971
Iteration 91/1000 | Loss: 0.00002886
Iteration 92/1000 | Loss: 0.00003113
Iteration 93/1000 | Loss: 0.00002812
Iteration 94/1000 | Loss: 0.00002771
Iteration 95/1000 | Loss: 0.00002730
Iteration 96/1000 | Loss: 0.00002699
Iteration 97/1000 | Loss: 0.00002681
Iteration 98/1000 | Loss: 0.00012936
Iteration 99/1000 | Loss: 0.00021566
Iteration 100/1000 | Loss: 0.00012424
Iteration 101/1000 | Loss: 0.00002787
Iteration 102/1000 | Loss: 0.00008991
Iteration 103/1000 | Loss: 0.00002704
Iteration 104/1000 | Loss: 0.00013841
Iteration 105/1000 | Loss: 0.00006134
Iteration 106/1000 | Loss: 0.00004364
Iteration 107/1000 | Loss: 0.00012708
Iteration 108/1000 | Loss: 0.00009424
Iteration 109/1000 | Loss: 0.00002785
Iteration 110/1000 | Loss: 0.00002990
Iteration 111/1000 | Loss: 0.00002632
Iteration 112/1000 | Loss: 0.00002623
Iteration 113/1000 | Loss: 0.00002622
Iteration 114/1000 | Loss: 0.00002621
Iteration 115/1000 | Loss: 0.00002621
Iteration 116/1000 | Loss: 0.00002621
Iteration 117/1000 | Loss: 0.00002620
Iteration 118/1000 | Loss: 0.00002620
Iteration 119/1000 | Loss: 0.00002620
Iteration 120/1000 | Loss: 0.00002619
Iteration 121/1000 | Loss: 0.00002619
Iteration 122/1000 | Loss: 0.00002618
Iteration 123/1000 | Loss: 0.00002616
Iteration 124/1000 | Loss: 0.00002615
Iteration 125/1000 | Loss: 0.00002613
Iteration 126/1000 | Loss: 0.00002611
Iteration 127/1000 | Loss: 0.00002610
Iteration 128/1000 | Loss: 0.00002610
Iteration 129/1000 | Loss: 0.00002609
Iteration 130/1000 | Loss: 0.00002609
Iteration 131/1000 | Loss: 0.00002609
Iteration 132/1000 | Loss: 0.00002609
Iteration 133/1000 | Loss: 0.00002608
Iteration 134/1000 | Loss: 0.00002608
Iteration 135/1000 | Loss: 0.00002608
Iteration 136/1000 | Loss: 0.00002607
Iteration 137/1000 | Loss: 0.00002607
Iteration 138/1000 | Loss: 0.00002607
Iteration 139/1000 | Loss: 0.00002607
Iteration 140/1000 | Loss: 0.00002607
Iteration 141/1000 | Loss: 0.00002606
Iteration 142/1000 | Loss: 0.00002606
Iteration 143/1000 | Loss: 0.00002606
Iteration 144/1000 | Loss: 0.00002606
Iteration 145/1000 | Loss: 0.00002606
Iteration 146/1000 | Loss: 0.00002606
Iteration 147/1000 | Loss: 0.00002606
Iteration 148/1000 | Loss: 0.00002605
Iteration 149/1000 | Loss: 0.00002605
Iteration 150/1000 | Loss: 0.00002605
Iteration 151/1000 | Loss: 0.00002604
Iteration 152/1000 | Loss: 0.00002604
Iteration 153/1000 | Loss: 0.00002603
Iteration 154/1000 | Loss: 0.00002603
Iteration 155/1000 | Loss: 0.00002603
Iteration 156/1000 | Loss: 0.00002603
Iteration 157/1000 | Loss: 0.00002602
Iteration 158/1000 | Loss: 0.00002602
Iteration 159/1000 | Loss: 0.00002602
Iteration 160/1000 | Loss: 0.00002602
Iteration 161/1000 | Loss: 0.00002602
Iteration 162/1000 | Loss: 0.00002602
Iteration 163/1000 | Loss: 0.00002602
Iteration 164/1000 | Loss: 0.00002601
Iteration 165/1000 | Loss: 0.00002601
Iteration 166/1000 | Loss: 0.00002600
Iteration 167/1000 | Loss: 0.00002600
Iteration 168/1000 | Loss: 0.00002599
Iteration 169/1000 | Loss: 0.00002599
Iteration 170/1000 | Loss: 0.00002599
Iteration 171/1000 | Loss: 0.00002598
Iteration 172/1000 | Loss: 0.00010417
Iteration 173/1000 | Loss: 0.00002598
Iteration 174/1000 | Loss: 0.00002598
Iteration 175/1000 | Loss: 0.00002598
Iteration 176/1000 | Loss: 0.00002597
Iteration 177/1000 | Loss: 0.00002597
Iteration 178/1000 | Loss: 0.00002596
Iteration 179/1000 | Loss: 0.00008726
Iteration 180/1000 | Loss: 0.00007087
Iteration 181/1000 | Loss: 0.00006283
Iteration 182/1000 | Loss: 0.00006869
Iteration 183/1000 | Loss: 0.00055820
Iteration 184/1000 | Loss: 0.00003749
Iteration 185/1000 | Loss: 0.00003217
Iteration 186/1000 | Loss: 0.00007711
Iteration 187/1000 | Loss: 0.00013206
Iteration 188/1000 | Loss: 0.00016232
Iteration 189/1000 | Loss: 0.00003305
Iteration 190/1000 | Loss: 0.00002948
Iteration 191/1000 | Loss: 0.00002747
Iteration 192/1000 | Loss: 0.00014141
Iteration 193/1000 | Loss: 0.00003297
Iteration 194/1000 | Loss: 0.00002504
Iteration 195/1000 | Loss: 0.00008039
Iteration 196/1000 | Loss: 0.00023049
Iteration 197/1000 | Loss: 0.00014839
Iteration 198/1000 | Loss: 0.00054133
Iteration 199/1000 | Loss: 0.00097212
Iteration 200/1000 | Loss: 0.00003077
Iteration 201/1000 | Loss: 0.00002562
Iteration 202/1000 | Loss: 0.00002428
Iteration 203/1000 | Loss: 0.00002365
Iteration 204/1000 | Loss: 0.00002326
Iteration 205/1000 | Loss: 0.00007059
Iteration 206/1000 | Loss: 0.00002338
Iteration 207/1000 | Loss: 0.00002312
Iteration 208/1000 | Loss: 0.00015359
Iteration 209/1000 | Loss: 0.00020849
Iteration 210/1000 | Loss: 0.00044738
Iteration 211/1000 | Loss: 0.00037970
Iteration 212/1000 | Loss: 0.00004115
Iteration 213/1000 | Loss: 0.00002944
Iteration 214/1000 | Loss: 0.00002379
Iteration 215/1000 | Loss: 0.00002320
Iteration 216/1000 | Loss: 0.00002308
Iteration 217/1000 | Loss: 0.00002300
Iteration 218/1000 | Loss: 0.00002297
Iteration 219/1000 | Loss: 0.00006413
Iteration 220/1000 | Loss: 0.00019125
Iteration 221/1000 | Loss: 0.00005822
Iteration 222/1000 | Loss: 0.00002372
Iteration 223/1000 | Loss: 0.00005938
Iteration 224/1000 | Loss: 0.00013270
Iteration 225/1000 | Loss: 0.00002605
Iteration 226/1000 | Loss: 0.00002528
Iteration 227/1000 | Loss: 0.00002291
Iteration 228/1000 | Loss: 0.00002981
Iteration 229/1000 | Loss: 0.00002289
Iteration 230/1000 | Loss: 0.00002289
Iteration 231/1000 | Loss: 0.00002288
Iteration 232/1000 | Loss: 0.00002288
Iteration 233/1000 | Loss: 0.00002288
Iteration 234/1000 | Loss: 0.00002288
Iteration 235/1000 | Loss: 0.00002288
Iteration 236/1000 | Loss: 0.00002288
Iteration 237/1000 | Loss: 0.00002287
Iteration 238/1000 | Loss: 0.00002287
Iteration 239/1000 | Loss: 0.00002286
Iteration 240/1000 | Loss: 0.00002286
Iteration 241/1000 | Loss: 0.00002286
Iteration 242/1000 | Loss: 0.00002286
Iteration 243/1000 | Loss: 0.00002286
Iteration 244/1000 | Loss: 0.00002286
Iteration 245/1000 | Loss: 0.00002286
Iteration 246/1000 | Loss: 0.00002286
Iteration 247/1000 | Loss: 0.00002286
Iteration 248/1000 | Loss: 0.00002286
Iteration 249/1000 | Loss: 0.00002286
Iteration 250/1000 | Loss: 0.00002286
Iteration 251/1000 | Loss: 0.00002286
Iteration 252/1000 | Loss: 0.00002286
Iteration 253/1000 | Loss: 0.00002286
Iteration 254/1000 | Loss: 0.00002286
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 254. Stopping optimization.
Last 5 losses: [2.2859992895973846e-05, 2.2859992895973846e-05, 2.2859992895973846e-05, 2.2859992895973846e-05, 2.2859992895973846e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2859992895973846e-05

Optimization complete. Final v2v error: 3.775263786315918 mm

Highest mean error: 11.141914367675781 mm for frame 199

Lowest mean error: 3.30256986618042 mm for frame 136

Saving results

Total time: 308.473726272583
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00349416
Iteration 2/25 | Loss: 0.00132094
Iteration 3/25 | Loss: 0.00122584
Iteration 4/25 | Loss: 0.00120822
Iteration 5/25 | Loss: 0.00120081
Iteration 6/25 | Loss: 0.00119950
Iteration 7/25 | Loss: 0.00119950
Iteration 8/25 | Loss: 0.00119950
Iteration 9/25 | Loss: 0.00119950
Iteration 10/25 | Loss: 0.00119950
Iteration 11/25 | Loss: 0.00119950
Iteration 12/25 | Loss: 0.00119950
Iteration 13/25 | Loss: 0.00119950
Iteration 14/25 | Loss: 0.00119950
Iteration 15/25 | Loss: 0.00119950
Iteration 16/25 | Loss: 0.00119950
Iteration 17/25 | Loss: 0.00119950
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011995035456493497, 0.0011995035456493497, 0.0011995035456493497, 0.0011995035456493497, 0.0011995035456493497]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011995035456493497

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30339038
Iteration 2/25 | Loss: 0.00122014
Iteration 3/25 | Loss: 0.00122014
Iteration 4/25 | Loss: 0.00122014
Iteration 5/25 | Loss: 0.00122014
Iteration 6/25 | Loss: 0.00122014
Iteration 7/25 | Loss: 0.00122014
Iteration 8/25 | Loss: 0.00122014
Iteration 9/25 | Loss: 0.00122014
Iteration 10/25 | Loss: 0.00122014
Iteration 11/25 | Loss: 0.00122014
Iteration 12/25 | Loss: 0.00122014
Iteration 13/25 | Loss: 0.00122014
Iteration 14/25 | Loss: 0.00122014
Iteration 15/25 | Loss: 0.00122014
Iteration 16/25 | Loss: 0.00122014
Iteration 17/25 | Loss: 0.00122014
Iteration 18/25 | Loss: 0.00122014
Iteration 19/25 | Loss: 0.00122014
Iteration 20/25 | Loss: 0.00122014
Iteration 21/25 | Loss: 0.00122014
Iteration 22/25 | Loss: 0.00122014
Iteration 23/25 | Loss: 0.00122014
Iteration 24/25 | Loss: 0.00122014
Iteration 25/25 | Loss: 0.00122014

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122014
Iteration 2/1000 | Loss: 0.00005132
Iteration 3/1000 | Loss: 0.00003382
Iteration 4/1000 | Loss: 0.00002686
Iteration 5/1000 | Loss: 0.00002432
Iteration 6/1000 | Loss: 0.00002239
Iteration 7/1000 | Loss: 0.00002088
Iteration 8/1000 | Loss: 0.00002019
Iteration 9/1000 | Loss: 0.00001957
Iteration 10/1000 | Loss: 0.00001912
Iteration 11/1000 | Loss: 0.00001877
Iteration 12/1000 | Loss: 0.00001846
Iteration 13/1000 | Loss: 0.00001840
Iteration 14/1000 | Loss: 0.00001830
Iteration 15/1000 | Loss: 0.00001813
Iteration 16/1000 | Loss: 0.00001795
Iteration 17/1000 | Loss: 0.00001793
Iteration 18/1000 | Loss: 0.00001790
Iteration 19/1000 | Loss: 0.00001790
Iteration 20/1000 | Loss: 0.00001789
Iteration 21/1000 | Loss: 0.00001789
Iteration 22/1000 | Loss: 0.00001788
Iteration 23/1000 | Loss: 0.00001785
Iteration 24/1000 | Loss: 0.00001780
Iteration 25/1000 | Loss: 0.00001772
Iteration 26/1000 | Loss: 0.00001769
Iteration 27/1000 | Loss: 0.00001768
Iteration 28/1000 | Loss: 0.00001767
Iteration 29/1000 | Loss: 0.00001767
Iteration 30/1000 | Loss: 0.00001766
Iteration 31/1000 | Loss: 0.00001766
Iteration 32/1000 | Loss: 0.00001765
Iteration 33/1000 | Loss: 0.00001765
Iteration 34/1000 | Loss: 0.00001764
Iteration 35/1000 | Loss: 0.00001764
Iteration 36/1000 | Loss: 0.00001764
Iteration 37/1000 | Loss: 0.00001763
Iteration 38/1000 | Loss: 0.00001762
Iteration 39/1000 | Loss: 0.00001762
Iteration 40/1000 | Loss: 0.00001761
Iteration 41/1000 | Loss: 0.00001761
Iteration 42/1000 | Loss: 0.00001760
Iteration 43/1000 | Loss: 0.00001760
Iteration 44/1000 | Loss: 0.00001759
Iteration 45/1000 | Loss: 0.00001759
Iteration 46/1000 | Loss: 0.00001758
Iteration 47/1000 | Loss: 0.00001758
Iteration 48/1000 | Loss: 0.00001757
Iteration 49/1000 | Loss: 0.00001757
Iteration 50/1000 | Loss: 0.00001756
Iteration 51/1000 | Loss: 0.00001756
Iteration 52/1000 | Loss: 0.00001755
Iteration 53/1000 | Loss: 0.00001755
Iteration 54/1000 | Loss: 0.00001755
Iteration 55/1000 | Loss: 0.00001755
Iteration 56/1000 | Loss: 0.00001754
Iteration 57/1000 | Loss: 0.00001754
Iteration 58/1000 | Loss: 0.00001754
Iteration 59/1000 | Loss: 0.00001753
Iteration 60/1000 | Loss: 0.00001753
Iteration 61/1000 | Loss: 0.00001753
Iteration 62/1000 | Loss: 0.00001752
Iteration 63/1000 | Loss: 0.00001752
Iteration 64/1000 | Loss: 0.00001752
Iteration 65/1000 | Loss: 0.00001752
Iteration 66/1000 | Loss: 0.00001751
Iteration 67/1000 | Loss: 0.00001751
Iteration 68/1000 | Loss: 0.00001750
Iteration 69/1000 | Loss: 0.00001750
Iteration 70/1000 | Loss: 0.00001750
Iteration 71/1000 | Loss: 0.00001750
Iteration 72/1000 | Loss: 0.00001750
Iteration 73/1000 | Loss: 0.00001750
Iteration 74/1000 | Loss: 0.00001749
Iteration 75/1000 | Loss: 0.00001749
Iteration 76/1000 | Loss: 0.00001749
Iteration 77/1000 | Loss: 0.00001749
Iteration 78/1000 | Loss: 0.00001749
Iteration 79/1000 | Loss: 0.00001749
Iteration 80/1000 | Loss: 0.00001749
Iteration 81/1000 | Loss: 0.00001748
Iteration 82/1000 | Loss: 0.00001748
Iteration 83/1000 | Loss: 0.00001748
Iteration 84/1000 | Loss: 0.00001746
Iteration 85/1000 | Loss: 0.00001746
Iteration 86/1000 | Loss: 0.00001746
Iteration 87/1000 | Loss: 0.00001746
Iteration 88/1000 | Loss: 0.00001746
Iteration 89/1000 | Loss: 0.00001746
Iteration 90/1000 | Loss: 0.00001745
Iteration 91/1000 | Loss: 0.00001744
Iteration 92/1000 | Loss: 0.00001744
Iteration 93/1000 | Loss: 0.00001743
Iteration 94/1000 | Loss: 0.00001743
Iteration 95/1000 | Loss: 0.00001743
Iteration 96/1000 | Loss: 0.00001743
Iteration 97/1000 | Loss: 0.00001742
Iteration 98/1000 | Loss: 0.00001742
Iteration 99/1000 | Loss: 0.00001742
Iteration 100/1000 | Loss: 0.00001741
Iteration 101/1000 | Loss: 0.00001741
Iteration 102/1000 | Loss: 0.00001741
Iteration 103/1000 | Loss: 0.00001740
Iteration 104/1000 | Loss: 0.00001740
Iteration 105/1000 | Loss: 0.00001740
Iteration 106/1000 | Loss: 0.00001740
Iteration 107/1000 | Loss: 0.00001739
Iteration 108/1000 | Loss: 0.00001739
Iteration 109/1000 | Loss: 0.00001739
Iteration 110/1000 | Loss: 0.00001739
Iteration 111/1000 | Loss: 0.00001739
Iteration 112/1000 | Loss: 0.00001739
Iteration 113/1000 | Loss: 0.00001739
Iteration 114/1000 | Loss: 0.00001739
Iteration 115/1000 | Loss: 0.00001739
Iteration 116/1000 | Loss: 0.00001739
Iteration 117/1000 | Loss: 0.00001739
Iteration 118/1000 | Loss: 0.00001739
Iteration 119/1000 | Loss: 0.00001739
Iteration 120/1000 | Loss: 0.00001738
Iteration 121/1000 | Loss: 0.00001738
Iteration 122/1000 | Loss: 0.00001738
Iteration 123/1000 | Loss: 0.00001738
Iteration 124/1000 | Loss: 0.00001738
Iteration 125/1000 | Loss: 0.00001738
Iteration 126/1000 | Loss: 0.00001738
Iteration 127/1000 | Loss: 0.00001738
Iteration 128/1000 | Loss: 0.00001738
Iteration 129/1000 | Loss: 0.00001738
Iteration 130/1000 | Loss: 0.00001738
Iteration 131/1000 | Loss: 0.00001738
Iteration 132/1000 | Loss: 0.00001738
Iteration 133/1000 | Loss: 0.00001738
Iteration 134/1000 | Loss: 0.00001737
Iteration 135/1000 | Loss: 0.00001737
Iteration 136/1000 | Loss: 0.00001737
Iteration 137/1000 | Loss: 0.00001737
Iteration 138/1000 | Loss: 0.00001737
Iteration 139/1000 | Loss: 0.00001737
Iteration 140/1000 | Loss: 0.00001737
Iteration 141/1000 | Loss: 0.00001737
Iteration 142/1000 | Loss: 0.00001737
Iteration 143/1000 | Loss: 0.00001737
Iteration 144/1000 | Loss: 0.00001737
Iteration 145/1000 | Loss: 0.00001737
Iteration 146/1000 | Loss: 0.00001737
Iteration 147/1000 | Loss: 0.00001737
Iteration 148/1000 | Loss: 0.00001737
Iteration 149/1000 | Loss: 0.00001737
Iteration 150/1000 | Loss: 0.00001736
Iteration 151/1000 | Loss: 0.00001736
Iteration 152/1000 | Loss: 0.00001736
Iteration 153/1000 | Loss: 0.00001736
Iteration 154/1000 | Loss: 0.00001736
Iteration 155/1000 | Loss: 0.00001736
Iteration 156/1000 | Loss: 0.00001736
Iteration 157/1000 | Loss: 0.00001736
Iteration 158/1000 | Loss: 0.00001736
Iteration 159/1000 | Loss: 0.00001736
Iteration 160/1000 | Loss: 0.00001736
Iteration 161/1000 | Loss: 0.00001736
Iteration 162/1000 | Loss: 0.00001736
Iteration 163/1000 | Loss: 0.00001736
Iteration 164/1000 | Loss: 0.00001736
Iteration 165/1000 | Loss: 0.00001736
Iteration 166/1000 | Loss: 0.00001736
Iteration 167/1000 | Loss: 0.00001736
Iteration 168/1000 | Loss: 0.00001736
Iteration 169/1000 | Loss: 0.00001736
Iteration 170/1000 | Loss: 0.00001736
Iteration 171/1000 | Loss: 0.00001736
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.7363359802402556e-05, 1.7363359802402556e-05, 1.7363359802402556e-05, 1.7363359802402556e-05, 1.7363359802402556e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7363359802402556e-05

Optimization complete. Final v2v error: 3.4935319423675537 mm

Highest mean error: 4.393631935119629 mm for frame 143

Lowest mean error: 2.9574451446533203 mm for frame 179

Saving results

Total time: 48.3544762134552
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00872786
Iteration 2/25 | Loss: 0.00290244
Iteration 3/25 | Loss: 0.00192964
Iteration 4/25 | Loss: 0.00163615
Iteration 5/25 | Loss: 0.00160797
Iteration 6/25 | Loss: 0.00158690
Iteration 7/25 | Loss: 0.00156871
Iteration 8/25 | Loss: 0.00156618
Iteration 9/25 | Loss: 0.00156724
Iteration 10/25 | Loss: 0.00153309
Iteration 11/25 | Loss: 0.00151441
Iteration 12/25 | Loss: 0.00151304
Iteration 13/25 | Loss: 0.00151120
Iteration 14/25 | Loss: 0.00151428
Iteration 15/25 | Loss: 0.00150702
Iteration 16/25 | Loss: 0.00150622
Iteration 17/25 | Loss: 0.00150420
Iteration 18/25 | Loss: 0.00150199
Iteration 19/25 | Loss: 0.00149928
Iteration 20/25 | Loss: 0.00149998
Iteration 21/25 | Loss: 0.00149717
Iteration 22/25 | Loss: 0.00149482
Iteration 23/25 | Loss: 0.00148900
Iteration 24/25 | Loss: 0.00148630
Iteration 25/25 | Loss: 0.00148352

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.03019667
Iteration 2/25 | Loss: 0.00376517
Iteration 3/25 | Loss: 0.00372145
Iteration 4/25 | Loss: 0.00372143
Iteration 5/25 | Loss: 0.00372143
Iteration 6/25 | Loss: 0.00372143
Iteration 7/25 | Loss: 0.00372143
Iteration 8/25 | Loss: 0.00372143
Iteration 9/25 | Loss: 0.00372143
Iteration 10/25 | Loss: 0.00372143
Iteration 11/25 | Loss: 0.00372143
Iteration 12/25 | Loss: 0.00372143
Iteration 13/25 | Loss: 0.00372143
Iteration 14/25 | Loss: 0.00372143
Iteration 15/25 | Loss: 0.00372143
Iteration 16/25 | Loss: 0.00372143
Iteration 17/25 | Loss: 0.00372143
Iteration 18/25 | Loss: 0.00372143
Iteration 19/25 | Loss: 0.00372143
Iteration 20/25 | Loss: 0.00372143
Iteration 21/25 | Loss: 0.00372143
Iteration 22/25 | Loss: 0.00372143
Iteration 23/25 | Loss: 0.00372143
Iteration 24/25 | Loss: 0.00372143
Iteration 25/25 | Loss: 0.00372143

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00372143
Iteration 2/1000 | Loss: 0.00038485
Iteration 3/1000 | Loss: 0.00135053
Iteration 4/1000 | Loss: 0.00079782
Iteration 5/1000 | Loss: 0.00205748
Iteration 6/1000 | Loss: 0.00287861
Iteration 7/1000 | Loss: 0.00015944
Iteration 8/1000 | Loss: 0.00211211
Iteration 9/1000 | Loss: 0.00025503
Iteration 10/1000 | Loss: 0.00061377
Iteration 11/1000 | Loss: 0.00069957
Iteration 12/1000 | Loss: 0.00058453
Iteration 13/1000 | Loss: 0.00007765
Iteration 14/1000 | Loss: 0.00008650
Iteration 15/1000 | Loss: 0.00061267
Iteration 16/1000 | Loss: 0.00005486
Iteration 17/1000 | Loss: 0.00005865
Iteration 18/1000 | Loss: 0.00004991
Iteration 19/1000 | Loss: 0.00005257
Iteration 20/1000 | Loss: 0.00004294
Iteration 21/1000 | Loss: 0.00005382
Iteration 22/1000 | Loss: 0.00086651
Iteration 23/1000 | Loss: 0.00004225
Iteration 24/1000 | Loss: 0.00004054
Iteration 25/1000 | Loss: 0.00004507
Iteration 26/1000 | Loss: 0.00003294
Iteration 27/1000 | Loss: 0.00003667
Iteration 28/1000 | Loss: 0.00003895
Iteration 29/1000 | Loss: 0.00004142
Iteration 30/1000 | Loss: 0.00003475
Iteration 31/1000 | Loss: 0.00003117
Iteration 32/1000 | Loss: 0.00004833
Iteration 33/1000 | Loss: 0.00003977
Iteration 34/1000 | Loss: 0.00004551
Iteration 35/1000 | Loss: 0.00017986
Iteration 36/1000 | Loss: 0.00027164
Iteration 37/1000 | Loss: 0.00005291
Iteration 38/1000 | Loss: 0.00005269
Iteration 39/1000 | Loss: 0.00004687
Iteration 40/1000 | Loss: 0.00004888
Iteration 41/1000 | Loss: 0.00004325
Iteration 42/1000 | Loss: 0.00004252
Iteration 43/1000 | Loss: 0.00003998
Iteration 44/1000 | Loss: 0.00004311
Iteration 45/1000 | Loss: 0.00007970
Iteration 46/1000 | Loss: 0.00006552
Iteration 47/1000 | Loss: 0.00004039
Iteration 48/1000 | Loss: 0.00003820
Iteration 49/1000 | Loss: 0.00003997
Iteration 50/1000 | Loss: 0.00008342
Iteration 51/1000 | Loss: 0.00003924
Iteration 52/1000 | Loss: 0.00005190
Iteration 53/1000 | Loss: 0.00004080
Iteration 54/1000 | Loss: 0.00010117
Iteration 55/1000 | Loss: 0.00003799
Iteration 56/1000 | Loss: 0.00004854
Iteration 57/1000 | Loss: 0.00006611
Iteration 58/1000 | Loss: 0.00005446
Iteration 59/1000 | Loss: 0.00004051
Iteration 60/1000 | Loss: 0.00007219
Iteration 61/1000 | Loss: 0.00015323
Iteration 62/1000 | Loss: 0.00006158
Iteration 63/1000 | Loss: 0.00004413
Iteration 64/1000 | Loss: 0.00004060
Iteration 65/1000 | Loss: 0.00006874
Iteration 66/1000 | Loss: 0.00003329
Iteration 67/1000 | Loss: 0.00004255
Iteration 68/1000 | Loss: 0.00003642
Iteration 69/1000 | Loss: 0.00005994
Iteration 70/1000 | Loss: 0.00002935
Iteration 71/1000 | Loss: 0.00004050
Iteration 72/1000 | Loss: 0.00004395
Iteration 73/1000 | Loss: 0.00004279
Iteration 74/1000 | Loss: 0.00003859
Iteration 75/1000 | Loss: 0.00004201
Iteration 76/1000 | Loss: 0.00004022
Iteration 77/1000 | Loss: 0.00004481
Iteration 78/1000 | Loss: 0.00004288
Iteration 79/1000 | Loss: 0.00004065
Iteration 80/1000 | Loss: 0.00004569
Iteration 81/1000 | Loss: 0.00007743
Iteration 82/1000 | Loss: 0.00005721
Iteration 83/1000 | Loss: 0.00003000
Iteration 84/1000 | Loss: 0.00004114
Iteration 85/1000 | Loss: 0.00004601
Iteration 86/1000 | Loss: 0.00003516
Iteration 87/1000 | Loss: 0.00004016
Iteration 88/1000 | Loss: 0.00004658
Iteration 89/1000 | Loss: 0.00003734
Iteration 90/1000 | Loss: 0.00005655
Iteration 91/1000 | Loss: 0.00004432
Iteration 92/1000 | Loss: 0.00004148
Iteration 93/1000 | Loss: 0.00004862
Iteration 94/1000 | Loss: 0.00003701
Iteration 95/1000 | Loss: 0.00005394
Iteration 96/1000 | Loss: 0.00002874
Iteration 97/1000 | Loss: 0.00002717
Iteration 98/1000 | Loss: 0.00005910
Iteration 99/1000 | Loss: 0.00002606
Iteration 100/1000 | Loss: 0.00005435
Iteration 101/1000 | Loss: 0.00003465
Iteration 102/1000 | Loss: 0.00003444
Iteration 103/1000 | Loss: 0.00002505
Iteration 104/1000 | Loss: 0.00002500
Iteration 105/1000 | Loss: 0.00002494
Iteration 106/1000 | Loss: 0.00002487
Iteration 107/1000 | Loss: 0.00002466
Iteration 108/1000 | Loss: 0.00002453
Iteration 109/1000 | Loss: 0.00002450
Iteration 110/1000 | Loss: 0.00002420
Iteration 111/1000 | Loss: 0.00002396
Iteration 112/1000 | Loss: 0.00002370
Iteration 113/1000 | Loss: 0.00061820
Iteration 114/1000 | Loss: 0.00020827
Iteration 115/1000 | Loss: 0.00006593
Iteration 116/1000 | Loss: 0.00003062
Iteration 117/1000 | Loss: 0.00002509
Iteration 118/1000 | Loss: 0.00002297
Iteration 119/1000 | Loss: 0.00002225
Iteration 120/1000 | Loss: 0.00002174
Iteration 121/1000 | Loss: 0.00002133
Iteration 122/1000 | Loss: 0.00002092
Iteration 123/1000 | Loss: 0.00002077
Iteration 124/1000 | Loss: 0.00002072
Iteration 125/1000 | Loss: 0.00002070
Iteration 126/1000 | Loss: 0.00002069
Iteration 127/1000 | Loss: 0.00002069
Iteration 128/1000 | Loss: 0.00002068
Iteration 129/1000 | Loss: 0.00002068
Iteration 130/1000 | Loss: 0.00002067
Iteration 131/1000 | Loss: 0.00002067
Iteration 132/1000 | Loss: 0.00002066
Iteration 133/1000 | Loss: 0.00002066
Iteration 134/1000 | Loss: 0.00002065
Iteration 135/1000 | Loss: 0.00002064
Iteration 136/1000 | Loss: 0.00002063
Iteration 137/1000 | Loss: 0.00002063
Iteration 138/1000 | Loss: 0.00002062
Iteration 139/1000 | Loss: 0.00002061
Iteration 140/1000 | Loss: 0.00002060
Iteration 141/1000 | Loss: 0.00002060
Iteration 142/1000 | Loss: 0.00002059
Iteration 143/1000 | Loss: 0.00002059
Iteration 144/1000 | Loss: 0.00002058
Iteration 145/1000 | Loss: 0.00002058
Iteration 146/1000 | Loss: 0.00002058
Iteration 147/1000 | Loss: 0.00002058
Iteration 148/1000 | Loss: 0.00002058
Iteration 149/1000 | Loss: 0.00002058
Iteration 150/1000 | Loss: 0.00002058
Iteration 151/1000 | Loss: 0.00002058
Iteration 152/1000 | Loss: 0.00002056
Iteration 153/1000 | Loss: 0.00002056
Iteration 154/1000 | Loss: 0.00002056
Iteration 155/1000 | Loss: 0.00002055
Iteration 156/1000 | Loss: 0.00002055
Iteration 157/1000 | Loss: 0.00002054
Iteration 158/1000 | Loss: 0.00002053
Iteration 159/1000 | Loss: 0.00002053
Iteration 160/1000 | Loss: 0.00002053
Iteration 161/1000 | Loss: 0.00002052
Iteration 162/1000 | Loss: 0.00002052
Iteration 163/1000 | Loss: 0.00002052
Iteration 164/1000 | Loss: 0.00002052
Iteration 165/1000 | Loss: 0.00002052
Iteration 166/1000 | Loss: 0.00002052
Iteration 167/1000 | Loss: 0.00002052
Iteration 168/1000 | Loss: 0.00002052
Iteration 169/1000 | Loss: 0.00002052
Iteration 170/1000 | Loss: 0.00002052
Iteration 171/1000 | Loss: 0.00002051
Iteration 172/1000 | Loss: 0.00002051
Iteration 173/1000 | Loss: 0.00002051
Iteration 174/1000 | Loss: 0.00002051
Iteration 175/1000 | Loss: 0.00002051
Iteration 176/1000 | Loss: 0.00002051
Iteration 177/1000 | Loss: 0.00002050
Iteration 178/1000 | Loss: 0.00002050
Iteration 179/1000 | Loss: 0.00002050
Iteration 180/1000 | Loss: 0.00002050
Iteration 181/1000 | Loss: 0.00002050
Iteration 182/1000 | Loss: 0.00002050
Iteration 183/1000 | Loss: 0.00002050
Iteration 184/1000 | Loss: 0.00002050
Iteration 185/1000 | Loss: 0.00002050
Iteration 186/1000 | Loss: 0.00002050
Iteration 187/1000 | Loss: 0.00002050
Iteration 188/1000 | Loss: 0.00002050
Iteration 189/1000 | Loss: 0.00002049
Iteration 190/1000 | Loss: 0.00002049
Iteration 191/1000 | Loss: 0.00002049
Iteration 192/1000 | Loss: 0.00002049
Iteration 193/1000 | Loss: 0.00002049
Iteration 194/1000 | Loss: 0.00002048
Iteration 195/1000 | Loss: 0.00002048
Iteration 196/1000 | Loss: 0.00002048
Iteration 197/1000 | Loss: 0.00002048
Iteration 198/1000 | Loss: 0.00002048
Iteration 199/1000 | Loss: 0.00002048
Iteration 200/1000 | Loss: 0.00002048
Iteration 201/1000 | Loss: 0.00002048
Iteration 202/1000 | Loss: 0.00002048
Iteration 203/1000 | Loss: 0.00002048
Iteration 204/1000 | Loss: 0.00002047
Iteration 205/1000 | Loss: 0.00002047
Iteration 206/1000 | Loss: 0.00002047
Iteration 207/1000 | Loss: 0.00002047
Iteration 208/1000 | Loss: 0.00002047
Iteration 209/1000 | Loss: 0.00002047
Iteration 210/1000 | Loss: 0.00002047
Iteration 211/1000 | Loss: 0.00002047
Iteration 212/1000 | Loss: 0.00002047
Iteration 213/1000 | Loss: 0.00002047
Iteration 214/1000 | Loss: 0.00002047
Iteration 215/1000 | Loss: 0.00002046
Iteration 216/1000 | Loss: 0.00002046
Iteration 217/1000 | Loss: 0.00002046
Iteration 218/1000 | Loss: 0.00002046
Iteration 219/1000 | Loss: 0.00002046
Iteration 220/1000 | Loss: 0.00002046
Iteration 221/1000 | Loss: 0.00002046
Iteration 222/1000 | Loss: 0.00002046
Iteration 223/1000 | Loss: 0.00002046
Iteration 224/1000 | Loss: 0.00002046
Iteration 225/1000 | Loss: 0.00002046
Iteration 226/1000 | Loss: 0.00002046
Iteration 227/1000 | Loss: 0.00002046
Iteration 228/1000 | Loss: 0.00002046
Iteration 229/1000 | Loss: 0.00002045
Iteration 230/1000 | Loss: 0.00002045
Iteration 231/1000 | Loss: 0.00002045
Iteration 232/1000 | Loss: 0.00002045
Iteration 233/1000 | Loss: 0.00002045
Iteration 234/1000 | Loss: 0.00002045
Iteration 235/1000 | Loss: 0.00002045
Iteration 236/1000 | Loss: 0.00002045
Iteration 237/1000 | Loss: 0.00002045
Iteration 238/1000 | Loss: 0.00002045
Iteration 239/1000 | Loss: 0.00002045
Iteration 240/1000 | Loss: 0.00002045
Iteration 241/1000 | Loss: 0.00002045
Iteration 242/1000 | Loss: 0.00002045
Iteration 243/1000 | Loss: 0.00002045
Iteration 244/1000 | Loss: 0.00002045
Iteration 245/1000 | Loss: 0.00002045
Iteration 246/1000 | Loss: 0.00002045
Iteration 247/1000 | Loss: 0.00002045
Iteration 248/1000 | Loss: 0.00002045
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 248. Stopping optimization.
Last 5 losses: [2.0452112948987633e-05, 2.0452112948987633e-05, 2.0452112948987633e-05, 2.0452112948987633e-05, 2.0452112948987633e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0452112948987633e-05

Optimization complete. Final v2v error: 3.352159261703491 mm

Highest mean error: 12.660035133361816 mm for frame 75

Lowest mean error: 2.6950254440307617 mm for frame 112

Saving results

Total time: 224.6396837234497
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00957459
Iteration 2/25 | Loss: 0.00957458
Iteration 3/25 | Loss: 0.00957458
Iteration 4/25 | Loss: 0.00957458
Iteration 5/25 | Loss: 0.00957458
Iteration 6/25 | Loss: 0.00957458
Iteration 7/25 | Loss: 0.00957458
Iteration 8/25 | Loss: 0.00957458
Iteration 9/25 | Loss: 0.00957457
Iteration 10/25 | Loss: 0.00957457
Iteration 11/25 | Loss: 0.00957457
Iteration 12/25 | Loss: 0.00957457
Iteration 13/25 | Loss: 0.00957457
Iteration 14/25 | Loss: 0.00957457
Iteration 15/25 | Loss: 0.00957457
Iteration 16/25 | Loss: 0.00957456
Iteration 17/25 | Loss: 0.00957456
Iteration 18/25 | Loss: 0.00957456
Iteration 19/25 | Loss: 0.00957456
Iteration 20/25 | Loss: 0.00957455
Iteration 21/25 | Loss: 0.00957455
Iteration 22/25 | Loss: 0.00957455
Iteration 23/25 | Loss: 0.00957455
Iteration 24/25 | Loss: 0.00957455
Iteration 25/25 | Loss: 0.00957455

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37399960
Iteration 2/25 | Loss: 0.17943762
Iteration 3/25 | Loss: 0.17829449
Iteration 4/25 | Loss: 0.17847435
Iteration 5/25 | Loss: 0.17798027
Iteration 6/25 | Loss: 0.17792785
Iteration 7/25 | Loss: 0.17792785
Iteration 8/25 | Loss: 0.17792781
Iteration 9/25 | Loss: 0.17792781
Iteration 10/25 | Loss: 0.17792781
Iteration 11/25 | Loss: 0.17792781
Iteration 12/25 | Loss: 0.17792781
Iteration 13/25 | Loss: 0.17792781
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.17792780697345734, 0.17792780697345734, 0.17792780697345734, 0.17792780697345734, 0.17792780697345734]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.17792780697345734

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17792781
Iteration 2/1000 | Loss: 0.00198320
Iteration 3/1000 | Loss: 0.00043479
Iteration 4/1000 | Loss: 0.00017347
Iteration 5/1000 | Loss: 0.00007855
Iteration 6/1000 | Loss: 0.00005003
Iteration 7/1000 | Loss: 0.00021372
Iteration 8/1000 | Loss: 0.00002997
Iteration 9/1000 | Loss: 0.00005064
Iteration 10/1000 | Loss: 0.00128022
Iteration 11/1000 | Loss: 0.00053994
Iteration 12/1000 | Loss: 0.00037229
Iteration 13/1000 | Loss: 0.00007300
Iteration 14/1000 | Loss: 0.00072469
Iteration 15/1000 | Loss: 0.00070640
Iteration 16/1000 | Loss: 0.00002092
Iteration 17/1000 | Loss: 0.00007474
Iteration 18/1000 | Loss: 0.00001736
Iteration 19/1000 | Loss: 0.00004185
Iteration 20/1000 | Loss: 0.00212883
Iteration 21/1000 | Loss: 0.00032868
Iteration 22/1000 | Loss: 0.00006622
Iteration 23/1000 | Loss: 0.00007932
Iteration 24/1000 | Loss: 0.00010733
Iteration 25/1000 | Loss: 0.00075365
Iteration 26/1000 | Loss: 0.00010084
Iteration 27/1000 | Loss: 0.00002986
Iteration 28/1000 | Loss: 0.00007358
Iteration 29/1000 | Loss: 0.00003447
Iteration 30/1000 | Loss: 0.00023696
Iteration 31/1000 | Loss: 0.00022611
Iteration 32/1000 | Loss: 0.00094041
Iteration 33/1000 | Loss: 0.00135444
Iteration 34/1000 | Loss: 0.00011711
Iteration 35/1000 | Loss: 0.00011404
Iteration 36/1000 | Loss: 0.00001636
Iteration 37/1000 | Loss: 0.00005504
Iteration 38/1000 | Loss: 0.00001790
Iteration 39/1000 | Loss: 0.00001381
Iteration 40/1000 | Loss: 0.00001371
Iteration 41/1000 | Loss: 0.00001371
Iteration 42/1000 | Loss: 0.00001368
Iteration 43/1000 | Loss: 0.00001367
Iteration 44/1000 | Loss: 0.00001367
Iteration 45/1000 | Loss: 0.00001365
Iteration 46/1000 | Loss: 0.00001364
Iteration 47/1000 | Loss: 0.00001364
Iteration 48/1000 | Loss: 0.00001363
Iteration 49/1000 | Loss: 0.00001363
Iteration 50/1000 | Loss: 0.00003022
Iteration 51/1000 | Loss: 0.00003953
Iteration 52/1000 | Loss: 0.00001679
Iteration 53/1000 | Loss: 0.00001356
Iteration 54/1000 | Loss: 0.00002292
Iteration 55/1000 | Loss: 0.00001353
Iteration 56/1000 | Loss: 0.00001353
Iteration 57/1000 | Loss: 0.00001353
Iteration 58/1000 | Loss: 0.00001352
Iteration 59/1000 | Loss: 0.00001352
Iteration 60/1000 | Loss: 0.00001352
Iteration 61/1000 | Loss: 0.00001352
Iteration 62/1000 | Loss: 0.00001352
Iteration 63/1000 | Loss: 0.00001352
Iteration 64/1000 | Loss: 0.00001352
Iteration 65/1000 | Loss: 0.00001351
Iteration 66/1000 | Loss: 0.00001351
Iteration 67/1000 | Loss: 0.00001351
Iteration 68/1000 | Loss: 0.00001350
Iteration 69/1000 | Loss: 0.00001350
Iteration 70/1000 | Loss: 0.00001350
Iteration 71/1000 | Loss: 0.00001350
Iteration 72/1000 | Loss: 0.00001350
Iteration 73/1000 | Loss: 0.00001349
Iteration 74/1000 | Loss: 0.00001349
Iteration 75/1000 | Loss: 0.00001348
Iteration 76/1000 | Loss: 0.00001346
Iteration 77/1000 | Loss: 0.00001346
Iteration 78/1000 | Loss: 0.00001346
Iteration 79/1000 | Loss: 0.00001346
Iteration 80/1000 | Loss: 0.00001346
Iteration 81/1000 | Loss: 0.00001344
Iteration 82/1000 | Loss: 0.00001343
Iteration 83/1000 | Loss: 0.00003221
Iteration 84/1000 | Loss: 0.00001340
Iteration 85/1000 | Loss: 0.00001338
Iteration 86/1000 | Loss: 0.00001338
Iteration 87/1000 | Loss: 0.00001338
Iteration 88/1000 | Loss: 0.00001338
Iteration 89/1000 | Loss: 0.00001338
Iteration 90/1000 | Loss: 0.00001337
Iteration 91/1000 | Loss: 0.00001337
Iteration 92/1000 | Loss: 0.00003620
Iteration 93/1000 | Loss: 0.00016993
Iteration 94/1000 | Loss: 0.00001402
Iteration 95/1000 | Loss: 0.00006788
Iteration 96/1000 | Loss: 0.00001349
Iteration 97/1000 | Loss: 0.00002839
Iteration 98/1000 | Loss: 0.00001337
Iteration 99/1000 | Loss: 0.00007477
Iteration 100/1000 | Loss: 0.00004403
Iteration 101/1000 | Loss: 0.00001961
Iteration 102/1000 | Loss: 0.00001637
Iteration 103/1000 | Loss: 0.00021357
Iteration 104/1000 | Loss: 0.00014391
Iteration 105/1000 | Loss: 0.00010162
Iteration 106/1000 | Loss: 0.00001595
Iteration 107/1000 | Loss: 0.00002758
Iteration 108/1000 | Loss: 0.00020676
Iteration 109/1000 | Loss: 0.00003751
Iteration 110/1000 | Loss: 0.00002780
Iteration 111/1000 | Loss: 0.00008281
Iteration 112/1000 | Loss: 0.00001438
Iteration 113/1000 | Loss: 0.00003108
Iteration 114/1000 | Loss: 0.00002154
Iteration 115/1000 | Loss: 0.00001350
Iteration 116/1000 | Loss: 0.00001336
Iteration 117/1000 | Loss: 0.00001336
Iteration 118/1000 | Loss: 0.00001336
Iteration 119/1000 | Loss: 0.00001336
Iteration 120/1000 | Loss: 0.00001336
Iteration 121/1000 | Loss: 0.00001336
Iteration 122/1000 | Loss: 0.00001336
Iteration 123/1000 | Loss: 0.00001335
Iteration 124/1000 | Loss: 0.00001335
Iteration 125/1000 | Loss: 0.00001335
Iteration 126/1000 | Loss: 0.00001333
Iteration 127/1000 | Loss: 0.00001333
Iteration 128/1000 | Loss: 0.00001333
Iteration 129/1000 | Loss: 0.00001333
Iteration 130/1000 | Loss: 0.00001333
Iteration 131/1000 | Loss: 0.00001333
Iteration 132/1000 | Loss: 0.00001332
Iteration 133/1000 | Loss: 0.00001332
Iteration 134/1000 | Loss: 0.00001332
Iteration 135/1000 | Loss: 0.00001332
Iteration 136/1000 | Loss: 0.00001332
Iteration 137/1000 | Loss: 0.00001331
Iteration 138/1000 | Loss: 0.00001330
Iteration 139/1000 | Loss: 0.00002233
Iteration 140/1000 | Loss: 0.00001330
Iteration 141/1000 | Loss: 0.00001330
Iteration 142/1000 | Loss: 0.00001329
Iteration 143/1000 | Loss: 0.00001329
Iteration 144/1000 | Loss: 0.00001328
Iteration 145/1000 | Loss: 0.00001327
Iteration 146/1000 | Loss: 0.00001327
Iteration 147/1000 | Loss: 0.00001327
Iteration 148/1000 | Loss: 0.00001326
Iteration 149/1000 | Loss: 0.00001326
Iteration 150/1000 | Loss: 0.00001326
Iteration 151/1000 | Loss: 0.00001326
Iteration 152/1000 | Loss: 0.00001326
Iteration 153/1000 | Loss: 0.00001326
Iteration 154/1000 | Loss: 0.00001326
Iteration 155/1000 | Loss: 0.00001326
Iteration 156/1000 | Loss: 0.00001326
Iteration 157/1000 | Loss: 0.00001326
Iteration 158/1000 | Loss: 0.00001326
Iteration 159/1000 | Loss: 0.00001326
Iteration 160/1000 | Loss: 0.00001326
Iteration 161/1000 | Loss: 0.00001325
Iteration 162/1000 | Loss: 0.00001325
Iteration 163/1000 | Loss: 0.00001325
Iteration 164/1000 | Loss: 0.00001325
Iteration 165/1000 | Loss: 0.00001325
Iteration 166/1000 | Loss: 0.00001325
Iteration 167/1000 | Loss: 0.00001325
Iteration 168/1000 | Loss: 0.00001325
Iteration 169/1000 | Loss: 0.00001325
Iteration 170/1000 | Loss: 0.00001325
Iteration 171/1000 | Loss: 0.00001325
Iteration 172/1000 | Loss: 0.00001325
Iteration 173/1000 | Loss: 0.00001325
Iteration 174/1000 | Loss: 0.00001325
Iteration 175/1000 | Loss: 0.00001325
Iteration 176/1000 | Loss: 0.00001325
Iteration 177/1000 | Loss: 0.00001325
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [1.3246172784420196e-05, 1.3246172784420196e-05, 1.3246172784420196e-05, 1.3246172784420196e-05, 1.3246172784420196e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3246172784420196e-05

Optimization complete. Final v2v error: 3.109422445297241 mm

Highest mean error: 3.3913378715515137 mm for frame 235

Lowest mean error: 2.9058375358581543 mm for frame 156

Saving results

Total time: 124.8704845905304
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00798773
Iteration 2/25 | Loss: 0.00210016
Iteration 3/25 | Loss: 0.00163880
Iteration 4/25 | Loss: 0.00150286
Iteration 5/25 | Loss: 0.00145288
Iteration 6/25 | Loss: 0.00137955
Iteration 7/25 | Loss: 0.00136596
Iteration 8/25 | Loss: 0.00136181
Iteration 9/25 | Loss: 0.00135493
Iteration 10/25 | Loss: 0.00135445
Iteration 11/25 | Loss: 0.00135436
Iteration 12/25 | Loss: 0.00135436
Iteration 13/25 | Loss: 0.00135436
Iteration 14/25 | Loss: 0.00135436
Iteration 15/25 | Loss: 0.00135436
Iteration 16/25 | Loss: 0.00135436
Iteration 17/25 | Loss: 0.00135436
Iteration 18/25 | Loss: 0.00135436
Iteration 19/25 | Loss: 0.00135436
Iteration 20/25 | Loss: 0.00135435
Iteration 21/25 | Loss: 0.00135435
Iteration 22/25 | Loss: 0.00135435
Iteration 23/25 | Loss: 0.00135435
Iteration 24/25 | Loss: 0.00135435
Iteration 25/25 | Loss: 0.00135435

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28455424
Iteration 2/25 | Loss: 0.00079230
Iteration 3/25 | Loss: 0.00079227
Iteration 4/25 | Loss: 0.00079227
Iteration 5/25 | Loss: 0.00079227
Iteration 6/25 | Loss: 0.00079227
Iteration 7/25 | Loss: 0.00079227
Iteration 8/25 | Loss: 0.00079227
Iteration 9/25 | Loss: 0.00079227
Iteration 10/25 | Loss: 0.00079227
Iteration 11/25 | Loss: 0.00079227
Iteration 12/25 | Loss: 0.00079227
Iteration 13/25 | Loss: 0.00079227
Iteration 14/25 | Loss: 0.00079227
Iteration 15/25 | Loss: 0.00079227
Iteration 16/25 | Loss: 0.00079227
Iteration 17/25 | Loss: 0.00079227
Iteration 18/25 | Loss: 0.00079227
Iteration 19/25 | Loss: 0.00079227
Iteration 20/25 | Loss: 0.00079227
Iteration 21/25 | Loss: 0.00079227
Iteration 22/25 | Loss: 0.00079227
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007922705844976008, 0.0007922705844976008, 0.0007922705844976008, 0.0007922705844976008, 0.0007922705844976008]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007922705844976008

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079227
Iteration 2/1000 | Loss: 0.00003878
Iteration 3/1000 | Loss: 0.00002359
Iteration 4/1000 | Loss: 0.00002098
Iteration 5/1000 | Loss: 0.00002003
Iteration 6/1000 | Loss: 0.00001944
Iteration 7/1000 | Loss: 0.00001911
Iteration 8/1000 | Loss: 0.00001886
Iteration 9/1000 | Loss: 0.00001869
Iteration 10/1000 | Loss: 0.00001857
Iteration 11/1000 | Loss: 0.00001856
Iteration 12/1000 | Loss: 0.00001855
Iteration 13/1000 | Loss: 0.00001851
Iteration 14/1000 | Loss: 0.00001851
Iteration 15/1000 | Loss: 0.00001851
Iteration 16/1000 | Loss: 0.00001851
Iteration 17/1000 | Loss: 0.00001850
Iteration 18/1000 | Loss: 0.00001850
Iteration 19/1000 | Loss: 0.00001849
Iteration 20/1000 | Loss: 0.00001846
Iteration 21/1000 | Loss: 0.00001843
Iteration 22/1000 | Loss: 0.00001843
Iteration 23/1000 | Loss: 0.00001842
Iteration 24/1000 | Loss: 0.00001840
Iteration 25/1000 | Loss: 0.00001839
Iteration 26/1000 | Loss: 0.00001839
Iteration 27/1000 | Loss: 0.00001838
Iteration 28/1000 | Loss: 0.00001837
Iteration 29/1000 | Loss: 0.00001837
Iteration 30/1000 | Loss: 0.00001836
Iteration 31/1000 | Loss: 0.00001835
Iteration 32/1000 | Loss: 0.00001833
Iteration 33/1000 | Loss: 0.00001832
Iteration 34/1000 | Loss: 0.00001831
Iteration 35/1000 | Loss: 0.00001830
Iteration 36/1000 | Loss: 0.00001829
Iteration 37/1000 | Loss: 0.00001829
Iteration 38/1000 | Loss: 0.00001829
Iteration 39/1000 | Loss: 0.00001828
Iteration 40/1000 | Loss: 0.00001828
Iteration 41/1000 | Loss: 0.00001826
Iteration 42/1000 | Loss: 0.00001819
Iteration 43/1000 | Loss: 0.00001816
Iteration 44/1000 | Loss: 0.00001816
Iteration 45/1000 | Loss: 0.00001815
Iteration 46/1000 | Loss: 0.00001813
Iteration 47/1000 | Loss: 0.00001811
Iteration 48/1000 | Loss: 0.00001807
Iteration 49/1000 | Loss: 0.00001807
Iteration 50/1000 | Loss: 0.00001807
Iteration 51/1000 | Loss: 0.00001807
Iteration 52/1000 | Loss: 0.00001807
Iteration 53/1000 | Loss: 0.00001807
Iteration 54/1000 | Loss: 0.00001806
Iteration 55/1000 | Loss: 0.00001806
Iteration 56/1000 | Loss: 0.00001806
Iteration 57/1000 | Loss: 0.00001806
Iteration 58/1000 | Loss: 0.00001806
Iteration 59/1000 | Loss: 0.00001806
Iteration 60/1000 | Loss: 0.00001806
Iteration 61/1000 | Loss: 0.00001805
Iteration 62/1000 | Loss: 0.00001804
Iteration 63/1000 | Loss: 0.00001804
Iteration 64/1000 | Loss: 0.00001804
Iteration 65/1000 | Loss: 0.00001804
Iteration 66/1000 | Loss: 0.00001804
Iteration 67/1000 | Loss: 0.00001804
Iteration 68/1000 | Loss: 0.00001803
Iteration 69/1000 | Loss: 0.00001803
Iteration 70/1000 | Loss: 0.00001803
Iteration 71/1000 | Loss: 0.00001803
Iteration 72/1000 | Loss: 0.00001803
Iteration 73/1000 | Loss: 0.00001803
Iteration 74/1000 | Loss: 0.00001802
Iteration 75/1000 | Loss: 0.00001802
Iteration 76/1000 | Loss: 0.00001802
Iteration 77/1000 | Loss: 0.00001801
Iteration 78/1000 | Loss: 0.00001801
Iteration 79/1000 | Loss: 0.00001801
Iteration 80/1000 | Loss: 0.00001801
Iteration 81/1000 | Loss: 0.00001801
Iteration 82/1000 | Loss: 0.00001800
Iteration 83/1000 | Loss: 0.00001800
Iteration 84/1000 | Loss: 0.00001800
Iteration 85/1000 | Loss: 0.00001800
Iteration 86/1000 | Loss: 0.00001800
Iteration 87/1000 | Loss: 0.00001799
Iteration 88/1000 | Loss: 0.00001799
Iteration 89/1000 | Loss: 0.00001799
Iteration 90/1000 | Loss: 0.00001799
Iteration 91/1000 | Loss: 0.00001799
Iteration 92/1000 | Loss: 0.00001799
Iteration 93/1000 | Loss: 0.00001799
Iteration 94/1000 | Loss: 0.00001799
Iteration 95/1000 | Loss: 0.00001799
Iteration 96/1000 | Loss: 0.00001799
Iteration 97/1000 | Loss: 0.00001799
Iteration 98/1000 | Loss: 0.00001798
Iteration 99/1000 | Loss: 0.00001798
Iteration 100/1000 | Loss: 0.00001798
Iteration 101/1000 | Loss: 0.00001798
Iteration 102/1000 | Loss: 0.00001798
Iteration 103/1000 | Loss: 0.00001798
Iteration 104/1000 | Loss: 0.00001797
Iteration 105/1000 | Loss: 0.00001797
Iteration 106/1000 | Loss: 0.00001797
Iteration 107/1000 | Loss: 0.00001797
Iteration 108/1000 | Loss: 0.00001796
Iteration 109/1000 | Loss: 0.00001796
Iteration 110/1000 | Loss: 0.00001796
Iteration 111/1000 | Loss: 0.00001796
Iteration 112/1000 | Loss: 0.00001796
Iteration 113/1000 | Loss: 0.00001795
Iteration 114/1000 | Loss: 0.00001795
Iteration 115/1000 | Loss: 0.00001795
Iteration 116/1000 | Loss: 0.00001795
Iteration 117/1000 | Loss: 0.00001795
Iteration 118/1000 | Loss: 0.00001795
Iteration 119/1000 | Loss: 0.00001795
Iteration 120/1000 | Loss: 0.00001795
Iteration 121/1000 | Loss: 0.00001795
Iteration 122/1000 | Loss: 0.00001795
Iteration 123/1000 | Loss: 0.00001795
Iteration 124/1000 | Loss: 0.00001795
Iteration 125/1000 | Loss: 0.00001795
Iteration 126/1000 | Loss: 0.00001795
Iteration 127/1000 | Loss: 0.00001795
Iteration 128/1000 | Loss: 0.00001795
Iteration 129/1000 | Loss: 0.00001795
Iteration 130/1000 | Loss: 0.00001795
Iteration 131/1000 | Loss: 0.00001795
Iteration 132/1000 | Loss: 0.00001795
Iteration 133/1000 | Loss: 0.00001795
Iteration 134/1000 | Loss: 0.00001795
Iteration 135/1000 | Loss: 0.00001795
Iteration 136/1000 | Loss: 0.00001795
Iteration 137/1000 | Loss: 0.00001795
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.7949008906725794e-05, 1.7949008906725794e-05, 1.7949008906725794e-05, 1.7949008906725794e-05, 1.7949008906725794e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7949008906725794e-05

Optimization complete. Final v2v error: 3.5299088954925537 mm

Highest mean error: 3.843099355697632 mm for frame 9

Lowest mean error: 3.411860704421997 mm for frame 67

Saving results

Total time: 42.99378275871277
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00777356
Iteration 2/25 | Loss: 0.00153514
Iteration 3/25 | Loss: 0.00126741
Iteration 4/25 | Loss: 0.00124552
Iteration 5/25 | Loss: 0.00124280
Iteration 6/25 | Loss: 0.00124275
Iteration 7/25 | Loss: 0.00124275
Iteration 8/25 | Loss: 0.00124275
Iteration 9/25 | Loss: 0.00124275
Iteration 10/25 | Loss: 0.00124275
Iteration 11/25 | Loss: 0.00124275
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001242749858647585, 0.001242749858647585, 0.001242749858647585, 0.001242749858647585, 0.001242749858647585]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001242749858647585

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34514010
Iteration 2/25 | Loss: 0.00093394
Iteration 3/25 | Loss: 0.00093393
Iteration 4/25 | Loss: 0.00093393
Iteration 5/25 | Loss: 0.00093393
Iteration 6/25 | Loss: 0.00093393
Iteration 7/25 | Loss: 0.00093393
Iteration 8/25 | Loss: 0.00093393
Iteration 9/25 | Loss: 0.00093393
Iteration 10/25 | Loss: 0.00093393
Iteration 11/25 | Loss: 0.00093393
Iteration 12/25 | Loss: 0.00093393
Iteration 13/25 | Loss: 0.00093393
Iteration 14/25 | Loss: 0.00093393
Iteration 15/25 | Loss: 0.00093393
Iteration 16/25 | Loss: 0.00093393
Iteration 17/25 | Loss: 0.00093393
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009339286480098963, 0.0009339286480098963, 0.0009339286480098963, 0.0009339286480098963, 0.0009339286480098963]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009339286480098963

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093393
Iteration 2/1000 | Loss: 0.00003177
Iteration 3/1000 | Loss: 0.00002366
Iteration 4/1000 | Loss: 0.00001934
Iteration 5/1000 | Loss: 0.00001738
Iteration 6/1000 | Loss: 0.00001628
Iteration 7/1000 | Loss: 0.00001563
Iteration 8/1000 | Loss: 0.00001505
Iteration 9/1000 | Loss: 0.00001474
Iteration 10/1000 | Loss: 0.00001440
Iteration 11/1000 | Loss: 0.00001418
Iteration 12/1000 | Loss: 0.00001410
Iteration 13/1000 | Loss: 0.00001394
Iteration 14/1000 | Loss: 0.00001392
Iteration 15/1000 | Loss: 0.00001382
Iteration 16/1000 | Loss: 0.00001380
Iteration 17/1000 | Loss: 0.00001376
Iteration 18/1000 | Loss: 0.00001376
Iteration 19/1000 | Loss: 0.00001375
Iteration 20/1000 | Loss: 0.00001375
Iteration 21/1000 | Loss: 0.00001374
Iteration 22/1000 | Loss: 0.00001374
Iteration 23/1000 | Loss: 0.00001373
Iteration 24/1000 | Loss: 0.00001371
Iteration 25/1000 | Loss: 0.00001370
Iteration 26/1000 | Loss: 0.00001370
Iteration 27/1000 | Loss: 0.00001370
Iteration 28/1000 | Loss: 0.00001368
Iteration 29/1000 | Loss: 0.00001368
Iteration 30/1000 | Loss: 0.00001367
Iteration 31/1000 | Loss: 0.00001367
Iteration 32/1000 | Loss: 0.00001366
Iteration 33/1000 | Loss: 0.00001365
Iteration 34/1000 | Loss: 0.00001365
Iteration 35/1000 | Loss: 0.00001364
Iteration 36/1000 | Loss: 0.00001362
Iteration 37/1000 | Loss: 0.00001362
Iteration 38/1000 | Loss: 0.00001359
Iteration 39/1000 | Loss: 0.00001359
Iteration 40/1000 | Loss: 0.00001358
Iteration 41/1000 | Loss: 0.00001357
Iteration 42/1000 | Loss: 0.00001357
Iteration 43/1000 | Loss: 0.00001356
Iteration 44/1000 | Loss: 0.00001356
Iteration 45/1000 | Loss: 0.00001356
Iteration 46/1000 | Loss: 0.00001355
Iteration 47/1000 | Loss: 0.00001355
Iteration 48/1000 | Loss: 0.00001355
Iteration 49/1000 | Loss: 0.00001355
Iteration 50/1000 | Loss: 0.00001355
Iteration 51/1000 | Loss: 0.00001355
Iteration 52/1000 | Loss: 0.00001355
Iteration 53/1000 | Loss: 0.00001354
Iteration 54/1000 | Loss: 0.00001354
Iteration 55/1000 | Loss: 0.00001354
Iteration 56/1000 | Loss: 0.00001354
Iteration 57/1000 | Loss: 0.00001354
Iteration 58/1000 | Loss: 0.00001353
Iteration 59/1000 | Loss: 0.00001353
Iteration 60/1000 | Loss: 0.00001353
Iteration 61/1000 | Loss: 0.00001353
Iteration 62/1000 | Loss: 0.00001353
Iteration 63/1000 | Loss: 0.00001353
Iteration 64/1000 | Loss: 0.00001353
Iteration 65/1000 | Loss: 0.00001352
Iteration 66/1000 | Loss: 0.00001351
Iteration 67/1000 | Loss: 0.00001351
Iteration 68/1000 | Loss: 0.00001350
Iteration 69/1000 | Loss: 0.00001350
Iteration 70/1000 | Loss: 0.00001350
Iteration 71/1000 | Loss: 0.00001349
Iteration 72/1000 | Loss: 0.00001349
Iteration 73/1000 | Loss: 0.00001348
Iteration 74/1000 | Loss: 0.00001348
Iteration 75/1000 | Loss: 0.00001348
Iteration 76/1000 | Loss: 0.00001347
Iteration 77/1000 | Loss: 0.00001347
Iteration 78/1000 | Loss: 0.00001347
Iteration 79/1000 | Loss: 0.00001347
Iteration 80/1000 | Loss: 0.00001347
Iteration 81/1000 | Loss: 0.00001347
Iteration 82/1000 | Loss: 0.00001346
Iteration 83/1000 | Loss: 0.00001346
Iteration 84/1000 | Loss: 0.00001346
Iteration 85/1000 | Loss: 0.00001346
Iteration 86/1000 | Loss: 0.00001345
Iteration 87/1000 | Loss: 0.00001345
Iteration 88/1000 | Loss: 0.00001345
Iteration 89/1000 | Loss: 0.00001344
Iteration 90/1000 | Loss: 0.00001344
Iteration 91/1000 | Loss: 0.00001344
Iteration 92/1000 | Loss: 0.00001344
Iteration 93/1000 | Loss: 0.00001344
Iteration 94/1000 | Loss: 0.00001344
Iteration 95/1000 | Loss: 0.00001344
Iteration 96/1000 | Loss: 0.00001344
Iteration 97/1000 | Loss: 0.00001344
Iteration 98/1000 | Loss: 0.00001344
Iteration 99/1000 | Loss: 0.00001344
Iteration 100/1000 | Loss: 0.00001344
Iteration 101/1000 | Loss: 0.00001344
Iteration 102/1000 | Loss: 0.00001344
Iteration 103/1000 | Loss: 0.00001343
Iteration 104/1000 | Loss: 0.00001342
Iteration 105/1000 | Loss: 0.00001342
Iteration 106/1000 | Loss: 0.00001342
Iteration 107/1000 | Loss: 0.00001342
Iteration 108/1000 | Loss: 0.00001341
Iteration 109/1000 | Loss: 0.00001341
Iteration 110/1000 | Loss: 0.00001340
Iteration 111/1000 | Loss: 0.00001340
Iteration 112/1000 | Loss: 0.00001340
Iteration 113/1000 | Loss: 0.00001340
Iteration 114/1000 | Loss: 0.00001339
Iteration 115/1000 | Loss: 0.00001339
Iteration 116/1000 | Loss: 0.00001339
Iteration 117/1000 | Loss: 0.00001339
Iteration 118/1000 | Loss: 0.00001339
Iteration 119/1000 | Loss: 0.00001339
Iteration 120/1000 | Loss: 0.00001339
Iteration 121/1000 | Loss: 0.00001339
Iteration 122/1000 | Loss: 0.00001339
Iteration 123/1000 | Loss: 0.00001339
Iteration 124/1000 | Loss: 0.00001339
Iteration 125/1000 | Loss: 0.00001339
Iteration 126/1000 | Loss: 0.00001339
Iteration 127/1000 | Loss: 0.00001338
Iteration 128/1000 | Loss: 0.00001338
Iteration 129/1000 | Loss: 0.00001338
Iteration 130/1000 | Loss: 0.00001338
Iteration 131/1000 | Loss: 0.00001338
Iteration 132/1000 | Loss: 0.00001338
Iteration 133/1000 | Loss: 0.00001338
Iteration 134/1000 | Loss: 0.00001338
Iteration 135/1000 | Loss: 0.00001338
Iteration 136/1000 | Loss: 0.00001338
Iteration 137/1000 | Loss: 0.00001338
Iteration 138/1000 | Loss: 0.00001337
Iteration 139/1000 | Loss: 0.00001337
Iteration 140/1000 | Loss: 0.00001337
Iteration 141/1000 | Loss: 0.00001337
Iteration 142/1000 | Loss: 0.00001336
Iteration 143/1000 | Loss: 0.00001336
Iteration 144/1000 | Loss: 0.00001336
Iteration 145/1000 | Loss: 0.00001336
Iteration 146/1000 | Loss: 0.00001336
Iteration 147/1000 | Loss: 0.00001336
Iteration 148/1000 | Loss: 0.00001336
Iteration 149/1000 | Loss: 0.00001335
Iteration 150/1000 | Loss: 0.00001335
Iteration 151/1000 | Loss: 0.00001335
Iteration 152/1000 | Loss: 0.00001335
Iteration 153/1000 | Loss: 0.00001335
Iteration 154/1000 | Loss: 0.00001335
Iteration 155/1000 | Loss: 0.00001335
Iteration 156/1000 | Loss: 0.00001335
Iteration 157/1000 | Loss: 0.00001335
Iteration 158/1000 | Loss: 0.00001335
Iteration 159/1000 | Loss: 0.00001335
Iteration 160/1000 | Loss: 0.00001335
Iteration 161/1000 | Loss: 0.00001335
Iteration 162/1000 | Loss: 0.00001335
Iteration 163/1000 | Loss: 0.00001335
Iteration 164/1000 | Loss: 0.00001335
Iteration 165/1000 | Loss: 0.00001335
Iteration 166/1000 | Loss: 0.00001335
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [1.335483102593571e-05, 1.335483102593571e-05, 1.335483102593571e-05, 1.335483102593571e-05, 1.335483102593571e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.335483102593571e-05

Optimization complete. Final v2v error: 3.0807271003723145 mm

Highest mean error: 3.4282212257385254 mm for frame 123

Lowest mean error: 2.7962963581085205 mm for frame 78

Saving results

Total time: 38.815388202667236
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00802104
Iteration 2/25 | Loss: 0.00164040
Iteration 3/25 | Loss: 0.00138242
Iteration 4/25 | Loss: 0.00135262
Iteration 5/25 | Loss: 0.00134773
Iteration 6/25 | Loss: 0.00134615
Iteration 7/25 | Loss: 0.00134598
Iteration 8/25 | Loss: 0.00134598
Iteration 9/25 | Loss: 0.00134598
Iteration 10/25 | Loss: 0.00134598
Iteration 11/25 | Loss: 0.00134598
Iteration 12/25 | Loss: 0.00134598
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013459797482937574, 0.0013459797482937574, 0.0013459797482937574, 0.0013459797482937574, 0.0013459797482937574]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013459797482937574

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39560246
Iteration 2/25 | Loss: 0.00086267
Iteration 3/25 | Loss: 0.00086267
Iteration 4/25 | Loss: 0.00086266
Iteration 5/25 | Loss: 0.00086266
Iteration 6/25 | Loss: 0.00086266
Iteration 7/25 | Loss: 0.00086266
Iteration 8/25 | Loss: 0.00086266
Iteration 9/25 | Loss: 0.00086266
Iteration 10/25 | Loss: 0.00086266
Iteration 11/25 | Loss: 0.00086266
Iteration 12/25 | Loss: 0.00086266
Iteration 13/25 | Loss: 0.00086266
Iteration 14/25 | Loss: 0.00086266
Iteration 15/25 | Loss: 0.00086266
Iteration 16/25 | Loss: 0.00086266
Iteration 17/25 | Loss: 0.00086266
Iteration 18/25 | Loss: 0.00086266
Iteration 19/25 | Loss: 0.00086266
Iteration 20/25 | Loss: 0.00086266
Iteration 21/25 | Loss: 0.00086266
Iteration 22/25 | Loss: 0.00086266
Iteration 23/25 | Loss: 0.00086266
Iteration 24/25 | Loss: 0.00086266
Iteration 25/25 | Loss: 0.00086266

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086266
Iteration 2/1000 | Loss: 0.00005261
Iteration 3/1000 | Loss: 0.00003942
Iteration 4/1000 | Loss: 0.00003426
Iteration 5/1000 | Loss: 0.00003190
Iteration 6/1000 | Loss: 0.00003082
Iteration 7/1000 | Loss: 0.00002985
Iteration 8/1000 | Loss: 0.00002935
Iteration 9/1000 | Loss: 0.00002873
Iteration 10/1000 | Loss: 0.00002830
Iteration 11/1000 | Loss: 0.00002795
Iteration 12/1000 | Loss: 0.00002772
Iteration 13/1000 | Loss: 0.00002753
Iteration 14/1000 | Loss: 0.00002747
Iteration 15/1000 | Loss: 0.00002744
Iteration 16/1000 | Loss: 0.00002744
Iteration 17/1000 | Loss: 0.00002738
Iteration 18/1000 | Loss: 0.00002738
Iteration 19/1000 | Loss: 0.00002737
Iteration 20/1000 | Loss: 0.00002734
Iteration 21/1000 | Loss: 0.00002732
Iteration 22/1000 | Loss: 0.00002731
Iteration 23/1000 | Loss: 0.00002731
Iteration 24/1000 | Loss: 0.00002730
Iteration 25/1000 | Loss: 0.00002730
Iteration 26/1000 | Loss: 0.00002729
Iteration 27/1000 | Loss: 0.00002728
Iteration 28/1000 | Loss: 0.00002725
Iteration 29/1000 | Loss: 0.00002724
Iteration 30/1000 | Loss: 0.00002724
Iteration 31/1000 | Loss: 0.00002723
Iteration 32/1000 | Loss: 0.00002722
Iteration 33/1000 | Loss: 0.00002721
Iteration 34/1000 | Loss: 0.00002721
Iteration 35/1000 | Loss: 0.00002719
Iteration 36/1000 | Loss: 0.00002717
Iteration 37/1000 | Loss: 0.00002717
Iteration 38/1000 | Loss: 0.00002716
Iteration 39/1000 | Loss: 0.00002715
Iteration 40/1000 | Loss: 0.00002715
Iteration 41/1000 | Loss: 0.00002714
Iteration 42/1000 | Loss: 0.00002714
Iteration 43/1000 | Loss: 0.00002713
Iteration 44/1000 | Loss: 0.00002713
Iteration 45/1000 | Loss: 0.00002713
Iteration 46/1000 | Loss: 0.00002712
Iteration 47/1000 | Loss: 0.00002712
Iteration 48/1000 | Loss: 0.00002712
Iteration 49/1000 | Loss: 0.00002711
Iteration 50/1000 | Loss: 0.00002711
Iteration 51/1000 | Loss: 0.00002710
Iteration 52/1000 | Loss: 0.00002709
Iteration 53/1000 | Loss: 0.00002709
Iteration 54/1000 | Loss: 0.00002709
Iteration 55/1000 | Loss: 0.00002709
Iteration 56/1000 | Loss: 0.00002709
Iteration 57/1000 | Loss: 0.00002709
Iteration 58/1000 | Loss: 0.00002709
Iteration 59/1000 | Loss: 0.00002709
Iteration 60/1000 | Loss: 0.00002708
Iteration 61/1000 | Loss: 0.00002708
Iteration 62/1000 | Loss: 0.00002708
Iteration 63/1000 | Loss: 0.00002708
Iteration 64/1000 | Loss: 0.00002708
Iteration 65/1000 | Loss: 0.00002707
Iteration 66/1000 | Loss: 0.00002707
Iteration 67/1000 | Loss: 0.00002707
Iteration 68/1000 | Loss: 0.00002706
Iteration 69/1000 | Loss: 0.00002706
Iteration 70/1000 | Loss: 0.00002706
Iteration 71/1000 | Loss: 0.00002706
Iteration 72/1000 | Loss: 0.00002706
Iteration 73/1000 | Loss: 0.00002706
Iteration 74/1000 | Loss: 0.00002706
Iteration 75/1000 | Loss: 0.00002706
Iteration 76/1000 | Loss: 0.00002705
Iteration 77/1000 | Loss: 0.00002705
Iteration 78/1000 | Loss: 0.00002705
Iteration 79/1000 | Loss: 0.00002704
Iteration 80/1000 | Loss: 0.00002704
Iteration 81/1000 | Loss: 0.00002704
Iteration 82/1000 | Loss: 0.00002703
Iteration 83/1000 | Loss: 0.00002703
Iteration 84/1000 | Loss: 0.00002703
Iteration 85/1000 | Loss: 0.00002703
Iteration 86/1000 | Loss: 0.00002703
Iteration 87/1000 | Loss: 0.00002702
Iteration 88/1000 | Loss: 0.00002702
Iteration 89/1000 | Loss: 0.00002702
Iteration 90/1000 | Loss: 0.00002702
Iteration 91/1000 | Loss: 0.00002702
Iteration 92/1000 | Loss: 0.00002702
Iteration 93/1000 | Loss: 0.00002702
Iteration 94/1000 | Loss: 0.00002701
Iteration 95/1000 | Loss: 0.00002701
Iteration 96/1000 | Loss: 0.00002701
Iteration 97/1000 | Loss: 0.00002701
Iteration 98/1000 | Loss: 0.00002701
Iteration 99/1000 | Loss: 0.00002701
Iteration 100/1000 | Loss: 0.00002701
Iteration 101/1000 | Loss: 0.00002700
Iteration 102/1000 | Loss: 0.00002700
Iteration 103/1000 | Loss: 0.00002700
Iteration 104/1000 | Loss: 0.00002699
Iteration 105/1000 | Loss: 0.00002699
Iteration 106/1000 | Loss: 0.00002699
Iteration 107/1000 | Loss: 0.00002699
Iteration 108/1000 | Loss: 0.00002699
Iteration 109/1000 | Loss: 0.00002699
Iteration 110/1000 | Loss: 0.00002699
Iteration 111/1000 | Loss: 0.00002698
Iteration 112/1000 | Loss: 0.00002698
Iteration 113/1000 | Loss: 0.00002697
Iteration 114/1000 | Loss: 0.00002697
Iteration 115/1000 | Loss: 0.00002697
Iteration 116/1000 | Loss: 0.00002697
Iteration 117/1000 | Loss: 0.00002697
Iteration 118/1000 | Loss: 0.00002696
Iteration 119/1000 | Loss: 0.00002696
Iteration 120/1000 | Loss: 0.00002696
Iteration 121/1000 | Loss: 0.00002696
Iteration 122/1000 | Loss: 0.00002696
Iteration 123/1000 | Loss: 0.00002695
Iteration 124/1000 | Loss: 0.00002695
Iteration 125/1000 | Loss: 0.00002695
Iteration 126/1000 | Loss: 0.00002695
Iteration 127/1000 | Loss: 0.00002694
Iteration 128/1000 | Loss: 0.00002694
Iteration 129/1000 | Loss: 0.00002694
Iteration 130/1000 | Loss: 0.00002694
Iteration 131/1000 | Loss: 0.00002694
Iteration 132/1000 | Loss: 0.00002694
Iteration 133/1000 | Loss: 0.00002694
Iteration 134/1000 | Loss: 0.00002694
Iteration 135/1000 | Loss: 0.00002693
Iteration 136/1000 | Loss: 0.00002693
Iteration 137/1000 | Loss: 0.00002693
Iteration 138/1000 | Loss: 0.00002693
Iteration 139/1000 | Loss: 0.00002693
Iteration 140/1000 | Loss: 0.00002693
Iteration 141/1000 | Loss: 0.00002693
Iteration 142/1000 | Loss: 0.00002693
Iteration 143/1000 | Loss: 0.00002693
Iteration 144/1000 | Loss: 0.00002693
Iteration 145/1000 | Loss: 0.00002693
Iteration 146/1000 | Loss: 0.00002692
Iteration 147/1000 | Loss: 0.00002692
Iteration 148/1000 | Loss: 0.00002692
Iteration 149/1000 | Loss: 0.00002692
Iteration 150/1000 | Loss: 0.00002692
Iteration 151/1000 | Loss: 0.00002692
Iteration 152/1000 | Loss: 0.00002691
Iteration 153/1000 | Loss: 0.00002691
Iteration 154/1000 | Loss: 0.00002691
Iteration 155/1000 | Loss: 0.00002691
Iteration 156/1000 | Loss: 0.00002691
Iteration 157/1000 | Loss: 0.00002691
Iteration 158/1000 | Loss: 0.00002691
Iteration 159/1000 | Loss: 0.00002690
Iteration 160/1000 | Loss: 0.00002690
Iteration 161/1000 | Loss: 0.00002690
Iteration 162/1000 | Loss: 0.00002690
Iteration 163/1000 | Loss: 0.00002690
Iteration 164/1000 | Loss: 0.00002690
Iteration 165/1000 | Loss: 0.00002690
Iteration 166/1000 | Loss: 0.00002689
Iteration 167/1000 | Loss: 0.00002689
Iteration 168/1000 | Loss: 0.00002689
Iteration 169/1000 | Loss: 0.00002689
Iteration 170/1000 | Loss: 0.00002689
Iteration 171/1000 | Loss: 0.00002689
Iteration 172/1000 | Loss: 0.00002689
Iteration 173/1000 | Loss: 0.00002689
Iteration 174/1000 | Loss: 0.00002689
Iteration 175/1000 | Loss: 0.00002689
Iteration 176/1000 | Loss: 0.00002689
Iteration 177/1000 | Loss: 0.00002689
Iteration 178/1000 | Loss: 0.00002689
Iteration 179/1000 | Loss: 0.00002689
Iteration 180/1000 | Loss: 0.00002689
Iteration 181/1000 | Loss: 0.00002689
Iteration 182/1000 | Loss: 0.00002689
Iteration 183/1000 | Loss: 0.00002688
Iteration 184/1000 | Loss: 0.00002688
Iteration 185/1000 | Loss: 0.00002688
Iteration 186/1000 | Loss: 0.00002688
Iteration 187/1000 | Loss: 0.00002688
Iteration 188/1000 | Loss: 0.00002688
Iteration 189/1000 | Loss: 0.00002688
Iteration 190/1000 | Loss: 0.00002688
Iteration 191/1000 | Loss: 0.00002688
Iteration 192/1000 | Loss: 0.00002688
Iteration 193/1000 | Loss: 0.00002688
Iteration 194/1000 | Loss: 0.00002688
Iteration 195/1000 | Loss: 0.00002688
Iteration 196/1000 | Loss: 0.00002688
Iteration 197/1000 | Loss: 0.00002688
Iteration 198/1000 | Loss: 0.00002688
Iteration 199/1000 | Loss: 0.00002688
Iteration 200/1000 | Loss: 0.00002688
Iteration 201/1000 | Loss: 0.00002687
Iteration 202/1000 | Loss: 0.00002687
Iteration 203/1000 | Loss: 0.00002687
Iteration 204/1000 | Loss: 0.00002687
Iteration 205/1000 | Loss: 0.00002687
Iteration 206/1000 | Loss: 0.00002687
Iteration 207/1000 | Loss: 0.00002687
Iteration 208/1000 | Loss: 0.00002687
Iteration 209/1000 | Loss: 0.00002687
Iteration 210/1000 | Loss: 0.00002687
Iteration 211/1000 | Loss: 0.00002687
Iteration 212/1000 | Loss: 0.00002687
Iteration 213/1000 | Loss: 0.00002687
Iteration 214/1000 | Loss: 0.00002687
Iteration 215/1000 | Loss: 0.00002687
Iteration 216/1000 | Loss: 0.00002687
Iteration 217/1000 | Loss: 0.00002687
Iteration 218/1000 | Loss: 0.00002687
Iteration 219/1000 | Loss: 0.00002687
Iteration 220/1000 | Loss: 0.00002687
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [2.68705771304667e-05, 2.68705771304667e-05, 2.68705771304667e-05, 2.68705771304667e-05, 2.68705771304667e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.68705771304667e-05

Optimization complete. Final v2v error: 4.349469184875488 mm

Highest mean error: 5.136437892913818 mm for frame 36

Lowest mean error: 3.8714962005615234 mm for frame 14

Saving results

Total time: 41.63690710067749
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00616585
Iteration 2/25 | Loss: 0.00148935
Iteration 3/25 | Loss: 0.00131882
Iteration 4/25 | Loss: 0.00128839
Iteration 5/25 | Loss: 0.00127820
Iteration 6/25 | Loss: 0.00126347
Iteration 7/25 | Loss: 0.00126122
Iteration 8/25 | Loss: 0.00125928
Iteration 9/25 | Loss: 0.00125543
Iteration 10/25 | Loss: 0.00125181
Iteration 11/25 | Loss: 0.00124676
Iteration 12/25 | Loss: 0.00124187
Iteration 13/25 | Loss: 0.00124155
Iteration 14/25 | Loss: 0.00124146
Iteration 15/25 | Loss: 0.00124146
Iteration 16/25 | Loss: 0.00124146
Iteration 17/25 | Loss: 0.00124146
Iteration 18/25 | Loss: 0.00124146
Iteration 19/25 | Loss: 0.00124146
Iteration 20/25 | Loss: 0.00124146
Iteration 21/25 | Loss: 0.00124146
Iteration 22/25 | Loss: 0.00124146
Iteration 23/25 | Loss: 0.00124146
Iteration 24/25 | Loss: 0.00124146
Iteration 25/25 | Loss: 0.00124146

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53688812
Iteration 2/25 | Loss: 0.00121508
Iteration 3/25 | Loss: 0.00121507
Iteration 4/25 | Loss: 0.00121507
Iteration 5/25 | Loss: 0.00121507
Iteration 6/25 | Loss: 0.00121507
Iteration 7/25 | Loss: 0.00121507
Iteration 8/25 | Loss: 0.00121507
Iteration 9/25 | Loss: 0.00121507
Iteration 10/25 | Loss: 0.00121507
Iteration 11/25 | Loss: 0.00121507
Iteration 12/25 | Loss: 0.00121507
Iteration 13/25 | Loss: 0.00121507
Iteration 14/25 | Loss: 0.00121507
Iteration 15/25 | Loss: 0.00121507
Iteration 16/25 | Loss: 0.00121507
Iteration 17/25 | Loss: 0.00121507
Iteration 18/25 | Loss: 0.00121507
Iteration 19/25 | Loss: 0.00121507
Iteration 20/25 | Loss: 0.00121507
Iteration 21/25 | Loss: 0.00121507
Iteration 22/25 | Loss: 0.00121507
Iteration 23/25 | Loss: 0.00121507
Iteration 24/25 | Loss: 0.00121507
Iteration 25/25 | Loss: 0.00121507

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121507
Iteration 2/1000 | Loss: 0.00003700
Iteration 3/1000 | Loss: 0.00002516
Iteration 4/1000 | Loss: 0.00002238
Iteration 5/1000 | Loss: 0.00002099
Iteration 6/1000 | Loss: 0.00001981
Iteration 7/1000 | Loss: 0.00001903
Iteration 8/1000 | Loss: 0.00001856
Iteration 9/1000 | Loss: 0.00001826
Iteration 10/1000 | Loss: 0.00001787
Iteration 11/1000 | Loss: 0.00001764
Iteration 12/1000 | Loss: 0.00001753
Iteration 13/1000 | Loss: 0.00001748
Iteration 14/1000 | Loss: 0.00001745
Iteration 15/1000 | Loss: 0.00001744
Iteration 16/1000 | Loss: 0.00001739
Iteration 17/1000 | Loss: 0.00001737
Iteration 18/1000 | Loss: 0.00001730
Iteration 19/1000 | Loss: 0.00001729
Iteration 20/1000 | Loss: 0.00001728
Iteration 21/1000 | Loss: 0.00001724
Iteration 22/1000 | Loss: 0.00001723
Iteration 23/1000 | Loss: 0.00001722
Iteration 24/1000 | Loss: 0.00001722
Iteration 25/1000 | Loss: 0.00001721
Iteration 26/1000 | Loss: 0.00001719
Iteration 27/1000 | Loss: 0.00001719
Iteration 28/1000 | Loss: 0.00001719
Iteration 29/1000 | Loss: 0.00001718
Iteration 30/1000 | Loss: 0.00001718
Iteration 31/1000 | Loss: 0.00001717
Iteration 32/1000 | Loss: 0.00001717
Iteration 33/1000 | Loss: 0.00001717
Iteration 34/1000 | Loss: 0.00001717
Iteration 35/1000 | Loss: 0.00001717
Iteration 36/1000 | Loss: 0.00001717
Iteration 37/1000 | Loss: 0.00001717
Iteration 38/1000 | Loss: 0.00001717
Iteration 39/1000 | Loss: 0.00001716
Iteration 40/1000 | Loss: 0.00001716
Iteration 41/1000 | Loss: 0.00001716
Iteration 42/1000 | Loss: 0.00001716
Iteration 43/1000 | Loss: 0.00001714
Iteration 44/1000 | Loss: 0.00001714
Iteration 45/1000 | Loss: 0.00001713
Iteration 46/1000 | Loss: 0.00001713
Iteration 47/1000 | Loss: 0.00001713
Iteration 48/1000 | Loss: 0.00001713
Iteration 49/1000 | Loss: 0.00001713
Iteration 50/1000 | Loss: 0.00001713
Iteration 51/1000 | Loss: 0.00001712
Iteration 52/1000 | Loss: 0.00001712
Iteration 53/1000 | Loss: 0.00001712
Iteration 54/1000 | Loss: 0.00001711
Iteration 55/1000 | Loss: 0.00001711
Iteration 56/1000 | Loss: 0.00001710
Iteration 57/1000 | Loss: 0.00001710
Iteration 58/1000 | Loss: 0.00001710
Iteration 59/1000 | Loss: 0.00001709
Iteration 60/1000 | Loss: 0.00001709
Iteration 61/1000 | Loss: 0.00001709
Iteration 62/1000 | Loss: 0.00001708
Iteration 63/1000 | Loss: 0.00001708
Iteration 64/1000 | Loss: 0.00001708
Iteration 65/1000 | Loss: 0.00001707
Iteration 66/1000 | Loss: 0.00001707
Iteration 67/1000 | Loss: 0.00001706
Iteration 68/1000 | Loss: 0.00001706
Iteration 69/1000 | Loss: 0.00001706
Iteration 70/1000 | Loss: 0.00001706
Iteration 71/1000 | Loss: 0.00001705
Iteration 72/1000 | Loss: 0.00001705
Iteration 73/1000 | Loss: 0.00001705
Iteration 74/1000 | Loss: 0.00001705
Iteration 75/1000 | Loss: 0.00001704
Iteration 76/1000 | Loss: 0.00001704
Iteration 77/1000 | Loss: 0.00001704
Iteration 78/1000 | Loss: 0.00001704
Iteration 79/1000 | Loss: 0.00001703
Iteration 80/1000 | Loss: 0.00001703
Iteration 81/1000 | Loss: 0.00001703
Iteration 82/1000 | Loss: 0.00001703
Iteration 83/1000 | Loss: 0.00001703
Iteration 84/1000 | Loss: 0.00001703
Iteration 85/1000 | Loss: 0.00001703
Iteration 86/1000 | Loss: 0.00001702
Iteration 87/1000 | Loss: 0.00001702
Iteration 88/1000 | Loss: 0.00001702
Iteration 89/1000 | Loss: 0.00001702
Iteration 90/1000 | Loss: 0.00001702
Iteration 91/1000 | Loss: 0.00001702
Iteration 92/1000 | Loss: 0.00001701
Iteration 93/1000 | Loss: 0.00001701
Iteration 94/1000 | Loss: 0.00001701
Iteration 95/1000 | Loss: 0.00001701
Iteration 96/1000 | Loss: 0.00001701
Iteration 97/1000 | Loss: 0.00001701
Iteration 98/1000 | Loss: 0.00001701
Iteration 99/1000 | Loss: 0.00001701
Iteration 100/1000 | Loss: 0.00001701
Iteration 101/1000 | Loss: 0.00001701
Iteration 102/1000 | Loss: 0.00001700
Iteration 103/1000 | Loss: 0.00001700
Iteration 104/1000 | Loss: 0.00001700
Iteration 105/1000 | Loss: 0.00001699
Iteration 106/1000 | Loss: 0.00001699
Iteration 107/1000 | Loss: 0.00001699
Iteration 108/1000 | Loss: 0.00001699
Iteration 109/1000 | Loss: 0.00001699
Iteration 110/1000 | Loss: 0.00001699
Iteration 111/1000 | Loss: 0.00001699
Iteration 112/1000 | Loss: 0.00001699
Iteration 113/1000 | Loss: 0.00001699
Iteration 114/1000 | Loss: 0.00001699
Iteration 115/1000 | Loss: 0.00001699
Iteration 116/1000 | Loss: 0.00001699
Iteration 117/1000 | Loss: 0.00001699
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [1.6990234144032e-05, 1.6990234144032e-05, 1.6990234144032e-05, 1.6990234144032e-05, 1.6990234144032e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6990234144032e-05

Optimization complete. Final v2v error: 3.4217848777770996 mm

Highest mean error: 4.367231369018555 mm for frame 126

Lowest mean error: 2.849193572998047 mm for frame 48

Saving results

Total time: 57.02301549911499
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00460808
Iteration 2/25 | Loss: 0.00132382
Iteration 3/25 | Loss: 0.00123812
Iteration 4/25 | Loss: 0.00122634
Iteration 5/25 | Loss: 0.00122332
Iteration 6/25 | Loss: 0.00122332
Iteration 7/25 | Loss: 0.00122332
Iteration 8/25 | Loss: 0.00122332
Iteration 9/25 | Loss: 0.00122332
Iteration 10/25 | Loss: 0.00122332
Iteration 11/25 | Loss: 0.00122332
Iteration 12/25 | Loss: 0.00122332
Iteration 13/25 | Loss: 0.00122332
Iteration 14/25 | Loss: 0.00122332
Iteration 15/25 | Loss: 0.00122332
Iteration 16/25 | Loss: 0.00122332
Iteration 17/25 | Loss: 0.00122332
Iteration 18/25 | Loss: 0.00122332
Iteration 19/25 | Loss: 0.00122332
Iteration 20/25 | Loss: 0.00122332
Iteration 21/25 | Loss: 0.00122332
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012233232846483588, 0.0012233232846483588, 0.0012233232846483588, 0.0012233232846483588, 0.0012233232846483588]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012233232846483588

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.30267525
Iteration 2/25 | Loss: 0.00102102
Iteration 3/25 | Loss: 0.00102101
Iteration 4/25 | Loss: 0.00102101
Iteration 5/25 | Loss: 0.00102101
Iteration 6/25 | Loss: 0.00102101
Iteration 7/25 | Loss: 0.00102101
Iteration 8/25 | Loss: 0.00102101
Iteration 9/25 | Loss: 0.00102101
Iteration 10/25 | Loss: 0.00102101
Iteration 11/25 | Loss: 0.00102101
Iteration 12/25 | Loss: 0.00102101
Iteration 13/25 | Loss: 0.00102101
Iteration 14/25 | Loss: 0.00102101
Iteration 15/25 | Loss: 0.00102101
Iteration 16/25 | Loss: 0.00102101
Iteration 17/25 | Loss: 0.00102101
Iteration 18/25 | Loss: 0.00102101
Iteration 19/25 | Loss: 0.00102101
Iteration 20/25 | Loss: 0.00102101
Iteration 21/25 | Loss: 0.00102101
Iteration 22/25 | Loss: 0.00102101
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010210061445832253, 0.0010210061445832253, 0.0010210061445832253, 0.0010210061445832253, 0.0010210061445832253]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010210061445832253

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102101
Iteration 2/1000 | Loss: 0.00002560
Iteration 3/1000 | Loss: 0.00001797
Iteration 4/1000 | Loss: 0.00001547
Iteration 5/1000 | Loss: 0.00001444
Iteration 6/1000 | Loss: 0.00001384
Iteration 7/1000 | Loss: 0.00001345
Iteration 8/1000 | Loss: 0.00001317
Iteration 9/1000 | Loss: 0.00001284
Iteration 10/1000 | Loss: 0.00001262
Iteration 11/1000 | Loss: 0.00001250
Iteration 12/1000 | Loss: 0.00001242
Iteration 13/1000 | Loss: 0.00001241
Iteration 14/1000 | Loss: 0.00001240
Iteration 15/1000 | Loss: 0.00001240
Iteration 16/1000 | Loss: 0.00001233
Iteration 17/1000 | Loss: 0.00001231
Iteration 18/1000 | Loss: 0.00001227
Iteration 19/1000 | Loss: 0.00001226
Iteration 20/1000 | Loss: 0.00001224
Iteration 21/1000 | Loss: 0.00001223
Iteration 22/1000 | Loss: 0.00001222
Iteration 23/1000 | Loss: 0.00001221
Iteration 24/1000 | Loss: 0.00001216
Iteration 25/1000 | Loss: 0.00001209
Iteration 26/1000 | Loss: 0.00001205
Iteration 27/1000 | Loss: 0.00001204
Iteration 28/1000 | Loss: 0.00001203
Iteration 29/1000 | Loss: 0.00001203
Iteration 30/1000 | Loss: 0.00001201
Iteration 31/1000 | Loss: 0.00001200
Iteration 32/1000 | Loss: 0.00001199
Iteration 33/1000 | Loss: 0.00001199
Iteration 34/1000 | Loss: 0.00001198
Iteration 35/1000 | Loss: 0.00001197
Iteration 36/1000 | Loss: 0.00001197
Iteration 37/1000 | Loss: 0.00001196
Iteration 38/1000 | Loss: 0.00001196
Iteration 39/1000 | Loss: 0.00001196
Iteration 40/1000 | Loss: 0.00001195
Iteration 41/1000 | Loss: 0.00001195
Iteration 42/1000 | Loss: 0.00001195
Iteration 43/1000 | Loss: 0.00001194
Iteration 44/1000 | Loss: 0.00001194
Iteration 45/1000 | Loss: 0.00001193
Iteration 46/1000 | Loss: 0.00001193
Iteration 47/1000 | Loss: 0.00001192
Iteration 48/1000 | Loss: 0.00001192
Iteration 49/1000 | Loss: 0.00001192
Iteration 50/1000 | Loss: 0.00001191
Iteration 51/1000 | Loss: 0.00001190
Iteration 52/1000 | Loss: 0.00001190
Iteration 53/1000 | Loss: 0.00001189
Iteration 54/1000 | Loss: 0.00001189
Iteration 55/1000 | Loss: 0.00001189
Iteration 56/1000 | Loss: 0.00001189
Iteration 57/1000 | Loss: 0.00001189
Iteration 58/1000 | Loss: 0.00001188
Iteration 59/1000 | Loss: 0.00001188
Iteration 60/1000 | Loss: 0.00001188
Iteration 61/1000 | Loss: 0.00001188
Iteration 62/1000 | Loss: 0.00001187
Iteration 63/1000 | Loss: 0.00001186
Iteration 64/1000 | Loss: 0.00001185
Iteration 65/1000 | Loss: 0.00001185
Iteration 66/1000 | Loss: 0.00001184
Iteration 67/1000 | Loss: 0.00001184
Iteration 68/1000 | Loss: 0.00001184
Iteration 69/1000 | Loss: 0.00001184
Iteration 70/1000 | Loss: 0.00001184
Iteration 71/1000 | Loss: 0.00001184
Iteration 72/1000 | Loss: 0.00001183
Iteration 73/1000 | Loss: 0.00001183
Iteration 74/1000 | Loss: 0.00001183
Iteration 75/1000 | Loss: 0.00001182
Iteration 76/1000 | Loss: 0.00001182
Iteration 77/1000 | Loss: 0.00001182
Iteration 78/1000 | Loss: 0.00001181
Iteration 79/1000 | Loss: 0.00001181
Iteration 80/1000 | Loss: 0.00001181
Iteration 81/1000 | Loss: 0.00001180
Iteration 82/1000 | Loss: 0.00001180
Iteration 83/1000 | Loss: 0.00001180
Iteration 84/1000 | Loss: 0.00001179
Iteration 85/1000 | Loss: 0.00001179
Iteration 86/1000 | Loss: 0.00001179
Iteration 87/1000 | Loss: 0.00001179
Iteration 88/1000 | Loss: 0.00001179
Iteration 89/1000 | Loss: 0.00001179
Iteration 90/1000 | Loss: 0.00001179
Iteration 91/1000 | Loss: 0.00001179
Iteration 92/1000 | Loss: 0.00001179
Iteration 93/1000 | Loss: 0.00001179
Iteration 94/1000 | Loss: 0.00001179
Iteration 95/1000 | Loss: 0.00001179
Iteration 96/1000 | Loss: 0.00001179
Iteration 97/1000 | Loss: 0.00001179
Iteration 98/1000 | Loss: 0.00001179
Iteration 99/1000 | Loss: 0.00001179
Iteration 100/1000 | Loss: 0.00001179
Iteration 101/1000 | Loss: 0.00001179
Iteration 102/1000 | Loss: 0.00001179
Iteration 103/1000 | Loss: 0.00001179
Iteration 104/1000 | Loss: 0.00001179
Iteration 105/1000 | Loss: 0.00001179
Iteration 106/1000 | Loss: 0.00001179
Iteration 107/1000 | Loss: 0.00001179
Iteration 108/1000 | Loss: 0.00001179
Iteration 109/1000 | Loss: 0.00001179
Iteration 110/1000 | Loss: 0.00001179
Iteration 111/1000 | Loss: 0.00001179
Iteration 112/1000 | Loss: 0.00001179
Iteration 113/1000 | Loss: 0.00001179
Iteration 114/1000 | Loss: 0.00001179
Iteration 115/1000 | Loss: 0.00001179
Iteration 116/1000 | Loss: 0.00001179
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [1.1785136848629918e-05, 1.1785136848629918e-05, 1.1785136848629918e-05, 1.1785136848629918e-05, 1.1785136848629918e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1785136848629918e-05

Optimization complete. Final v2v error: 2.958585023880005 mm

Highest mean error: 3.301499128341675 mm for frame 184

Lowest mean error: 2.715489625930786 mm for frame 165

Saving results

Total time: 37.593639850616455
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01010632
Iteration 2/25 | Loss: 0.00273758
Iteration 3/25 | Loss: 0.00183153
Iteration 4/25 | Loss: 0.00169829
Iteration 5/25 | Loss: 0.00153614
Iteration 6/25 | Loss: 0.00146399
Iteration 7/25 | Loss: 0.00144427
Iteration 8/25 | Loss: 0.00144303
Iteration 9/25 | Loss: 0.00139977
Iteration 10/25 | Loss: 0.00135340
Iteration 11/25 | Loss: 0.00134443
Iteration 12/25 | Loss: 0.00134169
Iteration 13/25 | Loss: 0.00136734
Iteration 14/25 | Loss: 0.00143341
Iteration 15/25 | Loss: 0.00140388
Iteration 16/25 | Loss: 0.00137415
Iteration 17/25 | Loss: 0.00133900
Iteration 18/25 | Loss: 0.00130172
Iteration 19/25 | Loss: 0.00128433
Iteration 20/25 | Loss: 0.00127878
Iteration 21/25 | Loss: 0.00127100
Iteration 22/25 | Loss: 0.00127015
Iteration 23/25 | Loss: 0.00126964
Iteration 24/25 | Loss: 0.00126247
Iteration 25/25 | Loss: 0.00125589

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.40722895
Iteration 2/25 | Loss: 0.00148470
Iteration 3/25 | Loss: 0.00143563
Iteration 4/25 | Loss: 0.00143563
Iteration 5/25 | Loss: 0.00143563
Iteration 6/25 | Loss: 0.00143563
Iteration 7/25 | Loss: 0.00143563
Iteration 8/25 | Loss: 0.00143563
Iteration 9/25 | Loss: 0.00143563
Iteration 10/25 | Loss: 0.00143563
Iteration 11/25 | Loss: 0.00143563
Iteration 12/25 | Loss: 0.00143563
Iteration 13/25 | Loss: 0.00143563
Iteration 14/25 | Loss: 0.00143563
Iteration 15/25 | Loss: 0.00143563
Iteration 16/25 | Loss: 0.00143563
Iteration 17/25 | Loss: 0.00143563
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0014356289757415652, 0.0014356289757415652, 0.0014356289757415652, 0.0014356289757415652, 0.0014356289757415652]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014356289757415652

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00143563
Iteration 2/1000 | Loss: 0.00006843
Iteration 3/1000 | Loss: 0.00045754
Iteration 4/1000 | Loss: 0.00031146
Iteration 5/1000 | Loss: 0.00033721
Iteration 6/1000 | Loss: 0.00045792
Iteration 7/1000 | Loss: 0.00022378
Iteration 8/1000 | Loss: 0.00014433
Iteration 9/1000 | Loss: 0.00020543
Iteration 10/1000 | Loss: 0.00015536
Iteration 11/1000 | Loss: 0.00016516
Iteration 12/1000 | Loss: 0.00033857
Iteration 13/1000 | Loss: 0.00004732
Iteration 14/1000 | Loss: 0.00052493
Iteration 15/1000 | Loss: 0.00013227
Iteration 16/1000 | Loss: 0.00007517
Iteration 17/1000 | Loss: 0.00002771
Iteration 18/1000 | Loss: 0.00002843
Iteration 19/1000 | Loss: 0.00002451
Iteration 20/1000 | Loss: 0.00009547
Iteration 21/1000 | Loss: 0.00006913
Iteration 22/1000 | Loss: 0.00010214
Iteration 23/1000 | Loss: 0.00013142
Iteration 24/1000 | Loss: 0.00008940
Iteration 25/1000 | Loss: 0.00008887
Iteration 26/1000 | Loss: 0.00003181
Iteration 27/1000 | Loss: 0.00015595
Iteration 28/1000 | Loss: 0.00014789
Iteration 29/1000 | Loss: 0.00014490
Iteration 30/1000 | Loss: 0.00017206
Iteration 31/1000 | Loss: 0.00015734
Iteration 32/1000 | Loss: 0.00019177
Iteration 33/1000 | Loss: 0.00003755
Iteration 34/1000 | Loss: 0.00002226
Iteration 35/1000 | Loss: 0.00013186
Iteration 36/1000 | Loss: 0.00012022
Iteration 37/1000 | Loss: 0.00002105
Iteration 38/1000 | Loss: 0.00002056
Iteration 39/1000 | Loss: 0.00002744
Iteration 40/1000 | Loss: 0.00003380
Iteration 41/1000 | Loss: 0.00001923
Iteration 42/1000 | Loss: 0.00015896
Iteration 43/1000 | Loss: 0.00009270
Iteration 44/1000 | Loss: 0.00022932
Iteration 45/1000 | Loss: 0.00017603
Iteration 46/1000 | Loss: 0.00007306
Iteration 47/1000 | Loss: 0.00002027
Iteration 48/1000 | Loss: 0.00002528
Iteration 49/1000 | Loss: 0.00012105
Iteration 50/1000 | Loss: 0.00002158
Iteration 51/1000 | Loss: 0.00001844
Iteration 52/1000 | Loss: 0.00002158
Iteration 53/1000 | Loss: 0.00001843
Iteration 54/1000 | Loss: 0.00001893
Iteration 55/1000 | Loss: 0.00001824
Iteration 56/1000 | Loss: 0.00001823
Iteration 57/1000 | Loss: 0.00001822
Iteration 58/1000 | Loss: 0.00016182
Iteration 59/1000 | Loss: 0.00009566
Iteration 60/1000 | Loss: 0.00012071
Iteration 61/1000 | Loss: 0.00006782
Iteration 62/1000 | Loss: 0.00003744
Iteration 63/1000 | Loss: 0.00009394
Iteration 64/1000 | Loss: 0.00007195
Iteration 65/1000 | Loss: 0.00009730
Iteration 66/1000 | Loss: 0.00003747
Iteration 67/1000 | Loss: 0.00008638
Iteration 68/1000 | Loss: 0.00013766
Iteration 69/1000 | Loss: 0.00001871
Iteration 70/1000 | Loss: 0.00004568
Iteration 71/1000 | Loss: 0.00003911
Iteration 72/1000 | Loss: 0.00004062
Iteration 73/1000 | Loss: 0.00005483
Iteration 74/1000 | Loss: 0.00001858
Iteration 75/1000 | Loss: 0.00001765
Iteration 76/1000 | Loss: 0.00001754
Iteration 77/1000 | Loss: 0.00001753
Iteration 78/1000 | Loss: 0.00001752
Iteration 79/1000 | Loss: 0.00002571
Iteration 80/1000 | Loss: 0.00001757
Iteration 81/1000 | Loss: 0.00002241
Iteration 82/1000 | Loss: 0.00002241
Iteration 83/1000 | Loss: 0.00001912
Iteration 84/1000 | Loss: 0.00002258
Iteration 85/1000 | Loss: 0.00001839
Iteration 86/1000 | Loss: 0.00002050
Iteration 87/1000 | Loss: 0.00001719
Iteration 88/1000 | Loss: 0.00001719
Iteration 89/1000 | Loss: 0.00001719
Iteration 90/1000 | Loss: 0.00001719
Iteration 91/1000 | Loss: 0.00001718
Iteration 92/1000 | Loss: 0.00001711
Iteration 93/1000 | Loss: 0.00001711
Iteration 94/1000 | Loss: 0.00001750
Iteration 95/1000 | Loss: 0.00002631
Iteration 96/1000 | Loss: 0.00001702
Iteration 97/1000 | Loss: 0.00001700
Iteration 98/1000 | Loss: 0.00001699
Iteration 99/1000 | Loss: 0.00001698
Iteration 100/1000 | Loss: 0.00001695
Iteration 101/1000 | Loss: 0.00001693
Iteration 102/1000 | Loss: 0.00001693
Iteration 103/1000 | Loss: 0.00001692
Iteration 104/1000 | Loss: 0.00001692
Iteration 105/1000 | Loss: 0.00001692
Iteration 106/1000 | Loss: 0.00001692
Iteration 107/1000 | Loss: 0.00001692
Iteration 108/1000 | Loss: 0.00001881
Iteration 109/1000 | Loss: 0.00001745
Iteration 110/1000 | Loss: 0.00001718
Iteration 111/1000 | Loss: 0.00005627
Iteration 112/1000 | Loss: 0.00001689
Iteration 113/1000 | Loss: 0.00002270
Iteration 114/1000 | Loss: 0.00001685
Iteration 115/1000 | Loss: 0.00001685
Iteration 116/1000 | Loss: 0.00001685
Iteration 117/1000 | Loss: 0.00001685
Iteration 118/1000 | Loss: 0.00001684
Iteration 119/1000 | Loss: 0.00001684
Iteration 120/1000 | Loss: 0.00001684
Iteration 121/1000 | Loss: 0.00001684
Iteration 122/1000 | Loss: 0.00001684
Iteration 123/1000 | Loss: 0.00001684
Iteration 124/1000 | Loss: 0.00001684
Iteration 125/1000 | Loss: 0.00001684
Iteration 126/1000 | Loss: 0.00001684
Iteration 127/1000 | Loss: 0.00001682
Iteration 128/1000 | Loss: 0.00001682
Iteration 129/1000 | Loss: 0.00001682
Iteration 130/1000 | Loss: 0.00001681
Iteration 131/1000 | Loss: 0.00001681
Iteration 132/1000 | Loss: 0.00001681
Iteration 133/1000 | Loss: 0.00001680
Iteration 134/1000 | Loss: 0.00002754
Iteration 135/1000 | Loss: 0.00001677
Iteration 136/1000 | Loss: 0.00001677
Iteration 137/1000 | Loss: 0.00001677
Iteration 138/1000 | Loss: 0.00001677
Iteration 139/1000 | Loss: 0.00001677
Iteration 140/1000 | Loss: 0.00001677
Iteration 141/1000 | Loss: 0.00001676
Iteration 142/1000 | Loss: 0.00001676
Iteration 143/1000 | Loss: 0.00001676
Iteration 144/1000 | Loss: 0.00001676
Iteration 145/1000 | Loss: 0.00001676
Iteration 146/1000 | Loss: 0.00001676
Iteration 147/1000 | Loss: 0.00001676
Iteration 148/1000 | Loss: 0.00001676
Iteration 149/1000 | Loss: 0.00001675
Iteration 150/1000 | Loss: 0.00001675
Iteration 151/1000 | Loss: 0.00001675
Iteration 152/1000 | Loss: 0.00001675
Iteration 153/1000 | Loss: 0.00001674
Iteration 154/1000 | Loss: 0.00001674
Iteration 155/1000 | Loss: 0.00001674
Iteration 156/1000 | Loss: 0.00001674
Iteration 157/1000 | Loss: 0.00001674
Iteration 158/1000 | Loss: 0.00001673
Iteration 159/1000 | Loss: 0.00001673
Iteration 160/1000 | Loss: 0.00004308
Iteration 161/1000 | Loss: 0.00005594
Iteration 162/1000 | Loss: 0.00034632
Iteration 163/1000 | Loss: 0.00013280
Iteration 164/1000 | Loss: 0.00018572
Iteration 165/1000 | Loss: 0.00015328
Iteration 166/1000 | Loss: 0.00015244
Iteration 167/1000 | Loss: 0.00003782
Iteration 168/1000 | Loss: 0.00004174
Iteration 169/1000 | Loss: 0.00007265
Iteration 170/1000 | Loss: 0.00005186
Iteration 171/1000 | Loss: 0.00004649
Iteration 172/1000 | Loss: 0.00004134
Iteration 173/1000 | Loss: 0.00007345
Iteration 174/1000 | Loss: 0.00012634
Iteration 175/1000 | Loss: 0.00035953
Iteration 176/1000 | Loss: 0.00009274
Iteration 177/1000 | Loss: 0.00003772
Iteration 178/1000 | Loss: 0.00010109
Iteration 179/1000 | Loss: 0.00045967
Iteration 180/1000 | Loss: 0.00004757
Iteration 181/1000 | Loss: 0.00002292
Iteration 182/1000 | Loss: 0.00002018
Iteration 183/1000 | Loss: 0.00003893
Iteration 184/1000 | Loss: 0.00011075
Iteration 185/1000 | Loss: 0.00004250
Iteration 186/1000 | Loss: 0.00002229
Iteration 187/1000 | Loss: 0.00001809
Iteration 188/1000 | Loss: 0.00004540
Iteration 189/1000 | Loss: 0.00001728
Iteration 190/1000 | Loss: 0.00004064
Iteration 191/1000 | Loss: 0.00016689
Iteration 192/1000 | Loss: 0.00002289
Iteration 193/1000 | Loss: 0.00004110
Iteration 194/1000 | Loss: 0.00014483
Iteration 195/1000 | Loss: 0.00001949
Iteration 196/1000 | Loss: 0.00002016
Iteration 197/1000 | Loss: 0.00001801
Iteration 198/1000 | Loss: 0.00003889
Iteration 199/1000 | Loss: 0.00004514
Iteration 200/1000 | Loss: 0.00014011
Iteration 201/1000 | Loss: 0.00012805
Iteration 202/1000 | Loss: 0.00010416
Iteration 203/1000 | Loss: 0.00004381
Iteration 204/1000 | Loss: 0.00018214
Iteration 205/1000 | Loss: 0.00002795
Iteration 206/1000 | Loss: 0.00005907
Iteration 207/1000 | Loss: 0.00001906
Iteration 208/1000 | Loss: 0.00001761
Iteration 209/1000 | Loss: 0.00001761
Iteration 210/1000 | Loss: 0.00001761
Iteration 211/1000 | Loss: 0.00001761
Iteration 212/1000 | Loss: 0.00001761
Iteration 213/1000 | Loss: 0.00001761
Iteration 214/1000 | Loss: 0.00001761
Iteration 215/1000 | Loss: 0.00001761
Iteration 216/1000 | Loss: 0.00001761
Iteration 217/1000 | Loss: 0.00001760
Iteration 218/1000 | Loss: 0.00001760
Iteration 219/1000 | Loss: 0.00001760
Iteration 220/1000 | Loss: 0.00001760
Iteration 221/1000 | Loss: 0.00001760
Iteration 222/1000 | Loss: 0.00001760
Iteration 223/1000 | Loss: 0.00001760
Iteration 224/1000 | Loss: 0.00001760
Iteration 225/1000 | Loss: 0.00001759
Iteration 226/1000 | Loss: 0.00001758
Iteration 227/1000 | Loss: 0.00003433
Iteration 228/1000 | Loss: 0.00009282
Iteration 229/1000 | Loss: 0.00011072
Iteration 230/1000 | Loss: 0.00012521
Iteration 231/1000 | Loss: 0.00015063
Iteration 232/1000 | Loss: 0.00014192
Iteration 233/1000 | Loss: 0.00009320
Iteration 234/1000 | Loss: 0.00021699
Iteration 235/1000 | Loss: 0.00004573
Iteration 236/1000 | Loss: 0.00002854
Iteration 237/1000 | Loss: 0.00019483
Iteration 238/1000 | Loss: 0.00011315
Iteration 239/1000 | Loss: 0.00023153
Iteration 240/1000 | Loss: 0.00013295
Iteration 241/1000 | Loss: 0.00021639
Iteration 242/1000 | Loss: 0.00011780
Iteration 243/1000 | Loss: 0.00009701
Iteration 244/1000 | Loss: 0.00017897
Iteration 245/1000 | Loss: 0.00035156
Iteration 246/1000 | Loss: 0.00022412
Iteration 247/1000 | Loss: 0.00010397
Iteration 248/1000 | Loss: 0.00072349
Iteration 249/1000 | Loss: 0.00002115
Iteration 250/1000 | Loss: 0.00001876
Iteration 251/1000 | Loss: 0.00006446
Iteration 252/1000 | Loss: 0.00006464
Iteration 253/1000 | Loss: 0.00001776
Iteration 254/1000 | Loss: 0.00002904
Iteration 255/1000 | Loss: 0.00002539
Iteration 256/1000 | Loss: 0.00001665
Iteration 257/1000 | Loss: 0.00001665
Iteration 258/1000 | Loss: 0.00002656
Iteration 259/1000 | Loss: 0.00001625
Iteration 260/1000 | Loss: 0.00001616
Iteration 261/1000 | Loss: 0.00001616
Iteration 262/1000 | Loss: 0.00001644
Iteration 263/1000 | Loss: 0.00002430
Iteration 264/1000 | Loss: 0.00001608
Iteration 265/1000 | Loss: 0.00002742
Iteration 266/1000 | Loss: 0.00001602
Iteration 267/1000 | Loss: 0.00001601
Iteration 268/1000 | Loss: 0.00001601
Iteration 269/1000 | Loss: 0.00001601
Iteration 270/1000 | Loss: 0.00001601
Iteration 271/1000 | Loss: 0.00001601
Iteration 272/1000 | Loss: 0.00001601
Iteration 273/1000 | Loss: 0.00001601
Iteration 274/1000 | Loss: 0.00001601
Iteration 275/1000 | Loss: 0.00001600
Iteration 276/1000 | Loss: 0.00001600
Iteration 277/1000 | Loss: 0.00001600
Iteration 278/1000 | Loss: 0.00001600
Iteration 279/1000 | Loss: 0.00001600
Iteration 280/1000 | Loss: 0.00001600
Iteration 281/1000 | Loss: 0.00001599
Iteration 282/1000 | Loss: 0.00001599
Iteration 283/1000 | Loss: 0.00001599
Iteration 284/1000 | Loss: 0.00001599
Iteration 285/1000 | Loss: 0.00001599
Iteration 286/1000 | Loss: 0.00001598
Iteration 287/1000 | Loss: 0.00001598
Iteration 288/1000 | Loss: 0.00001598
Iteration 289/1000 | Loss: 0.00001598
Iteration 290/1000 | Loss: 0.00001598
Iteration 291/1000 | Loss: 0.00001598
Iteration 292/1000 | Loss: 0.00001598
Iteration 293/1000 | Loss: 0.00001598
Iteration 294/1000 | Loss: 0.00002308
Iteration 295/1000 | Loss: 0.00002438
Iteration 296/1000 | Loss: 0.00002248
Iteration 297/1000 | Loss: 0.00001599
Iteration 298/1000 | Loss: 0.00001598
Iteration 299/1000 | Loss: 0.00001598
Iteration 300/1000 | Loss: 0.00001597
Iteration 301/1000 | Loss: 0.00001596
Iteration 302/1000 | Loss: 0.00001596
Iteration 303/1000 | Loss: 0.00001596
Iteration 304/1000 | Loss: 0.00001596
Iteration 305/1000 | Loss: 0.00001596
Iteration 306/1000 | Loss: 0.00001596
Iteration 307/1000 | Loss: 0.00001595
Iteration 308/1000 | Loss: 0.00001595
Iteration 309/1000 | Loss: 0.00001595
Iteration 310/1000 | Loss: 0.00001595
Iteration 311/1000 | Loss: 0.00001595
Iteration 312/1000 | Loss: 0.00001595
Iteration 313/1000 | Loss: 0.00001595
Iteration 314/1000 | Loss: 0.00001595
Iteration 315/1000 | Loss: 0.00001595
Iteration 316/1000 | Loss: 0.00001595
Iteration 317/1000 | Loss: 0.00001595
Iteration 318/1000 | Loss: 0.00001595
Iteration 319/1000 | Loss: 0.00001595
Iteration 320/1000 | Loss: 0.00001595
Iteration 321/1000 | Loss: 0.00001595
Iteration 322/1000 | Loss: 0.00001595
Iteration 323/1000 | Loss: 0.00001595
Iteration 324/1000 | Loss: 0.00001595
Iteration 325/1000 | Loss: 0.00001595
Iteration 326/1000 | Loss: 0.00001595
Iteration 327/1000 | Loss: 0.00001595
Iteration 328/1000 | Loss: 0.00001595
Iteration 329/1000 | Loss: 0.00001595
Iteration 330/1000 | Loss: 0.00001595
Iteration 331/1000 | Loss: 0.00001595
Iteration 332/1000 | Loss: 0.00001595
Iteration 333/1000 | Loss: 0.00001595
Iteration 334/1000 | Loss: 0.00001595
Iteration 335/1000 | Loss: 0.00001595
Iteration 336/1000 | Loss: 0.00001595
Iteration 337/1000 | Loss: 0.00001595
Iteration 338/1000 | Loss: 0.00001595
Iteration 339/1000 | Loss: 0.00001595
Iteration 340/1000 | Loss: 0.00001595
Iteration 341/1000 | Loss: 0.00001595
Iteration 342/1000 | Loss: 0.00001595
Iteration 343/1000 | Loss: 0.00001595
Iteration 344/1000 | Loss: 0.00001595
Iteration 345/1000 | Loss: 0.00001595
Iteration 346/1000 | Loss: 0.00001595
Iteration 347/1000 | Loss: 0.00001595
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 347. Stopping optimization.
Last 5 losses: [1.59473238454666e-05, 1.59473238454666e-05, 1.59473238454666e-05, 1.59473238454666e-05, 1.59473238454666e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.59473238454666e-05

Optimization complete. Final v2v error: 3.313047409057617 mm

Highest mean error: 10.035138130187988 mm for frame 81

Lowest mean error: 2.5733914375305176 mm for frame 220

Saving results

Total time: 327.0174150466919
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00921427
Iteration 2/25 | Loss: 0.00172215
Iteration 3/25 | Loss: 0.00137201
Iteration 4/25 | Loss: 0.00134843
Iteration 5/25 | Loss: 0.00134367
Iteration 6/25 | Loss: 0.00134242
Iteration 7/25 | Loss: 0.00134242
Iteration 8/25 | Loss: 0.00134242
Iteration 9/25 | Loss: 0.00134242
Iteration 10/25 | Loss: 0.00134242
Iteration 11/25 | Loss: 0.00134242
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001342419651336968, 0.001342419651336968, 0.001342419651336968, 0.001342419651336968, 0.001342419651336968]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001342419651336968

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.95282006
Iteration 2/25 | Loss: 0.00099938
Iteration 3/25 | Loss: 0.00099937
Iteration 4/25 | Loss: 0.00099937
Iteration 5/25 | Loss: 0.00099937
Iteration 6/25 | Loss: 0.00099937
Iteration 7/25 | Loss: 0.00099937
Iteration 8/25 | Loss: 0.00099937
Iteration 9/25 | Loss: 0.00099937
Iteration 10/25 | Loss: 0.00099937
Iteration 11/25 | Loss: 0.00099937
Iteration 12/25 | Loss: 0.00099937
Iteration 13/25 | Loss: 0.00099937
Iteration 14/25 | Loss: 0.00099937
Iteration 15/25 | Loss: 0.00099937
Iteration 16/25 | Loss: 0.00099937
Iteration 17/25 | Loss: 0.00099937
Iteration 18/25 | Loss: 0.00099937
Iteration 19/25 | Loss: 0.00099937
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.000999369891360402, 0.000999369891360402, 0.000999369891360402, 0.000999369891360402, 0.000999369891360402]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000999369891360402

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099937
Iteration 2/1000 | Loss: 0.00004490
Iteration 3/1000 | Loss: 0.00003105
Iteration 4/1000 | Loss: 0.00002683
Iteration 5/1000 | Loss: 0.00002539
Iteration 6/1000 | Loss: 0.00002444
Iteration 7/1000 | Loss: 0.00002357
Iteration 8/1000 | Loss: 0.00002295
Iteration 9/1000 | Loss: 0.00002260
Iteration 10/1000 | Loss: 0.00002231
Iteration 11/1000 | Loss: 0.00002206
Iteration 12/1000 | Loss: 0.00002197
Iteration 13/1000 | Loss: 0.00002179
Iteration 14/1000 | Loss: 0.00002162
Iteration 15/1000 | Loss: 0.00002143
Iteration 16/1000 | Loss: 0.00002137
Iteration 17/1000 | Loss: 0.00002137
Iteration 18/1000 | Loss: 0.00002132
Iteration 19/1000 | Loss: 0.00002124
Iteration 20/1000 | Loss: 0.00002118
Iteration 21/1000 | Loss: 0.00002105
Iteration 22/1000 | Loss: 0.00002105
Iteration 23/1000 | Loss: 0.00002098
Iteration 24/1000 | Loss: 0.00002096
Iteration 25/1000 | Loss: 0.00002092
Iteration 26/1000 | Loss: 0.00002092
Iteration 27/1000 | Loss: 0.00002086
Iteration 28/1000 | Loss: 0.00002086
Iteration 29/1000 | Loss: 0.00002084
Iteration 30/1000 | Loss: 0.00002084
Iteration 31/1000 | Loss: 0.00002083
Iteration 32/1000 | Loss: 0.00002082
Iteration 33/1000 | Loss: 0.00002082
Iteration 34/1000 | Loss: 0.00002081
Iteration 35/1000 | Loss: 0.00002081
Iteration 36/1000 | Loss: 0.00002080
Iteration 37/1000 | Loss: 0.00002080
Iteration 38/1000 | Loss: 0.00002080
Iteration 39/1000 | Loss: 0.00002079
Iteration 40/1000 | Loss: 0.00002079
Iteration 41/1000 | Loss: 0.00002079
Iteration 42/1000 | Loss: 0.00002077
Iteration 43/1000 | Loss: 0.00002077
Iteration 44/1000 | Loss: 0.00002077
Iteration 45/1000 | Loss: 0.00002077
Iteration 46/1000 | Loss: 0.00002076
Iteration 47/1000 | Loss: 0.00002076
Iteration 48/1000 | Loss: 0.00002076
Iteration 49/1000 | Loss: 0.00002076
Iteration 50/1000 | Loss: 0.00002076
Iteration 51/1000 | Loss: 0.00002076
Iteration 52/1000 | Loss: 0.00002076
Iteration 53/1000 | Loss: 0.00002076
Iteration 54/1000 | Loss: 0.00002075
Iteration 55/1000 | Loss: 0.00002075
Iteration 56/1000 | Loss: 0.00002074
Iteration 57/1000 | Loss: 0.00002074
Iteration 58/1000 | Loss: 0.00002074
Iteration 59/1000 | Loss: 0.00002074
Iteration 60/1000 | Loss: 0.00002073
Iteration 61/1000 | Loss: 0.00002073
Iteration 62/1000 | Loss: 0.00002073
Iteration 63/1000 | Loss: 0.00002073
Iteration 64/1000 | Loss: 0.00002073
Iteration 65/1000 | Loss: 0.00002073
Iteration 66/1000 | Loss: 0.00002073
Iteration 67/1000 | Loss: 0.00002072
Iteration 68/1000 | Loss: 0.00002072
Iteration 69/1000 | Loss: 0.00002072
Iteration 70/1000 | Loss: 0.00002071
Iteration 71/1000 | Loss: 0.00002071
Iteration 72/1000 | Loss: 0.00002071
Iteration 73/1000 | Loss: 0.00002071
Iteration 74/1000 | Loss: 0.00002071
Iteration 75/1000 | Loss: 0.00002071
Iteration 76/1000 | Loss: 0.00002071
Iteration 77/1000 | Loss: 0.00002070
Iteration 78/1000 | Loss: 0.00002070
Iteration 79/1000 | Loss: 0.00002069
Iteration 80/1000 | Loss: 0.00002069
Iteration 81/1000 | Loss: 0.00002069
Iteration 82/1000 | Loss: 0.00002069
Iteration 83/1000 | Loss: 0.00002069
Iteration 84/1000 | Loss: 0.00002068
Iteration 85/1000 | Loss: 0.00002068
Iteration 86/1000 | Loss: 0.00002068
Iteration 87/1000 | Loss: 0.00002068
Iteration 88/1000 | Loss: 0.00002068
Iteration 89/1000 | Loss: 0.00002068
Iteration 90/1000 | Loss: 0.00002068
Iteration 91/1000 | Loss: 0.00002068
Iteration 92/1000 | Loss: 0.00002068
Iteration 93/1000 | Loss: 0.00002068
Iteration 94/1000 | Loss: 0.00002068
Iteration 95/1000 | Loss: 0.00002068
Iteration 96/1000 | Loss: 0.00002067
Iteration 97/1000 | Loss: 0.00002067
Iteration 98/1000 | Loss: 0.00002067
Iteration 99/1000 | Loss: 0.00002067
Iteration 100/1000 | Loss: 0.00002067
Iteration 101/1000 | Loss: 0.00002066
Iteration 102/1000 | Loss: 0.00002066
Iteration 103/1000 | Loss: 0.00002066
Iteration 104/1000 | Loss: 0.00002066
Iteration 105/1000 | Loss: 0.00002066
Iteration 106/1000 | Loss: 0.00002066
Iteration 107/1000 | Loss: 0.00002065
Iteration 108/1000 | Loss: 0.00002065
Iteration 109/1000 | Loss: 0.00002065
Iteration 110/1000 | Loss: 0.00002065
Iteration 111/1000 | Loss: 0.00002065
Iteration 112/1000 | Loss: 0.00002065
Iteration 113/1000 | Loss: 0.00002065
Iteration 114/1000 | Loss: 0.00002064
Iteration 115/1000 | Loss: 0.00002064
Iteration 116/1000 | Loss: 0.00002064
Iteration 117/1000 | Loss: 0.00002064
Iteration 118/1000 | Loss: 0.00002064
Iteration 119/1000 | Loss: 0.00002064
Iteration 120/1000 | Loss: 0.00002064
Iteration 121/1000 | Loss: 0.00002064
Iteration 122/1000 | Loss: 0.00002064
Iteration 123/1000 | Loss: 0.00002064
Iteration 124/1000 | Loss: 0.00002063
Iteration 125/1000 | Loss: 0.00002063
Iteration 126/1000 | Loss: 0.00002063
Iteration 127/1000 | Loss: 0.00002063
Iteration 128/1000 | Loss: 0.00002063
Iteration 129/1000 | Loss: 0.00002063
Iteration 130/1000 | Loss: 0.00002063
Iteration 131/1000 | Loss: 0.00002063
Iteration 132/1000 | Loss: 0.00002063
Iteration 133/1000 | Loss: 0.00002063
Iteration 134/1000 | Loss: 0.00002063
Iteration 135/1000 | Loss: 0.00002063
Iteration 136/1000 | Loss: 0.00002063
Iteration 137/1000 | Loss: 0.00002063
Iteration 138/1000 | Loss: 0.00002062
Iteration 139/1000 | Loss: 0.00002062
Iteration 140/1000 | Loss: 0.00002062
Iteration 141/1000 | Loss: 0.00002062
Iteration 142/1000 | Loss: 0.00002062
Iteration 143/1000 | Loss: 0.00002062
Iteration 144/1000 | Loss: 0.00002062
Iteration 145/1000 | Loss: 0.00002062
Iteration 146/1000 | Loss: 0.00002062
Iteration 147/1000 | Loss: 0.00002062
Iteration 148/1000 | Loss: 0.00002062
Iteration 149/1000 | Loss: 0.00002062
Iteration 150/1000 | Loss: 0.00002062
Iteration 151/1000 | Loss: 0.00002062
Iteration 152/1000 | Loss: 0.00002062
Iteration 153/1000 | Loss: 0.00002062
Iteration 154/1000 | Loss: 0.00002062
Iteration 155/1000 | Loss: 0.00002062
Iteration 156/1000 | Loss: 0.00002062
Iteration 157/1000 | Loss: 0.00002062
Iteration 158/1000 | Loss: 0.00002062
Iteration 159/1000 | Loss: 0.00002062
Iteration 160/1000 | Loss: 0.00002062
Iteration 161/1000 | Loss: 0.00002062
Iteration 162/1000 | Loss: 0.00002062
Iteration 163/1000 | Loss: 0.00002062
Iteration 164/1000 | Loss: 0.00002062
Iteration 165/1000 | Loss: 0.00002062
Iteration 166/1000 | Loss: 0.00002062
Iteration 167/1000 | Loss: 0.00002062
Iteration 168/1000 | Loss: 0.00002062
Iteration 169/1000 | Loss: 0.00002062
Iteration 170/1000 | Loss: 0.00002062
Iteration 171/1000 | Loss: 0.00002062
Iteration 172/1000 | Loss: 0.00002062
Iteration 173/1000 | Loss: 0.00002062
Iteration 174/1000 | Loss: 0.00002062
Iteration 175/1000 | Loss: 0.00002062
Iteration 176/1000 | Loss: 0.00002062
Iteration 177/1000 | Loss: 0.00002062
Iteration 178/1000 | Loss: 0.00002062
Iteration 179/1000 | Loss: 0.00002062
Iteration 180/1000 | Loss: 0.00002062
Iteration 181/1000 | Loss: 0.00002062
Iteration 182/1000 | Loss: 0.00002062
Iteration 183/1000 | Loss: 0.00002062
Iteration 184/1000 | Loss: 0.00002062
Iteration 185/1000 | Loss: 0.00002062
Iteration 186/1000 | Loss: 0.00002062
Iteration 187/1000 | Loss: 0.00002062
Iteration 188/1000 | Loss: 0.00002062
Iteration 189/1000 | Loss: 0.00002062
Iteration 190/1000 | Loss: 0.00002062
Iteration 191/1000 | Loss: 0.00002062
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [2.0621861040126532e-05, 2.0621861040126532e-05, 2.0621861040126532e-05, 2.0621861040126532e-05, 2.0621861040126532e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0621861040126532e-05

Optimization complete. Final v2v error: 3.8075854778289795 mm

Highest mean error: 4.345672130584717 mm for frame 136

Lowest mean error: 3.0950825214385986 mm for frame 26

Saving results

Total time: 44.270421743392944
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00512712
Iteration 2/25 | Loss: 0.00128110
Iteration 3/25 | Loss: 0.00122271
Iteration 4/25 | Loss: 0.00121328
Iteration 5/25 | Loss: 0.00121114
Iteration 6/25 | Loss: 0.00121114
Iteration 7/25 | Loss: 0.00121114
Iteration 8/25 | Loss: 0.00121114
Iteration 9/25 | Loss: 0.00121114
Iteration 10/25 | Loss: 0.00121114
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012111380929127336, 0.0012111380929127336, 0.0012111380929127336, 0.0012111380929127336, 0.0012111380929127336]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012111380929127336

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.37859201
Iteration 2/25 | Loss: 0.00100267
Iteration 3/25 | Loss: 0.00100267
Iteration 4/25 | Loss: 0.00100266
Iteration 5/25 | Loss: 0.00100266
Iteration 6/25 | Loss: 0.00100266
Iteration 7/25 | Loss: 0.00100266
Iteration 8/25 | Loss: 0.00100266
Iteration 9/25 | Loss: 0.00100266
Iteration 10/25 | Loss: 0.00100266
Iteration 11/25 | Loss: 0.00100266
Iteration 12/25 | Loss: 0.00100266
Iteration 13/25 | Loss: 0.00100266
Iteration 14/25 | Loss: 0.00100266
Iteration 15/25 | Loss: 0.00100266
Iteration 16/25 | Loss: 0.00100266
Iteration 17/25 | Loss: 0.00100266
Iteration 18/25 | Loss: 0.00100266
Iteration 19/25 | Loss: 0.00100266
Iteration 20/25 | Loss: 0.00100266
Iteration 21/25 | Loss: 0.00100266
Iteration 22/25 | Loss: 0.00100266
Iteration 23/25 | Loss: 0.00100266
Iteration 24/25 | Loss: 0.00100266
Iteration 25/25 | Loss: 0.00100266

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100266
Iteration 2/1000 | Loss: 0.00002677
Iteration 3/1000 | Loss: 0.00001879
Iteration 4/1000 | Loss: 0.00001665
Iteration 5/1000 | Loss: 0.00001551
Iteration 6/1000 | Loss: 0.00001477
Iteration 7/1000 | Loss: 0.00001445
Iteration 8/1000 | Loss: 0.00001417
Iteration 9/1000 | Loss: 0.00001369
Iteration 10/1000 | Loss: 0.00001341
Iteration 11/1000 | Loss: 0.00001341
Iteration 12/1000 | Loss: 0.00001323
Iteration 13/1000 | Loss: 0.00001320
Iteration 14/1000 | Loss: 0.00001310
Iteration 15/1000 | Loss: 0.00001300
Iteration 16/1000 | Loss: 0.00001290
Iteration 17/1000 | Loss: 0.00001282
Iteration 18/1000 | Loss: 0.00001268
Iteration 19/1000 | Loss: 0.00001267
Iteration 20/1000 | Loss: 0.00001263
Iteration 21/1000 | Loss: 0.00001262
Iteration 22/1000 | Loss: 0.00001262
Iteration 23/1000 | Loss: 0.00001253
Iteration 24/1000 | Loss: 0.00001248
Iteration 25/1000 | Loss: 0.00001247
Iteration 26/1000 | Loss: 0.00001247
Iteration 27/1000 | Loss: 0.00001247
Iteration 28/1000 | Loss: 0.00001245
Iteration 29/1000 | Loss: 0.00001244
Iteration 30/1000 | Loss: 0.00001239
Iteration 31/1000 | Loss: 0.00001236
Iteration 32/1000 | Loss: 0.00001235
Iteration 33/1000 | Loss: 0.00001235
Iteration 34/1000 | Loss: 0.00001234
Iteration 35/1000 | Loss: 0.00001233
Iteration 36/1000 | Loss: 0.00001232
Iteration 37/1000 | Loss: 0.00001229
Iteration 38/1000 | Loss: 0.00001227
Iteration 39/1000 | Loss: 0.00001226
Iteration 40/1000 | Loss: 0.00001221
Iteration 41/1000 | Loss: 0.00001216
Iteration 42/1000 | Loss: 0.00001214
Iteration 43/1000 | Loss: 0.00001210
Iteration 44/1000 | Loss: 0.00001209
Iteration 45/1000 | Loss: 0.00001209
Iteration 46/1000 | Loss: 0.00001209
Iteration 47/1000 | Loss: 0.00001209
Iteration 48/1000 | Loss: 0.00001209
Iteration 49/1000 | Loss: 0.00001209
Iteration 50/1000 | Loss: 0.00001209
Iteration 51/1000 | Loss: 0.00001209
Iteration 52/1000 | Loss: 0.00001209
Iteration 53/1000 | Loss: 0.00001209
Iteration 54/1000 | Loss: 0.00001208
Iteration 55/1000 | Loss: 0.00001208
Iteration 56/1000 | Loss: 0.00001206
Iteration 57/1000 | Loss: 0.00001206
Iteration 58/1000 | Loss: 0.00001206
Iteration 59/1000 | Loss: 0.00001205
Iteration 60/1000 | Loss: 0.00001205
Iteration 61/1000 | Loss: 0.00001205
Iteration 62/1000 | Loss: 0.00001205
Iteration 63/1000 | Loss: 0.00001205
Iteration 64/1000 | Loss: 0.00001205
Iteration 65/1000 | Loss: 0.00001205
Iteration 66/1000 | Loss: 0.00001204
Iteration 67/1000 | Loss: 0.00001204
Iteration 68/1000 | Loss: 0.00001204
Iteration 69/1000 | Loss: 0.00001203
Iteration 70/1000 | Loss: 0.00001203
Iteration 71/1000 | Loss: 0.00001203
Iteration 72/1000 | Loss: 0.00001203
Iteration 73/1000 | Loss: 0.00001203
Iteration 74/1000 | Loss: 0.00001203
Iteration 75/1000 | Loss: 0.00001203
Iteration 76/1000 | Loss: 0.00001203
Iteration 77/1000 | Loss: 0.00001203
Iteration 78/1000 | Loss: 0.00001203
Iteration 79/1000 | Loss: 0.00001202
Iteration 80/1000 | Loss: 0.00001202
Iteration 81/1000 | Loss: 0.00001202
Iteration 82/1000 | Loss: 0.00001202
Iteration 83/1000 | Loss: 0.00001202
Iteration 84/1000 | Loss: 0.00001202
Iteration 85/1000 | Loss: 0.00001202
Iteration 86/1000 | Loss: 0.00001201
Iteration 87/1000 | Loss: 0.00001201
Iteration 88/1000 | Loss: 0.00001201
Iteration 89/1000 | Loss: 0.00001201
Iteration 90/1000 | Loss: 0.00001201
Iteration 91/1000 | Loss: 0.00001201
Iteration 92/1000 | Loss: 0.00001201
Iteration 93/1000 | Loss: 0.00001200
Iteration 94/1000 | Loss: 0.00001200
Iteration 95/1000 | Loss: 0.00001200
Iteration 96/1000 | Loss: 0.00001200
Iteration 97/1000 | Loss: 0.00001200
Iteration 98/1000 | Loss: 0.00001200
Iteration 99/1000 | Loss: 0.00001200
Iteration 100/1000 | Loss: 0.00001200
Iteration 101/1000 | Loss: 0.00001200
Iteration 102/1000 | Loss: 0.00001200
Iteration 103/1000 | Loss: 0.00001199
Iteration 104/1000 | Loss: 0.00001199
Iteration 105/1000 | Loss: 0.00001199
Iteration 106/1000 | Loss: 0.00001199
Iteration 107/1000 | Loss: 0.00001199
Iteration 108/1000 | Loss: 0.00001199
Iteration 109/1000 | Loss: 0.00001199
Iteration 110/1000 | Loss: 0.00001198
Iteration 111/1000 | Loss: 0.00001198
Iteration 112/1000 | Loss: 0.00001198
Iteration 113/1000 | Loss: 0.00001198
Iteration 114/1000 | Loss: 0.00001198
Iteration 115/1000 | Loss: 0.00001198
Iteration 116/1000 | Loss: 0.00001198
Iteration 117/1000 | Loss: 0.00001198
Iteration 118/1000 | Loss: 0.00001198
Iteration 119/1000 | Loss: 0.00001198
Iteration 120/1000 | Loss: 0.00001198
Iteration 121/1000 | Loss: 0.00001198
Iteration 122/1000 | Loss: 0.00001198
Iteration 123/1000 | Loss: 0.00001198
Iteration 124/1000 | Loss: 0.00001197
Iteration 125/1000 | Loss: 0.00001197
Iteration 126/1000 | Loss: 0.00001197
Iteration 127/1000 | Loss: 0.00001197
Iteration 128/1000 | Loss: 0.00001197
Iteration 129/1000 | Loss: 0.00001197
Iteration 130/1000 | Loss: 0.00001197
Iteration 131/1000 | Loss: 0.00001197
Iteration 132/1000 | Loss: 0.00001197
Iteration 133/1000 | Loss: 0.00001197
Iteration 134/1000 | Loss: 0.00001196
Iteration 135/1000 | Loss: 0.00001196
Iteration 136/1000 | Loss: 0.00001196
Iteration 137/1000 | Loss: 0.00001196
Iteration 138/1000 | Loss: 0.00001196
Iteration 139/1000 | Loss: 0.00001196
Iteration 140/1000 | Loss: 0.00001196
Iteration 141/1000 | Loss: 0.00001196
Iteration 142/1000 | Loss: 0.00001196
Iteration 143/1000 | Loss: 0.00001196
Iteration 144/1000 | Loss: 0.00001196
Iteration 145/1000 | Loss: 0.00001196
Iteration 146/1000 | Loss: 0.00001196
Iteration 147/1000 | Loss: 0.00001196
Iteration 148/1000 | Loss: 0.00001196
Iteration 149/1000 | Loss: 0.00001196
Iteration 150/1000 | Loss: 0.00001196
Iteration 151/1000 | Loss: 0.00001196
Iteration 152/1000 | Loss: 0.00001196
Iteration 153/1000 | Loss: 0.00001196
Iteration 154/1000 | Loss: 0.00001196
Iteration 155/1000 | Loss: 0.00001196
Iteration 156/1000 | Loss: 0.00001196
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.1960686606471427e-05, 1.1960686606471427e-05, 1.1960686606471427e-05, 1.1960686606471427e-05, 1.1960686606471427e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1960686606471427e-05

Optimization complete. Final v2v error: 3.0005276203155518 mm

Highest mean error: 3.2466180324554443 mm for frame 234

Lowest mean error: 2.762180805206299 mm for frame 13

Saving results

Total time: 48.447784185409546
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00488264
Iteration 2/25 | Loss: 0.00137574
Iteration 3/25 | Loss: 0.00128168
Iteration 4/25 | Loss: 0.00127089
Iteration 5/25 | Loss: 0.00126779
Iteration 6/25 | Loss: 0.00126779
Iteration 7/25 | Loss: 0.00126779
Iteration 8/25 | Loss: 0.00126779
Iteration 9/25 | Loss: 0.00126779
Iteration 10/25 | Loss: 0.00126779
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012677948689088225, 0.0012677948689088225, 0.0012677948689088225, 0.0012677948689088225, 0.0012677948689088225]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012677948689088225

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33725250
Iteration 2/25 | Loss: 0.00108526
Iteration 3/25 | Loss: 0.00108526
Iteration 4/25 | Loss: 0.00108526
Iteration 5/25 | Loss: 0.00108526
Iteration 6/25 | Loss: 0.00108526
Iteration 7/25 | Loss: 0.00108525
Iteration 8/25 | Loss: 0.00108525
Iteration 9/25 | Loss: 0.00108525
Iteration 10/25 | Loss: 0.00108525
Iteration 11/25 | Loss: 0.00108525
Iteration 12/25 | Loss: 0.00108525
Iteration 13/25 | Loss: 0.00108525
Iteration 14/25 | Loss: 0.00108525
Iteration 15/25 | Loss: 0.00108525
Iteration 16/25 | Loss: 0.00108525
Iteration 17/25 | Loss: 0.00108525
Iteration 18/25 | Loss: 0.00108525
Iteration 19/25 | Loss: 0.00108525
Iteration 20/25 | Loss: 0.00108525
Iteration 21/25 | Loss: 0.00108525
Iteration 22/25 | Loss: 0.00108525
Iteration 23/25 | Loss: 0.00108525
Iteration 24/25 | Loss: 0.00108525
Iteration 25/25 | Loss: 0.00108525

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108525
Iteration 2/1000 | Loss: 0.00004092
Iteration 3/1000 | Loss: 0.00003069
Iteration 4/1000 | Loss: 0.00002664
Iteration 5/1000 | Loss: 0.00002520
Iteration 6/1000 | Loss: 0.00002458
Iteration 7/1000 | Loss: 0.00002397
Iteration 8/1000 | Loss: 0.00002345
Iteration 9/1000 | Loss: 0.00002307
Iteration 10/1000 | Loss: 0.00002275
Iteration 11/1000 | Loss: 0.00002267
Iteration 12/1000 | Loss: 0.00002243
Iteration 13/1000 | Loss: 0.00002236
Iteration 14/1000 | Loss: 0.00002230
Iteration 15/1000 | Loss: 0.00002220
Iteration 16/1000 | Loss: 0.00002212
Iteration 17/1000 | Loss: 0.00002209
Iteration 18/1000 | Loss: 0.00002208
Iteration 19/1000 | Loss: 0.00002208
Iteration 20/1000 | Loss: 0.00002205
Iteration 21/1000 | Loss: 0.00002205
Iteration 22/1000 | Loss: 0.00002204
Iteration 23/1000 | Loss: 0.00002204
Iteration 24/1000 | Loss: 0.00002203
Iteration 25/1000 | Loss: 0.00002202
Iteration 26/1000 | Loss: 0.00002202
Iteration 27/1000 | Loss: 0.00002201
Iteration 28/1000 | Loss: 0.00002198
Iteration 29/1000 | Loss: 0.00002198
Iteration 30/1000 | Loss: 0.00002197
Iteration 31/1000 | Loss: 0.00002195
Iteration 32/1000 | Loss: 0.00002194
Iteration 33/1000 | Loss: 0.00002193
Iteration 34/1000 | Loss: 0.00002192
Iteration 35/1000 | Loss: 0.00002191
Iteration 36/1000 | Loss: 0.00002191
Iteration 37/1000 | Loss: 0.00002190
Iteration 38/1000 | Loss: 0.00002189
Iteration 39/1000 | Loss: 0.00002188
Iteration 40/1000 | Loss: 0.00002180
Iteration 41/1000 | Loss: 0.00002174
Iteration 42/1000 | Loss: 0.00002174
Iteration 43/1000 | Loss: 0.00002172
Iteration 44/1000 | Loss: 0.00002172
Iteration 45/1000 | Loss: 0.00002172
Iteration 46/1000 | Loss: 0.00002172
Iteration 47/1000 | Loss: 0.00002171
Iteration 48/1000 | Loss: 0.00002171
Iteration 49/1000 | Loss: 0.00002171
Iteration 50/1000 | Loss: 0.00002170
Iteration 51/1000 | Loss: 0.00002170
Iteration 52/1000 | Loss: 0.00002168
Iteration 53/1000 | Loss: 0.00002168
Iteration 54/1000 | Loss: 0.00002167
Iteration 55/1000 | Loss: 0.00002166
Iteration 56/1000 | Loss: 0.00002166
Iteration 57/1000 | Loss: 0.00002166
Iteration 58/1000 | Loss: 0.00002166
Iteration 59/1000 | Loss: 0.00002166
Iteration 60/1000 | Loss: 0.00002166
Iteration 61/1000 | Loss: 0.00002166
Iteration 62/1000 | Loss: 0.00002166
Iteration 63/1000 | Loss: 0.00002166
Iteration 64/1000 | Loss: 0.00002165
Iteration 65/1000 | Loss: 0.00002165
Iteration 66/1000 | Loss: 0.00002163
Iteration 67/1000 | Loss: 0.00002162
Iteration 68/1000 | Loss: 0.00002162
Iteration 69/1000 | Loss: 0.00002162
Iteration 70/1000 | Loss: 0.00002162
Iteration 71/1000 | Loss: 0.00002161
Iteration 72/1000 | Loss: 0.00002161
Iteration 73/1000 | Loss: 0.00002161
Iteration 74/1000 | Loss: 0.00002161
Iteration 75/1000 | Loss: 0.00002161
Iteration 76/1000 | Loss: 0.00002160
Iteration 77/1000 | Loss: 0.00002160
Iteration 78/1000 | Loss: 0.00002160
Iteration 79/1000 | Loss: 0.00002160
Iteration 80/1000 | Loss: 0.00002160
Iteration 81/1000 | Loss: 0.00002159
Iteration 82/1000 | Loss: 0.00002159
Iteration 83/1000 | Loss: 0.00002159
Iteration 84/1000 | Loss: 0.00002159
Iteration 85/1000 | Loss: 0.00002159
Iteration 86/1000 | Loss: 0.00002159
Iteration 87/1000 | Loss: 0.00002158
Iteration 88/1000 | Loss: 0.00002158
Iteration 89/1000 | Loss: 0.00002158
Iteration 90/1000 | Loss: 0.00002158
Iteration 91/1000 | Loss: 0.00002158
Iteration 92/1000 | Loss: 0.00002157
Iteration 93/1000 | Loss: 0.00002157
Iteration 94/1000 | Loss: 0.00002156
Iteration 95/1000 | Loss: 0.00002156
Iteration 96/1000 | Loss: 0.00002156
Iteration 97/1000 | Loss: 0.00002155
Iteration 98/1000 | Loss: 0.00002155
Iteration 99/1000 | Loss: 0.00002155
Iteration 100/1000 | Loss: 0.00002154
Iteration 101/1000 | Loss: 0.00002154
Iteration 102/1000 | Loss: 0.00002154
Iteration 103/1000 | Loss: 0.00002154
Iteration 104/1000 | Loss: 0.00002154
Iteration 105/1000 | Loss: 0.00002154
Iteration 106/1000 | Loss: 0.00002153
Iteration 107/1000 | Loss: 0.00002153
Iteration 108/1000 | Loss: 0.00002153
Iteration 109/1000 | Loss: 0.00002153
Iteration 110/1000 | Loss: 0.00002153
Iteration 111/1000 | Loss: 0.00002152
Iteration 112/1000 | Loss: 0.00002152
Iteration 113/1000 | Loss: 0.00002152
Iteration 114/1000 | Loss: 0.00002152
Iteration 115/1000 | Loss: 0.00002152
Iteration 116/1000 | Loss: 0.00002152
Iteration 117/1000 | Loss: 0.00002152
Iteration 118/1000 | Loss: 0.00002151
Iteration 119/1000 | Loss: 0.00002151
Iteration 120/1000 | Loss: 0.00002151
Iteration 121/1000 | Loss: 0.00002151
Iteration 122/1000 | Loss: 0.00002151
Iteration 123/1000 | Loss: 0.00002150
Iteration 124/1000 | Loss: 0.00002150
Iteration 125/1000 | Loss: 0.00002150
Iteration 126/1000 | Loss: 0.00002150
Iteration 127/1000 | Loss: 0.00002150
Iteration 128/1000 | Loss: 0.00002150
Iteration 129/1000 | Loss: 0.00002150
Iteration 130/1000 | Loss: 0.00002149
Iteration 131/1000 | Loss: 0.00002149
Iteration 132/1000 | Loss: 0.00002149
Iteration 133/1000 | Loss: 0.00002149
Iteration 134/1000 | Loss: 0.00002149
Iteration 135/1000 | Loss: 0.00002148
Iteration 136/1000 | Loss: 0.00002148
Iteration 137/1000 | Loss: 0.00002148
Iteration 138/1000 | Loss: 0.00002148
Iteration 139/1000 | Loss: 0.00002148
Iteration 140/1000 | Loss: 0.00002148
Iteration 141/1000 | Loss: 0.00002148
Iteration 142/1000 | Loss: 0.00002148
Iteration 143/1000 | Loss: 0.00002147
Iteration 144/1000 | Loss: 0.00002147
Iteration 145/1000 | Loss: 0.00002147
Iteration 146/1000 | Loss: 0.00002147
Iteration 147/1000 | Loss: 0.00002147
Iteration 148/1000 | Loss: 0.00002147
Iteration 149/1000 | Loss: 0.00002147
Iteration 150/1000 | Loss: 0.00002147
Iteration 151/1000 | Loss: 0.00002147
Iteration 152/1000 | Loss: 0.00002147
Iteration 153/1000 | Loss: 0.00002147
Iteration 154/1000 | Loss: 0.00002147
Iteration 155/1000 | Loss: 0.00002147
Iteration 156/1000 | Loss: 0.00002146
Iteration 157/1000 | Loss: 0.00002146
Iteration 158/1000 | Loss: 0.00002146
Iteration 159/1000 | Loss: 0.00002146
Iteration 160/1000 | Loss: 0.00002146
Iteration 161/1000 | Loss: 0.00002146
Iteration 162/1000 | Loss: 0.00002146
Iteration 163/1000 | Loss: 0.00002146
Iteration 164/1000 | Loss: 0.00002145
Iteration 165/1000 | Loss: 0.00002145
Iteration 166/1000 | Loss: 0.00002145
Iteration 167/1000 | Loss: 0.00002145
Iteration 168/1000 | Loss: 0.00002145
Iteration 169/1000 | Loss: 0.00002145
Iteration 170/1000 | Loss: 0.00002145
Iteration 171/1000 | Loss: 0.00002145
Iteration 172/1000 | Loss: 0.00002145
Iteration 173/1000 | Loss: 0.00002145
Iteration 174/1000 | Loss: 0.00002145
Iteration 175/1000 | Loss: 0.00002145
Iteration 176/1000 | Loss: 0.00002145
Iteration 177/1000 | Loss: 0.00002145
Iteration 178/1000 | Loss: 0.00002145
Iteration 179/1000 | Loss: 0.00002145
Iteration 180/1000 | Loss: 0.00002145
Iteration 181/1000 | Loss: 0.00002144
Iteration 182/1000 | Loss: 0.00002144
Iteration 183/1000 | Loss: 0.00002144
Iteration 184/1000 | Loss: 0.00002144
Iteration 185/1000 | Loss: 0.00002144
Iteration 186/1000 | Loss: 0.00002144
Iteration 187/1000 | Loss: 0.00002144
Iteration 188/1000 | Loss: 0.00002144
Iteration 189/1000 | Loss: 0.00002144
Iteration 190/1000 | Loss: 0.00002144
Iteration 191/1000 | Loss: 0.00002144
Iteration 192/1000 | Loss: 0.00002144
Iteration 193/1000 | Loss: 0.00002143
Iteration 194/1000 | Loss: 0.00002143
Iteration 195/1000 | Loss: 0.00002143
Iteration 196/1000 | Loss: 0.00002143
Iteration 197/1000 | Loss: 0.00002143
Iteration 198/1000 | Loss: 0.00002143
Iteration 199/1000 | Loss: 0.00002143
Iteration 200/1000 | Loss: 0.00002143
Iteration 201/1000 | Loss: 0.00002143
Iteration 202/1000 | Loss: 0.00002143
Iteration 203/1000 | Loss: 0.00002143
Iteration 204/1000 | Loss: 0.00002143
Iteration 205/1000 | Loss: 0.00002143
Iteration 206/1000 | Loss: 0.00002143
Iteration 207/1000 | Loss: 0.00002143
Iteration 208/1000 | Loss: 0.00002143
Iteration 209/1000 | Loss: 0.00002143
Iteration 210/1000 | Loss: 0.00002143
Iteration 211/1000 | Loss: 0.00002143
Iteration 212/1000 | Loss: 0.00002143
Iteration 213/1000 | Loss: 0.00002143
Iteration 214/1000 | Loss: 0.00002143
Iteration 215/1000 | Loss: 0.00002143
Iteration 216/1000 | Loss: 0.00002143
Iteration 217/1000 | Loss: 0.00002143
Iteration 218/1000 | Loss: 0.00002142
Iteration 219/1000 | Loss: 0.00002142
Iteration 220/1000 | Loss: 0.00002142
Iteration 221/1000 | Loss: 0.00002142
Iteration 222/1000 | Loss: 0.00002142
Iteration 223/1000 | Loss: 0.00002142
Iteration 224/1000 | Loss: 0.00002142
Iteration 225/1000 | Loss: 0.00002142
Iteration 226/1000 | Loss: 0.00002142
Iteration 227/1000 | Loss: 0.00002142
Iteration 228/1000 | Loss: 0.00002142
Iteration 229/1000 | Loss: 0.00002142
Iteration 230/1000 | Loss: 0.00002142
Iteration 231/1000 | Loss: 0.00002142
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 231. Stopping optimization.
Last 5 losses: [2.1421636120066978e-05, 2.1421636120066978e-05, 2.1421636120066978e-05, 2.1421636120066978e-05, 2.1421636120066978e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1421636120066978e-05

Optimization complete. Final v2v error: 3.6209428310394287 mm

Highest mean error: 4.624831676483154 mm for frame 79

Lowest mean error: 3.1824917793273926 mm for frame 116

Saving results

Total time: 43.78268814086914
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00811022
Iteration 2/25 | Loss: 0.00126826
Iteration 3/25 | Loss: 0.00120522
Iteration 4/25 | Loss: 0.00119722
Iteration 5/25 | Loss: 0.00119560
Iteration 6/25 | Loss: 0.00119560
Iteration 7/25 | Loss: 0.00119560
Iteration 8/25 | Loss: 0.00119560
Iteration 9/25 | Loss: 0.00119560
Iteration 10/25 | Loss: 0.00119560
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001195595832541585, 0.001195595832541585, 0.001195595832541585, 0.001195595832541585, 0.001195595832541585]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001195595832541585

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34884942
Iteration 2/25 | Loss: 0.00099006
Iteration 3/25 | Loss: 0.00099006
Iteration 4/25 | Loss: 0.00099006
Iteration 5/25 | Loss: 0.00099005
Iteration 6/25 | Loss: 0.00099005
Iteration 7/25 | Loss: 0.00099005
Iteration 8/25 | Loss: 0.00099005
Iteration 9/25 | Loss: 0.00099005
Iteration 10/25 | Loss: 0.00099005
Iteration 11/25 | Loss: 0.00099005
Iteration 12/25 | Loss: 0.00099005
Iteration 13/25 | Loss: 0.00099005
Iteration 14/25 | Loss: 0.00099005
Iteration 15/25 | Loss: 0.00099005
Iteration 16/25 | Loss: 0.00099005
Iteration 17/25 | Loss: 0.00099005
Iteration 18/25 | Loss: 0.00099005
Iteration 19/25 | Loss: 0.00099005
Iteration 20/25 | Loss: 0.00099005
Iteration 21/25 | Loss: 0.00099005
Iteration 22/25 | Loss: 0.00099005
Iteration 23/25 | Loss: 0.00099005
Iteration 24/25 | Loss: 0.00099005
Iteration 25/25 | Loss: 0.00099005

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099005
Iteration 2/1000 | Loss: 0.00001937
Iteration 3/1000 | Loss: 0.00001501
Iteration 4/1000 | Loss: 0.00001331
Iteration 5/1000 | Loss: 0.00001250
Iteration 6/1000 | Loss: 0.00001182
Iteration 7/1000 | Loss: 0.00001152
Iteration 8/1000 | Loss: 0.00001127
Iteration 9/1000 | Loss: 0.00001094
Iteration 10/1000 | Loss: 0.00001078
Iteration 11/1000 | Loss: 0.00001075
Iteration 12/1000 | Loss: 0.00001066
Iteration 13/1000 | Loss: 0.00001058
Iteration 14/1000 | Loss: 0.00001056
Iteration 15/1000 | Loss: 0.00001054
Iteration 16/1000 | Loss: 0.00001053
Iteration 17/1000 | Loss: 0.00001052
Iteration 18/1000 | Loss: 0.00001052
Iteration 19/1000 | Loss: 0.00001050
Iteration 20/1000 | Loss: 0.00001047
Iteration 21/1000 | Loss: 0.00001046
Iteration 22/1000 | Loss: 0.00001046
Iteration 23/1000 | Loss: 0.00001046
Iteration 24/1000 | Loss: 0.00001045
Iteration 25/1000 | Loss: 0.00001045
Iteration 26/1000 | Loss: 0.00001045
Iteration 27/1000 | Loss: 0.00001045
Iteration 28/1000 | Loss: 0.00001045
Iteration 29/1000 | Loss: 0.00001044
Iteration 30/1000 | Loss: 0.00001043
Iteration 31/1000 | Loss: 0.00001042
Iteration 32/1000 | Loss: 0.00001042
Iteration 33/1000 | Loss: 0.00001042
Iteration 34/1000 | Loss: 0.00001041
Iteration 35/1000 | Loss: 0.00001041
Iteration 36/1000 | Loss: 0.00001036
Iteration 37/1000 | Loss: 0.00001034
Iteration 38/1000 | Loss: 0.00001033
Iteration 39/1000 | Loss: 0.00001033
Iteration 40/1000 | Loss: 0.00001025
Iteration 41/1000 | Loss: 0.00001021
Iteration 42/1000 | Loss: 0.00001018
Iteration 43/1000 | Loss: 0.00001017
Iteration 44/1000 | Loss: 0.00001016
Iteration 45/1000 | Loss: 0.00001015
Iteration 46/1000 | Loss: 0.00001015
Iteration 47/1000 | Loss: 0.00001015
Iteration 48/1000 | Loss: 0.00001015
Iteration 49/1000 | Loss: 0.00001014
Iteration 50/1000 | Loss: 0.00001014
Iteration 51/1000 | Loss: 0.00001014
Iteration 52/1000 | Loss: 0.00001013
Iteration 53/1000 | Loss: 0.00001012
Iteration 54/1000 | Loss: 0.00001011
Iteration 55/1000 | Loss: 0.00001011
Iteration 56/1000 | Loss: 0.00001011
Iteration 57/1000 | Loss: 0.00001011
Iteration 58/1000 | Loss: 0.00001011
Iteration 59/1000 | Loss: 0.00001011
Iteration 60/1000 | Loss: 0.00001010
Iteration 61/1000 | Loss: 0.00001010
Iteration 62/1000 | Loss: 0.00001010
Iteration 63/1000 | Loss: 0.00001010
Iteration 64/1000 | Loss: 0.00001009
Iteration 65/1000 | Loss: 0.00001009
Iteration 66/1000 | Loss: 0.00001009
Iteration 67/1000 | Loss: 0.00001009
Iteration 68/1000 | Loss: 0.00001008
Iteration 69/1000 | Loss: 0.00001008
Iteration 70/1000 | Loss: 0.00001007
Iteration 71/1000 | Loss: 0.00001007
Iteration 72/1000 | Loss: 0.00001007
Iteration 73/1000 | Loss: 0.00001007
Iteration 74/1000 | Loss: 0.00001007
Iteration 75/1000 | Loss: 0.00001007
Iteration 76/1000 | Loss: 0.00001006
Iteration 77/1000 | Loss: 0.00001006
Iteration 78/1000 | Loss: 0.00001006
Iteration 79/1000 | Loss: 0.00001005
Iteration 80/1000 | Loss: 0.00001005
Iteration 81/1000 | Loss: 0.00001005
Iteration 82/1000 | Loss: 0.00001004
Iteration 83/1000 | Loss: 0.00001004
Iteration 84/1000 | Loss: 0.00001004
Iteration 85/1000 | Loss: 0.00001004
Iteration 86/1000 | Loss: 0.00001003
Iteration 87/1000 | Loss: 0.00001003
Iteration 88/1000 | Loss: 0.00001003
Iteration 89/1000 | Loss: 0.00001003
Iteration 90/1000 | Loss: 0.00001003
Iteration 91/1000 | Loss: 0.00001003
Iteration 92/1000 | Loss: 0.00001003
Iteration 93/1000 | Loss: 0.00001003
Iteration 94/1000 | Loss: 0.00001003
Iteration 95/1000 | Loss: 0.00001003
Iteration 96/1000 | Loss: 0.00001002
Iteration 97/1000 | Loss: 0.00001002
Iteration 98/1000 | Loss: 0.00001002
Iteration 99/1000 | Loss: 0.00001002
Iteration 100/1000 | Loss: 0.00001002
Iteration 101/1000 | Loss: 0.00001002
Iteration 102/1000 | Loss: 0.00001002
Iteration 103/1000 | Loss: 0.00001002
Iteration 104/1000 | Loss: 0.00001002
Iteration 105/1000 | Loss: 0.00001002
Iteration 106/1000 | Loss: 0.00001002
Iteration 107/1000 | Loss: 0.00001002
Iteration 108/1000 | Loss: 0.00001001
Iteration 109/1000 | Loss: 0.00001001
Iteration 110/1000 | Loss: 0.00001001
Iteration 111/1000 | Loss: 0.00001000
Iteration 112/1000 | Loss: 0.00001000
Iteration 113/1000 | Loss: 0.00001000
Iteration 114/1000 | Loss: 0.00001000
Iteration 115/1000 | Loss: 0.00001000
Iteration 116/1000 | Loss: 0.00000999
Iteration 117/1000 | Loss: 0.00000999
Iteration 118/1000 | Loss: 0.00000999
Iteration 119/1000 | Loss: 0.00000999
Iteration 120/1000 | Loss: 0.00000998
Iteration 121/1000 | Loss: 0.00000998
Iteration 122/1000 | Loss: 0.00000998
Iteration 123/1000 | Loss: 0.00000997
Iteration 124/1000 | Loss: 0.00000997
Iteration 125/1000 | Loss: 0.00000997
Iteration 126/1000 | Loss: 0.00000997
Iteration 127/1000 | Loss: 0.00000997
Iteration 128/1000 | Loss: 0.00000997
Iteration 129/1000 | Loss: 0.00000996
Iteration 130/1000 | Loss: 0.00000996
Iteration 131/1000 | Loss: 0.00000996
Iteration 132/1000 | Loss: 0.00000996
Iteration 133/1000 | Loss: 0.00000995
Iteration 134/1000 | Loss: 0.00000995
Iteration 135/1000 | Loss: 0.00000994
Iteration 136/1000 | Loss: 0.00000994
Iteration 137/1000 | Loss: 0.00000993
Iteration 138/1000 | Loss: 0.00000993
Iteration 139/1000 | Loss: 0.00000993
Iteration 140/1000 | Loss: 0.00000993
Iteration 141/1000 | Loss: 0.00000993
Iteration 142/1000 | Loss: 0.00000992
Iteration 143/1000 | Loss: 0.00000992
Iteration 144/1000 | Loss: 0.00000992
Iteration 145/1000 | Loss: 0.00000992
Iteration 146/1000 | Loss: 0.00000992
Iteration 147/1000 | Loss: 0.00000992
Iteration 148/1000 | Loss: 0.00000991
Iteration 149/1000 | Loss: 0.00000991
Iteration 150/1000 | Loss: 0.00000991
Iteration 151/1000 | Loss: 0.00000991
Iteration 152/1000 | Loss: 0.00000991
Iteration 153/1000 | Loss: 0.00000991
Iteration 154/1000 | Loss: 0.00000991
Iteration 155/1000 | Loss: 0.00000991
Iteration 156/1000 | Loss: 0.00000991
Iteration 157/1000 | Loss: 0.00000990
Iteration 158/1000 | Loss: 0.00000990
Iteration 159/1000 | Loss: 0.00000990
Iteration 160/1000 | Loss: 0.00000990
Iteration 161/1000 | Loss: 0.00000990
Iteration 162/1000 | Loss: 0.00000989
Iteration 163/1000 | Loss: 0.00000989
Iteration 164/1000 | Loss: 0.00000989
Iteration 165/1000 | Loss: 0.00000989
Iteration 166/1000 | Loss: 0.00000989
Iteration 167/1000 | Loss: 0.00000988
Iteration 168/1000 | Loss: 0.00000988
Iteration 169/1000 | Loss: 0.00000988
Iteration 170/1000 | Loss: 0.00000988
Iteration 171/1000 | Loss: 0.00000987
Iteration 172/1000 | Loss: 0.00000987
Iteration 173/1000 | Loss: 0.00000987
Iteration 174/1000 | Loss: 0.00000987
Iteration 175/1000 | Loss: 0.00000987
Iteration 176/1000 | Loss: 0.00000987
Iteration 177/1000 | Loss: 0.00000987
Iteration 178/1000 | Loss: 0.00000987
Iteration 179/1000 | Loss: 0.00000987
Iteration 180/1000 | Loss: 0.00000987
Iteration 181/1000 | Loss: 0.00000987
Iteration 182/1000 | Loss: 0.00000987
Iteration 183/1000 | Loss: 0.00000986
Iteration 184/1000 | Loss: 0.00000986
Iteration 185/1000 | Loss: 0.00000986
Iteration 186/1000 | Loss: 0.00000986
Iteration 187/1000 | Loss: 0.00000986
Iteration 188/1000 | Loss: 0.00000986
Iteration 189/1000 | Loss: 0.00000986
Iteration 190/1000 | Loss: 0.00000986
Iteration 191/1000 | Loss: 0.00000986
Iteration 192/1000 | Loss: 0.00000986
Iteration 193/1000 | Loss: 0.00000986
Iteration 194/1000 | Loss: 0.00000986
Iteration 195/1000 | Loss: 0.00000985
Iteration 196/1000 | Loss: 0.00000985
Iteration 197/1000 | Loss: 0.00000985
Iteration 198/1000 | Loss: 0.00000985
Iteration 199/1000 | Loss: 0.00000985
Iteration 200/1000 | Loss: 0.00000985
Iteration 201/1000 | Loss: 0.00000985
Iteration 202/1000 | Loss: 0.00000985
Iteration 203/1000 | Loss: 0.00000985
Iteration 204/1000 | Loss: 0.00000985
Iteration 205/1000 | Loss: 0.00000985
Iteration 206/1000 | Loss: 0.00000985
Iteration 207/1000 | Loss: 0.00000985
Iteration 208/1000 | Loss: 0.00000985
Iteration 209/1000 | Loss: 0.00000985
Iteration 210/1000 | Loss: 0.00000985
Iteration 211/1000 | Loss: 0.00000985
Iteration 212/1000 | Loss: 0.00000985
Iteration 213/1000 | Loss: 0.00000985
Iteration 214/1000 | Loss: 0.00000985
Iteration 215/1000 | Loss: 0.00000985
Iteration 216/1000 | Loss: 0.00000985
Iteration 217/1000 | Loss: 0.00000985
Iteration 218/1000 | Loss: 0.00000985
Iteration 219/1000 | Loss: 0.00000985
Iteration 220/1000 | Loss: 0.00000985
Iteration 221/1000 | Loss: 0.00000985
Iteration 222/1000 | Loss: 0.00000985
Iteration 223/1000 | Loss: 0.00000985
Iteration 224/1000 | Loss: 0.00000985
Iteration 225/1000 | Loss: 0.00000985
Iteration 226/1000 | Loss: 0.00000985
Iteration 227/1000 | Loss: 0.00000985
Iteration 228/1000 | Loss: 0.00000985
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 228. Stopping optimization.
Last 5 losses: [9.848911759036127e-06, 9.848911759036127e-06, 9.848911759036127e-06, 9.848911759036127e-06, 9.848911759036127e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.848911759036127e-06

Optimization complete. Final v2v error: 2.6782171726226807 mm

Highest mean error: 2.8420417308807373 mm for frame 105

Lowest mean error: 2.535076141357422 mm for frame 213

Saving results

Total time: 47.678242206573486
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00465760
Iteration 2/25 | Loss: 0.00133812
Iteration 3/25 | Loss: 0.00123800
Iteration 4/25 | Loss: 0.00122920
Iteration 5/25 | Loss: 0.00122731
Iteration 6/25 | Loss: 0.00122731
Iteration 7/25 | Loss: 0.00122731
Iteration 8/25 | Loss: 0.00122731
Iteration 9/25 | Loss: 0.00122731
Iteration 10/25 | Loss: 0.00122731
Iteration 11/25 | Loss: 0.00122731
Iteration 12/25 | Loss: 0.00122731
Iteration 13/25 | Loss: 0.00122731
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012273070169612765, 0.0012273070169612765, 0.0012273070169612765, 0.0012273070169612765, 0.0012273070169612765]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012273070169612765

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.64074326
Iteration 2/25 | Loss: 0.00099871
Iteration 3/25 | Loss: 0.00099870
Iteration 4/25 | Loss: 0.00099869
Iteration 5/25 | Loss: 0.00099869
Iteration 6/25 | Loss: 0.00099869
Iteration 7/25 | Loss: 0.00099869
Iteration 8/25 | Loss: 0.00099869
Iteration 9/25 | Loss: 0.00099869
Iteration 10/25 | Loss: 0.00099869
Iteration 11/25 | Loss: 0.00099869
Iteration 12/25 | Loss: 0.00099869
Iteration 13/25 | Loss: 0.00099869
Iteration 14/25 | Loss: 0.00099869
Iteration 15/25 | Loss: 0.00099869
Iteration 16/25 | Loss: 0.00099869
Iteration 17/25 | Loss: 0.00099869
Iteration 18/25 | Loss: 0.00099869
Iteration 19/25 | Loss: 0.00099869
Iteration 20/25 | Loss: 0.00099869
Iteration 21/25 | Loss: 0.00099869
Iteration 22/25 | Loss: 0.00099869
Iteration 23/25 | Loss: 0.00099869
Iteration 24/25 | Loss: 0.00099869
Iteration 25/25 | Loss: 0.00099869

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099869
Iteration 2/1000 | Loss: 0.00002244
Iteration 3/1000 | Loss: 0.00001759
Iteration 4/1000 | Loss: 0.00001609
Iteration 5/1000 | Loss: 0.00001508
Iteration 6/1000 | Loss: 0.00001447
Iteration 7/1000 | Loss: 0.00001418
Iteration 8/1000 | Loss: 0.00001391
Iteration 9/1000 | Loss: 0.00001349
Iteration 10/1000 | Loss: 0.00001342
Iteration 11/1000 | Loss: 0.00001320
Iteration 12/1000 | Loss: 0.00001301
Iteration 13/1000 | Loss: 0.00001298
Iteration 14/1000 | Loss: 0.00001297
Iteration 15/1000 | Loss: 0.00001291
Iteration 16/1000 | Loss: 0.00001280
Iteration 17/1000 | Loss: 0.00001270
Iteration 18/1000 | Loss: 0.00001267
Iteration 19/1000 | Loss: 0.00001263
Iteration 20/1000 | Loss: 0.00001261
Iteration 21/1000 | Loss: 0.00001259
Iteration 22/1000 | Loss: 0.00001256
Iteration 23/1000 | Loss: 0.00001255
Iteration 24/1000 | Loss: 0.00001254
Iteration 25/1000 | Loss: 0.00001250
Iteration 26/1000 | Loss: 0.00001250
Iteration 27/1000 | Loss: 0.00001248
Iteration 28/1000 | Loss: 0.00001248
Iteration 29/1000 | Loss: 0.00001248
Iteration 30/1000 | Loss: 0.00001248
Iteration 31/1000 | Loss: 0.00001248
Iteration 32/1000 | Loss: 0.00001247
Iteration 33/1000 | Loss: 0.00001247
Iteration 34/1000 | Loss: 0.00001246
Iteration 35/1000 | Loss: 0.00001246
Iteration 36/1000 | Loss: 0.00001244
Iteration 37/1000 | Loss: 0.00001244
Iteration 38/1000 | Loss: 0.00001244
Iteration 39/1000 | Loss: 0.00001244
Iteration 40/1000 | Loss: 0.00001244
Iteration 41/1000 | Loss: 0.00001244
Iteration 42/1000 | Loss: 0.00001244
Iteration 43/1000 | Loss: 0.00001243
Iteration 44/1000 | Loss: 0.00001243
Iteration 45/1000 | Loss: 0.00001243
Iteration 46/1000 | Loss: 0.00001241
Iteration 47/1000 | Loss: 0.00001241
Iteration 48/1000 | Loss: 0.00001241
Iteration 49/1000 | Loss: 0.00001240
Iteration 50/1000 | Loss: 0.00001240
Iteration 51/1000 | Loss: 0.00001239
Iteration 52/1000 | Loss: 0.00001239
Iteration 53/1000 | Loss: 0.00001238
Iteration 54/1000 | Loss: 0.00001238
Iteration 55/1000 | Loss: 0.00001237
Iteration 56/1000 | Loss: 0.00001236
Iteration 57/1000 | Loss: 0.00001236
Iteration 58/1000 | Loss: 0.00001236
Iteration 59/1000 | Loss: 0.00001236
Iteration 60/1000 | Loss: 0.00001235
Iteration 61/1000 | Loss: 0.00001235
Iteration 62/1000 | Loss: 0.00001235
Iteration 63/1000 | Loss: 0.00001235
Iteration 64/1000 | Loss: 0.00001235
Iteration 65/1000 | Loss: 0.00001235
Iteration 66/1000 | Loss: 0.00001235
Iteration 67/1000 | Loss: 0.00001234
Iteration 68/1000 | Loss: 0.00001234
Iteration 69/1000 | Loss: 0.00001233
Iteration 70/1000 | Loss: 0.00001233
Iteration 71/1000 | Loss: 0.00001233
Iteration 72/1000 | Loss: 0.00001232
Iteration 73/1000 | Loss: 0.00001232
Iteration 74/1000 | Loss: 0.00001232
Iteration 75/1000 | Loss: 0.00001232
Iteration 76/1000 | Loss: 0.00001232
Iteration 77/1000 | Loss: 0.00001231
Iteration 78/1000 | Loss: 0.00001231
Iteration 79/1000 | Loss: 0.00001231
Iteration 80/1000 | Loss: 0.00001231
Iteration 81/1000 | Loss: 0.00001230
Iteration 82/1000 | Loss: 0.00001229
Iteration 83/1000 | Loss: 0.00001229
Iteration 84/1000 | Loss: 0.00001229
Iteration 85/1000 | Loss: 0.00001229
Iteration 86/1000 | Loss: 0.00001229
Iteration 87/1000 | Loss: 0.00001229
Iteration 88/1000 | Loss: 0.00001229
Iteration 89/1000 | Loss: 0.00001228
Iteration 90/1000 | Loss: 0.00001228
Iteration 91/1000 | Loss: 0.00001228
Iteration 92/1000 | Loss: 0.00001227
Iteration 93/1000 | Loss: 0.00001227
Iteration 94/1000 | Loss: 0.00001226
Iteration 95/1000 | Loss: 0.00001226
Iteration 96/1000 | Loss: 0.00001226
Iteration 97/1000 | Loss: 0.00001225
Iteration 98/1000 | Loss: 0.00001225
Iteration 99/1000 | Loss: 0.00001225
Iteration 100/1000 | Loss: 0.00001225
Iteration 101/1000 | Loss: 0.00001225
Iteration 102/1000 | Loss: 0.00001225
Iteration 103/1000 | Loss: 0.00001224
Iteration 104/1000 | Loss: 0.00001224
Iteration 105/1000 | Loss: 0.00001223
Iteration 106/1000 | Loss: 0.00001223
Iteration 107/1000 | Loss: 0.00001223
Iteration 108/1000 | Loss: 0.00001222
Iteration 109/1000 | Loss: 0.00001222
Iteration 110/1000 | Loss: 0.00001222
Iteration 111/1000 | Loss: 0.00001222
Iteration 112/1000 | Loss: 0.00001222
Iteration 113/1000 | Loss: 0.00001222
Iteration 114/1000 | Loss: 0.00001222
Iteration 115/1000 | Loss: 0.00001222
Iteration 116/1000 | Loss: 0.00001222
Iteration 117/1000 | Loss: 0.00001222
Iteration 118/1000 | Loss: 0.00001221
Iteration 119/1000 | Loss: 0.00001221
Iteration 120/1000 | Loss: 0.00001221
Iteration 121/1000 | Loss: 0.00001220
Iteration 122/1000 | Loss: 0.00001220
Iteration 123/1000 | Loss: 0.00001219
Iteration 124/1000 | Loss: 0.00001219
Iteration 125/1000 | Loss: 0.00001219
Iteration 126/1000 | Loss: 0.00001219
Iteration 127/1000 | Loss: 0.00001219
Iteration 128/1000 | Loss: 0.00001219
Iteration 129/1000 | Loss: 0.00001219
Iteration 130/1000 | Loss: 0.00001219
Iteration 131/1000 | Loss: 0.00001219
Iteration 132/1000 | Loss: 0.00001219
Iteration 133/1000 | Loss: 0.00001219
Iteration 134/1000 | Loss: 0.00001219
Iteration 135/1000 | Loss: 0.00001219
Iteration 136/1000 | Loss: 0.00001219
Iteration 137/1000 | Loss: 0.00001218
Iteration 138/1000 | Loss: 0.00001218
Iteration 139/1000 | Loss: 0.00001218
Iteration 140/1000 | Loss: 0.00001218
Iteration 141/1000 | Loss: 0.00001218
Iteration 142/1000 | Loss: 0.00001218
Iteration 143/1000 | Loss: 0.00001218
Iteration 144/1000 | Loss: 0.00001218
Iteration 145/1000 | Loss: 0.00001218
Iteration 146/1000 | Loss: 0.00001218
Iteration 147/1000 | Loss: 0.00001218
Iteration 148/1000 | Loss: 0.00001217
Iteration 149/1000 | Loss: 0.00001217
Iteration 150/1000 | Loss: 0.00001217
Iteration 151/1000 | Loss: 0.00001217
Iteration 152/1000 | Loss: 0.00001217
Iteration 153/1000 | Loss: 0.00001217
Iteration 154/1000 | Loss: 0.00001217
Iteration 155/1000 | Loss: 0.00001217
Iteration 156/1000 | Loss: 0.00001217
Iteration 157/1000 | Loss: 0.00001217
Iteration 158/1000 | Loss: 0.00001217
Iteration 159/1000 | Loss: 0.00001217
Iteration 160/1000 | Loss: 0.00001217
Iteration 161/1000 | Loss: 0.00001217
Iteration 162/1000 | Loss: 0.00001217
Iteration 163/1000 | Loss: 0.00001216
Iteration 164/1000 | Loss: 0.00001216
Iteration 165/1000 | Loss: 0.00001216
Iteration 166/1000 | Loss: 0.00001216
Iteration 167/1000 | Loss: 0.00001216
Iteration 168/1000 | Loss: 0.00001216
Iteration 169/1000 | Loss: 0.00001216
Iteration 170/1000 | Loss: 0.00001216
Iteration 171/1000 | Loss: 0.00001216
Iteration 172/1000 | Loss: 0.00001216
Iteration 173/1000 | Loss: 0.00001216
Iteration 174/1000 | Loss: 0.00001215
Iteration 175/1000 | Loss: 0.00001215
Iteration 176/1000 | Loss: 0.00001215
Iteration 177/1000 | Loss: 0.00001215
Iteration 178/1000 | Loss: 0.00001215
Iteration 179/1000 | Loss: 0.00001215
Iteration 180/1000 | Loss: 0.00001215
Iteration 181/1000 | Loss: 0.00001215
Iteration 182/1000 | Loss: 0.00001215
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [1.2154794603702612e-05, 1.2154794603702612e-05, 1.2154794603702612e-05, 1.2154794603702612e-05, 1.2154794603702612e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2154794603702612e-05

Optimization complete. Final v2v error: 2.983039140701294 mm

Highest mean error: 3.264779806137085 mm for frame 53

Lowest mean error: 2.7213101387023926 mm for frame 40

Saving results

Total time: 45.12145972251892
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00597371
Iteration 2/25 | Loss: 0.00144025
Iteration 3/25 | Loss: 0.00131510
Iteration 4/25 | Loss: 0.00129430
Iteration 5/25 | Loss: 0.00128823
Iteration 6/25 | Loss: 0.00128909
Iteration 7/25 | Loss: 0.00128687
Iteration 8/25 | Loss: 0.00128608
Iteration 9/25 | Loss: 0.00128590
Iteration 10/25 | Loss: 0.00128915
Iteration 11/25 | Loss: 0.00128832
Iteration 12/25 | Loss: 0.00128746
Iteration 13/25 | Loss: 0.00128903
Iteration 14/25 | Loss: 0.00128800
Iteration 15/25 | Loss: 0.00128808
Iteration 16/25 | Loss: 0.00128933
Iteration 17/25 | Loss: 0.00128791
Iteration 18/25 | Loss: 0.00128796
Iteration 19/25 | Loss: 0.00128956
Iteration 20/25 | Loss: 0.00128782
Iteration 21/25 | Loss: 0.00128839
Iteration 22/25 | Loss: 0.00128591
Iteration 23/25 | Loss: 0.00128581
Iteration 24/25 | Loss: 0.00128581
Iteration 25/25 | Loss: 0.00128581

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46216464
Iteration 2/25 | Loss: 0.00129353
Iteration 3/25 | Loss: 0.00129352
Iteration 4/25 | Loss: 0.00129352
Iteration 5/25 | Loss: 0.00129352
Iteration 6/25 | Loss: 0.00129351
Iteration 7/25 | Loss: 0.00129351
Iteration 8/25 | Loss: 0.00129351
Iteration 9/25 | Loss: 0.00129351
Iteration 10/25 | Loss: 0.00129351
Iteration 11/25 | Loss: 0.00129351
Iteration 12/25 | Loss: 0.00129351
Iteration 13/25 | Loss: 0.00129351
Iteration 14/25 | Loss: 0.00129351
Iteration 15/25 | Loss: 0.00129351
Iteration 16/25 | Loss: 0.00129351
Iteration 17/25 | Loss: 0.00129351
Iteration 18/25 | Loss: 0.00129351
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001293513341806829, 0.001293513341806829, 0.001293513341806829, 0.001293513341806829, 0.001293513341806829]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001293513341806829

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129351
Iteration 2/1000 | Loss: 0.00005788
Iteration 3/1000 | Loss: 0.00003659
Iteration 4/1000 | Loss: 0.00004468
Iteration 5/1000 | Loss: 0.00003855
Iteration 6/1000 | Loss: 0.00015150
Iteration 7/1000 | Loss: 0.00012570
Iteration 8/1000 | Loss: 0.00015521
Iteration 9/1000 | Loss: 0.00003707
Iteration 10/1000 | Loss: 0.00006499
Iteration 11/1000 | Loss: 0.00017458
Iteration 12/1000 | Loss: 0.00010255
Iteration 13/1000 | Loss: 0.00009333
Iteration 14/1000 | Loss: 0.00012728
Iteration 15/1000 | Loss: 0.00011497
Iteration 16/1000 | Loss: 0.00004371
Iteration 17/1000 | Loss: 0.00013059
Iteration 18/1000 | Loss: 0.00004847
Iteration 19/1000 | Loss: 0.00008202
Iteration 20/1000 | Loss: 0.00013461
Iteration 21/1000 | Loss: 0.00010955
Iteration 22/1000 | Loss: 0.00023221
Iteration 23/1000 | Loss: 0.00015941
Iteration 24/1000 | Loss: 0.00021330
Iteration 25/1000 | Loss: 0.00008668
Iteration 26/1000 | Loss: 0.00011979
Iteration 27/1000 | Loss: 0.00006121
Iteration 28/1000 | Loss: 0.00004196
Iteration 29/1000 | Loss: 0.00002466
Iteration 30/1000 | Loss: 0.00002395
Iteration 31/1000 | Loss: 0.00013721
Iteration 32/1000 | Loss: 0.00011867
Iteration 33/1000 | Loss: 0.00003317
Iteration 34/1000 | Loss: 0.00003005
Iteration 35/1000 | Loss: 0.00013714
Iteration 36/1000 | Loss: 0.00003107
Iteration 37/1000 | Loss: 0.00008000
Iteration 38/1000 | Loss: 0.00009506
Iteration 39/1000 | Loss: 0.00007398
Iteration 40/1000 | Loss: 0.00012297
Iteration 41/1000 | Loss: 0.00007270
Iteration 42/1000 | Loss: 0.00008391
Iteration 43/1000 | Loss: 0.00004930
Iteration 44/1000 | Loss: 0.00012445
Iteration 45/1000 | Loss: 0.00007214
Iteration 46/1000 | Loss: 0.00011784
Iteration 47/1000 | Loss: 0.00011209
Iteration 48/1000 | Loss: 0.00008340
Iteration 49/1000 | Loss: 0.00005457
Iteration 50/1000 | Loss: 0.00010221
Iteration 51/1000 | Loss: 0.00007107
Iteration 52/1000 | Loss: 0.00006412
Iteration 53/1000 | Loss: 0.00008233
Iteration 54/1000 | Loss: 0.00013697
Iteration 55/1000 | Loss: 0.00013827
Iteration 56/1000 | Loss: 0.00013126
Iteration 57/1000 | Loss: 0.00005447
Iteration 58/1000 | Loss: 0.00009892
Iteration 59/1000 | Loss: 0.00011043
Iteration 60/1000 | Loss: 0.00009736
Iteration 61/1000 | Loss: 0.00011359
Iteration 62/1000 | Loss: 0.00009387
Iteration 63/1000 | Loss: 0.00011648
Iteration 64/1000 | Loss: 0.00011380
Iteration 65/1000 | Loss: 0.00002719
Iteration 66/1000 | Loss: 0.00002412
Iteration 67/1000 | Loss: 0.00002311
Iteration 68/1000 | Loss: 0.00002279
Iteration 69/1000 | Loss: 0.00002255
Iteration 70/1000 | Loss: 0.00002244
Iteration 71/1000 | Loss: 0.00002243
Iteration 72/1000 | Loss: 0.00002242
Iteration 73/1000 | Loss: 0.00002224
Iteration 74/1000 | Loss: 0.00002218
Iteration 75/1000 | Loss: 0.00002199
Iteration 76/1000 | Loss: 0.00002183
Iteration 77/1000 | Loss: 0.00002181
Iteration 78/1000 | Loss: 0.00002180
Iteration 79/1000 | Loss: 0.00002179
Iteration 80/1000 | Loss: 0.00002178
Iteration 81/1000 | Loss: 0.00002178
Iteration 82/1000 | Loss: 0.00002177
Iteration 83/1000 | Loss: 0.00002176
Iteration 84/1000 | Loss: 0.00002174
Iteration 85/1000 | Loss: 0.00002170
Iteration 86/1000 | Loss: 0.00002168
Iteration 87/1000 | Loss: 0.00002168
Iteration 88/1000 | Loss: 0.00002165
Iteration 89/1000 | Loss: 0.00002165
Iteration 90/1000 | Loss: 0.00002164
Iteration 91/1000 | Loss: 0.00002164
Iteration 92/1000 | Loss: 0.00002163
Iteration 93/1000 | Loss: 0.00002162
Iteration 94/1000 | Loss: 0.00002162
Iteration 95/1000 | Loss: 0.00002162
Iteration 96/1000 | Loss: 0.00002161
Iteration 97/1000 | Loss: 0.00002161
Iteration 98/1000 | Loss: 0.00002160
Iteration 99/1000 | Loss: 0.00002160
Iteration 100/1000 | Loss: 0.00002160
Iteration 101/1000 | Loss: 0.00002158
Iteration 102/1000 | Loss: 0.00002156
Iteration 103/1000 | Loss: 0.00002152
Iteration 104/1000 | Loss: 0.00002151
Iteration 105/1000 | Loss: 0.00002150
Iteration 106/1000 | Loss: 0.00002150
Iteration 107/1000 | Loss: 0.00002150
Iteration 108/1000 | Loss: 0.00002149
Iteration 109/1000 | Loss: 0.00002149
Iteration 110/1000 | Loss: 0.00002149
Iteration 111/1000 | Loss: 0.00002148
Iteration 112/1000 | Loss: 0.00002148
Iteration 113/1000 | Loss: 0.00002148
Iteration 114/1000 | Loss: 0.00002147
Iteration 115/1000 | Loss: 0.00002147
Iteration 116/1000 | Loss: 0.00002146
Iteration 117/1000 | Loss: 0.00002146
Iteration 118/1000 | Loss: 0.00002146
Iteration 119/1000 | Loss: 0.00002145
Iteration 120/1000 | Loss: 0.00002144
Iteration 121/1000 | Loss: 0.00002143
Iteration 122/1000 | Loss: 0.00002143
Iteration 123/1000 | Loss: 0.00002143
Iteration 124/1000 | Loss: 0.00002142
Iteration 125/1000 | Loss: 0.00002142
Iteration 126/1000 | Loss: 0.00002141
Iteration 127/1000 | Loss: 0.00002140
Iteration 128/1000 | Loss: 0.00002139
Iteration 129/1000 | Loss: 0.00002139
Iteration 130/1000 | Loss: 0.00002139
Iteration 131/1000 | Loss: 0.00002139
Iteration 132/1000 | Loss: 0.00002138
Iteration 133/1000 | Loss: 0.00002138
Iteration 134/1000 | Loss: 0.00002137
Iteration 135/1000 | Loss: 0.00002137
Iteration 136/1000 | Loss: 0.00002137
Iteration 137/1000 | Loss: 0.00002136
Iteration 138/1000 | Loss: 0.00002136
Iteration 139/1000 | Loss: 0.00002136
Iteration 140/1000 | Loss: 0.00002135
Iteration 141/1000 | Loss: 0.00002135
Iteration 142/1000 | Loss: 0.00002135
Iteration 143/1000 | Loss: 0.00002135
Iteration 144/1000 | Loss: 0.00002134
Iteration 145/1000 | Loss: 0.00002134
Iteration 146/1000 | Loss: 0.00002134
Iteration 147/1000 | Loss: 0.00002133
Iteration 148/1000 | Loss: 0.00002133
Iteration 149/1000 | Loss: 0.00002133
Iteration 150/1000 | Loss: 0.00002132
Iteration 151/1000 | Loss: 0.00002132
Iteration 152/1000 | Loss: 0.00002131
Iteration 153/1000 | Loss: 0.00002131
Iteration 154/1000 | Loss: 0.00002131
Iteration 155/1000 | Loss: 0.00002131
Iteration 156/1000 | Loss: 0.00002130
Iteration 157/1000 | Loss: 0.00002130
Iteration 158/1000 | Loss: 0.00002130
Iteration 159/1000 | Loss: 0.00002130
Iteration 160/1000 | Loss: 0.00002129
Iteration 161/1000 | Loss: 0.00002129
Iteration 162/1000 | Loss: 0.00002129
Iteration 163/1000 | Loss: 0.00002129
Iteration 164/1000 | Loss: 0.00002129
Iteration 165/1000 | Loss: 0.00002129
Iteration 166/1000 | Loss: 0.00002129
Iteration 167/1000 | Loss: 0.00002129
Iteration 168/1000 | Loss: 0.00002128
Iteration 169/1000 | Loss: 0.00002128
Iteration 170/1000 | Loss: 0.00002128
Iteration 171/1000 | Loss: 0.00002128
Iteration 172/1000 | Loss: 0.00002128
Iteration 173/1000 | Loss: 0.00002128
Iteration 174/1000 | Loss: 0.00002128
Iteration 175/1000 | Loss: 0.00002128
Iteration 176/1000 | Loss: 0.00002128
Iteration 177/1000 | Loss: 0.00002128
Iteration 178/1000 | Loss: 0.00002128
Iteration 179/1000 | Loss: 0.00002128
Iteration 180/1000 | Loss: 0.00002128
Iteration 181/1000 | Loss: 0.00002128
Iteration 182/1000 | Loss: 0.00002128
Iteration 183/1000 | Loss: 0.00002127
Iteration 184/1000 | Loss: 0.00002127
Iteration 185/1000 | Loss: 0.00002127
Iteration 186/1000 | Loss: 0.00002127
Iteration 187/1000 | Loss: 0.00002127
Iteration 188/1000 | Loss: 0.00002127
Iteration 189/1000 | Loss: 0.00002127
Iteration 190/1000 | Loss: 0.00002127
Iteration 191/1000 | Loss: 0.00002127
Iteration 192/1000 | Loss: 0.00002127
Iteration 193/1000 | Loss: 0.00002127
Iteration 194/1000 | Loss: 0.00002127
Iteration 195/1000 | Loss: 0.00002127
Iteration 196/1000 | Loss: 0.00002127
Iteration 197/1000 | Loss: 0.00002127
Iteration 198/1000 | Loss: 0.00002127
Iteration 199/1000 | Loss: 0.00002127
Iteration 200/1000 | Loss: 0.00002127
Iteration 201/1000 | Loss: 0.00002127
Iteration 202/1000 | Loss: 0.00002127
Iteration 203/1000 | Loss: 0.00002127
Iteration 204/1000 | Loss: 0.00002127
Iteration 205/1000 | Loss: 0.00002127
Iteration 206/1000 | Loss: 0.00002127
Iteration 207/1000 | Loss: 0.00002127
Iteration 208/1000 | Loss: 0.00002127
Iteration 209/1000 | Loss: 0.00002127
Iteration 210/1000 | Loss: 0.00002127
Iteration 211/1000 | Loss: 0.00002127
Iteration 212/1000 | Loss: 0.00002127
Iteration 213/1000 | Loss: 0.00002127
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [2.1266101612127386e-05, 2.1266101612127386e-05, 2.1266101612127386e-05, 2.1266101612127386e-05, 2.1266101612127386e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1266101612127386e-05

Optimization complete. Final v2v error: 3.7720165252685547 mm

Highest mean error: 4.722982406616211 mm for frame 101

Lowest mean error: 2.8489110469818115 mm for frame 196

Saving results

Total time: 170.9225583076477
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00951832
Iteration 2/25 | Loss: 0.00951832
Iteration 3/25 | Loss: 0.00951832
Iteration 4/25 | Loss: 0.00951832
Iteration 5/25 | Loss: 0.00951832
Iteration 6/25 | Loss: 0.00951832
Iteration 7/25 | Loss: 0.00951832
Iteration 8/25 | Loss: 0.00951831
Iteration 9/25 | Loss: 0.00951831
Iteration 10/25 | Loss: 0.00951831
Iteration 11/25 | Loss: 0.00951831
Iteration 12/25 | Loss: 0.00951831
Iteration 13/25 | Loss: 0.00951831
Iteration 14/25 | Loss: 0.00951831
Iteration 15/25 | Loss: 0.00951830
Iteration 16/25 | Loss: 0.00951830
Iteration 17/25 | Loss: 0.00951830
Iteration 18/25 | Loss: 0.00951830
Iteration 19/25 | Loss: 0.00951830
Iteration 20/25 | Loss: 0.00951830
Iteration 21/25 | Loss: 0.00951830
Iteration 22/25 | Loss: 0.00951830
Iteration 23/25 | Loss: 0.00951829
Iteration 24/25 | Loss: 0.00951829
Iteration 25/25 | Loss: 0.00951829

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.72817850
Iteration 2/25 | Loss: 0.18182440
Iteration 3/25 | Loss: 0.17896570
Iteration 4/25 | Loss: 0.17817353
Iteration 5/25 | Loss: 0.17800249
Iteration 6/25 | Loss: 0.17800246
Iteration 7/25 | Loss: 0.17800246
Iteration 8/25 | Loss: 0.17800246
Iteration 9/25 | Loss: 0.17800246
Iteration 10/25 | Loss: 0.17800246
Iteration 11/25 | Loss: 0.17800246
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.1780024617910385, 0.1780024617910385, 0.1780024617910385, 0.1780024617910385, 0.1780024617910385]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.1780024617910385

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17800245
Iteration 2/1000 | Loss: 0.01991984
Iteration 3/1000 | Loss: 0.00166170
Iteration 4/1000 | Loss: 0.00087883
Iteration 5/1000 | Loss: 0.00043667
Iteration 6/1000 | Loss: 0.00026165
Iteration 7/1000 | Loss: 0.00016490
Iteration 8/1000 | Loss: 0.00014219
Iteration 9/1000 | Loss: 0.00012331
Iteration 10/1000 | Loss: 0.00010352
Iteration 11/1000 | Loss: 0.00008890
Iteration 12/1000 | Loss: 0.00007690
Iteration 13/1000 | Loss: 0.00006877
Iteration 14/1000 | Loss: 0.00006026
Iteration 15/1000 | Loss: 0.00005351
Iteration 16/1000 | Loss: 0.00004522
Iteration 17/1000 | Loss: 0.00004042
Iteration 18/1000 | Loss: 0.00003717
Iteration 19/1000 | Loss: 0.00003424
Iteration 20/1000 | Loss: 0.00003192
Iteration 21/1000 | Loss: 0.00003026
Iteration 22/1000 | Loss: 0.00002921
Iteration 23/1000 | Loss: 0.00002842
Iteration 24/1000 | Loss: 0.00002780
Iteration 25/1000 | Loss: 0.00002725
Iteration 26/1000 | Loss: 0.00002676
Iteration 27/1000 | Loss: 0.00002626
Iteration 28/1000 | Loss: 0.00002592
Iteration 29/1000 | Loss: 0.00002565
Iteration 30/1000 | Loss: 0.00002544
Iteration 31/1000 | Loss: 0.00002541
Iteration 32/1000 | Loss: 0.00002526
Iteration 33/1000 | Loss: 0.00002519
Iteration 34/1000 | Loss: 0.00002508
Iteration 35/1000 | Loss: 0.00002506
Iteration 36/1000 | Loss: 0.00002505
Iteration 37/1000 | Loss: 0.00002505
Iteration 38/1000 | Loss: 0.00002504
Iteration 39/1000 | Loss: 0.00002504
Iteration 40/1000 | Loss: 0.00002504
Iteration 41/1000 | Loss: 0.00002503
Iteration 42/1000 | Loss: 0.00002503
Iteration 43/1000 | Loss: 0.00002501
Iteration 44/1000 | Loss: 0.00002499
Iteration 45/1000 | Loss: 0.00002498
Iteration 46/1000 | Loss: 0.00002498
Iteration 47/1000 | Loss: 0.00002496
Iteration 48/1000 | Loss: 0.00002496
Iteration 49/1000 | Loss: 0.00002495
Iteration 50/1000 | Loss: 0.00002495
Iteration 51/1000 | Loss: 0.00002494
Iteration 52/1000 | Loss: 0.00002491
Iteration 53/1000 | Loss: 0.00002487
Iteration 54/1000 | Loss: 0.00002487
Iteration 55/1000 | Loss: 0.00002486
Iteration 56/1000 | Loss: 0.00002486
Iteration 57/1000 | Loss: 0.00002485
Iteration 58/1000 | Loss: 0.00002482
Iteration 59/1000 | Loss: 0.00002482
Iteration 60/1000 | Loss: 0.00002481
Iteration 61/1000 | Loss: 0.00002478
Iteration 62/1000 | Loss: 0.00002477
Iteration 63/1000 | Loss: 0.00002476
Iteration 64/1000 | Loss: 0.00002476
Iteration 65/1000 | Loss: 0.00002476
Iteration 66/1000 | Loss: 0.00002475
Iteration 67/1000 | Loss: 0.00002475
Iteration 68/1000 | Loss: 0.00002475
Iteration 69/1000 | Loss: 0.00002475
Iteration 70/1000 | Loss: 0.00002475
Iteration 71/1000 | Loss: 0.00002474
Iteration 72/1000 | Loss: 0.00002474
Iteration 73/1000 | Loss: 0.00002474
Iteration 74/1000 | Loss: 0.00002474
Iteration 75/1000 | Loss: 0.00002474
Iteration 76/1000 | Loss: 0.00002474
Iteration 77/1000 | Loss: 0.00002474
Iteration 78/1000 | Loss: 0.00002474
Iteration 79/1000 | Loss: 0.00002474
Iteration 80/1000 | Loss: 0.00002474
Iteration 81/1000 | Loss: 0.00002473
Iteration 82/1000 | Loss: 0.00002473
Iteration 83/1000 | Loss: 0.00002473
Iteration 84/1000 | Loss: 0.00002473
Iteration 85/1000 | Loss: 0.00002473
Iteration 86/1000 | Loss: 0.00002473
Iteration 87/1000 | Loss: 0.00002473
Iteration 88/1000 | Loss: 0.00002473
Iteration 89/1000 | Loss: 0.00002472
Iteration 90/1000 | Loss: 0.00002472
Iteration 91/1000 | Loss: 0.00002472
Iteration 92/1000 | Loss: 0.00002472
Iteration 93/1000 | Loss: 0.00002471
Iteration 94/1000 | Loss: 0.00002471
Iteration 95/1000 | Loss: 0.00002471
Iteration 96/1000 | Loss: 0.00002470
Iteration 97/1000 | Loss: 0.00002470
Iteration 98/1000 | Loss: 0.00002469
Iteration 99/1000 | Loss: 0.00002469
Iteration 100/1000 | Loss: 0.00002469
Iteration 101/1000 | Loss: 0.00002469
Iteration 102/1000 | Loss: 0.00002468
Iteration 103/1000 | Loss: 0.00002468
Iteration 104/1000 | Loss: 0.00002468
Iteration 105/1000 | Loss: 0.00002468
Iteration 106/1000 | Loss: 0.00002468
Iteration 107/1000 | Loss: 0.00002467
Iteration 108/1000 | Loss: 0.00002467
Iteration 109/1000 | Loss: 0.00002467
Iteration 110/1000 | Loss: 0.00002467
Iteration 111/1000 | Loss: 0.00002467
Iteration 112/1000 | Loss: 0.00002466
Iteration 113/1000 | Loss: 0.00002466
Iteration 114/1000 | Loss: 0.00002466
Iteration 115/1000 | Loss: 0.00002466
Iteration 116/1000 | Loss: 0.00002465
Iteration 117/1000 | Loss: 0.00002465
Iteration 118/1000 | Loss: 0.00002465
Iteration 119/1000 | Loss: 0.00002465
Iteration 120/1000 | Loss: 0.00002465
Iteration 121/1000 | Loss: 0.00002465
Iteration 122/1000 | Loss: 0.00002465
Iteration 123/1000 | Loss: 0.00002465
Iteration 124/1000 | Loss: 0.00002465
Iteration 125/1000 | Loss: 0.00002465
Iteration 126/1000 | Loss: 0.00002465
Iteration 127/1000 | Loss: 0.00002465
Iteration 128/1000 | Loss: 0.00002465
Iteration 129/1000 | Loss: 0.00002465
Iteration 130/1000 | Loss: 0.00002465
Iteration 131/1000 | Loss: 0.00002465
Iteration 132/1000 | Loss: 0.00002465
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [2.4653509171912447e-05, 2.4653509171912447e-05, 2.4653509171912447e-05, 2.4653509171912447e-05, 2.4653509171912447e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4653509171912447e-05

Optimization complete. Final v2v error: 4.30104923248291 mm

Highest mean error: 4.605949878692627 mm for frame 126

Lowest mean error: 3.6798999309539795 mm for frame 36

Saving results

Total time: 69.65067267417908
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00783250
Iteration 2/25 | Loss: 0.00152864
Iteration 3/25 | Loss: 0.00142425
Iteration 4/25 | Loss: 0.00141465
Iteration 5/25 | Loss: 0.00141072
Iteration 6/25 | Loss: 0.00140985
Iteration 7/25 | Loss: 0.00140985
Iteration 8/25 | Loss: 0.00140985
Iteration 9/25 | Loss: 0.00140985
Iteration 10/25 | Loss: 0.00140985
Iteration 11/25 | Loss: 0.00140985
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014098469400778413, 0.0014098469400778413, 0.0014098469400778413, 0.0014098469400778413, 0.0014098469400778413]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014098469400778413

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11082649
Iteration 2/25 | Loss: 0.00093782
Iteration 3/25 | Loss: 0.00093779
Iteration 4/25 | Loss: 0.00093779
Iteration 5/25 | Loss: 0.00093779
Iteration 6/25 | Loss: 0.00093779
Iteration 7/25 | Loss: 0.00093779
Iteration 8/25 | Loss: 0.00093779
Iteration 9/25 | Loss: 0.00093779
Iteration 10/25 | Loss: 0.00093779
Iteration 11/25 | Loss: 0.00093779
Iteration 12/25 | Loss: 0.00093779
Iteration 13/25 | Loss: 0.00093779
Iteration 14/25 | Loss: 0.00093779
Iteration 15/25 | Loss: 0.00093779
Iteration 16/25 | Loss: 0.00093779
Iteration 17/25 | Loss: 0.00093779
Iteration 18/25 | Loss: 0.00093779
Iteration 19/25 | Loss: 0.00093779
Iteration 20/25 | Loss: 0.00093779
Iteration 21/25 | Loss: 0.00093779
Iteration 22/25 | Loss: 0.00093779
Iteration 23/25 | Loss: 0.00093779
Iteration 24/25 | Loss: 0.00093779
Iteration 25/25 | Loss: 0.00093779

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093779
Iteration 2/1000 | Loss: 0.00004497
Iteration 3/1000 | Loss: 0.00003428
Iteration 4/1000 | Loss: 0.00003138
Iteration 5/1000 | Loss: 0.00002995
Iteration 6/1000 | Loss: 0.00002926
Iteration 7/1000 | Loss: 0.00002861
Iteration 8/1000 | Loss: 0.00002813
Iteration 9/1000 | Loss: 0.00002762
Iteration 10/1000 | Loss: 0.00002720
Iteration 11/1000 | Loss: 0.00002696
Iteration 12/1000 | Loss: 0.00002670
Iteration 13/1000 | Loss: 0.00002646
Iteration 14/1000 | Loss: 0.00002635
Iteration 15/1000 | Loss: 0.00002632
Iteration 16/1000 | Loss: 0.00002631
Iteration 17/1000 | Loss: 0.00002631
Iteration 18/1000 | Loss: 0.00002630
Iteration 19/1000 | Loss: 0.00002630
Iteration 20/1000 | Loss: 0.00002629
Iteration 21/1000 | Loss: 0.00002628
Iteration 22/1000 | Loss: 0.00002627
Iteration 23/1000 | Loss: 0.00002626
Iteration 24/1000 | Loss: 0.00002626
Iteration 25/1000 | Loss: 0.00002625
Iteration 26/1000 | Loss: 0.00002624
Iteration 27/1000 | Loss: 0.00002623
Iteration 28/1000 | Loss: 0.00002615
Iteration 29/1000 | Loss: 0.00002615
Iteration 30/1000 | Loss: 0.00002615
Iteration 31/1000 | Loss: 0.00002615
Iteration 32/1000 | Loss: 0.00002615
Iteration 33/1000 | Loss: 0.00002615
Iteration 34/1000 | Loss: 0.00002613
Iteration 35/1000 | Loss: 0.00002612
Iteration 36/1000 | Loss: 0.00002612
Iteration 37/1000 | Loss: 0.00002611
Iteration 38/1000 | Loss: 0.00002611
Iteration 39/1000 | Loss: 0.00002610
Iteration 40/1000 | Loss: 0.00002610
Iteration 41/1000 | Loss: 0.00002606
Iteration 42/1000 | Loss: 0.00002595
Iteration 43/1000 | Loss: 0.00002587
Iteration 44/1000 | Loss: 0.00002583
Iteration 45/1000 | Loss: 0.00002583
Iteration 46/1000 | Loss: 0.00002583
Iteration 47/1000 | Loss: 0.00002583
Iteration 48/1000 | Loss: 0.00002583
Iteration 49/1000 | Loss: 0.00002583
Iteration 50/1000 | Loss: 0.00002583
Iteration 51/1000 | Loss: 0.00002582
Iteration 52/1000 | Loss: 0.00002581
Iteration 53/1000 | Loss: 0.00002581
Iteration 54/1000 | Loss: 0.00002581
Iteration 55/1000 | Loss: 0.00002580
Iteration 56/1000 | Loss: 0.00002580
Iteration 57/1000 | Loss: 0.00002579
Iteration 58/1000 | Loss: 0.00002579
Iteration 59/1000 | Loss: 0.00002579
Iteration 60/1000 | Loss: 0.00002579
Iteration 61/1000 | Loss: 0.00002579
Iteration 62/1000 | Loss: 0.00002579
Iteration 63/1000 | Loss: 0.00002579
Iteration 64/1000 | Loss: 0.00002579
Iteration 65/1000 | Loss: 0.00002579
Iteration 66/1000 | Loss: 0.00002578
Iteration 67/1000 | Loss: 0.00002578
Iteration 68/1000 | Loss: 0.00002578
Iteration 69/1000 | Loss: 0.00002578
Iteration 70/1000 | Loss: 0.00002578
Iteration 71/1000 | Loss: 0.00002578
Iteration 72/1000 | Loss: 0.00002578
Iteration 73/1000 | Loss: 0.00002577
Iteration 74/1000 | Loss: 0.00002577
Iteration 75/1000 | Loss: 0.00002576
Iteration 76/1000 | Loss: 0.00002576
Iteration 77/1000 | Loss: 0.00002576
Iteration 78/1000 | Loss: 0.00002575
Iteration 79/1000 | Loss: 0.00002575
Iteration 80/1000 | Loss: 0.00002575
Iteration 81/1000 | Loss: 0.00002575
Iteration 82/1000 | Loss: 0.00002575
Iteration 83/1000 | Loss: 0.00002574
Iteration 84/1000 | Loss: 0.00002574
Iteration 85/1000 | Loss: 0.00002574
Iteration 86/1000 | Loss: 0.00002573
Iteration 87/1000 | Loss: 0.00002573
Iteration 88/1000 | Loss: 0.00002573
Iteration 89/1000 | Loss: 0.00002573
Iteration 90/1000 | Loss: 0.00002573
Iteration 91/1000 | Loss: 0.00002573
Iteration 92/1000 | Loss: 0.00002573
Iteration 93/1000 | Loss: 0.00002573
Iteration 94/1000 | Loss: 0.00002573
Iteration 95/1000 | Loss: 0.00002573
Iteration 96/1000 | Loss: 0.00002573
Iteration 97/1000 | Loss: 0.00002573
Iteration 98/1000 | Loss: 0.00002572
Iteration 99/1000 | Loss: 0.00002572
Iteration 100/1000 | Loss: 0.00002572
Iteration 101/1000 | Loss: 0.00002572
Iteration 102/1000 | Loss: 0.00002572
Iteration 103/1000 | Loss: 0.00002572
Iteration 104/1000 | Loss: 0.00002572
Iteration 105/1000 | Loss: 0.00002572
Iteration 106/1000 | Loss: 0.00002572
Iteration 107/1000 | Loss: 0.00002572
Iteration 108/1000 | Loss: 0.00002571
Iteration 109/1000 | Loss: 0.00002571
Iteration 110/1000 | Loss: 0.00002571
Iteration 111/1000 | Loss: 0.00002571
Iteration 112/1000 | Loss: 0.00002571
Iteration 113/1000 | Loss: 0.00002571
Iteration 114/1000 | Loss: 0.00002571
Iteration 115/1000 | Loss: 0.00002570
Iteration 116/1000 | Loss: 0.00002570
Iteration 117/1000 | Loss: 0.00002570
Iteration 118/1000 | Loss: 0.00002570
Iteration 119/1000 | Loss: 0.00002570
Iteration 120/1000 | Loss: 0.00002570
Iteration 121/1000 | Loss: 0.00002570
Iteration 122/1000 | Loss: 0.00002570
Iteration 123/1000 | Loss: 0.00002569
Iteration 124/1000 | Loss: 0.00002569
Iteration 125/1000 | Loss: 0.00002569
Iteration 126/1000 | Loss: 0.00002569
Iteration 127/1000 | Loss: 0.00002569
Iteration 128/1000 | Loss: 0.00002569
Iteration 129/1000 | Loss: 0.00002569
Iteration 130/1000 | Loss: 0.00002568
Iteration 131/1000 | Loss: 0.00002568
Iteration 132/1000 | Loss: 0.00002568
Iteration 133/1000 | Loss: 0.00002568
Iteration 134/1000 | Loss: 0.00002568
Iteration 135/1000 | Loss: 0.00002568
Iteration 136/1000 | Loss: 0.00002568
Iteration 137/1000 | Loss: 0.00002567
Iteration 138/1000 | Loss: 0.00002567
Iteration 139/1000 | Loss: 0.00002567
Iteration 140/1000 | Loss: 0.00002567
Iteration 141/1000 | Loss: 0.00002567
Iteration 142/1000 | Loss: 0.00002567
Iteration 143/1000 | Loss: 0.00002567
Iteration 144/1000 | Loss: 0.00002567
Iteration 145/1000 | Loss: 0.00002567
Iteration 146/1000 | Loss: 0.00002567
Iteration 147/1000 | Loss: 0.00002567
Iteration 148/1000 | Loss: 0.00002567
Iteration 149/1000 | Loss: 0.00002567
Iteration 150/1000 | Loss: 0.00002567
Iteration 151/1000 | Loss: 0.00002567
Iteration 152/1000 | Loss: 0.00002567
Iteration 153/1000 | Loss: 0.00002567
Iteration 154/1000 | Loss: 0.00002567
Iteration 155/1000 | Loss: 0.00002567
Iteration 156/1000 | Loss: 0.00002567
Iteration 157/1000 | Loss: 0.00002567
Iteration 158/1000 | Loss: 0.00002567
Iteration 159/1000 | Loss: 0.00002567
Iteration 160/1000 | Loss: 0.00002567
Iteration 161/1000 | Loss: 0.00002567
Iteration 162/1000 | Loss: 0.00002567
Iteration 163/1000 | Loss: 0.00002567
Iteration 164/1000 | Loss: 0.00002567
Iteration 165/1000 | Loss: 0.00002567
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [2.5666478904895484e-05, 2.5666478904895484e-05, 2.5666478904895484e-05, 2.5666478904895484e-05, 2.5666478904895484e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5666478904895484e-05

Optimization complete. Final v2v error: 4.188126564025879 mm

Highest mean error: 4.368417263031006 mm for frame 97

Lowest mean error: 3.9753658771514893 mm for frame 2

Saving results

Total time: 40.737499475479126
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00951145
Iteration 2/25 | Loss: 0.00356846
Iteration 3/25 | Loss: 0.00272500
Iteration 4/25 | Loss: 0.00256644
Iteration 5/25 | Loss: 0.00226802
Iteration 6/25 | Loss: 0.00224094
Iteration 7/25 | Loss: 0.00211867
Iteration 8/25 | Loss: 0.00205377
Iteration 9/25 | Loss: 0.00204736
Iteration 10/25 | Loss: 0.00197226
Iteration 11/25 | Loss: 0.00195415
Iteration 12/25 | Loss: 0.00193100
Iteration 13/25 | Loss: 0.00191167
Iteration 14/25 | Loss: 0.00192765
Iteration 15/25 | Loss: 0.00190711
Iteration 16/25 | Loss: 0.00191675
Iteration 17/25 | Loss: 0.00191259
Iteration 18/25 | Loss: 0.00189058
Iteration 19/25 | Loss: 0.00188878
Iteration 20/25 | Loss: 0.00188741
Iteration 21/25 | Loss: 0.00188921
Iteration 22/25 | Loss: 0.00191380
Iteration 23/25 | Loss: 0.00189204
Iteration 24/25 | Loss: 0.00189007
Iteration 25/25 | Loss: 0.00187870

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29980707
Iteration 2/25 | Loss: 0.00494708
Iteration 3/25 | Loss: 0.00377588
Iteration 4/25 | Loss: 0.00377588
Iteration 5/25 | Loss: 0.00377588
Iteration 6/25 | Loss: 0.00377588
Iteration 7/25 | Loss: 0.00377588
Iteration 8/25 | Loss: 0.00377588
Iteration 9/25 | Loss: 0.00377588
Iteration 10/25 | Loss: 0.00377588
Iteration 11/25 | Loss: 0.00377588
Iteration 12/25 | Loss: 0.00377588
Iteration 13/25 | Loss: 0.00377588
Iteration 14/25 | Loss: 0.00377588
Iteration 15/25 | Loss: 0.00377588
Iteration 16/25 | Loss: 0.00377588
Iteration 17/25 | Loss: 0.00377588
Iteration 18/25 | Loss: 0.00377588
Iteration 19/25 | Loss: 0.00377588
Iteration 20/25 | Loss: 0.00377588
Iteration 21/25 | Loss: 0.00377588
Iteration 22/25 | Loss: 0.00377588
Iteration 23/25 | Loss: 0.00377588
Iteration 24/25 | Loss: 0.00377588
Iteration 25/25 | Loss: 0.00377588

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00377588
Iteration 2/1000 | Loss: 0.00076627
Iteration 3/1000 | Loss: 0.00117966
Iteration 4/1000 | Loss: 0.00104265
Iteration 5/1000 | Loss: 0.00124671
Iteration 6/1000 | Loss: 0.00069214
Iteration 7/1000 | Loss: 0.00146887
Iteration 8/1000 | Loss: 0.00033478
Iteration 9/1000 | Loss: 0.00031473
Iteration 10/1000 | Loss: 0.00029777
Iteration 11/1000 | Loss: 0.00030881
Iteration 12/1000 | Loss: 0.00027755
Iteration 13/1000 | Loss: 0.00637428
Iteration 14/1000 | Loss: 0.00347037
Iteration 15/1000 | Loss: 0.00137595
Iteration 16/1000 | Loss: 0.00061677
Iteration 17/1000 | Loss: 0.00089607
Iteration 18/1000 | Loss: 0.00063232
Iteration 19/1000 | Loss: 0.00172197
Iteration 20/1000 | Loss: 0.00305534
Iteration 21/1000 | Loss: 0.00108368
Iteration 22/1000 | Loss: 0.00092870
Iteration 23/1000 | Loss: 0.00039887
Iteration 24/1000 | Loss: 0.00007054
Iteration 25/1000 | Loss: 0.00027518
Iteration 26/1000 | Loss: 0.00030867
Iteration 27/1000 | Loss: 0.00047236
Iteration 28/1000 | Loss: 0.00023570
Iteration 29/1000 | Loss: 0.00117929
Iteration 30/1000 | Loss: 0.00007398
Iteration 31/1000 | Loss: 0.00060171
Iteration 32/1000 | Loss: 0.00356388
Iteration 33/1000 | Loss: 0.00007679
Iteration 34/1000 | Loss: 0.00020563
Iteration 35/1000 | Loss: 0.00002141
Iteration 36/1000 | Loss: 0.00008956
Iteration 37/1000 | Loss: 0.00016486
Iteration 38/1000 | Loss: 0.00001817
Iteration 39/1000 | Loss: 0.00014437
Iteration 40/1000 | Loss: 0.00009841
Iteration 41/1000 | Loss: 0.00001659
Iteration 42/1000 | Loss: 0.00006047
Iteration 43/1000 | Loss: 0.00004387
Iteration 44/1000 | Loss: 0.00022772
Iteration 45/1000 | Loss: 0.00004034
Iteration 46/1000 | Loss: 0.00001500
Iteration 47/1000 | Loss: 0.00002545
Iteration 48/1000 | Loss: 0.00001469
Iteration 49/1000 | Loss: 0.00001798
Iteration 50/1000 | Loss: 0.00001453
Iteration 51/1000 | Loss: 0.00012341
Iteration 52/1000 | Loss: 0.00003749
Iteration 53/1000 | Loss: 0.00004982
Iteration 54/1000 | Loss: 0.00018056
Iteration 55/1000 | Loss: 0.00026217
Iteration 56/1000 | Loss: 0.00003786
Iteration 57/1000 | Loss: 0.00001419
Iteration 58/1000 | Loss: 0.00001408
Iteration 59/1000 | Loss: 0.00001400
Iteration 60/1000 | Loss: 0.00001400
Iteration 61/1000 | Loss: 0.00001399
Iteration 62/1000 | Loss: 0.00001399
Iteration 63/1000 | Loss: 0.00001399
Iteration 64/1000 | Loss: 0.00001399
Iteration 65/1000 | Loss: 0.00001399
Iteration 66/1000 | Loss: 0.00001399
Iteration 67/1000 | Loss: 0.00001399
Iteration 68/1000 | Loss: 0.00001399
Iteration 69/1000 | Loss: 0.00001399
Iteration 70/1000 | Loss: 0.00001399
Iteration 71/1000 | Loss: 0.00001399
Iteration 72/1000 | Loss: 0.00001398
Iteration 73/1000 | Loss: 0.00001398
Iteration 74/1000 | Loss: 0.00001398
Iteration 75/1000 | Loss: 0.00001398
Iteration 76/1000 | Loss: 0.00001398
Iteration 77/1000 | Loss: 0.00001397
Iteration 78/1000 | Loss: 0.00001396
Iteration 79/1000 | Loss: 0.00001396
Iteration 80/1000 | Loss: 0.00001395
Iteration 81/1000 | Loss: 0.00001395
Iteration 82/1000 | Loss: 0.00001394
Iteration 83/1000 | Loss: 0.00001394
Iteration 84/1000 | Loss: 0.00001393
Iteration 85/1000 | Loss: 0.00001393
Iteration 86/1000 | Loss: 0.00001392
Iteration 87/1000 | Loss: 0.00001392
Iteration 88/1000 | Loss: 0.00001392
Iteration 89/1000 | Loss: 0.00001392
Iteration 90/1000 | Loss: 0.00001392
Iteration 91/1000 | Loss: 0.00001392
Iteration 92/1000 | Loss: 0.00001392
Iteration 93/1000 | Loss: 0.00001392
Iteration 94/1000 | Loss: 0.00001391
Iteration 95/1000 | Loss: 0.00001391
Iteration 96/1000 | Loss: 0.00001391
Iteration 97/1000 | Loss: 0.00001391
Iteration 98/1000 | Loss: 0.00001391
Iteration 99/1000 | Loss: 0.00001391
Iteration 100/1000 | Loss: 0.00001391
Iteration 101/1000 | Loss: 0.00001391
Iteration 102/1000 | Loss: 0.00001391
Iteration 103/1000 | Loss: 0.00001391
Iteration 104/1000 | Loss: 0.00001391
Iteration 105/1000 | Loss: 0.00001391
Iteration 106/1000 | Loss: 0.00001391
Iteration 107/1000 | Loss: 0.00001391
Iteration 108/1000 | Loss: 0.00001390
Iteration 109/1000 | Loss: 0.00001390
Iteration 110/1000 | Loss: 0.00001390
Iteration 111/1000 | Loss: 0.00001390
Iteration 112/1000 | Loss: 0.00001390
Iteration 113/1000 | Loss: 0.00001389
Iteration 114/1000 | Loss: 0.00001389
Iteration 115/1000 | Loss: 0.00001389
Iteration 116/1000 | Loss: 0.00001389
Iteration 117/1000 | Loss: 0.00001389
Iteration 118/1000 | Loss: 0.00001389
Iteration 119/1000 | Loss: 0.00006459
Iteration 120/1000 | Loss: 0.00001406
Iteration 121/1000 | Loss: 0.00001388
Iteration 122/1000 | Loss: 0.00001388
Iteration 123/1000 | Loss: 0.00001388
Iteration 124/1000 | Loss: 0.00001388
Iteration 125/1000 | Loss: 0.00001388
Iteration 126/1000 | Loss: 0.00001388
Iteration 127/1000 | Loss: 0.00001387
Iteration 128/1000 | Loss: 0.00001387
Iteration 129/1000 | Loss: 0.00007158
Iteration 130/1000 | Loss: 0.00044841
Iteration 131/1000 | Loss: 0.00008327
Iteration 132/1000 | Loss: 0.00001396
Iteration 133/1000 | Loss: 0.00001393
Iteration 134/1000 | Loss: 0.00004601
Iteration 135/1000 | Loss: 0.00001391
Iteration 136/1000 | Loss: 0.00001387
Iteration 137/1000 | Loss: 0.00001386
Iteration 138/1000 | Loss: 0.00001386
Iteration 139/1000 | Loss: 0.00001386
Iteration 140/1000 | Loss: 0.00004025
Iteration 141/1000 | Loss: 0.00001392
Iteration 142/1000 | Loss: 0.00001386
Iteration 143/1000 | Loss: 0.00001386
Iteration 144/1000 | Loss: 0.00001385
Iteration 145/1000 | Loss: 0.00001385
Iteration 146/1000 | Loss: 0.00001384
Iteration 147/1000 | Loss: 0.00001383
Iteration 148/1000 | Loss: 0.00001383
Iteration 149/1000 | Loss: 0.00001383
Iteration 150/1000 | Loss: 0.00001383
Iteration 151/1000 | Loss: 0.00001383
Iteration 152/1000 | Loss: 0.00001383
Iteration 153/1000 | Loss: 0.00001383
Iteration 154/1000 | Loss: 0.00001383
Iteration 155/1000 | Loss: 0.00001383
Iteration 156/1000 | Loss: 0.00001383
Iteration 157/1000 | Loss: 0.00001383
Iteration 158/1000 | Loss: 0.00001383
Iteration 159/1000 | Loss: 0.00001382
Iteration 160/1000 | Loss: 0.00001382
Iteration 161/1000 | Loss: 0.00001382
Iteration 162/1000 | Loss: 0.00001382
Iteration 163/1000 | Loss: 0.00001382
Iteration 164/1000 | Loss: 0.00001382
Iteration 165/1000 | Loss: 0.00001382
Iteration 166/1000 | Loss: 0.00001382
Iteration 167/1000 | Loss: 0.00001382
Iteration 168/1000 | Loss: 0.00001382
Iteration 169/1000 | Loss: 0.00001382
Iteration 170/1000 | Loss: 0.00001381
Iteration 171/1000 | Loss: 0.00001381
Iteration 172/1000 | Loss: 0.00001381
Iteration 173/1000 | Loss: 0.00001381
Iteration 174/1000 | Loss: 0.00001381
Iteration 175/1000 | Loss: 0.00001381
Iteration 176/1000 | Loss: 0.00001381
Iteration 177/1000 | Loss: 0.00001381
Iteration 178/1000 | Loss: 0.00001381
Iteration 179/1000 | Loss: 0.00001381
Iteration 180/1000 | Loss: 0.00001381
Iteration 181/1000 | Loss: 0.00001381
Iteration 182/1000 | Loss: 0.00001381
Iteration 183/1000 | Loss: 0.00001381
Iteration 184/1000 | Loss: 0.00001381
Iteration 185/1000 | Loss: 0.00001381
Iteration 186/1000 | Loss: 0.00001381
Iteration 187/1000 | Loss: 0.00001380
Iteration 188/1000 | Loss: 0.00001380
Iteration 189/1000 | Loss: 0.00001380
Iteration 190/1000 | Loss: 0.00001380
Iteration 191/1000 | Loss: 0.00001380
Iteration 192/1000 | Loss: 0.00001380
Iteration 193/1000 | Loss: 0.00001380
Iteration 194/1000 | Loss: 0.00001380
Iteration 195/1000 | Loss: 0.00001380
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [1.3804838999931235e-05, 1.3804838999931235e-05, 1.3804838999931235e-05, 1.3804838999931235e-05, 1.3804838999931235e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3804838999931235e-05

Optimization complete. Final v2v error: 3.181995153427124 mm

Highest mean error: 3.4517767429351807 mm for frame 141

Lowest mean error: 2.9311819076538086 mm for frame 46

Saving results

Total time: 141.68266534805298
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00414553
Iteration 2/25 | Loss: 0.00142719
Iteration 3/25 | Loss: 0.00124068
Iteration 4/25 | Loss: 0.00121937
Iteration 5/25 | Loss: 0.00121404
Iteration 6/25 | Loss: 0.00121322
Iteration 7/25 | Loss: 0.00121322
Iteration 8/25 | Loss: 0.00121322
Iteration 9/25 | Loss: 0.00121322
Iteration 10/25 | Loss: 0.00121322
Iteration 11/25 | Loss: 0.00121322
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012132154079154134, 0.0012132154079154134, 0.0012132154079154134, 0.0012132154079154134, 0.0012132154079154134]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012132154079154134

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41586876
Iteration 2/25 | Loss: 0.00117981
Iteration 3/25 | Loss: 0.00117981
Iteration 4/25 | Loss: 0.00117981
Iteration 5/25 | Loss: 0.00117981
Iteration 6/25 | Loss: 0.00117980
Iteration 7/25 | Loss: 0.00117980
Iteration 8/25 | Loss: 0.00117980
Iteration 9/25 | Loss: 0.00117980
Iteration 10/25 | Loss: 0.00117980
Iteration 11/25 | Loss: 0.00117980
Iteration 12/25 | Loss: 0.00117980
Iteration 13/25 | Loss: 0.00117980
Iteration 14/25 | Loss: 0.00117980
Iteration 15/25 | Loss: 0.00117980
Iteration 16/25 | Loss: 0.00117980
Iteration 17/25 | Loss: 0.00117980
Iteration 18/25 | Loss: 0.00117980
Iteration 19/25 | Loss: 0.00117980
Iteration 20/25 | Loss: 0.00117980
Iteration 21/25 | Loss: 0.00117980
Iteration 22/25 | Loss: 0.00117980
Iteration 23/25 | Loss: 0.00117980
Iteration 24/25 | Loss: 0.00117980
Iteration 25/25 | Loss: 0.00117980

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117980
Iteration 2/1000 | Loss: 0.00002736
Iteration 3/1000 | Loss: 0.00002044
Iteration 4/1000 | Loss: 0.00001906
Iteration 5/1000 | Loss: 0.00001808
Iteration 6/1000 | Loss: 0.00001746
Iteration 7/1000 | Loss: 0.00001713
Iteration 8/1000 | Loss: 0.00001686
Iteration 9/1000 | Loss: 0.00001678
Iteration 10/1000 | Loss: 0.00001660
Iteration 11/1000 | Loss: 0.00001647
Iteration 12/1000 | Loss: 0.00001634
Iteration 13/1000 | Loss: 0.00001628
Iteration 14/1000 | Loss: 0.00001621
Iteration 15/1000 | Loss: 0.00001616
Iteration 16/1000 | Loss: 0.00001608
Iteration 17/1000 | Loss: 0.00001604
Iteration 18/1000 | Loss: 0.00001604
Iteration 19/1000 | Loss: 0.00001599
Iteration 20/1000 | Loss: 0.00001599
Iteration 21/1000 | Loss: 0.00001598
Iteration 22/1000 | Loss: 0.00001598
Iteration 23/1000 | Loss: 0.00001597
Iteration 24/1000 | Loss: 0.00001597
Iteration 25/1000 | Loss: 0.00001597
Iteration 26/1000 | Loss: 0.00001592
Iteration 27/1000 | Loss: 0.00001590
Iteration 28/1000 | Loss: 0.00001588
Iteration 29/1000 | Loss: 0.00001588
Iteration 30/1000 | Loss: 0.00001587
Iteration 31/1000 | Loss: 0.00001586
Iteration 32/1000 | Loss: 0.00001586
Iteration 33/1000 | Loss: 0.00001585
Iteration 34/1000 | Loss: 0.00001584
Iteration 35/1000 | Loss: 0.00001584
Iteration 36/1000 | Loss: 0.00001583
Iteration 37/1000 | Loss: 0.00001583
Iteration 38/1000 | Loss: 0.00001582
Iteration 39/1000 | Loss: 0.00001579
Iteration 40/1000 | Loss: 0.00001577
Iteration 41/1000 | Loss: 0.00001576
Iteration 42/1000 | Loss: 0.00001576
Iteration 43/1000 | Loss: 0.00001575
Iteration 44/1000 | Loss: 0.00001575
Iteration 45/1000 | Loss: 0.00001575
Iteration 46/1000 | Loss: 0.00001574
Iteration 47/1000 | Loss: 0.00001574
Iteration 48/1000 | Loss: 0.00001573
Iteration 49/1000 | Loss: 0.00001573
Iteration 50/1000 | Loss: 0.00001573
Iteration 51/1000 | Loss: 0.00001573
Iteration 52/1000 | Loss: 0.00001572
Iteration 53/1000 | Loss: 0.00001572
Iteration 54/1000 | Loss: 0.00001572
Iteration 55/1000 | Loss: 0.00001572
Iteration 56/1000 | Loss: 0.00001571
Iteration 57/1000 | Loss: 0.00001569
Iteration 58/1000 | Loss: 0.00001568
Iteration 59/1000 | Loss: 0.00001568
Iteration 60/1000 | Loss: 0.00001568
Iteration 61/1000 | Loss: 0.00001568
Iteration 62/1000 | Loss: 0.00001568
Iteration 63/1000 | Loss: 0.00001568
Iteration 64/1000 | Loss: 0.00001568
Iteration 65/1000 | Loss: 0.00001568
Iteration 66/1000 | Loss: 0.00001568
Iteration 67/1000 | Loss: 0.00001567
Iteration 68/1000 | Loss: 0.00001567
Iteration 69/1000 | Loss: 0.00001566
Iteration 70/1000 | Loss: 0.00001565
Iteration 71/1000 | Loss: 0.00001565
Iteration 72/1000 | Loss: 0.00001565
Iteration 73/1000 | Loss: 0.00001565
Iteration 74/1000 | Loss: 0.00001564
Iteration 75/1000 | Loss: 0.00001564
Iteration 76/1000 | Loss: 0.00001564
Iteration 77/1000 | Loss: 0.00001564
Iteration 78/1000 | Loss: 0.00001563
Iteration 79/1000 | Loss: 0.00001563
Iteration 80/1000 | Loss: 0.00001563
Iteration 81/1000 | Loss: 0.00001562
Iteration 82/1000 | Loss: 0.00001562
Iteration 83/1000 | Loss: 0.00001562
Iteration 84/1000 | Loss: 0.00001562
Iteration 85/1000 | Loss: 0.00001562
Iteration 86/1000 | Loss: 0.00001562
Iteration 87/1000 | Loss: 0.00001562
Iteration 88/1000 | Loss: 0.00001562
Iteration 89/1000 | Loss: 0.00001562
Iteration 90/1000 | Loss: 0.00001562
Iteration 91/1000 | Loss: 0.00001561
Iteration 92/1000 | Loss: 0.00001561
Iteration 93/1000 | Loss: 0.00001561
Iteration 94/1000 | Loss: 0.00001561
Iteration 95/1000 | Loss: 0.00001561
Iteration 96/1000 | Loss: 0.00001560
Iteration 97/1000 | Loss: 0.00001560
Iteration 98/1000 | Loss: 0.00001560
Iteration 99/1000 | Loss: 0.00001560
Iteration 100/1000 | Loss: 0.00001560
Iteration 101/1000 | Loss: 0.00001560
Iteration 102/1000 | Loss: 0.00001560
Iteration 103/1000 | Loss: 0.00001560
Iteration 104/1000 | Loss: 0.00001559
Iteration 105/1000 | Loss: 0.00001559
Iteration 106/1000 | Loss: 0.00001559
Iteration 107/1000 | Loss: 0.00001559
Iteration 108/1000 | Loss: 0.00001559
Iteration 109/1000 | Loss: 0.00001558
Iteration 110/1000 | Loss: 0.00001558
Iteration 111/1000 | Loss: 0.00001558
Iteration 112/1000 | Loss: 0.00001558
Iteration 113/1000 | Loss: 0.00001558
Iteration 114/1000 | Loss: 0.00001558
Iteration 115/1000 | Loss: 0.00001558
Iteration 116/1000 | Loss: 0.00001558
Iteration 117/1000 | Loss: 0.00001558
Iteration 118/1000 | Loss: 0.00001558
Iteration 119/1000 | Loss: 0.00001558
Iteration 120/1000 | Loss: 0.00001558
Iteration 121/1000 | Loss: 0.00001558
Iteration 122/1000 | Loss: 0.00001558
Iteration 123/1000 | Loss: 0.00001558
Iteration 124/1000 | Loss: 0.00001558
Iteration 125/1000 | Loss: 0.00001558
Iteration 126/1000 | Loss: 0.00001558
Iteration 127/1000 | Loss: 0.00001558
Iteration 128/1000 | Loss: 0.00001558
Iteration 129/1000 | Loss: 0.00001558
Iteration 130/1000 | Loss: 0.00001558
Iteration 131/1000 | Loss: 0.00001558
Iteration 132/1000 | Loss: 0.00001558
Iteration 133/1000 | Loss: 0.00001558
Iteration 134/1000 | Loss: 0.00001558
Iteration 135/1000 | Loss: 0.00001558
Iteration 136/1000 | Loss: 0.00001558
Iteration 137/1000 | Loss: 0.00001558
Iteration 138/1000 | Loss: 0.00001558
Iteration 139/1000 | Loss: 0.00001558
Iteration 140/1000 | Loss: 0.00001558
Iteration 141/1000 | Loss: 0.00001558
Iteration 142/1000 | Loss: 0.00001558
Iteration 143/1000 | Loss: 0.00001558
Iteration 144/1000 | Loss: 0.00001558
Iteration 145/1000 | Loss: 0.00001558
Iteration 146/1000 | Loss: 0.00001558
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.5577121303067543e-05, 1.5577121303067543e-05, 1.5577121303067543e-05, 1.5577121303067543e-05, 1.5577121303067543e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5577121303067543e-05

Optimization complete. Final v2v error: 3.301339626312256 mm

Highest mean error: 3.849086046218872 mm for frame 93

Lowest mean error: 2.6754791736602783 mm for frame 198

Saving results

Total time: 39.9865300655365
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00945513
Iteration 2/25 | Loss: 0.00153858
Iteration 3/25 | Loss: 0.00133969
Iteration 4/25 | Loss: 0.00132400
Iteration 5/25 | Loss: 0.00131957
Iteration 6/25 | Loss: 0.00131830
Iteration 7/25 | Loss: 0.00131830
Iteration 8/25 | Loss: 0.00131830
Iteration 9/25 | Loss: 0.00131830
Iteration 10/25 | Loss: 0.00131830
Iteration 11/25 | Loss: 0.00131830
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013182992115616798, 0.0013182992115616798, 0.0013182992115616798, 0.0013182992115616798, 0.0013182992115616798]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013182992115616798

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33186984
Iteration 2/25 | Loss: 0.00127397
Iteration 3/25 | Loss: 0.00127396
Iteration 4/25 | Loss: 0.00127396
Iteration 5/25 | Loss: 0.00127396
Iteration 6/25 | Loss: 0.00127396
Iteration 7/25 | Loss: 0.00127396
Iteration 8/25 | Loss: 0.00127396
Iteration 9/25 | Loss: 0.00127396
Iteration 10/25 | Loss: 0.00127396
Iteration 11/25 | Loss: 0.00127396
Iteration 12/25 | Loss: 0.00127396
Iteration 13/25 | Loss: 0.00127396
Iteration 14/25 | Loss: 0.00127396
Iteration 15/25 | Loss: 0.00127396
Iteration 16/25 | Loss: 0.00127396
Iteration 17/25 | Loss: 0.00127396
Iteration 18/25 | Loss: 0.00127396
Iteration 19/25 | Loss: 0.00127396
Iteration 20/25 | Loss: 0.00127396
Iteration 21/25 | Loss: 0.00127396
Iteration 22/25 | Loss: 0.00127396
Iteration 23/25 | Loss: 0.00127396
Iteration 24/25 | Loss: 0.00127396
Iteration 25/25 | Loss: 0.00127396

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127396
Iteration 2/1000 | Loss: 0.00003906
Iteration 3/1000 | Loss: 0.00003112
Iteration 4/1000 | Loss: 0.00002806
Iteration 5/1000 | Loss: 0.00002683
Iteration 6/1000 | Loss: 0.00002544
Iteration 7/1000 | Loss: 0.00002456
Iteration 8/1000 | Loss: 0.00002393
Iteration 9/1000 | Loss: 0.00002339
Iteration 10/1000 | Loss: 0.00002302
Iteration 11/1000 | Loss: 0.00002271
Iteration 12/1000 | Loss: 0.00002248
Iteration 13/1000 | Loss: 0.00002233
Iteration 14/1000 | Loss: 0.00002232
Iteration 15/1000 | Loss: 0.00002232
Iteration 16/1000 | Loss: 0.00002231
Iteration 17/1000 | Loss: 0.00002231
Iteration 18/1000 | Loss: 0.00002230
Iteration 19/1000 | Loss: 0.00002230
Iteration 20/1000 | Loss: 0.00002227
Iteration 21/1000 | Loss: 0.00002227
Iteration 22/1000 | Loss: 0.00002227
Iteration 23/1000 | Loss: 0.00002227
Iteration 24/1000 | Loss: 0.00002227
Iteration 25/1000 | Loss: 0.00002227
Iteration 26/1000 | Loss: 0.00002226
Iteration 27/1000 | Loss: 0.00002223
Iteration 28/1000 | Loss: 0.00002223
Iteration 29/1000 | Loss: 0.00002223
Iteration 30/1000 | Loss: 0.00002222
Iteration 31/1000 | Loss: 0.00002222
Iteration 32/1000 | Loss: 0.00002220
Iteration 33/1000 | Loss: 0.00002220
Iteration 34/1000 | Loss: 0.00002219
Iteration 35/1000 | Loss: 0.00002218
Iteration 36/1000 | Loss: 0.00002218
Iteration 37/1000 | Loss: 0.00002217
Iteration 38/1000 | Loss: 0.00002217
Iteration 39/1000 | Loss: 0.00002216
Iteration 40/1000 | Loss: 0.00002213
Iteration 41/1000 | Loss: 0.00002212
Iteration 42/1000 | Loss: 0.00002212
Iteration 43/1000 | Loss: 0.00002212
Iteration 44/1000 | Loss: 0.00002212
Iteration 45/1000 | Loss: 0.00002212
Iteration 46/1000 | Loss: 0.00002212
Iteration 47/1000 | Loss: 0.00002211
Iteration 48/1000 | Loss: 0.00002211
Iteration 49/1000 | Loss: 0.00002211
Iteration 50/1000 | Loss: 0.00002210
Iteration 51/1000 | Loss: 0.00002210
Iteration 52/1000 | Loss: 0.00002210
Iteration 53/1000 | Loss: 0.00002209
Iteration 54/1000 | Loss: 0.00002209
Iteration 55/1000 | Loss: 0.00002209
Iteration 56/1000 | Loss: 0.00002209
Iteration 57/1000 | Loss: 0.00002208
Iteration 58/1000 | Loss: 0.00002208
Iteration 59/1000 | Loss: 0.00002208
Iteration 60/1000 | Loss: 0.00002208
Iteration 61/1000 | Loss: 0.00002207
Iteration 62/1000 | Loss: 0.00002207
Iteration 63/1000 | Loss: 0.00002207
Iteration 64/1000 | Loss: 0.00002207
Iteration 65/1000 | Loss: 0.00002207
Iteration 66/1000 | Loss: 0.00002207
Iteration 67/1000 | Loss: 0.00002207
Iteration 68/1000 | Loss: 0.00002207
Iteration 69/1000 | Loss: 0.00002207
Iteration 70/1000 | Loss: 0.00002206
Iteration 71/1000 | Loss: 0.00002206
Iteration 72/1000 | Loss: 0.00002206
Iteration 73/1000 | Loss: 0.00002206
Iteration 74/1000 | Loss: 0.00002206
Iteration 75/1000 | Loss: 0.00002206
Iteration 76/1000 | Loss: 0.00002206
Iteration 77/1000 | Loss: 0.00002206
Iteration 78/1000 | Loss: 0.00002206
Iteration 79/1000 | Loss: 0.00002206
Iteration 80/1000 | Loss: 0.00002206
Iteration 81/1000 | Loss: 0.00002206
Iteration 82/1000 | Loss: 0.00002206
Iteration 83/1000 | Loss: 0.00002206
Iteration 84/1000 | Loss: 0.00002205
Iteration 85/1000 | Loss: 0.00002205
Iteration 86/1000 | Loss: 0.00002205
Iteration 87/1000 | Loss: 0.00002205
Iteration 88/1000 | Loss: 0.00002205
Iteration 89/1000 | Loss: 0.00002205
Iteration 90/1000 | Loss: 0.00002204
Iteration 91/1000 | Loss: 0.00002204
Iteration 92/1000 | Loss: 0.00002204
Iteration 93/1000 | Loss: 0.00002204
Iteration 94/1000 | Loss: 0.00002204
Iteration 95/1000 | Loss: 0.00002204
Iteration 96/1000 | Loss: 0.00002204
Iteration 97/1000 | Loss: 0.00002204
Iteration 98/1000 | Loss: 0.00002204
Iteration 99/1000 | Loss: 0.00002204
Iteration 100/1000 | Loss: 0.00002204
Iteration 101/1000 | Loss: 0.00002204
Iteration 102/1000 | Loss: 0.00002204
Iteration 103/1000 | Loss: 0.00002203
Iteration 104/1000 | Loss: 0.00002203
Iteration 105/1000 | Loss: 0.00002203
Iteration 106/1000 | Loss: 0.00002202
Iteration 107/1000 | Loss: 0.00002202
Iteration 108/1000 | Loss: 0.00002202
Iteration 109/1000 | Loss: 0.00002202
Iteration 110/1000 | Loss: 0.00002202
Iteration 111/1000 | Loss: 0.00002202
Iteration 112/1000 | Loss: 0.00002202
Iteration 113/1000 | Loss: 0.00002202
Iteration 114/1000 | Loss: 0.00002202
Iteration 115/1000 | Loss: 0.00002202
Iteration 116/1000 | Loss: 0.00002201
Iteration 117/1000 | Loss: 0.00002201
Iteration 118/1000 | Loss: 0.00002201
Iteration 119/1000 | Loss: 0.00002201
Iteration 120/1000 | Loss: 0.00002201
Iteration 121/1000 | Loss: 0.00002201
Iteration 122/1000 | Loss: 0.00002201
Iteration 123/1000 | Loss: 0.00002200
Iteration 124/1000 | Loss: 0.00002200
Iteration 125/1000 | Loss: 0.00002200
Iteration 126/1000 | Loss: 0.00002200
Iteration 127/1000 | Loss: 0.00002200
Iteration 128/1000 | Loss: 0.00002200
Iteration 129/1000 | Loss: 0.00002200
Iteration 130/1000 | Loss: 0.00002199
Iteration 131/1000 | Loss: 0.00002199
Iteration 132/1000 | Loss: 0.00002199
Iteration 133/1000 | Loss: 0.00002199
Iteration 134/1000 | Loss: 0.00002199
Iteration 135/1000 | Loss: 0.00002199
Iteration 136/1000 | Loss: 0.00002199
Iteration 137/1000 | Loss: 0.00002199
Iteration 138/1000 | Loss: 0.00002199
Iteration 139/1000 | Loss: 0.00002199
Iteration 140/1000 | Loss: 0.00002199
Iteration 141/1000 | Loss: 0.00002198
Iteration 142/1000 | Loss: 0.00002198
Iteration 143/1000 | Loss: 0.00002198
Iteration 144/1000 | Loss: 0.00002198
Iteration 145/1000 | Loss: 0.00002198
Iteration 146/1000 | Loss: 0.00002198
Iteration 147/1000 | Loss: 0.00002198
Iteration 148/1000 | Loss: 0.00002198
Iteration 149/1000 | Loss: 0.00002198
Iteration 150/1000 | Loss: 0.00002198
Iteration 151/1000 | Loss: 0.00002198
Iteration 152/1000 | Loss: 0.00002198
Iteration 153/1000 | Loss: 0.00002198
Iteration 154/1000 | Loss: 0.00002198
Iteration 155/1000 | Loss: 0.00002198
Iteration 156/1000 | Loss: 0.00002198
Iteration 157/1000 | Loss: 0.00002197
Iteration 158/1000 | Loss: 0.00002197
Iteration 159/1000 | Loss: 0.00002197
Iteration 160/1000 | Loss: 0.00002197
Iteration 161/1000 | Loss: 0.00002197
Iteration 162/1000 | Loss: 0.00002197
Iteration 163/1000 | Loss: 0.00002197
Iteration 164/1000 | Loss: 0.00002197
Iteration 165/1000 | Loss: 0.00002197
Iteration 166/1000 | Loss: 0.00002197
Iteration 167/1000 | Loss: 0.00002197
Iteration 168/1000 | Loss: 0.00002197
Iteration 169/1000 | Loss: 0.00002197
Iteration 170/1000 | Loss: 0.00002197
Iteration 171/1000 | Loss: 0.00002197
Iteration 172/1000 | Loss: 0.00002197
Iteration 173/1000 | Loss: 0.00002197
Iteration 174/1000 | Loss: 0.00002197
Iteration 175/1000 | Loss: 0.00002197
Iteration 176/1000 | Loss: 0.00002197
Iteration 177/1000 | Loss: 0.00002196
Iteration 178/1000 | Loss: 0.00002196
Iteration 179/1000 | Loss: 0.00002196
Iteration 180/1000 | Loss: 0.00002196
Iteration 181/1000 | Loss: 0.00002196
Iteration 182/1000 | Loss: 0.00002196
Iteration 183/1000 | Loss: 0.00002196
Iteration 184/1000 | Loss: 0.00002196
Iteration 185/1000 | Loss: 0.00002196
Iteration 186/1000 | Loss: 0.00002196
Iteration 187/1000 | Loss: 0.00002196
Iteration 188/1000 | Loss: 0.00002196
Iteration 189/1000 | Loss: 0.00002196
Iteration 190/1000 | Loss: 0.00002196
Iteration 191/1000 | Loss: 0.00002196
Iteration 192/1000 | Loss: 0.00002196
Iteration 193/1000 | Loss: 0.00002196
Iteration 194/1000 | Loss: 0.00002196
Iteration 195/1000 | Loss: 0.00002195
Iteration 196/1000 | Loss: 0.00002195
Iteration 197/1000 | Loss: 0.00002195
Iteration 198/1000 | Loss: 0.00002195
Iteration 199/1000 | Loss: 0.00002195
Iteration 200/1000 | Loss: 0.00002195
Iteration 201/1000 | Loss: 0.00002195
Iteration 202/1000 | Loss: 0.00002195
Iteration 203/1000 | Loss: 0.00002195
Iteration 204/1000 | Loss: 0.00002195
Iteration 205/1000 | Loss: 0.00002195
Iteration 206/1000 | Loss: 0.00002195
Iteration 207/1000 | Loss: 0.00002195
Iteration 208/1000 | Loss: 0.00002195
Iteration 209/1000 | Loss: 0.00002195
Iteration 210/1000 | Loss: 0.00002195
Iteration 211/1000 | Loss: 0.00002195
Iteration 212/1000 | Loss: 0.00002195
Iteration 213/1000 | Loss: 0.00002194
Iteration 214/1000 | Loss: 0.00002194
Iteration 215/1000 | Loss: 0.00002194
Iteration 216/1000 | Loss: 0.00002194
Iteration 217/1000 | Loss: 0.00002194
Iteration 218/1000 | Loss: 0.00002194
Iteration 219/1000 | Loss: 0.00002194
Iteration 220/1000 | Loss: 0.00002194
Iteration 221/1000 | Loss: 0.00002194
Iteration 222/1000 | Loss: 0.00002194
Iteration 223/1000 | Loss: 0.00002194
Iteration 224/1000 | Loss: 0.00002194
Iteration 225/1000 | Loss: 0.00002194
Iteration 226/1000 | Loss: 0.00002194
Iteration 227/1000 | Loss: 0.00002194
Iteration 228/1000 | Loss: 0.00002194
Iteration 229/1000 | Loss: 0.00002194
Iteration 230/1000 | Loss: 0.00002194
Iteration 231/1000 | Loss: 0.00002194
Iteration 232/1000 | Loss: 0.00002194
Iteration 233/1000 | Loss: 0.00002193
Iteration 234/1000 | Loss: 0.00002193
Iteration 235/1000 | Loss: 0.00002193
Iteration 236/1000 | Loss: 0.00002193
Iteration 237/1000 | Loss: 0.00002193
Iteration 238/1000 | Loss: 0.00002193
Iteration 239/1000 | Loss: 0.00002193
Iteration 240/1000 | Loss: 0.00002193
Iteration 241/1000 | Loss: 0.00002193
Iteration 242/1000 | Loss: 0.00002193
Iteration 243/1000 | Loss: 0.00002193
Iteration 244/1000 | Loss: 0.00002193
Iteration 245/1000 | Loss: 0.00002193
Iteration 246/1000 | Loss: 0.00002193
Iteration 247/1000 | Loss: 0.00002193
Iteration 248/1000 | Loss: 0.00002193
Iteration 249/1000 | Loss: 0.00002193
Iteration 250/1000 | Loss: 0.00002193
Iteration 251/1000 | Loss: 0.00002193
Iteration 252/1000 | Loss: 0.00002193
Iteration 253/1000 | Loss: 0.00002193
Iteration 254/1000 | Loss: 0.00002193
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 254. Stopping optimization.
Last 5 losses: [2.1928877686150372e-05, 2.1928877686150372e-05, 2.1928877686150372e-05, 2.1928877686150372e-05, 2.1928877686150372e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1928877686150372e-05

Optimization complete. Final v2v error: 3.967740058898926 mm

Highest mean error: 4.037228107452393 mm for frame 35

Lowest mean error: 3.7871615886688232 mm for frame 151

Saving results

Total time: 41.42555475234985
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00996996
Iteration 2/25 | Loss: 0.00996996
Iteration 3/25 | Loss: 0.00996996
Iteration 4/25 | Loss: 0.00996996
Iteration 5/25 | Loss: 0.00996995
Iteration 6/25 | Loss: 0.00996995
Iteration 7/25 | Loss: 0.00996995
Iteration 8/25 | Loss: 0.00996995
Iteration 9/25 | Loss: 0.00996995
Iteration 10/25 | Loss: 0.00996994
Iteration 11/25 | Loss: 0.00996994
Iteration 12/25 | Loss: 0.00996994
Iteration 13/25 | Loss: 0.00996994
Iteration 14/25 | Loss: 0.00337158
Iteration 15/25 | Loss: 0.00219584
Iteration 16/25 | Loss: 0.00181683
Iteration 17/25 | Loss: 0.00167851
Iteration 18/25 | Loss: 0.00159822
Iteration 19/25 | Loss: 0.00151275
Iteration 20/25 | Loss: 0.00146757
Iteration 21/25 | Loss: 0.00143793
Iteration 22/25 | Loss: 0.00141925
Iteration 23/25 | Loss: 0.00141821
Iteration 24/25 | Loss: 0.00140885
Iteration 25/25 | Loss: 0.00140392

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33042300
Iteration 2/25 | Loss: 0.00242824
Iteration 3/25 | Loss: 0.00202322
Iteration 4/25 | Loss: 0.00202322
Iteration 5/25 | Loss: 0.00202322
Iteration 6/25 | Loss: 0.00202322
Iteration 7/25 | Loss: 0.00202322
Iteration 8/25 | Loss: 0.00202322
Iteration 9/25 | Loss: 0.00202322
Iteration 10/25 | Loss: 0.00202322
Iteration 11/25 | Loss: 0.00202322
Iteration 12/25 | Loss: 0.00202322
Iteration 13/25 | Loss: 0.00202322
Iteration 14/25 | Loss: 0.00202322
Iteration 15/25 | Loss: 0.00202322
Iteration 16/25 | Loss: 0.00202322
Iteration 17/25 | Loss: 0.00202322
Iteration 18/25 | Loss: 0.00202322
Iteration 19/25 | Loss: 0.00202322
Iteration 20/25 | Loss: 0.00202322
Iteration 21/25 | Loss: 0.00202322
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.002023220295086503, 0.002023220295086503, 0.002023220295086503, 0.002023220295086503, 0.002023220295086503]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002023220295086503

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00202322
Iteration 2/1000 | Loss: 0.00035951
Iteration 3/1000 | Loss: 0.00028888
Iteration 4/1000 | Loss: 0.00016042
Iteration 5/1000 | Loss: 0.00016228
Iteration 6/1000 | Loss: 0.00016603
Iteration 7/1000 | Loss: 0.00031959
Iteration 8/1000 | Loss: 0.00015288
Iteration 9/1000 | Loss: 0.00014659
Iteration 10/1000 | Loss: 0.00053064
Iteration 11/1000 | Loss: 0.00010027
Iteration 12/1000 | Loss: 0.00009959
Iteration 13/1000 | Loss: 0.00011485
Iteration 14/1000 | Loss: 0.00013429
Iteration 15/1000 | Loss: 0.00013939
Iteration 16/1000 | Loss: 0.00023192
Iteration 17/1000 | Loss: 0.00112803
Iteration 18/1000 | Loss: 0.00758673
Iteration 19/1000 | Loss: 0.00185937
Iteration 20/1000 | Loss: 0.00021504
Iteration 21/1000 | Loss: 0.00038945
Iteration 22/1000 | Loss: 0.00010083
Iteration 23/1000 | Loss: 0.00007367
Iteration 24/1000 | Loss: 0.00005245
Iteration 25/1000 | Loss: 0.00006439
Iteration 26/1000 | Loss: 0.00005971
Iteration 27/1000 | Loss: 0.00003084
Iteration 28/1000 | Loss: 0.00002912
Iteration 29/1000 | Loss: 0.00002503
Iteration 30/1000 | Loss: 0.00004317
Iteration 31/1000 | Loss: 0.00003134
Iteration 32/1000 | Loss: 0.00002748
Iteration 33/1000 | Loss: 0.00003022
Iteration 34/1000 | Loss: 0.00001911
Iteration 35/1000 | Loss: 0.00018189
Iteration 36/1000 | Loss: 0.00001752
Iteration 37/1000 | Loss: 0.00004297
Iteration 38/1000 | Loss: 0.00001703
Iteration 39/1000 | Loss: 0.00003170
Iteration 40/1000 | Loss: 0.00016879
Iteration 41/1000 | Loss: 0.00002002
Iteration 42/1000 | Loss: 0.00001622
Iteration 43/1000 | Loss: 0.00002455
Iteration 44/1000 | Loss: 0.00002556
Iteration 45/1000 | Loss: 0.00002674
Iteration 46/1000 | Loss: 0.00001913
Iteration 47/1000 | Loss: 0.00004109
Iteration 48/1000 | Loss: 0.00001564
Iteration 49/1000 | Loss: 0.00003424
Iteration 50/1000 | Loss: 0.00001538
Iteration 51/1000 | Loss: 0.00001538
Iteration 52/1000 | Loss: 0.00001571
Iteration 53/1000 | Loss: 0.00001570
Iteration 54/1000 | Loss: 0.00001570
Iteration 55/1000 | Loss: 0.00001570
Iteration 56/1000 | Loss: 0.00001585
Iteration 57/1000 | Loss: 0.00001660
Iteration 58/1000 | Loss: 0.00001951
Iteration 59/1000 | Loss: 0.00001599
Iteration 60/1000 | Loss: 0.00001514
Iteration 61/1000 | Loss: 0.00001514
Iteration 62/1000 | Loss: 0.00001514
Iteration 63/1000 | Loss: 0.00001514
Iteration 64/1000 | Loss: 0.00001514
Iteration 65/1000 | Loss: 0.00001514
Iteration 66/1000 | Loss: 0.00001514
Iteration 67/1000 | Loss: 0.00001514
Iteration 68/1000 | Loss: 0.00001514
Iteration 69/1000 | Loss: 0.00001514
Iteration 70/1000 | Loss: 0.00001514
Iteration 71/1000 | Loss: 0.00001514
Iteration 72/1000 | Loss: 0.00001513
Iteration 73/1000 | Loss: 0.00001513
Iteration 74/1000 | Loss: 0.00001513
Iteration 75/1000 | Loss: 0.00001513
Iteration 76/1000 | Loss: 0.00001513
Iteration 77/1000 | Loss: 0.00001513
Iteration 78/1000 | Loss: 0.00001513
Iteration 79/1000 | Loss: 0.00001513
Iteration 80/1000 | Loss: 0.00001513
Iteration 81/1000 | Loss: 0.00001513
Iteration 82/1000 | Loss: 0.00001512
Iteration 83/1000 | Loss: 0.00001512
Iteration 84/1000 | Loss: 0.00001512
Iteration 85/1000 | Loss: 0.00001512
Iteration 86/1000 | Loss: 0.00002297
Iteration 87/1000 | Loss: 0.00001743
Iteration 88/1000 | Loss: 0.00001731
Iteration 89/1000 | Loss: 0.00001716
Iteration 90/1000 | Loss: 0.00001523
Iteration 91/1000 | Loss: 0.00001523
Iteration 92/1000 | Loss: 0.00001549
Iteration 93/1000 | Loss: 0.00002701
Iteration 94/1000 | Loss: 0.00001517
Iteration 95/1000 | Loss: 0.00001509
Iteration 96/1000 | Loss: 0.00001508
Iteration 97/1000 | Loss: 0.00001508
Iteration 98/1000 | Loss: 0.00001508
Iteration 99/1000 | Loss: 0.00001508
Iteration 100/1000 | Loss: 0.00001508
Iteration 101/1000 | Loss: 0.00001508
Iteration 102/1000 | Loss: 0.00001508
Iteration 103/1000 | Loss: 0.00001508
Iteration 104/1000 | Loss: 0.00001508
Iteration 105/1000 | Loss: 0.00001508
Iteration 106/1000 | Loss: 0.00001508
Iteration 107/1000 | Loss: 0.00001508
Iteration 108/1000 | Loss: 0.00001508
Iteration 109/1000 | Loss: 0.00001508
Iteration 110/1000 | Loss: 0.00001507
Iteration 111/1000 | Loss: 0.00001513
Iteration 112/1000 | Loss: 0.00001507
Iteration 113/1000 | Loss: 0.00001507
Iteration 114/1000 | Loss: 0.00001507
Iteration 115/1000 | Loss: 0.00001506
Iteration 116/1000 | Loss: 0.00001506
Iteration 117/1000 | Loss: 0.00001506
Iteration 118/1000 | Loss: 0.00001506
Iteration 119/1000 | Loss: 0.00001506
Iteration 120/1000 | Loss: 0.00001506
Iteration 121/1000 | Loss: 0.00001506
Iteration 122/1000 | Loss: 0.00001506
Iteration 123/1000 | Loss: 0.00001506
Iteration 124/1000 | Loss: 0.00001506
Iteration 125/1000 | Loss: 0.00001506
Iteration 126/1000 | Loss: 0.00001505
Iteration 127/1000 | Loss: 0.00001505
Iteration 128/1000 | Loss: 0.00001505
Iteration 129/1000 | Loss: 0.00001505
Iteration 130/1000 | Loss: 0.00001505
Iteration 131/1000 | Loss: 0.00001505
Iteration 132/1000 | Loss: 0.00001505
Iteration 133/1000 | Loss: 0.00001505
Iteration 134/1000 | Loss: 0.00001505
Iteration 135/1000 | Loss: 0.00001505
Iteration 136/1000 | Loss: 0.00001505
Iteration 137/1000 | Loss: 0.00001505
Iteration 138/1000 | Loss: 0.00001532
Iteration 139/1000 | Loss: 0.00001533
Iteration 140/1000 | Loss: 0.00003388
Iteration 141/1000 | Loss: 0.00001879
Iteration 142/1000 | Loss: 0.00001907
Iteration 143/1000 | Loss: 0.00001564
Iteration 144/1000 | Loss: 0.00001645
Iteration 145/1000 | Loss: 0.00001516
Iteration 146/1000 | Loss: 0.00001501
Iteration 147/1000 | Loss: 0.00001502
Iteration 148/1000 | Loss: 0.00001501
Iteration 149/1000 | Loss: 0.00001501
Iteration 150/1000 | Loss: 0.00001501
Iteration 151/1000 | Loss: 0.00001501
Iteration 152/1000 | Loss: 0.00001501
Iteration 153/1000 | Loss: 0.00001501
Iteration 154/1000 | Loss: 0.00001501
Iteration 155/1000 | Loss: 0.00001501
Iteration 156/1000 | Loss: 0.00001501
Iteration 157/1000 | Loss: 0.00001501
Iteration 158/1000 | Loss: 0.00001501
Iteration 159/1000 | Loss: 0.00001501
Iteration 160/1000 | Loss: 0.00001501
Iteration 161/1000 | Loss: 0.00001501
Iteration 162/1000 | Loss: 0.00001501
Iteration 163/1000 | Loss: 0.00001501
Iteration 164/1000 | Loss: 0.00001501
Iteration 165/1000 | Loss: 0.00001501
Iteration 166/1000 | Loss: 0.00001501
Iteration 167/1000 | Loss: 0.00001501
Iteration 168/1000 | Loss: 0.00001501
Iteration 169/1000 | Loss: 0.00001501
Iteration 170/1000 | Loss: 0.00001501
Iteration 171/1000 | Loss: 0.00001501
Iteration 172/1000 | Loss: 0.00001501
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.5011391951702535e-05, 1.5011391951702535e-05, 1.5011391951702535e-05, 1.5011391951702535e-05, 1.5011391951702535e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5011391951702535e-05

Optimization complete. Final v2v error: 3.268420934677124 mm

Highest mean error: 4.178790092468262 mm for frame 20

Lowest mean error: 2.922382354736328 mm for frame 171

Saving results

Total time: 135.6087610721588
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00872260
Iteration 2/25 | Loss: 0.00131893
Iteration 3/25 | Loss: 0.00122434
Iteration 4/25 | Loss: 0.00121216
Iteration 5/25 | Loss: 0.00120926
Iteration 6/25 | Loss: 0.00120926
Iteration 7/25 | Loss: 0.00120926
Iteration 8/25 | Loss: 0.00120926
Iteration 9/25 | Loss: 0.00120926
Iteration 10/25 | Loss: 0.00120926
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001209261012263596, 0.001209261012263596, 0.001209261012263596, 0.001209261012263596, 0.001209261012263596]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001209261012263596

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.77220678
Iteration 2/25 | Loss: 0.00105188
Iteration 3/25 | Loss: 0.00105188
Iteration 4/25 | Loss: 0.00105188
Iteration 5/25 | Loss: 0.00105188
Iteration 6/25 | Loss: 0.00105188
Iteration 7/25 | Loss: 0.00105188
Iteration 8/25 | Loss: 0.00105188
Iteration 9/25 | Loss: 0.00105188
Iteration 10/25 | Loss: 0.00105188
Iteration 11/25 | Loss: 0.00105188
Iteration 12/25 | Loss: 0.00105188
Iteration 13/25 | Loss: 0.00105188
Iteration 14/25 | Loss: 0.00105188
Iteration 15/25 | Loss: 0.00105188
Iteration 16/25 | Loss: 0.00105188
Iteration 17/25 | Loss: 0.00105188
Iteration 18/25 | Loss: 0.00105188
Iteration 19/25 | Loss: 0.00105188
Iteration 20/25 | Loss: 0.00105188
Iteration 21/25 | Loss: 0.00105188
Iteration 22/25 | Loss: 0.00105188
Iteration 23/25 | Loss: 0.00105188
Iteration 24/25 | Loss: 0.00105188
Iteration 25/25 | Loss: 0.00105188

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105188
Iteration 2/1000 | Loss: 0.00002378
Iteration 3/1000 | Loss: 0.00001592
Iteration 4/1000 | Loss: 0.00001396
Iteration 5/1000 | Loss: 0.00001301
Iteration 6/1000 | Loss: 0.00001233
Iteration 7/1000 | Loss: 0.00001180
Iteration 8/1000 | Loss: 0.00001171
Iteration 9/1000 | Loss: 0.00001143
Iteration 10/1000 | Loss: 0.00001112
Iteration 11/1000 | Loss: 0.00001093
Iteration 12/1000 | Loss: 0.00001081
Iteration 13/1000 | Loss: 0.00001077
Iteration 14/1000 | Loss: 0.00001076
Iteration 15/1000 | Loss: 0.00001071
Iteration 16/1000 | Loss: 0.00001064
Iteration 17/1000 | Loss: 0.00001064
Iteration 18/1000 | Loss: 0.00001063
Iteration 19/1000 | Loss: 0.00001062
Iteration 20/1000 | Loss: 0.00001059
Iteration 21/1000 | Loss: 0.00001059
Iteration 22/1000 | Loss: 0.00001058
Iteration 23/1000 | Loss: 0.00001058
Iteration 24/1000 | Loss: 0.00001056
Iteration 25/1000 | Loss: 0.00001053
Iteration 26/1000 | Loss: 0.00001052
Iteration 27/1000 | Loss: 0.00001050
Iteration 28/1000 | Loss: 0.00001049
Iteration 29/1000 | Loss: 0.00001046
Iteration 30/1000 | Loss: 0.00001042
Iteration 31/1000 | Loss: 0.00001042
Iteration 32/1000 | Loss: 0.00001041
Iteration 33/1000 | Loss: 0.00001041
Iteration 34/1000 | Loss: 0.00001040
Iteration 35/1000 | Loss: 0.00001038
Iteration 36/1000 | Loss: 0.00001038
Iteration 37/1000 | Loss: 0.00001034
Iteration 38/1000 | Loss: 0.00001033
Iteration 39/1000 | Loss: 0.00001033
Iteration 40/1000 | Loss: 0.00001032
Iteration 41/1000 | Loss: 0.00001030
Iteration 42/1000 | Loss: 0.00001030
Iteration 43/1000 | Loss: 0.00001029
Iteration 44/1000 | Loss: 0.00001028
Iteration 45/1000 | Loss: 0.00001028
Iteration 46/1000 | Loss: 0.00001027
Iteration 47/1000 | Loss: 0.00001026
Iteration 48/1000 | Loss: 0.00001024
Iteration 49/1000 | Loss: 0.00001024
Iteration 50/1000 | Loss: 0.00001023
Iteration 51/1000 | Loss: 0.00001022
Iteration 52/1000 | Loss: 0.00001022
Iteration 53/1000 | Loss: 0.00001022
Iteration 54/1000 | Loss: 0.00001022
Iteration 55/1000 | Loss: 0.00001021
Iteration 56/1000 | Loss: 0.00001021
Iteration 57/1000 | Loss: 0.00001021
Iteration 58/1000 | Loss: 0.00001021
Iteration 59/1000 | Loss: 0.00001021
Iteration 60/1000 | Loss: 0.00001021
Iteration 61/1000 | Loss: 0.00001021
Iteration 62/1000 | Loss: 0.00001021
Iteration 63/1000 | Loss: 0.00001021
Iteration 64/1000 | Loss: 0.00001021
Iteration 65/1000 | Loss: 0.00001017
Iteration 66/1000 | Loss: 0.00001017
Iteration 67/1000 | Loss: 0.00001017
Iteration 68/1000 | Loss: 0.00001017
Iteration 69/1000 | Loss: 0.00001017
Iteration 70/1000 | Loss: 0.00001017
Iteration 71/1000 | Loss: 0.00001017
Iteration 72/1000 | Loss: 0.00001016
Iteration 73/1000 | Loss: 0.00001015
Iteration 74/1000 | Loss: 0.00001015
Iteration 75/1000 | Loss: 0.00001015
Iteration 76/1000 | Loss: 0.00001014
Iteration 77/1000 | Loss: 0.00001013
Iteration 78/1000 | Loss: 0.00001013
Iteration 79/1000 | Loss: 0.00001012
Iteration 80/1000 | Loss: 0.00001011
Iteration 81/1000 | Loss: 0.00001011
Iteration 82/1000 | Loss: 0.00001011
Iteration 83/1000 | Loss: 0.00001011
Iteration 84/1000 | Loss: 0.00001011
Iteration 85/1000 | Loss: 0.00001011
Iteration 86/1000 | Loss: 0.00001011
Iteration 87/1000 | Loss: 0.00001011
Iteration 88/1000 | Loss: 0.00001011
Iteration 89/1000 | Loss: 0.00001011
Iteration 90/1000 | Loss: 0.00001010
Iteration 91/1000 | Loss: 0.00001010
Iteration 92/1000 | Loss: 0.00001010
Iteration 93/1000 | Loss: 0.00001010
Iteration 94/1000 | Loss: 0.00001009
Iteration 95/1000 | Loss: 0.00001009
Iteration 96/1000 | Loss: 0.00001009
Iteration 97/1000 | Loss: 0.00001009
Iteration 98/1000 | Loss: 0.00001009
Iteration 99/1000 | Loss: 0.00001009
Iteration 100/1000 | Loss: 0.00001009
Iteration 101/1000 | Loss: 0.00001009
Iteration 102/1000 | Loss: 0.00001009
Iteration 103/1000 | Loss: 0.00001007
Iteration 104/1000 | Loss: 0.00001007
Iteration 105/1000 | Loss: 0.00001006
Iteration 106/1000 | Loss: 0.00001006
Iteration 107/1000 | Loss: 0.00001006
Iteration 108/1000 | Loss: 0.00001006
Iteration 109/1000 | Loss: 0.00001006
Iteration 110/1000 | Loss: 0.00001005
Iteration 111/1000 | Loss: 0.00001005
Iteration 112/1000 | Loss: 0.00001005
Iteration 113/1000 | Loss: 0.00001005
Iteration 114/1000 | Loss: 0.00001005
Iteration 115/1000 | Loss: 0.00001005
Iteration 116/1000 | Loss: 0.00001005
Iteration 117/1000 | Loss: 0.00001005
Iteration 118/1000 | Loss: 0.00001003
Iteration 119/1000 | Loss: 0.00001003
Iteration 120/1000 | Loss: 0.00001002
Iteration 121/1000 | Loss: 0.00001001
Iteration 122/1000 | Loss: 0.00001001
Iteration 123/1000 | Loss: 0.00001001
Iteration 124/1000 | Loss: 0.00001001
Iteration 125/1000 | Loss: 0.00001001
Iteration 126/1000 | Loss: 0.00001000
Iteration 127/1000 | Loss: 0.00001000
Iteration 128/1000 | Loss: 0.00001000
Iteration 129/1000 | Loss: 0.00001000
Iteration 130/1000 | Loss: 0.00001000
Iteration 131/1000 | Loss: 0.00001000
Iteration 132/1000 | Loss: 0.00001000
Iteration 133/1000 | Loss: 0.00001000
Iteration 134/1000 | Loss: 0.00001000
Iteration 135/1000 | Loss: 0.00000999
Iteration 136/1000 | Loss: 0.00000999
Iteration 137/1000 | Loss: 0.00000999
Iteration 138/1000 | Loss: 0.00000998
Iteration 139/1000 | Loss: 0.00000998
Iteration 140/1000 | Loss: 0.00000997
Iteration 141/1000 | Loss: 0.00000997
Iteration 142/1000 | Loss: 0.00000997
Iteration 143/1000 | Loss: 0.00000996
Iteration 144/1000 | Loss: 0.00000996
Iteration 145/1000 | Loss: 0.00000996
Iteration 146/1000 | Loss: 0.00000996
Iteration 147/1000 | Loss: 0.00000996
Iteration 148/1000 | Loss: 0.00000996
Iteration 149/1000 | Loss: 0.00000996
Iteration 150/1000 | Loss: 0.00000995
Iteration 151/1000 | Loss: 0.00000995
Iteration 152/1000 | Loss: 0.00000995
Iteration 153/1000 | Loss: 0.00000995
Iteration 154/1000 | Loss: 0.00000995
Iteration 155/1000 | Loss: 0.00000994
Iteration 156/1000 | Loss: 0.00000994
Iteration 157/1000 | Loss: 0.00000994
Iteration 158/1000 | Loss: 0.00000994
Iteration 159/1000 | Loss: 0.00000994
Iteration 160/1000 | Loss: 0.00000993
Iteration 161/1000 | Loss: 0.00000993
Iteration 162/1000 | Loss: 0.00000993
Iteration 163/1000 | Loss: 0.00000993
Iteration 164/1000 | Loss: 0.00000993
Iteration 165/1000 | Loss: 0.00000993
Iteration 166/1000 | Loss: 0.00000993
Iteration 167/1000 | Loss: 0.00000993
Iteration 168/1000 | Loss: 0.00000992
Iteration 169/1000 | Loss: 0.00000992
Iteration 170/1000 | Loss: 0.00000992
Iteration 171/1000 | Loss: 0.00000992
Iteration 172/1000 | Loss: 0.00000992
Iteration 173/1000 | Loss: 0.00000992
Iteration 174/1000 | Loss: 0.00000992
Iteration 175/1000 | Loss: 0.00000992
Iteration 176/1000 | Loss: 0.00000992
Iteration 177/1000 | Loss: 0.00000992
Iteration 178/1000 | Loss: 0.00000991
Iteration 179/1000 | Loss: 0.00000991
Iteration 180/1000 | Loss: 0.00000991
Iteration 181/1000 | Loss: 0.00000991
Iteration 182/1000 | Loss: 0.00000991
Iteration 183/1000 | Loss: 0.00000991
Iteration 184/1000 | Loss: 0.00000991
Iteration 185/1000 | Loss: 0.00000991
Iteration 186/1000 | Loss: 0.00000991
Iteration 187/1000 | Loss: 0.00000991
Iteration 188/1000 | Loss: 0.00000991
Iteration 189/1000 | Loss: 0.00000991
Iteration 190/1000 | Loss: 0.00000991
Iteration 191/1000 | Loss: 0.00000990
Iteration 192/1000 | Loss: 0.00000990
Iteration 193/1000 | Loss: 0.00000990
Iteration 194/1000 | Loss: 0.00000990
Iteration 195/1000 | Loss: 0.00000990
Iteration 196/1000 | Loss: 0.00000990
Iteration 197/1000 | Loss: 0.00000990
Iteration 198/1000 | Loss: 0.00000990
Iteration 199/1000 | Loss: 0.00000990
Iteration 200/1000 | Loss: 0.00000990
Iteration 201/1000 | Loss: 0.00000990
Iteration 202/1000 | Loss: 0.00000990
Iteration 203/1000 | Loss: 0.00000990
Iteration 204/1000 | Loss: 0.00000989
Iteration 205/1000 | Loss: 0.00000989
Iteration 206/1000 | Loss: 0.00000989
Iteration 207/1000 | Loss: 0.00000989
Iteration 208/1000 | Loss: 0.00000989
Iteration 209/1000 | Loss: 0.00000989
Iteration 210/1000 | Loss: 0.00000989
Iteration 211/1000 | Loss: 0.00000989
Iteration 212/1000 | Loss: 0.00000989
Iteration 213/1000 | Loss: 0.00000989
Iteration 214/1000 | Loss: 0.00000989
Iteration 215/1000 | Loss: 0.00000989
Iteration 216/1000 | Loss: 0.00000989
Iteration 217/1000 | Loss: 0.00000989
Iteration 218/1000 | Loss: 0.00000989
Iteration 219/1000 | Loss: 0.00000989
Iteration 220/1000 | Loss: 0.00000989
Iteration 221/1000 | Loss: 0.00000989
Iteration 222/1000 | Loss: 0.00000989
Iteration 223/1000 | Loss: 0.00000989
Iteration 224/1000 | Loss: 0.00000989
Iteration 225/1000 | Loss: 0.00000989
Iteration 226/1000 | Loss: 0.00000989
Iteration 227/1000 | Loss: 0.00000989
Iteration 228/1000 | Loss: 0.00000989
Iteration 229/1000 | Loss: 0.00000989
Iteration 230/1000 | Loss: 0.00000989
Iteration 231/1000 | Loss: 0.00000989
Iteration 232/1000 | Loss: 0.00000989
Iteration 233/1000 | Loss: 0.00000989
Iteration 234/1000 | Loss: 0.00000989
Iteration 235/1000 | Loss: 0.00000989
Iteration 236/1000 | Loss: 0.00000989
Iteration 237/1000 | Loss: 0.00000989
Iteration 238/1000 | Loss: 0.00000989
Iteration 239/1000 | Loss: 0.00000989
Iteration 240/1000 | Loss: 0.00000989
Iteration 241/1000 | Loss: 0.00000989
Iteration 242/1000 | Loss: 0.00000989
Iteration 243/1000 | Loss: 0.00000989
Iteration 244/1000 | Loss: 0.00000989
Iteration 245/1000 | Loss: 0.00000989
Iteration 246/1000 | Loss: 0.00000989
Iteration 247/1000 | Loss: 0.00000989
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 247. Stopping optimization.
Last 5 losses: [9.891863555822056e-06, 9.891863555822056e-06, 9.891863555822056e-06, 9.891863555822056e-06, 9.891863555822056e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.891863555822056e-06

Optimization complete. Final v2v error: 2.687685966491699 mm

Highest mean error: 3.006909132003784 mm for frame 231

Lowest mean error: 2.496690034866333 mm for frame 129

Saving results

Total time: 47.90672945976257
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00499863
Iteration 2/25 | Loss: 0.00145345
Iteration 3/25 | Loss: 0.00133583
Iteration 4/25 | Loss: 0.00132131
Iteration 5/25 | Loss: 0.00131661
Iteration 6/25 | Loss: 0.00131653
Iteration 7/25 | Loss: 0.00131653
Iteration 8/25 | Loss: 0.00131653
Iteration 9/25 | Loss: 0.00131653
Iteration 10/25 | Loss: 0.00131653
Iteration 11/25 | Loss: 0.00131653
Iteration 12/25 | Loss: 0.00131653
Iteration 13/25 | Loss: 0.00131653
Iteration 14/25 | Loss: 0.00131653
Iteration 15/25 | Loss: 0.00131653
Iteration 16/25 | Loss: 0.00131653
Iteration 17/25 | Loss: 0.00131653
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001316534006036818, 0.001316534006036818, 0.001316534006036818, 0.001316534006036818, 0.001316534006036818]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001316534006036818

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34694028
Iteration 2/25 | Loss: 0.00107128
Iteration 3/25 | Loss: 0.00107127
Iteration 4/25 | Loss: 0.00107127
Iteration 5/25 | Loss: 0.00107127
Iteration 6/25 | Loss: 0.00107127
Iteration 7/25 | Loss: 0.00107127
Iteration 8/25 | Loss: 0.00107127
Iteration 9/25 | Loss: 0.00107127
Iteration 10/25 | Loss: 0.00107127
Iteration 11/25 | Loss: 0.00107127
Iteration 12/25 | Loss: 0.00107127
Iteration 13/25 | Loss: 0.00107127
Iteration 14/25 | Loss: 0.00107127
Iteration 15/25 | Loss: 0.00107127
Iteration 16/25 | Loss: 0.00107127
Iteration 17/25 | Loss: 0.00107127
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010712710209190845, 0.0010712710209190845, 0.0010712710209190845, 0.0010712710209190845, 0.0010712710209190845]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010712710209190845

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107127
Iteration 2/1000 | Loss: 0.00005099
Iteration 3/1000 | Loss: 0.00003418
Iteration 4/1000 | Loss: 0.00003101
Iteration 5/1000 | Loss: 0.00002974
Iteration 6/1000 | Loss: 0.00002857
Iteration 7/1000 | Loss: 0.00002749
Iteration 8/1000 | Loss: 0.00002703
Iteration 9/1000 | Loss: 0.00002660
Iteration 10/1000 | Loss: 0.00002619
Iteration 11/1000 | Loss: 0.00002595
Iteration 12/1000 | Loss: 0.00002575
Iteration 13/1000 | Loss: 0.00002555
Iteration 14/1000 | Loss: 0.00002536
Iteration 15/1000 | Loss: 0.00002517
Iteration 16/1000 | Loss: 0.00002505
Iteration 17/1000 | Loss: 0.00002504
Iteration 18/1000 | Loss: 0.00002504
Iteration 19/1000 | Loss: 0.00002503
Iteration 20/1000 | Loss: 0.00002499
Iteration 21/1000 | Loss: 0.00002491
Iteration 22/1000 | Loss: 0.00002486
Iteration 23/1000 | Loss: 0.00002486
Iteration 24/1000 | Loss: 0.00002483
Iteration 25/1000 | Loss: 0.00002480
Iteration 26/1000 | Loss: 0.00002478
Iteration 27/1000 | Loss: 0.00002477
Iteration 28/1000 | Loss: 0.00002475
Iteration 29/1000 | Loss: 0.00002473
Iteration 30/1000 | Loss: 0.00002473
Iteration 31/1000 | Loss: 0.00002472
Iteration 32/1000 | Loss: 0.00002472
Iteration 33/1000 | Loss: 0.00002472
Iteration 34/1000 | Loss: 0.00002472
Iteration 35/1000 | Loss: 0.00002472
Iteration 36/1000 | Loss: 0.00002472
Iteration 37/1000 | Loss: 0.00002472
Iteration 38/1000 | Loss: 0.00002471
Iteration 39/1000 | Loss: 0.00002471
Iteration 40/1000 | Loss: 0.00002470
Iteration 41/1000 | Loss: 0.00002469
Iteration 42/1000 | Loss: 0.00002469
Iteration 43/1000 | Loss: 0.00002469
Iteration 44/1000 | Loss: 0.00002469
Iteration 45/1000 | Loss: 0.00002468
Iteration 46/1000 | Loss: 0.00002468
Iteration 47/1000 | Loss: 0.00002467
Iteration 48/1000 | Loss: 0.00002467
Iteration 49/1000 | Loss: 0.00002467
Iteration 50/1000 | Loss: 0.00002466
Iteration 51/1000 | Loss: 0.00002466
Iteration 52/1000 | Loss: 0.00002466
Iteration 53/1000 | Loss: 0.00002466
Iteration 54/1000 | Loss: 0.00002465
Iteration 55/1000 | Loss: 0.00002465
Iteration 56/1000 | Loss: 0.00002465
Iteration 57/1000 | Loss: 0.00002465
Iteration 58/1000 | Loss: 0.00002465
Iteration 59/1000 | Loss: 0.00002465
Iteration 60/1000 | Loss: 0.00002465
Iteration 61/1000 | Loss: 0.00002464
Iteration 62/1000 | Loss: 0.00002464
Iteration 63/1000 | Loss: 0.00002464
Iteration 64/1000 | Loss: 0.00002464
Iteration 65/1000 | Loss: 0.00002464
Iteration 66/1000 | Loss: 0.00002463
Iteration 67/1000 | Loss: 0.00002463
Iteration 68/1000 | Loss: 0.00002463
Iteration 69/1000 | Loss: 0.00002463
Iteration 70/1000 | Loss: 0.00002462
Iteration 71/1000 | Loss: 0.00002462
Iteration 72/1000 | Loss: 0.00002462
Iteration 73/1000 | Loss: 0.00002462
Iteration 74/1000 | Loss: 0.00002461
Iteration 75/1000 | Loss: 0.00002461
Iteration 76/1000 | Loss: 0.00002461
Iteration 77/1000 | Loss: 0.00002461
Iteration 78/1000 | Loss: 0.00002461
Iteration 79/1000 | Loss: 0.00002461
Iteration 80/1000 | Loss: 0.00002461
Iteration 81/1000 | Loss: 0.00002461
Iteration 82/1000 | Loss: 0.00002461
Iteration 83/1000 | Loss: 0.00002461
Iteration 84/1000 | Loss: 0.00002461
Iteration 85/1000 | Loss: 0.00002461
Iteration 86/1000 | Loss: 0.00002461
Iteration 87/1000 | Loss: 0.00002461
Iteration 88/1000 | Loss: 0.00002460
Iteration 89/1000 | Loss: 0.00002460
Iteration 90/1000 | Loss: 0.00002460
Iteration 91/1000 | Loss: 0.00002460
Iteration 92/1000 | Loss: 0.00002460
Iteration 93/1000 | Loss: 0.00002460
Iteration 94/1000 | Loss: 0.00002460
Iteration 95/1000 | Loss: 0.00002460
Iteration 96/1000 | Loss: 0.00002460
Iteration 97/1000 | Loss: 0.00002459
Iteration 98/1000 | Loss: 0.00002458
Iteration 99/1000 | Loss: 0.00002458
Iteration 100/1000 | Loss: 0.00002458
Iteration 101/1000 | Loss: 0.00002458
Iteration 102/1000 | Loss: 0.00002458
Iteration 103/1000 | Loss: 0.00002458
Iteration 104/1000 | Loss: 0.00002458
Iteration 105/1000 | Loss: 0.00002457
Iteration 106/1000 | Loss: 0.00002457
Iteration 107/1000 | Loss: 0.00002457
Iteration 108/1000 | Loss: 0.00002456
Iteration 109/1000 | Loss: 0.00002455
Iteration 110/1000 | Loss: 0.00002455
Iteration 111/1000 | Loss: 0.00002455
Iteration 112/1000 | Loss: 0.00002455
Iteration 113/1000 | Loss: 0.00002455
Iteration 114/1000 | Loss: 0.00002455
Iteration 115/1000 | Loss: 0.00002455
Iteration 116/1000 | Loss: 0.00002455
Iteration 117/1000 | Loss: 0.00002455
Iteration 118/1000 | Loss: 0.00002455
Iteration 119/1000 | Loss: 0.00002455
Iteration 120/1000 | Loss: 0.00002455
Iteration 121/1000 | Loss: 0.00002455
Iteration 122/1000 | Loss: 0.00002455
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [2.4548997316742316e-05, 2.4548997316742316e-05, 2.4548997316742316e-05, 2.4548997316742316e-05, 2.4548997316742316e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4548997316742316e-05

Optimization complete. Final v2v error: 4.221405506134033 mm

Highest mean error: 4.677252769470215 mm for frame 105

Lowest mean error: 3.8436498641967773 mm for frame 215

Saving results

Total time: 45.38739275932312
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01073629
Iteration 2/25 | Loss: 0.00226833
Iteration 3/25 | Loss: 0.00172572
Iteration 4/25 | Loss: 0.00177877
Iteration 5/25 | Loss: 0.00176629
Iteration 6/25 | Loss: 0.00177451
Iteration 7/25 | Loss: 0.00165244
Iteration 8/25 | Loss: 0.00175974
Iteration 9/25 | Loss: 0.00181320
Iteration 10/25 | Loss: 0.00178090
Iteration 11/25 | Loss: 0.00165965
Iteration 12/25 | Loss: 0.00161870
Iteration 13/25 | Loss: 0.00155541
Iteration 14/25 | Loss: 0.00151199
Iteration 15/25 | Loss: 0.00146245
Iteration 16/25 | Loss: 0.00144487
Iteration 17/25 | Loss: 0.00143870
Iteration 18/25 | Loss: 0.00143672
Iteration 19/25 | Loss: 0.00141433
Iteration 20/25 | Loss: 0.00141339
Iteration 21/25 | Loss: 0.00139453
Iteration 22/25 | Loss: 0.00137319
Iteration 23/25 | Loss: 0.00137308
Iteration 24/25 | Loss: 0.00138165
Iteration 25/25 | Loss: 0.00136172

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21898639
Iteration 2/25 | Loss: 0.00148487
Iteration 3/25 | Loss: 0.00142281
Iteration 4/25 | Loss: 0.00142281
Iteration 5/25 | Loss: 0.00142281
Iteration 6/25 | Loss: 0.00142281
Iteration 7/25 | Loss: 0.00142281
Iteration 8/25 | Loss: 0.00142281
Iteration 9/25 | Loss: 0.00142281
Iteration 10/25 | Loss: 0.00142281
Iteration 11/25 | Loss: 0.00142281
Iteration 12/25 | Loss: 0.00142281
Iteration 13/25 | Loss: 0.00142281
Iteration 14/25 | Loss: 0.00142281
Iteration 15/25 | Loss: 0.00142281
Iteration 16/25 | Loss: 0.00142281
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0014228109503164887, 0.0014228109503164887, 0.0014228109503164887, 0.0014228109503164887, 0.0014228109503164887]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014228109503164887

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00142281
Iteration 2/1000 | Loss: 0.00054048
Iteration 3/1000 | Loss: 0.00053062
Iteration 4/1000 | Loss: 0.00014649
Iteration 5/1000 | Loss: 0.00013598
Iteration 6/1000 | Loss: 0.00015636
Iteration 7/1000 | Loss: 0.00020371
Iteration 8/1000 | Loss: 0.00024491
Iteration 9/1000 | Loss: 0.00020509
Iteration 10/1000 | Loss: 0.00018674
Iteration 11/1000 | Loss: 0.00024700
Iteration 12/1000 | Loss: 0.00015559
Iteration 13/1000 | Loss: 0.00018676
Iteration 14/1000 | Loss: 0.00016305
Iteration 15/1000 | Loss: 0.00019453
Iteration 16/1000 | Loss: 0.00024413
Iteration 17/1000 | Loss: 0.00029914
Iteration 18/1000 | Loss: 0.00017716
Iteration 19/1000 | Loss: 0.00019401
Iteration 20/1000 | Loss: 0.00018836
Iteration 21/1000 | Loss: 0.00024713
Iteration 22/1000 | Loss: 0.00020280
Iteration 23/1000 | Loss: 0.00019116
Iteration 24/1000 | Loss: 0.00016319
Iteration 25/1000 | Loss: 0.00018155
Iteration 26/1000 | Loss: 0.00012429
Iteration 27/1000 | Loss: 0.00029174
Iteration 28/1000 | Loss: 0.00019621
Iteration 29/1000 | Loss: 0.00016173
Iteration 30/1000 | Loss: 0.00013840
Iteration 31/1000 | Loss: 0.00014492
Iteration 32/1000 | Loss: 0.00025068
Iteration 33/1000 | Loss: 0.00019506
Iteration 34/1000 | Loss: 0.00022325
Iteration 35/1000 | Loss: 0.00019327
Iteration 36/1000 | Loss: 0.00020516
Iteration 37/1000 | Loss: 0.00020212
Iteration 38/1000 | Loss: 0.00021928
Iteration 39/1000 | Loss: 0.00019870
Iteration 40/1000 | Loss: 0.00015376
Iteration 41/1000 | Loss: 0.00019884
Iteration 42/1000 | Loss: 0.00018711
Iteration 43/1000 | Loss: 0.00018497
Iteration 44/1000 | Loss: 0.00023111
Iteration 45/1000 | Loss: 0.00017738
Iteration 46/1000 | Loss: 0.00020871
Iteration 47/1000 | Loss: 0.00020822
Iteration 48/1000 | Loss: 0.00012949
Iteration 49/1000 | Loss: 0.00033677
Iteration 50/1000 | Loss: 0.00011354
Iteration 51/1000 | Loss: 0.00021011
Iteration 52/1000 | Loss: 0.00012553
Iteration 53/1000 | Loss: 0.00020241
Iteration 54/1000 | Loss: 0.00010832
Iteration 55/1000 | Loss: 0.00010477
Iteration 56/1000 | Loss: 0.00016213
Iteration 57/1000 | Loss: 0.00013081
Iteration 58/1000 | Loss: 0.00020388
Iteration 59/1000 | Loss: 0.00018221
Iteration 60/1000 | Loss: 0.00019462
Iteration 61/1000 | Loss: 0.00021426
Iteration 62/1000 | Loss: 0.00016767
Iteration 63/1000 | Loss: 0.00016645
Iteration 64/1000 | Loss: 0.00018055
Iteration 65/1000 | Loss: 0.00019531
Iteration 66/1000 | Loss: 0.00018180
Iteration 67/1000 | Loss: 0.00021229
Iteration 68/1000 | Loss: 0.00023923
Iteration 69/1000 | Loss: 0.00025211
Iteration 70/1000 | Loss: 0.00021250
Iteration 71/1000 | Loss: 0.00018882
Iteration 72/1000 | Loss: 0.00018699
Iteration 73/1000 | Loss: 0.00015556
Iteration 74/1000 | Loss: 0.00016940
Iteration 75/1000 | Loss: 0.00009396
Iteration 76/1000 | Loss: 0.00012072
Iteration 77/1000 | Loss: 0.00010780
Iteration 78/1000 | Loss: 0.00013382
Iteration 79/1000 | Loss: 0.00011563
Iteration 80/1000 | Loss: 0.00011812
Iteration 81/1000 | Loss: 0.00014962
Iteration 82/1000 | Loss: 0.00014603
Iteration 83/1000 | Loss: 0.00016670
Iteration 84/1000 | Loss: 0.00015284
Iteration 85/1000 | Loss: 0.00032941
Iteration 86/1000 | Loss: 0.00022364
Iteration 87/1000 | Loss: 0.00020314
Iteration 88/1000 | Loss: 0.00015031
Iteration 89/1000 | Loss: 0.00014223
Iteration 90/1000 | Loss: 0.00018265
Iteration 91/1000 | Loss: 0.00021325
Iteration 92/1000 | Loss: 0.00020306
Iteration 93/1000 | Loss: 0.00051731
Iteration 94/1000 | Loss: 0.00016281
Iteration 95/1000 | Loss: 0.00015129
Iteration 96/1000 | Loss: 0.00017524
Iteration 97/1000 | Loss: 0.00017812
Iteration 98/1000 | Loss: 0.00019093
Iteration 99/1000 | Loss: 0.00034713
Iteration 100/1000 | Loss: 0.00022263
Iteration 101/1000 | Loss: 0.00013297
Iteration 102/1000 | Loss: 0.00007891
Iteration 103/1000 | Loss: 0.00011777
Iteration 104/1000 | Loss: 0.00023602
Iteration 105/1000 | Loss: 0.00010871
Iteration 106/1000 | Loss: 0.00011399
Iteration 107/1000 | Loss: 0.00011459
Iteration 108/1000 | Loss: 0.00014843
Iteration 109/1000 | Loss: 0.00006746
Iteration 110/1000 | Loss: 0.00014279
Iteration 111/1000 | Loss: 0.00013166
Iteration 112/1000 | Loss: 0.00011794
Iteration 113/1000 | Loss: 0.00015316
Iteration 114/1000 | Loss: 0.00031972
Iteration 115/1000 | Loss: 0.00013892
Iteration 116/1000 | Loss: 0.00009134
Iteration 117/1000 | Loss: 0.00016971
Iteration 118/1000 | Loss: 0.00006429
Iteration 119/1000 | Loss: 0.00013076
Iteration 120/1000 | Loss: 0.00026566
Iteration 121/1000 | Loss: 0.00012082
Iteration 122/1000 | Loss: 0.00007494
Iteration 123/1000 | Loss: 0.00008050
Iteration 124/1000 | Loss: 0.00008619
Iteration 125/1000 | Loss: 0.00007839
Iteration 126/1000 | Loss: 0.00007394
Iteration 127/1000 | Loss: 0.00009579
Iteration 128/1000 | Loss: 0.00006941
Iteration 129/1000 | Loss: 0.00005163
Iteration 130/1000 | Loss: 0.00002914
Iteration 131/1000 | Loss: 0.00005188
Iteration 132/1000 | Loss: 0.00007048
Iteration 133/1000 | Loss: 0.00007240
Iteration 134/1000 | Loss: 0.00007593
Iteration 135/1000 | Loss: 0.00008391
Iteration 136/1000 | Loss: 0.00007517
Iteration 137/1000 | Loss: 0.00038042
Iteration 138/1000 | Loss: 0.00009065
Iteration 139/1000 | Loss: 0.00007725
Iteration 140/1000 | Loss: 0.00007112
Iteration 141/1000 | Loss: 0.00005382
Iteration 142/1000 | Loss: 0.00005797
Iteration 143/1000 | Loss: 0.00004920
Iteration 144/1000 | Loss: 0.00006599
Iteration 145/1000 | Loss: 0.00005297
Iteration 146/1000 | Loss: 0.00004318
Iteration 147/1000 | Loss: 0.00004715
Iteration 148/1000 | Loss: 0.00006448
Iteration 149/1000 | Loss: 0.00034543
Iteration 150/1000 | Loss: 0.00008839
Iteration 151/1000 | Loss: 0.00011801
Iteration 152/1000 | Loss: 0.00009369
Iteration 153/1000 | Loss: 0.00007781
Iteration 154/1000 | Loss: 0.00014983
Iteration 155/1000 | Loss: 0.00006448
Iteration 156/1000 | Loss: 0.00005835
Iteration 157/1000 | Loss: 0.00006824
Iteration 158/1000 | Loss: 0.00006203
Iteration 159/1000 | Loss: 0.00021326
Iteration 160/1000 | Loss: 0.00009608
Iteration 161/1000 | Loss: 0.00025422
Iteration 162/1000 | Loss: 0.00011582
Iteration 163/1000 | Loss: 0.00021378
Iteration 164/1000 | Loss: 0.00007281
Iteration 165/1000 | Loss: 0.00007538
Iteration 166/1000 | Loss: 0.00004293
Iteration 167/1000 | Loss: 0.00002994
Iteration 168/1000 | Loss: 0.00002607
Iteration 169/1000 | Loss: 0.00002323
Iteration 170/1000 | Loss: 0.00014772
Iteration 171/1000 | Loss: 0.00002183
Iteration 172/1000 | Loss: 0.00002120
Iteration 173/1000 | Loss: 0.00002040
Iteration 174/1000 | Loss: 0.00001971
Iteration 175/1000 | Loss: 0.00001919
Iteration 176/1000 | Loss: 0.00001880
Iteration 177/1000 | Loss: 0.00001849
Iteration 178/1000 | Loss: 0.00001821
Iteration 179/1000 | Loss: 0.00001802
Iteration 180/1000 | Loss: 0.00001800
Iteration 181/1000 | Loss: 0.00001794
Iteration 182/1000 | Loss: 0.00012884
Iteration 183/1000 | Loss: 0.00002127
Iteration 184/1000 | Loss: 0.00001803
Iteration 185/1000 | Loss: 0.00001775
Iteration 186/1000 | Loss: 0.00001766
Iteration 187/1000 | Loss: 0.00001766
Iteration 188/1000 | Loss: 0.00001765
Iteration 189/1000 | Loss: 0.00001761
Iteration 190/1000 | Loss: 0.00001761
Iteration 191/1000 | Loss: 0.00001760
Iteration 192/1000 | Loss: 0.00001759
Iteration 193/1000 | Loss: 0.00001759
Iteration 194/1000 | Loss: 0.00001759
Iteration 195/1000 | Loss: 0.00001759
Iteration 196/1000 | Loss: 0.00001759
Iteration 197/1000 | Loss: 0.00001758
Iteration 198/1000 | Loss: 0.00001757
Iteration 199/1000 | Loss: 0.00001757
Iteration 200/1000 | Loss: 0.00001754
Iteration 201/1000 | Loss: 0.00001753
Iteration 202/1000 | Loss: 0.00001753
Iteration 203/1000 | Loss: 0.00001752
Iteration 204/1000 | Loss: 0.00001752
Iteration 205/1000 | Loss: 0.00001749
Iteration 206/1000 | Loss: 0.00001749
Iteration 207/1000 | Loss: 0.00001749
Iteration 208/1000 | Loss: 0.00001749
Iteration 209/1000 | Loss: 0.00001749
Iteration 210/1000 | Loss: 0.00001749
Iteration 211/1000 | Loss: 0.00001749
Iteration 212/1000 | Loss: 0.00001748
Iteration 213/1000 | Loss: 0.00001748
Iteration 214/1000 | Loss: 0.00001748
Iteration 215/1000 | Loss: 0.00001746
Iteration 216/1000 | Loss: 0.00001746
Iteration 217/1000 | Loss: 0.00001743
Iteration 218/1000 | Loss: 0.00001741
Iteration 219/1000 | Loss: 0.00001740
Iteration 220/1000 | Loss: 0.00001740
Iteration 221/1000 | Loss: 0.00001739
Iteration 222/1000 | Loss: 0.00001737
Iteration 223/1000 | Loss: 0.00001736
Iteration 224/1000 | Loss: 0.00001736
Iteration 225/1000 | Loss: 0.00001736
Iteration 226/1000 | Loss: 0.00001735
Iteration 227/1000 | Loss: 0.00001734
Iteration 228/1000 | Loss: 0.00001734
Iteration 229/1000 | Loss: 0.00001733
Iteration 230/1000 | Loss: 0.00001733
Iteration 231/1000 | Loss: 0.00001728
Iteration 232/1000 | Loss: 0.00001727
Iteration 233/1000 | Loss: 0.00001726
Iteration 234/1000 | Loss: 0.00001725
Iteration 235/1000 | Loss: 0.00001725
Iteration 236/1000 | Loss: 0.00001724
Iteration 237/1000 | Loss: 0.00001724
Iteration 238/1000 | Loss: 0.00001724
Iteration 239/1000 | Loss: 0.00001723
Iteration 240/1000 | Loss: 0.00001723
Iteration 241/1000 | Loss: 0.00001723
Iteration 242/1000 | Loss: 0.00001723
Iteration 243/1000 | Loss: 0.00001723
Iteration 244/1000 | Loss: 0.00001723
Iteration 245/1000 | Loss: 0.00001723
Iteration 246/1000 | Loss: 0.00001723
Iteration 247/1000 | Loss: 0.00001723
Iteration 248/1000 | Loss: 0.00001723
Iteration 249/1000 | Loss: 0.00001723
Iteration 250/1000 | Loss: 0.00001723
Iteration 251/1000 | Loss: 0.00001722
Iteration 252/1000 | Loss: 0.00001722
Iteration 253/1000 | Loss: 0.00001722
Iteration 254/1000 | Loss: 0.00001722
Iteration 255/1000 | Loss: 0.00001722
Iteration 256/1000 | Loss: 0.00001722
Iteration 257/1000 | Loss: 0.00001722
Iteration 258/1000 | Loss: 0.00001721
Iteration 259/1000 | Loss: 0.00001721
Iteration 260/1000 | Loss: 0.00001721
Iteration 261/1000 | Loss: 0.00001721
Iteration 262/1000 | Loss: 0.00001721
Iteration 263/1000 | Loss: 0.00001720
Iteration 264/1000 | Loss: 0.00001720
Iteration 265/1000 | Loss: 0.00001720
Iteration 266/1000 | Loss: 0.00001720
Iteration 267/1000 | Loss: 0.00001720
Iteration 268/1000 | Loss: 0.00001720
Iteration 269/1000 | Loss: 0.00001719
Iteration 270/1000 | Loss: 0.00001719
Iteration 271/1000 | Loss: 0.00001719
Iteration 272/1000 | Loss: 0.00001719
Iteration 273/1000 | Loss: 0.00001718
Iteration 274/1000 | Loss: 0.00001718
Iteration 275/1000 | Loss: 0.00001718
Iteration 276/1000 | Loss: 0.00001718
Iteration 277/1000 | Loss: 0.00001717
Iteration 278/1000 | Loss: 0.00001717
Iteration 279/1000 | Loss: 0.00001717
Iteration 280/1000 | Loss: 0.00001717
Iteration 281/1000 | Loss: 0.00001717
Iteration 282/1000 | Loss: 0.00001717
Iteration 283/1000 | Loss: 0.00001716
Iteration 284/1000 | Loss: 0.00001716
Iteration 285/1000 | Loss: 0.00001716
Iteration 286/1000 | Loss: 0.00001716
Iteration 287/1000 | Loss: 0.00001716
Iteration 288/1000 | Loss: 0.00001715
Iteration 289/1000 | Loss: 0.00001715
Iteration 290/1000 | Loss: 0.00001715
Iteration 291/1000 | Loss: 0.00001715
Iteration 292/1000 | Loss: 0.00001715
Iteration 293/1000 | Loss: 0.00001715
Iteration 294/1000 | Loss: 0.00001715
Iteration 295/1000 | Loss: 0.00001714
Iteration 296/1000 | Loss: 0.00001714
Iteration 297/1000 | Loss: 0.00001714
Iteration 298/1000 | Loss: 0.00001714
Iteration 299/1000 | Loss: 0.00001714
Iteration 300/1000 | Loss: 0.00001714
Iteration 301/1000 | Loss: 0.00001714
Iteration 302/1000 | Loss: 0.00001714
Iteration 303/1000 | Loss: 0.00001714
Iteration 304/1000 | Loss: 0.00001714
Iteration 305/1000 | Loss: 0.00001714
Iteration 306/1000 | Loss: 0.00001714
Iteration 307/1000 | Loss: 0.00001714
Iteration 308/1000 | Loss: 0.00001714
Iteration 309/1000 | Loss: 0.00001714
Iteration 310/1000 | Loss: 0.00001714
Iteration 311/1000 | Loss: 0.00001714
Iteration 312/1000 | Loss: 0.00001714
Iteration 313/1000 | Loss: 0.00001714
Iteration 314/1000 | Loss: 0.00001714
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 314. Stopping optimization.
Last 5 losses: [1.713808160275221e-05, 1.713808160275221e-05, 1.713808160275221e-05, 1.713808160275221e-05, 1.713808160275221e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.713808160275221e-05

Optimization complete. Final v2v error: 3.5252926349639893 mm

Highest mean error: 3.911031484603882 mm for frame 7

Lowest mean error: 3.4342856407165527 mm for frame 51

Saving results

Total time: 297.6414227485657
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00351711
Iteration 2/25 | Loss: 0.00154821
Iteration 3/25 | Loss: 0.00127680
Iteration 4/25 | Loss: 0.00123577
Iteration 5/25 | Loss: 0.00122866
Iteration 6/25 | Loss: 0.00122623
Iteration 7/25 | Loss: 0.00122548
Iteration 8/25 | Loss: 0.00122544
Iteration 9/25 | Loss: 0.00122544
Iteration 10/25 | Loss: 0.00122544
Iteration 11/25 | Loss: 0.00122544
Iteration 12/25 | Loss: 0.00122544
Iteration 13/25 | Loss: 0.00122544
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012254394823685288, 0.0012254394823685288, 0.0012254394823685288, 0.0012254394823685288, 0.0012254394823685288]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012254394823685288

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37496364
Iteration 2/25 | Loss: 0.00112775
Iteration 3/25 | Loss: 0.00112775
Iteration 4/25 | Loss: 0.00112775
Iteration 5/25 | Loss: 0.00112775
Iteration 6/25 | Loss: 0.00112775
Iteration 7/25 | Loss: 0.00112775
Iteration 8/25 | Loss: 0.00112775
Iteration 9/25 | Loss: 0.00112775
Iteration 10/25 | Loss: 0.00112775
Iteration 11/25 | Loss: 0.00112775
Iteration 12/25 | Loss: 0.00112775
Iteration 13/25 | Loss: 0.00112775
Iteration 14/25 | Loss: 0.00112775
Iteration 15/25 | Loss: 0.00112775
Iteration 16/25 | Loss: 0.00112775
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011277480516582727, 0.0011277480516582727, 0.0011277480516582727, 0.0011277480516582727, 0.0011277480516582727]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011277480516582727

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112775
Iteration 2/1000 | Loss: 0.00004968
Iteration 3/1000 | Loss: 0.00002832
Iteration 4/1000 | Loss: 0.00001987
Iteration 5/1000 | Loss: 0.00001764
Iteration 6/1000 | Loss: 0.00001671
Iteration 7/1000 | Loss: 0.00001592
Iteration 8/1000 | Loss: 0.00001546
Iteration 9/1000 | Loss: 0.00001506
Iteration 10/1000 | Loss: 0.00001470
Iteration 11/1000 | Loss: 0.00001447
Iteration 12/1000 | Loss: 0.00001436
Iteration 13/1000 | Loss: 0.00001428
Iteration 14/1000 | Loss: 0.00001413
Iteration 15/1000 | Loss: 0.00001412
Iteration 16/1000 | Loss: 0.00001412
Iteration 17/1000 | Loss: 0.00001408
Iteration 18/1000 | Loss: 0.00001405
Iteration 19/1000 | Loss: 0.00001404
Iteration 20/1000 | Loss: 0.00001404
Iteration 21/1000 | Loss: 0.00001404
Iteration 22/1000 | Loss: 0.00001403
Iteration 23/1000 | Loss: 0.00001403
Iteration 24/1000 | Loss: 0.00001403
Iteration 25/1000 | Loss: 0.00001402
Iteration 26/1000 | Loss: 0.00001401
Iteration 27/1000 | Loss: 0.00001401
Iteration 28/1000 | Loss: 0.00001400
Iteration 29/1000 | Loss: 0.00001400
Iteration 30/1000 | Loss: 0.00001399
Iteration 31/1000 | Loss: 0.00001399
Iteration 32/1000 | Loss: 0.00001399
Iteration 33/1000 | Loss: 0.00001398
Iteration 34/1000 | Loss: 0.00001398
Iteration 35/1000 | Loss: 0.00001398
Iteration 36/1000 | Loss: 0.00001398
Iteration 37/1000 | Loss: 0.00001397
Iteration 38/1000 | Loss: 0.00001397
Iteration 39/1000 | Loss: 0.00001397
Iteration 40/1000 | Loss: 0.00001397
Iteration 41/1000 | Loss: 0.00001396
Iteration 42/1000 | Loss: 0.00001396
Iteration 43/1000 | Loss: 0.00001396
Iteration 44/1000 | Loss: 0.00001395
Iteration 45/1000 | Loss: 0.00001395
Iteration 46/1000 | Loss: 0.00001394
Iteration 47/1000 | Loss: 0.00001394
Iteration 48/1000 | Loss: 0.00001394
Iteration 49/1000 | Loss: 0.00001394
Iteration 50/1000 | Loss: 0.00001394
Iteration 51/1000 | Loss: 0.00001393
Iteration 52/1000 | Loss: 0.00001393
Iteration 53/1000 | Loss: 0.00001391
Iteration 54/1000 | Loss: 0.00001391
Iteration 55/1000 | Loss: 0.00001390
Iteration 56/1000 | Loss: 0.00001390
Iteration 57/1000 | Loss: 0.00001390
Iteration 58/1000 | Loss: 0.00001390
Iteration 59/1000 | Loss: 0.00001389
Iteration 60/1000 | Loss: 0.00001389
Iteration 61/1000 | Loss: 0.00001389
Iteration 62/1000 | Loss: 0.00001389
Iteration 63/1000 | Loss: 0.00001389
Iteration 64/1000 | Loss: 0.00001389
Iteration 65/1000 | Loss: 0.00001389
Iteration 66/1000 | Loss: 0.00001389
Iteration 67/1000 | Loss: 0.00001389
Iteration 68/1000 | Loss: 0.00001389
Iteration 69/1000 | Loss: 0.00001389
Iteration 70/1000 | Loss: 0.00001388
Iteration 71/1000 | Loss: 0.00001388
Iteration 72/1000 | Loss: 0.00001388
Iteration 73/1000 | Loss: 0.00001388
Iteration 74/1000 | Loss: 0.00001387
Iteration 75/1000 | Loss: 0.00001387
Iteration 76/1000 | Loss: 0.00001387
Iteration 77/1000 | Loss: 0.00001387
Iteration 78/1000 | Loss: 0.00001386
Iteration 79/1000 | Loss: 0.00001386
Iteration 80/1000 | Loss: 0.00001386
Iteration 81/1000 | Loss: 0.00001386
Iteration 82/1000 | Loss: 0.00001386
Iteration 83/1000 | Loss: 0.00001385
Iteration 84/1000 | Loss: 0.00001385
Iteration 85/1000 | Loss: 0.00001385
Iteration 86/1000 | Loss: 0.00001385
Iteration 87/1000 | Loss: 0.00001385
Iteration 88/1000 | Loss: 0.00001384
Iteration 89/1000 | Loss: 0.00001384
Iteration 90/1000 | Loss: 0.00001384
Iteration 91/1000 | Loss: 0.00001384
Iteration 92/1000 | Loss: 0.00001384
Iteration 93/1000 | Loss: 0.00001384
Iteration 94/1000 | Loss: 0.00001384
Iteration 95/1000 | Loss: 0.00001383
Iteration 96/1000 | Loss: 0.00001383
Iteration 97/1000 | Loss: 0.00001383
Iteration 98/1000 | Loss: 0.00001383
Iteration 99/1000 | Loss: 0.00001383
Iteration 100/1000 | Loss: 0.00001383
Iteration 101/1000 | Loss: 0.00001383
Iteration 102/1000 | Loss: 0.00001383
Iteration 103/1000 | Loss: 0.00001383
Iteration 104/1000 | Loss: 0.00001383
Iteration 105/1000 | Loss: 0.00001383
Iteration 106/1000 | Loss: 0.00001383
Iteration 107/1000 | Loss: 0.00001382
Iteration 108/1000 | Loss: 0.00001382
Iteration 109/1000 | Loss: 0.00001382
Iteration 110/1000 | Loss: 0.00001382
Iteration 111/1000 | Loss: 0.00001382
Iteration 112/1000 | Loss: 0.00001382
Iteration 113/1000 | Loss: 0.00001382
Iteration 114/1000 | Loss: 0.00001382
Iteration 115/1000 | Loss: 0.00001382
Iteration 116/1000 | Loss: 0.00001381
Iteration 117/1000 | Loss: 0.00001381
Iteration 118/1000 | Loss: 0.00001381
Iteration 119/1000 | Loss: 0.00001381
Iteration 120/1000 | Loss: 0.00001381
Iteration 121/1000 | Loss: 0.00001381
Iteration 122/1000 | Loss: 0.00001381
Iteration 123/1000 | Loss: 0.00001381
Iteration 124/1000 | Loss: 0.00001381
Iteration 125/1000 | Loss: 0.00001381
Iteration 126/1000 | Loss: 0.00001381
Iteration 127/1000 | Loss: 0.00001380
Iteration 128/1000 | Loss: 0.00001380
Iteration 129/1000 | Loss: 0.00001380
Iteration 130/1000 | Loss: 0.00001380
Iteration 131/1000 | Loss: 0.00001380
Iteration 132/1000 | Loss: 0.00001380
Iteration 133/1000 | Loss: 0.00001380
Iteration 134/1000 | Loss: 0.00001380
Iteration 135/1000 | Loss: 0.00001380
Iteration 136/1000 | Loss: 0.00001380
Iteration 137/1000 | Loss: 0.00001380
Iteration 138/1000 | Loss: 0.00001380
Iteration 139/1000 | Loss: 0.00001380
Iteration 140/1000 | Loss: 0.00001380
Iteration 141/1000 | Loss: 0.00001380
Iteration 142/1000 | Loss: 0.00001380
Iteration 143/1000 | Loss: 0.00001380
Iteration 144/1000 | Loss: 0.00001380
Iteration 145/1000 | Loss: 0.00001379
Iteration 146/1000 | Loss: 0.00001379
Iteration 147/1000 | Loss: 0.00001379
Iteration 148/1000 | Loss: 0.00001379
Iteration 149/1000 | Loss: 0.00001379
Iteration 150/1000 | Loss: 0.00001379
Iteration 151/1000 | Loss: 0.00001379
Iteration 152/1000 | Loss: 0.00001379
Iteration 153/1000 | Loss: 0.00001379
Iteration 154/1000 | Loss: 0.00001379
Iteration 155/1000 | Loss: 0.00001379
Iteration 156/1000 | Loss: 0.00001379
Iteration 157/1000 | Loss: 0.00001379
Iteration 158/1000 | Loss: 0.00001379
Iteration 159/1000 | Loss: 0.00001379
Iteration 160/1000 | Loss: 0.00001379
Iteration 161/1000 | Loss: 0.00001379
Iteration 162/1000 | Loss: 0.00001378
Iteration 163/1000 | Loss: 0.00001378
Iteration 164/1000 | Loss: 0.00001378
Iteration 165/1000 | Loss: 0.00001378
Iteration 166/1000 | Loss: 0.00001378
Iteration 167/1000 | Loss: 0.00001378
Iteration 168/1000 | Loss: 0.00001378
Iteration 169/1000 | Loss: 0.00001378
Iteration 170/1000 | Loss: 0.00001378
Iteration 171/1000 | Loss: 0.00001378
Iteration 172/1000 | Loss: 0.00001378
Iteration 173/1000 | Loss: 0.00001378
Iteration 174/1000 | Loss: 0.00001378
Iteration 175/1000 | Loss: 0.00001378
Iteration 176/1000 | Loss: 0.00001378
Iteration 177/1000 | Loss: 0.00001378
Iteration 178/1000 | Loss: 0.00001378
Iteration 179/1000 | Loss: 0.00001378
Iteration 180/1000 | Loss: 0.00001378
Iteration 181/1000 | Loss: 0.00001378
Iteration 182/1000 | Loss: 0.00001378
Iteration 183/1000 | Loss: 0.00001378
Iteration 184/1000 | Loss: 0.00001378
Iteration 185/1000 | Loss: 0.00001378
Iteration 186/1000 | Loss: 0.00001378
Iteration 187/1000 | Loss: 0.00001378
Iteration 188/1000 | Loss: 0.00001378
Iteration 189/1000 | Loss: 0.00001378
Iteration 190/1000 | Loss: 0.00001378
Iteration 191/1000 | Loss: 0.00001378
Iteration 192/1000 | Loss: 0.00001378
Iteration 193/1000 | Loss: 0.00001378
Iteration 194/1000 | Loss: 0.00001378
Iteration 195/1000 | Loss: 0.00001378
Iteration 196/1000 | Loss: 0.00001378
Iteration 197/1000 | Loss: 0.00001378
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [1.377582702843938e-05, 1.377582702843938e-05, 1.377582702843938e-05, 1.377582702843938e-05, 1.377582702843938e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.377582702843938e-05

Optimization complete. Final v2v error: 3.183656930923462 mm

Highest mean error: 3.573312997817993 mm for frame 38

Lowest mean error: 2.7572250366210938 mm for frame 11

Saving results

Total time: 38.45663380622864
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00962438
Iteration 2/25 | Loss: 0.00274774
Iteration 3/25 | Loss: 0.00214613
Iteration 4/25 | Loss: 0.00212781
Iteration 5/25 | Loss: 0.00194065
Iteration 6/25 | Loss: 0.00184249
Iteration 7/25 | Loss: 0.00178809
Iteration 8/25 | Loss: 0.00172329
Iteration 9/25 | Loss: 0.00159065
Iteration 10/25 | Loss: 0.00150791
Iteration 11/25 | Loss: 0.00145984
Iteration 12/25 | Loss: 0.00145243
Iteration 13/25 | Loss: 0.00142165
Iteration 14/25 | Loss: 0.00141395
Iteration 15/25 | Loss: 0.00139892
Iteration 16/25 | Loss: 0.00139083
Iteration 17/25 | Loss: 0.00138753
Iteration 18/25 | Loss: 0.00138600
Iteration 19/25 | Loss: 0.00138484
Iteration 20/25 | Loss: 0.00140288
Iteration 21/25 | Loss: 0.00137500
Iteration 22/25 | Loss: 0.00136683
Iteration 23/25 | Loss: 0.00135443
Iteration 24/25 | Loss: 0.00134685
Iteration 25/25 | Loss: 0.00134496

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32556152
Iteration 2/25 | Loss: 0.00158198
Iteration 3/25 | Loss: 0.00179172
Iteration 4/25 | Loss: 0.00158184
Iteration 5/25 | Loss: 0.00158184
Iteration 6/25 | Loss: 0.00158184
Iteration 7/25 | Loss: 0.00158184
Iteration 8/25 | Loss: 0.00158184
Iteration 9/25 | Loss: 0.00158184
Iteration 10/25 | Loss: 0.00158184
Iteration 11/25 | Loss: 0.00158184
Iteration 12/25 | Loss: 0.00158184
Iteration 13/25 | Loss: 0.00158184
Iteration 14/25 | Loss: 0.00158184
Iteration 15/25 | Loss: 0.00158184
Iteration 16/25 | Loss: 0.00158184
Iteration 17/25 | Loss: 0.00158184
Iteration 18/25 | Loss: 0.00158184
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0015818369574844837, 0.0015818369574844837, 0.0015818369574844837, 0.0015818369574844837, 0.0015818369574844837]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015818369574844837

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00158184
Iteration 2/1000 | Loss: 0.00098128
Iteration 3/1000 | Loss: 0.00032106
Iteration 4/1000 | Loss: 0.00023266
Iteration 5/1000 | Loss: 0.00008865
Iteration 6/1000 | Loss: 0.00006161
Iteration 7/1000 | Loss: 0.00004247
Iteration 8/1000 | Loss: 0.00003375
Iteration 9/1000 | Loss: 0.00002998
Iteration 10/1000 | Loss: 0.00018370
Iteration 11/1000 | Loss: 0.00008713
Iteration 12/1000 | Loss: 0.00002569
Iteration 13/1000 | Loss: 0.00024767
Iteration 14/1000 | Loss: 0.00003436
Iteration 15/1000 | Loss: 0.00003015
Iteration 16/1000 | Loss: 0.00002748
Iteration 17/1000 | Loss: 0.00016908
Iteration 18/1000 | Loss: 0.00005059
Iteration 19/1000 | Loss: 0.00003455
Iteration 20/1000 | Loss: 0.00002748
Iteration 21/1000 | Loss: 0.00002471
Iteration 22/1000 | Loss: 0.00002278
Iteration 23/1000 | Loss: 0.00002197
Iteration 24/1000 | Loss: 0.00002147
Iteration 25/1000 | Loss: 0.00002090
Iteration 26/1000 | Loss: 0.00002037
Iteration 27/1000 | Loss: 0.00021485
Iteration 28/1000 | Loss: 0.00002764
Iteration 29/1000 | Loss: 0.00002377
Iteration 30/1000 | Loss: 0.00002258
Iteration 31/1000 | Loss: 0.00002051
Iteration 32/1000 | Loss: 0.00001896
Iteration 33/1000 | Loss: 0.00001842
Iteration 34/1000 | Loss: 0.00001803
Iteration 35/1000 | Loss: 0.00001775
Iteration 36/1000 | Loss: 0.00002081
Iteration 37/1000 | Loss: 0.00001776
Iteration 38/1000 | Loss: 0.00001749
Iteration 39/1000 | Loss: 0.00001727
Iteration 40/1000 | Loss: 0.00001712
Iteration 41/1000 | Loss: 0.00001712
Iteration 42/1000 | Loss: 0.00001711
Iteration 43/1000 | Loss: 0.00001711
Iteration 44/1000 | Loss: 0.00001709
Iteration 45/1000 | Loss: 0.00001709
Iteration 46/1000 | Loss: 0.00001706
Iteration 47/1000 | Loss: 0.00001701
Iteration 48/1000 | Loss: 0.00001693
Iteration 49/1000 | Loss: 0.00001682
Iteration 50/1000 | Loss: 0.00001674
Iteration 51/1000 | Loss: 0.00001672
Iteration 52/1000 | Loss: 0.00001671
Iteration 53/1000 | Loss: 0.00001670
Iteration 54/1000 | Loss: 0.00001669
Iteration 55/1000 | Loss: 0.00001668
Iteration 56/1000 | Loss: 0.00001667
Iteration 57/1000 | Loss: 0.00001667
Iteration 58/1000 | Loss: 0.00001666
Iteration 59/1000 | Loss: 0.00001666
Iteration 60/1000 | Loss: 0.00001666
Iteration 61/1000 | Loss: 0.00001666
Iteration 62/1000 | Loss: 0.00001666
Iteration 63/1000 | Loss: 0.00001665
Iteration 64/1000 | Loss: 0.00001665
Iteration 65/1000 | Loss: 0.00001664
Iteration 66/1000 | Loss: 0.00001664
Iteration 67/1000 | Loss: 0.00001663
Iteration 68/1000 | Loss: 0.00001663
Iteration 69/1000 | Loss: 0.00001661
Iteration 70/1000 | Loss: 0.00001661
Iteration 71/1000 | Loss: 0.00001660
Iteration 72/1000 | Loss: 0.00001660
Iteration 73/1000 | Loss: 0.00001660
Iteration 74/1000 | Loss: 0.00001660
Iteration 75/1000 | Loss: 0.00001660
Iteration 76/1000 | Loss: 0.00001659
Iteration 77/1000 | Loss: 0.00001659
Iteration 78/1000 | Loss: 0.00001658
Iteration 79/1000 | Loss: 0.00001658
Iteration 80/1000 | Loss: 0.00001658
Iteration 81/1000 | Loss: 0.00001658
Iteration 82/1000 | Loss: 0.00001657
Iteration 83/1000 | Loss: 0.00001657
Iteration 84/1000 | Loss: 0.00001657
Iteration 85/1000 | Loss: 0.00001657
Iteration 86/1000 | Loss: 0.00001656
Iteration 87/1000 | Loss: 0.00001656
Iteration 88/1000 | Loss: 0.00001655
Iteration 89/1000 | Loss: 0.00001655
Iteration 90/1000 | Loss: 0.00001655
Iteration 91/1000 | Loss: 0.00001655
Iteration 92/1000 | Loss: 0.00001655
Iteration 93/1000 | Loss: 0.00001655
Iteration 94/1000 | Loss: 0.00001655
Iteration 95/1000 | Loss: 0.00001654
Iteration 96/1000 | Loss: 0.00001654
Iteration 97/1000 | Loss: 0.00001654
Iteration 98/1000 | Loss: 0.00001654
Iteration 99/1000 | Loss: 0.00001654
Iteration 100/1000 | Loss: 0.00001654
Iteration 101/1000 | Loss: 0.00001654
Iteration 102/1000 | Loss: 0.00001654
Iteration 103/1000 | Loss: 0.00001654
Iteration 104/1000 | Loss: 0.00001654
Iteration 105/1000 | Loss: 0.00001654
Iteration 106/1000 | Loss: 0.00001653
Iteration 107/1000 | Loss: 0.00001653
Iteration 108/1000 | Loss: 0.00001653
Iteration 109/1000 | Loss: 0.00001653
Iteration 110/1000 | Loss: 0.00001653
Iteration 111/1000 | Loss: 0.00001653
Iteration 112/1000 | Loss: 0.00001653
Iteration 113/1000 | Loss: 0.00001652
Iteration 114/1000 | Loss: 0.00001652
Iteration 115/1000 | Loss: 0.00001652
Iteration 116/1000 | Loss: 0.00001651
Iteration 117/1000 | Loss: 0.00001651
Iteration 118/1000 | Loss: 0.00001651
Iteration 119/1000 | Loss: 0.00001651
Iteration 120/1000 | Loss: 0.00001651
Iteration 121/1000 | Loss: 0.00001651
Iteration 122/1000 | Loss: 0.00001650
Iteration 123/1000 | Loss: 0.00001650
Iteration 124/1000 | Loss: 0.00001650
Iteration 125/1000 | Loss: 0.00001650
Iteration 126/1000 | Loss: 0.00001650
Iteration 127/1000 | Loss: 0.00001650
Iteration 128/1000 | Loss: 0.00001650
Iteration 129/1000 | Loss: 0.00001650
Iteration 130/1000 | Loss: 0.00001650
Iteration 131/1000 | Loss: 0.00001650
Iteration 132/1000 | Loss: 0.00001650
Iteration 133/1000 | Loss: 0.00001650
Iteration 134/1000 | Loss: 0.00001650
Iteration 135/1000 | Loss: 0.00001650
Iteration 136/1000 | Loss: 0.00001650
Iteration 137/1000 | Loss: 0.00001650
Iteration 138/1000 | Loss: 0.00001650
Iteration 139/1000 | Loss: 0.00001650
Iteration 140/1000 | Loss: 0.00001649
Iteration 141/1000 | Loss: 0.00001649
Iteration 142/1000 | Loss: 0.00001649
Iteration 143/1000 | Loss: 0.00001649
Iteration 144/1000 | Loss: 0.00001649
Iteration 145/1000 | Loss: 0.00001649
Iteration 146/1000 | Loss: 0.00001649
Iteration 147/1000 | Loss: 0.00001649
Iteration 148/1000 | Loss: 0.00001649
Iteration 149/1000 | Loss: 0.00001649
Iteration 150/1000 | Loss: 0.00001649
Iteration 151/1000 | Loss: 0.00001649
Iteration 152/1000 | Loss: 0.00001649
Iteration 153/1000 | Loss: 0.00001649
Iteration 154/1000 | Loss: 0.00001649
Iteration 155/1000 | Loss: 0.00001649
Iteration 156/1000 | Loss: 0.00001649
Iteration 157/1000 | Loss: 0.00001649
Iteration 158/1000 | Loss: 0.00001649
Iteration 159/1000 | Loss: 0.00001649
Iteration 160/1000 | Loss: 0.00001649
Iteration 161/1000 | Loss: 0.00001649
Iteration 162/1000 | Loss: 0.00001649
Iteration 163/1000 | Loss: 0.00001649
Iteration 164/1000 | Loss: 0.00001649
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.6488378605572507e-05, 1.6488378605572507e-05, 1.6488378605572507e-05, 1.6488378605572507e-05, 1.6488378605572507e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6488378605572507e-05

Optimization complete. Final v2v error: 3.4257142543792725 mm

Highest mean error: 4.326991558074951 mm for frame 84

Lowest mean error: 3.0039215087890625 mm for frame 216

Saving results

Total time: 127.64019012451172
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00987235
Iteration 2/25 | Loss: 0.00987234
Iteration 3/25 | Loss: 0.00217405
Iteration 4/25 | Loss: 0.00141079
Iteration 5/25 | Loss: 0.00132587
Iteration 6/25 | Loss: 0.00130622
Iteration 7/25 | Loss: 0.00131827
Iteration 8/25 | Loss: 0.00130398
Iteration 9/25 | Loss: 0.00129182
Iteration 10/25 | Loss: 0.00128668
Iteration 11/25 | Loss: 0.00127678
Iteration 12/25 | Loss: 0.00127751
Iteration 13/25 | Loss: 0.00127234
Iteration 14/25 | Loss: 0.00127134
Iteration 15/25 | Loss: 0.00126869
Iteration 16/25 | Loss: 0.00126720
Iteration 17/25 | Loss: 0.00126673
Iteration 18/25 | Loss: 0.00126817
Iteration 19/25 | Loss: 0.00126905
Iteration 20/25 | Loss: 0.00126702
Iteration 21/25 | Loss: 0.00126374
Iteration 22/25 | Loss: 0.00126263
Iteration 23/25 | Loss: 0.00126165
Iteration 24/25 | Loss: 0.00126203
Iteration 25/25 | Loss: 0.00126113

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42654276
Iteration 2/25 | Loss: 0.00104550
Iteration 3/25 | Loss: 0.00104550
Iteration 4/25 | Loss: 0.00104550
Iteration 5/25 | Loss: 0.00104550
Iteration 6/25 | Loss: 0.00104550
Iteration 7/25 | Loss: 0.00104550
Iteration 8/25 | Loss: 0.00104550
Iteration 9/25 | Loss: 0.00104550
Iteration 10/25 | Loss: 0.00104550
Iteration 11/25 | Loss: 0.00104550
Iteration 12/25 | Loss: 0.00104550
Iteration 13/25 | Loss: 0.00104550
Iteration 14/25 | Loss: 0.00104550
Iteration 15/25 | Loss: 0.00104550
Iteration 16/25 | Loss: 0.00104550
Iteration 17/25 | Loss: 0.00104550
Iteration 18/25 | Loss: 0.00104550
Iteration 19/25 | Loss: 0.00104550
Iteration 20/25 | Loss: 0.00104550
Iteration 21/25 | Loss: 0.00104550
Iteration 22/25 | Loss: 0.00104550
Iteration 23/25 | Loss: 0.00104550
Iteration 24/25 | Loss: 0.00104550
Iteration 25/25 | Loss: 0.00104550

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104550
Iteration 2/1000 | Loss: 0.00003410
Iteration 3/1000 | Loss: 0.00002593
Iteration 4/1000 | Loss: 0.00003436
Iteration 5/1000 | Loss: 0.00002241
Iteration 6/1000 | Loss: 0.00002646
Iteration 7/1000 | Loss: 0.00002675
Iteration 8/1000 | Loss: 0.00002145
Iteration 9/1000 | Loss: 0.00002749
Iteration 10/1000 | Loss: 0.00002149
Iteration 11/1000 | Loss: 0.00002657
Iteration 12/1000 | Loss: 0.00002494
Iteration 13/1000 | Loss: 0.00002287
Iteration 14/1000 | Loss: 0.00003324
Iteration 15/1000 | Loss: 0.00002964
Iteration 16/1000 | Loss: 0.00002152
Iteration 17/1000 | Loss: 0.00002729
Iteration 18/1000 | Loss: 0.00003176
Iteration 19/1000 | Loss: 0.00003092
Iteration 20/1000 | Loss: 0.00004228
Iteration 21/1000 | Loss: 0.00003278
Iteration 22/1000 | Loss: 0.00004185
Iteration 23/1000 | Loss: 0.00002866
Iteration 24/1000 | Loss: 0.00001731
Iteration 25/1000 | Loss: 0.00002661
Iteration 26/1000 | Loss: 0.00003538
Iteration 27/1000 | Loss: 0.00002990
Iteration 28/1000 | Loss: 0.00003068
Iteration 29/1000 | Loss: 0.00002879
Iteration 30/1000 | Loss: 0.00002676
Iteration 31/1000 | Loss: 0.00002341
Iteration 32/1000 | Loss: 0.00003123
Iteration 33/1000 | Loss: 0.00002719
Iteration 34/1000 | Loss: 0.00002039
Iteration 35/1000 | Loss: 0.00002031
Iteration 36/1000 | Loss: 0.00002565
Iteration 37/1000 | Loss: 0.00002580
Iteration 38/1000 | Loss: 0.00002867
Iteration 39/1000 | Loss: 0.00002630
Iteration 40/1000 | Loss: 0.00002985
Iteration 41/1000 | Loss: 0.00002934
Iteration 42/1000 | Loss: 0.00003201
Iteration 43/1000 | Loss: 0.00002695
Iteration 44/1000 | Loss: 0.00004060
Iteration 45/1000 | Loss: 0.00003371
Iteration 46/1000 | Loss: 0.00002672
Iteration 47/1000 | Loss: 0.00002572
Iteration 48/1000 | Loss: 0.00001917
Iteration 49/1000 | Loss: 0.00001754
Iteration 50/1000 | Loss: 0.00001660
Iteration 51/1000 | Loss: 0.00001620
Iteration 52/1000 | Loss: 0.00001617
Iteration 53/1000 | Loss: 0.00001616
Iteration 54/1000 | Loss: 0.00001616
Iteration 55/1000 | Loss: 0.00001613
Iteration 56/1000 | Loss: 0.00001613
Iteration 57/1000 | Loss: 0.00001612
Iteration 58/1000 | Loss: 0.00001612
Iteration 59/1000 | Loss: 0.00001609
Iteration 60/1000 | Loss: 0.00001603
Iteration 61/1000 | Loss: 0.00001599
Iteration 62/1000 | Loss: 0.00001599
Iteration 63/1000 | Loss: 0.00001598
Iteration 64/1000 | Loss: 0.00001598
Iteration 65/1000 | Loss: 0.00001598
Iteration 66/1000 | Loss: 0.00001596
Iteration 67/1000 | Loss: 0.00001595
Iteration 68/1000 | Loss: 0.00001593
Iteration 69/1000 | Loss: 0.00001592
Iteration 70/1000 | Loss: 0.00001592
Iteration 71/1000 | Loss: 0.00001592
Iteration 72/1000 | Loss: 0.00001591
Iteration 73/1000 | Loss: 0.00001591
Iteration 74/1000 | Loss: 0.00001590
Iteration 75/1000 | Loss: 0.00001590
Iteration 76/1000 | Loss: 0.00001589
Iteration 77/1000 | Loss: 0.00001589
Iteration 78/1000 | Loss: 0.00001589
Iteration 79/1000 | Loss: 0.00001589
Iteration 80/1000 | Loss: 0.00001589
Iteration 81/1000 | Loss: 0.00001589
Iteration 82/1000 | Loss: 0.00001589
Iteration 83/1000 | Loss: 0.00001589
Iteration 84/1000 | Loss: 0.00001589
Iteration 85/1000 | Loss: 0.00001589
Iteration 86/1000 | Loss: 0.00001589
Iteration 87/1000 | Loss: 0.00001586
Iteration 88/1000 | Loss: 0.00001586
Iteration 89/1000 | Loss: 0.00001585
Iteration 90/1000 | Loss: 0.00001585
Iteration 91/1000 | Loss: 0.00001585
Iteration 92/1000 | Loss: 0.00001585
Iteration 93/1000 | Loss: 0.00001584
Iteration 94/1000 | Loss: 0.00001584
Iteration 95/1000 | Loss: 0.00001583
Iteration 96/1000 | Loss: 0.00001583
Iteration 97/1000 | Loss: 0.00001582
Iteration 98/1000 | Loss: 0.00001582
Iteration 99/1000 | Loss: 0.00001582
Iteration 100/1000 | Loss: 0.00001582
Iteration 101/1000 | Loss: 0.00001582
Iteration 102/1000 | Loss: 0.00001582
Iteration 103/1000 | Loss: 0.00001581
Iteration 104/1000 | Loss: 0.00001581
Iteration 105/1000 | Loss: 0.00001581
Iteration 106/1000 | Loss: 0.00001581
Iteration 107/1000 | Loss: 0.00001581
Iteration 108/1000 | Loss: 0.00001581
Iteration 109/1000 | Loss: 0.00001580
Iteration 110/1000 | Loss: 0.00001580
Iteration 111/1000 | Loss: 0.00001579
Iteration 112/1000 | Loss: 0.00001579
Iteration 113/1000 | Loss: 0.00001579
Iteration 114/1000 | Loss: 0.00001578
Iteration 115/1000 | Loss: 0.00001578
Iteration 116/1000 | Loss: 0.00001578
Iteration 117/1000 | Loss: 0.00001577
Iteration 118/1000 | Loss: 0.00001577
Iteration 119/1000 | Loss: 0.00001576
Iteration 120/1000 | Loss: 0.00001576
Iteration 121/1000 | Loss: 0.00001575
Iteration 122/1000 | Loss: 0.00001575
Iteration 123/1000 | Loss: 0.00001574
Iteration 124/1000 | Loss: 0.00001574
Iteration 125/1000 | Loss: 0.00001573
Iteration 126/1000 | Loss: 0.00001573
Iteration 127/1000 | Loss: 0.00001572
Iteration 128/1000 | Loss: 0.00001572
Iteration 129/1000 | Loss: 0.00001572
Iteration 130/1000 | Loss: 0.00001571
Iteration 131/1000 | Loss: 0.00001571
Iteration 132/1000 | Loss: 0.00001570
Iteration 133/1000 | Loss: 0.00001570
Iteration 134/1000 | Loss: 0.00001570
Iteration 135/1000 | Loss: 0.00001569
Iteration 136/1000 | Loss: 0.00001569
Iteration 137/1000 | Loss: 0.00001568
Iteration 138/1000 | Loss: 0.00001568
Iteration 139/1000 | Loss: 0.00001567
Iteration 140/1000 | Loss: 0.00001567
Iteration 141/1000 | Loss: 0.00001567
Iteration 142/1000 | Loss: 0.00001567
Iteration 143/1000 | Loss: 0.00001566
Iteration 144/1000 | Loss: 0.00001566
Iteration 145/1000 | Loss: 0.00001566
Iteration 146/1000 | Loss: 0.00001566
Iteration 147/1000 | Loss: 0.00001566
Iteration 148/1000 | Loss: 0.00001566
Iteration 149/1000 | Loss: 0.00001566
Iteration 150/1000 | Loss: 0.00001566
Iteration 151/1000 | Loss: 0.00001565
Iteration 152/1000 | Loss: 0.00001565
Iteration 153/1000 | Loss: 0.00001565
Iteration 154/1000 | Loss: 0.00001565
Iteration 155/1000 | Loss: 0.00001565
Iteration 156/1000 | Loss: 0.00001565
Iteration 157/1000 | Loss: 0.00001565
Iteration 158/1000 | Loss: 0.00001564
Iteration 159/1000 | Loss: 0.00001564
Iteration 160/1000 | Loss: 0.00001564
Iteration 161/1000 | Loss: 0.00001563
Iteration 162/1000 | Loss: 0.00001562
Iteration 163/1000 | Loss: 0.00001562
Iteration 164/1000 | Loss: 0.00001562
Iteration 165/1000 | Loss: 0.00001562
Iteration 166/1000 | Loss: 0.00001561
Iteration 167/1000 | Loss: 0.00001561
Iteration 168/1000 | Loss: 0.00001561
Iteration 169/1000 | Loss: 0.00001561
Iteration 170/1000 | Loss: 0.00001560
Iteration 171/1000 | Loss: 0.00001559
Iteration 172/1000 | Loss: 0.00001559
Iteration 173/1000 | Loss: 0.00001558
Iteration 174/1000 | Loss: 0.00001557
Iteration 175/1000 | Loss: 0.00001556
Iteration 176/1000 | Loss: 0.00001556
Iteration 177/1000 | Loss: 0.00001556
Iteration 178/1000 | Loss: 0.00001556
Iteration 179/1000 | Loss: 0.00001556
Iteration 180/1000 | Loss: 0.00001556
Iteration 181/1000 | Loss: 0.00001556
Iteration 182/1000 | Loss: 0.00001556
Iteration 183/1000 | Loss: 0.00001556
Iteration 184/1000 | Loss: 0.00001556
Iteration 185/1000 | Loss: 0.00001556
Iteration 186/1000 | Loss: 0.00001555
Iteration 187/1000 | Loss: 0.00001555
Iteration 188/1000 | Loss: 0.00001555
Iteration 189/1000 | Loss: 0.00001555
Iteration 190/1000 | Loss: 0.00001555
Iteration 191/1000 | Loss: 0.00001554
Iteration 192/1000 | Loss: 0.00001554
Iteration 193/1000 | Loss: 0.00001554
Iteration 194/1000 | Loss: 0.00001553
Iteration 195/1000 | Loss: 0.00001553
Iteration 196/1000 | Loss: 0.00001553
Iteration 197/1000 | Loss: 0.00001553
Iteration 198/1000 | Loss: 0.00001553
Iteration 199/1000 | Loss: 0.00001553
Iteration 200/1000 | Loss: 0.00001553
Iteration 201/1000 | Loss: 0.00001552
Iteration 202/1000 | Loss: 0.00001552
Iteration 203/1000 | Loss: 0.00001552
Iteration 204/1000 | Loss: 0.00001552
Iteration 205/1000 | Loss: 0.00001552
Iteration 206/1000 | Loss: 0.00001552
Iteration 207/1000 | Loss: 0.00001551
Iteration 208/1000 | Loss: 0.00001551
Iteration 209/1000 | Loss: 0.00001551
Iteration 210/1000 | Loss: 0.00001551
Iteration 211/1000 | Loss: 0.00001551
Iteration 212/1000 | Loss: 0.00001550
Iteration 213/1000 | Loss: 0.00001550
Iteration 214/1000 | Loss: 0.00001550
Iteration 215/1000 | Loss: 0.00001549
Iteration 216/1000 | Loss: 0.00001549
Iteration 217/1000 | Loss: 0.00001549
Iteration 218/1000 | Loss: 0.00001549
Iteration 219/1000 | Loss: 0.00001549
Iteration 220/1000 | Loss: 0.00001549
Iteration 221/1000 | Loss: 0.00001549
Iteration 222/1000 | Loss: 0.00001549
Iteration 223/1000 | Loss: 0.00001549
Iteration 224/1000 | Loss: 0.00001549
Iteration 225/1000 | Loss: 0.00001549
Iteration 226/1000 | Loss: 0.00001549
Iteration 227/1000 | Loss: 0.00001549
Iteration 228/1000 | Loss: 0.00001548
Iteration 229/1000 | Loss: 0.00001548
Iteration 230/1000 | Loss: 0.00001547
Iteration 231/1000 | Loss: 0.00001547
Iteration 232/1000 | Loss: 0.00001547
Iteration 233/1000 | Loss: 0.00001547
Iteration 234/1000 | Loss: 0.00001546
Iteration 235/1000 | Loss: 0.00001546
Iteration 236/1000 | Loss: 0.00001546
Iteration 237/1000 | Loss: 0.00001546
Iteration 238/1000 | Loss: 0.00001546
Iteration 239/1000 | Loss: 0.00001546
Iteration 240/1000 | Loss: 0.00001545
Iteration 241/1000 | Loss: 0.00001545
Iteration 242/1000 | Loss: 0.00001545
Iteration 243/1000 | Loss: 0.00001545
Iteration 244/1000 | Loss: 0.00001545
Iteration 245/1000 | Loss: 0.00001545
Iteration 246/1000 | Loss: 0.00001544
Iteration 247/1000 | Loss: 0.00001544
Iteration 248/1000 | Loss: 0.00001544
Iteration 249/1000 | Loss: 0.00001544
Iteration 250/1000 | Loss: 0.00001544
Iteration 251/1000 | Loss: 0.00001544
Iteration 252/1000 | Loss: 0.00001544
Iteration 253/1000 | Loss: 0.00001544
Iteration 254/1000 | Loss: 0.00001544
Iteration 255/1000 | Loss: 0.00001544
Iteration 256/1000 | Loss: 0.00001544
Iteration 257/1000 | Loss: 0.00001543
Iteration 258/1000 | Loss: 0.00001543
Iteration 259/1000 | Loss: 0.00001543
Iteration 260/1000 | Loss: 0.00001543
Iteration 261/1000 | Loss: 0.00001542
Iteration 262/1000 | Loss: 0.00001542
Iteration 263/1000 | Loss: 0.00001542
Iteration 264/1000 | Loss: 0.00001542
Iteration 265/1000 | Loss: 0.00001541
Iteration 266/1000 | Loss: 0.00001541
Iteration 267/1000 | Loss: 0.00001541
Iteration 268/1000 | Loss: 0.00001541
Iteration 269/1000 | Loss: 0.00001541
Iteration 270/1000 | Loss: 0.00001541
Iteration 271/1000 | Loss: 0.00001541
Iteration 272/1000 | Loss: 0.00001540
Iteration 273/1000 | Loss: 0.00001540
Iteration 274/1000 | Loss: 0.00001540
Iteration 275/1000 | Loss: 0.00001540
Iteration 276/1000 | Loss: 0.00001540
Iteration 277/1000 | Loss: 0.00001540
Iteration 278/1000 | Loss: 0.00001540
Iteration 279/1000 | Loss: 0.00001539
Iteration 280/1000 | Loss: 0.00001539
Iteration 281/1000 | Loss: 0.00001539
Iteration 282/1000 | Loss: 0.00001539
Iteration 283/1000 | Loss: 0.00001539
Iteration 284/1000 | Loss: 0.00001539
Iteration 285/1000 | Loss: 0.00001538
Iteration 286/1000 | Loss: 0.00001538
Iteration 287/1000 | Loss: 0.00001538
Iteration 288/1000 | Loss: 0.00001538
Iteration 289/1000 | Loss: 0.00001538
Iteration 290/1000 | Loss: 0.00001538
Iteration 291/1000 | Loss: 0.00001538
Iteration 292/1000 | Loss: 0.00001538
Iteration 293/1000 | Loss: 0.00001538
Iteration 294/1000 | Loss: 0.00001538
Iteration 295/1000 | Loss: 0.00001538
Iteration 296/1000 | Loss: 0.00001538
Iteration 297/1000 | Loss: 0.00001537
Iteration 298/1000 | Loss: 0.00001537
Iteration 299/1000 | Loss: 0.00001537
Iteration 300/1000 | Loss: 0.00001537
Iteration 301/1000 | Loss: 0.00001537
Iteration 302/1000 | Loss: 0.00001537
Iteration 303/1000 | Loss: 0.00001537
Iteration 304/1000 | Loss: 0.00001536
Iteration 305/1000 | Loss: 0.00001536
Iteration 306/1000 | Loss: 0.00001536
Iteration 307/1000 | Loss: 0.00001536
Iteration 308/1000 | Loss: 0.00001536
Iteration 309/1000 | Loss: 0.00001536
Iteration 310/1000 | Loss: 0.00001536
Iteration 311/1000 | Loss: 0.00001536
Iteration 312/1000 | Loss: 0.00001536
Iteration 313/1000 | Loss: 0.00001536
Iteration 314/1000 | Loss: 0.00001536
Iteration 315/1000 | Loss: 0.00001535
Iteration 316/1000 | Loss: 0.00001535
Iteration 317/1000 | Loss: 0.00001535
Iteration 318/1000 | Loss: 0.00001535
Iteration 319/1000 | Loss: 0.00001535
Iteration 320/1000 | Loss: 0.00001535
Iteration 321/1000 | Loss: 0.00001535
Iteration 322/1000 | Loss: 0.00001535
Iteration 323/1000 | Loss: 0.00001535
Iteration 324/1000 | Loss: 0.00001535
Iteration 325/1000 | Loss: 0.00001535
Iteration 326/1000 | Loss: 0.00001535
Iteration 327/1000 | Loss: 0.00001535
Iteration 328/1000 | Loss: 0.00001535
Iteration 329/1000 | Loss: 0.00001535
Iteration 330/1000 | Loss: 0.00001535
Iteration 331/1000 | Loss: 0.00001535
Iteration 332/1000 | Loss: 0.00001535
Iteration 333/1000 | Loss: 0.00001535
Iteration 334/1000 | Loss: 0.00001535
Iteration 335/1000 | Loss: 0.00001535
Iteration 336/1000 | Loss: 0.00001535
Iteration 337/1000 | Loss: 0.00001535
Iteration 338/1000 | Loss: 0.00001535
Iteration 339/1000 | Loss: 0.00001535
Iteration 340/1000 | Loss: 0.00001535
Iteration 341/1000 | Loss: 0.00001535
Iteration 342/1000 | Loss: 0.00001535
Iteration 343/1000 | Loss: 0.00001535
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 343. Stopping optimization.
Last 5 losses: [1.5352057744166814e-05, 1.5352057744166814e-05, 1.5352057744166814e-05, 1.5352057744166814e-05, 1.5352057744166814e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5352057744166814e-05

Optimization complete. Final v2v error: 3.2936134338378906 mm

Highest mean error: 5.549293518066406 mm for frame 186

Lowest mean error: 2.9112703800201416 mm for frame 17

Saving results

Total time: 157.21880555152893
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00679248
Iteration 2/25 | Loss: 0.00140678
Iteration 3/25 | Loss: 0.00128699
Iteration 4/25 | Loss: 0.00122562
Iteration 5/25 | Loss: 0.00121846
Iteration 6/25 | Loss: 0.00122949
Iteration 7/25 | Loss: 0.00121169
Iteration 8/25 | Loss: 0.00121447
Iteration 9/25 | Loss: 0.00120899
Iteration 10/25 | Loss: 0.00120895
Iteration 11/25 | Loss: 0.00120894
Iteration 12/25 | Loss: 0.00120894
Iteration 13/25 | Loss: 0.00120894
Iteration 14/25 | Loss: 0.00120894
Iteration 15/25 | Loss: 0.00120894
Iteration 16/25 | Loss: 0.00120894
Iteration 17/25 | Loss: 0.00120894
Iteration 18/25 | Loss: 0.00120894
Iteration 19/25 | Loss: 0.00120893
Iteration 20/25 | Loss: 0.00120893
Iteration 21/25 | Loss: 0.00120893
Iteration 22/25 | Loss: 0.00120893
Iteration 23/25 | Loss: 0.00120893
Iteration 24/25 | Loss: 0.00120893
Iteration 25/25 | Loss: 0.00120893

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.61480570
Iteration 2/25 | Loss: 0.00107926
Iteration 3/25 | Loss: 0.00107926
Iteration 4/25 | Loss: 0.00107926
Iteration 5/25 | Loss: 0.00107926
Iteration 6/25 | Loss: 0.00107926
Iteration 7/25 | Loss: 0.00107926
Iteration 8/25 | Loss: 0.00107926
Iteration 9/25 | Loss: 0.00107926
Iteration 10/25 | Loss: 0.00107926
Iteration 11/25 | Loss: 0.00107926
Iteration 12/25 | Loss: 0.00107925
Iteration 13/25 | Loss: 0.00107925
Iteration 14/25 | Loss: 0.00107925
Iteration 15/25 | Loss: 0.00107925
Iteration 16/25 | Loss: 0.00107925
Iteration 17/25 | Loss: 0.00107925
Iteration 18/25 | Loss: 0.00107925
Iteration 19/25 | Loss: 0.00107925
Iteration 20/25 | Loss: 0.00107925
Iteration 21/25 | Loss: 0.00107925
Iteration 22/25 | Loss: 0.00107925
Iteration 23/25 | Loss: 0.00107925
Iteration 24/25 | Loss: 0.00107925
Iteration 25/25 | Loss: 0.00107925

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107925
Iteration 2/1000 | Loss: 0.00001882
Iteration 3/1000 | Loss: 0.00001530
Iteration 4/1000 | Loss: 0.00001383
Iteration 5/1000 | Loss: 0.00001312
Iteration 6/1000 | Loss: 0.00001276
Iteration 7/1000 | Loss: 0.00001244
Iteration 8/1000 | Loss: 0.00001213
Iteration 9/1000 | Loss: 0.00001195
Iteration 10/1000 | Loss: 0.00001181
Iteration 11/1000 | Loss: 0.00001172
Iteration 12/1000 | Loss: 0.00001170
Iteration 13/1000 | Loss: 0.00001170
Iteration 14/1000 | Loss: 0.00001169
Iteration 15/1000 | Loss: 0.00001169
Iteration 16/1000 | Loss: 0.00001169
Iteration 17/1000 | Loss: 0.00001163
Iteration 18/1000 | Loss: 0.00001163
Iteration 19/1000 | Loss: 0.00001163
Iteration 20/1000 | Loss: 0.00001162
Iteration 21/1000 | Loss: 0.00001162
Iteration 22/1000 | Loss: 0.00001162
Iteration 23/1000 | Loss: 0.00001162
Iteration 24/1000 | Loss: 0.00001158
Iteration 25/1000 | Loss: 0.00001157
Iteration 26/1000 | Loss: 0.00001157
Iteration 27/1000 | Loss: 0.00001153
Iteration 28/1000 | Loss: 0.00001153
Iteration 29/1000 | Loss: 0.00001152
Iteration 30/1000 | Loss: 0.00001152
Iteration 31/1000 | Loss: 0.00001151
Iteration 32/1000 | Loss: 0.00001150
Iteration 33/1000 | Loss: 0.00001150
Iteration 34/1000 | Loss: 0.00001146
Iteration 35/1000 | Loss: 0.00001146
Iteration 36/1000 | Loss: 0.00001145
Iteration 37/1000 | Loss: 0.00001144
Iteration 38/1000 | Loss: 0.00001144
Iteration 39/1000 | Loss: 0.00001143
Iteration 40/1000 | Loss: 0.00001143
Iteration 41/1000 | Loss: 0.00001143
Iteration 42/1000 | Loss: 0.00001142
Iteration 43/1000 | Loss: 0.00001142
Iteration 44/1000 | Loss: 0.00001142
Iteration 45/1000 | Loss: 0.00001142
Iteration 46/1000 | Loss: 0.00001141
Iteration 47/1000 | Loss: 0.00001140
Iteration 48/1000 | Loss: 0.00001140
Iteration 49/1000 | Loss: 0.00001140
Iteration 50/1000 | Loss: 0.00001140
Iteration 51/1000 | Loss: 0.00001139
Iteration 52/1000 | Loss: 0.00001139
Iteration 53/1000 | Loss: 0.00001138
Iteration 54/1000 | Loss: 0.00001138
Iteration 55/1000 | Loss: 0.00001138
Iteration 56/1000 | Loss: 0.00001138
Iteration 57/1000 | Loss: 0.00001138
Iteration 58/1000 | Loss: 0.00001138
Iteration 59/1000 | Loss: 0.00001138
Iteration 60/1000 | Loss: 0.00001138
Iteration 61/1000 | Loss: 0.00001137
Iteration 62/1000 | Loss: 0.00001137
Iteration 63/1000 | Loss: 0.00001134
Iteration 64/1000 | Loss: 0.00001134
Iteration 65/1000 | Loss: 0.00001134
Iteration 66/1000 | Loss: 0.00001134
Iteration 67/1000 | Loss: 0.00001134
Iteration 68/1000 | Loss: 0.00001134
Iteration 69/1000 | Loss: 0.00001134
Iteration 70/1000 | Loss: 0.00001133
Iteration 71/1000 | Loss: 0.00001133
Iteration 72/1000 | Loss: 0.00001133
Iteration 73/1000 | Loss: 0.00001133
Iteration 74/1000 | Loss: 0.00001133
Iteration 75/1000 | Loss: 0.00001132
Iteration 76/1000 | Loss: 0.00001132
Iteration 77/1000 | Loss: 0.00001132
Iteration 78/1000 | Loss: 0.00001131
Iteration 79/1000 | Loss: 0.00001131
Iteration 80/1000 | Loss: 0.00001131
Iteration 81/1000 | Loss: 0.00001131
Iteration 82/1000 | Loss: 0.00001131
Iteration 83/1000 | Loss: 0.00001130
Iteration 84/1000 | Loss: 0.00001130
Iteration 85/1000 | Loss: 0.00001130
Iteration 86/1000 | Loss: 0.00001130
Iteration 87/1000 | Loss: 0.00001129
Iteration 88/1000 | Loss: 0.00001129
Iteration 89/1000 | Loss: 0.00001129
Iteration 90/1000 | Loss: 0.00001129
Iteration 91/1000 | Loss: 0.00001128
Iteration 92/1000 | Loss: 0.00001128
Iteration 93/1000 | Loss: 0.00001128
Iteration 94/1000 | Loss: 0.00001128
Iteration 95/1000 | Loss: 0.00001128
Iteration 96/1000 | Loss: 0.00001128
Iteration 97/1000 | Loss: 0.00001128
Iteration 98/1000 | Loss: 0.00001128
Iteration 99/1000 | Loss: 0.00001127
Iteration 100/1000 | Loss: 0.00001127
Iteration 101/1000 | Loss: 0.00001127
Iteration 102/1000 | Loss: 0.00001127
Iteration 103/1000 | Loss: 0.00001127
Iteration 104/1000 | Loss: 0.00001127
Iteration 105/1000 | Loss: 0.00001126
Iteration 106/1000 | Loss: 0.00001126
Iteration 107/1000 | Loss: 0.00001126
Iteration 108/1000 | Loss: 0.00001126
Iteration 109/1000 | Loss: 0.00001126
Iteration 110/1000 | Loss: 0.00001126
Iteration 111/1000 | Loss: 0.00001126
Iteration 112/1000 | Loss: 0.00001126
Iteration 113/1000 | Loss: 0.00001126
Iteration 114/1000 | Loss: 0.00001126
Iteration 115/1000 | Loss: 0.00001126
Iteration 116/1000 | Loss: 0.00001126
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [1.1257186088187154e-05, 1.1257186088187154e-05, 1.1257186088187154e-05, 1.1257186088187154e-05, 1.1257186088187154e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1257186088187154e-05

Optimization complete. Final v2v error: 2.882535219192505 mm

Highest mean error: 3.225451946258545 mm for frame 201

Lowest mean error: 2.657351493835449 mm for frame 40

Saving results

Total time: 47.698389530181885
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00392693
Iteration 2/25 | Loss: 0.00125500
Iteration 3/25 | Loss: 0.00119911
Iteration 4/25 | Loss: 0.00119256
Iteration 5/25 | Loss: 0.00119092
Iteration 6/25 | Loss: 0.00119092
Iteration 7/25 | Loss: 0.00119092
Iteration 8/25 | Loss: 0.00119092
Iteration 9/25 | Loss: 0.00119092
Iteration 10/25 | Loss: 0.00119092
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011909176828339696, 0.0011909176828339696, 0.0011909176828339696, 0.0011909176828339696, 0.0011909176828339696]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011909176828339696

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36956120
Iteration 2/25 | Loss: 0.00109123
Iteration 3/25 | Loss: 0.00109123
Iteration 4/25 | Loss: 0.00109123
Iteration 5/25 | Loss: 0.00109123
Iteration 6/25 | Loss: 0.00109123
Iteration 7/25 | Loss: 0.00109123
Iteration 8/25 | Loss: 0.00109123
Iteration 9/25 | Loss: 0.00109123
Iteration 10/25 | Loss: 0.00109123
Iteration 11/25 | Loss: 0.00109123
Iteration 12/25 | Loss: 0.00109123
Iteration 13/25 | Loss: 0.00109123
Iteration 14/25 | Loss: 0.00109123
Iteration 15/25 | Loss: 0.00109123
Iteration 16/25 | Loss: 0.00109123
Iteration 17/25 | Loss: 0.00109123
Iteration 18/25 | Loss: 0.00109123
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010912285652011633, 0.0010912285652011633, 0.0010912285652011633, 0.0010912285652011633, 0.0010912285652011633]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010912285652011633

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109123
Iteration 2/1000 | Loss: 0.00002275
Iteration 3/1000 | Loss: 0.00001527
Iteration 4/1000 | Loss: 0.00001331
Iteration 5/1000 | Loss: 0.00001244
Iteration 6/1000 | Loss: 0.00001189
Iteration 7/1000 | Loss: 0.00001189
Iteration 8/1000 | Loss: 0.00001147
Iteration 9/1000 | Loss: 0.00001131
Iteration 10/1000 | Loss: 0.00001115
Iteration 11/1000 | Loss: 0.00001082
Iteration 12/1000 | Loss: 0.00001072
Iteration 13/1000 | Loss: 0.00001066
Iteration 14/1000 | Loss: 0.00001059
Iteration 15/1000 | Loss: 0.00001043
Iteration 16/1000 | Loss: 0.00001041
Iteration 17/1000 | Loss: 0.00001037
Iteration 18/1000 | Loss: 0.00001036
Iteration 19/1000 | Loss: 0.00001036
Iteration 20/1000 | Loss: 0.00001035
Iteration 21/1000 | Loss: 0.00001035
Iteration 22/1000 | Loss: 0.00001035
Iteration 23/1000 | Loss: 0.00001034
Iteration 24/1000 | Loss: 0.00001034
Iteration 25/1000 | Loss: 0.00001033
Iteration 26/1000 | Loss: 0.00001033
Iteration 27/1000 | Loss: 0.00001033
Iteration 28/1000 | Loss: 0.00001033
Iteration 29/1000 | Loss: 0.00001032
Iteration 30/1000 | Loss: 0.00001031
Iteration 31/1000 | Loss: 0.00001031
Iteration 32/1000 | Loss: 0.00001029
Iteration 33/1000 | Loss: 0.00001029
Iteration 34/1000 | Loss: 0.00001029
Iteration 35/1000 | Loss: 0.00001029
Iteration 36/1000 | Loss: 0.00001029
Iteration 37/1000 | Loss: 0.00001028
Iteration 38/1000 | Loss: 0.00001028
Iteration 39/1000 | Loss: 0.00001028
Iteration 40/1000 | Loss: 0.00001028
Iteration 41/1000 | Loss: 0.00001027
Iteration 42/1000 | Loss: 0.00001027
Iteration 43/1000 | Loss: 0.00001027
Iteration 44/1000 | Loss: 0.00001025
Iteration 45/1000 | Loss: 0.00001024
Iteration 46/1000 | Loss: 0.00001022
Iteration 47/1000 | Loss: 0.00001022
Iteration 48/1000 | Loss: 0.00001022
Iteration 49/1000 | Loss: 0.00001022
Iteration 50/1000 | Loss: 0.00001022
Iteration 51/1000 | Loss: 0.00001021
Iteration 52/1000 | Loss: 0.00001020
Iteration 53/1000 | Loss: 0.00001019
Iteration 54/1000 | Loss: 0.00001019
Iteration 55/1000 | Loss: 0.00001019
Iteration 56/1000 | Loss: 0.00001018
Iteration 57/1000 | Loss: 0.00001018
Iteration 58/1000 | Loss: 0.00001017
Iteration 59/1000 | Loss: 0.00001017
Iteration 60/1000 | Loss: 0.00001017
Iteration 61/1000 | Loss: 0.00001017
Iteration 62/1000 | Loss: 0.00001016
Iteration 63/1000 | Loss: 0.00001016
Iteration 64/1000 | Loss: 0.00001015
Iteration 65/1000 | Loss: 0.00001015
Iteration 66/1000 | Loss: 0.00001015
Iteration 67/1000 | Loss: 0.00001014
Iteration 68/1000 | Loss: 0.00001014
Iteration 69/1000 | Loss: 0.00001013
Iteration 70/1000 | Loss: 0.00001012
Iteration 71/1000 | Loss: 0.00001012
Iteration 72/1000 | Loss: 0.00001008
Iteration 73/1000 | Loss: 0.00001007
Iteration 74/1000 | Loss: 0.00001007
Iteration 75/1000 | Loss: 0.00001006
Iteration 76/1000 | Loss: 0.00001006
Iteration 77/1000 | Loss: 0.00001005
Iteration 78/1000 | Loss: 0.00001005
Iteration 79/1000 | Loss: 0.00001005
Iteration 80/1000 | Loss: 0.00001004
Iteration 81/1000 | Loss: 0.00001004
Iteration 82/1000 | Loss: 0.00001003
Iteration 83/1000 | Loss: 0.00001003
Iteration 84/1000 | Loss: 0.00001003
Iteration 85/1000 | Loss: 0.00000998
Iteration 86/1000 | Loss: 0.00000998
Iteration 87/1000 | Loss: 0.00000998
Iteration 88/1000 | Loss: 0.00000997
Iteration 89/1000 | Loss: 0.00000997
Iteration 90/1000 | Loss: 0.00000996
Iteration 91/1000 | Loss: 0.00000995
Iteration 92/1000 | Loss: 0.00000995
Iteration 93/1000 | Loss: 0.00000995
Iteration 94/1000 | Loss: 0.00000995
Iteration 95/1000 | Loss: 0.00000995
Iteration 96/1000 | Loss: 0.00000994
Iteration 97/1000 | Loss: 0.00000994
Iteration 98/1000 | Loss: 0.00000994
Iteration 99/1000 | Loss: 0.00000994
Iteration 100/1000 | Loss: 0.00000994
Iteration 101/1000 | Loss: 0.00000994
Iteration 102/1000 | Loss: 0.00000994
Iteration 103/1000 | Loss: 0.00000994
Iteration 104/1000 | Loss: 0.00000993
Iteration 105/1000 | Loss: 0.00000993
Iteration 106/1000 | Loss: 0.00000993
Iteration 107/1000 | Loss: 0.00000993
Iteration 108/1000 | Loss: 0.00000992
Iteration 109/1000 | Loss: 0.00000992
Iteration 110/1000 | Loss: 0.00000992
Iteration 111/1000 | Loss: 0.00000992
Iteration 112/1000 | Loss: 0.00000992
Iteration 113/1000 | Loss: 0.00000992
Iteration 114/1000 | Loss: 0.00000992
Iteration 115/1000 | Loss: 0.00000992
Iteration 116/1000 | Loss: 0.00000992
Iteration 117/1000 | Loss: 0.00000992
Iteration 118/1000 | Loss: 0.00000992
Iteration 119/1000 | Loss: 0.00000992
Iteration 120/1000 | Loss: 0.00000992
Iteration 121/1000 | Loss: 0.00000992
Iteration 122/1000 | Loss: 0.00000991
Iteration 123/1000 | Loss: 0.00000991
Iteration 124/1000 | Loss: 0.00000991
Iteration 125/1000 | Loss: 0.00000991
Iteration 126/1000 | Loss: 0.00000991
Iteration 127/1000 | Loss: 0.00000991
Iteration 128/1000 | Loss: 0.00000991
Iteration 129/1000 | Loss: 0.00000991
Iteration 130/1000 | Loss: 0.00000990
Iteration 131/1000 | Loss: 0.00000990
Iteration 132/1000 | Loss: 0.00000990
Iteration 133/1000 | Loss: 0.00000990
Iteration 134/1000 | Loss: 0.00000990
Iteration 135/1000 | Loss: 0.00000990
Iteration 136/1000 | Loss: 0.00000989
Iteration 137/1000 | Loss: 0.00000989
Iteration 138/1000 | Loss: 0.00000989
Iteration 139/1000 | Loss: 0.00000989
Iteration 140/1000 | Loss: 0.00000989
Iteration 141/1000 | Loss: 0.00000988
Iteration 142/1000 | Loss: 0.00000988
Iteration 143/1000 | Loss: 0.00000988
Iteration 144/1000 | Loss: 0.00000988
Iteration 145/1000 | Loss: 0.00000988
Iteration 146/1000 | Loss: 0.00000988
Iteration 147/1000 | Loss: 0.00000988
Iteration 148/1000 | Loss: 0.00000988
Iteration 149/1000 | Loss: 0.00000988
Iteration 150/1000 | Loss: 0.00000987
Iteration 151/1000 | Loss: 0.00000987
Iteration 152/1000 | Loss: 0.00000987
Iteration 153/1000 | Loss: 0.00000987
Iteration 154/1000 | Loss: 0.00000987
Iteration 155/1000 | Loss: 0.00000987
Iteration 156/1000 | Loss: 0.00000986
Iteration 157/1000 | Loss: 0.00000986
Iteration 158/1000 | Loss: 0.00000986
Iteration 159/1000 | Loss: 0.00000986
Iteration 160/1000 | Loss: 0.00000986
Iteration 161/1000 | Loss: 0.00000986
Iteration 162/1000 | Loss: 0.00000986
Iteration 163/1000 | Loss: 0.00000986
Iteration 164/1000 | Loss: 0.00000986
Iteration 165/1000 | Loss: 0.00000986
Iteration 166/1000 | Loss: 0.00000985
Iteration 167/1000 | Loss: 0.00000985
Iteration 168/1000 | Loss: 0.00000985
Iteration 169/1000 | Loss: 0.00000985
Iteration 170/1000 | Loss: 0.00000985
Iteration 171/1000 | Loss: 0.00000984
Iteration 172/1000 | Loss: 0.00000984
Iteration 173/1000 | Loss: 0.00000984
Iteration 174/1000 | Loss: 0.00000984
Iteration 175/1000 | Loss: 0.00000984
Iteration 176/1000 | Loss: 0.00000984
Iteration 177/1000 | Loss: 0.00000984
Iteration 178/1000 | Loss: 0.00000983
Iteration 179/1000 | Loss: 0.00000983
Iteration 180/1000 | Loss: 0.00000983
Iteration 181/1000 | Loss: 0.00000983
Iteration 182/1000 | Loss: 0.00000982
Iteration 183/1000 | Loss: 0.00000982
Iteration 184/1000 | Loss: 0.00000982
Iteration 185/1000 | Loss: 0.00000982
Iteration 186/1000 | Loss: 0.00000982
Iteration 187/1000 | Loss: 0.00000982
Iteration 188/1000 | Loss: 0.00000982
Iteration 189/1000 | Loss: 0.00000982
Iteration 190/1000 | Loss: 0.00000982
Iteration 191/1000 | Loss: 0.00000981
Iteration 192/1000 | Loss: 0.00000981
Iteration 193/1000 | Loss: 0.00000981
Iteration 194/1000 | Loss: 0.00000981
Iteration 195/1000 | Loss: 0.00000981
Iteration 196/1000 | Loss: 0.00000981
Iteration 197/1000 | Loss: 0.00000981
Iteration 198/1000 | Loss: 0.00000981
Iteration 199/1000 | Loss: 0.00000981
Iteration 200/1000 | Loss: 0.00000980
Iteration 201/1000 | Loss: 0.00000980
Iteration 202/1000 | Loss: 0.00000980
Iteration 203/1000 | Loss: 0.00000980
Iteration 204/1000 | Loss: 0.00000980
Iteration 205/1000 | Loss: 0.00000980
Iteration 206/1000 | Loss: 0.00000980
Iteration 207/1000 | Loss: 0.00000980
Iteration 208/1000 | Loss: 0.00000980
Iteration 209/1000 | Loss: 0.00000980
Iteration 210/1000 | Loss: 0.00000980
Iteration 211/1000 | Loss: 0.00000980
Iteration 212/1000 | Loss: 0.00000980
Iteration 213/1000 | Loss: 0.00000980
Iteration 214/1000 | Loss: 0.00000980
Iteration 215/1000 | Loss: 0.00000980
Iteration 216/1000 | Loss: 0.00000980
Iteration 217/1000 | Loss: 0.00000980
Iteration 218/1000 | Loss: 0.00000980
Iteration 219/1000 | Loss: 0.00000980
Iteration 220/1000 | Loss: 0.00000980
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [9.795094229048118e-06, 9.795094229048118e-06, 9.795094229048118e-06, 9.795094229048118e-06, 9.795094229048118e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.795094229048118e-06

Optimization complete. Final v2v error: 2.6952321529388428 mm

Highest mean error: 2.895524024963379 mm for frame 104

Lowest mean error: 2.5379390716552734 mm for frame 140

Saving results

Total time: 39.73431873321533
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_carla_posed_004/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_carla_posed_004/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00793842
Iteration 2/25 | Loss: 0.00147671
Iteration 3/25 | Loss: 0.00130686
Iteration 4/25 | Loss: 0.00128029
Iteration 5/25 | Loss: 0.00127419
Iteration 6/25 | Loss: 0.00127365
Iteration 7/25 | Loss: 0.00127365
Iteration 8/25 | Loss: 0.00127365
Iteration 9/25 | Loss: 0.00127365
Iteration 10/25 | Loss: 0.00127365
Iteration 11/25 | Loss: 0.00127365
Iteration 12/25 | Loss: 0.00127365
Iteration 13/25 | Loss: 0.00127365
Iteration 14/25 | Loss: 0.00127365
Iteration 15/25 | Loss: 0.00127365
Iteration 16/25 | Loss: 0.00127365
Iteration 17/25 | Loss: 0.00127365
Iteration 18/25 | Loss: 0.00127365
Iteration 19/25 | Loss: 0.00127365
Iteration 20/25 | Loss: 0.00127365
Iteration 21/25 | Loss: 0.00127365
Iteration 22/25 | Loss: 0.00127365
Iteration 23/25 | Loss: 0.00127365
Iteration 24/25 | Loss: 0.00127365
Iteration 25/25 | Loss: 0.00127365

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14917815
Iteration 2/25 | Loss: 0.00094066
Iteration 3/25 | Loss: 0.00094066
Iteration 4/25 | Loss: 0.00094066
Iteration 5/25 | Loss: 0.00094066
Iteration 6/25 | Loss: 0.00094066
Iteration 7/25 | Loss: 0.00094066
Iteration 8/25 | Loss: 0.00094066
Iteration 9/25 | Loss: 0.00094066
Iteration 10/25 | Loss: 0.00094066
Iteration 11/25 | Loss: 0.00094066
Iteration 12/25 | Loss: 0.00094066
Iteration 13/25 | Loss: 0.00094066
Iteration 14/25 | Loss: 0.00094066
Iteration 15/25 | Loss: 0.00094066
Iteration 16/25 | Loss: 0.00094066
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000940659549087286, 0.000940659549087286, 0.000940659549087286, 0.000940659549087286, 0.000940659549087286]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000940659549087286

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094066
Iteration 2/1000 | Loss: 0.00005839
Iteration 3/1000 | Loss: 0.00003847
Iteration 4/1000 | Loss: 0.00003082
Iteration 5/1000 | Loss: 0.00002807
Iteration 6/1000 | Loss: 0.00002650
Iteration 7/1000 | Loss: 0.00002527
Iteration 8/1000 | Loss: 0.00002448
Iteration 9/1000 | Loss: 0.00002380
Iteration 10/1000 | Loss: 0.00002335
Iteration 11/1000 | Loss: 0.00002290
Iteration 12/1000 | Loss: 0.00002255
Iteration 13/1000 | Loss: 0.00002225
Iteration 14/1000 | Loss: 0.00002197
Iteration 15/1000 | Loss: 0.00002179
Iteration 16/1000 | Loss: 0.00002160
Iteration 17/1000 | Loss: 0.00002147
Iteration 18/1000 | Loss: 0.00002142
Iteration 19/1000 | Loss: 0.00002137
Iteration 20/1000 | Loss: 0.00002132
Iteration 21/1000 | Loss: 0.00002131
Iteration 22/1000 | Loss: 0.00002131
Iteration 23/1000 | Loss: 0.00002126
Iteration 24/1000 | Loss: 0.00002126
Iteration 25/1000 | Loss: 0.00002126
Iteration 26/1000 | Loss: 0.00002125
Iteration 27/1000 | Loss: 0.00002125
Iteration 28/1000 | Loss: 0.00002124
Iteration 29/1000 | Loss: 0.00002124
Iteration 30/1000 | Loss: 0.00002124
Iteration 31/1000 | Loss: 0.00002124
Iteration 32/1000 | Loss: 0.00002124
Iteration 33/1000 | Loss: 0.00002123
Iteration 34/1000 | Loss: 0.00002123
Iteration 35/1000 | Loss: 0.00002123
Iteration 36/1000 | Loss: 0.00002122
Iteration 37/1000 | Loss: 0.00002122
Iteration 38/1000 | Loss: 0.00002122
Iteration 39/1000 | Loss: 0.00002122
Iteration 40/1000 | Loss: 0.00002122
Iteration 41/1000 | Loss: 0.00002121
Iteration 42/1000 | Loss: 0.00002121
Iteration 43/1000 | Loss: 0.00002121
Iteration 44/1000 | Loss: 0.00002121
Iteration 45/1000 | Loss: 0.00002121
Iteration 46/1000 | Loss: 0.00002121
Iteration 47/1000 | Loss: 0.00002120
Iteration 48/1000 | Loss: 0.00002120
Iteration 49/1000 | Loss: 0.00002120
Iteration 50/1000 | Loss: 0.00002119
Iteration 51/1000 | Loss: 0.00002119
Iteration 52/1000 | Loss: 0.00002119
Iteration 53/1000 | Loss: 0.00002118
Iteration 54/1000 | Loss: 0.00002118
Iteration 55/1000 | Loss: 0.00002118
Iteration 56/1000 | Loss: 0.00002117
Iteration 57/1000 | Loss: 0.00002117
Iteration 58/1000 | Loss: 0.00002117
Iteration 59/1000 | Loss: 0.00002117
Iteration 60/1000 | Loss: 0.00002117
Iteration 61/1000 | Loss: 0.00002117
Iteration 62/1000 | Loss: 0.00002117
Iteration 63/1000 | Loss: 0.00002117
Iteration 64/1000 | Loss: 0.00002117
Iteration 65/1000 | Loss: 0.00002117
Iteration 66/1000 | Loss: 0.00002117
Iteration 67/1000 | Loss: 0.00002116
Iteration 68/1000 | Loss: 0.00002116
Iteration 69/1000 | Loss: 0.00002115
Iteration 70/1000 | Loss: 0.00002115
Iteration 71/1000 | Loss: 0.00002115
Iteration 72/1000 | Loss: 0.00002115
Iteration 73/1000 | Loss: 0.00002114
Iteration 74/1000 | Loss: 0.00002114
Iteration 75/1000 | Loss: 0.00002114
Iteration 76/1000 | Loss: 0.00002114
Iteration 77/1000 | Loss: 0.00002113
Iteration 78/1000 | Loss: 0.00002113
Iteration 79/1000 | Loss: 0.00002113
Iteration 80/1000 | Loss: 0.00002113
Iteration 81/1000 | Loss: 0.00002113
Iteration 82/1000 | Loss: 0.00002113
Iteration 83/1000 | Loss: 0.00002113
Iteration 84/1000 | Loss: 0.00002113
Iteration 85/1000 | Loss: 0.00002112
Iteration 86/1000 | Loss: 0.00002112
Iteration 87/1000 | Loss: 0.00002112
Iteration 88/1000 | Loss: 0.00002112
Iteration 89/1000 | Loss: 0.00002111
Iteration 90/1000 | Loss: 0.00002111
Iteration 91/1000 | Loss: 0.00002111
Iteration 92/1000 | Loss: 0.00002111
Iteration 93/1000 | Loss: 0.00002111
Iteration 94/1000 | Loss: 0.00002110
Iteration 95/1000 | Loss: 0.00002110
Iteration 96/1000 | Loss: 0.00002110
Iteration 97/1000 | Loss: 0.00002110
Iteration 98/1000 | Loss: 0.00002110
Iteration 99/1000 | Loss: 0.00002110
Iteration 100/1000 | Loss: 0.00002109
Iteration 101/1000 | Loss: 0.00002109
Iteration 102/1000 | Loss: 0.00002109
Iteration 103/1000 | Loss: 0.00002109
Iteration 104/1000 | Loss: 0.00002109
Iteration 105/1000 | Loss: 0.00002108
Iteration 106/1000 | Loss: 0.00002108
Iteration 107/1000 | Loss: 0.00002108
Iteration 108/1000 | Loss: 0.00002108
Iteration 109/1000 | Loss: 0.00002108
Iteration 110/1000 | Loss: 0.00002108
Iteration 111/1000 | Loss: 0.00002108
Iteration 112/1000 | Loss: 0.00002108
Iteration 113/1000 | Loss: 0.00002107
Iteration 114/1000 | Loss: 0.00002107
Iteration 115/1000 | Loss: 0.00002107
Iteration 116/1000 | Loss: 0.00002107
Iteration 117/1000 | Loss: 0.00002107
Iteration 118/1000 | Loss: 0.00002107
Iteration 119/1000 | Loss: 0.00002107
Iteration 120/1000 | Loss: 0.00002107
Iteration 121/1000 | Loss: 0.00002107
Iteration 122/1000 | Loss: 0.00002107
Iteration 123/1000 | Loss: 0.00002107
Iteration 124/1000 | Loss: 0.00002107
Iteration 125/1000 | Loss: 0.00002107
Iteration 126/1000 | Loss: 0.00002106
Iteration 127/1000 | Loss: 0.00002106
Iteration 128/1000 | Loss: 0.00002106
Iteration 129/1000 | Loss: 0.00002106
Iteration 130/1000 | Loss: 0.00002106
Iteration 131/1000 | Loss: 0.00002106
Iteration 132/1000 | Loss: 0.00002106
Iteration 133/1000 | Loss: 0.00002106
Iteration 134/1000 | Loss: 0.00002106
Iteration 135/1000 | Loss: 0.00002106
Iteration 136/1000 | Loss: 0.00002106
Iteration 137/1000 | Loss: 0.00002106
Iteration 138/1000 | Loss: 0.00002106
Iteration 139/1000 | Loss: 0.00002106
Iteration 140/1000 | Loss: 0.00002106
Iteration 141/1000 | Loss: 0.00002106
Iteration 142/1000 | Loss: 0.00002105
Iteration 143/1000 | Loss: 0.00002105
Iteration 144/1000 | Loss: 0.00002105
Iteration 145/1000 | Loss: 0.00002105
Iteration 146/1000 | Loss: 0.00002105
Iteration 147/1000 | Loss: 0.00002105
Iteration 148/1000 | Loss: 0.00002105
Iteration 149/1000 | Loss: 0.00002105
Iteration 150/1000 | Loss: 0.00002105
Iteration 151/1000 | Loss: 0.00002105
Iteration 152/1000 | Loss: 0.00002105
Iteration 153/1000 | Loss: 0.00002105
Iteration 154/1000 | Loss: 0.00002105
Iteration 155/1000 | Loss: 0.00002104
Iteration 156/1000 | Loss: 0.00002104
Iteration 157/1000 | Loss: 0.00002104
Iteration 158/1000 | Loss: 0.00002104
Iteration 159/1000 | Loss: 0.00002104
Iteration 160/1000 | Loss: 0.00002104
Iteration 161/1000 | Loss: 0.00002104
Iteration 162/1000 | Loss: 0.00002104
Iteration 163/1000 | Loss: 0.00002104
Iteration 164/1000 | Loss: 0.00002104
Iteration 165/1000 | Loss: 0.00002104
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [2.1044652385171503e-05, 2.1044652385171503e-05, 2.1044652385171503e-05, 2.1044652385171503e-05, 2.1044652385171503e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1044652385171503e-05

Optimization complete. Final v2v error: 3.850473165512085 mm

Highest mean error: 5.098264217376709 mm for frame 4

Lowest mean error: 2.9244518280029297 mm for frame 169

Saving results

Total time: 48.38637924194336
