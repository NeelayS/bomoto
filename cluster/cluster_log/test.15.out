Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=15, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 840-895
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01053210
Iteration 2/25 | Loss: 0.00336785
Iteration 3/25 | Loss: 0.00182216
Iteration 4/25 | Loss: 0.00118818
Iteration 5/25 | Loss: 0.00107706
Iteration 6/25 | Loss: 0.00099419
Iteration 7/25 | Loss: 0.00088013
Iteration 8/25 | Loss: 0.00084555
Iteration 9/25 | Loss: 0.00080307
Iteration 10/25 | Loss: 0.00078193
Iteration 11/25 | Loss: 0.00075991
Iteration 12/25 | Loss: 0.00075079
Iteration 13/25 | Loss: 0.00074673
Iteration 14/25 | Loss: 0.00074426
Iteration 15/25 | Loss: 0.00074179
Iteration 16/25 | Loss: 0.00074063
Iteration 17/25 | Loss: 0.00073991
Iteration 18/25 | Loss: 0.00073960
Iteration 19/25 | Loss: 0.00073953
Iteration 20/25 | Loss: 0.00073950
Iteration 21/25 | Loss: 0.00073950
Iteration 22/25 | Loss: 0.00073950
Iteration 23/25 | Loss: 0.00073950
Iteration 24/25 | Loss: 0.00073949
Iteration 25/25 | Loss: 0.00073949

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.09087706
Iteration 2/25 | Loss: 0.00043498
Iteration 3/25 | Loss: 0.00043498
Iteration 4/25 | Loss: 0.00043498
Iteration 5/25 | Loss: 0.00043498
Iteration 6/25 | Loss: 0.00043498
Iteration 7/25 | Loss: 0.00043498
Iteration 8/25 | Loss: 0.00043498
Iteration 9/25 | Loss: 0.00043498
Iteration 10/25 | Loss: 0.00043498
Iteration 11/25 | Loss: 0.00043498
Iteration 12/25 | Loss: 0.00043498
Iteration 13/25 | Loss: 0.00043498
Iteration 14/25 | Loss: 0.00043498
Iteration 15/25 | Loss: 0.00043498
Iteration 16/25 | Loss: 0.00043498
Iteration 17/25 | Loss: 0.00043498
Iteration 18/25 | Loss: 0.00043498
Iteration 19/25 | Loss: 0.00043498
Iteration 20/25 | Loss: 0.00043498
Iteration 21/25 | Loss: 0.00043498
Iteration 22/25 | Loss: 0.00043498
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00043497580918483436, 0.00043497580918483436, 0.00043497580918483436, 0.00043497580918483436, 0.00043497580918483436]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00043497580918483436

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043498
Iteration 2/1000 | Loss: 0.00004749
Iteration 3/1000 | Loss: 0.00003632
Iteration 4/1000 | Loss: 0.00003224
Iteration 5/1000 | Loss: 0.00002992
Iteration 6/1000 | Loss: 0.00002838
Iteration 7/1000 | Loss: 0.00002761
Iteration 8/1000 | Loss: 0.00002682
Iteration 9/1000 | Loss: 0.00084172
Iteration 10/1000 | Loss: 0.00021374
Iteration 11/1000 | Loss: 0.00003944
Iteration 12/1000 | Loss: 0.00002883
Iteration 13/1000 | Loss: 0.00002517
Iteration 14/1000 | Loss: 0.00002263
Iteration 15/1000 | Loss: 0.00002094
Iteration 16/1000 | Loss: 0.00001971
Iteration 17/1000 | Loss: 0.00001908
Iteration 18/1000 | Loss: 0.00001862
Iteration 19/1000 | Loss: 0.00001831
Iteration 20/1000 | Loss: 0.00001805
Iteration 21/1000 | Loss: 0.00001784
Iteration 22/1000 | Loss: 0.00001766
Iteration 23/1000 | Loss: 0.00001760
Iteration 24/1000 | Loss: 0.00001760
Iteration 25/1000 | Loss: 0.00001758
Iteration 26/1000 | Loss: 0.00001758
Iteration 27/1000 | Loss: 0.00001757
Iteration 28/1000 | Loss: 0.00001756
Iteration 29/1000 | Loss: 0.00001754
Iteration 30/1000 | Loss: 0.00001753
Iteration 31/1000 | Loss: 0.00001752
Iteration 32/1000 | Loss: 0.00001751
Iteration 33/1000 | Loss: 0.00001750
Iteration 34/1000 | Loss: 0.00001750
Iteration 35/1000 | Loss: 0.00001745
Iteration 36/1000 | Loss: 0.00001745
Iteration 37/1000 | Loss: 0.00001744
Iteration 38/1000 | Loss: 0.00001744
Iteration 39/1000 | Loss: 0.00001743
Iteration 40/1000 | Loss: 0.00001743
Iteration 41/1000 | Loss: 0.00001742
Iteration 42/1000 | Loss: 0.00001741
Iteration 43/1000 | Loss: 0.00001741
Iteration 44/1000 | Loss: 0.00001739
Iteration 45/1000 | Loss: 0.00001738
Iteration 46/1000 | Loss: 0.00001738
Iteration 47/1000 | Loss: 0.00001738
Iteration 48/1000 | Loss: 0.00001737
Iteration 49/1000 | Loss: 0.00001737
Iteration 50/1000 | Loss: 0.00001737
Iteration 51/1000 | Loss: 0.00001736
Iteration 52/1000 | Loss: 0.00001736
Iteration 53/1000 | Loss: 0.00001735
Iteration 54/1000 | Loss: 0.00001734
Iteration 55/1000 | Loss: 0.00001733
Iteration 56/1000 | Loss: 0.00001733
Iteration 57/1000 | Loss: 0.00001732
Iteration 58/1000 | Loss: 0.00001732
Iteration 59/1000 | Loss: 0.00001732
Iteration 60/1000 | Loss: 0.00001731
Iteration 61/1000 | Loss: 0.00001731
Iteration 62/1000 | Loss: 0.00001731
Iteration 63/1000 | Loss: 0.00001731
Iteration 64/1000 | Loss: 0.00001731
Iteration 65/1000 | Loss: 0.00001730
Iteration 66/1000 | Loss: 0.00001730
Iteration 67/1000 | Loss: 0.00001730
Iteration 68/1000 | Loss: 0.00001729
Iteration 69/1000 | Loss: 0.00001729
Iteration 70/1000 | Loss: 0.00001729
Iteration 71/1000 | Loss: 0.00001729
Iteration 72/1000 | Loss: 0.00001729
Iteration 73/1000 | Loss: 0.00001728
Iteration 74/1000 | Loss: 0.00001728
Iteration 75/1000 | Loss: 0.00001728
Iteration 76/1000 | Loss: 0.00001728
Iteration 77/1000 | Loss: 0.00001728
Iteration 78/1000 | Loss: 0.00001728
Iteration 79/1000 | Loss: 0.00001728
Iteration 80/1000 | Loss: 0.00001728
Iteration 81/1000 | Loss: 0.00001728
Iteration 82/1000 | Loss: 0.00001728
Iteration 83/1000 | Loss: 0.00001728
Iteration 84/1000 | Loss: 0.00001727
Iteration 85/1000 | Loss: 0.00001727
Iteration 86/1000 | Loss: 0.00001727
Iteration 87/1000 | Loss: 0.00001727
Iteration 88/1000 | Loss: 0.00001727
Iteration 89/1000 | Loss: 0.00001727
Iteration 90/1000 | Loss: 0.00001727
Iteration 91/1000 | Loss: 0.00001727
Iteration 92/1000 | Loss: 0.00001727
Iteration 93/1000 | Loss: 0.00001727
Iteration 94/1000 | Loss: 0.00001726
Iteration 95/1000 | Loss: 0.00001726
Iteration 96/1000 | Loss: 0.00001726
Iteration 97/1000 | Loss: 0.00001726
Iteration 98/1000 | Loss: 0.00001726
Iteration 99/1000 | Loss: 0.00001726
Iteration 100/1000 | Loss: 0.00001726
Iteration 101/1000 | Loss: 0.00001726
Iteration 102/1000 | Loss: 0.00001726
Iteration 103/1000 | Loss: 0.00001726
Iteration 104/1000 | Loss: 0.00001726
Iteration 105/1000 | Loss: 0.00001726
Iteration 106/1000 | Loss: 0.00001726
Iteration 107/1000 | Loss: 0.00001726
Iteration 108/1000 | Loss: 0.00001725
Iteration 109/1000 | Loss: 0.00001725
Iteration 110/1000 | Loss: 0.00001725
Iteration 111/1000 | Loss: 0.00001725
Iteration 112/1000 | Loss: 0.00001725
Iteration 113/1000 | Loss: 0.00001725
Iteration 114/1000 | Loss: 0.00001725
Iteration 115/1000 | Loss: 0.00001725
Iteration 116/1000 | Loss: 0.00001725
Iteration 117/1000 | Loss: 0.00001725
Iteration 118/1000 | Loss: 0.00001725
Iteration 119/1000 | Loss: 0.00001725
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.7251697499887086e-05, 1.7251697499887086e-05, 1.7251697499887086e-05, 1.7251697499887086e-05, 1.7251697499887086e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7251697499887086e-05

Optimization complete. Final v2v error: 3.3957948684692383 mm

Highest mean error: 9.054298400878906 mm for frame 208

Lowest mean error: 2.9820361137390137 mm for frame 130

Saving results

Total time: 86.14796829223633
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00813962
Iteration 2/25 | Loss: 0.00085035
Iteration 3/25 | Loss: 0.00064945
Iteration 4/25 | Loss: 0.00062139
Iteration 5/25 | Loss: 0.00061142
Iteration 6/25 | Loss: 0.00060941
Iteration 7/25 | Loss: 0.00060888
Iteration 8/25 | Loss: 0.00060879
Iteration 9/25 | Loss: 0.00060879
Iteration 10/25 | Loss: 0.00060879
Iteration 11/25 | Loss: 0.00060879
Iteration 12/25 | Loss: 0.00060879
Iteration 13/25 | Loss: 0.00060879
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006087881047278643, 0.0006087881047278643, 0.0006087881047278643, 0.0006087881047278643, 0.0006087881047278643]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006087881047278643

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56897819
Iteration 2/25 | Loss: 0.00031477
Iteration 3/25 | Loss: 0.00031477
Iteration 4/25 | Loss: 0.00031477
Iteration 5/25 | Loss: 0.00031477
Iteration 6/25 | Loss: 0.00031477
Iteration 7/25 | Loss: 0.00031477
Iteration 8/25 | Loss: 0.00031477
Iteration 9/25 | Loss: 0.00031477
Iteration 10/25 | Loss: 0.00031477
Iteration 11/25 | Loss: 0.00031477
Iteration 12/25 | Loss: 0.00031477
Iteration 13/25 | Loss: 0.00031477
Iteration 14/25 | Loss: 0.00031477
Iteration 15/25 | Loss: 0.00031477
Iteration 16/25 | Loss: 0.00031477
Iteration 17/25 | Loss: 0.00031477
Iteration 18/25 | Loss: 0.00031477
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00031476723961532116, 0.00031476723961532116, 0.00031476723961532116, 0.00031476723961532116, 0.00031476723961532116]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00031476723961532116

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031477
Iteration 2/1000 | Loss: 0.00002628
Iteration 3/1000 | Loss: 0.00001911
Iteration 4/1000 | Loss: 0.00001538
Iteration 5/1000 | Loss: 0.00001459
Iteration 6/1000 | Loss: 0.00001410
Iteration 7/1000 | Loss: 0.00001378
Iteration 8/1000 | Loss: 0.00001359
Iteration 9/1000 | Loss: 0.00001352
Iteration 10/1000 | Loss: 0.00001345
Iteration 11/1000 | Loss: 0.00001339
Iteration 12/1000 | Loss: 0.00001330
Iteration 13/1000 | Loss: 0.00001330
Iteration 14/1000 | Loss: 0.00001323
Iteration 15/1000 | Loss: 0.00001320
Iteration 16/1000 | Loss: 0.00001320
Iteration 17/1000 | Loss: 0.00001319
Iteration 18/1000 | Loss: 0.00001315
Iteration 19/1000 | Loss: 0.00001313
Iteration 20/1000 | Loss: 0.00001313
Iteration 21/1000 | Loss: 0.00001312
Iteration 22/1000 | Loss: 0.00001312
Iteration 23/1000 | Loss: 0.00001311
Iteration 24/1000 | Loss: 0.00001311
Iteration 25/1000 | Loss: 0.00001311
Iteration 26/1000 | Loss: 0.00001310
Iteration 27/1000 | Loss: 0.00001310
Iteration 28/1000 | Loss: 0.00001309
Iteration 29/1000 | Loss: 0.00001309
Iteration 30/1000 | Loss: 0.00001309
Iteration 31/1000 | Loss: 0.00001308
Iteration 32/1000 | Loss: 0.00001308
Iteration 33/1000 | Loss: 0.00001308
Iteration 34/1000 | Loss: 0.00001307
Iteration 35/1000 | Loss: 0.00001306
Iteration 36/1000 | Loss: 0.00001306
Iteration 37/1000 | Loss: 0.00001305
Iteration 38/1000 | Loss: 0.00001305
Iteration 39/1000 | Loss: 0.00001304
Iteration 40/1000 | Loss: 0.00001304
Iteration 41/1000 | Loss: 0.00001303
Iteration 42/1000 | Loss: 0.00001303
Iteration 43/1000 | Loss: 0.00001303
Iteration 44/1000 | Loss: 0.00001302
Iteration 45/1000 | Loss: 0.00001302
Iteration 46/1000 | Loss: 0.00001302
Iteration 47/1000 | Loss: 0.00001302
Iteration 48/1000 | Loss: 0.00001302
Iteration 49/1000 | Loss: 0.00001302
Iteration 50/1000 | Loss: 0.00001302
Iteration 51/1000 | Loss: 0.00001302
Iteration 52/1000 | Loss: 0.00001302
Iteration 53/1000 | Loss: 0.00001302
Iteration 54/1000 | Loss: 0.00001301
Iteration 55/1000 | Loss: 0.00001301
Iteration 56/1000 | Loss: 0.00001301
Iteration 57/1000 | Loss: 0.00001301
Iteration 58/1000 | Loss: 0.00001301
Iteration 59/1000 | Loss: 0.00001301
Iteration 60/1000 | Loss: 0.00001301
Iteration 61/1000 | Loss: 0.00001301
Iteration 62/1000 | Loss: 0.00001301
Iteration 63/1000 | Loss: 0.00001301
Iteration 64/1000 | Loss: 0.00001301
Iteration 65/1000 | Loss: 0.00001301
Iteration 66/1000 | Loss: 0.00001301
Iteration 67/1000 | Loss: 0.00001301
Iteration 68/1000 | Loss: 0.00001301
Iteration 69/1000 | Loss: 0.00001300
Iteration 70/1000 | Loss: 0.00001300
Iteration 71/1000 | Loss: 0.00001300
Iteration 72/1000 | Loss: 0.00001299
Iteration 73/1000 | Loss: 0.00001299
Iteration 74/1000 | Loss: 0.00001299
Iteration 75/1000 | Loss: 0.00001298
Iteration 76/1000 | Loss: 0.00001298
Iteration 77/1000 | Loss: 0.00001298
Iteration 78/1000 | Loss: 0.00001298
Iteration 79/1000 | Loss: 0.00001298
Iteration 80/1000 | Loss: 0.00001298
Iteration 81/1000 | Loss: 0.00001298
Iteration 82/1000 | Loss: 0.00001298
Iteration 83/1000 | Loss: 0.00001298
Iteration 84/1000 | Loss: 0.00001298
Iteration 85/1000 | Loss: 0.00001297
Iteration 86/1000 | Loss: 0.00001297
Iteration 87/1000 | Loss: 0.00001297
Iteration 88/1000 | Loss: 0.00001297
Iteration 89/1000 | Loss: 0.00001297
Iteration 90/1000 | Loss: 0.00001297
Iteration 91/1000 | Loss: 0.00001297
Iteration 92/1000 | Loss: 0.00001296
Iteration 93/1000 | Loss: 0.00001296
Iteration 94/1000 | Loss: 0.00001295
Iteration 95/1000 | Loss: 0.00001295
Iteration 96/1000 | Loss: 0.00001295
Iteration 97/1000 | Loss: 0.00001295
Iteration 98/1000 | Loss: 0.00001294
Iteration 99/1000 | Loss: 0.00001294
Iteration 100/1000 | Loss: 0.00001294
Iteration 101/1000 | Loss: 0.00001293
Iteration 102/1000 | Loss: 0.00001293
Iteration 103/1000 | Loss: 0.00001292
Iteration 104/1000 | Loss: 0.00001292
Iteration 105/1000 | Loss: 0.00001292
Iteration 106/1000 | Loss: 0.00001292
Iteration 107/1000 | Loss: 0.00001292
Iteration 108/1000 | Loss: 0.00001291
Iteration 109/1000 | Loss: 0.00001291
Iteration 110/1000 | Loss: 0.00001291
Iteration 111/1000 | Loss: 0.00001290
Iteration 112/1000 | Loss: 0.00001290
Iteration 113/1000 | Loss: 0.00001290
Iteration 114/1000 | Loss: 0.00001289
Iteration 115/1000 | Loss: 0.00001289
Iteration 116/1000 | Loss: 0.00001289
Iteration 117/1000 | Loss: 0.00001289
Iteration 118/1000 | Loss: 0.00001288
Iteration 119/1000 | Loss: 0.00001288
Iteration 120/1000 | Loss: 0.00001288
Iteration 121/1000 | Loss: 0.00001288
Iteration 122/1000 | Loss: 0.00001287
Iteration 123/1000 | Loss: 0.00001287
Iteration 124/1000 | Loss: 0.00001287
Iteration 125/1000 | Loss: 0.00001287
Iteration 126/1000 | Loss: 0.00001286
Iteration 127/1000 | Loss: 0.00001286
Iteration 128/1000 | Loss: 0.00001286
Iteration 129/1000 | Loss: 0.00001286
Iteration 130/1000 | Loss: 0.00001286
Iteration 131/1000 | Loss: 0.00001286
Iteration 132/1000 | Loss: 0.00001286
Iteration 133/1000 | Loss: 0.00001286
Iteration 134/1000 | Loss: 0.00001286
Iteration 135/1000 | Loss: 0.00001286
Iteration 136/1000 | Loss: 0.00001286
Iteration 137/1000 | Loss: 0.00001285
Iteration 138/1000 | Loss: 0.00001285
Iteration 139/1000 | Loss: 0.00001285
Iteration 140/1000 | Loss: 0.00001285
Iteration 141/1000 | Loss: 0.00001285
Iteration 142/1000 | Loss: 0.00001285
Iteration 143/1000 | Loss: 0.00001285
Iteration 144/1000 | Loss: 0.00001285
Iteration 145/1000 | Loss: 0.00001285
Iteration 146/1000 | Loss: 0.00001285
Iteration 147/1000 | Loss: 0.00001285
Iteration 148/1000 | Loss: 0.00001285
Iteration 149/1000 | Loss: 0.00001285
Iteration 150/1000 | Loss: 0.00001285
Iteration 151/1000 | Loss: 0.00001284
Iteration 152/1000 | Loss: 0.00001284
Iteration 153/1000 | Loss: 0.00001284
Iteration 154/1000 | Loss: 0.00001284
Iteration 155/1000 | Loss: 0.00001284
Iteration 156/1000 | Loss: 0.00001284
Iteration 157/1000 | Loss: 0.00001284
Iteration 158/1000 | Loss: 0.00001284
Iteration 159/1000 | Loss: 0.00001284
Iteration 160/1000 | Loss: 0.00001284
Iteration 161/1000 | Loss: 0.00001284
Iteration 162/1000 | Loss: 0.00001284
Iteration 163/1000 | Loss: 0.00001284
Iteration 164/1000 | Loss: 0.00001284
Iteration 165/1000 | Loss: 0.00001284
Iteration 166/1000 | Loss: 0.00001284
Iteration 167/1000 | Loss: 0.00001284
Iteration 168/1000 | Loss: 0.00001284
Iteration 169/1000 | Loss: 0.00001284
Iteration 170/1000 | Loss: 0.00001284
Iteration 171/1000 | Loss: 0.00001284
Iteration 172/1000 | Loss: 0.00001284
Iteration 173/1000 | Loss: 0.00001284
Iteration 174/1000 | Loss: 0.00001284
Iteration 175/1000 | Loss: 0.00001284
Iteration 176/1000 | Loss: 0.00001284
Iteration 177/1000 | Loss: 0.00001284
Iteration 178/1000 | Loss: 0.00001284
Iteration 179/1000 | Loss: 0.00001284
Iteration 180/1000 | Loss: 0.00001284
Iteration 181/1000 | Loss: 0.00001284
Iteration 182/1000 | Loss: 0.00001284
Iteration 183/1000 | Loss: 0.00001284
Iteration 184/1000 | Loss: 0.00001284
Iteration 185/1000 | Loss: 0.00001284
Iteration 186/1000 | Loss: 0.00001284
Iteration 187/1000 | Loss: 0.00001284
Iteration 188/1000 | Loss: 0.00001284
Iteration 189/1000 | Loss: 0.00001284
Iteration 190/1000 | Loss: 0.00001284
Iteration 191/1000 | Loss: 0.00001284
Iteration 192/1000 | Loss: 0.00001284
Iteration 193/1000 | Loss: 0.00001284
Iteration 194/1000 | Loss: 0.00001284
Iteration 195/1000 | Loss: 0.00001284
Iteration 196/1000 | Loss: 0.00001284
Iteration 197/1000 | Loss: 0.00001284
Iteration 198/1000 | Loss: 0.00001284
Iteration 199/1000 | Loss: 0.00001284
Iteration 200/1000 | Loss: 0.00001284
Iteration 201/1000 | Loss: 0.00001284
Iteration 202/1000 | Loss: 0.00001284
Iteration 203/1000 | Loss: 0.00001284
Iteration 204/1000 | Loss: 0.00001284
Iteration 205/1000 | Loss: 0.00001284
Iteration 206/1000 | Loss: 0.00001284
Iteration 207/1000 | Loss: 0.00001284
Iteration 208/1000 | Loss: 0.00001284
Iteration 209/1000 | Loss: 0.00001284
Iteration 210/1000 | Loss: 0.00001284
Iteration 211/1000 | Loss: 0.00001284
Iteration 212/1000 | Loss: 0.00001284
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [1.283631627302384e-05, 1.283631627302384e-05, 1.283631627302384e-05, 1.283631627302384e-05, 1.283631627302384e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.283631627302384e-05

Optimization complete. Final v2v error: 3.046450138092041 mm

Highest mean error: 4.017776966094971 mm for frame 86

Lowest mean error: 2.8025763034820557 mm for frame 100

Saving results

Total time: 37.907794713974
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00501389
Iteration 2/25 | Loss: 0.00082361
Iteration 3/25 | Loss: 0.00066109
Iteration 4/25 | Loss: 0.00064061
Iteration 5/25 | Loss: 0.00063249
Iteration 6/25 | Loss: 0.00063055
Iteration 7/25 | Loss: 0.00062991
Iteration 8/25 | Loss: 0.00062989
Iteration 9/25 | Loss: 0.00062989
Iteration 10/25 | Loss: 0.00062989
Iteration 11/25 | Loss: 0.00062989
Iteration 12/25 | Loss: 0.00062989
Iteration 13/25 | Loss: 0.00062989
Iteration 14/25 | Loss: 0.00062988
Iteration 15/25 | Loss: 0.00062988
Iteration 16/25 | Loss: 0.00062988
Iteration 17/25 | Loss: 0.00062988
Iteration 18/25 | Loss: 0.00062988
Iteration 19/25 | Loss: 0.00062988
Iteration 20/25 | Loss: 0.00062988
Iteration 21/25 | Loss: 0.00062988
Iteration 22/25 | Loss: 0.00062988
Iteration 23/25 | Loss: 0.00062988
Iteration 24/25 | Loss: 0.00062988
Iteration 25/25 | Loss: 0.00062988

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.57798910
Iteration 2/25 | Loss: 0.00030456
Iteration 3/25 | Loss: 0.00030453
Iteration 4/25 | Loss: 0.00030453
Iteration 5/25 | Loss: 0.00030453
Iteration 6/25 | Loss: 0.00030453
Iteration 7/25 | Loss: 0.00030453
Iteration 8/25 | Loss: 0.00030453
Iteration 9/25 | Loss: 0.00030453
Iteration 10/25 | Loss: 0.00030453
Iteration 11/25 | Loss: 0.00030453
Iteration 12/25 | Loss: 0.00030453
Iteration 13/25 | Loss: 0.00030453
Iteration 14/25 | Loss: 0.00030453
Iteration 15/25 | Loss: 0.00030453
Iteration 16/25 | Loss: 0.00030453
Iteration 17/25 | Loss: 0.00030453
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00030452522332780063, 0.00030452522332780063, 0.00030452522332780063, 0.00030452522332780063, 0.00030452522332780063]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00030452522332780063

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030453
Iteration 2/1000 | Loss: 0.00002953
Iteration 3/1000 | Loss: 0.00001852
Iteration 4/1000 | Loss: 0.00001691
Iteration 5/1000 | Loss: 0.00001611
Iteration 6/1000 | Loss: 0.00001563
Iteration 7/1000 | Loss: 0.00001530
Iteration 8/1000 | Loss: 0.00001496
Iteration 9/1000 | Loss: 0.00001469
Iteration 10/1000 | Loss: 0.00001456
Iteration 11/1000 | Loss: 0.00001444
Iteration 12/1000 | Loss: 0.00001434
Iteration 13/1000 | Loss: 0.00001426
Iteration 14/1000 | Loss: 0.00001425
Iteration 15/1000 | Loss: 0.00001425
Iteration 16/1000 | Loss: 0.00001422
Iteration 17/1000 | Loss: 0.00001420
Iteration 18/1000 | Loss: 0.00001419
Iteration 19/1000 | Loss: 0.00001417
Iteration 20/1000 | Loss: 0.00001417
Iteration 21/1000 | Loss: 0.00001415
Iteration 22/1000 | Loss: 0.00001414
Iteration 23/1000 | Loss: 0.00001414
Iteration 24/1000 | Loss: 0.00001413
Iteration 25/1000 | Loss: 0.00001408
Iteration 26/1000 | Loss: 0.00001401
Iteration 27/1000 | Loss: 0.00001401
Iteration 28/1000 | Loss: 0.00001401
Iteration 29/1000 | Loss: 0.00001400
Iteration 30/1000 | Loss: 0.00001399
Iteration 31/1000 | Loss: 0.00001399
Iteration 32/1000 | Loss: 0.00001399
Iteration 33/1000 | Loss: 0.00001398
Iteration 34/1000 | Loss: 0.00001397
Iteration 35/1000 | Loss: 0.00001396
Iteration 36/1000 | Loss: 0.00001396
Iteration 37/1000 | Loss: 0.00001395
Iteration 38/1000 | Loss: 0.00001395
Iteration 39/1000 | Loss: 0.00001395
Iteration 40/1000 | Loss: 0.00001395
Iteration 41/1000 | Loss: 0.00001394
Iteration 42/1000 | Loss: 0.00001394
Iteration 43/1000 | Loss: 0.00001393
Iteration 44/1000 | Loss: 0.00001393
Iteration 45/1000 | Loss: 0.00001392
Iteration 46/1000 | Loss: 0.00001392
Iteration 47/1000 | Loss: 0.00001391
Iteration 48/1000 | Loss: 0.00001391
Iteration 49/1000 | Loss: 0.00001390
Iteration 50/1000 | Loss: 0.00001390
Iteration 51/1000 | Loss: 0.00001389
Iteration 52/1000 | Loss: 0.00001389
Iteration 53/1000 | Loss: 0.00001389
Iteration 54/1000 | Loss: 0.00001388
Iteration 55/1000 | Loss: 0.00001388
Iteration 56/1000 | Loss: 0.00001387
Iteration 57/1000 | Loss: 0.00001385
Iteration 58/1000 | Loss: 0.00001385
Iteration 59/1000 | Loss: 0.00001384
Iteration 60/1000 | Loss: 0.00001384
Iteration 61/1000 | Loss: 0.00001384
Iteration 62/1000 | Loss: 0.00001384
Iteration 63/1000 | Loss: 0.00001383
Iteration 64/1000 | Loss: 0.00001383
Iteration 65/1000 | Loss: 0.00001382
Iteration 66/1000 | Loss: 0.00001382
Iteration 67/1000 | Loss: 0.00001382
Iteration 68/1000 | Loss: 0.00001382
Iteration 69/1000 | Loss: 0.00001381
Iteration 70/1000 | Loss: 0.00001381
Iteration 71/1000 | Loss: 0.00001381
Iteration 72/1000 | Loss: 0.00001381
Iteration 73/1000 | Loss: 0.00001381
Iteration 74/1000 | Loss: 0.00001381
Iteration 75/1000 | Loss: 0.00001381
Iteration 76/1000 | Loss: 0.00001380
Iteration 77/1000 | Loss: 0.00001380
Iteration 78/1000 | Loss: 0.00001380
Iteration 79/1000 | Loss: 0.00001380
Iteration 80/1000 | Loss: 0.00001380
Iteration 81/1000 | Loss: 0.00001380
Iteration 82/1000 | Loss: 0.00001380
Iteration 83/1000 | Loss: 0.00001380
Iteration 84/1000 | Loss: 0.00001380
Iteration 85/1000 | Loss: 0.00001379
Iteration 86/1000 | Loss: 0.00001379
Iteration 87/1000 | Loss: 0.00001379
Iteration 88/1000 | Loss: 0.00001379
Iteration 89/1000 | Loss: 0.00001378
Iteration 90/1000 | Loss: 0.00001378
Iteration 91/1000 | Loss: 0.00001378
Iteration 92/1000 | Loss: 0.00001378
Iteration 93/1000 | Loss: 0.00001378
Iteration 94/1000 | Loss: 0.00001378
Iteration 95/1000 | Loss: 0.00001378
Iteration 96/1000 | Loss: 0.00001377
Iteration 97/1000 | Loss: 0.00001377
Iteration 98/1000 | Loss: 0.00001377
Iteration 99/1000 | Loss: 0.00001377
Iteration 100/1000 | Loss: 0.00001377
Iteration 101/1000 | Loss: 0.00001377
Iteration 102/1000 | Loss: 0.00001377
Iteration 103/1000 | Loss: 0.00001377
Iteration 104/1000 | Loss: 0.00001377
Iteration 105/1000 | Loss: 0.00001377
Iteration 106/1000 | Loss: 0.00001376
Iteration 107/1000 | Loss: 0.00001376
Iteration 108/1000 | Loss: 0.00001376
Iteration 109/1000 | Loss: 0.00001376
Iteration 110/1000 | Loss: 0.00001376
Iteration 111/1000 | Loss: 0.00001376
Iteration 112/1000 | Loss: 0.00001376
Iteration 113/1000 | Loss: 0.00001376
Iteration 114/1000 | Loss: 0.00001376
Iteration 115/1000 | Loss: 0.00001376
Iteration 116/1000 | Loss: 0.00001376
Iteration 117/1000 | Loss: 0.00001376
Iteration 118/1000 | Loss: 0.00001376
Iteration 119/1000 | Loss: 0.00001375
Iteration 120/1000 | Loss: 0.00001375
Iteration 121/1000 | Loss: 0.00001375
Iteration 122/1000 | Loss: 0.00001375
Iteration 123/1000 | Loss: 0.00001375
Iteration 124/1000 | Loss: 0.00001375
Iteration 125/1000 | Loss: 0.00001375
Iteration 126/1000 | Loss: 0.00001375
Iteration 127/1000 | Loss: 0.00001375
Iteration 128/1000 | Loss: 0.00001375
Iteration 129/1000 | Loss: 0.00001375
Iteration 130/1000 | Loss: 0.00001374
Iteration 131/1000 | Loss: 0.00001374
Iteration 132/1000 | Loss: 0.00001374
Iteration 133/1000 | Loss: 0.00001374
Iteration 134/1000 | Loss: 0.00001374
Iteration 135/1000 | Loss: 0.00001374
Iteration 136/1000 | Loss: 0.00001374
Iteration 137/1000 | Loss: 0.00001374
Iteration 138/1000 | Loss: 0.00001374
Iteration 139/1000 | Loss: 0.00001374
Iteration 140/1000 | Loss: 0.00001374
Iteration 141/1000 | Loss: 0.00001374
Iteration 142/1000 | Loss: 0.00001374
Iteration 143/1000 | Loss: 0.00001373
Iteration 144/1000 | Loss: 0.00001373
Iteration 145/1000 | Loss: 0.00001373
Iteration 146/1000 | Loss: 0.00001373
Iteration 147/1000 | Loss: 0.00001373
Iteration 148/1000 | Loss: 0.00001373
Iteration 149/1000 | Loss: 0.00001373
Iteration 150/1000 | Loss: 0.00001373
Iteration 151/1000 | Loss: 0.00001373
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.3734169442614075e-05, 1.3734169442614075e-05, 1.3734169442614075e-05, 1.3734169442614075e-05, 1.3734169442614075e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3734169442614075e-05

Optimization complete. Final v2v error: 3.1227846145629883 mm

Highest mean error: 3.507711410522461 mm for frame 111

Lowest mean error: 2.7481939792633057 mm for frame 143

Saving results

Total time: 40.05355143547058
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00424229
Iteration 2/25 | Loss: 0.00084519
Iteration 3/25 | Loss: 0.00062350
Iteration 4/25 | Loss: 0.00060418
Iteration 5/25 | Loss: 0.00059717
Iteration 6/25 | Loss: 0.00059469
Iteration 7/25 | Loss: 0.00059440
Iteration 8/25 | Loss: 0.00059440
Iteration 9/25 | Loss: 0.00059440
Iteration 10/25 | Loss: 0.00059440
Iteration 11/25 | Loss: 0.00059440
Iteration 12/25 | Loss: 0.00059440
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005943994619883597, 0.0005943994619883597, 0.0005943994619883597, 0.0005943994619883597, 0.0005943994619883597]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005943994619883597

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48855233
Iteration 2/25 | Loss: 0.00029200
Iteration 3/25 | Loss: 0.00029200
Iteration 4/25 | Loss: 0.00029200
Iteration 5/25 | Loss: 0.00029200
Iteration 6/25 | Loss: 0.00029200
Iteration 7/25 | Loss: 0.00029200
Iteration 8/25 | Loss: 0.00029200
Iteration 9/25 | Loss: 0.00029200
Iteration 10/25 | Loss: 0.00029200
Iteration 11/25 | Loss: 0.00029200
Iteration 12/25 | Loss: 0.00029200
Iteration 13/25 | Loss: 0.00029200
Iteration 14/25 | Loss: 0.00029200
Iteration 15/25 | Loss: 0.00029200
Iteration 16/25 | Loss: 0.00029200
Iteration 17/25 | Loss: 0.00029200
Iteration 18/25 | Loss: 0.00029200
Iteration 19/25 | Loss: 0.00029200
Iteration 20/25 | Loss: 0.00029200
Iteration 21/25 | Loss: 0.00029200
Iteration 22/25 | Loss: 0.00029200
Iteration 23/25 | Loss: 0.00029200
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0002919971593655646, 0.0002919971593655646, 0.0002919971593655646, 0.0002919971593655646, 0.0002919971593655646]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002919971593655646

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029200
Iteration 2/1000 | Loss: 0.00001910
Iteration 3/1000 | Loss: 0.00001332
Iteration 4/1000 | Loss: 0.00001237
Iteration 5/1000 | Loss: 0.00001193
Iteration 6/1000 | Loss: 0.00001168
Iteration 7/1000 | Loss: 0.00001166
Iteration 8/1000 | Loss: 0.00001150
Iteration 9/1000 | Loss: 0.00001142
Iteration 10/1000 | Loss: 0.00001139
Iteration 11/1000 | Loss: 0.00001137
Iteration 12/1000 | Loss: 0.00001133
Iteration 13/1000 | Loss: 0.00001132
Iteration 14/1000 | Loss: 0.00001129
Iteration 15/1000 | Loss: 0.00001128
Iteration 16/1000 | Loss: 0.00001128
Iteration 17/1000 | Loss: 0.00001128
Iteration 18/1000 | Loss: 0.00001128
Iteration 19/1000 | Loss: 0.00001127
Iteration 20/1000 | Loss: 0.00001127
Iteration 21/1000 | Loss: 0.00001126
Iteration 22/1000 | Loss: 0.00001126
Iteration 23/1000 | Loss: 0.00001126
Iteration 24/1000 | Loss: 0.00001126
Iteration 25/1000 | Loss: 0.00001125
Iteration 26/1000 | Loss: 0.00001125
Iteration 27/1000 | Loss: 0.00001125
Iteration 28/1000 | Loss: 0.00001125
Iteration 29/1000 | Loss: 0.00001125
Iteration 30/1000 | Loss: 0.00001125
Iteration 31/1000 | Loss: 0.00001125
Iteration 32/1000 | Loss: 0.00001125
Iteration 33/1000 | Loss: 0.00001125
Iteration 34/1000 | Loss: 0.00001125
Iteration 35/1000 | Loss: 0.00001124
Iteration 36/1000 | Loss: 0.00001123
Iteration 37/1000 | Loss: 0.00001122
Iteration 38/1000 | Loss: 0.00001122
Iteration 39/1000 | Loss: 0.00001122
Iteration 40/1000 | Loss: 0.00001121
Iteration 41/1000 | Loss: 0.00001120
Iteration 42/1000 | Loss: 0.00001119
Iteration 43/1000 | Loss: 0.00001119
Iteration 44/1000 | Loss: 0.00001119
Iteration 45/1000 | Loss: 0.00001118
Iteration 46/1000 | Loss: 0.00001118
Iteration 47/1000 | Loss: 0.00001118
Iteration 48/1000 | Loss: 0.00001118
Iteration 49/1000 | Loss: 0.00001117
Iteration 50/1000 | Loss: 0.00001117
Iteration 51/1000 | Loss: 0.00001116
Iteration 52/1000 | Loss: 0.00001116
Iteration 53/1000 | Loss: 0.00001116
Iteration 54/1000 | Loss: 0.00001116
Iteration 55/1000 | Loss: 0.00001116
Iteration 56/1000 | Loss: 0.00001115
Iteration 57/1000 | Loss: 0.00001115
Iteration 58/1000 | Loss: 0.00001114
Iteration 59/1000 | Loss: 0.00001113
Iteration 60/1000 | Loss: 0.00001113
Iteration 61/1000 | Loss: 0.00001113
Iteration 62/1000 | Loss: 0.00001113
Iteration 63/1000 | Loss: 0.00001112
Iteration 64/1000 | Loss: 0.00001112
Iteration 65/1000 | Loss: 0.00001112
Iteration 66/1000 | Loss: 0.00001112
Iteration 67/1000 | Loss: 0.00001112
Iteration 68/1000 | Loss: 0.00001112
Iteration 69/1000 | Loss: 0.00001112
Iteration 70/1000 | Loss: 0.00001112
Iteration 71/1000 | Loss: 0.00001112
Iteration 72/1000 | Loss: 0.00001112
Iteration 73/1000 | Loss: 0.00001112
Iteration 74/1000 | Loss: 0.00001111
Iteration 75/1000 | Loss: 0.00001111
Iteration 76/1000 | Loss: 0.00001111
Iteration 77/1000 | Loss: 0.00001110
Iteration 78/1000 | Loss: 0.00001110
Iteration 79/1000 | Loss: 0.00001110
Iteration 80/1000 | Loss: 0.00001110
Iteration 81/1000 | Loss: 0.00001110
Iteration 82/1000 | Loss: 0.00001110
Iteration 83/1000 | Loss: 0.00001110
Iteration 84/1000 | Loss: 0.00001110
Iteration 85/1000 | Loss: 0.00001110
Iteration 86/1000 | Loss: 0.00001110
Iteration 87/1000 | Loss: 0.00001109
Iteration 88/1000 | Loss: 0.00001109
Iteration 89/1000 | Loss: 0.00001109
Iteration 90/1000 | Loss: 0.00001109
Iteration 91/1000 | Loss: 0.00001109
Iteration 92/1000 | Loss: 0.00001109
Iteration 93/1000 | Loss: 0.00001109
Iteration 94/1000 | Loss: 0.00001109
Iteration 95/1000 | Loss: 0.00001109
Iteration 96/1000 | Loss: 0.00001109
Iteration 97/1000 | Loss: 0.00001108
Iteration 98/1000 | Loss: 0.00001108
Iteration 99/1000 | Loss: 0.00001108
Iteration 100/1000 | Loss: 0.00001108
Iteration 101/1000 | Loss: 0.00001108
Iteration 102/1000 | Loss: 0.00001108
Iteration 103/1000 | Loss: 0.00001108
Iteration 104/1000 | Loss: 0.00001107
Iteration 105/1000 | Loss: 0.00001107
Iteration 106/1000 | Loss: 0.00001107
Iteration 107/1000 | Loss: 0.00001107
Iteration 108/1000 | Loss: 0.00001107
Iteration 109/1000 | Loss: 0.00001107
Iteration 110/1000 | Loss: 0.00001107
Iteration 111/1000 | Loss: 0.00001106
Iteration 112/1000 | Loss: 0.00001106
Iteration 113/1000 | Loss: 0.00001106
Iteration 114/1000 | Loss: 0.00001106
Iteration 115/1000 | Loss: 0.00001106
Iteration 116/1000 | Loss: 0.00001106
Iteration 117/1000 | Loss: 0.00001106
Iteration 118/1000 | Loss: 0.00001106
Iteration 119/1000 | Loss: 0.00001106
Iteration 120/1000 | Loss: 0.00001106
Iteration 121/1000 | Loss: 0.00001106
Iteration 122/1000 | Loss: 0.00001106
Iteration 123/1000 | Loss: 0.00001106
Iteration 124/1000 | Loss: 0.00001105
Iteration 125/1000 | Loss: 0.00001105
Iteration 126/1000 | Loss: 0.00001105
Iteration 127/1000 | Loss: 0.00001105
Iteration 128/1000 | Loss: 0.00001105
Iteration 129/1000 | Loss: 0.00001105
Iteration 130/1000 | Loss: 0.00001105
Iteration 131/1000 | Loss: 0.00001105
Iteration 132/1000 | Loss: 0.00001105
Iteration 133/1000 | Loss: 0.00001105
Iteration 134/1000 | Loss: 0.00001105
Iteration 135/1000 | Loss: 0.00001105
Iteration 136/1000 | Loss: 0.00001105
Iteration 137/1000 | Loss: 0.00001105
Iteration 138/1000 | Loss: 0.00001105
Iteration 139/1000 | Loss: 0.00001105
Iteration 140/1000 | Loss: 0.00001105
Iteration 141/1000 | Loss: 0.00001105
Iteration 142/1000 | Loss: 0.00001104
Iteration 143/1000 | Loss: 0.00001104
Iteration 144/1000 | Loss: 0.00001104
Iteration 145/1000 | Loss: 0.00001104
Iteration 146/1000 | Loss: 0.00001104
Iteration 147/1000 | Loss: 0.00001104
Iteration 148/1000 | Loss: 0.00001104
Iteration 149/1000 | Loss: 0.00001104
Iteration 150/1000 | Loss: 0.00001104
Iteration 151/1000 | Loss: 0.00001103
Iteration 152/1000 | Loss: 0.00001103
Iteration 153/1000 | Loss: 0.00001102
Iteration 154/1000 | Loss: 0.00001102
Iteration 155/1000 | Loss: 0.00001102
Iteration 156/1000 | Loss: 0.00001101
Iteration 157/1000 | Loss: 0.00001101
Iteration 158/1000 | Loss: 0.00001101
Iteration 159/1000 | Loss: 0.00001101
Iteration 160/1000 | Loss: 0.00001101
Iteration 161/1000 | Loss: 0.00001101
Iteration 162/1000 | Loss: 0.00001101
Iteration 163/1000 | Loss: 0.00001101
Iteration 164/1000 | Loss: 0.00001101
Iteration 165/1000 | Loss: 0.00001100
Iteration 166/1000 | Loss: 0.00001100
Iteration 167/1000 | Loss: 0.00001100
Iteration 168/1000 | Loss: 0.00001100
Iteration 169/1000 | Loss: 0.00001100
Iteration 170/1000 | Loss: 0.00001100
Iteration 171/1000 | Loss: 0.00001100
Iteration 172/1000 | Loss: 0.00001100
Iteration 173/1000 | Loss: 0.00001100
Iteration 174/1000 | Loss: 0.00001100
Iteration 175/1000 | Loss: 0.00001100
Iteration 176/1000 | Loss: 0.00001100
Iteration 177/1000 | Loss: 0.00001100
Iteration 178/1000 | Loss: 0.00001100
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [1.0998628567904234e-05, 1.0998628567904234e-05, 1.0998628567904234e-05, 1.0998628567904234e-05, 1.0998628567904234e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0998628567904234e-05

Optimization complete. Final v2v error: 2.7807059288024902 mm

Highest mean error: 2.9471254348754883 mm for frame 98

Lowest mean error: 2.665652275085449 mm for frame 16

Saving results

Total time: 37.89662194252014
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00499938
Iteration 2/25 | Loss: 0.00114956
Iteration 3/25 | Loss: 0.00075779
Iteration 4/25 | Loss: 0.00066234
Iteration 5/25 | Loss: 0.00064277
Iteration 6/25 | Loss: 0.00063802
Iteration 7/25 | Loss: 0.00063681
Iteration 8/25 | Loss: 0.00063655
Iteration 9/25 | Loss: 0.00063655
Iteration 10/25 | Loss: 0.00063655
Iteration 11/25 | Loss: 0.00063655
Iteration 12/25 | Loss: 0.00063655
Iteration 13/25 | Loss: 0.00063655
Iteration 14/25 | Loss: 0.00063655
Iteration 15/25 | Loss: 0.00063655
Iteration 16/25 | Loss: 0.00063655
Iteration 17/25 | Loss: 0.00063655
Iteration 18/25 | Loss: 0.00063655
Iteration 19/25 | Loss: 0.00063655
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0006365547305904329, 0.0006365547305904329, 0.0006365547305904329, 0.0006365547305904329, 0.0006365547305904329]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006365547305904329

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52161312
Iteration 2/25 | Loss: 0.00029795
Iteration 3/25 | Loss: 0.00029795
Iteration 4/25 | Loss: 0.00029794
Iteration 5/25 | Loss: 0.00029794
Iteration 6/25 | Loss: 0.00029794
Iteration 7/25 | Loss: 0.00029794
Iteration 8/25 | Loss: 0.00029794
Iteration 9/25 | Loss: 0.00029794
Iteration 10/25 | Loss: 0.00029794
Iteration 11/25 | Loss: 0.00029794
Iteration 12/25 | Loss: 0.00029794
Iteration 13/25 | Loss: 0.00029794
Iteration 14/25 | Loss: 0.00029794
Iteration 15/25 | Loss: 0.00029794
Iteration 16/25 | Loss: 0.00029794
Iteration 17/25 | Loss: 0.00029794
Iteration 18/25 | Loss: 0.00029794
Iteration 19/25 | Loss: 0.00029794
Iteration 20/25 | Loss: 0.00029794
Iteration 21/25 | Loss: 0.00029794
Iteration 22/25 | Loss: 0.00029794
Iteration 23/25 | Loss: 0.00029794
Iteration 24/25 | Loss: 0.00029794
Iteration 25/25 | Loss: 0.00029794

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029794
Iteration 2/1000 | Loss: 0.00002688
Iteration 3/1000 | Loss: 0.00001959
Iteration 4/1000 | Loss: 0.00001664
Iteration 5/1000 | Loss: 0.00001581
Iteration 6/1000 | Loss: 0.00001498
Iteration 7/1000 | Loss: 0.00001449
Iteration 8/1000 | Loss: 0.00001418
Iteration 9/1000 | Loss: 0.00001395
Iteration 10/1000 | Loss: 0.00001383
Iteration 11/1000 | Loss: 0.00001379
Iteration 12/1000 | Loss: 0.00001370
Iteration 13/1000 | Loss: 0.00001366
Iteration 14/1000 | Loss: 0.00001366
Iteration 15/1000 | Loss: 0.00001363
Iteration 16/1000 | Loss: 0.00001362
Iteration 17/1000 | Loss: 0.00001362
Iteration 18/1000 | Loss: 0.00001361
Iteration 19/1000 | Loss: 0.00001361
Iteration 20/1000 | Loss: 0.00001358
Iteration 21/1000 | Loss: 0.00001358
Iteration 22/1000 | Loss: 0.00001357
Iteration 23/1000 | Loss: 0.00001356
Iteration 24/1000 | Loss: 0.00001356
Iteration 25/1000 | Loss: 0.00001355
Iteration 26/1000 | Loss: 0.00001355
Iteration 27/1000 | Loss: 0.00001355
Iteration 28/1000 | Loss: 0.00001354
Iteration 29/1000 | Loss: 0.00001352
Iteration 30/1000 | Loss: 0.00001352
Iteration 31/1000 | Loss: 0.00001352
Iteration 32/1000 | Loss: 0.00001352
Iteration 33/1000 | Loss: 0.00001352
Iteration 34/1000 | Loss: 0.00001352
Iteration 35/1000 | Loss: 0.00001352
Iteration 36/1000 | Loss: 0.00001352
Iteration 37/1000 | Loss: 0.00001352
Iteration 38/1000 | Loss: 0.00001352
Iteration 39/1000 | Loss: 0.00001351
Iteration 40/1000 | Loss: 0.00001350
Iteration 41/1000 | Loss: 0.00001350
Iteration 42/1000 | Loss: 0.00001349
Iteration 43/1000 | Loss: 0.00001348
Iteration 44/1000 | Loss: 0.00001348
Iteration 45/1000 | Loss: 0.00001347
Iteration 46/1000 | Loss: 0.00001347
Iteration 47/1000 | Loss: 0.00001347
Iteration 48/1000 | Loss: 0.00001346
Iteration 49/1000 | Loss: 0.00001346
Iteration 50/1000 | Loss: 0.00001345
Iteration 51/1000 | Loss: 0.00001345
Iteration 52/1000 | Loss: 0.00001345
Iteration 53/1000 | Loss: 0.00001345
Iteration 54/1000 | Loss: 0.00001344
Iteration 55/1000 | Loss: 0.00001344
Iteration 56/1000 | Loss: 0.00001344
Iteration 57/1000 | Loss: 0.00001343
Iteration 58/1000 | Loss: 0.00001342
Iteration 59/1000 | Loss: 0.00001342
Iteration 60/1000 | Loss: 0.00001342
Iteration 61/1000 | Loss: 0.00001342
Iteration 62/1000 | Loss: 0.00001342
Iteration 63/1000 | Loss: 0.00001341
Iteration 64/1000 | Loss: 0.00001341
Iteration 65/1000 | Loss: 0.00001341
Iteration 66/1000 | Loss: 0.00001341
Iteration 67/1000 | Loss: 0.00001340
Iteration 68/1000 | Loss: 0.00001340
Iteration 69/1000 | Loss: 0.00001340
Iteration 70/1000 | Loss: 0.00001340
Iteration 71/1000 | Loss: 0.00001339
Iteration 72/1000 | Loss: 0.00001339
Iteration 73/1000 | Loss: 0.00001338
Iteration 74/1000 | Loss: 0.00001338
Iteration 75/1000 | Loss: 0.00001338
Iteration 76/1000 | Loss: 0.00001338
Iteration 77/1000 | Loss: 0.00001338
Iteration 78/1000 | Loss: 0.00001338
Iteration 79/1000 | Loss: 0.00001338
Iteration 80/1000 | Loss: 0.00001338
Iteration 81/1000 | Loss: 0.00001338
Iteration 82/1000 | Loss: 0.00001338
Iteration 83/1000 | Loss: 0.00001338
Iteration 84/1000 | Loss: 0.00001338
Iteration 85/1000 | Loss: 0.00001338
Iteration 86/1000 | Loss: 0.00001337
Iteration 87/1000 | Loss: 0.00001337
Iteration 88/1000 | Loss: 0.00001337
Iteration 89/1000 | Loss: 0.00001337
Iteration 90/1000 | Loss: 0.00001336
Iteration 91/1000 | Loss: 0.00001336
Iteration 92/1000 | Loss: 0.00001336
Iteration 93/1000 | Loss: 0.00001336
Iteration 94/1000 | Loss: 0.00001336
Iteration 95/1000 | Loss: 0.00001336
Iteration 96/1000 | Loss: 0.00001336
Iteration 97/1000 | Loss: 0.00001336
Iteration 98/1000 | Loss: 0.00001336
Iteration 99/1000 | Loss: 0.00001336
Iteration 100/1000 | Loss: 0.00001336
Iteration 101/1000 | Loss: 0.00001336
Iteration 102/1000 | Loss: 0.00001335
Iteration 103/1000 | Loss: 0.00001335
Iteration 104/1000 | Loss: 0.00001335
Iteration 105/1000 | Loss: 0.00001335
Iteration 106/1000 | Loss: 0.00001335
Iteration 107/1000 | Loss: 0.00001335
Iteration 108/1000 | Loss: 0.00001335
Iteration 109/1000 | Loss: 0.00001335
Iteration 110/1000 | Loss: 0.00001335
Iteration 111/1000 | Loss: 0.00001335
Iteration 112/1000 | Loss: 0.00001335
Iteration 113/1000 | Loss: 0.00001334
Iteration 114/1000 | Loss: 0.00001334
Iteration 115/1000 | Loss: 0.00001334
Iteration 116/1000 | Loss: 0.00001334
Iteration 117/1000 | Loss: 0.00001334
Iteration 118/1000 | Loss: 0.00001334
Iteration 119/1000 | Loss: 0.00001334
Iteration 120/1000 | Loss: 0.00001334
Iteration 121/1000 | Loss: 0.00001334
Iteration 122/1000 | Loss: 0.00001334
Iteration 123/1000 | Loss: 0.00001334
Iteration 124/1000 | Loss: 0.00001334
Iteration 125/1000 | Loss: 0.00001334
Iteration 126/1000 | Loss: 0.00001333
Iteration 127/1000 | Loss: 0.00001333
Iteration 128/1000 | Loss: 0.00001333
Iteration 129/1000 | Loss: 0.00001333
Iteration 130/1000 | Loss: 0.00001333
Iteration 131/1000 | Loss: 0.00001333
Iteration 132/1000 | Loss: 0.00001333
Iteration 133/1000 | Loss: 0.00001333
Iteration 134/1000 | Loss: 0.00001333
Iteration 135/1000 | Loss: 0.00001333
Iteration 136/1000 | Loss: 0.00001332
Iteration 137/1000 | Loss: 0.00001332
Iteration 138/1000 | Loss: 0.00001332
Iteration 139/1000 | Loss: 0.00001332
Iteration 140/1000 | Loss: 0.00001332
Iteration 141/1000 | Loss: 0.00001332
Iteration 142/1000 | Loss: 0.00001332
Iteration 143/1000 | Loss: 0.00001332
Iteration 144/1000 | Loss: 0.00001332
Iteration 145/1000 | Loss: 0.00001332
Iteration 146/1000 | Loss: 0.00001332
Iteration 147/1000 | Loss: 0.00001332
Iteration 148/1000 | Loss: 0.00001332
Iteration 149/1000 | Loss: 0.00001332
Iteration 150/1000 | Loss: 0.00001331
Iteration 151/1000 | Loss: 0.00001331
Iteration 152/1000 | Loss: 0.00001331
Iteration 153/1000 | Loss: 0.00001331
Iteration 154/1000 | Loss: 0.00001331
Iteration 155/1000 | Loss: 0.00001331
Iteration 156/1000 | Loss: 0.00001331
Iteration 157/1000 | Loss: 0.00001331
Iteration 158/1000 | Loss: 0.00001331
Iteration 159/1000 | Loss: 0.00001331
Iteration 160/1000 | Loss: 0.00001331
Iteration 161/1000 | Loss: 0.00001331
Iteration 162/1000 | Loss: 0.00001330
Iteration 163/1000 | Loss: 0.00001330
Iteration 164/1000 | Loss: 0.00001330
Iteration 165/1000 | Loss: 0.00001330
Iteration 166/1000 | Loss: 0.00001330
Iteration 167/1000 | Loss: 0.00001330
Iteration 168/1000 | Loss: 0.00001330
Iteration 169/1000 | Loss: 0.00001330
Iteration 170/1000 | Loss: 0.00001330
Iteration 171/1000 | Loss: 0.00001330
Iteration 172/1000 | Loss: 0.00001330
Iteration 173/1000 | Loss: 0.00001329
Iteration 174/1000 | Loss: 0.00001329
Iteration 175/1000 | Loss: 0.00001329
Iteration 176/1000 | Loss: 0.00001329
Iteration 177/1000 | Loss: 0.00001329
Iteration 178/1000 | Loss: 0.00001329
Iteration 179/1000 | Loss: 0.00001329
Iteration 180/1000 | Loss: 0.00001328
Iteration 181/1000 | Loss: 0.00001328
Iteration 182/1000 | Loss: 0.00001328
Iteration 183/1000 | Loss: 0.00001328
Iteration 184/1000 | Loss: 0.00001328
Iteration 185/1000 | Loss: 0.00001328
Iteration 186/1000 | Loss: 0.00001328
Iteration 187/1000 | Loss: 0.00001328
Iteration 188/1000 | Loss: 0.00001328
Iteration 189/1000 | Loss: 0.00001328
Iteration 190/1000 | Loss: 0.00001328
Iteration 191/1000 | Loss: 0.00001328
Iteration 192/1000 | Loss: 0.00001328
Iteration 193/1000 | Loss: 0.00001327
Iteration 194/1000 | Loss: 0.00001327
Iteration 195/1000 | Loss: 0.00001327
Iteration 196/1000 | Loss: 0.00001327
Iteration 197/1000 | Loss: 0.00001327
Iteration 198/1000 | Loss: 0.00001327
Iteration 199/1000 | Loss: 0.00001327
Iteration 200/1000 | Loss: 0.00001327
Iteration 201/1000 | Loss: 0.00001327
Iteration 202/1000 | Loss: 0.00001327
Iteration 203/1000 | Loss: 0.00001327
Iteration 204/1000 | Loss: 0.00001327
Iteration 205/1000 | Loss: 0.00001327
Iteration 206/1000 | Loss: 0.00001327
Iteration 207/1000 | Loss: 0.00001327
Iteration 208/1000 | Loss: 0.00001327
Iteration 209/1000 | Loss: 0.00001327
Iteration 210/1000 | Loss: 0.00001327
Iteration 211/1000 | Loss: 0.00001327
Iteration 212/1000 | Loss: 0.00001327
Iteration 213/1000 | Loss: 0.00001327
Iteration 214/1000 | Loss: 0.00001327
Iteration 215/1000 | Loss: 0.00001327
Iteration 216/1000 | Loss: 0.00001327
Iteration 217/1000 | Loss: 0.00001327
Iteration 218/1000 | Loss: 0.00001327
Iteration 219/1000 | Loss: 0.00001327
Iteration 220/1000 | Loss: 0.00001327
Iteration 221/1000 | Loss: 0.00001327
Iteration 222/1000 | Loss: 0.00001327
Iteration 223/1000 | Loss: 0.00001327
Iteration 224/1000 | Loss: 0.00001327
Iteration 225/1000 | Loss: 0.00001327
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [1.326857181993546e-05, 1.326857181993546e-05, 1.326857181993546e-05, 1.326857181993546e-05, 1.326857181993546e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.326857181993546e-05

Optimization complete. Final v2v error: 3.0488171577453613 mm

Highest mean error: 4.02822208404541 mm for frame 76

Lowest mean error: 2.666447401046753 mm for frame 120

Saving results

Total time: 41.53645157814026
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00422778
Iteration 2/25 | Loss: 0.00080347
Iteration 3/25 | Loss: 0.00068403
Iteration 4/25 | Loss: 0.00065467
Iteration 5/25 | Loss: 0.00064935
Iteration 6/25 | Loss: 0.00064842
Iteration 7/25 | Loss: 0.00064819
Iteration 8/25 | Loss: 0.00064819
Iteration 9/25 | Loss: 0.00064819
Iteration 10/25 | Loss: 0.00064819
Iteration 11/25 | Loss: 0.00064819
Iteration 12/25 | Loss: 0.00064819
Iteration 13/25 | Loss: 0.00064819
Iteration 14/25 | Loss: 0.00064819
Iteration 15/25 | Loss: 0.00064819
Iteration 16/25 | Loss: 0.00064819
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006481893942691386, 0.0006481893942691386, 0.0006481893942691386, 0.0006481893942691386, 0.0006481893942691386]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006481893942691386

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44696963
Iteration 2/25 | Loss: 0.00028231
Iteration 3/25 | Loss: 0.00028231
Iteration 4/25 | Loss: 0.00028231
Iteration 5/25 | Loss: 0.00028231
Iteration 6/25 | Loss: 0.00028231
Iteration 7/25 | Loss: 0.00028231
Iteration 8/25 | Loss: 0.00028231
Iteration 9/25 | Loss: 0.00028231
Iteration 10/25 | Loss: 0.00028231
Iteration 11/25 | Loss: 0.00028231
Iteration 12/25 | Loss: 0.00028231
Iteration 13/25 | Loss: 0.00028231
Iteration 14/25 | Loss: 0.00028231
Iteration 15/25 | Loss: 0.00028231
Iteration 16/25 | Loss: 0.00028231
Iteration 17/25 | Loss: 0.00028231
Iteration 18/25 | Loss: 0.00028231
Iteration 19/25 | Loss: 0.00028231
Iteration 20/25 | Loss: 0.00028231
Iteration 21/25 | Loss: 0.00028231
Iteration 22/25 | Loss: 0.00028231
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0002823097165673971, 0.0002823097165673971, 0.0002823097165673971, 0.0002823097165673971, 0.0002823097165673971]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002823097165673971

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028231
Iteration 2/1000 | Loss: 0.00003444
Iteration 3/1000 | Loss: 0.00002099
Iteration 4/1000 | Loss: 0.00001967
Iteration 5/1000 | Loss: 0.00001910
Iteration 6/1000 | Loss: 0.00001855
Iteration 7/1000 | Loss: 0.00001816
Iteration 8/1000 | Loss: 0.00001787
Iteration 9/1000 | Loss: 0.00001785
Iteration 10/1000 | Loss: 0.00001776
Iteration 11/1000 | Loss: 0.00001763
Iteration 12/1000 | Loss: 0.00001763
Iteration 13/1000 | Loss: 0.00001762
Iteration 14/1000 | Loss: 0.00001762
Iteration 15/1000 | Loss: 0.00001761
Iteration 16/1000 | Loss: 0.00001761
Iteration 17/1000 | Loss: 0.00001761
Iteration 18/1000 | Loss: 0.00001759
Iteration 19/1000 | Loss: 0.00001758
Iteration 20/1000 | Loss: 0.00001758
Iteration 21/1000 | Loss: 0.00001758
Iteration 22/1000 | Loss: 0.00001758
Iteration 23/1000 | Loss: 0.00001758
Iteration 24/1000 | Loss: 0.00001758
Iteration 25/1000 | Loss: 0.00001758
Iteration 26/1000 | Loss: 0.00001758
Iteration 27/1000 | Loss: 0.00001758
Iteration 28/1000 | Loss: 0.00001758
Iteration 29/1000 | Loss: 0.00001758
Iteration 30/1000 | Loss: 0.00001758
Iteration 31/1000 | Loss: 0.00001757
Iteration 32/1000 | Loss: 0.00001756
Iteration 33/1000 | Loss: 0.00001755
Iteration 34/1000 | Loss: 0.00001755
Iteration 35/1000 | Loss: 0.00001755
Iteration 36/1000 | Loss: 0.00001755
Iteration 37/1000 | Loss: 0.00001755
Iteration 38/1000 | Loss: 0.00001754
Iteration 39/1000 | Loss: 0.00001754
Iteration 40/1000 | Loss: 0.00001752
Iteration 41/1000 | Loss: 0.00001749
Iteration 42/1000 | Loss: 0.00001748
Iteration 43/1000 | Loss: 0.00001747
Iteration 44/1000 | Loss: 0.00001747
Iteration 45/1000 | Loss: 0.00001746
Iteration 46/1000 | Loss: 0.00001746
Iteration 47/1000 | Loss: 0.00001746
Iteration 48/1000 | Loss: 0.00001746
Iteration 49/1000 | Loss: 0.00001746
Iteration 50/1000 | Loss: 0.00001746
Iteration 51/1000 | Loss: 0.00001746
Iteration 52/1000 | Loss: 0.00001746
Iteration 53/1000 | Loss: 0.00001746
Iteration 54/1000 | Loss: 0.00001746
Iteration 55/1000 | Loss: 0.00001745
Iteration 56/1000 | Loss: 0.00001745
Iteration 57/1000 | Loss: 0.00001745
Iteration 58/1000 | Loss: 0.00001744
Iteration 59/1000 | Loss: 0.00001744
Iteration 60/1000 | Loss: 0.00001744
Iteration 61/1000 | Loss: 0.00001744
Iteration 62/1000 | Loss: 0.00001744
Iteration 63/1000 | Loss: 0.00001744
Iteration 64/1000 | Loss: 0.00001743
Iteration 65/1000 | Loss: 0.00001743
Iteration 66/1000 | Loss: 0.00001743
Iteration 67/1000 | Loss: 0.00001743
Iteration 68/1000 | Loss: 0.00001743
Iteration 69/1000 | Loss: 0.00001743
Iteration 70/1000 | Loss: 0.00001743
Iteration 71/1000 | Loss: 0.00001743
Iteration 72/1000 | Loss: 0.00001743
Iteration 73/1000 | Loss: 0.00001743
Iteration 74/1000 | Loss: 0.00001743
Iteration 75/1000 | Loss: 0.00001742
Iteration 76/1000 | Loss: 0.00001742
Iteration 77/1000 | Loss: 0.00001742
Iteration 78/1000 | Loss: 0.00001742
Iteration 79/1000 | Loss: 0.00001742
Iteration 80/1000 | Loss: 0.00001741
Iteration 81/1000 | Loss: 0.00001741
Iteration 82/1000 | Loss: 0.00001741
Iteration 83/1000 | Loss: 0.00001741
Iteration 84/1000 | Loss: 0.00001741
Iteration 85/1000 | Loss: 0.00001741
Iteration 86/1000 | Loss: 0.00001741
Iteration 87/1000 | Loss: 0.00001740
Iteration 88/1000 | Loss: 0.00001740
Iteration 89/1000 | Loss: 0.00001740
Iteration 90/1000 | Loss: 0.00001740
Iteration 91/1000 | Loss: 0.00001740
Iteration 92/1000 | Loss: 0.00001740
Iteration 93/1000 | Loss: 0.00001740
Iteration 94/1000 | Loss: 0.00001739
Iteration 95/1000 | Loss: 0.00001739
Iteration 96/1000 | Loss: 0.00001739
Iteration 97/1000 | Loss: 0.00001739
Iteration 98/1000 | Loss: 0.00001739
Iteration 99/1000 | Loss: 0.00001739
Iteration 100/1000 | Loss: 0.00001739
Iteration 101/1000 | Loss: 0.00001739
Iteration 102/1000 | Loss: 0.00001739
Iteration 103/1000 | Loss: 0.00001739
Iteration 104/1000 | Loss: 0.00001739
Iteration 105/1000 | Loss: 0.00001739
Iteration 106/1000 | Loss: 0.00001739
Iteration 107/1000 | Loss: 0.00001739
Iteration 108/1000 | Loss: 0.00001739
Iteration 109/1000 | Loss: 0.00001739
Iteration 110/1000 | Loss: 0.00001738
Iteration 111/1000 | Loss: 0.00001738
Iteration 112/1000 | Loss: 0.00001738
Iteration 113/1000 | Loss: 0.00001738
Iteration 114/1000 | Loss: 0.00001738
Iteration 115/1000 | Loss: 0.00001738
Iteration 116/1000 | Loss: 0.00001738
Iteration 117/1000 | Loss: 0.00001738
Iteration 118/1000 | Loss: 0.00001737
Iteration 119/1000 | Loss: 0.00001737
Iteration 120/1000 | Loss: 0.00001737
Iteration 121/1000 | Loss: 0.00001737
Iteration 122/1000 | Loss: 0.00001737
Iteration 123/1000 | Loss: 0.00001737
Iteration 124/1000 | Loss: 0.00001737
Iteration 125/1000 | Loss: 0.00001737
Iteration 126/1000 | Loss: 0.00001736
Iteration 127/1000 | Loss: 0.00001736
Iteration 128/1000 | Loss: 0.00001736
Iteration 129/1000 | Loss: 0.00001736
Iteration 130/1000 | Loss: 0.00001736
Iteration 131/1000 | Loss: 0.00001736
Iteration 132/1000 | Loss: 0.00001736
Iteration 133/1000 | Loss: 0.00001736
Iteration 134/1000 | Loss: 0.00001736
Iteration 135/1000 | Loss: 0.00001736
Iteration 136/1000 | Loss: 0.00001736
Iteration 137/1000 | Loss: 0.00001736
Iteration 138/1000 | Loss: 0.00001735
Iteration 139/1000 | Loss: 0.00001735
Iteration 140/1000 | Loss: 0.00001735
Iteration 141/1000 | Loss: 0.00001735
Iteration 142/1000 | Loss: 0.00001734
Iteration 143/1000 | Loss: 0.00001734
Iteration 144/1000 | Loss: 0.00001733
Iteration 145/1000 | Loss: 0.00001733
Iteration 146/1000 | Loss: 0.00001733
Iteration 147/1000 | Loss: 0.00001733
Iteration 148/1000 | Loss: 0.00001733
Iteration 149/1000 | Loss: 0.00001733
Iteration 150/1000 | Loss: 0.00001733
Iteration 151/1000 | Loss: 0.00001733
Iteration 152/1000 | Loss: 0.00001732
Iteration 153/1000 | Loss: 0.00001732
Iteration 154/1000 | Loss: 0.00001732
Iteration 155/1000 | Loss: 0.00001731
Iteration 156/1000 | Loss: 0.00001731
Iteration 157/1000 | Loss: 0.00001731
Iteration 158/1000 | Loss: 0.00001730
Iteration 159/1000 | Loss: 0.00001730
Iteration 160/1000 | Loss: 0.00001730
Iteration 161/1000 | Loss: 0.00001730
Iteration 162/1000 | Loss: 0.00001730
Iteration 163/1000 | Loss: 0.00001730
Iteration 164/1000 | Loss: 0.00001730
Iteration 165/1000 | Loss: 0.00001730
Iteration 166/1000 | Loss: 0.00001730
Iteration 167/1000 | Loss: 0.00001729
Iteration 168/1000 | Loss: 0.00001729
Iteration 169/1000 | Loss: 0.00001729
Iteration 170/1000 | Loss: 0.00001729
Iteration 171/1000 | Loss: 0.00001729
Iteration 172/1000 | Loss: 0.00001729
Iteration 173/1000 | Loss: 0.00001729
Iteration 174/1000 | Loss: 0.00001729
Iteration 175/1000 | Loss: 0.00001729
Iteration 176/1000 | Loss: 0.00001729
Iteration 177/1000 | Loss: 0.00001729
Iteration 178/1000 | Loss: 0.00001729
Iteration 179/1000 | Loss: 0.00001729
Iteration 180/1000 | Loss: 0.00001729
Iteration 181/1000 | Loss: 0.00001729
Iteration 182/1000 | Loss: 0.00001729
Iteration 183/1000 | Loss: 0.00001728
Iteration 184/1000 | Loss: 0.00001728
Iteration 185/1000 | Loss: 0.00001728
Iteration 186/1000 | Loss: 0.00001728
Iteration 187/1000 | Loss: 0.00001728
Iteration 188/1000 | Loss: 0.00001728
Iteration 189/1000 | Loss: 0.00001728
Iteration 190/1000 | Loss: 0.00001728
Iteration 191/1000 | Loss: 0.00001728
Iteration 192/1000 | Loss: 0.00001728
Iteration 193/1000 | Loss: 0.00001728
Iteration 194/1000 | Loss: 0.00001728
Iteration 195/1000 | Loss: 0.00001728
Iteration 196/1000 | Loss: 0.00001728
Iteration 197/1000 | Loss: 0.00001728
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [1.728066308714915e-05, 1.728066308714915e-05, 1.728066308714915e-05, 1.728066308714915e-05, 1.728066308714915e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.728066308714915e-05

Optimization complete. Final v2v error: 3.4973273277282715 mm

Highest mean error: 3.7865209579467773 mm for frame 97

Lowest mean error: 3.148076295852661 mm for frame 4

Saving results

Total time: 34.627124071121216
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01073312
Iteration 2/25 | Loss: 0.00188281
Iteration 3/25 | Loss: 0.00126105
Iteration 4/25 | Loss: 0.00113215
Iteration 5/25 | Loss: 0.00103589
Iteration 6/25 | Loss: 0.00114456
Iteration 7/25 | Loss: 0.00095427
Iteration 8/25 | Loss: 0.00094521
Iteration 9/25 | Loss: 0.00092334
Iteration 10/25 | Loss: 0.00085592
Iteration 11/25 | Loss: 0.00085894
Iteration 12/25 | Loss: 0.00081325
Iteration 13/25 | Loss: 0.00080656
Iteration 14/25 | Loss: 0.00080333
Iteration 15/25 | Loss: 0.00079816
Iteration 16/25 | Loss: 0.00079244
Iteration 17/25 | Loss: 0.00079106
Iteration 18/25 | Loss: 0.00079012
Iteration 19/25 | Loss: 0.00078975
Iteration 20/25 | Loss: 0.00078942
Iteration 21/25 | Loss: 0.00078893
Iteration 22/25 | Loss: 0.00078875
Iteration 23/25 | Loss: 0.00078872
Iteration 24/25 | Loss: 0.00078872
Iteration 25/25 | Loss: 0.00078872

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45481873
Iteration 2/25 | Loss: 0.00095822
Iteration 3/25 | Loss: 0.00095822
Iteration 4/25 | Loss: 0.00095822
Iteration 5/25 | Loss: 0.00095822
Iteration 6/25 | Loss: 0.00095822
Iteration 7/25 | Loss: 0.00095822
Iteration 8/25 | Loss: 0.00095822
Iteration 9/25 | Loss: 0.00095822
Iteration 10/25 | Loss: 0.00095822
Iteration 11/25 | Loss: 0.00095822
Iteration 12/25 | Loss: 0.00095822
Iteration 13/25 | Loss: 0.00095822
Iteration 14/25 | Loss: 0.00095822
Iteration 15/25 | Loss: 0.00095822
Iteration 16/25 | Loss: 0.00095822
Iteration 17/25 | Loss: 0.00095822
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009582195780239999, 0.0009582195780239999, 0.0009582195780239999, 0.0009582195780239999, 0.0009582195780239999]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009582195780239999

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095822
Iteration 2/1000 | Loss: 0.00237427
Iteration 3/1000 | Loss: 0.00024132
Iteration 4/1000 | Loss: 0.00027372
Iteration 5/1000 | Loss: 0.00011698
Iteration 6/1000 | Loss: 0.00009109
Iteration 7/1000 | Loss: 0.00006966
Iteration 8/1000 | Loss: 0.00064888
Iteration 9/1000 | Loss: 0.00067305
Iteration 10/1000 | Loss: 0.00028196
Iteration 11/1000 | Loss: 0.00005520
Iteration 12/1000 | Loss: 0.00004382
Iteration 13/1000 | Loss: 0.00003504
Iteration 14/1000 | Loss: 0.00003153
Iteration 15/1000 | Loss: 0.00002991
Iteration 16/1000 | Loss: 0.00100306
Iteration 17/1000 | Loss: 0.00111130
Iteration 18/1000 | Loss: 0.00077635
Iteration 19/1000 | Loss: 0.00058106
Iteration 20/1000 | Loss: 0.00043722
Iteration 21/1000 | Loss: 0.00052695
Iteration 22/1000 | Loss: 0.00003127
Iteration 23/1000 | Loss: 0.00002506
Iteration 24/1000 | Loss: 0.00002177
Iteration 25/1000 | Loss: 0.00002026
Iteration 26/1000 | Loss: 0.00001954
Iteration 27/1000 | Loss: 0.00001918
Iteration 28/1000 | Loss: 0.00001884
Iteration 29/1000 | Loss: 0.00001855
Iteration 30/1000 | Loss: 0.00001835
Iteration 31/1000 | Loss: 0.00001823
Iteration 32/1000 | Loss: 0.00001823
Iteration 33/1000 | Loss: 0.00001819
Iteration 34/1000 | Loss: 0.00001818
Iteration 35/1000 | Loss: 0.00001818
Iteration 36/1000 | Loss: 0.00001814
Iteration 37/1000 | Loss: 0.00001813
Iteration 38/1000 | Loss: 0.00001812
Iteration 39/1000 | Loss: 0.00001812
Iteration 40/1000 | Loss: 0.00001811
Iteration 41/1000 | Loss: 0.00001811
Iteration 42/1000 | Loss: 0.00001810
Iteration 43/1000 | Loss: 0.00001807
Iteration 44/1000 | Loss: 0.00001807
Iteration 45/1000 | Loss: 0.00001805
Iteration 46/1000 | Loss: 0.00001805
Iteration 47/1000 | Loss: 0.00001804
Iteration 48/1000 | Loss: 0.00001803
Iteration 49/1000 | Loss: 0.00001803
Iteration 50/1000 | Loss: 0.00001803
Iteration 51/1000 | Loss: 0.00001803
Iteration 52/1000 | Loss: 0.00001803
Iteration 53/1000 | Loss: 0.00001803
Iteration 54/1000 | Loss: 0.00001803
Iteration 55/1000 | Loss: 0.00001802
Iteration 56/1000 | Loss: 0.00001802
Iteration 57/1000 | Loss: 0.00001802
Iteration 58/1000 | Loss: 0.00001802
Iteration 59/1000 | Loss: 0.00001802
Iteration 60/1000 | Loss: 0.00001801
Iteration 61/1000 | Loss: 0.00001801
Iteration 62/1000 | Loss: 0.00001801
Iteration 63/1000 | Loss: 0.00001800
Iteration 64/1000 | Loss: 0.00001800
Iteration 65/1000 | Loss: 0.00001800
Iteration 66/1000 | Loss: 0.00001800
Iteration 67/1000 | Loss: 0.00001799
Iteration 68/1000 | Loss: 0.00001799
Iteration 69/1000 | Loss: 0.00001799
Iteration 70/1000 | Loss: 0.00001798
Iteration 71/1000 | Loss: 0.00001797
Iteration 72/1000 | Loss: 0.00001797
Iteration 73/1000 | Loss: 0.00001797
Iteration 74/1000 | Loss: 0.00001797
Iteration 75/1000 | Loss: 0.00001797
Iteration 76/1000 | Loss: 0.00001796
Iteration 77/1000 | Loss: 0.00001796
Iteration 78/1000 | Loss: 0.00001796
Iteration 79/1000 | Loss: 0.00001795
Iteration 80/1000 | Loss: 0.00001795
Iteration 81/1000 | Loss: 0.00001795
Iteration 82/1000 | Loss: 0.00001795
Iteration 83/1000 | Loss: 0.00001795
Iteration 84/1000 | Loss: 0.00001795
Iteration 85/1000 | Loss: 0.00001795
Iteration 86/1000 | Loss: 0.00001795
Iteration 87/1000 | Loss: 0.00001794
Iteration 88/1000 | Loss: 0.00001794
Iteration 89/1000 | Loss: 0.00001794
Iteration 90/1000 | Loss: 0.00001794
Iteration 91/1000 | Loss: 0.00001794
Iteration 92/1000 | Loss: 0.00001794
Iteration 93/1000 | Loss: 0.00001794
Iteration 94/1000 | Loss: 0.00001794
Iteration 95/1000 | Loss: 0.00001794
Iteration 96/1000 | Loss: 0.00001794
Iteration 97/1000 | Loss: 0.00001794
Iteration 98/1000 | Loss: 0.00001794
Iteration 99/1000 | Loss: 0.00001794
Iteration 100/1000 | Loss: 0.00001793
Iteration 101/1000 | Loss: 0.00001793
Iteration 102/1000 | Loss: 0.00001793
Iteration 103/1000 | Loss: 0.00001793
Iteration 104/1000 | Loss: 0.00001793
Iteration 105/1000 | Loss: 0.00001793
Iteration 106/1000 | Loss: 0.00001793
Iteration 107/1000 | Loss: 0.00001793
Iteration 108/1000 | Loss: 0.00001793
Iteration 109/1000 | Loss: 0.00001793
Iteration 110/1000 | Loss: 0.00001793
Iteration 111/1000 | Loss: 0.00001793
Iteration 112/1000 | Loss: 0.00001793
Iteration 113/1000 | Loss: 0.00001793
Iteration 114/1000 | Loss: 0.00001793
Iteration 115/1000 | Loss: 0.00001793
Iteration 116/1000 | Loss: 0.00001793
Iteration 117/1000 | Loss: 0.00001793
Iteration 118/1000 | Loss: 0.00001793
Iteration 119/1000 | Loss: 0.00001793
Iteration 120/1000 | Loss: 0.00001793
Iteration 121/1000 | Loss: 0.00001793
Iteration 122/1000 | Loss: 0.00001793
Iteration 123/1000 | Loss: 0.00001793
Iteration 124/1000 | Loss: 0.00001793
Iteration 125/1000 | Loss: 0.00001793
Iteration 126/1000 | Loss: 0.00001793
Iteration 127/1000 | Loss: 0.00001793
Iteration 128/1000 | Loss: 0.00001793
Iteration 129/1000 | Loss: 0.00001793
Iteration 130/1000 | Loss: 0.00001793
Iteration 131/1000 | Loss: 0.00001793
Iteration 132/1000 | Loss: 0.00001793
Iteration 133/1000 | Loss: 0.00001793
Iteration 134/1000 | Loss: 0.00001793
Iteration 135/1000 | Loss: 0.00001793
Iteration 136/1000 | Loss: 0.00001793
Iteration 137/1000 | Loss: 0.00001793
Iteration 138/1000 | Loss: 0.00001793
Iteration 139/1000 | Loss: 0.00001793
Iteration 140/1000 | Loss: 0.00001793
Iteration 141/1000 | Loss: 0.00001793
Iteration 142/1000 | Loss: 0.00001793
Iteration 143/1000 | Loss: 0.00001793
Iteration 144/1000 | Loss: 0.00001793
Iteration 145/1000 | Loss: 0.00001793
Iteration 146/1000 | Loss: 0.00001793
Iteration 147/1000 | Loss: 0.00001793
Iteration 148/1000 | Loss: 0.00001793
Iteration 149/1000 | Loss: 0.00001793
Iteration 150/1000 | Loss: 0.00001793
Iteration 151/1000 | Loss: 0.00001793
Iteration 152/1000 | Loss: 0.00001793
Iteration 153/1000 | Loss: 0.00001793
Iteration 154/1000 | Loss: 0.00001793
Iteration 155/1000 | Loss: 0.00001793
Iteration 156/1000 | Loss: 0.00001793
Iteration 157/1000 | Loss: 0.00001793
Iteration 158/1000 | Loss: 0.00001793
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [1.792810689948965e-05, 1.792810689948965e-05, 1.792810689948965e-05, 1.792810689948965e-05, 1.792810689948965e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.792810689948965e-05

Optimization complete. Final v2v error: 3.5447845458984375 mm

Highest mean error: 4.011144161224365 mm for frame 154

Lowest mean error: 3.180586099624634 mm for frame 228

Saving results

Total time: 103.6422131061554
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01033118
Iteration 2/25 | Loss: 0.00229265
Iteration 3/25 | Loss: 0.00136605
Iteration 4/25 | Loss: 0.00111794
Iteration 5/25 | Loss: 0.00116426
Iteration 6/25 | Loss: 0.00103161
Iteration 7/25 | Loss: 0.00090917
Iteration 8/25 | Loss: 0.00083738
Iteration 9/25 | Loss: 0.00079396
Iteration 10/25 | Loss: 0.00077602
Iteration 11/25 | Loss: 0.00077032
Iteration 12/25 | Loss: 0.00076898
Iteration 13/25 | Loss: 0.00076818
Iteration 14/25 | Loss: 0.00076755
Iteration 15/25 | Loss: 0.00076711
Iteration 16/25 | Loss: 0.00076688
Iteration 17/25 | Loss: 0.00076681
Iteration 18/25 | Loss: 0.00076681
Iteration 19/25 | Loss: 0.00076680
Iteration 20/25 | Loss: 0.00076680
Iteration 21/25 | Loss: 0.00076680
Iteration 22/25 | Loss: 0.00076680
Iteration 23/25 | Loss: 0.00076680
Iteration 24/25 | Loss: 0.00076680
Iteration 25/25 | Loss: 0.00076680

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45266235
Iteration 2/25 | Loss: 0.00032482
Iteration 3/25 | Loss: 0.00032482
Iteration 4/25 | Loss: 0.00032482
Iteration 5/25 | Loss: 0.00032482
Iteration 6/25 | Loss: 0.00032482
Iteration 7/25 | Loss: 0.00032482
Iteration 8/25 | Loss: 0.00032482
Iteration 9/25 | Loss: 0.00032482
Iteration 10/25 | Loss: 0.00032481
Iteration 11/25 | Loss: 0.00032481
Iteration 12/25 | Loss: 0.00032481
Iteration 13/25 | Loss: 0.00032481
Iteration 14/25 | Loss: 0.00032481
Iteration 15/25 | Loss: 0.00032481
Iteration 16/25 | Loss: 0.00032481
Iteration 17/25 | Loss: 0.00032481
Iteration 18/25 | Loss: 0.00032481
Iteration 19/25 | Loss: 0.00032481
Iteration 20/25 | Loss: 0.00032481
Iteration 21/25 | Loss: 0.00032481
Iteration 22/25 | Loss: 0.00032481
Iteration 23/25 | Loss: 0.00032481
Iteration 24/25 | Loss: 0.00032481
Iteration 25/25 | Loss: 0.00032481

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032481
Iteration 2/1000 | Loss: 0.00003004
Iteration 3/1000 | Loss: 0.00002539
Iteration 4/1000 | Loss: 0.00002395
Iteration 5/1000 | Loss: 0.00002305
Iteration 6/1000 | Loss: 0.00002298
Iteration 7/1000 | Loss: 0.00002269
Iteration 8/1000 | Loss: 0.00002249
Iteration 9/1000 | Loss: 0.00002248
Iteration 10/1000 | Loss: 0.00002243
Iteration 11/1000 | Loss: 0.00002237
Iteration 12/1000 | Loss: 0.00002225
Iteration 13/1000 | Loss: 0.00002206
Iteration 14/1000 | Loss: 0.00002197
Iteration 15/1000 | Loss: 0.00002196
Iteration 16/1000 | Loss: 0.00002193
Iteration 17/1000 | Loss: 0.00002193
Iteration 18/1000 | Loss: 0.00002193
Iteration 19/1000 | Loss: 0.00002192
Iteration 20/1000 | Loss: 0.00002191
Iteration 21/1000 | Loss: 0.00002191
Iteration 22/1000 | Loss: 0.00002190
Iteration 23/1000 | Loss: 0.00002190
Iteration 24/1000 | Loss: 0.00002190
Iteration 25/1000 | Loss: 0.00002190
Iteration 26/1000 | Loss: 0.00002190
Iteration 27/1000 | Loss: 0.00002190
Iteration 28/1000 | Loss: 0.00002190
Iteration 29/1000 | Loss: 0.00002190
Iteration 30/1000 | Loss: 0.00002190
Iteration 31/1000 | Loss: 0.00002189
Iteration 32/1000 | Loss: 0.00002189
Iteration 33/1000 | Loss: 0.00002188
Iteration 34/1000 | Loss: 0.00002188
Iteration 35/1000 | Loss: 0.00002188
Iteration 36/1000 | Loss: 0.00002188
Iteration 37/1000 | Loss: 0.00002187
Iteration 38/1000 | Loss: 0.00002187
Iteration 39/1000 | Loss: 0.00002187
Iteration 40/1000 | Loss: 0.00002187
Iteration 41/1000 | Loss: 0.00002187
Iteration 42/1000 | Loss: 0.00002187
Iteration 43/1000 | Loss: 0.00002186
Iteration 44/1000 | Loss: 0.00002186
Iteration 45/1000 | Loss: 0.00002186
Iteration 46/1000 | Loss: 0.00002186
Iteration 47/1000 | Loss: 0.00002186
Iteration 48/1000 | Loss: 0.00002183
Iteration 49/1000 | Loss: 0.00002182
Iteration 50/1000 | Loss: 0.00002176
Iteration 51/1000 | Loss: 0.00002176
Iteration 52/1000 | Loss: 0.00002176
Iteration 53/1000 | Loss: 0.00002176
Iteration 54/1000 | Loss: 0.00002176
Iteration 55/1000 | Loss: 0.00002176
Iteration 56/1000 | Loss: 0.00002176
Iteration 57/1000 | Loss: 0.00002176
Iteration 58/1000 | Loss: 0.00002176
Iteration 59/1000 | Loss: 0.00002175
Iteration 60/1000 | Loss: 0.00002175
Iteration 61/1000 | Loss: 0.00002175
Iteration 62/1000 | Loss: 0.00002175
Iteration 63/1000 | Loss: 0.00002175
Iteration 64/1000 | Loss: 0.00002175
Iteration 65/1000 | Loss: 0.00002175
Iteration 66/1000 | Loss: 0.00002175
Iteration 67/1000 | Loss: 0.00002175
Iteration 68/1000 | Loss: 0.00002175
Iteration 69/1000 | Loss: 0.00002175
Iteration 70/1000 | Loss: 0.00002175
Iteration 71/1000 | Loss: 0.00002175
Iteration 72/1000 | Loss: 0.00002175
Iteration 73/1000 | Loss: 0.00002175
Iteration 74/1000 | Loss: 0.00002174
Iteration 75/1000 | Loss: 0.00002174
Iteration 76/1000 | Loss: 0.00002174
Iteration 77/1000 | Loss: 0.00002174
Iteration 78/1000 | Loss: 0.00002174
Iteration 79/1000 | Loss: 0.00002174
Iteration 80/1000 | Loss: 0.00002174
Iteration 81/1000 | Loss: 0.00002174
Iteration 82/1000 | Loss: 0.00002174
Iteration 83/1000 | Loss: 0.00002174
Iteration 84/1000 | Loss: 0.00002173
Iteration 85/1000 | Loss: 0.00002173
Iteration 86/1000 | Loss: 0.00002173
Iteration 87/1000 | Loss: 0.00002173
Iteration 88/1000 | Loss: 0.00002173
Iteration 89/1000 | Loss: 0.00002173
Iteration 90/1000 | Loss: 0.00002173
Iteration 91/1000 | Loss: 0.00002173
Iteration 92/1000 | Loss: 0.00002173
Iteration 93/1000 | Loss: 0.00002173
Iteration 94/1000 | Loss: 0.00002173
Iteration 95/1000 | Loss: 0.00002173
Iteration 96/1000 | Loss: 0.00002173
Iteration 97/1000 | Loss: 0.00002173
Iteration 98/1000 | Loss: 0.00002173
Iteration 99/1000 | Loss: 0.00002173
Iteration 100/1000 | Loss: 0.00002173
Iteration 101/1000 | Loss: 0.00002173
Iteration 102/1000 | Loss: 0.00002173
Iteration 103/1000 | Loss: 0.00002173
Iteration 104/1000 | Loss: 0.00002172
Iteration 105/1000 | Loss: 0.00002172
Iteration 106/1000 | Loss: 0.00002172
Iteration 107/1000 | Loss: 0.00002172
Iteration 108/1000 | Loss: 0.00002172
Iteration 109/1000 | Loss: 0.00002172
Iteration 110/1000 | Loss: 0.00002172
Iteration 111/1000 | Loss: 0.00002172
Iteration 112/1000 | Loss: 0.00002172
Iteration 113/1000 | Loss: 0.00002172
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [2.172455242543947e-05, 2.172455242543947e-05, 2.172455242543947e-05, 2.172455242543947e-05, 2.172455242543947e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.172455242543947e-05

Optimization complete. Final v2v error: 3.8170907497406006 mm

Highest mean error: 3.8950045108795166 mm for frame 44

Lowest mean error: 3.7593994140625 mm for frame 95

Saving results

Total time: 57.734827280044556
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01099917
Iteration 2/25 | Loss: 0.00112147
Iteration 3/25 | Loss: 0.00076435
Iteration 4/25 | Loss: 0.00069086
Iteration 5/25 | Loss: 0.00067019
Iteration 6/25 | Loss: 0.00066554
Iteration 7/25 | Loss: 0.00067017
Iteration 8/25 | Loss: 0.00067827
Iteration 9/25 | Loss: 0.00067490
Iteration 10/25 | Loss: 0.00066534
Iteration 11/25 | Loss: 0.00067695
Iteration 12/25 | Loss: 0.00067330
Iteration 13/25 | Loss: 0.00066750
Iteration 14/25 | Loss: 0.00067541
Iteration 15/25 | Loss: 0.00067510
Iteration 16/25 | Loss: 0.00067698
Iteration 17/25 | Loss: 0.00067620
Iteration 18/25 | Loss: 0.00067682
Iteration 19/25 | Loss: 0.00067552
Iteration 20/25 | Loss: 0.00067608
Iteration 21/25 | Loss: 0.00067438
Iteration 22/25 | Loss: 0.00067525
Iteration 23/25 | Loss: 0.00067310
Iteration 24/25 | Loss: 0.00067671
Iteration 25/25 | Loss: 0.00067596

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.39223456
Iteration 2/25 | Loss: 0.00039478
Iteration 3/25 | Loss: 0.00039478
Iteration 4/25 | Loss: 0.00039478
Iteration 5/25 | Loss: 0.00039478
Iteration 6/25 | Loss: 0.00039478
Iteration 7/25 | Loss: 0.00039478
Iteration 8/25 | Loss: 0.00039478
Iteration 9/25 | Loss: 0.00039478
Iteration 10/25 | Loss: 0.00039478
Iteration 11/25 | Loss: 0.00039477
Iteration 12/25 | Loss: 0.00039477
Iteration 13/25 | Loss: 0.00039477
Iteration 14/25 | Loss: 0.00039477
Iteration 15/25 | Loss: 0.00039477
Iteration 16/25 | Loss: 0.00039477
Iteration 17/25 | Loss: 0.00039477
Iteration 18/25 | Loss: 0.00039477
Iteration 19/25 | Loss: 0.00039477
Iteration 20/25 | Loss: 0.00039477
Iteration 21/25 | Loss: 0.00039477
Iteration 22/25 | Loss: 0.00039477
Iteration 23/25 | Loss: 0.00039477
Iteration 24/25 | Loss: 0.00039477
Iteration 25/25 | Loss: 0.00039477

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039477
Iteration 2/1000 | Loss: 0.00024247
Iteration 3/1000 | Loss: 0.00041746
Iteration 4/1000 | Loss: 0.00048506
Iteration 5/1000 | Loss: 0.00047481
Iteration 6/1000 | Loss: 0.00045717
Iteration 7/1000 | Loss: 0.00048196
Iteration 8/1000 | Loss: 0.00050724
Iteration 9/1000 | Loss: 0.00049583
Iteration 10/1000 | Loss: 0.00035611
Iteration 11/1000 | Loss: 0.00037672
Iteration 12/1000 | Loss: 0.00035637
Iteration 13/1000 | Loss: 0.00036514
Iteration 14/1000 | Loss: 0.00046244
Iteration 15/1000 | Loss: 0.00040012
Iteration 16/1000 | Loss: 0.00039866
Iteration 17/1000 | Loss: 0.00058038
Iteration 18/1000 | Loss: 0.00043887
Iteration 19/1000 | Loss: 0.00061742
Iteration 20/1000 | Loss: 0.00038808
Iteration 21/1000 | Loss: 0.00050162
Iteration 22/1000 | Loss: 0.00053011
Iteration 23/1000 | Loss: 0.00034752
Iteration 24/1000 | Loss: 0.00042974
Iteration 25/1000 | Loss: 0.00044241
Iteration 26/1000 | Loss: 0.00049256
Iteration 27/1000 | Loss: 0.00046125
Iteration 28/1000 | Loss: 0.00046416
Iteration 29/1000 | Loss: 0.00044557
Iteration 30/1000 | Loss: 0.00019978
Iteration 31/1000 | Loss: 0.00033860
Iteration 32/1000 | Loss: 0.00043822
Iteration 33/1000 | Loss: 0.00039992
Iteration 34/1000 | Loss: 0.00041339
Iteration 35/1000 | Loss: 0.00043704
Iteration 36/1000 | Loss: 0.00041638
Iteration 37/1000 | Loss: 0.00060766
Iteration 38/1000 | Loss: 0.00072621
Iteration 39/1000 | Loss: 0.00073118
Iteration 40/1000 | Loss: 0.00037942
Iteration 41/1000 | Loss: 0.00039299
Iteration 42/1000 | Loss: 0.00040513
Iteration 43/1000 | Loss: 0.00045706
Iteration 44/1000 | Loss: 0.00052167
Iteration 45/1000 | Loss: 0.00041661
Iteration 46/1000 | Loss: 0.00039882
Iteration 47/1000 | Loss: 0.00046963
Iteration 48/1000 | Loss: 0.00061822
Iteration 49/1000 | Loss: 0.00054506
Iteration 50/1000 | Loss: 0.00058183
Iteration 51/1000 | Loss: 0.00058196
Iteration 52/1000 | Loss: 0.00055315
Iteration 53/1000 | Loss: 0.00049278
Iteration 54/1000 | Loss: 0.00024428
Iteration 55/1000 | Loss: 0.00026357
Iteration 56/1000 | Loss: 0.00035269
Iteration 57/1000 | Loss: 0.00031511
Iteration 58/1000 | Loss: 0.00031361
Iteration 59/1000 | Loss: 0.00026950
Iteration 60/1000 | Loss: 0.00029648
Iteration 61/1000 | Loss: 0.00055789
Iteration 62/1000 | Loss: 0.00020789
Iteration 63/1000 | Loss: 0.00028721
Iteration 64/1000 | Loss: 0.00031631
Iteration 65/1000 | Loss: 0.00031237
Iteration 66/1000 | Loss: 0.00041118
Iteration 67/1000 | Loss: 0.00017662
Iteration 68/1000 | Loss: 0.00025859
Iteration 69/1000 | Loss: 0.00028949
Iteration 70/1000 | Loss: 0.00023654
Iteration 71/1000 | Loss: 0.00027078
Iteration 72/1000 | Loss: 0.00027701
Iteration 73/1000 | Loss: 0.00035862
Iteration 74/1000 | Loss: 0.00028334
Iteration 75/1000 | Loss: 0.00034019
Iteration 76/1000 | Loss: 0.00032104
Iteration 77/1000 | Loss: 0.00029919
Iteration 78/1000 | Loss: 0.00035247
Iteration 79/1000 | Loss: 0.00030839
Iteration 80/1000 | Loss: 0.00038341
Iteration 81/1000 | Loss: 0.00029464
Iteration 82/1000 | Loss: 0.00026450
Iteration 83/1000 | Loss: 0.00020936
Iteration 84/1000 | Loss: 0.00029461
Iteration 85/1000 | Loss: 0.00032301
Iteration 86/1000 | Loss: 0.00044315
Iteration 87/1000 | Loss: 0.00030824
Iteration 88/1000 | Loss: 0.00028194
Iteration 89/1000 | Loss: 0.00040511
Iteration 90/1000 | Loss: 0.00031226
Iteration 91/1000 | Loss: 0.00031074
Iteration 92/1000 | Loss: 0.00033044
Iteration 93/1000 | Loss: 0.00038792
Iteration 94/1000 | Loss: 0.00031598
Iteration 95/1000 | Loss: 0.00030225
Iteration 96/1000 | Loss: 0.00025975
Iteration 97/1000 | Loss: 0.00027474
Iteration 98/1000 | Loss: 0.00032564
Iteration 99/1000 | Loss: 0.00029579
Iteration 100/1000 | Loss: 0.00039181
Iteration 101/1000 | Loss: 0.00023977
Iteration 102/1000 | Loss: 0.00035936
Iteration 103/1000 | Loss: 0.00033789
Iteration 104/1000 | Loss: 0.00032872
Iteration 105/1000 | Loss: 0.00024873
Iteration 106/1000 | Loss: 0.00034955
Iteration 107/1000 | Loss: 0.00025738
Iteration 108/1000 | Loss: 0.00037022
Iteration 109/1000 | Loss: 0.00043050
Iteration 110/1000 | Loss: 0.00025541
Iteration 111/1000 | Loss: 0.00028177
Iteration 112/1000 | Loss: 0.00035358
Iteration 113/1000 | Loss: 0.00024099
Iteration 114/1000 | Loss: 0.00042415
Iteration 115/1000 | Loss: 0.00038256
Iteration 116/1000 | Loss: 0.00036220
Iteration 117/1000 | Loss: 0.00037909
Iteration 118/1000 | Loss: 0.00036831
Iteration 119/1000 | Loss: 0.00037492
Iteration 120/1000 | Loss: 0.00036516
Iteration 121/1000 | Loss: 0.00033845
Iteration 122/1000 | Loss: 0.00025017
Iteration 123/1000 | Loss: 0.00030339
Iteration 124/1000 | Loss: 0.00028828
Iteration 125/1000 | Loss: 0.00034766
Iteration 126/1000 | Loss: 0.00013004
Iteration 127/1000 | Loss: 0.00022414
Iteration 128/1000 | Loss: 0.00011357
Iteration 129/1000 | Loss: 0.00066859
Iteration 130/1000 | Loss: 0.00051242
Iteration 131/1000 | Loss: 0.00027869
Iteration 132/1000 | Loss: 0.00014952
Iteration 133/1000 | Loss: 0.00020253
Iteration 134/1000 | Loss: 0.00009904
Iteration 135/1000 | Loss: 0.00023713
Iteration 136/1000 | Loss: 0.00021920
Iteration 137/1000 | Loss: 0.00027105
Iteration 138/1000 | Loss: 0.00024754
Iteration 139/1000 | Loss: 0.00022957
Iteration 140/1000 | Loss: 0.00022351
Iteration 141/1000 | Loss: 0.00029422
Iteration 142/1000 | Loss: 0.00018890
Iteration 143/1000 | Loss: 0.00019878
Iteration 144/1000 | Loss: 0.00024322
Iteration 145/1000 | Loss: 0.00027228
Iteration 146/1000 | Loss: 0.00066316
Iteration 147/1000 | Loss: 0.00017240
Iteration 148/1000 | Loss: 0.00016110
Iteration 149/1000 | Loss: 0.00023271
Iteration 150/1000 | Loss: 0.00026896
Iteration 151/1000 | Loss: 0.00020863
Iteration 152/1000 | Loss: 0.00027024
Iteration 153/1000 | Loss: 0.00023671
Iteration 154/1000 | Loss: 0.00027304
Iteration 155/1000 | Loss: 0.00021816
Iteration 156/1000 | Loss: 0.00020346
Iteration 157/1000 | Loss: 0.00033583
Iteration 158/1000 | Loss: 0.00058864
Iteration 159/1000 | Loss: 0.00017694
Iteration 160/1000 | Loss: 0.00007870
Iteration 161/1000 | Loss: 0.00007299
Iteration 162/1000 | Loss: 0.00037655
Iteration 163/1000 | Loss: 0.00029163
Iteration 164/1000 | Loss: 0.00020296
Iteration 165/1000 | Loss: 0.00032812
Iteration 166/1000 | Loss: 0.00008074
Iteration 167/1000 | Loss: 0.00005948
Iteration 168/1000 | Loss: 0.00006714
Iteration 169/1000 | Loss: 0.00004906
Iteration 170/1000 | Loss: 0.00075100
Iteration 171/1000 | Loss: 0.00013654
Iteration 172/1000 | Loss: 0.00063431
Iteration 173/1000 | Loss: 0.00016011
Iteration 174/1000 | Loss: 0.00058566
Iteration 175/1000 | Loss: 0.00037088
Iteration 176/1000 | Loss: 0.00032241
Iteration 177/1000 | Loss: 0.00023277
Iteration 178/1000 | Loss: 0.00028223
Iteration 179/1000 | Loss: 0.00033924
Iteration 180/1000 | Loss: 0.00027524
Iteration 181/1000 | Loss: 0.00034182
Iteration 182/1000 | Loss: 0.00027333
Iteration 183/1000 | Loss: 0.00033850
Iteration 184/1000 | Loss: 0.00038012
Iteration 185/1000 | Loss: 0.00031265
Iteration 186/1000 | Loss: 0.00022061
Iteration 187/1000 | Loss: 0.00004907
Iteration 188/1000 | Loss: 0.00035286
Iteration 189/1000 | Loss: 0.00011233
Iteration 190/1000 | Loss: 0.00048512
Iteration 191/1000 | Loss: 0.00022471
Iteration 192/1000 | Loss: 0.00007311
Iteration 193/1000 | Loss: 0.00064731
Iteration 194/1000 | Loss: 0.00005434
Iteration 195/1000 | Loss: 0.00048521
Iteration 196/1000 | Loss: 0.00005480
Iteration 197/1000 | Loss: 0.00019891
Iteration 198/1000 | Loss: 0.00014383
Iteration 199/1000 | Loss: 0.00029653
Iteration 200/1000 | Loss: 0.00021701
Iteration 201/1000 | Loss: 0.00015529
Iteration 202/1000 | Loss: 0.00006042
Iteration 203/1000 | Loss: 0.00037049
Iteration 204/1000 | Loss: 0.00024162
Iteration 205/1000 | Loss: 0.00004626
Iteration 206/1000 | Loss: 0.00004642
Iteration 207/1000 | Loss: 0.00036237
Iteration 208/1000 | Loss: 0.00033649
Iteration 209/1000 | Loss: 0.00005894
Iteration 210/1000 | Loss: 0.00004427
Iteration 211/1000 | Loss: 0.00005053
Iteration 212/1000 | Loss: 0.00005887
Iteration 213/1000 | Loss: 0.00004283
Iteration 214/1000 | Loss: 0.00003001
Iteration 215/1000 | Loss: 0.00004541
Iteration 216/1000 | Loss: 0.00062690
Iteration 217/1000 | Loss: 0.00042115
Iteration 218/1000 | Loss: 0.00049046
Iteration 219/1000 | Loss: 0.00030778
Iteration 220/1000 | Loss: 0.00004814
Iteration 221/1000 | Loss: 0.00004183
Iteration 222/1000 | Loss: 0.00004847
Iteration 223/1000 | Loss: 0.00005395
Iteration 224/1000 | Loss: 0.00005946
Iteration 225/1000 | Loss: 0.00058115
Iteration 226/1000 | Loss: 0.00011023
Iteration 227/1000 | Loss: 0.00004918
Iteration 228/1000 | Loss: 0.00003830
Iteration 229/1000 | Loss: 0.00002919
Iteration 230/1000 | Loss: 0.00004990
Iteration 231/1000 | Loss: 0.00047691
Iteration 232/1000 | Loss: 0.00050430
Iteration 233/1000 | Loss: 0.00005241
Iteration 234/1000 | Loss: 0.00061806
Iteration 235/1000 | Loss: 0.00027260
Iteration 236/1000 | Loss: 0.00004774
Iteration 237/1000 | Loss: 0.00005038
Iteration 238/1000 | Loss: 0.00004735
Iteration 239/1000 | Loss: 0.00005213
Iteration 240/1000 | Loss: 0.00005381
Iteration 241/1000 | Loss: 0.00060793
Iteration 242/1000 | Loss: 0.00025422
Iteration 243/1000 | Loss: 0.00005730
Iteration 244/1000 | Loss: 0.00005445
Iteration 245/1000 | Loss: 0.00004506
Iteration 246/1000 | Loss: 0.00005558
Iteration 247/1000 | Loss: 0.00005174
Iteration 248/1000 | Loss: 0.00054786
Iteration 249/1000 | Loss: 0.00022571
Iteration 250/1000 | Loss: 0.00007028
Iteration 251/1000 | Loss: 0.00039273
Iteration 252/1000 | Loss: 0.00006389
Iteration 253/1000 | Loss: 0.00005761
Iteration 254/1000 | Loss: 0.00006225
Iteration 255/1000 | Loss: 0.00004605
Iteration 256/1000 | Loss: 0.00005574
Iteration 257/1000 | Loss: 0.00004931
Iteration 258/1000 | Loss: 0.00005708
Iteration 259/1000 | Loss: 0.00004521
Iteration 260/1000 | Loss: 0.00068529
Iteration 261/1000 | Loss: 0.00051100
Iteration 262/1000 | Loss: 0.00045500
Iteration 263/1000 | Loss: 0.00051099
Iteration 264/1000 | Loss: 0.00033486
Iteration 265/1000 | Loss: 0.00042847
Iteration 266/1000 | Loss: 0.00038115
Iteration 267/1000 | Loss: 0.00005329
Iteration 268/1000 | Loss: 0.00003334
Iteration 269/1000 | Loss: 0.00004030
Iteration 270/1000 | Loss: 0.00004156
Iteration 271/1000 | Loss: 0.00004373
Iteration 272/1000 | Loss: 0.00005227
Iteration 273/1000 | Loss: 0.00004529
Iteration 274/1000 | Loss: 0.00004157
Iteration 275/1000 | Loss: 0.00004938
Iteration 276/1000 | Loss: 0.00005332
Iteration 277/1000 | Loss: 0.00004265
Iteration 278/1000 | Loss: 0.00003688
Iteration 279/1000 | Loss: 0.00005081
Iteration 280/1000 | Loss: 0.00005009
Iteration 281/1000 | Loss: 0.00004149
Iteration 282/1000 | Loss: 0.00004365
Iteration 283/1000 | Loss: 0.00004090
Iteration 284/1000 | Loss: 0.00004626
Iteration 285/1000 | Loss: 0.00004743
Iteration 286/1000 | Loss: 0.00005242
Iteration 287/1000 | Loss: 0.00004270
Iteration 288/1000 | Loss: 0.00005934
Iteration 289/1000 | Loss: 0.00005251
Iteration 290/1000 | Loss: 0.00005225
Iteration 291/1000 | Loss: 0.00005532
Iteration 292/1000 | Loss: 0.00005152
Iteration 293/1000 | Loss: 0.00005916
Iteration 294/1000 | Loss: 0.00005112
Iteration 295/1000 | Loss: 0.00002100
Iteration 296/1000 | Loss: 0.00004250
Iteration 297/1000 | Loss: 0.00004846
Iteration 298/1000 | Loss: 0.00005100
Iteration 299/1000 | Loss: 0.00005822
Iteration 300/1000 | Loss: 0.00005411
Iteration 301/1000 | Loss: 0.00005481
Iteration 302/1000 | Loss: 0.00005360
Iteration 303/1000 | Loss: 0.00004567
Iteration 304/1000 | Loss: 0.00003074
Iteration 305/1000 | Loss: 0.00003974
Iteration 306/1000 | Loss: 0.00003991
Iteration 307/1000 | Loss: 0.00004038
Iteration 308/1000 | Loss: 0.00006286
Iteration 309/1000 | Loss: 0.00004013
Iteration 310/1000 | Loss: 0.00005531
Iteration 311/1000 | Loss: 0.00004114
Iteration 312/1000 | Loss: 0.00004928
Iteration 313/1000 | Loss: 0.00003357
Iteration 314/1000 | Loss: 0.00003666
Iteration 315/1000 | Loss: 0.00005268
Iteration 316/1000 | Loss: 0.00004670
Iteration 317/1000 | Loss: 0.00005052
Iteration 318/1000 | Loss: 0.00005517
Iteration 319/1000 | Loss: 0.00004557
Iteration 320/1000 | Loss: 0.00004605
Iteration 321/1000 | Loss: 0.00004394
Iteration 322/1000 | Loss: 0.00004878
Iteration 323/1000 | Loss: 0.00005525
Iteration 324/1000 | Loss: 0.00004475
Iteration 325/1000 | Loss: 0.00003841
Iteration 326/1000 | Loss: 0.00004931
Iteration 327/1000 | Loss: 0.00004021
Iteration 328/1000 | Loss: 0.00004396
Iteration 329/1000 | Loss: 0.00003780
Iteration 330/1000 | Loss: 0.00004280
Iteration 331/1000 | Loss: 0.00003086
Iteration 332/1000 | Loss: 0.00004727
Iteration 333/1000 | Loss: 0.00003788
Iteration 334/1000 | Loss: 0.00004675
Iteration 335/1000 | Loss: 0.00004988
Iteration 336/1000 | Loss: 0.00005181
Iteration 337/1000 | Loss: 0.00005711
Iteration 338/1000 | Loss: 0.00004963
Iteration 339/1000 | Loss: 0.00004299
Iteration 340/1000 | Loss: 0.00004491
Iteration 341/1000 | Loss: 0.00004960
Iteration 342/1000 | Loss: 0.00004541
Iteration 343/1000 | Loss: 0.00005113
Iteration 344/1000 | Loss: 0.00005716
Iteration 345/1000 | Loss: 0.00005229
Iteration 346/1000 | Loss: 0.00005179
Iteration 347/1000 | Loss: 0.00005214
Iteration 348/1000 | Loss: 0.00004790
Iteration 349/1000 | Loss: 0.00005194
Iteration 350/1000 | Loss: 0.00004722
Iteration 351/1000 | Loss: 0.00004679
Iteration 352/1000 | Loss: 0.00004874
Iteration 353/1000 | Loss: 0.00005652
Iteration 354/1000 | Loss: 0.00005203
Iteration 355/1000 | Loss: 0.00005424
Iteration 356/1000 | Loss: 0.00005527
Iteration 357/1000 | Loss: 0.00005402
Iteration 358/1000 | Loss: 0.00005228
Iteration 359/1000 | Loss: 0.00005368
Iteration 360/1000 | Loss: 0.00005294
Iteration 361/1000 | Loss: 0.00005332
Iteration 362/1000 | Loss: 0.00005232
Iteration 363/1000 | Loss: 0.00004970
Iteration 364/1000 | Loss: 0.00006323
Iteration 365/1000 | Loss: 0.00004191
Iteration 366/1000 | Loss: 0.00003690
Iteration 367/1000 | Loss: 0.00004148
Iteration 368/1000 | Loss: 0.00003433
Iteration 369/1000 | Loss: 0.00004123
Iteration 370/1000 | Loss: 0.00004005
Iteration 371/1000 | Loss: 0.00004142
Iteration 372/1000 | Loss: 0.00003389
Iteration 373/1000 | Loss: 0.00005830
Iteration 374/1000 | Loss: 0.00003551
Iteration 375/1000 | Loss: 0.00003314
Iteration 376/1000 | Loss: 0.00004452
Iteration 377/1000 | Loss: 0.00004145
Iteration 378/1000 | Loss: 0.00003837
Iteration 379/1000 | Loss: 0.00003499
Iteration 380/1000 | Loss: 0.00003028
Iteration 381/1000 | Loss: 0.00003866
Iteration 382/1000 | Loss: 0.00003652
Iteration 383/1000 | Loss: 0.00003320
Iteration 384/1000 | Loss: 0.00003995
Iteration 385/1000 | Loss: 0.00003207
Iteration 386/1000 | Loss: 0.00003763
Iteration 387/1000 | Loss: 0.00003877
Iteration 388/1000 | Loss: 0.00004188
Iteration 389/1000 | Loss: 0.00003154
Iteration 390/1000 | Loss: 0.00003758
Iteration 391/1000 | Loss: 0.00003039
Iteration 392/1000 | Loss: 0.00003726
Iteration 393/1000 | Loss: 0.00003792
Iteration 394/1000 | Loss: 0.00004680
Iteration 395/1000 | Loss: 0.00003788
Iteration 396/1000 | Loss: 0.00004380
Iteration 397/1000 | Loss: 0.00003980
Iteration 398/1000 | Loss: 0.00003662
Iteration 399/1000 | Loss: 0.00003350
Iteration 400/1000 | Loss: 0.00003143
Iteration 401/1000 | Loss: 0.00004695
Iteration 402/1000 | Loss: 0.00002683
Iteration 403/1000 | Loss: 0.00001949
Iteration 404/1000 | Loss: 0.00002941
Iteration 405/1000 | Loss: 0.00003107
Iteration 406/1000 | Loss: 0.00001644
Iteration 407/1000 | Loss: 0.00001461
Iteration 408/1000 | Loss: 0.00001399
Iteration 409/1000 | Loss: 0.00001353
Iteration 410/1000 | Loss: 0.00001342
Iteration 411/1000 | Loss: 0.00001326
Iteration 412/1000 | Loss: 0.00001317
Iteration 413/1000 | Loss: 0.00001300
Iteration 414/1000 | Loss: 0.00001283
Iteration 415/1000 | Loss: 0.00001282
Iteration 416/1000 | Loss: 0.00001282
Iteration 417/1000 | Loss: 0.00001280
Iteration 418/1000 | Loss: 0.00001279
Iteration 419/1000 | Loss: 0.00001279
Iteration 420/1000 | Loss: 0.00001279
Iteration 421/1000 | Loss: 0.00001278
Iteration 422/1000 | Loss: 0.00001278
Iteration 423/1000 | Loss: 0.00001278
Iteration 424/1000 | Loss: 0.00001277
Iteration 425/1000 | Loss: 0.00001277
Iteration 426/1000 | Loss: 0.00001277
Iteration 427/1000 | Loss: 0.00001277
Iteration 428/1000 | Loss: 0.00001277
Iteration 429/1000 | Loss: 0.00001276
Iteration 430/1000 | Loss: 0.00001276
Iteration 431/1000 | Loss: 0.00001276
Iteration 432/1000 | Loss: 0.00001276
Iteration 433/1000 | Loss: 0.00001276
Iteration 434/1000 | Loss: 0.00001276
Iteration 435/1000 | Loss: 0.00001276
Iteration 436/1000 | Loss: 0.00001275
Iteration 437/1000 | Loss: 0.00001275
Iteration 438/1000 | Loss: 0.00001275
Iteration 439/1000 | Loss: 0.00001275
Iteration 440/1000 | Loss: 0.00001275
Iteration 441/1000 | Loss: 0.00001274
Iteration 442/1000 | Loss: 0.00001274
Iteration 443/1000 | Loss: 0.00001271
Iteration 444/1000 | Loss: 0.00001270
Iteration 445/1000 | Loss: 0.00001270
Iteration 446/1000 | Loss: 0.00001269
Iteration 447/1000 | Loss: 0.00001268
Iteration 448/1000 | Loss: 0.00001268
Iteration 449/1000 | Loss: 0.00001268
Iteration 450/1000 | Loss: 0.00001267
Iteration 451/1000 | Loss: 0.00001267
Iteration 452/1000 | Loss: 0.00001267
Iteration 453/1000 | Loss: 0.00001267
Iteration 454/1000 | Loss: 0.00001267
Iteration 455/1000 | Loss: 0.00001266
Iteration 456/1000 | Loss: 0.00001266
Iteration 457/1000 | Loss: 0.00001266
Iteration 458/1000 | Loss: 0.00001266
Iteration 459/1000 | Loss: 0.00001265
Iteration 460/1000 | Loss: 0.00001265
Iteration 461/1000 | Loss: 0.00001265
Iteration 462/1000 | Loss: 0.00001265
Iteration 463/1000 | Loss: 0.00001265
Iteration 464/1000 | Loss: 0.00001265
Iteration 465/1000 | Loss: 0.00001264
Iteration 466/1000 | Loss: 0.00001264
Iteration 467/1000 | Loss: 0.00001264
Iteration 468/1000 | Loss: 0.00001264
Iteration 469/1000 | Loss: 0.00001264
Iteration 470/1000 | Loss: 0.00001263
Iteration 471/1000 | Loss: 0.00001263
Iteration 472/1000 | Loss: 0.00001263
Iteration 473/1000 | Loss: 0.00001263
Iteration 474/1000 | Loss: 0.00001263
Iteration 475/1000 | Loss: 0.00001263
Iteration 476/1000 | Loss: 0.00001263
Iteration 477/1000 | Loss: 0.00001263
Iteration 478/1000 | Loss: 0.00001263
Iteration 479/1000 | Loss: 0.00001263
Iteration 480/1000 | Loss: 0.00001263
Iteration 481/1000 | Loss: 0.00001263
Iteration 482/1000 | Loss: 0.00001263
Iteration 483/1000 | Loss: 0.00001263
Iteration 484/1000 | Loss: 0.00001263
Iteration 485/1000 | Loss: 0.00001263
Iteration 486/1000 | Loss: 0.00001263
Iteration 487/1000 | Loss: 0.00001263
Iteration 488/1000 | Loss: 0.00001263
Iteration 489/1000 | Loss: 0.00001263
Iteration 490/1000 | Loss: 0.00001263
Iteration 491/1000 | Loss: 0.00001263
Iteration 492/1000 | Loss: 0.00001263
Iteration 493/1000 | Loss: 0.00001263
Iteration 494/1000 | Loss: 0.00001263
Iteration 495/1000 | Loss: 0.00001263
Iteration 496/1000 | Loss: 0.00001263
Iteration 497/1000 | Loss: 0.00001263
Iteration 498/1000 | Loss: 0.00001263
Iteration 499/1000 | Loss: 0.00001263
Iteration 500/1000 | Loss: 0.00001263
Iteration 501/1000 | Loss: 0.00001263
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 501. Stopping optimization.
Last 5 losses: [1.2628500371647533e-05, 1.2628500371647533e-05, 1.2628500371647533e-05, 1.2628500371647533e-05, 1.2628500371647533e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2628500371647533e-05

Optimization complete. Final v2v error: 3.0086376667022705 mm

Highest mean error: 4.76194953918457 mm for frame 54

Lowest mean error: 2.8000941276550293 mm for frame 105

Saving results

Total time: 721.4910395145416
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01078494
Iteration 2/25 | Loss: 0.00277276
Iteration 3/25 | Loss: 0.00197060
Iteration 4/25 | Loss: 0.00146120
Iteration 5/25 | Loss: 0.00130935
Iteration 6/25 | Loss: 0.00118078
Iteration 7/25 | Loss: 0.00119538
Iteration 8/25 | Loss: 0.00110987
Iteration 9/25 | Loss: 0.00103159
Iteration 10/25 | Loss: 0.00097805
Iteration 11/25 | Loss: 0.00095595
Iteration 12/25 | Loss: 0.00093418
Iteration 13/25 | Loss: 0.00092271
Iteration 14/25 | Loss: 0.00091846
Iteration 15/25 | Loss: 0.00092934
Iteration 16/25 | Loss: 0.00092339
Iteration 17/25 | Loss: 0.00091593
Iteration 18/25 | Loss: 0.00092270
Iteration 19/25 | Loss: 0.00092239
Iteration 20/25 | Loss: 0.00092576
Iteration 21/25 | Loss: 0.00093380
Iteration 22/25 | Loss: 0.00092018
Iteration 23/25 | Loss: 0.00092232
Iteration 24/25 | Loss: 0.00091245
Iteration 25/25 | Loss: 0.00089821

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42510092
Iteration 2/25 | Loss: 0.00486580
Iteration 3/25 | Loss: 0.00238423
Iteration 4/25 | Loss: 0.00238423
Iteration 5/25 | Loss: 0.00238423
Iteration 6/25 | Loss: 0.00238423
Iteration 7/25 | Loss: 0.00238423
Iteration 8/25 | Loss: 0.00238423
Iteration 9/25 | Loss: 0.00238423
Iteration 10/25 | Loss: 0.00238423
Iteration 11/25 | Loss: 0.00238423
Iteration 12/25 | Loss: 0.00238423
Iteration 13/25 | Loss: 0.00238423
Iteration 14/25 | Loss: 0.00238423
Iteration 15/25 | Loss: 0.00238423
Iteration 16/25 | Loss: 0.00238423
Iteration 17/25 | Loss: 0.00238423
Iteration 18/25 | Loss: 0.00238423
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.002384229563176632, 0.002384229563176632, 0.002384229563176632, 0.002384229563176632, 0.002384229563176632]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002384229563176632

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00238423
Iteration 2/1000 | Loss: 0.00124378
Iteration 3/1000 | Loss: 0.00102473
Iteration 4/1000 | Loss: 0.00118554
Iteration 5/1000 | Loss: 0.00065752
Iteration 6/1000 | Loss: 0.00095573
Iteration 7/1000 | Loss: 0.00131736
Iteration 8/1000 | Loss: 0.00084487
Iteration 9/1000 | Loss: 0.00087993
Iteration 10/1000 | Loss: 0.00186061
Iteration 11/1000 | Loss: 0.00409164
Iteration 12/1000 | Loss: 0.00198414
Iteration 13/1000 | Loss: 0.00193548
Iteration 14/1000 | Loss: 0.00106972
Iteration 15/1000 | Loss: 0.00072776
Iteration 16/1000 | Loss: 0.00097519
Iteration 17/1000 | Loss: 0.00328481
Iteration 18/1000 | Loss: 0.00354039
Iteration 19/1000 | Loss: 0.00198155
Iteration 20/1000 | Loss: 0.00046925
Iteration 21/1000 | Loss: 0.00030833
Iteration 22/1000 | Loss: 0.00027396
Iteration 23/1000 | Loss: 0.00034252
Iteration 24/1000 | Loss: 0.00078827
Iteration 25/1000 | Loss: 0.00033849
Iteration 26/1000 | Loss: 0.00018893
Iteration 27/1000 | Loss: 0.00016520
Iteration 28/1000 | Loss: 0.00048854
Iteration 29/1000 | Loss: 0.00158762
Iteration 30/1000 | Loss: 0.00102285
Iteration 31/1000 | Loss: 0.00052390
Iteration 32/1000 | Loss: 0.00043246
Iteration 33/1000 | Loss: 0.00036791
Iteration 34/1000 | Loss: 0.00061027
Iteration 35/1000 | Loss: 0.00099080
Iteration 36/1000 | Loss: 0.00106001
Iteration 37/1000 | Loss: 0.00062379
Iteration 38/1000 | Loss: 0.00083675
Iteration 39/1000 | Loss: 0.00129049
Iteration 40/1000 | Loss: 0.00129182
Iteration 41/1000 | Loss: 0.00151765
Iteration 42/1000 | Loss: 0.00173146
Iteration 43/1000 | Loss: 0.00437760
Iteration 44/1000 | Loss: 0.00114822
Iteration 45/1000 | Loss: 0.00119828
Iteration 46/1000 | Loss: 0.00095519
Iteration 47/1000 | Loss: 0.00041795
Iteration 48/1000 | Loss: 0.00085266
Iteration 49/1000 | Loss: 0.00234650
Iteration 50/1000 | Loss: 0.00071928
Iteration 51/1000 | Loss: 0.00144156
Iteration 52/1000 | Loss: 0.00097856
Iteration 53/1000 | Loss: 0.00093651
Iteration 54/1000 | Loss: 0.00173512
Iteration 55/1000 | Loss: 0.00069038
Iteration 56/1000 | Loss: 0.00077341
Iteration 57/1000 | Loss: 0.00061776
Iteration 58/1000 | Loss: 0.00197715
Iteration 59/1000 | Loss: 0.00084582
Iteration 60/1000 | Loss: 0.00057224
Iteration 61/1000 | Loss: 0.00034818
Iteration 62/1000 | Loss: 0.00053325
Iteration 63/1000 | Loss: 0.00079753
Iteration 64/1000 | Loss: 0.00084789
Iteration 65/1000 | Loss: 0.00186961
Iteration 66/1000 | Loss: 0.00129718
Iteration 67/1000 | Loss: 0.00060515
Iteration 68/1000 | Loss: 0.00216930
Iteration 69/1000 | Loss: 0.00146927
Iteration 70/1000 | Loss: 0.00096517
Iteration 71/1000 | Loss: 0.00083153
Iteration 72/1000 | Loss: 0.00066747
Iteration 73/1000 | Loss: 0.00104643
Iteration 74/1000 | Loss: 0.00099069
Iteration 75/1000 | Loss: 0.00065565
Iteration 76/1000 | Loss: 0.00045059
Iteration 77/1000 | Loss: 0.00075494
Iteration 78/1000 | Loss: 0.00084673
Iteration 79/1000 | Loss: 0.00069207
Iteration 80/1000 | Loss: 0.00080477
Iteration 81/1000 | Loss: 0.00140706
Iteration 82/1000 | Loss: 0.00010027
Iteration 83/1000 | Loss: 0.00009228
Iteration 84/1000 | Loss: 0.00008885
Iteration 85/1000 | Loss: 0.00015867
Iteration 86/1000 | Loss: 0.00078750
Iteration 87/1000 | Loss: 0.00236468
Iteration 88/1000 | Loss: 0.00082672
Iteration 89/1000 | Loss: 0.00036770
Iteration 90/1000 | Loss: 0.00103013
Iteration 91/1000 | Loss: 0.00054724
Iteration 92/1000 | Loss: 0.00035683
Iteration 93/1000 | Loss: 0.00204484
Iteration 94/1000 | Loss: 0.00125575
Iteration 95/1000 | Loss: 0.00204149
Iteration 96/1000 | Loss: 0.00122217
Iteration 97/1000 | Loss: 0.00064179
Iteration 98/1000 | Loss: 0.00035689
Iteration 99/1000 | Loss: 0.00071457
Iteration 100/1000 | Loss: 0.00041057
Iteration 101/1000 | Loss: 0.00027903
Iteration 102/1000 | Loss: 0.00047293
Iteration 103/1000 | Loss: 0.00173839
Iteration 104/1000 | Loss: 0.00064711
Iteration 105/1000 | Loss: 0.00011195
Iteration 106/1000 | Loss: 0.00048667
Iteration 107/1000 | Loss: 0.00062960
Iteration 108/1000 | Loss: 0.00043399
Iteration 109/1000 | Loss: 0.00036668
Iteration 110/1000 | Loss: 0.00027275
Iteration 111/1000 | Loss: 0.00055845
Iteration 112/1000 | Loss: 0.00049410
Iteration 113/1000 | Loss: 0.00030019
Iteration 114/1000 | Loss: 0.00016817
Iteration 115/1000 | Loss: 0.00007758
Iteration 116/1000 | Loss: 0.00063642
Iteration 117/1000 | Loss: 0.00023884
Iteration 118/1000 | Loss: 0.00015962
Iteration 119/1000 | Loss: 0.00015384
Iteration 120/1000 | Loss: 0.00127317
Iteration 121/1000 | Loss: 0.00058579
Iteration 122/1000 | Loss: 0.00044598
Iteration 123/1000 | Loss: 0.00022899
Iteration 124/1000 | Loss: 0.00142586
Iteration 125/1000 | Loss: 0.00075701
Iteration 126/1000 | Loss: 0.00124563
Iteration 127/1000 | Loss: 0.00059561
Iteration 128/1000 | Loss: 0.00080411
Iteration 129/1000 | Loss: 0.00059632
Iteration 130/1000 | Loss: 0.00018489
Iteration 131/1000 | Loss: 0.00008054
Iteration 132/1000 | Loss: 0.00029719
Iteration 133/1000 | Loss: 0.00006288
Iteration 134/1000 | Loss: 0.00018211
Iteration 135/1000 | Loss: 0.00009797
Iteration 136/1000 | Loss: 0.00005912
Iteration 137/1000 | Loss: 0.00004286
Iteration 138/1000 | Loss: 0.00069268
Iteration 139/1000 | Loss: 0.00023691
Iteration 140/1000 | Loss: 0.00021318
Iteration 141/1000 | Loss: 0.00004424
Iteration 142/1000 | Loss: 0.00004221
Iteration 143/1000 | Loss: 0.00006820
Iteration 144/1000 | Loss: 0.00005118
Iteration 145/1000 | Loss: 0.00004291
Iteration 146/1000 | Loss: 0.00008542
Iteration 147/1000 | Loss: 0.00004136
Iteration 148/1000 | Loss: 0.00005415
Iteration 149/1000 | Loss: 0.00005067
Iteration 150/1000 | Loss: 0.00004735
Iteration 151/1000 | Loss: 0.00004624
Iteration 152/1000 | Loss: 0.00004645
Iteration 153/1000 | Loss: 0.00004745
Iteration 154/1000 | Loss: 0.00036768
Iteration 155/1000 | Loss: 0.00014885
Iteration 156/1000 | Loss: 0.00004632
Iteration 157/1000 | Loss: 0.00004920
Iteration 158/1000 | Loss: 0.00005639
Iteration 159/1000 | Loss: 0.00004421
Iteration 160/1000 | Loss: 0.00005086
Iteration 161/1000 | Loss: 0.00026818
Iteration 162/1000 | Loss: 0.00004315
Iteration 163/1000 | Loss: 0.00005959
Iteration 164/1000 | Loss: 0.00004098
Iteration 165/1000 | Loss: 0.00005305
Iteration 166/1000 | Loss: 0.00003958
Iteration 167/1000 | Loss: 0.00003466
Iteration 168/1000 | Loss: 0.00002930
Iteration 169/1000 | Loss: 0.00002741
Iteration 170/1000 | Loss: 0.00002794
Iteration 171/1000 | Loss: 0.00003045
Iteration 172/1000 | Loss: 0.00002978
Iteration 173/1000 | Loss: 0.00002993
Iteration 174/1000 | Loss: 0.00002934
Iteration 175/1000 | Loss: 0.00003068
Iteration 176/1000 | Loss: 0.00002914
Iteration 177/1000 | Loss: 0.00003092
Iteration 178/1000 | Loss: 0.00002593
Iteration 179/1000 | Loss: 0.00002505
Iteration 180/1000 | Loss: 0.00002614
Iteration 181/1000 | Loss: 0.00002946
Iteration 182/1000 | Loss: 0.00002793
Iteration 183/1000 | Loss: 0.00002856
Iteration 184/1000 | Loss: 0.00002899
Iteration 185/1000 | Loss: 0.00002871
Iteration 186/1000 | Loss: 0.00026592
Iteration 187/1000 | Loss: 0.00015885
Iteration 188/1000 | Loss: 0.00014106
Iteration 189/1000 | Loss: 0.00033545
Iteration 190/1000 | Loss: 0.00003073
Iteration 191/1000 | Loss: 0.00005882
Iteration 192/1000 | Loss: 0.00002992
Iteration 193/1000 | Loss: 0.00002695
Iteration 194/1000 | Loss: 0.00002587
Iteration 195/1000 | Loss: 0.00007692
Iteration 196/1000 | Loss: 0.00002512
Iteration 197/1000 | Loss: 0.00024932
Iteration 198/1000 | Loss: 0.00018113
Iteration 199/1000 | Loss: 0.00003936
Iteration 200/1000 | Loss: 0.00006053
Iteration 201/1000 | Loss: 0.00002671
Iteration 202/1000 | Loss: 0.00002683
Iteration 203/1000 | Loss: 0.00002531
Iteration 204/1000 | Loss: 0.00002435
Iteration 205/1000 | Loss: 0.00002443
Iteration 206/1000 | Loss: 0.00002413
Iteration 207/1000 | Loss: 0.00022720
Iteration 208/1000 | Loss: 0.00019786
Iteration 209/1000 | Loss: 0.00002541
Iteration 210/1000 | Loss: 0.00002412
Iteration 211/1000 | Loss: 0.00024568
Iteration 212/1000 | Loss: 0.00014353
Iteration 213/1000 | Loss: 0.00028268
Iteration 214/1000 | Loss: 0.00013234
Iteration 215/1000 | Loss: 0.00002523
Iteration 216/1000 | Loss: 0.00004459
Iteration 217/1000 | Loss: 0.00002416
Iteration 218/1000 | Loss: 0.00002411
Iteration 219/1000 | Loss: 0.00002404
Iteration 220/1000 | Loss: 0.00002397
Iteration 221/1000 | Loss: 0.00002389
Iteration 222/1000 | Loss: 0.00002387
Iteration 223/1000 | Loss: 0.00002386
Iteration 224/1000 | Loss: 0.00002385
Iteration 225/1000 | Loss: 0.00002384
Iteration 226/1000 | Loss: 0.00002383
Iteration 227/1000 | Loss: 0.00023774
Iteration 228/1000 | Loss: 0.00010166
Iteration 229/1000 | Loss: 0.00002702
Iteration 230/1000 | Loss: 0.00003855
Iteration 231/1000 | Loss: 0.00002399
Iteration 232/1000 | Loss: 0.00024573
Iteration 233/1000 | Loss: 0.00009909
Iteration 234/1000 | Loss: 0.00003929
Iteration 235/1000 | Loss: 0.00003117
Iteration 236/1000 | Loss: 0.00002491
Iteration 237/1000 | Loss: 0.00002416
Iteration 238/1000 | Loss: 0.00002445
Iteration 239/1000 | Loss: 0.00002378
Iteration 240/1000 | Loss: 0.00002377
Iteration 241/1000 | Loss: 0.00002377
Iteration 242/1000 | Loss: 0.00002376
Iteration 243/1000 | Loss: 0.00002376
Iteration 244/1000 | Loss: 0.00002376
Iteration 245/1000 | Loss: 0.00002376
Iteration 246/1000 | Loss: 0.00002376
Iteration 247/1000 | Loss: 0.00002376
Iteration 248/1000 | Loss: 0.00002375
Iteration 249/1000 | Loss: 0.00002375
Iteration 250/1000 | Loss: 0.00002374
Iteration 251/1000 | Loss: 0.00002372
Iteration 252/1000 | Loss: 0.00024659
Iteration 253/1000 | Loss: 0.00007503
Iteration 254/1000 | Loss: 0.00003639
Iteration 255/1000 | Loss: 0.00002750
Iteration 256/1000 | Loss: 0.00002508
Iteration 257/1000 | Loss: 0.00002429
Iteration 258/1000 | Loss: 0.00002572
Iteration 259/1000 | Loss: 0.00023562
Iteration 260/1000 | Loss: 0.00008538
Iteration 261/1000 | Loss: 0.00003345
Iteration 262/1000 | Loss: 0.00002863
Iteration 263/1000 | Loss: 0.00003209
Iteration 264/1000 | Loss: 0.00002789
Iteration 265/1000 | Loss: 0.00002410
Iteration 266/1000 | Loss: 0.00002393
Iteration 267/1000 | Loss: 0.00002385
Iteration 268/1000 | Loss: 0.00002374
Iteration 269/1000 | Loss: 0.00026446
Iteration 270/1000 | Loss: 0.00007940
Iteration 271/1000 | Loss: 0.00002467
Iteration 272/1000 | Loss: 0.00026131
Iteration 273/1000 | Loss: 0.00009282
Iteration 274/1000 | Loss: 0.00031171
Iteration 275/1000 | Loss: 0.00027807
Iteration 276/1000 | Loss: 0.00021719
Iteration 277/1000 | Loss: 0.00010334
Iteration 278/1000 | Loss: 0.00003182
Iteration 279/1000 | Loss: 0.00002406
Iteration 280/1000 | Loss: 0.00003162
Iteration 281/1000 | Loss: 0.00002380
Iteration 282/1000 | Loss: 0.00002751
Iteration 283/1000 | Loss: 0.00002377
Iteration 284/1000 | Loss: 0.00002900
Iteration 285/1000 | Loss: 0.00002371
Iteration 286/1000 | Loss: 0.00002371
Iteration 287/1000 | Loss: 0.00002371
Iteration 288/1000 | Loss: 0.00002371
Iteration 289/1000 | Loss: 0.00002371
Iteration 290/1000 | Loss: 0.00002371
Iteration 291/1000 | Loss: 0.00002371
Iteration 292/1000 | Loss: 0.00002371
Iteration 293/1000 | Loss: 0.00002371
Iteration 294/1000 | Loss: 0.00002371
Iteration 295/1000 | Loss: 0.00002370
Iteration 296/1000 | Loss: 0.00002368
Iteration 297/1000 | Loss: 0.00002368
Iteration 298/1000 | Loss: 0.00002366
Iteration 299/1000 | Loss: 0.00024321
Iteration 300/1000 | Loss: 0.00006358
Iteration 301/1000 | Loss: 0.00003322
Iteration 302/1000 | Loss: 0.00010434
Iteration 303/1000 | Loss: 0.00004771
Iteration 304/1000 | Loss: 0.00002432
Iteration 305/1000 | Loss: 0.00003050
Iteration 306/1000 | Loss: 0.00002386
Iteration 307/1000 | Loss: 0.00002383
Iteration 308/1000 | Loss: 0.00002503
Iteration 309/1000 | Loss: 0.00002373
Iteration 310/1000 | Loss: 0.00002372
Iteration 311/1000 | Loss: 0.00002372
Iteration 312/1000 | Loss: 0.00003001
Iteration 313/1000 | Loss: 0.00002371
Iteration 314/1000 | Loss: 0.00002369
Iteration 315/1000 | Loss: 0.00024204
Iteration 316/1000 | Loss: 0.00004428
Iteration 317/1000 | Loss: 0.00003474
Iteration 318/1000 | Loss: 0.00002868
Iteration 319/1000 | Loss: 0.00002576
Iteration 320/1000 | Loss: 0.00005036
Iteration 321/1000 | Loss: 0.00003028
Iteration 322/1000 | Loss: 0.00002381
Iteration 323/1000 | Loss: 0.00004931
Iteration 324/1000 | Loss: 0.00002363
Iteration 325/1000 | Loss: 0.00002536
Iteration 326/1000 | Loss: 0.00002295
Iteration 327/1000 | Loss: 0.00002295
Iteration 328/1000 | Loss: 0.00002295
Iteration 329/1000 | Loss: 0.00002294
Iteration 330/1000 | Loss: 0.00002294
Iteration 331/1000 | Loss: 0.00002294
Iteration 332/1000 | Loss: 0.00002294
Iteration 333/1000 | Loss: 0.00002294
Iteration 334/1000 | Loss: 0.00002294
Iteration 335/1000 | Loss: 0.00002294
Iteration 336/1000 | Loss: 0.00002294
Iteration 337/1000 | Loss: 0.00002294
Iteration 338/1000 | Loss: 0.00002294
Iteration 339/1000 | Loss: 0.00002294
Iteration 340/1000 | Loss: 0.00002294
Iteration 341/1000 | Loss: 0.00002294
Iteration 342/1000 | Loss: 0.00002293
Iteration 343/1000 | Loss: 0.00002293
Iteration 344/1000 | Loss: 0.00002293
Iteration 345/1000 | Loss: 0.00002293
Iteration 346/1000 | Loss: 0.00002293
Iteration 347/1000 | Loss: 0.00002293
Iteration 348/1000 | Loss: 0.00002292
Iteration 349/1000 | Loss: 0.00002292
Iteration 350/1000 | Loss: 0.00002291
Iteration 351/1000 | Loss: 0.00002291
Iteration 352/1000 | Loss: 0.00002290
Iteration 353/1000 | Loss: 0.00002287
Iteration 354/1000 | Loss: 0.00002287
Iteration 355/1000 | Loss: 0.00002287
Iteration 356/1000 | Loss: 0.00002286
Iteration 357/1000 | Loss: 0.00002286
Iteration 358/1000 | Loss: 0.00002285
Iteration 359/1000 | Loss: 0.00002285
Iteration 360/1000 | Loss: 0.00002284
Iteration 361/1000 | Loss: 0.00002284
Iteration 362/1000 | Loss: 0.00002284
Iteration 363/1000 | Loss: 0.00002284
Iteration 364/1000 | Loss: 0.00002284
Iteration 365/1000 | Loss: 0.00002283
Iteration 366/1000 | Loss: 0.00002283
Iteration 367/1000 | Loss: 0.00002283
Iteration 368/1000 | Loss: 0.00002283
Iteration 369/1000 | Loss: 0.00002283
Iteration 370/1000 | Loss: 0.00002283
Iteration 371/1000 | Loss: 0.00002283
Iteration 372/1000 | Loss: 0.00002282
Iteration 373/1000 | Loss: 0.00002282
Iteration 374/1000 | Loss: 0.00002282
Iteration 375/1000 | Loss: 0.00002282
Iteration 376/1000 | Loss: 0.00002281
Iteration 377/1000 | Loss: 0.00002281
Iteration 378/1000 | Loss: 0.00002280
Iteration 379/1000 | Loss: 0.00002280
Iteration 380/1000 | Loss: 0.00002471
Iteration 381/1000 | Loss: 0.00002271
Iteration 382/1000 | Loss: 0.00002271
Iteration 383/1000 | Loss: 0.00002271
Iteration 384/1000 | Loss: 0.00002271
Iteration 385/1000 | Loss: 0.00002271
Iteration 386/1000 | Loss: 0.00002271
Iteration 387/1000 | Loss: 0.00002271
Iteration 388/1000 | Loss: 0.00002270
Iteration 389/1000 | Loss: 0.00002270
Iteration 390/1000 | Loss: 0.00002270
Iteration 391/1000 | Loss: 0.00002270
Iteration 392/1000 | Loss: 0.00002270
Iteration 393/1000 | Loss: 0.00002270
Iteration 394/1000 | Loss: 0.00002270
Iteration 395/1000 | Loss: 0.00002270
Iteration 396/1000 | Loss: 0.00002270
Iteration 397/1000 | Loss: 0.00002270
Iteration 398/1000 | Loss: 0.00002270
Iteration 399/1000 | Loss: 0.00002270
Iteration 400/1000 | Loss: 0.00002270
Iteration 401/1000 | Loss: 0.00002270
Iteration 402/1000 | Loss: 0.00002270
Iteration 403/1000 | Loss: 0.00002270
Iteration 404/1000 | Loss: 0.00002270
Iteration 405/1000 | Loss: 0.00002270
Iteration 406/1000 | Loss: 0.00002270
Iteration 407/1000 | Loss: 0.00002270
Iteration 408/1000 | Loss: 0.00002270
Iteration 409/1000 | Loss: 0.00002270
Iteration 410/1000 | Loss: 0.00002269
Iteration 411/1000 | Loss: 0.00002269
Iteration 412/1000 | Loss: 0.00002269
Iteration 413/1000 | Loss: 0.00002269
Iteration 414/1000 | Loss: 0.00002269
Iteration 415/1000 | Loss: 0.00002269
Iteration 416/1000 | Loss: 0.00002269
Iteration 417/1000 | Loss: 0.00002269
Iteration 418/1000 | Loss: 0.00002269
Iteration 419/1000 | Loss: 0.00002269
Iteration 420/1000 | Loss: 0.00002269
Iteration 421/1000 | Loss: 0.00002269
Iteration 422/1000 | Loss: 0.00002269
Iteration 423/1000 | Loss: 0.00002268
Iteration 424/1000 | Loss: 0.00002268
Iteration 425/1000 | Loss: 0.00002268
Iteration 426/1000 | Loss: 0.00002268
Iteration 427/1000 | Loss: 0.00002268
Iteration 428/1000 | Loss: 0.00002268
Iteration 429/1000 | Loss: 0.00002268
Iteration 430/1000 | Loss: 0.00002268
Iteration 431/1000 | Loss: 0.00002268
Iteration 432/1000 | Loss: 0.00002268
Iteration 433/1000 | Loss: 0.00002268
Iteration 434/1000 | Loss: 0.00002268
Iteration 435/1000 | Loss: 0.00002268
Iteration 436/1000 | Loss: 0.00002268
Iteration 437/1000 | Loss: 0.00002268
Iteration 438/1000 | Loss: 0.00002268
Iteration 439/1000 | Loss: 0.00002268
Iteration 440/1000 | Loss: 0.00002268
Iteration 441/1000 | Loss: 0.00002268
Iteration 442/1000 | Loss: 0.00002268
Iteration 443/1000 | Loss: 0.00002268
Iteration 444/1000 | Loss: 0.00002267
Iteration 445/1000 | Loss: 0.00002267
Iteration 446/1000 | Loss: 0.00002267
Iteration 447/1000 | Loss: 0.00002267
Iteration 448/1000 | Loss: 0.00002267
Iteration 449/1000 | Loss: 0.00002267
Iteration 450/1000 | Loss: 0.00002267
Iteration 451/1000 | Loss: 0.00002267
Iteration 452/1000 | Loss: 0.00002267
Iteration 453/1000 | Loss: 0.00002267
Iteration 454/1000 | Loss: 0.00002267
Iteration 455/1000 | Loss: 0.00002267
Iteration 456/1000 | Loss: 0.00002267
Iteration 457/1000 | Loss: 0.00002267
Iteration 458/1000 | Loss: 0.00002267
Iteration 459/1000 | Loss: 0.00002267
Iteration 460/1000 | Loss: 0.00002267
Iteration 461/1000 | Loss: 0.00002267
Iteration 462/1000 | Loss: 0.00002267
Iteration 463/1000 | Loss: 0.00002267
Iteration 464/1000 | Loss: 0.00002267
Iteration 465/1000 | Loss: 0.00002267
Iteration 466/1000 | Loss: 0.00002267
Iteration 467/1000 | Loss: 0.00002267
Iteration 468/1000 | Loss: 0.00002267
Iteration 469/1000 | Loss: 0.00002267
Iteration 470/1000 | Loss: 0.00002267
Iteration 471/1000 | Loss: 0.00002267
Iteration 472/1000 | Loss: 0.00002267
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 472. Stopping optimization.
Last 5 losses: [2.2674383217236027e-05, 2.2674383217236027e-05, 2.2674383217236027e-05, 2.2674383217236027e-05, 2.2674383217236027e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2674383217236027e-05

Optimization complete. Final v2v error: 3.722722291946411 mm

Highest mean error: 14.395666122436523 mm for frame 202

Lowest mean error: 3.191196918487549 mm for frame 177

Saving results

Total time: 510.5751760005951
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00399874
Iteration 2/25 | Loss: 0.00081173
Iteration 3/25 | Loss: 0.00070660
Iteration 4/25 | Loss: 0.00068630
Iteration 5/25 | Loss: 0.00067931
Iteration 6/25 | Loss: 0.00067793
Iteration 7/25 | Loss: 0.00067763
Iteration 8/25 | Loss: 0.00067763
Iteration 9/25 | Loss: 0.00067760
Iteration 10/25 | Loss: 0.00067760
Iteration 11/25 | Loss: 0.00067760
Iteration 12/25 | Loss: 0.00067760
Iteration 13/25 | Loss: 0.00067760
Iteration 14/25 | Loss: 0.00067760
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0006775973015464842, 0.0006775973015464842, 0.0006775973015464842, 0.0006775973015464842, 0.0006775973015464842]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006775973015464842

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.42012000
Iteration 2/25 | Loss: 0.00027557
Iteration 3/25 | Loss: 0.00027555
Iteration 4/25 | Loss: 0.00027555
Iteration 5/25 | Loss: 0.00027555
Iteration 6/25 | Loss: 0.00027555
Iteration 7/25 | Loss: 0.00027555
Iteration 8/25 | Loss: 0.00027555
Iteration 9/25 | Loss: 0.00027555
Iteration 10/25 | Loss: 0.00027555
Iteration 11/25 | Loss: 0.00027555
Iteration 12/25 | Loss: 0.00027555
Iteration 13/25 | Loss: 0.00027555
Iteration 14/25 | Loss: 0.00027555
Iteration 15/25 | Loss: 0.00027555
Iteration 16/25 | Loss: 0.00027555
Iteration 17/25 | Loss: 0.00027555
Iteration 18/25 | Loss: 0.00027555
Iteration 19/25 | Loss: 0.00027555
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00027555087581276894, 0.00027555087581276894, 0.00027555087581276894, 0.00027555087581276894, 0.00027555087581276894]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00027555087581276894

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027555
Iteration 2/1000 | Loss: 0.00003598
Iteration 3/1000 | Loss: 0.00002606
Iteration 4/1000 | Loss: 0.00002453
Iteration 5/1000 | Loss: 0.00002343
Iteration 6/1000 | Loss: 0.00002284
Iteration 7/1000 | Loss: 0.00002226
Iteration 8/1000 | Loss: 0.00002196
Iteration 9/1000 | Loss: 0.00002175
Iteration 10/1000 | Loss: 0.00002167
Iteration 11/1000 | Loss: 0.00002159
Iteration 12/1000 | Loss: 0.00002155
Iteration 13/1000 | Loss: 0.00002153
Iteration 14/1000 | Loss: 0.00002151
Iteration 15/1000 | Loss: 0.00002151
Iteration 16/1000 | Loss: 0.00002148
Iteration 17/1000 | Loss: 0.00002144
Iteration 18/1000 | Loss: 0.00002139
Iteration 19/1000 | Loss: 0.00002139
Iteration 20/1000 | Loss: 0.00002138
Iteration 21/1000 | Loss: 0.00002133
Iteration 22/1000 | Loss: 0.00002131
Iteration 23/1000 | Loss: 0.00002128
Iteration 24/1000 | Loss: 0.00002128
Iteration 25/1000 | Loss: 0.00002128
Iteration 26/1000 | Loss: 0.00002128
Iteration 27/1000 | Loss: 0.00002127
Iteration 28/1000 | Loss: 0.00002127
Iteration 29/1000 | Loss: 0.00002125
Iteration 30/1000 | Loss: 0.00002124
Iteration 31/1000 | Loss: 0.00002124
Iteration 32/1000 | Loss: 0.00002124
Iteration 33/1000 | Loss: 0.00002123
Iteration 34/1000 | Loss: 0.00002123
Iteration 35/1000 | Loss: 0.00002122
Iteration 36/1000 | Loss: 0.00002120
Iteration 37/1000 | Loss: 0.00002115
Iteration 38/1000 | Loss: 0.00002114
Iteration 39/1000 | Loss: 0.00002113
Iteration 40/1000 | Loss: 0.00002113
Iteration 41/1000 | Loss: 0.00002110
Iteration 42/1000 | Loss: 0.00002110
Iteration 43/1000 | Loss: 0.00002109
Iteration 44/1000 | Loss: 0.00002109
Iteration 45/1000 | Loss: 0.00002109
Iteration 46/1000 | Loss: 0.00002109
Iteration 47/1000 | Loss: 0.00002109
Iteration 48/1000 | Loss: 0.00002108
Iteration 49/1000 | Loss: 0.00002107
Iteration 50/1000 | Loss: 0.00002107
Iteration 51/1000 | Loss: 0.00002106
Iteration 52/1000 | Loss: 0.00002106
Iteration 53/1000 | Loss: 0.00002106
Iteration 54/1000 | Loss: 0.00002106
Iteration 55/1000 | Loss: 0.00002105
Iteration 56/1000 | Loss: 0.00002105
Iteration 57/1000 | Loss: 0.00002105
Iteration 58/1000 | Loss: 0.00002105
Iteration 59/1000 | Loss: 0.00002105
Iteration 60/1000 | Loss: 0.00002105
Iteration 61/1000 | Loss: 0.00002104
Iteration 62/1000 | Loss: 0.00002104
Iteration 63/1000 | Loss: 0.00002104
Iteration 64/1000 | Loss: 0.00002104
Iteration 65/1000 | Loss: 0.00002104
Iteration 66/1000 | Loss: 0.00002103
Iteration 67/1000 | Loss: 0.00002103
Iteration 68/1000 | Loss: 0.00002103
Iteration 69/1000 | Loss: 0.00002103
Iteration 70/1000 | Loss: 0.00002103
Iteration 71/1000 | Loss: 0.00002103
Iteration 72/1000 | Loss: 0.00002103
Iteration 73/1000 | Loss: 0.00002103
Iteration 74/1000 | Loss: 0.00002103
Iteration 75/1000 | Loss: 0.00002103
Iteration 76/1000 | Loss: 0.00002103
Iteration 77/1000 | Loss: 0.00002102
Iteration 78/1000 | Loss: 0.00002102
Iteration 79/1000 | Loss: 0.00002102
Iteration 80/1000 | Loss: 0.00002102
Iteration 81/1000 | Loss: 0.00002102
Iteration 82/1000 | Loss: 0.00002102
Iteration 83/1000 | Loss: 0.00002102
Iteration 84/1000 | Loss: 0.00002102
Iteration 85/1000 | Loss: 0.00002102
Iteration 86/1000 | Loss: 0.00002102
Iteration 87/1000 | Loss: 0.00002102
Iteration 88/1000 | Loss: 0.00002102
Iteration 89/1000 | Loss: 0.00002101
Iteration 90/1000 | Loss: 0.00002101
Iteration 91/1000 | Loss: 0.00002101
Iteration 92/1000 | Loss: 0.00002101
Iteration 93/1000 | Loss: 0.00002101
Iteration 94/1000 | Loss: 0.00002101
Iteration 95/1000 | Loss: 0.00002101
Iteration 96/1000 | Loss: 0.00002101
Iteration 97/1000 | Loss: 0.00002101
Iteration 98/1000 | Loss: 0.00002101
Iteration 99/1000 | Loss: 0.00002101
Iteration 100/1000 | Loss: 0.00002100
Iteration 101/1000 | Loss: 0.00002100
Iteration 102/1000 | Loss: 0.00002100
Iteration 103/1000 | Loss: 0.00002100
Iteration 104/1000 | Loss: 0.00002100
Iteration 105/1000 | Loss: 0.00002099
Iteration 106/1000 | Loss: 0.00002099
Iteration 107/1000 | Loss: 0.00002099
Iteration 108/1000 | Loss: 0.00002099
Iteration 109/1000 | Loss: 0.00002099
Iteration 110/1000 | Loss: 0.00002099
Iteration 111/1000 | Loss: 0.00002099
Iteration 112/1000 | Loss: 0.00002099
Iteration 113/1000 | Loss: 0.00002099
Iteration 114/1000 | Loss: 0.00002099
Iteration 115/1000 | Loss: 0.00002098
Iteration 116/1000 | Loss: 0.00002098
Iteration 117/1000 | Loss: 0.00002098
Iteration 118/1000 | Loss: 0.00002098
Iteration 119/1000 | Loss: 0.00002098
Iteration 120/1000 | Loss: 0.00002098
Iteration 121/1000 | Loss: 0.00002098
Iteration 122/1000 | Loss: 0.00002097
Iteration 123/1000 | Loss: 0.00002097
Iteration 124/1000 | Loss: 0.00002097
Iteration 125/1000 | Loss: 0.00002097
Iteration 126/1000 | Loss: 0.00002097
Iteration 127/1000 | Loss: 0.00002097
Iteration 128/1000 | Loss: 0.00002097
Iteration 129/1000 | Loss: 0.00002097
Iteration 130/1000 | Loss: 0.00002097
Iteration 131/1000 | Loss: 0.00002097
Iteration 132/1000 | Loss: 0.00002097
Iteration 133/1000 | Loss: 0.00002097
Iteration 134/1000 | Loss: 0.00002097
Iteration 135/1000 | Loss: 0.00002097
Iteration 136/1000 | Loss: 0.00002097
Iteration 137/1000 | Loss: 0.00002097
Iteration 138/1000 | Loss: 0.00002097
Iteration 139/1000 | Loss: 0.00002097
Iteration 140/1000 | Loss: 0.00002097
Iteration 141/1000 | Loss: 0.00002097
Iteration 142/1000 | Loss: 0.00002097
Iteration 143/1000 | Loss: 0.00002097
Iteration 144/1000 | Loss: 0.00002097
Iteration 145/1000 | Loss: 0.00002097
Iteration 146/1000 | Loss: 0.00002097
Iteration 147/1000 | Loss: 0.00002097
Iteration 148/1000 | Loss: 0.00002097
Iteration 149/1000 | Loss: 0.00002097
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [2.0969213437638246e-05, 2.0969213437638246e-05, 2.0969213437638246e-05, 2.0969213437638246e-05, 2.0969213437638246e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0969213437638246e-05

Optimization complete. Final v2v error: 3.774688482284546 mm

Highest mean error: 4.377722263336182 mm for frame 120

Lowest mean error: 3.381277084350586 mm for frame 68

Saving results

Total time: 37.65697956085205
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00811067
Iteration 2/25 | Loss: 0.00106049
Iteration 3/25 | Loss: 0.00078184
Iteration 4/25 | Loss: 0.00071194
Iteration 5/25 | Loss: 0.00069432
Iteration 6/25 | Loss: 0.00068955
Iteration 7/25 | Loss: 0.00068912
Iteration 8/25 | Loss: 0.00068912
Iteration 9/25 | Loss: 0.00068912
Iteration 10/25 | Loss: 0.00068912
Iteration 11/25 | Loss: 0.00068912
Iteration 12/25 | Loss: 0.00068912
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000689116190187633, 0.000689116190187633, 0.000689116190187633, 0.000689116190187633, 0.000689116190187633]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000689116190187633

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45556283
Iteration 2/25 | Loss: 0.00036445
Iteration 3/25 | Loss: 0.00036444
Iteration 4/25 | Loss: 0.00036444
Iteration 5/25 | Loss: 0.00036444
Iteration 6/25 | Loss: 0.00036444
Iteration 7/25 | Loss: 0.00036444
Iteration 8/25 | Loss: 0.00036444
Iteration 9/25 | Loss: 0.00036444
Iteration 10/25 | Loss: 0.00036444
Iteration 11/25 | Loss: 0.00036444
Iteration 12/25 | Loss: 0.00036444
Iteration 13/25 | Loss: 0.00036444
Iteration 14/25 | Loss: 0.00036444
Iteration 15/25 | Loss: 0.00036444
Iteration 16/25 | Loss: 0.00036444
Iteration 17/25 | Loss: 0.00036444
Iteration 18/25 | Loss: 0.00036444
Iteration 19/25 | Loss: 0.00036444
Iteration 20/25 | Loss: 0.00036444
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00036443694261834025, 0.00036443694261834025, 0.00036443694261834025, 0.00036443694261834025, 0.00036443694261834025]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00036443694261834025

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036444
Iteration 2/1000 | Loss: 0.00003470
Iteration 3/1000 | Loss: 0.00002596
Iteration 4/1000 | Loss: 0.00002431
Iteration 5/1000 | Loss: 0.00002304
Iteration 6/1000 | Loss: 0.00002227
Iteration 7/1000 | Loss: 0.00002156
Iteration 8/1000 | Loss: 0.00002112
Iteration 9/1000 | Loss: 0.00002079
Iteration 10/1000 | Loss: 0.00002059
Iteration 11/1000 | Loss: 0.00002052
Iteration 12/1000 | Loss: 0.00002038
Iteration 13/1000 | Loss: 0.00002030
Iteration 14/1000 | Loss: 0.00002028
Iteration 15/1000 | Loss: 0.00002027
Iteration 16/1000 | Loss: 0.00002025
Iteration 17/1000 | Loss: 0.00002024
Iteration 18/1000 | Loss: 0.00002024
Iteration 19/1000 | Loss: 0.00002024
Iteration 20/1000 | Loss: 0.00002024
Iteration 21/1000 | Loss: 0.00002023
Iteration 22/1000 | Loss: 0.00002023
Iteration 23/1000 | Loss: 0.00002023
Iteration 24/1000 | Loss: 0.00002023
Iteration 25/1000 | Loss: 0.00002023
Iteration 26/1000 | Loss: 0.00002023
Iteration 27/1000 | Loss: 0.00002022
Iteration 28/1000 | Loss: 0.00002022
Iteration 29/1000 | Loss: 0.00002022
Iteration 30/1000 | Loss: 0.00002022
Iteration 31/1000 | Loss: 0.00002021
Iteration 32/1000 | Loss: 0.00002021
Iteration 33/1000 | Loss: 0.00002021
Iteration 34/1000 | Loss: 0.00002021
Iteration 35/1000 | Loss: 0.00002021
Iteration 36/1000 | Loss: 0.00002021
Iteration 37/1000 | Loss: 0.00002021
Iteration 38/1000 | Loss: 0.00002021
Iteration 39/1000 | Loss: 0.00002020
Iteration 40/1000 | Loss: 0.00002020
Iteration 41/1000 | Loss: 0.00002020
Iteration 42/1000 | Loss: 0.00002020
Iteration 43/1000 | Loss: 0.00002020
Iteration 44/1000 | Loss: 0.00002019
Iteration 45/1000 | Loss: 0.00002019
Iteration 46/1000 | Loss: 0.00002019
Iteration 47/1000 | Loss: 0.00002018
Iteration 48/1000 | Loss: 0.00002018
Iteration 49/1000 | Loss: 0.00002018
Iteration 50/1000 | Loss: 0.00002018
Iteration 51/1000 | Loss: 0.00002017
Iteration 52/1000 | Loss: 0.00002017
Iteration 53/1000 | Loss: 0.00002016
Iteration 54/1000 | Loss: 0.00002016
Iteration 55/1000 | Loss: 0.00002016
Iteration 56/1000 | Loss: 0.00002016
Iteration 57/1000 | Loss: 0.00002015
Iteration 58/1000 | Loss: 0.00002015
Iteration 59/1000 | Loss: 0.00002015
Iteration 60/1000 | Loss: 0.00002014
Iteration 61/1000 | Loss: 0.00002014
Iteration 62/1000 | Loss: 0.00002014
Iteration 63/1000 | Loss: 0.00002013
Iteration 64/1000 | Loss: 0.00002013
Iteration 65/1000 | Loss: 0.00002012
Iteration 66/1000 | Loss: 0.00002012
Iteration 67/1000 | Loss: 0.00002012
Iteration 68/1000 | Loss: 0.00002012
Iteration 69/1000 | Loss: 0.00002012
Iteration 70/1000 | Loss: 0.00002011
Iteration 71/1000 | Loss: 0.00002011
Iteration 72/1000 | Loss: 0.00002011
Iteration 73/1000 | Loss: 0.00002011
Iteration 74/1000 | Loss: 0.00002009
Iteration 75/1000 | Loss: 0.00002009
Iteration 76/1000 | Loss: 0.00002009
Iteration 77/1000 | Loss: 0.00002009
Iteration 78/1000 | Loss: 0.00002009
Iteration 79/1000 | Loss: 0.00002009
Iteration 80/1000 | Loss: 0.00002009
Iteration 81/1000 | Loss: 0.00002009
Iteration 82/1000 | Loss: 0.00002009
Iteration 83/1000 | Loss: 0.00002009
Iteration 84/1000 | Loss: 0.00002009
Iteration 85/1000 | Loss: 0.00002008
Iteration 86/1000 | Loss: 0.00002008
Iteration 87/1000 | Loss: 0.00002008
Iteration 88/1000 | Loss: 0.00002008
Iteration 89/1000 | Loss: 0.00002008
Iteration 90/1000 | Loss: 0.00002007
Iteration 91/1000 | Loss: 0.00002007
Iteration 92/1000 | Loss: 0.00002007
Iteration 93/1000 | Loss: 0.00002006
Iteration 94/1000 | Loss: 0.00002006
Iteration 95/1000 | Loss: 0.00002006
Iteration 96/1000 | Loss: 0.00002006
Iteration 97/1000 | Loss: 0.00002006
Iteration 98/1000 | Loss: 0.00002006
Iteration 99/1000 | Loss: 0.00002006
Iteration 100/1000 | Loss: 0.00002006
Iteration 101/1000 | Loss: 0.00002006
Iteration 102/1000 | Loss: 0.00002005
Iteration 103/1000 | Loss: 0.00002005
Iteration 104/1000 | Loss: 0.00002005
Iteration 105/1000 | Loss: 0.00002004
Iteration 106/1000 | Loss: 0.00002004
Iteration 107/1000 | Loss: 0.00002004
Iteration 108/1000 | Loss: 0.00002004
Iteration 109/1000 | Loss: 0.00002004
Iteration 110/1000 | Loss: 0.00002004
Iteration 111/1000 | Loss: 0.00002004
Iteration 112/1000 | Loss: 0.00002004
Iteration 113/1000 | Loss: 0.00002004
Iteration 114/1000 | Loss: 0.00002003
Iteration 115/1000 | Loss: 0.00002003
Iteration 116/1000 | Loss: 0.00002003
Iteration 117/1000 | Loss: 0.00002003
Iteration 118/1000 | Loss: 0.00002002
Iteration 119/1000 | Loss: 0.00002002
Iteration 120/1000 | Loss: 0.00002002
Iteration 121/1000 | Loss: 0.00002002
Iteration 122/1000 | Loss: 0.00002002
Iteration 123/1000 | Loss: 0.00002002
Iteration 124/1000 | Loss: 0.00002002
Iteration 125/1000 | Loss: 0.00002002
Iteration 126/1000 | Loss: 0.00002002
Iteration 127/1000 | Loss: 0.00002002
Iteration 128/1000 | Loss: 0.00002002
Iteration 129/1000 | Loss: 0.00002002
Iteration 130/1000 | Loss: 0.00002002
Iteration 131/1000 | Loss: 0.00002002
Iteration 132/1000 | Loss: 0.00002002
Iteration 133/1000 | Loss: 0.00002002
Iteration 134/1000 | Loss: 0.00002002
Iteration 135/1000 | Loss: 0.00002002
Iteration 136/1000 | Loss: 0.00002002
Iteration 137/1000 | Loss: 0.00002002
Iteration 138/1000 | Loss: 0.00002002
Iteration 139/1000 | Loss: 0.00002002
Iteration 140/1000 | Loss: 0.00002002
Iteration 141/1000 | Loss: 0.00002002
Iteration 142/1000 | Loss: 0.00002002
Iteration 143/1000 | Loss: 0.00002002
Iteration 144/1000 | Loss: 0.00002002
Iteration 145/1000 | Loss: 0.00002002
Iteration 146/1000 | Loss: 0.00002002
Iteration 147/1000 | Loss: 0.00002002
Iteration 148/1000 | Loss: 0.00002002
Iteration 149/1000 | Loss: 0.00002002
Iteration 150/1000 | Loss: 0.00002002
Iteration 151/1000 | Loss: 0.00002002
Iteration 152/1000 | Loss: 0.00002002
Iteration 153/1000 | Loss: 0.00002002
Iteration 154/1000 | Loss: 0.00002002
Iteration 155/1000 | Loss: 0.00002002
Iteration 156/1000 | Loss: 0.00002002
Iteration 157/1000 | Loss: 0.00002002
Iteration 158/1000 | Loss: 0.00002002
Iteration 159/1000 | Loss: 0.00002002
Iteration 160/1000 | Loss: 0.00002002
Iteration 161/1000 | Loss: 0.00002002
Iteration 162/1000 | Loss: 0.00002002
Iteration 163/1000 | Loss: 0.00002002
Iteration 164/1000 | Loss: 0.00002002
Iteration 165/1000 | Loss: 0.00002002
Iteration 166/1000 | Loss: 0.00002002
Iteration 167/1000 | Loss: 0.00002002
Iteration 168/1000 | Loss: 0.00002002
Iteration 169/1000 | Loss: 0.00002002
Iteration 170/1000 | Loss: 0.00002002
Iteration 171/1000 | Loss: 0.00002002
Iteration 172/1000 | Loss: 0.00002002
Iteration 173/1000 | Loss: 0.00002002
Iteration 174/1000 | Loss: 0.00002002
Iteration 175/1000 | Loss: 0.00002002
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [2.0018687791889533e-05, 2.0018687791889533e-05, 2.0018687791889533e-05, 2.0018687791889533e-05, 2.0018687791889533e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0018687791889533e-05

Optimization complete. Final v2v error: 3.7678511142730713 mm

Highest mean error: 4.4921979904174805 mm for frame 156

Lowest mean error: 3.3580591678619385 mm for frame 236

Saving results

Total time: 41.4986526966095
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00351127
Iteration 2/25 | Loss: 0.00066778
Iteration 3/25 | Loss: 0.00057841
Iteration 4/25 | Loss: 0.00056463
Iteration 5/25 | Loss: 0.00056024
Iteration 6/25 | Loss: 0.00055916
Iteration 7/25 | Loss: 0.00055889
Iteration 8/25 | Loss: 0.00055889
Iteration 9/25 | Loss: 0.00055889
Iteration 10/25 | Loss: 0.00055889
Iteration 11/25 | Loss: 0.00055889
Iteration 12/25 | Loss: 0.00055889
Iteration 13/25 | Loss: 0.00055889
Iteration 14/25 | Loss: 0.00055889
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0005588913918472826, 0.0005588913918472826, 0.0005588913918472826, 0.0005588913918472826, 0.0005588913918472826]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005588913918472826

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46812344
Iteration 2/25 | Loss: 0.00026659
Iteration 3/25 | Loss: 0.00026659
Iteration 4/25 | Loss: 0.00026659
Iteration 5/25 | Loss: 0.00026658
Iteration 6/25 | Loss: 0.00026658
Iteration 7/25 | Loss: 0.00026658
Iteration 8/25 | Loss: 0.00026658
Iteration 9/25 | Loss: 0.00026658
Iteration 10/25 | Loss: 0.00026658
Iteration 11/25 | Loss: 0.00026658
Iteration 12/25 | Loss: 0.00026658
Iteration 13/25 | Loss: 0.00026658
Iteration 14/25 | Loss: 0.00026658
Iteration 15/25 | Loss: 0.00026658
Iteration 16/25 | Loss: 0.00026658
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0002665833162609488, 0.0002665833162609488, 0.0002665833162609488, 0.0002665833162609488, 0.0002665833162609488]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002665833162609488

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026658
Iteration 2/1000 | Loss: 0.00002135
Iteration 3/1000 | Loss: 0.00001276
Iteration 4/1000 | Loss: 0.00001167
Iteration 5/1000 | Loss: 0.00001110
Iteration 6/1000 | Loss: 0.00001069
Iteration 7/1000 | Loss: 0.00001056
Iteration 8/1000 | Loss: 0.00001055
Iteration 9/1000 | Loss: 0.00001041
Iteration 10/1000 | Loss: 0.00001040
Iteration 11/1000 | Loss: 0.00001032
Iteration 12/1000 | Loss: 0.00001026
Iteration 13/1000 | Loss: 0.00001024
Iteration 14/1000 | Loss: 0.00001024
Iteration 15/1000 | Loss: 0.00001021
Iteration 16/1000 | Loss: 0.00001021
Iteration 17/1000 | Loss: 0.00001021
Iteration 18/1000 | Loss: 0.00001020
Iteration 19/1000 | Loss: 0.00001020
Iteration 20/1000 | Loss: 0.00001020
Iteration 21/1000 | Loss: 0.00001020
Iteration 22/1000 | Loss: 0.00001018
Iteration 23/1000 | Loss: 0.00001014
Iteration 24/1000 | Loss: 0.00001014
Iteration 25/1000 | Loss: 0.00001013
Iteration 26/1000 | Loss: 0.00001005
Iteration 27/1000 | Loss: 0.00001005
Iteration 28/1000 | Loss: 0.00001003
Iteration 29/1000 | Loss: 0.00001002
Iteration 30/1000 | Loss: 0.00001002
Iteration 31/1000 | Loss: 0.00001002
Iteration 32/1000 | Loss: 0.00001002
Iteration 33/1000 | Loss: 0.00001002
Iteration 34/1000 | Loss: 0.00001001
Iteration 35/1000 | Loss: 0.00001001
Iteration 36/1000 | Loss: 0.00001000
Iteration 37/1000 | Loss: 0.00001000
Iteration 38/1000 | Loss: 0.00001000
Iteration 39/1000 | Loss: 0.00001000
Iteration 40/1000 | Loss: 0.00001000
Iteration 41/1000 | Loss: 0.00001000
Iteration 42/1000 | Loss: 0.00001000
Iteration 43/1000 | Loss: 0.00001000
Iteration 44/1000 | Loss: 0.00001000
Iteration 45/1000 | Loss: 0.00001000
Iteration 46/1000 | Loss: 0.00001000
Iteration 47/1000 | Loss: 0.00001000
Iteration 48/1000 | Loss: 0.00001000
Iteration 49/1000 | Loss: 0.00001000
Iteration 50/1000 | Loss: 0.00000999
Iteration 51/1000 | Loss: 0.00000999
Iteration 52/1000 | Loss: 0.00000999
Iteration 53/1000 | Loss: 0.00000999
Iteration 54/1000 | Loss: 0.00000999
Iteration 55/1000 | Loss: 0.00000999
Iteration 56/1000 | Loss: 0.00000999
Iteration 57/1000 | Loss: 0.00000999
Iteration 58/1000 | Loss: 0.00000998
Iteration 59/1000 | Loss: 0.00000998
Iteration 60/1000 | Loss: 0.00000998
Iteration 61/1000 | Loss: 0.00000998
Iteration 62/1000 | Loss: 0.00000997
Iteration 63/1000 | Loss: 0.00000997
Iteration 64/1000 | Loss: 0.00000997
Iteration 65/1000 | Loss: 0.00000997
Iteration 66/1000 | Loss: 0.00000997
Iteration 67/1000 | Loss: 0.00000997
Iteration 68/1000 | Loss: 0.00000996
Iteration 69/1000 | Loss: 0.00000996
Iteration 70/1000 | Loss: 0.00000995
Iteration 71/1000 | Loss: 0.00000993
Iteration 72/1000 | Loss: 0.00000993
Iteration 73/1000 | Loss: 0.00000992
Iteration 74/1000 | Loss: 0.00000992
Iteration 75/1000 | Loss: 0.00000992
Iteration 76/1000 | Loss: 0.00000991
Iteration 77/1000 | Loss: 0.00000991
Iteration 78/1000 | Loss: 0.00000990
Iteration 79/1000 | Loss: 0.00000990
Iteration 80/1000 | Loss: 0.00000990
Iteration 81/1000 | Loss: 0.00000989
Iteration 82/1000 | Loss: 0.00000989
Iteration 83/1000 | Loss: 0.00000988
Iteration 84/1000 | Loss: 0.00000988
Iteration 85/1000 | Loss: 0.00000987
Iteration 86/1000 | Loss: 0.00000987
Iteration 87/1000 | Loss: 0.00000986
Iteration 88/1000 | Loss: 0.00000986
Iteration 89/1000 | Loss: 0.00000986
Iteration 90/1000 | Loss: 0.00000985
Iteration 91/1000 | Loss: 0.00000985
Iteration 92/1000 | Loss: 0.00000985
Iteration 93/1000 | Loss: 0.00000985
Iteration 94/1000 | Loss: 0.00000985
Iteration 95/1000 | Loss: 0.00000985
Iteration 96/1000 | Loss: 0.00000985
Iteration 97/1000 | Loss: 0.00000985
Iteration 98/1000 | Loss: 0.00000985
Iteration 99/1000 | Loss: 0.00000985
Iteration 100/1000 | Loss: 0.00000985
Iteration 101/1000 | Loss: 0.00000985
Iteration 102/1000 | Loss: 0.00000984
Iteration 103/1000 | Loss: 0.00000984
Iteration 104/1000 | Loss: 0.00000984
Iteration 105/1000 | Loss: 0.00000984
Iteration 106/1000 | Loss: 0.00000984
Iteration 107/1000 | Loss: 0.00000983
Iteration 108/1000 | Loss: 0.00000983
Iteration 109/1000 | Loss: 0.00000983
Iteration 110/1000 | Loss: 0.00000983
Iteration 111/1000 | Loss: 0.00000983
Iteration 112/1000 | Loss: 0.00000983
Iteration 113/1000 | Loss: 0.00000983
Iteration 114/1000 | Loss: 0.00000983
Iteration 115/1000 | Loss: 0.00000983
Iteration 116/1000 | Loss: 0.00000983
Iteration 117/1000 | Loss: 0.00000983
Iteration 118/1000 | Loss: 0.00000982
Iteration 119/1000 | Loss: 0.00000982
Iteration 120/1000 | Loss: 0.00000982
Iteration 121/1000 | Loss: 0.00000982
Iteration 122/1000 | Loss: 0.00000982
Iteration 123/1000 | Loss: 0.00000982
Iteration 124/1000 | Loss: 0.00000982
Iteration 125/1000 | Loss: 0.00000982
Iteration 126/1000 | Loss: 0.00000982
Iteration 127/1000 | Loss: 0.00000982
Iteration 128/1000 | Loss: 0.00000981
Iteration 129/1000 | Loss: 0.00000981
Iteration 130/1000 | Loss: 0.00000981
Iteration 131/1000 | Loss: 0.00000981
Iteration 132/1000 | Loss: 0.00000981
Iteration 133/1000 | Loss: 0.00000981
Iteration 134/1000 | Loss: 0.00000981
Iteration 135/1000 | Loss: 0.00000981
Iteration 136/1000 | Loss: 0.00000981
Iteration 137/1000 | Loss: 0.00000981
Iteration 138/1000 | Loss: 0.00000981
Iteration 139/1000 | Loss: 0.00000981
Iteration 140/1000 | Loss: 0.00000981
Iteration 141/1000 | Loss: 0.00000981
Iteration 142/1000 | Loss: 0.00000981
Iteration 143/1000 | Loss: 0.00000981
Iteration 144/1000 | Loss: 0.00000981
Iteration 145/1000 | Loss: 0.00000981
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [9.81176253844751e-06, 9.81176253844751e-06, 9.81176253844751e-06, 9.81176253844751e-06, 9.81176253844751e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.81176253844751e-06

Optimization complete. Final v2v error: 2.681293487548828 mm

Highest mean error: 2.833308219909668 mm for frame 146

Lowest mean error: 2.6457643508911133 mm for frame 33

Saving results

Total time: 34.247596979141235
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00583066
Iteration 2/25 | Loss: 0.00117891
Iteration 3/25 | Loss: 0.00081158
Iteration 4/25 | Loss: 0.00074448
Iteration 5/25 | Loss: 0.00072757
Iteration 6/25 | Loss: 0.00072241
Iteration 7/25 | Loss: 0.00072107
Iteration 8/25 | Loss: 0.00072059
Iteration 9/25 | Loss: 0.00072035
Iteration 10/25 | Loss: 0.00072034
Iteration 11/25 | Loss: 0.00072034
Iteration 12/25 | Loss: 0.00072034
Iteration 13/25 | Loss: 0.00072034
Iteration 14/25 | Loss: 0.00072034
Iteration 15/25 | Loss: 0.00072034
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0007203434943221509, 0.0007203434943221509, 0.0007203434943221509, 0.0007203434943221509, 0.0007203434943221509]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007203434943221509

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57052863
Iteration 2/25 | Loss: 0.00036322
Iteration 3/25 | Loss: 0.00036321
Iteration 4/25 | Loss: 0.00036321
Iteration 5/25 | Loss: 0.00036321
Iteration 6/25 | Loss: 0.00036321
Iteration 7/25 | Loss: 0.00036321
Iteration 8/25 | Loss: 0.00036321
Iteration 9/25 | Loss: 0.00036321
Iteration 10/25 | Loss: 0.00036321
Iteration 11/25 | Loss: 0.00036321
Iteration 12/25 | Loss: 0.00036321
Iteration 13/25 | Loss: 0.00036321
Iteration 14/25 | Loss: 0.00036321
Iteration 15/25 | Loss: 0.00036321
Iteration 16/25 | Loss: 0.00036321
Iteration 17/25 | Loss: 0.00036321
Iteration 18/25 | Loss: 0.00036321
Iteration 19/25 | Loss: 0.00036321
Iteration 20/25 | Loss: 0.00036321
Iteration 21/25 | Loss: 0.00036321
Iteration 22/25 | Loss: 0.00036321
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00036320832441560924, 0.00036320832441560924, 0.00036320832441560924, 0.00036320832441560924, 0.00036320832441560924]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00036320832441560924

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036321
Iteration 2/1000 | Loss: 0.00004850
Iteration 3/1000 | Loss: 0.00003392
Iteration 4/1000 | Loss: 0.00002857
Iteration 5/1000 | Loss: 0.00002704
Iteration 6/1000 | Loss: 0.00002556
Iteration 7/1000 | Loss: 0.00002477
Iteration 8/1000 | Loss: 0.00002417
Iteration 9/1000 | Loss: 0.00002354
Iteration 10/1000 | Loss: 0.00002297
Iteration 11/1000 | Loss: 0.00053886
Iteration 12/1000 | Loss: 0.00028820
Iteration 13/1000 | Loss: 0.00005372
Iteration 14/1000 | Loss: 0.00004306
Iteration 15/1000 | Loss: 0.00003563
Iteration 16/1000 | Loss: 0.00002978
Iteration 17/1000 | Loss: 0.00002748
Iteration 18/1000 | Loss: 0.00002557
Iteration 19/1000 | Loss: 0.00002424
Iteration 20/1000 | Loss: 0.00002680
Iteration 21/1000 | Loss: 0.00002332
Iteration 22/1000 | Loss: 0.00022176
Iteration 23/1000 | Loss: 0.00006629
Iteration 24/1000 | Loss: 0.00011774
Iteration 25/1000 | Loss: 0.00021488
Iteration 26/1000 | Loss: 0.00003183
Iteration 27/1000 | Loss: 0.00029963
Iteration 28/1000 | Loss: 0.00018418
Iteration 29/1000 | Loss: 0.00002964
Iteration 30/1000 | Loss: 0.00002520
Iteration 31/1000 | Loss: 0.00002682
Iteration 32/1000 | Loss: 0.00028577
Iteration 33/1000 | Loss: 0.00046246
Iteration 34/1000 | Loss: 0.00036860
Iteration 35/1000 | Loss: 0.00025858
Iteration 36/1000 | Loss: 0.00014107
Iteration 37/1000 | Loss: 0.00003124
Iteration 38/1000 | Loss: 0.00002619
Iteration 39/1000 | Loss: 0.00002418
Iteration 40/1000 | Loss: 0.00017642
Iteration 41/1000 | Loss: 0.00003592
Iteration 42/1000 | Loss: 0.00002499
Iteration 43/1000 | Loss: 0.00002352
Iteration 44/1000 | Loss: 0.00002283
Iteration 45/1000 | Loss: 0.00002229
Iteration 46/1000 | Loss: 0.00011843
Iteration 47/1000 | Loss: 0.00017321
Iteration 48/1000 | Loss: 0.00002341
Iteration 49/1000 | Loss: 0.00006229
Iteration 50/1000 | Loss: 0.00002228
Iteration 51/1000 | Loss: 0.00002144
Iteration 52/1000 | Loss: 0.00002109
Iteration 53/1000 | Loss: 0.00002092
Iteration 54/1000 | Loss: 0.00002092
Iteration 55/1000 | Loss: 0.00002085
Iteration 56/1000 | Loss: 0.00002078
Iteration 57/1000 | Loss: 0.00002062
Iteration 58/1000 | Loss: 0.00002058
Iteration 59/1000 | Loss: 0.00002041
Iteration 60/1000 | Loss: 0.00002040
Iteration 61/1000 | Loss: 0.00002039
Iteration 62/1000 | Loss: 0.00002039
Iteration 63/1000 | Loss: 0.00002038
Iteration 64/1000 | Loss: 0.00002036
Iteration 65/1000 | Loss: 0.00018173
Iteration 66/1000 | Loss: 0.00012284
Iteration 67/1000 | Loss: 0.00018639
Iteration 68/1000 | Loss: 0.00020722
Iteration 69/1000 | Loss: 0.00013227
Iteration 70/1000 | Loss: 0.00018628
Iteration 71/1000 | Loss: 0.00018960
Iteration 72/1000 | Loss: 0.00027389
Iteration 73/1000 | Loss: 0.00016047
Iteration 74/1000 | Loss: 0.00002661
Iteration 75/1000 | Loss: 0.00002367
Iteration 76/1000 | Loss: 0.00018690
Iteration 77/1000 | Loss: 0.00002912
Iteration 78/1000 | Loss: 0.00002276
Iteration 79/1000 | Loss: 0.00002092
Iteration 80/1000 | Loss: 0.00001984
Iteration 81/1000 | Loss: 0.00001897
Iteration 82/1000 | Loss: 0.00001865
Iteration 83/1000 | Loss: 0.00001839
Iteration 84/1000 | Loss: 0.00001818
Iteration 85/1000 | Loss: 0.00001816
Iteration 86/1000 | Loss: 0.00001811
Iteration 87/1000 | Loss: 0.00001811
Iteration 88/1000 | Loss: 0.00001810
Iteration 89/1000 | Loss: 0.00001809
Iteration 90/1000 | Loss: 0.00001800
Iteration 91/1000 | Loss: 0.00001799
Iteration 92/1000 | Loss: 0.00001798
Iteration 93/1000 | Loss: 0.00001798
Iteration 94/1000 | Loss: 0.00001798
Iteration 95/1000 | Loss: 0.00001797
Iteration 96/1000 | Loss: 0.00001797
Iteration 97/1000 | Loss: 0.00001797
Iteration 98/1000 | Loss: 0.00001797
Iteration 99/1000 | Loss: 0.00001796
Iteration 100/1000 | Loss: 0.00001796
Iteration 101/1000 | Loss: 0.00001796
Iteration 102/1000 | Loss: 0.00001796
Iteration 103/1000 | Loss: 0.00001796
Iteration 104/1000 | Loss: 0.00001795
Iteration 105/1000 | Loss: 0.00001795
Iteration 106/1000 | Loss: 0.00001795
Iteration 107/1000 | Loss: 0.00001795
Iteration 108/1000 | Loss: 0.00001795
Iteration 109/1000 | Loss: 0.00001794
Iteration 110/1000 | Loss: 0.00001794
Iteration 111/1000 | Loss: 0.00001794
Iteration 112/1000 | Loss: 0.00001794
Iteration 113/1000 | Loss: 0.00001794
Iteration 114/1000 | Loss: 0.00001794
Iteration 115/1000 | Loss: 0.00001794
Iteration 116/1000 | Loss: 0.00001794
Iteration 117/1000 | Loss: 0.00001794
Iteration 118/1000 | Loss: 0.00001794
Iteration 119/1000 | Loss: 0.00001794
Iteration 120/1000 | Loss: 0.00001793
Iteration 121/1000 | Loss: 0.00001793
Iteration 122/1000 | Loss: 0.00001793
Iteration 123/1000 | Loss: 0.00001793
Iteration 124/1000 | Loss: 0.00001792
Iteration 125/1000 | Loss: 0.00001792
Iteration 126/1000 | Loss: 0.00001792
Iteration 127/1000 | Loss: 0.00001792
Iteration 128/1000 | Loss: 0.00001791
Iteration 129/1000 | Loss: 0.00001791
Iteration 130/1000 | Loss: 0.00001791
Iteration 131/1000 | Loss: 0.00001791
Iteration 132/1000 | Loss: 0.00001791
Iteration 133/1000 | Loss: 0.00001791
Iteration 134/1000 | Loss: 0.00001791
Iteration 135/1000 | Loss: 0.00001790
Iteration 136/1000 | Loss: 0.00001790
Iteration 137/1000 | Loss: 0.00001790
Iteration 138/1000 | Loss: 0.00001790
Iteration 139/1000 | Loss: 0.00001789
Iteration 140/1000 | Loss: 0.00001789
Iteration 141/1000 | Loss: 0.00001789
Iteration 142/1000 | Loss: 0.00001789
Iteration 143/1000 | Loss: 0.00001788
Iteration 144/1000 | Loss: 0.00001788
Iteration 145/1000 | Loss: 0.00001788
Iteration 146/1000 | Loss: 0.00001788
Iteration 147/1000 | Loss: 0.00001788
Iteration 148/1000 | Loss: 0.00001788
Iteration 149/1000 | Loss: 0.00001788
Iteration 150/1000 | Loss: 0.00001787
Iteration 151/1000 | Loss: 0.00001787
Iteration 152/1000 | Loss: 0.00001787
Iteration 153/1000 | Loss: 0.00001787
Iteration 154/1000 | Loss: 0.00001787
Iteration 155/1000 | Loss: 0.00001787
Iteration 156/1000 | Loss: 0.00001787
Iteration 157/1000 | Loss: 0.00001787
Iteration 158/1000 | Loss: 0.00001787
Iteration 159/1000 | Loss: 0.00001787
Iteration 160/1000 | Loss: 0.00001786
Iteration 161/1000 | Loss: 0.00001786
Iteration 162/1000 | Loss: 0.00001786
Iteration 163/1000 | Loss: 0.00001786
Iteration 164/1000 | Loss: 0.00001786
Iteration 165/1000 | Loss: 0.00001786
Iteration 166/1000 | Loss: 0.00001786
Iteration 167/1000 | Loss: 0.00001786
Iteration 168/1000 | Loss: 0.00001785
Iteration 169/1000 | Loss: 0.00001785
Iteration 170/1000 | Loss: 0.00001785
Iteration 171/1000 | Loss: 0.00001785
Iteration 172/1000 | Loss: 0.00001785
Iteration 173/1000 | Loss: 0.00001785
Iteration 174/1000 | Loss: 0.00001785
Iteration 175/1000 | Loss: 0.00001785
Iteration 176/1000 | Loss: 0.00001785
Iteration 177/1000 | Loss: 0.00001785
Iteration 178/1000 | Loss: 0.00001784
Iteration 179/1000 | Loss: 0.00001784
Iteration 180/1000 | Loss: 0.00001784
Iteration 181/1000 | Loss: 0.00001784
Iteration 182/1000 | Loss: 0.00001784
Iteration 183/1000 | Loss: 0.00001784
Iteration 184/1000 | Loss: 0.00001784
Iteration 185/1000 | Loss: 0.00001784
Iteration 186/1000 | Loss: 0.00001784
Iteration 187/1000 | Loss: 0.00001784
Iteration 188/1000 | Loss: 0.00001784
Iteration 189/1000 | Loss: 0.00001784
Iteration 190/1000 | Loss: 0.00001784
Iteration 191/1000 | Loss: 0.00001784
Iteration 192/1000 | Loss: 0.00001784
Iteration 193/1000 | Loss: 0.00001784
Iteration 194/1000 | Loss: 0.00001784
Iteration 195/1000 | Loss: 0.00001784
Iteration 196/1000 | Loss: 0.00001784
Iteration 197/1000 | Loss: 0.00001784
Iteration 198/1000 | Loss: 0.00001784
Iteration 199/1000 | Loss: 0.00001784
Iteration 200/1000 | Loss: 0.00001783
Iteration 201/1000 | Loss: 0.00001783
Iteration 202/1000 | Loss: 0.00001783
Iteration 203/1000 | Loss: 0.00001783
Iteration 204/1000 | Loss: 0.00001783
Iteration 205/1000 | Loss: 0.00001783
Iteration 206/1000 | Loss: 0.00001783
Iteration 207/1000 | Loss: 0.00001783
Iteration 208/1000 | Loss: 0.00001783
Iteration 209/1000 | Loss: 0.00001783
Iteration 210/1000 | Loss: 0.00001783
Iteration 211/1000 | Loss: 0.00001783
Iteration 212/1000 | Loss: 0.00001783
Iteration 213/1000 | Loss: 0.00001783
Iteration 214/1000 | Loss: 0.00001783
Iteration 215/1000 | Loss: 0.00001783
Iteration 216/1000 | Loss: 0.00001783
Iteration 217/1000 | Loss: 0.00001783
Iteration 218/1000 | Loss: 0.00001783
Iteration 219/1000 | Loss: 0.00001783
Iteration 220/1000 | Loss: 0.00001783
Iteration 221/1000 | Loss: 0.00001783
Iteration 222/1000 | Loss: 0.00001783
Iteration 223/1000 | Loss: 0.00001783
Iteration 224/1000 | Loss: 0.00001783
Iteration 225/1000 | Loss: 0.00001783
Iteration 226/1000 | Loss: 0.00001783
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [1.7828662748797797e-05, 1.7828662748797797e-05, 1.7828662748797797e-05, 1.7828662748797797e-05, 1.7828662748797797e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7828662748797797e-05

Optimization complete. Final v2v error: 3.541496515274048 mm

Highest mean error: 5.1845269203186035 mm for frame 147

Lowest mean error: 2.8322551250457764 mm for frame 74

Saving results

Total time: 148.0381326675415
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00469578
Iteration 2/25 | Loss: 0.00093943
Iteration 3/25 | Loss: 0.00072910
Iteration 4/25 | Loss: 0.00069799
Iteration 5/25 | Loss: 0.00068991
Iteration 6/25 | Loss: 0.00068823
Iteration 7/25 | Loss: 0.00068800
Iteration 8/25 | Loss: 0.00068800
Iteration 9/25 | Loss: 0.00068800
Iteration 10/25 | Loss: 0.00068800
Iteration 11/25 | Loss: 0.00068800
Iteration 12/25 | Loss: 0.00068800
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006880012224428356, 0.0006880012224428356, 0.0006880012224428356, 0.0006880012224428356, 0.0006880012224428356]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006880012224428356

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47654569
Iteration 2/25 | Loss: 0.00033405
Iteration 3/25 | Loss: 0.00033405
Iteration 4/25 | Loss: 0.00033405
Iteration 5/25 | Loss: 0.00033405
Iteration 6/25 | Loss: 0.00033405
Iteration 7/25 | Loss: 0.00033404
Iteration 8/25 | Loss: 0.00033404
Iteration 9/25 | Loss: 0.00033404
Iteration 10/25 | Loss: 0.00033404
Iteration 11/25 | Loss: 0.00033404
Iteration 12/25 | Loss: 0.00033404
Iteration 13/25 | Loss: 0.00033404
Iteration 14/25 | Loss: 0.00033404
Iteration 15/25 | Loss: 0.00033404
Iteration 16/25 | Loss: 0.00033404
Iteration 17/25 | Loss: 0.00033404
Iteration 18/25 | Loss: 0.00033404
Iteration 19/25 | Loss: 0.00033404
Iteration 20/25 | Loss: 0.00033404
Iteration 21/25 | Loss: 0.00033404
Iteration 22/25 | Loss: 0.00033404
Iteration 23/25 | Loss: 0.00033404
Iteration 24/25 | Loss: 0.00033404
Iteration 25/25 | Loss: 0.00033404

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033404
Iteration 2/1000 | Loss: 0.00003935
Iteration 3/1000 | Loss: 0.00002680
Iteration 4/1000 | Loss: 0.00002491
Iteration 5/1000 | Loss: 0.00002391
Iteration 6/1000 | Loss: 0.00002313
Iteration 7/1000 | Loss: 0.00002249
Iteration 8/1000 | Loss: 0.00002211
Iteration 9/1000 | Loss: 0.00002179
Iteration 10/1000 | Loss: 0.00002158
Iteration 11/1000 | Loss: 0.00002156
Iteration 12/1000 | Loss: 0.00002142
Iteration 13/1000 | Loss: 0.00002138
Iteration 14/1000 | Loss: 0.00002129
Iteration 15/1000 | Loss: 0.00002127
Iteration 16/1000 | Loss: 0.00002126
Iteration 17/1000 | Loss: 0.00002126
Iteration 18/1000 | Loss: 0.00002126
Iteration 19/1000 | Loss: 0.00002125
Iteration 20/1000 | Loss: 0.00002125
Iteration 21/1000 | Loss: 0.00002125
Iteration 22/1000 | Loss: 0.00002125
Iteration 23/1000 | Loss: 0.00002124
Iteration 24/1000 | Loss: 0.00002124
Iteration 25/1000 | Loss: 0.00002124
Iteration 26/1000 | Loss: 0.00002124
Iteration 27/1000 | Loss: 0.00002123
Iteration 28/1000 | Loss: 0.00002123
Iteration 29/1000 | Loss: 0.00002123
Iteration 30/1000 | Loss: 0.00002122
Iteration 31/1000 | Loss: 0.00002122
Iteration 32/1000 | Loss: 0.00002122
Iteration 33/1000 | Loss: 0.00002122
Iteration 34/1000 | Loss: 0.00002122
Iteration 35/1000 | Loss: 0.00002121
Iteration 36/1000 | Loss: 0.00002121
Iteration 37/1000 | Loss: 0.00002121
Iteration 38/1000 | Loss: 0.00002121
Iteration 39/1000 | Loss: 0.00002120
Iteration 40/1000 | Loss: 0.00002120
Iteration 41/1000 | Loss: 0.00002120
Iteration 42/1000 | Loss: 0.00002120
Iteration 43/1000 | Loss: 0.00002120
Iteration 44/1000 | Loss: 0.00002120
Iteration 45/1000 | Loss: 0.00002120
Iteration 46/1000 | Loss: 0.00002120
Iteration 47/1000 | Loss: 0.00002120
Iteration 48/1000 | Loss: 0.00002119
Iteration 49/1000 | Loss: 0.00002119
Iteration 50/1000 | Loss: 0.00002119
Iteration 51/1000 | Loss: 0.00002119
Iteration 52/1000 | Loss: 0.00002118
Iteration 53/1000 | Loss: 0.00002118
Iteration 54/1000 | Loss: 0.00002118
Iteration 55/1000 | Loss: 0.00002117
Iteration 56/1000 | Loss: 0.00002117
Iteration 57/1000 | Loss: 0.00002117
Iteration 58/1000 | Loss: 0.00002116
Iteration 59/1000 | Loss: 0.00002116
Iteration 60/1000 | Loss: 0.00002116
Iteration 61/1000 | Loss: 0.00002115
Iteration 62/1000 | Loss: 0.00002115
Iteration 63/1000 | Loss: 0.00002114
Iteration 64/1000 | Loss: 0.00002114
Iteration 65/1000 | Loss: 0.00002114
Iteration 66/1000 | Loss: 0.00002114
Iteration 67/1000 | Loss: 0.00002113
Iteration 68/1000 | Loss: 0.00002113
Iteration 69/1000 | Loss: 0.00002113
Iteration 70/1000 | Loss: 0.00002113
Iteration 71/1000 | Loss: 0.00002112
Iteration 72/1000 | Loss: 0.00002111
Iteration 73/1000 | Loss: 0.00002111
Iteration 74/1000 | Loss: 0.00002111
Iteration 75/1000 | Loss: 0.00002111
Iteration 76/1000 | Loss: 0.00002111
Iteration 77/1000 | Loss: 0.00002111
Iteration 78/1000 | Loss: 0.00002111
Iteration 79/1000 | Loss: 0.00002110
Iteration 80/1000 | Loss: 0.00002110
Iteration 81/1000 | Loss: 0.00002110
Iteration 82/1000 | Loss: 0.00002110
Iteration 83/1000 | Loss: 0.00002110
Iteration 84/1000 | Loss: 0.00002110
Iteration 85/1000 | Loss: 0.00002110
Iteration 86/1000 | Loss: 0.00002109
Iteration 87/1000 | Loss: 0.00002109
Iteration 88/1000 | Loss: 0.00002109
Iteration 89/1000 | Loss: 0.00002109
Iteration 90/1000 | Loss: 0.00002109
Iteration 91/1000 | Loss: 0.00002108
Iteration 92/1000 | Loss: 0.00002108
Iteration 93/1000 | Loss: 0.00002108
Iteration 94/1000 | Loss: 0.00002108
Iteration 95/1000 | Loss: 0.00002108
Iteration 96/1000 | Loss: 0.00002108
Iteration 97/1000 | Loss: 0.00002108
Iteration 98/1000 | Loss: 0.00002108
Iteration 99/1000 | Loss: 0.00002108
Iteration 100/1000 | Loss: 0.00002107
Iteration 101/1000 | Loss: 0.00002107
Iteration 102/1000 | Loss: 0.00002107
Iteration 103/1000 | Loss: 0.00002107
Iteration 104/1000 | Loss: 0.00002107
Iteration 105/1000 | Loss: 0.00002107
Iteration 106/1000 | Loss: 0.00002106
Iteration 107/1000 | Loss: 0.00002106
Iteration 108/1000 | Loss: 0.00002106
Iteration 109/1000 | Loss: 0.00002106
Iteration 110/1000 | Loss: 0.00002106
Iteration 111/1000 | Loss: 0.00002106
Iteration 112/1000 | Loss: 0.00002106
Iteration 113/1000 | Loss: 0.00002106
Iteration 114/1000 | Loss: 0.00002106
Iteration 115/1000 | Loss: 0.00002106
Iteration 116/1000 | Loss: 0.00002105
Iteration 117/1000 | Loss: 0.00002105
Iteration 118/1000 | Loss: 0.00002105
Iteration 119/1000 | Loss: 0.00002105
Iteration 120/1000 | Loss: 0.00002105
Iteration 121/1000 | Loss: 0.00002105
Iteration 122/1000 | Loss: 0.00002105
Iteration 123/1000 | Loss: 0.00002105
Iteration 124/1000 | Loss: 0.00002105
Iteration 125/1000 | Loss: 0.00002105
Iteration 126/1000 | Loss: 0.00002105
Iteration 127/1000 | Loss: 0.00002105
Iteration 128/1000 | Loss: 0.00002105
Iteration 129/1000 | Loss: 0.00002105
Iteration 130/1000 | Loss: 0.00002105
Iteration 131/1000 | Loss: 0.00002105
Iteration 132/1000 | Loss: 0.00002105
Iteration 133/1000 | Loss: 0.00002105
Iteration 134/1000 | Loss: 0.00002105
Iteration 135/1000 | Loss: 0.00002105
Iteration 136/1000 | Loss: 0.00002105
Iteration 137/1000 | Loss: 0.00002105
Iteration 138/1000 | Loss: 0.00002105
Iteration 139/1000 | Loss: 0.00002105
Iteration 140/1000 | Loss: 0.00002105
Iteration 141/1000 | Loss: 0.00002105
Iteration 142/1000 | Loss: 0.00002105
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [2.104593841067981e-05, 2.104593841067981e-05, 2.104593841067981e-05, 2.104593841067981e-05, 2.104593841067981e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.104593841067981e-05

Optimization complete. Final v2v error: 3.8551104068756104 mm

Highest mean error: 4.571164131164551 mm for frame 58

Lowest mean error: 3.3538594245910645 mm for frame 5

Saving results

Total time: 40.082056283950806
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00874298
Iteration 2/25 | Loss: 0.00132518
Iteration 3/25 | Loss: 0.00085873
Iteration 4/25 | Loss: 0.00079328
Iteration 5/25 | Loss: 0.00078203
Iteration 6/25 | Loss: 0.00077961
Iteration 7/25 | Loss: 0.00077827
Iteration 8/25 | Loss: 0.00077655
Iteration 9/25 | Loss: 0.00077622
Iteration 10/25 | Loss: 0.00077609
Iteration 11/25 | Loss: 0.00077605
Iteration 12/25 | Loss: 0.00077605
Iteration 13/25 | Loss: 0.00077605
Iteration 14/25 | Loss: 0.00077605
Iteration 15/25 | Loss: 0.00077605
Iteration 16/25 | Loss: 0.00077605
Iteration 17/25 | Loss: 0.00077605
Iteration 18/25 | Loss: 0.00077604
Iteration 19/25 | Loss: 0.00077604
Iteration 20/25 | Loss: 0.00077604
Iteration 21/25 | Loss: 0.00077604
Iteration 22/25 | Loss: 0.00077604
Iteration 23/25 | Loss: 0.00077604
Iteration 24/25 | Loss: 0.00077604
Iteration 25/25 | Loss: 0.00077604

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37909198
Iteration 2/25 | Loss: 0.00039272
Iteration 3/25 | Loss: 0.00039272
Iteration 4/25 | Loss: 0.00039272
Iteration 5/25 | Loss: 0.00039272
Iteration 6/25 | Loss: 0.00039272
Iteration 7/25 | Loss: 0.00039272
Iteration 8/25 | Loss: 0.00039272
Iteration 9/25 | Loss: 0.00039272
Iteration 10/25 | Loss: 0.00039272
Iteration 11/25 | Loss: 0.00039272
Iteration 12/25 | Loss: 0.00039272
Iteration 13/25 | Loss: 0.00039272
Iteration 14/25 | Loss: 0.00039272
Iteration 15/25 | Loss: 0.00039272
Iteration 16/25 | Loss: 0.00039272
Iteration 17/25 | Loss: 0.00039272
Iteration 18/25 | Loss: 0.00039272
Iteration 19/25 | Loss: 0.00039272
Iteration 20/25 | Loss: 0.00039272
Iteration 21/25 | Loss: 0.00039272
Iteration 22/25 | Loss: 0.00039272
Iteration 23/25 | Loss: 0.00039272
Iteration 24/25 | Loss: 0.00039272
Iteration 25/25 | Loss: 0.00039272

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039272
Iteration 2/1000 | Loss: 0.00003900
Iteration 3/1000 | Loss: 0.00003069
Iteration 4/1000 | Loss: 0.00002852
Iteration 5/1000 | Loss: 0.00002754
Iteration 6/1000 | Loss: 0.00002649
Iteration 7/1000 | Loss: 0.00002582
Iteration 8/1000 | Loss: 0.00002531
Iteration 9/1000 | Loss: 0.00002503
Iteration 10/1000 | Loss: 0.00002491
Iteration 11/1000 | Loss: 0.00002481
Iteration 12/1000 | Loss: 0.00002473
Iteration 13/1000 | Loss: 0.00002468
Iteration 14/1000 | Loss: 0.00002467
Iteration 15/1000 | Loss: 0.00002466
Iteration 16/1000 | Loss: 0.00002466
Iteration 17/1000 | Loss: 0.00002464
Iteration 18/1000 | Loss: 0.00002464
Iteration 19/1000 | Loss: 0.00002464
Iteration 20/1000 | Loss: 0.00002464
Iteration 21/1000 | Loss: 0.00002464
Iteration 22/1000 | Loss: 0.00002464
Iteration 23/1000 | Loss: 0.00002464
Iteration 24/1000 | Loss: 0.00002464
Iteration 25/1000 | Loss: 0.00002463
Iteration 26/1000 | Loss: 0.00002463
Iteration 27/1000 | Loss: 0.00002463
Iteration 28/1000 | Loss: 0.00002463
Iteration 29/1000 | Loss: 0.00002462
Iteration 30/1000 | Loss: 0.00002461
Iteration 31/1000 | Loss: 0.00002461
Iteration 32/1000 | Loss: 0.00002461
Iteration 33/1000 | Loss: 0.00002461
Iteration 34/1000 | Loss: 0.00002461
Iteration 35/1000 | Loss: 0.00002461
Iteration 36/1000 | Loss: 0.00002461
Iteration 37/1000 | Loss: 0.00002461
Iteration 38/1000 | Loss: 0.00002460
Iteration 39/1000 | Loss: 0.00002460
Iteration 40/1000 | Loss: 0.00002460
Iteration 41/1000 | Loss: 0.00002460
Iteration 42/1000 | Loss: 0.00002460
Iteration 43/1000 | Loss: 0.00002460
Iteration 44/1000 | Loss: 0.00002460
Iteration 45/1000 | Loss: 0.00002460
Iteration 46/1000 | Loss: 0.00002459
Iteration 47/1000 | Loss: 0.00002459
Iteration 48/1000 | Loss: 0.00002459
Iteration 49/1000 | Loss: 0.00002459
Iteration 50/1000 | Loss: 0.00002459
Iteration 51/1000 | Loss: 0.00002459
Iteration 52/1000 | Loss: 0.00002459
Iteration 53/1000 | Loss: 0.00002459
Iteration 54/1000 | Loss: 0.00002459
Iteration 55/1000 | Loss: 0.00002458
Iteration 56/1000 | Loss: 0.00002458
Iteration 57/1000 | Loss: 0.00002458
Iteration 58/1000 | Loss: 0.00002458
Iteration 59/1000 | Loss: 0.00002458
Iteration 60/1000 | Loss: 0.00002458
Iteration 61/1000 | Loss: 0.00002458
Iteration 62/1000 | Loss: 0.00002457
Iteration 63/1000 | Loss: 0.00002457
Iteration 64/1000 | Loss: 0.00002456
Iteration 65/1000 | Loss: 0.00002456
Iteration 66/1000 | Loss: 0.00002456
Iteration 67/1000 | Loss: 0.00002456
Iteration 68/1000 | Loss: 0.00002456
Iteration 69/1000 | Loss: 0.00002456
Iteration 70/1000 | Loss: 0.00002456
Iteration 71/1000 | Loss: 0.00002456
Iteration 72/1000 | Loss: 0.00002455
Iteration 73/1000 | Loss: 0.00002455
Iteration 74/1000 | Loss: 0.00002455
Iteration 75/1000 | Loss: 0.00002455
Iteration 76/1000 | Loss: 0.00002454
Iteration 77/1000 | Loss: 0.00002454
Iteration 78/1000 | Loss: 0.00002454
Iteration 79/1000 | Loss: 0.00002454
Iteration 80/1000 | Loss: 0.00002454
Iteration 81/1000 | Loss: 0.00002453
Iteration 82/1000 | Loss: 0.00002453
Iteration 83/1000 | Loss: 0.00002453
Iteration 84/1000 | Loss: 0.00002453
Iteration 85/1000 | Loss: 0.00002453
Iteration 86/1000 | Loss: 0.00002453
Iteration 87/1000 | Loss: 0.00002453
Iteration 88/1000 | Loss: 0.00002453
Iteration 89/1000 | Loss: 0.00002453
Iteration 90/1000 | Loss: 0.00002453
Iteration 91/1000 | Loss: 0.00002453
Iteration 92/1000 | Loss: 0.00002453
Iteration 93/1000 | Loss: 0.00002453
Iteration 94/1000 | Loss: 0.00002453
Iteration 95/1000 | Loss: 0.00002453
Iteration 96/1000 | Loss: 0.00002453
Iteration 97/1000 | Loss: 0.00002453
Iteration 98/1000 | Loss: 0.00002453
Iteration 99/1000 | Loss: 0.00002453
Iteration 100/1000 | Loss: 0.00002453
Iteration 101/1000 | Loss: 0.00002453
Iteration 102/1000 | Loss: 0.00002453
Iteration 103/1000 | Loss: 0.00002453
Iteration 104/1000 | Loss: 0.00002453
Iteration 105/1000 | Loss: 0.00002453
Iteration 106/1000 | Loss: 0.00002453
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [2.452975786582101e-05, 2.452975786582101e-05, 2.452975786582101e-05, 2.452975786582101e-05, 2.452975786582101e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.452975786582101e-05

Optimization complete. Final v2v error: 4.135781764984131 mm

Highest mean error: 4.832762718200684 mm for frame 134

Lowest mean error: 3.52980637550354 mm for frame 82

Saving results

Total time: 45.11860728263855
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00356150
Iteration 2/25 | Loss: 0.00093272
Iteration 3/25 | Loss: 0.00071366
Iteration 4/25 | Loss: 0.00066848
Iteration 5/25 | Loss: 0.00065384
Iteration 6/25 | Loss: 0.00064729
Iteration 7/25 | Loss: 0.00064598
Iteration 8/25 | Loss: 0.00064598
Iteration 9/25 | Loss: 0.00064598
Iteration 10/25 | Loss: 0.00064598
Iteration 11/25 | Loss: 0.00064598
Iteration 12/25 | Loss: 0.00064598
Iteration 13/25 | Loss: 0.00064598
Iteration 14/25 | Loss: 0.00064598
Iteration 15/25 | Loss: 0.00064598
Iteration 16/25 | Loss: 0.00064598
Iteration 17/25 | Loss: 0.00064598
Iteration 18/25 | Loss: 0.00064598
Iteration 19/25 | Loss: 0.00064598
Iteration 20/25 | Loss: 0.00064598
Iteration 21/25 | Loss: 0.00064598
Iteration 22/25 | Loss: 0.00064598
Iteration 23/25 | Loss: 0.00064598
Iteration 24/25 | Loss: 0.00064598
Iteration 25/25 | Loss: 0.00064598

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45175231
Iteration 2/25 | Loss: 0.00026527
Iteration 3/25 | Loss: 0.00026527
Iteration 4/25 | Loss: 0.00026527
Iteration 5/25 | Loss: 0.00026527
Iteration 6/25 | Loss: 0.00026527
Iteration 7/25 | Loss: 0.00026527
Iteration 8/25 | Loss: 0.00026527
Iteration 9/25 | Loss: 0.00026526
Iteration 10/25 | Loss: 0.00026526
Iteration 11/25 | Loss: 0.00026526
Iteration 12/25 | Loss: 0.00026526
Iteration 13/25 | Loss: 0.00026526
Iteration 14/25 | Loss: 0.00026526
Iteration 15/25 | Loss: 0.00026526
Iteration 16/25 | Loss: 0.00026526
Iteration 17/25 | Loss: 0.00026526
Iteration 18/25 | Loss: 0.00026526
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0002652641851454973, 0.0002652641851454973, 0.0002652641851454973, 0.0002652641851454973, 0.0002652641851454973]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002652641851454973

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026526
Iteration 2/1000 | Loss: 0.00003059
Iteration 3/1000 | Loss: 0.00002187
Iteration 4/1000 | Loss: 0.00002035
Iteration 5/1000 | Loss: 0.00001897
Iteration 6/1000 | Loss: 0.00001833
Iteration 7/1000 | Loss: 0.00001786
Iteration 8/1000 | Loss: 0.00001758
Iteration 9/1000 | Loss: 0.00001741
Iteration 10/1000 | Loss: 0.00001732
Iteration 11/1000 | Loss: 0.00001720
Iteration 12/1000 | Loss: 0.00001720
Iteration 13/1000 | Loss: 0.00001706
Iteration 14/1000 | Loss: 0.00001704
Iteration 15/1000 | Loss: 0.00001703
Iteration 16/1000 | Loss: 0.00001701
Iteration 17/1000 | Loss: 0.00001700
Iteration 18/1000 | Loss: 0.00001699
Iteration 19/1000 | Loss: 0.00001698
Iteration 20/1000 | Loss: 0.00001698
Iteration 21/1000 | Loss: 0.00001697
Iteration 22/1000 | Loss: 0.00001697
Iteration 23/1000 | Loss: 0.00001694
Iteration 24/1000 | Loss: 0.00001693
Iteration 25/1000 | Loss: 0.00001691
Iteration 26/1000 | Loss: 0.00001690
Iteration 27/1000 | Loss: 0.00001690
Iteration 28/1000 | Loss: 0.00001688
Iteration 29/1000 | Loss: 0.00001688
Iteration 30/1000 | Loss: 0.00001688
Iteration 31/1000 | Loss: 0.00001687
Iteration 32/1000 | Loss: 0.00001687
Iteration 33/1000 | Loss: 0.00001686
Iteration 34/1000 | Loss: 0.00001686
Iteration 35/1000 | Loss: 0.00001686
Iteration 36/1000 | Loss: 0.00001685
Iteration 37/1000 | Loss: 0.00001685
Iteration 38/1000 | Loss: 0.00001685
Iteration 39/1000 | Loss: 0.00001685
Iteration 40/1000 | Loss: 0.00001685
Iteration 41/1000 | Loss: 0.00001685
Iteration 42/1000 | Loss: 0.00001685
Iteration 43/1000 | Loss: 0.00001684
Iteration 44/1000 | Loss: 0.00001684
Iteration 45/1000 | Loss: 0.00001684
Iteration 46/1000 | Loss: 0.00001684
Iteration 47/1000 | Loss: 0.00001684
Iteration 48/1000 | Loss: 0.00001683
Iteration 49/1000 | Loss: 0.00001683
Iteration 50/1000 | Loss: 0.00001682
Iteration 51/1000 | Loss: 0.00001682
Iteration 52/1000 | Loss: 0.00001682
Iteration 53/1000 | Loss: 0.00001681
Iteration 54/1000 | Loss: 0.00001681
Iteration 55/1000 | Loss: 0.00001681
Iteration 56/1000 | Loss: 0.00001680
Iteration 57/1000 | Loss: 0.00001679
Iteration 58/1000 | Loss: 0.00001678
Iteration 59/1000 | Loss: 0.00001678
Iteration 60/1000 | Loss: 0.00001677
Iteration 61/1000 | Loss: 0.00001677
Iteration 62/1000 | Loss: 0.00001677
Iteration 63/1000 | Loss: 0.00001677
Iteration 64/1000 | Loss: 0.00001676
Iteration 65/1000 | Loss: 0.00001676
Iteration 66/1000 | Loss: 0.00001676
Iteration 67/1000 | Loss: 0.00001676
Iteration 68/1000 | Loss: 0.00001676
Iteration 69/1000 | Loss: 0.00001676
Iteration 70/1000 | Loss: 0.00001676
Iteration 71/1000 | Loss: 0.00001675
Iteration 72/1000 | Loss: 0.00001675
Iteration 73/1000 | Loss: 0.00001675
Iteration 74/1000 | Loss: 0.00001675
Iteration 75/1000 | Loss: 0.00001675
Iteration 76/1000 | Loss: 0.00001675
Iteration 77/1000 | Loss: 0.00001675
Iteration 78/1000 | Loss: 0.00001675
Iteration 79/1000 | Loss: 0.00001675
Iteration 80/1000 | Loss: 0.00001674
Iteration 81/1000 | Loss: 0.00001674
Iteration 82/1000 | Loss: 0.00001674
Iteration 83/1000 | Loss: 0.00001674
Iteration 84/1000 | Loss: 0.00001674
Iteration 85/1000 | Loss: 0.00001674
Iteration 86/1000 | Loss: 0.00001674
Iteration 87/1000 | Loss: 0.00001674
Iteration 88/1000 | Loss: 0.00001674
Iteration 89/1000 | Loss: 0.00001674
Iteration 90/1000 | Loss: 0.00001674
Iteration 91/1000 | Loss: 0.00001674
Iteration 92/1000 | Loss: 0.00001674
Iteration 93/1000 | Loss: 0.00001674
Iteration 94/1000 | Loss: 0.00001674
Iteration 95/1000 | Loss: 0.00001674
Iteration 96/1000 | Loss: 0.00001674
Iteration 97/1000 | Loss: 0.00001674
Iteration 98/1000 | Loss: 0.00001674
Iteration 99/1000 | Loss: 0.00001674
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [1.6739051716285758e-05, 1.6739051716285758e-05, 1.6739051716285758e-05, 1.6739051716285758e-05, 1.6739051716285758e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6739051716285758e-05

Optimization complete. Final v2v error: 3.3606109619140625 mm

Highest mean error: 3.5069260597229004 mm for frame 222

Lowest mean error: 2.968294382095337 mm for frame 3

Saving results

Total time: 40.02131128311157
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00422100
Iteration 2/25 | Loss: 0.00092371
Iteration 3/25 | Loss: 0.00066891
Iteration 4/25 | Loss: 0.00062844
Iteration 5/25 | Loss: 0.00061701
Iteration 6/25 | Loss: 0.00061295
Iteration 7/25 | Loss: 0.00061198
Iteration 8/25 | Loss: 0.00061191
Iteration 9/25 | Loss: 0.00061191
Iteration 10/25 | Loss: 0.00061191
Iteration 11/25 | Loss: 0.00061191
Iteration 12/25 | Loss: 0.00061191
Iteration 13/25 | Loss: 0.00061191
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.000611908093560487, 0.000611908093560487, 0.000611908093560487, 0.000611908093560487, 0.000611908093560487]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000611908093560487

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.73275185
Iteration 2/25 | Loss: 0.00033314
Iteration 3/25 | Loss: 0.00033313
Iteration 4/25 | Loss: 0.00033313
Iteration 5/25 | Loss: 0.00033313
Iteration 6/25 | Loss: 0.00033313
Iteration 7/25 | Loss: 0.00033313
Iteration 8/25 | Loss: 0.00033313
Iteration 9/25 | Loss: 0.00033313
Iteration 10/25 | Loss: 0.00033313
Iteration 11/25 | Loss: 0.00033313
Iteration 12/25 | Loss: 0.00033313
Iteration 13/25 | Loss: 0.00033313
Iteration 14/25 | Loss: 0.00033313
Iteration 15/25 | Loss: 0.00033313
Iteration 16/25 | Loss: 0.00033313
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0003331313200760633, 0.0003331313200760633, 0.0003331313200760633, 0.0003331313200760633, 0.0003331313200760633]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003331313200760633

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033313
Iteration 2/1000 | Loss: 0.00002442
Iteration 3/1000 | Loss: 0.00001742
Iteration 4/1000 | Loss: 0.00001612
Iteration 5/1000 | Loss: 0.00001528
Iteration 6/1000 | Loss: 0.00001487
Iteration 7/1000 | Loss: 0.00001451
Iteration 8/1000 | Loss: 0.00001438
Iteration 9/1000 | Loss: 0.00001419
Iteration 10/1000 | Loss: 0.00001419
Iteration 11/1000 | Loss: 0.00001416
Iteration 12/1000 | Loss: 0.00001401
Iteration 13/1000 | Loss: 0.00001400
Iteration 14/1000 | Loss: 0.00001398
Iteration 15/1000 | Loss: 0.00001382
Iteration 16/1000 | Loss: 0.00001379
Iteration 17/1000 | Loss: 0.00001378
Iteration 18/1000 | Loss: 0.00001377
Iteration 19/1000 | Loss: 0.00001375
Iteration 20/1000 | Loss: 0.00001375
Iteration 21/1000 | Loss: 0.00001374
Iteration 22/1000 | Loss: 0.00001373
Iteration 23/1000 | Loss: 0.00001372
Iteration 24/1000 | Loss: 0.00001370
Iteration 25/1000 | Loss: 0.00001368
Iteration 26/1000 | Loss: 0.00001366
Iteration 27/1000 | Loss: 0.00001364
Iteration 28/1000 | Loss: 0.00001364
Iteration 29/1000 | Loss: 0.00001363
Iteration 30/1000 | Loss: 0.00001363
Iteration 31/1000 | Loss: 0.00001362
Iteration 32/1000 | Loss: 0.00001362
Iteration 33/1000 | Loss: 0.00001361
Iteration 34/1000 | Loss: 0.00001361
Iteration 35/1000 | Loss: 0.00001360
Iteration 36/1000 | Loss: 0.00001360
Iteration 37/1000 | Loss: 0.00001359
Iteration 38/1000 | Loss: 0.00001359
Iteration 39/1000 | Loss: 0.00001359
Iteration 40/1000 | Loss: 0.00001358
Iteration 41/1000 | Loss: 0.00001358
Iteration 42/1000 | Loss: 0.00001358
Iteration 43/1000 | Loss: 0.00001357
Iteration 44/1000 | Loss: 0.00001357
Iteration 45/1000 | Loss: 0.00001357
Iteration 46/1000 | Loss: 0.00001356
Iteration 47/1000 | Loss: 0.00001356
Iteration 48/1000 | Loss: 0.00001356
Iteration 49/1000 | Loss: 0.00001356
Iteration 50/1000 | Loss: 0.00001355
Iteration 51/1000 | Loss: 0.00001355
Iteration 52/1000 | Loss: 0.00001355
Iteration 53/1000 | Loss: 0.00001355
Iteration 54/1000 | Loss: 0.00001355
Iteration 55/1000 | Loss: 0.00001354
Iteration 56/1000 | Loss: 0.00001354
Iteration 57/1000 | Loss: 0.00001354
Iteration 58/1000 | Loss: 0.00001354
Iteration 59/1000 | Loss: 0.00001353
Iteration 60/1000 | Loss: 0.00001353
Iteration 61/1000 | Loss: 0.00001353
Iteration 62/1000 | Loss: 0.00001353
Iteration 63/1000 | Loss: 0.00001353
Iteration 64/1000 | Loss: 0.00001353
Iteration 65/1000 | Loss: 0.00001353
Iteration 66/1000 | Loss: 0.00001353
Iteration 67/1000 | Loss: 0.00001352
Iteration 68/1000 | Loss: 0.00001352
Iteration 69/1000 | Loss: 0.00001352
Iteration 70/1000 | Loss: 0.00001352
Iteration 71/1000 | Loss: 0.00001351
Iteration 72/1000 | Loss: 0.00001351
Iteration 73/1000 | Loss: 0.00001351
Iteration 74/1000 | Loss: 0.00001351
Iteration 75/1000 | Loss: 0.00001350
Iteration 76/1000 | Loss: 0.00001350
Iteration 77/1000 | Loss: 0.00001350
Iteration 78/1000 | Loss: 0.00001350
Iteration 79/1000 | Loss: 0.00001350
Iteration 80/1000 | Loss: 0.00001350
Iteration 81/1000 | Loss: 0.00001349
Iteration 82/1000 | Loss: 0.00001349
Iteration 83/1000 | Loss: 0.00001349
Iteration 84/1000 | Loss: 0.00001349
Iteration 85/1000 | Loss: 0.00001349
Iteration 86/1000 | Loss: 0.00001349
Iteration 87/1000 | Loss: 0.00001349
Iteration 88/1000 | Loss: 0.00001348
Iteration 89/1000 | Loss: 0.00001348
Iteration 90/1000 | Loss: 0.00001348
Iteration 91/1000 | Loss: 0.00001348
Iteration 92/1000 | Loss: 0.00001348
Iteration 93/1000 | Loss: 0.00001348
Iteration 94/1000 | Loss: 0.00001348
Iteration 95/1000 | Loss: 0.00001348
Iteration 96/1000 | Loss: 0.00001348
Iteration 97/1000 | Loss: 0.00001348
Iteration 98/1000 | Loss: 0.00001348
Iteration 99/1000 | Loss: 0.00001348
Iteration 100/1000 | Loss: 0.00001347
Iteration 101/1000 | Loss: 0.00001347
Iteration 102/1000 | Loss: 0.00001347
Iteration 103/1000 | Loss: 0.00001347
Iteration 104/1000 | Loss: 0.00001347
Iteration 105/1000 | Loss: 0.00001347
Iteration 106/1000 | Loss: 0.00001347
Iteration 107/1000 | Loss: 0.00001347
Iteration 108/1000 | Loss: 0.00001347
Iteration 109/1000 | Loss: 0.00001347
Iteration 110/1000 | Loss: 0.00001347
Iteration 111/1000 | Loss: 0.00001347
Iteration 112/1000 | Loss: 0.00001346
Iteration 113/1000 | Loss: 0.00001346
Iteration 114/1000 | Loss: 0.00001346
Iteration 115/1000 | Loss: 0.00001346
Iteration 116/1000 | Loss: 0.00001345
Iteration 117/1000 | Loss: 0.00001345
Iteration 118/1000 | Loss: 0.00001345
Iteration 119/1000 | Loss: 0.00001345
Iteration 120/1000 | Loss: 0.00001345
Iteration 121/1000 | Loss: 0.00001345
Iteration 122/1000 | Loss: 0.00001345
Iteration 123/1000 | Loss: 0.00001345
Iteration 124/1000 | Loss: 0.00001345
Iteration 125/1000 | Loss: 0.00001345
Iteration 126/1000 | Loss: 0.00001345
Iteration 127/1000 | Loss: 0.00001345
Iteration 128/1000 | Loss: 0.00001345
Iteration 129/1000 | Loss: 0.00001345
Iteration 130/1000 | Loss: 0.00001345
Iteration 131/1000 | Loss: 0.00001345
Iteration 132/1000 | Loss: 0.00001345
Iteration 133/1000 | Loss: 0.00001345
Iteration 134/1000 | Loss: 0.00001345
Iteration 135/1000 | Loss: 0.00001345
Iteration 136/1000 | Loss: 0.00001345
Iteration 137/1000 | Loss: 0.00001345
Iteration 138/1000 | Loss: 0.00001345
Iteration 139/1000 | Loss: 0.00001345
Iteration 140/1000 | Loss: 0.00001345
Iteration 141/1000 | Loss: 0.00001345
Iteration 142/1000 | Loss: 0.00001345
Iteration 143/1000 | Loss: 0.00001345
Iteration 144/1000 | Loss: 0.00001345
Iteration 145/1000 | Loss: 0.00001345
Iteration 146/1000 | Loss: 0.00001345
Iteration 147/1000 | Loss: 0.00001345
Iteration 148/1000 | Loss: 0.00001345
Iteration 149/1000 | Loss: 0.00001345
Iteration 150/1000 | Loss: 0.00001345
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.3446134289551992e-05, 1.3446134289551992e-05, 1.3446134289551992e-05, 1.3446134289551992e-05, 1.3446134289551992e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3446134289551992e-05

Optimization complete. Final v2v error: 3.0960826873779297 mm

Highest mean error: 4.02435827255249 mm for frame 84

Lowest mean error: 2.526470899581909 mm for frame 59

Saving results

Total time: 40.742875814437866
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00382137
Iteration 2/25 | Loss: 0.00090760
Iteration 3/25 | Loss: 0.00072731
Iteration 4/25 | Loss: 0.00069032
Iteration 5/25 | Loss: 0.00067729
Iteration 6/25 | Loss: 0.00067402
Iteration 7/25 | Loss: 0.00067306
Iteration 8/25 | Loss: 0.00067305
Iteration 9/25 | Loss: 0.00067305
Iteration 10/25 | Loss: 0.00067305
Iteration 11/25 | Loss: 0.00067305
Iteration 12/25 | Loss: 0.00067305
Iteration 13/25 | Loss: 0.00067305
Iteration 14/25 | Loss: 0.00067305
Iteration 15/25 | Loss: 0.00067305
Iteration 16/25 | Loss: 0.00067305
Iteration 17/25 | Loss: 0.00067305
Iteration 18/25 | Loss: 0.00067305
Iteration 19/25 | Loss: 0.00067305
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.000673046219162643, 0.000673046219162643, 0.000673046219162643, 0.000673046219162643, 0.000673046219162643]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000673046219162643

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47254801
Iteration 2/25 | Loss: 0.00034267
Iteration 3/25 | Loss: 0.00034267
Iteration 4/25 | Loss: 0.00034267
Iteration 5/25 | Loss: 0.00034267
Iteration 6/25 | Loss: 0.00034267
Iteration 7/25 | Loss: 0.00034267
Iteration 8/25 | Loss: 0.00034267
Iteration 9/25 | Loss: 0.00034267
Iteration 10/25 | Loss: 0.00034267
Iteration 11/25 | Loss: 0.00034267
Iteration 12/25 | Loss: 0.00034267
Iteration 13/25 | Loss: 0.00034267
Iteration 14/25 | Loss: 0.00034267
Iteration 15/25 | Loss: 0.00034267
Iteration 16/25 | Loss: 0.00034267
Iteration 17/25 | Loss: 0.00034267
Iteration 18/25 | Loss: 0.00034267
Iteration 19/25 | Loss: 0.00034267
Iteration 20/25 | Loss: 0.00034267
Iteration 21/25 | Loss: 0.00034267
Iteration 22/25 | Loss: 0.00034267
Iteration 23/25 | Loss: 0.00034267
Iteration 24/25 | Loss: 0.00034267
Iteration 25/25 | Loss: 0.00034267

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00034267
Iteration 2/1000 | Loss: 0.00004159
Iteration 3/1000 | Loss: 0.00002826
Iteration 4/1000 | Loss: 0.00002424
Iteration 5/1000 | Loss: 0.00002264
Iteration 6/1000 | Loss: 0.00002143
Iteration 7/1000 | Loss: 0.00002070
Iteration 8/1000 | Loss: 0.00002011
Iteration 9/1000 | Loss: 0.00001966
Iteration 10/1000 | Loss: 0.00001951
Iteration 11/1000 | Loss: 0.00001926
Iteration 12/1000 | Loss: 0.00001908
Iteration 13/1000 | Loss: 0.00001904
Iteration 14/1000 | Loss: 0.00001902
Iteration 15/1000 | Loss: 0.00001900
Iteration 16/1000 | Loss: 0.00001897
Iteration 17/1000 | Loss: 0.00001892
Iteration 18/1000 | Loss: 0.00001890
Iteration 19/1000 | Loss: 0.00001887
Iteration 20/1000 | Loss: 0.00001880
Iteration 21/1000 | Loss: 0.00001876
Iteration 22/1000 | Loss: 0.00001875
Iteration 23/1000 | Loss: 0.00001874
Iteration 24/1000 | Loss: 0.00001874
Iteration 25/1000 | Loss: 0.00001873
Iteration 26/1000 | Loss: 0.00001873
Iteration 27/1000 | Loss: 0.00001872
Iteration 28/1000 | Loss: 0.00001871
Iteration 29/1000 | Loss: 0.00001870
Iteration 30/1000 | Loss: 0.00001869
Iteration 31/1000 | Loss: 0.00001869
Iteration 32/1000 | Loss: 0.00001868
Iteration 33/1000 | Loss: 0.00001868
Iteration 34/1000 | Loss: 0.00001867
Iteration 35/1000 | Loss: 0.00001867
Iteration 36/1000 | Loss: 0.00001866
Iteration 37/1000 | Loss: 0.00001866
Iteration 38/1000 | Loss: 0.00001866
Iteration 39/1000 | Loss: 0.00001865
Iteration 40/1000 | Loss: 0.00001864
Iteration 41/1000 | Loss: 0.00001864
Iteration 42/1000 | Loss: 0.00001863
Iteration 43/1000 | Loss: 0.00001862
Iteration 44/1000 | Loss: 0.00001861
Iteration 45/1000 | Loss: 0.00001860
Iteration 46/1000 | Loss: 0.00001859
Iteration 47/1000 | Loss: 0.00001859
Iteration 48/1000 | Loss: 0.00001859
Iteration 49/1000 | Loss: 0.00001859
Iteration 50/1000 | Loss: 0.00001858
Iteration 51/1000 | Loss: 0.00001858
Iteration 52/1000 | Loss: 0.00001857
Iteration 53/1000 | Loss: 0.00001857
Iteration 54/1000 | Loss: 0.00001856
Iteration 55/1000 | Loss: 0.00001856
Iteration 56/1000 | Loss: 0.00001856
Iteration 57/1000 | Loss: 0.00001856
Iteration 58/1000 | Loss: 0.00001856
Iteration 59/1000 | Loss: 0.00001856
Iteration 60/1000 | Loss: 0.00001856
Iteration 61/1000 | Loss: 0.00001856
Iteration 62/1000 | Loss: 0.00001855
Iteration 63/1000 | Loss: 0.00001855
Iteration 64/1000 | Loss: 0.00001855
Iteration 65/1000 | Loss: 0.00001855
Iteration 66/1000 | Loss: 0.00001855
Iteration 67/1000 | Loss: 0.00001854
Iteration 68/1000 | Loss: 0.00001854
Iteration 69/1000 | Loss: 0.00001854
Iteration 70/1000 | Loss: 0.00001854
Iteration 71/1000 | Loss: 0.00001854
Iteration 72/1000 | Loss: 0.00001853
Iteration 73/1000 | Loss: 0.00001853
Iteration 74/1000 | Loss: 0.00001853
Iteration 75/1000 | Loss: 0.00001852
Iteration 76/1000 | Loss: 0.00001852
Iteration 77/1000 | Loss: 0.00001851
Iteration 78/1000 | Loss: 0.00001851
Iteration 79/1000 | Loss: 0.00001851
Iteration 80/1000 | Loss: 0.00001850
Iteration 81/1000 | Loss: 0.00001850
Iteration 82/1000 | Loss: 0.00001850
Iteration 83/1000 | Loss: 0.00001850
Iteration 84/1000 | Loss: 0.00001849
Iteration 85/1000 | Loss: 0.00001849
Iteration 86/1000 | Loss: 0.00001849
Iteration 87/1000 | Loss: 0.00001849
Iteration 88/1000 | Loss: 0.00001848
Iteration 89/1000 | Loss: 0.00001848
Iteration 90/1000 | Loss: 0.00001848
Iteration 91/1000 | Loss: 0.00001847
Iteration 92/1000 | Loss: 0.00001847
Iteration 93/1000 | Loss: 0.00001847
Iteration 94/1000 | Loss: 0.00001847
Iteration 95/1000 | Loss: 0.00001846
Iteration 96/1000 | Loss: 0.00001846
Iteration 97/1000 | Loss: 0.00001846
Iteration 98/1000 | Loss: 0.00001846
Iteration 99/1000 | Loss: 0.00001846
Iteration 100/1000 | Loss: 0.00001846
Iteration 101/1000 | Loss: 0.00001845
Iteration 102/1000 | Loss: 0.00001845
Iteration 103/1000 | Loss: 0.00001845
Iteration 104/1000 | Loss: 0.00001845
Iteration 105/1000 | Loss: 0.00001845
Iteration 106/1000 | Loss: 0.00001845
Iteration 107/1000 | Loss: 0.00001845
Iteration 108/1000 | Loss: 0.00001845
Iteration 109/1000 | Loss: 0.00001845
Iteration 110/1000 | Loss: 0.00001845
Iteration 111/1000 | Loss: 0.00001844
Iteration 112/1000 | Loss: 0.00001844
Iteration 113/1000 | Loss: 0.00001844
Iteration 114/1000 | Loss: 0.00001844
Iteration 115/1000 | Loss: 0.00001844
Iteration 116/1000 | Loss: 0.00001844
Iteration 117/1000 | Loss: 0.00001844
Iteration 118/1000 | Loss: 0.00001844
Iteration 119/1000 | Loss: 0.00001844
Iteration 120/1000 | Loss: 0.00001844
Iteration 121/1000 | Loss: 0.00001844
Iteration 122/1000 | Loss: 0.00001844
Iteration 123/1000 | Loss: 0.00001844
Iteration 124/1000 | Loss: 0.00001844
Iteration 125/1000 | Loss: 0.00001844
Iteration 126/1000 | Loss: 0.00001844
Iteration 127/1000 | Loss: 0.00001843
Iteration 128/1000 | Loss: 0.00001843
Iteration 129/1000 | Loss: 0.00001843
Iteration 130/1000 | Loss: 0.00001843
Iteration 131/1000 | Loss: 0.00001843
Iteration 132/1000 | Loss: 0.00001843
Iteration 133/1000 | Loss: 0.00001843
Iteration 134/1000 | Loss: 0.00001843
Iteration 135/1000 | Loss: 0.00001843
Iteration 136/1000 | Loss: 0.00001843
Iteration 137/1000 | Loss: 0.00001843
Iteration 138/1000 | Loss: 0.00001843
Iteration 139/1000 | Loss: 0.00001843
Iteration 140/1000 | Loss: 0.00001843
Iteration 141/1000 | Loss: 0.00001843
Iteration 142/1000 | Loss: 0.00001843
Iteration 143/1000 | Loss: 0.00001843
Iteration 144/1000 | Loss: 0.00001843
Iteration 145/1000 | Loss: 0.00001843
Iteration 146/1000 | Loss: 0.00001843
Iteration 147/1000 | Loss: 0.00001843
Iteration 148/1000 | Loss: 0.00001843
Iteration 149/1000 | Loss: 0.00001843
Iteration 150/1000 | Loss: 0.00001843
Iteration 151/1000 | Loss: 0.00001843
Iteration 152/1000 | Loss: 0.00001843
Iteration 153/1000 | Loss: 0.00001843
Iteration 154/1000 | Loss: 0.00001843
Iteration 155/1000 | Loss: 0.00001843
Iteration 156/1000 | Loss: 0.00001843
Iteration 157/1000 | Loss: 0.00001843
Iteration 158/1000 | Loss: 0.00001843
Iteration 159/1000 | Loss: 0.00001843
Iteration 160/1000 | Loss: 0.00001843
Iteration 161/1000 | Loss: 0.00001843
Iteration 162/1000 | Loss: 0.00001843
Iteration 163/1000 | Loss: 0.00001843
Iteration 164/1000 | Loss: 0.00001843
Iteration 165/1000 | Loss: 0.00001843
Iteration 166/1000 | Loss: 0.00001843
Iteration 167/1000 | Loss: 0.00001843
Iteration 168/1000 | Loss: 0.00001843
Iteration 169/1000 | Loss: 0.00001843
Iteration 170/1000 | Loss: 0.00001843
Iteration 171/1000 | Loss: 0.00001843
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.843134123191703e-05, 1.843134123191703e-05, 1.843134123191703e-05, 1.843134123191703e-05, 1.843134123191703e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.843134123191703e-05

Optimization complete. Final v2v error: 3.483079433441162 mm

Highest mean error: 4.068371295928955 mm for frame 47

Lowest mean error: 2.916198253631592 mm for frame 100

Saving results

Total time: 45.58331036567688
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00447334
Iteration 2/25 | Loss: 0.00084034
Iteration 3/25 | Loss: 0.00066270
Iteration 4/25 | Loss: 0.00064493
Iteration 5/25 | Loss: 0.00063874
Iteration 6/25 | Loss: 0.00063701
Iteration 7/25 | Loss: 0.00063690
Iteration 8/25 | Loss: 0.00063690
Iteration 9/25 | Loss: 0.00063690
Iteration 10/25 | Loss: 0.00063690
Iteration 11/25 | Loss: 0.00063690
Iteration 12/25 | Loss: 0.00063690
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006369004840962589, 0.0006369004840962589, 0.0006369004840962589, 0.0006369004840962589, 0.0006369004840962589]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006369004840962589

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44573414
Iteration 2/25 | Loss: 0.00035178
Iteration 3/25 | Loss: 0.00035178
Iteration 4/25 | Loss: 0.00035178
Iteration 5/25 | Loss: 0.00035178
Iteration 6/25 | Loss: 0.00035178
Iteration 7/25 | Loss: 0.00035178
Iteration 8/25 | Loss: 0.00035178
Iteration 9/25 | Loss: 0.00035178
Iteration 10/25 | Loss: 0.00035178
Iteration 11/25 | Loss: 0.00035178
Iteration 12/25 | Loss: 0.00035178
Iteration 13/25 | Loss: 0.00035178
Iteration 14/25 | Loss: 0.00035178
Iteration 15/25 | Loss: 0.00035178
Iteration 16/25 | Loss: 0.00035178
Iteration 17/25 | Loss: 0.00035178
Iteration 18/25 | Loss: 0.00035178
Iteration 19/25 | Loss: 0.00035178
Iteration 20/25 | Loss: 0.00035178
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00035177840618416667, 0.00035177840618416667, 0.00035177840618416667, 0.00035177840618416667, 0.00035177840618416667]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00035177840618416667

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00035178
Iteration 2/1000 | Loss: 0.00002097
Iteration 3/1000 | Loss: 0.00001386
Iteration 4/1000 | Loss: 0.00001229
Iteration 5/1000 | Loss: 0.00001172
Iteration 6/1000 | Loss: 0.00001151
Iteration 7/1000 | Loss: 0.00001143
Iteration 8/1000 | Loss: 0.00001142
Iteration 9/1000 | Loss: 0.00001142
Iteration 10/1000 | Loss: 0.00001142
Iteration 11/1000 | Loss: 0.00001142
Iteration 12/1000 | Loss: 0.00001142
Iteration 13/1000 | Loss: 0.00001139
Iteration 14/1000 | Loss: 0.00001136
Iteration 15/1000 | Loss: 0.00001131
Iteration 16/1000 | Loss: 0.00001126
Iteration 17/1000 | Loss: 0.00001126
Iteration 18/1000 | Loss: 0.00001125
Iteration 19/1000 | Loss: 0.00001125
Iteration 20/1000 | Loss: 0.00001124
Iteration 21/1000 | Loss: 0.00001124
Iteration 22/1000 | Loss: 0.00001122
Iteration 23/1000 | Loss: 0.00001122
Iteration 24/1000 | Loss: 0.00001122
Iteration 25/1000 | Loss: 0.00001121
Iteration 26/1000 | Loss: 0.00001121
Iteration 27/1000 | Loss: 0.00001121
Iteration 28/1000 | Loss: 0.00001121
Iteration 29/1000 | Loss: 0.00001121
Iteration 30/1000 | Loss: 0.00001121
Iteration 31/1000 | Loss: 0.00001121
Iteration 32/1000 | Loss: 0.00001121
Iteration 33/1000 | Loss: 0.00001120
Iteration 34/1000 | Loss: 0.00001120
Iteration 35/1000 | Loss: 0.00001120
Iteration 36/1000 | Loss: 0.00001120
Iteration 37/1000 | Loss: 0.00001120
Iteration 38/1000 | Loss: 0.00001120
Iteration 39/1000 | Loss: 0.00001120
Iteration 40/1000 | Loss: 0.00001120
Iteration 41/1000 | Loss: 0.00001120
Iteration 42/1000 | Loss: 0.00001120
Iteration 43/1000 | Loss: 0.00001120
Iteration 44/1000 | Loss: 0.00001120
Iteration 45/1000 | Loss: 0.00001119
Iteration 46/1000 | Loss: 0.00001119
Iteration 47/1000 | Loss: 0.00001119
Iteration 48/1000 | Loss: 0.00001119
Iteration 49/1000 | Loss: 0.00001119
Iteration 50/1000 | Loss: 0.00001119
Iteration 51/1000 | Loss: 0.00001118
Iteration 52/1000 | Loss: 0.00001118
Iteration 53/1000 | Loss: 0.00001118
Iteration 54/1000 | Loss: 0.00001117
Iteration 55/1000 | Loss: 0.00001117
Iteration 56/1000 | Loss: 0.00001117
Iteration 57/1000 | Loss: 0.00001116
Iteration 58/1000 | Loss: 0.00001116
Iteration 59/1000 | Loss: 0.00001116
Iteration 60/1000 | Loss: 0.00001115
Iteration 61/1000 | Loss: 0.00001114
Iteration 62/1000 | Loss: 0.00001114
Iteration 63/1000 | Loss: 0.00001114
Iteration 64/1000 | Loss: 0.00001113
Iteration 65/1000 | Loss: 0.00001113
Iteration 66/1000 | Loss: 0.00001113
Iteration 67/1000 | Loss: 0.00001113
Iteration 68/1000 | Loss: 0.00001113
Iteration 69/1000 | Loss: 0.00001113
Iteration 70/1000 | Loss: 0.00001113
Iteration 71/1000 | Loss: 0.00001113
Iteration 72/1000 | Loss: 0.00001113
Iteration 73/1000 | Loss: 0.00001113
Iteration 74/1000 | Loss: 0.00001112
Iteration 75/1000 | Loss: 0.00001112
Iteration 76/1000 | Loss: 0.00001112
Iteration 77/1000 | Loss: 0.00001112
Iteration 78/1000 | Loss: 0.00001112
Iteration 79/1000 | Loss: 0.00001112
Iteration 80/1000 | Loss: 0.00001112
Iteration 81/1000 | Loss: 0.00001111
Iteration 82/1000 | Loss: 0.00001111
Iteration 83/1000 | Loss: 0.00001111
Iteration 84/1000 | Loss: 0.00001111
Iteration 85/1000 | Loss: 0.00001111
Iteration 86/1000 | Loss: 0.00001111
Iteration 87/1000 | Loss: 0.00001111
Iteration 88/1000 | Loss: 0.00001111
Iteration 89/1000 | Loss: 0.00001111
Iteration 90/1000 | Loss: 0.00001111
Iteration 91/1000 | Loss: 0.00001111
Iteration 92/1000 | Loss: 0.00001111
Iteration 93/1000 | Loss: 0.00001111
Iteration 94/1000 | Loss: 0.00001111
Iteration 95/1000 | Loss: 0.00001111
Iteration 96/1000 | Loss: 0.00001111
Iteration 97/1000 | Loss: 0.00001111
Iteration 98/1000 | Loss: 0.00001111
Iteration 99/1000 | Loss: 0.00001111
Iteration 100/1000 | Loss: 0.00001111
Iteration 101/1000 | Loss: 0.00001111
Iteration 102/1000 | Loss: 0.00001111
Iteration 103/1000 | Loss: 0.00001111
Iteration 104/1000 | Loss: 0.00001111
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [1.1105596968263853e-05, 1.1105596968263853e-05, 1.1105596968263853e-05, 1.1105596968263853e-05, 1.1105596968263853e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1105596968263853e-05

Optimization complete. Final v2v error: 2.820390224456787 mm

Highest mean error: 3.0373685359954834 mm for frame 214

Lowest mean error: 2.678377628326416 mm for frame 48

Saving results

Total time: 28.44313359260559
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01050506
Iteration 2/25 | Loss: 0.00205785
Iteration 3/25 | Loss: 0.00155557
Iteration 4/25 | Loss: 0.00114404
Iteration 5/25 | Loss: 0.00176438
Iteration 6/25 | Loss: 0.00092029
Iteration 7/25 | Loss: 0.00086083
Iteration 8/25 | Loss: 0.00082726
Iteration 9/25 | Loss: 0.00080431
Iteration 10/25 | Loss: 0.00077671
Iteration 11/25 | Loss: 0.00077266
Iteration 12/25 | Loss: 0.00076340
Iteration 13/25 | Loss: 0.00076581
Iteration 14/25 | Loss: 0.00075933
Iteration 15/25 | Loss: 0.00075755
Iteration 16/25 | Loss: 0.00075692
Iteration 17/25 | Loss: 0.00075677
Iteration 18/25 | Loss: 0.00075672
Iteration 19/25 | Loss: 0.00075672
Iteration 20/25 | Loss: 0.00075672
Iteration 21/25 | Loss: 0.00075671
Iteration 22/25 | Loss: 0.00075670
Iteration 23/25 | Loss: 0.00075670
Iteration 24/25 | Loss: 0.00075670
Iteration 25/25 | Loss: 0.00075670

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45034790
Iteration 2/25 | Loss: 0.00038891
Iteration 3/25 | Loss: 0.00038891
Iteration 4/25 | Loss: 0.00038891
Iteration 5/25 | Loss: 0.00043949
Iteration 6/25 | Loss: 0.00038891
Iteration 7/25 | Loss: 0.00038891
Iteration 8/25 | Loss: 0.00038891
Iteration 9/25 | Loss: 0.00038891
Iteration 10/25 | Loss: 0.00038891
Iteration 11/25 | Loss: 0.00038891
Iteration 12/25 | Loss: 0.00038891
Iteration 13/25 | Loss: 0.00038891
Iteration 14/25 | Loss: 0.00038891
Iteration 15/25 | Loss: 0.00038891
Iteration 16/25 | Loss: 0.00038891
Iteration 17/25 | Loss: 0.00038891
Iteration 18/25 | Loss: 0.00038891
Iteration 19/25 | Loss: 0.00038891
Iteration 20/25 | Loss: 0.00038891
Iteration 21/25 | Loss: 0.00038891
Iteration 22/25 | Loss: 0.00038891
Iteration 23/25 | Loss: 0.00038891
Iteration 24/25 | Loss: 0.00038891
Iteration 25/25 | Loss: 0.00038891

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038891
Iteration 2/1000 | Loss: 0.00009290
Iteration 3/1000 | Loss: 0.00003881
Iteration 4/1000 | Loss: 0.00003347
Iteration 5/1000 | Loss: 0.00014494
Iteration 6/1000 | Loss: 0.00002915
Iteration 7/1000 | Loss: 0.00002790
Iteration 8/1000 | Loss: 0.00006076
Iteration 9/1000 | Loss: 0.00002642
Iteration 10/1000 | Loss: 0.00034858
Iteration 11/1000 | Loss: 0.00002869
Iteration 12/1000 | Loss: 0.00002488
Iteration 13/1000 | Loss: 0.00002281
Iteration 14/1000 | Loss: 0.00002137
Iteration 15/1000 | Loss: 0.00002045
Iteration 16/1000 | Loss: 0.00002003
Iteration 17/1000 | Loss: 0.00001981
Iteration 18/1000 | Loss: 0.00001961
Iteration 19/1000 | Loss: 0.00001960
Iteration 20/1000 | Loss: 0.00001959
Iteration 21/1000 | Loss: 0.00001958
Iteration 22/1000 | Loss: 0.00001953
Iteration 23/1000 | Loss: 0.00001953
Iteration 24/1000 | Loss: 0.00001952
Iteration 25/1000 | Loss: 0.00001952
Iteration 26/1000 | Loss: 0.00001952
Iteration 27/1000 | Loss: 0.00001951
Iteration 28/1000 | Loss: 0.00001948
Iteration 29/1000 | Loss: 0.00001948
Iteration 30/1000 | Loss: 0.00001948
Iteration 31/1000 | Loss: 0.00001948
Iteration 32/1000 | Loss: 0.00001946
Iteration 33/1000 | Loss: 0.00001946
Iteration 34/1000 | Loss: 0.00001946
Iteration 35/1000 | Loss: 0.00001946
Iteration 36/1000 | Loss: 0.00001946
Iteration 37/1000 | Loss: 0.00001946
Iteration 38/1000 | Loss: 0.00001946
Iteration 39/1000 | Loss: 0.00001946
Iteration 40/1000 | Loss: 0.00001946
Iteration 41/1000 | Loss: 0.00001946
Iteration 42/1000 | Loss: 0.00001946
Iteration 43/1000 | Loss: 0.00001945
Iteration 44/1000 | Loss: 0.00001945
Iteration 45/1000 | Loss: 0.00001943
Iteration 46/1000 | Loss: 0.00001943
Iteration 47/1000 | Loss: 0.00001943
Iteration 48/1000 | Loss: 0.00001943
Iteration 49/1000 | Loss: 0.00001943
Iteration 50/1000 | Loss: 0.00001943
Iteration 51/1000 | Loss: 0.00001943
Iteration 52/1000 | Loss: 0.00001942
Iteration 53/1000 | Loss: 0.00001942
Iteration 54/1000 | Loss: 0.00001942
Iteration 55/1000 | Loss: 0.00001942
Iteration 56/1000 | Loss: 0.00001942
Iteration 57/1000 | Loss: 0.00001942
Iteration 58/1000 | Loss: 0.00001942
Iteration 59/1000 | Loss: 0.00001941
Iteration 60/1000 | Loss: 0.00001941
Iteration 61/1000 | Loss: 0.00001941
Iteration 62/1000 | Loss: 0.00001941
Iteration 63/1000 | Loss: 0.00001941
Iteration 64/1000 | Loss: 0.00001940
Iteration 65/1000 | Loss: 0.00001940
Iteration 66/1000 | Loss: 0.00001940
Iteration 67/1000 | Loss: 0.00001940
Iteration 68/1000 | Loss: 0.00001940
Iteration 69/1000 | Loss: 0.00001940
Iteration 70/1000 | Loss: 0.00001940
Iteration 71/1000 | Loss: 0.00001940
Iteration 72/1000 | Loss: 0.00001939
Iteration 73/1000 | Loss: 0.00001939
Iteration 74/1000 | Loss: 0.00001938
Iteration 75/1000 | Loss: 0.00001938
Iteration 76/1000 | Loss: 0.00001938
Iteration 77/1000 | Loss: 0.00001937
Iteration 78/1000 | Loss: 0.00001937
Iteration 79/1000 | Loss: 0.00001937
Iteration 80/1000 | Loss: 0.00001937
Iteration 81/1000 | Loss: 0.00001937
Iteration 82/1000 | Loss: 0.00001937
Iteration 83/1000 | Loss: 0.00001937
Iteration 84/1000 | Loss: 0.00001937
Iteration 85/1000 | Loss: 0.00001937
Iteration 86/1000 | Loss: 0.00001937
Iteration 87/1000 | Loss: 0.00001937
Iteration 88/1000 | Loss: 0.00001937
Iteration 89/1000 | Loss: 0.00001937
Iteration 90/1000 | Loss: 0.00001937
Iteration 91/1000 | Loss: 0.00001937
Iteration 92/1000 | Loss: 0.00001937
Iteration 93/1000 | Loss: 0.00001937
Iteration 94/1000 | Loss: 0.00001937
Iteration 95/1000 | Loss: 0.00001937
Iteration 96/1000 | Loss: 0.00001937
Iteration 97/1000 | Loss: 0.00001937
Iteration 98/1000 | Loss: 0.00001937
Iteration 99/1000 | Loss: 0.00001936
Iteration 100/1000 | Loss: 0.00001936
Iteration 101/1000 | Loss: 0.00001936
Iteration 102/1000 | Loss: 0.00001936
Iteration 103/1000 | Loss: 0.00001936
Iteration 104/1000 | Loss: 0.00001936
Iteration 105/1000 | Loss: 0.00001936
Iteration 106/1000 | Loss: 0.00001936
Iteration 107/1000 | Loss: 0.00001936
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [1.9364555555512197e-05, 1.9364555555512197e-05, 1.9364555555512197e-05, 1.9364555555512197e-05, 1.9364555555512197e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9364555555512197e-05

Optimization complete. Final v2v error: 3.637308359146118 mm

Highest mean error: 4.140707015991211 mm for frame 0

Lowest mean error: 3.4920296669006348 mm for frame 108

Saving results

Total time: 61.147483825683594
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00814755
Iteration 2/25 | Loss: 0.00086502
Iteration 3/25 | Loss: 0.00065637
Iteration 4/25 | Loss: 0.00061985
Iteration 5/25 | Loss: 0.00061288
Iteration 6/25 | Loss: 0.00061023
Iteration 7/25 | Loss: 0.00060993
Iteration 8/25 | Loss: 0.00060993
Iteration 9/25 | Loss: 0.00060993
Iteration 10/25 | Loss: 0.00060993
Iteration 11/25 | Loss: 0.00060993
Iteration 12/25 | Loss: 0.00060993
Iteration 13/25 | Loss: 0.00060993
Iteration 14/25 | Loss: 0.00060993
Iteration 15/25 | Loss: 0.00060993
Iteration 16/25 | Loss: 0.00060993
Iteration 17/25 | Loss: 0.00060993
Iteration 18/25 | Loss: 0.00060993
Iteration 19/25 | Loss: 0.00060993
Iteration 20/25 | Loss: 0.00060993
Iteration 21/25 | Loss: 0.00060993
Iteration 22/25 | Loss: 0.00060993
Iteration 23/25 | Loss: 0.00060993
Iteration 24/25 | Loss: 0.00060993
Iteration 25/25 | Loss: 0.00060993
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006099313031882048, 0.0006099313031882048, 0.0006099313031882048, 0.0006099313031882048, 0.0006099313031882048]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006099313031882048

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46845388
Iteration 2/25 | Loss: 0.00033244
Iteration 3/25 | Loss: 0.00033244
Iteration 4/25 | Loss: 0.00033244
Iteration 5/25 | Loss: 0.00033244
Iteration 6/25 | Loss: 0.00033244
Iteration 7/25 | Loss: 0.00033244
Iteration 8/25 | Loss: 0.00033244
Iteration 9/25 | Loss: 0.00033244
Iteration 10/25 | Loss: 0.00033244
Iteration 11/25 | Loss: 0.00033244
Iteration 12/25 | Loss: 0.00033244
Iteration 13/25 | Loss: 0.00033244
Iteration 14/25 | Loss: 0.00033244
Iteration 15/25 | Loss: 0.00033244
Iteration 16/25 | Loss: 0.00033244
Iteration 17/25 | Loss: 0.00033244
Iteration 18/25 | Loss: 0.00033244
Iteration 19/25 | Loss: 0.00033244
Iteration 20/25 | Loss: 0.00033244
Iteration 21/25 | Loss: 0.00033244
Iteration 22/25 | Loss: 0.00033244
Iteration 23/25 | Loss: 0.00033244
Iteration 24/25 | Loss: 0.00033244
Iteration 25/25 | Loss: 0.00033244

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033244
Iteration 2/1000 | Loss: 0.00002011
Iteration 3/1000 | Loss: 0.00001393
Iteration 4/1000 | Loss: 0.00001306
Iteration 5/1000 | Loss: 0.00001234
Iteration 6/1000 | Loss: 0.00001184
Iteration 7/1000 | Loss: 0.00001169
Iteration 8/1000 | Loss: 0.00001142
Iteration 9/1000 | Loss: 0.00001123
Iteration 10/1000 | Loss: 0.00001121
Iteration 11/1000 | Loss: 0.00001111
Iteration 12/1000 | Loss: 0.00001105
Iteration 13/1000 | Loss: 0.00001102
Iteration 14/1000 | Loss: 0.00001094
Iteration 15/1000 | Loss: 0.00001091
Iteration 16/1000 | Loss: 0.00001090
Iteration 17/1000 | Loss: 0.00001090
Iteration 18/1000 | Loss: 0.00001089
Iteration 19/1000 | Loss: 0.00001087
Iteration 20/1000 | Loss: 0.00001086
Iteration 21/1000 | Loss: 0.00001085
Iteration 22/1000 | Loss: 0.00001084
Iteration 23/1000 | Loss: 0.00001080
Iteration 24/1000 | Loss: 0.00001079
Iteration 25/1000 | Loss: 0.00001078
Iteration 26/1000 | Loss: 0.00001078
Iteration 27/1000 | Loss: 0.00001078
Iteration 28/1000 | Loss: 0.00001078
Iteration 29/1000 | Loss: 0.00001077
Iteration 30/1000 | Loss: 0.00001077
Iteration 31/1000 | Loss: 0.00001077
Iteration 32/1000 | Loss: 0.00001077
Iteration 33/1000 | Loss: 0.00001077
Iteration 34/1000 | Loss: 0.00001076
Iteration 35/1000 | Loss: 0.00001076
Iteration 36/1000 | Loss: 0.00001076
Iteration 37/1000 | Loss: 0.00001075
Iteration 38/1000 | Loss: 0.00001075
Iteration 39/1000 | Loss: 0.00001074
Iteration 40/1000 | Loss: 0.00001074
Iteration 41/1000 | Loss: 0.00001074
Iteration 42/1000 | Loss: 0.00001074
Iteration 43/1000 | Loss: 0.00001074
Iteration 44/1000 | Loss: 0.00001073
Iteration 45/1000 | Loss: 0.00001073
Iteration 46/1000 | Loss: 0.00001073
Iteration 47/1000 | Loss: 0.00001072
Iteration 48/1000 | Loss: 0.00001072
Iteration 49/1000 | Loss: 0.00001072
Iteration 50/1000 | Loss: 0.00001072
Iteration 51/1000 | Loss: 0.00001071
Iteration 52/1000 | Loss: 0.00001070
Iteration 53/1000 | Loss: 0.00001069
Iteration 54/1000 | Loss: 0.00001069
Iteration 55/1000 | Loss: 0.00001068
Iteration 56/1000 | Loss: 0.00001068
Iteration 57/1000 | Loss: 0.00001067
Iteration 58/1000 | Loss: 0.00001066
Iteration 59/1000 | Loss: 0.00001066
Iteration 60/1000 | Loss: 0.00001066
Iteration 61/1000 | Loss: 0.00001065
Iteration 62/1000 | Loss: 0.00001065
Iteration 63/1000 | Loss: 0.00001065
Iteration 64/1000 | Loss: 0.00001065
Iteration 65/1000 | Loss: 0.00001064
Iteration 66/1000 | Loss: 0.00001064
Iteration 67/1000 | Loss: 0.00001063
Iteration 68/1000 | Loss: 0.00001063
Iteration 69/1000 | Loss: 0.00001063
Iteration 70/1000 | Loss: 0.00001063
Iteration 71/1000 | Loss: 0.00001063
Iteration 72/1000 | Loss: 0.00001062
Iteration 73/1000 | Loss: 0.00001062
Iteration 74/1000 | Loss: 0.00001062
Iteration 75/1000 | Loss: 0.00001062
Iteration 76/1000 | Loss: 0.00001061
Iteration 77/1000 | Loss: 0.00001061
Iteration 78/1000 | Loss: 0.00001061
Iteration 79/1000 | Loss: 0.00001061
Iteration 80/1000 | Loss: 0.00001061
Iteration 81/1000 | Loss: 0.00001061
Iteration 82/1000 | Loss: 0.00001061
Iteration 83/1000 | Loss: 0.00001061
Iteration 84/1000 | Loss: 0.00001061
Iteration 85/1000 | Loss: 0.00001060
Iteration 86/1000 | Loss: 0.00001060
Iteration 87/1000 | Loss: 0.00001060
Iteration 88/1000 | Loss: 0.00001060
Iteration 89/1000 | Loss: 0.00001060
Iteration 90/1000 | Loss: 0.00001060
Iteration 91/1000 | Loss: 0.00001060
Iteration 92/1000 | Loss: 0.00001060
Iteration 93/1000 | Loss: 0.00001060
Iteration 94/1000 | Loss: 0.00001060
Iteration 95/1000 | Loss: 0.00001060
Iteration 96/1000 | Loss: 0.00001060
Iteration 97/1000 | Loss: 0.00001060
Iteration 98/1000 | Loss: 0.00001059
Iteration 99/1000 | Loss: 0.00001059
Iteration 100/1000 | Loss: 0.00001059
Iteration 101/1000 | Loss: 0.00001059
Iteration 102/1000 | Loss: 0.00001059
Iteration 103/1000 | Loss: 0.00001059
Iteration 104/1000 | Loss: 0.00001059
Iteration 105/1000 | Loss: 0.00001059
Iteration 106/1000 | Loss: 0.00001059
Iteration 107/1000 | Loss: 0.00001059
Iteration 108/1000 | Loss: 0.00001059
Iteration 109/1000 | Loss: 0.00001059
Iteration 110/1000 | Loss: 0.00001059
Iteration 111/1000 | Loss: 0.00001058
Iteration 112/1000 | Loss: 0.00001058
Iteration 113/1000 | Loss: 0.00001058
Iteration 114/1000 | Loss: 0.00001058
Iteration 115/1000 | Loss: 0.00001058
Iteration 116/1000 | Loss: 0.00001058
Iteration 117/1000 | Loss: 0.00001058
Iteration 118/1000 | Loss: 0.00001058
Iteration 119/1000 | Loss: 0.00001058
Iteration 120/1000 | Loss: 0.00001058
Iteration 121/1000 | Loss: 0.00001058
Iteration 122/1000 | Loss: 0.00001058
Iteration 123/1000 | Loss: 0.00001058
Iteration 124/1000 | Loss: 0.00001058
Iteration 125/1000 | Loss: 0.00001057
Iteration 126/1000 | Loss: 0.00001057
Iteration 127/1000 | Loss: 0.00001057
Iteration 128/1000 | Loss: 0.00001057
Iteration 129/1000 | Loss: 0.00001057
Iteration 130/1000 | Loss: 0.00001057
Iteration 131/1000 | Loss: 0.00001057
Iteration 132/1000 | Loss: 0.00001057
Iteration 133/1000 | Loss: 0.00001057
Iteration 134/1000 | Loss: 0.00001057
Iteration 135/1000 | Loss: 0.00001057
Iteration 136/1000 | Loss: 0.00001057
Iteration 137/1000 | Loss: 0.00001057
Iteration 138/1000 | Loss: 0.00001057
Iteration 139/1000 | Loss: 0.00001057
Iteration 140/1000 | Loss: 0.00001057
Iteration 141/1000 | Loss: 0.00001057
Iteration 142/1000 | Loss: 0.00001056
Iteration 143/1000 | Loss: 0.00001056
Iteration 144/1000 | Loss: 0.00001056
Iteration 145/1000 | Loss: 0.00001056
Iteration 146/1000 | Loss: 0.00001056
Iteration 147/1000 | Loss: 0.00001056
Iteration 148/1000 | Loss: 0.00001055
Iteration 149/1000 | Loss: 0.00001055
Iteration 150/1000 | Loss: 0.00001055
Iteration 151/1000 | Loss: 0.00001055
Iteration 152/1000 | Loss: 0.00001055
Iteration 153/1000 | Loss: 0.00001055
Iteration 154/1000 | Loss: 0.00001055
Iteration 155/1000 | Loss: 0.00001055
Iteration 156/1000 | Loss: 0.00001055
Iteration 157/1000 | Loss: 0.00001055
Iteration 158/1000 | Loss: 0.00001055
Iteration 159/1000 | Loss: 0.00001055
Iteration 160/1000 | Loss: 0.00001055
Iteration 161/1000 | Loss: 0.00001055
Iteration 162/1000 | Loss: 0.00001055
Iteration 163/1000 | Loss: 0.00001055
Iteration 164/1000 | Loss: 0.00001055
Iteration 165/1000 | Loss: 0.00001054
Iteration 166/1000 | Loss: 0.00001054
Iteration 167/1000 | Loss: 0.00001054
Iteration 168/1000 | Loss: 0.00001054
Iteration 169/1000 | Loss: 0.00001054
Iteration 170/1000 | Loss: 0.00001054
Iteration 171/1000 | Loss: 0.00001054
Iteration 172/1000 | Loss: 0.00001054
Iteration 173/1000 | Loss: 0.00001054
Iteration 174/1000 | Loss: 0.00001054
Iteration 175/1000 | Loss: 0.00001054
Iteration 176/1000 | Loss: 0.00001054
Iteration 177/1000 | Loss: 0.00001054
Iteration 178/1000 | Loss: 0.00001054
Iteration 179/1000 | Loss: 0.00001054
Iteration 180/1000 | Loss: 0.00001054
Iteration 181/1000 | Loss: 0.00001054
Iteration 182/1000 | Loss: 0.00001054
Iteration 183/1000 | Loss: 0.00001054
Iteration 184/1000 | Loss: 0.00001054
Iteration 185/1000 | Loss: 0.00001054
Iteration 186/1000 | Loss: 0.00001054
Iteration 187/1000 | Loss: 0.00001054
Iteration 188/1000 | Loss: 0.00001054
Iteration 189/1000 | Loss: 0.00001054
Iteration 190/1000 | Loss: 0.00001054
Iteration 191/1000 | Loss: 0.00001054
Iteration 192/1000 | Loss: 0.00001054
Iteration 193/1000 | Loss: 0.00001054
Iteration 194/1000 | Loss: 0.00001054
Iteration 195/1000 | Loss: 0.00001054
Iteration 196/1000 | Loss: 0.00001054
Iteration 197/1000 | Loss: 0.00001054
Iteration 198/1000 | Loss: 0.00001054
Iteration 199/1000 | Loss: 0.00001054
Iteration 200/1000 | Loss: 0.00001054
Iteration 201/1000 | Loss: 0.00001054
Iteration 202/1000 | Loss: 0.00001054
Iteration 203/1000 | Loss: 0.00001054
Iteration 204/1000 | Loss: 0.00001054
Iteration 205/1000 | Loss: 0.00001054
Iteration 206/1000 | Loss: 0.00001054
Iteration 207/1000 | Loss: 0.00001054
Iteration 208/1000 | Loss: 0.00001054
Iteration 209/1000 | Loss: 0.00001054
Iteration 210/1000 | Loss: 0.00001054
Iteration 211/1000 | Loss: 0.00001054
Iteration 212/1000 | Loss: 0.00001054
Iteration 213/1000 | Loss: 0.00001054
Iteration 214/1000 | Loss: 0.00001054
Iteration 215/1000 | Loss: 0.00001054
Iteration 216/1000 | Loss: 0.00001054
Iteration 217/1000 | Loss: 0.00001054
Iteration 218/1000 | Loss: 0.00001054
Iteration 219/1000 | Loss: 0.00001054
Iteration 220/1000 | Loss: 0.00001054
Iteration 221/1000 | Loss: 0.00001054
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 221. Stopping optimization.
Last 5 losses: [1.0536236914049368e-05, 1.0536236914049368e-05, 1.0536236914049368e-05, 1.0536236914049368e-05, 1.0536236914049368e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0536236914049368e-05

Optimization complete. Final v2v error: 2.733637571334839 mm

Highest mean error: 3.0727686882019043 mm for frame 100

Lowest mean error: 2.560044050216675 mm for frame 214

Saving results

Total time: 42.640260219573975
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01033674
Iteration 2/25 | Loss: 0.00217249
Iteration 3/25 | Loss: 0.00136630
Iteration 4/25 | Loss: 0.00106482
Iteration 5/25 | Loss: 0.00117652
Iteration 6/25 | Loss: 0.00107764
Iteration 7/25 | Loss: 0.00094096
Iteration 8/25 | Loss: 0.00083390
Iteration 9/25 | Loss: 0.00075576
Iteration 10/25 | Loss: 0.00074088
Iteration 11/25 | Loss: 0.00072211
Iteration 12/25 | Loss: 0.00072215
Iteration 13/25 | Loss: 0.00071135
Iteration 14/25 | Loss: 0.00070562
Iteration 15/25 | Loss: 0.00069339
Iteration 16/25 | Loss: 0.00069378
Iteration 17/25 | Loss: 0.00069661
Iteration 18/25 | Loss: 0.00069511
Iteration 19/25 | Loss: 0.00069513
Iteration 20/25 | Loss: 0.00068818
Iteration 21/25 | Loss: 0.00067523
Iteration 22/25 | Loss: 0.00067401
Iteration 23/25 | Loss: 0.00067899
Iteration 24/25 | Loss: 0.00066707
Iteration 25/25 | Loss: 0.00066329

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51091444
Iteration 2/25 | Loss: 0.00045715
Iteration 3/25 | Loss: 0.00045715
Iteration 4/25 | Loss: 0.00045715
Iteration 5/25 | Loss: 0.00045715
Iteration 6/25 | Loss: 0.00045715
Iteration 7/25 | Loss: 0.00045715
Iteration 8/25 | Loss: 0.00045715
Iteration 9/25 | Loss: 0.00045715
Iteration 10/25 | Loss: 0.00045715
Iteration 11/25 | Loss: 0.00045715
Iteration 12/25 | Loss: 0.00045715
Iteration 13/25 | Loss: 0.00045715
Iteration 14/25 | Loss: 0.00045715
Iteration 15/25 | Loss: 0.00045715
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.00045714914449490607, 0.00045714914449490607, 0.00045714914449490607, 0.00045714914449490607, 0.00045714914449490607]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00045714914449490607

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045715
Iteration 2/1000 | Loss: 0.00004081
Iteration 3/1000 | Loss: 0.00003998
Iteration 4/1000 | Loss: 0.00002690
Iteration 5/1000 | Loss: 0.00001989
Iteration 6/1000 | Loss: 0.00001898
Iteration 7/1000 | Loss: 0.00001834
Iteration 8/1000 | Loss: 0.00020343
Iteration 9/1000 | Loss: 0.00002323
Iteration 10/1000 | Loss: 0.00004039
Iteration 11/1000 | Loss: 0.00001744
Iteration 12/1000 | Loss: 0.00003727
Iteration 13/1000 | Loss: 0.00001570
Iteration 14/1000 | Loss: 0.00001501
Iteration 15/1000 | Loss: 0.00001468
Iteration 16/1000 | Loss: 0.00001455
Iteration 17/1000 | Loss: 0.00001453
Iteration 18/1000 | Loss: 0.00001446
Iteration 19/1000 | Loss: 0.00001444
Iteration 20/1000 | Loss: 0.00001441
Iteration 21/1000 | Loss: 0.00001440
Iteration 22/1000 | Loss: 0.00001437
Iteration 23/1000 | Loss: 0.00001436
Iteration 24/1000 | Loss: 0.00001435
Iteration 25/1000 | Loss: 0.00001434
Iteration 26/1000 | Loss: 0.00001434
Iteration 27/1000 | Loss: 0.00004202
Iteration 28/1000 | Loss: 0.00001416
Iteration 29/1000 | Loss: 0.00001412
Iteration 30/1000 | Loss: 0.00001406
Iteration 31/1000 | Loss: 0.00001405
Iteration 32/1000 | Loss: 0.00001404
Iteration 33/1000 | Loss: 0.00001404
Iteration 34/1000 | Loss: 0.00001404
Iteration 35/1000 | Loss: 0.00001404
Iteration 36/1000 | Loss: 0.00001404
Iteration 37/1000 | Loss: 0.00001404
Iteration 38/1000 | Loss: 0.00001404
Iteration 39/1000 | Loss: 0.00001403
Iteration 40/1000 | Loss: 0.00001402
Iteration 41/1000 | Loss: 0.00001401
Iteration 42/1000 | Loss: 0.00001401
Iteration 43/1000 | Loss: 0.00001400
Iteration 44/1000 | Loss: 0.00001400
Iteration 45/1000 | Loss: 0.00001400
Iteration 46/1000 | Loss: 0.00001399
Iteration 47/1000 | Loss: 0.00001399
Iteration 48/1000 | Loss: 0.00001398
Iteration 49/1000 | Loss: 0.00001398
Iteration 50/1000 | Loss: 0.00001398
Iteration 51/1000 | Loss: 0.00001398
Iteration 52/1000 | Loss: 0.00001397
Iteration 53/1000 | Loss: 0.00001397
Iteration 54/1000 | Loss: 0.00001397
Iteration 55/1000 | Loss: 0.00001397
Iteration 56/1000 | Loss: 0.00001397
Iteration 57/1000 | Loss: 0.00001397
Iteration 58/1000 | Loss: 0.00001397
Iteration 59/1000 | Loss: 0.00001397
Iteration 60/1000 | Loss: 0.00001396
Iteration 61/1000 | Loss: 0.00001396
Iteration 62/1000 | Loss: 0.00001396
Iteration 63/1000 | Loss: 0.00001396
Iteration 64/1000 | Loss: 0.00001396
Iteration 65/1000 | Loss: 0.00001396
Iteration 66/1000 | Loss: 0.00001396
Iteration 67/1000 | Loss: 0.00001396
Iteration 68/1000 | Loss: 0.00001395
Iteration 69/1000 | Loss: 0.00001395
Iteration 70/1000 | Loss: 0.00001395
Iteration 71/1000 | Loss: 0.00001395
Iteration 72/1000 | Loss: 0.00001395
Iteration 73/1000 | Loss: 0.00001395
Iteration 74/1000 | Loss: 0.00001395
Iteration 75/1000 | Loss: 0.00001394
Iteration 76/1000 | Loss: 0.00001394
Iteration 77/1000 | Loss: 0.00001394
Iteration 78/1000 | Loss: 0.00001394
Iteration 79/1000 | Loss: 0.00001394
Iteration 80/1000 | Loss: 0.00001394
Iteration 81/1000 | Loss: 0.00001394
Iteration 82/1000 | Loss: 0.00001393
Iteration 83/1000 | Loss: 0.00001393
Iteration 84/1000 | Loss: 0.00001393
Iteration 85/1000 | Loss: 0.00001393
Iteration 86/1000 | Loss: 0.00001393
Iteration 87/1000 | Loss: 0.00001393
Iteration 88/1000 | Loss: 0.00001393
Iteration 89/1000 | Loss: 0.00001393
Iteration 90/1000 | Loss: 0.00001393
Iteration 91/1000 | Loss: 0.00001393
Iteration 92/1000 | Loss: 0.00001393
Iteration 93/1000 | Loss: 0.00001393
Iteration 94/1000 | Loss: 0.00001393
Iteration 95/1000 | Loss: 0.00001393
Iteration 96/1000 | Loss: 0.00001393
Iteration 97/1000 | Loss: 0.00001393
Iteration 98/1000 | Loss: 0.00001392
Iteration 99/1000 | Loss: 0.00001392
Iteration 100/1000 | Loss: 0.00001392
Iteration 101/1000 | Loss: 0.00001392
Iteration 102/1000 | Loss: 0.00001392
Iteration 103/1000 | Loss: 0.00001392
Iteration 104/1000 | Loss: 0.00001392
Iteration 105/1000 | Loss: 0.00001392
Iteration 106/1000 | Loss: 0.00001392
Iteration 107/1000 | Loss: 0.00001392
Iteration 108/1000 | Loss: 0.00001392
Iteration 109/1000 | Loss: 0.00001392
Iteration 110/1000 | Loss: 0.00001392
Iteration 111/1000 | Loss: 0.00001392
Iteration 112/1000 | Loss: 0.00001391
Iteration 113/1000 | Loss: 0.00001391
Iteration 114/1000 | Loss: 0.00001391
Iteration 115/1000 | Loss: 0.00001391
Iteration 116/1000 | Loss: 0.00001391
Iteration 117/1000 | Loss: 0.00001391
Iteration 118/1000 | Loss: 0.00001391
Iteration 119/1000 | Loss: 0.00001391
Iteration 120/1000 | Loss: 0.00001390
Iteration 121/1000 | Loss: 0.00001390
Iteration 122/1000 | Loss: 0.00001390
Iteration 123/1000 | Loss: 0.00001390
Iteration 124/1000 | Loss: 0.00001390
Iteration 125/1000 | Loss: 0.00001390
Iteration 126/1000 | Loss: 0.00001390
Iteration 127/1000 | Loss: 0.00001390
Iteration 128/1000 | Loss: 0.00001390
Iteration 129/1000 | Loss: 0.00001390
Iteration 130/1000 | Loss: 0.00001389
Iteration 131/1000 | Loss: 0.00001389
Iteration 132/1000 | Loss: 0.00001389
Iteration 133/1000 | Loss: 0.00001389
Iteration 134/1000 | Loss: 0.00001389
Iteration 135/1000 | Loss: 0.00001389
Iteration 136/1000 | Loss: 0.00001388
Iteration 137/1000 | Loss: 0.00001388
Iteration 138/1000 | Loss: 0.00001388
Iteration 139/1000 | Loss: 0.00001388
Iteration 140/1000 | Loss: 0.00001388
Iteration 141/1000 | Loss: 0.00001388
Iteration 142/1000 | Loss: 0.00001387
Iteration 143/1000 | Loss: 0.00001387
Iteration 144/1000 | Loss: 0.00001387
Iteration 145/1000 | Loss: 0.00001386
Iteration 146/1000 | Loss: 0.00001386
Iteration 147/1000 | Loss: 0.00001386
Iteration 148/1000 | Loss: 0.00001386
Iteration 149/1000 | Loss: 0.00001386
Iteration 150/1000 | Loss: 0.00001386
Iteration 151/1000 | Loss: 0.00001385
Iteration 152/1000 | Loss: 0.00001385
Iteration 153/1000 | Loss: 0.00001385
Iteration 154/1000 | Loss: 0.00001385
Iteration 155/1000 | Loss: 0.00001385
Iteration 156/1000 | Loss: 0.00001385
Iteration 157/1000 | Loss: 0.00001385
Iteration 158/1000 | Loss: 0.00001385
Iteration 159/1000 | Loss: 0.00001384
Iteration 160/1000 | Loss: 0.00001384
Iteration 161/1000 | Loss: 0.00001384
Iteration 162/1000 | Loss: 0.00001384
Iteration 163/1000 | Loss: 0.00001384
Iteration 164/1000 | Loss: 0.00001384
Iteration 165/1000 | Loss: 0.00001383
Iteration 166/1000 | Loss: 0.00001383
Iteration 167/1000 | Loss: 0.00001383
Iteration 168/1000 | Loss: 0.00001383
Iteration 169/1000 | Loss: 0.00001383
Iteration 170/1000 | Loss: 0.00001383
Iteration 171/1000 | Loss: 0.00001382
Iteration 172/1000 | Loss: 0.00001382
Iteration 173/1000 | Loss: 0.00003624
Iteration 174/1000 | Loss: 0.00003624
Iteration 175/1000 | Loss: 0.00001598
Iteration 176/1000 | Loss: 0.00001405
Iteration 177/1000 | Loss: 0.00001382
Iteration 178/1000 | Loss: 0.00001380
Iteration 179/1000 | Loss: 0.00001380
Iteration 180/1000 | Loss: 0.00001380
Iteration 181/1000 | Loss: 0.00001380
Iteration 182/1000 | Loss: 0.00001380
Iteration 183/1000 | Loss: 0.00001380
Iteration 184/1000 | Loss: 0.00001380
Iteration 185/1000 | Loss: 0.00001379
Iteration 186/1000 | Loss: 0.00001379
Iteration 187/1000 | Loss: 0.00001379
Iteration 188/1000 | Loss: 0.00001379
Iteration 189/1000 | Loss: 0.00001379
Iteration 190/1000 | Loss: 0.00001379
Iteration 191/1000 | Loss: 0.00001379
Iteration 192/1000 | Loss: 0.00001379
Iteration 193/1000 | Loss: 0.00001379
Iteration 194/1000 | Loss: 0.00001379
Iteration 195/1000 | Loss: 0.00001379
Iteration 196/1000 | Loss: 0.00001379
Iteration 197/1000 | Loss: 0.00001379
Iteration 198/1000 | Loss: 0.00001379
Iteration 199/1000 | Loss: 0.00001379
Iteration 200/1000 | Loss: 0.00001379
Iteration 201/1000 | Loss: 0.00001379
Iteration 202/1000 | Loss: 0.00001379
Iteration 203/1000 | Loss: 0.00001379
Iteration 204/1000 | Loss: 0.00001379
Iteration 205/1000 | Loss: 0.00001379
Iteration 206/1000 | Loss: 0.00001379
Iteration 207/1000 | Loss: 0.00001379
Iteration 208/1000 | Loss: 0.00001379
Iteration 209/1000 | Loss: 0.00001379
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [1.378617434966145e-05, 1.378617434966145e-05, 1.378617434966145e-05, 1.378617434966145e-05, 1.378617434966145e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.378617434966145e-05

Optimization complete. Final v2v error: 3.075343370437622 mm

Highest mean error: 3.9173994064331055 mm for frame 34

Lowest mean error: 2.530843496322632 mm for frame 150

Saving results

Total time: 91.68587136268616
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00471952
Iteration 2/25 | Loss: 0.00089180
Iteration 3/25 | Loss: 0.00068207
Iteration 4/25 | Loss: 0.00064968
Iteration 5/25 | Loss: 0.00063848
Iteration 6/25 | Loss: 0.00063597
Iteration 7/25 | Loss: 0.00063534
Iteration 8/25 | Loss: 0.00063534
Iteration 9/25 | Loss: 0.00063534
Iteration 10/25 | Loss: 0.00063534
Iteration 11/25 | Loss: 0.00063534
Iteration 12/25 | Loss: 0.00063534
Iteration 13/25 | Loss: 0.00063534
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006353372591547668, 0.0006353372591547668, 0.0006353372591547668, 0.0006353372591547668, 0.0006353372591547668]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006353372591547668

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.82415497
Iteration 2/25 | Loss: 0.00021144
Iteration 3/25 | Loss: 0.00021144
Iteration 4/25 | Loss: 0.00021143
Iteration 5/25 | Loss: 0.00021143
Iteration 6/25 | Loss: 0.00021143
Iteration 7/25 | Loss: 0.00021143
Iteration 8/25 | Loss: 0.00021143
Iteration 9/25 | Loss: 0.00021143
Iteration 10/25 | Loss: 0.00021143
Iteration 11/25 | Loss: 0.00021143
Iteration 12/25 | Loss: 0.00021143
Iteration 13/25 | Loss: 0.00021143
Iteration 14/25 | Loss: 0.00021143
Iteration 15/25 | Loss: 0.00021143
Iteration 16/25 | Loss: 0.00021143
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0002114329836331308, 0.0002114329836331308, 0.0002114329836331308, 0.0002114329836331308, 0.0002114329836331308]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002114329836331308

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00021143
Iteration 2/1000 | Loss: 0.00002917
Iteration 3/1000 | Loss: 0.00002073
Iteration 4/1000 | Loss: 0.00001943
Iteration 5/1000 | Loss: 0.00001861
Iteration 6/1000 | Loss: 0.00001807
Iteration 7/1000 | Loss: 0.00001766
Iteration 8/1000 | Loss: 0.00001722
Iteration 9/1000 | Loss: 0.00001694
Iteration 10/1000 | Loss: 0.00001688
Iteration 11/1000 | Loss: 0.00001670
Iteration 12/1000 | Loss: 0.00001664
Iteration 13/1000 | Loss: 0.00001649
Iteration 14/1000 | Loss: 0.00001645
Iteration 15/1000 | Loss: 0.00001639
Iteration 16/1000 | Loss: 0.00001638
Iteration 17/1000 | Loss: 0.00001636
Iteration 18/1000 | Loss: 0.00001636
Iteration 19/1000 | Loss: 0.00001635
Iteration 20/1000 | Loss: 0.00001635
Iteration 21/1000 | Loss: 0.00001634
Iteration 22/1000 | Loss: 0.00001634
Iteration 23/1000 | Loss: 0.00001634
Iteration 24/1000 | Loss: 0.00001633
Iteration 25/1000 | Loss: 0.00001631
Iteration 26/1000 | Loss: 0.00001631
Iteration 27/1000 | Loss: 0.00001630
Iteration 28/1000 | Loss: 0.00001629
Iteration 29/1000 | Loss: 0.00001628
Iteration 30/1000 | Loss: 0.00001628
Iteration 31/1000 | Loss: 0.00001627
Iteration 32/1000 | Loss: 0.00001627
Iteration 33/1000 | Loss: 0.00001627
Iteration 34/1000 | Loss: 0.00001627
Iteration 35/1000 | Loss: 0.00001627
Iteration 36/1000 | Loss: 0.00001627
Iteration 37/1000 | Loss: 0.00001627
Iteration 38/1000 | Loss: 0.00001627
Iteration 39/1000 | Loss: 0.00001626
Iteration 40/1000 | Loss: 0.00001626
Iteration 41/1000 | Loss: 0.00001626
Iteration 42/1000 | Loss: 0.00001626
Iteration 43/1000 | Loss: 0.00001626
Iteration 44/1000 | Loss: 0.00001626
Iteration 45/1000 | Loss: 0.00001626
Iteration 46/1000 | Loss: 0.00001626
Iteration 47/1000 | Loss: 0.00001626
Iteration 48/1000 | Loss: 0.00001626
Iteration 49/1000 | Loss: 0.00001626
Iteration 50/1000 | Loss: 0.00001625
Iteration 51/1000 | Loss: 0.00001625
Iteration 52/1000 | Loss: 0.00001625
Iteration 53/1000 | Loss: 0.00001625
Iteration 54/1000 | Loss: 0.00001624
Iteration 55/1000 | Loss: 0.00001623
Iteration 56/1000 | Loss: 0.00001623
Iteration 57/1000 | Loss: 0.00001623
Iteration 58/1000 | Loss: 0.00001622
Iteration 59/1000 | Loss: 0.00001622
Iteration 60/1000 | Loss: 0.00001622
Iteration 61/1000 | Loss: 0.00001621
Iteration 62/1000 | Loss: 0.00001621
Iteration 63/1000 | Loss: 0.00001620
Iteration 64/1000 | Loss: 0.00001619
Iteration 65/1000 | Loss: 0.00001619
Iteration 66/1000 | Loss: 0.00001618
Iteration 67/1000 | Loss: 0.00001618
Iteration 68/1000 | Loss: 0.00001616
Iteration 69/1000 | Loss: 0.00001615
Iteration 70/1000 | Loss: 0.00001615
Iteration 71/1000 | Loss: 0.00001614
Iteration 72/1000 | Loss: 0.00001614
Iteration 73/1000 | Loss: 0.00001614
Iteration 74/1000 | Loss: 0.00001613
Iteration 75/1000 | Loss: 0.00001613
Iteration 76/1000 | Loss: 0.00001613
Iteration 77/1000 | Loss: 0.00001612
Iteration 78/1000 | Loss: 0.00001612
Iteration 79/1000 | Loss: 0.00001612
Iteration 80/1000 | Loss: 0.00001612
Iteration 81/1000 | Loss: 0.00001612
Iteration 82/1000 | Loss: 0.00001612
Iteration 83/1000 | Loss: 0.00001612
Iteration 84/1000 | Loss: 0.00001611
Iteration 85/1000 | Loss: 0.00001611
Iteration 86/1000 | Loss: 0.00001611
Iteration 87/1000 | Loss: 0.00001610
Iteration 88/1000 | Loss: 0.00001610
Iteration 89/1000 | Loss: 0.00001608
Iteration 90/1000 | Loss: 0.00001608
Iteration 91/1000 | Loss: 0.00001607
Iteration 92/1000 | Loss: 0.00001607
Iteration 93/1000 | Loss: 0.00001607
Iteration 94/1000 | Loss: 0.00001607
Iteration 95/1000 | Loss: 0.00001607
Iteration 96/1000 | Loss: 0.00001607
Iteration 97/1000 | Loss: 0.00001606
Iteration 98/1000 | Loss: 0.00001606
Iteration 99/1000 | Loss: 0.00001606
Iteration 100/1000 | Loss: 0.00001606
Iteration 101/1000 | Loss: 0.00001605
Iteration 102/1000 | Loss: 0.00001605
Iteration 103/1000 | Loss: 0.00001605
Iteration 104/1000 | Loss: 0.00001605
Iteration 105/1000 | Loss: 0.00001605
Iteration 106/1000 | Loss: 0.00001604
Iteration 107/1000 | Loss: 0.00001604
Iteration 108/1000 | Loss: 0.00001604
Iteration 109/1000 | Loss: 0.00001603
Iteration 110/1000 | Loss: 0.00001603
Iteration 111/1000 | Loss: 0.00001603
Iteration 112/1000 | Loss: 0.00001602
Iteration 113/1000 | Loss: 0.00001602
Iteration 114/1000 | Loss: 0.00001602
Iteration 115/1000 | Loss: 0.00001602
Iteration 116/1000 | Loss: 0.00001602
Iteration 117/1000 | Loss: 0.00001601
Iteration 118/1000 | Loss: 0.00001601
Iteration 119/1000 | Loss: 0.00001601
Iteration 120/1000 | Loss: 0.00001601
Iteration 121/1000 | Loss: 0.00001601
Iteration 122/1000 | Loss: 0.00001601
Iteration 123/1000 | Loss: 0.00001600
Iteration 124/1000 | Loss: 0.00001600
Iteration 125/1000 | Loss: 0.00001600
Iteration 126/1000 | Loss: 0.00001600
Iteration 127/1000 | Loss: 0.00001599
Iteration 128/1000 | Loss: 0.00001599
Iteration 129/1000 | Loss: 0.00001599
Iteration 130/1000 | Loss: 0.00001598
Iteration 131/1000 | Loss: 0.00001598
Iteration 132/1000 | Loss: 0.00001598
Iteration 133/1000 | Loss: 0.00001598
Iteration 134/1000 | Loss: 0.00001597
Iteration 135/1000 | Loss: 0.00001597
Iteration 136/1000 | Loss: 0.00001597
Iteration 137/1000 | Loss: 0.00001597
Iteration 138/1000 | Loss: 0.00001597
Iteration 139/1000 | Loss: 0.00001596
Iteration 140/1000 | Loss: 0.00001596
Iteration 141/1000 | Loss: 0.00001596
Iteration 142/1000 | Loss: 0.00001596
Iteration 143/1000 | Loss: 0.00001596
Iteration 144/1000 | Loss: 0.00001596
Iteration 145/1000 | Loss: 0.00001596
Iteration 146/1000 | Loss: 0.00001596
Iteration 147/1000 | Loss: 0.00001595
Iteration 148/1000 | Loss: 0.00001595
Iteration 149/1000 | Loss: 0.00001595
Iteration 150/1000 | Loss: 0.00001595
Iteration 151/1000 | Loss: 0.00001595
Iteration 152/1000 | Loss: 0.00001595
Iteration 153/1000 | Loss: 0.00001595
Iteration 154/1000 | Loss: 0.00001595
Iteration 155/1000 | Loss: 0.00001595
Iteration 156/1000 | Loss: 0.00001595
Iteration 157/1000 | Loss: 0.00001594
Iteration 158/1000 | Loss: 0.00001594
Iteration 159/1000 | Loss: 0.00001594
Iteration 160/1000 | Loss: 0.00001594
Iteration 161/1000 | Loss: 0.00001593
Iteration 162/1000 | Loss: 0.00001593
Iteration 163/1000 | Loss: 0.00001593
Iteration 164/1000 | Loss: 0.00001593
Iteration 165/1000 | Loss: 0.00001593
Iteration 166/1000 | Loss: 0.00001593
Iteration 167/1000 | Loss: 0.00001593
Iteration 168/1000 | Loss: 0.00001593
Iteration 169/1000 | Loss: 0.00001593
Iteration 170/1000 | Loss: 0.00001592
Iteration 171/1000 | Loss: 0.00001592
Iteration 172/1000 | Loss: 0.00001592
Iteration 173/1000 | Loss: 0.00001592
Iteration 174/1000 | Loss: 0.00001592
Iteration 175/1000 | Loss: 0.00001592
Iteration 176/1000 | Loss: 0.00001592
Iteration 177/1000 | Loss: 0.00001592
Iteration 178/1000 | Loss: 0.00001592
Iteration 179/1000 | Loss: 0.00001591
Iteration 180/1000 | Loss: 0.00001591
Iteration 181/1000 | Loss: 0.00001591
Iteration 182/1000 | Loss: 0.00001591
Iteration 183/1000 | Loss: 0.00001591
Iteration 184/1000 | Loss: 0.00001591
Iteration 185/1000 | Loss: 0.00001591
Iteration 186/1000 | Loss: 0.00001591
Iteration 187/1000 | Loss: 0.00001591
Iteration 188/1000 | Loss: 0.00001591
Iteration 189/1000 | Loss: 0.00001591
Iteration 190/1000 | Loss: 0.00001591
Iteration 191/1000 | Loss: 0.00001591
Iteration 192/1000 | Loss: 0.00001591
Iteration 193/1000 | Loss: 0.00001591
Iteration 194/1000 | Loss: 0.00001591
Iteration 195/1000 | Loss: 0.00001591
Iteration 196/1000 | Loss: 0.00001591
Iteration 197/1000 | Loss: 0.00001591
Iteration 198/1000 | Loss: 0.00001591
Iteration 199/1000 | Loss: 0.00001591
Iteration 200/1000 | Loss: 0.00001591
Iteration 201/1000 | Loss: 0.00001591
Iteration 202/1000 | Loss: 0.00001591
Iteration 203/1000 | Loss: 0.00001591
Iteration 204/1000 | Loss: 0.00001591
Iteration 205/1000 | Loss: 0.00001591
Iteration 206/1000 | Loss: 0.00001591
Iteration 207/1000 | Loss: 0.00001591
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [1.5914145478745922e-05, 1.5914145478745922e-05, 1.5914145478745922e-05, 1.5914145478745922e-05, 1.5914145478745922e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5914145478745922e-05

Optimization complete. Final v2v error: 3.3582968711853027 mm

Highest mean error: 4.258073806762695 mm for frame 255

Lowest mean error: 3.1339354515075684 mm for frame 221

Saving results

Total time: 48.77953553199768
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00563856
Iteration 2/25 | Loss: 0.00102611
Iteration 3/25 | Loss: 0.00072984
Iteration 4/25 | Loss: 0.00068463
Iteration 5/25 | Loss: 0.00067055
Iteration 6/25 | Loss: 0.00066677
Iteration 7/25 | Loss: 0.00066607
Iteration 8/25 | Loss: 0.00066607
Iteration 9/25 | Loss: 0.00066607
Iteration 10/25 | Loss: 0.00066607
Iteration 11/25 | Loss: 0.00066607
Iteration 12/25 | Loss: 0.00066607
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006660661310888827, 0.0006660661310888827, 0.0006660661310888827, 0.0006660661310888827, 0.0006660661310888827]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006660661310888827

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38052249
Iteration 2/25 | Loss: 0.00039241
Iteration 3/25 | Loss: 0.00039240
Iteration 4/25 | Loss: 0.00039240
Iteration 5/25 | Loss: 0.00039240
Iteration 6/25 | Loss: 0.00039240
Iteration 7/25 | Loss: 0.00039239
Iteration 8/25 | Loss: 0.00039239
Iteration 9/25 | Loss: 0.00039239
Iteration 10/25 | Loss: 0.00039239
Iteration 11/25 | Loss: 0.00039239
Iteration 12/25 | Loss: 0.00039239
Iteration 13/25 | Loss: 0.00039239
Iteration 14/25 | Loss: 0.00039239
Iteration 15/25 | Loss: 0.00039239
Iteration 16/25 | Loss: 0.00039239
Iteration 17/25 | Loss: 0.00039239
Iteration 18/25 | Loss: 0.00039239
Iteration 19/25 | Loss: 0.00039239
Iteration 20/25 | Loss: 0.00039239
Iteration 21/25 | Loss: 0.00039239
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0003923939657397568, 0.0003923939657397568, 0.0003923939657397568, 0.0003923939657397568, 0.0003923939657397568]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003923939657397568

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039239
Iteration 2/1000 | Loss: 0.00003048
Iteration 3/1000 | Loss: 0.00002043
Iteration 4/1000 | Loss: 0.00001839
Iteration 5/1000 | Loss: 0.00001727
Iteration 6/1000 | Loss: 0.00001649
Iteration 7/1000 | Loss: 0.00001630
Iteration 8/1000 | Loss: 0.00001598
Iteration 9/1000 | Loss: 0.00001568
Iteration 10/1000 | Loss: 0.00001562
Iteration 11/1000 | Loss: 0.00001550
Iteration 12/1000 | Loss: 0.00001532
Iteration 13/1000 | Loss: 0.00001527
Iteration 14/1000 | Loss: 0.00001520
Iteration 15/1000 | Loss: 0.00001515
Iteration 16/1000 | Loss: 0.00001515
Iteration 17/1000 | Loss: 0.00001511
Iteration 18/1000 | Loss: 0.00001511
Iteration 19/1000 | Loss: 0.00001511
Iteration 20/1000 | Loss: 0.00001510
Iteration 21/1000 | Loss: 0.00001508
Iteration 22/1000 | Loss: 0.00001508
Iteration 23/1000 | Loss: 0.00001508
Iteration 24/1000 | Loss: 0.00001507
Iteration 25/1000 | Loss: 0.00001507
Iteration 26/1000 | Loss: 0.00001507
Iteration 27/1000 | Loss: 0.00001507
Iteration 28/1000 | Loss: 0.00001507
Iteration 29/1000 | Loss: 0.00001506
Iteration 30/1000 | Loss: 0.00001505
Iteration 31/1000 | Loss: 0.00001504
Iteration 32/1000 | Loss: 0.00001504
Iteration 33/1000 | Loss: 0.00001504
Iteration 34/1000 | Loss: 0.00001504
Iteration 35/1000 | Loss: 0.00001504
Iteration 36/1000 | Loss: 0.00001504
Iteration 37/1000 | Loss: 0.00001504
Iteration 38/1000 | Loss: 0.00001504
Iteration 39/1000 | Loss: 0.00001504
Iteration 40/1000 | Loss: 0.00001504
Iteration 41/1000 | Loss: 0.00001504
Iteration 42/1000 | Loss: 0.00001504
Iteration 43/1000 | Loss: 0.00001503
Iteration 44/1000 | Loss: 0.00001503
Iteration 45/1000 | Loss: 0.00001503
Iteration 46/1000 | Loss: 0.00001503
Iteration 47/1000 | Loss: 0.00001503
Iteration 48/1000 | Loss: 0.00001503
Iteration 49/1000 | Loss: 0.00001503
Iteration 50/1000 | Loss: 0.00001503
Iteration 51/1000 | Loss: 0.00001501
Iteration 52/1000 | Loss: 0.00001501
Iteration 53/1000 | Loss: 0.00001500
Iteration 54/1000 | Loss: 0.00001500
Iteration 55/1000 | Loss: 0.00001500
Iteration 56/1000 | Loss: 0.00001500
Iteration 57/1000 | Loss: 0.00001500
Iteration 58/1000 | Loss: 0.00001499
Iteration 59/1000 | Loss: 0.00001499
Iteration 60/1000 | Loss: 0.00001499
Iteration 61/1000 | Loss: 0.00001499
Iteration 62/1000 | Loss: 0.00001498
Iteration 63/1000 | Loss: 0.00001498
Iteration 64/1000 | Loss: 0.00001498
Iteration 65/1000 | Loss: 0.00001498
Iteration 66/1000 | Loss: 0.00001498
Iteration 67/1000 | Loss: 0.00001498
Iteration 68/1000 | Loss: 0.00001498
Iteration 69/1000 | Loss: 0.00001498
Iteration 70/1000 | Loss: 0.00001498
Iteration 71/1000 | Loss: 0.00001498
Iteration 72/1000 | Loss: 0.00001498
Iteration 73/1000 | Loss: 0.00001498
Iteration 74/1000 | Loss: 0.00001498
Iteration 75/1000 | Loss: 0.00001498
Iteration 76/1000 | Loss: 0.00001498
Iteration 77/1000 | Loss: 0.00001498
Iteration 78/1000 | Loss: 0.00001498
Iteration 79/1000 | Loss: 0.00001498
Iteration 80/1000 | Loss: 0.00001498
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 80. Stopping optimization.
Last 5 losses: [1.4982259926910046e-05, 1.4982259926910046e-05, 1.4982259926910046e-05, 1.4982259926910046e-05, 1.4982259926910046e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4982259926910046e-05

Optimization complete. Final v2v error: 3.217371940612793 mm

Highest mean error: 3.9081642627716064 mm for frame 73

Lowest mean error: 2.640554428100586 mm for frame 17

Saving results

Total time: 37.202563524246216
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00801204
Iteration 2/25 | Loss: 0.00129587
Iteration 3/25 | Loss: 0.00092184
Iteration 4/25 | Loss: 0.00085061
Iteration 5/25 | Loss: 0.00083053
Iteration 6/25 | Loss: 0.00081862
Iteration 7/25 | Loss: 0.00082395
Iteration 8/25 | Loss: 0.00081770
Iteration 9/25 | Loss: 0.00081646
Iteration 10/25 | Loss: 0.00082237
Iteration 11/25 | Loss: 0.00081976
Iteration 12/25 | Loss: 0.00081506
Iteration 13/25 | Loss: 0.00081471
Iteration 14/25 | Loss: 0.00080738
Iteration 15/25 | Loss: 0.00081369
Iteration 16/25 | Loss: 0.00080995
Iteration 17/25 | Loss: 0.00081259
Iteration 18/25 | Loss: 0.00081163
Iteration 19/25 | Loss: 0.00081004
Iteration 20/25 | Loss: 0.00081016
Iteration 21/25 | Loss: 0.00081030
Iteration 22/25 | Loss: 0.00080390
Iteration 23/25 | Loss: 0.00080096
Iteration 24/25 | Loss: 0.00079463
Iteration 25/25 | Loss: 0.00080329

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42105162
Iteration 2/25 | Loss: 0.00116325
Iteration 3/25 | Loss: 0.00112733
Iteration 4/25 | Loss: 0.00112733
Iteration 5/25 | Loss: 0.00112733
Iteration 6/25 | Loss: 0.00112733
Iteration 7/25 | Loss: 0.00112733
Iteration 8/25 | Loss: 0.00112733
Iteration 9/25 | Loss: 0.00112733
Iteration 10/25 | Loss: 0.00112733
Iteration 11/25 | Loss: 0.00112733
Iteration 12/25 | Loss: 0.00112733
Iteration 13/25 | Loss: 0.00112733
Iteration 14/25 | Loss: 0.00112733
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001127328141592443, 0.001127328141592443, 0.001127328141592443, 0.001127328141592443, 0.001127328141592443]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001127328141592443

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112733
Iteration 2/1000 | Loss: 0.00046365
Iteration 3/1000 | Loss: 0.00019647
Iteration 4/1000 | Loss: 0.00008684
Iteration 5/1000 | Loss: 0.00005770
Iteration 6/1000 | Loss: 0.00022165
Iteration 7/1000 | Loss: 0.00037724
Iteration 8/1000 | Loss: 0.00033643
Iteration 9/1000 | Loss: 0.00008337
Iteration 10/1000 | Loss: 0.00026156
Iteration 11/1000 | Loss: 0.00005815
Iteration 12/1000 | Loss: 0.00060363
Iteration 13/1000 | Loss: 0.00149659
Iteration 14/1000 | Loss: 0.00038122
Iteration 15/1000 | Loss: 0.00037620
Iteration 16/1000 | Loss: 0.00008445
Iteration 17/1000 | Loss: 0.00007935
Iteration 18/1000 | Loss: 0.00009201
Iteration 19/1000 | Loss: 0.00034505
Iteration 20/1000 | Loss: 0.00006590
Iteration 21/1000 | Loss: 0.00009845
Iteration 22/1000 | Loss: 0.00005191
Iteration 23/1000 | Loss: 0.00008909
Iteration 24/1000 | Loss: 0.00041629
Iteration 25/1000 | Loss: 0.00006109
Iteration 26/1000 | Loss: 0.00006403
Iteration 27/1000 | Loss: 0.00004385
Iteration 28/1000 | Loss: 0.00004758
Iteration 29/1000 | Loss: 0.00006431
Iteration 30/1000 | Loss: 0.00004702
Iteration 31/1000 | Loss: 0.00010449
Iteration 32/1000 | Loss: 0.00006793
Iteration 33/1000 | Loss: 0.00004041
Iteration 34/1000 | Loss: 0.00003858
Iteration 35/1000 | Loss: 0.00006063
Iteration 36/1000 | Loss: 0.00005105
Iteration 37/1000 | Loss: 0.00005458
Iteration 38/1000 | Loss: 0.00005212
Iteration 39/1000 | Loss: 0.00006397
Iteration 40/1000 | Loss: 0.00004893
Iteration 41/1000 | Loss: 0.00006856
Iteration 42/1000 | Loss: 0.00005608
Iteration 43/1000 | Loss: 0.00005253
Iteration 44/1000 | Loss: 0.00004275
Iteration 45/1000 | Loss: 0.00004391
Iteration 46/1000 | Loss: 0.00004342
Iteration 47/1000 | Loss: 0.00004070
Iteration 48/1000 | Loss: 0.00005275
Iteration 49/1000 | Loss: 0.00004996
Iteration 50/1000 | Loss: 0.00005666
Iteration 51/1000 | Loss: 0.00004952
Iteration 52/1000 | Loss: 0.00005092
Iteration 53/1000 | Loss: 0.00004712
Iteration 54/1000 | Loss: 0.00005346
Iteration 55/1000 | Loss: 0.00004676
Iteration 56/1000 | Loss: 0.00006117
Iteration 57/1000 | Loss: 0.00004690
Iteration 58/1000 | Loss: 0.00005995
Iteration 59/1000 | Loss: 0.00011913
Iteration 60/1000 | Loss: 0.00007961
Iteration 61/1000 | Loss: 0.00003372
Iteration 62/1000 | Loss: 0.00004446
Iteration 63/1000 | Loss: 0.00005164
Iteration 64/1000 | Loss: 0.00003893
Iteration 65/1000 | Loss: 0.00004670
Iteration 66/1000 | Loss: 0.00003579
Iteration 67/1000 | Loss: 0.00012216
Iteration 68/1000 | Loss: 0.00003798
Iteration 69/1000 | Loss: 0.00004758
Iteration 70/1000 | Loss: 0.00004389
Iteration 71/1000 | Loss: 0.00005061
Iteration 72/1000 | Loss: 0.00004754
Iteration 73/1000 | Loss: 0.00004406
Iteration 74/1000 | Loss: 0.00003997
Iteration 75/1000 | Loss: 0.00004707
Iteration 76/1000 | Loss: 0.00004451
Iteration 77/1000 | Loss: 0.00003253
Iteration 78/1000 | Loss: 0.00003105
Iteration 79/1000 | Loss: 0.00003588
Iteration 80/1000 | Loss: 0.00002872
Iteration 81/1000 | Loss: 0.00004319
Iteration 82/1000 | Loss: 0.00004236
Iteration 83/1000 | Loss: 0.00004235
Iteration 84/1000 | Loss: 0.00004218
Iteration 85/1000 | Loss: 0.00004006
Iteration 86/1000 | Loss: 0.00004177
Iteration 87/1000 | Loss: 0.00002891
Iteration 88/1000 | Loss: 0.00002746
Iteration 89/1000 | Loss: 0.00004230
Iteration 90/1000 | Loss: 0.00004184
Iteration 91/1000 | Loss: 0.00003904
Iteration 92/1000 | Loss: 0.00004041
Iteration 93/1000 | Loss: 0.00003886
Iteration 94/1000 | Loss: 0.00002756
Iteration 95/1000 | Loss: 0.00002615
Iteration 96/1000 | Loss: 0.00002539
Iteration 97/1000 | Loss: 0.00002491
Iteration 98/1000 | Loss: 0.00002464
Iteration 99/1000 | Loss: 0.00002436
Iteration 100/1000 | Loss: 0.00002412
Iteration 101/1000 | Loss: 0.00002399
Iteration 102/1000 | Loss: 0.00018921
Iteration 103/1000 | Loss: 0.00013310
Iteration 104/1000 | Loss: 0.00021374
Iteration 105/1000 | Loss: 0.00020951
Iteration 106/1000 | Loss: 0.00014137
Iteration 107/1000 | Loss: 0.00005669
Iteration 108/1000 | Loss: 0.00011480
Iteration 109/1000 | Loss: 0.00003841
Iteration 110/1000 | Loss: 0.00004705
Iteration 111/1000 | Loss: 0.00002619
Iteration 112/1000 | Loss: 0.00003230
Iteration 113/1000 | Loss: 0.00002517
Iteration 114/1000 | Loss: 0.00002497
Iteration 115/1000 | Loss: 0.00002496
Iteration 116/1000 | Loss: 0.00002496
Iteration 117/1000 | Loss: 0.00002493
Iteration 118/1000 | Loss: 0.00002473
Iteration 119/1000 | Loss: 0.00002466
Iteration 120/1000 | Loss: 0.00002466
Iteration 121/1000 | Loss: 0.00002462
Iteration 122/1000 | Loss: 0.00002457
Iteration 123/1000 | Loss: 0.00019628
Iteration 124/1000 | Loss: 0.00017693
Iteration 125/1000 | Loss: 0.00012964
Iteration 126/1000 | Loss: 0.00003482
Iteration 127/1000 | Loss: 0.00002906
Iteration 128/1000 | Loss: 0.00002576
Iteration 129/1000 | Loss: 0.00002499
Iteration 130/1000 | Loss: 0.00002472
Iteration 131/1000 | Loss: 0.00002468
Iteration 132/1000 | Loss: 0.00002468
Iteration 133/1000 | Loss: 0.00002464
Iteration 134/1000 | Loss: 0.00002450
Iteration 135/1000 | Loss: 0.00002437
Iteration 136/1000 | Loss: 0.00021160
Iteration 137/1000 | Loss: 0.00006613
Iteration 138/1000 | Loss: 0.00002731
Iteration 139/1000 | Loss: 0.00002477
Iteration 140/1000 | Loss: 0.00021206
Iteration 141/1000 | Loss: 0.00008758
Iteration 142/1000 | Loss: 0.00002510
Iteration 143/1000 | Loss: 0.00021800
Iteration 144/1000 | Loss: 0.00006504
Iteration 145/1000 | Loss: 0.00003352
Iteration 146/1000 | Loss: 0.00002433
Iteration 147/1000 | Loss: 0.00002433
Iteration 148/1000 | Loss: 0.00002433
Iteration 149/1000 | Loss: 0.00002432
Iteration 150/1000 | Loss: 0.00002432
Iteration 151/1000 | Loss: 0.00002431
Iteration 152/1000 | Loss: 0.00002431
Iteration 153/1000 | Loss: 0.00002430
Iteration 154/1000 | Loss: 0.00002429
Iteration 155/1000 | Loss: 0.00021778
Iteration 156/1000 | Loss: 0.00006526
Iteration 157/1000 | Loss: 0.00002996
Iteration 158/1000 | Loss: 0.00021436
Iteration 159/1000 | Loss: 0.00007507
Iteration 160/1000 | Loss: 0.00004889
Iteration 161/1000 | Loss: 0.00002451
Iteration 162/1000 | Loss: 0.00020645
Iteration 163/1000 | Loss: 0.00007249
Iteration 164/1000 | Loss: 0.00003241
Iteration 165/1000 | Loss: 0.00002694
Iteration 166/1000 | Loss: 0.00002538
Iteration 167/1000 | Loss: 0.00002441
Iteration 168/1000 | Loss: 0.00002418
Iteration 169/1000 | Loss: 0.00002406
Iteration 170/1000 | Loss: 0.00002406
Iteration 171/1000 | Loss: 0.00002404
Iteration 172/1000 | Loss: 0.00002403
Iteration 173/1000 | Loss: 0.00002425
Iteration 174/1000 | Loss: 0.00002380
Iteration 175/1000 | Loss: 0.00002380
Iteration 176/1000 | Loss: 0.00002380
Iteration 177/1000 | Loss: 0.00002380
Iteration 178/1000 | Loss: 0.00002380
Iteration 179/1000 | Loss: 0.00002379
Iteration 180/1000 | Loss: 0.00002358
Iteration 181/1000 | Loss: 0.00005822
Iteration 182/1000 | Loss: 0.00002363
Iteration 183/1000 | Loss: 0.00002319
Iteration 184/1000 | Loss: 0.00002313
Iteration 185/1000 | Loss: 0.00002313
Iteration 186/1000 | Loss: 0.00002313
Iteration 187/1000 | Loss: 0.00002313
Iteration 188/1000 | Loss: 0.00002313
Iteration 189/1000 | Loss: 0.00002313
Iteration 190/1000 | Loss: 0.00002313
Iteration 191/1000 | Loss: 0.00002313
Iteration 192/1000 | Loss: 0.00002309
Iteration 193/1000 | Loss: 0.00002307
Iteration 194/1000 | Loss: 0.00002306
Iteration 195/1000 | Loss: 0.00002299
Iteration 196/1000 | Loss: 0.00002298
Iteration 197/1000 | Loss: 0.00002298
Iteration 198/1000 | Loss: 0.00002297
Iteration 199/1000 | Loss: 0.00002297
Iteration 200/1000 | Loss: 0.00002297
Iteration 201/1000 | Loss: 0.00002296
Iteration 202/1000 | Loss: 0.00002296
Iteration 203/1000 | Loss: 0.00002296
Iteration 204/1000 | Loss: 0.00002294
Iteration 205/1000 | Loss: 0.00002294
Iteration 206/1000 | Loss: 0.00002290
Iteration 207/1000 | Loss: 0.00002289
Iteration 208/1000 | Loss: 0.00002289
Iteration 209/1000 | Loss: 0.00002289
Iteration 210/1000 | Loss: 0.00002288
Iteration 211/1000 | Loss: 0.00002288
Iteration 212/1000 | Loss: 0.00002288
Iteration 213/1000 | Loss: 0.00002287
Iteration 214/1000 | Loss: 0.00002287
Iteration 215/1000 | Loss: 0.00002278
Iteration 216/1000 | Loss: 0.00002277
Iteration 217/1000 | Loss: 0.00002276
Iteration 218/1000 | Loss: 0.00002276
Iteration 219/1000 | Loss: 0.00002275
Iteration 220/1000 | Loss: 0.00002275
Iteration 221/1000 | Loss: 0.00002268
Iteration 222/1000 | Loss: 0.00002267
Iteration 223/1000 | Loss: 0.00002267
Iteration 224/1000 | Loss: 0.00002267
Iteration 225/1000 | Loss: 0.00002266
Iteration 226/1000 | Loss: 0.00002266
Iteration 227/1000 | Loss: 0.00002266
Iteration 228/1000 | Loss: 0.00002265
Iteration 229/1000 | Loss: 0.00002265
Iteration 230/1000 | Loss: 0.00002265
Iteration 231/1000 | Loss: 0.00002264
Iteration 232/1000 | Loss: 0.00002261
Iteration 233/1000 | Loss: 0.00002259
Iteration 234/1000 | Loss: 0.00002258
Iteration 235/1000 | Loss: 0.00002257
Iteration 236/1000 | Loss: 0.00002256
Iteration 237/1000 | Loss: 0.00002256
Iteration 238/1000 | Loss: 0.00002256
Iteration 239/1000 | Loss: 0.00002256
Iteration 240/1000 | Loss: 0.00002255
Iteration 241/1000 | Loss: 0.00002255
Iteration 242/1000 | Loss: 0.00002255
Iteration 243/1000 | Loss: 0.00002254
Iteration 244/1000 | Loss: 0.00016802
Iteration 245/1000 | Loss: 0.00011017
Iteration 246/1000 | Loss: 0.00021214
Iteration 247/1000 | Loss: 0.00009525
Iteration 248/1000 | Loss: 0.00014868
Iteration 249/1000 | Loss: 0.00014120
Iteration 250/1000 | Loss: 0.00003398
Iteration 251/1000 | Loss: 0.00007363
Iteration 252/1000 | Loss: 0.00002430
Iteration 253/1000 | Loss: 0.00002297
Iteration 254/1000 | Loss: 0.00002287
Iteration 255/1000 | Loss: 0.00002281
Iteration 256/1000 | Loss: 0.00002277
Iteration 257/1000 | Loss: 0.00002275
Iteration 258/1000 | Loss: 0.00002264
Iteration 259/1000 | Loss: 0.00002263
Iteration 260/1000 | Loss: 0.00002263
Iteration 261/1000 | Loss: 0.00002263
Iteration 262/1000 | Loss: 0.00002262
Iteration 263/1000 | Loss: 0.00002262
Iteration 264/1000 | Loss: 0.00002261
Iteration 265/1000 | Loss: 0.00002261
Iteration 266/1000 | Loss: 0.00002260
Iteration 267/1000 | Loss: 0.00002260
Iteration 268/1000 | Loss: 0.00002260
Iteration 269/1000 | Loss: 0.00002259
Iteration 270/1000 | Loss: 0.00002259
Iteration 271/1000 | Loss: 0.00002259
Iteration 272/1000 | Loss: 0.00002258
Iteration 273/1000 | Loss: 0.00002258
Iteration 274/1000 | Loss: 0.00002258
Iteration 275/1000 | Loss: 0.00002258
Iteration 276/1000 | Loss: 0.00002257
Iteration 277/1000 | Loss: 0.00002257
Iteration 278/1000 | Loss: 0.00002257
Iteration 279/1000 | Loss: 0.00002257
Iteration 280/1000 | Loss: 0.00002257
Iteration 281/1000 | Loss: 0.00002257
Iteration 282/1000 | Loss: 0.00002257
Iteration 283/1000 | Loss: 0.00002257
Iteration 284/1000 | Loss: 0.00002256
Iteration 285/1000 | Loss: 0.00002256
Iteration 286/1000 | Loss: 0.00002256
Iteration 287/1000 | Loss: 0.00002256
Iteration 288/1000 | Loss: 0.00002255
Iteration 289/1000 | Loss: 0.00002255
Iteration 290/1000 | Loss: 0.00002254
Iteration 291/1000 | Loss: 0.00002253
Iteration 292/1000 | Loss: 0.00002252
Iteration 293/1000 | Loss: 0.00002252
Iteration 294/1000 | Loss: 0.00002252
Iteration 295/1000 | Loss: 0.00002251
Iteration 296/1000 | Loss: 0.00002251
Iteration 297/1000 | Loss: 0.00002251
Iteration 298/1000 | Loss: 0.00002250
Iteration 299/1000 | Loss: 0.00002250
Iteration 300/1000 | Loss: 0.00002250
Iteration 301/1000 | Loss: 0.00002250
Iteration 302/1000 | Loss: 0.00002249
Iteration 303/1000 | Loss: 0.00002249
Iteration 304/1000 | Loss: 0.00002248
Iteration 305/1000 | Loss: 0.00002248
Iteration 306/1000 | Loss: 0.00002248
Iteration 307/1000 | Loss: 0.00002247
Iteration 308/1000 | Loss: 0.00017523
Iteration 309/1000 | Loss: 0.00010512
Iteration 310/1000 | Loss: 0.00002451
Iteration 311/1000 | Loss: 0.00002264
Iteration 312/1000 | Loss: 0.00002256
Iteration 313/1000 | Loss: 0.00002248
Iteration 314/1000 | Loss: 0.00002248
Iteration 315/1000 | Loss: 0.00002247
Iteration 316/1000 | Loss: 0.00002246
Iteration 317/1000 | Loss: 0.00002246
Iteration 318/1000 | Loss: 0.00002246
Iteration 319/1000 | Loss: 0.00002245
Iteration 320/1000 | Loss: 0.00017795
Iteration 321/1000 | Loss: 0.00008796
Iteration 322/1000 | Loss: 0.00018398
Iteration 323/1000 | Loss: 0.00009388
Iteration 324/1000 | Loss: 0.00018536
Iteration 325/1000 | Loss: 0.00018311
Iteration 326/1000 | Loss: 0.00014558
Iteration 327/1000 | Loss: 0.00003423
Iteration 328/1000 | Loss: 0.00002949
Iteration 329/1000 | Loss: 0.00002714
Iteration 330/1000 | Loss: 0.00004345
Iteration 331/1000 | Loss: 0.00003232
Iteration 332/1000 | Loss: 0.00003271
Iteration 333/1000 | Loss: 0.00003197
Iteration 334/1000 | Loss: 0.00018921
Iteration 335/1000 | Loss: 0.00002729
Iteration 336/1000 | Loss: 0.00003291
Iteration 337/1000 | Loss: 0.00002609
Iteration 338/1000 | Loss: 0.00003769
Iteration 339/1000 | Loss: 0.00002282
Iteration 340/1000 | Loss: 0.00002244
Iteration 341/1000 | Loss: 0.00002224
Iteration 342/1000 | Loss: 0.00002215
Iteration 343/1000 | Loss: 0.00002213
Iteration 344/1000 | Loss: 0.00002210
Iteration 345/1000 | Loss: 0.00002201
Iteration 346/1000 | Loss: 0.00002199
Iteration 347/1000 | Loss: 0.00002198
Iteration 348/1000 | Loss: 0.00002197
Iteration 349/1000 | Loss: 0.00002197
Iteration 350/1000 | Loss: 0.00002197
Iteration 351/1000 | Loss: 0.00002196
Iteration 352/1000 | Loss: 0.00002196
Iteration 353/1000 | Loss: 0.00002196
Iteration 354/1000 | Loss: 0.00002196
Iteration 355/1000 | Loss: 0.00002195
Iteration 356/1000 | Loss: 0.00002195
Iteration 357/1000 | Loss: 0.00002194
Iteration 358/1000 | Loss: 0.00002194
Iteration 359/1000 | Loss: 0.00002194
Iteration 360/1000 | Loss: 0.00002193
Iteration 361/1000 | Loss: 0.00002193
Iteration 362/1000 | Loss: 0.00002192
Iteration 363/1000 | Loss: 0.00002192
Iteration 364/1000 | Loss: 0.00002192
Iteration 365/1000 | Loss: 0.00002191
Iteration 366/1000 | Loss: 0.00002191
Iteration 367/1000 | Loss: 0.00002191
Iteration 368/1000 | Loss: 0.00002190
Iteration 369/1000 | Loss: 0.00002190
Iteration 370/1000 | Loss: 0.00002189
Iteration 371/1000 | Loss: 0.00002188
Iteration 372/1000 | Loss: 0.00002186
Iteration 373/1000 | Loss: 0.00002185
Iteration 374/1000 | Loss: 0.00002185
Iteration 375/1000 | Loss: 0.00002185
Iteration 376/1000 | Loss: 0.00002184
Iteration 377/1000 | Loss: 0.00002184
Iteration 378/1000 | Loss: 0.00002184
Iteration 379/1000 | Loss: 0.00002184
Iteration 380/1000 | Loss: 0.00002183
Iteration 381/1000 | Loss: 0.00002183
Iteration 382/1000 | Loss: 0.00002183
Iteration 383/1000 | Loss: 0.00002182
Iteration 384/1000 | Loss: 0.00002182
Iteration 385/1000 | Loss: 0.00002181
Iteration 386/1000 | Loss: 0.00002181
Iteration 387/1000 | Loss: 0.00002179
Iteration 388/1000 | Loss: 0.00004854
Iteration 389/1000 | Loss: 0.00002180
Iteration 390/1000 | Loss: 0.00002175
Iteration 391/1000 | Loss: 0.00002775
Iteration 392/1000 | Loss: 0.00002174
Iteration 393/1000 | Loss: 0.00002174
Iteration 394/1000 | Loss: 0.00002174
Iteration 395/1000 | Loss: 0.00002174
Iteration 396/1000 | Loss: 0.00002173
Iteration 397/1000 | Loss: 0.00002173
Iteration 398/1000 | Loss: 0.00002173
Iteration 399/1000 | Loss: 0.00002173
Iteration 400/1000 | Loss: 0.00002173
Iteration 401/1000 | Loss: 0.00002173
Iteration 402/1000 | Loss: 0.00002173
Iteration 403/1000 | Loss: 0.00002173
Iteration 404/1000 | Loss: 0.00002173
Iteration 405/1000 | Loss: 0.00002173
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 405. Stopping optimization.
Last 5 losses: [2.173180655518081e-05, 2.173180655518081e-05, 2.173180655518081e-05, 2.173180655518081e-05, 2.173180655518081e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.173180655518081e-05

Optimization complete. Final v2v error: 3.867159605026245 mm

Highest mean error: 5.029099464416504 mm for frame 100

Lowest mean error: 3.184299945831299 mm for frame 188

Saving results

Total time: 391.6670846939087
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01098649
Iteration 2/25 | Loss: 0.00258260
Iteration 3/25 | Loss: 0.00132872
Iteration 4/25 | Loss: 0.00105226
Iteration 5/25 | Loss: 0.00100114
Iteration 6/25 | Loss: 0.00088962
Iteration 7/25 | Loss: 0.00081057
Iteration 8/25 | Loss: 0.00071476
Iteration 9/25 | Loss: 0.00067887
Iteration 10/25 | Loss: 0.00066313
Iteration 11/25 | Loss: 0.00065770
Iteration 12/25 | Loss: 0.00065577
Iteration 13/25 | Loss: 0.00065535
Iteration 14/25 | Loss: 0.00065516
Iteration 15/25 | Loss: 0.00065509
Iteration 16/25 | Loss: 0.00065508
Iteration 17/25 | Loss: 0.00065508
Iteration 18/25 | Loss: 0.00065508
Iteration 19/25 | Loss: 0.00065508
Iteration 20/25 | Loss: 0.00065508
Iteration 21/25 | Loss: 0.00065508
Iteration 22/25 | Loss: 0.00065508
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0006550773978233337, 0.0006550773978233337, 0.0006550773978233337, 0.0006550773978233337, 0.0006550773978233337]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006550773978233337

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41316259
Iteration 2/25 | Loss: 0.00026733
Iteration 3/25 | Loss: 0.00026733
Iteration 4/25 | Loss: 0.00026733
Iteration 5/25 | Loss: 0.00026733
Iteration 6/25 | Loss: 0.00026733
Iteration 7/25 | Loss: 0.00026733
Iteration 8/25 | Loss: 0.00026733
Iteration 9/25 | Loss: 0.00026733
Iteration 10/25 | Loss: 0.00026733
Iteration 11/25 | Loss: 0.00026733
Iteration 12/25 | Loss: 0.00026733
Iteration 13/25 | Loss: 0.00026733
Iteration 14/25 | Loss: 0.00026733
Iteration 15/25 | Loss: 0.00026733
Iteration 16/25 | Loss: 0.00026733
Iteration 17/25 | Loss: 0.00026733
Iteration 18/25 | Loss: 0.00026733
Iteration 19/25 | Loss: 0.00026733
Iteration 20/25 | Loss: 0.00026733
Iteration 21/25 | Loss: 0.00026733
Iteration 22/25 | Loss: 0.00026733
Iteration 23/25 | Loss: 0.00026733
Iteration 24/25 | Loss: 0.00026733
Iteration 25/25 | Loss: 0.00026733

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026733
Iteration 2/1000 | Loss: 0.00003421
Iteration 3/1000 | Loss: 0.00002556
Iteration 4/1000 | Loss: 0.00002393
Iteration 5/1000 | Loss: 0.00002252
Iteration 6/1000 | Loss: 0.00002176
Iteration 7/1000 | Loss: 0.00002106
Iteration 8/1000 | Loss: 0.00050393
Iteration 9/1000 | Loss: 0.00005874
Iteration 10/1000 | Loss: 0.00003066
Iteration 11/1000 | Loss: 0.00002085
Iteration 12/1000 | Loss: 0.00001912
Iteration 13/1000 | Loss: 0.00001863
Iteration 14/1000 | Loss: 0.00001826
Iteration 15/1000 | Loss: 0.00001808
Iteration 16/1000 | Loss: 0.00001792
Iteration 17/1000 | Loss: 0.00001778
Iteration 18/1000 | Loss: 0.00001777
Iteration 19/1000 | Loss: 0.00001775
Iteration 20/1000 | Loss: 0.00001772
Iteration 21/1000 | Loss: 0.00001770
Iteration 22/1000 | Loss: 0.00001769
Iteration 23/1000 | Loss: 0.00001765
Iteration 24/1000 | Loss: 0.00001762
Iteration 25/1000 | Loss: 0.00001761
Iteration 26/1000 | Loss: 0.00001759
Iteration 27/1000 | Loss: 0.00001759
Iteration 28/1000 | Loss: 0.00001755
Iteration 29/1000 | Loss: 0.00001751
Iteration 30/1000 | Loss: 0.00001749
Iteration 31/1000 | Loss: 0.00001749
Iteration 32/1000 | Loss: 0.00001748
Iteration 33/1000 | Loss: 0.00001748
Iteration 34/1000 | Loss: 0.00001748
Iteration 35/1000 | Loss: 0.00001748
Iteration 36/1000 | Loss: 0.00001748
Iteration 37/1000 | Loss: 0.00001748
Iteration 38/1000 | Loss: 0.00001747
Iteration 39/1000 | Loss: 0.00001747
Iteration 40/1000 | Loss: 0.00001747
Iteration 41/1000 | Loss: 0.00001747
Iteration 42/1000 | Loss: 0.00001746
Iteration 43/1000 | Loss: 0.00001746
Iteration 44/1000 | Loss: 0.00001746
Iteration 45/1000 | Loss: 0.00001746
Iteration 46/1000 | Loss: 0.00001746
Iteration 47/1000 | Loss: 0.00001745
Iteration 48/1000 | Loss: 0.00001745
Iteration 49/1000 | Loss: 0.00001745
Iteration 50/1000 | Loss: 0.00001742
Iteration 51/1000 | Loss: 0.00001742
Iteration 52/1000 | Loss: 0.00001742
Iteration 53/1000 | Loss: 0.00001742
Iteration 54/1000 | Loss: 0.00001741
Iteration 55/1000 | Loss: 0.00001740
Iteration 56/1000 | Loss: 0.00001739
Iteration 57/1000 | Loss: 0.00001739
Iteration 58/1000 | Loss: 0.00001739
Iteration 59/1000 | Loss: 0.00001738
Iteration 60/1000 | Loss: 0.00001738
Iteration 61/1000 | Loss: 0.00001738
Iteration 62/1000 | Loss: 0.00001738
Iteration 63/1000 | Loss: 0.00001737
Iteration 64/1000 | Loss: 0.00001737
Iteration 65/1000 | Loss: 0.00001737
Iteration 66/1000 | Loss: 0.00001737
Iteration 67/1000 | Loss: 0.00001737
Iteration 68/1000 | Loss: 0.00001736
Iteration 69/1000 | Loss: 0.00001736
Iteration 70/1000 | Loss: 0.00001736
Iteration 71/1000 | Loss: 0.00001736
Iteration 72/1000 | Loss: 0.00001736
Iteration 73/1000 | Loss: 0.00001735
Iteration 74/1000 | Loss: 0.00001735
Iteration 75/1000 | Loss: 0.00001735
Iteration 76/1000 | Loss: 0.00001735
Iteration 77/1000 | Loss: 0.00001735
Iteration 78/1000 | Loss: 0.00001735
Iteration 79/1000 | Loss: 0.00001735
Iteration 80/1000 | Loss: 0.00001735
Iteration 81/1000 | Loss: 0.00001735
Iteration 82/1000 | Loss: 0.00001735
Iteration 83/1000 | Loss: 0.00001735
Iteration 84/1000 | Loss: 0.00001734
Iteration 85/1000 | Loss: 0.00001734
Iteration 86/1000 | Loss: 0.00001733
Iteration 87/1000 | Loss: 0.00001733
Iteration 88/1000 | Loss: 0.00001733
Iteration 89/1000 | Loss: 0.00001733
Iteration 90/1000 | Loss: 0.00001732
Iteration 91/1000 | Loss: 0.00001732
Iteration 92/1000 | Loss: 0.00001732
Iteration 93/1000 | Loss: 0.00001732
Iteration 94/1000 | Loss: 0.00001732
Iteration 95/1000 | Loss: 0.00001732
Iteration 96/1000 | Loss: 0.00001732
Iteration 97/1000 | Loss: 0.00001732
Iteration 98/1000 | Loss: 0.00001732
Iteration 99/1000 | Loss: 0.00001732
Iteration 100/1000 | Loss: 0.00001732
Iteration 101/1000 | Loss: 0.00001731
Iteration 102/1000 | Loss: 0.00001731
Iteration 103/1000 | Loss: 0.00001731
Iteration 104/1000 | Loss: 0.00001730
Iteration 105/1000 | Loss: 0.00001730
Iteration 106/1000 | Loss: 0.00001730
Iteration 107/1000 | Loss: 0.00001730
Iteration 108/1000 | Loss: 0.00001730
Iteration 109/1000 | Loss: 0.00001730
Iteration 110/1000 | Loss: 0.00001730
Iteration 111/1000 | Loss: 0.00001730
Iteration 112/1000 | Loss: 0.00001729
Iteration 113/1000 | Loss: 0.00001729
Iteration 114/1000 | Loss: 0.00001729
Iteration 115/1000 | Loss: 0.00001729
Iteration 116/1000 | Loss: 0.00001729
Iteration 117/1000 | Loss: 0.00001729
Iteration 118/1000 | Loss: 0.00001729
Iteration 119/1000 | Loss: 0.00001729
Iteration 120/1000 | Loss: 0.00001729
Iteration 121/1000 | Loss: 0.00001729
Iteration 122/1000 | Loss: 0.00001729
Iteration 123/1000 | Loss: 0.00001729
Iteration 124/1000 | Loss: 0.00001729
Iteration 125/1000 | Loss: 0.00001729
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.729279938444961e-05, 1.729279938444961e-05, 1.729279938444961e-05, 1.729279938444961e-05, 1.729279938444961e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.729279938444961e-05

Optimization complete. Final v2v error: 3.4958040714263916 mm

Highest mean error: 4.177154541015625 mm for frame 52

Lowest mean error: 3.3548524379730225 mm for frame 62

Saving results

Total time: 59.06175088882446
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00699415
Iteration 2/25 | Loss: 0.00141801
Iteration 3/25 | Loss: 0.00096283
Iteration 4/25 | Loss: 0.00083932
Iteration 5/25 | Loss: 0.00077118
Iteration 6/25 | Loss: 0.00073490
Iteration 7/25 | Loss: 0.00072658
Iteration 8/25 | Loss: 0.00072972
Iteration 9/25 | Loss: 0.00073180
Iteration 10/25 | Loss: 0.00072420
Iteration 11/25 | Loss: 0.00072025
Iteration 12/25 | Loss: 0.00072057
Iteration 13/25 | Loss: 0.00072056
Iteration 14/25 | Loss: 0.00071935
Iteration 15/25 | Loss: 0.00071590
Iteration 16/25 | Loss: 0.00071202
Iteration 17/25 | Loss: 0.00070969
Iteration 18/25 | Loss: 0.00070815
Iteration 19/25 | Loss: 0.00070738
Iteration 20/25 | Loss: 0.00070710
Iteration 21/25 | Loss: 0.00070709
Iteration 22/25 | Loss: 0.00070709
Iteration 23/25 | Loss: 0.00070709
Iteration 24/25 | Loss: 0.00070709
Iteration 25/25 | Loss: 0.00070709

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.34164548
Iteration 2/25 | Loss: 0.00037607
Iteration 3/25 | Loss: 0.00037602
Iteration 4/25 | Loss: 0.00037602
Iteration 5/25 | Loss: 0.00037602
Iteration 6/25 | Loss: 0.00037602
Iteration 7/25 | Loss: 0.00037602
Iteration 8/25 | Loss: 0.00037602
Iteration 9/25 | Loss: 0.00037602
Iteration 10/25 | Loss: 0.00037602
Iteration 11/25 | Loss: 0.00037601
Iteration 12/25 | Loss: 0.00037601
Iteration 13/25 | Loss: 0.00037601
Iteration 14/25 | Loss: 0.00037601
Iteration 15/25 | Loss: 0.00037601
Iteration 16/25 | Loss: 0.00037601
Iteration 17/25 | Loss: 0.00037601
Iteration 18/25 | Loss: 0.00037601
Iteration 19/25 | Loss: 0.00037601
Iteration 20/25 | Loss: 0.00037601
Iteration 21/25 | Loss: 0.00037601
Iteration 22/25 | Loss: 0.00037601
Iteration 23/25 | Loss: 0.00037601
Iteration 24/25 | Loss: 0.00037601
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.00037601470830850303, 0.00037601470830850303, 0.00037601470830850303, 0.00037601470830850303, 0.00037601470830850303]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00037601470830850303

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037601
Iteration 2/1000 | Loss: 0.00003381
Iteration 3/1000 | Loss: 0.00002450
Iteration 4/1000 | Loss: 0.00002241
Iteration 5/1000 | Loss: 0.00002076
Iteration 6/1000 | Loss: 0.00001986
Iteration 7/1000 | Loss: 0.00001928
Iteration 8/1000 | Loss: 0.00001886
Iteration 9/1000 | Loss: 0.00001857
Iteration 10/1000 | Loss: 0.00013326
Iteration 11/1000 | Loss: 0.00001951
Iteration 12/1000 | Loss: 0.00001827
Iteration 13/1000 | Loss: 0.00001736
Iteration 14/1000 | Loss: 0.00001660
Iteration 15/1000 | Loss: 0.00001623
Iteration 16/1000 | Loss: 0.00001611
Iteration 17/1000 | Loss: 0.00001607
Iteration 18/1000 | Loss: 0.00001606
Iteration 19/1000 | Loss: 0.00001597
Iteration 20/1000 | Loss: 0.00001594
Iteration 21/1000 | Loss: 0.00001588
Iteration 22/1000 | Loss: 0.00001586
Iteration 23/1000 | Loss: 0.00001585
Iteration 24/1000 | Loss: 0.00001584
Iteration 25/1000 | Loss: 0.00001584
Iteration 26/1000 | Loss: 0.00001582
Iteration 27/1000 | Loss: 0.00001582
Iteration 28/1000 | Loss: 0.00001581
Iteration 29/1000 | Loss: 0.00001581
Iteration 30/1000 | Loss: 0.00001580
Iteration 31/1000 | Loss: 0.00001580
Iteration 32/1000 | Loss: 0.00001580
Iteration 33/1000 | Loss: 0.00001579
Iteration 34/1000 | Loss: 0.00001579
Iteration 35/1000 | Loss: 0.00001579
Iteration 36/1000 | Loss: 0.00001579
Iteration 37/1000 | Loss: 0.00001579
Iteration 38/1000 | Loss: 0.00001579
Iteration 39/1000 | Loss: 0.00001579
Iteration 40/1000 | Loss: 0.00001579
Iteration 41/1000 | Loss: 0.00001579
Iteration 42/1000 | Loss: 0.00001579
Iteration 43/1000 | Loss: 0.00001578
Iteration 44/1000 | Loss: 0.00001578
Iteration 45/1000 | Loss: 0.00001578
Iteration 46/1000 | Loss: 0.00001577
Iteration 47/1000 | Loss: 0.00001577
Iteration 48/1000 | Loss: 0.00001577
Iteration 49/1000 | Loss: 0.00001577
Iteration 50/1000 | Loss: 0.00001577
Iteration 51/1000 | Loss: 0.00001577
Iteration 52/1000 | Loss: 0.00001577
Iteration 53/1000 | Loss: 0.00001576
Iteration 54/1000 | Loss: 0.00001576
Iteration 55/1000 | Loss: 0.00001576
Iteration 56/1000 | Loss: 0.00001576
Iteration 57/1000 | Loss: 0.00001576
Iteration 58/1000 | Loss: 0.00001576
Iteration 59/1000 | Loss: 0.00001576
Iteration 60/1000 | Loss: 0.00001576
Iteration 61/1000 | Loss: 0.00001576
Iteration 62/1000 | Loss: 0.00001576
Iteration 63/1000 | Loss: 0.00001576
Iteration 64/1000 | Loss: 0.00001575
Iteration 65/1000 | Loss: 0.00001575
Iteration 66/1000 | Loss: 0.00001575
Iteration 67/1000 | Loss: 0.00001575
Iteration 68/1000 | Loss: 0.00001574
Iteration 69/1000 | Loss: 0.00001573
Iteration 70/1000 | Loss: 0.00001573
Iteration 71/1000 | Loss: 0.00001573
Iteration 72/1000 | Loss: 0.00001573
Iteration 73/1000 | Loss: 0.00001573
Iteration 74/1000 | Loss: 0.00001573
Iteration 75/1000 | Loss: 0.00001573
Iteration 76/1000 | Loss: 0.00001573
Iteration 77/1000 | Loss: 0.00001572
Iteration 78/1000 | Loss: 0.00001572
Iteration 79/1000 | Loss: 0.00001572
Iteration 80/1000 | Loss: 0.00001572
Iteration 81/1000 | Loss: 0.00001572
Iteration 82/1000 | Loss: 0.00001572
Iteration 83/1000 | Loss: 0.00001572
Iteration 84/1000 | Loss: 0.00001571
Iteration 85/1000 | Loss: 0.00001571
Iteration 86/1000 | Loss: 0.00001571
Iteration 87/1000 | Loss: 0.00001571
Iteration 88/1000 | Loss: 0.00001570
Iteration 89/1000 | Loss: 0.00001570
Iteration 90/1000 | Loss: 0.00001570
Iteration 91/1000 | Loss: 0.00001569
Iteration 92/1000 | Loss: 0.00001569
Iteration 93/1000 | Loss: 0.00001569
Iteration 94/1000 | Loss: 0.00001569
Iteration 95/1000 | Loss: 0.00001569
Iteration 96/1000 | Loss: 0.00001569
Iteration 97/1000 | Loss: 0.00001569
Iteration 98/1000 | Loss: 0.00001569
Iteration 99/1000 | Loss: 0.00001569
Iteration 100/1000 | Loss: 0.00001568
Iteration 101/1000 | Loss: 0.00001568
Iteration 102/1000 | Loss: 0.00001568
Iteration 103/1000 | Loss: 0.00001568
Iteration 104/1000 | Loss: 0.00001568
Iteration 105/1000 | Loss: 0.00001568
Iteration 106/1000 | Loss: 0.00001568
Iteration 107/1000 | Loss: 0.00001568
Iteration 108/1000 | Loss: 0.00001568
Iteration 109/1000 | Loss: 0.00001568
Iteration 110/1000 | Loss: 0.00001567
Iteration 111/1000 | Loss: 0.00001567
Iteration 112/1000 | Loss: 0.00001567
Iteration 113/1000 | Loss: 0.00001567
Iteration 114/1000 | Loss: 0.00001567
Iteration 115/1000 | Loss: 0.00001567
Iteration 116/1000 | Loss: 0.00001567
Iteration 117/1000 | Loss: 0.00001567
Iteration 118/1000 | Loss: 0.00001567
Iteration 119/1000 | Loss: 0.00001566
Iteration 120/1000 | Loss: 0.00001566
Iteration 121/1000 | Loss: 0.00001566
Iteration 122/1000 | Loss: 0.00001566
Iteration 123/1000 | Loss: 0.00001566
Iteration 124/1000 | Loss: 0.00001566
Iteration 125/1000 | Loss: 0.00001566
Iteration 126/1000 | Loss: 0.00001566
Iteration 127/1000 | Loss: 0.00001566
Iteration 128/1000 | Loss: 0.00001566
Iteration 129/1000 | Loss: 0.00001566
Iteration 130/1000 | Loss: 0.00001566
Iteration 131/1000 | Loss: 0.00001565
Iteration 132/1000 | Loss: 0.00001565
Iteration 133/1000 | Loss: 0.00001565
Iteration 134/1000 | Loss: 0.00001565
Iteration 135/1000 | Loss: 0.00001565
Iteration 136/1000 | Loss: 0.00001565
Iteration 137/1000 | Loss: 0.00001565
Iteration 138/1000 | Loss: 0.00001565
Iteration 139/1000 | Loss: 0.00001565
Iteration 140/1000 | Loss: 0.00001564
Iteration 141/1000 | Loss: 0.00001564
Iteration 142/1000 | Loss: 0.00001564
Iteration 143/1000 | Loss: 0.00001564
Iteration 144/1000 | Loss: 0.00001564
Iteration 145/1000 | Loss: 0.00001564
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [1.564349622640293e-05, 1.564349622640293e-05, 1.564349622640293e-05, 1.564349622640293e-05, 1.564349622640293e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.564349622640293e-05

Optimization complete. Final v2v error: 3.3819169998168945 mm

Highest mean error: 3.8886914253234863 mm for frame 34

Lowest mean error: 2.853498697280884 mm for frame 132

Saving results

Total time: 69.508220911026
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00395423
Iteration 2/25 | Loss: 0.00077905
Iteration 3/25 | Loss: 0.00065707
Iteration 4/25 | Loss: 0.00063363
Iteration 5/25 | Loss: 0.00062482
Iteration 6/25 | Loss: 0.00062332
Iteration 7/25 | Loss: 0.00062289
Iteration 8/25 | Loss: 0.00062289
Iteration 9/25 | Loss: 0.00062289
Iteration 10/25 | Loss: 0.00062289
Iteration 11/25 | Loss: 0.00062289
Iteration 12/25 | Loss: 0.00062286
Iteration 13/25 | Loss: 0.00062286
Iteration 14/25 | Loss: 0.00062286
Iteration 15/25 | Loss: 0.00062286
Iteration 16/25 | Loss: 0.00062286
Iteration 17/25 | Loss: 0.00062286
Iteration 18/25 | Loss: 0.00062286
Iteration 19/25 | Loss: 0.00062286
Iteration 20/25 | Loss: 0.00062286
Iteration 21/25 | Loss: 0.00062286
Iteration 22/25 | Loss: 0.00062286
Iteration 23/25 | Loss: 0.00062286
Iteration 24/25 | Loss: 0.00062286
Iteration 25/25 | Loss: 0.00062286

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50016844
Iteration 2/25 | Loss: 0.00031624
Iteration 3/25 | Loss: 0.00031624
Iteration 4/25 | Loss: 0.00031623
Iteration 5/25 | Loss: 0.00031623
Iteration 6/25 | Loss: 0.00031623
Iteration 7/25 | Loss: 0.00031623
Iteration 8/25 | Loss: 0.00031623
Iteration 9/25 | Loss: 0.00031623
Iteration 10/25 | Loss: 0.00031623
Iteration 11/25 | Loss: 0.00031623
Iteration 12/25 | Loss: 0.00031623
Iteration 13/25 | Loss: 0.00031623
Iteration 14/25 | Loss: 0.00031623
Iteration 15/25 | Loss: 0.00031623
Iteration 16/25 | Loss: 0.00031623
Iteration 17/25 | Loss: 0.00031623
Iteration 18/25 | Loss: 0.00031623
Iteration 19/25 | Loss: 0.00031623
Iteration 20/25 | Loss: 0.00031623
Iteration 21/25 | Loss: 0.00031623
Iteration 22/25 | Loss: 0.00031623
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0003162320062983781, 0.0003162320062983781, 0.0003162320062983781, 0.0003162320062983781, 0.0003162320062983781]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003162320062983781

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031623
Iteration 2/1000 | Loss: 0.00002465
Iteration 3/1000 | Loss: 0.00001649
Iteration 4/1000 | Loss: 0.00001527
Iteration 5/1000 | Loss: 0.00001453
Iteration 6/1000 | Loss: 0.00001428
Iteration 7/1000 | Loss: 0.00001400
Iteration 8/1000 | Loss: 0.00001382
Iteration 9/1000 | Loss: 0.00001364
Iteration 10/1000 | Loss: 0.00001362
Iteration 11/1000 | Loss: 0.00001362
Iteration 12/1000 | Loss: 0.00001357
Iteration 13/1000 | Loss: 0.00001357
Iteration 14/1000 | Loss: 0.00001350
Iteration 15/1000 | Loss: 0.00001350
Iteration 16/1000 | Loss: 0.00001348
Iteration 17/1000 | Loss: 0.00001344
Iteration 18/1000 | Loss: 0.00001344
Iteration 19/1000 | Loss: 0.00001343
Iteration 20/1000 | Loss: 0.00001342
Iteration 21/1000 | Loss: 0.00001342
Iteration 22/1000 | Loss: 0.00001341
Iteration 23/1000 | Loss: 0.00001341
Iteration 24/1000 | Loss: 0.00001340
Iteration 25/1000 | Loss: 0.00001338
Iteration 26/1000 | Loss: 0.00001337
Iteration 27/1000 | Loss: 0.00001336
Iteration 28/1000 | Loss: 0.00001335
Iteration 29/1000 | Loss: 0.00001335
Iteration 30/1000 | Loss: 0.00001335
Iteration 31/1000 | Loss: 0.00001334
Iteration 32/1000 | Loss: 0.00001334
Iteration 33/1000 | Loss: 0.00001334
Iteration 34/1000 | Loss: 0.00001334
Iteration 35/1000 | Loss: 0.00001333
Iteration 36/1000 | Loss: 0.00001333
Iteration 37/1000 | Loss: 0.00001333
Iteration 38/1000 | Loss: 0.00001333
Iteration 39/1000 | Loss: 0.00001333
Iteration 40/1000 | Loss: 0.00001333
Iteration 41/1000 | Loss: 0.00001333
Iteration 42/1000 | Loss: 0.00001333
Iteration 43/1000 | Loss: 0.00001333
Iteration 44/1000 | Loss: 0.00001333
Iteration 45/1000 | Loss: 0.00001333
Iteration 46/1000 | Loss: 0.00001333
Iteration 47/1000 | Loss: 0.00001333
Iteration 48/1000 | Loss: 0.00001332
Iteration 49/1000 | Loss: 0.00001332
Iteration 50/1000 | Loss: 0.00001332
Iteration 51/1000 | Loss: 0.00001332
Iteration 52/1000 | Loss: 0.00001332
Iteration 53/1000 | Loss: 0.00001332
Iteration 54/1000 | Loss: 0.00001331
Iteration 55/1000 | Loss: 0.00001331
Iteration 56/1000 | Loss: 0.00001331
Iteration 57/1000 | Loss: 0.00001331
Iteration 58/1000 | Loss: 0.00001331
Iteration 59/1000 | Loss: 0.00001331
Iteration 60/1000 | Loss: 0.00001331
Iteration 61/1000 | Loss: 0.00001331
Iteration 62/1000 | Loss: 0.00001331
Iteration 63/1000 | Loss: 0.00001331
Iteration 64/1000 | Loss: 0.00001331
Iteration 65/1000 | Loss: 0.00001331
Iteration 66/1000 | Loss: 0.00001330
Iteration 67/1000 | Loss: 0.00001330
Iteration 68/1000 | Loss: 0.00001330
Iteration 69/1000 | Loss: 0.00001330
Iteration 70/1000 | Loss: 0.00001329
Iteration 71/1000 | Loss: 0.00001329
Iteration 72/1000 | Loss: 0.00001329
Iteration 73/1000 | Loss: 0.00001328
Iteration 74/1000 | Loss: 0.00001328
Iteration 75/1000 | Loss: 0.00001328
Iteration 76/1000 | Loss: 0.00001328
Iteration 77/1000 | Loss: 0.00001328
Iteration 78/1000 | Loss: 0.00001328
Iteration 79/1000 | Loss: 0.00001327
Iteration 80/1000 | Loss: 0.00001327
Iteration 81/1000 | Loss: 0.00001326
Iteration 82/1000 | Loss: 0.00001326
Iteration 83/1000 | Loss: 0.00001326
Iteration 84/1000 | Loss: 0.00001326
Iteration 85/1000 | Loss: 0.00001326
Iteration 86/1000 | Loss: 0.00001326
Iteration 87/1000 | Loss: 0.00001326
Iteration 88/1000 | Loss: 0.00001326
Iteration 89/1000 | Loss: 0.00001326
Iteration 90/1000 | Loss: 0.00001325
Iteration 91/1000 | Loss: 0.00001325
Iteration 92/1000 | Loss: 0.00001325
Iteration 93/1000 | Loss: 0.00001325
Iteration 94/1000 | Loss: 0.00001325
Iteration 95/1000 | Loss: 0.00001325
Iteration 96/1000 | Loss: 0.00001325
Iteration 97/1000 | Loss: 0.00001325
Iteration 98/1000 | Loss: 0.00001325
Iteration 99/1000 | Loss: 0.00001325
Iteration 100/1000 | Loss: 0.00001325
Iteration 101/1000 | Loss: 0.00001325
Iteration 102/1000 | Loss: 0.00001324
Iteration 103/1000 | Loss: 0.00001324
Iteration 104/1000 | Loss: 0.00001324
Iteration 105/1000 | Loss: 0.00001324
Iteration 106/1000 | Loss: 0.00001323
Iteration 107/1000 | Loss: 0.00001323
Iteration 108/1000 | Loss: 0.00001323
Iteration 109/1000 | Loss: 0.00001323
Iteration 110/1000 | Loss: 0.00001322
Iteration 111/1000 | Loss: 0.00001322
Iteration 112/1000 | Loss: 0.00001322
Iteration 113/1000 | Loss: 0.00001322
Iteration 114/1000 | Loss: 0.00001322
Iteration 115/1000 | Loss: 0.00001322
Iteration 116/1000 | Loss: 0.00001322
Iteration 117/1000 | Loss: 0.00001322
Iteration 118/1000 | Loss: 0.00001322
Iteration 119/1000 | Loss: 0.00001322
Iteration 120/1000 | Loss: 0.00001322
Iteration 121/1000 | Loss: 0.00001322
Iteration 122/1000 | Loss: 0.00001322
Iteration 123/1000 | Loss: 0.00001321
Iteration 124/1000 | Loss: 0.00001321
Iteration 125/1000 | Loss: 0.00001321
Iteration 126/1000 | Loss: 0.00001321
Iteration 127/1000 | Loss: 0.00001321
Iteration 128/1000 | Loss: 0.00001321
Iteration 129/1000 | Loss: 0.00001321
Iteration 130/1000 | Loss: 0.00001321
Iteration 131/1000 | Loss: 0.00001321
Iteration 132/1000 | Loss: 0.00001321
Iteration 133/1000 | Loss: 0.00001321
Iteration 134/1000 | Loss: 0.00001321
Iteration 135/1000 | Loss: 0.00001321
Iteration 136/1000 | Loss: 0.00001321
Iteration 137/1000 | Loss: 0.00001321
Iteration 138/1000 | Loss: 0.00001321
Iteration 139/1000 | Loss: 0.00001321
Iteration 140/1000 | Loss: 0.00001321
Iteration 141/1000 | Loss: 0.00001321
Iteration 142/1000 | Loss: 0.00001321
Iteration 143/1000 | Loss: 0.00001321
Iteration 144/1000 | Loss: 0.00001321
Iteration 145/1000 | Loss: 0.00001321
Iteration 146/1000 | Loss: 0.00001321
Iteration 147/1000 | Loss: 0.00001321
Iteration 148/1000 | Loss: 0.00001321
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.3210072211222723e-05, 1.3210072211222723e-05, 1.3210072211222723e-05, 1.3210072211222723e-05, 1.3210072211222723e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3210072211222723e-05

Optimization complete. Final v2v error: 3.100996255874634 mm

Highest mean error: 3.282238483428955 mm for frame 45

Lowest mean error: 2.9760806560516357 mm for frame 102

Saving results

Total time: 34.36987614631653
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00860360
Iteration 2/25 | Loss: 0.00099216
Iteration 3/25 | Loss: 0.00070067
Iteration 4/25 | Loss: 0.00063718
Iteration 5/25 | Loss: 0.00062832
Iteration 6/25 | Loss: 0.00062586
Iteration 7/25 | Loss: 0.00062536
Iteration 8/25 | Loss: 0.00062536
Iteration 9/25 | Loss: 0.00062536
Iteration 10/25 | Loss: 0.00062536
Iteration 11/25 | Loss: 0.00062536
Iteration 12/25 | Loss: 0.00062536
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006253635510802269, 0.0006253635510802269, 0.0006253635510802269, 0.0006253635510802269, 0.0006253635510802269]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006253635510802269

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47703099
Iteration 2/25 | Loss: 0.00029150
Iteration 3/25 | Loss: 0.00029150
Iteration 4/25 | Loss: 0.00029150
Iteration 5/25 | Loss: 0.00029150
Iteration 6/25 | Loss: 0.00029150
Iteration 7/25 | Loss: 0.00029150
Iteration 8/25 | Loss: 0.00029150
Iteration 9/25 | Loss: 0.00029150
Iteration 10/25 | Loss: 0.00029150
Iteration 11/25 | Loss: 0.00029150
Iteration 12/25 | Loss: 0.00029150
Iteration 13/25 | Loss: 0.00029150
Iteration 14/25 | Loss: 0.00029150
Iteration 15/25 | Loss: 0.00029150
Iteration 16/25 | Loss: 0.00029150
Iteration 17/25 | Loss: 0.00029150
Iteration 18/25 | Loss: 0.00029150
Iteration 19/25 | Loss: 0.00029150
Iteration 20/25 | Loss: 0.00029150
Iteration 21/25 | Loss: 0.00029150
Iteration 22/25 | Loss: 0.00029150
Iteration 23/25 | Loss: 0.00029150
Iteration 24/25 | Loss: 0.00029150
Iteration 25/25 | Loss: 0.00029150

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029150
Iteration 2/1000 | Loss: 0.00002350
Iteration 3/1000 | Loss: 0.00001710
Iteration 4/1000 | Loss: 0.00001423
Iteration 5/1000 | Loss: 0.00001333
Iteration 6/1000 | Loss: 0.00001275
Iteration 7/1000 | Loss: 0.00001250
Iteration 8/1000 | Loss: 0.00001220
Iteration 9/1000 | Loss: 0.00001205
Iteration 10/1000 | Loss: 0.00001201
Iteration 11/1000 | Loss: 0.00001197
Iteration 12/1000 | Loss: 0.00001197
Iteration 13/1000 | Loss: 0.00001196
Iteration 14/1000 | Loss: 0.00001194
Iteration 15/1000 | Loss: 0.00001192
Iteration 16/1000 | Loss: 0.00001191
Iteration 17/1000 | Loss: 0.00001190
Iteration 18/1000 | Loss: 0.00001188
Iteration 19/1000 | Loss: 0.00001187
Iteration 20/1000 | Loss: 0.00001186
Iteration 21/1000 | Loss: 0.00001182
Iteration 22/1000 | Loss: 0.00001182
Iteration 23/1000 | Loss: 0.00001182
Iteration 24/1000 | Loss: 0.00001182
Iteration 25/1000 | Loss: 0.00001182
Iteration 26/1000 | Loss: 0.00001181
Iteration 27/1000 | Loss: 0.00001181
Iteration 28/1000 | Loss: 0.00001181
Iteration 29/1000 | Loss: 0.00001181
Iteration 30/1000 | Loss: 0.00001181
Iteration 31/1000 | Loss: 0.00001180
Iteration 32/1000 | Loss: 0.00001180
Iteration 33/1000 | Loss: 0.00001177
Iteration 34/1000 | Loss: 0.00001176
Iteration 35/1000 | Loss: 0.00001175
Iteration 36/1000 | Loss: 0.00001175
Iteration 37/1000 | Loss: 0.00001175
Iteration 38/1000 | Loss: 0.00001174
Iteration 39/1000 | Loss: 0.00001174
Iteration 40/1000 | Loss: 0.00001174
Iteration 41/1000 | Loss: 0.00001173
Iteration 42/1000 | Loss: 0.00001172
Iteration 43/1000 | Loss: 0.00001170
Iteration 44/1000 | Loss: 0.00001170
Iteration 45/1000 | Loss: 0.00001170
Iteration 46/1000 | Loss: 0.00001170
Iteration 47/1000 | Loss: 0.00001170
Iteration 48/1000 | Loss: 0.00001170
Iteration 49/1000 | Loss: 0.00001170
Iteration 50/1000 | Loss: 0.00001170
Iteration 51/1000 | Loss: 0.00001169
Iteration 52/1000 | Loss: 0.00001169
Iteration 53/1000 | Loss: 0.00001169
Iteration 54/1000 | Loss: 0.00001169
Iteration 55/1000 | Loss: 0.00001169
Iteration 56/1000 | Loss: 0.00001169
Iteration 57/1000 | Loss: 0.00001169
Iteration 58/1000 | Loss: 0.00001168
Iteration 59/1000 | Loss: 0.00001168
Iteration 60/1000 | Loss: 0.00001167
Iteration 61/1000 | Loss: 0.00001167
Iteration 62/1000 | Loss: 0.00001167
Iteration 63/1000 | Loss: 0.00001167
Iteration 64/1000 | Loss: 0.00001167
Iteration 65/1000 | Loss: 0.00001166
Iteration 66/1000 | Loss: 0.00001166
Iteration 67/1000 | Loss: 0.00001166
Iteration 68/1000 | Loss: 0.00001166
Iteration 69/1000 | Loss: 0.00001166
Iteration 70/1000 | Loss: 0.00001166
Iteration 71/1000 | Loss: 0.00001166
Iteration 72/1000 | Loss: 0.00001166
Iteration 73/1000 | Loss: 0.00001165
Iteration 74/1000 | Loss: 0.00001165
Iteration 75/1000 | Loss: 0.00001165
Iteration 76/1000 | Loss: 0.00001165
Iteration 77/1000 | Loss: 0.00001164
Iteration 78/1000 | Loss: 0.00001164
Iteration 79/1000 | Loss: 0.00001164
Iteration 80/1000 | Loss: 0.00001163
Iteration 81/1000 | Loss: 0.00001163
Iteration 82/1000 | Loss: 0.00001163
Iteration 83/1000 | Loss: 0.00001162
Iteration 84/1000 | Loss: 0.00001162
Iteration 85/1000 | Loss: 0.00001161
Iteration 86/1000 | Loss: 0.00001161
Iteration 87/1000 | Loss: 0.00001161
Iteration 88/1000 | Loss: 0.00001161
Iteration 89/1000 | Loss: 0.00001160
Iteration 90/1000 | Loss: 0.00001160
Iteration 91/1000 | Loss: 0.00001160
Iteration 92/1000 | Loss: 0.00001160
Iteration 93/1000 | Loss: 0.00001160
Iteration 94/1000 | Loss: 0.00001160
Iteration 95/1000 | Loss: 0.00001160
Iteration 96/1000 | Loss: 0.00001160
Iteration 97/1000 | Loss: 0.00001160
Iteration 98/1000 | Loss: 0.00001160
Iteration 99/1000 | Loss: 0.00001160
Iteration 100/1000 | Loss: 0.00001159
Iteration 101/1000 | Loss: 0.00001159
Iteration 102/1000 | Loss: 0.00001159
Iteration 103/1000 | Loss: 0.00001159
Iteration 104/1000 | Loss: 0.00001159
Iteration 105/1000 | Loss: 0.00001159
Iteration 106/1000 | Loss: 0.00001159
Iteration 107/1000 | Loss: 0.00001158
Iteration 108/1000 | Loss: 0.00001158
Iteration 109/1000 | Loss: 0.00001158
Iteration 110/1000 | Loss: 0.00001158
Iteration 111/1000 | Loss: 0.00001158
Iteration 112/1000 | Loss: 0.00001158
Iteration 113/1000 | Loss: 0.00001158
Iteration 114/1000 | Loss: 0.00001158
Iteration 115/1000 | Loss: 0.00001157
Iteration 116/1000 | Loss: 0.00001157
Iteration 117/1000 | Loss: 0.00001157
Iteration 118/1000 | Loss: 0.00001157
Iteration 119/1000 | Loss: 0.00001157
Iteration 120/1000 | Loss: 0.00001157
Iteration 121/1000 | Loss: 0.00001157
Iteration 122/1000 | Loss: 0.00001157
Iteration 123/1000 | Loss: 0.00001157
Iteration 124/1000 | Loss: 0.00001157
Iteration 125/1000 | Loss: 0.00001157
Iteration 126/1000 | Loss: 0.00001157
Iteration 127/1000 | Loss: 0.00001157
Iteration 128/1000 | Loss: 0.00001157
Iteration 129/1000 | Loss: 0.00001157
Iteration 130/1000 | Loss: 0.00001157
Iteration 131/1000 | Loss: 0.00001157
Iteration 132/1000 | Loss: 0.00001156
Iteration 133/1000 | Loss: 0.00001156
Iteration 134/1000 | Loss: 0.00001156
Iteration 135/1000 | Loss: 0.00001156
Iteration 136/1000 | Loss: 0.00001156
Iteration 137/1000 | Loss: 0.00001156
Iteration 138/1000 | Loss: 0.00001156
Iteration 139/1000 | Loss: 0.00001156
Iteration 140/1000 | Loss: 0.00001156
Iteration 141/1000 | Loss: 0.00001156
Iteration 142/1000 | Loss: 0.00001156
Iteration 143/1000 | Loss: 0.00001156
Iteration 144/1000 | Loss: 0.00001156
Iteration 145/1000 | Loss: 0.00001156
Iteration 146/1000 | Loss: 0.00001156
Iteration 147/1000 | Loss: 0.00001156
Iteration 148/1000 | Loss: 0.00001156
Iteration 149/1000 | Loss: 0.00001156
Iteration 150/1000 | Loss: 0.00001156
Iteration 151/1000 | Loss: 0.00001156
Iteration 152/1000 | Loss: 0.00001156
Iteration 153/1000 | Loss: 0.00001156
Iteration 154/1000 | Loss: 0.00001156
Iteration 155/1000 | Loss: 0.00001156
Iteration 156/1000 | Loss: 0.00001156
Iteration 157/1000 | Loss: 0.00001156
Iteration 158/1000 | Loss: 0.00001156
Iteration 159/1000 | Loss: 0.00001156
Iteration 160/1000 | Loss: 0.00001156
Iteration 161/1000 | Loss: 0.00001156
Iteration 162/1000 | Loss: 0.00001156
Iteration 163/1000 | Loss: 0.00001156
Iteration 164/1000 | Loss: 0.00001156
Iteration 165/1000 | Loss: 0.00001156
Iteration 166/1000 | Loss: 0.00001156
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [1.1555316632438917e-05, 1.1555316632438917e-05, 1.1555316632438917e-05, 1.1555316632438917e-05, 1.1555316632438917e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1555316632438917e-05

Optimization complete. Final v2v error: 2.8907856941223145 mm

Highest mean error: 3.338632345199585 mm for frame 114

Lowest mean error: 2.689405918121338 mm for frame 27

Saving results

Total time: 39.009339809417725
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00489837
Iteration 2/25 | Loss: 0.00101459
Iteration 3/25 | Loss: 0.00073547
Iteration 4/25 | Loss: 0.00069589
Iteration 5/25 | Loss: 0.00068694
Iteration 6/25 | Loss: 0.00068482
Iteration 7/25 | Loss: 0.00068434
Iteration 8/25 | Loss: 0.00068425
Iteration 9/25 | Loss: 0.00068425
Iteration 10/25 | Loss: 0.00068425
Iteration 11/25 | Loss: 0.00068425
Iteration 12/25 | Loss: 0.00068425
Iteration 13/25 | Loss: 0.00068425
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006842502043582499, 0.0006842502043582499, 0.0006842502043582499, 0.0006842502043582499, 0.0006842502043582499]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006842502043582499

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57131195
Iteration 2/25 | Loss: 0.00029872
Iteration 3/25 | Loss: 0.00029868
Iteration 4/25 | Loss: 0.00029868
Iteration 5/25 | Loss: 0.00029868
Iteration 6/25 | Loss: 0.00029868
Iteration 7/25 | Loss: 0.00029868
Iteration 8/25 | Loss: 0.00029868
Iteration 9/25 | Loss: 0.00029868
Iteration 10/25 | Loss: 0.00029868
Iteration 11/25 | Loss: 0.00029868
Iteration 12/25 | Loss: 0.00029868
Iteration 13/25 | Loss: 0.00029868
Iteration 14/25 | Loss: 0.00029868
Iteration 15/25 | Loss: 0.00029868
Iteration 16/25 | Loss: 0.00029868
Iteration 17/25 | Loss: 0.00029868
Iteration 18/25 | Loss: 0.00029868
Iteration 19/25 | Loss: 0.00029868
Iteration 20/25 | Loss: 0.00029868
Iteration 21/25 | Loss: 0.00029868
Iteration 22/25 | Loss: 0.00029868
Iteration 23/25 | Loss: 0.00029868
Iteration 24/25 | Loss: 0.00029868
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0002986811741720885, 0.0002986811741720885, 0.0002986811741720885, 0.0002986811741720885, 0.0002986811741720885]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002986811741720885

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029868
Iteration 2/1000 | Loss: 0.00002645
Iteration 3/1000 | Loss: 0.00002061
Iteration 4/1000 | Loss: 0.00001808
Iteration 5/1000 | Loss: 0.00001713
Iteration 6/1000 | Loss: 0.00001656
Iteration 7/1000 | Loss: 0.00001627
Iteration 8/1000 | Loss: 0.00001600
Iteration 9/1000 | Loss: 0.00001580
Iteration 10/1000 | Loss: 0.00001559
Iteration 11/1000 | Loss: 0.00001553
Iteration 12/1000 | Loss: 0.00001548
Iteration 13/1000 | Loss: 0.00001546
Iteration 14/1000 | Loss: 0.00001532
Iteration 15/1000 | Loss: 0.00001527
Iteration 16/1000 | Loss: 0.00001526
Iteration 17/1000 | Loss: 0.00001525
Iteration 18/1000 | Loss: 0.00001525
Iteration 19/1000 | Loss: 0.00001524
Iteration 20/1000 | Loss: 0.00001524
Iteration 21/1000 | Loss: 0.00001523
Iteration 22/1000 | Loss: 0.00001523
Iteration 23/1000 | Loss: 0.00001523
Iteration 24/1000 | Loss: 0.00001522
Iteration 25/1000 | Loss: 0.00001521
Iteration 26/1000 | Loss: 0.00001520
Iteration 27/1000 | Loss: 0.00001520
Iteration 28/1000 | Loss: 0.00001519
Iteration 29/1000 | Loss: 0.00001519
Iteration 30/1000 | Loss: 0.00001519
Iteration 31/1000 | Loss: 0.00001519
Iteration 32/1000 | Loss: 0.00001518
Iteration 33/1000 | Loss: 0.00001517
Iteration 34/1000 | Loss: 0.00001517
Iteration 35/1000 | Loss: 0.00001517
Iteration 36/1000 | Loss: 0.00001517
Iteration 37/1000 | Loss: 0.00001516
Iteration 38/1000 | Loss: 0.00001516
Iteration 39/1000 | Loss: 0.00001516
Iteration 40/1000 | Loss: 0.00001516
Iteration 41/1000 | Loss: 0.00001516
Iteration 42/1000 | Loss: 0.00001516
Iteration 43/1000 | Loss: 0.00001516
Iteration 44/1000 | Loss: 0.00001516
Iteration 45/1000 | Loss: 0.00001516
Iteration 46/1000 | Loss: 0.00001516
Iteration 47/1000 | Loss: 0.00001516
Iteration 48/1000 | Loss: 0.00001515
Iteration 49/1000 | Loss: 0.00001515
Iteration 50/1000 | Loss: 0.00001515
Iteration 51/1000 | Loss: 0.00001515
Iteration 52/1000 | Loss: 0.00001515
Iteration 53/1000 | Loss: 0.00001515
Iteration 54/1000 | Loss: 0.00001515
Iteration 55/1000 | Loss: 0.00001515
Iteration 56/1000 | Loss: 0.00001515
Iteration 57/1000 | Loss: 0.00001515
Iteration 58/1000 | Loss: 0.00001515
Iteration 59/1000 | Loss: 0.00001515
Iteration 60/1000 | Loss: 0.00001515
Iteration 61/1000 | Loss: 0.00001514
Iteration 62/1000 | Loss: 0.00001514
Iteration 63/1000 | Loss: 0.00001514
Iteration 64/1000 | Loss: 0.00001514
Iteration 65/1000 | Loss: 0.00001514
Iteration 66/1000 | Loss: 0.00001514
Iteration 67/1000 | Loss: 0.00001514
Iteration 68/1000 | Loss: 0.00001514
Iteration 69/1000 | Loss: 0.00001514
Iteration 70/1000 | Loss: 0.00001514
Iteration 71/1000 | Loss: 0.00001513
Iteration 72/1000 | Loss: 0.00001513
Iteration 73/1000 | Loss: 0.00001513
Iteration 74/1000 | Loss: 0.00001513
Iteration 75/1000 | Loss: 0.00001513
Iteration 76/1000 | Loss: 0.00001513
Iteration 77/1000 | Loss: 0.00001513
Iteration 78/1000 | Loss: 0.00001513
Iteration 79/1000 | Loss: 0.00001512
Iteration 80/1000 | Loss: 0.00001512
Iteration 81/1000 | Loss: 0.00001512
Iteration 82/1000 | Loss: 0.00001512
Iteration 83/1000 | Loss: 0.00001512
Iteration 84/1000 | Loss: 0.00001512
Iteration 85/1000 | Loss: 0.00001512
Iteration 86/1000 | Loss: 0.00001512
Iteration 87/1000 | Loss: 0.00001512
Iteration 88/1000 | Loss: 0.00001512
Iteration 89/1000 | Loss: 0.00001511
Iteration 90/1000 | Loss: 0.00001511
Iteration 91/1000 | Loss: 0.00001511
Iteration 92/1000 | Loss: 0.00001511
Iteration 93/1000 | Loss: 0.00001511
Iteration 94/1000 | Loss: 0.00001511
Iteration 95/1000 | Loss: 0.00001511
Iteration 96/1000 | Loss: 0.00001511
Iteration 97/1000 | Loss: 0.00001511
Iteration 98/1000 | Loss: 0.00001511
Iteration 99/1000 | Loss: 0.00001511
Iteration 100/1000 | Loss: 0.00001510
Iteration 101/1000 | Loss: 0.00001510
Iteration 102/1000 | Loss: 0.00001510
Iteration 103/1000 | Loss: 0.00001510
Iteration 104/1000 | Loss: 0.00001510
Iteration 105/1000 | Loss: 0.00001510
Iteration 106/1000 | Loss: 0.00001510
Iteration 107/1000 | Loss: 0.00001509
Iteration 108/1000 | Loss: 0.00001509
Iteration 109/1000 | Loss: 0.00001509
Iteration 110/1000 | Loss: 0.00001509
Iteration 111/1000 | Loss: 0.00001509
Iteration 112/1000 | Loss: 0.00001509
Iteration 113/1000 | Loss: 0.00001509
Iteration 114/1000 | Loss: 0.00001509
Iteration 115/1000 | Loss: 0.00001509
Iteration 116/1000 | Loss: 0.00001509
Iteration 117/1000 | Loss: 0.00001509
Iteration 118/1000 | Loss: 0.00001509
Iteration 119/1000 | Loss: 0.00001509
Iteration 120/1000 | Loss: 0.00001508
Iteration 121/1000 | Loss: 0.00001508
Iteration 122/1000 | Loss: 0.00001508
Iteration 123/1000 | Loss: 0.00001508
Iteration 124/1000 | Loss: 0.00001508
Iteration 125/1000 | Loss: 0.00001508
Iteration 126/1000 | Loss: 0.00001508
Iteration 127/1000 | Loss: 0.00001508
Iteration 128/1000 | Loss: 0.00001508
Iteration 129/1000 | Loss: 0.00001508
Iteration 130/1000 | Loss: 0.00001508
Iteration 131/1000 | Loss: 0.00001508
Iteration 132/1000 | Loss: 0.00001508
Iteration 133/1000 | Loss: 0.00001508
Iteration 134/1000 | Loss: 0.00001508
Iteration 135/1000 | Loss: 0.00001508
Iteration 136/1000 | Loss: 0.00001508
Iteration 137/1000 | Loss: 0.00001508
Iteration 138/1000 | Loss: 0.00001508
Iteration 139/1000 | Loss: 0.00001508
Iteration 140/1000 | Loss: 0.00001508
Iteration 141/1000 | Loss: 0.00001508
Iteration 142/1000 | Loss: 0.00001508
Iteration 143/1000 | Loss: 0.00001508
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.5077037460287102e-05, 1.5077037460287102e-05, 1.5077037460287102e-05, 1.5077037460287102e-05, 1.5077037460287102e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5077037460287102e-05

Optimization complete. Final v2v error: 3.303166627883911 mm

Highest mean error: 3.835784435272217 mm for frame 69

Lowest mean error: 2.779275894165039 mm for frame 93

Saving results

Total time: 34.868680000305176
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01098885
Iteration 2/25 | Loss: 0.01098885
Iteration 3/25 | Loss: 0.00288209
Iteration 4/25 | Loss: 0.00174965
Iteration 5/25 | Loss: 0.00136811
Iteration 6/25 | Loss: 0.00132135
Iteration 7/25 | Loss: 0.00110984
Iteration 8/25 | Loss: 0.00098275
Iteration 9/25 | Loss: 0.00080023
Iteration 10/25 | Loss: 0.00074764
Iteration 11/25 | Loss: 0.00071383
Iteration 12/25 | Loss: 0.00070520
Iteration 13/25 | Loss: 0.00070262
Iteration 14/25 | Loss: 0.00070160
Iteration 15/25 | Loss: 0.00070030
Iteration 16/25 | Loss: 0.00070443
Iteration 17/25 | Loss: 0.00069785
Iteration 18/25 | Loss: 0.00069577
Iteration 19/25 | Loss: 0.00069506
Iteration 20/25 | Loss: 0.00069487
Iteration 21/25 | Loss: 0.00069478
Iteration 22/25 | Loss: 0.00069472
Iteration 23/25 | Loss: 0.00069471
Iteration 24/25 | Loss: 0.00069471
Iteration 25/25 | Loss: 0.00069470

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40801561
Iteration 2/25 | Loss: 0.00040212
Iteration 3/25 | Loss: 0.00040212
Iteration 4/25 | Loss: 0.00040212
Iteration 5/25 | Loss: 0.00040212
Iteration 6/25 | Loss: 0.00040212
Iteration 7/25 | Loss: 0.00040211
Iteration 8/25 | Loss: 0.00040211
Iteration 9/25 | Loss: 0.00040211
Iteration 10/25 | Loss: 0.00040211
Iteration 11/25 | Loss: 0.00040211
Iteration 12/25 | Loss: 0.00040211
Iteration 13/25 | Loss: 0.00040211
Iteration 14/25 | Loss: 0.00040211
Iteration 15/25 | Loss: 0.00040211
Iteration 16/25 | Loss: 0.00040211
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0004021142958663404, 0.0004021142958663404, 0.0004021142958663404, 0.0004021142958663404, 0.0004021142958663404]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004021142958663404

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00040211
Iteration 2/1000 | Loss: 0.00007269
Iteration 3/1000 | Loss: 0.00005037
Iteration 4/1000 | Loss: 0.00004437
Iteration 5/1000 | Loss: 0.00004024
Iteration 6/1000 | Loss: 0.00003874
Iteration 7/1000 | Loss: 0.00110212
Iteration 8/1000 | Loss: 0.00100700
Iteration 9/1000 | Loss: 0.00004518
Iteration 10/1000 | Loss: 0.00003326
Iteration 11/1000 | Loss: 0.00002688
Iteration 12/1000 | Loss: 0.00002332
Iteration 13/1000 | Loss: 0.00002192
Iteration 14/1000 | Loss: 0.00002102
Iteration 15/1000 | Loss: 0.00002044
Iteration 16/1000 | Loss: 0.00002008
Iteration 17/1000 | Loss: 0.00001977
Iteration 18/1000 | Loss: 0.00001948
Iteration 19/1000 | Loss: 0.00001930
Iteration 20/1000 | Loss: 0.00001930
Iteration 21/1000 | Loss: 0.00001927
Iteration 22/1000 | Loss: 0.00001908
Iteration 23/1000 | Loss: 0.00001907
Iteration 24/1000 | Loss: 0.00001897
Iteration 25/1000 | Loss: 0.00001896
Iteration 26/1000 | Loss: 0.00001892
Iteration 27/1000 | Loss: 0.00001892
Iteration 28/1000 | Loss: 0.00001892
Iteration 29/1000 | Loss: 0.00001891
Iteration 30/1000 | Loss: 0.00001891
Iteration 31/1000 | Loss: 0.00001891
Iteration 32/1000 | Loss: 0.00001890
Iteration 33/1000 | Loss: 0.00001890
Iteration 34/1000 | Loss: 0.00001890
Iteration 35/1000 | Loss: 0.00001889
Iteration 36/1000 | Loss: 0.00001889
Iteration 37/1000 | Loss: 0.00001889
Iteration 38/1000 | Loss: 0.00001889
Iteration 39/1000 | Loss: 0.00001888
Iteration 40/1000 | Loss: 0.00001888
Iteration 41/1000 | Loss: 0.00001888
Iteration 42/1000 | Loss: 0.00001888
Iteration 43/1000 | Loss: 0.00001887
Iteration 44/1000 | Loss: 0.00001887
Iteration 45/1000 | Loss: 0.00001887
Iteration 46/1000 | Loss: 0.00001887
Iteration 47/1000 | Loss: 0.00001887
Iteration 48/1000 | Loss: 0.00001887
Iteration 49/1000 | Loss: 0.00001887
Iteration 50/1000 | Loss: 0.00001887
Iteration 51/1000 | Loss: 0.00001887
Iteration 52/1000 | Loss: 0.00001886
Iteration 53/1000 | Loss: 0.00001886
Iteration 54/1000 | Loss: 0.00001886
Iteration 55/1000 | Loss: 0.00001886
Iteration 56/1000 | Loss: 0.00001886
Iteration 57/1000 | Loss: 0.00001886
Iteration 58/1000 | Loss: 0.00001886
Iteration 59/1000 | Loss: 0.00001886
Iteration 60/1000 | Loss: 0.00001886
Iteration 61/1000 | Loss: 0.00001886
Iteration 62/1000 | Loss: 0.00001886
Iteration 63/1000 | Loss: 0.00001886
Iteration 64/1000 | Loss: 0.00001886
Iteration 65/1000 | Loss: 0.00001886
Iteration 66/1000 | Loss: 0.00001885
Iteration 67/1000 | Loss: 0.00001885
Iteration 68/1000 | Loss: 0.00001885
Iteration 69/1000 | Loss: 0.00001885
Iteration 70/1000 | Loss: 0.00001885
Iteration 71/1000 | Loss: 0.00001885
Iteration 72/1000 | Loss: 0.00001885
Iteration 73/1000 | Loss: 0.00001885
Iteration 74/1000 | Loss: 0.00001885
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 74. Stopping optimization.
Last 5 losses: [1.8852042558137327e-05, 1.8852042558137327e-05, 1.8852042558137327e-05, 1.8852042558137327e-05, 1.8852042558137327e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8852042558137327e-05

Optimization complete. Final v2v error: 3.639547109603882 mm

Highest mean error: 4.005002498626709 mm for frame 174

Lowest mean error: 3.2098677158355713 mm for frame 98

Saving results

Total time: 70.3613555431366
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00422038
Iteration 2/25 | Loss: 0.00087109
Iteration 3/25 | Loss: 0.00066485
Iteration 4/25 | Loss: 0.00064144
Iteration 5/25 | Loss: 0.00063514
Iteration 6/25 | Loss: 0.00063309
Iteration 7/25 | Loss: 0.00063285
Iteration 8/25 | Loss: 0.00063285
Iteration 9/25 | Loss: 0.00063285
Iteration 10/25 | Loss: 0.00063285
Iteration 11/25 | Loss: 0.00063285
Iteration 12/25 | Loss: 0.00063285
Iteration 13/25 | Loss: 0.00063285
Iteration 14/25 | Loss: 0.00063285
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0006328527815639973, 0.0006328527815639973, 0.0006328527815639973, 0.0006328527815639973, 0.0006328527815639973]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006328527815639973

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44909787
Iteration 2/25 | Loss: 0.00037771
Iteration 3/25 | Loss: 0.00037771
Iteration 4/25 | Loss: 0.00037771
Iteration 5/25 | Loss: 0.00037771
Iteration 6/25 | Loss: 0.00037771
Iteration 7/25 | Loss: 0.00037771
Iteration 8/25 | Loss: 0.00037771
Iteration 9/25 | Loss: 0.00037771
Iteration 10/25 | Loss: 0.00037771
Iteration 11/25 | Loss: 0.00037771
Iteration 12/25 | Loss: 0.00037771
Iteration 13/25 | Loss: 0.00037771
Iteration 14/25 | Loss: 0.00037771
Iteration 15/25 | Loss: 0.00037771
Iteration 16/25 | Loss: 0.00037771
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00037770907511003315, 0.00037770907511003315, 0.00037770907511003315, 0.00037770907511003315, 0.00037770907511003315]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00037770907511003315

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037771
Iteration 2/1000 | Loss: 0.00002265
Iteration 3/1000 | Loss: 0.00001599
Iteration 4/1000 | Loss: 0.00001478
Iteration 5/1000 | Loss: 0.00001399
Iteration 6/1000 | Loss: 0.00001362
Iteration 7/1000 | Loss: 0.00001337
Iteration 8/1000 | Loss: 0.00001328
Iteration 9/1000 | Loss: 0.00001323
Iteration 10/1000 | Loss: 0.00001315
Iteration 11/1000 | Loss: 0.00001312
Iteration 12/1000 | Loss: 0.00001311
Iteration 13/1000 | Loss: 0.00001309
Iteration 14/1000 | Loss: 0.00001308
Iteration 15/1000 | Loss: 0.00001308
Iteration 16/1000 | Loss: 0.00001308
Iteration 17/1000 | Loss: 0.00001308
Iteration 18/1000 | Loss: 0.00001308
Iteration 19/1000 | Loss: 0.00001306
Iteration 20/1000 | Loss: 0.00001306
Iteration 21/1000 | Loss: 0.00001301
Iteration 22/1000 | Loss: 0.00001301
Iteration 23/1000 | Loss: 0.00001300
Iteration 24/1000 | Loss: 0.00001299
Iteration 25/1000 | Loss: 0.00001298
Iteration 26/1000 | Loss: 0.00001294
Iteration 27/1000 | Loss: 0.00001291
Iteration 28/1000 | Loss: 0.00001291
Iteration 29/1000 | Loss: 0.00001289
Iteration 30/1000 | Loss: 0.00001288
Iteration 31/1000 | Loss: 0.00001288
Iteration 32/1000 | Loss: 0.00001288
Iteration 33/1000 | Loss: 0.00001287
Iteration 34/1000 | Loss: 0.00001287
Iteration 35/1000 | Loss: 0.00001287
Iteration 36/1000 | Loss: 0.00001285
Iteration 37/1000 | Loss: 0.00001285
Iteration 38/1000 | Loss: 0.00001284
Iteration 39/1000 | Loss: 0.00001284
Iteration 40/1000 | Loss: 0.00001284
Iteration 41/1000 | Loss: 0.00001284
Iteration 42/1000 | Loss: 0.00001284
Iteration 43/1000 | Loss: 0.00001283
Iteration 44/1000 | Loss: 0.00001283
Iteration 45/1000 | Loss: 0.00001283
Iteration 46/1000 | Loss: 0.00001282
Iteration 47/1000 | Loss: 0.00001282
Iteration 48/1000 | Loss: 0.00001282
Iteration 49/1000 | Loss: 0.00001282
Iteration 50/1000 | Loss: 0.00001281
Iteration 51/1000 | Loss: 0.00001281
Iteration 52/1000 | Loss: 0.00001280
Iteration 53/1000 | Loss: 0.00001280
Iteration 54/1000 | Loss: 0.00001280
Iteration 55/1000 | Loss: 0.00001280
Iteration 56/1000 | Loss: 0.00001280
Iteration 57/1000 | Loss: 0.00001279
Iteration 58/1000 | Loss: 0.00001279
Iteration 59/1000 | Loss: 0.00001279
Iteration 60/1000 | Loss: 0.00001279
Iteration 61/1000 | Loss: 0.00001278
Iteration 62/1000 | Loss: 0.00001278
Iteration 63/1000 | Loss: 0.00001278
Iteration 64/1000 | Loss: 0.00001278
Iteration 65/1000 | Loss: 0.00001278
Iteration 66/1000 | Loss: 0.00001278
Iteration 67/1000 | Loss: 0.00001278
Iteration 68/1000 | Loss: 0.00001278
Iteration 69/1000 | Loss: 0.00001278
Iteration 70/1000 | Loss: 0.00001278
Iteration 71/1000 | Loss: 0.00001278
Iteration 72/1000 | Loss: 0.00001278
Iteration 73/1000 | Loss: 0.00001278
Iteration 74/1000 | Loss: 0.00001277
Iteration 75/1000 | Loss: 0.00001277
Iteration 76/1000 | Loss: 0.00001277
Iteration 77/1000 | Loss: 0.00001277
Iteration 78/1000 | Loss: 0.00001277
Iteration 79/1000 | Loss: 0.00001277
Iteration 80/1000 | Loss: 0.00001277
Iteration 81/1000 | Loss: 0.00001277
Iteration 82/1000 | Loss: 0.00001277
Iteration 83/1000 | Loss: 0.00001277
Iteration 84/1000 | Loss: 0.00001276
Iteration 85/1000 | Loss: 0.00001276
Iteration 86/1000 | Loss: 0.00001276
Iteration 87/1000 | Loss: 0.00001276
Iteration 88/1000 | Loss: 0.00001276
Iteration 89/1000 | Loss: 0.00001276
Iteration 90/1000 | Loss: 0.00001276
Iteration 91/1000 | Loss: 0.00001276
Iteration 92/1000 | Loss: 0.00001276
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [1.2763714948960114e-05, 1.2763714948960114e-05, 1.2763714948960114e-05, 1.2763714948960114e-05, 1.2763714948960114e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2763714948960114e-05

Optimization complete. Final v2v error: 2.999872922897339 mm

Highest mean error: 3.495478868484497 mm for frame 8

Lowest mean error: 2.78659987449646 mm for frame 20

Saving results

Total time: 29.79742932319641
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00389756
Iteration 2/25 | Loss: 0.00082457
Iteration 3/25 | Loss: 0.00064955
Iteration 4/25 | Loss: 0.00062078
Iteration 5/25 | Loss: 0.00061229
Iteration 6/25 | Loss: 0.00060912
Iteration 7/25 | Loss: 0.00060848
Iteration 8/25 | Loss: 0.00060848
Iteration 9/25 | Loss: 0.00060848
Iteration 10/25 | Loss: 0.00060848
Iteration 11/25 | Loss: 0.00060848
Iteration 12/25 | Loss: 0.00060848
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006084847846068442, 0.0006084847846068442, 0.0006084847846068442, 0.0006084847846068442, 0.0006084847846068442]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006084847846068442

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44683206
Iteration 2/25 | Loss: 0.00023300
Iteration 3/25 | Loss: 0.00023300
Iteration 4/25 | Loss: 0.00023300
Iteration 5/25 | Loss: 0.00023299
Iteration 6/25 | Loss: 0.00023299
Iteration 7/25 | Loss: 0.00023299
Iteration 8/25 | Loss: 0.00023299
Iteration 9/25 | Loss: 0.00023299
Iteration 10/25 | Loss: 0.00023299
Iteration 11/25 | Loss: 0.00023299
Iteration 12/25 | Loss: 0.00023299
Iteration 13/25 | Loss: 0.00023299
Iteration 14/25 | Loss: 0.00023299
Iteration 15/25 | Loss: 0.00023299
Iteration 16/25 | Loss: 0.00023299
Iteration 17/25 | Loss: 0.00023299
Iteration 18/25 | Loss: 0.00023299
Iteration 19/25 | Loss: 0.00023299
Iteration 20/25 | Loss: 0.00023299
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00023299259191844612, 0.00023299259191844612, 0.00023299259191844612, 0.00023299259191844612, 0.00023299259191844612]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00023299259191844612

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00023299
Iteration 2/1000 | Loss: 0.00002408
Iteration 3/1000 | Loss: 0.00001890
Iteration 4/1000 | Loss: 0.00001753
Iteration 5/1000 | Loss: 0.00001653
Iteration 6/1000 | Loss: 0.00001597
Iteration 7/1000 | Loss: 0.00001559
Iteration 8/1000 | Loss: 0.00001547
Iteration 9/1000 | Loss: 0.00001547
Iteration 10/1000 | Loss: 0.00001524
Iteration 11/1000 | Loss: 0.00001509
Iteration 12/1000 | Loss: 0.00001507
Iteration 13/1000 | Loss: 0.00001506
Iteration 14/1000 | Loss: 0.00001506
Iteration 15/1000 | Loss: 0.00001506
Iteration 16/1000 | Loss: 0.00001506
Iteration 17/1000 | Loss: 0.00001506
Iteration 18/1000 | Loss: 0.00001506
Iteration 19/1000 | Loss: 0.00001506
Iteration 20/1000 | Loss: 0.00001505
Iteration 21/1000 | Loss: 0.00001504
Iteration 22/1000 | Loss: 0.00001500
Iteration 23/1000 | Loss: 0.00001499
Iteration 24/1000 | Loss: 0.00001497
Iteration 25/1000 | Loss: 0.00001489
Iteration 26/1000 | Loss: 0.00001487
Iteration 27/1000 | Loss: 0.00001487
Iteration 28/1000 | Loss: 0.00001486
Iteration 29/1000 | Loss: 0.00001486
Iteration 30/1000 | Loss: 0.00001485
Iteration 31/1000 | Loss: 0.00001484
Iteration 32/1000 | Loss: 0.00001484
Iteration 33/1000 | Loss: 0.00001484
Iteration 34/1000 | Loss: 0.00001483
Iteration 35/1000 | Loss: 0.00001483
Iteration 36/1000 | Loss: 0.00001483
Iteration 37/1000 | Loss: 0.00001483
Iteration 38/1000 | Loss: 0.00001483
Iteration 39/1000 | Loss: 0.00001483
Iteration 40/1000 | Loss: 0.00001483
Iteration 41/1000 | Loss: 0.00001483
Iteration 42/1000 | Loss: 0.00001483
Iteration 43/1000 | Loss: 0.00001483
Iteration 44/1000 | Loss: 0.00001483
Iteration 45/1000 | Loss: 0.00001483
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 45. Stopping optimization.
Last 5 losses: [1.4832154192845337e-05, 1.4832154192845337e-05, 1.4832154192845337e-05, 1.4832154192845337e-05, 1.4832154192845337e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4832154192845337e-05

Optimization complete. Final v2v error: 3.2295029163360596 mm

Highest mean error: 3.524003267288208 mm for frame 111

Lowest mean error: 2.9509716033935547 mm for frame 16

Saving results

Total time: 27.627996683120728
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00483437
Iteration 2/25 | Loss: 0.00089284
Iteration 3/25 | Loss: 0.00067990
Iteration 4/25 | Loss: 0.00064801
Iteration 5/25 | Loss: 0.00063670
Iteration 6/25 | Loss: 0.00063428
Iteration 7/25 | Loss: 0.00063391
Iteration 8/25 | Loss: 0.00063391
Iteration 9/25 | Loss: 0.00063391
Iteration 10/25 | Loss: 0.00063391
Iteration 11/25 | Loss: 0.00063391
Iteration 12/25 | Loss: 0.00063391
Iteration 13/25 | Loss: 0.00063391
Iteration 14/25 | Loss: 0.00063391
Iteration 15/25 | Loss: 0.00063391
Iteration 16/25 | Loss: 0.00063391
Iteration 17/25 | Loss: 0.00063391
Iteration 18/25 | Loss: 0.00063391
Iteration 19/25 | Loss: 0.00063391
Iteration 20/25 | Loss: 0.00063391
Iteration 21/25 | Loss: 0.00063391
Iteration 22/25 | Loss: 0.00063391
Iteration 23/25 | Loss: 0.00063391
Iteration 24/25 | Loss: 0.00063391
Iteration 25/25 | Loss: 0.00063391

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.85992879
Iteration 2/25 | Loss: 0.00023791
Iteration 3/25 | Loss: 0.00023791
Iteration 4/25 | Loss: 0.00023791
Iteration 5/25 | Loss: 0.00023791
Iteration 6/25 | Loss: 0.00023791
Iteration 7/25 | Loss: 0.00023791
Iteration 8/25 | Loss: 0.00023791
Iteration 9/25 | Loss: 0.00023791
Iteration 10/25 | Loss: 0.00023791
Iteration 11/25 | Loss: 0.00023791
Iteration 12/25 | Loss: 0.00023791
Iteration 13/25 | Loss: 0.00023791
Iteration 14/25 | Loss: 0.00023791
Iteration 15/25 | Loss: 0.00023791
Iteration 16/25 | Loss: 0.00023791
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00023790550767444074, 0.00023790550767444074, 0.00023790550767444074, 0.00023790550767444074, 0.00023790550767444074]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00023790550767444074

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00023791
Iteration 2/1000 | Loss: 0.00002758
Iteration 3/1000 | Loss: 0.00001893
Iteration 4/1000 | Loss: 0.00001775
Iteration 5/1000 | Loss: 0.00001703
Iteration 6/1000 | Loss: 0.00001637
Iteration 7/1000 | Loss: 0.00001595
Iteration 8/1000 | Loss: 0.00001557
Iteration 9/1000 | Loss: 0.00001537
Iteration 10/1000 | Loss: 0.00001520
Iteration 11/1000 | Loss: 0.00001519
Iteration 12/1000 | Loss: 0.00001517
Iteration 13/1000 | Loss: 0.00001515
Iteration 14/1000 | Loss: 0.00001507
Iteration 15/1000 | Loss: 0.00001498
Iteration 16/1000 | Loss: 0.00001498
Iteration 17/1000 | Loss: 0.00001497
Iteration 18/1000 | Loss: 0.00001497
Iteration 19/1000 | Loss: 0.00001496
Iteration 20/1000 | Loss: 0.00001494
Iteration 21/1000 | Loss: 0.00001491
Iteration 22/1000 | Loss: 0.00001490
Iteration 23/1000 | Loss: 0.00001490
Iteration 24/1000 | Loss: 0.00001487
Iteration 25/1000 | Loss: 0.00001477
Iteration 26/1000 | Loss: 0.00001476
Iteration 27/1000 | Loss: 0.00001476
Iteration 28/1000 | Loss: 0.00001475
Iteration 29/1000 | Loss: 0.00001474
Iteration 30/1000 | Loss: 0.00001468
Iteration 31/1000 | Loss: 0.00001468
Iteration 32/1000 | Loss: 0.00001467
Iteration 33/1000 | Loss: 0.00001467
Iteration 34/1000 | Loss: 0.00001466
Iteration 35/1000 | Loss: 0.00001466
Iteration 36/1000 | Loss: 0.00001465
Iteration 37/1000 | Loss: 0.00001465
Iteration 38/1000 | Loss: 0.00001465
Iteration 39/1000 | Loss: 0.00001464
Iteration 40/1000 | Loss: 0.00001464
Iteration 41/1000 | Loss: 0.00001463
Iteration 42/1000 | Loss: 0.00001463
Iteration 43/1000 | Loss: 0.00001463
Iteration 44/1000 | Loss: 0.00001462
Iteration 45/1000 | Loss: 0.00001462
Iteration 46/1000 | Loss: 0.00001462
Iteration 47/1000 | Loss: 0.00001461
Iteration 48/1000 | Loss: 0.00001461
Iteration 49/1000 | Loss: 0.00001461
Iteration 50/1000 | Loss: 0.00001460
Iteration 51/1000 | Loss: 0.00001460
Iteration 52/1000 | Loss: 0.00001458
Iteration 53/1000 | Loss: 0.00001458
Iteration 54/1000 | Loss: 0.00001457
Iteration 55/1000 | Loss: 0.00001456
Iteration 56/1000 | Loss: 0.00001455
Iteration 57/1000 | Loss: 0.00001455
Iteration 58/1000 | Loss: 0.00001454
Iteration 59/1000 | Loss: 0.00001454
Iteration 60/1000 | Loss: 0.00001454
Iteration 61/1000 | Loss: 0.00001453
Iteration 62/1000 | Loss: 0.00001453
Iteration 63/1000 | Loss: 0.00001453
Iteration 64/1000 | Loss: 0.00001453
Iteration 65/1000 | Loss: 0.00001453
Iteration 66/1000 | Loss: 0.00001453
Iteration 67/1000 | Loss: 0.00001453
Iteration 68/1000 | Loss: 0.00001452
Iteration 69/1000 | Loss: 0.00001452
Iteration 70/1000 | Loss: 0.00001452
Iteration 71/1000 | Loss: 0.00001452
Iteration 72/1000 | Loss: 0.00001452
Iteration 73/1000 | Loss: 0.00001451
Iteration 74/1000 | Loss: 0.00001451
Iteration 75/1000 | Loss: 0.00001450
Iteration 76/1000 | Loss: 0.00001449
Iteration 77/1000 | Loss: 0.00001449
Iteration 78/1000 | Loss: 0.00001449
Iteration 79/1000 | Loss: 0.00001448
Iteration 80/1000 | Loss: 0.00001448
Iteration 81/1000 | Loss: 0.00001448
Iteration 82/1000 | Loss: 0.00001448
Iteration 83/1000 | Loss: 0.00001448
Iteration 84/1000 | Loss: 0.00001448
Iteration 85/1000 | Loss: 0.00001447
Iteration 86/1000 | Loss: 0.00001447
Iteration 87/1000 | Loss: 0.00001447
Iteration 88/1000 | Loss: 0.00001447
Iteration 89/1000 | Loss: 0.00001447
Iteration 90/1000 | Loss: 0.00001447
Iteration 91/1000 | Loss: 0.00001447
Iteration 92/1000 | Loss: 0.00001447
Iteration 93/1000 | Loss: 0.00001446
Iteration 94/1000 | Loss: 0.00001446
Iteration 95/1000 | Loss: 0.00001446
Iteration 96/1000 | Loss: 0.00001445
Iteration 97/1000 | Loss: 0.00001445
Iteration 98/1000 | Loss: 0.00001445
Iteration 99/1000 | Loss: 0.00001445
Iteration 100/1000 | Loss: 0.00001445
Iteration 101/1000 | Loss: 0.00001445
Iteration 102/1000 | Loss: 0.00001445
Iteration 103/1000 | Loss: 0.00001445
Iteration 104/1000 | Loss: 0.00001445
Iteration 105/1000 | Loss: 0.00001445
Iteration 106/1000 | Loss: 0.00001444
Iteration 107/1000 | Loss: 0.00001444
Iteration 108/1000 | Loss: 0.00001444
Iteration 109/1000 | Loss: 0.00001444
Iteration 110/1000 | Loss: 0.00001444
Iteration 111/1000 | Loss: 0.00001444
Iteration 112/1000 | Loss: 0.00001444
Iteration 113/1000 | Loss: 0.00001444
Iteration 114/1000 | Loss: 0.00001443
Iteration 115/1000 | Loss: 0.00001443
Iteration 116/1000 | Loss: 0.00001443
Iteration 117/1000 | Loss: 0.00001443
Iteration 118/1000 | Loss: 0.00001443
Iteration 119/1000 | Loss: 0.00001443
Iteration 120/1000 | Loss: 0.00001442
Iteration 121/1000 | Loss: 0.00001442
Iteration 122/1000 | Loss: 0.00001442
Iteration 123/1000 | Loss: 0.00001442
Iteration 124/1000 | Loss: 0.00001441
Iteration 125/1000 | Loss: 0.00001441
Iteration 126/1000 | Loss: 0.00001441
Iteration 127/1000 | Loss: 0.00001441
Iteration 128/1000 | Loss: 0.00001440
Iteration 129/1000 | Loss: 0.00001440
Iteration 130/1000 | Loss: 0.00001440
Iteration 131/1000 | Loss: 0.00001440
Iteration 132/1000 | Loss: 0.00001440
Iteration 133/1000 | Loss: 0.00001439
Iteration 134/1000 | Loss: 0.00001439
Iteration 135/1000 | Loss: 0.00001439
Iteration 136/1000 | Loss: 0.00001439
Iteration 137/1000 | Loss: 0.00001439
Iteration 138/1000 | Loss: 0.00001439
Iteration 139/1000 | Loss: 0.00001439
Iteration 140/1000 | Loss: 0.00001439
Iteration 141/1000 | Loss: 0.00001438
Iteration 142/1000 | Loss: 0.00001438
Iteration 143/1000 | Loss: 0.00001438
Iteration 144/1000 | Loss: 0.00001438
Iteration 145/1000 | Loss: 0.00001438
Iteration 146/1000 | Loss: 0.00001438
Iteration 147/1000 | Loss: 0.00001438
Iteration 148/1000 | Loss: 0.00001438
Iteration 149/1000 | Loss: 0.00001437
Iteration 150/1000 | Loss: 0.00001437
Iteration 151/1000 | Loss: 0.00001437
Iteration 152/1000 | Loss: 0.00001437
Iteration 153/1000 | Loss: 0.00001437
Iteration 154/1000 | Loss: 0.00001437
Iteration 155/1000 | Loss: 0.00001436
Iteration 156/1000 | Loss: 0.00001436
Iteration 157/1000 | Loss: 0.00001436
Iteration 158/1000 | Loss: 0.00001436
Iteration 159/1000 | Loss: 0.00001436
Iteration 160/1000 | Loss: 0.00001436
Iteration 161/1000 | Loss: 0.00001436
Iteration 162/1000 | Loss: 0.00001436
Iteration 163/1000 | Loss: 0.00001436
Iteration 164/1000 | Loss: 0.00001436
Iteration 165/1000 | Loss: 0.00001436
Iteration 166/1000 | Loss: 0.00001436
Iteration 167/1000 | Loss: 0.00001436
Iteration 168/1000 | Loss: 0.00001436
Iteration 169/1000 | Loss: 0.00001435
Iteration 170/1000 | Loss: 0.00001435
Iteration 171/1000 | Loss: 0.00001435
Iteration 172/1000 | Loss: 0.00001435
Iteration 173/1000 | Loss: 0.00001435
Iteration 174/1000 | Loss: 0.00001435
Iteration 175/1000 | Loss: 0.00001435
Iteration 176/1000 | Loss: 0.00001435
Iteration 177/1000 | Loss: 0.00001435
Iteration 178/1000 | Loss: 0.00001435
Iteration 179/1000 | Loss: 0.00001435
Iteration 180/1000 | Loss: 0.00001435
Iteration 181/1000 | Loss: 0.00001435
Iteration 182/1000 | Loss: 0.00001435
Iteration 183/1000 | Loss: 0.00001435
Iteration 184/1000 | Loss: 0.00001435
Iteration 185/1000 | Loss: 0.00001435
Iteration 186/1000 | Loss: 0.00001435
Iteration 187/1000 | Loss: 0.00001435
Iteration 188/1000 | Loss: 0.00001435
Iteration 189/1000 | Loss: 0.00001435
Iteration 190/1000 | Loss: 0.00001435
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [1.4350345736602321e-05, 1.4350345736602321e-05, 1.4350345736602321e-05, 1.4350345736602321e-05, 1.4350345736602321e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4350345736602321e-05

Optimization complete. Final v2v error: 3.2483928203582764 mm

Highest mean error: 3.793548107147217 mm for frame 4

Lowest mean error: 3.148042678833008 mm for frame 100

Saving results

Total time: 48.001851320266724
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01117578
Iteration 2/25 | Loss: 0.00278230
Iteration 3/25 | Loss: 0.00198260
Iteration 4/25 | Loss: 0.00167037
Iteration 5/25 | Loss: 0.00167826
Iteration 6/25 | Loss: 0.00146968
Iteration 7/25 | Loss: 0.00134228
Iteration 8/25 | Loss: 0.00123697
Iteration 9/25 | Loss: 0.00120569
Iteration 10/25 | Loss: 0.00116491
Iteration 11/25 | Loss: 0.00114289
Iteration 12/25 | Loss: 0.00112857
Iteration 13/25 | Loss: 0.00112898
Iteration 14/25 | Loss: 0.00111979
Iteration 15/25 | Loss: 0.00108978
Iteration 16/25 | Loss: 0.00108564
Iteration 17/25 | Loss: 0.00108193
Iteration 18/25 | Loss: 0.00108576
Iteration 19/25 | Loss: 0.00107803
Iteration 20/25 | Loss: 0.00107401
Iteration 21/25 | Loss: 0.00106904
Iteration 22/25 | Loss: 0.00106835
Iteration 23/25 | Loss: 0.00107263
Iteration 24/25 | Loss: 0.00106790
Iteration 25/25 | Loss: 0.00106751

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.58698225
Iteration 2/25 | Loss: 0.00177471
Iteration 3/25 | Loss: 0.00153166
Iteration 4/25 | Loss: 0.00153166
Iteration 5/25 | Loss: 0.00153166
Iteration 6/25 | Loss: 0.00153166
Iteration 7/25 | Loss: 0.00153166
Iteration 8/25 | Loss: 0.00153166
Iteration 9/25 | Loss: 0.00153166
Iteration 10/25 | Loss: 0.00153166
Iteration 11/25 | Loss: 0.00153166
Iteration 12/25 | Loss: 0.00153166
Iteration 13/25 | Loss: 0.00153166
Iteration 14/25 | Loss: 0.00153166
Iteration 15/25 | Loss: 0.00153166
Iteration 16/25 | Loss: 0.00153166
Iteration 17/25 | Loss: 0.00153166
Iteration 18/25 | Loss: 0.00153166
Iteration 19/25 | Loss: 0.00153166
Iteration 20/25 | Loss: 0.00153166
Iteration 21/25 | Loss: 0.00153166
Iteration 22/25 | Loss: 0.00153166
Iteration 23/25 | Loss: 0.00153166
Iteration 24/25 | Loss: 0.00153166
Iteration 25/25 | Loss: 0.00153166

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00153166
Iteration 2/1000 | Loss: 0.00049654
Iteration 3/1000 | Loss: 0.00034451
Iteration 4/1000 | Loss: 0.00026722
Iteration 5/1000 | Loss: 0.00015260
Iteration 6/1000 | Loss: 0.00012841
Iteration 7/1000 | Loss: 0.00067641
Iteration 8/1000 | Loss: 0.00314947
Iteration 9/1000 | Loss: 0.00241857
Iteration 10/1000 | Loss: 0.00191299
Iteration 11/1000 | Loss: 0.00020439
Iteration 12/1000 | Loss: 0.00026101
Iteration 13/1000 | Loss: 0.00012157
Iteration 14/1000 | Loss: 0.00012060
Iteration 15/1000 | Loss: 0.00072361
Iteration 16/1000 | Loss: 0.00169097
Iteration 17/1000 | Loss: 0.00161761
Iteration 18/1000 | Loss: 0.00433131
Iteration 19/1000 | Loss: 0.00252061
Iteration 20/1000 | Loss: 0.00110019
Iteration 21/1000 | Loss: 0.00009594
Iteration 22/1000 | Loss: 0.00016076
Iteration 23/1000 | Loss: 0.00023458
Iteration 24/1000 | Loss: 0.00007943
Iteration 25/1000 | Loss: 0.00012379
Iteration 26/1000 | Loss: 0.00005743
Iteration 27/1000 | Loss: 0.00007814
Iteration 28/1000 | Loss: 0.00006144
Iteration 29/1000 | Loss: 0.00008591
Iteration 30/1000 | Loss: 0.00005422
Iteration 31/1000 | Loss: 0.00012024
Iteration 32/1000 | Loss: 0.00008307
Iteration 33/1000 | Loss: 0.00013106
Iteration 34/1000 | Loss: 0.00005591
Iteration 35/1000 | Loss: 0.00005913
Iteration 36/1000 | Loss: 0.00005112
Iteration 37/1000 | Loss: 0.00005071
Iteration 38/1000 | Loss: 0.00005035
Iteration 39/1000 | Loss: 0.00005012
Iteration 40/1000 | Loss: 0.00004993
Iteration 41/1000 | Loss: 0.00004978
Iteration 42/1000 | Loss: 0.00004970
Iteration 43/1000 | Loss: 0.00004960
Iteration 44/1000 | Loss: 0.00004953
Iteration 45/1000 | Loss: 0.00004948
Iteration 46/1000 | Loss: 0.00004943
Iteration 47/1000 | Loss: 0.00004942
Iteration 48/1000 | Loss: 0.00004937
Iteration 49/1000 | Loss: 0.00004937
Iteration 50/1000 | Loss: 0.00004936
Iteration 51/1000 | Loss: 0.00004936
Iteration 52/1000 | Loss: 0.00004933
Iteration 53/1000 | Loss: 0.00004932
Iteration 54/1000 | Loss: 0.00004931
Iteration 55/1000 | Loss: 0.00004931
Iteration 56/1000 | Loss: 0.00004930
Iteration 57/1000 | Loss: 0.00004930
Iteration 58/1000 | Loss: 0.00004930
Iteration 59/1000 | Loss: 0.00004929
Iteration 60/1000 | Loss: 0.00004924
Iteration 61/1000 | Loss: 0.00004922
Iteration 62/1000 | Loss: 0.00004921
Iteration 63/1000 | Loss: 0.00004921
Iteration 64/1000 | Loss: 0.00004921
Iteration 65/1000 | Loss: 0.00004921
Iteration 66/1000 | Loss: 0.00004921
Iteration 67/1000 | Loss: 0.00004917
Iteration 68/1000 | Loss: 0.00004917
Iteration 69/1000 | Loss: 0.00004917
Iteration 70/1000 | Loss: 0.00004917
Iteration 71/1000 | Loss: 0.00004916
Iteration 72/1000 | Loss: 0.00004916
Iteration 73/1000 | Loss: 0.00004916
Iteration 74/1000 | Loss: 0.00004915
Iteration 75/1000 | Loss: 0.00004913
Iteration 76/1000 | Loss: 0.00004913
Iteration 77/1000 | Loss: 0.00004913
Iteration 78/1000 | Loss: 0.00004912
Iteration 79/1000 | Loss: 0.00004912
Iteration 80/1000 | Loss: 0.00004912
Iteration 81/1000 | Loss: 0.00004912
Iteration 82/1000 | Loss: 0.00004912
Iteration 83/1000 | Loss: 0.00004912
Iteration 84/1000 | Loss: 0.00004912
Iteration 85/1000 | Loss: 0.00004912
Iteration 86/1000 | Loss: 0.00004912
Iteration 87/1000 | Loss: 0.00004912
Iteration 88/1000 | Loss: 0.00004911
Iteration 89/1000 | Loss: 0.00004911
Iteration 90/1000 | Loss: 0.00004911
Iteration 91/1000 | Loss: 0.00004910
Iteration 92/1000 | Loss: 0.00004910
Iteration 93/1000 | Loss: 0.00004909
Iteration 94/1000 | Loss: 0.00004909
Iteration 95/1000 | Loss: 0.00004909
Iteration 96/1000 | Loss: 0.00004909
Iteration 97/1000 | Loss: 0.00004909
Iteration 98/1000 | Loss: 0.00004909
Iteration 99/1000 | Loss: 0.00004909
Iteration 100/1000 | Loss: 0.00004909
Iteration 101/1000 | Loss: 0.00004909
Iteration 102/1000 | Loss: 0.00004909
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [4.9089321692008525e-05, 4.9089321692008525e-05, 4.9089321692008525e-05, 4.9089321692008525e-05, 4.9089321692008525e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.9089321692008525e-05

Optimization complete. Final v2v error: 5.854186534881592 mm

Highest mean error: 6.961870193481445 mm for frame 207

Lowest mean error: 5.377981185913086 mm for frame 25

Saving results

Total time: 119.51175093650818
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01197242
Iteration 2/25 | Loss: 0.00285164
Iteration 3/25 | Loss: 0.00180079
Iteration 4/25 | Loss: 0.00168324
Iteration 5/25 | Loss: 0.00154615
Iteration 6/25 | Loss: 0.00152150
Iteration 7/25 | Loss: 0.00147788
Iteration 8/25 | Loss: 0.00145804
Iteration 9/25 | Loss: 0.00141342
Iteration 10/25 | Loss: 0.00139240
Iteration 11/25 | Loss: 0.00137115
Iteration 12/25 | Loss: 0.00133317
Iteration 13/25 | Loss: 0.00127531
Iteration 14/25 | Loss: 0.00126866
Iteration 15/25 | Loss: 0.00125071
Iteration 16/25 | Loss: 0.00122860
Iteration 17/25 | Loss: 0.00120910
Iteration 18/25 | Loss: 0.00120467
Iteration 19/25 | Loss: 0.00120317
Iteration 20/25 | Loss: 0.00120256
Iteration 21/25 | Loss: 0.00120245
Iteration 22/25 | Loss: 0.00120245
Iteration 23/25 | Loss: 0.00120245
Iteration 24/25 | Loss: 0.00120245
Iteration 25/25 | Loss: 0.00120245

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.36926794
Iteration 2/25 | Loss: 0.00052135
Iteration 3/25 | Loss: 0.00052135
Iteration 4/25 | Loss: 0.00052135
Iteration 5/25 | Loss: 0.00052134
Iteration 6/25 | Loss: 0.00052134
Iteration 7/25 | Loss: 0.00052134
Iteration 8/25 | Loss: 0.00052134
Iteration 9/25 | Loss: 0.00052134
Iteration 10/25 | Loss: 0.00052134
Iteration 11/25 | Loss: 0.00052134
Iteration 12/25 | Loss: 0.00052134
Iteration 13/25 | Loss: 0.00052134
Iteration 14/25 | Loss: 0.00052134
Iteration 15/25 | Loss: 0.00052134
Iteration 16/25 | Loss: 0.00052134
Iteration 17/25 | Loss: 0.00052134
Iteration 18/25 | Loss: 0.00052134
Iteration 19/25 | Loss: 0.00052134
Iteration 20/25 | Loss: 0.00052134
Iteration 21/25 | Loss: 0.00052134
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005213425611145794, 0.0005213425611145794, 0.0005213425611145794, 0.0005213425611145794, 0.0005213425611145794]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005213425611145794

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052134
Iteration 2/1000 | Loss: 0.00007497
Iteration 3/1000 | Loss: 0.00006046
Iteration 4/1000 | Loss: 0.00005606
Iteration 5/1000 | Loss: 0.00005463
Iteration 6/1000 | Loss: 0.00005368
Iteration 7/1000 | Loss: 0.00005298
Iteration 8/1000 | Loss: 0.00005217
Iteration 9/1000 | Loss: 0.00005172
Iteration 10/1000 | Loss: 0.00005143
Iteration 11/1000 | Loss: 0.00005109
Iteration 12/1000 | Loss: 0.00005102
Iteration 13/1000 | Loss: 0.00005082
Iteration 14/1000 | Loss: 0.00005062
Iteration 15/1000 | Loss: 0.00005048
Iteration 16/1000 | Loss: 0.00005040
Iteration 17/1000 | Loss: 0.00005027
Iteration 18/1000 | Loss: 0.00005014
Iteration 19/1000 | Loss: 0.00005013
Iteration 20/1000 | Loss: 0.00005009
Iteration 21/1000 | Loss: 0.00005009
Iteration 22/1000 | Loss: 0.00005009
Iteration 23/1000 | Loss: 0.00005009
Iteration 24/1000 | Loss: 0.00005009
Iteration 25/1000 | Loss: 0.00005009
Iteration 26/1000 | Loss: 0.00005009
Iteration 27/1000 | Loss: 0.00005009
Iteration 28/1000 | Loss: 0.00005009
Iteration 29/1000 | Loss: 0.00005009
Iteration 30/1000 | Loss: 0.00005008
Iteration 31/1000 | Loss: 0.00005007
Iteration 32/1000 | Loss: 0.00005000
Iteration 33/1000 | Loss: 0.00004999
Iteration 34/1000 | Loss: 0.00004997
Iteration 35/1000 | Loss: 0.00004997
Iteration 36/1000 | Loss: 0.00004996
Iteration 37/1000 | Loss: 0.00004996
Iteration 38/1000 | Loss: 0.00004996
Iteration 39/1000 | Loss: 0.00004996
Iteration 40/1000 | Loss: 0.00004996
Iteration 41/1000 | Loss: 0.00004996
Iteration 42/1000 | Loss: 0.00004996
Iteration 43/1000 | Loss: 0.00004996
Iteration 44/1000 | Loss: 0.00004996
Iteration 45/1000 | Loss: 0.00004996
Iteration 46/1000 | Loss: 0.00004996
Iteration 47/1000 | Loss: 0.00004995
Iteration 48/1000 | Loss: 0.00004995
Iteration 49/1000 | Loss: 0.00004995
Iteration 50/1000 | Loss: 0.00004995
Iteration 51/1000 | Loss: 0.00004994
Iteration 52/1000 | Loss: 0.00004994
Iteration 53/1000 | Loss: 0.00004994
Iteration 54/1000 | Loss: 0.00004993
Iteration 55/1000 | Loss: 0.00004993
Iteration 56/1000 | Loss: 0.00004992
Iteration 57/1000 | Loss: 0.00004992
Iteration 58/1000 | Loss: 0.00004991
Iteration 59/1000 | Loss: 0.00004991
Iteration 60/1000 | Loss: 0.00004991
Iteration 61/1000 | Loss: 0.00004991
Iteration 62/1000 | Loss: 0.00004990
Iteration 63/1000 | Loss: 0.00004990
Iteration 64/1000 | Loss: 0.00004990
Iteration 65/1000 | Loss: 0.00004990
Iteration 66/1000 | Loss: 0.00004987
Iteration 67/1000 | Loss: 0.00004987
Iteration 68/1000 | Loss: 0.00004986
Iteration 69/1000 | Loss: 0.00004986
Iteration 70/1000 | Loss: 0.00004986
Iteration 71/1000 | Loss: 0.00004985
Iteration 72/1000 | Loss: 0.00004985
Iteration 73/1000 | Loss: 0.00004985
Iteration 74/1000 | Loss: 0.00004985
Iteration 75/1000 | Loss: 0.00004985
Iteration 76/1000 | Loss: 0.00004985
Iteration 77/1000 | Loss: 0.00004985
Iteration 78/1000 | Loss: 0.00004984
Iteration 79/1000 | Loss: 0.00004984
Iteration 80/1000 | Loss: 0.00004984
Iteration 81/1000 | Loss: 0.00004984
Iteration 82/1000 | Loss: 0.00004984
Iteration 83/1000 | Loss: 0.00004984
Iteration 84/1000 | Loss: 0.00004984
Iteration 85/1000 | Loss: 0.00004984
Iteration 86/1000 | Loss: 0.00004984
Iteration 87/1000 | Loss: 0.00004984
Iteration 88/1000 | Loss: 0.00004984
Iteration 89/1000 | Loss: 0.00004984
Iteration 90/1000 | Loss: 0.00004983
Iteration 91/1000 | Loss: 0.00004983
Iteration 92/1000 | Loss: 0.00004981
Iteration 93/1000 | Loss: 0.00004981
Iteration 94/1000 | Loss: 0.00004981
Iteration 95/1000 | Loss: 0.00004981
Iteration 96/1000 | Loss: 0.00004981
Iteration 97/1000 | Loss: 0.00004981
Iteration 98/1000 | Loss: 0.00004980
Iteration 99/1000 | Loss: 0.00004980
Iteration 100/1000 | Loss: 0.00004980
Iteration 101/1000 | Loss: 0.00004980
Iteration 102/1000 | Loss: 0.00004980
Iteration 103/1000 | Loss: 0.00004979
Iteration 104/1000 | Loss: 0.00004979
Iteration 105/1000 | Loss: 0.00004978
Iteration 106/1000 | Loss: 0.00004978
Iteration 107/1000 | Loss: 0.00004978
Iteration 108/1000 | Loss: 0.00004978
Iteration 109/1000 | Loss: 0.00004978
Iteration 110/1000 | Loss: 0.00004978
Iteration 111/1000 | Loss: 0.00004978
Iteration 112/1000 | Loss: 0.00004978
Iteration 113/1000 | Loss: 0.00004977
Iteration 114/1000 | Loss: 0.00004977
Iteration 115/1000 | Loss: 0.00004977
Iteration 116/1000 | Loss: 0.00004977
Iteration 117/1000 | Loss: 0.00004977
Iteration 118/1000 | Loss: 0.00004977
Iteration 119/1000 | Loss: 0.00004977
Iteration 120/1000 | Loss: 0.00004976
Iteration 121/1000 | Loss: 0.00004976
Iteration 122/1000 | Loss: 0.00004976
Iteration 123/1000 | Loss: 0.00004976
Iteration 124/1000 | Loss: 0.00004976
Iteration 125/1000 | Loss: 0.00004976
Iteration 126/1000 | Loss: 0.00004976
Iteration 127/1000 | Loss: 0.00004976
Iteration 128/1000 | Loss: 0.00004976
Iteration 129/1000 | Loss: 0.00004976
Iteration 130/1000 | Loss: 0.00004976
Iteration 131/1000 | Loss: 0.00004976
Iteration 132/1000 | Loss: 0.00004976
Iteration 133/1000 | Loss: 0.00004976
Iteration 134/1000 | Loss: 0.00004976
Iteration 135/1000 | Loss: 0.00004976
Iteration 136/1000 | Loss: 0.00004976
Iteration 137/1000 | Loss: 0.00004976
Iteration 138/1000 | Loss: 0.00004976
Iteration 139/1000 | Loss: 0.00004976
Iteration 140/1000 | Loss: 0.00004976
Iteration 141/1000 | Loss: 0.00004976
Iteration 142/1000 | Loss: 0.00004976
Iteration 143/1000 | Loss: 0.00004976
Iteration 144/1000 | Loss: 0.00004976
Iteration 145/1000 | Loss: 0.00004976
Iteration 146/1000 | Loss: 0.00004976
Iteration 147/1000 | Loss: 0.00004976
Iteration 148/1000 | Loss: 0.00004976
Iteration 149/1000 | Loss: 0.00004976
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [4.9757214583223686e-05, 4.9757214583223686e-05, 4.9757214583223686e-05, 4.9757214583223686e-05, 4.9757214583223686e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.9757214583223686e-05

Optimization complete. Final v2v error: 5.678734302520752 mm

Highest mean error: 5.940222263336182 mm for frame 44

Lowest mean error: 5.506078720092773 mm for frame 139

Saving results

Total time: 80.07655811309814
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00928073
Iteration 2/25 | Loss: 0.00190511
Iteration 3/25 | Loss: 0.00125592
Iteration 4/25 | Loss: 0.00106199
Iteration 5/25 | Loss: 0.00096294
Iteration 6/25 | Loss: 0.00089970
Iteration 7/25 | Loss: 0.00085217
Iteration 8/25 | Loss: 0.00085065
Iteration 9/25 | Loss: 0.00079590
Iteration 10/25 | Loss: 0.00079433
Iteration 11/25 | Loss: 0.00077890
Iteration 12/25 | Loss: 0.00077345
Iteration 13/25 | Loss: 0.00076832
Iteration 14/25 | Loss: 0.00076204
Iteration 15/25 | Loss: 0.00075891
Iteration 16/25 | Loss: 0.00076372
Iteration 17/25 | Loss: 0.00075671
Iteration 18/25 | Loss: 0.00075445
Iteration 19/25 | Loss: 0.00075386
Iteration 20/25 | Loss: 0.00075357
Iteration 21/25 | Loss: 0.00075294
Iteration 22/25 | Loss: 0.00075184
Iteration 23/25 | Loss: 0.00075072
Iteration 24/25 | Loss: 0.00075705
Iteration 25/25 | Loss: 0.00075346

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60345411
Iteration 2/25 | Loss: 0.00060774
Iteration 3/25 | Loss: 0.00060773
Iteration 4/25 | Loss: 0.00060773
Iteration 5/25 | Loss: 0.00060773
Iteration 6/25 | Loss: 0.00060773
Iteration 7/25 | Loss: 0.00060773
Iteration 8/25 | Loss: 0.00060773
Iteration 9/25 | Loss: 0.00060773
Iteration 10/25 | Loss: 0.00060773
Iteration 11/25 | Loss: 0.00060773
Iteration 12/25 | Loss: 0.00060773
Iteration 13/25 | Loss: 0.00060773
Iteration 14/25 | Loss: 0.00060773
Iteration 15/25 | Loss: 0.00060773
Iteration 16/25 | Loss: 0.00060773
Iteration 17/25 | Loss: 0.00060773
Iteration 18/25 | Loss: 0.00060773
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006077252328395844, 0.0006077252328395844, 0.0006077252328395844, 0.0006077252328395844, 0.0006077252328395844]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006077252328395844

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060773
Iteration 2/1000 | Loss: 0.00009129
Iteration 3/1000 | Loss: 0.00006719
Iteration 4/1000 | Loss: 0.00005694
Iteration 5/1000 | Loss: 0.00048802
Iteration 6/1000 | Loss: 0.00075203
Iteration 7/1000 | Loss: 0.00031402
Iteration 8/1000 | Loss: 0.00041937
Iteration 9/1000 | Loss: 0.00244305
Iteration 10/1000 | Loss: 0.00006566
Iteration 11/1000 | Loss: 0.00003976
Iteration 12/1000 | Loss: 0.00003015
Iteration 13/1000 | Loss: 0.00002576
Iteration 14/1000 | Loss: 0.00002316
Iteration 15/1000 | Loss: 0.00002179
Iteration 16/1000 | Loss: 0.00002098
Iteration 17/1000 | Loss: 0.00002042
Iteration 18/1000 | Loss: 0.00001995
Iteration 19/1000 | Loss: 0.00001956
Iteration 20/1000 | Loss: 0.00001914
Iteration 21/1000 | Loss: 0.00001889
Iteration 22/1000 | Loss: 0.00001885
Iteration 23/1000 | Loss: 0.00001869
Iteration 24/1000 | Loss: 0.00001868
Iteration 25/1000 | Loss: 0.00001867
Iteration 26/1000 | Loss: 0.00001866
Iteration 27/1000 | Loss: 0.00001865
Iteration 28/1000 | Loss: 0.00001865
Iteration 29/1000 | Loss: 0.00001864
Iteration 30/1000 | Loss: 0.00001864
Iteration 31/1000 | Loss: 0.00001863
Iteration 32/1000 | Loss: 0.00001862
Iteration 33/1000 | Loss: 0.00001862
Iteration 34/1000 | Loss: 0.00001861
Iteration 35/1000 | Loss: 0.00001861
Iteration 36/1000 | Loss: 0.00001860
Iteration 37/1000 | Loss: 0.00001860
Iteration 38/1000 | Loss: 0.00001859
Iteration 39/1000 | Loss: 0.00001859
Iteration 40/1000 | Loss: 0.00001858
Iteration 41/1000 | Loss: 0.00001858
Iteration 42/1000 | Loss: 0.00001858
Iteration 43/1000 | Loss: 0.00001858
Iteration 44/1000 | Loss: 0.00001858
Iteration 45/1000 | Loss: 0.00001858
Iteration 46/1000 | Loss: 0.00001857
Iteration 47/1000 | Loss: 0.00001857
Iteration 48/1000 | Loss: 0.00001857
Iteration 49/1000 | Loss: 0.00001856
Iteration 50/1000 | Loss: 0.00001856
Iteration 51/1000 | Loss: 0.00001856
Iteration 52/1000 | Loss: 0.00001856
Iteration 53/1000 | Loss: 0.00001855
Iteration 54/1000 | Loss: 0.00001855
Iteration 55/1000 | Loss: 0.00001855
Iteration 56/1000 | Loss: 0.00001855
Iteration 57/1000 | Loss: 0.00001855
Iteration 58/1000 | Loss: 0.00001855
Iteration 59/1000 | Loss: 0.00001855
Iteration 60/1000 | Loss: 0.00001855
Iteration 61/1000 | Loss: 0.00001855
Iteration 62/1000 | Loss: 0.00001855
Iteration 63/1000 | Loss: 0.00001855
Iteration 64/1000 | Loss: 0.00001854
Iteration 65/1000 | Loss: 0.00001854
Iteration 66/1000 | Loss: 0.00001854
Iteration 67/1000 | Loss: 0.00001854
Iteration 68/1000 | Loss: 0.00001854
Iteration 69/1000 | Loss: 0.00001854
Iteration 70/1000 | Loss: 0.00001853
Iteration 71/1000 | Loss: 0.00001853
Iteration 72/1000 | Loss: 0.00001853
Iteration 73/1000 | Loss: 0.00001853
Iteration 74/1000 | Loss: 0.00001853
Iteration 75/1000 | Loss: 0.00001853
Iteration 76/1000 | Loss: 0.00001853
Iteration 77/1000 | Loss: 0.00001853
Iteration 78/1000 | Loss: 0.00001853
Iteration 79/1000 | Loss: 0.00001852
Iteration 80/1000 | Loss: 0.00001852
Iteration 81/1000 | Loss: 0.00001852
Iteration 82/1000 | Loss: 0.00001852
Iteration 83/1000 | Loss: 0.00001852
Iteration 84/1000 | Loss: 0.00001852
Iteration 85/1000 | Loss: 0.00001851
Iteration 86/1000 | Loss: 0.00001851
Iteration 87/1000 | Loss: 0.00001851
Iteration 88/1000 | Loss: 0.00001851
Iteration 89/1000 | Loss: 0.00001851
Iteration 90/1000 | Loss: 0.00001851
Iteration 91/1000 | Loss: 0.00001851
Iteration 92/1000 | Loss: 0.00001851
Iteration 93/1000 | Loss: 0.00001851
Iteration 94/1000 | Loss: 0.00001851
Iteration 95/1000 | Loss: 0.00001851
Iteration 96/1000 | Loss: 0.00001851
Iteration 97/1000 | Loss: 0.00001851
Iteration 98/1000 | Loss: 0.00001851
Iteration 99/1000 | Loss: 0.00001851
Iteration 100/1000 | Loss: 0.00001851
Iteration 101/1000 | Loss: 0.00001851
Iteration 102/1000 | Loss: 0.00001851
Iteration 103/1000 | Loss: 0.00001851
Iteration 104/1000 | Loss: 0.00001851
Iteration 105/1000 | Loss: 0.00001851
Iteration 106/1000 | Loss: 0.00001850
Iteration 107/1000 | Loss: 0.00001850
Iteration 108/1000 | Loss: 0.00001850
Iteration 109/1000 | Loss: 0.00001850
Iteration 110/1000 | Loss: 0.00001850
Iteration 111/1000 | Loss: 0.00001850
Iteration 112/1000 | Loss: 0.00001850
Iteration 113/1000 | Loss: 0.00001850
Iteration 114/1000 | Loss: 0.00001850
Iteration 115/1000 | Loss: 0.00001850
Iteration 116/1000 | Loss: 0.00001850
Iteration 117/1000 | Loss: 0.00001850
Iteration 118/1000 | Loss: 0.00001850
Iteration 119/1000 | Loss: 0.00001850
Iteration 120/1000 | Loss: 0.00001850
Iteration 121/1000 | Loss: 0.00001850
Iteration 122/1000 | Loss: 0.00001850
Iteration 123/1000 | Loss: 0.00001850
Iteration 124/1000 | Loss: 0.00001850
Iteration 125/1000 | Loss: 0.00001850
Iteration 126/1000 | Loss: 0.00001850
Iteration 127/1000 | Loss: 0.00001850
Iteration 128/1000 | Loss: 0.00001850
Iteration 129/1000 | Loss: 0.00001850
Iteration 130/1000 | Loss: 0.00001850
Iteration 131/1000 | Loss: 0.00001850
Iteration 132/1000 | Loss: 0.00001850
Iteration 133/1000 | Loss: 0.00001850
Iteration 134/1000 | Loss: 0.00001850
Iteration 135/1000 | Loss: 0.00001850
Iteration 136/1000 | Loss: 0.00001850
Iteration 137/1000 | Loss: 0.00001850
Iteration 138/1000 | Loss: 0.00001850
Iteration 139/1000 | Loss: 0.00001850
Iteration 140/1000 | Loss: 0.00001850
Iteration 141/1000 | Loss: 0.00001850
Iteration 142/1000 | Loss: 0.00001850
Iteration 143/1000 | Loss: 0.00001850
Iteration 144/1000 | Loss: 0.00001850
Iteration 145/1000 | Loss: 0.00001850
Iteration 146/1000 | Loss: 0.00001850
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.8500766600482166e-05, 1.8500766600482166e-05, 1.8500766600482166e-05, 1.8500766600482166e-05, 1.8500766600482166e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8500766600482166e-05

Optimization complete. Final v2v error: 3.644146203994751 mm

Highest mean error: 4.3291473388671875 mm for frame 94

Lowest mean error: 2.875265598297119 mm for frame 139

Saving results

Total time: 81.87775325775146
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00797614
Iteration 2/25 | Loss: 0.00104671
Iteration 3/25 | Loss: 0.00070791
Iteration 4/25 | Loss: 0.00064963
Iteration 5/25 | Loss: 0.00063619
Iteration 6/25 | Loss: 0.00063500
Iteration 7/25 | Loss: 0.00063491
Iteration 8/25 | Loss: 0.00063491
Iteration 9/25 | Loss: 0.00063491
Iteration 10/25 | Loss: 0.00063491
Iteration 11/25 | Loss: 0.00063491
Iteration 12/25 | Loss: 0.00063491
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006349114701151848, 0.0006349114701151848, 0.0006349114701151848, 0.0006349114701151848, 0.0006349114701151848]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006349114701151848

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43657386
Iteration 2/25 | Loss: 0.00022376
Iteration 3/25 | Loss: 0.00022374
Iteration 4/25 | Loss: 0.00022374
Iteration 5/25 | Loss: 0.00022374
Iteration 6/25 | Loss: 0.00022374
Iteration 7/25 | Loss: 0.00022374
Iteration 8/25 | Loss: 0.00022374
Iteration 9/25 | Loss: 0.00022374
Iteration 10/25 | Loss: 0.00022374
Iteration 11/25 | Loss: 0.00022374
Iteration 12/25 | Loss: 0.00022374
Iteration 13/25 | Loss: 0.00022374
Iteration 14/25 | Loss: 0.00022374
Iteration 15/25 | Loss: 0.00022374
Iteration 16/25 | Loss: 0.00022374
Iteration 17/25 | Loss: 0.00022374
Iteration 18/25 | Loss: 0.00022374
Iteration 19/25 | Loss: 0.00022374
Iteration 20/25 | Loss: 0.00022374
Iteration 21/25 | Loss: 0.00022374
Iteration 22/25 | Loss: 0.00022374
Iteration 23/25 | Loss: 0.00022374
Iteration 24/25 | Loss: 0.00022374
Iteration 25/25 | Loss: 0.00022374

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00022374
Iteration 2/1000 | Loss: 0.00002793
Iteration 3/1000 | Loss: 0.00002041
Iteration 4/1000 | Loss: 0.00001857
Iteration 5/1000 | Loss: 0.00001762
Iteration 6/1000 | Loss: 0.00001687
Iteration 7/1000 | Loss: 0.00001645
Iteration 8/1000 | Loss: 0.00001615
Iteration 9/1000 | Loss: 0.00001606
Iteration 10/1000 | Loss: 0.00001605
Iteration 11/1000 | Loss: 0.00001605
Iteration 12/1000 | Loss: 0.00001605
Iteration 13/1000 | Loss: 0.00001605
Iteration 14/1000 | Loss: 0.00001592
Iteration 15/1000 | Loss: 0.00001590
Iteration 16/1000 | Loss: 0.00001585
Iteration 17/1000 | Loss: 0.00001581
Iteration 18/1000 | Loss: 0.00001581
Iteration 19/1000 | Loss: 0.00001580
Iteration 20/1000 | Loss: 0.00001579
Iteration 21/1000 | Loss: 0.00001578
Iteration 22/1000 | Loss: 0.00001578
Iteration 23/1000 | Loss: 0.00001576
Iteration 24/1000 | Loss: 0.00001575
Iteration 25/1000 | Loss: 0.00001575
Iteration 26/1000 | Loss: 0.00001575
Iteration 27/1000 | Loss: 0.00001574
Iteration 28/1000 | Loss: 0.00001572
Iteration 29/1000 | Loss: 0.00001570
Iteration 30/1000 | Loss: 0.00001569
Iteration 31/1000 | Loss: 0.00001569
Iteration 32/1000 | Loss: 0.00001569
Iteration 33/1000 | Loss: 0.00001569
Iteration 34/1000 | Loss: 0.00001568
Iteration 35/1000 | Loss: 0.00001568
Iteration 36/1000 | Loss: 0.00001568
Iteration 37/1000 | Loss: 0.00001567
Iteration 38/1000 | Loss: 0.00001567
Iteration 39/1000 | Loss: 0.00001567
Iteration 40/1000 | Loss: 0.00001567
Iteration 41/1000 | Loss: 0.00001567
Iteration 42/1000 | Loss: 0.00001567
Iteration 43/1000 | Loss: 0.00001567
Iteration 44/1000 | Loss: 0.00001567
Iteration 45/1000 | Loss: 0.00001567
Iteration 46/1000 | Loss: 0.00001566
Iteration 47/1000 | Loss: 0.00001566
Iteration 48/1000 | Loss: 0.00001566
Iteration 49/1000 | Loss: 0.00001566
Iteration 50/1000 | Loss: 0.00001566
Iteration 51/1000 | Loss: 0.00001566
Iteration 52/1000 | Loss: 0.00001565
Iteration 53/1000 | Loss: 0.00001564
Iteration 54/1000 | Loss: 0.00001564
Iteration 55/1000 | Loss: 0.00001564
Iteration 56/1000 | Loss: 0.00001564
Iteration 57/1000 | Loss: 0.00001564
Iteration 58/1000 | Loss: 0.00001564
Iteration 59/1000 | Loss: 0.00001564
Iteration 60/1000 | Loss: 0.00001564
Iteration 61/1000 | Loss: 0.00001564
Iteration 62/1000 | Loss: 0.00001563
Iteration 63/1000 | Loss: 0.00001563
Iteration 64/1000 | Loss: 0.00001563
Iteration 65/1000 | Loss: 0.00001563
Iteration 66/1000 | Loss: 0.00001562
Iteration 67/1000 | Loss: 0.00001562
Iteration 68/1000 | Loss: 0.00001562
Iteration 69/1000 | Loss: 0.00001562
Iteration 70/1000 | Loss: 0.00001562
Iteration 71/1000 | Loss: 0.00001561
Iteration 72/1000 | Loss: 0.00001561
Iteration 73/1000 | Loss: 0.00001561
Iteration 74/1000 | Loss: 0.00001561
Iteration 75/1000 | Loss: 0.00001561
Iteration 76/1000 | Loss: 0.00001561
Iteration 77/1000 | Loss: 0.00001561
Iteration 78/1000 | Loss: 0.00001561
Iteration 79/1000 | Loss: 0.00001561
Iteration 80/1000 | Loss: 0.00001561
Iteration 81/1000 | Loss: 0.00001561
Iteration 82/1000 | Loss: 0.00001561
Iteration 83/1000 | Loss: 0.00001561
Iteration 84/1000 | Loss: 0.00001561
Iteration 85/1000 | Loss: 0.00001560
Iteration 86/1000 | Loss: 0.00001560
Iteration 87/1000 | Loss: 0.00001560
Iteration 88/1000 | Loss: 0.00001560
Iteration 89/1000 | Loss: 0.00001560
Iteration 90/1000 | Loss: 0.00001560
Iteration 91/1000 | Loss: 0.00001560
Iteration 92/1000 | Loss: 0.00001560
Iteration 93/1000 | Loss: 0.00001560
Iteration 94/1000 | Loss: 0.00001560
Iteration 95/1000 | Loss: 0.00001560
Iteration 96/1000 | Loss: 0.00001560
Iteration 97/1000 | Loss: 0.00001560
Iteration 98/1000 | Loss: 0.00001560
Iteration 99/1000 | Loss: 0.00001560
Iteration 100/1000 | Loss: 0.00001560
Iteration 101/1000 | Loss: 0.00001559
Iteration 102/1000 | Loss: 0.00001559
Iteration 103/1000 | Loss: 0.00001559
Iteration 104/1000 | Loss: 0.00001559
Iteration 105/1000 | Loss: 0.00001559
Iteration 106/1000 | Loss: 0.00001559
Iteration 107/1000 | Loss: 0.00001559
Iteration 108/1000 | Loss: 0.00001559
Iteration 109/1000 | Loss: 0.00001559
Iteration 110/1000 | Loss: 0.00001558
Iteration 111/1000 | Loss: 0.00001558
Iteration 112/1000 | Loss: 0.00001558
Iteration 113/1000 | Loss: 0.00001558
Iteration 114/1000 | Loss: 0.00001558
Iteration 115/1000 | Loss: 0.00001558
Iteration 116/1000 | Loss: 0.00001558
Iteration 117/1000 | Loss: 0.00001558
Iteration 118/1000 | Loss: 0.00001558
Iteration 119/1000 | Loss: 0.00001558
Iteration 120/1000 | Loss: 0.00001558
Iteration 121/1000 | Loss: 0.00001558
Iteration 122/1000 | Loss: 0.00001558
Iteration 123/1000 | Loss: 0.00001558
Iteration 124/1000 | Loss: 0.00001558
Iteration 125/1000 | Loss: 0.00001558
Iteration 126/1000 | Loss: 0.00001558
Iteration 127/1000 | Loss: 0.00001558
Iteration 128/1000 | Loss: 0.00001557
Iteration 129/1000 | Loss: 0.00001557
Iteration 130/1000 | Loss: 0.00001557
Iteration 131/1000 | Loss: 0.00001557
Iteration 132/1000 | Loss: 0.00001557
Iteration 133/1000 | Loss: 0.00001557
Iteration 134/1000 | Loss: 0.00001557
Iteration 135/1000 | Loss: 0.00001557
Iteration 136/1000 | Loss: 0.00001557
Iteration 137/1000 | Loss: 0.00001557
Iteration 138/1000 | Loss: 0.00001557
Iteration 139/1000 | Loss: 0.00001557
Iteration 140/1000 | Loss: 0.00001557
Iteration 141/1000 | Loss: 0.00001557
Iteration 142/1000 | Loss: 0.00001557
Iteration 143/1000 | Loss: 0.00001557
Iteration 144/1000 | Loss: 0.00001557
Iteration 145/1000 | Loss: 0.00001557
Iteration 146/1000 | Loss: 0.00001557
Iteration 147/1000 | Loss: 0.00001557
Iteration 148/1000 | Loss: 0.00001557
Iteration 149/1000 | Loss: 0.00001557
Iteration 150/1000 | Loss: 0.00001557
Iteration 151/1000 | Loss: 0.00001557
Iteration 152/1000 | Loss: 0.00001557
Iteration 153/1000 | Loss: 0.00001557
Iteration 154/1000 | Loss: 0.00001557
Iteration 155/1000 | Loss: 0.00001557
Iteration 156/1000 | Loss: 0.00001557
Iteration 157/1000 | Loss: 0.00001557
Iteration 158/1000 | Loss: 0.00001557
Iteration 159/1000 | Loss: 0.00001557
Iteration 160/1000 | Loss: 0.00001557
Iteration 161/1000 | Loss: 0.00001557
Iteration 162/1000 | Loss: 0.00001557
Iteration 163/1000 | Loss: 0.00001557
Iteration 164/1000 | Loss: 0.00001557
Iteration 165/1000 | Loss: 0.00001557
Iteration 166/1000 | Loss: 0.00001557
Iteration 167/1000 | Loss: 0.00001557
Iteration 168/1000 | Loss: 0.00001557
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.556755341880489e-05, 1.556755341880489e-05, 1.556755341880489e-05, 1.556755341880489e-05, 1.556755341880489e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.556755341880489e-05

Optimization complete. Final v2v error: 3.3318586349487305 mm

Highest mean error: 3.6581010818481445 mm for frame 109

Lowest mean error: 3.1472184658050537 mm for frame 7

Saving results

Total time: 32.51396656036377
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00833542
Iteration 2/25 | Loss: 0.00086456
Iteration 3/25 | Loss: 0.00069946
Iteration 4/25 | Loss: 0.00066734
Iteration 5/25 | Loss: 0.00065743
Iteration 6/25 | Loss: 0.00065555
Iteration 7/25 | Loss: 0.00065523
Iteration 8/25 | Loss: 0.00065523
Iteration 9/25 | Loss: 0.00065523
Iteration 10/25 | Loss: 0.00065523
Iteration 11/25 | Loss: 0.00065523
Iteration 12/25 | Loss: 0.00065523
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006552256527356803, 0.0006552256527356803, 0.0006552256527356803, 0.0006552256527356803, 0.0006552256527356803]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006552256527356803

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.57381582
Iteration 2/25 | Loss: 0.00029241
Iteration 3/25 | Loss: 0.00029239
Iteration 4/25 | Loss: 0.00029239
Iteration 5/25 | Loss: 0.00029239
Iteration 6/25 | Loss: 0.00029238
Iteration 7/25 | Loss: 0.00029238
Iteration 8/25 | Loss: 0.00029238
Iteration 9/25 | Loss: 0.00029238
Iteration 10/25 | Loss: 0.00029238
Iteration 11/25 | Loss: 0.00029238
Iteration 12/25 | Loss: 0.00029238
Iteration 13/25 | Loss: 0.00029238
Iteration 14/25 | Loss: 0.00029238
Iteration 15/25 | Loss: 0.00029238
Iteration 16/25 | Loss: 0.00029238
Iteration 17/25 | Loss: 0.00029238
Iteration 18/25 | Loss: 0.00029238
Iteration 19/25 | Loss: 0.00029238
Iteration 20/25 | Loss: 0.00029238
Iteration 21/25 | Loss: 0.00029238
Iteration 22/25 | Loss: 0.00029238
Iteration 23/25 | Loss: 0.00029238
Iteration 24/25 | Loss: 0.00029238
Iteration 25/25 | Loss: 0.00029238

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029238
Iteration 2/1000 | Loss: 0.00003606
Iteration 3/1000 | Loss: 0.00002486
Iteration 4/1000 | Loss: 0.00002291
Iteration 5/1000 | Loss: 0.00002151
Iteration 6/1000 | Loss: 0.00002080
Iteration 7/1000 | Loss: 0.00002002
Iteration 8/1000 | Loss: 0.00001963
Iteration 9/1000 | Loss: 0.00001940
Iteration 10/1000 | Loss: 0.00001929
Iteration 11/1000 | Loss: 0.00001913
Iteration 12/1000 | Loss: 0.00001912
Iteration 13/1000 | Loss: 0.00001909
Iteration 14/1000 | Loss: 0.00001908
Iteration 15/1000 | Loss: 0.00001904
Iteration 16/1000 | Loss: 0.00001903
Iteration 17/1000 | Loss: 0.00001899
Iteration 18/1000 | Loss: 0.00001898
Iteration 19/1000 | Loss: 0.00001898
Iteration 20/1000 | Loss: 0.00001897
Iteration 21/1000 | Loss: 0.00001896
Iteration 22/1000 | Loss: 0.00001896
Iteration 23/1000 | Loss: 0.00001896
Iteration 24/1000 | Loss: 0.00001895
Iteration 25/1000 | Loss: 0.00001895
Iteration 26/1000 | Loss: 0.00001895
Iteration 27/1000 | Loss: 0.00001895
Iteration 28/1000 | Loss: 0.00001895
Iteration 29/1000 | Loss: 0.00001895
Iteration 30/1000 | Loss: 0.00001894
Iteration 31/1000 | Loss: 0.00001894
Iteration 32/1000 | Loss: 0.00001894
Iteration 33/1000 | Loss: 0.00001894
Iteration 34/1000 | Loss: 0.00001894
Iteration 35/1000 | Loss: 0.00001893
Iteration 36/1000 | Loss: 0.00001893
Iteration 37/1000 | Loss: 0.00001893
Iteration 38/1000 | Loss: 0.00001893
Iteration 39/1000 | Loss: 0.00001892
Iteration 40/1000 | Loss: 0.00001892
Iteration 41/1000 | Loss: 0.00001892
Iteration 42/1000 | Loss: 0.00001892
Iteration 43/1000 | Loss: 0.00001892
Iteration 44/1000 | Loss: 0.00001892
Iteration 45/1000 | Loss: 0.00001891
Iteration 46/1000 | Loss: 0.00001891
Iteration 47/1000 | Loss: 0.00001891
Iteration 48/1000 | Loss: 0.00001891
Iteration 49/1000 | Loss: 0.00001891
Iteration 50/1000 | Loss: 0.00001891
Iteration 51/1000 | Loss: 0.00001891
Iteration 52/1000 | Loss: 0.00001891
Iteration 53/1000 | Loss: 0.00001891
Iteration 54/1000 | Loss: 0.00001891
Iteration 55/1000 | Loss: 0.00001890
Iteration 56/1000 | Loss: 0.00001890
Iteration 57/1000 | Loss: 0.00001890
Iteration 58/1000 | Loss: 0.00001890
Iteration 59/1000 | Loss: 0.00001890
Iteration 60/1000 | Loss: 0.00001890
Iteration 61/1000 | Loss: 0.00001890
Iteration 62/1000 | Loss: 0.00001889
Iteration 63/1000 | Loss: 0.00001889
Iteration 64/1000 | Loss: 0.00001889
Iteration 65/1000 | Loss: 0.00001888
Iteration 66/1000 | Loss: 0.00001888
Iteration 67/1000 | Loss: 0.00001888
Iteration 68/1000 | Loss: 0.00001888
Iteration 69/1000 | Loss: 0.00001888
Iteration 70/1000 | Loss: 0.00001888
Iteration 71/1000 | Loss: 0.00001887
Iteration 72/1000 | Loss: 0.00001887
Iteration 73/1000 | Loss: 0.00001887
Iteration 74/1000 | Loss: 0.00001887
Iteration 75/1000 | Loss: 0.00001887
Iteration 76/1000 | Loss: 0.00001887
Iteration 77/1000 | Loss: 0.00001887
Iteration 78/1000 | Loss: 0.00001887
Iteration 79/1000 | Loss: 0.00001886
Iteration 80/1000 | Loss: 0.00001886
Iteration 81/1000 | Loss: 0.00001885
Iteration 82/1000 | Loss: 0.00001885
Iteration 83/1000 | Loss: 0.00001885
Iteration 84/1000 | Loss: 0.00001885
Iteration 85/1000 | Loss: 0.00001884
Iteration 86/1000 | Loss: 0.00001884
Iteration 87/1000 | Loss: 0.00001884
Iteration 88/1000 | Loss: 0.00001883
Iteration 89/1000 | Loss: 0.00001883
Iteration 90/1000 | Loss: 0.00001883
Iteration 91/1000 | Loss: 0.00001882
Iteration 92/1000 | Loss: 0.00001882
Iteration 93/1000 | Loss: 0.00001882
Iteration 94/1000 | Loss: 0.00001882
Iteration 95/1000 | Loss: 0.00001882
Iteration 96/1000 | Loss: 0.00001882
Iteration 97/1000 | Loss: 0.00001882
Iteration 98/1000 | Loss: 0.00001881
Iteration 99/1000 | Loss: 0.00001881
Iteration 100/1000 | Loss: 0.00001881
Iteration 101/1000 | Loss: 0.00001881
Iteration 102/1000 | Loss: 0.00001881
Iteration 103/1000 | Loss: 0.00001880
Iteration 104/1000 | Loss: 0.00001880
Iteration 105/1000 | Loss: 0.00001880
Iteration 106/1000 | Loss: 0.00001880
Iteration 107/1000 | Loss: 0.00001879
Iteration 108/1000 | Loss: 0.00001879
Iteration 109/1000 | Loss: 0.00001879
Iteration 110/1000 | Loss: 0.00001879
Iteration 111/1000 | Loss: 0.00001879
Iteration 112/1000 | Loss: 0.00001879
Iteration 113/1000 | Loss: 0.00001879
Iteration 114/1000 | Loss: 0.00001879
Iteration 115/1000 | Loss: 0.00001879
Iteration 116/1000 | Loss: 0.00001879
Iteration 117/1000 | Loss: 0.00001878
Iteration 118/1000 | Loss: 0.00001878
Iteration 119/1000 | Loss: 0.00001878
Iteration 120/1000 | Loss: 0.00001878
Iteration 121/1000 | Loss: 0.00001878
Iteration 122/1000 | Loss: 0.00001878
Iteration 123/1000 | Loss: 0.00001878
Iteration 124/1000 | Loss: 0.00001878
Iteration 125/1000 | Loss: 0.00001878
Iteration 126/1000 | Loss: 0.00001878
Iteration 127/1000 | Loss: 0.00001877
Iteration 128/1000 | Loss: 0.00001877
Iteration 129/1000 | Loss: 0.00001877
Iteration 130/1000 | Loss: 0.00001877
Iteration 131/1000 | Loss: 0.00001877
Iteration 132/1000 | Loss: 0.00001877
Iteration 133/1000 | Loss: 0.00001877
Iteration 134/1000 | Loss: 0.00001877
Iteration 135/1000 | Loss: 0.00001876
Iteration 136/1000 | Loss: 0.00001876
Iteration 137/1000 | Loss: 0.00001876
Iteration 138/1000 | Loss: 0.00001876
Iteration 139/1000 | Loss: 0.00001876
Iteration 140/1000 | Loss: 0.00001876
Iteration 141/1000 | Loss: 0.00001876
Iteration 142/1000 | Loss: 0.00001876
Iteration 143/1000 | Loss: 0.00001875
Iteration 144/1000 | Loss: 0.00001875
Iteration 145/1000 | Loss: 0.00001875
Iteration 146/1000 | Loss: 0.00001875
Iteration 147/1000 | Loss: 0.00001875
Iteration 148/1000 | Loss: 0.00001875
Iteration 149/1000 | Loss: 0.00001875
Iteration 150/1000 | Loss: 0.00001875
Iteration 151/1000 | Loss: 0.00001875
Iteration 152/1000 | Loss: 0.00001875
Iteration 153/1000 | Loss: 0.00001875
Iteration 154/1000 | Loss: 0.00001875
Iteration 155/1000 | Loss: 0.00001875
Iteration 156/1000 | Loss: 0.00001875
Iteration 157/1000 | Loss: 0.00001875
Iteration 158/1000 | Loss: 0.00001875
Iteration 159/1000 | Loss: 0.00001875
Iteration 160/1000 | Loss: 0.00001875
Iteration 161/1000 | Loss: 0.00001875
Iteration 162/1000 | Loss: 0.00001875
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.874795270850882e-05, 1.874795270850882e-05, 1.874795270850882e-05, 1.874795270850882e-05, 1.874795270850882e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.874795270850882e-05

Optimization complete. Final v2v error: 3.6392295360565186 mm

Highest mean error: 4.714548110961914 mm for frame 146

Lowest mean error: 3.1637792587280273 mm for frame 37

Saving results

Total time: 36.12665295600891
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00397480
Iteration 2/25 | Loss: 0.00073549
Iteration 3/25 | Loss: 0.00059781
Iteration 4/25 | Loss: 0.00058243
Iteration 5/25 | Loss: 0.00057977
Iteration 6/25 | Loss: 0.00057898
Iteration 7/25 | Loss: 0.00057888
Iteration 8/25 | Loss: 0.00057888
Iteration 9/25 | Loss: 0.00057888
Iteration 10/25 | Loss: 0.00057888
Iteration 11/25 | Loss: 0.00057888
Iteration 12/25 | Loss: 0.00057888
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005788833950646222, 0.0005788833950646222, 0.0005788833950646222, 0.0005788833950646222, 0.0005788833950646222]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005788833950646222

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46152318
Iteration 2/25 | Loss: 0.00023763
Iteration 3/25 | Loss: 0.00023763
Iteration 4/25 | Loss: 0.00023763
Iteration 5/25 | Loss: 0.00023763
Iteration 6/25 | Loss: 0.00023763
Iteration 7/25 | Loss: 0.00023763
Iteration 8/25 | Loss: 0.00023763
Iteration 9/25 | Loss: 0.00023763
Iteration 10/25 | Loss: 0.00023763
Iteration 11/25 | Loss: 0.00023763
Iteration 12/25 | Loss: 0.00023763
Iteration 13/25 | Loss: 0.00023763
Iteration 14/25 | Loss: 0.00023763
Iteration 15/25 | Loss: 0.00023763
Iteration 16/25 | Loss: 0.00023763
Iteration 17/25 | Loss: 0.00023763
Iteration 18/25 | Loss: 0.00023763
Iteration 19/25 | Loss: 0.00023763
Iteration 20/25 | Loss: 0.00023763
Iteration 21/25 | Loss: 0.00023763
Iteration 22/25 | Loss: 0.00023763
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00023762614000588655, 0.00023762614000588655, 0.00023762614000588655, 0.00023762614000588655, 0.00023762614000588655]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00023762614000588655

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00023763
Iteration 2/1000 | Loss: 0.00002168
Iteration 3/1000 | Loss: 0.00001252
Iteration 4/1000 | Loss: 0.00001127
Iteration 5/1000 | Loss: 0.00001056
Iteration 6/1000 | Loss: 0.00001027
Iteration 7/1000 | Loss: 0.00001000
Iteration 8/1000 | Loss: 0.00000988
Iteration 9/1000 | Loss: 0.00000988
Iteration 10/1000 | Loss: 0.00000987
Iteration 11/1000 | Loss: 0.00000986
Iteration 12/1000 | Loss: 0.00000984
Iteration 13/1000 | Loss: 0.00000984
Iteration 14/1000 | Loss: 0.00000983
Iteration 15/1000 | Loss: 0.00000983
Iteration 16/1000 | Loss: 0.00000982
Iteration 17/1000 | Loss: 0.00000981
Iteration 18/1000 | Loss: 0.00000981
Iteration 19/1000 | Loss: 0.00000981
Iteration 20/1000 | Loss: 0.00000981
Iteration 21/1000 | Loss: 0.00000980
Iteration 22/1000 | Loss: 0.00000980
Iteration 23/1000 | Loss: 0.00000979
Iteration 24/1000 | Loss: 0.00000979
Iteration 25/1000 | Loss: 0.00000978
Iteration 26/1000 | Loss: 0.00000978
Iteration 27/1000 | Loss: 0.00000977
Iteration 28/1000 | Loss: 0.00000977
Iteration 29/1000 | Loss: 0.00000976
Iteration 30/1000 | Loss: 0.00000976
Iteration 31/1000 | Loss: 0.00000975
Iteration 32/1000 | Loss: 0.00000975
Iteration 33/1000 | Loss: 0.00000973
Iteration 34/1000 | Loss: 0.00000972
Iteration 35/1000 | Loss: 0.00000971
Iteration 36/1000 | Loss: 0.00000971
Iteration 37/1000 | Loss: 0.00000971
Iteration 38/1000 | Loss: 0.00000970
Iteration 39/1000 | Loss: 0.00000970
Iteration 40/1000 | Loss: 0.00000970
Iteration 41/1000 | Loss: 0.00000970
Iteration 42/1000 | Loss: 0.00000970
Iteration 43/1000 | Loss: 0.00000970
Iteration 44/1000 | Loss: 0.00000970
Iteration 45/1000 | Loss: 0.00000969
Iteration 46/1000 | Loss: 0.00000969
Iteration 47/1000 | Loss: 0.00000968
Iteration 48/1000 | Loss: 0.00000968
Iteration 49/1000 | Loss: 0.00000968
Iteration 50/1000 | Loss: 0.00000968
Iteration 51/1000 | Loss: 0.00000968
Iteration 52/1000 | Loss: 0.00000968
Iteration 53/1000 | Loss: 0.00000968
Iteration 54/1000 | Loss: 0.00000968
Iteration 55/1000 | Loss: 0.00000968
Iteration 56/1000 | Loss: 0.00000968
Iteration 57/1000 | Loss: 0.00000968
Iteration 58/1000 | Loss: 0.00000968
Iteration 59/1000 | Loss: 0.00000968
Iteration 60/1000 | Loss: 0.00000968
Iteration 61/1000 | Loss: 0.00000968
Iteration 62/1000 | Loss: 0.00000968
Iteration 63/1000 | Loss: 0.00000968
Iteration 64/1000 | Loss: 0.00000968
Iteration 65/1000 | Loss: 0.00000968
Iteration 66/1000 | Loss: 0.00000968
Iteration 67/1000 | Loss: 0.00000968
Iteration 68/1000 | Loss: 0.00000968
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 68. Stopping optimization.
Last 5 losses: [9.680218681751285e-06, 9.680218681751285e-06, 9.680218681751285e-06, 9.680218681751285e-06, 9.680218681751285e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.680218681751285e-06

Optimization complete. Final v2v error: 2.638587474822998 mm

Highest mean error: 3.3880157470703125 mm for frame 54

Lowest mean error: 2.5191755294799805 mm for frame 143

Saving results

Total time: 24.795016288757324
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00501820
Iteration 2/25 | Loss: 0.00081776
Iteration 3/25 | Loss: 0.00066987
Iteration 4/25 | Loss: 0.00064241
Iteration 5/25 | Loss: 0.00063302
Iteration 6/25 | Loss: 0.00063076
Iteration 7/25 | Loss: 0.00063000
Iteration 8/25 | Loss: 0.00062992
Iteration 9/25 | Loss: 0.00062992
Iteration 10/25 | Loss: 0.00062992
Iteration 11/25 | Loss: 0.00062992
Iteration 12/25 | Loss: 0.00062992
Iteration 13/25 | Loss: 0.00062992
Iteration 14/25 | Loss: 0.00062992
Iteration 15/25 | Loss: 0.00062992
Iteration 16/25 | Loss: 0.00062992
Iteration 17/25 | Loss: 0.00062992
Iteration 18/25 | Loss: 0.00062992
Iteration 19/25 | Loss: 0.00062992
Iteration 20/25 | Loss: 0.00062992
Iteration 21/25 | Loss: 0.00062992
Iteration 22/25 | Loss: 0.00062992
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0006299242959357798, 0.0006299242959357798, 0.0006299242959357798, 0.0006299242959357798, 0.0006299242959357798]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006299242959357798

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.62528419
Iteration 2/25 | Loss: 0.00030525
Iteration 3/25 | Loss: 0.00030520
Iteration 4/25 | Loss: 0.00030520
Iteration 5/25 | Loss: 0.00030520
Iteration 6/25 | Loss: 0.00030520
Iteration 7/25 | Loss: 0.00030520
Iteration 8/25 | Loss: 0.00030520
Iteration 9/25 | Loss: 0.00030520
Iteration 10/25 | Loss: 0.00030520
Iteration 11/25 | Loss: 0.00030520
Iteration 12/25 | Loss: 0.00030520
Iteration 13/25 | Loss: 0.00030520
Iteration 14/25 | Loss: 0.00030520
Iteration 15/25 | Loss: 0.00030520
Iteration 16/25 | Loss: 0.00030520
Iteration 17/25 | Loss: 0.00030520
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00030519801657646894, 0.00030519801657646894, 0.00030519801657646894, 0.00030519801657646894, 0.00030519801657646894]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00030519801657646894

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030520
Iteration 2/1000 | Loss: 0.00003048
Iteration 3/1000 | Loss: 0.00001934
Iteration 4/1000 | Loss: 0.00001730
Iteration 5/1000 | Loss: 0.00001637
Iteration 6/1000 | Loss: 0.00001581
Iteration 7/1000 | Loss: 0.00001542
Iteration 8/1000 | Loss: 0.00001504
Iteration 9/1000 | Loss: 0.00001481
Iteration 10/1000 | Loss: 0.00001476
Iteration 11/1000 | Loss: 0.00001459
Iteration 12/1000 | Loss: 0.00001459
Iteration 13/1000 | Loss: 0.00001454
Iteration 14/1000 | Loss: 0.00001453
Iteration 15/1000 | Loss: 0.00001450
Iteration 16/1000 | Loss: 0.00001449
Iteration 17/1000 | Loss: 0.00001447
Iteration 18/1000 | Loss: 0.00001437
Iteration 19/1000 | Loss: 0.00001437
Iteration 20/1000 | Loss: 0.00001430
Iteration 21/1000 | Loss: 0.00001424
Iteration 22/1000 | Loss: 0.00001421
Iteration 23/1000 | Loss: 0.00001421
Iteration 24/1000 | Loss: 0.00001421
Iteration 25/1000 | Loss: 0.00001421
Iteration 26/1000 | Loss: 0.00001421
Iteration 27/1000 | Loss: 0.00001421
Iteration 28/1000 | Loss: 0.00001421
Iteration 29/1000 | Loss: 0.00001420
Iteration 30/1000 | Loss: 0.00001420
Iteration 31/1000 | Loss: 0.00001420
Iteration 32/1000 | Loss: 0.00001420
Iteration 33/1000 | Loss: 0.00001420
Iteration 34/1000 | Loss: 0.00001420
Iteration 35/1000 | Loss: 0.00001420
Iteration 36/1000 | Loss: 0.00001419
Iteration 37/1000 | Loss: 0.00001419
Iteration 38/1000 | Loss: 0.00001418
Iteration 39/1000 | Loss: 0.00001417
Iteration 40/1000 | Loss: 0.00001417
Iteration 41/1000 | Loss: 0.00001417
Iteration 42/1000 | Loss: 0.00001416
Iteration 43/1000 | Loss: 0.00001416
Iteration 44/1000 | Loss: 0.00001415
Iteration 45/1000 | Loss: 0.00001415
Iteration 46/1000 | Loss: 0.00001414
Iteration 47/1000 | Loss: 0.00001414
Iteration 48/1000 | Loss: 0.00001414
Iteration 49/1000 | Loss: 0.00001413
Iteration 50/1000 | Loss: 0.00001413
Iteration 51/1000 | Loss: 0.00001412
Iteration 52/1000 | Loss: 0.00001412
Iteration 53/1000 | Loss: 0.00001411
Iteration 54/1000 | Loss: 0.00001411
Iteration 55/1000 | Loss: 0.00001411
Iteration 56/1000 | Loss: 0.00001410
Iteration 57/1000 | Loss: 0.00001410
Iteration 58/1000 | Loss: 0.00001410
Iteration 59/1000 | Loss: 0.00001409
Iteration 60/1000 | Loss: 0.00001409
Iteration 61/1000 | Loss: 0.00001408
Iteration 62/1000 | Loss: 0.00001408
Iteration 63/1000 | Loss: 0.00001408
Iteration 64/1000 | Loss: 0.00001407
Iteration 65/1000 | Loss: 0.00001407
Iteration 66/1000 | Loss: 0.00001407
Iteration 67/1000 | Loss: 0.00001406
Iteration 68/1000 | Loss: 0.00001405
Iteration 69/1000 | Loss: 0.00001405
Iteration 70/1000 | Loss: 0.00001404
Iteration 71/1000 | Loss: 0.00001404
Iteration 72/1000 | Loss: 0.00001404
Iteration 73/1000 | Loss: 0.00001404
Iteration 74/1000 | Loss: 0.00001404
Iteration 75/1000 | Loss: 0.00001404
Iteration 76/1000 | Loss: 0.00001404
Iteration 77/1000 | Loss: 0.00001404
Iteration 78/1000 | Loss: 0.00001404
Iteration 79/1000 | Loss: 0.00001404
Iteration 80/1000 | Loss: 0.00001404
Iteration 81/1000 | Loss: 0.00001404
Iteration 82/1000 | Loss: 0.00001403
Iteration 83/1000 | Loss: 0.00001403
Iteration 84/1000 | Loss: 0.00001403
Iteration 85/1000 | Loss: 0.00001403
Iteration 86/1000 | Loss: 0.00001402
Iteration 87/1000 | Loss: 0.00001402
Iteration 88/1000 | Loss: 0.00001402
Iteration 89/1000 | Loss: 0.00001402
Iteration 90/1000 | Loss: 0.00001402
Iteration 91/1000 | Loss: 0.00001402
Iteration 92/1000 | Loss: 0.00001402
Iteration 93/1000 | Loss: 0.00001401
Iteration 94/1000 | Loss: 0.00001401
Iteration 95/1000 | Loss: 0.00001401
Iteration 96/1000 | Loss: 0.00001400
Iteration 97/1000 | Loss: 0.00001400
Iteration 98/1000 | Loss: 0.00001400
Iteration 99/1000 | Loss: 0.00001399
Iteration 100/1000 | Loss: 0.00001399
Iteration 101/1000 | Loss: 0.00001399
Iteration 102/1000 | Loss: 0.00001399
Iteration 103/1000 | Loss: 0.00001398
Iteration 104/1000 | Loss: 0.00001398
Iteration 105/1000 | Loss: 0.00001398
Iteration 106/1000 | Loss: 0.00001398
Iteration 107/1000 | Loss: 0.00001398
Iteration 108/1000 | Loss: 0.00001398
Iteration 109/1000 | Loss: 0.00001398
Iteration 110/1000 | Loss: 0.00001398
Iteration 111/1000 | Loss: 0.00001398
Iteration 112/1000 | Loss: 0.00001398
Iteration 113/1000 | Loss: 0.00001398
Iteration 114/1000 | Loss: 0.00001398
Iteration 115/1000 | Loss: 0.00001398
Iteration 116/1000 | Loss: 0.00001397
Iteration 117/1000 | Loss: 0.00001397
Iteration 118/1000 | Loss: 0.00001397
Iteration 119/1000 | Loss: 0.00001397
Iteration 120/1000 | Loss: 0.00001397
Iteration 121/1000 | Loss: 0.00001397
Iteration 122/1000 | Loss: 0.00001397
Iteration 123/1000 | Loss: 0.00001397
Iteration 124/1000 | Loss: 0.00001397
Iteration 125/1000 | Loss: 0.00001397
Iteration 126/1000 | Loss: 0.00001397
Iteration 127/1000 | Loss: 0.00001396
Iteration 128/1000 | Loss: 0.00001396
Iteration 129/1000 | Loss: 0.00001396
Iteration 130/1000 | Loss: 0.00001396
Iteration 131/1000 | Loss: 0.00001396
Iteration 132/1000 | Loss: 0.00001396
Iteration 133/1000 | Loss: 0.00001396
Iteration 134/1000 | Loss: 0.00001396
Iteration 135/1000 | Loss: 0.00001396
Iteration 136/1000 | Loss: 0.00001396
Iteration 137/1000 | Loss: 0.00001396
Iteration 138/1000 | Loss: 0.00001396
Iteration 139/1000 | Loss: 0.00001396
Iteration 140/1000 | Loss: 0.00001396
Iteration 141/1000 | Loss: 0.00001396
Iteration 142/1000 | Loss: 0.00001395
Iteration 143/1000 | Loss: 0.00001395
Iteration 144/1000 | Loss: 0.00001395
Iteration 145/1000 | Loss: 0.00001395
Iteration 146/1000 | Loss: 0.00001395
Iteration 147/1000 | Loss: 0.00001395
Iteration 148/1000 | Loss: 0.00001395
Iteration 149/1000 | Loss: 0.00001395
Iteration 150/1000 | Loss: 0.00001395
Iteration 151/1000 | Loss: 0.00001395
Iteration 152/1000 | Loss: 0.00001395
Iteration 153/1000 | Loss: 0.00001395
Iteration 154/1000 | Loss: 0.00001395
Iteration 155/1000 | Loss: 0.00001395
Iteration 156/1000 | Loss: 0.00001394
Iteration 157/1000 | Loss: 0.00001394
Iteration 158/1000 | Loss: 0.00001394
Iteration 159/1000 | Loss: 0.00001394
Iteration 160/1000 | Loss: 0.00001394
Iteration 161/1000 | Loss: 0.00001394
Iteration 162/1000 | Loss: 0.00001394
Iteration 163/1000 | Loss: 0.00001394
Iteration 164/1000 | Loss: 0.00001394
Iteration 165/1000 | Loss: 0.00001394
Iteration 166/1000 | Loss: 0.00001394
Iteration 167/1000 | Loss: 0.00001394
Iteration 168/1000 | Loss: 0.00001394
Iteration 169/1000 | Loss: 0.00001394
Iteration 170/1000 | Loss: 0.00001394
Iteration 171/1000 | Loss: 0.00001394
Iteration 172/1000 | Loss: 0.00001394
Iteration 173/1000 | Loss: 0.00001394
Iteration 174/1000 | Loss: 0.00001394
Iteration 175/1000 | Loss: 0.00001394
Iteration 176/1000 | Loss: 0.00001394
Iteration 177/1000 | Loss: 0.00001394
Iteration 178/1000 | Loss: 0.00001394
Iteration 179/1000 | Loss: 0.00001394
Iteration 180/1000 | Loss: 0.00001394
Iteration 181/1000 | Loss: 0.00001394
Iteration 182/1000 | Loss: 0.00001394
Iteration 183/1000 | Loss: 0.00001394
Iteration 184/1000 | Loss: 0.00001394
Iteration 185/1000 | Loss: 0.00001394
Iteration 186/1000 | Loss: 0.00001394
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [1.3941422366769984e-05, 1.3941422366769984e-05, 1.3941422366769984e-05, 1.3941422366769984e-05, 1.3941422366769984e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3941422366769984e-05

Optimization complete. Final v2v error: 3.1438074111938477 mm

Highest mean error: 3.569800853729248 mm for frame 121

Lowest mean error: 2.7952282428741455 mm for frame 30

Saving results

Total time: 38.54037284851074
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00637974
Iteration 2/25 | Loss: 0.00075265
Iteration 3/25 | Loss: 0.00064953
Iteration 4/25 | Loss: 0.00062676
Iteration 5/25 | Loss: 0.00061930
Iteration 6/25 | Loss: 0.00061776
Iteration 7/25 | Loss: 0.00061747
Iteration 8/25 | Loss: 0.00061747
Iteration 9/25 | Loss: 0.00061747
Iteration 10/25 | Loss: 0.00061747
Iteration 11/25 | Loss: 0.00061747
Iteration 12/25 | Loss: 0.00061747
Iteration 13/25 | Loss: 0.00061747
Iteration 14/25 | Loss: 0.00061747
Iteration 15/25 | Loss: 0.00061747
Iteration 16/25 | Loss: 0.00061747
Iteration 17/25 | Loss: 0.00061747
Iteration 18/25 | Loss: 0.00061747
Iteration 19/25 | Loss: 0.00061747
Iteration 20/25 | Loss: 0.00061747
Iteration 21/25 | Loss: 0.00061747
Iteration 22/25 | Loss: 0.00061747
Iteration 23/25 | Loss: 0.00061747
Iteration 24/25 | Loss: 0.00061747
Iteration 25/25 | Loss: 0.00061747
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006174712907522917, 0.0006174712907522917, 0.0006174712907522917, 0.0006174712907522917, 0.0006174712907522917]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006174712907522917

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.01993346
Iteration 2/25 | Loss: 0.00024853
Iteration 3/25 | Loss: 0.00024853
Iteration 4/25 | Loss: 0.00024853
Iteration 5/25 | Loss: 0.00024853
Iteration 6/25 | Loss: 0.00024853
Iteration 7/25 | Loss: 0.00024853
Iteration 8/25 | Loss: 0.00024853
Iteration 9/25 | Loss: 0.00024853
Iteration 10/25 | Loss: 0.00024853
Iteration 11/25 | Loss: 0.00024853
Iteration 12/25 | Loss: 0.00024853
Iteration 13/25 | Loss: 0.00024853
Iteration 14/25 | Loss: 0.00024853
Iteration 15/25 | Loss: 0.00024853
Iteration 16/25 | Loss: 0.00024853
Iteration 17/25 | Loss: 0.00024853
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0002485265431459993, 0.0002485265431459993, 0.0002485265431459993, 0.0002485265431459993, 0.0002485265431459993]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002485265431459993

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024853
Iteration 2/1000 | Loss: 0.00003178
Iteration 3/1000 | Loss: 0.00002026
Iteration 4/1000 | Loss: 0.00001883
Iteration 5/1000 | Loss: 0.00001774
Iteration 6/1000 | Loss: 0.00001709
Iteration 7/1000 | Loss: 0.00001653
Iteration 8/1000 | Loss: 0.00001626
Iteration 9/1000 | Loss: 0.00001623
Iteration 10/1000 | Loss: 0.00001619
Iteration 11/1000 | Loss: 0.00001600
Iteration 12/1000 | Loss: 0.00001596
Iteration 13/1000 | Loss: 0.00001593
Iteration 14/1000 | Loss: 0.00001592
Iteration 15/1000 | Loss: 0.00001591
Iteration 16/1000 | Loss: 0.00001580
Iteration 17/1000 | Loss: 0.00001580
Iteration 18/1000 | Loss: 0.00001579
Iteration 19/1000 | Loss: 0.00001579
Iteration 20/1000 | Loss: 0.00001578
Iteration 21/1000 | Loss: 0.00001575
Iteration 22/1000 | Loss: 0.00001575
Iteration 23/1000 | Loss: 0.00001574
Iteration 24/1000 | Loss: 0.00001573
Iteration 25/1000 | Loss: 0.00001573
Iteration 26/1000 | Loss: 0.00001572
Iteration 27/1000 | Loss: 0.00001571
Iteration 28/1000 | Loss: 0.00001571
Iteration 29/1000 | Loss: 0.00001570
Iteration 30/1000 | Loss: 0.00001570
Iteration 31/1000 | Loss: 0.00001570
Iteration 32/1000 | Loss: 0.00001569
Iteration 33/1000 | Loss: 0.00001567
Iteration 34/1000 | Loss: 0.00001567
Iteration 35/1000 | Loss: 0.00001567
Iteration 36/1000 | Loss: 0.00001567
Iteration 37/1000 | Loss: 0.00001567
Iteration 38/1000 | Loss: 0.00001566
Iteration 39/1000 | Loss: 0.00001565
Iteration 40/1000 | Loss: 0.00001565
Iteration 41/1000 | Loss: 0.00001565
Iteration 42/1000 | Loss: 0.00001565
Iteration 43/1000 | Loss: 0.00001565
Iteration 44/1000 | Loss: 0.00001564
Iteration 45/1000 | Loss: 0.00001564
Iteration 46/1000 | Loss: 0.00001564
Iteration 47/1000 | Loss: 0.00001564
Iteration 48/1000 | Loss: 0.00001563
Iteration 49/1000 | Loss: 0.00001563
Iteration 50/1000 | Loss: 0.00001563
Iteration 51/1000 | Loss: 0.00001562
Iteration 52/1000 | Loss: 0.00001561
Iteration 53/1000 | Loss: 0.00001561
Iteration 54/1000 | Loss: 0.00001561
Iteration 55/1000 | Loss: 0.00001560
Iteration 56/1000 | Loss: 0.00001559
Iteration 57/1000 | Loss: 0.00001558
Iteration 58/1000 | Loss: 0.00001557
Iteration 59/1000 | Loss: 0.00001557
Iteration 60/1000 | Loss: 0.00001556
Iteration 61/1000 | Loss: 0.00001556
Iteration 62/1000 | Loss: 0.00001555
Iteration 63/1000 | Loss: 0.00001555
Iteration 64/1000 | Loss: 0.00001554
Iteration 65/1000 | Loss: 0.00001551
Iteration 66/1000 | Loss: 0.00001550
Iteration 67/1000 | Loss: 0.00001550
Iteration 68/1000 | Loss: 0.00001550
Iteration 69/1000 | Loss: 0.00001549
Iteration 70/1000 | Loss: 0.00001549
Iteration 71/1000 | Loss: 0.00001548
Iteration 72/1000 | Loss: 0.00001548
Iteration 73/1000 | Loss: 0.00001547
Iteration 74/1000 | Loss: 0.00001547
Iteration 75/1000 | Loss: 0.00001547
Iteration 76/1000 | Loss: 0.00001547
Iteration 77/1000 | Loss: 0.00001547
Iteration 78/1000 | Loss: 0.00001547
Iteration 79/1000 | Loss: 0.00001546
Iteration 80/1000 | Loss: 0.00001546
Iteration 81/1000 | Loss: 0.00001546
Iteration 82/1000 | Loss: 0.00001546
Iteration 83/1000 | Loss: 0.00001546
Iteration 84/1000 | Loss: 0.00001546
Iteration 85/1000 | Loss: 0.00001546
Iteration 86/1000 | Loss: 0.00001546
Iteration 87/1000 | Loss: 0.00001546
Iteration 88/1000 | Loss: 0.00001546
Iteration 89/1000 | Loss: 0.00001546
Iteration 90/1000 | Loss: 0.00001546
Iteration 91/1000 | Loss: 0.00001545
Iteration 92/1000 | Loss: 0.00001545
Iteration 93/1000 | Loss: 0.00001545
Iteration 94/1000 | Loss: 0.00001545
Iteration 95/1000 | Loss: 0.00001545
Iteration 96/1000 | Loss: 0.00001545
Iteration 97/1000 | Loss: 0.00001544
Iteration 98/1000 | Loss: 0.00001544
Iteration 99/1000 | Loss: 0.00001544
Iteration 100/1000 | Loss: 0.00001544
Iteration 101/1000 | Loss: 0.00001544
Iteration 102/1000 | Loss: 0.00001544
Iteration 103/1000 | Loss: 0.00001544
Iteration 104/1000 | Loss: 0.00001544
Iteration 105/1000 | Loss: 0.00001544
Iteration 106/1000 | Loss: 0.00001544
Iteration 107/1000 | Loss: 0.00001544
Iteration 108/1000 | Loss: 0.00001544
Iteration 109/1000 | Loss: 0.00001544
Iteration 110/1000 | Loss: 0.00001544
Iteration 111/1000 | Loss: 0.00001544
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [1.543796679470688e-05, 1.543796679470688e-05, 1.543796679470688e-05, 1.543796679470688e-05, 1.543796679470688e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.543796679470688e-05

Optimization complete. Final v2v error: 3.355278968811035 mm

Highest mean error: 3.7684457302093506 mm for frame 106

Lowest mean error: 3.111696481704712 mm for frame 141

Saving results

Total time: 34.091631174087524
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00476407
Iteration 2/25 | Loss: 0.00076476
Iteration 3/25 | Loss: 0.00062753
Iteration 4/25 | Loss: 0.00060568
Iteration 5/25 | Loss: 0.00059913
Iteration 6/25 | Loss: 0.00059750
Iteration 7/25 | Loss: 0.00059710
Iteration 8/25 | Loss: 0.00059710
Iteration 9/25 | Loss: 0.00059710
Iteration 10/25 | Loss: 0.00059710
Iteration 11/25 | Loss: 0.00059710
Iteration 12/25 | Loss: 0.00059710
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005970962229184806, 0.0005970962229184806, 0.0005970962229184806, 0.0005970962229184806, 0.0005970962229184806]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005970962229184806

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.17454863
Iteration 2/25 | Loss: 0.00027035
Iteration 3/25 | Loss: 0.00027035
Iteration 4/25 | Loss: 0.00027035
Iteration 5/25 | Loss: 0.00027034
Iteration 6/25 | Loss: 0.00027034
Iteration 7/25 | Loss: 0.00027034
Iteration 8/25 | Loss: 0.00027034
Iteration 9/25 | Loss: 0.00027034
Iteration 10/25 | Loss: 0.00027034
Iteration 11/25 | Loss: 0.00027034
Iteration 12/25 | Loss: 0.00027034
Iteration 13/25 | Loss: 0.00027034
Iteration 14/25 | Loss: 0.00027034
Iteration 15/25 | Loss: 0.00027034
Iteration 16/25 | Loss: 0.00027034
Iteration 17/25 | Loss: 0.00027034
Iteration 18/25 | Loss: 0.00027034
Iteration 19/25 | Loss: 0.00027034
Iteration 20/25 | Loss: 0.00027034
Iteration 21/25 | Loss: 0.00027034
Iteration 22/25 | Loss: 0.00027034
Iteration 23/25 | Loss: 0.00027034
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.000270342716248706, 0.000270342716248706, 0.000270342716248706, 0.000270342716248706, 0.000270342716248706]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000270342716248706

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027034
Iteration 2/1000 | Loss: 0.00002558
Iteration 3/1000 | Loss: 0.00001716
Iteration 4/1000 | Loss: 0.00001513
Iteration 5/1000 | Loss: 0.00001433
Iteration 6/1000 | Loss: 0.00001384
Iteration 7/1000 | Loss: 0.00001347
Iteration 8/1000 | Loss: 0.00001327
Iteration 9/1000 | Loss: 0.00001318
Iteration 10/1000 | Loss: 0.00001310
Iteration 11/1000 | Loss: 0.00001307
Iteration 12/1000 | Loss: 0.00001303
Iteration 13/1000 | Loss: 0.00001301
Iteration 14/1000 | Loss: 0.00001300
Iteration 15/1000 | Loss: 0.00001299
Iteration 16/1000 | Loss: 0.00001295
Iteration 17/1000 | Loss: 0.00001295
Iteration 18/1000 | Loss: 0.00001292
Iteration 19/1000 | Loss: 0.00001291
Iteration 20/1000 | Loss: 0.00001286
Iteration 21/1000 | Loss: 0.00001279
Iteration 22/1000 | Loss: 0.00001273
Iteration 23/1000 | Loss: 0.00001273
Iteration 24/1000 | Loss: 0.00001272
Iteration 25/1000 | Loss: 0.00001271
Iteration 26/1000 | Loss: 0.00001271
Iteration 27/1000 | Loss: 0.00001270
Iteration 28/1000 | Loss: 0.00001270
Iteration 29/1000 | Loss: 0.00001269
Iteration 30/1000 | Loss: 0.00001269
Iteration 31/1000 | Loss: 0.00001269
Iteration 32/1000 | Loss: 0.00001269
Iteration 33/1000 | Loss: 0.00001269
Iteration 34/1000 | Loss: 0.00001268
Iteration 35/1000 | Loss: 0.00001268
Iteration 36/1000 | Loss: 0.00001268
Iteration 37/1000 | Loss: 0.00001268
Iteration 38/1000 | Loss: 0.00001267
Iteration 39/1000 | Loss: 0.00001267
Iteration 40/1000 | Loss: 0.00001267
Iteration 41/1000 | Loss: 0.00001267
Iteration 42/1000 | Loss: 0.00001266
Iteration 43/1000 | Loss: 0.00001266
Iteration 44/1000 | Loss: 0.00001266
Iteration 45/1000 | Loss: 0.00001266
Iteration 46/1000 | Loss: 0.00001265
Iteration 47/1000 | Loss: 0.00001265
Iteration 48/1000 | Loss: 0.00001264
Iteration 49/1000 | Loss: 0.00001264
Iteration 50/1000 | Loss: 0.00001264
Iteration 51/1000 | Loss: 0.00001264
Iteration 52/1000 | Loss: 0.00001264
Iteration 53/1000 | Loss: 0.00001264
Iteration 54/1000 | Loss: 0.00001263
Iteration 55/1000 | Loss: 0.00001263
Iteration 56/1000 | Loss: 0.00001263
Iteration 57/1000 | Loss: 0.00001262
Iteration 58/1000 | Loss: 0.00001262
Iteration 59/1000 | Loss: 0.00001262
Iteration 60/1000 | Loss: 0.00001261
Iteration 61/1000 | Loss: 0.00001261
Iteration 62/1000 | Loss: 0.00001261
Iteration 63/1000 | Loss: 0.00001260
Iteration 64/1000 | Loss: 0.00001260
Iteration 65/1000 | Loss: 0.00001260
Iteration 66/1000 | Loss: 0.00001259
Iteration 67/1000 | Loss: 0.00001259
Iteration 68/1000 | Loss: 0.00001259
Iteration 69/1000 | Loss: 0.00001259
Iteration 70/1000 | Loss: 0.00001259
Iteration 71/1000 | Loss: 0.00001258
Iteration 72/1000 | Loss: 0.00001258
Iteration 73/1000 | Loss: 0.00001258
Iteration 74/1000 | Loss: 0.00001257
Iteration 75/1000 | Loss: 0.00001257
Iteration 76/1000 | Loss: 0.00001257
Iteration 77/1000 | Loss: 0.00001257
Iteration 78/1000 | Loss: 0.00001256
Iteration 79/1000 | Loss: 0.00001256
Iteration 80/1000 | Loss: 0.00001255
Iteration 81/1000 | Loss: 0.00001255
Iteration 82/1000 | Loss: 0.00001255
Iteration 83/1000 | Loss: 0.00001255
Iteration 84/1000 | Loss: 0.00001255
Iteration 85/1000 | Loss: 0.00001255
Iteration 86/1000 | Loss: 0.00001254
Iteration 87/1000 | Loss: 0.00001254
Iteration 88/1000 | Loss: 0.00001254
Iteration 89/1000 | Loss: 0.00001254
Iteration 90/1000 | Loss: 0.00001254
Iteration 91/1000 | Loss: 0.00001254
Iteration 92/1000 | Loss: 0.00001254
Iteration 93/1000 | Loss: 0.00001254
Iteration 94/1000 | Loss: 0.00001253
Iteration 95/1000 | Loss: 0.00001253
Iteration 96/1000 | Loss: 0.00001253
Iteration 97/1000 | Loss: 0.00001253
Iteration 98/1000 | Loss: 0.00001253
Iteration 99/1000 | Loss: 0.00001253
Iteration 100/1000 | Loss: 0.00001253
Iteration 101/1000 | Loss: 0.00001253
Iteration 102/1000 | Loss: 0.00001252
Iteration 103/1000 | Loss: 0.00001252
Iteration 104/1000 | Loss: 0.00001252
Iteration 105/1000 | Loss: 0.00001252
Iteration 106/1000 | Loss: 0.00001252
Iteration 107/1000 | Loss: 0.00001252
Iteration 108/1000 | Loss: 0.00001252
Iteration 109/1000 | Loss: 0.00001252
Iteration 110/1000 | Loss: 0.00001251
Iteration 111/1000 | Loss: 0.00001251
Iteration 112/1000 | Loss: 0.00001251
Iteration 113/1000 | Loss: 0.00001251
Iteration 114/1000 | Loss: 0.00001251
Iteration 115/1000 | Loss: 0.00001251
Iteration 116/1000 | Loss: 0.00001251
Iteration 117/1000 | Loss: 0.00001251
Iteration 118/1000 | Loss: 0.00001251
Iteration 119/1000 | Loss: 0.00001251
Iteration 120/1000 | Loss: 0.00001251
Iteration 121/1000 | Loss: 0.00001251
Iteration 122/1000 | Loss: 0.00001251
Iteration 123/1000 | Loss: 0.00001251
Iteration 124/1000 | Loss: 0.00001251
Iteration 125/1000 | Loss: 0.00001250
Iteration 126/1000 | Loss: 0.00001250
Iteration 127/1000 | Loss: 0.00001250
Iteration 128/1000 | Loss: 0.00001250
Iteration 129/1000 | Loss: 0.00001250
Iteration 130/1000 | Loss: 0.00001250
Iteration 131/1000 | Loss: 0.00001250
Iteration 132/1000 | Loss: 0.00001250
Iteration 133/1000 | Loss: 0.00001250
Iteration 134/1000 | Loss: 0.00001250
Iteration 135/1000 | Loss: 0.00001249
Iteration 136/1000 | Loss: 0.00001249
Iteration 137/1000 | Loss: 0.00001249
Iteration 138/1000 | Loss: 0.00001249
Iteration 139/1000 | Loss: 0.00001249
Iteration 140/1000 | Loss: 0.00001249
Iteration 141/1000 | Loss: 0.00001249
Iteration 142/1000 | Loss: 0.00001249
Iteration 143/1000 | Loss: 0.00001249
Iteration 144/1000 | Loss: 0.00001249
Iteration 145/1000 | Loss: 0.00001249
Iteration 146/1000 | Loss: 0.00001249
Iteration 147/1000 | Loss: 0.00001248
Iteration 148/1000 | Loss: 0.00001248
Iteration 149/1000 | Loss: 0.00001248
Iteration 150/1000 | Loss: 0.00001248
Iteration 151/1000 | Loss: 0.00001248
Iteration 152/1000 | Loss: 0.00001248
Iteration 153/1000 | Loss: 0.00001248
Iteration 154/1000 | Loss: 0.00001248
Iteration 155/1000 | Loss: 0.00001248
Iteration 156/1000 | Loss: 0.00001248
Iteration 157/1000 | Loss: 0.00001248
Iteration 158/1000 | Loss: 0.00001248
Iteration 159/1000 | Loss: 0.00001248
Iteration 160/1000 | Loss: 0.00001248
Iteration 161/1000 | Loss: 0.00001248
Iteration 162/1000 | Loss: 0.00001248
Iteration 163/1000 | Loss: 0.00001248
Iteration 164/1000 | Loss: 0.00001248
Iteration 165/1000 | Loss: 0.00001248
Iteration 166/1000 | Loss: 0.00001248
Iteration 167/1000 | Loss: 0.00001248
Iteration 168/1000 | Loss: 0.00001248
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.2476988558773883e-05, 1.2476988558773883e-05, 1.2476988558773883e-05, 1.2476988558773883e-05, 1.2476988558773883e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2476988558773883e-05

Optimization complete. Final v2v error: 3.022023916244507 mm

Highest mean error: 3.4068078994750977 mm for frame 90

Lowest mean error: 2.8111112117767334 mm for frame 121

Saving results

Total time: 36.55291557312012
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00771163
Iteration 2/25 | Loss: 0.00155348
Iteration 3/25 | Loss: 0.00091867
Iteration 4/25 | Loss: 0.00079857
Iteration 5/25 | Loss: 0.00077770
Iteration 6/25 | Loss: 0.00076212
Iteration 7/25 | Loss: 0.00075430
Iteration 8/25 | Loss: 0.00074169
Iteration 9/25 | Loss: 0.00074060
Iteration 10/25 | Loss: 0.00073316
Iteration 11/25 | Loss: 0.00072867
Iteration 12/25 | Loss: 0.00072735
Iteration 13/25 | Loss: 0.00072647
Iteration 14/25 | Loss: 0.00072609
Iteration 15/25 | Loss: 0.00072603
Iteration 16/25 | Loss: 0.00072603
Iteration 17/25 | Loss: 0.00072603
Iteration 18/25 | Loss: 0.00072603
Iteration 19/25 | Loss: 0.00072603
Iteration 20/25 | Loss: 0.00072602
Iteration 21/25 | Loss: 0.00072602
Iteration 22/25 | Loss: 0.00072602
Iteration 23/25 | Loss: 0.00072602
Iteration 24/25 | Loss: 0.00072602
Iteration 25/25 | Loss: 0.00072602

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47457540
Iteration 2/25 | Loss: 0.00029933
Iteration 3/25 | Loss: 0.00029929
Iteration 4/25 | Loss: 0.00029929
Iteration 5/25 | Loss: 0.00029929
Iteration 6/25 | Loss: 0.00029929
Iteration 7/25 | Loss: 0.00029929
Iteration 8/25 | Loss: 0.00029929
Iteration 9/25 | Loss: 0.00029929
Iteration 10/25 | Loss: 0.00029929
Iteration 11/25 | Loss: 0.00029929
Iteration 12/25 | Loss: 0.00029929
Iteration 13/25 | Loss: 0.00029929
Iteration 14/25 | Loss: 0.00029929
Iteration 15/25 | Loss: 0.00029929
Iteration 16/25 | Loss: 0.00029929
Iteration 17/25 | Loss: 0.00029929
Iteration 18/25 | Loss: 0.00029929
Iteration 19/25 | Loss: 0.00029929
Iteration 20/25 | Loss: 0.00029929
Iteration 21/25 | Loss: 0.00029929
Iteration 22/25 | Loss: 0.00029929
Iteration 23/25 | Loss: 0.00029929
Iteration 24/25 | Loss: 0.00029929
Iteration 25/25 | Loss: 0.00029929

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029929
Iteration 2/1000 | Loss: 0.00003614
Iteration 3/1000 | Loss: 0.00002557
Iteration 4/1000 | Loss: 0.00002352
Iteration 5/1000 | Loss: 0.00002231
Iteration 6/1000 | Loss: 0.00002144
Iteration 7/1000 | Loss: 0.00002090
Iteration 8/1000 | Loss: 0.00002035
Iteration 9/1000 | Loss: 0.00001997
Iteration 10/1000 | Loss: 0.00001961
Iteration 11/1000 | Loss: 0.00001947
Iteration 12/1000 | Loss: 0.00001940
Iteration 13/1000 | Loss: 0.00001924
Iteration 14/1000 | Loss: 0.00001921
Iteration 15/1000 | Loss: 0.00001910
Iteration 16/1000 | Loss: 0.00001901
Iteration 17/1000 | Loss: 0.00001898
Iteration 18/1000 | Loss: 0.00001897
Iteration 19/1000 | Loss: 0.00001897
Iteration 20/1000 | Loss: 0.00001894
Iteration 21/1000 | Loss: 0.00001894
Iteration 22/1000 | Loss: 0.00001893
Iteration 23/1000 | Loss: 0.00001893
Iteration 24/1000 | Loss: 0.00001893
Iteration 25/1000 | Loss: 0.00001893
Iteration 26/1000 | Loss: 0.00001892
Iteration 27/1000 | Loss: 0.00001892
Iteration 28/1000 | Loss: 0.00001892
Iteration 29/1000 | Loss: 0.00001892
Iteration 30/1000 | Loss: 0.00001891
Iteration 31/1000 | Loss: 0.00001891
Iteration 32/1000 | Loss: 0.00001891
Iteration 33/1000 | Loss: 0.00001890
Iteration 34/1000 | Loss: 0.00001890
Iteration 35/1000 | Loss: 0.00001890
Iteration 36/1000 | Loss: 0.00001890
Iteration 37/1000 | Loss: 0.00001889
Iteration 38/1000 | Loss: 0.00001889
Iteration 39/1000 | Loss: 0.00001889
Iteration 40/1000 | Loss: 0.00001888
Iteration 41/1000 | Loss: 0.00001888
Iteration 42/1000 | Loss: 0.00001888
Iteration 43/1000 | Loss: 0.00001888
Iteration 44/1000 | Loss: 0.00001888
Iteration 45/1000 | Loss: 0.00001888
Iteration 46/1000 | Loss: 0.00001888
Iteration 47/1000 | Loss: 0.00001888
Iteration 48/1000 | Loss: 0.00001888
Iteration 49/1000 | Loss: 0.00001888
Iteration 50/1000 | Loss: 0.00001888
Iteration 51/1000 | Loss: 0.00001888
Iteration 52/1000 | Loss: 0.00001887
Iteration 53/1000 | Loss: 0.00001887
Iteration 54/1000 | Loss: 0.00001887
Iteration 55/1000 | Loss: 0.00001887
Iteration 56/1000 | Loss: 0.00001887
Iteration 57/1000 | Loss: 0.00001887
Iteration 58/1000 | Loss: 0.00001887
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 58. Stopping optimization.
Last 5 losses: [1.8873870430979878e-05, 1.8873870430979878e-05, 1.8873870430979878e-05, 1.8873870430979878e-05, 1.8873870430979878e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8873870430979878e-05

Optimization complete. Final v2v error: 3.7166242599487305 mm

Highest mean error: 4.381824970245361 mm for frame 104

Lowest mean error: 3.309826374053955 mm for frame 170

Saving results

Total time: 53.93866181373596
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00765570
Iteration 2/25 | Loss: 0.00178840
Iteration 3/25 | Loss: 0.00097166
Iteration 4/25 | Loss: 0.00079320
Iteration 5/25 | Loss: 0.00076128
Iteration 6/25 | Loss: 0.00074700
Iteration 7/25 | Loss: 0.00072803
Iteration 8/25 | Loss: 0.00072360
Iteration 9/25 | Loss: 0.00072281
Iteration 10/25 | Loss: 0.00072256
Iteration 11/25 | Loss: 0.00072246
Iteration 12/25 | Loss: 0.00072245
Iteration 13/25 | Loss: 0.00072245
Iteration 14/25 | Loss: 0.00072245
Iteration 15/25 | Loss: 0.00072245
Iteration 16/25 | Loss: 0.00072245
Iteration 17/25 | Loss: 0.00072244
Iteration 18/25 | Loss: 0.00072244
Iteration 19/25 | Loss: 0.00072244
Iteration 20/25 | Loss: 0.00072244
Iteration 21/25 | Loss: 0.00072244
Iteration 22/25 | Loss: 0.00072244
Iteration 23/25 | Loss: 0.00072244
Iteration 24/25 | Loss: 0.00072244
Iteration 25/25 | Loss: 0.00072244

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.15207577
Iteration 2/25 | Loss: 0.00031893
Iteration 3/25 | Loss: 0.00031893
Iteration 4/25 | Loss: 0.00031892
Iteration 5/25 | Loss: 0.00031892
Iteration 6/25 | Loss: 0.00031892
Iteration 7/25 | Loss: 0.00031892
Iteration 8/25 | Loss: 0.00031892
Iteration 9/25 | Loss: 0.00031892
Iteration 10/25 | Loss: 0.00031892
Iteration 11/25 | Loss: 0.00031892
Iteration 12/25 | Loss: 0.00031892
Iteration 13/25 | Loss: 0.00031892
Iteration 14/25 | Loss: 0.00031892
Iteration 15/25 | Loss: 0.00031892
Iteration 16/25 | Loss: 0.00031892
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0003189233539160341, 0.0003189233539160341, 0.0003189233539160341, 0.0003189233539160341, 0.0003189233539160341]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003189233539160341

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031892
Iteration 2/1000 | Loss: 0.00003182
Iteration 3/1000 | Loss: 0.00002307
Iteration 4/1000 | Loss: 0.00002119
Iteration 5/1000 | Loss: 0.00002011
Iteration 6/1000 | Loss: 0.00001938
Iteration 7/1000 | Loss: 0.00001899
Iteration 8/1000 | Loss: 0.00001880
Iteration 9/1000 | Loss: 0.00001867
Iteration 10/1000 | Loss: 0.00001866
Iteration 11/1000 | Loss: 0.00001865
Iteration 12/1000 | Loss: 0.00001861
Iteration 13/1000 | Loss: 0.00001861
Iteration 14/1000 | Loss: 0.00001861
Iteration 15/1000 | Loss: 0.00001860
Iteration 16/1000 | Loss: 0.00001860
Iteration 17/1000 | Loss: 0.00001858
Iteration 18/1000 | Loss: 0.00001857
Iteration 19/1000 | Loss: 0.00001857
Iteration 20/1000 | Loss: 0.00001853
Iteration 21/1000 | Loss: 0.00001851
Iteration 22/1000 | Loss: 0.00001849
Iteration 23/1000 | Loss: 0.00001847
Iteration 24/1000 | Loss: 0.00001846
Iteration 25/1000 | Loss: 0.00001846
Iteration 26/1000 | Loss: 0.00001846
Iteration 27/1000 | Loss: 0.00001846
Iteration 28/1000 | Loss: 0.00001845
Iteration 29/1000 | Loss: 0.00001845
Iteration 30/1000 | Loss: 0.00001845
Iteration 31/1000 | Loss: 0.00001844
Iteration 32/1000 | Loss: 0.00001843
Iteration 33/1000 | Loss: 0.00001841
Iteration 34/1000 | Loss: 0.00001841
Iteration 35/1000 | Loss: 0.00001837
Iteration 36/1000 | Loss: 0.00001835
Iteration 37/1000 | Loss: 0.00001835
Iteration 38/1000 | Loss: 0.00001833
Iteration 39/1000 | Loss: 0.00001831
Iteration 40/1000 | Loss: 0.00001831
Iteration 41/1000 | Loss: 0.00001831
Iteration 42/1000 | Loss: 0.00001830
Iteration 43/1000 | Loss: 0.00001830
Iteration 44/1000 | Loss: 0.00001827
Iteration 45/1000 | Loss: 0.00001827
Iteration 46/1000 | Loss: 0.00001827
Iteration 47/1000 | Loss: 0.00001827
Iteration 48/1000 | Loss: 0.00001827
Iteration 49/1000 | Loss: 0.00001827
Iteration 50/1000 | Loss: 0.00001827
Iteration 51/1000 | Loss: 0.00001827
Iteration 52/1000 | Loss: 0.00001827
Iteration 53/1000 | Loss: 0.00001827
Iteration 54/1000 | Loss: 0.00001827
Iteration 55/1000 | Loss: 0.00001827
Iteration 56/1000 | Loss: 0.00001827
Iteration 57/1000 | Loss: 0.00001826
Iteration 58/1000 | Loss: 0.00001826
Iteration 59/1000 | Loss: 0.00001825
Iteration 60/1000 | Loss: 0.00001824
Iteration 61/1000 | Loss: 0.00001824
Iteration 62/1000 | Loss: 0.00001824
Iteration 63/1000 | Loss: 0.00001823
Iteration 64/1000 | Loss: 0.00001823
Iteration 65/1000 | Loss: 0.00001823
Iteration 66/1000 | Loss: 0.00001823
Iteration 67/1000 | Loss: 0.00001823
Iteration 68/1000 | Loss: 0.00001823
Iteration 69/1000 | Loss: 0.00001823
Iteration 70/1000 | Loss: 0.00001822
Iteration 71/1000 | Loss: 0.00001822
Iteration 72/1000 | Loss: 0.00001822
Iteration 73/1000 | Loss: 0.00001820
Iteration 74/1000 | Loss: 0.00001819
Iteration 75/1000 | Loss: 0.00001819
Iteration 76/1000 | Loss: 0.00001818
Iteration 77/1000 | Loss: 0.00001818
Iteration 78/1000 | Loss: 0.00001817
Iteration 79/1000 | Loss: 0.00001817
Iteration 80/1000 | Loss: 0.00001817
Iteration 81/1000 | Loss: 0.00001816
Iteration 82/1000 | Loss: 0.00001816
Iteration 83/1000 | Loss: 0.00001816
Iteration 84/1000 | Loss: 0.00001816
Iteration 85/1000 | Loss: 0.00001816
Iteration 86/1000 | Loss: 0.00001815
Iteration 87/1000 | Loss: 0.00001815
Iteration 88/1000 | Loss: 0.00001815
Iteration 89/1000 | Loss: 0.00001815
Iteration 90/1000 | Loss: 0.00001815
Iteration 91/1000 | Loss: 0.00001815
Iteration 92/1000 | Loss: 0.00001815
Iteration 93/1000 | Loss: 0.00001814
Iteration 94/1000 | Loss: 0.00001814
Iteration 95/1000 | Loss: 0.00001814
Iteration 96/1000 | Loss: 0.00001814
Iteration 97/1000 | Loss: 0.00001814
Iteration 98/1000 | Loss: 0.00001814
Iteration 99/1000 | Loss: 0.00001813
Iteration 100/1000 | Loss: 0.00001813
Iteration 101/1000 | Loss: 0.00001813
Iteration 102/1000 | Loss: 0.00001813
Iteration 103/1000 | Loss: 0.00001813
Iteration 104/1000 | Loss: 0.00001813
Iteration 105/1000 | Loss: 0.00001813
Iteration 106/1000 | Loss: 0.00001813
Iteration 107/1000 | Loss: 0.00001813
Iteration 108/1000 | Loss: 0.00001813
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [1.8130745957023464e-05, 1.8130745957023464e-05, 1.8130745957023464e-05, 1.8130745957023464e-05, 1.8130745957023464e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8130745957023464e-05

Optimization complete. Final v2v error: 3.6202361583709717 mm

Highest mean error: 4.10227632522583 mm for frame 112

Lowest mean error: 3.383208751678467 mm for frame 28

Saving results

Total time: 46.75460910797119
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01020353
Iteration 2/25 | Loss: 0.00305290
Iteration 3/25 | Loss: 0.00218066
Iteration 4/25 | Loss: 0.00189664
Iteration 5/25 | Loss: 0.00192517
Iteration 6/25 | Loss: 0.00182355
Iteration 7/25 | Loss: 0.00158739
Iteration 8/25 | Loss: 0.00134002
Iteration 9/25 | Loss: 0.00118802
Iteration 10/25 | Loss: 0.00110383
Iteration 11/25 | Loss: 0.00104621
Iteration 12/25 | Loss: 0.00102604
Iteration 13/25 | Loss: 0.00098306
Iteration 14/25 | Loss: 0.00097078
Iteration 15/25 | Loss: 0.00096007
Iteration 16/25 | Loss: 0.00094916
Iteration 17/25 | Loss: 0.00092593
Iteration 18/25 | Loss: 0.00092435
Iteration 19/25 | Loss: 0.00092299
Iteration 20/25 | Loss: 0.00092208
Iteration 21/25 | Loss: 0.00091108
Iteration 22/25 | Loss: 0.00091001
Iteration 23/25 | Loss: 0.00090993
Iteration 24/25 | Loss: 0.00090502
Iteration 25/25 | Loss: 0.00090295

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43622553
Iteration 2/25 | Loss: 0.00237445
Iteration 3/25 | Loss: 0.00197598
Iteration 4/25 | Loss: 0.00197598
Iteration 5/25 | Loss: 0.00197598
Iteration 6/25 | Loss: 0.00197598
Iteration 7/25 | Loss: 0.00197598
Iteration 8/25 | Loss: 0.00197598
Iteration 9/25 | Loss: 0.00197598
Iteration 10/25 | Loss: 0.00197598
Iteration 11/25 | Loss: 0.00197598
Iteration 12/25 | Loss: 0.00197598
Iteration 13/25 | Loss: 0.00197598
Iteration 14/25 | Loss: 0.00197598
Iteration 15/25 | Loss: 0.00197598
Iteration 16/25 | Loss: 0.00197598
Iteration 17/25 | Loss: 0.00197598
Iteration 18/25 | Loss: 0.00197598
Iteration 19/25 | Loss: 0.00197598
Iteration 20/25 | Loss: 0.00197598
Iteration 21/25 | Loss: 0.00197598
Iteration 22/25 | Loss: 0.00197598
Iteration 23/25 | Loss: 0.00197598
Iteration 24/25 | Loss: 0.00197598
Iteration 25/25 | Loss: 0.00197598

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00197598
Iteration 2/1000 | Loss: 0.00129789
Iteration 3/1000 | Loss: 0.00287897
Iteration 4/1000 | Loss: 0.00039851
Iteration 5/1000 | Loss: 0.00034723
Iteration 6/1000 | Loss: 0.00039315
Iteration 7/1000 | Loss: 0.00018217
Iteration 8/1000 | Loss: 0.00028123
Iteration 9/1000 | Loss: 0.00018366
Iteration 10/1000 | Loss: 0.00027525
Iteration 11/1000 | Loss: 0.00026851
Iteration 12/1000 | Loss: 0.00038368
Iteration 13/1000 | Loss: 0.00021493
Iteration 14/1000 | Loss: 0.00023052
Iteration 15/1000 | Loss: 0.00041183
Iteration 16/1000 | Loss: 0.00018848
Iteration 17/1000 | Loss: 0.00015264
Iteration 18/1000 | Loss: 0.00031331
Iteration 19/1000 | Loss: 0.00026553
Iteration 20/1000 | Loss: 0.00018765
Iteration 21/1000 | Loss: 0.00020556
Iteration 22/1000 | Loss: 0.00021988
Iteration 23/1000 | Loss: 0.00021995
Iteration 24/1000 | Loss: 0.00017607
Iteration 25/1000 | Loss: 0.00062411
Iteration 26/1000 | Loss: 0.00101895
Iteration 27/1000 | Loss: 0.00106740
Iteration 28/1000 | Loss: 0.00320023
Iteration 29/1000 | Loss: 0.00321615
Iteration 30/1000 | Loss: 0.00240671
Iteration 31/1000 | Loss: 0.00106388
Iteration 32/1000 | Loss: 0.00106606
Iteration 33/1000 | Loss: 0.00033179
Iteration 34/1000 | Loss: 0.00025277
Iteration 35/1000 | Loss: 0.00057567
Iteration 36/1000 | Loss: 0.00031654
Iteration 37/1000 | Loss: 0.00039175
Iteration 38/1000 | Loss: 0.00022043
Iteration 39/1000 | Loss: 0.00017277
Iteration 40/1000 | Loss: 0.00027328
Iteration 41/1000 | Loss: 0.00030019
Iteration 42/1000 | Loss: 0.00023374
Iteration 43/1000 | Loss: 0.00025957
Iteration 44/1000 | Loss: 0.00038728
Iteration 45/1000 | Loss: 0.00011404
Iteration 46/1000 | Loss: 0.00012681
Iteration 47/1000 | Loss: 0.00012346
Iteration 48/1000 | Loss: 0.00013982
Iteration 49/1000 | Loss: 0.00021378
Iteration 50/1000 | Loss: 0.00012706
Iteration 51/1000 | Loss: 0.00009855
Iteration 52/1000 | Loss: 0.00016146
Iteration 53/1000 | Loss: 0.00009813
Iteration 54/1000 | Loss: 0.00012808
Iteration 55/1000 | Loss: 0.00010987
Iteration 56/1000 | Loss: 0.00010233
Iteration 57/1000 | Loss: 0.00011179
Iteration 58/1000 | Loss: 0.00010361
Iteration 59/1000 | Loss: 0.00007023
Iteration 60/1000 | Loss: 0.00007477
Iteration 61/1000 | Loss: 0.00006586
Iteration 62/1000 | Loss: 0.00006162
Iteration 63/1000 | Loss: 0.00006386
Iteration 64/1000 | Loss: 0.00006499
Iteration 65/1000 | Loss: 0.00008012
Iteration 66/1000 | Loss: 0.00006595
Iteration 67/1000 | Loss: 0.00006493
Iteration 68/1000 | Loss: 0.00007570
Iteration 69/1000 | Loss: 0.00008124
Iteration 70/1000 | Loss: 0.00007756
Iteration 71/1000 | Loss: 0.00007561
Iteration 72/1000 | Loss: 0.00006838
Iteration 73/1000 | Loss: 0.00007968
Iteration 74/1000 | Loss: 0.00007280
Iteration 75/1000 | Loss: 0.00007911
Iteration 76/1000 | Loss: 0.00006186
Iteration 77/1000 | Loss: 0.00006508
Iteration 78/1000 | Loss: 0.00006856
Iteration 79/1000 | Loss: 0.00006896
Iteration 80/1000 | Loss: 0.00007206
Iteration 81/1000 | Loss: 0.00006905
Iteration 82/1000 | Loss: 0.00006843
Iteration 83/1000 | Loss: 0.00005765
Iteration 84/1000 | Loss: 0.00007170
Iteration 85/1000 | Loss: 0.00006010
Iteration 86/1000 | Loss: 0.00006587
Iteration 87/1000 | Loss: 0.00006478
Iteration 88/1000 | Loss: 0.00006531
Iteration 89/1000 | Loss: 0.00007186
Iteration 90/1000 | Loss: 0.00007903
Iteration 91/1000 | Loss: 0.00007379
Iteration 92/1000 | Loss: 0.00008518
Iteration 93/1000 | Loss: 0.00008285
Iteration 94/1000 | Loss: 0.00007848
Iteration 95/1000 | Loss: 0.00008723
Iteration 96/1000 | Loss: 0.00008079
Iteration 97/1000 | Loss: 0.00007887
Iteration 98/1000 | Loss: 0.00008376
Iteration 99/1000 | Loss: 0.00008543
Iteration 100/1000 | Loss: 0.00008030
Iteration 101/1000 | Loss: 0.00006416
Iteration 102/1000 | Loss: 0.00007556
Iteration 103/1000 | Loss: 0.00007943
Iteration 104/1000 | Loss: 0.00006073
Iteration 105/1000 | Loss: 0.00007058
Iteration 106/1000 | Loss: 0.00008430
Iteration 107/1000 | Loss: 0.00007712
Iteration 108/1000 | Loss: 0.00006738
Iteration 109/1000 | Loss: 0.00006830
Iteration 110/1000 | Loss: 0.00006564
Iteration 111/1000 | Loss: 0.00006588
Iteration 112/1000 | Loss: 0.00007402
Iteration 113/1000 | Loss: 0.00007118
Iteration 114/1000 | Loss: 0.00006110
Iteration 115/1000 | Loss: 0.00005833
Iteration 116/1000 | Loss: 0.00005614
Iteration 117/1000 | Loss: 0.00005869
Iteration 118/1000 | Loss: 0.00005179
Iteration 119/1000 | Loss: 0.00005591
Iteration 120/1000 | Loss: 0.00008667
Iteration 121/1000 | Loss: 0.00008218
Iteration 122/1000 | Loss: 0.00006275
Iteration 123/1000 | Loss: 0.00008135
Iteration 124/1000 | Loss: 0.00007972
Iteration 125/1000 | Loss: 0.00005070
Iteration 126/1000 | Loss: 0.00005534
Iteration 127/1000 | Loss: 0.00005503
Iteration 128/1000 | Loss: 0.00005463
Iteration 129/1000 | Loss: 0.00008223
Iteration 130/1000 | Loss: 0.00007875
Iteration 131/1000 | Loss: 0.00009541
Iteration 132/1000 | Loss: 0.00007715
Iteration 133/1000 | Loss: 0.00008518
Iteration 134/1000 | Loss: 0.00007567
Iteration 135/1000 | Loss: 0.00008246
Iteration 136/1000 | Loss: 0.00005467
Iteration 137/1000 | Loss: 0.00005858
Iteration 138/1000 | Loss: 0.00005883
Iteration 139/1000 | Loss: 0.00006445
Iteration 140/1000 | Loss: 0.00005855
Iteration 141/1000 | Loss: 0.00006229
Iteration 142/1000 | Loss: 0.00005670
Iteration 143/1000 | Loss: 0.00005890
Iteration 144/1000 | Loss: 0.00005610
Iteration 145/1000 | Loss: 0.00005619
Iteration 146/1000 | Loss: 0.00005489
Iteration 147/1000 | Loss: 0.00005561
Iteration 148/1000 | Loss: 0.00005163
Iteration 149/1000 | Loss: 0.00006511
Iteration 150/1000 | Loss: 0.00005489
Iteration 151/1000 | Loss: 0.00005496
Iteration 152/1000 | Loss: 0.00005283
Iteration 153/1000 | Loss: 0.00005483
Iteration 154/1000 | Loss: 0.00005895
Iteration 155/1000 | Loss: 0.00005127
Iteration 156/1000 | Loss: 0.00005397
Iteration 157/1000 | Loss: 0.00005418
Iteration 158/1000 | Loss: 0.00005344
Iteration 159/1000 | Loss: 0.00005572
Iteration 160/1000 | Loss: 0.00005295
Iteration 161/1000 | Loss: 0.00005264
Iteration 162/1000 | Loss: 0.00005449
Iteration 163/1000 | Loss: 0.00005175
Iteration 164/1000 | Loss: 0.00006558
Iteration 165/1000 | Loss: 0.00006091
Iteration 166/1000 | Loss: 0.00006243
Iteration 167/1000 | Loss: 0.00006035
Iteration 168/1000 | Loss: 0.00006225
Iteration 169/1000 | Loss: 0.00005320
Iteration 170/1000 | Loss: 0.00006435
Iteration 171/1000 | Loss: 0.00006525
Iteration 172/1000 | Loss: 0.00005509
Iteration 173/1000 | Loss: 0.00006221
Iteration 174/1000 | Loss: 0.00006314
Iteration 175/1000 | Loss: 0.00007642
Iteration 176/1000 | Loss: 0.00005600
Iteration 177/1000 | Loss: 0.00007366
Iteration 178/1000 | Loss: 0.00019500
Iteration 179/1000 | Loss: 0.00015489
Iteration 180/1000 | Loss: 0.00012613
Iteration 181/1000 | Loss: 0.00004894
Iteration 182/1000 | Loss: 0.00004602
Iteration 183/1000 | Loss: 0.00004490
Iteration 184/1000 | Loss: 0.00004386
Iteration 185/1000 | Loss: 0.00004324
Iteration 186/1000 | Loss: 0.00004282
Iteration 187/1000 | Loss: 0.00004272
Iteration 188/1000 | Loss: 0.00004271
Iteration 189/1000 | Loss: 0.00004258
Iteration 190/1000 | Loss: 0.00004252
Iteration 191/1000 | Loss: 0.00004233
Iteration 192/1000 | Loss: 0.00004225
Iteration 193/1000 | Loss: 0.00004225
Iteration 194/1000 | Loss: 0.00004225
Iteration 195/1000 | Loss: 0.00004224
Iteration 196/1000 | Loss: 0.00004224
Iteration 197/1000 | Loss: 0.00004222
Iteration 198/1000 | Loss: 0.00004220
Iteration 199/1000 | Loss: 0.00004220
Iteration 200/1000 | Loss: 0.00004220
Iteration 201/1000 | Loss: 0.00004220
Iteration 202/1000 | Loss: 0.00004220
Iteration 203/1000 | Loss: 0.00004219
Iteration 204/1000 | Loss: 0.00004219
Iteration 205/1000 | Loss: 0.00004219
Iteration 206/1000 | Loss: 0.00004218
Iteration 207/1000 | Loss: 0.00004218
Iteration 208/1000 | Loss: 0.00004217
Iteration 209/1000 | Loss: 0.00004217
Iteration 210/1000 | Loss: 0.00004217
Iteration 211/1000 | Loss: 0.00004217
Iteration 212/1000 | Loss: 0.00004216
Iteration 213/1000 | Loss: 0.00004216
Iteration 214/1000 | Loss: 0.00004216
Iteration 215/1000 | Loss: 0.00004216
Iteration 216/1000 | Loss: 0.00004216
Iteration 217/1000 | Loss: 0.00004215
Iteration 218/1000 | Loss: 0.00004215
Iteration 219/1000 | Loss: 0.00004215
Iteration 220/1000 | Loss: 0.00004214
Iteration 221/1000 | Loss: 0.00004214
Iteration 222/1000 | Loss: 0.00004214
Iteration 223/1000 | Loss: 0.00004214
Iteration 224/1000 | Loss: 0.00004214
Iteration 225/1000 | Loss: 0.00004214
Iteration 226/1000 | Loss: 0.00004213
Iteration 227/1000 | Loss: 0.00004213
Iteration 228/1000 | Loss: 0.00004213
Iteration 229/1000 | Loss: 0.00004213
Iteration 230/1000 | Loss: 0.00004212
Iteration 231/1000 | Loss: 0.00004212
Iteration 232/1000 | Loss: 0.00004212
Iteration 233/1000 | Loss: 0.00004212
Iteration 234/1000 | Loss: 0.00004211
Iteration 235/1000 | Loss: 0.00004211
Iteration 236/1000 | Loss: 0.00004211
Iteration 237/1000 | Loss: 0.00004211
Iteration 238/1000 | Loss: 0.00004211
Iteration 239/1000 | Loss: 0.00004210
Iteration 240/1000 | Loss: 0.00004210
Iteration 241/1000 | Loss: 0.00004210
Iteration 242/1000 | Loss: 0.00004210
Iteration 243/1000 | Loss: 0.00004210
Iteration 244/1000 | Loss: 0.00004210
Iteration 245/1000 | Loss: 0.00004209
Iteration 246/1000 | Loss: 0.00004209
Iteration 247/1000 | Loss: 0.00004209
Iteration 248/1000 | Loss: 0.00004209
Iteration 249/1000 | Loss: 0.00004209
Iteration 250/1000 | Loss: 0.00004208
Iteration 251/1000 | Loss: 0.00004208
Iteration 252/1000 | Loss: 0.00004208
Iteration 253/1000 | Loss: 0.00004208
Iteration 254/1000 | Loss: 0.00004207
Iteration 255/1000 | Loss: 0.00004207
Iteration 256/1000 | Loss: 0.00004207
Iteration 257/1000 | Loss: 0.00004206
Iteration 258/1000 | Loss: 0.00004206
Iteration 259/1000 | Loss: 0.00004206
Iteration 260/1000 | Loss: 0.00004205
Iteration 261/1000 | Loss: 0.00004205
Iteration 262/1000 | Loss: 0.00004205
Iteration 263/1000 | Loss: 0.00004204
Iteration 264/1000 | Loss: 0.00004204
Iteration 265/1000 | Loss: 0.00004203
Iteration 266/1000 | Loss: 0.00004203
Iteration 267/1000 | Loss: 0.00004203
Iteration 268/1000 | Loss: 0.00004203
Iteration 269/1000 | Loss: 0.00004203
Iteration 270/1000 | Loss: 0.00004203
Iteration 271/1000 | Loss: 0.00004203
Iteration 272/1000 | Loss: 0.00004203
Iteration 273/1000 | Loss: 0.00004203
Iteration 274/1000 | Loss: 0.00004203
Iteration 275/1000 | Loss: 0.00004202
Iteration 276/1000 | Loss: 0.00004202
Iteration 277/1000 | Loss: 0.00004202
Iteration 278/1000 | Loss: 0.00004202
Iteration 279/1000 | Loss: 0.00004202
Iteration 280/1000 | Loss: 0.00004202
Iteration 281/1000 | Loss: 0.00004202
Iteration 282/1000 | Loss: 0.00004202
Iteration 283/1000 | Loss: 0.00004202
Iteration 284/1000 | Loss: 0.00004202
Iteration 285/1000 | Loss: 0.00004202
Iteration 286/1000 | Loss: 0.00004202
Iteration 287/1000 | Loss: 0.00004202
Iteration 288/1000 | Loss: 0.00004202
Iteration 289/1000 | Loss: 0.00004202
Iteration 290/1000 | Loss: 0.00004202
Iteration 291/1000 | Loss: 0.00004202
Iteration 292/1000 | Loss: 0.00004202
Iteration 293/1000 | Loss: 0.00004202
Iteration 294/1000 | Loss: 0.00004202
Iteration 295/1000 | Loss: 0.00004202
Iteration 296/1000 | Loss: 0.00004202
Iteration 297/1000 | Loss: 0.00004202
Iteration 298/1000 | Loss: 0.00004202
Iteration 299/1000 | Loss: 0.00004202
Iteration 300/1000 | Loss: 0.00004202
Iteration 301/1000 | Loss: 0.00004202
Iteration 302/1000 | Loss: 0.00004202
Iteration 303/1000 | Loss: 0.00004202
Iteration 304/1000 | Loss: 0.00004202
Iteration 305/1000 | Loss: 0.00004202
Iteration 306/1000 | Loss: 0.00004202
Iteration 307/1000 | Loss: 0.00004202
Iteration 308/1000 | Loss: 0.00004202
Iteration 309/1000 | Loss: 0.00004202
Iteration 310/1000 | Loss: 0.00004202
Iteration 311/1000 | Loss: 0.00004202
Iteration 312/1000 | Loss: 0.00004202
Iteration 313/1000 | Loss: 0.00004202
Iteration 314/1000 | Loss: 0.00004202
Iteration 315/1000 | Loss: 0.00004202
Iteration 316/1000 | Loss: 0.00004202
Iteration 317/1000 | Loss: 0.00004202
Iteration 318/1000 | Loss: 0.00004202
Iteration 319/1000 | Loss: 0.00004202
Iteration 320/1000 | Loss: 0.00004202
Iteration 321/1000 | Loss: 0.00004202
Iteration 322/1000 | Loss: 0.00004202
Iteration 323/1000 | Loss: 0.00004202
Iteration 324/1000 | Loss: 0.00004202
Iteration 325/1000 | Loss: 0.00004202
Iteration 326/1000 | Loss: 0.00004202
Iteration 327/1000 | Loss: 0.00004202
Iteration 328/1000 | Loss: 0.00004202
Iteration 329/1000 | Loss: 0.00004202
Iteration 330/1000 | Loss: 0.00004202
Iteration 331/1000 | Loss: 0.00004202
Iteration 332/1000 | Loss: 0.00004202
Iteration 333/1000 | Loss: 0.00004202
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 333. Stopping optimization.
Last 5 losses: [4.2023144487757236e-05, 4.2023144487757236e-05, 4.2023144487757236e-05, 4.2023144487757236e-05, 4.2023144487757236e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.2023144487757236e-05

Optimization complete. Final v2v error: 4.2867536544799805 mm

Highest mean error: 11.274555206298828 mm for frame 199

Lowest mean error: 3.630117893218994 mm for frame 10

Saving results

Total time: 357.2521941661835
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00832979
Iteration 2/25 | Loss: 0.00109690
Iteration 3/25 | Loss: 0.00073065
Iteration 4/25 | Loss: 0.00070097
Iteration 5/25 | Loss: 0.00069570
Iteration 6/25 | Loss: 0.00069488
Iteration 7/25 | Loss: 0.00069488
Iteration 8/25 | Loss: 0.00069488
Iteration 9/25 | Loss: 0.00069488
Iteration 10/25 | Loss: 0.00069488
Iteration 11/25 | Loss: 0.00069488
Iteration 12/25 | Loss: 0.00069488
Iteration 13/25 | Loss: 0.00069488
Iteration 14/25 | Loss: 0.00069488
Iteration 15/25 | Loss: 0.00069488
Iteration 16/25 | Loss: 0.00069488
Iteration 17/25 | Loss: 0.00069488
Iteration 18/25 | Loss: 0.00069488
Iteration 19/25 | Loss: 0.00069488
Iteration 20/25 | Loss: 0.00069488
Iteration 21/25 | Loss: 0.00069488
Iteration 22/25 | Loss: 0.00069488
Iteration 23/25 | Loss: 0.00069488
Iteration 24/25 | Loss: 0.00069488
Iteration 25/25 | Loss: 0.00069488

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42372942
Iteration 2/25 | Loss: 0.00033968
Iteration 3/25 | Loss: 0.00033967
Iteration 4/25 | Loss: 0.00033967
Iteration 5/25 | Loss: 0.00033967
Iteration 6/25 | Loss: 0.00033967
Iteration 7/25 | Loss: 0.00033967
Iteration 8/25 | Loss: 0.00033967
Iteration 9/25 | Loss: 0.00033967
Iteration 10/25 | Loss: 0.00033967
Iteration 11/25 | Loss: 0.00033967
Iteration 12/25 | Loss: 0.00033967
Iteration 13/25 | Loss: 0.00033967
Iteration 14/25 | Loss: 0.00033967
Iteration 15/25 | Loss: 0.00033967
Iteration 16/25 | Loss: 0.00033967
Iteration 17/25 | Loss: 0.00033967
Iteration 18/25 | Loss: 0.00033967
Iteration 19/25 | Loss: 0.00033967
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0003396652464289218, 0.0003396652464289218, 0.0003396652464289218, 0.0003396652464289218, 0.0003396652464289218]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003396652464289218

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033967
Iteration 2/1000 | Loss: 0.00002293
Iteration 3/1000 | Loss: 0.00001855
Iteration 4/1000 | Loss: 0.00001737
Iteration 5/1000 | Loss: 0.00001672
Iteration 6/1000 | Loss: 0.00001628
Iteration 7/1000 | Loss: 0.00001587
Iteration 8/1000 | Loss: 0.00001583
Iteration 9/1000 | Loss: 0.00001563
Iteration 10/1000 | Loss: 0.00001562
Iteration 11/1000 | Loss: 0.00001561
Iteration 12/1000 | Loss: 0.00001561
Iteration 13/1000 | Loss: 0.00001561
Iteration 14/1000 | Loss: 0.00001561
Iteration 15/1000 | Loss: 0.00001561
Iteration 16/1000 | Loss: 0.00001560
Iteration 17/1000 | Loss: 0.00001560
Iteration 18/1000 | Loss: 0.00001559
Iteration 19/1000 | Loss: 0.00001559
Iteration 20/1000 | Loss: 0.00001558
Iteration 21/1000 | Loss: 0.00001558
Iteration 22/1000 | Loss: 0.00001555
Iteration 23/1000 | Loss: 0.00001555
Iteration 24/1000 | Loss: 0.00001555
Iteration 25/1000 | Loss: 0.00001555
Iteration 26/1000 | Loss: 0.00001555
Iteration 27/1000 | Loss: 0.00001555
Iteration 28/1000 | Loss: 0.00001554
Iteration 29/1000 | Loss: 0.00001554
Iteration 30/1000 | Loss: 0.00001554
Iteration 31/1000 | Loss: 0.00001554
Iteration 32/1000 | Loss: 0.00001554
Iteration 33/1000 | Loss: 0.00001553
Iteration 34/1000 | Loss: 0.00001553
Iteration 35/1000 | Loss: 0.00001552
Iteration 36/1000 | Loss: 0.00001552
Iteration 37/1000 | Loss: 0.00001552
Iteration 38/1000 | Loss: 0.00001551
Iteration 39/1000 | Loss: 0.00001550
Iteration 40/1000 | Loss: 0.00001550
Iteration 41/1000 | Loss: 0.00001550
Iteration 42/1000 | Loss: 0.00001550
Iteration 43/1000 | Loss: 0.00001550
Iteration 44/1000 | Loss: 0.00001550
Iteration 45/1000 | Loss: 0.00001550
Iteration 46/1000 | Loss: 0.00001550
Iteration 47/1000 | Loss: 0.00001550
Iteration 48/1000 | Loss: 0.00001549
Iteration 49/1000 | Loss: 0.00001549
Iteration 50/1000 | Loss: 0.00001548
Iteration 51/1000 | Loss: 0.00001548
Iteration 52/1000 | Loss: 0.00001548
Iteration 53/1000 | Loss: 0.00001547
Iteration 54/1000 | Loss: 0.00001547
Iteration 55/1000 | Loss: 0.00001547
Iteration 56/1000 | Loss: 0.00001546
Iteration 57/1000 | Loss: 0.00001546
Iteration 58/1000 | Loss: 0.00001546
Iteration 59/1000 | Loss: 0.00001546
Iteration 60/1000 | Loss: 0.00001546
Iteration 61/1000 | Loss: 0.00001546
Iteration 62/1000 | Loss: 0.00001546
Iteration 63/1000 | Loss: 0.00001546
Iteration 64/1000 | Loss: 0.00001546
Iteration 65/1000 | Loss: 0.00001546
Iteration 66/1000 | Loss: 0.00001546
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 66. Stopping optimization.
Last 5 losses: [1.5461137081729248e-05, 1.5461137081729248e-05, 1.5461137081729248e-05, 1.5461137081729248e-05, 1.5461137081729248e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5461137081729248e-05

Optimization complete. Final v2v error: 3.3284127712249756 mm

Highest mean error: 3.607963800430298 mm for frame 55

Lowest mean error: 3.166644811630249 mm for frame 165

Saving results

Total time: 27.519979000091553
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01061258
Iteration 2/25 | Loss: 0.00199656
Iteration 3/25 | Loss: 0.00107691
Iteration 4/25 | Loss: 0.00088681
Iteration 5/25 | Loss: 0.00096311
Iteration 6/25 | Loss: 0.00091578
Iteration 7/25 | Loss: 0.00084739
Iteration 8/25 | Loss: 0.00077348
Iteration 9/25 | Loss: 0.00079208
Iteration 10/25 | Loss: 0.00077399
Iteration 11/25 | Loss: 0.00087571
Iteration 12/25 | Loss: 0.00082380
Iteration 13/25 | Loss: 0.00076685
Iteration 14/25 | Loss: 0.00080773
Iteration 15/25 | Loss: 0.00072750
Iteration 16/25 | Loss: 0.00072358
Iteration 17/25 | Loss: 0.00072558
Iteration 18/25 | Loss: 0.00071389
Iteration 19/25 | Loss: 0.00069479
Iteration 20/25 | Loss: 0.00069090
Iteration 21/25 | Loss: 0.00068997
Iteration 22/25 | Loss: 0.00068018
Iteration 23/25 | Loss: 0.00067218
Iteration 24/25 | Loss: 0.00067262
Iteration 25/25 | Loss: 0.00067605

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51830769
Iteration 2/25 | Loss: 0.00090633
Iteration 3/25 | Loss: 0.00090633
Iteration 4/25 | Loss: 0.00090633
Iteration 5/25 | Loss: 0.00090632
Iteration 6/25 | Loss: 0.00090632
Iteration 7/25 | Loss: 0.00090632
Iteration 8/25 | Loss: 0.00090632
Iteration 9/25 | Loss: 0.00090632
Iteration 10/25 | Loss: 0.00090632
Iteration 11/25 | Loss: 0.00090632
Iteration 12/25 | Loss: 0.00090632
Iteration 13/25 | Loss: 0.00090632
Iteration 14/25 | Loss: 0.00090632
Iteration 15/25 | Loss: 0.00090632
Iteration 16/25 | Loss: 0.00090632
Iteration 17/25 | Loss: 0.00090632
Iteration 18/25 | Loss: 0.00090632
Iteration 19/25 | Loss: 0.00090632
Iteration 20/25 | Loss: 0.00090632
Iteration 21/25 | Loss: 0.00090632
Iteration 22/25 | Loss: 0.00090632
Iteration 23/25 | Loss: 0.00090632
Iteration 24/25 | Loss: 0.00090632
Iteration 25/25 | Loss: 0.00090632

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090632
Iteration 2/1000 | Loss: 0.00040385
Iteration 3/1000 | Loss: 0.00061020
Iteration 4/1000 | Loss: 0.00065289
Iteration 5/1000 | Loss: 0.00053373
Iteration 6/1000 | Loss: 0.00034655
Iteration 7/1000 | Loss: 0.00029903
Iteration 8/1000 | Loss: 0.00021168
Iteration 9/1000 | Loss: 0.00019573
Iteration 10/1000 | Loss: 0.00014644
Iteration 11/1000 | Loss: 0.00035431
Iteration 12/1000 | Loss: 0.00025537
Iteration 13/1000 | Loss: 0.00019739
Iteration 14/1000 | Loss: 0.00035842
Iteration 15/1000 | Loss: 0.00046185
Iteration 16/1000 | Loss: 0.00046317
Iteration 17/1000 | Loss: 0.00025771
Iteration 18/1000 | Loss: 0.00011660
Iteration 19/1000 | Loss: 0.00023481
Iteration 20/1000 | Loss: 0.00005866
Iteration 21/1000 | Loss: 0.00012385
Iteration 22/1000 | Loss: 0.00010941
Iteration 23/1000 | Loss: 0.00010020
Iteration 24/1000 | Loss: 0.00006946
Iteration 25/1000 | Loss: 0.00032939
Iteration 26/1000 | Loss: 0.00004656
Iteration 27/1000 | Loss: 0.00004869
Iteration 28/1000 | Loss: 0.00008386
Iteration 29/1000 | Loss: 0.00002647
Iteration 30/1000 | Loss: 0.00002391
Iteration 31/1000 | Loss: 0.00031708
Iteration 32/1000 | Loss: 0.00021208
Iteration 33/1000 | Loss: 0.00014428
Iteration 34/1000 | Loss: 0.00002369
Iteration 35/1000 | Loss: 0.00015818
Iteration 36/1000 | Loss: 0.00034720
Iteration 37/1000 | Loss: 0.00026846
Iteration 38/1000 | Loss: 0.00002148
Iteration 39/1000 | Loss: 0.00002019
Iteration 40/1000 | Loss: 0.00001881
Iteration 41/1000 | Loss: 0.00001678
Iteration 42/1000 | Loss: 0.00001571
Iteration 43/1000 | Loss: 0.00001493
Iteration 44/1000 | Loss: 0.00001445
Iteration 45/1000 | Loss: 0.00001431
Iteration 46/1000 | Loss: 0.00001412
Iteration 47/1000 | Loss: 0.00001406
Iteration 48/1000 | Loss: 0.00001402
Iteration 49/1000 | Loss: 0.00001401
Iteration 50/1000 | Loss: 0.00001401
Iteration 51/1000 | Loss: 0.00001398
Iteration 52/1000 | Loss: 0.00001398
Iteration 53/1000 | Loss: 0.00001396
Iteration 54/1000 | Loss: 0.00001396
Iteration 55/1000 | Loss: 0.00001393
Iteration 56/1000 | Loss: 0.00001390
Iteration 57/1000 | Loss: 0.00001381
Iteration 58/1000 | Loss: 0.00001380
Iteration 59/1000 | Loss: 0.00001377
Iteration 60/1000 | Loss: 0.00001377
Iteration 61/1000 | Loss: 0.00001377
Iteration 62/1000 | Loss: 0.00001376
Iteration 63/1000 | Loss: 0.00001376
Iteration 64/1000 | Loss: 0.00001370
Iteration 65/1000 | Loss: 0.00001368
Iteration 66/1000 | Loss: 0.00002436
Iteration 67/1000 | Loss: 0.00001463
Iteration 68/1000 | Loss: 0.00001367
Iteration 69/1000 | Loss: 0.00001298
Iteration 70/1000 | Loss: 0.00001288
Iteration 71/1000 | Loss: 0.00001279
Iteration 72/1000 | Loss: 0.00001277
Iteration 73/1000 | Loss: 0.00001276
Iteration 74/1000 | Loss: 0.00001276
Iteration 75/1000 | Loss: 0.00001275
Iteration 76/1000 | Loss: 0.00001275
Iteration 77/1000 | Loss: 0.00001275
Iteration 78/1000 | Loss: 0.00001275
Iteration 79/1000 | Loss: 0.00001275
Iteration 80/1000 | Loss: 0.00001275
Iteration 81/1000 | Loss: 0.00001275
Iteration 82/1000 | Loss: 0.00001275
Iteration 83/1000 | Loss: 0.00001275
Iteration 84/1000 | Loss: 0.00001275
Iteration 85/1000 | Loss: 0.00001274
Iteration 86/1000 | Loss: 0.00001273
Iteration 87/1000 | Loss: 0.00001272
Iteration 88/1000 | Loss: 0.00001272
Iteration 89/1000 | Loss: 0.00001270
Iteration 90/1000 | Loss: 0.00001270
Iteration 91/1000 | Loss: 0.00001270
Iteration 92/1000 | Loss: 0.00001269
Iteration 93/1000 | Loss: 0.00001269
Iteration 94/1000 | Loss: 0.00001269
Iteration 95/1000 | Loss: 0.00001269
Iteration 96/1000 | Loss: 0.00001269
Iteration 97/1000 | Loss: 0.00001269
Iteration 98/1000 | Loss: 0.00001269
Iteration 99/1000 | Loss: 0.00001269
Iteration 100/1000 | Loss: 0.00001268
Iteration 101/1000 | Loss: 0.00001268
Iteration 102/1000 | Loss: 0.00001267
Iteration 103/1000 | Loss: 0.00001267
Iteration 104/1000 | Loss: 0.00001266
Iteration 105/1000 | Loss: 0.00001266
Iteration 106/1000 | Loss: 0.00001266
Iteration 107/1000 | Loss: 0.00001266
Iteration 108/1000 | Loss: 0.00001265
Iteration 109/1000 | Loss: 0.00001265
Iteration 110/1000 | Loss: 0.00001264
Iteration 111/1000 | Loss: 0.00001264
Iteration 112/1000 | Loss: 0.00001264
Iteration 113/1000 | Loss: 0.00001264
Iteration 114/1000 | Loss: 0.00001264
Iteration 115/1000 | Loss: 0.00001264
Iteration 116/1000 | Loss: 0.00001264
Iteration 117/1000 | Loss: 0.00001264
Iteration 118/1000 | Loss: 0.00001264
Iteration 119/1000 | Loss: 0.00001264
Iteration 120/1000 | Loss: 0.00001263
Iteration 121/1000 | Loss: 0.00001263
Iteration 122/1000 | Loss: 0.00001263
Iteration 123/1000 | Loss: 0.00001263
Iteration 124/1000 | Loss: 0.00001262
Iteration 125/1000 | Loss: 0.00001261
Iteration 126/1000 | Loss: 0.00001261
Iteration 127/1000 | Loss: 0.00001260
Iteration 128/1000 | Loss: 0.00001260
Iteration 129/1000 | Loss: 0.00001260
Iteration 130/1000 | Loss: 0.00001260
Iteration 131/1000 | Loss: 0.00001259
Iteration 132/1000 | Loss: 0.00001259
Iteration 133/1000 | Loss: 0.00001259
Iteration 134/1000 | Loss: 0.00001259
Iteration 135/1000 | Loss: 0.00001259
Iteration 136/1000 | Loss: 0.00001259
Iteration 137/1000 | Loss: 0.00001259
Iteration 138/1000 | Loss: 0.00001259
Iteration 139/1000 | Loss: 0.00001259
Iteration 140/1000 | Loss: 0.00001258
Iteration 141/1000 | Loss: 0.00001258
Iteration 142/1000 | Loss: 0.00001258
Iteration 143/1000 | Loss: 0.00001258
Iteration 144/1000 | Loss: 0.00001258
Iteration 145/1000 | Loss: 0.00001258
Iteration 146/1000 | Loss: 0.00001257
Iteration 147/1000 | Loss: 0.00001257
Iteration 148/1000 | Loss: 0.00001257
Iteration 149/1000 | Loss: 0.00001257
Iteration 150/1000 | Loss: 0.00001257
Iteration 151/1000 | Loss: 0.00001257
Iteration 152/1000 | Loss: 0.00001257
Iteration 153/1000 | Loss: 0.00001257
Iteration 154/1000 | Loss: 0.00001257
Iteration 155/1000 | Loss: 0.00001257
Iteration 156/1000 | Loss: 0.00001257
Iteration 157/1000 | Loss: 0.00001256
Iteration 158/1000 | Loss: 0.00001256
Iteration 159/1000 | Loss: 0.00001256
Iteration 160/1000 | Loss: 0.00001256
Iteration 161/1000 | Loss: 0.00001256
Iteration 162/1000 | Loss: 0.00001256
Iteration 163/1000 | Loss: 0.00001256
Iteration 164/1000 | Loss: 0.00001256
Iteration 165/1000 | Loss: 0.00001256
Iteration 166/1000 | Loss: 0.00001256
Iteration 167/1000 | Loss: 0.00001256
Iteration 168/1000 | Loss: 0.00001255
Iteration 169/1000 | Loss: 0.00001255
Iteration 170/1000 | Loss: 0.00001255
Iteration 171/1000 | Loss: 0.00001255
Iteration 172/1000 | Loss: 0.00001255
Iteration 173/1000 | Loss: 0.00001255
Iteration 174/1000 | Loss: 0.00001255
Iteration 175/1000 | Loss: 0.00001255
Iteration 176/1000 | Loss: 0.00001255
Iteration 177/1000 | Loss: 0.00001255
Iteration 178/1000 | Loss: 0.00001255
Iteration 179/1000 | Loss: 0.00001255
Iteration 180/1000 | Loss: 0.00001255
Iteration 181/1000 | Loss: 0.00001255
Iteration 182/1000 | Loss: 0.00001255
Iteration 183/1000 | Loss: 0.00001255
Iteration 184/1000 | Loss: 0.00001255
Iteration 185/1000 | Loss: 0.00001255
Iteration 186/1000 | Loss: 0.00001255
Iteration 187/1000 | Loss: 0.00001255
Iteration 188/1000 | Loss: 0.00001255
Iteration 189/1000 | Loss: 0.00001255
Iteration 190/1000 | Loss: 0.00001254
Iteration 191/1000 | Loss: 0.00001254
Iteration 192/1000 | Loss: 0.00001254
Iteration 193/1000 | Loss: 0.00001254
Iteration 194/1000 | Loss: 0.00001254
Iteration 195/1000 | Loss: 0.00001254
Iteration 196/1000 | Loss: 0.00001254
Iteration 197/1000 | Loss: 0.00001254
Iteration 198/1000 | Loss: 0.00001254
Iteration 199/1000 | Loss: 0.00001254
Iteration 200/1000 | Loss: 0.00001254
Iteration 201/1000 | Loss: 0.00001254
Iteration 202/1000 | Loss: 0.00001254
Iteration 203/1000 | Loss: 0.00001254
Iteration 204/1000 | Loss: 0.00001254
Iteration 205/1000 | Loss: 0.00001254
Iteration 206/1000 | Loss: 0.00001254
Iteration 207/1000 | Loss: 0.00001254
Iteration 208/1000 | Loss: 0.00001254
Iteration 209/1000 | Loss: 0.00001254
Iteration 210/1000 | Loss: 0.00001254
Iteration 211/1000 | Loss: 0.00001254
Iteration 212/1000 | Loss: 0.00001254
Iteration 213/1000 | Loss: 0.00001254
Iteration 214/1000 | Loss: 0.00001254
Iteration 215/1000 | Loss: 0.00001254
Iteration 216/1000 | Loss: 0.00001254
Iteration 217/1000 | Loss: 0.00001254
Iteration 218/1000 | Loss: 0.00001254
Iteration 219/1000 | Loss: 0.00001254
Iteration 220/1000 | Loss: 0.00001254
Iteration 221/1000 | Loss: 0.00001254
Iteration 222/1000 | Loss: 0.00001254
Iteration 223/1000 | Loss: 0.00001254
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [1.2541969226731453e-05, 1.2541969226731453e-05, 1.2541969226731453e-05, 1.2541969226731453e-05, 1.2541969226731453e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2541969226731453e-05

Optimization complete. Final v2v error: 2.9439120292663574 mm

Highest mean error: 4.221887588500977 mm for frame 63

Lowest mean error: 2.5154428482055664 mm for frame 3

Saving results

Total time: 127.60048770904541
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00437719
Iteration 2/25 | Loss: 0.00079022
Iteration 3/25 | Loss: 0.00063000
Iteration 4/25 | Loss: 0.00061068
Iteration 5/25 | Loss: 0.00060432
Iteration 6/25 | Loss: 0.00060296
Iteration 7/25 | Loss: 0.00060275
Iteration 8/25 | Loss: 0.00060275
Iteration 9/25 | Loss: 0.00060275
Iteration 10/25 | Loss: 0.00060275
Iteration 11/25 | Loss: 0.00060275
Iteration 12/25 | Loss: 0.00060275
Iteration 13/25 | Loss: 0.00060275
Iteration 14/25 | Loss: 0.00060275
Iteration 15/25 | Loss: 0.00060275
Iteration 16/25 | Loss: 0.00060275
Iteration 17/25 | Loss: 0.00060275
Iteration 18/25 | Loss: 0.00060275
Iteration 19/25 | Loss: 0.00060275
Iteration 20/25 | Loss: 0.00060275
Iteration 21/25 | Loss: 0.00060275
Iteration 22/25 | Loss: 0.00060275
Iteration 23/25 | Loss: 0.00060275
Iteration 24/25 | Loss: 0.00060275
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.000602752435952425, 0.000602752435952425, 0.000602752435952425, 0.000602752435952425, 0.000602752435952425]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000602752435952425

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45732808
Iteration 2/25 | Loss: 0.00028693
Iteration 3/25 | Loss: 0.00028693
Iteration 4/25 | Loss: 0.00028693
Iteration 5/25 | Loss: 0.00028693
Iteration 6/25 | Loss: 0.00028693
Iteration 7/25 | Loss: 0.00028693
Iteration 8/25 | Loss: 0.00028692
Iteration 9/25 | Loss: 0.00028692
Iteration 10/25 | Loss: 0.00028692
Iteration 11/25 | Loss: 0.00028692
Iteration 12/25 | Loss: 0.00028692
Iteration 13/25 | Loss: 0.00028692
Iteration 14/25 | Loss: 0.00028692
Iteration 15/25 | Loss: 0.00028692
Iteration 16/25 | Loss: 0.00028692
Iteration 17/25 | Loss: 0.00028692
Iteration 18/25 | Loss: 0.00028692
Iteration 19/25 | Loss: 0.00028692
Iteration 20/25 | Loss: 0.00028692
Iteration 21/25 | Loss: 0.00028692
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00028692433261312544, 0.00028692433261312544, 0.00028692433261312544, 0.00028692433261312544, 0.00028692433261312544]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00028692433261312544

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028692
Iteration 2/1000 | Loss: 0.00002046
Iteration 3/1000 | Loss: 0.00001495
Iteration 4/1000 | Loss: 0.00001406
Iteration 5/1000 | Loss: 0.00001353
Iteration 6/1000 | Loss: 0.00001323
Iteration 7/1000 | Loss: 0.00001293
Iteration 8/1000 | Loss: 0.00001278
Iteration 9/1000 | Loss: 0.00001276
Iteration 10/1000 | Loss: 0.00001266
Iteration 11/1000 | Loss: 0.00001266
Iteration 12/1000 | Loss: 0.00001265
Iteration 13/1000 | Loss: 0.00001265
Iteration 14/1000 | Loss: 0.00001259
Iteration 15/1000 | Loss: 0.00001258
Iteration 16/1000 | Loss: 0.00001251
Iteration 17/1000 | Loss: 0.00001245
Iteration 18/1000 | Loss: 0.00001244
Iteration 19/1000 | Loss: 0.00001240
Iteration 20/1000 | Loss: 0.00001235
Iteration 21/1000 | Loss: 0.00001231
Iteration 22/1000 | Loss: 0.00001230
Iteration 23/1000 | Loss: 0.00001229
Iteration 24/1000 | Loss: 0.00001226
Iteration 25/1000 | Loss: 0.00001225
Iteration 26/1000 | Loss: 0.00001222
Iteration 27/1000 | Loss: 0.00001222
Iteration 28/1000 | Loss: 0.00001222
Iteration 29/1000 | Loss: 0.00001221
Iteration 30/1000 | Loss: 0.00001221
Iteration 31/1000 | Loss: 0.00001220
Iteration 32/1000 | Loss: 0.00001219
Iteration 33/1000 | Loss: 0.00001219
Iteration 34/1000 | Loss: 0.00001218
Iteration 35/1000 | Loss: 0.00001218
Iteration 36/1000 | Loss: 0.00001218
Iteration 37/1000 | Loss: 0.00001217
Iteration 38/1000 | Loss: 0.00001217
Iteration 39/1000 | Loss: 0.00001216
Iteration 40/1000 | Loss: 0.00001216
Iteration 41/1000 | Loss: 0.00001215
Iteration 42/1000 | Loss: 0.00001215
Iteration 43/1000 | Loss: 0.00001215
Iteration 44/1000 | Loss: 0.00001214
Iteration 45/1000 | Loss: 0.00001214
Iteration 46/1000 | Loss: 0.00001214
Iteration 47/1000 | Loss: 0.00001214
Iteration 48/1000 | Loss: 0.00001211
Iteration 49/1000 | Loss: 0.00001211
Iteration 50/1000 | Loss: 0.00001210
Iteration 51/1000 | Loss: 0.00001209
Iteration 52/1000 | Loss: 0.00001209
Iteration 53/1000 | Loss: 0.00001209
Iteration 54/1000 | Loss: 0.00001208
Iteration 55/1000 | Loss: 0.00001208
Iteration 56/1000 | Loss: 0.00001208
Iteration 57/1000 | Loss: 0.00001208
Iteration 58/1000 | Loss: 0.00001208
Iteration 59/1000 | Loss: 0.00001208
Iteration 60/1000 | Loss: 0.00001207
Iteration 61/1000 | Loss: 0.00001207
Iteration 62/1000 | Loss: 0.00001207
Iteration 63/1000 | Loss: 0.00001207
Iteration 64/1000 | Loss: 0.00001207
Iteration 65/1000 | Loss: 0.00001207
Iteration 66/1000 | Loss: 0.00001207
Iteration 67/1000 | Loss: 0.00001206
Iteration 68/1000 | Loss: 0.00001206
Iteration 69/1000 | Loss: 0.00001206
Iteration 70/1000 | Loss: 0.00001206
Iteration 71/1000 | Loss: 0.00001206
Iteration 72/1000 | Loss: 0.00001206
Iteration 73/1000 | Loss: 0.00001206
Iteration 74/1000 | Loss: 0.00001206
Iteration 75/1000 | Loss: 0.00001205
Iteration 76/1000 | Loss: 0.00001205
Iteration 77/1000 | Loss: 0.00001205
Iteration 78/1000 | Loss: 0.00001205
Iteration 79/1000 | Loss: 0.00001204
Iteration 80/1000 | Loss: 0.00001204
Iteration 81/1000 | Loss: 0.00001204
Iteration 82/1000 | Loss: 0.00001204
Iteration 83/1000 | Loss: 0.00001204
Iteration 84/1000 | Loss: 0.00001204
Iteration 85/1000 | Loss: 0.00001203
Iteration 86/1000 | Loss: 0.00001203
Iteration 87/1000 | Loss: 0.00001203
Iteration 88/1000 | Loss: 0.00001203
Iteration 89/1000 | Loss: 0.00001203
Iteration 90/1000 | Loss: 0.00001203
Iteration 91/1000 | Loss: 0.00001203
Iteration 92/1000 | Loss: 0.00001203
Iteration 93/1000 | Loss: 0.00001203
Iteration 94/1000 | Loss: 0.00001203
Iteration 95/1000 | Loss: 0.00001203
Iteration 96/1000 | Loss: 0.00001203
Iteration 97/1000 | Loss: 0.00001203
Iteration 98/1000 | Loss: 0.00001203
Iteration 99/1000 | Loss: 0.00001203
Iteration 100/1000 | Loss: 0.00001203
Iteration 101/1000 | Loss: 0.00001203
Iteration 102/1000 | Loss: 0.00001203
Iteration 103/1000 | Loss: 0.00001203
Iteration 104/1000 | Loss: 0.00001203
Iteration 105/1000 | Loss: 0.00001203
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [1.202776184072718e-05, 1.202776184072718e-05, 1.202776184072718e-05, 1.202776184072718e-05, 1.202776184072718e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.202776184072718e-05

Optimization complete. Final v2v error: 2.9308648109436035 mm

Highest mean error: 3.3630144596099854 mm for frame 221

Lowest mean error: 2.7369017601013184 mm for frame 168

Saving results

Total time: 37.49742579460144
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00861533
Iteration 2/25 | Loss: 0.00142828
Iteration 3/25 | Loss: 0.00113393
Iteration 4/25 | Loss: 0.00101783
Iteration 5/25 | Loss: 0.00099192
Iteration 6/25 | Loss: 0.00100031
Iteration 7/25 | Loss: 0.00104941
Iteration 8/25 | Loss: 0.00098153
Iteration 9/25 | Loss: 0.00090043
Iteration 10/25 | Loss: 0.00086056
Iteration 11/25 | Loss: 0.00083172
Iteration 12/25 | Loss: 0.00081396
Iteration 13/25 | Loss: 0.00080102
Iteration 14/25 | Loss: 0.00079924
Iteration 15/25 | Loss: 0.00079924
Iteration 16/25 | Loss: 0.00079096
Iteration 17/25 | Loss: 0.00078881
Iteration 18/25 | Loss: 0.00081277
Iteration 19/25 | Loss: 0.00081706
Iteration 20/25 | Loss: 0.00080781
Iteration 21/25 | Loss: 0.00081285
Iteration 22/25 | Loss: 0.00081604
Iteration 23/25 | Loss: 0.00082058
Iteration 24/25 | Loss: 0.00083046
Iteration 25/25 | Loss: 0.00084008

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46910799
Iteration 2/25 | Loss: 0.00170161
Iteration 3/25 | Loss: 0.00170157
Iteration 4/25 | Loss: 0.00170157
Iteration 5/25 | Loss: 0.00170157
Iteration 6/25 | Loss: 0.00170157
Iteration 7/25 | Loss: 0.00170157
Iteration 8/25 | Loss: 0.00170157
Iteration 9/25 | Loss: 0.00170157
Iteration 10/25 | Loss: 0.00170157
Iteration 11/25 | Loss: 0.00170157
Iteration 12/25 | Loss: 0.00170157
Iteration 13/25 | Loss: 0.00170157
Iteration 14/25 | Loss: 0.00170157
Iteration 15/25 | Loss: 0.00170157
Iteration 16/25 | Loss: 0.00170157
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0017015674384310842, 0.0017015674384310842, 0.0017015674384310842, 0.0017015674384310842, 0.0017015674384310842]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017015674384310842

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00170157
Iteration 2/1000 | Loss: 0.00112622
Iteration 3/1000 | Loss: 0.00042489
Iteration 4/1000 | Loss: 0.00028435
Iteration 5/1000 | Loss: 0.00054170
Iteration 6/1000 | Loss: 0.00040651
Iteration 7/1000 | Loss: 0.00034306
Iteration 8/1000 | Loss: 0.00037480
Iteration 9/1000 | Loss: 0.00029726
Iteration 10/1000 | Loss: 0.00046074
Iteration 11/1000 | Loss: 0.00022823
Iteration 12/1000 | Loss: 0.00018051
Iteration 13/1000 | Loss: 0.00044773
Iteration 14/1000 | Loss: 0.00027224
Iteration 15/1000 | Loss: 0.00021929
Iteration 16/1000 | Loss: 0.00122712
Iteration 17/1000 | Loss: 0.00053913
Iteration 18/1000 | Loss: 0.00033821
Iteration 19/1000 | Loss: 0.00025639
Iteration 20/1000 | Loss: 0.00038406
Iteration 21/1000 | Loss: 0.00024630
Iteration 22/1000 | Loss: 0.00020464
Iteration 23/1000 | Loss: 0.00033933
Iteration 24/1000 | Loss: 0.00028156
Iteration 25/1000 | Loss: 0.00029185
Iteration 26/1000 | Loss: 0.00029292
Iteration 27/1000 | Loss: 0.00021809
Iteration 28/1000 | Loss: 0.00027036
Iteration 29/1000 | Loss: 0.00038617
Iteration 30/1000 | Loss: 0.00040816
Iteration 31/1000 | Loss: 0.00037613
Iteration 32/1000 | Loss: 0.00025617
Iteration 33/1000 | Loss: 0.00027868
Iteration 34/1000 | Loss: 0.00028763
Iteration 35/1000 | Loss: 0.00027569
Iteration 36/1000 | Loss: 0.00027662
Iteration 37/1000 | Loss: 0.00027224
Iteration 38/1000 | Loss: 0.00025514
Iteration 39/1000 | Loss: 0.00022962
Iteration 40/1000 | Loss: 0.00018307
Iteration 41/1000 | Loss: 0.00027780
Iteration 42/1000 | Loss: 0.00028128
Iteration 43/1000 | Loss: 0.00025616
Iteration 44/1000 | Loss: 0.00023496
Iteration 45/1000 | Loss: 0.00018123
Iteration 46/1000 | Loss: 0.00020016
Iteration 47/1000 | Loss: 0.00020075
Iteration 48/1000 | Loss: 0.00021701
Iteration 49/1000 | Loss: 0.00019637
Iteration 50/1000 | Loss: 0.00019978
Iteration 51/1000 | Loss: 0.00018540
Iteration 52/1000 | Loss: 0.00021359
Iteration 53/1000 | Loss: 0.00018882
Iteration 54/1000 | Loss: 0.00123814
Iteration 55/1000 | Loss: 0.00148612
Iteration 56/1000 | Loss: 0.00166878
Iteration 57/1000 | Loss: 0.00154717
Iteration 58/1000 | Loss: 0.00098039
Iteration 59/1000 | Loss: 0.00147007
Iteration 60/1000 | Loss: 0.00080917
Iteration 61/1000 | Loss: 0.00068936
Iteration 62/1000 | Loss: 0.00052587
Iteration 63/1000 | Loss: 0.00113414
Iteration 64/1000 | Loss: 0.00048882
Iteration 65/1000 | Loss: 0.00024194
Iteration 66/1000 | Loss: 0.00021109
Iteration 67/1000 | Loss: 0.00043905
Iteration 68/1000 | Loss: 0.00014687
Iteration 69/1000 | Loss: 0.00016165
Iteration 70/1000 | Loss: 0.00016237
Iteration 71/1000 | Loss: 0.00014373
Iteration 72/1000 | Loss: 0.00012377
Iteration 73/1000 | Loss: 0.00018690
Iteration 74/1000 | Loss: 0.00021962
Iteration 75/1000 | Loss: 0.00025594
Iteration 76/1000 | Loss: 0.00005904
Iteration 77/1000 | Loss: 0.00005375
Iteration 78/1000 | Loss: 0.00006799
Iteration 79/1000 | Loss: 0.00005255
Iteration 80/1000 | Loss: 0.00005236
Iteration 81/1000 | Loss: 0.00054414
Iteration 82/1000 | Loss: 0.00062933
Iteration 83/1000 | Loss: 0.00006392
Iteration 84/1000 | Loss: 0.00006946
Iteration 85/1000 | Loss: 0.00005928
Iteration 86/1000 | Loss: 0.00006356
Iteration 87/1000 | Loss: 0.00039591
Iteration 88/1000 | Loss: 0.00005957
Iteration 89/1000 | Loss: 0.00006306
Iteration 90/1000 | Loss: 0.00005692
Iteration 91/1000 | Loss: 0.00005435
Iteration 92/1000 | Loss: 0.00005307
Iteration 93/1000 | Loss: 0.00005275
Iteration 94/1000 | Loss: 0.00005923
Iteration 95/1000 | Loss: 0.00004577
Iteration 96/1000 | Loss: 0.00005211
Iteration 97/1000 | Loss: 0.00004069
Iteration 98/1000 | Loss: 0.00005420
Iteration 99/1000 | Loss: 0.00004402
Iteration 100/1000 | Loss: 0.00012681
Iteration 101/1000 | Loss: 0.00010399
Iteration 102/1000 | Loss: 0.00004435
Iteration 103/1000 | Loss: 0.00005122
Iteration 104/1000 | Loss: 0.00003718
Iteration 105/1000 | Loss: 0.00003370
Iteration 106/1000 | Loss: 0.00004413
Iteration 107/1000 | Loss: 0.00004903
Iteration 108/1000 | Loss: 0.00003820
Iteration 109/1000 | Loss: 0.00018713
Iteration 110/1000 | Loss: 0.00010231
Iteration 111/1000 | Loss: 0.00010376
Iteration 112/1000 | Loss: 0.00005266
Iteration 113/1000 | Loss: 0.00005426
Iteration 114/1000 | Loss: 0.00006425
Iteration 115/1000 | Loss: 0.00004590
Iteration 116/1000 | Loss: 0.00012795
Iteration 117/1000 | Loss: 0.00003491
Iteration 118/1000 | Loss: 0.00002867
Iteration 119/1000 | Loss: 0.00003338
Iteration 120/1000 | Loss: 0.00005348
Iteration 121/1000 | Loss: 0.00014049
Iteration 122/1000 | Loss: 0.00004765
Iteration 123/1000 | Loss: 0.00004899
Iteration 124/1000 | Loss: 0.00005465
Iteration 125/1000 | Loss: 0.00023531
Iteration 126/1000 | Loss: 0.00004049
Iteration 127/1000 | Loss: 0.00015286
Iteration 128/1000 | Loss: 0.00012337
Iteration 129/1000 | Loss: 0.00004381
Iteration 130/1000 | Loss: 0.00003866
Iteration 131/1000 | Loss: 0.00003066
Iteration 132/1000 | Loss: 0.00002972
Iteration 133/1000 | Loss: 0.00003163
Iteration 134/1000 | Loss: 0.00003689
Iteration 135/1000 | Loss: 0.00004227
Iteration 136/1000 | Loss: 0.00003449
Iteration 137/1000 | Loss: 0.00003389
Iteration 138/1000 | Loss: 0.00004068
Iteration 139/1000 | Loss: 0.00003028
Iteration 140/1000 | Loss: 0.00005238
Iteration 141/1000 | Loss: 0.00004117
Iteration 142/1000 | Loss: 0.00020807
Iteration 143/1000 | Loss: 0.00005427
Iteration 144/1000 | Loss: 0.00004678
Iteration 145/1000 | Loss: 0.00003172
Iteration 146/1000 | Loss: 0.00002914
Iteration 147/1000 | Loss: 0.00004169
Iteration 148/1000 | Loss: 0.00003399
Iteration 149/1000 | Loss: 0.00002708
Iteration 150/1000 | Loss: 0.00003819
Iteration 151/1000 | Loss: 0.00003622
Iteration 152/1000 | Loss: 0.00003800
Iteration 153/1000 | Loss: 0.00003611
Iteration 154/1000 | Loss: 0.00003738
Iteration 155/1000 | Loss: 0.00003702
Iteration 156/1000 | Loss: 0.00003523
Iteration 157/1000 | Loss: 0.00003459
Iteration 158/1000 | Loss: 0.00003656
Iteration 159/1000 | Loss: 0.00003436
Iteration 160/1000 | Loss: 0.00004136
Iteration 161/1000 | Loss: 0.00003331
Iteration 162/1000 | Loss: 0.00004360
Iteration 163/1000 | Loss: 0.00003139
Iteration 164/1000 | Loss: 0.00003389
Iteration 165/1000 | Loss: 0.00003793
Iteration 166/1000 | Loss: 0.00003088
Iteration 167/1000 | Loss: 0.00004207
Iteration 168/1000 | Loss: 0.00003086
Iteration 169/1000 | Loss: 0.00004077
Iteration 170/1000 | Loss: 0.00003830
Iteration 171/1000 | Loss: 0.00003168
Iteration 172/1000 | Loss: 0.00004123
Iteration 173/1000 | Loss: 0.00003129
Iteration 174/1000 | Loss: 0.00004300
Iteration 175/1000 | Loss: 0.00003127
Iteration 176/1000 | Loss: 0.00003962
Iteration 177/1000 | Loss: 0.00003093
Iteration 178/1000 | Loss: 0.00005264
Iteration 179/1000 | Loss: 0.00003394
Iteration 180/1000 | Loss: 0.00004111
Iteration 181/1000 | Loss: 0.00004214
Iteration 182/1000 | Loss: 0.00002881
Iteration 183/1000 | Loss: 0.00002657
Iteration 184/1000 | Loss: 0.00002564
Iteration 185/1000 | Loss: 0.00002512
Iteration 186/1000 | Loss: 0.00002448
Iteration 187/1000 | Loss: 0.00002395
Iteration 188/1000 | Loss: 0.00002367
Iteration 189/1000 | Loss: 0.00002333
Iteration 190/1000 | Loss: 0.00002316
Iteration 191/1000 | Loss: 0.00002301
Iteration 192/1000 | Loss: 0.00002294
Iteration 193/1000 | Loss: 0.00002293
Iteration 194/1000 | Loss: 0.00002293
Iteration 195/1000 | Loss: 0.00002292
Iteration 196/1000 | Loss: 0.00002292
Iteration 197/1000 | Loss: 0.00002292
Iteration 198/1000 | Loss: 0.00002291
Iteration 199/1000 | Loss: 0.00002291
Iteration 200/1000 | Loss: 0.00002290
Iteration 201/1000 | Loss: 0.00002289
Iteration 202/1000 | Loss: 0.00002289
Iteration 203/1000 | Loss: 0.00002289
Iteration 204/1000 | Loss: 0.00002288
Iteration 205/1000 | Loss: 0.00002288
Iteration 206/1000 | Loss: 0.00002288
Iteration 207/1000 | Loss: 0.00002286
Iteration 208/1000 | Loss: 0.00002286
Iteration 209/1000 | Loss: 0.00002286
Iteration 210/1000 | Loss: 0.00002286
Iteration 211/1000 | Loss: 0.00002285
Iteration 212/1000 | Loss: 0.00002285
Iteration 213/1000 | Loss: 0.00002285
Iteration 214/1000 | Loss: 0.00002285
Iteration 215/1000 | Loss: 0.00002285
Iteration 216/1000 | Loss: 0.00002285
Iteration 217/1000 | Loss: 0.00002285
Iteration 218/1000 | Loss: 0.00002285
Iteration 219/1000 | Loss: 0.00002285
Iteration 220/1000 | Loss: 0.00002284
Iteration 221/1000 | Loss: 0.00002284
Iteration 222/1000 | Loss: 0.00002284
Iteration 223/1000 | Loss: 0.00002284
Iteration 224/1000 | Loss: 0.00002284
Iteration 225/1000 | Loss: 0.00002284
Iteration 226/1000 | Loss: 0.00002284
Iteration 227/1000 | Loss: 0.00002283
Iteration 228/1000 | Loss: 0.00002283
Iteration 229/1000 | Loss: 0.00002283
Iteration 230/1000 | Loss: 0.00002283
Iteration 231/1000 | Loss: 0.00002282
Iteration 232/1000 | Loss: 0.00002282
Iteration 233/1000 | Loss: 0.00002282
Iteration 234/1000 | Loss: 0.00002282
Iteration 235/1000 | Loss: 0.00002282
Iteration 236/1000 | Loss: 0.00002282
Iteration 237/1000 | Loss: 0.00002282
Iteration 238/1000 | Loss: 0.00002282
Iteration 239/1000 | Loss: 0.00002282
Iteration 240/1000 | Loss: 0.00002282
Iteration 241/1000 | Loss: 0.00002282
Iteration 242/1000 | Loss: 0.00002282
Iteration 243/1000 | Loss: 0.00002281
Iteration 244/1000 | Loss: 0.00002281
Iteration 245/1000 | Loss: 0.00002281
Iteration 246/1000 | Loss: 0.00002281
Iteration 247/1000 | Loss: 0.00002281
Iteration 248/1000 | Loss: 0.00002281
Iteration 249/1000 | Loss: 0.00002281
Iteration 250/1000 | Loss: 0.00002281
Iteration 251/1000 | Loss: 0.00002281
Iteration 252/1000 | Loss: 0.00002281
Iteration 253/1000 | Loss: 0.00002280
Iteration 254/1000 | Loss: 0.00002280
Iteration 255/1000 | Loss: 0.00002280
Iteration 256/1000 | Loss: 0.00002280
Iteration 257/1000 | Loss: 0.00002280
Iteration 258/1000 | Loss: 0.00002280
Iteration 259/1000 | Loss: 0.00002280
Iteration 260/1000 | Loss: 0.00002280
Iteration 261/1000 | Loss: 0.00002280
Iteration 262/1000 | Loss: 0.00002280
Iteration 263/1000 | Loss: 0.00002280
Iteration 264/1000 | Loss: 0.00002280
Iteration 265/1000 | Loss: 0.00002280
Iteration 266/1000 | Loss: 0.00002280
Iteration 267/1000 | Loss: 0.00002280
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 267. Stopping optimization.
Last 5 losses: [2.2799680664320476e-05, 2.2799680664320476e-05, 2.2799680664320476e-05, 2.2799680664320476e-05, 2.2799680664320476e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2799680664320476e-05

Optimization complete. Final v2v error: 3.9112279415130615 mm

Highest mean error: 7.1930460929870605 mm for frame 100

Lowest mean error: 3.4867687225341797 mm for frame 60

Saving results

Total time: 361.17227935791016
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00846500
Iteration 2/25 | Loss: 0.00119824
Iteration 3/25 | Loss: 0.00084850
Iteration 4/25 | Loss: 0.00077025
Iteration 5/25 | Loss: 0.00074214
Iteration 6/25 | Loss: 0.00073509
Iteration 7/25 | Loss: 0.00073297
Iteration 8/25 | Loss: 0.00073230
Iteration 9/25 | Loss: 0.00073213
Iteration 10/25 | Loss: 0.00073208
Iteration 11/25 | Loss: 0.00073208
Iteration 12/25 | Loss: 0.00073208
Iteration 13/25 | Loss: 0.00073208
Iteration 14/25 | Loss: 0.00073208
Iteration 15/25 | Loss: 0.00073208
Iteration 16/25 | Loss: 0.00073208
Iteration 17/25 | Loss: 0.00073208
Iteration 18/25 | Loss: 0.00073208
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007320806616917253, 0.0007320806616917253, 0.0007320806616917253, 0.0007320806616917253, 0.0007320806616917253]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007320806616917253

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.73272371
Iteration 2/25 | Loss: 0.00033617
Iteration 3/25 | Loss: 0.00033617
Iteration 4/25 | Loss: 0.00033617
Iteration 5/25 | Loss: 0.00033617
Iteration 6/25 | Loss: 0.00033617
Iteration 7/25 | Loss: 0.00033617
Iteration 8/25 | Loss: 0.00033617
Iteration 9/25 | Loss: 0.00033617
Iteration 10/25 | Loss: 0.00033617
Iteration 11/25 | Loss: 0.00033617
Iteration 12/25 | Loss: 0.00033617
Iteration 13/25 | Loss: 0.00033617
Iteration 14/25 | Loss: 0.00033617
Iteration 15/25 | Loss: 0.00033617
Iteration 16/25 | Loss: 0.00033616
Iteration 17/25 | Loss: 0.00033616
Iteration 18/25 | Loss: 0.00033616
Iteration 19/25 | Loss: 0.00033616
Iteration 20/25 | Loss: 0.00033617
Iteration 21/25 | Loss: 0.00033617
Iteration 22/25 | Loss: 0.00033617
Iteration 23/25 | Loss: 0.00033617
Iteration 24/25 | Loss: 0.00033617
Iteration 25/25 | Loss: 0.00033617
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0003361650160513818, 0.0003361650160513818, 0.0003361650160513818, 0.0003361650160513818, 0.0003361650160513818]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003361650160513818

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033617
Iteration 2/1000 | Loss: 0.00005198
Iteration 3/1000 | Loss: 0.00003772
Iteration 4/1000 | Loss: 0.00003181
Iteration 5/1000 | Loss: 0.00002975
Iteration 6/1000 | Loss: 0.00002849
Iteration 7/1000 | Loss: 0.00002747
Iteration 8/1000 | Loss: 0.00002681
Iteration 9/1000 | Loss: 0.00002628
Iteration 10/1000 | Loss: 0.00002585
Iteration 11/1000 | Loss: 0.00002555
Iteration 12/1000 | Loss: 0.00002536
Iteration 13/1000 | Loss: 0.00002518
Iteration 14/1000 | Loss: 0.00002509
Iteration 15/1000 | Loss: 0.00002508
Iteration 16/1000 | Loss: 0.00002507
Iteration 17/1000 | Loss: 0.00002507
Iteration 18/1000 | Loss: 0.00002506
Iteration 19/1000 | Loss: 0.00002505
Iteration 20/1000 | Loss: 0.00002502
Iteration 21/1000 | Loss: 0.00002499
Iteration 22/1000 | Loss: 0.00002498
Iteration 23/1000 | Loss: 0.00002497
Iteration 24/1000 | Loss: 0.00002496
Iteration 25/1000 | Loss: 0.00002493
Iteration 26/1000 | Loss: 0.00002488
Iteration 27/1000 | Loss: 0.00002488
Iteration 28/1000 | Loss: 0.00002484
Iteration 29/1000 | Loss: 0.00002484
Iteration 30/1000 | Loss: 0.00002483
Iteration 31/1000 | Loss: 0.00002483
Iteration 32/1000 | Loss: 0.00002482
Iteration 33/1000 | Loss: 0.00002482
Iteration 34/1000 | Loss: 0.00002482
Iteration 35/1000 | Loss: 0.00002481
Iteration 36/1000 | Loss: 0.00002481
Iteration 37/1000 | Loss: 0.00002481
Iteration 38/1000 | Loss: 0.00002480
Iteration 39/1000 | Loss: 0.00002479
Iteration 40/1000 | Loss: 0.00002479
Iteration 41/1000 | Loss: 0.00002478
Iteration 42/1000 | Loss: 0.00002478
Iteration 43/1000 | Loss: 0.00002478
Iteration 44/1000 | Loss: 0.00002477
Iteration 45/1000 | Loss: 0.00002477
Iteration 46/1000 | Loss: 0.00002477
Iteration 47/1000 | Loss: 0.00002476
Iteration 48/1000 | Loss: 0.00002476
Iteration 49/1000 | Loss: 0.00002476
Iteration 50/1000 | Loss: 0.00002475
Iteration 51/1000 | Loss: 0.00002475
Iteration 52/1000 | Loss: 0.00002475
Iteration 53/1000 | Loss: 0.00002474
Iteration 54/1000 | Loss: 0.00002474
Iteration 55/1000 | Loss: 0.00002474
Iteration 56/1000 | Loss: 0.00002474
Iteration 57/1000 | Loss: 0.00002474
Iteration 58/1000 | Loss: 0.00002473
Iteration 59/1000 | Loss: 0.00002473
Iteration 60/1000 | Loss: 0.00002473
Iteration 61/1000 | Loss: 0.00002472
Iteration 62/1000 | Loss: 0.00002472
Iteration 63/1000 | Loss: 0.00002472
Iteration 64/1000 | Loss: 0.00002471
Iteration 65/1000 | Loss: 0.00002471
Iteration 66/1000 | Loss: 0.00002471
Iteration 67/1000 | Loss: 0.00002471
Iteration 68/1000 | Loss: 0.00002471
Iteration 69/1000 | Loss: 0.00002471
Iteration 70/1000 | Loss: 0.00002471
Iteration 71/1000 | Loss: 0.00002471
Iteration 72/1000 | Loss: 0.00002471
Iteration 73/1000 | Loss: 0.00002471
Iteration 74/1000 | Loss: 0.00002470
Iteration 75/1000 | Loss: 0.00002470
Iteration 76/1000 | Loss: 0.00002470
Iteration 77/1000 | Loss: 0.00002470
Iteration 78/1000 | Loss: 0.00002469
Iteration 79/1000 | Loss: 0.00002469
Iteration 80/1000 | Loss: 0.00002469
Iteration 81/1000 | Loss: 0.00002469
Iteration 82/1000 | Loss: 0.00002469
Iteration 83/1000 | Loss: 0.00002469
Iteration 84/1000 | Loss: 0.00002469
Iteration 85/1000 | Loss: 0.00002468
Iteration 86/1000 | Loss: 0.00002468
Iteration 87/1000 | Loss: 0.00002468
Iteration 88/1000 | Loss: 0.00002468
Iteration 89/1000 | Loss: 0.00002468
Iteration 90/1000 | Loss: 0.00002468
Iteration 91/1000 | Loss: 0.00002468
Iteration 92/1000 | Loss: 0.00002468
Iteration 93/1000 | Loss: 0.00002468
Iteration 94/1000 | Loss: 0.00002467
Iteration 95/1000 | Loss: 0.00002467
Iteration 96/1000 | Loss: 0.00002467
Iteration 97/1000 | Loss: 0.00002467
Iteration 98/1000 | Loss: 0.00002467
Iteration 99/1000 | Loss: 0.00002467
Iteration 100/1000 | Loss: 0.00002467
Iteration 101/1000 | Loss: 0.00002467
Iteration 102/1000 | Loss: 0.00002467
Iteration 103/1000 | Loss: 0.00002466
Iteration 104/1000 | Loss: 0.00002466
Iteration 105/1000 | Loss: 0.00002466
Iteration 106/1000 | Loss: 0.00002466
Iteration 107/1000 | Loss: 0.00002466
Iteration 108/1000 | Loss: 0.00002466
Iteration 109/1000 | Loss: 0.00002465
Iteration 110/1000 | Loss: 0.00002465
Iteration 111/1000 | Loss: 0.00002465
Iteration 112/1000 | Loss: 0.00002465
Iteration 113/1000 | Loss: 0.00002465
Iteration 114/1000 | Loss: 0.00002465
Iteration 115/1000 | Loss: 0.00002465
Iteration 116/1000 | Loss: 0.00002465
Iteration 117/1000 | Loss: 0.00002465
Iteration 118/1000 | Loss: 0.00002464
Iteration 119/1000 | Loss: 0.00002464
Iteration 120/1000 | Loss: 0.00002464
Iteration 121/1000 | Loss: 0.00002464
Iteration 122/1000 | Loss: 0.00002464
Iteration 123/1000 | Loss: 0.00002464
Iteration 124/1000 | Loss: 0.00002463
Iteration 125/1000 | Loss: 0.00002463
Iteration 126/1000 | Loss: 0.00002463
Iteration 127/1000 | Loss: 0.00002463
Iteration 128/1000 | Loss: 0.00002463
Iteration 129/1000 | Loss: 0.00002463
Iteration 130/1000 | Loss: 0.00002463
Iteration 131/1000 | Loss: 0.00002463
Iteration 132/1000 | Loss: 0.00002463
Iteration 133/1000 | Loss: 0.00002463
Iteration 134/1000 | Loss: 0.00002463
Iteration 135/1000 | Loss: 0.00002463
Iteration 136/1000 | Loss: 0.00002463
Iteration 137/1000 | Loss: 0.00002463
Iteration 138/1000 | Loss: 0.00002463
Iteration 139/1000 | Loss: 0.00002463
Iteration 140/1000 | Loss: 0.00002463
Iteration 141/1000 | Loss: 0.00002463
Iteration 142/1000 | Loss: 0.00002463
Iteration 143/1000 | Loss: 0.00002463
Iteration 144/1000 | Loss: 0.00002463
Iteration 145/1000 | Loss: 0.00002463
Iteration 146/1000 | Loss: 0.00002463
Iteration 147/1000 | Loss: 0.00002463
Iteration 148/1000 | Loss: 0.00002463
Iteration 149/1000 | Loss: 0.00002463
Iteration 150/1000 | Loss: 0.00002463
Iteration 151/1000 | Loss: 0.00002463
Iteration 152/1000 | Loss: 0.00002463
Iteration 153/1000 | Loss: 0.00002463
Iteration 154/1000 | Loss: 0.00002463
Iteration 155/1000 | Loss: 0.00002463
Iteration 156/1000 | Loss: 0.00002463
Iteration 157/1000 | Loss: 0.00002463
Iteration 158/1000 | Loss: 0.00002463
Iteration 159/1000 | Loss: 0.00002463
Iteration 160/1000 | Loss: 0.00002463
Iteration 161/1000 | Loss: 0.00002463
Iteration 162/1000 | Loss: 0.00002463
Iteration 163/1000 | Loss: 0.00002463
Iteration 164/1000 | Loss: 0.00002463
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [2.462721568008419e-05, 2.462721568008419e-05, 2.462721568008419e-05, 2.462721568008419e-05, 2.462721568008419e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.462721568008419e-05

Optimization complete. Final v2v error: 4.1583051681518555 mm

Highest mean error: 5.975607395172119 mm for frame 95

Lowest mean error: 2.985877513885498 mm for frame 27

Saving results

Total time: 44.89790177345276
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00476406
Iteration 2/25 | Loss: 0.00076606
Iteration 3/25 | Loss: 0.00062768
Iteration 4/25 | Loss: 0.00060554
Iteration 5/25 | Loss: 0.00059900
Iteration 6/25 | Loss: 0.00059737
Iteration 7/25 | Loss: 0.00059696
Iteration 8/25 | Loss: 0.00059696
Iteration 9/25 | Loss: 0.00059696
Iteration 10/25 | Loss: 0.00059696
Iteration 11/25 | Loss: 0.00059696
Iteration 12/25 | Loss: 0.00059696
Iteration 13/25 | Loss: 0.00059696
Iteration 14/25 | Loss: 0.00059696
Iteration 15/25 | Loss: 0.00059696
Iteration 16/25 | Loss: 0.00059696
Iteration 17/25 | Loss: 0.00059696
Iteration 18/25 | Loss: 0.00059696
Iteration 19/25 | Loss: 0.00059696
Iteration 20/25 | Loss: 0.00059696
Iteration 21/25 | Loss: 0.00059696
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005969643825665116, 0.0005969643825665116, 0.0005969643825665116, 0.0005969643825665116, 0.0005969643825665116]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005969643825665116

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.17443943
Iteration 2/25 | Loss: 0.00027028
Iteration 3/25 | Loss: 0.00027027
Iteration 4/25 | Loss: 0.00027027
Iteration 5/25 | Loss: 0.00027027
Iteration 6/25 | Loss: 0.00027027
Iteration 7/25 | Loss: 0.00027027
Iteration 8/25 | Loss: 0.00027027
Iteration 9/25 | Loss: 0.00027027
Iteration 10/25 | Loss: 0.00027027
Iteration 11/25 | Loss: 0.00027027
Iteration 12/25 | Loss: 0.00027027
Iteration 13/25 | Loss: 0.00027027
Iteration 14/25 | Loss: 0.00027027
Iteration 15/25 | Loss: 0.00027027
Iteration 16/25 | Loss: 0.00027027
Iteration 17/25 | Loss: 0.00027027
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00027026893803849816, 0.00027026893803849816, 0.00027026893803849816, 0.00027026893803849816, 0.00027026893803849816]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00027026893803849816

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027027
Iteration 2/1000 | Loss: 0.00002423
Iteration 3/1000 | Loss: 0.00001605
Iteration 4/1000 | Loss: 0.00001494
Iteration 5/1000 | Loss: 0.00001418
Iteration 6/1000 | Loss: 0.00001381
Iteration 7/1000 | Loss: 0.00001342
Iteration 8/1000 | Loss: 0.00001321
Iteration 9/1000 | Loss: 0.00001321
Iteration 10/1000 | Loss: 0.00001315
Iteration 11/1000 | Loss: 0.00001314
Iteration 12/1000 | Loss: 0.00001314
Iteration 13/1000 | Loss: 0.00001313
Iteration 14/1000 | Loss: 0.00001312
Iteration 15/1000 | Loss: 0.00001305
Iteration 16/1000 | Loss: 0.00001299
Iteration 17/1000 | Loss: 0.00001298
Iteration 18/1000 | Loss: 0.00001297
Iteration 19/1000 | Loss: 0.00001294
Iteration 20/1000 | Loss: 0.00001292
Iteration 21/1000 | Loss: 0.00001288
Iteration 22/1000 | Loss: 0.00001286
Iteration 23/1000 | Loss: 0.00001282
Iteration 24/1000 | Loss: 0.00001279
Iteration 25/1000 | Loss: 0.00001278
Iteration 26/1000 | Loss: 0.00001273
Iteration 27/1000 | Loss: 0.00001273
Iteration 28/1000 | Loss: 0.00001270
Iteration 29/1000 | Loss: 0.00001270
Iteration 30/1000 | Loss: 0.00001270
Iteration 31/1000 | Loss: 0.00001269
Iteration 32/1000 | Loss: 0.00001269
Iteration 33/1000 | Loss: 0.00001269
Iteration 34/1000 | Loss: 0.00001269
Iteration 35/1000 | Loss: 0.00001269
Iteration 36/1000 | Loss: 0.00001269
Iteration 37/1000 | Loss: 0.00001269
Iteration 38/1000 | Loss: 0.00001269
Iteration 39/1000 | Loss: 0.00001268
Iteration 40/1000 | Loss: 0.00001268
Iteration 41/1000 | Loss: 0.00001268
Iteration 42/1000 | Loss: 0.00001268
Iteration 43/1000 | Loss: 0.00001268
Iteration 44/1000 | Loss: 0.00001268
Iteration 45/1000 | Loss: 0.00001268
Iteration 46/1000 | Loss: 0.00001268
Iteration 47/1000 | Loss: 0.00001268
Iteration 48/1000 | Loss: 0.00001268
Iteration 49/1000 | Loss: 0.00001268
Iteration 50/1000 | Loss: 0.00001268
Iteration 51/1000 | Loss: 0.00001268
Iteration 52/1000 | Loss: 0.00001268
Iteration 53/1000 | Loss: 0.00001268
Iteration 54/1000 | Loss: 0.00001268
Iteration 55/1000 | Loss: 0.00001268
Iteration 56/1000 | Loss: 0.00001268
Iteration 57/1000 | Loss: 0.00001268
Iteration 58/1000 | Loss: 0.00001268
Iteration 59/1000 | Loss: 0.00001268
Iteration 60/1000 | Loss: 0.00001268
Iteration 61/1000 | Loss: 0.00001268
Iteration 62/1000 | Loss: 0.00001268
Iteration 63/1000 | Loss: 0.00001268
Iteration 64/1000 | Loss: 0.00001268
Iteration 65/1000 | Loss: 0.00001268
Iteration 66/1000 | Loss: 0.00001268
Iteration 67/1000 | Loss: 0.00001268
Iteration 68/1000 | Loss: 0.00001268
Iteration 69/1000 | Loss: 0.00001268
Iteration 70/1000 | Loss: 0.00001268
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 70. Stopping optimization.
Last 5 losses: [1.2680796316999476e-05, 1.2680796316999476e-05, 1.2680796316999476e-05, 1.2680796316999476e-05, 1.2680796316999476e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2680796316999476e-05

Optimization complete. Final v2v error: 3.0462236404418945 mm

Highest mean error: 3.4265122413635254 mm for frame 90

Lowest mean error: 2.840700626373291 mm for frame 122

Saving results

Total time: 29.080836534500122
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01101607
Iteration 2/25 | Loss: 0.00185537
Iteration 3/25 | Loss: 0.00102760
Iteration 4/25 | Loss: 0.00090617
Iteration 5/25 | Loss: 0.00089270
Iteration 6/25 | Loss: 0.00087799
Iteration 7/25 | Loss: 0.00085786
Iteration 8/25 | Loss: 0.00081514
Iteration 9/25 | Loss: 0.00080271
Iteration 10/25 | Loss: 0.00079953
Iteration 11/25 | Loss: 0.00079643
Iteration 12/25 | Loss: 0.00079592
Iteration 13/25 | Loss: 0.00078807
Iteration 14/25 | Loss: 0.00078561
Iteration 15/25 | Loss: 0.00078396
Iteration 16/25 | Loss: 0.00078116
Iteration 17/25 | Loss: 0.00078061
Iteration 18/25 | Loss: 0.00077969
Iteration 19/25 | Loss: 0.00078187
Iteration 20/25 | Loss: 0.00078455
Iteration 21/25 | Loss: 0.00078288
Iteration 22/25 | Loss: 0.00077487
Iteration 23/25 | Loss: 0.00077798
Iteration 24/25 | Loss: 0.00077213
Iteration 25/25 | Loss: 0.00077374

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.25364161
Iteration 2/25 | Loss: 0.00079432
Iteration 3/25 | Loss: 0.00065572
Iteration 4/25 | Loss: 0.00065572
Iteration 5/25 | Loss: 0.00065572
Iteration 6/25 | Loss: 0.00065572
Iteration 7/25 | Loss: 0.00065572
Iteration 8/25 | Loss: 0.00065572
Iteration 9/25 | Loss: 0.00065572
Iteration 10/25 | Loss: 0.00065572
Iteration 11/25 | Loss: 0.00065572
Iteration 12/25 | Loss: 0.00065572
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006557170418091118, 0.0006557170418091118, 0.0006557170418091118, 0.0006557170418091118, 0.0006557170418091118]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006557170418091118

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065572
Iteration 2/1000 | Loss: 0.00013608
Iteration 3/1000 | Loss: 0.00004642
Iteration 4/1000 | Loss: 0.00003458
Iteration 5/1000 | Loss: 0.00016516
Iteration 6/1000 | Loss: 0.00006307
Iteration 7/1000 | Loss: 0.00012417
Iteration 8/1000 | Loss: 0.00014136
Iteration 9/1000 | Loss: 0.00018350
Iteration 10/1000 | Loss: 0.00017824
Iteration 11/1000 | Loss: 0.00012547
Iteration 12/1000 | Loss: 0.00033717
Iteration 13/1000 | Loss: 0.00019969
Iteration 14/1000 | Loss: 0.00033432
Iteration 15/1000 | Loss: 0.00012990
Iteration 16/1000 | Loss: 0.00012764
Iteration 17/1000 | Loss: 0.00016966
Iteration 18/1000 | Loss: 0.00015450
Iteration 19/1000 | Loss: 0.00011765
Iteration 20/1000 | Loss: 0.00007365
Iteration 21/1000 | Loss: 0.00011534
Iteration 22/1000 | Loss: 0.00024113
Iteration 23/1000 | Loss: 0.00027928
Iteration 24/1000 | Loss: 0.00011984
Iteration 25/1000 | Loss: 0.00018791
Iteration 26/1000 | Loss: 0.00015124
Iteration 27/1000 | Loss: 0.00019320
Iteration 28/1000 | Loss: 0.00006083
Iteration 29/1000 | Loss: 0.00011768
Iteration 30/1000 | Loss: 0.00010850
Iteration 31/1000 | Loss: 0.00008596
Iteration 32/1000 | Loss: 0.00004869
Iteration 33/1000 | Loss: 0.00023391
Iteration 34/1000 | Loss: 0.00019567
Iteration 35/1000 | Loss: 0.00004945
Iteration 36/1000 | Loss: 0.00003336
Iteration 37/1000 | Loss: 0.00005625
Iteration 38/1000 | Loss: 0.00007859
Iteration 39/1000 | Loss: 0.00011080
Iteration 40/1000 | Loss: 0.00005119
Iteration 41/1000 | Loss: 0.00011700
Iteration 42/1000 | Loss: 0.00008456
Iteration 43/1000 | Loss: 0.00006439
Iteration 44/1000 | Loss: 0.00009680
Iteration 45/1000 | Loss: 0.00010149
Iteration 46/1000 | Loss: 0.00010502
Iteration 47/1000 | Loss: 0.00009938
Iteration 48/1000 | Loss: 0.00015906
Iteration 49/1000 | Loss: 0.00004328
Iteration 50/1000 | Loss: 0.00028704
Iteration 51/1000 | Loss: 0.00044153
Iteration 52/1000 | Loss: 0.00015298
Iteration 53/1000 | Loss: 0.00030716
Iteration 54/1000 | Loss: 0.00007436
Iteration 55/1000 | Loss: 0.00009923
Iteration 56/1000 | Loss: 0.00005725
Iteration 57/1000 | Loss: 0.00009876
Iteration 58/1000 | Loss: 0.00005405
Iteration 59/1000 | Loss: 0.00016089
Iteration 60/1000 | Loss: 0.00007735
Iteration 61/1000 | Loss: 0.00004096
Iteration 62/1000 | Loss: 0.00004366
Iteration 63/1000 | Loss: 0.00007853
Iteration 64/1000 | Loss: 0.00003916
Iteration 65/1000 | Loss: 0.00009013
Iteration 66/1000 | Loss: 0.00012008
Iteration 67/1000 | Loss: 0.00005494
Iteration 68/1000 | Loss: 0.00003628
Iteration 69/1000 | Loss: 0.00002879
Iteration 70/1000 | Loss: 0.00003041
Iteration 71/1000 | Loss: 0.00025794
Iteration 72/1000 | Loss: 0.00023957
Iteration 73/1000 | Loss: 0.00024360
Iteration 74/1000 | Loss: 0.00063907
Iteration 75/1000 | Loss: 0.00005075
Iteration 76/1000 | Loss: 0.00016608
Iteration 77/1000 | Loss: 0.00003381
Iteration 78/1000 | Loss: 0.00005310
Iteration 79/1000 | Loss: 0.00004098
Iteration 80/1000 | Loss: 0.00022285
Iteration 81/1000 | Loss: 0.00002910
Iteration 82/1000 | Loss: 0.00008977
Iteration 83/1000 | Loss: 0.00002400
Iteration 84/1000 | Loss: 0.00004038
Iteration 85/1000 | Loss: 0.00002231
Iteration 86/1000 | Loss: 0.00002872
Iteration 87/1000 | Loss: 0.00002146
Iteration 88/1000 | Loss: 0.00002831
Iteration 89/1000 | Loss: 0.00003110
Iteration 90/1000 | Loss: 0.00002131
Iteration 91/1000 | Loss: 0.00002807
Iteration 92/1000 | Loss: 0.00002073
Iteration 93/1000 | Loss: 0.00002070
Iteration 94/1000 | Loss: 0.00002068
Iteration 95/1000 | Loss: 0.00002067
Iteration 96/1000 | Loss: 0.00002067
Iteration 97/1000 | Loss: 0.00002067
Iteration 98/1000 | Loss: 0.00002067
Iteration 99/1000 | Loss: 0.00002067
Iteration 100/1000 | Loss: 0.00002066
Iteration 101/1000 | Loss: 0.00002066
Iteration 102/1000 | Loss: 0.00002066
Iteration 103/1000 | Loss: 0.00002066
Iteration 104/1000 | Loss: 0.00002065
Iteration 105/1000 | Loss: 0.00002065
Iteration 106/1000 | Loss: 0.00002063
Iteration 107/1000 | Loss: 0.00002063
Iteration 108/1000 | Loss: 0.00002062
Iteration 109/1000 | Loss: 0.00002059
Iteration 110/1000 | Loss: 0.00002059
Iteration 111/1000 | Loss: 0.00002058
Iteration 112/1000 | Loss: 0.00002058
Iteration 113/1000 | Loss: 0.00002058
Iteration 114/1000 | Loss: 0.00002057
Iteration 115/1000 | Loss: 0.00002057
Iteration 116/1000 | Loss: 0.00002057
Iteration 117/1000 | Loss: 0.00002057
Iteration 118/1000 | Loss: 0.00002056
Iteration 119/1000 | Loss: 0.00002056
Iteration 120/1000 | Loss: 0.00002056
Iteration 121/1000 | Loss: 0.00002056
Iteration 122/1000 | Loss: 0.00002056
Iteration 123/1000 | Loss: 0.00002056
Iteration 124/1000 | Loss: 0.00002055
Iteration 125/1000 | Loss: 0.00002055
Iteration 126/1000 | Loss: 0.00002055
Iteration 127/1000 | Loss: 0.00002055
Iteration 128/1000 | Loss: 0.00002055
Iteration 129/1000 | Loss: 0.00002054
Iteration 130/1000 | Loss: 0.00002054
Iteration 131/1000 | Loss: 0.00002054
Iteration 132/1000 | Loss: 0.00002054
Iteration 133/1000 | Loss: 0.00002053
Iteration 134/1000 | Loss: 0.00002053
Iteration 135/1000 | Loss: 0.00002053
Iteration 136/1000 | Loss: 0.00003852
Iteration 137/1000 | Loss: 0.00002166
Iteration 138/1000 | Loss: 0.00002735
Iteration 139/1000 | Loss: 0.00002054
Iteration 140/1000 | Loss: 0.00002053
Iteration 141/1000 | Loss: 0.00002051
Iteration 142/1000 | Loss: 0.00002051
Iteration 143/1000 | Loss: 0.00002051
Iteration 144/1000 | Loss: 0.00002050
Iteration 145/1000 | Loss: 0.00002050
Iteration 146/1000 | Loss: 0.00002050
Iteration 147/1000 | Loss: 0.00002050
Iteration 148/1000 | Loss: 0.00002049
Iteration 149/1000 | Loss: 0.00002049
Iteration 150/1000 | Loss: 0.00002048
Iteration 151/1000 | Loss: 0.00002048
Iteration 152/1000 | Loss: 0.00002048
Iteration 153/1000 | Loss: 0.00002048
Iteration 154/1000 | Loss: 0.00002048
Iteration 155/1000 | Loss: 0.00002048
Iteration 156/1000 | Loss: 0.00002048
Iteration 157/1000 | Loss: 0.00002048
Iteration 158/1000 | Loss: 0.00002048
Iteration 159/1000 | Loss: 0.00002048
Iteration 160/1000 | Loss: 0.00002048
Iteration 161/1000 | Loss: 0.00002047
Iteration 162/1000 | Loss: 0.00002047
Iteration 163/1000 | Loss: 0.00002047
Iteration 164/1000 | Loss: 0.00002047
Iteration 165/1000 | Loss: 0.00002046
Iteration 166/1000 | Loss: 0.00002046
Iteration 167/1000 | Loss: 0.00002046
Iteration 168/1000 | Loss: 0.00002046
Iteration 169/1000 | Loss: 0.00002046
Iteration 170/1000 | Loss: 0.00002046
Iteration 171/1000 | Loss: 0.00002046
Iteration 172/1000 | Loss: 0.00002046
Iteration 173/1000 | Loss: 0.00002046
Iteration 174/1000 | Loss: 0.00002046
Iteration 175/1000 | Loss: 0.00002046
Iteration 176/1000 | Loss: 0.00002046
Iteration 177/1000 | Loss: 0.00002045
Iteration 178/1000 | Loss: 0.00002045
Iteration 179/1000 | Loss: 0.00002045
Iteration 180/1000 | Loss: 0.00002045
Iteration 181/1000 | Loss: 0.00002045
Iteration 182/1000 | Loss: 0.00002045
Iteration 183/1000 | Loss: 0.00002045
Iteration 184/1000 | Loss: 0.00002045
Iteration 185/1000 | Loss: 0.00002045
Iteration 186/1000 | Loss: 0.00002045
Iteration 187/1000 | Loss: 0.00002045
Iteration 188/1000 | Loss: 0.00002045
Iteration 189/1000 | Loss: 0.00002045
Iteration 190/1000 | Loss: 0.00002045
Iteration 191/1000 | Loss: 0.00002045
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [2.0452345779631287e-05, 2.0452345779631287e-05, 2.0452345779631287e-05, 2.0452345779631287e-05, 2.0452345779631287e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0452345779631287e-05

Optimization complete. Final v2v error: 3.841317653656006 mm

Highest mean error: 5.659704208374023 mm for frame 181

Lowest mean error: 3.5013389587402344 mm for frame 112

Saving results

Total time: 208.16253924369812
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01065112
Iteration 2/25 | Loss: 0.01065111
Iteration 3/25 | Loss: 0.00177847
Iteration 4/25 | Loss: 0.00132988
Iteration 5/25 | Loss: 0.00122459
Iteration 6/25 | Loss: 0.00105575
Iteration 7/25 | Loss: 0.00068573
Iteration 8/25 | Loss: 0.00068375
Iteration 9/25 | Loss: 0.00066922
Iteration 10/25 | Loss: 0.00066867
Iteration 11/25 | Loss: 0.00066835
Iteration 12/25 | Loss: 0.00066821
Iteration 13/25 | Loss: 0.00066811
Iteration 14/25 | Loss: 0.00066810
Iteration 15/25 | Loss: 0.00066810
Iteration 16/25 | Loss: 0.00066810
Iteration 17/25 | Loss: 0.00066809
Iteration 18/25 | Loss: 0.00066809
Iteration 19/25 | Loss: 0.00066809
Iteration 20/25 | Loss: 0.00066808
Iteration 21/25 | Loss: 0.00066808
Iteration 22/25 | Loss: 0.00066808
Iteration 23/25 | Loss: 0.00066808
Iteration 24/25 | Loss: 0.00066808
Iteration 25/25 | Loss: 0.00066808

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45746064
Iteration 2/25 | Loss: 0.00028973
Iteration 3/25 | Loss: 0.00028972
Iteration 4/25 | Loss: 0.00028972
Iteration 5/25 | Loss: 0.00028972
Iteration 6/25 | Loss: 0.00028972
Iteration 7/25 | Loss: 0.00028972
Iteration 8/25 | Loss: 0.00028972
Iteration 9/25 | Loss: 0.00028972
Iteration 10/25 | Loss: 0.00028972
Iteration 11/25 | Loss: 0.00028972
Iteration 12/25 | Loss: 0.00028972
Iteration 13/25 | Loss: 0.00028972
Iteration 14/25 | Loss: 0.00028972
Iteration 15/25 | Loss: 0.00028972
Iteration 16/25 | Loss: 0.00028972
Iteration 17/25 | Loss: 0.00028972
Iteration 18/25 | Loss: 0.00028972
Iteration 19/25 | Loss: 0.00028972
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0002897180966101587, 0.0002897180966101587, 0.0002897180966101587, 0.0002897180966101587, 0.0002897180966101587]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002897180966101587

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028972
Iteration 2/1000 | Loss: 0.00002838
Iteration 3/1000 | Loss: 0.00015824
Iteration 4/1000 | Loss: 0.00002551
Iteration 5/1000 | Loss: 0.00018948
Iteration 6/1000 | Loss: 0.00002839
Iteration 7/1000 | Loss: 0.00002553
Iteration 8/1000 | Loss: 0.00002404
Iteration 9/1000 | Loss: 0.00002264
Iteration 10/1000 | Loss: 0.00014234
Iteration 11/1000 | Loss: 0.00003642
Iteration 12/1000 | Loss: 0.00002299
Iteration 13/1000 | Loss: 0.00001950
Iteration 14/1000 | Loss: 0.00001850
Iteration 15/1000 | Loss: 0.00001734
Iteration 16/1000 | Loss: 0.00001645
Iteration 17/1000 | Loss: 0.00001581
Iteration 18/1000 | Loss: 0.00001536
Iteration 19/1000 | Loss: 0.00001495
Iteration 20/1000 | Loss: 0.00001469
Iteration 21/1000 | Loss: 0.00001465
Iteration 22/1000 | Loss: 0.00001449
Iteration 23/1000 | Loss: 0.00001447
Iteration 24/1000 | Loss: 0.00001446
Iteration 25/1000 | Loss: 0.00001445
Iteration 26/1000 | Loss: 0.00001445
Iteration 27/1000 | Loss: 0.00001444
Iteration 28/1000 | Loss: 0.00001443
Iteration 29/1000 | Loss: 0.00001435
Iteration 30/1000 | Loss: 0.00001432
Iteration 31/1000 | Loss: 0.00001432
Iteration 32/1000 | Loss: 0.00001432
Iteration 33/1000 | Loss: 0.00001431
Iteration 34/1000 | Loss: 0.00001431
Iteration 35/1000 | Loss: 0.00001431
Iteration 36/1000 | Loss: 0.00001431
Iteration 37/1000 | Loss: 0.00001430
Iteration 38/1000 | Loss: 0.00001430
Iteration 39/1000 | Loss: 0.00001430
Iteration 40/1000 | Loss: 0.00001430
Iteration 41/1000 | Loss: 0.00001430
Iteration 42/1000 | Loss: 0.00001430
Iteration 43/1000 | Loss: 0.00001429
Iteration 44/1000 | Loss: 0.00001429
Iteration 45/1000 | Loss: 0.00001428
Iteration 46/1000 | Loss: 0.00001428
Iteration 47/1000 | Loss: 0.00001427
Iteration 48/1000 | Loss: 0.00001426
Iteration 49/1000 | Loss: 0.00001426
Iteration 50/1000 | Loss: 0.00001425
Iteration 51/1000 | Loss: 0.00001425
Iteration 52/1000 | Loss: 0.00001424
Iteration 53/1000 | Loss: 0.00001424
Iteration 54/1000 | Loss: 0.00001423
Iteration 55/1000 | Loss: 0.00001423
Iteration 56/1000 | Loss: 0.00001422
Iteration 57/1000 | Loss: 0.00001421
Iteration 58/1000 | Loss: 0.00001421
Iteration 59/1000 | Loss: 0.00001421
Iteration 60/1000 | Loss: 0.00001421
Iteration 61/1000 | Loss: 0.00001421
Iteration 62/1000 | Loss: 0.00001421
Iteration 63/1000 | Loss: 0.00001421
Iteration 64/1000 | Loss: 0.00001420
Iteration 65/1000 | Loss: 0.00001420
Iteration 66/1000 | Loss: 0.00001420
Iteration 67/1000 | Loss: 0.00001419
Iteration 68/1000 | Loss: 0.00001419
Iteration 69/1000 | Loss: 0.00001418
Iteration 70/1000 | Loss: 0.00001418
Iteration 71/1000 | Loss: 0.00001417
Iteration 72/1000 | Loss: 0.00001417
Iteration 73/1000 | Loss: 0.00001415
Iteration 74/1000 | Loss: 0.00001415
Iteration 75/1000 | Loss: 0.00001415
Iteration 76/1000 | Loss: 0.00001414
Iteration 77/1000 | Loss: 0.00001414
Iteration 78/1000 | Loss: 0.00001414
Iteration 79/1000 | Loss: 0.00001414
Iteration 80/1000 | Loss: 0.00001414
Iteration 81/1000 | Loss: 0.00001414
Iteration 82/1000 | Loss: 0.00001414
Iteration 83/1000 | Loss: 0.00001413
Iteration 84/1000 | Loss: 0.00001413
Iteration 85/1000 | Loss: 0.00001413
Iteration 86/1000 | Loss: 0.00001413
Iteration 87/1000 | Loss: 0.00001413
Iteration 88/1000 | Loss: 0.00001413
Iteration 89/1000 | Loss: 0.00001412
Iteration 90/1000 | Loss: 0.00001412
Iteration 91/1000 | Loss: 0.00001412
Iteration 92/1000 | Loss: 0.00001412
Iteration 93/1000 | Loss: 0.00001412
Iteration 94/1000 | Loss: 0.00001412
Iteration 95/1000 | Loss: 0.00001412
Iteration 96/1000 | Loss: 0.00001412
Iteration 97/1000 | Loss: 0.00001412
Iteration 98/1000 | Loss: 0.00001412
Iteration 99/1000 | Loss: 0.00001411
Iteration 100/1000 | Loss: 0.00001411
Iteration 101/1000 | Loss: 0.00001411
Iteration 102/1000 | Loss: 0.00001411
Iteration 103/1000 | Loss: 0.00001411
Iteration 104/1000 | Loss: 0.00001411
Iteration 105/1000 | Loss: 0.00001411
Iteration 106/1000 | Loss: 0.00001411
Iteration 107/1000 | Loss: 0.00001411
Iteration 108/1000 | Loss: 0.00001411
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [1.4114535588305444e-05, 1.4114535588305444e-05, 1.4114535588305444e-05, 1.4114535588305444e-05, 1.4114535588305444e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4114535588305444e-05

Optimization complete. Final v2v error: 3.2128655910491943 mm

Highest mean error: 4.0457587242126465 mm for frame 54

Lowest mean error: 2.9993748664855957 mm for frame 52

Saving results

Total time: 57.497315883636475
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_002/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_002/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00869155
Iteration 2/25 | Loss: 0.00129056
Iteration 3/25 | Loss: 0.00077992
Iteration 4/25 | Loss: 0.00073441
Iteration 5/25 | Loss: 0.00067663
Iteration 6/25 | Loss: 0.00068124
Iteration 7/25 | Loss: 0.00068149
Iteration 8/25 | Loss: 0.00067936
Iteration 9/25 | Loss: 0.00067157
Iteration 10/25 | Loss: 0.00066624
Iteration 11/25 | Loss: 0.00065946
Iteration 12/25 | Loss: 0.00065428
Iteration 13/25 | Loss: 0.00064795
Iteration 14/25 | Loss: 0.00064770
Iteration 15/25 | Loss: 0.00064372
Iteration 16/25 | Loss: 0.00064220
Iteration 17/25 | Loss: 0.00064180
Iteration 18/25 | Loss: 0.00064547
Iteration 19/25 | Loss: 0.00064478
Iteration 20/25 | Loss: 0.00064468
Iteration 21/25 | Loss: 0.00064410
Iteration 22/25 | Loss: 0.00064286
Iteration 23/25 | Loss: 0.00064235
Iteration 24/25 | Loss: 0.00064181
Iteration 25/25 | Loss: 0.00064298

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.99148512
Iteration 2/25 | Loss: 0.00037445
Iteration 3/25 | Loss: 0.00029789
Iteration 4/25 | Loss: 0.00029788
Iteration 5/25 | Loss: 0.00029788
Iteration 6/25 | Loss: 0.00029788
Iteration 7/25 | Loss: 0.00029788
Iteration 8/25 | Loss: 0.00029788
Iteration 9/25 | Loss: 0.00029788
Iteration 10/25 | Loss: 0.00029788
Iteration 11/25 | Loss: 0.00029788
Iteration 12/25 | Loss: 0.00029788
Iteration 13/25 | Loss: 0.00029788
Iteration 14/25 | Loss: 0.00029788
Iteration 15/25 | Loss: 0.00029788
Iteration 16/25 | Loss: 0.00029788
Iteration 17/25 | Loss: 0.00029788
Iteration 18/25 | Loss: 0.00029788
Iteration 19/25 | Loss: 0.00029788
Iteration 20/25 | Loss: 0.00029788
Iteration 21/25 | Loss: 0.00029788
Iteration 22/25 | Loss: 0.00029788
Iteration 23/25 | Loss: 0.00029788
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00029788463143631816, 0.00029788463143631816, 0.00029788463143631816, 0.00029788463143631816, 0.00029788463143631816]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00029788463143631816

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029788
Iteration 2/1000 | Loss: 0.00002882
Iteration 3/1000 | Loss: 0.00002273
Iteration 4/1000 | Loss: 0.00039797
Iteration 5/1000 | Loss: 0.00026341
Iteration 6/1000 | Loss: 0.00022474
Iteration 7/1000 | Loss: 0.00006649
Iteration 8/1000 | Loss: 0.00002086
Iteration 9/1000 | Loss: 0.00001897
Iteration 10/1000 | Loss: 0.00006048
Iteration 11/1000 | Loss: 0.00001825
Iteration 12/1000 | Loss: 0.00001769
Iteration 13/1000 | Loss: 0.00001734
Iteration 14/1000 | Loss: 0.00001712
Iteration 15/1000 | Loss: 0.00001690
Iteration 16/1000 | Loss: 0.00001685
Iteration 17/1000 | Loss: 0.00001684
Iteration 18/1000 | Loss: 0.00001683
Iteration 19/1000 | Loss: 0.00001679
Iteration 20/1000 | Loss: 0.00001669
Iteration 21/1000 | Loss: 0.00001661
Iteration 22/1000 | Loss: 0.00001661
Iteration 23/1000 | Loss: 0.00012290
Iteration 24/1000 | Loss: 0.00001683
Iteration 25/1000 | Loss: 0.00001649
Iteration 26/1000 | Loss: 0.00001649
Iteration 27/1000 | Loss: 0.00001649
Iteration 28/1000 | Loss: 0.00001649
Iteration 29/1000 | Loss: 0.00001649
Iteration 30/1000 | Loss: 0.00001649
Iteration 31/1000 | Loss: 0.00001649
Iteration 32/1000 | Loss: 0.00001649
Iteration 33/1000 | Loss: 0.00001648
Iteration 34/1000 | Loss: 0.00001648
Iteration 35/1000 | Loss: 0.00001648
Iteration 36/1000 | Loss: 0.00001647
Iteration 37/1000 | Loss: 0.00001647
Iteration 38/1000 | Loss: 0.00001647
Iteration 39/1000 | Loss: 0.00001645
Iteration 40/1000 | Loss: 0.00001645
Iteration 41/1000 | Loss: 0.00001644
Iteration 42/1000 | Loss: 0.00001644
Iteration 43/1000 | Loss: 0.00001644
Iteration 44/1000 | Loss: 0.00001644
Iteration 45/1000 | Loss: 0.00001643
Iteration 46/1000 | Loss: 0.00001643
Iteration 47/1000 | Loss: 0.00001643
Iteration 48/1000 | Loss: 0.00001642
Iteration 49/1000 | Loss: 0.00001642
Iteration 50/1000 | Loss: 0.00001642
Iteration 51/1000 | Loss: 0.00001641
Iteration 52/1000 | Loss: 0.00001641
Iteration 53/1000 | Loss: 0.00001641
Iteration 54/1000 | Loss: 0.00001641
Iteration 55/1000 | Loss: 0.00001641
Iteration 56/1000 | Loss: 0.00001641
Iteration 57/1000 | Loss: 0.00001641
Iteration 58/1000 | Loss: 0.00001641
Iteration 59/1000 | Loss: 0.00001641
Iteration 60/1000 | Loss: 0.00001641
Iteration 61/1000 | Loss: 0.00001640
Iteration 62/1000 | Loss: 0.00001640
Iteration 63/1000 | Loss: 0.00001640
Iteration 64/1000 | Loss: 0.00001639
Iteration 65/1000 | Loss: 0.00001639
Iteration 66/1000 | Loss: 0.00001639
Iteration 67/1000 | Loss: 0.00001638
Iteration 68/1000 | Loss: 0.00001638
Iteration 69/1000 | Loss: 0.00001638
Iteration 70/1000 | Loss: 0.00001638
Iteration 71/1000 | Loss: 0.00001638
Iteration 72/1000 | Loss: 0.00001638
Iteration 73/1000 | Loss: 0.00001637
Iteration 74/1000 | Loss: 0.00001637
Iteration 75/1000 | Loss: 0.00001637
Iteration 76/1000 | Loss: 0.00001637
Iteration 77/1000 | Loss: 0.00001637
Iteration 78/1000 | Loss: 0.00001637
Iteration 79/1000 | Loss: 0.00001637
Iteration 80/1000 | Loss: 0.00001636
Iteration 81/1000 | Loss: 0.00001636
Iteration 82/1000 | Loss: 0.00001636
Iteration 83/1000 | Loss: 0.00001636
Iteration 84/1000 | Loss: 0.00001636
Iteration 85/1000 | Loss: 0.00001636
Iteration 86/1000 | Loss: 0.00001635
Iteration 87/1000 | Loss: 0.00001635
Iteration 88/1000 | Loss: 0.00001632
Iteration 89/1000 | Loss: 0.00001632
Iteration 90/1000 | Loss: 0.00001632
Iteration 91/1000 | Loss: 0.00001632
Iteration 92/1000 | Loss: 0.00014519
Iteration 93/1000 | Loss: 0.00002270
Iteration 94/1000 | Loss: 0.00001708
Iteration 95/1000 | Loss: 0.00001642
Iteration 96/1000 | Loss: 0.00001636
Iteration 97/1000 | Loss: 0.00001635
Iteration 98/1000 | Loss: 0.00001634
Iteration 99/1000 | Loss: 0.00001632
Iteration 100/1000 | Loss: 0.00001632
Iteration 101/1000 | Loss: 0.00001631
Iteration 102/1000 | Loss: 0.00001631
Iteration 103/1000 | Loss: 0.00001631
Iteration 104/1000 | Loss: 0.00001631
Iteration 105/1000 | Loss: 0.00001631
Iteration 106/1000 | Loss: 0.00001631
Iteration 107/1000 | Loss: 0.00001631
Iteration 108/1000 | Loss: 0.00001631
Iteration 109/1000 | Loss: 0.00001631
Iteration 110/1000 | Loss: 0.00001631
Iteration 111/1000 | Loss: 0.00001631
Iteration 112/1000 | Loss: 0.00001631
Iteration 113/1000 | Loss: 0.00001630
Iteration 114/1000 | Loss: 0.00001630
Iteration 115/1000 | Loss: 0.00001630
Iteration 116/1000 | Loss: 0.00001630
Iteration 117/1000 | Loss: 0.00001630
Iteration 118/1000 | Loss: 0.00001630
Iteration 119/1000 | Loss: 0.00001630
Iteration 120/1000 | Loss: 0.00001630
Iteration 121/1000 | Loss: 0.00001629
Iteration 122/1000 | Loss: 0.00001629
Iteration 123/1000 | Loss: 0.00001629
Iteration 124/1000 | Loss: 0.00001628
Iteration 125/1000 | Loss: 0.00001628
Iteration 126/1000 | Loss: 0.00001628
Iteration 127/1000 | Loss: 0.00001628
Iteration 128/1000 | Loss: 0.00001628
Iteration 129/1000 | Loss: 0.00001628
Iteration 130/1000 | Loss: 0.00001627
Iteration 131/1000 | Loss: 0.00001627
Iteration 132/1000 | Loss: 0.00001627
Iteration 133/1000 | Loss: 0.00001627
Iteration 134/1000 | Loss: 0.00001627
Iteration 135/1000 | Loss: 0.00001627
Iteration 136/1000 | Loss: 0.00001627
Iteration 137/1000 | Loss: 0.00001627
Iteration 138/1000 | Loss: 0.00001627
Iteration 139/1000 | Loss: 0.00001626
Iteration 140/1000 | Loss: 0.00001626
Iteration 141/1000 | Loss: 0.00001626
Iteration 142/1000 | Loss: 0.00001626
Iteration 143/1000 | Loss: 0.00001626
Iteration 144/1000 | Loss: 0.00001626
Iteration 145/1000 | Loss: 0.00001626
Iteration 146/1000 | Loss: 0.00001626
Iteration 147/1000 | Loss: 0.00001625
Iteration 148/1000 | Loss: 0.00001625
Iteration 149/1000 | Loss: 0.00001625
Iteration 150/1000 | Loss: 0.00001625
Iteration 151/1000 | Loss: 0.00001625
Iteration 152/1000 | Loss: 0.00001625
Iteration 153/1000 | Loss: 0.00001625
Iteration 154/1000 | Loss: 0.00001625
Iteration 155/1000 | Loss: 0.00001625
Iteration 156/1000 | Loss: 0.00001625
Iteration 157/1000 | Loss: 0.00001625
Iteration 158/1000 | Loss: 0.00001625
Iteration 159/1000 | Loss: 0.00001625
Iteration 160/1000 | Loss: 0.00001625
Iteration 161/1000 | Loss: 0.00001625
Iteration 162/1000 | Loss: 0.00001625
Iteration 163/1000 | Loss: 0.00001625
Iteration 164/1000 | Loss: 0.00001625
Iteration 165/1000 | Loss: 0.00001625
Iteration 166/1000 | Loss: 0.00001624
Iteration 167/1000 | Loss: 0.00001624
Iteration 168/1000 | Loss: 0.00001624
Iteration 169/1000 | Loss: 0.00001624
Iteration 170/1000 | Loss: 0.00001624
Iteration 171/1000 | Loss: 0.00001624
Iteration 172/1000 | Loss: 0.00001624
Iteration 173/1000 | Loss: 0.00001624
Iteration 174/1000 | Loss: 0.00001624
Iteration 175/1000 | Loss: 0.00001624
Iteration 176/1000 | Loss: 0.00001624
Iteration 177/1000 | Loss: 0.00001624
Iteration 178/1000 | Loss: 0.00001624
Iteration 179/1000 | Loss: 0.00001624
Iteration 180/1000 | Loss: 0.00001624
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.6244272046606056e-05, 1.6244272046606056e-05, 1.6244272046606056e-05, 1.6244272046606056e-05, 1.6244272046606056e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6244272046606056e-05

Optimization complete. Final v2v error: 3.4296607971191406 mm

Highest mean error: 4.0568647384643555 mm for frame 7

Lowest mean error: 3.093506097793579 mm for frame 32

Saving results

Total time: 99.09911584854126
