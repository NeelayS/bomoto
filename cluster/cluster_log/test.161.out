Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=161, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 9016-9071
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_58_us_2424/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_58_us_2424/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_58_us_2424/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00749828
Iteration 2/25 | Loss: 0.00214964
Iteration 3/25 | Loss: 0.00208204
Iteration 4/25 | Loss: 0.00207704
Iteration 5/25 | Loss: 0.00207426
Iteration 6/25 | Loss: 0.00207354
Iteration 7/25 | Loss: 0.00207354
Iteration 8/25 | Loss: 0.00207354
Iteration 9/25 | Loss: 0.00207354
Iteration 10/25 | Loss: 0.00207354
Iteration 11/25 | Loss: 0.00207354
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.002073537325486541, 0.002073537325486541, 0.002073537325486541, 0.002073537325486541, 0.002073537325486541]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002073537325486541

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41448152
Iteration 2/25 | Loss: 0.00173172
Iteration 3/25 | Loss: 0.00173171
Iteration 4/25 | Loss: 0.00173171
Iteration 5/25 | Loss: 0.00173171
Iteration 6/25 | Loss: 0.00173171
Iteration 7/25 | Loss: 0.00173171
Iteration 8/25 | Loss: 0.00173171
Iteration 9/25 | Loss: 0.00173171
Iteration 10/25 | Loss: 0.00173171
Iteration 11/25 | Loss: 0.00173171
Iteration 12/25 | Loss: 0.00173171
Iteration 13/25 | Loss: 0.00173171
Iteration 14/25 | Loss: 0.00173171
Iteration 15/25 | Loss: 0.00173171
Iteration 16/25 | Loss: 0.00173171
Iteration 17/25 | Loss: 0.00173171
Iteration 18/25 | Loss: 0.00173171
Iteration 19/25 | Loss: 0.00173171
Iteration 20/25 | Loss: 0.00173171
Iteration 21/25 | Loss: 0.00173171
Iteration 22/25 | Loss: 0.00173171
Iteration 23/25 | Loss: 0.00173171
Iteration 24/25 | Loss: 0.00173171
Iteration 25/25 | Loss: 0.00173171

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00173171
Iteration 2/1000 | Loss: 0.00009181
Iteration 3/1000 | Loss: 0.00005550
Iteration 4/1000 | Loss: 0.00004982
Iteration 5/1000 | Loss: 0.00004746
Iteration 6/1000 | Loss: 0.00004608
Iteration 7/1000 | Loss: 0.00004456
Iteration 8/1000 | Loss: 0.00004372
Iteration 9/1000 | Loss: 0.00004316
Iteration 10/1000 | Loss: 0.00004266
Iteration 11/1000 | Loss: 0.00004227
Iteration 12/1000 | Loss: 0.00004192
Iteration 13/1000 | Loss: 0.00004178
Iteration 14/1000 | Loss: 0.00004174
Iteration 15/1000 | Loss: 0.00004169
Iteration 16/1000 | Loss: 0.00004155
Iteration 17/1000 | Loss: 0.00004148
Iteration 18/1000 | Loss: 0.00004148
Iteration 19/1000 | Loss: 0.00004148
Iteration 20/1000 | Loss: 0.00004148
Iteration 21/1000 | Loss: 0.00004147
Iteration 22/1000 | Loss: 0.00004147
Iteration 23/1000 | Loss: 0.00004147
Iteration 24/1000 | Loss: 0.00004147
Iteration 25/1000 | Loss: 0.00004147
Iteration 26/1000 | Loss: 0.00004147
Iteration 27/1000 | Loss: 0.00004147
Iteration 28/1000 | Loss: 0.00004147
Iteration 29/1000 | Loss: 0.00004147
Iteration 30/1000 | Loss: 0.00004147
Iteration 31/1000 | Loss: 0.00004147
Iteration 32/1000 | Loss: 0.00004144
Iteration 33/1000 | Loss: 0.00004144
Iteration 34/1000 | Loss: 0.00004144
Iteration 35/1000 | Loss: 0.00004144
Iteration 36/1000 | Loss: 0.00004144
Iteration 37/1000 | Loss: 0.00004144
Iteration 38/1000 | Loss: 0.00004144
Iteration 39/1000 | Loss: 0.00004143
Iteration 40/1000 | Loss: 0.00004143
Iteration 41/1000 | Loss: 0.00004142
Iteration 42/1000 | Loss: 0.00004142
Iteration 43/1000 | Loss: 0.00004142
Iteration 44/1000 | Loss: 0.00004142
Iteration 45/1000 | Loss: 0.00004141
Iteration 46/1000 | Loss: 0.00004141
Iteration 47/1000 | Loss: 0.00004141
Iteration 48/1000 | Loss: 0.00004141
Iteration 49/1000 | Loss: 0.00004141
Iteration 50/1000 | Loss: 0.00004141
Iteration 51/1000 | Loss: 0.00004141
Iteration 52/1000 | Loss: 0.00004140
Iteration 53/1000 | Loss: 0.00004140
Iteration 54/1000 | Loss: 0.00004140
Iteration 55/1000 | Loss: 0.00004140
Iteration 56/1000 | Loss: 0.00004140
Iteration 57/1000 | Loss: 0.00004140
Iteration 58/1000 | Loss: 0.00004140
Iteration 59/1000 | Loss: 0.00004139
Iteration 60/1000 | Loss: 0.00004139
Iteration 61/1000 | Loss: 0.00004139
Iteration 62/1000 | Loss: 0.00004139
Iteration 63/1000 | Loss: 0.00004139
Iteration 64/1000 | Loss: 0.00004139
Iteration 65/1000 | Loss: 0.00004139
Iteration 66/1000 | Loss: 0.00004139
Iteration 67/1000 | Loss: 0.00004139
Iteration 68/1000 | Loss: 0.00004139
Iteration 69/1000 | Loss: 0.00004139
Iteration 70/1000 | Loss: 0.00004138
Iteration 71/1000 | Loss: 0.00004138
Iteration 72/1000 | Loss: 0.00004138
Iteration 73/1000 | Loss: 0.00004138
Iteration 74/1000 | Loss: 0.00004138
Iteration 75/1000 | Loss: 0.00004137
Iteration 76/1000 | Loss: 0.00004137
Iteration 77/1000 | Loss: 0.00004137
Iteration 78/1000 | Loss: 0.00004137
Iteration 79/1000 | Loss: 0.00004137
Iteration 80/1000 | Loss: 0.00004136
Iteration 81/1000 | Loss: 0.00004136
Iteration 82/1000 | Loss: 0.00004136
Iteration 83/1000 | Loss: 0.00004136
Iteration 84/1000 | Loss: 0.00004136
Iteration 85/1000 | Loss: 0.00004136
Iteration 86/1000 | Loss: 0.00004136
Iteration 87/1000 | Loss: 0.00004135
Iteration 88/1000 | Loss: 0.00004135
Iteration 89/1000 | Loss: 0.00004135
Iteration 90/1000 | Loss: 0.00004134
Iteration 91/1000 | Loss: 0.00004134
Iteration 92/1000 | Loss: 0.00004134
Iteration 93/1000 | Loss: 0.00004134
Iteration 94/1000 | Loss: 0.00004134
Iteration 95/1000 | Loss: 0.00004134
Iteration 96/1000 | Loss: 0.00004134
Iteration 97/1000 | Loss: 0.00004134
Iteration 98/1000 | Loss: 0.00004134
Iteration 99/1000 | Loss: 0.00004133
Iteration 100/1000 | Loss: 0.00004133
Iteration 101/1000 | Loss: 0.00004133
Iteration 102/1000 | Loss: 0.00004133
Iteration 103/1000 | Loss: 0.00004133
Iteration 104/1000 | Loss: 0.00004133
Iteration 105/1000 | Loss: 0.00004133
Iteration 106/1000 | Loss: 0.00004132
Iteration 107/1000 | Loss: 0.00004132
Iteration 108/1000 | Loss: 0.00004132
Iteration 109/1000 | Loss: 0.00004132
Iteration 110/1000 | Loss: 0.00004132
Iteration 111/1000 | Loss: 0.00004132
Iteration 112/1000 | Loss: 0.00004132
Iteration 113/1000 | Loss: 0.00004132
Iteration 114/1000 | Loss: 0.00004132
Iteration 115/1000 | Loss: 0.00004132
Iteration 116/1000 | Loss: 0.00004132
Iteration 117/1000 | Loss: 0.00004132
Iteration 118/1000 | Loss: 0.00004132
Iteration 119/1000 | Loss: 0.00004132
Iteration 120/1000 | Loss: 0.00004132
Iteration 121/1000 | Loss: 0.00004132
Iteration 122/1000 | Loss: 0.00004132
Iteration 123/1000 | Loss: 0.00004132
Iteration 124/1000 | Loss: 0.00004132
Iteration 125/1000 | Loss: 0.00004132
Iteration 126/1000 | Loss: 0.00004131
Iteration 127/1000 | Loss: 0.00004131
Iteration 128/1000 | Loss: 0.00004131
Iteration 129/1000 | Loss: 0.00004131
Iteration 130/1000 | Loss: 0.00004131
Iteration 131/1000 | Loss: 0.00004131
Iteration 132/1000 | Loss: 0.00004130
Iteration 133/1000 | Loss: 0.00004130
Iteration 134/1000 | Loss: 0.00004130
Iteration 135/1000 | Loss: 0.00004130
Iteration 136/1000 | Loss: 0.00004130
Iteration 137/1000 | Loss: 0.00004130
Iteration 138/1000 | Loss: 0.00004130
Iteration 139/1000 | Loss: 0.00004130
Iteration 140/1000 | Loss: 0.00004130
Iteration 141/1000 | Loss: 0.00004130
Iteration 142/1000 | Loss: 0.00004130
Iteration 143/1000 | Loss: 0.00004129
Iteration 144/1000 | Loss: 0.00004129
Iteration 145/1000 | Loss: 0.00004129
Iteration 146/1000 | Loss: 0.00004129
Iteration 147/1000 | Loss: 0.00004129
Iteration 148/1000 | Loss: 0.00004129
Iteration 149/1000 | Loss: 0.00004129
Iteration 150/1000 | Loss: 0.00004129
Iteration 151/1000 | Loss: 0.00004129
Iteration 152/1000 | Loss: 0.00004129
Iteration 153/1000 | Loss: 0.00004129
Iteration 154/1000 | Loss: 0.00004129
Iteration 155/1000 | Loss: 0.00004129
Iteration 156/1000 | Loss: 0.00004129
Iteration 157/1000 | Loss: 0.00004129
Iteration 158/1000 | Loss: 0.00004129
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [4.128578439122066e-05, 4.128578439122066e-05, 4.128578439122066e-05, 4.128578439122066e-05, 4.128578439122066e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.128578439122066e-05

Optimization complete. Final v2v error: 5.57540225982666 mm

Highest mean error: 5.770935535430908 mm for frame 34

Lowest mean error: 5.379942893981934 mm for frame 46

Saving results

Total time: 40.27407217025757
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_58_us_2424/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_58_us_2424/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_58_us_2424/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00773497
Iteration 2/25 | Loss: 0.00229677
Iteration 3/25 | Loss: 0.00218834
Iteration 4/25 | Loss: 0.00217372
Iteration 5/25 | Loss: 0.00216645
Iteration 6/25 | Loss: 0.00216486
Iteration 7/25 | Loss: 0.00216486
Iteration 8/25 | Loss: 0.00216486
Iteration 9/25 | Loss: 0.00216486
Iteration 10/25 | Loss: 0.00216486
Iteration 11/25 | Loss: 0.00216486
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0021648588590323925, 0.0021648588590323925, 0.0021648588590323925, 0.0021648588590323925, 0.0021648588590323925]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021648588590323925

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37773800
Iteration 2/25 | Loss: 0.00196531
Iteration 3/25 | Loss: 0.00196530
Iteration 4/25 | Loss: 0.00196530
Iteration 5/25 | Loss: 0.00196530
Iteration 6/25 | Loss: 0.00196530
Iteration 7/25 | Loss: 0.00196530
Iteration 8/25 | Loss: 0.00196530
Iteration 9/25 | Loss: 0.00196530
Iteration 10/25 | Loss: 0.00196530
Iteration 11/25 | Loss: 0.00196530
Iteration 12/25 | Loss: 0.00196530
Iteration 13/25 | Loss: 0.00196530
Iteration 14/25 | Loss: 0.00196530
Iteration 15/25 | Loss: 0.00196530
Iteration 16/25 | Loss: 0.00196530
Iteration 17/25 | Loss: 0.00196530
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00196530157700181, 0.00196530157700181, 0.00196530157700181, 0.00196530157700181, 0.00196530157700181]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00196530157700181

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00196530
Iteration 2/1000 | Loss: 0.00009448
Iteration 3/1000 | Loss: 0.00005690
Iteration 4/1000 | Loss: 0.00004943
Iteration 5/1000 | Loss: 0.00004643
Iteration 6/1000 | Loss: 0.00004454
Iteration 7/1000 | Loss: 0.00004333
Iteration 8/1000 | Loss: 0.00004247
Iteration 9/1000 | Loss: 0.00004171
Iteration 10/1000 | Loss: 0.00004108
Iteration 11/1000 | Loss: 0.00004069
Iteration 12/1000 | Loss: 0.00004043
Iteration 13/1000 | Loss: 0.00004023
Iteration 14/1000 | Loss: 0.00004008
Iteration 15/1000 | Loss: 0.00004000
Iteration 16/1000 | Loss: 0.00003991
Iteration 17/1000 | Loss: 0.00003990
Iteration 18/1000 | Loss: 0.00003989
Iteration 19/1000 | Loss: 0.00003987
Iteration 20/1000 | Loss: 0.00003985
Iteration 21/1000 | Loss: 0.00003981
Iteration 22/1000 | Loss: 0.00003981
Iteration 23/1000 | Loss: 0.00003979
Iteration 24/1000 | Loss: 0.00003978
Iteration 25/1000 | Loss: 0.00003978
Iteration 26/1000 | Loss: 0.00003978
Iteration 27/1000 | Loss: 0.00003977
Iteration 28/1000 | Loss: 0.00003976
Iteration 29/1000 | Loss: 0.00003976
Iteration 30/1000 | Loss: 0.00003976
Iteration 31/1000 | Loss: 0.00003975
Iteration 32/1000 | Loss: 0.00003975
Iteration 33/1000 | Loss: 0.00003975
Iteration 34/1000 | Loss: 0.00003975
Iteration 35/1000 | Loss: 0.00003975
Iteration 36/1000 | Loss: 0.00003975
Iteration 37/1000 | Loss: 0.00003975
Iteration 38/1000 | Loss: 0.00003975
Iteration 39/1000 | Loss: 0.00003974
Iteration 40/1000 | Loss: 0.00003974
Iteration 41/1000 | Loss: 0.00003974
Iteration 42/1000 | Loss: 0.00003973
Iteration 43/1000 | Loss: 0.00003973
Iteration 44/1000 | Loss: 0.00003973
Iteration 45/1000 | Loss: 0.00003972
Iteration 46/1000 | Loss: 0.00003971
Iteration 47/1000 | Loss: 0.00003971
Iteration 48/1000 | Loss: 0.00003971
Iteration 49/1000 | Loss: 0.00003971
Iteration 50/1000 | Loss: 0.00003970
Iteration 51/1000 | Loss: 0.00003970
Iteration 52/1000 | Loss: 0.00003970
Iteration 53/1000 | Loss: 0.00003969
Iteration 54/1000 | Loss: 0.00003969
Iteration 55/1000 | Loss: 0.00003969
Iteration 56/1000 | Loss: 0.00003969
Iteration 57/1000 | Loss: 0.00003969
Iteration 58/1000 | Loss: 0.00003968
Iteration 59/1000 | Loss: 0.00003968
Iteration 60/1000 | Loss: 0.00003968
Iteration 61/1000 | Loss: 0.00003968
Iteration 62/1000 | Loss: 0.00003968
Iteration 63/1000 | Loss: 0.00003967
Iteration 64/1000 | Loss: 0.00003967
Iteration 65/1000 | Loss: 0.00003967
Iteration 66/1000 | Loss: 0.00003967
Iteration 67/1000 | Loss: 0.00003967
Iteration 68/1000 | Loss: 0.00003967
Iteration 69/1000 | Loss: 0.00003967
Iteration 70/1000 | Loss: 0.00003967
Iteration 71/1000 | Loss: 0.00003967
Iteration 72/1000 | Loss: 0.00003967
Iteration 73/1000 | Loss: 0.00003966
Iteration 74/1000 | Loss: 0.00003966
Iteration 75/1000 | Loss: 0.00003966
Iteration 76/1000 | Loss: 0.00003965
Iteration 77/1000 | Loss: 0.00003965
Iteration 78/1000 | Loss: 0.00003965
Iteration 79/1000 | Loss: 0.00003965
Iteration 80/1000 | Loss: 0.00003965
Iteration 81/1000 | Loss: 0.00003965
Iteration 82/1000 | Loss: 0.00003964
Iteration 83/1000 | Loss: 0.00003964
Iteration 84/1000 | Loss: 0.00003964
Iteration 85/1000 | Loss: 0.00003964
Iteration 86/1000 | Loss: 0.00003964
Iteration 87/1000 | Loss: 0.00003963
Iteration 88/1000 | Loss: 0.00003963
Iteration 89/1000 | Loss: 0.00003963
Iteration 90/1000 | Loss: 0.00003963
Iteration 91/1000 | Loss: 0.00003963
Iteration 92/1000 | Loss: 0.00003963
Iteration 93/1000 | Loss: 0.00003963
Iteration 94/1000 | Loss: 0.00003963
Iteration 95/1000 | Loss: 0.00003963
Iteration 96/1000 | Loss: 0.00003963
Iteration 97/1000 | Loss: 0.00003963
Iteration 98/1000 | Loss: 0.00003963
Iteration 99/1000 | Loss: 0.00003963
Iteration 100/1000 | Loss: 0.00003963
Iteration 101/1000 | Loss: 0.00003963
Iteration 102/1000 | Loss: 0.00003963
Iteration 103/1000 | Loss: 0.00003963
Iteration 104/1000 | Loss: 0.00003963
Iteration 105/1000 | Loss: 0.00003963
Iteration 106/1000 | Loss: 0.00003963
Iteration 107/1000 | Loss: 0.00003963
Iteration 108/1000 | Loss: 0.00003963
Iteration 109/1000 | Loss: 0.00003963
Iteration 110/1000 | Loss: 0.00003963
Iteration 111/1000 | Loss: 0.00003963
Iteration 112/1000 | Loss: 0.00003963
Iteration 113/1000 | Loss: 0.00003963
Iteration 114/1000 | Loss: 0.00003963
Iteration 115/1000 | Loss: 0.00003963
Iteration 116/1000 | Loss: 0.00003963
Iteration 117/1000 | Loss: 0.00003963
Iteration 118/1000 | Loss: 0.00003963
Iteration 119/1000 | Loss: 0.00003963
Iteration 120/1000 | Loss: 0.00003963
Iteration 121/1000 | Loss: 0.00003963
Iteration 122/1000 | Loss: 0.00003963
Iteration 123/1000 | Loss: 0.00003963
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [3.9629507227800786e-05, 3.9629507227800786e-05, 3.9629507227800786e-05, 3.9629507227800786e-05, 3.9629507227800786e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.9629507227800786e-05

Optimization complete. Final v2v error: 5.4712138175964355 mm

Highest mean error: 5.847931861877441 mm for frame 34

Lowest mean error: 5.079269886016846 mm for frame 223

Saving results

Total time: 43.97231125831604
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_58_us_2424/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_58_us_2424/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_58_us_2424/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00944275
Iteration 2/25 | Loss: 0.00237872
Iteration 3/25 | Loss: 0.00222192
Iteration 4/25 | Loss: 0.00220019
Iteration 5/25 | Loss: 0.00219337
Iteration 6/25 | Loss: 0.00219123
Iteration 7/25 | Loss: 0.00219101
Iteration 8/25 | Loss: 0.00219101
Iteration 9/25 | Loss: 0.00219101
Iteration 10/25 | Loss: 0.00219101
Iteration 11/25 | Loss: 0.00219101
Iteration 12/25 | Loss: 0.00219101
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0021910052746534348, 0.0021910052746534348, 0.0021910052746534348, 0.0021910052746534348, 0.0021910052746534348]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021910052746534348

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.69499278
Iteration 2/25 | Loss: 0.00215239
Iteration 3/25 | Loss: 0.00215231
Iteration 4/25 | Loss: 0.00215230
Iteration 5/25 | Loss: 0.00215230
Iteration 6/25 | Loss: 0.00215230
Iteration 7/25 | Loss: 0.00215230
Iteration 8/25 | Loss: 0.00215230
Iteration 9/25 | Loss: 0.00215230
Iteration 10/25 | Loss: 0.00215230
Iteration 11/25 | Loss: 0.00215230
Iteration 12/25 | Loss: 0.00215230
Iteration 13/25 | Loss: 0.00215230
Iteration 14/25 | Loss: 0.00215230
Iteration 15/25 | Loss: 0.00215230
Iteration 16/25 | Loss: 0.00215230
Iteration 17/25 | Loss: 0.00215230
Iteration 18/25 | Loss: 0.00215230
Iteration 19/25 | Loss: 0.00215230
Iteration 20/25 | Loss: 0.00215230
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0021523027680814266, 0.0021523027680814266, 0.0021523027680814266, 0.0021523027680814266, 0.0021523027680814266]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021523027680814266

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00215230
Iteration 2/1000 | Loss: 0.00010451
Iteration 3/1000 | Loss: 0.00007203
Iteration 4/1000 | Loss: 0.00006302
Iteration 5/1000 | Loss: 0.00005782
Iteration 6/1000 | Loss: 0.00005435
Iteration 7/1000 | Loss: 0.00005171
Iteration 8/1000 | Loss: 0.00004928
Iteration 9/1000 | Loss: 0.00004788
Iteration 10/1000 | Loss: 0.00004659
Iteration 11/1000 | Loss: 0.00004564
Iteration 12/1000 | Loss: 0.00004498
Iteration 13/1000 | Loss: 0.00004435
Iteration 14/1000 | Loss: 0.00004386
Iteration 15/1000 | Loss: 0.00004357
Iteration 16/1000 | Loss: 0.00004341
Iteration 17/1000 | Loss: 0.00004323
Iteration 18/1000 | Loss: 0.00004309
Iteration 19/1000 | Loss: 0.00004305
Iteration 20/1000 | Loss: 0.00004300
Iteration 21/1000 | Loss: 0.00004293
Iteration 22/1000 | Loss: 0.00004288
Iteration 23/1000 | Loss: 0.00004288
Iteration 24/1000 | Loss: 0.00004287
Iteration 25/1000 | Loss: 0.00004287
Iteration 26/1000 | Loss: 0.00004286
Iteration 27/1000 | Loss: 0.00004282
Iteration 28/1000 | Loss: 0.00004281
Iteration 29/1000 | Loss: 0.00004281
Iteration 30/1000 | Loss: 0.00004278
Iteration 31/1000 | Loss: 0.00004278
Iteration 32/1000 | Loss: 0.00004278
Iteration 33/1000 | Loss: 0.00004278
Iteration 34/1000 | Loss: 0.00004278
Iteration 35/1000 | Loss: 0.00004278
Iteration 36/1000 | Loss: 0.00004278
Iteration 37/1000 | Loss: 0.00004278
Iteration 38/1000 | Loss: 0.00004277
Iteration 39/1000 | Loss: 0.00004277
Iteration 40/1000 | Loss: 0.00004277
Iteration 41/1000 | Loss: 0.00004277
Iteration 42/1000 | Loss: 0.00004277
Iteration 43/1000 | Loss: 0.00004277
Iteration 44/1000 | Loss: 0.00004277
Iteration 45/1000 | Loss: 0.00004276
Iteration 46/1000 | Loss: 0.00004275
Iteration 47/1000 | Loss: 0.00004274
Iteration 48/1000 | Loss: 0.00004274
Iteration 49/1000 | Loss: 0.00004274
Iteration 50/1000 | Loss: 0.00004273
Iteration 51/1000 | Loss: 0.00004273
Iteration 52/1000 | Loss: 0.00004273
Iteration 53/1000 | Loss: 0.00004272
Iteration 54/1000 | Loss: 0.00004272
Iteration 55/1000 | Loss: 0.00004271
Iteration 56/1000 | Loss: 0.00004271
Iteration 57/1000 | Loss: 0.00004270
Iteration 58/1000 | Loss: 0.00004270
Iteration 59/1000 | Loss: 0.00004270
Iteration 60/1000 | Loss: 0.00004270
Iteration 61/1000 | Loss: 0.00004270
Iteration 62/1000 | Loss: 0.00004269
Iteration 63/1000 | Loss: 0.00004269
Iteration 64/1000 | Loss: 0.00004269
Iteration 65/1000 | Loss: 0.00004269
Iteration 66/1000 | Loss: 0.00004269
Iteration 67/1000 | Loss: 0.00004269
Iteration 68/1000 | Loss: 0.00004269
Iteration 69/1000 | Loss: 0.00004268
Iteration 70/1000 | Loss: 0.00004266
Iteration 71/1000 | Loss: 0.00004266
Iteration 72/1000 | Loss: 0.00004266
Iteration 73/1000 | Loss: 0.00004266
Iteration 74/1000 | Loss: 0.00004266
Iteration 75/1000 | Loss: 0.00004266
Iteration 76/1000 | Loss: 0.00004266
Iteration 77/1000 | Loss: 0.00004265
Iteration 78/1000 | Loss: 0.00004265
Iteration 79/1000 | Loss: 0.00004265
Iteration 80/1000 | Loss: 0.00004265
Iteration 81/1000 | Loss: 0.00004265
Iteration 82/1000 | Loss: 0.00004265
Iteration 83/1000 | Loss: 0.00004264
Iteration 84/1000 | Loss: 0.00004263
Iteration 85/1000 | Loss: 0.00004263
Iteration 86/1000 | Loss: 0.00004262
Iteration 87/1000 | Loss: 0.00004262
Iteration 88/1000 | Loss: 0.00004262
Iteration 89/1000 | Loss: 0.00004262
Iteration 90/1000 | Loss: 0.00004262
Iteration 91/1000 | Loss: 0.00004262
Iteration 92/1000 | Loss: 0.00004262
Iteration 93/1000 | Loss: 0.00004262
Iteration 94/1000 | Loss: 0.00004262
Iteration 95/1000 | Loss: 0.00004262
Iteration 96/1000 | Loss: 0.00004262
Iteration 97/1000 | Loss: 0.00004262
Iteration 98/1000 | Loss: 0.00004261
Iteration 99/1000 | Loss: 0.00004261
Iteration 100/1000 | Loss: 0.00004261
Iteration 101/1000 | Loss: 0.00004261
Iteration 102/1000 | Loss: 0.00004260
Iteration 103/1000 | Loss: 0.00004260
Iteration 104/1000 | Loss: 0.00004260
Iteration 105/1000 | Loss: 0.00004259
Iteration 106/1000 | Loss: 0.00004259
Iteration 107/1000 | Loss: 0.00004259
Iteration 108/1000 | Loss: 0.00004259
Iteration 109/1000 | Loss: 0.00004259
Iteration 110/1000 | Loss: 0.00004259
Iteration 111/1000 | Loss: 0.00004258
Iteration 112/1000 | Loss: 0.00004258
Iteration 113/1000 | Loss: 0.00004258
Iteration 114/1000 | Loss: 0.00004258
Iteration 115/1000 | Loss: 0.00004258
Iteration 116/1000 | Loss: 0.00004258
Iteration 117/1000 | Loss: 0.00004258
Iteration 118/1000 | Loss: 0.00004258
Iteration 119/1000 | Loss: 0.00004258
Iteration 120/1000 | Loss: 0.00004258
Iteration 121/1000 | Loss: 0.00004258
Iteration 122/1000 | Loss: 0.00004257
Iteration 123/1000 | Loss: 0.00004257
Iteration 124/1000 | Loss: 0.00004257
Iteration 125/1000 | Loss: 0.00004257
Iteration 126/1000 | Loss: 0.00004257
Iteration 127/1000 | Loss: 0.00004257
Iteration 128/1000 | Loss: 0.00004257
Iteration 129/1000 | Loss: 0.00004256
Iteration 130/1000 | Loss: 0.00004256
Iteration 131/1000 | Loss: 0.00004256
Iteration 132/1000 | Loss: 0.00004256
Iteration 133/1000 | Loss: 0.00004256
Iteration 134/1000 | Loss: 0.00004256
Iteration 135/1000 | Loss: 0.00004256
Iteration 136/1000 | Loss: 0.00004256
Iteration 137/1000 | Loss: 0.00004256
Iteration 138/1000 | Loss: 0.00004256
Iteration 139/1000 | Loss: 0.00004256
Iteration 140/1000 | Loss: 0.00004256
Iteration 141/1000 | Loss: 0.00004256
Iteration 142/1000 | Loss: 0.00004256
Iteration 143/1000 | Loss: 0.00004256
Iteration 144/1000 | Loss: 0.00004255
Iteration 145/1000 | Loss: 0.00004255
Iteration 146/1000 | Loss: 0.00004255
Iteration 147/1000 | Loss: 0.00004255
Iteration 148/1000 | Loss: 0.00004255
Iteration 149/1000 | Loss: 0.00004255
Iteration 150/1000 | Loss: 0.00004255
Iteration 151/1000 | Loss: 0.00004255
Iteration 152/1000 | Loss: 0.00004255
Iteration 153/1000 | Loss: 0.00004255
Iteration 154/1000 | Loss: 0.00004255
Iteration 155/1000 | Loss: 0.00004254
Iteration 156/1000 | Loss: 0.00004254
Iteration 157/1000 | Loss: 0.00004254
Iteration 158/1000 | Loss: 0.00004254
Iteration 159/1000 | Loss: 0.00004253
Iteration 160/1000 | Loss: 0.00004253
Iteration 161/1000 | Loss: 0.00004253
Iteration 162/1000 | Loss: 0.00004253
Iteration 163/1000 | Loss: 0.00004253
Iteration 164/1000 | Loss: 0.00004253
Iteration 165/1000 | Loss: 0.00004253
Iteration 166/1000 | Loss: 0.00004253
Iteration 167/1000 | Loss: 0.00004253
Iteration 168/1000 | Loss: 0.00004253
Iteration 169/1000 | Loss: 0.00004253
Iteration 170/1000 | Loss: 0.00004253
Iteration 171/1000 | Loss: 0.00004253
Iteration 172/1000 | Loss: 0.00004253
Iteration 173/1000 | Loss: 0.00004253
Iteration 174/1000 | Loss: 0.00004253
Iteration 175/1000 | Loss: 0.00004252
Iteration 176/1000 | Loss: 0.00004252
Iteration 177/1000 | Loss: 0.00004252
Iteration 178/1000 | Loss: 0.00004252
Iteration 179/1000 | Loss: 0.00004252
Iteration 180/1000 | Loss: 0.00004252
Iteration 181/1000 | Loss: 0.00004251
Iteration 182/1000 | Loss: 0.00004251
Iteration 183/1000 | Loss: 0.00004251
Iteration 184/1000 | Loss: 0.00004251
Iteration 185/1000 | Loss: 0.00004251
Iteration 186/1000 | Loss: 0.00004251
Iteration 187/1000 | Loss: 0.00004251
Iteration 188/1000 | Loss: 0.00004251
Iteration 189/1000 | Loss: 0.00004251
Iteration 190/1000 | Loss: 0.00004251
Iteration 191/1000 | Loss: 0.00004251
Iteration 192/1000 | Loss: 0.00004251
Iteration 193/1000 | Loss: 0.00004251
Iteration 194/1000 | Loss: 0.00004251
Iteration 195/1000 | Loss: 0.00004251
Iteration 196/1000 | Loss: 0.00004251
Iteration 197/1000 | Loss: 0.00004251
Iteration 198/1000 | Loss: 0.00004251
Iteration 199/1000 | Loss: 0.00004251
Iteration 200/1000 | Loss: 0.00004251
Iteration 201/1000 | Loss: 0.00004251
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [4.251257632859051e-05, 4.251257632859051e-05, 4.251257632859051e-05, 4.251257632859051e-05, 4.251257632859051e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.251257632859051e-05

Optimization complete. Final v2v error: 5.589107513427734 mm

Highest mean error: 6.074408531188965 mm for frame 15

Lowest mean error: 5.134950160980225 mm for frame 39

Saving results

Total time: 57.25522565841675
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_58_us_2424/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_58_us_2424/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_58_us_2424/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01158347
Iteration 2/25 | Loss: 0.00254626
Iteration 3/25 | Loss: 0.00222886
Iteration 4/25 | Loss: 0.00217402
Iteration 5/25 | Loss: 0.00219900
Iteration 6/25 | Loss: 0.00215446
Iteration 7/25 | Loss: 0.00211627
Iteration 8/25 | Loss: 0.00211826
Iteration 9/25 | Loss: 0.00211427
Iteration 10/25 | Loss: 0.00211088
Iteration 11/25 | Loss: 0.00211046
Iteration 12/25 | Loss: 0.00210901
Iteration 13/25 | Loss: 0.00210520
Iteration 14/25 | Loss: 0.00210208
Iteration 15/25 | Loss: 0.00210148
Iteration 16/25 | Loss: 0.00210139
Iteration 17/25 | Loss: 0.00210138
Iteration 18/25 | Loss: 0.00210138
Iteration 19/25 | Loss: 0.00210137
Iteration 20/25 | Loss: 0.00210137
Iteration 21/25 | Loss: 0.00210137
Iteration 22/25 | Loss: 0.00210137
Iteration 23/25 | Loss: 0.00210137
Iteration 24/25 | Loss: 0.00210137
Iteration 25/25 | Loss: 0.00210137

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 26.22223854
Iteration 2/25 | Loss: 0.00179283
Iteration 3/25 | Loss: 0.00179274
Iteration 4/25 | Loss: 0.00179274
Iteration 5/25 | Loss: 0.00179274
Iteration 6/25 | Loss: 0.00179274
Iteration 7/25 | Loss: 0.00179274
Iteration 8/25 | Loss: 0.00179274
Iteration 9/25 | Loss: 0.00179274
Iteration 10/25 | Loss: 0.00179274
Iteration 11/25 | Loss: 0.00179274
Iteration 12/25 | Loss: 0.00179274
Iteration 13/25 | Loss: 0.00179274
Iteration 14/25 | Loss: 0.00179274
Iteration 15/25 | Loss: 0.00179274
Iteration 16/25 | Loss: 0.00179274
Iteration 17/25 | Loss: 0.00179274
Iteration 18/25 | Loss: 0.00179274
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0017927364679053426, 0.0017927364679053426, 0.0017927364679053426, 0.0017927364679053426, 0.0017927364679053426]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017927364679053426

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00179274
Iteration 2/1000 | Loss: 0.00008893
Iteration 3/1000 | Loss: 0.00005974
Iteration 4/1000 | Loss: 0.00005118
Iteration 5/1000 | Loss: 0.00004775
Iteration 6/1000 | Loss: 0.00004567
Iteration 7/1000 | Loss: 0.00004430
Iteration 8/1000 | Loss: 0.00004352
Iteration 9/1000 | Loss: 0.00004293
Iteration 10/1000 | Loss: 0.00004236
Iteration 11/1000 | Loss: 0.00004179
Iteration 12/1000 | Loss: 0.00004144
Iteration 13/1000 | Loss: 0.00004120
Iteration 14/1000 | Loss: 0.00004119
Iteration 15/1000 | Loss: 0.00004106
Iteration 16/1000 | Loss: 0.00004104
Iteration 17/1000 | Loss: 0.00004103
Iteration 18/1000 | Loss: 0.00004102
Iteration 19/1000 | Loss: 0.00004101
Iteration 20/1000 | Loss: 0.00004099
Iteration 21/1000 | Loss: 0.00004098
Iteration 22/1000 | Loss: 0.00004092
Iteration 23/1000 | Loss: 0.00004083
Iteration 24/1000 | Loss: 0.00004082
Iteration 25/1000 | Loss: 0.00004082
Iteration 26/1000 | Loss: 0.00004079
Iteration 27/1000 | Loss: 0.00004079
Iteration 28/1000 | Loss: 0.00004079
Iteration 29/1000 | Loss: 0.00004077
Iteration 30/1000 | Loss: 0.00004077
Iteration 31/1000 | Loss: 0.00004076
Iteration 32/1000 | Loss: 0.00004075
Iteration 33/1000 | Loss: 0.00004075
Iteration 34/1000 | Loss: 0.00004075
Iteration 35/1000 | Loss: 0.00004074
Iteration 36/1000 | Loss: 0.00004074
Iteration 37/1000 | Loss: 0.00004074
Iteration 38/1000 | Loss: 0.00004074
Iteration 39/1000 | Loss: 0.00004074
Iteration 40/1000 | Loss: 0.00004073
Iteration 41/1000 | Loss: 0.00004073
Iteration 42/1000 | Loss: 0.00004073
Iteration 43/1000 | Loss: 0.00004073
Iteration 44/1000 | Loss: 0.00004073
Iteration 45/1000 | Loss: 0.00004073
Iteration 46/1000 | Loss: 0.00004073
Iteration 47/1000 | Loss: 0.00004073
Iteration 48/1000 | Loss: 0.00004073
Iteration 49/1000 | Loss: 0.00004073
Iteration 50/1000 | Loss: 0.00004073
Iteration 51/1000 | Loss: 0.00004073
Iteration 52/1000 | Loss: 0.00004073
Iteration 53/1000 | Loss: 0.00004072
Iteration 54/1000 | Loss: 0.00004072
Iteration 55/1000 | Loss: 0.00004072
Iteration 56/1000 | Loss: 0.00004072
Iteration 57/1000 | Loss: 0.00004072
Iteration 58/1000 | Loss: 0.00004072
Iteration 59/1000 | Loss: 0.00004072
Iteration 60/1000 | Loss: 0.00004072
Iteration 61/1000 | Loss: 0.00004072
Iteration 62/1000 | Loss: 0.00004072
Iteration 63/1000 | Loss: 0.00004072
Iteration 64/1000 | Loss: 0.00004072
Iteration 65/1000 | Loss: 0.00004072
Iteration 66/1000 | Loss: 0.00004072
Iteration 67/1000 | Loss: 0.00004072
Iteration 68/1000 | Loss: 0.00004072
Iteration 69/1000 | Loss: 0.00004072
Iteration 70/1000 | Loss: 0.00004072
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 70. Stopping optimization.
Last 5 losses: [4.071689545526169e-05, 4.071689545526169e-05, 4.071689545526169e-05, 4.071689545526169e-05, 4.071689545526169e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.071689545526169e-05

Optimization complete. Final v2v error: 5.510478496551514 mm

Highest mean error: 6.260033130645752 mm for frame 7

Lowest mean error: 5.233886241912842 mm for frame 52

Saving results

Total time: 55.8412971496582
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_58_us_2424/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_58_us_2424/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_58_us_2424/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00980040
Iteration 2/25 | Loss: 0.00222946
Iteration 3/25 | Loss: 0.00209541
Iteration 4/25 | Loss: 0.00208106
Iteration 5/25 | Loss: 0.00207786
Iteration 6/25 | Loss: 0.00207689
Iteration 7/25 | Loss: 0.00207689
Iteration 8/25 | Loss: 0.00207689
Iteration 9/25 | Loss: 0.00207689
Iteration 10/25 | Loss: 0.00207689
Iteration 11/25 | Loss: 0.00207689
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.002076893113553524, 0.002076893113553524, 0.002076893113553524, 0.002076893113553524, 0.002076893113553524]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002076893113553524

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43721569
Iteration 2/25 | Loss: 0.00182764
Iteration 3/25 | Loss: 0.00182764
Iteration 4/25 | Loss: 0.00182764
Iteration 5/25 | Loss: 0.00182764
Iteration 6/25 | Loss: 0.00182763
Iteration 7/25 | Loss: 0.00182763
Iteration 8/25 | Loss: 0.00182763
Iteration 9/25 | Loss: 0.00182763
Iteration 10/25 | Loss: 0.00182763
Iteration 11/25 | Loss: 0.00182763
Iteration 12/25 | Loss: 0.00182763
Iteration 13/25 | Loss: 0.00182763
Iteration 14/25 | Loss: 0.00182763
Iteration 15/25 | Loss: 0.00182763
Iteration 16/25 | Loss: 0.00182763
Iteration 17/25 | Loss: 0.00182763
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0018276333576068282, 0.0018276333576068282, 0.0018276333576068282, 0.0018276333576068282, 0.0018276333576068282]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018276333576068282

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00182763
Iteration 2/1000 | Loss: 0.00008193
Iteration 3/1000 | Loss: 0.00005737
Iteration 4/1000 | Loss: 0.00005220
Iteration 5/1000 | Loss: 0.00004893
Iteration 6/1000 | Loss: 0.00004703
Iteration 7/1000 | Loss: 0.00004543
Iteration 8/1000 | Loss: 0.00004460
Iteration 9/1000 | Loss: 0.00004391
Iteration 10/1000 | Loss: 0.00004335
Iteration 11/1000 | Loss: 0.00004300
Iteration 12/1000 | Loss: 0.00004269
Iteration 13/1000 | Loss: 0.00004236
Iteration 14/1000 | Loss: 0.00004223
Iteration 15/1000 | Loss: 0.00004213
Iteration 16/1000 | Loss: 0.00004201
Iteration 17/1000 | Loss: 0.00004196
Iteration 18/1000 | Loss: 0.00004186
Iteration 19/1000 | Loss: 0.00004183
Iteration 20/1000 | Loss: 0.00004182
Iteration 21/1000 | Loss: 0.00004182
Iteration 22/1000 | Loss: 0.00004181
Iteration 23/1000 | Loss: 0.00004181
Iteration 24/1000 | Loss: 0.00004180
Iteration 25/1000 | Loss: 0.00004180
Iteration 26/1000 | Loss: 0.00004178
Iteration 27/1000 | Loss: 0.00004178
Iteration 28/1000 | Loss: 0.00004178
Iteration 29/1000 | Loss: 0.00004178
Iteration 30/1000 | Loss: 0.00004177
Iteration 31/1000 | Loss: 0.00004177
Iteration 32/1000 | Loss: 0.00004177
Iteration 33/1000 | Loss: 0.00004177
Iteration 34/1000 | Loss: 0.00004177
Iteration 35/1000 | Loss: 0.00004177
Iteration 36/1000 | Loss: 0.00004176
Iteration 37/1000 | Loss: 0.00004176
Iteration 38/1000 | Loss: 0.00004176
Iteration 39/1000 | Loss: 0.00004176
Iteration 40/1000 | Loss: 0.00004176
Iteration 41/1000 | Loss: 0.00004176
Iteration 42/1000 | Loss: 0.00004176
Iteration 43/1000 | Loss: 0.00004176
Iteration 44/1000 | Loss: 0.00004176
Iteration 45/1000 | Loss: 0.00004176
Iteration 46/1000 | Loss: 0.00004175
Iteration 47/1000 | Loss: 0.00004175
Iteration 48/1000 | Loss: 0.00004175
Iteration 49/1000 | Loss: 0.00004175
Iteration 50/1000 | Loss: 0.00004175
Iteration 51/1000 | Loss: 0.00004175
Iteration 52/1000 | Loss: 0.00004175
Iteration 53/1000 | Loss: 0.00004175
Iteration 54/1000 | Loss: 0.00004175
Iteration 55/1000 | Loss: 0.00004175
Iteration 56/1000 | Loss: 0.00004175
Iteration 57/1000 | Loss: 0.00004175
Iteration 58/1000 | Loss: 0.00004175
Iteration 59/1000 | Loss: 0.00004174
Iteration 60/1000 | Loss: 0.00004174
Iteration 61/1000 | Loss: 0.00004174
Iteration 62/1000 | Loss: 0.00004174
Iteration 63/1000 | Loss: 0.00004174
Iteration 64/1000 | Loss: 0.00004174
Iteration 65/1000 | Loss: 0.00004174
Iteration 66/1000 | Loss: 0.00004174
Iteration 67/1000 | Loss: 0.00004174
Iteration 68/1000 | Loss: 0.00004173
Iteration 69/1000 | Loss: 0.00004173
Iteration 70/1000 | Loss: 0.00004173
Iteration 71/1000 | Loss: 0.00004173
Iteration 72/1000 | Loss: 0.00004173
Iteration 73/1000 | Loss: 0.00004173
Iteration 74/1000 | Loss: 0.00004173
Iteration 75/1000 | Loss: 0.00004173
Iteration 76/1000 | Loss: 0.00004172
Iteration 77/1000 | Loss: 0.00004172
Iteration 78/1000 | Loss: 0.00004172
Iteration 79/1000 | Loss: 0.00004172
Iteration 80/1000 | Loss: 0.00004171
Iteration 81/1000 | Loss: 0.00004171
Iteration 82/1000 | Loss: 0.00004171
Iteration 83/1000 | Loss: 0.00004170
Iteration 84/1000 | Loss: 0.00004170
Iteration 85/1000 | Loss: 0.00004170
Iteration 86/1000 | Loss: 0.00004170
Iteration 87/1000 | Loss: 0.00004169
Iteration 88/1000 | Loss: 0.00004169
Iteration 89/1000 | Loss: 0.00004168
Iteration 90/1000 | Loss: 0.00004168
Iteration 91/1000 | Loss: 0.00004168
Iteration 92/1000 | Loss: 0.00004168
Iteration 93/1000 | Loss: 0.00004168
Iteration 94/1000 | Loss: 0.00004168
Iteration 95/1000 | Loss: 0.00004168
Iteration 96/1000 | Loss: 0.00004168
Iteration 97/1000 | Loss: 0.00004168
Iteration 98/1000 | Loss: 0.00004168
Iteration 99/1000 | Loss: 0.00004168
Iteration 100/1000 | Loss: 0.00004168
Iteration 101/1000 | Loss: 0.00004168
Iteration 102/1000 | Loss: 0.00004168
Iteration 103/1000 | Loss: 0.00004167
Iteration 104/1000 | Loss: 0.00004167
Iteration 105/1000 | Loss: 0.00004167
Iteration 106/1000 | Loss: 0.00004167
Iteration 107/1000 | Loss: 0.00004167
Iteration 108/1000 | Loss: 0.00004167
Iteration 109/1000 | Loss: 0.00004167
Iteration 110/1000 | Loss: 0.00004167
Iteration 111/1000 | Loss: 0.00004167
Iteration 112/1000 | Loss: 0.00004167
Iteration 113/1000 | Loss: 0.00004167
Iteration 114/1000 | Loss: 0.00004166
Iteration 115/1000 | Loss: 0.00004166
Iteration 116/1000 | Loss: 0.00004166
Iteration 117/1000 | Loss: 0.00004166
Iteration 118/1000 | Loss: 0.00004166
Iteration 119/1000 | Loss: 0.00004166
Iteration 120/1000 | Loss: 0.00004166
Iteration 121/1000 | Loss: 0.00004166
Iteration 122/1000 | Loss: 0.00004166
Iteration 123/1000 | Loss: 0.00004166
Iteration 124/1000 | Loss: 0.00004166
Iteration 125/1000 | Loss: 0.00004166
Iteration 126/1000 | Loss: 0.00004166
Iteration 127/1000 | Loss: 0.00004166
Iteration 128/1000 | Loss: 0.00004165
Iteration 129/1000 | Loss: 0.00004165
Iteration 130/1000 | Loss: 0.00004165
Iteration 131/1000 | Loss: 0.00004165
Iteration 132/1000 | Loss: 0.00004165
Iteration 133/1000 | Loss: 0.00004165
Iteration 134/1000 | Loss: 0.00004165
Iteration 135/1000 | Loss: 0.00004165
Iteration 136/1000 | Loss: 0.00004164
Iteration 137/1000 | Loss: 0.00004164
Iteration 138/1000 | Loss: 0.00004164
Iteration 139/1000 | Loss: 0.00004164
Iteration 140/1000 | Loss: 0.00004164
Iteration 141/1000 | Loss: 0.00004164
Iteration 142/1000 | Loss: 0.00004164
Iteration 143/1000 | Loss: 0.00004164
Iteration 144/1000 | Loss: 0.00004164
Iteration 145/1000 | Loss: 0.00004164
Iteration 146/1000 | Loss: 0.00004164
Iteration 147/1000 | Loss: 0.00004164
Iteration 148/1000 | Loss: 0.00004164
Iteration 149/1000 | Loss: 0.00004164
Iteration 150/1000 | Loss: 0.00004164
Iteration 151/1000 | Loss: 0.00004164
Iteration 152/1000 | Loss: 0.00004164
Iteration 153/1000 | Loss: 0.00004164
Iteration 154/1000 | Loss: 0.00004164
Iteration 155/1000 | Loss: 0.00004164
Iteration 156/1000 | Loss: 0.00004164
Iteration 157/1000 | Loss: 0.00004164
Iteration 158/1000 | Loss: 0.00004164
Iteration 159/1000 | Loss: 0.00004163
Iteration 160/1000 | Loss: 0.00004163
Iteration 161/1000 | Loss: 0.00004163
Iteration 162/1000 | Loss: 0.00004163
Iteration 163/1000 | Loss: 0.00004163
Iteration 164/1000 | Loss: 0.00004163
Iteration 165/1000 | Loss: 0.00004163
Iteration 166/1000 | Loss: 0.00004163
Iteration 167/1000 | Loss: 0.00004163
Iteration 168/1000 | Loss: 0.00004163
Iteration 169/1000 | Loss: 0.00004163
Iteration 170/1000 | Loss: 0.00004163
Iteration 171/1000 | Loss: 0.00004163
Iteration 172/1000 | Loss: 0.00004163
Iteration 173/1000 | Loss: 0.00004163
Iteration 174/1000 | Loss: 0.00004163
Iteration 175/1000 | Loss: 0.00004163
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [4.163384801358916e-05, 4.163384801358916e-05, 4.163384801358916e-05, 4.163384801358916e-05, 4.163384801358916e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.163384801358916e-05

Optimization complete. Final v2v error: 5.497048377990723 mm

Highest mean error: 6.423101902008057 mm for frame 111

Lowest mean error: 5.202213287353516 mm for frame 69

Saving results

Total time: 47.31354641914368
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_58_us_2424/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_58_us_2424/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_58_us_2424/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01176911
Iteration 2/25 | Loss: 0.01176911
Iteration 3/25 | Loss: 0.01176910
Iteration 4/25 | Loss: 0.01176910
Iteration 5/25 | Loss: 0.01176910
Iteration 6/25 | Loss: 0.01176910
Iteration 7/25 | Loss: 0.01176910
Iteration 8/25 | Loss: 0.01176910
Iteration 9/25 | Loss: 0.01176910
Iteration 10/25 | Loss: 0.01176910
Iteration 11/25 | Loss: 0.01176910
Iteration 12/25 | Loss: 0.01176909
Iteration 13/25 | Loss: 0.01176909
Iteration 14/25 | Loss: 0.01176909
Iteration 15/25 | Loss: 0.01176909
Iteration 16/25 | Loss: 0.01176909
Iteration 17/25 | Loss: 0.01176909
Iteration 18/25 | Loss: 0.01176908
Iteration 19/25 | Loss: 0.01176908
Iteration 20/25 | Loss: 0.01176908
Iteration 21/25 | Loss: 0.01176908
Iteration 22/25 | Loss: 0.01176908
Iteration 23/25 | Loss: 0.01176908
Iteration 24/25 | Loss: 0.01176908
Iteration 25/25 | Loss: 0.01176908

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44678700
Iteration 2/25 | Loss: 0.13684015
Iteration 3/25 | Loss: 0.12538420
Iteration 4/25 | Loss: 0.12423453
Iteration 5/25 | Loss: 0.11850838
Iteration 6/25 | Loss: 0.11850730
Iteration 7/25 | Loss: 0.11850730
Iteration 8/25 | Loss: 0.11850730
Iteration 9/25 | Loss: 0.11850727
Iteration 10/25 | Loss: 0.11850727
Iteration 11/25 | Loss: 0.11850727
Iteration 12/25 | Loss: 0.11850727
Iteration 13/25 | Loss: 0.11850727
Iteration 14/25 | Loss: 0.11850727
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.1185072734951973, 0.1185072734951973, 0.1185072734951973, 0.1185072734951973, 0.1185072734951973]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.1185072734951973

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.11850727
Iteration 2/1000 | Loss: 0.00487303
Iteration 3/1000 | Loss: 0.00255682
Iteration 4/1000 | Loss: 0.00243973
Iteration 5/1000 | Loss: 0.00038089
Iteration 6/1000 | Loss: 0.00273369
Iteration 7/1000 | Loss: 0.00032012
Iteration 8/1000 | Loss: 0.00010921
Iteration 9/1000 | Loss: 0.00015648
Iteration 10/1000 | Loss: 0.00006572
Iteration 11/1000 | Loss: 0.00209738
Iteration 12/1000 | Loss: 0.00005872
Iteration 13/1000 | Loss: 0.00021499
Iteration 14/1000 | Loss: 0.00004973
Iteration 15/1000 | Loss: 0.00004466
Iteration 16/1000 | Loss: 0.00004023
Iteration 17/1000 | Loss: 0.00003818
Iteration 18/1000 | Loss: 0.00003614
Iteration 19/1000 | Loss: 0.00065715
Iteration 20/1000 | Loss: 0.00003603
Iteration 21/1000 | Loss: 0.00003451
Iteration 22/1000 | Loss: 0.00003384
Iteration 23/1000 | Loss: 0.00003340
Iteration 24/1000 | Loss: 0.00003312
Iteration 25/1000 | Loss: 0.00003294
Iteration 26/1000 | Loss: 0.00003273
Iteration 27/1000 | Loss: 0.00003272
Iteration 28/1000 | Loss: 0.00003270
Iteration 29/1000 | Loss: 0.00003257
Iteration 30/1000 | Loss: 0.00003252
Iteration 31/1000 | Loss: 0.00003252
Iteration 32/1000 | Loss: 0.00003251
Iteration 33/1000 | Loss: 0.00003251
Iteration 34/1000 | Loss: 0.00003242
Iteration 35/1000 | Loss: 0.00003241
Iteration 36/1000 | Loss: 0.00003239
Iteration 37/1000 | Loss: 0.00003235
Iteration 38/1000 | Loss: 0.00003235
Iteration 39/1000 | Loss: 0.00003234
Iteration 40/1000 | Loss: 0.00003233
Iteration 41/1000 | Loss: 0.00003233
Iteration 42/1000 | Loss: 0.00003233
Iteration 43/1000 | Loss: 0.00003232
Iteration 44/1000 | Loss: 0.00003231
Iteration 45/1000 | Loss: 0.00003230
Iteration 46/1000 | Loss: 0.00003230
Iteration 47/1000 | Loss: 0.00003230
Iteration 48/1000 | Loss: 0.00003229
Iteration 49/1000 | Loss: 0.00003229
Iteration 50/1000 | Loss: 0.00003229
Iteration 51/1000 | Loss: 0.00003228
Iteration 52/1000 | Loss: 0.00003228
Iteration 53/1000 | Loss: 0.00003227
Iteration 54/1000 | Loss: 0.00003227
Iteration 55/1000 | Loss: 0.00003227
Iteration 56/1000 | Loss: 0.00003226
Iteration 57/1000 | Loss: 0.00003226
Iteration 58/1000 | Loss: 0.00003226
Iteration 59/1000 | Loss: 0.00003226
Iteration 60/1000 | Loss: 0.00003223
Iteration 61/1000 | Loss: 0.00003221
Iteration 62/1000 | Loss: 0.00003221
Iteration 63/1000 | Loss: 0.00003220
Iteration 64/1000 | Loss: 0.00003220
Iteration 65/1000 | Loss: 0.00003219
Iteration 66/1000 | Loss: 0.00003219
Iteration 67/1000 | Loss: 0.00003218
Iteration 68/1000 | Loss: 0.00003218
Iteration 69/1000 | Loss: 0.00003218
Iteration 70/1000 | Loss: 0.00003218
Iteration 71/1000 | Loss: 0.00003218
Iteration 72/1000 | Loss: 0.00003217
Iteration 73/1000 | Loss: 0.00003217
Iteration 74/1000 | Loss: 0.00003217
Iteration 75/1000 | Loss: 0.00003217
Iteration 76/1000 | Loss: 0.00003217
Iteration 77/1000 | Loss: 0.00003217
Iteration 78/1000 | Loss: 0.00003217
Iteration 79/1000 | Loss: 0.00003216
Iteration 80/1000 | Loss: 0.00003216
Iteration 81/1000 | Loss: 0.00003216
Iteration 82/1000 | Loss: 0.00003215
Iteration 83/1000 | Loss: 0.00003215
Iteration 84/1000 | Loss: 0.00003215
Iteration 85/1000 | Loss: 0.00003215
Iteration 86/1000 | Loss: 0.00003215
Iteration 87/1000 | Loss: 0.00003215
Iteration 88/1000 | Loss: 0.00003215
Iteration 89/1000 | Loss: 0.00003215
Iteration 90/1000 | Loss: 0.00003215
Iteration 91/1000 | Loss: 0.00003214
Iteration 92/1000 | Loss: 0.00003214
Iteration 93/1000 | Loss: 0.00003214
Iteration 94/1000 | Loss: 0.00003213
Iteration 95/1000 | Loss: 0.00003213
Iteration 96/1000 | Loss: 0.00003213
Iteration 97/1000 | Loss: 0.00003213
Iteration 98/1000 | Loss: 0.00003213
Iteration 99/1000 | Loss: 0.00003213
Iteration 100/1000 | Loss: 0.00003213
Iteration 101/1000 | Loss: 0.00003212
Iteration 102/1000 | Loss: 0.00003212
Iteration 103/1000 | Loss: 0.00003212
Iteration 104/1000 | Loss: 0.00003212
Iteration 105/1000 | Loss: 0.00003212
Iteration 106/1000 | Loss: 0.00003212
Iteration 107/1000 | Loss: 0.00003212
Iteration 108/1000 | Loss: 0.00003212
Iteration 109/1000 | Loss: 0.00003212
Iteration 110/1000 | Loss: 0.00003212
Iteration 111/1000 | Loss: 0.00003212
Iteration 112/1000 | Loss: 0.00003212
Iteration 113/1000 | Loss: 0.00003212
Iteration 114/1000 | Loss: 0.00003211
Iteration 115/1000 | Loss: 0.00003211
Iteration 116/1000 | Loss: 0.00003211
Iteration 117/1000 | Loss: 0.00003211
Iteration 118/1000 | Loss: 0.00003211
Iteration 119/1000 | Loss: 0.00003210
Iteration 120/1000 | Loss: 0.00003210
Iteration 121/1000 | Loss: 0.00003210
Iteration 122/1000 | Loss: 0.00003210
Iteration 123/1000 | Loss: 0.00003210
Iteration 124/1000 | Loss: 0.00003210
Iteration 125/1000 | Loss: 0.00003210
Iteration 126/1000 | Loss: 0.00003210
Iteration 127/1000 | Loss: 0.00003210
Iteration 128/1000 | Loss: 0.00003210
Iteration 129/1000 | Loss: 0.00003210
Iteration 130/1000 | Loss: 0.00003210
Iteration 131/1000 | Loss: 0.00003210
Iteration 132/1000 | Loss: 0.00003210
Iteration 133/1000 | Loss: 0.00003210
Iteration 134/1000 | Loss: 0.00003210
Iteration 135/1000 | Loss: 0.00003210
Iteration 136/1000 | Loss: 0.00003210
Iteration 137/1000 | Loss: 0.00003210
Iteration 138/1000 | Loss: 0.00003210
Iteration 139/1000 | Loss: 0.00003210
Iteration 140/1000 | Loss: 0.00003210
Iteration 141/1000 | Loss: 0.00003210
Iteration 142/1000 | Loss: 0.00003210
Iteration 143/1000 | Loss: 0.00003210
Iteration 144/1000 | Loss: 0.00003210
Iteration 145/1000 | Loss: 0.00003210
Iteration 146/1000 | Loss: 0.00003210
Iteration 147/1000 | Loss: 0.00003210
Iteration 148/1000 | Loss: 0.00003210
Iteration 149/1000 | Loss: 0.00003210
Iteration 150/1000 | Loss: 0.00003210
Iteration 151/1000 | Loss: 0.00003210
Iteration 152/1000 | Loss: 0.00003210
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [3.209783244528808e-05, 3.209783244528808e-05, 3.209783244528808e-05, 3.209783244528808e-05, 3.209783244528808e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.209783244528808e-05

Optimization complete. Final v2v error: 4.7550225257873535 mm

Highest mean error: 11.297246932983398 mm for frame 28

Lowest mean error: 4.380350589752197 mm for frame 162

Saving results

Total time: 65.7695381641388
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_58_us_2424/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_58_us_2424/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_58_us_2424/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00488883
Iteration 2/25 | Loss: 0.00217966
Iteration 3/25 | Loss: 0.00208493
Iteration 4/25 | Loss: 0.00207230
Iteration 5/25 | Loss: 0.00206511
Iteration 6/25 | Loss: 0.00206418
Iteration 7/25 | Loss: 0.00206418
Iteration 8/25 | Loss: 0.00206418
Iteration 9/25 | Loss: 0.00206418
Iteration 10/25 | Loss: 0.00206418
Iteration 11/25 | Loss: 0.00206418
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0020641786977648735, 0.0020641786977648735, 0.0020641786977648735, 0.0020641786977648735, 0.0020641786977648735]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020641786977648735

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54008806
Iteration 2/25 | Loss: 0.00172070
Iteration 3/25 | Loss: 0.00172070
Iteration 4/25 | Loss: 0.00172070
Iteration 5/25 | Loss: 0.00172070
Iteration 6/25 | Loss: 0.00172070
Iteration 7/25 | Loss: 0.00172070
Iteration 8/25 | Loss: 0.00172070
Iteration 9/25 | Loss: 0.00172070
Iteration 10/25 | Loss: 0.00172070
Iteration 11/25 | Loss: 0.00172070
Iteration 12/25 | Loss: 0.00172070
Iteration 13/25 | Loss: 0.00172070
Iteration 14/25 | Loss: 0.00172070
Iteration 15/25 | Loss: 0.00172070
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0017206997144967318, 0.0017206997144967318, 0.0017206997144967318, 0.0017206997144967318, 0.0017206997144967318]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017206997144967318

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00172070
Iteration 2/1000 | Loss: 0.00006924
Iteration 3/1000 | Loss: 0.00005342
Iteration 4/1000 | Loss: 0.00004910
Iteration 5/1000 | Loss: 0.00004731
Iteration 6/1000 | Loss: 0.00004561
Iteration 7/1000 | Loss: 0.00004477
Iteration 8/1000 | Loss: 0.00004416
Iteration 9/1000 | Loss: 0.00004367
Iteration 10/1000 | Loss: 0.00004315
Iteration 11/1000 | Loss: 0.00004274
Iteration 12/1000 | Loss: 0.00004245
Iteration 13/1000 | Loss: 0.00004226
Iteration 14/1000 | Loss: 0.00004223
Iteration 15/1000 | Loss: 0.00004219
Iteration 16/1000 | Loss: 0.00004218
Iteration 17/1000 | Loss: 0.00004216
Iteration 18/1000 | Loss: 0.00004215
Iteration 19/1000 | Loss: 0.00004215
Iteration 20/1000 | Loss: 0.00004212
Iteration 21/1000 | Loss: 0.00004212
Iteration 22/1000 | Loss: 0.00004212
Iteration 23/1000 | Loss: 0.00004212
Iteration 24/1000 | Loss: 0.00004211
Iteration 25/1000 | Loss: 0.00004211
Iteration 26/1000 | Loss: 0.00004211
Iteration 27/1000 | Loss: 0.00004210
Iteration 28/1000 | Loss: 0.00004210
Iteration 29/1000 | Loss: 0.00004210
Iteration 30/1000 | Loss: 0.00004209
Iteration 31/1000 | Loss: 0.00004209
Iteration 32/1000 | Loss: 0.00004209
Iteration 33/1000 | Loss: 0.00004209
Iteration 34/1000 | Loss: 0.00004208
Iteration 35/1000 | Loss: 0.00004208
Iteration 36/1000 | Loss: 0.00004208
Iteration 37/1000 | Loss: 0.00004208
Iteration 38/1000 | Loss: 0.00004208
Iteration 39/1000 | Loss: 0.00004207
Iteration 40/1000 | Loss: 0.00004207
Iteration 41/1000 | Loss: 0.00004207
Iteration 42/1000 | Loss: 0.00004207
Iteration 43/1000 | Loss: 0.00004207
Iteration 44/1000 | Loss: 0.00004207
Iteration 45/1000 | Loss: 0.00004207
Iteration 46/1000 | Loss: 0.00004206
Iteration 47/1000 | Loss: 0.00004206
Iteration 48/1000 | Loss: 0.00004206
Iteration 49/1000 | Loss: 0.00004206
Iteration 50/1000 | Loss: 0.00004206
Iteration 51/1000 | Loss: 0.00004205
Iteration 52/1000 | Loss: 0.00004205
Iteration 53/1000 | Loss: 0.00004205
Iteration 54/1000 | Loss: 0.00004205
Iteration 55/1000 | Loss: 0.00004205
Iteration 56/1000 | Loss: 0.00004205
Iteration 57/1000 | Loss: 0.00004205
Iteration 58/1000 | Loss: 0.00004205
Iteration 59/1000 | Loss: 0.00004205
Iteration 60/1000 | Loss: 0.00004205
Iteration 61/1000 | Loss: 0.00004205
Iteration 62/1000 | Loss: 0.00004205
Iteration 63/1000 | Loss: 0.00004205
Iteration 64/1000 | Loss: 0.00004205
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 64. Stopping optimization.
Last 5 losses: [4.20543146901764e-05, 4.20543146901764e-05, 4.20543146901764e-05, 4.20543146901764e-05, 4.20543146901764e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.20543146901764e-05

Optimization complete. Final v2v error: 5.599251747131348 mm

Highest mean error: 5.956881523132324 mm for frame 180

Lowest mean error: 5.386594772338867 mm for frame 167

Saving results

Total time: 35.67378258705139
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_58_us_2424/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_58_us_2424/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_58_us_2424/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00999338
Iteration 2/25 | Loss: 0.00291079
Iteration 3/25 | Loss: 0.00232830
Iteration 4/25 | Loss: 0.00223421
Iteration 5/25 | Loss: 0.00221745
Iteration 6/25 | Loss: 0.00224339
Iteration 7/25 | Loss: 0.00219786
Iteration 8/25 | Loss: 0.00219402
Iteration 9/25 | Loss: 0.00218313
Iteration 10/25 | Loss: 0.00219423
Iteration 11/25 | Loss: 0.00216893
Iteration 12/25 | Loss: 0.00216585
Iteration 13/25 | Loss: 0.00216293
Iteration 14/25 | Loss: 0.00215514
Iteration 15/25 | Loss: 0.00215915
Iteration 16/25 | Loss: 0.00215566
Iteration 17/25 | Loss: 0.00215007
Iteration 18/25 | Loss: 0.00214805
Iteration 19/25 | Loss: 0.00214742
Iteration 20/25 | Loss: 0.00214726
Iteration 21/25 | Loss: 0.00214710
Iteration 22/25 | Loss: 0.00215067
Iteration 23/25 | Loss: 0.00214816
Iteration 24/25 | Loss: 0.00214739
Iteration 25/25 | Loss: 0.00214613

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.47333002
Iteration 2/25 | Loss: 0.00187847
Iteration 3/25 | Loss: 0.00187438
Iteration 4/25 | Loss: 0.00187436
Iteration 5/25 | Loss: 0.00187436
Iteration 6/25 | Loss: 0.00187437
Iteration 7/25 | Loss: 0.00187437
Iteration 8/25 | Loss: 0.00187437
Iteration 9/25 | Loss: 0.00187437
Iteration 10/25 | Loss: 0.00187437
Iteration 11/25 | Loss: 0.00187437
Iteration 12/25 | Loss: 0.00187437
Iteration 13/25 | Loss: 0.00187437
Iteration 14/25 | Loss: 0.00187437
Iteration 15/25 | Loss: 0.00187437
Iteration 16/25 | Loss: 0.00187437
Iteration 17/25 | Loss: 0.00187437
Iteration 18/25 | Loss: 0.00187437
Iteration 19/25 | Loss: 0.00187437
Iteration 20/25 | Loss: 0.00187437
Iteration 21/25 | Loss: 0.00187437
Iteration 22/25 | Loss: 0.00187437
Iteration 23/25 | Loss: 0.00187437
Iteration 24/25 | Loss: 0.00187437
Iteration 25/25 | Loss: 0.00187437

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00187437
Iteration 2/1000 | Loss: 0.00008978
Iteration 3/1000 | Loss: 0.00005868
Iteration 4/1000 | Loss: 0.00005243
Iteration 5/1000 | Loss: 0.00004983
Iteration 6/1000 | Loss: 0.00004811
Iteration 7/1000 | Loss: 0.00004679
Iteration 8/1000 | Loss: 0.00004592
Iteration 9/1000 | Loss: 0.00004547
Iteration 10/1000 | Loss: 0.00004516
Iteration 11/1000 | Loss: 0.00004484
Iteration 12/1000 | Loss: 0.00004461
Iteration 13/1000 | Loss: 0.00004452
Iteration 14/1000 | Loss: 0.00004460
Iteration 15/1000 | Loss: 0.00004443
Iteration 16/1000 | Loss: 0.00004442
Iteration 17/1000 | Loss: 0.00004440
Iteration 18/1000 | Loss: 0.00004439
Iteration 19/1000 | Loss: 0.00004439
Iteration 20/1000 | Loss: 0.00004439
Iteration 21/1000 | Loss: 0.00004435
Iteration 22/1000 | Loss: 0.00004433
Iteration 23/1000 | Loss: 0.00004432
Iteration 24/1000 | Loss: 0.00004432
Iteration 25/1000 | Loss: 0.00004432
Iteration 26/1000 | Loss: 0.00004432
Iteration 27/1000 | Loss: 0.00004432
Iteration 28/1000 | Loss: 0.00004432
Iteration 29/1000 | Loss: 0.00004432
Iteration 30/1000 | Loss: 0.00004432
Iteration 31/1000 | Loss: 0.00004431
Iteration 32/1000 | Loss: 0.00004431
Iteration 33/1000 | Loss: 0.00004431
Iteration 34/1000 | Loss: 0.00004440
Iteration 35/1000 | Loss: 0.00004430
Iteration 36/1000 | Loss: 0.00004430
Iteration 37/1000 | Loss: 0.00004430
Iteration 38/1000 | Loss: 0.00004429
Iteration 39/1000 | Loss: 0.00004428
Iteration 40/1000 | Loss: 0.00004428
Iteration 41/1000 | Loss: 0.00004428
Iteration 42/1000 | Loss: 0.00004428
Iteration 43/1000 | Loss: 0.00004428
Iteration 44/1000 | Loss: 0.00004428
Iteration 45/1000 | Loss: 0.00004428
Iteration 46/1000 | Loss: 0.00004428
Iteration 47/1000 | Loss: 0.00004428
Iteration 48/1000 | Loss: 0.00004428
Iteration 49/1000 | Loss: 0.00004428
Iteration 50/1000 | Loss: 0.00004428
Iteration 51/1000 | Loss: 0.00004428
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 51. Stopping optimization.
Last 5 losses: [4.4278771383687854e-05, 4.4278771383687854e-05, 4.4278771383687854e-05, 4.4278771383687854e-05, 4.4278771383687854e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.4278771383687854e-05

Optimization complete. Final v2v error: 5.603150367736816 mm

Highest mean error: 11.387121200561523 mm for frame 10

Lowest mean error: 5.163621425628662 mm for frame 239

Saving results

Total time: 79.52239084243774
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_58_us_2424/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_58_us_2424/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_58_us_2424/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00956937
Iteration 2/25 | Loss: 0.00244863
Iteration 3/25 | Loss: 0.00219277
Iteration 4/25 | Loss: 0.00217039
Iteration 5/25 | Loss: 0.00216700
Iteration 6/25 | Loss: 0.00216700
Iteration 7/25 | Loss: 0.00216700
Iteration 8/25 | Loss: 0.00216700
Iteration 9/25 | Loss: 0.00216700
Iteration 10/25 | Loss: 0.00216700
Iteration 11/25 | Loss: 0.00216700
Iteration 12/25 | Loss: 0.00216700
Iteration 13/25 | Loss: 0.00216700
Iteration 14/25 | Loss: 0.00216700
Iteration 15/25 | Loss: 0.00216700
Iteration 16/25 | Loss: 0.00216700
Iteration 17/25 | Loss: 0.00216700
Iteration 18/25 | Loss: 0.00216700
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0021670018322765827, 0.0021670018322765827, 0.0021670018322765827, 0.0021670018322765827, 0.0021670018322765827]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021670018322765827

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.26607996
Iteration 2/25 | Loss: 0.00208914
Iteration 3/25 | Loss: 0.00208912
Iteration 4/25 | Loss: 0.00208912
Iteration 5/25 | Loss: 0.00208912
Iteration 6/25 | Loss: 0.00208912
Iteration 7/25 | Loss: 0.00208912
Iteration 8/25 | Loss: 0.00208911
Iteration 9/25 | Loss: 0.00208911
Iteration 10/25 | Loss: 0.00208911
Iteration 11/25 | Loss: 0.00208911
Iteration 12/25 | Loss: 0.00208911
Iteration 13/25 | Loss: 0.00208911
Iteration 14/25 | Loss: 0.00208911
Iteration 15/25 | Loss: 0.00208911
Iteration 16/25 | Loss: 0.00208911
Iteration 17/25 | Loss: 0.00208911
Iteration 18/25 | Loss: 0.00208911
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0020891139283776283, 0.0020891139283776283, 0.0020891139283776283, 0.0020891139283776283, 0.0020891139283776283]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020891139283776283

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00208911
Iteration 2/1000 | Loss: 0.00010620
Iteration 3/1000 | Loss: 0.00007121
Iteration 4/1000 | Loss: 0.00005329
Iteration 5/1000 | Loss: 0.00004821
Iteration 6/1000 | Loss: 0.00004441
Iteration 7/1000 | Loss: 0.00004208
Iteration 8/1000 | Loss: 0.00004070
Iteration 9/1000 | Loss: 0.00003955
Iteration 10/1000 | Loss: 0.00003875
Iteration 11/1000 | Loss: 0.00003827
Iteration 12/1000 | Loss: 0.00003785
Iteration 13/1000 | Loss: 0.00003760
Iteration 14/1000 | Loss: 0.00003755
Iteration 15/1000 | Loss: 0.00003735
Iteration 16/1000 | Loss: 0.00003722
Iteration 17/1000 | Loss: 0.00003719
Iteration 18/1000 | Loss: 0.00003714
Iteration 19/1000 | Loss: 0.00003714
Iteration 20/1000 | Loss: 0.00003714
Iteration 21/1000 | Loss: 0.00003714
Iteration 22/1000 | Loss: 0.00003714
Iteration 23/1000 | Loss: 0.00003713
Iteration 24/1000 | Loss: 0.00003713
Iteration 25/1000 | Loss: 0.00003713
Iteration 26/1000 | Loss: 0.00003713
Iteration 27/1000 | Loss: 0.00003713
Iteration 28/1000 | Loss: 0.00003713
Iteration 29/1000 | Loss: 0.00003713
Iteration 30/1000 | Loss: 0.00003712
Iteration 31/1000 | Loss: 0.00003712
Iteration 32/1000 | Loss: 0.00003711
Iteration 33/1000 | Loss: 0.00003711
Iteration 34/1000 | Loss: 0.00003711
Iteration 35/1000 | Loss: 0.00003710
Iteration 36/1000 | Loss: 0.00003710
Iteration 37/1000 | Loss: 0.00003710
Iteration 38/1000 | Loss: 0.00003710
Iteration 39/1000 | Loss: 0.00003710
Iteration 40/1000 | Loss: 0.00003709
Iteration 41/1000 | Loss: 0.00003709
Iteration 42/1000 | Loss: 0.00003709
Iteration 43/1000 | Loss: 0.00003709
Iteration 44/1000 | Loss: 0.00003709
Iteration 45/1000 | Loss: 0.00003709
Iteration 46/1000 | Loss: 0.00003709
Iteration 47/1000 | Loss: 0.00003709
Iteration 48/1000 | Loss: 0.00003709
Iteration 49/1000 | Loss: 0.00003709
Iteration 50/1000 | Loss: 0.00003709
Iteration 51/1000 | Loss: 0.00003708
Iteration 52/1000 | Loss: 0.00003708
Iteration 53/1000 | Loss: 0.00003708
Iteration 54/1000 | Loss: 0.00003708
Iteration 55/1000 | Loss: 0.00003708
Iteration 56/1000 | Loss: 0.00003708
Iteration 57/1000 | Loss: 0.00003708
Iteration 58/1000 | Loss: 0.00003707
Iteration 59/1000 | Loss: 0.00003707
Iteration 60/1000 | Loss: 0.00003707
Iteration 61/1000 | Loss: 0.00003707
Iteration 62/1000 | Loss: 0.00003707
Iteration 63/1000 | Loss: 0.00003707
Iteration 64/1000 | Loss: 0.00003707
Iteration 65/1000 | Loss: 0.00003706
Iteration 66/1000 | Loss: 0.00003706
Iteration 67/1000 | Loss: 0.00003706
Iteration 68/1000 | Loss: 0.00003706
Iteration 69/1000 | Loss: 0.00003705
Iteration 70/1000 | Loss: 0.00003705
Iteration 71/1000 | Loss: 0.00003705
Iteration 72/1000 | Loss: 0.00003705
Iteration 73/1000 | Loss: 0.00003705
Iteration 74/1000 | Loss: 0.00003705
Iteration 75/1000 | Loss: 0.00003705
Iteration 76/1000 | Loss: 0.00003705
Iteration 77/1000 | Loss: 0.00003705
Iteration 78/1000 | Loss: 0.00003705
Iteration 79/1000 | Loss: 0.00003704
Iteration 80/1000 | Loss: 0.00003704
Iteration 81/1000 | Loss: 0.00003704
Iteration 82/1000 | Loss: 0.00003704
Iteration 83/1000 | Loss: 0.00003704
Iteration 84/1000 | Loss: 0.00003704
Iteration 85/1000 | Loss: 0.00003704
Iteration 86/1000 | Loss: 0.00003704
Iteration 87/1000 | Loss: 0.00003704
Iteration 88/1000 | Loss: 0.00003704
Iteration 89/1000 | Loss: 0.00003704
Iteration 90/1000 | Loss: 0.00003704
Iteration 91/1000 | Loss: 0.00003704
Iteration 92/1000 | Loss: 0.00003704
Iteration 93/1000 | Loss: 0.00003704
Iteration 94/1000 | Loss: 0.00003704
Iteration 95/1000 | Loss: 0.00003704
Iteration 96/1000 | Loss: 0.00003704
Iteration 97/1000 | Loss: 0.00003704
Iteration 98/1000 | Loss: 0.00003704
Iteration 99/1000 | Loss: 0.00003704
Iteration 100/1000 | Loss: 0.00003704
Iteration 101/1000 | Loss: 0.00003704
Iteration 102/1000 | Loss: 0.00003704
Iteration 103/1000 | Loss: 0.00003704
Iteration 104/1000 | Loss: 0.00003704
Iteration 105/1000 | Loss: 0.00003704
Iteration 106/1000 | Loss: 0.00003704
Iteration 107/1000 | Loss: 0.00003704
Iteration 108/1000 | Loss: 0.00003704
Iteration 109/1000 | Loss: 0.00003704
Iteration 110/1000 | Loss: 0.00003704
Iteration 111/1000 | Loss: 0.00003704
Iteration 112/1000 | Loss: 0.00003704
Iteration 113/1000 | Loss: 0.00003704
Iteration 114/1000 | Loss: 0.00003704
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [3.703807306010276e-05, 3.703807306010276e-05, 3.703807306010276e-05, 3.703807306010276e-05, 3.703807306010276e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.703807306010276e-05

Optimization complete. Final v2v error: 5.295045375823975 mm

Highest mean error: 5.625547885894775 mm for frame 54

Lowest mean error: 5.10203742980957 mm for frame 90

Saving results

Total time: 40.3981146812439
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00782298
Iteration 2/25 | Loss: 0.00116773
Iteration 3/25 | Loss: 0.00107945
Iteration 4/25 | Loss: 0.00106860
Iteration 5/25 | Loss: 0.00106643
Iteration 6/25 | Loss: 0.00106643
Iteration 7/25 | Loss: 0.00106643
Iteration 8/25 | Loss: 0.00106643
Iteration 9/25 | Loss: 0.00106643
Iteration 10/25 | Loss: 0.00106643
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010664280271157622, 0.0010664280271157622, 0.0010664280271157622, 0.0010664280271157622, 0.0010664280271157622]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010664280271157622

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35195947
Iteration 2/25 | Loss: 0.00080160
Iteration 3/25 | Loss: 0.00080160
Iteration 4/25 | Loss: 0.00080160
Iteration 5/25 | Loss: 0.00080160
Iteration 6/25 | Loss: 0.00080160
Iteration 7/25 | Loss: 0.00080160
Iteration 8/25 | Loss: 0.00080160
Iteration 9/25 | Loss: 0.00080160
Iteration 10/25 | Loss: 0.00080160
Iteration 11/25 | Loss: 0.00080160
Iteration 12/25 | Loss: 0.00080160
Iteration 13/25 | Loss: 0.00080160
Iteration 14/25 | Loss: 0.00080160
Iteration 15/25 | Loss: 0.00080160
Iteration 16/25 | Loss: 0.00080160
Iteration 17/25 | Loss: 0.00080160
Iteration 18/25 | Loss: 0.00080160
Iteration 19/25 | Loss: 0.00080160
Iteration 20/25 | Loss: 0.00080160
Iteration 21/25 | Loss: 0.00080160
Iteration 22/25 | Loss: 0.00080160
Iteration 23/25 | Loss: 0.00080160
Iteration 24/25 | Loss: 0.00080160
Iteration 25/25 | Loss: 0.00080160

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080160
Iteration 2/1000 | Loss: 0.00002310
Iteration 3/1000 | Loss: 0.00001455
Iteration 4/1000 | Loss: 0.00001207
Iteration 5/1000 | Loss: 0.00001114
Iteration 6/1000 | Loss: 0.00001048
Iteration 7/1000 | Loss: 0.00001005
Iteration 8/1000 | Loss: 0.00000979
Iteration 9/1000 | Loss: 0.00000952
Iteration 10/1000 | Loss: 0.00000924
Iteration 11/1000 | Loss: 0.00000916
Iteration 12/1000 | Loss: 0.00000914
Iteration 13/1000 | Loss: 0.00000914
Iteration 14/1000 | Loss: 0.00000913
Iteration 15/1000 | Loss: 0.00000910
Iteration 16/1000 | Loss: 0.00000909
Iteration 17/1000 | Loss: 0.00000902
Iteration 18/1000 | Loss: 0.00000899
Iteration 19/1000 | Loss: 0.00000898
Iteration 20/1000 | Loss: 0.00000898
Iteration 21/1000 | Loss: 0.00000898
Iteration 22/1000 | Loss: 0.00000897
Iteration 23/1000 | Loss: 0.00000896
Iteration 24/1000 | Loss: 0.00000896
Iteration 25/1000 | Loss: 0.00000892
Iteration 26/1000 | Loss: 0.00000890
Iteration 27/1000 | Loss: 0.00000890
Iteration 28/1000 | Loss: 0.00000886
Iteration 29/1000 | Loss: 0.00000886
Iteration 30/1000 | Loss: 0.00000886
Iteration 31/1000 | Loss: 0.00000886
Iteration 32/1000 | Loss: 0.00000885
Iteration 33/1000 | Loss: 0.00000885
Iteration 34/1000 | Loss: 0.00000883
Iteration 35/1000 | Loss: 0.00000883
Iteration 36/1000 | Loss: 0.00000883
Iteration 37/1000 | Loss: 0.00000883
Iteration 38/1000 | Loss: 0.00000883
Iteration 39/1000 | Loss: 0.00000883
Iteration 40/1000 | Loss: 0.00000883
Iteration 41/1000 | Loss: 0.00000882
Iteration 42/1000 | Loss: 0.00000882
Iteration 43/1000 | Loss: 0.00000882
Iteration 44/1000 | Loss: 0.00000881
Iteration 45/1000 | Loss: 0.00000878
Iteration 46/1000 | Loss: 0.00000878
Iteration 47/1000 | Loss: 0.00000877
Iteration 48/1000 | Loss: 0.00000877
Iteration 49/1000 | Loss: 0.00000876
Iteration 50/1000 | Loss: 0.00000876
Iteration 51/1000 | Loss: 0.00000876
Iteration 52/1000 | Loss: 0.00000875
Iteration 53/1000 | Loss: 0.00000875
Iteration 54/1000 | Loss: 0.00000875
Iteration 55/1000 | Loss: 0.00000874
Iteration 56/1000 | Loss: 0.00000874
Iteration 57/1000 | Loss: 0.00000870
Iteration 58/1000 | Loss: 0.00000868
Iteration 59/1000 | Loss: 0.00000867
Iteration 60/1000 | Loss: 0.00000866
Iteration 61/1000 | Loss: 0.00000866
Iteration 62/1000 | Loss: 0.00000865
Iteration 63/1000 | Loss: 0.00000865
Iteration 64/1000 | Loss: 0.00000864
Iteration 65/1000 | Loss: 0.00000864
Iteration 66/1000 | Loss: 0.00000864
Iteration 67/1000 | Loss: 0.00000863
Iteration 68/1000 | Loss: 0.00000863
Iteration 69/1000 | Loss: 0.00000863
Iteration 70/1000 | Loss: 0.00000863
Iteration 71/1000 | Loss: 0.00000862
Iteration 72/1000 | Loss: 0.00000862
Iteration 73/1000 | Loss: 0.00000862
Iteration 74/1000 | Loss: 0.00000862
Iteration 75/1000 | Loss: 0.00000862
Iteration 76/1000 | Loss: 0.00000862
Iteration 77/1000 | Loss: 0.00000862
Iteration 78/1000 | Loss: 0.00000862
Iteration 79/1000 | Loss: 0.00000862
Iteration 80/1000 | Loss: 0.00000862
Iteration 81/1000 | Loss: 0.00000862
Iteration 82/1000 | Loss: 0.00000862
Iteration 83/1000 | Loss: 0.00000861
Iteration 84/1000 | Loss: 0.00000861
Iteration 85/1000 | Loss: 0.00000861
Iteration 86/1000 | Loss: 0.00000861
Iteration 87/1000 | Loss: 0.00000860
Iteration 88/1000 | Loss: 0.00000860
Iteration 89/1000 | Loss: 0.00000859
Iteration 90/1000 | Loss: 0.00000859
Iteration 91/1000 | Loss: 0.00000859
Iteration 92/1000 | Loss: 0.00000859
Iteration 93/1000 | Loss: 0.00000858
Iteration 94/1000 | Loss: 0.00000858
Iteration 95/1000 | Loss: 0.00000858
Iteration 96/1000 | Loss: 0.00000858
Iteration 97/1000 | Loss: 0.00000858
Iteration 98/1000 | Loss: 0.00000857
Iteration 99/1000 | Loss: 0.00000857
Iteration 100/1000 | Loss: 0.00000857
Iteration 101/1000 | Loss: 0.00000857
Iteration 102/1000 | Loss: 0.00000857
Iteration 103/1000 | Loss: 0.00000857
Iteration 104/1000 | Loss: 0.00000857
Iteration 105/1000 | Loss: 0.00000857
Iteration 106/1000 | Loss: 0.00000857
Iteration 107/1000 | Loss: 0.00000857
Iteration 108/1000 | Loss: 0.00000857
Iteration 109/1000 | Loss: 0.00000857
Iteration 110/1000 | Loss: 0.00000857
Iteration 111/1000 | Loss: 0.00000857
Iteration 112/1000 | Loss: 0.00000857
Iteration 113/1000 | Loss: 0.00000857
Iteration 114/1000 | Loss: 0.00000857
Iteration 115/1000 | Loss: 0.00000857
Iteration 116/1000 | Loss: 0.00000857
Iteration 117/1000 | Loss: 0.00000856
Iteration 118/1000 | Loss: 0.00000856
Iteration 119/1000 | Loss: 0.00000855
Iteration 120/1000 | Loss: 0.00000855
Iteration 121/1000 | Loss: 0.00000855
Iteration 122/1000 | Loss: 0.00000855
Iteration 123/1000 | Loss: 0.00000854
Iteration 124/1000 | Loss: 0.00000854
Iteration 125/1000 | Loss: 0.00000854
Iteration 126/1000 | Loss: 0.00000853
Iteration 127/1000 | Loss: 0.00000853
Iteration 128/1000 | Loss: 0.00000853
Iteration 129/1000 | Loss: 0.00000853
Iteration 130/1000 | Loss: 0.00000852
Iteration 131/1000 | Loss: 0.00000852
Iteration 132/1000 | Loss: 0.00000852
Iteration 133/1000 | Loss: 0.00000852
Iteration 134/1000 | Loss: 0.00000851
Iteration 135/1000 | Loss: 0.00000851
Iteration 136/1000 | Loss: 0.00000851
Iteration 137/1000 | Loss: 0.00000851
Iteration 138/1000 | Loss: 0.00000851
Iteration 139/1000 | Loss: 0.00000851
Iteration 140/1000 | Loss: 0.00000851
Iteration 141/1000 | Loss: 0.00000851
Iteration 142/1000 | Loss: 0.00000851
Iteration 143/1000 | Loss: 0.00000851
Iteration 144/1000 | Loss: 0.00000851
Iteration 145/1000 | Loss: 0.00000851
Iteration 146/1000 | Loss: 0.00000851
Iteration 147/1000 | Loss: 0.00000851
Iteration 148/1000 | Loss: 0.00000851
Iteration 149/1000 | Loss: 0.00000851
Iteration 150/1000 | Loss: 0.00000851
Iteration 151/1000 | Loss: 0.00000851
Iteration 152/1000 | Loss: 0.00000851
Iteration 153/1000 | Loss: 0.00000851
Iteration 154/1000 | Loss: 0.00000851
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [8.506117410433944e-06, 8.506117410433944e-06, 8.506117410433944e-06, 8.506117410433944e-06, 8.506117410433944e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.506117410433944e-06

Optimization complete. Final v2v error: 2.500300645828247 mm

Highest mean error: 2.7745704650878906 mm for frame 75

Lowest mean error: 2.349503755569458 mm for frame 206

Saving results

Total time: 40.14186072349548
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01008016
Iteration 2/25 | Loss: 0.00271263
Iteration 3/25 | Loss: 0.00174212
Iteration 4/25 | Loss: 0.00151136
Iteration 5/25 | Loss: 0.00151527
Iteration 6/25 | Loss: 0.00148548
Iteration 7/25 | Loss: 0.00138368
Iteration 8/25 | Loss: 0.00133365
Iteration 9/25 | Loss: 0.00126776
Iteration 10/25 | Loss: 0.00124494
Iteration 11/25 | Loss: 0.00122619
Iteration 12/25 | Loss: 0.00121194
Iteration 13/25 | Loss: 0.00120817
Iteration 14/25 | Loss: 0.00120006
Iteration 15/25 | Loss: 0.00119051
Iteration 16/25 | Loss: 0.00118020
Iteration 17/25 | Loss: 0.00118571
Iteration 18/25 | Loss: 0.00119376
Iteration 19/25 | Loss: 0.00117681
Iteration 20/25 | Loss: 0.00116614
Iteration 21/25 | Loss: 0.00116052
Iteration 22/25 | Loss: 0.00116461
Iteration 23/25 | Loss: 0.00116129
Iteration 24/25 | Loss: 0.00115713
Iteration 25/25 | Loss: 0.00115500

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33594477
Iteration 2/25 | Loss: 0.00103405
Iteration 3/25 | Loss: 0.00087989
Iteration 4/25 | Loss: 0.00087989
Iteration 5/25 | Loss: 0.00087988
Iteration 6/25 | Loss: 0.00087988
Iteration 7/25 | Loss: 0.00087988
Iteration 8/25 | Loss: 0.00087988
Iteration 9/25 | Loss: 0.00087988
Iteration 10/25 | Loss: 0.00087988
Iteration 11/25 | Loss: 0.00087988
Iteration 12/25 | Loss: 0.00087988
Iteration 13/25 | Loss: 0.00087988
Iteration 14/25 | Loss: 0.00087988
Iteration 15/25 | Loss: 0.00087988
Iteration 16/25 | Loss: 0.00087988
Iteration 17/25 | Loss: 0.00087988
Iteration 18/25 | Loss: 0.00087988
Iteration 19/25 | Loss: 0.00087988
Iteration 20/25 | Loss: 0.00087988
Iteration 21/25 | Loss: 0.00087988
Iteration 22/25 | Loss: 0.00087988
Iteration 23/25 | Loss: 0.00087988
Iteration 24/25 | Loss: 0.00087988
Iteration 25/25 | Loss: 0.00087988

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087988
Iteration 2/1000 | Loss: 0.00032878
Iteration 3/1000 | Loss: 0.00004842
Iteration 4/1000 | Loss: 0.00004301
Iteration 5/1000 | Loss: 0.00004052
Iteration 6/1000 | Loss: 0.00003861
Iteration 7/1000 | Loss: 0.00003766
Iteration 8/1000 | Loss: 0.00003694
Iteration 9/1000 | Loss: 0.00003639
Iteration 10/1000 | Loss: 0.00003584
Iteration 11/1000 | Loss: 0.00003533
Iteration 12/1000 | Loss: 0.00033412
Iteration 13/1000 | Loss: 0.00069098
Iteration 14/1000 | Loss: 0.00091788
Iteration 15/1000 | Loss: 0.00004360
Iteration 16/1000 | Loss: 0.00003348
Iteration 17/1000 | Loss: 0.00003036
Iteration 18/1000 | Loss: 0.00002665
Iteration 19/1000 | Loss: 0.00002394
Iteration 20/1000 | Loss: 0.00002249
Iteration 21/1000 | Loss: 0.00002155
Iteration 22/1000 | Loss: 0.00002097
Iteration 23/1000 | Loss: 0.00002048
Iteration 24/1000 | Loss: 0.00002006
Iteration 25/1000 | Loss: 0.00001970
Iteration 26/1000 | Loss: 0.00001946
Iteration 27/1000 | Loss: 0.00001940
Iteration 28/1000 | Loss: 0.00001928
Iteration 29/1000 | Loss: 0.00001927
Iteration 30/1000 | Loss: 0.00001924
Iteration 31/1000 | Loss: 0.00001923
Iteration 32/1000 | Loss: 0.00001923
Iteration 33/1000 | Loss: 0.00001922
Iteration 34/1000 | Loss: 0.00001921
Iteration 35/1000 | Loss: 0.00001920
Iteration 36/1000 | Loss: 0.00001919
Iteration 37/1000 | Loss: 0.00001915
Iteration 38/1000 | Loss: 0.00001915
Iteration 39/1000 | Loss: 0.00001912
Iteration 40/1000 | Loss: 0.00001911
Iteration 41/1000 | Loss: 0.00001909
Iteration 42/1000 | Loss: 0.00001909
Iteration 43/1000 | Loss: 0.00001909
Iteration 44/1000 | Loss: 0.00001908
Iteration 45/1000 | Loss: 0.00001908
Iteration 46/1000 | Loss: 0.00001908
Iteration 47/1000 | Loss: 0.00001908
Iteration 48/1000 | Loss: 0.00001908
Iteration 49/1000 | Loss: 0.00001907
Iteration 50/1000 | Loss: 0.00001907
Iteration 51/1000 | Loss: 0.00001906
Iteration 52/1000 | Loss: 0.00001906
Iteration 53/1000 | Loss: 0.00001906
Iteration 54/1000 | Loss: 0.00001906
Iteration 55/1000 | Loss: 0.00001906
Iteration 56/1000 | Loss: 0.00001906
Iteration 57/1000 | Loss: 0.00001906
Iteration 58/1000 | Loss: 0.00001906
Iteration 59/1000 | Loss: 0.00001906
Iteration 60/1000 | Loss: 0.00001906
Iteration 61/1000 | Loss: 0.00001906
Iteration 62/1000 | Loss: 0.00001906
Iteration 63/1000 | Loss: 0.00001906
Iteration 64/1000 | Loss: 0.00001906
Iteration 65/1000 | Loss: 0.00001905
Iteration 66/1000 | Loss: 0.00001905
Iteration 67/1000 | Loss: 0.00001905
Iteration 68/1000 | Loss: 0.00001904
Iteration 69/1000 | Loss: 0.00001904
Iteration 70/1000 | Loss: 0.00001904
Iteration 71/1000 | Loss: 0.00001904
Iteration 72/1000 | Loss: 0.00001904
Iteration 73/1000 | Loss: 0.00001904
Iteration 74/1000 | Loss: 0.00001904
Iteration 75/1000 | Loss: 0.00001903
Iteration 76/1000 | Loss: 0.00001903
Iteration 77/1000 | Loss: 0.00001903
Iteration 78/1000 | Loss: 0.00001903
Iteration 79/1000 | Loss: 0.00001903
Iteration 80/1000 | Loss: 0.00001902
Iteration 81/1000 | Loss: 0.00001902
Iteration 82/1000 | Loss: 0.00001902
Iteration 83/1000 | Loss: 0.00001902
Iteration 84/1000 | Loss: 0.00001902
Iteration 85/1000 | Loss: 0.00001902
Iteration 86/1000 | Loss: 0.00001902
Iteration 87/1000 | Loss: 0.00001902
Iteration 88/1000 | Loss: 0.00001902
Iteration 89/1000 | Loss: 0.00001901
Iteration 90/1000 | Loss: 0.00001901
Iteration 91/1000 | Loss: 0.00001901
Iteration 92/1000 | Loss: 0.00001901
Iteration 93/1000 | Loss: 0.00001901
Iteration 94/1000 | Loss: 0.00001901
Iteration 95/1000 | Loss: 0.00001901
Iteration 96/1000 | Loss: 0.00001901
Iteration 97/1000 | Loss: 0.00001900
Iteration 98/1000 | Loss: 0.00001900
Iteration 99/1000 | Loss: 0.00001900
Iteration 100/1000 | Loss: 0.00001900
Iteration 101/1000 | Loss: 0.00001900
Iteration 102/1000 | Loss: 0.00001900
Iteration 103/1000 | Loss: 0.00001900
Iteration 104/1000 | Loss: 0.00001900
Iteration 105/1000 | Loss: 0.00001900
Iteration 106/1000 | Loss: 0.00001900
Iteration 107/1000 | Loss: 0.00001900
Iteration 108/1000 | Loss: 0.00001900
Iteration 109/1000 | Loss: 0.00001900
Iteration 110/1000 | Loss: 0.00001900
Iteration 111/1000 | Loss: 0.00001900
Iteration 112/1000 | Loss: 0.00001900
Iteration 113/1000 | Loss: 0.00001900
Iteration 114/1000 | Loss: 0.00001900
Iteration 115/1000 | Loss: 0.00001900
Iteration 116/1000 | Loss: 0.00001900
Iteration 117/1000 | Loss: 0.00001899
Iteration 118/1000 | Loss: 0.00001899
Iteration 119/1000 | Loss: 0.00001899
Iteration 120/1000 | Loss: 0.00001899
Iteration 121/1000 | Loss: 0.00001899
Iteration 122/1000 | Loss: 0.00001899
Iteration 123/1000 | Loss: 0.00001899
Iteration 124/1000 | Loss: 0.00001899
Iteration 125/1000 | Loss: 0.00001899
Iteration 126/1000 | Loss: 0.00001899
Iteration 127/1000 | Loss: 0.00001899
Iteration 128/1000 | Loss: 0.00001899
Iteration 129/1000 | Loss: 0.00001899
Iteration 130/1000 | Loss: 0.00001899
Iteration 131/1000 | Loss: 0.00001899
Iteration 132/1000 | Loss: 0.00001899
Iteration 133/1000 | Loss: 0.00001898
Iteration 134/1000 | Loss: 0.00001898
Iteration 135/1000 | Loss: 0.00001898
Iteration 136/1000 | Loss: 0.00001898
Iteration 137/1000 | Loss: 0.00001898
Iteration 138/1000 | Loss: 0.00001898
Iteration 139/1000 | Loss: 0.00001898
Iteration 140/1000 | Loss: 0.00001898
Iteration 141/1000 | Loss: 0.00001898
Iteration 142/1000 | Loss: 0.00001898
Iteration 143/1000 | Loss: 0.00001897
Iteration 144/1000 | Loss: 0.00001897
Iteration 145/1000 | Loss: 0.00001897
Iteration 146/1000 | Loss: 0.00001897
Iteration 147/1000 | Loss: 0.00001897
Iteration 148/1000 | Loss: 0.00001897
Iteration 149/1000 | Loss: 0.00001897
Iteration 150/1000 | Loss: 0.00001897
Iteration 151/1000 | Loss: 0.00001897
Iteration 152/1000 | Loss: 0.00001897
Iteration 153/1000 | Loss: 0.00001897
Iteration 154/1000 | Loss: 0.00001897
Iteration 155/1000 | Loss: 0.00001897
Iteration 156/1000 | Loss: 0.00001897
Iteration 157/1000 | Loss: 0.00001897
Iteration 158/1000 | Loss: 0.00001896
Iteration 159/1000 | Loss: 0.00001896
Iteration 160/1000 | Loss: 0.00001896
Iteration 161/1000 | Loss: 0.00001896
Iteration 162/1000 | Loss: 0.00001896
Iteration 163/1000 | Loss: 0.00001896
Iteration 164/1000 | Loss: 0.00001896
Iteration 165/1000 | Loss: 0.00001896
Iteration 166/1000 | Loss: 0.00001896
Iteration 167/1000 | Loss: 0.00001896
Iteration 168/1000 | Loss: 0.00001896
Iteration 169/1000 | Loss: 0.00001896
Iteration 170/1000 | Loss: 0.00001896
Iteration 171/1000 | Loss: 0.00001896
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.8961940440931357e-05, 1.8961940440931357e-05, 1.8961940440931357e-05, 1.8961940440931357e-05, 1.8961940440931357e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8961940440931357e-05

Optimization complete. Final v2v error: 3.687969207763672 mm

Highest mean error: 4.152520656585693 mm for frame 69

Lowest mean error: 3.0765879154205322 mm for frame 4

Saving results

Total time: 97.94230675697327
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00935503
Iteration 2/25 | Loss: 0.00258141
Iteration 3/25 | Loss: 0.00197167
Iteration 4/25 | Loss: 0.00168493
Iteration 5/25 | Loss: 0.00188896
Iteration 6/25 | Loss: 0.00219074
Iteration 7/25 | Loss: 0.00172651
Iteration 8/25 | Loss: 0.00134370
Iteration 9/25 | Loss: 0.00130200
Iteration 10/25 | Loss: 0.00135078
Iteration 11/25 | Loss: 0.00131139
Iteration 12/25 | Loss: 0.00124906
Iteration 13/25 | Loss: 0.00121407
Iteration 14/25 | Loss: 0.00119126
Iteration 15/25 | Loss: 0.00117486
Iteration 16/25 | Loss: 0.00116994
Iteration 17/25 | Loss: 0.00116857
Iteration 18/25 | Loss: 0.00116829
Iteration 19/25 | Loss: 0.00116820
Iteration 20/25 | Loss: 0.00116820
Iteration 21/25 | Loss: 0.00116820
Iteration 22/25 | Loss: 0.00116820
Iteration 23/25 | Loss: 0.00116820
Iteration 24/25 | Loss: 0.00116820
Iteration 25/25 | Loss: 0.00116820
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0011682020267471671, 0.0011682020267471671, 0.0011682020267471671, 0.0011682020267471671, 0.0011682020267471671]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011682020267471671

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34030128
Iteration 2/25 | Loss: 0.00051998
Iteration 3/25 | Loss: 0.00051998
Iteration 4/25 | Loss: 0.00051998
Iteration 5/25 | Loss: 0.00051998
Iteration 6/25 | Loss: 0.00051998
Iteration 7/25 | Loss: 0.00051998
Iteration 8/25 | Loss: 0.00051998
Iteration 9/25 | Loss: 0.00051998
Iteration 10/25 | Loss: 0.00051998
Iteration 11/25 | Loss: 0.00051998
Iteration 12/25 | Loss: 0.00051998
Iteration 13/25 | Loss: 0.00051998
Iteration 14/25 | Loss: 0.00051998
Iteration 15/25 | Loss: 0.00051998
Iteration 16/25 | Loss: 0.00051998
Iteration 17/25 | Loss: 0.00051998
Iteration 18/25 | Loss: 0.00051998
Iteration 19/25 | Loss: 0.00051998
Iteration 20/25 | Loss: 0.00051998
Iteration 21/25 | Loss: 0.00051998
Iteration 22/25 | Loss: 0.00051998
Iteration 23/25 | Loss: 0.00051998
Iteration 24/25 | Loss: 0.00051998
Iteration 25/25 | Loss: 0.00051998

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051998
Iteration 2/1000 | Loss: 0.00003427
Iteration 3/1000 | Loss: 0.00002192
Iteration 4/1000 | Loss: 0.00001932
Iteration 5/1000 | Loss: 0.00001841
Iteration 6/1000 | Loss: 0.00001785
Iteration 7/1000 | Loss: 0.00008944
Iteration 8/1000 | Loss: 0.00002389
Iteration 9/1000 | Loss: 0.00009385
Iteration 10/1000 | Loss: 0.00005115
Iteration 11/1000 | Loss: 0.00009743
Iteration 12/1000 | Loss: 0.00002159
Iteration 13/1000 | Loss: 0.00009213
Iteration 14/1000 | Loss: 0.00002089
Iteration 15/1000 | Loss: 0.00007564
Iteration 16/1000 | Loss: 0.00001876
Iteration 17/1000 | Loss: 0.00001777
Iteration 18/1000 | Loss: 0.00001717
Iteration 19/1000 | Loss: 0.00001681
Iteration 20/1000 | Loss: 0.00001655
Iteration 21/1000 | Loss: 0.00001622
Iteration 22/1000 | Loss: 0.00001600
Iteration 23/1000 | Loss: 0.00001583
Iteration 24/1000 | Loss: 0.00001568
Iteration 25/1000 | Loss: 0.00001565
Iteration 26/1000 | Loss: 0.00001546
Iteration 27/1000 | Loss: 0.00001545
Iteration 28/1000 | Loss: 0.00001541
Iteration 29/1000 | Loss: 0.00001541
Iteration 30/1000 | Loss: 0.00001540
Iteration 31/1000 | Loss: 0.00001535
Iteration 32/1000 | Loss: 0.00001534
Iteration 33/1000 | Loss: 0.00001534
Iteration 34/1000 | Loss: 0.00001528
Iteration 35/1000 | Loss: 0.00001528
Iteration 36/1000 | Loss: 0.00001528
Iteration 37/1000 | Loss: 0.00001526
Iteration 38/1000 | Loss: 0.00001525
Iteration 39/1000 | Loss: 0.00001523
Iteration 40/1000 | Loss: 0.00001522
Iteration 41/1000 | Loss: 0.00001521
Iteration 42/1000 | Loss: 0.00001520
Iteration 43/1000 | Loss: 0.00001519
Iteration 44/1000 | Loss: 0.00001518
Iteration 45/1000 | Loss: 0.00001515
Iteration 46/1000 | Loss: 0.00001515
Iteration 47/1000 | Loss: 0.00001515
Iteration 48/1000 | Loss: 0.00001515
Iteration 49/1000 | Loss: 0.00001515
Iteration 50/1000 | Loss: 0.00001515
Iteration 51/1000 | Loss: 0.00001515
Iteration 52/1000 | Loss: 0.00001515
Iteration 53/1000 | Loss: 0.00001515
Iteration 54/1000 | Loss: 0.00001515
Iteration 55/1000 | Loss: 0.00001514
Iteration 56/1000 | Loss: 0.00001514
Iteration 57/1000 | Loss: 0.00001511
Iteration 58/1000 | Loss: 0.00001511
Iteration 59/1000 | Loss: 0.00001511
Iteration 60/1000 | Loss: 0.00001511
Iteration 61/1000 | Loss: 0.00001511
Iteration 62/1000 | Loss: 0.00001511
Iteration 63/1000 | Loss: 0.00001510
Iteration 64/1000 | Loss: 0.00001510
Iteration 65/1000 | Loss: 0.00001508
Iteration 66/1000 | Loss: 0.00001508
Iteration 67/1000 | Loss: 0.00001508
Iteration 68/1000 | Loss: 0.00001507
Iteration 69/1000 | Loss: 0.00001507
Iteration 70/1000 | Loss: 0.00001507
Iteration 71/1000 | Loss: 0.00001507
Iteration 72/1000 | Loss: 0.00001507
Iteration 73/1000 | Loss: 0.00001507
Iteration 74/1000 | Loss: 0.00001506
Iteration 75/1000 | Loss: 0.00001506
Iteration 76/1000 | Loss: 0.00001506
Iteration 77/1000 | Loss: 0.00001506
Iteration 78/1000 | Loss: 0.00001506
Iteration 79/1000 | Loss: 0.00001506
Iteration 80/1000 | Loss: 0.00001506
Iteration 81/1000 | Loss: 0.00001506
Iteration 82/1000 | Loss: 0.00001506
Iteration 83/1000 | Loss: 0.00001505
Iteration 84/1000 | Loss: 0.00001505
Iteration 85/1000 | Loss: 0.00001505
Iteration 86/1000 | Loss: 0.00001505
Iteration 87/1000 | Loss: 0.00001505
Iteration 88/1000 | Loss: 0.00001505
Iteration 89/1000 | Loss: 0.00001505
Iteration 90/1000 | Loss: 0.00001505
Iteration 91/1000 | Loss: 0.00001505
Iteration 92/1000 | Loss: 0.00001505
Iteration 93/1000 | Loss: 0.00001505
Iteration 94/1000 | Loss: 0.00001505
Iteration 95/1000 | Loss: 0.00001504
Iteration 96/1000 | Loss: 0.00001504
Iteration 97/1000 | Loss: 0.00001504
Iteration 98/1000 | Loss: 0.00001504
Iteration 99/1000 | Loss: 0.00001504
Iteration 100/1000 | Loss: 0.00001504
Iteration 101/1000 | Loss: 0.00001504
Iteration 102/1000 | Loss: 0.00001504
Iteration 103/1000 | Loss: 0.00001504
Iteration 104/1000 | Loss: 0.00001503
Iteration 105/1000 | Loss: 0.00001503
Iteration 106/1000 | Loss: 0.00001503
Iteration 107/1000 | Loss: 0.00001503
Iteration 108/1000 | Loss: 0.00001503
Iteration 109/1000 | Loss: 0.00001503
Iteration 110/1000 | Loss: 0.00001503
Iteration 111/1000 | Loss: 0.00001503
Iteration 112/1000 | Loss: 0.00001503
Iteration 113/1000 | Loss: 0.00001503
Iteration 114/1000 | Loss: 0.00001503
Iteration 115/1000 | Loss: 0.00001503
Iteration 116/1000 | Loss: 0.00001503
Iteration 117/1000 | Loss: 0.00001503
Iteration 118/1000 | Loss: 0.00001503
Iteration 119/1000 | Loss: 0.00001503
Iteration 120/1000 | Loss: 0.00001503
Iteration 121/1000 | Loss: 0.00001503
Iteration 122/1000 | Loss: 0.00001503
Iteration 123/1000 | Loss: 0.00001503
Iteration 124/1000 | Loss: 0.00001503
Iteration 125/1000 | Loss: 0.00001503
Iteration 126/1000 | Loss: 0.00001503
Iteration 127/1000 | Loss: 0.00001503
Iteration 128/1000 | Loss: 0.00001503
Iteration 129/1000 | Loss: 0.00001503
Iteration 130/1000 | Loss: 0.00001503
Iteration 131/1000 | Loss: 0.00001503
Iteration 132/1000 | Loss: 0.00001503
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [1.503433031757595e-05, 1.503433031757595e-05, 1.503433031757595e-05, 1.503433031757595e-05, 1.503433031757595e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.503433031757595e-05

Optimization complete. Final v2v error: 3.3219592571258545 mm

Highest mean error: 4.307339668273926 mm for frame 5

Lowest mean error: 3.2246339321136475 mm for frame 46

Saving results

Total time: 75.62912440299988
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00505901
Iteration 2/25 | Loss: 0.00121905
Iteration 3/25 | Loss: 0.00113516
Iteration 4/25 | Loss: 0.00112165
Iteration 5/25 | Loss: 0.00111766
Iteration 6/25 | Loss: 0.00111759
Iteration 7/25 | Loss: 0.00111759
Iteration 8/25 | Loss: 0.00111759
Iteration 9/25 | Loss: 0.00111759
Iteration 10/25 | Loss: 0.00111759
Iteration 11/25 | Loss: 0.00111759
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011175868567079306, 0.0011175868567079306, 0.0011175868567079306, 0.0011175868567079306, 0.0011175868567079306]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011175868567079306

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32599783
Iteration 2/25 | Loss: 0.00081391
Iteration 3/25 | Loss: 0.00081391
Iteration 4/25 | Loss: 0.00081391
Iteration 5/25 | Loss: 0.00081391
Iteration 6/25 | Loss: 0.00081391
Iteration 7/25 | Loss: 0.00081391
Iteration 8/25 | Loss: 0.00081391
Iteration 9/25 | Loss: 0.00081391
Iteration 10/25 | Loss: 0.00081391
Iteration 11/25 | Loss: 0.00081391
Iteration 12/25 | Loss: 0.00081391
Iteration 13/25 | Loss: 0.00081391
Iteration 14/25 | Loss: 0.00081391
Iteration 15/25 | Loss: 0.00081391
Iteration 16/25 | Loss: 0.00081391
Iteration 17/25 | Loss: 0.00081391
Iteration 18/25 | Loss: 0.00081391
Iteration 19/25 | Loss: 0.00081391
Iteration 20/25 | Loss: 0.00081391
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008139105048030615, 0.0008139105048030615, 0.0008139105048030615, 0.0008139105048030615, 0.0008139105048030615]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008139105048030615

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081391
Iteration 2/1000 | Loss: 0.00002234
Iteration 3/1000 | Loss: 0.00001532
Iteration 4/1000 | Loss: 0.00001373
Iteration 5/1000 | Loss: 0.00001267
Iteration 6/1000 | Loss: 0.00001223
Iteration 7/1000 | Loss: 0.00001189
Iteration 8/1000 | Loss: 0.00001167
Iteration 9/1000 | Loss: 0.00001145
Iteration 10/1000 | Loss: 0.00001138
Iteration 11/1000 | Loss: 0.00001137
Iteration 12/1000 | Loss: 0.00001137
Iteration 13/1000 | Loss: 0.00001133
Iteration 14/1000 | Loss: 0.00001128
Iteration 15/1000 | Loss: 0.00001126
Iteration 16/1000 | Loss: 0.00001123
Iteration 17/1000 | Loss: 0.00001120
Iteration 18/1000 | Loss: 0.00001114
Iteration 19/1000 | Loss: 0.00001112
Iteration 20/1000 | Loss: 0.00001107
Iteration 21/1000 | Loss: 0.00001105
Iteration 22/1000 | Loss: 0.00001104
Iteration 23/1000 | Loss: 0.00001104
Iteration 24/1000 | Loss: 0.00001103
Iteration 25/1000 | Loss: 0.00001102
Iteration 26/1000 | Loss: 0.00001100
Iteration 27/1000 | Loss: 0.00001100
Iteration 28/1000 | Loss: 0.00001100
Iteration 29/1000 | Loss: 0.00001099
Iteration 30/1000 | Loss: 0.00001099
Iteration 31/1000 | Loss: 0.00001098
Iteration 32/1000 | Loss: 0.00001097
Iteration 33/1000 | Loss: 0.00001097
Iteration 34/1000 | Loss: 0.00001096
Iteration 35/1000 | Loss: 0.00001096
Iteration 36/1000 | Loss: 0.00001095
Iteration 37/1000 | Loss: 0.00001095
Iteration 38/1000 | Loss: 0.00001094
Iteration 39/1000 | Loss: 0.00001094
Iteration 40/1000 | Loss: 0.00001094
Iteration 41/1000 | Loss: 0.00001094
Iteration 42/1000 | Loss: 0.00001094
Iteration 43/1000 | Loss: 0.00001093
Iteration 44/1000 | Loss: 0.00001092
Iteration 45/1000 | Loss: 0.00001092
Iteration 46/1000 | Loss: 0.00001092
Iteration 47/1000 | Loss: 0.00001092
Iteration 48/1000 | Loss: 0.00001092
Iteration 49/1000 | Loss: 0.00001091
Iteration 50/1000 | Loss: 0.00001091
Iteration 51/1000 | Loss: 0.00001091
Iteration 52/1000 | Loss: 0.00001091
Iteration 53/1000 | Loss: 0.00001091
Iteration 54/1000 | Loss: 0.00001091
Iteration 55/1000 | Loss: 0.00001091
Iteration 56/1000 | Loss: 0.00001091
Iteration 57/1000 | Loss: 0.00001091
Iteration 58/1000 | Loss: 0.00001090
Iteration 59/1000 | Loss: 0.00001090
Iteration 60/1000 | Loss: 0.00001090
Iteration 61/1000 | Loss: 0.00001090
Iteration 62/1000 | Loss: 0.00001090
Iteration 63/1000 | Loss: 0.00001090
Iteration 64/1000 | Loss: 0.00001090
Iteration 65/1000 | Loss: 0.00001089
Iteration 66/1000 | Loss: 0.00001089
Iteration 67/1000 | Loss: 0.00001089
Iteration 68/1000 | Loss: 0.00001089
Iteration 69/1000 | Loss: 0.00001089
Iteration 70/1000 | Loss: 0.00001089
Iteration 71/1000 | Loss: 0.00001089
Iteration 72/1000 | Loss: 0.00001089
Iteration 73/1000 | Loss: 0.00001089
Iteration 74/1000 | Loss: 0.00001089
Iteration 75/1000 | Loss: 0.00001089
Iteration 76/1000 | Loss: 0.00001089
Iteration 77/1000 | Loss: 0.00001089
Iteration 78/1000 | Loss: 0.00001089
Iteration 79/1000 | Loss: 0.00001089
Iteration 80/1000 | Loss: 0.00001089
Iteration 81/1000 | Loss: 0.00001089
Iteration 82/1000 | Loss: 0.00001089
Iteration 83/1000 | Loss: 0.00001089
Iteration 84/1000 | Loss: 0.00001089
Iteration 85/1000 | Loss: 0.00001089
Iteration 86/1000 | Loss: 0.00001089
Iteration 87/1000 | Loss: 0.00001089
Iteration 88/1000 | Loss: 0.00001089
Iteration 89/1000 | Loss: 0.00001089
Iteration 90/1000 | Loss: 0.00001089
Iteration 91/1000 | Loss: 0.00001089
Iteration 92/1000 | Loss: 0.00001089
Iteration 93/1000 | Loss: 0.00001089
Iteration 94/1000 | Loss: 0.00001089
Iteration 95/1000 | Loss: 0.00001089
Iteration 96/1000 | Loss: 0.00001089
Iteration 97/1000 | Loss: 0.00001089
Iteration 98/1000 | Loss: 0.00001089
Iteration 99/1000 | Loss: 0.00001089
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [1.0888476026593708e-05, 1.0888476026593708e-05, 1.0888476026593708e-05, 1.0888476026593708e-05, 1.0888476026593708e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0888476026593708e-05

Optimization complete. Final v2v error: 2.829813003540039 mm

Highest mean error: 3.0273044109344482 mm for frame 148

Lowest mean error: 2.7024450302124023 mm for frame 212

Saving results

Total time: 32.777992963790894
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01052976
Iteration 2/25 | Loss: 0.00181453
Iteration 3/25 | Loss: 0.00139155
Iteration 4/25 | Loss: 0.00135054
Iteration 5/25 | Loss: 0.00133347
Iteration 6/25 | Loss: 0.00131030
Iteration 7/25 | Loss: 0.00127378
Iteration 8/25 | Loss: 0.00126720
Iteration 9/25 | Loss: 0.00126364
Iteration 10/25 | Loss: 0.00126014
Iteration 11/25 | Loss: 0.00125807
Iteration 12/25 | Loss: 0.00125996
Iteration 13/25 | Loss: 0.00125732
Iteration 14/25 | Loss: 0.00125658
Iteration 15/25 | Loss: 0.00124965
Iteration 16/25 | Loss: 0.00125448
Iteration 17/25 | Loss: 0.00126448
Iteration 18/25 | Loss: 0.00126172
Iteration 19/25 | Loss: 0.00124762
Iteration 20/25 | Loss: 0.00123431
Iteration 21/25 | Loss: 0.00122749
Iteration 22/25 | Loss: 0.00122635
Iteration 23/25 | Loss: 0.00122643
Iteration 24/25 | Loss: 0.00122894
Iteration 25/25 | Loss: 0.00122927

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.14025021
Iteration 2/25 | Loss: 0.00104534
Iteration 3/25 | Loss: 0.00104533
Iteration 4/25 | Loss: 0.00092692
Iteration 5/25 | Loss: 0.00092692
Iteration 6/25 | Loss: 0.00092692
Iteration 7/25 | Loss: 0.00092692
Iteration 8/25 | Loss: 0.00092692
Iteration 9/25 | Loss: 0.00092692
Iteration 10/25 | Loss: 0.00092692
Iteration 11/25 | Loss: 0.00092692
Iteration 12/25 | Loss: 0.00092692
Iteration 13/25 | Loss: 0.00092692
Iteration 14/25 | Loss: 0.00092692
Iteration 15/25 | Loss: 0.00092692
Iteration 16/25 | Loss: 0.00092692
Iteration 17/25 | Loss: 0.00092692
Iteration 18/25 | Loss: 0.00092692
Iteration 19/25 | Loss: 0.00092692
Iteration 20/25 | Loss: 0.00092692
Iteration 21/25 | Loss: 0.00092692
Iteration 22/25 | Loss: 0.00092692
Iteration 23/25 | Loss: 0.00092692
Iteration 24/25 | Loss: 0.00092692
Iteration 25/25 | Loss: 0.00092692

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092692
Iteration 2/1000 | Loss: 0.00008170
Iteration 3/1000 | Loss: 0.00012725
Iteration 4/1000 | Loss: 0.00002879
Iteration 5/1000 | Loss: 0.00002469
Iteration 6/1000 | Loss: 0.00002355
Iteration 7/1000 | Loss: 0.00018274
Iteration 8/1000 | Loss: 0.00010664
Iteration 9/1000 | Loss: 0.00040144
Iteration 10/1000 | Loss: 0.00010891
Iteration 11/1000 | Loss: 0.00021181
Iteration 12/1000 | Loss: 0.00010845
Iteration 13/1000 | Loss: 0.00038385
Iteration 14/1000 | Loss: 0.00074882
Iteration 15/1000 | Loss: 0.00018692
Iteration 16/1000 | Loss: 0.00002968
Iteration 17/1000 | Loss: 0.00009907
Iteration 18/1000 | Loss: 0.00015923
Iteration 19/1000 | Loss: 0.00026548
Iteration 20/1000 | Loss: 0.00024948
Iteration 21/1000 | Loss: 0.00002610
Iteration 22/1000 | Loss: 0.00002353
Iteration 23/1000 | Loss: 0.00002278
Iteration 24/1000 | Loss: 0.00003206
Iteration 25/1000 | Loss: 0.00002341
Iteration 26/1000 | Loss: 0.00027869
Iteration 27/1000 | Loss: 0.00002270
Iteration 28/1000 | Loss: 0.00002140
Iteration 29/1000 | Loss: 0.00002094
Iteration 30/1000 | Loss: 0.00002037
Iteration 31/1000 | Loss: 0.00001998
Iteration 32/1000 | Loss: 0.00001971
Iteration 33/1000 | Loss: 0.00001954
Iteration 34/1000 | Loss: 0.00001953
Iteration 35/1000 | Loss: 0.00001953
Iteration 36/1000 | Loss: 0.00001948
Iteration 37/1000 | Loss: 0.00001948
Iteration 38/1000 | Loss: 0.00001947
Iteration 39/1000 | Loss: 0.00001946
Iteration 40/1000 | Loss: 0.00002331
Iteration 41/1000 | Loss: 0.00001930
Iteration 42/1000 | Loss: 0.00001927
Iteration 43/1000 | Loss: 0.00001927
Iteration 44/1000 | Loss: 0.00001926
Iteration 45/1000 | Loss: 0.00001926
Iteration 46/1000 | Loss: 0.00001925
Iteration 47/1000 | Loss: 0.00001925
Iteration 48/1000 | Loss: 0.00001925
Iteration 49/1000 | Loss: 0.00001924
Iteration 50/1000 | Loss: 0.00001924
Iteration 51/1000 | Loss: 0.00001924
Iteration 52/1000 | Loss: 0.00001924
Iteration 53/1000 | Loss: 0.00001923
Iteration 54/1000 | Loss: 0.00001921
Iteration 55/1000 | Loss: 0.00002372
Iteration 56/1000 | Loss: 0.00001917
Iteration 57/1000 | Loss: 0.00001913
Iteration 58/1000 | Loss: 0.00001913
Iteration 59/1000 | Loss: 0.00001913
Iteration 60/1000 | Loss: 0.00001913
Iteration 61/1000 | Loss: 0.00001913
Iteration 62/1000 | Loss: 0.00001913
Iteration 63/1000 | Loss: 0.00001913
Iteration 64/1000 | Loss: 0.00001913
Iteration 65/1000 | Loss: 0.00001913
Iteration 66/1000 | Loss: 0.00001913
Iteration 67/1000 | Loss: 0.00001913
Iteration 68/1000 | Loss: 0.00001913
Iteration 69/1000 | Loss: 0.00001913
Iteration 70/1000 | Loss: 0.00001912
Iteration 71/1000 | Loss: 0.00001912
Iteration 72/1000 | Loss: 0.00001912
Iteration 73/1000 | Loss: 0.00001912
Iteration 74/1000 | Loss: 0.00001912
Iteration 75/1000 | Loss: 0.00001912
Iteration 76/1000 | Loss: 0.00001912
Iteration 77/1000 | Loss: 0.00001912
Iteration 78/1000 | Loss: 0.00001912
Iteration 79/1000 | Loss: 0.00001912
Iteration 80/1000 | Loss: 0.00001912
Iteration 81/1000 | Loss: 0.00001912
Iteration 82/1000 | Loss: 0.00001911
Iteration 83/1000 | Loss: 0.00001911
Iteration 84/1000 | Loss: 0.00001911
Iteration 85/1000 | Loss: 0.00001911
Iteration 86/1000 | Loss: 0.00001911
Iteration 87/1000 | Loss: 0.00001911
Iteration 88/1000 | Loss: 0.00001911
Iteration 89/1000 | Loss: 0.00001911
Iteration 90/1000 | Loss: 0.00001911
Iteration 91/1000 | Loss: 0.00001911
Iteration 92/1000 | Loss: 0.00001911
Iteration 93/1000 | Loss: 0.00001911
Iteration 94/1000 | Loss: 0.00001910
Iteration 95/1000 | Loss: 0.00001910
Iteration 96/1000 | Loss: 0.00001910
Iteration 97/1000 | Loss: 0.00001909
Iteration 98/1000 | Loss: 0.00001909
Iteration 99/1000 | Loss: 0.00001909
Iteration 100/1000 | Loss: 0.00001909
Iteration 101/1000 | Loss: 0.00001908
Iteration 102/1000 | Loss: 0.00001908
Iteration 103/1000 | Loss: 0.00001908
Iteration 104/1000 | Loss: 0.00001907
Iteration 105/1000 | Loss: 0.00001907
Iteration 106/1000 | Loss: 0.00001907
Iteration 107/1000 | Loss: 0.00001907
Iteration 108/1000 | Loss: 0.00001907
Iteration 109/1000 | Loss: 0.00001906
Iteration 110/1000 | Loss: 0.00001906
Iteration 111/1000 | Loss: 0.00001906
Iteration 112/1000 | Loss: 0.00001906
Iteration 113/1000 | Loss: 0.00001906
Iteration 114/1000 | Loss: 0.00001906
Iteration 115/1000 | Loss: 0.00001906
Iteration 116/1000 | Loss: 0.00001906
Iteration 117/1000 | Loss: 0.00001906
Iteration 118/1000 | Loss: 0.00001906
Iteration 119/1000 | Loss: 0.00001906
Iteration 120/1000 | Loss: 0.00001906
Iteration 121/1000 | Loss: 0.00001906
Iteration 122/1000 | Loss: 0.00001906
Iteration 123/1000 | Loss: 0.00001906
Iteration 124/1000 | Loss: 0.00001906
Iteration 125/1000 | Loss: 0.00001906
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.9060973500018008e-05, 1.9060973500018008e-05, 1.9060973500018008e-05, 1.9060973500018008e-05, 1.9060973500018008e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9060973500018008e-05

Optimization complete. Final v2v error: 3.623917818069458 mm

Highest mean error: 5.3534464836120605 mm for frame 187

Lowest mean error: 3.258781671524048 mm for frame 212

Saving results

Total time: 118.871901512146
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00827396
Iteration 2/25 | Loss: 0.00135776
Iteration 3/25 | Loss: 0.00113113
Iteration 4/25 | Loss: 0.00109927
Iteration 5/25 | Loss: 0.00109267
Iteration 6/25 | Loss: 0.00109144
Iteration 7/25 | Loss: 0.00109138
Iteration 8/25 | Loss: 0.00109138
Iteration 9/25 | Loss: 0.00109138
Iteration 10/25 | Loss: 0.00109138
Iteration 11/25 | Loss: 0.00109138
Iteration 12/25 | Loss: 0.00109138
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010913815349340439, 0.0010913815349340439, 0.0010913815349340439, 0.0010913815349340439, 0.0010913815349340439]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010913815349340439

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.95845813
Iteration 2/25 | Loss: 0.00049263
Iteration 3/25 | Loss: 0.00049262
Iteration 4/25 | Loss: 0.00049261
Iteration 5/25 | Loss: 0.00049261
Iteration 6/25 | Loss: 0.00049261
Iteration 7/25 | Loss: 0.00049261
Iteration 8/25 | Loss: 0.00049261
Iteration 9/25 | Loss: 0.00049261
Iteration 10/25 | Loss: 0.00049261
Iteration 11/25 | Loss: 0.00049261
Iteration 12/25 | Loss: 0.00049261
Iteration 13/25 | Loss: 0.00049261
Iteration 14/25 | Loss: 0.00049261
Iteration 15/25 | Loss: 0.00049261
Iteration 16/25 | Loss: 0.00049261
Iteration 17/25 | Loss: 0.00049261
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0004926123074255884, 0.0004926123074255884, 0.0004926123074255884, 0.0004926123074255884, 0.0004926123074255884]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004926123074255884

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049261
Iteration 2/1000 | Loss: 0.00003896
Iteration 3/1000 | Loss: 0.00002988
Iteration 4/1000 | Loss: 0.00002568
Iteration 5/1000 | Loss: 0.00002372
Iteration 6/1000 | Loss: 0.00002233
Iteration 7/1000 | Loss: 0.00002148
Iteration 8/1000 | Loss: 0.00002091
Iteration 9/1000 | Loss: 0.00002059
Iteration 10/1000 | Loss: 0.00002034
Iteration 11/1000 | Loss: 0.00002009
Iteration 12/1000 | Loss: 0.00001987
Iteration 13/1000 | Loss: 0.00001981
Iteration 14/1000 | Loss: 0.00001973
Iteration 15/1000 | Loss: 0.00001967
Iteration 16/1000 | Loss: 0.00001951
Iteration 17/1000 | Loss: 0.00001947
Iteration 18/1000 | Loss: 0.00001947
Iteration 19/1000 | Loss: 0.00001945
Iteration 20/1000 | Loss: 0.00001944
Iteration 21/1000 | Loss: 0.00001944
Iteration 22/1000 | Loss: 0.00001944
Iteration 23/1000 | Loss: 0.00001944
Iteration 24/1000 | Loss: 0.00001944
Iteration 25/1000 | Loss: 0.00001943
Iteration 26/1000 | Loss: 0.00001943
Iteration 27/1000 | Loss: 0.00001943
Iteration 28/1000 | Loss: 0.00001942
Iteration 29/1000 | Loss: 0.00001942
Iteration 30/1000 | Loss: 0.00001941
Iteration 31/1000 | Loss: 0.00001941
Iteration 32/1000 | Loss: 0.00001941
Iteration 33/1000 | Loss: 0.00001941
Iteration 34/1000 | Loss: 0.00001940
Iteration 35/1000 | Loss: 0.00001940
Iteration 36/1000 | Loss: 0.00001940
Iteration 37/1000 | Loss: 0.00001938
Iteration 38/1000 | Loss: 0.00001938
Iteration 39/1000 | Loss: 0.00001938
Iteration 40/1000 | Loss: 0.00001938
Iteration 41/1000 | Loss: 0.00001938
Iteration 42/1000 | Loss: 0.00001938
Iteration 43/1000 | Loss: 0.00001938
Iteration 44/1000 | Loss: 0.00001938
Iteration 45/1000 | Loss: 0.00001938
Iteration 46/1000 | Loss: 0.00001938
Iteration 47/1000 | Loss: 0.00001937
Iteration 48/1000 | Loss: 0.00001937
Iteration 49/1000 | Loss: 0.00001937
Iteration 50/1000 | Loss: 0.00001937
Iteration 51/1000 | Loss: 0.00001937
Iteration 52/1000 | Loss: 0.00001937
Iteration 53/1000 | Loss: 0.00001937
Iteration 54/1000 | Loss: 0.00001936
Iteration 55/1000 | Loss: 0.00001935
Iteration 56/1000 | Loss: 0.00001935
Iteration 57/1000 | Loss: 0.00001935
Iteration 58/1000 | Loss: 0.00001935
Iteration 59/1000 | Loss: 0.00001935
Iteration 60/1000 | Loss: 0.00001934
Iteration 61/1000 | Loss: 0.00001934
Iteration 62/1000 | Loss: 0.00001934
Iteration 63/1000 | Loss: 0.00001934
Iteration 64/1000 | Loss: 0.00001934
Iteration 65/1000 | Loss: 0.00001933
Iteration 66/1000 | Loss: 0.00001933
Iteration 67/1000 | Loss: 0.00001932
Iteration 68/1000 | Loss: 0.00001932
Iteration 69/1000 | Loss: 0.00001932
Iteration 70/1000 | Loss: 0.00001932
Iteration 71/1000 | Loss: 0.00001932
Iteration 72/1000 | Loss: 0.00001932
Iteration 73/1000 | Loss: 0.00001931
Iteration 74/1000 | Loss: 0.00001931
Iteration 75/1000 | Loss: 0.00001931
Iteration 76/1000 | Loss: 0.00001931
Iteration 77/1000 | Loss: 0.00001931
Iteration 78/1000 | Loss: 0.00001931
Iteration 79/1000 | Loss: 0.00001931
Iteration 80/1000 | Loss: 0.00001930
Iteration 81/1000 | Loss: 0.00001930
Iteration 82/1000 | Loss: 0.00001930
Iteration 83/1000 | Loss: 0.00001930
Iteration 84/1000 | Loss: 0.00001929
Iteration 85/1000 | Loss: 0.00001929
Iteration 86/1000 | Loss: 0.00001929
Iteration 87/1000 | Loss: 0.00001929
Iteration 88/1000 | Loss: 0.00001929
Iteration 89/1000 | Loss: 0.00001929
Iteration 90/1000 | Loss: 0.00001929
Iteration 91/1000 | Loss: 0.00001929
Iteration 92/1000 | Loss: 0.00001929
Iteration 93/1000 | Loss: 0.00001928
Iteration 94/1000 | Loss: 0.00001928
Iteration 95/1000 | Loss: 0.00001928
Iteration 96/1000 | Loss: 0.00001928
Iteration 97/1000 | Loss: 0.00001928
Iteration 98/1000 | Loss: 0.00001928
Iteration 99/1000 | Loss: 0.00001928
Iteration 100/1000 | Loss: 0.00001927
Iteration 101/1000 | Loss: 0.00001927
Iteration 102/1000 | Loss: 0.00001927
Iteration 103/1000 | Loss: 0.00001927
Iteration 104/1000 | Loss: 0.00001927
Iteration 105/1000 | Loss: 0.00001927
Iteration 106/1000 | Loss: 0.00001927
Iteration 107/1000 | Loss: 0.00001926
Iteration 108/1000 | Loss: 0.00001926
Iteration 109/1000 | Loss: 0.00001926
Iteration 110/1000 | Loss: 0.00001926
Iteration 111/1000 | Loss: 0.00001925
Iteration 112/1000 | Loss: 0.00001925
Iteration 113/1000 | Loss: 0.00001925
Iteration 114/1000 | Loss: 0.00001924
Iteration 115/1000 | Loss: 0.00001924
Iteration 116/1000 | Loss: 0.00001924
Iteration 117/1000 | Loss: 0.00001924
Iteration 118/1000 | Loss: 0.00001924
Iteration 119/1000 | Loss: 0.00001923
Iteration 120/1000 | Loss: 0.00001923
Iteration 121/1000 | Loss: 0.00001923
Iteration 122/1000 | Loss: 0.00001923
Iteration 123/1000 | Loss: 0.00001923
Iteration 124/1000 | Loss: 0.00001923
Iteration 125/1000 | Loss: 0.00001923
Iteration 126/1000 | Loss: 0.00001923
Iteration 127/1000 | Loss: 0.00001922
Iteration 128/1000 | Loss: 0.00001921
Iteration 129/1000 | Loss: 0.00001921
Iteration 130/1000 | Loss: 0.00001921
Iteration 131/1000 | Loss: 0.00001921
Iteration 132/1000 | Loss: 0.00001920
Iteration 133/1000 | Loss: 0.00001919
Iteration 134/1000 | Loss: 0.00001919
Iteration 135/1000 | Loss: 0.00001919
Iteration 136/1000 | Loss: 0.00001919
Iteration 137/1000 | Loss: 0.00001919
Iteration 138/1000 | Loss: 0.00001919
Iteration 139/1000 | Loss: 0.00001918
Iteration 140/1000 | Loss: 0.00001918
Iteration 141/1000 | Loss: 0.00001918
Iteration 142/1000 | Loss: 0.00001918
Iteration 143/1000 | Loss: 0.00001918
Iteration 144/1000 | Loss: 0.00001918
Iteration 145/1000 | Loss: 0.00001918
Iteration 146/1000 | Loss: 0.00001917
Iteration 147/1000 | Loss: 0.00001917
Iteration 148/1000 | Loss: 0.00001917
Iteration 149/1000 | Loss: 0.00001917
Iteration 150/1000 | Loss: 0.00001917
Iteration 151/1000 | Loss: 0.00001916
Iteration 152/1000 | Loss: 0.00001916
Iteration 153/1000 | Loss: 0.00001916
Iteration 154/1000 | Loss: 0.00001916
Iteration 155/1000 | Loss: 0.00001916
Iteration 156/1000 | Loss: 0.00001916
Iteration 157/1000 | Loss: 0.00001916
Iteration 158/1000 | Loss: 0.00001916
Iteration 159/1000 | Loss: 0.00001916
Iteration 160/1000 | Loss: 0.00001916
Iteration 161/1000 | Loss: 0.00001916
Iteration 162/1000 | Loss: 0.00001915
Iteration 163/1000 | Loss: 0.00001915
Iteration 164/1000 | Loss: 0.00001915
Iteration 165/1000 | Loss: 0.00001915
Iteration 166/1000 | Loss: 0.00001915
Iteration 167/1000 | Loss: 0.00001915
Iteration 168/1000 | Loss: 0.00001914
Iteration 169/1000 | Loss: 0.00001914
Iteration 170/1000 | Loss: 0.00001914
Iteration 171/1000 | Loss: 0.00001914
Iteration 172/1000 | Loss: 0.00001914
Iteration 173/1000 | Loss: 0.00001914
Iteration 174/1000 | Loss: 0.00001914
Iteration 175/1000 | Loss: 0.00001914
Iteration 176/1000 | Loss: 0.00001913
Iteration 177/1000 | Loss: 0.00001913
Iteration 178/1000 | Loss: 0.00001913
Iteration 179/1000 | Loss: 0.00001913
Iteration 180/1000 | Loss: 0.00001913
Iteration 181/1000 | Loss: 0.00001913
Iteration 182/1000 | Loss: 0.00001912
Iteration 183/1000 | Loss: 0.00001912
Iteration 184/1000 | Loss: 0.00001912
Iteration 185/1000 | Loss: 0.00001912
Iteration 186/1000 | Loss: 0.00001912
Iteration 187/1000 | Loss: 0.00001912
Iteration 188/1000 | Loss: 0.00001912
Iteration 189/1000 | Loss: 0.00001911
Iteration 190/1000 | Loss: 0.00001911
Iteration 191/1000 | Loss: 0.00001911
Iteration 192/1000 | Loss: 0.00001911
Iteration 193/1000 | Loss: 0.00001911
Iteration 194/1000 | Loss: 0.00001911
Iteration 195/1000 | Loss: 0.00001911
Iteration 196/1000 | Loss: 0.00001911
Iteration 197/1000 | Loss: 0.00001911
Iteration 198/1000 | Loss: 0.00001911
Iteration 199/1000 | Loss: 0.00001911
Iteration 200/1000 | Loss: 0.00001911
Iteration 201/1000 | Loss: 0.00001911
Iteration 202/1000 | Loss: 0.00001911
Iteration 203/1000 | Loss: 0.00001911
Iteration 204/1000 | Loss: 0.00001911
Iteration 205/1000 | Loss: 0.00001911
Iteration 206/1000 | Loss: 0.00001910
Iteration 207/1000 | Loss: 0.00001910
Iteration 208/1000 | Loss: 0.00001910
Iteration 209/1000 | Loss: 0.00001910
Iteration 210/1000 | Loss: 0.00001910
Iteration 211/1000 | Loss: 0.00001910
Iteration 212/1000 | Loss: 0.00001910
Iteration 213/1000 | Loss: 0.00001910
Iteration 214/1000 | Loss: 0.00001910
Iteration 215/1000 | Loss: 0.00001909
Iteration 216/1000 | Loss: 0.00001909
Iteration 217/1000 | Loss: 0.00001909
Iteration 218/1000 | Loss: 0.00001909
Iteration 219/1000 | Loss: 0.00001909
Iteration 220/1000 | Loss: 0.00001909
Iteration 221/1000 | Loss: 0.00001909
Iteration 222/1000 | Loss: 0.00001909
Iteration 223/1000 | Loss: 0.00001909
Iteration 224/1000 | Loss: 0.00001909
Iteration 225/1000 | Loss: 0.00001909
Iteration 226/1000 | Loss: 0.00001909
Iteration 227/1000 | Loss: 0.00001909
Iteration 228/1000 | Loss: 0.00001909
Iteration 229/1000 | Loss: 0.00001909
Iteration 230/1000 | Loss: 0.00001908
Iteration 231/1000 | Loss: 0.00001908
Iteration 232/1000 | Loss: 0.00001908
Iteration 233/1000 | Loss: 0.00001908
Iteration 234/1000 | Loss: 0.00001908
Iteration 235/1000 | Loss: 0.00001908
Iteration 236/1000 | Loss: 0.00001908
Iteration 237/1000 | Loss: 0.00001908
Iteration 238/1000 | Loss: 0.00001908
Iteration 239/1000 | Loss: 0.00001908
Iteration 240/1000 | Loss: 0.00001908
Iteration 241/1000 | Loss: 0.00001908
Iteration 242/1000 | Loss: 0.00001908
Iteration 243/1000 | Loss: 0.00001908
Iteration 244/1000 | Loss: 0.00001908
Iteration 245/1000 | Loss: 0.00001908
Iteration 246/1000 | Loss: 0.00001908
Iteration 247/1000 | Loss: 0.00001907
Iteration 248/1000 | Loss: 0.00001907
Iteration 249/1000 | Loss: 0.00001907
Iteration 250/1000 | Loss: 0.00001907
Iteration 251/1000 | Loss: 0.00001907
Iteration 252/1000 | Loss: 0.00001907
Iteration 253/1000 | Loss: 0.00001907
Iteration 254/1000 | Loss: 0.00001907
Iteration 255/1000 | Loss: 0.00001907
Iteration 256/1000 | Loss: 0.00001907
Iteration 257/1000 | Loss: 0.00001907
Iteration 258/1000 | Loss: 0.00001907
Iteration 259/1000 | Loss: 0.00001907
Iteration 260/1000 | Loss: 0.00001907
Iteration 261/1000 | Loss: 0.00001907
Iteration 262/1000 | Loss: 0.00001906
Iteration 263/1000 | Loss: 0.00001906
Iteration 264/1000 | Loss: 0.00001906
Iteration 265/1000 | Loss: 0.00001906
Iteration 266/1000 | Loss: 0.00001906
Iteration 267/1000 | Loss: 0.00001906
Iteration 268/1000 | Loss: 0.00001906
Iteration 269/1000 | Loss: 0.00001906
Iteration 270/1000 | Loss: 0.00001906
Iteration 271/1000 | Loss: 0.00001906
Iteration 272/1000 | Loss: 0.00001906
Iteration 273/1000 | Loss: 0.00001906
Iteration 274/1000 | Loss: 0.00001906
Iteration 275/1000 | Loss: 0.00001906
Iteration 276/1000 | Loss: 0.00001906
Iteration 277/1000 | Loss: 0.00001906
Iteration 278/1000 | Loss: 0.00001906
Iteration 279/1000 | Loss: 0.00001906
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 279. Stopping optimization.
Last 5 losses: [1.9060680642724037e-05, 1.9060680642724037e-05, 1.9060680642724037e-05, 1.9060680642724037e-05, 1.9060680642724037e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9060680642724037e-05

Optimization complete. Final v2v error: 3.792771339416504 mm

Highest mean error: 4.0650458335876465 mm for frame 131

Lowest mean error: 3.571211576461792 mm for frame 109

Saving results

Total time: 46.043860912323
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00798957
Iteration 2/25 | Loss: 0.00123449
Iteration 3/25 | Loss: 0.00113035
Iteration 4/25 | Loss: 0.00111704
Iteration 5/25 | Loss: 0.00111364
Iteration 6/25 | Loss: 0.00111303
Iteration 7/25 | Loss: 0.00111303
Iteration 8/25 | Loss: 0.00111303
Iteration 9/25 | Loss: 0.00111303
Iteration 10/25 | Loss: 0.00111303
Iteration 11/25 | Loss: 0.00111303
Iteration 12/25 | Loss: 0.00111303
Iteration 13/25 | Loss: 0.00111303
Iteration 14/25 | Loss: 0.00111303
Iteration 15/25 | Loss: 0.00111303
Iteration 16/25 | Loss: 0.00111303
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011130310595035553, 0.0011130310595035553, 0.0011130310595035553, 0.0011130310595035553, 0.0011130310595035553]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011130310595035553

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42779112
Iteration 2/25 | Loss: 0.00078848
Iteration 3/25 | Loss: 0.00078848
Iteration 4/25 | Loss: 0.00078848
Iteration 5/25 | Loss: 0.00078848
Iteration 6/25 | Loss: 0.00078848
Iteration 7/25 | Loss: 0.00078848
Iteration 8/25 | Loss: 0.00078848
Iteration 9/25 | Loss: 0.00078848
Iteration 10/25 | Loss: 0.00078848
Iteration 11/25 | Loss: 0.00078848
Iteration 12/25 | Loss: 0.00078848
Iteration 13/25 | Loss: 0.00078848
Iteration 14/25 | Loss: 0.00078848
Iteration 15/25 | Loss: 0.00078848
Iteration 16/25 | Loss: 0.00078848
Iteration 17/25 | Loss: 0.00078848
Iteration 18/25 | Loss: 0.00078848
Iteration 19/25 | Loss: 0.00078848
Iteration 20/25 | Loss: 0.00078848
Iteration 21/25 | Loss: 0.00078848
Iteration 22/25 | Loss: 0.00078848
Iteration 23/25 | Loss: 0.00078848
Iteration 24/25 | Loss: 0.00078848
Iteration 25/25 | Loss: 0.00078848
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.00078847527038306, 0.00078847527038306, 0.00078847527038306, 0.00078847527038306, 0.00078847527038306]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00078847527038306

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078848
Iteration 2/1000 | Loss: 0.00003219
Iteration 3/1000 | Loss: 0.00001886
Iteration 4/1000 | Loss: 0.00001703
Iteration 5/1000 | Loss: 0.00001600
Iteration 6/1000 | Loss: 0.00001532
Iteration 7/1000 | Loss: 0.00001484
Iteration 8/1000 | Loss: 0.00001476
Iteration 9/1000 | Loss: 0.00001449
Iteration 10/1000 | Loss: 0.00001427
Iteration 11/1000 | Loss: 0.00001405
Iteration 12/1000 | Loss: 0.00001395
Iteration 13/1000 | Loss: 0.00001394
Iteration 14/1000 | Loss: 0.00001388
Iteration 15/1000 | Loss: 0.00001385
Iteration 16/1000 | Loss: 0.00001383
Iteration 17/1000 | Loss: 0.00001381
Iteration 18/1000 | Loss: 0.00001380
Iteration 19/1000 | Loss: 0.00001379
Iteration 20/1000 | Loss: 0.00001379
Iteration 21/1000 | Loss: 0.00001378
Iteration 22/1000 | Loss: 0.00001376
Iteration 23/1000 | Loss: 0.00001376
Iteration 24/1000 | Loss: 0.00001376
Iteration 25/1000 | Loss: 0.00001375
Iteration 26/1000 | Loss: 0.00001375
Iteration 27/1000 | Loss: 0.00001374
Iteration 28/1000 | Loss: 0.00001373
Iteration 29/1000 | Loss: 0.00001372
Iteration 30/1000 | Loss: 0.00001372
Iteration 31/1000 | Loss: 0.00001370
Iteration 32/1000 | Loss: 0.00001370
Iteration 33/1000 | Loss: 0.00001369
Iteration 34/1000 | Loss: 0.00001368
Iteration 35/1000 | Loss: 0.00001367
Iteration 36/1000 | Loss: 0.00001366
Iteration 37/1000 | Loss: 0.00001363
Iteration 38/1000 | Loss: 0.00001363
Iteration 39/1000 | Loss: 0.00001363
Iteration 40/1000 | Loss: 0.00001363
Iteration 41/1000 | Loss: 0.00001363
Iteration 42/1000 | Loss: 0.00001363
Iteration 43/1000 | Loss: 0.00001363
Iteration 44/1000 | Loss: 0.00001362
Iteration 45/1000 | Loss: 0.00001361
Iteration 46/1000 | Loss: 0.00001359
Iteration 47/1000 | Loss: 0.00001358
Iteration 48/1000 | Loss: 0.00001358
Iteration 49/1000 | Loss: 0.00001358
Iteration 50/1000 | Loss: 0.00001357
Iteration 51/1000 | Loss: 0.00001357
Iteration 52/1000 | Loss: 0.00001356
Iteration 53/1000 | Loss: 0.00001355
Iteration 54/1000 | Loss: 0.00001355
Iteration 55/1000 | Loss: 0.00001355
Iteration 56/1000 | Loss: 0.00001354
Iteration 57/1000 | Loss: 0.00001354
Iteration 58/1000 | Loss: 0.00001354
Iteration 59/1000 | Loss: 0.00001353
Iteration 60/1000 | Loss: 0.00001353
Iteration 61/1000 | Loss: 0.00001353
Iteration 62/1000 | Loss: 0.00001352
Iteration 63/1000 | Loss: 0.00001352
Iteration 64/1000 | Loss: 0.00001351
Iteration 65/1000 | Loss: 0.00001351
Iteration 66/1000 | Loss: 0.00001350
Iteration 67/1000 | Loss: 0.00001350
Iteration 68/1000 | Loss: 0.00001350
Iteration 69/1000 | Loss: 0.00001350
Iteration 70/1000 | Loss: 0.00001350
Iteration 71/1000 | Loss: 0.00001350
Iteration 72/1000 | Loss: 0.00001349
Iteration 73/1000 | Loss: 0.00001349
Iteration 74/1000 | Loss: 0.00001349
Iteration 75/1000 | Loss: 0.00001349
Iteration 76/1000 | Loss: 0.00001349
Iteration 77/1000 | Loss: 0.00001349
Iteration 78/1000 | Loss: 0.00001349
Iteration 79/1000 | Loss: 0.00001349
Iteration 80/1000 | Loss: 0.00001349
Iteration 81/1000 | Loss: 0.00001349
Iteration 82/1000 | Loss: 0.00001348
Iteration 83/1000 | Loss: 0.00001347
Iteration 84/1000 | Loss: 0.00001347
Iteration 85/1000 | Loss: 0.00001347
Iteration 86/1000 | Loss: 0.00001347
Iteration 87/1000 | Loss: 0.00001347
Iteration 88/1000 | Loss: 0.00001347
Iteration 89/1000 | Loss: 0.00001347
Iteration 90/1000 | Loss: 0.00001347
Iteration 91/1000 | Loss: 0.00001347
Iteration 92/1000 | Loss: 0.00001347
Iteration 93/1000 | Loss: 0.00001346
Iteration 94/1000 | Loss: 0.00001346
Iteration 95/1000 | Loss: 0.00001346
Iteration 96/1000 | Loss: 0.00001346
Iteration 97/1000 | Loss: 0.00001346
Iteration 98/1000 | Loss: 0.00001346
Iteration 99/1000 | Loss: 0.00001346
Iteration 100/1000 | Loss: 0.00001346
Iteration 101/1000 | Loss: 0.00001345
Iteration 102/1000 | Loss: 0.00001345
Iteration 103/1000 | Loss: 0.00001345
Iteration 104/1000 | Loss: 0.00001345
Iteration 105/1000 | Loss: 0.00001345
Iteration 106/1000 | Loss: 0.00001344
Iteration 107/1000 | Loss: 0.00001344
Iteration 108/1000 | Loss: 0.00001343
Iteration 109/1000 | Loss: 0.00001343
Iteration 110/1000 | Loss: 0.00001343
Iteration 111/1000 | Loss: 0.00001343
Iteration 112/1000 | Loss: 0.00001343
Iteration 113/1000 | Loss: 0.00001343
Iteration 114/1000 | Loss: 0.00001342
Iteration 115/1000 | Loss: 0.00001342
Iteration 116/1000 | Loss: 0.00001342
Iteration 117/1000 | Loss: 0.00001342
Iteration 118/1000 | Loss: 0.00001342
Iteration 119/1000 | Loss: 0.00001342
Iteration 120/1000 | Loss: 0.00001341
Iteration 121/1000 | Loss: 0.00001341
Iteration 122/1000 | Loss: 0.00001341
Iteration 123/1000 | Loss: 0.00001341
Iteration 124/1000 | Loss: 0.00001340
Iteration 125/1000 | Loss: 0.00001340
Iteration 126/1000 | Loss: 0.00001340
Iteration 127/1000 | Loss: 0.00001340
Iteration 128/1000 | Loss: 0.00001340
Iteration 129/1000 | Loss: 0.00001339
Iteration 130/1000 | Loss: 0.00001339
Iteration 131/1000 | Loss: 0.00001339
Iteration 132/1000 | Loss: 0.00001339
Iteration 133/1000 | Loss: 0.00001339
Iteration 134/1000 | Loss: 0.00001339
Iteration 135/1000 | Loss: 0.00001339
Iteration 136/1000 | Loss: 0.00001339
Iteration 137/1000 | Loss: 0.00001338
Iteration 138/1000 | Loss: 0.00001338
Iteration 139/1000 | Loss: 0.00001338
Iteration 140/1000 | Loss: 0.00001337
Iteration 141/1000 | Loss: 0.00001337
Iteration 142/1000 | Loss: 0.00001337
Iteration 143/1000 | Loss: 0.00001337
Iteration 144/1000 | Loss: 0.00001337
Iteration 145/1000 | Loss: 0.00001336
Iteration 146/1000 | Loss: 0.00001336
Iteration 147/1000 | Loss: 0.00001336
Iteration 148/1000 | Loss: 0.00001336
Iteration 149/1000 | Loss: 0.00001336
Iteration 150/1000 | Loss: 0.00001336
Iteration 151/1000 | Loss: 0.00001336
Iteration 152/1000 | Loss: 0.00001336
Iteration 153/1000 | Loss: 0.00001336
Iteration 154/1000 | Loss: 0.00001336
Iteration 155/1000 | Loss: 0.00001336
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.3362612662604079e-05, 1.3362612662604079e-05, 1.3362612662604079e-05, 1.3362612662604079e-05, 1.3362612662604079e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3362612662604079e-05

Optimization complete. Final v2v error: 3.0405359268188477 mm

Highest mean error: 4.165042400360107 mm for frame 136

Lowest mean error: 2.449455499649048 mm for frame 202

Saving results

Total time: 42.8717839717865
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00464376
Iteration 2/25 | Loss: 0.00140686
Iteration 3/25 | Loss: 0.00121379
Iteration 4/25 | Loss: 0.00118848
Iteration 5/25 | Loss: 0.00118545
Iteration 6/25 | Loss: 0.00118518
Iteration 7/25 | Loss: 0.00118518
Iteration 8/25 | Loss: 0.00118518
Iteration 9/25 | Loss: 0.00118518
Iteration 10/25 | Loss: 0.00118518
Iteration 11/25 | Loss: 0.00118518
Iteration 12/25 | Loss: 0.00118518
Iteration 13/25 | Loss: 0.00118518
Iteration 14/25 | Loss: 0.00118518
Iteration 15/25 | Loss: 0.00118518
Iteration 16/25 | Loss: 0.00118518
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011851786402985454, 0.0011851786402985454, 0.0011851786402985454, 0.0011851786402985454, 0.0011851786402985454]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011851786402985454

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35221243
Iteration 2/25 | Loss: 0.00088347
Iteration 3/25 | Loss: 0.00088346
Iteration 4/25 | Loss: 0.00088346
Iteration 5/25 | Loss: 0.00088345
Iteration 6/25 | Loss: 0.00088345
Iteration 7/25 | Loss: 0.00088345
Iteration 8/25 | Loss: 0.00088345
Iteration 9/25 | Loss: 0.00088345
Iteration 10/25 | Loss: 0.00088345
Iteration 11/25 | Loss: 0.00088345
Iteration 12/25 | Loss: 0.00088345
Iteration 13/25 | Loss: 0.00088345
Iteration 14/25 | Loss: 0.00088345
Iteration 15/25 | Loss: 0.00088345
Iteration 16/25 | Loss: 0.00088345
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008834530017338693, 0.0008834530017338693, 0.0008834530017338693, 0.0008834530017338693, 0.0008834530017338693]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008834530017338693

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088345
Iteration 2/1000 | Loss: 0.00004434
Iteration 3/1000 | Loss: 0.00002620
Iteration 4/1000 | Loss: 0.00002390
Iteration 5/1000 | Loss: 0.00002253
Iteration 6/1000 | Loss: 0.00002137
Iteration 7/1000 | Loss: 0.00002063
Iteration 8/1000 | Loss: 0.00002004
Iteration 9/1000 | Loss: 0.00001946
Iteration 10/1000 | Loss: 0.00001900
Iteration 11/1000 | Loss: 0.00001869
Iteration 12/1000 | Loss: 0.00001849
Iteration 13/1000 | Loss: 0.00001842
Iteration 14/1000 | Loss: 0.00001828
Iteration 15/1000 | Loss: 0.00001820
Iteration 16/1000 | Loss: 0.00001819
Iteration 17/1000 | Loss: 0.00001809
Iteration 18/1000 | Loss: 0.00001807
Iteration 19/1000 | Loss: 0.00001803
Iteration 20/1000 | Loss: 0.00001802
Iteration 21/1000 | Loss: 0.00001801
Iteration 22/1000 | Loss: 0.00001801
Iteration 23/1000 | Loss: 0.00001796
Iteration 24/1000 | Loss: 0.00001795
Iteration 25/1000 | Loss: 0.00001793
Iteration 26/1000 | Loss: 0.00001793
Iteration 27/1000 | Loss: 0.00001793
Iteration 28/1000 | Loss: 0.00001793
Iteration 29/1000 | Loss: 0.00001793
Iteration 30/1000 | Loss: 0.00001792
Iteration 31/1000 | Loss: 0.00001792
Iteration 32/1000 | Loss: 0.00001792
Iteration 33/1000 | Loss: 0.00001792
Iteration 34/1000 | Loss: 0.00001792
Iteration 35/1000 | Loss: 0.00001791
Iteration 36/1000 | Loss: 0.00001791
Iteration 37/1000 | Loss: 0.00001791
Iteration 38/1000 | Loss: 0.00001790
Iteration 39/1000 | Loss: 0.00001790
Iteration 40/1000 | Loss: 0.00001789
Iteration 41/1000 | Loss: 0.00001789
Iteration 42/1000 | Loss: 0.00001788
Iteration 43/1000 | Loss: 0.00001788
Iteration 44/1000 | Loss: 0.00001788
Iteration 45/1000 | Loss: 0.00001788
Iteration 46/1000 | Loss: 0.00001788
Iteration 47/1000 | Loss: 0.00001788
Iteration 48/1000 | Loss: 0.00001788
Iteration 49/1000 | Loss: 0.00001788
Iteration 50/1000 | Loss: 0.00001787
Iteration 51/1000 | Loss: 0.00001787
Iteration 52/1000 | Loss: 0.00001786
Iteration 53/1000 | Loss: 0.00001786
Iteration 54/1000 | Loss: 0.00001786
Iteration 55/1000 | Loss: 0.00001785
Iteration 56/1000 | Loss: 0.00001785
Iteration 57/1000 | Loss: 0.00001784
Iteration 58/1000 | Loss: 0.00001784
Iteration 59/1000 | Loss: 0.00001783
Iteration 60/1000 | Loss: 0.00001783
Iteration 61/1000 | Loss: 0.00001783
Iteration 62/1000 | Loss: 0.00001782
Iteration 63/1000 | Loss: 0.00001782
Iteration 64/1000 | Loss: 0.00001782
Iteration 65/1000 | Loss: 0.00001782
Iteration 66/1000 | Loss: 0.00001781
Iteration 67/1000 | Loss: 0.00001781
Iteration 68/1000 | Loss: 0.00001781
Iteration 69/1000 | Loss: 0.00001781
Iteration 70/1000 | Loss: 0.00001781
Iteration 71/1000 | Loss: 0.00001780
Iteration 72/1000 | Loss: 0.00001780
Iteration 73/1000 | Loss: 0.00001780
Iteration 74/1000 | Loss: 0.00001779
Iteration 75/1000 | Loss: 0.00001779
Iteration 76/1000 | Loss: 0.00001779
Iteration 77/1000 | Loss: 0.00001778
Iteration 78/1000 | Loss: 0.00001778
Iteration 79/1000 | Loss: 0.00001777
Iteration 80/1000 | Loss: 0.00001777
Iteration 81/1000 | Loss: 0.00001777
Iteration 82/1000 | Loss: 0.00001777
Iteration 83/1000 | Loss: 0.00001777
Iteration 84/1000 | Loss: 0.00001776
Iteration 85/1000 | Loss: 0.00001776
Iteration 86/1000 | Loss: 0.00001776
Iteration 87/1000 | Loss: 0.00001776
Iteration 88/1000 | Loss: 0.00001775
Iteration 89/1000 | Loss: 0.00001775
Iteration 90/1000 | Loss: 0.00001775
Iteration 91/1000 | Loss: 0.00001774
Iteration 92/1000 | Loss: 0.00001774
Iteration 93/1000 | Loss: 0.00001774
Iteration 94/1000 | Loss: 0.00001774
Iteration 95/1000 | Loss: 0.00001774
Iteration 96/1000 | Loss: 0.00001773
Iteration 97/1000 | Loss: 0.00001773
Iteration 98/1000 | Loss: 0.00001773
Iteration 99/1000 | Loss: 0.00001773
Iteration 100/1000 | Loss: 0.00001772
Iteration 101/1000 | Loss: 0.00001772
Iteration 102/1000 | Loss: 0.00001772
Iteration 103/1000 | Loss: 0.00001772
Iteration 104/1000 | Loss: 0.00001772
Iteration 105/1000 | Loss: 0.00001772
Iteration 106/1000 | Loss: 0.00001772
Iteration 107/1000 | Loss: 0.00001772
Iteration 108/1000 | Loss: 0.00001772
Iteration 109/1000 | Loss: 0.00001771
Iteration 110/1000 | Loss: 0.00001771
Iteration 111/1000 | Loss: 0.00001771
Iteration 112/1000 | Loss: 0.00001771
Iteration 113/1000 | Loss: 0.00001771
Iteration 114/1000 | Loss: 0.00001771
Iteration 115/1000 | Loss: 0.00001771
Iteration 116/1000 | Loss: 0.00001771
Iteration 117/1000 | Loss: 0.00001770
Iteration 118/1000 | Loss: 0.00001770
Iteration 119/1000 | Loss: 0.00001770
Iteration 120/1000 | Loss: 0.00001770
Iteration 121/1000 | Loss: 0.00001769
Iteration 122/1000 | Loss: 0.00001769
Iteration 123/1000 | Loss: 0.00001769
Iteration 124/1000 | Loss: 0.00001769
Iteration 125/1000 | Loss: 0.00001768
Iteration 126/1000 | Loss: 0.00001768
Iteration 127/1000 | Loss: 0.00001768
Iteration 128/1000 | Loss: 0.00001768
Iteration 129/1000 | Loss: 0.00001768
Iteration 130/1000 | Loss: 0.00001767
Iteration 131/1000 | Loss: 0.00001767
Iteration 132/1000 | Loss: 0.00001767
Iteration 133/1000 | Loss: 0.00001767
Iteration 134/1000 | Loss: 0.00001766
Iteration 135/1000 | Loss: 0.00001766
Iteration 136/1000 | Loss: 0.00001766
Iteration 137/1000 | Loss: 0.00001766
Iteration 138/1000 | Loss: 0.00001766
Iteration 139/1000 | Loss: 0.00001766
Iteration 140/1000 | Loss: 0.00001766
Iteration 141/1000 | Loss: 0.00001766
Iteration 142/1000 | Loss: 0.00001766
Iteration 143/1000 | Loss: 0.00001766
Iteration 144/1000 | Loss: 0.00001766
Iteration 145/1000 | Loss: 0.00001766
Iteration 146/1000 | Loss: 0.00001766
Iteration 147/1000 | Loss: 0.00001766
Iteration 148/1000 | Loss: 0.00001765
Iteration 149/1000 | Loss: 0.00001765
Iteration 150/1000 | Loss: 0.00001765
Iteration 151/1000 | Loss: 0.00001765
Iteration 152/1000 | Loss: 0.00001765
Iteration 153/1000 | Loss: 0.00001765
Iteration 154/1000 | Loss: 0.00001765
Iteration 155/1000 | Loss: 0.00001765
Iteration 156/1000 | Loss: 0.00001765
Iteration 157/1000 | Loss: 0.00001765
Iteration 158/1000 | Loss: 0.00001765
Iteration 159/1000 | Loss: 0.00001765
Iteration 160/1000 | Loss: 0.00001765
Iteration 161/1000 | Loss: 0.00001765
Iteration 162/1000 | Loss: 0.00001764
Iteration 163/1000 | Loss: 0.00001764
Iteration 164/1000 | Loss: 0.00001764
Iteration 165/1000 | Loss: 0.00001764
Iteration 166/1000 | Loss: 0.00001764
Iteration 167/1000 | Loss: 0.00001764
Iteration 168/1000 | Loss: 0.00001764
Iteration 169/1000 | Loss: 0.00001764
Iteration 170/1000 | Loss: 0.00001764
Iteration 171/1000 | Loss: 0.00001764
Iteration 172/1000 | Loss: 0.00001764
Iteration 173/1000 | Loss: 0.00001764
Iteration 174/1000 | Loss: 0.00001764
Iteration 175/1000 | Loss: 0.00001764
Iteration 176/1000 | Loss: 0.00001764
Iteration 177/1000 | Loss: 0.00001764
Iteration 178/1000 | Loss: 0.00001764
Iteration 179/1000 | Loss: 0.00001764
Iteration 180/1000 | Loss: 0.00001764
Iteration 181/1000 | Loss: 0.00001764
Iteration 182/1000 | Loss: 0.00001764
Iteration 183/1000 | Loss: 0.00001764
Iteration 184/1000 | Loss: 0.00001764
Iteration 185/1000 | Loss: 0.00001764
Iteration 186/1000 | Loss: 0.00001764
Iteration 187/1000 | Loss: 0.00001764
Iteration 188/1000 | Loss: 0.00001764
Iteration 189/1000 | Loss: 0.00001764
Iteration 190/1000 | Loss: 0.00001764
Iteration 191/1000 | Loss: 0.00001764
Iteration 192/1000 | Loss: 0.00001764
Iteration 193/1000 | Loss: 0.00001764
Iteration 194/1000 | Loss: 0.00001764
Iteration 195/1000 | Loss: 0.00001764
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [1.7640095393289812e-05, 1.7640095393289812e-05, 1.7640095393289812e-05, 1.7640095393289812e-05, 1.7640095393289812e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7640095393289812e-05

Optimization complete. Final v2v error: 3.503972291946411 mm

Highest mean error: 3.8075387477874756 mm for frame 172

Lowest mean error: 3.094993829727173 mm for frame 37

Saving results

Total time: 48.56322121620178
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00906601
Iteration 2/25 | Loss: 0.00182797
Iteration 3/25 | Loss: 0.00146253
Iteration 4/25 | Loss: 0.00139388
Iteration 5/25 | Loss: 0.00138109
Iteration 6/25 | Loss: 0.00130693
Iteration 7/25 | Loss: 0.00127831
Iteration 8/25 | Loss: 0.00125963
Iteration 9/25 | Loss: 0.00124743
Iteration 10/25 | Loss: 0.00124465
Iteration 11/25 | Loss: 0.00127173
Iteration 12/25 | Loss: 0.00124953
Iteration 13/25 | Loss: 0.00122826
Iteration 14/25 | Loss: 0.00123099
Iteration 15/25 | Loss: 0.00122444
Iteration 16/25 | Loss: 0.00122120
Iteration 17/25 | Loss: 0.00121994
Iteration 18/25 | Loss: 0.00121971
Iteration 19/25 | Loss: 0.00121959
Iteration 20/25 | Loss: 0.00121957
Iteration 21/25 | Loss: 0.00121957
Iteration 22/25 | Loss: 0.00121957
Iteration 23/25 | Loss: 0.00121957
Iteration 24/25 | Loss: 0.00121957
Iteration 25/25 | Loss: 0.00121957

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.33942842
Iteration 2/25 | Loss: 0.00093126
Iteration 3/25 | Loss: 0.00093124
Iteration 4/25 | Loss: 0.00093124
Iteration 5/25 | Loss: 0.00093124
Iteration 6/25 | Loss: 0.00093124
Iteration 7/25 | Loss: 0.00093124
Iteration 8/25 | Loss: 0.00093124
Iteration 9/25 | Loss: 0.00093124
Iteration 10/25 | Loss: 0.00093124
Iteration 11/25 | Loss: 0.00093124
Iteration 12/25 | Loss: 0.00093124
Iteration 13/25 | Loss: 0.00093124
Iteration 14/25 | Loss: 0.00093124
Iteration 15/25 | Loss: 0.00093124
Iteration 16/25 | Loss: 0.00093124
Iteration 17/25 | Loss: 0.00093124
Iteration 18/25 | Loss: 0.00093124
Iteration 19/25 | Loss: 0.00093124
Iteration 20/25 | Loss: 0.00093124
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009312380570918322, 0.0009312380570918322, 0.0009312380570918322, 0.0009312380570918322, 0.0009312380570918322]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009312380570918322

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093124
Iteration 2/1000 | Loss: 0.00027101
Iteration 3/1000 | Loss: 0.00039972
Iteration 4/1000 | Loss: 0.00021735
Iteration 5/1000 | Loss: 0.00020123
Iteration 6/1000 | Loss: 0.00013140
Iteration 7/1000 | Loss: 0.00009463
Iteration 8/1000 | Loss: 0.00008352
Iteration 9/1000 | Loss: 0.00007697
Iteration 10/1000 | Loss: 0.00019306
Iteration 11/1000 | Loss: 0.00012922
Iteration 12/1000 | Loss: 0.00018186
Iteration 13/1000 | Loss: 0.00013791
Iteration 14/1000 | Loss: 0.00012971
Iteration 15/1000 | Loss: 0.00013455
Iteration 16/1000 | Loss: 0.00009303
Iteration 17/1000 | Loss: 0.00008217
Iteration 18/1000 | Loss: 0.00007213
Iteration 19/1000 | Loss: 0.00009299
Iteration 20/1000 | Loss: 0.00006499
Iteration 21/1000 | Loss: 0.00005918
Iteration 22/1000 | Loss: 0.00005061
Iteration 23/1000 | Loss: 0.00008206
Iteration 24/1000 | Loss: 0.00004591
Iteration 25/1000 | Loss: 0.00004270
Iteration 26/1000 | Loss: 0.00003780
Iteration 27/1000 | Loss: 0.00003504
Iteration 28/1000 | Loss: 0.00003312
Iteration 29/1000 | Loss: 0.00003186
Iteration 30/1000 | Loss: 0.00003068
Iteration 31/1000 | Loss: 0.00002979
Iteration 32/1000 | Loss: 0.00002918
Iteration 33/1000 | Loss: 0.00002856
Iteration 34/1000 | Loss: 0.00002807
Iteration 35/1000 | Loss: 0.00002770
Iteration 36/1000 | Loss: 0.00006580
Iteration 37/1000 | Loss: 0.00009102
Iteration 38/1000 | Loss: 0.00005480
Iteration 39/1000 | Loss: 0.00004107
Iteration 40/1000 | Loss: 0.00003762
Iteration 41/1000 | Loss: 0.00005665
Iteration 42/1000 | Loss: 0.00005879
Iteration 43/1000 | Loss: 0.00003862
Iteration 44/1000 | Loss: 0.00003641
Iteration 45/1000 | Loss: 0.00008812
Iteration 46/1000 | Loss: 0.00010259
Iteration 47/1000 | Loss: 0.00007846
Iteration 48/1000 | Loss: 0.00003889
Iteration 49/1000 | Loss: 0.00003644
Iteration 50/1000 | Loss: 0.00003121
Iteration 51/1000 | Loss: 0.00002959
Iteration 52/1000 | Loss: 0.00002888
Iteration 53/1000 | Loss: 0.00003581
Iteration 54/1000 | Loss: 0.00003609
Iteration 55/1000 | Loss: 0.00003481
Iteration 56/1000 | Loss: 0.00003683
Iteration 57/1000 | Loss: 0.00003725
Iteration 58/1000 | Loss: 0.00003424
Iteration 59/1000 | Loss: 0.00003351
Iteration 60/1000 | Loss: 0.00005965
Iteration 61/1000 | Loss: 0.00005204
Iteration 62/1000 | Loss: 0.00003124
Iteration 63/1000 | Loss: 0.00003035
Iteration 64/1000 | Loss: 0.00002977
Iteration 65/1000 | Loss: 0.00002895
Iteration 66/1000 | Loss: 0.00002848
Iteration 67/1000 | Loss: 0.00002836
Iteration 68/1000 | Loss: 0.00002798
Iteration 69/1000 | Loss: 0.00002645
Iteration 70/1000 | Loss: 0.00006511
Iteration 71/1000 | Loss: 0.00006921
Iteration 72/1000 | Loss: 0.00004229
Iteration 73/1000 | Loss: 0.00003591
Iteration 74/1000 | Loss: 0.00005416
Iteration 75/1000 | Loss: 0.00003328
Iteration 76/1000 | Loss: 0.00004741
Iteration 77/1000 | Loss: 0.00003045
Iteration 78/1000 | Loss: 0.00003654
Iteration 79/1000 | Loss: 0.00004801
Iteration 80/1000 | Loss: 0.00003567
Iteration 81/1000 | Loss: 0.00003630
Iteration 82/1000 | Loss: 0.00003560
Iteration 83/1000 | Loss: 0.00003495
Iteration 84/1000 | Loss: 0.00003679
Iteration 85/1000 | Loss: 0.00003748
Iteration 86/1000 | Loss: 0.00005132
Iteration 87/1000 | Loss: 0.00005069
Iteration 88/1000 | Loss: 0.00005373
Iteration 89/1000 | Loss: 0.00005347
Iteration 90/1000 | Loss: 0.00005026
Iteration 91/1000 | Loss: 0.00004316
Iteration 92/1000 | Loss: 0.00003981
Iteration 93/1000 | Loss: 0.00005319
Iteration 94/1000 | Loss: 0.00005373
Iteration 95/1000 | Loss: 0.00003896
Iteration 96/1000 | Loss: 0.00003967
Iteration 97/1000 | Loss: 0.00003671
Iteration 98/1000 | Loss: 0.00003493
Iteration 99/1000 | Loss: 0.00003622
Iteration 100/1000 | Loss: 0.00003013
Iteration 101/1000 | Loss: 0.00003208
Iteration 102/1000 | Loss: 0.00003529
Iteration 103/1000 | Loss: 0.00002895
Iteration 104/1000 | Loss: 0.00002571
Iteration 105/1000 | Loss: 0.00005447
Iteration 106/1000 | Loss: 0.00004738
Iteration 107/1000 | Loss: 0.00005306
Iteration 108/1000 | Loss: 0.00005661
Iteration 109/1000 | Loss: 0.00003463
Iteration 110/1000 | Loss: 0.00003421
Iteration 111/1000 | Loss: 0.00002836
Iteration 112/1000 | Loss: 0.00002517
Iteration 113/1000 | Loss: 0.00002940
Iteration 114/1000 | Loss: 0.00002617
Iteration 115/1000 | Loss: 0.00002515
Iteration 116/1000 | Loss: 0.00002589
Iteration 117/1000 | Loss: 0.00003386
Iteration 118/1000 | Loss: 0.00002645
Iteration 119/1000 | Loss: 0.00003004
Iteration 120/1000 | Loss: 0.00002978
Iteration 121/1000 | Loss: 0.00003339
Iteration 122/1000 | Loss: 0.00002940
Iteration 123/1000 | Loss: 0.00005769
Iteration 124/1000 | Loss: 0.00005626
Iteration 125/1000 | Loss: 0.00005823
Iteration 126/1000 | Loss: 0.00006406
Iteration 127/1000 | Loss: 0.00003560
Iteration 128/1000 | Loss: 0.00005821
Iteration 129/1000 | Loss: 0.00005460
Iteration 130/1000 | Loss: 0.00005601
Iteration 131/1000 | Loss: 0.00005378
Iteration 132/1000 | Loss: 0.00003176
Iteration 133/1000 | Loss: 0.00006139
Iteration 134/1000 | Loss: 0.00003444
Iteration 135/1000 | Loss: 0.00002968
Iteration 136/1000 | Loss: 0.00005510
Iteration 137/1000 | Loss: 0.00003042
Iteration 138/1000 | Loss: 0.00002785
Iteration 139/1000 | Loss: 0.00002599
Iteration 140/1000 | Loss: 0.00002535
Iteration 141/1000 | Loss: 0.00002468
Iteration 142/1000 | Loss: 0.00002428
Iteration 143/1000 | Loss: 0.00002403
Iteration 144/1000 | Loss: 0.00002386
Iteration 145/1000 | Loss: 0.00002364
Iteration 146/1000 | Loss: 0.00002360
Iteration 147/1000 | Loss: 0.00002359
Iteration 148/1000 | Loss: 0.00002359
Iteration 149/1000 | Loss: 0.00002359
Iteration 150/1000 | Loss: 0.00002358
Iteration 151/1000 | Loss: 0.00002358
Iteration 152/1000 | Loss: 0.00002355
Iteration 153/1000 | Loss: 0.00002349
Iteration 154/1000 | Loss: 0.00002345
Iteration 155/1000 | Loss: 0.00002344
Iteration 156/1000 | Loss: 0.00002343
Iteration 157/1000 | Loss: 0.00002343
Iteration 158/1000 | Loss: 0.00002343
Iteration 159/1000 | Loss: 0.00002343
Iteration 160/1000 | Loss: 0.00002343
Iteration 161/1000 | Loss: 0.00002343
Iteration 162/1000 | Loss: 0.00002343
Iteration 163/1000 | Loss: 0.00002343
Iteration 164/1000 | Loss: 0.00002342
Iteration 165/1000 | Loss: 0.00002342
Iteration 166/1000 | Loss: 0.00002342
Iteration 167/1000 | Loss: 0.00002342
Iteration 168/1000 | Loss: 0.00002342
Iteration 169/1000 | Loss: 0.00002342
Iteration 170/1000 | Loss: 0.00002342
Iteration 171/1000 | Loss: 0.00002342
Iteration 172/1000 | Loss: 0.00002342
Iteration 173/1000 | Loss: 0.00002341
Iteration 174/1000 | Loss: 0.00002341
Iteration 175/1000 | Loss: 0.00002341
Iteration 176/1000 | Loss: 0.00002341
Iteration 177/1000 | Loss: 0.00002341
Iteration 178/1000 | Loss: 0.00002341
Iteration 179/1000 | Loss: 0.00002341
Iteration 180/1000 | Loss: 0.00002341
Iteration 181/1000 | Loss: 0.00002340
Iteration 182/1000 | Loss: 0.00002340
Iteration 183/1000 | Loss: 0.00002340
Iteration 184/1000 | Loss: 0.00002340
Iteration 185/1000 | Loss: 0.00002340
Iteration 186/1000 | Loss: 0.00002340
Iteration 187/1000 | Loss: 0.00002339
Iteration 188/1000 | Loss: 0.00002339
Iteration 189/1000 | Loss: 0.00002339
Iteration 190/1000 | Loss: 0.00002339
Iteration 191/1000 | Loss: 0.00002339
Iteration 192/1000 | Loss: 0.00002339
Iteration 193/1000 | Loss: 0.00002338
Iteration 194/1000 | Loss: 0.00002338
Iteration 195/1000 | Loss: 0.00002338
Iteration 196/1000 | Loss: 0.00002337
Iteration 197/1000 | Loss: 0.00002337
Iteration 198/1000 | Loss: 0.00002337
Iteration 199/1000 | Loss: 0.00002337
Iteration 200/1000 | Loss: 0.00002336
Iteration 201/1000 | Loss: 0.00002336
Iteration 202/1000 | Loss: 0.00002336
Iteration 203/1000 | Loss: 0.00002336
Iteration 204/1000 | Loss: 0.00002336
Iteration 205/1000 | Loss: 0.00002336
Iteration 206/1000 | Loss: 0.00002335
Iteration 207/1000 | Loss: 0.00002335
Iteration 208/1000 | Loss: 0.00002335
Iteration 209/1000 | Loss: 0.00002335
Iteration 210/1000 | Loss: 0.00002335
Iteration 211/1000 | Loss: 0.00002335
Iteration 212/1000 | Loss: 0.00002334
Iteration 213/1000 | Loss: 0.00002334
Iteration 214/1000 | Loss: 0.00002333
Iteration 215/1000 | Loss: 0.00002333
Iteration 216/1000 | Loss: 0.00002332
Iteration 217/1000 | Loss: 0.00002332
Iteration 218/1000 | Loss: 0.00002331
Iteration 219/1000 | Loss: 0.00002331
Iteration 220/1000 | Loss: 0.00002331
Iteration 221/1000 | Loss: 0.00002331
Iteration 222/1000 | Loss: 0.00002331
Iteration 223/1000 | Loss: 0.00002331
Iteration 224/1000 | Loss: 0.00002330
Iteration 225/1000 | Loss: 0.00002330
Iteration 226/1000 | Loss: 0.00002330
Iteration 227/1000 | Loss: 0.00002330
Iteration 228/1000 | Loss: 0.00002330
Iteration 229/1000 | Loss: 0.00002329
Iteration 230/1000 | Loss: 0.00002329
Iteration 231/1000 | Loss: 0.00002329
Iteration 232/1000 | Loss: 0.00002329
Iteration 233/1000 | Loss: 0.00002328
Iteration 234/1000 | Loss: 0.00002328
Iteration 235/1000 | Loss: 0.00002328
Iteration 236/1000 | Loss: 0.00002327
Iteration 237/1000 | Loss: 0.00002327
Iteration 238/1000 | Loss: 0.00002327
Iteration 239/1000 | Loss: 0.00002326
Iteration 240/1000 | Loss: 0.00002326
Iteration 241/1000 | Loss: 0.00002325
Iteration 242/1000 | Loss: 0.00002324
Iteration 243/1000 | Loss: 0.00002324
Iteration 244/1000 | Loss: 0.00002323
Iteration 245/1000 | Loss: 0.00002323
Iteration 246/1000 | Loss: 0.00002322
Iteration 247/1000 | Loss: 0.00002322
Iteration 248/1000 | Loss: 0.00002322
Iteration 249/1000 | Loss: 0.00002321
Iteration 250/1000 | Loss: 0.00002321
Iteration 251/1000 | Loss: 0.00002320
Iteration 252/1000 | Loss: 0.00002320
Iteration 253/1000 | Loss: 0.00002320
Iteration 254/1000 | Loss: 0.00002319
Iteration 255/1000 | Loss: 0.00002319
Iteration 256/1000 | Loss: 0.00002319
Iteration 257/1000 | Loss: 0.00002319
Iteration 258/1000 | Loss: 0.00002319
Iteration 259/1000 | Loss: 0.00002318
Iteration 260/1000 | Loss: 0.00002318
Iteration 261/1000 | Loss: 0.00002318
Iteration 262/1000 | Loss: 0.00002318
Iteration 263/1000 | Loss: 0.00002318
Iteration 264/1000 | Loss: 0.00002318
Iteration 265/1000 | Loss: 0.00002318
Iteration 266/1000 | Loss: 0.00002318
Iteration 267/1000 | Loss: 0.00002318
Iteration 268/1000 | Loss: 0.00002318
Iteration 269/1000 | Loss: 0.00002317
Iteration 270/1000 | Loss: 0.00002317
Iteration 271/1000 | Loss: 0.00002317
Iteration 272/1000 | Loss: 0.00002317
Iteration 273/1000 | Loss: 0.00002316
Iteration 274/1000 | Loss: 0.00002316
Iteration 275/1000 | Loss: 0.00002315
Iteration 276/1000 | Loss: 0.00002315
Iteration 277/1000 | Loss: 0.00002314
Iteration 278/1000 | Loss: 0.00002314
Iteration 279/1000 | Loss: 0.00002314
Iteration 280/1000 | Loss: 0.00002313
Iteration 281/1000 | Loss: 0.00002313
Iteration 282/1000 | Loss: 0.00002313
Iteration 283/1000 | Loss: 0.00002313
Iteration 284/1000 | Loss: 0.00002313
Iteration 285/1000 | Loss: 0.00002312
Iteration 286/1000 | Loss: 0.00002312
Iteration 287/1000 | Loss: 0.00002312
Iteration 288/1000 | Loss: 0.00002312
Iteration 289/1000 | Loss: 0.00002312
Iteration 290/1000 | Loss: 0.00002312
Iteration 291/1000 | Loss: 0.00002311
Iteration 292/1000 | Loss: 0.00002311
Iteration 293/1000 | Loss: 0.00002311
Iteration 294/1000 | Loss: 0.00002311
Iteration 295/1000 | Loss: 0.00002311
Iteration 296/1000 | Loss: 0.00002311
Iteration 297/1000 | Loss: 0.00002311
Iteration 298/1000 | Loss: 0.00002311
Iteration 299/1000 | Loss: 0.00002311
Iteration 300/1000 | Loss: 0.00002311
Iteration 301/1000 | Loss: 0.00002311
Iteration 302/1000 | Loss: 0.00002311
Iteration 303/1000 | Loss: 0.00002310
Iteration 304/1000 | Loss: 0.00002310
Iteration 305/1000 | Loss: 0.00002310
Iteration 306/1000 | Loss: 0.00002310
Iteration 307/1000 | Loss: 0.00002310
Iteration 308/1000 | Loss: 0.00002310
Iteration 309/1000 | Loss: 0.00002310
Iteration 310/1000 | Loss: 0.00002310
Iteration 311/1000 | Loss: 0.00002310
Iteration 312/1000 | Loss: 0.00002310
Iteration 313/1000 | Loss: 0.00002310
Iteration 314/1000 | Loss: 0.00002310
Iteration 315/1000 | Loss: 0.00002310
Iteration 316/1000 | Loss: 0.00002310
Iteration 317/1000 | Loss: 0.00002310
Iteration 318/1000 | Loss: 0.00002310
Iteration 319/1000 | Loss: 0.00002310
Iteration 320/1000 | Loss: 0.00002309
Iteration 321/1000 | Loss: 0.00002309
Iteration 322/1000 | Loss: 0.00002309
Iteration 323/1000 | Loss: 0.00002309
Iteration 324/1000 | Loss: 0.00002309
Iteration 325/1000 | Loss: 0.00002309
Iteration 326/1000 | Loss: 0.00002309
Iteration 327/1000 | Loss: 0.00002309
Iteration 328/1000 | Loss: 0.00002309
Iteration 329/1000 | Loss: 0.00002309
Iteration 330/1000 | Loss: 0.00002308
Iteration 331/1000 | Loss: 0.00002308
Iteration 332/1000 | Loss: 0.00002308
Iteration 333/1000 | Loss: 0.00002308
Iteration 334/1000 | Loss: 0.00002308
Iteration 335/1000 | Loss: 0.00002308
Iteration 336/1000 | Loss: 0.00002308
Iteration 337/1000 | Loss: 0.00002308
Iteration 338/1000 | Loss: 0.00002308
Iteration 339/1000 | Loss: 0.00002308
Iteration 340/1000 | Loss: 0.00002308
Iteration 341/1000 | Loss: 0.00002308
Iteration 342/1000 | Loss: 0.00002308
Iteration 343/1000 | Loss: 0.00002308
Iteration 344/1000 | Loss: 0.00002308
Iteration 345/1000 | Loss: 0.00002308
Iteration 346/1000 | Loss: 0.00002308
Iteration 347/1000 | Loss: 0.00002308
Iteration 348/1000 | Loss: 0.00002308
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 348. Stopping optimization.
Last 5 losses: [2.3079373931977898e-05, 2.3079373931977898e-05, 2.3079373931977898e-05, 2.3079373931977898e-05, 2.3079373931977898e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3079373931977898e-05

Optimization complete. Final v2v error: 3.842285394668579 mm

Highest mean error: 7.8786797523498535 mm for frame 113

Lowest mean error: 2.8655059337615967 mm for frame 74

Saving results

Total time: 270.0098934173584
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00938160
Iteration 2/25 | Loss: 0.00166268
Iteration 3/25 | Loss: 0.00130842
Iteration 4/25 | Loss: 0.00128665
Iteration 5/25 | Loss: 0.00127964
Iteration 6/25 | Loss: 0.00127837
Iteration 7/25 | Loss: 0.00127837
Iteration 8/25 | Loss: 0.00127837
Iteration 9/25 | Loss: 0.00127837
Iteration 10/25 | Loss: 0.00127837
Iteration 11/25 | Loss: 0.00127837
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012783727142959833, 0.0012783727142959833, 0.0012783727142959833, 0.0012783727142959833, 0.0012783727142959833]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012783727142959833

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.86487669
Iteration 2/25 | Loss: 0.00107713
Iteration 3/25 | Loss: 0.00107713
Iteration 4/25 | Loss: 0.00107713
Iteration 5/25 | Loss: 0.00107713
Iteration 6/25 | Loss: 0.00107713
Iteration 7/25 | Loss: 0.00107713
Iteration 8/25 | Loss: 0.00107713
Iteration 9/25 | Loss: 0.00107713
Iteration 10/25 | Loss: 0.00107713
Iteration 11/25 | Loss: 0.00107713
Iteration 12/25 | Loss: 0.00107713
Iteration 13/25 | Loss: 0.00107713
Iteration 14/25 | Loss: 0.00107713
Iteration 15/25 | Loss: 0.00107713
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010771270608529449, 0.0010771270608529449, 0.0010771270608529449, 0.0010771270608529449, 0.0010771270608529449]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010771270608529449

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107713
Iteration 2/1000 | Loss: 0.00006137
Iteration 3/1000 | Loss: 0.00004238
Iteration 4/1000 | Loss: 0.00003483
Iteration 5/1000 | Loss: 0.00003287
Iteration 6/1000 | Loss: 0.00003202
Iteration 7/1000 | Loss: 0.00003128
Iteration 8/1000 | Loss: 0.00003066
Iteration 9/1000 | Loss: 0.00003018
Iteration 10/1000 | Loss: 0.00002986
Iteration 11/1000 | Loss: 0.00002960
Iteration 12/1000 | Loss: 0.00002931
Iteration 13/1000 | Loss: 0.00002900
Iteration 14/1000 | Loss: 0.00002880
Iteration 15/1000 | Loss: 0.00002858
Iteration 16/1000 | Loss: 0.00002833
Iteration 17/1000 | Loss: 0.00002814
Iteration 18/1000 | Loss: 0.00002805
Iteration 19/1000 | Loss: 0.00002796
Iteration 20/1000 | Loss: 0.00002788
Iteration 21/1000 | Loss: 0.00002784
Iteration 22/1000 | Loss: 0.00002778
Iteration 23/1000 | Loss: 0.00002778
Iteration 24/1000 | Loss: 0.00002775
Iteration 25/1000 | Loss: 0.00002774
Iteration 26/1000 | Loss: 0.00002774
Iteration 27/1000 | Loss: 0.00002773
Iteration 28/1000 | Loss: 0.00002773
Iteration 29/1000 | Loss: 0.00002770
Iteration 30/1000 | Loss: 0.00002770
Iteration 31/1000 | Loss: 0.00002769
Iteration 32/1000 | Loss: 0.00002767
Iteration 33/1000 | Loss: 0.00002767
Iteration 34/1000 | Loss: 0.00002767
Iteration 35/1000 | Loss: 0.00002767
Iteration 36/1000 | Loss: 0.00002766
Iteration 37/1000 | Loss: 0.00002766
Iteration 38/1000 | Loss: 0.00002766
Iteration 39/1000 | Loss: 0.00002766
Iteration 40/1000 | Loss: 0.00002766
Iteration 41/1000 | Loss: 0.00002766
Iteration 42/1000 | Loss: 0.00002766
Iteration 43/1000 | Loss: 0.00002766
Iteration 44/1000 | Loss: 0.00002766
Iteration 45/1000 | Loss: 0.00002765
Iteration 46/1000 | Loss: 0.00002764
Iteration 47/1000 | Loss: 0.00002764
Iteration 48/1000 | Loss: 0.00002763
Iteration 49/1000 | Loss: 0.00002763
Iteration 50/1000 | Loss: 0.00002763
Iteration 51/1000 | Loss: 0.00002763
Iteration 52/1000 | Loss: 0.00002762
Iteration 53/1000 | Loss: 0.00002762
Iteration 54/1000 | Loss: 0.00002761
Iteration 55/1000 | Loss: 0.00002761
Iteration 56/1000 | Loss: 0.00002761
Iteration 57/1000 | Loss: 0.00002761
Iteration 58/1000 | Loss: 0.00002761
Iteration 59/1000 | Loss: 0.00002761
Iteration 60/1000 | Loss: 0.00002761
Iteration 61/1000 | Loss: 0.00002761
Iteration 62/1000 | Loss: 0.00002761
Iteration 63/1000 | Loss: 0.00002760
Iteration 64/1000 | Loss: 0.00002760
Iteration 65/1000 | Loss: 0.00002760
Iteration 66/1000 | Loss: 0.00002760
Iteration 67/1000 | Loss: 0.00002759
Iteration 68/1000 | Loss: 0.00002759
Iteration 69/1000 | Loss: 0.00002759
Iteration 70/1000 | Loss: 0.00002759
Iteration 71/1000 | Loss: 0.00002758
Iteration 72/1000 | Loss: 0.00002758
Iteration 73/1000 | Loss: 0.00002758
Iteration 74/1000 | Loss: 0.00002758
Iteration 75/1000 | Loss: 0.00002758
Iteration 76/1000 | Loss: 0.00002758
Iteration 77/1000 | Loss: 0.00002758
Iteration 78/1000 | Loss: 0.00002757
Iteration 79/1000 | Loss: 0.00002757
Iteration 80/1000 | Loss: 0.00002757
Iteration 81/1000 | Loss: 0.00002756
Iteration 82/1000 | Loss: 0.00002756
Iteration 83/1000 | Loss: 0.00002756
Iteration 84/1000 | Loss: 0.00002756
Iteration 85/1000 | Loss: 0.00002755
Iteration 86/1000 | Loss: 0.00002755
Iteration 87/1000 | Loss: 0.00002755
Iteration 88/1000 | Loss: 0.00002755
Iteration 89/1000 | Loss: 0.00002755
Iteration 90/1000 | Loss: 0.00002755
Iteration 91/1000 | Loss: 0.00002755
Iteration 92/1000 | Loss: 0.00002755
Iteration 93/1000 | Loss: 0.00002755
Iteration 94/1000 | Loss: 0.00002755
Iteration 95/1000 | Loss: 0.00002755
Iteration 96/1000 | Loss: 0.00002755
Iteration 97/1000 | Loss: 0.00002755
Iteration 98/1000 | Loss: 0.00002755
Iteration 99/1000 | Loss: 0.00002755
Iteration 100/1000 | Loss: 0.00002755
Iteration 101/1000 | Loss: 0.00002755
Iteration 102/1000 | Loss: 0.00002755
Iteration 103/1000 | Loss: 0.00002755
Iteration 104/1000 | Loss: 0.00002755
Iteration 105/1000 | Loss: 0.00002755
Iteration 106/1000 | Loss: 0.00002755
Iteration 107/1000 | Loss: 0.00002755
Iteration 108/1000 | Loss: 0.00002755
Iteration 109/1000 | Loss: 0.00002755
Iteration 110/1000 | Loss: 0.00002755
Iteration 111/1000 | Loss: 0.00002755
Iteration 112/1000 | Loss: 0.00002755
Iteration 113/1000 | Loss: 0.00002755
Iteration 114/1000 | Loss: 0.00002755
Iteration 115/1000 | Loss: 0.00002755
Iteration 116/1000 | Loss: 0.00002755
Iteration 117/1000 | Loss: 0.00002755
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [2.7550295271794312e-05, 2.7550295271794312e-05, 2.7550295271794312e-05, 2.7550295271794312e-05, 2.7550295271794312e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7550295271794312e-05

Optimization complete. Final v2v error: 4.3254780769348145 mm

Highest mean error: 5.570496559143066 mm for frame 118

Lowest mean error: 3.38187313079834 mm for frame 0

Saving results

Total time: 48.7750518321991
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00511630
Iteration 2/25 | Loss: 0.00132250
Iteration 3/25 | Loss: 0.00117929
Iteration 4/25 | Loss: 0.00115824
Iteration 5/25 | Loss: 0.00114903
Iteration 6/25 | Loss: 0.00114707
Iteration 7/25 | Loss: 0.00114707
Iteration 8/25 | Loss: 0.00114707
Iteration 9/25 | Loss: 0.00114707
Iteration 10/25 | Loss: 0.00114707
Iteration 11/25 | Loss: 0.00114707
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011470678728073835, 0.0011470678728073835, 0.0011470678728073835, 0.0011470678728073835, 0.0011470678728073835]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011470678728073835

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.76270509
Iteration 2/25 | Loss: 0.00075174
Iteration 3/25 | Loss: 0.00075174
Iteration 4/25 | Loss: 0.00075174
Iteration 5/25 | Loss: 0.00075174
Iteration 6/25 | Loss: 0.00075174
Iteration 7/25 | Loss: 0.00075173
Iteration 8/25 | Loss: 0.00075173
Iteration 9/25 | Loss: 0.00075173
Iteration 10/25 | Loss: 0.00075173
Iteration 11/25 | Loss: 0.00075173
Iteration 12/25 | Loss: 0.00075173
Iteration 13/25 | Loss: 0.00075173
Iteration 14/25 | Loss: 0.00075173
Iteration 15/25 | Loss: 0.00075173
Iteration 16/25 | Loss: 0.00075173
Iteration 17/25 | Loss: 0.00075173
Iteration 18/25 | Loss: 0.00075173
Iteration 19/25 | Loss: 0.00075173
Iteration 20/25 | Loss: 0.00075173
Iteration 21/25 | Loss: 0.00075173
Iteration 22/25 | Loss: 0.00075173
Iteration 23/25 | Loss: 0.00075173
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007517336052842438, 0.0007517336052842438, 0.0007517336052842438, 0.0007517336052842438, 0.0007517336052842438]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007517336052842438

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075173
Iteration 2/1000 | Loss: 0.00003483
Iteration 3/1000 | Loss: 0.00002710
Iteration 4/1000 | Loss: 0.00002515
Iteration 5/1000 | Loss: 0.00002411
Iteration 6/1000 | Loss: 0.00002334
Iteration 7/1000 | Loss: 0.00002285
Iteration 8/1000 | Loss: 0.00002244
Iteration 9/1000 | Loss: 0.00002204
Iteration 10/1000 | Loss: 0.00002174
Iteration 11/1000 | Loss: 0.00002155
Iteration 12/1000 | Loss: 0.00002131
Iteration 13/1000 | Loss: 0.00002109
Iteration 14/1000 | Loss: 0.00002090
Iteration 15/1000 | Loss: 0.00002082
Iteration 16/1000 | Loss: 0.00002074
Iteration 17/1000 | Loss: 0.00002070
Iteration 18/1000 | Loss: 0.00002064
Iteration 19/1000 | Loss: 0.00002064
Iteration 20/1000 | Loss: 0.00002063
Iteration 21/1000 | Loss: 0.00002063
Iteration 22/1000 | Loss: 0.00002057
Iteration 23/1000 | Loss: 0.00002049
Iteration 24/1000 | Loss: 0.00002046
Iteration 25/1000 | Loss: 0.00002045
Iteration 26/1000 | Loss: 0.00002044
Iteration 27/1000 | Loss: 0.00002043
Iteration 28/1000 | Loss: 0.00002042
Iteration 29/1000 | Loss: 0.00002042
Iteration 30/1000 | Loss: 0.00002042
Iteration 31/1000 | Loss: 0.00002042
Iteration 32/1000 | Loss: 0.00002041
Iteration 33/1000 | Loss: 0.00002041
Iteration 34/1000 | Loss: 0.00002040
Iteration 35/1000 | Loss: 0.00002039
Iteration 36/1000 | Loss: 0.00002033
Iteration 37/1000 | Loss: 0.00002028
Iteration 38/1000 | Loss: 0.00002024
Iteration 39/1000 | Loss: 0.00002024
Iteration 40/1000 | Loss: 0.00002023
Iteration 41/1000 | Loss: 0.00002022
Iteration 42/1000 | Loss: 0.00002022
Iteration 43/1000 | Loss: 0.00002022
Iteration 44/1000 | Loss: 0.00002022
Iteration 45/1000 | Loss: 0.00002022
Iteration 46/1000 | Loss: 0.00002022
Iteration 47/1000 | Loss: 0.00002022
Iteration 48/1000 | Loss: 0.00002022
Iteration 49/1000 | Loss: 0.00002021
Iteration 50/1000 | Loss: 0.00002021
Iteration 51/1000 | Loss: 0.00002021
Iteration 52/1000 | Loss: 0.00002021
Iteration 53/1000 | Loss: 0.00002021
Iteration 54/1000 | Loss: 0.00002020
Iteration 55/1000 | Loss: 0.00002020
Iteration 56/1000 | Loss: 0.00002020
Iteration 57/1000 | Loss: 0.00002019
Iteration 58/1000 | Loss: 0.00002019
Iteration 59/1000 | Loss: 0.00002019
Iteration 60/1000 | Loss: 0.00002019
Iteration 61/1000 | Loss: 0.00002019
Iteration 62/1000 | Loss: 0.00002019
Iteration 63/1000 | Loss: 0.00002019
Iteration 64/1000 | Loss: 0.00002018
Iteration 65/1000 | Loss: 0.00002018
Iteration 66/1000 | Loss: 0.00002018
Iteration 67/1000 | Loss: 0.00002018
Iteration 68/1000 | Loss: 0.00002018
Iteration 69/1000 | Loss: 0.00002018
Iteration 70/1000 | Loss: 0.00002016
Iteration 71/1000 | Loss: 0.00002016
Iteration 72/1000 | Loss: 0.00002015
Iteration 73/1000 | Loss: 0.00002015
Iteration 74/1000 | Loss: 0.00002015
Iteration 75/1000 | Loss: 0.00002015
Iteration 76/1000 | Loss: 0.00002015
Iteration 77/1000 | Loss: 0.00002014
Iteration 78/1000 | Loss: 0.00002014
Iteration 79/1000 | Loss: 0.00002013
Iteration 80/1000 | Loss: 0.00002013
Iteration 81/1000 | Loss: 0.00002013
Iteration 82/1000 | Loss: 0.00002013
Iteration 83/1000 | Loss: 0.00002013
Iteration 84/1000 | Loss: 0.00002013
Iteration 85/1000 | Loss: 0.00002013
Iteration 86/1000 | Loss: 0.00002013
Iteration 87/1000 | Loss: 0.00002011
Iteration 88/1000 | Loss: 0.00002011
Iteration 89/1000 | Loss: 0.00002011
Iteration 90/1000 | Loss: 0.00002011
Iteration 91/1000 | Loss: 0.00002011
Iteration 92/1000 | Loss: 0.00002011
Iteration 93/1000 | Loss: 0.00002011
Iteration 94/1000 | Loss: 0.00002011
Iteration 95/1000 | Loss: 0.00002011
Iteration 96/1000 | Loss: 0.00002011
Iteration 97/1000 | Loss: 0.00002010
Iteration 98/1000 | Loss: 0.00002010
Iteration 99/1000 | Loss: 0.00002010
Iteration 100/1000 | Loss: 0.00002010
Iteration 101/1000 | Loss: 0.00002010
Iteration 102/1000 | Loss: 0.00002008
Iteration 103/1000 | Loss: 0.00002008
Iteration 104/1000 | Loss: 0.00002008
Iteration 105/1000 | Loss: 0.00002008
Iteration 106/1000 | Loss: 0.00002008
Iteration 107/1000 | Loss: 0.00002007
Iteration 108/1000 | Loss: 0.00002007
Iteration 109/1000 | Loss: 0.00002007
Iteration 110/1000 | Loss: 0.00002006
Iteration 111/1000 | Loss: 0.00002006
Iteration 112/1000 | Loss: 0.00002006
Iteration 113/1000 | Loss: 0.00002006
Iteration 114/1000 | Loss: 0.00002005
Iteration 115/1000 | Loss: 0.00002005
Iteration 116/1000 | Loss: 0.00002005
Iteration 117/1000 | Loss: 0.00002004
Iteration 118/1000 | Loss: 0.00002003
Iteration 119/1000 | Loss: 0.00002002
Iteration 120/1000 | Loss: 0.00002002
Iteration 121/1000 | Loss: 0.00002002
Iteration 122/1000 | Loss: 0.00002001
Iteration 123/1000 | Loss: 0.00002001
Iteration 124/1000 | Loss: 0.00002001
Iteration 125/1000 | Loss: 0.00002001
Iteration 126/1000 | Loss: 0.00002000
Iteration 127/1000 | Loss: 0.00002000
Iteration 128/1000 | Loss: 0.00002000
Iteration 129/1000 | Loss: 0.00002000
Iteration 130/1000 | Loss: 0.00002000
Iteration 131/1000 | Loss: 0.00002000
Iteration 132/1000 | Loss: 0.00002000
Iteration 133/1000 | Loss: 0.00002000
Iteration 134/1000 | Loss: 0.00002000
Iteration 135/1000 | Loss: 0.00002000
Iteration 136/1000 | Loss: 0.00002000
Iteration 137/1000 | Loss: 0.00002000
Iteration 138/1000 | Loss: 0.00002000
Iteration 139/1000 | Loss: 0.00002000
Iteration 140/1000 | Loss: 0.00002000
Iteration 141/1000 | Loss: 0.00002000
Iteration 142/1000 | Loss: 0.00002000
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [1.9999881260446273e-05, 1.9999881260446273e-05, 1.9999881260446273e-05, 1.9999881260446273e-05, 1.9999881260446273e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9999881260446273e-05

Optimization complete. Final v2v error: 3.801658868789673 mm

Highest mean error: 4.137278079986572 mm for frame 4

Lowest mean error: 3.621973991394043 mm for frame 50

Saving results

Total time: 52.81013751029968
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00422561
Iteration 2/25 | Loss: 0.00124615
Iteration 3/25 | Loss: 0.00114990
Iteration 4/25 | Loss: 0.00112854
Iteration 5/25 | Loss: 0.00112185
Iteration 6/25 | Loss: 0.00112031
Iteration 7/25 | Loss: 0.00112006
Iteration 8/25 | Loss: 0.00112006
Iteration 9/25 | Loss: 0.00112006
Iteration 10/25 | Loss: 0.00112006
Iteration 11/25 | Loss: 0.00112006
Iteration 12/25 | Loss: 0.00112006
Iteration 13/25 | Loss: 0.00112006
Iteration 14/25 | Loss: 0.00112006
Iteration 15/25 | Loss: 0.00112006
Iteration 16/25 | Loss: 0.00112006
Iteration 17/25 | Loss: 0.00112006
Iteration 18/25 | Loss: 0.00112006
Iteration 19/25 | Loss: 0.00112006
Iteration 20/25 | Loss: 0.00112006
Iteration 21/25 | Loss: 0.00112006
Iteration 22/25 | Loss: 0.00112006
Iteration 23/25 | Loss: 0.00112006
Iteration 24/25 | Loss: 0.00112006
Iteration 25/25 | Loss: 0.00112006

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36437225
Iteration 2/25 | Loss: 0.00087015
Iteration 3/25 | Loss: 0.00087015
Iteration 4/25 | Loss: 0.00087015
Iteration 5/25 | Loss: 0.00087015
Iteration 6/25 | Loss: 0.00087015
Iteration 7/25 | Loss: 0.00087014
Iteration 8/25 | Loss: 0.00087014
Iteration 9/25 | Loss: 0.00087014
Iteration 10/25 | Loss: 0.00087014
Iteration 11/25 | Loss: 0.00087014
Iteration 12/25 | Loss: 0.00087014
Iteration 13/25 | Loss: 0.00087014
Iteration 14/25 | Loss: 0.00087014
Iteration 15/25 | Loss: 0.00087014
Iteration 16/25 | Loss: 0.00087014
Iteration 17/25 | Loss: 0.00087014
Iteration 18/25 | Loss: 0.00087014
Iteration 19/25 | Loss: 0.00087014
Iteration 20/25 | Loss: 0.00087014
Iteration 21/25 | Loss: 0.00087014
Iteration 22/25 | Loss: 0.00087014
Iteration 23/25 | Loss: 0.00087014
Iteration 24/25 | Loss: 0.00087014
Iteration 25/25 | Loss: 0.00087014

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087014
Iteration 2/1000 | Loss: 0.00002714
Iteration 3/1000 | Loss: 0.00001898
Iteration 4/1000 | Loss: 0.00001734
Iteration 5/1000 | Loss: 0.00001663
Iteration 6/1000 | Loss: 0.00001624
Iteration 7/1000 | Loss: 0.00001596
Iteration 8/1000 | Loss: 0.00001568
Iteration 9/1000 | Loss: 0.00001548
Iteration 10/1000 | Loss: 0.00001546
Iteration 11/1000 | Loss: 0.00001528
Iteration 12/1000 | Loss: 0.00001512
Iteration 13/1000 | Loss: 0.00001504
Iteration 14/1000 | Loss: 0.00001502
Iteration 15/1000 | Loss: 0.00001494
Iteration 16/1000 | Loss: 0.00001486
Iteration 17/1000 | Loss: 0.00001486
Iteration 18/1000 | Loss: 0.00001485
Iteration 19/1000 | Loss: 0.00001481
Iteration 20/1000 | Loss: 0.00001480
Iteration 21/1000 | Loss: 0.00001480
Iteration 22/1000 | Loss: 0.00001479
Iteration 23/1000 | Loss: 0.00001479
Iteration 24/1000 | Loss: 0.00001479
Iteration 25/1000 | Loss: 0.00001477
Iteration 26/1000 | Loss: 0.00001476
Iteration 27/1000 | Loss: 0.00001474
Iteration 28/1000 | Loss: 0.00001473
Iteration 29/1000 | Loss: 0.00001473
Iteration 30/1000 | Loss: 0.00001473
Iteration 31/1000 | Loss: 0.00001472
Iteration 32/1000 | Loss: 0.00001472
Iteration 33/1000 | Loss: 0.00001472
Iteration 34/1000 | Loss: 0.00001471
Iteration 35/1000 | Loss: 0.00001471
Iteration 36/1000 | Loss: 0.00001469
Iteration 37/1000 | Loss: 0.00001467
Iteration 38/1000 | Loss: 0.00001467
Iteration 39/1000 | Loss: 0.00001466
Iteration 40/1000 | Loss: 0.00001464
Iteration 41/1000 | Loss: 0.00001463
Iteration 42/1000 | Loss: 0.00001462
Iteration 43/1000 | Loss: 0.00001462
Iteration 44/1000 | Loss: 0.00001462
Iteration 45/1000 | Loss: 0.00001462
Iteration 46/1000 | Loss: 0.00001461
Iteration 47/1000 | Loss: 0.00001461
Iteration 48/1000 | Loss: 0.00001460
Iteration 49/1000 | Loss: 0.00001460
Iteration 50/1000 | Loss: 0.00001460
Iteration 51/1000 | Loss: 0.00001459
Iteration 52/1000 | Loss: 0.00001459
Iteration 53/1000 | Loss: 0.00001458
Iteration 54/1000 | Loss: 0.00001458
Iteration 55/1000 | Loss: 0.00001458
Iteration 56/1000 | Loss: 0.00001458
Iteration 57/1000 | Loss: 0.00001458
Iteration 58/1000 | Loss: 0.00001458
Iteration 59/1000 | Loss: 0.00001458
Iteration 60/1000 | Loss: 0.00001457
Iteration 61/1000 | Loss: 0.00001456
Iteration 62/1000 | Loss: 0.00001456
Iteration 63/1000 | Loss: 0.00001456
Iteration 64/1000 | Loss: 0.00001455
Iteration 65/1000 | Loss: 0.00001455
Iteration 66/1000 | Loss: 0.00001455
Iteration 67/1000 | Loss: 0.00001454
Iteration 68/1000 | Loss: 0.00001454
Iteration 69/1000 | Loss: 0.00001454
Iteration 70/1000 | Loss: 0.00001453
Iteration 71/1000 | Loss: 0.00001452
Iteration 72/1000 | Loss: 0.00001452
Iteration 73/1000 | Loss: 0.00001451
Iteration 74/1000 | Loss: 0.00001451
Iteration 75/1000 | Loss: 0.00001451
Iteration 76/1000 | Loss: 0.00001451
Iteration 77/1000 | Loss: 0.00001450
Iteration 78/1000 | Loss: 0.00001450
Iteration 79/1000 | Loss: 0.00001449
Iteration 80/1000 | Loss: 0.00001449
Iteration 81/1000 | Loss: 0.00001449
Iteration 82/1000 | Loss: 0.00001447
Iteration 83/1000 | Loss: 0.00001447
Iteration 84/1000 | Loss: 0.00001447
Iteration 85/1000 | Loss: 0.00001447
Iteration 86/1000 | Loss: 0.00001447
Iteration 87/1000 | Loss: 0.00001447
Iteration 88/1000 | Loss: 0.00001447
Iteration 89/1000 | Loss: 0.00001447
Iteration 90/1000 | Loss: 0.00001447
Iteration 91/1000 | Loss: 0.00001446
Iteration 92/1000 | Loss: 0.00001446
Iteration 93/1000 | Loss: 0.00001446
Iteration 94/1000 | Loss: 0.00001446
Iteration 95/1000 | Loss: 0.00001445
Iteration 96/1000 | Loss: 0.00001445
Iteration 97/1000 | Loss: 0.00001444
Iteration 98/1000 | Loss: 0.00001444
Iteration 99/1000 | Loss: 0.00001444
Iteration 100/1000 | Loss: 0.00001444
Iteration 101/1000 | Loss: 0.00001444
Iteration 102/1000 | Loss: 0.00001444
Iteration 103/1000 | Loss: 0.00001444
Iteration 104/1000 | Loss: 0.00001444
Iteration 105/1000 | Loss: 0.00001444
Iteration 106/1000 | Loss: 0.00001444
Iteration 107/1000 | Loss: 0.00001443
Iteration 108/1000 | Loss: 0.00001443
Iteration 109/1000 | Loss: 0.00001443
Iteration 110/1000 | Loss: 0.00001443
Iteration 111/1000 | Loss: 0.00001443
Iteration 112/1000 | Loss: 0.00001442
Iteration 113/1000 | Loss: 0.00001442
Iteration 114/1000 | Loss: 0.00001442
Iteration 115/1000 | Loss: 0.00001441
Iteration 116/1000 | Loss: 0.00001441
Iteration 117/1000 | Loss: 0.00001441
Iteration 118/1000 | Loss: 0.00001441
Iteration 119/1000 | Loss: 0.00001441
Iteration 120/1000 | Loss: 0.00001441
Iteration 121/1000 | Loss: 0.00001441
Iteration 122/1000 | Loss: 0.00001441
Iteration 123/1000 | Loss: 0.00001441
Iteration 124/1000 | Loss: 0.00001441
Iteration 125/1000 | Loss: 0.00001440
Iteration 126/1000 | Loss: 0.00001440
Iteration 127/1000 | Loss: 0.00001440
Iteration 128/1000 | Loss: 0.00001440
Iteration 129/1000 | Loss: 0.00001439
Iteration 130/1000 | Loss: 0.00001439
Iteration 131/1000 | Loss: 0.00001439
Iteration 132/1000 | Loss: 0.00001438
Iteration 133/1000 | Loss: 0.00001438
Iteration 134/1000 | Loss: 0.00001438
Iteration 135/1000 | Loss: 0.00001438
Iteration 136/1000 | Loss: 0.00001438
Iteration 137/1000 | Loss: 0.00001438
Iteration 138/1000 | Loss: 0.00001438
Iteration 139/1000 | Loss: 0.00001438
Iteration 140/1000 | Loss: 0.00001438
Iteration 141/1000 | Loss: 0.00001438
Iteration 142/1000 | Loss: 0.00001438
Iteration 143/1000 | Loss: 0.00001438
Iteration 144/1000 | Loss: 0.00001438
Iteration 145/1000 | Loss: 0.00001438
Iteration 146/1000 | Loss: 0.00001438
Iteration 147/1000 | Loss: 0.00001438
Iteration 148/1000 | Loss: 0.00001438
Iteration 149/1000 | Loss: 0.00001438
Iteration 150/1000 | Loss: 0.00001438
Iteration 151/1000 | Loss: 0.00001438
Iteration 152/1000 | Loss: 0.00001438
Iteration 153/1000 | Loss: 0.00001438
Iteration 154/1000 | Loss: 0.00001438
Iteration 155/1000 | Loss: 0.00001438
Iteration 156/1000 | Loss: 0.00001438
Iteration 157/1000 | Loss: 0.00001438
Iteration 158/1000 | Loss: 0.00001438
Iteration 159/1000 | Loss: 0.00001438
Iteration 160/1000 | Loss: 0.00001438
Iteration 161/1000 | Loss: 0.00001438
Iteration 162/1000 | Loss: 0.00001438
Iteration 163/1000 | Loss: 0.00001438
Iteration 164/1000 | Loss: 0.00001438
Iteration 165/1000 | Loss: 0.00001438
Iteration 166/1000 | Loss: 0.00001438
Iteration 167/1000 | Loss: 0.00001438
Iteration 168/1000 | Loss: 0.00001438
Iteration 169/1000 | Loss: 0.00001438
Iteration 170/1000 | Loss: 0.00001438
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [1.437735954823438e-05, 1.437735954823438e-05, 1.437735954823438e-05, 1.437735954823438e-05, 1.437735954823438e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.437735954823438e-05

Optimization complete. Final v2v error: 3.1928534507751465 mm

Highest mean error: 3.939504384994507 mm for frame 70

Lowest mean error: 2.8593454360961914 mm for frame 44

Saving results

Total time: 40.249338150024414
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00794051
Iteration 2/25 | Loss: 0.00122381
Iteration 3/25 | Loss: 0.00112903
Iteration 4/25 | Loss: 0.00111416
Iteration 5/25 | Loss: 0.00111052
Iteration 6/25 | Loss: 0.00111013
Iteration 7/25 | Loss: 0.00111013
Iteration 8/25 | Loss: 0.00111013
Iteration 9/25 | Loss: 0.00111013
Iteration 10/25 | Loss: 0.00111013
Iteration 11/25 | Loss: 0.00111013
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001110127312131226, 0.001110127312131226, 0.001110127312131226, 0.001110127312131226, 0.001110127312131226]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001110127312131226

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31959939
Iteration 2/25 | Loss: 0.00071732
Iteration 3/25 | Loss: 0.00071729
Iteration 4/25 | Loss: 0.00071729
Iteration 5/25 | Loss: 0.00071729
Iteration 6/25 | Loss: 0.00071729
Iteration 7/25 | Loss: 0.00071729
Iteration 8/25 | Loss: 0.00071729
Iteration 9/25 | Loss: 0.00071729
Iteration 10/25 | Loss: 0.00071729
Iteration 11/25 | Loss: 0.00071729
Iteration 12/25 | Loss: 0.00071729
Iteration 13/25 | Loss: 0.00071729
Iteration 14/25 | Loss: 0.00071729
Iteration 15/25 | Loss: 0.00071729
Iteration 16/25 | Loss: 0.00071729
Iteration 17/25 | Loss: 0.00071729
Iteration 18/25 | Loss: 0.00071729
Iteration 19/25 | Loss: 0.00071729
Iteration 20/25 | Loss: 0.00071729
Iteration 21/25 | Loss: 0.00071729
Iteration 22/25 | Loss: 0.00071729
Iteration 23/25 | Loss: 0.00071729
Iteration 24/25 | Loss: 0.00071729
Iteration 25/25 | Loss: 0.00071729

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071729
Iteration 2/1000 | Loss: 0.00002837
Iteration 3/1000 | Loss: 0.00002066
Iteration 4/1000 | Loss: 0.00001734
Iteration 5/1000 | Loss: 0.00001651
Iteration 6/1000 | Loss: 0.00001563
Iteration 7/1000 | Loss: 0.00001516
Iteration 8/1000 | Loss: 0.00001473
Iteration 9/1000 | Loss: 0.00001450
Iteration 10/1000 | Loss: 0.00001414
Iteration 11/1000 | Loss: 0.00001413
Iteration 12/1000 | Loss: 0.00001406
Iteration 13/1000 | Loss: 0.00001395
Iteration 14/1000 | Loss: 0.00001395
Iteration 15/1000 | Loss: 0.00001395
Iteration 16/1000 | Loss: 0.00001395
Iteration 17/1000 | Loss: 0.00001394
Iteration 18/1000 | Loss: 0.00001394
Iteration 19/1000 | Loss: 0.00001393
Iteration 20/1000 | Loss: 0.00001392
Iteration 21/1000 | Loss: 0.00001392
Iteration 22/1000 | Loss: 0.00001385
Iteration 23/1000 | Loss: 0.00001380
Iteration 24/1000 | Loss: 0.00001377
Iteration 25/1000 | Loss: 0.00001372
Iteration 26/1000 | Loss: 0.00001367
Iteration 27/1000 | Loss: 0.00001366
Iteration 28/1000 | Loss: 0.00001365
Iteration 29/1000 | Loss: 0.00001364
Iteration 30/1000 | Loss: 0.00001361
Iteration 31/1000 | Loss: 0.00001360
Iteration 32/1000 | Loss: 0.00001357
Iteration 33/1000 | Loss: 0.00001350
Iteration 34/1000 | Loss: 0.00001348
Iteration 35/1000 | Loss: 0.00001348
Iteration 36/1000 | Loss: 0.00001347
Iteration 37/1000 | Loss: 0.00001346
Iteration 38/1000 | Loss: 0.00001346
Iteration 39/1000 | Loss: 0.00001345
Iteration 40/1000 | Loss: 0.00001343
Iteration 41/1000 | Loss: 0.00001343
Iteration 42/1000 | Loss: 0.00001343
Iteration 43/1000 | Loss: 0.00001343
Iteration 44/1000 | Loss: 0.00001342
Iteration 45/1000 | Loss: 0.00001341
Iteration 46/1000 | Loss: 0.00001336
Iteration 47/1000 | Loss: 0.00001336
Iteration 48/1000 | Loss: 0.00001336
Iteration 49/1000 | Loss: 0.00001335
Iteration 50/1000 | Loss: 0.00001334
Iteration 51/1000 | Loss: 0.00001333
Iteration 52/1000 | Loss: 0.00001333
Iteration 53/1000 | Loss: 0.00001332
Iteration 54/1000 | Loss: 0.00001332
Iteration 55/1000 | Loss: 0.00001331
Iteration 56/1000 | Loss: 0.00001331
Iteration 57/1000 | Loss: 0.00001330
Iteration 58/1000 | Loss: 0.00001330
Iteration 59/1000 | Loss: 0.00001329
Iteration 60/1000 | Loss: 0.00001329
Iteration 61/1000 | Loss: 0.00001329
Iteration 62/1000 | Loss: 0.00001329
Iteration 63/1000 | Loss: 0.00001328
Iteration 64/1000 | Loss: 0.00001328
Iteration 65/1000 | Loss: 0.00001326
Iteration 66/1000 | Loss: 0.00001326
Iteration 67/1000 | Loss: 0.00001326
Iteration 68/1000 | Loss: 0.00001326
Iteration 69/1000 | Loss: 0.00001326
Iteration 70/1000 | Loss: 0.00001326
Iteration 71/1000 | Loss: 0.00001326
Iteration 72/1000 | Loss: 0.00001325
Iteration 73/1000 | Loss: 0.00001325
Iteration 74/1000 | Loss: 0.00001324
Iteration 75/1000 | Loss: 0.00001322
Iteration 76/1000 | Loss: 0.00001321
Iteration 77/1000 | Loss: 0.00001321
Iteration 78/1000 | Loss: 0.00001321
Iteration 79/1000 | Loss: 0.00001320
Iteration 80/1000 | Loss: 0.00001320
Iteration 81/1000 | Loss: 0.00001319
Iteration 82/1000 | Loss: 0.00001318
Iteration 83/1000 | Loss: 0.00001318
Iteration 84/1000 | Loss: 0.00001318
Iteration 85/1000 | Loss: 0.00001318
Iteration 86/1000 | Loss: 0.00001317
Iteration 87/1000 | Loss: 0.00001317
Iteration 88/1000 | Loss: 0.00001316
Iteration 89/1000 | Loss: 0.00001316
Iteration 90/1000 | Loss: 0.00001316
Iteration 91/1000 | Loss: 0.00001316
Iteration 92/1000 | Loss: 0.00001316
Iteration 93/1000 | Loss: 0.00001315
Iteration 94/1000 | Loss: 0.00001315
Iteration 95/1000 | Loss: 0.00001315
Iteration 96/1000 | Loss: 0.00001315
Iteration 97/1000 | Loss: 0.00001315
Iteration 98/1000 | Loss: 0.00001315
Iteration 99/1000 | Loss: 0.00001315
Iteration 100/1000 | Loss: 0.00001315
Iteration 101/1000 | Loss: 0.00001314
Iteration 102/1000 | Loss: 0.00001314
Iteration 103/1000 | Loss: 0.00001314
Iteration 104/1000 | Loss: 0.00001314
Iteration 105/1000 | Loss: 0.00001314
Iteration 106/1000 | Loss: 0.00001314
Iteration 107/1000 | Loss: 0.00001314
Iteration 108/1000 | Loss: 0.00001314
Iteration 109/1000 | Loss: 0.00001314
Iteration 110/1000 | Loss: 0.00001314
Iteration 111/1000 | Loss: 0.00001314
Iteration 112/1000 | Loss: 0.00001314
Iteration 113/1000 | Loss: 0.00001313
Iteration 114/1000 | Loss: 0.00001313
Iteration 115/1000 | Loss: 0.00001313
Iteration 116/1000 | Loss: 0.00001313
Iteration 117/1000 | Loss: 0.00001313
Iteration 118/1000 | Loss: 0.00001313
Iteration 119/1000 | Loss: 0.00001313
Iteration 120/1000 | Loss: 0.00001313
Iteration 121/1000 | Loss: 0.00001313
Iteration 122/1000 | Loss: 0.00001313
Iteration 123/1000 | Loss: 0.00001312
Iteration 124/1000 | Loss: 0.00001312
Iteration 125/1000 | Loss: 0.00001312
Iteration 126/1000 | Loss: 0.00001312
Iteration 127/1000 | Loss: 0.00001312
Iteration 128/1000 | Loss: 0.00001312
Iteration 129/1000 | Loss: 0.00001312
Iteration 130/1000 | Loss: 0.00001312
Iteration 131/1000 | Loss: 0.00001312
Iteration 132/1000 | Loss: 0.00001312
Iteration 133/1000 | Loss: 0.00001312
Iteration 134/1000 | Loss: 0.00001312
Iteration 135/1000 | Loss: 0.00001312
Iteration 136/1000 | Loss: 0.00001311
Iteration 137/1000 | Loss: 0.00001311
Iteration 138/1000 | Loss: 0.00001311
Iteration 139/1000 | Loss: 0.00001311
Iteration 140/1000 | Loss: 0.00001311
Iteration 141/1000 | Loss: 0.00001311
Iteration 142/1000 | Loss: 0.00001311
Iteration 143/1000 | Loss: 0.00001311
Iteration 144/1000 | Loss: 0.00001311
Iteration 145/1000 | Loss: 0.00001311
Iteration 146/1000 | Loss: 0.00001311
Iteration 147/1000 | Loss: 0.00001311
Iteration 148/1000 | Loss: 0.00001311
Iteration 149/1000 | Loss: 0.00001311
Iteration 150/1000 | Loss: 0.00001311
Iteration 151/1000 | Loss: 0.00001311
Iteration 152/1000 | Loss: 0.00001311
Iteration 153/1000 | Loss: 0.00001311
Iteration 154/1000 | Loss: 0.00001311
Iteration 155/1000 | Loss: 0.00001311
Iteration 156/1000 | Loss: 0.00001311
Iteration 157/1000 | Loss: 0.00001311
Iteration 158/1000 | Loss: 0.00001311
Iteration 159/1000 | Loss: 0.00001311
Iteration 160/1000 | Loss: 0.00001311
Iteration 161/1000 | Loss: 0.00001311
Iteration 162/1000 | Loss: 0.00001311
Iteration 163/1000 | Loss: 0.00001311
Iteration 164/1000 | Loss: 0.00001311
Iteration 165/1000 | Loss: 0.00001311
Iteration 166/1000 | Loss: 0.00001311
Iteration 167/1000 | Loss: 0.00001311
Iteration 168/1000 | Loss: 0.00001311
Iteration 169/1000 | Loss: 0.00001311
Iteration 170/1000 | Loss: 0.00001311
Iteration 171/1000 | Loss: 0.00001311
Iteration 172/1000 | Loss: 0.00001311
Iteration 173/1000 | Loss: 0.00001311
Iteration 174/1000 | Loss: 0.00001311
Iteration 175/1000 | Loss: 0.00001311
Iteration 176/1000 | Loss: 0.00001311
Iteration 177/1000 | Loss: 0.00001311
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [1.3105611287755892e-05, 1.3105611287755892e-05, 1.3105611287755892e-05, 1.3105611287755892e-05, 1.3105611287755892e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3105611287755892e-05

Optimization complete. Final v2v error: 3.073369264602661 mm

Highest mean error: 3.306658983230591 mm for frame 65

Lowest mean error: 2.821489095687866 mm for frame 0

Saving results

Total time: 43.463422775268555
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01088107
Iteration 2/25 | Loss: 0.01088107
Iteration 3/25 | Loss: 0.01088107
Iteration 4/25 | Loss: 0.01088107
Iteration 5/25 | Loss: 0.01088107
Iteration 6/25 | Loss: 0.01088107
Iteration 7/25 | Loss: 0.01088107
Iteration 8/25 | Loss: 0.01088107
Iteration 9/25 | Loss: 0.01088107
Iteration 10/25 | Loss: 0.01088107
Iteration 11/25 | Loss: 0.01088106
Iteration 12/25 | Loss: 0.01088106
Iteration 13/25 | Loss: 0.01088106
Iteration 14/25 | Loss: 0.01088106
Iteration 15/25 | Loss: 0.01088106
Iteration 16/25 | Loss: 0.01088106
Iteration 17/25 | Loss: 0.01088106
Iteration 18/25 | Loss: 0.01088106
Iteration 19/25 | Loss: 0.01088106
Iteration 20/25 | Loss: 0.01088106
Iteration 21/25 | Loss: 0.01088106
Iteration 22/25 | Loss: 0.01088106
Iteration 23/25 | Loss: 0.01088106
Iteration 24/25 | Loss: 0.01088106
Iteration 25/25 | Loss: 0.01088106

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.30314827
Iteration 2/25 | Loss: 0.18303533
Iteration 3/25 | Loss: 0.17603177
Iteration 4/25 | Loss: 0.17482893
Iteration 5/25 | Loss: 0.17482889
Iteration 6/25 | Loss: 0.17482886
Iteration 7/25 | Loss: 0.17482886
Iteration 8/25 | Loss: 0.17482881
Iteration 9/25 | Loss: 0.17482881
Iteration 10/25 | Loss: 0.17482881
Iteration 11/25 | Loss: 0.17482881
Iteration 12/25 | Loss: 0.17482881
Iteration 13/25 | Loss: 0.17482881
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.17482881247997284, 0.17482881247997284, 0.17482881247997284, 0.17482881247997284, 0.17482881247997284]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.17482881247997284

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17482881
Iteration 2/1000 | Loss: 0.00687087
Iteration 3/1000 | Loss: 0.00051217
Iteration 4/1000 | Loss: 0.00112112
Iteration 5/1000 | Loss: 0.00034132
Iteration 6/1000 | Loss: 0.00120756
Iteration 7/1000 | Loss: 0.00010716
Iteration 8/1000 | Loss: 0.00021828
Iteration 9/1000 | Loss: 0.00009087
Iteration 10/1000 | Loss: 0.00002818
Iteration 11/1000 | Loss: 0.00006701
Iteration 12/1000 | Loss: 0.00002841
Iteration 13/1000 | Loss: 0.00004316
Iteration 14/1000 | Loss: 0.00002019
Iteration 15/1000 | Loss: 0.00022708
Iteration 16/1000 | Loss: 0.00014791
Iteration 17/1000 | Loss: 0.00011046
Iteration 18/1000 | Loss: 0.00002034
Iteration 19/1000 | Loss: 0.00009081
Iteration 20/1000 | Loss: 0.00003446
Iteration 21/1000 | Loss: 0.00002101
Iteration 22/1000 | Loss: 0.00002582
Iteration 23/1000 | Loss: 0.00001614
Iteration 24/1000 | Loss: 0.00001549
Iteration 25/1000 | Loss: 0.00001509
Iteration 26/1000 | Loss: 0.00001479
Iteration 27/1000 | Loss: 0.00001627
Iteration 28/1000 | Loss: 0.00001450
Iteration 29/1000 | Loss: 0.00006269
Iteration 30/1000 | Loss: 0.00001393
Iteration 31/1000 | Loss: 0.00001355
Iteration 32/1000 | Loss: 0.00001327
Iteration 33/1000 | Loss: 0.00011297
Iteration 34/1000 | Loss: 0.00001327
Iteration 35/1000 | Loss: 0.00001295
Iteration 36/1000 | Loss: 0.00001288
Iteration 37/1000 | Loss: 0.00005466
Iteration 38/1000 | Loss: 0.00007956
Iteration 39/1000 | Loss: 0.00001997
Iteration 40/1000 | Loss: 0.00001672
Iteration 41/1000 | Loss: 0.00001365
Iteration 42/1000 | Loss: 0.00004056
Iteration 43/1000 | Loss: 0.00001335
Iteration 44/1000 | Loss: 0.00001286
Iteration 45/1000 | Loss: 0.00001266
Iteration 46/1000 | Loss: 0.00001308
Iteration 47/1000 | Loss: 0.00001287
Iteration 48/1000 | Loss: 0.00001297
Iteration 49/1000 | Loss: 0.00001292
Iteration 50/1000 | Loss: 0.00001259
Iteration 51/1000 | Loss: 0.00001291
Iteration 52/1000 | Loss: 0.00001291
Iteration 53/1000 | Loss: 0.00008516
Iteration 54/1000 | Loss: 0.00001650
Iteration 55/1000 | Loss: 0.00001532
Iteration 56/1000 | Loss: 0.00003599
Iteration 57/1000 | Loss: 0.00001281
Iteration 58/1000 | Loss: 0.00001301
Iteration 59/1000 | Loss: 0.00001300
Iteration 60/1000 | Loss: 0.00001285
Iteration 61/1000 | Loss: 0.00001288
Iteration 62/1000 | Loss: 0.00001285
Iteration 63/1000 | Loss: 0.00001281
Iteration 64/1000 | Loss: 0.00001286
Iteration 65/1000 | Loss: 0.00001284
Iteration 66/1000 | Loss: 0.00001277
Iteration 67/1000 | Loss: 0.00001276
Iteration 68/1000 | Loss: 0.00001261
Iteration 69/1000 | Loss: 0.00001245
Iteration 70/1000 | Loss: 0.00001244
Iteration 71/1000 | Loss: 0.00001244
Iteration 72/1000 | Loss: 0.00001244
Iteration 73/1000 | Loss: 0.00001244
Iteration 74/1000 | Loss: 0.00001244
Iteration 75/1000 | Loss: 0.00001244
Iteration 76/1000 | Loss: 0.00001268
Iteration 77/1000 | Loss: 0.00001268
Iteration 78/1000 | Loss: 0.00001273
Iteration 79/1000 | Loss: 0.00001258
Iteration 80/1000 | Loss: 0.00001241
Iteration 81/1000 | Loss: 0.00001267
Iteration 82/1000 | Loss: 0.00001267
Iteration 83/1000 | Loss: 0.00001267
Iteration 84/1000 | Loss: 0.00001267
Iteration 85/1000 | Loss: 0.00001266
Iteration 86/1000 | Loss: 0.00001233
Iteration 87/1000 | Loss: 0.00001228
Iteration 88/1000 | Loss: 0.00001228
Iteration 89/1000 | Loss: 0.00001228
Iteration 90/1000 | Loss: 0.00001228
Iteration 91/1000 | Loss: 0.00001228
Iteration 92/1000 | Loss: 0.00001228
Iteration 93/1000 | Loss: 0.00001228
Iteration 94/1000 | Loss: 0.00001228
Iteration 95/1000 | Loss: 0.00001228
Iteration 96/1000 | Loss: 0.00001228
Iteration 97/1000 | Loss: 0.00001228
Iteration 98/1000 | Loss: 0.00001227
Iteration 99/1000 | Loss: 0.00001227
Iteration 100/1000 | Loss: 0.00001227
Iteration 101/1000 | Loss: 0.00001227
Iteration 102/1000 | Loss: 0.00001226
Iteration 103/1000 | Loss: 0.00001226
Iteration 104/1000 | Loss: 0.00007340
Iteration 105/1000 | Loss: 0.00002013
Iteration 106/1000 | Loss: 0.00001235
Iteration 107/1000 | Loss: 0.00003581
Iteration 108/1000 | Loss: 0.00001276
Iteration 109/1000 | Loss: 0.00001939
Iteration 110/1000 | Loss: 0.00002133
Iteration 111/1000 | Loss: 0.00004895
Iteration 112/1000 | Loss: 0.00001912
Iteration 113/1000 | Loss: 0.00001270
Iteration 114/1000 | Loss: 0.00001243
Iteration 115/1000 | Loss: 0.00001233
Iteration 116/1000 | Loss: 0.00001230
Iteration 117/1000 | Loss: 0.00001216
Iteration 118/1000 | Loss: 0.00001216
Iteration 119/1000 | Loss: 0.00001216
Iteration 120/1000 | Loss: 0.00001216
Iteration 121/1000 | Loss: 0.00001216
Iteration 122/1000 | Loss: 0.00001231
Iteration 123/1000 | Loss: 0.00001223
Iteration 124/1000 | Loss: 0.00001217
Iteration 125/1000 | Loss: 0.00001216
Iteration 126/1000 | Loss: 0.00001216
Iteration 127/1000 | Loss: 0.00001216
Iteration 128/1000 | Loss: 0.00001216
Iteration 129/1000 | Loss: 0.00001216
Iteration 130/1000 | Loss: 0.00001216
Iteration 131/1000 | Loss: 0.00001216
Iteration 132/1000 | Loss: 0.00001215
Iteration 133/1000 | Loss: 0.00001215
Iteration 134/1000 | Loss: 0.00001215
Iteration 135/1000 | Loss: 0.00001215
Iteration 136/1000 | Loss: 0.00001233
Iteration 137/1000 | Loss: 0.00001215
Iteration 138/1000 | Loss: 0.00001233
Iteration 139/1000 | Loss: 0.00001233
Iteration 140/1000 | Loss: 0.00001233
Iteration 141/1000 | Loss: 0.00001233
Iteration 142/1000 | Loss: 0.00001233
Iteration 143/1000 | Loss: 0.00001233
Iteration 144/1000 | Loss: 0.00001233
Iteration 145/1000 | Loss: 0.00001233
Iteration 146/1000 | Loss: 0.00001233
Iteration 147/1000 | Loss: 0.00001233
Iteration 148/1000 | Loss: 0.00001233
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.2325871466600802e-05, 1.2325871466600802e-05, 1.2325871466600802e-05, 1.2325871466600802e-05, 1.2325871466600802e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2325871466600802e-05

Optimization complete. Final v2v error: 2.930860996246338 mm

Highest mean error: 9.966084480285645 mm for frame 100

Lowest mean error: 2.6619367599487305 mm for frame 10

Saving results

Total time: 138.53202486038208
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00881404
Iteration 2/25 | Loss: 0.00166249
Iteration 3/25 | Loss: 0.00129969
Iteration 4/25 | Loss: 0.00123889
Iteration 5/25 | Loss: 0.00122605
Iteration 6/25 | Loss: 0.00121539
Iteration 7/25 | Loss: 0.00119992
Iteration 8/25 | Loss: 0.00120314
Iteration 9/25 | Loss: 0.00119209
Iteration 10/25 | Loss: 0.00119376
Iteration 11/25 | Loss: 0.00118804
Iteration 12/25 | Loss: 0.00118576
Iteration 13/25 | Loss: 0.00118529
Iteration 14/25 | Loss: 0.00119087
Iteration 15/25 | Loss: 0.00119171
Iteration 16/25 | Loss: 0.00118388
Iteration 17/25 | Loss: 0.00118267
Iteration 18/25 | Loss: 0.00118213
Iteration 19/25 | Loss: 0.00118172
Iteration 20/25 | Loss: 0.00118165
Iteration 21/25 | Loss: 0.00118165
Iteration 22/25 | Loss: 0.00118164
Iteration 23/25 | Loss: 0.00118164
Iteration 24/25 | Loss: 0.00118164
Iteration 25/25 | Loss: 0.00118164

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.70562363
Iteration 2/25 | Loss: 0.00075883
Iteration 3/25 | Loss: 0.00075861
Iteration 4/25 | Loss: 0.00075861
Iteration 5/25 | Loss: 0.00075861
Iteration 6/25 | Loss: 0.00075861
Iteration 7/25 | Loss: 0.00075861
Iteration 8/25 | Loss: 0.00075861
Iteration 9/25 | Loss: 0.00075861
Iteration 10/25 | Loss: 0.00075861
Iteration 11/25 | Loss: 0.00075861
Iteration 12/25 | Loss: 0.00075861
Iteration 13/25 | Loss: 0.00075861
Iteration 14/25 | Loss: 0.00075861
Iteration 15/25 | Loss: 0.00075861
Iteration 16/25 | Loss: 0.00075861
Iteration 17/25 | Loss: 0.00075861
Iteration 18/25 | Loss: 0.00075861
Iteration 19/25 | Loss: 0.00075861
Iteration 20/25 | Loss: 0.00075861
Iteration 21/25 | Loss: 0.00075861
Iteration 22/25 | Loss: 0.00075861
Iteration 23/25 | Loss: 0.00075861
Iteration 24/25 | Loss: 0.00075861
Iteration 25/25 | Loss: 0.00075861

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075861
Iteration 2/1000 | Loss: 0.00031980
Iteration 3/1000 | Loss: 0.00005434
Iteration 4/1000 | Loss: 0.00010130
Iteration 5/1000 | Loss: 0.00005253
Iteration 6/1000 | Loss: 0.00012333
Iteration 7/1000 | Loss: 0.00013478
Iteration 8/1000 | Loss: 0.00002730
Iteration 9/1000 | Loss: 0.00002424
Iteration 10/1000 | Loss: 0.00021319
Iteration 11/1000 | Loss: 0.00002933
Iteration 12/1000 | Loss: 0.00002701
Iteration 13/1000 | Loss: 0.00002493
Iteration 14/1000 | Loss: 0.00002397
Iteration 15/1000 | Loss: 0.00002330
Iteration 16/1000 | Loss: 0.00002290
Iteration 17/1000 | Loss: 0.00002245
Iteration 18/1000 | Loss: 0.00010042
Iteration 19/1000 | Loss: 0.00005994
Iteration 20/1000 | Loss: 0.00002131
Iteration 21/1000 | Loss: 0.00003874
Iteration 22/1000 | Loss: 0.00002305
Iteration 23/1000 | Loss: 0.00002121
Iteration 24/1000 | Loss: 0.00002046
Iteration 25/1000 | Loss: 0.00002024
Iteration 26/1000 | Loss: 0.00012154
Iteration 27/1000 | Loss: 0.00002142
Iteration 28/1000 | Loss: 0.00001972
Iteration 29/1000 | Loss: 0.00001918
Iteration 30/1000 | Loss: 0.00001885
Iteration 31/1000 | Loss: 0.00020491
Iteration 32/1000 | Loss: 0.00074893
Iteration 33/1000 | Loss: 0.00035507
Iteration 34/1000 | Loss: 0.00026106
Iteration 35/1000 | Loss: 0.00019865
Iteration 36/1000 | Loss: 0.00016657
Iteration 37/1000 | Loss: 0.00027382
Iteration 38/1000 | Loss: 0.00002952
Iteration 39/1000 | Loss: 0.00002367
Iteration 40/1000 | Loss: 0.00002214
Iteration 41/1000 | Loss: 0.00002140
Iteration 42/1000 | Loss: 0.00002056
Iteration 43/1000 | Loss: 0.00012544
Iteration 44/1000 | Loss: 0.00002231
Iteration 45/1000 | Loss: 0.00002056
Iteration 46/1000 | Loss: 0.00001980
Iteration 47/1000 | Loss: 0.00001919
Iteration 48/1000 | Loss: 0.00001847
Iteration 49/1000 | Loss: 0.00001793
Iteration 50/1000 | Loss: 0.00001769
Iteration 51/1000 | Loss: 0.00001749
Iteration 52/1000 | Loss: 0.00001747
Iteration 53/1000 | Loss: 0.00001742
Iteration 54/1000 | Loss: 0.00001741
Iteration 55/1000 | Loss: 0.00001738
Iteration 56/1000 | Loss: 0.00001738
Iteration 57/1000 | Loss: 0.00001737
Iteration 58/1000 | Loss: 0.00001737
Iteration 59/1000 | Loss: 0.00001736
Iteration 60/1000 | Loss: 0.00001736
Iteration 61/1000 | Loss: 0.00001735
Iteration 62/1000 | Loss: 0.00001735
Iteration 63/1000 | Loss: 0.00001735
Iteration 64/1000 | Loss: 0.00001734
Iteration 65/1000 | Loss: 0.00001734
Iteration 66/1000 | Loss: 0.00001734
Iteration 67/1000 | Loss: 0.00001734
Iteration 68/1000 | Loss: 0.00001734
Iteration 69/1000 | Loss: 0.00001734
Iteration 70/1000 | Loss: 0.00001734
Iteration 71/1000 | Loss: 0.00001733
Iteration 72/1000 | Loss: 0.00001733
Iteration 73/1000 | Loss: 0.00001733
Iteration 74/1000 | Loss: 0.00001733
Iteration 75/1000 | Loss: 0.00001733
Iteration 76/1000 | Loss: 0.00001733
Iteration 77/1000 | Loss: 0.00001733
Iteration 78/1000 | Loss: 0.00001733
Iteration 79/1000 | Loss: 0.00001733
Iteration 80/1000 | Loss: 0.00001733
Iteration 81/1000 | Loss: 0.00001733
Iteration 82/1000 | Loss: 0.00001733
Iteration 83/1000 | Loss: 0.00001733
Iteration 84/1000 | Loss: 0.00001732
Iteration 85/1000 | Loss: 0.00001732
Iteration 86/1000 | Loss: 0.00001732
Iteration 87/1000 | Loss: 0.00001732
Iteration 88/1000 | Loss: 0.00001732
Iteration 89/1000 | Loss: 0.00001732
Iteration 90/1000 | Loss: 0.00001732
Iteration 91/1000 | Loss: 0.00001732
Iteration 92/1000 | Loss: 0.00001732
Iteration 93/1000 | Loss: 0.00001731
Iteration 94/1000 | Loss: 0.00001731
Iteration 95/1000 | Loss: 0.00001731
Iteration 96/1000 | Loss: 0.00001731
Iteration 97/1000 | Loss: 0.00001731
Iteration 98/1000 | Loss: 0.00001731
Iteration 99/1000 | Loss: 0.00001731
Iteration 100/1000 | Loss: 0.00001731
Iteration 101/1000 | Loss: 0.00001731
Iteration 102/1000 | Loss: 0.00001731
Iteration 103/1000 | Loss: 0.00001731
Iteration 104/1000 | Loss: 0.00001731
Iteration 105/1000 | Loss: 0.00001731
Iteration 106/1000 | Loss: 0.00001731
Iteration 107/1000 | Loss: 0.00001731
Iteration 108/1000 | Loss: 0.00001730
Iteration 109/1000 | Loss: 0.00001730
Iteration 110/1000 | Loss: 0.00001730
Iteration 111/1000 | Loss: 0.00001730
Iteration 112/1000 | Loss: 0.00001730
Iteration 113/1000 | Loss: 0.00001730
Iteration 114/1000 | Loss: 0.00001729
Iteration 115/1000 | Loss: 0.00001729
Iteration 116/1000 | Loss: 0.00001729
Iteration 117/1000 | Loss: 0.00001729
Iteration 118/1000 | Loss: 0.00001729
Iteration 119/1000 | Loss: 0.00001728
Iteration 120/1000 | Loss: 0.00001728
Iteration 121/1000 | Loss: 0.00001728
Iteration 122/1000 | Loss: 0.00001728
Iteration 123/1000 | Loss: 0.00001728
Iteration 124/1000 | Loss: 0.00001728
Iteration 125/1000 | Loss: 0.00001728
Iteration 126/1000 | Loss: 0.00001728
Iteration 127/1000 | Loss: 0.00001727
Iteration 128/1000 | Loss: 0.00001727
Iteration 129/1000 | Loss: 0.00001727
Iteration 130/1000 | Loss: 0.00001727
Iteration 131/1000 | Loss: 0.00001727
Iteration 132/1000 | Loss: 0.00001727
Iteration 133/1000 | Loss: 0.00001727
Iteration 134/1000 | Loss: 0.00001726
Iteration 135/1000 | Loss: 0.00001726
Iteration 136/1000 | Loss: 0.00001726
Iteration 137/1000 | Loss: 0.00001726
Iteration 138/1000 | Loss: 0.00001726
Iteration 139/1000 | Loss: 0.00001726
Iteration 140/1000 | Loss: 0.00001726
Iteration 141/1000 | Loss: 0.00001726
Iteration 142/1000 | Loss: 0.00001726
Iteration 143/1000 | Loss: 0.00001726
Iteration 144/1000 | Loss: 0.00001726
Iteration 145/1000 | Loss: 0.00001726
Iteration 146/1000 | Loss: 0.00001726
Iteration 147/1000 | Loss: 0.00001726
Iteration 148/1000 | Loss: 0.00001726
Iteration 149/1000 | Loss: 0.00001726
Iteration 150/1000 | Loss: 0.00001726
Iteration 151/1000 | Loss: 0.00001726
Iteration 152/1000 | Loss: 0.00001725
Iteration 153/1000 | Loss: 0.00001725
Iteration 154/1000 | Loss: 0.00001725
Iteration 155/1000 | Loss: 0.00001725
Iteration 156/1000 | Loss: 0.00001725
Iteration 157/1000 | Loss: 0.00001725
Iteration 158/1000 | Loss: 0.00001725
Iteration 159/1000 | Loss: 0.00001725
Iteration 160/1000 | Loss: 0.00001725
Iteration 161/1000 | Loss: 0.00001725
Iteration 162/1000 | Loss: 0.00001725
Iteration 163/1000 | Loss: 0.00001725
Iteration 164/1000 | Loss: 0.00001725
Iteration 165/1000 | Loss: 0.00001725
Iteration 166/1000 | Loss: 0.00001725
Iteration 167/1000 | Loss: 0.00001725
Iteration 168/1000 | Loss: 0.00001724
Iteration 169/1000 | Loss: 0.00001724
Iteration 170/1000 | Loss: 0.00001724
Iteration 171/1000 | Loss: 0.00001724
Iteration 172/1000 | Loss: 0.00001724
Iteration 173/1000 | Loss: 0.00001724
Iteration 174/1000 | Loss: 0.00001724
Iteration 175/1000 | Loss: 0.00001724
Iteration 176/1000 | Loss: 0.00001724
Iteration 177/1000 | Loss: 0.00001724
Iteration 178/1000 | Loss: 0.00001724
Iteration 179/1000 | Loss: 0.00001724
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.7243617548956536e-05, 1.7243617548956536e-05, 1.7243617548956536e-05, 1.7243617548956536e-05, 1.7243617548956536e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7243617548956536e-05

Optimization complete. Final v2v error: 3.4491467475891113 mm

Highest mean error: 5.388988971710205 mm for frame 98

Lowest mean error: 2.785015106201172 mm for frame 19

Saving results

Total time: 121.00377082824707
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00722820
Iteration 2/25 | Loss: 0.00144169
Iteration 3/25 | Loss: 0.00127062
Iteration 4/25 | Loss: 0.00125665
Iteration 5/25 | Loss: 0.00125144
Iteration 6/25 | Loss: 0.00125057
Iteration 7/25 | Loss: 0.00125057
Iteration 8/25 | Loss: 0.00125057
Iteration 9/25 | Loss: 0.00125057
Iteration 10/25 | Loss: 0.00125057
Iteration 11/25 | Loss: 0.00125057
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012505673803389072, 0.0012505673803389072, 0.0012505673803389072, 0.0012505673803389072, 0.0012505673803389072]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012505673803389072

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.78967661
Iteration 2/25 | Loss: 0.00092256
Iteration 3/25 | Loss: 0.00092250
Iteration 4/25 | Loss: 0.00092250
Iteration 5/25 | Loss: 0.00092250
Iteration 6/25 | Loss: 0.00092250
Iteration 7/25 | Loss: 0.00092250
Iteration 8/25 | Loss: 0.00092250
Iteration 9/25 | Loss: 0.00092250
Iteration 10/25 | Loss: 0.00092250
Iteration 11/25 | Loss: 0.00092250
Iteration 12/25 | Loss: 0.00092250
Iteration 13/25 | Loss: 0.00092250
Iteration 14/25 | Loss: 0.00092250
Iteration 15/25 | Loss: 0.00092250
Iteration 16/25 | Loss: 0.00092250
Iteration 17/25 | Loss: 0.00092250
Iteration 18/25 | Loss: 0.00092250
Iteration 19/25 | Loss: 0.00092250
Iteration 20/25 | Loss: 0.00092250
Iteration 21/25 | Loss: 0.00092250
Iteration 22/25 | Loss: 0.00092250
Iteration 23/25 | Loss: 0.00092250
Iteration 24/25 | Loss: 0.00092250
Iteration 25/25 | Loss: 0.00092250

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092250
Iteration 2/1000 | Loss: 0.00007351
Iteration 3/1000 | Loss: 0.00004574
Iteration 4/1000 | Loss: 0.00003754
Iteration 5/1000 | Loss: 0.00003453
Iteration 6/1000 | Loss: 0.00003292
Iteration 7/1000 | Loss: 0.00003224
Iteration 8/1000 | Loss: 0.00003159
Iteration 9/1000 | Loss: 0.00003104
Iteration 10/1000 | Loss: 0.00003053
Iteration 11/1000 | Loss: 0.00003019
Iteration 12/1000 | Loss: 0.00002990
Iteration 13/1000 | Loss: 0.00002965
Iteration 14/1000 | Loss: 0.00002939
Iteration 15/1000 | Loss: 0.00002914
Iteration 16/1000 | Loss: 0.00002898
Iteration 17/1000 | Loss: 0.00002894
Iteration 18/1000 | Loss: 0.00002888
Iteration 19/1000 | Loss: 0.00002885
Iteration 20/1000 | Loss: 0.00002882
Iteration 21/1000 | Loss: 0.00002875
Iteration 22/1000 | Loss: 0.00002869
Iteration 23/1000 | Loss: 0.00002866
Iteration 24/1000 | Loss: 0.00002856
Iteration 25/1000 | Loss: 0.00002851
Iteration 26/1000 | Loss: 0.00002851
Iteration 27/1000 | Loss: 0.00002849
Iteration 28/1000 | Loss: 0.00002849
Iteration 29/1000 | Loss: 0.00002848
Iteration 30/1000 | Loss: 0.00002845
Iteration 31/1000 | Loss: 0.00002845
Iteration 32/1000 | Loss: 0.00002845
Iteration 33/1000 | Loss: 0.00002845
Iteration 34/1000 | Loss: 0.00002845
Iteration 35/1000 | Loss: 0.00002845
Iteration 36/1000 | Loss: 0.00002845
Iteration 37/1000 | Loss: 0.00002845
Iteration 38/1000 | Loss: 0.00002845
Iteration 39/1000 | Loss: 0.00002845
Iteration 40/1000 | Loss: 0.00002845
Iteration 41/1000 | Loss: 0.00002845
Iteration 42/1000 | Loss: 0.00002844
Iteration 43/1000 | Loss: 0.00002844
Iteration 44/1000 | Loss: 0.00002843
Iteration 45/1000 | Loss: 0.00002842
Iteration 46/1000 | Loss: 0.00002842
Iteration 47/1000 | Loss: 0.00002841
Iteration 48/1000 | Loss: 0.00002839
Iteration 49/1000 | Loss: 0.00002839
Iteration 50/1000 | Loss: 0.00002839
Iteration 51/1000 | Loss: 0.00002839
Iteration 52/1000 | Loss: 0.00002838
Iteration 53/1000 | Loss: 0.00002837
Iteration 54/1000 | Loss: 0.00002837
Iteration 55/1000 | Loss: 0.00002837
Iteration 56/1000 | Loss: 0.00002837
Iteration 57/1000 | Loss: 0.00002837
Iteration 58/1000 | Loss: 0.00002836
Iteration 59/1000 | Loss: 0.00002836
Iteration 60/1000 | Loss: 0.00002835
Iteration 61/1000 | Loss: 0.00002833
Iteration 62/1000 | Loss: 0.00002832
Iteration 63/1000 | Loss: 0.00002832
Iteration 64/1000 | Loss: 0.00002832
Iteration 65/1000 | Loss: 0.00002831
Iteration 66/1000 | Loss: 0.00002831
Iteration 67/1000 | Loss: 0.00002830
Iteration 68/1000 | Loss: 0.00002830
Iteration 69/1000 | Loss: 0.00002830
Iteration 70/1000 | Loss: 0.00002830
Iteration 71/1000 | Loss: 0.00002829
Iteration 72/1000 | Loss: 0.00002829
Iteration 73/1000 | Loss: 0.00002829
Iteration 74/1000 | Loss: 0.00002829
Iteration 75/1000 | Loss: 0.00002829
Iteration 76/1000 | Loss: 0.00002829
Iteration 77/1000 | Loss: 0.00002829
Iteration 78/1000 | Loss: 0.00002827
Iteration 79/1000 | Loss: 0.00002827
Iteration 80/1000 | Loss: 0.00002826
Iteration 81/1000 | Loss: 0.00002825
Iteration 82/1000 | Loss: 0.00002825
Iteration 83/1000 | Loss: 0.00002825
Iteration 84/1000 | Loss: 0.00002825
Iteration 85/1000 | Loss: 0.00002825
Iteration 86/1000 | Loss: 0.00002825
Iteration 87/1000 | Loss: 0.00002825
Iteration 88/1000 | Loss: 0.00002825
Iteration 89/1000 | Loss: 0.00002825
Iteration 90/1000 | Loss: 0.00002824
Iteration 91/1000 | Loss: 0.00002823
Iteration 92/1000 | Loss: 0.00002823
Iteration 93/1000 | Loss: 0.00002823
Iteration 94/1000 | Loss: 0.00002823
Iteration 95/1000 | Loss: 0.00002823
Iteration 96/1000 | Loss: 0.00002823
Iteration 97/1000 | Loss: 0.00002823
Iteration 98/1000 | Loss: 0.00002822
Iteration 99/1000 | Loss: 0.00002822
Iteration 100/1000 | Loss: 0.00002822
Iteration 101/1000 | Loss: 0.00002822
Iteration 102/1000 | Loss: 0.00002822
Iteration 103/1000 | Loss: 0.00002822
Iteration 104/1000 | Loss: 0.00002822
Iteration 105/1000 | Loss: 0.00002822
Iteration 106/1000 | Loss: 0.00002822
Iteration 107/1000 | Loss: 0.00002822
Iteration 108/1000 | Loss: 0.00002821
Iteration 109/1000 | Loss: 0.00002821
Iteration 110/1000 | Loss: 0.00002820
Iteration 111/1000 | Loss: 0.00002820
Iteration 112/1000 | Loss: 0.00002819
Iteration 113/1000 | Loss: 0.00002819
Iteration 114/1000 | Loss: 0.00002819
Iteration 115/1000 | Loss: 0.00002819
Iteration 116/1000 | Loss: 0.00002819
Iteration 117/1000 | Loss: 0.00002819
Iteration 118/1000 | Loss: 0.00002819
Iteration 119/1000 | Loss: 0.00002818
Iteration 120/1000 | Loss: 0.00002818
Iteration 121/1000 | Loss: 0.00002818
Iteration 122/1000 | Loss: 0.00002818
Iteration 123/1000 | Loss: 0.00002818
Iteration 124/1000 | Loss: 0.00002818
Iteration 125/1000 | Loss: 0.00002817
Iteration 126/1000 | Loss: 0.00002817
Iteration 127/1000 | Loss: 0.00002817
Iteration 128/1000 | Loss: 0.00002817
Iteration 129/1000 | Loss: 0.00002817
Iteration 130/1000 | Loss: 0.00002816
Iteration 131/1000 | Loss: 0.00002816
Iteration 132/1000 | Loss: 0.00002816
Iteration 133/1000 | Loss: 0.00002816
Iteration 134/1000 | Loss: 0.00002816
Iteration 135/1000 | Loss: 0.00002816
Iteration 136/1000 | Loss: 0.00002815
Iteration 137/1000 | Loss: 0.00002815
Iteration 138/1000 | Loss: 0.00002815
Iteration 139/1000 | Loss: 0.00002815
Iteration 140/1000 | Loss: 0.00002814
Iteration 141/1000 | Loss: 0.00002814
Iteration 142/1000 | Loss: 0.00002814
Iteration 143/1000 | Loss: 0.00002814
Iteration 144/1000 | Loss: 0.00002814
Iteration 145/1000 | Loss: 0.00002814
Iteration 146/1000 | Loss: 0.00002814
Iteration 147/1000 | Loss: 0.00002814
Iteration 148/1000 | Loss: 0.00002814
Iteration 149/1000 | Loss: 0.00002814
Iteration 150/1000 | Loss: 0.00002813
Iteration 151/1000 | Loss: 0.00002813
Iteration 152/1000 | Loss: 0.00002813
Iteration 153/1000 | Loss: 0.00002813
Iteration 154/1000 | Loss: 0.00002813
Iteration 155/1000 | Loss: 0.00002812
Iteration 156/1000 | Loss: 0.00002812
Iteration 157/1000 | Loss: 0.00002812
Iteration 158/1000 | Loss: 0.00002811
Iteration 159/1000 | Loss: 0.00002811
Iteration 160/1000 | Loss: 0.00002811
Iteration 161/1000 | Loss: 0.00002811
Iteration 162/1000 | Loss: 0.00002811
Iteration 163/1000 | Loss: 0.00002810
Iteration 164/1000 | Loss: 0.00002810
Iteration 165/1000 | Loss: 0.00002810
Iteration 166/1000 | Loss: 0.00002810
Iteration 167/1000 | Loss: 0.00002810
Iteration 168/1000 | Loss: 0.00002810
Iteration 169/1000 | Loss: 0.00002810
Iteration 170/1000 | Loss: 0.00002810
Iteration 171/1000 | Loss: 0.00002810
Iteration 172/1000 | Loss: 0.00002810
Iteration 173/1000 | Loss: 0.00002810
Iteration 174/1000 | Loss: 0.00002810
Iteration 175/1000 | Loss: 0.00002810
Iteration 176/1000 | Loss: 0.00002809
Iteration 177/1000 | Loss: 0.00002809
Iteration 178/1000 | Loss: 0.00002809
Iteration 179/1000 | Loss: 0.00002809
Iteration 180/1000 | Loss: 0.00002809
Iteration 181/1000 | Loss: 0.00002808
Iteration 182/1000 | Loss: 0.00002808
Iteration 183/1000 | Loss: 0.00002808
Iteration 184/1000 | Loss: 0.00002808
Iteration 185/1000 | Loss: 0.00002808
Iteration 186/1000 | Loss: 0.00002808
Iteration 187/1000 | Loss: 0.00002808
Iteration 188/1000 | Loss: 0.00002808
Iteration 189/1000 | Loss: 0.00002808
Iteration 190/1000 | Loss: 0.00002808
Iteration 191/1000 | Loss: 0.00002808
Iteration 192/1000 | Loss: 0.00002808
Iteration 193/1000 | Loss: 0.00002808
Iteration 194/1000 | Loss: 0.00002808
Iteration 195/1000 | Loss: 0.00002808
Iteration 196/1000 | Loss: 0.00002808
Iteration 197/1000 | Loss: 0.00002808
Iteration 198/1000 | Loss: 0.00002808
Iteration 199/1000 | Loss: 0.00002807
Iteration 200/1000 | Loss: 0.00002807
Iteration 201/1000 | Loss: 0.00002807
Iteration 202/1000 | Loss: 0.00002807
Iteration 203/1000 | Loss: 0.00002806
Iteration 204/1000 | Loss: 0.00002806
Iteration 205/1000 | Loss: 0.00002806
Iteration 206/1000 | Loss: 0.00002806
Iteration 207/1000 | Loss: 0.00002806
Iteration 208/1000 | Loss: 0.00002806
Iteration 209/1000 | Loss: 0.00002806
Iteration 210/1000 | Loss: 0.00002806
Iteration 211/1000 | Loss: 0.00002806
Iteration 212/1000 | Loss: 0.00002806
Iteration 213/1000 | Loss: 0.00002806
Iteration 214/1000 | Loss: 0.00002806
Iteration 215/1000 | Loss: 0.00002806
Iteration 216/1000 | Loss: 0.00002806
Iteration 217/1000 | Loss: 0.00002806
Iteration 218/1000 | Loss: 0.00002806
Iteration 219/1000 | Loss: 0.00002806
Iteration 220/1000 | Loss: 0.00002806
Iteration 221/1000 | Loss: 0.00002806
Iteration 222/1000 | Loss: 0.00002806
Iteration 223/1000 | Loss: 0.00002806
Iteration 224/1000 | Loss: 0.00002806
Iteration 225/1000 | Loss: 0.00002806
Iteration 226/1000 | Loss: 0.00002806
Iteration 227/1000 | Loss: 0.00002806
Iteration 228/1000 | Loss: 0.00002806
Iteration 229/1000 | Loss: 0.00002806
Iteration 230/1000 | Loss: 0.00002806
Iteration 231/1000 | Loss: 0.00002806
Iteration 232/1000 | Loss: 0.00002806
Iteration 233/1000 | Loss: 0.00002806
Iteration 234/1000 | Loss: 0.00002806
Iteration 235/1000 | Loss: 0.00002806
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 235. Stopping optimization.
Last 5 losses: [2.805568146868609e-05, 2.805568146868609e-05, 2.805568146868609e-05, 2.805568146868609e-05, 2.805568146868609e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.805568146868609e-05

Optimization complete. Final v2v error: 4.177460670471191 mm

Highest mean error: 5.614850044250488 mm for frame 126

Lowest mean error: 3.248263120651245 mm for frame 0

Saving results

Total time: 53.554067611694336
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00482639
Iteration 2/25 | Loss: 0.00128701
Iteration 3/25 | Loss: 0.00116356
Iteration 4/25 | Loss: 0.00114443
Iteration 5/25 | Loss: 0.00113916
Iteration 6/25 | Loss: 0.00113832
Iteration 7/25 | Loss: 0.00113832
Iteration 8/25 | Loss: 0.00113832
Iteration 9/25 | Loss: 0.00113832
Iteration 10/25 | Loss: 0.00113832
Iteration 11/25 | Loss: 0.00113832
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011383177479729056, 0.0011383177479729056, 0.0011383177479729056, 0.0011383177479729056, 0.0011383177479729056]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011383177479729056

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36748981
Iteration 2/25 | Loss: 0.00079585
Iteration 3/25 | Loss: 0.00079585
Iteration 4/25 | Loss: 0.00079585
Iteration 5/25 | Loss: 0.00079585
Iteration 6/25 | Loss: 0.00079585
Iteration 7/25 | Loss: 0.00079585
Iteration 8/25 | Loss: 0.00079585
Iteration 9/25 | Loss: 0.00079585
Iteration 10/25 | Loss: 0.00079585
Iteration 11/25 | Loss: 0.00079585
Iteration 12/25 | Loss: 0.00079585
Iteration 13/25 | Loss: 0.00079585
Iteration 14/25 | Loss: 0.00079585
Iteration 15/25 | Loss: 0.00079585
Iteration 16/25 | Loss: 0.00079585
Iteration 17/25 | Loss: 0.00079585
Iteration 18/25 | Loss: 0.00079585
Iteration 19/25 | Loss: 0.00079585
Iteration 20/25 | Loss: 0.00079585
Iteration 21/25 | Loss: 0.00079585
Iteration 22/25 | Loss: 0.00079585
Iteration 23/25 | Loss: 0.00079585
Iteration 24/25 | Loss: 0.00079585
Iteration 25/25 | Loss: 0.00079585

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079585
Iteration 2/1000 | Loss: 0.00003714
Iteration 3/1000 | Loss: 0.00002704
Iteration 4/1000 | Loss: 0.00002399
Iteration 5/1000 | Loss: 0.00002280
Iteration 6/1000 | Loss: 0.00002208
Iteration 7/1000 | Loss: 0.00002152
Iteration 8/1000 | Loss: 0.00002102
Iteration 9/1000 | Loss: 0.00002069
Iteration 10/1000 | Loss: 0.00002048
Iteration 11/1000 | Loss: 0.00002019
Iteration 12/1000 | Loss: 0.00002003
Iteration 13/1000 | Loss: 0.00001980
Iteration 14/1000 | Loss: 0.00001973
Iteration 15/1000 | Loss: 0.00001968
Iteration 16/1000 | Loss: 0.00001964
Iteration 17/1000 | Loss: 0.00001963
Iteration 18/1000 | Loss: 0.00001955
Iteration 19/1000 | Loss: 0.00001949
Iteration 20/1000 | Loss: 0.00001947
Iteration 21/1000 | Loss: 0.00001943
Iteration 22/1000 | Loss: 0.00001941
Iteration 23/1000 | Loss: 0.00001939
Iteration 24/1000 | Loss: 0.00001933
Iteration 25/1000 | Loss: 0.00001931
Iteration 26/1000 | Loss: 0.00001931
Iteration 27/1000 | Loss: 0.00001930
Iteration 28/1000 | Loss: 0.00001930
Iteration 29/1000 | Loss: 0.00001930
Iteration 30/1000 | Loss: 0.00001929
Iteration 31/1000 | Loss: 0.00001929
Iteration 32/1000 | Loss: 0.00001928
Iteration 33/1000 | Loss: 0.00001928
Iteration 34/1000 | Loss: 0.00001927
Iteration 35/1000 | Loss: 0.00001927
Iteration 36/1000 | Loss: 0.00001927
Iteration 37/1000 | Loss: 0.00001927
Iteration 38/1000 | Loss: 0.00001927
Iteration 39/1000 | Loss: 0.00001927
Iteration 40/1000 | Loss: 0.00001927
Iteration 41/1000 | Loss: 0.00001927
Iteration 42/1000 | Loss: 0.00001927
Iteration 43/1000 | Loss: 0.00001926
Iteration 44/1000 | Loss: 0.00001926
Iteration 45/1000 | Loss: 0.00001925
Iteration 46/1000 | Loss: 0.00001925
Iteration 47/1000 | Loss: 0.00001925
Iteration 48/1000 | Loss: 0.00001924
Iteration 49/1000 | Loss: 0.00001924
Iteration 50/1000 | Loss: 0.00001924
Iteration 51/1000 | Loss: 0.00001924
Iteration 52/1000 | Loss: 0.00001924
Iteration 53/1000 | Loss: 0.00001924
Iteration 54/1000 | Loss: 0.00001924
Iteration 55/1000 | Loss: 0.00001924
Iteration 56/1000 | Loss: 0.00001924
Iteration 57/1000 | Loss: 0.00001924
Iteration 58/1000 | Loss: 0.00001923
Iteration 59/1000 | Loss: 0.00001923
Iteration 60/1000 | Loss: 0.00001923
Iteration 61/1000 | Loss: 0.00001923
Iteration 62/1000 | Loss: 0.00001923
Iteration 63/1000 | Loss: 0.00001923
Iteration 64/1000 | Loss: 0.00001923
Iteration 65/1000 | Loss: 0.00001922
Iteration 66/1000 | Loss: 0.00001922
Iteration 67/1000 | Loss: 0.00001922
Iteration 68/1000 | Loss: 0.00001922
Iteration 69/1000 | Loss: 0.00001922
Iteration 70/1000 | Loss: 0.00001921
Iteration 71/1000 | Loss: 0.00001921
Iteration 72/1000 | Loss: 0.00001921
Iteration 73/1000 | Loss: 0.00001921
Iteration 74/1000 | Loss: 0.00001921
Iteration 75/1000 | Loss: 0.00001921
Iteration 76/1000 | Loss: 0.00001921
Iteration 77/1000 | Loss: 0.00001921
Iteration 78/1000 | Loss: 0.00001921
Iteration 79/1000 | Loss: 0.00001921
Iteration 80/1000 | Loss: 0.00001921
Iteration 81/1000 | Loss: 0.00001920
Iteration 82/1000 | Loss: 0.00001920
Iteration 83/1000 | Loss: 0.00001920
Iteration 84/1000 | Loss: 0.00001919
Iteration 85/1000 | Loss: 0.00001919
Iteration 86/1000 | Loss: 0.00001919
Iteration 87/1000 | Loss: 0.00001919
Iteration 88/1000 | Loss: 0.00001919
Iteration 89/1000 | Loss: 0.00001918
Iteration 90/1000 | Loss: 0.00001918
Iteration 91/1000 | Loss: 0.00001918
Iteration 92/1000 | Loss: 0.00001918
Iteration 93/1000 | Loss: 0.00001918
Iteration 94/1000 | Loss: 0.00001918
Iteration 95/1000 | Loss: 0.00001918
Iteration 96/1000 | Loss: 0.00001918
Iteration 97/1000 | Loss: 0.00001918
Iteration 98/1000 | Loss: 0.00001917
Iteration 99/1000 | Loss: 0.00001917
Iteration 100/1000 | Loss: 0.00001917
Iteration 101/1000 | Loss: 0.00001917
Iteration 102/1000 | Loss: 0.00001917
Iteration 103/1000 | Loss: 0.00001916
Iteration 104/1000 | Loss: 0.00001916
Iteration 105/1000 | Loss: 0.00001916
Iteration 106/1000 | Loss: 0.00001916
Iteration 107/1000 | Loss: 0.00001916
Iteration 108/1000 | Loss: 0.00001916
Iteration 109/1000 | Loss: 0.00001916
Iteration 110/1000 | Loss: 0.00001916
Iteration 111/1000 | Loss: 0.00001916
Iteration 112/1000 | Loss: 0.00001915
Iteration 113/1000 | Loss: 0.00001915
Iteration 114/1000 | Loss: 0.00001915
Iteration 115/1000 | Loss: 0.00001915
Iteration 116/1000 | Loss: 0.00001915
Iteration 117/1000 | Loss: 0.00001915
Iteration 118/1000 | Loss: 0.00001914
Iteration 119/1000 | Loss: 0.00001914
Iteration 120/1000 | Loss: 0.00001914
Iteration 121/1000 | Loss: 0.00001914
Iteration 122/1000 | Loss: 0.00001914
Iteration 123/1000 | Loss: 0.00001914
Iteration 124/1000 | Loss: 0.00001914
Iteration 125/1000 | Loss: 0.00001914
Iteration 126/1000 | Loss: 0.00001914
Iteration 127/1000 | Loss: 0.00001914
Iteration 128/1000 | Loss: 0.00001914
Iteration 129/1000 | Loss: 0.00001914
Iteration 130/1000 | Loss: 0.00001914
Iteration 131/1000 | Loss: 0.00001914
Iteration 132/1000 | Loss: 0.00001914
Iteration 133/1000 | Loss: 0.00001914
Iteration 134/1000 | Loss: 0.00001914
Iteration 135/1000 | Loss: 0.00001914
Iteration 136/1000 | Loss: 0.00001914
Iteration 137/1000 | Loss: 0.00001914
Iteration 138/1000 | Loss: 0.00001914
Iteration 139/1000 | Loss: 0.00001914
Iteration 140/1000 | Loss: 0.00001914
Iteration 141/1000 | Loss: 0.00001914
Iteration 142/1000 | Loss: 0.00001914
Iteration 143/1000 | Loss: 0.00001914
Iteration 144/1000 | Loss: 0.00001914
Iteration 145/1000 | Loss: 0.00001914
Iteration 146/1000 | Loss: 0.00001914
Iteration 147/1000 | Loss: 0.00001914
Iteration 148/1000 | Loss: 0.00001914
Iteration 149/1000 | Loss: 0.00001914
Iteration 150/1000 | Loss: 0.00001914
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.9135755792376585e-05, 1.9135755792376585e-05, 1.9135755792376585e-05, 1.9135755792376585e-05, 1.9135755792376585e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9135755792376585e-05

Optimization complete. Final v2v error: 3.625839948654175 mm

Highest mean error: 5.1833906173706055 mm for frame 39

Lowest mean error: 3.324244260787964 mm for frame 20

Saving results

Total time: 40.75771689414978
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00748303
Iteration 2/25 | Loss: 0.00129590
Iteration 3/25 | Loss: 0.00113699
Iteration 4/25 | Loss: 0.00111886
Iteration 5/25 | Loss: 0.00111432
Iteration 6/25 | Loss: 0.00111432
Iteration 7/25 | Loss: 0.00111432
Iteration 8/25 | Loss: 0.00111432
Iteration 9/25 | Loss: 0.00111432
Iteration 10/25 | Loss: 0.00111432
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011143209412693977, 0.0011143209412693977, 0.0011143209412693977, 0.0011143209412693977, 0.0011143209412693977]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011143209412693977

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30462170
Iteration 2/25 | Loss: 0.00081954
Iteration 3/25 | Loss: 0.00081952
Iteration 4/25 | Loss: 0.00081952
Iteration 5/25 | Loss: 0.00081951
Iteration 6/25 | Loss: 0.00081951
Iteration 7/25 | Loss: 0.00081951
Iteration 8/25 | Loss: 0.00081951
Iteration 9/25 | Loss: 0.00081951
Iteration 10/25 | Loss: 0.00081951
Iteration 11/25 | Loss: 0.00081951
Iteration 12/25 | Loss: 0.00081951
Iteration 13/25 | Loss: 0.00081951
Iteration 14/25 | Loss: 0.00081951
Iteration 15/25 | Loss: 0.00081951
Iteration 16/25 | Loss: 0.00081951
Iteration 17/25 | Loss: 0.00081951
Iteration 18/25 | Loss: 0.00081951
Iteration 19/25 | Loss: 0.00081951
Iteration 20/25 | Loss: 0.00081951
Iteration 21/25 | Loss: 0.00081951
Iteration 22/25 | Loss: 0.00081951
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008195129921659827, 0.0008195129921659827, 0.0008195129921659827, 0.0008195129921659827, 0.0008195129921659827]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008195129921659827

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081951
Iteration 2/1000 | Loss: 0.00003072
Iteration 3/1000 | Loss: 0.00002210
Iteration 4/1000 | Loss: 0.00001780
Iteration 5/1000 | Loss: 0.00001673
Iteration 6/1000 | Loss: 0.00001584
Iteration 7/1000 | Loss: 0.00001522
Iteration 8/1000 | Loss: 0.00001487
Iteration 9/1000 | Loss: 0.00001478
Iteration 10/1000 | Loss: 0.00001453
Iteration 11/1000 | Loss: 0.00001427
Iteration 12/1000 | Loss: 0.00001419
Iteration 13/1000 | Loss: 0.00001418
Iteration 14/1000 | Loss: 0.00001417
Iteration 15/1000 | Loss: 0.00001416
Iteration 16/1000 | Loss: 0.00001416
Iteration 17/1000 | Loss: 0.00001415
Iteration 18/1000 | Loss: 0.00001415
Iteration 19/1000 | Loss: 0.00001415
Iteration 20/1000 | Loss: 0.00001415
Iteration 21/1000 | Loss: 0.00001415
Iteration 22/1000 | Loss: 0.00001414
Iteration 23/1000 | Loss: 0.00001414
Iteration 24/1000 | Loss: 0.00001413
Iteration 25/1000 | Loss: 0.00001410
Iteration 26/1000 | Loss: 0.00001408
Iteration 27/1000 | Loss: 0.00001408
Iteration 28/1000 | Loss: 0.00001408
Iteration 29/1000 | Loss: 0.00001401
Iteration 30/1000 | Loss: 0.00001401
Iteration 31/1000 | Loss: 0.00001401
Iteration 32/1000 | Loss: 0.00001400
Iteration 33/1000 | Loss: 0.00001400
Iteration 34/1000 | Loss: 0.00001400
Iteration 35/1000 | Loss: 0.00001400
Iteration 36/1000 | Loss: 0.00001400
Iteration 37/1000 | Loss: 0.00001400
Iteration 38/1000 | Loss: 0.00001399
Iteration 39/1000 | Loss: 0.00001392
Iteration 40/1000 | Loss: 0.00001383
Iteration 41/1000 | Loss: 0.00001383
Iteration 42/1000 | Loss: 0.00001383
Iteration 43/1000 | Loss: 0.00001379
Iteration 44/1000 | Loss: 0.00001379
Iteration 45/1000 | Loss: 0.00001378
Iteration 46/1000 | Loss: 0.00001377
Iteration 47/1000 | Loss: 0.00001377
Iteration 48/1000 | Loss: 0.00001376
Iteration 49/1000 | Loss: 0.00001376
Iteration 50/1000 | Loss: 0.00001376
Iteration 51/1000 | Loss: 0.00001368
Iteration 52/1000 | Loss: 0.00001366
Iteration 53/1000 | Loss: 0.00001360
Iteration 54/1000 | Loss: 0.00001359
Iteration 55/1000 | Loss: 0.00001358
Iteration 56/1000 | Loss: 0.00001358
Iteration 57/1000 | Loss: 0.00001358
Iteration 58/1000 | Loss: 0.00001358
Iteration 59/1000 | Loss: 0.00001358
Iteration 60/1000 | Loss: 0.00001358
Iteration 61/1000 | Loss: 0.00001358
Iteration 62/1000 | Loss: 0.00001358
Iteration 63/1000 | Loss: 0.00001358
Iteration 64/1000 | Loss: 0.00001357
Iteration 65/1000 | Loss: 0.00001357
Iteration 66/1000 | Loss: 0.00001357
Iteration 67/1000 | Loss: 0.00001357
Iteration 68/1000 | Loss: 0.00001357
Iteration 69/1000 | Loss: 0.00001357
Iteration 70/1000 | Loss: 0.00001357
Iteration 71/1000 | Loss: 0.00001357
Iteration 72/1000 | Loss: 0.00001357
Iteration 73/1000 | Loss: 0.00001356
Iteration 74/1000 | Loss: 0.00001356
Iteration 75/1000 | Loss: 0.00001356
Iteration 76/1000 | Loss: 0.00001356
Iteration 77/1000 | Loss: 0.00001355
Iteration 78/1000 | Loss: 0.00001355
Iteration 79/1000 | Loss: 0.00001354
Iteration 80/1000 | Loss: 0.00001354
Iteration 81/1000 | Loss: 0.00001354
Iteration 82/1000 | Loss: 0.00001354
Iteration 83/1000 | Loss: 0.00001354
Iteration 84/1000 | Loss: 0.00001354
Iteration 85/1000 | Loss: 0.00001354
Iteration 86/1000 | Loss: 0.00001354
Iteration 87/1000 | Loss: 0.00001353
Iteration 88/1000 | Loss: 0.00001352
Iteration 89/1000 | Loss: 0.00001352
Iteration 90/1000 | Loss: 0.00001351
Iteration 91/1000 | Loss: 0.00001351
Iteration 92/1000 | Loss: 0.00001351
Iteration 93/1000 | Loss: 0.00001351
Iteration 94/1000 | Loss: 0.00001351
Iteration 95/1000 | Loss: 0.00001351
Iteration 96/1000 | Loss: 0.00001351
Iteration 97/1000 | Loss: 0.00001351
Iteration 98/1000 | Loss: 0.00001351
Iteration 99/1000 | Loss: 0.00001351
Iteration 100/1000 | Loss: 0.00001351
Iteration 101/1000 | Loss: 0.00001350
Iteration 102/1000 | Loss: 0.00001350
Iteration 103/1000 | Loss: 0.00001350
Iteration 104/1000 | Loss: 0.00001350
Iteration 105/1000 | Loss: 0.00001350
Iteration 106/1000 | Loss: 0.00001350
Iteration 107/1000 | Loss: 0.00001350
Iteration 108/1000 | Loss: 0.00001349
Iteration 109/1000 | Loss: 0.00001349
Iteration 110/1000 | Loss: 0.00001349
Iteration 111/1000 | Loss: 0.00001349
Iteration 112/1000 | Loss: 0.00001349
Iteration 113/1000 | Loss: 0.00001349
Iteration 114/1000 | Loss: 0.00001348
Iteration 115/1000 | Loss: 0.00001348
Iteration 116/1000 | Loss: 0.00001348
Iteration 117/1000 | Loss: 0.00001348
Iteration 118/1000 | Loss: 0.00001348
Iteration 119/1000 | Loss: 0.00001348
Iteration 120/1000 | Loss: 0.00001347
Iteration 121/1000 | Loss: 0.00001347
Iteration 122/1000 | Loss: 0.00001347
Iteration 123/1000 | Loss: 0.00001347
Iteration 124/1000 | Loss: 0.00001347
Iteration 125/1000 | Loss: 0.00001347
Iteration 126/1000 | Loss: 0.00001347
Iteration 127/1000 | Loss: 0.00001347
Iteration 128/1000 | Loss: 0.00001347
Iteration 129/1000 | Loss: 0.00001347
Iteration 130/1000 | Loss: 0.00001346
Iteration 131/1000 | Loss: 0.00001346
Iteration 132/1000 | Loss: 0.00001346
Iteration 133/1000 | Loss: 0.00001346
Iteration 134/1000 | Loss: 0.00001345
Iteration 135/1000 | Loss: 0.00001345
Iteration 136/1000 | Loss: 0.00001345
Iteration 137/1000 | Loss: 0.00001345
Iteration 138/1000 | Loss: 0.00001345
Iteration 139/1000 | Loss: 0.00001344
Iteration 140/1000 | Loss: 0.00001344
Iteration 141/1000 | Loss: 0.00001344
Iteration 142/1000 | Loss: 0.00001344
Iteration 143/1000 | Loss: 0.00001344
Iteration 144/1000 | Loss: 0.00001344
Iteration 145/1000 | Loss: 0.00001344
Iteration 146/1000 | Loss: 0.00001343
Iteration 147/1000 | Loss: 0.00001343
Iteration 148/1000 | Loss: 0.00001343
Iteration 149/1000 | Loss: 0.00001343
Iteration 150/1000 | Loss: 0.00001343
Iteration 151/1000 | Loss: 0.00001343
Iteration 152/1000 | Loss: 0.00001343
Iteration 153/1000 | Loss: 0.00001342
Iteration 154/1000 | Loss: 0.00001342
Iteration 155/1000 | Loss: 0.00001342
Iteration 156/1000 | Loss: 0.00001342
Iteration 157/1000 | Loss: 0.00001342
Iteration 158/1000 | Loss: 0.00001342
Iteration 159/1000 | Loss: 0.00001342
Iteration 160/1000 | Loss: 0.00001342
Iteration 161/1000 | Loss: 0.00001342
Iteration 162/1000 | Loss: 0.00001342
Iteration 163/1000 | Loss: 0.00001342
Iteration 164/1000 | Loss: 0.00001342
Iteration 165/1000 | Loss: 0.00001342
Iteration 166/1000 | Loss: 0.00001342
Iteration 167/1000 | Loss: 0.00001342
Iteration 168/1000 | Loss: 0.00001342
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.341517963737715e-05, 1.341517963737715e-05, 1.341517963737715e-05, 1.341517963737715e-05, 1.341517963737715e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.341517963737715e-05

Optimization complete. Final v2v error: 3.1250579357147217 mm

Highest mean error: 3.546276569366455 mm for frame 210

Lowest mean error: 2.9052488803863525 mm for frame 41

Saving results

Total time: 45.15017867088318
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00413439
Iteration 2/25 | Loss: 0.00124269
Iteration 3/25 | Loss: 0.00114280
Iteration 4/25 | Loss: 0.00112886
Iteration 5/25 | Loss: 0.00112420
Iteration 6/25 | Loss: 0.00112356
Iteration 7/25 | Loss: 0.00112356
Iteration 8/25 | Loss: 0.00112356
Iteration 9/25 | Loss: 0.00112356
Iteration 10/25 | Loss: 0.00112356
Iteration 11/25 | Loss: 0.00112356
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011235583806410432, 0.0011235583806410432, 0.0011235583806410432, 0.0011235583806410432, 0.0011235583806410432]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011235583806410432

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38089859
Iteration 2/25 | Loss: 0.00084025
Iteration 3/25 | Loss: 0.00084025
Iteration 4/25 | Loss: 0.00084025
Iteration 5/25 | Loss: 0.00084025
Iteration 6/25 | Loss: 0.00084025
Iteration 7/25 | Loss: 0.00084025
Iteration 8/25 | Loss: 0.00084025
Iteration 9/25 | Loss: 0.00084025
Iteration 10/25 | Loss: 0.00084025
Iteration 11/25 | Loss: 0.00084025
Iteration 12/25 | Loss: 0.00084025
Iteration 13/25 | Loss: 0.00084025
Iteration 14/25 | Loss: 0.00084025
Iteration 15/25 | Loss: 0.00084025
Iteration 16/25 | Loss: 0.00084025
Iteration 17/25 | Loss: 0.00084025
Iteration 18/25 | Loss: 0.00084025
Iteration 19/25 | Loss: 0.00084025
Iteration 20/25 | Loss: 0.00084025
Iteration 21/25 | Loss: 0.00084025
Iteration 22/25 | Loss: 0.00084025
Iteration 23/25 | Loss: 0.00084025
Iteration 24/25 | Loss: 0.00084025
Iteration 25/25 | Loss: 0.00084025

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084025
Iteration 2/1000 | Loss: 0.00002649
Iteration 3/1000 | Loss: 0.00002047
Iteration 4/1000 | Loss: 0.00001910
Iteration 5/1000 | Loss: 0.00001836
Iteration 6/1000 | Loss: 0.00001771
Iteration 7/1000 | Loss: 0.00001725
Iteration 8/1000 | Loss: 0.00001685
Iteration 9/1000 | Loss: 0.00001656
Iteration 10/1000 | Loss: 0.00001632
Iteration 11/1000 | Loss: 0.00001618
Iteration 12/1000 | Loss: 0.00001614
Iteration 13/1000 | Loss: 0.00001613
Iteration 14/1000 | Loss: 0.00001612
Iteration 15/1000 | Loss: 0.00001611
Iteration 16/1000 | Loss: 0.00001610
Iteration 17/1000 | Loss: 0.00001608
Iteration 18/1000 | Loss: 0.00001607
Iteration 19/1000 | Loss: 0.00001605
Iteration 20/1000 | Loss: 0.00001604
Iteration 21/1000 | Loss: 0.00001603
Iteration 22/1000 | Loss: 0.00001589
Iteration 23/1000 | Loss: 0.00001586
Iteration 24/1000 | Loss: 0.00001585
Iteration 25/1000 | Loss: 0.00001584
Iteration 26/1000 | Loss: 0.00001583
Iteration 27/1000 | Loss: 0.00001583
Iteration 28/1000 | Loss: 0.00001583
Iteration 29/1000 | Loss: 0.00001582
Iteration 30/1000 | Loss: 0.00001582
Iteration 31/1000 | Loss: 0.00001581
Iteration 32/1000 | Loss: 0.00001580
Iteration 33/1000 | Loss: 0.00001577
Iteration 34/1000 | Loss: 0.00001569
Iteration 35/1000 | Loss: 0.00001567
Iteration 36/1000 | Loss: 0.00001567
Iteration 37/1000 | Loss: 0.00001566
Iteration 38/1000 | Loss: 0.00001566
Iteration 39/1000 | Loss: 0.00001565
Iteration 40/1000 | Loss: 0.00001565
Iteration 41/1000 | Loss: 0.00001565
Iteration 42/1000 | Loss: 0.00001565
Iteration 43/1000 | Loss: 0.00001564
Iteration 44/1000 | Loss: 0.00001564
Iteration 45/1000 | Loss: 0.00001562
Iteration 46/1000 | Loss: 0.00001561
Iteration 47/1000 | Loss: 0.00001561
Iteration 48/1000 | Loss: 0.00001561
Iteration 49/1000 | Loss: 0.00001561
Iteration 50/1000 | Loss: 0.00001561
Iteration 51/1000 | Loss: 0.00001561
Iteration 52/1000 | Loss: 0.00001560
Iteration 53/1000 | Loss: 0.00001559
Iteration 54/1000 | Loss: 0.00001558
Iteration 55/1000 | Loss: 0.00001557
Iteration 56/1000 | Loss: 0.00001557
Iteration 57/1000 | Loss: 0.00001557
Iteration 58/1000 | Loss: 0.00001556
Iteration 59/1000 | Loss: 0.00001556
Iteration 60/1000 | Loss: 0.00001555
Iteration 61/1000 | Loss: 0.00001555
Iteration 62/1000 | Loss: 0.00001555
Iteration 63/1000 | Loss: 0.00001554
Iteration 64/1000 | Loss: 0.00001554
Iteration 65/1000 | Loss: 0.00001553
Iteration 66/1000 | Loss: 0.00001553
Iteration 67/1000 | Loss: 0.00001552
Iteration 68/1000 | Loss: 0.00001552
Iteration 69/1000 | Loss: 0.00001552
Iteration 70/1000 | Loss: 0.00001551
Iteration 71/1000 | Loss: 0.00001551
Iteration 72/1000 | Loss: 0.00001549
Iteration 73/1000 | Loss: 0.00001549
Iteration 74/1000 | Loss: 0.00001548
Iteration 75/1000 | Loss: 0.00001548
Iteration 76/1000 | Loss: 0.00001548
Iteration 77/1000 | Loss: 0.00001547
Iteration 78/1000 | Loss: 0.00001547
Iteration 79/1000 | Loss: 0.00001547
Iteration 80/1000 | Loss: 0.00001547
Iteration 81/1000 | Loss: 0.00001546
Iteration 82/1000 | Loss: 0.00001546
Iteration 83/1000 | Loss: 0.00001545
Iteration 84/1000 | Loss: 0.00001545
Iteration 85/1000 | Loss: 0.00001545
Iteration 86/1000 | Loss: 0.00001545
Iteration 87/1000 | Loss: 0.00001544
Iteration 88/1000 | Loss: 0.00001544
Iteration 89/1000 | Loss: 0.00001544
Iteration 90/1000 | Loss: 0.00001544
Iteration 91/1000 | Loss: 0.00001544
Iteration 92/1000 | Loss: 0.00001544
Iteration 93/1000 | Loss: 0.00001544
Iteration 94/1000 | Loss: 0.00001544
Iteration 95/1000 | Loss: 0.00001544
Iteration 96/1000 | Loss: 0.00001543
Iteration 97/1000 | Loss: 0.00001543
Iteration 98/1000 | Loss: 0.00001543
Iteration 99/1000 | Loss: 0.00001543
Iteration 100/1000 | Loss: 0.00001543
Iteration 101/1000 | Loss: 0.00001542
Iteration 102/1000 | Loss: 0.00001542
Iteration 103/1000 | Loss: 0.00001542
Iteration 104/1000 | Loss: 0.00001542
Iteration 105/1000 | Loss: 0.00001542
Iteration 106/1000 | Loss: 0.00001541
Iteration 107/1000 | Loss: 0.00001541
Iteration 108/1000 | Loss: 0.00001541
Iteration 109/1000 | Loss: 0.00001541
Iteration 110/1000 | Loss: 0.00001540
Iteration 111/1000 | Loss: 0.00001540
Iteration 112/1000 | Loss: 0.00001540
Iteration 113/1000 | Loss: 0.00001540
Iteration 114/1000 | Loss: 0.00001540
Iteration 115/1000 | Loss: 0.00001540
Iteration 116/1000 | Loss: 0.00001540
Iteration 117/1000 | Loss: 0.00001540
Iteration 118/1000 | Loss: 0.00001540
Iteration 119/1000 | Loss: 0.00001540
Iteration 120/1000 | Loss: 0.00001540
Iteration 121/1000 | Loss: 0.00001540
Iteration 122/1000 | Loss: 0.00001540
Iteration 123/1000 | Loss: 0.00001539
Iteration 124/1000 | Loss: 0.00001539
Iteration 125/1000 | Loss: 0.00001539
Iteration 126/1000 | Loss: 0.00001539
Iteration 127/1000 | Loss: 0.00001539
Iteration 128/1000 | Loss: 0.00001539
Iteration 129/1000 | Loss: 0.00001539
Iteration 130/1000 | Loss: 0.00001539
Iteration 131/1000 | Loss: 0.00001539
Iteration 132/1000 | Loss: 0.00001539
Iteration 133/1000 | Loss: 0.00001539
Iteration 134/1000 | Loss: 0.00001539
Iteration 135/1000 | Loss: 0.00001539
Iteration 136/1000 | Loss: 0.00001539
Iteration 137/1000 | Loss: 0.00001539
Iteration 138/1000 | Loss: 0.00001539
Iteration 139/1000 | Loss: 0.00001539
Iteration 140/1000 | Loss: 0.00001539
Iteration 141/1000 | Loss: 0.00001539
Iteration 142/1000 | Loss: 0.00001539
Iteration 143/1000 | Loss: 0.00001539
Iteration 144/1000 | Loss: 0.00001539
Iteration 145/1000 | Loss: 0.00001539
Iteration 146/1000 | Loss: 0.00001539
Iteration 147/1000 | Loss: 0.00001539
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.5385434380732477e-05, 1.5385434380732477e-05, 1.5385434380732477e-05, 1.5385434380732477e-05, 1.5385434380732477e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5385434380732477e-05

Optimization complete. Final v2v error: 3.328881025314331 mm

Highest mean error: 3.7934765815734863 mm for frame 15

Lowest mean error: 3.0960514545440674 mm for frame 34

Saving results

Total time: 38.74424958229065
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01052301
Iteration 2/25 | Loss: 0.00215943
Iteration 3/25 | Loss: 0.00199982
Iteration 4/25 | Loss: 0.00157849
Iteration 5/25 | Loss: 0.00132424
Iteration 6/25 | Loss: 0.00124392
Iteration 7/25 | Loss: 0.00111575
Iteration 8/25 | Loss: 0.00109540
Iteration 9/25 | Loss: 0.00108840
Iteration 10/25 | Loss: 0.00108725
Iteration 11/25 | Loss: 0.00108680
Iteration 12/25 | Loss: 0.00108651
Iteration 13/25 | Loss: 0.00108639
Iteration 14/25 | Loss: 0.00108639
Iteration 15/25 | Loss: 0.00108638
Iteration 16/25 | Loss: 0.00108638
Iteration 17/25 | Loss: 0.00108637
Iteration 18/25 | Loss: 0.00108637
Iteration 19/25 | Loss: 0.00108637
Iteration 20/25 | Loss: 0.00108637
Iteration 21/25 | Loss: 0.00108637
Iteration 22/25 | Loss: 0.00108637
Iteration 23/25 | Loss: 0.00108636
Iteration 24/25 | Loss: 0.00108636
Iteration 25/25 | Loss: 0.00108636

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32099092
Iteration 2/25 | Loss: 0.00087772
Iteration 3/25 | Loss: 0.00087772
Iteration 4/25 | Loss: 0.00087772
Iteration 5/25 | Loss: 0.00087772
Iteration 6/25 | Loss: 0.00087772
Iteration 7/25 | Loss: 0.00087772
Iteration 8/25 | Loss: 0.00087772
Iteration 9/25 | Loss: 0.00087772
Iteration 10/25 | Loss: 0.00087772
Iteration 11/25 | Loss: 0.00087772
Iteration 12/25 | Loss: 0.00087772
Iteration 13/25 | Loss: 0.00087772
Iteration 14/25 | Loss: 0.00087772
Iteration 15/25 | Loss: 0.00087772
Iteration 16/25 | Loss: 0.00087772
Iteration 17/25 | Loss: 0.00087772
Iteration 18/25 | Loss: 0.00087772
Iteration 19/25 | Loss: 0.00087772
Iteration 20/25 | Loss: 0.00087772
Iteration 21/25 | Loss: 0.00087772
Iteration 22/25 | Loss: 0.00087772
Iteration 23/25 | Loss: 0.00087772
Iteration 24/25 | Loss: 0.00087772
Iteration 25/25 | Loss: 0.00087772

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087772
Iteration 2/1000 | Loss: 0.00002644
Iteration 3/1000 | Loss: 0.00001967
Iteration 4/1000 | Loss: 0.00001793
Iteration 5/1000 | Loss: 0.00001707
Iteration 6/1000 | Loss: 0.00001649
Iteration 7/1000 | Loss: 0.00001591
Iteration 8/1000 | Loss: 0.00001559
Iteration 9/1000 | Loss: 0.00001524
Iteration 10/1000 | Loss: 0.00050332
Iteration 11/1000 | Loss: 0.00001988
Iteration 12/1000 | Loss: 0.00001567
Iteration 13/1000 | Loss: 0.00001376
Iteration 14/1000 | Loss: 0.00001321
Iteration 15/1000 | Loss: 0.00001287
Iteration 16/1000 | Loss: 0.00001254
Iteration 17/1000 | Loss: 0.00001244
Iteration 18/1000 | Loss: 0.00001244
Iteration 19/1000 | Loss: 0.00001240
Iteration 20/1000 | Loss: 0.00001238
Iteration 21/1000 | Loss: 0.00001237
Iteration 22/1000 | Loss: 0.00001235
Iteration 23/1000 | Loss: 0.00001234
Iteration 24/1000 | Loss: 0.00001230
Iteration 25/1000 | Loss: 0.00001229
Iteration 26/1000 | Loss: 0.00001229
Iteration 27/1000 | Loss: 0.00001229
Iteration 28/1000 | Loss: 0.00001228
Iteration 29/1000 | Loss: 0.00001223
Iteration 30/1000 | Loss: 0.00001222
Iteration 31/1000 | Loss: 0.00001222
Iteration 32/1000 | Loss: 0.00001220
Iteration 33/1000 | Loss: 0.00001218
Iteration 34/1000 | Loss: 0.00001217
Iteration 35/1000 | Loss: 0.00001217
Iteration 36/1000 | Loss: 0.00001216
Iteration 37/1000 | Loss: 0.00001216
Iteration 38/1000 | Loss: 0.00001211
Iteration 39/1000 | Loss: 0.00001210
Iteration 40/1000 | Loss: 0.00001210
Iteration 41/1000 | Loss: 0.00001208
Iteration 42/1000 | Loss: 0.00001205
Iteration 43/1000 | Loss: 0.00001202
Iteration 44/1000 | Loss: 0.00001201
Iteration 45/1000 | Loss: 0.00001201
Iteration 46/1000 | Loss: 0.00001201
Iteration 47/1000 | Loss: 0.00001201
Iteration 48/1000 | Loss: 0.00001199
Iteration 49/1000 | Loss: 0.00001199
Iteration 50/1000 | Loss: 0.00001199
Iteration 51/1000 | Loss: 0.00001199
Iteration 52/1000 | Loss: 0.00001198
Iteration 53/1000 | Loss: 0.00001198
Iteration 54/1000 | Loss: 0.00001197
Iteration 55/1000 | Loss: 0.00001197
Iteration 56/1000 | Loss: 0.00001197
Iteration 57/1000 | Loss: 0.00001196
Iteration 58/1000 | Loss: 0.00001196
Iteration 59/1000 | Loss: 0.00001196
Iteration 60/1000 | Loss: 0.00001196
Iteration 61/1000 | Loss: 0.00001195
Iteration 62/1000 | Loss: 0.00001195
Iteration 63/1000 | Loss: 0.00001195
Iteration 64/1000 | Loss: 0.00001195
Iteration 65/1000 | Loss: 0.00001194
Iteration 66/1000 | Loss: 0.00001194
Iteration 67/1000 | Loss: 0.00001193
Iteration 68/1000 | Loss: 0.00001193
Iteration 69/1000 | Loss: 0.00001193
Iteration 70/1000 | Loss: 0.00001193
Iteration 71/1000 | Loss: 0.00001193
Iteration 72/1000 | Loss: 0.00001193
Iteration 73/1000 | Loss: 0.00001192
Iteration 74/1000 | Loss: 0.00001192
Iteration 75/1000 | Loss: 0.00001192
Iteration 76/1000 | Loss: 0.00001192
Iteration 77/1000 | Loss: 0.00001192
Iteration 78/1000 | Loss: 0.00001192
Iteration 79/1000 | Loss: 0.00001192
Iteration 80/1000 | Loss: 0.00001192
Iteration 81/1000 | Loss: 0.00001191
Iteration 82/1000 | Loss: 0.00001191
Iteration 83/1000 | Loss: 0.00001191
Iteration 84/1000 | Loss: 0.00001191
Iteration 85/1000 | Loss: 0.00001191
Iteration 86/1000 | Loss: 0.00001191
Iteration 87/1000 | Loss: 0.00001191
Iteration 88/1000 | Loss: 0.00001191
Iteration 89/1000 | Loss: 0.00001191
Iteration 90/1000 | Loss: 0.00001191
Iteration 91/1000 | Loss: 0.00001191
Iteration 92/1000 | Loss: 0.00001191
Iteration 93/1000 | Loss: 0.00001191
Iteration 94/1000 | Loss: 0.00001190
Iteration 95/1000 | Loss: 0.00001190
Iteration 96/1000 | Loss: 0.00001190
Iteration 97/1000 | Loss: 0.00001190
Iteration 98/1000 | Loss: 0.00001190
Iteration 99/1000 | Loss: 0.00001190
Iteration 100/1000 | Loss: 0.00001190
Iteration 101/1000 | Loss: 0.00001190
Iteration 102/1000 | Loss: 0.00001189
Iteration 103/1000 | Loss: 0.00001189
Iteration 104/1000 | Loss: 0.00001189
Iteration 105/1000 | Loss: 0.00001189
Iteration 106/1000 | Loss: 0.00001189
Iteration 107/1000 | Loss: 0.00001189
Iteration 108/1000 | Loss: 0.00001188
Iteration 109/1000 | Loss: 0.00001188
Iteration 110/1000 | Loss: 0.00001188
Iteration 111/1000 | Loss: 0.00001188
Iteration 112/1000 | Loss: 0.00001187
Iteration 113/1000 | Loss: 0.00001187
Iteration 114/1000 | Loss: 0.00001187
Iteration 115/1000 | Loss: 0.00001186
Iteration 116/1000 | Loss: 0.00001186
Iteration 117/1000 | Loss: 0.00001186
Iteration 118/1000 | Loss: 0.00001186
Iteration 119/1000 | Loss: 0.00001186
Iteration 120/1000 | Loss: 0.00001186
Iteration 121/1000 | Loss: 0.00001185
Iteration 122/1000 | Loss: 0.00001185
Iteration 123/1000 | Loss: 0.00001185
Iteration 124/1000 | Loss: 0.00001185
Iteration 125/1000 | Loss: 0.00001185
Iteration 126/1000 | Loss: 0.00001185
Iteration 127/1000 | Loss: 0.00001185
Iteration 128/1000 | Loss: 0.00001184
Iteration 129/1000 | Loss: 0.00001184
Iteration 130/1000 | Loss: 0.00001184
Iteration 131/1000 | Loss: 0.00001184
Iteration 132/1000 | Loss: 0.00001184
Iteration 133/1000 | Loss: 0.00001184
Iteration 134/1000 | Loss: 0.00001183
Iteration 135/1000 | Loss: 0.00001183
Iteration 136/1000 | Loss: 0.00001183
Iteration 137/1000 | Loss: 0.00001183
Iteration 138/1000 | Loss: 0.00001183
Iteration 139/1000 | Loss: 0.00001183
Iteration 140/1000 | Loss: 0.00001183
Iteration 141/1000 | Loss: 0.00001183
Iteration 142/1000 | Loss: 0.00001183
Iteration 143/1000 | Loss: 0.00001182
Iteration 144/1000 | Loss: 0.00001182
Iteration 145/1000 | Loss: 0.00001182
Iteration 146/1000 | Loss: 0.00001182
Iteration 147/1000 | Loss: 0.00001182
Iteration 148/1000 | Loss: 0.00001182
Iteration 149/1000 | Loss: 0.00001182
Iteration 150/1000 | Loss: 0.00001182
Iteration 151/1000 | Loss: 0.00001182
Iteration 152/1000 | Loss: 0.00001182
Iteration 153/1000 | Loss: 0.00001182
Iteration 154/1000 | Loss: 0.00001182
Iteration 155/1000 | Loss: 0.00001182
Iteration 156/1000 | Loss: 0.00001181
Iteration 157/1000 | Loss: 0.00001181
Iteration 158/1000 | Loss: 0.00001181
Iteration 159/1000 | Loss: 0.00001181
Iteration 160/1000 | Loss: 0.00001181
Iteration 161/1000 | Loss: 0.00001181
Iteration 162/1000 | Loss: 0.00001181
Iteration 163/1000 | Loss: 0.00001181
Iteration 164/1000 | Loss: 0.00001181
Iteration 165/1000 | Loss: 0.00001181
Iteration 166/1000 | Loss: 0.00001181
Iteration 167/1000 | Loss: 0.00001181
Iteration 168/1000 | Loss: 0.00001180
Iteration 169/1000 | Loss: 0.00001180
Iteration 170/1000 | Loss: 0.00001180
Iteration 171/1000 | Loss: 0.00001180
Iteration 172/1000 | Loss: 0.00001180
Iteration 173/1000 | Loss: 0.00001180
Iteration 174/1000 | Loss: 0.00001180
Iteration 175/1000 | Loss: 0.00001180
Iteration 176/1000 | Loss: 0.00001180
Iteration 177/1000 | Loss: 0.00001180
Iteration 178/1000 | Loss: 0.00001180
Iteration 179/1000 | Loss: 0.00001180
Iteration 180/1000 | Loss: 0.00001180
Iteration 181/1000 | Loss: 0.00001180
Iteration 182/1000 | Loss: 0.00001180
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [1.1796668331953697e-05, 1.1796668331953697e-05, 1.1796668331953697e-05, 1.1796668331953697e-05, 1.1796668331953697e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1796668331953697e-05

Optimization complete. Final v2v error: 2.934483289718628 mm

Highest mean error: 3.798590898513794 mm for frame 51

Lowest mean error: 2.7954952716827393 mm for frame 146

Saving results

Total time: 62.51464629173279
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00411822
Iteration 2/25 | Loss: 0.00149081
Iteration 3/25 | Loss: 0.00117178
Iteration 4/25 | Loss: 0.00112254
Iteration 5/25 | Loss: 0.00111514
Iteration 6/25 | Loss: 0.00111298
Iteration 7/25 | Loss: 0.00111261
Iteration 8/25 | Loss: 0.00111260
Iteration 9/25 | Loss: 0.00111260
Iteration 10/25 | Loss: 0.00111260
Iteration 11/25 | Loss: 0.00111260
Iteration 12/25 | Loss: 0.00111260
Iteration 13/25 | Loss: 0.00111260
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001112603466026485, 0.001112603466026485, 0.001112603466026485, 0.001112603466026485, 0.001112603466026485]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001112603466026485

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31952369
Iteration 2/25 | Loss: 0.00073726
Iteration 3/25 | Loss: 0.00073726
Iteration 4/25 | Loss: 0.00073726
Iteration 5/25 | Loss: 0.00073725
Iteration 6/25 | Loss: 0.00073725
Iteration 7/25 | Loss: 0.00073725
Iteration 8/25 | Loss: 0.00073725
Iteration 9/25 | Loss: 0.00073725
Iteration 10/25 | Loss: 0.00073725
Iteration 11/25 | Loss: 0.00073725
Iteration 12/25 | Loss: 0.00073725
Iteration 13/25 | Loss: 0.00073725
Iteration 14/25 | Loss: 0.00073725
Iteration 15/25 | Loss: 0.00073725
Iteration 16/25 | Loss: 0.00073725
Iteration 17/25 | Loss: 0.00073725
Iteration 18/25 | Loss: 0.00073725
Iteration 19/25 | Loss: 0.00073725
Iteration 20/25 | Loss: 0.00073725
Iteration 21/25 | Loss: 0.00073725
Iteration 22/25 | Loss: 0.00073725
Iteration 23/25 | Loss: 0.00073725
Iteration 24/25 | Loss: 0.00073725
Iteration 25/25 | Loss: 0.00073725

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073725
Iteration 2/1000 | Loss: 0.00004119
Iteration 3/1000 | Loss: 0.00002408
Iteration 4/1000 | Loss: 0.00001784
Iteration 5/1000 | Loss: 0.00001659
Iteration 6/1000 | Loss: 0.00001556
Iteration 7/1000 | Loss: 0.00001492
Iteration 8/1000 | Loss: 0.00001439
Iteration 9/1000 | Loss: 0.00001401
Iteration 10/1000 | Loss: 0.00001374
Iteration 11/1000 | Loss: 0.00001366
Iteration 12/1000 | Loss: 0.00001348
Iteration 13/1000 | Loss: 0.00001342
Iteration 14/1000 | Loss: 0.00001333
Iteration 15/1000 | Loss: 0.00001330
Iteration 16/1000 | Loss: 0.00001329
Iteration 17/1000 | Loss: 0.00001329
Iteration 18/1000 | Loss: 0.00001329
Iteration 19/1000 | Loss: 0.00001328
Iteration 20/1000 | Loss: 0.00001327
Iteration 21/1000 | Loss: 0.00001326
Iteration 22/1000 | Loss: 0.00001326
Iteration 23/1000 | Loss: 0.00001326
Iteration 24/1000 | Loss: 0.00001325
Iteration 25/1000 | Loss: 0.00001325
Iteration 26/1000 | Loss: 0.00001323
Iteration 27/1000 | Loss: 0.00001323
Iteration 28/1000 | Loss: 0.00001323
Iteration 29/1000 | Loss: 0.00001323
Iteration 30/1000 | Loss: 0.00001323
Iteration 31/1000 | Loss: 0.00001322
Iteration 32/1000 | Loss: 0.00001321
Iteration 33/1000 | Loss: 0.00001319
Iteration 34/1000 | Loss: 0.00001319
Iteration 35/1000 | Loss: 0.00001318
Iteration 36/1000 | Loss: 0.00001318
Iteration 37/1000 | Loss: 0.00001317
Iteration 38/1000 | Loss: 0.00001317
Iteration 39/1000 | Loss: 0.00001316
Iteration 40/1000 | Loss: 0.00001315
Iteration 41/1000 | Loss: 0.00001314
Iteration 42/1000 | Loss: 0.00001314
Iteration 43/1000 | Loss: 0.00001314
Iteration 44/1000 | Loss: 0.00001313
Iteration 45/1000 | Loss: 0.00001313
Iteration 46/1000 | Loss: 0.00001312
Iteration 47/1000 | Loss: 0.00001312
Iteration 48/1000 | Loss: 0.00001312
Iteration 49/1000 | Loss: 0.00001311
Iteration 50/1000 | Loss: 0.00001309
Iteration 51/1000 | Loss: 0.00001309
Iteration 52/1000 | Loss: 0.00001308
Iteration 53/1000 | Loss: 0.00001308
Iteration 54/1000 | Loss: 0.00001307
Iteration 55/1000 | Loss: 0.00001307
Iteration 56/1000 | Loss: 0.00001305
Iteration 57/1000 | Loss: 0.00001304
Iteration 58/1000 | Loss: 0.00001304
Iteration 59/1000 | Loss: 0.00001303
Iteration 60/1000 | Loss: 0.00001302
Iteration 61/1000 | Loss: 0.00001302
Iteration 62/1000 | Loss: 0.00001302
Iteration 63/1000 | Loss: 0.00001302
Iteration 64/1000 | Loss: 0.00001302
Iteration 65/1000 | Loss: 0.00001301
Iteration 66/1000 | Loss: 0.00001301
Iteration 67/1000 | Loss: 0.00001301
Iteration 68/1000 | Loss: 0.00001300
Iteration 69/1000 | Loss: 0.00001299
Iteration 70/1000 | Loss: 0.00001299
Iteration 71/1000 | Loss: 0.00001299
Iteration 72/1000 | Loss: 0.00001299
Iteration 73/1000 | Loss: 0.00001299
Iteration 74/1000 | Loss: 0.00001299
Iteration 75/1000 | Loss: 0.00001299
Iteration 76/1000 | Loss: 0.00001299
Iteration 77/1000 | Loss: 0.00001299
Iteration 78/1000 | Loss: 0.00001298
Iteration 79/1000 | Loss: 0.00001298
Iteration 80/1000 | Loss: 0.00001298
Iteration 81/1000 | Loss: 0.00001297
Iteration 82/1000 | Loss: 0.00001297
Iteration 83/1000 | Loss: 0.00001297
Iteration 84/1000 | Loss: 0.00001297
Iteration 85/1000 | Loss: 0.00001296
Iteration 86/1000 | Loss: 0.00001296
Iteration 87/1000 | Loss: 0.00001295
Iteration 88/1000 | Loss: 0.00001295
Iteration 89/1000 | Loss: 0.00001295
Iteration 90/1000 | Loss: 0.00001295
Iteration 91/1000 | Loss: 0.00001295
Iteration 92/1000 | Loss: 0.00001295
Iteration 93/1000 | Loss: 0.00001295
Iteration 94/1000 | Loss: 0.00001294
Iteration 95/1000 | Loss: 0.00001294
Iteration 96/1000 | Loss: 0.00001294
Iteration 97/1000 | Loss: 0.00001294
Iteration 98/1000 | Loss: 0.00001294
Iteration 99/1000 | Loss: 0.00001294
Iteration 100/1000 | Loss: 0.00001294
Iteration 101/1000 | Loss: 0.00001293
Iteration 102/1000 | Loss: 0.00001293
Iteration 103/1000 | Loss: 0.00001293
Iteration 104/1000 | Loss: 0.00001293
Iteration 105/1000 | Loss: 0.00001292
Iteration 106/1000 | Loss: 0.00001292
Iteration 107/1000 | Loss: 0.00001290
Iteration 108/1000 | Loss: 0.00001290
Iteration 109/1000 | Loss: 0.00001290
Iteration 110/1000 | Loss: 0.00001290
Iteration 111/1000 | Loss: 0.00001290
Iteration 112/1000 | Loss: 0.00001290
Iteration 113/1000 | Loss: 0.00001290
Iteration 114/1000 | Loss: 0.00001290
Iteration 115/1000 | Loss: 0.00001290
Iteration 116/1000 | Loss: 0.00001290
Iteration 117/1000 | Loss: 0.00001289
Iteration 118/1000 | Loss: 0.00001289
Iteration 119/1000 | Loss: 0.00001288
Iteration 120/1000 | Loss: 0.00001288
Iteration 121/1000 | Loss: 0.00001288
Iteration 122/1000 | Loss: 0.00001287
Iteration 123/1000 | Loss: 0.00001286
Iteration 124/1000 | Loss: 0.00001286
Iteration 125/1000 | Loss: 0.00001286
Iteration 126/1000 | Loss: 0.00001286
Iteration 127/1000 | Loss: 0.00001286
Iteration 128/1000 | Loss: 0.00001285
Iteration 129/1000 | Loss: 0.00001285
Iteration 130/1000 | Loss: 0.00001284
Iteration 131/1000 | Loss: 0.00001284
Iteration 132/1000 | Loss: 0.00001283
Iteration 133/1000 | Loss: 0.00001283
Iteration 134/1000 | Loss: 0.00001283
Iteration 135/1000 | Loss: 0.00001283
Iteration 136/1000 | Loss: 0.00001283
Iteration 137/1000 | Loss: 0.00001282
Iteration 138/1000 | Loss: 0.00001282
Iteration 139/1000 | Loss: 0.00001282
Iteration 140/1000 | Loss: 0.00001282
Iteration 141/1000 | Loss: 0.00001282
Iteration 142/1000 | Loss: 0.00001282
Iteration 143/1000 | Loss: 0.00001282
Iteration 144/1000 | Loss: 0.00001282
Iteration 145/1000 | Loss: 0.00001281
Iteration 146/1000 | Loss: 0.00001281
Iteration 147/1000 | Loss: 0.00001281
Iteration 148/1000 | Loss: 0.00001281
Iteration 149/1000 | Loss: 0.00001280
Iteration 150/1000 | Loss: 0.00001280
Iteration 151/1000 | Loss: 0.00001280
Iteration 152/1000 | Loss: 0.00001280
Iteration 153/1000 | Loss: 0.00001280
Iteration 154/1000 | Loss: 0.00001280
Iteration 155/1000 | Loss: 0.00001280
Iteration 156/1000 | Loss: 0.00001280
Iteration 157/1000 | Loss: 0.00001280
Iteration 158/1000 | Loss: 0.00001279
Iteration 159/1000 | Loss: 0.00001279
Iteration 160/1000 | Loss: 0.00001279
Iteration 161/1000 | Loss: 0.00001279
Iteration 162/1000 | Loss: 0.00001279
Iteration 163/1000 | Loss: 0.00001279
Iteration 164/1000 | Loss: 0.00001279
Iteration 165/1000 | Loss: 0.00001279
Iteration 166/1000 | Loss: 0.00001278
Iteration 167/1000 | Loss: 0.00001278
Iteration 168/1000 | Loss: 0.00001278
Iteration 169/1000 | Loss: 0.00001278
Iteration 170/1000 | Loss: 0.00001277
Iteration 171/1000 | Loss: 0.00001277
Iteration 172/1000 | Loss: 0.00001277
Iteration 173/1000 | Loss: 0.00001277
Iteration 174/1000 | Loss: 0.00001277
Iteration 175/1000 | Loss: 0.00001277
Iteration 176/1000 | Loss: 0.00001277
Iteration 177/1000 | Loss: 0.00001277
Iteration 178/1000 | Loss: 0.00001277
Iteration 179/1000 | Loss: 0.00001277
Iteration 180/1000 | Loss: 0.00001277
Iteration 181/1000 | Loss: 0.00001277
Iteration 182/1000 | Loss: 0.00001276
Iteration 183/1000 | Loss: 0.00001276
Iteration 184/1000 | Loss: 0.00001276
Iteration 185/1000 | Loss: 0.00001276
Iteration 186/1000 | Loss: 0.00001276
Iteration 187/1000 | Loss: 0.00001276
Iteration 188/1000 | Loss: 0.00001276
Iteration 189/1000 | Loss: 0.00001276
Iteration 190/1000 | Loss: 0.00001276
Iteration 191/1000 | Loss: 0.00001276
Iteration 192/1000 | Loss: 0.00001276
Iteration 193/1000 | Loss: 0.00001276
Iteration 194/1000 | Loss: 0.00001275
Iteration 195/1000 | Loss: 0.00001275
Iteration 196/1000 | Loss: 0.00001275
Iteration 197/1000 | Loss: 0.00001275
Iteration 198/1000 | Loss: 0.00001275
Iteration 199/1000 | Loss: 0.00001275
Iteration 200/1000 | Loss: 0.00001275
Iteration 201/1000 | Loss: 0.00001275
Iteration 202/1000 | Loss: 0.00001275
Iteration 203/1000 | Loss: 0.00001275
Iteration 204/1000 | Loss: 0.00001275
Iteration 205/1000 | Loss: 0.00001275
Iteration 206/1000 | Loss: 0.00001275
Iteration 207/1000 | Loss: 0.00001275
Iteration 208/1000 | Loss: 0.00001275
Iteration 209/1000 | Loss: 0.00001275
Iteration 210/1000 | Loss: 0.00001275
Iteration 211/1000 | Loss: 0.00001275
Iteration 212/1000 | Loss: 0.00001275
Iteration 213/1000 | Loss: 0.00001275
Iteration 214/1000 | Loss: 0.00001275
Iteration 215/1000 | Loss: 0.00001275
Iteration 216/1000 | Loss: 0.00001275
Iteration 217/1000 | Loss: 0.00001275
Iteration 218/1000 | Loss: 0.00001275
Iteration 219/1000 | Loss: 0.00001275
Iteration 220/1000 | Loss: 0.00001275
Iteration 221/1000 | Loss: 0.00001275
Iteration 222/1000 | Loss: 0.00001275
Iteration 223/1000 | Loss: 0.00001275
Iteration 224/1000 | Loss: 0.00001275
Iteration 225/1000 | Loss: 0.00001275
Iteration 226/1000 | Loss: 0.00001275
Iteration 227/1000 | Loss: 0.00001275
Iteration 228/1000 | Loss: 0.00001275
Iteration 229/1000 | Loss: 0.00001275
Iteration 230/1000 | Loss: 0.00001275
Iteration 231/1000 | Loss: 0.00001275
Iteration 232/1000 | Loss: 0.00001275
Iteration 233/1000 | Loss: 0.00001275
Iteration 234/1000 | Loss: 0.00001275
Iteration 235/1000 | Loss: 0.00001275
Iteration 236/1000 | Loss: 0.00001275
Iteration 237/1000 | Loss: 0.00001275
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 237. Stopping optimization.
Last 5 losses: [1.2752020666084718e-05, 1.2752020666084718e-05, 1.2752020666084718e-05, 1.2752020666084718e-05, 1.2752020666084718e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2752020666084718e-05

Optimization complete. Final v2v error: 3.040996551513672 mm

Highest mean error: 3.8443803787231445 mm for frame 72

Lowest mean error: 2.5795559883117676 mm for frame 13

Saving results

Total time: 43.76045536994934
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00466370
Iteration 2/25 | Loss: 0.00130914
Iteration 3/25 | Loss: 0.00112586
Iteration 4/25 | Loss: 0.00110611
Iteration 5/25 | Loss: 0.00110356
Iteration 6/25 | Loss: 0.00110281
Iteration 7/25 | Loss: 0.00110281
Iteration 8/25 | Loss: 0.00110281
Iteration 9/25 | Loss: 0.00110281
Iteration 10/25 | Loss: 0.00110281
Iteration 11/25 | Loss: 0.00110281
Iteration 12/25 | Loss: 0.00110281
Iteration 13/25 | Loss: 0.00110281
Iteration 14/25 | Loss: 0.00110281
Iteration 15/25 | Loss: 0.00110281
Iteration 16/25 | Loss: 0.00110281
Iteration 17/25 | Loss: 0.00110281
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011028111912310123, 0.0011028111912310123, 0.0011028111912310123, 0.0011028111912310123, 0.0011028111912310123]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011028111912310123

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35864627
Iteration 2/25 | Loss: 0.00077321
Iteration 3/25 | Loss: 0.00077320
Iteration 4/25 | Loss: 0.00077320
Iteration 5/25 | Loss: 0.00077320
Iteration 6/25 | Loss: 0.00077320
Iteration 7/25 | Loss: 0.00077320
Iteration 8/25 | Loss: 0.00077320
Iteration 9/25 | Loss: 0.00077320
Iteration 10/25 | Loss: 0.00077320
Iteration 11/25 | Loss: 0.00077320
Iteration 12/25 | Loss: 0.00077320
Iteration 13/25 | Loss: 0.00077320
Iteration 14/25 | Loss: 0.00077320
Iteration 15/25 | Loss: 0.00077320
Iteration 16/25 | Loss: 0.00077320
Iteration 17/25 | Loss: 0.00077320
Iteration 18/25 | Loss: 0.00077320
Iteration 19/25 | Loss: 0.00077320
Iteration 20/25 | Loss: 0.00077320
Iteration 21/25 | Loss: 0.00077320
Iteration 22/25 | Loss: 0.00077320
Iteration 23/25 | Loss: 0.00077320
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007732000085525215, 0.0007732000085525215, 0.0007732000085525215, 0.0007732000085525215, 0.0007732000085525215]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007732000085525215

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077320
Iteration 2/1000 | Loss: 0.00002279
Iteration 3/1000 | Loss: 0.00001524
Iteration 4/1000 | Loss: 0.00001352
Iteration 5/1000 | Loss: 0.00001286
Iteration 6/1000 | Loss: 0.00001234
Iteration 7/1000 | Loss: 0.00001192
Iteration 8/1000 | Loss: 0.00001162
Iteration 9/1000 | Loss: 0.00001140
Iteration 10/1000 | Loss: 0.00001117
Iteration 11/1000 | Loss: 0.00001113
Iteration 12/1000 | Loss: 0.00001108
Iteration 13/1000 | Loss: 0.00001107
Iteration 14/1000 | Loss: 0.00001100
Iteration 15/1000 | Loss: 0.00001096
Iteration 16/1000 | Loss: 0.00001093
Iteration 17/1000 | Loss: 0.00001092
Iteration 18/1000 | Loss: 0.00001091
Iteration 19/1000 | Loss: 0.00001088
Iteration 20/1000 | Loss: 0.00001088
Iteration 21/1000 | Loss: 0.00001087
Iteration 22/1000 | Loss: 0.00001086
Iteration 23/1000 | Loss: 0.00001086
Iteration 24/1000 | Loss: 0.00001082
Iteration 25/1000 | Loss: 0.00001082
Iteration 26/1000 | Loss: 0.00001081
Iteration 27/1000 | Loss: 0.00001080
Iteration 28/1000 | Loss: 0.00001080
Iteration 29/1000 | Loss: 0.00001080
Iteration 30/1000 | Loss: 0.00001080
Iteration 31/1000 | Loss: 0.00001080
Iteration 32/1000 | Loss: 0.00001079
Iteration 33/1000 | Loss: 0.00001079
Iteration 34/1000 | Loss: 0.00001079
Iteration 35/1000 | Loss: 0.00001078
Iteration 36/1000 | Loss: 0.00001078
Iteration 37/1000 | Loss: 0.00001078
Iteration 38/1000 | Loss: 0.00001078
Iteration 39/1000 | Loss: 0.00001077
Iteration 40/1000 | Loss: 0.00001077
Iteration 41/1000 | Loss: 0.00001076
Iteration 42/1000 | Loss: 0.00001076
Iteration 43/1000 | Loss: 0.00001076
Iteration 44/1000 | Loss: 0.00001075
Iteration 45/1000 | Loss: 0.00001075
Iteration 46/1000 | Loss: 0.00001075
Iteration 47/1000 | Loss: 0.00001075
Iteration 48/1000 | Loss: 0.00001074
Iteration 49/1000 | Loss: 0.00001074
Iteration 50/1000 | Loss: 0.00001073
Iteration 51/1000 | Loss: 0.00001073
Iteration 52/1000 | Loss: 0.00001073
Iteration 53/1000 | Loss: 0.00001073
Iteration 54/1000 | Loss: 0.00001072
Iteration 55/1000 | Loss: 0.00001072
Iteration 56/1000 | Loss: 0.00001072
Iteration 57/1000 | Loss: 0.00001072
Iteration 58/1000 | Loss: 0.00001072
Iteration 59/1000 | Loss: 0.00001071
Iteration 60/1000 | Loss: 0.00001071
Iteration 61/1000 | Loss: 0.00001071
Iteration 62/1000 | Loss: 0.00001071
Iteration 63/1000 | Loss: 0.00001070
Iteration 64/1000 | Loss: 0.00001070
Iteration 65/1000 | Loss: 0.00001070
Iteration 66/1000 | Loss: 0.00001069
Iteration 67/1000 | Loss: 0.00001069
Iteration 68/1000 | Loss: 0.00001068
Iteration 69/1000 | Loss: 0.00001068
Iteration 70/1000 | Loss: 0.00001068
Iteration 71/1000 | Loss: 0.00001068
Iteration 72/1000 | Loss: 0.00001068
Iteration 73/1000 | Loss: 0.00001067
Iteration 74/1000 | Loss: 0.00001067
Iteration 75/1000 | Loss: 0.00001067
Iteration 76/1000 | Loss: 0.00001066
Iteration 77/1000 | Loss: 0.00001065
Iteration 78/1000 | Loss: 0.00001065
Iteration 79/1000 | Loss: 0.00001065
Iteration 80/1000 | Loss: 0.00001065
Iteration 81/1000 | Loss: 0.00001065
Iteration 82/1000 | Loss: 0.00001065
Iteration 83/1000 | Loss: 0.00001065
Iteration 84/1000 | Loss: 0.00001065
Iteration 85/1000 | Loss: 0.00001065
Iteration 86/1000 | Loss: 0.00001065
Iteration 87/1000 | Loss: 0.00001065
Iteration 88/1000 | Loss: 0.00001065
Iteration 89/1000 | Loss: 0.00001064
Iteration 90/1000 | Loss: 0.00001064
Iteration 91/1000 | Loss: 0.00001063
Iteration 92/1000 | Loss: 0.00001063
Iteration 93/1000 | Loss: 0.00001063
Iteration 94/1000 | Loss: 0.00001062
Iteration 95/1000 | Loss: 0.00001062
Iteration 96/1000 | Loss: 0.00001062
Iteration 97/1000 | Loss: 0.00001062
Iteration 98/1000 | Loss: 0.00001062
Iteration 99/1000 | Loss: 0.00001062
Iteration 100/1000 | Loss: 0.00001062
Iteration 101/1000 | Loss: 0.00001061
Iteration 102/1000 | Loss: 0.00001061
Iteration 103/1000 | Loss: 0.00001061
Iteration 104/1000 | Loss: 0.00001061
Iteration 105/1000 | Loss: 0.00001060
Iteration 106/1000 | Loss: 0.00001060
Iteration 107/1000 | Loss: 0.00001060
Iteration 108/1000 | Loss: 0.00001060
Iteration 109/1000 | Loss: 0.00001060
Iteration 110/1000 | Loss: 0.00001060
Iteration 111/1000 | Loss: 0.00001059
Iteration 112/1000 | Loss: 0.00001059
Iteration 113/1000 | Loss: 0.00001059
Iteration 114/1000 | Loss: 0.00001059
Iteration 115/1000 | Loss: 0.00001059
Iteration 116/1000 | Loss: 0.00001059
Iteration 117/1000 | Loss: 0.00001059
Iteration 118/1000 | Loss: 0.00001059
Iteration 119/1000 | Loss: 0.00001059
Iteration 120/1000 | Loss: 0.00001059
Iteration 121/1000 | Loss: 0.00001058
Iteration 122/1000 | Loss: 0.00001058
Iteration 123/1000 | Loss: 0.00001058
Iteration 124/1000 | Loss: 0.00001058
Iteration 125/1000 | Loss: 0.00001058
Iteration 126/1000 | Loss: 0.00001058
Iteration 127/1000 | Loss: 0.00001058
Iteration 128/1000 | Loss: 0.00001057
Iteration 129/1000 | Loss: 0.00001057
Iteration 130/1000 | Loss: 0.00001057
Iteration 131/1000 | Loss: 0.00001057
Iteration 132/1000 | Loss: 0.00001057
Iteration 133/1000 | Loss: 0.00001057
Iteration 134/1000 | Loss: 0.00001057
Iteration 135/1000 | Loss: 0.00001057
Iteration 136/1000 | Loss: 0.00001056
Iteration 137/1000 | Loss: 0.00001056
Iteration 138/1000 | Loss: 0.00001056
Iteration 139/1000 | Loss: 0.00001056
Iteration 140/1000 | Loss: 0.00001056
Iteration 141/1000 | Loss: 0.00001055
Iteration 142/1000 | Loss: 0.00001055
Iteration 143/1000 | Loss: 0.00001055
Iteration 144/1000 | Loss: 0.00001055
Iteration 145/1000 | Loss: 0.00001055
Iteration 146/1000 | Loss: 0.00001055
Iteration 147/1000 | Loss: 0.00001054
Iteration 148/1000 | Loss: 0.00001054
Iteration 149/1000 | Loss: 0.00001054
Iteration 150/1000 | Loss: 0.00001054
Iteration 151/1000 | Loss: 0.00001054
Iteration 152/1000 | Loss: 0.00001054
Iteration 153/1000 | Loss: 0.00001054
Iteration 154/1000 | Loss: 0.00001054
Iteration 155/1000 | Loss: 0.00001054
Iteration 156/1000 | Loss: 0.00001054
Iteration 157/1000 | Loss: 0.00001054
Iteration 158/1000 | Loss: 0.00001053
Iteration 159/1000 | Loss: 0.00001053
Iteration 160/1000 | Loss: 0.00001053
Iteration 161/1000 | Loss: 0.00001053
Iteration 162/1000 | Loss: 0.00001053
Iteration 163/1000 | Loss: 0.00001053
Iteration 164/1000 | Loss: 0.00001052
Iteration 165/1000 | Loss: 0.00001052
Iteration 166/1000 | Loss: 0.00001052
Iteration 167/1000 | Loss: 0.00001052
Iteration 168/1000 | Loss: 0.00001052
Iteration 169/1000 | Loss: 0.00001052
Iteration 170/1000 | Loss: 0.00001052
Iteration 171/1000 | Loss: 0.00001051
Iteration 172/1000 | Loss: 0.00001051
Iteration 173/1000 | Loss: 0.00001050
Iteration 174/1000 | Loss: 0.00001050
Iteration 175/1000 | Loss: 0.00001050
Iteration 176/1000 | Loss: 0.00001050
Iteration 177/1000 | Loss: 0.00001050
Iteration 178/1000 | Loss: 0.00001050
Iteration 179/1000 | Loss: 0.00001050
Iteration 180/1000 | Loss: 0.00001049
Iteration 181/1000 | Loss: 0.00001049
Iteration 182/1000 | Loss: 0.00001049
Iteration 183/1000 | Loss: 0.00001049
Iteration 184/1000 | Loss: 0.00001049
Iteration 185/1000 | Loss: 0.00001049
Iteration 186/1000 | Loss: 0.00001049
Iteration 187/1000 | Loss: 0.00001049
Iteration 188/1000 | Loss: 0.00001049
Iteration 189/1000 | Loss: 0.00001049
Iteration 190/1000 | Loss: 0.00001048
Iteration 191/1000 | Loss: 0.00001048
Iteration 192/1000 | Loss: 0.00001048
Iteration 193/1000 | Loss: 0.00001048
Iteration 194/1000 | Loss: 0.00001048
Iteration 195/1000 | Loss: 0.00001048
Iteration 196/1000 | Loss: 0.00001048
Iteration 197/1000 | Loss: 0.00001048
Iteration 198/1000 | Loss: 0.00001048
Iteration 199/1000 | Loss: 0.00001048
Iteration 200/1000 | Loss: 0.00001048
Iteration 201/1000 | Loss: 0.00001047
Iteration 202/1000 | Loss: 0.00001047
Iteration 203/1000 | Loss: 0.00001047
Iteration 204/1000 | Loss: 0.00001047
Iteration 205/1000 | Loss: 0.00001047
Iteration 206/1000 | Loss: 0.00001047
Iteration 207/1000 | Loss: 0.00001047
Iteration 208/1000 | Loss: 0.00001047
Iteration 209/1000 | Loss: 0.00001047
Iteration 210/1000 | Loss: 0.00001046
Iteration 211/1000 | Loss: 0.00001046
Iteration 212/1000 | Loss: 0.00001046
Iteration 213/1000 | Loss: 0.00001046
Iteration 214/1000 | Loss: 0.00001046
Iteration 215/1000 | Loss: 0.00001046
Iteration 216/1000 | Loss: 0.00001046
Iteration 217/1000 | Loss: 0.00001046
Iteration 218/1000 | Loss: 0.00001046
Iteration 219/1000 | Loss: 0.00001046
Iteration 220/1000 | Loss: 0.00001046
Iteration 221/1000 | Loss: 0.00001046
Iteration 222/1000 | Loss: 0.00001046
Iteration 223/1000 | Loss: 0.00001046
Iteration 224/1000 | Loss: 0.00001046
Iteration 225/1000 | Loss: 0.00001046
Iteration 226/1000 | Loss: 0.00001046
Iteration 227/1000 | Loss: 0.00001046
Iteration 228/1000 | Loss: 0.00001046
Iteration 229/1000 | Loss: 0.00001046
Iteration 230/1000 | Loss: 0.00001046
Iteration 231/1000 | Loss: 0.00001046
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 231. Stopping optimization.
Last 5 losses: [1.0461200872668996e-05, 1.0461200872668996e-05, 1.0461200872668996e-05, 1.0461200872668996e-05, 1.0461200872668996e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0461200872668996e-05

Optimization complete. Final v2v error: 2.69852876663208 mm

Highest mean error: 3.518951177597046 mm for frame 76

Lowest mean error: 2.402029514312744 mm for frame 23

Saving results

Total time: 42.34927773475647
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00357113
Iteration 2/25 | Loss: 0.00109678
Iteration 3/25 | Loss: 0.00104149
Iteration 4/25 | Loss: 0.00103497
Iteration 5/25 | Loss: 0.00103334
Iteration 6/25 | Loss: 0.00103324
Iteration 7/25 | Loss: 0.00103324
Iteration 8/25 | Loss: 0.00103324
Iteration 9/25 | Loss: 0.00103324
Iteration 10/25 | Loss: 0.00103324
Iteration 11/25 | Loss: 0.00103324
Iteration 12/25 | Loss: 0.00103324
Iteration 13/25 | Loss: 0.00103324
Iteration 14/25 | Loss: 0.00103324
Iteration 15/25 | Loss: 0.00103324
Iteration 16/25 | Loss: 0.00103324
Iteration 17/25 | Loss: 0.00103324
Iteration 18/25 | Loss: 0.00103324
Iteration 19/25 | Loss: 0.00103324
Iteration 20/25 | Loss: 0.00103324
Iteration 21/25 | Loss: 0.00103324
Iteration 22/25 | Loss: 0.00103324
Iteration 23/25 | Loss: 0.00103324
Iteration 24/25 | Loss: 0.00103324
Iteration 25/25 | Loss: 0.00103324

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42402697
Iteration 2/25 | Loss: 0.00084230
Iteration 3/25 | Loss: 0.00084230
Iteration 4/25 | Loss: 0.00084230
Iteration 5/25 | Loss: 0.00084230
Iteration 6/25 | Loss: 0.00084230
Iteration 7/25 | Loss: 0.00084230
Iteration 8/25 | Loss: 0.00084230
Iteration 9/25 | Loss: 0.00084230
Iteration 10/25 | Loss: 0.00084230
Iteration 11/25 | Loss: 0.00084230
Iteration 12/25 | Loss: 0.00084230
Iteration 13/25 | Loss: 0.00084230
Iteration 14/25 | Loss: 0.00084230
Iteration 15/25 | Loss: 0.00084230
Iteration 16/25 | Loss: 0.00084230
Iteration 17/25 | Loss: 0.00084230
Iteration 18/25 | Loss: 0.00084230
Iteration 19/25 | Loss: 0.00084230
Iteration 20/25 | Loss: 0.00084230
Iteration 21/25 | Loss: 0.00084230
Iteration 22/25 | Loss: 0.00084230
Iteration 23/25 | Loss: 0.00084230
Iteration 24/25 | Loss: 0.00084230
Iteration 25/25 | Loss: 0.00084230

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084230
Iteration 2/1000 | Loss: 0.00001912
Iteration 3/1000 | Loss: 0.00001249
Iteration 4/1000 | Loss: 0.00000993
Iteration 5/1000 | Loss: 0.00000934
Iteration 6/1000 | Loss: 0.00000880
Iteration 7/1000 | Loss: 0.00000846
Iteration 8/1000 | Loss: 0.00000835
Iteration 9/1000 | Loss: 0.00000824
Iteration 10/1000 | Loss: 0.00000822
Iteration 11/1000 | Loss: 0.00000804
Iteration 12/1000 | Loss: 0.00000793
Iteration 13/1000 | Loss: 0.00000792
Iteration 14/1000 | Loss: 0.00000792
Iteration 15/1000 | Loss: 0.00000790
Iteration 16/1000 | Loss: 0.00000790
Iteration 17/1000 | Loss: 0.00000789
Iteration 18/1000 | Loss: 0.00000789
Iteration 19/1000 | Loss: 0.00000788
Iteration 20/1000 | Loss: 0.00000788
Iteration 21/1000 | Loss: 0.00000784
Iteration 22/1000 | Loss: 0.00000782
Iteration 23/1000 | Loss: 0.00000782
Iteration 24/1000 | Loss: 0.00000781
Iteration 25/1000 | Loss: 0.00000781
Iteration 26/1000 | Loss: 0.00000780
Iteration 27/1000 | Loss: 0.00000780
Iteration 28/1000 | Loss: 0.00000775
Iteration 29/1000 | Loss: 0.00000774
Iteration 30/1000 | Loss: 0.00000769
Iteration 31/1000 | Loss: 0.00000768
Iteration 32/1000 | Loss: 0.00000768
Iteration 33/1000 | Loss: 0.00000767
Iteration 34/1000 | Loss: 0.00000766
Iteration 35/1000 | Loss: 0.00000766
Iteration 36/1000 | Loss: 0.00000765
Iteration 37/1000 | Loss: 0.00000765
Iteration 38/1000 | Loss: 0.00000765
Iteration 39/1000 | Loss: 0.00000764
Iteration 40/1000 | Loss: 0.00000764
Iteration 41/1000 | Loss: 0.00000763
Iteration 42/1000 | Loss: 0.00000763
Iteration 43/1000 | Loss: 0.00000762
Iteration 44/1000 | Loss: 0.00000762
Iteration 45/1000 | Loss: 0.00000761
Iteration 46/1000 | Loss: 0.00000761
Iteration 47/1000 | Loss: 0.00000761
Iteration 48/1000 | Loss: 0.00000760
Iteration 49/1000 | Loss: 0.00000760
Iteration 50/1000 | Loss: 0.00000760
Iteration 51/1000 | Loss: 0.00000760
Iteration 52/1000 | Loss: 0.00000760
Iteration 53/1000 | Loss: 0.00000759
Iteration 54/1000 | Loss: 0.00000759
Iteration 55/1000 | Loss: 0.00000759
Iteration 56/1000 | Loss: 0.00000759
Iteration 57/1000 | Loss: 0.00000759
Iteration 58/1000 | Loss: 0.00000759
Iteration 59/1000 | Loss: 0.00000758
Iteration 60/1000 | Loss: 0.00000758
Iteration 61/1000 | Loss: 0.00000758
Iteration 62/1000 | Loss: 0.00000758
Iteration 63/1000 | Loss: 0.00000758
Iteration 64/1000 | Loss: 0.00000758
Iteration 65/1000 | Loss: 0.00000757
Iteration 66/1000 | Loss: 0.00000757
Iteration 67/1000 | Loss: 0.00000757
Iteration 68/1000 | Loss: 0.00000757
Iteration 69/1000 | Loss: 0.00000757
Iteration 70/1000 | Loss: 0.00000756
Iteration 71/1000 | Loss: 0.00000756
Iteration 72/1000 | Loss: 0.00000756
Iteration 73/1000 | Loss: 0.00000756
Iteration 74/1000 | Loss: 0.00000756
Iteration 75/1000 | Loss: 0.00000755
Iteration 76/1000 | Loss: 0.00000755
Iteration 77/1000 | Loss: 0.00000755
Iteration 78/1000 | Loss: 0.00000755
Iteration 79/1000 | Loss: 0.00000755
Iteration 80/1000 | Loss: 0.00000755
Iteration 81/1000 | Loss: 0.00000755
Iteration 82/1000 | Loss: 0.00000754
Iteration 83/1000 | Loss: 0.00000754
Iteration 84/1000 | Loss: 0.00000754
Iteration 85/1000 | Loss: 0.00000754
Iteration 86/1000 | Loss: 0.00000754
Iteration 87/1000 | Loss: 0.00000753
Iteration 88/1000 | Loss: 0.00000753
Iteration 89/1000 | Loss: 0.00000753
Iteration 90/1000 | Loss: 0.00000753
Iteration 91/1000 | Loss: 0.00000752
Iteration 92/1000 | Loss: 0.00000752
Iteration 93/1000 | Loss: 0.00000752
Iteration 94/1000 | Loss: 0.00000752
Iteration 95/1000 | Loss: 0.00000752
Iteration 96/1000 | Loss: 0.00000751
Iteration 97/1000 | Loss: 0.00000751
Iteration 98/1000 | Loss: 0.00000750
Iteration 99/1000 | Loss: 0.00000750
Iteration 100/1000 | Loss: 0.00000749
Iteration 101/1000 | Loss: 0.00000749
Iteration 102/1000 | Loss: 0.00000748
Iteration 103/1000 | Loss: 0.00000748
Iteration 104/1000 | Loss: 0.00000748
Iteration 105/1000 | Loss: 0.00000748
Iteration 106/1000 | Loss: 0.00000748
Iteration 107/1000 | Loss: 0.00000748
Iteration 108/1000 | Loss: 0.00000748
Iteration 109/1000 | Loss: 0.00000748
Iteration 110/1000 | Loss: 0.00000748
Iteration 111/1000 | Loss: 0.00000748
Iteration 112/1000 | Loss: 0.00000747
Iteration 113/1000 | Loss: 0.00000747
Iteration 114/1000 | Loss: 0.00000747
Iteration 115/1000 | Loss: 0.00000747
Iteration 116/1000 | Loss: 0.00000746
Iteration 117/1000 | Loss: 0.00000746
Iteration 118/1000 | Loss: 0.00000746
Iteration 119/1000 | Loss: 0.00000745
Iteration 120/1000 | Loss: 0.00000745
Iteration 121/1000 | Loss: 0.00000745
Iteration 122/1000 | Loss: 0.00000745
Iteration 123/1000 | Loss: 0.00000745
Iteration 124/1000 | Loss: 0.00000744
Iteration 125/1000 | Loss: 0.00000744
Iteration 126/1000 | Loss: 0.00000743
Iteration 127/1000 | Loss: 0.00000743
Iteration 128/1000 | Loss: 0.00000743
Iteration 129/1000 | Loss: 0.00000743
Iteration 130/1000 | Loss: 0.00000743
Iteration 131/1000 | Loss: 0.00000743
Iteration 132/1000 | Loss: 0.00000743
Iteration 133/1000 | Loss: 0.00000743
Iteration 134/1000 | Loss: 0.00000742
Iteration 135/1000 | Loss: 0.00000742
Iteration 136/1000 | Loss: 0.00000742
Iteration 137/1000 | Loss: 0.00000742
Iteration 138/1000 | Loss: 0.00000742
Iteration 139/1000 | Loss: 0.00000742
Iteration 140/1000 | Loss: 0.00000742
Iteration 141/1000 | Loss: 0.00000741
Iteration 142/1000 | Loss: 0.00000741
Iteration 143/1000 | Loss: 0.00000741
Iteration 144/1000 | Loss: 0.00000741
Iteration 145/1000 | Loss: 0.00000741
Iteration 146/1000 | Loss: 0.00000741
Iteration 147/1000 | Loss: 0.00000741
Iteration 148/1000 | Loss: 0.00000741
Iteration 149/1000 | Loss: 0.00000741
Iteration 150/1000 | Loss: 0.00000741
Iteration 151/1000 | Loss: 0.00000741
Iteration 152/1000 | Loss: 0.00000740
Iteration 153/1000 | Loss: 0.00000740
Iteration 154/1000 | Loss: 0.00000740
Iteration 155/1000 | Loss: 0.00000740
Iteration 156/1000 | Loss: 0.00000740
Iteration 157/1000 | Loss: 0.00000740
Iteration 158/1000 | Loss: 0.00000740
Iteration 159/1000 | Loss: 0.00000740
Iteration 160/1000 | Loss: 0.00000740
Iteration 161/1000 | Loss: 0.00000740
Iteration 162/1000 | Loss: 0.00000740
Iteration 163/1000 | Loss: 0.00000740
Iteration 164/1000 | Loss: 0.00000740
Iteration 165/1000 | Loss: 0.00000740
Iteration 166/1000 | Loss: 0.00000740
Iteration 167/1000 | Loss: 0.00000739
Iteration 168/1000 | Loss: 0.00000739
Iteration 169/1000 | Loss: 0.00000739
Iteration 170/1000 | Loss: 0.00000739
Iteration 171/1000 | Loss: 0.00000739
Iteration 172/1000 | Loss: 0.00000739
Iteration 173/1000 | Loss: 0.00000739
Iteration 174/1000 | Loss: 0.00000739
Iteration 175/1000 | Loss: 0.00000738
Iteration 176/1000 | Loss: 0.00000738
Iteration 177/1000 | Loss: 0.00000738
Iteration 178/1000 | Loss: 0.00000738
Iteration 179/1000 | Loss: 0.00000738
Iteration 180/1000 | Loss: 0.00000737
Iteration 181/1000 | Loss: 0.00000737
Iteration 182/1000 | Loss: 0.00000737
Iteration 183/1000 | Loss: 0.00000737
Iteration 184/1000 | Loss: 0.00000737
Iteration 185/1000 | Loss: 0.00000737
Iteration 186/1000 | Loss: 0.00000736
Iteration 187/1000 | Loss: 0.00000736
Iteration 188/1000 | Loss: 0.00000736
Iteration 189/1000 | Loss: 0.00000736
Iteration 190/1000 | Loss: 0.00000736
Iteration 191/1000 | Loss: 0.00000736
Iteration 192/1000 | Loss: 0.00000736
Iteration 193/1000 | Loss: 0.00000736
Iteration 194/1000 | Loss: 0.00000736
Iteration 195/1000 | Loss: 0.00000736
Iteration 196/1000 | Loss: 0.00000736
Iteration 197/1000 | Loss: 0.00000736
Iteration 198/1000 | Loss: 0.00000735
Iteration 199/1000 | Loss: 0.00000735
Iteration 200/1000 | Loss: 0.00000735
Iteration 201/1000 | Loss: 0.00000735
Iteration 202/1000 | Loss: 0.00000735
Iteration 203/1000 | Loss: 0.00000735
Iteration 204/1000 | Loss: 0.00000735
Iteration 205/1000 | Loss: 0.00000735
Iteration 206/1000 | Loss: 0.00000735
Iteration 207/1000 | Loss: 0.00000735
Iteration 208/1000 | Loss: 0.00000735
Iteration 209/1000 | Loss: 0.00000735
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [7.352763532253448e-06, 7.352763532253448e-06, 7.352763532253448e-06, 7.352763532253448e-06, 7.352763532253448e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.352763532253448e-06

Optimization complete. Final v2v error: 2.351416826248169 mm

Highest mean error: 2.4681334495544434 mm for frame 123

Lowest mean error: 2.306796073913574 mm for frame 153

Saving results

Total time: 39.280219316482544
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01045209
Iteration 2/25 | Loss: 0.01045209
Iteration 3/25 | Loss: 0.01045209
Iteration 4/25 | Loss: 0.01045209
Iteration 5/25 | Loss: 0.01045209
Iteration 6/25 | Loss: 0.01045209
Iteration 7/25 | Loss: 0.01045209
Iteration 8/25 | Loss: 0.01045209
Iteration 9/25 | Loss: 0.01045209
Iteration 10/25 | Loss: 0.01045209
Iteration 11/25 | Loss: 0.01045208
Iteration 12/25 | Loss: 0.01045208
Iteration 13/25 | Loss: 0.01045208
Iteration 14/25 | Loss: 0.01045208
Iteration 15/25 | Loss: 0.01045208
Iteration 16/25 | Loss: 0.01045208
Iteration 17/25 | Loss: 0.01045208
Iteration 18/25 | Loss: 0.01045208
Iteration 19/25 | Loss: 0.01045207
Iteration 20/25 | Loss: 0.01045207
Iteration 21/25 | Loss: 0.01045207
Iteration 22/25 | Loss: 0.01045207
Iteration 23/25 | Loss: 0.01045207
Iteration 24/25 | Loss: 0.01045207
Iteration 25/25 | Loss: 0.01045207

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.69463980
Iteration 2/25 | Loss: 0.09542163
Iteration 3/25 | Loss: 0.09224225
Iteration 4/25 | Loss: 0.09172998
Iteration 5/25 | Loss: 0.09042371
Iteration 6/25 | Loss: 0.09042371
Iteration 7/25 | Loss: 0.09042371
Iteration 8/25 | Loss: 0.09042371
Iteration 9/25 | Loss: 0.09042370
Iteration 10/25 | Loss: 0.09042370
Iteration 11/25 | Loss: 0.09042370
Iteration 12/25 | Loss: 0.09042370
Iteration 13/25 | Loss: 0.09042370
Iteration 14/25 | Loss: 0.09042370
Iteration 15/25 | Loss: 0.09042370
Iteration 16/25 | Loss: 0.09042370
Iteration 17/25 | Loss: 0.09042370
Iteration 18/25 | Loss: 0.09042370
Iteration 19/25 | Loss: 0.09042370
Iteration 20/25 | Loss: 0.09042370
Iteration 21/25 | Loss: 0.09042370
Iteration 22/25 | Loss: 0.09042370
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.09042369574308395, 0.09042369574308395, 0.09042369574308395, 0.09042369574308395, 0.09042369574308395]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.09042369574308395

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.09042370
Iteration 2/1000 | Loss: 0.00414877
Iteration 3/1000 | Loss: 0.00133973
Iteration 4/1000 | Loss: 0.00083910
Iteration 5/1000 | Loss: 0.00082409
Iteration 6/1000 | Loss: 0.00045637
Iteration 7/1000 | Loss: 0.00017347
Iteration 8/1000 | Loss: 0.00016255
Iteration 9/1000 | Loss: 0.00004794
Iteration 10/1000 | Loss: 0.00008421
Iteration 11/1000 | Loss: 0.00012721
Iteration 12/1000 | Loss: 0.00044580
Iteration 13/1000 | Loss: 0.00102254
Iteration 14/1000 | Loss: 0.00003553
Iteration 15/1000 | Loss: 0.00002869
Iteration 16/1000 | Loss: 0.00006032
Iteration 17/1000 | Loss: 0.00017139
Iteration 18/1000 | Loss: 0.00012052
Iteration 19/1000 | Loss: 0.00075910
Iteration 20/1000 | Loss: 0.00016645
Iteration 21/1000 | Loss: 0.00108138
Iteration 22/1000 | Loss: 0.00002494
Iteration 23/1000 | Loss: 0.00017226
Iteration 24/1000 | Loss: 0.00014413
Iteration 25/1000 | Loss: 0.00004389
Iteration 26/1000 | Loss: 0.00034293
Iteration 27/1000 | Loss: 0.00003291
Iteration 28/1000 | Loss: 0.00006125
Iteration 29/1000 | Loss: 0.00003288
Iteration 30/1000 | Loss: 0.00005289
Iteration 31/1000 | Loss: 0.00007400
Iteration 32/1000 | Loss: 0.00001898
Iteration 33/1000 | Loss: 0.00005848
Iteration 34/1000 | Loss: 0.00005621
Iteration 35/1000 | Loss: 0.00020238
Iteration 36/1000 | Loss: 0.00001550
Iteration 37/1000 | Loss: 0.00006540
Iteration 38/1000 | Loss: 0.00001730
Iteration 39/1000 | Loss: 0.00001436
Iteration 40/1000 | Loss: 0.00001962
Iteration 41/1000 | Loss: 0.00016124
Iteration 42/1000 | Loss: 0.00001410
Iteration 43/1000 | Loss: 0.00001383
Iteration 44/1000 | Loss: 0.00002386
Iteration 45/1000 | Loss: 0.00002239
Iteration 46/1000 | Loss: 0.00001312
Iteration 47/1000 | Loss: 0.00005292
Iteration 48/1000 | Loss: 0.00002782
Iteration 49/1000 | Loss: 0.00004696
Iteration 50/1000 | Loss: 0.00002001
Iteration 51/1000 | Loss: 0.00012251
Iteration 52/1000 | Loss: 0.00001244
Iteration 53/1000 | Loss: 0.00001232
Iteration 54/1000 | Loss: 0.00001230
Iteration 55/1000 | Loss: 0.00001218
Iteration 56/1000 | Loss: 0.00001213
Iteration 57/1000 | Loss: 0.00015143
Iteration 58/1000 | Loss: 0.00041328
Iteration 59/1000 | Loss: 0.00005665
Iteration 60/1000 | Loss: 0.00003410
Iteration 61/1000 | Loss: 0.00001216
Iteration 62/1000 | Loss: 0.00001707
Iteration 63/1000 | Loss: 0.00012942
Iteration 64/1000 | Loss: 0.00006630
Iteration 65/1000 | Loss: 0.00003012
Iteration 66/1000 | Loss: 0.00001285
Iteration 67/1000 | Loss: 0.00001191
Iteration 68/1000 | Loss: 0.00001187
Iteration 69/1000 | Loss: 0.00001187
Iteration 70/1000 | Loss: 0.00001187
Iteration 71/1000 | Loss: 0.00001186
Iteration 72/1000 | Loss: 0.00001186
Iteration 73/1000 | Loss: 0.00001186
Iteration 74/1000 | Loss: 0.00001186
Iteration 75/1000 | Loss: 0.00001185
Iteration 76/1000 | Loss: 0.00001185
Iteration 77/1000 | Loss: 0.00001184
Iteration 78/1000 | Loss: 0.00001183
Iteration 79/1000 | Loss: 0.00001183
Iteration 80/1000 | Loss: 0.00003017
Iteration 81/1000 | Loss: 0.00001189
Iteration 82/1000 | Loss: 0.00001181
Iteration 83/1000 | Loss: 0.00001180
Iteration 84/1000 | Loss: 0.00001179
Iteration 85/1000 | Loss: 0.00001179
Iteration 86/1000 | Loss: 0.00001178
Iteration 87/1000 | Loss: 0.00001178
Iteration 88/1000 | Loss: 0.00001178
Iteration 89/1000 | Loss: 0.00001178
Iteration 90/1000 | Loss: 0.00001178
Iteration 91/1000 | Loss: 0.00001178
Iteration 92/1000 | Loss: 0.00001178
Iteration 93/1000 | Loss: 0.00001178
Iteration 94/1000 | Loss: 0.00001178
Iteration 95/1000 | Loss: 0.00001177
Iteration 96/1000 | Loss: 0.00001177
Iteration 97/1000 | Loss: 0.00001176
Iteration 98/1000 | Loss: 0.00001176
Iteration 99/1000 | Loss: 0.00001176
Iteration 100/1000 | Loss: 0.00001176
Iteration 101/1000 | Loss: 0.00001176
Iteration 102/1000 | Loss: 0.00001175
Iteration 103/1000 | Loss: 0.00001175
Iteration 104/1000 | Loss: 0.00001175
Iteration 105/1000 | Loss: 0.00001175
Iteration 106/1000 | Loss: 0.00001175
Iteration 107/1000 | Loss: 0.00001175
Iteration 108/1000 | Loss: 0.00001174
Iteration 109/1000 | Loss: 0.00001174
Iteration 110/1000 | Loss: 0.00001174
Iteration 111/1000 | Loss: 0.00001174
Iteration 112/1000 | Loss: 0.00001173
Iteration 113/1000 | Loss: 0.00001173
Iteration 114/1000 | Loss: 0.00001173
Iteration 115/1000 | Loss: 0.00001173
Iteration 116/1000 | Loss: 0.00001173
Iteration 117/1000 | Loss: 0.00001171
Iteration 118/1000 | Loss: 0.00001171
Iteration 119/1000 | Loss: 0.00001171
Iteration 120/1000 | Loss: 0.00001171
Iteration 121/1000 | Loss: 0.00001171
Iteration 122/1000 | Loss: 0.00001171
Iteration 123/1000 | Loss: 0.00001171
Iteration 124/1000 | Loss: 0.00001171
Iteration 125/1000 | Loss: 0.00001170
Iteration 126/1000 | Loss: 0.00001170
Iteration 127/1000 | Loss: 0.00001170
Iteration 128/1000 | Loss: 0.00001170
Iteration 129/1000 | Loss: 0.00001170
Iteration 130/1000 | Loss: 0.00005461
Iteration 131/1000 | Loss: 0.00001172
Iteration 132/1000 | Loss: 0.00001169
Iteration 133/1000 | Loss: 0.00001169
Iteration 134/1000 | Loss: 0.00001166
Iteration 135/1000 | Loss: 0.00006827
Iteration 136/1000 | Loss: 0.00003450
Iteration 137/1000 | Loss: 0.00002297
Iteration 138/1000 | Loss: 0.00001163
Iteration 139/1000 | Loss: 0.00001163
Iteration 140/1000 | Loss: 0.00001163
Iteration 141/1000 | Loss: 0.00001163
Iteration 142/1000 | Loss: 0.00001163
Iteration 143/1000 | Loss: 0.00001163
Iteration 144/1000 | Loss: 0.00001163
Iteration 145/1000 | Loss: 0.00001162
Iteration 146/1000 | Loss: 0.00001162
Iteration 147/1000 | Loss: 0.00001162
Iteration 148/1000 | Loss: 0.00001162
Iteration 149/1000 | Loss: 0.00001162
Iteration 150/1000 | Loss: 0.00001162
Iteration 151/1000 | Loss: 0.00001162
Iteration 152/1000 | Loss: 0.00001162
Iteration 153/1000 | Loss: 0.00001161
Iteration 154/1000 | Loss: 0.00001161
Iteration 155/1000 | Loss: 0.00001161
Iteration 156/1000 | Loss: 0.00001161
Iteration 157/1000 | Loss: 0.00001161
Iteration 158/1000 | Loss: 0.00001161
Iteration 159/1000 | Loss: 0.00001161
Iteration 160/1000 | Loss: 0.00001161
Iteration 161/1000 | Loss: 0.00001161
Iteration 162/1000 | Loss: 0.00001161
Iteration 163/1000 | Loss: 0.00001161
Iteration 164/1000 | Loss: 0.00001161
Iteration 165/1000 | Loss: 0.00001161
Iteration 166/1000 | Loss: 0.00001161
Iteration 167/1000 | Loss: 0.00001161
Iteration 168/1000 | Loss: 0.00001161
Iteration 169/1000 | Loss: 0.00001161
Iteration 170/1000 | Loss: 0.00001161
Iteration 171/1000 | Loss: 0.00001161
Iteration 172/1000 | Loss: 0.00001161
Iteration 173/1000 | Loss: 0.00001161
Iteration 174/1000 | Loss: 0.00001161
Iteration 175/1000 | Loss: 0.00001161
Iteration 176/1000 | Loss: 0.00001161
Iteration 177/1000 | Loss: 0.00001161
Iteration 178/1000 | Loss: 0.00001161
Iteration 179/1000 | Loss: 0.00001161
Iteration 180/1000 | Loss: 0.00001161
Iteration 181/1000 | Loss: 0.00001161
Iteration 182/1000 | Loss: 0.00001161
Iteration 183/1000 | Loss: 0.00001161
Iteration 184/1000 | Loss: 0.00001161
Iteration 185/1000 | Loss: 0.00001161
Iteration 186/1000 | Loss: 0.00001161
Iteration 187/1000 | Loss: 0.00001161
Iteration 188/1000 | Loss: 0.00001161
Iteration 189/1000 | Loss: 0.00001161
Iteration 190/1000 | Loss: 0.00001161
Iteration 191/1000 | Loss: 0.00001161
Iteration 192/1000 | Loss: 0.00001161
Iteration 193/1000 | Loss: 0.00001161
Iteration 194/1000 | Loss: 0.00001161
Iteration 195/1000 | Loss: 0.00001161
Iteration 196/1000 | Loss: 0.00001161
Iteration 197/1000 | Loss: 0.00001161
Iteration 198/1000 | Loss: 0.00001161
Iteration 199/1000 | Loss: 0.00001161
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [1.1608123713813256e-05, 1.1608123713813256e-05, 1.1608123713813256e-05, 1.1608123713813256e-05, 1.1608123713813256e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1608123713813256e-05

Optimization complete. Final v2v error: 2.9015233516693115 mm

Highest mean error: 3.4293034076690674 mm for frame 235

Lowest mean error: 2.3543100357055664 mm for frame 150

Saving results

Total time: 126.90455985069275
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00486466
Iteration 2/25 | Loss: 0.00118612
Iteration 3/25 | Loss: 0.00110531
Iteration 4/25 | Loss: 0.00109429
Iteration 5/25 | Loss: 0.00109105
Iteration 6/25 | Loss: 0.00109001
Iteration 7/25 | Loss: 0.00108992
Iteration 8/25 | Loss: 0.00108992
Iteration 9/25 | Loss: 0.00108992
Iteration 10/25 | Loss: 0.00108992
Iteration 11/25 | Loss: 0.00108992
Iteration 12/25 | Loss: 0.00108992
Iteration 13/25 | Loss: 0.00108992
Iteration 14/25 | Loss: 0.00108992
Iteration 15/25 | Loss: 0.00108992
Iteration 16/25 | Loss: 0.00108992
Iteration 17/25 | Loss: 0.00108992
Iteration 18/25 | Loss: 0.00108992
Iteration 19/25 | Loss: 0.00108992
Iteration 20/25 | Loss: 0.00108992
Iteration 21/25 | Loss: 0.00108992
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0010899242479354143, 0.0010899242479354143, 0.0010899242479354143, 0.0010899242479354143, 0.0010899242479354143]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010899242479354143

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35738218
Iteration 2/25 | Loss: 0.00084950
Iteration 3/25 | Loss: 0.00084946
Iteration 4/25 | Loss: 0.00084946
Iteration 5/25 | Loss: 0.00084946
Iteration 6/25 | Loss: 0.00084946
Iteration 7/25 | Loss: 0.00084946
Iteration 8/25 | Loss: 0.00084946
Iteration 9/25 | Loss: 0.00084946
Iteration 10/25 | Loss: 0.00084946
Iteration 11/25 | Loss: 0.00084946
Iteration 12/25 | Loss: 0.00084946
Iteration 13/25 | Loss: 0.00084946
Iteration 14/25 | Loss: 0.00084946
Iteration 15/25 | Loss: 0.00084946
Iteration 16/25 | Loss: 0.00084946
Iteration 17/25 | Loss: 0.00084946
Iteration 18/25 | Loss: 0.00084946
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008494580397382379, 0.0008494580397382379, 0.0008494580397382379, 0.0008494580397382379, 0.0008494580397382379]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008494580397382379

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084946
Iteration 2/1000 | Loss: 0.00002904
Iteration 3/1000 | Loss: 0.00001783
Iteration 4/1000 | Loss: 0.00001356
Iteration 5/1000 | Loss: 0.00001265
Iteration 6/1000 | Loss: 0.00001185
Iteration 7/1000 | Loss: 0.00001148
Iteration 8/1000 | Loss: 0.00001114
Iteration 9/1000 | Loss: 0.00001094
Iteration 10/1000 | Loss: 0.00001087
Iteration 11/1000 | Loss: 0.00001078
Iteration 12/1000 | Loss: 0.00001074
Iteration 13/1000 | Loss: 0.00001069
Iteration 14/1000 | Loss: 0.00001059
Iteration 15/1000 | Loss: 0.00001055
Iteration 16/1000 | Loss: 0.00001055
Iteration 17/1000 | Loss: 0.00001054
Iteration 18/1000 | Loss: 0.00001047
Iteration 19/1000 | Loss: 0.00001047
Iteration 20/1000 | Loss: 0.00001046
Iteration 21/1000 | Loss: 0.00001046
Iteration 22/1000 | Loss: 0.00001045
Iteration 23/1000 | Loss: 0.00001044
Iteration 24/1000 | Loss: 0.00001044
Iteration 25/1000 | Loss: 0.00001044
Iteration 26/1000 | Loss: 0.00001043
Iteration 27/1000 | Loss: 0.00001043
Iteration 28/1000 | Loss: 0.00001043
Iteration 29/1000 | Loss: 0.00001042
Iteration 30/1000 | Loss: 0.00001042
Iteration 31/1000 | Loss: 0.00001041
Iteration 32/1000 | Loss: 0.00001040
Iteration 33/1000 | Loss: 0.00001037
Iteration 34/1000 | Loss: 0.00001034
Iteration 35/1000 | Loss: 0.00001034
Iteration 36/1000 | Loss: 0.00001033
Iteration 37/1000 | Loss: 0.00001033
Iteration 38/1000 | Loss: 0.00001031
Iteration 39/1000 | Loss: 0.00001031
Iteration 40/1000 | Loss: 0.00001030
Iteration 41/1000 | Loss: 0.00001030
Iteration 42/1000 | Loss: 0.00001029
Iteration 43/1000 | Loss: 0.00001029
Iteration 44/1000 | Loss: 0.00001028
Iteration 45/1000 | Loss: 0.00001028
Iteration 46/1000 | Loss: 0.00001027
Iteration 47/1000 | Loss: 0.00001027
Iteration 48/1000 | Loss: 0.00001026
Iteration 49/1000 | Loss: 0.00001026
Iteration 50/1000 | Loss: 0.00001025
Iteration 51/1000 | Loss: 0.00001025
Iteration 52/1000 | Loss: 0.00001024
Iteration 53/1000 | Loss: 0.00001024
Iteration 54/1000 | Loss: 0.00001024
Iteration 55/1000 | Loss: 0.00001023
Iteration 56/1000 | Loss: 0.00001022
Iteration 57/1000 | Loss: 0.00001022
Iteration 58/1000 | Loss: 0.00001021
Iteration 59/1000 | Loss: 0.00001021
Iteration 60/1000 | Loss: 0.00001021
Iteration 61/1000 | Loss: 0.00001021
Iteration 62/1000 | Loss: 0.00001020
Iteration 63/1000 | Loss: 0.00001020
Iteration 64/1000 | Loss: 0.00001020
Iteration 65/1000 | Loss: 0.00001020
Iteration 66/1000 | Loss: 0.00001020
Iteration 67/1000 | Loss: 0.00001020
Iteration 68/1000 | Loss: 0.00001020
Iteration 69/1000 | Loss: 0.00001020
Iteration 70/1000 | Loss: 0.00001020
Iteration 71/1000 | Loss: 0.00001019
Iteration 72/1000 | Loss: 0.00001019
Iteration 73/1000 | Loss: 0.00001019
Iteration 74/1000 | Loss: 0.00001018
Iteration 75/1000 | Loss: 0.00001018
Iteration 76/1000 | Loss: 0.00001018
Iteration 77/1000 | Loss: 0.00001018
Iteration 78/1000 | Loss: 0.00001017
Iteration 79/1000 | Loss: 0.00001017
Iteration 80/1000 | Loss: 0.00001017
Iteration 81/1000 | Loss: 0.00001017
Iteration 82/1000 | Loss: 0.00001016
Iteration 83/1000 | Loss: 0.00001016
Iteration 84/1000 | Loss: 0.00001016
Iteration 85/1000 | Loss: 0.00001016
Iteration 86/1000 | Loss: 0.00001016
Iteration 87/1000 | Loss: 0.00001015
Iteration 88/1000 | Loss: 0.00001015
Iteration 89/1000 | Loss: 0.00001015
Iteration 90/1000 | Loss: 0.00001015
Iteration 91/1000 | Loss: 0.00001014
Iteration 92/1000 | Loss: 0.00001014
Iteration 93/1000 | Loss: 0.00001014
Iteration 94/1000 | Loss: 0.00001013
Iteration 95/1000 | Loss: 0.00001013
Iteration 96/1000 | Loss: 0.00001012
Iteration 97/1000 | Loss: 0.00001012
Iteration 98/1000 | Loss: 0.00001011
Iteration 99/1000 | Loss: 0.00001011
Iteration 100/1000 | Loss: 0.00001011
Iteration 101/1000 | Loss: 0.00001011
Iteration 102/1000 | Loss: 0.00001011
Iteration 103/1000 | Loss: 0.00001010
Iteration 104/1000 | Loss: 0.00001010
Iteration 105/1000 | Loss: 0.00001010
Iteration 106/1000 | Loss: 0.00001009
Iteration 107/1000 | Loss: 0.00001009
Iteration 108/1000 | Loss: 0.00001009
Iteration 109/1000 | Loss: 0.00001009
Iteration 110/1000 | Loss: 0.00001009
Iteration 111/1000 | Loss: 0.00001009
Iteration 112/1000 | Loss: 0.00001008
Iteration 113/1000 | Loss: 0.00001008
Iteration 114/1000 | Loss: 0.00001008
Iteration 115/1000 | Loss: 0.00001008
Iteration 116/1000 | Loss: 0.00001008
Iteration 117/1000 | Loss: 0.00001007
Iteration 118/1000 | Loss: 0.00001007
Iteration 119/1000 | Loss: 0.00001007
Iteration 120/1000 | Loss: 0.00001007
Iteration 121/1000 | Loss: 0.00001007
Iteration 122/1000 | Loss: 0.00001007
Iteration 123/1000 | Loss: 0.00001007
Iteration 124/1000 | Loss: 0.00001007
Iteration 125/1000 | Loss: 0.00001007
Iteration 126/1000 | Loss: 0.00001006
Iteration 127/1000 | Loss: 0.00001006
Iteration 128/1000 | Loss: 0.00001006
Iteration 129/1000 | Loss: 0.00001006
Iteration 130/1000 | Loss: 0.00001005
Iteration 131/1000 | Loss: 0.00001005
Iteration 132/1000 | Loss: 0.00001005
Iteration 133/1000 | Loss: 0.00001005
Iteration 134/1000 | Loss: 0.00001005
Iteration 135/1000 | Loss: 0.00001005
Iteration 136/1000 | Loss: 0.00001004
Iteration 137/1000 | Loss: 0.00001004
Iteration 138/1000 | Loss: 0.00001004
Iteration 139/1000 | Loss: 0.00001004
Iteration 140/1000 | Loss: 0.00001004
Iteration 141/1000 | Loss: 0.00001004
Iteration 142/1000 | Loss: 0.00001004
Iteration 143/1000 | Loss: 0.00001004
Iteration 144/1000 | Loss: 0.00001004
Iteration 145/1000 | Loss: 0.00001004
Iteration 146/1000 | Loss: 0.00001003
Iteration 147/1000 | Loss: 0.00001003
Iteration 148/1000 | Loss: 0.00001003
Iteration 149/1000 | Loss: 0.00001003
Iteration 150/1000 | Loss: 0.00001003
Iteration 151/1000 | Loss: 0.00001002
Iteration 152/1000 | Loss: 0.00001002
Iteration 153/1000 | Loss: 0.00001002
Iteration 154/1000 | Loss: 0.00001002
Iteration 155/1000 | Loss: 0.00001002
Iteration 156/1000 | Loss: 0.00001001
Iteration 157/1000 | Loss: 0.00001001
Iteration 158/1000 | Loss: 0.00001001
Iteration 159/1000 | Loss: 0.00001001
Iteration 160/1000 | Loss: 0.00001000
Iteration 161/1000 | Loss: 0.00001000
Iteration 162/1000 | Loss: 0.00001000
Iteration 163/1000 | Loss: 0.00001000
Iteration 164/1000 | Loss: 0.00001000
Iteration 165/1000 | Loss: 0.00001000
Iteration 166/1000 | Loss: 0.00000999
Iteration 167/1000 | Loss: 0.00000999
Iteration 168/1000 | Loss: 0.00000999
Iteration 169/1000 | Loss: 0.00000998
Iteration 170/1000 | Loss: 0.00000998
Iteration 171/1000 | Loss: 0.00000998
Iteration 172/1000 | Loss: 0.00000998
Iteration 173/1000 | Loss: 0.00000998
Iteration 174/1000 | Loss: 0.00000998
Iteration 175/1000 | Loss: 0.00000998
Iteration 176/1000 | Loss: 0.00000998
Iteration 177/1000 | Loss: 0.00000998
Iteration 178/1000 | Loss: 0.00000998
Iteration 179/1000 | Loss: 0.00000998
Iteration 180/1000 | Loss: 0.00000998
Iteration 181/1000 | Loss: 0.00000998
Iteration 182/1000 | Loss: 0.00000998
Iteration 183/1000 | Loss: 0.00000998
Iteration 184/1000 | Loss: 0.00000998
Iteration 185/1000 | Loss: 0.00000998
Iteration 186/1000 | Loss: 0.00000998
Iteration 187/1000 | Loss: 0.00000998
Iteration 188/1000 | Loss: 0.00000997
Iteration 189/1000 | Loss: 0.00000997
Iteration 190/1000 | Loss: 0.00000997
Iteration 191/1000 | Loss: 0.00000997
Iteration 192/1000 | Loss: 0.00000997
Iteration 193/1000 | Loss: 0.00000997
Iteration 194/1000 | Loss: 0.00000996
Iteration 195/1000 | Loss: 0.00000996
Iteration 196/1000 | Loss: 0.00000996
Iteration 197/1000 | Loss: 0.00000996
Iteration 198/1000 | Loss: 0.00000996
Iteration 199/1000 | Loss: 0.00000996
Iteration 200/1000 | Loss: 0.00000996
Iteration 201/1000 | Loss: 0.00000996
Iteration 202/1000 | Loss: 0.00000996
Iteration 203/1000 | Loss: 0.00000996
Iteration 204/1000 | Loss: 0.00000996
Iteration 205/1000 | Loss: 0.00000996
Iteration 206/1000 | Loss: 0.00000996
Iteration 207/1000 | Loss: 0.00000996
Iteration 208/1000 | Loss: 0.00000996
Iteration 209/1000 | Loss: 0.00000996
Iteration 210/1000 | Loss: 0.00000996
Iteration 211/1000 | Loss: 0.00000996
Iteration 212/1000 | Loss: 0.00000996
Iteration 213/1000 | Loss: 0.00000996
Iteration 214/1000 | Loss: 0.00000996
Iteration 215/1000 | Loss: 0.00000996
Iteration 216/1000 | Loss: 0.00000996
Iteration 217/1000 | Loss: 0.00000996
Iteration 218/1000 | Loss: 0.00000996
Iteration 219/1000 | Loss: 0.00000996
Iteration 220/1000 | Loss: 0.00000996
Iteration 221/1000 | Loss: 0.00000996
Iteration 222/1000 | Loss: 0.00000996
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 222. Stopping optimization.
Last 5 losses: [9.961309842765331e-06, 9.961309842765331e-06, 9.961309842765331e-06, 9.961309842765331e-06, 9.961309842765331e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.961309842765331e-06

Optimization complete. Final v2v error: 2.664538621902466 mm

Highest mean error: 2.991511106491089 mm for frame 41

Lowest mean error: 2.3272924423217773 mm for frame 13

Saving results

Total time: 42.347078800201416
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00802317
Iteration 2/25 | Loss: 0.00159351
Iteration 3/25 | Loss: 0.00135304
Iteration 4/25 | Loss: 0.00133900
Iteration 5/25 | Loss: 0.00133705
Iteration 6/25 | Loss: 0.00133705
Iteration 7/25 | Loss: 0.00133705
Iteration 8/25 | Loss: 0.00133705
Iteration 9/25 | Loss: 0.00133705
Iteration 10/25 | Loss: 0.00133705
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013370452215895057, 0.0013370452215895057, 0.0013370452215895057, 0.0013370452215895057, 0.0013370452215895057]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013370452215895057

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.26092941
Iteration 2/25 | Loss: 0.00129153
Iteration 3/25 | Loss: 0.00129153
Iteration 4/25 | Loss: 0.00129153
Iteration 5/25 | Loss: 0.00129153
Iteration 6/25 | Loss: 0.00129153
Iteration 7/25 | Loss: 0.00129153
Iteration 8/25 | Loss: 0.00129153
Iteration 9/25 | Loss: 0.00129153
Iteration 10/25 | Loss: 0.00129153
Iteration 11/25 | Loss: 0.00129153
Iteration 12/25 | Loss: 0.00129153
Iteration 13/25 | Loss: 0.00129153
Iteration 14/25 | Loss: 0.00129153
Iteration 15/25 | Loss: 0.00129153
Iteration 16/25 | Loss: 0.00129153
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012915318366140127, 0.0012915318366140127, 0.0012915318366140127, 0.0012915318366140127, 0.0012915318366140127]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012915318366140127

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129153
Iteration 2/1000 | Loss: 0.00005675
Iteration 3/1000 | Loss: 0.00003568
Iteration 4/1000 | Loss: 0.00003051
Iteration 5/1000 | Loss: 0.00002830
Iteration 6/1000 | Loss: 0.00002694
Iteration 7/1000 | Loss: 0.00002597
Iteration 8/1000 | Loss: 0.00002520
Iteration 9/1000 | Loss: 0.00002482
Iteration 10/1000 | Loss: 0.00002458
Iteration 11/1000 | Loss: 0.00002445
Iteration 12/1000 | Loss: 0.00002428
Iteration 13/1000 | Loss: 0.00002417
Iteration 14/1000 | Loss: 0.00002402
Iteration 15/1000 | Loss: 0.00002399
Iteration 16/1000 | Loss: 0.00002397
Iteration 17/1000 | Loss: 0.00002393
Iteration 18/1000 | Loss: 0.00002388
Iteration 19/1000 | Loss: 0.00002388
Iteration 20/1000 | Loss: 0.00002385
Iteration 21/1000 | Loss: 0.00002385
Iteration 22/1000 | Loss: 0.00002385
Iteration 23/1000 | Loss: 0.00002383
Iteration 24/1000 | Loss: 0.00002383
Iteration 25/1000 | Loss: 0.00002383
Iteration 26/1000 | Loss: 0.00002383
Iteration 27/1000 | Loss: 0.00002382
Iteration 28/1000 | Loss: 0.00002382
Iteration 29/1000 | Loss: 0.00002382
Iteration 30/1000 | Loss: 0.00002382
Iteration 31/1000 | Loss: 0.00002382
Iteration 32/1000 | Loss: 0.00002382
Iteration 33/1000 | Loss: 0.00002382
Iteration 34/1000 | Loss: 0.00002382
Iteration 35/1000 | Loss: 0.00002382
Iteration 36/1000 | Loss: 0.00002382
Iteration 37/1000 | Loss: 0.00002382
Iteration 38/1000 | Loss: 0.00002382
Iteration 39/1000 | Loss: 0.00002381
Iteration 40/1000 | Loss: 0.00002381
Iteration 41/1000 | Loss: 0.00002381
Iteration 42/1000 | Loss: 0.00002381
Iteration 43/1000 | Loss: 0.00002381
Iteration 44/1000 | Loss: 0.00002380
Iteration 45/1000 | Loss: 0.00002380
Iteration 46/1000 | Loss: 0.00002380
Iteration 47/1000 | Loss: 0.00002379
Iteration 48/1000 | Loss: 0.00002379
Iteration 49/1000 | Loss: 0.00002379
Iteration 50/1000 | Loss: 0.00002379
Iteration 51/1000 | Loss: 0.00002379
Iteration 52/1000 | Loss: 0.00002378
Iteration 53/1000 | Loss: 0.00002378
Iteration 54/1000 | Loss: 0.00002378
Iteration 55/1000 | Loss: 0.00002378
Iteration 56/1000 | Loss: 0.00002378
Iteration 57/1000 | Loss: 0.00002378
Iteration 58/1000 | Loss: 0.00002378
Iteration 59/1000 | Loss: 0.00002377
Iteration 60/1000 | Loss: 0.00002377
Iteration 61/1000 | Loss: 0.00002377
Iteration 62/1000 | Loss: 0.00002377
Iteration 63/1000 | Loss: 0.00002377
Iteration 64/1000 | Loss: 0.00002377
Iteration 65/1000 | Loss: 0.00002376
Iteration 66/1000 | Loss: 0.00002376
Iteration 67/1000 | Loss: 0.00002376
Iteration 68/1000 | Loss: 0.00002376
Iteration 69/1000 | Loss: 0.00002376
Iteration 70/1000 | Loss: 0.00002376
Iteration 71/1000 | Loss: 0.00002376
Iteration 72/1000 | Loss: 0.00002376
Iteration 73/1000 | Loss: 0.00002376
Iteration 74/1000 | Loss: 0.00002376
Iteration 75/1000 | Loss: 0.00002376
Iteration 76/1000 | Loss: 0.00002376
Iteration 77/1000 | Loss: 0.00002376
Iteration 78/1000 | Loss: 0.00002375
Iteration 79/1000 | Loss: 0.00002375
Iteration 80/1000 | Loss: 0.00002375
Iteration 81/1000 | Loss: 0.00002375
Iteration 82/1000 | Loss: 0.00002375
Iteration 83/1000 | Loss: 0.00002375
Iteration 84/1000 | Loss: 0.00002375
Iteration 85/1000 | Loss: 0.00002375
Iteration 86/1000 | Loss: 0.00002375
Iteration 87/1000 | Loss: 0.00002375
Iteration 88/1000 | Loss: 0.00002375
Iteration 89/1000 | Loss: 0.00002375
Iteration 90/1000 | Loss: 0.00002375
Iteration 91/1000 | Loss: 0.00002374
Iteration 92/1000 | Loss: 0.00002374
Iteration 93/1000 | Loss: 0.00002374
Iteration 94/1000 | Loss: 0.00002374
Iteration 95/1000 | Loss: 0.00002374
Iteration 96/1000 | Loss: 0.00002374
Iteration 97/1000 | Loss: 0.00002374
Iteration 98/1000 | Loss: 0.00002374
Iteration 99/1000 | Loss: 0.00002374
Iteration 100/1000 | Loss: 0.00002374
Iteration 101/1000 | Loss: 0.00002374
Iteration 102/1000 | Loss: 0.00002374
Iteration 103/1000 | Loss: 0.00002374
Iteration 104/1000 | Loss: 0.00002373
Iteration 105/1000 | Loss: 0.00002373
Iteration 106/1000 | Loss: 0.00002373
Iteration 107/1000 | Loss: 0.00002373
Iteration 108/1000 | Loss: 0.00002373
Iteration 109/1000 | Loss: 0.00002373
Iteration 110/1000 | Loss: 0.00002373
Iteration 111/1000 | Loss: 0.00002373
Iteration 112/1000 | Loss: 0.00002372
Iteration 113/1000 | Loss: 0.00002372
Iteration 114/1000 | Loss: 0.00002372
Iteration 115/1000 | Loss: 0.00002372
Iteration 116/1000 | Loss: 0.00002372
Iteration 117/1000 | Loss: 0.00002372
Iteration 118/1000 | Loss: 0.00002372
Iteration 119/1000 | Loss: 0.00002372
Iteration 120/1000 | Loss: 0.00002372
Iteration 121/1000 | Loss: 0.00002371
Iteration 122/1000 | Loss: 0.00002371
Iteration 123/1000 | Loss: 0.00002371
Iteration 124/1000 | Loss: 0.00002371
Iteration 125/1000 | Loss: 0.00002371
Iteration 126/1000 | Loss: 0.00002371
Iteration 127/1000 | Loss: 0.00002371
Iteration 128/1000 | Loss: 0.00002371
Iteration 129/1000 | Loss: 0.00002371
Iteration 130/1000 | Loss: 0.00002371
Iteration 131/1000 | Loss: 0.00002371
Iteration 132/1000 | Loss: 0.00002371
Iteration 133/1000 | Loss: 0.00002371
Iteration 134/1000 | Loss: 0.00002371
Iteration 135/1000 | Loss: 0.00002371
Iteration 136/1000 | Loss: 0.00002371
Iteration 137/1000 | Loss: 0.00002371
Iteration 138/1000 | Loss: 0.00002371
Iteration 139/1000 | Loss: 0.00002371
Iteration 140/1000 | Loss: 0.00002371
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [2.371188747929409e-05, 2.371188747929409e-05, 2.371188747929409e-05, 2.371188747929409e-05, 2.371188747929409e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.371188747929409e-05

Optimization complete. Final v2v error: 3.9103286266326904 mm

Highest mean error: 4.709346294403076 mm for frame 5

Lowest mean error: 3.283082962036133 mm for frame 141

Saving results

Total time: 34.60443878173828
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00595896
Iteration 2/25 | Loss: 0.00113994
Iteration 3/25 | Loss: 0.00106699
Iteration 4/25 | Loss: 0.00105724
Iteration 5/25 | Loss: 0.00105392
Iteration 6/25 | Loss: 0.00105341
Iteration 7/25 | Loss: 0.00105341
Iteration 8/25 | Loss: 0.00105341
Iteration 9/25 | Loss: 0.00105341
Iteration 10/25 | Loss: 0.00105341
Iteration 11/25 | Loss: 0.00105341
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010534089524298906, 0.0010534089524298906, 0.0010534089524298906, 0.0010534089524298906, 0.0010534089524298906]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010534089524298906

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.90836859
Iteration 2/25 | Loss: 0.00082843
Iteration 3/25 | Loss: 0.00082842
Iteration 4/25 | Loss: 0.00082842
Iteration 5/25 | Loss: 0.00082842
Iteration 6/25 | Loss: 0.00082842
Iteration 7/25 | Loss: 0.00082842
Iteration 8/25 | Loss: 0.00082842
Iteration 9/25 | Loss: 0.00082842
Iteration 10/25 | Loss: 0.00082842
Iteration 11/25 | Loss: 0.00082842
Iteration 12/25 | Loss: 0.00082842
Iteration 13/25 | Loss: 0.00082842
Iteration 14/25 | Loss: 0.00082842
Iteration 15/25 | Loss: 0.00082842
Iteration 16/25 | Loss: 0.00082842
Iteration 17/25 | Loss: 0.00082842
Iteration 18/25 | Loss: 0.00082842
Iteration 19/25 | Loss: 0.00082842
Iteration 20/25 | Loss: 0.00082842
Iteration 21/25 | Loss: 0.00082842
Iteration 22/25 | Loss: 0.00082842
Iteration 23/25 | Loss: 0.00082842
Iteration 24/25 | Loss: 0.00082842
Iteration 25/25 | Loss: 0.00082842

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082842
Iteration 2/1000 | Loss: 0.00002154
Iteration 3/1000 | Loss: 0.00001456
Iteration 4/1000 | Loss: 0.00001233
Iteration 5/1000 | Loss: 0.00001159
Iteration 6/1000 | Loss: 0.00001097
Iteration 7/1000 | Loss: 0.00001051
Iteration 8/1000 | Loss: 0.00001030
Iteration 9/1000 | Loss: 0.00001001
Iteration 10/1000 | Loss: 0.00000986
Iteration 11/1000 | Loss: 0.00000983
Iteration 12/1000 | Loss: 0.00000982
Iteration 13/1000 | Loss: 0.00000979
Iteration 14/1000 | Loss: 0.00000978
Iteration 15/1000 | Loss: 0.00000977
Iteration 16/1000 | Loss: 0.00000966
Iteration 17/1000 | Loss: 0.00000959
Iteration 18/1000 | Loss: 0.00000957
Iteration 19/1000 | Loss: 0.00000955
Iteration 20/1000 | Loss: 0.00000954
Iteration 21/1000 | Loss: 0.00000954
Iteration 22/1000 | Loss: 0.00000953
Iteration 23/1000 | Loss: 0.00000952
Iteration 24/1000 | Loss: 0.00000951
Iteration 25/1000 | Loss: 0.00000951
Iteration 26/1000 | Loss: 0.00000950
Iteration 27/1000 | Loss: 0.00000949
Iteration 28/1000 | Loss: 0.00000949
Iteration 29/1000 | Loss: 0.00000948
Iteration 30/1000 | Loss: 0.00000948
Iteration 31/1000 | Loss: 0.00000947
Iteration 32/1000 | Loss: 0.00000946
Iteration 33/1000 | Loss: 0.00000945
Iteration 34/1000 | Loss: 0.00000945
Iteration 35/1000 | Loss: 0.00000944
Iteration 36/1000 | Loss: 0.00000944
Iteration 37/1000 | Loss: 0.00000943
Iteration 38/1000 | Loss: 0.00000942
Iteration 39/1000 | Loss: 0.00000942
Iteration 40/1000 | Loss: 0.00000942
Iteration 41/1000 | Loss: 0.00000942
Iteration 42/1000 | Loss: 0.00000942
Iteration 43/1000 | Loss: 0.00000942
Iteration 44/1000 | Loss: 0.00000942
Iteration 45/1000 | Loss: 0.00000941
Iteration 46/1000 | Loss: 0.00000941
Iteration 47/1000 | Loss: 0.00000941
Iteration 48/1000 | Loss: 0.00000941
Iteration 49/1000 | Loss: 0.00000941
Iteration 50/1000 | Loss: 0.00000941
Iteration 51/1000 | Loss: 0.00000938
Iteration 52/1000 | Loss: 0.00000938
Iteration 53/1000 | Loss: 0.00000937
Iteration 54/1000 | Loss: 0.00000937
Iteration 55/1000 | Loss: 0.00000936
Iteration 56/1000 | Loss: 0.00000936
Iteration 57/1000 | Loss: 0.00000935
Iteration 58/1000 | Loss: 0.00000935
Iteration 59/1000 | Loss: 0.00000934
Iteration 60/1000 | Loss: 0.00000933
Iteration 61/1000 | Loss: 0.00000933
Iteration 62/1000 | Loss: 0.00000932
Iteration 63/1000 | Loss: 0.00000932
Iteration 64/1000 | Loss: 0.00000932
Iteration 65/1000 | Loss: 0.00000932
Iteration 66/1000 | Loss: 0.00000932
Iteration 67/1000 | Loss: 0.00000932
Iteration 68/1000 | Loss: 0.00000932
Iteration 69/1000 | Loss: 0.00000932
Iteration 70/1000 | Loss: 0.00000931
Iteration 71/1000 | Loss: 0.00000931
Iteration 72/1000 | Loss: 0.00000931
Iteration 73/1000 | Loss: 0.00000930
Iteration 74/1000 | Loss: 0.00000930
Iteration 75/1000 | Loss: 0.00000928
Iteration 76/1000 | Loss: 0.00000927
Iteration 77/1000 | Loss: 0.00000927
Iteration 78/1000 | Loss: 0.00000927
Iteration 79/1000 | Loss: 0.00000927
Iteration 80/1000 | Loss: 0.00000927
Iteration 81/1000 | Loss: 0.00000927
Iteration 82/1000 | Loss: 0.00000927
Iteration 83/1000 | Loss: 0.00000927
Iteration 84/1000 | Loss: 0.00000927
Iteration 85/1000 | Loss: 0.00000926
Iteration 86/1000 | Loss: 0.00000926
Iteration 87/1000 | Loss: 0.00000926
Iteration 88/1000 | Loss: 0.00000925
Iteration 89/1000 | Loss: 0.00000925
Iteration 90/1000 | Loss: 0.00000924
Iteration 91/1000 | Loss: 0.00000923
Iteration 92/1000 | Loss: 0.00000923
Iteration 93/1000 | Loss: 0.00000923
Iteration 94/1000 | Loss: 0.00000923
Iteration 95/1000 | Loss: 0.00000923
Iteration 96/1000 | Loss: 0.00000923
Iteration 97/1000 | Loss: 0.00000923
Iteration 98/1000 | Loss: 0.00000923
Iteration 99/1000 | Loss: 0.00000923
Iteration 100/1000 | Loss: 0.00000923
Iteration 101/1000 | Loss: 0.00000923
Iteration 102/1000 | Loss: 0.00000922
Iteration 103/1000 | Loss: 0.00000922
Iteration 104/1000 | Loss: 0.00000922
Iteration 105/1000 | Loss: 0.00000921
Iteration 106/1000 | Loss: 0.00000921
Iteration 107/1000 | Loss: 0.00000920
Iteration 108/1000 | Loss: 0.00000920
Iteration 109/1000 | Loss: 0.00000920
Iteration 110/1000 | Loss: 0.00000920
Iteration 111/1000 | Loss: 0.00000920
Iteration 112/1000 | Loss: 0.00000920
Iteration 113/1000 | Loss: 0.00000920
Iteration 114/1000 | Loss: 0.00000920
Iteration 115/1000 | Loss: 0.00000919
Iteration 116/1000 | Loss: 0.00000919
Iteration 117/1000 | Loss: 0.00000919
Iteration 118/1000 | Loss: 0.00000919
Iteration 119/1000 | Loss: 0.00000918
Iteration 120/1000 | Loss: 0.00000918
Iteration 121/1000 | Loss: 0.00000918
Iteration 122/1000 | Loss: 0.00000918
Iteration 123/1000 | Loss: 0.00000918
Iteration 124/1000 | Loss: 0.00000918
Iteration 125/1000 | Loss: 0.00000917
Iteration 126/1000 | Loss: 0.00000917
Iteration 127/1000 | Loss: 0.00000917
Iteration 128/1000 | Loss: 0.00000917
Iteration 129/1000 | Loss: 0.00000916
Iteration 130/1000 | Loss: 0.00000916
Iteration 131/1000 | Loss: 0.00000916
Iteration 132/1000 | Loss: 0.00000915
Iteration 133/1000 | Loss: 0.00000915
Iteration 134/1000 | Loss: 0.00000915
Iteration 135/1000 | Loss: 0.00000914
Iteration 136/1000 | Loss: 0.00000914
Iteration 137/1000 | Loss: 0.00000914
Iteration 138/1000 | Loss: 0.00000914
Iteration 139/1000 | Loss: 0.00000914
Iteration 140/1000 | Loss: 0.00000914
Iteration 141/1000 | Loss: 0.00000914
Iteration 142/1000 | Loss: 0.00000914
Iteration 143/1000 | Loss: 0.00000914
Iteration 144/1000 | Loss: 0.00000913
Iteration 145/1000 | Loss: 0.00000913
Iteration 146/1000 | Loss: 0.00000913
Iteration 147/1000 | Loss: 0.00000913
Iteration 148/1000 | Loss: 0.00000912
Iteration 149/1000 | Loss: 0.00000912
Iteration 150/1000 | Loss: 0.00000912
Iteration 151/1000 | Loss: 0.00000912
Iteration 152/1000 | Loss: 0.00000912
Iteration 153/1000 | Loss: 0.00000912
Iteration 154/1000 | Loss: 0.00000912
Iteration 155/1000 | Loss: 0.00000912
Iteration 156/1000 | Loss: 0.00000911
Iteration 157/1000 | Loss: 0.00000911
Iteration 158/1000 | Loss: 0.00000911
Iteration 159/1000 | Loss: 0.00000911
Iteration 160/1000 | Loss: 0.00000911
Iteration 161/1000 | Loss: 0.00000911
Iteration 162/1000 | Loss: 0.00000911
Iteration 163/1000 | Loss: 0.00000911
Iteration 164/1000 | Loss: 0.00000911
Iteration 165/1000 | Loss: 0.00000911
Iteration 166/1000 | Loss: 0.00000911
Iteration 167/1000 | Loss: 0.00000911
Iteration 168/1000 | Loss: 0.00000910
Iteration 169/1000 | Loss: 0.00000910
Iteration 170/1000 | Loss: 0.00000910
Iteration 171/1000 | Loss: 0.00000910
Iteration 172/1000 | Loss: 0.00000910
Iteration 173/1000 | Loss: 0.00000910
Iteration 174/1000 | Loss: 0.00000910
Iteration 175/1000 | Loss: 0.00000910
Iteration 176/1000 | Loss: 0.00000910
Iteration 177/1000 | Loss: 0.00000910
Iteration 178/1000 | Loss: 0.00000910
Iteration 179/1000 | Loss: 0.00000910
Iteration 180/1000 | Loss: 0.00000910
Iteration 181/1000 | Loss: 0.00000910
Iteration 182/1000 | Loss: 0.00000910
Iteration 183/1000 | Loss: 0.00000910
Iteration 184/1000 | Loss: 0.00000910
Iteration 185/1000 | Loss: 0.00000910
Iteration 186/1000 | Loss: 0.00000910
Iteration 187/1000 | Loss: 0.00000910
Iteration 188/1000 | Loss: 0.00000910
Iteration 189/1000 | Loss: 0.00000910
Iteration 190/1000 | Loss: 0.00000910
Iteration 191/1000 | Loss: 0.00000910
Iteration 192/1000 | Loss: 0.00000910
Iteration 193/1000 | Loss: 0.00000910
Iteration 194/1000 | Loss: 0.00000910
Iteration 195/1000 | Loss: 0.00000910
Iteration 196/1000 | Loss: 0.00000910
Iteration 197/1000 | Loss: 0.00000910
Iteration 198/1000 | Loss: 0.00000910
Iteration 199/1000 | Loss: 0.00000910
Iteration 200/1000 | Loss: 0.00000910
Iteration 201/1000 | Loss: 0.00000910
Iteration 202/1000 | Loss: 0.00000910
Iteration 203/1000 | Loss: 0.00000910
Iteration 204/1000 | Loss: 0.00000910
Iteration 205/1000 | Loss: 0.00000910
Iteration 206/1000 | Loss: 0.00000910
Iteration 207/1000 | Loss: 0.00000910
Iteration 208/1000 | Loss: 0.00000910
Iteration 209/1000 | Loss: 0.00000910
Iteration 210/1000 | Loss: 0.00000910
Iteration 211/1000 | Loss: 0.00000910
Iteration 212/1000 | Loss: 0.00000910
Iteration 213/1000 | Loss: 0.00000910
Iteration 214/1000 | Loss: 0.00000910
Iteration 215/1000 | Loss: 0.00000910
Iteration 216/1000 | Loss: 0.00000910
Iteration 217/1000 | Loss: 0.00000910
Iteration 218/1000 | Loss: 0.00000910
Iteration 219/1000 | Loss: 0.00000910
Iteration 220/1000 | Loss: 0.00000910
Iteration 221/1000 | Loss: 0.00000910
Iteration 222/1000 | Loss: 0.00000910
Iteration 223/1000 | Loss: 0.00000910
Iteration 224/1000 | Loss: 0.00000910
Iteration 225/1000 | Loss: 0.00000910
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [9.095406312553678e-06, 9.095406312553678e-06, 9.095406312553678e-06, 9.095406312553678e-06, 9.095406312553678e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.095406312553678e-06

Optimization complete. Final v2v error: 2.617729902267456 mm

Highest mean error: 2.870861530303955 mm for frame 142

Lowest mean error: 2.4233410358428955 mm for frame 170

Saving results

Total time: 39.95976805686951
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00899446
Iteration 2/25 | Loss: 0.00151293
Iteration 3/25 | Loss: 0.00130514
Iteration 4/25 | Loss: 0.00129047
Iteration 5/25 | Loss: 0.00130423
Iteration 6/25 | Loss: 0.00127814
Iteration 7/25 | Loss: 0.00127949
Iteration 8/25 | Loss: 0.00126465
Iteration 9/25 | Loss: 0.00126466
Iteration 10/25 | Loss: 0.00124916
Iteration 11/25 | Loss: 0.00124411
Iteration 12/25 | Loss: 0.00124719
Iteration 13/25 | Loss: 0.00123231
Iteration 14/25 | Loss: 0.00123408
Iteration 15/25 | Loss: 0.00123110
Iteration 16/25 | Loss: 0.00123231
Iteration 17/25 | Loss: 0.00123128
Iteration 18/25 | Loss: 0.00122941
Iteration 19/25 | Loss: 0.00122879
Iteration 20/25 | Loss: 0.00122800
Iteration 21/25 | Loss: 0.00122712
Iteration 22/25 | Loss: 0.00122675
Iteration 23/25 | Loss: 0.00122664
Iteration 24/25 | Loss: 0.00122653
Iteration 25/25 | Loss: 0.00122929

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62793660
Iteration 2/25 | Loss: 0.00094374
Iteration 3/25 | Loss: 0.00094374
Iteration 4/25 | Loss: 0.00086463
Iteration 5/25 | Loss: 0.00086462
Iteration 6/25 | Loss: 0.00086462
Iteration 7/25 | Loss: 0.00086462
Iteration 8/25 | Loss: 0.00086462
Iteration 9/25 | Loss: 0.00086462
Iteration 10/25 | Loss: 0.00086462
Iteration 11/25 | Loss: 0.00086462
Iteration 12/25 | Loss: 0.00086462
Iteration 13/25 | Loss: 0.00086462
Iteration 14/25 | Loss: 0.00086462
Iteration 15/25 | Loss: 0.00086462
Iteration 16/25 | Loss: 0.00086462
Iteration 17/25 | Loss: 0.00086462
Iteration 18/25 | Loss: 0.00086462
Iteration 19/25 | Loss: 0.00086462
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008646155474707484, 0.0008646155474707484, 0.0008646155474707484, 0.0008646155474707484, 0.0008646155474707484]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008646155474707484

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086462
Iteration 2/1000 | Loss: 0.00008880
Iteration 3/1000 | Loss: 0.00016556
Iteration 4/1000 | Loss: 0.00005223
Iteration 5/1000 | Loss: 0.00004721
Iteration 6/1000 | Loss: 0.00004397
Iteration 7/1000 | Loss: 0.00004191
Iteration 8/1000 | Loss: 0.00031034
Iteration 9/1000 | Loss: 0.00003940
Iteration 10/1000 | Loss: 0.00003793
Iteration 11/1000 | Loss: 0.00003627
Iteration 12/1000 | Loss: 0.00003467
Iteration 13/1000 | Loss: 0.00021809
Iteration 14/1000 | Loss: 0.00003486
Iteration 15/1000 | Loss: 0.00155656
Iteration 16/1000 | Loss: 0.00072729
Iteration 17/1000 | Loss: 0.00004339
Iteration 18/1000 | Loss: 0.00011086
Iteration 19/1000 | Loss: 0.00074415
Iteration 20/1000 | Loss: 0.00034641
Iteration 21/1000 | Loss: 0.00008763
Iteration 22/1000 | Loss: 0.00003323
Iteration 23/1000 | Loss: 0.00072446
Iteration 24/1000 | Loss: 0.00019982
Iteration 25/1000 | Loss: 0.00053806
Iteration 26/1000 | Loss: 0.00022764
Iteration 27/1000 | Loss: 0.00003396
Iteration 28/1000 | Loss: 0.00115518
Iteration 29/1000 | Loss: 0.00025475
Iteration 30/1000 | Loss: 0.00004610
Iteration 31/1000 | Loss: 0.00044637
Iteration 32/1000 | Loss: 0.00013222
Iteration 33/1000 | Loss: 0.00003663
Iteration 34/1000 | Loss: 0.00024671
Iteration 35/1000 | Loss: 0.00230539
Iteration 36/1000 | Loss: 0.00018839
Iteration 37/1000 | Loss: 0.00004380
Iteration 38/1000 | Loss: 0.00003995
Iteration 39/1000 | Loss: 0.00071088
Iteration 40/1000 | Loss: 0.00034850
Iteration 41/1000 | Loss: 0.00067446
Iteration 42/1000 | Loss: 0.00228186
Iteration 43/1000 | Loss: 0.00103202
Iteration 44/1000 | Loss: 0.00023891
Iteration 45/1000 | Loss: 0.00015548
Iteration 46/1000 | Loss: 0.00005393
Iteration 47/1000 | Loss: 0.00024987
Iteration 48/1000 | Loss: 0.00004411
Iteration 49/1000 | Loss: 0.00003942
Iteration 50/1000 | Loss: 0.00003769
Iteration 51/1000 | Loss: 0.00028061
Iteration 52/1000 | Loss: 0.00053034
Iteration 53/1000 | Loss: 0.00004210
Iteration 54/1000 | Loss: 0.00003666
Iteration 55/1000 | Loss: 0.00017197
Iteration 56/1000 | Loss: 0.00029481
Iteration 57/1000 | Loss: 0.00037614
Iteration 58/1000 | Loss: 0.00063999
Iteration 59/1000 | Loss: 0.00031034
Iteration 60/1000 | Loss: 0.00073572
Iteration 61/1000 | Loss: 0.00033862
Iteration 62/1000 | Loss: 0.00016422
Iteration 63/1000 | Loss: 0.00004209
Iteration 64/1000 | Loss: 0.00003791
Iteration 65/1000 | Loss: 0.00003549
Iteration 66/1000 | Loss: 0.00058314
Iteration 67/1000 | Loss: 0.00032322
Iteration 68/1000 | Loss: 0.00028965
Iteration 69/1000 | Loss: 0.00004212
Iteration 70/1000 | Loss: 0.00003423
Iteration 71/1000 | Loss: 0.00002999
Iteration 72/1000 | Loss: 0.00016110
Iteration 73/1000 | Loss: 0.00003457
Iteration 74/1000 | Loss: 0.00002513
Iteration 75/1000 | Loss: 0.00002448
Iteration 76/1000 | Loss: 0.00002399
Iteration 77/1000 | Loss: 0.00013519
Iteration 78/1000 | Loss: 0.00002421
Iteration 79/1000 | Loss: 0.00002354
Iteration 80/1000 | Loss: 0.00002346
Iteration 81/1000 | Loss: 0.00002322
Iteration 82/1000 | Loss: 0.00039667
Iteration 83/1000 | Loss: 0.00002997
Iteration 84/1000 | Loss: 0.00002535
Iteration 85/1000 | Loss: 0.00002338
Iteration 86/1000 | Loss: 0.00002226
Iteration 87/1000 | Loss: 0.00002165
Iteration 88/1000 | Loss: 0.00002138
Iteration 89/1000 | Loss: 0.00002137
Iteration 90/1000 | Loss: 0.00002135
Iteration 91/1000 | Loss: 0.00002120
Iteration 92/1000 | Loss: 0.00002120
Iteration 93/1000 | Loss: 0.00002119
Iteration 94/1000 | Loss: 0.00002119
Iteration 95/1000 | Loss: 0.00002117
Iteration 96/1000 | Loss: 0.00002116
Iteration 97/1000 | Loss: 0.00002116
Iteration 98/1000 | Loss: 0.00002111
Iteration 99/1000 | Loss: 0.00002111
Iteration 100/1000 | Loss: 0.00002111
Iteration 101/1000 | Loss: 0.00002111
Iteration 102/1000 | Loss: 0.00002111
Iteration 103/1000 | Loss: 0.00002110
Iteration 104/1000 | Loss: 0.00002110
Iteration 105/1000 | Loss: 0.00002109
Iteration 106/1000 | Loss: 0.00002109
Iteration 107/1000 | Loss: 0.00002109
Iteration 108/1000 | Loss: 0.00002109
Iteration 109/1000 | Loss: 0.00002108
Iteration 110/1000 | Loss: 0.00002108
Iteration 111/1000 | Loss: 0.00002108
Iteration 112/1000 | Loss: 0.00002107
Iteration 113/1000 | Loss: 0.00002107
Iteration 114/1000 | Loss: 0.00002107
Iteration 115/1000 | Loss: 0.00002106
Iteration 116/1000 | Loss: 0.00002105
Iteration 117/1000 | Loss: 0.00002105
Iteration 118/1000 | Loss: 0.00002104
Iteration 119/1000 | Loss: 0.00002103
Iteration 120/1000 | Loss: 0.00002103
Iteration 121/1000 | Loss: 0.00002103
Iteration 122/1000 | Loss: 0.00002102
Iteration 123/1000 | Loss: 0.00002102
Iteration 124/1000 | Loss: 0.00002101
Iteration 125/1000 | Loss: 0.00002101
Iteration 126/1000 | Loss: 0.00002101
Iteration 127/1000 | Loss: 0.00002101
Iteration 128/1000 | Loss: 0.00002100
Iteration 129/1000 | Loss: 0.00002100
Iteration 130/1000 | Loss: 0.00002100
Iteration 131/1000 | Loss: 0.00002100
Iteration 132/1000 | Loss: 0.00002100
Iteration 133/1000 | Loss: 0.00002100
Iteration 134/1000 | Loss: 0.00002100
Iteration 135/1000 | Loss: 0.00002100
Iteration 136/1000 | Loss: 0.00002099
Iteration 137/1000 | Loss: 0.00002099
Iteration 138/1000 | Loss: 0.00002098
Iteration 139/1000 | Loss: 0.00002098
Iteration 140/1000 | Loss: 0.00002098
Iteration 141/1000 | Loss: 0.00002097
Iteration 142/1000 | Loss: 0.00002097
Iteration 143/1000 | Loss: 0.00002097
Iteration 144/1000 | Loss: 0.00002097
Iteration 145/1000 | Loss: 0.00002097
Iteration 146/1000 | Loss: 0.00002097
Iteration 147/1000 | Loss: 0.00002097
Iteration 148/1000 | Loss: 0.00002097
Iteration 149/1000 | Loss: 0.00002097
Iteration 150/1000 | Loss: 0.00002097
Iteration 151/1000 | Loss: 0.00002097
Iteration 152/1000 | Loss: 0.00002097
Iteration 153/1000 | Loss: 0.00002097
Iteration 154/1000 | Loss: 0.00002097
Iteration 155/1000 | Loss: 0.00002097
Iteration 156/1000 | Loss: 0.00002097
Iteration 157/1000 | Loss: 0.00002097
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [2.0966328520444222e-05, 2.0966328520444222e-05, 2.0966328520444222e-05, 2.0966328520444222e-05, 2.0966328520444222e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0966328520444222e-05

Optimization complete. Final v2v error: 3.6052918434143066 mm

Highest mean error: 12.604131698608398 mm for frame 40

Lowest mean error: 2.8857688903808594 mm for frame 140

Saving results

Total time: 201.59123873710632
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00360405
Iteration 2/25 | Loss: 0.00110547
Iteration 3/25 | Loss: 0.00104445
Iteration 4/25 | Loss: 0.00103528
Iteration 5/25 | Loss: 0.00103208
Iteration 6/25 | Loss: 0.00103134
Iteration 7/25 | Loss: 0.00103134
Iteration 8/25 | Loss: 0.00103134
Iteration 9/25 | Loss: 0.00103134
Iteration 10/25 | Loss: 0.00103134
Iteration 11/25 | Loss: 0.00103134
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010313394013792276, 0.0010313394013792276, 0.0010313394013792276, 0.0010313394013792276, 0.0010313394013792276]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010313394013792276

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43821001
Iteration 2/25 | Loss: 0.00081814
Iteration 3/25 | Loss: 0.00081814
Iteration 4/25 | Loss: 0.00081814
Iteration 5/25 | Loss: 0.00081814
Iteration 6/25 | Loss: 0.00081814
Iteration 7/25 | Loss: 0.00081813
Iteration 8/25 | Loss: 0.00081813
Iteration 9/25 | Loss: 0.00081813
Iteration 10/25 | Loss: 0.00081813
Iteration 11/25 | Loss: 0.00081813
Iteration 12/25 | Loss: 0.00081813
Iteration 13/25 | Loss: 0.00081813
Iteration 14/25 | Loss: 0.00081813
Iteration 15/25 | Loss: 0.00081813
Iteration 16/25 | Loss: 0.00081813
Iteration 17/25 | Loss: 0.00081813
Iteration 18/25 | Loss: 0.00081813
Iteration 19/25 | Loss: 0.00081813
Iteration 20/25 | Loss: 0.00081813
Iteration 21/25 | Loss: 0.00081813
Iteration 22/25 | Loss: 0.00081813
Iteration 23/25 | Loss: 0.00081813
Iteration 24/25 | Loss: 0.00081813
Iteration 25/25 | Loss: 0.00081813

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081813
Iteration 2/1000 | Loss: 0.00002005
Iteration 3/1000 | Loss: 0.00001286
Iteration 4/1000 | Loss: 0.00001017
Iteration 5/1000 | Loss: 0.00000947
Iteration 6/1000 | Loss: 0.00000887
Iteration 7/1000 | Loss: 0.00000854
Iteration 8/1000 | Loss: 0.00000843
Iteration 9/1000 | Loss: 0.00000833
Iteration 10/1000 | Loss: 0.00000831
Iteration 11/1000 | Loss: 0.00000830
Iteration 12/1000 | Loss: 0.00000823
Iteration 13/1000 | Loss: 0.00000814
Iteration 14/1000 | Loss: 0.00000811
Iteration 15/1000 | Loss: 0.00000799
Iteration 16/1000 | Loss: 0.00000793
Iteration 17/1000 | Loss: 0.00000792
Iteration 18/1000 | Loss: 0.00000792
Iteration 19/1000 | Loss: 0.00000792
Iteration 20/1000 | Loss: 0.00000792
Iteration 21/1000 | Loss: 0.00000791
Iteration 22/1000 | Loss: 0.00000791
Iteration 23/1000 | Loss: 0.00000790
Iteration 24/1000 | Loss: 0.00000790
Iteration 25/1000 | Loss: 0.00000789
Iteration 26/1000 | Loss: 0.00000782
Iteration 27/1000 | Loss: 0.00000782
Iteration 28/1000 | Loss: 0.00000780
Iteration 29/1000 | Loss: 0.00000779
Iteration 30/1000 | Loss: 0.00000778
Iteration 31/1000 | Loss: 0.00000778
Iteration 32/1000 | Loss: 0.00000778
Iteration 33/1000 | Loss: 0.00000777
Iteration 34/1000 | Loss: 0.00000772
Iteration 35/1000 | Loss: 0.00000772
Iteration 36/1000 | Loss: 0.00000772
Iteration 37/1000 | Loss: 0.00000771
Iteration 38/1000 | Loss: 0.00000771
Iteration 39/1000 | Loss: 0.00000771
Iteration 40/1000 | Loss: 0.00000770
Iteration 41/1000 | Loss: 0.00000767
Iteration 42/1000 | Loss: 0.00000767
Iteration 43/1000 | Loss: 0.00000765
Iteration 44/1000 | Loss: 0.00000765
Iteration 45/1000 | Loss: 0.00000765
Iteration 46/1000 | Loss: 0.00000764
Iteration 47/1000 | Loss: 0.00000764
Iteration 48/1000 | Loss: 0.00000762
Iteration 49/1000 | Loss: 0.00000762
Iteration 50/1000 | Loss: 0.00000762
Iteration 51/1000 | Loss: 0.00000762
Iteration 52/1000 | Loss: 0.00000762
Iteration 53/1000 | Loss: 0.00000762
Iteration 54/1000 | Loss: 0.00000762
Iteration 55/1000 | Loss: 0.00000762
Iteration 56/1000 | Loss: 0.00000761
Iteration 57/1000 | Loss: 0.00000761
Iteration 58/1000 | Loss: 0.00000760
Iteration 59/1000 | Loss: 0.00000760
Iteration 60/1000 | Loss: 0.00000760
Iteration 61/1000 | Loss: 0.00000760
Iteration 62/1000 | Loss: 0.00000760
Iteration 63/1000 | Loss: 0.00000759
Iteration 64/1000 | Loss: 0.00000759
Iteration 65/1000 | Loss: 0.00000758
Iteration 66/1000 | Loss: 0.00000758
Iteration 67/1000 | Loss: 0.00000758
Iteration 68/1000 | Loss: 0.00000758
Iteration 69/1000 | Loss: 0.00000758
Iteration 70/1000 | Loss: 0.00000758
Iteration 71/1000 | Loss: 0.00000757
Iteration 72/1000 | Loss: 0.00000757
Iteration 73/1000 | Loss: 0.00000756
Iteration 74/1000 | Loss: 0.00000756
Iteration 75/1000 | Loss: 0.00000756
Iteration 76/1000 | Loss: 0.00000756
Iteration 77/1000 | Loss: 0.00000756
Iteration 78/1000 | Loss: 0.00000756
Iteration 79/1000 | Loss: 0.00000756
Iteration 80/1000 | Loss: 0.00000755
Iteration 81/1000 | Loss: 0.00000755
Iteration 82/1000 | Loss: 0.00000755
Iteration 83/1000 | Loss: 0.00000755
Iteration 84/1000 | Loss: 0.00000754
Iteration 85/1000 | Loss: 0.00000754
Iteration 86/1000 | Loss: 0.00000754
Iteration 87/1000 | Loss: 0.00000753
Iteration 88/1000 | Loss: 0.00000753
Iteration 89/1000 | Loss: 0.00000753
Iteration 90/1000 | Loss: 0.00000753
Iteration 91/1000 | Loss: 0.00000753
Iteration 92/1000 | Loss: 0.00000753
Iteration 93/1000 | Loss: 0.00000753
Iteration 94/1000 | Loss: 0.00000753
Iteration 95/1000 | Loss: 0.00000753
Iteration 96/1000 | Loss: 0.00000752
Iteration 97/1000 | Loss: 0.00000752
Iteration 98/1000 | Loss: 0.00000752
Iteration 99/1000 | Loss: 0.00000752
Iteration 100/1000 | Loss: 0.00000752
Iteration 101/1000 | Loss: 0.00000752
Iteration 102/1000 | Loss: 0.00000752
Iteration 103/1000 | Loss: 0.00000752
Iteration 104/1000 | Loss: 0.00000752
Iteration 105/1000 | Loss: 0.00000751
Iteration 106/1000 | Loss: 0.00000751
Iteration 107/1000 | Loss: 0.00000751
Iteration 108/1000 | Loss: 0.00000751
Iteration 109/1000 | Loss: 0.00000751
Iteration 110/1000 | Loss: 0.00000750
Iteration 111/1000 | Loss: 0.00000750
Iteration 112/1000 | Loss: 0.00000750
Iteration 113/1000 | Loss: 0.00000750
Iteration 114/1000 | Loss: 0.00000749
Iteration 115/1000 | Loss: 0.00000749
Iteration 116/1000 | Loss: 0.00000749
Iteration 117/1000 | Loss: 0.00000748
Iteration 118/1000 | Loss: 0.00000748
Iteration 119/1000 | Loss: 0.00000747
Iteration 120/1000 | Loss: 0.00000747
Iteration 121/1000 | Loss: 0.00000747
Iteration 122/1000 | Loss: 0.00000747
Iteration 123/1000 | Loss: 0.00000747
Iteration 124/1000 | Loss: 0.00000746
Iteration 125/1000 | Loss: 0.00000746
Iteration 126/1000 | Loss: 0.00000745
Iteration 127/1000 | Loss: 0.00000745
Iteration 128/1000 | Loss: 0.00000745
Iteration 129/1000 | Loss: 0.00000744
Iteration 130/1000 | Loss: 0.00000744
Iteration 131/1000 | Loss: 0.00000744
Iteration 132/1000 | Loss: 0.00000744
Iteration 133/1000 | Loss: 0.00000744
Iteration 134/1000 | Loss: 0.00000744
Iteration 135/1000 | Loss: 0.00000744
Iteration 136/1000 | Loss: 0.00000744
Iteration 137/1000 | Loss: 0.00000744
Iteration 138/1000 | Loss: 0.00000744
Iteration 139/1000 | Loss: 0.00000744
Iteration 140/1000 | Loss: 0.00000744
Iteration 141/1000 | Loss: 0.00000744
Iteration 142/1000 | Loss: 0.00000744
Iteration 143/1000 | Loss: 0.00000743
Iteration 144/1000 | Loss: 0.00000743
Iteration 145/1000 | Loss: 0.00000743
Iteration 146/1000 | Loss: 0.00000743
Iteration 147/1000 | Loss: 0.00000743
Iteration 148/1000 | Loss: 0.00000743
Iteration 149/1000 | Loss: 0.00000743
Iteration 150/1000 | Loss: 0.00000743
Iteration 151/1000 | Loss: 0.00000743
Iteration 152/1000 | Loss: 0.00000742
Iteration 153/1000 | Loss: 0.00000742
Iteration 154/1000 | Loss: 0.00000742
Iteration 155/1000 | Loss: 0.00000742
Iteration 156/1000 | Loss: 0.00000742
Iteration 157/1000 | Loss: 0.00000742
Iteration 158/1000 | Loss: 0.00000742
Iteration 159/1000 | Loss: 0.00000742
Iteration 160/1000 | Loss: 0.00000742
Iteration 161/1000 | Loss: 0.00000741
Iteration 162/1000 | Loss: 0.00000741
Iteration 163/1000 | Loss: 0.00000741
Iteration 164/1000 | Loss: 0.00000741
Iteration 165/1000 | Loss: 0.00000741
Iteration 166/1000 | Loss: 0.00000741
Iteration 167/1000 | Loss: 0.00000741
Iteration 168/1000 | Loss: 0.00000741
Iteration 169/1000 | Loss: 0.00000741
Iteration 170/1000 | Loss: 0.00000741
Iteration 171/1000 | Loss: 0.00000740
Iteration 172/1000 | Loss: 0.00000740
Iteration 173/1000 | Loss: 0.00000740
Iteration 174/1000 | Loss: 0.00000740
Iteration 175/1000 | Loss: 0.00000740
Iteration 176/1000 | Loss: 0.00000739
Iteration 177/1000 | Loss: 0.00000739
Iteration 178/1000 | Loss: 0.00000739
Iteration 179/1000 | Loss: 0.00000739
Iteration 180/1000 | Loss: 0.00000739
Iteration 181/1000 | Loss: 0.00000739
Iteration 182/1000 | Loss: 0.00000738
Iteration 183/1000 | Loss: 0.00000738
Iteration 184/1000 | Loss: 0.00000738
Iteration 185/1000 | Loss: 0.00000738
Iteration 186/1000 | Loss: 0.00000738
Iteration 187/1000 | Loss: 0.00000738
Iteration 188/1000 | Loss: 0.00000738
Iteration 189/1000 | Loss: 0.00000738
Iteration 190/1000 | Loss: 0.00000738
Iteration 191/1000 | Loss: 0.00000738
Iteration 192/1000 | Loss: 0.00000737
Iteration 193/1000 | Loss: 0.00000737
Iteration 194/1000 | Loss: 0.00000737
Iteration 195/1000 | Loss: 0.00000737
Iteration 196/1000 | Loss: 0.00000737
Iteration 197/1000 | Loss: 0.00000737
Iteration 198/1000 | Loss: 0.00000737
Iteration 199/1000 | Loss: 0.00000737
Iteration 200/1000 | Loss: 0.00000737
Iteration 201/1000 | Loss: 0.00000737
Iteration 202/1000 | Loss: 0.00000737
Iteration 203/1000 | Loss: 0.00000737
Iteration 204/1000 | Loss: 0.00000737
Iteration 205/1000 | Loss: 0.00000737
Iteration 206/1000 | Loss: 0.00000737
Iteration 207/1000 | Loss: 0.00000737
Iteration 208/1000 | Loss: 0.00000737
Iteration 209/1000 | Loss: 0.00000737
Iteration 210/1000 | Loss: 0.00000737
Iteration 211/1000 | Loss: 0.00000737
Iteration 212/1000 | Loss: 0.00000736
Iteration 213/1000 | Loss: 0.00000736
Iteration 214/1000 | Loss: 0.00000736
Iteration 215/1000 | Loss: 0.00000736
Iteration 216/1000 | Loss: 0.00000736
Iteration 217/1000 | Loss: 0.00000736
Iteration 218/1000 | Loss: 0.00000736
Iteration 219/1000 | Loss: 0.00000736
Iteration 220/1000 | Loss: 0.00000736
Iteration 221/1000 | Loss: 0.00000736
Iteration 222/1000 | Loss: 0.00000736
Iteration 223/1000 | Loss: 0.00000736
Iteration 224/1000 | Loss: 0.00000736
Iteration 225/1000 | Loss: 0.00000736
Iteration 226/1000 | Loss: 0.00000736
Iteration 227/1000 | Loss: 0.00000736
Iteration 228/1000 | Loss: 0.00000736
Iteration 229/1000 | Loss: 0.00000736
Iteration 230/1000 | Loss: 0.00000736
Iteration 231/1000 | Loss: 0.00000736
Iteration 232/1000 | Loss: 0.00000736
Iteration 233/1000 | Loss: 0.00000736
Iteration 234/1000 | Loss: 0.00000736
Iteration 235/1000 | Loss: 0.00000736
Iteration 236/1000 | Loss: 0.00000736
Iteration 237/1000 | Loss: 0.00000736
Iteration 238/1000 | Loss: 0.00000736
Iteration 239/1000 | Loss: 0.00000736
Iteration 240/1000 | Loss: 0.00000736
Iteration 241/1000 | Loss: 0.00000736
Iteration 242/1000 | Loss: 0.00000736
Iteration 243/1000 | Loss: 0.00000736
Iteration 244/1000 | Loss: 0.00000736
Iteration 245/1000 | Loss: 0.00000736
Iteration 246/1000 | Loss: 0.00000736
Iteration 247/1000 | Loss: 0.00000736
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 247. Stopping optimization.
Last 5 losses: [7.36345009499928e-06, 7.36345009499928e-06, 7.36345009499928e-06, 7.36345009499928e-06, 7.36345009499928e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.36345009499928e-06

Optimization complete. Final v2v error: 2.3536834716796875 mm

Highest mean error: 2.5229320526123047 mm for frame 110

Lowest mean error: 2.300475835800171 mm for frame 126

Saving results

Total time: 39.527658462524414
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00815057
Iteration 2/25 | Loss: 0.00139786
Iteration 3/25 | Loss: 0.00116865
Iteration 4/25 | Loss: 0.00114989
Iteration 5/25 | Loss: 0.00114659
Iteration 6/25 | Loss: 0.00114625
Iteration 7/25 | Loss: 0.00114625
Iteration 8/25 | Loss: 0.00114625
Iteration 9/25 | Loss: 0.00114625
Iteration 10/25 | Loss: 0.00114625
Iteration 11/25 | Loss: 0.00114625
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001146249589510262, 0.001146249589510262, 0.001146249589510262, 0.001146249589510262, 0.001146249589510262]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001146249589510262

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.90569353
Iteration 2/25 | Loss: 0.00048430
Iteration 3/25 | Loss: 0.00048430
Iteration 4/25 | Loss: 0.00048430
Iteration 5/25 | Loss: 0.00048430
Iteration 6/25 | Loss: 0.00048430
Iteration 7/25 | Loss: 0.00048430
Iteration 8/25 | Loss: 0.00048430
Iteration 9/25 | Loss: 0.00048430
Iteration 10/25 | Loss: 0.00048430
Iteration 11/25 | Loss: 0.00048430
Iteration 12/25 | Loss: 0.00048430
Iteration 13/25 | Loss: 0.00048430
Iteration 14/25 | Loss: 0.00048430
Iteration 15/25 | Loss: 0.00048430
Iteration 16/25 | Loss: 0.00048430
Iteration 17/25 | Loss: 0.00048430
Iteration 18/25 | Loss: 0.00048430
Iteration 19/25 | Loss: 0.00048430
Iteration 20/25 | Loss: 0.00048430
Iteration 21/25 | Loss: 0.00048430
Iteration 22/25 | Loss: 0.00048430
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0004842965572606772, 0.0004842965572606772, 0.0004842965572606772, 0.0004842965572606772, 0.0004842965572606772]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004842965572606772

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048430
Iteration 2/1000 | Loss: 0.00002968
Iteration 3/1000 | Loss: 0.00002328
Iteration 4/1000 | Loss: 0.00002158
Iteration 5/1000 | Loss: 0.00002075
Iteration 6/1000 | Loss: 0.00002021
Iteration 7/1000 | Loss: 0.00001993
Iteration 8/1000 | Loss: 0.00001973
Iteration 9/1000 | Loss: 0.00001950
Iteration 10/1000 | Loss: 0.00001931
Iteration 11/1000 | Loss: 0.00001928
Iteration 12/1000 | Loss: 0.00001928
Iteration 13/1000 | Loss: 0.00001926
Iteration 14/1000 | Loss: 0.00001923
Iteration 15/1000 | Loss: 0.00001922
Iteration 16/1000 | Loss: 0.00001922
Iteration 17/1000 | Loss: 0.00001922
Iteration 18/1000 | Loss: 0.00001922
Iteration 19/1000 | Loss: 0.00001922
Iteration 20/1000 | Loss: 0.00001922
Iteration 21/1000 | Loss: 0.00001921
Iteration 22/1000 | Loss: 0.00001917
Iteration 23/1000 | Loss: 0.00001915
Iteration 24/1000 | Loss: 0.00001913
Iteration 25/1000 | Loss: 0.00001913
Iteration 26/1000 | Loss: 0.00001913
Iteration 27/1000 | Loss: 0.00001912
Iteration 28/1000 | Loss: 0.00001911
Iteration 29/1000 | Loss: 0.00001908
Iteration 30/1000 | Loss: 0.00001908
Iteration 31/1000 | Loss: 0.00001908
Iteration 32/1000 | Loss: 0.00001908
Iteration 33/1000 | Loss: 0.00001908
Iteration 34/1000 | Loss: 0.00001907
Iteration 35/1000 | Loss: 0.00001902
Iteration 36/1000 | Loss: 0.00001900
Iteration 37/1000 | Loss: 0.00001900
Iteration 38/1000 | Loss: 0.00001900
Iteration 39/1000 | Loss: 0.00001900
Iteration 40/1000 | Loss: 0.00001900
Iteration 41/1000 | Loss: 0.00001900
Iteration 42/1000 | Loss: 0.00001899
Iteration 43/1000 | Loss: 0.00001899
Iteration 44/1000 | Loss: 0.00001899
Iteration 45/1000 | Loss: 0.00001899
Iteration 46/1000 | Loss: 0.00001898
Iteration 47/1000 | Loss: 0.00001898
Iteration 48/1000 | Loss: 0.00001898
Iteration 49/1000 | Loss: 0.00001898
Iteration 50/1000 | Loss: 0.00001898
Iteration 51/1000 | Loss: 0.00001898
Iteration 52/1000 | Loss: 0.00001897
Iteration 53/1000 | Loss: 0.00001897
Iteration 54/1000 | Loss: 0.00001897
Iteration 55/1000 | Loss: 0.00001897
Iteration 56/1000 | Loss: 0.00001897
Iteration 57/1000 | Loss: 0.00001897
Iteration 58/1000 | Loss: 0.00001897
Iteration 59/1000 | Loss: 0.00001897
Iteration 60/1000 | Loss: 0.00001897
Iteration 61/1000 | Loss: 0.00001896
Iteration 62/1000 | Loss: 0.00001896
Iteration 63/1000 | Loss: 0.00001893
Iteration 64/1000 | Loss: 0.00001892
Iteration 65/1000 | Loss: 0.00001892
Iteration 66/1000 | Loss: 0.00001892
Iteration 67/1000 | Loss: 0.00001892
Iteration 68/1000 | Loss: 0.00001892
Iteration 69/1000 | Loss: 0.00001892
Iteration 70/1000 | Loss: 0.00001891
Iteration 71/1000 | Loss: 0.00001891
Iteration 72/1000 | Loss: 0.00001891
Iteration 73/1000 | Loss: 0.00001891
Iteration 74/1000 | Loss: 0.00001890
Iteration 75/1000 | Loss: 0.00001890
Iteration 76/1000 | Loss: 0.00001890
Iteration 77/1000 | Loss: 0.00001890
Iteration 78/1000 | Loss: 0.00001890
Iteration 79/1000 | Loss: 0.00001890
Iteration 80/1000 | Loss: 0.00001889
Iteration 81/1000 | Loss: 0.00001889
Iteration 82/1000 | Loss: 0.00001889
Iteration 83/1000 | Loss: 0.00001889
Iteration 84/1000 | Loss: 0.00001889
Iteration 85/1000 | Loss: 0.00001888
Iteration 86/1000 | Loss: 0.00001888
Iteration 87/1000 | Loss: 0.00001887
Iteration 88/1000 | Loss: 0.00001887
Iteration 89/1000 | Loss: 0.00001887
Iteration 90/1000 | Loss: 0.00001887
Iteration 91/1000 | Loss: 0.00001887
Iteration 92/1000 | Loss: 0.00001885
Iteration 93/1000 | Loss: 0.00001885
Iteration 94/1000 | Loss: 0.00001885
Iteration 95/1000 | Loss: 0.00001885
Iteration 96/1000 | Loss: 0.00001885
Iteration 97/1000 | Loss: 0.00001885
Iteration 98/1000 | Loss: 0.00001885
Iteration 99/1000 | Loss: 0.00001885
Iteration 100/1000 | Loss: 0.00001884
Iteration 101/1000 | Loss: 0.00001884
Iteration 102/1000 | Loss: 0.00001884
Iteration 103/1000 | Loss: 0.00001883
Iteration 104/1000 | Loss: 0.00001883
Iteration 105/1000 | Loss: 0.00001883
Iteration 106/1000 | Loss: 0.00001883
Iteration 107/1000 | Loss: 0.00001882
Iteration 108/1000 | Loss: 0.00001882
Iteration 109/1000 | Loss: 0.00001882
Iteration 110/1000 | Loss: 0.00001882
Iteration 111/1000 | Loss: 0.00001882
Iteration 112/1000 | Loss: 0.00001882
Iteration 113/1000 | Loss: 0.00001882
Iteration 114/1000 | Loss: 0.00001882
Iteration 115/1000 | Loss: 0.00001882
Iteration 116/1000 | Loss: 0.00001882
Iteration 117/1000 | Loss: 0.00001882
Iteration 118/1000 | Loss: 0.00001881
Iteration 119/1000 | Loss: 0.00001881
Iteration 120/1000 | Loss: 0.00001881
Iteration 121/1000 | Loss: 0.00001881
Iteration 122/1000 | Loss: 0.00001881
Iteration 123/1000 | Loss: 0.00001881
Iteration 124/1000 | Loss: 0.00001881
Iteration 125/1000 | Loss: 0.00001881
Iteration 126/1000 | Loss: 0.00001881
Iteration 127/1000 | Loss: 0.00001881
Iteration 128/1000 | Loss: 0.00001880
Iteration 129/1000 | Loss: 0.00001880
Iteration 130/1000 | Loss: 0.00001880
Iteration 131/1000 | Loss: 0.00001880
Iteration 132/1000 | Loss: 0.00001879
Iteration 133/1000 | Loss: 0.00001879
Iteration 134/1000 | Loss: 0.00001879
Iteration 135/1000 | Loss: 0.00001878
Iteration 136/1000 | Loss: 0.00001878
Iteration 137/1000 | Loss: 0.00001878
Iteration 138/1000 | Loss: 0.00001878
Iteration 139/1000 | Loss: 0.00001878
Iteration 140/1000 | Loss: 0.00001878
Iteration 141/1000 | Loss: 0.00001878
Iteration 142/1000 | Loss: 0.00001878
Iteration 143/1000 | Loss: 0.00001878
Iteration 144/1000 | Loss: 0.00001878
Iteration 145/1000 | Loss: 0.00001878
Iteration 146/1000 | Loss: 0.00001877
Iteration 147/1000 | Loss: 0.00001877
Iteration 148/1000 | Loss: 0.00001877
Iteration 149/1000 | Loss: 0.00001877
Iteration 150/1000 | Loss: 0.00001877
Iteration 151/1000 | Loss: 0.00001877
Iteration 152/1000 | Loss: 0.00001877
Iteration 153/1000 | Loss: 0.00001877
Iteration 154/1000 | Loss: 0.00001877
Iteration 155/1000 | Loss: 0.00001877
Iteration 156/1000 | Loss: 0.00001877
Iteration 157/1000 | Loss: 0.00001877
Iteration 158/1000 | Loss: 0.00001876
Iteration 159/1000 | Loss: 0.00001876
Iteration 160/1000 | Loss: 0.00001876
Iteration 161/1000 | Loss: 0.00001876
Iteration 162/1000 | Loss: 0.00001876
Iteration 163/1000 | Loss: 0.00001876
Iteration 164/1000 | Loss: 0.00001876
Iteration 165/1000 | Loss: 0.00001876
Iteration 166/1000 | Loss: 0.00001876
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [1.876383066701237e-05, 1.876383066701237e-05, 1.876383066701237e-05, 1.876383066701237e-05, 1.876383066701237e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.876383066701237e-05

Optimization complete. Final v2v error: 3.6063168048858643 mm

Highest mean error: 3.801255464553833 mm for frame 121

Lowest mean error: 3.489642858505249 mm for frame 135

Saving results

Total time: 35.27651119232178
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01062574
Iteration 2/25 | Loss: 0.00454591
Iteration 3/25 | Loss: 0.00531913
Iteration 4/25 | Loss: 0.00230903
Iteration 5/25 | Loss: 0.00219993
Iteration 6/25 | Loss: 0.00197107
Iteration 7/25 | Loss: 0.00186434
Iteration 8/25 | Loss: 0.00184697
Iteration 9/25 | Loss: 0.00178039
Iteration 10/25 | Loss: 0.00174016
Iteration 11/25 | Loss: 0.00168672
Iteration 12/25 | Loss: 0.00164863
Iteration 13/25 | Loss: 0.00167931
Iteration 14/25 | Loss: 0.00149193
Iteration 15/25 | Loss: 0.00147383
Iteration 16/25 | Loss: 0.00144694
Iteration 17/25 | Loss: 0.00142925
Iteration 18/25 | Loss: 0.00142325
Iteration 19/25 | Loss: 0.00140519
Iteration 20/25 | Loss: 0.00139868
Iteration 21/25 | Loss: 0.00139894
Iteration 22/25 | Loss: 0.00138480
Iteration 23/25 | Loss: 0.00138279
Iteration 24/25 | Loss: 0.00138210
Iteration 25/25 | Loss: 0.00138174

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.57183266
Iteration 2/25 | Loss: 0.00276809
Iteration 3/25 | Loss: 0.00276809
Iteration 4/25 | Loss: 0.00276809
Iteration 5/25 | Loss: 0.00276809
Iteration 6/25 | Loss: 0.00276809
Iteration 7/25 | Loss: 0.00276809
Iteration 8/25 | Loss: 0.00276809
Iteration 9/25 | Loss: 0.00276809
Iteration 10/25 | Loss: 0.00276809
Iteration 11/25 | Loss: 0.00276808
Iteration 12/25 | Loss: 0.00276808
Iteration 13/25 | Loss: 0.00276808
Iteration 14/25 | Loss: 0.00276808
Iteration 15/25 | Loss: 0.00276808
Iteration 16/25 | Loss: 0.00276808
Iteration 17/25 | Loss: 0.00276808
Iteration 18/25 | Loss: 0.00276808
Iteration 19/25 | Loss: 0.00276808
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.002768084406852722, 0.002768084406852722, 0.002768084406852722, 0.002768084406852722, 0.002768084406852722]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002768084406852722

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00276808
Iteration 2/1000 | Loss: 0.00033177
Iteration 3/1000 | Loss: 0.00023424
Iteration 4/1000 | Loss: 0.00018361
Iteration 5/1000 | Loss: 0.00015762
Iteration 6/1000 | Loss: 0.00028821
Iteration 7/1000 | Loss: 0.00014740
Iteration 8/1000 | Loss: 0.00038151
Iteration 9/1000 | Loss: 0.00017684
Iteration 10/1000 | Loss: 0.00015789
Iteration 11/1000 | Loss: 0.00012713
Iteration 12/1000 | Loss: 0.00012143
Iteration 13/1000 | Loss: 0.00011706
Iteration 14/1000 | Loss: 0.00015910
Iteration 15/1000 | Loss: 0.00011579
Iteration 16/1000 | Loss: 0.00011266
Iteration 17/1000 | Loss: 0.00011011
Iteration 18/1000 | Loss: 0.00010829
Iteration 19/1000 | Loss: 0.00010743
Iteration 20/1000 | Loss: 0.00010664
Iteration 21/1000 | Loss: 0.00010580
Iteration 22/1000 | Loss: 0.00010537
Iteration 23/1000 | Loss: 0.00018349
Iteration 24/1000 | Loss: 0.00020435
Iteration 25/1000 | Loss: 0.00056096
Iteration 26/1000 | Loss: 0.00025407
Iteration 27/1000 | Loss: 0.00013443
Iteration 28/1000 | Loss: 0.00012016
Iteration 29/1000 | Loss: 0.00010438
Iteration 30/1000 | Loss: 0.00009577
Iteration 31/1000 | Loss: 0.00009164
Iteration 32/1000 | Loss: 0.00008870
Iteration 33/1000 | Loss: 0.00008708
Iteration 34/1000 | Loss: 0.00008571
Iteration 35/1000 | Loss: 0.00008481
Iteration 36/1000 | Loss: 0.00008400
Iteration 37/1000 | Loss: 0.00008343
Iteration 38/1000 | Loss: 0.00008310
Iteration 39/1000 | Loss: 0.00008269
Iteration 40/1000 | Loss: 0.00008228
Iteration 41/1000 | Loss: 0.00008183
Iteration 42/1000 | Loss: 0.00018076
Iteration 43/1000 | Loss: 0.00016455
Iteration 44/1000 | Loss: 0.00010611
Iteration 45/1000 | Loss: 0.00008561
Iteration 46/1000 | Loss: 0.00008231
Iteration 47/1000 | Loss: 0.00007815
Iteration 48/1000 | Loss: 0.00007543
Iteration 49/1000 | Loss: 0.00007242
Iteration 50/1000 | Loss: 0.00007083
Iteration 51/1000 | Loss: 0.00006922
Iteration 52/1000 | Loss: 0.00027208
Iteration 53/1000 | Loss: 0.00056150
Iteration 54/1000 | Loss: 0.00117509
Iteration 55/1000 | Loss: 0.00065379
Iteration 56/1000 | Loss: 0.00017338
Iteration 57/1000 | Loss: 0.00020271
Iteration 58/1000 | Loss: 0.00015361
Iteration 59/1000 | Loss: 0.00012400
Iteration 60/1000 | Loss: 0.00031819
Iteration 61/1000 | Loss: 0.00014248
Iteration 62/1000 | Loss: 0.00004801
Iteration 63/1000 | Loss: 0.00003963
Iteration 64/1000 | Loss: 0.00003468
Iteration 65/1000 | Loss: 0.00003112
Iteration 66/1000 | Loss: 0.00002824
Iteration 67/1000 | Loss: 0.00002604
Iteration 68/1000 | Loss: 0.00002446
Iteration 69/1000 | Loss: 0.00002359
Iteration 70/1000 | Loss: 0.00002300
Iteration 71/1000 | Loss: 0.00002271
Iteration 72/1000 | Loss: 0.00002252
Iteration 73/1000 | Loss: 0.00002250
Iteration 74/1000 | Loss: 0.00002230
Iteration 75/1000 | Loss: 0.00002228
Iteration 76/1000 | Loss: 0.00002224
Iteration 77/1000 | Loss: 0.00002213
Iteration 78/1000 | Loss: 0.00002211
Iteration 79/1000 | Loss: 0.00002211
Iteration 80/1000 | Loss: 0.00002210
Iteration 81/1000 | Loss: 0.00002206
Iteration 82/1000 | Loss: 0.00003656
Iteration 83/1000 | Loss: 0.00002350
Iteration 84/1000 | Loss: 0.00002256
Iteration 85/1000 | Loss: 0.00002200
Iteration 86/1000 | Loss: 0.00002185
Iteration 87/1000 | Loss: 0.00002184
Iteration 88/1000 | Loss: 0.00002184
Iteration 89/1000 | Loss: 0.00002184
Iteration 90/1000 | Loss: 0.00002184
Iteration 91/1000 | Loss: 0.00002184
Iteration 92/1000 | Loss: 0.00002183
Iteration 93/1000 | Loss: 0.00002183
Iteration 94/1000 | Loss: 0.00002183
Iteration 95/1000 | Loss: 0.00002183
Iteration 96/1000 | Loss: 0.00002183
Iteration 97/1000 | Loss: 0.00002183
Iteration 98/1000 | Loss: 0.00002183
Iteration 99/1000 | Loss: 0.00002182
Iteration 100/1000 | Loss: 0.00002182
Iteration 101/1000 | Loss: 0.00002182
Iteration 102/1000 | Loss: 0.00002176
Iteration 103/1000 | Loss: 0.00002173
Iteration 104/1000 | Loss: 0.00002173
Iteration 105/1000 | Loss: 0.00002172
Iteration 106/1000 | Loss: 0.00002172
Iteration 107/1000 | Loss: 0.00002172
Iteration 108/1000 | Loss: 0.00002172
Iteration 109/1000 | Loss: 0.00002172
Iteration 110/1000 | Loss: 0.00002172
Iteration 111/1000 | Loss: 0.00002172
Iteration 112/1000 | Loss: 0.00002172
Iteration 113/1000 | Loss: 0.00002171
Iteration 114/1000 | Loss: 0.00002171
Iteration 115/1000 | Loss: 0.00002171
Iteration 116/1000 | Loss: 0.00002171
Iteration 117/1000 | Loss: 0.00002170
Iteration 118/1000 | Loss: 0.00002170
Iteration 119/1000 | Loss: 0.00002170
Iteration 120/1000 | Loss: 0.00002170
Iteration 121/1000 | Loss: 0.00002170
Iteration 122/1000 | Loss: 0.00002170
Iteration 123/1000 | Loss: 0.00002170
Iteration 124/1000 | Loss: 0.00002170
Iteration 125/1000 | Loss: 0.00002169
Iteration 126/1000 | Loss: 0.00002169
Iteration 127/1000 | Loss: 0.00002169
Iteration 128/1000 | Loss: 0.00002169
Iteration 129/1000 | Loss: 0.00002169
Iteration 130/1000 | Loss: 0.00002169
Iteration 131/1000 | Loss: 0.00002167
Iteration 132/1000 | Loss: 0.00002164
Iteration 133/1000 | Loss: 0.00002164
Iteration 134/1000 | Loss: 0.00002164
Iteration 135/1000 | Loss: 0.00002164
Iteration 136/1000 | Loss: 0.00002163
Iteration 137/1000 | Loss: 0.00002163
Iteration 138/1000 | Loss: 0.00002163
Iteration 139/1000 | Loss: 0.00002162
Iteration 140/1000 | Loss: 0.00002162
Iteration 141/1000 | Loss: 0.00002162
Iteration 142/1000 | Loss: 0.00002162
Iteration 143/1000 | Loss: 0.00002162
Iteration 144/1000 | Loss: 0.00002161
Iteration 145/1000 | Loss: 0.00002161
Iteration 146/1000 | Loss: 0.00002161
Iteration 147/1000 | Loss: 0.00002161
Iteration 148/1000 | Loss: 0.00002161
Iteration 149/1000 | Loss: 0.00002161
Iteration 150/1000 | Loss: 0.00002161
Iteration 151/1000 | Loss: 0.00002161
Iteration 152/1000 | Loss: 0.00002161
Iteration 153/1000 | Loss: 0.00002160
Iteration 154/1000 | Loss: 0.00002160
Iteration 155/1000 | Loss: 0.00002160
Iteration 156/1000 | Loss: 0.00002159
Iteration 157/1000 | Loss: 0.00002159
Iteration 158/1000 | Loss: 0.00002158
Iteration 159/1000 | Loss: 0.00002158
Iteration 160/1000 | Loss: 0.00002156
Iteration 161/1000 | Loss: 0.00002154
Iteration 162/1000 | Loss: 0.00002154
Iteration 163/1000 | Loss: 0.00002154
Iteration 164/1000 | Loss: 0.00002154
Iteration 165/1000 | Loss: 0.00002154
Iteration 166/1000 | Loss: 0.00002154
Iteration 167/1000 | Loss: 0.00002154
Iteration 168/1000 | Loss: 0.00002154
Iteration 169/1000 | Loss: 0.00002154
Iteration 170/1000 | Loss: 0.00002154
Iteration 171/1000 | Loss: 0.00002154
Iteration 172/1000 | Loss: 0.00002153
Iteration 173/1000 | Loss: 0.00002153
Iteration 174/1000 | Loss: 0.00002153
Iteration 175/1000 | Loss: 0.00002152
Iteration 176/1000 | Loss: 0.00002152
Iteration 177/1000 | Loss: 0.00002152
Iteration 178/1000 | Loss: 0.00002151
Iteration 179/1000 | Loss: 0.00002151
Iteration 180/1000 | Loss: 0.00002151
Iteration 181/1000 | Loss: 0.00002151
Iteration 182/1000 | Loss: 0.00002151
Iteration 183/1000 | Loss: 0.00002151
Iteration 184/1000 | Loss: 0.00002151
Iteration 185/1000 | Loss: 0.00002151
Iteration 186/1000 | Loss: 0.00002151
Iteration 187/1000 | Loss: 0.00002151
Iteration 188/1000 | Loss: 0.00002151
Iteration 189/1000 | Loss: 0.00002150
Iteration 190/1000 | Loss: 0.00002150
Iteration 191/1000 | Loss: 0.00002149
Iteration 192/1000 | Loss: 0.00002149
Iteration 193/1000 | Loss: 0.00002149
Iteration 194/1000 | Loss: 0.00002149
Iteration 195/1000 | Loss: 0.00002149
Iteration 196/1000 | Loss: 0.00002149
Iteration 197/1000 | Loss: 0.00002149
Iteration 198/1000 | Loss: 0.00002149
Iteration 199/1000 | Loss: 0.00002149
Iteration 200/1000 | Loss: 0.00002149
Iteration 201/1000 | Loss: 0.00002149
Iteration 202/1000 | Loss: 0.00002148
Iteration 203/1000 | Loss: 0.00002148
Iteration 204/1000 | Loss: 0.00002148
Iteration 205/1000 | Loss: 0.00002148
Iteration 206/1000 | Loss: 0.00002148
Iteration 207/1000 | Loss: 0.00002148
Iteration 208/1000 | Loss: 0.00002148
Iteration 209/1000 | Loss: 0.00002148
Iteration 210/1000 | Loss: 0.00002147
Iteration 211/1000 | Loss: 0.00002147
Iteration 212/1000 | Loss: 0.00002147
Iteration 213/1000 | Loss: 0.00002147
Iteration 214/1000 | Loss: 0.00002147
Iteration 215/1000 | Loss: 0.00002147
Iteration 216/1000 | Loss: 0.00002147
Iteration 217/1000 | Loss: 0.00002147
Iteration 218/1000 | Loss: 0.00002147
Iteration 219/1000 | Loss: 0.00002147
Iteration 220/1000 | Loss: 0.00002147
Iteration 221/1000 | Loss: 0.00002147
Iteration 222/1000 | Loss: 0.00002147
Iteration 223/1000 | Loss: 0.00002147
Iteration 224/1000 | Loss: 0.00002147
Iteration 225/1000 | Loss: 0.00002147
Iteration 226/1000 | Loss: 0.00002147
Iteration 227/1000 | Loss: 0.00002147
Iteration 228/1000 | Loss: 0.00002147
Iteration 229/1000 | Loss: 0.00002147
Iteration 230/1000 | Loss: 0.00002147
Iteration 231/1000 | Loss: 0.00002147
Iteration 232/1000 | Loss: 0.00002147
Iteration 233/1000 | Loss: 0.00002147
Iteration 234/1000 | Loss: 0.00002147
Iteration 235/1000 | Loss: 0.00002147
Iteration 236/1000 | Loss: 0.00002147
Iteration 237/1000 | Loss: 0.00002147
Iteration 238/1000 | Loss: 0.00002147
Iteration 239/1000 | Loss: 0.00002147
Iteration 240/1000 | Loss: 0.00002147
Iteration 241/1000 | Loss: 0.00002147
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 241. Stopping optimization.
Last 5 losses: [2.1466337784659117e-05, 2.1466337784659117e-05, 2.1466337784659117e-05, 2.1466337784659117e-05, 2.1466337784659117e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1466337784659117e-05

Optimization complete. Final v2v error: 3.8326497077941895 mm

Highest mean error: 7.839135646820068 mm for frame 76

Lowest mean error: 3.612036943435669 mm for frame 56

Saving results

Total time: 172.34003329277039
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00838364
Iteration 2/25 | Loss: 0.00117638
Iteration 3/25 | Loss: 0.00108843
Iteration 4/25 | Loss: 0.00107336
Iteration 5/25 | Loss: 0.00106804
Iteration 6/25 | Loss: 0.00106681
Iteration 7/25 | Loss: 0.00106668
Iteration 8/25 | Loss: 0.00106668
Iteration 9/25 | Loss: 0.00106668
Iteration 10/25 | Loss: 0.00106668
Iteration 11/25 | Loss: 0.00106668
Iteration 12/25 | Loss: 0.00106668
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001066679134964943, 0.001066679134964943, 0.001066679134964943, 0.001066679134964943, 0.001066679134964943]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001066679134964943

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.80996919
Iteration 2/25 | Loss: 0.00084566
Iteration 3/25 | Loss: 0.00084566
Iteration 4/25 | Loss: 0.00084566
Iteration 5/25 | Loss: 0.00084566
Iteration 6/25 | Loss: 0.00084566
Iteration 7/25 | Loss: 0.00084566
Iteration 8/25 | Loss: 0.00084566
Iteration 9/25 | Loss: 0.00084566
Iteration 10/25 | Loss: 0.00084566
Iteration 11/25 | Loss: 0.00084566
Iteration 12/25 | Loss: 0.00084566
Iteration 13/25 | Loss: 0.00084566
Iteration 14/25 | Loss: 0.00084566
Iteration 15/25 | Loss: 0.00084566
Iteration 16/25 | Loss: 0.00084566
Iteration 17/25 | Loss: 0.00084566
Iteration 18/25 | Loss: 0.00084566
Iteration 19/25 | Loss: 0.00084566
Iteration 20/25 | Loss: 0.00084566
Iteration 21/25 | Loss: 0.00084566
Iteration 22/25 | Loss: 0.00084566
Iteration 23/25 | Loss: 0.00084566
Iteration 24/25 | Loss: 0.00084566
Iteration 25/25 | Loss: 0.00084566

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084566
Iteration 2/1000 | Loss: 0.00001716
Iteration 3/1000 | Loss: 0.00001261
Iteration 4/1000 | Loss: 0.00001149
Iteration 5/1000 | Loss: 0.00001094
Iteration 6/1000 | Loss: 0.00001064
Iteration 7/1000 | Loss: 0.00001038
Iteration 8/1000 | Loss: 0.00001036
Iteration 9/1000 | Loss: 0.00001020
Iteration 10/1000 | Loss: 0.00001002
Iteration 11/1000 | Loss: 0.00000997
Iteration 12/1000 | Loss: 0.00000993
Iteration 13/1000 | Loss: 0.00000993
Iteration 14/1000 | Loss: 0.00000992
Iteration 15/1000 | Loss: 0.00000992
Iteration 16/1000 | Loss: 0.00000991
Iteration 17/1000 | Loss: 0.00000990
Iteration 18/1000 | Loss: 0.00000989
Iteration 19/1000 | Loss: 0.00000988
Iteration 20/1000 | Loss: 0.00000987
Iteration 21/1000 | Loss: 0.00000986
Iteration 22/1000 | Loss: 0.00000985
Iteration 23/1000 | Loss: 0.00000985
Iteration 24/1000 | Loss: 0.00000984
Iteration 25/1000 | Loss: 0.00000981
Iteration 26/1000 | Loss: 0.00000981
Iteration 27/1000 | Loss: 0.00000980
Iteration 28/1000 | Loss: 0.00000980
Iteration 29/1000 | Loss: 0.00000980
Iteration 30/1000 | Loss: 0.00000980
Iteration 31/1000 | Loss: 0.00000980
Iteration 32/1000 | Loss: 0.00000980
Iteration 33/1000 | Loss: 0.00000978
Iteration 34/1000 | Loss: 0.00000974
Iteration 35/1000 | Loss: 0.00000971
Iteration 36/1000 | Loss: 0.00000971
Iteration 37/1000 | Loss: 0.00000970
Iteration 38/1000 | Loss: 0.00000969
Iteration 39/1000 | Loss: 0.00000969
Iteration 40/1000 | Loss: 0.00000968
Iteration 41/1000 | Loss: 0.00000968
Iteration 42/1000 | Loss: 0.00000967
Iteration 43/1000 | Loss: 0.00000966
Iteration 44/1000 | Loss: 0.00000966
Iteration 45/1000 | Loss: 0.00000965
Iteration 46/1000 | Loss: 0.00000965
Iteration 47/1000 | Loss: 0.00000965
Iteration 48/1000 | Loss: 0.00000964
Iteration 49/1000 | Loss: 0.00000964
Iteration 50/1000 | Loss: 0.00000963
Iteration 51/1000 | Loss: 0.00000962
Iteration 52/1000 | Loss: 0.00000962
Iteration 53/1000 | Loss: 0.00000962
Iteration 54/1000 | Loss: 0.00000961
Iteration 55/1000 | Loss: 0.00000961
Iteration 56/1000 | Loss: 0.00000961
Iteration 57/1000 | Loss: 0.00000960
Iteration 58/1000 | Loss: 0.00000960
Iteration 59/1000 | Loss: 0.00000960
Iteration 60/1000 | Loss: 0.00000960
Iteration 61/1000 | Loss: 0.00000960
Iteration 62/1000 | Loss: 0.00000960
Iteration 63/1000 | Loss: 0.00000960
Iteration 64/1000 | Loss: 0.00000960
Iteration 65/1000 | Loss: 0.00000960
Iteration 66/1000 | Loss: 0.00000960
Iteration 67/1000 | Loss: 0.00000959
Iteration 68/1000 | Loss: 0.00000959
Iteration 69/1000 | Loss: 0.00000959
Iteration 70/1000 | Loss: 0.00000958
Iteration 71/1000 | Loss: 0.00000958
Iteration 72/1000 | Loss: 0.00000958
Iteration 73/1000 | Loss: 0.00000958
Iteration 74/1000 | Loss: 0.00000957
Iteration 75/1000 | Loss: 0.00000957
Iteration 76/1000 | Loss: 0.00000957
Iteration 77/1000 | Loss: 0.00000957
Iteration 78/1000 | Loss: 0.00000956
Iteration 79/1000 | Loss: 0.00000956
Iteration 80/1000 | Loss: 0.00000956
Iteration 81/1000 | Loss: 0.00000955
Iteration 82/1000 | Loss: 0.00000955
Iteration 83/1000 | Loss: 0.00000955
Iteration 84/1000 | Loss: 0.00000955
Iteration 85/1000 | Loss: 0.00000955
Iteration 86/1000 | Loss: 0.00000955
Iteration 87/1000 | Loss: 0.00000954
Iteration 88/1000 | Loss: 0.00000954
Iteration 89/1000 | Loss: 0.00000953
Iteration 90/1000 | Loss: 0.00000953
Iteration 91/1000 | Loss: 0.00000953
Iteration 92/1000 | Loss: 0.00000952
Iteration 93/1000 | Loss: 0.00000952
Iteration 94/1000 | Loss: 0.00000951
Iteration 95/1000 | Loss: 0.00000951
Iteration 96/1000 | Loss: 0.00000950
Iteration 97/1000 | Loss: 0.00000949
Iteration 98/1000 | Loss: 0.00000949
Iteration 99/1000 | Loss: 0.00000949
Iteration 100/1000 | Loss: 0.00000949
Iteration 101/1000 | Loss: 0.00000949
Iteration 102/1000 | Loss: 0.00000948
Iteration 103/1000 | Loss: 0.00000948
Iteration 104/1000 | Loss: 0.00000948
Iteration 105/1000 | Loss: 0.00000947
Iteration 106/1000 | Loss: 0.00000947
Iteration 107/1000 | Loss: 0.00000946
Iteration 108/1000 | Loss: 0.00000946
Iteration 109/1000 | Loss: 0.00000946
Iteration 110/1000 | Loss: 0.00000946
Iteration 111/1000 | Loss: 0.00000946
Iteration 112/1000 | Loss: 0.00000945
Iteration 113/1000 | Loss: 0.00000945
Iteration 114/1000 | Loss: 0.00000945
Iteration 115/1000 | Loss: 0.00000944
Iteration 116/1000 | Loss: 0.00000944
Iteration 117/1000 | Loss: 0.00000944
Iteration 118/1000 | Loss: 0.00000943
Iteration 119/1000 | Loss: 0.00000943
Iteration 120/1000 | Loss: 0.00000943
Iteration 121/1000 | Loss: 0.00000943
Iteration 122/1000 | Loss: 0.00000942
Iteration 123/1000 | Loss: 0.00000942
Iteration 124/1000 | Loss: 0.00000942
Iteration 125/1000 | Loss: 0.00000942
Iteration 126/1000 | Loss: 0.00000942
Iteration 127/1000 | Loss: 0.00000942
Iteration 128/1000 | Loss: 0.00000942
Iteration 129/1000 | Loss: 0.00000942
Iteration 130/1000 | Loss: 0.00000942
Iteration 131/1000 | Loss: 0.00000942
Iteration 132/1000 | Loss: 0.00000942
Iteration 133/1000 | Loss: 0.00000941
Iteration 134/1000 | Loss: 0.00000941
Iteration 135/1000 | Loss: 0.00000941
Iteration 136/1000 | Loss: 0.00000941
Iteration 137/1000 | Loss: 0.00000941
Iteration 138/1000 | Loss: 0.00000941
Iteration 139/1000 | Loss: 0.00000941
Iteration 140/1000 | Loss: 0.00000941
Iteration 141/1000 | Loss: 0.00000941
Iteration 142/1000 | Loss: 0.00000941
Iteration 143/1000 | Loss: 0.00000941
Iteration 144/1000 | Loss: 0.00000940
Iteration 145/1000 | Loss: 0.00000940
Iteration 146/1000 | Loss: 0.00000940
Iteration 147/1000 | Loss: 0.00000940
Iteration 148/1000 | Loss: 0.00000939
Iteration 149/1000 | Loss: 0.00000939
Iteration 150/1000 | Loss: 0.00000939
Iteration 151/1000 | Loss: 0.00000939
Iteration 152/1000 | Loss: 0.00000939
Iteration 153/1000 | Loss: 0.00000939
Iteration 154/1000 | Loss: 0.00000939
Iteration 155/1000 | Loss: 0.00000939
Iteration 156/1000 | Loss: 0.00000939
Iteration 157/1000 | Loss: 0.00000939
Iteration 158/1000 | Loss: 0.00000939
Iteration 159/1000 | Loss: 0.00000939
Iteration 160/1000 | Loss: 0.00000939
Iteration 161/1000 | Loss: 0.00000938
Iteration 162/1000 | Loss: 0.00000938
Iteration 163/1000 | Loss: 0.00000938
Iteration 164/1000 | Loss: 0.00000938
Iteration 165/1000 | Loss: 0.00000937
Iteration 166/1000 | Loss: 0.00000937
Iteration 167/1000 | Loss: 0.00000937
Iteration 168/1000 | Loss: 0.00000937
Iteration 169/1000 | Loss: 0.00000937
Iteration 170/1000 | Loss: 0.00000937
Iteration 171/1000 | Loss: 0.00000937
Iteration 172/1000 | Loss: 0.00000936
Iteration 173/1000 | Loss: 0.00000936
Iteration 174/1000 | Loss: 0.00000936
Iteration 175/1000 | Loss: 0.00000936
Iteration 176/1000 | Loss: 0.00000936
Iteration 177/1000 | Loss: 0.00000936
Iteration 178/1000 | Loss: 0.00000936
Iteration 179/1000 | Loss: 0.00000936
Iteration 180/1000 | Loss: 0.00000936
Iteration 181/1000 | Loss: 0.00000936
Iteration 182/1000 | Loss: 0.00000935
Iteration 183/1000 | Loss: 0.00000935
Iteration 184/1000 | Loss: 0.00000935
Iteration 185/1000 | Loss: 0.00000935
Iteration 186/1000 | Loss: 0.00000935
Iteration 187/1000 | Loss: 0.00000935
Iteration 188/1000 | Loss: 0.00000935
Iteration 189/1000 | Loss: 0.00000935
Iteration 190/1000 | Loss: 0.00000935
Iteration 191/1000 | Loss: 0.00000935
Iteration 192/1000 | Loss: 0.00000935
Iteration 193/1000 | Loss: 0.00000935
Iteration 194/1000 | Loss: 0.00000935
Iteration 195/1000 | Loss: 0.00000935
Iteration 196/1000 | Loss: 0.00000934
Iteration 197/1000 | Loss: 0.00000934
Iteration 198/1000 | Loss: 0.00000934
Iteration 199/1000 | Loss: 0.00000934
Iteration 200/1000 | Loss: 0.00000934
Iteration 201/1000 | Loss: 0.00000934
Iteration 202/1000 | Loss: 0.00000934
Iteration 203/1000 | Loss: 0.00000934
Iteration 204/1000 | Loss: 0.00000934
Iteration 205/1000 | Loss: 0.00000934
Iteration 206/1000 | Loss: 0.00000934
Iteration 207/1000 | Loss: 0.00000934
Iteration 208/1000 | Loss: 0.00000934
Iteration 209/1000 | Loss: 0.00000934
Iteration 210/1000 | Loss: 0.00000934
Iteration 211/1000 | Loss: 0.00000934
Iteration 212/1000 | Loss: 0.00000934
Iteration 213/1000 | Loss: 0.00000934
Iteration 214/1000 | Loss: 0.00000934
Iteration 215/1000 | Loss: 0.00000934
Iteration 216/1000 | Loss: 0.00000934
Iteration 217/1000 | Loss: 0.00000934
Iteration 218/1000 | Loss: 0.00000934
Iteration 219/1000 | Loss: 0.00000934
Iteration 220/1000 | Loss: 0.00000934
Iteration 221/1000 | Loss: 0.00000934
Iteration 222/1000 | Loss: 0.00000934
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 222. Stopping optimization.
Last 5 losses: [9.338229574495926e-06, 9.338229574495926e-06, 9.338229574495926e-06, 9.338229574495926e-06, 9.338229574495926e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.338229574495926e-06

Optimization complete. Final v2v error: 2.6040151119232178 mm

Highest mean error: 3.225003242492676 mm for frame 84

Lowest mean error: 2.408015727996826 mm for frame 123

Saving results

Total time: 39.637147665023804
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00380336
Iteration 2/25 | Loss: 0.00115963
Iteration 3/25 | Loss: 0.00109256
Iteration 4/25 | Loss: 0.00108850
Iteration 5/25 | Loss: 0.00108718
Iteration 6/25 | Loss: 0.00108716
Iteration 7/25 | Loss: 0.00108716
Iteration 8/25 | Loss: 0.00108716
Iteration 9/25 | Loss: 0.00108716
Iteration 10/25 | Loss: 0.00108716
Iteration 11/25 | Loss: 0.00108716
Iteration 12/25 | Loss: 0.00108716
Iteration 13/25 | Loss: 0.00108716
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0010871641570702195, 0.0010871641570702195, 0.0010871641570702195, 0.0010871641570702195, 0.0010871641570702195]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010871641570702195

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37934005
Iteration 2/25 | Loss: 0.00083837
Iteration 3/25 | Loss: 0.00083837
Iteration 4/25 | Loss: 0.00083837
Iteration 5/25 | Loss: 0.00083837
Iteration 6/25 | Loss: 0.00083837
Iteration 7/25 | Loss: 0.00083837
Iteration 8/25 | Loss: 0.00083837
Iteration 9/25 | Loss: 0.00083837
Iteration 10/25 | Loss: 0.00083837
Iteration 11/25 | Loss: 0.00083837
Iteration 12/25 | Loss: 0.00083837
Iteration 13/25 | Loss: 0.00083837
Iteration 14/25 | Loss: 0.00083837
Iteration 15/25 | Loss: 0.00083837
Iteration 16/25 | Loss: 0.00083837
Iteration 17/25 | Loss: 0.00083837
Iteration 18/25 | Loss: 0.00083837
Iteration 19/25 | Loss: 0.00083837
Iteration 20/25 | Loss: 0.00083837
Iteration 21/25 | Loss: 0.00083837
Iteration 22/25 | Loss: 0.00083837
Iteration 23/25 | Loss: 0.00083837
Iteration 24/25 | Loss: 0.00083837
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008383694803342223, 0.0008383694803342223, 0.0008383694803342223, 0.0008383694803342223, 0.0008383694803342223]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008383694803342223

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083837
Iteration 2/1000 | Loss: 0.00001888
Iteration 3/1000 | Loss: 0.00001338
Iteration 4/1000 | Loss: 0.00001226
Iteration 5/1000 | Loss: 0.00001163
Iteration 6/1000 | Loss: 0.00001128
Iteration 7/1000 | Loss: 0.00001107
Iteration 8/1000 | Loss: 0.00001091
Iteration 9/1000 | Loss: 0.00001080
Iteration 10/1000 | Loss: 0.00001066
Iteration 11/1000 | Loss: 0.00001065
Iteration 12/1000 | Loss: 0.00001060
Iteration 13/1000 | Loss: 0.00001059
Iteration 14/1000 | Loss: 0.00001059
Iteration 15/1000 | Loss: 0.00001059
Iteration 16/1000 | Loss: 0.00001058
Iteration 17/1000 | Loss: 0.00001057
Iteration 18/1000 | Loss: 0.00001055
Iteration 19/1000 | Loss: 0.00001053
Iteration 20/1000 | Loss: 0.00001052
Iteration 21/1000 | Loss: 0.00001051
Iteration 22/1000 | Loss: 0.00001051
Iteration 23/1000 | Loss: 0.00001050
Iteration 24/1000 | Loss: 0.00001049
Iteration 25/1000 | Loss: 0.00001047
Iteration 26/1000 | Loss: 0.00001046
Iteration 27/1000 | Loss: 0.00001046
Iteration 28/1000 | Loss: 0.00001045
Iteration 29/1000 | Loss: 0.00001045
Iteration 30/1000 | Loss: 0.00001045
Iteration 31/1000 | Loss: 0.00001044
Iteration 32/1000 | Loss: 0.00001043
Iteration 33/1000 | Loss: 0.00001043
Iteration 34/1000 | Loss: 0.00001042
Iteration 35/1000 | Loss: 0.00001041
Iteration 36/1000 | Loss: 0.00001041
Iteration 37/1000 | Loss: 0.00001036
Iteration 38/1000 | Loss: 0.00001036
Iteration 39/1000 | Loss: 0.00001035
Iteration 40/1000 | Loss: 0.00001035
Iteration 41/1000 | Loss: 0.00001034
Iteration 42/1000 | Loss: 0.00001034
Iteration 43/1000 | Loss: 0.00001034
Iteration 44/1000 | Loss: 0.00001034
Iteration 45/1000 | Loss: 0.00001033
Iteration 46/1000 | Loss: 0.00001033
Iteration 47/1000 | Loss: 0.00001031
Iteration 48/1000 | Loss: 0.00001031
Iteration 49/1000 | Loss: 0.00001031
Iteration 50/1000 | Loss: 0.00001031
Iteration 51/1000 | Loss: 0.00001031
Iteration 52/1000 | Loss: 0.00001031
Iteration 53/1000 | Loss: 0.00001031
Iteration 54/1000 | Loss: 0.00001031
Iteration 55/1000 | Loss: 0.00001031
Iteration 56/1000 | Loss: 0.00001031
Iteration 57/1000 | Loss: 0.00001031
Iteration 58/1000 | Loss: 0.00001030
Iteration 59/1000 | Loss: 0.00001030
Iteration 60/1000 | Loss: 0.00001030
Iteration 61/1000 | Loss: 0.00001030
Iteration 62/1000 | Loss: 0.00001030
Iteration 63/1000 | Loss: 0.00001030
Iteration 64/1000 | Loss: 0.00001030
Iteration 65/1000 | Loss: 0.00001029
Iteration 66/1000 | Loss: 0.00001028
Iteration 67/1000 | Loss: 0.00001028
Iteration 68/1000 | Loss: 0.00001027
Iteration 69/1000 | Loss: 0.00001027
Iteration 70/1000 | Loss: 0.00001027
Iteration 71/1000 | Loss: 0.00001027
Iteration 72/1000 | Loss: 0.00001027
Iteration 73/1000 | Loss: 0.00001027
Iteration 74/1000 | Loss: 0.00001027
Iteration 75/1000 | Loss: 0.00001027
Iteration 76/1000 | Loss: 0.00001027
Iteration 77/1000 | Loss: 0.00001027
Iteration 78/1000 | Loss: 0.00001026
Iteration 79/1000 | Loss: 0.00001026
Iteration 80/1000 | Loss: 0.00001026
Iteration 81/1000 | Loss: 0.00001025
Iteration 82/1000 | Loss: 0.00001025
Iteration 83/1000 | Loss: 0.00001025
Iteration 84/1000 | Loss: 0.00001025
Iteration 85/1000 | Loss: 0.00001025
Iteration 86/1000 | Loss: 0.00001025
Iteration 87/1000 | Loss: 0.00001025
Iteration 88/1000 | Loss: 0.00001024
Iteration 89/1000 | Loss: 0.00001024
Iteration 90/1000 | Loss: 0.00001024
Iteration 91/1000 | Loss: 0.00001024
Iteration 92/1000 | Loss: 0.00001024
Iteration 93/1000 | Loss: 0.00001024
Iteration 94/1000 | Loss: 0.00001024
Iteration 95/1000 | Loss: 0.00001024
Iteration 96/1000 | Loss: 0.00001023
Iteration 97/1000 | Loss: 0.00001023
Iteration 98/1000 | Loss: 0.00001023
Iteration 99/1000 | Loss: 0.00001023
Iteration 100/1000 | Loss: 0.00001023
Iteration 101/1000 | Loss: 0.00001022
Iteration 102/1000 | Loss: 0.00001022
Iteration 103/1000 | Loss: 0.00001022
Iteration 104/1000 | Loss: 0.00001022
Iteration 105/1000 | Loss: 0.00001022
Iteration 106/1000 | Loss: 0.00001022
Iteration 107/1000 | Loss: 0.00001022
Iteration 108/1000 | Loss: 0.00001022
Iteration 109/1000 | Loss: 0.00001022
Iteration 110/1000 | Loss: 0.00001022
Iteration 111/1000 | Loss: 0.00001022
Iteration 112/1000 | Loss: 0.00001022
Iteration 113/1000 | Loss: 0.00001022
Iteration 114/1000 | Loss: 0.00001021
Iteration 115/1000 | Loss: 0.00001021
Iteration 116/1000 | Loss: 0.00001021
Iteration 117/1000 | Loss: 0.00001021
Iteration 118/1000 | Loss: 0.00001020
Iteration 119/1000 | Loss: 0.00001020
Iteration 120/1000 | Loss: 0.00001020
Iteration 121/1000 | Loss: 0.00001020
Iteration 122/1000 | Loss: 0.00001020
Iteration 123/1000 | Loss: 0.00001020
Iteration 124/1000 | Loss: 0.00001020
Iteration 125/1000 | Loss: 0.00001019
Iteration 126/1000 | Loss: 0.00001019
Iteration 127/1000 | Loss: 0.00001019
Iteration 128/1000 | Loss: 0.00001019
Iteration 129/1000 | Loss: 0.00001019
Iteration 130/1000 | Loss: 0.00001019
Iteration 131/1000 | Loss: 0.00001019
Iteration 132/1000 | Loss: 0.00001018
Iteration 133/1000 | Loss: 0.00001018
Iteration 134/1000 | Loss: 0.00001018
Iteration 135/1000 | Loss: 0.00001018
Iteration 136/1000 | Loss: 0.00001018
Iteration 137/1000 | Loss: 0.00001018
Iteration 138/1000 | Loss: 0.00001018
Iteration 139/1000 | Loss: 0.00001017
Iteration 140/1000 | Loss: 0.00001017
Iteration 141/1000 | Loss: 0.00001017
Iteration 142/1000 | Loss: 0.00001017
Iteration 143/1000 | Loss: 0.00001016
Iteration 144/1000 | Loss: 0.00001016
Iteration 145/1000 | Loss: 0.00001016
Iteration 146/1000 | Loss: 0.00001016
Iteration 147/1000 | Loss: 0.00001016
Iteration 148/1000 | Loss: 0.00001016
Iteration 149/1000 | Loss: 0.00001016
Iteration 150/1000 | Loss: 0.00001016
Iteration 151/1000 | Loss: 0.00001016
Iteration 152/1000 | Loss: 0.00001015
Iteration 153/1000 | Loss: 0.00001015
Iteration 154/1000 | Loss: 0.00001014
Iteration 155/1000 | Loss: 0.00001014
Iteration 156/1000 | Loss: 0.00001014
Iteration 157/1000 | Loss: 0.00001014
Iteration 158/1000 | Loss: 0.00001014
Iteration 159/1000 | Loss: 0.00001014
Iteration 160/1000 | Loss: 0.00001014
Iteration 161/1000 | Loss: 0.00001014
Iteration 162/1000 | Loss: 0.00001014
Iteration 163/1000 | Loss: 0.00001014
Iteration 164/1000 | Loss: 0.00001014
Iteration 165/1000 | Loss: 0.00001014
Iteration 166/1000 | Loss: 0.00001013
Iteration 167/1000 | Loss: 0.00001013
Iteration 168/1000 | Loss: 0.00001013
Iteration 169/1000 | Loss: 0.00001013
Iteration 170/1000 | Loss: 0.00001013
Iteration 171/1000 | Loss: 0.00001013
Iteration 172/1000 | Loss: 0.00001012
Iteration 173/1000 | Loss: 0.00001012
Iteration 174/1000 | Loss: 0.00001012
Iteration 175/1000 | Loss: 0.00001012
Iteration 176/1000 | Loss: 0.00001012
Iteration 177/1000 | Loss: 0.00001012
Iteration 178/1000 | Loss: 0.00001011
Iteration 179/1000 | Loss: 0.00001011
Iteration 180/1000 | Loss: 0.00001011
Iteration 181/1000 | Loss: 0.00001011
Iteration 182/1000 | Loss: 0.00001011
Iteration 183/1000 | Loss: 0.00001011
Iteration 184/1000 | Loss: 0.00001011
Iteration 185/1000 | Loss: 0.00001011
Iteration 186/1000 | Loss: 0.00001011
Iteration 187/1000 | Loss: 0.00001011
Iteration 188/1000 | Loss: 0.00001011
Iteration 189/1000 | Loss: 0.00001011
Iteration 190/1000 | Loss: 0.00001010
Iteration 191/1000 | Loss: 0.00001010
Iteration 192/1000 | Loss: 0.00001010
Iteration 193/1000 | Loss: 0.00001010
Iteration 194/1000 | Loss: 0.00001010
Iteration 195/1000 | Loss: 0.00001010
Iteration 196/1000 | Loss: 0.00001010
Iteration 197/1000 | Loss: 0.00001010
Iteration 198/1000 | Loss: 0.00001010
Iteration 199/1000 | Loss: 0.00001010
Iteration 200/1000 | Loss: 0.00001010
Iteration 201/1000 | Loss: 0.00001009
Iteration 202/1000 | Loss: 0.00001009
Iteration 203/1000 | Loss: 0.00001009
Iteration 204/1000 | Loss: 0.00001009
Iteration 205/1000 | Loss: 0.00001009
Iteration 206/1000 | Loss: 0.00001009
Iteration 207/1000 | Loss: 0.00001009
Iteration 208/1000 | Loss: 0.00001009
Iteration 209/1000 | Loss: 0.00001009
Iteration 210/1000 | Loss: 0.00001009
Iteration 211/1000 | Loss: 0.00001009
Iteration 212/1000 | Loss: 0.00001009
Iteration 213/1000 | Loss: 0.00001009
Iteration 214/1000 | Loss: 0.00001009
Iteration 215/1000 | Loss: 0.00001009
Iteration 216/1000 | Loss: 0.00001009
Iteration 217/1000 | Loss: 0.00001009
Iteration 218/1000 | Loss: 0.00001009
Iteration 219/1000 | Loss: 0.00001008
Iteration 220/1000 | Loss: 0.00001008
Iteration 221/1000 | Loss: 0.00001008
Iteration 222/1000 | Loss: 0.00001008
Iteration 223/1000 | Loss: 0.00001008
Iteration 224/1000 | Loss: 0.00001008
Iteration 225/1000 | Loss: 0.00001008
Iteration 226/1000 | Loss: 0.00001008
Iteration 227/1000 | Loss: 0.00001008
Iteration 228/1000 | Loss: 0.00001008
Iteration 229/1000 | Loss: 0.00001008
Iteration 230/1000 | Loss: 0.00001008
Iteration 231/1000 | Loss: 0.00001008
Iteration 232/1000 | Loss: 0.00001008
Iteration 233/1000 | Loss: 0.00001008
Iteration 234/1000 | Loss: 0.00001008
Iteration 235/1000 | Loss: 0.00001008
Iteration 236/1000 | Loss: 0.00001008
Iteration 237/1000 | Loss: 0.00001008
Iteration 238/1000 | Loss: 0.00001008
Iteration 239/1000 | Loss: 0.00001008
Iteration 240/1000 | Loss: 0.00001008
Iteration 241/1000 | Loss: 0.00001008
Iteration 242/1000 | Loss: 0.00001008
Iteration 243/1000 | Loss: 0.00001008
Iteration 244/1000 | Loss: 0.00001008
Iteration 245/1000 | Loss: 0.00001008
Iteration 246/1000 | Loss: 0.00001008
Iteration 247/1000 | Loss: 0.00001008
Iteration 248/1000 | Loss: 0.00001008
Iteration 249/1000 | Loss: 0.00001008
Iteration 250/1000 | Loss: 0.00001008
Iteration 251/1000 | Loss: 0.00001008
Iteration 252/1000 | Loss: 0.00001008
Iteration 253/1000 | Loss: 0.00001008
Iteration 254/1000 | Loss: 0.00001008
Iteration 255/1000 | Loss: 0.00001008
Iteration 256/1000 | Loss: 0.00001008
Iteration 257/1000 | Loss: 0.00001008
Iteration 258/1000 | Loss: 0.00001008
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 258. Stopping optimization.
Last 5 losses: [1.0078525519929826e-05, 1.0078525519929826e-05, 1.0078525519929826e-05, 1.0078525519929826e-05, 1.0078525519929826e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0078525519929826e-05

Optimization complete. Final v2v error: 2.700364351272583 mm

Highest mean error: 2.89927339553833 mm for frame 59

Lowest mean error: 2.528186082839966 mm for frame 68

Saving results

Total time: 38.224117040634155
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00785325
Iteration 2/25 | Loss: 0.00121971
Iteration 3/25 | Loss: 0.00109812
Iteration 4/25 | Loss: 0.00108234
Iteration 5/25 | Loss: 0.00107902
Iteration 6/25 | Loss: 0.00107889
Iteration 7/25 | Loss: 0.00107889
Iteration 8/25 | Loss: 0.00107889
Iteration 9/25 | Loss: 0.00107889
Iteration 10/25 | Loss: 0.00107889
Iteration 11/25 | Loss: 0.00107889
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010788912186399102, 0.0010788912186399102, 0.0010788912186399102, 0.0010788912186399102, 0.0010788912186399102]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010788912186399102

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34707987
Iteration 2/25 | Loss: 0.00083178
Iteration 3/25 | Loss: 0.00083178
Iteration 4/25 | Loss: 0.00083178
Iteration 5/25 | Loss: 0.00083178
Iteration 6/25 | Loss: 0.00083177
Iteration 7/25 | Loss: 0.00083177
Iteration 8/25 | Loss: 0.00083177
Iteration 9/25 | Loss: 0.00083177
Iteration 10/25 | Loss: 0.00083177
Iteration 11/25 | Loss: 0.00083177
Iteration 12/25 | Loss: 0.00083177
Iteration 13/25 | Loss: 0.00083177
Iteration 14/25 | Loss: 0.00083177
Iteration 15/25 | Loss: 0.00083177
Iteration 16/25 | Loss: 0.00083177
Iteration 17/25 | Loss: 0.00083177
Iteration 18/25 | Loss: 0.00083177
Iteration 19/25 | Loss: 0.00083177
Iteration 20/25 | Loss: 0.00083177
Iteration 21/25 | Loss: 0.00083177
Iteration 22/25 | Loss: 0.00083177
Iteration 23/25 | Loss: 0.00083177
Iteration 24/25 | Loss: 0.00083177
Iteration 25/25 | Loss: 0.00083177

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083177
Iteration 2/1000 | Loss: 0.00002040
Iteration 3/1000 | Loss: 0.00001363
Iteration 4/1000 | Loss: 0.00001220
Iteration 5/1000 | Loss: 0.00001135
Iteration 6/1000 | Loss: 0.00001081
Iteration 7/1000 | Loss: 0.00001047
Iteration 8/1000 | Loss: 0.00001040
Iteration 9/1000 | Loss: 0.00001018
Iteration 10/1000 | Loss: 0.00000993
Iteration 11/1000 | Loss: 0.00000985
Iteration 12/1000 | Loss: 0.00000983
Iteration 13/1000 | Loss: 0.00000981
Iteration 14/1000 | Loss: 0.00000980
Iteration 15/1000 | Loss: 0.00000978
Iteration 16/1000 | Loss: 0.00000978
Iteration 17/1000 | Loss: 0.00000977
Iteration 18/1000 | Loss: 0.00000977
Iteration 19/1000 | Loss: 0.00000976
Iteration 20/1000 | Loss: 0.00000975
Iteration 21/1000 | Loss: 0.00000974
Iteration 22/1000 | Loss: 0.00000973
Iteration 23/1000 | Loss: 0.00000968
Iteration 24/1000 | Loss: 0.00000965
Iteration 25/1000 | Loss: 0.00000965
Iteration 26/1000 | Loss: 0.00000965
Iteration 27/1000 | Loss: 0.00000964
Iteration 28/1000 | Loss: 0.00000963
Iteration 29/1000 | Loss: 0.00000963
Iteration 30/1000 | Loss: 0.00000962
Iteration 31/1000 | Loss: 0.00000962
Iteration 32/1000 | Loss: 0.00000962
Iteration 33/1000 | Loss: 0.00000962
Iteration 34/1000 | Loss: 0.00000962
Iteration 35/1000 | Loss: 0.00000958
Iteration 36/1000 | Loss: 0.00000957
Iteration 37/1000 | Loss: 0.00000957
Iteration 38/1000 | Loss: 0.00000957
Iteration 39/1000 | Loss: 0.00000957
Iteration 40/1000 | Loss: 0.00000957
Iteration 41/1000 | Loss: 0.00000956
Iteration 42/1000 | Loss: 0.00000956
Iteration 43/1000 | Loss: 0.00000956
Iteration 44/1000 | Loss: 0.00000955
Iteration 45/1000 | Loss: 0.00000955
Iteration 46/1000 | Loss: 0.00000955
Iteration 47/1000 | Loss: 0.00000955
Iteration 48/1000 | Loss: 0.00000955
Iteration 49/1000 | Loss: 0.00000955
Iteration 50/1000 | Loss: 0.00000955
Iteration 51/1000 | Loss: 0.00000955
Iteration 52/1000 | Loss: 0.00000954
Iteration 53/1000 | Loss: 0.00000954
Iteration 54/1000 | Loss: 0.00000953
Iteration 55/1000 | Loss: 0.00000953
Iteration 56/1000 | Loss: 0.00000952
Iteration 57/1000 | Loss: 0.00000952
Iteration 58/1000 | Loss: 0.00000951
Iteration 59/1000 | Loss: 0.00000951
Iteration 60/1000 | Loss: 0.00000951
Iteration 61/1000 | Loss: 0.00000950
Iteration 62/1000 | Loss: 0.00000950
Iteration 63/1000 | Loss: 0.00000950
Iteration 64/1000 | Loss: 0.00000950
Iteration 65/1000 | Loss: 0.00000950
Iteration 66/1000 | Loss: 0.00000949
Iteration 67/1000 | Loss: 0.00000949
Iteration 68/1000 | Loss: 0.00000949
Iteration 69/1000 | Loss: 0.00000949
Iteration 70/1000 | Loss: 0.00000949
Iteration 71/1000 | Loss: 0.00000949
Iteration 72/1000 | Loss: 0.00000949
Iteration 73/1000 | Loss: 0.00000949
Iteration 74/1000 | Loss: 0.00000949
Iteration 75/1000 | Loss: 0.00000948
Iteration 76/1000 | Loss: 0.00000948
Iteration 77/1000 | Loss: 0.00000947
Iteration 78/1000 | Loss: 0.00000947
Iteration 79/1000 | Loss: 0.00000947
Iteration 80/1000 | Loss: 0.00000947
Iteration 81/1000 | Loss: 0.00000947
Iteration 82/1000 | Loss: 0.00000947
Iteration 83/1000 | Loss: 0.00000946
Iteration 84/1000 | Loss: 0.00000946
Iteration 85/1000 | Loss: 0.00000945
Iteration 86/1000 | Loss: 0.00000945
Iteration 87/1000 | Loss: 0.00000945
Iteration 88/1000 | Loss: 0.00000944
Iteration 89/1000 | Loss: 0.00000944
Iteration 90/1000 | Loss: 0.00000944
Iteration 91/1000 | Loss: 0.00000943
Iteration 92/1000 | Loss: 0.00000943
Iteration 93/1000 | Loss: 0.00000943
Iteration 94/1000 | Loss: 0.00000943
Iteration 95/1000 | Loss: 0.00000943
Iteration 96/1000 | Loss: 0.00000942
Iteration 97/1000 | Loss: 0.00000942
Iteration 98/1000 | Loss: 0.00000942
Iteration 99/1000 | Loss: 0.00000942
Iteration 100/1000 | Loss: 0.00000942
Iteration 101/1000 | Loss: 0.00000942
Iteration 102/1000 | Loss: 0.00000942
Iteration 103/1000 | Loss: 0.00000942
Iteration 104/1000 | Loss: 0.00000942
Iteration 105/1000 | Loss: 0.00000942
Iteration 106/1000 | Loss: 0.00000941
Iteration 107/1000 | Loss: 0.00000941
Iteration 108/1000 | Loss: 0.00000941
Iteration 109/1000 | Loss: 0.00000941
Iteration 110/1000 | Loss: 0.00000941
Iteration 111/1000 | Loss: 0.00000941
Iteration 112/1000 | Loss: 0.00000941
Iteration 113/1000 | Loss: 0.00000940
Iteration 114/1000 | Loss: 0.00000940
Iteration 115/1000 | Loss: 0.00000940
Iteration 116/1000 | Loss: 0.00000940
Iteration 117/1000 | Loss: 0.00000940
Iteration 118/1000 | Loss: 0.00000940
Iteration 119/1000 | Loss: 0.00000940
Iteration 120/1000 | Loss: 0.00000940
Iteration 121/1000 | Loss: 0.00000940
Iteration 122/1000 | Loss: 0.00000940
Iteration 123/1000 | Loss: 0.00000940
Iteration 124/1000 | Loss: 0.00000940
Iteration 125/1000 | Loss: 0.00000940
Iteration 126/1000 | Loss: 0.00000939
Iteration 127/1000 | Loss: 0.00000939
Iteration 128/1000 | Loss: 0.00000939
Iteration 129/1000 | Loss: 0.00000939
Iteration 130/1000 | Loss: 0.00000939
Iteration 131/1000 | Loss: 0.00000939
Iteration 132/1000 | Loss: 0.00000939
Iteration 133/1000 | Loss: 0.00000939
Iteration 134/1000 | Loss: 0.00000939
Iteration 135/1000 | Loss: 0.00000939
Iteration 136/1000 | Loss: 0.00000938
Iteration 137/1000 | Loss: 0.00000938
Iteration 138/1000 | Loss: 0.00000938
Iteration 139/1000 | Loss: 0.00000938
Iteration 140/1000 | Loss: 0.00000937
Iteration 141/1000 | Loss: 0.00000937
Iteration 142/1000 | Loss: 0.00000937
Iteration 143/1000 | Loss: 0.00000937
Iteration 144/1000 | Loss: 0.00000937
Iteration 145/1000 | Loss: 0.00000937
Iteration 146/1000 | Loss: 0.00000937
Iteration 147/1000 | Loss: 0.00000937
Iteration 148/1000 | Loss: 0.00000937
Iteration 149/1000 | Loss: 0.00000936
Iteration 150/1000 | Loss: 0.00000936
Iteration 151/1000 | Loss: 0.00000936
Iteration 152/1000 | Loss: 0.00000936
Iteration 153/1000 | Loss: 0.00000936
Iteration 154/1000 | Loss: 0.00000936
Iteration 155/1000 | Loss: 0.00000936
Iteration 156/1000 | Loss: 0.00000936
Iteration 157/1000 | Loss: 0.00000936
Iteration 158/1000 | Loss: 0.00000936
Iteration 159/1000 | Loss: 0.00000935
Iteration 160/1000 | Loss: 0.00000935
Iteration 161/1000 | Loss: 0.00000935
Iteration 162/1000 | Loss: 0.00000935
Iteration 163/1000 | Loss: 0.00000935
Iteration 164/1000 | Loss: 0.00000935
Iteration 165/1000 | Loss: 0.00000935
Iteration 166/1000 | Loss: 0.00000935
Iteration 167/1000 | Loss: 0.00000935
Iteration 168/1000 | Loss: 0.00000935
Iteration 169/1000 | Loss: 0.00000935
Iteration 170/1000 | Loss: 0.00000934
Iteration 171/1000 | Loss: 0.00000934
Iteration 172/1000 | Loss: 0.00000934
Iteration 173/1000 | Loss: 0.00000933
Iteration 174/1000 | Loss: 0.00000933
Iteration 175/1000 | Loss: 0.00000933
Iteration 176/1000 | Loss: 0.00000933
Iteration 177/1000 | Loss: 0.00000933
Iteration 178/1000 | Loss: 0.00000933
Iteration 179/1000 | Loss: 0.00000933
Iteration 180/1000 | Loss: 0.00000933
Iteration 181/1000 | Loss: 0.00000933
Iteration 182/1000 | Loss: 0.00000933
Iteration 183/1000 | Loss: 0.00000933
Iteration 184/1000 | Loss: 0.00000933
Iteration 185/1000 | Loss: 0.00000933
Iteration 186/1000 | Loss: 0.00000933
Iteration 187/1000 | Loss: 0.00000933
Iteration 188/1000 | Loss: 0.00000933
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [9.329036402050406e-06, 9.329036402050406e-06, 9.329036402050406e-06, 9.329036402050406e-06, 9.329036402050406e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.329036402050406e-06

Optimization complete. Final v2v error: 2.5954487323760986 mm

Highest mean error: 2.8450911045074463 mm for frame 82

Lowest mean error: 2.413115978240967 mm for frame 24

Saving results

Total time: 41.32523036003113
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00925400
Iteration 2/25 | Loss: 0.00155493
Iteration 3/25 | Loss: 0.00124829
Iteration 4/25 | Loss: 0.00122642
Iteration 5/25 | Loss: 0.00122096
Iteration 6/25 | Loss: 0.00121976
Iteration 7/25 | Loss: 0.00121967
Iteration 8/25 | Loss: 0.00121967
Iteration 9/25 | Loss: 0.00121967
Iteration 10/25 | Loss: 0.00121967
Iteration 11/25 | Loss: 0.00121967
Iteration 12/25 | Loss: 0.00121967
Iteration 13/25 | Loss: 0.00121967
Iteration 14/25 | Loss: 0.00121967
Iteration 15/25 | Loss: 0.00121967
Iteration 16/25 | Loss: 0.00121967
Iteration 17/25 | Loss: 0.00121967
Iteration 18/25 | Loss: 0.00121967
Iteration 19/25 | Loss: 0.00121967
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012196727329865098, 0.0012196727329865098, 0.0012196727329865098, 0.0012196727329865098, 0.0012196727329865098]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012196727329865098

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.88654631
Iteration 2/25 | Loss: 0.00082066
Iteration 3/25 | Loss: 0.00082064
Iteration 4/25 | Loss: 0.00082064
Iteration 5/25 | Loss: 0.00082064
Iteration 6/25 | Loss: 0.00082064
Iteration 7/25 | Loss: 0.00082064
Iteration 8/25 | Loss: 0.00082064
Iteration 9/25 | Loss: 0.00082064
Iteration 10/25 | Loss: 0.00082064
Iteration 11/25 | Loss: 0.00082064
Iteration 12/25 | Loss: 0.00082064
Iteration 13/25 | Loss: 0.00082064
Iteration 14/25 | Loss: 0.00082064
Iteration 15/25 | Loss: 0.00082064
Iteration 16/25 | Loss: 0.00082064
Iteration 17/25 | Loss: 0.00082064
Iteration 18/25 | Loss: 0.00082064
Iteration 19/25 | Loss: 0.00082064
Iteration 20/25 | Loss: 0.00082064
Iteration 21/25 | Loss: 0.00082064
Iteration 22/25 | Loss: 0.00082064
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008206412894651294, 0.0008206412894651294, 0.0008206412894651294, 0.0008206412894651294, 0.0008206412894651294]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008206412894651294

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082064
Iteration 2/1000 | Loss: 0.00006394
Iteration 3/1000 | Loss: 0.00004281
Iteration 4/1000 | Loss: 0.00003501
Iteration 5/1000 | Loss: 0.00003291
Iteration 6/1000 | Loss: 0.00003183
Iteration 7/1000 | Loss: 0.00003025
Iteration 8/1000 | Loss: 0.00002947
Iteration 9/1000 | Loss: 0.00002880
Iteration 10/1000 | Loss: 0.00002824
Iteration 11/1000 | Loss: 0.00002786
Iteration 12/1000 | Loss: 0.00002757
Iteration 13/1000 | Loss: 0.00002726
Iteration 14/1000 | Loss: 0.00002704
Iteration 15/1000 | Loss: 0.00002681
Iteration 16/1000 | Loss: 0.00002656
Iteration 17/1000 | Loss: 0.00002643
Iteration 18/1000 | Loss: 0.00002630
Iteration 19/1000 | Loss: 0.00002615
Iteration 20/1000 | Loss: 0.00002614
Iteration 21/1000 | Loss: 0.00002613
Iteration 22/1000 | Loss: 0.00002606
Iteration 23/1000 | Loss: 0.00002604
Iteration 24/1000 | Loss: 0.00002603
Iteration 25/1000 | Loss: 0.00002602
Iteration 26/1000 | Loss: 0.00002598
Iteration 27/1000 | Loss: 0.00002593
Iteration 28/1000 | Loss: 0.00002593
Iteration 29/1000 | Loss: 0.00002591
Iteration 30/1000 | Loss: 0.00002591
Iteration 31/1000 | Loss: 0.00002588
Iteration 32/1000 | Loss: 0.00002587
Iteration 33/1000 | Loss: 0.00002584
Iteration 34/1000 | Loss: 0.00002584
Iteration 35/1000 | Loss: 0.00002584
Iteration 36/1000 | Loss: 0.00002582
Iteration 37/1000 | Loss: 0.00002581
Iteration 38/1000 | Loss: 0.00002581
Iteration 39/1000 | Loss: 0.00002581
Iteration 40/1000 | Loss: 0.00002581
Iteration 41/1000 | Loss: 0.00002580
Iteration 42/1000 | Loss: 0.00002580
Iteration 43/1000 | Loss: 0.00002580
Iteration 44/1000 | Loss: 0.00002579
Iteration 45/1000 | Loss: 0.00002579
Iteration 46/1000 | Loss: 0.00002579
Iteration 47/1000 | Loss: 0.00002579
Iteration 48/1000 | Loss: 0.00002579
Iteration 49/1000 | Loss: 0.00002579
Iteration 50/1000 | Loss: 0.00002579
Iteration 51/1000 | Loss: 0.00002579
Iteration 52/1000 | Loss: 0.00002579
Iteration 53/1000 | Loss: 0.00002578
Iteration 54/1000 | Loss: 0.00002578
Iteration 55/1000 | Loss: 0.00002578
Iteration 56/1000 | Loss: 0.00002578
Iteration 57/1000 | Loss: 0.00002577
Iteration 58/1000 | Loss: 0.00002577
Iteration 59/1000 | Loss: 0.00002577
Iteration 60/1000 | Loss: 0.00002577
Iteration 61/1000 | Loss: 0.00002577
Iteration 62/1000 | Loss: 0.00002577
Iteration 63/1000 | Loss: 0.00002577
Iteration 64/1000 | Loss: 0.00002577
Iteration 65/1000 | Loss: 0.00002576
Iteration 66/1000 | Loss: 0.00002576
Iteration 67/1000 | Loss: 0.00002576
Iteration 68/1000 | Loss: 0.00002576
Iteration 69/1000 | Loss: 0.00002576
Iteration 70/1000 | Loss: 0.00002576
Iteration 71/1000 | Loss: 0.00002576
Iteration 72/1000 | Loss: 0.00002576
Iteration 73/1000 | Loss: 0.00002576
Iteration 74/1000 | Loss: 0.00002576
Iteration 75/1000 | Loss: 0.00002575
Iteration 76/1000 | Loss: 0.00002575
Iteration 77/1000 | Loss: 0.00002575
Iteration 78/1000 | Loss: 0.00002575
Iteration 79/1000 | Loss: 0.00002575
Iteration 80/1000 | Loss: 0.00002575
Iteration 81/1000 | Loss: 0.00002575
Iteration 82/1000 | Loss: 0.00002575
Iteration 83/1000 | Loss: 0.00002575
Iteration 84/1000 | Loss: 0.00002575
Iteration 85/1000 | Loss: 0.00002575
Iteration 86/1000 | Loss: 0.00002575
Iteration 87/1000 | Loss: 0.00002575
Iteration 88/1000 | Loss: 0.00002574
Iteration 89/1000 | Loss: 0.00002574
Iteration 90/1000 | Loss: 0.00002574
Iteration 91/1000 | Loss: 0.00002574
Iteration 92/1000 | Loss: 0.00002574
Iteration 93/1000 | Loss: 0.00002574
Iteration 94/1000 | Loss: 0.00002574
Iteration 95/1000 | Loss: 0.00002573
Iteration 96/1000 | Loss: 0.00002573
Iteration 97/1000 | Loss: 0.00002573
Iteration 98/1000 | Loss: 0.00002573
Iteration 99/1000 | Loss: 0.00002573
Iteration 100/1000 | Loss: 0.00002573
Iteration 101/1000 | Loss: 0.00002573
Iteration 102/1000 | Loss: 0.00002572
Iteration 103/1000 | Loss: 0.00002572
Iteration 104/1000 | Loss: 0.00002572
Iteration 105/1000 | Loss: 0.00002572
Iteration 106/1000 | Loss: 0.00002572
Iteration 107/1000 | Loss: 0.00002572
Iteration 108/1000 | Loss: 0.00002572
Iteration 109/1000 | Loss: 0.00002572
Iteration 110/1000 | Loss: 0.00002572
Iteration 111/1000 | Loss: 0.00002572
Iteration 112/1000 | Loss: 0.00002572
Iteration 113/1000 | Loss: 0.00002572
Iteration 114/1000 | Loss: 0.00002572
Iteration 115/1000 | Loss: 0.00002572
Iteration 116/1000 | Loss: 0.00002572
Iteration 117/1000 | Loss: 0.00002572
Iteration 118/1000 | Loss: 0.00002572
Iteration 119/1000 | Loss: 0.00002571
Iteration 120/1000 | Loss: 0.00002571
Iteration 121/1000 | Loss: 0.00002571
Iteration 122/1000 | Loss: 0.00002571
Iteration 123/1000 | Loss: 0.00002571
Iteration 124/1000 | Loss: 0.00002571
Iteration 125/1000 | Loss: 0.00002571
Iteration 126/1000 | Loss: 0.00002571
Iteration 127/1000 | Loss: 0.00002571
Iteration 128/1000 | Loss: 0.00002571
Iteration 129/1000 | Loss: 0.00002571
Iteration 130/1000 | Loss: 0.00002571
Iteration 131/1000 | Loss: 0.00002571
Iteration 132/1000 | Loss: 0.00002571
Iteration 133/1000 | Loss: 0.00002571
Iteration 134/1000 | Loss: 0.00002570
Iteration 135/1000 | Loss: 0.00002570
Iteration 136/1000 | Loss: 0.00002570
Iteration 137/1000 | Loss: 0.00002570
Iteration 138/1000 | Loss: 0.00002570
Iteration 139/1000 | Loss: 0.00002570
Iteration 140/1000 | Loss: 0.00002570
Iteration 141/1000 | Loss: 0.00002570
Iteration 142/1000 | Loss: 0.00002570
Iteration 143/1000 | Loss: 0.00002570
Iteration 144/1000 | Loss: 0.00002570
Iteration 145/1000 | Loss: 0.00002570
Iteration 146/1000 | Loss: 0.00002570
Iteration 147/1000 | Loss: 0.00002569
Iteration 148/1000 | Loss: 0.00002569
Iteration 149/1000 | Loss: 0.00002569
Iteration 150/1000 | Loss: 0.00002569
Iteration 151/1000 | Loss: 0.00002569
Iteration 152/1000 | Loss: 0.00002569
Iteration 153/1000 | Loss: 0.00002569
Iteration 154/1000 | Loss: 0.00002569
Iteration 155/1000 | Loss: 0.00002569
Iteration 156/1000 | Loss: 0.00002569
Iteration 157/1000 | Loss: 0.00002569
Iteration 158/1000 | Loss: 0.00002569
Iteration 159/1000 | Loss: 0.00002569
Iteration 160/1000 | Loss: 0.00002569
Iteration 161/1000 | Loss: 0.00002569
Iteration 162/1000 | Loss: 0.00002569
Iteration 163/1000 | Loss: 0.00002569
Iteration 164/1000 | Loss: 0.00002569
Iteration 165/1000 | Loss: 0.00002569
Iteration 166/1000 | Loss: 0.00002568
Iteration 167/1000 | Loss: 0.00002568
Iteration 168/1000 | Loss: 0.00002568
Iteration 169/1000 | Loss: 0.00002568
Iteration 170/1000 | Loss: 0.00002568
Iteration 171/1000 | Loss: 0.00002568
Iteration 172/1000 | Loss: 0.00002568
Iteration 173/1000 | Loss: 0.00002568
Iteration 174/1000 | Loss: 0.00002568
Iteration 175/1000 | Loss: 0.00002568
Iteration 176/1000 | Loss: 0.00002568
Iteration 177/1000 | Loss: 0.00002567
Iteration 178/1000 | Loss: 0.00002567
Iteration 179/1000 | Loss: 0.00002567
Iteration 180/1000 | Loss: 0.00002567
Iteration 181/1000 | Loss: 0.00002567
Iteration 182/1000 | Loss: 0.00002567
Iteration 183/1000 | Loss: 0.00002567
Iteration 184/1000 | Loss: 0.00002567
Iteration 185/1000 | Loss: 0.00002567
Iteration 186/1000 | Loss: 0.00002567
Iteration 187/1000 | Loss: 0.00002567
Iteration 188/1000 | Loss: 0.00002567
Iteration 189/1000 | Loss: 0.00002567
Iteration 190/1000 | Loss: 0.00002567
Iteration 191/1000 | Loss: 0.00002567
Iteration 192/1000 | Loss: 0.00002567
Iteration 193/1000 | Loss: 0.00002567
Iteration 194/1000 | Loss: 0.00002567
Iteration 195/1000 | Loss: 0.00002566
Iteration 196/1000 | Loss: 0.00002566
Iteration 197/1000 | Loss: 0.00002566
Iteration 198/1000 | Loss: 0.00002566
Iteration 199/1000 | Loss: 0.00002566
Iteration 200/1000 | Loss: 0.00002566
Iteration 201/1000 | Loss: 0.00002566
Iteration 202/1000 | Loss: 0.00002566
Iteration 203/1000 | Loss: 0.00002566
Iteration 204/1000 | Loss: 0.00002566
Iteration 205/1000 | Loss: 0.00002566
Iteration 206/1000 | Loss: 0.00002566
Iteration 207/1000 | Loss: 0.00002566
Iteration 208/1000 | Loss: 0.00002566
Iteration 209/1000 | Loss: 0.00002566
Iteration 210/1000 | Loss: 0.00002566
Iteration 211/1000 | Loss: 0.00002566
Iteration 212/1000 | Loss: 0.00002566
Iteration 213/1000 | Loss: 0.00002566
Iteration 214/1000 | Loss: 0.00002566
Iteration 215/1000 | Loss: 0.00002566
Iteration 216/1000 | Loss: 0.00002566
Iteration 217/1000 | Loss: 0.00002566
Iteration 218/1000 | Loss: 0.00002566
Iteration 219/1000 | Loss: 0.00002566
Iteration 220/1000 | Loss: 0.00002566
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [2.5663888663984835e-05, 2.5663888663984835e-05, 2.5663888663984835e-05, 2.5663888663984835e-05, 2.5663888663984835e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5663888663984835e-05

Optimization complete. Final v2v error: 4.2174296379089355 mm

Highest mean error: 5.113841533660889 mm for frame 128

Lowest mean error: 3.3453023433685303 mm for frame 26

Saving results

Total time: 50.19445466995239
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01047714
Iteration 2/25 | Loss: 0.01047713
Iteration 3/25 | Loss: 0.00174748
Iteration 4/25 | Loss: 0.00138456
Iteration 5/25 | Loss: 0.00129410
Iteration 6/25 | Loss: 0.00134705
Iteration 7/25 | Loss: 0.00123836
Iteration 8/25 | Loss: 0.00117172
Iteration 9/25 | Loss: 0.00115130
Iteration 10/25 | Loss: 0.00114785
Iteration 11/25 | Loss: 0.00114706
Iteration 12/25 | Loss: 0.00114696
Iteration 13/25 | Loss: 0.00114696
Iteration 14/25 | Loss: 0.00114695
Iteration 15/25 | Loss: 0.00114694
Iteration 16/25 | Loss: 0.00114694
Iteration 17/25 | Loss: 0.00114694
Iteration 18/25 | Loss: 0.00114693
Iteration 19/25 | Loss: 0.00114693
Iteration 20/25 | Loss: 0.00114693
Iteration 21/25 | Loss: 0.00114693
Iteration 22/25 | Loss: 0.00114693
Iteration 23/25 | Loss: 0.00114693
Iteration 24/25 | Loss: 0.00114693
Iteration 25/25 | Loss: 0.00114693

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40177166
Iteration 2/25 | Loss: 0.00075091
Iteration 3/25 | Loss: 0.00075091
Iteration 4/25 | Loss: 0.00075091
Iteration 5/25 | Loss: 0.00075091
Iteration 6/25 | Loss: 0.00075091
Iteration 7/25 | Loss: 0.00075091
Iteration 8/25 | Loss: 0.00075091
Iteration 9/25 | Loss: 0.00075091
Iteration 10/25 | Loss: 0.00075091
Iteration 11/25 | Loss: 0.00075091
Iteration 12/25 | Loss: 0.00075091
Iteration 13/25 | Loss: 0.00075091
Iteration 14/25 | Loss: 0.00075091
Iteration 15/25 | Loss: 0.00075091
Iteration 16/25 | Loss: 0.00075091
Iteration 17/25 | Loss: 0.00075091
Iteration 18/25 | Loss: 0.00075091
Iteration 19/25 | Loss: 0.00075091
Iteration 20/25 | Loss: 0.00075091
Iteration 21/25 | Loss: 0.00075091
Iteration 22/25 | Loss: 0.00075091
Iteration 23/25 | Loss: 0.00075091
Iteration 24/25 | Loss: 0.00075091
Iteration 25/25 | Loss: 0.00075091

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075091
Iteration 2/1000 | Loss: 0.00002496
Iteration 3/1000 | Loss: 0.00002018
Iteration 4/1000 | Loss: 0.00001933
Iteration 5/1000 | Loss: 0.00001858
Iteration 6/1000 | Loss: 0.00001803
Iteration 7/1000 | Loss: 0.00001771
Iteration 8/1000 | Loss: 0.00001744
Iteration 9/1000 | Loss: 0.00001724
Iteration 10/1000 | Loss: 0.00001703
Iteration 11/1000 | Loss: 0.00001685
Iteration 12/1000 | Loss: 0.00001679
Iteration 13/1000 | Loss: 0.00001676
Iteration 14/1000 | Loss: 0.00001675
Iteration 15/1000 | Loss: 0.00001673
Iteration 16/1000 | Loss: 0.00001672
Iteration 17/1000 | Loss: 0.00001672
Iteration 18/1000 | Loss: 0.00001672
Iteration 19/1000 | Loss: 0.00001672
Iteration 20/1000 | Loss: 0.00001671
Iteration 21/1000 | Loss: 0.00001671
Iteration 22/1000 | Loss: 0.00001670
Iteration 23/1000 | Loss: 0.00001669
Iteration 24/1000 | Loss: 0.00001668
Iteration 25/1000 | Loss: 0.00001664
Iteration 26/1000 | Loss: 0.00001660
Iteration 27/1000 | Loss: 0.00001660
Iteration 28/1000 | Loss: 0.00001660
Iteration 29/1000 | Loss: 0.00001659
Iteration 30/1000 | Loss: 0.00001659
Iteration 31/1000 | Loss: 0.00001659
Iteration 32/1000 | Loss: 0.00001659
Iteration 33/1000 | Loss: 0.00001658
Iteration 34/1000 | Loss: 0.00001657
Iteration 35/1000 | Loss: 0.00001654
Iteration 36/1000 | Loss: 0.00001654
Iteration 37/1000 | Loss: 0.00001654
Iteration 38/1000 | Loss: 0.00001653
Iteration 39/1000 | Loss: 0.00001652
Iteration 40/1000 | Loss: 0.00001652
Iteration 41/1000 | Loss: 0.00001652
Iteration 42/1000 | Loss: 0.00001652
Iteration 43/1000 | Loss: 0.00001652
Iteration 44/1000 | Loss: 0.00001651
Iteration 45/1000 | Loss: 0.00001651
Iteration 46/1000 | Loss: 0.00001651
Iteration 47/1000 | Loss: 0.00001650
Iteration 48/1000 | Loss: 0.00001650
Iteration 49/1000 | Loss: 0.00001650
Iteration 50/1000 | Loss: 0.00001650
Iteration 51/1000 | Loss: 0.00001650
Iteration 52/1000 | Loss: 0.00001649
Iteration 53/1000 | Loss: 0.00001649
Iteration 54/1000 | Loss: 0.00001649
Iteration 55/1000 | Loss: 0.00001649
Iteration 56/1000 | Loss: 0.00001649
Iteration 57/1000 | Loss: 0.00001649
Iteration 58/1000 | Loss: 0.00001649
Iteration 59/1000 | Loss: 0.00001649
Iteration 60/1000 | Loss: 0.00001648
Iteration 61/1000 | Loss: 0.00001648
Iteration 62/1000 | Loss: 0.00001648
Iteration 63/1000 | Loss: 0.00001648
Iteration 64/1000 | Loss: 0.00001648
Iteration 65/1000 | Loss: 0.00001648
Iteration 66/1000 | Loss: 0.00001647
Iteration 67/1000 | Loss: 0.00001647
Iteration 68/1000 | Loss: 0.00001647
Iteration 69/1000 | Loss: 0.00001647
Iteration 70/1000 | Loss: 0.00001647
Iteration 71/1000 | Loss: 0.00001647
Iteration 72/1000 | Loss: 0.00001647
Iteration 73/1000 | Loss: 0.00001647
Iteration 74/1000 | Loss: 0.00001647
Iteration 75/1000 | Loss: 0.00001647
Iteration 76/1000 | Loss: 0.00001647
Iteration 77/1000 | Loss: 0.00001647
Iteration 78/1000 | Loss: 0.00001647
Iteration 79/1000 | Loss: 0.00001646
Iteration 80/1000 | Loss: 0.00001646
Iteration 81/1000 | Loss: 0.00001646
Iteration 82/1000 | Loss: 0.00001646
Iteration 83/1000 | Loss: 0.00001646
Iteration 84/1000 | Loss: 0.00001646
Iteration 85/1000 | Loss: 0.00001645
Iteration 86/1000 | Loss: 0.00001645
Iteration 87/1000 | Loss: 0.00001645
Iteration 88/1000 | Loss: 0.00001645
Iteration 89/1000 | Loss: 0.00001645
Iteration 90/1000 | Loss: 0.00001645
Iteration 91/1000 | Loss: 0.00001644
Iteration 92/1000 | Loss: 0.00001644
Iteration 93/1000 | Loss: 0.00001644
Iteration 94/1000 | Loss: 0.00001644
Iteration 95/1000 | Loss: 0.00001644
Iteration 96/1000 | Loss: 0.00001644
Iteration 97/1000 | Loss: 0.00001644
Iteration 98/1000 | Loss: 0.00001644
Iteration 99/1000 | Loss: 0.00001644
Iteration 100/1000 | Loss: 0.00001644
Iteration 101/1000 | Loss: 0.00001644
Iteration 102/1000 | Loss: 0.00001644
Iteration 103/1000 | Loss: 0.00001643
Iteration 104/1000 | Loss: 0.00001643
Iteration 105/1000 | Loss: 0.00001643
Iteration 106/1000 | Loss: 0.00001643
Iteration 107/1000 | Loss: 0.00001643
Iteration 108/1000 | Loss: 0.00001643
Iteration 109/1000 | Loss: 0.00001643
Iteration 110/1000 | Loss: 0.00001643
Iteration 111/1000 | Loss: 0.00001642
Iteration 112/1000 | Loss: 0.00001642
Iteration 113/1000 | Loss: 0.00001642
Iteration 114/1000 | Loss: 0.00001641
Iteration 115/1000 | Loss: 0.00001641
Iteration 116/1000 | Loss: 0.00001641
Iteration 117/1000 | Loss: 0.00001641
Iteration 118/1000 | Loss: 0.00001641
Iteration 119/1000 | Loss: 0.00001641
Iteration 120/1000 | Loss: 0.00001641
Iteration 121/1000 | Loss: 0.00001641
Iteration 122/1000 | Loss: 0.00001641
Iteration 123/1000 | Loss: 0.00001641
Iteration 124/1000 | Loss: 0.00001641
Iteration 125/1000 | Loss: 0.00001641
Iteration 126/1000 | Loss: 0.00001641
Iteration 127/1000 | Loss: 0.00001641
Iteration 128/1000 | Loss: 0.00001640
Iteration 129/1000 | Loss: 0.00001640
Iteration 130/1000 | Loss: 0.00001640
Iteration 131/1000 | Loss: 0.00001640
Iteration 132/1000 | Loss: 0.00001640
Iteration 133/1000 | Loss: 0.00001640
Iteration 134/1000 | Loss: 0.00001640
Iteration 135/1000 | Loss: 0.00001640
Iteration 136/1000 | Loss: 0.00001640
Iteration 137/1000 | Loss: 0.00001640
Iteration 138/1000 | Loss: 0.00001640
Iteration 139/1000 | Loss: 0.00001640
Iteration 140/1000 | Loss: 0.00001640
Iteration 141/1000 | Loss: 0.00001640
Iteration 142/1000 | Loss: 0.00001639
Iteration 143/1000 | Loss: 0.00001639
Iteration 144/1000 | Loss: 0.00001639
Iteration 145/1000 | Loss: 0.00001639
Iteration 146/1000 | Loss: 0.00001639
Iteration 147/1000 | Loss: 0.00001639
Iteration 148/1000 | Loss: 0.00001639
Iteration 149/1000 | Loss: 0.00001639
Iteration 150/1000 | Loss: 0.00001639
Iteration 151/1000 | Loss: 0.00001638
Iteration 152/1000 | Loss: 0.00001638
Iteration 153/1000 | Loss: 0.00001638
Iteration 154/1000 | Loss: 0.00001638
Iteration 155/1000 | Loss: 0.00001638
Iteration 156/1000 | Loss: 0.00001638
Iteration 157/1000 | Loss: 0.00001638
Iteration 158/1000 | Loss: 0.00001637
Iteration 159/1000 | Loss: 0.00001637
Iteration 160/1000 | Loss: 0.00001637
Iteration 161/1000 | Loss: 0.00001637
Iteration 162/1000 | Loss: 0.00001637
Iteration 163/1000 | Loss: 0.00001637
Iteration 164/1000 | Loss: 0.00001637
Iteration 165/1000 | Loss: 0.00001637
Iteration 166/1000 | Loss: 0.00001637
Iteration 167/1000 | Loss: 0.00001637
Iteration 168/1000 | Loss: 0.00001637
Iteration 169/1000 | Loss: 0.00001637
Iteration 170/1000 | Loss: 0.00001637
Iteration 171/1000 | Loss: 0.00001637
Iteration 172/1000 | Loss: 0.00001637
Iteration 173/1000 | Loss: 0.00001637
Iteration 174/1000 | Loss: 0.00001637
Iteration 175/1000 | Loss: 0.00001637
Iteration 176/1000 | Loss: 0.00001637
Iteration 177/1000 | Loss: 0.00001637
Iteration 178/1000 | Loss: 0.00001637
Iteration 179/1000 | Loss: 0.00001637
Iteration 180/1000 | Loss: 0.00001637
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.6365362171200104e-05, 1.6365362171200104e-05, 1.6365362171200104e-05, 1.6365362171200104e-05, 1.6365362171200104e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6365362171200104e-05

Optimization complete. Final v2v error: 3.4066085815429688 mm

Highest mean error: 3.9450204372406006 mm for frame 47

Lowest mean error: 3.0508644580841064 mm for frame 151

Saving results

Total time: 54.524123668670654
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00803703
Iteration 2/25 | Loss: 0.00133306
Iteration 3/25 | Loss: 0.00112759
Iteration 4/25 | Loss: 0.00109909
Iteration 5/25 | Loss: 0.00109199
Iteration 6/25 | Loss: 0.00108966
Iteration 7/25 | Loss: 0.00108966
Iteration 8/25 | Loss: 0.00108966
Iteration 9/25 | Loss: 0.00108966
Iteration 10/25 | Loss: 0.00108966
Iteration 11/25 | Loss: 0.00108966
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001089661382138729, 0.001089661382138729, 0.001089661382138729, 0.001089661382138729, 0.001089661382138729]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001089661382138729

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37714946
Iteration 2/25 | Loss: 0.00100568
Iteration 3/25 | Loss: 0.00100567
Iteration 4/25 | Loss: 0.00100567
Iteration 5/25 | Loss: 0.00100567
Iteration 6/25 | Loss: 0.00100567
Iteration 7/25 | Loss: 0.00100567
Iteration 8/25 | Loss: 0.00100567
Iteration 9/25 | Loss: 0.00100567
Iteration 10/25 | Loss: 0.00100567
Iteration 11/25 | Loss: 0.00100567
Iteration 12/25 | Loss: 0.00100567
Iteration 13/25 | Loss: 0.00100567
Iteration 14/25 | Loss: 0.00100567
Iteration 15/25 | Loss: 0.00100567
Iteration 16/25 | Loss: 0.00100567
Iteration 17/25 | Loss: 0.00100567
Iteration 18/25 | Loss: 0.00100567
Iteration 19/25 | Loss: 0.00100567
Iteration 20/25 | Loss: 0.00100567
Iteration 21/25 | Loss: 0.00100567
Iteration 22/25 | Loss: 0.00100567
Iteration 23/25 | Loss: 0.00100567
Iteration 24/25 | Loss: 0.00100567
Iteration 25/25 | Loss: 0.00100567

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100567
Iteration 2/1000 | Loss: 0.00004010
Iteration 3/1000 | Loss: 0.00002378
Iteration 4/1000 | Loss: 0.00001832
Iteration 5/1000 | Loss: 0.00001624
Iteration 6/1000 | Loss: 0.00001503
Iteration 7/1000 | Loss: 0.00001425
Iteration 8/1000 | Loss: 0.00001363
Iteration 9/1000 | Loss: 0.00001320
Iteration 10/1000 | Loss: 0.00001287
Iteration 11/1000 | Loss: 0.00001263
Iteration 12/1000 | Loss: 0.00001258
Iteration 13/1000 | Loss: 0.00001251
Iteration 14/1000 | Loss: 0.00001234
Iteration 15/1000 | Loss: 0.00001231
Iteration 16/1000 | Loss: 0.00001230
Iteration 17/1000 | Loss: 0.00001229
Iteration 18/1000 | Loss: 0.00001227
Iteration 19/1000 | Loss: 0.00001226
Iteration 20/1000 | Loss: 0.00001225
Iteration 21/1000 | Loss: 0.00001224
Iteration 22/1000 | Loss: 0.00001223
Iteration 23/1000 | Loss: 0.00001223
Iteration 24/1000 | Loss: 0.00001222
Iteration 25/1000 | Loss: 0.00001220
Iteration 26/1000 | Loss: 0.00001219
Iteration 27/1000 | Loss: 0.00001218
Iteration 28/1000 | Loss: 0.00001218
Iteration 29/1000 | Loss: 0.00001217
Iteration 30/1000 | Loss: 0.00001215
Iteration 31/1000 | Loss: 0.00001214
Iteration 32/1000 | Loss: 0.00001214
Iteration 33/1000 | Loss: 0.00001213
Iteration 34/1000 | Loss: 0.00001212
Iteration 35/1000 | Loss: 0.00001211
Iteration 36/1000 | Loss: 0.00001210
Iteration 37/1000 | Loss: 0.00001210
Iteration 38/1000 | Loss: 0.00001209
Iteration 39/1000 | Loss: 0.00001209
Iteration 40/1000 | Loss: 0.00001209
Iteration 41/1000 | Loss: 0.00001208
Iteration 42/1000 | Loss: 0.00001208
Iteration 43/1000 | Loss: 0.00001207
Iteration 44/1000 | Loss: 0.00001207
Iteration 45/1000 | Loss: 0.00001206
Iteration 46/1000 | Loss: 0.00001206
Iteration 47/1000 | Loss: 0.00001205
Iteration 48/1000 | Loss: 0.00001205
Iteration 49/1000 | Loss: 0.00001205
Iteration 50/1000 | Loss: 0.00001205
Iteration 51/1000 | Loss: 0.00001205
Iteration 52/1000 | Loss: 0.00001204
Iteration 53/1000 | Loss: 0.00001204
Iteration 54/1000 | Loss: 0.00001203
Iteration 55/1000 | Loss: 0.00001201
Iteration 56/1000 | Loss: 0.00001200
Iteration 57/1000 | Loss: 0.00001199
Iteration 58/1000 | Loss: 0.00001199
Iteration 59/1000 | Loss: 0.00001199
Iteration 60/1000 | Loss: 0.00001198
Iteration 61/1000 | Loss: 0.00001197
Iteration 62/1000 | Loss: 0.00001196
Iteration 63/1000 | Loss: 0.00001195
Iteration 64/1000 | Loss: 0.00001195
Iteration 65/1000 | Loss: 0.00001194
Iteration 66/1000 | Loss: 0.00001194
Iteration 67/1000 | Loss: 0.00001194
Iteration 68/1000 | Loss: 0.00001193
Iteration 69/1000 | Loss: 0.00001193
Iteration 70/1000 | Loss: 0.00001192
Iteration 71/1000 | Loss: 0.00001192
Iteration 72/1000 | Loss: 0.00001192
Iteration 73/1000 | Loss: 0.00001192
Iteration 74/1000 | Loss: 0.00001192
Iteration 75/1000 | Loss: 0.00001191
Iteration 76/1000 | Loss: 0.00001191
Iteration 77/1000 | Loss: 0.00001190
Iteration 78/1000 | Loss: 0.00001190
Iteration 79/1000 | Loss: 0.00001190
Iteration 80/1000 | Loss: 0.00001189
Iteration 81/1000 | Loss: 0.00001189
Iteration 82/1000 | Loss: 0.00001189
Iteration 83/1000 | Loss: 0.00001188
Iteration 84/1000 | Loss: 0.00001188
Iteration 85/1000 | Loss: 0.00001187
Iteration 86/1000 | Loss: 0.00001187
Iteration 87/1000 | Loss: 0.00001187
Iteration 88/1000 | Loss: 0.00001187
Iteration 89/1000 | Loss: 0.00001186
Iteration 90/1000 | Loss: 0.00001186
Iteration 91/1000 | Loss: 0.00001186
Iteration 92/1000 | Loss: 0.00001185
Iteration 93/1000 | Loss: 0.00001185
Iteration 94/1000 | Loss: 0.00001185
Iteration 95/1000 | Loss: 0.00001185
Iteration 96/1000 | Loss: 0.00001185
Iteration 97/1000 | Loss: 0.00001184
Iteration 98/1000 | Loss: 0.00001184
Iteration 99/1000 | Loss: 0.00001184
Iteration 100/1000 | Loss: 0.00001183
Iteration 101/1000 | Loss: 0.00001183
Iteration 102/1000 | Loss: 0.00001183
Iteration 103/1000 | Loss: 0.00001182
Iteration 104/1000 | Loss: 0.00001182
Iteration 105/1000 | Loss: 0.00001182
Iteration 106/1000 | Loss: 0.00001182
Iteration 107/1000 | Loss: 0.00001182
Iteration 108/1000 | Loss: 0.00001181
Iteration 109/1000 | Loss: 0.00001181
Iteration 110/1000 | Loss: 0.00001181
Iteration 111/1000 | Loss: 0.00001181
Iteration 112/1000 | Loss: 0.00001181
Iteration 113/1000 | Loss: 0.00001181
Iteration 114/1000 | Loss: 0.00001181
Iteration 115/1000 | Loss: 0.00001181
Iteration 116/1000 | Loss: 0.00001180
Iteration 117/1000 | Loss: 0.00001180
Iteration 118/1000 | Loss: 0.00001180
Iteration 119/1000 | Loss: 0.00001179
Iteration 120/1000 | Loss: 0.00001179
Iteration 121/1000 | Loss: 0.00001179
Iteration 122/1000 | Loss: 0.00001179
Iteration 123/1000 | Loss: 0.00001179
Iteration 124/1000 | Loss: 0.00001179
Iteration 125/1000 | Loss: 0.00001179
Iteration 126/1000 | Loss: 0.00001179
Iteration 127/1000 | Loss: 0.00001179
Iteration 128/1000 | Loss: 0.00001178
Iteration 129/1000 | Loss: 0.00001178
Iteration 130/1000 | Loss: 0.00001178
Iteration 131/1000 | Loss: 0.00001178
Iteration 132/1000 | Loss: 0.00001178
Iteration 133/1000 | Loss: 0.00001177
Iteration 134/1000 | Loss: 0.00001177
Iteration 135/1000 | Loss: 0.00001177
Iteration 136/1000 | Loss: 0.00001177
Iteration 137/1000 | Loss: 0.00001177
Iteration 138/1000 | Loss: 0.00001177
Iteration 139/1000 | Loss: 0.00001177
Iteration 140/1000 | Loss: 0.00001177
Iteration 141/1000 | Loss: 0.00001177
Iteration 142/1000 | Loss: 0.00001177
Iteration 143/1000 | Loss: 0.00001177
Iteration 144/1000 | Loss: 0.00001177
Iteration 145/1000 | Loss: 0.00001176
Iteration 146/1000 | Loss: 0.00001176
Iteration 147/1000 | Loss: 0.00001176
Iteration 148/1000 | Loss: 0.00001175
Iteration 149/1000 | Loss: 0.00001175
Iteration 150/1000 | Loss: 0.00001175
Iteration 151/1000 | Loss: 0.00001175
Iteration 152/1000 | Loss: 0.00001175
Iteration 153/1000 | Loss: 0.00001174
Iteration 154/1000 | Loss: 0.00001174
Iteration 155/1000 | Loss: 0.00001174
Iteration 156/1000 | Loss: 0.00001173
Iteration 157/1000 | Loss: 0.00001173
Iteration 158/1000 | Loss: 0.00001173
Iteration 159/1000 | Loss: 0.00001173
Iteration 160/1000 | Loss: 0.00001172
Iteration 161/1000 | Loss: 0.00001172
Iteration 162/1000 | Loss: 0.00001172
Iteration 163/1000 | Loss: 0.00001172
Iteration 164/1000 | Loss: 0.00001172
Iteration 165/1000 | Loss: 0.00001171
Iteration 166/1000 | Loss: 0.00001171
Iteration 167/1000 | Loss: 0.00001171
Iteration 168/1000 | Loss: 0.00001171
Iteration 169/1000 | Loss: 0.00001170
Iteration 170/1000 | Loss: 0.00001170
Iteration 171/1000 | Loss: 0.00001170
Iteration 172/1000 | Loss: 0.00001170
Iteration 173/1000 | Loss: 0.00001170
Iteration 174/1000 | Loss: 0.00001170
Iteration 175/1000 | Loss: 0.00001169
Iteration 176/1000 | Loss: 0.00001169
Iteration 177/1000 | Loss: 0.00001169
Iteration 178/1000 | Loss: 0.00001169
Iteration 179/1000 | Loss: 0.00001169
Iteration 180/1000 | Loss: 0.00001169
Iteration 181/1000 | Loss: 0.00001169
Iteration 182/1000 | Loss: 0.00001169
Iteration 183/1000 | Loss: 0.00001169
Iteration 184/1000 | Loss: 0.00001169
Iteration 185/1000 | Loss: 0.00001169
Iteration 186/1000 | Loss: 0.00001168
Iteration 187/1000 | Loss: 0.00001168
Iteration 188/1000 | Loss: 0.00001168
Iteration 189/1000 | Loss: 0.00001168
Iteration 190/1000 | Loss: 0.00001168
Iteration 191/1000 | Loss: 0.00001168
Iteration 192/1000 | Loss: 0.00001168
Iteration 193/1000 | Loss: 0.00001167
Iteration 194/1000 | Loss: 0.00001167
Iteration 195/1000 | Loss: 0.00001167
Iteration 196/1000 | Loss: 0.00001167
Iteration 197/1000 | Loss: 0.00001167
Iteration 198/1000 | Loss: 0.00001167
Iteration 199/1000 | Loss: 0.00001167
Iteration 200/1000 | Loss: 0.00001167
Iteration 201/1000 | Loss: 0.00001166
Iteration 202/1000 | Loss: 0.00001166
Iteration 203/1000 | Loss: 0.00001166
Iteration 204/1000 | Loss: 0.00001165
Iteration 205/1000 | Loss: 0.00001165
Iteration 206/1000 | Loss: 0.00001165
Iteration 207/1000 | Loss: 0.00001165
Iteration 208/1000 | Loss: 0.00001165
Iteration 209/1000 | Loss: 0.00001165
Iteration 210/1000 | Loss: 0.00001164
Iteration 211/1000 | Loss: 0.00001164
Iteration 212/1000 | Loss: 0.00001164
Iteration 213/1000 | Loss: 0.00001164
Iteration 214/1000 | Loss: 0.00001164
Iteration 215/1000 | Loss: 0.00001163
Iteration 216/1000 | Loss: 0.00001163
Iteration 217/1000 | Loss: 0.00001163
Iteration 218/1000 | Loss: 0.00001163
Iteration 219/1000 | Loss: 0.00001163
Iteration 220/1000 | Loss: 0.00001163
Iteration 221/1000 | Loss: 0.00001163
Iteration 222/1000 | Loss: 0.00001163
Iteration 223/1000 | Loss: 0.00001162
Iteration 224/1000 | Loss: 0.00001162
Iteration 225/1000 | Loss: 0.00001162
Iteration 226/1000 | Loss: 0.00001162
Iteration 227/1000 | Loss: 0.00001161
Iteration 228/1000 | Loss: 0.00001161
Iteration 229/1000 | Loss: 0.00001161
Iteration 230/1000 | Loss: 0.00001161
Iteration 231/1000 | Loss: 0.00001161
Iteration 232/1000 | Loss: 0.00001161
Iteration 233/1000 | Loss: 0.00001161
Iteration 234/1000 | Loss: 0.00001161
Iteration 235/1000 | Loss: 0.00001160
Iteration 236/1000 | Loss: 0.00001160
Iteration 237/1000 | Loss: 0.00001160
Iteration 238/1000 | Loss: 0.00001160
Iteration 239/1000 | Loss: 0.00001160
Iteration 240/1000 | Loss: 0.00001160
Iteration 241/1000 | Loss: 0.00001160
Iteration 242/1000 | Loss: 0.00001160
Iteration 243/1000 | Loss: 0.00001160
Iteration 244/1000 | Loss: 0.00001160
Iteration 245/1000 | Loss: 0.00001159
Iteration 246/1000 | Loss: 0.00001159
Iteration 247/1000 | Loss: 0.00001159
Iteration 248/1000 | Loss: 0.00001159
Iteration 249/1000 | Loss: 0.00001159
Iteration 250/1000 | Loss: 0.00001159
Iteration 251/1000 | Loss: 0.00001158
Iteration 252/1000 | Loss: 0.00001158
Iteration 253/1000 | Loss: 0.00001158
Iteration 254/1000 | Loss: 0.00001158
Iteration 255/1000 | Loss: 0.00001158
Iteration 256/1000 | Loss: 0.00001158
Iteration 257/1000 | Loss: 0.00001158
Iteration 258/1000 | Loss: 0.00001158
Iteration 259/1000 | Loss: 0.00001158
Iteration 260/1000 | Loss: 0.00001158
Iteration 261/1000 | Loss: 0.00001158
Iteration 262/1000 | Loss: 0.00001158
Iteration 263/1000 | Loss: 0.00001158
Iteration 264/1000 | Loss: 0.00001158
Iteration 265/1000 | Loss: 0.00001158
Iteration 266/1000 | Loss: 0.00001158
Iteration 267/1000 | Loss: 0.00001158
Iteration 268/1000 | Loss: 0.00001158
Iteration 269/1000 | Loss: 0.00001158
Iteration 270/1000 | Loss: 0.00001158
Iteration 271/1000 | Loss: 0.00001158
Iteration 272/1000 | Loss: 0.00001158
Iteration 273/1000 | Loss: 0.00001158
Iteration 274/1000 | Loss: 0.00001158
Iteration 275/1000 | Loss: 0.00001158
Iteration 276/1000 | Loss: 0.00001158
Iteration 277/1000 | Loss: 0.00001158
Iteration 278/1000 | Loss: 0.00001158
Iteration 279/1000 | Loss: 0.00001158
Iteration 280/1000 | Loss: 0.00001158
Iteration 281/1000 | Loss: 0.00001158
Iteration 282/1000 | Loss: 0.00001158
Iteration 283/1000 | Loss: 0.00001158
Iteration 284/1000 | Loss: 0.00001158
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 284. Stopping optimization.
Last 5 losses: [1.1575057214940898e-05, 1.1575057214940898e-05, 1.1575057214940898e-05, 1.1575057214940898e-05, 1.1575057214940898e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1575057214940898e-05

Optimization complete. Final v2v error: 2.897254467010498 mm

Highest mean error: 3.3703410625457764 mm for frame 79

Lowest mean error: 2.542846202850342 mm for frame 154

Saving results

Total time: 48.6424343585968
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00550438
Iteration 2/25 | Loss: 0.00130296
Iteration 3/25 | Loss: 0.00118492
Iteration 4/25 | Loss: 0.00116834
Iteration 5/25 | Loss: 0.00116292
Iteration 6/25 | Loss: 0.00116229
Iteration 7/25 | Loss: 0.00116229
Iteration 8/25 | Loss: 0.00116229
Iteration 9/25 | Loss: 0.00116229
Iteration 10/25 | Loss: 0.00116229
Iteration 11/25 | Loss: 0.00116229
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001162285334430635, 0.001162285334430635, 0.001162285334430635, 0.001162285334430635, 0.001162285334430635]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001162285334430635

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25843096
Iteration 2/25 | Loss: 0.00101026
Iteration 3/25 | Loss: 0.00101025
Iteration 4/25 | Loss: 0.00101025
Iteration 5/25 | Loss: 0.00101025
Iteration 6/25 | Loss: 0.00101025
Iteration 7/25 | Loss: 0.00101025
Iteration 8/25 | Loss: 0.00101025
Iteration 9/25 | Loss: 0.00101025
Iteration 10/25 | Loss: 0.00101025
Iteration 11/25 | Loss: 0.00101025
Iteration 12/25 | Loss: 0.00101025
Iteration 13/25 | Loss: 0.00101025
Iteration 14/25 | Loss: 0.00101025
Iteration 15/25 | Loss: 0.00101025
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001010247622616589, 0.001010247622616589, 0.001010247622616589, 0.001010247622616589, 0.001010247622616589]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001010247622616589

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101025
Iteration 2/1000 | Loss: 0.00004115
Iteration 3/1000 | Loss: 0.00002257
Iteration 4/1000 | Loss: 0.00001982
Iteration 5/1000 | Loss: 0.00001868
Iteration 6/1000 | Loss: 0.00001788
Iteration 7/1000 | Loss: 0.00001729
Iteration 8/1000 | Loss: 0.00001697
Iteration 9/1000 | Loss: 0.00001665
Iteration 10/1000 | Loss: 0.00001635
Iteration 11/1000 | Loss: 0.00001623
Iteration 12/1000 | Loss: 0.00001601
Iteration 13/1000 | Loss: 0.00001596
Iteration 14/1000 | Loss: 0.00001579
Iteration 15/1000 | Loss: 0.00001576
Iteration 16/1000 | Loss: 0.00001571
Iteration 17/1000 | Loss: 0.00001570
Iteration 18/1000 | Loss: 0.00001569
Iteration 19/1000 | Loss: 0.00001566
Iteration 20/1000 | Loss: 0.00001565
Iteration 21/1000 | Loss: 0.00001565
Iteration 22/1000 | Loss: 0.00001562
Iteration 23/1000 | Loss: 0.00001562
Iteration 24/1000 | Loss: 0.00001560
Iteration 25/1000 | Loss: 0.00001559
Iteration 26/1000 | Loss: 0.00001558
Iteration 27/1000 | Loss: 0.00001558
Iteration 28/1000 | Loss: 0.00001557
Iteration 29/1000 | Loss: 0.00001557
Iteration 30/1000 | Loss: 0.00001557
Iteration 31/1000 | Loss: 0.00001556
Iteration 32/1000 | Loss: 0.00001556
Iteration 33/1000 | Loss: 0.00001555
Iteration 34/1000 | Loss: 0.00001555
Iteration 35/1000 | Loss: 0.00001554
Iteration 36/1000 | Loss: 0.00001554
Iteration 37/1000 | Loss: 0.00001550
Iteration 38/1000 | Loss: 0.00001550
Iteration 39/1000 | Loss: 0.00001549
Iteration 40/1000 | Loss: 0.00001548
Iteration 41/1000 | Loss: 0.00001548
Iteration 42/1000 | Loss: 0.00001547
Iteration 43/1000 | Loss: 0.00001547
Iteration 44/1000 | Loss: 0.00001546
Iteration 45/1000 | Loss: 0.00001546
Iteration 46/1000 | Loss: 0.00001546
Iteration 47/1000 | Loss: 0.00001545
Iteration 48/1000 | Loss: 0.00001545
Iteration 49/1000 | Loss: 0.00001545
Iteration 50/1000 | Loss: 0.00001545
Iteration 51/1000 | Loss: 0.00001545
Iteration 52/1000 | Loss: 0.00001544
Iteration 53/1000 | Loss: 0.00001544
Iteration 54/1000 | Loss: 0.00001544
Iteration 55/1000 | Loss: 0.00001544
Iteration 56/1000 | Loss: 0.00001543
Iteration 57/1000 | Loss: 0.00001543
Iteration 58/1000 | Loss: 0.00001543
Iteration 59/1000 | Loss: 0.00001543
Iteration 60/1000 | Loss: 0.00001543
Iteration 61/1000 | Loss: 0.00001543
Iteration 62/1000 | Loss: 0.00001543
Iteration 63/1000 | Loss: 0.00001542
Iteration 64/1000 | Loss: 0.00001542
Iteration 65/1000 | Loss: 0.00001542
Iteration 66/1000 | Loss: 0.00001542
Iteration 67/1000 | Loss: 0.00001542
Iteration 68/1000 | Loss: 0.00001542
Iteration 69/1000 | Loss: 0.00001542
Iteration 70/1000 | Loss: 0.00001542
Iteration 71/1000 | Loss: 0.00001542
Iteration 72/1000 | Loss: 0.00001542
Iteration 73/1000 | Loss: 0.00001542
Iteration 74/1000 | Loss: 0.00001542
Iteration 75/1000 | Loss: 0.00001542
Iteration 76/1000 | Loss: 0.00001542
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 76. Stopping optimization.
Last 5 losses: [1.5424673620145768e-05, 1.5424673620145768e-05, 1.5424673620145768e-05, 1.5424673620145768e-05, 1.5424673620145768e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5424673620145768e-05

Optimization complete. Final v2v error: 3.2772724628448486 mm

Highest mean error: 4.185503005981445 mm for frame 143

Lowest mean error: 2.5279769897460938 mm for frame 30

Saving results

Total time: 36.74155139923096
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00967238
Iteration 2/25 | Loss: 0.00210048
Iteration 3/25 | Loss: 0.00151731
Iteration 4/25 | Loss: 0.00147288
Iteration 5/25 | Loss: 0.00142344
Iteration 6/25 | Loss: 0.00130524
Iteration 7/25 | Loss: 0.00125401
Iteration 8/25 | Loss: 0.00122686
Iteration 9/25 | Loss: 0.00120320
Iteration 10/25 | Loss: 0.00119786
Iteration 11/25 | Loss: 0.00119135
Iteration 12/25 | Loss: 0.00118791
Iteration 13/25 | Loss: 0.00118708
Iteration 14/25 | Loss: 0.00118684
Iteration 15/25 | Loss: 0.00118655
Iteration 16/25 | Loss: 0.00119233
Iteration 17/25 | Loss: 0.00118653
Iteration 18/25 | Loss: 0.00118485
Iteration 19/25 | Loss: 0.00118421
Iteration 20/25 | Loss: 0.00118411
Iteration 21/25 | Loss: 0.00118401
Iteration 22/25 | Loss: 0.00118401
Iteration 23/25 | Loss: 0.00118400
Iteration 24/25 | Loss: 0.00118400
Iteration 25/25 | Loss: 0.00118400

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48664618
Iteration 2/25 | Loss: 0.00096283
Iteration 3/25 | Loss: 0.00096283
Iteration 4/25 | Loss: 0.00096283
Iteration 5/25 | Loss: 0.00096283
Iteration 6/25 | Loss: 0.00096283
Iteration 7/25 | Loss: 0.00096282
Iteration 8/25 | Loss: 0.00096282
Iteration 9/25 | Loss: 0.00096282
Iteration 10/25 | Loss: 0.00096282
Iteration 11/25 | Loss: 0.00096282
Iteration 12/25 | Loss: 0.00096282
Iteration 13/25 | Loss: 0.00096282
Iteration 14/25 | Loss: 0.00096282
Iteration 15/25 | Loss: 0.00096282
Iteration 16/25 | Loss: 0.00096282
Iteration 17/25 | Loss: 0.00096282
Iteration 18/25 | Loss: 0.00096282
Iteration 19/25 | Loss: 0.00096282
Iteration 20/25 | Loss: 0.00096282
Iteration 21/25 | Loss: 0.00096282
Iteration 22/25 | Loss: 0.00096282
Iteration 23/25 | Loss: 0.00096282
Iteration 24/25 | Loss: 0.00096282
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0009628233383409679, 0.0009628233383409679, 0.0009628233383409679, 0.0009628233383409679, 0.0009628233383409679]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009628233383409679

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096282
Iteration 2/1000 | Loss: 0.00028888
Iteration 3/1000 | Loss: 0.00026699
Iteration 4/1000 | Loss: 0.00132434
Iteration 5/1000 | Loss: 0.00003754
Iteration 6/1000 | Loss: 0.00020373
Iteration 7/1000 | Loss: 0.00004962
Iteration 8/1000 | Loss: 0.00007167
Iteration 9/1000 | Loss: 0.00001971
Iteration 10/1000 | Loss: 0.00001790
Iteration 11/1000 | Loss: 0.00001713
Iteration 12/1000 | Loss: 0.00001663
Iteration 13/1000 | Loss: 0.00001617
Iteration 14/1000 | Loss: 0.00001580
Iteration 15/1000 | Loss: 0.00001556
Iteration 16/1000 | Loss: 0.00001541
Iteration 17/1000 | Loss: 0.00001526
Iteration 18/1000 | Loss: 0.00001523
Iteration 19/1000 | Loss: 0.00001522
Iteration 20/1000 | Loss: 0.00001521
Iteration 21/1000 | Loss: 0.00001521
Iteration 22/1000 | Loss: 0.00001517
Iteration 23/1000 | Loss: 0.00003934
Iteration 24/1000 | Loss: 0.00001804
Iteration 25/1000 | Loss: 0.00001501
Iteration 26/1000 | Loss: 0.00001497
Iteration 27/1000 | Loss: 0.00001497
Iteration 28/1000 | Loss: 0.00001497
Iteration 29/1000 | Loss: 0.00001497
Iteration 30/1000 | Loss: 0.00001497
Iteration 31/1000 | Loss: 0.00001496
Iteration 32/1000 | Loss: 0.00001496
Iteration 33/1000 | Loss: 0.00001495
Iteration 34/1000 | Loss: 0.00001495
Iteration 35/1000 | Loss: 0.00001495
Iteration 36/1000 | Loss: 0.00001495
Iteration 37/1000 | Loss: 0.00001495
Iteration 38/1000 | Loss: 0.00001495
Iteration 39/1000 | Loss: 0.00001495
Iteration 40/1000 | Loss: 0.00001495
Iteration 41/1000 | Loss: 0.00001494
Iteration 42/1000 | Loss: 0.00001494
Iteration 43/1000 | Loss: 0.00001494
Iteration 44/1000 | Loss: 0.00001494
Iteration 45/1000 | Loss: 0.00001494
Iteration 46/1000 | Loss: 0.00001494
Iteration 47/1000 | Loss: 0.00001494
Iteration 48/1000 | Loss: 0.00001493
Iteration 49/1000 | Loss: 0.00001493
Iteration 50/1000 | Loss: 0.00001493
Iteration 51/1000 | Loss: 0.00001491
Iteration 52/1000 | Loss: 0.00001491
Iteration 53/1000 | Loss: 0.00001490
Iteration 54/1000 | Loss: 0.00001489
Iteration 55/1000 | Loss: 0.00001489
Iteration 56/1000 | Loss: 0.00001488
Iteration 57/1000 | Loss: 0.00001488
Iteration 58/1000 | Loss: 0.00001488
Iteration 59/1000 | Loss: 0.00001487
Iteration 60/1000 | Loss: 0.00001487
Iteration 61/1000 | Loss: 0.00001485
Iteration 62/1000 | Loss: 0.00001485
Iteration 63/1000 | Loss: 0.00001485
Iteration 64/1000 | Loss: 0.00001485
Iteration 65/1000 | Loss: 0.00001485
Iteration 66/1000 | Loss: 0.00001485
Iteration 67/1000 | Loss: 0.00001485
Iteration 68/1000 | Loss: 0.00001485
Iteration 69/1000 | Loss: 0.00001484
Iteration 70/1000 | Loss: 0.00001484
Iteration 71/1000 | Loss: 0.00001484
Iteration 72/1000 | Loss: 0.00001484
Iteration 73/1000 | Loss: 0.00001484
Iteration 74/1000 | Loss: 0.00001484
Iteration 75/1000 | Loss: 0.00001484
Iteration 76/1000 | Loss: 0.00001484
Iteration 77/1000 | Loss: 0.00001483
Iteration 78/1000 | Loss: 0.00001483
Iteration 79/1000 | Loss: 0.00001483
Iteration 80/1000 | Loss: 0.00001483
Iteration 81/1000 | Loss: 0.00001483
Iteration 82/1000 | Loss: 0.00001482
Iteration 83/1000 | Loss: 0.00001482
Iteration 84/1000 | Loss: 0.00001482
Iteration 85/1000 | Loss: 0.00001482
Iteration 86/1000 | Loss: 0.00001482
Iteration 87/1000 | Loss: 0.00001482
Iteration 88/1000 | Loss: 0.00001482
Iteration 89/1000 | Loss: 0.00001482
Iteration 90/1000 | Loss: 0.00001482
Iteration 91/1000 | Loss: 0.00001482
Iteration 92/1000 | Loss: 0.00001482
Iteration 93/1000 | Loss: 0.00001482
Iteration 94/1000 | Loss: 0.00001482
Iteration 95/1000 | Loss: 0.00001482
Iteration 96/1000 | Loss: 0.00001482
Iteration 97/1000 | Loss: 0.00001482
Iteration 98/1000 | Loss: 0.00001482
Iteration 99/1000 | Loss: 0.00001482
Iteration 100/1000 | Loss: 0.00001482
Iteration 101/1000 | Loss: 0.00001482
Iteration 102/1000 | Loss: 0.00001482
Iteration 103/1000 | Loss: 0.00001481
Iteration 104/1000 | Loss: 0.00001481
Iteration 105/1000 | Loss: 0.00001481
Iteration 106/1000 | Loss: 0.00001481
Iteration 107/1000 | Loss: 0.00001481
Iteration 108/1000 | Loss: 0.00001481
Iteration 109/1000 | Loss: 0.00001481
Iteration 110/1000 | Loss: 0.00001481
Iteration 111/1000 | Loss: 0.00001481
Iteration 112/1000 | Loss: 0.00001481
Iteration 113/1000 | Loss: 0.00001481
Iteration 114/1000 | Loss: 0.00001481
Iteration 115/1000 | Loss: 0.00001481
Iteration 116/1000 | Loss: 0.00001481
Iteration 117/1000 | Loss: 0.00001481
Iteration 118/1000 | Loss: 0.00001481
Iteration 119/1000 | Loss: 0.00001481
Iteration 120/1000 | Loss: 0.00001481
Iteration 121/1000 | Loss: 0.00001481
Iteration 122/1000 | Loss: 0.00001481
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.4812645531492308e-05, 1.4812645531492308e-05, 1.4812645531492308e-05, 1.4812645531492308e-05, 1.4812645531492308e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4812645531492308e-05

Optimization complete. Final v2v error: 3.2049636840820312 mm

Highest mean error: 3.8723766803741455 mm for frame 25

Lowest mean error: 2.6909759044647217 mm for frame 5

Saving results

Total time: 69.28395676612854
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01054083
Iteration 2/25 | Loss: 0.00131581
Iteration 3/25 | Loss: 0.00112882
Iteration 4/25 | Loss: 0.00110922
Iteration 5/25 | Loss: 0.00110997
Iteration 6/25 | Loss: 0.00111140
Iteration 7/25 | Loss: 0.00110717
Iteration 8/25 | Loss: 0.00111207
Iteration 9/25 | Loss: 0.00111418
Iteration 10/25 | Loss: 0.00111123
Iteration 11/25 | Loss: 0.00111305
Iteration 12/25 | Loss: 0.00111455
Iteration 13/25 | Loss: 0.00111354
Iteration 14/25 | Loss: 0.00111268
Iteration 15/25 | Loss: 0.00111151
Iteration 16/25 | Loss: 0.00111073
Iteration 17/25 | Loss: 0.00111107
Iteration 18/25 | Loss: 0.00110978
Iteration 19/25 | Loss: 0.00110927
Iteration 20/25 | Loss: 0.00111180
Iteration 21/25 | Loss: 0.00111053
Iteration 22/25 | Loss: 0.00110870
Iteration 23/25 | Loss: 0.00110987
Iteration 24/25 | Loss: 0.00111089
Iteration 25/25 | Loss: 0.00110937

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.24857783
Iteration 2/25 | Loss: 0.00093704
Iteration 3/25 | Loss: 0.00093704
Iteration 4/25 | Loss: 0.00093704
Iteration 5/25 | Loss: 0.00093704
Iteration 6/25 | Loss: 0.00093704
Iteration 7/25 | Loss: 0.00093704
Iteration 8/25 | Loss: 0.00093704
Iteration 9/25 | Loss: 0.00093704
Iteration 10/25 | Loss: 0.00093704
Iteration 11/25 | Loss: 0.00093704
Iteration 12/25 | Loss: 0.00093704
Iteration 13/25 | Loss: 0.00093704
Iteration 14/25 | Loss: 0.00093704
Iteration 15/25 | Loss: 0.00093704
Iteration 16/25 | Loss: 0.00093704
Iteration 17/25 | Loss: 0.00093704
Iteration 18/25 | Loss: 0.00093704
Iteration 19/25 | Loss: 0.00093704
Iteration 20/25 | Loss: 0.00093704
Iteration 21/25 | Loss: 0.00093704
Iteration 22/25 | Loss: 0.00093704
Iteration 23/25 | Loss: 0.00093704
Iteration 24/25 | Loss: 0.00093704
Iteration 25/25 | Loss: 0.00093704

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093704
Iteration 2/1000 | Loss: 0.00013111
Iteration 3/1000 | Loss: 0.00024454
Iteration 4/1000 | Loss: 0.00026583
Iteration 5/1000 | Loss: 0.00021174
Iteration 6/1000 | Loss: 0.00021351
Iteration 7/1000 | Loss: 0.00008435
Iteration 8/1000 | Loss: 0.00017958
Iteration 9/1000 | Loss: 0.00014347
Iteration 10/1000 | Loss: 0.00022320
Iteration 11/1000 | Loss: 0.00023956
Iteration 12/1000 | Loss: 0.00017289
Iteration 13/1000 | Loss: 0.00022810
Iteration 14/1000 | Loss: 0.00013703
Iteration 15/1000 | Loss: 0.00016493
Iteration 16/1000 | Loss: 0.00014101
Iteration 17/1000 | Loss: 0.00018814
Iteration 18/1000 | Loss: 0.00012548
Iteration 19/1000 | Loss: 0.00018996
Iteration 20/1000 | Loss: 0.00022595
Iteration 21/1000 | Loss: 0.00008742
Iteration 22/1000 | Loss: 0.00029437
Iteration 23/1000 | Loss: 0.00014202
Iteration 24/1000 | Loss: 0.00017946
Iteration 25/1000 | Loss: 0.00024111
Iteration 26/1000 | Loss: 0.00026881
Iteration 27/1000 | Loss: 0.00022690
Iteration 28/1000 | Loss: 0.00023713
Iteration 29/1000 | Loss: 0.00027362
Iteration 30/1000 | Loss: 0.00026180
Iteration 31/1000 | Loss: 0.00027402
Iteration 32/1000 | Loss: 0.00023024
Iteration 33/1000 | Loss: 0.00024733
Iteration 34/1000 | Loss: 0.00026791
Iteration 35/1000 | Loss: 0.00025638
Iteration 36/1000 | Loss: 0.00019135
Iteration 37/1000 | Loss: 0.00025091
Iteration 38/1000 | Loss: 0.00024343
Iteration 39/1000 | Loss: 0.00022944
Iteration 40/1000 | Loss: 0.00029152
Iteration 41/1000 | Loss: 0.00023194
Iteration 42/1000 | Loss: 0.00021753
Iteration 43/1000 | Loss: 0.00031331
Iteration 44/1000 | Loss: 0.00008982
Iteration 45/1000 | Loss: 0.00009754
Iteration 46/1000 | Loss: 0.00005397
Iteration 47/1000 | Loss: 0.00006544
Iteration 48/1000 | Loss: 0.00004369
Iteration 49/1000 | Loss: 0.00007851
Iteration 50/1000 | Loss: 0.00002394
Iteration 51/1000 | Loss: 0.00005506
Iteration 52/1000 | Loss: 0.00003235
Iteration 53/1000 | Loss: 0.00012057
Iteration 54/1000 | Loss: 0.00005622
Iteration 55/1000 | Loss: 0.00006754
Iteration 56/1000 | Loss: 0.00003724
Iteration 57/1000 | Loss: 0.00006525
Iteration 58/1000 | Loss: 0.00010402
Iteration 59/1000 | Loss: 0.00009406
Iteration 60/1000 | Loss: 0.00005187
Iteration 61/1000 | Loss: 0.00019355
Iteration 62/1000 | Loss: 0.00011126
Iteration 63/1000 | Loss: 0.00012711
Iteration 64/1000 | Loss: 0.00010463
Iteration 65/1000 | Loss: 0.00005419
Iteration 66/1000 | Loss: 0.00009036
Iteration 67/1000 | Loss: 0.00007253
Iteration 68/1000 | Loss: 0.00013969
Iteration 69/1000 | Loss: 0.00007693
Iteration 70/1000 | Loss: 0.00011393
Iteration 71/1000 | Loss: 0.00006791
Iteration 72/1000 | Loss: 0.00010049
Iteration 73/1000 | Loss: 0.00006546
Iteration 74/1000 | Loss: 0.00003239
Iteration 75/1000 | Loss: 0.00007276
Iteration 76/1000 | Loss: 0.00011559
Iteration 77/1000 | Loss: 0.00009466
Iteration 78/1000 | Loss: 0.00009575
Iteration 79/1000 | Loss: 0.00011017
Iteration 80/1000 | Loss: 0.00009488
Iteration 81/1000 | Loss: 0.00009420
Iteration 82/1000 | Loss: 0.00009638
Iteration 83/1000 | Loss: 0.00006992
Iteration 84/1000 | Loss: 0.00010244
Iteration 85/1000 | Loss: 0.00017709
Iteration 86/1000 | Loss: 0.00009370
Iteration 87/1000 | Loss: 0.00010300
Iteration 88/1000 | Loss: 0.00010044
Iteration 89/1000 | Loss: 0.00007829
Iteration 90/1000 | Loss: 0.00001860
Iteration 91/1000 | Loss: 0.00016096
Iteration 92/1000 | Loss: 0.00001600
Iteration 93/1000 | Loss: 0.00001271
Iteration 94/1000 | Loss: 0.00001306
Iteration 95/1000 | Loss: 0.00001208
Iteration 96/1000 | Loss: 0.00001111
Iteration 97/1000 | Loss: 0.00002160
Iteration 98/1000 | Loss: 0.00002020
Iteration 99/1000 | Loss: 0.00001834
Iteration 100/1000 | Loss: 0.00002387
Iteration 101/1000 | Loss: 0.00001899
Iteration 102/1000 | Loss: 0.00002029
Iteration 103/1000 | Loss: 0.00001970
Iteration 104/1000 | Loss: 0.00002017
Iteration 105/1000 | Loss: 0.00001948
Iteration 106/1000 | Loss: 0.00002328
Iteration 107/1000 | Loss: 0.00002043
Iteration 108/1000 | Loss: 0.00002089
Iteration 109/1000 | Loss: 0.00002628
Iteration 110/1000 | Loss: 0.00002042
Iteration 111/1000 | Loss: 0.00002563
Iteration 112/1000 | Loss: 0.00002244
Iteration 113/1000 | Loss: 0.00001603
Iteration 114/1000 | Loss: 0.00001183
Iteration 115/1000 | Loss: 0.00001090
Iteration 116/1000 | Loss: 0.00001058
Iteration 117/1000 | Loss: 0.00001040
Iteration 118/1000 | Loss: 0.00001034
Iteration 119/1000 | Loss: 0.00001033
Iteration 120/1000 | Loss: 0.00001030
Iteration 121/1000 | Loss: 0.00001029
Iteration 122/1000 | Loss: 0.00001026
Iteration 123/1000 | Loss: 0.00001025
Iteration 124/1000 | Loss: 0.00001022
Iteration 125/1000 | Loss: 0.00001022
Iteration 126/1000 | Loss: 0.00001022
Iteration 127/1000 | Loss: 0.00001022
Iteration 128/1000 | Loss: 0.00001021
Iteration 129/1000 | Loss: 0.00001021
Iteration 130/1000 | Loss: 0.00001021
Iteration 131/1000 | Loss: 0.00001020
Iteration 132/1000 | Loss: 0.00001020
Iteration 133/1000 | Loss: 0.00001017
Iteration 134/1000 | Loss: 0.00001017
Iteration 135/1000 | Loss: 0.00001016
Iteration 136/1000 | Loss: 0.00001016
Iteration 137/1000 | Loss: 0.00001016
Iteration 138/1000 | Loss: 0.00001016
Iteration 139/1000 | Loss: 0.00001015
Iteration 140/1000 | Loss: 0.00001015
Iteration 141/1000 | Loss: 0.00001015
Iteration 142/1000 | Loss: 0.00001015
Iteration 143/1000 | Loss: 0.00001015
Iteration 144/1000 | Loss: 0.00001015
Iteration 145/1000 | Loss: 0.00001015
Iteration 146/1000 | Loss: 0.00001015
Iteration 147/1000 | Loss: 0.00001015
Iteration 148/1000 | Loss: 0.00001014
Iteration 149/1000 | Loss: 0.00001014
Iteration 150/1000 | Loss: 0.00001014
Iteration 151/1000 | Loss: 0.00001014
Iteration 152/1000 | Loss: 0.00001014
Iteration 153/1000 | Loss: 0.00001014
Iteration 154/1000 | Loss: 0.00001013
Iteration 155/1000 | Loss: 0.00001013
Iteration 156/1000 | Loss: 0.00001013
Iteration 157/1000 | Loss: 0.00001013
Iteration 158/1000 | Loss: 0.00001013
Iteration 159/1000 | Loss: 0.00001013
Iteration 160/1000 | Loss: 0.00001013
Iteration 161/1000 | Loss: 0.00001013
Iteration 162/1000 | Loss: 0.00001013
Iteration 163/1000 | Loss: 0.00001012
Iteration 164/1000 | Loss: 0.00001012
Iteration 165/1000 | Loss: 0.00001012
Iteration 166/1000 | Loss: 0.00001012
Iteration 167/1000 | Loss: 0.00001012
Iteration 168/1000 | Loss: 0.00001012
Iteration 169/1000 | Loss: 0.00001012
Iteration 170/1000 | Loss: 0.00001011
Iteration 171/1000 | Loss: 0.00001011
Iteration 172/1000 | Loss: 0.00001011
Iteration 173/1000 | Loss: 0.00001010
Iteration 174/1000 | Loss: 0.00001010
Iteration 175/1000 | Loss: 0.00001010
Iteration 176/1000 | Loss: 0.00001010
Iteration 177/1000 | Loss: 0.00001010
Iteration 178/1000 | Loss: 0.00001010
Iteration 179/1000 | Loss: 0.00001010
Iteration 180/1000 | Loss: 0.00001010
Iteration 181/1000 | Loss: 0.00001010
Iteration 182/1000 | Loss: 0.00001010
Iteration 183/1000 | Loss: 0.00001010
Iteration 184/1000 | Loss: 0.00001009
Iteration 185/1000 | Loss: 0.00001009
Iteration 186/1000 | Loss: 0.00001009
Iteration 187/1000 | Loss: 0.00001009
Iteration 188/1000 | Loss: 0.00001009
Iteration 189/1000 | Loss: 0.00001009
Iteration 190/1000 | Loss: 0.00001009
Iteration 191/1000 | Loss: 0.00001009
Iteration 192/1000 | Loss: 0.00001008
Iteration 193/1000 | Loss: 0.00001008
Iteration 194/1000 | Loss: 0.00001008
Iteration 195/1000 | Loss: 0.00001008
Iteration 196/1000 | Loss: 0.00001007
Iteration 197/1000 | Loss: 0.00001007
Iteration 198/1000 | Loss: 0.00001007
Iteration 199/1000 | Loss: 0.00001007
Iteration 200/1000 | Loss: 0.00001007
Iteration 201/1000 | Loss: 0.00001007
Iteration 202/1000 | Loss: 0.00001007
Iteration 203/1000 | Loss: 0.00001006
Iteration 204/1000 | Loss: 0.00001006
Iteration 205/1000 | Loss: 0.00001006
Iteration 206/1000 | Loss: 0.00001006
Iteration 207/1000 | Loss: 0.00001006
Iteration 208/1000 | Loss: 0.00001006
Iteration 209/1000 | Loss: 0.00001006
Iteration 210/1000 | Loss: 0.00001006
Iteration 211/1000 | Loss: 0.00001006
Iteration 212/1000 | Loss: 0.00001006
Iteration 213/1000 | Loss: 0.00001006
Iteration 214/1000 | Loss: 0.00001006
Iteration 215/1000 | Loss: 0.00001006
Iteration 216/1000 | Loss: 0.00001005
Iteration 217/1000 | Loss: 0.00001005
Iteration 218/1000 | Loss: 0.00001005
Iteration 219/1000 | Loss: 0.00001005
Iteration 220/1000 | Loss: 0.00001005
Iteration 221/1000 | Loss: 0.00001005
Iteration 222/1000 | Loss: 0.00001005
Iteration 223/1000 | Loss: 0.00001005
Iteration 224/1000 | Loss: 0.00001005
Iteration 225/1000 | Loss: 0.00001005
Iteration 226/1000 | Loss: 0.00001005
Iteration 227/1000 | Loss: 0.00001004
Iteration 228/1000 | Loss: 0.00001004
Iteration 229/1000 | Loss: 0.00001004
Iteration 230/1000 | Loss: 0.00001004
Iteration 231/1000 | Loss: 0.00001004
Iteration 232/1000 | Loss: 0.00001004
Iteration 233/1000 | Loss: 0.00001004
Iteration 234/1000 | Loss: 0.00001004
Iteration 235/1000 | Loss: 0.00001004
Iteration 236/1000 | Loss: 0.00001004
Iteration 237/1000 | Loss: 0.00001004
Iteration 238/1000 | Loss: 0.00001003
Iteration 239/1000 | Loss: 0.00001003
Iteration 240/1000 | Loss: 0.00001003
Iteration 241/1000 | Loss: 0.00001003
Iteration 242/1000 | Loss: 0.00001003
Iteration 243/1000 | Loss: 0.00001003
Iteration 244/1000 | Loss: 0.00001003
Iteration 245/1000 | Loss: 0.00001002
Iteration 246/1000 | Loss: 0.00001002
Iteration 247/1000 | Loss: 0.00001002
Iteration 248/1000 | Loss: 0.00001002
Iteration 249/1000 | Loss: 0.00001002
Iteration 250/1000 | Loss: 0.00001002
Iteration 251/1000 | Loss: 0.00001001
Iteration 252/1000 | Loss: 0.00001001
Iteration 253/1000 | Loss: 0.00001001
Iteration 254/1000 | Loss: 0.00001001
Iteration 255/1000 | Loss: 0.00001001
Iteration 256/1000 | Loss: 0.00001000
Iteration 257/1000 | Loss: 0.00001000
Iteration 258/1000 | Loss: 0.00001000
Iteration 259/1000 | Loss: 0.00001000
Iteration 260/1000 | Loss: 0.00001000
Iteration 261/1000 | Loss: 0.00001000
Iteration 262/1000 | Loss: 0.00001000
Iteration 263/1000 | Loss: 0.00001000
Iteration 264/1000 | Loss: 0.00001000
Iteration 265/1000 | Loss: 0.00001000
Iteration 266/1000 | Loss: 0.00000999
Iteration 267/1000 | Loss: 0.00000999
Iteration 268/1000 | Loss: 0.00000999
Iteration 269/1000 | Loss: 0.00000999
Iteration 270/1000 | Loss: 0.00000999
Iteration 271/1000 | Loss: 0.00000999
Iteration 272/1000 | Loss: 0.00000999
Iteration 273/1000 | Loss: 0.00000999
Iteration 274/1000 | Loss: 0.00000998
Iteration 275/1000 | Loss: 0.00000998
Iteration 276/1000 | Loss: 0.00000998
Iteration 277/1000 | Loss: 0.00000998
Iteration 278/1000 | Loss: 0.00000998
Iteration 279/1000 | Loss: 0.00000998
Iteration 280/1000 | Loss: 0.00000998
Iteration 281/1000 | Loss: 0.00000998
Iteration 282/1000 | Loss: 0.00000998
Iteration 283/1000 | Loss: 0.00000998
Iteration 284/1000 | Loss: 0.00000998
Iteration 285/1000 | Loss: 0.00000998
Iteration 286/1000 | Loss: 0.00000998
Iteration 287/1000 | Loss: 0.00000998
Iteration 288/1000 | Loss: 0.00000998
Iteration 289/1000 | Loss: 0.00000998
Iteration 290/1000 | Loss: 0.00000998
Iteration 291/1000 | Loss: 0.00000998
Iteration 292/1000 | Loss: 0.00000998
Iteration 293/1000 | Loss: 0.00000998
Iteration 294/1000 | Loss: 0.00000998
Iteration 295/1000 | Loss: 0.00000998
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 295. Stopping optimization.
Last 5 losses: [9.982099072658457e-06, 9.982099072658457e-06, 9.982099072658457e-06, 9.982099072658457e-06, 9.982099072658457e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.982099072658457e-06

Optimization complete. Final v2v error: 2.6951467990875244 mm

Highest mean error: 3.3835108280181885 mm for frame 53

Lowest mean error: 2.5342209339141846 mm for frame 194

Saving results

Total time: 256.88421082496643
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00781568
Iteration 2/25 | Loss: 0.00139875
Iteration 3/25 | Loss: 0.00120889
Iteration 4/25 | Loss: 0.00117896
Iteration 5/25 | Loss: 0.00116085
Iteration 6/25 | Loss: 0.00117043
Iteration 7/25 | Loss: 0.00115615
Iteration 8/25 | Loss: 0.00114856
Iteration 9/25 | Loss: 0.00114468
Iteration 10/25 | Loss: 0.00114307
Iteration 11/25 | Loss: 0.00114257
Iteration 12/25 | Loss: 0.00114388
Iteration 13/25 | Loss: 0.00114162
Iteration 14/25 | Loss: 0.00114069
Iteration 15/25 | Loss: 0.00114038
Iteration 16/25 | Loss: 0.00114024
Iteration 17/25 | Loss: 0.00114023
Iteration 18/25 | Loss: 0.00114022
Iteration 19/25 | Loss: 0.00114022
Iteration 20/25 | Loss: 0.00114022
Iteration 21/25 | Loss: 0.00114022
Iteration 22/25 | Loss: 0.00114021
Iteration 23/25 | Loss: 0.00114020
Iteration 24/25 | Loss: 0.00114020
Iteration 25/25 | Loss: 0.00114020

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63732386
Iteration 2/25 | Loss: 0.00074526
Iteration 3/25 | Loss: 0.00074525
Iteration 4/25 | Loss: 0.00074525
Iteration 5/25 | Loss: 0.00074525
Iteration 6/25 | Loss: 0.00074525
Iteration 7/25 | Loss: 0.00074525
Iteration 8/25 | Loss: 0.00074525
Iteration 9/25 | Loss: 0.00074525
Iteration 10/25 | Loss: 0.00074525
Iteration 11/25 | Loss: 0.00074525
Iteration 12/25 | Loss: 0.00074525
Iteration 13/25 | Loss: 0.00074525
Iteration 14/25 | Loss: 0.00074525
Iteration 15/25 | Loss: 0.00074525
Iteration 16/25 | Loss: 0.00074525
Iteration 17/25 | Loss: 0.00074525
Iteration 18/25 | Loss: 0.00074525
Iteration 19/25 | Loss: 0.00074525
Iteration 20/25 | Loss: 0.00074525
Iteration 21/25 | Loss: 0.00074525
Iteration 22/25 | Loss: 0.00074525
Iteration 23/25 | Loss: 0.00074525
Iteration 24/25 | Loss: 0.00074525
Iteration 25/25 | Loss: 0.00074525
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007452466525137424, 0.0007452466525137424, 0.0007452466525137424, 0.0007452466525137424, 0.0007452466525137424]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007452466525137424

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074525
Iteration 2/1000 | Loss: 0.00002822
Iteration 3/1000 | Loss: 0.00001859
Iteration 4/1000 | Loss: 0.00001742
Iteration 5/1000 | Loss: 0.00001680
Iteration 6/1000 | Loss: 0.00001637
Iteration 7/1000 | Loss: 0.00001612
Iteration 8/1000 | Loss: 0.00001590
Iteration 9/1000 | Loss: 0.00001580
Iteration 10/1000 | Loss: 0.00001572
Iteration 11/1000 | Loss: 0.00001563
Iteration 12/1000 | Loss: 0.00001560
Iteration 13/1000 | Loss: 0.00001560
Iteration 14/1000 | Loss: 0.00001559
Iteration 15/1000 | Loss: 0.00001554
Iteration 16/1000 | Loss: 0.00001551
Iteration 17/1000 | Loss: 0.00001551
Iteration 18/1000 | Loss: 0.00001551
Iteration 19/1000 | Loss: 0.00001551
Iteration 20/1000 | Loss: 0.00001551
Iteration 21/1000 | Loss: 0.00001545
Iteration 22/1000 | Loss: 0.00001545
Iteration 23/1000 | Loss: 0.00001544
Iteration 24/1000 | Loss: 0.00001544
Iteration 25/1000 | Loss: 0.00001543
Iteration 26/1000 | Loss: 0.00001536
Iteration 27/1000 | Loss: 0.00001535
Iteration 28/1000 | Loss: 0.00001535
Iteration 29/1000 | Loss: 0.00001534
Iteration 30/1000 | Loss: 0.00001533
Iteration 31/1000 | Loss: 0.00001531
Iteration 32/1000 | Loss: 0.00001529
Iteration 33/1000 | Loss: 0.00001528
Iteration 34/1000 | Loss: 0.00001528
Iteration 35/1000 | Loss: 0.00001528
Iteration 36/1000 | Loss: 0.00001528
Iteration 37/1000 | Loss: 0.00001527
Iteration 38/1000 | Loss: 0.00001527
Iteration 39/1000 | Loss: 0.00001525
Iteration 40/1000 | Loss: 0.00001524
Iteration 41/1000 | Loss: 0.00001523
Iteration 42/1000 | Loss: 0.00001523
Iteration 43/1000 | Loss: 0.00001522
Iteration 44/1000 | Loss: 0.00001522
Iteration 45/1000 | Loss: 0.00001522
Iteration 46/1000 | Loss: 0.00001522
Iteration 47/1000 | Loss: 0.00001522
Iteration 48/1000 | Loss: 0.00001522
Iteration 49/1000 | Loss: 0.00001522
Iteration 50/1000 | Loss: 0.00001521
Iteration 51/1000 | Loss: 0.00001521
Iteration 52/1000 | Loss: 0.00001521
Iteration 53/1000 | Loss: 0.00001521
Iteration 54/1000 | Loss: 0.00001521
Iteration 55/1000 | Loss: 0.00001520
Iteration 56/1000 | Loss: 0.00001520
Iteration 57/1000 | Loss: 0.00001520
Iteration 58/1000 | Loss: 0.00001520
Iteration 59/1000 | Loss: 0.00001520
Iteration 60/1000 | Loss: 0.00001519
Iteration 61/1000 | Loss: 0.00001519
Iteration 62/1000 | Loss: 0.00001519
Iteration 63/1000 | Loss: 0.00001519
Iteration 64/1000 | Loss: 0.00001519
Iteration 65/1000 | Loss: 0.00001519
Iteration 66/1000 | Loss: 0.00001519
Iteration 67/1000 | Loss: 0.00001519
Iteration 68/1000 | Loss: 0.00001519
Iteration 69/1000 | Loss: 0.00001519
Iteration 70/1000 | Loss: 0.00001519
Iteration 71/1000 | Loss: 0.00001519
Iteration 72/1000 | Loss: 0.00001518
Iteration 73/1000 | Loss: 0.00001518
Iteration 74/1000 | Loss: 0.00001518
Iteration 75/1000 | Loss: 0.00001518
Iteration 76/1000 | Loss: 0.00001517
Iteration 77/1000 | Loss: 0.00001517
Iteration 78/1000 | Loss: 0.00001517
Iteration 79/1000 | Loss: 0.00001516
Iteration 80/1000 | Loss: 0.00001516
Iteration 81/1000 | Loss: 0.00001516
Iteration 82/1000 | Loss: 0.00001516
Iteration 83/1000 | Loss: 0.00001515
Iteration 84/1000 | Loss: 0.00001515
Iteration 85/1000 | Loss: 0.00001515
Iteration 86/1000 | Loss: 0.00001514
Iteration 87/1000 | Loss: 0.00001514
Iteration 88/1000 | Loss: 0.00001514
Iteration 89/1000 | Loss: 0.00001514
Iteration 90/1000 | Loss: 0.00001514
Iteration 91/1000 | Loss: 0.00001514
Iteration 92/1000 | Loss: 0.00001513
Iteration 93/1000 | Loss: 0.00001513
Iteration 94/1000 | Loss: 0.00001513
Iteration 95/1000 | Loss: 0.00001513
Iteration 96/1000 | Loss: 0.00001512
Iteration 97/1000 | Loss: 0.00001512
Iteration 98/1000 | Loss: 0.00001511
Iteration 99/1000 | Loss: 0.00001511
Iteration 100/1000 | Loss: 0.00001511
Iteration 101/1000 | Loss: 0.00001511
Iteration 102/1000 | Loss: 0.00001510
Iteration 103/1000 | Loss: 0.00001510
Iteration 104/1000 | Loss: 0.00001509
Iteration 105/1000 | Loss: 0.00001509
Iteration 106/1000 | Loss: 0.00001509
Iteration 107/1000 | Loss: 0.00001509
Iteration 108/1000 | Loss: 0.00001509
Iteration 109/1000 | Loss: 0.00001509
Iteration 110/1000 | Loss: 0.00001508
Iteration 111/1000 | Loss: 0.00001508
Iteration 112/1000 | Loss: 0.00001508
Iteration 113/1000 | Loss: 0.00001508
Iteration 114/1000 | Loss: 0.00001508
Iteration 115/1000 | Loss: 0.00001508
Iteration 116/1000 | Loss: 0.00001508
Iteration 117/1000 | Loss: 0.00001508
Iteration 118/1000 | Loss: 0.00001508
Iteration 119/1000 | Loss: 0.00001508
Iteration 120/1000 | Loss: 0.00001508
Iteration 121/1000 | Loss: 0.00001508
Iteration 122/1000 | Loss: 0.00001507
Iteration 123/1000 | Loss: 0.00001507
Iteration 124/1000 | Loss: 0.00001507
Iteration 125/1000 | Loss: 0.00001507
Iteration 126/1000 | Loss: 0.00001507
Iteration 127/1000 | Loss: 0.00001507
Iteration 128/1000 | Loss: 0.00001506
Iteration 129/1000 | Loss: 0.00001506
Iteration 130/1000 | Loss: 0.00001506
Iteration 131/1000 | Loss: 0.00001505
Iteration 132/1000 | Loss: 0.00001505
Iteration 133/1000 | Loss: 0.00001505
Iteration 134/1000 | Loss: 0.00001504
Iteration 135/1000 | Loss: 0.00001504
Iteration 136/1000 | Loss: 0.00001504
Iteration 137/1000 | Loss: 0.00001503
Iteration 138/1000 | Loss: 0.00001503
Iteration 139/1000 | Loss: 0.00001503
Iteration 140/1000 | Loss: 0.00001503
Iteration 141/1000 | Loss: 0.00001503
Iteration 142/1000 | Loss: 0.00001503
Iteration 143/1000 | Loss: 0.00001503
Iteration 144/1000 | Loss: 0.00001503
Iteration 145/1000 | Loss: 0.00001503
Iteration 146/1000 | Loss: 0.00001503
Iteration 147/1000 | Loss: 0.00001503
Iteration 148/1000 | Loss: 0.00001503
Iteration 149/1000 | Loss: 0.00001503
Iteration 150/1000 | Loss: 0.00001503
Iteration 151/1000 | Loss: 0.00001503
Iteration 152/1000 | Loss: 0.00001503
Iteration 153/1000 | Loss: 0.00001503
Iteration 154/1000 | Loss: 0.00001503
Iteration 155/1000 | Loss: 0.00001503
Iteration 156/1000 | Loss: 0.00001503
Iteration 157/1000 | Loss: 0.00001503
Iteration 158/1000 | Loss: 0.00001503
Iteration 159/1000 | Loss: 0.00001503
Iteration 160/1000 | Loss: 0.00001503
Iteration 161/1000 | Loss: 0.00001503
Iteration 162/1000 | Loss: 0.00001503
Iteration 163/1000 | Loss: 0.00001503
Iteration 164/1000 | Loss: 0.00001503
Iteration 165/1000 | Loss: 0.00001503
Iteration 166/1000 | Loss: 0.00001503
Iteration 167/1000 | Loss: 0.00001503
Iteration 168/1000 | Loss: 0.00001503
Iteration 169/1000 | Loss: 0.00001503
Iteration 170/1000 | Loss: 0.00001503
Iteration 171/1000 | Loss: 0.00001503
Iteration 172/1000 | Loss: 0.00001503
Iteration 173/1000 | Loss: 0.00001503
Iteration 174/1000 | Loss: 0.00001503
Iteration 175/1000 | Loss: 0.00001503
Iteration 176/1000 | Loss: 0.00001503
Iteration 177/1000 | Loss: 0.00001503
Iteration 178/1000 | Loss: 0.00001503
Iteration 179/1000 | Loss: 0.00001503
Iteration 180/1000 | Loss: 0.00001503
Iteration 181/1000 | Loss: 0.00001503
Iteration 182/1000 | Loss: 0.00001503
Iteration 183/1000 | Loss: 0.00001503
Iteration 184/1000 | Loss: 0.00001503
Iteration 185/1000 | Loss: 0.00001503
Iteration 186/1000 | Loss: 0.00001503
Iteration 187/1000 | Loss: 0.00001503
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.5030406757432502e-05, 1.5030406757432502e-05, 1.5030406757432502e-05, 1.5030406757432502e-05, 1.5030406757432502e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5030406757432502e-05

Optimization complete. Final v2v error: 3.167773962020874 mm

Highest mean error: 4.140965938568115 mm for frame 118

Lowest mean error: 2.73893666267395 mm for frame 157

Saving results

Total time: 57.67254996299744
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00724130
Iteration 2/25 | Loss: 0.00120906
Iteration 3/25 | Loss: 0.00110748
Iteration 4/25 | Loss: 0.00109849
Iteration 5/25 | Loss: 0.00109644
Iteration 6/25 | Loss: 0.00109644
Iteration 7/25 | Loss: 0.00109644
Iteration 8/25 | Loss: 0.00109644
Iteration 9/25 | Loss: 0.00109644
Iteration 10/25 | Loss: 0.00109644
Iteration 11/25 | Loss: 0.00109644
Iteration 12/25 | Loss: 0.00109644
Iteration 13/25 | Loss: 0.00109644
Iteration 14/25 | Loss: 0.00109644
Iteration 15/25 | Loss: 0.00109644
Iteration 16/25 | Loss: 0.00109644
Iteration 17/25 | Loss: 0.00109644
Iteration 18/25 | Loss: 0.00109644
Iteration 19/25 | Loss: 0.00109644
Iteration 20/25 | Loss: 0.00109644
Iteration 21/25 | Loss: 0.00109644
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0010964443208649755, 0.0010964443208649755, 0.0010964443208649755, 0.0010964443208649755, 0.0010964443208649755]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010964443208649755

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 16.76948738
Iteration 2/25 | Loss: 0.00084203
Iteration 3/25 | Loss: 0.00084198
Iteration 4/25 | Loss: 0.00084197
Iteration 5/25 | Loss: 0.00084197
Iteration 6/25 | Loss: 0.00084197
Iteration 7/25 | Loss: 0.00084197
Iteration 8/25 | Loss: 0.00084197
Iteration 9/25 | Loss: 0.00084197
Iteration 10/25 | Loss: 0.00084197
Iteration 11/25 | Loss: 0.00084197
Iteration 12/25 | Loss: 0.00084197
Iteration 13/25 | Loss: 0.00084197
Iteration 14/25 | Loss: 0.00084197
Iteration 15/25 | Loss: 0.00084197
Iteration 16/25 | Loss: 0.00084197
Iteration 17/25 | Loss: 0.00084197
Iteration 18/25 | Loss: 0.00084197
Iteration 19/25 | Loss: 0.00084197
Iteration 20/25 | Loss: 0.00084197
Iteration 21/25 | Loss: 0.00084197
Iteration 22/25 | Loss: 0.00084197
Iteration 23/25 | Loss: 0.00084197
Iteration 24/25 | Loss: 0.00084197
Iteration 25/25 | Loss: 0.00084197

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084197
Iteration 2/1000 | Loss: 0.00001713
Iteration 3/1000 | Loss: 0.00001266
Iteration 4/1000 | Loss: 0.00001170
Iteration 5/1000 | Loss: 0.00001112
Iteration 6/1000 | Loss: 0.00001075
Iteration 7/1000 | Loss: 0.00001054
Iteration 8/1000 | Loss: 0.00001029
Iteration 9/1000 | Loss: 0.00001006
Iteration 10/1000 | Loss: 0.00001002
Iteration 11/1000 | Loss: 0.00001001
Iteration 12/1000 | Loss: 0.00001001
Iteration 13/1000 | Loss: 0.00000997
Iteration 14/1000 | Loss: 0.00000987
Iteration 15/1000 | Loss: 0.00000981
Iteration 16/1000 | Loss: 0.00000977
Iteration 17/1000 | Loss: 0.00000977
Iteration 18/1000 | Loss: 0.00000977
Iteration 19/1000 | Loss: 0.00000977
Iteration 20/1000 | Loss: 0.00000968
Iteration 21/1000 | Loss: 0.00000965
Iteration 22/1000 | Loss: 0.00000964
Iteration 23/1000 | Loss: 0.00000963
Iteration 24/1000 | Loss: 0.00000963
Iteration 25/1000 | Loss: 0.00000961
Iteration 26/1000 | Loss: 0.00000961
Iteration 27/1000 | Loss: 0.00000961
Iteration 28/1000 | Loss: 0.00000961
Iteration 29/1000 | Loss: 0.00000961
Iteration 30/1000 | Loss: 0.00000961
Iteration 31/1000 | Loss: 0.00000961
Iteration 32/1000 | Loss: 0.00000961
Iteration 33/1000 | Loss: 0.00000960
Iteration 34/1000 | Loss: 0.00000960
Iteration 35/1000 | Loss: 0.00000960
Iteration 36/1000 | Loss: 0.00000960
Iteration 37/1000 | Loss: 0.00000960
Iteration 38/1000 | Loss: 0.00000960
Iteration 39/1000 | Loss: 0.00000960
Iteration 40/1000 | Loss: 0.00000960
Iteration 41/1000 | Loss: 0.00000959
Iteration 42/1000 | Loss: 0.00000957
Iteration 43/1000 | Loss: 0.00000956
Iteration 44/1000 | Loss: 0.00000956
Iteration 45/1000 | Loss: 0.00000956
Iteration 46/1000 | Loss: 0.00000955
Iteration 47/1000 | Loss: 0.00000955
Iteration 48/1000 | Loss: 0.00000954
Iteration 49/1000 | Loss: 0.00000953
Iteration 50/1000 | Loss: 0.00000952
Iteration 51/1000 | Loss: 0.00000952
Iteration 52/1000 | Loss: 0.00000952
Iteration 53/1000 | Loss: 0.00000952
Iteration 54/1000 | Loss: 0.00000951
Iteration 55/1000 | Loss: 0.00000951
Iteration 56/1000 | Loss: 0.00000951
Iteration 57/1000 | Loss: 0.00000950
Iteration 58/1000 | Loss: 0.00000950
Iteration 59/1000 | Loss: 0.00000949
Iteration 60/1000 | Loss: 0.00000947
Iteration 61/1000 | Loss: 0.00000947
Iteration 62/1000 | Loss: 0.00000947
Iteration 63/1000 | Loss: 0.00000946
Iteration 64/1000 | Loss: 0.00000946
Iteration 65/1000 | Loss: 0.00000946
Iteration 66/1000 | Loss: 0.00000945
Iteration 67/1000 | Loss: 0.00000945
Iteration 68/1000 | Loss: 0.00000944
Iteration 69/1000 | Loss: 0.00000944
Iteration 70/1000 | Loss: 0.00000943
Iteration 71/1000 | Loss: 0.00000943
Iteration 72/1000 | Loss: 0.00000942
Iteration 73/1000 | Loss: 0.00000942
Iteration 74/1000 | Loss: 0.00000942
Iteration 75/1000 | Loss: 0.00000941
Iteration 76/1000 | Loss: 0.00000941
Iteration 77/1000 | Loss: 0.00000941
Iteration 78/1000 | Loss: 0.00000941
Iteration 79/1000 | Loss: 0.00000940
Iteration 80/1000 | Loss: 0.00000940
Iteration 81/1000 | Loss: 0.00000940
Iteration 82/1000 | Loss: 0.00000939
Iteration 83/1000 | Loss: 0.00000939
Iteration 84/1000 | Loss: 0.00000939
Iteration 85/1000 | Loss: 0.00000939
Iteration 86/1000 | Loss: 0.00000939
Iteration 87/1000 | Loss: 0.00000938
Iteration 88/1000 | Loss: 0.00000938
Iteration 89/1000 | Loss: 0.00000937
Iteration 90/1000 | Loss: 0.00000937
Iteration 91/1000 | Loss: 0.00000937
Iteration 92/1000 | Loss: 0.00000937
Iteration 93/1000 | Loss: 0.00000937
Iteration 94/1000 | Loss: 0.00000937
Iteration 95/1000 | Loss: 0.00000937
Iteration 96/1000 | Loss: 0.00000937
Iteration 97/1000 | Loss: 0.00000937
Iteration 98/1000 | Loss: 0.00000936
Iteration 99/1000 | Loss: 0.00000936
Iteration 100/1000 | Loss: 0.00000936
Iteration 101/1000 | Loss: 0.00000936
Iteration 102/1000 | Loss: 0.00000935
Iteration 103/1000 | Loss: 0.00000935
Iteration 104/1000 | Loss: 0.00000934
Iteration 105/1000 | Loss: 0.00000934
Iteration 106/1000 | Loss: 0.00000934
Iteration 107/1000 | Loss: 0.00000934
Iteration 108/1000 | Loss: 0.00000933
Iteration 109/1000 | Loss: 0.00000933
Iteration 110/1000 | Loss: 0.00000933
Iteration 111/1000 | Loss: 0.00000933
Iteration 112/1000 | Loss: 0.00000933
Iteration 113/1000 | Loss: 0.00000933
Iteration 114/1000 | Loss: 0.00000933
Iteration 115/1000 | Loss: 0.00000932
Iteration 116/1000 | Loss: 0.00000932
Iteration 117/1000 | Loss: 0.00000932
Iteration 118/1000 | Loss: 0.00000932
Iteration 119/1000 | Loss: 0.00000932
Iteration 120/1000 | Loss: 0.00000932
Iteration 121/1000 | Loss: 0.00000931
Iteration 122/1000 | Loss: 0.00000931
Iteration 123/1000 | Loss: 0.00000931
Iteration 124/1000 | Loss: 0.00000931
Iteration 125/1000 | Loss: 0.00000931
Iteration 126/1000 | Loss: 0.00000931
Iteration 127/1000 | Loss: 0.00000931
Iteration 128/1000 | Loss: 0.00000931
Iteration 129/1000 | Loss: 0.00000930
Iteration 130/1000 | Loss: 0.00000930
Iteration 131/1000 | Loss: 0.00000930
Iteration 132/1000 | Loss: 0.00000930
Iteration 133/1000 | Loss: 0.00000930
Iteration 134/1000 | Loss: 0.00000930
Iteration 135/1000 | Loss: 0.00000930
Iteration 136/1000 | Loss: 0.00000930
Iteration 137/1000 | Loss: 0.00000930
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [9.30494843487395e-06, 9.30494843487395e-06, 9.30494843487395e-06, 9.30494843487395e-06, 9.30494843487395e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.30494843487395e-06

Optimization complete. Final v2v error: 2.609323740005493 mm

Highest mean error: 3.1147565841674805 mm for frame 148

Lowest mean error: 2.353116750717163 mm for frame 58

Saving results

Total time: 38.74067735671997
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00479596
Iteration 2/25 | Loss: 0.00123381
Iteration 3/25 | Loss: 0.00112441
Iteration 4/25 | Loss: 0.00111066
Iteration 5/25 | Loss: 0.00110699
Iteration 6/25 | Loss: 0.00110699
Iteration 7/25 | Loss: 0.00110699
Iteration 8/25 | Loss: 0.00110699
Iteration 9/25 | Loss: 0.00110699
Iteration 10/25 | Loss: 0.00110699
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011069860775023699, 0.0011069860775023699, 0.0011069860775023699, 0.0011069860775023699, 0.0011069860775023699]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011069860775023699

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.40716839
Iteration 2/25 | Loss: 0.00076209
Iteration 3/25 | Loss: 0.00076209
Iteration 4/25 | Loss: 0.00076209
Iteration 5/25 | Loss: 0.00076209
Iteration 6/25 | Loss: 0.00076209
Iteration 7/25 | Loss: 0.00076209
Iteration 8/25 | Loss: 0.00076208
Iteration 9/25 | Loss: 0.00076208
Iteration 10/25 | Loss: 0.00076208
Iteration 11/25 | Loss: 0.00076208
Iteration 12/25 | Loss: 0.00076208
Iteration 13/25 | Loss: 0.00076208
Iteration 14/25 | Loss: 0.00076208
Iteration 15/25 | Loss: 0.00076208
Iteration 16/25 | Loss: 0.00076208
Iteration 17/25 | Loss: 0.00076208
Iteration 18/25 | Loss: 0.00076208
Iteration 19/25 | Loss: 0.00076208
Iteration 20/25 | Loss: 0.00076208
Iteration 21/25 | Loss: 0.00076208
Iteration 22/25 | Loss: 0.00076208
Iteration 23/25 | Loss: 0.00076208
Iteration 24/25 | Loss: 0.00076208
Iteration 25/25 | Loss: 0.00076208

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076208
Iteration 2/1000 | Loss: 0.00001986
Iteration 3/1000 | Loss: 0.00001592
Iteration 4/1000 | Loss: 0.00001475
Iteration 5/1000 | Loss: 0.00001397
Iteration 6/1000 | Loss: 0.00001354
Iteration 7/1000 | Loss: 0.00001321
Iteration 8/1000 | Loss: 0.00001320
Iteration 9/1000 | Loss: 0.00001289
Iteration 10/1000 | Loss: 0.00001276
Iteration 11/1000 | Loss: 0.00001276
Iteration 12/1000 | Loss: 0.00001276
Iteration 13/1000 | Loss: 0.00001261
Iteration 14/1000 | Loss: 0.00001249
Iteration 15/1000 | Loss: 0.00001238
Iteration 16/1000 | Loss: 0.00001235
Iteration 17/1000 | Loss: 0.00001235
Iteration 18/1000 | Loss: 0.00001234
Iteration 19/1000 | Loss: 0.00001233
Iteration 20/1000 | Loss: 0.00001232
Iteration 21/1000 | Loss: 0.00001231
Iteration 22/1000 | Loss: 0.00001230
Iteration 23/1000 | Loss: 0.00001229
Iteration 24/1000 | Loss: 0.00001228
Iteration 25/1000 | Loss: 0.00001227
Iteration 26/1000 | Loss: 0.00001227
Iteration 27/1000 | Loss: 0.00001226
Iteration 28/1000 | Loss: 0.00001217
Iteration 29/1000 | Loss: 0.00001215
Iteration 30/1000 | Loss: 0.00001208
Iteration 31/1000 | Loss: 0.00001202
Iteration 32/1000 | Loss: 0.00001201
Iteration 33/1000 | Loss: 0.00001201
Iteration 34/1000 | Loss: 0.00001201
Iteration 35/1000 | Loss: 0.00001201
Iteration 36/1000 | Loss: 0.00001201
Iteration 37/1000 | Loss: 0.00001200
Iteration 38/1000 | Loss: 0.00001200
Iteration 39/1000 | Loss: 0.00001200
Iteration 40/1000 | Loss: 0.00001200
Iteration 41/1000 | Loss: 0.00001200
Iteration 42/1000 | Loss: 0.00001199
Iteration 43/1000 | Loss: 0.00001199
Iteration 44/1000 | Loss: 0.00001199
Iteration 45/1000 | Loss: 0.00001198
Iteration 46/1000 | Loss: 0.00001198
Iteration 47/1000 | Loss: 0.00001197
Iteration 48/1000 | Loss: 0.00001197
Iteration 49/1000 | Loss: 0.00001197
Iteration 50/1000 | Loss: 0.00001196
Iteration 51/1000 | Loss: 0.00001195
Iteration 52/1000 | Loss: 0.00001195
Iteration 53/1000 | Loss: 0.00001192
Iteration 54/1000 | Loss: 0.00001190
Iteration 55/1000 | Loss: 0.00001190
Iteration 56/1000 | Loss: 0.00001190
Iteration 57/1000 | Loss: 0.00001190
Iteration 58/1000 | Loss: 0.00001189
Iteration 59/1000 | Loss: 0.00001189
Iteration 60/1000 | Loss: 0.00001189
Iteration 61/1000 | Loss: 0.00001187
Iteration 62/1000 | Loss: 0.00001187
Iteration 63/1000 | Loss: 0.00001187
Iteration 64/1000 | Loss: 0.00001187
Iteration 65/1000 | Loss: 0.00001187
Iteration 66/1000 | Loss: 0.00001187
Iteration 67/1000 | Loss: 0.00001187
Iteration 68/1000 | Loss: 0.00001187
Iteration 69/1000 | Loss: 0.00001187
Iteration 70/1000 | Loss: 0.00001186
Iteration 71/1000 | Loss: 0.00001186
Iteration 72/1000 | Loss: 0.00001186
Iteration 73/1000 | Loss: 0.00001185
Iteration 74/1000 | Loss: 0.00001185
Iteration 75/1000 | Loss: 0.00001184
Iteration 76/1000 | Loss: 0.00001184
Iteration 77/1000 | Loss: 0.00001183
Iteration 78/1000 | Loss: 0.00001183
Iteration 79/1000 | Loss: 0.00001183
Iteration 80/1000 | Loss: 0.00001183
Iteration 81/1000 | Loss: 0.00001183
Iteration 82/1000 | Loss: 0.00001182
Iteration 83/1000 | Loss: 0.00001182
Iteration 84/1000 | Loss: 0.00001182
Iteration 85/1000 | Loss: 0.00001182
Iteration 86/1000 | Loss: 0.00001182
Iteration 87/1000 | Loss: 0.00001182
Iteration 88/1000 | Loss: 0.00001182
Iteration 89/1000 | Loss: 0.00001182
Iteration 90/1000 | Loss: 0.00001182
Iteration 91/1000 | Loss: 0.00001182
Iteration 92/1000 | Loss: 0.00001182
Iteration 93/1000 | Loss: 0.00001181
Iteration 94/1000 | Loss: 0.00001181
Iteration 95/1000 | Loss: 0.00001181
Iteration 96/1000 | Loss: 0.00001181
Iteration 97/1000 | Loss: 0.00001181
Iteration 98/1000 | Loss: 0.00001180
Iteration 99/1000 | Loss: 0.00001180
Iteration 100/1000 | Loss: 0.00001180
Iteration 101/1000 | Loss: 0.00001180
Iteration 102/1000 | Loss: 0.00001180
Iteration 103/1000 | Loss: 0.00001180
Iteration 104/1000 | Loss: 0.00001180
Iteration 105/1000 | Loss: 0.00001179
Iteration 106/1000 | Loss: 0.00001179
Iteration 107/1000 | Loss: 0.00001179
Iteration 108/1000 | Loss: 0.00001179
Iteration 109/1000 | Loss: 0.00001178
Iteration 110/1000 | Loss: 0.00001178
Iteration 111/1000 | Loss: 0.00001178
Iteration 112/1000 | Loss: 0.00001178
Iteration 113/1000 | Loss: 0.00001178
Iteration 114/1000 | Loss: 0.00001177
Iteration 115/1000 | Loss: 0.00001177
Iteration 116/1000 | Loss: 0.00001177
Iteration 117/1000 | Loss: 0.00001176
Iteration 118/1000 | Loss: 0.00001176
Iteration 119/1000 | Loss: 0.00001176
Iteration 120/1000 | Loss: 0.00001175
Iteration 121/1000 | Loss: 0.00001175
Iteration 122/1000 | Loss: 0.00001175
Iteration 123/1000 | Loss: 0.00001175
Iteration 124/1000 | Loss: 0.00001175
Iteration 125/1000 | Loss: 0.00001175
Iteration 126/1000 | Loss: 0.00001175
Iteration 127/1000 | Loss: 0.00001174
Iteration 128/1000 | Loss: 0.00001174
Iteration 129/1000 | Loss: 0.00001174
Iteration 130/1000 | Loss: 0.00001174
Iteration 131/1000 | Loss: 0.00001174
Iteration 132/1000 | Loss: 0.00001174
Iteration 133/1000 | Loss: 0.00001174
Iteration 134/1000 | Loss: 0.00001174
Iteration 135/1000 | Loss: 0.00001174
Iteration 136/1000 | Loss: 0.00001174
Iteration 137/1000 | Loss: 0.00001174
Iteration 138/1000 | Loss: 0.00001173
Iteration 139/1000 | Loss: 0.00001173
Iteration 140/1000 | Loss: 0.00001173
Iteration 141/1000 | Loss: 0.00001173
Iteration 142/1000 | Loss: 0.00001173
Iteration 143/1000 | Loss: 0.00001173
Iteration 144/1000 | Loss: 0.00001173
Iteration 145/1000 | Loss: 0.00001173
Iteration 146/1000 | Loss: 0.00001173
Iteration 147/1000 | Loss: 0.00001173
Iteration 148/1000 | Loss: 0.00001172
Iteration 149/1000 | Loss: 0.00001172
Iteration 150/1000 | Loss: 0.00001172
Iteration 151/1000 | Loss: 0.00001172
Iteration 152/1000 | Loss: 0.00001172
Iteration 153/1000 | Loss: 0.00001172
Iteration 154/1000 | Loss: 0.00001172
Iteration 155/1000 | Loss: 0.00001172
Iteration 156/1000 | Loss: 0.00001172
Iteration 157/1000 | Loss: 0.00001172
Iteration 158/1000 | Loss: 0.00001172
Iteration 159/1000 | Loss: 0.00001172
Iteration 160/1000 | Loss: 0.00001172
Iteration 161/1000 | Loss: 0.00001172
Iteration 162/1000 | Loss: 0.00001172
Iteration 163/1000 | Loss: 0.00001172
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.1718152563844342e-05, 1.1718152563844342e-05, 1.1718152563844342e-05, 1.1718152563844342e-05, 1.1718152563844342e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1718152563844342e-05

Optimization complete. Final v2v error: 2.9346234798431396 mm

Highest mean error: 3.196173667907715 mm for frame 220

Lowest mean error: 2.767240524291992 mm for frame 169

Saving results

Total time: 44.08525013923645
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00803873
Iteration 2/25 | Loss: 0.00134039
Iteration 3/25 | Loss: 0.00113293
Iteration 4/25 | Loss: 0.00110913
Iteration 5/25 | Loss: 0.00110414
Iteration 6/25 | Loss: 0.00110295
Iteration 7/25 | Loss: 0.00110295
Iteration 8/25 | Loss: 0.00110295
Iteration 9/25 | Loss: 0.00110295
Iteration 10/25 | Loss: 0.00110295
Iteration 11/25 | Loss: 0.00110295
Iteration 12/25 | Loss: 0.00110295
Iteration 13/25 | Loss: 0.00110295
Iteration 14/25 | Loss: 0.00110295
Iteration 15/25 | Loss: 0.00110295
Iteration 16/25 | Loss: 0.00110295
Iteration 17/25 | Loss: 0.00110295
Iteration 18/25 | Loss: 0.00110295
Iteration 19/25 | Loss: 0.00110295
Iteration 20/25 | Loss: 0.00110295
Iteration 21/25 | Loss: 0.00110295
Iteration 22/25 | Loss: 0.00110295
Iteration 23/25 | Loss: 0.00110295
Iteration 24/25 | Loss: 0.00110295
Iteration 25/25 | Loss: 0.00110295

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26521850
Iteration 2/25 | Loss: 0.00065501
Iteration 3/25 | Loss: 0.00065498
Iteration 4/25 | Loss: 0.00065498
Iteration 5/25 | Loss: 0.00065498
Iteration 6/25 | Loss: 0.00065497
Iteration 7/25 | Loss: 0.00065497
Iteration 8/25 | Loss: 0.00065497
Iteration 9/25 | Loss: 0.00065497
Iteration 10/25 | Loss: 0.00065497
Iteration 11/25 | Loss: 0.00065497
Iteration 12/25 | Loss: 0.00065497
Iteration 13/25 | Loss: 0.00065497
Iteration 14/25 | Loss: 0.00065497
Iteration 15/25 | Loss: 0.00065497
Iteration 16/25 | Loss: 0.00065497
Iteration 17/25 | Loss: 0.00065497
Iteration 18/25 | Loss: 0.00065497
Iteration 19/25 | Loss: 0.00065497
Iteration 20/25 | Loss: 0.00065497
Iteration 21/25 | Loss: 0.00065497
Iteration 22/25 | Loss: 0.00065497
Iteration 23/25 | Loss: 0.00065497
Iteration 24/25 | Loss: 0.00065497
Iteration 25/25 | Loss: 0.00065497

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065497
Iteration 2/1000 | Loss: 0.00003634
Iteration 3/1000 | Loss: 0.00002401
Iteration 4/1000 | Loss: 0.00001887
Iteration 5/1000 | Loss: 0.00001740
Iteration 6/1000 | Loss: 0.00001635
Iteration 7/1000 | Loss: 0.00001562
Iteration 8/1000 | Loss: 0.00001507
Iteration 9/1000 | Loss: 0.00001471
Iteration 10/1000 | Loss: 0.00001438
Iteration 11/1000 | Loss: 0.00001417
Iteration 12/1000 | Loss: 0.00001413
Iteration 13/1000 | Loss: 0.00001387
Iteration 14/1000 | Loss: 0.00001383
Iteration 15/1000 | Loss: 0.00001378
Iteration 16/1000 | Loss: 0.00001363
Iteration 17/1000 | Loss: 0.00001358
Iteration 18/1000 | Loss: 0.00001346
Iteration 19/1000 | Loss: 0.00001343
Iteration 20/1000 | Loss: 0.00001342
Iteration 21/1000 | Loss: 0.00001340
Iteration 22/1000 | Loss: 0.00001338
Iteration 23/1000 | Loss: 0.00001337
Iteration 24/1000 | Loss: 0.00001332
Iteration 25/1000 | Loss: 0.00001331
Iteration 26/1000 | Loss: 0.00001327
Iteration 27/1000 | Loss: 0.00001325
Iteration 28/1000 | Loss: 0.00001324
Iteration 29/1000 | Loss: 0.00001324
Iteration 30/1000 | Loss: 0.00001323
Iteration 31/1000 | Loss: 0.00001323
Iteration 32/1000 | Loss: 0.00001323
Iteration 33/1000 | Loss: 0.00001323
Iteration 34/1000 | Loss: 0.00001323
Iteration 35/1000 | Loss: 0.00001323
Iteration 36/1000 | Loss: 0.00001323
Iteration 37/1000 | Loss: 0.00001322
Iteration 38/1000 | Loss: 0.00001322
Iteration 39/1000 | Loss: 0.00001322
Iteration 40/1000 | Loss: 0.00001321
Iteration 41/1000 | Loss: 0.00001321
Iteration 42/1000 | Loss: 0.00001321
Iteration 43/1000 | Loss: 0.00001321
Iteration 44/1000 | Loss: 0.00001321
Iteration 45/1000 | Loss: 0.00001321
Iteration 46/1000 | Loss: 0.00001321
Iteration 47/1000 | Loss: 0.00001321
Iteration 48/1000 | Loss: 0.00001321
Iteration 49/1000 | Loss: 0.00001320
Iteration 50/1000 | Loss: 0.00001320
Iteration 51/1000 | Loss: 0.00001319
Iteration 52/1000 | Loss: 0.00001319
Iteration 53/1000 | Loss: 0.00001319
Iteration 54/1000 | Loss: 0.00001318
Iteration 55/1000 | Loss: 0.00001318
Iteration 56/1000 | Loss: 0.00001318
Iteration 57/1000 | Loss: 0.00001318
Iteration 58/1000 | Loss: 0.00001318
Iteration 59/1000 | Loss: 0.00001317
Iteration 60/1000 | Loss: 0.00001317
Iteration 61/1000 | Loss: 0.00001317
Iteration 62/1000 | Loss: 0.00001317
Iteration 63/1000 | Loss: 0.00001316
Iteration 64/1000 | Loss: 0.00001316
Iteration 65/1000 | Loss: 0.00001316
Iteration 66/1000 | Loss: 0.00001315
Iteration 67/1000 | Loss: 0.00001315
Iteration 68/1000 | Loss: 0.00001315
Iteration 69/1000 | Loss: 0.00001315
Iteration 70/1000 | Loss: 0.00001314
Iteration 71/1000 | Loss: 0.00001314
Iteration 72/1000 | Loss: 0.00001314
Iteration 73/1000 | Loss: 0.00001314
Iteration 74/1000 | Loss: 0.00001313
Iteration 75/1000 | Loss: 0.00001313
Iteration 76/1000 | Loss: 0.00001313
Iteration 77/1000 | Loss: 0.00001313
Iteration 78/1000 | Loss: 0.00001313
Iteration 79/1000 | Loss: 0.00001313
Iteration 80/1000 | Loss: 0.00001313
Iteration 81/1000 | Loss: 0.00001313
Iteration 82/1000 | Loss: 0.00001313
Iteration 83/1000 | Loss: 0.00001312
Iteration 84/1000 | Loss: 0.00001312
Iteration 85/1000 | Loss: 0.00001312
Iteration 86/1000 | Loss: 0.00001312
Iteration 87/1000 | Loss: 0.00001311
Iteration 88/1000 | Loss: 0.00001311
Iteration 89/1000 | Loss: 0.00001311
Iteration 90/1000 | Loss: 0.00001311
Iteration 91/1000 | Loss: 0.00001311
Iteration 92/1000 | Loss: 0.00001311
Iteration 93/1000 | Loss: 0.00001311
Iteration 94/1000 | Loss: 0.00001311
Iteration 95/1000 | Loss: 0.00001311
Iteration 96/1000 | Loss: 0.00001311
Iteration 97/1000 | Loss: 0.00001311
Iteration 98/1000 | Loss: 0.00001311
Iteration 99/1000 | Loss: 0.00001311
Iteration 100/1000 | Loss: 0.00001311
Iteration 101/1000 | Loss: 0.00001310
Iteration 102/1000 | Loss: 0.00001310
Iteration 103/1000 | Loss: 0.00001310
Iteration 104/1000 | Loss: 0.00001310
Iteration 105/1000 | Loss: 0.00001310
Iteration 106/1000 | Loss: 0.00001310
Iteration 107/1000 | Loss: 0.00001310
Iteration 108/1000 | Loss: 0.00001310
Iteration 109/1000 | Loss: 0.00001310
Iteration 110/1000 | Loss: 0.00001310
Iteration 111/1000 | Loss: 0.00001310
Iteration 112/1000 | Loss: 0.00001310
Iteration 113/1000 | Loss: 0.00001309
Iteration 114/1000 | Loss: 0.00001309
Iteration 115/1000 | Loss: 0.00001309
Iteration 116/1000 | Loss: 0.00001309
Iteration 117/1000 | Loss: 0.00001309
Iteration 118/1000 | Loss: 0.00001309
Iteration 119/1000 | Loss: 0.00001309
Iteration 120/1000 | Loss: 0.00001309
Iteration 121/1000 | Loss: 0.00001309
Iteration 122/1000 | Loss: 0.00001309
Iteration 123/1000 | Loss: 0.00001309
Iteration 124/1000 | Loss: 0.00001309
Iteration 125/1000 | Loss: 0.00001308
Iteration 126/1000 | Loss: 0.00001308
Iteration 127/1000 | Loss: 0.00001308
Iteration 128/1000 | Loss: 0.00001308
Iteration 129/1000 | Loss: 0.00001307
Iteration 130/1000 | Loss: 0.00001307
Iteration 131/1000 | Loss: 0.00001307
Iteration 132/1000 | Loss: 0.00001307
Iteration 133/1000 | Loss: 0.00001306
Iteration 134/1000 | Loss: 0.00001306
Iteration 135/1000 | Loss: 0.00001306
Iteration 136/1000 | Loss: 0.00001306
Iteration 137/1000 | Loss: 0.00001306
Iteration 138/1000 | Loss: 0.00001306
Iteration 139/1000 | Loss: 0.00001306
Iteration 140/1000 | Loss: 0.00001306
Iteration 141/1000 | Loss: 0.00001306
Iteration 142/1000 | Loss: 0.00001306
Iteration 143/1000 | Loss: 0.00001306
Iteration 144/1000 | Loss: 0.00001306
Iteration 145/1000 | Loss: 0.00001306
Iteration 146/1000 | Loss: 0.00001305
Iteration 147/1000 | Loss: 0.00001305
Iteration 148/1000 | Loss: 0.00001305
Iteration 149/1000 | Loss: 0.00001305
Iteration 150/1000 | Loss: 0.00001305
Iteration 151/1000 | Loss: 0.00001305
Iteration 152/1000 | Loss: 0.00001305
Iteration 153/1000 | Loss: 0.00001305
Iteration 154/1000 | Loss: 0.00001305
Iteration 155/1000 | Loss: 0.00001305
Iteration 156/1000 | Loss: 0.00001305
Iteration 157/1000 | Loss: 0.00001305
Iteration 158/1000 | Loss: 0.00001305
Iteration 159/1000 | Loss: 0.00001305
Iteration 160/1000 | Loss: 0.00001305
Iteration 161/1000 | Loss: 0.00001305
Iteration 162/1000 | Loss: 0.00001305
Iteration 163/1000 | Loss: 0.00001305
Iteration 164/1000 | Loss: 0.00001305
Iteration 165/1000 | Loss: 0.00001305
Iteration 166/1000 | Loss: 0.00001305
Iteration 167/1000 | Loss: 0.00001305
Iteration 168/1000 | Loss: 0.00001305
Iteration 169/1000 | Loss: 0.00001305
Iteration 170/1000 | Loss: 0.00001305
Iteration 171/1000 | Loss: 0.00001305
Iteration 172/1000 | Loss: 0.00001305
Iteration 173/1000 | Loss: 0.00001305
Iteration 174/1000 | Loss: 0.00001305
Iteration 175/1000 | Loss: 0.00001305
Iteration 176/1000 | Loss: 0.00001305
Iteration 177/1000 | Loss: 0.00001305
Iteration 178/1000 | Loss: 0.00001305
Iteration 179/1000 | Loss: 0.00001305
Iteration 180/1000 | Loss: 0.00001305
Iteration 181/1000 | Loss: 0.00001305
Iteration 182/1000 | Loss: 0.00001305
Iteration 183/1000 | Loss: 0.00001305
Iteration 184/1000 | Loss: 0.00001305
Iteration 185/1000 | Loss: 0.00001305
Iteration 186/1000 | Loss: 0.00001305
Iteration 187/1000 | Loss: 0.00001305
Iteration 188/1000 | Loss: 0.00001305
Iteration 189/1000 | Loss: 0.00001305
Iteration 190/1000 | Loss: 0.00001305
Iteration 191/1000 | Loss: 0.00001305
Iteration 192/1000 | Loss: 0.00001305
Iteration 193/1000 | Loss: 0.00001305
Iteration 194/1000 | Loss: 0.00001305
Iteration 195/1000 | Loss: 0.00001305
Iteration 196/1000 | Loss: 0.00001305
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [1.305239766224986e-05, 1.305239766224986e-05, 1.305239766224986e-05, 1.305239766224986e-05, 1.305239766224986e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.305239766224986e-05

Optimization complete. Final v2v error: 3.0450820922851562 mm

Highest mean error: 3.5395023822784424 mm for frame 92

Lowest mean error: 2.711233139038086 mm for frame 171

Saving results

Total time: 42.68605279922485
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01010500
Iteration 2/25 | Loss: 0.00210521
Iteration 3/25 | Loss: 0.00194856
Iteration 4/25 | Loss: 0.00155217
Iteration 5/25 | Loss: 0.00142180
Iteration 6/25 | Loss: 0.00135272
Iteration 7/25 | Loss: 0.00151948
Iteration 8/25 | Loss: 0.00126407
Iteration 9/25 | Loss: 0.00122439
Iteration 10/25 | Loss: 0.00120805
Iteration 11/25 | Loss: 0.00120516
Iteration 12/25 | Loss: 0.00121423
Iteration 13/25 | Loss: 0.00120117
Iteration 14/25 | Loss: 0.00119435
Iteration 15/25 | Loss: 0.00119028
Iteration 16/25 | Loss: 0.00118465
Iteration 17/25 | Loss: 0.00118591
Iteration 18/25 | Loss: 0.00118699
Iteration 19/25 | Loss: 0.00118859
Iteration 20/25 | Loss: 0.00118465
Iteration 21/25 | Loss: 0.00118827
Iteration 22/25 | Loss: 0.00118482
Iteration 23/25 | Loss: 0.00118706
Iteration 24/25 | Loss: 0.00118772
Iteration 25/25 | Loss: 0.00118969

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38320994
Iteration 2/25 | Loss: 0.00117844
Iteration 3/25 | Loss: 0.00117844
Iteration 4/25 | Loss: 0.00117844
Iteration 5/25 | Loss: 0.00117844
Iteration 6/25 | Loss: 0.00117844
Iteration 7/25 | Loss: 0.00117844
Iteration 8/25 | Loss: 0.00117844
Iteration 9/25 | Loss: 0.00117844
Iteration 10/25 | Loss: 0.00117844
Iteration 11/25 | Loss: 0.00117844
Iteration 12/25 | Loss: 0.00117844
Iteration 13/25 | Loss: 0.00117844
Iteration 14/25 | Loss: 0.00117844
Iteration 15/25 | Loss: 0.00117844
Iteration 16/25 | Loss: 0.00117844
Iteration 17/25 | Loss: 0.00117844
Iteration 18/25 | Loss: 0.00117844
Iteration 19/25 | Loss: 0.00117844
Iteration 20/25 | Loss: 0.00117844
Iteration 21/25 | Loss: 0.00117844
Iteration 22/25 | Loss: 0.00117844
Iteration 23/25 | Loss: 0.00117844
Iteration 24/25 | Loss: 0.00117844
Iteration 25/25 | Loss: 0.00117844

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117844
Iteration 2/1000 | Loss: 0.00011363
Iteration 3/1000 | Loss: 0.00017831
Iteration 4/1000 | Loss: 0.00025204
Iteration 5/1000 | Loss: 0.00015108
Iteration 6/1000 | Loss: 0.00017862
Iteration 7/1000 | Loss: 0.00013844
Iteration 8/1000 | Loss: 0.00017837
Iteration 9/1000 | Loss: 0.00026470
Iteration 10/1000 | Loss: 0.00007957
Iteration 11/1000 | Loss: 0.00006523
Iteration 12/1000 | Loss: 0.00004378
Iteration 13/1000 | Loss: 0.00007598
Iteration 14/1000 | Loss: 0.00006761
Iteration 15/1000 | Loss: 0.00021687
Iteration 16/1000 | Loss: 0.00007499
Iteration 17/1000 | Loss: 0.00025885
Iteration 18/1000 | Loss: 0.00019871
Iteration 19/1000 | Loss: 0.00008932
Iteration 20/1000 | Loss: 0.00009727
Iteration 21/1000 | Loss: 0.00010492
Iteration 22/1000 | Loss: 0.00011759
Iteration 23/1000 | Loss: 0.00008603
Iteration 24/1000 | Loss: 0.00007543
Iteration 25/1000 | Loss: 0.00006818
Iteration 26/1000 | Loss: 0.00007516
Iteration 27/1000 | Loss: 0.00006525
Iteration 28/1000 | Loss: 0.00006469
Iteration 29/1000 | Loss: 0.00008260
Iteration 30/1000 | Loss: 0.00015581
Iteration 31/1000 | Loss: 0.00011128
Iteration 32/1000 | Loss: 0.00008899
Iteration 33/1000 | Loss: 0.00009509
Iteration 34/1000 | Loss: 0.00011221
Iteration 35/1000 | Loss: 0.00009758
Iteration 36/1000 | Loss: 0.00007939
Iteration 37/1000 | Loss: 0.00010097
Iteration 38/1000 | Loss: 0.00007778
Iteration 39/1000 | Loss: 0.00009653
Iteration 40/1000 | Loss: 0.00008490
Iteration 41/1000 | Loss: 0.00008777
Iteration 42/1000 | Loss: 0.00008414
Iteration 43/1000 | Loss: 0.00008065
Iteration 44/1000 | Loss: 0.00011486
Iteration 45/1000 | Loss: 0.00006474
Iteration 46/1000 | Loss: 0.00008632
Iteration 47/1000 | Loss: 0.00006660
Iteration 48/1000 | Loss: 0.00011883
Iteration 49/1000 | Loss: 0.00005687
Iteration 50/1000 | Loss: 0.00010344
Iteration 51/1000 | Loss: 0.00009643
Iteration 52/1000 | Loss: 0.00011923
Iteration 53/1000 | Loss: 0.00008223
Iteration 54/1000 | Loss: 0.00009611
Iteration 55/1000 | Loss: 0.00007542
Iteration 56/1000 | Loss: 0.00011473
Iteration 57/1000 | Loss: 0.00009237
Iteration 58/1000 | Loss: 0.00009907
Iteration 59/1000 | Loss: 0.00008740
Iteration 60/1000 | Loss: 0.00010636
Iteration 61/1000 | Loss: 0.00009499
Iteration 62/1000 | Loss: 0.00011017
Iteration 63/1000 | Loss: 0.00009823
Iteration 64/1000 | Loss: 0.00004106
Iteration 65/1000 | Loss: 0.00003120
Iteration 66/1000 | Loss: 0.00002784
Iteration 67/1000 | Loss: 0.00006043
Iteration 68/1000 | Loss: 0.00006192
Iteration 69/1000 | Loss: 0.00003108
Iteration 70/1000 | Loss: 0.00005417
Iteration 71/1000 | Loss: 0.00005373
Iteration 72/1000 | Loss: 0.00002273
Iteration 73/1000 | Loss: 0.00002098
Iteration 74/1000 | Loss: 0.00001935
Iteration 75/1000 | Loss: 0.00001848
Iteration 76/1000 | Loss: 0.00001766
Iteration 77/1000 | Loss: 0.00001718
Iteration 78/1000 | Loss: 0.00001686
Iteration 79/1000 | Loss: 0.00001659
Iteration 80/1000 | Loss: 0.00001638
Iteration 81/1000 | Loss: 0.00001634
Iteration 82/1000 | Loss: 0.00001631
Iteration 83/1000 | Loss: 0.00001617
Iteration 84/1000 | Loss: 0.00001614
Iteration 85/1000 | Loss: 0.00001598
Iteration 86/1000 | Loss: 0.00001589
Iteration 87/1000 | Loss: 0.00001586
Iteration 88/1000 | Loss: 0.00001584
Iteration 89/1000 | Loss: 0.00001584
Iteration 90/1000 | Loss: 0.00001582
Iteration 91/1000 | Loss: 0.00001581
Iteration 92/1000 | Loss: 0.00001581
Iteration 93/1000 | Loss: 0.00001581
Iteration 94/1000 | Loss: 0.00001580
Iteration 95/1000 | Loss: 0.00001580
Iteration 96/1000 | Loss: 0.00001580
Iteration 97/1000 | Loss: 0.00001580
Iteration 98/1000 | Loss: 0.00001580
Iteration 99/1000 | Loss: 0.00001579
Iteration 100/1000 | Loss: 0.00001579
Iteration 101/1000 | Loss: 0.00001579
Iteration 102/1000 | Loss: 0.00001579
Iteration 103/1000 | Loss: 0.00001579
Iteration 104/1000 | Loss: 0.00001579
Iteration 105/1000 | Loss: 0.00001579
Iteration 106/1000 | Loss: 0.00001579
Iteration 107/1000 | Loss: 0.00001579
Iteration 108/1000 | Loss: 0.00001579
Iteration 109/1000 | Loss: 0.00001578
Iteration 110/1000 | Loss: 0.00001578
Iteration 111/1000 | Loss: 0.00001577
Iteration 112/1000 | Loss: 0.00001577
Iteration 113/1000 | Loss: 0.00001577
Iteration 114/1000 | Loss: 0.00001576
Iteration 115/1000 | Loss: 0.00001576
Iteration 116/1000 | Loss: 0.00001575
Iteration 117/1000 | Loss: 0.00001575
Iteration 118/1000 | Loss: 0.00001575
Iteration 119/1000 | Loss: 0.00001574
Iteration 120/1000 | Loss: 0.00001574
Iteration 121/1000 | Loss: 0.00001574
Iteration 122/1000 | Loss: 0.00001574
Iteration 123/1000 | Loss: 0.00001574
Iteration 124/1000 | Loss: 0.00001574
Iteration 125/1000 | Loss: 0.00001574
Iteration 126/1000 | Loss: 0.00001573
Iteration 127/1000 | Loss: 0.00001573
Iteration 128/1000 | Loss: 0.00001573
Iteration 129/1000 | Loss: 0.00001573
Iteration 130/1000 | Loss: 0.00001572
Iteration 131/1000 | Loss: 0.00001572
Iteration 132/1000 | Loss: 0.00001572
Iteration 133/1000 | Loss: 0.00001572
Iteration 134/1000 | Loss: 0.00001572
Iteration 135/1000 | Loss: 0.00001572
Iteration 136/1000 | Loss: 0.00001572
Iteration 137/1000 | Loss: 0.00001571
Iteration 138/1000 | Loss: 0.00001571
Iteration 139/1000 | Loss: 0.00001571
Iteration 140/1000 | Loss: 0.00001571
Iteration 141/1000 | Loss: 0.00001571
Iteration 142/1000 | Loss: 0.00001571
Iteration 143/1000 | Loss: 0.00001570
Iteration 144/1000 | Loss: 0.00001570
Iteration 145/1000 | Loss: 0.00001570
Iteration 146/1000 | Loss: 0.00001569
Iteration 147/1000 | Loss: 0.00001569
Iteration 148/1000 | Loss: 0.00001569
Iteration 149/1000 | Loss: 0.00001569
Iteration 150/1000 | Loss: 0.00001569
Iteration 151/1000 | Loss: 0.00001568
Iteration 152/1000 | Loss: 0.00001568
Iteration 153/1000 | Loss: 0.00001568
Iteration 154/1000 | Loss: 0.00001568
Iteration 155/1000 | Loss: 0.00001568
Iteration 156/1000 | Loss: 0.00001568
Iteration 157/1000 | Loss: 0.00001568
Iteration 158/1000 | Loss: 0.00001567
Iteration 159/1000 | Loss: 0.00001567
Iteration 160/1000 | Loss: 0.00001567
Iteration 161/1000 | Loss: 0.00001567
Iteration 162/1000 | Loss: 0.00001567
Iteration 163/1000 | Loss: 0.00001567
Iteration 164/1000 | Loss: 0.00001567
Iteration 165/1000 | Loss: 0.00001567
Iteration 166/1000 | Loss: 0.00001567
Iteration 167/1000 | Loss: 0.00001566
Iteration 168/1000 | Loss: 0.00001566
Iteration 169/1000 | Loss: 0.00001566
Iteration 170/1000 | Loss: 0.00001566
Iteration 171/1000 | Loss: 0.00001566
Iteration 172/1000 | Loss: 0.00001566
Iteration 173/1000 | Loss: 0.00001566
Iteration 174/1000 | Loss: 0.00001566
Iteration 175/1000 | Loss: 0.00001565
Iteration 176/1000 | Loss: 0.00001565
Iteration 177/1000 | Loss: 0.00001565
Iteration 178/1000 | Loss: 0.00001565
Iteration 179/1000 | Loss: 0.00001565
Iteration 180/1000 | Loss: 0.00001565
Iteration 181/1000 | Loss: 0.00001565
Iteration 182/1000 | Loss: 0.00001565
Iteration 183/1000 | Loss: 0.00001565
Iteration 184/1000 | Loss: 0.00001565
Iteration 185/1000 | Loss: 0.00001565
Iteration 186/1000 | Loss: 0.00001565
Iteration 187/1000 | Loss: 0.00001565
Iteration 188/1000 | Loss: 0.00001565
Iteration 189/1000 | Loss: 0.00001565
Iteration 190/1000 | Loss: 0.00001565
Iteration 191/1000 | Loss: 0.00001565
Iteration 192/1000 | Loss: 0.00001565
Iteration 193/1000 | Loss: 0.00001565
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [1.5651014109607786e-05, 1.5651014109607786e-05, 1.5651014109607786e-05, 1.5651014109607786e-05, 1.5651014109607786e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5651014109607786e-05

Optimization complete. Final v2v error: 3.3250997066497803 mm

Highest mean error: 4.177591323852539 mm for frame 66

Lowest mean error: 2.6867406368255615 mm for frame 42

Saving results

Total time: 169.4369969367981
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00552073
Iteration 2/25 | Loss: 0.00160225
Iteration 3/25 | Loss: 0.00128091
Iteration 4/25 | Loss: 0.00125872
Iteration 5/25 | Loss: 0.00125591
Iteration 6/25 | Loss: 0.00125534
Iteration 7/25 | Loss: 0.00125534
Iteration 8/25 | Loss: 0.00125534
Iteration 9/25 | Loss: 0.00125534
Iteration 10/25 | Loss: 0.00125534
Iteration 11/25 | Loss: 0.00125534
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012553429696708918, 0.0012553429696708918, 0.0012553429696708918, 0.0012553429696708918, 0.0012553429696708918]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012553429696708918

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.99705142
Iteration 2/25 | Loss: 0.00091674
Iteration 3/25 | Loss: 0.00091673
Iteration 4/25 | Loss: 0.00091673
Iteration 5/25 | Loss: 0.00091673
Iteration 6/25 | Loss: 0.00091673
Iteration 7/25 | Loss: 0.00091672
Iteration 8/25 | Loss: 0.00091672
Iteration 9/25 | Loss: 0.00091672
Iteration 10/25 | Loss: 0.00091672
Iteration 11/25 | Loss: 0.00091672
Iteration 12/25 | Loss: 0.00091672
Iteration 13/25 | Loss: 0.00091672
Iteration 14/25 | Loss: 0.00091672
Iteration 15/25 | Loss: 0.00091672
Iteration 16/25 | Loss: 0.00091672
Iteration 17/25 | Loss: 0.00091672
Iteration 18/25 | Loss: 0.00091672
Iteration 19/25 | Loss: 0.00091672
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009167234529741108, 0.0009167234529741108, 0.0009167234529741108, 0.0009167234529741108, 0.0009167234529741108]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009167234529741108

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091672
Iteration 2/1000 | Loss: 0.00004883
Iteration 3/1000 | Loss: 0.00003279
Iteration 4/1000 | Loss: 0.00002649
Iteration 5/1000 | Loss: 0.00002457
Iteration 6/1000 | Loss: 0.00002374
Iteration 7/1000 | Loss: 0.00002325
Iteration 8/1000 | Loss: 0.00002267
Iteration 9/1000 | Loss: 0.00002227
Iteration 10/1000 | Loss: 0.00002193
Iteration 11/1000 | Loss: 0.00002166
Iteration 12/1000 | Loss: 0.00002143
Iteration 13/1000 | Loss: 0.00002119
Iteration 14/1000 | Loss: 0.00002099
Iteration 15/1000 | Loss: 0.00002084
Iteration 16/1000 | Loss: 0.00002072
Iteration 17/1000 | Loss: 0.00002066
Iteration 18/1000 | Loss: 0.00002058
Iteration 19/1000 | Loss: 0.00002056
Iteration 20/1000 | Loss: 0.00002054
Iteration 21/1000 | Loss: 0.00002052
Iteration 22/1000 | Loss: 0.00002052
Iteration 23/1000 | Loss: 0.00002047
Iteration 24/1000 | Loss: 0.00002046
Iteration 25/1000 | Loss: 0.00002045
Iteration 26/1000 | Loss: 0.00002044
Iteration 27/1000 | Loss: 0.00002042
Iteration 28/1000 | Loss: 0.00002042
Iteration 29/1000 | Loss: 0.00002042
Iteration 30/1000 | Loss: 0.00002041
Iteration 31/1000 | Loss: 0.00002041
Iteration 32/1000 | Loss: 0.00002041
Iteration 33/1000 | Loss: 0.00002040
Iteration 34/1000 | Loss: 0.00002039
Iteration 35/1000 | Loss: 0.00002038
Iteration 36/1000 | Loss: 0.00002038
Iteration 37/1000 | Loss: 0.00002037
Iteration 38/1000 | Loss: 0.00002036
Iteration 39/1000 | Loss: 0.00002036
Iteration 40/1000 | Loss: 0.00002036
Iteration 41/1000 | Loss: 0.00002036
Iteration 42/1000 | Loss: 0.00002036
Iteration 43/1000 | Loss: 0.00002035
Iteration 44/1000 | Loss: 0.00002032
Iteration 45/1000 | Loss: 0.00002032
Iteration 46/1000 | Loss: 0.00002032
Iteration 47/1000 | Loss: 0.00002032
Iteration 48/1000 | Loss: 0.00002031
Iteration 49/1000 | Loss: 0.00002031
Iteration 50/1000 | Loss: 0.00002031
Iteration 51/1000 | Loss: 0.00002031
Iteration 52/1000 | Loss: 0.00002031
Iteration 53/1000 | Loss: 0.00002031
Iteration 54/1000 | Loss: 0.00002031
Iteration 55/1000 | Loss: 0.00002031
Iteration 56/1000 | Loss: 0.00002031
Iteration 57/1000 | Loss: 0.00002031
Iteration 58/1000 | Loss: 0.00002030
Iteration 59/1000 | Loss: 0.00002029
Iteration 60/1000 | Loss: 0.00002029
Iteration 61/1000 | Loss: 0.00002029
Iteration 62/1000 | Loss: 0.00002028
Iteration 63/1000 | Loss: 0.00002028
Iteration 64/1000 | Loss: 0.00002028
Iteration 65/1000 | Loss: 0.00002028
Iteration 66/1000 | Loss: 0.00002028
Iteration 67/1000 | Loss: 0.00002028
Iteration 68/1000 | Loss: 0.00002028
Iteration 69/1000 | Loss: 0.00002027
Iteration 70/1000 | Loss: 0.00002027
Iteration 71/1000 | Loss: 0.00002027
Iteration 72/1000 | Loss: 0.00002027
Iteration 73/1000 | Loss: 0.00002026
Iteration 74/1000 | Loss: 0.00002026
Iteration 75/1000 | Loss: 0.00002026
Iteration 76/1000 | Loss: 0.00002025
Iteration 77/1000 | Loss: 0.00002025
Iteration 78/1000 | Loss: 0.00002025
Iteration 79/1000 | Loss: 0.00002025
Iteration 80/1000 | Loss: 0.00002025
Iteration 81/1000 | Loss: 0.00002025
Iteration 82/1000 | Loss: 0.00002025
Iteration 83/1000 | Loss: 0.00002025
Iteration 84/1000 | Loss: 0.00002025
Iteration 85/1000 | Loss: 0.00002025
Iteration 86/1000 | Loss: 0.00002024
Iteration 87/1000 | Loss: 0.00002024
Iteration 88/1000 | Loss: 0.00002024
Iteration 89/1000 | Loss: 0.00002023
Iteration 90/1000 | Loss: 0.00002023
Iteration 91/1000 | Loss: 0.00002023
Iteration 92/1000 | Loss: 0.00002023
Iteration 93/1000 | Loss: 0.00002023
Iteration 94/1000 | Loss: 0.00002023
Iteration 95/1000 | Loss: 0.00002023
Iteration 96/1000 | Loss: 0.00002023
Iteration 97/1000 | Loss: 0.00002023
Iteration 98/1000 | Loss: 0.00002023
Iteration 99/1000 | Loss: 0.00002022
Iteration 100/1000 | Loss: 0.00002022
Iteration 101/1000 | Loss: 0.00002022
Iteration 102/1000 | Loss: 0.00002022
Iteration 103/1000 | Loss: 0.00002021
Iteration 104/1000 | Loss: 0.00002021
Iteration 105/1000 | Loss: 0.00002021
Iteration 106/1000 | Loss: 0.00002020
Iteration 107/1000 | Loss: 0.00002020
Iteration 108/1000 | Loss: 0.00002020
Iteration 109/1000 | Loss: 0.00002020
Iteration 110/1000 | Loss: 0.00002019
Iteration 111/1000 | Loss: 0.00002019
Iteration 112/1000 | Loss: 0.00002019
Iteration 113/1000 | Loss: 0.00002019
Iteration 114/1000 | Loss: 0.00002019
Iteration 115/1000 | Loss: 0.00002019
Iteration 116/1000 | Loss: 0.00002019
Iteration 117/1000 | Loss: 0.00002019
Iteration 118/1000 | Loss: 0.00002019
Iteration 119/1000 | Loss: 0.00002019
Iteration 120/1000 | Loss: 0.00002019
Iteration 121/1000 | Loss: 0.00002019
Iteration 122/1000 | Loss: 0.00002019
Iteration 123/1000 | Loss: 0.00002018
Iteration 124/1000 | Loss: 0.00002018
Iteration 125/1000 | Loss: 0.00002018
Iteration 126/1000 | Loss: 0.00002018
Iteration 127/1000 | Loss: 0.00002018
Iteration 128/1000 | Loss: 0.00002018
Iteration 129/1000 | Loss: 0.00002018
Iteration 130/1000 | Loss: 0.00002018
Iteration 131/1000 | Loss: 0.00002018
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [2.0184877939755097e-05, 2.0184877939755097e-05, 2.0184877939755097e-05, 2.0184877939755097e-05, 2.0184877939755097e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0184877939755097e-05

Optimization complete. Final v2v error: 3.568356990814209 mm

Highest mean error: 4.613343238830566 mm for frame 59

Lowest mean error: 2.6858811378479004 mm for frame 136

Saving results

Total time: 43.79398250579834
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_021/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_021/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00928259
Iteration 2/25 | Loss: 0.00162315
Iteration 3/25 | Loss: 0.00125082
Iteration 4/25 | Loss: 0.00122558
Iteration 5/25 | Loss: 0.00122073
Iteration 6/25 | Loss: 0.00121927
Iteration 7/25 | Loss: 0.00121927
Iteration 8/25 | Loss: 0.00121927
Iteration 9/25 | Loss: 0.00121927
Iteration 10/25 | Loss: 0.00121927
Iteration 11/25 | Loss: 0.00121927
Iteration 12/25 | Loss: 0.00121927
Iteration 13/25 | Loss: 0.00121927
Iteration 14/25 | Loss: 0.00121927
Iteration 15/25 | Loss: 0.00121927
Iteration 16/25 | Loss: 0.00121927
Iteration 17/25 | Loss: 0.00121927
Iteration 18/25 | Loss: 0.00121927
Iteration 19/25 | Loss: 0.00121927
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012192667927592993, 0.0012192667927592993, 0.0012192667927592993, 0.0012192667927592993, 0.0012192667927592993]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012192667927592993

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.95847112
Iteration 2/25 | Loss: 0.00080858
Iteration 3/25 | Loss: 0.00080858
Iteration 4/25 | Loss: 0.00080858
Iteration 5/25 | Loss: 0.00080858
Iteration 6/25 | Loss: 0.00080858
Iteration 7/25 | Loss: 0.00080858
Iteration 8/25 | Loss: 0.00080858
Iteration 9/25 | Loss: 0.00080858
Iteration 10/25 | Loss: 0.00080858
Iteration 11/25 | Loss: 0.00080858
Iteration 12/25 | Loss: 0.00080858
Iteration 13/25 | Loss: 0.00080858
Iteration 14/25 | Loss: 0.00080858
Iteration 15/25 | Loss: 0.00080858
Iteration 16/25 | Loss: 0.00080858
Iteration 17/25 | Loss: 0.00080858
Iteration 18/25 | Loss: 0.00080858
Iteration 19/25 | Loss: 0.00080858
Iteration 20/25 | Loss: 0.00080858
Iteration 21/25 | Loss: 0.00080858
Iteration 22/25 | Loss: 0.00080858
Iteration 23/25 | Loss: 0.00080858
Iteration 24/25 | Loss: 0.00080858
Iteration 25/25 | Loss: 0.00080858

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080858
Iteration 2/1000 | Loss: 0.00005456
Iteration 3/1000 | Loss: 0.00003464
Iteration 4/1000 | Loss: 0.00002549
Iteration 5/1000 | Loss: 0.00002375
Iteration 6/1000 | Loss: 0.00002257
Iteration 7/1000 | Loss: 0.00002184
Iteration 8/1000 | Loss: 0.00002116
Iteration 9/1000 | Loss: 0.00002069
Iteration 10/1000 | Loss: 0.00002068
Iteration 11/1000 | Loss: 0.00002041
Iteration 12/1000 | Loss: 0.00002016
Iteration 13/1000 | Loss: 0.00001998
Iteration 14/1000 | Loss: 0.00001994
Iteration 15/1000 | Loss: 0.00001982
Iteration 16/1000 | Loss: 0.00001967
Iteration 17/1000 | Loss: 0.00001958
Iteration 18/1000 | Loss: 0.00001955
Iteration 19/1000 | Loss: 0.00001955
Iteration 20/1000 | Loss: 0.00001953
Iteration 21/1000 | Loss: 0.00001948
Iteration 22/1000 | Loss: 0.00001947
Iteration 23/1000 | Loss: 0.00001936
Iteration 24/1000 | Loss: 0.00001933
Iteration 25/1000 | Loss: 0.00001930
Iteration 26/1000 | Loss: 0.00001928
Iteration 27/1000 | Loss: 0.00001928
Iteration 28/1000 | Loss: 0.00001924
Iteration 29/1000 | Loss: 0.00001924
Iteration 30/1000 | Loss: 0.00001924
Iteration 31/1000 | Loss: 0.00001924
Iteration 32/1000 | Loss: 0.00001924
Iteration 33/1000 | Loss: 0.00001923
Iteration 34/1000 | Loss: 0.00001922
Iteration 35/1000 | Loss: 0.00001922
Iteration 36/1000 | Loss: 0.00001921
Iteration 37/1000 | Loss: 0.00001921
Iteration 38/1000 | Loss: 0.00001921
Iteration 39/1000 | Loss: 0.00001920
Iteration 40/1000 | Loss: 0.00001920
Iteration 41/1000 | Loss: 0.00001919
Iteration 42/1000 | Loss: 0.00001919
Iteration 43/1000 | Loss: 0.00001918
Iteration 44/1000 | Loss: 0.00001918
Iteration 45/1000 | Loss: 0.00001917
Iteration 46/1000 | Loss: 0.00001917
Iteration 47/1000 | Loss: 0.00001916
Iteration 48/1000 | Loss: 0.00001916
Iteration 49/1000 | Loss: 0.00001915
Iteration 50/1000 | Loss: 0.00001915
Iteration 51/1000 | Loss: 0.00001914
Iteration 52/1000 | Loss: 0.00001914
Iteration 53/1000 | Loss: 0.00001914
Iteration 54/1000 | Loss: 0.00001913
Iteration 55/1000 | Loss: 0.00001913
Iteration 56/1000 | Loss: 0.00001913
Iteration 57/1000 | Loss: 0.00001912
Iteration 58/1000 | Loss: 0.00001911
Iteration 59/1000 | Loss: 0.00001911
Iteration 60/1000 | Loss: 0.00001910
Iteration 61/1000 | Loss: 0.00001910
Iteration 62/1000 | Loss: 0.00001910
Iteration 63/1000 | Loss: 0.00001909
Iteration 64/1000 | Loss: 0.00001909
Iteration 65/1000 | Loss: 0.00001909
Iteration 66/1000 | Loss: 0.00001908
Iteration 67/1000 | Loss: 0.00001908
Iteration 68/1000 | Loss: 0.00001908
Iteration 69/1000 | Loss: 0.00001908
Iteration 70/1000 | Loss: 0.00001908
Iteration 71/1000 | Loss: 0.00001908
Iteration 72/1000 | Loss: 0.00001908
Iteration 73/1000 | Loss: 0.00001908
Iteration 74/1000 | Loss: 0.00001907
Iteration 75/1000 | Loss: 0.00001907
Iteration 76/1000 | Loss: 0.00001907
Iteration 77/1000 | Loss: 0.00001907
Iteration 78/1000 | Loss: 0.00001907
Iteration 79/1000 | Loss: 0.00001907
Iteration 80/1000 | Loss: 0.00001906
Iteration 81/1000 | Loss: 0.00001906
Iteration 82/1000 | Loss: 0.00001906
Iteration 83/1000 | Loss: 0.00001906
Iteration 84/1000 | Loss: 0.00001906
Iteration 85/1000 | Loss: 0.00001906
Iteration 86/1000 | Loss: 0.00001906
Iteration 87/1000 | Loss: 0.00001906
Iteration 88/1000 | Loss: 0.00001906
Iteration 89/1000 | Loss: 0.00001906
Iteration 90/1000 | Loss: 0.00001905
Iteration 91/1000 | Loss: 0.00001905
Iteration 92/1000 | Loss: 0.00001905
Iteration 93/1000 | Loss: 0.00001904
Iteration 94/1000 | Loss: 0.00001904
Iteration 95/1000 | Loss: 0.00001904
Iteration 96/1000 | Loss: 0.00001904
Iteration 97/1000 | Loss: 0.00001904
Iteration 98/1000 | Loss: 0.00001904
Iteration 99/1000 | Loss: 0.00001904
Iteration 100/1000 | Loss: 0.00001904
Iteration 101/1000 | Loss: 0.00001904
Iteration 102/1000 | Loss: 0.00001903
Iteration 103/1000 | Loss: 0.00001903
Iteration 104/1000 | Loss: 0.00001903
Iteration 105/1000 | Loss: 0.00001903
Iteration 106/1000 | Loss: 0.00001903
Iteration 107/1000 | Loss: 0.00001903
Iteration 108/1000 | Loss: 0.00001903
Iteration 109/1000 | Loss: 0.00001903
Iteration 110/1000 | Loss: 0.00001902
Iteration 111/1000 | Loss: 0.00001902
Iteration 112/1000 | Loss: 0.00001902
Iteration 113/1000 | Loss: 0.00001902
Iteration 114/1000 | Loss: 0.00001902
Iteration 115/1000 | Loss: 0.00001901
Iteration 116/1000 | Loss: 0.00001901
Iteration 117/1000 | Loss: 0.00001901
Iteration 118/1000 | Loss: 0.00001901
Iteration 119/1000 | Loss: 0.00001901
Iteration 120/1000 | Loss: 0.00001901
Iteration 121/1000 | Loss: 0.00001901
Iteration 122/1000 | Loss: 0.00001900
Iteration 123/1000 | Loss: 0.00001900
Iteration 124/1000 | Loss: 0.00001900
Iteration 125/1000 | Loss: 0.00001900
Iteration 126/1000 | Loss: 0.00001900
Iteration 127/1000 | Loss: 0.00001900
Iteration 128/1000 | Loss: 0.00001900
Iteration 129/1000 | Loss: 0.00001900
Iteration 130/1000 | Loss: 0.00001900
Iteration 131/1000 | Loss: 0.00001900
Iteration 132/1000 | Loss: 0.00001900
Iteration 133/1000 | Loss: 0.00001900
Iteration 134/1000 | Loss: 0.00001900
Iteration 135/1000 | Loss: 0.00001899
Iteration 136/1000 | Loss: 0.00001899
Iteration 137/1000 | Loss: 0.00001899
Iteration 138/1000 | Loss: 0.00001899
Iteration 139/1000 | Loss: 0.00001899
Iteration 140/1000 | Loss: 0.00001899
Iteration 141/1000 | Loss: 0.00001899
Iteration 142/1000 | Loss: 0.00001899
Iteration 143/1000 | Loss: 0.00001899
Iteration 144/1000 | Loss: 0.00001899
Iteration 145/1000 | Loss: 0.00001899
Iteration 146/1000 | Loss: 0.00001898
Iteration 147/1000 | Loss: 0.00001898
Iteration 148/1000 | Loss: 0.00001898
Iteration 149/1000 | Loss: 0.00001898
Iteration 150/1000 | Loss: 0.00001898
Iteration 151/1000 | Loss: 0.00001898
Iteration 152/1000 | Loss: 0.00001898
Iteration 153/1000 | Loss: 0.00001898
Iteration 154/1000 | Loss: 0.00001898
Iteration 155/1000 | Loss: 0.00001898
Iteration 156/1000 | Loss: 0.00001898
Iteration 157/1000 | Loss: 0.00001898
Iteration 158/1000 | Loss: 0.00001898
Iteration 159/1000 | Loss: 0.00001898
Iteration 160/1000 | Loss: 0.00001898
Iteration 161/1000 | Loss: 0.00001898
Iteration 162/1000 | Loss: 0.00001898
Iteration 163/1000 | Loss: 0.00001898
Iteration 164/1000 | Loss: 0.00001898
Iteration 165/1000 | Loss: 0.00001898
Iteration 166/1000 | Loss: 0.00001898
Iteration 167/1000 | Loss: 0.00001898
Iteration 168/1000 | Loss: 0.00001898
Iteration 169/1000 | Loss: 0.00001898
Iteration 170/1000 | Loss: 0.00001898
Iteration 171/1000 | Loss: 0.00001898
Iteration 172/1000 | Loss: 0.00001898
Iteration 173/1000 | Loss: 0.00001898
Iteration 174/1000 | Loss: 0.00001898
Iteration 175/1000 | Loss: 0.00001898
Iteration 176/1000 | Loss: 0.00001898
Iteration 177/1000 | Loss: 0.00001898
Iteration 178/1000 | Loss: 0.00001898
Iteration 179/1000 | Loss: 0.00001898
Iteration 180/1000 | Loss: 0.00001898
Iteration 181/1000 | Loss: 0.00001898
Iteration 182/1000 | Loss: 0.00001898
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [1.8982131223310716e-05, 1.8982131223310716e-05, 1.8982131223310716e-05, 1.8982131223310716e-05, 1.8982131223310716e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8982131223310716e-05

Optimization complete. Final v2v error: 3.6295464038848877 mm

Highest mean error: 4.204164981842041 mm for frame 137

Lowest mean error: 2.921332359313965 mm for frame 27

Saving results

Total time: 43.90440130233765
