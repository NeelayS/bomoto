Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=203, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 11368-11423
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00404879
Iteration 2/25 | Loss: 0.00120339
Iteration 3/25 | Loss: 0.00108554
Iteration 4/25 | Loss: 0.00106670
Iteration 5/25 | Loss: 0.00106118
Iteration 6/25 | Loss: 0.00105967
Iteration 7/25 | Loss: 0.00105962
Iteration 8/25 | Loss: 0.00105962
Iteration 9/25 | Loss: 0.00105962
Iteration 10/25 | Loss: 0.00105962
Iteration 11/25 | Loss: 0.00105962
Iteration 12/25 | Loss: 0.00105962
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010596233187243342, 0.0010596233187243342, 0.0010596233187243342, 0.0010596233187243342, 0.0010596233187243342]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010596233187243342

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.06552005
Iteration 2/25 | Loss: 0.00075229
Iteration 3/25 | Loss: 0.00075228
Iteration 4/25 | Loss: 0.00075228
Iteration 5/25 | Loss: 0.00075228
Iteration 6/25 | Loss: 0.00075228
Iteration 7/25 | Loss: 0.00075227
Iteration 8/25 | Loss: 0.00075227
Iteration 9/25 | Loss: 0.00075227
Iteration 10/25 | Loss: 0.00075227
Iteration 11/25 | Loss: 0.00075227
Iteration 12/25 | Loss: 0.00075227
Iteration 13/25 | Loss: 0.00075227
Iteration 14/25 | Loss: 0.00075227
Iteration 15/25 | Loss: 0.00075227
Iteration 16/25 | Loss: 0.00075227
Iteration 17/25 | Loss: 0.00075227
Iteration 18/25 | Loss: 0.00075227
Iteration 19/25 | Loss: 0.00075227
Iteration 20/25 | Loss: 0.00075227
Iteration 21/25 | Loss: 0.00075227
Iteration 22/25 | Loss: 0.00075227
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007522734231315553, 0.0007522734231315553, 0.0007522734231315553, 0.0007522734231315553, 0.0007522734231315553]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007522734231315553

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075227
Iteration 2/1000 | Loss: 0.00002751
Iteration 3/1000 | Loss: 0.00001857
Iteration 4/1000 | Loss: 0.00001467
Iteration 5/1000 | Loss: 0.00001360
Iteration 6/1000 | Loss: 0.00001291
Iteration 7/1000 | Loss: 0.00001239
Iteration 8/1000 | Loss: 0.00001207
Iteration 9/1000 | Loss: 0.00001181
Iteration 10/1000 | Loss: 0.00001175
Iteration 11/1000 | Loss: 0.00001157
Iteration 12/1000 | Loss: 0.00001135
Iteration 13/1000 | Loss: 0.00001130
Iteration 14/1000 | Loss: 0.00001122
Iteration 15/1000 | Loss: 0.00001121
Iteration 16/1000 | Loss: 0.00001116
Iteration 17/1000 | Loss: 0.00001113
Iteration 18/1000 | Loss: 0.00001113
Iteration 19/1000 | Loss: 0.00001112
Iteration 20/1000 | Loss: 0.00001112
Iteration 21/1000 | Loss: 0.00001106
Iteration 22/1000 | Loss: 0.00001106
Iteration 23/1000 | Loss: 0.00001102
Iteration 24/1000 | Loss: 0.00001102
Iteration 25/1000 | Loss: 0.00001102
Iteration 26/1000 | Loss: 0.00001101
Iteration 27/1000 | Loss: 0.00001100
Iteration 28/1000 | Loss: 0.00001099
Iteration 29/1000 | Loss: 0.00001098
Iteration 30/1000 | Loss: 0.00001098
Iteration 31/1000 | Loss: 0.00001098
Iteration 32/1000 | Loss: 0.00001097
Iteration 33/1000 | Loss: 0.00001096
Iteration 34/1000 | Loss: 0.00001096
Iteration 35/1000 | Loss: 0.00001095
Iteration 36/1000 | Loss: 0.00001093
Iteration 37/1000 | Loss: 0.00001093
Iteration 38/1000 | Loss: 0.00001093
Iteration 39/1000 | Loss: 0.00001093
Iteration 40/1000 | Loss: 0.00001093
Iteration 41/1000 | Loss: 0.00001092
Iteration 42/1000 | Loss: 0.00001092
Iteration 43/1000 | Loss: 0.00001092
Iteration 44/1000 | Loss: 0.00001091
Iteration 45/1000 | Loss: 0.00001091
Iteration 46/1000 | Loss: 0.00001090
Iteration 47/1000 | Loss: 0.00001090
Iteration 48/1000 | Loss: 0.00001090
Iteration 49/1000 | Loss: 0.00001089
Iteration 50/1000 | Loss: 0.00001089
Iteration 51/1000 | Loss: 0.00001088
Iteration 52/1000 | Loss: 0.00001088
Iteration 53/1000 | Loss: 0.00001088
Iteration 54/1000 | Loss: 0.00001088
Iteration 55/1000 | Loss: 0.00001087
Iteration 56/1000 | Loss: 0.00001086
Iteration 57/1000 | Loss: 0.00001086
Iteration 58/1000 | Loss: 0.00001086
Iteration 59/1000 | Loss: 0.00001085
Iteration 60/1000 | Loss: 0.00001085
Iteration 61/1000 | Loss: 0.00001085
Iteration 62/1000 | Loss: 0.00001085
Iteration 63/1000 | Loss: 0.00001084
Iteration 64/1000 | Loss: 0.00001084
Iteration 65/1000 | Loss: 0.00001084
Iteration 66/1000 | Loss: 0.00001083
Iteration 67/1000 | Loss: 0.00001083
Iteration 68/1000 | Loss: 0.00001083
Iteration 69/1000 | Loss: 0.00001083
Iteration 70/1000 | Loss: 0.00001083
Iteration 71/1000 | Loss: 0.00001082
Iteration 72/1000 | Loss: 0.00001082
Iteration 73/1000 | Loss: 0.00001081
Iteration 74/1000 | Loss: 0.00001081
Iteration 75/1000 | Loss: 0.00001081
Iteration 76/1000 | Loss: 0.00001081
Iteration 77/1000 | Loss: 0.00001080
Iteration 78/1000 | Loss: 0.00001080
Iteration 79/1000 | Loss: 0.00001080
Iteration 80/1000 | Loss: 0.00001079
Iteration 81/1000 | Loss: 0.00001079
Iteration 82/1000 | Loss: 0.00001079
Iteration 83/1000 | Loss: 0.00001079
Iteration 84/1000 | Loss: 0.00001079
Iteration 85/1000 | Loss: 0.00001078
Iteration 86/1000 | Loss: 0.00001078
Iteration 87/1000 | Loss: 0.00001078
Iteration 88/1000 | Loss: 0.00001077
Iteration 89/1000 | Loss: 0.00001077
Iteration 90/1000 | Loss: 0.00001077
Iteration 91/1000 | Loss: 0.00001076
Iteration 92/1000 | Loss: 0.00001076
Iteration 93/1000 | Loss: 0.00001076
Iteration 94/1000 | Loss: 0.00001076
Iteration 95/1000 | Loss: 0.00001075
Iteration 96/1000 | Loss: 0.00001075
Iteration 97/1000 | Loss: 0.00001075
Iteration 98/1000 | Loss: 0.00001075
Iteration 99/1000 | Loss: 0.00001074
Iteration 100/1000 | Loss: 0.00001074
Iteration 101/1000 | Loss: 0.00001074
Iteration 102/1000 | Loss: 0.00001074
Iteration 103/1000 | Loss: 0.00001073
Iteration 104/1000 | Loss: 0.00001073
Iteration 105/1000 | Loss: 0.00001073
Iteration 106/1000 | Loss: 0.00001073
Iteration 107/1000 | Loss: 0.00001072
Iteration 108/1000 | Loss: 0.00001072
Iteration 109/1000 | Loss: 0.00001072
Iteration 110/1000 | Loss: 0.00001072
Iteration 111/1000 | Loss: 0.00001072
Iteration 112/1000 | Loss: 0.00001072
Iteration 113/1000 | Loss: 0.00001072
Iteration 114/1000 | Loss: 0.00001072
Iteration 115/1000 | Loss: 0.00001071
Iteration 116/1000 | Loss: 0.00001071
Iteration 117/1000 | Loss: 0.00001071
Iteration 118/1000 | Loss: 0.00001071
Iteration 119/1000 | Loss: 0.00001071
Iteration 120/1000 | Loss: 0.00001071
Iteration 121/1000 | Loss: 0.00001071
Iteration 122/1000 | Loss: 0.00001071
Iteration 123/1000 | Loss: 0.00001071
Iteration 124/1000 | Loss: 0.00001071
Iteration 125/1000 | Loss: 0.00001071
Iteration 126/1000 | Loss: 0.00001071
Iteration 127/1000 | Loss: 0.00001071
Iteration 128/1000 | Loss: 0.00001070
Iteration 129/1000 | Loss: 0.00001070
Iteration 130/1000 | Loss: 0.00001070
Iteration 131/1000 | Loss: 0.00001070
Iteration 132/1000 | Loss: 0.00001070
Iteration 133/1000 | Loss: 0.00001070
Iteration 134/1000 | Loss: 0.00001070
Iteration 135/1000 | Loss: 0.00001070
Iteration 136/1000 | Loss: 0.00001070
Iteration 137/1000 | Loss: 0.00001070
Iteration 138/1000 | Loss: 0.00001069
Iteration 139/1000 | Loss: 0.00001069
Iteration 140/1000 | Loss: 0.00001069
Iteration 141/1000 | Loss: 0.00001069
Iteration 142/1000 | Loss: 0.00001069
Iteration 143/1000 | Loss: 0.00001069
Iteration 144/1000 | Loss: 0.00001069
Iteration 145/1000 | Loss: 0.00001069
Iteration 146/1000 | Loss: 0.00001069
Iteration 147/1000 | Loss: 0.00001068
Iteration 148/1000 | Loss: 0.00001068
Iteration 149/1000 | Loss: 0.00001068
Iteration 150/1000 | Loss: 0.00001068
Iteration 151/1000 | Loss: 0.00001068
Iteration 152/1000 | Loss: 0.00001068
Iteration 153/1000 | Loss: 0.00001068
Iteration 154/1000 | Loss: 0.00001068
Iteration 155/1000 | Loss: 0.00001068
Iteration 156/1000 | Loss: 0.00001068
Iteration 157/1000 | Loss: 0.00001068
Iteration 158/1000 | Loss: 0.00001068
Iteration 159/1000 | Loss: 0.00001067
Iteration 160/1000 | Loss: 0.00001067
Iteration 161/1000 | Loss: 0.00001067
Iteration 162/1000 | Loss: 0.00001067
Iteration 163/1000 | Loss: 0.00001067
Iteration 164/1000 | Loss: 0.00001067
Iteration 165/1000 | Loss: 0.00001067
Iteration 166/1000 | Loss: 0.00001067
Iteration 167/1000 | Loss: 0.00001067
Iteration 168/1000 | Loss: 0.00001067
Iteration 169/1000 | Loss: 0.00001067
Iteration 170/1000 | Loss: 0.00001067
Iteration 171/1000 | Loss: 0.00001067
Iteration 172/1000 | Loss: 0.00001066
Iteration 173/1000 | Loss: 0.00001066
Iteration 174/1000 | Loss: 0.00001066
Iteration 175/1000 | Loss: 0.00001066
Iteration 176/1000 | Loss: 0.00001066
Iteration 177/1000 | Loss: 0.00001066
Iteration 178/1000 | Loss: 0.00001066
Iteration 179/1000 | Loss: 0.00001066
Iteration 180/1000 | Loss: 0.00001065
Iteration 181/1000 | Loss: 0.00001065
Iteration 182/1000 | Loss: 0.00001065
Iteration 183/1000 | Loss: 0.00001065
Iteration 184/1000 | Loss: 0.00001065
Iteration 185/1000 | Loss: 0.00001065
Iteration 186/1000 | Loss: 0.00001065
Iteration 187/1000 | Loss: 0.00001064
Iteration 188/1000 | Loss: 0.00001064
Iteration 189/1000 | Loss: 0.00001064
Iteration 190/1000 | Loss: 0.00001064
Iteration 191/1000 | Loss: 0.00001064
Iteration 192/1000 | Loss: 0.00001064
Iteration 193/1000 | Loss: 0.00001063
Iteration 194/1000 | Loss: 0.00001063
Iteration 195/1000 | Loss: 0.00001063
Iteration 196/1000 | Loss: 0.00001063
Iteration 197/1000 | Loss: 0.00001063
Iteration 198/1000 | Loss: 0.00001063
Iteration 199/1000 | Loss: 0.00001063
Iteration 200/1000 | Loss: 0.00001063
Iteration 201/1000 | Loss: 0.00001063
Iteration 202/1000 | Loss: 0.00001063
Iteration 203/1000 | Loss: 0.00001063
Iteration 204/1000 | Loss: 0.00001063
Iteration 205/1000 | Loss: 0.00001063
Iteration 206/1000 | Loss: 0.00001063
Iteration 207/1000 | Loss: 0.00001063
Iteration 208/1000 | Loss: 0.00001062
Iteration 209/1000 | Loss: 0.00001062
Iteration 210/1000 | Loss: 0.00001062
Iteration 211/1000 | Loss: 0.00001062
Iteration 212/1000 | Loss: 0.00001062
Iteration 213/1000 | Loss: 0.00001062
Iteration 214/1000 | Loss: 0.00001062
Iteration 215/1000 | Loss: 0.00001062
Iteration 216/1000 | Loss: 0.00001062
Iteration 217/1000 | Loss: 0.00001062
Iteration 218/1000 | Loss: 0.00001062
Iteration 219/1000 | Loss: 0.00001062
Iteration 220/1000 | Loss: 0.00001062
Iteration 221/1000 | Loss: 0.00001062
Iteration 222/1000 | Loss: 0.00001062
Iteration 223/1000 | Loss: 0.00001062
Iteration 224/1000 | Loss: 0.00001062
Iteration 225/1000 | Loss: 0.00001061
Iteration 226/1000 | Loss: 0.00001061
Iteration 227/1000 | Loss: 0.00001061
Iteration 228/1000 | Loss: 0.00001061
Iteration 229/1000 | Loss: 0.00001061
Iteration 230/1000 | Loss: 0.00001061
Iteration 231/1000 | Loss: 0.00001061
Iteration 232/1000 | Loss: 0.00001061
Iteration 233/1000 | Loss: 0.00001061
Iteration 234/1000 | Loss: 0.00001061
Iteration 235/1000 | Loss: 0.00001061
Iteration 236/1000 | Loss: 0.00001061
Iteration 237/1000 | Loss: 0.00001061
Iteration 238/1000 | Loss: 0.00001061
Iteration 239/1000 | Loss: 0.00001061
Iteration 240/1000 | Loss: 0.00001061
Iteration 241/1000 | Loss: 0.00001061
Iteration 242/1000 | Loss: 0.00001060
Iteration 243/1000 | Loss: 0.00001060
Iteration 244/1000 | Loss: 0.00001060
Iteration 245/1000 | Loss: 0.00001060
Iteration 246/1000 | Loss: 0.00001060
Iteration 247/1000 | Loss: 0.00001060
Iteration 248/1000 | Loss: 0.00001060
Iteration 249/1000 | Loss: 0.00001060
Iteration 250/1000 | Loss: 0.00001060
Iteration 251/1000 | Loss: 0.00001060
Iteration 252/1000 | Loss: 0.00001060
Iteration 253/1000 | Loss: 0.00001060
Iteration 254/1000 | Loss: 0.00001059
Iteration 255/1000 | Loss: 0.00001059
Iteration 256/1000 | Loss: 0.00001059
Iteration 257/1000 | Loss: 0.00001059
Iteration 258/1000 | Loss: 0.00001059
Iteration 259/1000 | Loss: 0.00001059
Iteration 260/1000 | Loss: 0.00001059
Iteration 261/1000 | Loss: 0.00001059
Iteration 262/1000 | Loss: 0.00001059
Iteration 263/1000 | Loss: 0.00001059
Iteration 264/1000 | Loss: 0.00001059
Iteration 265/1000 | Loss: 0.00001059
Iteration 266/1000 | Loss: 0.00001059
Iteration 267/1000 | Loss: 0.00001059
Iteration 268/1000 | Loss: 0.00001059
Iteration 269/1000 | Loss: 0.00001059
Iteration 270/1000 | Loss: 0.00001059
Iteration 271/1000 | Loss: 0.00001059
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 271. Stopping optimization.
Last 5 losses: [1.0589810699457303e-05, 1.0589810699457303e-05, 1.0589810699457303e-05, 1.0589810699457303e-05, 1.0589810699457303e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0589810699457303e-05

Optimization complete. Final v2v error: 2.7974190711975098 mm

Highest mean error: 3.297306776046753 mm for frame 69

Lowest mean error: 2.5201256275177 mm for frame 29

Saving results

Total time: 49.40072774887085
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00401167
Iteration 2/25 | Loss: 0.00118639
Iteration 3/25 | Loss: 0.00108333
Iteration 4/25 | Loss: 0.00107394
Iteration 5/25 | Loss: 0.00107055
Iteration 6/25 | Loss: 0.00106964
Iteration 7/25 | Loss: 0.00106964
Iteration 8/25 | Loss: 0.00106964
Iteration 9/25 | Loss: 0.00106964
Iteration 10/25 | Loss: 0.00106964
Iteration 11/25 | Loss: 0.00106964
Iteration 12/25 | Loss: 0.00106964
Iteration 13/25 | Loss: 0.00106964
Iteration 14/25 | Loss: 0.00106964
Iteration 15/25 | Loss: 0.00106964
Iteration 16/25 | Loss: 0.00106964
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010696429526433349, 0.0010696429526433349, 0.0010696429526433349, 0.0010696429526433349, 0.0010696429526433349]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010696429526433349

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42424738
Iteration 2/25 | Loss: 0.00081676
Iteration 3/25 | Loss: 0.00081675
Iteration 4/25 | Loss: 0.00081675
Iteration 5/25 | Loss: 0.00081675
Iteration 6/25 | Loss: 0.00081675
Iteration 7/25 | Loss: 0.00081675
Iteration 8/25 | Loss: 0.00081675
Iteration 9/25 | Loss: 0.00081675
Iteration 10/25 | Loss: 0.00081675
Iteration 11/25 | Loss: 0.00081675
Iteration 12/25 | Loss: 0.00081675
Iteration 13/25 | Loss: 0.00081675
Iteration 14/25 | Loss: 0.00081675
Iteration 15/25 | Loss: 0.00081675
Iteration 16/25 | Loss: 0.00081675
Iteration 17/25 | Loss: 0.00081675
Iteration 18/25 | Loss: 0.00081675
Iteration 19/25 | Loss: 0.00081675
Iteration 20/25 | Loss: 0.00081675
Iteration 21/25 | Loss: 0.00081675
Iteration 22/25 | Loss: 0.00081675
Iteration 23/25 | Loss: 0.00081675
Iteration 24/25 | Loss: 0.00081675
Iteration 25/25 | Loss: 0.00081675

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081675
Iteration 2/1000 | Loss: 0.00003564
Iteration 3/1000 | Loss: 0.00002229
Iteration 4/1000 | Loss: 0.00001635
Iteration 5/1000 | Loss: 0.00001455
Iteration 6/1000 | Loss: 0.00001369
Iteration 7/1000 | Loss: 0.00001292
Iteration 8/1000 | Loss: 0.00001260
Iteration 9/1000 | Loss: 0.00001226
Iteration 10/1000 | Loss: 0.00001204
Iteration 11/1000 | Loss: 0.00001199
Iteration 12/1000 | Loss: 0.00001183
Iteration 13/1000 | Loss: 0.00001177
Iteration 14/1000 | Loss: 0.00001170
Iteration 15/1000 | Loss: 0.00001167
Iteration 16/1000 | Loss: 0.00001167
Iteration 17/1000 | Loss: 0.00001164
Iteration 18/1000 | Loss: 0.00001164
Iteration 19/1000 | Loss: 0.00001163
Iteration 20/1000 | Loss: 0.00001162
Iteration 21/1000 | Loss: 0.00001161
Iteration 22/1000 | Loss: 0.00001161
Iteration 23/1000 | Loss: 0.00001160
Iteration 24/1000 | Loss: 0.00001160
Iteration 25/1000 | Loss: 0.00001160
Iteration 26/1000 | Loss: 0.00001159
Iteration 27/1000 | Loss: 0.00001156
Iteration 28/1000 | Loss: 0.00001153
Iteration 29/1000 | Loss: 0.00001149
Iteration 30/1000 | Loss: 0.00001149
Iteration 31/1000 | Loss: 0.00001148
Iteration 32/1000 | Loss: 0.00001147
Iteration 33/1000 | Loss: 0.00001146
Iteration 34/1000 | Loss: 0.00001146
Iteration 35/1000 | Loss: 0.00001146
Iteration 36/1000 | Loss: 0.00001146
Iteration 37/1000 | Loss: 0.00001145
Iteration 38/1000 | Loss: 0.00001145
Iteration 39/1000 | Loss: 0.00001144
Iteration 40/1000 | Loss: 0.00001143
Iteration 41/1000 | Loss: 0.00001143
Iteration 42/1000 | Loss: 0.00001143
Iteration 43/1000 | Loss: 0.00001143
Iteration 44/1000 | Loss: 0.00001143
Iteration 45/1000 | Loss: 0.00001142
Iteration 46/1000 | Loss: 0.00001142
Iteration 47/1000 | Loss: 0.00001142
Iteration 48/1000 | Loss: 0.00001142
Iteration 49/1000 | Loss: 0.00001142
Iteration 50/1000 | Loss: 0.00001142
Iteration 51/1000 | Loss: 0.00001142
Iteration 52/1000 | Loss: 0.00001142
Iteration 53/1000 | Loss: 0.00001142
Iteration 54/1000 | Loss: 0.00001142
Iteration 55/1000 | Loss: 0.00001142
Iteration 56/1000 | Loss: 0.00001141
Iteration 57/1000 | Loss: 0.00001141
Iteration 58/1000 | Loss: 0.00001141
Iteration 59/1000 | Loss: 0.00001141
Iteration 60/1000 | Loss: 0.00001140
Iteration 61/1000 | Loss: 0.00001140
Iteration 62/1000 | Loss: 0.00001140
Iteration 63/1000 | Loss: 0.00001140
Iteration 64/1000 | Loss: 0.00001139
Iteration 65/1000 | Loss: 0.00001139
Iteration 66/1000 | Loss: 0.00001139
Iteration 67/1000 | Loss: 0.00001139
Iteration 68/1000 | Loss: 0.00001139
Iteration 69/1000 | Loss: 0.00001139
Iteration 70/1000 | Loss: 0.00001138
Iteration 71/1000 | Loss: 0.00001138
Iteration 72/1000 | Loss: 0.00001138
Iteration 73/1000 | Loss: 0.00001138
Iteration 74/1000 | Loss: 0.00001137
Iteration 75/1000 | Loss: 0.00001137
Iteration 76/1000 | Loss: 0.00001137
Iteration 77/1000 | Loss: 0.00001137
Iteration 78/1000 | Loss: 0.00001137
Iteration 79/1000 | Loss: 0.00001137
Iteration 80/1000 | Loss: 0.00001137
Iteration 81/1000 | Loss: 0.00001137
Iteration 82/1000 | Loss: 0.00001137
Iteration 83/1000 | Loss: 0.00001136
Iteration 84/1000 | Loss: 0.00001136
Iteration 85/1000 | Loss: 0.00001136
Iteration 86/1000 | Loss: 0.00001136
Iteration 87/1000 | Loss: 0.00001136
Iteration 88/1000 | Loss: 0.00001136
Iteration 89/1000 | Loss: 0.00001136
Iteration 90/1000 | Loss: 0.00001136
Iteration 91/1000 | Loss: 0.00001136
Iteration 92/1000 | Loss: 0.00001136
Iteration 93/1000 | Loss: 0.00001135
Iteration 94/1000 | Loss: 0.00001135
Iteration 95/1000 | Loss: 0.00001135
Iteration 96/1000 | Loss: 0.00001135
Iteration 97/1000 | Loss: 0.00001135
Iteration 98/1000 | Loss: 0.00001135
Iteration 99/1000 | Loss: 0.00001135
Iteration 100/1000 | Loss: 0.00001135
Iteration 101/1000 | Loss: 0.00001135
Iteration 102/1000 | Loss: 0.00001135
Iteration 103/1000 | Loss: 0.00001135
Iteration 104/1000 | Loss: 0.00001135
Iteration 105/1000 | Loss: 0.00001135
Iteration 106/1000 | Loss: 0.00001135
Iteration 107/1000 | Loss: 0.00001135
Iteration 108/1000 | Loss: 0.00001134
Iteration 109/1000 | Loss: 0.00001134
Iteration 110/1000 | Loss: 0.00001134
Iteration 111/1000 | Loss: 0.00001134
Iteration 112/1000 | Loss: 0.00001134
Iteration 113/1000 | Loss: 0.00001134
Iteration 114/1000 | Loss: 0.00001134
Iteration 115/1000 | Loss: 0.00001134
Iteration 116/1000 | Loss: 0.00001134
Iteration 117/1000 | Loss: 0.00001134
Iteration 118/1000 | Loss: 0.00001134
Iteration 119/1000 | Loss: 0.00001134
Iteration 120/1000 | Loss: 0.00001134
Iteration 121/1000 | Loss: 0.00001134
Iteration 122/1000 | Loss: 0.00001134
Iteration 123/1000 | Loss: 0.00001134
Iteration 124/1000 | Loss: 0.00001134
Iteration 125/1000 | Loss: 0.00001134
Iteration 126/1000 | Loss: 0.00001134
Iteration 127/1000 | Loss: 0.00001134
Iteration 128/1000 | Loss: 0.00001133
Iteration 129/1000 | Loss: 0.00001133
Iteration 130/1000 | Loss: 0.00001133
Iteration 131/1000 | Loss: 0.00001133
Iteration 132/1000 | Loss: 0.00001133
Iteration 133/1000 | Loss: 0.00001133
Iteration 134/1000 | Loss: 0.00001133
Iteration 135/1000 | Loss: 0.00001133
Iteration 136/1000 | Loss: 0.00001133
Iteration 137/1000 | Loss: 0.00001133
Iteration 138/1000 | Loss: 0.00001133
Iteration 139/1000 | Loss: 0.00001133
Iteration 140/1000 | Loss: 0.00001133
Iteration 141/1000 | Loss: 0.00001133
Iteration 142/1000 | Loss: 0.00001133
Iteration 143/1000 | Loss: 0.00001133
Iteration 144/1000 | Loss: 0.00001133
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.1332082067383453e-05, 1.1332082067383453e-05, 1.1332082067383453e-05, 1.1332082067383453e-05, 1.1332082067383453e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1332082067383453e-05

Optimization complete. Final v2v error: 2.869501829147339 mm

Highest mean error: 3.6045007705688477 mm for frame 106

Lowest mean error: 2.378833293914795 mm for frame 161

Saving results

Total time: 37.995566606521606
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00876816
Iteration 2/25 | Loss: 0.00125461
Iteration 3/25 | Loss: 0.00114333
Iteration 4/25 | Loss: 0.00112107
Iteration 5/25 | Loss: 0.00111306
Iteration 6/25 | Loss: 0.00111113
Iteration 7/25 | Loss: 0.00111087
Iteration 8/25 | Loss: 0.00111086
Iteration 9/25 | Loss: 0.00111086
Iteration 10/25 | Loss: 0.00111086
Iteration 11/25 | Loss: 0.00111086
Iteration 12/25 | Loss: 0.00111086
Iteration 13/25 | Loss: 0.00111086
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011108616599813104, 0.0011108616599813104, 0.0011108616599813104, 0.0011108616599813104, 0.0011108616599813104]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011108616599813104

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38158834
Iteration 2/25 | Loss: 0.00071950
Iteration 3/25 | Loss: 0.00071950
Iteration 4/25 | Loss: 0.00071950
Iteration 5/25 | Loss: 0.00071950
Iteration 6/25 | Loss: 0.00071950
Iteration 7/25 | Loss: 0.00071950
Iteration 8/25 | Loss: 0.00071950
Iteration 9/25 | Loss: 0.00071950
Iteration 10/25 | Loss: 0.00071950
Iteration 11/25 | Loss: 0.00071950
Iteration 12/25 | Loss: 0.00071950
Iteration 13/25 | Loss: 0.00071950
Iteration 14/25 | Loss: 0.00071950
Iteration 15/25 | Loss: 0.00071950
Iteration 16/25 | Loss: 0.00071950
Iteration 17/25 | Loss: 0.00071950
Iteration 18/25 | Loss: 0.00071950
Iteration 19/25 | Loss: 0.00071950
Iteration 20/25 | Loss: 0.00071950
Iteration 21/25 | Loss: 0.00071950
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007194984937086701, 0.0007194984937086701, 0.0007194984937086701, 0.0007194984937086701, 0.0007194984937086701]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007194984937086701

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071950
Iteration 2/1000 | Loss: 0.00003545
Iteration 3/1000 | Loss: 0.00002518
Iteration 4/1000 | Loss: 0.00002214
Iteration 5/1000 | Loss: 0.00002090
Iteration 6/1000 | Loss: 0.00002007
Iteration 7/1000 | Loss: 0.00001955
Iteration 8/1000 | Loss: 0.00001900
Iteration 9/1000 | Loss: 0.00001856
Iteration 10/1000 | Loss: 0.00001840
Iteration 11/1000 | Loss: 0.00001825
Iteration 12/1000 | Loss: 0.00001812
Iteration 13/1000 | Loss: 0.00001806
Iteration 14/1000 | Loss: 0.00001801
Iteration 15/1000 | Loss: 0.00001800
Iteration 16/1000 | Loss: 0.00001799
Iteration 17/1000 | Loss: 0.00001795
Iteration 18/1000 | Loss: 0.00001794
Iteration 19/1000 | Loss: 0.00001790
Iteration 20/1000 | Loss: 0.00001788
Iteration 21/1000 | Loss: 0.00001787
Iteration 22/1000 | Loss: 0.00001787
Iteration 23/1000 | Loss: 0.00001786
Iteration 24/1000 | Loss: 0.00001786
Iteration 25/1000 | Loss: 0.00001785
Iteration 26/1000 | Loss: 0.00001784
Iteration 27/1000 | Loss: 0.00001784
Iteration 28/1000 | Loss: 0.00001782
Iteration 29/1000 | Loss: 0.00001781
Iteration 30/1000 | Loss: 0.00001780
Iteration 31/1000 | Loss: 0.00001780
Iteration 32/1000 | Loss: 0.00001779
Iteration 33/1000 | Loss: 0.00001778
Iteration 34/1000 | Loss: 0.00001777
Iteration 35/1000 | Loss: 0.00001774
Iteration 36/1000 | Loss: 0.00001773
Iteration 37/1000 | Loss: 0.00001772
Iteration 38/1000 | Loss: 0.00001772
Iteration 39/1000 | Loss: 0.00001772
Iteration 40/1000 | Loss: 0.00001771
Iteration 41/1000 | Loss: 0.00001771
Iteration 42/1000 | Loss: 0.00001770
Iteration 43/1000 | Loss: 0.00001770
Iteration 44/1000 | Loss: 0.00001769
Iteration 45/1000 | Loss: 0.00001769
Iteration 46/1000 | Loss: 0.00001769
Iteration 47/1000 | Loss: 0.00001769
Iteration 48/1000 | Loss: 0.00001769
Iteration 49/1000 | Loss: 0.00001769
Iteration 50/1000 | Loss: 0.00001769
Iteration 51/1000 | Loss: 0.00001769
Iteration 52/1000 | Loss: 0.00001768
Iteration 53/1000 | Loss: 0.00001768
Iteration 54/1000 | Loss: 0.00001768
Iteration 55/1000 | Loss: 0.00001768
Iteration 56/1000 | Loss: 0.00001767
Iteration 57/1000 | Loss: 0.00001767
Iteration 58/1000 | Loss: 0.00001767
Iteration 59/1000 | Loss: 0.00001766
Iteration 60/1000 | Loss: 0.00001766
Iteration 61/1000 | Loss: 0.00001766
Iteration 62/1000 | Loss: 0.00001766
Iteration 63/1000 | Loss: 0.00001766
Iteration 64/1000 | Loss: 0.00001765
Iteration 65/1000 | Loss: 0.00001765
Iteration 66/1000 | Loss: 0.00001765
Iteration 67/1000 | Loss: 0.00001765
Iteration 68/1000 | Loss: 0.00001765
Iteration 69/1000 | Loss: 0.00001764
Iteration 70/1000 | Loss: 0.00001764
Iteration 71/1000 | Loss: 0.00001764
Iteration 72/1000 | Loss: 0.00001764
Iteration 73/1000 | Loss: 0.00001764
Iteration 74/1000 | Loss: 0.00001763
Iteration 75/1000 | Loss: 0.00001763
Iteration 76/1000 | Loss: 0.00001763
Iteration 77/1000 | Loss: 0.00001763
Iteration 78/1000 | Loss: 0.00001762
Iteration 79/1000 | Loss: 0.00001762
Iteration 80/1000 | Loss: 0.00001762
Iteration 81/1000 | Loss: 0.00001762
Iteration 82/1000 | Loss: 0.00001762
Iteration 83/1000 | Loss: 0.00001762
Iteration 84/1000 | Loss: 0.00001762
Iteration 85/1000 | Loss: 0.00001762
Iteration 86/1000 | Loss: 0.00001762
Iteration 87/1000 | Loss: 0.00001761
Iteration 88/1000 | Loss: 0.00001761
Iteration 89/1000 | Loss: 0.00001761
Iteration 90/1000 | Loss: 0.00001761
Iteration 91/1000 | Loss: 0.00001761
Iteration 92/1000 | Loss: 0.00001761
Iteration 93/1000 | Loss: 0.00001761
Iteration 94/1000 | Loss: 0.00001761
Iteration 95/1000 | Loss: 0.00001760
Iteration 96/1000 | Loss: 0.00001760
Iteration 97/1000 | Loss: 0.00001760
Iteration 98/1000 | Loss: 0.00001760
Iteration 99/1000 | Loss: 0.00001760
Iteration 100/1000 | Loss: 0.00001760
Iteration 101/1000 | Loss: 0.00001760
Iteration 102/1000 | Loss: 0.00001760
Iteration 103/1000 | Loss: 0.00001759
Iteration 104/1000 | Loss: 0.00001759
Iteration 105/1000 | Loss: 0.00001759
Iteration 106/1000 | Loss: 0.00001759
Iteration 107/1000 | Loss: 0.00001759
Iteration 108/1000 | Loss: 0.00001759
Iteration 109/1000 | Loss: 0.00001759
Iteration 110/1000 | Loss: 0.00001759
Iteration 111/1000 | Loss: 0.00001759
Iteration 112/1000 | Loss: 0.00001759
Iteration 113/1000 | Loss: 0.00001758
Iteration 114/1000 | Loss: 0.00001758
Iteration 115/1000 | Loss: 0.00001758
Iteration 116/1000 | Loss: 0.00001758
Iteration 117/1000 | Loss: 0.00001758
Iteration 118/1000 | Loss: 0.00001758
Iteration 119/1000 | Loss: 0.00001758
Iteration 120/1000 | Loss: 0.00001758
Iteration 121/1000 | Loss: 0.00001758
Iteration 122/1000 | Loss: 0.00001758
Iteration 123/1000 | Loss: 0.00001757
Iteration 124/1000 | Loss: 0.00001757
Iteration 125/1000 | Loss: 0.00001757
Iteration 126/1000 | Loss: 0.00001757
Iteration 127/1000 | Loss: 0.00001757
Iteration 128/1000 | Loss: 0.00001757
Iteration 129/1000 | Loss: 0.00001757
Iteration 130/1000 | Loss: 0.00001757
Iteration 131/1000 | Loss: 0.00001756
Iteration 132/1000 | Loss: 0.00001756
Iteration 133/1000 | Loss: 0.00001756
Iteration 134/1000 | Loss: 0.00001756
Iteration 135/1000 | Loss: 0.00001756
Iteration 136/1000 | Loss: 0.00001756
Iteration 137/1000 | Loss: 0.00001756
Iteration 138/1000 | Loss: 0.00001756
Iteration 139/1000 | Loss: 0.00001756
Iteration 140/1000 | Loss: 0.00001756
Iteration 141/1000 | Loss: 0.00001755
Iteration 142/1000 | Loss: 0.00001755
Iteration 143/1000 | Loss: 0.00001755
Iteration 144/1000 | Loss: 0.00001755
Iteration 145/1000 | Loss: 0.00001755
Iteration 146/1000 | Loss: 0.00001755
Iteration 147/1000 | Loss: 0.00001755
Iteration 148/1000 | Loss: 0.00001755
Iteration 149/1000 | Loss: 0.00001755
Iteration 150/1000 | Loss: 0.00001754
Iteration 151/1000 | Loss: 0.00001754
Iteration 152/1000 | Loss: 0.00001754
Iteration 153/1000 | Loss: 0.00001754
Iteration 154/1000 | Loss: 0.00001754
Iteration 155/1000 | Loss: 0.00001754
Iteration 156/1000 | Loss: 0.00001754
Iteration 157/1000 | Loss: 0.00001754
Iteration 158/1000 | Loss: 0.00001754
Iteration 159/1000 | Loss: 0.00001754
Iteration 160/1000 | Loss: 0.00001754
Iteration 161/1000 | Loss: 0.00001754
Iteration 162/1000 | Loss: 0.00001754
Iteration 163/1000 | Loss: 0.00001754
Iteration 164/1000 | Loss: 0.00001754
Iteration 165/1000 | Loss: 0.00001753
Iteration 166/1000 | Loss: 0.00001753
Iteration 167/1000 | Loss: 0.00001753
Iteration 168/1000 | Loss: 0.00001753
Iteration 169/1000 | Loss: 0.00001753
Iteration 170/1000 | Loss: 0.00001753
Iteration 171/1000 | Loss: 0.00001753
Iteration 172/1000 | Loss: 0.00001753
Iteration 173/1000 | Loss: 0.00001753
Iteration 174/1000 | Loss: 0.00001753
Iteration 175/1000 | Loss: 0.00001753
Iteration 176/1000 | Loss: 0.00001753
Iteration 177/1000 | Loss: 0.00001753
Iteration 178/1000 | Loss: 0.00001753
Iteration 179/1000 | Loss: 0.00001753
Iteration 180/1000 | Loss: 0.00001753
Iteration 181/1000 | Loss: 0.00001753
Iteration 182/1000 | Loss: 0.00001753
Iteration 183/1000 | Loss: 0.00001753
Iteration 184/1000 | Loss: 0.00001752
Iteration 185/1000 | Loss: 0.00001752
Iteration 186/1000 | Loss: 0.00001752
Iteration 187/1000 | Loss: 0.00001752
Iteration 188/1000 | Loss: 0.00001752
Iteration 189/1000 | Loss: 0.00001752
Iteration 190/1000 | Loss: 0.00001752
Iteration 191/1000 | Loss: 0.00001752
Iteration 192/1000 | Loss: 0.00001752
Iteration 193/1000 | Loss: 0.00001752
Iteration 194/1000 | Loss: 0.00001752
Iteration 195/1000 | Loss: 0.00001752
Iteration 196/1000 | Loss: 0.00001752
Iteration 197/1000 | Loss: 0.00001752
Iteration 198/1000 | Loss: 0.00001752
Iteration 199/1000 | Loss: 0.00001752
Iteration 200/1000 | Loss: 0.00001752
Iteration 201/1000 | Loss: 0.00001752
Iteration 202/1000 | Loss: 0.00001752
Iteration 203/1000 | Loss: 0.00001752
Iteration 204/1000 | Loss: 0.00001752
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [1.7518610547995195e-05, 1.7518610547995195e-05, 1.7518610547995195e-05, 1.7518610547995195e-05, 1.7518610547995195e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7518610547995195e-05

Optimization complete. Final v2v error: 3.4753029346466064 mm

Highest mean error: 5.101316452026367 mm for frame 66

Lowest mean error: 2.8911917209625244 mm for frame 98

Saving results

Total time: 42.42529559135437
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00745877
Iteration 2/25 | Loss: 0.00115658
Iteration 3/25 | Loss: 0.00108779
Iteration 4/25 | Loss: 0.00107455
Iteration 5/25 | Loss: 0.00106949
Iteration 6/25 | Loss: 0.00106854
Iteration 7/25 | Loss: 0.00106854
Iteration 8/25 | Loss: 0.00106854
Iteration 9/25 | Loss: 0.00106854
Iteration 10/25 | Loss: 0.00106854
Iteration 11/25 | Loss: 0.00106854
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010685431770980358, 0.0010685431770980358, 0.0010685431770980358, 0.0010685431770980358, 0.0010685431770980358]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010685431770980358

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27071977
Iteration 2/25 | Loss: 0.00066296
Iteration 3/25 | Loss: 0.00066290
Iteration 4/25 | Loss: 0.00066290
Iteration 5/25 | Loss: 0.00066290
Iteration 6/25 | Loss: 0.00066290
Iteration 7/25 | Loss: 0.00066290
Iteration 8/25 | Loss: 0.00066290
Iteration 9/25 | Loss: 0.00066290
Iteration 10/25 | Loss: 0.00066290
Iteration 11/25 | Loss: 0.00066290
Iteration 12/25 | Loss: 0.00066290
Iteration 13/25 | Loss: 0.00066290
Iteration 14/25 | Loss: 0.00066290
Iteration 15/25 | Loss: 0.00066290
Iteration 16/25 | Loss: 0.00066290
Iteration 17/25 | Loss: 0.00066290
Iteration 18/25 | Loss: 0.00066290
Iteration 19/25 | Loss: 0.00066290
Iteration 20/25 | Loss: 0.00066290
Iteration 21/25 | Loss: 0.00066290
Iteration 22/25 | Loss: 0.00066290
Iteration 23/25 | Loss: 0.00066290
Iteration 24/25 | Loss: 0.00066290
Iteration 25/25 | Loss: 0.00066290

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066290
Iteration 2/1000 | Loss: 0.00002963
Iteration 3/1000 | Loss: 0.00001654
Iteration 4/1000 | Loss: 0.00001474
Iteration 5/1000 | Loss: 0.00001374
Iteration 6/1000 | Loss: 0.00001324
Iteration 7/1000 | Loss: 0.00001282
Iteration 8/1000 | Loss: 0.00001248
Iteration 9/1000 | Loss: 0.00001222
Iteration 10/1000 | Loss: 0.00001203
Iteration 11/1000 | Loss: 0.00001198
Iteration 12/1000 | Loss: 0.00001187
Iteration 13/1000 | Loss: 0.00001186
Iteration 14/1000 | Loss: 0.00001181
Iteration 15/1000 | Loss: 0.00001176
Iteration 16/1000 | Loss: 0.00001170
Iteration 17/1000 | Loss: 0.00001168
Iteration 18/1000 | Loss: 0.00001166
Iteration 19/1000 | Loss: 0.00001160
Iteration 20/1000 | Loss: 0.00001157
Iteration 21/1000 | Loss: 0.00001152
Iteration 22/1000 | Loss: 0.00001151
Iteration 23/1000 | Loss: 0.00001150
Iteration 24/1000 | Loss: 0.00001150
Iteration 25/1000 | Loss: 0.00001149
Iteration 26/1000 | Loss: 0.00001149
Iteration 27/1000 | Loss: 0.00001144
Iteration 28/1000 | Loss: 0.00001143
Iteration 29/1000 | Loss: 0.00001143
Iteration 30/1000 | Loss: 0.00001142
Iteration 31/1000 | Loss: 0.00001139
Iteration 32/1000 | Loss: 0.00001139
Iteration 33/1000 | Loss: 0.00001137
Iteration 34/1000 | Loss: 0.00001137
Iteration 35/1000 | Loss: 0.00001137
Iteration 36/1000 | Loss: 0.00001136
Iteration 37/1000 | Loss: 0.00001136
Iteration 38/1000 | Loss: 0.00001136
Iteration 39/1000 | Loss: 0.00001136
Iteration 40/1000 | Loss: 0.00001136
Iteration 41/1000 | Loss: 0.00001135
Iteration 42/1000 | Loss: 0.00001134
Iteration 43/1000 | Loss: 0.00001134
Iteration 44/1000 | Loss: 0.00001133
Iteration 45/1000 | Loss: 0.00001133
Iteration 46/1000 | Loss: 0.00001133
Iteration 47/1000 | Loss: 0.00001133
Iteration 48/1000 | Loss: 0.00001133
Iteration 49/1000 | Loss: 0.00001132
Iteration 50/1000 | Loss: 0.00001132
Iteration 51/1000 | Loss: 0.00001132
Iteration 52/1000 | Loss: 0.00001131
Iteration 53/1000 | Loss: 0.00001131
Iteration 54/1000 | Loss: 0.00001131
Iteration 55/1000 | Loss: 0.00001131
Iteration 56/1000 | Loss: 0.00001131
Iteration 57/1000 | Loss: 0.00001131
Iteration 58/1000 | Loss: 0.00001131
Iteration 59/1000 | Loss: 0.00001131
Iteration 60/1000 | Loss: 0.00001130
Iteration 61/1000 | Loss: 0.00001130
Iteration 62/1000 | Loss: 0.00001130
Iteration 63/1000 | Loss: 0.00001130
Iteration 64/1000 | Loss: 0.00001130
Iteration 65/1000 | Loss: 0.00001130
Iteration 66/1000 | Loss: 0.00001130
Iteration 67/1000 | Loss: 0.00001130
Iteration 68/1000 | Loss: 0.00001130
Iteration 69/1000 | Loss: 0.00001130
Iteration 70/1000 | Loss: 0.00001129
Iteration 71/1000 | Loss: 0.00001129
Iteration 72/1000 | Loss: 0.00001129
Iteration 73/1000 | Loss: 0.00001129
Iteration 74/1000 | Loss: 0.00001129
Iteration 75/1000 | Loss: 0.00001129
Iteration 76/1000 | Loss: 0.00001129
Iteration 77/1000 | Loss: 0.00001129
Iteration 78/1000 | Loss: 0.00001128
Iteration 79/1000 | Loss: 0.00001128
Iteration 80/1000 | Loss: 0.00001128
Iteration 81/1000 | Loss: 0.00001128
Iteration 82/1000 | Loss: 0.00001128
Iteration 83/1000 | Loss: 0.00001128
Iteration 84/1000 | Loss: 0.00001128
Iteration 85/1000 | Loss: 0.00001127
Iteration 86/1000 | Loss: 0.00001127
Iteration 87/1000 | Loss: 0.00001127
Iteration 88/1000 | Loss: 0.00001127
Iteration 89/1000 | Loss: 0.00001127
Iteration 90/1000 | Loss: 0.00001127
Iteration 91/1000 | Loss: 0.00001126
Iteration 92/1000 | Loss: 0.00001126
Iteration 93/1000 | Loss: 0.00001126
Iteration 94/1000 | Loss: 0.00001126
Iteration 95/1000 | Loss: 0.00001126
Iteration 96/1000 | Loss: 0.00001126
Iteration 97/1000 | Loss: 0.00001125
Iteration 98/1000 | Loss: 0.00001125
Iteration 99/1000 | Loss: 0.00001125
Iteration 100/1000 | Loss: 0.00001125
Iteration 101/1000 | Loss: 0.00001125
Iteration 102/1000 | Loss: 0.00001125
Iteration 103/1000 | Loss: 0.00001125
Iteration 104/1000 | Loss: 0.00001124
Iteration 105/1000 | Loss: 0.00001124
Iteration 106/1000 | Loss: 0.00001124
Iteration 107/1000 | Loss: 0.00001124
Iteration 108/1000 | Loss: 0.00001123
Iteration 109/1000 | Loss: 0.00001123
Iteration 110/1000 | Loss: 0.00001123
Iteration 111/1000 | Loss: 0.00001123
Iteration 112/1000 | Loss: 0.00001123
Iteration 113/1000 | Loss: 0.00001123
Iteration 114/1000 | Loss: 0.00001122
Iteration 115/1000 | Loss: 0.00001122
Iteration 116/1000 | Loss: 0.00001122
Iteration 117/1000 | Loss: 0.00001121
Iteration 118/1000 | Loss: 0.00001121
Iteration 119/1000 | Loss: 0.00001121
Iteration 120/1000 | Loss: 0.00001120
Iteration 121/1000 | Loss: 0.00001120
Iteration 122/1000 | Loss: 0.00001120
Iteration 123/1000 | Loss: 0.00001119
Iteration 124/1000 | Loss: 0.00001119
Iteration 125/1000 | Loss: 0.00001119
Iteration 126/1000 | Loss: 0.00001119
Iteration 127/1000 | Loss: 0.00001118
Iteration 128/1000 | Loss: 0.00001118
Iteration 129/1000 | Loss: 0.00001118
Iteration 130/1000 | Loss: 0.00001118
Iteration 131/1000 | Loss: 0.00001117
Iteration 132/1000 | Loss: 0.00001117
Iteration 133/1000 | Loss: 0.00001117
Iteration 134/1000 | Loss: 0.00001117
Iteration 135/1000 | Loss: 0.00001117
Iteration 136/1000 | Loss: 0.00001117
Iteration 137/1000 | Loss: 0.00001116
Iteration 138/1000 | Loss: 0.00001116
Iteration 139/1000 | Loss: 0.00001116
Iteration 140/1000 | Loss: 0.00001116
Iteration 141/1000 | Loss: 0.00001116
Iteration 142/1000 | Loss: 0.00001116
Iteration 143/1000 | Loss: 0.00001115
Iteration 144/1000 | Loss: 0.00001115
Iteration 145/1000 | Loss: 0.00001115
Iteration 146/1000 | Loss: 0.00001115
Iteration 147/1000 | Loss: 0.00001114
Iteration 148/1000 | Loss: 0.00001114
Iteration 149/1000 | Loss: 0.00001114
Iteration 150/1000 | Loss: 0.00001114
Iteration 151/1000 | Loss: 0.00001114
Iteration 152/1000 | Loss: 0.00001113
Iteration 153/1000 | Loss: 0.00001113
Iteration 154/1000 | Loss: 0.00001113
Iteration 155/1000 | Loss: 0.00001113
Iteration 156/1000 | Loss: 0.00001113
Iteration 157/1000 | Loss: 0.00001113
Iteration 158/1000 | Loss: 0.00001113
Iteration 159/1000 | Loss: 0.00001113
Iteration 160/1000 | Loss: 0.00001113
Iteration 161/1000 | Loss: 0.00001113
Iteration 162/1000 | Loss: 0.00001113
Iteration 163/1000 | Loss: 0.00001113
Iteration 164/1000 | Loss: 0.00001113
Iteration 165/1000 | Loss: 0.00001113
Iteration 166/1000 | Loss: 0.00001113
Iteration 167/1000 | Loss: 0.00001112
Iteration 168/1000 | Loss: 0.00001112
Iteration 169/1000 | Loss: 0.00001112
Iteration 170/1000 | Loss: 0.00001112
Iteration 171/1000 | Loss: 0.00001112
Iteration 172/1000 | Loss: 0.00001112
Iteration 173/1000 | Loss: 0.00001112
Iteration 174/1000 | Loss: 0.00001112
Iteration 175/1000 | Loss: 0.00001112
Iteration 176/1000 | Loss: 0.00001112
Iteration 177/1000 | Loss: 0.00001112
Iteration 178/1000 | Loss: 0.00001112
Iteration 179/1000 | Loss: 0.00001112
Iteration 180/1000 | Loss: 0.00001112
Iteration 181/1000 | Loss: 0.00001111
Iteration 182/1000 | Loss: 0.00001111
Iteration 183/1000 | Loss: 0.00001111
Iteration 184/1000 | Loss: 0.00001111
Iteration 185/1000 | Loss: 0.00001111
Iteration 186/1000 | Loss: 0.00001111
Iteration 187/1000 | Loss: 0.00001111
Iteration 188/1000 | Loss: 0.00001111
Iteration 189/1000 | Loss: 0.00001111
Iteration 190/1000 | Loss: 0.00001111
Iteration 191/1000 | Loss: 0.00001111
Iteration 192/1000 | Loss: 0.00001111
Iteration 193/1000 | Loss: 0.00001111
Iteration 194/1000 | Loss: 0.00001111
Iteration 195/1000 | Loss: 0.00001111
Iteration 196/1000 | Loss: 0.00001111
Iteration 197/1000 | Loss: 0.00001111
Iteration 198/1000 | Loss: 0.00001111
Iteration 199/1000 | Loss: 0.00001111
Iteration 200/1000 | Loss: 0.00001111
Iteration 201/1000 | Loss: 0.00001111
Iteration 202/1000 | Loss: 0.00001111
Iteration 203/1000 | Loss: 0.00001111
Iteration 204/1000 | Loss: 0.00001111
Iteration 205/1000 | Loss: 0.00001111
Iteration 206/1000 | Loss: 0.00001111
Iteration 207/1000 | Loss: 0.00001111
Iteration 208/1000 | Loss: 0.00001111
Iteration 209/1000 | Loss: 0.00001111
Iteration 210/1000 | Loss: 0.00001111
Iteration 211/1000 | Loss: 0.00001111
Iteration 212/1000 | Loss: 0.00001111
Iteration 213/1000 | Loss: 0.00001111
Iteration 214/1000 | Loss: 0.00001111
Iteration 215/1000 | Loss: 0.00001111
Iteration 216/1000 | Loss: 0.00001111
Iteration 217/1000 | Loss: 0.00001111
Iteration 218/1000 | Loss: 0.00001111
Iteration 219/1000 | Loss: 0.00001111
Iteration 220/1000 | Loss: 0.00001111
Iteration 221/1000 | Loss: 0.00001111
Iteration 222/1000 | Loss: 0.00001111
Iteration 223/1000 | Loss: 0.00001111
Iteration 224/1000 | Loss: 0.00001111
Iteration 225/1000 | Loss: 0.00001111
Iteration 226/1000 | Loss: 0.00001111
Iteration 227/1000 | Loss: 0.00001111
Iteration 228/1000 | Loss: 0.00001111
Iteration 229/1000 | Loss: 0.00001111
Iteration 230/1000 | Loss: 0.00001111
Iteration 231/1000 | Loss: 0.00001111
Iteration 232/1000 | Loss: 0.00001111
Iteration 233/1000 | Loss: 0.00001111
Iteration 234/1000 | Loss: 0.00001111
Iteration 235/1000 | Loss: 0.00001111
Iteration 236/1000 | Loss: 0.00001111
Iteration 237/1000 | Loss: 0.00001111
Iteration 238/1000 | Loss: 0.00001111
Iteration 239/1000 | Loss: 0.00001111
Iteration 240/1000 | Loss: 0.00001111
Iteration 241/1000 | Loss: 0.00001111
Iteration 242/1000 | Loss: 0.00001111
Iteration 243/1000 | Loss: 0.00001111
Iteration 244/1000 | Loss: 0.00001111
Iteration 245/1000 | Loss: 0.00001111
Iteration 246/1000 | Loss: 0.00001111
Iteration 247/1000 | Loss: 0.00001111
Iteration 248/1000 | Loss: 0.00001111
Iteration 249/1000 | Loss: 0.00001111
Iteration 250/1000 | Loss: 0.00001111
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 250. Stopping optimization.
Last 5 losses: [1.1109508704976179e-05, 1.1109508704976179e-05, 1.1109508704976179e-05, 1.1109508704976179e-05, 1.1109508704976179e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1109508704976179e-05

Optimization complete. Final v2v error: 2.81730318069458 mm

Highest mean error: 3.4806718826293945 mm for frame 9

Lowest mean error: 2.6340672969818115 mm for frame 49

Saving results

Total time: 50.73788809776306
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01063483
Iteration 2/25 | Loss: 0.00160668
Iteration 3/25 | Loss: 0.00131437
Iteration 4/25 | Loss: 0.00124369
Iteration 5/25 | Loss: 0.00122729
Iteration 6/25 | Loss: 0.00120556
Iteration 7/25 | Loss: 0.00118727
Iteration 8/25 | Loss: 0.00118292
Iteration 9/25 | Loss: 0.00118483
Iteration 10/25 | Loss: 0.00118040
Iteration 11/25 | Loss: 0.00117813
Iteration 12/25 | Loss: 0.00117778
Iteration 13/25 | Loss: 0.00117766
Iteration 14/25 | Loss: 0.00117766
Iteration 15/25 | Loss: 0.00117766
Iteration 16/25 | Loss: 0.00117766
Iteration 17/25 | Loss: 0.00117766
Iteration 18/25 | Loss: 0.00117765
Iteration 19/25 | Loss: 0.00117765
Iteration 20/25 | Loss: 0.00117765
Iteration 21/25 | Loss: 0.00117765
Iteration 22/25 | Loss: 0.00117765
Iteration 23/25 | Loss: 0.00117765
Iteration 24/25 | Loss: 0.00117765
Iteration 25/25 | Loss: 0.00117765

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.07794285
Iteration 2/25 | Loss: 0.00099903
Iteration 3/25 | Loss: 0.00099882
Iteration 4/25 | Loss: 0.00099882
Iteration 5/25 | Loss: 0.00099881
Iteration 6/25 | Loss: 0.00099881
Iteration 7/25 | Loss: 0.00099881
Iteration 8/25 | Loss: 0.00099881
Iteration 9/25 | Loss: 0.00099881
Iteration 10/25 | Loss: 0.00099881
Iteration 11/25 | Loss: 0.00099881
Iteration 12/25 | Loss: 0.00099881
Iteration 13/25 | Loss: 0.00099881
Iteration 14/25 | Loss: 0.00099881
Iteration 15/25 | Loss: 0.00099881
Iteration 16/25 | Loss: 0.00099881
Iteration 17/25 | Loss: 0.00099881
Iteration 18/25 | Loss: 0.00099881
Iteration 19/25 | Loss: 0.00099881
Iteration 20/25 | Loss: 0.00099881
Iteration 21/25 | Loss: 0.00099881
Iteration 22/25 | Loss: 0.00099881
Iteration 23/25 | Loss: 0.00099881
Iteration 24/25 | Loss: 0.00099881
Iteration 25/25 | Loss: 0.00099881

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099881
Iteration 2/1000 | Loss: 0.00009807
Iteration 3/1000 | Loss: 0.00006008
Iteration 4/1000 | Loss: 0.00004785
Iteration 5/1000 | Loss: 0.00004352
Iteration 6/1000 | Loss: 0.00004151
Iteration 7/1000 | Loss: 0.00003982
Iteration 8/1000 | Loss: 0.00003875
Iteration 9/1000 | Loss: 0.00003803
Iteration 10/1000 | Loss: 0.00003743
Iteration 11/1000 | Loss: 0.00003691
Iteration 12/1000 | Loss: 0.00003655
Iteration 13/1000 | Loss: 0.00003625
Iteration 14/1000 | Loss: 0.00003597
Iteration 15/1000 | Loss: 0.00003574
Iteration 16/1000 | Loss: 0.00003560
Iteration 17/1000 | Loss: 0.00003543
Iteration 18/1000 | Loss: 0.00003531
Iteration 19/1000 | Loss: 0.00003522
Iteration 20/1000 | Loss: 0.00003514
Iteration 21/1000 | Loss: 0.00003506
Iteration 22/1000 | Loss: 0.00003499
Iteration 23/1000 | Loss: 0.00003494
Iteration 24/1000 | Loss: 0.00003494
Iteration 25/1000 | Loss: 0.00003492
Iteration 26/1000 | Loss: 0.00003491
Iteration 27/1000 | Loss: 0.00003491
Iteration 28/1000 | Loss: 0.00003490
Iteration 29/1000 | Loss: 0.00003490
Iteration 30/1000 | Loss: 0.00003490
Iteration 31/1000 | Loss: 0.00003489
Iteration 32/1000 | Loss: 0.00003489
Iteration 33/1000 | Loss: 0.00003486
Iteration 34/1000 | Loss: 0.00003486
Iteration 35/1000 | Loss: 0.00003485
Iteration 36/1000 | Loss: 0.00003482
Iteration 37/1000 | Loss: 0.00003482
Iteration 38/1000 | Loss: 0.00003482
Iteration 39/1000 | Loss: 0.00003481
Iteration 40/1000 | Loss: 0.00003481
Iteration 41/1000 | Loss: 0.00003480
Iteration 42/1000 | Loss: 0.00003480
Iteration 43/1000 | Loss: 0.00003480
Iteration 44/1000 | Loss: 0.00003479
Iteration 45/1000 | Loss: 0.00003479
Iteration 46/1000 | Loss: 0.00003479
Iteration 47/1000 | Loss: 0.00003478
Iteration 48/1000 | Loss: 0.00003478
Iteration 49/1000 | Loss: 0.00003478
Iteration 50/1000 | Loss: 0.00003477
Iteration 51/1000 | Loss: 0.00003477
Iteration 52/1000 | Loss: 0.00003477
Iteration 53/1000 | Loss: 0.00003476
Iteration 54/1000 | Loss: 0.00003476
Iteration 55/1000 | Loss: 0.00003475
Iteration 56/1000 | Loss: 0.00003474
Iteration 57/1000 | Loss: 0.00003474
Iteration 58/1000 | Loss: 0.00003474
Iteration 59/1000 | Loss: 0.00003473
Iteration 60/1000 | Loss: 0.00003473
Iteration 61/1000 | Loss: 0.00003473
Iteration 62/1000 | Loss: 0.00003472
Iteration 63/1000 | Loss: 0.00003472
Iteration 64/1000 | Loss: 0.00003472
Iteration 65/1000 | Loss: 0.00003471
Iteration 66/1000 | Loss: 0.00003471
Iteration 67/1000 | Loss: 0.00003471
Iteration 68/1000 | Loss: 0.00003470
Iteration 69/1000 | Loss: 0.00003470
Iteration 70/1000 | Loss: 0.00003470
Iteration 71/1000 | Loss: 0.00003470
Iteration 72/1000 | Loss: 0.00003470
Iteration 73/1000 | Loss: 0.00003469
Iteration 74/1000 | Loss: 0.00003469
Iteration 75/1000 | Loss: 0.00003469
Iteration 76/1000 | Loss: 0.00003468
Iteration 77/1000 | Loss: 0.00003468
Iteration 78/1000 | Loss: 0.00003468
Iteration 79/1000 | Loss: 0.00003467
Iteration 80/1000 | Loss: 0.00003467
Iteration 81/1000 | Loss: 0.00003467
Iteration 82/1000 | Loss: 0.00003467
Iteration 83/1000 | Loss: 0.00003467
Iteration 84/1000 | Loss: 0.00003467
Iteration 85/1000 | Loss: 0.00003466
Iteration 86/1000 | Loss: 0.00003466
Iteration 87/1000 | Loss: 0.00003466
Iteration 88/1000 | Loss: 0.00003466
Iteration 89/1000 | Loss: 0.00003465
Iteration 90/1000 | Loss: 0.00003465
Iteration 91/1000 | Loss: 0.00003465
Iteration 92/1000 | Loss: 0.00003464
Iteration 93/1000 | Loss: 0.00003464
Iteration 94/1000 | Loss: 0.00003464
Iteration 95/1000 | Loss: 0.00003463
Iteration 96/1000 | Loss: 0.00003463
Iteration 97/1000 | Loss: 0.00003463
Iteration 98/1000 | Loss: 0.00003463
Iteration 99/1000 | Loss: 0.00003462
Iteration 100/1000 | Loss: 0.00003462
Iteration 101/1000 | Loss: 0.00003462
Iteration 102/1000 | Loss: 0.00003461
Iteration 103/1000 | Loss: 0.00003461
Iteration 104/1000 | Loss: 0.00003461
Iteration 105/1000 | Loss: 0.00003461
Iteration 106/1000 | Loss: 0.00003461
Iteration 107/1000 | Loss: 0.00003460
Iteration 108/1000 | Loss: 0.00003460
Iteration 109/1000 | Loss: 0.00003460
Iteration 110/1000 | Loss: 0.00003459
Iteration 111/1000 | Loss: 0.00003459
Iteration 112/1000 | Loss: 0.00003459
Iteration 113/1000 | Loss: 0.00003459
Iteration 114/1000 | Loss: 0.00003459
Iteration 115/1000 | Loss: 0.00003458
Iteration 116/1000 | Loss: 0.00003458
Iteration 117/1000 | Loss: 0.00003458
Iteration 118/1000 | Loss: 0.00003458
Iteration 119/1000 | Loss: 0.00003457
Iteration 120/1000 | Loss: 0.00003457
Iteration 121/1000 | Loss: 0.00003457
Iteration 122/1000 | Loss: 0.00003457
Iteration 123/1000 | Loss: 0.00003456
Iteration 124/1000 | Loss: 0.00003456
Iteration 125/1000 | Loss: 0.00003456
Iteration 126/1000 | Loss: 0.00003456
Iteration 127/1000 | Loss: 0.00003456
Iteration 128/1000 | Loss: 0.00003456
Iteration 129/1000 | Loss: 0.00003455
Iteration 130/1000 | Loss: 0.00003455
Iteration 131/1000 | Loss: 0.00003455
Iteration 132/1000 | Loss: 0.00003455
Iteration 133/1000 | Loss: 0.00003455
Iteration 134/1000 | Loss: 0.00003455
Iteration 135/1000 | Loss: 0.00003454
Iteration 136/1000 | Loss: 0.00003454
Iteration 137/1000 | Loss: 0.00003454
Iteration 138/1000 | Loss: 0.00003454
Iteration 139/1000 | Loss: 0.00003454
Iteration 140/1000 | Loss: 0.00003454
Iteration 141/1000 | Loss: 0.00003454
Iteration 142/1000 | Loss: 0.00003453
Iteration 143/1000 | Loss: 0.00003453
Iteration 144/1000 | Loss: 0.00003453
Iteration 145/1000 | Loss: 0.00003453
Iteration 146/1000 | Loss: 0.00003453
Iteration 147/1000 | Loss: 0.00003452
Iteration 148/1000 | Loss: 0.00003452
Iteration 149/1000 | Loss: 0.00003452
Iteration 150/1000 | Loss: 0.00003452
Iteration 151/1000 | Loss: 0.00003452
Iteration 152/1000 | Loss: 0.00003452
Iteration 153/1000 | Loss: 0.00003452
Iteration 154/1000 | Loss: 0.00003452
Iteration 155/1000 | Loss: 0.00003452
Iteration 156/1000 | Loss: 0.00003452
Iteration 157/1000 | Loss: 0.00003452
Iteration 158/1000 | Loss: 0.00003452
Iteration 159/1000 | Loss: 0.00003452
Iteration 160/1000 | Loss: 0.00003452
Iteration 161/1000 | Loss: 0.00003452
Iteration 162/1000 | Loss: 0.00003452
Iteration 163/1000 | Loss: 0.00003451
Iteration 164/1000 | Loss: 0.00003451
Iteration 165/1000 | Loss: 0.00003451
Iteration 166/1000 | Loss: 0.00003451
Iteration 167/1000 | Loss: 0.00003451
Iteration 168/1000 | Loss: 0.00003451
Iteration 169/1000 | Loss: 0.00003451
Iteration 170/1000 | Loss: 0.00003451
Iteration 171/1000 | Loss: 0.00003451
Iteration 172/1000 | Loss: 0.00003451
Iteration 173/1000 | Loss: 0.00003451
Iteration 174/1000 | Loss: 0.00003451
Iteration 175/1000 | Loss: 0.00003451
Iteration 176/1000 | Loss: 0.00003450
Iteration 177/1000 | Loss: 0.00003450
Iteration 178/1000 | Loss: 0.00003450
Iteration 179/1000 | Loss: 0.00003450
Iteration 180/1000 | Loss: 0.00003450
Iteration 181/1000 | Loss: 0.00003450
Iteration 182/1000 | Loss: 0.00003450
Iteration 183/1000 | Loss: 0.00003450
Iteration 184/1000 | Loss: 0.00003450
Iteration 185/1000 | Loss: 0.00003450
Iteration 186/1000 | Loss: 0.00003450
Iteration 187/1000 | Loss: 0.00003450
Iteration 188/1000 | Loss: 0.00003450
Iteration 189/1000 | Loss: 0.00003450
Iteration 190/1000 | Loss: 0.00003450
Iteration 191/1000 | Loss: 0.00003449
Iteration 192/1000 | Loss: 0.00003449
Iteration 193/1000 | Loss: 0.00003449
Iteration 194/1000 | Loss: 0.00003449
Iteration 195/1000 | Loss: 0.00003449
Iteration 196/1000 | Loss: 0.00003449
Iteration 197/1000 | Loss: 0.00003449
Iteration 198/1000 | Loss: 0.00003449
Iteration 199/1000 | Loss: 0.00003449
Iteration 200/1000 | Loss: 0.00003449
Iteration 201/1000 | Loss: 0.00003449
Iteration 202/1000 | Loss: 0.00003449
Iteration 203/1000 | Loss: 0.00003449
Iteration 204/1000 | Loss: 0.00003449
Iteration 205/1000 | Loss: 0.00003449
Iteration 206/1000 | Loss: 0.00003449
Iteration 207/1000 | Loss: 0.00003449
Iteration 208/1000 | Loss: 0.00003449
Iteration 209/1000 | Loss: 0.00003449
Iteration 210/1000 | Loss: 0.00003449
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [3.448669667704962e-05, 3.448669667704962e-05, 3.448669667704962e-05, 3.448669667704962e-05, 3.448669667704962e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.448669667704962e-05

Optimization complete. Final v2v error: 4.654781818389893 mm

Highest mean error: 7.283727645874023 mm for frame 99

Lowest mean error: 3.127067804336548 mm for frame 139

Saving results

Total time: 70.78402805328369
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00955917
Iteration 2/25 | Loss: 0.00229247
Iteration 3/25 | Loss: 0.00200870
Iteration 4/25 | Loss: 0.00196083
Iteration 5/25 | Loss: 0.00193846
Iteration 6/25 | Loss: 0.00191602
Iteration 7/25 | Loss: 0.00190010
Iteration 8/25 | Loss: 0.00189253
Iteration 9/25 | Loss: 0.00189167
Iteration 10/25 | Loss: 0.00189167
Iteration 11/25 | Loss: 0.00189167
Iteration 12/25 | Loss: 0.00189167
Iteration 13/25 | Loss: 0.00189167
Iteration 14/25 | Loss: 0.00189167
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0018916669068858027, 0.0018916669068858027, 0.0018916669068858027, 0.0018916669068858027, 0.0018916669068858027]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018916669068858027

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33523190
Iteration 2/25 | Loss: 0.00638773
Iteration 3/25 | Loss: 0.00638773
Iteration 4/25 | Loss: 0.00638773
Iteration 5/25 | Loss: 0.00638772
Iteration 6/25 | Loss: 0.00638772
Iteration 7/25 | Loss: 0.00638772
Iteration 8/25 | Loss: 0.00638772
Iteration 9/25 | Loss: 0.00638772
Iteration 10/25 | Loss: 0.00638772
Iteration 11/25 | Loss: 0.00638772
Iteration 12/25 | Loss: 0.00638772
Iteration 13/25 | Loss: 0.00638772
Iteration 14/25 | Loss: 0.00638772
Iteration 15/25 | Loss: 0.00638772
Iteration 16/25 | Loss: 0.00638772
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0063877226784825325, 0.0063877226784825325, 0.0063877226784825325, 0.0063877226784825325, 0.0063877226784825325]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0063877226784825325

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00638772
Iteration 2/1000 | Loss: 0.00226883
Iteration 3/1000 | Loss: 0.00374382
Iteration 4/1000 | Loss: 0.00095715
Iteration 5/1000 | Loss: 0.00124621
Iteration 6/1000 | Loss: 0.00103504
Iteration 7/1000 | Loss: 0.00081861
Iteration 8/1000 | Loss: 0.00069892
Iteration 9/1000 | Loss: 0.00057190
Iteration 10/1000 | Loss: 0.00055377
Iteration 11/1000 | Loss: 0.00051342
Iteration 12/1000 | Loss: 0.00080868
Iteration 13/1000 | Loss: 0.00052197
Iteration 14/1000 | Loss: 0.00062814
Iteration 15/1000 | Loss: 0.00034288
Iteration 16/1000 | Loss: 0.00028160
Iteration 17/1000 | Loss: 0.00041865
Iteration 18/1000 | Loss: 0.00043330
Iteration 19/1000 | Loss: 0.00024157
Iteration 20/1000 | Loss: 0.00051650
Iteration 21/1000 | Loss: 0.00665405
Iteration 22/1000 | Loss: 0.01692279
Iteration 23/1000 | Loss: 0.00424172
Iteration 24/1000 | Loss: 0.00187848
Iteration 25/1000 | Loss: 0.00166639
Iteration 26/1000 | Loss: 0.00113325
Iteration 27/1000 | Loss: 0.00114471
Iteration 28/1000 | Loss: 0.00043968
Iteration 29/1000 | Loss: 0.00064777
Iteration 30/1000 | Loss: 0.00027393
Iteration 31/1000 | Loss: 0.00033138
Iteration 32/1000 | Loss: 0.00046428
Iteration 33/1000 | Loss: 0.00024669
Iteration 34/1000 | Loss: 0.00014783
Iteration 35/1000 | Loss: 0.00028646
Iteration 36/1000 | Loss: 0.00012466
Iteration 37/1000 | Loss: 0.00007223
Iteration 38/1000 | Loss: 0.00006741
Iteration 39/1000 | Loss: 0.00035306
Iteration 40/1000 | Loss: 0.00032336
Iteration 41/1000 | Loss: 0.00142173
Iteration 42/1000 | Loss: 0.00006063
Iteration 43/1000 | Loss: 0.00006366
Iteration 44/1000 | Loss: 0.00006063
Iteration 45/1000 | Loss: 0.00005706
Iteration 46/1000 | Loss: 0.00004247
Iteration 47/1000 | Loss: 0.00034501
Iteration 48/1000 | Loss: 0.00081457
Iteration 49/1000 | Loss: 0.00037616
Iteration 50/1000 | Loss: 0.00021419
Iteration 51/1000 | Loss: 0.00006173
Iteration 52/1000 | Loss: 0.00016196
Iteration 53/1000 | Loss: 0.00017675
Iteration 54/1000 | Loss: 0.00018922
Iteration 55/1000 | Loss: 0.00020993
Iteration 56/1000 | Loss: 0.00003888
Iteration 57/1000 | Loss: 0.00011302
Iteration 58/1000 | Loss: 0.00013514
Iteration 59/1000 | Loss: 0.00010970
Iteration 60/1000 | Loss: 0.00018789
Iteration 61/1000 | Loss: 0.00014517
Iteration 62/1000 | Loss: 0.00005343
Iteration 63/1000 | Loss: 0.00010267
Iteration 64/1000 | Loss: 0.00008790
Iteration 65/1000 | Loss: 0.00006569
Iteration 66/1000 | Loss: 0.00020401
Iteration 67/1000 | Loss: 0.00006178
Iteration 68/1000 | Loss: 0.00007415
Iteration 69/1000 | Loss: 0.00004350
Iteration 70/1000 | Loss: 0.00013228
Iteration 71/1000 | Loss: 0.00004549
Iteration 72/1000 | Loss: 0.00031465
Iteration 73/1000 | Loss: 0.00013834
Iteration 74/1000 | Loss: 0.00006185
Iteration 75/1000 | Loss: 0.00011039
Iteration 76/1000 | Loss: 0.00004817
Iteration 77/1000 | Loss: 0.00009091
Iteration 78/1000 | Loss: 0.00009290
Iteration 79/1000 | Loss: 0.00007631
Iteration 80/1000 | Loss: 0.00011008
Iteration 81/1000 | Loss: 0.00007971
Iteration 82/1000 | Loss: 0.00006459
Iteration 83/1000 | Loss: 0.00007345
Iteration 84/1000 | Loss: 0.00007571
Iteration 85/1000 | Loss: 0.00051128
Iteration 86/1000 | Loss: 0.00031749
Iteration 87/1000 | Loss: 0.00011347
Iteration 88/1000 | Loss: 0.00016134
Iteration 89/1000 | Loss: 0.00004038
Iteration 90/1000 | Loss: 0.00002838
Iteration 91/1000 | Loss: 0.00013254
Iteration 92/1000 | Loss: 0.00002545
Iteration 93/1000 | Loss: 0.00002266
Iteration 94/1000 | Loss: 0.00002179
Iteration 95/1000 | Loss: 0.00002120
Iteration 96/1000 | Loss: 0.00015373
Iteration 97/1000 | Loss: 0.00006829
Iteration 98/1000 | Loss: 0.00013041
Iteration 99/1000 | Loss: 0.00006495
Iteration 100/1000 | Loss: 0.00012923
Iteration 101/1000 | Loss: 0.00011816
Iteration 102/1000 | Loss: 0.00016498
Iteration 103/1000 | Loss: 0.00013661
Iteration 104/1000 | Loss: 0.00013666
Iteration 105/1000 | Loss: 0.00012499
Iteration 106/1000 | Loss: 0.00002749
Iteration 107/1000 | Loss: 0.00002386
Iteration 108/1000 | Loss: 0.00027213
Iteration 109/1000 | Loss: 0.00002045
Iteration 110/1000 | Loss: 0.00001876
Iteration 111/1000 | Loss: 0.00001790
Iteration 112/1000 | Loss: 0.00001718
Iteration 113/1000 | Loss: 0.00003299
Iteration 114/1000 | Loss: 0.00002943
Iteration 115/1000 | Loss: 0.00002110
Iteration 116/1000 | Loss: 0.00019054
Iteration 117/1000 | Loss: 0.00010055
Iteration 118/1000 | Loss: 0.00004722
Iteration 119/1000 | Loss: 0.00003812
Iteration 120/1000 | Loss: 0.00003527
Iteration 121/1000 | Loss: 0.00002159
Iteration 122/1000 | Loss: 0.00013059
Iteration 123/1000 | Loss: 0.00012947
Iteration 124/1000 | Loss: 0.00002486
Iteration 125/1000 | Loss: 0.00005124
Iteration 126/1000 | Loss: 0.00002759
Iteration 127/1000 | Loss: 0.00003520
Iteration 128/1000 | Loss: 0.00002155
Iteration 129/1000 | Loss: 0.00001850
Iteration 130/1000 | Loss: 0.00001695
Iteration 131/1000 | Loss: 0.00001823
Iteration 132/1000 | Loss: 0.00001592
Iteration 133/1000 | Loss: 0.00001525
Iteration 134/1000 | Loss: 0.00001484
Iteration 135/1000 | Loss: 0.00001434
Iteration 136/1000 | Loss: 0.00001389
Iteration 137/1000 | Loss: 0.00001351
Iteration 138/1000 | Loss: 0.00001332
Iteration 139/1000 | Loss: 0.00003010
Iteration 140/1000 | Loss: 0.00001627
Iteration 141/1000 | Loss: 0.00002341
Iteration 142/1000 | Loss: 0.00001539
Iteration 143/1000 | Loss: 0.00001443
Iteration 144/1000 | Loss: 0.00002707
Iteration 145/1000 | Loss: 0.00001806
Iteration 146/1000 | Loss: 0.00001631
Iteration 147/1000 | Loss: 0.00001508
Iteration 148/1000 | Loss: 0.00001736
Iteration 149/1000 | Loss: 0.00002540
Iteration 150/1000 | Loss: 0.00001814
Iteration 151/1000 | Loss: 0.00002875
Iteration 152/1000 | Loss: 0.00002004
Iteration 153/1000 | Loss: 0.00002906
Iteration 154/1000 | Loss: 0.00002238
Iteration 155/1000 | Loss: 0.00002856
Iteration 156/1000 | Loss: 0.00003622
Iteration 157/1000 | Loss: 0.00003116
Iteration 158/1000 | Loss: 0.00001407
Iteration 159/1000 | Loss: 0.00001318
Iteration 160/1000 | Loss: 0.00001265
Iteration 161/1000 | Loss: 0.00001237
Iteration 162/1000 | Loss: 0.00001235
Iteration 163/1000 | Loss: 0.00001233
Iteration 164/1000 | Loss: 0.00001231
Iteration 165/1000 | Loss: 0.00001231
Iteration 166/1000 | Loss: 0.00001227
Iteration 167/1000 | Loss: 0.00001221
Iteration 168/1000 | Loss: 0.00001221
Iteration 169/1000 | Loss: 0.00001221
Iteration 170/1000 | Loss: 0.00001220
Iteration 171/1000 | Loss: 0.00001220
Iteration 172/1000 | Loss: 0.00001218
Iteration 173/1000 | Loss: 0.00001217
Iteration 174/1000 | Loss: 0.00001217
Iteration 175/1000 | Loss: 0.00001214
Iteration 176/1000 | Loss: 0.00001214
Iteration 177/1000 | Loss: 0.00001213
Iteration 178/1000 | Loss: 0.00001205
Iteration 179/1000 | Loss: 0.00001204
Iteration 180/1000 | Loss: 0.00001198
Iteration 181/1000 | Loss: 0.00001198
Iteration 182/1000 | Loss: 0.00001194
Iteration 183/1000 | Loss: 0.00001194
Iteration 184/1000 | Loss: 0.00001194
Iteration 185/1000 | Loss: 0.00001194
Iteration 186/1000 | Loss: 0.00001194
Iteration 187/1000 | Loss: 0.00001194
Iteration 188/1000 | Loss: 0.00001194
Iteration 189/1000 | Loss: 0.00001194
Iteration 190/1000 | Loss: 0.00001194
Iteration 191/1000 | Loss: 0.00001193
Iteration 192/1000 | Loss: 0.00001193
Iteration 193/1000 | Loss: 0.00001193
Iteration 194/1000 | Loss: 0.00001193
Iteration 195/1000 | Loss: 0.00001193
Iteration 196/1000 | Loss: 0.00001193
Iteration 197/1000 | Loss: 0.00001192
Iteration 198/1000 | Loss: 0.00001192
Iteration 199/1000 | Loss: 0.00001192
Iteration 200/1000 | Loss: 0.00001192
Iteration 201/1000 | Loss: 0.00001191
Iteration 202/1000 | Loss: 0.00001191
Iteration 203/1000 | Loss: 0.00001191
Iteration 204/1000 | Loss: 0.00001190
Iteration 205/1000 | Loss: 0.00001189
Iteration 206/1000 | Loss: 0.00001189
Iteration 207/1000 | Loss: 0.00001189
Iteration 208/1000 | Loss: 0.00001188
Iteration 209/1000 | Loss: 0.00001188
Iteration 210/1000 | Loss: 0.00001188
Iteration 211/1000 | Loss: 0.00001188
Iteration 212/1000 | Loss: 0.00001187
Iteration 213/1000 | Loss: 0.00001187
Iteration 214/1000 | Loss: 0.00001187
Iteration 215/1000 | Loss: 0.00001186
Iteration 216/1000 | Loss: 0.00001186
Iteration 217/1000 | Loss: 0.00001183
Iteration 218/1000 | Loss: 0.00001178
Iteration 219/1000 | Loss: 0.00001177
Iteration 220/1000 | Loss: 0.00001175
Iteration 221/1000 | Loss: 0.00001173
Iteration 222/1000 | Loss: 0.00001173
Iteration 223/1000 | Loss: 0.00001172
Iteration 224/1000 | Loss: 0.00001172
Iteration 225/1000 | Loss: 0.00001171
Iteration 226/1000 | Loss: 0.00001167
Iteration 227/1000 | Loss: 0.00001161
Iteration 228/1000 | Loss: 0.00001161
Iteration 229/1000 | Loss: 0.00001160
Iteration 230/1000 | Loss: 0.00001159
Iteration 231/1000 | Loss: 0.00001159
Iteration 232/1000 | Loss: 0.00001159
Iteration 233/1000 | Loss: 0.00001158
Iteration 234/1000 | Loss: 0.00001158
Iteration 235/1000 | Loss: 0.00001157
Iteration 236/1000 | Loss: 0.00001157
Iteration 237/1000 | Loss: 0.00001157
Iteration 238/1000 | Loss: 0.00001157
Iteration 239/1000 | Loss: 0.00001156
Iteration 240/1000 | Loss: 0.00001156
Iteration 241/1000 | Loss: 0.00001155
Iteration 242/1000 | Loss: 0.00001155
Iteration 243/1000 | Loss: 0.00001155
Iteration 244/1000 | Loss: 0.00001154
Iteration 245/1000 | Loss: 0.00001154
Iteration 246/1000 | Loss: 0.00001154
Iteration 247/1000 | Loss: 0.00001154
Iteration 248/1000 | Loss: 0.00001153
Iteration 249/1000 | Loss: 0.00001153
Iteration 250/1000 | Loss: 0.00001153
Iteration 251/1000 | Loss: 0.00001153
Iteration 252/1000 | Loss: 0.00001153
Iteration 253/1000 | Loss: 0.00001153
Iteration 254/1000 | Loss: 0.00001153
Iteration 255/1000 | Loss: 0.00001153
Iteration 256/1000 | Loss: 0.00001153
Iteration 257/1000 | Loss: 0.00001153
Iteration 258/1000 | Loss: 0.00001153
Iteration 259/1000 | Loss: 0.00001153
Iteration 260/1000 | Loss: 0.00001153
Iteration 261/1000 | Loss: 0.00001153
Iteration 262/1000 | Loss: 0.00001153
Iteration 263/1000 | Loss: 0.00001153
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 263. Stopping optimization.
Last 5 losses: [1.1528226423251908e-05, 1.1528226423251908e-05, 1.1528226423251908e-05, 1.1528226423251908e-05, 1.1528226423251908e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1528226423251908e-05

Optimization complete. Final v2v error: 2.8951292037963867 mm

Highest mean error: 4.404029369354248 mm for frame 159

Lowest mean error: 2.7282426357269287 mm for frame 109

Saving results

Total time: 291.584805727005
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00460683
Iteration 2/25 | Loss: 0.00137470
Iteration 3/25 | Loss: 0.00116422
Iteration 4/25 | Loss: 0.00114695
Iteration 5/25 | Loss: 0.00114495
Iteration 6/25 | Loss: 0.00114485
Iteration 7/25 | Loss: 0.00114485
Iteration 8/25 | Loss: 0.00114485
Iteration 9/25 | Loss: 0.00114485
Iteration 10/25 | Loss: 0.00114485
Iteration 11/25 | Loss: 0.00114485
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001144848414696753, 0.001144848414696753, 0.001144848414696753, 0.001144848414696753, 0.001144848414696753]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001144848414696753

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38213372
Iteration 2/25 | Loss: 0.00076176
Iteration 3/25 | Loss: 0.00076174
Iteration 4/25 | Loss: 0.00076174
Iteration 5/25 | Loss: 0.00076174
Iteration 6/25 | Loss: 0.00076174
Iteration 7/25 | Loss: 0.00076174
Iteration 8/25 | Loss: 0.00076174
Iteration 9/25 | Loss: 0.00076174
Iteration 10/25 | Loss: 0.00076174
Iteration 11/25 | Loss: 0.00076174
Iteration 12/25 | Loss: 0.00076174
Iteration 13/25 | Loss: 0.00076174
Iteration 14/25 | Loss: 0.00076174
Iteration 15/25 | Loss: 0.00076174
Iteration 16/25 | Loss: 0.00076174
Iteration 17/25 | Loss: 0.00076174
Iteration 18/25 | Loss: 0.00076174
Iteration 19/25 | Loss: 0.00076174
Iteration 20/25 | Loss: 0.00076174
Iteration 21/25 | Loss: 0.00076174
Iteration 22/25 | Loss: 0.00076174
Iteration 23/25 | Loss: 0.00076174
Iteration 24/25 | Loss: 0.00076174
Iteration 25/25 | Loss: 0.00076174

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076174
Iteration 2/1000 | Loss: 0.00002306
Iteration 3/1000 | Loss: 0.00001612
Iteration 4/1000 | Loss: 0.00001472
Iteration 5/1000 | Loss: 0.00001414
Iteration 6/1000 | Loss: 0.00001388
Iteration 7/1000 | Loss: 0.00001355
Iteration 8/1000 | Loss: 0.00001329
Iteration 9/1000 | Loss: 0.00001311
Iteration 10/1000 | Loss: 0.00001293
Iteration 11/1000 | Loss: 0.00001286
Iteration 12/1000 | Loss: 0.00001286
Iteration 13/1000 | Loss: 0.00001286
Iteration 14/1000 | Loss: 0.00001286
Iteration 15/1000 | Loss: 0.00001285
Iteration 16/1000 | Loss: 0.00001285
Iteration 17/1000 | Loss: 0.00001284
Iteration 18/1000 | Loss: 0.00001284
Iteration 19/1000 | Loss: 0.00001283
Iteration 20/1000 | Loss: 0.00001281
Iteration 21/1000 | Loss: 0.00001281
Iteration 22/1000 | Loss: 0.00001280
Iteration 23/1000 | Loss: 0.00001280
Iteration 24/1000 | Loss: 0.00001280
Iteration 25/1000 | Loss: 0.00001278
Iteration 26/1000 | Loss: 0.00001278
Iteration 27/1000 | Loss: 0.00001278
Iteration 28/1000 | Loss: 0.00001278
Iteration 29/1000 | Loss: 0.00001278
Iteration 30/1000 | Loss: 0.00001278
Iteration 31/1000 | Loss: 0.00001278
Iteration 32/1000 | Loss: 0.00001277
Iteration 33/1000 | Loss: 0.00001277
Iteration 34/1000 | Loss: 0.00001276
Iteration 35/1000 | Loss: 0.00001276
Iteration 36/1000 | Loss: 0.00001276
Iteration 37/1000 | Loss: 0.00001275
Iteration 38/1000 | Loss: 0.00001275
Iteration 39/1000 | Loss: 0.00001274
Iteration 40/1000 | Loss: 0.00001274
Iteration 41/1000 | Loss: 0.00001273
Iteration 42/1000 | Loss: 0.00001273
Iteration 43/1000 | Loss: 0.00001270
Iteration 44/1000 | Loss: 0.00001270
Iteration 45/1000 | Loss: 0.00001270
Iteration 46/1000 | Loss: 0.00001269
Iteration 47/1000 | Loss: 0.00001269
Iteration 48/1000 | Loss: 0.00001267
Iteration 49/1000 | Loss: 0.00001267
Iteration 50/1000 | Loss: 0.00001266
Iteration 51/1000 | Loss: 0.00001265
Iteration 52/1000 | Loss: 0.00001265
Iteration 53/1000 | Loss: 0.00001265
Iteration 54/1000 | Loss: 0.00001264
Iteration 55/1000 | Loss: 0.00001264
Iteration 56/1000 | Loss: 0.00001264
Iteration 57/1000 | Loss: 0.00001264
Iteration 58/1000 | Loss: 0.00001263
Iteration 59/1000 | Loss: 0.00001263
Iteration 60/1000 | Loss: 0.00001262
Iteration 61/1000 | Loss: 0.00001262
Iteration 62/1000 | Loss: 0.00001262
Iteration 63/1000 | Loss: 0.00001261
Iteration 64/1000 | Loss: 0.00001261
Iteration 65/1000 | Loss: 0.00001261
Iteration 66/1000 | Loss: 0.00001261
Iteration 67/1000 | Loss: 0.00001260
Iteration 68/1000 | Loss: 0.00001259
Iteration 69/1000 | Loss: 0.00001259
Iteration 70/1000 | Loss: 0.00001259
Iteration 71/1000 | Loss: 0.00001259
Iteration 72/1000 | Loss: 0.00001259
Iteration 73/1000 | Loss: 0.00001258
Iteration 74/1000 | Loss: 0.00001258
Iteration 75/1000 | Loss: 0.00001257
Iteration 76/1000 | Loss: 0.00001257
Iteration 77/1000 | Loss: 0.00001256
Iteration 78/1000 | Loss: 0.00001256
Iteration 79/1000 | Loss: 0.00001256
Iteration 80/1000 | Loss: 0.00001255
Iteration 81/1000 | Loss: 0.00001255
Iteration 82/1000 | Loss: 0.00001254
Iteration 83/1000 | Loss: 0.00001254
Iteration 84/1000 | Loss: 0.00001254
Iteration 85/1000 | Loss: 0.00001254
Iteration 86/1000 | Loss: 0.00001254
Iteration 87/1000 | Loss: 0.00001254
Iteration 88/1000 | Loss: 0.00001253
Iteration 89/1000 | Loss: 0.00001253
Iteration 90/1000 | Loss: 0.00001253
Iteration 91/1000 | Loss: 0.00001252
Iteration 92/1000 | Loss: 0.00001252
Iteration 93/1000 | Loss: 0.00001252
Iteration 94/1000 | Loss: 0.00001251
Iteration 95/1000 | Loss: 0.00001251
Iteration 96/1000 | Loss: 0.00001251
Iteration 97/1000 | Loss: 0.00001251
Iteration 98/1000 | Loss: 0.00001251
Iteration 99/1000 | Loss: 0.00001251
Iteration 100/1000 | Loss: 0.00001250
Iteration 101/1000 | Loss: 0.00001250
Iteration 102/1000 | Loss: 0.00001250
Iteration 103/1000 | Loss: 0.00001249
Iteration 104/1000 | Loss: 0.00001249
Iteration 105/1000 | Loss: 0.00001249
Iteration 106/1000 | Loss: 0.00001249
Iteration 107/1000 | Loss: 0.00001248
Iteration 108/1000 | Loss: 0.00001248
Iteration 109/1000 | Loss: 0.00001248
Iteration 110/1000 | Loss: 0.00001247
Iteration 111/1000 | Loss: 0.00001247
Iteration 112/1000 | Loss: 0.00001247
Iteration 113/1000 | Loss: 0.00001247
Iteration 114/1000 | Loss: 0.00001247
Iteration 115/1000 | Loss: 0.00001247
Iteration 116/1000 | Loss: 0.00001247
Iteration 117/1000 | Loss: 0.00001247
Iteration 118/1000 | Loss: 0.00001247
Iteration 119/1000 | Loss: 0.00001247
Iteration 120/1000 | Loss: 0.00001246
Iteration 121/1000 | Loss: 0.00001246
Iteration 122/1000 | Loss: 0.00001246
Iteration 123/1000 | Loss: 0.00001246
Iteration 124/1000 | Loss: 0.00001246
Iteration 125/1000 | Loss: 0.00001246
Iteration 126/1000 | Loss: 0.00001246
Iteration 127/1000 | Loss: 0.00001245
Iteration 128/1000 | Loss: 0.00001245
Iteration 129/1000 | Loss: 0.00001245
Iteration 130/1000 | Loss: 0.00001245
Iteration 131/1000 | Loss: 0.00001245
Iteration 132/1000 | Loss: 0.00001244
Iteration 133/1000 | Loss: 0.00001244
Iteration 134/1000 | Loss: 0.00001244
Iteration 135/1000 | Loss: 0.00001244
Iteration 136/1000 | Loss: 0.00001244
Iteration 137/1000 | Loss: 0.00001244
Iteration 138/1000 | Loss: 0.00001244
Iteration 139/1000 | Loss: 0.00001244
Iteration 140/1000 | Loss: 0.00001243
Iteration 141/1000 | Loss: 0.00001243
Iteration 142/1000 | Loss: 0.00001243
Iteration 143/1000 | Loss: 0.00001243
Iteration 144/1000 | Loss: 0.00001243
Iteration 145/1000 | Loss: 0.00001243
Iteration 146/1000 | Loss: 0.00001243
Iteration 147/1000 | Loss: 0.00001242
Iteration 148/1000 | Loss: 0.00001242
Iteration 149/1000 | Loss: 0.00001242
Iteration 150/1000 | Loss: 0.00001242
Iteration 151/1000 | Loss: 0.00001242
Iteration 152/1000 | Loss: 0.00001242
Iteration 153/1000 | Loss: 0.00001242
Iteration 154/1000 | Loss: 0.00001242
Iteration 155/1000 | Loss: 0.00001241
Iteration 156/1000 | Loss: 0.00001241
Iteration 157/1000 | Loss: 0.00001241
Iteration 158/1000 | Loss: 0.00001241
Iteration 159/1000 | Loss: 0.00001241
Iteration 160/1000 | Loss: 0.00001240
Iteration 161/1000 | Loss: 0.00001240
Iteration 162/1000 | Loss: 0.00001240
Iteration 163/1000 | Loss: 0.00001240
Iteration 164/1000 | Loss: 0.00001239
Iteration 165/1000 | Loss: 0.00001239
Iteration 166/1000 | Loss: 0.00001239
Iteration 167/1000 | Loss: 0.00001239
Iteration 168/1000 | Loss: 0.00001239
Iteration 169/1000 | Loss: 0.00001239
Iteration 170/1000 | Loss: 0.00001239
Iteration 171/1000 | Loss: 0.00001239
Iteration 172/1000 | Loss: 0.00001239
Iteration 173/1000 | Loss: 0.00001239
Iteration 174/1000 | Loss: 0.00001239
Iteration 175/1000 | Loss: 0.00001239
Iteration 176/1000 | Loss: 0.00001239
Iteration 177/1000 | Loss: 0.00001239
Iteration 178/1000 | Loss: 0.00001239
Iteration 179/1000 | Loss: 0.00001239
Iteration 180/1000 | Loss: 0.00001239
Iteration 181/1000 | Loss: 0.00001239
Iteration 182/1000 | Loss: 0.00001239
Iteration 183/1000 | Loss: 0.00001239
Iteration 184/1000 | Loss: 0.00001239
Iteration 185/1000 | Loss: 0.00001239
Iteration 186/1000 | Loss: 0.00001239
Iteration 187/1000 | Loss: 0.00001239
Iteration 188/1000 | Loss: 0.00001239
Iteration 189/1000 | Loss: 0.00001239
Iteration 190/1000 | Loss: 0.00001239
Iteration 191/1000 | Loss: 0.00001239
Iteration 192/1000 | Loss: 0.00001239
Iteration 193/1000 | Loss: 0.00001239
Iteration 194/1000 | Loss: 0.00001239
Iteration 195/1000 | Loss: 0.00001239
Iteration 196/1000 | Loss: 0.00001239
Iteration 197/1000 | Loss: 0.00001239
Iteration 198/1000 | Loss: 0.00001239
Iteration 199/1000 | Loss: 0.00001239
Iteration 200/1000 | Loss: 0.00001239
Iteration 201/1000 | Loss: 0.00001239
Iteration 202/1000 | Loss: 0.00001239
Iteration 203/1000 | Loss: 0.00001239
Iteration 204/1000 | Loss: 0.00001239
Iteration 205/1000 | Loss: 0.00001239
Iteration 206/1000 | Loss: 0.00001239
Iteration 207/1000 | Loss: 0.00001239
Iteration 208/1000 | Loss: 0.00001239
Iteration 209/1000 | Loss: 0.00001239
Iteration 210/1000 | Loss: 0.00001239
Iteration 211/1000 | Loss: 0.00001239
Iteration 212/1000 | Loss: 0.00001239
Iteration 213/1000 | Loss: 0.00001239
Iteration 214/1000 | Loss: 0.00001239
Iteration 215/1000 | Loss: 0.00001239
Iteration 216/1000 | Loss: 0.00001239
Iteration 217/1000 | Loss: 0.00001239
Iteration 218/1000 | Loss: 0.00001239
Iteration 219/1000 | Loss: 0.00001239
Iteration 220/1000 | Loss: 0.00001239
Iteration 221/1000 | Loss: 0.00001239
Iteration 222/1000 | Loss: 0.00001239
Iteration 223/1000 | Loss: 0.00001239
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [1.2390390111249872e-05, 1.2390390111249872e-05, 1.2390390111249872e-05, 1.2390390111249872e-05, 1.2390390111249872e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2390390111249872e-05

Optimization complete. Final v2v error: 2.9467077255249023 mm

Highest mean error: 3.2075247764587402 mm for frame 93

Lowest mean error: 2.690019369125366 mm for frame 44

Saving results

Total time: 38.10578274726868
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00427564
Iteration 2/25 | Loss: 0.00118035
Iteration 3/25 | Loss: 0.00110184
Iteration 4/25 | Loss: 0.00108691
Iteration 5/25 | Loss: 0.00108202
Iteration 6/25 | Loss: 0.00108172
Iteration 7/25 | Loss: 0.00108172
Iteration 8/25 | Loss: 0.00108172
Iteration 9/25 | Loss: 0.00108172
Iteration 10/25 | Loss: 0.00108172
Iteration 11/25 | Loss: 0.00108172
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010817233705893159, 0.0010817233705893159, 0.0010817233705893159, 0.0010817233705893159, 0.0010817233705893159]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010817233705893159

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41320622
Iteration 2/25 | Loss: 0.00062702
Iteration 3/25 | Loss: 0.00062701
Iteration 4/25 | Loss: 0.00062701
Iteration 5/25 | Loss: 0.00062701
Iteration 6/25 | Loss: 0.00062701
Iteration 7/25 | Loss: 0.00062701
Iteration 8/25 | Loss: 0.00062701
Iteration 9/25 | Loss: 0.00062701
Iteration 10/25 | Loss: 0.00062701
Iteration 11/25 | Loss: 0.00062701
Iteration 12/25 | Loss: 0.00062701
Iteration 13/25 | Loss: 0.00062701
Iteration 14/25 | Loss: 0.00062701
Iteration 15/25 | Loss: 0.00062701
Iteration 16/25 | Loss: 0.00062701
Iteration 17/25 | Loss: 0.00062701
Iteration 18/25 | Loss: 0.00062701
Iteration 19/25 | Loss: 0.00062701
Iteration 20/25 | Loss: 0.00062701
Iteration 21/25 | Loss: 0.00062701
Iteration 22/25 | Loss: 0.00062701
Iteration 23/25 | Loss: 0.00062701
Iteration 24/25 | Loss: 0.00062701
Iteration 25/25 | Loss: 0.00062701

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062701
Iteration 2/1000 | Loss: 0.00002816
Iteration 3/1000 | Loss: 0.00002056
Iteration 4/1000 | Loss: 0.00001751
Iteration 5/1000 | Loss: 0.00001641
Iteration 6/1000 | Loss: 0.00001558
Iteration 7/1000 | Loss: 0.00001500
Iteration 8/1000 | Loss: 0.00001465
Iteration 9/1000 | Loss: 0.00001443
Iteration 10/1000 | Loss: 0.00001438
Iteration 11/1000 | Loss: 0.00001413
Iteration 12/1000 | Loss: 0.00001409
Iteration 13/1000 | Loss: 0.00001396
Iteration 14/1000 | Loss: 0.00001381
Iteration 15/1000 | Loss: 0.00001376
Iteration 16/1000 | Loss: 0.00001373
Iteration 17/1000 | Loss: 0.00001371
Iteration 18/1000 | Loss: 0.00001371
Iteration 19/1000 | Loss: 0.00001368
Iteration 20/1000 | Loss: 0.00001358
Iteration 21/1000 | Loss: 0.00001357
Iteration 22/1000 | Loss: 0.00001357
Iteration 23/1000 | Loss: 0.00001356
Iteration 24/1000 | Loss: 0.00001356
Iteration 25/1000 | Loss: 0.00001355
Iteration 26/1000 | Loss: 0.00001355
Iteration 27/1000 | Loss: 0.00001354
Iteration 28/1000 | Loss: 0.00001354
Iteration 29/1000 | Loss: 0.00001349
Iteration 30/1000 | Loss: 0.00001349
Iteration 31/1000 | Loss: 0.00001348
Iteration 32/1000 | Loss: 0.00001345
Iteration 33/1000 | Loss: 0.00001345
Iteration 34/1000 | Loss: 0.00001344
Iteration 35/1000 | Loss: 0.00001344
Iteration 36/1000 | Loss: 0.00001343
Iteration 37/1000 | Loss: 0.00001343
Iteration 38/1000 | Loss: 0.00001342
Iteration 39/1000 | Loss: 0.00001341
Iteration 40/1000 | Loss: 0.00001341
Iteration 41/1000 | Loss: 0.00001340
Iteration 42/1000 | Loss: 0.00001336
Iteration 43/1000 | Loss: 0.00001336
Iteration 44/1000 | Loss: 0.00001333
Iteration 45/1000 | Loss: 0.00001332
Iteration 46/1000 | Loss: 0.00001332
Iteration 47/1000 | Loss: 0.00001332
Iteration 48/1000 | Loss: 0.00001332
Iteration 49/1000 | Loss: 0.00001331
Iteration 50/1000 | Loss: 0.00001331
Iteration 51/1000 | Loss: 0.00001331
Iteration 52/1000 | Loss: 0.00001331
Iteration 53/1000 | Loss: 0.00001330
Iteration 54/1000 | Loss: 0.00001330
Iteration 55/1000 | Loss: 0.00001330
Iteration 56/1000 | Loss: 0.00001329
Iteration 57/1000 | Loss: 0.00001329
Iteration 58/1000 | Loss: 0.00001328
Iteration 59/1000 | Loss: 0.00001328
Iteration 60/1000 | Loss: 0.00001327
Iteration 61/1000 | Loss: 0.00001327
Iteration 62/1000 | Loss: 0.00001327
Iteration 63/1000 | Loss: 0.00001326
Iteration 64/1000 | Loss: 0.00001326
Iteration 65/1000 | Loss: 0.00001326
Iteration 66/1000 | Loss: 0.00001325
Iteration 67/1000 | Loss: 0.00001325
Iteration 68/1000 | Loss: 0.00001325
Iteration 69/1000 | Loss: 0.00001325
Iteration 70/1000 | Loss: 0.00001324
Iteration 71/1000 | Loss: 0.00001324
Iteration 72/1000 | Loss: 0.00001324
Iteration 73/1000 | Loss: 0.00001323
Iteration 74/1000 | Loss: 0.00001323
Iteration 75/1000 | Loss: 0.00001323
Iteration 76/1000 | Loss: 0.00001323
Iteration 77/1000 | Loss: 0.00001322
Iteration 78/1000 | Loss: 0.00001322
Iteration 79/1000 | Loss: 0.00001322
Iteration 80/1000 | Loss: 0.00001321
Iteration 81/1000 | Loss: 0.00001321
Iteration 82/1000 | Loss: 0.00001321
Iteration 83/1000 | Loss: 0.00001320
Iteration 84/1000 | Loss: 0.00001320
Iteration 85/1000 | Loss: 0.00001320
Iteration 86/1000 | Loss: 0.00001320
Iteration 87/1000 | Loss: 0.00001319
Iteration 88/1000 | Loss: 0.00001319
Iteration 89/1000 | Loss: 0.00001319
Iteration 90/1000 | Loss: 0.00001319
Iteration 91/1000 | Loss: 0.00001318
Iteration 92/1000 | Loss: 0.00001318
Iteration 93/1000 | Loss: 0.00001318
Iteration 94/1000 | Loss: 0.00001318
Iteration 95/1000 | Loss: 0.00001318
Iteration 96/1000 | Loss: 0.00001318
Iteration 97/1000 | Loss: 0.00001317
Iteration 98/1000 | Loss: 0.00001317
Iteration 99/1000 | Loss: 0.00001317
Iteration 100/1000 | Loss: 0.00001317
Iteration 101/1000 | Loss: 0.00001316
Iteration 102/1000 | Loss: 0.00001316
Iteration 103/1000 | Loss: 0.00001316
Iteration 104/1000 | Loss: 0.00001316
Iteration 105/1000 | Loss: 0.00001316
Iteration 106/1000 | Loss: 0.00001316
Iteration 107/1000 | Loss: 0.00001316
Iteration 108/1000 | Loss: 0.00001316
Iteration 109/1000 | Loss: 0.00001316
Iteration 110/1000 | Loss: 0.00001316
Iteration 111/1000 | Loss: 0.00001315
Iteration 112/1000 | Loss: 0.00001315
Iteration 113/1000 | Loss: 0.00001315
Iteration 114/1000 | Loss: 0.00001314
Iteration 115/1000 | Loss: 0.00001314
Iteration 116/1000 | Loss: 0.00001313
Iteration 117/1000 | Loss: 0.00001313
Iteration 118/1000 | Loss: 0.00001313
Iteration 119/1000 | Loss: 0.00001313
Iteration 120/1000 | Loss: 0.00001313
Iteration 121/1000 | Loss: 0.00001312
Iteration 122/1000 | Loss: 0.00001312
Iteration 123/1000 | Loss: 0.00001312
Iteration 124/1000 | Loss: 0.00001312
Iteration 125/1000 | Loss: 0.00001311
Iteration 126/1000 | Loss: 0.00001311
Iteration 127/1000 | Loss: 0.00001311
Iteration 128/1000 | Loss: 0.00001311
Iteration 129/1000 | Loss: 0.00001311
Iteration 130/1000 | Loss: 0.00001310
Iteration 131/1000 | Loss: 0.00001310
Iteration 132/1000 | Loss: 0.00001310
Iteration 133/1000 | Loss: 0.00001310
Iteration 134/1000 | Loss: 0.00001310
Iteration 135/1000 | Loss: 0.00001310
Iteration 136/1000 | Loss: 0.00001310
Iteration 137/1000 | Loss: 0.00001310
Iteration 138/1000 | Loss: 0.00001310
Iteration 139/1000 | Loss: 0.00001309
Iteration 140/1000 | Loss: 0.00001309
Iteration 141/1000 | Loss: 0.00001309
Iteration 142/1000 | Loss: 0.00001309
Iteration 143/1000 | Loss: 0.00001309
Iteration 144/1000 | Loss: 0.00001309
Iteration 145/1000 | Loss: 0.00001309
Iteration 146/1000 | Loss: 0.00001309
Iteration 147/1000 | Loss: 0.00001309
Iteration 148/1000 | Loss: 0.00001309
Iteration 149/1000 | Loss: 0.00001309
Iteration 150/1000 | Loss: 0.00001308
Iteration 151/1000 | Loss: 0.00001308
Iteration 152/1000 | Loss: 0.00001308
Iteration 153/1000 | Loss: 0.00001308
Iteration 154/1000 | Loss: 0.00001308
Iteration 155/1000 | Loss: 0.00001308
Iteration 156/1000 | Loss: 0.00001308
Iteration 157/1000 | Loss: 0.00001308
Iteration 158/1000 | Loss: 0.00001308
Iteration 159/1000 | Loss: 0.00001308
Iteration 160/1000 | Loss: 0.00001308
Iteration 161/1000 | Loss: 0.00001308
Iteration 162/1000 | Loss: 0.00001308
Iteration 163/1000 | Loss: 0.00001308
Iteration 164/1000 | Loss: 0.00001308
Iteration 165/1000 | Loss: 0.00001308
Iteration 166/1000 | Loss: 0.00001308
Iteration 167/1000 | Loss: 0.00001308
Iteration 168/1000 | Loss: 0.00001307
Iteration 169/1000 | Loss: 0.00001307
Iteration 170/1000 | Loss: 0.00001307
Iteration 171/1000 | Loss: 0.00001307
Iteration 172/1000 | Loss: 0.00001307
Iteration 173/1000 | Loss: 0.00001307
Iteration 174/1000 | Loss: 0.00001307
Iteration 175/1000 | Loss: 0.00001307
Iteration 176/1000 | Loss: 0.00001307
Iteration 177/1000 | Loss: 0.00001307
Iteration 178/1000 | Loss: 0.00001307
Iteration 179/1000 | Loss: 0.00001307
Iteration 180/1000 | Loss: 0.00001307
Iteration 181/1000 | Loss: 0.00001307
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [1.3073457012069412e-05, 1.3073457012069412e-05, 1.3073457012069412e-05, 1.3073457012069412e-05, 1.3073457012069412e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3073457012069412e-05

Optimization complete. Final v2v error: 3.0837910175323486 mm

Highest mean error: 3.4397692680358887 mm for frame 70

Lowest mean error: 2.755303144454956 mm for frame 88

Saving results

Total time: 47.85902714729309
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00821980
Iteration 2/25 | Loss: 0.00156740
Iteration 3/25 | Loss: 0.00125826
Iteration 4/25 | Loss: 0.00122887
Iteration 5/25 | Loss: 0.00122422
Iteration 6/25 | Loss: 0.00122310
Iteration 7/25 | Loss: 0.00122310
Iteration 8/25 | Loss: 0.00122310
Iteration 9/25 | Loss: 0.00122310
Iteration 10/25 | Loss: 0.00122310
Iteration 11/25 | Loss: 0.00122310
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012231030268594623, 0.0012231030268594623, 0.0012231030268594623, 0.0012231030268594623, 0.0012231030268594623]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012231030268594623

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29159331
Iteration 2/25 | Loss: 0.00060979
Iteration 3/25 | Loss: 0.00060974
Iteration 4/25 | Loss: 0.00060974
Iteration 5/25 | Loss: 0.00060974
Iteration 6/25 | Loss: 0.00060974
Iteration 7/25 | Loss: 0.00060974
Iteration 8/25 | Loss: 0.00060973
Iteration 9/25 | Loss: 0.00060973
Iteration 10/25 | Loss: 0.00060973
Iteration 11/25 | Loss: 0.00060973
Iteration 12/25 | Loss: 0.00060973
Iteration 13/25 | Loss: 0.00060973
Iteration 14/25 | Loss: 0.00060973
Iteration 15/25 | Loss: 0.00060973
Iteration 16/25 | Loss: 0.00060973
Iteration 17/25 | Loss: 0.00060973
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006097342702560127, 0.0006097342702560127, 0.0006097342702560127, 0.0006097342702560127, 0.0006097342702560127]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006097342702560127

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060973
Iteration 2/1000 | Loss: 0.00004321
Iteration 3/1000 | Loss: 0.00002908
Iteration 4/1000 | Loss: 0.00002599
Iteration 5/1000 | Loss: 0.00002399
Iteration 6/1000 | Loss: 0.00002293
Iteration 7/1000 | Loss: 0.00002221
Iteration 8/1000 | Loss: 0.00002168
Iteration 9/1000 | Loss: 0.00002126
Iteration 10/1000 | Loss: 0.00002084
Iteration 11/1000 | Loss: 0.00002058
Iteration 12/1000 | Loss: 0.00002039
Iteration 13/1000 | Loss: 0.00002037
Iteration 14/1000 | Loss: 0.00002033
Iteration 15/1000 | Loss: 0.00002030
Iteration 16/1000 | Loss: 0.00002028
Iteration 17/1000 | Loss: 0.00002014
Iteration 18/1000 | Loss: 0.00002010
Iteration 19/1000 | Loss: 0.00002007
Iteration 20/1000 | Loss: 0.00002006
Iteration 21/1000 | Loss: 0.00002006
Iteration 22/1000 | Loss: 0.00002000
Iteration 23/1000 | Loss: 0.00001997
Iteration 24/1000 | Loss: 0.00001996
Iteration 25/1000 | Loss: 0.00001996
Iteration 26/1000 | Loss: 0.00001996
Iteration 27/1000 | Loss: 0.00001996
Iteration 28/1000 | Loss: 0.00001995
Iteration 29/1000 | Loss: 0.00001995
Iteration 30/1000 | Loss: 0.00001995
Iteration 31/1000 | Loss: 0.00001995
Iteration 32/1000 | Loss: 0.00001995
Iteration 33/1000 | Loss: 0.00001994
Iteration 34/1000 | Loss: 0.00001992
Iteration 35/1000 | Loss: 0.00001992
Iteration 36/1000 | Loss: 0.00001992
Iteration 37/1000 | Loss: 0.00001990
Iteration 38/1000 | Loss: 0.00001988
Iteration 39/1000 | Loss: 0.00001988
Iteration 40/1000 | Loss: 0.00001987
Iteration 41/1000 | Loss: 0.00001987
Iteration 42/1000 | Loss: 0.00001987
Iteration 43/1000 | Loss: 0.00001986
Iteration 44/1000 | Loss: 0.00001985
Iteration 45/1000 | Loss: 0.00001984
Iteration 46/1000 | Loss: 0.00001983
Iteration 47/1000 | Loss: 0.00001983
Iteration 48/1000 | Loss: 0.00001983
Iteration 49/1000 | Loss: 0.00001983
Iteration 50/1000 | Loss: 0.00001983
Iteration 51/1000 | Loss: 0.00001983
Iteration 52/1000 | Loss: 0.00001983
Iteration 53/1000 | Loss: 0.00001982
Iteration 54/1000 | Loss: 0.00001982
Iteration 55/1000 | Loss: 0.00001982
Iteration 56/1000 | Loss: 0.00001982
Iteration 57/1000 | Loss: 0.00001982
Iteration 58/1000 | Loss: 0.00001982
Iteration 59/1000 | Loss: 0.00001982
Iteration 60/1000 | Loss: 0.00001981
Iteration 61/1000 | Loss: 0.00001980
Iteration 62/1000 | Loss: 0.00001980
Iteration 63/1000 | Loss: 0.00001979
Iteration 64/1000 | Loss: 0.00001979
Iteration 65/1000 | Loss: 0.00001978
Iteration 66/1000 | Loss: 0.00001978
Iteration 67/1000 | Loss: 0.00001978
Iteration 68/1000 | Loss: 0.00001978
Iteration 69/1000 | Loss: 0.00001978
Iteration 70/1000 | Loss: 0.00001977
Iteration 71/1000 | Loss: 0.00001977
Iteration 72/1000 | Loss: 0.00001977
Iteration 73/1000 | Loss: 0.00001976
Iteration 74/1000 | Loss: 0.00001976
Iteration 75/1000 | Loss: 0.00001976
Iteration 76/1000 | Loss: 0.00001975
Iteration 77/1000 | Loss: 0.00001974
Iteration 78/1000 | Loss: 0.00001974
Iteration 79/1000 | Loss: 0.00001973
Iteration 80/1000 | Loss: 0.00001972
Iteration 81/1000 | Loss: 0.00001970
Iteration 82/1000 | Loss: 0.00001970
Iteration 83/1000 | Loss: 0.00001969
Iteration 84/1000 | Loss: 0.00001968
Iteration 85/1000 | Loss: 0.00001967
Iteration 86/1000 | Loss: 0.00001966
Iteration 87/1000 | Loss: 0.00001966
Iteration 88/1000 | Loss: 0.00001966
Iteration 89/1000 | Loss: 0.00001965
Iteration 90/1000 | Loss: 0.00001965
Iteration 91/1000 | Loss: 0.00001965
Iteration 92/1000 | Loss: 0.00001965
Iteration 93/1000 | Loss: 0.00001965
Iteration 94/1000 | Loss: 0.00001965
Iteration 95/1000 | Loss: 0.00001965
Iteration 96/1000 | Loss: 0.00001965
Iteration 97/1000 | Loss: 0.00001965
Iteration 98/1000 | Loss: 0.00001965
Iteration 99/1000 | Loss: 0.00001964
Iteration 100/1000 | Loss: 0.00001964
Iteration 101/1000 | Loss: 0.00001964
Iteration 102/1000 | Loss: 0.00001964
Iteration 103/1000 | Loss: 0.00001963
Iteration 104/1000 | Loss: 0.00001963
Iteration 105/1000 | Loss: 0.00001962
Iteration 106/1000 | Loss: 0.00001962
Iteration 107/1000 | Loss: 0.00001962
Iteration 108/1000 | Loss: 0.00001962
Iteration 109/1000 | Loss: 0.00001962
Iteration 110/1000 | Loss: 0.00001962
Iteration 111/1000 | Loss: 0.00001962
Iteration 112/1000 | Loss: 0.00001961
Iteration 113/1000 | Loss: 0.00001961
Iteration 114/1000 | Loss: 0.00001961
Iteration 115/1000 | Loss: 0.00001961
Iteration 116/1000 | Loss: 0.00001961
Iteration 117/1000 | Loss: 0.00001961
Iteration 118/1000 | Loss: 0.00001961
Iteration 119/1000 | Loss: 0.00001961
Iteration 120/1000 | Loss: 0.00001961
Iteration 121/1000 | Loss: 0.00001961
Iteration 122/1000 | Loss: 0.00001961
Iteration 123/1000 | Loss: 0.00001960
Iteration 124/1000 | Loss: 0.00001960
Iteration 125/1000 | Loss: 0.00001960
Iteration 126/1000 | Loss: 0.00001960
Iteration 127/1000 | Loss: 0.00001960
Iteration 128/1000 | Loss: 0.00001960
Iteration 129/1000 | Loss: 0.00001959
Iteration 130/1000 | Loss: 0.00001959
Iteration 131/1000 | Loss: 0.00001959
Iteration 132/1000 | Loss: 0.00001959
Iteration 133/1000 | Loss: 0.00001958
Iteration 134/1000 | Loss: 0.00001958
Iteration 135/1000 | Loss: 0.00001958
Iteration 136/1000 | Loss: 0.00001957
Iteration 137/1000 | Loss: 0.00001957
Iteration 138/1000 | Loss: 0.00001957
Iteration 139/1000 | Loss: 0.00001957
Iteration 140/1000 | Loss: 0.00001957
Iteration 141/1000 | Loss: 0.00001956
Iteration 142/1000 | Loss: 0.00001956
Iteration 143/1000 | Loss: 0.00001956
Iteration 144/1000 | Loss: 0.00001956
Iteration 145/1000 | Loss: 0.00001956
Iteration 146/1000 | Loss: 0.00001956
Iteration 147/1000 | Loss: 0.00001956
Iteration 148/1000 | Loss: 0.00001956
Iteration 149/1000 | Loss: 0.00001956
Iteration 150/1000 | Loss: 0.00001956
Iteration 151/1000 | Loss: 0.00001956
Iteration 152/1000 | Loss: 0.00001955
Iteration 153/1000 | Loss: 0.00001955
Iteration 154/1000 | Loss: 0.00001955
Iteration 155/1000 | Loss: 0.00001954
Iteration 156/1000 | Loss: 0.00001954
Iteration 157/1000 | Loss: 0.00001954
Iteration 158/1000 | Loss: 0.00001954
Iteration 159/1000 | Loss: 0.00001954
Iteration 160/1000 | Loss: 0.00001953
Iteration 161/1000 | Loss: 0.00001953
Iteration 162/1000 | Loss: 0.00001953
Iteration 163/1000 | Loss: 0.00001953
Iteration 164/1000 | Loss: 0.00001953
Iteration 165/1000 | Loss: 0.00001953
Iteration 166/1000 | Loss: 0.00001953
Iteration 167/1000 | Loss: 0.00001953
Iteration 168/1000 | Loss: 0.00001953
Iteration 169/1000 | Loss: 0.00001952
Iteration 170/1000 | Loss: 0.00001952
Iteration 171/1000 | Loss: 0.00001952
Iteration 172/1000 | Loss: 0.00001952
Iteration 173/1000 | Loss: 0.00001952
Iteration 174/1000 | Loss: 0.00001952
Iteration 175/1000 | Loss: 0.00001952
Iteration 176/1000 | Loss: 0.00001952
Iteration 177/1000 | Loss: 0.00001952
Iteration 178/1000 | Loss: 0.00001951
Iteration 179/1000 | Loss: 0.00001951
Iteration 180/1000 | Loss: 0.00001951
Iteration 181/1000 | Loss: 0.00001951
Iteration 182/1000 | Loss: 0.00001951
Iteration 183/1000 | Loss: 0.00001951
Iteration 184/1000 | Loss: 0.00001951
Iteration 185/1000 | Loss: 0.00001951
Iteration 186/1000 | Loss: 0.00001951
Iteration 187/1000 | Loss: 0.00001951
Iteration 188/1000 | Loss: 0.00001951
Iteration 189/1000 | Loss: 0.00001950
Iteration 190/1000 | Loss: 0.00001950
Iteration 191/1000 | Loss: 0.00001950
Iteration 192/1000 | Loss: 0.00001950
Iteration 193/1000 | Loss: 0.00001950
Iteration 194/1000 | Loss: 0.00001950
Iteration 195/1000 | Loss: 0.00001950
Iteration 196/1000 | Loss: 0.00001949
Iteration 197/1000 | Loss: 0.00001949
Iteration 198/1000 | Loss: 0.00001949
Iteration 199/1000 | Loss: 0.00001949
Iteration 200/1000 | Loss: 0.00001949
Iteration 201/1000 | Loss: 0.00001949
Iteration 202/1000 | Loss: 0.00001949
Iteration 203/1000 | Loss: 0.00001949
Iteration 204/1000 | Loss: 0.00001949
Iteration 205/1000 | Loss: 0.00001949
Iteration 206/1000 | Loss: 0.00001949
Iteration 207/1000 | Loss: 0.00001949
Iteration 208/1000 | Loss: 0.00001949
Iteration 209/1000 | Loss: 0.00001948
Iteration 210/1000 | Loss: 0.00001948
Iteration 211/1000 | Loss: 0.00001947
Iteration 212/1000 | Loss: 0.00001947
Iteration 213/1000 | Loss: 0.00001947
Iteration 214/1000 | Loss: 0.00001947
Iteration 215/1000 | Loss: 0.00001947
Iteration 216/1000 | Loss: 0.00001947
Iteration 217/1000 | Loss: 0.00001947
Iteration 218/1000 | Loss: 0.00001947
Iteration 219/1000 | Loss: 0.00001947
Iteration 220/1000 | Loss: 0.00001947
Iteration 221/1000 | Loss: 0.00001947
Iteration 222/1000 | Loss: 0.00001947
Iteration 223/1000 | Loss: 0.00001947
Iteration 224/1000 | Loss: 0.00001947
Iteration 225/1000 | Loss: 0.00001947
Iteration 226/1000 | Loss: 0.00001947
Iteration 227/1000 | Loss: 0.00001947
Iteration 228/1000 | Loss: 0.00001947
Iteration 229/1000 | Loss: 0.00001947
Iteration 230/1000 | Loss: 0.00001947
Iteration 231/1000 | Loss: 0.00001947
Iteration 232/1000 | Loss: 0.00001947
Iteration 233/1000 | Loss: 0.00001947
Iteration 234/1000 | Loss: 0.00001947
Iteration 235/1000 | Loss: 0.00001947
Iteration 236/1000 | Loss: 0.00001947
Iteration 237/1000 | Loss: 0.00001947
Iteration 238/1000 | Loss: 0.00001947
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 238. Stopping optimization.
Last 5 losses: [1.9465120203676634e-05, 1.9465120203676634e-05, 1.9465120203676634e-05, 1.9465120203676634e-05, 1.9465120203676634e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9465120203676634e-05

Optimization complete. Final v2v error: 3.780400276184082 mm

Highest mean error: 4.126175403594971 mm for frame 137

Lowest mean error: 3.376887321472168 mm for frame 1

Saving results

Total time: 45.69030809402466
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00768086
Iteration 2/25 | Loss: 0.00114838
Iteration 3/25 | Loss: 0.00107455
Iteration 4/25 | Loss: 0.00106205
Iteration 5/25 | Loss: 0.00105807
Iteration 6/25 | Loss: 0.00105709
Iteration 7/25 | Loss: 0.00105707
Iteration 8/25 | Loss: 0.00105707
Iteration 9/25 | Loss: 0.00105704
Iteration 10/25 | Loss: 0.00105704
Iteration 11/25 | Loss: 0.00105704
Iteration 12/25 | Loss: 0.00105704
Iteration 13/25 | Loss: 0.00105704
Iteration 14/25 | Loss: 0.00105704
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0010570392478257418, 0.0010570392478257418, 0.0010570392478257418, 0.0010570392478257418, 0.0010570392478257418]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010570392478257418

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36896157
Iteration 2/25 | Loss: 0.00083505
Iteration 3/25 | Loss: 0.00083504
Iteration 4/25 | Loss: 0.00083504
Iteration 5/25 | Loss: 0.00083504
Iteration 6/25 | Loss: 0.00083504
Iteration 7/25 | Loss: 0.00083503
Iteration 8/25 | Loss: 0.00083503
Iteration 9/25 | Loss: 0.00083503
Iteration 10/25 | Loss: 0.00083503
Iteration 11/25 | Loss: 0.00083503
Iteration 12/25 | Loss: 0.00083503
Iteration 13/25 | Loss: 0.00083503
Iteration 14/25 | Loss: 0.00083503
Iteration 15/25 | Loss: 0.00083503
Iteration 16/25 | Loss: 0.00083503
Iteration 17/25 | Loss: 0.00083503
Iteration 18/25 | Loss: 0.00083503
Iteration 19/25 | Loss: 0.00083503
Iteration 20/25 | Loss: 0.00083503
Iteration 21/25 | Loss: 0.00083503
Iteration 22/25 | Loss: 0.00083503
Iteration 23/25 | Loss: 0.00083503
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008350331336259842, 0.0008350331336259842, 0.0008350331336259842, 0.0008350331336259842, 0.0008350331336259842]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008350331336259842

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083503
Iteration 2/1000 | Loss: 0.00003466
Iteration 3/1000 | Loss: 0.00001909
Iteration 4/1000 | Loss: 0.00001479
Iteration 5/1000 | Loss: 0.00001322
Iteration 6/1000 | Loss: 0.00001243
Iteration 7/1000 | Loss: 0.00001191
Iteration 8/1000 | Loss: 0.00001161
Iteration 9/1000 | Loss: 0.00001141
Iteration 10/1000 | Loss: 0.00001128
Iteration 11/1000 | Loss: 0.00001110
Iteration 12/1000 | Loss: 0.00001096
Iteration 13/1000 | Loss: 0.00001081
Iteration 14/1000 | Loss: 0.00001080
Iteration 15/1000 | Loss: 0.00001074
Iteration 16/1000 | Loss: 0.00001065
Iteration 17/1000 | Loss: 0.00001061
Iteration 18/1000 | Loss: 0.00001061
Iteration 19/1000 | Loss: 0.00001060
Iteration 20/1000 | Loss: 0.00001060
Iteration 21/1000 | Loss: 0.00001056
Iteration 22/1000 | Loss: 0.00001051
Iteration 23/1000 | Loss: 0.00001051
Iteration 24/1000 | Loss: 0.00001049
Iteration 25/1000 | Loss: 0.00001049
Iteration 26/1000 | Loss: 0.00001048
Iteration 27/1000 | Loss: 0.00001048
Iteration 28/1000 | Loss: 0.00001048
Iteration 29/1000 | Loss: 0.00001047
Iteration 30/1000 | Loss: 0.00001046
Iteration 31/1000 | Loss: 0.00001045
Iteration 32/1000 | Loss: 0.00001045
Iteration 33/1000 | Loss: 0.00001045
Iteration 34/1000 | Loss: 0.00001045
Iteration 35/1000 | Loss: 0.00001045
Iteration 36/1000 | Loss: 0.00001044
Iteration 37/1000 | Loss: 0.00001044
Iteration 38/1000 | Loss: 0.00001044
Iteration 39/1000 | Loss: 0.00001043
Iteration 40/1000 | Loss: 0.00001043
Iteration 41/1000 | Loss: 0.00001043
Iteration 42/1000 | Loss: 0.00001043
Iteration 43/1000 | Loss: 0.00001043
Iteration 44/1000 | Loss: 0.00001042
Iteration 45/1000 | Loss: 0.00001042
Iteration 46/1000 | Loss: 0.00001042
Iteration 47/1000 | Loss: 0.00001042
Iteration 48/1000 | Loss: 0.00001042
Iteration 49/1000 | Loss: 0.00001041
Iteration 50/1000 | Loss: 0.00001041
Iteration 51/1000 | Loss: 0.00001041
Iteration 52/1000 | Loss: 0.00001041
Iteration 53/1000 | Loss: 0.00001041
Iteration 54/1000 | Loss: 0.00001041
Iteration 55/1000 | Loss: 0.00001040
Iteration 56/1000 | Loss: 0.00001040
Iteration 57/1000 | Loss: 0.00001040
Iteration 58/1000 | Loss: 0.00001039
Iteration 59/1000 | Loss: 0.00001039
Iteration 60/1000 | Loss: 0.00001039
Iteration 61/1000 | Loss: 0.00001039
Iteration 62/1000 | Loss: 0.00001039
Iteration 63/1000 | Loss: 0.00001039
Iteration 64/1000 | Loss: 0.00001039
Iteration 65/1000 | Loss: 0.00001039
Iteration 66/1000 | Loss: 0.00001039
Iteration 67/1000 | Loss: 0.00001039
Iteration 68/1000 | Loss: 0.00001038
Iteration 69/1000 | Loss: 0.00001038
Iteration 70/1000 | Loss: 0.00001038
Iteration 71/1000 | Loss: 0.00001038
Iteration 72/1000 | Loss: 0.00001038
Iteration 73/1000 | Loss: 0.00001038
Iteration 74/1000 | Loss: 0.00001038
Iteration 75/1000 | Loss: 0.00001038
Iteration 76/1000 | Loss: 0.00001038
Iteration 77/1000 | Loss: 0.00001038
Iteration 78/1000 | Loss: 0.00001038
Iteration 79/1000 | Loss: 0.00001037
Iteration 80/1000 | Loss: 0.00001037
Iteration 81/1000 | Loss: 0.00001037
Iteration 82/1000 | Loss: 0.00001037
Iteration 83/1000 | Loss: 0.00001037
Iteration 84/1000 | Loss: 0.00001037
Iteration 85/1000 | Loss: 0.00001037
Iteration 86/1000 | Loss: 0.00001037
Iteration 87/1000 | Loss: 0.00001037
Iteration 88/1000 | Loss: 0.00001037
Iteration 89/1000 | Loss: 0.00001037
Iteration 90/1000 | Loss: 0.00001037
Iteration 91/1000 | Loss: 0.00001037
Iteration 92/1000 | Loss: 0.00001037
Iteration 93/1000 | Loss: 0.00001037
Iteration 94/1000 | Loss: 0.00001037
Iteration 95/1000 | Loss: 0.00001036
Iteration 96/1000 | Loss: 0.00001036
Iteration 97/1000 | Loss: 0.00001036
Iteration 98/1000 | Loss: 0.00001036
Iteration 99/1000 | Loss: 0.00001036
Iteration 100/1000 | Loss: 0.00001036
Iteration 101/1000 | Loss: 0.00001036
Iteration 102/1000 | Loss: 0.00001036
Iteration 103/1000 | Loss: 0.00001036
Iteration 104/1000 | Loss: 0.00001036
Iteration 105/1000 | Loss: 0.00001036
Iteration 106/1000 | Loss: 0.00001036
Iteration 107/1000 | Loss: 0.00001036
Iteration 108/1000 | Loss: 0.00001036
Iteration 109/1000 | Loss: 0.00001036
Iteration 110/1000 | Loss: 0.00001036
Iteration 111/1000 | Loss: 0.00001036
Iteration 112/1000 | Loss: 0.00001035
Iteration 113/1000 | Loss: 0.00001035
Iteration 114/1000 | Loss: 0.00001035
Iteration 115/1000 | Loss: 0.00001035
Iteration 116/1000 | Loss: 0.00001035
Iteration 117/1000 | Loss: 0.00001035
Iteration 118/1000 | Loss: 0.00001035
Iteration 119/1000 | Loss: 0.00001035
Iteration 120/1000 | Loss: 0.00001035
Iteration 121/1000 | Loss: 0.00001035
Iteration 122/1000 | Loss: 0.00001035
Iteration 123/1000 | Loss: 0.00001035
Iteration 124/1000 | Loss: 0.00001035
Iteration 125/1000 | Loss: 0.00001035
Iteration 126/1000 | Loss: 0.00001035
Iteration 127/1000 | Loss: 0.00001035
Iteration 128/1000 | Loss: 0.00001034
Iteration 129/1000 | Loss: 0.00001034
Iteration 130/1000 | Loss: 0.00001034
Iteration 131/1000 | Loss: 0.00001034
Iteration 132/1000 | Loss: 0.00001034
Iteration 133/1000 | Loss: 0.00001034
Iteration 134/1000 | Loss: 0.00001034
Iteration 135/1000 | Loss: 0.00001034
Iteration 136/1000 | Loss: 0.00001034
Iteration 137/1000 | Loss: 0.00001033
Iteration 138/1000 | Loss: 0.00001033
Iteration 139/1000 | Loss: 0.00001033
Iteration 140/1000 | Loss: 0.00001033
Iteration 141/1000 | Loss: 0.00001033
Iteration 142/1000 | Loss: 0.00001033
Iteration 143/1000 | Loss: 0.00001033
Iteration 144/1000 | Loss: 0.00001032
Iteration 145/1000 | Loss: 0.00001032
Iteration 146/1000 | Loss: 0.00001032
Iteration 147/1000 | Loss: 0.00001032
Iteration 148/1000 | Loss: 0.00001032
Iteration 149/1000 | Loss: 0.00001032
Iteration 150/1000 | Loss: 0.00001032
Iteration 151/1000 | Loss: 0.00001032
Iteration 152/1000 | Loss: 0.00001032
Iteration 153/1000 | Loss: 0.00001032
Iteration 154/1000 | Loss: 0.00001031
Iteration 155/1000 | Loss: 0.00001031
Iteration 156/1000 | Loss: 0.00001031
Iteration 157/1000 | Loss: 0.00001031
Iteration 158/1000 | Loss: 0.00001031
Iteration 159/1000 | Loss: 0.00001031
Iteration 160/1000 | Loss: 0.00001030
Iteration 161/1000 | Loss: 0.00001030
Iteration 162/1000 | Loss: 0.00001030
Iteration 163/1000 | Loss: 0.00001030
Iteration 164/1000 | Loss: 0.00001030
Iteration 165/1000 | Loss: 0.00001030
Iteration 166/1000 | Loss: 0.00001030
Iteration 167/1000 | Loss: 0.00001030
Iteration 168/1000 | Loss: 0.00001030
Iteration 169/1000 | Loss: 0.00001030
Iteration 170/1000 | Loss: 0.00001030
Iteration 171/1000 | Loss: 0.00001029
Iteration 172/1000 | Loss: 0.00001029
Iteration 173/1000 | Loss: 0.00001029
Iteration 174/1000 | Loss: 0.00001029
Iteration 175/1000 | Loss: 0.00001029
Iteration 176/1000 | Loss: 0.00001029
Iteration 177/1000 | Loss: 0.00001029
Iteration 178/1000 | Loss: 0.00001029
Iteration 179/1000 | Loss: 0.00001029
Iteration 180/1000 | Loss: 0.00001029
Iteration 181/1000 | Loss: 0.00001029
Iteration 182/1000 | Loss: 0.00001029
Iteration 183/1000 | Loss: 0.00001029
Iteration 184/1000 | Loss: 0.00001029
Iteration 185/1000 | Loss: 0.00001029
Iteration 186/1000 | Loss: 0.00001029
Iteration 187/1000 | Loss: 0.00001029
Iteration 188/1000 | Loss: 0.00001029
Iteration 189/1000 | Loss: 0.00001029
Iteration 190/1000 | Loss: 0.00001029
Iteration 191/1000 | Loss: 0.00001029
Iteration 192/1000 | Loss: 0.00001029
Iteration 193/1000 | Loss: 0.00001029
Iteration 194/1000 | Loss: 0.00001029
Iteration 195/1000 | Loss: 0.00001029
Iteration 196/1000 | Loss: 0.00001029
Iteration 197/1000 | Loss: 0.00001029
Iteration 198/1000 | Loss: 0.00001029
Iteration 199/1000 | Loss: 0.00001029
Iteration 200/1000 | Loss: 0.00001029
Iteration 201/1000 | Loss: 0.00001029
Iteration 202/1000 | Loss: 0.00001029
Iteration 203/1000 | Loss: 0.00001029
Iteration 204/1000 | Loss: 0.00001029
Iteration 205/1000 | Loss: 0.00001029
Iteration 206/1000 | Loss: 0.00001029
Iteration 207/1000 | Loss: 0.00001029
Iteration 208/1000 | Loss: 0.00001029
Iteration 209/1000 | Loss: 0.00001029
Iteration 210/1000 | Loss: 0.00001029
Iteration 211/1000 | Loss: 0.00001029
Iteration 212/1000 | Loss: 0.00001029
Iteration 213/1000 | Loss: 0.00001029
Iteration 214/1000 | Loss: 0.00001029
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [1.0286049473506864e-05, 1.0286049473506864e-05, 1.0286049473506864e-05, 1.0286049473506864e-05, 1.0286049473506864e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0286049473506864e-05

Optimization complete. Final v2v error: 2.7355544567108154 mm

Highest mean error: 3.212476968765259 mm for frame 101

Lowest mean error: 2.437828540802002 mm for frame 31

Saving results

Total time: 40.68778944015503
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01084895
Iteration 2/25 | Loss: 0.01084894
Iteration 3/25 | Loss: 0.00246200
Iteration 4/25 | Loss: 0.00199146
Iteration 5/25 | Loss: 0.00238911
Iteration 6/25 | Loss: 0.00246536
Iteration 7/25 | Loss: 0.00190035
Iteration 8/25 | Loss: 0.00147782
Iteration 9/25 | Loss: 0.00132185
Iteration 10/25 | Loss: 0.00129517
Iteration 11/25 | Loss: 0.00128918
Iteration 12/25 | Loss: 0.00127724
Iteration 13/25 | Loss: 0.00127371
Iteration 14/25 | Loss: 0.00126666
Iteration 15/25 | Loss: 0.00126141
Iteration 16/25 | Loss: 0.00126352
Iteration 17/25 | Loss: 0.00126578
Iteration 18/25 | Loss: 0.00126367
Iteration 19/25 | Loss: 0.00127274
Iteration 20/25 | Loss: 0.00126216
Iteration 21/25 | Loss: 0.00125995
Iteration 22/25 | Loss: 0.00125945
Iteration 23/25 | Loss: 0.00125697
Iteration 24/25 | Loss: 0.00125521
Iteration 25/25 | Loss: 0.00125495

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26312697
Iteration 2/25 | Loss: 0.00084403
Iteration 3/25 | Loss: 0.00084402
Iteration 4/25 | Loss: 0.00084401
Iteration 5/25 | Loss: 0.00084401
Iteration 6/25 | Loss: 0.00084401
Iteration 7/25 | Loss: 0.00084401
Iteration 8/25 | Loss: 0.00084401
Iteration 9/25 | Loss: 0.00084401
Iteration 10/25 | Loss: 0.00084401
Iteration 11/25 | Loss: 0.00084401
Iteration 12/25 | Loss: 0.00084401
Iteration 13/25 | Loss: 0.00084401
Iteration 14/25 | Loss: 0.00084401
Iteration 15/25 | Loss: 0.00084401
Iteration 16/25 | Loss: 0.00084401
Iteration 17/25 | Loss: 0.00084401
Iteration 18/25 | Loss: 0.00084401
Iteration 19/25 | Loss: 0.00084401
Iteration 20/25 | Loss: 0.00084401
Iteration 21/25 | Loss: 0.00084401
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008440114906989038, 0.0008440114906989038, 0.0008440114906989038, 0.0008440114906989038, 0.0008440114906989038]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008440114906989038

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084401
Iteration 2/1000 | Loss: 0.00027024
Iteration 3/1000 | Loss: 0.00014967
Iteration 4/1000 | Loss: 0.00016710
Iteration 5/1000 | Loss: 0.00005307
Iteration 6/1000 | Loss: 0.00004162
Iteration 7/1000 | Loss: 0.00004569
Iteration 8/1000 | Loss: 0.00003221
Iteration 9/1000 | Loss: 0.00004240
Iteration 10/1000 | Loss: 0.00003916
Iteration 11/1000 | Loss: 0.00003422
Iteration 12/1000 | Loss: 0.00002880
Iteration 13/1000 | Loss: 0.00003438
Iteration 14/1000 | Loss: 0.00003588
Iteration 15/1000 | Loss: 0.00004375
Iteration 16/1000 | Loss: 0.00004098
Iteration 17/1000 | Loss: 0.00004177
Iteration 18/1000 | Loss: 0.00002744
Iteration 19/1000 | Loss: 0.00002795
Iteration 20/1000 | Loss: 0.00003351
Iteration 21/1000 | Loss: 0.00003209
Iteration 22/1000 | Loss: 0.00003287
Iteration 23/1000 | Loss: 0.00003791
Iteration 24/1000 | Loss: 0.00003715
Iteration 25/1000 | Loss: 0.00002893
Iteration 26/1000 | Loss: 0.00002622
Iteration 27/1000 | Loss: 0.00002557
Iteration 28/1000 | Loss: 0.00002523
Iteration 29/1000 | Loss: 0.00002493
Iteration 30/1000 | Loss: 0.00002467
Iteration 31/1000 | Loss: 0.00002466
Iteration 32/1000 | Loss: 0.00002464
Iteration 33/1000 | Loss: 0.00002464
Iteration 34/1000 | Loss: 0.00002464
Iteration 35/1000 | Loss: 0.00002464
Iteration 36/1000 | Loss: 0.00002461
Iteration 37/1000 | Loss: 0.00002460
Iteration 38/1000 | Loss: 0.00002459
Iteration 39/1000 | Loss: 0.00002458
Iteration 40/1000 | Loss: 0.00002456
Iteration 41/1000 | Loss: 0.00002456
Iteration 42/1000 | Loss: 0.00002455
Iteration 43/1000 | Loss: 0.00002455
Iteration 44/1000 | Loss: 0.00002454
Iteration 45/1000 | Loss: 0.00002454
Iteration 46/1000 | Loss: 0.00002454
Iteration 47/1000 | Loss: 0.00002454
Iteration 48/1000 | Loss: 0.00002454
Iteration 49/1000 | Loss: 0.00002454
Iteration 50/1000 | Loss: 0.00002454
Iteration 51/1000 | Loss: 0.00002454
Iteration 52/1000 | Loss: 0.00002453
Iteration 53/1000 | Loss: 0.00002453
Iteration 54/1000 | Loss: 0.00002453
Iteration 55/1000 | Loss: 0.00002453
Iteration 56/1000 | Loss: 0.00002453
Iteration 57/1000 | Loss: 0.00002453
Iteration 58/1000 | Loss: 0.00002452
Iteration 59/1000 | Loss: 0.00002452
Iteration 60/1000 | Loss: 0.00002452
Iteration 61/1000 | Loss: 0.00002452
Iteration 62/1000 | Loss: 0.00002452
Iteration 63/1000 | Loss: 0.00002452
Iteration 64/1000 | Loss: 0.00002452
Iteration 65/1000 | Loss: 0.00002452
Iteration 66/1000 | Loss: 0.00002451
Iteration 67/1000 | Loss: 0.00002451
Iteration 68/1000 | Loss: 0.00002451
Iteration 69/1000 | Loss: 0.00002451
Iteration 70/1000 | Loss: 0.00002451
Iteration 71/1000 | Loss: 0.00002451
Iteration 72/1000 | Loss: 0.00002451
Iteration 73/1000 | Loss: 0.00002451
Iteration 74/1000 | Loss: 0.00002451
Iteration 75/1000 | Loss: 0.00002451
Iteration 76/1000 | Loss: 0.00002451
Iteration 77/1000 | Loss: 0.00002451
Iteration 78/1000 | Loss: 0.00002451
Iteration 79/1000 | Loss: 0.00002451
Iteration 80/1000 | Loss: 0.00002451
Iteration 81/1000 | Loss: 0.00002450
Iteration 82/1000 | Loss: 0.00002450
Iteration 83/1000 | Loss: 0.00002450
Iteration 84/1000 | Loss: 0.00002450
Iteration 85/1000 | Loss: 0.00002450
Iteration 86/1000 | Loss: 0.00002450
Iteration 87/1000 | Loss: 0.00002450
Iteration 88/1000 | Loss: 0.00002450
Iteration 89/1000 | Loss: 0.00002450
Iteration 90/1000 | Loss: 0.00002450
Iteration 91/1000 | Loss: 0.00002450
Iteration 92/1000 | Loss: 0.00002450
Iteration 93/1000 | Loss: 0.00002450
Iteration 94/1000 | Loss: 0.00002450
Iteration 95/1000 | Loss: 0.00002449
Iteration 96/1000 | Loss: 0.00002449
Iteration 97/1000 | Loss: 0.00002449
Iteration 98/1000 | Loss: 0.00002449
Iteration 99/1000 | Loss: 0.00002449
Iteration 100/1000 | Loss: 0.00002449
Iteration 101/1000 | Loss: 0.00002449
Iteration 102/1000 | Loss: 0.00002449
Iteration 103/1000 | Loss: 0.00002449
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [2.449380190228112e-05, 2.449380190228112e-05, 2.449380190228112e-05, 2.449380190228112e-05, 2.449380190228112e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.449380190228112e-05

Optimization complete. Final v2v error: 3.9584264755249023 mm

Highest mean error: 4.91416072845459 mm for frame 195

Lowest mean error: 3.6572866439819336 mm for frame 1

Saving results

Total time: 107.01295781135559
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01013258
Iteration 2/25 | Loss: 0.00271957
Iteration 3/25 | Loss: 0.00179611
Iteration 4/25 | Loss: 0.00152277
Iteration 5/25 | Loss: 0.00136730
Iteration 6/25 | Loss: 0.00130192
Iteration 7/25 | Loss: 0.00131100
Iteration 8/25 | Loss: 0.00129605
Iteration 9/25 | Loss: 0.00128112
Iteration 10/25 | Loss: 0.00126750
Iteration 11/25 | Loss: 0.00126320
Iteration 12/25 | Loss: 0.00126026
Iteration 13/25 | Loss: 0.00125795
Iteration 14/25 | Loss: 0.00125968
Iteration 15/25 | Loss: 0.00125618
Iteration 16/25 | Loss: 0.00125022
Iteration 17/25 | Loss: 0.00124771
Iteration 18/25 | Loss: 0.00124843
Iteration 19/25 | Loss: 0.00124718
Iteration 20/25 | Loss: 0.00125010
Iteration 21/25 | Loss: 0.00124704
Iteration 22/25 | Loss: 0.00124701
Iteration 23/25 | Loss: 0.00124701
Iteration 24/25 | Loss: 0.00124701
Iteration 25/25 | Loss: 0.00124701

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41115034
Iteration 2/25 | Loss: 0.00136852
Iteration 3/25 | Loss: 0.00132113
Iteration 4/25 | Loss: 0.00132113
Iteration 5/25 | Loss: 0.00132113
Iteration 6/25 | Loss: 0.00132113
Iteration 7/25 | Loss: 0.00132113
Iteration 8/25 | Loss: 0.00132113
Iteration 9/25 | Loss: 0.00132113
Iteration 10/25 | Loss: 0.00132113
Iteration 11/25 | Loss: 0.00132113
Iteration 12/25 | Loss: 0.00132113
Iteration 13/25 | Loss: 0.00132113
Iteration 14/25 | Loss: 0.00132113
Iteration 15/25 | Loss: 0.00132113
Iteration 16/25 | Loss: 0.00132113
Iteration 17/25 | Loss: 0.00132113
Iteration 18/25 | Loss: 0.00132113
Iteration 19/25 | Loss: 0.00132113
Iteration 20/25 | Loss: 0.00132113
Iteration 21/25 | Loss: 0.00132113
Iteration 22/25 | Loss: 0.00132113
Iteration 23/25 | Loss: 0.00132113
Iteration 24/25 | Loss: 0.00132113
Iteration 25/25 | Loss: 0.00132113

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132113
Iteration 2/1000 | Loss: 0.00019349
Iteration 3/1000 | Loss: 0.00026735
Iteration 4/1000 | Loss: 0.00010442
Iteration 5/1000 | Loss: 0.00013299
Iteration 6/1000 | Loss: 0.00011300
Iteration 7/1000 | Loss: 0.00035136
Iteration 8/1000 | Loss: 0.00020080
Iteration 9/1000 | Loss: 0.00015406
Iteration 10/1000 | Loss: 0.00011109
Iteration 11/1000 | Loss: 0.00009117
Iteration 12/1000 | Loss: 0.00008093
Iteration 13/1000 | Loss: 0.00006006
Iteration 14/1000 | Loss: 0.00009981
Iteration 15/1000 | Loss: 0.00006778
Iteration 16/1000 | Loss: 0.00013394
Iteration 17/1000 | Loss: 0.00007747
Iteration 18/1000 | Loss: 0.00026113
Iteration 19/1000 | Loss: 0.00071101
Iteration 20/1000 | Loss: 0.00423326
Iteration 21/1000 | Loss: 0.00170134
Iteration 22/1000 | Loss: 0.00091026
Iteration 23/1000 | Loss: 0.00178302
Iteration 24/1000 | Loss: 0.00040852
Iteration 25/1000 | Loss: 0.00016838
Iteration 26/1000 | Loss: 0.00036179
Iteration 27/1000 | Loss: 0.00014675
Iteration 28/1000 | Loss: 0.00017939
Iteration 29/1000 | Loss: 0.00009479
Iteration 30/1000 | Loss: 0.00032557
Iteration 31/1000 | Loss: 0.00003917
Iteration 32/1000 | Loss: 0.00015908
Iteration 33/1000 | Loss: 0.00013683
Iteration 34/1000 | Loss: 0.00017366
Iteration 35/1000 | Loss: 0.00006551
Iteration 36/1000 | Loss: 0.00003246
Iteration 37/1000 | Loss: 0.00013007
Iteration 38/1000 | Loss: 0.00004629
Iteration 39/1000 | Loss: 0.00008159
Iteration 40/1000 | Loss: 0.00013066
Iteration 41/1000 | Loss: 0.00006032
Iteration 42/1000 | Loss: 0.00005523
Iteration 43/1000 | Loss: 0.00031463
Iteration 44/1000 | Loss: 0.00001960
Iteration 45/1000 | Loss: 0.00002627
Iteration 46/1000 | Loss: 0.00011166
Iteration 47/1000 | Loss: 0.00007049
Iteration 48/1000 | Loss: 0.00016565
Iteration 49/1000 | Loss: 0.00002955
Iteration 50/1000 | Loss: 0.00001659
Iteration 51/1000 | Loss: 0.00006530
Iteration 52/1000 | Loss: 0.00007947
Iteration 53/1000 | Loss: 0.00056220
Iteration 54/1000 | Loss: 0.00011731
Iteration 55/1000 | Loss: 0.00002620
Iteration 56/1000 | Loss: 0.00005104
Iteration 57/1000 | Loss: 0.00001581
Iteration 58/1000 | Loss: 0.00001581
Iteration 59/1000 | Loss: 0.00001580
Iteration 60/1000 | Loss: 0.00001579
Iteration 61/1000 | Loss: 0.00001579
Iteration 62/1000 | Loss: 0.00001577
Iteration 63/1000 | Loss: 0.00003303
Iteration 64/1000 | Loss: 0.00002945
Iteration 65/1000 | Loss: 0.00001571
Iteration 66/1000 | Loss: 0.00002048
Iteration 67/1000 | Loss: 0.00001563
Iteration 68/1000 | Loss: 0.00001559
Iteration 69/1000 | Loss: 0.00002681
Iteration 70/1000 | Loss: 0.00004440
Iteration 71/1000 | Loss: 0.00003427
Iteration 72/1000 | Loss: 0.00002812
Iteration 73/1000 | Loss: 0.00004780
Iteration 74/1000 | Loss: 0.00002239
Iteration 75/1000 | Loss: 0.00004115
Iteration 76/1000 | Loss: 0.00001545
Iteration 77/1000 | Loss: 0.00001542
Iteration 78/1000 | Loss: 0.00001542
Iteration 79/1000 | Loss: 0.00001542
Iteration 80/1000 | Loss: 0.00001542
Iteration 81/1000 | Loss: 0.00002854
Iteration 82/1000 | Loss: 0.00001667
Iteration 83/1000 | Loss: 0.00001546
Iteration 84/1000 | Loss: 0.00001544
Iteration 85/1000 | Loss: 0.00001543
Iteration 86/1000 | Loss: 0.00001543
Iteration 87/1000 | Loss: 0.00001543
Iteration 88/1000 | Loss: 0.00001543
Iteration 89/1000 | Loss: 0.00001542
Iteration 90/1000 | Loss: 0.00001542
Iteration 91/1000 | Loss: 0.00001541
Iteration 92/1000 | Loss: 0.00001541
Iteration 93/1000 | Loss: 0.00005506
Iteration 94/1000 | Loss: 0.00031097
Iteration 95/1000 | Loss: 0.00002753
Iteration 96/1000 | Loss: 0.00007018
Iteration 97/1000 | Loss: 0.00007718
Iteration 98/1000 | Loss: 0.00003213
Iteration 99/1000 | Loss: 0.00002751
Iteration 100/1000 | Loss: 0.00001549
Iteration 101/1000 | Loss: 0.00002422
Iteration 102/1000 | Loss: 0.00001543
Iteration 103/1000 | Loss: 0.00003756
Iteration 104/1000 | Loss: 0.00005719
Iteration 105/1000 | Loss: 0.00004556
Iteration 106/1000 | Loss: 0.00005844
Iteration 107/1000 | Loss: 0.00005174
Iteration 108/1000 | Loss: 0.00009060
Iteration 109/1000 | Loss: 0.00112973
Iteration 110/1000 | Loss: 0.00028780
Iteration 111/1000 | Loss: 0.00007793
Iteration 112/1000 | Loss: 0.00011635
Iteration 113/1000 | Loss: 0.00003079
Iteration 114/1000 | Loss: 0.00001644
Iteration 115/1000 | Loss: 0.00001570
Iteration 116/1000 | Loss: 0.00001546
Iteration 117/1000 | Loss: 0.00007183
Iteration 118/1000 | Loss: 0.00033787
Iteration 119/1000 | Loss: 0.00001582
Iteration 120/1000 | Loss: 0.00001539
Iteration 121/1000 | Loss: 0.00005418
Iteration 122/1000 | Loss: 0.00015629
Iteration 123/1000 | Loss: 0.00001562
Iteration 124/1000 | Loss: 0.00004267
Iteration 125/1000 | Loss: 0.00001530
Iteration 126/1000 | Loss: 0.00003604
Iteration 127/1000 | Loss: 0.00001530
Iteration 128/1000 | Loss: 0.00002426
Iteration 129/1000 | Loss: 0.00001526
Iteration 130/1000 | Loss: 0.00001526
Iteration 131/1000 | Loss: 0.00001526
Iteration 132/1000 | Loss: 0.00001525
Iteration 133/1000 | Loss: 0.00001525
Iteration 134/1000 | Loss: 0.00001525
Iteration 135/1000 | Loss: 0.00001525
Iteration 136/1000 | Loss: 0.00001525
Iteration 137/1000 | Loss: 0.00002802
Iteration 138/1000 | Loss: 0.00005814
Iteration 139/1000 | Loss: 0.00003417
Iteration 140/1000 | Loss: 0.00004735
Iteration 141/1000 | Loss: 0.00001939
Iteration 142/1000 | Loss: 0.00001527
Iteration 143/1000 | Loss: 0.00001525
Iteration 144/1000 | Loss: 0.00001525
Iteration 145/1000 | Loss: 0.00001525
Iteration 146/1000 | Loss: 0.00001525
Iteration 147/1000 | Loss: 0.00001525
Iteration 148/1000 | Loss: 0.00001524
Iteration 149/1000 | Loss: 0.00001524
Iteration 150/1000 | Loss: 0.00001524
Iteration 151/1000 | Loss: 0.00001524
Iteration 152/1000 | Loss: 0.00001524
Iteration 153/1000 | Loss: 0.00001524
Iteration 154/1000 | Loss: 0.00001524
Iteration 155/1000 | Loss: 0.00001523
Iteration 156/1000 | Loss: 0.00001523
Iteration 157/1000 | Loss: 0.00001523
Iteration 158/1000 | Loss: 0.00001523
Iteration 159/1000 | Loss: 0.00001523
Iteration 160/1000 | Loss: 0.00001523
Iteration 161/1000 | Loss: 0.00002912
Iteration 162/1000 | Loss: 0.00001778
Iteration 163/1000 | Loss: 0.00001526
Iteration 164/1000 | Loss: 0.00001526
Iteration 165/1000 | Loss: 0.00001526
Iteration 166/1000 | Loss: 0.00001525
Iteration 167/1000 | Loss: 0.00001525
Iteration 168/1000 | Loss: 0.00001525
Iteration 169/1000 | Loss: 0.00001525
Iteration 170/1000 | Loss: 0.00001525
Iteration 171/1000 | Loss: 0.00001525
Iteration 172/1000 | Loss: 0.00001525
Iteration 173/1000 | Loss: 0.00001525
Iteration 174/1000 | Loss: 0.00001524
Iteration 175/1000 | Loss: 0.00001524
Iteration 176/1000 | Loss: 0.00001524
Iteration 177/1000 | Loss: 0.00001524
Iteration 178/1000 | Loss: 0.00001524
Iteration 179/1000 | Loss: 0.00001524
Iteration 180/1000 | Loss: 0.00001524
Iteration 181/1000 | Loss: 0.00001524
Iteration 182/1000 | Loss: 0.00001524
Iteration 183/1000 | Loss: 0.00001524
Iteration 184/1000 | Loss: 0.00001524
Iteration 185/1000 | Loss: 0.00001524
Iteration 186/1000 | Loss: 0.00001524
Iteration 187/1000 | Loss: 0.00001524
Iteration 188/1000 | Loss: 0.00001523
Iteration 189/1000 | Loss: 0.00001523
Iteration 190/1000 | Loss: 0.00001523
Iteration 191/1000 | Loss: 0.00001523
Iteration 192/1000 | Loss: 0.00001523
Iteration 193/1000 | Loss: 0.00001523
Iteration 194/1000 | Loss: 0.00001523
Iteration 195/1000 | Loss: 0.00001523
Iteration 196/1000 | Loss: 0.00001522
Iteration 197/1000 | Loss: 0.00001522
Iteration 198/1000 | Loss: 0.00001522
Iteration 199/1000 | Loss: 0.00001522
Iteration 200/1000 | Loss: 0.00001522
Iteration 201/1000 | Loss: 0.00001522
Iteration 202/1000 | Loss: 0.00001522
Iteration 203/1000 | Loss: 0.00001522
Iteration 204/1000 | Loss: 0.00001522
Iteration 205/1000 | Loss: 0.00001522
Iteration 206/1000 | Loss: 0.00001522
Iteration 207/1000 | Loss: 0.00001522
Iteration 208/1000 | Loss: 0.00001522
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [1.5223252376017626e-05, 1.5223252376017626e-05, 1.5223252376017626e-05, 1.5223252376017626e-05, 1.5223252376017626e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5223252376017626e-05

Optimization complete. Final v2v error: 3.247356414794922 mm

Highest mean error: 3.5766217708587646 mm for frame 81

Lowest mean error: 2.885392189025879 mm for frame 103

Saving results

Total time: 200.44568872451782
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00818784
Iteration 2/25 | Loss: 0.00146061
Iteration 3/25 | Loss: 0.00120877
Iteration 4/25 | Loss: 0.00117187
Iteration 5/25 | Loss: 0.00118148
Iteration 6/25 | Loss: 0.00115240
Iteration 7/25 | Loss: 0.00113362
Iteration 8/25 | Loss: 0.00113642
Iteration 9/25 | Loss: 0.00112630
Iteration 10/25 | Loss: 0.00111672
Iteration 11/25 | Loss: 0.00111358
Iteration 12/25 | Loss: 0.00111273
Iteration 13/25 | Loss: 0.00111251
Iteration 14/25 | Loss: 0.00111243
Iteration 15/25 | Loss: 0.00111233
Iteration 16/25 | Loss: 0.00111939
Iteration 17/25 | Loss: 0.00111971
Iteration 18/25 | Loss: 0.00111322
Iteration 19/25 | Loss: 0.00111098
Iteration 20/25 | Loss: 0.00110973
Iteration 21/25 | Loss: 0.00110938
Iteration 22/25 | Loss: 0.00110922
Iteration 23/25 | Loss: 0.00110918
Iteration 24/25 | Loss: 0.00110918
Iteration 25/25 | Loss: 0.00110918

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27984560
Iteration 2/25 | Loss: 0.00052171
Iteration 3/25 | Loss: 0.00052168
Iteration 4/25 | Loss: 0.00052168
Iteration 5/25 | Loss: 0.00052168
Iteration 6/25 | Loss: 0.00052168
Iteration 7/25 | Loss: 0.00052168
Iteration 8/25 | Loss: 0.00052168
Iteration 9/25 | Loss: 0.00052168
Iteration 10/25 | Loss: 0.00052168
Iteration 11/25 | Loss: 0.00052168
Iteration 12/25 | Loss: 0.00052168
Iteration 13/25 | Loss: 0.00052168
Iteration 14/25 | Loss: 0.00052168
Iteration 15/25 | Loss: 0.00052168
Iteration 16/25 | Loss: 0.00052168
Iteration 17/25 | Loss: 0.00052168
Iteration 18/25 | Loss: 0.00052168
Iteration 19/25 | Loss: 0.00052168
Iteration 20/25 | Loss: 0.00052168
Iteration 21/25 | Loss: 0.00052168
Iteration 22/25 | Loss: 0.00052168
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0005216760910116136, 0.0005216760910116136, 0.0005216760910116136, 0.0005216760910116136, 0.0005216760910116136]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005216760910116136

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052168
Iteration 2/1000 | Loss: 0.00003451
Iteration 3/1000 | Loss: 0.00002474
Iteration 4/1000 | Loss: 0.00001994
Iteration 5/1000 | Loss: 0.00001824
Iteration 6/1000 | Loss: 0.00001756
Iteration 7/1000 | Loss: 0.00001688
Iteration 8/1000 | Loss: 0.00001649
Iteration 9/1000 | Loss: 0.00001612
Iteration 10/1000 | Loss: 0.00001583
Iteration 11/1000 | Loss: 0.00001580
Iteration 12/1000 | Loss: 0.00001571
Iteration 13/1000 | Loss: 0.00001548
Iteration 14/1000 | Loss: 0.00001537
Iteration 15/1000 | Loss: 0.00001527
Iteration 16/1000 | Loss: 0.00001524
Iteration 17/1000 | Loss: 0.00001522
Iteration 18/1000 | Loss: 0.00001518
Iteration 19/1000 | Loss: 0.00001510
Iteration 20/1000 | Loss: 0.00001506
Iteration 21/1000 | Loss: 0.00001499
Iteration 22/1000 | Loss: 0.00001497
Iteration 23/1000 | Loss: 0.00001497
Iteration 24/1000 | Loss: 0.00001496
Iteration 25/1000 | Loss: 0.00001495
Iteration 26/1000 | Loss: 0.00001495
Iteration 27/1000 | Loss: 0.00001495
Iteration 28/1000 | Loss: 0.00001495
Iteration 29/1000 | Loss: 0.00001494
Iteration 30/1000 | Loss: 0.00001493
Iteration 31/1000 | Loss: 0.00001493
Iteration 32/1000 | Loss: 0.00001493
Iteration 33/1000 | Loss: 0.00001493
Iteration 34/1000 | Loss: 0.00001492
Iteration 35/1000 | Loss: 0.00001492
Iteration 36/1000 | Loss: 0.00001492
Iteration 37/1000 | Loss: 0.00001492
Iteration 38/1000 | Loss: 0.00001491
Iteration 39/1000 | Loss: 0.00001491
Iteration 40/1000 | Loss: 0.00001491
Iteration 41/1000 | Loss: 0.00001491
Iteration 42/1000 | Loss: 0.00001490
Iteration 43/1000 | Loss: 0.00001490
Iteration 44/1000 | Loss: 0.00001490
Iteration 45/1000 | Loss: 0.00001489
Iteration 46/1000 | Loss: 0.00001489
Iteration 47/1000 | Loss: 0.00001489
Iteration 48/1000 | Loss: 0.00001488
Iteration 49/1000 | Loss: 0.00001488
Iteration 50/1000 | Loss: 0.00001488
Iteration 51/1000 | Loss: 0.00001487
Iteration 52/1000 | Loss: 0.00001487
Iteration 53/1000 | Loss: 0.00001487
Iteration 54/1000 | Loss: 0.00001487
Iteration 55/1000 | Loss: 0.00001487
Iteration 56/1000 | Loss: 0.00001486
Iteration 57/1000 | Loss: 0.00001486
Iteration 58/1000 | Loss: 0.00001486
Iteration 59/1000 | Loss: 0.00001486
Iteration 60/1000 | Loss: 0.00001486
Iteration 61/1000 | Loss: 0.00001486
Iteration 62/1000 | Loss: 0.00001486
Iteration 63/1000 | Loss: 0.00001485
Iteration 64/1000 | Loss: 0.00001485
Iteration 65/1000 | Loss: 0.00001485
Iteration 66/1000 | Loss: 0.00001485
Iteration 67/1000 | Loss: 0.00001485
Iteration 68/1000 | Loss: 0.00001485
Iteration 69/1000 | Loss: 0.00001485
Iteration 70/1000 | Loss: 0.00001485
Iteration 71/1000 | Loss: 0.00001485
Iteration 72/1000 | Loss: 0.00001485
Iteration 73/1000 | Loss: 0.00001485
Iteration 74/1000 | Loss: 0.00001485
Iteration 75/1000 | Loss: 0.00001485
Iteration 76/1000 | Loss: 0.00001485
Iteration 77/1000 | Loss: 0.00001485
Iteration 78/1000 | Loss: 0.00001484
Iteration 79/1000 | Loss: 0.00001484
Iteration 80/1000 | Loss: 0.00001484
Iteration 81/1000 | Loss: 0.00001484
Iteration 82/1000 | Loss: 0.00001484
Iteration 83/1000 | Loss: 0.00001484
Iteration 84/1000 | Loss: 0.00001484
Iteration 85/1000 | Loss: 0.00001484
Iteration 86/1000 | Loss: 0.00001484
Iteration 87/1000 | Loss: 0.00001484
Iteration 88/1000 | Loss: 0.00001484
Iteration 89/1000 | Loss: 0.00001484
Iteration 90/1000 | Loss: 0.00001484
Iteration 91/1000 | Loss: 0.00001484
Iteration 92/1000 | Loss: 0.00001483
Iteration 93/1000 | Loss: 0.00001483
Iteration 94/1000 | Loss: 0.00001483
Iteration 95/1000 | Loss: 0.00001483
Iteration 96/1000 | Loss: 0.00001483
Iteration 97/1000 | Loss: 0.00001483
Iteration 98/1000 | Loss: 0.00001483
Iteration 99/1000 | Loss: 0.00001483
Iteration 100/1000 | Loss: 0.00001483
Iteration 101/1000 | Loss: 0.00001483
Iteration 102/1000 | Loss: 0.00001483
Iteration 103/1000 | Loss: 0.00001483
Iteration 104/1000 | Loss: 0.00001483
Iteration 105/1000 | Loss: 0.00001483
Iteration 106/1000 | Loss: 0.00001483
Iteration 107/1000 | Loss: 0.00001483
Iteration 108/1000 | Loss: 0.00001483
Iteration 109/1000 | Loss: 0.00001483
Iteration 110/1000 | Loss: 0.00001483
Iteration 111/1000 | Loss: 0.00001483
Iteration 112/1000 | Loss: 0.00001483
Iteration 113/1000 | Loss: 0.00001483
Iteration 114/1000 | Loss: 0.00001483
Iteration 115/1000 | Loss: 0.00001483
Iteration 116/1000 | Loss: 0.00001483
Iteration 117/1000 | Loss: 0.00001483
Iteration 118/1000 | Loss: 0.00001483
Iteration 119/1000 | Loss: 0.00001483
Iteration 120/1000 | Loss: 0.00001483
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [1.4829625797574408e-05, 1.4829625797574408e-05, 1.4829625797574408e-05, 1.4829625797574408e-05, 1.4829625797574408e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4829625797574408e-05

Optimization complete. Final v2v error: 3.1400582790374756 mm

Highest mean error: 3.7044713497161865 mm for frame 58

Lowest mean error: 2.6074838638305664 mm for frame 19

Saving results

Total time: 67.32816672325134
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01105164
Iteration 2/25 | Loss: 0.00372591
Iteration 3/25 | Loss: 0.00294715
Iteration 4/25 | Loss: 0.00213603
Iteration 5/25 | Loss: 0.00189806
Iteration 6/25 | Loss: 0.00178011
Iteration 7/25 | Loss: 0.00170241
Iteration 8/25 | Loss: 0.00168632
Iteration 9/25 | Loss: 0.00166377
Iteration 10/25 | Loss: 0.00164984
Iteration 11/25 | Loss: 0.00164216
Iteration 12/25 | Loss: 0.00163934
Iteration 13/25 | Loss: 0.00163577
Iteration 14/25 | Loss: 0.00163535
Iteration 15/25 | Loss: 0.00162803
Iteration 16/25 | Loss: 0.00162078
Iteration 17/25 | Loss: 0.00161021
Iteration 18/25 | Loss: 0.00161409
Iteration 19/25 | Loss: 0.00160888
Iteration 20/25 | Loss: 0.00160416
Iteration 21/25 | Loss: 0.00160160
Iteration 22/25 | Loss: 0.00160052
Iteration 23/25 | Loss: 0.00160014
Iteration 24/25 | Loss: 0.00160001
Iteration 25/25 | Loss: 0.00159995

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.42942700
Iteration 2/25 | Loss: 0.00976104
Iteration 3/25 | Loss: 0.00137779
Iteration 4/25 | Loss: 0.00137775
Iteration 5/25 | Loss: 0.00137775
Iteration 6/25 | Loss: 0.00137775
Iteration 7/25 | Loss: 0.00137775
Iteration 8/25 | Loss: 0.00137775
Iteration 9/25 | Loss: 0.00137775
Iteration 10/25 | Loss: 0.00137775
Iteration 11/25 | Loss: 0.00137775
Iteration 12/25 | Loss: 0.00137775
Iteration 13/25 | Loss: 0.00137775
Iteration 14/25 | Loss: 0.00137775
Iteration 15/25 | Loss: 0.00137775
Iteration 16/25 | Loss: 0.00137775
Iteration 17/25 | Loss: 0.00137775
Iteration 18/25 | Loss: 0.00137775
Iteration 19/25 | Loss: 0.00137775
Iteration 20/25 | Loss: 0.00137775
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0013777489075437188, 0.0013777489075437188, 0.0013777489075437188, 0.0013777489075437188, 0.0013777489075437188]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013777489075437188

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137775
Iteration 2/1000 | Loss: 0.00214283
Iteration 3/1000 | Loss: 0.00251985
Iteration 4/1000 | Loss: 0.00291418
Iteration 5/1000 | Loss: 0.00285941
Iteration 6/1000 | Loss: 0.00276043
Iteration 7/1000 | Loss: 0.00316005
Iteration 8/1000 | Loss: 0.00325798
Iteration 9/1000 | Loss: 0.00195164
Iteration 10/1000 | Loss: 0.00251644
Iteration 11/1000 | Loss: 0.00086651
Iteration 12/1000 | Loss: 0.00097005
Iteration 13/1000 | Loss: 0.00085801
Iteration 14/1000 | Loss: 0.00131210
Iteration 15/1000 | Loss: 0.00104488
Iteration 16/1000 | Loss: 0.00093279
Iteration 17/1000 | Loss: 0.00050468
Iteration 18/1000 | Loss: 0.00045178
Iteration 19/1000 | Loss: 0.00028619
Iteration 20/1000 | Loss: 0.00030790
Iteration 21/1000 | Loss: 0.00035075
Iteration 22/1000 | Loss: 0.00046225
Iteration 23/1000 | Loss: 0.00023483
Iteration 24/1000 | Loss: 0.00012696
Iteration 25/1000 | Loss: 0.00010977
Iteration 26/1000 | Loss: 0.00020170
Iteration 27/1000 | Loss: 0.00020212
Iteration 28/1000 | Loss: 0.00022498
Iteration 29/1000 | Loss: 0.00010075
Iteration 30/1000 | Loss: 0.00008911
Iteration 31/1000 | Loss: 0.00008280
Iteration 32/1000 | Loss: 0.00007734
Iteration 33/1000 | Loss: 0.00007215
Iteration 34/1000 | Loss: 0.00006878
Iteration 35/1000 | Loss: 0.00006618
Iteration 36/1000 | Loss: 0.00006423
Iteration 37/1000 | Loss: 0.00006293
Iteration 38/1000 | Loss: 0.00006174
Iteration 39/1000 | Loss: 0.00006061
Iteration 40/1000 | Loss: 0.00005974
Iteration 41/1000 | Loss: 0.00005908
Iteration 42/1000 | Loss: 0.00005840
Iteration 43/1000 | Loss: 0.00005793
Iteration 44/1000 | Loss: 0.00005749
Iteration 45/1000 | Loss: 0.00005724
Iteration 46/1000 | Loss: 0.00005698
Iteration 47/1000 | Loss: 0.00005680
Iteration 48/1000 | Loss: 0.00005678
Iteration 49/1000 | Loss: 0.00005662
Iteration 50/1000 | Loss: 0.00005660
Iteration 51/1000 | Loss: 0.00005654
Iteration 52/1000 | Loss: 0.00005652
Iteration 53/1000 | Loss: 0.00005635
Iteration 54/1000 | Loss: 0.00005619
Iteration 55/1000 | Loss: 0.00005613
Iteration 56/1000 | Loss: 0.00005609
Iteration 57/1000 | Loss: 0.00005609
Iteration 58/1000 | Loss: 0.00005608
Iteration 59/1000 | Loss: 0.00005607
Iteration 60/1000 | Loss: 0.00005606
Iteration 61/1000 | Loss: 0.00005606
Iteration 62/1000 | Loss: 0.00005604
Iteration 63/1000 | Loss: 0.00005604
Iteration 64/1000 | Loss: 0.00005603
Iteration 65/1000 | Loss: 0.00005603
Iteration 66/1000 | Loss: 0.00005603
Iteration 67/1000 | Loss: 0.00005603
Iteration 68/1000 | Loss: 0.00005603
Iteration 69/1000 | Loss: 0.00005603
Iteration 70/1000 | Loss: 0.00005603
Iteration 71/1000 | Loss: 0.00005602
Iteration 72/1000 | Loss: 0.00005601
Iteration 73/1000 | Loss: 0.00005601
Iteration 74/1000 | Loss: 0.00005600
Iteration 75/1000 | Loss: 0.00005600
Iteration 76/1000 | Loss: 0.00005600
Iteration 77/1000 | Loss: 0.00005600
Iteration 78/1000 | Loss: 0.00005600
Iteration 79/1000 | Loss: 0.00005600
Iteration 80/1000 | Loss: 0.00005600
Iteration 81/1000 | Loss: 0.00005600
Iteration 82/1000 | Loss: 0.00005600
Iteration 83/1000 | Loss: 0.00005600
Iteration 84/1000 | Loss: 0.00005600
Iteration 85/1000 | Loss: 0.00005600
Iteration 86/1000 | Loss: 0.00005600
Iteration 87/1000 | Loss: 0.00005599
Iteration 88/1000 | Loss: 0.00005599
Iteration 89/1000 | Loss: 0.00005598
Iteration 90/1000 | Loss: 0.00005598
Iteration 91/1000 | Loss: 0.00005597
Iteration 92/1000 | Loss: 0.00005597
Iteration 93/1000 | Loss: 0.00005597
Iteration 94/1000 | Loss: 0.00005597
Iteration 95/1000 | Loss: 0.00005597
Iteration 96/1000 | Loss: 0.00005597
Iteration 97/1000 | Loss: 0.00005597
Iteration 98/1000 | Loss: 0.00005597
Iteration 99/1000 | Loss: 0.00005597
Iteration 100/1000 | Loss: 0.00005597
Iteration 101/1000 | Loss: 0.00005596
Iteration 102/1000 | Loss: 0.00005595
Iteration 103/1000 | Loss: 0.00005595
Iteration 104/1000 | Loss: 0.00005595
Iteration 105/1000 | Loss: 0.00005594
Iteration 106/1000 | Loss: 0.00005594
Iteration 107/1000 | Loss: 0.00005594
Iteration 108/1000 | Loss: 0.00005594
Iteration 109/1000 | Loss: 0.00005594
Iteration 110/1000 | Loss: 0.00005594
Iteration 111/1000 | Loss: 0.00005594
Iteration 112/1000 | Loss: 0.00005594
Iteration 113/1000 | Loss: 0.00005594
Iteration 114/1000 | Loss: 0.00005593
Iteration 115/1000 | Loss: 0.00005593
Iteration 116/1000 | Loss: 0.00005593
Iteration 117/1000 | Loss: 0.00005593
Iteration 118/1000 | Loss: 0.00005593
Iteration 119/1000 | Loss: 0.00005593
Iteration 120/1000 | Loss: 0.00005593
Iteration 121/1000 | Loss: 0.00005592
Iteration 122/1000 | Loss: 0.00005592
Iteration 123/1000 | Loss: 0.00005592
Iteration 124/1000 | Loss: 0.00005592
Iteration 125/1000 | Loss: 0.00044604
Iteration 126/1000 | Loss: 0.00055848
Iteration 127/1000 | Loss: 0.00011483
Iteration 128/1000 | Loss: 0.00029252
Iteration 129/1000 | Loss: 0.00024700
Iteration 130/1000 | Loss: 0.00008103
Iteration 131/1000 | Loss: 0.00006272
Iteration 132/1000 | Loss: 0.00005984
Iteration 133/1000 | Loss: 0.00005870
Iteration 134/1000 | Loss: 0.00005798
Iteration 135/1000 | Loss: 0.00005725
Iteration 136/1000 | Loss: 0.00005660
Iteration 137/1000 | Loss: 0.00005621
Iteration 138/1000 | Loss: 0.00005603
Iteration 139/1000 | Loss: 0.00005597
Iteration 140/1000 | Loss: 0.00005597
Iteration 141/1000 | Loss: 0.00005597
Iteration 142/1000 | Loss: 0.00005597
Iteration 143/1000 | Loss: 0.00005597
Iteration 144/1000 | Loss: 0.00005597
Iteration 145/1000 | Loss: 0.00005597
Iteration 146/1000 | Loss: 0.00005597
Iteration 147/1000 | Loss: 0.00005597
Iteration 148/1000 | Loss: 0.00005597
Iteration 149/1000 | Loss: 0.00005597
Iteration 150/1000 | Loss: 0.00005597
Iteration 151/1000 | Loss: 0.00005596
Iteration 152/1000 | Loss: 0.00005596
Iteration 153/1000 | Loss: 0.00005596
Iteration 154/1000 | Loss: 0.00005596
Iteration 155/1000 | Loss: 0.00005596
Iteration 156/1000 | Loss: 0.00005596
Iteration 157/1000 | Loss: 0.00005596
Iteration 158/1000 | Loss: 0.00005596
Iteration 159/1000 | Loss: 0.00005596
Iteration 160/1000 | Loss: 0.00005595
Iteration 161/1000 | Loss: 0.00005595
Iteration 162/1000 | Loss: 0.00005595
Iteration 163/1000 | Loss: 0.00005595
Iteration 164/1000 | Loss: 0.00005595
Iteration 165/1000 | Loss: 0.00005595
Iteration 166/1000 | Loss: 0.00005595
Iteration 167/1000 | Loss: 0.00005594
Iteration 168/1000 | Loss: 0.00005594
Iteration 169/1000 | Loss: 0.00005594
Iteration 170/1000 | Loss: 0.00005594
Iteration 171/1000 | Loss: 0.00005594
Iteration 172/1000 | Loss: 0.00005594
Iteration 173/1000 | Loss: 0.00005594
Iteration 174/1000 | Loss: 0.00005594
Iteration 175/1000 | Loss: 0.00005594
Iteration 176/1000 | Loss: 0.00005593
Iteration 177/1000 | Loss: 0.00005593
Iteration 178/1000 | Loss: 0.00005593
Iteration 179/1000 | Loss: 0.00005593
Iteration 180/1000 | Loss: 0.00005593
Iteration 181/1000 | Loss: 0.00005593
Iteration 182/1000 | Loss: 0.00005593
Iteration 183/1000 | Loss: 0.00005593
Iteration 184/1000 | Loss: 0.00005593
Iteration 185/1000 | Loss: 0.00005593
Iteration 186/1000 | Loss: 0.00005592
Iteration 187/1000 | Loss: 0.00005592
Iteration 188/1000 | Loss: 0.00005592
Iteration 189/1000 | Loss: 0.00005592
Iteration 190/1000 | Loss: 0.00005592
Iteration 191/1000 | Loss: 0.00005592
Iteration 192/1000 | Loss: 0.00005592
Iteration 193/1000 | Loss: 0.00005592
Iteration 194/1000 | Loss: 0.00005592
Iteration 195/1000 | Loss: 0.00005592
Iteration 196/1000 | Loss: 0.00005591
Iteration 197/1000 | Loss: 0.00005591
Iteration 198/1000 | Loss: 0.00005591
Iteration 199/1000 | Loss: 0.00005591
Iteration 200/1000 | Loss: 0.00005591
Iteration 201/1000 | Loss: 0.00005591
Iteration 202/1000 | Loss: 0.00005591
Iteration 203/1000 | Loss: 0.00005591
Iteration 204/1000 | Loss: 0.00005591
Iteration 205/1000 | Loss: 0.00005591
Iteration 206/1000 | Loss: 0.00005591
Iteration 207/1000 | Loss: 0.00005591
Iteration 208/1000 | Loss: 0.00005591
Iteration 209/1000 | Loss: 0.00005591
Iteration 210/1000 | Loss: 0.00005591
Iteration 211/1000 | Loss: 0.00005591
Iteration 212/1000 | Loss: 0.00005591
Iteration 213/1000 | Loss: 0.00005591
Iteration 214/1000 | Loss: 0.00005591
Iteration 215/1000 | Loss: 0.00005591
Iteration 216/1000 | Loss: 0.00005591
Iteration 217/1000 | Loss: 0.00005591
Iteration 218/1000 | Loss: 0.00005591
Iteration 219/1000 | Loss: 0.00005591
Iteration 220/1000 | Loss: 0.00005591
Iteration 221/1000 | Loss: 0.00005591
Iteration 222/1000 | Loss: 0.00005591
Iteration 223/1000 | Loss: 0.00005591
Iteration 224/1000 | Loss: 0.00005591
Iteration 225/1000 | Loss: 0.00005591
Iteration 226/1000 | Loss: 0.00005591
Iteration 227/1000 | Loss: 0.00005591
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [5.5908971262397245e-05, 5.5908971262397245e-05, 5.5908971262397245e-05, 5.5908971262397245e-05, 5.5908971262397245e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.5908971262397245e-05

Optimization complete. Final v2v error: 5.823050498962402 mm

Highest mean error: 13.729883193969727 mm for frame 73

Lowest mean error: 4.726354598999023 mm for frame 121

Saving results

Total time: 147.45070457458496
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00669415
Iteration 2/25 | Loss: 0.00141561
Iteration 3/25 | Loss: 0.00120490
Iteration 4/25 | Loss: 0.00118701
Iteration 5/25 | Loss: 0.00118486
Iteration 6/25 | Loss: 0.00118484
Iteration 7/25 | Loss: 0.00118484
Iteration 8/25 | Loss: 0.00118484
Iteration 9/25 | Loss: 0.00118484
Iteration 10/25 | Loss: 0.00118484
Iteration 11/25 | Loss: 0.00118484
Iteration 12/25 | Loss: 0.00118484
Iteration 13/25 | Loss: 0.00118484
Iteration 14/25 | Loss: 0.00118484
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0011848417343571782, 0.0011848417343571782, 0.0011848417343571782, 0.0011848417343571782, 0.0011848417343571782]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011848417343571782

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.69102645
Iteration 2/25 | Loss: 0.00076476
Iteration 3/25 | Loss: 0.00076473
Iteration 4/25 | Loss: 0.00076473
Iteration 5/25 | Loss: 0.00076473
Iteration 6/25 | Loss: 0.00076473
Iteration 7/25 | Loss: 0.00076473
Iteration 8/25 | Loss: 0.00076473
Iteration 9/25 | Loss: 0.00076473
Iteration 10/25 | Loss: 0.00076473
Iteration 11/25 | Loss: 0.00076473
Iteration 12/25 | Loss: 0.00076473
Iteration 13/25 | Loss: 0.00076473
Iteration 14/25 | Loss: 0.00076473
Iteration 15/25 | Loss: 0.00076473
Iteration 16/25 | Loss: 0.00076473
Iteration 17/25 | Loss: 0.00076473
Iteration 18/25 | Loss: 0.00076473
Iteration 19/25 | Loss: 0.00076473
Iteration 20/25 | Loss: 0.00076473
Iteration 21/25 | Loss: 0.00076473
Iteration 22/25 | Loss: 0.00076473
Iteration 23/25 | Loss: 0.00076473
Iteration 24/25 | Loss: 0.00076473
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007647289312444627, 0.0007647289312444627, 0.0007647289312444627, 0.0007647289312444627, 0.0007647289312444627]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007647289312444627

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076473
Iteration 2/1000 | Loss: 0.00003360
Iteration 3/1000 | Loss: 0.00002310
Iteration 4/1000 | Loss: 0.00001982
Iteration 5/1000 | Loss: 0.00001857
Iteration 6/1000 | Loss: 0.00001801
Iteration 7/1000 | Loss: 0.00001757
Iteration 8/1000 | Loss: 0.00001734
Iteration 9/1000 | Loss: 0.00001705
Iteration 10/1000 | Loss: 0.00001681
Iteration 11/1000 | Loss: 0.00001671
Iteration 12/1000 | Loss: 0.00001666
Iteration 13/1000 | Loss: 0.00001653
Iteration 14/1000 | Loss: 0.00001652
Iteration 15/1000 | Loss: 0.00001651
Iteration 16/1000 | Loss: 0.00001644
Iteration 17/1000 | Loss: 0.00001641
Iteration 18/1000 | Loss: 0.00001640
Iteration 19/1000 | Loss: 0.00001639
Iteration 20/1000 | Loss: 0.00001638
Iteration 21/1000 | Loss: 0.00001638
Iteration 22/1000 | Loss: 0.00001637
Iteration 23/1000 | Loss: 0.00001636
Iteration 24/1000 | Loss: 0.00001636
Iteration 25/1000 | Loss: 0.00001635
Iteration 26/1000 | Loss: 0.00001634
Iteration 27/1000 | Loss: 0.00001633
Iteration 28/1000 | Loss: 0.00001632
Iteration 29/1000 | Loss: 0.00001632
Iteration 30/1000 | Loss: 0.00001628
Iteration 31/1000 | Loss: 0.00001627
Iteration 32/1000 | Loss: 0.00001626
Iteration 33/1000 | Loss: 0.00001626
Iteration 34/1000 | Loss: 0.00001623
Iteration 35/1000 | Loss: 0.00001622
Iteration 36/1000 | Loss: 0.00001622
Iteration 37/1000 | Loss: 0.00001621
Iteration 38/1000 | Loss: 0.00001621
Iteration 39/1000 | Loss: 0.00001621
Iteration 40/1000 | Loss: 0.00001620
Iteration 41/1000 | Loss: 0.00001620
Iteration 42/1000 | Loss: 0.00001619
Iteration 43/1000 | Loss: 0.00001618
Iteration 44/1000 | Loss: 0.00001618
Iteration 45/1000 | Loss: 0.00001617
Iteration 46/1000 | Loss: 0.00001617
Iteration 47/1000 | Loss: 0.00001617
Iteration 48/1000 | Loss: 0.00001616
Iteration 49/1000 | Loss: 0.00001616
Iteration 50/1000 | Loss: 0.00001616
Iteration 51/1000 | Loss: 0.00001615
Iteration 52/1000 | Loss: 0.00001615
Iteration 53/1000 | Loss: 0.00001615
Iteration 54/1000 | Loss: 0.00001615
Iteration 55/1000 | Loss: 0.00001615
Iteration 56/1000 | Loss: 0.00001614
Iteration 57/1000 | Loss: 0.00001614
Iteration 58/1000 | Loss: 0.00001614
Iteration 59/1000 | Loss: 0.00001613
Iteration 60/1000 | Loss: 0.00001613
Iteration 61/1000 | Loss: 0.00001613
Iteration 62/1000 | Loss: 0.00001612
Iteration 63/1000 | Loss: 0.00001612
Iteration 64/1000 | Loss: 0.00001612
Iteration 65/1000 | Loss: 0.00001612
Iteration 66/1000 | Loss: 0.00001612
Iteration 67/1000 | Loss: 0.00001611
Iteration 68/1000 | Loss: 0.00001611
Iteration 69/1000 | Loss: 0.00001611
Iteration 70/1000 | Loss: 0.00001611
Iteration 71/1000 | Loss: 0.00001610
Iteration 72/1000 | Loss: 0.00001610
Iteration 73/1000 | Loss: 0.00001609
Iteration 74/1000 | Loss: 0.00001609
Iteration 75/1000 | Loss: 0.00001609
Iteration 76/1000 | Loss: 0.00001609
Iteration 77/1000 | Loss: 0.00001608
Iteration 78/1000 | Loss: 0.00001608
Iteration 79/1000 | Loss: 0.00001607
Iteration 80/1000 | Loss: 0.00001607
Iteration 81/1000 | Loss: 0.00001607
Iteration 82/1000 | Loss: 0.00001607
Iteration 83/1000 | Loss: 0.00001607
Iteration 84/1000 | Loss: 0.00001606
Iteration 85/1000 | Loss: 0.00001606
Iteration 86/1000 | Loss: 0.00001605
Iteration 87/1000 | Loss: 0.00001605
Iteration 88/1000 | Loss: 0.00001605
Iteration 89/1000 | Loss: 0.00001604
Iteration 90/1000 | Loss: 0.00001604
Iteration 91/1000 | Loss: 0.00001604
Iteration 92/1000 | Loss: 0.00001604
Iteration 93/1000 | Loss: 0.00001603
Iteration 94/1000 | Loss: 0.00001603
Iteration 95/1000 | Loss: 0.00001603
Iteration 96/1000 | Loss: 0.00001603
Iteration 97/1000 | Loss: 0.00001603
Iteration 98/1000 | Loss: 0.00001603
Iteration 99/1000 | Loss: 0.00001603
Iteration 100/1000 | Loss: 0.00001603
Iteration 101/1000 | Loss: 0.00001603
Iteration 102/1000 | Loss: 0.00001603
Iteration 103/1000 | Loss: 0.00001603
Iteration 104/1000 | Loss: 0.00001603
Iteration 105/1000 | Loss: 0.00001602
Iteration 106/1000 | Loss: 0.00001602
Iteration 107/1000 | Loss: 0.00001602
Iteration 108/1000 | Loss: 0.00001602
Iteration 109/1000 | Loss: 0.00001602
Iteration 110/1000 | Loss: 0.00001601
Iteration 111/1000 | Loss: 0.00001601
Iteration 112/1000 | Loss: 0.00001601
Iteration 113/1000 | Loss: 0.00001601
Iteration 114/1000 | Loss: 0.00001601
Iteration 115/1000 | Loss: 0.00001601
Iteration 116/1000 | Loss: 0.00001601
Iteration 117/1000 | Loss: 0.00001600
Iteration 118/1000 | Loss: 0.00001600
Iteration 119/1000 | Loss: 0.00001600
Iteration 120/1000 | Loss: 0.00001600
Iteration 121/1000 | Loss: 0.00001600
Iteration 122/1000 | Loss: 0.00001599
Iteration 123/1000 | Loss: 0.00001599
Iteration 124/1000 | Loss: 0.00001598
Iteration 125/1000 | Loss: 0.00001598
Iteration 126/1000 | Loss: 0.00001598
Iteration 127/1000 | Loss: 0.00001598
Iteration 128/1000 | Loss: 0.00001598
Iteration 129/1000 | Loss: 0.00001598
Iteration 130/1000 | Loss: 0.00001597
Iteration 131/1000 | Loss: 0.00001597
Iteration 132/1000 | Loss: 0.00001597
Iteration 133/1000 | Loss: 0.00001597
Iteration 134/1000 | Loss: 0.00001597
Iteration 135/1000 | Loss: 0.00001597
Iteration 136/1000 | Loss: 0.00001597
Iteration 137/1000 | Loss: 0.00001597
Iteration 138/1000 | Loss: 0.00001597
Iteration 139/1000 | Loss: 0.00001597
Iteration 140/1000 | Loss: 0.00001597
Iteration 141/1000 | Loss: 0.00001597
Iteration 142/1000 | Loss: 0.00001597
Iteration 143/1000 | Loss: 0.00001597
Iteration 144/1000 | Loss: 0.00001597
Iteration 145/1000 | Loss: 0.00001597
Iteration 146/1000 | Loss: 0.00001597
Iteration 147/1000 | Loss: 0.00001597
Iteration 148/1000 | Loss: 0.00001597
Iteration 149/1000 | Loss: 0.00001597
Iteration 150/1000 | Loss: 0.00001597
Iteration 151/1000 | Loss: 0.00001597
Iteration 152/1000 | Loss: 0.00001597
Iteration 153/1000 | Loss: 0.00001597
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.597031223354861e-05, 1.597031223354861e-05, 1.597031223354861e-05, 1.597031223354861e-05, 1.597031223354861e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.597031223354861e-05

Optimization complete. Final v2v error: 3.2628262042999268 mm

Highest mean error: 4.162929534912109 mm for frame 177

Lowest mean error: 2.7233259677886963 mm for frame 62

Saving results

Total time: 38.22590136528015
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00810353
Iteration 2/25 | Loss: 0.00134419
Iteration 3/25 | Loss: 0.00109499
Iteration 4/25 | Loss: 0.00106783
Iteration 5/25 | Loss: 0.00106401
Iteration 6/25 | Loss: 0.00106308
Iteration 7/25 | Loss: 0.00106278
Iteration 8/25 | Loss: 0.00106273
Iteration 9/25 | Loss: 0.00106273
Iteration 10/25 | Loss: 0.00106273
Iteration 11/25 | Loss: 0.00106273
Iteration 12/25 | Loss: 0.00106273
Iteration 13/25 | Loss: 0.00106273
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0010627327719703317, 0.0010627327719703317, 0.0010627327719703317, 0.0010627327719703317, 0.0010627327719703317]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010627327719703317

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37670696
Iteration 2/25 | Loss: 0.00073947
Iteration 3/25 | Loss: 0.00073947
Iteration 4/25 | Loss: 0.00073947
Iteration 5/25 | Loss: 0.00073947
Iteration 6/25 | Loss: 0.00073947
Iteration 7/25 | Loss: 0.00073947
Iteration 8/25 | Loss: 0.00073947
Iteration 9/25 | Loss: 0.00073947
Iteration 10/25 | Loss: 0.00073947
Iteration 11/25 | Loss: 0.00073947
Iteration 12/25 | Loss: 0.00073947
Iteration 13/25 | Loss: 0.00073947
Iteration 14/25 | Loss: 0.00073947
Iteration 15/25 | Loss: 0.00073947
Iteration 16/25 | Loss: 0.00073947
Iteration 17/25 | Loss: 0.00073947
Iteration 18/25 | Loss: 0.00073947
Iteration 19/25 | Loss: 0.00073947
Iteration 20/25 | Loss: 0.00073947
Iteration 21/25 | Loss: 0.00073947
Iteration 22/25 | Loss: 0.00073947
Iteration 23/25 | Loss: 0.00073947
Iteration 24/25 | Loss: 0.00073947
Iteration 25/25 | Loss: 0.00073947
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007394683198072016, 0.0007394683198072016, 0.0007394683198072016, 0.0007394683198072016, 0.0007394683198072016]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007394683198072016

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073947
Iteration 2/1000 | Loss: 0.00002112
Iteration 3/1000 | Loss: 0.00001318
Iteration 4/1000 | Loss: 0.00001173
Iteration 5/1000 | Loss: 0.00001090
Iteration 6/1000 | Loss: 0.00001040
Iteration 7/1000 | Loss: 0.00001007
Iteration 8/1000 | Loss: 0.00000981
Iteration 9/1000 | Loss: 0.00000975
Iteration 10/1000 | Loss: 0.00000954
Iteration 11/1000 | Loss: 0.00000940
Iteration 12/1000 | Loss: 0.00000938
Iteration 13/1000 | Loss: 0.00000930
Iteration 14/1000 | Loss: 0.00000922
Iteration 15/1000 | Loss: 0.00000920
Iteration 16/1000 | Loss: 0.00000919
Iteration 17/1000 | Loss: 0.00000919
Iteration 18/1000 | Loss: 0.00000918
Iteration 19/1000 | Loss: 0.00000918
Iteration 20/1000 | Loss: 0.00000917
Iteration 21/1000 | Loss: 0.00000917
Iteration 22/1000 | Loss: 0.00000916
Iteration 23/1000 | Loss: 0.00000916
Iteration 24/1000 | Loss: 0.00000916
Iteration 25/1000 | Loss: 0.00000916
Iteration 26/1000 | Loss: 0.00000915
Iteration 27/1000 | Loss: 0.00000915
Iteration 28/1000 | Loss: 0.00000914
Iteration 29/1000 | Loss: 0.00000914
Iteration 30/1000 | Loss: 0.00000914
Iteration 31/1000 | Loss: 0.00000913
Iteration 32/1000 | Loss: 0.00000913
Iteration 33/1000 | Loss: 0.00000913
Iteration 34/1000 | Loss: 0.00000912
Iteration 35/1000 | Loss: 0.00000912
Iteration 36/1000 | Loss: 0.00000911
Iteration 37/1000 | Loss: 0.00000911
Iteration 38/1000 | Loss: 0.00000911
Iteration 39/1000 | Loss: 0.00000910
Iteration 40/1000 | Loss: 0.00000910
Iteration 41/1000 | Loss: 0.00000910
Iteration 42/1000 | Loss: 0.00000910
Iteration 43/1000 | Loss: 0.00000910
Iteration 44/1000 | Loss: 0.00000910
Iteration 45/1000 | Loss: 0.00000910
Iteration 46/1000 | Loss: 0.00000909
Iteration 47/1000 | Loss: 0.00000909
Iteration 48/1000 | Loss: 0.00000908
Iteration 49/1000 | Loss: 0.00000908
Iteration 50/1000 | Loss: 0.00000908
Iteration 51/1000 | Loss: 0.00000908
Iteration 52/1000 | Loss: 0.00000908
Iteration 53/1000 | Loss: 0.00000908
Iteration 54/1000 | Loss: 0.00000908
Iteration 55/1000 | Loss: 0.00000908
Iteration 56/1000 | Loss: 0.00000908
Iteration 57/1000 | Loss: 0.00000908
Iteration 58/1000 | Loss: 0.00000907
Iteration 59/1000 | Loss: 0.00000907
Iteration 60/1000 | Loss: 0.00000907
Iteration 61/1000 | Loss: 0.00000906
Iteration 62/1000 | Loss: 0.00000906
Iteration 63/1000 | Loss: 0.00000906
Iteration 64/1000 | Loss: 0.00000906
Iteration 65/1000 | Loss: 0.00000906
Iteration 66/1000 | Loss: 0.00000906
Iteration 67/1000 | Loss: 0.00000906
Iteration 68/1000 | Loss: 0.00000906
Iteration 69/1000 | Loss: 0.00000906
Iteration 70/1000 | Loss: 0.00000905
Iteration 71/1000 | Loss: 0.00000905
Iteration 72/1000 | Loss: 0.00000905
Iteration 73/1000 | Loss: 0.00000905
Iteration 74/1000 | Loss: 0.00000905
Iteration 75/1000 | Loss: 0.00000904
Iteration 76/1000 | Loss: 0.00000904
Iteration 77/1000 | Loss: 0.00000904
Iteration 78/1000 | Loss: 0.00000904
Iteration 79/1000 | Loss: 0.00000904
Iteration 80/1000 | Loss: 0.00000904
Iteration 81/1000 | Loss: 0.00000904
Iteration 82/1000 | Loss: 0.00000904
Iteration 83/1000 | Loss: 0.00000904
Iteration 84/1000 | Loss: 0.00000904
Iteration 85/1000 | Loss: 0.00000903
Iteration 86/1000 | Loss: 0.00000903
Iteration 87/1000 | Loss: 0.00000903
Iteration 88/1000 | Loss: 0.00000903
Iteration 89/1000 | Loss: 0.00000903
Iteration 90/1000 | Loss: 0.00000903
Iteration 91/1000 | Loss: 0.00000903
Iteration 92/1000 | Loss: 0.00000903
Iteration 93/1000 | Loss: 0.00000902
Iteration 94/1000 | Loss: 0.00000902
Iteration 95/1000 | Loss: 0.00000902
Iteration 96/1000 | Loss: 0.00000902
Iteration 97/1000 | Loss: 0.00000902
Iteration 98/1000 | Loss: 0.00000901
Iteration 99/1000 | Loss: 0.00000901
Iteration 100/1000 | Loss: 0.00000901
Iteration 101/1000 | Loss: 0.00000901
Iteration 102/1000 | Loss: 0.00000901
Iteration 103/1000 | Loss: 0.00000901
Iteration 104/1000 | Loss: 0.00000901
Iteration 105/1000 | Loss: 0.00000901
Iteration 106/1000 | Loss: 0.00000901
Iteration 107/1000 | Loss: 0.00000901
Iteration 108/1000 | Loss: 0.00000900
Iteration 109/1000 | Loss: 0.00000900
Iteration 110/1000 | Loss: 0.00000900
Iteration 111/1000 | Loss: 0.00000900
Iteration 112/1000 | Loss: 0.00000900
Iteration 113/1000 | Loss: 0.00000900
Iteration 114/1000 | Loss: 0.00000899
Iteration 115/1000 | Loss: 0.00000899
Iteration 116/1000 | Loss: 0.00000899
Iteration 117/1000 | Loss: 0.00000899
Iteration 118/1000 | Loss: 0.00000899
Iteration 119/1000 | Loss: 0.00000899
Iteration 120/1000 | Loss: 0.00000899
Iteration 121/1000 | Loss: 0.00000898
Iteration 122/1000 | Loss: 0.00000898
Iteration 123/1000 | Loss: 0.00000898
Iteration 124/1000 | Loss: 0.00000898
Iteration 125/1000 | Loss: 0.00000898
Iteration 126/1000 | Loss: 0.00000898
Iteration 127/1000 | Loss: 0.00000898
Iteration 128/1000 | Loss: 0.00000898
Iteration 129/1000 | Loss: 0.00000898
Iteration 130/1000 | Loss: 0.00000898
Iteration 131/1000 | Loss: 0.00000898
Iteration 132/1000 | Loss: 0.00000898
Iteration 133/1000 | Loss: 0.00000898
Iteration 134/1000 | Loss: 0.00000898
Iteration 135/1000 | Loss: 0.00000898
Iteration 136/1000 | Loss: 0.00000898
Iteration 137/1000 | Loss: 0.00000897
Iteration 138/1000 | Loss: 0.00000897
Iteration 139/1000 | Loss: 0.00000897
Iteration 140/1000 | Loss: 0.00000897
Iteration 141/1000 | Loss: 0.00000897
Iteration 142/1000 | Loss: 0.00000897
Iteration 143/1000 | Loss: 0.00000897
Iteration 144/1000 | Loss: 0.00000897
Iteration 145/1000 | Loss: 0.00000896
Iteration 146/1000 | Loss: 0.00000896
Iteration 147/1000 | Loss: 0.00000896
Iteration 148/1000 | Loss: 0.00000896
Iteration 149/1000 | Loss: 0.00000896
Iteration 150/1000 | Loss: 0.00000896
Iteration 151/1000 | Loss: 0.00000896
Iteration 152/1000 | Loss: 0.00000896
Iteration 153/1000 | Loss: 0.00000896
Iteration 154/1000 | Loss: 0.00000896
Iteration 155/1000 | Loss: 0.00000896
Iteration 156/1000 | Loss: 0.00000895
Iteration 157/1000 | Loss: 0.00000895
Iteration 158/1000 | Loss: 0.00000895
Iteration 159/1000 | Loss: 0.00000895
Iteration 160/1000 | Loss: 0.00000895
Iteration 161/1000 | Loss: 0.00000895
Iteration 162/1000 | Loss: 0.00000895
Iteration 163/1000 | Loss: 0.00000895
Iteration 164/1000 | Loss: 0.00000895
Iteration 165/1000 | Loss: 0.00000895
Iteration 166/1000 | Loss: 0.00000895
Iteration 167/1000 | Loss: 0.00000895
Iteration 168/1000 | Loss: 0.00000895
Iteration 169/1000 | Loss: 0.00000894
Iteration 170/1000 | Loss: 0.00000894
Iteration 171/1000 | Loss: 0.00000894
Iteration 172/1000 | Loss: 0.00000894
Iteration 173/1000 | Loss: 0.00000894
Iteration 174/1000 | Loss: 0.00000894
Iteration 175/1000 | Loss: 0.00000894
Iteration 176/1000 | Loss: 0.00000894
Iteration 177/1000 | Loss: 0.00000894
Iteration 178/1000 | Loss: 0.00000894
Iteration 179/1000 | Loss: 0.00000894
Iteration 180/1000 | Loss: 0.00000894
Iteration 181/1000 | Loss: 0.00000894
Iteration 182/1000 | Loss: 0.00000894
Iteration 183/1000 | Loss: 0.00000894
Iteration 184/1000 | Loss: 0.00000894
Iteration 185/1000 | Loss: 0.00000894
Iteration 186/1000 | Loss: 0.00000893
Iteration 187/1000 | Loss: 0.00000893
Iteration 188/1000 | Loss: 0.00000893
Iteration 189/1000 | Loss: 0.00000893
Iteration 190/1000 | Loss: 0.00000893
Iteration 191/1000 | Loss: 0.00000893
Iteration 192/1000 | Loss: 0.00000893
Iteration 193/1000 | Loss: 0.00000893
Iteration 194/1000 | Loss: 0.00000893
Iteration 195/1000 | Loss: 0.00000893
Iteration 196/1000 | Loss: 0.00000893
Iteration 197/1000 | Loss: 0.00000893
Iteration 198/1000 | Loss: 0.00000893
Iteration 199/1000 | Loss: 0.00000893
Iteration 200/1000 | Loss: 0.00000893
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [8.933109711506404e-06, 8.933109711506404e-06, 8.933109711506404e-06, 8.933109711506404e-06, 8.933109711506404e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.933109711506404e-06

Optimization complete. Final v2v error: 2.570629119873047 mm

Highest mean error: 2.917513608932495 mm for frame 103

Lowest mean error: 2.46496844291687 mm for frame 43

Saving results

Total time: 42.999616861343384
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00977843
Iteration 2/25 | Loss: 0.00154874
Iteration 3/25 | Loss: 0.00130447
Iteration 4/25 | Loss: 0.00128263
Iteration 5/25 | Loss: 0.00127707
Iteration 6/25 | Loss: 0.00127674
Iteration 7/25 | Loss: 0.00127674
Iteration 8/25 | Loss: 0.00127674
Iteration 9/25 | Loss: 0.00127674
Iteration 10/25 | Loss: 0.00127674
Iteration 11/25 | Loss: 0.00127674
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012767395237460732, 0.0012767395237460732, 0.0012767395237460732, 0.0012767395237460732, 0.0012767395237460732]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012767395237460732

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.61198902
Iteration 2/25 | Loss: 0.00081674
Iteration 3/25 | Loss: 0.00081671
Iteration 4/25 | Loss: 0.00081671
Iteration 5/25 | Loss: 0.00081671
Iteration 6/25 | Loss: 0.00081671
Iteration 7/25 | Loss: 0.00081671
Iteration 8/25 | Loss: 0.00081671
Iteration 9/25 | Loss: 0.00081671
Iteration 10/25 | Loss: 0.00081671
Iteration 11/25 | Loss: 0.00081671
Iteration 12/25 | Loss: 0.00081671
Iteration 13/25 | Loss: 0.00081671
Iteration 14/25 | Loss: 0.00081671
Iteration 15/25 | Loss: 0.00081671
Iteration 16/25 | Loss: 0.00081671
Iteration 17/25 | Loss: 0.00081671
Iteration 18/25 | Loss: 0.00081671
Iteration 19/25 | Loss: 0.00081671
Iteration 20/25 | Loss: 0.00081671
Iteration 21/25 | Loss: 0.00081671
Iteration 22/25 | Loss: 0.00081671
Iteration 23/25 | Loss: 0.00081671
Iteration 24/25 | Loss: 0.00081671
Iteration 25/25 | Loss: 0.00081671

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081671
Iteration 2/1000 | Loss: 0.00007246
Iteration 3/1000 | Loss: 0.00004411
Iteration 4/1000 | Loss: 0.00003842
Iteration 5/1000 | Loss: 0.00003675
Iteration 6/1000 | Loss: 0.00003572
Iteration 7/1000 | Loss: 0.00003472
Iteration 8/1000 | Loss: 0.00003414
Iteration 9/1000 | Loss: 0.00003374
Iteration 10/1000 | Loss: 0.00003334
Iteration 11/1000 | Loss: 0.00003304
Iteration 12/1000 | Loss: 0.00003281
Iteration 13/1000 | Loss: 0.00003263
Iteration 14/1000 | Loss: 0.00003248
Iteration 15/1000 | Loss: 0.00003238
Iteration 16/1000 | Loss: 0.00003238
Iteration 17/1000 | Loss: 0.00003233
Iteration 18/1000 | Loss: 0.00003226
Iteration 19/1000 | Loss: 0.00003224
Iteration 20/1000 | Loss: 0.00003223
Iteration 21/1000 | Loss: 0.00003222
Iteration 22/1000 | Loss: 0.00003221
Iteration 23/1000 | Loss: 0.00003220
Iteration 24/1000 | Loss: 0.00003219
Iteration 25/1000 | Loss: 0.00003219
Iteration 26/1000 | Loss: 0.00003218
Iteration 27/1000 | Loss: 0.00003217
Iteration 28/1000 | Loss: 0.00003213
Iteration 29/1000 | Loss: 0.00003212
Iteration 30/1000 | Loss: 0.00003211
Iteration 31/1000 | Loss: 0.00003211
Iteration 32/1000 | Loss: 0.00003211
Iteration 33/1000 | Loss: 0.00003211
Iteration 34/1000 | Loss: 0.00003210
Iteration 35/1000 | Loss: 0.00003210
Iteration 36/1000 | Loss: 0.00003210
Iteration 37/1000 | Loss: 0.00003209
Iteration 38/1000 | Loss: 0.00003208
Iteration 39/1000 | Loss: 0.00003208
Iteration 40/1000 | Loss: 0.00003208
Iteration 41/1000 | Loss: 0.00003207
Iteration 42/1000 | Loss: 0.00003207
Iteration 43/1000 | Loss: 0.00003206
Iteration 44/1000 | Loss: 0.00003206
Iteration 45/1000 | Loss: 0.00003206
Iteration 46/1000 | Loss: 0.00003206
Iteration 47/1000 | Loss: 0.00003206
Iteration 48/1000 | Loss: 0.00003205
Iteration 49/1000 | Loss: 0.00003205
Iteration 50/1000 | Loss: 0.00003204
Iteration 51/1000 | Loss: 0.00003204
Iteration 52/1000 | Loss: 0.00003204
Iteration 53/1000 | Loss: 0.00003203
Iteration 54/1000 | Loss: 0.00003203
Iteration 55/1000 | Loss: 0.00003203
Iteration 56/1000 | Loss: 0.00003202
Iteration 57/1000 | Loss: 0.00003202
Iteration 58/1000 | Loss: 0.00003202
Iteration 59/1000 | Loss: 0.00003201
Iteration 60/1000 | Loss: 0.00003201
Iteration 61/1000 | Loss: 0.00003201
Iteration 62/1000 | Loss: 0.00003200
Iteration 63/1000 | Loss: 0.00003200
Iteration 64/1000 | Loss: 0.00003199
Iteration 65/1000 | Loss: 0.00003199
Iteration 66/1000 | Loss: 0.00003199
Iteration 67/1000 | Loss: 0.00003198
Iteration 68/1000 | Loss: 0.00003198
Iteration 69/1000 | Loss: 0.00003197
Iteration 70/1000 | Loss: 0.00003197
Iteration 71/1000 | Loss: 0.00003196
Iteration 72/1000 | Loss: 0.00003196
Iteration 73/1000 | Loss: 0.00003196
Iteration 74/1000 | Loss: 0.00003195
Iteration 75/1000 | Loss: 0.00003195
Iteration 76/1000 | Loss: 0.00003194
Iteration 77/1000 | Loss: 0.00003193
Iteration 78/1000 | Loss: 0.00003193
Iteration 79/1000 | Loss: 0.00003193
Iteration 80/1000 | Loss: 0.00003193
Iteration 81/1000 | Loss: 0.00003192
Iteration 82/1000 | Loss: 0.00003192
Iteration 83/1000 | Loss: 0.00003192
Iteration 84/1000 | Loss: 0.00003192
Iteration 85/1000 | Loss: 0.00003192
Iteration 86/1000 | Loss: 0.00003192
Iteration 87/1000 | Loss: 0.00003191
Iteration 88/1000 | Loss: 0.00003191
Iteration 89/1000 | Loss: 0.00003191
Iteration 90/1000 | Loss: 0.00003191
Iteration 91/1000 | Loss: 0.00003191
Iteration 92/1000 | Loss: 0.00003191
Iteration 93/1000 | Loss: 0.00003191
Iteration 94/1000 | Loss: 0.00003190
Iteration 95/1000 | Loss: 0.00003190
Iteration 96/1000 | Loss: 0.00003190
Iteration 97/1000 | Loss: 0.00003190
Iteration 98/1000 | Loss: 0.00003189
Iteration 99/1000 | Loss: 0.00003189
Iteration 100/1000 | Loss: 0.00003189
Iteration 101/1000 | Loss: 0.00003189
Iteration 102/1000 | Loss: 0.00003189
Iteration 103/1000 | Loss: 0.00003189
Iteration 104/1000 | Loss: 0.00003189
Iteration 105/1000 | Loss: 0.00003189
Iteration 106/1000 | Loss: 0.00003189
Iteration 107/1000 | Loss: 0.00003189
Iteration 108/1000 | Loss: 0.00003189
Iteration 109/1000 | Loss: 0.00003189
Iteration 110/1000 | Loss: 0.00003189
Iteration 111/1000 | Loss: 0.00003189
Iteration 112/1000 | Loss: 0.00003189
Iteration 113/1000 | Loss: 0.00003188
Iteration 114/1000 | Loss: 0.00003188
Iteration 115/1000 | Loss: 0.00003188
Iteration 116/1000 | Loss: 0.00003188
Iteration 117/1000 | Loss: 0.00003188
Iteration 118/1000 | Loss: 0.00003188
Iteration 119/1000 | Loss: 0.00003188
Iteration 120/1000 | Loss: 0.00003188
Iteration 121/1000 | Loss: 0.00003188
Iteration 122/1000 | Loss: 0.00003187
Iteration 123/1000 | Loss: 0.00003187
Iteration 124/1000 | Loss: 0.00003187
Iteration 125/1000 | Loss: 0.00003186
Iteration 126/1000 | Loss: 0.00003186
Iteration 127/1000 | Loss: 0.00003186
Iteration 128/1000 | Loss: 0.00003186
Iteration 129/1000 | Loss: 0.00003186
Iteration 130/1000 | Loss: 0.00003186
Iteration 131/1000 | Loss: 0.00003185
Iteration 132/1000 | Loss: 0.00003185
Iteration 133/1000 | Loss: 0.00003185
Iteration 134/1000 | Loss: 0.00003185
Iteration 135/1000 | Loss: 0.00003184
Iteration 136/1000 | Loss: 0.00003184
Iteration 137/1000 | Loss: 0.00003184
Iteration 138/1000 | Loss: 0.00003184
Iteration 139/1000 | Loss: 0.00003184
Iteration 140/1000 | Loss: 0.00003183
Iteration 141/1000 | Loss: 0.00003183
Iteration 142/1000 | Loss: 0.00003183
Iteration 143/1000 | Loss: 0.00003183
Iteration 144/1000 | Loss: 0.00003183
Iteration 145/1000 | Loss: 0.00003182
Iteration 146/1000 | Loss: 0.00003182
Iteration 147/1000 | Loss: 0.00003182
Iteration 148/1000 | Loss: 0.00003182
Iteration 149/1000 | Loss: 0.00003182
Iteration 150/1000 | Loss: 0.00003182
Iteration 151/1000 | Loss: 0.00003182
Iteration 152/1000 | Loss: 0.00003181
Iteration 153/1000 | Loss: 0.00003181
Iteration 154/1000 | Loss: 0.00003181
Iteration 155/1000 | Loss: 0.00003181
Iteration 156/1000 | Loss: 0.00003181
Iteration 157/1000 | Loss: 0.00003181
Iteration 158/1000 | Loss: 0.00003181
Iteration 159/1000 | Loss: 0.00003180
Iteration 160/1000 | Loss: 0.00003180
Iteration 161/1000 | Loss: 0.00003180
Iteration 162/1000 | Loss: 0.00003180
Iteration 163/1000 | Loss: 0.00003180
Iteration 164/1000 | Loss: 0.00003180
Iteration 165/1000 | Loss: 0.00003180
Iteration 166/1000 | Loss: 0.00003180
Iteration 167/1000 | Loss: 0.00003180
Iteration 168/1000 | Loss: 0.00003180
Iteration 169/1000 | Loss: 0.00003180
Iteration 170/1000 | Loss: 0.00003180
Iteration 171/1000 | Loss: 0.00003180
Iteration 172/1000 | Loss: 0.00003180
Iteration 173/1000 | Loss: 0.00003180
Iteration 174/1000 | Loss: 0.00003180
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [3.179837221978232e-05, 3.179837221978232e-05, 3.179837221978232e-05, 3.179837221978232e-05, 3.179837221978232e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.179837221978232e-05

Optimization complete. Final v2v error: 4.494778633117676 mm

Highest mean error: 6.1295552253723145 mm for frame 48

Lowest mean error: 3.2782604694366455 mm for frame 75

Saving results

Total time: 49.913784980773926
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01001349
Iteration 2/25 | Loss: 0.00248643
Iteration 3/25 | Loss: 0.00184012
Iteration 4/25 | Loss: 0.00185787
Iteration 5/25 | Loss: 0.00146674
Iteration 6/25 | Loss: 0.00120218
Iteration 7/25 | Loss: 0.00115113
Iteration 8/25 | Loss: 0.00113064
Iteration 9/25 | Loss: 0.00114727
Iteration 10/25 | Loss: 0.00114243
Iteration 11/25 | Loss: 0.00113579
Iteration 12/25 | Loss: 0.00111662
Iteration 13/25 | Loss: 0.00110469
Iteration 14/25 | Loss: 0.00110115
Iteration 15/25 | Loss: 0.00109922
Iteration 16/25 | Loss: 0.00110318
Iteration 17/25 | Loss: 0.00110317
Iteration 18/25 | Loss: 0.00110317
Iteration 19/25 | Loss: 0.00110306
Iteration 20/25 | Loss: 0.00110287
Iteration 21/25 | Loss: 0.00109895
Iteration 22/25 | Loss: 0.00109796
Iteration 23/25 | Loss: 0.00109783
Iteration 24/25 | Loss: 0.00109783
Iteration 25/25 | Loss: 0.00109783

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32659650
Iteration 2/25 | Loss: 0.00050732
Iteration 3/25 | Loss: 0.00050731
Iteration 4/25 | Loss: 0.00050731
Iteration 5/25 | Loss: 0.00050731
Iteration 6/25 | Loss: 0.00050731
Iteration 7/25 | Loss: 0.00050731
Iteration 8/25 | Loss: 0.00050731
Iteration 9/25 | Loss: 0.00050731
Iteration 10/25 | Loss: 0.00050731
Iteration 11/25 | Loss: 0.00050731
Iteration 12/25 | Loss: 0.00050731
Iteration 13/25 | Loss: 0.00050731
Iteration 14/25 | Loss: 0.00050731
Iteration 15/25 | Loss: 0.00050731
Iteration 16/25 | Loss: 0.00050731
Iteration 17/25 | Loss: 0.00050731
Iteration 18/25 | Loss: 0.00050731
Iteration 19/25 | Loss: 0.00050731
Iteration 20/25 | Loss: 0.00050731
Iteration 21/25 | Loss: 0.00050731
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005073072970844805, 0.0005073072970844805, 0.0005073072970844805, 0.0005073072970844805, 0.0005073072970844805]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005073072970844805

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050731
Iteration 2/1000 | Loss: 0.00002228
Iteration 3/1000 | Loss: 0.00001729
Iteration 4/1000 | Loss: 0.00001628
Iteration 5/1000 | Loss: 0.00001554
Iteration 6/1000 | Loss: 0.00001507
Iteration 7/1000 | Loss: 0.00001471
Iteration 8/1000 | Loss: 0.00001469
Iteration 9/1000 | Loss: 0.00001438
Iteration 10/1000 | Loss: 0.00001418
Iteration 11/1000 | Loss: 0.00001409
Iteration 12/1000 | Loss: 0.00001407
Iteration 13/1000 | Loss: 0.00001400
Iteration 14/1000 | Loss: 0.00001387
Iteration 15/1000 | Loss: 0.00001385
Iteration 16/1000 | Loss: 0.00001384
Iteration 17/1000 | Loss: 0.00001379
Iteration 18/1000 | Loss: 0.00001378
Iteration 19/1000 | Loss: 0.00001378
Iteration 20/1000 | Loss: 0.00001378
Iteration 21/1000 | Loss: 0.00001378
Iteration 22/1000 | Loss: 0.00001377
Iteration 23/1000 | Loss: 0.00001377
Iteration 24/1000 | Loss: 0.00001375
Iteration 25/1000 | Loss: 0.00001374
Iteration 26/1000 | Loss: 0.00001370
Iteration 27/1000 | Loss: 0.00001368
Iteration 28/1000 | Loss: 0.00001368
Iteration 29/1000 | Loss: 0.00001367
Iteration 30/1000 | Loss: 0.00001366
Iteration 31/1000 | Loss: 0.00001365
Iteration 32/1000 | Loss: 0.00001364
Iteration 33/1000 | Loss: 0.00001364
Iteration 34/1000 | Loss: 0.00001363
Iteration 35/1000 | Loss: 0.00001363
Iteration 36/1000 | Loss: 0.00001362
Iteration 37/1000 | Loss: 0.00001362
Iteration 38/1000 | Loss: 0.00001361
Iteration 39/1000 | Loss: 0.00001360
Iteration 40/1000 | Loss: 0.00001360
Iteration 41/1000 | Loss: 0.00001360
Iteration 42/1000 | Loss: 0.00001360
Iteration 43/1000 | Loss: 0.00001360
Iteration 44/1000 | Loss: 0.00001360
Iteration 45/1000 | Loss: 0.00001359
Iteration 46/1000 | Loss: 0.00001359
Iteration 47/1000 | Loss: 0.00001354
Iteration 48/1000 | Loss: 0.00001354
Iteration 49/1000 | Loss: 0.00001354
Iteration 50/1000 | Loss: 0.00001354
Iteration 51/1000 | Loss: 0.00001354
Iteration 52/1000 | Loss: 0.00001354
Iteration 53/1000 | Loss: 0.00001354
Iteration 54/1000 | Loss: 0.00001354
Iteration 55/1000 | Loss: 0.00001354
Iteration 56/1000 | Loss: 0.00001353
Iteration 57/1000 | Loss: 0.00001353
Iteration 58/1000 | Loss: 0.00001353
Iteration 59/1000 | Loss: 0.00001353
Iteration 60/1000 | Loss: 0.00001353
Iteration 61/1000 | Loss: 0.00001353
Iteration 62/1000 | Loss: 0.00001353
Iteration 63/1000 | Loss: 0.00001353
Iteration 64/1000 | Loss: 0.00001353
Iteration 65/1000 | Loss: 0.00001353
Iteration 66/1000 | Loss: 0.00001353
Iteration 67/1000 | Loss: 0.00001353
Iteration 68/1000 | Loss: 0.00001353
Iteration 69/1000 | Loss: 0.00001353
Iteration 70/1000 | Loss: 0.00001353
Iteration 71/1000 | Loss: 0.00001353
Iteration 72/1000 | Loss: 0.00001353
Iteration 73/1000 | Loss: 0.00001353
Iteration 74/1000 | Loss: 0.00001353
Iteration 75/1000 | Loss: 0.00001353
Iteration 76/1000 | Loss: 0.00001353
Iteration 77/1000 | Loss: 0.00001353
Iteration 78/1000 | Loss: 0.00001353
Iteration 79/1000 | Loss: 0.00001353
Iteration 80/1000 | Loss: 0.00001353
Iteration 81/1000 | Loss: 0.00001353
Iteration 82/1000 | Loss: 0.00001353
Iteration 83/1000 | Loss: 0.00001353
Iteration 84/1000 | Loss: 0.00001353
Iteration 85/1000 | Loss: 0.00001353
Iteration 86/1000 | Loss: 0.00001353
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 86. Stopping optimization.
Last 5 losses: [1.3529265743272845e-05, 1.3529265743272845e-05, 1.3529265743272845e-05, 1.3529265743272845e-05, 1.3529265743272845e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3529265743272845e-05

Optimization complete. Final v2v error: 3.0723471641540527 mm

Highest mean error: 3.2453408241271973 mm for frame 0

Lowest mean error: 2.96811842918396 mm for frame 188

Saving results

Total time: 67.28946208953857
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00337964
Iteration 2/25 | Loss: 0.00117965
Iteration 3/25 | Loss: 0.00108539
Iteration 4/25 | Loss: 0.00106592
Iteration 5/25 | Loss: 0.00105807
Iteration 6/25 | Loss: 0.00105651
Iteration 7/25 | Loss: 0.00105651
Iteration 8/25 | Loss: 0.00105651
Iteration 9/25 | Loss: 0.00105651
Iteration 10/25 | Loss: 0.00105651
Iteration 11/25 | Loss: 0.00105651
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010565149132162333, 0.0010565149132162333, 0.0010565149132162333, 0.0010565149132162333, 0.0010565149132162333]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010565149132162333

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.79657364
Iteration 2/25 | Loss: 0.00091956
Iteration 3/25 | Loss: 0.00091952
Iteration 4/25 | Loss: 0.00091952
Iteration 5/25 | Loss: 0.00091952
Iteration 6/25 | Loss: 0.00091952
Iteration 7/25 | Loss: 0.00091952
Iteration 8/25 | Loss: 0.00091952
Iteration 9/25 | Loss: 0.00091951
Iteration 10/25 | Loss: 0.00091951
Iteration 11/25 | Loss: 0.00091951
Iteration 12/25 | Loss: 0.00091951
Iteration 13/25 | Loss: 0.00091951
Iteration 14/25 | Loss: 0.00091951
Iteration 15/25 | Loss: 0.00091951
Iteration 16/25 | Loss: 0.00091951
Iteration 17/25 | Loss: 0.00091951
Iteration 18/25 | Loss: 0.00091951
Iteration 19/25 | Loss: 0.00091951
Iteration 20/25 | Loss: 0.00091951
Iteration 21/25 | Loss: 0.00091951
Iteration 22/25 | Loss: 0.00091951
Iteration 23/25 | Loss: 0.00091951
Iteration 24/25 | Loss: 0.00091951
Iteration 25/25 | Loss: 0.00091951

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091951
Iteration 2/1000 | Loss: 0.00002443
Iteration 3/1000 | Loss: 0.00001660
Iteration 4/1000 | Loss: 0.00001512
Iteration 5/1000 | Loss: 0.00001414
Iteration 6/1000 | Loss: 0.00001372
Iteration 7/1000 | Loss: 0.00001328
Iteration 8/1000 | Loss: 0.00001295
Iteration 9/1000 | Loss: 0.00001267
Iteration 10/1000 | Loss: 0.00001239
Iteration 11/1000 | Loss: 0.00001231
Iteration 12/1000 | Loss: 0.00001230
Iteration 13/1000 | Loss: 0.00001218
Iteration 14/1000 | Loss: 0.00001213
Iteration 15/1000 | Loss: 0.00001212
Iteration 16/1000 | Loss: 0.00001208
Iteration 17/1000 | Loss: 0.00001203
Iteration 18/1000 | Loss: 0.00001201
Iteration 19/1000 | Loss: 0.00001199
Iteration 20/1000 | Loss: 0.00001199
Iteration 21/1000 | Loss: 0.00001198
Iteration 22/1000 | Loss: 0.00001197
Iteration 23/1000 | Loss: 0.00001196
Iteration 24/1000 | Loss: 0.00001196
Iteration 25/1000 | Loss: 0.00001195
Iteration 26/1000 | Loss: 0.00001195
Iteration 27/1000 | Loss: 0.00001194
Iteration 28/1000 | Loss: 0.00001192
Iteration 29/1000 | Loss: 0.00001189
Iteration 30/1000 | Loss: 0.00001187
Iteration 31/1000 | Loss: 0.00001183
Iteration 32/1000 | Loss: 0.00001181
Iteration 33/1000 | Loss: 0.00001181
Iteration 34/1000 | Loss: 0.00001180
Iteration 35/1000 | Loss: 0.00001179
Iteration 36/1000 | Loss: 0.00001179
Iteration 37/1000 | Loss: 0.00001178
Iteration 38/1000 | Loss: 0.00001177
Iteration 39/1000 | Loss: 0.00001177
Iteration 40/1000 | Loss: 0.00001176
Iteration 41/1000 | Loss: 0.00001176
Iteration 42/1000 | Loss: 0.00001176
Iteration 43/1000 | Loss: 0.00001175
Iteration 44/1000 | Loss: 0.00001175
Iteration 45/1000 | Loss: 0.00001174
Iteration 46/1000 | Loss: 0.00001173
Iteration 47/1000 | Loss: 0.00001173
Iteration 48/1000 | Loss: 0.00001173
Iteration 49/1000 | Loss: 0.00001173
Iteration 50/1000 | Loss: 0.00001172
Iteration 51/1000 | Loss: 0.00001171
Iteration 52/1000 | Loss: 0.00001171
Iteration 53/1000 | Loss: 0.00001170
Iteration 54/1000 | Loss: 0.00001170
Iteration 55/1000 | Loss: 0.00001170
Iteration 56/1000 | Loss: 0.00001170
Iteration 57/1000 | Loss: 0.00001169
Iteration 58/1000 | Loss: 0.00001168
Iteration 59/1000 | Loss: 0.00001168
Iteration 60/1000 | Loss: 0.00001168
Iteration 61/1000 | Loss: 0.00001167
Iteration 62/1000 | Loss: 0.00001167
Iteration 63/1000 | Loss: 0.00001167
Iteration 64/1000 | Loss: 0.00001167
Iteration 65/1000 | Loss: 0.00001166
Iteration 66/1000 | Loss: 0.00001166
Iteration 67/1000 | Loss: 0.00001166
Iteration 68/1000 | Loss: 0.00001166
Iteration 69/1000 | Loss: 0.00001166
Iteration 70/1000 | Loss: 0.00001166
Iteration 71/1000 | Loss: 0.00001165
Iteration 72/1000 | Loss: 0.00001165
Iteration 73/1000 | Loss: 0.00001165
Iteration 74/1000 | Loss: 0.00001165
Iteration 75/1000 | Loss: 0.00001165
Iteration 76/1000 | Loss: 0.00001165
Iteration 77/1000 | Loss: 0.00001165
Iteration 78/1000 | Loss: 0.00001165
Iteration 79/1000 | Loss: 0.00001165
Iteration 80/1000 | Loss: 0.00001164
Iteration 81/1000 | Loss: 0.00001164
Iteration 82/1000 | Loss: 0.00001164
Iteration 83/1000 | Loss: 0.00001164
Iteration 84/1000 | Loss: 0.00001164
Iteration 85/1000 | Loss: 0.00001163
Iteration 86/1000 | Loss: 0.00001163
Iteration 87/1000 | Loss: 0.00001163
Iteration 88/1000 | Loss: 0.00001163
Iteration 89/1000 | Loss: 0.00001163
Iteration 90/1000 | Loss: 0.00001163
Iteration 91/1000 | Loss: 0.00001163
Iteration 92/1000 | Loss: 0.00001163
Iteration 93/1000 | Loss: 0.00001163
Iteration 94/1000 | Loss: 0.00001162
Iteration 95/1000 | Loss: 0.00001162
Iteration 96/1000 | Loss: 0.00001162
Iteration 97/1000 | Loss: 0.00001162
Iteration 98/1000 | Loss: 0.00001162
Iteration 99/1000 | Loss: 0.00001162
Iteration 100/1000 | Loss: 0.00001161
Iteration 101/1000 | Loss: 0.00001161
Iteration 102/1000 | Loss: 0.00001161
Iteration 103/1000 | Loss: 0.00001161
Iteration 104/1000 | Loss: 0.00001160
Iteration 105/1000 | Loss: 0.00001160
Iteration 106/1000 | Loss: 0.00001160
Iteration 107/1000 | Loss: 0.00001160
Iteration 108/1000 | Loss: 0.00001160
Iteration 109/1000 | Loss: 0.00001160
Iteration 110/1000 | Loss: 0.00001160
Iteration 111/1000 | Loss: 0.00001160
Iteration 112/1000 | Loss: 0.00001159
Iteration 113/1000 | Loss: 0.00001159
Iteration 114/1000 | Loss: 0.00001159
Iteration 115/1000 | Loss: 0.00001159
Iteration 116/1000 | Loss: 0.00001159
Iteration 117/1000 | Loss: 0.00001159
Iteration 118/1000 | Loss: 0.00001159
Iteration 119/1000 | Loss: 0.00001159
Iteration 120/1000 | Loss: 0.00001159
Iteration 121/1000 | Loss: 0.00001158
Iteration 122/1000 | Loss: 0.00001158
Iteration 123/1000 | Loss: 0.00001158
Iteration 124/1000 | Loss: 0.00001158
Iteration 125/1000 | Loss: 0.00001158
Iteration 126/1000 | Loss: 0.00001158
Iteration 127/1000 | Loss: 0.00001158
Iteration 128/1000 | Loss: 0.00001158
Iteration 129/1000 | Loss: 0.00001157
Iteration 130/1000 | Loss: 0.00001157
Iteration 131/1000 | Loss: 0.00001157
Iteration 132/1000 | Loss: 0.00001157
Iteration 133/1000 | Loss: 0.00001157
Iteration 134/1000 | Loss: 0.00001157
Iteration 135/1000 | Loss: 0.00001157
Iteration 136/1000 | Loss: 0.00001157
Iteration 137/1000 | Loss: 0.00001157
Iteration 138/1000 | Loss: 0.00001157
Iteration 139/1000 | Loss: 0.00001157
Iteration 140/1000 | Loss: 0.00001157
Iteration 141/1000 | Loss: 0.00001157
Iteration 142/1000 | Loss: 0.00001157
Iteration 143/1000 | Loss: 0.00001157
Iteration 144/1000 | Loss: 0.00001157
Iteration 145/1000 | Loss: 0.00001157
Iteration 146/1000 | Loss: 0.00001157
Iteration 147/1000 | Loss: 0.00001157
Iteration 148/1000 | Loss: 0.00001157
Iteration 149/1000 | Loss: 0.00001157
Iteration 150/1000 | Loss: 0.00001157
Iteration 151/1000 | Loss: 0.00001157
Iteration 152/1000 | Loss: 0.00001157
Iteration 153/1000 | Loss: 0.00001157
Iteration 154/1000 | Loss: 0.00001157
Iteration 155/1000 | Loss: 0.00001157
Iteration 156/1000 | Loss: 0.00001157
Iteration 157/1000 | Loss: 0.00001157
Iteration 158/1000 | Loss: 0.00001157
Iteration 159/1000 | Loss: 0.00001157
Iteration 160/1000 | Loss: 0.00001157
Iteration 161/1000 | Loss: 0.00001157
Iteration 162/1000 | Loss: 0.00001157
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.1571565664780792e-05, 1.1571565664780792e-05, 1.1571565664780792e-05, 1.1571565664780792e-05, 1.1571565664780792e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1571565664780792e-05

Optimization complete. Final v2v error: 2.9286839962005615 mm

Highest mean error: 3.365985870361328 mm for frame 153

Lowest mean error: 2.6447789669036865 mm for frame 222

Saving results

Total time: 43.12631845474243
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00720099
Iteration 2/25 | Loss: 0.00117871
Iteration 3/25 | Loss: 0.00109711
Iteration 4/25 | Loss: 0.00108741
Iteration 5/25 | Loss: 0.00108520
Iteration 6/25 | Loss: 0.00108520
Iteration 7/25 | Loss: 0.00108520
Iteration 8/25 | Loss: 0.00108520
Iteration 9/25 | Loss: 0.00108520
Iteration 10/25 | Loss: 0.00108520
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001085201627574861, 0.001085201627574861, 0.001085201627574861, 0.001085201627574861, 0.001085201627574861]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001085201627574861

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50140440
Iteration 2/25 | Loss: 0.00070699
Iteration 3/25 | Loss: 0.00070699
Iteration 4/25 | Loss: 0.00070698
Iteration 5/25 | Loss: 0.00070698
Iteration 6/25 | Loss: 0.00070698
Iteration 7/25 | Loss: 0.00070698
Iteration 8/25 | Loss: 0.00070698
Iteration 9/25 | Loss: 0.00070698
Iteration 10/25 | Loss: 0.00070698
Iteration 11/25 | Loss: 0.00070698
Iteration 12/25 | Loss: 0.00070698
Iteration 13/25 | Loss: 0.00070698
Iteration 14/25 | Loss: 0.00070698
Iteration 15/25 | Loss: 0.00070698
Iteration 16/25 | Loss: 0.00070698
Iteration 17/25 | Loss: 0.00070698
Iteration 18/25 | Loss: 0.00070698
Iteration 19/25 | Loss: 0.00070698
Iteration 20/25 | Loss: 0.00070698
Iteration 21/25 | Loss: 0.00070698
Iteration 22/25 | Loss: 0.00070698
Iteration 23/25 | Loss: 0.00070698
Iteration 24/25 | Loss: 0.00070698
Iteration 25/25 | Loss: 0.00070698

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070698
Iteration 2/1000 | Loss: 0.00001883
Iteration 3/1000 | Loss: 0.00001358
Iteration 4/1000 | Loss: 0.00001220
Iteration 5/1000 | Loss: 0.00001153
Iteration 6/1000 | Loss: 0.00001131
Iteration 7/1000 | Loss: 0.00001102
Iteration 8/1000 | Loss: 0.00001085
Iteration 9/1000 | Loss: 0.00001064
Iteration 10/1000 | Loss: 0.00001045
Iteration 11/1000 | Loss: 0.00001043
Iteration 12/1000 | Loss: 0.00001039
Iteration 13/1000 | Loss: 0.00001037
Iteration 14/1000 | Loss: 0.00001036
Iteration 15/1000 | Loss: 0.00001034
Iteration 16/1000 | Loss: 0.00001025
Iteration 17/1000 | Loss: 0.00001023
Iteration 18/1000 | Loss: 0.00001023
Iteration 19/1000 | Loss: 0.00001017
Iteration 20/1000 | Loss: 0.00001015
Iteration 21/1000 | Loss: 0.00001014
Iteration 22/1000 | Loss: 0.00001014
Iteration 23/1000 | Loss: 0.00001011
Iteration 24/1000 | Loss: 0.00001008
Iteration 25/1000 | Loss: 0.00001007
Iteration 26/1000 | Loss: 0.00001006
Iteration 27/1000 | Loss: 0.00001006
Iteration 28/1000 | Loss: 0.00001005
Iteration 29/1000 | Loss: 0.00001005
Iteration 30/1000 | Loss: 0.00001004
Iteration 31/1000 | Loss: 0.00001004
Iteration 32/1000 | Loss: 0.00001003
Iteration 33/1000 | Loss: 0.00001003
Iteration 34/1000 | Loss: 0.00001003
Iteration 35/1000 | Loss: 0.00001002
Iteration 36/1000 | Loss: 0.00001000
Iteration 37/1000 | Loss: 0.00000999
Iteration 38/1000 | Loss: 0.00000999
Iteration 39/1000 | Loss: 0.00000998
Iteration 40/1000 | Loss: 0.00000998
Iteration 41/1000 | Loss: 0.00000998
Iteration 42/1000 | Loss: 0.00000997
Iteration 43/1000 | Loss: 0.00000997
Iteration 44/1000 | Loss: 0.00000996
Iteration 45/1000 | Loss: 0.00000995
Iteration 46/1000 | Loss: 0.00000994
Iteration 47/1000 | Loss: 0.00000994
Iteration 48/1000 | Loss: 0.00000993
Iteration 49/1000 | Loss: 0.00000993
Iteration 50/1000 | Loss: 0.00000992
Iteration 51/1000 | Loss: 0.00000992
Iteration 52/1000 | Loss: 0.00000992
Iteration 53/1000 | Loss: 0.00000991
Iteration 54/1000 | Loss: 0.00000991
Iteration 55/1000 | Loss: 0.00000990
Iteration 56/1000 | Loss: 0.00000990
Iteration 57/1000 | Loss: 0.00000990
Iteration 58/1000 | Loss: 0.00000989
Iteration 59/1000 | Loss: 0.00000989
Iteration 60/1000 | Loss: 0.00000989
Iteration 61/1000 | Loss: 0.00000989
Iteration 62/1000 | Loss: 0.00000988
Iteration 63/1000 | Loss: 0.00000988
Iteration 64/1000 | Loss: 0.00000987
Iteration 65/1000 | Loss: 0.00000984
Iteration 66/1000 | Loss: 0.00000984
Iteration 67/1000 | Loss: 0.00000984
Iteration 68/1000 | Loss: 0.00000984
Iteration 69/1000 | Loss: 0.00000983
Iteration 70/1000 | Loss: 0.00000983
Iteration 71/1000 | Loss: 0.00000982
Iteration 72/1000 | Loss: 0.00000981
Iteration 73/1000 | Loss: 0.00000981
Iteration 74/1000 | Loss: 0.00000981
Iteration 75/1000 | Loss: 0.00000980
Iteration 76/1000 | Loss: 0.00000980
Iteration 77/1000 | Loss: 0.00000979
Iteration 78/1000 | Loss: 0.00000979
Iteration 79/1000 | Loss: 0.00000979
Iteration 80/1000 | Loss: 0.00000978
Iteration 81/1000 | Loss: 0.00000978
Iteration 82/1000 | Loss: 0.00000977
Iteration 83/1000 | Loss: 0.00000977
Iteration 84/1000 | Loss: 0.00000977
Iteration 85/1000 | Loss: 0.00000976
Iteration 86/1000 | Loss: 0.00000976
Iteration 87/1000 | Loss: 0.00000976
Iteration 88/1000 | Loss: 0.00000976
Iteration 89/1000 | Loss: 0.00000976
Iteration 90/1000 | Loss: 0.00000976
Iteration 91/1000 | Loss: 0.00000976
Iteration 92/1000 | Loss: 0.00000976
Iteration 93/1000 | Loss: 0.00000976
Iteration 94/1000 | Loss: 0.00000975
Iteration 95/1000 | Loss: 0.00000975
Iteration 96/1000 | Loss: 0.00000975
Iteration 97/1000 | Loss: 0.00000975
Iteration 98/1000 | Loss: 0.00000975
Iteration 99/1000 | Loss: 0.00000974
Iteration 100/1000 | Loss: 0.00000974
Iteration 101/1000 | Loss: 0.00000974
Iteration 102/1000 | Loss: 0.00000973
Iteration 103/1000 | Loss: 0.00000973
Iteration 104/1000 | Loss: 0.00000972
Iteration 105/1000 | Loss: 0.00000972
Iteration 106/1000 | Loss: 0.00000972
Iteration 107/1000 | Loss: 0.00000971
Iteration 108/1000 | Loss: 0.00000971
Iteration 109/1000 | Loss: 0.00000971
Iteration 110/1000 | Loss: 0.00000971
Iteration 111/1000 | Loss: 0.00000970
Iteration 112/1000 | Loss: 0.00000970
Iteration 113/1000 | Loss: 0.00000970
Iteration 114/1000 | Loss: 0.00000970
Iteration 115/1000 | Loss: 0.00000970
Iteration 116/1000 | Loss: 0.00000970
Iteration 117/1000 | Loss: 0.00000970
Iteration 118/1000 | Loss: 0.00000969
Iteration 119/1000 | Loss: 0.00000969
Iteration 120/1000 | Loss: 0.00000969
Iteration 121/1000 | Loss: 0.00000969
Iteration 122/1000 | Loss: 0.00000969
Iteration 123/1000 | Loss: 0.00000969
Iteration 124/1000 | Loss: 0.00000969
Iteration 125/1000 | Loss: 0.00000969
Iteration 126/1000 | Loss: 0.00000969
Iteration 127/1000 | Loss: 0.00000969
Iteration 128/1000 | Loss: 0.00000968
Iteration 129/1000 | Loss: 0.00000968
Iteration 130/1000 | Loss: 0.00000968
Iteration 131/1000 | Loss: 0.00000967
Iteration 132/1000 | Loss: 0.00000967
Iteration 133/1000 | Loss: 0.00000967
Iteration 134/1000 | Loss: 0.00000967
Iteration 135/1000 | Loss: 0.00000967
Iteration 136/1000 | Loss: 0.00000967
Iteration 137/1000 | Loss: 0.00000967
Iteration 138/1000 | Loss: 0.00000966
Iteration 139/1000 | Loss: 0.00000966
Iteration 140/1000 | Loss: 0.00000966
Iteration 141/1000 | Loss: 0.00000966
Iteration 142/1000 | Loss: 0.00000966
Iteration 143/1000 | Loss: 0.00000966
Iteration 144/1000 | Loss: 0.00000966
Iteration 145/1000 | Loss: 0.00000966
Iteration 146/1000 | Loss: 0.00000966
Iteration 147/1000 | Loss: 0.00000966
Iteration 148/1000 | Loss: 0.00000966
Iteration 149/1000 | Loss: 0.00000966
Iteration 150/1000 | Loss: 0.00000965
Iteration 151/1000 | Loss: 0.00000965
Iteration 152/1000 | Loss: 0.00000965
Iteration 153/1000 | Loss: 0.00000965
Iteration 154/1000 | Loss: 0.00000965
Iteration 155/1000 | Loss: 0.00000965
Iteration 156/1000 | Loss: 0.00000965
Iteration 157/1000 | Loss: 0.00000965
Iteration 158/1000 | Loss: 0.00000965
Iteration 159/1000 | Loss: 0.00000965
Iteration 160/1000 | Loss: 0.00000965
Iteration 161/1000 | Loss: 0.00000965
Iteration 162/1000 | Loss: 0.00000965
Iteration 163/1000 | Loss: 0.00000965
Iteration 164/1000 | Loss: 0.00000964
Iteration 165/1000 | Loss: 0.00000964
Iteration 166/1000 | Loss: 0.00000964
Iteration 167/1000 | Loss: 0.00000964
Iteration 168/1000 | Loss: 0.00000964
Iteration 169/1000 | Loss: 0.00000964
Iteration 170/1000 | Loss: 0.00000964
Iteration 171/1000 | Loss: 0.00000964
Iteration 172/1000 | Loss: 0.00000964
Iteration 173/1000 | Loss: 0.00000964
Iteration 174/1000 | Loss: 0.00000964
Iteration 175/1000 | Loss: 0.00000964
Iteration 176/1000 | Loss: 0.00000964
Iteration 177/1000 | Loss: 0.00000964
Iteration 178/1000 | Loss: 0.00000964
Iteration 179/1000 | Loss: 0.00000964
Iteration 180/1000 | Loss: 0.00000964
Iteration 181/1000 | Loss: 0.00000963
Iteration 182/1000 | Loss: 0.00000963
Iteration 183/1000 | Loss: 0.00000963
Iteration 184/1000 | Loss: 0.00000963
Iteration 185/1000 | Loss: 0.00000963
Iteration 186/1000 | Loss: 0.00000963
Iteration 187/1000 | Loss: 0.00000963
Iteration 188/1000 | Loss: 0.00000963
Iteration 189/1000 | Loss: 0.00000963
Iteration 190/1000 | Loss: 0.00000963
Iteration 191/1000 | Loss: 0.00000963
Iteration 192/1000 | Loss: 0.00000963
Iteration 193/1000 | Loss: 0.00000963
Iteration 194/1000 | Loss: 0.00000963
Iteration 195/1000 | Loss: 0.00000963
Iteration 196/1000 | Loss: 0.00000962
Iteration 197/1000 | Loss: 0.00000962
Iteration 198/1000 | Loss: 0.00000962
Iteration 199/1000 | Loss: 0.00000962
Iteration 200/1000 | Loss: 0.00000962
Iteration 201/1000 | Loss: 0.00000962
Iteration 202/1000 | Loss: 0.00000962
Iteration 203/1000 | Loss: 0.00000962
Iteration 204/1000 | Loss: 0.00000962
Iteration 205/1000 | Loss: 0.00000962
Iteration 206/1000 | Loss: 0.00000962
Iteration 207/1000 | Loss: 0.00000962
Iteration 208/1000 | Loss: 0.00000962
Iteration 209/1000 | Loss: 0.00000962
Iteration 210/1000 | Loss: 0.00000962
Iteration 211/1000 | Loss: 0.00000962
Iteration 212/1000 | Loss: 0.00000962
Iteration 213/1000 | Loss: 0.00000962
Iteration 214/1000 | Loss: 0.00000962
Iteration 215/1000 | Loss: 0.00000962
Iteration 216/1000 | Loss: 0.00000961
Iteration 217/1000 | Loss: 0.00000961
Iteration 218/1000 | Loss: 0.00000961
Iteration 219/1000 | Loss: 0.00000961
Iteration 220/1000 | Loss: 0.00000961
Iteration 221/1000 | Loss: 0.00000961
Iteration 222/1000 | Loss: 0.00000961
Iteration 223/1000 | Loss: 0.00000961
Iteration 224/1000 | Loss: 0.00000961
Iteration 225/1000 | Loss: 0.00000961
Iteration 226/1000 | Loss: 0.00000961
Iteration 227/1000 | Loss: 0.00000961
Iteration 228/1000 | Loss: 0.00000961
Iteration 229/1000 | Loss: 0.00000961
Iteration 230/1000 | Loss: 0.00000961
Iteration 231/1000 | Loss: 0.00000961
Iteration 232/1000 | Loss: 0.00000961
Iteration 233/1000 | Loss: 0.00000961
Iteration 234/1000 | Loss: 0.00000961
Iteration 235/1000 | Loss: 0.00000961
Iteration 236/1000 | Loss: 0.00000961
Iteration 237/1000 | Loss: 0.00000961
Iteration 238/1000 | Loss: 0.00000961
Iteration 239/1000 | Loss: 0.00000961
Iteration 240/1000 | Loss: 0.00000961
Iteration 241/1000 | Loss: 0.00000961
Iteration 242/1000 | Loss: 0.00000961
Iteration 243/1000 | Loss: 0.00000961
Iteration 244/1000 | Loss: 0.00000961
Iteration 245/1000 | Loss: 0.00000961
Iteration 246/1000 | Loss: 0.00000961
Iteration 247/1000 | Loss: 0.00000961
Iteration 248/1000 | Loss: 0.00000961
Iteration 249/1000 | Loss: 0.00000961
Iteration 250/1000 | Loss: 0.00000961
Iteration 251/1000 | Loss: 0.00000961
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 251. Stopping optimization.
Last 5 losses: [9.605721970729064e-06, 9.605721970729064e-06, 9.605721970729064e-06, 9.605721970729064e-06, 9.605721970729064e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.605721970729064e-06

Optimization complete. Final v2v error: 2.6546926498413086 mm

Highest mean error: 2.973422050476074 mm for frame 21

Lowest mean error: 2.381455659866333 mm for frame 45

Saving results

Total time: 42.28727865219116
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00475534
Iteration 2/25 | Loss: 0.00134968
Iteration 3/25 | Loss: 0.00112188
Iteration 4/25 | Loss: 0.00109896
Iteration 5/25 | Loss: 0.00109579
Iteration 6/25 | Loss: 0.00109459
Iteration 7/25 | Loss: 0.00109459
Iteration 8/25 | Loss: 0.00109459
Iteration 9/25 | Loss: 0.00109459
Iteration 10/25 | Loss: 0.00109459
Iteration 11/25 | Loss: 0.00109459
Iteration 12/25 | Loss: 0.00109459
Iteration 13/25 | Loss: 0.00109459
Iteration 14/25 | Loss: 0.00109459
Iteration 15/25 | Loss: 0.00109459
Iteration 16/25 | Loss: 0.00109459
Iteration 17/25 | Loss: 0.00109459
Iteration 18/25 | Loss: 0.00109459
Iteration 19/25 | Loss: 0.00109459
Iteration 20/25 | Loss: 0.00109459
Iteration 21/25 | Loss: 0.00109459
Iteration 22/25 | Loss: 0.00109459
Iteration 23/25 | Loss: 0.00109459
Iteration 24/25 | Loss: 0.00109459
Iteration 25/25 | Loss: 0.00109459

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39046299
Iteration 2/25 | Loss: 0.00068870
Iteration 3/25 | Loss: 0.00068870
Iteration 4/25 | Loss: 0.00068870
Iteration 5/25 | Loss: 0.00068870
Iteration 6/25 | Loss: 0.00068870
Iteration 7/25 | Loss: 0.00068870
Iteration 8/25 | Loss: 0.00068870
Iteration 9/25 | Loss: 0.00068870
Iteration 10/25 | Loss: 0.00068870
Iteration 11/25 | Loss: 0.00068870
Iteration 12/25 | Loss: 0.00068870
Iteration 13/25 | Loss: 0.00068870
Iteration 14/25 | Loss: 0.00068870
Iteration 15/25 | Loss: 0.00068870
Iteration 16/25 | Loss: 0.00068870
Iteration 17/25 | Loss: 0.00068870
Iteration 18/25 | Loss: 0.00068870
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006886995397508144, 0.0006886995397508144, 0.0006886995397508144, 0.0006886995397508144, 0.0006886995397508144]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006886995397508144

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068870
Iteration 2/1000 | Loss: 0.00003275
Iteration 3/1000 | Loss: 0.00002154
Iteration 4/1000 | Loss: 0.00001778
Iteration 5/1000 | Loss: 0.00001648
Iteration 6/1000 | Loss: 0.00001573
Iteration 7/1000 | Loss: 0.00001513
Iteration 8/1000 | Loss: 0.00001478
Iteration 9/1000 | Loss: 0.00001453
Iteration 10/1000 | Loss: 0.00001431
Iteration 11/1000 | Loss: 0.00001417
Iteration 12/1000 | Loss: 0.00001415
Iteration 13/1000 | Loss: 0.00001415
Iteration 14/1000 | Loss: 0.00001413
Iteration 15/1000 | Loss: 0.00001413
Iteration 16/1000 | Loss: 0.00001411
Iteration 17/1000 | Loss: 0.00001411
Iteration 18/1000 | Loss: 0.00001410
Iteration 19/1000 | Loss: 0.00001409
Iteration 20/1000 | Loss: 0.00001408
Iteration 21/1000 | Loss: 0.00001408
Iteration 22/1000 | Loss: 0.00001407
Iteration 23/1000 | Loss: 0.00001407
Iteration 24/1000 | Loss: 0.00001406
Iteration 25/1000 | Loss: 0.00001405
Iteration 26/1000 | Loss: 0.00001405
Iteration 27/1000 | Loss: 0.00001404
Iteration 28/1000 | Loss: 0.00001403
Iteration 29/1000 | Loss: 0.00001403
Iteration 30/1000 | Loss: 0.00001403
Iteration 31/1000 | Loss: 0.00001403
Iteration 32/1000 | Loss: 0.00001403
Iteration 33/1000 | Loss: 0.00001403
Iteration 34/1000 | Loss: 0.00001402
Iteration 35/1000 | Loss: 0.00001401
Iteration 36/1000 | Loss: 0.00001400
Iteration 37/1000 | Loss: 0.00001395
Iteration 38/1000 | Loss: 0.00001393
Iteration 39/1000 | Loss: 0.00001392
Iteration 40/1000 | Loss: 0.00001392
Iteration 41/1000 | Loss: 0.00001391
Iteration 42/1000 | Loss: 0.00001391
Iteration 43/1000 | Loss: 0.00001391
Iteration 44/1000 | Loss: 0.00001391
Iteration 45/1000 | Loss: 0.00001391
Iteration 46/1000 | Loss: 0.00001391
Iteration 47/1000 | Loss: 0.00001391
Iteration 48/1000 | Loss: 0.00001391
Iteration 49/1000 | Loss: 0.00001390
Iteration 50/1000 | Loss: 0.00001390
Iteration 51/1000 | Loss: 0.00001390
Iteration 52/1000 | Loss: 0.00001389
Iteration 53/1000 | Loss: 0.00001388
Iteration 54/1000 | Loss: 0.00001388
Iteration 55/1000 | Loss: 0.00001388
Iteration 56/1000 | Loss: 0.00001388
Iteration 57/1000 | Loss: 0.00001388
Iteration 58/1000 | Loss: 0.00001388
Iteration 59/1000 | Loss: 0.00001388
Iteration 60/1000 | Loss: 0.00001388
Iteration 61/1000 | Loss: 0.00001388
Iteration 62/1000 | Loss: 0.00001388
Iteration 63/1000 | Loss: 0.00001388
Iteration 64/1000 | Loss: 0.00001388
Iteration 65/1000 | Loss: 0.00001387
Iteration 66/1000 | Loss: 0.00001386
Iteration 67/1000 | Loss: 0.00001386
Iteration 68/1000 | Loss: 0.00001386
Iteration 69/1000 | Loss: 0.00001386
Iteration 70/1000 | Loss: 0.00001386
Iteration 71/1000 | Loss: 0.00001386
Iteration 72/1000 | Loss: 0.00001386
Iteration 73/1000 | Loss: 0.00001385
Iteration 74/1000 | Loss: 0.00001385
Iteration 75/1000 | Loss: 0.00001385
Iteration 76/1000 | Loss: 0.00001385
Iteration 77/1000 | Loss: 0.00001385
Iteration 78/1000 | Loss: 0.00001385
Iteration 79/1000 | Loss: 0.00001385
Iteration 80/1000 | Loss: 0.00001385
Iteration 81/1000 | Loss: 0.00001385
Iteration 82/1000 | Loss: 0.00001385
Iteration 83/1000 | Loss: 0.00001384
Iteration 84/1000 | Loss: 0.00001384
Iteration 85/1000 | Loss: 0.00001384
Iteration 86/1000 | Loss: 0.00001384
Iteration 87/1000 | Loss: 0.00001383
Iteration 88/1000 | Loss: 0.00001383
Iteration 89/1000 | Loss: 0.00001383
Iteration 90/1000 | Loss: 0.00001382
Iteration 91/1000 | Loss: 0.00001382
Iteration 92/1000 | Loss: 0.00001382
Iteration 93/1000 | Loss: 0.00001382
Iteration 94/1000 | Loss: 0.00001382
Iteration 95/1000 | Loss: 0.00001382
Iteration 96/1000 | Loss: 0.00001382
Iteration 97/1000 | Loss: 0.00001381
Iteration 98/1000 | Loss: 0.00001381
Iteration 99/1000 | Loss: 0.00001381
Iteration 100/1000 | Loss: 0.00001380
Iteration 101/1000 | Loss: 0.00001380
Iteration 102/1000 | Loss: 0.00001379
Iteration 103/1000 | Loss: 0.00001379
Iteration 104/1000 | Loss: 0.00001379
Iteration 105/1000 | Loss: 0.00001379
Iteration 106/1000 | Loss: 0.00001379
Iteration 107/1000 | Loss: 0.00001379
Iteration 108/1000 | Loss: 0.00001379
Iteration 109/1000 | Loss: 0.00001379
Iteration 110/1000 | Loss: 0.00001378
Iteration 111/1000 | Loss: 0.00001378
Iteration 112/1000 | Loss: 0.00001378
Iteration 113/1000 | Loss: 0.00001378
Iteration 114/1000 | Loss: 0.00001378
Iteration 115/1000 | Loss: 0.00001378
Iteration 116/1000 | Loss: 0.00001378
Iteration 117/1000 | Loss: 0.00001377
Iteration 118/1000 | Loss: 0.00001377
Iteration 119/1000 | Loss: 0.00001377
Iteration 120/1000 | Loss: 0.00001376
Iteration 121/1000 | Loss: 0.00001376
Iteration 122/1000 | Loss: 0.00001376
Iteration 123/1000 | Loss: 0.00001375
Iteration 124/1000 | Loss: 0.00001375
Iteration 125/1000 | Loss: 0.00001375
Iteration 126/1000 | Loss: 0.00001374
Iteration 127/1000 | Loss: 0.00001374
Iteration 128/1000 | Loss: 0.00001374
Iteration 129/1000 | Loss: 0.00001373
Iteration 130/1000 | Loss: 0.00001373
Iteration 131/1000 | Loss: 0.00001373
Iteration 132/1000 | Loss: 0.00001373
Iteration 133/1000 | Loss: 0.00001373
Iteration 134/1000 | Loss: 0.00001373
Iteration 135/1000 | Loss: 0.00001373
Iteration 136/1000 | Loss: 0.00001373
Iteration 137/1000 | Loss: 0.00001372
Iteration 138/1000 | Loss: 0.00001372
Iteration 139/1000 | Loss: 0.00001372
Iteration 140/1000 | Loss: 0.00001371
Iteration 141/1000 | Loss: 0.00001371
Iteration 142/1000 | Loss: 0.00001371
Iteration 143/1000 | Loss: 0.00001371
Iteration 144/1000 | Loss: 0.00001370
Iteration 145/1000 | Loss: 0.00001370
Iteration 146/1000 | Loss: 0.00001370
Iteration 147/1000 | Loss: 0.00001370
Iteration 148/1000 | Loss: 0.00001369
Iteration 149/1000 | Loss: 0.00001369
Iteration 150/1000 | Loss: 0.00001369
Iteration 151/1000 | Loss: 0.00001369
Iteration 152/1000 | Loss: 0.00001369
Iteration 153/1000 | Loss: 0.00001368
Iteration 154/1000 | Loss: 0.00001368
Iteration 155/1000 | Loss: 0.00001368
Iteration 156/1000 | Loss: 0.00001368
Iteration 157/1000 | Loss: 0.00001368
Iteration 158/1000 | Loss: 0.00001368
Iteration 159/1000 | Loss: 0.00001368
Iteration 160/1000 | Loss: 0.00001368
Iteration 161/1000 | Loss: 0.00001368
Iteration 162/1000 | Loss: 0.00001368
Iteration 163/1000 | Loss: 0.00001368
Iteration 164/1000 | Loss: 0.00001367
Iteration 165/1000 | Loss: 0.00001367
Iteration 166/1000 | Loss: 0.00001367
Iteration 167/1000 | Loss: 0.00001367
Iteration 168/1000 | Loss: 0.00001367
Iteration 169/1000 | Loss: 0.00001367
Iteration 170/1000 | Loss: 0.00001367
Iteration 171/1000 | Loss: 0.00001366
Iteration 172/1000 | Loss: 0.00001366
Iteration 173/1000 | Loss: 0.00001366
Iteration 174/1000 | Loss: 0.00001366
Iteration 175/1000 | Loss: 0.00001366
Iteration 176/1000 | Loss: 0.00001366
Iteration 177/1000 | Loss: 0.00001366
Iteration 178/1000 | Loss: 0.00001366
Iteration 179/1000 | Loss: 0.00001365
Iteration 180/1000 | Loss: 0.00001365
Iteration 181/1000 | Loss: 0.00001365
Iteration 182/1000 | Loss: 0.00001365
Iteration 183/1000 | Loss: 0.00001365
Iteration 184/1000 | Loss: 0.00001365
Iteration 185/1000 | Loss: 0.00001365
Iteration 186/1000 | Loss: 0.00001365
Iteration 187/1000 | Loss: 0.00001365
Iteration 188/1000 | Loss: 0.00001365
Iteration 189/1000 | Loss: 0.00001365
Iteration 190/1000 | Loss: 0.00001364
Iteration 191/1000 | Loss: 0.00001364
Iteration 192/1000 | Loss: 0.00001364
Iteration 193/1000 | Loss: 0.00001364
Iteration 194/1000 | Loss: 0.00001364
Iteration 195/1000 | Loss: 0.00001364
Iteration 196/1000 | Loss: 0.00001363
Iteration 197/1000 | Loss: 0.00001363
Iteration 198/1000 | Loss: 0.00001363
Iteration 199/1000 | Loss: 0.00001363
Iteration 200/1000 | Loss: 0.00001363
Iteration 201/1000 | Loss: 0.00001363
Iteration 202/1000 | Loss: 0.00001362
Iteration 203/1000 | Loss: 0.00001362
Iteration 204/1000 | Loss: 0.00001362
Iteration 205/1000 | Loss: 0.00001362
Iteration 206/1000 | Loss: 0.00001362
Iteration 207/1000 | Loss: 0.00001361
Iteration 208/1000 | Loss: 0.00001361
Iteration 209/1000 | Loss: 0.00001361
Iteration 210/1000 | Loss: 0.00001361
Iteration 211/1000 | Loss: 0.00001361
Iteration 212/1000 | Loss: 0.00001361
Iteration 213/1000 | Loss: 0.00001361
Iteration 214/1000 | Loss: 0.00001361
Iteration 215/1000 | Loss: 0.00001361
Iteration 216/1000 | Loss: 0.00001361
Iteration 217/1000 | Loss: 0.00001360
Iteration 218/1000 | Loss: 0.00001360
Iteration 219/1000 | Loss: 0.00001360
Iteration 220/1000 | Loss: 0.00001360
Iteration 221/1000 | Loss: 0.00001360
Iteration 222/1000 | Loss: 0.00001360
Iteration 223/1000 | Loss: 0.00001360
Iteration 224/1000 | Loss: 0.00001360
Iteration 225/1000 | Loss: 0.00001360
Iteration 226/1000 | Loss: 0.00001360
Iteration 227/1000 | Loss: 0.00001360
Iteration 228/1000 | Loss: 0.00001360
Iteration 229/1000 | Loss: 0.00001360
Iteration 230/1000 | Loss: 0.00001359
Iteration 231/1000 | Loss: 0.00001359
Iteration 232/1000 | Loss: 0.00001359
Iteration 233/1000 | Loss: 0.00001359
Iteration 234/1000 | Loss: 0.00001359
Iteration 235/1000 | Loss: 0.00001359
Iteration 236/1000 | Loss: 0.00001359
Iteration 237/1000 | Loss: 0.00001359
Iteration 238/1000 | Loss: 0.00001359
Iteration 239/1000 | Loss: 0.00001359
Iteration 240/1000 | Loss: 0.00001359
Iteration 241/1000 | Loss: 0.00001359
Iteration 242/1000 | Loss: 0.00001359
Iteration 243/1000 | Loss: 0.00001359
Iteration 244/1000 | Loss: 0.00001359
Iteration 245/1000 | Loss: 0.00001359
Iteration 246/1000 | Loss: 0.00001359
Iteration 247/1000 | Loss: 0.00001359
Iteration 248/1000 | Loss: 0.00001359
Iteration 249/1000 | Loss: 0.00001359
Iteration 250/1000 | Loss: 0.00001359
Iteration 251/1000 | Loss: 0.00001359
Iteration 252/1000 | Loss: 0.00001359
Iteration 253/1000 | Loss: 0.00001359
Iteration 254/1000 | Loss: 0.00001359
Iteration 255/1000 | Loss: 0.00001359
Iteration 256/1000 | Loss: 0.00001359
Iteration 257/1000 | Loss: 0.00001359
Iteration 258/1000 | Loss: 0.00001359
Iteration 259/1000 | Loss: 0.00001359
Iteration 260/1000 | Loss: 0.00001359
Iteration 261/1000 | Loss: 0.00001359
Iteration 262/1000 | Loss: 0.00001359
Iteration 263/1000 | Loss: 0.00001359
Iteration 264/1000 | Loss: 0.00001359
Iteration 265/1000 | Loss: 0.00001359
Iteration 266/1000 | Loss: 0.00001359
Iteration 267/1000 | Loss: 0.00001359
Iteration 268/1000 | Loss: 0.00001359
Iteration 269/1000 | Loss: 0.00001359
Iteration 270/1000 | Loss: 0.00001359
Iteration 271/1000 | Loss: 0.00001359
Iteration 272/1000 | Loss: 0.00001359
Iteration 273/1000 | Loss: 0.00001359
Iteration 274/1000 | Loss: 0.00001359
Iteration 275/1000 | Loss: 0.00001359
Iteration 276/1000 | Loss: 0.00001359
Iteration 277/1000 | Loss: 0.00001359
Iteration 278/1000 | Loss: 0.00001359
Iteration 279/1000 | Loss: 0.00001359
Iteration 280/1000 | Loss: 0.00001359
Iteration 281/1000 | Loss: 0.00001359
Iteration 282/1000 | Loss: 0.00001359
Iteration 283/1000 | Loss: 0.00001359
Iteration 284/1000 | Loss: 0.00001359
Iteration 285/1000 | Loss: 0.00001359
Iteration 286/1000 | Loss: 0.00001359
Iteration 287/1000 | Loss: 0.00001359
Iteration 288/1000 | Loss: 0.00001359
Iteration 289/1000 | Loss: 0.00001359
Iteration 290/1000 | Loss: 0.00001359
Iteration 291/1000 | Loss: 0.00001359
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 291. Stopping optimization.
Last 5 losses: [1.3590471098723356e-05, 1.3590471098723356e-05, 1.3590471098723356e-05, 1.3590471098723356e-05, 1.3590471098723356e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3590471098723356e-05

Optimization complete. Final v2v error: 2.9264049530029297 mm

Highest mean error: 4.545541286468506 mm for frame 72

Lowest mean error: 2.445305347442627 mm for frame 161

Saving results

Total time: 44.057841777801514
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00812623
Iteration 2/25 | Loss: 0.00128183
Iteration 3/25 | Loss: 0.00109031
Iteration 4/25 | Loss: 0.00107452
Iteration 5/25 | Loss: 0.00107120
Iteration 6/25 | Loss: 0.00107120
Iteration 7/25 | Loss: 0.00107120
Iteration 8/25 | Loss: 0.00107120
Iteration 9/25 | Loss: 0.00107120
Iteration 10/25 | Loss: 0.00107120
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010711966315284371, 0.0010711966315284371, 0.0010711966315284371, 0.0010711966315284371, 0.0010711966315284371]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010711966315284371

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37803996
Iteration 2/25 | Loss: 0.00073983
Iteration 3/25 | Loss: 0.00073983
Iteration 4/25 | Loss: 0.00073983
Iteration 5/25 | Loss: 0.00073983
Iteration 6/25 | Loss: 0.00073983
Iteration 7/25 | Loss: 0.00073983
Iteration 8/25 | Loss: 0.00073983
Iteration 9/25 | Loss: 0.00073983
Iteration 10/25 | Loss: 0.00073983
Iteration 11/25 | Loss: 0.00073983
Iteration 12/25 | Loss: 0.00073983
Iteration 13/25 | Loss: 0.00073983
Iteration 14/25 | Loss: 0.00073983
Iteration 15/25 | Loss: 0.00073983
Iteration 16/25 | Loss: 0.00073983
Iteration 17/25 | Loss: 0.00073983
Iteration 18/25 | Loss: 0.00073983
Iteration 19/25 | Loss: 0.00073983
Iteration 20/25 | Loss: 0.00073983
Iteration 21/25 | Loss: 0.00073983
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007398290908895433, 0.0007398290908895433, 0.0007398290908895433, 0.0007398290908895433, 0.0007398290908895433]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007398290908895433

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073983
Iteration 2/1000 | Loss: 0.00002397
Iteration 3/1000 | Loss: 0.00001299
Iteration 4/1000 | Loss: 0.00001165
Iteration 5/1000 | Loss: 0.00001100
Iteration 6/1000 | Loss: 0.00001043
Iteration 7/1000 | Loss: 0.00001020
Iteration 8/1000 | Loss: 0.00000992
Iteration 9/1000 | Loss: 0.00000965
Iteration 10/1000 | Loss: 0.00000956
Iteration 11/1000 | Loss: 0.00000955
Iteration 12/1000 | Loss: 0.00000945
Iteration 13/1000 | Loss: 0.00000941
Iteration 14/1000 | Loss: 0.00000941
Iteration 15/1000 | Loss: 0.00000940
Iteration 16/1000 | Loss: 0.00000940
Iteration 17/1000 | Loss: 0.00000938
Iteration 18/1000 | Loss: 0.00000935
Iteration 19/1000 | Loss: 0.00000934
Iteration 20/1000 | Loss: 0.00000934
Iteration 21/1000 | Loss: 0.00000933
Iteration 22/1000 | Loss: 0.00000932
Iteration 23/1000 | Loss: 0.00000931
Iteration 24/1000 | Loss: 0.00000931
Iteration 25/1000 | Loss: 0.00000931
Iteration 26/1000 | Loss: 0.00000931
Iteration 27/1000 | Loss: 0.00000930
Iteration 28/1000 | Loss: 0.00000930
Iteration 29/1000 | Loss: 0.00000929
Iteration 30/1000 | Loss: 0.00000929
Iteration 31/1000 | Loss: 0.00000928
Iteration 32/1000 | Loss: 0.00000928
Iteration 33/1000 | Loss: 0.00000928
Iteration 34/1000 | Loss: 0.00000928
Iteration 35/1000 | Loss: 0.00000928
Iteration 36/1000 | Loss: 0.00000927
Iteration 37/1000 | Loss: 0.00000927
Iteration 38/1000 | Loss: 0.00000927
Iteration 39/1000 | Loss: 0.00000927
Iteration 40/1000 | Loss: 0.00000927
Iteration 41/1000 | Loss: 0.00000927
Iteration 42/1000 | Loss: 0.00000926
Iteration 43/1000 | Loss: 0.00000925
Iteration 44/1000 | Loss: 0.00000925
Iteration 45/1000 | Loss: 0.00000925
Iteration 46/1000 | Loss: 0.00000924
Iteration 47/1000 | Loss: 0.00000924
Iteration 48/1000 | Loss: 0.00000924
Iteration 49/1000 | Loss: 0.00000924
Iteration 50/1000 | Loss: 0.00000923
Iteration 51/1000 | Loss: 0.00000923
Iteration 52/1000 | Loss: 0.00000923
Iteration 53/1000 | Loss: 0.00000923
Iteration 54/1000 | Loss: 0.00000923
Iteration 55/1000 | Loss: 0.00000923
Iteration 56/1000 | Loss: 0.00000923
Iteration 57/1000 | Loss: 0.00000923
Iteration 58/1000 | Loss: 0.00000923
Iteration 59/1000 | Loss: 0.00000923
Iteration 60/1000 | Loss: 0.00000922
Iteration 61/1000 | Loss: 0.00000922
Iteration 62/1000 | Loss: 0.00000922
Iteration 63/1000 | Loss: 0.00000921
Iteration 64/1000 | Loss: 0.00000921
Iteration 65/1000 | Loss: 0.00000921
Iteration 66/1000 | Loss: 0.00000920
Iteration 67/1000 | Loss: 0.00000920
Iteration 68/1000 | Loss: 0.00000919
Iteration 69/1000 | Loss: 0.00000919
Iteration 70/1000 | Loss: 0.00000919
Iteration 71/1000 | Loss: 0.00000919
Iteration 72/1000 | Loss: 0.00000919
Iteration 73/1000 | Loss: 0.00000918
Iteration 74/1000 | Loss: 0.00000918
Iteration 75/1000 | Loss: 0.00000918
Iteration 76/1000 | Loss: 0.00000917
Iteration 77/1000 | Loss: 0.00000917
Iteration 78/1000 | Loss: 0.00000917
Iteration 79/1000 | Loss: 0.00000917
Iteration 80/1000 | Loss: 0.00000917
Iteration 81/1000 | Loss: 0.00000917
Iteration 82/1000 | Loss: 0.00000917
Iteration 83/1000 | Loss: 0.00000917
Iteration 84/1000 | Loss: 0.00000916
Iteration 85/1000 | Loss: 0.00000916
Iteration 86/1000 | Loss: 0.00000915
Iteration 87/1000 | Loss: 0.00000915
Iteration 88/1000 | Loss: 0.00000915
Iteration 89/1000 | Loss: 0.00000915
Iteration 90/1000 | Loss: 0.00000914
Iteration 91/1000 | Loss: 0.00000914
Iteration 92/1000 | Loss: 0.00000913
Iteration 93/1000 | Loss: 0.00000913
Iteration 94/1000 | Loss: 0.00000912
Iteration 95/1000 | Loss: 0.00000912
Iteration 96/1000 | Loss: 0.00000912
Iteration 97/1000 | Loss: 0.00000912
Iteration 98/1000 | Loss: 0.00000911
Iteration 99/1000 | Loss: 0.00000911
Iteration 100/1000 | Loss: 0.00000911
Iteration 101/1000 | Loss: 0.00000911
Iteration 102/1000 | Loss: 0.00000911
Iteration 103/1000 | Loss: 0.00000911
Iteration 104/1000 | Loss: 0.00000911
Iteration 105/1000 | Loss: 0.00000911
Iteration 106/1000 | Loss: 0.00000911
Iteration 107/1000 | Loss: 0.00000910
Iteration 108/1000 | Loss: 0.00000910
Iteration 109/1000 | Loss: 0.00000910
Iteration 110/1000 | Loss: 0.00000910
Iteration 111/1000 | Loss: 0.00000910
Iteration 112/1000 | Loss: 0.00000909
Iteration 113/1000 | Loss: 0.00000909
Iteration 114/1000 | Loss: 0.00000909
Iteration 115/1000 | Loss: 0.00000909
Iteration 116/1000 | Loss: 0.00000909
Iteration 117/1000 | Loss: 0.00000909
Iteration 118/1000 | Loss: 0.00000908
Iteration 119/1000 | Loss: 0.00000908
Iteration 120/1000 | Loss: 0.00000908
Iteration 121/1000 | Loss: 0.00000908
Iteration 122/1000 | Loss: 0.00000908
Iteration 123/1000 | Loss: 0.00000908
Iteration 124/1000 | Loss: 0.00000907
Iteration 125/1000 | Loss: 0.00000907
Iteration 126/1000 | Loss: 0.00000907
Iteration 127/1000 | Loss: 0.00000907
Iteration 128/1000 | Loss: 0.00000907
Iteration 129/1000 | Loss: 0.00000907
Iteration 130/1000 | Loss: 0.00000907
Iteration 131/1000 | Loss: 0.00000907
Iteration 132/1000 | Loss: 0.00000907
Iteration 133/1000 | Loss: 0.00000907
Iteration 134/1000 | Loss: 0.00000906
Iteration 135/1000 | Loss: 0.00000906
Iteration 136/1000 | Loss: 0.00000906
Iteration 137/1000 | Loss: 0.00000906
Iteration 138/1000 | Loss: 0.00000906
Iteration 139/1000 | Loss: 0.00000906
Iteration 140/1000 | Loss: 0.00000905
Iteration 141/1000 | Loss: 0.00000905
Iteration 142/1000 | Loss: 0.00000905
Iteration 143/1000 | Loss: 0.00000905
Iteration 144/1000 | Loss: 0.00000905
Iteration 145/1000 | Loss: 0.00000905
Iteration 146/1000 | Loss: 0.00000905
Iteration 147/1000 | Loss: 0.00000905
Iteration 148/1000 | Loss: 0.00000905
Iteration 149/1000 | Loss: 0.00000905
Iteration 150/1000 | Loss: 0.00000905
Iteration 151/1000 | Loss: 0.00000904
Iteration 152/1000 | Loss: 0.00000904
Iteration 153/1000 | Loss: 0.00000904
Iteration 154/1000 | Loss: 0.00000904
Iteration 155/1000 | Loss: 0.00000904
Iteration 156/1000 | Loss: 0.00000904
Iteration 157/1000 | Loss: 0.00000904
Iteration 158/1000 | Loss: 0.00000904
Iteration 159/1000 | Loss: 0.00000904
Iteration 160/1000 | Loss: 0.00000904
Iteration 161/1000 | Loss: 0.00000904
Iteration 162/1000 | Loss: 0.00000903
Iteration 163/1000 | Loss: 0.00000903
Iteration 164/1000 | Loss: 0.00000903
Iteration 165/1000 | Loss: 0.00000903
Iteration 166/1000 | Loss: 0.00000903
Iteration 167/1000 | Loss: 0.00000903
Iteration 168/1000 | Loss: 0.00000903
Iteration 169/1000 | Loss: 0.00000903
Iteration 170/1000 | Loss: 0.00000903
Iteration 171/1000 | Loss: 0.00000903
Iteration 172/1000 | Loss: 0.00000902
Iteration 173/1000 | Loss: 0.00000902
Iteration 174/1000 | Loss: 0.00000902
Iteration 175/1000 | Loss: 0.00000902
Iteration 176/1000 | Loss: 0.00000902
Iteration 177/1000 | Loss: 0.00000902
Iteration 178/1000 | Loss: 0.00000902
Iteration 179/1000 | Loss: 0.00000902
Iteration 180/1000 | Loss: 0.00000902
Iteration 181/1000 | Loss: 0.00000902
Iteration 182/1000 | Loss: 0.00000901
Iteration 183/1000 | Loss: 0.00000901
Iteration 184/1000 | Loss: 0.00000901
Iteration 185/1000 | Loss: 0.00000901
Iteration 186/1000 | Loss: 0.00000901
Iteration 187/1000 | Loss: 0.00000900
Iteration 188/1000 | Loss: 0.00000900
Iteration 189/1000 | Loss: 0.00000900
Iteration 190/1000 | Loss: 0.00000900
Iteration 191/1000 | Loss: 0.00000900
Iteration 192/1000 | Loss: 0.00000900
Iteration 193/1000 | Loss: 0.00000900
Iteration 194/1000 | Loss: 0.00000900
Iteration 195/1000 | Loss: 0.00000900
Iteration 196/1000 | Loss: 0.00000900
Iteration 197/1000 | Loss: 0.00000900
Iteration 198/1000 | Loss: 0.00000900
Iteration 199/1000 | Loss: 0.00000900
Iteration 200/1000 | Loss: 0.00000900
Iteration 201/1000 | Loss: 0.00000900
Iteration 202/1000 | Loss: 0.00000900
Iteration 203/1000 | Loss: 0.00000900
Iteration 204/1000 | Loss: 0.00000900
Iteration 205/1000 | Loss: 0.00000900
Iteration 206/1000 | Loss: 0.00000900
Iteration 207/1000 | Loss: 0.00000900
Iteration 208/1000 | Loss: 0.00000900
Iteration 209/1000 | Loss: 0.00000900
Iteration 210/1000 | Loss: 0.00000900
Iteration 211/1000 | Loss: 0.00000900
Iteration 212/1000 | Loss: 0.00000900
Iteration 213/1000 | Loss: 0.00000900
Iteration 214/1000 | Loss: 0.00000900
Iteration 215/1000 | Loss: 0.00000900
Iteration 216/1000 | Loss: 0.00000900
Iteration 217/1000 | Loss: 0.00000900
Iteration 218/1000 | Loss: 0.00000900
Iteration 219/1000 | Loss: 0.00000900
Iteration 220/1000 | Loss: 0.00000900
Iteration 221/1000 | Loss: 0.00000900
Iteration 222/1000 | Loss: 0.00000900
Iteration 223/1000 | Loss: 0.00000900
Iteration 224/1000 | Loss: 0.00000900
Iteration 225/1000 | Loss: 0.00000900
Iteration 226/1000 | Loss: 0.00000900
Iteration 227/1000 | Loss: 0.00000900
Iteration 228/1000 | Loss: 0.00000900
Iteration 229/1000 | Loss: 0.00000900
Iteration 230/1000 | Loss: 0.00000900
Iteration 231/1000 | Loss: 0.00000900
Iteration 232/1000 | Loss: 0.00000900
Iteration 233/1000 | Loss: 0.00000900
Iteration 234/1000 | Loss: 0.00000900
Iteration 235/1000 | Loss: 0.00000900
Iteration 236/1000 | Loss: 0.00000900
Iteration 237/1000 | Loss: 0.00000900
Iteration 238/1000 | Loss: 0.00000900
Iteration 239/1000 | Loss: 0.00000900
Iteration 240/1000 | Loss: 0.00000900
Iteration 241/1000 | Loss: 0.00000900
Iteration 242/1000 | Loss: 0.00000900
Iteration 243/1000 | Loss: 0.00000900
Iteration 244/1000 | Loss: 0.00000900
Iteration 245/1000 | Loss: 0.00000900
Iteration 246/1000 | Loss: 0.00000900
Iteration 247/1000 | Loss: 0.00000900
Iteration 248/1000 | Loss: 0.00000900
Iteration 249/1000 | Loss: 0.00000900
Iteration 250/1000 | Loss: 0.00000900
Iteration 251/1000 | Loss: 0.00000900
Iteration 252/1000 | Loss: 0.00000900
Iteration 253/1000 | Loss: 0.00000900
Iteration 254/1000 | Loss: 0.00000900
Iteration 255/1000 | Loss: 0.00000900
Iteration 256/1000 | Loss: 0.00000900
Iteration 257/1000 | Loss: 0.00000900
Iteration 258/1000 | Loss: 0.00000900
Iteration 259/1000 | Loss: 0.00000900
Iteration 260/1000 | Loss: 0.00000900
Iteration 261/1000 | Loss: 0.00000900
Iteration 262/1000 | Loss: 0.00000900
Iteration 263/1000 | Loss: 0.00000900
Iteration 264/1000 | Loss: 0.00000900
Iteration 265/1000 | Loss: 0.00000900
Iteration 266/1000 | Loss: 0.00000900
Iteration 267/1000 | Loss: 0.00000900
Iteration 268/1000 | Loss: 0.00000900
Iteration 269/1000 | Loss: 0.00000900
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 269. Stopping optimization.
Last 5 losses: [9.002626029541716e-06, 9.002626029541716e-06, 9.002626029541716e-06, 9.002626029541716e-06, 9.002626029541716e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.002626029541716e-06

Optimization complete. Final v2v error: 2.578626871109009 mm

Highest mean error: 2.763305187225342 mm for frame 150

Lowest mean error: 2.4396798610687256 mm for frame 75

Saving results

Total time: 42.856365442276
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00981377
Iteration 2/25 | Loss: 0.00981377
Iteration 3/25 | Loss: 0.00981377
Iteration 4/25 | Loss: 0.00277479
Iteration 5/25 | Loss: 0.00201409
Iteration 6/25 | Loss: 0.00180994
Iteration 7/25 | Loss: 0.00172727
Iteration 8/25 | Loss: 0.00170108
Iteration 9/25 | Loss: 0.00169209
Iteration 10/25 | Loss: 0.00170202
Iteration 11/25 | Loss: 0.00165903
Iteration 12/25 | Loss: 0.00157869
Iteration 13/25 | Loss: 0.00155763
Iteration 14/25 | Loss: 0.00153292
Iteration 15/25 | Loss: 0.00154308
Iteration 16/25 | Loss: 0.00151124
Iteration 17/25 | Loss: 0.00149498
Iteration 18/25 | Loss: 0.00148311
Iteration 19/25 | Loss: 0.00146412
Iteration 20/25 | Loss: 0.00145996
Iteration 21/25 | Loss: 0.00144690
Iteration 22/25 | Loss: 0.00143087
Iteration 23/25 | Loss: 0.00143097
Iteration 24/25 | Loss: 0.00142270
Iteration 25/25 | Loss: 0.00141304

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38133740
Iteration 2/25 | Loss: 0.00436517
Iteration 3/25 | Loss: 0.00293689
Iteration 4/25 | Loss: 0.00293689
Iteration 5/25 | Loss: 0.00293689
Iteration 6/25 | Loss: 0.00293689
Iteration 7/25 | Loss: 0.00293689
Iteration 8/25 | Loss: 0.00293689
Iteration 9/25 | Loss: 0.00293689
Iteration 10/25 | Loss: 0.00293689
Iteration 11/25 | Loss: 0.00293689
Iteration 12/25 | Loss: 0.00293689
Iteration 13/25 | Loss: 0.00293689
Iteration 14/25 | Loss: 0.00293689
Iteration 15/25 | Loss: 0.00293689
Iteration 16/25 | Loss: 0.00293689
Iteration 17/25 | Loss: 0.00293689
Iteration 18/25 | Loss: 0.00293689
Iteration 19/25 | Loss: 0.00293689
Iteration 20/25 | Loss: 0.00293689
Iteration 21/25 | Loss: 0.00293689
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0029368852265179157, 0.0029368852265179157, 0.0029368852265179157, 0.0029368852265179157, 0.0029368852265179157]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0029368852265179157

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00293689
Iteration 2/1000 | Loss: 0.00339967
Iteration 3/1000 | Loss: 0.01271274
Iteration 4/1000 | Loss: 0.00063552
Iteration 5/1000 | Loss: 0.00219050
Iteration 6/1000 | Loss: 0.00082331
Iteration 7/1000 | Loss: 0.00134606
Iteration 8/1000 | Loss: 0.00056386
Iteration 9/1000 | Loss: 0.00077337
Iteration 10/1000 | Loss: 0.00095058
Iteration 11/1000 | Loss: 0.00046066
Iteration 12/1000 | Loss: 0.00053049
Iteration 13/1000 | Loss: 0.00051279
Iteration 14/1000 | Loss: 0.00065407
Iteration 15/1000 | Loss: 0.00030538
Iteration 16/1000 | Loss: 0.00037986
Iteration 17/1000 | Loss: 0.00106407
Iteration 18/1000 | Loss: 0.00044909
Iteration 19/1000 | Loss: 0.00025801
Iteration 20/1000 | Loss: 0.00046644
Iteration 21/1000 | Loss: 0.00110069
Iteration 22/1000 | Loss: 0.00069837
Iteration 23/1000 | Loss: 0.00050039
Iteration 24/1000 | Loss: 0.00032793
Iteration 25/1000 | Loss: 0.00047761
Iteration 26/1000 | Loss: 0.00229401
Iteration 27/1000 | Loss: 0.00177216
Iteration 28/1000 | Loss: 0.00051593
Iteration 29/1000 | Loss: 0.00046786
Iteration 30/1000 | Loss: 0.00026008
Iteration 31/1000 | Loss: 0.00063814
Iteration 32/1000 | Loss: 0.00029763
Iteration 33/1000 | Loss: 0.00020224
Iteration 34/1000 | Loss: 0.00045625
Iteration 35/1000 | Loss: 0.00016347
Iteration 36/1000 | Loss: 0.00049600
Iteration 37/1000 | Loss: 0.00046005
Iteration 38/1000 | Loss: 0.00194020
Iteration 39/1000 | Loss: 0.00033084
Iteration 40/1000 | Loss: 0.00064727
Iteration 41/1000 | Loss: 0.00017607
Iteration 42/1000 | Loss: 0.00018441
Iteration 43/1000 | Loss: 0.00014291
Iteration 44/1000 | Loss: 0.00016171
Iteration 45/1000 | Loss: 0.00085679
Iteration 46/1000 | Loss: 0.00021380
Iteration 47/1000 | Loss: 0.00030301
Iteration 48/1000 | Loss: 0.00278082
Iteration 49/1000 | Loss: 0.00027496
Iteration 50/1000 | Loss: 0.00068688
Iteration 51/1000 | Loss: 0.00042201
Iteration 52/1000 | Loss: 0.00073421
Iteration 53/1000 | Loss: 0.00041474
Iteration 54/1000 | Loss: 0.00158441
Iteration 55/1000 | Loss: 0.00148691
Iteration 56/1000 | Loss: 0.00018368
Iteration 57/1000 | Loss: 0.00026783
Iteration 58/1000 | Loss: 0.00034012
Iteration 59/1000 | Loss: 0.00064317
Iteration 60/1000 | Loss: 0.00290316
Iteration 61/1000 | Loss: 0.00063248
Iteration 62/1000 | Loss: 0.00015463
Iteration 63/1000 | Loss: 0.00017958
Iteration 64/1000 | Loss: 0.00035056
Iteration 65/1000 | Loss: 0.00035374
Iteration 66/1000 | Loss: 0.00027205
Iteration 67/1000 | Loss: 0.00155162
Iteration 68/1000 | Loss: 0.00029015
Iteration 69/1000 | Loss: 0.00019088
Iteration 70/1000 | Loss: 0.00015290
Iteration 71/1000 | Loss: 0.00047237
Iteration 72/1000 | Loss: 0.00032308
Iteration 73/1000 | Loss: 0.00081643
Iteration 74/1000 | Loss: 0.00037505
Iteration 75/1000 | Loss: 0.00041021
Iteration 76/1000 | Loss: 0.00012765
Iteration 77/1000 | Loss: 0.00012691
Iteration 78/1000 | Loss: 0.00018441
Iteration 79/1000 | Loss: 0.00049898
Iteration 80/1000 | Loss: 0.00027196
Iteration 81/1000 | Loss: 0.00021617
Iteration 82/1000 | Loss: 0.00016865
Iteration 83/1000 | Loss: 0.00008710
Iteration 84/1000 | Loss: 0.00034785
Iteration 85/1000 | Loss: 0.00171929
Iteration 86/1000 | Loss: 0.00048275
Iteration 87/1000 | Loss: 0.00220441
Iteration 88/1000 | Loss: 0.00303169
Iteration 89/1000 | Loss: 0.00239832
Iteration 90/1000 | Loss: 0.00228090
Iteration 91/1000 | Loss: 0.00070483
Iteration 92/1000 | Loss: 0.00018819
Iteration 93/1000 | Loss: 0.00083608
Iteration 94/1000 | Loss: 0.00020623
Iteration 95/1000 | Loss: 0.00029513
Iteration 96/1000 | Loss: 0.00138217
Iteration 97/1000 | Loss: 0.00100362
Iteration 98/1000 | Loss: 0.00026226
Iteration 99/1000 | Loss: 0.00018946
Iteration 100/1000 | Loss: 0.00046771
Iteration 101/1000 | Loss: 0.00014747
Iteration 102/1000 | Loss: 0.00012660
Iteration 103/1000 | Loss: 0.00109875
Iteration 104/1000 | Loss: 0.00120865
Iteration 105/1000 | Loss: 0.00234876
Iteration 106/1000 | Loss: 0.00177568
Iteration 107/1000 | Loss: 0.00117425
Iteration 108/1000 | Loss: 0.00183384
Iteration 109/1000 | Loss: 0.00081305
Iteration 110/1000 | Loss: 0.00097139
Iteration 111/1000 | Loss: 0.00043798
Iteration 112/1000 | Loss: 0.00082480
Iteration 113/1000 | Loss: 0.00074625
Iteration 114/1000 | Loss: 0.00060348
Iteration 115/1000 | Loss: 0.00089542
Iteration 116/1000 | Loss: 0.00094573
Iteration 117/1000 | Loss: 0.00010163
Iteration 118/1000 | Loss: 0.00025349
Iteration 119/1000 | Loss: 0.00130981
Iteration 120/1000 | Loss: 0.00040025
Iteration 121/1000 | Loss: 0.00076942
Iteration 122/1000 | Loss: 0.00040505
Iteration 123/1000 | Loss: 0.00010611
Iteration 124/1000 | Loss: 0.00041480
Iteration 125/1000 | Loss: 0.00009440
Iteration 126/1000 | Loss: 0.00124849
Iteration 127/1000 | Loss: 0.00159145
Iteration 128/1000 | Loss: 0.00296147
Iteration 129/1000 | Loss: 0.00125034
Iteration 130/1000 | Loss: 0.00297227
Iteration 131/1000 | Loss: 0.00182540
Iteration 132/1000 | Loss: 0.00054244
Iteration 133/1000 | Loss: 0.00077979
Iteration 134/1000 | Loss: 0.00086851
Iteration 135/1000 | Loss: 0.00098260
Iteration 136/1000 | Loss: 0.00068360
Iteration 137/1000 | Loss: 0.00052452
Iteration 138/1000 | Loss: 0.00065081
Iteration 139/1000 | Loss: 0.00047271
Iteration 140/1000 | Loss: 0.00066053
Iteration 141/1000 | Loss: 0.00008795
Iteration 142/1000 | Loss: 0.00016610
Iteration 143/1000 | Loss: 0.00005589
Iteration 144/1000 | Loss: 0.00025318
Iteration 145/1000 | Loss: 0.00032620
Iteration 146/1000 | Loss: 0.00021778
Iteration 147/1000 | Loss: 0.00007696
Iteration 148/1000 | Loss: 0.00006461
Iteration 149/1000 | Loss: 0.00006042
Iteration 150/1000 | Loss: 0.00048198
Iteration 151/1000 | Loss: 0.00059809
Iteration 152/1000 | Loss: 0.00008713
Iteration 153/1000 | Loss: 0.00022502
Iteration 154/1000 | Loss: 0.00012932
Iteration 155/1000 | Loss: 0.00010414
Iteration 156/1000 | Loss: 0.00005012
Iteration 157/1000 | Loss: 0.00021026
Iteration 158/1000 | Loss: 0.00004551
Iteration 159/1000 | Loss: 0.00066617
Iteration 160/1000 | Loss: 0.00010681
Iteration 161/1000 | Loss: 0.00025057
Iteration 162/1000 | Loss: 0.00005191
Iteration 163/1000 | Loss: 0.00007826
Iteration 164/1000 | Loss: 0.00006207
Iteration 165/1000 | Loss: 0.00010316
Iteration 166/1000 | Loss: 0.00006613
Iteration 167/1000 | Loss: 0.00009808
Iteration 168/1000 | Loss: 0.00058748
Iteration 169/1000 | Loss: 0.00007722
Iteration 170/1000 | Loss: 0.00006969
Iteration 171/1000 | Loss: 0.00022433
Iteration 172/1000 | Loss: 0.00030978
Iteration 173/1000 | Loss: 0.00078461
Iteration 174/1000 | Loss: 0.00022570
Iteration 175/1000 | Loss: 0.00003797
Iteration 176/1000 | Loss: 0.00014176
Iteration 177/1000 | Loss: 0.00005020
Iteration 178/1000 | Loss: 0.00007566
Iteration 179/1000 | Loss: 0.00017674
Iteration 180/1000 | Loss: 0.00003953
Iteration 181/1000 | Loss: 0.00006602
Iteration 182/1000 | Loss: 0.00021671
Iteration 183/1000 | Loss: 0.00010923
Iteration 184/1000 | Loss: 0.00005140
Iteration 185/1000 | Loss: 0.00008144
Iteration 186/1000 | Loss: 0.00033503
Iteration 187/1000 | Loss: 0.00020341
Iteration 188/1000 | Loss: 0.00018408
Iteration 189/1000 | Loss: 0.00013905
Iteration 190/1000 | Loss: 0.00013310
Iteration 191/1000 | Loss: 0.00003358
Iteration 192/1000 | Loss: 0.00004489
Iteration 193/1000 | Loss: 0.00005274
Iteration 194/1000 | Loss: 0.00006076
Iteration 195/1000 | Loss: 0.00005432
Iteration 196/1000 | Loss: 0.00007372
Iteration 197/1000 | Loss: 0.00006906
Iteration 198/1000 | Loss: 0.00068417
Iteration 199/1000 | Loss: 0.00008842
Iteration 200/1000 | Loss: 0.00004115
Iteration 201/1000 | Loss: 0.00003261
Iteration 202/1000 | Loss: 0.00017207
Iteration 203/1000 | Loss: 0.00002676
Iteration 204/1000 | Loss: 0.00019554
Iteration 205/1000 | Loss: 0.00013911
Iteration 206/1000 | Loss: 0.00033353
Iteration 207/1000 | Loss: 0.00011732
Iteration 208/1000 | Loss: 0.00002503
Iteration 209/1000 | Loss: 0.00013201
Iteration 210/1000 | Loss: 0.00009264
Iteration 211/1000 | Loss: 0.00006378
Iteration 212/1000 | Loss: 0.00004569
Iteration 213/1000 | Loss: 0.00005534
Iteration 214/1000 | Loss: 0.00002583
Iteration 215/1000 | Loss: 0.00004104
Iteration 216/1000 | Loss: 0.00001856
Iteration 217/1000 | Loss: 0.00002389
Iteration 218/1000 | Loss: 0.00106391
Iteration 219/1000 | Loss: 0.00104580
Iteration 220/1000 | Loss: 0.00002008
Iteration 221/1000 | Loss: 0.00002836
Iteration 222/1000 | Loss: 0.00002911
Iteration 223/1000 | Loss: 0.00001722
Iteration 224/1000 | Loss: 0.00001690
Iteration 225/1000 | Loss: 0.00001667
Iteration 226/1000 | Loss: 0.00001652
Iteration 227/1000 | Loss: 0.00001650
Iteration 228/1000 | Loss: 0.00001648
Iteration 229/1000 | Loss: 0.00001647
Iteration 230/1000 | Loss: 0.00106026
Iteration 231/1000 | Loss: 0.00067122
Iteration 232/1000 | Loss: 0.00002723
Iteration 233/1000 | Loss: 0.00001903
Iteration 234/1000 | Loss: 0.00001743
Iteration 235/1000 | Loss: 0.00001658
Iteration 236/1000 | Loss: 0.00001633
Iteration 237/1000 | Loss: 0.00001626
Iteration 238/1000 | Loss: 0.00001623
Iteration 239/1000 | Loss: 0.00001622
Iteration 240/1000 | Loss: 0.00001621
Iteration 241/1000 | Loss: 0.00001619
Iteration 242/1000 | Loss: 0.00001619
Iteration 243/1000 | Loss: 0.00001618
Iteration 244/1000 | Loss: 0.00007500
Iteration 245/1000 | Loss: 0.00069095
Iteration 246/1000 | Loss: 0.00032022
Iteration 247/1000 | Loss: 0.00014743
Iteration 248/1000 | Loss: 0.00011958
Iteration 249/1000 | Loss: 0.00003003
Iteration 250/1000 | Loss: 0.00001785
Iteration 251/1000 | Loss: 0.00007056
Iteration 252/1000 | Loss: 0.00003118
Iteration 253/1000 | Loss: 0.00001610
Iteration 254/1000 | Loss: 0.00001610
Iteration 255/1000 | Loss: 0.00001610
Iteration 256/1000 | Loss: 0.00001610
Iteration 257/1000 | Loss: 0.00001610
Iteration 258/1000 | Loss: 0.00001610
Iteration 259/1000 | Loss: 0.00001610
Iteration 260/1000 | Loss: 0.00001610
Iteration 261/1000 | Loss: 0.00001610
Iteration 262/1000 | Loss: 0.00001610
Iteration 263/1000 | Loss: 0.00001610
Iteration 264/1000 | Loss: 0.00001610
Iteration 265/1000 | Loss: 0.00001609
Iteration 266/1000 | Loss: 0.00001609
Iteration 267/1000 | Loss: 0.00001608
Iteration 268/1000 | Loss: 0.00001606
Iteration 269/1000 | Loss: 0.00001605
Iteration 270/1000 | Loss: 0.00001605
Iteration 271/1000 | Loss: 0.00001605
Iteration 272/1000 | Loss: 0.00001605
Iteration 273/1000 | Loss: 0.00001605
Iteration 274/1000 | Loss: 0.00001605
Iteration 275/1000 | Loss: 0.00003341
Iteration 276/1000 | Loss: 0.00024875
Iteration 277/1000 | Loss: 0.00023008
Iteration 278/1000 | Loss: 0.00003284
Iteration 279/1000 | Loss: 0.00005166
Iteration 280/1000 | Loss: 0.00002499
Iteration 281/1000 | Loss: 0.00001636
Iteration 282/1000 | Loss: 0.00001613
Iteration 283/1000 | Loss: 0.00001641
Iteration 284/1000 | Loss: 0.00001607
Iteration 285/1000 | Loss: 0.00001606
Iteration 286/1000 | Loss: 0.00001606
Iteration 287/1000 | Loss: 0.00001606
Iteration 288/1000 | Loss: 0.00001606
Iteration 289/1000 | Loss: 0.00001606
Iteration 290/1000 | Loss: 0.00001606
Iteration 291/1000 | Loss: 0.00001606
Iteration 292/1000 | Loss: 0.00001606
Iteration 293/1000 | Loss: 0.00001605
Iteration 294/1000 | Loss: 0.00001603
Iteration 295/1000 | Loss: 0.00001602
Iteration 296/1000 | Loss: 0.00001600
Iteration 297/1000 | Loss: 0.00006082
Iteration 298/1000 | Loss: 0.00001609
Iteration 299/1000 | Loss: 0.00008711
Iteration 300/1000 | Loss: 0.00006167
Iteration 301/1000 | Loss: 0.00008751
Iteration 302/1000 | Loss: 0.00012707
Iteration 303/1000 | Loss: 0.00003578
Iteration 304/1000 | Loss: 0.00003586
Iteration 305/1000 | Loss: 0.00002020
Iteration 306/1000 | Loss: 0.00001609
Iteration 307/1000 | Loss: 0.00001606
Iteration 308/1000 | Loss: 0.00006043
Iteration 309/1000 | Loss: 0.00001606
Iteration 310/1000 | Loss: 0.00005902
Iteration 311/1000 | Loss: 0.00003695
Iteration 312/1000 | Loss: 0.00003256
Iteration 313/1000 | Loss: 0.00001609
Iteration 314/1000 | Loss: 0.00001602
Iteration 315/1000 | Loss: 0.00001601
Iteration 316/1000 | Loss: 0.00001600
Iteration 317/1000 | Loss: 0.00001849
Iteration 318/1000 | Loss: 0.00001597
Iteration 319/1000 | Loss: 0.00001597
Iteration 320/1000 | Loss: 0.00001597
Iteration 321/1000 | Loss: 0.00001597
Iteration 322/1000 | Loss: 0.00001597
Iteration 323/1000 | Loss: 0.00001597
Iteration 324/1000 | Loss: 0.00001597
Iteration 325/1000 | Loss: 0.00004577
Iteration 326/1000 | Loss: 0.00001599
Iteration 327/1000 | Loss: 0.00001598
Iteration 328/1000 | Loss: 0.00001598
Iteration 329/1000 | Loss: 0.00001597
Iteration 330/1000 | Loss: 0.00001596
Iteration 331/1000 | Loss: 0.00002086
Iteration 332/1000 | Loss: 0.00011396
Iteration 333/1000 | Loss: 0.00005606
Iteration 334/1000 | Loss: 0.00003052
Iteration 335/1000 | Loss: 0.00003387
Iteration 336/1000 | Loss: 0.00001608
Iteration 337/1000 | Loss: 0.00001876
Iteration 338/1000 | Loss: 0.00001769
Iteration 339/1000 | Loss: 0.00007485
Iteration 340/1000 | Loss: 0.00001605
Iteration 341/1000 | Loss: 0.00001595
Iteration 342/1000 | Loss: 0.00001595
Iteration 343/1000 | Loss: 0.00001594
Iteration 344/1000 | Loss: 0.00001594
Iteration 345/1000 | Loss: 0.00001593
Iteration 346/1000 | Loss: 0.00001593
Iteration 347/1000 | Loss: 0.00001593
Iteration 348/1000 | Loss: 0.00001593
Iteration 349/1000 | Loss: 0.00001593
Iteration 350/1000 | Loss: 0.00001593
Iteration 351/1000 | Loss: 0.00001593
Iteration 352/1000 | Loss: 0.00001592
Iteration 353/1000 | Loss: 0.00001592
Iteration 354/1000 | Loss: 0.00001592
Iteration 355/1000 | Loss: 0.00001592
Iteration 356/1000 | Loss: 0.00001592
Iteration 357/1000 | Loss: 0.00001592
Iteration 358/1000 | Loss: 0.00001592
Iteration 359/1000 | Loss: 0.00001592
Iteration 360/1000 | Loss: 0.00001592
Iteration 361/1000 | Loss: 0.00001592
Iteration 362/1000 | Loss: 0.00001592
Iteration 363/1000 | Loss: 0.00001592
Iteration 364/1000 | Loss: 0.00001591
Iteration 365/1000 | Loss: 0.00001591
Iteration 366/1000 | Loss: 0.00001591
Iteration 367/1000 | Loss: 0.00001591
Iteration 368/1000 | Loss: 0.00001591
Iteration 369/1000 | Loss: 0.00001591
Iteration 370/1000 | Loss: 0.00001591
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 370. Stopping optimization.
Last 5 losses: [1.591473483131267e-05, 1.591473483131267e-05, 1.591473483131267e-05, 1.591473483131267e-05, 1.591473483131267e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.591473483131267e-05

Optimization complete. Final v2v error: 2.8671634197235107 mm

Highest mean error: 18.565059661865234 mm for frame 192

Lowest mean error: 2.436335802078247 mm for frame 161

Saving results

Total time: 501.6185839176178
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01022204
Iteration 2/25 | Loss: 0.00204370
Iteration 3/25 | Loss: 0.00139520
Iteration 4/25 | Loss: 0.00131148
Iteration 5/25 | Loss: 0.00136460
Iteration 6/25 | Loss: 0.00131969
Iteration 7/25 | Loss: 0.00130077
Iteration 8/25 | Loss: 0.00124757
Iteration 9/25 | Loss: 0.00115751
Iteration 10/25 | Loss: 0.00113060
Iteration 11/25 | Loss: 0.00113366
Iteration 12/25 | Loss: 0.00112617
Iteration 13/25 | Loss: 0.00111920
Iteration 14/25 | Loss: 0.00111259
Iteration 15/25 | Loss: 0.00111913
Iteration 16/25 | Loss: 0.00111515
Iteration 17/25 | Loss: 0.00110687
Iteration 18/25 | Loss: 0.00110429
Iteration 19/25 | Loss: 0.00110441
Iteration 20/25 | Loss: 0.00110260
Iteration 21/25 | Loss: 0.00109852
Iteration 22/25 | Loss: 0.00109651
Iteration 23/25 | Loss: 0.00110025
Iteration 24/25 | Loss: 0.00109735
Iteration 25/25 | Loss: 0.00109691

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42978537
Iteration 2/25 | Loss: 0.00086378
Iteration 3/25 | Loss: 0.00086377
Iteration 4/25 | Loss: 0.00086377
Iteration 5/25 | Loss: 0.00086377
Iteration 6/25 | Loss: 0.00086377
Iteration 7/25 | Loss: 0.00086377
Iteration 8/25 | Loss: 0.00086377
Iteration 9/25 | Loss: 0.00086377
Iteration 10/25 | Loss: 0.00086377
Iteration 11/25 | Loss: 0.00086377
Iteration 12/25 | Loss: 0.00086377
Iteration 13/25 | Loss: 0.00086377
Iteration 14/25 | Loss: 0.00086377
Iteration 15/25 | Loss: 0.00086377
Iteration 16/25 | Loss: 0.00086377
Iteration 17/25 | Loss: 0.00086377
Iteration 18/25 | Loss: 0.00086377
Iteration 19/25 | Loss: 0.00086377
Iteration 20/25 | Loss: 0.00086377
Iteration 21/25 | Loss: 0.00086377
Iteration 22/25 | Loss: 0.00086377
Iteration 23/25 | Loss: 0.00086377
Iteration 24/25 | Loss: 0.00086377
Iteration 25/25 | Loss: 0.00086377

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086377
Iteration 2/1000 | Loss: 0.00003100
Iteration 3/1000 | Loss: 0.00004242
Iteration 4/1000 | Loss: 0.00002285
Iteration 5/1000 | Loss: 0.00002093
Iteration 6/1000 | Loss: 0.00001725
Iteration 7/1000 | Loss: 0.00003105
Iteration 8/1000 | Loss: 0.00003133
Iteration 9/1000 | Loss: 0.00002770
Iteration 10/1000 | Loss: 0.00008412
Iteration 11/1000 | Loss: 0.00012089
Iteration 12/1000 | Loss: 0.00004580
Iteration 13/1000 | Loss: 0.00007219
Iteration 14/1000 | Loss: 0.00002140
Iteration 15/1000 | Loss: 0.00006536
Iteration 16/1000 | Loss: 0.00006974
Iteration 17/1000 | Loss: 0.00003712
Iteration 18/1000 | Loss: 0.00003281
Iteration 19/1000 | Loss: 0.00004286
Iteration 20/1000 | Loss: 0.00003340
Iteration 21/1000 | Loss: 0.00003333
Iteration 22/1000 | Loss: 0.00004420
Iteration 23/1000 | Loss: 0.00003662
Iteration 24/1000 | Loss: 0.00003479
Iteration 25/1000 | Loss: 0.00003283
Iteration 26/1000 | Loss: 0.00004696
Iteration 27/1000 | Loss: 0.00001869
Iteration 28/1000 | Loss: 0.00006306
Iteration 29/1000 | Loss: 0.00002352
Iteration 30/1000 | Loss: 0.00001310
Iteration 31/1000 | Loss: 0.00001434
Iteration 32/1000 | Loss: 0.00001323
Iteration 33/1000 | Loss: 0.00001186
Iteration 34/1000 | Loss: 0.00001171
Iteration 35/1000 | Loss: 0.00001158
Iteration 36/1000 | Loss: 0.00001157
Iteration 37/1000 | Loss: 0.00006997
Iteration 38/1000 | Loss: 0.00001177
Iteration 39/1000 | Loss: 0.00001146
Iteration 40/1000 | Loss: 0.00001139
Iteration 41/1000 | Loss: 0.00001138
Iteration 42/1000 | Loss: 0.00001137
Iteration 43/1000 | Loss: 0.00001136
Iteration 44/1000 | Loss: 0.00001136
Iteration 45/1000 | Loss: 0.00001136
Iteration 46/1000 | Loss: 0.00001136
Iteration 47/1000 | Loss: 0.00001135
Iteration 48/1000 | Loss: 0.00001135
Iteration 49/1000 | Loss: 0.00010896
Iteration 50/1000 | Loss: 0.00005613
Iteration 51/1000 | Loss: 0.00002286
Iteration 52/1000 | Loss: 0.00001309
Iteration 53/1000 | Loss: 0.00001123
Iteration 54/1000 | Loss: 0.00001121
Iteration 55/1000 | Loss: 0.00001120
Iteration 56/1000 | Loss: 0.00001120
Iteration 57/1000 | Loss: 0.00001120
Iteration 58/1000 | Loss: 0.00001120
Iteration 59/1000 | Loss: 0.00001120
Iteration 60/1000 | Loss: 0.00001120
Iteration 61/1000 | Loss: 0.00001120
Iteration 62/1000 | Loss: 0.00001120
Iteration 63/1000 | Loss: 0.00001120
Iteration 64/1000 | Loss: 0.00001120
Iteration 65/1000 | Loss: 0.00001120
Iteration 66/1000 | Loss: 0.00001119
Iteration 67/1000 | Loss: 0.00001116
Iteration 68/1000 | Loss: 0.00001116
Iteration 69/1000 | Loss: 0.00001116
Iteration 70/1000 | Loss: 0.00001116
Iteration 71/1000 | Loss: 0.00001116
Iteration 72/1000 | Loss: 0.00001116
Iteration 73/1000 | Loss: 0.00001116
Iteration 74/1000 | Loss: 0.00001115
Iteration 75/1000 | Loss: 0.00001115
Iteration 76/1000 | Loss: 0.00001115
Iteration 77/1000 | Loss: 0.00001114
Iteration 78/1000 | Loss: 0.00001114
Iteration 79/1000 | Loss: 0.00001113
Iteration 80/1000 | Loss: 0.00001113
Iteration 81/1000 | Loss: 0.00001113
Iteration 82/1000 | Loss: 0.00001113
Iteration 83/1000 | Loss: 0.00001113
Iteration 84/1000 | Loss: 0.00001113
Iteration 85/1000 | Loss: 0.00001113
Iteration 86/1000 | Loss: 0.00001113
Iteration 87/1000 | Loss: 0.00001112
Iteration 88/1000 | Loss: 0.00001112
Iteration 89/1000 | Loss: 0.00001112
Iteration 90/1000 | Loss: 0.00001111
Iteration 91/1000 | Loss: 0.00001111
Iteration 92/1000 | Loss: 0.00001110
Iteration 93/1000 | Loss: 0.00001109
Iteration 94/1000 | Loss: 0.00001109
Iteration 95/1000 | Loss: 0.00001108
Iteration 96/1000 | Loss: 0.00001108
Iteration 97/1000 | Loss: 0.00001108
Iteration 98/1000 | Loss: 0.00001108
Iteration 99/1000 | Loss: 0.00001107
Iteration 100/1000 | Loss: 0.00001107
Iteration 101/1000 | Loss: 0.00001107
Iteration 102/1000 | Loss: 0.00001107
Iteration 103/1000 | Loss: 0.00009928
Iteration 104/1000 | Loss: 0.00001819
Iteration 105/1000 | Loss: 0.00006206
Iteration 106/1000 | Loss: 0.00001765
Iteration 107/1000 | Loss: 0.00001112
Iteration 108/1000 | Loss: 0.00001111
Iteration 109/1000 | Loss: 0.00001107
Iteration 110/1000 | Loss: 0.00001106
Iteration 111/1000 | Loss: 0.00001105
Iteration 112/1000 | Loss: 0.00001105
Iteration 113/1000 | Loss: 0.00001105
Iteration 114/1000 | Loss: 0.00001105
Iteration 115/1000 | Loss: 0.00001104
Iteration 116/1000 | Loss: 0.00001104
Iteration 117/1000 | Loss: 0.00001104
Iteration 118/1000 | Loss: 0.00001104
Iteration 119/1000 | Loss: 0.00001104
Iteration 120/1000 | Loss: 0.00001103
Iteration 121/1000 | Loss: 0.00001103
Iteration 122/1000 | Loss: 0.00001103
Iteration 123/1000 | Loss: 0.00001102
Iteration 124/1000 | Loss: 0.00001102
Iteration 125/1000 | Loss: 0.00001102
Iteration 126/1000 | Loss: 0.00001102
Iteration 127/1000 | Loss: 0.00001102
Iteration 128/1000 | Loss: 0.00001102
Iteration 129/1000 | Loss: 0.00001102
Iteration 130/1000 | Loss: 0.00001102
Iteration 131/1000 | Loss: 0.00001102
Iteration 132/1000 | Loss: 0.00001102
Iteration 133/1000 | Loss: 0.00001102
Iteration 134/1000 | Loss: 0.00001102
Iteration 135/1000 | Loss: 0.00001102
Iteration 136/1000 | Loss: 0.00001102
Iteration 137/1000 | Loss: 0.00001102
Iteration 138/1000 | Loss: 0.00001102
Iteration 139/1000 | Loss: 0.00001102
Iteration 140/1000 | Loss: 0.00001102
Iteration 141/1000 | Loss: 0.00001102
Iteration 142/1000 | Loss: 0.00001102
Iteration 143/1000 | Loss: 0.00001102
Iteration 144/1000 | Loss: 0.00001102
Iteration 145/1000 | Loss: 0.00001102
Iteration 146/1000 | Loss: 0.00001102
Iteration 147/1000 | Loss: 0.00001102
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.1016819371434394e-05, 1.1016819371434394e-05, 1.1016819371434394e-05, 1.1016819371434394e-05, 1.1016819371434394e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1016819371434394e-05

Optimization complete. Final v2v error: 2.7725322246551514 mm

Highest mean error: 3.996577501296997 mm for frame 64

Lowest mean error: 2.3936970233917236 mm for frame 0

Saving results

Total time: 121.15817093849182
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00851142
Iteration 2/25 | Loss: 0.00139817
Iteration 3/25 | Loss: 0.00115773
Iteration 4/25 | Loss: 0.00112756
Iteration 5/25 | Loss: 0.00112422
Iteration 6/25 | Loss: 0.00112422
Iteration 7/25 | Loss: 0.00112422
Iteration 8/25 | Loss: 0.00112422
Iteration 9/25 | Loss: 0.00112422
Iteration 10/25 | Loss: 0.00112422
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011242175241932273, 0.0011242175241932273, 0.0011242175241932273, 0.0011242175241932273, 0.0011242175241932273]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011242175241932273

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.93095821
Iteration 2/25 | Loss: 0.00041146
Iteration 3/25 | Loss: 0.00041145
Iteration 4/25 | Loss: 0.00041145
Iteration 5/25 | Loss: 0.00041145
Iteration 6/25 | Loss: 0.00041145
Iteration 7/25 | Loss: 0.00041145
Iteration 8/25 | Loss: 0.00041145
Iteration 9/25 | Loss: 0.00041145
Iteration 10/25 | Loss: 0.00041145
Iteration 11/25 | Loss: 0.00041145
Iteration 12/25 | Loss: 0.00041145
Iteration 13/25 | Loss: 0.00041145
Iteration 14/25 | Loss: 0.00041145
Iteration 15/25 | Loss: 0.00041145
Iteration 16/25 | Loss: 0.00041145
Iteration 17/25 | Loss: 0.00041145
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00041145135764963925, 0.00041145135764963925, 0.00041145135764963925, 0.00041145135764963925, 0.00041145135764963925]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00041145135764963925

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041145
Iteration 2/1000 | Loss: 0.00003036
Iteration 3/1000 | Loss: 0.00002361
Iteration 4/1000 | Loss: 0.00002206
Iteration 5/1000 | Loss: 0.00002078
Iteration 6/1000 | Loss: 0.00002025
Iteration 7/1000 | Loss: 0.00001978
Iteration 8/1000 | Loss: 0.00001942
Iteration 9/1000 | Loss: 0.00001898
Iteration 10/1000 | Loss: 0.00001872
Iteration 11/1000 | Loss: 0.00001850
Iteration 12/1000 | Loss: 0.00001845
Iteration 13/1000 | Loss: 0.00001833
Iteration 14/1000 | Loss: 0.00001833
Iteration 15/1000 | Loss: 0.00001833
Iteration 16/1000 | Loss: 0.00001832
Iteration 17/1000 | Loss: 0.00001830
Iteration 18/1000 | Loss: 0.00001830
Iteration 19/1000 | Loss: 0.00001829
Iteration 20/1000 | Loss: 0.00001829
Iteration 21/1000 | Loss: 0.00001827
Iteration 22/1000 | Loss: 0.00001826
Iteration 23/1000 | Loss: 0.00001824
Iteration 24/1000 | Loss: 0.00001821
Iteration 25/1000 | Loss: 0.00001819
Iteration 26/1000 | Loss: 0.00001819
Iteration 27/1000 | Loss: 0.00001818
Iteration 28/1000 | Loss: 0.00001818
Iteration 29/1000 | Loss: 0.00001818
Iteration 30/1000 | Loss: 0.00001818
Iteration 31/1000 | Loss: 0.00001817
Iteration 32/1000 | Loss: 0.00001817
Iteration 33/1000 | Loss: 0.00001815
Iteration 34/1000 | Loss: 0.00001815
Iteration 35/1000 | Loss: 0.00001814
Iteration 36/1000 | Loss: 0.00001814
Iteration 37/1000 | Loss: 0.00001814
Iteration 38/1000 | Loss: 0.00001813
Iteration 39/1000 | Loss: 0.00001811
Iteration 40/1000 | Loss: 0.00001811
Iteration 41/1000 | Loss: 0.00001811
Iteration 42/1000 | Loss: 0.00001811
Iteration 43/1000 | Loss: 0.00001811
Iteration 44/1000 | Loss: 0.00001811
Iteration 45/1000 | Loss: 0.00001811
Iteration 46/1000 | Loss: 0.00001810
Iteration 47/1000 | Loss: 0.00001810
Iteration 48/1000 | Loss: 0.00001810
Iteration 49/1000 | Loss: 0.00001810
Iteration 50/1000 | Loss: 0.00001810
Iteration 51/1000 | Loss: 0.00001810
Iteration 52/1000 | Loss: 0.00001810
Iteration 53/1000 | Loss: 0.00001810
Iteration 54/1000 | Loss: 0.00001809
Iteration 55/1000 | Loss: 0.00001809
Iteration 56/1000 | Loss: 0.00001809
Iteration 57/1000 | Loss: 0.00001809
Iteration 58/1000 | Loss: 0.00001809
Iteration 59/1000 | Loss: 0.00001809
Iteration 60/1000 | Loss: 0.00001809
Iteration 61/1000 | Loss: 0.00001809
Iteration 62/1000 | Loss: 0.00001809
Iteration 63/1000 | Loss: 0.00001809
Iteration 64/1000 | Loss: 0.00001809
Iteration 65/1000 | Loss: 0.00001809
Iteration 66/1000 | Loss: 0.00001809
Iteration 67/1000 | Loss: 0.00001809
Iteration 68/1000 | Loss: 0.00001809
Iteration 69/1000 | Loss: 0.00001809
Iteration 70/1000 | Loss: 0.00001808
Iteration 71/1000 | Loss: 0.00001808
Iteration 72/1000 | Loss: 0.00001808
Iteration 73/1000 | Loss: 0.00001808
Iteration 74/1000 | Loss: 0.00001808
Iteration 75/1000 | Loss: 0.00001808
Iteration 76/1000 | Loss: 0.00001808
Iteration 77/1000 | Loss: 0.00001808
Iteration 78/1000 | Loss: 0.00001808
Iteration 79/1000 | Loss: 0.00001807
Iteration 80/1000 | Loss: 0.00001807
Iteration 81/1000 | Loss: 0.00001807
Iteration 82/1000 | Loss: 0.00001807
Iteration 83/1000 | Loss: 0.00001807
Iteration 84/1000 | Loss: 0.00001807
Iteration 85/1000 | Loss: 0.00001807
Iteration 86/1000 | Loss: 0.00001806
Iteration 87/1000 | Loss: 0.00001806
Iteration 88/1000 | Loss: 0.00001806
Iteration 89/1000 | Loss: 0.00001806
Iteration 90/1000 | Loss: 0.00001806
Iteration 91/1000 | Loss: 0.00001806
Iteration 92/1000 | Loss: 0.00001806
Iteration 93/1000 | Loss: 0.00001805
Iteration 94/1000 | Loss: 0.00001805
Iteration 95/1000 | Loss: 0.00001805
Iteration 96/1000 | Loss: 0.00001805
Iteration 97/1000 | Loss: 0.00001805
Iteration 98/1000 | Loss: 0.00001805
Iteration 99/1000 | Loss: 0.00001805
Iteration 100/1000 | Loss: 0.00001805
Iteration 101/1000 | Loss: 0.00001805
Iteration 102/1000 | Loss: 0.00001805
Iteration 103/1000 | Loss: 0.00001805
Iteration 104/1000 | Loss: 0.00001804
Iteration 105/1000 | Loss: 0.00001804
Iteration 106/1000 | Loss: 0.00001804
Iteration 107/1000 | Loss: 0.00001804
Iteration 108/1000 | Loss: 0.00001804
Iteration 109/1000 | Loss: 0.00001804
Iteration 110/1000 | Loss: 0.00001804
Iteration 111/1000 | Loss: 0.00001804
Iteration 112/1000 | Loss: 0.00001804
Iteration 113/1000 | Loss: 0.00001804
Iteration 114/1000 | Loss: 0.00001803
Iteration 115/1000 | Loss: 0.00001803
Iteration 116/1000 | Loss: 0.00001803
Iteration 117/1000 | Loss: 0.00001803
Iteration 118/1000 | Loss: 0.00001803
Iteration 119/1000 | Loss: 0.00001803
Iteration 120/1000 | Loss: 0.00001803
Iteration 121/1000 | Loss: 0.00001803
Iteration 122/1000 | Loss: 0.00001803
Iteration 123/1000 | Loss: 0.00001802
Iteration 124/1000 | Loss: 0.00001802
Iteration 125/1000 | Loss: 0.00001802
Iteration 126/1000 | Loss: 0.00001802
Iteration 127/1000 | Loss: 0.00001802
Iteration 128/1000 | Loss: 0.00001802
Iteration 129/1000 | Loss: 0.00001802
Iteration 130/1000 | Loss: 0.00001802
Iteration 131/1000 | Loss: 0.00001802
Iteration 132/1000 | Loss: 0.00001802
Iteration 133/1000 | Loss: 0.00001802
Iteration 134/1000 | Loss: 0.00001801
Iteration 135/1000 | Loss: 0.00001801
Iteration 136/1000 | Loss: 0.00001801
Iteration 137/1000 | Loss: 0.00001801
Iteration 138/1000 | Loss: 0.00001801
Iteration 139/1000 | Loss: 0.00001801
Iteration 140/1000 | Loss: 0.00001801
Iteration 141/1000 | Loss: 0.00001801
Iteration 142/1000 | Loss: 0.00001801
Iteration 143/1000 | Loss: 0.00001801
Iteration 144/1000 | Loss: 0.00001801
Iteration 145/1000 | Loss: 0.00001801
Iteration 146/1000 | Loss: 0.00001800
Iteration 147/1000 | Loss: 0.00001800
Iteration 148/1000 | Loss: 0.00001800
Iteration 149/1000 | Loss: 0.00001800
Iteration 150/1000 | Loss: 0.00001800
Iteration 151/1000 | Loss: 0.00001800
Iteration 152/1000 | Loss: 0.00001800
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.8004197045229375e-05, 1.8004197045229375e-05, 1.8004197045229375e-05, 1.8004197045229375e-05, 1.8004197045229375e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8004197045229375e-05

Optimization complete. Final v2v error: 3.5476326942443848 mm

Highest mean error: 3.675414800643921 mm for frame 127

Lowest mean error: 3.4618518352508545 mm for frame 11

Saving results

Total time: 39.95481896400452
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01042718
Iteration 2/25 | Loss: 0.00129931
Iteration 3/25 | Loss: 0.00110665
Iteration 4/25 | Loss: 0.00107758
Iteration 5/25 | Loss: 0.00107391
Iteration 6/25 | Loss: 0.00107346
Iteration 7/25 | Loss: 0.00107346
Iteration 8/25 | Loss: 0.00107346
Iteration 9/25 | Loss: 0.00107346
Iteration 10/25 | Loss: 0.00107346
Iteration 11/25 | Loss: 0.00107346
Iteration 12/25 | Loss: 0.00107346
Iteration 13/25 | Loss: 0.00107346
Iteration 14/25 | Loss: 0.00107346
Iteration 15/25 | Loss: 0.00107346
Iteration 16/25 | Loss: 0.00107346
Iteration 17/25 | Loss: 0.00107346
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010734593961387873, 0.0010734593961387873, 0.0010734593961387873, 0.0010734593961387873, 0.0010734593961387873]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010734593961387873

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.58473301
Iteration 2/25 | Loss: 0.00069449
Iteration 3/25 | Loss: 0.00069449
Iteration 4/25 | Loss: 0.00069449
Iteration 5/25 | Loss: 0.00069449
Iteration 6/25 | Loss: 0.00069449
Iteration 7/25 | Loss: 0.00069449
Iteration 8/25 | Loss: 0.00069449
Iteration 9/25 | Loss: 0.00069449
Iteration 10/25 | Loss: 0.00069449
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0006944871856831014, 0.0006944871856831014, 0.0006944871856831014, 0.0006944871856831014, 0.0006944871856831014]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006944871856831014

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069449
Iteration 2/1000 | Loss: 0.00002169
Iteration 3/1000 | Loss: 0.00001540
Iteration 4/1000 | Loss: 0.00001395
Iteration 5/1000 | Loss: 0.00001303
Iteration 6/1000 | Loss: 0.00001235
Iteration 7/1000 | Loss: 0.00001199
Iteration 8/1000 | Loss: 0.00001172
Iteration 9/1000 | Loss: 0.00001138
Iteration 10/1000 | Loss: 0.00001120
Iteration 11/1000 | Loss: 0.00001102
Iteration 12/1000 | Loss: 0.00001094
Iteration 13/1000 | Loss: 0.00001086
Iteration 14/1000 | Loss: 0.00001086
Iteration 15/1000 | Loss: 0.00001084
Iteration 16/1000 | Loss: 0.00001084
Iteration 17/1000 | Loss: 0.00001084
Iteration 18/1000 | Loss: 0.00001083
Iteration 19/1000 | Loss: 0.00001082
Iteration 20/1000 | Loss: 0.00001082
Iteration 21/1000 | Loss: 0.00001081
Iteration 22/1000 | Loss: 0.00001080
Iteration 23/1000 | Loss: 0.00001080
Iteration 24/1000 | Loss: 0.00001080
Iteration 25/1000 | Loss: 0.00001080
Iteration 26/1000 | Loss: 0.00001079
Iteration 27/1000 | Loss: 0.00001079
Iteration 28/1000 | Loss: 0.00001078
Iteration 29/1000 | Loss: 0.00001078
Iteration 30/1000 | Loss: 0.00001078
Iteration 31/1000 | Loss: 0.00001078
Iteration 32/1000 | Loss: 0.00001077
Iteration 33/1000 | Loss: 0.00001077
Iteration 34/1000 | Loss: 0.00001077
Iteration 35/1000 | Loss: 0.00001077
Iteration 36/1000 | Loss: 0.00001077
Iteration 37/1000 | Loss: 0.00001076
Iteration 38/1000 | Loss: 0.00001076
Iteration 39/1000 | Loss: 0.00001076
Iteration 40/1000 | Loss: 0.00001075
Iteration 41/1000 | Loss: 0.00001075
Iteration 42/1000 | Loss: 0.00001075
Iteration 43/1000 | Loss: 0.00001074
Iteration 44/1000 | Loss: 0.00001074
Iteration 45/1000 | Loss: 0.00001073
Iteration 46/1000 | Loss: 0.00001073
Iteration 47/1000 | Loss: 0.00001073
Iteration 48/1000 | Loss: 0.00001072
Iteration 49/1000 | Loss: 0.00001072
Iteration 50/1000 | Loss: 0.00001072
Iteration 51/1000 | Loss: 0.00001072
Iteration 52/1000 | Loss: 0.00001072
Iteration 53/1000 | Loss: 0.00001072
Iteration 54/1000 | Loss: 0.00001071
Iteration 55/1000 | Loss: 0.00001071
Iteration 56/1000 | Loss: 0.00001071
Iteration 57/1000 | Loss: 0.00001070
Iteration 58/1000 | Loss: 0.00001070
Iteration 59/1000 | Loss: 0.00001070
Iteration 60/1000 | Loss: 0.00001070
Iteration 61/1000 | Loss: 0.00001070
Iteration 62/1000 | Loss: 0.00001070
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 62. Stopping optimization.
Last 5 losses: [1.0701764949772041e-05, 1.0701764949772041e-05, 1.0701764949772041e-05, 1.0701764949772041e-05, 1.0701764949772041e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0701764949772041e-05

Optimization complete. Final v2v error: 2.7872650623321533 mm

Highest mean error: 3.0667049884796143 mm for frame 43

Lowest mean error: 2.500105619430542 mm for frame 203

Saving results

Total time: 34.6566903591156
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00462950
Iteration 2/25 | Loss: 0.00138240
Iteration 3/25 | Loss: 0.00118730
Iteration 4/25 | Loss: 0.00117269
Iteration 5/25 | Loss: 0.00116868
Iteration 6/25 | Loss: 0.00116848
Iteration 7/25 | Loss: 0.00116848
Iteration 8/25 | Loss: 0.00116848
Iteration 9/25 | Loss: 0.00116848
Iteration 10/25 | Loss: 0.00116848
Iteration 11/25 | Loss: 0.00116848
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001168475253507495, 0.001168475253507495, 0.001168475253507495, 0.001168475253507495, 0.001168475253507495]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001168475253507495

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35836315
Iteration 2/25 | Loss: 0.00078455
Iteration 3/25 | Loss: 0.00078455
Iteration 4/25 | Loss: 0.00078455
Iteration 5/25 | Loss: 0.00078455
Iteration 6/25 | Loss: 0.00078455
Iteration 7/25 | Loss: 0.00078455
Iteration 8/25 | Loss: 0.00078455
Iteration 9/25 | Loss: 0.00078455
Iteration 10/25 | Loss: 0.00078455
Iteration 11/25 | Loss: 0.00078455
Iteration 12/25 | Loss: 0.00078455
Iteration 13/25 | Loss: 0.00078455
Iteration 14/25 | Loss: 0.00078455
Iteration 15/25 | Loss: 0.00078455
Iteration 16/25 | Loss: 0.00078455
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007845463114790618, 0.0007845463114790618, 0.0007845463114790618, 0.0007845463114790618, 0.0007845463114790618]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007845463114790618

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078455
Iteration 2/1000 | Loss: 0.00004557
Iteration 3/1000 | Loss: 0.00003447
Iteration 4/1000 | Loss: 0.00003198
Iteration 5/1000 | Loss: 0.00003020
Iteration 6/1000 | Loss: 0.00002902
Iteration 7/1000 | Loss: 0.00002811
Iteration 8/1000 | Loss: 0.00002753
Iteration 9/1000 | Loss: 0.00002713
Iteration 10/1000 | Loss: 0.00002690
Iteration 11/1000 | Loss: 0.00002677
Iteration 12/1000 | Loss: 0.00002675
Iteration 13/1000 | Loss: 0.00002664
Iteration 14/1000 | Loss: 0.00002658
Iteration 15/1000 | Loss: 0.00002642
Iteration 16/1000 | Loss: 0.00002628
Iteration 17/1000 | Loss: 0.00002625
Iteration 18/1000 | Loss: 0.00002616
Iteration 19/1000 | Loss: 0.00002613
Iteration 20/1000 | Loss: 0.00002609
Iteration 21/1000 | Loss: 0.00002608
Iteration 22/1000 | Loss: 0.00002604
Iteration 23/1000 | Loss: 0.00002601
Iteration 24/1000 | Loss: 0.00002600
Iteration 25/1000 | Loss: 0.00002600
Iteration 26/1000 | Loss: 0.00002599
Iteration 27/1000 | Loss: 0.00002597
Iteration 28/1000 | Loss: 0.00002597
Iteration 29/1000 | Loss: 0.00002597
Iteration 30/1000 | Loss: 0.00002597
Iteration 31/1000 | Loss: 0.00002597
Iteration 32/1000 | Loss: 0.00002596
Iteration 33/1000 | Loss: 0.00002596
Iteration 34/1000 | Loss: 0.00002596
Iteration 35/1000 | Loss: 0.00002596
Iteration 36/1000 | Loss: 0.00002596
Iteration 37/1000 | Loss: 0.00002595
Iteration 38/1000 | Loss: 0.00002594
Iteration 39/1000 | Loss: 0.00002593
Iteration 40/1000 | Loss: 0.00002593
Iteration 41/1000 | Loss: 0.00002593
Iteration 42/1000 | Loss: 0.00002593
Iteration 43/1000 | Loss: 0.00002593
Iteration 44/1000 | Loss: 0.00002593
Iteration 45/1000 | Loss: 0.00002593
Iteration 46/1000 | Loss: 0.00002593
Iteration 47/1000 | Loss: 0.00002592
Iteration 48/1000 | Loss: 0.00002592
Iteration 49/1000 | Loss: 0.00002592
Iteration 50/1000 | Loss: 0.00002592
Iteration 51/1000 | Loss: 0.00002592
Iteration 52/1000 | Loss: 0.00002592
Iteration 53/1000 | Loss: 0.00002592
Iteration 54/1000 | Loss: 0.00002592
Iteration 55/1000 | Loss: 0.00002591
Iteration 56/1000 | Loss: 0.00002591
Iteration 57/1000 | Loss: 0.00002591
Iteration 58/1000 | Loss: 0.00002591
Iteration 59/1000 | Loss: 0.00002590
Iteration 60/1000 | Loss: 0.00002590
Iteration 61/1000 | Loss: 0.00002590
Iteration 62/1000 | Loss: 0.00002590
Iteration 63/1000 | Loss: 0.00002590
Iteration 64/1000 | Loss: 0.00002589
Iteration 65/1000 | Loss: 0.00002589
Iteration 66/1000 | Loss: 0.00002588
Iteration 67/1000 | Loss: 0.00002588
Iteration 68/1000 | Loss: 0.00002588
Iteration 69/1000 | Loss: 0.00002588
Iteration 70/1000 | Loss: 0.00002587
Iteration 71/1000 | Loss: 0.00002587
Iteration 72/1000 | Loss: 0.00002587
Iteration 73/1000 | Loss: 0.00002586
Iteration 74/1000 | Loss: 0.00002586
Iteration 75/1000 | Loss: 0.00002586
Iteration 76/1000 | Loss: 0.00002586
Iteration 77/1000 | Loss: 0.00002586
Iteration 78/1000 | Loss: 0.00002585
Iteration 79/1000 | Loss: 0.00002585
Iteration 80/1000 | Loss: 0.00002585
Iteration 81/1000 | Loss: 0.00002585
Iteration 82/1000 | Loss: 0.00002585
Iteration 83/1000 | Loss: 0.00002585
Iteration 84/1000 | Loss: 0.00002585
Iteration 85/1000 | Loss: 0.00002585
Iteration 86/1000 | Loss: 0.00002584
Iteration 87/1000 | Loss: 0.00002584
Iteration 88/1000 | Loss: 0.00002584
Iteration 89/1000 | Loss: 0.00002584
Iteration 90/1000 | Loss: 0.00002584
Iteration 91/1000 | Loss: 0.00002584
Iteration 92/1000 | Loss: 0.00002584
Iteration 93/1000 | Loss: 0.00002584
Iteration 94/1000 | Loss: 0.00002584
Iteration 95/1000 | Loss: 0.00002584
Iteration 96/1000 | Loss: 0.00002584
Iteration 97/1000 | Loss: 0.00002584
Iteration 98/1000 | Loss: 0.00002583
Iteration 99/1000 | Loss: 0.00002583
Iteration 100/1000 | Loss: 0.00002583
Iteration 101/1000 | Loss: 0.00002583
Iteration 102/1000 | Loss: 0.00002583
Iteration 103/1000 | Loss: 0.00002583
Iteration 104/1000 | Loss: 0.00002582
Iteration 105/1000 | Loss: 0.00002582
Iteration 106/1000 | Loss: 0.00002582
Iteration 107/1000 | Loss: 0.00002582
Iteration 108/1000 | Loss: 0.00002582
Iteration 109/1000 | Loss: 0.00002582
Iteration 110/1000 | Loss: 0.00002582
Iteration 111/1000 | Loss: 0.00002581
Iteration 112/1000 | Loss: 0.00002581
Iteration 113/1000 | Loss: 0.00002581
Iteration 114/1000 | Loss: 0.00002581
Iteration 115/1000 | Loss: 0.00002581
Iteration 116/1000 | Loss: 0.00002581
Iteration 117/1000 | Loss: 0.00002580
Iteration 118/1000 | Loss: 0.00002580
Iteration 119/1000 | Loss: 0.00002580
Iteration 120/1000 | Loss: 0.00002580
Iteration 121/1000 | Loss: 0.00002580
Iteration 122/1000 | Loss: 0.00002580
Iteration 123/1000 | Loss: 0.00002580
Iteration 124/1000 | Loss: 0.00002580
Iteration 125/1000 | Loss: 0.00002580
Iteration 126/1000 | Loss: 0.00002580
Iteration 127/1000 | Loss: 0.00002580
Iteration 128/1000 | Loss: 0.00002580
Iteration 129/1000 | Loss: 0.00002580
Iteration 130/1000 | Loss: 0.00002580
Iteration 131/1000 | Loss: 0.00002579
Iteration 132/1000 | Loss: 0.00002579
Iteration 133/1000 | Loss: 0.00002579
Iteration 134/1000 | Loss: 0.00002579
Iteration 135/1000 | Loss: 0.00002579
Iteration 136/1000 | Loss: 0.00002579
Iteration 137/1000 | Loss: 0.00002579
Iteration 138/1000 | Loss: 0.00002579
Iteration 139/1000 | Loss: 0.00002579
Iteration 140/1000 | Loss: 0.00002579
Iteration 141/1000 | Loss: 0.00002579
Iteration 142/1000 | Loss: 0.00002579
Iteration 143/1000 | Loss: 0.00002579
Iteration 144/1000 | Loss: 0.00002579
Iteration 145/1000 | Loss: 0.00002579
Iteration 146/1000 | Loss: 0.00002579
Iteration 147/1000 | Loss: 0.00002579
Iteration 148/1000 | Loss: 0.00002579
Iteration 149/1000 | Loss: 0.00002579
Iteration 150/1000 | Loss: 0.00002579
Iteration 151/1000 | Loss: 0.00002579
Iteration 152/1000 | Loss: 0.00002579
Iteration 153/1000 | Loss: 0.00002579
Iteration 154/1000 | Loss: 0.00002579
Iteration 155/1000 | Loss: 0.00002579
Iteration 156/1000 | Loss: 0.00002579
Iteration 157/1000 | Loss: 0.00002579
Iteration 158/1000 | Loss: 0.00002579
Iteration 159/1000 | Loss: 0.00002579
Iteration 160/1000 | Loss: 0.00002579
Iteration 161/1000 | Loss: 0.00002579
Iteration 162/1000 | Loss: 0.00002579
Iteration 163/1000 | Loss: 0.00002579
Iteration 164/1000 | Loss: 0.00002579
Iteration 165/1000 | Loss: 0.00002579
Iteration 166/1000 | Loss: 0.00002579
Iteration 167/1000 | Loss: 0.00002579
Iteration 168/1000 | Loss: 0.00002579
Iteration 169/1000 | Loss: 0.00002579
Iteration 170/1000 | Loss: 0.00002579
Iteration 171/1000 | Loss: 0.00002579
Iteration 172/1000 | Loss: 0.00002579
Iteration 173/1000 | Loss: 0.00002579
Iteration 174/1000 | Loss: 0.00002579
Iteration 175/1000 | Loss: 0.00002579
Iteration 176/1000 | Loss: 0.00002579
Iteration 177/1000 | Loss: 0.00002579
Iteration 178/1000 | Loss: 0.00002579
Iteration 179/1000 | Loss: 0.00002579
Iteration 180/1000 | Loss: 0.00002579
Iteration 181/1000 | Loss: 0.00002579
Iteration 182/1000 | Loss: 0.00002579
Iteration 183/1000 | Loss: 0.00002579
Iteration 184/1000 | Loss: 0.00002579
Iteration 185/1000 | Loss: 0.00002579
Iteration 186/1000 | Loss: 0.00002579
Iteration 187/1000 | Loss: 0.00002579
Iteration 188/1000 | Loss: 0.00002579
Iteration 189/1000 | Loss: 0.00002579
Iteration 190/1000 | Loss: 0.00002579
Iteration 191/1000 | Loss: 0.00002579
Iteration 192/1000 | Loss: 0.00002579
Iteration 193/1000 | Loss: 0.00002579
Iteration 194/1000 | Loss: 0.00002579
Iteration 195/1000 | Loss: 0.00002579
Iteration 196/1000 | Loss: 0.00002579
Iteration 197/1000 | Loss: 0.00002579
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [2.579025567683857e-05, 2.579025567683857e-05, 2.579025567683857e-05, 2.579025567683857e-05, 2.579025567683857e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.579025567683857e-05

Optimization complete. Final v2v error: 4.058974742889404 mm

Highest mean error: 4.489213943481445 mm for frame 158

Lowest mean error: 3.3281490802764893 mm for frame 120

Saving results

Total time: 48.51558542251587
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00355997
Iteration 2/25 | Loss: 0.00111621
Iteration 3/25 | Loss: 0.00104432
Iteration 4/25 | Loss: 0.00103354
Iteration 5/25 | Loss: 0.00102967
Iteration 6/25 | Loss: 0.00102944
Iteration 7/25 | Loss: 0.00102944
Iteration 8/25 | Loss: 0.00102944
Iteration 9/25 | Loss: 0.00102944
Iteration 10/25 | Loss: 0.00102944
Iteration 11/25 | Loss: 0.00102944
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010294400854036212, 0.0010294400854036212, 0.0010294400854036212, 0.0010294400854036212, 0.0010294400854036212]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010294400854036212

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60505116
Iteration 2/25 | Loss: 0.00075127
Iteration 3/25 | Loss: 0.00075127
Iteration 4/25 | Loss: 0.00075127
Iteration 5/25 | Loss: 0.00075127
Iteration 6/25 | Loss: 0.00075127
Iteration 7/25 | Loss: 0.00075127
Iteration 8/25 | Loss: 0.00075127
Iteration 9/25 | Loss: 0.00075127
Iteration 10/25 | Loss: 0.00075127
Iteration 11/25 | Loss: 0.00075127
Iteration 12/25 | Loss: 0.00075127
Iteration 13/25 | Loss: 0.00075127
Iteration 14/25 | Loss: 0.00075127
Iteration 15/25 | Loss: 0.00075127
Iteration 16/25 | Loss: 0.00075127
Iteration 17/25 | Loss: 0.00075127
Iteration 18/25 | Loss: 0.00075127
Iteration 19/25 | Loss: 0.00075127
Iteration 20/25 | Loss: 0.00075127
Iteration 21/25 | Loss: 0.00075127
Iteration 22/25 | Loss: 0.00075127
Iteration 23/25 | Loss: 0.00075127
Iteration 24/25 | Loss: 0.00075127
Iteration 25/25 | Loss: 0.00075127

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075127
Iteration 2/1000 | Loss: 0.00001584
Iteration 3/1000 | Loss: 0.00001098
Iteration 4/1000 | Loss: 0.00000966
Iteration 5/1000 | Loss: 0.00000923
Iteration 6/1000 | Loss: 0.00000878
Iteration 7/1000 | Loss: 0.00000878
Iteration 8/1000 | Loss: 0.00000864
Iteration 9/1000 | Loss: 0.00000864
Iteration 10/1000 | Loss: 0.00000859
Iteration 11/1000 | Loss: 0.00000837
Iteration 12/1000 | Loss: 0.00000833
Iteration 13/1000 | Loss: 0.00000827
Iteration 14/1000 | Loss: 0.00000827
Iteration 15/1000 | Loss: 0.00000821
Iteration 16/1000 | Loss: 0.00000818
Iteration 17/1000 | Loss: 0.00000818
Iteration 18/1000 | Loss: 0.00000817
Iteration 19/1000 | Loss: 0.00000817
Iteration 20/1000 | Loss: 0.00000816
Iteration 21/1000 | Loss: 0.00000811
Iteration 22/1000 | Loss: 0.00000810
Iteration 23/1000 | Loss: 0.00000809
Iteration 24/1000 | Loss: 0.00000808
Iteration 25/1000 | Loss: 0.00000808
Iteration 26/1000 | Loss: 0.00000806
Iteration 27/1000 | Loss: 0.00000806
Iteration 28/1000 | Loss: 0.00000805
Iteration 29/1000 | Loss: 0.00000804
Iteration 30/1000 | Loss: 0.00000804
Iteration 31/1000 | Loss: 0.00000804
Iteration 32/1000 | Loss: 0.00000804
Iteration 33/1000 | Loss: 0.00000804
Iteration 34/1000 | Loss: 0.00000803
Iteration 35/1000 | Loss: 0.00000802
Iteration 36/1000 | Loss: 0.00000802
Iteration 37/1000 | Loss: 0.00000800
Iteration 38/1000 | Loss: 0.00000800
Iteration 39/1000 | Loss: 0.00000800
Iteration 40/1000 | Loss: 0.00000799
Iteration 41/1000 | Loss: 0.00000799
Iteration 42/1000 | Loss: 0.00000799
Iteration 43/1000 | Loss: 0.00000799
Iteration 44/1000 | Loss: 0.00000799
Iteration 45/1000 | Loss: 0.00000799
Iteration 46/1000 | Loss: 0.00000799
Iteration 47/1000 | Loss: 0.00000799
Iteration 48/1000 | Loss: 0.00000799
Iteration 49/1000 | Loss: 0.00000797
Iteration 50/1000 | Loss: 0.00000797
Iteration 51/1000 | Loss: 0.00000796
Iteration 52/1000 | Loss: 0.00000795
Iteration 53/1000 | Loss: 0.00000795
Iteration 54/1000 | Loss: 0.00000795
Iteration 55/1000 | Loss: 0.00000795
Iteration 56/1000 | Loss: 0.00000795
Iteration 57/1000 | Loss: 0.00000795
Iteration 58/1000 | Loss: 0.00000794
Iteration 59/1000 | Loss: 0.00000794
Iteration 60/1000 | Loss: 0.00000794
Iteration 61/1000 | Loss: 0.00000794
Iteration 62/1000 | Loss: 0.00000793
Iteration 63/1000 | Loss: 0.00000793
Iteration 64/1000 | Loss: 0.00000793
Iteration 65/1000 | Loss: 0.00000792
Iteration 66/1000 | Loss: 0.00000792
Iteration 67/1000 | Loss: 0.00000791
Iteration 68/1000 | Loss: 0.00000791
Iteration 69/1000 | Loss: 0.00000791
Iteration 70/1000 | Loss: 0.00000791
Iteration 71/1000 | Loss: 0.00000789
Iteration 72/1000 | Loss: 0.00000788
Iteration 73/1000 | Loss: 0.00000787
Iteration 74/1000 | Loss: 0.00000787
Iteration 75/1000 | Loss: 0.00000786
Iteration 76/1000 | Loss: 0.00000786
Iteration 77/1000 | Loss: 0.00000786
Iteration 78/1000 | Loss: 0.00000786
Iteration 79/1000 | Loss: 0.00000785
Iteration 80/1000 | Loss: 0.00000785
Iteration 81/1000 | Loss: 0.00000785
Iteration 82/1000 | Loss: 0.00000785
Iteration 83/1000 | Loss: 0.00000784
Iteration 84/1000 | Loss: 0.00000783
Iteration 85/1000 | Loss: 0.00000783
Iteration 86/1000 | Loss: 0.00000783
Iteration 87/1000 | Loss: 0.00000782
Iteration 88/1000 | Loss: 0.00000782
Iteration 89/1000 | Loss: 0.00000781
Iteration 90/1000 | Loss: 0.00000781
Iteration 91/1000 | Loss: 0.00000781
Iteration 92/1000 | Loss: 0.00000781
Iteration 93/1000 | Loss: 0.00000781
Iteration 94/1000 | Loss: 0.00000781
Iteration 95/1000 | Loss: 0.00000781
Iteration 96/1000 | Loss: 0.00000781
Iteration 97/1000 | Loss: 0.00000780
Iteration 98/1000 | Loss: 0.00000780
Iteration 99/1000 | Loss: 0.00000780
Iteration 100/1000 | Loss: 0.00000780
Iteration 101/1000 | Loss: 0.00000780
Iteration 102/1000 | Loss: 0.00000780
Iteration 103/1000 | Loss: 0.00000779
Iteration 104/1000 | Loss: 0.00000779
Iteration 105/1000 | Loss: 0.00000779
Iteration 106/1000 | Loss: 0.00000778
Iteration 107/1000 | Loss: 0.00000778
Iteration 108/1000 | Loss: 0.00000778
Iteration 109/1000 | Loss: 0.00000778
Iteration 110/1000 | Loss: 0.00000778
Iteration 111/1000 | Loss: 0.00000778
Iteration 112/1000 | Loss: 0.00000778
Iteration 113/1000 | Loss: 0.00000778
Iteration 114/1000 | Loss: 0.00000777
Iteration 115/1000 | Loss: 0.00000777
Iteration 116/1000 | Loss: 0.00000776
Iteration 117/1000 | Loss: 0.00000776
Iteration 118/1000 | Loss: 0.00000776
Iteration 119/1000 | Loss: 0.00000776
Iteration 120/1000 | Loss: 0.00000776
Iteration 121/1000 | Loss: 0.00000776
Iteration 122/1000 | Loss: 0.00000776
Iteration 123/1000 | Loss: 0.00000775
Iteration 124/1000 | Loss: 0.00000775
Iteration 125/1000 | Loss: 0.00000775
Iteration 126/1000 | Loss: 0.00000775
Iteration 127/1000 | Loss: 0.00000775
Iteration 128/1000 | Loss: 0.00000775
Iteration 129/1000 | Loss: 0.00000775
Iteration 130/1000 | Loss: 0.00000775
Iteration 131/1000 | Loss: 0.00000775
Iteration 132/1000 | Loss: 0.00000775
Iteration 133/1000 | Loss: 0.00000775
Iteration 134/1000 | Loss: 0.00000775
Iteration 135/1000 | Loss: 0.00000775
Iteration 136/1000 | Loss: 0.00000775
Iteration 137/1000 | Loss: 0.00000774
Iteration 138/1000 | Loss: 0.00000774
Iteration 139/1000 | Loss: 0.00000774
Iteration 140/1000 | Loss: 0.00000774
Iteration 141/1000 | Loss: 0.00000774
Iteration 142/1000 | Loss: 0.00000774
Iteration 143/1000 | Loss: 0.00000774
Iteration 144/1000 | Loss: 0.00000773
Iteration 145/1000 | Loss: 0.00000773
Iteration 146/1000 | Loss: 0.00000773
Iteration 147/1000 | Loss: 0.00000773
Iteration 148/1000 | Loss: 0.00000773
Iteration 149/1000 | Loss: 0.00000773
Iteration 150/1000 | Loss: 0.00000773
Iteration 151/1000 | Loss: 0.00000773
Iteration 152/1000 | Loss: 0.00000773
Iteration 153/1000 | Loss: 0.00000772
Iteration 154/1000 | Loss: 0.00000772
Iteration 155/1000 | Loss: 0.00000772
Iteration 156/1000 | Loss: 0.00000772
Iteration 157/1000 | Loss: 0.00000772
Iteration 158/1000 | Loss: 0.00000772
Iteration 159/1000 | Loss: 0.00000772
Iteration 160/1000 | Loss: 0.00000772
Iteration 161/1000 | Loss: 0.00000772
Iteration 162/1000 | Loss: 0.00000772
Iteration 163/1000 | Loss: 0.00000772
Iteration 164/1000 | Loss: 0.00000772
Iteration 165/1000 | Loss: 0.00000772
Iteration 166/1000 | Loss: 0.00000772
Iteration 167/1000 | Loss: 0.00000771
Iteration 168/1000 | Loss: 0.00000771
Iteration 169/1000 | Loss: 0.00000771
Iteration 170/1000 | Loss: 0.00000771
Iteration 171/1000 | Loss: 0.00000771
Iteration 172/1000 | Loss: 0.00000771
Iteration 173/1000 | Loss: 0.00000771
Iteration 174/1000 | Loss: 0.00000771
Iteration 175/1000 | Loss: 0.00000771
Iteration 176/1000 | Loss: 0.00000771
Iteration 177/1000 | Loss: 0.00000771
Iteration 178/1000 | Loss: 0.00000771
Iteration 179/1000 | Loss: 0.00000771
Iteration 180/1000 | Loss: 0.00000771
Iteration 181/1000 | Loss: 0.00000771
Iteration 182/1000 | Loss: 0.00000770
Iteration 183/1000 | Loss: 0.00000770
Iteration 184/1000 | Loss: 0.00000770
Iteration 185/1000 | Loss: 0.00000770
Iteration 186/1000 | Loss: 0.00000770
Iteration 187/1000 | Loss: 0.00000770
Iteration 188/1000 | Loss: 0.00000770
Iteration 189/1000 | Loss: 0.00000770
Iteration 190/1000 | Loss: 0.00000770
Iteration 191/1000 | Loss: 0.00000770
Iteration 192/1000 | Loss: 0.00000770
Iteration 193/1000 | Loss: 0.00000770
Iteration 194/1000 | Loss: 0.00000770
Iteration 195/1000 | Loss: 0.00000769
Iteration 196/1000 | Loss: 0.00000769
Iteration 197/1000 | Loss: 0.00000769
Iteration 198/1000 | Loss: 0.00000769
Iteration 199/1000 | Loss: 0.00000769
Iteration 200/1000 | Loss: 0.00000769
Iteration 201/1000 | Loss: 0.00000769
Iteration 202/1000 | Loss: 0.00000769
Iteration 203/1000 | Loss: 0.00000769
Iteration 204/1000 | Loss: 0.00000769
Iteration 205/1000 | Loss: 0.00000769
Iteration 206/1000 | Loss: 0.00000769
Iteration 207/1000 | Loss: 0.00000769
Iteration 208/1000 | Loss: 0.00000768
Iteration 209/1000 | Loss: 0.00000768
Iteration 210/1000 | Loss: 0.00000768
Iteration 211/1000 | Loss: 0.00000768
Iteration 212/1000 | Loss: 0.00000768
Iteration 213/1000 | Loss: 0.00000768
Iteration 214/1000 | Loss: 0.00000768
Iteration 215/1000 | Loss: 0.00000768
Iteration 216/1000 | Loss: 0.00000768
Iteration 217/1000 | Loss: 0.00000768
Iteration 218/1000 | Loss: 0.00000768
Iteration 219/1000 | Loss: 0.00000768
Iteration 220/1000 | Loss: 0.00000768
Iteration 221/1000 | Loss: 0.00000768
Iteration 222/1000 | Loss: 0.00000768
Iteration 223/1000 | Loss: 0.00000768
Iteration 224/1000 | Loss: 0.00000768
Iteration 225/1000 | Loss: 0.00000768
Iteration 226/1000 | Loss: 0.00000768
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [7.678365363972262e-06, 7.678365363972262e-06, 7.678365363972262e-06, 7.678365363972262e-06, 7.678365363972262e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.678365363972262e-06

Optimization complete. Final v2v error: 2.389272928237915 mm

Highest mean error: 2.8595709800720215 mm for frame 139

Lowest mean error: 2.320333957672119 mm for frame 159

Saving results

Total time: 42.56422710418701
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00864362
Iteration 2/25 | Loss: 0.00119989
Iteration 3/25 | Loss: 0.00111552
Iteration 4/25 | Loss: 0.00110593
Iteration 5/25 | Loss: 0.00110539
Iteration 6/25 | Loss: 0.00110539
Iteration 7/25 | Loss: 0.00110539
Iteration 8/25 | Loss: 0.00110539
Iteration 9/25 | Loss: 0.00110539
Iteration 10/25 | Loss: 0.00110539
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011053855996578932, 0.0011053855996578932, 0.0011053855996578932, 0.0011053855996578932, 0.0011053855996578932]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011053855996578932

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30600798
Iteration 2/25 | Loss: 0.00061595
Iteration 3/25 | Loss: 0.00061586
Iteration 4/25 | Loss: 0.00061586
Iteration 5/25 | Loss: 0.00061586
Iteration 6/25 | Loss: 0.00061586
Iteration 7/25 | Loss: 0.00061586
Iteration 8/25 | Loss: 0.00061585
Iteration 9/25 | Loss: 0.00061585
Iteration 10/25 | Loss: 0.00061585
Iteration 11/25 | Loss: 0.00061585
Iteration 12/25 | Loss: 0.00061585
Iteration 13/25 | Loss: 0.00061585
Iteration 14/25 | Loss: 0.00061585
Iteration 15/25 | Loss: 0.00061585
Iteration 16/25 | Loss: 0.00061585
Iteration 17/25 | Loss: 0.00061585
Iteration 18/25 | Loss: 0.00061585
Iteration 19/25 | Loss: 0.00061585
Iteration 20/25 | Loss: 0.00061585
Iteration 21/25 | Loss: 0.00061585
Iteration 22/25 | Loss: 0.00061585
Iteration 23/25 | Loss: 0.00061585
Iteration 24/25 | Loss: 0.00061585
Iteration 25/25 | Loss: 0.00061585

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061585
Iteration 2/1000 | Loss: 0.00002636
Iteration 3/1000 | Loss: 0.00001779
Iteration 4/1000 | Loss: 0.00001598
Iteration 5/1000 | Loss: 0.00001470
Iteration 6/1000 | Loss: 0.00001404
Iteration 7/1000 | Loss: 0.00001351
Iteration 8/1000 | Loss: 0.00001335
Iteration 9/1000 | Loss: 0.00001303
Iteration 10/1000 | Loss: 0.00001283
Iteration 11/1000 | Loss: 0.00001264
Iteration 12/1000 | Loss: 0.00001258
Iteration 13/1000 | Loss: 0.00001257
Iteration 14/1000 | Loss: 0.00001248
Iteration 15/1000 | Loss: 0.00001233
Iteration 16/1000 | Loss: 0.00001231
Iteration 17/1000 | Loss: 0.00001226
Iteration 18/1000 | Loss: 0.00001225
Iteration 19/1000 | Loss: 0.00001219
Iteration 20/1000 | Loss: 0.00001219
Iteration 21/1000 | Loss: 0.00001218
Iteration 22/1000 | Loss: 0.00001212
Iteration 23/1000 | Loss: 0.00001210
Iteration 24/1000 | Loss: 0.00001209
Iteration 25/1000 | Loss: 0.00001208
Iteration 26/1000 | Loss: 0.00001207
Iteration 27/1000 | Loss: 0.00001207
Iteration 28/1000 | Loss: 0.00001206
Iteration 29/1000 | Loss: 0.00001205
Iteration 30/1000 | Loss: 0.00001205
Iteration 31/1000 | Loss: 0.00001204
Iteration 32/1000 | Loss: 0.00001202
Iteration 33/1000 | Loss: 0.00001202
Iteration 34/1000 | Loss: 0.00001202
Iteration 35/1000 | Loss: 0.00001202
Iteration 36/1000 | Loss: 0.00001201
Iteration 37/1000 | Loss: 0.00001201
Iteration 38/1000 | Loss: 0.00001200
Iteration 39/1000 | Loss: 0.00001200
Iteration 40/1000 | Loss: 0.00001199
Iteration 41/1000 | Loss: 0.00001199
Iteration 42/1000 | Loss: 0.00001199
Iteration 43/1000 | Loss: 0.00001199
Iteration 44/1000 | Loss: 0.00001198
Iteration 45/1000 | Loss: 0.00001198
Iteration 46/1000 | Loss: 0.00001197
Iteration 47/1000 | Loss: 0.00001196
Iteration 48/1000 | Loss: 0.00001196
Iteration 49/1000 | Loss: 0.00001195
Iteration 50/1000 | Loss: 0.00001194
Iteration 51/1000 | Loss: 0.00001194
Iteration 52/1000 | Loss: 0.00001193
Iteration 53/1000 | Loss: 0.00001193
Iteration 54/1000 | Loss: 0.00001192
Iteration 55/1000 | Loss: 0.00001192
Iteration 56/1000 | Loss: 0.00001192
Iteration 57/1000 | Loss: 0.00001192
Iteration 58/1000 | Loss: 0.00001192
Iteration 59/1000 | Loss: 0.00001192
Iteration 60/1000 | Loss: 0.00001191
Iteration 61/1000 | Loss: 0.00001191
Iteration 62/1000 | Loss: 0.00001191
Iteration 63/1000 | Loss: 0.00001191
Iteration 64/1000 | Loss: 0.00001191
Iteration 65/1000 | Loss: 0.00001190
Iteration 66/1000 | Loss: 0.00001190
Iteration 67/1000 | Loss: 0.00001190
Iteration 68/1000 | Loss: 0.00001190
Iteration 69/1000 | Loss: 0.00001189
Iteration 70/1000 | Loss: 0.00001189
Iteration 71/1000 | Loss: 0.00001189
Iteration 72/1000 | Loss: 0.00001189
Iteration 73/1000 | Loss: 0.00001189
Iteration 74/1000 | Loss: 0.00001189
Iteration 75/1000 | Loss: 0.00001189
Iteration 76/1000 | Loss: 0.00001188
Iteration 77/1000 | Loss: 0.00001188
Iteration 78/1000 | Loss: 0.00001188
Iteration 79/1000 | Loss: 0.00001188
Iteration 80/1000 | Loss: 0.00001187
Iteration 81/1000 | Loss: 0.00001187
Iteration 82/1000 | Loss: 0.00001186
Iteration 83/1000 | Loss: 0.00001186
Iteration 84/1000 | Loss: 0.00001185
Iteration 85/1000 | Loss: 0.00001185
Iteration 86/1000 | Loss: 0.00001185
Iteration 87/1000 | Loss: 0.00001184
Iteration 88/1000 | Loss: 0.00001184
Iteration 89/1000 | Loss: 0.00001184
Iteration 90/1000 | Loss: 0.00001183
Iteration 91/1000 | Loss: 0.00001182
Iteration 92/1000 | Loss: 0.00001182
Iteration 93/1000 | Loss: 0.00001182
Iteration 94/1000 | Loss: 0.00001182
Iteration 95/1000 | Loss: 0.00001182
Iteration 96/1000 | Loss: 0.00001182
Iteration 97/1000 | Loss: 0.00001182
Iteration 98/1000 | Loss: 0.00001181
Iteration 99/1000 | Loss: 0.00001181
Iteration 100/1000 | Loss: 0.00001181
Iteration 101/1000 | Loss: 0.00001181
Iteration 102/1000 | Loss: 0.00001181
Iteration 103/1000 | Loss: 0.00001181
Iteration 104/1000 | Loss: 0.00001181
Iteration 105/1000 | Loss: 0.00001181
Iteration 106/1000 | Loss: 0.00001181
Iteration 107/1000 | Loss: 0.00001181
Iteration 108/1000 | Loss: 0.00001181
Iteration 109/1000 | Loss: 0.00001181
Iteration 110/1000 | Loss: 0.00001181
Iteration 111/1000 | Loss: 0.00001181
Iteration 112/1000 | Loss: 0.00001181
Iteration 113/1000 | Loss: 0.00001181
Iteration 114/1000 | Loss: 0.00001181
Iteration 115/1000 | Loss: 0.00001181
Iteration 116/1000 | Loss: 0.00001181
Iteration 117/1000 | Loss: 0.00001181
Iteration 118/1000 | Loss: 0.00001181
Iteration 119/1000 | Loss: 0.00001181
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.1808711860794574e-05, 1.1808711860794574e-05, 1.1808711860794574e-05, 1.1808711860794574e-05, 1.1808711860794574e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1808711860794574e-05

Optimization complete. Final v2v error: 2.9279625415802 mm

Highest mean error: 3.233144760131836 mm for frame 220

Lowest mean error: 2.7618937492370605 mm for frame 54

Saving results

Total time: 39.182451486587524
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01014672
Iteration 2/25 | Loss: 0.00167723
Iteration 3/25 | Loss: 0.00126976
Iteration 4/25 | Loss: 0.00115985
Iteration 5/25 | Loss: 0.00114845
Iteration 6/25 | Loss: 0.00114769
Iteration 7/25 | Loss: 0.00115271
Iteration 8/25 | Loss: 0.00115372
Iteration 9/25 | Loss: 0.00114901
Iteration 10/25 | Loss: 0.00113210
Iteration 11/25 | Loss: 0.00112861
Iteration 12/25 | Loss: 0.00112592
Iteration 13/25 | Loss: 0.00112464
Iteration 14/25 | Loss: 0.00112525
Iteration 15/25 | Loss: 0.00112476
Iteration 16/25 | Loss: 0.00112382
Iteration 17/25 | Loss: 0.00112160
Iteration 18/25 | Loss: 0.00112111
Iteration 19/25 | Loss: 0.00112105
Iteration 20/25 | Loss: 0.00112104
Iteration 21/25 | Loss: 0.00112103
Iteration 22/25 | Loss: 0.00112103
Iteration 23/25 | Loss: 0.00112103
Iteration 24/25 | Loss: 0.00112103
Iteration 25/25 | Loss: 0.00112103

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49407899
Iteration 2/25 | Loss: 0.00098559
Iteration 3/25 | Loss: 0.00098559
Iteration 4/25 | Loss: 0.00098559
Iteration 5/25 | Loss: 0.00098559
Iteration 6/25 | Loss: 0.00098559
Iteration 7/25 | Loss: 0.00098559
Iteration 8/25 | Loss: 0.00098559
Iteration 9/25 | Loss: 0.00098559
Iteration 10/25 | Loss: 0.00098559
Iteration 11/25 | Loss: 0.00098559
Iteration 12/25 | Loss: 0.00098559
Iteration 13/25 | Loss: 0.00098559
Iteration 14/25 | Loss: 0.00098559
Iteration 15/25 | Loss: 0.00098559
Iteration 16/25 | Loss: 0.00098559
Iteration 17/25 | Loss: 0.00098559
Iteration 18/25 | Loss: 0.00098559
Iteration 19/25 | Loss: 0.00098559
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009855852695181966, 0.0009855852695181966, 0.0009855852695181966, 0.0009855852695181966, 0.0009855852695181966]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009855852695181966

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098559
Iteration 2/1000 | Loss: 0.00003675
Iteration 3/1000 | Loss: 0.00002457
Iteration 4/1000 | Loss: 0.00002149
Iteration 5/1000 | Loss: 0.00001947
Iteration 6/1000 | Loss: 0.00001825
Iteration 7/1000 | Loss: 0.00001765
Iteration 8/1000 | Loss: 0.00001707
Iteration 9/1000 | Loss: 0.00001656
Iteration 10/1000 | Loss: 0.00001630
Iteration 11/1000 | Loss: 0.00001613
Iteration 12/1000 | Loss: 0.00018230
Iteration 13/1000 | Loss: 0.00015245
Iteration 14/1000 | Loss: 0.00006742
Iteration 15/1000 | Loss: 0.00001630
Iteration 16/1000 | Loss: 0.00001581
Iteration 17/1000 | Loss: 0.00016763
Iteration 18/1000 | Loss: 0.00020055
Iteration 19/1000 | Loss: 0.00009358
Iteration 20/1000 | Loss: 0.00019500
Iteration 21/1000 | Loss: 0.00001984
Iteration 22/1000 | Loss: 0.00001782
Iteration 23/1000 | Loss: 0.00001717
Iteration 24/1000 | Loss: 0.00001658
Iteration 25/1000 | Loss: 0.00001630
Iteration 26/1000 | Loss: 0.00001606
Iteration 27/1000 | Loss: 0.00001605
Iteration 28/1000 | Loss: 0.00001604
Iteration 29/1000 | Loss: 0.00001602
Iteration 30/1000 | Loss: 0.00001602
Iteration 31/1000 | Loss: 0.00001602
Iteration 32/1000 | Loss: 0.00001589
Iteration 33/1000 | Loss: 0.00001579
Iteration 34/1000 | Loss: 0.00001578
Iteration 35/1000 | Loss: 0.00001577
Iteration 36/1000 | Loss: 0.00001576
Iteration 37/1000 | Loss: 0.00001575
Iteration 38/1000 | Loss: 0.00001569
Iteration 39/1000 | Loss: 0.00001555
Iteration 40/1000 | Loss: 0.00001555
Iteration 41/1000 | Loss: 0.00001554
Iteration 42/1000 | Loss: 0.00001541
Iteration 43/1000 | Loss: 0.00001540
Iteration 44/1000 | Loss: 0.00001539
Iteration 45/1000 | Loss: 0.00001539
Iteration 46/1000 | Loss: 0.00001538
Iteration 47/1000 | Loss: 0.00019831
Iteration 48/1000 | Loss: 0.00001868
Iteration 49/1000 | Loss: 0.00010892
Iteration 50/1000 | Loss: 0.00007783
Iteration 51/1000 | Loss: 0.00001746
Iteration 52/1000 | Loss: 0.00001710
Iteration 53/1000 | Loss: 0.00001686
Iteration 54/1000 | Loss: 0.00001654
Iteration 55/1000 | Loss: 0.00016090
Iteration 56/1000 | Loss: 0.00002508
Iteration 57/1000 | Loss: 0.00002043
Iteration 58/1000 | Loss: 0.00001952
Iteration 59/1000 | Loss: 0.00001892
Iteration 60/1000 | Loss: 0.00001852
Iteration 61/1000 | Loss: 0.00001822
Iteration 62/1000 | Loss: 0.00001766
Iteration 63/1000 | Loss: 0.00001733
Iteration 64/1000 | Loss: 0.00001724
Iteration 65/1000 | Loss: 0.00001723
Iteration 66/1000 | Loss: 0.00001701
Iteration 67/1000 | Loss: 0.00001683
Iteration 68/1000 | Loss: 0.00001670
Iteration 69/1000 | Loss: 0.00001662
Iteration 70/1000 | Loss: 0.00001632
Iteration 71/1000 | Loss: 0.00001605
Iteration 72/1000 | Loss: 0.00001583
Iteration 73/1000 | Loss: 0.00001556
Iteration 74/1000 | Loss: 0.00020863
Iteration 75/1000 | Loss: 0.00004354
Iteration 76/1000 | Loss: 0.00001567
Iteration 77/1000 | Loss: 0.00001548
Iteration 78/1000 | Loss: 0.00001542
Iteration 79/1000 | Loss: 0.00001542
Iteration 80/1000 | Loss: 0.00001540
Iteration 81/1000 | Loss: 0.00001539
Iteration 82/1000 | Loss: 0.00001539
Iteration 83/1000 | Loss: 0.00001539
Iteration 84/1000 | Loss: 0.00020952
Iteration 85/1000 | Loss: 0.00021807
Iteration 86/1000 | Loss: 0.00015321
Iteration 87/1000 | Loss: 0.00023341
Iteration 88/1000 | Loss: 0.00024992
Iteration 89/1000 | Loss: 0.00015103
Iteration 90/1000 | Loss: 0.00003349
Iteration 91/1000 | Loss: 0.00001997
Iteration 92/1000 | Loss: 0.00001821
Iteration 93/1000 | Loss: 0.00001705
Iteration 94/1000 | Loss: 0.00001628
Iteration 95/1000 | Loss: 0.00001594
Iteration 96/1000 | Loss: 0.00001571
Iteration 97/1000 | Loss: 0.00001569
Iteration 98/1000 | Loss: 0.00001567
Iteration 99/1000 | Loss: 0.00001567
Iteration 100/1000 | Loss: 0.00001562
Iteration 101/1000 | Loss: 0.00001557
Iteration 102/1000 | Loss: 0.00001557
Iteration 103/1000 | Loss: 0.00001556
Iteration 104/1000 | Loss: 0.00001556
Iteration 105/1000 | Loss: 0.00001555
Iteration 106/1000 | Loss: 0.00001555
Iteration 107/1000 | Loss: 0.00001554
Iteration 108/1000 | Loss: 0.00001554
Iteration 109/1000 | Loss: 0.00001554
Iteration 110/1000 | Loss: 0.00001554
Iteration 111/1000 | Loss: 0.00001554
Iteration 112/1000 | Loss: 0.00001553
Iteration 113/1000 | Loss: 0.00001553
Iteration 114/1000 | Loss: 0.00001553
Iteration 115/1000 | Loss: 0.00001553
Iteration 116/1000 | Loss: 0.00001553
Iteration 117/1000 | Loss: 0.00001552
Iteration 118/1000 | Loss: 0.00001552
Iteration 119/1000 | Loss: 0.00001552
Iteration 120/1000 | Loss: 0.00001552
Iteration 121/1000 | Loss: 0.00001552
Iteration 122/1000 | Loss: 0.00001552
Iteration 123/1000 | Loss: 0.00001551
Iteration 124/1000 | Loss: 0.00001551
Iteration 125/1000 | Loss: 0.00001551
Iteration 126/1000 | Loss: 0.00001551
Iteration 127/1000 | Loss: 0.00001551
Iteration 128/1000 | Loss: 0.00001551
Iteration 129/1000 | Loss: 0.00001551
Iteration 130/1000 | Loss: 0.00001550
Iteration 131/1000 | Loss: 0.00001550
Iteration 132/1000 | Loss: 0.00001550
Iteration 133/1000 | Loss: 0.00001550
Iteration 134/1000 | Loss: 0.00001550
Iteration 135/1000 | Loss: 0.00001549
Iteration 136/1000 | Loss: 0.00001549
Iteration 137/1000 | Loss: 0.00001549
Iteration 138/1000 | Loss: 0.00001549
Iteration 139/1000 | Loss: 0.00001548
Iteration 140/1000 | Loss: 0.00001548
Iteration 141/1000 | Loss: 0.00001548
Iteration 142/1000 | Loss: 0.00001548
Iteration 143/1000 | Loss: 0.00001548
Iteration 144/1000 | Loss: 0.00001548
Iteration 145/1000 | Loss: 0.00001548
Iteration 146/1000 | Loss: 0.00001548
Iteration 147/1000 | Loss: 0.00001548
Iteration 148/1000 | Loss: 0.00001548
Iteration 149/1000 | Loss: 0.00001547
Iteration 150/1000 | Loss: 0.00001547
Iteration 151/1000 | Loss: 0.00001547
Iteration 152/1000 | Loss: 0.00001547
Iteration 153/1000 | Loss: 0.00001547
Iteration 154/1000 | Loss: 0.00001547
Iteration 155/1000 | Loss: 0.00001547
Iteration 156/1000 | Loss: 0.00001546
Iteration 157/1000 | Loss: 0.00001546
Iteration 158/1000 | Loss: 0.00001546
Iteration 159/1000 | Loss: 0.00001545
Iteration 160/1000 | Loss: 0.00001545
Iteration 161/1000 | Loss: 0.00001545
Iteration 162/1000 | Loss: 0.00001545
Iteration 163/1000 | Loss: 0.00001544
Iteration 164/1000 | Loss: 0.00001544
Iteration 165/1000 | Loss: 0.00001544
Iteration 166/1000 | Loss: 0.00001544
Iteration 167/1000 | Loss: 0.00001543
Iteration 168/1000 | Loss: 0.00001543
Iteration 169/1000 | Loss: 0.00001543
Iteration 170/1000 | Loss: 0.00001543
Iteration 171/1000 | Loss: 0.00001543
Iteration 172/1000 | Loss: 0.00001543
Iteration 173/1000 | Loss: 0.00001542
Iteration 174/1000 | Loss: 0.00001542
Iteration 175/1000 | Loss: 0.00001542
Iteration 176/1000 | Loss: 0.00001541
Iteration 177/1000 | Loss: 0.00001541
Iteration 178/1000 | Loss: 0.00001541
Iteration 179/1000 | Loss: 0.00001541
Iteration 180/1000 | Loss: 0.00001541
Iteration 181/1000 | Loss: 0.00001540
Iteration 182/1000 | Loss: 0.00001540
Iteration 183/1000 | Loss: 0.00001540
Iteration 184/1000 | Loss: 0.00001540
Iteration 185/1000 | Loss: 0.00001540
Iteration 186/1000 | Loss: 0.00001540
Iteration 187/1000 | Loss: 0.00001540
Iteration 188/1000 | Loss: 0.00001540
Iteration 189/1000 | Loss: 0.00001540
Iteration 190/1000 | Loss: 0.00001540
Iteration 191/1000 | Loss: 0.00001540
Iteration 192/1000 | Loss: 0.00001540
Iteration 193/1000 | Loss: 0.00001540
Iteration 194/1000 | Loss: 0.00001540
Iteration 195/1000 | Loss: 0.00001540
Iteration 196/1000 | Loss: 0.00001540
Iteration 197/1000 | Loss: 0.00001540
Iteration 198/1000 | Loss: 0.00001540
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [1.539819459139835e-05, 1.539819459139835e-05, 1.539819459139835e-05, 1.539819459139835e-05, 1.539819459139835e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.539819459139835e-05

Optimization complete. Final v2v error: 3.3055458068847656 mm

Highest mean error: 7.559574604034424 mm for frame 144

Lowest mean error: 2.812544107437134 mm for frame 23

Saving results

Total time: 154.9866943359375
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00816388
Iteration 2/25 | Loss: 0.00114600
Iteration 3/25 | Loss: 0.00104609
Iteration 4/25 | Loss: 0.00104053
Iteration 5/25 | Loss: 0.00104016
Iteration 6/25 | Loss: 0.00104016
Iteration 7/25 | Loss: 0.00104016
Iteration 8/25 | Loss: 0.00104016
Iteration 9/25 | Loss: 0.00104016
Iteration 10/25 | Loss: 0.00104016
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001040155882947147, 0.001040155882947147, 0.001040155882947147, 0.001040155882947147, 0.001040155882947147]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001040155882947147

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37365150
Iteration 2/25 | Loss: 0.00066637
Iteration 3/25 | Loss: 0.00066637
Iteration 4/25 | Loss: 0.00066636
Iteration 5/25 | Loss: 0.00066636
Iteration 6/25 | Loss: 0.00066636
Iteration 7/25 | Loss: 0.00066636
Iteration 8/25 | Loss: 0.00066636
Iteration 9/25 | Loss: 0.00066636
Iteration 10/25 | Loss: 0.00066636
Iteration 11/25 | Loss: 0.00066636
Iteration 12/25 | Loss: 0.00066636
Iteration 13/25 | Loss: 0.00066636
Iteration 14/25 | Loss: 0.00066636
Iteration 15/25 | Loss: 0.00066636
Iteration 16/25 | Loss: 0.00066636
Iteration 17/25 | Loss: 0.00066636
Iteration 18/25 | Loss: 0.00066636
Iteration 19/25 | Loss: 0.00066636
Iteration 20/25 | Loss: 0.00066636
Iteration 21/25 | Loss: 0.00066636
Iteration 22/25 | Loss: 0.00066636
Iteration 23/25 | Loss: 0.00066636
Iteration 24/25 | Loss: 0.00066636
Iteration 25/25 | Loss: 0.00066636

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066636
Iteration 2/1000 | Loss: 0.00001758
Iteration 3/1000 | Loss: 0.00001238
Iteration 4/1000 | Loss: 0.00001102
Iteration 5/1000 | Loss: 0.00001028
Iteration 6/1000 | Loss: 0.00000986
Iteration 7/1000 | Loss: 0.00000955
Iteration 8/1000 | Loss: 0.00000941
Iteration 9/1000 | Loss: 0.00000914
Iteration 10/1000 | Loss: 0.00000906
Iteration 11/1000 | Loss: 0.00000906
Iteration 12/1000 | Loss: 0.00000903
Iteration 13/1000 | Loss: 0.00000902
Iteration 14/1000 | Loss: 0.00000901
Iteration 15/1000 | Loss: 0.00000901
Iteration 16/1000 | Loss: 0.00000896
Iteration 17/1000 | Loss: 0.00000895
Iteration 18/1000 | Loss: 0.00000890
Iteration 19/1000 | Loss: 0.00000883
Iteration 20/1000 | Loss: 0.00000883
Iteration 21/1000 | Loss: 0.00000881
Iteration 22/1000 | Loss: 0.00000881
Iteration 23/1000 | Loss: 0.00000878
Iteration 24/1000 | Loss: 0.00000878
Iteration 25/1000 | Loss: 0.00000877
Iteration 26/1000 | Loss: 0.00000877
Iteration 27/1000 | Loss: 0.00000877
Iteration 28/1000 | Loss: 0.00000877
Iteration 29/1000 | Loss: 0.00000877
Iteration 30/1000 | Loss: 0.00000876
Iteration 31/1000 | Loss: 0.00000876
Iteration 32/1000 | Loss: 0.00000876
Iteration 33/1000 | Loss: 0.00000876
Iteration 34/1000 | Loss: 0.00000876
Iteration 35/1000 | Loss: 0.00000875
Iteration 36/1000 | Loss: 0.00000875
Iteration 37/1000 | Loss: 0.00000874
Iteration 38/1000 | Loss: 0.00000874
Iteration 39/1000 | Loss: 0.00000873
Iteration 40/1000 | Loss: 0.00000873
Iteration 41/1000 | Loss: 0.00000873
Iteration 42/1000 | Loss: 0.00000872
Iteration 43/1000 | Loss: 0.00000871
Iteration 44/1000 | Loss: 0.00000871
Iteration 45/1000 | Loss: 0.00000870
Iteration 46/1000 | Loss: 0.00000869
Iteration 47/1000 | Loss: 0.00000869
Iteration 48/1000 | Loss: 0.00000868
Iteration 49/1000 | Loss: 0.00000868
Iteration 50/1000 | Loss: 0.00000867
Iteration 51/1000 | Loss: 0.00000867
Iteration 52/1000 | Loss: 0.00000866
Iteration 53/1000 | Loss: 0.00000865
Iteration 54/1000 | Loss: 0.00000865
Iteration 55/1000 | Loss: 0.00000864
Iteration 56/1000 | Loss: 0.00000863
Iteration 57/1000 | Loss: 0.00000863
Iteration 58/1000 | Loss: 0.00000861
Iteration 59/1000 | Loss: 0.00000861
Iteration 60/1000 | Loss: 0.00000860
Iteration 61/1000 | Loss: 0.00000859
Iteration 62/1000 | Loss: 0.00000859
Iteration 63/1000 | Loss: 0.00000859
Iteration 64/1000 | Loss: 0.00000858
Iteration 65/1000 | Loss: 0.00000858
Iteration 66/1000 | Loss: 0.00000857
Iteration 67/1000 | Loss: 0.00000857
Iteration 68/1000 | Loss: 0.00000856
Iteration 69/1000 | Loss: 0.00000856
Iteration 70/1000 | Loss: 0.00000855
Iteration 71/1000 | Loss: 0.00000855
Iteration 72/1000 | Loss: 0.00000854
Iteration 73/1000 | Loss: 0.00000854
Iteration 74/1000 | Loss: 0.00000854
Iteration 75/1000 | Loss: 0.00000854
Iteration 76/1000 | Loss: 0.00000854
Iteration 77/1000 | Loss: 0.00000854
Iteration 78/1000 | Loss: 0.00000854
Iteration 79/1000 | Loss: 0.00000854
Iteration 80/1000 | Loss: 0.00000854
Iteration 81/1000 | Loss: 0.00000854
Iteration 82/1000 | Loss: 0.00000854
Iteration 83/1000 | Loss: 0.00000854
Iteration 84/1000 | Loss: 0.00000854
Iteration 85/1000 | Loss: 0.00000854
Iteration 86/1000 | Loss: 0.00000854
Iteration 87/1000 | Loss: 0.00000854
Iteration 88/1000 | Loss: 0.00000854
Iteration 89/1000 | Loss: 0.00000854
Iteration 90/1000 | Loss: 0.00000854
Iteration 91/1000 | Loss: 0.00000854
Iteration 92/1000 | Loss: 0.00000854
Iteration 93/1000 | Loss: 0.00000854
Iteration 94/1000 | Loss: 0.00000854
Iteration 95/1000 | Loss: 0.00000854
Iteration 96/1000 | Loss: 0.00000853
Iteration 97/1000 | Loss: 0.00000853
Iteration 98/1000 | Loss: 0.00000853
Iteration 99/1000 | Loss: 0.00000853
Iteration 100/1000 | Loss: 0.00000853
Iteration 101/1000 | Loss: 0.00000853
Iteration 102/1000 | Loss: 0.00000853
Iteration 103/1000 | Loss: 0.00000853
Iteration 104/1000 | Loss: 0.00000853
Iteration 105/1000 | Loss: 0.00000853
Iteration 106/1000 | Loss: 0.00000853
Iteration 107/1000 | Loss: 0.00000853
Iteration 108/1000 | Loss: 0.00000853
Iteration 109/1000 | Loss: 0.00000853
Iteration 110/1000 | Loss: 0.00000853
Iteration 111/1000 | Loss: 0.00000853
Iteration 112/1000 | Loss: 0.00000853
Iteration 113/1000 | Loss: 0.00000853
Iteration 114/1000 | Loss: 0.00000853
Iteration 115/1000 | Loss: 0.00000853
Iteration 116/1000 | Loss: 0.00000853
Iteration 117/1000 | Loss: 0.00000853
Iteration 118/1000 | Loss: 0.00000853
Iteration 119/1000 | Loss: 0.00000853
Iteration 120/1000 | Loss: 0.00000853
Iteration 121/1000 | Loss: 0.00000853
Iteration 122/1000 | Loss: 0.00000853
Iteration 123/1000 | Loss: 0.00000853
Iteration 124/1000 | Loss: 0.00000853
Iteration 125/1000 | Loss: 0.00000853
Iteration 126/1000 | Loss: 0.00000853
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [8.533500476914924e-06, 8.533500476914924e-06, 8.533500476914924e-06, 8.533500476914924e-06, 8.533500476914924e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.533500476914924e-06

Optimization complete. Final v2v error: 2.486238956451416 mm

Highest mean error: 2.6548173427581787 mm for frame 58

Lowest mean error: 2.359930992126465 mm for frame 195

Saving results

Total time: 34.9562292098999
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00808866
Iteration 2/25 | Loss: 0.00147114
Iteration 3/25 | Loss: 0.00119256
Iteration 4/25 | Loss: 0.00116096
Iteration 5/25 | Loss: 0.00115140
Iteration 6/25 | Loss: 0.00114878
Iteration 7/25 | Loss: 0.00114878
Iteration 8/25 | Loss: 0.00114878
Iteration 9/25 | Loss: 0.00114878
Iteration 10/25 | Loss: 0.00114878
Iteration 11/25 | Loss: 0.00114878
Iteration 12/25 | Loss: 0.00114878
Iteration 13/25 | Loss: 0.00114878
Iteration 14/25 | Loss: 0.00114878
Iteration 15/25 | Loss: 0.00114878
Iteration 16/25 | Loss: 0.00114878
Iteration 17/25 | Loss: 0.00114878
Iteration 18/25 | Loss: 0.00114878
Iteration 19/25 | Loss: 0.00114878
Iteration 20/25 | Loss: 0.00114878
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011487845331430435, 0.0011487845331430435, 0.0011487845331430435, 0.0011487845331430435, 0.0011487845331430435]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011487845331430435

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.17966509
Iteration 2/25 | Loss: 0.00081272
Iteration 3/25 | Loss: 0.00081266
Iteration 4/25 | Loss: 0.00081266
Iteration 5/25 | Loss: 0.00081266
Iteration 6/25 | Loss: 0.00081266
Iteration 7/25 | Loss: 0.00081266
Iteration 8/25 | Loss: 0.00081266
Iteration 9/25 | Loss: 0.00081266
Iteration 10/25 | Loss: 0.00081266
Iteration 11/25 | Loss: 0.00081266
Iteration 12/25 | Loss: 0.00081266
Iteration 13/25 | Loss: 0.00081266
Iteration 14/25 | Loss: 0.00081266
Iteration 15/25 | Loss: 0.00081266
Iteration 16/25 | Loss: 0.00081266
Iteration 17/25 | Loss: 0.00081266
Iteration 18/25 | Loss: 0.00081266
Iteration 19/25 | Loss: 0.00081266
Iteration 20/25 | Loss: 0.00081266
Iteration 21/25 | Loss: 0.00081266
Iteration 22/25 | Loss: 0.00081266
Iteration 23/25 | Loss: 0.00081266
Iteration 24/25 | Loss: 0.00081266
Iteration 25/25 | Loss: 0.00081266

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081266
Iteration 2/1000 | Loss: 0.00005385
Iteration 3/1000 | Loss: 0.00003412
Iteration 4/1000 | Loss: 0.00002832
Iteration 5/1000 | Loss: 0.00002657
Iteration 6/1000 | Loss: 0.00002528
Iteration 7/1000 | Loss: 0.00002440
Iteration 8/1000 | Loss: 0.00002369
Iteration 9/1000 | Loss: 0.00002321
Iteration 10/1000 | Loss: 0.00002281
Iteration 11/1000 | Loss: 0.00002249
Iteration 12/1000 | Loss: 0.00002228
Iteration 13/1000 | Loss: 0.00002212
Iteration 14/1000 | Loss: 0.00002205
Iteration 15/1000 | Loss: 0.00002191
Iteration 16/1000 | Loss: 0.00002187
Iteration 17/1000 | Loss: 0.00002185
Iteration 18/1000 | Loss: 0.00002180
Iteration 19/1000 | Loss: 0.00002180
Iteration 20/1000 | Loss: 0.00002174
Iteration 21/1000 | Loss: 0.00002174
Iteration 22/1000 | Loss: 0.00002172
Iteration 23/1000 | Loss: 0.00002171
Iteration 24/1000 | Loss: 0.00002171
Iteration 25/1000 | Loss: 0.00002170
Iteration 26/1000 | Loss: 0.00002170
Iteration 27/1000 | Loss: 0.00002170
Iteration 28/1000 | Loss: 0.00002169
Iteration 29/1000 | Loss: 0.00002168
Iteration 30/1000 | Loss: 0.00002168
Iteration 31/1000 | Loss: 0.00002167
Iteration 32/1000 | Loss: 0.00002165
Iteration 33/1000 | Loss: 0.00002164
Iteration 34/1000 | Loss: 0.00002164
Iteration 35/1000 | Loss: 0.00002163
Iteration 36/1000 | Loss: 0.00002163
Iteration 37/1000 | Loss: 0.00002162
Iteration 38/1000 | Loss: 0.00002162
Iteration 39/1000 | Loss: 0.00002162
Iteration 40/1000 | Loss: 0.00002162
Iteration 41/1000 | Loss: 0.00002161
Iteration 42/1000 | Loss: 0.00002161
Iteration 43/1000 | Loss: 0.00002160
Iteration 44/1000 | Loss: 0.00002160
Iteration 45/1000 | Loss: 0.00002160
Iteration 46/1000 | Loss: 0.00002160
Iteration 47/1000 | Loss: 0.00002160
Iteration 48/1000 | Loss: 0.00002160
Iteration 49/1000 | Loss: 0.00002160
Iteration 50/1000 | Loss: 0.00002160
Iteration 51/1000 | Loss: 0.00002160
Iteration 52/1000 | Loss: 0.00002160
Iteration 53/1000 | Loss: 0.00002159
Iteration 54/1000 | Loss: 0.00002159
Iteration 55/1000 | Loss: 0.00002159
Iteration 56/1000 | Loss: 0.00002159
Iteration 57/1000 | Loss: 0.00002159
Iteration 58/1000 | Loss: 0.00002158
Iteration 59/1000 | Loss: 0.00002158
Iteration 60/1000 | Loss: 0.00002158
Iteration 61/1000 | Loss: 0.00002158
Iteration 62/1000 | Loss: 0.00002158
Iteration 63/1000 | Loss: 0.00002158
Iteration 64/1000 | Loss: 0.00002158
Iteration 65/1000 | Loss: 0.00002158
Iteration 66/1000 | Loss: 0.00002157
Iteration 67/1000 | Loss: 0.00002157
Iteration 68/1000 | Loss: 0.00002157
Iteration 69/1000 | Loss: 0.00002157
Iteration 70/1000 | Loss: 0.00002157
Iteration 71/1000 | Loss: 0.00002157
Iteration 72/1000 | Loss: 0.00002156
Iteration 73/1000 | Loss: 0.00002156
Iteration 74/1000 | Loss: 0.00002156
Iteration 75/1000 | Loss: 0.00002156
Iteration 76/1000 | Loss: 0.00002155
Iteration 77/1000 | Loss: 0.00002155
Iteration 78/1000 | Loss: 0.00002155
Iteration 79/1000 | Loss: 0.00002155
Iteration 80/1000 | Loss: 0.00002155
Iteration 81/1000 | Loss: 0.00002155
Iteration 82/1000 | Loss: 0.00002155
Iteration 83/1000 | Loss: 0.00002155
Iteration 84/1000 | Loss: 0.00002155
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 84. Stopping optimization.
Last 5 losses: [2.154987487301696e-05, 2.154987487301696e-05, 2.154987487301696e-05, 2.154987487301696e-05, 2.154987487301696e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.154987487301696e-05

Optimization complete. Final v2v error: 3.8166913986206055 mm

Highest mean error: 4.652253150939941 mm for frame 181

Lowest mean error: 2.946622371673584 mm for frame 127

Saving results

Total time: 43.178027868270874
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01018103
Iteration 2/25 | Loss: 0.00174499
Iteration 3/25 | Loss: 0.00133176
Iteration 4/25 | Loss: 0.00106055
Iteration 5/25 | Loss: 0.00116839
Iteration 6/25 | Loss: 0.00112556
Iteration 7/25 | Loss: 0.00092373
Iteration 8/25 | Loss: 0.00081646
Iteration 9/25 | Loss: 0.00079097
Iteration 10/25 | Loss: 0.00076710
Iteration 11/25 | Loss: 0.00075434
Iteration 12/25 | Loss: 0.00083110
Iteration 13/25 | Loss: 0.00075606
Iteration 14/25 | Loss: 0.00073859
Iteration 15/25 | Loss: 0.00072874
Iteration 16/25 | Loss: 0.00073688
Iteration 17/25 | Loss: 0.00073438
Iteration 18/25 | Loss: 0.00073071
Iteration 19/25 | Loss: 0.00073076
Iteration 20/25 | Loss: 0.00072890
Iteration 21/25 | Loss: 0.00072856
Iteration 22/25 | Loss: 0.00073058
Iteration 23/25 | Loss: 0.00072595
Iteration 24/25 | Loss: 0.00071998
Iteration 25/25 | Loss: 0.00072421

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.89848542
Iteration 2/25 | Loss: 0.00075203
Iteration 3/25 | Loss: 0.00075202
Iteration 4/25 | Loss: 0.00075202
Iteration 5/25 | Loss: 0.00075202
Iteration 6/25 | Loss: 0.00075202
Iteration 7/25 | Loss: 0.00075202
Iteration 8/25 | Loss: 0.00075202
Iteration 9/25 | Loss: 0.00075202
Iteration 10/25 | Loss: 0.00075202
Iteration 11/25 | Loss: 0.00075202
Iteration 12/25 | Loss: 0.00075202
Iteration 13/25 | Loss: 0.00075202
Iteration 14/25 | Loss: 0.00075202
Iteration 15/25 | Loss: 0.00075202
Iteration 16/25 | Loss: 0.00075202
Iteration 17/25 | Loss: 0.00075202
Iteration 18/25 | Loss: 0.00075202
Iteration 19/25 | Loss: 0.00075202
Iteration 20/25 | Loss: 0.00075202
Iteration 21/25 | Loss: 0.00075202
Iteration 22/25 | Loss: 0.00075202
Iteration 23/25 | Loss: 0.00075202
Iteration 24/25 | Loss: 0.00075202
Iteration 25/25 | Loss: 0.00075202

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075202
Iteration 2/1000 | Loss: 0.00011140
Iteration 3/1000 | Loss: 0.00029550
Iteration 4/1000 | Loss: 0.00009113
Iteration 5/1000 | Loss: 0.00027736
Iteration 6/1000 | Loss: 0.00021274
Iteration 7/1000 | Loss: 0.00006331
Iteration 8/1000 | Loss: 0.00046649
Iteration 9/1000 | Loss: 0.00006091
Iteration 10/1000 | Loss: 0.00004525
Iteration 11/1000 | Loss: 0.00031540
Iteration 12/1000 | Loss: 0.00004345
Iteration 13/1000 | Loss: 0.00003425
Iteration 14/1000 | Loss: 0.00003030
Iteration 15/1000 | Loss: 0.00002628
Iteration 16/1000 | Loss: 0.00002410
Iteration 17/1000 | Loss: 0.00002270
Iteration 18/1000 | Loss: 0.00002166
Iteration 19/1000 | Loss: 0.00042560
Iteration 20/1000 | Loss: 0.00002479
Iteration 21/1000 | Loss: 0.00002114
Iteration 22/1000 | Loss: 0.00001956
Iteration 23/1000 | Loss: 0.00001830
Iteration 24/1000 | Loss: 0.00001757
Iteration 25/1000 | Loss: 0.00001706
Iteration 26/1000 | Loss: 0.00001683
Iteration 27/1000 | Loss: 0.00001663
Iteration 28/1000 | Loss: 0.00001658
Iteration 29/1000 | Loss: 0.00001658
Iteration 30/1000 | Loss: 0.00001658
Iteration 31/1000 | Loss: 0.00001657
Iteration 32/1000 | Loss: 0.00001657
Iteration 33/1000 | Loss: 0.00001657
Iteration 34/1000 | Loss: 0.00001656
Iteration 35/1000 | Loss: 0.00001656
Iteration 36/1000 | Loss: 0.00001655
Iteration 37/1000 | Loss: 0.00001654
Iteration 38/1000 | Loss: 0.00001653
Iteration 39/1000 | Loss: 0.00001653
Iteration 40/1000 | Loss: 0.00001651
Iteration 41/1000 | Loss: 0.00001651
Iteration 42/1000 | Loss: 0.00001650
Iteration 43/1000 | Loss: 0.00001650
Iteration 44/1000 | Loss: 0.00001650
Iteration 45/1000 | Loss: 0.00001646
Iteration 46/1000 | Loss: 0.00001637
Iteration 47/1000 | Loss: 0.00001627
Iteration 48/1000 | Loss: 0.00001625
Iteration 49/1000 | Loss: 0.00001622
Iteration 50/1000 | Loss: 0.00001888
Iteration 51/1000 | Loss: 0.00001623
Iteration 52/1000 | Loss: 0.00001585
Iteration 53/1000 | Loss: 0.00001564
Iteration 54/1000 | Loss: 0.00001558
Iteration 55/1000 | Loss: 0.00001555
Iteration 56/1000 | Loss: 0.00001546
Iteration 57/1000 | Loss: 0.00001539
Iteration 58/1000 | Loss: 0.00001532
Iteration 59/1000 | Loss: 0.00001530
Iteration 60/1000 | Loss: 0.00001528
Iteration 61/1000 | Loss: 0.00001528
Iteration 62/1000 | Loss: 0.00001528
Iteration 63/1000 | Loss: 0.00001528
Iteration 64/1000 | Loss: 0.00001527
Iteration 65/1000 | Loss: 0.00001527
Iteration 66/1000 | Loss: 0.00001527
Iteration 67/1000 | Loss: 0.00001527
Iteration 68/1000 | Loss: 0.00001527
Iteration 69/1000 | Loss: 0.00001527
Iteration 70/1000 | Loss: 0.00001527
Iteration 71/1000 | Loss: 0.00001527
Iteration 72/1000 | Loss: 0.00001527
Iteration 73/1000 | Loss: 0.00001527
Iteration 74/1000 | Loss: 0.00001527
Iteration 75/1000 | Loss: 0.00001527
Iteration 76/1000 | Loss: 0.00001527
Iteration 77/1000 | Loss: 0.00001527
Iteration 78/1000 | Loss: 0.00001526
Iteration 79/1000 | Loss: 0.00001526
Iteration 80/1000 | Loss: 0.00001525
Iteration 81/1000 | Loss: 0.00001525
Iteration 82/1000 | Loss: 0.00001525
Iteration 83/1000 | Loss: 0.00001525
Iteration 84/1000 | Loss: 0.00001524
Iteration 85/1000 | Loss: 0.00001524
Iteration 86/1000 | Loss: 0.00001523
Iteration 87/1000 | Loss: 0.00001523
Iteration 88/1000 | Loss: 0.00001522
Iteration 89/1000 | Loss: 0.00001522
Iteration 90/1000 | Loss: 0.00001522
Iteration 91/1000 | Loss: 0.00001521
Iteration 92/1000 | Loss: 0.00001521
Iteration 93/1000 | Loss: 0.00001521
Iteration 94/1000 | Loss: 0.00001521
Iteration 95/1000 | Loss: 0.00001521
Iteration 96/1000 | Loss: 0.00001521
Iteration 97/1000 | Loss: 0.00001520
Iteration 98/1000 | Loss: 0.00001520
Iteration 99/1000 | Loss: 0.00001520
Iteration 100/1000 | Loss: 0.00001520
Iteration 101/1000 | Loss: 0.00001520
Iteration 102/1000 | Loss: 0.00001520
Iteration 103/1000 | Loss: 0.00001520
Iteration 104/1000 | Loss: 0.00001520
Iteration 105/1000 | Loss: 0.00001520
Iteration 106/1000 | Loss: 0.00001520
Iteration 107/1000 | Loss: 0.00001520
Iteration 108/1000 | Loss: 0.00001520
Iteration 109/1000 | Loss: 0.00001519
Iteration 110/1000 | Loss: 0.00001519
Iteration 111/1000 | Loss: 0.00001519
Iteration 112/1000 | Loss: 0.00001519
Iteration 113/1000 | Loss: 0.00001519
Iteration 114/1000 | Loss: 0.00001519
Iteration 115/1000 | Loss: 0.00001519
Iteration 116/1000 | Loss: 0.00001519
Iteration 117/1000 | Loss: 0.00001519
Iteration 118/1000 | Loss: 0.00001519
Iteration 119/1000 | Loss: 0.00001519
Iteration 120/1000 | Loss: 0.00001519
Iteration 121/1000 | Loss: 0.00001519
Iteration 122/1000 | Loss: 0.00001518
Iteration 123/1000 | Loss: 0.00001518
Iteration 124/1000 | Loss: 0.00001518
Iteration 125/1000 | Loss: 0.00001518
Iteration 126/1000 | Loss: 0.00001518
Iteration 127/1000 | Loss: 0.00001518
Iteration 128/1000 | Loss: 0.00001518
Iteration 129/1000 | Loss: 0.00001518
Iteration 130/1000 | Loss: 0.00001518
Iteration 131/1000 | Loss: 0.00001518
Iteration 132/1000 | Loss: 0.00001518
Iteration 133/1000 | Loss: 0.00001517
Iteration 134/1000 | Loss: 0.00001517
Iteration 135/1000 | Loss: 0.00001517
Iteration 136/1000 | Loss: 0.00001517
Iteration 137/1000 | Loss: 0.00001517
Iteration 138/1000 | Loss: 0.00001517
Iteration 139/1000 | Loss: 0.00001516
Iteration 140/1000 | Loss: 0.00001516
Iteration 141/1000 | Loss: 0.00001516
Iteration 142/1000 | Loss: 0.00001516
Iteration 143/1000 | Loss: 0.00001515
Iteration 144/1000 | Loss: 0.00001515
Iteration 145/1000 | Loss: 0.00001515
Iteration 146/1000 | Loss: 0.00001515
Iteration 147/1000 | Loss: 0.00001515
Iteration 148/1000 | Loss: 0.00001515
Iteration 149/1000 | Loss: 0.00001514
Iteration 150/1000 | Loss: 0.00001514
Iteration 151/1000 | Loss: 0.00001514
Iteration 152/1000 | Loss: 0.00001513
Iteration 153/1000 | Loss: 0.00001513
Iteration 154/1000 | Loss: 0.00001513
Iteration 155/1000 | Loss: 0.00001513
Iteration 156/1000 | Loss: 0.00001513
Iteration 157/1000 | Loss: 0.00001512
Iteration 158/1000 | Loss: 0.00001512
Iteration 159/1000 | Loss: 0.00001512
Iteration 160/1000 | Loss: 0.00001512
Iteration 161/1000 | Loss: 0.00001511
Iteration 162/1000 | Loss: 0.00001511
Iteration 163/1000 | Loss: 0.00001511
Iteration 164/1000 | Loss: 0.00001511
Iteration 165/1000 | Loss: 0.00001511
Iteration 166/1000 | Loss: 0.00001511
Iteration 167/1000 | Loss: 0.00001511
Iteration 168/1000 | Loss: 0.00001511
Iteration 169/1000 | Loss: 0.00001511
Iteration 170/1000 | Loss: 0.00001511
Iteration 171/1000 | Loss: 0.00001511
Iteration 172/1000 | Loss: 0.00001511
Iteration 173/1000 | Loss: 0.00001511
Iteration 174/1000 | Loss: 0.00001511
Iteration 175/1000 | Loss: 0.00001511
Iteration 176/1000 | Loss: 0.00001510
Iteration 177/1000 | Loss: 0.00001510
Iteration 178/1000 | Loss: 0.00001510
Iteration 179/1000 | Loss: 0.00001510
Iteration 180/1000 | Loss: 0.00001510
Iteration 181/1000 | Loss: 0.00001510
Iteration 182/1000 | Loss: 0.00001510
Iteration 183/1000 | Loss: 0.00001510
Iteration 184/1000 | Loss: 0.00001510
Iteration 185/1000 | Loss: 0.00001510
Iteration 186/1000 | Loss: 0.00001510
Iteration 187/1000 | Loss: 0.00001509
Iteration 188/1000 | Loss: 0.00001509
Iteration 189/1000 | Loss: 0.00001509
Iteration 190/1000 | Loss: 0.00001509
Iteration 191/1000 | Loss: 0.00001509
Iteration 192/1000 | Loss: 0.00001509
Iteration 193/1000 | Loss: 0.00001509
Iteration 194/1000 | Loss: 0.00001509
Iteration 195/1000 | Loss: 0.00001509
Iteration 196/1000 | Loss: 0.00001509
Iteration 197/1000 | Loss: 0.00001509
Iteration 198/1000 | Loss: 0.00001509
Iteration 199/1000 | Loss: 0.00001509
Iteration 200/1000 | Loss: 0.00001509
Iteration 201/1000 | Loss: 0.00001509
Iteration 202/1000 | Loss: 0.00001509
Iteration 203/1000 | Loss: 0.00001509
Iteration 204/1000 | Loss: 0.00001509
Iteration 205/1000 | Loss: 0.00001509
Iteration 206/1000 | Loss: 0.00001509
Iteration 207/1000 | Loss: 0.00001509
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [1.5088964573806152e-05, 1.5088964573806152e-05, 1.5088964573806152e-05, 1.5088964573806152e-05, 1.5088964573806152e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5088964573806152e-05

Optimization complete. Final v2v error: 3.2685933113098145 mm

Highest mean error: 4.035370826721191 mm for frame 86

Lowest mean error: 2.827446937561035 mm for frame 2

Saving results

Total time: 108.7531681060791
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01259933
Iteration 2/25 | Loss: 0.00387623
Iteration 3/25 | Loss: 0.00262064
Iteration 4/25 | Loss: 0.00228371
Iteration 5/25 | Loss: 0.00190551
Iteration 6/25 | Loss: 0.00172395
Iteration 7/25 | Loss: 0.00151970
Iteration 8/25 | Loss: 0.00144687
Iteration 9/25 | Loss: 0.00135710
Iteration 10/25 | Loss: 0.00134384
Iteration 11/25 | Loss: 0.00134145
Iteration 12/25 | Loss: 0.00131743
Iteration 13/25 | Loss: 0.00131785
Iteration 14/25 | Loss: 0.00131267
Iteration 15/25 | Loss: 0.00130074
Iteration 16/25 | Loss: 0.00128739
Iteration 17/25 | Loss: 0.00129429
Iteration 18/25 | Loss: 0.00129283
Iteration 19/25 | Loss: 0.00128495
Iteration 20/25 | Loss: 0.00128041
Iteration 21/25 | Loss: 0.00128402
Iteration 22/25 | Loss: 0.00127883
Iteration 23/25 | Loss: 0.00128249
Iteration 24/25 | Loss: 0.00128119
Iteration 25/25 | Loss: 0.00128215

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.59872121
Iteration 2/25 | Loss: 0.00417652
Iteration 3/25 | Loss: 0.00417652
Iteration 4/25 | Loss: 0.00417651
Iteration 5/25 | Loss: 0.00417651
Iteration 6/25 | Loss: 0.00417651
Iteration 7/25 | Loss: 0.00417651
Iteration 8/25 | Loss: 0.00417651
Iteration 9/25 | Loss: 0.00417651
Iteration 10/25 | Loss: 0.00417651
Iteration 11/25 | Loss: 0.00417651
Iteration 12/25 | Loss: 0.00417651
Iteration 13/25 | Loss: 0.00417651
Iteration 14/25 | Loss: 0.00417651
Iteration 15/25 | Loss: 0.00417651
Iteration 16/25 | Loss: 0.00417651
Iteration 17/25 | Loss: 0.00417651
Iteration 18/25 | Loss: 0.00417651
Iteration 19/25 | Loss: 0.00417651
Iteration 20/25 | Loss: 0.00417651
Iteration 21/25 | Loss: 0.00417651
Iteration 22/25 | Loss: 0.00417651
Iteration 23/25 | Loss: 0.00417651
Iteration 24/25 | Loss: 0.00417651
Iteration 25/25 | Loss: 0.00417651

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00417651
Iteration 2/1000 | Loss: 0.00141682
Iteration 3/1000 | Loss: 0.00163183
Iteration 4/1000 | Loss: 0.00192789
Iteration 5/1000 | Loss: 0.00159259
Iteration 6/1000 | Loss: 0.00073760
Iteration 7/1000 | Loss: 0.00341431
Iteration 8/1000 | Loss: 0.00306764
Iteration 9/1000 | Loss: 0.00125619
Iteration 10/1000 | Loss: 0.00338449
Iteration 11/1000 | Loss: 0.00117041
Iteration 12/1000 | Loss: 0.00287351
Iteration 13/1000 | Loss: 0.00255076
Iteration 14/1000 | Loss: 0.00104338
Iteration 15/1000 | Loss: 0.00082332
Iteration 16/1000 | Loss: 0.00077202
Iteration 17/1000 | Loss: 0.00158083
Iteration 18/1000 | Loss: 0.00600337
Iteration 19/1000 | Loss: 0.00193003
Iteration 20/1000 | Loss: 0.00124303
Iteration 21/1000 | Loss: 0.00134679
Iteration 22/1000 | Loss: 0.00076977
Iteration 23/1000 | Loss: 0.00165308
Iteration 24/1000 | Loss: 0.00257259
Iteration 25/1000 | Loss: 0.00179141
Iteration 26/1000 | Loss: 0.00158921
Iteration 27/1000 | Loss: 0.00134627
Iteration 28/1000 | Loss: 0.00125442
Iteration 29/1000 | Loss: 0.00284332
Iteration 30/1000 | Loss: 0.00138617
Iteration 31/1000 | Loss: 0.00051368
Iteration 32/1000 | Loss: 0.00103193
Iteration 33/1000 | Loss: 0.00072961
Iteration 34/1000 | Loss: 0.00083601
Iteration 35/1000 | Loss: 0.00037145
Iteration 36/1000 | Loss: 0.00055055
Iteration 37/1000 | Loss: 0.00063992
Iteration 38/1000 | Loss: 0.00186177
Iteration 39/1000 | Loss: 0.00153219
Iteration 40/1000 | Loss: 0.00086462
Iteration 41/1000 | Loss: 0.00126146
Iteration 42/1000 | Loss: 0.00239934
Iteration 43/1000 | Loss: 0.00098104
Iteration 44/1000 | Loss: 0.00064876
Iteration 45/1000 | Loss: 0.00061826
Iteration 46/1000 | Loss: 0.00054941
Iteration 47/1000 | Loss: 0.00050079
Iteration 48/1000 | Loss: 0.00049430
Iteration 49/1000 | Loss: 0.00061893
Iteration 50/1000 | Loss: 0.00052565
Iteration 51/1000 | Loss: 0.00046646
Iteration 52/1000 | Loss: 0.00137530
Iteration 53/1000 | Loss: 0.00060218
Iteration 54/1000 | Loss: 0.00068057
Iteration 55/1000 | Loss: 0.00027846
Iteration 56/1000 | Loss: 0.00098750
Iteration 57/1000 | Loss: 0.00043340
Iteration 58/1000 | Loss: 0.00033711
Iteration 59/1000 | Loss: 0.00027135
Iteration 60/1000 | Loss: 0.00161390
Iteration 61/1000 | Loss: 0.00038920
Iteration 62/1000 | Loss: 0.00041274
Iteration 63/1000 | Loss: 0.00040043
Iteration 64/1000 | Loss: 0.00037064
Iteration 65/1000 | Loss: 0.00038514
Iteration 66/1000 | Loss: 0.00030867
Iteration 67/1000 | Loss: 0.00017645
Iteration 68/1000 | Loss: 0.00028162
Iteration 69/1000 | Loss: 0.00027511
Iteration 70/1000 | Loss: 0.00023353
Iteration 71/1000 | Loss: 0.00028482
Iteration 72/1000 | Loss: 0.00040486
Iteration 73/1000 | Loss: 0.00040452
Iteration 74/1000 | Loss: 0.00049739
Iteration 75/1000 | Loss: 0.00034611
Iteration 76/1000 | Loss: 0.00033376
Iteration 77/1000 | Loss: 0.00023823
Iteration 78/1000 | Loss: 0.00015561
Iteration 79/1000 | Loss: 0.00020694
Iteration 80/1000 | Loss: 0.00029980
Iteration 81/1000 | Loss: 0.00093859
Iteration 82/1000 | Loss: 0.00047610
Iteration 83/1000 | Loss: 0.00040212
Iteration 84/1000 | Loss: 0.00014309
Iteration 85/1000 | Loss: 0.00012282
Iteration 86/1000 | Loss: 0.00011859
Iteration 87/1000 | Loss: 0.00011573
Iteration 88/1000 | Loss: 0.00011262
Iteration 89/1000 | Loss: 0.00011058
Iteration 90/1000 | Loss: 0.00136610
Iteration 91/1000 | Loss: 0.00013089
Iteration 92/1000 | Loss: 0.00011555
Iteration 93/1000 | Loss: 0.00010724
Iteration 94/1000 | Loss: 0.00010463
Iteration 95/1000 | Loss: 0.00010347
Iteration 96/1000 | Loss: 0.00147101
Iteration 97/1000 | Loss: 0.00012023
Iteration 98/1000 | Loss: 0.00010747
Iteration 99/1000 | Loss: 0.00063244
Iteration 100/1000 | Loss: 0.00010654
Iteration 101/1000 | Loss: 0.00078156
Iteration 102/1000 | Loss: 0.00010456
Iteration 103/1000 | Loss: 0.00181401
Iteration 104/1000 | Loss: 0.00201799
Iteration 105/1000 | Loss: 0.00264914
Iteration 106/1000 | Loss: 0.00160666
Iteration 107/1000 | Loss: 0.00041603
Iteration 108/1000 | Loss: 0.00145518
Iteration 109/1000 | Loss: 0.00014047
Iteration 110/1000 | Loss: 0.00015274
Iteration 111/1000 | Loss: 0.00010032
Iteration 112/1000 | Loss: 0.00009622
Iteration 113/1000 | Loss: 0.00084210
Iteration 114/1000 | Loss: 0.00009543
Iteration 115/1000 | Loss: 0.00009057
Iteration 116/1000 | Loss: 0.00008890
Iteration 117/1000 | Loss: 0.00008767
Iteration 118/1000 | Loss: 0.00081658
Iteration 119/1000 | Loss: 0.00034345
Iteration 120/1000 | Loss: 0.00083905
Iteration 121/1000 | Loss: 0.00055897
Iteration 122/1000 | Loss: 0.00077416
Iteration 123/1000 | Loss: 0.00009974
Iteration 124/1000 | Loss: 0.00008804
Iteration 125/1000 | Loss: 0.00079886
Iteration 126/1000 | Loss: 0.00008626
Iteration 127/1000 | Loss: 0.00139961
Iteration 128/1000 | Loss: 0.00008490
Iteration 129/1000 | Loss: 0.00007915
Iteration 130/1000 | Loss: 0.00007562
Iteration 131/1000 | Loss: 0.00007183
Iteration 132/1000 | Loss: 0.00081553
Iteration 133/1000 | Loss: 0.00007255
Iteration 134/1000 | Loss: 0.00007034
Iteration 135/1000 | Loss: 0.00006872
Iteration 136/1000 | Loss: 0.00006716
Iteration 137/1000 | Loss: 0.00006612
Iteration 138/1000 | Loss: 0.00006558
Iteration 139/1000 | Loss: 0.00080878
Iteration 140/1000 | Loss: 0.00006960
Iteration 141/1000 | Loss: 0.00006631
Iteration 142/1000 | Loss: 0.00006474
Iteration 143/1000 | Loss: 0.00057220
Iteration 144/1000 | Loss: 0.00006707
Iteration 145/1000 | Loss: 0.00006370
Iteration 146/1000 | Loss: 0.00006205
Iteration 147/1000 | Loss: 0.00079471
Iteration 148/1000 | Loss: 0.00006371
Iteration 149/1000 | Loss: 0.00055937
Iteration 150/1000 | Loss: 0.00078151
Iteration 151/1000 | Loss: 0.00006284
Iteration 152/1000 | Loss: 0.00005855
Iteration 153/1000 | Loss: 0.00005549
Iteration 154/1000 | Loss: 0.00005307
Iteration 155/1000 | Loss: 0.00005179
Iteration 156/1000 | Loss: 0.00005123
Iteration 157/1000 | Loss: 0.00005084
Iteration 158/1000 | Loss: 0.00005058
Iteration 159/1000 | Loss: 0.00005034
Iteration 160/1000 | Loss: 0.00005019
Iteration 161/1000 | Loss: 0.00005013
Iteration 162/1000 | Loss: 0.00005010
Iteration 163/1000 | Loss: 0.00005004
Iteration 164/1000 | Loss: 0.00005004
Iteration 165/1000 | Loss: 0.00005001
Iteration 166/1000 | Loss: 0.00005001
Iteration 167/1000 | Loss: 0.00005001
Iteration 168/1000 | Loss: 0.00005001
Iteration 169/1000 | Loss: 0.00005001
Iteration 170/1000 | Loss: 0.00005000
Iteration 171/1000 | Loss: 0.00142684
Iteration 172/1000 | Loss: 0.00251838
Iteration 173/1000 | Loss: 0.00007849
Iteration 174/1000 | Loss: 0.00005247
Iteration 175/1000 | Loss: 0.00004546
Iteration 176/1000 | Loss: 0.00004136
Iteration 177/1000 | Loss: 0.00003792
Iteration 178/1000 | Loss: 0.00003597
Iteration 179/1000 | Loss: 0.00003491
Iteration 180/1000 | Loss: 0.00003431
Iteration 181/1000 | Loss: 0.00003378
Iteration 182/1000 | Loss: 0.00003340
Iteration 183/1000 | Loss: 0.00003321
Iteration 184/1000 | Loss: 0.00003305
Iteration 185/1000 | Loss: 0.00003288
Iteration 186/1000 | Loss: 0.00003288
Iteration 187/1000 | Loss: 0.00003288
Iteration 188/1000 | Loss: 0.00003287
Iteration 189/1000 | Loss: 0.00003286
Iteration 190/1000 | Loss: 0.00003286
Iteration 191/1000 | Loss: 0.00003286
Iteration 192/1000 | Loss: 0.00003286
Iteration 193/1000 | Loss: 0.00003286
Iteration 194/1000 | Loss: 0.00003285
Iteration 195/1000 | Loss: 0.00003285
Iteration 196/1000 | Loss: 0.00003285
Iteration 197/1000 | Loss: 0.00003285
Iteration 198/1000 | Loss: 0.00003285
Iteration 199/1000 | Loss: 0.00003285
Iteration 200/1000 | Loss: 0.00003284
Iteration 201/1000 | Loss: 0.00003284
Iteration 202/1000 | Loss: 0.00003284
Iteration 203/1000 | Loss: 0.00003284
Iteration 204/1000 | Loss: 0.00003283
Iteration 205/1000 | Loss: 0.00003281
Iteration 206/1000 | Loss: 0.00003278
Iteration 207/1000 | Loss: 0.00003275
Iteration 208/1000 | Loss: 0.00003275
Iteration 209/1000 | Loss: 0.00003273
Iteration 210/1000 | Loss: 0.00003272
Iteration 211/1000 | Loss: 0.00003272
Iteration 212/1000 | Loss: 0.00003271
Iteration 213/1000 | Loss: 0.00003271
Iteration 214/1000 | Loss: 0.00003271
Iteration 215/1000 | Loss: 0.00003270
Iteration 216/1000 | Loss: 0.00003270
Iteration 217/1000 | Loss: 0.00003270
Iteration 218/1000 | Loss: 0.00003270
Iteration 219/1000 | Loss: 0.00003270
Iteration 220/1000 | Loss: 0.00003268
Iteration 221/1000 | Loss: 0.00003268
Iteration 222/1000 | Loss: 0.00003268
Iteration 223/1000 | Loss: 0.00003268
Iteration 224/1000 | Loss: 0.00003268
Iteration 225/1000 | Loss: 0.00003268
Iteration 226/1000 | Loss: 0.00003268
Iteration 227/1000 | Loss: 0.00003268
Iteration 228/1000 | Loss: 0.00003265
Iteration 229/1000 | Loss: 0.00003265
Iteration 230/1000 | Loss: 0.00003265
Iteration 231/1000 | Loss: 0.00003265
Iteration 232/1000 | Loss: 0.00003265
Iteration 233/1000 | Loss: 0.00003265
Iteration 234/1000 | Loss: 0.00003265
Iteration 235/1000 | Loss: 0.00003265
Iteration 236/1000 | Loss: 0.00003264
Iteration 237/1000 | Loss: 0.00003264
Iteration 238/1000 | Loss: 0.00003263
Iteration 239/1000 | Loss: 0.00003263
Iteration 240/1000 | Loss: 0.00003261
Iteration 241/1000 | Loss: 0.00003260
Iteration 242/1000 | Loss: 0.00003260
Iteration 243/1000 | Loss: 0.00003260
Iteration 244/1000 | Loss: 0.00003260
Iteration 245/1000 | Loss: 0.00003260
Iteration 246/1000 | Loss: 0.00003260
Iteration 247/1000 | Loss: 0.00003260
Iteration 248/1000 | Loss: 0.00003260
Iteration 249/1000 | Loss: 0.00003260
Iteration 250/1000 | Loss: 0.00003259
Iteration 251/1000 | Loss: 0.00003259
Iteration 252/1000 | Loss: 0.00003259
Iteration 253/1000 | Loss: 0.00003259
Iteration 254/1000 | Loss: 0.00003259
Iteration 255/1000 | Loss: 0.00003259
Iteration 256/1000 | Loss: 0.00003258
Iteration 257/1000 | Loss: 0.00003258
Iteration 258/1000 | Loss: 0.00003258
Iteration 259/1000 | Loss: 0.00003257
Iteration 260/1000 | Loss: 0.00003257
Iteration 261/1000 | Loss: 0.00003257
Iteration 262/1000 | Loss: 0.00003256
Iteration 263/1000 | Loss: 0.00003256
Iteration 264/1000 | Loss: 0.00003256
Iteration 265/1000 | Loss: 0.00003256
Iteration 266/1000 | Loss: 0.00003256
Iteration 267/1000 | Loss: 0.00003256
Iteration 268/1000 | Loss: 0.00003255
Iteration 269/1000 | Loss: 0.00003255
Iteration 270/1000 | Loss: 0.00003255
Iteration 271/1000 | Loss: 0.00003255
Iteration 272/1000 | Loss: 0.00003255
Iteration 273/1000 | Loss: 0.00003255
Iteration 274/1000 | Loss: 0.00003255
Iteration 275/1000 | Loss: 0.00003255
Iteration 276/1000 | Loss: 0.00003255
Iteration 277/1000 | Loss: 0.00003254
Iteration 278/1000 | Loss: 0.00003254
Iteration 279/1000 | Loss: 0.00003254
Iteration 280/1000 | Loss: 0.00003254
Iteration 281/1000 | Loss: 0.00003253
Iteration 282/1000 | Loss: 0.00003253
Iteration 283/1000 | Loss: 0.00003253
Iteration 284/1000 | Loss: 0.00003252
Iteration 285/1000 | Loss: 0.00003252
Iteration 286/1000 | Loss: 0.00003252
Iteration 287/1000 | Loss: 0.00003251
Iteration 288/1000 | Loss: 0.00003251
Iteration 289/1000 | Loss: 0.00003251
Iteration 290/1000 | Loss: 0.00003250
Iteration 291/1000 | Loss: 0.00003250
Iteration 292/1000 | Loss: 0.00003249
Iteration 293/1000 | Loss: 0.00003249
Iteration 294/1000 | Loss: 0.00003249
Iteration 295/1000 | Loss: 0.00003248
Iteration 296/1000 | Loss: 0.00003248
Iteration 297/1000 | Loss: 0.00003248
Iteration 298/1000 | Loss: 0.00003248
Iteration 299/1000 | Loss: 0.00003248
Iteration 300/1000 | Loss: 0.00003248
Iteration 301/1000 | Loss: 0.00003247
Iteration 302/1000 | Loss: 0.00003247
Iteration 303/1000 | Loss: 0.00003247
Iteration 304/1000 | Loss: 0.00003247
Iteration 305/1000 | Loss: 0.00003247
Iteration 306/1000 | Loss: 0.00003246
Iteration 307/1000 | Loss: 0.00003246
Iteration 308/1000 | Loss: 0.00003246
Iteration 309/1000 | Loss: 0.00003246
Iteration 310/1000 | Loss: 0.00003246
Iteration 311/1000 | Loss: 0.00003246
Iteration 312/1000 | Loss: 0.00003246
Iteration 313/1000 | Loss: 0.00003246
Iteration 314/1000 | Loss: 0.00003246
Iteration 315/1000 | Loss: 0.00003246
Iteration 316/1000 | Loss: 0.00003246
Iteration 317/1000 | Loss: 0.00003245
Iteration 318/1000 | Loss: 0.00003245
Iteration 319/1000 | Loss: 0.00003245
Iteration 320/1000 | Loss: 0.00003245
Iteration 321/1000 | Loss: 0.00003245
Iteration 322/1000 | Loss: 0.00003245
Iteration 323/1000 | Loss: 0.00003245
Iteration 324/1000 | Loss: 0.00003245
Iteration 325/1000 | Loss: 0.00003245
Iteration 326/1000 | Loss: 0.00003245
Iteration 327/1000 | Loss: 0.00003245
Iteration 328/1000 | Loss: 0.00003244
Iteration 329/1000 | Loss: 0.00003244
Iteration 330/1000 | Loss: 0.00003244
Iteration 331/1000 | Loss: 0.00003244
Iteration 332/1000 | Loss: 0.00003244
Iteration 333/1000 | Loss: 0.00003244
Iteration 334/1000 | Loss: 0.00003243
Iteration 335/1000 | Loss: 0.00003243
Iteration 336/1000 | Loss: 0.00003243
Iteration 337/1000 | Loss: 0.00003243
Iteration 338/1000 | Loss: 0.00003243
Iteration 339/1000 | Loss: 0.00003243
Iteration 340/1000 | Loss: 0.00003243
Iteration 341/1000 | Loss: 0.00003242
Iteration 342/1000 | Loss: 0.00003242
Iteration 343/1000 | Loss: 0.00003242
Iteration 344/1000 | Loss: 0.00003242
Iteration 345/1000 | Loss: 0.00003241
Iteration 346/1000 | Loss: 0.00003241
Iteration 347/1000 | Loss: 0.00003241
Iteration 348/1000 | Loss: 0.00003241
Iteration 349/1000 | Loss: 0.00003241
Iteration 350/1000 | Loss: 0.00003241
Iteration 351/1000 | Loss: 0.00003241
Iteration 352/1000 | Loss: 0.00003241
Iteration 353/1000 | Loss: 0.00003241
Iteration 354/1000 | Loss: 0.00003241
Iteration 355/1000 | Loss: 0.00003241
Iteration 356/1000 | Loss: 0.00003241
Iteration 357/1000 | Loss: 0.00003241
Iteration 358/1000 | Loss: 0.00003241
Iteration 359/1000 | Loss: 0.00003241
Iteration 360/1000 | Loss: 0.00003241
Iteration 361/1000 | Loss: 0.00003241
Iteration 362/1000 | Loss: 0.00003241
Iteration 363/1000 | Loss: 0.00003241
Iteration 364/1000 | Loss: 0.00003241
Iteration 365/1000 | Loss: 0.00003241
Iteration 366/1000 | Loss: 0.00003241
Iteration 367/1000 | Loss: 0.00003241
Iteration 368/1000 | Loss: 0.00003241
Iteration 369/1000 | Loss: 0.00003241
Iteration 370/1000 | Loss: 0.00003241
Iteration 371/1000 | Loss: 0.00003241
Iteration 372/1000 | Loss: 0.00003241
Iteration 373/1000 | Loss: 0.00003241
Iteration 374/1000 | Loss: 0.00003241
Iteration 375/1000 | Loss: 0.00003241
Iteration 376/1000 | Loss: 0.00003241
Iteration 377/1000 | Loss: 0.00003241
Iteration 378/1000 | Loss: 0.00003241
Iteration 379/1000 | Loss: 0.00003241
Iteration 380/1000 | Loss: 0.00003241
Iteration 381/1000 | Loss: 0.00003241
Iteration 382/1000 | Loss: 0.00003241
Iteration 383/1000 | Loss: 0.00003241
Iteration 384/1000 | Loss: 0.00003241
Iteration 385/1000 | Loss: 0.00003241
Iteration 386/1000 | Loss: 0.00003241
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 386. Stopping optimization.
Last 5 losses: [3.240936348447576e-05, 3.240936348447576e-05, 3.240936348447576e-05, 3.240936348447576e-05, 3.240936348447576e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.240936348447576e-05

Optimization complete. Final v2v error: 4.5000715255737305 mm

Highest mean error: 5.16099214553833 mm for frame 143

Lowest mean error: 3.9199135303497314 mm for frame 65

Saving results

Total time: 319.46716594696045
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01071682
Iteration 2/25 | Loss: 0.00289652
Iteration 3/25 | Loss: 0.00171870
Iteration 4/25 | Loss: 0.00148153
Iteration 5/25 | Loss: 0.00136453
Iteration 6/25 | Loss: 0.00123823
Iteration 7/25 | Loss: 0.00119892
Iteration 8/25 | Loss: 0.00111504
Iteration 9/25 | Loss: 0.00107881
Iteration 10/25 | Loss: 0.00105693
Iteration 11/25 | Loss: 0.00102548
Iteration 12/25 | Loss: 0.00101871
Iteration 13/25 | Loss: 0.00101476
Iteration 14/25 | Loss: 0.00100592
Iteration 15/25 | Loss: 0.00099520
Iteration 16/25 | Loss: 0.00098402
Iteration 17/25 | Loss: 0.00097977
Iteration 18/25 | Loss: 0.00097865
Iteration 19/25 | Loss: 0.00098622
Iteration 20/25 | Loss: 0.00098370
Iteration 21/25 | Loss: 0.00097665
Iteration 22/25 | Loss: 0.00097219
Iteration 23/25 | Loss: 0.00097085
Iteration 24/25 | Loss: 0.00097056
Iteration 25/25 | Loss: 0.00097037

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.91695881
Iteration 2/25 | Loss: 0.00247292
Iteration 3/25 | Loss: 0.00247292
Iteration 4/25 | Loss: 0.00247292
Iteration 5/25 | Loss: 0.00247292
Iteration 6/25 | Loss: 0.00247292
Iteration 7/25 | Loss: 0.00247292
Iteration 8/25 | Loss: 0.00247292
Iteration 9/25 | Loss: 0.00247292
Iteration 10/25 | Loss: 0.00247292
Iteration 11/25 | Loss: 0.00247292
Iteration 12/25 | Loss: 0.00247292
Iteration 13/25 | Loss: 0.00247292
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0024729175493121147, 0.0024729175493121147, 0.0024729175493121147, 0.0024729175493121147, 0.0024729175493121147]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024729175493121147

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00247292
Iteration 2/1000 | Loss: 0.00058412
Iteration 3/1000 | Loss: 0.00124190
Iteration 4/1000 | Loss: 0.00084559
Iteration 5/1000 | Loss: 0.00050373
Iteration 6/1000 | Loss: 0.00028889
Iteration 7/1000 | Loss: 0.00065768
Iteration 8/1000 | Loss: 0.00076749
Iteration 9/1000 | Loss: 0.00026271
Iteration 10/1000 | Loss: 0.00018976
Iteration 11/1000 | Loss: 0.00061728
Iteration 12/1000 | Loss: 0.00017725
Iteration 13/1000 | Loss: 0.00015744
Iteration 14/1000 | Loss: 0.00025071
Iteration 15/1000 | Loss: 0.00015666
Iteration 16/1000 | Loss: 0.00014567
Iteration 17/1000 | Loss: 0.00014137
Iteration 18/1000 | Loss: 0.00292437
Iteration 19/1000 | Loss: 0.00050093
Iteration 20/1000 | Loss: 0.00022276
Iteration 21/1000 | Loss: 0.00014716
Iteration 22/1000 | Loss: 0.00208615
Iteration 23/1000 | Loss: 0.00036652
Iteration 24/1000 | Loss: 0.00081244
Iteration 25/1000 | Loss: 0.00207116
Iteration 26/1000 | Loss: 0.00041223
Iteration 27/1000 | Loss: 0.00083761
Iteration 28/1000 | Loss: 0.00048179
Iteration 29/1000 | Loss: 0.00076654
Iteration 30/1000 | Loss: 0.00045917
Iteration 31/1000 | Loss: 0.00063078
Iteration 32/1000 | Loss: 0.00034027
Iteration 33/1000 | Loss: 0.00054893
Iteration 34/1000 | Loss: 0.00020092
Iteration 35/1000 | Loss: 0.00064482
Iteration 36/1000 | Loss: 0.00042735
Iteration 37/1000 | Loss: 0.00056964
Iteration 38/1000 | Loss: 0.00067203
Iteration 39/1000 | Loss: 0.00055291
Iteration 40/1000 | Loss: 0.00066848
Iteration 41/1000 | Loss: 0.00166375
Iteration 42/1000 | Loss: 0.00019074
Iteration 43/1000 | Loss: 0.00029193
Iteration 44/1000 | Loss: 0.00064200
Iteration 45/1000 | Loss: 0.00023148
Iteration 46/1000 | Loss: 0.00019617
Iteration 47/1000 | Loss: 0.00015512
Iteration 48/1000 | Loss: 0.00023994
Iteration 49/1000 | Loss: 0.00021007
Iteration 50/1000 | Loss: 0.00015033
Iteration 51/1000 | Loss: 0.00025330
Iteration 52/1000 | Loss: 0.00014915
Iteration 53/1000 | Loss: 0.00023361
Iteration 54/1000 | Loss: 0.00027179
Iteration 55/1000 | Loss: 0.00034186
Iteration 56/1000 | Loss: 0.00040588
Iteration 57/1000 | Loss: 0.00019084
Iteration 58/1000 | Loss: 0.00052976
Iteration 59/1000 | Loss: 0.00045478
Iteration 60/1000 | Loss: 0.00053598
Iteration 61/1000 | Loss: 0.00058952
Iteration 62/1000 | Loss: 0.00051179
Iteration 63/1000 | Loss: 0.00035000
Iteration 64/1000 | Loss: 0.00034571
Iteration 65/1000 | Loss: 0.00018945
Iteration 66/1000 | Loss: 0.00017154
Iteration 67/1000 | Loss: 0.00034643
Iteration 68/1000 | Loss: 0.00014633
Iteration 69/1000 | Loss: 0.00065031
Iteration 70/1000 | Loss: 0.00018749
Iteration 71/1000 | Loss: 0.00055515
Iteration 72/1000 | Loss: 0.00052540
Iteration 73/1000 | Loss: 0.00076532
Iteration 74/1000 | Loss: 0.00036398
Iteration 75/1000 | Loss: 0.00064697
Iteration 76/1000 | Loss: 0.00012914
Iteration 77/1000 | Loss: 0.00010608
Iteration 78/1000 | Loss: 0.00009781
Iteration 79/1000 | Loss: 0.00009585
Iteration 80/1000 | Loss: 0.00009852
Iteration 81/1000 | Loss: 0.00053376
Iteration 82/1000 | Loss: 0.00045381
Iteration 83/1000 | Loss: 0.00009899
Iteration 84/1000 | Loss: 0.00009421
Iteration 85/1000 | Loss: 0.00009016
Iteration 86/1000 | Loss: 0.00008861
Iteration 87/1000 | Loss: 0.00008787
Iteration 88/1000 | Loss: 0.00014544
Iteration 89/1000 | Loss: 0.00008622
Iteration 90/1000 | Loss: 0.00008571
Iteration 91/1000 | Loss: 0.00008535
Iteration 92/1000 | Loss: 0.00008492
Iteration 93/1000 | Loss: 0.00008464
Iteration 94/1000 | Loss: 0.00008442
Iteration 95/1000 | Loss: 0.00008402
Iteration 96/1000 | Loss: 0.00045481
Iteration 97/1000 | Loss: 0.00019266
Iteration 98/1000 | Loss: 0.00008459
Iteration 99/1000 | Loss: 0.00049094
Iteration 100/1000 | Loss: 0.00011299
Iteration 101/1000 | Loss: 0.00009276
Iteration 102/1000 | Loss: 0.00008906
Iteration 103/1000 | Loss: 0.00008708
Iteration 104/1000 | Loss: 0.00019920
Iteration 105/1000 | Loss: 0.00008333
Iteration 106/1000 | Loss: 0.00008235
Iteration 107/1000 | Loss: 0.00008135
Iteration 108/1000 | Loss: 0.00008080
Iteration 109/1000 | Loss: 0.00008032
Iteration 110/1000 | Loss: 0.00008012
Iteration 111/1000 | Loss: 0.00008007
Iteration 112/1000 | Loss: 0.00008006
Iteration 113/1000 | Loss: 0.00008005
Iteration 114/1000 | Loss: 0.00008004
Iteration 115/1000 | Loss: 0.00008004
Iteration 116/1000 | Loss: 0.00008002
Iteration 117/1000 | Loss: 0.00008002
Iteration 118/1000 | Loss: 0.00008002
Iteration 119/1000 | Loss: 0.00008002
Iteration 120/1000 | Loss: 0.00008002
Iteration 121/1000 | Loss: 0.00008002
Iteration 122/1000 | Loss: 0.00008002
Iteration 123/1000 | Loss: 0.00008002
Iteration 124/1000 | Loss: 0.00008002
Iteration 125/1000 | Loss: 0.00008001
Iteration 126/1000 | Loss: 0.00008000
Iteration 127/1000 | Loss: 0.00008000
Iteration 128/1000 | Loss: 0.00007999
Iteration 129/1000 | Loss: 0.00007997
Iteration 130/1000 | Loss: 0.00007997
Iteration 131/1000 | Loss: 0.00007997
Iteration 132/1000 | Loss: 0.00007996
Iteration 133/1000 | Loss: 0.00007996
Iteration 134/1000 | Loss: 0.00007995
Iteration 135/1000 | Loss: 0.00007994
Iteration 136/1000 | Loss: 0.00007993
Iteration 137/1000 | Loss: 0.00007992
Iteration 138/1000 | Loss: 0.00007990
Iteration 139/1000 | Loss: 0.00007987
Iteration 140/1000 | Loss: 0.00007987
Iteration 141/1000 | Loss: 0.00007987
Iteration 142/1000 | Loss: 0.00007987
Iteration 143/1000 | Loss: 0.00007987
Iteration 144/1000 | Loss: 0.00007987
Iteration 145/1000 | Loss: 0.00007987
Iteration 146/1000 | Loss: 0.00007986
Iteration 147/1000 | Loss: 0.00007986
Iteration 148/1000 | Loss: 0.00007985
Iteration 149/1000 | Loss: 0.00007984
Iteration 150/1000 | Loss: 0.00007984
Iteration 151/1000 | Loss: 0.00007983
Iteration 152/1000 | Loss: 0.00007982
Iteration 153/1000 | Loss: 0.00007981
Iteration 154/1000 | Loss: 0.00007978
Iteration 155/1000 | Loss: 0.00007978
Iteration 156/1000 | Loss: 0.00007977
Iteration 157/1000 | Loss: 0.00007977
Iteration 158/1000 | Loss: 0.00007977
Iteration 159/1000 | Loss: 0.00007977
Iteration 160/1000 | Loss: 0.00007977
Iteration 161/1000 | Loss: 0.00007976
Iteration 162/1000 | Loss: 0.00007976
Iteration 163/1000 | Loss: 0.00007976
Iteration 164/1000 | Loss: 0.00007976
Iteration 165/1000 | Loss: 0.00007975
Iteration 166/1000 | Loss: 0.00007974
Iteration 167/1000 | Loss: 0.00007973
Iteration 168/1000 | Loss: 0.00007973
Iteration 169/1000 | Loss: 0.00007972
Iteration 170/1000 | Loss: 0.00007972
Iteration 171/1000 | Loss: 0.00007972
Iteration 172/1000 | Loss: 0.00007971
Iteration 173/1000 | Loss: 0.00007970
Iteration 174/1000 | Loss: 0.00007969
Iteration 175/1000 | Loss: 0.00007969
Iteration 176/1000 | Loss: 0.00007969
Iteration 177/1000 | Loss: 0.00007969
Iteration 178/1000 | Loss: 0.00007969
Iteration 179/1000 | Loss: 0.00007969
Iteration 180/1000 | Loss: 0.00007969
Iteration 181/1000 | Loss: 0.00007969
Iteration 182/1000 | Loss: 0.00007969
Iteration 183/1000 | Loss: 0.00007969
Iteration 184/1000 | Loss: 0.00007969
Iteration 185/1000 | Loss: 0.00007968
Iteration 186/1000 | Loss: 0.00007968
Iteration 187/1000 | Loss: 0.00007968
Iteration 188/1000 | Loss: 0.00007968
Iteration 189/1000 | Loss: 0.00007968
Iteration 190/1000 | Loss: 0.00007968
Iteration 191/1000 | Loss: 0.00007968
Iteration 192/1000 | Loss: 0.00007967
Iteration 193/1000 | Loss: 0.00007967
Iteration 194/1000 | Loss: 0.00007967
Iteration 195/1000 | Loss: 0.00007967
Iteration 196/1000 | Loss: 0.00007967
Iteration 197/1000 | Loss: 0.00007967
Iteration 198/1000 | Loss: 0.00007967
Iteration 199/1000 | Loss: 0.00007967
Iteration 200/1000 | Loss: 0.00007967
Iteration 201/1000 | Loss: 0.00007967
Iteration 202/1000 | Loss: 0.00007967
Iteration 203/1000 | Loss: 0.00007967
Iteration 204/1000 | Loss: 0.00007966
Iteration 205/1000 | Loss: 0.00007966
Iteration 206/1000 | Loss: 0.00007966
Iteration 207/1000 | Loss: 0.00007966
Iteration 208/1000 | Loss: 0.00007966
Iteration 209/1000 | Loss: 0.00007966
Iteration 210/1000 | Loss: 0.00007966
Iteration 211/1000 | Loss: 0.00007966
Iteration 212/1000 | Loss: 0.00007966
Iteration 213/1000 | Loss: 0.00007966
Iteration 214/1000 | Loss: 0.00007966
Iteration 215/1000 | Loss: 0.00007965
Iteration 216/1000 | Loss: 0.00007965
Iteration 217/1000 | Loss: 0.00007965
Iteration 218/1000 | Loss: 0.00007965
Iteration 219/1000 | Loss: 0.00007965
Iteration 220/1000 | Loss: 0.00007965
Iteration 221/1000 | Loss: 0.00007965
Iteration 222/1000 | Loss: 0.00007965
Iteration 223/1000 | Loss: 0.00007965
Iteration 224/1000 | Loss: 0.00007965
Iteration 225/1000 | Loss: 0.00007965
Iteration 226/1000 | Loss: 0.00007965
Iteration 227/1000 | Loss: 0.00007965
Iteration 228/1000 | Loss: 0.00007965
Iteration 229/1000 | Loss: 0.00007965
Iteration 230/1000 | Loss: 0.00007965
Iteration 231/1000 | Loss: 0.00007964
Iteration 232/1000 | Loss: 0.00007964
Iteration 233/1000 | Loss: 0.00007964
Iteration 234/1000 | Loss: 0.00007964
Iteration 235/1000 | Loss: 0.00007964
Iteration 236/1000 | Loss: 0.00007964
Iteration 237/1000 | Loss: 0.00007964
Iteration 238/1000 | Loss: 0.00007964
Iteration 239/1000 | Loss: 0.00007964
Iteration 240/1000 | Loss: 0.00007964
Iteration 241/1000 | Loss: 0.00007964
Iteration 242/1000 | Loss: 0.00007964
Iteration 243/1000 | Loss: 0.00007964
Iteration 244/1000 | Loss: 0.00007964
Iteration 245/1000 | Loss: 0.00007964
Iteration 246/1000 | Loss: 0.00007964
Iteration 247/1000 | Loss: 0.00007964
Iteration 248/1000 | Loss: 0.00007963
Iteration 249/1000 | Loss: 0.00007963
Iteration 250/1000 | Loss: 0.00007963
Iteration 251/1000 | Loss: 0.00007963
Iteration 252/1000 | Loss: 0.00007963
Iteration 253/1000 | Loss: 0.00007963
Iteration 254/1000 | Loss: 0.00007963
Iteration 255/1000 | Loss: 0.00007963
Iteration 256/1000 | Loss: 0.00007963
Iteration 257/1000 | Loss: 0.00007963
Iteration 258/1000 | Loss: 0.00007962
Iteration 259/1000 | Loss: 0.00007962
Iteration 260/1000 | Loss: 0.00007962
Iteration 261/1000 | Loss: 0.00007962
Iteration 262/1000 | Loss: 0.00007962
Iteration 263/1000 | Loss: 0.00007962
Iteration 264/1000 | Loss: 0.00007962
Iteration 265/1000 | Loss: 0.00007962
Iteration 266/1000 | Loss: 0.00007962
Iteration 267/1000 | Loss: 0.00007962
Iteration 268/1000 | Loss: 0.00007962
Iteration 269/1000 | Loss: 0.00007962
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 269. Stopping optimization.
Last 5 losses: [7.9621713666711e-05, 7.9621713666711e-05, 7.9621713666711e-05, 7.9621713666711e-05, 7.9621713666711e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.9621713666711e-05

Optimization complete. Final v2v error: 4.9161481857299805 mm

Highest mean error: 12.216416358947754 mm for frame 54

Lowest mean error: 3.1207263469696045 mm for frame 5

Saving results

Total time: 211.9216549396515
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00488914
Iteration 2/25 | Loss: 0.00079231
Iteration 3/25 | Loss: 0.00063966
Iteration 4/25 | Loss: 0.00060768
Iteration 5/25 | Loss: 0.00060050
Iteration 6/25 | Loss: 0.00059942
Iteration 7/25 | Loss: 0.00059929
Iteration 8/25 | Loss: 0.00059929
Iteration 9/25 | Loss: 0.00059929
Iteration 10/25 | Loss: 0.00059929
Iteration 11/25 | Loss: 0.00059929
Iteration 12/25 | Loss: 0.00059929
Iteration 13/25 | Loss: 0.00059929
Iteration 14/25 | Loss: 0.00059929
Iteration 15/25 | Loss: 0.00059929
Iteration 16/25 | Loss: 0.00059929
Iteration 17/25 | Loss: 0.00059929
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005992924561724067, 0.0005992924561724067, 0.0005992924561724067, 0.0005992924561724067, 0.0005992924561724067]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005992924561724067

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46134353
Iteration 2/25 | Loss: 0.00022514
Iteration 3/25 | Loss: 0.00022510
Iteration 4/25 | Loss: 0.00022510
Iteration 5/25 | Loss: 0.00022510
Iteration 6/25 | Loss: 0.00022510
Iteration 7/25 | Loss: 0.00022510
Iteration 8/25 | Loss: 0.00022509
Iteration 9/25 | Loss: 0.00022509
Iteration 10/25 | Loss: 0.00022509
Iteration 11/25 | Loss: 0.00022509
Iteration 12/25 | Loss: 0.00022509
Iteration 13/25 | Loss: 0.00022509
Iteration 14/25 | Loss: 0.00022509
Iteration 15/25 | Loss: 0.00022509
Iteration 16/25 | Loss: 0.00022509
Iteration 17/25 | Loss: 0.00022509
Iteration 18/25 | Loss: 0.00022509
Iteration 19/25 | Loss: 0.00022509
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00022509424888994545, 0.00022509424888994545, 0.00022509424888994545, 0.00022509424888994545, 0.00022509424888994545]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00022509424888994545

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00022509
Iteration 2/1000 | Loss: 0.00002103
Iteration 3/1000 | Loss: 0.00001509
Iteration 4/1000 | Loss: 0.00001375
Iteration 5/1000 | Loss: 0.00001311
Iteration 6/1000 | Loss: 0.00001259
Iteration 7/1000 | Loss: 0.00001224
Iteration 8/1000 | Loss: 0.00001202
Iteration 9/1000 | Loss: 0.00001201
Iteration 10/1000 | Loss: 0.00001199
Iteration 11/1000 | Loss: 0.00001198
Iteration 12/1000 | Loss: 0.00001197
Iteration 13/1000 | Loss: 0.00001193
Iteration 14/1000 | Loss: 0.00001187
Iteration 15/1000 | Loss: 0.00001187
Iteration 16/1000 | Loss: 0.00001182
Iteration 17/1000 | Loss: 0.00001182
Iteration 18/1000 | Loss: 0.00001179
Iteration 19/1000 | Loss: 0.00001178
Iteration 20/1000 | Loss: 0.00001178
Iteration 21/1000 | Loss: 0.00001178
Iteration 22/1000 | Loss: 0.00001178
Iteration 23/1000 | Loss: 0.00001177
Iteration 24/1000 | Loss: 0.00001177
Iteration 25/1000 | Loss: 0.00001177
Iteration 26/1000 | Loss: 0.00001176
Iteration 27/1000 | Loss: 0.00001176
Iteration 28/1000 | Loss: 0.00001175
Iteration 29/1000 | Loss: 0.00001173
Iteration 30/1000 | Loss: 0.00001173
Iteration 31/1000 | Loss: 0.00001173
Iteration 32/1000 | Loss: 0.00001172
Iteration 33/1000 | Loss: 0.00001172
Iteration 34/1000 | Loss: 0.00001171
Iteration 35/1000 | Loss: 0.00001170
Iteration 36/1000 | Loss: 0.00001170
Iteration 37/1000 | Loss: 0.00001170
Iteration 38/1000 | Loss: 0.00001169
Iteration 39/1000 | Loss: 0.00001169
Iteration 40/1000 | Loss: 0.00001169
Iteration 41/1000 | Loss: 0.00001168
Iteration 42/1000 | Loss: 0.00001168
Iteration 43/1000 | Loss: 0.00001167
Iteration 44/1000 | Loss: 0.00001167
Iteration 45/1000 | Loss: 0.00001167
Iteration 46/1000 | Loss: 0.00001167
Iteration 47/1000 | Loss: 0.00001166
Iteration 48/1000 | Loss: 0.00001166
Iteration 49/1000 | Loss: 0.00001166
Iteration 50/1000 | Loss: 0.00001166
Iteration 51/1000 | Loss: 0.00001165
Iteration 52/1000 | Loss: 0.00001165
Iteration 53/1000 | Loss: 0.00001165
Iteration 54/1000 | Loss: 0.00001165
Iteration 55/1000 | Loss: 0.00001165
Iteration 56/1000 | Loss: 0.00001165
Iteration 57/1000 | Loss: 0.00001165
Iteration 58/1000 | Loss: 0.00001165
Iteration 59/1000 | Loss: 0.00001165
Iteration 60/1000 | Loss: 0.00001165
Iteration 61/1000 | Loss: 0.00001165
Iteration 62/1000 | Loss: 0.00001164
Iteration 63/1000 | Loss: 0.00001164
Iteration 64/1000 | Loss: 0.00001163
Iteration 65/1000 | Loss: 0.00001163
Iteration 66/1000 | Loss: 0.00001162
Iteration 67/1000 | Loss: 0.00001162
Iteration 68/1000 | Loss: 0.00001162
Iteration 69/1000 | Loss: 0.00001161
Iteration 70/1000 | Loss: 0.00001161
Iteration 71/1000 | Loss: 0.00001161
Iteration 72/1000 | Loss: 0.00001160
Iteration 73/1000 | Loss: 0.00001160
Iteration 74/1000 | Loss: 0.00001159
Iteration 75/1000 | Loss: 0.00001159
Iteration 76/1000 | Loss: 0.00001159
Iteration 77/1000 | Loss: 0.00001159
Iteration 78/1000 | Loss: 0.00001159
Iteration 79/1000 | Loss: 0.00001158
Iteration 80/1000 | Loss: 0.00001158
Iteration 81/1000 | Loss: 0.00001158
Iteration 82/1000 | Loss: 0.00001158
Iteration 83/1000 | Loss: 0.00001158
Iteration 84/1000 | Loss: 0.00001158
Iteration 85/1000 | Loss: 0.00001158
Iteration 86/1000 | Loss: 0.00001158
Iteration 87/1000 | Loss: 0.00001157
Iteration 88/1000 | Loss: 0.00001157
Iteration 89/1000 | Loss: 0.00001157
Iteration 90/1000 | Loss: 0.00001157
Iteration 91/1000 | Loss: 0.00001157
Iteration 92/1000 | Loss: 0.00001157
Iteration 93/1000 | Loss: 0.00001157
Iteration 94/1000 | Loss: 0.00001157
Iteration 95/1000 | Loss: 0.00001156
Iteration 96/1000 | Loss: 0.00001156
Iteration 97/1000 | Loss: 0.00001156
Iteration 98/1000 | Loss: 0.00001156
Iteration 99/1000 | Loss: 0.00001156
Iteration 100/1000 | Loss: 0.00001156
Iteration 101/1000 | Loss: 0.00001156
Iteration 102/1000 | Loss: 0.00001155
Iteration 103/1000 | Loss: 0.00001155
Iteration 104/1000 | Loss: 0.00001155
Iteration 105/1000 | Loss: 0.00001155
Iteration 106/1000 | Loss: 0.00001155
Iteration 107/1000 | Loss: 0.00001155
Iteration 108/1000 | Loss: 0.00001155
Iteration 109/1000 | Loss: 0.00001155
Iteration 110/1000 | Loss: 0.00001155
Iteration 111/1000 | Loss: 0.00001155
Iteration 112/1000 | Loss: 0.00001155
Iteration 113/1000 | Loss: 0.00001155
Iteration 114/1000 | Loss: 0.00001154
Iteration 115/1000 | Loss: 0.00001154
Iteration 116/1000 | Loss: 0.00001154
Iteration 117/1000 | Loss: 0.00001154
Iteration 118/1000 | Loss: 0.00001154
Iteration 119/1000 | Loss: 0.00001154
Iteration 120/1000 | Loss: 0.00001154
Iteration 121/1000 | Loss: 0.00001154
Iteration 122/1000 | Loss: 0.00001154
Iteration 123/1000 | Loss: 0.00001154
Iteration 124/1000 | Loss: 0.00001154
Iteration 125/1000 | Loss: 0.00001154
Iteration 126/1000 | Loss: 0.00001153
Iteration 127/1000 | Loss: 0.00001153
Iteration 128/1000 | Loss: 0.00001153
Iteration 129/1000 | Loss: 0.00001153
Iteration 130/1000 | Loss: 0.00001153
Iteration 131/1000 | Loss: 0.00001153
Iteration 132/1000 | Loss: 0.00001153
Iteration 133/1000 | Loss: 0.00001153
Iteration 134/1000 | Loss: 0.00001153
Iteration 135/1000 | Loss: 0.00001153
Iteration 136/1000 | Loss: 0.00001153
Iteration 137/1000 | Loss: 0.00001153
Iteration 138/1000 | Loss: 0.00001153
Iteration 139/1000 | Loss: 0.00001152
Iteration 140/1000 | Loss: 0.00001152
Iteration 141/1000 | Loss: 0.00001152
Iteration 142/1000 | Loss: 0.00001152
Iteration 143/1000 | Loss: 0.00001152
Iteration 144/1000 | Loss: 0.00001152
Iteration 145/1000 | Loss: 0.00001152
Iteration 146/1000 | Loss: 0.00001152
Iteration 147/1000 | Loss: 0.00001152
Iteration 148/1000 | Loss: 0.00001151
Iteration 149/1000 | Loss: 0.00001151
Iteration 150/1000 | Loss: 0.00001151
Iteration 151/1000 | Loss: 0.00001151
Iteration 152/1000 | Loss: 0.00001151
Iteration 153/1000 | Loss: 0.00001150
Iteration 154/1000 | Loss: 0.00001150
Iteration 155/1000 | Loss: 0.00001150
Iteration 156/1000 | Loss: 0.00001150
Iteration 157/1000 | Loss: 0.00001150
Iteration 158/1000 | Loss: 0.00001149
Iteration 159/1000 | Loss: 0.00001149
Iteration 160/1000 | Loss: 0.00001149
Iteration 161/1000 | Loss: 0.00001149
Iteration 162/1000 | Loss: 0.00001149
Iteration 163/1000 | Loss: 0.00001149
Iteration 164/1000 | Loss: 0.00001149
Iteration 165/1000 | Loss: 0.00001149
Iteration 166/1000 | Loss: 0.00001149
Iteration 167/1000 | Loss: 0.00001149
Iteration 168/1000 | Loss: 0.00001149
Iteration 169/1000 | Loss: 0.00001149
Iteration 170/1000 | Loss: 0.00001149
Iteration 171/1000 | Loss: 0.00001149
Iteration 172/1000 | Loss: 0.00001148
Iteration 173/1000 | Loss: 0.00001148
Iteration 174/1000 | Loss: 0.00001148
Iteration 175/1000 | Loss: 0.00001148
Iteration 176/1000 | Loss: 0.00001148
Iteration 177/1000 | Loss: 0.00001148
Iteration 178/1000 | Loss: 0.00001148
Iteration 179/1000 | Loss: 0.00001148
Iteration 180/1000 | Loss: 0.00001148
Iteration 181/1000 | Loss: 0.00001148
Iteration 182/1000 | Loss: 0.00001148
Iteration 183/1000 | Loss: 0.00001148
Iteration 184/1000 | Loss: 0.00001148
Iteration 185/1000 | Loss: 0.00001147
Iteration 186/1000 | Loss: 0.00001147
Iteration 187/1000 | Loss: 0.00001147
Iteration 188/1000 | Loss: 0.00001147
Iteration 189/1000 | Loss: 0.00001147
Iteration 190/1000 | Loss: 0.00001147
Iteration 191/1000 | Loss: 0.00001147
Iteration 192/1000 | Loss: 0.00001147
Iteration 193/1000 | Loss: 0.00001147
Iteration 194/1000 | Loss: 0.00001147
Iteration 195/1000 | Loss: 0.00001147
Iteration 196/1000 | Loss: 0.00001147
Iteration 197/1000 | Loss: 0.00001147
Iteration 198/1000 | Loss: 0.00001147
Iteration 199/1000 | Loss: 0.00001147
Iteration 200/1000 | Loss: 0.00001147
Iteration 201/1000 | Loss: 0.00001147
Iteration 202/1000 | Loss: 0.00001147
Iteration 203/1000 | Loss: 0.00001146
Iteration 204/1000 | Loss: 0.00001146
Iteration 205/1000 | Loss: 0.00001146
Iteration 206/1000 | Loss: 0.00001146
Iteration 207/1000 | Loss: 0.00001146
Iteration 208/1000 | Loss: 0.00001146
Iteration 209/1000 | Loss: 0.00001146
Iteration 210/1000 | Loss: 0.00001146
Iteration 211/1000 | Loss: 0.00001146
Iteration 212/1000 | Loss: 0.00001146
Iteration 213/1000 | Loss: 0.00001146
Iteration 214/1000 | Loss: 0.00001146
Iteration 215/1000 | Loss: 0.00001146
Iteration 216/1000 | Loss: 0.00001145
Iteration 217/1000 | Loss: 0.00001145
Iteration 218/1000 | Loss: 0.00001145
Iteration 219/1000 | Loss: 0.00001145
Iteration 220/1000 | Loss: 0.00001145
Iteration 221/1000 | Loss: 0.00001145
Iteration 222/1000 | Loss: 0.00001145
Iteration 223/1000 | Loss: 0.00001145
Iteration 224/1000 | Loss: 0.00001145
Iteration 225/1000 | Loss: 0.00001145
Iteration 226/1000 | Loss: 0.00001145
Iteration 227/1000 | Loss: 0.00001145
Iteration 228/1000 | Loss: 0.00001145
Iteration 229/1000 | Loss: 0.00001145
Iteration 230/1000 | Loss: 0.00001145
Iteration 231/1000 | Loss: 0.00001145
Iteration 232/1000 | Loss: 0.00001145
Iteration 233/1000 | Loss: 0.00001145
Iteration 234/1000 | Loss: 0.00001145
Iteration 235/1000 | Loss: 0.00001145
Iteration 236/1000 | Loss: 0.00001145
Iteration 237/1000 | Loss: 0.00001144
Iteration 238/1000 | Loss: 0.00001144
Iteration 239/1000 | Loss: 0.00001144
Iteration 240/1000 | Loss: 0.00001144
Iteration 241/1000 | Loss: 0.00001144
Iteration 242/1000 | Loss: 0.00001144
Iteration 243/1000 | Loss: 0.00001144
Iteration 244/1000 | Loss: 0.00001144
Iteration 245/1000 | Loss: 0.00001144
Iteration 246/1000 | Loss: 0.00001144
Iteration 247/1000 | Loss: 0.00001144
Iteration 248/1000 | Loss: 0.00001144
Iteration 249/1000 | Loss: 0.00001144
Iteration 250/1000 | Loss: 0.00001144
Iteration 251/1000 | Loss: 0.00001144
Iteration 252/1000 | Loss: 0.00001144
Iteration 253/1000 | Loss: 0.00001144
Iteration 254/1000 | Loss: 0.00001144
Iteration 255/1000 | Loss: 0.00001144
Iteration 256/1000 | Loss: 0.00001144
Iteration 257/1000 | Loss: 0.00001144
Iteration 258/1000 | Loss: 0.00001144
Iteration 259/1000 | Loss: 0.00001144
Iteration 260/1000 | Loss: 0.00001144
Iteration 261/1000 | Loss: 0.00001144
Iteration 262/1000 | Loss: 0.00001144
Iteration 263/1000 | Loss: 0.00001144
Iteration 264/1000 | Loss: 0.00001144
Iteration 265/1000 | Loss: 0.00001144
Iteration 266/1000 | Loss: 0.00001144
Iteration 267/1000 | Loss: 0.00001144
Iteration 268/1000 | Loss: 0.00001144
Iteration 269/1000 | Loss: 0.00001144
Iteration 270/1000 | Loss: 0.00001144
Iteration 271/1000 | Loss: 0.00001144
Iteration 272/1000 | Loss: 0.00001144
Iteration 273/1000 | Loss: 0.00001144
Iteration 274/1000 | Loss: 0.00001144
Iteration 275/1000 | Loss: 0.00001144
Iteration 276/1000 | Loss: 0.00001144
Iteration 277/1000 | Loss: 0.00001144
Iteration 278/1000 | Loss: 0.00001144
Iteration 279/1000 | Loss: 0.00001144
Iteration 280/1000 | Loss: 0.00001144
Iteration 281/1000 | Loss: 0.00001144
Iteration 282/1000 | Loss: 0.00001144
Iteration 283/1000 | Loss: 0.00001144
Iteration 284/1000 | Loss: 0.00001144
Iteration 285/1000 | Loss: 0.00001144
Iteration 286/1000 | Loss: 0.00001144
Iteration 287/1000 | Loss: 0.00001144
Iteration 288/1000 | Loss: 0.00001144
Iteration 289/1000 | Loss: 0.00001144
Iteration 290/1000 | Loss: 0.00001144
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 290. Stopping optimization.
Last 5 losses: [1.143560166383395e-05, 1.143560166383395e-05, 1.143560166383395e-05, 1.143560166383395e-05, 1.143560166383395e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.143560166383395e-05

Optimization complete. Final v2v error: 2.8939366340637207 mm

Highest mean error: 3.2825872898101807 mm for frame 136

Lowest mean error: 2.571516275405884 mm for frame 16

Saving results

Total time: 41.14289116859436
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00735235
Iteration 2/25 | Loss: 0.00107902
Iteration 3/25 | Loss: 0.00079402
Iteration 4/25 | Loss: 0.00074361
Iteration 5/25 | Loss: 0.00073522
Iteration 6/25 | Loss: 0.00073388
Iteration 7/25 | Loss: 0.00073388
Iteration 8/25 | Loss: 0.00073388
Iteration 9/25 | Loss: 0.00073388
Iteration 10/25 | Loss: 0.00073388
Iteration 11/25 | Loss: 0.00073388
Iteration 12/25 | Loss: 0.00073388
Iteration 13/25 | Loss: 0.00073388
Iteration 14/25 | Loss: 0.00073388
Iteration 15/25 | Loss: 0.00073388
Iteration 16/25 | Loss: 0.00073388
Iteration 17/25 | Loss: 0.00073388
Iteration 18/25 | Loss: 0.00073388
Iteration 19/25 | Loss: 0.00073388
Iteration 20/25 | Loss: 0.00073388
Iteration 21/25 | Loss: 0.00073388
Iteration 22/25 | Loss: 0.00073388
Iteration 23/25 | Loss: 0.00073388
Iteration 24/25 | Loss: 0.00073388
Iteration 25/25 | Loss: 0.00073388

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.24262667
Iteration 2/25 | Loss: 0.00052873
Iteration 3/25 | Loss: 0.00052871
Iteration 4/25 | Loss: 0.00052870
Iteration 5/25 | Loss: 0.00052870
Iteration 6/25 | Loss: 0.00052870
Iteration 7/25 | Loss: 0.00052870
Iteration 8/25 | Loss: 0.00052870
Iteration 9/25 | Loss: 0.00052870
Iteration 10/25 | Loss: 0.00052870
Iteration 11/25 | Loss: 0.00052870
Iteration 12/25 | Loss: 0.00052870
Iteration 13/25 | Loss: 0.00052870
Iteration 14/25 | Loss: 0.00052870
Iteration 15/25 | Loss: 0.00052870
Iteration 16/25 | Loss: 0.00052870
Iteration 17/25 | Loss: 0.00052870
Iteration 18/25 | Loss: 0.00052870
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005287016974762082, 0.0005287016974762082, 0.0005287016974762082, 0.0005287016974762082, 0.0005287016974762082]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005287016974762082

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052870
Iteration 2/1000 | Loss: 0.00004120
Iteration 3/1000 | Loss: 0.00002570
Iteration 4/1000 | Loss: 0.00002272
Iteration 5/1000 | Loss: 0.00002063
Iteration 6/1000 | Loss: 0.00001969
Iteration 7/1000 | Loss: 0.00001899
Iteration 8/1000 | Loss: 0.00001863
Iteration 9/1000 | Loss: 0.00001837
Iteration 10/1000 | Loss: 0.00001832
Iteration 11/1000 | Loss: 0.00001816
Iteration 12/1000 | Loss: 0.00001806
Iteration 13/1000 | Loss: 0.00001803
Iteration 14/1000 | Loss: 0.00001802
Iteration 15/1000 | Loss: 0.00001800
Iteration 16/1000 | Loss: 0.00001798
Iteration 17/1000 | Loss: 0.00001798
Iteration 18/1000 | Loss: 0.00001797
Iteration 19/1000 | Loss: 0.00001797
Iteration 20/1000 | Loss: 0.00001796
Iteration 21/1000 | Loss: 0.00001795
Iteration 22/1000 | Loss: 0.00001793
Iteration 23/1000 | Loss: 0.00001792
Iteration 24/1000 | Loss: 0.00001792
Iteration 25/1000 | Loss: 0.00001791
Iteration 26/1000 | Loss: 0.00001791
Iteration 27/1000 | Loss: 0.00001791
Iteration 28/1000 | Loss: 0.00001787
Iteration 29/1000 | Loss: 0.00001786
Iteration 30/1000 | Loss: 0.00001786
Iteration 31/1000 | Loss: 0.00001786
Iteration 32/1000 | Loss: 0.00001785
Iteration 33/1000 | Loss: 0.00001784
Iteration 34/1000 | Loss: 0.00001784
Iteration 35/1000 | Loss: 0.00001784
Iteration 36/1000 | Loss: 0.00001784
Iteration 37/1000 | Loss: 0.00001783
Iteration 38/1000 | Loss: 0.00001783
Iteration 39/1000 | Loss: 0.00001783
Iteration 40/1000 | Loss: 0.00001783
Iteration 41/1000 | Loss: 0.00001782
Iteration 42/1000 | Loss: 0.00001782
Iteration 43/1000 | Loss: 0.00001782
Iteration 44/1000 | Loss: 0.00001782
Iteration 45/1000 | Loss: 0.00001782
Iteration 46/1000 | Loss: 0.00001781
Iteration 47/1000 | Loss: 0.00001781
Iteration 48/1000 | Loss: 0.00001781
Iteration 49/1000 | Loss: 0.00001781
Iteration 50/1000 | Loss: 0.00001781
Iteration 51/1000 | Loss: 0.00001781
Iteration 52/1000 | Loss: 0.00001781
Iteration 53/1000 | Loss: 0.00001781
Iteration 54/1000 | Loss: 0.00001781
Iteration 55/1000 | Loss: 0.00001780
Iteration 56/1000 | Loss: 0.00001780
Iteration 57/1000 | Loss: 0.00001780
Iteration 58/1000 | Loss: 0.00001779
Iteration 59/1000 | Loss: 0.00001779
Iteration 60/1000 | Loss: 0.00001779
Iteration 61/1000 | Loss: 0.00001778
Iteration 62/1000 | Loss: 0.00001778
Iteration 63/1000 | Loss: 0.00001778
Iteration 64/1000 | Loss: 0.00001778
Iteration 65/1000 | Loss: 0.00001778
Iteration 66/1000 | Loss: 0.00001778
Iteration 67/1000 | Loss: 0.00001778
Iteration 68/1000 | Loss: 0.00001777
Iteration 69/1000 | Loss: 0.00001777
Iteration 70/1000 | Loss: 0.00001777
Iteration 71/1000 | Loss: 0.00001777
Iteration 72/1000 | Loss: 0.00001777
Iteration 73/1000 | Loss: 0.00001776
Iteration 74/1000 | Loss: 0.00001776
Iteration 75/1000 | Loss: 0.00001776
Iteration 76/1000 | Loss: 0.00001776
Iteration 77/1000 | Loss: 0.00001775
Iteration 78/1000 | Loss: 0.00001775
Iteration 79/1000 | Loss: 0.00001775
Iteration 80/1000 | Loss: 0.00001775
Iteration 81/1000 | Loss: 0.00001775
Iteration 82/1000 | Loss: 0.00001775
Iteration 83/1000 | Loss: 0.00001774
Iteration 84/1000 | Loss: 0.00001774
Iteration 85/1000 | Loss: 0.00001774
Iteration 86/1000 | Loss: 0.00001774
Iteration 87/1000 | Loss: 0.00001774
Iteration 88/1000 | Loss: 0.00001774
Iteration 89/1000 | Loss: 0.00001774
Iteration 90/1000 | Loss: 0.00001774
Iteration 91/1000 | Loss: 0.00001774
Iteration 92/1000 | Loss: 0.00001774
Iteration 93/1000 | Loss: 0.00001774
Iteration 94/1000 | Loss: 0.00001774
Iteration 95/1000 | Loss: 0.00001774
Iteration 96/1000 | Loss: 0.00001774
Iteration 97/1000 | Loss: 0.00001774
Iteration 98/1000 | Loss: 0.00001774
Iteration 99/1000 | Loss: 0.00001774
Iteration 100/1000 | Loss: 0.00001774
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [1.773596159182489e-05, 1.773596159182489e-05, 1.773596159182489e-05, 1.773596159182489e-05, 1.773596159182489e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.773596159182489e-05

Optimization complete. Final v2v error: 3.4887499809265137 mm

Highest mean error: 3.9438154697418213 mm for frame 5

Lowest mean error: 3.00394344329834 mm for frame 238

Saving results

Total time: 37.120659828186035
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00860101
Iteration 2/25 | Loss: 0.00083908
Iteration 3/25 | Loss: 0.00064409
Iteration 4/25 | Loss: 0.00061642
Iteration 5/25 | Loss: 0.00060945
Iteration 6/25 | Loss: 0.00060715
Iteration 7/25 | Loss: 0.00060686
Iteration 8/25 | Loss: 0.00060686
Iteration 9/25 | Loss: 0.00060686
Iteration 10/25 | Loss: 0.00060686
Iteration 11/25 | Loss: 0.00060686
Iteration 12/25 | Loss: 0.00060686
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006068596849218011, 0.0006068596849218011, 0.0006068596849218011, 0.0006068596849218011, 0.0006068596849218011]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006068596849218011

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46094513
Iteration 2/25 | Loss: 0.00025215
Iteration 3/25 | Loss: 0.00025213
Iteration 4/25 | Loss: 0.00025213
Iteration 5/25 | Loss: 0.00025213
Iteration 6/25 | Loss: 0.00025213
Iteration 7/25 | Loss: 0.00025213
Iteration 8/25 | Loss: 0.00025213
Iteration 9/25 | Loss: 0.00025213
Iteration 10/25 | Loss: 0.00025213
Iteration 11/25 | Loss: 0.00025213
Iteration 12/25 | Loss: 0.00025213
Iteration 13/25 | Loss: 0.00025213
Iteration 14/25 | Loss: 0.00025213
Iteration 15/25 | Loss: 0.00025213
Iteration 16/25 | Loss: 0.00025213
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00025212668697349727, 0.00025212668697349727, 0.00025212668697349727, 0.00025212668697349727, 0.00025212668697349727]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00025212668697349727

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025213
Iteration 2/1000 | Loss: 0.00002051
Iteration 3/1000 | Loss: 0.00001524
Iteration 4/1000 | Loss: 0.00001383
Iteration 5/1000 | Loss: 0.00001300
Iteration 6/1000 | Loss: 0.00001246
Iteration 7/1000 | Loss: 0.00001200
Iteration 8/1000 | Loss: 0.00001196
Iteration 9/1000 | Loss: 0.00001189
Iteration 10/1000 | Loss: 0.00001189
Iteration 11/1000 | Loss: 0.00001188
Iteration 12/1000 | Loss: 0.00001185
Iteration 13/1000 | Loss: 0.00001179
Iteration 14/1000 | Loss: 0.00001176
Iteration 15/1000 | Loss: 0.00001168
Iteration 16/1000 | Loss: 0.00001161
Iteration 17/1000 | Loss: 0.00001161
Iteration 18/1000 | Loss: 0.00001159
Iteration 19/1000 | Loss: 0.00001158
Iteration 20/1000 | Loss: 0.00001155
Iteration 21/1000 | Loss: 0.00001153
Iteration 22/1000 | Loss: 0.00001152
Iteration 23/1000 | Loss: 0.00001152
Iteration 24/1000 | Loss: 0.00001152
Iteration 25/1000 | Loss: 0.00001152
Iteration 26/1000 | Loss: 0.00001152
Iteration 27/1000 | Loss: 0.00001152
Iteration 28/1000 | Loss: 0.00001152
Iteration 29/1000 | Loss: 0.00001152
Iteration 30/1000 | Loss: 0.00001152
Iteration 31/1000 | Loss: 0.00001152
Iteration 32/1000 | Loss: 0.00001152
Iteration 33/1000 | Loss: 0.00001151
Iteration 34/1000 | Loss: 0.00001151
Iteration 35/1000 | Loss: 0.00001151
Iteration 36/1000 | Loss: 0.00001150
Iteration 37/1000 | Loss: 0.00001150
Iteration 38/1000 | Loss: 0.00001150
Iteration 39/1000 | Loss: 0.00001150
Iteration 40/1000 | Loss: 0.00001149
Iteration 41/1000 | Loss: 0.00001149
Iteration 42/1000 | Loss: 0.00001149
Iteration 43/1000 | Loss: 0.00001149
Iteration 44/1000 | Loss: 0.00001149
Iteration 45/1000 | Loss: 0.00001148
Iteration 46/1000 | Loss: 0.00001148
Iteration 47/1000 | Loss: 0.00001148
Iteration 48/1000 | Loss: 0.00001147
Iteration 49/1000 | Loss: 0.00001147
Iteration 50/1000 | Loss: 0.00001147
Iteration 51/1000 | Loss: 0.00001147
Iteration 52/1000 | Loss: 0.00001147
Iteration 53/1000 | Loss: 0.00001147
Iteration 54/1000 | Loss: 0.00001147
Iteration 55/1000 | Loss: 0.00001147
Iteration 56/1000 | Loss: 0.00001147
Iteration 57/1000 | Loss: 0.00001147
Iteration 58/1000 | Loss: 0.00001147
Iteration 59/1000 | Loss: 0.00001146
Iteration 60/1000 | Loss: 0.00001146
Iteration 61/1000 | Loss: 0.00001146
Iteration 62/1000 | Loss: 0.00001146
Iteration 63/1000 | Loss: 0.00001146
Iteration 64/1000 | Loss: 0.00001145
Iteration 65/1000 | Loss: 0.00001145
Iteration 66/1000 | Loss: 0.00001145
Iteration 67/1000 | Loss: 0.00001145
Iteration 68/1000 | Loss: 0.00001144
Iteration 69/1000 | Loss: 0.00001144
Iteration 70/1000 | Loss: 0.00001144
Iteration 71/1000 | Loss: 0.00001144
Iteration 72/1000 | Loss: 0.00001144
Iteration 73/1000 | Loss: 0.00001144
Iteration 74/1000 | Loss: 0.00001144
Iteration 75/1000 | Loss: 0.00001144
Iteration 76/1000 | Loss: 0.00001144
Iteration 77/1000 | Loss: 0.00001144
Iteration 78/1000 | Loss: 0.00001144
Iteration 79/1000 | Loss: 0.00001143
Iteration 80/1000 | Loss: 0.00001143
Iteration 81/1000 | Loss: 0.00001143
Iteration 82/1000 | Loss: 0.00001143
Iteration 83/1000 | Loss: 0.00001143
Iteration 84/1000 | Loss: 0.00001143
Iteration 85/1000 | Loss: 0.00001143
Iteration 86/1000 | Loss: 0.00001143
Iteration 87/1000 | Loss: 0.00001143
Iteration 88/1000 | Loss: 0.00001143
Iteration 89/1000 | Loss: 0.00001143
Iteration 90/1000 | Loss: 0.00001143
Iteration 91/1000 | Loss: 0.00001143
Iteration 92/1000 | Loss: 0.00001143
Iteration 93/1000 | Loss: 0.00001142
Iteration 94/1000 | Loss: 0.00001142
Iteration 95/1000 | Loss: 0.00001142
Iteration 96/1000 | Loss: 0.00001142
Iteration 97/1000 | Loss: 0.00001142
Iteration 98/1000 | Loss: 0.00001142
Iteration 99/1000 | Loss: 0.00001142
Iteration 100/1000 | Loss: 0.00001142
Iteration 101/1000 | Loss: 0.00001142
Iteration 102/1000 | Loss: 0.00001142
Iteration 103/1000 | Loss: 0.00001142
Iteration 104/1000 | Loss: 0.00001142
Iteration 105/1000 | Loss: 0.00001142
Iteration 106/1000 | Loss: 0.00001142
Iteration 107/1000 | Loss: 0.00001142
Iteration 108/1000 | Loss: 0.00001142
Iteration 109/1000 | Loss: 0.00001142
Iteration 110/1000 | Loss: 0.00001141
Iteration 111/1000 | Loss: 0.00001141
Iteration 112/1000 | Loss: 0.00001141
Iteration 113/1000 | Loss: 0.00001141
Iteration 114/1000 | Loss: 0.00001141
Iteration 115/1000 | Loss: 0.00001141
Iteration 116/1000 | Loss: 0.00001141
Iteration 117/1000 | Loss: 0.00001141
Iteration 118/1000 | Loss: 0.00001141
Iteration 119/1000 | Loss: 0.00001141
Iteration 120/1000 | Loss: 0.00001141
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [1.1411539162509143e-05, 1.1411539162509143e-05, 1.1411539162509143e-05, 1.1411539162509143e-05, 1.1411539162509143e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1411539162509143e-05

Optimization complete. Final v2v error: 2.8831849098205566 mm

Highest mean error: 3.0808799266815186 mm for frame 3

Lowest mean error: 2.7751893997192383 mm for frame 138

Saving results

Total time: 31.883328437805176
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00781516
Iteration 2/25 | Loss: 0.00142706
Iteration 3/25 | Loss: 0.00095551
Iteration 4/25 | Loss: 0.00086333
Iteration 5/25 | Loss: 0.00088585
Iteration 6/25 | Loss: 0.00088897
Iteration 7/25 | Loss: 0.00081659
Iteration 8/25 | Loss: 0.00079539
Iteration 9/25 | Loss: 0.00079165
Iteration 10/25 | Loss: 0.00078712
Iteration 11/25 | Loss: 0.00078893
Iteration 12/25 | Loss: 0.00077915
Iteration 13/25 | Loss: 0.00076859
Iteration 14/25 | Loss: 0.00076398
Iteration 15/25 | Loss: 0.00076233
Iteration 16/25 | Loss: 0.00076199
Iteration 17/25 | Loss: 0.00076195
Iteration 18/25 | Loss: 0.00076195
Iteration 19/25 | Loss: 0.00076194
Iteration 20/25 | Loss: 0.00076194
Iteration 21/25 | Loss: 0.00076194
Iteration 22/25 | Loss: 0.00076194
Iteration 23/25 | Loss: 0.00076194
Iteration 24/25 | Loss: 0.00076194
Iteration 25/25 | Loss: 0.00076194

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47525883
Iteration 2/25 | Loss: 0.00068122
Iteration 3/25 | Loss: 0.00068120
Iteration 4/25 | Loss: 0.00068120
Iteration 5/25 | Loss: 0.00068120
Iteration 6/25 | Loss: 0.00068120
Iteration 7/25 | Loss: 0.00068120
Iteration 8/25 | Loss: 0.00068120
Iteration 9/25 | Loss: 0.00068120
Iteration 10/25 | Loss: 0.00068120
Iteration 11/25 | Loss: 0.00068120
Iteration 12/25 | Loss: 0.00068120
Iteration 13/25 | Loss: 0.00068120
Iteration 14/25 | Loss: 0.00068120
Iteration 15/25 | Loss: 0.00068120
Iteration 16/25 | Loss: 0.00068120
Iteration 17/25 | Loss: 0.00068120
Iteration 18/25 | Loss: 0.00068120
Iteration 19/25 | Loss: 0.00068120
Iteration 20/25 | Loss: 0.00068120
Iteration 21/25 | Loss: 0.00068120
Iteration 22/25 | Loss: 0.00068120
Iteration 23/25 | Loss: 0.00068120
Iteration 24/25 | Loss: 0.00068120
Iteration 25/25 | Loss: 0.00068120

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068120
Iteration 2/1000 | Loss: 0.00007897
Iteration 3/1000 | Loss: 0.00005475
Iteration 4/1000 | Loss: 0.00004809
Iteration 5/1000 | Loss: 0.00004488
Iteration 6/1000 | Loss: 0.00004274
Iteration 7/1000 | Loss: 0.00004071
Iteration 8/1000 | Loss: 0.00003982
Iteration 9/1000 | Loss: 0.00003910
Iteration 10/1000 | Loss: 0.00003856
Iteration 11/1000 | Loss: 0.00003821
Iteration 12/1000 | Loss: 0.00003793
Iteration 13/1000 | Loss: 0.00003770
Iteration 14/1000 | Loss: 0.00046831
Iteration 15/1000 | Loss: 0.00006727
Iteration 16/1000 | Loss: 0.00004236
Iteration 17/1000 | Loss: 0.00004021
Iteration 18/1000 | Loss: 0.00003937
Iteration 19/1000 | Loss: 0.00003868
Iteration 20/1000 | Loss: 0.00003785
Iteration 21/1000 | Loss: 0.00050341
Iteration 22/1000 | Loss: 0.00053638
Iteration 23/1000 | Loss: 0.00096057
Iteration 24/1000 | Loss: 0.00010414
Iteration 25/1000 | Loss: 0.00022913
Iteration 26/1000 | Loss: 0.00014845
Iteration 27/1000 | Loss: 0.00004461
Iteration 28/1000 | Loss: 0.00003887
Iteration 29/1000 | Loss: 0.00003614
Iteration 30/1000 | Loss: 0.00034572
Iteration 31/1000 | Loss: 0.00004005
Iteration 32/1000 | Loss: 0.00003528
Iteration 33/1000 | Loss: 0.00003318
Iteration 34/1000 | Loss: 0.00003238
Iteration 35/1000 | Loss: 0.00003147
Iteration 36/1000 | Loss: 0.00003016
Iteration 37/1000 | Loss: 0.00002967
Iteration 38/1000 | Loss: 0.00002944
Iteration 39/1000 | Loss: 0.00002923
Iteration 40/1000 | Loss: 0.00002915
Iteration 41/1000 | Loss: 0.00002906
Iteration 42/1000 | Loss: 0.00002903
Iteration 43/1000 | Loss: 0.00002888
Iteration 44/1000 | Loss: 0.00002887
Iteration 45/1000 | Loss: 0.00002887
Iteration 46/1000 | Loss: 0.00002886
Iteration 47/1000 | Loss: 0.00002886
Iteration 48/1000 | Loss: 0.00002885
Iteration 49/1000 | Loss: 0.00002885
Iteration 50/1000 | Loss: 0.00002885
Iteration 51/1000 | Loss: 0.00002885
Iteration 52/1000 | Loss: 0.00002884
Iteration 53/1000 | Loss: 0.00002883
Iteration 54/1000 | Loss: 0.00002881
Iteration 55/1000 | Loss: 0.00002880
Iteration 56/1000 | Loss: 0.00002880
Iteration 57/1000 | Loss: 0.00002879
Iteration 58/1000 | Loss: 0.00002879
Iteration 59/1000 | Loss: 0.00002878
Iteration 60/1000 | Loss: 0.00002878
Iteration 61/1000 | Loss: 0.00002877
Iteration 62/1000 | Loss: 0.00002877
Iteration 63/1000 | Loss: 0.00002877
Iteration 64/1000 | Loss: 0.00002875
Iteration 65/1000 | Loss: 0.00002875
Iteration 66/1000 | Loss: 0.00002875
Iteration 67/1000 | Loss: 0.00002875
Iteration 68/1000 | Loss: 0.00002875
Iteration 69/1000 | Loss: 0.00002875
Iteration 70/1000 | Loss: 0.00002875
Iteration 71/1000 | Loss: 0.00002875
Iteration 72/1000 | Loss: 0.00002875
Iteration 73/1000 | Loss: 0.00002875
Iteration 74/1000 | Loss: 0.00002874
Iteration 75/1000 | Loss: 0.00002874
Iteration 76/1000 | Loss: 0.00002874
Iteration 77/1000 | Loss: 0.00002874
Iteration 78/1000 | Loss: 0.00002874
Iteration 79/1000 | Loss: 0.00002872
Iteration 80/1000 | Loss: 0.00002872
Iteration 81/1000 | Loss: 0.00002872
Iteration 82/1000 | Loss: 0.00002871
Iteration 83/1000 | Loss: 0.00002871
Iteration 84/1000 | Loss: 0.00002871
Iteration 85/1000 | Loss: 0.00002871
Iteration 86/1000 | Loss: 0.00002871
Iteration 87/1000 | Loss: 0.00002871
Iteration 88/1000 | Loss: 0.00002871
Iteration 89/1000 | Loss: 0.00002871
Iteration 90/1000 | Loss: 0.00002871
Iteration 91/1000 | Loss: 0.00002871
Iteration 92/1000 | Loss: 0.00002871
Iteration 93/1000 | Loss: 0.00002870
Iteration 94/1000 | Loss: 0.00002870
Iteration 95/1000 | Loss: 0.00002870
Iteration 96/1000 | Loss: 0.00002870
Iteration 97/1000 | Loss: 0.00002870
Iteration 98/1000 | Loss: 0.00002870
Iteration 99/1000 | Loss: 0.00002870
Iteration 100/1000 | Loss: 0.00002870
Iteration 101/1000 | Loss: 0.00002870
Iteration 102/1000 | Loss: 0.00002870
Iteration 103/1000 | Loss: 0.00002870
Iteration 104/1000 | Loss: 0.00002869
Iteration 105/1000 | Loss: 0.00002869
Iteration 106/1000 | Loss: 0.00002869
Iteration 107/1000 | Loss: 0.00002869
Iteration 108/1000 | Loss: 0.00002869
Iteration 109/1000 | Loss: 0.00002869
Iteration 110/1000 | Loss: 0.00002869
Iteration 111/1000 | Loss: 0.00002869
Iteration 112/1000 | Loss: 0.00002869
Iteration 113/1000 | Loss: 0.00002869
Iteration 114/1000 | Loss: 0.00002869
Iteration 115/1000 | Loss: 0.00002869
Iteration 116/1000 | Loss: 0.00002869
Iteration 117/1000 | Loss: 0.00002869
Iteration 118/1000 | Loss: 0.00002868
Iteration 119/1000 | Loss: 0.00002868
Iteration 120/1000 | Loss: 0.00002868
Iteration 121/1000 | Loss: 0.00002868
Iteration 122/1000 | Loss: 0.00002868
Iteration 123/1000 | Loss: 0.00002868
Iteration 124/1000 | Loss: 0.00002868
Iteration 125/1000 | Loss: 0.00002868
Iteration 126/1000 | Loss: 0.00002868
Iteration 127/1000 | Loss: 0.00002868
Iteration 128/1000 | Loss: 0.00002867
Iteration 129/1000 | Loss: 0.00002867
Iteration 130/1000 | Loss: 0.00002867
Iteration 131/1000 | Loss: 0.00002867
Iteration 132/1000 | Loss: 0.00002867
Iteration 133/1000 | Loss: 0.00002867
Iteration 134/1000 | Loss: 0.00002867
Iteration 135/1000 | Loss: 0.00002867
Iteration 136/1000 | Loss: 0.00002867
Iteration 137/1000 | Loss: 0.00002867
Iteration 138/1000 | Loss: 0.00002867
Iteration 139/1000 | Loss: 0.00002866
Iteration 140/1000 | Loss: 0.00002866
Iteration 141/1000 | Loss: 0.00002866
Iteration 142/1000 | Loss: 0.00002866
Iteration 143/1000 | Loss: 0.00002865
Iteration 144/1000 | Loss: 0.00002865
Iteration 145/1000 | Loss: 0.00002865
Iteration 146/1000 | Loss: 0.00002865
Iteration 147/1000 | Loss: 0.00002865
Iteration 148/1000 | Loss: 0.00002865
Iteration 149/1000 | Loss: 0.00002864
Iteration 150/1000 | Loss: 0.00002864
Iteration 151/1000 | Loss: 0.00002864
Iteration 152/1000 | Loss: 0.00002864
Iteration 153/1000 | Loss: 0.00002864
Iteration 154/1000 | Loss: 0.00002864
Iteration 155/1000 | Loss: 0.00002864
Iteration 156/1000 | Loss: 0.00002864
Iteration 157/1000 | Loss: 0.00002864
Iteration 158/1000 | Loss: 0.00002864
Iteration 159/1000 | Loss: 0.00002864
Iteration 160/1000 | Loss: 0.00002863
Iteration 161/1000 | Loss: 0.00002863
Iteration 162/1000 | Loss: 0.00002863
Iteration 163/1000 | Loss: 0.00002863
Iteration 164/1000 | Loss: 0.00002863
Iteration 165/1000 | Loss: 0.00002863
Iteration 166/1000 | Loss: 0.00002862
Iteration 167/1000 | Loss: 0.00002862
Iteration 168/1000 | Loss: 0.00002862
Iteration 169/1000 | Loss: 0.00002862
Iteration 170/1000 | Loss: 0.00002862
Iteration 171/1000 | Loss: 0.00002862
Iteration 172/1000 | Loss: 0.00002862
Iteration 173/1000 | Loss: 0.00002862
Iteration 174/1000 | Loss: 0.00002861
Iteration 175/1000 | Loss: 0.00002861
Iteration 176/1000 | Loss: 0.00002861
Iteration 177/1000 | Loss: 0.00002861
Iteration 178/1000 | Loss: 0.00002861
Iteration 179/1000 | Loss: 0.00002861
Iteration 180/1000 | Loss: 0.00002861
Iteration 181/1000 | Loss: 0.00002860
Iteration 182/1000 | Loss: 0.00002860
Iteration 183/1000 | Loss: 0.00002860
Iteration 184/1000 | Loss: 0.00002860
Iteration 185/1000 | Loss: 0.00002860
Iteration 186/1000 | Loss: 0.00002859
Iteration 187/1000 | Loss: 0.00002859
Iteration 188/1000 | Loss: 0.00002859
Iteration 189/1000 | Loss: 0.00002859
Iteration 190/1000 | Loss: 0.00002859
Iteration 191/1000 | Loss: 0.00002859
Iteration 192/1000 | Loss: 0.00002859
Iteration 193/1000 | Loss: 0.00002858
Iteration 194/1000 | Loss: 0.00002858
Iteration 195/1000 | Loss: 0.00002858
Iteration 196/1000 | Loss: 0.00002858
Iteration 197/1000 | Loss: 0.00002858
Iteration 198/1000 | Loss: 0.00002858
Iteration 199/1000 | Loss: 0.00002858
Iteration 200/1000 | Loss: 0.00002858
Iteration 201/1000 | Loss: 0.00002858
Iteration 202/1000 | Loss: 0.00002857
Iteration 203/1000 | Loss: 0.00002857
Iteration 204/1000 | Loss: 0.00002857
Iteration 205/1000 | Loss: 0.00002857
Iteration 206/1000 | Loss: 0.00002857
Iteration 207/1000 | Loss: 0.00002857
Iteration 208/1000 | Loss: 0.00002857
Iteration 209/1000 | Loss: 0.00002857
Iteration 210/1000 | Loss: 0.00002857
Iteration 211/1000 | Loss: 0.00002857
Iteration 212/1000 | Loss: 0.00002857
Iteration 213/1000 | Loss: 0.00002856
Iteration 214/1000 | Loss: 0.00002856
Iteration 215/1000 | Loss: 0.00002856
Iteration 216/1000 | Loss: 0.00002856
Iteration 217/1000 | Loss: 0.00002856
Iteration 218/1000 | Loss: 0.00002856
Iteration 219/1000 | Loss: 0.00002856
Iteration 220/1000 | Loss: 0.00002856
Iteration 221/1000 | Loss: 0.00002856
Iteration 222/1000 | Loss: 0.00002856
Iteration 223/1000 | Loss: 0.00002856
Iteration 224/1000 | Loss: 0.00002856
Iteration 225/1000 | Loss: 0.00002856
Iteration 226/1000 | Loss: 0.00002856
Iteration 227/1000 | Loss: 0.00002856
Iteration 228/1000 | Loss: 0.00002856
Iteration 229/1000 | Loss: 0.00002856
Iteration 230/1000 | Loss: 0.00002855
Iteration 231/1000 | Loss: 0.00002855
Iteration 232/1000 | Loss: 0.00002855
Iteration 233/1000 | Loss: 0.00002855
Iteration 234/1000 | Loss: 0.00002855
Iteration 235/1000 | Loss: 0.00002855
Iteration 236/1000 | Loss: 0.00002855
Iteration 237/1000 | Loss: 0.00002855
Iteration 238/1000 | Loss: 0.00002855
Iteration 239/1000 | Loss: 0.00002855
Iteration 240/1000 | Loss: 0.00002855
Iteration 241/1000 | Loss: 0.00002855
Iteration 242/1000 | Loss: 0.00002855
Iteration 243/1000 | Loss: 0.00002855
Iteration 244/1000 | Loss: 0.00002855
Iteration 245/1000 | Loss: 0.00002855
Iteration 246/1000 | Loss: 0.00002855
Iteration 247/1000 | Loss: 0.00002855
Iteration 248/1000 | Loss: 0.00002855
Iteration 249/1000 | Loss: 0.00002855
Iteration 250/1000 | Loss: 0.00002855
Iteration 251/1000 | Loss: 0.00002855
Iteration 252/1000 | Loss: 0.00002855
Iteration 253/1000 | Loss: 0.00002855
Iteration 254/1000 | Loss: 0.00002855
Iteration 255/1000 | Loss: 0.00002855
Iteration 256/1000 | Loss: 0.00002855
Iteration 257/1000 | Loss: 0.00002855
Iteration 258/1000 | Loss: 0.00002855
Iteration 259/1000 | Loss: 0.00002855
Iteration 260/1000 | Loss: 0.00002855
Iteration 261/1000 | Loss: 0.00002855
Iteration 262/1000 | Loss: 0.00002855
Iteration 263/1000 | Loss: 0.00002855
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 263. Stopping optimization.
Last 5 losses: [2.85456444544252e-05, 2.85456444544252e-05, 2.85456444544252e-05, 2.85456444544252e-05, 2.85456444544252e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.85456444544252e-05

Optimization complete. Final v2v error: 4.207237720489502 mm

Highest mean error: 6.590024948120117 mm for frame 28

Lowest mean error: 3.4318747520446777 mm for frame 104

Saving results

Total time: 117.237961769104
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00418257
Iteration 2/25 | Loss: 0.00095151
Iteration 3/25 | Loss: 0.00068569
Iteration 4/25 | Loss: 0.00062725
Iteration 5/25 | Loss: 0.00061360
Iteration 6/25 | Loss: 0.00060964
Iteration 7/25 | Loss: 0.00060837
Iteration 8/25 | Loss: 0.00060836
Iteration 9/25 | Loss: 0.00060836
Iteration 10/25 | Loss: 0.00060836
Iteration 11/25 | Loss: 0.00060836
Iteration 12/25 | Loss: 0.00060836
Iteration 13/25 | Loss: 0.00060836
Iteration 14/25 | Loss: 0.00060836
Iteration 15/25 | Loss: 0.00060836
Iteration 16/25 | Loss: 0.00060836
Iteration 17/25 | Loss: 0.00060836
Iteration 18/25 | Loss: 0.00060836
Iteration 19/25 | Loss: 0.00060836
Iteration 20/25 | Loss: 0.00060836
Iteration 21/25 | Loss: 0.00060836
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006083596963435411, 0.0006083596963435411, 0.0006083596963435411, 0.0006083596963435411, 0.0006083596963435411]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006083596963435411

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45440757
Iteration 2/25 | Loss: 0.00027953
Iteration 3/25 | Loss: 0.00027953
Iteration 4/25 | Loss: 0.00027953
Iteration 5/25 | Loss: 0.00027953
Iteration 6/25 | Loss: 0.00027953
Iteration 7/25 | Loss: 0.00027953
Iteration 8/25 | Loss: 0.00027953
Iteration 9/25 | Loss: 0.00027953
Iteration 10/25 | Loss: 0.00027953
Iteration 11/25 | Loss: 0.00027953
Iteration 12/25 | Loss: 0.00027953
Iteration 13/25 | Loss: 0.00027953
Iteration 14/25 | Loss: 0.00027953
Iteration 15/25 | Loss: 0.00027953
Iteration 16/25 | Loss: 0.00027953
Iteration 17/25 | Loss: 0.00027953
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0002795292530208826, 0.0002795292530208826, 0.0002795292530208826, 0.0002795292530208826, 0.0002795292530208826]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002795292530208826

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027953
Iteration 2/1000 | Loss: 0.00002222
Iteration 3/1000 | Loss: 0.00001375
Iteration 4/1000 | Loss: 0.00001250
Iteration 5/1000 | Loss: 0.00001176
Iteration 6/1000 | Loss: 0.00001123
Iteration 7/1000 | Loss: 0.00001095
Iteration 8/1000 | Loss: 0.00001092
Iteration 9/1000 | Loss: 0.00001066
Iteration 10/1000 | Loss: 0.00001054
Iteration 11/1000 | Loss: 0.00001043
Iteration 12/1000 | Loss: 0.00001040
Iteration 13/1000 | Loss: 0.00001036
Iteration 14/1000 | Loss: 0.00001033
Iteration 15/1000 | Loss: 0.00001033
Iteration 16/1000 | Loss: 0.00001032
Iteration 17/1000 | Loss: 0.00001032
Iteration 18/1000 | Loss: 0.00001031
Iteration 19/1000 | Loss: 0.00001031
Iteration 20/1000 | Loss: 0.00001030
Iteration 21/1000 | Loss: 0.00001029
Iteration 22/1000 | Loss: 0.00001029
Iteration 23/1000 | Loss: 0.00001028
Iteration 24/1000 | Loss: 0.00001028
Iteration 25/1000 | Loss: 0.00001027
Iteration 26/1000 | Loss: 0.00001027
Iteration 27/1000 | Loss: 0.00001025
Iteration 28/1000 | Loss: 0.00001025
Iteration 29/1000 | Loss: 0.00001025
Iteration 30/1000 | Loss: 0.00001024
Iteration 31/1000 | Loss: 0.00001023
Iteration 32/1000 | Loss: 0.00001023
Iteration 33/1000 | Loss: 0.00001022
Iteration 34/1000 | Loss: 0.00001022
Iteration 35/1000 | Loss: 0.00001022
Iteration 36/1000 | Loss: 0.00001021
Iteration 37/1000 | Loss: 0.00001020
Iteration 38/1000 | Loss: 0.00001020
Iteration 39/1000 | Loss: 0.00001020
Iteration 40/1000 | Loss: 0.00001019
Iteration 41/1000 | Loss: 0.00001019
Iteration 42/1000 | Loss: 0.00001019
Iteration 43/1000 | Loss: 0.00001019
Iteration 44/1000 | Loss: 0.00001018
Iteration 45/1000 | Loss: 0.00001018
Iteration 46/1000 | Loss: 0.00001017
Iteration 47/1000 | Loss: 0.00001017
Iteration 48/1000 | Loss: 0.00001017
Iteration 49/1000 | Loss: 0.00001017
Iteration 50/1000 | Loss: 0.00001017
Iteration 51/1000 | Loss: 0.00001017
Iteration 52/1000 | Loss: 0.00001016
Iteration 53/1000 | Loss: 0.00001016
Iteration 54/1000 | Loss: 0.00001016
Iteration 55/1000 | Loss: 0.00001015
Iteration 56/1000 | Loss: 0.00001015
Iteration 57/1000 | Loss: 0.00001014
Iteration 58/1000 | Loss: 0.00001013
Iteration 59/1000 | Loss: 0.00001011
Iteration 60/1000 | Loss: 0.00001011
Iteration 61/1000 | Loss: 0.00001011
Iteration 62/1000 | Loss: 0.00001011
Iteration 63/1000 | Loss: 0.00001011
Iteration 64/1000 | Loss: 0.00001011
Iteration 65/1000 | Loss: 0.00001011
Iteration 66/1000 | Loss: 0.00001010
Iteration 67/1000 | Loss: 0.00001010
Iteration 68/1000 | Loss: 0.00001010
Iteration 69/1000 | Loss: 0.00001010
Iteration 70/1000 | Loss: 0.00001010
Iteration 71/1000 | Loss: 0.00001010
Iteration 72/1000 | Loss: 0.00001010
Iteration 73/1000 | Loss: 0.00001010
Iteration 74/1000 | Loss: 0.00001010
Iteration 75/1000 | Loss: 0.00001010
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 75. Stopping optimization.
Last 5 losses: [1.0101554835273419e-05, 1.0101554835273419e-05, 1.0101554835273419e-05, 1.0101554835273419e-05, 1.0101554835273419e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0101554835273419e-05

Optimization complete. Final v2v error: 2.705700635910034 mm

Highest mean error: 3.5062122344970703 mm for frame 90

Lowest mean error: 2.576808214187622 mm for frame 217

Saving results

Total time: 35.467204570770264
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01002062
Iteration 2/25 | Loss: 0.00240447
Iteration 3/25 | Loss: 0.00162788
Iteration 4/25 | Loss: 0.00153251
Iteration 5/25 | Loss: 0.00147629
Iteration 6/25 | Loss: 0.00139575
Iteration 7/25 | Loss: 0.00137167
Iteration 8/25 | Loss: 0.00137072
Iteration 9/25 | Loss: 0.00134495
Iteration 10/25 | Loss: 0.00128829
Iteration 11/25 | Loss: 0.00133979
Iteration 12/25 | Loss: 0.00117196
Iteration 13/25 | Loss: 0.00114769
Iteration 14/25 | Loss: 0.00115067
Iteration 15/25 | Loss: 0.00115252
Iteration 16/25 | Loss: 0.00113624
Iteration 17/25 | Loss: 0.00112952
Iteration 18/25 | Loss: 0.00112775
Iteration 19/25 | Loss: 0.00112690
Iteration 20/25 | Loss: 0.00112679
Iteration 21/25 | Loss: 0.00112679
Iteration 22/25 | Loss: 0.00112678
Iteration 23/25 | Loss: 0.00112678
Iteration 24/25 | Loss: 0.00112678
Iteration 25/25 | Loss: 0.00112678

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45539868
Iteration 2/25 | Loss: 0.00270416
Iteration 3/25 | Loss: 0.00270415
Iteration 4/25 | Loss: 0.00270415
Iteration 5/25 | Loss: 0.00270415
Iteration 6/25 | Loss: 0.00270415
Iteration 7/25 | Loss: 0.00270415
Iteration 8/25 | Loss: 0.00270415
Iteration 9/25 | Loss: 0.00270415
Iteration 10/25 | Loss: 0.00270415
Iteration 11/25 | Loss: 0.00270415
Iteration 12/25 | Loss: 0.00270415
Iteration 13/25 | Loss: 0.00270415
Iteration 14/25 | Loss: 0.00270415
Iteration 15/25 | Loss: 0.00270415
Iteration 16/25 | Loss: 0.00270415
Iteration 17/25 | Loss: 0.00270415
Iteration 18/25 | Loss: 0.00270415
Iteration 19/25 | Loss: 0.00270415
Iteration 20/25 | Loss: 0.00270415
Iteration 21/25 | Loss: 0.00270415
Iteration 22/25 | Loss: 0.00270415
Iteration 23/25 | Loss: 0.00270415
Iteration 24/25 | Loss: 0.00270415
Iteration 25/25 | Loss: 0.00270415

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00270415
Iteration 2/1000 | Loss: 0.00050209
Iteration 3/1000 | Loss: 0.00036423
Iteration 4/1000 | Loss: 0.00029549
Iteration 5/1000 | Loss: 0.00026189
Iteration 6/1000 | Loss: 0.00024102
Iteration 7/1000 | Loss: 0.00050956
Iteration 8/1000 | Loss: 0.00239797
Iteration 9/1000 | Loss: 0.00939840
Iteration 10/1000 | Loss: 0.00039576
Iteration 11/1000 | Loss: 0.00025553
Iteration 12/1000 | Loss: 0.00015486
Iteration 13/1000 | Loss: 0.00009461
Iteration 14/1000 | Loss: 0.00006265
Iteration 15/1000 | Loss: 0.00004433
Iteration 16/1000 | Loss: 0.00003438
Iteration 17/1000 | Loss: 0.00002814
Iteration 18/1000 | Loss: 0.00002342
Iteration 19/1000 | Loss: 0.00002040
Iteration 20/1000 | Loss: 0.00001821
Iteration 21/1000 | Loss: 0.00001644
Iteration 22/1000 | Loss: 0.00001515
Iteration 23/1000 | Loss: 0.00001429
Iteration 24/1000 | Loss: 0.00001371
Iteration 25/1000 | Loss: 0.00001325
Iteration 26/1000 | Loss: 0.00001295
Iteration 27/1000 | Loss: 0.00001269
Iteration 28/1000 | Loss: 0.00001267
Iteration 29/1000 | Loss: 0.00001257
Iteration 30/1000 | Loss: 0.00001250
Iteration 31/1000 | Loss: 0.00001248
Iteration 32/1000 | Loss: 0.00001247
Iteration 33/1000 | Loss: 0.00001246
Iteration 34/1000 | Loss: 0.00001246
Iteration 35/1000 | Loss: 0.00001245
Iteration 36/1000 | Loss: 0.00001244
Iteration 37/1000 | Loss: 0.00001243
Iteration 38/1000 | Loss: 0.00001243
Iteration 39/1000 | Loss: 0.00001242
Iteration 40/1000 | Loss: 0.00001241
Iteration 41/1000 | Loss: 0.00001239
Iteration 42/1000 | Loss: 0.00001238
Iteration 43/1000 | Loss: 0.00001238
Iteration 44/1000 | Loss: 0.00001238
Iteration 45/1000 | Loss: 0.00001238
Iteration 46/1000 | Loss: 0.00001238
Iteration 47/1000 | Loss: 0.00001238
Iteration 48/1000 | Loss: 0.00001238
Iteration 49/1000 | Loss: 0.00001238
Iteration 50/1000 | Loss: 0.00001238
Iteration 51/1000 | Loss: 0.00001238
Iteration 52/1000 | Loss: 0.00001238
Iteration 53/1000 | Loss: 0.00001238
Iteration 54/1000 | Loss: 0.00001238
Iteration 55/1000 | Loss: 0.00001238
Iteration 56/1000 | Loss: 0.00001238
Iteration 57/1000 | Loss: 0.00001238
Iteration 58/1000 | Loss: 0.00001238
Iteration 59/1000 | Loss: 0.00001238
Iteration 60/1000 | Loss: 0.00001238
Iteration 61/1000 | Loss: 0.00001238
Iteration 62/1000 | Loss: 0.00001238
Iteration 63/1000 | Loss: 0.00001238
Iteration 64/1000 | Loss: 0.00001238
Iteration 65/1000 | Loss: 0.00001238
Iteration 66/1000 | Loss: 0.00001238
Iteration 67/1000 | Loss: 0.00001238
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 67. Stopping optimization.
Last 5 losses: [1.2375023288768716e-05, 1.2375023288768716e-05, 1.2375023288768716e-05, 1.2375023288768716e-05, 1.2375023288768716e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2375023288768716e-05

Optimization complete. Final v2v error: 2.9911110401153564 mm

Highest mean error: 3.177722454071045 mm for frame 67

Lowest mean error: 2.7687840461730957 mm for frame 6

Saving results

Total time: 77.27595496177673
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00400012
Iteration 2/25 | Loss: 0.00087031
Iteration 3/25 | Loss: 0.00074873
Iteration 4/25 | Loss: 0.00072316
Iteration 5/25 | Loss: 0.00071538
Iteration 6/25 | Loss: 0.00071393
Iteration 7/25 | Loss: 0.00071362
Iteration 8/25 | Loss: 0.00071362
Iteration 9/25 | Loss: 0.00071362
Iteration 10/25 | Loss: 0.00071362
Iteration 11/25 | Loss: 0.00071362
Iteration 12/25 | Loss: 0.00071362
Iteration 13/25 | Loss: 0.00071362
Iteration 14/25 | Loss: 0.00071362
Iteration 15/25 | Loss: 0.00071362
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0007136237691156566, 0.0007136237691156566, 0.0007136237691156566, 0.0007136237691156566, 0.0007136237691156566]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007136237691156566

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.06579494
Iteration 2/25 | Loss: 0.00032139
Iteration 3/25 | Loss: 0.00032138
Iteration 4/25 | Loss: 0.00032138
Iteration 5/25 | Loss: 0.00032138
Iteration 6/25 | Loss: 0.00032138
Iteration 7/25 | Loss: 0.00032138
Iteration 8/25 | Loss: 0.00032138
Iteration 9/25 | Loss: 0.00032138
Iteration 10/25 | Loss: 0.00032138
Iteration 11/25 | Loss: 0.00032138
Iteration 12/25 | Loss: 0.00032138
Iteration 13/25 | Loss: 0.00032138
Iteration 14/25 | Loss: 0.00032138
Iteration 15/25 | Loss: 0.00032138
Iteration 16/25 | Loss: 0.00032138
Iteration 17/25 | Loss: 0.00032138
Iteration 18/25 | Loss: 0.00032138
Iteration 19/25 | Loss: 0.00032138
Iteration 20/25 | Loss: 0.00032138
Iteration 21/25 | Loss: 0.00032138
Iteration 22/25 | Loss: 0.00032138
Iteration 23/25 | Loss: 0.00032138
Iteration 24/25 | Loss: 0.00032138
Iteration 25/25 | Loss: 0.00032138

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032138
Iteration 2/1000 | Loss: 0.00004356
Iteration 3/1000 | Loss: 0.00003277
Iteration 4/1000 | Loss: 0.00003090
Iteration 5/1000 | Loss: 0.00002915
Iteration 6/1000 | Loss: 0.00002812
Iteration 7/1000 | Loss: 0.00002732
Iteration 8/1000 | Loss: 0.00002689
Iteration 9/1000 | Loss: 0.00002662
Iteration 10/1000 | Loss: 0.00002640
Iteration 11/1000 | Loss: 0.00002637
Iteration 12/1000 | Loss: 0.00002619
Iteration 13/1000 | Loss: 0.00002610
Iteration 14/1000 | Loss: 0.00002603
Iteration 15/1000 | Loss: 0.00002599
Iteration 16/1000 | Loss: 0.00002598
Iteration 17/1000 | Loss: 0.00002597
Iteration 18/1000 | Loss: 0.00002597
Iteration 19/1000 | Loss: 0.00002596
Iteration 20/1000 | Loss: 0.00002595
Iteration 21/1000 | Loss: 0.00002595
Iteration 22/1000 | Loss: 0.00002595
Iteration 23/1000 | Loss: 0.00002594
Iteration 24/1000 | Loss: 0.00002594
Iteration 25/1000 | Loss: 0.00002594
Iteration 26/1000 | Loss: 0.00002593
Iteration 27/1000 | Loss: 0.00002591
Iteration 28/1000 | Loss: 0.00002590
Iteration 29/1000 | Loss: 0.00002588
Iteration 30/1000 | Loss: 0.00002587
Iteration 31/1000 | Loss: 0.00002585
Iteration 32/1000 | Loss: 0.00002585
Iteration 33/1000 | Loss: 0.00002584
Iteration 34/1000 | Loss: 0.00002584
Iteration 35/1000 | Loss: 0.00002583
Iteration 36/1000 | Loss: 0.00002583
Iteration 37/1000 | Loss: 0.00002582
Iteration 38/1000 | Loss: 0.00002582
Iteration 39/1000 | Loss: 0.00002580
Iteration 40/1000 | Loss: 0.00002580
Iteration 41/1000 | Loss: 0.00002580
Iteration 42/1000 | Loss: 0.00002579
Iteration 43/1000 | Loss: 0.00002579
Iteration 44/1000 | Loss: 0.00002578
Iteration 45/1000 | Loss: 0.00002578
Iteration 46/1000 | Loss: 0.00002576
Iteration 47/1000 | Loss: 0.00002575
Iteration 48/1000 | Loss: 0.00002575
Iteration 49/1000 | Loss: 0.00002574
Iteration 50/1000 | Loss: 0.00002574
Iteration 51/1000 | Loss: 0.00002574
Iteration 52/1000 | Loss: 0.00002573
Iteration 53/1000 | Loss: 0.00002572
Iteration 54/1000 | Loss: 0.00002571
Iteration 55/1000 | Loss: 0.00002571
Iteration 56/1000 | Loss: 0.00002571
Iteration 57/1000 | Loss: 0.00002570
Iteration 58/1000 | Loss: 0.00002570
Iteration 59/1000 | Loss: 0.00002570
Iteration 60/1000 | Loss: 0.00002570
Iteration 61/1000 | Loss: 0.00002569
Iteration 62/1000 | Loss: 0.00002569
Iteration 63/1000 | Loss: 0.00002568
Iteration 64/1000 | Loss: 0.00002568
Iteration 65/1000 | Loss: 0.00002568
Iteration 66/1000 | Loss: 0.00002568
Iteration 67/1000 | Loss: 0.00002568
Iteration 68/1000 | Loss: 0.00002568
Iteration 69/1000 | Loss: 0.00002568
Iteration 70/1000 | Loss: 0.00002567
Iteration 71/1000 | Loss: 0.00002567
Iteration 72/1000 | Loss: 0.00002567
Iteration 73/1000 | Loss: 0.00002567
Iteration 74/1000 | Loss: 0.00002566
Iteration 75/1000 | Loss: 0.00002566
Iteration 76/1000 | Loss: 0.00002566
Iteration 77/1000 | Loss: 0.00002566
Iteration 78/1000 | Loss: 0.00002566
Iteration 79/1000 | Loss: 0.00002566
Iteration 80/1000 | Loss: 0.00002566
Iteration 81/1000 | Loss: 0.00002565
Iteration 82/1000 | Loss: 0.00002565
Iteration 83/1000 | Loss: 0.00002565
Iteration 84/1000 | Loss: 0.00002565
Iteration 85/1000 | Loss: 0.00002565
Iteration 86/1000 | Loss: 0.00002565
Iteration 87/1000 | Loss: 0.00002564
Iteration 88/1000 | Loss: 0.00002564
Iteration 89/1000 | Loss: 0.00002564
Iteration 90/1000 | Loss: 0.00002564
Iteration 91/1000 | Loss: 0.00002564
Iteration 92/1000 | Loss: 0.00002564
Iteration 93/1000 | Loss: 0.00002564
Iteration 94/1000 | Loss: 0.00002563
Iteration 95/1000 | Loss: 0.00002563
Iteration 96/1000 | Loss: 0.00002563
Iteration 97/1000 | Loss: 0.00002563
Iteration 98/1000 | Loss: 0.00002563
Iteration 99/1000 | Loss: 0.00002563
Iteration 100/1000 | Loss: 0.00002563
Iteration 101/1000 | Loss: 0.00002562
Iteration 102/1000 | Loss: 0.00002562
Iteration 103/1000 | Loss: 0.00002562
Iteration 104/1000 | Loss: 0.00002562
Iteration 105/1000 | Loss: 0.00002562
Iteration 106/1000 | Loss: 0.00002562
Iteration 107/1000 | Loss: 0.00002561
Iteration 108/1000 | Loss: 0.00002561
Iteration 109/1000 | Loss: 0.00002561
Iteration 110/1000 | Loss: 0.00002561
Iteration 111/1000 | Loss: 0.00002561
Iteration 112/1000 | Loss: 0.00002561
Iteration 113/1000 | Loss: 0.00002561
Iteration 114/1000 | Loss: 0.00002561
Iteration 115/1000 | Loss: 0.00002560
Iteration 116/1000 | Loss: 0.00002560
Iteration 117/1000 | Loss: 0.00002560
Iteration 118/1000 | Loss: 0.00002560
Iteration 119/1000 | Loss: 0.00002560
Iteration 120/1000 | Loss: 0.00002560
Iteration 121/1000 | Loss: 0.00002560
Iteration 122/1000 | Loss: 0.00002560
Iteration 123/1000 | Loss: 0.00002560
Iteration 124/1000 | Loss: 0.00002560
Iteration 125/1000 | Loss: 0.00002560
Iteration 126/1000 | Loss: 0.00002560
Iteration 127/1000 | Loss: 0.00002560
Iteration 128/1000 | Loss: 0.00002560
Iteration 129/1000 | Loss: 0.00002560
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [2.5598401407478377e-05, 2.5598401407478377e-05, 2.5598401407478377e-05, 2.5598401407478377e-05, 2.5598401407478377e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5598401407478377e-05

Optimization complete. Final v2v error: 4.1973114013671875 mm

Highest mean error: 4.659225940704346 mm for frame 29

Lowest mean error: 3.810396432876587 mm for frame 41

Saving results

Total time: 37.33650994300842
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00424477
Iteration 2/25 | Loss: 0.00080374
Iteration 3/25 | Loss: 0.00063020
Iteration 4/25 | Loss: 0.00061448
Iteration 5/25 | Loss: 0.00060864
Iteration 6/25 | Loss: 0.00060667
Iteration 7/25 | Loss: 0.00060665
Iteration 8/25 | Loss: 0.00060665
Iteration 9/25 | Loss: 0.00060665
Iteration 10/25 | Loss: 0.00060665
Iteration 11/25 | Loss: 0.00060665
Iteration 12/25 | Loss: 0.00060665
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006066464120522141, 0.0006066464120522141, 0.0006066464120522141, 0.0006066464120522141, 0.0006066464120522141]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006066464120522141

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48831761
Iteration 2/25 | Loss: 0.00029699
Iteration 3/25 | Loss: 0.00029699
Iteration 4/25 | Loss: 0.00029699
Iteration 5/25 | Loss: 0.00029699
Iteration 6/25 | Loss: 0.00029699
Iteration 7/25 | Loss: 0.00029699
Iteration 8/25 | Loss: 0.00029699
Iteration 9/25 | Loss: 0.00029699
Iteration 10/25 | Loss: 0.00029699
Iteration 11/25 | Loss: 0.00029699
Iteration 12/25 | Loss: 0.00029699
Iteration 13/25 | Loss: 0.00029699
Iteration 14/25 | Loss: 0.00029699
Iteration 15/25 | Loss: 0.00029699
Iteration 16/25 | Loss: 0.00029699
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000296986399916932, 0.000296986399916932, 0.000296986399916932, 0.000296986399916932, 0.000296986399916932]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000296986399916932

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029699
Iteration 2/1000 | Loss: 0.00001851
Iteration 3/1000 | Loss: 0.00001353
Iteration 4/1000 | Loss: 0.00001260
Iteration 5/1000 | Loss: 0.00001214
Iteration 6/1000 | Loss: 0.00001188
Iteration 7/1000 | Loss: 0.00001167
Iteration 8/1000 | Loss: 0.00001162
Iteration 9/1000 | Loss: 0.00001157
Iteration 10/1000 | Loss: 0.00001157
Iteration 11/1000 | Loss: 0.00001156
Iteration 12/1000 | Loss: 0.00001151
Iteration 13/1000 | Loss: 0.00001150
Iteration 14/1000 | Loss: 0.00001149
Iteration 15/1000 | Loss: 0.00001143
Iteration 16/1000 | Loss: 0.00001142
Iteration 17/1000 | Loss: 0.00001141
Iteration 18/1000 | Loss: 0.00001140
Iteration 19/1000 | Loss: 0.00001139
Iteration 20/1000 | Loss: 0.00001139
Iteration 21/1000 | Loss: 0.00001137
Iteration 22/1000 | Loss: 0.00001137
Iteration 23/1000 | Loss: 0.00001137
Iteration 24/1000 | Loss: 0.00001137
Iteration 25/1000 | Loss: 0.00001137
Iteration 26/1000 | Loss: 0.00001137
Iteration 27/1000 | Loss: 0.00001137
Iteration 28/1000 | Loss: 0.00001136
Iteration 29/1000 | Loss: 0.00001136
Iteration 30/1000 | Loss: 0.00001135
Iteration 31/1000 | Loss: 0.00001135
Iteration 32/1000 | Loss: 0.00001133
Iteration 33/1000 | Loss: 0.00001133
Iteration 34/1000 | Loss: 0.00001133
Iteration 35/1000 | Loss: 0.00001132
Iteration 36/1000 | Loss: 0.00001132
Iteration 37/1000 | Loss: 0.00001132
Iteration 38/1000 | Loss: 0.00001131
Iteration 39/1000 | Loss: 0.00001131
Iteration 40/1000 | Loss: 0.00001131
Iteration 41/1000 | Loss: 0.00001131
Iteration 42/1000 | Loss: 0.00001130
Iteration 43/1000 | Loss: 0.00001130
Iteration 44/1000 | Loss: 0.00001129
Iteration 45/1000 | Loss: 0.00001129
Iteration 46/1000 | Loss: 0.00001128
Iteration 47/1000 | Loss: 0.00001128
Iteration 48/1000 | Loss: 0.00001128
Iteration 49/1000 | Loss: 0.00001128
Iteration 50/1000 | Loss: 0.00001128
Iteration 51/1000 | Loss: 0.00001128
Iteration 52/1000 | Loss: 0.00001128
Iteration 53/1000 | Loss: 0.00001127
Iteration 54/1000 | Loss: 0.00001127
Iteration 55/1000 | Loss: 0.00001127
Iteration 56/1000 | Loss: 0.00001127
Iteration 57/1000 | Loss: 0.00001127
Iteration 58/1000 | Loss: 0.00001126
Iteration 59/1000 | Loss: 0.00001125
Iteration 60/1000 | Loss: 0.00001125
Iteration 61/1000 | Loss: 0.00001125
Iteration 62/1000 | Loss: 0.00001125
Iteration 63/1000 | Loss: 0.00001125
Iteration 64/1000 | Loss: 0.00001123
Iteration 65/1000 | Loss: 0.00001123
Iteration 66/1000 | Loss: 0.00001122
Iteration 67/1000 | Loss: 0.00001122
Iteration 68/1000 | Loss: 0.00001122
Iteration 69/1000 | Loss: 0.00001121
Iteration 70/1000 | Loss: 0.00001121
Iteration 71/1000 | Loss: 0.00001121
Iteration 72/1000 | Loss: 0.00001121
Iteration 73/1000 | Loss: 0.00001121
Iteration 74/1000 | Loss: 0.00001120
Iteration 75/1000 | Loss: 0.00001120
Iteration 76/1000 | Loss: 0.00001120
Iteration 77/1000 | Loss: 0.00001120
Iteration 78/1000 | Loss: 0.00001120
Iteration 79/1000 | Loss: 0.00001120
Iteration 80/1000 | Loss: 0.00001119
Iteration 81/1000 | Loss: 0.00001119
Iteration 82/1000 | Loss: 0.00001119
Iteration 83/1000 | Loss: 0.00001119
Iteration 84/1000 | Loss: 0.00001119
Iteration 85/1000 | Loss: 0.00001119
Iteration 86/1000 | Loss: 0.00001119
Iteration 87/1000 | Loss: 0.00001119
Iteration 88/1000 | Loss: 0.00001119
Iteration 89/1000 | Loss: 0.00001119
Iteration 90/1000 | Loss: 0.00001119
Iteration 91/1000 | Loss: 0.00001119
Iteration 92/1000 | Loss: 0.00001119
Iteration 93/1000 | Loss: 0.00001118
Iteration 94/1000 | Loss: 0.00001118
Iteration 95/1000 | Loss: 0.00001118
Iteration 96/1000 | Loss: 0.00001118
Iteration 97/1000 | Loss: 0.00001118
Iteration 98/1000 | Loss: 0.00001118
Iteration 99/1000 | Loss: 0.00001118
Iteration 100/1000 | Loss: 0.00001117
Iteration 101/1000 | Loss: 0.00001117
Iteration 102/1000 | Loss: 0.00001117
Iteration 103/1000 | Loss: 0.00001117
Iteration 104/1000 | Loss: 0.00001117
Iteration 105/1000 | Loss: 0.00001117
Iteration 106/1000 | Loss: 0.00001117
Iteration 107/1000 | Loss: 0.00001117
Iteration 108/1000 | Loss: 0.00001117
Iteration 109/1000 | Loss: 0.00001117
Iteration 110/1000 | Loss: 0.00001116
Iteration 111/1000 | Loss: 0.00001116
Iteration 112/1000 | Loss: 0.00001116
Iteration 113/1000 | Loss: 0.00001116
Iteration 114/1000 | Loss: 0.00001116
Iteration 115/1000 | Loss: 0.00001116
Iteration 116/1000 | Loss: 0.00001116
Iteration 117/1000 | Loss: 0.00001116
Iteration 118/1000 | Loss: 0.00001116
Iteration 119/1000 | Loss: 0.00001116
Iteration 120/1000 | Loss: 0.00001116
Iteration 121/1000 | Loss: 0.00001115
Iteration 122/1000 | Loss: 0.00001115
Iteration 123/1000 | Loss: 0.00001115
Iteration 124/1000 | Loss: 0.00001115
Iteration 125/1000 | Loss: 0.00001115
Iteration 126/1000 | Loss: 0.00001115
Iteration 127/1000 | Loss: 0.00001115
Iteration 128/1000 | Loss: 0.00001115
Iteration 129/1000 | Loss: 0.00001115
Iteration 130/1000 | Loss: 0.00001115
Iteration 131/1000 | Loss: 0.00001115
Iteration 132/1000 | Loss: 0.00001115
Iteration 133/1000 | Loss: 0.00001115
Iteration 134/1000 | Loss: 0.00001115
Iteration 135/1000 | Loss: 0.00001115
Iteration 136/1000 | Loss: 0.00001114
Iteration 137/1000 | Loss: 0.00001114
Iteration 138/1000 | Loss: 0.00001114
Iteration 139/1000 | Loss: 0.00001114
Iteration 140/1000 | Loss: 0.00001114
Iteration 141/1000 | Loss: 0.00001114
Iteration 142/1000 | Loss: 0.00001114
Iteration 143/1000 | Loss: 0.00001114
Iteration 144/1000 | Loss: 0.00001114
Iteration 145/1000 | Loss: 0.00001113
Iteration 146/1000 | Loss: 0.00001113
Iteration 147/1000 | Loss: 0.00001113
Iteration 148/1000 | Loss: 0.00001113
Iteration 149/1000 | Loss: 0.00001112
Iteration 150/1000 | Loss: 0.00001112
Iteration 151/1000 | Loss: 0.00001112
Iteration 152/1000 | Loss: 0.00001111
Iteration 153/1000 | Loss: 0.00001111
Iteration 154/1000 | Loss: 0.00001111
Iteration 155/1000 | Loss: 0.00001111
Iteration 156/1000 | Loss: 0.00001110
Iteration 157/1000 | Loss: 0.00001110
Iteration 158/1000 | Loss: 0.00001110
Iteration 159/1000 | Loss: 0.00001110
Iteration 160/1000 | Loss: 0.00001110
Iteration 161/1000 | Loss: 0.00001110
Iteration 162/1000 | Loss: 0.00001110
Iteration 163/1000 | Loss: 0.00001110
Iteration 164/1000 | Loss: 0.00001110
Iteration 165/1000 | Loss: 0.00001110
Iteration 166/1000 | Loss: 0.00001110
Iteration 167/1000 | Loss: 0.00001110
Iteration 168/1000 | Loss: 0.00001109
Iteration 169/1000 | Loss: 0.00001109
Iteration 170/1000 | Loss: 0.00001109
Iteration 171/1000 | Loss: 0.00001108
Iteration 172/1000 | Loss: 0.00001108
Iteration 173/1000 | Loss: 0.00001108
Iteration 174/1000 | Loss: 0.00001108
Iteration 175/1000 | Loss: 0.00001108
Iteration 176/1000 | Loss: 0.00001108
Iteration 177/1000 | Loss: 0.00001108
Iteration 178/1000 | Loss: 0.00001108
Iteration 179/1000 | Loss: 0.00001108
Iteration 180/1000 | Loss: 0.00001108
Iteration 181/1000 | Loss: 0.00001108
Iteration 182/1000 | Loss: 0.00001108
Iteration 183/1000 | Loss: 0.00001108
Iteration 184/1000 | Loss: 0.00001108
Iteration 185/1000 | Loss: 0.00001108
Iteration 186/1000 | Loss: 0.00001108
Iteration 187/1000 | Loss: 0.00001108
Iteration 188/1000 | Loss: 0.00001108
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [1.1083556273661088e-05, 1.1083556273661088e-05, 1.1083556273661088e-05, 1.1083556273661088e-05, 1.1083556273661088e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1083556273661088e-05

Optimization complete. Final v2v error: 2.7907769680023193 mm

Highest mean error: 2.949725866317749 mm for frame 195

Lowest mean error: 2.67625093460083 mm for frame 230

Saving results

Total time: 37.99911570549011
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00548445
Iteration 2/25 | Loss: 0.00079919
Iteration 3/25 | Loss: 0.00065997
Iteration 4/25 | Loss: 0.00063534
Iteration 5/25 | Loss: 0.00062751
Iteration 6/25 | Loss: 0.00062587
Iteration 7/25 | Loss: 0.00062537
Iteration 8/25 | Loss: 0.00062535
Iteration 9/25 | Loss: 0.00062535
Iteration 10/25 | Loss: 0.00062535
Iteration 11/25 | Loss: 0.00062535
Iteration 12/25 | Loss: 0.00062535
Iteration 13/25 | Loss: 0.00062535
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006253527826629579, 0.0006253527826629579, 0.0006253527826629579, 0.0006253527826629579, 0.0006253527826629579]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006253527826629579

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.49291134
Iteration 2/25 | Loss: 0.00025960
Iteration 3/25 | Loss: 0.00025958
Iteration 4/25 | Loss: 0.00025958
Iteration 5/25 | Loss: 0.00025958
Iteration 6/25 | Loss: 0.00025958
Iteration 7/25 | Loss: 0.00025958
Iteration 8/25 | Loss: 0.00025958
Iteration 9/25 | Loss: 0.00025958
Iteration 10/25 | Loss: 0.00025958
Iteration 11/25 | Loss: 0.00025958
Iteration 12/25 | Loss: 0.00025958
Iteration 13/25 | Loss: 0.00025958
Iteration 14/25 | Loss: 0.00025958
Iteration 15/25 | Loss: 0.00025958
Iteration 16/25 | Loss: 0.00025958
Iteration 17/25 | Loss: 0.00025958
Iteration 18/25 | Loss: 0.00025958
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0002595808473415673, 0.0002595808473415673, 0.0002595808473415673, 0.0002595808473415673, 0.0002595808473415673]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002595808473415673

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025958
Iteration 2/1000 | Loss: 0.00002691
Iteration 3/1000 | Loss: 0.00001911
Iteration 4/1000 | Loss: 0.00001787
Iteration 5/1000 | Loss: 0.00001693
Iteration 6/1000 | Loss: 0.00001644
Iteration 7/1000 | Loss: 0.00001601
Iteration 8/1000 | Loss: 0.00001573
Iteration 9/1000 | Loss: 0.00001557
Iteration 10/1000 | Loss: 0.00001550
Iteration 11/1000 | Loss: 0.00001544
Iteration 12/1000 | Loss: 0.00001544
Iteration 13/1000 | Loss: 0.00001543
Iteration 14/1000 | Loss: 0.00001543
Iteration 15/1000 | Loss: 0.00001538
Iteration 16/1000 | Loss: 0.00001533
Iteration 17/1000 | Loss: 0.00001529
Iteration 18/1000 | Loss: 0.00001527
Iteration 19/1000 | Loss: 0.00001524
Iteration 20/1000 | Loss: 0.00001522
Iteration 21/1000 | Loss: 0.00001520
Iteration 22/1000 | Loss: 0.00001520
Iteration 23/1000 | Loss: 0.00001519
Iteration 24/1000 | Loss: 0.00001518
Iteration 25/1000 | Loss: 0.00001518
Iteration 26/1000 | Loss: 0.00001517
Iteration 27/1000 | Loss: 0.00001517
Iteration 28/1000 | Loss: 0.00001516
Iteration 29/1000 | Loss: 0.00001516
Iteration 30/1000 | Loss: 0.00001516
Iteration 31/1000 | Loss: 0.00001516
Iteration 32/1000 | Loss: 0.00001515
Iteration 33/1000 | Loss: 0.00001515
Iteration 34/1000 | Loss: 0.00001515
Iteration 35/1000 | Loss: 0.00001515
Iteration 36/1000 | Loss: 0.00001515
Iteration 37/1000 | Loss: 0.00001515
Iteration 38/1000 | Loss: 0.00001515
Iteration 39/1000 | Loss: 0.00001515
Iteration 40/1000 | Loss: 0.00001514
Iteration 41/1000 | Loss: 0.00001514
Iteration 42/1000 | Loss: 0.00001513
Iteration 43/1000 | Loss: 0.00001513
Iteration 44/1000 | Loss: 0.00001513
Iteration 45/1000 | Loss: 0.00001513
Iteration 46/1000 | Loss: 0.00001513
Iteration 47/1000 | Loss: 0.00001513
Iteration 48/1000 | Loss: 0.00001512
Iteration 49/1000 | Loss: 0.00001512
Iteration 50/1000 | Loss: 0.00001512
Iteration 51/1000 | Loss: 0.00001511
Iteration 52/1000 | Loss: 0.00001511
Iteration 53/1000 | Loss: 0.00001510
Iteration 54/1000 | Loss: 0.00001510
Iteration 55/1000 | Loss: 0.00001510
Iteration 56/1000 | Loss: 0.00001510
Iteration 57/1000 | Loss: 0.00001510
Iteration 58/1000 | Loss: 0.00001509
Iteration 59/1000 | Loss: 0.00001509
Iteration 60/1000 | Loss: 0.00001508
Iteration 61/1000 | Loss: 0.00001508
Iteration 62/1000 | Loss: 0.00001508
Iteration 63/1000 | Loss: 0.00001506
Iteration 64/1000 | Loss: 0.00001506
Iteration 65/1000 | Loss: 0.00001506
Iteration 66/1000 | Loss: 0.00001506
Iteration 67/1000 | Loss: 0.00001505
Iteration 68/1000 | Loss: 0.00001505
Iteration 69/1000 | Loss: 0.00001505
Iteration 70/1000 | Loss: 0.00001505
Iteration 71/1000 | Loss: 0.00001505
Iteration 72/1000 | Loss: 0.00001504
Iteration 73/1000 | Loss: 0.00001503
Iteration 74/1000 | Loss: 0.00001503
Iteration 75/1000 | Loss: 0.00001502
Iteration 76/1000 | Loss: 0.00001502
Iteration 77/1000 | Loss: 0.00001502
Iteration 78/1000 | Loss: 0.00001501
Iteration 79/1000 | Loss: 0.00001501
Iteration 80/1000 | Loss: 0.00001501
Iteration 81/1000 | Loss: 0.00001500
Iteration 82/1000 | Loss: 0.00001500
Iteration 83/1000 | Loss: 0.00001500
Iteration 84/1000 | Loss: 0.00001500
Iteration 85/1000 | Loss: 0.00001499
Iteration 86/1000 | Loss: 0.00001499
Iteration 87/1000 | Loss: 0.00001499
Iteration 88/1000 | Loss: 0.00001499
Iteration 89/1000 | Loss: 0.00001499
Iteration 90/1000 | Loss: 0.00001499
Iteration 91/1000 | Loss: 0.00001499
Iteration 92/1000 | Loss: 0.00001499
Iteration 93/1000 | Loss: 0.00001498
Iteration 94/1000 | Loss: 0.00001498
Iteration 95/1000 | Loss: 0.00001498
Iteration 96/1000 | Loss: 0.00001497
Iteration 97/1000 | Loss: 0.00001497
Iteration 98/1000 | Loss: 0.00001497
Iteration 99/1000 | Loss: 0.00001497
Iteration 100/1000 | Loss: 0.00001496
Iteration 101/1000 | Loss: 0.00001496
Iteration 102/1000 | Loss: 0.00001496
Iteration 103/1000 | Loss: 0.00001496
Iteration 104/1000 | Loss: 0.00001496
Iteration 105/1000 | Loss: 0.00001496
Iteration 106/1000 | Loss: 0.00001496
Iteration 107/1000 | Loss: 0.00001495
Iteration 108/1000 | Loss: 0.00001495
Iteration 109/1000 | Loss: 0.00001495
Iteration 110/1000 | Loss: 0.00001495
Iteration 111/1000 | Loss: 0.00001495
Iteration 112/1000 | Loss: 0.00001495
Iteration 113/1000 | Loss: 0.00001495
Iteration 114/1000 | Loss: 0.00001495
Iteration 115/1000 | Loss: 0.00001495
Iteration 116/1000 | Loss: 0.00001495
Iteration 117/1000 | Loss: 0.00001495
Iteration 118/1000 | Loss: 0.00001495
Iteration 119/1000 | Loss: 0.00001495
Iteration 120/1000 | Loss: 0.00001495
Iteration 121/1000 | Loss: 0.00001494
Iteration 122/1000 | Loss: 0.00001494
Iteration 123/1000 | Loss: 0.00001494
Iteration 124/1000 | Loss: 0.00001494
Iteration 125/1000 | Loss: 0.00001494
Iteration 126/1000 | Loss: 0.00001494
Iteration 127/1000 | Loss: 0.00001494
Iteration 128/1000 | Loss: 0.00001494
Iteration 129/1000 | Loss: 0.00001494
Iteration 130/1000 | Loss: 0.00001494
Iteration 131/1000 | Loss: 0.00001494
Iteration 132/1000 | Loss: 0.00001494
Iteration 133/1000 | Loss: 0.00001494
Iteration 134/1000 | Loss: 0.00001494
Iteration 135/1000 | Loss: 0.00001494
Iteration 136/1000 | Loss: 0.00001493
Iteration 137/1000 | Loss: 0.00001493
Iteration 138/1000 | Loss: 0.00001493
Iteration 139/1000 | Loss: 0.00001493
Iteration 140/1000 | Loss: 0.00001493
Iteration 141/1000 | Loss: 0.00001493
Iteration 142/1000 | Loss: 0.00001493
Iteration 143/1000 | Loss: 0.00001493
Iteration 144/1000 | Loss: 0.00001493
Iteration 145/1000 | Loss: 0.00001493
Iteration 146/1000 | Loss: 0.00001493
Iteration 147/1000 | Loss: 0.00001493
Iteration 148/1000 | Loss: 0.00001492
Iteration 149/1000 | Loss: 0.00001492
Iteration 150/1000 | Loss: 0.00001492
Iteration 151/1000 | Loss: 0.00001492
Iteration 152/1000 | Loss: 0.00001492
Iteration 153/1000 | Loss: 0.00001492
Iteration 154/1000 | Loss: 0.00001492
Iteration 155/1000 | Loss: 0.00001492
Iteration 156/1000 | Loss: 0.00001492
Iteration 157/1000 | Loss: 0.00001492
Iteration 158/1000 | Loss: 0.00001492
Iteration 159/1000 | Loss: 0.00001492
Iteration 160/1000 | Loss: 0.00001492
Iteration 161/1000 | Loss: 0.00001492
Iteration 162/1000 | Loss: 0.00001492
Iteration 163/1000 | Loss: 0.00001492
Iteration 164/1000 | Loss: 0.00001492
Iteration 165/1000 | Loss: 0.00001492
Iteration 166/1000 | Loss: 0.00001492
Iteration 167/1000 | Loss: 0.00001492
Iteration 168/1000 | Loss: 0.00001492
Iteration 169/1000 | Loss: 0.00001492
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.4921681213309057e-05, 1.4921681213309057e-05, 1.4921681213309057e-05, 1.4921681213309057e-05, 1.4921681213309057e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4921681213309057e-05

Optimization complete. Final v2v error: 3.2816684246063232 mm

Highest mean error: 3.95098876953125 mm for frame 84

Lowest mean error: 3.0250277519226074 mm for frame 131

Saving results

Total time: 38.032395124435425
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00975990
Iteration 2/25 | Loss: 0.00275807
Iteration 3/25 | Loss: 0.00157616
Iteration 4/25 | Loss: 0.00136776
Iteration 5/25 | Loss: 0.00131135
Iteration 6/25 | Loss: 0.00124381
Iteration 7/25 | Loss: 0.00127410
Iteration 8/25 | Loss: 0.00120495
Iteration 9/25 | Loss: 0.00107626
Iteration 10/25 | Loss: 0.00101640
Iteration 11/25 | Loss: 0.00099217
Iteration 12/25 | Loss: 0.00097163
Iteration 13/25 | Loss: 0.00093563
Iteration 14/25 | Loss: 0.00092461
Iteration 15/25 | Loss: 0.00093051
Iteration 16/25 | Loss: 0.00093472
Iteration 17/25 | Loss: 0.00092324
Iteration 18/25 | Loss: 0.00090809
Iteration 19/25 | Loss: 0.00089714
Iteration 20/25 | Loss: 0.00088807
Iteration 21/25 | Loss: 0.00088143
Iteration 22/25 | Loss: 0.00086983
Iteration 23/25 | Loss: 0.00087398
Iteration 24/25 | Loss: 0.00086793
Iteration 25/25 | Loss: 0.00087800

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21496177
Iteration 2/25 | Loss: 0.00214152
Iteration 3/25 | Loss: 0.00202205
Iteration 4/25 | Loss: 0.00202205
Iteration 5/25 | Loss: 0.00202205
Iteration 6/25 | Loss: 0.00202205
Iteration 7/25 | Loss: 0.00202205
Iteration 8/25 | Loss: 0.00202205
Iteration 9/25 | Loss: 0.00202205
Iteration 10/25 | Loss: 0.00202205
Iteration 11/25 | Loss: 0.00202205
Iteration 12/25 | Loss: 0.00202205
Iteration 13/25 | Loss: 0.00202205
Iteration 14/25 | Loss: 0.00202205
Iteration 15/25 | Loss: 0.00202205
Iteration 16/25 | Loss: 0.00202205
Iteration 17/25 | Loss: 0.00202205
Iteration 18/25 | Loss: 0.00202205
Iteration 19/25 | Loss: 0.00202205
Iteration 20/25 | Loss: 0.00202205
Iteration 21/25 | Loss: 0.00202205
Iteration 22/25 | Loss: 0.00202205
Iteration 23/25 | Loss: 0.00202205
Iteration 24/25 | Loss: 0.00202205
Iteration 25/25 | Loss: 0.00202205

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00202205
Iteration 2/1000 | Loss: 0.00043897
Iteration 3/1000 | Loss: 0.00160374
Iteration 4/1000 | Loss: 0.00037015
Iteration 5/1000 | Loss: 0.00027808
Iteration 6/1000 | Loss: 0.00081335
Iteration 7/1000 | Loss: 0.00137298
Iteration 8/1000 | Loss: 0.00030873
Iteration 9/1000 | Loss: 0.00035334
Iteration 10/1000 | Loss: 0.00083056
Iteration 11/1000 | Loss: 0.00037652
Iteration 12/1000 | Loss: 0.00075903
Iteration 13/1000 | Loss: 0.00036393
Iteration 14/1000 | Loss: 0.00032825
Iteration 15/1000 | Loss: 0.00069555
Iteration 16/1000 | Loss: 0.00086199
Iteration 17/1000 | Loss: 0.00044629
Iteration 18/1000 | Loss: 0.00044470
Iteration 19/1000 | Loss: 0.00027424
Iteration 20/1000 | Loss: 0.00045788
Iteration 21/1000 | Loss: 0.00045586
Iteration 22/1000 | Loss: 0.00032554
Iteration 23/1000 | Loss: 0.00024141
Iteration 24/1000 | Loss: 0.00038237
Iteration 25/1000 | Loss: 0.00057990
Iteration 26/1000 | Loss: 0.00067882
Iteration 27/1000 | Loss: 0.00041897
Iteration 28/1000 | Loss: 0.00044332
Iteration 29/1000 | Loss: 0.00031590
Iteration 30/1000 | Loss: 0.00026225
Iteration 31/1000 | Loss: 0.00035452
Iteration 32/1000 | Loss: 0.00027304
Iteration 33/1000 | Loss: 0.00024625
Iteration 34/1000 | Loss: 0.00028505
Iteration 35/1000 | Loss: 0.00069090
Iteration 36/1000 | Loss: 0.00023781
Iteration 37/1000 | Loss: 0.00054398
Iteration 38/1000 | Loss: 0.00046791
Iteration 39/1000 | Loss: 0.00021489
Iteration 40/1000 | Loss: 0.00041761
Iteration 41/1000 | Loss: 0.00033879
Iteration 42/1000 | Loss: 0.00035000
Iteration 43/1000 | Loss: 0.00027343
Iteration 44/1000 | Loss: 0.00021164
Iteration 45/1000 | Loss: 0.00028313
Iteration 46/1000 | Loss: 0.00036852
Iteration 47/1000 | Loss: 0.00056487
Iteration 48/1000 | Loss: 0.00054996
Iteration 49/1000 | Loss: 0.00030585
Iteration 50/1000 | Loss: 0.00026678
Iteration 51/1000 | Loss: 0.00044817
Iteration 52/1000 | Loss: 0.00038922
Iteration 53/1000 | Loss: 0.00036905
Iteration 54/1000 | Loss: 0.00048108
Iteration 55/1000 | Loss: 0.00037311
Iteration 56/1000 | Loss: 0.00032743
Iteration 57/1000 | Loss: 0.00026616
Iteration 58/1000 | Loss: 0.00027102
Iteration 59/1000 | Loss: 0.00026089
Iteration 60/1000 | Loss: 0.00029134
Iteration 61/1000 | Loss: 0.00027283
Iteration 62/1000 | Loss: 0.00040399
Iteration 63/1000 | Loss: 0.00038596
Iteration 64/1000 | Loss: 0.00031615
Iteration 65/1000 | Loss: 0.00037233
Iteration 66/1000 | Loss: 0.00029131
Iteration 67/1000 | Loss: 0.00027597
Iteration 68/1000 | Loss: 0.00026063
Iteration 69/1000 | Loss: 0.00023506
Iteration 70/1000 | Loss: 0.00038134
Iteration 71/1000 | Loss: 0.00047851
Iteration 72/1000 | Loss: 0.00020554
Iteration 73/1000 | Loss: 0.00031725
Iteration 74/1000 | Loss: 0.00038438
Iteration 75/1000 | Loss: 0.00039895
Iteration 76/1000 | Loss: 0.00050187
Iteration 77/1000 | Loss: 0.00023624
Iteration 78/1000 | Loss: 0.00012630
Iteration 79/1000 | Loss: 0.00030635
Iteration 80/1000 | Loss: 0.00030353
Iteration 81/1000 | Loss: 0.00034913
Iteration 82/1000 | Loss: 0.00051094
Iteration 83/1000 | Loss: 0.00024083
Iteration 84/1000 | Loss: 0.00026480
Iteration 85/1000 | Loss: 0.00037522
Iteration 86/1000 | Loss: 0.00027442
Iteration 87/1000 | Loss: 0.00019359
Iteration 88/1000 | Loss: 0.00044885
Iteration 89/1000 | Loss: 0.00034138
Iteration 90/1000 | Loss: 0.00033200
Iteration 91/1000 | Loss: 0.00048023
Iteration 92/1000 | Loss: 0.00044181
Iteration 93/1000 | Loss: 0.00037106
Iteration 94/1000 | Loss: 0.00045413
Iteration 95/1000 | Loss: 0.00028188
Iteration 96/1000 | Loss: 0.00014495
Iteration 97/1000 | Loss: 0.00032676
Iteration 98/1000 | Loss: 0.00024801
Iteration 99/1000 | Loss: 0.00024721
Iteration 100/1000 | Loss: 0.00023207
Iteration 101/1000 | Loss: 0.00022431
Iteration 102/1000 | Loss: 0.00026956
Iteration 103/1000 | Loss: 0.00034501
Iteration 104/1000 | Loss: 0.00031802
Iteration 105/1000 | Loss: 0.00038403
Iteration 106/1000 | Loss: 0.00033920
Iteration 107/1000 | Loss: 0.00017870
Iteration 108/1000 | Loss: 0.00029568
Iteration 109/1000 | Loss: 0.00021136
Iteration 110/1000 | Loss: 0.00039552
Iteration 111/1000 | Loss: 0.00020441
Iteration 112/1000 | Loss: 0.00022906
Iteration 113/1000 | Loss: 0.00010519
Iteration 114/1000 | Loss: 0.00018268
Iteration 115/1000 | Loss: 0.00022552
Iteration 116/1000 | Loss: 0.00015098
Iteration 117/1000 | Loss: 0.00014450
Iteration 118/1000 | Loss: 0.00015809
Iteration 119/1000 | Loss: 0.00016536
Iteration 120/1000 | Loss: 0.00022147
Iteration 121/1000 | Loss: 0.00016657
Iteration 122/1000 | Loss: 0.00062877
Iteration 123/1000 | Loss: 0.00039146
Iteration 124/1000 | Loss: 0.00050896
Iteration 125/1000 | Loss: 0.00033790
Iteration 126/1000 | Loss: 0.00049764
Iteration 127/1000 | Loss: 0.00034206
Iteration 128/1000 | Loss: 0.00034446
Iteration 129/1000 | Loss: 0.00028499
Iteration 130/1000 | Loss: 0.00053556
Iteration 131/1000 | Loss: 0.00052170
Iteration 132/1000 | Loss: 0.00050290
Iteration 133/1000 | Loss: 0.00035504
Iteration 134/1000 | Loss: 0.00040408
Iteration 135/1000 | Loss: 0.00030278
Iteration 136/1000 | Loss: 0.00044823
Iteration 137/1000 | Loss: 0.00017026
Iteration 138/1000 | Loss: 0.00024554
Iteration 139/1000 | Loss: 0.00019938
Iteration 140/1000 | Loss: 0.00017405
Iteration 141/1000 | Loss: 0.00014313
Iteration 142/1000 | Loss: 0.00025981
Iteration 143/1000 | Loss: 0.00020774
Iteration 144/1000 | Loss: 0.00020745
Iteration 145/1000 | Loss: 0.00031331
Iteration 146/1000 | Loss: 0.00022484
Iteration 147/1000 | Loss: 0.00022650
Iteration 148/1000 | Loss: 0.00034887
Iteration 149/1000 | Loss: 0.00238630
Iteration 150/1000 | Loss: 0.00062977
Iteration 151/1000 | Loss: 0.00023098
Iteration 152/1000 | Loss: 0.00035186
Iteration 153/1000 | Loss: 0.00034884
Iteration 154/1000 | Loss: 0.00108208
Iteration 155/1000 | Loss: 0.00052944
Iteration 156/1000 | Loss: 0.00023202
Iteration 157/1000 | Loss: 0.00041353
Iteration 158/1000 | Loss: 0.00032682
Iteration 159/1000 | Loss: 0.00013606
Iteration 160/1000 | Loss: 0.00111571
Iteration 161/1000 | Loss: 0.00071565
Iteration 162/1000 | Loss: 0.00028978
Iteration 163/1000 | Loss: 0.00043882
Iteration 164/1000 | Loss: 0.00028605
Iteration 165/1000 | Loss: 0.00038269
Iteration 166/1000 | Loss: 0.00083978
Iteration 167/1000 | Loss: 0.00027383
Iteration 168/1000 | Loss: 0.00011184
Iteration 169/1000 | Loss: 0.00060247
Iteration 170/1000 | Loss: 0.00061155
Iteration 171/1000 | Loss: 0.00037923
Iteration 172/1000 | Loss: 0.00047530
Iteration 173/1000 | Loss: 0.00021365
Iteration 174/1000 | Loss: 0.00060423
Iteration 175/1000 | Loss: 0.00025012
Iteration 176/1000 | Loss: 0.00016731
Iteration 177/1000 | Loss: 0.00027828
Iteration 178/1000 | Loss: 0.00034930
Iteration 179/1000 | Loss: 0.00066126
Iteration 180/1000 | Loss: 0.00018954
Iteration 181/1000 | Loss: 0.00018267
Iteration 182/1000 | Loss: 0.00009400
Iteration 183/1000 | Loss: 0.00010422
Iteration 184/1000 | Loss: 0.00011034
Iteration 185/1000 | Loss: 0.00022585
Iteration 186/1000 | Loss: 0.00016222
Iteration 187/1000 | Loss: 0.00015851
Iteration 188/1000 | Loss: 0.00021996
Iteration 189/1000 | Loss: 0.00038591
Iteration 190/1000 | Loss: 0.00017807
Iteration 191/1000 | Loss: 0.00021811
Iteration 192/1000 | Loss: 0.00034927
Iteration 193/1000 | Loss: 0.00017317
Iteration 194/1000 | Loss: 0.00057046
Iteration 195/1000 | Loss: 0.00011072
Iteration 196/1000 | Loss: 0.00015338
Iteration 197/1000 | Loss: 0.00025931
Iteration 198/1000 | Loss: 0.00032417
Iteration 199/1000 | Loss: 0.00023636
Iteration 200/1000 | Loss: 0.00022736
Iteration 201/1000 | Loss: 0.00020880
Iteration 202/1000 | Loss: 0.00018871
Iteration 203/1000 | Loss: 0.00042125
Iteration 204/1000 | Loss: 0.00014165
Iteration 205/1000 | Loss: 0.00039383
Iteration 206/1000 | Loss: 0.00022337
Iteration 207/1000 | Loss: 0.00022955
Iteration 208/1000 | Loss: 0.00107848
Iteration 209/1000 | Loss: 0.00065558
Iteration 210/1000 | Loss: 0.00012715
Iteration 211/1000 | Loss: 0.00009407
Iteration 212/1000 | Loss: 0.00008545
Iteration 213/1000 | Loss: 0.00010010
Iteration 214/1000 | Loss: 0.00009200
Iteration 215/1000 | Loss: 0.00008955
Iteration 216/1000 | Loss: 0.00011704
Iteration 217/1000 | Loss: 0.00009256
Iteration 218/1000 | Loss: 0.00011148
Iteration 219/1000 | Loss: 0.00009070
Iteration 220/1000 | Loss: 0.00008469
Iteration 221/1000 | Loss: 0.00009856
Iteration 222/1000 | Loss: 0.00011915
Iteration 223/1000 | Loss: 0.00011344
Iteration 224/1000 | Loss: 0.00011494
Iteration 225/1000 | Loss: 0.00011126
Iteration 226/1000 | Loss: 0.00011137
Iteration 227/1000 | Loss: 0.00011439
Iteration 228/1000 | Loss: 0.00011436
Iteration 229/1000 | Loss: 0.00021512
Iteration 230/1000 | Loss: 0.00010681
Iteration 231/1000 | Loss: 0.00011997
Iteration 232/1000 | Loss: 0.00022251
Iteration 233/1000 | Loss: 0.00008239
Iteration 234/1000 | Loss: 0.00005685
Iteration 235/1000 | Loss: 0.00005267
Iteration 236/1000 | Loss: 0.00005058
Iteration 237/1000 | Loss: 0.00160767
Iteration 238/1000 | Loss: 0.00057739
Iteration 239/1000 | Loss: 0.00132026
Iteration 240/1000 | Loss: 0.00104052
Iteration 241/1000 | Loss: 0.00053546
Iteration 242/1000 | Loss: 0.00008125
Iteration 243/1000 | Loss: 0.00005411
Iteration 244/1000 | Loss: 0.00041397
Iteration 245/1000 | Loss: 0.00038198
Iteration 246/1000 | Loss: 0.00029095
Iteration 247/1000 | Loss: 0.00070602
Iteration 248/1000 | Loss: 0.00056254
Iteration 249/1000 | Loss: 0.00046890
Iteration 250/1000 | Loss: 0.00036446
Iteration 251/1000 | Loss: 0.00073045
Iteration 252/1000 | Loss: 0.00006390
Iteration 253/1000 | Loss: 0.00005157
Iteration 254/1000 | Loss: 0.00042634
Iteration 255/1000 | Loss: 0.00007022
Iteration 256/1000 | Loss: 0.00006011
Iteration 257/1000 | Loss: 0.00005329
Iteration 258/1000 | Loss: 0.00005083
Iteration 259/1000 | Loss: 0.00004884
Iteration 260/1000 | Loss: 0.00004781
Iteration 261/1000 | Loss: 0.00004646
Iteration 262/1000 | Loss: 0.00004581
Iteration 263/1000 | Loss: 0.00049031
Iteration 264/1000 | Loss: 0.00031322
Iteration 265/1000 | Loss: 0.00024920
Iteration 266/1000 | Loss: 0.00031637
Iteration 267/1000 | Loss: 0.00024050
Iteration 268/1000 | Loss: 0.00040105
Iteration 269/1000 | Loss: 0.00022546
Iteration 270/1000 | Loss: 0.00029221
Iteration 271/1000 | Loss: 0.00029921
Iteration 272/1000 | Loss: 0.00006057
Iteration 273/1000 | Loss: 0.00019613
Iteration 274/1000 | Loss: 0.00014461
Iteration 275/1000 | Loss: 0.00004544
Iteration 276/1000 | Loss: 0.00004367
Iteration 277/1000 | Loss: 0.00021349
Iteration 278/1000 | Loss: 0.00034249
Iteration 279/1000 | Loss: 0.00026951
Iteration 280/1000 | Loss: 0.00011286
Iteration 281/1000 | Loss: 0.00004348
Iteration 282/1000 | Loss: 0.00020250
Iteration 283/1000 | Loss: 0.00024969
Iteration 284/1000 | Loss: 0.00010379
Iteration 285/1000 | Loss: 0.00024404
Iteration 286/1000 | Loss: 0.00017654
Iteration 287/1000 | Loss: 0.00007689
Iteration 288/1000 | Loss: 0.00004135
Iteration 289/1000 | Loss: 0.00024249
Iteration 290/1000 | Loss: 0.00022412
Iteration 291/1000 | Loss: 0.00021778
Iteration 292/1000 | Loss: 0.00105351
Iteration 293/1000 | Loss: 0.00022405
Iteration 294/1000 | Loss: 0.00024776
Iteration 295/1000 | Loss: 0.00008137
Iteration 296/1000 | Loss: 0.00018649
Iteration 297/1000 | Loss: 0.00015709
Iteration 298/1000 | Loss: 0.00016677
Iteration 299/1000 | Loss: 0.00025090
Iteration 300/1000 | Loss: 0.00015020
Iteration 301/1000 | Loss: 0.00005325
Iteration 302/1000 | Loss: 0.00004626
Iteration 303/1000 | Loss: 0.00021056
Iteration 304/1000 | Loss: 0.00012159
Iteration 305/1000 | Loss: 0.00014548
Iteration 306/1000 | Loss: 0.00012608
Iteration 307/1000 | Loss: 0.00005394
Iteration 308/1000 | Loss: 0.00026272
Iteration 309/1000 | Loss: 0.00029148
Iteration 310/1000 | Loss: 0.00040815
Iteration 311/1000 | Loss: 0.00025225
Iteration 312/1000 | Loss: 0.00029414
Iteration 313/1000 | Loss: 0.00064702
Iteration 314/1000 | Loss: 0.00027365
Iteration 315/1000 | Loss: 0.00009005
Iteration 316/1000 | Loss: 0.00004442
Iteration 317/1000 | Loss: 0.00004782
Iteration 318/1000 | Loss: 0.00004109
Iteration 319/1000 | Loss: 0.00003976
Iteration 320/1000 | Loss: 0.00074907
Iteration 321/1000 | Loss: 0.00008633
Iteration 322/1000 | Loss: 0.00029271
Iteration 323/1000 | Loss: 0.00104109
Iteration 324/1000 | Loss: 0.00041584
Iteration 325/1000 | Loss: 0.00014459
Iteration 326/1000 | Loss: 0.00012144
Iteration 327/1000 | Loss: 0.00004417
Iteration 328/1000 | Loss: 0.00004262
Iteration 329/1000 | Loss: 0.00006701
Iteration 330/1000 | Loss: 0.00003966
Iteration 331/1000 | Loss: 0.00003859
Iteration 332/1000 | Loss: 0.00003823
Iteration 333/1000 | Loss: 0.00065131
Iteration 334/1000 | Loss: 0.00004537
Iteration 335/1000 | Loss: 0.00003862
Iteration 336/1000 | Loss: 0.00003735
Iteration 337/1000 | Loss: 0.00003609
Iteration 338/1000 | Loss: 0.00003531
Iteration 339/1000 | Loss: 0.00003491
Iteration 340/1000 | Loss: 0.00003455
Iteration 341/1000 | Loss: 0.00003432
Iteration 342/1000 | Loss: 0.00003427
Iteration 343/1000 | Loss: 0.00003427
Iteration 344/1000 | Loss: 0.00003426
Iteration 345/1000 | Loss: 0.00003426
Iteration 346/1000 | Loss: 0.00003426
Iteration 347/1000 | Loss: 0.00003425
Iteration 348/1000 | Loss: 0.00003425
Iteration 349/1000 | Loss: 0.00003425
Iteration 350/1000 | Loss: 0.00003424
Iteration 351/1000 | Loss: 0.00003422
Iteration 352/1000 | Loss: 0.00003422
Iteration 353/1000 | Loss: 0.00003421
Iteration 354/1000 | Loss: 0.00003421
Iteration 355/1000 | Loss: 0.00003421
Iteration 356/1000 | Loss: 0.00003420
Iteration 357/1000 | Loss: 0.00003419
Iteration 358/1000 | Loss: 0.00003417
Iteration 359/1000 | Loss: 0.00003417
Iteration 360/1000 | Loss: 0.00003416
Iteration 361/1000 | Loss: 0.00003416
Iteration 362/1000 | Loss: 0.00003416
Iteration 363/1000 | Loss: 0.00003414
Iteration 364/1000 | Loss: 0.00003413
Iteration 365/1000 | Loss: 0.00003413
Iteration 366/1000 | Loss: 0.00003413
Iteration 367/1000 | Loss: 0.00003413
Iteration 368/1000 | Loss: 0.00003412
Iteration 369/1000 | Loss: 0.00003412
Iteration 370/1000 | Loss: 0.00003411
Iteration 371/1000 | Loss: 0.00003411
Iteration 372/1000 | Loss: 0.00003410
Iteration 373/1000 | Loss: 0.00003410
Iteration 374/1000 | Loss: 0.00003410
Iteration 375/1000 | Loss: 0.00003410
Iteration 376/1000 | Loss: 0.00003410
Iteration 377/1000 | Loss: 0.00003409
Iteration 378/1000 | Loss: 0.00003409
Iteration 379/1000 | Loss: 0.00003408
Iteration 380/1000 | Loss: 0.00003408
Iteration 381/1000 | Loss: 0.00003408
Iteration 382/1000 | Loss: 0.00003408
Iteration 383/1000 | Loss: 0.00003407
Iteration 384/1000 | Loss: 0.00003407
Iteration 385/1000 | Loss: 0.00003407
Iteration 386/1000 | Loss: 0.00003407
Iteration 387/1000 | Loss: 0.00003407
Iteration 388/1000 | Loss: 0.00003407
Iteration 389/1000 | Loss: 0.00003406
Iteration 390/1000 | Loss: 0.00003406
Iteration 391/1000 | Loss: 0.00003406
Iteration 392/1000 | Loss: 0.00003406
Iteration 393/1000 | Loss: 0.00003406
Iteration 394/1000 | Loss: 0.00003406
Iteration 395/1000 | Loss: 0.00003406
Iteration 396/1000 | Loss: 0.00003406
Iteration 397/1000 | Loss: 0.00003406
Iteration 398/1000 | Loss: 0.00003406
Iteration 399/1000 | Loss: 0.00003406
Iteration 400/1000 | Loss: 0.00003405
Iteration 401/1000 | Loss: 0.00003405
Iteration 402/1000 | Loss: 0.00003405
Iteration 403/1000 | Loss: 0.00003405
Iteration 404/1000 | Loss: 0.00003405
Iteration 405/1000 | Loss: 0.00003405
Iteration 406/1000 | Loss: 0.00003405
Iteration 407/1000 | Loss: 0.00003405
Iteration 408/1000 | Loss: 0.00003405
Iteration 409/1000 | Loss: 0.00003405
Iteration 410/1000 | Loss: 0.00003405
Iteration 411/1000 | Loss: 0.00003405
Iteration 412/1000 | Loss: 0.00003405
Iteration 413/1000 | Loss: 0.00003405
Iteration 414/1000 | Loss: 0.00003404
Iteration 415/1000 | Loss: 0.00003404
Iteration 416/1000 | Loss: 0.00003404
Iteration 417/1000 | Loss: 0.00003404
Iteration 418/1000 | Loss: 0.00003404
Iteration 419/1000 | Loss: 0.00003404
Iteration 420/1000 | Loss: 0.00003403
Iteration 421/1000 | Loss: 0.00003403
Iteration 422/1000 | Loss: 0.00003403
Iteration 423/1000 | Loss: 0.00003403
Iteration 424/1000 | Loss: 0.00003403
Iteration 425/1000 | Loss: 0.00003403
Iteration 426/1000 | Loss: 0.00003403
Iteration 427/1000 | Loss: 0.00003403
Iteration 428/1000 | Loss: 0.00003403
Iteration 429/1000 | Loss: 0.00003403
Iteration 430/1000 | Loss: 0.00003403
Iteration 431/1000 | Loss: 0.00003403
Iteration 432/1000 | Loss: 0.00003403
Iteration 433/1000 | Loss: 0.00003403
Iteration 434/1000 | Loss: 0.00003403
Iteration 435/1000 | Loss: 0.00003403
Iteration 436/1000 | Loss: 0.00003402
Iteration 437/1000 | Loss: 0.00003402
Iteration 438/1000 | Loss: 0.00003402
Iteration 439/1000 | Loss: 0.00003402
Iteration 440/1000 | Loss: 0.00003402
Iteration 441/1000 | Loss: 0.00003402
Iteration 442/1000 | Loss: 0.00003402
Iteration 443/1000 | Loss: 0.00003402
Iteration 444/1000 | Loss: 0.00003402
Iteration 445/1000 | Loss: 0.00003402
Iteration 446/1000 | Loss: 0.00003402
Iteration 447/1000 | Loss: 0.00003402
Iteration 448/1000 | Loss: 0.00003401
Iteration 449/1000 | Loss: 0.00003401
Iteration 450/1000 | Loss: 0.00003401
Iteration 451/1000 | Loss: 0.00003401
Iteration 452/1000 | Loss: 0.00003401
Iteration 453/1000 | Loss: 0.00003401
Iteration 454/1000 | Loss: 0.00003401
Iteration 455/1000 | Loss: 0.00003401
Iteration 456/1000 | Loss: 0.00003401
Iteration 457/1000 | Loss: 0.00003401
Iteration 458/1000 | Loss: 0.00003401
Iteration 459/1000 | Loss: 0.00003401
Iteration 460/1000 | Loss: 0.00003401
Iteration 461/1000 | Loss: 0.00003400
Iteration 462/1000 | Loss: 0.00003400
Iteration 463/1000 | Loss: 0.00003400
Iteration 464/1000 | Loss: 0.00003400
Iteration 465/1000 | Loss: 0.00003400
Iteration 466/1000 | Loss: 0.00003399
Iteration 467/1000 | Loss: 0.00003399
Iteration 468/1000 | Loss: 0.00003399
Iteration 469/1000 | Loss: 0.00003399
Iteration 470/1000 | Loss: 0.00003399
Iteration 471/1000 | Loss: 0.00003399
Iteration 472/1000 | Loss: 0.00003399
Iteration 473/1000 | Loss: 0.00003399
Iteration 474/1000 | Loss: 0.00003399
Iteration 475/1000 | Loss: 0.00003399
Iteration 476/1000 | Loss: 0.00003399
Iteration 477/1000 | Loss: 0.00003399
Iteration 478/1000 | Loss: 0.00003399
Iteration 479/1000 | Loss: 0.00003399
Iteration 480/1000 | Loss: 0.00003399
Iteration 481/1000 | Loss: 0.00003399
Iteration 482/1000 | Loss: 0.00003399
Iteration 483/1000 | Loss: 0.00003399
Iteration 484/1000 | Loss: 0.00003398
Iteration 485/1000 | Loss: 0.00003398
Iteration 486/1000 | Loss: 0.00003398
Iteration 487/1000 | Loss: 0.00003398
Iteration 488/1000 | Loss: 0.00003398
Iteration 489/1000 | Loss: 0.00003398
Iteration 490/1000 | Loss: 0.00003398
Iteration 491/1000 | Loss: 0.00003398
Iteration 492/1000 | Loss: 0.00003398
Iteration 493/1000 | Loss: 0.00003398
Iteration 494/1000 | Loss: 0.00003398
Iteration 495/1000 | Loss: 0.00003398
Iteration 496/1000 | Loss: 0.00003398
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 496. Stopping optimization.
Last 5 losses: [3.3983269531745464e-05, 3.3983269531745464e-05, 3.3983269531745464e-05, 3.3983269531745464e-05, 3.3983269531745464e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.3983269531745464e-05

Optimization complete. Final v2v error: 4.248525619506836 mm

Highest mean error: 12.280627250671387 mm for frame 21

Lowest mean error: 3.0706064701080322 mm for frame 92

Saving results

Total time: 556.5779118537903
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00862170
Iteration 2/25 | Loss: 0.00077477
Iteration 3/25 | Loss: 0.00063809
Iteration 4/25 | Loss: 0.00061543
Iteration 5/25 | Loss: 0.00060669
Iteration 6/25 | Loss: 0.00060542
Iteration 7/25 | Loss: 0.00060518
Iteration 8/25 | Loss: 0.00060518
Iteration 9/25 | Loss: 0.00060518
Iteration 10/25 | Loss: 0.00060518
Iteration 11/25 | Loss: 0.00060518
Iteration 12/25 | Loss: 0.00060518
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006051849341019988, 0.0006051849341019988, 0.0006051849341019988, 0.0006051849341019988, 0.0006051849341019988]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006051849341019988

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53278625
Iteration 2/25 | Loss: 0.00028893
Iteration 3/25 | Loss: 0.00028893
Iteration 4/25 | Loss: 0.00028893
Iteration 5/25 | Loss: 0.00028893
Iteration 6/25 | Loss: 0.00028893
Iteration 7/25 | Loss: 0.00028892
Iteration 8/25 | Loss: 0.00028892
Iteration 9/25 | Loss: 0.00028892
Iteration 10/25 | Loss: 0.00028892
Iteration 11/25 | Loss: 0.00028892
Iteration 12/25 | Loss: 0.00028892
Iteration 13/25 | Loss: 0.00028892
Iteration 14/25 | Loss: 0.00028892
Iteration 15/25 | Loss: 0.00028892
Iteration 16/25 | Loss: 0.00028892
Iteration 17/25 | Loss: 0.00028892
Iteration 18/25 | Loss: 0.00028892
Iteration 19/25 | Loss: 0.00028892
Iteration 20/25 | Loss: 0.00028892
Iteration 21/25 | Loss: 0.00028892
Iteration 22/25 | Loss: 0.00028892
Iteration 23/25 | Loss: 0.00028892
Iteration 24/25 | Loss: 0.00028892
Iteration 25/25 | Loss: 0.00028892

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028892
Iteration 2/1000 | Loss: 0.00002119
Iteration 3/1000 | Loss: 0.00001495
Iteration 4/1000 | Loss: 0.00001384
Iteration 5/1000 | Loss: 0.00001320
Iteration 6/1000 | Loss: 0.00001284
Iteration 7/1000 | Loss: 0.00001251
Iteration 8/1000 | Loss: 0.00001229
Iteration 9/1000 | Loss: 0.00001213
Iteration 10/1000 | Loss: 0.00001208
Iteration 11/1000 | Loss: 0.00001202
Iteration 12/1000 | Loss: 0.00001201
Iteration 13/1000 | Loss: 0.00001201
Iteration 14/1000 | Loss: 0.00001200
Iteration 15/1000 | Loss: 0.00001199
Iteration 16/1000 | Loss: 0.00001196
Iteration 17/1000 | Loss: 0.00001195
Iteration 18/1000 | Loss: 0.00001194
Iteration 19/1000 | Loss: 0.00001193
Iteration 20/1000 | Loss: 0.00001193
Iteration 21/1000 | Loss: 0.00001192
Iteration 22/1000 | Loss: 0.00001191
Iteration 23/1000 | Loss: 0.00001191
Iteration 24/1000 | Loss: 0.00001190
Iteration 25/1000 | Loss: 0.00001190
Iteration 26/1000 | Loss: 0.00001189
Iteration 27/1000 | Loss: 0.00001189
Iteration 28/1000 | Loss: 0.00001189
Iteration 29/1000 | Loss: 0.00001188
Iteration 30/1000 | Loss: 0.00001188
Iteration 31/1000 | Loss: 0.00001185
Iteration 32/1000 | Loss: 0.00001184
Iteration 33/1000 | Loss: 0.00001184
Iteration 34/1000 | Loss: 0.00001183
Iteration 35/1000 | Loss: 0.00001180
Iteration 36/1000 | Loss: 0.00001179
Iteration 37/1000 | Loss: 0.00001178
Iteration 38/1000 | Loss: 0.00001178
Iteration 39/1000 | Loss: 0.00001178
Iteration 40/1000 | Loss: 0.00001177
Iteration 41/1000 | Loss: 0.00001175
Iteration 42/1000 | Loss: 0.00001175
Iteration 43/1000 | Loss: 0.00001175
Iteration 44/1000 | Loss: 0.00001174
Iteration 45/1000 | Loss: 0.00001174
Iteration 46/1000 | Loss: 0.00001174
Iteration 47/1000 | Loss: 0.00001173
Iteration 48/1000 | Loss: 0.00001173
Iteration 49/1000 | Loss: 0.00001173
Iteration 50/1000 | Loss: 0.00001172
Iteration 51/1000 | Loss: 0.00001172
Iteration 52/1000 | Loss: 0.00001172
Iteration 53/1000 | Loss: 0.00001172
Iteration 54/1000 | Loss: 0.00001171
Iteration 55/1000 | Loss: 0.00001171
Iteration 56/1000 | Loss: 0.00001170
Iteration 57/1000 | Loss: 0.00001170
Iteration 58/1000 | Loss: 0.00001170
Iteration 59/1000 | Loss: 0.00001169
Iteration 60/1000 | Loss: 0.00001169
Iteration 61/1000 | Loss: 0.00001169
Iteration 62/1000 | Loss: 0.00001169
Iteration 63/1000 | Loss: 0.00001168
Iteration 64/1000 | Loss: 0.00001167
Iteration 65/1000 | Loss: 0.00001166
Iteration 66/1000 | Loss: 0.00001166
Iteration 67/1000 | Loss: 0.00001165
Iteration 68/1000 | Loss: 0.00001165
Iteration 69/1000 | Loss: 0.00001165
Iteration 70/1000 | Loss: 0.00001165
Iteration 71/1000 | Loss: 0.00001164
Iteration 72/1000 | Loss: 0.00001164
Iteration 73/1000 | Loss: 0.00001163
Iteration 74/1000 | Loss: 0.00001163
Iteration 75/1000 | Loss: 0.00001162
Iteration 76/1000 | Loss: 0.00001162
Iteration 77/1000 | Loss: 0.00001162
Iteration 78/1000 | Loss: 0.00001162
Iteration 79/1000 | Loss: 0.00001162
Iteration 80/1000 | Loss: 0.00001161
Iteration 81/1000 | Loss: 0.00001161
Iteration 82/1000 | Loss: 0.00001161
Iteration 83/1000 | Loss: 0.00001161
Iteration 84/1000 | Loss: 0.00001161
Iteration 85/1000 | Loss: 0.00001160
Iteration 86/1000 | Loss: 0.00001160
Iteration 87/1000 | Loss: 0.00001159
Iteration 88/1000 | Loss: 0.00001159
Iteration 89/1000 | Loss: 0.00001158
Iteration 90/1000 | Loss: 0.00001158
Iteration 91/1000 | Loss: 0.00001158
Iteration 92/1000 | Loss: 0.00001157
Iteration 93/1000 | Loss: 0.00001157
Iteration 94/1000 | Loss: 0.00001157
Iteration 95/1000 | Loss: 0.00001156
Iteration 96/1000 | Loss: 0.00001156
Iteration 97/1000 | Loss: 0.00001156
Iteration 98/1000 | Loss: 0.00001155
Iteration 99/1000 | Loss: 0.00001155
Iteration 100/1000 | Loss: 0.00001155
Iteration 101/1000 | Loss: 0.00001155
Iteration 102/1000 | Loss: 0.00001155
Iteration 103/1000 | Loss: 0.00001155
Iteration 104/1000 | Loss: 0.00001155
Iteration 105/1000 | Loss: 0.00001154
Iteration 106/1000 | Loss: 0.00001154
Iteration 107/1000 | Loss: 0.00001154
Iteration 108/1000 | Loss: 0.00001154
Iteration 109/1000 | Loss: 0.00001154
Iteration 110/1000 | Loss: 0.00001154
Iteration 111/1000 | Loss: 0.00001154
Iteration 112/1000 | Loss: 0.00001153
Iteration 113/1000 | Loss: 0.00001153
Iteration 114/1000 | Loss: 0.00001153
Iteration 115/1000 | Loss: 0.00001153
Iteration 116/1000 | Loss: 0.00001152
Iteration 117/1000 | Loss: 0.00001152
Iteration 118/1000 | Loss: 0.00001152
Iteration 119/1000 | Loss: 0.00001152
Iteration 120/1000 | Loss: 0.00001152
Iteration 121/1000 | Loss: 0.00001152
Iteration 122/1000 | Loss: 0.00001152
Iteration 123/1000 | Loss: 0.00001152
Iteration 124/1000 | Loss: 0.00001152
Iteration 125/1000 | Loss: 0.00001152
Iteration 126/1000 | Loss: 0.00001151
Iteration 127/1000 | Loss: 0.00001151
Iteration 128/1000 | Loss: 0.00001151
Iteration 129/1000 | Loss: 0.00001151
Iteration 130/1000 | Loss: 0.00001151
Iteration 131/1000 | Loss: 0.00001151
Iteration 132/1000 | Loss: 0.00001151
Iteration 133/1000 | Loss: 0.00001151
Iteration 134/1000 | Loss: 0.00001151
Iteration 135/1000 | Loss: 0.00001151
Iteration 136/1000 | Loss: 0.00001151
Iteration 137/1000 | Loss: 0.00001151
Iteration 138/1000 | Loss: 0.00001151
Iteration 139/1000 | Loss: 0.00001151
Iteration 140/1000 | Loss: 0.00001150
Iteration 141/1000 | Loss: 0.00001150
Iteration 142/1000 | Loss: 0.00001150
Iteration 143/1000 | Loss: 0.00001150
Iteration 144/1000 | Loss: 0.00001150
Iteration 145/1000 | Loss: 0.00001150
Iteration 146/1000 | Loss: 0.00001150
Iteration 147/1000 | Loss: 0.00001150
Iteration 148/1000 | Loss: 0.00001150
Iteration 149/1000 | Loss: 0.00001150
Iteration 150/1000 | Loss: 0.00001150
Iteration 151/1000 | Loss: 0.00001150
Iteration 152/1000 | Loss: 0.00001150
Iteration 153/1000 | Loss: 0.00001150
Iteration 154/1000 | Loss: 0.00001150
Iteration 155/1000 | Loss: 0.00001150
Iteration 156/1000 | Loss: 0.00001150
Iteration 157/1000 | Loss: 0.00001149
Iteration 158/1000 | Loss: 0.00001149
Iteration 159/1000 | Loss: 0.00001149
Iteration 160/1000 | Loss: 0.00001149
Iteration 161/1000 | Loss: 0.00001149
Iteration 162/1000 | Loss: 0.00001149
Iteration 163/1000 | Loss: 0.00001149
Iteration 164/1000 | Loss: 0.00001149
Iteration 165/1000 | Loss: 0.00001149
Iteration 166/1000 | Loss: 0.00001149
Iteration 167/1000 | Loss: 0.00001149
Iteration 168/1000 | Loss: 0.00001149
Iteration 169/1000 | Loss: 0.00001149
Iteration 170/1000 | Loss: 0.00001149
Iteration 171/1000 | Loss: 0.00001149
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.1492721569084097e-05, 1.1492721569084097e-05, 1.1492721569084097e-05, 1.1492721569084097e-05, 1.1492721569084097e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1492721569084097e-05

Optimization complete. Final v2v error: 2.891916036605835 mm

Highest mean error: 3.2819578647613525 mm for frame 99

Lowest mean error: 2.7423880100250244 mm for frame 13

Saving results

Total time: 36.523717403411865
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01118441
Iteration 2/25 | Loss: 0.00644376
Iteration 3/25 | Loss: 0.00396614
Iteration 4/25 | Loss: 0.00326418
Iteration 5/25 | Loss: 0.00244523
Iteration 6/25 | Loss: 0.00206202
Iteration 7/25 | Loss: 0.00165600
Iteration 8/25 | Loss: 0.00140917
Iteration 9/25 | Loss: 0.00137358
Iteration 10/25 | Loss: 0.00123331
Iteration 11/25 | Loss: 0.00116897
Iteration 12/25 | Loss: 0.00109407
Iteration 13/25 | Loss: 0.00104022
Iteration 14/25 | Loss: 0.00102393
Iteration 15/25 | Loss: 0.00103099
Iteration 16/25 | Loss: 0.00101359
Iteration 17/25 | Loss: 0.00099820
Iteration 18/25 | Loss: 0.00099466
Iteration 19/25 | Loss: 0.00099342
Iteration 20/25 | Loss: 0.00099154
Iteration 21/25 | Loss: 0.00099140
Iteration 22/25 | Loss: 0.00099137
Iteration 23/25 | Loss: 0.00099137
Iteration 24/25 | Loss: 0.00099136
Iteration 25/25 | Loss: 0.00099136

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.57533950
Iteration 2/25 | Loss: 0.00120585
Iteration 3/25 | Loss: 0.00120585
Iteration 4/25 | Loss: 0.00120585
Iteration 5/25 | Loss: 0.00120585
Iteration 6/25 | Loss: 0.00120585
Iteration 7/25 | Loss: 0.00120585
Iteration 8/25 | Loss: 0.00120585
Iteration 9/25 | Loss: 0.00120585
Iteration 10/25 | Loss: 0.00120585
Iteration 11/25 | Loss: 0.00120585
Iteration 12/25 | Loss: 0.00120585
Iteration 13/25 | Loss: 0.00120585
Iteration 14/25 | Loss: 0.00120585
Iteration 15/25 | Loss: 0.00120585
Iteration 16/25 | Loss: 0.00120585
Iteration 17/25 | Loss: 0.00120585
Iteration 18/25 | Loss: 0.00120585
Iteration 19/25 | Loss: 0.00120585
Iteration 20/25 | Loss: 0.00120585
Iteration 21/25 | Loss: 0.00120585
Iteration 22/25 | Loss: 0.00120585
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012058474821969867, 0.0012058474821969867, 0.0012058474821969867, 0.0012058474821969867, 0.0012058474821969867]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012058474821969867

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120585
Iteration 2/1000 | Loss: 0.00021090
Iteration 3/1000 | Loss: 0.00014872
Iteration 4/1000 | Loss: 0.00012591
Iteration 5/1000 | Loss: 0.00011718
Iteration 6/1000 | Loss: 0.00011168
Iteration 7/1000 | Loss: 0.00010688
Iteration 8/1000 | Loss: 0.00010445
Iteration 9/1000 | Loss: 0.01914722
Iteration 10/1000 | Loss: 0.00010925
Iteration 11/1000 | Loss: 0.00008498
Iteration 12/1000 | Loss: 0.00006502
Iteration 13/1000 | Loss: 0.00031237
Iteration 14/1000 | Loss: 0.00013496
Iteration 15/1000 | Loss: 0.00003946
Iteration 16/1000 | Loss: 0.00045849
Iteration 17/1000 | Loss: 0.00013633
Iteration 18/1000 | Loss: 0.00007478
Iteration 19/1000 | Loss: 0.00003408
Iteration 20/1000 | Loss: 0.00003419
Iteration 21/1000 | Loss: 0.00006376
Iteration 22/1000 | Loss: 0.00012286
Iteration 23/1000 | Loss: 0.00027012
Iteration 24/1000 | Loss: 0.00009027
Iteration 25/1000 | Loss: 0.00012660
Iteration 26/1000 | Loss: 0.00004307
Iteration 27/1000 | Loss: 0.00017363
Iteration 28/1000 | Loss: 0.00003556
Iteration 29/1000 | Loss: 0.00006000
Iteration 30/1000 | Loss: 0.00002337
Iteration 31/1000 | Loss: 0.00002263
Iteration 32/1000 | Loss: 0.00002208
Iteration 33/1000 | Loss: 0.00002153
Iteration 34/1000 | Loss: 0.00002124
Iteration 35/1000 | Loss: 0.00002104
Iteration 36/1000 | Loss: 0.00002094
Iteration 37/1000 | Loss: 0.00002088
Iteration 38/1000 | Loss: 0.00002088
Iteration 39/1000 | Loss: 0.00002088
Iteration 40/1000 | Loss: 0.00002088
Iteration 41/1000 | Loss: 0.00002088
Iteration 42/1000 | Loss: 0.00002088
Iteration 43/1000 | Loss: 0.00002088
Iteration 44/1000 | Loss: 0.00002087
Iteration 45/1000 | Loss: 0.00002083
Iteration 46/1000 | Loss: 0.00002083
Iteration 47/1000 | Loss: 0.00002082
Iteration 48/1000 | Loss: 0.00002081
Iteration 49/1000 | Loss: 0.00002080
Iteration 50/1000 | Loss: 0.00002080
Iteration 51/1000 | Loss: 0.00002079
Iteration 52/1000 | Loss: 0.00002079
Iteration 53/1000 | Loss: 0.00002078
Iteration 54/1000 | Loss: 0.00002076
Iteration 55/1000 | Loss: 0.00002076
Iteration 56/1000 | Loss: 0.00002076
Iteration 57/1000 | Loss: 0.00002075
Iteration 58/1000 | Loss: 0.00002075
Iteration 59/1000 | Loss: 0.00002075
Iteration 60/1000 | Loss: 0.00002075
Iteration 61/1000 | Loss: 0.00002075
Iteration 62/1000 | Loss: 0.00002075
Iteration 63/1000 | Loss: 0.00002074
Iteration 64/1000 | Loss: 0.00002074
Iteration 65/1000 | Loss: 0.00002074
Iteration 66/1000 | Loss: 0.00002074
Iteration 67/1000 | Loss: 0.00002074
Iteration 68/1000 | Loss: 0.00002073
Iteration 69/1000 | Loss: 0.00002073
Iteration 70/1000 | Loss: 0.00002073
Iteration 71/1000 | Loss: 0.00002072
Iteration 72/1000 | Loss: 0.00002072
Iteration 73/1000 | Loss: 0.00002072
Iteration 74/1000 | Loss: 0.00002072
Iteration 75/1000 | Loss: 0.00002071
Iteration 76/1000 | Loss: 0.00002071
Iteration 77/1000 | Loss: 0.00002071
Iteration 78/1000 | Loss: 0.00002071
Iteration 79/1000 | Loss: 0.00002071
Iteration 80/1000 | Loss: 0.00002071
Iteration 81/1000 | Loss: 0.00002071
Iteration 82/1000 | Loss: 0.00002070
Iteration 83/1000 | Loss: 0.00002070
Iteration 84/1000 | Loss: 0.00002070
Iteration 85/1000 | Loss: 0.00002070
Iteration 86/1000 | Loss: 0.00002070
Iteration 87/1000 | Loss: 0.00002070
Iteration 88/1000 | Loss: 0.00002070
Iteration 89/1000 | Loss: 0.00002070
Iteration 90/1000 | Loss: 0.00002070
Iteration 91/1000 | Loss: 0.00002069
Iteration 92/1000 | Loss: 0.00002069
Iteration 93/1000 | Loss: 0.00002068
Iteration 94/1000 | Loss: 0.00002067
Iteration 95/1000 | Loss: 0.00002067
Iteration 96/1000 | Loss: 0.00002067
Iteration 97/1000 | Loss: 0.00002067
Iteration 98/1000 | Loss: 0.00002066
Iteration 99/1000 | Loss: 0.00002066
Iteration 100/1000 | Loss: 0.00002066
Iteration 101/1000 | Loss: 0.00002066
Iteration 102/1000 | Loss: 0.00002066
Iteration 103/1000 | Loss: 0.00002066
Iteration 104/1000 | Loss: 0.00002066
Iteration 105/1000 | Loss: 0.00002065
Iteration 106/1000 | Loss: 0.00002065
Iteration 107/1000 | Loss: 0.00002065
Iteration 108/1000 | Loss: 0.00002065
Iteration 109/1000 | Loss: 0.00002065
Iteration 110/1000 | Loss: 0.00002065
Iteration 111/1000 | Loss: 0.00002065
Iteration 112/1000 | Loss: 0.00002065
Iteration 113/1000 | Loss: 0.00002065
Iteration 114/1000 | Loss: 0.00002065
Iteration 115/1000 | Loss: 0.00002065
Iteration 116/1000 | Loss: 0.00002065
Iteration 117/1000 | Loss: 0.00002064
Iteration 118/1000 | Loss: 0.00002064
Iteration 119/1000 | Loss: 0.00002064
Iteration 120/1000 | Loss: 0.00002064
Iteration 121/1000 | Loss: 0.00002064
Iteration 122/1000 | Loss: 0.00002064
Iteration 123/1000 | Loss: 0.00002064
Iteration 124/1000 | Loss: 0.00002064
Iteration 125/1000 | Loss: 0.00002064
Iteration 126/1000 | Loss: 0.00002064
Iteration 127/1000 | Loss: 0.00002064
Iteration 128/1000 | Loss: 0.00002064
Iteration 129/1000 | Loss: 0.00002063
Iteration 130/1000 | Loss: 0.00002063
Iteration 131/1000 | Loss: 0.00002063
Iteration 132/1000 | Loss: 0.00002063
Iteration 133/1000 | Loss: 0.00002063
Iteration 134/1000 | Loss: 0.00002063
Iteration 135/1000 | Loss: 0.00002063
Iteration 136/1000 | Loss: 0.00002063
Iteration 137/1000 | Loss: 0.00002063
Iteration 138/1000 | Loss: 0.00002063
Iteration 139/1000 | Loss: 0.00002063
Iteration 140/1000 | Loss: 0.00002063
Iteration 141/1000 | Loss: 0.00002063
Iteration 142/1000 | Loss: 0.00002063
Iteration 143/1000 | Loss: 0.00002063
Iteration 144/1000 | Loss: 0.00002063
Iteration 145/1000 | Loss: 0.00002063
Iteration 146/1000 | Loss: 0.00002063
Iteration 147/1000 | Loss: 0.00002063
Iteration 148/1000 | Loss: 0.00002063
Iteration 149/1000 | Loss: 0.00002062
Iteration 150/1000 | Loss: 0.00002062
Iteration 151/1000 | Loss: 0.00002062
Iteration 152/1000 | Loss: 0.00002062
Iteration 153/1000 | Loss: 0.00002062
Iteration 154/1000 | Loss: 0.00002062
Iteration 155/1000 | Loss: 0.00002062
Iteration 156/1000 | Loss: 0.00002062
Iteration 157/1000 | Loss: 0.00002062
Iteration 158/1000 | Loss: 0.00002062
Iteration 159/1000 | Loss: 0.00002062
Iteration 160/1000 | Loss: 0.00002062
Iteration 161/1000 | Loss: 0.00002062
Iteration 162/1000 | Loss: 0.00002062
Iteration 163/1000 | Loss: 0.00002062
Iteration 164/1000 | Loss: 0.00002062
Iteration 165/1000 | Loss: 0.00002062
Iteration 166/1000 | Loss: 0.00002062
Iteration 167/1000 | Loss: 0.00002062
Iteration 168/1000 | Loss: 0.00002062
Iteration 169/1000 | Loss: 0.00002062
Iteration 170/1000 | Loss: 0.00002062
Iteration 171/1000 | Loss: 0.00002062
Iteration 172/1000 | Loss: 0.00002062
Iteration 173/1000 | Loss: 0.00002062
Iteration 174/1000 | Loss: 0.00002062
Iteration 175/1000 | Loss: 0.00002062
Iteration 176/1000 | Loss: 0.00002062
Iteration 177/1000 | Loss: 0.00002062
Iteration 178/1000 | Loss: 0.00002062
Iteration 179/1000 | Loss: 0.00002062
Iteration 180/1000 | Loss: 0.00002062
Iteration 181/1000 | Loss: 0.00002062
Iteration 182/1000 | Loss: 0.00002062
Iteration 183/1000 | Loss: 0.00002062
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [2.061556551780086e-05, 2.061556551780086e-05, 2.061556551780086e-05, 2.061556551780086e-05, 2.061556551780086e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.061556551780086e-05

Optimization complete. Final v2v error: 3.8894667625427246 mm

Highest mean error: 3.9937896728515625 mm for frame 77

Lowest mean error: 3.7077815532684326 mm for frame 189

Saving results

Total time: 112.60733461380005
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00869738
Iteration 2/25 | Loss: 0.00083902
Iteration 3/25 | Loss: 0.00064466
Iteration 4/25 | Loss: 0.00061670
Iteration 5/25 | Loss: 0.00060943
Iteration 6/25 | Loss: 0.00060719
Iteration 7/25 | Loss: 0.00060694
Iteration 8/25 | Loss: 0.00060694
Iteration 9/25 | Loss: 0.00060694
Iteration 10/25 | Loss: 0.00060694
Iteration 11/25 | Loss: 0.00060694
Iteration 12/25 | Loss: 0.00060694
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006069398368708789, 0.0006069398368708789, 0.0006069398368708789, 0.0006069398368708789, 0.0006069398368708789]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006069398368708789

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45677459
Iteration 2/25 | Loss: 0.00025515
Iteration 3/25 | Loss: 0.00025513
Iteration 4/25 | Loss: 0.00025512
Iteration 5/25 | Loss: 0.00025512
Iteration 6/25 | Loss: 0.00025512
Iteration 7/25 | Loss: 0.00025512
Iteration 8/25 | Loss: 0.00025512
Iteration 9/25 | Loss: 0.00025512
Iteration 10/25 | Loss: 0.00025512
Iteration 11/25 | Loss: 0.00025512
Iteration 12/25 | Loss: 0.00025512
Iteration 13/25 | Loss: 0.00025512
Iteration 14/25 | Loss: 0.00025512
Iteration 15/25 | Loss: 0.00025512
Iteration 16/25 | Loss: 0.00025512
Iteration 17/25 | Loss: 0.00025512
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00025512216961942613, 0.00025512216961942613, 0.00025512216961942613, 0.00025512216961942613, 0.00025512216961942613]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00025512216961942613

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025512
Iteration 2/1000 | Loss: 0.00002087
Iteration 3/1000 | Loss: 0.00001497
Iteration 4/1000 | Loss: 0.00001343
Iteration 5/1000 | Loss: 0.00001255
Iteration 6/1000 | Loss: 0.00001209
Iteration 7/1000 | Loss: 0.00001169
Iteration 8/1000 | Loss: 0.00001154
Iteration 9/1000 | Loss: 0.00001152
Iteration 10/1000 | Loss: 0.00001148
Iteration 11/1000 | Loss: 0.00001138
Iteration 12/1000 | Loss: 0.00001134
Iteration 13/1000 | Loss: 0.00001133
Iteration 14/1000 | Loss: 0.00001133
Iteration 15/1000 | Loss: 0.00001131
Iteration 16/1000 | Loss: 0.00001129
Iteration 17/1000 | Loss: 0.00001128
Iteration 18/1000 | Loss: 0.00001127
Iteration 19/1000 | Loss: 0.00001126
Iteration 20/1000 | Loss: 0.00001125
Iteration 21/1000 | Loss: 0.00001120
Iteration 22/1000 | Loss: 0.00001119
Iteration 23/1000 | Loss: 0.00001119
Iteration 24/1000 | Loss: 0.00001119
Iteration 25/1000 | Loss: 0.00001118
Iteration 26/1000 | Loss: 0.00001117
Iteration 27/1000 | Loss: 0.00001117
Iteration 28/1000 | Loss: 0.00001117
Iteration 29/1000 | Loss: 0.00001116
Iteration 30/1000 | Loss: 0.00001116
Iteration 31/1000 | Loss: 0.00001116
Iteration 32/1000 | Loss: 0.00001116
Iteration 33/1000 | Loss: 0.00001116
Iteration 34/1000 | Loss: 0.00001116
Iteration 35/1000 | Loss: 0.00001116
Iteration 36/1000 | Loss: 0.00001116
Iteration 37/1000 | Loss: 0.00001116
Iteration 38/1000 | Loss: 0.00001115
Iteration 39/1000 | Loss: 0.00001115
Iteration 40/1000 | Loss: 0.00001115
Iteration 41/1000 | Loss: 0.00001114
Iteration 42/1000 | Loss: 0.00001114
Iteration 43/1000 | Loss: 0.00001114
Iteration 44/1000 | Loss: 0.00001114
Iteration 45/1000 | Loss: 0.00001113
Iteration 46/1000 | Loss: 0.00001113
Iteration 47/1000 | Loss: 0.00001113
Iteration 48/1000 | Loss: 0.00001112
Iteration 49/1000 | Loss: 0.00001112
Iteration 50/1000 | Loss: 0.00001112
Iteration 51/1000 | Loss: 0.00001112
Iteration 52/1000 | Loss: 0.00001112
Iteration 53/1000 | Loss: 0.00001112
Iteration 54/1000 | Loss: 0.00001112
Iteration 55/1000 | Loss: 0.00001112
Iteration 56/1000 | Loss: 0.00001111
Iteration 57/1000 | Loss: 0.00001111
Iteration 58/1000 | Loss: 0.00001111
Iteration 59/1000 | Loss: 0.00001111
Iteration 60/1000 | Loss: 0.00001111
Iteration 61/1000 | Loss: 0.00001111
Iteration 62/1000 | Loss: 0.00001111
Iteration 63/1000 | Loss: 0.00001111
Iteration 64/1000 | Loss: 0.00001110
Iteration 65/1000 | Loss: 0.00001110
Iteration 66/1000 | Loss: 0.00001110
Iteration 67/1000 | Loss: 0.00001110
Iteration 68/1000 | Loss: 0.00001110
Iteration 69/1000 | Loss: 0.00001110
Iteration 70/1000 | Loss: 0.00001110
Iteration 71/1000 | Loss: 0.00001110
Iteration 72/1000 | Loss: 0.00001110
Iteration 73/1000 | Loss: 0.00001110
Iteration 74/1000 | Loss: 0.00001110
Iteration 75/1000 | Loss: 0.00001110
Iteration 76/1000 | Loss: 0.00001110
Iteration 77/1000 | Loss: 0.00001110
Iteration 78/1000 | Loss: 0.00001110
Iteration 79/1000 | Loss: 0.00001110
Iteration 80/1000 | Loss: 0.00001110
Iteration 81/1000 | Loss: 0.00001110
Iteration 82/1000 | Loss: 0.00001110
Iteration 83/1000 | Loss: 0.00001110
Iteration 84/1000 | Loss: 0.00001110
Iteration 85/1000 | Loss: 0.00001110
Iteration 86/1000 | Loss: 0.00001110
Iteration 87/1000 | Loss: 0.00001110
Iteration 88/1000 | Loss: 0.00001110
Iteration 89/1000 | Loss: 0.00001110
Iteration 90/1000 | Loss: 0.00001110
Iteration 91/1000 | Loss: 0.00001110
Iteration 92/1000 | Loss: 0.00001110
Iteration 93/1000 | Loss: 0.00001110
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [1.1095345143985469e-05, 1.1095345143985469e-05, 1.1095345143985469e-05, 1.1095345143985469e-05, 1.1095345143985469e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1095345143985469e-05

Optimization complete. Final v2v error: 2.848839044570923 mm

Highest mean error: 2.928471803665161 mm for frame 0

Lowest mean error: 2.75455641746521 mm for frame 146

Saving results

Total time: 30.00502610206604
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00800349
Iteration 2/25 | Loss: 0.00093063
Iteration 3/25 | Loss: 0.00071379
Iteration 4/25 | Loss: 0.00067643
Iteration 5/25 | Loss: 0.00066201
Iteration 6/25 | Loss: 0.00065991
Iteration 7/25 | Loss: 0.00065931
Iteration 8/25 | Loss: 0.00065922
Iteration 9/25 | Loss: 0.00065922
Iteration 10/25 | Loss: 0.00065922
Iteration 11/25 | Loss: 0.00065922
Iteration 12/25 | Loss: 0.00065922
Iteration 13/25 | Loss: 0.00065922
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006592200952582061, 0.0006592200952582061, 0.0006592200952582061, 0.0006592200952582061, 0.0006592200952582061]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006592200952582061

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40423095
Iteration 2/25 | Loss: 0.00027733
Iteration 3/25 | Loss: 0.00027733
Iteration 4/25 | Loss: 0.00027733
Iteration 5/25 | Loss: 0.00027733
Iteration 6/25 | Loss: 0.00027733
Iteration 7/25 | Loss: 0.00027733
Iteration 8/25 | Loss: 0.00027733
Iteration 9/25 | Loss: 0.00027733
Iteration 10/25 | Loss: 0.00027733
Iteration 11/25 | Loss: 0.00027733
Iteration 12/25 | Loss: 0.00027733
Iteration 13/25 | Loss: 0.00027733
Iteration 14/25 | Loss: 0.00027733
Iteration 15/25 | Loss: 0.00027733
Iteration 16/25 | Loss: 0.00027733
Iteration 17/25 | Loss: 0.00027733
Iteration 18/25 | Loss: 0.00027733
Iteration 19/25 | Loss: 0.00027733
Iteration 20/25 | Loss: 0.00027733
Iteration 21/25 | Loss: 0.00027733
Iteration 22/25 | Loss: 0.00027733
Iteration 23/25 | Loss: 0.00027733
Iteration 24/25 | Loss: 0.00027733
Iteration 25/25 | Loss: 0.00027733
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0002773308951873332, 0.0002773308951873332, 0.0002773308951873332, 0.0002773308951873332, 0.0002773308951873332]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002773308951873332

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027733
Iteration 2/1000 | Loss: 0.00004485
Iteration 3/1000 | Loss: 0.00003075
Iteration 4/1000 | Loss: 0.00002327
Iteration 5/1000 | Loss: 0.00002138
Iteration 6/1000 | Loss: 0.00002047
Iteration 7/1000 | Loss: 0.00001981
Iteration 8/1000 | Loss: 0.00001923
Iteration 9/1000 | Loss: 0.00001868
Iteration 10/1000 | Loss: 0.00001833
Iteration 11/1000 | Loss: 0.00001807
Iteration 12/1000 | Loss: 0.00001794
Iteration 13/1000 | Loss: 0.00001786
Iteration 14/1000 | Loss: 0.00001785
Iteration 15/1000 | Loss: 0.00001778
Iteration 16/1000 | Loss: 0.00001776
Iteration 17/1000 | Loss: 0.00001773
Iteration 18/1000 | Loss: 0.00001772
Iteration 19/1000 | Loss: 0.00001768
Iteration 20/1000 | Loss: 0.00001766
Iteration 21/1000 | Loss: 0.00001765
Iteration 22/1000 | Loss: 0.00001764
Iteration 23/1000 | Loss: 0.00001763
Iteration 24/1000 | Loss: 0.00001762
Iteration 25/1000 | Loss: 0.00001761
Iteration 26/1000 | Loss: 0.00001760
Iteration 27/1000 | Loss: 0.00001760
Iteration 28/1000 | Loss: 0.00001760
Iteration 29/1000 | Loss: 0.00001759
Iteration 30/1000 | Loss: 0.00001759
Iteration 31/1000 | Loss: 0.00001758
Iteration 32/1000 | Loss: 0.00001758
Iteration 33/1000 | Loss: 0.00001757
Iteration 34/1000 | Loss: 0.00001757
Iteration 35/1000 | Loss: 0.00001756
Iteration 36/1000 | Loss: 0.00001756
Iteration 37/1000 | Loss: 0.00001755
Iteration 38/1000 | Loss: 0.00001754
Iteration 39/1000 | Loss: 0.00001754
Iteration 40/1000 | Loss: 0.00001753
Iteration 41/1000 | Loss: 0.00001753
Iteration 42/1000 | Loss: 0.00001752
Iteration 43/1000 | Loss: 0.00001752
Iteration 44/1000 | Loss: 0.00001752
Iteration 45/1000 | Loss: 0.00001752
Iteration 46/1000 | Loss: 0.00001751
Iteration 47/1000 | Loss: 0.00001751
Iteration 48/1000 | Loss: 0.00001751
Iteration 49/1000 | Loss: 0.00001750
Iteration 50/1000 | Loss: 0.00001750
Iteration 51/1000 | Loss: 0.00001749
Iteration 52/1000 | Loss: 0.00001749
Iteration 53/1000 | Loss: 0.00001749
Iteration 54/1000 | Loss: 0.00001749
Iteration 55/1000 | Loss: 0.00001748
Iteration 56/1000 | Loss: 0.00001748
Iteration 57/1000 | Loss: 0.00001748
Iteration 58/1000 | Loss: 0.00001748
Iteration 59/1000 | Loss: 0.00001748
Iteration 60/1000 | Loss: 0.00001747
Iteration 61/1000 | Loss: 0.00001747
Iteration 62/1000 | Loss: 0.00001747
Iteration 63/1000 | Loss: 0.00001747
Iteration 64/1000 | Loss: 0.00001747
Iteration 65/1000 | Loss: 0.00001746
Iteration 66/1000 | Loss: 0.00001746
Iteration 67/1000 | Loss: 0.00001746
Iteration 68/1000 | Loss: 0.00001746
Iteration 69/1000 | Loss: 0.00001746
Iteration 70/1000 | Loss: 0.00001745
Iteration 71/1000 | Loss: 0.00001745
Iteration 72/1000 | Loss: 0.00001745
Iteration 73/1000 | Loss: 0.00001745
Iteration 74/1000 | Loss: 0.00001745
Iteration 75/1000 | Loss: 0.00001744
Iteration 76/1000 | Loss: 0.00001744
Iteration 77/1000 | Loss: 0.00001744
Iteration 78/1000 | Loss: 0.00001744
Iteration 79/1000 | Loss: 0.00001744
Iteration 80/1000 | Loss: 0.00001743
Iteration 81/1000 | Loss: 0.00001743
Iteration 82/1000 | Loss: 0.00001743
Iteration 83/1000 | Loss: 0.00001742
Iteration 84/1000 | Loss: 0.00001742
Iteration 85/1000 | Loss: 0.00001742
Iteration 86/1000 | Loss: 0.00001741
Iteration 87/1000 | Loss: 0.00001741
Iteration 88/1000 | Loss: 0.00001741
Iteration 89/1000 | Loss: 0.00001741
Iteration 90/1000 | Loss: 0.00001741
Iteration 91/1000 | Loss: 0.00001741
Iteration 92/1000 | Loss: 0.00001741
Iteration 93/1000 | Loss: 0.00001740
Iteration 94/1000 | Loss: 0.00001740
Iteration 95/1000 | Loss: 0.00001740
Iteration 96/1000 | Loss: 0.00001740
Iteration 97/1000 | Loss: 0.00001740
Iteration 98/1000 | Loss: 0.00001740
Iteration 99/1000 | Loss: 0.00001740
Iteration 100/1000 | Loss: 0.00001740
Iteration 101/1000 | Loss: 0.00001740
Iteration 102/1000 | Loss: 0.00001740
Iteration 103/1000 | Loss: 0.00001740
Iteration 104/1000 | Loss: 0.00001740
Iteration 105/1000 | Loss: 0.00001740
Iteration 106/1000 | Loss: 0.00001739
Iteration 107/1000 | Loss: 0.00001739
Iteration 108/1000 | Loss: 0.00001739
Iteration 109/1000 | Loss: 0.00001739
Iteration 110/1000 | Loss: 0.00001739
Iteration 111/1000 | Loss: 0.00001739
Iteration 112/1000 | Loss: 0.00001739
Iteration 113/1000 | Loss: 0.00001739
Iteration 114/1000 | Loss: 0.00001738
Iteration 115/1000 | Loss: 0.00001738
Iteration 116/1000 | Loss: 0.00001738
Iteration 117/1000 | Loss: 0.00001738
Iteration 118/1000 | Loss: 0.00001738
Iteration 119/1000 | Loss: 0.00001738
Iteration 120/1000 | Loss: 0.00001738
Iteration 121/1000 | Loss: 0.00001738
Iteration 122/1000 | Loss: 0.00001738
Iteration 123/1000 | Loss: 0.00001738
Iteration 124/1000 | Loss: 0.00001738
Iteration 125/1000 | Loss: 0.00001737
Iteration 126/1000 | Loss: 0.00001737
Iteration 127/1000 | Loss: 0.00001737
Iteration 128/1000 | Loss: 0.00001737
Iteration 129/1000 | Loss: 0.00001737
Iteration 130/1000 | Loss: 0.00001737
Iteration 131/1000 | Loss: 0.00001737
Iteration 132/1000 | Loss: 0.00001737
Iteration 133/1000 | Loss: 0.00001737
Iteration 134/1000 | Loss: 0.00001737
Iteration 135/1000 | Loss: 0.00001737
Iteration 136/1000 | Loss: 0.00001737
Iteration 137/1000 | Loss: 0.00001737
Iteration 138/1000 | Loss: 0.00001737
Iteration 139/1000 | Loss: 0.00001736
Iteration 140/1000 | Loss: 0.00001736
Iteration 141/1000 | Loss: 0.00001736
Iteration 142/1000 | Loss: 0.00001736
Iteration 143/1000 | Loss: 0.00001736
Iteration 144/1000 | Loss: 0.00001736
Iteration 145/1000 | Loss: 0.00001736
Iteration 146/1000 | Loss: 0.00001736
Iteration 147/1000 | Loss: 0.00001736
Iteration 148/1000 | Loss: 0.00001736
Iteration 149/1000 | Loss: 0.00001736
Iteration 150/1000 | Loss: 0.00001736
Iteration 151/1000 | Loss: 0.00001736
Iteration 152/1000 | Loss: 0.00001736
Iteration 153/1000 | Loss: 0.00001736
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.7359810954076238e-05, 1.7359810954076238e-05, 1.7359810954076238e-05, 1.7359810954076238e-05, 1.7359810954076238e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7359810954076238e-05

Optimization complete. Final v2v error: 3.4844229221343994 mm

Highest mean error: 3.832150459289551 mm for frame 45

Lowest mean error: 3.191375255584717 mm for frame 108

Saving results

Total time: 39.69824552536011
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00822372
Iteration 2/25 | Loss: 0.00101610
Iteration 3/25 | Loss: 0.00067501
Iteration 4/25 | Loss: 0.00064568
Iteration 5/25 | Loss: 0.00063745
Iteration 6/25 | Loss: 0.00063568
Iteration 7/25 | Loss: 0.00063549
Iteration 8/25 | Loss: 0.00063549
Iteration 9/25 | Loss: 0.00063549
Iteration 10/25 | Loss: 0.00063549
Iteration 11/25 | Loss: 0.00063549
Iteration 12/25 | Loss: 0.00063549
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006354859797284007, 0.0006354859797284007, 0.0006354859797284007, 0.0006354859797284007, 0.0006354859797284007]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006354859797284007

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27263641
Iteration 2/25 | Loss: 0.00025739
Iteration 3/25 | Loss: 0.00025738
Iteration 4/25 | Loss: 0.00025738
Iteration 5/25 | Loss: 0.00025738
Iteration 6/25 | Loss: 0.00025738
Iteration 7/25 | Loss: 0.00025738
Iteration 8/25 | Loss: 0.00025738
Iteration 9/25 | Loss: 0.00025738
Iteration 10/25 | Loss: 0.00025738
Iteration 11/25 | Loss: 0.00025738
Iteration 12/25 | Loss: 0.00025738
Iteration 13/25 | Loss: 0.00025738
Iteration 14/25 | Loss: 0.00025738
Iteration 15/25 | Loss: 0.00025738
Iteration 16/25 | Loss: 0.00025738
Iteration 17/25 | Loss: 0.00025738
Iteration 18/25 | Loss: 0.00025738
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00025738051044754684, 0.00025738051044754684, 0.00025738051044754684, 0.00025738051044754684, 0.00025738051044754684]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00025738051044754684

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025738
Iteration 2/1000 | Loss: 0.00003723
Iteration 3/1000 | Loss: 0.00002586
Iteration 4/1000 | Loss: 0.00002153
Iteration 5/1000 | Loss: 0.00002035
Iteration 6/1000 | Loss: 0.00001921
Iteration 7/1000 | Loss: 0.00001865
Iteration 8/1000 | Loss: 0.00001821
Iteration 9/1000 | Loss: 0.00001789
Iteration 10/1000 | Loss: 0.00001783
Iteration 11/1000 | Loss: 0.00001775
Iteration 12/1000 | Loss: 0.00001757
Iteration 13/1000 | Loss: 0.00001752
Iteration 14/1000 | Loss: 0.00001751
Iteration 15/1000 | Loss: 0.00001748
Iteration 16/1000 | Loss: 0.00001743
Iteration 17/1000 | Loss: 0.00001735
Iteration 18/1000 | Loss: 0.00001729
Iteration 19/1000 | Loss: 0.00001726
Iteration 20/1000 | Loss: 0.00001725
Iteration 21/1000 | Loss: 0.00001725
Iteration 22/1000 | Loss: 0.00001724
Iteration 23/1000 | Loss: 0.00001724
Iteration 24/1000 | Loss: 0.00001724
Iteration 25/1000 | Loss: 0.00001724
Iteration 26/1000 | Loss: 0.00001723
Iteration 27/1000 | Loss: 0.00001723
Iteration 28/1000 | Loss: 0.00001722
Iteration 29/1000 | Loss: 0.00001721
Iteration 30/1000 | Loss: 0.00001721
Iteration 31/1000 | Loss: 0.00001718
Iteration 32/1000 | Loss: 0.00001717
Iteration 33/1000 | Loss: 0.00001716
Iteration 34/1000 | Loss: 0.00001714
Iteration 35/1000 | Loss: 0.00001713
Iteration 36/1000 | Loss: 0.00001712
Iteration 37/1000 | Loss: 0.00001711
Iteration 38/1000 | Loss: 0.00001711
Iteration 39/1000 | Loss: 0.00001710
Iteration 40/1000 | Loss: 0.00001710
Iteration 41/1000 | Loss: 0.00001709
Iteration 42/1000 | Loss: 0.00001707
Iteration 43/1000 | Loss: 0.00001707
Iteration 44/1000 | Loss: 0.00001707
Iteration 45/1000 | Loss: 0.00001706
Iteration 46/1000 | Loss: 0.00001706
Iteration 47/1000 | Loss: 0.00001706
Iteration 48/1000 | Loss: 0.00001706
Iteration 49/1000 | Loss: 0.00001705
Iteration 50/1000 | Loss: 0.00001705
Iteration 51/1000 | Loss: 0.00001705
Iteration 52/1000 | Loss: 0.00001704
Iteration 53/1000 | Loss: 0.00001704
Iteration 54/1000 | Loss: 0.00001704
Iteration 55/1000 | Loss: 0.00001703
Iteration 56/1000 | Loss: 0.00001703
Iteration 57/1000 | Loss: 0.00001703
Iteration 58/1000 | Loss: 0.00001703
Iteration 59/1000 | Loss: 0.00001703
Iteration 60/1000 | Loss: 0.00001702
Iteration 61/1000 | Loss: 0.00001702
Iteration 62/1000 | Loss: 0.00001702
Iteration 63/1000 | Loss: 0.00001702
Iteration 64/1000 | Loss: 0.00001702
Iteration 65/1000 | Loss: 0.00001702
Iteration 66/1000 | Loss: 0.00001702
Iteration 67/1000 | Loss: 0.00001702
Iteration 68/1000 | Loss: 0.00001702
Iteration 69/1000 | Loss: 0.00001701
Iteration 70/1000 | Loss: 0.00001701
Iteration 71/1000 | Loss: 0.00001701
Iteration 72/1000 | Loss: 0.00001701
Iteration 73/1000 | Loss: 0.00001701
Iteration 74/1000 | Loss: 0.00001701
Iteration 75/1000 | Loss: 0.00001701
Iteration 76/1000 | Loss: 0.00001701
Iteration 77/1000 | Loss: 0.00001701
Iteration 78/1000 | Loss: 0.00001701
Iteration 79/1000 | Loss: 0.00001701
Iteration 80/1000 | Loss: 0.00001701
Iteration 81/1000 | Loss: 0.00001701
Iteration 82/1000 | Loss: 0.00001701
Iteration 83/1000 | Loss: 0.00001701
Iteration 84/1000 | Loss: 0.00001701
Iteration 85/1000 | Loss: 0.00001700
Iteration 86/1000 | Loss: 0.00001700
Iteration 87/1000 | Loss: 0.00001700
Iteration 88/1000 | Loss: 0.00001700
Iteration 89/1000 | Loss: 0.00001700
Iteration 90/1000 | Loss: 0.00001700
Iteration 91/1000 | Loss: 0.00001699
Iteration 92/1000 | Loss: 0.00001699
Iteration 93/1000 | Loss: 0.00001699
Iteration 94/1000 | Loss: 0.00001699
Iteration 95/1000 | Loss: 0.00001699
Iteration 96/1000 | Loss: 0.00001698
Iteration 97/1000 | Loss: 0.00001698
Iteration 98/1000 | Loss: 0.00001698
Iteration 99/1000 | Loss: 0.00001698
Iteration 100/1000 | Loss: 0.00001698
Iteration 101/1000 | Loss: 0.00001697
Iteration 102/1000 | Loss: 0.00001697
Iteration 103/1000 | Loss: 0.00001697
Iteration 104/1000 | Loss: 0.00001697
Iteration 105/1000 | Loss: 0.00001696
Iteration 106/1000 | Loss: 0.00001696
Iteration 107/1000 | Loss: 0.00001696
Iteration 108/1000 | Loss: 0.00001696
Iteration 109/1000 | Loss: 0.00001696
Iteration 110/1000 | Loss: 0.00001695
Iteration 111/1000 | Loss: 0.00001695
Iteration 112/1000 | Loss: 0.00001695
Iteration 113/1000 | Loss: 0.00001694
Iteration 114/1000 | Loss: 0.00001694
Iteration 115/1000 | Loss: 0.00001694
Iteration 116/1000 | Loss: 0.00001694
Iteration 117/1000 | Loss: 0.00001694
Iteration 118/1000 | Loss: 0.00001693
Iteration 119/1000 | Loss: 0.00001691
Iteration 120/1000 | Loss: 0.00001691
Iteration 121/1000 | Loss: 0.00001690
Iteration 122/1000 | Loss: 0.00001690
Iteration 123/1000 | Loss: 0.00001690
Iteration 124/1000 | Loss: 0.00001690
Iteration 125/1000 | Loss: 0.00001690
Iteration 126/1000 | Loss: 0.00001690
Iteration 127/1000 | Loss: 0.00001689
Iteration 128/1000 | Loss: 0.00001689
Iteration 129/1000 | Loss: 0.00001689
Iteration 130/1000 | Loss: 0.00001689
Iteration 131/1000 | Loss: 0.00001689
Iteration 132/1000 | Loss: 0.00001689
Iteration 133/1000 | Loss: 0.00001689
Iteration 134/1000 | Loss: 0.00001689
Iteration 135/1000 | Loss: 0.00001688
Iteration 136/1000 | Loss: 0.00001688
Iteration 137/1000 | Loss: 0.00001688
Iteration 138/1000 | Loss: 0.00001688
Iteration 139/1000 | Loss: 0.00001688
Iteration 140/1000 | Loss: 0.00001688
Iteration 141/1000 | Loss: 0.00001688
Iteration 142/1000 | Loss: 0.00001688
Iteration 143/1000 | Loss: 0.00001688
Iteration 144/1000 | Loss: 0.00001688
Iteration 145/1000 | Loss: 0.00001687
Iteration 146/1000 | Loss: 0.00001687
Iteration 147/1000 | Loss: 0.00001687
Iteration 148/1000 | Loss: 0.00001687
Iteration 149/1000 | Loss: 0.00001687
Iteration 150/1000 | Loss: 0.00001687
Iteration 151/1000 | Loss: 0.00001687
Iteration 152/1000 | Loss: 0.00001687
Iteration 153/1000 | Loss: 0.00001687
Iteration 154/1000 | Loss: 0.00001687
Iteration 155/1000 | Loss: 0.00001687
Iteration 156/1000 | Loss: 0.00001686
Iteration 157/1000 | Loss: 0.00001686
Iteration 158/1000 | Loss: 0.00001686
Iteration 159/1000 | Loss: 0.00001686
Iteration 160/1000 | Loss: 0.00001685
Iteration 161/1000 | Loss: 0.00001685
Iteration 162/1000 | Loss: 0.00001685
Iteration 163/1000 | Loss: 0.00001685
Iteration 164/1000 | Loss: 0.00001684
Iteration 165/1000 | Loss: 0.00001684
Iteration 166/1000 | Loss: 0.00001684
Iteration 167/1000 | Loss: 0.00001684
Iteration 168/1000 | Loss: 0.00001684
Iteration 169/1000 | Loss: 0.00001684
Iteration 170/1000 | Loss: 0.00001684
Iteration 171/1000 | Loss: 0.00001684
Iteration 172/1000 | Loss: 0.00001684
Iteration 173/1000 | Loss: 0.00001684
Iteration 174/1000 | Loss: 0.00001684
Iteration 175/1000 | Loss: 0.00001684
Iteration 176/1000 | Loss: 0.00001684
Iteration 177/1000 | Loss: 0.00001684
Iteration 178/1000 | Loss: 0.00001684
Iteration 179/1000 | Loss: 0.00001684
Iteration 180/1000 | Loss: 0.00001684
Iteration 181/1000 | Loss: 0.00001684
Iteration 182/1000 | Loss: 0.00001683
Iteration 183/1000 | Loss: 0.00001683
Iteration 184/1000 | Loss: 0.00001683
Iteration 185/1000 | Loss: 0.00001683
Iteration 186/1000 | Loss: 0.00001683
Iteration 187/1000 | Loss: 0.00001683
Iteration 188/1000 | Loss: 0.00001683
Iteration 189/1000 | Loss: 0.00001683
Iteration 190/1000 | Loss: 0.00001683
Iteration 191/1000 | Loss: 0.00001683
Iteration 192/1000 | Loss: 0.00001683
Iteration 193/1000 | Loss: 0.00001683
Iteration 194/1000 | Loss: 0.00001683
Iteration 195/1000 | Loss: 0.00001683
Iteration 196/1000 | Loss: 0.00001683
Iteration 197/1000 | Loss: 0.00001683
Iteration 198/1000 | Loss: 0.00001683
Iteration 199/1000 | Loss: 0.00001683
Iteration 200/1000 | Loss: 0.00001683
Iteration 201/1000 | Loss: 0.00001683
Iteration 202/1000 | Loss: 0.00001683
Iteration 203/1000 | Loss: 0.00001683
Iteration 204/1000 | Loss: 0.00001683
Iteration 205/1000 | Loss: 0.00001683
Iteration 206/1000 | Loss: 0.00001683
Iteration 207/1000 | Loss: 0.00001683
Iteration 208/1000 | Loss: 0.00001683
Iteration 209/1000 | Loss: 0.00001683
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [1.6827316358103417e-05, 1.6827316358103417e-05, 1.6827316358103417e-05, 1.6827316358103417e-05, 1.6827316358103417e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6827316358103417e-05

Optimization complete. Final v2v error: 3.409856081008911 mm

Highest mean error: 5.050487041473389 mm for frame 77

Lowest mean error: 2.7980310916900635 mm for frame 107

Saving results

Total time: 44.37382173538208
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00801838
Iteration 2/25 | Loss: 0.00129223
Iteration 3/25 | Loss: 0.00089856
Iteration 4/25 | Loss: 0.00082350
Iteration 5/25 | Loss: 0.00080889
Iteration 6/25 | Loss: 0.00080735
Iteration 7/25 | Loss: 0.00080717
Iteration 8/25 | Loss: 0.00080717
Iteration 9/25 | Loss: 0.00080717
Iteration 10/25 | Loss: 0.00080717
Iteration 11/25 | Loss: 0.00080717
Iteration 12/25 | Loss: 0.00080717
Iteration 13/25 | Loss: 0.00080717
Iteration 14/25 | Loss: 0.00080717
Iteration 15/25 | Loss: 0.00080717
Iteration 16/25 | Loss: 0.00080717
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008071715710684657, 0.0008071715710684657, 0.0008071715710684657, 0.0008071715710684657, 0.0008071715710684657]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008071715710684657

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44866884
Iteration 2/25 | Loss: 0.00026796
Iteration 3/25 | Loss: 0.00026793
Iteration 4/25 | Loss: 0.00026793
Iteration 5/25 | Loss: 0.00026792
Iteration 6/25 | Loss: 0.00026792
Iteration 7/25 | Loss: 0.00026792
Iteration 8/25 | Loss: 0.00026792
Iteration 9/25 | Loss: 0.00026792
Iteration 10/25 | Loss: 0.00026792
Iteration 11/25 | Loss: 0.00026792
Iteration 12/25 | Loss: 0.00026792
Iteration 13/25 | Loss: 0.00026792
Iteration 14/25 | Loss: 0.00026792
Iteration 15/25 | Loss: 0.00026792
Iteration 16/25 | Loss: 0.00026792
Iteration 17/25 | Loss: 0.00026792
Iteration 18/25 | Loss: 0.00026792
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0002679238095879555, 0.0002679238095879555, 0.0002679238095879555, 0.0002679238095879555, 0.0002679238095879555]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002679238095879555

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026792
Iteration 2/1000 | Loss: 0.00003878
Iteration 3/1000 | Loss: 0.00003117
Iteration 4/1000 | Loss: 0.00002860
Iteration 5/1000 | Loss: 0.00002751
Iteration 6/1000 | Loss: 0.00002702
Iteration 7/1000 | Loss: 0.00002666
Iteration 8/1000 | Loss: 0.00002634
Iteration 9/1000 | Loss: 0.00002599
Iteration 10/1000 | Loss: 0.00002573
Iteration 11/1000 | Loss: 0.00002553
Iteration 12/1000 | Loss: 0.00002544
Iteration 13/1000 | Loss: 0.00002538
Iteration 14/1000 | Loss: 0.00002531
Iteration 15/1000 | Loss: 0.00002529
Iteration 16/1000 | Loss: 0.00002526
Iteration 17/1000 | Loss: 0.00002526
Iteration 18/1000 | Loss: 0.00002526
Iteration 19/1000 | Loss: 0.00002526
Iteration 20/1000 | Loss: 0.00002526
Iteration 21/1000 | Loss: 0.00002524
Iteration 22/1000 | Loss: 0.00002521
Iteration 23/1000 | Loss: 0.00002516
Iteration 24/1000 | Loss: 0.00002516
Iteration 25/1000 | Loss: 0.00002516
Iteration 26/1000 | Loss: 0.00002516
Iteration 27/1000 | Loss: 0.00002516
Iteration 28/1000 | Loss: 0.00002516
Iteration 29/1000 | Loss: 0.00002516
Iteration 30/1000 | Loss: 0.00002515
Iteration 31/1000 | Loss: 0.00002515
Iteration 32/1000 | Loss: 0.00002515
Iteration 33/1000 | Loss: 0.00002512
Iteration 34/1000 | Loss: 0.00002512
Iteration 35/1000 | Loss: 0.00002512
Iteration 36/1000 | Loss: 0.00002511
Iteration 37/1000 | Loss: 0.00002511
Iteration 38/1000 | Loss: 0.00002511
Iteration 39/1000 | Loss: 0.00002510
Iteration 40/1000 | Loss: 0.00002510
Iteration 41/1000 | Loss: 0.00002510
Iteration 42/1000 | Loss: 0.00002510
Iteration 43/1000 | Loss: 0.00002510
Iteration 44/1000 | Loss: 0.00002510
Iteration 45/1000 | Loss: 0.00002510
Iteration 46/1000 | Loss: 0.00002510
Iteration 47/1000 | Loss: 0.00002510
Iteration 48/1000 | Loss: 0.00002510
Iteration 49/1000 | Loss: 0.00002510
Iteration 50/1000 | Loss: 0.00002510
Iteration 51/1000 | Loss: 0.00002509
Iteration 52/1000 | Loss: 0.00002509
Iteration 53/1000 | Loss: 0.00002509
Iteration 54/1000 | Loss: 0.00002509
Iteration 55/1000 | Loss: 0.00002509
Iteration 56/1000 | Loss: 0.00002509
Iteration 57/1000 | Loss: 0.00002509
Iteration 58/1000 | Loss: 0.00002509
Iteration 59/1000 | Loss: 0.00002509
Iteration 60/1000 | Loss: 0.00002508
Iteration 61/1000 | Loss: 0.00002508
Iteration 62/1000 | Loss: 0.00002508
Iteration 63/1000 | Loss: 0.00002508
Iteration 64/1000 | Loss: 0.00002508
Iteration 65/1000 | Loss: 0.00002508
Iteration 66/1000 | Loss: 0.00002508
Iteration 67/1000 | Loss: 0.00002508
Iteration 68/1000 | Loss: 0.00002507
Iteration 69/1000 | Loss: 0.00002507
Iteration 70/1000 | Loss: 0.00002507
Iteration 71/1000 | Loss: 0.00002507
Iteration 72/1000 | Loss: 0.00002507
Iteration 73/1000 | Loss: 0.00002507
Iteration 74/1000 | Loss: 0.00002507
Iteration 75/1000 | Loss: 0.00002507
Iteration 76/1000 | Loss: 0.00002507
Iteration 77/1000 | Loss: 0.00002507
Iteration 78/1000 | Loss: 0.00002507
Iteration 79/1000 | Loss: 0.00002507
Iteration 80/1000 | Loss: 0.00002507
Iteration 81/1000 | Loss: 0.00002507
Iteration 82/1000 | Loss: 0.00002507
Iteration 83/1000 | Loss: 0.00002507
Iteration 84/1000 | Loss: 0.00002507
Iteration 85/1000 | Loss: 0.00002507
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 85. Stopping optimization.
Last 5 losses: [2.5066541638807394e-05, 2.5066541638807394e-05, 2.5066541638807394e-05, 2.5066541638807394e-05, 2.5066541638807394e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5066541638807394e-05

Optimization complete. Final v2v error: 4.223635673522949 mm

Highest mean error: 4.44533634185791 mm for frame 119

Lowest mean error: 4.038524150848389 mm for frame 97

Saving results

Total time: 32.888917446136475
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00650378
Iteration 2/25 | Loss: 0.00127190
Iteration 3/25 | Loss: 0.00076840
Iteration 4/25 | Loss: 0.00068906
Iteration 5/25 | Loss: 0.00064451
Iteration 6/25 | Loss: 0.00063798
Iteration 7/25 | Loss: 0.00063332
Iteration 8/25 | Loss: 0.00063437
Iteration 9/25 | Loss: 0.00063464
Iteration 10/25 | Loss: 0.00062868
Iteration 11/25 | Loss: 0.00063244
Iteration 12/25 | Loss: 0.00062807
Iteration 13/25 | Loss: 0.00062788
Iteration 14/25 | Loss: 0.00062778
Iteration 15/25 | Loss: 0.00062770
Iteration 16/25 | Loss: 0.00062768
Iteration 17/25 | Loss: 0.00062768
Iteration 18/25 | Loss: 0.00062768
Iteration 19/25 | Loss: 0.00062767
Iteration 20/25 | Loss: 0.00062767
Iteration 21/25 | Loss: 0.00062767
Iteration 22/25 | Loss: 0.00062767
Iteration 23/25 | Loss: 0.00062767
Iteration 24/25 | Loss: 0.00062767
Iteration 25/25 | Loss: 0.00062767

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.00115156
Iteration 2/25 | Loss: 0.00021587
Iteration 3/25 | Loss: 0.00021582
Iteration 4/25 | Loss: 0.00021582
Iteration 5/25 | Loss: 0.00021582
Iteration 6/25 | Loss: 0.00021582
Iteration 7/25 | Loss: 0.00021582
Iteration 8/25 | Loss: 0.00021582
Iteration 9/25 | Loss: 0.00021582
Iteration 10/25 | Loss: 0.00021582
Iteration 11/25 | Loss: 0.00021582
Iteration 12/25 | Loss: 0.00021582
Iteration 13/25 | Loss: 0.00021582
Iteration 14/25 | Loss: 0.00021582
Iteration 15/25 | Loss: 0.00021582
Iteration 16/25 | Loss: 0.00021582
Iteration 17/25 | Loss: 0.00021582
Iteration 18/25 | Loss: 0.00021582
Iteration 19/25 | Loss: 0.00021582
Iteration 20/25 | Loss: 0.00021582
Iteration 21/25 | Loss: 0.00021582
Iteration 22/25 | Loss: 0.00021582
Iteration 23/25 | Loss: 0.00021582
Iteration 24/25 | Loss: 0.00021582
Iteration 25/25 | Loss: 0.00021582

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00021582
Iteration 2/1000 | Loss: 0.00002170
Iteration 3/1000 | Loss: 0.00001484
Iteration 4/1000 | Loss: 0.00001367
Iteration 5/1000 | Loss: 0.00001283
Iteration 6/1000 | Loss: 0.00001245
Iteration 7/1000 | Loss: 0.00001212
Iteration 8/1000 | Loss: 0.00001191
Iteration 9/1000 | Loss: 0.00001177
Iteration 10/1000 | Loss: 0.00001162
Iteration 11/1000 | Loss: 0.00001160
Iteration 12/1000 | Loss: 0.00001160
Iteration 13/1000 | Loss: 0.00001160
Iteration 14/1000 | Loss: 0.00001159
Iteration 15/1000 | Loss: 0.00001158
Iteration 16/1000 | Loss: 0.00001149
Iteration 17/1000 | Loss: 0.00001148
Iteration 18/1000 | Loss: 0.00001142
Iteration 19/1000 | Loss: 0.00001140
Iteration 20/1000 | Loss: 0.00001137
Iteration 21/1000 | Loss: 0.00001136
Iteration 22/1000 | Loss: 0.00001131
Iteration 23/1000 | Loss: 0.00001131
Iteration 24/1000 | Loss: 0.00001131
Iteration 25/1000 | Loss: 0.00001131
Iteration 26/1000 | Loss: 0.00001129
Iteration 27/1000 | Loss: 0.00001129
Iteration 28/1000 | Loss: 0.00001128
Iteration 29/1000 | Loss: 0.00001128
Iteration 30/1000 | Loss: 0.00001128
Iteration 31/1000 | Loss: 0.00001128
Iteration 32/1000 | Loss: 0.00001127
Iteration 33/1000 | Loss: 0.00001127
Iteration 34/1000 | Loss: 0.00001127
Iteration 35/1000 | Loss: 0.00001127
Iteration 36/1000 | Loss: 0.00001126
Iteration 37/1000 | Loss: 0.00001126
Iteration 38/1000 | Loss: 0.00001126
Iteration 39/1000 | Loss: 0.00001126
Iteration 40/1000 | Loss: 0.00001125
Iteration 41/1000 | Loss: 0.00001125
Iteration 42/1000 | Loss: 0.00001125
Iteration 43/1000 | Loss: 0.00001125
Iteration 44/1000 | Loss: 0.00001125
Iteration 45/1000 | Loss: 0.00001125
Iteration 46/1000 | Loss: 0.00001125
Iteration 47/1000 | Loss: 0.00001125
Iteration 48/1000 | Loss: 0.00001125
Iteration 49/1000 | Loss: 0.00001125
Iteration 50/1000 | Loss: 0.00001124
Iteration 51/1000 | Loss: 0.00001124
Iteration 52/1000 | Loss: 0.00001124
Iteration 53/1000 | Loss: 0.00001124
Iteration 54/1000 | Loss: 0.00001124
Iteration 55/1000 | Loss: 0.00001124
Iteration 56/1000 | Loss: 0.00001124
Iteration 57/1000 | Loss: 0.00001123
Iteration 58/1000 | Loss: 0.00001123
Iteration 59/1000 | Loss: 0.00001123
Iteration 60/1000 | Loss: 0.00001122
Iteration 61/1000 | Loss: 0.00001122
Iteration 62/1000 | Loss: 0.00001121
Iteration 63/1000 | Loss: 0.00001121
Iteration 64/1000 | Loss: 0.00001121
Iteration 65/1000 | Loss: 0.00001121
Iteration 66/1000 | Loss: 0.00001121
Iteration 67/1000 | Loss: 0.00001120
Iteration 68/1000 | Loss: 0.00001118
Iteration 69/1000 | Loss: 0.00001118
Iteration 70/1000 | Loss: 0.00001118
Iteration 71/1000 | Loss: 0.00001118
Iteration 72/1000 | Loss: 0.00001118
Iteration 73/1000 | Loss: 0.00001118
Iteration 74/1000 | Loss: 0.00001118
Iteration 75/1000 | Loss: 0.00001117
Iteration 76/1000 | Loss: 0.00001117
Iteration 77/1000 | Loss: 0.00001116
Iteration 78/1000 | Loss: 0.00001115
Iteration 79/1000 | Loss: 0.00001115
Iteration 80/1000 | Loss: 0.00001115
Iteration 81/1000 | Loss: 0.00001115
Iteration 82/1000 | Loss: 0.00001115
Iteration 83/1000 | Loss: 0.00001115
Iteration 84/1000 | Loss: 0.00001115
Iteration 85/1000 | Loss: 0.00001115
Iteration 86/1000 | Loss: 0.00001115
Iteration 87/1000 | Loss: 0.00001115
Iteration 88/1000 | Loss: 0.00001115
Iteration 89/1000 | Loss: 0.00001115
Iteration 90/1000 | Loss: 0.00001115
Iteration 91/1000 | Loss: 0.00001115
Iteration 92/1000 | Loss: 0.00001114
Iteration 93/1000 | Loss: 0.00001114
Iteration 94/1000 | Loss: 0.00001114
Iteration 95/1000 | Loss: 0.00001114
Iteration 96/1000 | Loss: 0.00001113
Iteration 97/1000 | Loss: 0.00001113
Iteration 98/1000 | Loss: 0.00001113
Iteration 99/1000 | Loss: 0.00001113
Iteration 100/1000 | Loss: 0.00001112
Iteration 101/1000 | Loss: 0.00001112
Iteration 102/1000 | Loss: 0.00001112
Iteration 103/1000 | Loss: 0.00001112
Iteration 104/1000 | Loss: 0.00001112
Iteration 105/1000 | Loss: 0.00001111
Iteration 106/1000 | Loss: 0.00001111
Iteration 107/1000 | Loss: 0.00001111
Iteration 108/1000 | Loss: 0.00001111
Iteration 109/1000 | Loss: 0.00001111
Iteration 110/1000 | Loss: 0.00001111
Iteration 111/1000 | Loss: 0.00001111
Iteration 112/1000 | Loss: 0.00001111
Iteration 113/1000 | Loss: 0.00001110
Iteration 114/1000 | Loss: 0.00001110
Iteration 115/1000 | Loss: 0.00001110
Iteration 116/1000 | Loss: 0.00001110
Iteration 117/1000 | Loss: 0.00001110
Iteration 118/1000 | Loss: 0.00001109
Iteration 119/1000 | Loss: 0.00001109
Iteration 120/1000 | Loss: 0.00001108
Iteration 121/1000 | Loss: 0.00001108
Iteration 122/1000 | Loss: 0.00001108
Iteration 123/1000 | Loss: 0.00001108
Iteration 124/1000 | Loss: 0.00001108
Iteration 125/1000 | Loss: 0.00001108
Iteration 126/1000 | Loss: 0.00001108
Iteration 127/1000 | Loss: 0.00001108
Iteration 128/1000 | Loss: 0.00001108
Iteration 129/1000 | Loss: 0.00001107
Iteration 130/1000 | Loss: 0.00001107
Iteration 131/1000 | Loss: 0.00001107
Iteration 132/1000 | Loss: 0.00001107
Iteration 133/1000 | Loss: 0.00001107
Iteration 134/1000 | Loss: 0.00001107
Iteration 135/1000 | Loss: 0.00001107
Iteration 136/1000 | Loss: 0.00001107
Iteration 137/1000 | Loss: 0.00001107
Iteration 138/1000 | Loss: 0.00001107
Iteration 139/1000 | Loss: 0.00001107
Iteration 140/1000 | Loss: 0.00001107
Iteration 141/1000 | Loss: 0.00001106
Iteration 142/1000 | Loss: 0.00001106
Iteration 143/1000 | Loss: 0.00001106
Iteration 144/1000 | Loss: 0.00001106
Iteration 145/1000 | Loss: 0.00001106
Iteration 146/1000 | Loss: 0.00001106
Iteration 147/1000 | Loss: 0.00001106
Iteration 148/1000 | Loss: 0.00001106
Iteration 149/1000 | Loss: 0.00001106
Iteration 150/1000 | Loss: 0.00001106
Iteration 151/1000 | Loss: 0.00001106
Iteration 152/1000 | Loss: 0.00001106
Iteration 153/1000 | Loss: 0.00001106
Iteration 154/1000 | Loss: 0.00001106
Iteration 155/1000 | Loss: 0.00001106
Iteration 156/1000 | Loss: 0.00001106
Iteration 157/1000 | Loss: 0.00001106
Iteration 158/1000 | Loss: 0.00001106
Iteration 159/1000 | Loss: 0.00001106
Iteration 160/1000 | Loss: 0.00001106
Iteration 161/1000 | Loss: 0.00001106
Iteration 162/1000 | Loss: 0.00001106
Iteration 163/1000 | Loss: 0.00001106
Iteration 164/1000 | Loss: 0.00001106
Iteration 165/1000 | Loss: 0.00001106
Iteration 166/1000 | Loss: 0.00001106
Iteration 167/1000 | Loss: 0.00001106
Iteration 168/1000 | Loss: 0.00001106
Iteration 169/1000 | Loss: 0.00001106
Iteration 170/1000 | Loss: 0.00001106
Iteration 171/1000 | Loss: 0.00001106
Iteration 172/1000 | Loss: 0.00001106
Iteration 173/1000 | Loss: 0.00001106
Iteration 174/1000 | Loss: 0.00001106
Iteration 175/1000 | Loss: 0.00001106
Iteration 176/1000 | Loss: 0.00001106
Iteration 177/1000 | Loss: 0.00001106
Iteration 178/1000 | Loss: 0.00001106
Iteration 179/1000 | Loss: 0.00001106
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.106144736695569e-05, 1.106144736695569e-05, 1.106144736695569e-05, 1.106144736695569e-05, 1.106144736695569e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.106144736695569e-05

Optimization complete. Final v2v error: 2.7656896114349365 mm

Highest mean error: 3.725313663482666 mm for frame 92

Lowest mean error: 2.435750722885132 mm for frame 23

Saving results

Total time: 63.149893283843994
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00835095
Iteration 2/25 | Loss: 0.00096797
Iteration 3/25 | Loss: 0.00078858
Iteration 4/25 | Loss: 0.00074210
Iteration 5/25 | Loss: 0.00071800
Iteration 6/25 | Loss: 0.00071222
Iteration 7/25 | Loss: 0.00070957
Iteration 8/25 | Loss: 0.00070830
Iteration 9/25 | Loss: 0.00070825
Iteration 10/25 | Loss: 0.00070825
Iteration 11/25 | Loss: 0.00070825
Iteration 12/25 | Loss: 0.00070825
Iteration 13/25 | Loss: 0.00070825
Iteration 14/25 | Loss: 0.00070825
Iteration 15/25 | Loss: 0.00070825
Iteration 16/25 | Loss: 0.00070825
Iteration 17/25 | Loss: 0.00070825
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007082468364387751, 0.0007082468364387751, 0.0007082468364387751, 0.0007082468364387751, 0.0007082468364387751]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007082468364387751

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56799662
Iteration 2/25 | Loss: 0.00046327
Iteration 3/25 | Loss: 0.00046327
Iteration 4/25 | Loss: 0.00046327
Iteration 5/25 | Loss: 0.00046327
Iteration 6/25 | Loss: 0.00046327
Iteration 7/25 | Loss: 0.00046327
Iteration 8/25 | Loss: 0.00046327
Iteration 9/25 | Loss: 0.00046327
Iteration 10/25 | Loss: 0.00046327
Iteration 11/25 | Loss: 0.00046327
Iteration 12/25 | Loss: 0.00046327
Iteration 13/25 | Loss: 0.00046327
Iteration 14/25 | Loss: 0.00046327
Iteration 15/25 | Loss: 0.00046327
Iteration 16/25 | Loss: 0.00046327
Iteration 17/25 | Loss: 0.00046327
Iteration 18/25 | Loss: 0.00046327
Iteration 19/25 | Loss: 0.00046327
Iteration 20/25 | Loss: 0.00046327
Iteration 21/25 | Loss: 0.00046327
Iteration 22/25 | Loss: 0.00046327
Iteration 23/25 | Loss: 0.00046327
Iteration 24/25 | Loss: 0.00046327
Iteration 25/25 | Loss: 0.00046327

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046327
Iteration 2/1000 | Loss: 0.00004644
Iteration 3/1000 | Loss: 0.00003723
Iteration 4/1000 | Loss: 0.00003027
Iteration 5/1000 | Loss: 0.00002825
Iteration 6/1000 | Loss: 0.00002734
Iteration 7/1000 | Loss: 0.00002660
Iteration 8/1000 | Loss: 0.00002606
Iteration 9/1000 | Loss: 0.00002552
Iteration 10/1000 | Loss: 0.00002502
Iteration 11/1000 | Loss: 0.00002466
Iteration 12/1000 | Loss: 0.00002440
Iteration 13/1000 | Loss: 0.00002436
Iteration 14/1000 | Loss: 0.00002424
Iteration 15/1000 | Loss: 0.00002409
Iteration 16/1000 | Loss: 0.00002408
Iteration 17/1000 | Loss: 0.00002407
Iteration 18/1000 | Loss: 0.00002407
Iteration 19/1000 | Loss: 0.00002405
Iteration 20/1000 | Loss: 0.00002401
Iteration 21/1000 | Loss: 0.00002401
Iteration 22/1000 | Loss: 0.00002400
Iteration 23/1000 | Loss: 0.00002399
Iteration 24/1000 | Loss: 0.00002397
Iteration 25/1000 | Loss: 0.00002397
Iteration 26/1000 | Loss: 0.00002397
Iteration 27/1000 | Loss: 0.00002393
Iteration 28/1000 | Loss: 0.00002392
Iteration 29/1000 | Loss: 0.00002391
Iteration 30/1000 | Loss: 0.00002391
Iteration 31/1000 | Loss: 0.00002390
Iteration 32/1000 | Loss: 0.00002390
Iteration 33/1000 | Loss: 0.00002390
Iteration 34/1000 | Loss: 0.00002389
Iteration 35/1000 | Loss: 0.00002389
Iteration 36/1000 | Loss: 0.00002388
Iteration 37/1000 | Loss: 0.00002387
Iteration 38/1000 | Loss: 0.00002386
Iteration 39/1000 | Loss: 0.00002386
Iteration 40/1000 | Loss: 0.00002385
Iteration 41/1000 | Loss: 0.00002385
Iteration 42/1000 | Loss: 0.00002384
Iteration 43/1000 | Loss: 0.00002384
Iteration 44/1000 | Loss: 0.00002384
Iteration 45/1000 | Loss: 0.00002383
Iteration 46/1000 | Loss: 0.00002383
Iteration 47/1000 | Loss: 0.00002383
Iteration 48/1000 | Loss: 0.00002383
Iteration 49/1000 | Loss: 0.00002383
Iteration 50/1000 | Loss: 0.00002382
Iteration 51/1000 | Loss: 0.00002382
Iteration 52/1000 | Loss: 0.00002381
Iteration 53/1000 | Loss: 0.00002381
Iteration 54/1000 | Loss: 0.00002380
Iteration 55/1000 | Loss: 0.00002380
Iteration 56/1000 | Loss: 0.00002379
Iteration 57/1000 | Loss: 0.00002379
Iteration 58/1000 | Loss: 0.00002378
Iteration 59/1000 | Loss: 0.00002378
Iteration 60/1000 | Loss: 0.00002378
Iteration 61/1000 | Loss: 0.00002377
Iteration 62/1000 | Loss: 0.00002377
Iteration 63/1000 | Loss: 0.00002377
Iteration 64/1000 | Loss: 0.00002376
Iteration 65/1000 | Loss: 0.00002376
Iteration 66/1000 | Loss: 0.00002376
Iteration 67/1000 | Loss: 0.00002376
Iteration 68/1000 | Loss: 0.00002375
Iteration 69/1000 | Loss: 0.00002375
Iteration 70/1000 | Loss: 0.00002375
Iteration 71/1000 | Loss: 0.00002374
Iteration 72/1000 | Loss: 0.00002374
Iteration 73/1000 | Loss: 0.00002374
Iteration 74/1000 | Loss: 0.00002373
Iteration 75/1000 | Loss: 0.00002373
Iteration 76/1000 | Loss: 0.00002373
Iteration 77/1000 | Loss: 0.00002373
Iteration 78/1000 | Loss: 0.00002373
Iteration 79/1000 | Loss: 0.00002372
Iteration 80/1000 | Loss: 0.00002372
Iteration 81/1000 | Loss: 0.00002372
Iteration 82/1000 | Loss: 0.00002372
Iteration 83/1000 | Loss: 0.00002372
Iteration 84/1000 | Loss: 0.00002372
Iteration 85/1000 | Loss: 0.00002371
Iteration 86/1000 | Loss: 0.00002371
Iteration 87/1000 | Loss: 0.00002371
Iteration 88/1000 | Loss: 0.00002371
Iteration 89/1000 | Loss: 0.00002371
Iteration 90/1000 | Loss: 0.00002370
Iteration 91/1000 | Loss: 0.00002370
Iteration 92/1000 | Loss: 0.00002370
Iteration 93/1000 | Loss: 0.00002369
Iteration 94/1000 | Loss: 0.00002369
Iteration 95/1000 | Loss: 0.00002369
Iteration 96/1000 | Loss: 0.00002369
Iteration 97/1000 | Loss: 0.00002369
Iteration 98/1000 | Loss: 0.00002369
Iteration 99/1000 | Loss: 0.00002369
Iteration 100/1000 | Loss: 0.00002369
Iteration 101/1000 | Loss: 0.00002369
Iteration 102/1000 | Loss: 0.00002369
Iteration 103/1000 | Loss: 0.00002369
Iteration 104/1000 | Loss: 0.00002369
Iteration 105/1000 | Loss: 0.00002369
Iteration 106/1000 | Loss: 0.00002369
Iteration 107/1000 | Loss: 0.00002368
Iteration 108/1000 | Loss: 0.00002368
Iteration 109/1000 | Loss: 0.00002368
Iteration 110/1000 | Loss: 0.00002368
Iteration 111/1000 | Loss: 0.00002368
Iteration 112/1000 | Loss: 0.00002368
Iteration 113/1000 | Loss: 0.00002368
Iteration 114/1000 | Loss: 0.00002368
Iteration 115/1000 | Loss: 0.00002368
Iteration 116/1000 | Loss: 0.00002368
Iteration 117/1000 | Loss: 0.00002368
Iteration 118/1000 | Loss: 0.00002368
Iteration 119/1000 | Loss: 0.00002368
Iteration 120/1000 | Loss: 0.00002368
Iteration 121/1000 | Loss: 0.00002368
Iteration 122/1000 | Loss: 0.00002368
Iteration 123/1000 | Loss: 0.00002368
Iteration 124/1000 | Loss: 0.00002367
Iteration 125/1000 | Loss: 0.00002367
Iteration 126/1000 | Loss: 0.00002367
Iteration 127/1000 | Loss: 0.00002367
Iteration 128/1000 | Loss: 0.00002367
Iteration 129/1000 | Loss: 0.00002367
Iteration 130/1000 | Loss: 0.00002367
Iteration 131/1000 | Loss: 0.00002367
Iteration 132/1000 | Loss: 0.00002367
Iteration 133/1000 | Loss: 0.00002367
Iteration 134/1000 | Loss: 0.00002367
Iteration 135/1000 | Loss: 0.00002367
Iteration 136/1000 | Loss: 0.00002367
Iteration 137/1000 | Loss: 0.00002367
Iteration 138/1000 | Loss: 0.00002367
Iteration 139/1000 | Loss: 0.00002367
Iteration 140/1000 | Loss: 0.00002366
Iteration 141/1000 | Loss: 0.00002366
Iteration 142/1000 | Loss: 0.00002366
Iteration 143/1000 | Loss: 0.00002366
Iteration 144/1000 | Loss: 0.00002366
Iteration 145/1000 | Loss: 0.00002366
Iteration 146/1000 | Loss: 0.00002366
Iteration 147/1000 | Loss: 0.00002365
Iteration 148/1000 | Loss: 0.00002365
Iteration 149/1000 | Loss: 0.00002365
Iteration 150/1000 | Loss: 0.00002365
Iteration 151/1000 | Loss: 0.00002365
Iteration 152/1000 | Loss: 0.00002365
Iteration 153/1000 | Loss: 0.00002365
Iteration 154/1000 | Loss: 0.00002365
Iteration 155/1000 | Loss: 0.00002365
Iteration 156/1000 | Loss: 0.00002365
Iteration 157/1000 | Loss: 0.00002365
Iteration 158/1000 | Loss: 0.00002365
Iteration 159/1000 | Loss: 0.00002365
Iteration 160/1000 | Loss: 0.00002364
Iteration 161/1000 | Loss: 0.00002364
Iteration 162/1000 | Loss: 0.00002364
Iteration 163/1000 | Loss: 0.00002364
Iteration 164/1000 | Loss: 0.00002364
Iteration 165/1000 | Loss: 0.00002364
Iteration 166/1000 | Loss: 0.00002364
Iteration 167/1000 | Loss: 0.00002364
Iteration 168/1000 | Loss: 0.00002364
Iteration 169/1000 | Loss: 0.00002364
Iteration 170/1000 | Loss: 0.00002364
Iteration 171/1000 | Loss: 0.00002363
Iteration 172/1000 | Loss: 0.00002363
Iteration 173/1000 | Loss: 0.00002363
Iteration 174/1000 | Loss: 0.00002363
Iteration 175/1000 | Loss: 0.00002363
Iteration 176/1000 | Loss: 0.00002363
Iteration 177/1000 | Loss: 0.00002363
Iteration 178/1000 | Loss: 0.00002363
Iteration 179/1000 | Loss: 0.00002363
Iteration 180/1000 | Loss: 0.00002363
Iteration 181/1000 | Loss: 0.00002363
Iteration 182/1000 | Loss: 0.00002363
Iteration 183/1000 | Loss: 0.00002363
Iteration 184/1000 | Loss: 0.00002363
Iteration 185/1000 | Loss: 0.00002363
Iteration 186/1000 | Loss: 0.00002363
Iteration 187/1000 | Loss: 0.00002363
Iteration 188/1000 | Loss: 0.00002363
Iteration 189/1000 | Loss: 0.00002363
Iteration 190/1000 | Loss: 0.00002363
Iteration 191/1000 | Loss: 0.00002362
Iteration 192/1000 | Loss: 0.00002362
Iteration 193/1000 | Loss: 0.00002362
Iteration 194/1000 | Loss: 0.00002362
Iteration 195/1000 | Loss: 0.00002362
Iteration 196/1000 | Loss: 0.00002362
Iteration 197/1000 | Loss: 0.00002362
Iteration 198/1000 | Loss: 0.00002362
Iteration 199/1000 | Loss: 0.00002362
Iteration 200/1000 | Loss: 0.00002362
Iteration 201/1000 | Loss: 0.00002362
Iteration 202/1000 | Loss: 0.00002362
Iteration 203/1000 | Loss: 0.00002362
Iteration 204/1000 | Loss: 0.00002362
Iteration 205/1000 | Loss: 0.00002362
Iteration 206/1000 | Loss: 0.00002362
Iteration 207/1000 | Loss: 0.00002362
Iteration 208/1000 | Loss: 0.00002362
Iteration 209/1000 | Loss: 0.00002362
Iteration 210/1000 | Loss: 0.00002362
Iteration 211/1000 | Loss: 0.00002362
Iteration 212/1000 | Loss: 0.00002362
Iteration 213/1000 | Loss: 0.00002362
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [2.361501901759766e-05, 2.361501901759766e-05, 2.361501901759766e-05, 2.361501901759766e-05, 2.361501901759766e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.361501901759766e-05

Optimization complete. Final v2v error: 3.96988582611084 mm

Highest mean error: 5.7780914306640625 mm for frame 128

Lowest mean error: 3.1931357383728027 mm for frame 44

Saving results

Total time: 46.34948229789734
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00962922
Iteration 2/25 | Loss: 0.00136731
Iteration 3/25 | Loss: 0.00085794
Iteration 4/25 | Loss: 0.00081175
Iteration 5/25 | Loss: 0.00079536
Iteration 6/25 | Loss: 0.00079221
Iteration 7/25 | Loss: 0.00079156
Iteration 8/25 | Loss: 0.00079154
Iteration 9/25 | Loss: 0.00079154
Iteration 10/25 | Loss: 0.00079154
Iteration 11/25 | Loss: 0.00079154
Iteration 12/25 | Loss: 0.00079154
Iteration 13/25 | Loss: 0.00079154
Iteration 14/25 | Loss: 0.00079154
Iteration 15/25 | Loss: 0.00079154
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0007915371097624302, 0.0007915371097624302, 0.0007915371097624302, 0.0007915371097624302, 0.0007915371097624302]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007915371097624302

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.91132778
Iteration 2/25 | Loss: 0.00023136
Iteration 3/25 | Loss: 0.00023134
Iteration 4/25 | Loss: 0.00023134
Iteration 5/25 | Loss: 0.00023134
Iteration 6/25 | Loss: 0.00023134
Iteration 7/25 | Loss: 0.00023134
Iteration 8/25 | Loss: 0.00023134
Iteration 9/25 | Loss: 0.00023134
Iteration 10/25 | Loss: 0.00023134
Iteration 11/25 | Loss: 0.00023134
Iteration 12/25 | Loss: 0.00023134
Iteration 13/25 | Loss: 0.00023134
Iteration 14/25 | Loss: 0.00023134
Iteration 15/25 | Loss: 0.00023134
Iteration 16/25 | Loss: 0.00023134
Iteration 17/25 | Loss: 0.00023134
Iteration 18/25 | Loss: 0.00023134
Iteration 19/25 | Loss: 0.00023134
Iteration 20/25 | Loss: 0.00023134
Iteration 21/25 | Loss: 0.00023134
Iteration 22/25 | Loss: 0.00023134
Iteration 23/25 | Loss: 0.00023134
Iteration 24/25 | Loss: 0.00023134
Iteration 25/25 | Loss: 0.00023134

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00023134
Iteration 2/1000 | Loss: 0.00005286
Iteration 3/1000 | Loss: 0.00003950
Iteration 4/1000 | Loss: 0.00003525
Iteration 5/1000 | Loss: 0.00003321
Iteration 6/1000 | Loss: 0.00003199
Iteration 7/1000 | Loss: 0.00003132
Iteration 8/1000 | Loss: 0.00003070
Iteration 9/1000 | Loss: 0.00003028
Iteration 10/1000 | Loss: 0.00003001
Iteration 11/1000 | Loss: 0.00002983
Iteration 12/1000 | Loss: 0.00002966
Iteration 13/1000 | Loss: 0.00002963
Iteration 14/1000 | Loss: 0.00002957
Iteration 15/1000 | Loss: 0.00002954
Iteration 16/1000 | Loss: 0.00002946
Iteration 17/1000 | Loss: 0.00002936
Iteration 18/1000 | Loss: 0.00002933
Iteration 19/1000 | Loss: 0.00002933
Iteration 20/1000 | Loss: 0.00002931
Iteration 21/1000 | Loss: 0.00002927
Iteration 22/1000 | Loss: 0.00002926
Iteration 23/1000 | Loss: 0.00002923
Iteration 24/1000 | Loss: 0.00002923
Iteration 25/1000 | Loss: 0.00002922
Iteration 26/1000 | Loss: 0.00002922
Iteration 27/1000 | Loss: 0.00002921
Iteration 28/1000 | Loss: 0.00002919
Iteration 29/1000 | Loss: 0.00002919
Iteration 30/1000 | Loss: 0.00002918
Iteration 31/1000 | Loss: 0.00002918
Iteration 32/1000 | Loss: 0.00002917
Iteration 33/1000 | Loss: 0.00002916
Iteration 34/1000 | Loss: 0.00002915
Iteration 35/1000 | Loss: 0.00002914
Iteration 36/1000 | Loss: 0.00002914
Iteration 37/1000 | Loss: 0.00002914
Iteration 38/1000 | Loss: 0.00002914
Iteration 39/1000 | Loss: 0.00002914
Iteration 40/1000 | Loss: 0.00002913
Iteration 41/1000 | Loss: 0.00002913
Iteration 42/1000 | Loss: 0.00002913
Iteration 43/1000 | Loss: 0.00002913
Iteration 44/1000 | Loss: 0.00002913
Iteration 45/1000 | Loss: 0.00002913
Iteration 46/1000 | Loss: 0.00002913
Iteration 47/1000 | Loss: 0.00002913
Iteration 48/1000 | Loss: 0.00002913
Iteration 49/1000 | Loss: 0.00002913
Iteration 50/1000 | Loss: 0.00002913
Iteration 51/1000 | Loss: 0.00002912
Iteration 52/1000 | Loss: 0.00002912
Iteration 53/1000 | Loss: 0.00002912
Iteration 54/1000 | Loss: 0.00002912
Iteration 55/1000 | Loss: 0.00002912
Iteration 56/1000 | Loss: 0.00002912
Iteration 57/1000 | Loss: 0.00002912
Iteration 58/1000 | Loss: 0.00002912
Iteration 59/1000 | Loss: 0.00002912
Iteration 60/1000 | Loss: 0.00002911
Iteration 61/1000 | Loss: 0.00002911
Iteration 62/1000 | Loss: 0.00002910
Iteration 63/1000 | Loss: 0.00002910
Iteration 64/1000 | Loss: 0.00002910
Iteration 65/1000 | Loss: 0.00002909
Iteration 66/1000 | Loss: 0.00002909
Iteration 67/1000 | Loss: 0.00002909
Iteration 68/1000 | Loss: 0.00002908
Iteration 69/1000 | Loss: 0.00002908
Iteration 70/1000 | Loss: 0.00002908
Iteration 71/1000 | Loss: 0.00002908
Iteration 72/1000 | Loss: 0.00002908
Iteration 73/1000 | Loss: 0.00002907
Iteration 74/1000 | Loss: 0.00002907
Iteration 75/1000 | Loss: 0.00002906
Iteration 76/1000 | Loss: 0.00002906
Iteration 77/1000 | Loss: 0.00002905
Iteration 78/1000 | Loss: 0.00002905
Iteration 79/1000 | Loss: 0.00002905
Iteration 80/1000 | Loss: 0.00002904
Iteration 81/1000 | Loss: 0.00002904
Iteration 82/1000 | Loss: 0.00002904
Iteration 83/1000 | Loss: 0.00002904
Iteration 84/1000 | Loss: 0.00002904
Iteration 85/1000 | Loss: 0.00002904
Iteration 86/1000 | Loss: 0.00002903
Iteration 87/1000 | Loss: 0.00002903
Iteration 88/1000 | Loss: 0.00002903
Iteration 89/1000 | Loss: 0.00002903
Iteration 90/1000 | Loss: 0.00002902
Iteration 91/1000 | Loss: 0.00002902
Iteration 92/1000 | Loss: 0.00002902
Iteration 93/1000 | Loss: 0.00002901
Iteration 94/1000 | Loss: 0.00002901
Iteration 95/1000 | Loss: 0.00002901
Iteration 96/1000 | Loss: 0.00002901
Iteration 97/1000 | Loss: 0.00002901
Iteration 98/1000 | Loss: 0.00002901
Iteration 99/1000 | Loss: 0.00002901
Iteration 100/1000 | Loss: 0.00002901
Iteration 101/1000 | Loss: 0.00002901
Iteration 102/1000 | Loss: 0.00002901
Iteration 103/1000 | Loss: 0.00002900
Iteration 104/1000 | Loss: 0.00002900
Iteration 105/1000 | Loss: 0.00002900
Iteration 106/1000 | Loss: 0.00002900
Iteration 107/1000 | Loss: 0.00002900
Iteration 108/1000 | Loss: 0.00002900
Iteration 109/1000 | Loss: 0.00002900
Iteration 110/1000 | Loss: 0.00002900
Iteration 111/1000 | Loss: 0.00002900
Iteration 112/1000 | Loss: 0.00002900
Iteration 113/1000 | Loss: 0.00002900
Iteration 114/1000 | Loss: 0.00002900
Iteration 115/1000 | Loss: 0.00002900
Iteration 116/1000 | Loss: 0.00002900
Iteration 117/1000 | Loss: 0.00002899
Iteration 118/1000 | Loss: 0.00002899
Iteration 119/1000 | Loss: 0.00002899
Iteration 120/1000 | Loss: 0.00002899
Iteration 121/1000 | Loss: 0.00002898
Iteration 122/1000 | Loss: 0.00002898
Iteration 123/1000 | Loss: 0.00002898
Iteration 124/1000 | Loss: 0.00002898
Iteration 125/1000 | Loss: 0.00002898
Iteration 126/1000 | Loss: 0.00002898
Iteration 127/1000 | Loss: 0.00002898
Iteration 128/1000 | Loss: 0.00002898
Iteration 129/1000 | Loss: 0.00002897
Iteration 130/1000 | Loss: 0.00002897
Iteration 131/1000 | Loss: 0.00002897
Iteration 132/1000 | Loss: 0.00002897
Iteration 133/1000 | Loss: 0.00002897
Iteration 134/1000 | Loss: 0.00002896
Iteration 135/1000 | Loss: 0.00002896
Iteration 136/1000 | Loss: 0.00002896
Iteration 137/1000 | Loss: 0.00002896
Iteration 138/1000 | Loss: 0.00002896
Iteration 139/1000 | Loss: 0.00002896
Iteration 140/1000 | Loss: 0.00002896
Iteration 141/1000 | Loss: 0.00002895
Iteration 142/1000 | Loss: 0.00002895
Iteration 143/1000 | Loss: 0.00002895
Iteration 144/1000 | Loss: 0.00002895
Iteration 145/1000 | Loss: 0.00002894
Iteration 146/1000 | Loss: 0.00002894
Iteration 147/1000 | Loss: 0.00002894
Iteration 148/1000 | Loss: 0.00002894
Iteration 149/1000 | Loss: 0.00002894
Iteration 150/1000 | Loss: 0.00002894
Iteration 151/1000 | Loss: 0.00002894
Iteration 152/1000 | Loss: 0.00002894
Iteration 153/1000 | Loss: 0.00002894
Iteration 154/1000 | Loss: 0.00002894
Iteration 155/1000 | Loss: 0.00002893
Iteration 156/1000 | Loss: 0.00002893
Iteration 157/1000 | Loss: 0.00002893
Iteration 158/1000 | Loss: 0.00002893
Iteration 159/1000 | Loss: 0.00002893
Iteration 160/1000 | Loss: 0.00002892
Iteration 161/1000 | Loss: 0.00002892
Iteration 162/1000 | Loss: 0.00002892
Iteration 163/1000 | Loss: 0.00002892
Iteration 164/1000 | Loss: 0.00002892
Iteration 165/1000 | Loss: 0.00002892
Iteration 166/1000 | Loss: 0.00002892
Iteration 167/1000 | Loss: 0.00002892
Iteration 168/1000 | Loss: 0.00002892
Iteration 169/1000 | Loss: 0.00002892
Iteration 170/1000 | Loss: 0.00002892
Iteration 171/1000 | Loss: 0.00002892
Iteration 172/1000 | Loss: 0.00002891
Iteration 173/1000 | Loss: 0.00002891
Iteration 174/1000 | Loss: 0.00002891
Iteration 175/1000 | Loss: 0.00002891
Iteration 176/1000 | Loss: 0.00002891
Iteration 177/1000 | Loss: 0.00002891
Iteration 178/1000 | Loss: 0.00002891
Iteration 179/1000 | Loss: 0.00002891
Iteration 180/1000 | Loss: 0.00002891
Iteration 181/1000 | Loss: 0.00002891
Iteration 182/1000 | Loss: 0.00002891
Iteration 183/1000 | Loss: 0.00002890
Iteration 184/1000 | Loss: 0.00002890
Iteration 185/1000 | Loss: 0.00002890
Iteration 186/1000 | Loss: 0.00002890
Iteration 187/1000 | Loss: 0.00002890
Iteration 188/1000 | Loss: 0.00002890
Iteration 189/1000 | Loss: 0.00002890
Iteration 190/1000 | Loss: 0.00002890
Iteration 191/1000 | Loss: 0.00002890
Iteration 192/1000 | Loss: 0.00002890
Iteration 193/1000 | Loss: 0.00002890
Iteration 194/1000 | Loss: 0.00002889
Iteration 195/1000 | Loss: 0.00002889
Iteration 196/1000 | Loss: 0.00002889
Iteration 197/1000 | Loss: 0.00002889
Iteration 198/1000 | Loss: 0.00002889
Iteration 199/1000 | Loss: 0.00002889
Iteration 200/1000 | Loss: 0.00002889
Iteration 201/1000 | Loss: 0.00002889
Iteration 202/1000 | Loss: 0.00002889
Iteration 203/1000 | Loss: 0.00002889
Iteration 204/1000 | Loss: 0.00002889
Iteration 205/1000 | Loss: 0.00002889
Iteration 206/1000 | Loss: 0.00002889
Iteration 207/1000 | Loss: 0.00002889
Iteration 208/1000 | Loss: 0.00002889
Iteration 209/1000 | Loss: 0.00002889
Iteration 210/1000 | Loss: 0.00002889
Iteration 211/1000 | Loss: 0.00002889
Iteration 212/1000 | Loss: 0.00002889
Iteration 213/1000 | Loss: 0.00002889
Iteration 214/1000 | Loss: 0.00002889
Iteration 215/1000 | Loss: 0.00002889
Iteration 216/1000 | Loss: 0.00002889
Iteration 217/1000 | Loss: 0.00002889
Iteration 218/1000 | Loss: 0.00002889
Iteration 219/1000 | Loss: 0.00002889
Iteration 220/1000 | Loss: 0.00002889
Iteration 221/1000 | Loss: 0.00002889
Iteration 222/1000 | Loss: 0.00002889
Iteration 223/1000 | Loss: 0.00002889
Iteration 224/1000 | Loss: 0.00002889
Iteration 225/1000 | Loss: 0.00002889
Iteration 226/1000 | Loss: 0.00002889
Iteration 227/1000 | Loss: 0.00002889
Iteration 228/1000 | Loss: 0.00002889
Iteration 229/1000 | Loss: 0.00002889
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 229. Stopping optimization.
Last 5 losses: [2.8891308829770423e-05, 2.8891308829770423e-05, 2.8891308829770423e-05, 2.8891308829770423e-05, 2.8891308829770423e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8891308829770423e-05

Optimization complete. Final v2v error: 4.4685893058776855 mm

Highest mean error: 5.364099025726318 mm for frame 129

Lowest mean error: 3.6697609424591064 mm for frame 38

Saving results

Total time: 46.5662567615509
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00749908
Iteration 2/25 | Loss: 0.00079884
Iteration 3/25 | Loss: 0.00068262
Iteration 4/25 | Loss: 0.00065738
Iteration 5/25 | Loss: 0.00064699
Iteration 6/25 | Loss: 0.00064521
Iteration 7/25 | Loss: 0.00064481
Iteration 8/25 | Loss: 0.00064481
Iteration 9/25 | Loss: 0.00064481
Iteration 10/25 | Loss: 0.00064481
Iteration 11/25 | Loss: 0.00064481
Iteration 12/25 | Loss: 0.00064481
Iteration 13/25 | Loss: 0.00064481
Iteration 14/25 | Loss: 0.00064481
Iteration 15/25 | Loss: 0.00064481
Iteration 16/25 | Loss: 0.00064481
Iteration 17/25 | Loss: 0.00064481
Iteration 18/25 | Loss: 0.00064481
Iteration 19/25 | Loss: 0.00064481
Iteration 20/25 | Loss: 0.00064481
Iteration 21/25 | Loss: 0.00064481
Iteration 22/25 | Loss: 0.00064481
Iteration 23/25 | Loss: 0.00064481
Iteration 24/25 | Loss: 0.00064481
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0006448117201216519, 0.0006448117201216519, 0.0006448117201216519, 0.0006448117201216519, 0.0006448117201216519]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006448117201216519

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48416328
Iteration 2/25 | Loss: 0.00026612
Iteration 3/25 | Loss: 0.00026611
Iteration 4/25 | Loss: 0.00026611
Iteration 5/25 | Loss: 0.00026611
Iteration 6/25 | Loss: 0.00026611
Iteration 7/25 | Loss: 0.00026611
Iteration 8/25 | Loss: 0.00026611
Iteration 9/25 | Loss: 0.00026611
Iteration 10/25 | Loss: 0.00026611
Iteration 11/25 | Loss: 0.00026611
Iteration 12/25 | Loss: 0.00026611
Iteration 13/25 | Loss: 0.00026611
Iteration 14/25 | Loss: 0.00026611
Iteration 15/25 | Loss: 0.00026611
Iteration 16/25 | Loss: 0.00026611
Iteration 17/25 | Loss: 0.00026611
Iteration 18/25 | Loss: 0.00026611
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0002661124162841588, 0.0002661124162841588, 0.0002661124162841588, 0.0002661124162841588, 0.0002661124162841588]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002661124162841588

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026611
Iteration 2/1000 | Loss: 0.00003109
Iteration 3/1000 | Loss: 0.00002382
Iteration 4/1000 | Loss: 0.00002019
Iteration 5/1000 | Loss: 0.00001938
Iteration 6/1000 | Loss: 0.00001845
Iteration 7/1000 | Loss: 0.00001789
Iteration 8/1000 | Loss: 0.00001746
Iteration 9/1000 | Loss: 0.00001731
Iteration 10/1000 | Loss: 0.00001711
Iteration 11/1000 | Loss: 0.00001707
Iteration 12/1000 | Loss: 0.00001706
Iteration 13/1000 | Loss: 0.00001701
Iteration 14/1000 | Loss: 0.00001699
Iteration 15/1000 | Loss: 0.00001697
Iteration 16/1000 | Loss: 0.00001696
Iteration 17/1000 | Loss: 0.00001692
Iteration 18/1000 | Loss: 0.00001692
Iteration 19/1000 | Loss: 0.00001692
Iteration 20/1000 | Loss: 0.00001691
Iteration 21/1000 | Loss: 0.00001686
Iteration 22/1000 | Loss: 0.00001685
Iteration 23/1000 | Loss: 0.00001682
Iteration 24/1000 | Loss: 0.00001678
Iteration 25/1000 | Loss: 0.00001678
Iteration 26/1000 | Loss: 0.00001678
Iteration 27/1000 | Loss: 0.00001677
Iteration 28/1000 | Loss: 0.00001676
Iteration 29/1000 | Loss: 0.00001676
Iteration 30/1000 | Loss: 0.00001676
Iteration 31/1000 | Loss: 0.00001675
Iteration 32/1000 | Loss: 0.00001675
Iteration 33/1000 | Loss: 0.00001675
Iteration 34/1000 | Loss: 0.00001675
Iteration 35/1000 | Loss: 0.00001675
Iteration 36/1000 | Loss: 0.00001675
Iteration 37/1000 | Loss: 0.00001674
Iteration 38/1000 | Loss: 0.00001674
Iteration 39/1000 | Loss: 0.00001674
Iteration 40/1000 | Loss: 0.00001673
Iteration 41/1000 | Loss: 0.00001673
Iteration 42/1000 | Loss: 0.00001673
Iteration 43/1000 | Loss: 0.00001672
Iteration 44/1000 | Loss: 0.00001672
Iteration 45/1000 | Loss: 0.00001671
Iteration 46/1000 | Loss: 0.00001671
Iteration 47/1000 | Loss: 0.00001671
Iteration 48/1000 | Loss: 0.00001671
Iteration 49/1000 | Loss: 0.00001671
Iteration 50/1000 | Loss: 0.00001670
Iteration 51/1000 | Loss: 0.00001670
Iteration 52/1000 | Loss: 0.00001670
Iteration 53/1000 | Loss: 0.00001669
Iteration 54/1000 | Loss: 0.00001668
Iteration 55/1000 | Loss: 0.00001668
Iteration 56/1000 | Loss: 0.00001668
Iteration 57/1000 | Loss: 0.00001668
Iteration 58/1000 | Loss: 0.00001668
Iteration 59/1000 | Loss: 0.00001668
Iteration 60/1000 | Loss: 0.00001667
Iteration 61/1000 | Loss: 0.00001667
Iteration 62/1000 | Loss: 0.00001667
Iteration 63/1000 | Loss: 0.00001663
Iteration 64/1000 | Loss: 0.00001663
Iteration 65/1000 | Loss: 0.00001662
Iteration 66/1000 | Loss: 0.00001662
Iteration 67/1000 | Loss: 0.00001661
Iteration 68/1000 | Loss: 0.00001661
Iteration 69/1000 | Loss: 0.00001660
Iteration 70/1000 | Loss: 0.00001660
Iteration 71/1000 | Loss: 0.00001659
Iteration 72/1000 | Loss: 0.00001659
Iteration 73/1000 | Loss: 0.00001658
Iteration 74/1000 | Loss: 0.00001658
Iteration 75/1000 | Loss: 0.00001658
Iteration 76/1000 | Loss: 0.00001658
Iteration 77/1000 | Loss: 0.00001658
Iteration 78/1000 | Loss: 0.00001658
Iteration 79/1000 | Loss: 0.00001658
Iteration 80/1000 | Loss: 0.00001658
Iteration 81/1000 | Loss: 0.00001658
Iteration 82/1000 | Loss: 0.00001658
Iteration 83/1000 | Loss: 0.00001658
Iteration 84/1000 | Loss: 0.00001657
Iteration 85/1000 | Loss: 0.00001657
Iteration 86/1000 | Loss: 0.00001657
Iteration 87/1000 | Loss: 0.00001656
Iteration 88/1000 | Loss: 0.00001656
Iteration 89/1000 | Loss: 0.00001656
Iteration 90/1000 | Loss: 0.00001656
Iteration 91/1000 | Loss: 0.00001656
Iteration 92/1000 | Loss: 0.00001656
Iteration 93/1000 | Loss: 0.00001655
Iteration 94/1000 | Loss: 0.00001655
Iteration 95/1000 | Loss: 0.00001655
Iteration 96/1000 | Loss: 0.00001655
Iteration 97/1000 | Loss: 0.00001655
Iteration 98/1000 | Loss: 0.00001655
Iteration 99/1000 | Loss: 0.00001654
Iteration 100/1000 | Loss: 0.00001654
Iteration 101/1000 | Loss: 0.00001654
Iteration 102/1000 | Loss: 0.00001653
Iteration 103/1000 | Loss: 0.00001653
Iteration 104/1000 | Loss: 0.00001653
Iteration 105/1000 | Loss: 0.00001653
Iteration 106/1000 | Loss: 0.00001653
Iteration 107/1000 | Loss: 0.00001652
Iteration 108/1000 | Loss: 0.00001652
Iteration 109/1000 | Loss: 0.00001652
Iteration 110/1000 | Loss: 0.00001652
Iteration 111/1000 | Loss: 0.00001652
Iteration 112/1000 | Loss: 0.00001652
Iteration 113/1000 | Loss: 0.00001652
Iteration 114/1000 | Loss: 0.00001652
Iteration 115/1000 | Loss: 0.00001651
Iteration 116/1000 | Loss: 0.00001651
Iteration 117/1000 | Loss: 0.00001651
Iteration 118/1000 | Loss: 0.00001651
Iteration 119/1000 | Loss: 0.00001651
Iteration 120/1000 | Loss: 0.00001651
Iteration 121/1000 | Loss: 0.00001651
Iteration 122/1000 | Loss: 0.00001651
Iteration 123/1000 | Loss: 0.00001651
Iteration 124/1000 | Loss: 0.00001651
Iteration 125/1000 | Loss: 0.00001651
Iteration 126/1000 | Loss: 0.00001650
Iteration 127/1000 | Loss: 0.00001650
Iteration 128/1000 | Loss: 0.00001650
Iteration 129/1000 | Loss: 0.00001650
Iteration 130/1000 | Loss: 0.00001649
Iteration 131/1000 | Loss: 0.00001649
Iteration 132/1000 | Loss: 0.00001649
Iteration 133/1000 | Loss: 0.00001649
Iteration 134/1000 | Loss: 0.00001649
Iteration 135/1000 | Loss: 0.00001649
Iteration 136/1000 | Loss: 0.00001649
Iteration 137/1000 | Loss: 0.00001649
Iteration 138/1000 | Loss: 0.00001649
Iteration 139/1000 | Loss: 0.00001649
Iteration 140/1000 | Loss: 0.00001649
Iteration 141/1000 | Loss: 0.00001649
Iteration 142/1000 | Loss: 0.00001649
Iteration 143/1000 | Loss: 0.00001649
Iteration 144/1000 | Loss: 0.00001649
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.648680517973844e-05, 1.648680517973844e-05, 1.648680517973844e-05, 1.648680517973844e-05, 1.648680517973844e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.648680517973844e-05

Optimization complete. Final v2v error: 3.428967237472534 mm

Highest mean error: 3.635453462600708 mm for frame 115

Lowest mean error: 3.3346006870269775 mm for frame 142

Saving results

Total time: 38.391178369522095
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_015/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_015/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00761519
Iteration 2/25 | Loss: 0.00114640
Iteration 3/25 | Loss: 0.00078356
Iteration 4/25 | Loss: 0.00073984
Iteration 5/25 | Loss: 0.00073085
Iteration 6/25 | Loss: 0.00072800
Iteration 7/25 | Loss: 0.00072758
Iteration 8/25 | Loss: 0.00072758
Iteration 9/25 | Loss: 0.00072758
Iteration 10/25 | Loss: 0.00072758
Iteration 11/25 | Loss: 0.00072758
Iteration 12/25 | Loss: 0.00072758
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007275777752511203, 0.0007275777752511203, 0.0007275777752511203, 0.0007275777752511203, 0.0007275777752511203]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007275777752511203

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.65910101
Iteration 2/25 | Loss: 0.00036453
Iteration 3/25 | Loss: 0.00036452
Iteration 4/25 | Loss: 0.00036452
Iteration 5/25 | Loss: 0.00036452
Iteration 6/25 | Loss: 0.00036452
Iteration 7/25 | Loss: 0.00036452
Iteration 8/25 | Loss: 0.00036452
Iteration 9/25 | Loss: 0.00036452
Iteration 10/25 | Loss: 0.00036452
Iteration 11/25 | Loss: 0.00036452
Iteration 12/25 | Loss: 0.00036452
Iteration 13/25 | Loss: 0.00036452
Iteration 14/25 | Loss: 0.00036452
Iteration 15/25 | Loss: 0.00036452
Iteration 16/25 | Loss: 0.00036452
Iteration 17/25 | Loss: 0.00036452
Iteration 18/25 | Loss: 0.00036452
Iteration 19/25 | Loss: 0.00036452
Iteration 20/25 | Loss: 0.00036452
Iteration 21/25 | Loss: 0.00036452
Iteration 22/25 | Loss: 0.00036452
Iteration 23/25 | Loss: 0.00036452
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00036451552296057343, 0.00036451552296057343, 0.00036451552296057343, 0.00036451552296057343, 0.00036451552296057343]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00036451552296057343

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036452
Iteration 2/1000 | Loss: 0.00003420
Iteration 3/1000 | Loss: 0.00002385
Iteration 4/1000 | Loss: 0.00002215
Iteration 5/1000 | Loss: 0.00002107
Iteration 6/1000 | Loss: 0.00002037
Iteration 7/1000 | Loss: 0.00001990
Iteration 8/1000 | Loss: 0.00001966
Iteration 9/1000 | Loss: 0.00001937
Iteration 10/1000 | Loss: 0.00001913
Iteration 11/1000 | Loss: 0.00001910
Iteration 12/1000 | Loss: 0.00001902
Iteration 13/1000 | Loss: 0.00001893
Iteration 14/1000 | Loss: 0.00001892
Iteration 15/1000 | Loss: 0.00001892
Iteration 16/1000 | Loss: 0.00001891
Iteration 17/1000 | Loss: 0.00001886
Iteration 18/1000 | Loss: 0.00001877
Iteration 19/1000 | Loss: 0.00001877
Iteration 20/1000 | Loss: 0.00001876
Iteration 21/1000 | Loss: 0.00001875
Iteration 22/1000 | Loss: 0.00001871
Iteration 23/1000 | Loss: 0.00001871
Iteration 24/1000 | Loss: 0.00001869
Iteration 25/1000 | Loss: 0.00001869
Iteration 26/1000 | Loss: 0.00001868
Iteration 27/1000 | Loss: 0.00001868
Iteration 28/1000 | Loss: 0.00001868
Iteration 29/1000 | Loss: 0.00001868
Iteration 30/1000 | Loss: 0.00001868
Iteration 31/1000 | Loss: 0.00001868
Iteration 32/1000 | Loss: 0.00001868
Iteration 33/1000 | Loss: 0.00001868
Iteration 34/1000 | Loss: 0.00001867
Iteration 35/1000 | Loss: 0.00001867
Iteration 36/1000 | Loss: 0.00001865
Iteration 37/1000 | Loss: 0.00001865
Iteration 38/1000 | Loss: 0.00001865
Iteration 39/1000 | Loss: 0.00001865
Iteration 40/1000 | Loss: 0.00001864
Iteration 41/1000 | Loss: 0.00001864
Iteration 42/1000 | Loss: 0.00001864
Iteration 43/1000 | Loss: 0.00001864
Iteration 44/1000 | Loss: 0.00001863
Iteration 45/1000 | Loss: 0.00001863
Iteration 46/1000 | Loss: 0.00001863
Iteration 47/1000 | Loss: 0.00001862
Iteration 48/1000 | Loss: 0.00001862
Iteration 49/1000 | Loss: 0.00001862
Iteration 50/1000 | Loss: 0.00001862
Iteration 51/1000 | Loss: 0.00001861
Iteration 52/1000 | Loss: 0.00001861
Iteration 53/1000 | Loss: 0.00001861
Iteration 54/1000 | Loss: 0.00001861
Iteration 55/1000 | Loss: 0.00001860
Iteration 56/1000 | Loss: 0.00001860
Iteration 57/1000 | Loss: 0.00001860
Iteration 58/1000 | Loss: 0.00001860
Iteration 59/1000 | Loss: 0.00001860
Iteration 60/1000 | Loss: 0.00001860
Iteration 61/1000 | Loss: 0.00001860
Iteration 62/1000 | Loss: 0.00001859
Iteration 63/1000 | Loss: 0.00001859
Iteration 64/1000 | Loss: 0.00001859
Iteration 65/1000 | Loss: 0.00001859
Iteration 66/1000 | Loss: 0.00001859
Iteration 67/1000 | Loss: 0.00001859
Iteration 68/1000 | Loss: 0.00001859
Iteration 69/1000 | Loss: 0.00001859
Iteration 70/1000 | Loss: 0.00001858
Iteration 71/1000 | Loss: 0.00001858
Iteration 72/1000 | Loss: 0.00001858
Iteration 73/1000 | Loss: 0.00001857
Iteration 74/1000 | Loss: 0.00001857
Iteration 75/1000 | Loss: 0.00001857
Iteration 76/1000 | Loss: 0.00001857
Iteration 77/1000 | Loss: 0.00001857
Iteration 78/1000 | Loss: 0.00001857
Iteration 79/1000 | Loss: 0.00001857
Iteration 80/1000 | Loss: 0.00001856
Iteration 81/1000 | Loss: 0.00001856
Iteration 82/1000 | Loss: 0.00001856
Iteration 83/1000 | Loss: 0.00001856
Iteration 84/1000 | Loss: 0.00001856
Iteration 85/1000 | Loss: 0.00001856
Iteration 86/1000 | Loss: 0.00001856
Iteration 87/1000 | Loss: 0.00001856
Iteration 88/1000 | Loss: 0.00001856
Iteration 89/1000 | Loss: 0.00001856
Iteration 90/1000 | Loss: 0.00001856
Iteration 91/1000 | Loss: 0.00001856
Iteration 92/1000 | Loss: 0.00001856
Iteration 93/1000 | Loss: 0.00001856
Iteration 94/1000 | Loss: 0.00001856
Iteration 95/1000 | Loss: 0.00001856
Iteration 96/1000 | Loss: 0.00001856
Iteration 97/1000 | Loss: 0.00001856
Iteration 98/1000 | Loss: 0.00001856
Iteration 99/1000 | Loss: 0.00001856
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [1.8556862414698116e-05, 1.8556862414698116e-05, 1.8556862414698116e-05, 1.8556862414698116e-05, 1.8556862414698116e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8556862414698116e-05

Optimization complete. Final v2v error: 3.6203668117523193 mm

Highest mean error: 4.553045272827148 mm for frame 187

Lowest mean error: 2.9905946254730225 mm for frame 8

Saving results

Total time: 38.76033854484558
