Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=263, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 14728-14783
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_nl_6355/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6355/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6355/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00906478
Iteration 2/25 | Loss: 0.00129355
Iteration 3/25 | Loss: 0.00114277
Iteration 4/25 | Loss: 0.00113019
Iteration 5/25 | Loss: 0.00112714
Iteration 6/25 | Loss: 0.00112631
Iteration 7/25 | Loss: 0.00112631
Iteration 8/25 | Loss: 0.00112631
Iteration 9/25 | Loss: 0.00112631
Iteration 10/25 | Loss: 0.00112631
Iteration 11/25 | Loss: 0.00112631
Iteration 12/25 | Loss: 0.00112631
Iteration 13/25 | Loss: 0.00112631
Iteration 14/25 | Loss: 0.00112631
Iteration 15/25 | Loss: 0.00112631
Iteration 16/25 | Loss: 0.00112631
Iteration 17/25 | Loss: 0.00112631
Iteration 18/25 | Loss: 0.00112631
Iteration 19/25 | Loss: 0.00112631
Iteration 20/25 | Loss: 0.00112631
Iteration 21/25 | Loss: 0.00112631
Iteration 22/25 | Loss: 0.00112631
Iteration 23/25 | Loss: 0.00112631
Iteration 24/25 | Loss: 0.00112631
Iteration 25/25 | Loss: 0.00112631

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41798699
Iteration 2/25 | Loss: 0.00062897
Iteration 3/25 | Loss: 0.00062897
Iteration 4/25 | Loss: 0.00062897
Iteration 5/25 | Loss: 0.00062897
Iteration 6/25 | Loss: 0.00062897
Iteration 7/25 | Loss: 0.00062897
Iteration 8/25 | Loss: 0.00062897
Iteration 9/25 | Loss: 0.00062897
Iteration 10/25 | Loss: 0.00062897
Iteration 11/25 | Loss: 0.00062897
Iteration 12/25 | Loss: 0.00062897
Iteration 13/25 | Loss: 0.00062897
Iteration 14/25 | Loss: 0.00062897
Iteration 15/25 | Loss: 0.00062897
Iteration 16/25 | Loss: 0.00062897
Iteration 17/25 | Loss: 0.00062897
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006289671873673797, 0.0006289671873673797, 0.0006289671873673797, 0.0006289671873673797, 0.0006289671873673797]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006289671873673797

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062897
Iteration 2/1000 | Loss: 0.00003365
Iteration 3/1000 | Loss: 0.00002552
Iteration 4/1000 | Loss: 0.00002313
Iteration 5/1000 | Loss: 0.00002198
Iteration 6/1000 | Loss: 0.00002126
Iteration 7/1000 | Loss: 0.00002077
Iteration 8/1000 | Loss: 0.00002048
Iteration 9/1000 | Loss: 0.00002046
Iteration 10/1000 | Loss: 0.00002040
Iteration 11/1000 | Loss: 0.00002039
Iteration 12/1000 | Loss: 0.00002038
Iteration 13/1000 | Loss: 0.00002036
Iteration 14/1000 | Loss: 0.00002034
Iteration 15/1000 | Loss: 0.00002034
Iteration 16/1000 | Loss: 0.00002030
Iteration 17/1000 | Loss: 0.00002025
Iteration 18/1000 | Loss: 0.00002024
Iteration 19/1000 | Loss: 0.00002023
Iteration 20/1000 | Loss: 0.00002022
Iteration 21/1000 | Loss: 0.00002017
Iteration 22/1000 | Loss: 0.00002016
Iteration 23/1000 | Loss: 0.00002016
Iteration 24/1000 | Loss: 0.00002010
Iteration 25/1000 | Loss: 0.00002010
Iteration 26/1000 | Loss: 0.00002010
Iteration 27/1000 | Loss: 0.00002009
Iteration 28/1000 | Loss: 0.00002006
Iteration 29/1000 | Loss: 0.00002003
Iteration 30/1000 | Loss: 0.00002001
Iteration 31/1000 | Loss: 0.00001999
Iteration 32/1000 | Loss: 0.00001999
Iteration 33/1000 | Loss: 0.00001998
Iteration 34/1000 | Loss: 0.00001998
Iteration 35/1000 | Loss: 0.00001998
Iteration 36/1000 | Loss: 0.00001998
Iteration 37/1000 | Loss: 0.00001998
Iteration 38/1000 | Loss: 0.00001997
Iteration 39/1000 | Loss: 0.00001997
Iteration 40/1000 | Loss: 0.00001997
Iteration 41/1000 | Loss: 0.00001997
Iteration 42/1000 | Loss: 0.00001997
Iteration 43/1000 | Loss: 0.00001996
Iteration 44/1000 | Loss: 0.00001996
Iteration 45/1000 | Loss: 0.00001995
Iteration 46/1000 | Loss: 0.00001995
Iteration 47/1000 | Loss: 0.00001995
Iteration 48/1000 | Loss: 0.00001995
Iteration 49/1000 | Loss: 0.00001994
Iteration 50/1000 | Loss: 0.00001994
Iteration 51/1000 | Loss: 0.00001994
Iteration 52/1000 | Loss: 0.00001994
Iteration 53/1000 | Loss: 0.00001994
Iteration 54/1000 | Loss: 0.00001994
Iteration 55/1000 | Loss: 0.00001994
Iteration 56/1000 | Loss: 0.00001993
Iteration 57/1000 | Loss: 0.00001992
Iteration 58/1000 | Loss: 0.00001991
Iteration 59/1000 | Loss: 0.00001991
Iteration 60/1000 | Loss: 0.00001991
Iteration 61/1000 | Loss: 0.00001991
Iteration 62/1000 | Loss: 0.00001991
Iteration 63/1000 | Loss: 0.00001990
Iteration 64/1000 | Loss: 0.00001990
Iteration 65/1000 | Loss: 0.00001990
Iteration 66/1000 | Loss: 0.00001990
Iteration 67/1000 | Loss: 0.00001990
Iteration 68/1000 | Loss: 0.00001989
Iteration 69/1000 | Loss: 0.00001989
Iteration 70/1000 | Loss: 0.00001989
Iteration 71/1000 | Loss: 0.00001989
Iteration 72/1000 | Loss: 0.00001989
Iteration 73/1000 | Loss: 0.00001989
Iteration 74/1000 | Loss: 0.00001989
Iteration 75/1000 | Loss: 0.00001989
Iteration 76/1000 | Loss: 0.00001988
Iteration 77/1000 | Loss: 0.00001988
Iteration 78/1000 | Loss: 0.00001988
Iteration 79/1000 | Loss: 0.00001988
Iteration 80/1000 | Loss: 0.00001988
Iteration 81/1000 | Loss: 0.00001988
Iteration 82/1000 | Loss: 0.00001988
Iteration 83/1000 | Loss: 0.00001988
Iteration 84/1000 | Loss: 0.00001988
Iteration 85/1000 | Loss: 0.00001988
Iteration 86/1000 | Loss: 0.00001988
Iteration 87/1000 | Loss: 0.00001988
Iteration 88/1000 | Loss: 0.00001988
Iteration 89/1000 | Loss: 0.00001987
Iteration 90/1000 | Loss: 0.00001987
Iteration 91/1000 | Loss: 0.00001987
Iteration 92/1000 | Loss: 0.00001987
Iteration 93/1000 | Loss: 0.00001987
Iteration 94/1000 | Loss: 0.00001987
Iteration 95/1000 | Loss: 0.00001987
Iteration 96/1000 | Loss: 0.00001986
Iteration 97/1000 | Loss: 0.00001986
Iteration 98/1000 | Loss: 0.00001986
Iteration 99/1000 | Loss: 0.00001986
Iteration 100/1000 | Loss: 0.00001986
Iteration 101/1000 | Loss: 0.00001986
Iteration 102/1000 | Loss: 0.00001986
Iteration 103/1000 | Loss: 0.00001986
Iteration 104/1000 | Loss: 0.00001985
Iteration 105/1000 | Loss: 0.00001985
Iteration 106/1000 | Loss: 0.00001985
Iteration 107/1000 | Loss: 0.00001985
Iteration 108/1000 | Loss: 0.00001985
Iteration 109/1000 | Loss: 0.00001985
Iteration 110/1000 | Loss: 0.00001984
Iteration 111/1000 | Loss: 0.00001984
Iteration 112/1000 | Loss: 0.00001984
Iteration 113/1000 | Loss: 0.00001984
Iteration 114/1000 | Loss: 0.00001984
Iteration 115/1000 | Loss: 0.00001984
Iteration 116/1000 | Loss: 0.00001984
Iteration 117/1000 | Loss: 0.00001984
Iteration 118/1000 | Loss: 0.00001984
Iteration 119/1000 | Loss: 0.00001984
Iteration 120/1000 | Loss: 0.00001984
Iteration 121/1000 | Loss: 0.00001984
Iteration 122/1000 | Loss: 0.00001984
Iteration 123/1000 | Loss: 0.00001984
Iteration 124/1000 | Loss: 0.00001984
Iteration 125/1000 | Loss: 0.00001983
Iteration 126/1000 | Loss: 0.00001983
Iteration 127/1000 | Loss: 0.00001983
Iteration 128/1000 | Loss: 0.00001983
Iteration 129/1000 | Loss: 0.00001983
Iteration 130/1000 | Loss: 0.00001983
Iteration 131/1000 | Loss: 0.00001983
Iteration 132/1000 | Loss: 0.00001983
Iteration 133/1000 | Loss: 0.00001983
Iteration 134/1000 | Loss: 0.00001983
Iteration 135/1000 | Loss: 0.00001983
Iteration 136/1000 | Loss: 0.00001983
Iteration 137/1000 | Loss: 0.00001983
Iteration 138/1000 | Loss: 0.00001983
Iteration 139/1000 | Loss: 0.00001983
Iteration 140/1000 | Loss: 0.00001983
Iteration 141/1000 | Loss: 0.00001982
Iteration 142/1000 | Loss: 0.00001982
Iteration 143/1000 | Loss: 0.00001982
Iteration 144/1000 | Loss: 0.00001982
Iteration 145/1000 | Loss: 0.00001982
Iteration 146/1000 | Loss: 0.00001982
Iteration 147/1000 | Loss: 0.00001982
Iteration 148/1000 | Loss: 0.00001982
Iteration 149/1000 | Loss: 0.00001982
Iteration 150/1000 | Loss: 0.00001982
Iteration 151/1000 | Loss: 0.00001982
Iteration 152/1000 | Loss: 0.00001982
Iteration 153/1000 | Loss: 0.00001982
Iteration 154/1000 | Loss: 0.00001982
Iteration 155/1000 | Loss: 0.00001982
Iteration 156/1000 | Loss: 0.00001981
Iteration 157/1000 | Loss: 0.00001981
Iteration 158/1000 | Loss: 0.00001981
Iteration 159/1000 | Loss: 0.00001981
Iteration 160/1000 | Loss: 0.00001981
Iteration 161/1000 | Loss: 0.00001981
Iteration 162/1000 | Loss: 0.00001981
Iteration 163/1000 | Loss: 0.00001981
Iteration 164/1000 | Loss: 0.00001981
Iteration 165/1000 | Loss: 0.00001981
Iteration 166/1000 | Loss: 0.00001981
Iteration 167/1000 | Loss: 0.00001981
Iteration 168/1000 | Loss: 0.00001981
Iteration 169/1000 | Loss: 0.00001981
Iteration 170/1000 | Loss: 0.00001980
Iteration 171/1000 | Loss: 0.00001980
Iteration 172/1000 | Loss: 0.00001980
Iteration 173/1000 | Loss: 0.00001980
Iteration 174/1000 | Loss: 0.00001980
Iteration 175/1000 | Loss: 0.00001980
Iteration 176/1000 | Loss: 0.00001980
Iteration 177/1000 | Loss: 0.00001980
Iteration 178/1000 | Loss: 0.00001980
Iteration 179/1000 | Loss: 0.00001980
Iteration 180/1000 | Loss: 0.00001980
Iteration 181/1000 | Loss: 0.00001980
Iteration 182/1000 | Loss: 0.00001979
Iteration 183/1000 | Loss: 0.00001979
Iteration 184/1000 | Loss: 0.00001979
Iteration 185/1000 | Loss: 0.00001979
Iteration 186/1000 | Loss: 0.00001979
Iteration 187/1000 | Loss: 0.00001979
Iteration 188/1000 | Loss: 0.00001979
Iteration 189/1000 | Loss: 0.00001979
Iteration 190/1000 | Loss: 0.00001979
Iteration 191/1000 | Loss: 0.00001979
Iteration 192/1000 | Loss: 0.00001979
Iteration 193/1000 | Loss: 0.00001979
Iteration 194/1000 | Loss: 0.00001979
Iteration 195/1000 | Loss: 0.00001979
Iteration 196/1000 | Loss: 0.00001979
Iteration 197/1000 | Loss: 0.00001979
Iteration 198/1000 | Loss: 0.00001979
Iteration 199/1000 | Loss: 0.00001979
Iteration 200/1000 | Loss: 0.00001979
Iteration 201/1000 | Loss: 0.00001979
Iteration 202/1000 | Loss: 0.00001979
Iteration 203/1000 | Loss: 0.00001979
Iteration 204/1000 | Loss: 0.00001979
Iteration 205/1000 | Loss: 0.00001979
Iteration 206/1000 | Loss: 0.00001979
Iteration 207/1000 | Loss: 0.00001979
Iteration 208/1000 | Loss: 0.00001979
Iteration 209/1000 | Loss: 0.00001979
Iteration 210/1000 | Loss: 0.00001979
Iteration 211/1000 | Loss: 0.00001979
Iteration 212/1000 | Loss: 0.00001979
Iteration 213/1000 | Loss: 0.00001979
Iteration 214/1000 | Loss: 0.00001979
Iteration 215/1000 | Loss: 0.00001979
Iteration 216/1000 | Loss: 0.00001979
Iteration 217/1000 | Loss: 0.00001979
Iteration 218/1000 | Loss: 0.00001979
Iteration 219/1000 | Loss: 0.00001979
Iteration 220/1000 | Loss: 0.00001979
Iteration 221/1000 | Loss: 0.00001979
Iteration 222/1000 | Loss: 0.00001979
Iteration 223/1000 | Loss: 0.00001979
Iteration 224/1000 | Loss: 0.00001979
Iteration 225/1000 | Loss: 0.00001979
Iteration 226/1000 | Loss: 0.00001979
Iteration 227/1000 | Loss: 0.00001979
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [1.9788278223131783e-05, 1.9788278223131783e-05, 1.9788278223131783e-05, 1.9788278223131783e-05, 1.9788278223131783e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9788278223131783e-05

Optimization complete. Final v2v error: 3.8318943977355957 mm

Highest mean error: 4.06467866897583 mm for frame 91

Lowest mean error: 3.468642234802246 mm for frame 0

Saving results

Total time: 38.97566819190979
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_nl_6355/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6355/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6355/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00503850
Iteration 2/25 | Loss: 0.00139436
Iteration 3/25 | Loss: 0.00121084
Iteration 4/25 | Loss: 0.00118866
Iteration 5/25 | Loss: 0.00118174
Iteration 6/25 | Loss: 0.00118020
Iteration 7/25 | Loss: 0.00118020
Iteration 8/25 | Loss: 0.00118020
Iteration 9/25 | Loss: 0.00118020
Iteration 10/25 | Loss: 0.00118020
Iteration 11/25 | Loss: 0.00118020
Iteration 12/25 | Loss: 0.00118020
Iteration 13/25 | Loss: 0.00118020
Iteration 14/25 | Loss: 0.00118020
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0011802042135968804, 0.0011802042135968804, 0.0011802042135968804, 0.0011802042135968804, 0.0011802042135968804]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011802042135968804

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43119955
Iteration 2/25 | Loss: 0.00066380
Iteration 3/25 | Loss: 0.00066379
Iteration 4/25 | Loss: 0.00066379
Iteration 5/25 | Loss: 0.00066379
Iteration 6/25 | Loss: 0.00066379
Iteration 7/25 | Loss: 0.00066379
Iteration 8/25 | Loss: 0.00066379
Iteration 9/25 | Loss: 0.00066379
Iteration 10/25 | Loss: 0.00066379
Iteration 11/25 | Loss: 0.00066379
Iteration 12/25 | Loss: 0.00066379
Iteration 13/25 | Loss: 0.00066379
Iteration 14/25 | Loss: 0.00066379
Iteration 15/25 | Loss: 0.00066379
Iteration 16/25 | Loss: 0.00066379
Iteration 17/25 | Loss: 0.00066379
Iteration 18/25 | Loss: 0.00066379
Iteration 19/25 | Loss: 0.00066379
Iteration 20/25 | Loss: 0.00066379
Iteration 21/25 | Loss: 0.00066379
Iteration 22/25 | Loss: 0.00066379
Iteration 23/25 | Loss: 0.00066379
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0006637873593717813, 0.0006637873593717813, 0.0006637873593717813, 0.0006637873593717813, 0.0006637873593717813]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006637873593717813

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066379
Iteration 2/1000 | Loss: 0.00004397
Iteration 3/1000 | Loss: 0.00003310
Iteration 4/1000 | Loss: 0.00003149
Iteration 5/1000 | Loss: 0.00003018
Iteration 6/1000 | Loss: 0.00002951
Iteration 7/1000 | Loss: 0.00002903
Iteration 8/1000 | Loss: 0.00002864
Iteration 9/1000 | Loss: 0.00002846
Iteration 10/1000 | Loss: 0.00002835
Iteration 11/1000 | Loss: 0.00002820
Iteration 12/1000 | Loss: 0.00002817
Iteration 13/1000 | Loss: 0.00002814
Iteration 14/1000 | Loss: 0.00002814
Iteration 15/1000 | Loss: 0.00002813
Iteration 16/1000 | Loss: 0.00002813
Iteration 17/1000 | Loss: 0.00002810
Iteration 18/1000 | Loss: 0.00002809
Iteration 19/1000 | Loss: 0.00002805
Iteration 20/1000 | Loss: 0.00002804
Iteration 21/1000 | Loss: 0.00002804
Iteration 22/1000 | Loss: 0.00002803
Iteration 23/1000 | Loss: 0.00002801
Iteration 24/1000 | Loss: 0.00002801
Iteration 25/1000 | Loss: 0.00002799
Iteration 26/1000 | Loss: 0.00002799
Iteration 27/1000 | Loss: 0.00002798
Iteration 28/1000 | Loss: 0.00002798
Iteration 29/1000 | Loss: 0.00002798
Iteration 30/1000 | Loss: 0.00002798
Iteration 31/1000 | Loss: 0.00002798
Iteration 32/1000 | Loss: 0.00002798
Iteration 33/1000 | Loss: 0.00002798
Iteration 34/1000 | Loss: 0.00002798
Iteration 35/1000 | Loss: 0.00002798
Iteration 36/1000 | Loss: 0.00002797
Iteration 37/1000 | Loss: 0.00002797
Iteration 38/1000 | Loss: 0.00002797
Iteration 39/1000 | Loss: 0.00002797
Iteration 40/1000 | Loss: 0.00002797
Iteration 41/1000 | Loss: 0.00002797
Iteration 42/1000 | Loss: 0.00002797
Iteration 43/1000 | Loss: 0.00002797
Iteration 44/1000 | Loss: 0.00002797
Iteration 45/1000 | Loss: 0.00002797
Iteration 46/1000 | Loss: 0.00002797
Iteration 47/1000 | Loss: 0.00002797
Iteration 48/1000 | Loss: 0.00002796
Iteration 49/1000 | Loss: 0.00002796
Iteration 50/1000 | Loss: 0.00002796
Iteration 51/1000 | Loss: 0.00002796
Iteration 52/1000 | Loss: 0.00002796
Iteration 53/1000 | Loss: 0.00002796
Iteration 54/1000 | Loss: 0.00002795
Iteration 55/1000 | Loss: 0.00002795
Iteration 56/1000 | Loss: 0.00002795
Iteration 57/1000 | Loss: 0.00002794
Iteration 58/1000 | Loss: 0.00002794
Iteration 59/1000 | Loss: 0.00002794
Iteration 60/1000 | Loss: 0.00002793
Iteration 61/1000 | Loss: 0.00002793
Iteration 62/1000 | Loss: 0.00002793
Iteration 63/1000 | Loss: 0.00002793
Iteration 64/1000 | Loss: 0.00002793
Iteration 65/1000 | Loss: 0.00002793
Iteration 66/1000 | Loss: 0.00002793
Iteration 67/1000 | Loss: 0.00002792
Iteration 68/1000 | Loss: 0.00002792
Iteration 69/1000 | Loss: 0.00002792
Iteration 70/1000 | Loss: 0.00002792
Iteration 71/1000 | Loss: 0.00002792
Iteration 72/1000 | Loss: 0.00002792
Iteration 73/1000 | Loss: 0.00002791
Iteration 74/1000 | Loss: 0.00002791
Iteration 75/1000 | Loss: 0.00002791
Iteration 76/1000 | Loss: 0.00002791
Iteration 77/1000 | Loss: 0.00002791
Iteration 78/1000 | Loss: 0.00002790
Iteration 79/1000 | Loss: 0.00002790
Iteration 80/1000 | Loss: 0.00002790
Iteration 81/1000 | Loss: 0.00002790
Iteration 82/1000 | Loss: 0.00002790
Iteration 83/1000 | Loss: 0.00002790
Iteration 84/1000 | Loss: 0.00002790
Iteration 85/1000 | Loss: 0.00002790
Iteration 86/1000 | Loss: 0.00002790
Iteration 87/1000 | Loss: 0.00002790
Iteration 88/1000 | Loss: 0.00002790
Iteration 89/1000 | Loss: 0.00002790
Iteration 90/1000 | Loss: 0.00002790
Iteration 91/1000 | Loss: 0.00002790
Iteration 92/1000 | Loss: 0.00002790
Iteration 93/1000 | Loss: 0.00002790
Iteration 94/1000 | Loss: 0.00002789
Iteration 95/1000 | Loss: 0.00002789
Iteration 96/1000 | Loss: 0.00002789
Iteration 97/1000 | Loss: 0.00002789
Iteration 98/1000 | Loss: 0.00002789
Iteration 99/1000 | Loss: 0.00002788
Iteration 100/1000 | Loss: 0.00002788
Iteration 101/1000 | Loss: 0.00002788
Iteration 102/1000 | Loss: 0.00002788
Iteration 103/1000 | Loss: 0.00002788
Iteration 104/1000 | Loss: 0.00002788
Iteration 105/1000 | Loss: 0.00002788
Iteration 106/1000 | Loss: 0.00002788
Iteration 107/1000 | Loss: 0.00002788
Iteration 108/1000 | Loss: 0.00002788
Iteration 109/1000 | Loss: 0.00002788
Iteration 110/1000 | Loss: 0.00002788
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [2.7878730179509148e-05, 2.7878730179509148e-05, 2.7878730179509148e-05, 2.7878730179509148e-05, 2.7878730179509148e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7878730179509148e-05

Optimization complete. Final v2v error: 4.452587604522705 mm

Highest mean error: 5.153357028961182 mm for frame 161

Lowest mean error: 3.4170875549316406 mm for frame 6

Saving results

Total time: 36.2011981010437
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_nl_6355/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6355/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6355/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00649684
Iteration 2/25 | Loss: 0.00128190
Iteration 3/25 | Loss: 0.00118682
Iteration 4/25 | Loss: 0.00116822
Iteration 5/25 | Loss: 0.00116091
Iteration 6/25 | Loss: 0.00115925
Iteration 7/25 | Loss: 0.00115886
Iteration 8/25 | Loss: 0.00115886
Iteration 9/25 | Loss: 0.00115886
Iteration 10/25 | Loss: 0.00115886
Iteration 11/25 | Loss: 0.00115886
Iteration 12/25 | Loss: 0.00115886
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011588637717068195, 0.0011588637717068195, 0.0011588637717068195, 0.0011588637717068195, 0.0011588637717068195]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011588637717068195

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.76209378
Iteration 2/25 | Loss: 0.00060245
Iteration 3/25 | Loss: 0.00060245
Iteration 4/25 | Loss: 0.00060245
Iteration 5/25 | Loss: 0.00060244
Iteration 6/25 | Loss: 0.00060244
Iteration 7/25 | Loss: 0.00060244
Iteration 8/25 | Loss: 0.00060244
Iteration 9/25 | Loss: 0.00060244
Iteration 10/25 | Loss: 0.00060244
Iteration 11/25 | Loss: 0.00060244
Iteration 12/25 | Loss: 0.00060244
Iteration 13/25 | Loss: 0.00060244
Iteration 14/25 | Loss: 0.00060244
Iteration 15/25 | Loss: 0.00060244
Iteration 16/25 | Loss: 0.00060244
Iteration 17/25 | Loss: 0.00060244
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000602443004027009, 0.000602443004027009, 0.000602443004027009, 0.000602443004027009, 0.000602443004027009]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000602443004027009

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060244
Iteration 2/1000 | Loss: 0.00003632
Iteration 3/1000 | Loss: 0.00002911
Iteration 4/1000 | Loss: 0.00002685
Iteration 5/1000 | Loss: 0.00002557
Iteration 6/1000 | Loss: 0.00002486
Iteration 7/1000 | Loss: 0.00002425
Iteration 8/1000 | Loss: 0.00002399
Iteration 9/1000 | Loss: 0.00002391
Iteration 10/1000 | Loss: 0.00002378
Iteration 11/1000 | Loss: 0.00002368
Iteration 12/1000 | Loss: 0.00002363
Iteration 13/1000 | Loss: 0.00002362
Iteration 14/1000 | Loss: 0.00002362
Iteration 15/1000 | Loss: 0.00002361
Iteration 16/1000 | Loss: 0.00002358
Iteration 17/1000 | Loss: 0.00002358
Iteration 18/1000 | Loss: 0.00002357
Iteration 19/1000 | Loss: 0.00002356
Iteration 20/1000 | Loss: 0.00002356
Iteration 21/1000 | Loss: 0.00002356
Iteration 22/1000 | Loss: 0.00002355
Iteration 23/1000 | Loss: 0.00002354
Iteration 24/1000 | Loss: 0.00002351
Iteration 25/1000 | Loss: 0.00002351
Iteration 26/1000 | Loss: 0.00002351
Iteration 27/1000 | Loss: 0.00002350
Iteration 28/1000 | Loss: 0.00002350
Iteration 29/1000 | Loss: 0.00002350
Iteration 30/1000 | Loss: 0.00002350
Iteration 31/1000 | Loss: 0.00002349
Iteration 32/1000 | Loss: 0.00002349
Iteration 33/1000 | Loss: 0.00002349
Iteration 34/1000 | Loss: 0.00002348
Iteration 35/1000 | Loss: 0.00002348
Iteration 36/1000 | Loss: 0.00002348
Iteration 37/1000 | Loss: 0.00002348
Iteration 38/1000 | Loss: 0.00002347
Iteration 39/1000 | Loss: 0.00002347
Iteration 40/1000 | Loss: 0.00002347
Iteration 41/1000 | Loss: 0.00002347
Iteration 42/1000 | Loss: 0.00002347
Iteration 43/1000 | Loss: 0.00002347
Iteration 44/1000 | Loss: 0.00002346
Iteration 45/1000 | Loss: 0.00002346
Iteration 46/1000 | Loss: 0.00002346
Iteration 47/1000 | Loss: 0.00002346
Iteration 48/1000 | Loss: 0.00002346
Iteration 49/1000 | Loss: 0.00002345
Iteration 50/1000 | Loss: 0.00002345
Iteration 51/1000 | Loss: 0.00002345
Iteration 52/1000 | Loss: 0.00002345
Iteration 53/1000 | Loss: 0.00002344
Iteration 54/1000 | Loss: 0.00002344
Iteration 55/1000 | Loss: 0.00002344
Iteration 56/1000 | Loss: 0.00002343
Iteration 57/1000 | Loss: 0.00002343
Iteration 58/1000 | Loss: 0.00002343
Iteration 59/1000 | Loss: 0.00002343
Iteration 60/1000 | Loss: 0.00002343
Iteration 61/1000 | Loss: 0.00002343
Iteration 62/1000 | Loss: 0.00002343
Iteration 63/1000 | Loss: 0.00002343
Iteration 64/1000 | Loss: 0.00002343
Iteration 65/1000 | Loss: 0.00002342
Iteration 66/1000 | Loss: 0.00002342
Iteration 67/1000 | Loss: 0.00002342
Iteration 68/1000 | Loss: 0.00002342
Iteration 69/1000 | Loss: 0.00002342
Iteration 70/1000 | Loss: 0.00002342
Iteration 71/1000 | Loss: 0.00002342
Iteration 72/1000 | Loss: 0.00002342
Iteration 73/1000 | Loss: 0.00002342
Iteration 74/1000 | Loss: 0.00002342
Iteration 75/1000 | Loss: 0.00002342
Iteration 76/1000 | Loss: 0.00002341
Iteration 77/1000 | Loss: 0.00002341
Iteration 78/1000 | Loss: 0.00002341
Iteration 79/1000 | Loss: 0.00002341
Iteration 80/1000 | Loss: 0.00002341
Iteration 81/1000 | Loss: 0.00002341
Iteration 82/1000 | Loss: 0.00002341
Iteration 83/1000 | Loss: 0.00002340
Iteration 84/1000 | Loss: 0.00002340
Iteration 85/1000 | Loss: 0.00002340
Iteration 86/1000 | Loss: 0.00002340
Iteration 87/1000 | Loss: 0.00002340
Iteration 88/1000 | Loss: 0.00002340
Iteration 89/1000 | Loss: 0.00002340
Iteration 90/1000 | Loss: 0.00002339
Iteration 91/1000 | Loss: 0.00002339
Iteration 92/1000 | Loss: 0.00002339
Iteration 93/1000 | Loss: 0.00002339
Iteration 94/1000 | Loss: 0.00002339
Iteration 95/1000 | Loss: 0.00002339
Iteration 96/1000 | Loss: 0.00002339
Iteration 97/1000 | Loss: 0.00002339
Iteration 98/1000 | Loss: 0.00002339
Iteration 99/1000 | Loss: 0.00002339
Iteration 100/1000 | Loss: 0.00002338
Iteration 101/1000 | Loss: 0.00002338
Iteration 102/1000 | Loss: 0.00002338
Iteration 103/1000 | Loss: 0.00002338
Iteration 104/1000 | Loss: 0.00002338
Iteration 105/1000 | Loss: 0.00002338
Iteration 106/1000 | Loss: 0.00002338
Iteration 107/1000 | Loss: 0.00002338
Iteration 108/1000 | Loss: 0.00002337
Iteration 109/1000 | Loss: 0.00002337
Iteration 110/1000 | Loss: 0.00002337
Iteration 111/1000 | Loss: 0.00002337
Iteration 112/1000 | Loss: 0.00002337
Iteration 113/1000 | Loss: 0.00002337
Iteration 114/1000 | Loss: 0.00002337
Iteration 115/1000 | Loss: 0.00002337
Iteration 116/1000 | Loss: 0.00002336
Iteration 117/1000 | Loss: 0.00002336
Iteration 118/1000 | Loss: 0.00002336
Iteration 119/1000 | Loss: 0.00002336
Iteration 120/1000 | Loss: 0.00002336
Iteration 121/1000 | Loss: 0.00002336
Iteration 122/1000 | Loss: 0.00002336
Iteration 123/1000 | Loss: 0.00002335
Iteration 124/1000 | Loss: 0.00002335
Iteration 125/1000 | Loss: 0.00002335
Iteration 126/1000 | Loss: 0.00002335
Iteration 127/1000 | Loss: 0.00002335
Iteration 128/1000 | Loss: 0.00002335
Iteration 129/1000 | Loss: 0.00002335
Iteration 130/1000 | Loss: 0.00002335
Iteration 131/1000 | Loss: 0.00002335
Iteration 132/1000 | Loss: 0.00002334
Iteration 133/1000 | Loss: 0.00002334
Iteration 134/1000 | Loss: 0.00002334
Iteration 135/1000 | Loss: 0.00002334
Iteration 136/1000 | Loss: 0.00002334
Iteration 137/1000 | Loss: 0.00002334
Iteration 138/1000 | Loss: 0.00002333
Iteration 139/1000 | Loss: 0.00002333
Iteration 140/1000 | Loss: 0.00002333
Iteration 141/1000 | Loss: 0.00002333
Iteration 142/1000 | Loss: 0.00002333
Iteration 143/1000 | Loss: 0.00002333
Iteration 144/1000 | Loss: 0.00002333
Iteration 145/1000 | Loss: 0.00002333
Iteration 146/1000 | Loss: 0.00002332
Iteration 147/1000 | Loss: 0.00002332
Iteration 148/1000 | Loss: 0.00002332
Iteration 149/1000 | Loss: 0.00002332
Iteration 150/1000 | Loss: 0.00002332
Iteration 151/1000 | Loss: 0.00002332
Iteration 152/1000 | Loss: 0.00002332
Iteration 153/1000 | Loss: 0.00002332
Iteration 154/1000 | Loss: 0.00002332
Iteration 155/1000 | Loss: 0.00002332
Iteration 156/1000 | Loss: 0.00002332
Iteration 157/1000 | Loss: 0.00002332
Iteration 158/1000 | Loss: 0.00002332
Iteration 159/1000 | Loss: 0.00002332
Iteration 160/1000 | Loss: 0.00002332
Iteration 161/1000 | Loss: 0.00002332
Iteration 162/1000 | Loss: 0.00002331
Iteration 163/1000 | Loss: 0.00002331
Iteration 164/1000 | Loss: 0.00002331
Iteration 165/1000 | Loss: 0.00002331
Iteration 166/1000 | Loss: 0.00002331
Iteration 167/1000 | Loss: 0.00002331
Iteration 168/1000 | Loss: 0.00002331
Iteration 169/1000 | Loss: 0.00002331
Iteration 170/1000 | Loss: 0.00002331
Iteration 171/1000 | Loss: 0.00002331
Iteration 172/1000 | Loss: 0.00002331
Iteration 173/1000 | Loss: 0.00002330
Iteration 174/1000 | Loss: 0.00002330
Iteration 175/1000 | Loss: 0.00002330
Iteration 176/1000 | Loss: 0.00002330
Iteration 177/1000 | Loss: 0.00002330
Iteration 178/1000 | Loss: 0.00002330
Iteration 179/1000 | Loss: 0.00002330
Iteration 180/1000 | Loss: 0.00002330
Iteration 181/1000 | Loss: 0.00002330
Iteration 182/1000 | Loss: 0.00002330
Iteration 183/1000 | Loss: 0.00002330
Iteration 184/1000 | Loss: 0.00002330
Iteration 185/1000 | Loss: 0.00002330
Iteration 186/1000 | Loss: 0.00002330
Iteration 187/1000 | Loss: 0.00002330
Iteration 188/1000 | Loss: 0.00002330
Iteration 189/1000 | Loss: 0.00002330
Iteration 190/1000 | Loss: 0.00002330
Iteration 191/1000 | Loss: 0.00002330
Iteration 192/1000 | Loss: 0.00002330
Iteration 193/1000 | Loss: 0.00002330
Iteration 194/1000 | Loss: 0.00002330
Iteration 195/1000 | Loss: 0.00002330
Iteration 196/1000 | Loss: 0.00002330
Iteration 197/1000 | Loss: 0.00002330
Iteration 198/1000 | Loss: 0.00002330
Iteration 199/1000 | Loss: 0.00002330
Iteration 200/1000 | Loss: 0.00002330
Iteration 201/1000 | Loss: 0.00002330
Iteration 202/1000 | Loss: 0.00002330
Iteration 203/1000 | Loss: 0.00002330
Iteration 204/1000 | Loss: 0.00002330
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [2.3297547159017995e-05, 2.3297547159017995e-05, 2.3297547159017995e-05, 2.3297547159017995e-05, 2.3297547159017995e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3297547159017995e-05

Optimization complete. Final v2v error: 4.141812324523926 mm

Highest mean error: 4.454368591308594 mm for frame 104

Lowest mean error: 3.8139238357543945 mm for frame 124

Saving results

Total time: 38.131486892700195
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_nl_6355/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6355/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6355/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00921506
Iteration 2/25 | Loss: 0.00164352
Iteration 3/25 | Loss: 0.00131753
Iteration 4/25 | Loss: 0.00128067
Iteration 5/25 | Loss: 0.00127215
Iteration 6/25 | Loss: 0.00127042
Iteration 7/25 | Loss: 0.00127022
Iteration 8/25 | Loss: 0.00127022
Iteration 9/25 | Loss: 0.00127022
Iteration 10/25 | Loss: 0.00127022
Iteration 11/25 | Loss: 0.00127022
Iteration 12/25 | Loss: 0.00127022
Iteration 13/25 | Loss: 0.00127022
Iteration 14/25 | Loss: 0.00127022
Iteration 15/25 | Loss: 0.00127022
Iteration 16/25 | Loss: 0.00127022
Iteration 17/25 | Loss: 0.00127022
Iteration 18/25 | Loss: 0.00127022
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012702171225100756, 0.0012702171225100756, 0.0012702171225100756, 0.0012702171225100756, 0.0012702171225100756]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012702171225100756

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.00750053
Iteration 2/25 | Loss: 0.00078150
Iteration 3/25 | Loss: 0.00078149
Iteration 4/25 | Loss: 0.00078149
Iteration 5/25 | Loss: 0.00078149
Iteration 6/25 | Loss: 0.00078149
Iteration 7/25 | Loss: 0.00078149
Iteration 8/25 | Loss: 0.00078149
Iteration 9/25 | Loss: 0.00078149
Iteration 10/25 | Loss: 0.00078149
Iteration 11/25 | Loss: 0.00078149
Iteration 12/25 | Loss: 0.00078149
Iteration 13/25 | Loss: 0.00078149
Iteration 14/25 | Loss: 0.00078149
Iteration 15/25 | Loss: 0.00078149
Iteration 16/25 | Loss: 0.00078149
Iteration 17/25 | Loss: 0.00078149
Iteration 18/25 | Loss: 0.00078149
Iteration 19/25 | Loss: 0.00078149
Iteration 20/25 | Loss: 0.00078149
Iteration 21/25 | Loss: 0.00078149
Iteration 22/25 | Loss: 0.00078149
Iteration 23/25 | Loss: 0.00078149
Iteration 24/25 | Loss: 0.00078149
Iteration 25/25 | Loss: 0.00078149

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078149
Iteration 2/1000 | Loss: 0.00004723
Iteration 3/1000 | Loss: 0.00003672
Iteration 4/1000 | Loss: 0.00003424
Iteration 5/1000 | Loss: 0.00003202
Iteration 6/1000 | Loss: 0.00003097
Iteration 7/1000 | Loss: 0.00003029
Iteration 8/1000 | Loss: 0.00003000
Iteration 9/1000 | Loss: 0.00002963
Iteration 10/1000 | Loss: 0.00002939
Iteration 11/1000 | Loss: 0.00002928
Iteration 12/1000 | Loss: 0.00002928
Iteration 13/1000 | Loss: 0.00002927
Iteration 14/1000 | Loss: 0.00002927
Iteration 15/1000 | Loss: 0.00002927
Iteration 16/1000 | Loss: 0.00002927
Iteration 17/1000 | Loss: 0.00002927
Iteration 18/1000 | Loss: 0.00002926
Iteration 19/1000 | Loss: 0.00002926
Iteration 20/1000 | Loss: 0.00002924
Iteration 21/1000 | Loss: 0.00002924
Iteration 22/1000 | Loss: 0.00002924
Iteration 23/1000 | Loss: 0.00002924
Iteration 24/1000 | Loss: 0.00002924
Iteration 25/1000 | Loss: 0.00002924
Iteration 26/1000 | Loss: 0.00002924
Iteration 27/1000 | Loss: 0.00002924
Iteration 28/1000 | Loss: 0.00002924
Iteration 29/1000 | Loss: 0.00002924
Iteration 30/1000 | Loss: 0.00002924
Iteration 31/1000 | Loss: 0.00002924
Iteration 32/1000 | Loss: 0.00002924
Iteration 33/1000 | Loss: 0.00002924
Iteration 34/1000 | Loss: 0.00002924
Iteration 35/1000 | Loss: 0.00002924
Iteration 36/1000 | Loss: 0.00002924
Iteration 37/1000 | Loss: 0.00002924
Iteration 38/1000 | Loss: 0.00002924
Iteration 39/1000 | Loss: 0.00002924
Iteration 40/1000 | Loss: 0.00002924
Iteration 41/1000 | Loss: 0.00002924
Iteration 42/1000 | Loss: 0.00002924
Iteration 43/1000 | Loss: 0.00002924
Iteration 44/1000 | Loss: 0.00002924
Iteration 45/1000 | Loss: 0.00002924
Iteration 46/1000 | Loss: 0.00002924
Iteration 47/1000 | Loss: 0.00002924
Iteration 48/1000 | Loss: 0.00002924
Iteration 49/1000 | Loss: 0.00002924
Iteration 50/1000 | Loss: 0.00002924
Iteration 51/1000 | Loss: 0.00002924
Iteration 52/1000 | Loss: 0.00002924
Iteration 53/1000 | Loss: 0.00002924
Iteration 54/1000 | Loss: 0.00002924
Iteration 55/1000 | Loss: 0.00002924
Iteration 56/1000 | Loss: 0.00002924
Iteration 57/1000 | Loss: 0.00002924
Iteration 58/1000 | Loss: 0.00002924
Iteration 59/1000 | Loss: 0.00002924
Iteration 60/1000 | Loss: 0.00002924
Iteration 61/1000 | Loss: 0.00002924
Iteration 62/1000 | Loss: 0.00002924
Iteration 63/1000 | Loss: 0.00002924
Iteration 64/1000 | Loss: 0.00002924
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 64. Stopping optimization.
Last 5 losses: [2.9238466595415957e-05, 2.9238466595415957e-05, 2.9238466595415957e-05, 2.9238466595415957e-05, 2.9238466595415957e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9238466595415957e-05

Optimization complete. Final v2v error: 4.611761093139648 mm

Highest mean error: 5.158863067626953 mm for frame 1

Lowest mean error: 4.338991165161133 mm for frame 48

Saving results

Total time: 27.18512272834778
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_nl_6355/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6355/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6355/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01071509
Iteration 2/25 | Loss: 0.00258746
Iteration 3/25 | Loss: 0.00185576
Iteration 4/25 | Loss: 0.00168828
Iteration 5/25 | Loss: 0.00165305
Iteration 6/25 | Loss: 0.00162999
Iteration 7/25 | Loss: 0.00160151
Iteration 8/25 | Loss: 0.00159029
Iteration 9/25 | Loss: 0.00157373
Iteration 10/25 | Loss: 0.00158124
Iteration 11/25 | Loss: 0.00161814
Iteration 12/25 | Loss: 0.00159132
Iteration 13/25 | Loss: 0.00158408
Iteration 14/25 | Loss: 0.00162396
Iteration 15/25 | Loss: 0.00162202
Iteration 16/25 | Loss: 0.00159260
Iteration 17/25 | Loss: 0.00158077
Iteration 18/25 | Loss: 0.00157110
Iteration 19/25 | Loss: 0.00156688
Iteration 20/25 | Loss: 0.00157236
Iteration 21/25 | Loss: 0.00156551
Iteration 22/25 | Loss: 0.00156176
Iteration 23/25 | Loss: 0.00155874
Iteration 24/25 | Loss: 0.00156391
Iteration 25/25 | Loss: 0.00156360

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46321702
Iteration 2/25 | Loss: 0.00380969
Iteration 3/25 | Loss: 0.00376910
Iteration 4/25 | Loss: 0.00376910
Iteration 5/25 | Loss: 0.00376910
Iteration 6/25 | Loss: 0.00376910
Iteration 7/25 | Loss: 0.00376910
Iteration 8/25 | Loss: 0.00376910
Iteration 9/25 | Loss: 0.00376910
Iteration 10/25 | Loss: 0.00376910
Iteration 11/25 | Loss: 0.00376910
Iteration 12/25 | Loss: 0.00376910
Iteration 13/25 | Loss: 0.00376910
Iteration 14/25 | Loss: 0.00376910
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.003769097151234746, 0.003769097151234746, 0.003769097151234746, 0.003769097151234746, 0.003769097151234746]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003769097151234746

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00376910
Iteration 2/1000 | Loss: 0.00066372
Iteration 3/1000 | Loss: 0.00050138
Iteration 4/1000 | Loss: 0.00075470
Iteration 5/1000 | Loss: 0.00112754
Iteration 6/1000 | Loss: 0.00120268
Iteration 7/1000 | Loss: 0.00027518
Iteration 8/1000 | Loss: 0.00025882
Iteration 9/1000 | Loss: 0.00118119
Iteration 10/1000 | Loss: 0.00177488
Iteration 11/1000 | Loss: 0.00072443
Iteration 12/1000 | Loss: 0.00144534
Iteration 13/1000 | Loss: 0.00136174
Iteration 14/1000 | Loss: 0.00131059
Iteration 15/1000 | Loss: 0.00032723
Iteration 16/1000 | Loss: 0.00024322
Iteration 17/1000 | Loss: 0.00025189
Iteration 18/1000 | Loss: 0.00021332
Iteration 19/1000 | Loss: 0.00029277
Iteration 20/1000 | Loss: 0.00029557
Iteration 21/1000 | Loss: 0.00030090
Iteration 22/1000 | Loss: 0.00030560
Iteration 23/1000 | Loss: 0.00033274
Iteration 24/1000 | Loss: 0.00030125
Iteration 25/1000 | Loss: 0.00029938
Iteration 26/1000 | Loss: 0.00030540
Iteration 27/1000 | Loss: 0.00029720
Iteration 28/1000 | Loss: 0.00030916
Iteration 29/1000 | Loss: 0.00031261
Iteration 30/1000 | Loss: 0.00033476
Iteration 31/1000 | Loss: 0.00030778
Iteration 32/1000 | Loss: 0.00024851
Iteration 33/1000 | Loss: 0.00024677
Iteration 34/1000 | Loss: 0.00024098
Iteration 35/1000 | Loss: 0.00025480
Iteration 36/1000 | Loss: 0.00027825
Iteration 37/1000 | Loss: 0.00029015
Iteration 38/1000 | Loss: 0.00029231
Iteration 39/1000 | Loss: 0.00028563
Iteration 40/1000 | Loss: 0.00025123
Iteration 41/1000 | Loss: 0.00030559
Iteration 42/1000 | Loss: 0.00024467
Iteration 43/1000 | Loss: 0.00027966
Iteration 44/1000 | Loss: 0.00029076
Iteration 45/1000 | Loss: 0.00027997
Iteration 46/1000 | Loss: 0.00028854
Iteration 47/1000 | Loss: 0.00023570
Iteration 48/1000 | Loss: 0.00024024
Iteration 49/1000 | Loss: 0.00025252
Iteration 50/1000 | Loss: 0.00027494
Iteration 51/1000 | Loss: 0.00024337
Iteration 52/1000 | Loss: 0.00030397
Iteration 53/1000 | Loss: 0.00026137
Iteration 54/1000 | Loss: 0.00029924
Iteration 55/1000 | Loss: 0.00026384
Iteration 56/1000 | Loss: 0.00027369
Iteration 57/1000 | Loss: 0.00030367
Iteration 58/1000 | Loss: 0.00026206
Iteration 59/1000 | Loss: 0.00028027
Iteration 60/1000 | Loss: 0.00030984
Iteration 61/1000 | Loss: 0.00026198
Iteration 62/1000 | Loss: 0.00027925
Iteration 63/1000 | Loss: 0.00025513
Iteration 64/1000 | Loss: 0.00023413
Iteration 65/1000 | Loss: 0.00016928
Iteration 66/1000 | Loss: 0.00022693
Iteration 67/1000 | Loss: 0.00025755
Iteration 68/1000 | Loss: 0.00026280
Iteration 69/1000 | Loss: 0.00027449
Iteration 70/1000 | Loss: 0.00027862
Iteration 71/1000 | Loss: 0.00027175
Iteration 72/1000 | Loss: 0.00024515
Iteration 73/1000 | Loss: 0.00021847
Iteration 74/1000 | Loss: 0.00018176
Iteration 75/1000 | Loss: 0.00018920
Iteration 76/1000 | Loss: 0.00020667
Iteration 77/1000 | Loss: 0.00021685
Iteration 78/1000 | Loss: 0.00020032
Iteration 79/1000 | Loss: 0.00019149
Iteration 80/1000 | Loss: 0.00018228
Iteration 81/1000 | Loss: 0.00017592
Iteration 82/1000 | Loss: 0.00017630
Iteration 83/1000 | Loss: 0.00017748
Iteration 84/1000 | Loss: 0.00019127
Iteration 85/1000 | Loss: 0.00018926
Iteration 86/1000 | Loss: 0.00019088
Iteration 87/1000 | Loss: 0.00019842
Iteration 88/1000 | Loss: 0.00019377
Iteration 89/1000 | Loss: 0.00020461
Iteration 90/1000 | Loss: 0.00020514
Iteration 91/1000 | Loss: 0.00018015
Iteration 92/1000 | Loss: 0.00020297
Iteration 93/1000 | Loss: 0.00020308
Iteration 94/1000 | Loss: 0.00019557
Iteration 95/1000 | Loss: 0.00019876
Iteration 96/1000 | Loss: 0.00019118
Iteration 97/1000 | Loss: 0.00018090
Iteration 98/1000 | Loss: 0.00018857
Iteration 99/1000 | Loss: 0.00019825
Iteration 100/1000 | Loss: 0.00018653
Iteration 101/1000 | Loss: 0.00017053
Iteration 102/1000 | Loss: 0.00020324
Iteration 103/1000 | Loss: 0.00019017
Iteration 104/1000 | Loss: 0.00020548
Iteration 105/1000 | Loss: 0.00019855
Iteration 106/1000 | Loss: 0.00021203
Iteration 107/1000 | Loss: 0.00019154
Iteration 108/1000 | Loss: 0.00020908
Iteration 109/1000 | Loss: 0.00019655
Iteration 110/1000 | Loss: 0.00019590
Iteration 111/1000 | Loss: 0.00019641
Iteration 112/1000 | Loss: 0.00020326
Iteration 113/1000 | Loss: 0.00020464
Iteration 114/1000 | Loss: 0.00020221
Iteration 115/1000 | Loss: 0.00020303
Iteration 116/1000 | Loss: 0.00020001
Iteration 117/1000 | Loss: 0.00018845
Iteration 118/1000 | Loss: 0.00019311
Iteration 119/1000 | Loss: 0.00019043
Iteration 120/1000 | Loss: 0.00019110
Iteration 121/1000 | Loss: 0.00018136
Iteration 122/1000 | Loss: 0.00020141
Iteration 123/1000 | Loss: 0.00019927
Iteration 124/1000 | Loss: 0.00019532
Iteration 125/1000 | Loss: 0.00018666
Iteration 126/1000 | Loss: 0.00019451
Iteration 127/1000 | Loss: 0.00018981
Iteration 128/1000 | Loss: 0.00019491
Iteration 129/1000 | Loss: 0.00019037
Iteration 130/1000 | Loss: 0.00016755
Iteration 131/1000 | Loss: 0.00017940
Iteration 132/1000 | Loss: 0.00018034
Iteration 133/1000 | Loss: 0.00016272
Iteration 134/1000 | Loss: 0.00016095
Iteration 135/1000 | Loss: 0.00018350
Iteration 136/1000 | Loss: 0.00016033
Iteration 137/1000 | Loss: 0.00018309
Iteration 138/1000 | Loss: 0.00015878
Iteration 139/1000 | Loss: 0.00015229
Iteration 140/1000 | Loss: 0.00015026
Iteration 141/1000 | Loss: 0.00014937
Iteration 142/1000 | Loss: 0.00014847
Iteration 143/1000 | Loss: 0.00014777
Iteration 144/1000 | Loss: 0.00014738
Iteration 145/1000 | Loss: 0.00014708
Iteration 146/1000 | Loss: 0.00014684
Iteration 147/1000 | Loss: 0.00014664
Iteration 148/1000 | Loss: 0.00014647
Iteration 149/1000 | Loss: 0.00014643
Iteration 150/1000 | Loss: 0.00014636
Iteration 151/1000 | Loss: 0.00014636
Iteration 152/1000 | Loss: 0.00014633
Iteration 153/1000 | Loss: 0.00014633
Iteration 154/1000 | Loss: 0.00014631
Iteration 155/1000 | Loss: 0.00014630
Iteration 156/1000 | Loss: 0.00014630
Iteration 157/1000 | Loss: 0.00014629
Iteration 158/1000 | Loss: 0.00014629
Iteration 159/1000 | Loss: 0.00014627
Iteration 160/1000 | Loss: 0.00014626
Iteration 161/1000 | Loss: 0.00014624
Iteration 162/1000 | Loss: 0.00014623
Iteration 163/1000 | Loss: 0.00014623
Iteration 164/1000 | Loss: 0.00014621
Iteration 165/1000 | Loss: 0.00014621
Iteration 166/1000 | Loss: 0.00014621
Iteration 167/1000 | Loss: 0.00014620
Iteration 168/1000 | Loss: 0.00014618
Iteration 169/1000 | Loss: 0.00014617
Iteration 170/1000 | Loss: 0.00014616
Iteration 171/1000 | Loss: 0.00014615
Iteration 172/1000 | Loss: 0.00014615
Iteration 173/1000 | Loss: 0.00014615
Iteration 174/1000 | Loss: 0.00014614
Iteration 175/1000 | Loss: 0.00014613
Iteration 176/1000 | Loss: 0.00014613
Iteration 177/1000 | Loss: 0.00014613
Iteration 178/1000 | Loss: 0.00014613
Iteration 179/1000 | Loss: 0.00014613
Iteration 180/1000 | Loss: 0.00014613
Iteration 181/1000 | Loss: 0.00014612
Iteration 182/1000 | Loss: 0.00014612
Iteration 183/1000 | Loss: 0.00014612
Iteration 184/1000 | Loss: 0.00014612
Iteration 185/1000 | Loss: 0.00014612
Iteration 186/1000 | Loss: 0.00014612
Iteration 187/1000 | Loss: 0.00014612
Iteration 188/1000 | Loss: 0.00014611
Iteration 189/1000 | Loss: 0.00014611
Iteration 190/1000 | Loss: 0.00014611
Iteration 191/1000 | Loss: 0.00014611
Iteration 192/1000 | Loss: 0.00014611
Iteration 193/1000 | Loss: 0.00014611
Iteration 194/1000 | Loss: 0.00014611
Iteration 195/1000 | Loss: 0.00014611
Iteration 196/1000 | Loss: 0.00014611
Iteration 197/1000 | Loss: 0.00014611
Iteration 198/1000 | Loss: 0.00014611
Iteration 199/1000 | Loss: 0.00014611
Iteration 200/1000 | Loss: 0.00014610
Iteration 201/1000 | Loss: 0.00014610
Iteration 202/1000 | Loss: 0.00014610
Iteration 203/1000 | Loss: 0.00014610
Iteration 204/1000 | Loss: 0.00014610
Iteration 205/1000 | Loss: 0.00014609
Iteration 206/1000 | Loss: 0.00014609
Iteration 207/1000 | Loss: 0.00014609
Iteration 208/1000 | Loss: 0.00014609
Iteration 209/1000 | Loss: 0.00014609
Iteration 210/1000 | Loss: 0.00014609
Iteration 211/1000 | Loss: 0.00014609
Iteration 212/1000 | Loss: 0.00014609
Iteration 213/1000 | Loss: 0.00014609
Iteration 214/1000 | Loss: 0.00014609
Iteration 215/1000 | Loss: 0.00014609
Iteration 216/1000 | Loss: 0.00014609
Iteration 217/1000 | Loss: 0.00014609
Iteration 218/1000 | Loss: 0.00014609
Iteration 219/1000 | Loss: 0.00014609
Iteration 220/1000 | Loss: 0.00014609
Iteration 221/1000 | Loss: 0.00014609
Iteration 222/1000 | Loss: 0.00014609
Iteration 223/1000 | Loss: 0.00014609
Iteration 224/1000 | Loss: 0.00014609
Iteration 225/1000 | Loss: 0.00014609
Iteration 226/1000 | Loss: 0.00014609
Iteration 227/1000 | Loss: 0.00014609
Iteration 228/1000 | Loss: 0.00014609
Iteration 229/1000 | Loss: 0.00014609
Iteration 230/1000 | Loss: 0.00014609
Iteration 231/1000 | Loss: 0.00014609
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 231. Stopping optimization.
Last 5 losses: [0.0001460900530219078, 0.0001460900530219078, 0.0001460900530219078, 0.0001460900530219078, 0.0001460900530219078]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0001460900530219078

Optimization complete. Final v2v error: 6.518764495849609 mm

Highest mean error: 13.463432312011719 mm for frame 4

Lowest mean error: 4.177411079406738 mm for frame 26

Saving results

Total time: 252.45111846923828
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_nl_6355/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6355/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6355/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00838305
Iteration 2/25 | Loss: 0.00207026
Iteration 3/25 | Loss: 0.00147565
Iteration 4/25 | Loss: 0.00133598
Iteration 5/25 | Loss: 0.00132323
Iteration 6/25 | Loss: 0.00132268
Iteration 7/25 | Loss: 0.00132268
Iteration 8/25 | Loss: 0.00132268
Iteration 9/25 | Loss: 0.00132268
Iteration 10/25 | Loss: 0.00132268
Iteration 11/25 | Loss: 0.00132268
Iteration 12/25 | Loss: 0.00132268
Iteration 13/25 | Loss: 0.00132268
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0013226839946582913, 0.0013226839946582913, 0.0013226839946582913, 0.0013226839946582913, 0.0013226839946582913]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013226839946582913

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45147586
Iteration 2/25 | Loss: 0.00087177
Iteration 3/25 | Loss: 0.00087175
Iteration 4/25 | Loss: 0.00087175
Iteration 5/25 | Loss: 0.00087175
Iteration 6/25 | Loss: 0.00087175
Iteration 7/25 | Loss: 0.00087174
Iteration 8/25 | Loss: 0.00087174
Iteration 9/25 | Loss: 0.00087174
Iteration 10/25 | Loss: 0.00087174
Iteration 11/25 | Loss: 0.00087174
Iteration 12/25 | Loss: 0.00087174
Iteration 13/25 | Loss: 0.00087174
Iteration 14/25 | Loss: 0.00087174
Iteration 15/25 | Loss: 0.00087174
Iteration 16/25 | Loss: 0.00087174
Iteration 17/25 | Loss: 0.00087174
Iteration 18/25 | Loss: 0.00087174
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008717436576262116, 0.0008717436576262116, 0.0008717436576262116, 0.0008717436576262116, 0.0008717436576262116]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008717436576262116

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087174
Iteration 2/1000 | Loss: 0.00004911
Iteration 3/1000 | Loss: 0.00003989
Iteration 4/1000 | Loss: 0.00003751
Iteration 5/1000 | Loss: 0.00003620
Iteration 6/1000 | Loss: 0.00003564
Iteration 7/1000 | Loss: 0.00003520
Iteration 8/1000 | Loss: 0.00003470
Iteration 9/1000 | Loss: 0.00003429
Iteration 10/1000 | Loss: 0.00003393
Iteration 11/1000 | Loss: 0.00003372
Iteration 12/1000 | Loss: 0.00003368
Iteration 13/1000 | Loss: 0.00003363
Iteration 14/1000 | Loss: 0.00003348
Iteration 15/1000 | Loss: 0.00003345
Iteration 16/1000 | Loss: 0.00003339
Iteration 17/1000 | Loss: 0.00003333
Iteration 18/1000 | Loss: 0.00003333
Iteration 19/1000 | Loss: 0.00003332
Iteration 20/1000 | Loss: 0.00003332
Iteration 21/1000 | Loss: 0.00003332
Iteration 22/1000 | Loss: 0.00003332
Iteration 23/1000 | Loss: 0.00003331
Iteration 24/1000 | Loss: 0.00003329
Iteration 25/1000 | Loss: 0.00003329
Iteration 26/1000 | Loss: 0.00003329
Iteration 27/1000 | Loss: 0.00003329
Iteration 28/1000 | Loss: 0.00003329
Iteration 29/1000 | Loss: 0.00003329
Iteration 30/1000 | Loss: 0.00003329
Iteration 31/1000 | Loss: 0.00003329
Iteration 32/1000 | Loss: 0.00003329
Iteration 33/1000 | Loss: 0.00003329
Iteration 34/1000 | Loss: 0.00003329
Iteration 35/1000 | Loss: 0.00003329
Iteration 36/1000 | Loss: 0.00003328
Iteration 37/1000 | Loss: 0.00003328
Iteration 38/1000 | Loss: 0.00003328
Iteration 39/1000 | Loss: 0.00003328
Iteration 40/1000 | Loss: 0.00003327
Iteration 41/1000 | Loss: 0.00003327
Iteration 42/1000 | Loss: 0.00003327
Iteration 43/1000 | Loss: 0.00003326
Iteration 44/1000 | Loss: 0.00003326
Iteration 45/1000 | Loss: 0.00003326
Iteration 46/1000 | Loss: 0.00003326
Iteration 47/1000 | Loss: 0.00003326
Iteration 48/1000 | Loss: 0.00003326
Iteration 49/1000 | Loss: 0.00003326
Iteration 50/1000 | Loss: 0.00003326
Iteration 51/1000 | Loss: 0.00003326
Iteration 52/1000 | Loss: 0.00003326
Iteration 53/1000 | Loss: 0.00003326
Iteration 54/1000 | Loss: 0.00003326
Iteration 55/1000 | Loss: 0.00003326
Iteration 56/1000 | Loss: 0.00003326
Iteration 57/1000 | Loss: 0.00003325
Iteration 58/1000 | Loss: 0.00003325
Iteration 59/1000 | Loss: 0.00003325
Iteration 60/1000 | Loss: 0.00003325
Iteration 61/1000 | Loss: 0.00003325
Iteration 62/1000 | Loss: 0.00003325
Iteration 63/1000 | Loss: 0.00003325
Iteration 64/1000 | Loss: 0.00003325
Iteration 65/1000 | Loss: 0.00003325
Iteration 66/1000 | Loss: 0.00003325
Iteration 67/1000 | Loss: 0.00003325
Iteration 68/1000 | Loss: 0.00003325
Iteration 69/1000 | Loss: 0.00003325
Iteration 70/1000 | Loss: 0.00003325
Iteration 71/1000 | Loss: 0.00003324
Iteration 72/1000 | Loss: 0.00003324
Iteration 73/1000 | Loss: 0.00003324
Iteration 74/1000 | Loss: 0.00003324
Iteration 75/1000 | Loss: 0.00003324
Iteration 76/1000 | Loss: 0.00003324
Iteration 77/1000 | Loss: 0.00003324
Iteration 78/1000 | Loss: 0.00003324
Iteration 79/1000 | Loss: 0.00003324
Iteration 80/1000 | Loss: 0.00003324
Iteration 81/1000 | Loss: 0.00003324
Iteration 82/1000 | Loss: 0.00003324
Iteration 83/1000 | Loss: 0.00003324
Iteration 84/1000 | Loss: 0.00003323
Iteration 85/1000 | Loss: 0.00003323
Iteration 86/1000 | Loss: 0.00003323
Iteration 87/1000 | Loss: 0.00003323
Iteration 88/1000 | Loss: 0.00003323
Iteration 89/1000 | Loss: 0.00003323
Iteration 90/1000 | Loss: 0.00003323
Iteration 91/1000 | Loss: 0.00003323
Iteration 92/1000 | Loss: 0.00003323
Iteration 93/1000 | Loss: 0.00003323
Iteration 94/1000 | Loss: 0.00003323
Iteration 95/1000 | Loss: 0.00003323
Iteration 96/1000 | Loss: 0.00003323
Iteration 97/1000 | Loss: 0.00003323
Iteration 98/1000 | Loss: 0.00003323
Iteration 99/1000 | Loss: 0.00003323
Iteration 100/1000 | Loss: 0.00003323
Iteration 101/1000 | Loss: 0.00003323
Iteration 102/1000 | Loss: 0.00003323
Iteration 103/1000 | Loss: 0.00003323
Iteration 104/1000 | Loss: 0.00003323
Iteration 105/1000 | Loss: 0.00003323
Iteration 106/1000 | Loss: 0.00003323
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [3.3232314308406785e-05, 3.3232314308406785e-05, 3.3232314308406785e-05, 3.3232314308406785e-05, 3.3232314308406785e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.3232314308406785e-05

Optimization complete. Final v2v error: 5.070079803466797 mm

Highest mean error: 5.472443580627441 mm for frame 182

Lowest mean error: 4.629488945007324 mm for frame 50

Saving results

Total time: 36.108017444610596
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_nl_6355/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6355/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6355/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01184329
Iteration 2/25 | Loss: 0.01184329
Iteration 3/25 | Loss: 0.01184329
Iteration 4/25 | Loss: 0.01184329
Iteration 5/25 | Loss: 0.01184328
Iteration 6/25 | Loss: 0.01184328
Iteration 7/25 | Loss: 0.01184328
Iteration 8/25 | Loss: 0.01184328
Iteration 9/25 | Loss: 0.01184328
Iteration 10/25 | Loss: 0.01184328
Iteration 11/25 | Loss: 0.01184328
Iteration 12/25 | Loss: 0.01184328
Iteration 13/25 | Loss: 0.01184328
Iteration 14/25 | Loss: 0.01184328
Iteration 15/25 | Loss: 0.01184328
Iteration 16/25 | Loss: 0.01184328
Iteration 17/25 | Loss: 0.01184328
Iteration 18/25 | Loss: 0.01184328
Iteration 19/25 | Loss: 0.01184327
Iteration 20/25 | Loss: 0.01184327
Iteration 21/25 | Loss: 0.01184327
Iteration 22/25 | Loss: 0.01184327
Iteration 23/25 | Loss: 0.01184327
Iteration 24/25 | Loss: 0.01184327
Iteration 25/25 | Loss: 0.01184327

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.75216949
Iteration 2/25 | Loss: 0.07148185
Iteration 3/25 | Loss: 0.07146530
Iteration 4/25 | Loss: 0.07146530
Iteration 5/25 | Loss: 0.07146529
Iteration 6/25 | Loss: 0.07146529
Iteration 7/25 | Loss: 0.07146528
Iteration 8/25 | Loss: 0.07146528
Iteration 9/25 | Loss: 0.07146528
Iteration 10/25 | Loss: 0.07146528
Iteration 11/25 | Loss: 0.07146528
Iteration 12/25 | Loss: 0.07146528
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.07146528363227844, 0.07146528363227844, 0.07146528363227844, 0.07146528363227844, 0.07146528363227844]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.07146528363227844

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.07146528
Iteration 2/1000 | Loss: 0.00058594
Iteration 3/1000 | Loss: 0.00015582
Iteration 4/1000 | Loss: 0.00007066
Iteration 5/1000 | Loss: 0.00004651
Iteration 6/1000 | Loss: 0.00004091
Iteration 7/1000 | Loss: 0.00003575
Iteration 8/1000 | Loss: 0.00003310
Iteration 9/1000 | Loss: 0.00002933
Iteration 10/1000 | Loss: 0.00002710
Iteration 11/1000 | Loss: 0.00002576
Iteration 12/1000 | Loss: 0.00002445
Iteration 13/1000 | Loss: 0.00002352
Iteration 14/1000 | Loss: 0.00002257
Iteration 15/1000 | Loss: 0.00002194
Iteration 16/1000 | Loss: 0.00002125
Iteration 17/1000 | Loss: 0.00002065
Iteration 18/1000 | Loss: 0.00002009
Iteration 19/1000 | Loss: 0.00001977
Iteration 20/1000 | Loss: 0.00001947
Iteration 21/1000 | Loss: 0.00001925
Iteration 22/1000 | Loss: 0.00001917
Iteration 23/1000 | Loss: 0.00001900
Iteration 24/1000 | Loss: 0.00001898
Iteration 25/1000 | Loss: 0.00001891
Iteration 26/1000 | Loss: 0.00001884
Iteration 27/1000 | Loss: 0.00001884
Iteration 28/1000 | Loss: 0.00001884
Iteration 29/1000 | Loss: 0.00001884
Iteration 30/1000 | Loss: 0.00001884
Iteration 31/1000 | Loss: 0.00001884
Iteration 32/1000 | Loss: 0.00001884
Iteration 33/1000 | Loss: 0.00001883
Iteration 34/1000 | Loss: 0.00001883
Iteration 35/1000 | Loss: 0.00001880
Iteration 36/1000 | Loss: 0.00001880
Iteration 37/1000 | Loss: 0.00001880
Iteration 38/1000 | Loss: 0.00001880
Iteration 39/1000 | Loss: 0.00001880
Iteration 40/1000 | Loss: 0.00001880
Iteration 41/1000 | Loss: 0.00001880
Iteration 42/1000 | Loss: 0.00001879
Iteration 43/1000 | Loss: 0.00001877
Iteration 44/1000 | Loss: 0.00001877
Iteration 45/1000 | Loss: 0.00001877
Iteration 46/1000 | Loss: 0.00001877
Iteration 47/1000 | Loss: 0.00001877
Iteration 48/1000 | Loss: 0.00001876
Iteration 49/1000 | Loss: 0.00001876
Iteration 50/1000 | Loss: 0.00001876
Iteration 51/1000 | Loss: 0.00001876
Iteration 52/1000 | Loss: 0.00001876
Iteration 53/1000 | Loss: 0.00001876
Iteration 54/1000 | Loss: 0.00001875
Iteration 55/1000 | Loss: 0.00001875
Iteration 56/1000 | Loss: 0.00001875
Iteration 57/1000 | Loss: 0.00001875
Iteration 58/1000 | Loss: 0.00001874
Iteration 59/1000 | Loss: 0.00001874
Iteration 60/1000 | Loss: 0.00001874
Iteration 61/1000 | Loss: 0.00001874
Iteration 62/1000 | Loss: 0.00001874
Iteration 63/1000 | Loss: 0.00001874
Iteration 64/1000 | Loss: 0.00001874
Iteration 65/1000 | Loss: 0.00001874
Iteration 66/1000 | Loss: 0.00001874
Iteration 67/1000 | Loss: 0.00001874
Iteration 68/1000 | Loss: 0.00001874
Iteration 69/1000 | Loss: 0.00001874
Iteration 70/1000 | Loss: 0.00001874
Iteration 71/1000 | Loss: 0.00001874
Iteration 72/1000 | Loss: 0.00001874
Iteration 73/1000 | Loss: 0.00001874
Iteration 74/1000 | Loss: 0.00001874
Iteration 75/1000 | Loss: 0.00001874
Iteration 76/1000 | Loss: 0.00001874
Iteration 77/1000 | Loss: 0.00001874
Iteration 78/1000 | Loss: 0.00001874
Iteration 79/1000 | Loss: 0.00001874
Iteration 80/1000 | Loss: 0.00001874
Iteration 81/1000 | Loss: 0.00001874
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 81. Stopping optimization.
Last 5 losses: [1.8744322005659342e-05, 1.8744322005659342e-05, 1.8744322005659342e-05, 1.8744322005659342e-05, 1.8744322005659342e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8744322005659342e-05

Optimization complete. Final v2v error: 3.7652900218963623 mm

Highest mean error: 4.192674160003662 mm for frame 0

Lowest mean error: 3.55934739112854 mm for frame 167

Saving results

Total time: 41.511003494262695
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_nl_6355/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6355/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6355/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00438578
Iteration 2/25 | Loss: 0.00143166
Iteration 3/25 | Loss: 0.00125989
Iteration 4/25 | Loss: 0.00121831
Iteration 5/25 | Loss: 0.00120314
Iteration 6/25 | Loss: 0.00119762
Iteration 7/25 | Loss: 0.00119519
Iteration 8/25 | Loss: 0.00121710
Iteration 9/25 | Loss: 0.00118646
Iteration 10/25 | Loss: 0.00117212
Iteration 11/25 | Loss: 0.00116938
Iteration 12/25 | Loss: 0.00116900
Iteration 13/25 | Loss: 0.00116900
Iteration 14/25 | Loss: 0.00116900
Iteration 15/25 | Loss: 0.00116900
Iteration 16/25 | Loss: 0.00116900
Iteration 17/25 | Loss: 0.00116900
Iteration 18/25 | Loss: 0.00116900
Iteration 19/25 | Loss: 0.00116900
Iteration 20/25 | Loss: 0.00116900
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011689959792420268, 0.0011689959792420268, 0.0011689959792420268, 0.0011689959792420268, 0.0011689959792420268]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011689959792420268

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40685499
Iteration 2/25 | Loss: 0.00066153
Iteration 3/25 | Loss: 0.00066153
Iteration 4/25 | Loss: 0.00066153
Iteration 5/25 | Loss: 0.00066153
Iteration 6/25 | Loss: 0.00066153
Iteration 7/25 | Loss: 0.00066153
Iteration 8/25 | Loss: 0.00066153
Iteration 9/25 | Loss: 0.00066153
Iteration 10/25 | Loss: 0.00066153
Iteration 11/25 | Loss: 0.00066153
Iteration 12/25 | Loss: 0.00066153
Iteration 13/25 | Loss: 0.00066153
Iteration 14/25 | Loss: 0.00066153
Iteration 15/25 | Loss: 0.00066153
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0006615252350457013, 0.0006615252350457013, 0.0006615252350457013, 0.0006615252350457013, 0.0006615252350457013]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006615252350457013

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066153
Iteration 2/1000 | Loss: 0.00003201
Iteration 3/1000 | Loss: 0.00002649
Iteration 4/1000 | Loss: 0.00002479
Iteration 5/1000 | Loss: 0.00002371
Iteration 6/1000 | Loss: 0.00002296
Iteration 7/1000 | Loss: 0.00002242
Iteration 8/1000 | Loss: 0.00002206
Iteration 9/1000 | Loss: 0.00002183
Iteration 10/1000 | Loss: 0.00002169
Iteration 11/1000 | Loss: 0.00002168
Iteration 12/1000 | Loss: 0.00002162
Iteration 13/1000 | Loss: 0.00002161
Iteration 14/1000 | Loss: 0.00002161
Iteration 15/1000 | Loss: 0.00002161
Iteration 16/1000 | Loss: 0.00002160
Iteration 17/1000 | Loss: 0.00002160
Iteration 18/1000 | Loss: 0.00002160
Iteration 19/1000 | Loss: 0.00002159
Iteration 20/1000 | Loss: 0.00002159
Iteration 21/1000 | Loss: 0.00002159
Iteration 22/1000 | Loss: 0.00002158
Iteration 23/1000 | Loss: 0.00002158
Iteration 24/1000 | Loss: 0.00002158
Iteration 25/1000 | Loss: 0.00002158
Iteration 26/1000 | Loss: 0.00002158
Iteration 27/1000 | Loss: 0.00002158
Iteration 28/1000 | Loss: 0.00002158
Iteration 29/1000 | Loss: 0.00002158
Iteration 30/1000 | Loss: 0.00002157
Iteration 31/1000 | Loss: 0.00002157
Iteration 32/1000 | Loss: 0.00002156
Iteration 33/1000 | Loss: 0.00002156
Iteration 34/1000 | Loss: 0.00002156
Iteration 35/1000 | Loss: 0.00002155
Iteration 36/1000 | Loss: 0.00002155
Iteration 37/1000 | Loss: 0.00002155
Iteration 38/1000 | Loss: 0.00002154
Iteration 39/1000 | Loss: 0.00002154
Iteration 40/1000 | Loss: 0.00002153
Iteration 41/1000 | Loss: 0.00002152
Iteration 42/1000 | Loss: 0.00002152
Iteration 43/1000 | Loss: 0.00002152
Iteration 44/1000 | Loss: 0.00002151
Iteration 45/1000 | Loss: 0.00002151
Iteration 46/1000 | Loss: 0.00002151
Iteration 47/1000 | Loss: 0.00002151
Iteration 48/1000 | Loss: 0.00002151
Iteration 49/1000 | Loss: 0.00002150
Iteration 50/1000 | Loss: 0.00002150
Iteration 51/1000 | Loss: 0.00002150
Iteration 52/1000 | Loss: 0.00002149
Iteration 53/1000 | Loss: 0.00002149
Iteration 54/1000 | Loss: 0.00002149
Iteration 55/1000 | Loss: 0.00002149
Iteration 56/1000 | Loss: 0.00002149
Iteration 57/1000 | Loss: 0.00002149
Iteration 58/1000 | Loss: 0.00002149
Iteration 59/1000 | Loss: 0.00002148
Iteration 60/1000 | Loss: 0.00002148
Iteration 61/1000 | Loss: 0.00002148
Iteration 62/1000 | Loss: 0.00002148
Iteration 63/1000 | Loss: 0.00002148
Iteration 64/1000 | Loss: 0.00002147
Iteration 65/1000 | Loss: 0.00002147
Iteration 66/1000 | Loss: 0.00002147
Iteration 67/1000 | Loss: 0.00002147
Iteration 68/1000 | Loss: 0.00002147
Iteration 69/1000 | Loss: 0.00002147
Iteration 70/1000 | Loss: 0.00002147
Iteration 71/1000 | Loss: 0.00002146
Iteration 72/1000 | Loss: 0.00002146
Iteration 73/1000 | Loss: 0.00002146
Iteration 74/1000 | Loss: 0.00002146
Iteration 75/1000 | Loss: 0.00002146
Iteration 76/1000 | Loss: 0.00002146
Iteration 77/1000 | Loss: 0.00002146
Iteration 78/1000 | Loss: 0.00002146
Iteration 79/1000 | Loss: 0.00002146
Iteration 80/1000 | Loss: 0.00002145
Iteration 81/1000 | Loss: 0.00002145
Iteration 82/1000 | Loss: 0.00002145
Iteration 83/1000 | Loss: 0.00002145
Iteration 84/1000 | Loss: 0.00002145
Iteration 85/1000 | Loss: 0.00002145
Iteration 86/1000 | Loss: 0.00002145
Iteration 87/1000 | Loss: 0.00002145
Iteration 88/1000 | Loss: 0.00002145
Iteration 89/1000 | Loss: 0.00002145
Iteration 90/1000 | Loss: 0.00002145
Iteration 91/1000 | Loss: 0.00002144
Iteration 92/1000 | Loss: 0.00002144
Iteration 93/1000 | Loss: 0.00002144
Iteration 94/1000 | Loss: 0.00002144
Iteration 95/1000 | Loss: 0.00002144
Iteration 96/1000 | Loss: 0.00002144
Iteration 97/1000 | Loss: 0.00002144
Iteration 98/1000 | Loss: 0.00002144
Iteration 99/1000 | Loss: 0.00002144
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [2.144364952982869e-05, 2.144364952982869e-05, 2.144364952982869e-05, 2.144364952982869e-05, 2.144364952982869e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.144364952982869e-05

Optimization complete. Final v2v error: 3.9523723125457764 mm

Highest mean error: 4.472157955169678 mm for frame 126

Lowest mean error: 3.5708911418914795 mm for frame 256

Saving results

Total time: 47.00272798538208
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_nl_6355/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6355/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6355/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00409545
Iteration 2/25 | Loss: 0.00140483
Iteration 3/25 | Loss: 0.00123824
Iteration 4/25 | Loss: 0.00120169
Iteration 5/25 | Loss: 0.00119041
Iteration 6/25 | Loss: 0.00118576
Iteration 7/25 | Loss: 0.00118456
Iteration 8/25 | Loss: 0.00118443
Iteration 9/25 | Loss: 0.00118443
Iteration 10/25 | Loss: 0.00118443
Iteration 11/25 | Loss: 0.00118443
Iteration 12/25 | Loss: 0.00118443
Iteration 13/25 | Loss: 0.00118443
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001184427528642118, 0.001184427528642118, 0.001184427528642118, 0.001184427528642118, 0.001184427528642118]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001184427528642118

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41278160
Iteration 2/25 | Loss: 0.00074245
Iteration 3/25 | Loss: 0.00074245
Iteration 4/25 | Loss: 0.00074245
Iteration 5/25 | Loss: 0.00074245
Iteration 6/25 | Loss: 0.00074245
Iteration 7/25 | Loss: 0.00074245
Iteration 8/25 | Loss: 0.00074245
Iteration 9/25 | Loss: 0.00074245
Iteration 10/25 | Loss: 0.00074245
Iteration 11/25 | Loss: 0.00074245
Iteration 12/25 | Loss: 0.00074245
Iteration 13/25 | Loss: 0.00074245
Iteration 14/25 | Loss: 0.00074245
Iteration 15/25 | Loss: 0.00074245
Iteration 16/25 | Loss: 0.00074245
Iteration 17/25 | Loss: 0.00074245
Iteration 18/25 | Loss: 0.00074245
Iteration 19/25 | Loss: 0.00074245
Iteration 20/25 | Loss: 0.00074245
Iteration 21/25 | Loss: 0.00074245
Iteration 22/25 | Loss: 0.00074245
Iteration 23/25 | Loss: 0.00074245
Iteration 24/25 | Loss: 0.00074245
Iteration 25/25 | Loss: 0.00074245

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074245
Iteration 2/1000 | Loss: 0.00003626
Iteration 3/1000 | Loss: 0.00002891
Iteration 4/1000 | Loss: 0.00002665
Iteration 5/1000 | Loss: 0.00002563
Iteration 6/1000 | Loss: 0.00002480
Iteration 7/1000 | Loss: 0.00002413
Iteration 8/1000 | Loss: 0.00002372
Iteration 9/1000 | Loss: 0.00002341
Iteration 10/1000 | Loss: 0.00002326
Iteration 11/1000 | Loss: 0.00002312
Iteration 12/1000 | Loss: 0.00002306
Iteration 13/1000 | Loss: 0.00002303
Iteration 14/1000 | Loss: 0.00002303
Iteration 15/1000 | Loss: 0.00002302
Iteration 16/1000 | Loss: 0.00002298
Iteration 17/1000 | Loss: 0.00002298
Iteration 18/1000 | Loss: 0.00002298
Iteration 19/1000 | Loss: 0.00002298
Iteration 20/1000 | Loss: 0.00002298
Iteration 21/1000 | Loss: 0.00002297
Iteration 22/1000 | Loss: 0.00002296
Iteration 23/1000 | Loss: 0.00002296
Iteration 24/1000 | Loss: 0.00002295
Iteration 25/1000 | Loss: 0.00002295
Iteration 26/1000 | Loss: 0.00002295
Iteration 27/1000 | Loss: 0.00002294
Iteration 28/1000 | Loss: 0.00002294
Iteration 29/1000 | Loss: 0.00002294
Iteration 30/1000 | Loss: 0.00002293
Iteration 31/1000 | Loss: 0.00002293
Iteration 32/1000 | Loss: 0.00002292
Iteration 33/1000 | Loss: 0.00002292
Iteration 34/1000 | Loss: 0.00002291
Iteration 35/1000 | Loss: 0.00002291
Iteration 36/1000 | Loss: 0.00002291
Iteration 37/1000 | Loss: 0.00002291
Iteration 38/1000 | Loss: 0.00002291
Iteration 39/1000 | Loss: 0.00002291
Iteration 40/1000 | Loss: 0.00002291
Iteration 41/1000 | Loss: 0.00002291
Iteration 42/1000 | Loss: 0.00002290
Iteration 43/1000 | Loss: 0.00002290
Iteration 44/1000 | Loss: 0.00002290
Iteration 45/1000 | Loss: 0.00002290
Iteration 46/1000 | Loss: 0.00002290
Iteration 47/1000 | Loss: 0.00002290
Iteration 48/1000 | Loss: 0.00002290
Iteration 49/1000 | Loss: 0.00002290
Iteration 50/1000 | Loss: 0.00002290
Iteration 51/1000 | Loss: 0.00002290
Iteration 52/1000 | Loss: 0.00002290
Iteration 53/1000 | Loss: 0.00002290
Iteration 54/1000 | Loss: 0.00002290
Iteration 55/1000 | Loss: 0.00002289
Iteration 56/1000 | Loss: 0.00002289
Iteration 57/1000 | Loss: 0.00002289
Iteration 58/1000 | Loss: 0.00002289
Iteration 59/1000 | Loss: 0.00002289
Iteration 60/1000 | Loss: 0.00002289
Iteration 61/1000 | Loss: 0.00002289
Iteration 62/1000 | Loss: 0.00002289
Iteration 63/1000 | Loss: 0.00002289
Iteration 64/1000 | Loss: 0.00002288
Iteration 65/1000 | Loss: 0.00002288
Iteration 66/1000 | Loss: 0.00002288
Iteration 67/1000 | Loss: 0.00002288
Iteration 68/1000 | Loss: 0.00002288
Iteration 69/1000 | Loss: 0.00002288
Iteration 70/1000 | Loss: 0.00002288
Iteration 71/1000 | Loss: 0.00002288
Iteration 72/1000 | Loss: 0.00002288
Iteration 73/1000 | Loss: 0.00002288
Iteration 74/1000 | Loss: 0.00002288
Iteration 75/1000 | Loss: 0.00002288
Iteration 76/1000 | Loss: 0.00002288
Iteration 77/1000 | Loss: 0.00002288
Iteration 78/1000 | Loss: 0.00002288
Iteration 79/1000 | Loss: 0.00002288
Iteration 80/1000 | Loss: 0.00002288
Iteration 81/1000 | Loss: 0.00002288
Iteration 82/1000 | Loss: 0.00002288
Iteration 83/1000 | Loss: 0.00002288
Iteration 84/1000 | Loss: 0.00002288
Iteration 85/1000 | Loss: 0.00002288
Iteration 86/1000 | Loss: 0.00002288
Iteration 87/1000 | Loss: 0.00002288
Iteration 88/1000 | Loss: 0.00002288
Iteration 89/1000 | Loss: 0.00002288
Iteration 90/1000 | Loss: 0.00002288
Iteration 91/1000 | Loss: 0.00002288
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [2.287927236466203e-05, 2.287927236466203e-05, 2.287927236466203e-05, 2.287927236466203e-05, 2.287927236466203e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.287927236466203e-05

Optimization complete. Final v2v error: 4.042571544647217 mm

Highest mean error: 4.2878642082214355 mm for frame 125

Lowest mean error: 3.6487526893615723 mm for frame 8

Saving results

Total time: 36.29068446159363
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_nl_6355/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6355/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6355/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00651082
Iteration 2/25 | Loss: 0.00154095
Iteration 3/25 | Loss: 0.00125754
Iteration 4/25 | Loss: 0.00122921
Iteration 5/25 | Loss: 0.00122353
Iteration 6/25 | Loss: 0.00122146
Iteration 7/25 | Loss: 0.00122136
Iteration 8/25 | Loss: 0.00122136
Iteration 9/25 | Loss: 0.00122136
Iteration 10/25 | Loss: 0.00122136
Iteration 11/25 | Loss: 0.00122136
Iteration 12/25 | Loss: 0.00122136
Iteration 13/25 | Loss: 0.00122136
Iteration 14/25 | Loss: 0.00122136
Iteration 15/25 | Loss: 0.00122136
Iteration 16/25 | Loss: 0.00122136
Iteration 17/25 | Loss: 0.00122136
Iteration 18/25 | Loss: 0.00122136
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012213578447699547, 0.0012213578447699547, 0.0012213578447699547, 0.0012213578447699547, 0.0012213578447699547]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012213578447699547

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28456163
Iteration 2/25 | Loss: 0.00092715
Iteration 3/25 | Loss: 0.00092713
Iteration 4/25 | Loss: 0.00092713
Iteration 5/25 | Loss: 0.00092713
Iteration 6/25 | Loss: 0.00092713
Iteration 7/25 | Loss: 0.00092713
Iteration 8/25 | Loss: 0.00092713
Iteration 9/25 | Loss: 0.00092713
Iteration 10/25 | Loss: 0.00092713
Iteration 11/25 | Loss: 0.00092713
Iteration 12/25 | Loss: 0.00092713
Iteration 13/25 | Loss: 0.00092713
Iteration 14/25 | Loss: 0.00092713
Iteration 15/25 | Loss: 0.00092713
Iteration 16/25 | Loss: 0.00092713
Iteration 17/25 | Loss: 0.00092713
Iteration 18/25 | Loss: 0.00092713
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000927130167838186, 0.000927130167838186, 0.000927130167838186, 0.000927130167838186, 0.000927130167838186]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000927130167838186

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092713
Iteration 2/1000 | Loss: 0.00003300
Iteration 3/1000 | Loss: 0.00002480
Iteration 4/1000 | Loss: 0.00002284
Iteration 5/1000 | Loss: 0.00002169
Iteration 6/1000 | Loss: 0.00002118
Iteration 7/1000 | Loss: 0.00002077
Iteration 8/1000 | Loss: 0.00002051
Iteration 9/1000 | Loss: 0.00002028
Iteration 10/1000 | Loss: 0.00002022
Iteration 11/1000 | Loss: 0.00002008
Iteration 12/1000 | Loss: 0.00002007
Iteration 13/1000 | Loss: 0.00002006
Iteration 14/1000 | Loss: 0.00001995
Iteration 15/1000 | Loss: 0.00001995
Iteration 16/1000 | Loss: 0.00001992
Iteration 17/1000 | Loss: 0.00001992
Iteration 18/1000 | Loss: 0.00001992
Iteration 19/1000 | Loss: 0.00001992
Iteration 20/1000 | Loss: 0.00001992
Iteration 21/1000 | Loss: 0.00001992
Iteration 22/1000 | Loss: 0.00001992
Iteration 23/1000 | Loss: 0.00001992
Iteration 24/1000 | Loss: 0.00001992
Iteration 25/1000 | Loss: 0.00001992
Iteration 26/1000 | Loss: 0.00001991
Iteration 27/1000 | Loss: 0.00001991
Iteration 28/1000 | Loss: 0.00001991
Iteration 29/1000 | Loss: 0.00001991
Iteration 30/1000 | Loss: 0.00001991
Iteration 31/1000 | Loss: 0.00001990
Iteration 32/1000 | Loss: 0.00001990
Iteration 33/1000 | Loss: 0.00001990
Iteration 34/1000 | Loss: 0.00001990
Iteration 35/1000 | Loss: 0.00001990
Iteration 36/1000 | Loss: 0.00001990
Iteration 37/1000 | Loss: 0.00001990
Iteration 38/1000 | Loss: 0.00001990
Iteration 39/1000 | Loss: 0.00001990
Iteration 40/1000 | Loss: 0.00001990
Iteration 41/1000 | Loss: 0.00001990
Iteration 42/1000 | Loss: 0.00001989
Iteration 43/1000 | Loss: 0.00001989
Iteration 44/1000 | Loss: 0.00001989
Iteration 45/1000 | Loss: 0.00001989
Iteration 46/1000 | Loss: 0.00001989
Iteration 47/1000 | Loss: 0.00001989
Iteration 48/1000 | Loss: 0.00001988
Iteration 49/1000 | Loss: 0.00001988
Iteration 50/1000 | Loss: 0.00001988
Iteration 51/1000 | Loss: 0.00001988
Iteration 52/1000 | Loss: 0.00001988
Iteration 53/1000 | Loss: 0.00001988
Iteration 54/1000 | Loss: 0.00001988
Iteration 55/1000 | Loss: 0.00001988
Iteration 56/1000 | Loss: 0.00001988
Iteration 57/1000 | Loss: 0.00001988
Iteration 58/1000 | Loss: 0.00001988
Iteration 59/1000 | Loss: 0.00001988
Iteration 60/1000 | Loss: 0.00001988
Iteration 61/1000 | Loss: 0.00001988
Iteration 62/1000 | Loss: 0.00001988
Iteration 63/1000 | Loss: 0.00001988
Iteration 64/1000 | Loss: 0.00001988
Iteration 65/1000 | Loss: 0.00001988
Iteration 66/1000 | Loss: 0.00001988
Iteration 67/1000 | Loss: 0.00001988
Iteration 68/1000 | Loss: 0.00001988
Iteration 69/1000 | Loss: 0.00001988
Iteration 70/1000 | Loss: 0.00001988
Iteration 71/1000 | Loss: 0.00001988
Iteration 72/1000 | Loss: 0.00001988
Iteration 73/1000 | Loss: 0.00001988
Iteration 74/1000 | Loss: 0.00001988
Iteration 75/1000 | Loss: 0.00001988
Iteration 76/1000 | Loss: 0.00001988
Iteration 77/1000 | Loss: 0.00001988
Iteration 78/1000 | Loss: 0.00001988
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 78. Stopping optimization.
Last 5 losses: [1.9875606085406616e-05, 1.9875606085406616e-05, 1.9875606085406616e-05, 1.9875606085406616e-05, 1.9875606085406616e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9875606085406616e-05

Optimization complete. Final v2v error: 3.8475492000579834 mm

Highest mean error: 4.735718250274658 mm for frame 81

Lowest mean error: 3.6309361457824707 mm for frame 4

Saving results

Total time: 29.419761419296265
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_nl_6355/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6355/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6355/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01106622
Iteration 2/25 | Loss: 0.00259827
Iteration 3/25 | Loss: 0.00197911
Iteration 4/25 | Loss: 0.00173171
Iteration 5/25 | Loss: 0.00173086
Iteration 6/25 | Loss: 0.00171377
Iteration 7/25 | Loss: 0.00143557
Iteration 8/25 | Loss: 0.00133093
Iteration 9/25 | Loss: 0.00130551
Iteration 10/25 | Loss: 0.00133022
Iteration 11/25 | Loss: 0.00129151
Iteration 12/25 | Loss: 0.00125949
Iteration 13/25 | Loss: 0.00127094
Iteration 14/25 | Loss: 0.00125229
Iteration 15/25 | Loss: 0.00124568
Iteration 16/25 | Loss: 0.00124942
Iteration 17/25 | Loss: 0.00125180
Iteration 18/25 | Loss: 0.00124692
Iteration 19/25 | Loss: 0.00124740
Iteration 20/25 | Loss: 0.00124553
Iteration 21/25 | Loss: 0.00125441
Iteration 22/25 | Loss: 0.00125629
Iteration 23/25 | Loss: 0.00124852
Iteration 24/25 | Loss: 0.00124730
Iteration 25/25 | Loss: 0.00124626

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47476673
Iteration 2/25 | Loss: 0.00105264
Iteration 3/25 | Loss: 0.00105264
Iteration 4/25 | Loss: 0.00105264
Iteration 5/25 | Loss: 0.00105264
Iteration 6/25 | Loss: 0.00105264
Iteration 7/25 | Loss: 0.00105264
Iteration 8/25 | Loss: 0.00105264
Iteration 9/25 | Loss: 0.00105264
Iteration 10/25 | Loss: 0.00105264
Iteration 11/25 | Loss: 0.00105264
Iteration 12/25 | Loss: 0.00105264
Iteration 13/25 | Loss: 0.00105264
Iteration 14/25 | Loss: 0.00105264
Iteration 15/25 | Loss: 0.00105264
Iteration 16/25 | Loss: 0.00105264
Iteration 17/25 | Loss: 0.00105264
Iteration 18/25 | Loss: 0.00105264
Iteration 19/25 | Loss: 0.00105264
Iteration 20/25 | Loss: 0.00105264
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010526382829993963, 0.0010526382829993963, 0.0010526382829993963, 0.0010526382829993963, 0.0010526382829993963]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010526382829993963

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105264
Iteration 2/1000 | Loss: 0.00063024
Iteration 3/1000 | Loss: 0.00189180
Iteration 4/1000 | Loss: 0.00057229
Iteration 5/1000 | Loss: 0.00063174
Iteration 6/1000 | Loss: 0.00047342
Iteration 7/1000 | Loss: 0.00015420
Iteration 8/1000 | Loss: 0.00045480
Iteration 9/1000 | Loss: 0.00030804
Iteration 10/1000 | Loss: 0.00028021
Iteration 11/1000 | Loss: 0.00015525
Iteration 12/1000 | Loss: 0.00153717
Iteration 13/1000 | Loss: 0.00343801
Iteration 14/1000 | Loss: 0.00125250
Iteration 15/1000 | Loss: 0.00070930
Iteration 16/1000 | Loss: 0.00106823
Iteration 17/1000 | Loss: 0.00036734
Iteration 18/1000 | Loss: 0.00030127
Iteration 19/1000 | Loss: 0.00033973
Iteration 20/1000 | Loss: 0.00041051
Iteration 21/1000 | Loss: 0.00222420
Iteration 22/1000 | Loss: 0.00239698
Iteration 23/1000 | Loss: 0.00105576
Iteration 24/1000 | Loss: 0.00093518
Iteration 25/1000 | Loss: 0.00101973
Iteration 26/1000 | Loss: 0.00174422
Iteration 27/1000 | Loss: 0.00112359
Iteration 28/1000 | Loss: 0.00064926
Iteration 29/1000 | Loss: 0.00029490
Iteration 30/1000 | Loss: 0.00128132
Iteration 31/1000 | Loss: 0.00040363
Iteration 32/1000 | Loss: 0.00104314
Iteration 33/1000 | Loss: 0.00046100
Iteration 34/1000 | Loss: 0.00078178
Iteration 35/1000 | Loss: 0.00026109
Iteration 36/1000 | Loss: 0.00055188
Iteration 37/1000 | Loss: 0.00042402
Iteration 38/1000 | Loss: 0.00030063
Iteration 39/1000 | Loss: 0.00009405
Iteration 40/1000 | Loss: 0.00036933
Iteration 41/1000 | Loss: 0.00035160
Iteration 42/1000 | Loss: 0.00022668
Iteration 43/1000 | Loss: 0.00036553
Iteration 44/1000 | Loss: 0.00024017
Iteration 45/1000 | Loss: 0.00022820
Iteration 46/1000 | Loss: 0.00006871
Iteration 47/1000 | Loss: 0.00048271
Iteration 48/1000 | Loss: 0.00038155
Iteration 49/1000 | Loss: 0.00026898
Iteration 50/1000 | Loss: 0.00030117
Iteration 51/1000 | Loss: 0.00022277
Iteration 52/1000 | Loss: 0.00017055
Iteration 53/1000 | Loss: 0.00018633
Iteration 54/1000 | Loss: 0.00028498
Iteration 55/1000 | Loss: 0.00040040
Iteration 56/1000 | Loss: 0.00025523
Iteration 57/1000 | Loss: 0.00026052
Iteration 58/1000 | Loss: 0.00018651
Iteration 59/1000 | Loss: 0.00022340
Iteration 60/1000 | Loss: 0.00033199
Iteration 61/1000 | Loss: 0.00029857
Iteration 62/1000 | Loss: 0.00026086
Iteration 63/1000 | Loss: 0.00111849
Iteration 64/1000 | Loss: 0.00082022
Iteration 65/1000 | Loss: 0.00048793
Iteration 66/1000 | Loss: 0.00033283
Iteration 67/1000 | Loss: 0.00135759
Iteration 68/1000 | Loss: 0.00058651
Iteration 69/1000 | Loss: 0.00041931
Iteration 70/1000 | Loss: 0.00038923
Iteration 71/1000 | Loss: 0.00006812
Iteration 72/1000 | Loss: 0.00010647
Iteration 73/1000 | Loss: 0.00041051
Iteration 74/1000 | Loss: 0.00020839
Iteration 75/1000 | Loss: 0.00009252
Iteration 76/1000 | Loss: 0.00005818
Iteration 77/1000 | Loss: 0.00004854
Iteration 78/1000 | Loss: 0.00025761
Iteration 79/1000 | Loss: 0.00032514
Iteration 80/1000 | Loss: 0.00025904
Iteration 81/1000 | Loss: 0.00026938
Iteration 82/1000 | Loss: 0.00015233
Iteration 83/1000 | Loss: 0.00015045
Iteration 84/1000 | Loss: 0.00014255
Iteration 85/1000 | Loss: 0.00011932
Iteration 86/1000 | Loss: 0.00027067
Iteration 87/1000 | Loss: 0.00012688
Iteration 88/1000 | Loss: 0.00008404
Iteration 89/1000 | Loss: 0.00006491
Iteration 90/1000 | Loss: 0.00004120
Iteration 91/1000 | Loss: 0.00007753
Iteration 92/1000 | Loss: 0.00045843
Iteration 93/1000 | Loss: 0.00023686
Iteration 94/1000 | Loss: 0.00010516
Iteration 95/1000 | Loss: 0.00003635
Iteration 96/1000 | Loss: 0.00039058
Iteration 97/1000 | Loss: 0.00053674
Iteration 98/1000 | Loss: 0.00115568
Iteration 99/1000 | Loss: 0.00014587
Iteration 100/1000 | Loss: 0.00040709
Iteration 101/1000 | Loss: 0.00060691
Iteration 102/1000 | Loss: 0.00037366
Iteration 103/1000 | Loss: 0.00005675
Iteration 104/1000 | Loss: 0.00004629
Iteration 105/1000 | Loss: 0.00072796
Iteration 106/1000 | Loss: 0.00007464
Iteration 107/1000 | Loss: 0.00004421
Iteration 108/1000 | Loss: 0.00003805
Iteration 109/1000 | Loss: 0.00072629
Iteration 110/1000 | Loss: 0.00019711
Iteration 111/1000 | Loss: 0.00004816
Iteration 112/1000 | Loss: 0.00086248
Iteration 113/1000 | Loss: 0.00017284
Iteration 114/1000 | Loss: 0.00042467
Iteration 115/1000 | Loss: 0.00035274
Iteration 116/1000 | Loss: 0.00013048
Iteration 117/1000 | Loss: 0.00055960
Iteration 118/1000 | Loss: 0.00086875
Iteration 119/1000 | Loss: 0.00096260
Iteration 120/1000 | Loss: 0.00093103
Iteration 121/1000 | Loss: 0.00059128
Iteration 122/1000 | Loss: 0.00027927
Iteration 123/1000 | Loss: 0.00021434
Iteration 124/1000 | Loss: 0.00026493
Iteration 125/1000 | Loss: 0.00035491
Iteration 126/1000 | Loss: 0.00060053
Iteration 127/1000 | Loss: 0.00067338
Iteration 128/1000 | Loss: 0.00053343
Iteration 129/1000 | Loss: 0.00035738
Iteration 130/1000 | Loss: 0.00039753
Iteration 131/1000 | Loss: 0.00032382
Iteration 132/1000 | Loss: 0.00034692
Iteration 133/1000 | Loss: 0.00046721
Iteration 134/1000 | Loss: 0.00060605
Iteration 135/1000 | Loss: 0.00008119
Iteration 136/1000 | Loss: 0.00005189
Iteration 137/1000 | Loss: 0.00028346
Iteration 138/1000 | Loss: 0.00030605
Iteration 139/1000 | Loss: 0.00027998
Iteration 140/1000 | Loss: 0.00044240
Iteration 141/1000 | Loss: 0.00037712
Iteration 142/1000 | Loss: 0.00041688
Iteration 143/1000 | Loss: 0.00028202
Iteration 144/1000 | Loss: 0.00038960
Iteration 145/1000 | Loss: 0.00025420
Iteration 146/1000 | Loss: 0.00038681
Iteration 147/1000 | Loss: 0.00010507
Iteration 148/1000 | Loss: 0.00003219
Iteration 149/1000 | Loss: 0.00037128
Iteration 150/1000 | Loss: 0.00023972
Iteration 151/1000 | Loss: 0.00020515
Iteration 152/1000 | Loss: 0.00015304
Iteration 153/1000 | Loss: 0.00021294
Iteration 154/1000 | Loss: 0.00049741
Iteration 155/1000 | Loss: 0.00004043
Iteration 156/1000 | Loss: 0.00003189
Iteration 157/1000 | Loss: 0.00002726
Iteration 158/1000 | Loss: 0.00002606
Iteration 159/1000 | Loss: 0.00002563
Iteration 160/1000 | Loss: 0.00002536
Iteration 161/1000 | Loss: 0.00002524
Iteration 162/1000 | Loss: 0.00002523
Iteration 163/1000 | Loss: 0.00002522
Iteration 164/1000 | Loss: 0.00002521
Iteration 165/1000 | Loss: 0.00002520
Iteration 166/1000 | Loss: 0.00002520
Iteration 167/1000 | Loss: 0.00002519
Iteration 168/1000 | Loss: 0.00002519
Iteration 169/1000 | Loss: 0.00002502
Iteration 170/1000 | Loss: 0.00002502
Iteration 171/1000 | Loss: 0.00002498
Iteration 172/1000 | Loss: 0.00002498
Iteration 173/1000 | Loss: 0.00002497
Iteration 174/1000 | Loss: 0.00002495
Iteration 175/1000 | Loss: 0.00002495
Iteration 176/1000 | Loss: 0.00002494
Iteration 177/1000 | Loss: 0.00002493
Iteration 178/1000 | Loss: 0.00002493
Iteration 179/1000 | Loss: 0.00002492
Iteration 180/1000 | Loss: 0.00002492
Iteration 181/1000 | Loss: 0.00002491
Iteration 182/1000 | Loss: 0.00002489
Iteration 183/1000 | Loss: 0.00002488
Iteration 184/1000 | Loss: 0.00002488
Iteration 185/1000 | Loss: 0.00002487
Iteration 186/1000 | Loss: 0.00002485
Iteration 187/1000 | Loss: 0.00002485
Iteration 188/1000 | Loss: 0.00002485
Iteration 189/1000 | Loss: 0.00002485
Iteration 190/1000 | Loss: 0.00002484
Iteration 191/1000 | Loss: 0.00002484
Iteration 192/1000 | Loss: 0.00002484
Iteration 193/1000 | Loss: 0.00002484
Iteration 194/1000 | Loss: 0.00002484
Iteration 195/1000 | Loss: 0.00002484
Iteration 196/1000 | Loss: 0.00002484
Iteration 197/1000 | Loss: 0.00002483
Iteration 198/1000 | Loss: 0.00002483
Iteration 199/1000 | Loss: 0.00002483
Iteration 200/1000 | Loss: 0.00002483
Iteration 201/1000 | Loss: 0.00002483
Iteration 202/1000 | Loss: 0.00002483
Iteration 203/1000 | Loss: 0.00002483
Iteration 204/1000 | Loss: 0.00002482
Iteration 205/1000 | Loss: 0.00002482
Iteration 206/1000 | Loss: 0.00002482
Iteration 207/1000 | Loss: 0.00002481
Iteration 208/1000 | Loss: 0.00002481
Iteration 209/1000 | Loss: 0.00002481
Iteration 210/1000 | Loss: 0.00002481
Iteration 211/1000 | Loss: 0.00002481
Iteration 212/1000 | Loss: 0.00002481
Iteration 213/1000 | Loss: 0.00002481
Iteration 214/1000 | Loss: 0.00002481
Iteration 215/1000 | Loss: 0.00002481
Iteration 216/1000 | Loss: 0.00002481
Iteration 217/1000 | Loss: 0.00002481
Iteration 218/1000 | Loss: 0.00002480
Iteration 219/1000 | Loss: 0.00002480
Iteration 220/1000 | Loss: 0.00002480
Iteration 221/1000 | Loss: 0.00002480
Iteration 222/1000 | Loss: 0.00002479
Iteration 223/1000 | Loss: 0.00002479
Iteration 224/1000 | Loss: 0.00002479
Iteration 225/1000 | Loss: 0.00002479
Iteration 226/1000 | Loss: 0.00002479
Iteration 227/1000 | Loss: 0.00002479
Iteration 228/1000 | Loss: 0.00002479
Iteration 229/1000 | Loss: 0.00002478
Iteration 230/1000 | Loss: 0.00002478
Iteration 231/1000 | Loss: 0.00002478
Iteration 232/1000 | Loss: 0.00002478
Iteration 233/1000 | Loss: 0.00002478
Iteration 234/1000 | Loss: 0.00002478
Iteration 235/1000 | Loss: 0.00002477
Iteration 236/1000 | Loss: 0.00002477
Iteration 237/1000 | Loss: 0.00002477
Iteration 238/1000 | Loss: 0.00002477
Iteration 239/1000 | Loss: 0.00002477
Iteration 240/1000 | Loss: 0.00002477
Iteration 241/1000 | Loss: 0.00002477
Iteration 242/1000 | Loss: 0.00002477
Iteration 243/1000 | Loss: 0.00002477
Iteration 244/1000 | Loss: 0.00002477
Iteration 245/1000 | Loss: 0.00002477
Iteration 246/1000 | Loss: 0.00002477
Iteration 247/1000 | Loss: 0.00002477
Iteration 248/1000 | Loss: 0.00002476
Iteration 249/1000 | Loss: 0.00002476
Iteration 250/1000 | Loss: 0.00002476
Iteration 251/1000 | Loss: 0.00002476
Iteration 252/1000 | Loss: 0.00002476
Iteration 253/1000 | Loss: 0.00002475
Iteration 254/1000 | Loss: 0.00002475
Iteration 255/1000 | Loss: 0.00002475
Iteration 256/1000 | Loss: 0.00002475
Iteration 257/1000 | Loss: 0.00002475
Iteration 258/1000 | Loss: 0.00002475
Iteration 259/1000 | Loss: 0.00002475
Iteration 260/1000 | Loss: 0.00002475
Iteration 261/1000 | Loss: 0.00002475
Iteration 262/1000 | Loss: 0.00002475
Iteration 263/1000 | Loss: 0.00002475
Iteration 264/1000 | Loss: 0.00002475
Iteration 265/1000 | Loss: 0.00002475
Iteration 266/1000 | Loss: 0.00002474
Iteration 267/1000 | Loss: 0.00002474
Iteration 268/1000 | Loss: 0.00002474
Iteration 269/1000 | Loss: 0.00002474
Iteration 270/1000 | Loss: 0.00002474
Iteration 271/1000 | Loss: 0.00002474
Iteration 272/1000 | Loss: 0.00002474
Iteration 273/1000 | Loss: 0.00002474
Iteration 274/1000 | Loss: 0.00002474
Iteration 275/1000 | Loss: 0.00002474
Iteration 276/1000 | Loss: 0.00002474
Iteration 277/1000 | Loss: 0.00002474
Iteration 278/1000 | Loss: 0.00002474
Iteration 279/1000 | Loss: 0.00002474
Iteration 280/1000 | Loss: 0.00002474
Iteration 281/1000 | Loss: 0.00002474
Iteration 282/1000 | Loss: 0.00002474
Iteration 283/1000 | Loss: 0.00002474
Iteration 284/1000 | Loss: 0.00002474
Iteration 285/1000 | Loss: 0.00002474
Iteration 286/1000 | Loss: 0.00002474
Iteration 287/1000 | Loss: 0.00002474
Iteration 288/1000 | Loss: 0.00002474
Iteration 289/1000 | Loss: 0.00002474
Iteration 290/1000 | Loss: 0.00002474
Iteration 291/1000 | Loss: 0.00002474
Iteration 292/1000 | Loss: 0.00002474
Iteration 293/1000 | Loss: 0.00002474
Iteration 294/1000 | Loss: 0.00002474
Iteration 295/1000 | Loss: 0.00002474
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 295. Stopping optimization.
Last 5 losses: [2.4740076696616597e-05, 2.4740076696616597e-05, 2.4740076696616597e-05, 2.4740076696616597e-05, 2.4740076696616597e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4740076696616597e-05

Optimization complete. Final v2v error: 4.239895820617676 mm

Highest mean error: 4.795576095581055 mm for frame 45

Lowest mean error: 3.868835926055908 mm for frame 55

Saving results

Total time: 267.33224725723267
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_nl_6355/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6355/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6355/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00397982
Iteration 2/25 | Loss: 0.00121138
Iteration 3/25 | Loss: 0.00115025
Iteration 4/25 | Loss: 0.00113753
Iteration 5/25 | Loss: 0.00113261
Iteration 6/25 | Loss: 0.00113114
Iteration 7/25 | Loss: 0.00113090
Iteration 8/25 | Loss: 0.00113090
Iteration 9/25 | Loss: 0.00113090
Iteration 10/25 | Loss: 0.00113090
Iteration 11/25 | Loss: 0.00113090
Iteration 12/25 | Loss: 0.00113090
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011308969696983695, 0.0011308969696983695, 0.0011308969696983695, 0.0011308969696983695, 0.0011308969696983695]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011308969696983695

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50599444
Iteration 2/25 | Loss: 0.00058990
Iteration 3/25 | Loss: 0.00058990
Iteration 4/25 | Loss: 0.00058990
Iteration 5/25 | Loss: 0.00058990
Iteration 6/25 | Loss: 0.00058990
Iteration 7/25 | Loss: 0.00058989
Iteration 8/25 | Loss: 0.00058989
Iteration 9/25 | Loss: 0.00058989
Iteration 10/25 | Loss: 0.00058989
Iteration 11/25 | Loss: 0.00058989
Iteration 12/25 | Loss: 0.00058989
Iteration 13/25 | Loss: 0.00058989
Iteration 14/25 | Loss: 0.00058989
Iteration 15/25 | Loss: 0.00058989
Iteration 16/25 | Loss: 0.00058989
Iteration 17/25 | Loss: 0.00058989
Iteration 18/25 | Loss: 0.00058989
Iteration 19/25 | Loss: 0.00058989
Iteration 20/25 | Loss: 0.00058989
Iteration 21/25 | Loss: 0.00058989
Iteration 22/25 | Loss: 0.00058989
Iteration 23/25 | Loss: 0.00058989
Iteration 24/25 | Loss: 0.00058989
Iteration 25/25 | Loss: 0.00058989

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058989
Iteration 2/1000 | Loss: 0.00003173
Iteration 3/1000 | Loss: 0.00002388
Iteration 4/1000 | Loss: 0.00002168
Iteration 5/1000 | Loss: 0.00002077
Iteration 6/1000 | Loss: 0.00002005
Iteration 7/1000 | Loss: 0.00001965
Iteration 8/1000 | Loss: 0.00001933
Iteration 9/1000 | Loss: 0.00001918
Iteration 10/1000 | Loss: 0.00001916
Iteration 11/1000 | Loss: 0.00001912
Iteration 12/1000 | Loss: 0.00001910
Iteration 13/1000 | Loss: 0.00001909
Iteration 14/1000 | Loss: 0.00001907
Iteration 15/1000 | Loss: 0.00001906
Iteration 16/1000 | Loss: 0.00001906
Iteration 17/1000 | Loss: 0.00001901
Iteration 18/1000 | Loss: 0.00001901
Iteration 19/1000 | Loss: 0.00001888
Iteration 20/1000 | Loss: 0.00001887
Iteration 21/1000 | Loss: 0.00001886
Iteration 22/1000 | Loss: 0.00001883
Iteration 23/1000 | Loss: 0.00001883
Iteration 24/1000 | Loss: 0.00001882
Iteration 25/1000 | Loss: 0.00001882
Iteration 26/1000 | Loss: 0.00001881
Iteration 27/1000 | Loss: 0.00001881
Iteration 28/1000 | Loss: 0.00001880
Iteration 29/1000 | Loss: 0.00001880
Iteration 30/1000 | Loss: 0.00001879
Iteration 31/1000 | Loss: 0.00001879
Iteration 32/1000 | Loss: 0.00001878
Iteration 33/1000 | Loss: 0.00001877
Iteration 34/1000 | Loss: 0.00001877
Iteration 35/1000 | Loss: 0.00001877
Iteration 36/1000 | Loss: 0.00001877
Iteration 37/1000 | Loss: 0.00001877
Iteration 38/1000 | Loss: 0.00001877
Iteration 39/1000 | Loss: 0.00001877
Iteration 40/1000 | Loss: 0.00001876
Iteration 41/1000 | Loss: 0.00001876
Iteration 42/1000 | Loss: 0.00001876
Iteration 43/1000 | Loss: 0.00001876
Iteration 44/1000 | Loss: 0.00001876
Iteration 45/1000 | Loss: 0.00001876
Iteration 46/1000 | Loss: 0.00001876
Iteration 47/1000 | Loss: 0.00001875
Iteration 48/1000 | Loss: 0.00001875
Iteration 49/1000 | Loss: 0.00001874
Iteration 50/1000 | Loss: 0.00001874
Iteration 51/1000 | Loss: 0.00001874
Iteration 52/1000 | Loss: 0.00001874
Iteration 53/1000 | Loss: 0.00001874
Iteration 54/1000 | Loss: 0.00001873
Iteration 55/1000 | Loss: 0.00001873
Iteration 56/1000 | Loss: 0.00001873
Iteration 57/1000 | Loss: 0.00001873
Iteration 58/1000 | Loss: 0.00001873
Iteration 59/1000 | Loss: 0.00001872
Iteration 60/1000 | Loss: 0.00001872
Iteration 61/1000 | Loss: 0.00001872
Iteration 62/1000 | Loss: 0.00001872
Iteration 63/1000 | Loss: 0.00001872
Iteration 64/1000 | Loss: 0.00001872
Iteration 65/1000 | Loss: 0.00001872
Iteration 66/1000 | Loss: 0.00001871
Iteration 67/1000 | Loss: 0.00001871
Iteration 68/1000 | Loss: 0.00001871
Iteration 69/1000 | Loss: 0.00001871
Iteration 70/1000 | Loss: 0.00001871
Iteration 71/1000 | Loss: 0.00001871
Iteration 72/1000 | Loss: 0.00001871
Iteration 73/1000 | Loss: 0.00001871
Iteration 74/1000 | Loss: 0.00001871
Iteration 75/1000 | Loss: 0.00001871
Iteration 76/1000 | Loss: 0.00001871
Iteration 77/1000 | Loss: 0.00001871
Iteration 78/1000 | Loss: 0.00001871
Iteration 79/1000 | Loss: 0.00001871
Iteration 80/1000 | Loss: 0.00001871
Iteration 81/1000 | Loss: 0.00001871
Iteration 82/1000 | Loss: 0.00001870
Iteration 83/1000 | Loss: 0.00001870
Iteration 84/1000 | Loss: 0.00001870
Iteration 85/1000 | Loss: 0.00001870
Iteration 86/1000 | Loss: 0.00001870
Iteration 87/1000 | Loss: 0.00001870
Iteration 88/1000 | Loss: 0.00001870
Iteration 89/1000 | Loss: 0.00001870
Iteration 90/1000 | Loss: 0.00001870
Iteration 91/1000 | Loss: 0.00001870
Iteration 92/1000 | Loss: 0.00001870
Iteration 93/1000 | Loss: 0.00001870
Iteration 94/1000 | Loss: 0.00001870
Iteration 95/1000 | Loss: 0.00001870
Iteration 96/1000 | Loss: 0.00001870
Iteration 97/1000 | Loss: 0.00001869
Iteration 98/1000 | Loss: 0.00001869
Iteration 99/1000 | Loss: 0.00001869
Iteration 100/1000 | Loss: 0.00001869
Iteration 101/1000 | Loss: 0.00001869
Iteration 102/1000 | Loss: 0.00001869
Iteration 103/1000 | Loss: 0.00001869
Iteration 104/1000 | Loss: 0.00001869
Iteration 105/1000 | Loss: 0.00001869
Iteration 106/1000 | Loss: 0.00001869
Iteration 107/1000 | Loss: 0.00001869
Iteration 108/1000 | Loss: 0.00001869
Iteration 109/1000 | Loss: 0.00001869
Iteration 110/1000 | Loss: 0.00001869
Iteration 111/1000 | Loss: 0.00001869
Iteration 112/1000 | Loss: 0.00001869
Iteration 113/1000 | Loss: 0.00001868
Iteration 114/1000 | Loss: 0.00001868
Iteration 115/1000 | Loss: 0.00001868
Iteration 116/1000 | Loss: 0.00001868
Iteration 117/1000 | Loss: 0.00001868
Iteration 118/1000 | Loss: 0.00001868
Iteration 119/1000 | Loss: 0.00001868
Iteration 120/1000 | Loss: 0.00001868
Iteration 121/1000 | Loss: 0.00001868
Iteration 122/1000 | Loss: 0.00001868
Iteration 123/1000 | Loss: 0.00001867
Iteration 124/1000 | Loss: 0.00001867
Iteration 125/1000 | Loss: 0.00001867
Iteration 126/1000 | Loss: 0.00001867
Iteration 127/1000 | Loss: 0.00001867
Iteration 128/1000 | Loss: 0.00001867
Iteration 129/1000 | Loss: 0.00001867
Iteration 130/1000 | Loss: 0.00001867
Iteration 131/1000 | Loss: 0.00001867
Iteration 132/1000 | Loss: 0.00001867
Iteration 133/1000 | Loss: 0.00001867
Iteration 134/1000 | Loss: 0.00001867
Iteration 135/1000 | Loss: 0.00001866
Iteration 136/1000 | Loss: 0.00001866
Iteration 137/1000 | Loss: 0.00001866
Iteration 138/1000 | Loss: 0.00001866
Iteration 139/1000 | Loss: 0.00001866
Iteration 140/1000 | Loss: 0.00001866
Iteration 141/1000 | Loss: 0.00001866
Iteration 142/1000 | Loss: 0.00001866
Iteration 143/1000 | Loss: 0.00001866
Iteration 144/1000 | Loss: 0.00001866
Iteration 145/1000 | Loss: 0.00001866
Iteration 146/1000 | Loss: 0.00001866
Iteration 147/1000 | Loss: 0.00001866
Iteration 148/1000 | Loss: 0.00001866
Iteration 149/1000 | Loss: 0.00001866
Iteration 150/1000 | Loss: 0.00001866
Iteration 151/1000 | Loss: 0.00001865
Iteration 152/1000 | Loss: 0.00001865
Iteration 153/1000 | Loss: 0.00001865
Iteration 154/1000 | Loss: 0.00001865
Iteration 155/1000 | Loss: 0.00001865
Iteration 156/1000 | Loss: 0.00001865
Iteration 157/1000 | Loss: 0.00001865
Iteration 158/1000 | Loss: 0.00001865
Iteration 159/1000 | Loss: 0.00001865
Iteration 160/1000 | Loss: 0.00001865
Iteration 161/1000 | Loss: 0.00001865
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.8652668586582877e-05, 1.8652668586582877e-05, 1.8652668586582877e-05, 1.8652668586582877e-05, 1.8652668586582877e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8652668586582877e-05

Optimization complete. Final v2v error: 3.732046127319336 mm

Highest mean error: 3.943866014480591 mm for frame 110

Lowest mean error: 3.482391834259033 mm for frame 0

Saving results

Total time: 33.316808223724365
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_nl_6355/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6355/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6355/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00499009
Iteration 2/25 | Loss: 0.00148505
Iteration 3/25 | Loss: 0.00121603
Iteration 4/25 | Loss: 0.00118870
Iteration 5/25 | Loss: 0.00118069
Iteration 6/25 | Loss: 0.00117785
Iteration 7/25 | Loss: 0.00117720
Iteration 8/25 | Loss: 0.00117720
Iteration 9/25 | Loss: 0.00117720
Iteration 10/25 | Loss: 0.00117720
Iteration 11/25 | Loss: 0.00117720
Iteration 12/25 | Loss: 0.00117720
Iteration 13/25 | Loss: 0.00117720
Iteration 14/25 | Loss: 0.00117720
Iteration 15/25 | Loss: 0.00117720
Iteration 16/25 | Loss: 0.00117720
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011771953431889415, 0.0011771953431889415, 0.0011771953431889415, 0.0011771953431889415, 0.0011771953431889415]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011771953431889415

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47734785
Iteration 2/25 | Loss: 0.00069687
Iteration 3/25 | Loss: 0.00069687
Iteration 4/25 | Loss: 0.00069687
Iteration 5/25 | Loss: 0.00069687
Iteration 6/25 | Loss: 0.00069687
Iteration 7/25 | Loss: 0.00069687
Iteration 8/25 | Loss: 0.00069687
Iteration 9/25 | Loss: 0.00069687
Iteration 10/25 | Loss: 0.00069687
Iteration 11/25 | Loss: 0.00069687
Iteration 12/25 | Loss: 0.00069687
Iteration 13/25 | Loss: 0.00069687
Iteration 14/25 | Loss: 0.00069687
Iteration 15/25 | Loss: 0.00069687
Iteration 16/25 | Loss: 0.00069687
Iteration 17/25 | Loss: 0.00069687
Iteration 18/25 | Loss: 0.00069687
Iteration 19/25 | Loss: 0.00069687
Iteration 20/25 | Loss: 0.00069687
Iteration 21/25 | Loss: 0.00069687
Iteration 22/25 | Loss: 0.00069687
Iteration 23/25 | Loss: 0.00069687
Iteration 24/25 | Loss: 0.00069687
Iteration 25/25 | Loss: 0.00069687
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006968693342059851, 0.0006968693342059851, 0.0006968693342059851, 0.0006968693342059851, 0.0006968693342059851]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006968693342059851

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069687
Iteration 2/1000 | Loss: 0.00004052
Iteration 3/1000 | Loss: 0.00002895
Iteration 4/1000 | Loss: 0.00002619
Iteration 5/1000 | Loss: 0.00002509
Iteration 6/1000 | Loss: 0.00002437
Iteration 7/1000 | Loss: 0.00002399
Iteration 8/1000 | Loss: 0.00002360
Iteration 9/1000 | Loss: 0.00002326
Iteration 10/1000 | Loss: 0.00002301
Iteration 11/1000 | Loss: 0.00002283
Iteration 12/1000 | Loss: 0.00002276
Iteration 13/1000 | Loss: 0.00002261
Iteration 14/1000 | Loss: 0.00002255
Iteration 15/1000 | Loss: 0.00002254
Iteration 16/1000 | Loss: 0.00002252
Iteration 17/1000 | Loss: 0.00002249
Iteration 18/1000 | Loss: 0.00002248
Iteration 19/1000 | Loss: 0.00002247
Iteration 20/1000 | Loss: 0.00002246
Iteration 21/1000 | Loss: 0.00002246
Iteration 22/1000 | Loss: 0.00002245
Iteration 23/1000 | Loss: 0.00002245
Iteration 24/1000 | Loss: 0.00002244
Iteration 25/1000 | Loss: 0.00002244
Iteration 26/1000 | Loss: 0.00002244
Iteration 27/1000 | Loss: 0.00002243
Iteration 28/1000 | Loss: 0.00002243
Iteration 29/1000 | Loss: 0.00002241
Iteration 30/1000 | Loss: 0.00002241
Iteration 31/1000 | Loss: 0.00002240
Iteration 32/1000 | Loss: 0.00002239
Iteration 33/1000 | Loss: 0.00002239
Iteration 34/1000 | Loss: 0.00002239
Iteration 35/1000 | Loss: 0.00002239
Iteration 36/1000 | Loss: 0.00002239
Iteration 37/1000 | Loss: 0.00002239
Iteration 38/1000 | Loss: 0.00002239
Iteration 39/1000 | Loss: 0.00002239
Iteration 40/1000 | Loss: 0.00002239
Iteration 41/1000 | Loss: 0.00002239
Iteration 42/1000 | Loss: 0.00002238
Iteration 43/1000 | Loss: 0.00002237
Iteration 44/1000 | Loss: 0.00002237
Iteration 45/1000 | Loss: 0.00002237
Iteration 46/1000 | Loss: 0.00002237
Iteration 47/1000 | Loss: 0.00002236
Iteration 48/1000 | Loss: 0.00002236
Iteration 49/1000 | Loss: 0.00002236
Iteration 50/1000 | Loss: 0.00002236
Iteration 51/1000 | Loss: 0.00002236
Iteration 52/1000 | Loss: 0.00002236
Iteration 53/1000 | Loss: 0.00002235
Iteration 54/1000 | Loss: 0.00002235
Iteration 55/1000 | Loss: 0.00002235
Iteration 56/1000 | Loss: 0.00002235
Iteration 57/1000 | Loss: 0.00002234
Iteration 58/1000 | Loss: 0.00002234
Iteration 59/1000 | Loss: 0.00002234
Iteration 60/1000 | Loss: 0.00002234
Iteration 61/1000 | Loss: 0.00002234
Iteration 62/1000 | Loss: 0.00002234
Iteration 63/1000 | Loss: 0.00002233
Iteration 64/1000 | Loss: 0.00002233
Iteration 65/1000 | Loss: 0.00002233
Iteration 66/1000 | Loss: 0.00002233
Iteration 67/1000 | Loss: 0.00002233
Iteration 68/1000 | Loss: 0.00002233
Iteration 69/1000 | Loss: 0.00002233
Iteration 70/1000 | Loss: 0.00002233
Iteration 71/1000 | Loss: 0.00002233
Iteration 72/1000 | Loss: 0.00002233
Iteration 73/1000 | Loss: 0.00002233
Iteration 74/1000 | Loss: 0.00002232
Iteration 75/1000 | Loss: 0.00002232
Iteration 76/1000 | Loss: 0.00002232
Iteration 77/1000 | Loss: 0.00002232
Iteration 78/1000 | Loss: 0.00002232
Iteration 79/1000 | Loss: 0.00002232
Iteration 80/1000 | Loss: 0.00002231
Iteration 81/1000 | Loss: 0.00002231
Iteration 82/1000 | Loss: 0.00002231
Iteration 83/1000 | Loss: 0.00002231
Iteration 84/1000 | Loss: 0.00002231
Iteration 85/1000 | Loss: 0.00002231
Iteration 86/1000 | Loss: 0.00002231
Iteration 87/1000 | Loss: 0.00002231
Iteration 88/1000 | Loss: 0.00002231
Iteration 89/1000 | Loss: 0.00002231
Iteration 90/1000 | Loss: 0.00002231
Iteration 91/1000 | Loss: 0.00002231
Iteration 92/1000 | Loss: 0.00002230
Iteration 93/1000 | Loss: 0.00002230
Iteration 94/1000 | Loss: 0.00002230
Iteration 95/1000 | Loss: 0.00002230
Iteration 96/1000 | Loss: 0.00002230
Iteration 97/1000 | Loss: 0.00002230
Iteration 98/1000 | Loss: 0.00002230
Iteration 99/1000 | Loss: 0.00002230
Iteration 100/1000 | Loss: 0.00002230
Iteration 101/1000 | Loss: 0.00002230
Iteration 102/1000 | Loss: 0.00002230
Iteration 103/1000 | Loss: 0.00002229
Iteration 104/1000 | Loss: 0.00002229
Iteration 105/1000 | Loss: 0.00002229
Iteration 106/1000 | Loss: 0.00002229
Iteration 107/1000 | Loss: 0.00002229
Iteration 108/1000 | Loss: 0.00002229
Iteration 109/1000 | Loss: 0.00002229
Iteration 110/1000 | Loss: 0.00002229
Iteration 111/1000 | Loss: 0.00002229
Iteration 112/1000 | Loss: 0.00002229
Iteration 113/1000 | Loss: 0.00002229
Iteration 114/1000 | Loss: 0.00002229
Iteration 115/1000 | Loss: 0.00002229
Iteration 116/1000 | Loss: 0.00002229
Iteration 117/1000 | Loss: 0.00002229
Iteration 118/1000 | Loss: 0.00002228
Iteration 119/1000 | Loss: 0.00002228
Iteration 120/1000 | Loss: 0.00002228
Iteration 121/1000 | Loss: 0.00002228
Iteration 122/1000 | Loss: 0.00002228
Iteration 123/1000 | Loss: 0.00002228
Iteration 124/1000 | Loss: 0.00002228
Iteration 125/1000 | Loss: 0.00002228
Iteration 126/1000 | Loss: 0.00002228
Iteration 127/1000 | Loss: 0.00002228
Iteration 128/1000 | Loss: 0.00002228
Iteration 129/1000 | Loss: 0.00002228
Iteration 130/1000 | Loss: 0.00002228
Iteration 131/1000 | Loss: 0.00002228
Iteration 132/1000 | Loss: 0.00002228
Iteration 133/1000 | Loss: 0.00002228
Iteration 134/1000 | Loss: 0.00002228
Iteration 135/1000 | Loss: 0.00002228
Iteration 136/1000 | Loss: 0.00002228
Iteration 137/1000 | Loss: 0.00002228
Iteration 138/1000 | Loss: 0.00002228
Iteration 139/1000 | Loss: 0.00002228
Iteration 140/1000 | Loss: 0.00002228
Iteration 141/1000 | Loss: 0.00002228
Iteration 142/1000 | Loss: 0.00002228
Iteration 143/1000 | Loss: 0.00002228
Iteration 144/1000 | Loss: 0.00002228
Iteration 145/1000 | Loss: 0.00002228
Iteration 146/1000 | Loss: 0.00002228
Iteration 147/1000 | Loss: 0.00002228
Iteration 148/1000 | Loss: 0.00002228
Iteration 149/1000 | Loss: 0.00002228
Iteration 150/1000 | Loss: 0.00002228
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [2.2280271878116764e-05, 2.2280271878116764e-05, 2.2280271878116764e-05, 2.2280271878116764e-05, 2.2280271878116764e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2280271878116764e-05

Optimization complete. Final v2v error: 4.03977632522583 mm

Highest mean error: 5.150350570678711 mm for frame 53

Lowest mean error: 3.532672166824341 mm for frame 41

Saving results

Total time: 36.607316732406616
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_nl_6355/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6355/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6355/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00884022
Iteration 2/25 | Loss: 0.00144879
Iteration 3/25 | Loss: 0.00128951
Iteration 4/25 | Loss: 0.00125916
Iteration 5/25 | Loss: 0.00125023
Iteration 6/25 | Loss: 0.00124737
Iteration 7/25 | Loss: 0.00124677
Iteration 8/25 | Loss: 0.00124677
Iteration 9/25 | Loss: 0.00124677
Iteration 10/25 | Loss: 0.00124677
Iteration 11/25 | Loss: 0.00124677
Iteration 12/25 | Loss: 0.00124677
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012467652559280396, 0.0012467652559280396, 0.0012467652559280396, 0.0012467652559280396, 0.0012467652559280396]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012467652559280396

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41281199
Iteration 2/25 | Loss: 0.00074675
Iteration 3/25 | Loss: 0.00074675
Iteration 4/25 | Loss: 0.00074675
Iteration 5/25 | Loss: 0.00074674
Iteration 6/25 | Loss: 0.00074674
Iteration 7/25 | Loss: 0.00074674
Iteration 8/25 | Loss: 0.00074674
Iteration 9/25 | Loss: 0.00074674
Iteration 10/25 | Loss: 0.00074674
Iteration 11/25 | Loss: 0.00074674
Iteration 12/25 | Loss: 0.00074674
Iteration 13/25 | Loss: 0.00074674
Iteration 14/25 | Loss: 0.00074674
Iteration 15/25 | Loss: 0.00074674
Iteration 16/25 | Loss: 0.00074674
Iteration 17/25 | Loss: 0.00074674
Iteration 18/25 | Loss: 0.00074674
Iteration 19/25 | Loss: 0.00074674
Iteration 20/25 | Loss: 0.00074674
Iteration 21/25 | Loss: 0.00074674
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007467434043064713, 0.0007467434043064713, 0.0007467434043064713, 0.0007467434043064713, 0.0007467434043064713]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007467434043064713

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074674
Iteration 2/1000 | Loss: 0.00006061
Iteration 3/1000 | Loss: 0.00004529
Iteration 4/1000 | Loss: 0.00003818
Iteration 5/1000 | Loss: 0.00003597
Iteration 6/1000 | Loss: 0.00003479
Iteration 7/1000 | Loss: 0.00003387
Iteration 8/1000 | Loss: 0.00003304
Iteration 9/1000 | Loss: 0.00003251
Iteration 10/1000 | Loss: 0.00003219
Iteration 11/1000 | Loss: 0.00003197
Iteration 12/1000 | Loss: 0.00003185
Iteration 13/1000 | Loss: 0.00003168
Iteration 14/1000 | Loss: 0.00003164
Iteration 15/1000 | Loss: 0.00003157
Iteration 16/1000 | Loss: 0.00003157
Iteration 17/1000 | Loss: 0.00003156
Iteration 18/1000 | Loss: 0.00003155
Iteration 19/1000 | Loss: 0.00003152
Iteration 20/1000 | Loss: 0.00003151
Iteration 21/1000 | Loss: 0.00003150
Iteration 22/1000 | Loss: 0.00003146
Iteration 23/1000 | Loss: 0.00003146
Iteration 24/1000 | Loss: 0.00003145
Iteration 25/1000 | Loss: 0.00003145
Iteration 26/1000 | Loss: 0.00003143
Iteration 27/1000 | Loss: 0.00003142
Iteration 28/1000 | Loss: 0.00003141
Iteration 29/1000 | Loss: 0.00003140
Iteration 30/1000 | Loss: 0.00003140
Iteration 31/1000 | Loss: 0.00003140
Iteration 32/1000 | Loss: 0.00003139
Iteration 33/1000 | Loss: 0.00003137
Iteration 34/1000 | Loss: 0.00003137
Iteration 35/1000 | Loss: 0.00003137
Iteration 36/1000 | Loss: 0.00003136
Iteration 37/1000 | Loss: 0.00003136
Iteration 38/1000 | Loss: 0.00003136
Iteration 39/1000 | Loss: 0.00003135
Iteration 40/1000 | Loss: 0.00003135
Iteration 41/1000 | Loss: 0.00003135
Iteration 42/1000 | Loss: 0.00003135
Iteration 43/1000 | Loss: 0.00003135
Iteration 44/1000 | Loss: 0.00003135
Iteration 45/1000 | Loss: 0.00003135
Iteration 46/1000 | Loss: 0.00003135
Iteration 47/1000 | Loss: 0.00003135
Iteration 48/1000 | Loss: 0.00003135
Iteration 49/1000 | Loss: 0.00003135
Iteration 50/1000 | Loss: 0.00003133
Iteration 51/1000 | Loss: 0.00003133
Iteration 52/1000 | Loss: 0.00003133
Iteration 53/1000 | Loss: 0.00003132
Iteration 54/1000 | Loss: 0.00003132
Iteration 55/1000 | Loss: 0.00003132
Iteration 56/1000 | Loss: 0.00003132
Iteration 57/1000 | Loss: 0.00003131
Iteration 58/1000 | Loss: 0.00003131
Iteration 59/1000 | Loss: 0.00003131
Iteration 60/1000 | Loss: 0.00003131
Iteration 61/1000 | Loss: 0.00003131
Iteration 62/1000 | Loss: 0.00003131
Iteration 63/1000 | Loss: 0.00003130
Iteration 64/1000 | Loss: 0.00003130
Iteration 65/1000 | Loss: 0.00003130
Iteration 66/1000 | Loss: 0.00003130
Iteration 67/1000 | Loss: 0.00003129
Iteration 68/1000 | Loss: 0.00003129
Iteration 69/1000 | Loss: 0.00003129
Iteration 70/1000 | Loss: 0.00003128
Iteration 71/1000 | Loss: 0.00003128
Iteration 72/1000 | Loss: 0.00003128
Iteration 73/1000 | Loss: 0.00003128
Iteration 74/1000 | Loss: 0.00003127
Iteration 75/1000 | Loss: 0.00003127
Iteration 76/1000 | Loss: 0.00003126
Iteration 77/1000 | Loss: 0.00003126
Iteration 78/1000 | Loss: 0.00003126
Iteration 79/1000 | Loss: 0.00003126
Iteration 80/1000 | Loss: 0.00003125
Iteration 81/1000 | Loss: 0.00003125
Iteration 82/1000 | Loss: 0.00003125
Iteration 83/1000 | Loss: 0.00003125
Iteration 84/1000 | Loss: 0.00003125
Iteration 85/1000 | Loss: 0.00003125
Iteration 86/1000 | Loss: 0.00003125
Iteration 87/1000 | Loss: 0.00003125
Iteration 88/1000 | Loss: 0.00003125
Iteration 89/1000 | Loss: 0.00003125
Iteration 90/1000 | Loss: 0.00003125
Iteration 91/1000 | Loss: 0.00003125
Iteration 92/1000 | Loss: 0.00003125
Iteration 93/1000 | Loss: 0.00003125
Iteration 94/1000 | Loss: 0.00003125
Iteration 95/1000 | Loss: 0.00003124
Iteration 96/1000 | Loss: 0.00003124
Iteration 97/1000 | Loss: 0.00003124
Iteration 98/1000 | Loss: 0.00003123
Iteration 99/1000 | Loss: 0.00003123
Iteration 100/1000 | Loss: 0.00003123
Iteration 101/1000 | Loss: 0.00003123
Iteration 102/1000 | Loss: 0.00003123
Iteration 103/1000 | Loss: 0.00003122
Iteration 104/1000 | Loss: 0.00003122
Iteration 105/1000 | Loss: 0.00003122
Iteration 106/1000 | Loss: 0.00003122
Iteration 107/1000 | Loss: 0.00003122
Iteration 108/1000 | Loss: 0.00003122
Iteration 109/1000 | Loss: 0.00003122
Iteration 110/1000 | Loss: 0.00003121
Iteration 111/1000 | Loss: 0.00003121
Iteration 112/1000 | Loss: 0.00003121
Iteration 113/1000 | Loss: 0.00003121
Iteration 114/1000 | Loss: 0.00003120
Iteration 115/1000 | Loss: 0.00003120
Iteration 116/1000 | Loss: 0.00003120
Iteration 117/1000 | Loss: 0.00003120
Iteration 118/1000 | Loss: 0.00003119
Iteration 119/1000 | Loss: 0.00003119
Iteration 120/1000 | Loss: 0.00003119
Iteration 121/1000 | Loss: 0.00003119
Iteration 122/1000 | Loss: 0.00003119
Iteration 123/1000 | Loss: 0.00003119
Iteration 124/1000 | Loss: 0.00003118
Iteration 125/1000 | Loss: 0.00003118
Iteration 126/1000 | Loss: 0.00003118
Iteration 127/1000 | Loss: 0.00003118
Iteration 128/1000 | Loss: 0.00003118
Iteration 129/1000 | Loss: 0.00003118
Iteration 130/1000 | Loss: 0.00003118
Iteration 131/1000 | Loss: 0.00003118
Iteration 132/1000 | Loss: 0.00003118
Iteration 133/1000 | Loss: 0.00003118
Iteration 134/1000 | Loss: 0.00003118
Iteration 135/1000 | Loss: 0.00003118
Iteration 136/1000 | Loss: 0.00003117
Iteration 137/1000 | Loss: 0.00003117
Iteration 138/1000 | Loss: 0.00003117
Iteration 139/1000 | Loss: 0.00003117
Iteration 140/1000 | Loss: 0.00003117
Iteration 141/1000 | Loss: 0.00003117
Iteration 142/1000 | Loss: 0.00003117
Iteration 143/1000 | Loss: 0.00003117
Iteration 144/1000 | Loss: 0.00003117
Iteration 145/1000 | Loss: 0.00003117
Iteration 146/1000 | Loss: 0.00003117
Iteration 147/1000 | Loss: 0.00003117
Iteration 148/1000 | Loss: 0.00003116
Iteration 149/1000 | Loss: 0.00003116
Iteration 150/1000 | Loss: 0.00003116
Iteration 151/1000 | Loss: 0.00003116
Iteration 152/1000 | Loss: 0.00003116
Iteration 153/1000 | Loss: 0.00003116
Iteration 154/1000 | Loss: 0.00003116
Iteration 155/1000 | Loss: 0.00003116
Iteration 156/1000 | Loss: 0.00003116
Iteration 157/1000 | Loss: 0.00003115
Iteration 158/1000 | Loss: 0.00003115
Iteration 159/1000 | Loss: 0.00003115
Iteration 160/1000 | Loss: 0.00003115
Iteration 161/1000 | Loss: 0.00003115
Iteration 162/1000 | Loss: 0.00003115
Iteration 163/1000 | Loss: 0.00003115
Iteration 164/1000 | Loss: 0.00003115
Iteration 165/1000 | Loss: 0.00003115
Iteration 166/1000 | Loss: 0.00003115
Iteration 167/1000 | Loss: 0.00003115
Iteration 168/1000 | Loss: 0.00003115
Iteration 169/1000 | Loss: 0.00003115
Iteration 170/1000 | Loss: 0.00003115
Iteration 171/1000 | Loss: 0.00003115
Iteration 172/1000 | Loss: 0.00003114
Iteration 173/1000 | Loss: 0.00003114
Iteration 174/1000 | Loss: 0.00003114
Iteration 175/1000 | Loss: 0.00003114
Iteration 176/1000 | Loss: 0.00003114
Iteration 177/1000 | Loss: 0.00003114
Iteration 178/1000 | Loss: 0.00003114
Iteration 179/1000 | Loss: 0.00003114
Iteration 180/1000 | Loss: 0.00003114
Iteration 181/1000 | Loss: 0.00003114
Iteration 182/1000 | Loss: 0.00003114
Iteration 183/1000 | Loss: 0.00003114
Iteration 184/1000 | Loss: 0.00003114
Iteration 185/1000 | Loss: 0.00003114
Iteration 186/1000 | Loss: 0.00003114
Iteration 187/1000 | Loss: 0.00003114
Iteration 188/1000 | Loss: 0.00003114
Iteration 189/1000 | Loss: 0.00003114
Iteration 190/1000 | Loss: 0.00003114
Iteration 191/1000 | Loss: 0.00003114
Iteration 192/1000 | Loss: 0.00003114
Iteration 193/1000 | Loss: 0.00003114
Iteration 194/1000 | Loss: 0.00003114
Iteration 195/1000 | Loss: 0.00003114
Iteration 196/1000 | Loss: 0.00003114
Iteration 197/1000 | Loss: 0.00003114
Iteration 198/1000 | Loss: 0.00003114
Iteration 199/1000 | Loss: 0.00003114
Iteration 200/1000 | Loss: 0.00003114
Iteration 201/1000 | Loss: 0.00003114
Iteration 202/1000 | Loss: 0.00003114
Iteration 203/1000 | Loss: 0.00003114
Iteration 204/1000 | Loss: 0.00003114
Iteration 205/1000 | Loss: 0.00003114
Iteration 206/1000 | Loss: 0.00003114
Iteration 207/1000 | Loss: 0.00003114
Iteration 208/1000 | Loss: 0.00003114
Iteration 209/1000 | Loss: 0.00003114
Iteration 210/1000 | Loss: 0.00003114
Iteration 211/1000 | Loss: 0.00003114
Iteration 212/1000 | Loss: 0.00003114
Iteration 213/1000 | Loss: 0.00003114
Iteration 214/1000 | Loss: 0.00003114
Iteration 215/1000 | Loss: 0.00003114
Iteration 216/1000 | Loss: 0.00003114
Iteration 217/1000 | Loss: 0.00003114
Iteration 218/1000 | Loss: 0.00003114
Iteration 219/1000 | Loss: 0.00003114
Iteration 220/1000 | Loss: 0.00003114
Iteration 221/1000 | Loss: 0.00003114
Iteration 222/1000 | Loss: 0.00003114
Iteration 223/1000 | Loss: 0.00003114
Iteration 224/1000 | Loss: 0.00003114
Iteration 225/1000 | Loss: 0.00003114
Iteration 226/1000 | Loss: 0.00003114
Iteration 227/1000 | Loss: 0.00003114
Iteration 228/1000 | Loss: 0.00003114
Iteration 229/1000 | Loss: 0.00003114
Iteration 230/1000 | Loss: 0.00003114
Iteration 231/1000 | Loss: 0.00003114
Iteration 232/1000 | Loss: 0.00003114
Iteration 233/1000 | Loss: 0.00003114
Iteration 234/1000 | Loss: 0.00003114
Iteration 235/1000 | Loss: 0.00003114
Iteration 236/1000 | Loss: 0.00003114
Iteration 237/1000 | Loss: 0.00003114
Iteration 238/1000 | Loss: 0.00003114
Iteration 239/1000 | Loss: 0.00003114
Iteration 240/1000 | Loss: 0.00003114
Iteration 241/1000 | Loss: 0.00003114
Iteration 242/1000 | Loss: 0.00003114
Iteration 243/1000 | Loss: 0.00003114
Iteration 244/1000 | Loss: 0.00003114
Iteration 245/1000 | Loss: 0.00003114
Iteration 246/1000 | Loss: 0.00003114
Iteration 247/1000 | Loss: 0.00003114
Iteration 248/1000 | Loss: 0.00003114
Iteration 249/1000 | Loss: 0.00003114
Iteration 250/1000 | Loss: 0.00003114
Iteration 251/1000 | Loss: 0.00003114
Iteration 252/1000 | Loss: 0.00003114
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 252. Stopping optimization.
Last 5 losses: [3.114341598120518e-05, 3.114341598120518e-05, 3.114341598120518e-05, 3.114341598120518e-05, 3.114341598120518e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.114341598120518e-05

Optimization complete. Final v2v error: 4.779641628265381 mm

Highest mean error: 5.261679172515869 mm for frame 94

Lowest mean error: 4.215782165527344 mm for frame 0

Saving results

Total time: 43.004201889038086
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_nl_6355/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6355/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6355/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00614285
Iteration 2/25 | Loss: 0.00135817
Iteration 3/25 | Loss: 0.00127117
Iteration 4/25 | Loss: 0.00124865
Iteration 5/25 | Loss: 0.00124066
Iteration 6/25 | Loss: 0.00123959
Iteration 7/25 | Loss: 0.00123950
Iteration 8/25 | Loss: 0.00123949
Iteration 9/25 | Loss: 0.00123949
Iteration 10/25 | Loss: 0.00123950
Iteration 11/25 | Loss: 0.00123950
Iteration 12/25 | Loss: 0.00123950
Iteration 13/25 | Loss: 0.00123950
Iteration 14/25 | Loss: 0.00123950
Iteration 15/25 | Loss: 0.00123950
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012394950026646256, 0.0012394950026646256, 0.0012394950026646256, 0.0012394950026646256, 0.0012394950026646256]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012394950026646256

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37382340
Iteration 2/25 | Loss: 0.00079132
Iteration 3/25 | Loss: 0.00079128
Iteration 4/25 | Loss: 0.00079128
Iteration 5/25 | Loss: 0.00079128
Iteration 6/25 | Loss: 0.00079128
Iteration 7/25 | Loss: 0.00079128
Iteration 8/25 | Loss: 0.00079128
Iteration 9/25 | Loss: 0.00079128
Iteration 10/25 | Loss: 0.00079128
Iteration 11/25 | Loss: 0.00079128
Iteration 12/25 | Loss: 0.00079128
Iteration 13/25 | Loss: 0.00079128
Iteration 14/25 | Loss: 0.00079128
Iteration 15/25 | Loss: 0.00079128
Iteration 16/25 | Loss: 0.00079128
Iteration 17/25 | Loss: 0.00079128
Iteration 18/25 | Loss: 0.00079128
Iteration 19/25 | Loss: 0.00079128
Iteration 20/25 | Loss: 0.00079128
Iteration 21/25 | Loss: 0.00079128
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007912776782177389, 0.0007912776782177389, 0.0007912776782177389, 0.0007912776782177389, 0.0007912776782177389]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007912776782177389

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079128
Iteration 2/1000 | Loss: 0.00004199
Iteration 3/1000 | Loss: 0.00003501
Iteration 4/1000 | Loss: 0.00003351
Iteration 5/1000 | Loss: 0.00003258
Iteration 6/1000 | Loss: 0.00003178
Iteration 7/1000 | Loss: 0.00003111
Iteration 8/1000 | Loss: 0.00003070
Iteration 9/1000 | Loss: 0.00003052
Iteration 10/1000 | Loss: 0.00003037
Iteration 11/1000 | Loss: 0.00003032
Iteration 12/1000 | Loss: 0.00003026
Iteration 13/1000 | Loss: 0.00003016
Iteration 14/1000 | Loss: 0.00003015
Iteration 15/1000 | Loss: 0.00003014
Iteration 16/1000 | Loss: 0.00003010
Iteration 17/1000 | Loss: 0.00003010
Iteration 18/1000 | Loss: 0.00003009
Iteration 19/1000 | Loss: 0.00003008
Iteration 20/1000 | Loss: 0.00003008
Iteration 21/1000 | Loss: 0.00003007
Iteration 22/1000 | Loss: 0.00003006
Iteration 23/1000 | Loss: 0.00003005
Iteration 24/1000 | Loss: 0.00003005
Iteration 25/1000 | Loss: 0.00003005
Iteration 26/1000 | Loss: 0.00003004
Iteration 27/1000 | Loss: 0.00003004
Iteration 28/1000 | Loss: 0.00003002
Iteration 29/1000 | Loss: 0.00003002
Iteration 30/1000 | Loss: 0.00003001
Iteration 31/1000 | Loss: 0.00003001
Iteration 32/1000 | Loss: 0.00003000
Iteration 33/1000 | Loss: 0.00003000
Iteration 34/1000 | Loss: 0.00002999
Iteration 35/1000 | Loss: 0.00002998
Iteration 36/1000 | Loss: 0.00002997
Iteration 37/1000 | Loss: 0.00002997
Iteration 38/1000 | Loss: 0.00002996
Iteration 39/1000 | Loss: 0.00002996
Iteration 40/1000 | Loss: 0.00002996
Iteration 41/1000 | Loss: 0.00002996
Iteration 42/1000 | Loss: 0.00002996
Iteration 43/1000 | Loss: 0.00002996
Iteration 44/1000 | Loss: 0.00002996
Iteration 45/1000 | Loss: 0.00002995
Iteration 46/1000 | Loss: 0.00002995
Iteration 47/1000 | Loss: 0.00002995
Iteration 48/1000 | Loss: 0.00002995
Iteration 49/1000 | Loss: 0.00002994
Iteration 50/1000 | Loss: 0.00002994
Iteration 51/1000 | Loss: 0.00002994
Iteration 52/1000 | Loss: 0.00002993
Iteration 53/1000 | Loss: 0.00002993
Iteration 54/1000 | Loss: 0.00002993
Iteration 55/1000 | Loss: 0.00002993
Iteration 56/1000 | Loss: 0.00002993
Iteration 57/1000 | Loss: 0.00002993
Iteration 58/1000 | Loss: 0.00002992
Iteration 59/1000 | Loss: 0.00002992
Iteration 60/1000 | Loss: 0.00002992
Iteration 61/1000 | Loss: 0.00002992
Iteration 62/1000 | Loss: 0.00002992
Iteration 63/1000 | Loss: 0.00002991
Iteration 64/1000 | Loss: 0.00002991
Iteration 65/1000 | Loss: 0.00002991
Iteration 66/1000 | Loss: 0.00002991
Iteration 67/1000 | Loss: 0.00002991
Iteration 68/1000 | Loss: 0.00002990
Iteration 69/1000 | Loss: 0.00002990
Iteration 70/1000 | Loss: 0.00002990
Iteration 71/1000 | Loss: 0.00002990
Iteration 72/1000 | Loss: 0.00002990
Iteration 73/1000 | Loss: 0.00002990
Iteration 74/1000 | Loss: 0.00002990
Iteration 75/1000 | Loss: 0.00002989
Iteration 76/1000 | Loss: 0.00002989
Iteration 77/1000 | Loss: 0.00002989
Iteration 78/1000 | Loss: 0.00002989
Iteration 79/1000 | Loss: 0.00002989
Iteration 80/1000 | Loss: 0.00002989
Iteration 81/1000 | Loss: 0.00002989
Iteration 82/1000 | Loss: 0.00002989
Iteration 83/1000 | Loss: 0.00002989
Iteration 84/1000 | Loss: 0.00002988
Iteration 85/1000 | Loss: 0.00002988
Iteration 86/1000 | Loss: 0.00002988
Iteration 87/1000 | Loss: 0.00002988
Iteration 88/1000 | Loss: 0.00002988
Iteration 89/1000 | Loss: 0.00002988
Iteration 90/1000 | Loss: 0.00002988
Iteration 91/1000 | Loss: 0.00002988
Iteration 92/1000 | Loss: 0.00002988
Iteration 93/1000 | Loss: 0.00002988
Iteration 94/1000 | Loss: 0.00002988
Iteration 95/1000 | Loss: 0.00002988
Iteration 96/1000 | Loss: 0.00002988
Iteration 97/1000 | Loss: 0.00002988
Iteration 98/1000 | Loss: 0.00002988
Iteration 99/1000 | Loss: 0.00002988
Iteration 100/1000 | Loss: 0.00002988
Iteration 101/1000 | Loss: 0.00002988
Iteration 102/1000 | Loss: 0.00002988
Iteration 103/1000 | Loss: 0.00002988
Iteration 104/1000 | Loss: 0.00002988
Iteration 105/1000 | Loss: 0.00002988
Iteration 106/1000 | Loss: 0.00002988
Iteration 107/1000 | Loss: 0.00002988
Iteration 108/1000 | Loss: 0.00002988
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [2.9881628506700508e-05, 2.9881628506700508e-05, 2.9881628506700508e-05, 2.9881628506700508e-05, 2.9881628506700508e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9881628506700508e-05

Optimization complete. Final v2v error: 4.77618408203125 mm

Highest mean error: 5.308921813964844 mm for frame 117

Lowest mean error: 4.156213760375977 mm for frame 136

Saving results

Total time: 32.184999227523804
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_nl_6355/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6355/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6355/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00960835
Iteration 2/25 | Loss: 0.00189958
Iteration 3/25 | Loss: 0.00141880
Iteration 4/25 | Loss: 0.00133720
Iteration 5/25 | Loss: 0.00133063
Iteration 6/25 | Loss: 0.00127402
Iteration 7/25 | Loss: 0.00126198
Iteration 8/25 | Loss: 0.00126151
Iteration 9/25 | Loss: 0.00122412
Iteration 10/25 | Loss: 0.00122570
Iteration 11/25 | Loss: 0.00120844
Iteration 12/25 | Loss: 0.00120161
Iteration 13/25 | Loss: 0.00119740
Iteration 14/25 | Loss: 0.00119532
Iteration 15/25 | Loss: 0.00118857
Iteration 16/25 | Loss: 0.00119227
Iteration 17/25 | Loss: 0.00119196
Iteration 18/25 | Loss: 0.00119217
Iteration 19/25 | Loss: 0.00119095
Iteration 20/25 | Loss: 0.00119346
Iteration 21/25 | Loss: 0.00119089
Iteration 22/25 | Loss: 0.00118124
Iteration 23/25 | Loss: 0.00119368
Iteration 24/25 | Loss: 0.00119466
Iteration 25/25 | Loss: 0.00117911

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.55080366
Iteration 2/25 | Loss: 0.00082510
Iteration 3/25 | Loss: 0.00082510
Iteration 4/25 | Loss: 0.00080793
Iteration 5/25 | Loss: 0.00080793
Iteration 6/25 | Loss: 0.00080793
Iteration 7/25 | Loss: 0.00080793
Iteration 8/25 | Loss: 0.00080793
Iteration 9/25 | Loss: 0.00080793
Iteration 10/25 | Loss: 0.00080793
Iteration 11/25 | Loss: 0.00080793
Iteration 12/25 | Loss: 0.00080793
Iteration 13/25 | Loss: 0.00080793
Iteration 14/25 | Loss: 0.00080793
Iteration 15/25 | Loss: 0.00080793
Iteration 16/25 | Loss: 0.00080793
Iteration 17/25 | Loss: 0.00080793
Iteration 18/25 | Loss: 0.00080793
Iteration 19/25 | Loss: 0.00080793
Iteration 20/25 | Loss: 0.00080793
Iteration 21/25 | Loss: 0.00080793
Iteration 22/25 | Loss: 0.00080793
Iteration 23/25 | Loss: 0.00080793
Iteration 24/25 | Loss: 0.00080793
Iteration 25/25 | Loss: 0.00080793

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080793
Iteration 2/1000 | Loss: 0.00007169
Iteration 3/1000 | Loss: 0.00006537
Iteration 4/1000 | Loss: 0.00006302
Iteration 5/1000 | Loss: 0.00055873
Iteration 6/1000 | Loss: 0.00012765
Iteration 7/1000 | Loss: 0.00005816
Iteration 8/1000 | Loss: 0.00007088
Iteration 9/1000 | Loss: 0.00006321
Iteration 10/1000 | Loss: 0.00007211
Iteration 11/1000 | Loss: 0.00008148
Iteration 12/1000 | Loss: 0.00007532
Iteration 13/1000 | Loss: 0.00006033
Iteration 14/1000 | Loss: 0.00007527
Iteration 15/1000 | Loss: 0.00007531
Iteration 16/1000 | Loss: 0.00007296
Iteration 17/1000 | Loss: 0.00007974
Iteration 18/1000 | Loss: 0.00054268
Iteration 19/1000 | Loss: 0.00007969
Iteration 20/1000 | Loss: 0.00006743
Iteration 21/1000 | Loss: 0.00008242
Iteration 22/1000 | Loss: 0.00006564
Iteration 23/1000 | Loss: 0.00007590
Iteration 24/1000 | Loss: 0.00006288
Iteration 25/1000 | Loss: 0.00006101
Iteration 26/1000 | Loss: 0.00005067
Iteration 27/1000 | Loss: 0.00006685
Iteration 28/1000 | Loss: 0.00004588
Iteration 29/1000 | Loss: 0.00005224
Iteration 30/1000 | Loss: 0.00004937
Iteration 31/1000 | Loss: 0.00005276
Iteration 32/1000 | Loss: 0.00006178
Iteration 33/1000 | Loss: 0.00006655
Iteration 34/1000 | Loss: 0.00011684
Iteration 35/1000 | Loss: 0.00011132
Iteration 36/1000 | Loss: 0.00005193
Iteration 37/1000 | Loss: 0.00014370
Iteration 38/1000 | Loss: 0.00006401
Iteration 39/1000 | Loss: 0.00007443
Iteration 40/1000 | Loss: 0.00005690
Iteration 41/1000 | Loss: 0.00008520
Iteration 42/1000 | Loss: 0.00008922
Iteration 43/1000 | Loss: 0.00008598
Iteration 44/1000 | Loss: 0.00006761
Iteration 45/1000 | Loss: 0.00006493
Iteration 46/1000 | Loss: 0.00006584
Iteration 47/1000 | Loss: 0.00003612
Iteration 48/1000 | Loss: 0.00008557
Iteration 49/1000 | Loss: 0.00007183
Iteration 50/1000 | Loss: 0.00006502
Iteration 51/1000 | Loss: 0.00005142
Iteration 52/1000 | Loss: 0.00014668
Iteration 53/1000 | Loss: 0.00005169
Iteration 54/1000 | Loss: 0.00016775
Iteration 55/1000 | Loss: 0.00015911
Iteration 56/1000 | Loss: 0.00005589
Iteration 57/1000 | Loss: 0.00005691
Iteration 58/1000 | Loss: 0.00006108
Iteration 59/1000 | Loss: 0.00003758
Iteration 60/1000 | Loss: 0.00002674
Iteration 61/1000 | Loss: 0.00002435
Iteration 62/1000 | Loss: 0.00002364
Iteration 63/1000 | Loss: 0.00002322
Iteration 64/1000 | Loss: 0.00002288
Iteration 65/1000 | Loss: 0.00002280
Iteration 66/1000 | Loss: 0.00002276
Iteration 67/1000 | Loss: 0.00002276
Iteration 68/1000 | Loss: 0.00002253
Iteration 69/1000 | Loss: 0.00002252
Iteration 70/1000 | Loss: 0.00002248
Iteration 71/1000 | Loss: 0.00002247
Iteration 72/1000 | Loss: 0.00002246
Iteration 73/1000 | Loss: 0.00002244
Iteration 74/1000 | Loss: 0.00002236
Iteration 75/1000 | Loss: 0.00002230
Iteration 76/1000 | Loss: 0.00002223
Iteration 77/1000 | Loss: 0.00002220
Iteration 78/1000 | Loss: 0.00002220
Iteration 79/1000 | Loss: 0.00002219
Iteration 80/1000 | Loss: 0.00002219
Iteration 81/1000 | Loss: 0.00002219
Iteration 82/1000 | Loss: 0.00002218
Iteration 83/1000 | Loss: 0.00002218
Iteration 84/1000 | Loss: 0.00002217
Iteration 85/1000 | Loss: 0.00002217
Iteration 86/1000 | Loss: 0.00002216
Iteration 87/1000 | Loss: 0.00002215
Iteration 88/1000 | Loss: 0.00002215
Iteration 89/1000 | Loss: 0.00002215
Iteration 90/1000 | Loss: 0.00002215
Iteration 91/1000 | Loss: 0.00002215
Iteration 92/1000 | Loss: 0.00002215
Iteration 93/1000 | Loss: 0.00002215
Iteration 94/1000 | Loss: 0.00002215
Iteration 95/1000 | Loss: 0.00002214
Iteration 96/1000 | Loss: 0.00002214
Iteration 97/1000 | Loss: 0.00002214
Iteration 98/1000 | Loss: 0.00002211
Iteration 99/1000 | Loss: 0.00002211
Iteration 100/1000 | Loss: 0.00002211
Iteration 101/1000 | Loss: 0.00002210
Iteration 102/1000 | Loss: 0.00002210
Iteration 103/1000 | Loss: 0.00002210
Iteration 104/1000 | Loss: 0.00002209
Iteration 105/1000 | Loss: 0.00002209
Iteration 106/1000 | Loss: 0.00002209
Iteration 107/1000 | Loss: 0.00002209
Iteration 108/1000 | Loss: 0.00002209
Iteration 109/1000 | Loss: 0.00002209
Iteration 110/1000 | Loss: 0.00002209
Iteration 111/1000 | Loss: 0.00002209
Iteration 112/1000 | Loss: 0.00002209
Iteration 113/1000 | Loss: 0.00002209
Iteration 114/1000 | Loss: 0.00002209
Iteration 115/1000 | Loss: 0.00002209
Iteration 116/1000 | Loss: 0.00002209
Iteration 117/1000 | Loss: 0.00002209
Iteration 118/1000 | Loss: 0.00002208
Iteration 119/1000 | Loss: 0.00002208
Iteration 120/1000 | Loss: 0.00002208
Iteration 121/1000 | Loss: 0.00002208
Iteration 122/1000 | Loss: 0.00002208
Iteration 123/1000 | Loss: 0.00002208
Iteration 124/1000 | Loss: 0.00002208
Iteration 125/1000 | Loss: 0.00002208
Iteration 126/1000 | Loss: 0.00002208
Iteration 127/1000 | Loss: 0.00002208
Iteration 128/1000 | Loss: 0.00002207
Iteration 129/1000 | Loss: 0.00002207
Iteration 130/1000 | Loss: 0.00002207
Iteration 131/1000 | Loss: 0.00002207
Iteration 132/1000 | Loss: 0.00002207
Iteration 133/1000 | Loss: 0.00002206
Iteration 134/1000 | Loss: 0.00002206
Iteration 135/1000 | Loss: 0.00002205
Iteration 136/1000 | Loss: 0.00002205
Iteration 137/1000 | Loss: 0.00002205
Iteration 138/1000 | Loss: 0.00002205
Iteration 139/1000 | Loss: 0.00002205
Iteration 140/1000 | Loss: 0.00002205
Iteration 141/1000 | Loss: 0.00002205
Iteration 142/1000 | Loss: 0.00002205
Iteration 143/1000 | Loss: 0.00002204
Iteration 144/1000 | Loss: 0.00002204
Iteration 145/1000 | Loss: 0.00002204
Iteration 146/1000 | Loss: 0.00002204
Iteration 147/1000 | Loss: 0.00002204
Iteration 148/1000 | Loss: 0.00002204
Iteration 149/1000 | Loss: 0.00002203
Iteration 150/1000 | Loss: 0.00002203
Iteration 151/1000 | Loss: 0.00002203
Iteration 152/1000 | Loss: 0.00002202
Iteration 153/1000 | Loss: 0.00002202
Iteration 154/1000 | Loss: 0.00002201
Iteration 155/1000 | Loss: 0.00002201
Iteration 156/1000 | Loss: 0.00002201
Iteration 157/1000 | Loss: 0.00002201
Iteration 158/1000 | Loss: 0.00002201
Iteration 159/1000 | Loss: 0.00002201
Iteration 160/1000 | Loss: 0.00002201
Iteration 161/1000 | Loss: 0.00002201
Iteration 162/1000 | Loss: 0.00002200
Iteration 163/1000 | Loss: 0.00002199
Iteration 164/1000 | Loss: 0.00002199
Iteration 165/1000 | Loss: 0.00002198
Iteration 166/1000 | Loss: 0.00002198
Iteration 167/1000 | Loss: 0.00002198
Iteration 168/1000 | Loss: 0.00002198
Iteration 169/1000 | Loss: 0.00002198
Iteration 170/1000 | Loss: 0.00002198
Iteration 171/1000 | Loss: 0.00002198
Iteration 172/1000 | Loss: 0.00002198
Iteration 173/1000 | Loss: 0.00002197
Iteration 174/1000 | Loss: 0.00002197
Iteration 175/1000 | Loss: 0.00002197
Iteration 176/1000 | Loss: 0.00002197
Iteration 177/1000 | Loss: 0.00002197
Iteration 178/1000 | Loss: 0.00002197
Iteration 179/1000 | Loss: 0.00002197
Iteration 180/1000 | Loss: 0.00002197
Iteration 181/1000 | Loss: 0.00002197
Iteration 182/1000 | Loss: 0.00002197
Iteration 183/1000 | Loss: 0.00002197
Iteration 184/1000 | Loss: 0.00002197
Iteration 185/1000 | Loss: 0.00002197
Iteration 186/1000 | Loss: 0.00002197
Iteration 187/1000 | Loss: 0.00002197
Iteration 188/1000 | Loss: 0.00002197
Iteration 189/1000 | Loss: 0.00002197
Iteration 190/1000 | Loss: 0.00002197
Iteration 191/1000 | Loss: 0.00002197
Iteration 192/1000 | Loss: 0.00002197
Iteration 193/1000 | Loss: 0.00002197
Iteration 194/1000 | Loss: 0.00002197
Iteration 195/1000 | Loss: 0.00002197
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [2.1969488443573937e-05, 2.1969488443573937e-05, 2.1969488443573937e-05, 2.1969488443573937e-05, 2.1969488443573937e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1969488443573937e-05

Optimization complete. Final v2v error: 4.02174186706543 mm

Highest mean error: 4.788796424865723 mm for frame 171

Lowest mean error: 3.4953999519348145 mm for frame 51

Saving results

Total time: 157.2562220096588
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_nl_6355/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6355/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6355/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00586102
Iteration 2/25 | Loss: 0.00152126
Iteration 3/25 | Loss: 0.00132436
Iteration 4/25 | Loss: 0.00127423
Iteration 5/25 | Loss: 0.00125452
Iteration 6/25 | Loss: 0.00125545
Iteration 7/25 | Loss: 0.00123994
Iteration 8/25 | Loss: 0.00123306
Iteration 9/25 | Loss: 0.00122796
Iteration 10/25 | Loss: 0.00122223
Iteration 11/25 | Loss: 0.00122424
Iteration 12/25 | Loss: 0.00122211
Iteration 13/25 | Loss: 0.00122254
Iteration 14/25 | Loss: 0.00122418
Iteration 15/25 | Loss: 0.00122414
Iteration 16/25 | Loss: 0.00122491
Iteration 17/25 | Loss: 0.00122074
Iteration 18/25 | Loss: 0.00121906
Iteration 19/25 | Loss: 0.00122012
Iteration 20/25 | Loss: 0.00121996
Iteration 21/25 | Loss: 0.00121454
Iteration 22/25 | Loss: 0.00121601
Iteration 23/25 | Loss: 0.00121328
Iteration 24/25 | Loss: 0.00121467
Iteration 25/25 | Loss: 0.00121452

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46215951
Iteration 2/25 | Loss: 0.00067164
Iteration 3/25 | Loss: 0.00067163
Iteration 4/25 | Loss: 0.00067163
Iteration 5/25 | Loss: 0.00067163
Iteration 6/25 | Loss: 0.00067163
Iteration 7/25 | Loss: 0.00067163
Iteration 8/25 | Loss: 0.00067163
Iteration 9/25 | Loss: 0.00067163
Iteration 10/25 | Loss: 0.00067163
Iteration 11/25 | Loss: 0.00067163
Iteration 12/25 | Loss: 0.00067163
Iteration 13/25 | Loss: 0.00067163
Iteration 14/25 | Loss: 0.00067163
Iteration 15/25 | Loss: 0.00067163
Iteration 16/25 | Loss: 0.00067163
Iteration 17/25 | Loss: 0.00067163
Iteration 18/25 | Loss: 0.00067163
Iteration 19/25 | Loss: 0.00067163
Iteration 20/25 | Loss: 0.00067163
Iteration 21/25 | Loss: 0.00067163
Iteration 22/25 | Loss: 0.00067163
Iteration 23/25 | Loss: 0.00067163
Iteration 24/25 | Loss: 0.00067163
Iteration 25/25 | Loss: 0.00067163

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067163
Iteration 2/1000 | Loss: 0.00021644
Iteration 3/1000 | Loss: 0.00015329
Iteration 4/1000 | Loss: 0.00019463
Iteration 5/1000 | Loss: 0.00013793
Iteration 6/1000 | Loss: 0.00015534
Iteration 7/1000 | Loss: 0.00019508
Iteration 8/1000 | Loss: 0.00019487
Iteration 9/1000 | Loss: 0.00020057
Iteration 10/1000 | Loss: 0.00015950
Iteration 11/1000 | Loss: 0.00021076
Iteration 12/1000 | Loss: 0.00015210
Iteration 13/1000 | Loss: 0.00026664
Iteration 14/1000 | Loss: 0.00019558
Iteration 15/1000 | Loss: 0.00016174
Iteration 16/1000 | Loss: 0.00015403
Iteration 17/1000 | Loss: 0.00014724
Iteration 18/1000 | Loss: 0.00015965
Iteration 19/1000 | Loss: 0.00020479
Iteration 20/1000 | Loss: 0.00020463
Iteration 21/1000 | Loss: 0.00033886
Iteration 22/1000 | Loss: 0.00015156
Iteration 23/1000 | Loss: 0.00010968
Iteration 24/1000 | Loss: 0.00019449
Iteration 25/1000 | Loss: 0.00022001
Iteration 26/1000 | Loss: 0.00022332
Iteration 27/1000 | Loss: 0.00008472
Iteration 28/1000 | Loss: 0.00035026
Iteration 29/1000 | Loss: 0.00004715
Iteration 30/1000 | Loss: 0.00003606
Iteration 31/1000 | Loss: 0.00003264
Iteration 32/1000 | Loss: 0.00003082
Iteration 33/1000 | Loss: 0.00026185
Iteration 34/1000 | Loss: 0.00004856
Iteration 35/1000 | Loss: 0.00021977
Iteration 36/1000 | Loss: 0.00003113
Iteration 37/1000 | Loss: 0.00002959
Iteration 38/1000 | Loss: 0.00002859
Iteration 39/1000 | Loss: 0.00002795
Iteration 40/1000 | Loss: 0.00002748
Iteration 41/1000 | Loss: 0.00002709
Iteration 42/1000 | Loss: 0.00002676
Iteration 43/1000 | Loss: 0.00002660
Iteration 44/1000 | Loss: 0.00002650
Iteration 45/1000 | Loss: 0.00002648
Iteration 46/1000 | Loss: 0.00002648
Iteration 47/1000 | Loss: 0.00002647
Iteration 48/1000 | Loss: 0.00002647
Iteration 49/1000 | Loss: 0.00002647
Iteration 50/1000 | Loss: 0.00002646
Iteration 51/1000 | Loss: 0.00002646
Iteration 52/1000 | Loss: 0.00002646
Iteration 53/1000 | Loss: 0.00002646
Iteration 54/1000 | Loss: 0.00002646
Iteration 55/1000 | Loss: 0.00002645
Iteration 56/1000 | Loss: 0.00002645
Iteration 57/1000 | Loss: 0.00002645
Iteration 58/1000 | Loss: 0.00002645
Iteration 59/1000 | Loss: 0.00002644
Iteration 60/1000 | Loss: 0.00002644
Iteration 61/1000 | Loss: 0.00002644
Iteration 62/1000 | Loss: 0.00002644
Iteration 63/1000 | Loss: 0.00002643
Iteration 64/1000 | Loss: 0.00002643
Iteration 65/1000 | Loss: 0.00002643
Iteration 66/1000 | Loss: 0.00002643
Iteration 67/1000 | Loss: 0.00002643
Iteration 68/1000 | Loss: 0.00002642
Iteration 69/1000 | Loss: 0.00002642
Iteration 70/1000 | Loss: 0.00002642
Iteration 71/1000 | Loss: 0.00002642
Iteration 72/1000 | Loss: 0.00002642
Iteration 73/1000 | Loss: 0.00002641
Iteration 74/1000 | Loss: 0.00002641
Iteration 75/1000 | Loss: 0.00002641
Iteration 76/1000 | Loss: 0.00002640
Iteration 77/1000 | Loss: 0.00002640
Iteration 78/1000 | Loss: 0.00002639
Iteration 79/1000 | Loss: 0.00002639
Iteration 80/1000 | Loss: 0.00002639
Iteration 81/1000 | Loss: 0.00002639
Iteration 82/1000 | Loss: 0.00002639
Iteration 83/1000 | Loss: 0.00002638
Iteration 84/1000 | Loss: 0.00002638
Iteration 85/1000 | Loss: 0.00002637
Iteration 86/1000 | Loss: 0.00002637
Iteration 87/1000 | Loss: 0.00002636
Iteration 88/1000 | Loss: 0.00002636
Iteration 89/1000 | Loss: 0.00002636
Iteration 90/1000 | Loss: 0.00002636
Iteration 91/1000 | Loss: 0.00002635
Iteration 92/1000 | Loss: 0.00002635
Iteration 93/1000 | Loss: 0.00002635
Iteration 94/1000 | Loss: 0.00002635
Iteration 95/1000 | Loss: 0.00002634
Iteration 96/1000 | Loss: 0.00002634
Iteration 97/1000 | Loss: 0.00002634
Iteration 98/1000 | Loss: 0.00002634
Iteration 99/1000 | Loss: 0.00002633
Iteration 100/1000 | Loss: 0.00002633
Iteration 101/1000 | Loss: 0.00002633
Iteration 102/1000 | Loss: 0.00002632
Iteration 103/1000 | Loss: 0.00002632
Iteration 104/1000 | Loss: 0.00002631
Iteration 105/1000 | Loss: 0.00002631
Iteration 106/1000 | Loss: 0.00002631
Iteration 107/1000 | Loss: 0.00002631
Iteration 108/1000 | Loss: 0.00002631
Iteration 109/1000 | Loss: 0.00002631
Iteration 110/1000 | Loss: 0.00002630
Iteration 111/1000 | Loss: 0.00002630
Iteration 112/1000 | Loss: 0.00002630
Iteration 113/1000 | Loss: 0.00002630
Iteration 114/1000 | Loss: 0.00002630
Iteration 115/1000 | Loss: 0.00002630
Iteration 116/1000 | Loss: 0.00002630
Iteration 117/1000 | Loss: 0.00002629
Iteration 118/1000 | Loss: 0.00002629
Iteration 119/1000 | Loss: 0.00002629
Iteration 120/1000 | Loss: 0.00002629
Iteration 121/1000 | Loss: 0.00002628
Iteration 122/1000 | Loss: 0.00002628
Iteration 123/1000 | Loss: 0.00002628
Iteration 124/1000 | Loss: 0.00002628
Iteration 125/1000 | Loss: 0.00002628
Iteration 126/1000 | Loss: 0.00002628
Iteration 127/1000 | Loss: 0.00002628
Iteration 128/1000 | Loss: 0.00002628
Iteration 129/1000 | Loss: 0.00002628
Iteration 130/1000 | Loss: 0.00002628
Iteration 131/1000 | Loss: 0.00002628
Iteration 132/1000 | Loss: 0.00002628
Iteration 133/1000 | Loss: 0.00002628
Iteration 134/1000 | Loss: 0.00002628
Iteration 135/1000 | Loss: 0.00002628
Iteration 136/1000 | Loss: 0.00002628
Iteration 137/1000 | Loss: 0.00002628
Iteration 138/1000 | Loss: 0.00002628
Iteration 139/1000 | Loss: 0.00002628
Iteration 140/1000 | Loss: 0.00002628
Iteration 141/1000 | Loss: 0.00002628
Iteration 142/1000 | Loss: 0.00002628
Iteration 143/1000 | Loss: 0.00002628
Iteration 144/1000 | Loss: 0.00002628
Iteration 145/1000 | Loss: 0.00002628
Iteration 146/1000 | Loss: 0.00002628
Iteration 147/1000 | Loss: 0.00002628
Iteration 148/1000 | Loss: 0.00002628
Iteration 149/1000 | Loss: 0.00002628
Iteration 150/1000 | Loss: 0.00002628
Iteration 151/1000 | Loss: 0.00002628
Iteration 152/1000 | Loss: 0.00002628
Iteration 153/1000 | Loss: 0.00002628
Iteration 154/1000 | Loss: 0.00002628
Iteration 155/1000 | Loss: 0.00002628
Iteration 156/1000 | Loss: 0.00002628
Iteration 157/1000 | Loss: 0.00002628
Iteration 158/1000 | Loss: 0.00002628
Iteration 159/1000 | Loss: 0.00002628
Iteration 160/1000 | Loss: 0.00002628
Iteration 161/1000 | Loss: 0.00002628
Iteration 162/1000 | Loss: 0.00002628
Iteration 163/1000 | Loss: 0.00002628
Iteration 164/1000 | Loss: 0.00002628
Iteration 165/1000 | Loss: 0.00002628
Iteration 166/1000 | Loss: 0.00002628
Iteration 167/1000 | Loss: 0.00002628
Iteration 168/1000 | Loss: 0.00002628
Iteration 169/1000 | Loss: 0.00002628
Iteration 170/1000 | Loss: 0.00002628
Iteration 171/1000 | Loss: 0.00002628
Iteration 172/1000 | Loss: 0.00002628
Iteration 173/1000 | Loss: 0.00002628
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [2.6278892619302496e-05, 2.6278892619302496e-05, 2.6278892619302496e-05, 2.6278892619302496e-05, 2.6278892619302496e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6278892619302496e-05

Optimization complete. Final v2v error: 4.440178394317627 mm

Highest mean error: 5.139909744262695 mm for frame 39

Lowest mean error: 4.000436305999756 mm for frame 6

Saving results

Total time: 116.96500372886658
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_nl_6355/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6355/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6355/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00935699
Iteration 2/25 | Loss: 0.00141009
Iteration 3/25 | Loss: 0.00127286
Iteration 4/25 | Loss: 0.00124584
Iteration 5/25 | Loss: 0.00123587
Iteration 6/25 | Loss: 0.00123342
Iteration 7/25 | Loss: 0.00123317
Iteration 8/25 | Loss: 0.00123317
Iteration 9/25 | Loss: 0.00123317
Iteration 10/25 | Loss: 0.00123317
Iteration 11/25 | Loss: 0.00123317
Iteration 12/25 | Loss: 0.00123317
Iteration 13/25 | Loss: 0.00123317
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012331738835200667, 0.0012331738835200667, 0.0012331738835200667, 0.0012331738835200667, 0.0012331738835200667]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012331738835200667

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36510086
Iteration 2/25 | Loss: 0.00082355
Iteration 3/25 | Loss: 0.00082354
Iteration 4/25 | Loss: 0.00082354
Iteration 5/25 | Loss: 0.00082354
Iteration 6/25 | Loss: 0.00082354
Iteration 7/25 | Loss: 0.00082354
Iteration 8/25 | Loss: 0.00082354
Iteration 9/25 | Loss: 0.00082354
Iteration 10/25 | Loss: 0.00082354
Iteration 11/25 | Loss: 0.00082354
Iteration 12/25 | Loss: 0.00082354
Iteration 13/25 | Loss: 0.00082354
Iteration 14/25 | Loss: 0.00082354
Iteration 15/25 | Loss: 0.00082354
Iteration 16/25 | Loss: 0.00082354
Iteration 17/25 | Loss: 0.00082354
Iteration 18/25 | Loss: 0.00082354
Iteration 19/25 | Loss: 0.00082354
Iteration 20/25 | Loss: 0.00082354
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008235377026721835, 0.0008235377026721835, 0.0008235377026721835, 0.0008235377026721835, 0.0008235377026721835]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008235377026721835

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082354
Iteration 2/1000 | Loss: 0.00004293
Iteration 3/1000 | Loss: 0.00003331
Iteration 4/1000 | Loss: 0.00002938
Iteration 5/1000 | Loss: 0.00002812
Iteration 6/1000 | Loss: 0.00002731
Iteration 7/1000 | Loss: 0.00002636
Iteration 8/1000 | Loss: 0.00002577
Iteration 9/1000 | Loss: 0.00002527
Iteration 10/1000 | Loss: 0.00002490
Iteration 11/1000 | Loss: 0.00002469
Iteration 12/1000 | Loss: 0.00002447
Iteration 13/1000 | Loss: 0.00002446
Iteration 14/1000 | Loss: 0.00002442
Iteration 15/1000 | Loss: 0.00002433
Iteration 16/1000 | Loss: 0.00002431
Iteration 17/1000 | Loss: 0.00002430
Iteration 18/1000 | Loss: 0.00002428
Iteration 19/1000 | Loss: 0.00002426
Iteration 20/1000 | Loss: 0.00002425
Iteration 21/1000 | Loss: 0.00002422
Iteration 22/1000 | Loss: 0.00002421
Iteration 23/1000 | Loss: 0.00002421
Iteration 24/1000 | Loss: 0.00002421
Iteration 25/1000 | Loss: 0.00002421
Iteration 26/1000 | Loss: 0.00002421
Iteration 27/1000 | Loss: 0.00002421
Iteration 28/1000 | Loss: 0.00002420
Iteration 29/1000 | Loss: 0.00002418
Iteration 30/1000 | Loss: 0.00002417
Iteration 31/1000 | Loss: 0.00002417
Iteration 32/1000 | Loss: 0.00002417
Iteration 33/1000 | Loss: 0.00002416
Iteration 34/1000 | Loss: 0.00002416
Iteration 35/1000 | Loss: 0.00002416
Iteration 36/1000 | Loss: 0.00002416
Iteration 37/1000 | Loss: 0.00002416
Iteration 38/1000 | Loss: 0.00002416
Iteration 39/1000 | Loss: 0.00002416
Iteration 40/1000 | Loss: 0.00002416
Iteration 41/1000 | Loss: 0.00002415
Iteration 42/1000 | Loss: 0.00002415
Iteration 43/1000 | Loss: 0.00002414
Iteration 44/1000 | Loss: 0.00002414
Iteration 45/1000 | Loss: 0.00002413
Iteration 46/1000 | Loss: 0.00002413
Iteration 47/1000 | Loss: 0.00002413
Iteration 48/1000 | Loss: 0.00002413
Iteration 49/1000 | Loss: 0.00002412
Iteration 50/1000 | Loss: 0.00002412
Iteration 51/1000 | Loss: 0.00002412
Iteration 52/1000 | Loss: 0.00002412
Iteration 53/1000 | Loss: 0.00002412
Iteration 54/1000 | Loss: 0.00002412
Iteration 55/1000 | Loss: 0.00002412
Iteration 56/1000 | Loss: 0.00002411
Iteration 57/1000 | Loss: 0.00002411
Iteration 58/1000 | Loss: 0.00002411
Iteration 59/1000 | Loss: 0.00002411
Iteration 60/1000 | Loss: 0.00002410
Iteration 61/1000 | Loss: 0.00002410
Iteration 62/1000 | Loss: 0.00002409
Iteration 63/1000 | Loss: 0.00002409
Iteration 64/1000 | Loss: 0.00002409
Iteration 65/1000 | Loss: 0.00002408
Iteration 66/1000 | Loss: 0.00002408
Iteration 67/1000 | Loss: 0.00002408
Iteration 68/1000 | Loss: 0.00002407
Iteration 69/1000 | Loss: 0.00002407
Iteration 70/1000 | Loss: 0.00002407
Iteration 71/1000 | Loss: 0.00002406
Iteration 72/1000 | Loss: 0.00002406
Iteration 73/1000 | Loss: 0.00002405
Iteration 74/1000 | Loss: 0.00002405
Iteration 75/1000 | Loss: 0.00002405
Iteration 76/1000 | Loss: 0.00002405
Iteration 77/1000 | Loss: 0.00002405
Iteration 78/1000 | Loss: 0.00002405
Iteration 79/1000 | Loss: 0.00002405
Iteration 80/1000 | Loss: 0.00002405
Iteration 81/1000 | Loss: 0.00002405
Iteration 82/1000 | Loss: 0.00002405
Iteration 83/1000 | Loss: 0.00002405
Iteration 84/1000 | Loss: 0.00002405
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 84. Stopping optimization.
Last 5 losses: [2.4048509658314288e-05, 2.4048509658314288e-05, 2.4048509658314288e-05, 2.4048509658314288e-05, 2.4048509658314288e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4048509658314288e-05

Optimization complete. Final v2v error: 4.164816856384277 mm

Highest mean error: 4.970829486846924 mm for frame 191

Lowest mean error: 3.3614799976348877 mm for frame 124

Saving results

Total time: 38.551116943359375
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_nl_6355/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6355/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6355/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00441293
Iteration 2/25 | Loss: 0.00135575
Iteration 3/25 | Loss: 0.00123524
Iteration 4/25 | Loss: 0.00121830
Iteration 5/25 | Loss: 0.00120878
Iteration 6/25 | Loss: 0.00120739
Iteration 7/25 | Loss: 0.00120735
Iteration 8/25 | Loss: 0.00120735
Iteration 9/25 | Loss: 0.00120735
Iteration 10/25 | Loss: 0.00120735
Iteration 11/25 | Loss: 0.00120735
Iteration 12/25 | Loss: 0.00120735
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012073505204170942, 0.0012073505204170942, 0.0012073505204170942, 0.0012073505204170942, 0.0012073505204170942]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012073505204170942

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.48317981
Iteration 2/25 | Loss: 0.00066248
Iteration 3/25 | Loss: 0.00066248
Iteration 4/25 | Loss: 0.00066248
Iteration 5/25 | Loss: 0.00066248
Iteration 6/25 | Loss: 0.00066248
Iteration 7/25 | Loss: 0.00066248
Iteration 8/25 | Loss: 0.00066248
Iteration 9/25 | Loss: 0.00066248
Iteration 10/25 | Loss: 0.00066248
Iteration 11/25 | Loss: 0.00066248
Iteration 12/25 | Loss: 0.00066248
Iteration 13/25 | Loss: 0.00066248
Iteration 14/25 | Loss: 0.00066248
Iteration 15/25 | Loss: 0.00066248
Iteration 16/25 | Loss: 0.00066248
Iteration 17/25 | Loss: 0.00066248
Iteration 18/25 | Loss: 0.00066248
Iteration 19/25 | Loss: 0.00066248
Iteration 20/25 | Loss: 0.00066248
Iteration 21/25 | Loss: 0.00066248
Iteration 22/25 | Loss: 0.00066248
Iteration 23/25 | Loss: 0.00066248
Iteration 24/25 | Loss: 0.00066248
Iteration 25/25 | Loss: 0.00066248

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066248
Iteration 2/1000 | Loss: 0.00004315
Iteration 3/1000 | Loss: 0.00003521
Iteration 4/1000 | Loss: 0.00003382
Iteration 5/1000 | Loss: 0.00003255
Iteration 6/1000 | Loss: 0.00003154
Iteration 7/1000 | Loss: 0.00003100
Iteration 8/1000 | Loss: 0.00003070
Iteration 9/1000 | Loss: 0.00003047
Iteration 10/1000 | Loss: 0.00003046
Iteration 11/1000 | Loss: 0.00003040
Iteration 12/1000 | Loss: 0.00003025
Iteration 13/1000 | Loss: 0.00003018
Iteration 14/1000 | Loss: 0.00003018
Iteration 15/1000 | Loss: 0.00003017
Iteration 16/1000 | Loss: 0.00003017
Iteration 17/1000 | Loss: 0.00003016
Iteration 18/1000 | Loss: 0.00003016
Iteration 19/1000 | Loss: 0.00003016
Iteration 20/1000 | Loss: 0.00003016
Iteration 21/1000 | Loss: 0.00003015
Iteration 22/1000 | Loss: 0.00003015
Iteration 23/1000 | Loss: 0.00003015
Iteration 24/1000 | Loss: 0.00003015
Iteration 25/1000 | Loss: 0.00003014
Iteration 26/1000 | Loss: 0.00003013
Iteration 27/1000 | Loss: 0.00003013
Iteration 28/1000 | Loss: 0.00003013
Iteration 29/1000 | Loss: 0.00003013
Iteration 30/1000 | Loss: 0.00003013
Iteration 31/1000 | Loss: 0.00003013
Iteration 32/1000 | Loss: 0.00003013
Iteration 33/1000 | Loss: 0.00003013
Iteration 34/1000 | Loss: 0.00003012
Iteration 35/1000 | Loss: 0.00003012
Iteration 36/1000 | Loss: 0.00003012
Iteration 37/1000 | Loss: 0.00003012
Iteration 38/1000 | Loss: 0.00003012
Iteration 39/1000 | Loss: 0.00003011
Iteration 40/1000 | Loss: 0.00003011
Iteration 41/1000 | Loss: 0.00003011
Iteration 42/1000 | Loss: 0.00003011
Iteration 43/1000 | Loss: 0.00003011
Iteration 44/1000 | Loss: 0.00003010
Iteration 45/1000 | Loss: 0.00003010
Iteration 46/1000 | Loss: 0.00003010
Iteration 47/1000 | Loss: 0.00003010
Iteration 48/1000 | Loss: 0.00003010
Iteration 49/1000 | Loss: 0.00003010
Iteration 50/1000 | Loss: 0.00003010
Iteration 51/1000 | Loss: 0.00003010
Iteration 52/1000 | Loss: 0.00003010
Iteration 53/1000 | Loss: 0.00003010
Iteration 54/1000 | Loss: 0.00003010
Iteration 55/1000 | Loss: 0.00003010
Iteration 56/1000 | Loss: 0.00003010
Iteration 57/1000 | Loss: 0.00003010
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 57. Stopping optimization.
Last 5 losses: [3.010271757375449e-05, 3.010271757375449e-05, 3.010271757375449e-05, 3.010271757375449e-05, 3.010271757375449e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.010271757375449e-05

Optimization complete. Final v2v error: 4.65457820892334 mm

Highest mean error: 4.855676651000977 mm for frame 8

Lowest mean error: 4.282876491546631 mm for frame 48

Saving results

Total time: 27.42823362350464
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_nl_6355/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6355/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6355/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00450235
Iteration 2/25 | Loss: 0.00128873
Iteration 3/25 | Loss: 0.00116267
Iteration 4/25 | Loss: 0.00114173
Iteration 5/25 | Loss: 0.00113326
Iteration 6/25 | Loss: 0.00113148
Iteration 7/25 | Loss: 0.00113106
Iteration 8/25 | Loss: 0.00113106
Iteration 9/25 | Loss: 0.00113106
Iteration 10/25 | Loss: 0.00113106
Iteration 11/25 | Loss: 0.00113106
Iteration 12/25 | Loss: 0.00113106
Iteration 13/25 | Loss: 0.00113106
Iteration 14/25 | Loss: 0.00113106
Iteration 15/25 | Loss: 0.00113106
Iteration 16/25 | Loss: 0.00113106
Iteration 17/25 | Loss: 0.00113106
Iteration 18/25 | Loss: 0.00113106
Iteration 19/25 | Loss: 0.00113106
Iteration 20/25 | Loss: 0.00113106
Iteration 21/25 | Loss: 0.00113106
Iteration 22/25 | Loss: 0.00113106
Iteration 23/25 | Loss: 0.00113106
Iteration 24/25 | Loss: 0.00113106
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0011310605332255363, 0.0011310605332255363, 0.0011310605332255363, 0.0011310605332255363, 0.0011310605332255363]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011310605332255363

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51804018
Iteration 2/25 | Loss: 0.00063853
Iteration 3/25 | Loss: 0.00063853
Iteration 4/25 | Loss: 0.00063853
Iteration 5/25 | Loss: 0.00063853
Iteration 6/25 | Loss: 0.00063853
Iteration 7/25 | Loss: 0.00063853
Iteration 8/25 | Loss: 0.00063853
Iteration 9/25 | Loss: 0.00063853
Iteration 10/25 | Loss: 0.00063853
Iteration 11/25 | Loss: 0.00063852
Iteration 12/25 | Loss: 0.00063852
Iteration 13/25 | Loss: 0.00063852
Iteration 14/25 | Loss: 0.00063852
Iteration 15/25 | Loss: 0.00063852
Iteration 16/25 | Loss: 0.00063852
Iteration 17/25 | Loss: 0.00063852
Iteration 18/25 | Loss: 0.00063852
Iteration 19/25 | Loss: 0.00063852
Iteration 20/25 | Loss: 0.00063852
Iteration 21/25 | Loss: 0.00063852
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006385249434970319, 0.0006385249434970319, 0.0006385249434970319, 0.0006385249434970319, 0.0006385249434970319]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006385249434970319

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063852
Iteration 2/1000 | Loss: 0.00003847
Iteration 3/1000 | Loss: 0.00002584
Iteration 4/1000 | Loss: 0.00002303
Iteration 5/1000 | Loss: 0.00002179
Iteration 6/1000 | Loss: 0.00002084
Iteration 7/1000 | Loss: 0.00002037
Iteration 8/1000 | Loss: 0.00001987
Iteration 9/1000 | Loss: 0.00001953
Iteration 10/1000 | Loss: 0.00001933
Iteration 11/1000 | Loss: 0.00001923
Iteration 12/1000 | Loss: 0.00001911
Iteration 13/1000 | Loss: 0.00001900
Iteration 14/1000 | Loss: 0.00001895
Iteration 15/1000 | Loss: 0.00001895
Iteration 16/1000 | Loss: 0.00001893
Iteration 17/1000 | Loss: 0.00001892
Iteration 18/1000 | Loss: 0.00001891
Iteration 19/1000 | Loss: 0.00001891
Iteration 20/1000 | Loss: 0.00001891
Iteration 21/1000 | Loss: 0.00001890
Iteration 22/1000 | Loss: 0.00001890
Iteration 23/1000 | Loss: 0.00001890
Iteration 24/1000 | Loss: 0.00001890
Iteration 25/1000 | Loss: 0.00001890
Iteration 26/1000 | Loss: 0.00001890
Iteration 27/1000 | Loss: 0.00001889
Iteration 28/1000 | Loss: 0.00001889
Iteration 29/1000 | Loss: 0.00001889
Iteration 30/1000 | Loss: 0.00001888
Iteration 31/1000 | Loss: 0.00001888
Iteration 32/1000 | Loss: 0.00001888
Iteration 33/1000 | Loss: 0.00001888
Iteration 34/1000 | Loss: 0.00001887
Iteration 35/1000 | Loss: 0.00001887
Iteration 36/1000 | Loss: 0.00001887
Iteration 37/1000 | Loss: 0.00001886
Iteration 38/1000 | Loss: 0.00001886
Iteration 39/1000 | Loss: 0.00001886
Iteration 40/1000 | Loss: 0.00001885
Iteration 41/1000 | Loss: 0.00001885
Iteration 42/1000 | Loss: 0.00001884
Iteration 43/1000 | Loss: 0.00001883
Iteration 44/1000 | Loss: 0.00001883
Iteration 45/1000 | Loss: 0.00001883
Iteration 46/1000 | Loss: 0.00001882
Iteration 47/1000 | Loss: 0.00001882
Iteration 48/1000 | Loss: 0.00001881
Iteration 49/1000 | Loss: 0.00001881
Iteration 50/1000 | Loss: 0.00001881
Iteration 51/1000 | Loss: 0.00001881
Iteration 52/1000 | Loss: 0.00001880
Iteration 53/1000 | Loss: 0.00001880
Iteration 54/1000 | Loss: 0.00001879
Iteration 55/1000 | Loss: 0.00001879
Iteration 56/1000 | Loss: 0.00001879
Iteration 57/1000 | Loss: 0.00001879
Iteration 58/1000 | Loss: 0.00001878
Iteration 59/1000 | Loss: 0.00001878
Iteration 60/1000 | Loss: 0.00001878
Iteration 61/1000 | Loss: 0.00001877
Iteration 62/1000 | Loss: 0.00001877
Iteration 63/1000 | Loss: 0.00001876
Iteration 64/1000 | Loss: 0.00001876
Iteration 65/1000 | Loss: 0.00001876
Iteration 66/1000 | Loss: 0.00001876
Iteration 67/1000 | Loss: 0.00001875
Iteration 68/1000 | Loss: 0.00001875
Iteration 69/1000 | Loss: 0.00001875
Iteration 70/1000 | Loss: 0.00001875
Iteration 71/1000 | Loss: 0.00001875
Iteration 72/1000 | Loss: 0.00001875
Iteration 73/1000 | Loss: 0.00001874
Iteration 74/1000 | Loss: 0.00001874
Iteration 75/1000 | Loss: 0.00001874
Iteration 76/1000 | Loss: 0.00001874
Iteration 77/1000 | Loss: 0.00001873
Iteration 78/1000 | Loss: 0.00001873
Iteration 79/1000 | Loss: 0.00001873
Iteration 80/1000 | Loss: 0.00001873
Iteration 81/1000 | Loss: 0.00001873
Iteration 82/1000 | Loss: 0.00001873
Iteration 83/1000 | Loss: 0.00001873
Iteration 84/1000 | Loss: 0.00001873
Iteration 85/1000 | Loss: 0.00001873
Iteration 86/1000 | Loss: 0.00001873
Iteration 87/1000 | Loss: 0.00001873
Iteration 88/1000 | Loss: 0.00001873
Iteration 89/1000 | Loss: 0.00001872
Iteration 90/1000 | Loss: 0.00001872
Iteration 91/1000 | Loss: 0.00001872
Iteration 92/1000 | Loss: 0.00001872
Iteration 93/1000 | Loss: 0.00001872
Iteration 94/1000 | Loss: 0.00001872
Iteration 95/1000 | Loss: 0.00001872
Iteration 96/1000 | Loss: 0.00001872
Iteration 97/1000 | Loss: 0.00001872
Iteration 98/1000 | Loss: 0.00001872
Iteration 99/1000 | Loss: 0.00001872
Iteration 100/1000 | Loss: 0.00001872
Iteration 101/1000 | Loss: 0.00001872
Iteration 102/1000 | Loss: 0.00001872
Iteration 103/1000 | Loss: 0.00001872
Iteration 104/1000 | Loss: 0.00001871
Iteration 105/1000 | Loss: 0.00001871
Iteration 106/1000 | Loss: 0.00001871
Iteration 107/1000 | Loss: 0.00001871
Iteration 108/1000 | Loss: 0.00001871
Iteration 109/1000 | Loss: 0.00001871
Iteration 110/1000 | Loss: 0.00001871
Iteration 111/1000 | Loss: 0.00001871
Iteration 112/1000 | Loss: 0.00001871
Iteration 113/1000 | Loss: 0.00001871
Iteration 114/1000 | Loss: 0.00001871
Iteration 115/1000 | Loss: 0.00001871
Iteration 116/1000 | Loss: 0.00001871
Iteration 117/1000 | Loss: 0.00001871
Iteration 118/1000 | Loss: 0.00001871
Iteration 119/1000 | Loss: 0.00001871
Iteration 120/1000 | Loss: 0.00001871
Iteration 121/1000 | Loss: 0.00001871
Iteration 122/1000 | Loss: 0.00001870
Iteration 123/1000 | Loss: 0.00001870
Iteration 124/1000 | Loss: 0.00001870
Iteration 125/1000 | Loss: 0.00001870
Iteration 126/1000 | Loss: 0.00001870
Iteration 127/1000 | Loss: 0.00001870
Iteration 128/1000 | Loss: 0.00001870
Iteration 129/1000 | Loss: 0.00001870
Iteration 130/1000 | Loss: 0.00001870
Iteration 131/1000 | Loss: 0.00001870
Iteration 132/1000 | Loss: 0.00001870
Iteration 133/1000 | Loss: 0.00001870
Iteration 134/1000 | Loss: 0.00001870
Iteration 135/1000 | Loss: 0.00001869
Iteration 136/1000 | Loss: 0.00001869
Iteration 137/1000 | Loss: 0.00001869
Iteration 138/1000 | Loss: 0.00001869
Iteration 139/1000 | Loss: 0.00001869
Iteration 140/1000 | Loss: 0.00001869
Iteration 141/1000 | Loss: 0.00001869
Iteration 142/1000 | Loss: 0.00001869
Iteration 143/1000 | Loss: 0.00001869
Iteration 144/1000 | Loss: 0.00001869
Iteration 145/1000 | Loss: 0.00001869
Iteration 146/1000 | Loss: 0.00001869
Iteration 147/1000 | Loss: 0.00001869
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.869207335403189e-05, 1.869207335403189e-05, 1.869207335403189e-05, 1.869207335403189e-05, 1.869207335403189e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.869207335403189e-05

Optimization complete. Final v2v error: 3.6805765628814697 mm

Highest mean error: 4.350276947021484 mm for frame 26

Lowest mean error: 3.401890993118286 mm for frame 90

Saving results

Total time: 37.26496148109436
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_nl_6355/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6355/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6355/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00954937
Iteration 2/25 | Loss: 0.00136740
Iteration 3/25 | Loss: 0.00124136
Iteration 4/25 | Loss: 0.00121326
Iteration 5/25 | Loss: 0.00120484
Iteration 6/25 | Loss: 0.00120267
Iteration 7/25 | Loss: 0.00120169
Iteration 8/25 | Loss: 0.00120169
Iteration 9/25 | Loss: 0.00120169
Iteration 10/25 | Loss: 0.00120169
Iteration 11/25 | Loss: 0.00120169
Iteration 12/25 | Loss: 0.00120169
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012016907567158341, 0.0012016907567158341, 0.0012016907567158341, 0.0012016907567158341, 0.0012016907567158341]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012016907567158341

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41122234
Iteration 2/25 | Loss: 0.00072174
Iteration 3/25 | Loss: 0.00072173
Iteration 4/25 | Loss: 0.00072173
Iteration 5/25 | Loss: 0.00072173
Iteration 6/25 | Loss: 0.00072173
Iteration 7/25 | Loss: 0.00072173
Iteration 8/25 | Loss: 0.00072173
Iteration 9/25 | Loss: 0.00072173
Iteration 10/25 | Loss: 0.00072173
Iteration 11/25 | Loss: 0.00072173
Iteration 12/25 | Loss: 0.00072173
Iteration 13/25 | Loss: 0.00072173
Iteration 14/25 | Loss: 0.00072173
Iteration 15/25 | Loss: 0.00072173
Iteration 16/25 | Loss: 0.00072173
Iteration 17/25 | Loss: 0.00072173
Iteration 18/25 | Loss: 0.00072173
Iteration 19/25 | Loss: 0.00072173
Iteration 20/25 | Loss: 0.00072173
Iteration 21/25 | Loss: 0.00072173
Iteration 22/25 | Loss: 0.00072173
Iteration 23/25 | Loss: 0.00072173
Iteration 24/25 | Loss: 0.00072173
Iteration 25/25 | Loss: 0.00072173

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072173
Iteration 2/1000 | Loss: 0.00004233
Iteration 3/1000 | Loss: 0.00003146
Iteration 4/1000 | Loss: 0.00002734
Iteration 5/1000 | Loss: 0.00002648
Iteration 6/1000 | Loss: 0.00002554
Iteration 7/1000 | Loss: 0.00002498
Iteration 8/1000 | Loss: 0.00002446
Iteration 9/1000 | Loss: 0.00002408
Iteration 10/1000 | Loss: 0.00002384
Iteration 11/1000 | Loss: 0.00002372
Iteration 12/1000 | Loss: 0.00002368
Iteration 13/1000 | Loss: 0.00002366
Iteration 14/1000 | Loss: 0.00002362
Iteration 15/1000 | Loss: 0.00002358
Iteration 16/1000 | Loss: 0.00002354
Iteration 17/1000 | Loss: 0.00002354
Iteration 18/1000 | Loss: 0.00002351
Iteration 19/1000 | Loss: 0.00002351
Iteration 20/1000 | Loss: 0.00002350
Iteration 21/1000 | Loss: 0.00002350
Iteration 22/1000 | Loss: 0.00002350
Iteration 23/1000 | Loss: 0.00002349
Iteration 24/1000 | Loss: 0.00002349
Iteration 25/1000 | Loss: 0.00002349
Iteration 26/1000 | Loss: 0.00002348
Iteration 27/1000 | Loss: 0.00002348
Iteration 28/1000 | Loss: 0.00002347
Iteration 29/1000 | Loss: 0.00002347
Iteration 30/1000 | Loss: 0.00002347
Iteration 31/1000 | Loss: 0.00002346
Iteration 32/1000 | Loss: 0.00002346
Iteration 33/1000 | Loss: 0.00002346
Iteration 34/1000 | Loss: 0.00002346
Iteration 35/1000 | Loss: 0.00002346
Iteration 36/1000 | Loss: 0.00002346
Iteration 37/1000 | Loss: 0.00002346
Iteration 38/1000 | Loss: 0.00002346
Iteration 39/1000 | Loss: 0.00002346
Iteration 40/1000 | Loss: 0.00002346
Iteration 41/1000 | Loss: 0.00002345
Iteration 42/1000 | Loss: 0.00002345
Iteration 43/1000 | Loss: 0.00002344
Iteration 44/1000 | Loss: 0.00002344
Iteration 45/1000 | Loss: 0.00002344
Iteration 46/1000 | Loss: 0.00002344
Iteration 47/1000 | Loss: 0.00002344
Iteration 48/1000 | Loss: 0.00002343
Iteration 49/1000 | Loss: 0.00002343
Iteration 50/1000 | Loss: 0.00002343
Iteration 51/1000 | Loss: 0.00002342
Iteration 52/1000 | Loss: 0.00002342
Iteration 53/1000 | Loss: 0.00002342
Iteration 54/1000 | Loss: 0.00002342
Iteration 55/1000 | Loss: 0.00002342
Iteration 56/1000 | Loss: 0.00002341
Iteration 57/1000 | Loss: 0.00002341
Iteration 58/1000 | Loss: 0.00002341
Iteration 59/1000 | Loss: 0.00002340
Iteration 60/1000 | Loss: 0.00002340
Iteration 61/1000 | Loss: 0.00002340
Iteration 62/1000 | Loss: 0.00002340
Iteration 63/1000 | Loss: 0.00002339
Iteration 64/1000 | Loss: 0.00002339
Iteration 65/1000 | Loss: 0.00002339
Iteration 66/1000 | Loss: 0.00002339
Iteration 67/1000 | Loss: 0.00002339
Iteration 68/1000 | Loss: 0.00002339
Iteration 69/1000 | Loss: 0.00002339
Iteration 70/1000 | Loss: 0.00002339
Iteration 71/1000 | Loss: 0.00002339
Iteration 72/1000 | Loss: 0.00002339
Iteration 73/1000 | Loss: 0.00002339
Iteration 74/1000 | Loss: 0.00002339
Iteration 75/1000 | Loss: 0.00002339
Iteration 76/1000 | Loss: 0.00002338
Iteration 77/1000 | Loss: 0.00002338
Iteration 78/1000 | Loss: 0.00002338
Iteration 79/1000 | Loss: 0.00002338
Iteration 80/1000 | Loss: 0.00002338
Iteration 81/1000 | Loss: 0.00002338
Iteration 82/1000 | Loss: 0.00002338
Iteration 83/1000 | Loss: 0.00002338
Iteration 84/1000 | Loss: 0.00002338
Iteration 85/1000 | Loss: 0.00002338
Iteration 86/1000 | Loss: 0.00002338
Iteration 87/1000 | Loss: 0.00002338
Iteration 88/1000 | Loss: 0.00002338
Iteration 89/1000 | Loss: 0.00002338
Iteration 90/1000 | Loss: 0.00002338
Iteration 91/1000 | Loss: 0.00002338
Iteration 92/1000 | Loss: 0.00002338
Iteration 93/1000 | Loss: 0.00002338
Iteration 94/1000 | Loss: 0.00002338
Iteration 95/1000 | Loss: 0.00002338
Iteration 96/1000 | Loss: 0.00002337
Iteration 97/1000 | Loss: 0.00002337
Iteration 98/1000 | Loss: 0.00002337
Iteration 99/1000 | Loss: 0.00002337
Iteration 100/1000 | Loss: 0.00002337
Iteration 101/1000 | Loss: 0.00002337
Iteration 102/1000 | Loss: 0.00002337
Iteration 103/1000 | Loss: 0.00002337
Iteration 104/1000 | Loss: 0.00002337
Iteration 105/1000 | Loss: 0.00002337
Iteration 106/1000 | Loss: 0.00002337
Iteration 107/1000 | Loss: 0.00002337
Iteration 108/1000 | Loss: 0.00002337
Iteration 109/1000 | Loss: 0.00002337
Iteration 110/1000 | Loss: 0.00002337
Iteration 111/1000 | Loss: 0.00002337
Iteration 112/1000 | Loss: 0.00002337
Iteration 113/1000 | Loss: 0.00002337
Iteration 114/1000 | Loss: 0.00002337
Iteration 115/1000 | Loss: 0.00002337
Iteration 116/1000 | Loss: 0.00002336
Iteration 117/1000 | Loss: 0.00002336
Iteration 118/1000 | Loss: 0.00002336
Iteration 119/1000 | Loss: 0.00002336
Iteration 120/1000 | Loss: 0.00002336
Iteration 121/1000 | Loss: 0.00002336
Iteration 122/1000 | Loss: 0.00002336
Iteration 123/1000 | Loss: 0.00002336
Iteration 124/1000 | Loss: 0.00002336
Iteration 125/1000 | Loss: 0.00002336
Iteration 126/1000 | Loss: 0.00002336
Iteration 127/1000 | Loss: 0.00002336
Iteration 128/1000 | Loss: 0.00002336
Iteration 129/1000 | Loss: 0.00002336
Iteration 130/1000 | Loss: 0.00002336
Iteration 131/1000 | Loss: 0.00002336
Iteration 132/1000 | Loss: 0.00002336
Iteration 133/1000 | Loss: 0.00002336
Iteration 134/1000 | Loss: 0.00002336
Iteration 135/1000 | Loss: 0.00002336
Iteration 136/1000 | Loss: 0.00002336
Iteration 137/1000 | Loss: 0.00002336
Iteration 138/1000 | Loss: 0.00002336
Iteration 139/1000 | Loss: 0.00002336
Iteration 140/1000 | Loss: 0.00002336
Iteration 141/1000 | Loss: 0.00002336
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [2.3360576960840262e-05, 2.3360576960840262e-05, 2.3360576960840262e-05, 2.3360576960840262e-05, 2.3360576960840262e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3360576960840262e-05

Optimization complete. Final v2v error: 4.124441623687744 mm

Highest mean error: 4.792209148406982 mm for frame 152

Lowest mean error: 3.6027991771698 mm for frame 93

Saving results

Total time: 35.58192586898804
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_nl_6355/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6355/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_nl_6355/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00614251
Iteration 2/25 | Loss: 0.00142374
Iteration 3/25 | Loss: 0.00127536
Iteration 4/25 | Loss: 0.00125214
Iteration 5/25 | Loss: 0.00124717
Iteration 6/25 | Loss: 0.00124594
Iteration 7/25 | Loss: 0.00124594
Iteration 8/25 | Loss: 0.00124594
Iteration 9/25 | Loss: 0.00124594
Iteration 10/25 | Loss: 0.00124594
Iteration 11/25 | Loss: 0.00124594
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001245940220542252, 0.001245940220542252, 0.001245940220542252, 0.001245940220542252, 0.001245940220542252]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001245940220542252

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39510882
Iteration 2/25 | Loss: 0.00084779
Iteration 3/25 | Loss: 0.00084774
Iteration 4/25 | Loss: 0.00084774
Iteration 5/25 | Loss: 0.00084774
Iteration 6/25 | Loss: 0.00084774
Iteration 7/25 | Loss: 0.00084774
Iteration 8/25 | Loss: 0.00084774
Iteration 9/25 | Loss: 0.00084774
Iteration 10/25 | Loss: 0.00084774
Iteration 11/25 | Loss: 0.00084774
Iteration 12/25 | Loss: 0.00084774
Iteration 13/25 | Loss: 0.00084774
Iteration 14/25 | Loss: 0.00084774
Iteration 15/25 | Loss: 0.00084774
Iteration 16/25 | Loss: 0.00084774
Iteration 17/25 | Loss: 0.00084774
Iteration 18/25 | Loss: 0.00084774
Iteration 19/25 | Loss: 0.00084774
Iteration 20/25 | Loss: 0.00084774
Iteration 21/25 | Loss: 0.00084774
Iteration 22/25 | Loss: 0.00084774
Iteration 23/25 | Loss: 0.00084774
Iteration 24/25 | Loss: 0.00084774
Iteration 25/25 | Loss: 0.00084774

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084774
Iteration 2/1000 | Loss: 0.00003870
Iteration 3/1000 | Loss: 0.00002888
Iteration 4/1000 | Loss: 0.00002740
Iteration 5/1000 | Loss: 0.00002553
Iteration 6/1000 | Loss: 0.00002491
Iteration 7/1000 | Loss: 0.00002413
Iteration 8/1000 | Loss: 0.00002357
Iteration 9/1000 | Loss: 0.00002326
Iteration 10/1000 | Loss: 0.00002301
Iteration 11/1000 | Loss: 0.00002271
Iteration 12/1000 | Loss: 0.00002268
Iteration 13/1000 | Loss: 0.00002264
Iteration 14/1000 | Loss: 0.00002257
Iteration 15/1000 | Loss: 0.00002251
Iteration 16/1000 | Loss: 0.00002244
Iteration 17/1000 | Loss: 0.00002233
Iteration 18/1000 | Loss: 0.00002232
Iteration 19/1000 | Loss: 0.00002230
Iteration 20/1000 | Loss: 0.00002227
Iteration 21/1000 | Loss: 0.00002227
Iteration 22/1000 | Loss: 0.00002226
Iteration 23/1000 | Loss: 0.00002225
Iteration 24/1000 | Loss: 0.00002222
Iteration 25/1000 | Loss: 0.00002216
Iteration 26/1000 | Loss: 0.00002215
Iteration 27/1000 | Loss: 0.00002215
Iteration 28/1000 | Loss: 0.00002215
Iteration 29/1000 | Loss: 0.00002215
Iteration 30/1000 | Loss: 0.00002214
Iteration 31/1000 | Loss: 0.00002214
Iteration 32/1000 | Loss: 0.00002212
Iteration 33/1000 | Loss: 0.00002212
Iteration 34/1000 | Loss: 0.00002212
Iteration 35/1000 | Loss: 0.00002212
Iteration 36/1000 | Loss: 0.00002212
Iteration 37/1000 | Loss: 0.00002212
Iteration 38/1000 | Loss: 0.00002211
Iteration 39/1000 | Loss: 0.00002211
Iteration 40/1000 | Loss: 0.00002211
Iteration 41/1000 | Loss: 0.00002211
Iteration 42/1000 | Loss: 0.00002210
Iteration 43/1000 | Loss: 0.00002209
Iteration 44/1000 | Loss: 0.00002207
Iteration 45/1000 | Loss: 0.00002206
Iteration 46/1000 | Loss: 0.00002206
Iteration 47/1000 | Loss: 0.00002206
Iteration 48/1000 | Loss: 0.00002206
Iteration 49/1000 | Loss: 0.00002206
Iteration 50/1000 | Loss: 0.00002206
Iteration 51/1000 | Loss: 0.00002206
Iteration 52/1000 | Loss: 0.00002206
Iteration 53/1000 | Loss: 0.00002205
Iteration 54/1000 | Loss: 0.00002205
Iteration 55/1000 | Loss: 0.00002205
Iteration 56/1000 | Loss: 0.00002205
Iteration 57/1000 | Loss: 0.00002205
Iteration 58/1000 | Loss: 0.00002204
Iteration 59/1000 | Loss: 0.00002204
Iteration 60/1000 | Loss: 0.00002204
Iteration 61/1000 | Loss: 0.00002203
Iteration 62/1000 | Loss: 0.00002203
Iteration 63/1000 | Loss: 0.00002203
Iteration 64/1000 | Loss: 0.00002202
Iteration 65/1000 | Loss: 0.00002202
Iteration 66/1000 | Loss: 0.00002202
Iteration 67/1000 | Loss: 0.00002202
Iteration 68/1000 | Loss: 0.00002202
Iteration 69/1000 | Loss: 0.00002202
Iteration 70/1000 | Loss: 0.00002201
Iteration 71/1000 | Loss: 0.00002201
Iteration 72/1000 | Loss: 0.00002201
Iteration 73/1000 | Loss: 0.00002201
Iteration 74/1000 | Loss: 0.00002200
Iteration 75/1000 | Loss: 0.00002200
Iteration 76/1000 | Loss: 0.00002200
Iteration 77/1000 | Loss: 0.00002200
Iteration 78/1000 | Loss: 0.00002200
Iteration 79/1000 | Loss: 0.00002200
Iteration 80/1000 | Loss: 0.00002200
Iteration 81/1000 | Loss: 0.00002200
Iteration 82/1000 | Loss: 0.00002200
Iteration 83/1000 | Loss: 0.00002199
Iteration 84/1000 | Loss: 0.00002199
Iteration 85/1000 | Loss: 0.00002199
Iteration 86/1000 | Loss: 0.00002199
Iteration 87/1000 | Loss: 0.00002199
Iteration 88/1000 | Loss: 0.00002198
Iteration 89/1000 | Loss: 0.00002198
Iteration 90/1000 | Loss: 0.00002198
Iteration 91/1000 | Loss: 0.00002198
Iteration 92/1000 | Loss: 0.00002197
Iteration 93/1000 | Loss: 0.00002197
Iteration 94/1000 | Loss: 0.00002197
Iteration 95/1000 | Loss: 0.00002197
Iteration 96/1000 | Loss: 0.00002197
Iteration 97/1000 | Loss: 0.00002196
Iteration 98/1000 | Loss: 0.00002196
Iteration 99/1000 | Loss: 0.00002196
Iteration 100/1000 | Loss: 0.00002196
Iteration 101/1000 | Loss: 0.00002196
Iteration 102/1000 | Loss: 0.00002196
Iteration 103/1000 | Loss: 0.00002195
Iteration 104/1000 | Loss: 0.00002195
Iteration 105/1000 | Loss: 0.00002195
Iteration 106/1000 | Loss: 0.00002195
Iteration 107/1000 | Loss: 0.00002195
Iteration 108/1000 | Loss: 0.00002195
Iteration 109/1000 | Loss: 0.00002194
Iteration 110/1000 | Loss: 0.00002194
Iteration 111/1000 | Loss: 0.00002194
Iteration 112/1000 | Loss: 0.00002194
Iteration 113/1000 | Loss: 0.00002194
Iteration 114/1000 | Loss: 0.00002194
Iteration 115/1000 | Loss: 0.00002194
Iteration 116/1000 | Loss: 0.00002194
Iteration 117/1000 | Loss: 0.00002193
Iteration 118/1000 | Loss: 0.00002193
Iteration 119/1000 | Loss: 0.00002193
Iteration 120/1000 | Loss: 0.00002193
Iteration 121/1000 | Loss: 0.00002193
Iteration 122/1000 | Loss: 0.00002193
Iteration 123/1000 | Loss: 0.00002193
Iteration 124/1000 | Loss: 0.00002193
Iteration 125/1000 | Loss: 0.00002193
Iteration 126/1000 | Loss: 0.00002193
Iteration 127/1000 | Loss: 0.00002193
Iteration 128/1000 | Loss: 0.00002193
Iteration 129/1000 | Loss: 0.00002192
Iteration 130/1000 | Loss: 0.00002192
Iteration 131/1000 | Loss: 0.00002192
Iteration 132/1000 | Loss: 0.00002192
Iteration 133/1000 | Loss: 0.00002192
Iteration 134/1000 | Loss: 0.00002192
Iteration 135/1000 | Loss: 0.00002192
Iteration 136/1000 | Loss: 0.00002191
Iteration 137/1000 | Loss: 0.00002191
Iteration 138/1000 | Loss: 0.00002191
Iteration 139/1000 | Loss: 0.00002191
Iteration 140/1000 | Loss: 0.00002191
Iteration 141/1000 | Loss: 0.00002191
Iteration 142/1000 | Loss: 0.00002191
Iteration 143/1000 | Loss: 0.00002191
Iteration 144/1000 | Loss: 0.00002191
Iteration 145/1000 | Loss: 0.00002191
Iteration 146/1000 | Loss: 0.00002190
Iteration 147/1000 | Loss: 0.00002190
Iteration 148/1000 | Loss: 0.00002190
Iteration 149/1000 | Loss: 0.00002190
Iteration 150/1000 | Loss: 0.00002190
Iteration 151/1000 | Loss: 0.00002190
Iteration 152/1000 | Loss: 0.00002190
Iteration 153/1000 | Loss: 0.00002190
Iteration 154/1000 | Loss: 0.00002190
Iteration 155/1000 | Loss: 0.00002190
Iteration 156/1000 | Loss: 0.00002190
Iteration 157/1000 | Loss: 0.00002189
Iteration 158/1000 | Loss: 0.00002189
Iteration 159/1000 | Loss: 0.00002189
Iteration 160/1000 | Loss: 0.00002189
Iteration 161/1000 | Loss: 0.00002189
Iteration 162/1000 | Loss: 0.00002189
Iteration 163/1000 | Loss: 0.00002188
Iteration 164/1000 | Loss: 0.00002188
Iteration 165/1000 | Loss: 0.00002188
Iteration 166/1000 | Loss: 0.00002188
Iteration 167/1000 | Loss: 0.00002188
Iteration 168/1000 | Loss: 0.00002188
Iteration 169/1000 | Loss: 0.00002188
Iteration 170/1000 | Loss: 0.00002188
Iteration 171/1000 | Loss: 0.00002188
Iteration 172/1000 | Loss: 0.00002187
Iteration 173/1000 | Loss: 0.00002187
Iteration 174/1000 | Loss: 0.00002186
Iteration 175/1000 | Loss: 0.00002186
Iteration 176/1000 | Loss: 0.00002186
Iteration 177/1000 | Loss: 0.00002185
Iteration 178/1000 | Loss: 0.00002185
Iteration 179/1000 | Loss: 0.00002185
Iteration 180/1000 | Loss: 0.00002185
Iteration 181/1000 | Loss: 0.00002185
Iteration 182/1000 | Loss: 0.00002185
Iteration 183/1000 | Loss: 0.00002184
Iteration 184/1000 | Loss: 0.00002184
Iteration 185/1000 | Loss: 0.00002184
Iteration 186/1000 | Loss: 0.00002184
Iteration 187/1000 | Loss: 0.00002184
Iteration 188/1000 | Loss: 0.00002184
Iteration 189/1000 | Loss: 0.00002184
Iteration 190/1000 | Loss: 0.00002184
Iteration 191/1000 | Loss: 0.00002184
Iteration 192/1000 | Loss: 0.00002184
Iteration 193/1000 | Loss: 0.00002184
Iteration 194/1000 | Loss: 0.00002184
Iteration 195/1000 | Loss: 0.00002184
Iteration 196/1000 | Loss: 0.00002184
Iteration 197/1000 | Loss: 0.00002184
Iteration 198/1000 | Loss: 0.00002184
Iteration 199/1000 | Loss: 0.00002184
Iteration 200/1000 | Loss: 0.00002184
Iteration 201/1000 | Loss: 0.00002184
Iteration 202/1000 | Loss: 0.00002184
Iteration 203/1000 | Loss: 0.00002184
Iteration 204/1000 | Loss: 0.00002184
Iteration 205/1000 | Loss: 0.00002184
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [2.1842897695023566e-05, 2.1842897695023566e-05, 2.1842897695023566e-05, 2.1842897695023566e-05, 2.1842897695023566e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1842897695023566e-05

Optimization complete. Final v2v error: 4.052038669586182 mm

Highest mean error: 4.395554542541504 mm for frame 239

Lowest mean error: 3.876847743988037 mm for frame 189

Saving results

Total time: 48.238834857940674
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00560977
Iteration 2/25 | Loss: 0.00076847
Iteration 3/25 | Loss: 0.00062263
Iteration 4/25 | Loss: 0.00059403
Iteration 5/25 | Loss: 0.00058535
Iteration 6/25 | Loss: 0.00058347
Iteration 7/25 | Loss: 0.00058294
Iteration 8/25 | Loss: 0.00058294
Iteration 9/25 | Loss: 0.00058294
Iteration 10/25 | Loss: 0.00058294
Iteration 11/25 | Loss: 0.00058294
Iteration 12/25 | Loss: 0.00058294
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005829428555443883, 0.0005829428555443883, 0.0005829428555443883, 0.0005829428555443883, 0.0005829428555443883]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005829428555443883

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.01144123
Iteration 2/25 | Loss: 0.00020219
Iteration 3/25 | Loss: 0.00020218
Iteration 4/25 | Loss: 0.00020218
Iteration 5/25 | Loss: 0.00020218
Iteration 6/25 | Loss: 0.00020218
Iteration 7/25 | Loss: 0.00020218
Iteration 8/25 | Loss: 0.00020218
Iteration 9/25 | Loss: 0.00020218
Iteration 10/25 | Loss: 0.00020218
Iteration 11/25 | Loss: 0.00020218
Iteration 12/25 | Loss: 0.00020218
Iteration 13/25 | Loss: 0.00020218
Iteration 14/25 | Loss: 0.00020218
Iteration 15/25 | Loss: 0.00020218
Iteration 16/25 | Loss: 0.00020218
Iteration 17/25 | Loss: 0.00020218
Iteration 18/25 | Loss: 0.00020218
Iteration 19/25 | Loss: 0.00020218
Iteration 20/25 | Loss: 0.00020218
Iteration 21/25 | Loss: 0.00020218
Iteration 22/25 | Loss: 0.00020218
Iteration 23/25 | Loss: 0.00020218
Iteration 24/25 | Loss: 0.00020218
Iteration 25/25 | Loss: 0.00020218

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00020218
Iteration 2/1000 | Loss: 0.00002735
Iteration 3/1000 | Loss: 0.00001907
Iteration 4/1000 | Loss: 0.00001785
Iteration 5/1000 | Loss: 0.00001677
Iteration 6/1000 | Loss: 0.00001625
Iteration 7/1000 | Loss: 0.00001571
Iteration 8/1000 | Loss: 0.00001546
Iteration 9/1000 | Loss: 0.00001543
Iteration 10/1000 | Loss: 0.00001524
Iteration 11/1000 | Loss: 0.00001515
Iteration 12/1000 | Loss: 0.00001510
Iteration 13/1000 | Loss: 0.00001506
Iteration 14/1000 | Loss: 0.00001504
Iteration 15/1000 | Loss: 0.00001503
Iteration 16/1000 | Loss: 0.00001502
Iteration 17/1000 | Loss: 0.00001501
Iteration 18/1000 | Loss: 0.00001501
Iteration 19/1000 | Loss: 0.00001501
Iteration 20/1000 | Loss: 0.00001500
Iteration 21/1000 | Loss: 0.00001500
Iteration 22/1000 | Loss: 0.00001499
Iteration 23/1000 | Loss: 0.00001499
Iteration 24/1000 | Loss: 0.00001497
Iteration 25/1000 | Loss: 0.00001496
Iteration 26/1000 | Loss: 0.00001496
Iteration 27/1000 | Loss: 0.00001495
Iteration 28/1000 | Loss: 0.00001494
Iteration 29/1000 | Loss: 0.00001494
Iteration 30/1000 | Loss: 0.00001493
Iteration 31/1000 | Loss: 0.00001493
Iteration 32/1000 | Loss: 0.00001492
Iteration 33/1000 | Loss: 0.00001492
Iteration 34/1000 | Loss: 0.00001492
Iteration 35/1000 | Loss: 0.00001492
Iteration 36/1000 | Loss: 0.00001491
Iteration 37/1000 | Loss: 0.00001491
Iteration 38/1000 | Loss: 0.00001491
Iteration 39/1000 | Loss: 0.00001490
Iteration 40/1000 | Loss: 0.00001490
Iteration 41/1000 | Loss: 0.00001490
Iteration 42/1000 | Loss: 0.00001489
Iteration 43/1000 | Loss: 0.00001489
Iteration 44/1000 | Loss: 0.00001489
Iteration 45/1000 | Loss: 0.00001489
Iteration 46/1000 | Loss: 0.00001489
Iteration 47/1000 | Loss: 0.00001489
Iteration 48/1000 | Loss: 0.00001489
Iteration 49/1000 | Loss: 0.00001489
Iteration 50/1000 | Loss: 0.00001489
Iteration 51/1000 | Loss: 0.00001488
Iteration 52/1000 | Loss: 0.00001488
Iteration 53/1000 | Loss: 0.00001488
Iteration 54/1000 | Loss: 0.00001488
Iteration 55/1000 | Loss: 0.00001488
Iteration 56/1000 | Loss: 0.00001487
Iteration 57/1000 | Loss: 0.00001487
Iteration 58/1000 | Loss: 0.00001487
Iteration 59/1000 | Loss: 0.00001487
Iteration 60/1000 | Loss: 0.00001486
Iteration 61/1000 | Loss: 0.00001486
Iteration 62/1000 | Loss: 0.00001486
Iteration 63/1000 | Loss: 0.00001486
Iteration 64/1000 | Loss: 0.00001485
Iteration 65/1000 | Loss: 0.00001484
Iteration 66/1000 | Loss: 0.00001484
Iteration 67/1000 | Loss: 0.00001484
Iteration 68/1000 | Loss: 0.00001484
Iteration 69/1000 | Loss: 0.00001484
Iteration 70/1000 | Loss: 0.00001483
Iteration 71/1000 | Loss: 0.00001483
Iteration 72/1000 | Loss: 0.00001482
Iteration 73/1000 | Loss: 0.00001482
Iteration 74/1000 | Loss: 0.00001482
Iteration 75/1000 | Loss: 0.00001481
Iteration 76/1000 | Loss: 0.00001481
Iteration 77/1000 | Loss: 0.00001480
Iteration 78/1000 | Loss: 0.00001480
Iteration 79/1000 | Loss: 0.00001480
Iteration 80/1000 | Loss: 0.00001480
Iteration 81/1000 | Loss: 0.00001479
Iteration 82/1000 | Loss: 0.00001479
Iteration 83/1000 | Loss: 0.00001478
Iteration 84/1000 | Loss: 0.00001478
Iteration 85/1000 | Loss: 0.00001478
Iteration 86/1000 | Loss: 0.00001478
Iteration 87/1000 | Loss: 0.00001478
Iteration 88/1000 | Loss: 0.00001478
Iteration 89/1000 | Loss: 0.00001477
Iteration 90/1000 | Loss: 0.00001477
Iteration 91/1000 | Loss: 0.00001477
Iteration 92/1000 | Loss: 0.00001477
Iteration 93/1000 | Loss: 0.00001477
Iteration 94/1000 | Loss: 0.00001477
Iteration 95/1000 | Loss: 0.00001476
Iteration 96/1000 | Loss: 0.00001476
Iteration 97/1000 | Loss: 0.00001475
Iteration 98/1000 | Loss: 0.00001475
Iteration 99/1000 | Loss: 0.00001475
Iteration 100/1000 | Loss: 0.00001475
Iteration 101/1000 | Loss: 0.00001475
Iteration 102/1000 | Loss: 0.00001474
Iteration 103/1000 | Loss: 0.00001474
Iteration 104/1000 | Loss: 0.00001474
Iteration 105/1000 | Loss: 0.00001474
Iteration 106/1000 | Loss: 0.00001474
Iteration 107/1000 | Loss: 0.00001474
Iteration 108/1000 | Loss: 0.00001474
Iteration 109/1000 | Loss: 0.00001474
Iteration 110/1000 | Loss: 0.00001474
Iteration 111/1000 | Loss: 0.00001473
Iteration 112/1000 | Loss: 0.00001473
Iteration 113/1000 | Loss: 0.00001473
Iteration 114/1000 | Loss: 0.00001473
Iteration 115/1000 | Loss: 0.00001473
Iteration 116/1000 | Loss: 0.00001473
Iteration 117/1000 | Loss: 0.00001473
Iteration 118/1000 | Loss: 0.00001473
Iteration 119/1000 | Loss: 0.00001473
Iteration 120/1000 | Loss: 0.00001473
Iteration 121/1000 | Loss: 0.00001472
Iteration 122/1000 | Loss: 0.00001472
Iteration 123/1000 | Loss: 0.00001472
Iteration 124/1000 | Loss: 0.00001472
Iteration 125/1000 | Loss: 0.00001472
Iteration 126/1000 | Loss: 0.00001472
Iteration 127/1000 | Loss: 0.00001472
Iteration 128/1000 | Loss: 0.00001472
Iteration 129/1000 | Loss: 0.00001472
Iteration 130/1000 | Loss: 0.00001472
Iteration 131/1000 | Loss: 0.00001472
Iteration 132/1000 | Loss: 0.00001471
Iteration 133/1000 | Loss: 0.00001471
Iteration 134/1000 | Loss: 0.00001471
Iteration 135/1000 | Loss: 0.00001471
Iteration 136/1000 | Loss: 0.00001471
Iteration 137/1000 | Loss: 0.00001471
Iteration 138/1000 | Loss: 0.00001471
Iteration 139/1000 | Loss: 0.00001471
Iteration 140/1000 | Loss: 0.00001471
Iteration 141/1000 | Loss: 0.00001471
Iteration 142/1000 | Loss: 0.00001471
Iteration 143/1000 | Loss: 0.00001471
Iteration 144/1000 | Loss: 0.00001471
Iteration 145/1000 | Loss: 0.00001471
Iteration 146/1000 | Loss: 0.00001471
Iteration 147/1000 | Loss: 0.00001471
Iteration 148/1000 | Loss: 0.00001471
Iteration 149/1000 | Loss: 0.00001471
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.4708281014463864e-05, 1.4708281014463864e-05, 1.4708281014463864e-05, 1.4708281014463864e-05, 1.4708281014463864e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4708281014463864e-05

Optimization complete. Final v2v error: 3.232689619064331 mm

Highest mean error: 3.8185505867004395 mm for frame 65

Lowest mean error: 2.850511074066162 mm for frame 135

Saving results

Total time: 35.991395473480225
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00432102
Iteration 2/25 | Loss: 0.00074567
Iteration 3/25 | Loss: 0.00064393
Iteration 4/25 | Loss: 0.00061088
Iteration 5/25 | Loss: 0.00059993
Iteration 6/25 | Loss: 0.00059832
Iteration 7/25 | Loss: 0.00059774
Iteration 8/25 | Loss: 0.00059769
Iteration 9/25 | Loss: 0.00059769
Iteration 10/25 | Loss: 0.00059769
Iteration 11/25 | Loss: 0.00059769
Iteration 12/25 | Loss: 0.00059769
Iteration 13/25 | Loss: 0.00059769
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0005976878455840051, 0.0005976878455840051, 0.0005976878455840051, 0.0005976878455840051, 0.0005976878455840051]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005976878455840051

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51739812
Iteration 2/25 | Loss: 0.00021584
Iteration 3/25 | Loss: 0.00021584
Iteration 4/25 | Loss: 0.00021584
Iteration 5/25 | Loss: 0.00021584
Iteration 6/25 | Loss: 0.00021584
Iteration 7/25 | Loss: 0.00021584
Iteration 8/25 | Loss: 0.00021584
Iteration 9/25 | Loss: 0.00021584
Iteration 10/25 | Loss: 0.00021584
Iteration 11/25 | Loss: 0.00021584
Iteration 12/25 | Loss: 0.00021584
Iteration 13/25 | Loss: 0.00021584
Iteration 14/25 | Loss: 0.00021584
Iteration 15/25 | Loss: 0.00021584
Iteration 16/25 | Loss: 0.00021584
Iteration 17/25 | Loss: 0.00021584
Iteration 18/25 | Loss: 0.00021584
Iteration 19/25 | Loss: 0.00021584
Iteration 20/25 | Loss: 0.00021584
Iteration 21/25 | Loss: 0.00021584
Iteration 22/25 | Loss: 0.00021584
Iteration 23/25 | Loss: 0.00021584
Iteration 24/25 | Loss: 0.00021584
Iteration 25/25 | Loss: 0.00021584

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00021584
Iteration 2/1000 | Loss: 0.00002613
Iteration 3/1000 | Loss: 0.00001933
Iteration 4/1000 | Loss: 0.00001817
Iteration 5/1000 | Loss: 0.00001714
Iteration 6/1000 | Loss: 0.00001670
Iteration 7/1000 | Loss: 0.00001625
Iteration 8/1000 | Loss: 0.00001612
Iteration 9/1000 | Loss: 0.00001597
Iteration 10/1000 | Loss: 0.00001588
Iteration 11/1000 | Loss: 0.00001583
Iteration 12/1000 | Loss: 0.00001583
Iteration 13/1000 | Loss: 0.00001583
Iteration 14/1000 | Loss: 0.00001583
Iteration 15/1000 | Loss: 0.00001583
Iteration 16/1000 | Loss: 0.00001583
Iteration 17/1000 | Loss: 0.00001583
Iteration 18/1000 | Loss: 0.00001583
Iteration 19/1000 | Loss: 0.00001583
Iteration 20/1000 | Loss: 0.00001582
Iteration 21/1000 | Loss: 0.00001579
Iteration 22/1000 | Loss: 0.00001579
Iteration 23/1000 | Loss: 0.00001578
Iteration 24/1000 | Loss: 0.00001578
Iteration 25/1000 | Loss: 0.00001578
Iteration 26/1000 | Loss: 0.00001578
Iteration 27/1000 | Loss: 0.00001578
Iteration 28/1000 | Loss: 0.00001577
Iteration 29/1000 | Loss: 0.00001576
Iteration 30/1000 | Loss: 0.00001576
Iteration 31/1000 | Loss: 0.00001575
Iteration 32/1000 | Loss: 0.00001574
Iteration 33/1000 | Loss: 0.00001574
Iteration 34/1000 | Loss: 0.00001574
Iteration 35/1000 | Loss: 0.00001574
Iteration 36/1000 | Loss: 0.00001573
Iteration 37/1000 | Loss: 0.00001573
Iteration 38/1000 | Loss: 0.00001573
Iteration 39/1000 | Loss: 0.00001573
Iteration 40/1000 | Loss: 0.00001573
Iteration 41/1000 | Loss: 0.00001573
Iteration 42/1000 | Loss: 0.00001573
Iteration 43/1000 | Loss: 0.00001572
Iteration 44/1000 | Loss: 0.00001571
Iteration 45/1000 | Loss: 0.00001570
Iteration 46/1000 | Loss: 0.00001569
Iteration 47/1000 | Loss: 0.00001569
Iteration 48/1000 | Loss: 0.00001569
Iteration 49/1000 | Loss: 0.00001568
Iteration 50/1000 | Loss: 0.00001567
Iteration 51/1000 | Loss: 0.00001567
Iteration 52/1000 | Loss: 0.00001567
Iteration 53/1000 | Loss: 0.00001567
Iteration 54/1000 | Loss: 0.00001567
Iteration 55/1000 | Loss: 0.00001567
Iteration 56/1000 | Loss: 0.00001567
Iteration 57/1000 | Loss: 0.00001567
Iteration 58/1000 | Loss: 0.00001566
Iteration 59/1000 | Loss: 0.00001566
Iteration 60/1000 | Loss: 0.00001566
Iteration 61/1000 | Loss: 0.00001566
Iteration 62/1000 | Loss: 0.00001566
Iteration 63/1000 | Loss: 0.00001565
Iteration 64/1000 | Loss: 0.00001565
Iteration 65/1000 | Loss: 0.00001565
Iteration 66/1000 | Loss: 0.00001565
Iteration 67/1000 | Loss: 0.00001565
Iteration 68/1000 | Loss: 0.00001565
Iteration 69/1000 | Loss: 0.00001565
Iteration 70/1000 | Loss: 0.00001565
Iteration 71/1000 | Loss: 0.00001564
Iteration 72/1000 | Loss: 0.00001564
Iteration 73/1000 | Loss: 0.00001564
Iteration 74/1000 | Loss: 0.00001564
Iteration 75/1000 | Loss: 0.00001564
Iteration 76/1000 | Loss: 0.00001564
Iteration 77/1000 | Loss: 0.00001564
Iteration 78/1000 | Loss: 0.00001564
Iteration 79/1000 | Loss: 0.00001563
Iteration 80/1000 | Loss: 0.00001563
Iteration 81/1000 | Loss: 0.00001563
Iteration 82/1000 | Loss: 0.00001563
Iteration 83/1000 | Loss: 0.00001563
Iteration 84/1000 | Loss: 0.00001562
Iteration 85/1000 | Loss: 0.00001562
Iteration 86/1000 | Loss: 0.00001562
Iteration 87/1000 | Loss: 0.00001562
Iteration 88/1000 | Loss: 0.00001561
Iteration 89/1000 | Loss: 0.00001561
Iteration 90/1000 | Loss: 0.00001561
Iteration 91/1000 | Loss: 0.00001560
Iteration 92/1000 | Loss: 0.00001560
Iteration 93/1000 | Loss: 0.00001560
Iteration 94/1000 | Loss: 0.00001559
Iteration 95/1000 | Loss: 0.00001559
Iteration 96/1000 | Loss: 0.00001559
Iteration 97/1000 | Loss: 0.00001559
Iteration 98/1000 | Loss: 0.00001558
Iteration 99/1000 | Loss: 0.00001558
Iteration 100/1000 | Loss: 0.00001557
Iteration 101/1000 | Loss: 0.00001557
Iteration 102/1000 | Loss: 0.00001557
Iteration 103/1000 | Loss: 0.00001557
Iteration 104/1000 | Loss: 0.00001557
Iteration 105/1000 | Loss: 0.00001556
Iteration 106/1000 | Loss: 0.00001556
Iteration 107/1000 | Loss: 0.00001556
Iteration 108/1000 | Loss: 0.00001555
Iteration 109/1000 | Loss: 0.00001555
Iteration 110/1000 | Loss: 0.00001555
Iteration 111/1000 | Loss: 0.00001555
Iteration 112/1000 | Loss: 0.00001554
Iteration 113/1000 | Loss: 0.00001554
Iteration 114/1000 | Loss: 0.00001554
Iteration 115/1000 | Loss: 0.00001554
Iteration 116/1000 | Loss: 0.00001554
Iteration 117/1000 | Loss: 0.00001554
Iteration 118/1000 | Loss: 0.00001554
Iteration 119/1000 | Loss: 0.00001554
Iteration 120/1000 | Loss: 0.00001554
Iteration 121/1000 | Loss: 0.00001554
Iteration 122/1000 | Loss: 0.00001554
Iteration 123/1000 | Loss: 0.00001554
Iteration 124/1000 | Loss: 0.00001554
Iteration 125/1000 | Loss: 0.00001554
Iteration 126/1000 | Loss: 0.00001554
Iteration 127/1000 | Loss: 0.00001554
Iteration 128/1000 | Loss: 0.00001554
Iteration 129/1000 | Loss: 0.00001554
Iteration 130/1000 | Loss: 0.00001554
Iteration 131/1000 | Loss: 0.00001554
Iteration 132/1000 | Loss: 0.00001554
Iteration 133/1000 | Loss: 0.00001554
Iteration 134/1000 | Loss: 0.00001554
Iteration 135/1000 | Loss: 0.00001554
Iteration 136/1000 | Loss: 0.00001554
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [1.5537401850451715e-05, 1.5537401850451715e-05, 1.5537401850451715e-05, 1.5537401850451715e-05, 1.5537401850451715e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5537401850451715e-05

Optimization complete. Final v2v error: 3.3198647499084473 mm

Highest mean error: 3.9229369163513184 mm for frame 77

Lowest mean error: 3.1873085498809814 mm for frame 62

Saving results

Total time: 33.066768646240234
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00913044
Iteration 2/25 | Loss: 0.00135821
Iteration 3/25 | Loss: 0.00090332
Iteration 4/25 | Loss: 0.00082364
Iteration 5/25 | Loss: 0.00077720
Iteration 6/25 | Loss: 0.00077010
Iteration 7/25 | Loss: 0.00075967
Iteration 8/25 | Loss: 0.00075786
Iteration 9/25 | Loss: 0.00075228
Iteration 10/25 | Loss: 0.00074996
Iteration 11/25 | Loss: 0.00074923
Iteration 12/25 | Loss: 0.00074901
Iteration 13/25 | Loss: 0.00074888
Iteration 14/25 | Loss: 0.00074866
Iteration 15/25 | Loss: 0.00074757
Iteration 16/25 | Loss: 0.00074685
Iteration 17/25 | Loss: 0.00074658
Iteration 18/25 | Loss: 0.00074645
Iteration 19/25 | Loss: 0.00074642
Iteration 20/25 | Loss: 0.00074642
Iteration 21/25 | Loss: 0.00074642
Iteration 22/25 | Loss: 0.00074642
Iteration 23/25 | Loss: 0.00074641
Iteration 24/25 | Loss: 0.00074641
Iteration 25/25 | Loss: 0.00074641

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.50993848
Iteration 2/25 | Loss: 0.00026552
Iteration 3/25 | Loss: 0.00026548
Iteration 4/25 | Loss: 0.00026548
Iteration 5/25 | Loss: 0.00026548
Iteration 6/25 | Loss: 0.00026548
Iteration 7/25 | Loss: 0.00026548
Iteration 8/25 | Loss: 0.00026548
Iteration 9/25 | Loss: 0.00026548
Iteration 10/25 | Loss: 0.00026548
Iteration 11/25 | Loss: 0.00026548
Iteration 12/25 | Loss: 0.00026548
Iteration 13/25 | Loss: 0.00026548
Iteration 14/25 | Loss: 0.00026548
Iteration 15/25 | Loss: 0.00026548
Iteration 16/25 | Loss: 0.00026548
Iteration 17/25 | Loss: 0.00026548
Iteration 18/25 | Loss: 0.00026548
Iteration 19/25 | Loss: 0.00026548
Iteration 20/25 | Loss: 0.00026548
Iteration 21/25 | Loss: 0.00026548
Iteration 22/25 | Loss: 0.00026548
Iteration 23/25 | Loss: 0.00026548
Iteration 24/25 | Loss: 0.00026548
Iteration 25/25 | Loss: 0.00026548

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026548
Iteration 2/1000 | Loss: 0.00003996
Iteration 3/1000 | Loss: 0.00002751
Iteration 4/1000 | Loss: 0.00002571
Iteration 5/1000 | Loss: 0.00002455
Iteration 6/1000 | Loss: 0.00002375
Iteration 7/1000 | Loss: 0.00002298
Iteration 8/1000 | Loss: 0.00002256
Iteration 9/1000 | Loss: 0.00002230
Iteration 10/1000 | Loss: 0.00002213
Iteration 11/1000 | Loss: 0.00002212
Iteration 12/1000 | Loss: 0.00002209
Iteration 13/1000 | Loss: 0.00002208
Iteration 14/1000 | Loss: 0.00002208
Iteration 15/1000 | Loss: 0.00002207
Iteration 16/1000 | Loss: 0.00002207
Iteration 17/1000 | Loss: 0.00002206
Iteration 18/1000 | Loss: 0.00002205
Iteration 19/1000 | Loss: 0.00002205
Iteration 20/1000 | Loss: 0.00002204
Iteration 21/1000 | Loss: 0.00002204
Iteration 22/1000 | Loss: 0.00002204
Iteration 23/1000 | Loss: 0.00002203
Iteration 24/1000 | Loss: 0.00002203
Iteration 25/1000 | Loss: 0.00002201
Iteration 26/1000 | Loss: 0.00002201
Iteration 27/1000 | Loss: 0.00002201
Iteration 28/1000 | Loss: 0.00002201
Iteration 29/1000 | Loss: 0.00002200
Iteration 30/1000 | Loss: 0.00002200
Iteration 31/1000 | Loss: 0.00002200
Iteration 32/1000 | Loss: 0.00002199
Iteration 33/1000 | Loss: 0.00002199
Iteration 34/1000 | Loss: 0.00002199
Iteration 35/1000 | Loss: 0.00002198
Iteration 36/1000 | Loss: 0.00002198
Iteration 37/1000 | Loss: 0.00002198
Iteration 38/1000 | Loss: 0.00002198
Iteration 39/1000 | Loss: 0.00002198
Iteration 40/1000 | Loss: 0.00002197
Iteration 41/1000 | Loss: 0.00002197
Iteration 42/1000 | Loss: 0.00002197
Iteration 43/1000 | Loss: 0.00002197
Iteration 44/1000 | Loss: 0.00002197
Iteration 45/1000 | Loss: 0.00002196
Iteration 46/1000 | Loss: 0.00002196
Iteration 47/1000 | Loss: 0.00002196
Iteration 48/1000 | Loss: 0.00002195
Iteration 49/1000 | Loss: 0.00002195
Iteration 50/1000 | Loss: 0.00002195
Iteration 51/1000 | Loss: 0.00002195
Iteration 52/1000 | Loss: 0.00002195
Iteration 53/1000 | Loss: 0.00002195
Iteration 54/1000 | Loss: 0.00002194
Iteration 55/1000 | Loss: 0.00002194
Iteration 56/1000 | Loss: 0.00002194
Iteration 57/1000 | Loss: 0.00002194
Iteration 58/1000 | Loss: 0.00002194
Iteration 59/1000 | Loss: 0.00002194
Iteration 60/1000 | Loss: 0.00002194
Iteration 61/1000 | Loss: 0.00002194
Iteration 62/1000 | Loss: 0.00002194
Iteration 63/1000 | Loss: 0.00002194
Iteration 64/1000 | Loss: 0.00002194
Iteration 65/1000 | Loss: 0.00002194
Iteration 66/1000 | Loss: 0.00002194
Iteration 67/1000 | Loss: 0.00002193
Iteration 68/1000 | Loss: 0.00002193
Iteration 69/1000 | Loss: 0.00002193
Iteration 70/1000 | Loss: 0.00002193
Iteration 71/1000 | Loss: 0.00002193
Iteration 72/1000 | Loss: 0.00002193
Iteration 73/1000 | Loss: 0.00002193
Iteration 74/1000 | Loss: 0.00002193
Iteration 75/1000 | Loss: 0.00002193
Iteration 76/1000 | Loss: 0.00002193
Iteration 77/1000 | Loss: 0.00002193
Iteration 78/1000 | Loss: 0.00002193
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 78. Stopping optimization.
Last 5 losses: [2.1929499780526385e-05, 2.1929499780526385e-05, 2.1929499780526385e-05, 2.1929499780526385e-05, 2.1929499780526385e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1929499780526385e-05

Optimization complete. Final v2v error: 3.9189984798431396 mm

Highest mean error: 4.699972629547119 mm for frame 12

Lowest mean error: 3.4290218353271484 mm for frame 201

Saving results

Total time: 58.49136137962341
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00958861
Iteration 2/25 | Loss: 0.00150257
Iteration 3/25 | Loss: 0.00087546
Iteration 4/25 | Loss: 0.00078469
Iteration 5/25 | Loss: 0.00076026
Iteration 6/25 | Loss: 0.00075021
Iteration 7/25 | Loss: 0.00074860
Iteration 8/25 | Loss: 0.00074833
Iteration 9/25 | Loss: 0.00074833
Iteration 10/25 | Loss: 0.00074833
Iteration 11/25 | Loss: 0.00074833
Iteration 12/25 | Loss: 0.00074832
Iteration 13/25 | Loss: 0.00074832
Iteration 14/25 | Loss: 0.00074832
Iteration 15/25 | Loss: 0.00074832
Iteration 16/25 | Loss: 0.00074832
Iteration 17/25 | Loss: 0.00074832
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007483228109776974, 0.0007483228109776974, 0.0007483228109776974, 0.0007483228109776974, 0.0007483228109776974]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007483228109776974

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14740360
Iteration 2/25 | Loss: 0.00025992
Iteration 3/25 | Loss: 0.00025991
Iteration 4/25 | Loss: 0.00025991
Iteration 5/25 | Loss: 0.00025991
Iteration 6/25 | Loss: 0.00025991
Iteration 7/25 | Loss: 0.00025991
Iteration 8/25 | Loss: 0.00025991
Iteration 9/25 | Loss: 0.00025991
Iteration 10/25 | Loss: 0.00025991
Iteration 11/25 | Loss: 0.00025991
Iteration 12/25 | Loss: 0.00025991
Iteration 13/25 | Loss: 0.00025991
Iteration 14/25 | Loss: 0.00025991
Iteration 15/25 | Loss: 0.00025991
Iteration 16/25 | Loss: 0.00025991
Iteration 17/25 | Loss: 0.00025991
Iteration 18/25 | Loss: 0.00025991
Iteration 19/25 | Loss: 0.00025991
Iteration 20/25 | Loss: 0.00025991
Iteration 21/25 | Loss: 0.00025991
Iteration 22/25 | Loss: 0.00025991
Iteration 23/25 | Loss: 0.00025991
Iteration 24/25 | Loss: 0.00025991
Iteration 25/25 | Loss: 0.00025991

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025991
Iteration 2/1000 | Loss: 0.00005233
Iteration 3/1000 | Loss: 0.00003831
Iteration 4/1000 | Loss: 0.00003533
Iteration 5/1000 | Loss: 0.00003378
Iteration 6/1000 | Loss: 0.00003243
Iteration 7/1000 | Loss: 0.00003142
Iteration 8/1000 | Loss: 0.00003051
Iteration 9/1000 | Loss: 0.00003010
Iteration 10/1000 | Loss: 0.00002983
Iteration 11/1000 | Loss: 0.00002975
Iteration 12/1000 | Loss: 0.00002963
Iteration 13/1000 | Loss: 0.00002944
Iteration 14/1000 | Loss: 0.00002930
Iteration 15/1000 | Loss: 0.00002928
Iteration 16/1000 | Loss: 0.00002924
Iteration 17/1000 | Loss: 0.00002920
Iteration 18/1000 | Loss: 0.00002920
Iteration 19/1000 | Loss: 0.00002920
Iteration 20/1000 | Loss: 0.00002919
Iteration 21/1000 | Loss: 0.00002919
Iteration 22/1000 | Loss: 0.00002919
Iteration 23/1000 | Loss: 0.00002918
Iteration 24/1000 | Loss: 0.00002917
Iteration 25/1000 | Loss: 0.00002917
Iteration 26/1000 | Loss: 0.00002911
Iteration 27/1000 | Loss: 0.00002910
Iteration 28/1000 | Loss: 0.00002910
Iteration 29/1000 | Loss: 0.00002909
Iteration 30/1000 | Loss: 0.00002907
Iteration 31/1000 | Loss: 0.00002907
Iteration 32/1000 | Loss: 0.00002906
Iteration 33/1000 | Loss: 0.00002905
Iteration 34/1000 | Loss: 0.00002905
Iteration 35/1000 | Loss: 0.00002904
Iteration 36/1000 | Loss: 0.00002904
Iteration 37/1000 | Loss: 0.00002900
Iteration 38/1000 | Loss: 0.00002893
Iteration 39/1000 | Loss: 0.00002893
Iteration 40/1000 | Loss: 0.00002885
Iteration 41/1000 | Loss: 0.00002882
Iteration 42/1000 | Loss: 0.00002882
Iteration 43/1000 | Loss: 0.00002877
Iteration 44/1000 | Loss: 0.00002874
Iteration 45/1000 | Loss: 0.00002873
Iteration 46/1000 | Loss: 0.00002873
Iteration 47/1000 | Loss: 0.00002870
Iteration 48/1000 | Loss: 0.00002869
Iteration 49/1000 | Loss: 0.00002869
Iteration 50/1000 | Loss: 0.00002869
Iteration 51/1000 | Loss: 0.00002868
Iteration 52/1000 | Loss: 0.00002868
Iteration 53/1000 | Loss: 0.00002868
Iteration 54/1000 | Loss: 0.00002867
Iteration 55/1000 | Loss: 0.00002862
Iteration 56/1000 | Loss: 0.00002861
Iteration 57/1000 | Loss: 0.00002861
Iteration 58/1000 | Loss: 0.00002860
Iteration 59/1000 | Loss: 0.00002859
Iteration 60/1000 | Loss: 0.00002859
Iteration 61/1000 | Loss: 0.00002859
Iteration 62/1000 | Loss: 0.00002858
Iteration 63/1000 | Loss: 0.00002858
Iteration 64/1000 | Loss: 0.00002857
Iteration 65/1000 | Loss: 0.00002857
Iteration 66/1000 | Loss: 0.00002856
Iteration 67/1000 | Loss: 0.00002856
Iteration 68/1000 | Loss: 0.00002855
Iteration 69/1000 | Loss: 0.00002855
Iteration 70/1000 | Loss: 0.00002855
Iteration 71/1000 | Loss: 0.00002855
Iteration 72/1000 | Loss: 0.00002854
Iteration 73/1000 | Loss: 0.00002854
Iteration 74/1000 | Loss: 0.00002853
Iteration 75/1000 | Loss: 0.00002853
Iteration 76/1000 | Loss: 0.00002853
Iteration 77/1000 | Loss: 0.00002853
Iteration 78/1000 | Loss: 0.00002853
Iteration 79/1000 | Loss: 0.00002853
Iteration 80/1000 | Loss: 0.00002853
Iteration 81/1000 | Loss: 0.00002853
Iteration 82/1000 | Loss: 0.00002853
Iteration 83/1000 | Loss: 0.00002852
Iteration 84/1000 | Loss: 0.00002852
Iteration 85/1000 | Loss: 0.00002852
Iteration 86/1000 | Loss: 0.00002852
Iteration 87/1000 | Loss: 0.00002852
Iteration 88/1000 | Loss: 0.00002852
Iteration 89/1000 | Loss: 0.00002851
Iteration 90/1000 | Loss: 0.00002851
Iteration 91/1000 | Loss: 0.00002851
Iteration 92/1000 | Loss: 0.00002850
Iteration 93/1000 | Loss: 0.00002850
Iteration 94/1000 | Loss: 0.00002850
Iteration 95/1000 | Loss: 0.00002850
Iteration 96/1000 | Loss: 0.00002850
Iteration 97/1000 | Loss: 0.00002850
Iteration 98/1000 | Loss: 0.00002850
Iteration 99/1000 | Loss: 0.00002849
Iteration 100/1000 | Loss: 0.00002849
Iteration 101/1000 | Loss: 0.00002849
Iteration 102/1000 | Loss: 0.00002849
Iteration 103/1000 | Loss: 0.00002849
Iteration 104/1000 | Loss: 0.00002849
Iteration 105/1000 | Loss: 0.00002848
Iteration 106/1000 | Loss: 0.00002848
Iteration 107/1000 | Loss: 0.00002848
Iteration 108/1000 | Loss: 0.00002848
Iteration 109/1000 | Loss: 0.00002848
Iteration 110/1000 | Loss: 0.00002848
Iteration 111/1000 | Loss: 0.00002848
Iteration 112/1000 | Loss: 0.00002848
Iteration 113/1000 | Loss: 0.00002847
Iteration 114/1000 | Loss: 0.00002847
Iteration 115/1000 | Loss: 0.00002847
Iteration 116/1000 | Loss: 0.00002847
Iteration 117/1000 | Loss: 0.00002847
Iteration 118/1000 | Loss: 0.00002847
Iteration 119/1000 | Loss: 0.00002847
Iteration 120/1000 | Loss: 0.00002847
Iteration 121/1000 | Loss: 0.00002847
Iteration 122/1000 | Loss: 0.00002847
Iteration 123/1000 | Loss: 0.00002847
Iteration 124/1000 | Loss: 0.00002847
Iteration 125/1000 | Loss: 0.00002847
Iteration 126/1000 | Loss: 0.00002847
Iteration 127/1000 | Loss: 0.00002847
Iteration 128/1000 | Loss: 0.00002847
Iteration 129/1000 | Loss: 0.00002847
Iteration 130/1000 | Loss: 0.00002847
Iteration 131/1000 | Loss: 0.00002847
Iteration 132/1000 | Loss: 0.00002847
Iteration 133/1000 | Loss: 0.00002847
Iteration 134/1000 | Loss: 0.00002847
Iteration 135/1000 | Loss: 0.00002847
Iteration 136/1000 | Loss: 0.00002847
Iteration 137/1000 | Loss: 0.00002847
Iteration 138/1000 | Loss: 0.00002847
Iteration 139/1000 | Loss: 0.00002847
Iteration 140/1000 | Loss: 0.00002847
Iteration 141/1000 | Loss: 0.00002847
Iteration 142/1000 | Loss: 0.00002847
Iteration 143/1000 | Loss: 0.00002847
Iteration 144/1000 | Loss: 0.00002847
Iteration 145/1000 | Loss: 0.00002847
Iteration 146/1000 | Loss: 0.00002847
Iteration 147/1000 | Loss: 0.00002847
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [2.847149153240025e-05, 2.847149153240025e-05, 2.847149153240025e-05, 2.847149153240025e-05, 2.847149153240025e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.847149153240025e-05

Optimization complete. Final v2v error: 4.263614654541016 mm

Highest mean error: 5.46269416809082 mm for frame 86

Lowest mean error: 3.463852882385254 mm for frame 113

Saving results

Total time: 46.73999571800232
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01073677
Iteration 2/25 | Loss: 0.00220237
Iteration 3/25 | Loss: 0.00154489
Iteration 4/25 | Loss: 0.00142332
Iteration 5/25 | Loss: 0.00109708
Iteration 6/25 | Loss: 0.00105155
Iteration 7/25 | Loss: 0.00105572
Iteration 8/25 | Loss: 0.00105652
Iteration 9/25 | Loss: 0.00097157
Iteration 10/25 | Loss: 0.00095800
Iteration 11/25 | Loss: 0.00099688
Iteration 12/25 | Loss: 0.00095142
Iteration 13/25 | Loss: 0.00097128
Iteration 14/25 | Loss: 0.00097499
Iteration 15/25 | Loss: 0.00095797
Iteration 16/25 | Loss: 0.00095248
Iteration 17/25 | Loss: 0.00094764
Iteration 18/25 | Loss: 0.00094102
Iteration 19/25 | Loss: 0.00091619
Iteration 20/25 | Loss: 0.00090667
Iteration 21/25 | Loss: 0.00094592
Iteration 22/25 | Loss: 0.00089480
Iteration 23/25 | Loss: 0.00088498
Iteration 24/25 | Loss: 0.00088264
Iteration 25/25 | Loss: 0.00088225

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49952722
Iteration 2/25 | Loss: 0.00191221
Iteration 3/25 | Loss: 0.00149083
Iteration 4/25 | Loss: 0.00149083
Iteration 5/25 | Loss: 0.00149083
Iteration 6/25 | Loss: 0.00149083
Iteration 7/25 | Loss: 0.00149083
Iteration 8/25 | Loss: 0.00149083
Iteration 9/25 | Loss: 0.00149083
Iteration 10/25 | Loss: 0.00149083
Iteration 11/25 | Loss: 0.00149083
Iteration 12/25 | Loss: 0.00149083
Iteration 13/25 | Loss: 0.00149083
Iteration 14/25 | Loss: 0.00149083
Iteration 15/25 | Loss: 0.00149083
Iteration 16/25 | Loss: 0.00149083
Iteration 17/25 | Loss: 0.00149083
Iteration 18/25 | Loss: 0.00149083
Iteration 19/25 | Loss: 0.00149083
Iteration 20/25 | Loss: 0.00149083
Iteration 21/25 | Loss: 0.00149083
Iteration 22/25 | Loss: 0.00149083
Iteration 23/25 | Loss: 0.00149083
Iteration 24/25 | Loss: 0.00149083
Iteration 25/25 | Loss: 0.00149083
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0014908278826624155, 0.0014908278826624155, 0.0014908278826624155, 0.0014908278826624155, 0.0014908278826624155]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014908278826624155

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00149083
Iteration 2/1000 | Loss: 0.00043694
Iteration 3/1000 | Loss: 0.00095247
Iteration 4/1000 | Loss: 0.00048359
Iteration 5/1000 | Loss: 0.00051396
Iteration 6/1000 | Loss: 0.00043328
Iteration 7/1000 | Loss: 0.00014248
Iteration 8/1000 | Loss: 0.00056061
Iteration 9/1000 | Loss: 0.00024336
Iteration 10/1000 | Loss: 0.00041074
Iteration 11/1000 | Loss: 0.00051103
Iteration 12/1000 | Loss: 0.00014523
Iteration 13/1000 | Loss: 0.00037871
Iteration 14/1000 | Loss: 0.00197180
Iteration 15/1000 | Loss: 0.00148485
Iteration 16/1000 | Loss: 0.00402213
Iteration 17/1000 | Loss: 0.00065560
Iteration 18/1000 | Loss: 0.00058563
Iteration 19/1000 | Loss: 0.00049925
Iteration 20/1000 | Loss: 0.00031176
Iteration 21/1000 | Loss: 0.00081779
Iteration 22/1000 | Loss: 0.00029256
Iteration 23/1000 | Loss: 0.00025138
Iteration 24/1000 | Loss: 0.00021067
Iteration 25/1000 | Loss: 0.00014802
Iteration 26/1000 | Loss: 0.00077063
Iteration 27/1000 | Loss: 0.00018693
Iteration 28/1000 | Loss: 0.00005766
Iteration 29/1000 | Loss: 0.00041756
Iteration 30/1000 | Loss: 0.00017289
Iteration 31/1000 | Loss: 0.00014154
Iteration 32/1000 | Loss: 0.00024854
Iteration 33/1000 | Loss: 0.00013982
Iteration 34/1000 | Loss: 0.00012908
Iteration 35/1000 | Loss: 0.00019024
Iteration 36/1000 | Loss: 0.00006212
Iteration 37/1000 | Loss: 0.00008435
Iteration 38/1000 | Loss: 0.00016232
Iteration 39/1000 | Loss: 0.00004151
Iteration 40/1000 | Loss: 0.00014030
Iteration 41/1000 | Loss: 0.00046449
Iteration 42/1000 | Loss: 0.00023831
Iteration 43/1000 | Loss: 0.00027757
Iteration 44/1000 | Loss: 0.00014635
Iteration 45/1000 | Loss: 0.00045040
Iteration 46/1000 | Loss: 0.00013954
Iteration 47/1000 | Loss: 0.00031734
Iteration 48/1000 | Loss: 0.00004447
Iteration 49/1000 | Loss: 0.00002704
Iteration 50/1000 | Loss: 0.00002399
Iteration 51/1000 | Loss: 0.00002241
Iteration 52/1000 | Loss: 0.00002130
Iteration 53/1000 | Loss: 0.00002049
Iteration 54/1000 | Loss: 0.00001988
Iteration 55/1000 | Loss: 0.00001949
Iteration 56/1000 | Loss: 0.00001920
Iteration 57/1000 | Loss: 0.00001897
Iteration 58/1000 | Loss: 0.00001879
Iteration 59/1000 | Loss: 0.00001873
Iteration 60/1000 | Loss: 0.00001864
Iteration 61/1000 | Loss: 0.00001863
Iteration 62/1000 | Loss: 0.00001857
Iteration 63/1000 | Loss: 0.00001857
Iteration 64/1000 | Loss: 0.00001856
Iteration 65/1000 | Loss: 0.00001856
Iteration 66/1000 | Loss: 0.00001856
Iteration 67/1000 | Loss: 0.00001855
Iteration 68/1000 | Loss: 0.00001855
Iteration 69/1000 | Loss: 0.00001855
Iteration 70/1000 | Loss: 0.00005621
Iteration 71/1000 | Loss: 0.00003691
Iteration 72/1000 | Loss: 0.00005243
Iteration 73/1000 | Loss: 0.00002415
Iteration 74/1000 | Loss: 0.00002214
Iteration 75/1000 | Loss: 0.00003116
Iteration 76/1000 | Loss: 0.00005036
Iteration 77/1000 | Loss: 0.00002528
Iteration 78/1000 | Loss: 0.00001920
Iteration 79/1000 | Loss: 0.00004805
Iteration 80/1000 | Loss: 0.00002803
Iteration 81/1000 | Loss: 0.00004522
Iteration 82/1000 | Loss: 0.00002532
Iteration 83/1000 | Loss: 0.00001893
Iteration 84/1000 | Loss: 0.00001861
Iteration 85/1000 | Loss: 0.00001853
Iteration 86/1000 | Loss: 0.00001853
Iteration 87/1000 | Loss: 0.00001853
Iteration 88/1000 | Loss: 0.00001852
Iteration 89/1000 | Loss: 0.00001852
Iteration 90/1000 | Loss: 0.00001852
Iteration 91/1000 | Loss: 0.00001852
Iteration 92/1000 | Loss: 0.00001852
Iteration 93/1000 | Loss: 0.00001851
Iteration 94/1000 | Loss: 0.00001850
Iteration 95/1000 | Loss: 0.00001850
Iteration 96/1000 | Loss: 0.00001850
Iteration 97/1000 | Loss: 0.00001850
Iteration 98/1000 | Loss: 0.00001849
Iteration 99/1000 | Loss: 0.00001849
Iteration 100/1000 | Loss: 0.00001848
Iteration 101/1000 | Loss: 0.00001848
Iteration 102/1000 | Loss: 0.00001848
Iteration 103/1000 | Loss: 0.00001847
Iteration 104/1000 | Loss: 0.00001847
Iteration 105/1000 | Loss: 0.00001847
Iteration 106/1000 | Loss: 0.00001847
Iteration 107/1000 | Loss: 0.00001847
Iteration 108/1000 | Loss: 0.00001847
Iteration 109/1000 | Loss: 0.00001846
Iteration 110/1000 | Loss: 0.00001846
Iteration 111/1000 | Loss: 0.00001846
Iteration 112/1000 | Loss: 0.00001846
Iteration 113/1000 | Loss: 0.00001846
Iteration 114/1000 | Loss: 0.00001846
Iteration 115/1000 | Loss: 0.00001846
Iteration 116/1000 | Loss: 0.00001846
Iteration 117/1000 | Loss: 0.00001846
Iteration 118/1000 | Loss: 0.00001846
Iteration 119/1000 | Loss: 0.00001846
Iteration 120/1000 | Loss: 0.00001846
Iteration 121/1000 | Loss: 0.00001846
Iteration 122/1000 | Loss: 0.00001846
Iteration 123/1000 | Loss: 0.00001846
Iteration 124/1000 | Loss: 0.00001845
Iteration 125/1000 | Loss: 0.00001845
Iteration 126/1000 | Loss: 0.00001845
Iteration 127/1000 | Loss: 0.00001845
Iteration 128/1000 | Loss: 0.00001845
Iteration 129/1000 | Loss: 0.00001845
Iteration 130/1000 | Loss: 0.00001845
Iteration 131/1000 | Loss: 0.00001845
Iteration 132/1000 | Loss: 0.00001845
Iteration 133/1000 | Loss: 0.00001844
Iteration 134/1000 | Loss: 0.00001844
Iteration 135/1000 | Loss: 0.00001844
Iteration 136/1000 | Loss: 0.00001843
Iteration 137/1000 | Loss: 0.00001843
Iteration 138/1000 | Loss: 0.00001843
Iteration 139/1000 | Loss: 0.00001843
Iteration 140/1000 | Loss: 0.00001842
Iteration 141/1000 | Loss: 0.00001842
Iteration 142/1000 | Loss: 0.00001842
Iteration 143/1000 | Loss: 0.00001841
Iteration 144/1000 | Loss: 0.00001841
Iteration 145/1000 | Loss: 0.00001841
Iteration 146/1000 | Loss: 0.00001841
Iteration 147/1000 | Loss: 0.00001841
Iteration 148/1000 | Loss: 0.00001840
Iteration 149/1000 | Loss: 0.00001840
Iteration 150/1000 | Loss: 0.00001840
Iteration 151/1000 | Loss: 0.00001840
Iteration 152/1000 | Loss: 0.00001840
Iteration 153/1000 | Loss: 0.00001840
Iteration 154/1000 | Loss: 0.00001840
Iteration 155/1000 | Loss: 0.00001840
Iteration 156/1000 | Loss: 0.00001840
Iteration 157/1000 | Loss: 0.00001840
Iteration 158/1000 | Loss: 0.00001840
Iteration 159/1000 | Loss: 0.00001840
Iteration 160/1000 | Loss: 0.00001840
Iteration 161/1000 | Loss: 0.00001840
Iteration 162/1000 | Loss: 0.00001840
Iteration 163/1000 | Loss: 0.00001840
Iteration 164/1000 | Loss: 0.00001839
Iteration 165/1000 | Loss: 0.00001839
Iteration 166/1000 | Loss: 0.00001839
Iteration 167/1000 | Loss: 0.00001839
Iteration 168/1000 | Loss: 0.00001839
Iteration 169/1000 | Loss: 0.00001839
Iteration 170/1000 | Loss: 0.00001839
Iteration 171/1000 | Loss: 0.00001839
Iteration 172/1000 | Loss: 0.00001839
Iteration 173/1000 | Loss: 0.00001839
Iteration 174/1000 | Loss: 0.00001838
Iteration 175/1000 | Loss: 0.00001838
Iteration 176/1000 | Loss: 0.00001838
Iteration 177/1000 | Loss: 0.00001838
Iteration 178/1000 | Loss: 0.00001838
Iteration 179/1000 | Loss: 0.00001838
Iteration 180/1000 | Loss: 0.00005798
Iteration 181/1000 | Loss: 0.00002332
Iteration 182/1000 | Loss: 0.00003936
Iteration 183/1000 | Loss: 0.00002370
Iteration 184/1000 | Loss: 0.00006030
Iteration 185/1000 | Loss: 0.00004996
Iteration 186/1000 | Loss: 0.00005280
Iteration 187/1000 | Loss: 0.00002265
Iteration 188/1000 | Loss: 0.00002860
Iteration 189/1000 | Loss: 0.00005206
Iteration 190/1000 | Loss: 0.00003072
Iteration 191/1000 | Loss: 0.00001859
Iteration 192/1000 | Loss: 0.00005273
Iteration 193/1000 | Loss: 0.00002721
Iteration 194/1000 | Loss: 0.00001850
Iteration 195/1000 | Loss: 0.00001845
Iteration 196/1000 | Loss: 0.00001844
Iteration 197/1000 | Loss: 0.00001844
Iteration 198/1000 | Loss: 0.00001843
Iteration 199/1000 | Loss: 0.00001843
Iteration 200/1000 | Loss: 0.00001843
Iteration 201/1000 | Loss: 0.00001843
Iteration 202/1000 | Loss: 0.00005470
Iteration 203/1000 | Loss: 0.00002976
Iteration 204/1000 | Loss: 0.00001844
Iteration 205/1000 | Loss: 0.00001844
Iteration 206/1000 | Loss: 0.00001844
Iteration 207/1000 | Loss: 0.00001844
Iteration 208/1000 | Loss: 0.00001844
Iteration 209/1000 | Loss: 0.00001844
Iteration 210/1000 | Loss: 0.00001844
Iteration 211/1000 | Loss: 0.00001844
Iteration 212/1000 | Loss: 0.00001844
Iteration 213/1000 | Loss: 0.00001843
Iteration 214/1000 | Loss: 0.00001842
Iteration 215/1000 | Loss: 0.00001842
Iteration 216/1000 | Loss: 0.00001842
Iteration 217/1000 | Loss: 0.00001842
Iteration 218/1000 | Loss: 0.00001841
Iteration 219/1000 | Loss: 0.00001841
Iteration 220/1000 | Loss: 0.00001840
Iteration 221/1000 | Loss: 0.00001840
Iteration 222/1000 | Loss: 0.00001840
Iteration 223/1000 | Loss: 0.00001840
Iteration 224/1000 | Loss: 0.00005517
Iteration 225/1000 | Loss: 0.00002827
Iteration 226/1000 | Loss: 0.00005066
Iteration 227/1000 | Loss: 0.00002431
Iteration 228/1000 | Loss: 0.00001916
Iteration 229/1000 | Loss: 0.00001860
Iteration 230/1000 | Loss: 0.00001850
Iteration 231/1000 | Loss: 0.00001849
Iteration 232/1000 | Loss: 0.00001848
Iteration 233/1000 | Loss: 0.00001847
Iteration 234/1000 | Loss: 0.00005103
Iteration 235/1000 | Loss: 0.00003156
Iteration 236/1000 | Loss: 0.00001930
Iteration 237/1000 | Loss: 0.00001864
Iteration 238/1000 | Loss: 0.00005019
Iteration 239/1000 | Loss: 0.00002679
Iteration 240/1000 | Loss: 0.00003982
Iteration 241/1000 | Loss: 0.00005050
Iteration 242/1000 | Loss: 0.00003778
Iteration 243/1000 | Loss: 0.00005039
Iteration 244/1000 | Loss: 0.00004945
Iteration 245/1000 | Loss: 0.00004945
Iteration 246/1000 | Loss: 0.00004945
Iteration 247/1000 | Loss: 0.00004945
Iteration 248/1000 | Loss: 0.00003691
Iteration 249/1000 | Loss: 0.00005039
Iteration 250/1000 | Loss: 0.00005039
Iteration 251/1000 | Loss: 0.00002449
Iteration 252/1000 | Loss: 0.00004605
Iteration 253/1000 | Loss: 0.00002598
Iteration 254/1000 | Loss: 0.00002378
Iteration 255/1000 | Loss: 0.00002281
Iteration 256/1000 | Loss: 0.00004577
Iteration 257/1000 | Loss: 0.00002282
Iteration 258/1000 | Loss: 0.00002160
Iteration 259/1000 | Loss: 0.00004041
Iteration 260/1000 | Loss: 0.00002227
Iteration 261/1000 | Loss: 0.00002087
Iteration 262/1000 | Loss: 0.00001989
Iteration 263/1000 | Loss: 0.00001917
Iteration 264/1000 | Loss: 0.00001892
Iteration 265/1000 | Loss: 0.00001887
Iteration 266/1000 | Loss: 0.00001883
Iteration 267/1000 | Loss: 0.00001874
Iteration 268/1000 | Loss: 0.00001872
Iteration 269/1000 | Loss: 0.00001871
Iteration 270/1000 | Loss: 0.00001870
Iteration 271/1000 | Loss: 0.00001866
Iteration 272/1000 | Loss: 0.00001855
Iteration 273/1000 | Loss: 0.00001855
Iteration 274/1000 | Loss: 0.00001854
Iteration 275/1000 | Loss: 0.00001853
Iteration 276/1000 | Loss: 0.00001852
Iteration 277/1000 | Loss: 0.00001852
Iteration 278/1000 | Loss: 0.00001852
Iteration 279/1000 | Loss: 0.00001852
Iteration 280/1000 | Loss: 0.00001851
Iteration 281/1000 | Loss: 0.00001851
Iteration 282/1000 | Loss: 0.00001851
Iteration 283/1000 | Loss: 0.00001851
Iteration 284/1000 | Loss: 0.00001851
Iteration 285/1000 | Loss: 0.00001851
Iteration 286/1000 | Loss: 0.00001851
Iteration 287/1000 | Loss: 0.00001851
Iteration 288/1000 | Loss: 0.00001851
Iteration 289/1000 | Loss: 0.00001851
Iteration 290/1000 | Loss: 0.00001851
Iteration 291/1000 | Loss: 0.00001850
Iteration 292/1000 | Loss: 0.00001850
Iteration 293/1000 | Loss: 0.00001850
Iteration 294/1000 | Loss: 0.00001849
Iteration 295/1000 | Loss: 0.00001849
Iteration 296/1000 | Loss: 0.00001849
Iteration 297/1000 | Loss: 0.00001849
Iteration 298/1000 | Loss: 0.00001849
Iteration 299/1000 | Loss: 0.00001849
Iteration 300/1000 | Loss: 0.00001849
Iteration 301/1000 | Loss: 0.00001848
Iteration 302/1000 | Loss: 0.00001848
Iteration 303/1000 | Loss: 0.00001848
Iteration 304/1000 | Loss: 0.00001848
Iteration 305/1000 | Loss: 0.00001848
Iteration 306/1000 | Loss: 0.00001848
Iteration 307/1000 | Loss: 0.00001847
Iteration 308/1000 | Loss: 0.00001847
Iteration 309/1000 | Loss: 0.00001847
Iteration 310/1000 | Loss: 0.00001847
Iteration 311/1000 | Loss: 0.00001847
Iteration 312/1000 | Loss: 0.00001847
Iteration 313/1000 | Loss: 0.00001847
Iteration 314/1000 | Loss: 0.00001846
Iteration 315/1000 | Loss: 0.00001846
Iteration 316/1000 | Loss: 0.00001846
Iteration 317/1000 | Loss: 0.00001846
Iteration 318/1000 | Loss: 0.00001845
Iteration 319/1000 | Loss: 0.00005818
Iteration 320/1000 | Loss: 0.00003077
Iteration 321/1000 | Loss: 0.00002718
Iteration 322/1000 | Loss: 0.00002388
Iteration 323/1000 | Loss: 0.00002225
Iteration 324/1000 | Loss: 0.00003604
Iteration 325/1000 | Loss: 0.00004489
Iteration 326/1000 | Loss: 0.00003014
Iteration 327/1000 | Loss: 0.00004025
Iteration 328/1000 | Loss: 0.00002884
Iteration 329/1000 | Loss: 0.00004064
Iteration 330/1000 | Loss: 0.00002817
Iteration 331/1000 | Loss: 0.00003924
Iteration 332/1000 | Loss: 0.00002867
Iteration 333/1000 | Loss: 0.00001862
Iteration 334/1000 | Loss: 0.00001860
Iteration 335/1000 | Loss: 0.00001860
Iteration 336/1000 | Loss: 0.00001859
Iteration 337/1000 | Loss: 0.00001859
Iteration 338/1000 | Loss: 0.00001859
Iteration 339/1000 | Loss: 0.00001857
Iteration 340/1000 | Loss: 0.00001857
Iteration 341/1000 | Loss: 0.00001857
Iteration 342/1000 | Loss: 0.00001856
Iteration 343/1000 | Loss: 0.00001854
Iteration 344/1000 | Loss: 0.00001853
Iteration 345/1000 | Loss: 0.00001853
Iteration 346/1000 | Loss: 0.00001853
Iteration 347/1000 | Loss: 0.00001852
Iteration 348/1000 | Loss: 0.00001852
Iteration 349/1000 | Loss: 0.00001852
Iteration 350/1000 | Loss: 0.00001852
Iteration 351/1000 | Loss: 0.00001851
Iteration 352/1000 | Loss: 0.00001851
Iteration 353/1000 | Loss: 0.00001851
Iteration 354/1000 | Loss: 0.00001851
Iteration 355/1000 | Loss: 0.00001851
Iteration 356/1000 | Loss: 0.00001851
Iteration 357/1000 | Loss: 0.00001850
Iteration 358/1000 | Loss: 0.00001850
Iteration 359/1000 | Loss: 0.00001849
Iteration 360/1000 | Loss: 0.00001849
Iteration 361/1000 | Loss: 0.00001849
Iteration 362/1000 | Loss: 0.00001849
Iteration 363/1000 | Loss: 0.00001849
Iteration 364/1000 | Loss: 0.00001849
Iteration 365/1000 | Loss: 0.00001849
Iteration 366/1000 | Loss: 0.00001848
Iteration 367/1000 | Loss: 0.00001848
Iteration 368/1000 | Loss: 0.00001848
Iteration 369/1000 | Loss: 0.00001848
Iteration 370/1000 | Loss: 0.00001848
Iteration 371/1000 | Loss: 0.00001848
Iteration 372/1000 | Loss: 0.00001848
Iteration 373/1000 | Loss: 0.00001848
Iteration 374/1000 | Loss: 0.00001847
Iteration 375/1000 | Loss: 0.00001847
Iteration 376/1000 | Loss: 0.00001847
Iteration 377/1000 | Loss: 0.00001847
Iteration 378/1000 | Loss: 0.00001847
Iteration 379/1000 | Loss: 0.00001847
Iteration 380/1000 | Loss: 0.00001847
Iteration 381/1000 | Loss: 0.00001847
Iteration 382/1000 | Loss: 0.00001847
Iteration 383/1000 | Loss: 0.00001847
Iteration 384/1000 | Loss: 0.00001846
Iteration 385/1000 | Loss: 0.00001846
Iteration 386/1000 | Loss: 0.00001846
Iteration 387/1000 | Loss: 0.00001846
Iteration 388/1000 | Loss: 0.00001846
Iteration 389/1000 | Loss: 0.00001845
Iteration 390/1000 | Loss: 0.00001845
Iteration 391/1000 | Loss: 0.00001845
Iteration 392/1000 | Loss: 0.00001845
Iteration 393/1000 | Loss: 0.00001845
Iteration 394/1000 | Loss: 0.00001844
Iteration 395/1000 | Loss: 0.00001844
Iteration 396/1000 | Loss: 0.00001844
Iteration 397/1000 | Loss: 0.00001844
Iteration 398/1000 | Loss: 0.00001843
Iteration 399/1000 | Loss: 0.00001843
Iteration 400/1000 | Loss: 0.00001843
Iteration 401/1000 | Loss: 0.00001843
Iteration 402/1000 | Loss: 0.00001842
Iteration 403/1000 | Loss: 0.00001842
Iteration 404/1000 | Loss: 0.00001842
Iteration 405/1000 | Loss: 0.00001842
Iteration 406/1000 | Loss: 0.00001842
Iteration 407/1000 | Loss: 0.00001842
Iteration 408/1000 | Loss: 0.00001842
Iteration 409/1000 | Loss: 0.00001842
Iteration 410/1000 | Loss: 0.00001842
Iteration 411/1000 | Loss: 0.00001842
Iteration 412/1000 | Loss: 0.00001842
Iteration 413/1000 | Loss: 0.00001841
Iteration 414/1000 | Loss: 0.00001841
Iteration 415/1000 | Loss: 0.00001841
Iteration 416/1000 | Loss: 0.00001841
Iteration 417/1000 | Loss: 0.00001841
Iteration 418/1000 | Loss: 0.00001841
Iteration 419/1000 | Loss: 0.00001840
Iteration 420/1000 | Loss: 0.00001840
Iteration 421/1000 | Loss: 0.00001840
Iteration 422/1000 | Loss: 0.00001840
Iteration 423/1000 | Loss: 0.00001840
Iteration 424/1000 | Loss: 0.00001839
Iteration 425/1000 | Loss: 0.00001839
Iteration 426/1000 | Loss: 0.00001839
Iteration 427/1000 | Loss: 0.00001839
Iteration 428/1000 | Loss: 0.00001839
Iteration 429/1000 | Loss: 0.00001839
Iteration 430/1000 | Loss: 0.00001839
Iteration 431/1000 | Loss: 0.00001839
Iteration 432/1000 | Loss: 0.00001838
Iteration 433/1000 | Loss: 0.00001838
Iteration 434/1000 | Loss: 0.00001838
Iteration 435/1000 | Loss: 0.00001838
Iteration 436/1000 | Loss: 0.00001838
Iteration 437/1000 | Loss: 0.00001838
Iteration 438/1000 | Loss: 0.00001838
Iteration 439/1000 | Loss: 0.00001838
Iteration 440/1000 | Loss: 0.00001837
Iteration 441/1000 | Loss: 0.00001837
Iteration 442/1000 | Loss: 0.00001837
Iteration 443/1000 | Loss: 0.00001837
Iteration 444/1000 | Loss: 0.00001837
Iteration 445/1000 | Loss: 0.00001837
Iteration 446/1000 | Loss: 0.00001837
Iteration 447/1000 | Loss: 0.00001837
Iteration 448/1000 | Loss: 0.00001836
Iteration 449/1000 | Loss: 0.00001836
Iteration 450/1000 | Loss: 0.00001836
Iteration 451/1000 | Loss: 0.00001836
Iteration 452/1000 | Loss: 0.00001836
Iteration 453/1000 | Loss: 0.00001836
Iteration 454/1000 | Loss: 0.00001836
Iteration 455/1000 | Loss: 0.00001836
Iteration 456/1000 | Loss: 0.00001836
Iteration 457/1000 | Loss: 0.00001836
Iteration 458/1000 | Loss: 0.00001836
Iteration 459/1000 | Loss: 0.00001836
Iteration 460/1000 | Loss: 0.00001836
Iteration 461/1000 | Loss: 0.00001836
Iteration 462/1000 | Loss: 0.00001836
Iteration 463/1000 | Loss: 0.00001836
Iteration 464/1000 | Loss: 0.00001836
Iteration 465/1000 | Loss: 0.00001836
Iteration 466/1000 | Loss: 0.00001836
Iteration 467/1000 | Loss: 0.00001836
Iteration 468/1000 | Loss: 0.00001836
Iteration 469/1000 | Loss: 0.00001836
Iteration 470/1000 | Loss: 0.00001836
Iteration 471/1000 | Loss: 0.00001836
Iteration 472/1000 | Loss: 0.00001836
Iteration 473/1000 | Loss: 0.00001836
Iteration 474/1000 | Loss: 0.00001836
Iteration 475/1000 | Loss: 0.00001836
Iteration 476/1000 | Loss: 0.00001836
Iteration 477/1000 | Loss: 0.00001836
Iteration 478/1000 | Loss: 0.00001836
Iteration 479/1000 | Loss: 0.00001836
Iteration 480/1000 | Loss: 0.00001836
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 480. Stopping optimization.
Last 5 losses: [1.8364604329690337e-05, 1.8364604329690337e-05, 1.8364604329690337e-05, 1.8364604329690337e-05, 1.8364604329690337e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8364604329690337e-05

Optimization complete. Final v2v error: 3.4622042179107666 mm

Highest mean error: 10.89339828491211 mm for frame 56

Lowest mean error: 3.123352289199829 mm for frame 55

Saving results

Total time: 255.9145314693451
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00425590
Iteration 2/25 | Loss: 0.00086880
Iteration 3/25 | Loss: 0.00064128
Iteration 4/25 | Loss: 0.00060289
Iteration 5/25 | Loss: 0.00059117
Iteration 6/25 | Loss: 0.00058848
Iteration 7/25 | Loss: 0.00058772
Iteration 8/25 | Loss: 0.00058758
Iteration 9/25 | Loss: 0.00058758
Iteration 10/25 | Loss: 0.00058758
Iteration 11/25 | Loss: 0.00058758
Iteration 12/25 | Loss: 0.00058758
Iteration 13/25 | Loss: 0.00058758
Iteration 14/25 | Loss: 0.00058758
Iteration 15/25 | Loss: 0.00058758
Iteration 16/25 | Loss: 0.00058758
Iteration 17/25 | Loss: 0.00058758
Iteration 18/25 | Loss: 0.00058758
Iteration 19/25 | Loss: 0.00058758
Iteration 20/25 | Loss: 0.00058758
Iteration 21/25 | Loss: 0.00058758
Iteration 22/25 | Loss: 0.00058758
Iteration 23/25 | Loss: 0.00058758
Iteration 24/25 | Loss: 0.00058758
Iteration 25/25 | Loss: 0.00058758

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50062060
Iteration 2/25 | Loss: 0.00025022
Iteration 3/25 | Loss: 0.00025021
Iteration 4/25 | Loss: 0.00025021
Iteration 5/25 | Loss: 0.00025021
Iteration 6/25 | Loss: 0.00025021
Iteration 7/25 | Loss: 0.00025021
Iteration 8/25 | Loss: 0.00025021
Iteration 9/25 | Loss: 0.00025021
Iteration 10/25 | Loss: 0.00025021
Iteration 11/25 | Loss: 0.00025021
Iteration 12/25 | Loss: 0.00025021
Iteration 13/25 | Loss: 0.00025021
Iteration 14/25 | Loss: 0.00025021
Iteration 15/25 | Loss: 0.00025021
Iteration 16/25 | Loss: 0.00025021
Iteration 17/25 | Loss: 0.00025021
Iteration 18/25 | Loss: 0.00025021
Iteration 19/25 | Loss: 0.00025021
Iteration 20/25 | Loss: 0.00025021
Iteration 21/25 | Loss: 0.00025021
Iteration 22/25 | Loss: 0.00025021
Iteration 23/25 | Loss: 0.00025021
Iteration 24/25 | Loss: 0.00025021
Iteration 25/25 | Loss: 0.00025021

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025021
Iteration 2/1000 | Loss: 0.00002629
Iteration 3/1000 | Loss: 0.00001826
Iteration 4/1000 | Loss: 0.00001609
Iteration 5/1000 | Loss: 0.00001541
Iteration 6/1000 | Loss: 0.00001483
Iteration 7/1000 | Loss: 0.00001450
Iteration 8/1000 | Loss: 0.00001426
Iteration 9/1000 | Loss: 0.00001418
Iteration 10/1000 | Loss: 0.00001417
Iteration 11/1000 | Loss: 0.00001407
Iteration 12/1000 | Loss: 0.00001402
Iteration 13/1000 | Loss: 0.00001402
Iteration 14/1000 | Loss: 0.00001401
Iteration 15/1000 | Loss: 0.00001400
Iteration 16/1000 | Loss: 0.00001400
Iteration 17/1000 | Loss: 0.00001399
Iteration 18/1000 | Loss: 0.00001399
Iteration 19/1000 | Loss: 0.00001396
Iteration 20/1000 | Loss: 0.00001396
Iteration 21/1000 | Loss: 0.00001395
Iteration 22/1000 | Loss: 0.00001395
Iteration 23/1000 | Loss: 0.00001390
Iteration 24/1000 | Loss: 0.00001390
Iteration 25/1000 | Loss: 0.00001388
Iteration 26/1000 | Loss: 0.00001387
Iteration 27/1000 | Loss: 0.00001385
Iteration 28/1000 | Loss: 0.00001385
Iteration 29/1000 | Loss: 0.00001385
Iteration 30/1000 | Loss: 0.00001385
Iteration 31/1000 | Loss: 0.00001385
Iteration 32/1000 | Loss: 0.00001385
Iteration 33/1000 | Loss: 0.00001385
Iteration 34/1000 | Loss: 0.00001385
Iteration 35/1000 | Loss: 0.00001384
Iteration 36/1000 | Loss: 0.00001384
Iteration 37/1000 | Loss: 0.00001383
Iteration 38/1000 | Loss: 0.00001382
Iteration 39/1000 | Loss: 0.00001381
Iteration 40/1000 | Loss: 0.00001381
Iteration 41/1000 | Loss: 0.00001380
Iteration 42/1000 | Loss: 0.00001380
Iteration 43/1000 | Loss: 0.00001380
Iteration 44/1000 | Loss: 0.00001379
Iteration 45/1000 | Loss: 0.00001379
Iteration 46/1000 | Loss: 0.00001379
Iteration 47/1000 | Loss: 0.00001378
Iteration 48/1000 | Loss: 0.00001378
Iteration 49/1000 | Loss: 0.00001378
Iteration 50/1000 | Loss: 0.00001377
Iteration 51/1000 | Loss: 0.00001377
Iteration 52/1000 | Loss: 0.00001377
Iteration 53/1000 | Loss: 0.00001377
Iteration 54/1000 | Loss: 0.00001377
Iteration 55/1000 | Loss: 0.00001376
Iteration 56/1000 | Loss: 0.00001376
Iteration 57/1000 | Loss: 0.00001376
Iteration 58/1000 | Loss: 0.00001376
Iteration 59/1000 | Loss: 0.00001376
Iteration 60/1000 | Loss: 0.00001376
Iteration 61/1000 | Loss: 0.00001376
Iteration 62/1000 | Loss: 0.00001376
Iteration 63/1000 | Loss: 0.00001375
Iteration 64/1000 | Loss: 0.00001375
Iteration 65/1000 | Loss: 0.00001375
Iteration 66/1000 | Loss: 0.00001375
Iteration 67/1000 | Loss: 0.00001375
Iteration 68/1000 | Loss: 0.00001374
Iteration 69/1000 | Loss: 0.00001374
Iteration 70/1000 | Loss: 0.00001374
Iteration 71/1000 | Loss: 0.00001374
Iteration 72/1000 | Loss: 0.00001373
Iteration 73/1000 | Loss: 0.00001373
Iteration 74/1000 | Loss: 0.00001373
Iteration 75/1000 | Loss: 0.00001372
Iteration 76/1000 | Loss: 0.00001372
Iteration 77/1000 | Loss: 0.00001372
Iteration 78/1000 | Loss: 0.00001372
Iteration 79/1000 | Loss: 0.00001371
Iteration 80/1000 | Loss: 0.00001371
Iteration 81/1000 | Loss: 0.00001371
Iteration 82/1000 | Loss: 0.00001371
Iteration 83/1000 | Loss: 0.00001371
Iteration 84/1000 | Loss: 0.00001371
Iteration 85/1000 | Loss: 0.00001371
Iteration 86/1000 | Loss: 0.00001371
Iteration 87/1000 | Loss: 0.00001371
Iteration 88/1000 | Loss: 0.00001371
Iteration 89/1000 | Loss: 0.00001370
Iteration 90/1000 | Loss: 0.00001370
Iteration 91/1000 | Loss: 0.00001370
Iteration 92/1000 | Loss: 0.00001370
Iteration 93/1000 | Loss: 0.00001369
Iteration 94/1000 | Loss: 0.00001369
Iteration 95/1000 | Loss: 0.00001369
Iteration 96/1000 | Loss: 0.00001369
Iteration 97/1000 | Loss: 0.00001368
Iteration 98/1000 | Loss: 0.00001368
Iteration 99/1000 | Loss: 0.00001368
Iteration 100/1000 | Loss: 0.00001368
Iteration 101/1000 | Loss: 0.00001368
Iteration 102/1000 | Loss: 0.00001368
Iteration 103/1000 | Loss: 0.00001368
Iteration 104/1000 | Loss: 0.00001368
Iteration 105/1000 | Loss: 0.00001367
Iteration 106/1000 | Loss: 0.00001367
Iteration 107/1000 | Loss: 0.00001367
Iteration 108/1000 | Loss: 0.00001366
Iteration 109/1000 | Loss: 0.00001366
Iteration 110/1000 | Loss: 0.00001366
Iteration 111/1000 | Loss: 0.00001366
Iteration 112/1000 | Loss: 0.00001366
Iteration 113/1000 | Loss: 0.00001366
Iteration 114/1000 | Loss: 0.00001366
Iteration 115/1000 | Loss: 0.00001366
Iteration 116/1000 | Loss: 0.00001366
Iteration 117/1000 | Loss: 0.00001366
Iteration 118/1000 | Loss: 0.00001366
Iteration 119/1000 | Loss: 0.00001366
Iteration 120/1000 | Loss: 0.00001366
Iteration 121/1000 | Loss: 0.00001366
Iteration 122/1000 | Loss: 0.00001366
Iteration 123/1000 | Loss: 0.00001366
Iteration 124/1000 | Loss: 0.00001366
Iteration 125/1000 | Loss: 0.00001365
Iteration 126/1000 | Loss: 0.00001365
Iteration 127/1000 | Loss: 0.00001365
Iteration 128/1000 | Loss: 0.00001365
Iteration 129/1000 | Loss: 0.00001365
Iteration 130/1000 | Loss: 0.00001365
Iteration 131/1000 | Loss: 0.00001365
Iteration 132/1000 | Loss: 0.00001365
Iteration 133/1000 | Loss: 0.00001365
Iteration 134/1000 | Loss: 0.00001365
Iteration 135/1000 | Loss: 0.00001365
Iteration 136/1000 | Loss: 0.00001365
Iteration 137/1000 | Loss: 0.00001365
Iteration 138/1000 | Loss: 0.00001365
Iteration 139/1000 | Loss: 0.00001364
Iteration 140/1000 | Loss: 0.00001364
Iteration 141/1000 | Loss: 0.00001364
Iteration 142/1000 | Loss: 0.00001364
Iteration 143/1000 | Loss: 0.00001364
Iteration 144/1000 | Loss: 0.00001364
Iteration 145/1000 | Loss: 0.00001364
Iteration 146/1000 | Loss: 0.00001364
Iteration 147/1000 | Loss: 0.00001364
Iteration 148/1000 | Loss: 0.00001364
Iteration 149/1000 | Loss: 0.00001363
Iteration 150/1000 | Loss: 0.00001363
Iteration 151/1000 | Loss: 0.00001363
Iteration 152/1000 | Loss: 0.00001363
Iteration 153/1000 | Loss: 0.00001363
Iteration 154/1000 | Loss: 0.00001363
Iteration 155/1000 | Loss: 0.00001363
Iteration 156/1000 | Loss: 0.00001362
Iteration 157/1000 | Loss: 0.00001362
Iteration 158/1000 | Loss: 0.00001362
Iteration 159/1000 | Loss: 0.00001362
Iteration 160/1000 | Loss: 0.00001362
Iteration 161/1000 | Loss: 0.00001361
Iteration 162/1000 | Loss: 0.00001361
Iteration 163/1000 | Loss: 0.00001361
Iteration 164/1000 | Loss: 0.00001361
Iteration 165/1000 | Loss: 0.00001361
Iteration 166/1000 | Loss: 0.00001361
Iteration 167/1000 | Loss: 0.00001361
Iteration 168/1000 | Loss: 0.00001361
Iteration 169/1000 | Loss: 0.00001361
Iteration 170/1000 | Loss: 0.00001361
Iteration 171/1000 | Loss: 0.00001360
Iteration 172/1000 | Loss: 0.00001360
Iteration 173/1000 | Loss: 0.00001360
Iteration 174/1000 | Loss: 0.00001360
Iteration 175/1000 | Loss: 0.00001360
Iteration 176/1000 | Loss: 0.00001360
Iteration 177/1000 | Loss: 0.00001360
Iteration 178/1000 | Loss: 0.00001360
Iteration 179/1000 | Loss: 0.00001360
Iteration 180/1000 | Loss: 0.00001360
Iteration 181/1000 | Loss: 0.00001360
Iteration 182/1000 | Loss: 0.00001360
Iteration 183/1000 | Loss: 0.00001360
Iteration 184/1000 | Loss: 0.00001360
Iteration 185/1000 | Loss: 0.00001360
Iteration 186/1000 | Loss: 0.00001359
Iteration 187/1000 | Loss: 0.00001359
Iteration 188/1000 | Loss: 0.00001359
Iteration 189/1000 | Loss: 0.00001359
Iteration 190/1000 | Loss: 0.00001359
Iteration 191/1000 | Loss: 0.00001359
Iteration 192/1000 | Loss: 0.00001359
Iteration 193/1000 | Loss: 0.00001359
Iteration 194/1000 | Loss: 0.00001359
Iteration 195/1000 | Loss: 0.00001359
Iteration 196/1000 | Loss: 0.00001359
Iteration 197/1000 | Loss: 0.00001359
Iteration 198/1000 | Loss: 0.00001359
Iteration 199/1000 | Loss: 0.00001359
Iteration 200/1000 | Loss: 0.00001359
Iteration 201/1000 | Loss: 0.00001359
Iteration 202/1000 | Loss: 0.00001359
Iteration 203/1000 | Loss: 0.00001359
Iteration 204/1000 | Loss: 0.00001359
Iteration 205/1000 | Loss: 0.00001359
Iteration 206/1000 | Loss: 0.00001359
Iteration 207/1000 | Loss: 0.00001359
Iteration 208/1000 | Loss: 0.00001359
Iteration 209/1000 | Loss: 0.00001359
Iteration 210/1000 | Loss: 0.00001359
Iteration 211/1000 | Loss: 0.00001358
Iteration 212/1000 | Loss: 0.00001358
Iteration 213/1000 | Loss: 0.00001358
Iteration 214/1000 | Loss: 0.00001358
Iteration 215/1000 | Loss: 0.00001358
Iteration 216/1000 | Loss: 0.00001358
Iteration 217/1000 | Loss: 0.00001358
Iteration 218/1000 | Loss: 0.00001358
Iteration 219/1000 | Loss: 0.00001358
Iteration 220/1000 | Loss: 0.00001358
Iteration 221/1000 | Loss: 0.00001358
Iteration 222/1000 | Loss: 0.00001358
Iteration 223/1000 | Loss: 0.00001358
Iteration 224/1000 | Loss: 0.00001358
Iteration 225/1000 | Loss: 0.00001358
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [1.3582008250523359e-05, 1.3582008250523359e-05, 1.3582008250523359e-05, 1.3582008250523359e-05, 1.3582008250523359e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3582008250523359e-05

Optimization complete. Final v2v error: 3.115339756011963 mm

Highest mean error: 3.9988014698028564 mm for frame 52

Lowest mean error: 2.6826162338256836 mm for frame 87

Saving results

Total time: 39.29072570800781
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00600719
Iteration 2/25 | Loss: 0.00101457
Iteration 3/25 | Loss: 0.00079663
Iteration 4/25 | Loss: 0.00075684
Iteration 5/25 | Loss: 0.00074689
Iteration 6/25 | Loss: 0.00074397
Iteration 7/25 | Loss: 0.00074330
Iteration 8/25 | Loss: 0.00074329
Iteration 9/25 | Loss: 0.00074329
Iteration 10/25 | Loss: 0.00074329
Iteration 11/25 | Loss: 0.00074329
Iteration 12/25 | Loss: 0.00074329
Iteration 13/25 | Loss: 0.00074329
Iteration 14/25 | Loss: 0.00074329
Iteration 15/25 | Loss: 0.00074329
Iteration 16/25 | Loss: 0.00074329
Iteration 17/25 | Loss: 0.00074329
Iteration 18/25 | Loss: 0.00074329
Iteration 19/25 | Loss: 0.00074329
Iteration 20/25 | Loss: 0.00074329
Iteration 21/25 | Loss: 0.00074329
Iteration 22/25 | Loss: 0.00074329
Iteration 23/25 | Loss: 0.00074329
Iteration 24/25 | Loss: 0.00074329
Iteration 25/25 | Loss: 0.00074329

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09373605
Iteration 2/25 | Loss: 0.00036204
Iteration 3/25 | Loss: 0.00036195
Iteration 4/25 | Loss: 0.00036195
Iteration 5/25 | Loss: 0.00036195
Iteration 6/25 | Loss: 0.00036195
Iteration 7/25 | Loss: 0.00036195
Iteration 8/25 | Loss: 0.00036195
Iteration 9/25 | Loss: 0.00036195
Iteration 10/25 | Loss: 0.00036195
Iteration 11/25 | Loss: 0.00036195
Iteration 12/25 | Loss: 0.00036195
Iteration 13/25 | Loss: 0.00036195
Iteration 14/25 | Loss: 0.00036195
Iteration 15/25 | Loss: 0.00036195
Iteration 16/25 | Loss: 0.00036195
Iteration 17/25 | Loss: 0.00036195
Iteration 18/25 | Loss: 0.00036195
Iteration 19/25 | Loss: 0.00036195
Iteration 20/25 | Loss: 0.00036195
Iteration 21/25 | Loss: 0.00036195
Iteration 22/25 | Loss: 0.00036195
Iteration 23/25 | Loss: 0.00036195
Iteration 24/25 | Loss: 0.00036195
Iteration 25/25 | Loss: 0.00036195

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036195
Iteration 2/1000 | Loss: 0.00007107
Iteration 3/1000 | Loss: 0.00003815
Iteration 4/1000 | Loss: 0.00003048
Iteration 5/1000 | Loss: 0.00002787
Iteration 6/1000 | Loss: 0.00002674
Iteration 7/1000 | Loss: 0.00002629
Iteration 8/1000 | Loss: 0.00002584
Iteration 9/1000 | Loss: 0.00002545
Iteration 10/1000 | Loss: 0.00002528
Iteration 11/1000 | Loss: 0.00002509
Iteration 12/1000 | Loss: 0.00002493
Iteration 13/1000 | Loss: 0.00002487
Iteration 14/1000 | Loss: 0.00002486
Iteration 15/1000 | Loss: 0.00002479
Iteration 16/1000 | Loss: 0.00002467
Iteration 17/1000 | Loss: 0.00002457
Iteration 18/1000 | Loss: 0.00002453
Iteration 19/1000 | Loss: 0.00002453
Iteration 20/1000 | Loss: 0.00002453
Iteration 21/1000 | Loss: 0.00002453
Iteration 22/1000 | Loss: 0.00002453
Iteration 23/1000 | Loss: 0.00002452
Iteration 24/1000 | Loss: 0.00002452
Iteration 25/1000 | Loss: 0.00002452
Iteration 26/1000 | Loss: 0.00002452
Iteration 27/1000 | Loss: 0.00002452
Iteration 28/1000 | Loss: 0.00002452
Iteration 29/1000 | Loss: 0.00002452
Iteration 30/1000 | Loss: 0.00002450
Iteration 31/1000 | Loss: 0.00002447
Iteration 32/1000 | Loss: 0.00002447
Iteration 33/1000 | Loss: 0.00002447
Iteration 34/1000 | Loss: 0.00002446
Iteration 35/1000 | Loss: 0.00002446
Iteration 36/1000 | Loss: 0.00002445
Iteration 37/1000 | Loss: 0.00002445
Iteration 38/1000 | Loss: 0.00002445
Iteration 39/1000 | Loss: 0.00002444
Iteration 40/1000 | Loss: 0.00002444
Iteration 41/1000 | Loss: 0.00002444
Iteration 42/1000 | Loss: 0.00002444
Iteration 43/1000 | Loss: 0.00002444
Iteration 44/1000 | Loss: 0.00002444
Iteration 45/1000 | Loss: 0.00002443
Iteration 46/1000 | Loss: 0.00002443
Iteration 47/1000 | Loss: 0.00002443
Iteration 48/1000 | Loss: 0.00002443
Iteration 49/1000 | Loss: 0.00002443
Iteration 50/1000 | Loss: 0.00002443
Iteration 51/1000 | Loss: 0.00002443
Iteration 52/1000 | Loss: 0.00002443
Iteration 53/1000 | Loss: 0.00002443
Iteration 54/1000 | Loss: 0.00002442
Iteration 55/1000 | Loss: 0.00002442
Iteration 56/1000 | Loss: 0.00002442
Iteration 57/1000 | Loss: 0.00002442
Iteration 58/1000 | Loss: 0.00002442
Iteration 59/1000 | Loss: 0.00002441
Iteration 60/1000 | Loss: 0.00002440
Iteration 61/1000 | Loss: 0.00002440
Iteration 62/1000 | Loss: 0.00002440
Iteration 63/1000 | Loss: 0.00002440
Iteration 64/1000 | Loss: 0.00002440
Iteration 65/1000 | Loss: 0.00002440
Iteration 66/1000 | Loss: 0.00002439
Iteration 67/1000 | Loss: 0.00002439
Iteration 68/1000 | Loss: 0.00002439
Iteration 69/1000 | Loss: 0.00002436
Iteration 70/1000 | Loss: 0.00002436
Iteration 71/1000 | Loss: 0.00002435
Iteration 72/1000 | Loss: 0.00002434
Iteration 73/1000 | Loss: 0.00002433
Iteration 74/1000 | Loss: 0.00002433
Iteration 75/1000 | Loss: 0.00002432
Iteration 76/1000 | Loss: 0.00002431
Iteration 77/1000 | Loss: 0.00002431
Iteration 78/1000 | Loss: 0.00002431
Iteration 79/1000 | Loss: 0.00002431
Iteration 80/1000 | Loss: 0.00002431
Iteration 81/1000 | Loss: 0.00002431
Iteration 82/1000 | Loss: 0.00002431
Iteration 83/1000 | Loss: 0.00002431
Iteration 84/1000 | Loss: 0.00002431
Iteration 85/1000 | Loss: 0.00002431
Iteration 86/1000 | Loss: 0.00002430
Iteration 87/1000 | Loss: 0.00002430
Iteration 88/1000 | Loss: 0.00002430
Iteration 89/1000 | Loss: 0.00002429
Iteration 90/1000 | Loss: 0.00002429
Iteration 91/1000 | Loss: 0.00002428
Iteration 92/1000 | Loss: 0.00002428
Iteration 93/1000 | Loss: 0.00002428
Iteration 94/1000 | Loss: 0.00002428
Iteration 95/1000 | Loss: 0.00002428
Iteration 96/1000 | Loss: 0.00002428
Iteration 97/1000 | Loss: 0.00002428
Iteration 98/1000 | Loss: 0.00002428
Iteration 99/1000 | Loss: 0.00002428
Iteration 100/1000 | Loss: 0.00002428
Iteration 101/1000 | Loss: 0.00002427
Iteration 102/1000 | Loss: 0.00002427
Iteration 103/1000 | Loss: 0.00002425
Iteration 104/1000 | Loss: 0.00002424
Iteration 105/1000 | Loss: 0.00002424
Iteration 106/1000 | Loss: 0.00002424
Iteration 107/1000 | Loss: 0.00002423
Iteration 108/1000 | Loss: 0.00002423
Iteration 109/1000 | Loss: 0.00002423
Iteration 110/1000 | Loss: 0.00002423
Iteration 111/1000 | Loss: 0.00002423
Iteration 112/1000 | Loss: 0.00002423
Iteration 113/1000 | Loss: 0.00002423
Iteration 114/1000 | Loss: 0.00002422
Iteration 115/1000 | Loss: 0.00002422
Iteration 116/1000 | Loss: 0.00002422
Iteration 117/1000 | Loss: 0.00002422
Iteration 118/1000 | Loss: 0.00002421
Iteration 119/1000 | Loss: 0.00002421
Iteration 120/1000 | Loss: 0.00002421
Iteration 121/1000 | Loss: 0.00002421
Iteration 122/1000 | Loss: 0.00002420
Iteration 123/1000 | Loss: 0.00002420
Iteration 124/1000 | Loss: 0.00002419
Iteration 125/1000 | Loss: 0.00002418
Iteration 126/1000 | Loss: 0.00002418
Iteration 127/1000 | Loss: 0.00002418
Iteration 128/1000 | Loss: 0.00002416
Iteration 129/1000 | Loss: 0.00002416
Iteration 130/1000 | Loss: 0.00002416
Iteration 131/1000 | Loss: 0.00002416
Iteration 132/1000 | Loss: 0.00002416
Iteration 133/1000 | Loss: 0.00002416
Iteration 134/1000 | Loss: 0.00002415
Iteration 135/1000 | Loss: 0.00002415
Iteration 136/1000 | Loss: 0.00002415
Iteration 137/1000 | Loss: 0.00002415
Iteration 138/1000 | Loss: 0.00002415
Iteration 139/1000 | Loss: 0.00002415
Iteration 140/1000 | Loss: 0.00002415
Iteration 141/1000 | Loss: 0.00002415
Iteration 142/1000 | Loss: 0.00002415
Iteration 143/1000 | Loss: 0.00002414
Iteration 144/1000 | Loss: 0.00002414
Iteration 145/1000 | Loss: 0.00002414
Iteration 146/1000 | Loss: 0.00002414
Iteration 147/1000 | Loss: 0.00002414
Iteration 148/1000 | Loss: 0.00002414
Iteration 149/1000 | Loss: 0.00002414
Iteration 150/1000 | Loss: 0.00002413
Iteration 151/1000 | Loss: 0.00002413
Iteration 152/1000 | Loss: 0.00002413
Iteration 153/1000 | Loss: 0.00002413
Iteration 154/1000 | Loss: 0.00002413
Iteration 155/1000 | Loss: 0.00002413
Iteration 156/1000 | Loss: 0.00002413
Iteration 157/1000 | Loss: 0.00002413
Iteration 158/1000 | Loss: 0.00002413
Iteration 159/1000 | Loss: 0.00002413
Iteration 160/1000 | Loss: 0.00002413
Iteration 161/1000 | Loss: 0.00002413
Iteration 162/1000 | Loss: 0.00002413
Iteration 163/1000 | Loss: 0.00002412
Iteration 164/1000 | Loss: 0.00002412
Iteration 165/1000 | Loss: 0.00002412
Iteration 166/1000 | Loss: 0.00002411
Iteration 167/1000 | Loss: 0.00002410
Iteration 168/1000 | Loss: 0.00002410
Iteration 169/1000 | Loss: 0.00002410
Iteration 170/1000 | Loss: 0.00002410
Iteration 171/1000 | Loss: 0.00002410
Iteration 172/1000 | Loss: 0.00002410
Iteration 173/1000 | Loss: 0.00002410
Iteration 174/1000 | Loss: 0.00002410
Iteration 175/1000 | Loss: 0.00002410
Iteration 176/1000 | Loss: 0.00002410
Iteration 177/1000 | Loss: 0.00002410
Iteration 178/1000 | Loss: 0.00002410
Iteration 179/1000 | Loss: 0.00002410
Iteration 180/1000 | Loss: 0.00002410
Iteration 181/1000 | Loss: 0.00002410
Iteration 182/1000 | Loss: 0.00002409
Iteration 183/1000 | Loss: 0.00002409
Iteration 184/1000 | Loss: 0.00002409
Iteration 185/1000 | Loss: 0.00002409
Iteration 186/1000 | Loss: 0.00002409
Iteration 187/1000 | Loss: 0.00002409
Iteration 188/1000 | Loss: 0.00002409
Iteration 189/1000 | Loss: 0.00002408
Iteration 190/1000 | Loss: 0.00002408
Iteration 191/1000 | Loss: 0.00002408
Iteration 192/1000 | Loss: 0.00002408
Iteration 193/1000 | Loss: 0.00002408
Iteration 194/1000 | Loss: 0.00002408
Iteration 195/1000 | Loss: 0.00002408
Iteration 196/1000 | Loss: 0.00002408
Iteration 197/1000 | Loss: 0.00002408
Iteration 198/1000 | Loss: 0.00002408
Iteration 199/1000 | Loss: 0.00002408
Iteration 200/1000 | Loss: 0.00002408
Iteration 201/1000 | Loss: 0.00002407
Iteration 202/1000 | Loss: 0.00002407
Iteration 203/1000 | Loss: 0.00002407
Iteration 204/1000 | Loss: 0.00002407
Iteration 205/1000 | Loss: 0.00002407
Iteration 206/1000 | Loss: 0.00002407
Iteration 207/1000 | Loss: 0.00002407
Iteration 208/1000 | Loss: 0.00002407
Iteration 209/1000 | Loss: 0.00002407
Iteration 210/1000 | Loss: 0.00002407
Iteration 211/1000 | Loss: 0.00002407
Iteration 212/1000 | Loss: 0.00002407
Iteration 213/1000 | Loss: 0.00002407
Iteration 214/1000 | Loss: 0.00002407
Iteration 215/1000 | Loss: 0.00002407
Iteration 216/1000 | Loss: 0.00002407
Iteration 217/1000 | Loss: 0.00002407
Iteration 218/1000 | Loss: 0.00002406
Iteration 219/1000 | Loss: 0.00002406
Iteration 220/1000 | Loss: 0.00002406
Iteration 221/1000 | Loss: 0.00002406
Iteration 222/1000 | Loss: 0.00002406
Iteration 223/1000 | Loss: 0.00002406
Iteration 224/1000 | Loss: 0.00002406
Iteration 225/1000 | Loss: 0.00002406
Iteration 226/1000 | Loss: 0.00002406
Iteration 227/1000 | Loss: 0.00002406
Iteration 228/1000 | Loss: 0.00002406
Iteration 229/1000 | Loss: 0.00002406
Iteration 230/1000 | Loss: 0.00002406
Iteration 231/1000 | Loss: 0.00002406
Iteration 232/1000 | Loss: 0.00002406
Iteration 233/1000 | Loss: 0.00002406
Iteration 234/1000 | Loss: 0.00002406
Iteration 235/1000 | Loss: 0.00002406
Iteration 236/1000 | Loss: 0.00002406
Iteration 237/1000 | Loss: 0.00002406
Iteration 238/1000 | Loss: 0.00002406
Iteration 239/1000 | Loss: 0.00002406
Iteration 240/1000 | Loss: 0.00002406
Iteration 241/1000 | Loss: 0.00002406
Iteration 242/1000 | Loss: 0.00002406
Iteration 243/1000 | Loss: 0.00002406
Iteration 244/1000 | Loss: 0.00002406
Iteration 245/1000 | Loss: 0.00002406
Iteration 246/1000 | Loss: 0.00002406
Iteration 247/1000 | Loss: 0.00002406
Iteration 248/1000 | Loss: 0.00002406
Iteration 249/1000 | Loss: 0.00002406
Iteration 250/1000 | Loss: 0.00002406
Iteration 251/1000 | Loss: 0.00002406
Iteration 252/1000 | Loss: 0.00002406
Iteration 253/1000 | Loss: 0.00002406
Iteration 254/1000 | Loss: 0.00002406
Iteration 255/1000 | Loss: 0.00002406
Iteration 256/1000 | Loss: 0.00002406
Iteration 257/1000 | Loss: 0.00002406
Iteration 258/1000 | Loss: 0.00002406
Iteration 259/1000 | Loss: 0.00002406
Iteration 260/1000 | Loss: 0.00002406
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 260. Stopping optimization.
Last 5 losses: [2.4060085706878453e-05, 2.4060085706878453e-05, 2.4060085706878453e-05, 2.4060085706878453e-05, 2.4060085706878453e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4060085706878453e-05

Optimization complete. Final v2v error: 4.042888641357422 mm

Highest mean error: 4.367202281951904 mm for frame 57

Lowest mean error: 3.7691361904144287 mm for frame 10

Saving results

Total time: 48.4788384437561
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00398169
Iteration 2/25 | Loss: 0.00080246
Iteration 3/25 | Loss: 0.00063873
Iteration 4/25 | Loss: 0.00062229
Iteration 5/25 | Loss: 0.00061063
Iteration 6/25 | Loss: 0.00060977
Iteration 7/25 | Loss: 0.00060977
Iteration 8/25 | Loss: 0.00060977
Iteration 9/25 | Loss: 0.00060977
Iteration 10/25 | Loss: 0.00060977
Iteration 11/25 | Loss: 0.00060977
Iteration 12/25 | Loss: 0.00060977
Iteration 13/25 | Loss: 0.00060977
Iteration 14/25 | Loss: 0.00060977
Iteration 15/25 | Loss: 0.00060977
Iteration 16/25 | Loss: 0.00060977
Iteration 17/25 | Loss: 0.00060977
Iteration 18/25 | Loss: 0.00060977
Iteration 19/25 | Loss: 0.00060977
Iteration 20/25 | Loss: 0.00060977
Iteration 21/25 | Loss: 0.00060977
Iteration 22/25 | Loss: 0.00060977
Iteration 23/25 | Loss: 0.00060977
Iteration 24/25 | Loss: 0.00060977
Iteration 25/25 | Loss: 0.00060977

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.74093688
Iteration 2/25 | Loss: 0.00027334
Iteration 3/25 | Loss: 0.00027330
Iteration 4/25 | Loss: 0.00027330
Iteration 5/25 | Loss: 0.00027330
Iteration 6/25 | Loss: 0.00027330
Iteration 7/25 | Loss: 0.00027330
Iteration 8/25 | Loss: 0.00027330
Iteration 9/25 | Loss: 0.00027330
Iteration 10/25 | Loss: 0.00027330
Iteration 11/25 | Loss: 0.00027330
Iteration 12/25 | Loss: 0.00027330
Iteration 13/25 | Loss: 0.00027330
Iteration 14/25 | Loss: 0.00027330
Iteration 15/25 | Loss: 0.00027330
Iteration 16/25 | Loss: 0.00027330
Iteration 17/25 | Loss: 0.00027330
Iteration 18/25 | Loss: 0.00027330
Iteration 19/25 | Loss: 0.00027330
Iteration 20/25 | Loss: 0.00027330
Iteration 21/25 | Loss: 0.00027330
Iteration 22/25 | Loss: 0.00027330
Iteration 23/25 | Loss: 0.00027330
Iteration 24/25 | Loss: 0.00027330
Iteration 25/25 | Loss: 0.00027330

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027330
Iteration 2/1000 | Loss: 0.00002157
Iteration 3/1000 | Loss: 0.00001465
Iteration 4/1000 | Loss: 0.00001318
Iteration 5/1000 | Loss: 0.00001263
Iteration 6/1000 | Loss: 0.00001218
Iteration 7/1000 | Loss: 0.00001198
Iteration 8/1000 | Loss: 0.00001176
Iteration 9/1000 | Loss: 0.00001163
Iteration 10/1000 | Loss: 0.00001159
Iteration 11/1000 | Loss: 0.00001151
Iteration 12/1000 | Loss: 0.00001149
Iteration 13/1000 | Loss: 0.00001149
Iteration 14/1000 | Loss: 0.00001148
Iteration 15/1000 | Loss: 0.00001148
Iteration 16/1000 | Loss: 0.00001147
Iteration 17/1000 | Loss: 0.00001146
Iteration 18/1000 | Loss: 0.00001146
Iteration 19/1000 | Loss: 0.00001145
Iteration 20/1000 | Loss: 0.00001145
Iteration 21/1000 | Loss: 0.00001143
Iteration 22/1000 | Loss: 0.00001142
Iteration 23/1000 | Loss: 0.00001140
Iteration 24/1000 | Loss: 0.00001139
Iteration 25/1000 | Loss: 0.00001139
Iteration 26/1000 | Loss: 0.00001138
Iteration 27/1000 | Loss: 0.00001138
Iteration 28/1000 | Loss: 0.00001138
Iteration 29/1000 | Loss: 0.00001138
Iteration 30/1000 | Loss: 0.00001138
Iteration 31/1000 | Loss: 0.00001138
Iteration 32/1000 | Loss: 0.00001138
Iteration 33/1000 | Loss: 0.00001138
Iteration 34/1000 | Loss: 0.00001138
Iteration 35/1000 | Loss: 0.00001138
Iteration 36/1000 | Loss: 0.00001138
Iteration 37/1000 | Loss: 0.00001137
Iteration 38/1000 | Loss: 0.00001137
Iteration 39/1000 | Loss: 0.00001137
Iteration 40/1000 | Loss: 0.00001137
Iteration 41/1000 | Loss: 0.00001136
Iteration 42/1000 | Loss: 0.00001136
Iteration 43/1000 | Loss: 0.00001136
Iteration 44/1000 | Loss: 0.00001135
Iteration 45/1000 | Loss: 0.00001135
Iteration 46/1000 | Loss: 0.00001135
Iteration 47/1000 | Loss: 0.00001134
Iteration 48/1000 | Loss: 0.00001134
Iteration 49/1000 | Loss: 0.00001134
Iteration 50/1000 | Loss: 0.00001134
Iteration 51/1000 | Loss: 0.00001134
Iteration 52/1000 | Loss: 0.00001133
Iteration 53/1000 | Loss: 0.00001133
Iteration 54/1000 | Loss: 0.00001133
Iteration 55/1000 | Loss: 0.00001133
Iteration 56/1000 | Loss: 0.00001132
Iteration 57/1000 | Loss: 0.00001132
Iteration 58/1000 | Loss: 0.00001131
Iteration 59/1000 | Loss: 0.00001131
Iteration 60/1000 | Loss: 0.00001131
Iteration 61/1000 | Loss: 0.00001131
Iteration 62/1000 | Loss: 0.00001131
Iteration 63/1000 | Loss: 0.00001130
Iteration 64/1000 | Loss: 0.00001130
Iteration 65/1000 | Loss: 0.00001130
Iteration 66/1000 | Loss: 0.00001130
Iteration 67/1000 | Loss: 0.00001129
Iteration 68/1000 | Loss: 0.00001129
Iteration 69/1000 | Loss: 0.00001128
Iteration 70/1000 | Loss: 0.00001128
Iteration 71/1000 | Loss: 0.00001128
Iteration 72/1000 | Loss: 0.00001128
Iteration 73/1000 | Loss: 0.00001128
Iteration 74/1000 | Loss: 0.00001128
Iteration 75/1000 | Loss: 0.00001128
Iteration 76/1000 | Loss: 0.00001128
Iteration 77/1000 | Loss: 0.00001128
Iteration 78/1000 | Loss: 0.00001128
Iteration 79/1000 | Loss: 0.00001128
Iteration 80/1000 | Loss: 0.00001128
Iteration 81/1000 | Loss: 0.00001128
Iteration 82/1000 | Loss: 0.00001127
Iteration 83/1000 | Loss: 0.00001127
Iteration 84/1000 | Loss: 0.00001127
Iteration 85/1000 | Loss: 0.00001127
Iteration 86/1000 | Loss: 0.00001127
Iteration 87/1000 | Loss: 0.00001127
Iteration 88/1000 | Loss: 0.00001127
Iteration 89/1000 | Loss: 0.00001127
Iteration 90/1000 | Loss: 0.00001127
Iteration 91/1000 | Loss: 0.00001127
Iteration 92/1000 | Loss: 0.00001127
Iteration 93/1000 | Loss: 0.00001126
Iteration 94/1000 | Loss: 0.00001126
Iteration 95/1000 | Loss: 0.00001126
Iteration 96/1000 | Loss: 0.00001126
Iteration 97/1000 | Loss: 0.00001126
Iteration 98/1000 | Loss: 0.00001126
Iteration 99/1000 | Loss: 0.00001126
Iteration 100/1000 | Loss: 0.00001126
Iteration 101/1000 | Loss: 0.00001126
Iteration 102/1000 | Loss: 0.00001126
Iteration 103/1000 | Loss: 0.00001126
Iteration 104/1000 | Loss: 0.00001126
Iteration 105/1000 | Loss: 0.00001126
Iteration 106/1000 | Loss: 0.00001126
Iteration 107/1000 | Loss: 0.00001126
Iteration 108/1000 | Loss: 0.00001126
Iteration 109/1000 | Loss: 0.00001125
Iteration 110/1000 | Loss: 0.00001125
Iteration 111/1000 | Loss: 0.00001125
Iteration 112/1000 | Loss: 0.00001125
Iteration 113/1000 | Loss: 0.00001125
Iteration 114/1000 | Loss: 0.00001125
Iteration 115/1000 | Loss: 0.00001125
Iteration 116/1000 | Loss: 0.00001125
Iteration 117/1000 | Loss: 0.00001125
Iteration 118/1000 | Loss: 0.00001125
Iteration 119/1000 | Loss: 0.00001125
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.1253836419200525e-05, 1.1253836419200525e-05, 1.1253836419200525e-05, 1.1253836419200525e-05, 1.1253836419200525e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1253836419200525e-05

Optimization complete. Final v2v error: 2.868558883666992 mm

Highest mean error: 3.0726897716522217 mm for frame 2

Lowest mean error: 2.6807315349578857 mm for frame 195

Saving results

Total time: 34.969332695007324
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00816573
Iteration 2/25 | Loss: 0.00097337
Iteration 3/25 | Loss: 0.00075121
Iteration 4/25 | Loss: 0.00072240
Iteration 5/25 | Loss: 0.00071421
Iteration 6/25 | Loss: 0.00071153
Iteration 7/25 | Loss: 0.00071087
Iteration 8/25 | Loss: 0.00071087
Iteration 9/25 | Loss: 0.00071087
Iteration 10/25 | Loss: 0.00071087
Iteration 11/25 | Loss: 0.00071087
Iteration 12/25 | Loss: 0.00071087
Iteration 13/25 | Loss: 0.00071087
Iteration 14/25 | Loss: 0.00071087
Iteration 15/25 | Loss: 0.00071087
Iteration 16/25 | Loss: 0.00071087
Iteration 17/25 | Loss: 0.00071087
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007108749123290181, 0.0007108749123290181, 0.0007108749123290181, 0.0007108749123290181, 0.0007108749123290181]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007108749123290181

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20899975
Iteration 2/25 | Loss: 0.00029455
Iteration 3/25 | Loss: 0.00029452
Iteration 4/25 | Loss: 0.00029452
Iteration 5/25 | Loss: 0.00029452
Iteration 6/25 | Loss: 0.00029452
Iteration 7/25 | Loss: 0.00029452
Iteration 8/25 | Loss: 0.00029452
Iteration 9/25 | Loss: 0.00029452
Iteration 10/25 | Loss: 0.00029452
Iteration 11/25 | Loss: 0.00029452
Iteration 12/25 | Loss: 0.00029452
Iteration 13/25 | Loss: 0.00029452
Iteration 14/25 | Loss: 0.00029452
Iteration 15/25 | Loss: 0.00029452
Iteration 16/25 | Loss: 0.00029452
Iteration 17/25 | Loss: 0.00029452
Iteration 18/25 | Loss: 0.00029452
Iteration 19/25 | Loss: 0.00029452
Iteration 20/25 | Loss: 0.00029452
Iteration 21/25 | Loss: 0.00029452
Iteration 22/25 | Loss: 0.00029452
Iteration 23/25 | Loss: 0.00029452
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00029451577574945986, 0.00029451577574945986, 0.00029451577574945986, 0.00029451577574945986, 0.00029451577574945986]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00029451577574945986

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029452
Iteration 2/1000 | Loss: 0.00002576
Iteration 3/1000 | Loss: 0.00002045
Iteration 4/1000 | Loss: 0.00001923
Iteration 5/1000 | Loss: 0.00001883
Iteration 6/1000 | Loss: 0.00001854
Iteration 7/1000 | Loss: 0.00001829
Iteration 8/1000 | Loss: 0.00001805
Iteration 9/1000 | Loss: 0.00001801
Iteration 10/1000 | Loss: 0.00001800
Iteration 11/1000 | Loss: 0.00001792
Iteration 12/1000 | Loss: 0.00001780
Iteration 13/1000 | Loss: 0.00001779
Iteration 14/1000 | Loss: 0.00001778
Iteration 15/1000 | Loss: 0.00001777
Iteration 16/1000 | Loss: 0.00001775
Iteration 17/1000 | Loss: 0.00001774
Iteration 18/1000 | Loss: 0.00001773
Iteration 19/1000 | Loss: 0.00001771
Iteration 20/1000 | Loss: 0.00001771
Iteration 21/1000 | Loss: 0.00001770
Iteration 22/1000 | Loss: 0.00001770
Iteration 23/1000 | Loss: 0.00001770
Iteration 24/1000 | Loss: 0.00001769
Iteration 25/1000 | Loss: 0.00001769
Iteration 26/1000 | Loss: 0.00001767
Iteration 27/1000 | Loss: 0.00001766
Iteration 28/1000 | Loss: 0.00001765
Iteration 29/1000 | Loss: 0.00001765
Iteration 30/1000 | Loss: 0.00001763
Iteration 31/1000 | Loss: 0.00001762
Iteration 32/1000 | Loss: 0.00001762
Iteration 33/1000 | Loss: 0.00001762
Iteration 34/1000 | Loss: 0.00001762
Iteration 35/1000 | Loss: 0.00001762
Iteration 36/1000 | Loss: 0.00001762
Iteration 37/1000 | Loss: 0.00001762
Iteration 38/1000 | Loss: 0.00001762
Iteration 39/1000 | Loss: 0.00001762
Iteration 40/1000 | Loss: 0.00001762
Iteration 41/1000 | Loss: 0.00001762
Iteration 42/1000 | Loss: 0.00001761
Iteration 43/1000 | Loss: 0.00001761
Iteration 44/1000 | Loss: 0.00001761
Iteration 45/1000 | Loss: 0.00001761
Iteration 46/1000 | Loss: 0.00001760
Iteration 47/1000 | Loss: 0.00001760
Iteration 48/1000 | Loss: 0.00001759
Iteration 49/1000 | Loss: 0.00001759
Iteration 50/1000 | Loss: 0.00001758
Iteration 51/1000 | Loss: 0.00001758
Iteration 52/1000 | Loss: 0.00001757
Iteration 53/1000 | Loss: 0.00001757
Iteration 54/1000 | Loss: 0.00001757
Iteration 55/1000 | Loss: 0.00001757
Iteration 56/1000 | Loss: 0.00001757
Iteration 57/1000 | Loss: 0.00001757
Iteration 58/1000 | Loss: 0.00001756
Iteration 59/1000 | Loss: 0.00001756
Iteration 60/1000 | Loss: 0.00001756
Iteration 61/1000 | Loss: 0.00001756
Iteration 62/1000 | Loss: 0.00001756
Iteration 63/1000 | Loss: 0.00001756
Iteration 64/1000 | Loss: 0.00001756
Iteration 65/1000 | Loss: 0.00001756
Iteration 66/1000 | Loss: 0.00001756
Iteration 67/1000 | Loss: 0.00001756
Iteration 68/1000 | Loss: 0.00001756
Iteration 69/1000 | Loss: 0.00001755
Iteration 70/1000 | Loss: 0.00001755
Iteration 71/1000 | Loss: 0.00001755
Iteration 72/1000 | Loss: 0.00001755
Iteration 73/1000 | Loss: 0.00001755
Iteration 74/1000 | Loss: 0.00001755
Iteration 75/1000 | Loss: 0.00001755
Iteration 76/1000 | Loss: 0.00001755
Iteration 77/1000 | Loss: 0.00001755
Iteration 78/1000 | Loss: 0.00001755
Iteration 79/1000 | Loss: 0.00001754
Iteration 80/1000 | Loss: 0.00001754
Iteration 81/1000 | Loss: 0.00001754
Iteration 82/1000 | Loss: 0.00001754
Iteration 83/1000 | Loss: 0.00001754
Iteration 84/1000 | Loss: 0.00001754
Iteration 85/1000 | Loss: 0.00001754
Iteration 86/1000 | Loss: 0.00001753
Iteration 87/1000 | Loss: 0.00001753
Iteration 88/1000 | Loss: 0.00001753
Iteration 89/1000 | Loss: 0.00001753
Iteration 90/1000 | Loss: 0.00001753
Iteration 91/1000 | Loss: 0.00001753
Iteration 92/1000 | Loss: 0.00001753
Iteration 93/1000 | Loss: 0.00001753
Iteration 94/1000 | Loss: 0.00001752
Iteration 95/1000 | Loss: 0.00001752
Iteration 96/1000 | Loss: 0.00001752
Iteration 97/1000 | Loss: 0.00001752
Iteration 98/1000 | Loss: 0.00001752
Iteration 99/1000 | Loss: 0.00001752
Iteration 100/1000 | Loss: 0.00001752
Iteration 101/1000 | Loss: 0.00001752
Iteration 102/1000 | Loss: 0.00001752
Iteration 103/1000 | Loss: 0.00001751
Iteration 104/1000 | Loss: 0.00001751
Iteration 105/1000 | Loss: 0.00001751
Iteration 106/1000 | Loss: 0.00001751
Iteration 107/1000 | Loss: 0.00001751
Iteration 108/1000 | Loss: 0.00001751
Iteration 109/1000 | Loss: 0.00001751
Iteration 110/1000 | Loss: 0.00001751
Iteration 111/1000 | Loss: 0.00001751
Iteration 112/1000 | Loss: 0.00001751
Iteration 113/1000 | Loss: 0.00001750
Iteration 114/1000 | Loss: 0.00001750
Iteration 115/1000 | Loss: 0.00001750
Iteration 116/1000 | Loss: 0.00001750
Iteration 117/1000 | Loss: 0.00001750
Iteration 118/1000 | Loss: 0.00001750
Iteration 119/1000 | Loss: 0.00001750
Iteration 120/1000 | Loss: 0.00001750
Iteration 121/1000 | Loss: 0.00001750
Iteration 122/1000 | Loss: 0.00001750
Iteration 123/1000 | Loss: 0.00001750
Iteration 124/1000 | Loss: 0.00001750
Iteration 125/1000 | Loss: 0.00001750
Iteration 126/1000 | Loss: 0.00001750
Iteration 127/1000 | Loss: 0.00001749
Iteration 128/1000 | Loss: 0.00001749
Iteration 129/1000 | Loss: 0.00001749
Iteration 130/1000 | Loss: 0.00001749
Iteration 131/1000 | Loss: 0.00001749
Iteration 132/1000 | Loss: 0.00001749
Iteration 133/1000 | Loss: 0.00001749
Iteration 134/1000 | Loss: 0.00001749
Iteration 135/1000 | Loss: 0.00001749
Iteration 136/1000 | Loss: 0.00001749
Iteration 137/1000 | Loss: 0.00001749
Iteration 138/1000 | Loss: 0.00001749
Iteration 139/1000 | Loss: 0.00001749
Iteration 140/1000 | Loss: 0.00001749
Iteration 141/1000 | Loss: 0.00001749
Iteration 142/1000 | Loss: 0.00001749
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [1.7489042875240557e-05, 1.7489042875240557e-05, 1.7489042875240557e-05, 1.7489042875240557e-05, 1.7489042875240557e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7489042875240557e-05

Optimization complete. Final v2v error: 3.5172231197357178 mm

Highest mean error: 3.7115817070007324 mm for frame 82

Lowest mean error: 3.299098014831543 mm for frame 9

Saving results

Total time: 33.56778001785278
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00662631
Iteration 2/25 | Loss: 0.00085791
Iteration 3/25 | Loss: 0.00064484
Iteration 4/25 | Loss: 0.00061535
Iteration 5/25 | Loss: 0.00060790
Iteration 6/25 | Loss: 0.00060616
Iteration 7/25 | Loss: 0.00060605
Iteration 8/25 | Loss: 0.00060605
Iteration 9/25 | Loss: 0.00060605
Iteration 10/25 | Loss: 0.00060605
Iteration 11/25 | Loss: 0.00060605
Iteration 12/25 | Loss: 0.00060605
Iteration 13/25 | Loss: 0.00060605
Iteration 14/25 | Loss: 0.00060605
Iteration 15/25 | Loss: 0.00060605
Iteration 16/25 | Loss: 0.00060605
Iteration 17/25 | Loss: 0.00060605
Iteration 18/25 | Loss: 0.00060605
Iteration 19/25 | Loss: 0.00060605
Iteration 20/25 | Loss: 0.00060605
Iteration 21/25 | Loss: 0.00060605
Iteration 22/25 | Loss: 0.00060605
Iteration 23/25 | Loss: 0.00060605
Iteration 24/25 | Loss: 0.00060605
Iteration 25/25 | Loss: 0.00060605

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.18379211
Iteration 2/25 | Loss: 0.00022855
Iteration 3/25 | Loss: 0.00022847
Iteration 4/25 | Loss: 0.00022847
Iteration 5/25 | Loss: 0.00022847
Iteration 6/25 | Loss: 0.00022847
Iteration 7/25 | Loss: 0.00022847
Iteration 8/25 | Loss: 0.00022847
Iteration 9/25 | Loss: 0.00022847
Iteration 10/25 | Loss: 0.00022846
Iteration 11/25 | Loss: 0.00022846
Iteration 12/25 | Loss: 0.00022846
Iteration 13/25 | Loss: 0.00022846
Iteration 14/25 | Loss: 0.00022846
Iteration 15/25 | Loss: 0.00022846
Iteration 16/25 | Loss: 0.00022846
Iteration 17/25 | Loss: 0.00022846
Iteration 18/25 | Loss: 0.00022846
Iteration 19/25 | Loss: 0.00022846
Iteration 20/25 | Loss: 0.00022846
Iteration 21/25 | Loss: 0.00022846
Iteration 22/25 | Loss: 0.00022846
Iteration 23/25 | Loss: 0.00022846
Iteration 24/25 | Loss: 0.00022846
Iteration 25/25 | Loss: 0.00022846

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00022846
Iteration 2/1000 | Loss: 0.00002426
Iteration 3/1000 | Loss: 0.00001976
Iteration 4/1000 | Loss: 0.00001848
Iteration 5/1000 | Loss: 0.00001756
Iteration 6/1000 | Loss: 0.00001693
Iteration 7/1000 | Loss: 0.00001645
Iteration 8/1000 | Loss: 0.00001618
Iteration 9/1000 | Loss: 0.00001601
Iteration 10/1000 | Loss: 0.00001595
Iteration 11/1000 | Loss: 0.00001593
Iteration 12/1000 | Loss: 0.00001593
Iteration 13/1000 | Loss: 0.00001592
Iteration 14/1000 | Loss: 0.00001592
Iteration 15/1000 | Loss: 0.00001592
Iteration 16/1000 | Loss: 0.00001592
Iteration 17/1000 | Loss: 0.00001591
Iteration 18/1000 | Loss: 0.00001590
Iteration 19/1000 | Loss: 0.00001590
Iteration 20/1000 | Loss: 0.00001589
Iteration 21/1000 | Loss: 0.00001588
Iteration 22/1000 | Loss: 0.00001587
Iteration 23/1000 | Loss: 0.00001586
Iteration 24/1000 | Loss: 0.00001586
Iteration 25/1000 | Loss: 0.00001585
Iteration 26/1000 | Loss: 0.00001584
Iteration 27/1000 | Loss: 0.00001584
Iteration 28/1000 | Loss: 0.00001583
Iteration 29/1000 | Loss: 0.00001582
Iteration 30/1000 | Loss: 0.00001582
Iteration 31/1000 | Loss: 0.00001581
Iteration 32/1000 | Loss: 0.00001581
Iteration 33/1000 | Loss: 0.00001581
Iteration 34/1000 | Loss: 0.00001581
Iteration 35/1000 | Loss: 0.00001580
Iteration 36/1000 | Loss: 0.00001579
Iteration 37/1000 | Loss: 0.00001579
Iteration 38/1000 | Loss: 0.00001578
Iteration 39/1000 | Loss: 0.00001577
Iteration 40/1000 | Loss: 0.00001577
Iteration 41/1000 | Loss: 0.00001576
Iteration 42/1000 | Loss: 0.00001574
Iteration 43/1000 | Loss: 0.00001573
Iteration 44/1000 | Loss: 0.00001573
Iteration 45/1000 | Loss: 0.00001572
Iteration 46/1000 | Loss: 0.00001572
Iteration 47/1000 | Loss: 0.00001571
Iteration 48/1000 | Loss: 0.00001571
Iteration 49/1000 | Loss: 0.00001571
Iteration 50/1000 | Loss: 0.00001570
Iteration 51/1000 | Loss: 0.00001570
Iteration 52/1000 | Loss: 0.00001570
Iteration 53/1000 | Loss: 0.00001570
Iteration 54/1000 | Loss: 0.00001570
Iteration 55/1000 | Loss: 0.00001569
Iteration 56/1000 | Loss: 0.00001569
Iteration 57/1000 | Loss: 0.00001569
Iteration 58/1000 | Loss: 0.00001568
Iteration 59/1000 | Loss: 0.00001568
Iteration 60/1000 | Loss: 0.00001567
Iteration 61/1000 | Loss: 0.00001567
Iteration 62/1000 | Loss: 0.00001567
Iteration 63/1000 | Loss: 0.00001567
Iteration 64/1000 | Loss: 0.00001567
Iteration 65/1000 | Loss: 0.00001567
Iteration 66/1000 | Loss: 0.00001567
Iteration 67/1000 | Loss: 0.00001566
Iteration 68/1000 | Loss: 0.00001566
Iteration 69/1000 | Loss: 0.00001566
Iteration 70/1000 | Loss: 0.00001566
Iteration 71/1000 | Loss: 0.00001566
Iteration 72/1000 | Loss: 0.00001566
Iteration 73/1000 | Loss: 0.00001566
Iteration 74/1000 | Loss: 0.00001565
Iteration 75/1000 | Loss: 0.00001565
Iteration 76/1000 | Loss: 0.00001565
Iteration 77/1000 | Loss: 0.00001565
Iteration 78/1000 | Loss: 0.00001565
Iteration 79/1000 | Loss: 0.00001565
Iteration 80/1000 | Loss: 0.00001564
Iteration 81/1000 | Loss: 0.00001564
Iteration 82/1000 | Loss: 0.00001563
Iteration 83/1000 | Loss: 0.00001563
Iteration 84/1000 | Loss: 0.00001563
Iteration 85/1000 | Loss: 0.00001563
Iteration 86/1000 | Loss: 0.00001563
Iteration 87/1000 | Loss: 0.00001562
Iteration 88/1000 | Loss: 0.00001562
Iteration 89/1000 | Loss: 0.00001562
Iteration 90/1000 | Loss: 0.00001562
Iteration 91/1000 | Loss: 0.00001561
Iteration 92/1000 | Loss: 0.00001561
Iteration 93/1000 | Loss: 0.00001557
Iteration 94/1000 | Loss: 0.00001557
Iteration 95/1000 | Loss: 0.00001556
Iteration 96/1000 | Loss: 0.00001555
Iteration 97/1000 | Loss: 0.00001555
Iteration 98/1000 | Loss: 0.00001555
Iteration 99/1000 | Loss: 0.00001554
Iteration 100/1000 | Loss: 0.00001554
Iteration 101/1000 | Loss: 0.00001554
Iteration 102/1000 | Loss: 0.00001554
Iteration 103/1000 | Loss: 0.00001554
Iteration 104/1000 | Loss: 0.00001554
Iteration 105/1000 | Loss: 0.00001554
Iteration 106/1000 | Loss: 0.00001554
Iteration 107/1000 | Loss: 0.00001554
Iteration 108/1000 | Loss: 0.00001553
Iteration 109/1000 | Loss: 0.00001553
Iteration 110/1000 | Loss: 0.00001553
Iteration 111/1000 | Loss: 0.00001553
Iteration 112/1000 | Loss: 0.00001553
Iteration 113/1000 | Loss: 0.00001553
Iteration 114/1000 | Loss: 0.00001553
Iteration 115/1000 | Loss: 0.00001553
Iteration 116/1000 | Loss: 0.00001553
Iteration 117/1000 | Loss: 0.00001553
Iteration 118/1000 | Loss: 0.00001553
Iteration 119/1000 | Loss: 0.00001553
Iteration 120/1000 | Loss: 0.00001553
Iteration 121/1000 | Loss: 0.00001552
Iteration 122/1000 | Loss: 0.00001552
Iteration 123/1000 | Loss: 0.00001552
Iteration 124/1000 | Loss: 0.00001552
Iteration 125/1000 | Loss: 0.00001552
Iteration 126/1000 | Loss: 0.00001552
Iteration 127/1000 | Loss: 0.00001552
Iteration 128/1000 | Loss: 0.00001552
Iteration 129/1000 | Loss: 0.00001552
Iteration 130/1000 | Loss: 0.00001552
Iteration 131/1000 | Loss: 0.00001552
Iteration 132/1000 | Loss: 0.00001552
Iteration 133/1000 | Loss: 0.00001552
Iteration 134/1000 | Loss: 0.00001552
Iteration 135/1000 | Loss: 0.00001552
Iteration 136/1000 | Loss: 0.00001552
Iteration 137/1000 | Loss: 0.00001551
Iteration 138/1000 | Loss: 0.00001551
Iteration 139/1000 | Loss: 0.00001551
Iteration 140/1000 | Loss: 0.00001551
Iteration 141/1000 | Loss: 0.00001551
Iteration 142/1000 | Loss: 0.00001551
Iteration 143/1000 | Loss: 0.00001551
Iteration 144/1000 | Loss: 0.00001551
Iteration 145/1000 | Loss: 0.00001551
Iteration 146/1000 | Loss: 0.00001551
Iteration 147/1000 | Loss: 0.00001551
Iteration 148/1000 | Loss: 0.00001551
Iteration 149/1000 | Loss: 0.00001551
Iteration 150/1000 | Loss: 0.00001551
Iteration 151/1000 | Loss: 0.00001551
Iteration 152/1000 | Loss: 0.00001550
Iteration 153/1000 | Loss: 0.00001550
Iteration 154/1000 | Loss: 0.00001550
Iteration 155/1000 | Loss: 0.00001550
Iteration 156/1000 | Loss: 0.00001550
Iteration 157/1000 | Loss: 0.00001550
Iteration 158/1000 | Loss: 0.00001550
Iteration 159/1000 | Loss: 0.00001550
Iteration 160/1000 | Loss: 0.00001550
Iteration 161/1000 | Loss: 0.00001550
Iteration 162/1000 | Loss: 0.00001550
Iteration 163/1000 | Loss: 0.00001550
Iteration 164/1000 | Loss: 0.00001550
Iteration 165/1000 | Loss: 0.00001549
Iteration 166/1000 | Loss: 0.00001549
Iteration 167/1000 | Loss: 0.00001549
Iteration 168/1000 | Loss: 0.00001549
Iteration 169/1000 | Loss: 0.00001549
Iteration 170/1000 | Loss: 0.00001549
Iteration 171/1000 | Loss: 0.00001549
Iteration 172/1000 | Loss: 0.00001549
Iteration 173/1000 | Loss: 0.00001549
Iteration 174/1000 | Loss: 0.00001549
Iteration 175/1000 | Loss: 0.00001549
Iteration 176/1000 | Loss: 0.00001549
Iteration 177/1000 | Loss: 0.00001549
Iteration 178/1000 | Loss: 0.00001549
Iteration 179/1000 | Loss: 0.00001549
Iteration 180/1000 | Loss: 0.00001549
Iteration 181/1000 | Loss: 0.00001549
Iteration 182/1000 | Loss: 0.00001549
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [1.5488581993849948e-05, 1.5488581993849948e-05, 1.5488581993849948e-05, 1.5488581993849948e-05, 1.5488581993849948e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5488581993849948e-05

Optimization complete. Final v2v error: 3.3204808235168457 mm

Highest mean error: 3.6856255531311035 mm for frame 150

Lowest mean error: 3.071455717086792 mm for frame 100

Saving results

Total time: 37.486215114593506
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00398687
Iteration 2/25 | Loss: 0.00084633
Iteration 3/25 | Loss: 0.00063778
Iteration 4/25 | Loss: 0.00062253
Iteration 5/25 | Loss: 0.00061307
Iteration 6/25 | Loss: 0.00061076
Iteration 7/25 | Loss: 0.00061048
Iteration 8/25 | Loss: 0.00061048
Iteration 9/25 | Loss: 0.00061048
Iteration 10/25 | Loss: 0.00061048
Iteration 11/25 | Loss: 0.00061048
Iteration 12/25 | Loss: 0.00061048
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006104811909608543, 0.0006104811909608543, 0.0006104811909608543, 0.0006104811909608543, 0.0006104811909608543]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006104811909608543

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.68226683
Iteration 2/25 | Loss: 0.00027463
Iteration 3/25 | Loss: 0.00027463
Iteration 4/25 | Loss: 0.00027463
Iteration 5/25 | Loss: 0.00027463
Iteration 6/25 | Loss: 0.00027463
Iteration 7/25 | Loss: 0.00027463
Iteration 8/25 | Loss: 0.00027463
Iteration 9/25 | Loss: 0.00027463
Iteration 10/25 | Loss: 0.00027463
Iteration 11/25 | Loss: 0.00027463
Iteration 12/25 | Loss: 0.00027463
Iteration 13/25 | Loss: 0.00027463
Iteration 14/25 | Loss: 0.00027463
Iteration 15/25 | Loss: 0.00027463
Iteration 16/25 | Loss: 0.00027463
Iteration 17/25 | Loss: 0.00027463
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0002746313693933189, 0.0002746313693933189, 0.0002746313693933189, 0.0002746313693933189, 0.0002746313693933189]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002746313693933189

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027463
Iteration 2/1000 | Loss: 0.00002239
Iteration 3/1000 | Loss: 0.00001470
Iteration 4/1000 | Loss: 0.00001354
Iteration 5/1000 | Loss: 0.00001295
Iteration 6/1000 | Loss: 0.00001261
Iteration 7/1000 | Loss: 0.00001250
Iteration 8/1000 | Loss: 0.00001244
Iteration 9/1000 | Loss: 0.00001232
Iteration 10/1000 | Loss: 0.00001216
Iteration 11/1000 | Loss: 0.00001212
Iteration 12/1000 | Loss: 0.00001203
Iteration 13/1000 | Loss: 0.00001202
Iteration 14/1000 | Loss: 0.00001201
Iteration 15/1000 | Loss: 0.00001190
Iteration 16/1000 | Loss: 0.00001188
Iteration 17/1000 | Loss: 0.00001186
Iteration 18/1000 | Loss: 0.00001185
Iteration 19/1000 | Loss: 0.00001185
Iteration 20/1000 | Loss: 0.00001184
Iteration 21/1000 | Loss: 0.00001183
Iteration 22/1000 | Loss: 0.00001182
Iteration 23/1000 | Loss: 0.00001181
Iteration 24/1000 | Loss: 0.00001178
Iteration 25/1000 | Loss: 0.00001178
Iteration 26/1000 | Loss: 0.00001178
Iteration 27/1000 | Loss: 0.00001178
Iteration 28/1000 | Loss: 0.00001178
Iteration 29/1000 | Loss: 0.00001178
Iteration 30/1000 | Loss: 0.00001178
Iteration 31/1000 | Loss: 0.00001178
Iteration 32/1000 | Loss: 0.00001178
Iteration 33/1000 | Loss: 0.00001178
Iteration 34/1000 | Loss: 0.00001178
Iteration 35/1000 | Loss: 0.00001178
Iteration 36/1000 | Loss: 0.00001178
Iteration 37/1000 | Loss: 0.00001178
Iteration 38/1000 | Loss: 0.00001178
Iteration 39/1000 | Loss: 0.00001178
Iteration 40/1000 | Loss: 0.00001178
Iteration 41/1000 | Loss: 0.00001178
Iteration 42/1000 | Loss: 0.00001178
Iteration 43/1000 | Loss: 0.00001178
Iteration 44/1000 | Loss: 0.00001178
Iteration 45/1000 | Loss: 0.00001178
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 45. Stopping optimization.
Last 5 losses: [1.178086495201569e-05, 1.178086495201569e-05, 1.178086495201569e-05, 1.178086495201569e-05, 1.178086495201569e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.178086495201569e-05

Optimization complete. Final v2v error: 2.9246017932891846 mm

Highest mean error: 3.0977225303649902 mm for frame 1

Lowest mean error: 2.7446277141571045 mm for frame 203

Saving results

Total time: 29.44682812690735
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00474692
Iteration 2/25 | Loss: 0.00078791
Iteration 3/25 | Loss: 0.00063277
Iteration 4/25 | Loss: 0.00060047
Iteration 5/25 | Loss: 0.00059487
Iteration 6/25 | Loss: 0.00059405
Iteration 7/25 | Loss: 0.00059405
Iteration 8/25 | Loss: 0.00059405
Iteration 9/25 | Loss: 0.00059405
Iteration 10/25 | Loss: 0.00059405
Iteration 11/25 | Loss: 0.00059405
Iteration 12/25 | Loss: 0.00059405
Iteration 13/25 | Loss: 0.00059405
Iteration 14/25 | Loss: 0.00059405
Iteration 15/25 | Loss: 0.00059405
Iteration 16/25 | Loss: 0.00059405
Iteration 17/25 | Loss: 0.00059405
Iteration 18/25 | Loss: 0.00059405
Iteration 19/25 | Loss: 0.00059405
Iteration 20/25 | Loss: 0.00059405
Iteration 21/25 | Loss: 0.00059405
Iteration 22/25 | Loss: 0.00059405
Iteration 23/25 | Loss: 0.00059405
Iteration 24/25 | Loss: 0.00059405
Iteration 25/25 | Loss: 0.00059405

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.12968302
Iteration 2/25 | Loss: 0.00019789
Iteration 3/25 | Loss: 0.00019789
Iteration 4/25 | Loss: 0.00019789
Iteration 5/25 | Loss: 0.00019789
Iteration 6/25 | Loss: 0.00019789
Iteration 7/25 | Loss: 0.00019789
Iteration 8/25 | Loss: 0.00019789
Iteration 9/25 | Loss: 0.00019789
Iteration 10/25 | Loss: 0.00019789
Iteration 11/25 | Loss: 0.00019789
Iteration 12/25 | Loss: 0.00019789
Iteration 13/25 | Loss: 0.00019789
Iteration 14/25 | Loss: 0.00019789
Iteration 15/25 | Loss: 0.00019789
Iteration 16/25 | Loss: 0.00019789
Iteration 17/25 | Loss: 0.00019789
Iteration 18/25 | Loss: 0.00019789
Iteration 19/25 | Loss: 0.00019789
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00019788536883424968, 0.00019788536883424968, 0.00019788536883424968, 0.00019788536883424968, 0.00019788536883424968]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00019788536883424968

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00019789
Iteration 2/1000 | Loss: 0.00001872
Iteration 3/1000 | Loss: 0.00001570
Iteration 4/1000 | Loss: 0.00001500
Iteration 5/1000 | Loss: 0.00001402
Iteration 6/1000 | Loss: 0.00001367
Iteration 7/1000 | Loss: 0.00001335
Iteration 8/1000 | Loss: 0.00001329
Iteration 9/1000 | Loss: 0.00001324
Iteration 10/1000 | Loss: 0.00001312
Iteration 11/1000 | Loss: 0.00001312
Iteration 12/1000 | Loss: 0.00001306
Iteration 13/1000 | Loss: 0.00001304
Iteration 14/1000 | Loss: 0.00001302
Iteration 15/1000 | Loss: 0.00001302
Iteration 16/1000 | Loss: 0.00001302
Iteration 17/1000 | Loss: 0.00001301
Iteration 18/1000 | Loss: 0.00001300
Iteration 19/1000 | Loss: 0.00001299
Iteration 20/1000 | Loss: 0.00001298
Iteration 21/1000 | Loss: 0.00001297
Iteration 22/1000 | Loss: 0.00001296
Iteration 23/1000 | Loss: 0.00001291
Iteration 24/1000 | Loss: 0.00001291
Iteration 25/1000 | Loss: 0.00001290
Iteration 26/1000 | Loss: 0.00001290
Iteration 27/1000 | Loss: 0.00001289
Iteration 28/1000 | Loss: 0.00001287
Iteration 29/1000 | Loss: 0.00001285
Iteration 30/1000 | Loss: 0.00001285
Iteration 31/1000 | Loss: 0.00001284
Iteration 32/1000 | Loss: 0.00001284
Iteration 33/1000 | Loss: 0.00001284
Iteration 34/1000 | Loss: 0.00001283
Iteration 35/1000 | Loss: 0.00001279
Iteration 36/1000 | Loss: 0.00001279
Iteration 37/1000 | Loss: 0.00001279
Iteration 38/1000 | Loss: 0.00001279
Iteration 39/1000 | Loss: 0.00001278
Iteration 40/1000 | Loss: 0.00001276
Iteration 41/1000 | Loss: 0.00001276
Iteration 42/1000 | Loss: 0.00001276
Iteration 43/1000 | Loss: 0.00001276
Iteration 44/1000 | Loss: 0.00001276
Iteration 45/1000 | Loss: 0.00001276
Iteration 46/1000 | Loss: 0.00001276
Iteration 47/1000 | Loss: 0.00001275
Iteration 48/1000 | Loss: 0.00001275
Iteration 49/1000 | Loss: 0.00001275
Iteration 50/1000 | Loss: 0.00001275
Iteration 51/1000 | Loss: 0.00001275
Iteration 52/1000 | Loss: 0.00001273
Iteration 53/1000 | Loss: 0.00001272
Iteration 54/1000 | Loss: 0.00001271
Iteration 55/1000 | Loss: 0.00001271
Iteration 56/1000 | Loss: 0.00001271
Iteration 57/1000 | Loss: 0.00001271
Iteration 58/1000 | Loss: 0.00001270
Iteration 59/1000 | Loss: 0.00001270
Iteration 60/1000 | Loss: 0.00001269
Iteration 61/1000 | Loss: 0.00001269
Iteration 62/1000 | Loss: 0.00001269
Iteration 63/1000 | Loss: 0.00001269
Iteration 64/1000 | Loss: 0.00001268
Iteration 65/1000 | Loss: 0.00001268
Iteration 66/1000 | Loss: 0.00001268
Iteration 67/1000 | Loss: 0.00001267
Iteration 68/1000 | Loss: 0.00001267
Iteration 69/1000 | Loss: 0.00001266
Iteration 70/1000 | Loss: 0.00001266
Iteration 71/1000 | Loss: 0.00001266
Iteration 72/1000 | Loss: 0.00001265
Iteration 73/1000 | Loss: 0.00001265
Iteration 74/1000 | Loss: 0.00001265
Iteration 75/1000 | Loss: 0.00001265
Iteration 76/1000 | Loss: 0.00001264
Iteration 77/1000 | Loss: 0.00001264
Iteration 78/1000 | Loss: 0.00001263
Iteration 79/1000 | Loss: 0.00001263
Iteration 80/1000 | Loss: 0.00001263
Iteration 81/1000 | Loss: 0.00001262
Iteration 82/1000 | Loss: 0.00001262
Iteration 83/1000 | Loss: 0.00001262
Iteration 84/1000 | Loss: 0.00001262
Iteration 85/1000 | Loss: 0.00001261
Iteration 86/1000 | Loss: 0.00001261
Iteration 87/1000 | Loss: 0.00001259
Iteration 88/1000 | Loss: 0.00001258
Iteration 89/1000 | Loss: 0.00001258
Iteration 90/1000 | Loss: 0.00001258
Iteration 91/1000 | Loss: 0.00001257
Iteration 92/1000 | Loss: 0.00001257
Iteration 93/1000 | Loss: 0.00001257
Iteration 94/1000 | Loss: 0.00001257
Iteration 95/1000 | Loss: 0.00001256
Iteration 96/1000 | Loss: 0.00001256
Iteration 97/1000 | Loss: 0.00001256
Iteration 98/1000 | Loss: 0.00001256
Iteration 99/1000 | Loss: 0.00001256
Iteration 100/1000 | Loss: 0.00001255
Iteration 101/1000 | Loss: 0.00001255
Iteration 102/1000 | Loss: 0.00001255
Iteration 103/1000 | Loss: 0.00001255
Iteration 104/1000 | Loss: 0.00001255
Iteration 105/1000 | Loss: 0.00001255
Iteration 106/1000 | Loss: 0.00001255
Iteration 107/1000 | Loss: 0.00001255
Iteration 108/1000 | Loss: 0.00001255
Iteration 109/1000 | Loss: 0.00001255
Iteration 110/1000 | Loss: 0.00001255
Iteration 111/1000 | Loss: 0.00001255
Iteration 112/1000 | Loss: 0.00001254
Iteration 113/1000 | Loss: 0.00001254
Iteration 114/1000 | Loss: 0.00001254
Iteration 115/1000 | Loss: 0.00001254
Iteration 116/1000 | Loss: 0.00001254
Iteration 117/1000 | Loss: 0.00001254
Iteration 118/1000 | Loss: 0.00001254
Iteration 119/1000 | Loss: 0.00001254
Iteration 120/1000 | Loss: 0.00001254
Iteration 121/1000 | Loss: 0.00001254
Iteration 122/1000 | Loss: 0.00001254
Iteration 123/1000 | Loss: 0.00001254
Iteration 124/1000 | Loss: 0.00001254
Iteration 125/1000 | Loss: 0.00001254
Iteration 126/1000 | Loss: 0.00001254
Iteration 127/1000 | Loss: 0.00001253
Iteration 128/1000 | Loss: 0.00001253
Iteration 129/1000 | Loss: 0.00001253
Iteration 130/1000 | Loss: 0.00001253
Iteration 131/1000 | Loss: 0.00001252
Iteration 132/1000 | Loss: 0.00001252
Iteration 133/1000 | Loss: 0.00001252
Iteration 134/1000 | Loss: 0.00001252
Iteration 135/1000 | Loss: 0.00001252
Iteration 136/1000 | Loss: 0.00001252
Iteration 137/1000 | Loss: 0.00001252
Iteration 138/1000 | Loss: 0.00001252
Iteration 139/1000 | Loss: 0.00001252
Iteration 140/1000 | Loss: 0.00001251
Iteration 141/1000 | Loss: 0.00001251
Iteration 142/1000 | Loss: 0.00001251
Iteration 143/1000 | Loss: 0.00001251
Iteration 144/1000 | Loss: 0.00001251
Iteration 145/1000 | Loss: 0.00001251
Iteration 146/1000 | Loss: 0.00001250
Iteration 147/1000 | Loss: 0.00001250
Iteration 148/1000 | Loss: 0.00001250
Iteration 149/1000 | Loss: 0.00001250
Iteration 150/1000 | Loss: 0.00001250
Iteration 151/1000 | Loss: 0.00001250
Iteration 152/1000 | Loss: 0.00001250
Iteration 153/1000 | Loss: 0.00001250
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.2501976016210392e-05, 1.2501976016210392e-05, 1.2501976016210392e-05, 1.2501976016210392e-05, 1.2501976016210392e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2501976016210392e-05

Optimization complete. Final v2v error: 3.006697416305542 mm

Highest mean error: 3.308171272277832 mm for frame 228

Lowest mean error: 2.822965383529663 mm for frame 169

Saving results

Total time: 38.29979372024536
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00816832
Iteration 2/25 | Loss: 0.00101203
Iteration 3/25 | Loss: 0.00068621
Iteration 4/25 | Loss: 0.00066068
Iteration 5/25 | Loss: 0.00065563
Iteration 6/25 | Loss: 0.00065490
Iteration 7/25 | Loss: 0.00065490
Iteration 8/25 | Loss: 0.00065490
Iteration 9/25 | Loss: 0.00065490
Iteration 10/25 | Loss: 0.00065490
Iteration 11/25 | Loss: 0.00065490
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0006549012614414096, 0.0006549012614414096, 0.0006549012614414096, 0.0006549012614414096, 0.0006549012614414096]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006549012614414096

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44011819
Iteration 2/25 | Loss: 0.00032043
Iteration 3/25 | Loss: 0.00032043
Iteration 4/25 | Loss: 0.00032043
Iteration 5/25 | Loss: 0.00032042
Iteration 6/25 | Loss: 0.00032042
Iteration 7/25 | Loss: 0.00032042
Iteration 8/25 | Loss: 0.00032042
Iteration 9/25 | Loss: 0.00032042
Iteration 10/25 | Loss: 0.00032042
Iteration 11/25 | Loss: 0.00032042
Iteration 12/25 | Loss: 0.00032042
Iteration 13/25 | Loss: 0.00032042
Iteration 14/25 | Loss: 0.00032042
Iteration 15/25 | Loss: 0.00032042
Iteration 16/25 | Loss: 0.00032042
Iteration 17/25 | Loss: 0.00032042
Iteration 18/25 | Loss: 0.00032042
Iteration 19/25 | Loss: 0.00032042
Iteration 20/25 | Loss: 0.00032042
Iteration 21/25 | Loss: 0.00032042
Iteration 22/25 | Loss: 0.00032042
Iteration 23/25 | Loss: 0.00032042
Iteration 24/25 | Loss: 0.00032042
Iteration 25/25 | Loss: 0.00032042

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032042
Iteration 2/1000 | Loss: 0.00002152
Iteration 3/1000 | Loss: 0.00001861
Iteration 4/1000 | Loss: 0.00001756
Iteration 5/1000 | Loss: 0.00001694
Iteration 6/1000 | Loss: 0.00001647
Iteration 7/1000 | Loss: 0.00001614
Iteration 8/1000 | Loss: 0.00001596
Iteration 9/1000 | Loss: 0.00001589
Iteration 10/1000 | Loss: 0.00001588
Iteration 11/1000 | Loss: 0.00001588
Iteration 12/1000 | Loss: 0.00001587
Iteration 13/1000 | Loss: 0.00001585
Iteration 14/1000 | Loss: 0.00001585
Iteration 15/1000 | Loss: 0.00001584
Iteration 16/1000 | Loss: 0.00001584
Iteration 17/1000 | Loss: 0.00001584
Iteration 18/1000 | Loss: 0.00001583
Iteration 19/1000 | Loss: 0.00001579
Iteration 20/1000 | Loss: 0.00001579
Iteration 21/1000 | Loss: 0.00001579
Iteration 22/1000 | Loss: 0.00001578
Iteration 23/1000 | Loss: 0.00001578
Iteration 24/1000 | Loss: 0.00001578
Iteration 25/1000 | Loss: 0.00001577
Iteration 26/1000 | Loss: 0.00001577
Iteration 27/1000 | Loss: 0.00001577
Iteration 28/1000 | Loss: 0.00001576
Iteration 29/1000 | Loss: 0.00001575
Iteration 30/1000 | Loss: 0.00001575
Iteration 31/1000 | Loss: 0.00001574
Iteration 32/1000 | Loss: 0.00001574
Iteration 33/1000 | Loss: 0.00001574
Iteration 34/1000 | Loss: 0.00001574
Iteration 35/1000 | Loss: 0.00001574
Iteration 36/1000 | Loss: 0.00001574
Iteration 37/1000 | Loss: 0.00001574
Iteration 38/1000 | Loss: 0.00001574
Iteration 39/1000 | Loss: 0.00001574
Iteration 40/1000 | Loss: 0.00001574
Iteration 41/1000 | Loss: 0.00001574
Iteration 42/1000 | Loss: 0.00001574
Iteration 43/1000 | Loss: 0.00001574
Iteration 44/1000 | Loss: 0.00001573
Iteration 45/1000 | Loss: 0.00001573
Iteration 46/1000 | Loss: 0.00001573
Iteration 47/1000 | Loss: 0.00001573
Iteration 48/1000 | Loss: 0.00001573
Iteration 49/1000 | Loss: 0.00001573
Iteration 50/1000 | Loss: 0.00001572
Iteration 51/1000 | Loss: 0.00001572
Iteration 52/1000 | Loss: 0.00001572
Iteration 53/1000 | Loss: 0.00001572
Iteration 54/1000 | Loss: 0.00001572
Iteration 55/1000 | Loss: 0.00001572
Iteration 56/1000 | Loss: 0.00001572
Iteration 57/1000 | Loss: 0.00001572
Iteration 58/1000 | Loss: 0.00001572
Iteration 59/1000 | Loss: 0.00001571
Iteration 60/1000 | Loss: 0.00001571
Iteration 61/1000 | Loss: 0.00001571
Iteration 62/1000 | Loss: 0.00001571
Iteration 63/1000 | Loss: 0.00001571
Iteration 64/1000 | Loss: 0.00001570
Iteration 65/1000 | Loss: 0.00001570
Iteration 66/1000 | Loss: 0.00001570
Iteration 67/1000 | Loss: 0.00001570
Iteration 68/1000 | Loss: 0.00001570
Iteration 69/1000 | Loss: 0.00001570
Iteration 70/1000 | Loss: 0.00001570
Iteration 71/1000 | Loss: 0.00001570
Iteration 72/1000 | Loss: 0.00001570
Iteration 73/1000 | Loss: 0.00001570
Iteration 74/1000 | Loss: 0.00001570
Iteration 75/1000 | Loss: 0.00001570
Iteration 76/1000 | Loss: 0.00001570
Iteration 77/1000 | Loss: 0.00001570
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 77. Stopping optimization.
Last 5 losses: [1.5700583389843814e-05, 1.5700583389843814e-05, 1.5700583389843814e-05, 1.5700583389843814e-05, 1.5700583389843814e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5700583389843814e-05

Optimization complete. Final v2v error: 3.350024938583374 mm

Highest mean error: 3.6214492321014404 mm for frame 24

Lowest mean error: 3.0040199756622314 mm for frame 88

Saving results

Total time: 28.468099355697632
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01061865
Iteration 2/25 | Loss: 0.00138254
Iteration 3/25 | Loss: 0.00116082
Iteration 4/25 | Loss: 0.00094094
Iteration 5/25 | Loss: 0.00067512
Iteration 6/25 | Loss: 0.00065988
Iteration 7/25 | Loss: 0.00065688
Iteration 8/25 | Loss: 0.00065897
Iteration 9/25 | Loss: 0.00065692
Iteration 10/25 | Loss: 0.00064354
Iteration 11/25 | Loss: 0.00064789
Iteration 12/25 | Loss: 0.00064017
Iteration 13/25 | Loss: 0.00065097
Iteration 14/25 | Loss: 0.00064553
Iteration 15/25 | Loss: 0.00063570
Iteration 16/25 | Loss: 0.00063062
Iteration 17/25 | Loss: 0.00062830
Iteration 18/25 | Loss: 0.00062653
Iteration 19/25 | Loss: 0.00062678
Iteration 20/25 | Loss: 0.00062059
Iteration 21/25 | Loss: 0.00061311
Iteration 22/25 | Loss: 0.00061913
Iteration 23/25 | Loss: 0.00061735
Iteration 24/25 | Loss: 0.00062535
Iteration 25/25 | Loss: 0.00061720

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52200484
Iteration 2/25 | Loss: 0.00041063
Iteration 3/25 | Loss: 0.00041063
Iteration 4/25 | Loss: 0.00041063
Iteration 5/25 | Loss: 0.00041063
Iteration 6/25 | Loss: 0.00041063
Iteration 7/25 | Loss: 0.00041063
Iteration 8/25 | Loss: 0.00041063
Iteration 9/25 | Loss: 0.00041063
Iteration 10/25 | Loss: 0.00041063
Iteration 11/25 | Loss: 0.00041063
Iteration 12/25 | Loss: 0.00041063
Iteration 13/25 | Loss: 0.00041063
Iteration 14/25 | Loss: 0.00041063
Iteration 15/25 | Loss: 0.00041063
Iteration 16/25 | Loss: 0.00041063
Iteration 17/25 | Loss: 0.00041063
Iteration 18/25 | Loss: 0.00041063
Iteration 19/25 | Loss: 0.00041063
Iteration 20/25 | Loss: 0.00041063
Iteration 21/25 | Loss: 0.00041063
Iteration 22/25 | Loss: 0.00041063
Iteration 23/25 | Loss: 0.00041063
Iteration 24/25 | Loss: 0.00041063
Iteration 25/25 | Loss: 0.00041063

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041063
Iteration 2/1000 | Loss: 0.00032765
Iteration 3/1000 | Loss: 0.00016307
Iteration 4/1000 | Loss: 0.00011683
Iteration 5/1000 | Loss: 0.00002619
Iteration 6/1000 | Loss: 0.00002142
Iteration 7/1000 | Loss: 0.00001855
Iteration 8/1000 | Loss: 0.00022593
Iteration 9/1000 | Loss: 0.00002101
Iteration 10/1000 | Loss: 0.00001741
Iteration 11/1000 | Loss: 0.00001585
Iteration 12/1000 | Loss: 0.00001480
Iteration 13/1000 | Loss: 0.00027233
Iteration 14/1000 | Loss: 0.00002407
Iteration 15/1000 | Loss: 0.00001803
Iteration 16/1000 | Loss: 0.00001577
Iteration 17/1000 | Loss: 0.00041624
Iteration 18/1000 | Loss: 0.00013089
Iteration 19/1000 | Loss: 0.00020050
Iteration 20/1000 | Loss: 0.00002486
Iteration 21/1000 | Loss: 0.00001802
Iteration 22/1000 | Loss: 0.00001524
Iteration 23/1000 | Loss: 0.00001373
Iteration 24/1000 | Loss: 0.00001249
Iteration 25/1000 | Loss: 0.00001181
Iteration 26/1000 | Loss: 0.00001119
Iteration 27/1000 | Loss: 0.00001081
Iteration 28/1000 | Loss: 0.00001049
Iteration 29/1000 | Loss: 0.00001037
Iteration 30/1000 | Loss: 0.00001036
Iteration 31/1000 | Loss: 0.00001031
Iteration 32/1000 | Loss: 0.00001031
Iteration 33/1000 | Loss: 0.00001030
Iteration 34/1000 | Loss: 0.00001029
Iteration 35/1000 | Loss: 0.00001021
Iteration 36/1000 | Loss: 0.00001018
Iteration 37/1000 | Loss: 0.00001017
Iteration 38/1000 | Loss: 0.00001012
Iteration 39/1000 | Loss: 0.00001012
Iteration 40/1000 | Loss: 0.00001009
Iteration 41/1000 | Loss: 0.00001008
Iteration 42/1000 | Loss: 0.00001008
Iteration 43/1000 | Loss: 0.00001007
Iteration 44/1000 | Loss: 0.00001007
Iteration 45/1000 | Loss: 0.00001007
Iteration 46/1000 | Loss: 0.00001007
Iteration 47/1000 | Loss: 0.00001006
Iteration 48/1000 | Loss: 0.00001006
Iteration 49/1000 | Loss: 0.00001005
Iteration 50/1000 | Loss: 0.00001005
Iteration 51/1000 | Loss: 0.00001005
Iteration 52/1000 | Loss: 0.00001004
Iteration 53/1000 | Loss: 0.00001004
Iteration 54/1000 | Loss: 0.00001004
Iteration 55/1000 | Loss: 0.00001004
Iteration 56/1000 | Loss: 0.00001004
Iteration 57/1000 | Loss: 0.00001004
Iteration 58/1000 | Loss: 0.00001004
Iteration 59/1000 | Loss: 0.00001004
Iteration 60/1000 | Loss: 0.00001003
Iteration 61/1000 | Loss: 0.00001003
Iteration 62/1000 | Loss: 0.00001003
Iteration 63/1000 | Loss: 0.00001002
Iteration 64/1000 | Loss: 0.00001002
Iteration 65/1000 | Loss: 0.00001002
Iteration 66/1000 | Loss: 0.00001002
Iteration 67/1000 | Loss: 0.00001002
Iteration 68/1000 | Loss: 0.00001001
Iteration 69/1000 | Loss: 0.00001001
Iteration 70/1000 | Loss: 0.00001001
Iteration 71/1000 | Loss: 0.00001001
Iteration 72/1000 | Loss: 0.00001001
Iteration 73/1000 | Loss: 0.00001001
Iteration 74/1000 | Loss: 0.00001001
Iteration 75/1000 | Loss: 0.00001001
Iteration 76/1000 | Loss: 0.00001001
Iteration 77/1000 | Loss: 0.00001001
Iteration 78/1000 | Loss: 0.00001000
Iteration 79/1000 | Loss: 0.00001000
Iteration 80/1000 | Loss: 0.00001000
Iteration 81/1000 | Loss: 0.00001000
Iteration 82/1000 | Loss: 0.00001000
Iteration 83/1000 | Loss: 0.00000999
Iteration 84/1000 | Loss: 0.00000999
Iteration 85/1000 | Loss: 0.00000999
Iteration 86/1000 | Loss: 0.00000999
Iteration 87/1000 | Loss: 0.00000999
Iteration 88/1000 | Loss: 0.00000999
Iteration 89/1000 | Loss: 0.00000999
Iteration 90/1000 | Loss: 0.00000998
Iteration 91/1000 | Loss: 0.00000998
Iteration 92/1000 | Loss: 0.00000998
Iteration 93/1000 | Loss: 0.00000998
Iteration 94/1000 | Loss: 0.00000998
Iteration 95/1000 | Loss: 0.00000998
Iteration 96/1000 | Loss: 0.00000998
Iteration 97/1000 | Loss: 0.00000998
Iteration 98/1000 | Loss: 0.00000998
Iteration 99/1000 | Loss: 0.00000997
Iteration 100/1000 | Loss: 0.00000997
Iteration 101/1000 | Loss: 0.00000997
Iteration 102/1000 | Loss: 0.00000997
Iteration 103/1000 | Loss: 0.00000997
Iteration 104/1000 | Loss: 0.00000997
Iteration 105/1000 | Loss: 0.00000997
Iteration 106/1000 | Loss: 0.00000996
Iteration 107/1000 | Loss: 0.00000996
Iteration 108/1000 | Loss: 0.00000996
Iteration 109/1000 | Loss: 0.00000996
Iteration 110/1000 | Loss: 0.00000996
Iteration 111/1000 | Loss: 0.00000996
Iteration 112/1000 | Loss: 0.00000995
Iteration 113/1000 | Loss: 0.00000995
Iteration 114/1000 | Loss: 0.00000995
Iteration 115/1000 | Loss: 0.00000995
Iteration 116/1000 | Loss: 0.00000995
Iteration 117/1000 | Loss: 0.00000995
Iteration 118/1000 | Loss: 0.00000995
Iteration 119/1000 | Loss: 0.00000995
Iteration 120/1000 | Loss: 0.00000995
Iteration 121/1000 | Loss: 0.00000994
Iteration 122/1000 | Loss: 0.00000994
Iteration 123/1000 | Loss: 0.00000994
Iteration 124/1000 | Loss: 0.00000994
Iteration 125/1000 | Loss: 0.00000994
Iteration 126/1000 | Loss: 0.00000994
Iteration 127/1000 | Loss: 0.00000994
Iteration 128/1000 | Loss: 0.00000994
Iteration 129/1000 | Loss: 0.00000994
Iteration 130/1000 | Loss: 0.00000994
Iteration 131/1000 | Loss: 0.00000994
Iteration 132/1000 | Loss: 0.00000994
Iteration 133/1000 | Loss: 0.00000994
Iteration 134/1000 | Loss: 0.00000993
Iteration 135/1000 | Loss: 0.00000993
Iteration 136/1000 | Loss: 0.00000993
Iteration 137/1000 | Loss: 0.00000993
Iteration 138/1000 | Loss: 0.00000993
Iteration 139/1000 | Loss: 0.00000993
Iteration 140/1000 | Loss: 0.00000992
Iteration 141/1000 | Loss: 0.00000992
Iteration 142/1000 | Loss: 0.00000992
Iteration 143/1000 | Loss: 0.00000992
Iteration 144/1000 | Loss: 0.00000991
Iteration 145/1000 | Loss: 0.00000991
Iteration 146/1000 | Loss: 0.00000991
Iteration 147/1000 | Loss: 0.00000991
Iteration 148/1000 | Loss: 0.00000991
Iteration 149/1000 | Loss: 0.00000991
Iteration 150/1000 | Loss: 0.00000991
Iteration 151/1000 | Loss: 0.00000991
Iteration 152/1000 | Loss: 0.00000991
Iteration 153/1000 | Loss: 0.00000990
Iteration 154/1000 | Loss: 0.00000990
Iteration 155/1000 | Loss: 0.00000990
Iteration 156/1000 | Loss: 0.00000990
Iteration 157/1000 | Loss: 0.00000990
Iteration 158/1000 | Loss: 0.00000990
Iteration 159/1000 | Loss: 0.00000990
Iteration 160/1000 | Loss: 0.00000990
Iteration 161/1000 | Loss: 0.00000990
Iteration 162/1000 | Loss: 0.00000990
Iteration 163/1000 | Loss: 0.00000989
Iteration 164/1000 | Loss: 0.00000989
Iteration 165/1000 | Loss: 0.00000989
Iteration 166/1000 | Loss: 0.00000989
Iteration 167/1000 | Loss: 0.00000989
Iteration 168/1000 | Loss: 0.00000988
Iteration 169/1000 | Loss: 0.00000988
Iteration 170/1000 | Loss: 0.00000988
Iteration 171/1000 | Loss: 0.00000988
Iteration 172/1000 | Loss: 0.00000988
Iteration 173/1000 | Loss: 0.00000988
Iteration 174/1000 | Loss: 0.00000988
Iteration 175/1000 | Loss: 0.00000988
Iteration 176/1000 | Loss: 0.00000988
Iteration 177/1000 | Loss: 0.00000988
Iteration 178/1000 | Loss: 0.00000988
Iteration 179/1000 | Loss: 0.00000988
Iteration 180/1000 | Loss: 0.00000988
Iteration 181/1000 | Loss: 0.00000988
Iteration 182/1000 | Loss: 0.00000987
Iteration 183/1000 | Loss: 0.00000987
Iteration 184/1000 | Loss: 0.00000987
Iteration 185/1000 | Loss: 0.00000987
Iteration 186/1000 | Loss: 0.00000987
Iteration 187/1000 | Loss: 0.00000987
Iteration 188/1000 | Loss: 0.00000987
Iteration 189/1000 | Loss: 0.00000986
Iteration 190/1000 | Loss: 0.00000986
Iteration 191/1000 | Loss: 0.00000986
Iteration 192/1000 | Loss: 0.00000985
Iteration 193/1000 | Loss: 0.00000985
Iteration 194/1000 | Loss: 0.00000985
Iteration 195/1000 | Loss: 0.00000985
Iteration 196/1000 | Loss: 0.00000985
Iteration 197/1000 | Loss: 0.00000984
Iteration 198/1000 | Loss: 0.00000984
Iteration 199/1000 | Loss: 0.00000984
Iteration 200/1000 | Loss: 0.00000984
Iteration 201/1000 | Loss: 0.00000984
Iteration 202/1000 | Loss: 0.00000984
Iteration 203/1000 | Loss: 0.00000984
Iteration 204/1000 | Loss: 0.00000984
Iteration 205/1000 | Loss: 0.00000983
Iteration 206/1000 | Loss: 0.00000983
Iteration 207/1000 | Loss: 0.00000983
Iteration 208/1000 | Loss: 0.00000983
Iteration 209/1000 | Loss: 0.00000983
Iteration 210/1000 | Loss: 0.00000983
Iteration 211/1000 | Loss: 0.00000983
Iteration 212/1000 | Loss: 0.00000983
Iteration 213/1000 | Loss: 0.00000983
Iteration 214/1000 | Loss: 0.00000983
Iteration 215/1000 | Loss: 0.00000982
Iteration 216/1000 | Loss: 0.00000982
Iteration 217/1000 | Loss: 0.00000982
Iteration 218/1000 | Loss: 0.00000982
Iteration 219/1000 | Loss: 0.00000982
Iteration 220/1000 | Loss: 0.00000982
Iteration 221/1000 | Loss: 0.00000982
Iteration 222/1000 | Loss: 0.00000982
Iteration 223/1000 | Loss: 0.00000982
Iteration 224/1000 | Loss: 0.00000982
Iteration 225/1000 | Loss: 0.00000981
Iteration 226/1000 | Loss: 0.00000981
Iteration 227/1000 | Loss: 0.00000981
Iteration 228/1000 | Loss: 0.00000981
Iteration 229/1000 | Loss: 0.00000981
Iteration 230/1000 | Loss: 0.00000981
Iteration 231/1000 | Loss: 0.00000981
Iteration 232/1000 | Loss: 0.00000981
Iteration 233/1000 | Loss: 0.00000981
Iteration 234/1000 | Loss: 0.00000981
Iteration 235/1000 | Loss: 0.00000981
Iteration 236/1000 | Loss: 0.00000981
Iteration 237/1000 | Loss: 0.00000981
Iteration 238/1000 | Loss: 0.00000980
Iteration 239/1000 | Loss: 0.00000980
Iteration 240/1000 | Loss: 0.00000980
Iteration 241/1000 | Loss: 0.00000980
Iteration 242/1000 | Loss: 0.00000980
Iteration 243/1000 | Loss: 0.00000980
Iteration 244/1000 | Loss: 0.00000980
Iteration 245/1000 | Loss: 0.00000980
Iteration 246/1000 | Loss: 0.00000979
Iteration 247/1000 | Loss: 0.00000979
Iteration 248/1000 | Loss: 0.00000979
Iteration 249/1000 | Loss: 0.00000979
Iteration 250/1000 | Loss: 0.00000979
Iteration 251/1000 | Loss: 0.00000979
Iteration 252/1000 | Loss: 0.00000978
Iteration 253/1000 | Loss: 0.00000978
Iteration 254/1000 | Loss: 0.00000978
Iteration 255/1000 | Loss: 0.00000978
Iteration 256/1000 | Loss: 0.00000978
Iteration 257/1000 | Loss: 0.00000978
Iteration 258/1000 | Loss: 0.00000978
Iteration 259/1000 | Loss: 0.00000978
Iteration 260/1000 | Loss: 0.00000978
Iteration 261/1000 | Loss: 0.00000978
Iteration 262/1000 | Loss: 0.00000978
Iteration 263/1000 | Loss: 0.00000978
Iteration 264/1000 | Loss: 0.00000978
Iteration 265/1000 | Loss: 0.00000978
Iteration 266/1000 | Loss: 0.00000978
Iteration 267/1000 | Loss: 0.00000978
Iteration 268/1000 | Loss: 0.00000978
Iteration 269/1000 | Loss: 0.00000978
Iteration 270/1000 | Loss: 0.00000978
Iteration 271/1000 | Loss: 0.00000978
Iteration 272/1000 | Loss: 0.00000978
Iteration 273/1000 | Loss: 0.00000978
Iteration 274/1000 | Loss: 0.00000978
Iteration 275/1000 | Loss: 0.00000978
Iteration 276/1000 | Loss: 0.00000978
Iteration 277/1000 | Loss: 0.00000978
Iteration 278/1000 | Loss: 0.00000978
Iteration 279/1000 | Loss: 0.00000978
Iteration 280/1000 | Loss: 0.00000978
Iteration 281/1000 | Loss: 0.00000978
Iteration 282/1000 | Loss: 0.00000978
Iteration 283/1000 | Loss: 0.00000978
Iteration 284/1000 | Loss: 0.00000978
Iteration 285/1000 | Loss: 0.00000978
Iteration 286/1000 | Loss: 0.00000978
Iteration 287/1000 | Loss: 0.00000978
Iteration 288/1000 | Loss: 0.00000978
Iteration 289/1000 | Loss: 0.00000978
Iteration 290/1000 | Loss: 0.00000978
Iteration 291/1000 | Loss: 0.00000978
Iteration 292/1000 | Loss: 0.00000978
Iteration 293/1000 | Loss: 0.00000978
Iteration 294/1000 | Loss: 0.00000978
Iteration 295/1000 | Loss: 0.00000978
Iteration 296/1000 | Loss: 0.00000978
Iteration 297/1000 | Loss: 0.00000978
Iteration 298/1000 | Loss: 0.00000978
Iteration 299/1000 | Loss: 0.00000978
Iteration 300/1000 | Loss: 0.00000978
Iteration 301/1000 | Loss: 0.00000978
Iteration 302/1000 | Loss: 0.00000978
Iteration 303/1000 | Loss: 0.00000978
Iteration 304/1000 | Loss: 0.00000978
Iteration 305/1000 | Loss: 0.00000978
Iteration 306/1000 | Loss: 0.00000978
Iteration 307/1000 | Loss: 0.00000978
Iteration 308/1000 | Loss: 0.00000978
Iteration 309/1000 | Loss: 0.00000978
Iteration 310/1000 | Loss: 0.00000978
Iteration 311/1000 | Loss: 0.00000978
Iteration 312/1000 | Loss: 0.00000978
Iteration 313/1000 | Loss: 0.00000978
Iteration 314/1000 | Loss: 0.00000978
Iteration 315/1000 | Loss: 0.00000978
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 315. Stopping optimization.
Last 5 losses: [9.77836680249311e-06, 9.77836680249311e-06, 9.77836680249311e-06, 9.77836680249311e-06, 9.77836680249311e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.77836680249311e-06

Optimization complete. Final v2v error: 2.640353202819824 mm

Highest mean error: 3.7734556198120117 mm for frame 74

Lowest mean error: 2.298175573348999 mm for frame 109

Saving results

Total time: 103.96171951293945
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01008235
Iteration 2/25 | Loss: 0.00349894
Iteration 3/25 | Loss: 0.00207092
Iteration 4/25 | Loss: 0.00189481
Iteration 5/25 | Loss: 0.00179718
Iteration 6/25 | Loss: 0.00174965
Iteration 7/25 | Loss: 0.00167785
Iteration 8/25 | Loss: 0.00208715
Iteration 9/25 | Loss: 0.00177809
Iteration 10/25 | Loss: 0.00118961
Iteration 11/25 | Loss: 0.00079756
Iteration 12/25 | Loss: 0.00069180
Iteration 13/25 | Loss: 0.00066815
Iteration 14/25 | Loss: 0.00066452
Iteration 15/25 | Loss: 0.00066312
Iteration 16/25 | Loss: 0.00066232
Iteration 17/25 | Loss: 0.00066191
Iteration 18/25 | Loss: 0.00066169
Iteration 19/25 | Loss: 0.00066667
Iteration 20/25 | Loss: 0.00066121
Iteration 21/25 | Loss: 0.00066093
Iteration 22/25 | Loss: 0.00066062
Iteration 23/25 | Loss: 0.00066012
Iteration 24/25 | Loss: 0.00065951
Iteration 25/25 | Loss: 0.00065897

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45238018
Iteration 2/25 | Loss: 0.00030506
Iteration 3/25 | Loss: 0.00030505
Iteration 4/25 | Loss: 0.00030505
Iteration 5/25 | Loss: 0.00030505
Iteration 6/25 | Loss: 0.00030505
Iteration 7/25 | Loss: 0.00030505
Iteration 8/25 | Loss: 0.00030505
Iteration 9/25 | Loss: 0.00030505
Iteration 10/25 | Loss: 0.00030505
Iteration 11/25 | Loss: 0.00030505
Iteration 12/25 | Loss: 0.00030505
Iteration 13/25 | Loss: 0.00030505
Iteration 14/25 | Loss: 0.00030505
Iteration 15/25 | Loss: 0.00030505
Iteration 16/25 | Loss: 0.00030505
Iteration 17/25 | Loss: 0.00030505
Iteration 18/25 | Loss: 0.00030505
Iteration 19/25 | Loss: 0.00030505
Iteration 20/25 | Loss: 0.00030505
Iteration 21/25 | Loss: 0.00030505
Iteration 22/25 | Loss: 0.00030505
Iteration 23/25 | Loss: 0.00030505
Iteration 24/25 | Loss: 0.00030505
Iteration 25/25 | Loss: 0.00030505
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.000305052351905033, 0.000305052351905033, 0.000305052351905033, 0.000305052351905033, 0.000305052351905033]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000305052351905033

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030505
Iteration 2/1000 | Loss: 0.00003848
Iteration 3/1000 | Loss: 0.00002912
Iteration 4/1000 | Loss: 0.00002547
Iteration 5/1000 | Loss: 0.00002323
Iteration 6/1000 | Loss: 0.00002194
Iteration 7/1000 | Loss: 0.00002132
Iteration 8/1000 | Loss: 0.00086193
Iteration 9/1000 | Loss: 0.00107459
Iteration 10/1000 | Loss: 0.00084190
Iteration 11/1000 | Loss: 0.00004271
Iteration 12/1000 | Loss: 0.00002413
Iteration 13/1000 | Loss: 0.00001899
Iteration 14/1000 | Loss: 0.00001689
Iteration 15/1000 | Loss: 0.00001605
Iteration 16/1000 | Loss: 0.00001531
Iteration 17/1000 | Loss: 0.00001498
Iteration 18/1000 | Loss: 0.00001465
Iteration 19/1000 | Loss: 0.00001435
Iteration 20/1000 | Loss: 0.00001433
Iteration 21/1000 | Loss: 0.00001425
Iteration 22/1000 | Loss: 0.00001422
Iteration 23/1000 | Loss: 0.00001421
Iteration 24/1000 | Loss: 0.00001418
Iteration 25/1000 | Loss: 0.00001418
Iteration 26/1000 | Loss: 0.00001418
Iteration 27/1000 | Loss: 0.00001418
Iteration 28/1000 | Loss: 0.00001418
Iteration 29/1000 | Loss: 0.00001418
Iteration 30/1000 | Loss: 0.00001418
Iteration 31/1000 | Loss: 0.00001418
Iteration 32/1000 | Loss: 0.00001418
Iteration 33/1000 | Loss: 0.00001418
Iteration 34/1000 | Loss: 0.00001417
Iteration 35/1000 | Loss: 0.00001417
Iteration 36/1000 | Loss: 0.00001417
Iteration 37/1000 | Loss: 0.00001417
Iteration 38/1000 | Loss: 0.00001416
Iteration 39/1000 | Loss: 0.00001416
Iteration 40/1000 | Loss: 0.00001415
Iteration 41/1000 | Loss: 0.00001415
Iteration 42/1000 | Loss: 0.00001414
Iteration 43/1000 | Loss: 0.00001414
Iteration 44/1000 | Loss: 0.00001414
Iteration 45/1000 | Loss: 0.00001413
Iteration 46/1000 | Loss: 0.00001412
Iteration 47/1000 | Loss: 0.00001412
Iteration 48/1000 | Loss: 0.00001411
Iteration 49/1000 | Loss: 0.00001411
Iteration 50/1000 | Loss: 0.00001411
Iteration 51/1000 | Loss: 0.00001410
Iteration 52/1000 | Loss: 0.00001410
Iteration 53/1000 | Loss: 0.00001410
Iteration 54/1000 | Loss: 0.00001410
Iteration 55/1000 | Loss: 0.00001410
Iteration 56/1000 | Loss: 0.00001410
Iteration 57/1000 | Loss: 0.00001410
Iteration 58/1000 | Loss: 0.00001410
Iteration 59/1000 | Loss: 0.00001410
Iteration 60/1000 | Loss: 0.00001410
Iteration 61/1000 | Loss: 0.00001410
Iteration 62/1000 | Loss: 0.00001410
Iteration 63/1000 | Loss: 0.00001410
Iteration 64/1000 | Loss: 0.00001410
Iteration 65/1000 | Loss: 0.00001410
Iteration 66/1000 | Loss: 0.00001409
Iteration 67/1000 | Loss: 0.00001409
Iteration 68/1000 | Loss: 0.00001409
Iteration 69/1000 | Loss: 0.00001409
Iteration 70/1000 | Loss: 0.00001409
Iteration 71/1000 | Loss: 0.00001409
Iteration 72/1000 | Loss: 0.00001409
Iteration 73/1000 | Loss: 0.00001409
Iteration 74/1000 | Loss: 0.00001409
Iteration 75/1000 | Loss: 0.00001409
Iteration 76/1000 | Loss: 0.00001409
Iteration 77/1000 | Loss: 0.00001409
Iteration 78/1000 | Loss: 0.00001409
Iteration 79/1000 | Loss: 0.00001409
Iteration 80/1000 | Loss: 0.00001409
Iteration 81/1000 | Loss: 0.00001409
Iteration 82/1000 | Loss: 0.00001409
Iteration 83/1000 | Loss: 0.00001409
Iteration 84/1000 | Loss: 0.00001409
Iteration 85/1000 | Loss: 0.00001409
Iteration 86/1000 | Loss: 0.00001409
Iteration 87/1000 | Loss: 0.00001409
Iteration 88/1000 | Loss: 0.00001409
Iteration 89/1000 | Loss: 0.00001409
Iteration 90/1000 | Loss: 0.00001409
Iteration 91/1000 | Loss: 0.00001409
Iteration 92/1000 | Loss: 0.00001409
Iteration 93/1000 | Loss: 0.00001409
Iteration 94/1000 | Loss: 0.00001409
Iteration 95/1000 | Loss: 0.00001409
Iteration 96/1000 | Loss: 0.00001409
Iteration 97/1000 | Loss: 0.00001409
Iteration 98/1000 | Loss: 0.00001409
Iteration 99/1000 | Loss: 0.00001409
Iteration 100/1000 | Loss: 0.00001409
Iteration 101/1000 | Loss: 0.00001409
Iteration 102/1000 | Loss: 0.00001409
Iteration 103/1000 | Loss: 0.00001409
Iteration 104/1000 | Loss: 0.00001409
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [1.4094676771492232e-05, 1.4094676771492232e-05, 1.4094676771492232e-05, 1.4094676771492232e-05, 1.4094676771492232e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4094676771492232e-05

Optimization complete. Final v2v error: 3.1790285110473633 mm

Highest mean error: 3.78415846824646 mm for frame 226

Lowest mean error: 3.0201027393341064 mm for frame 3

Saving results

Total time: 85.61031246185303
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00876021
Iteration 2/25 | Loss: 0.00123628
Iteration 3/25 | Loss: 0.00079516
Iteration 4/25 | Loss: 0.00073005
Iteration 5/25 | Loss: 0.00071381
Iteration 6/25 | Loss: 0.00070888
Iteration 7/25 | Loss: 0.00070757
Iteration 8/25 | Loss: 0.00070732
Iteration 9/25 | Loss: 0.00070732
Iteration 10/25 | Loss: 0.00070732
Iteration 11/25 | Loss: 0.00070732
Iteration 12/25 | Loss: 0.00070732
Iteration 13/25 | Loss: 0.00070732
Iteration 14/25 | Loss: 0.00070732
Iteration 15/25 | Loss: 0.00070732
Iteration 16/25 | Loss: 0.00070732
Iteration 17/25 | Loss: 0.00070732
Iteration 18/25 | Loss: 0.00070732
Iteration 19/25 | Loss: 0.00070732
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007073223823681474, 0.0007073223823681474, 0.0007073223823681474, 0.0007073223823681474, 0.0007073223823681474]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007073223823681474

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48975492
Iteration 2/25 | Loss: 0.00030672
Iteration 3/25 | Loss: 0.00030672
Iteration 4/25 | Loss: 0.00030672
Iteration 5/25 | Loss: 0.00030671
Iteration 6/25 | Loss: 0.00030671
Iteration 7/25 | Loss: 0.00030671
Iteration 8/25 | Loss: 0.00030671
Iteration 9/25 | Loss: 0.00030671
Iteration 10/25 | Loss: 0.00030671
Iteration 11/25 | Loss: 0.00030671
Iteration 12/25 | Loss: 0.00030671
Iteration 13/25 | Loss: 0.00030671
Iteration 14/25 | Loss: 0.00030671
Iteration 15/25 | Loss: 0.00030671
Iteration 16/25 | Loss: 0.00030671
Iteration 17/25 | Loss: 0.00030671
Iteration 18/25 | Loss: 0.00030671
Iteration 19/25 | Loss: 0.00030671
Iteration 20/25 | Loss: 0.00030671
Iteration 21/25 | Loss: 0.00030671
Iteration 22/25 | Loss: 0.00030671
Iteration 23/25 | Loss: 0.00030671
Iteration 24/25 | Loss: 0.00030671
Iteration 25/25 | Loss: 0.00030671

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030671
Iteration 2/1000 | Loss: 0.00004002
Iteration 3/1000 | Loss: 0.00002726
Iteration 4/1000 | Loss: 0.00002507
Iteration 5/1000 | Loss: 0.00002414
Iteration 6/1000 | Loss: 0.00002351
Iteration 7/1000 | Loss: 0.00002307
Iteration 8/1000 | Loss: 0.00002277
Iteration 9/1000 | Loss: 0.00002246
Iteration 10/1000 | Loss: 0.00002222
Iteration 11/1000 | Loss: 0.00002203
Iteration 12/1000 | Loss: 0.00002194
Iteration 13/1000 | Loss: 0.00002184
Iteration 14/1000 | Loss: 0.00002183
Iteration 15/1000 | Loss: 0.00002182
Iteration 16/1000 | Loss: 0.00002180
Iteration 17/1000 | Loss: 0.00002180
Iteration 18/1000 | Loss: 0.00002179
Iteration 19/1000 | Loss: 0.00002179
Iteration 20/1000 | Loss: 0.00002178
Iteration 21/1000 | Loss: 0.00002177
Iteration 22/1000 | Loss: 0.00002174
Iteration 23/1000 | Loss: 0.00002174
Iteration 24/1000 | Loss: 0.00002174
Iteration 25/1000 | Loss: 0.00002173
Iteration 26/1000 | Loss: 0.00002171
Iteration 27/1000 | Loss: 0.00002171
Iteration 28/1000 | Loss: 0.00002170
Iteration 29/1000 | Loss: 0.00002170
Iteration 30/1000 | Loss: 0.00002169
Iteration 31/1000 | Loss: 0.00002169
Iteration 32/1000 | Loss: 0.00002169
Iteration 33/1000 | Loss: 0.00002168
Iteration 34/1000 | Loss: 0.00002167
Iteration 35/1000 | Loss: 0.00002167
Iteration 36/1000 | Loss: 0.00002166
Iteration 37/1000 | Loss: 0.00002166
Iteration 38/1000 | Loss: 0.00002166
Iteration 39/1000 | Loss: 0.00002166
Iteration 40/1000 | Loss: 0.00002166
Iteration 41/1000 | Loss: 0.00002166
Iteration 42/1000 | Loss: 0.00002165
Iteration 43/1000 | Loss: 0.00002164
Iteration 44/1000 | Loss: 0.00002164
Iteration 45/1000 | Loss: 0.00002164
Iteration 46/1000 | Loss: 0.00002164
Iteration 47/1000 | Loss: 0.00002163
Iteration 48/1000 | Loss: 0.00002163
Iteration 49/1000 | Loss: 0.00002163
Iteration 50/1000 | Loss: 0.00002163
Iteration 51/1000 | Loss: 0.00002163
Iteration 52/1000 | Loss: 0.00002163
Iteration 53/1000 | Loss: 0.00002163
Iteration 54/1000 | Loss: 0.00002163
Iteration 55/1000 | Loss: 0.00002163
Iteration 56/1000 | Loss: 0.00002162
Iteration 57/1000 | Loss: 0.00002162
Iteration 58/1000 | Loss: 0.00002161
Iteration 59/1000 | Loss: 0.00002161
Iteration 60/1000 | Loss: 0.00002160
Iteration 61/1000 | Loss: 0.00002160
Iteration 62/1000 | Loss: 0.00002160
Iteration 63/1000 | Loss: 0.00002160
Iteration 64/1000 | Loss: 0.00002160
Iteration 65/1000 | Loss: 0.00002160
Iteration 66/1000 | Loss: 0.00002160
Iteration 67/1000 | Loss: 0.00002160
Iteration 68/1000 | Loss: 0.00002159
Iteration 69/1000 | Loss: 0.00002159
Iteration 70/1000 | Loss: 0.00002159
Iteration 71/1000 | Loss: 0.00002158
Iteration 72/1000 | Loss: 0.00002158
Iteration 73/1000 | Loss: 0.00002158
Iteration 74/1000 | Loss: 0.00002158
Iteration 75/1000 | Loss: 0.00002158
Iteration 76/1000 | Loss: 0.00002158
Iteration 77/1000 | Loss: 0.00002158
Iteration 78/1000 | Loss: 0.00002158
Iteration 79/1000 | Loss: 0.00002158
Iteration 80/1000 | Loss: 0.00002158
Iteration 81/1000 | Loss: 0.00002158
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 81. Stopping optimization.
Last 5 losses: [2.157740709662903e-05, 2.157740709662903e-05, 2.157740709662903e-05, 2.157740709662903e-05, 2.157740709662903e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.157740709662903e-05

Optimization complete. Final v2v error: 3.7576687335968018 mm

Highest mean error: 5.1436567306518555 mm for frame 149

Lowest mean error: 2.8520238399505615 mm for frame 6

Saving results

Total time: 34.549386501312256
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00774794
Iteration 2/25 | Loss: 0.00147456
Iteration 3/25 | Loss: 0.00083999
Iteration 4/25 | Loss: 0.00073356
Iteration 5/25 | Loss: 0.00067996
Iteration 6/25 | Loss: 0.00067420
Iteration 7/25 | Loss: 0.00066525
Iteration 8/25 | Loss: 0.00064412
Iteration 9/25 | Loss: 0.00063601
Iteration 10/25 | Loss: 0.00063034
Iteration 11/25 | Loss: 0.00062755
Iteration 12/25 | Loss: 0.00063219
Iteration 13/25 | Loss: 0.00062470
Iteration 14/25 | Loss: 0.00062405
Iteration 15/25 | Loss: 0.00062028
Iteration 16/25 | Loss: 0.00062147
Iteration 17/25 | Loss: 0.00061847
Iteration 18/25 | Loss: 0.00061738
Iteration 19/25 | Loss: 0.00061635
Iteration 20/25 | Loss: 0.00061570
Iteration 21/25 | Loss: 0.00061539
Iteration 22/25 | Loss: 0.00061525
Iteration 23/25 | Loss: 0.00061513
Iteration 24/25 | Loss: 0.00061513
Iteration 25/25 | Loss: 0.00061512

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.70070124
Iteration 2/25 | Loss: 0.00032372
Iteration 3/25 | Loss: 0.00027918
Iteration 4/25 | Loss: 0.00027918
Iteration 5/25 | Loss: 0.00027918
Iteration 6/25 | Loss: 0.00027918
Iteration 7/25 | Loss: 0.00027918
Iteration 8/25 | Loss: 0.00027918
Iteration 9/25 | Loss: 0.00027918
Iteration 10/25 | Loss: 0.00027918
Iteration 11/25 | Loss: 0.00027918
Iteration 12/25 | Loss: 0.00027918
Iteration 13/25 | Loss: 0.00027918
Iteration 14/25 | Loss: 0.00027918
Iteration 15/25 | Loss: 0.00027918
Iteration 16/25 | Loss: 0.00027918
Iteration 17/25 | Loss: 0.00027918
Iteration 18/25 | Loss: 0.00027918
Iteration 19/25 | Loss: 0.00027918
Iteration 20/25 | Loss: 0.00027918
Iteration 21/25 | Loss: 0.00027918
Iteration 22/25 | Loss: 0.00027918
Iteration 23/25 | Loss: 0.00027918
Iteration 24/25 | Loss: 0.00027918
Iteration 25/25 | Loss: 0.00027918
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.00027917500119656324, 0.00027917500119656324, 0.00027917500119656324, 0.00027917500119656324, 0.00027917500119656324]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00027917500119656324

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027918
Iteration 2/1000 | Loss: 0.00009502
Iteration 3/1000 | Loss: 0.00002330
Iteration 4/1000 | Loss: 0.00001924
Iteration 5/1000 | Loss: 0.00001781
Iteration 6/1000 | Loss: 0.00026223
Iteration 7/1000 | Loss: 0.00002084
Iteration 8/1000 | Loss: 0.00001764
Iteration 9/1000 | Loss: 0.00001618
Iteration 10/1000 | Loss: 0.00001524
Iteration 11/1000 | Loss: 0.00001468
Iteration 12/1000 | Loss: 0.00001446
Iteration 13/1000 | Loss: 0.00001441
Iteration 14/1000 | Loss: 0.00001430
Iteration 15/1000 | Loss: 0.00001429
Iteration 16/1000 | Loss: 0.00001416
Iteration 17/1000 | Loss: 0.00001413
Iteration 18/1000 | Loss: 0.00001409
Iteration 19/1000 | Loss: 0.00001402
Iteration 20/1000 | Loss: 0.00001399
Iteration 21/1000 | Loss: 0.00001399
Iteration 22/1000 | Loss: 0.00001399
Iteration 23/1000 | Loss: 0.00001398
Iteration 24/1000 | Loss: 0.00001398
Iteration 25/1000 | Loss: 0.00001398
Iteration 26/1000 | Loss: 0.00001397
Iteration 27/1000 | Loss: 0.00001397
Iteration 28/1000 | Loss: 0.00001396
Iteration 29/1000 | Loss: 0.00001396
Iteration 30/1000 | Loss: 0.00001396
Iteration 31/1000 | Loss: 0.00001395
Iteration 32/1000 | Loss: 0.00001395
Iteration 33/1000 | Loss: 0.00001394
Iteration 34/1000 | Loss: 0.00001394
Iteration 35/1000 | Loss: 0.00001393
Iteration 36/1000 | Loss: 0.00001393
Iteration 37/1000 | Loss: 0.00001393
Iteration 38/1000 | Loss: 0.00001393
Iteration 39/1000 | Loss: 0.00001392
Iteration 40/1000 | Loss: 0.00001392
Iteration 41/1000 | Loss: 0.00001392
Iteration 42/1000 | Loss: 0.00001392
Iteration 43/1000 | Loss: 0.00001391
Iteration 44/1000 | Loss: 0.00001391
Iteration 45/1000 | Loss: 0.00001391
Iteration 46/1000 | Loss: 0.00001390
Iteration 47/1000 | Loss: 0.00001390
Iteration 48/1000 | Loss: 0.00001389
Iteration 49/1000 | Loss: 0.00004644
Iteration 50/1000 | Loss: 0.00001400
Iteration 51/1000 | Loss: 0.00001390
Iteration 52/1000 | Loss: 0.00001388
Iteration 53/1000 | Loss: 0.00001385
Iteration 54/1000 | Loss: 0.00001383
Iteration 55/1000 | Loss: 0.00001383
Iteration 56/1000 | Loss: 0.00001383
Iteration 57/1000 | Loss: 0.00001383
Iteration 58/1000 | Loss: 0.00001383
Iteration 59/1000 | Loss: 0.00001383
Iteration 60/1000 | Loss: 0.00001383
Iteration 61/1000 | Loss: 0.00001383
Iteration 62/1000 | Loss: 0.00001383
Iteration 63/1000 | Loss: 0.00001382
Iteration 64/1000 | Loss: 0.00001382
Iteration 65/1000 | Loss: 0.00001382
Iteration 66/1000 | Loss: 0.00001382
Iteration 67/1000 | Loss: 0.00001381
Iteration 68/1000 | Loss: 0.00001381
Iteration 69/1000 | Loss: 0.00001381
Iteration 70/1000 | Loss: 0.00001381
Iteration 71/1000 | Loss: 0.00001381
Iteration 72/1000 | Loss: 0.00001381
Iteration 73/1000 | Loss: 0.00001381
Iteration 74/1000 | Loss: 0.00001381
Iteration 75/1000 | Loss: 0.00001381
Iteration 76/1000 | Loss: 0.00001381
Iteration 77/1000 | Loss: 0.00001381
Iteration 78/1000 | Loss: 0.00001381
Iteration 79/1000 | Loss: 0.00001380
Iteration 80/1000 | Loss: 0.00001380
Iteration 81/1000 | Loss: 0.00001380
Iteration 82/1000 | Loss: 0.00001380
Iteration 83/1000 | Loss: 0.00001380
Iteration 84/1000 | Loss: 0.00001380
Iteration 85/1000 | Loss: 0.00001379
Iteration 86/1000 | Loss: 0.00001379
Iteration 87/1000 | Loss: 0.00001379
Iteration 88/1000 | Loss: 0.00001379
Iteration 89/1000 | Loss: 0.00001379
Iteration 90/1000 | Loss: 0.00001379
Iteration 91/1000 | Loss: 0.00001379
Iteration 92/1000 | Loss: 0.00001379
Iteration 93/1000 | Loss: 0.00001378
Iteration 94/1000 | Loss: 0.00001378
Iteration 95/1000 | Loss: 0.00001378
Iteration 96/1000 | Loss: 0.00001378
Iteration 97/1000 | Loss: 0.00001378
Iteration 98/1000 | Loss: 0.00001378
Iteration 99/1000 | Loss: 0.00001377
Iteration 100/1000 | Loss: 0.00001377
Iteration 101/1000 | Loss: 0.00001377
Iteration 102/1000 | Loss: 0.00001377
Iteration 103/1000 | Loss: 0.00001377
Iteration 104/1000 | Loss: 0.00001377
Iteration 105/1000 | Loss: 0.00001377
Iteration 106/1000 | Loss: 0.00001377
Iteration 107/1000 | Loss: 0.00001377
Iteration 108/1000 | Loss: 0.00001377
Iteration 109/1000 | Loss: 0.00001377
Iteration 110/1000 | Loss: 0.00001376
Iteration 111/1000 | Loss: 0.00001376
Iteration 112/1000 | Loss: 0.00001376
Iteration 113/1000 | Loss: 0.00001376
Iteration 114/1000 | Loss: 0.00001375
Iteration 115/1000 | Loss: 0.00001375
Iteration 116/1000 | Loss: 0.00001375
Iteration 117/1000 | Loss: 0.00001375
Iteration 118/1000 | Loss: 0.00001375
Iteration 119/1000 | Loss: 0.00001374
Iteration 120/1000 | Loss: 0.00001374
Iteration 121/1000 | Loss: 0.00001374
Iteration 122/1000 | Loss: 0.00001374
Iteration 123/1000 | Loss: 0.00001374
Iteration 124/1000 | Loss: 0.00001374
Iteration 125/1000 | Loss: 0.00001374
Iteration 126/1000 | Loss: 0.00001374
Iteration 127/1000 | Loss: 0.00001374
Iteration 128/1000 | Loss: 0.00001374
Iteration 129/1000 | Loss: 0.00001374
Iteration 130/1000 | Loss: 0.00001374
Iteration 131/1000 | Loss: 0.00001374
Iteration 132/1000 | Loss: 0.00001373
Iteration 133/1000 | Loss: 0.00001373
Iteration 134/1000 | Loss: 0.00001373
Iteration 135/1000 | Loss: 0.00001373
Iteration 136/1000 | Loss: 0.00001373
Iteration 137/1000 | Loss: 0.00001373
Iteration 138/1000 | Loss: 0.00001373
Iteration 139/1000 | Loss: 0.00001373
Iteration 140/1000 | Loss: 0.00001373
Iteration 141/1000 | Loss: 0.00001373
Iteration 142/1000 | Loss: 0.00001373
Iteration 143/1000 | Loss: 0.00001373
Iteration 144/1000 | Loss: 0.00001373
Iteration 145/1000 | Loss: 0.00001373
Iteration 146/1000 | Loss: 0.00001373
Iteration 147/1000 | Loss: 0.00001373
Iteration 148/1000 | Loss: 0.00001372
Iteration 149/1000 | Loss: 0.00001372
Iteration 150/1000 | Loss: 0.00001372
Iteration 151/1000 | Loss: 0.00001372
Iteration 152/1000 | Loss: 0.00001372
Iteration 153/1000 | Loss: 0.00001372
Iteration 154/1000 | Loss: 0.00001372
Iteration 155/1000 | Loss: 0.00001372
Iteration 156/1000 | Loss: 0.00001372
Iteration 157/1000 | Loss: 0.00001372
Iteration 158/1000 | Loss: 0.00001371
Iteration 159/1000 | Loss: 0.00001371
Iteration 160/1000 | Loss: 0.00001371
Iteration 161/1000 | Loss: 0.00001371
Iteration 162/1000 | Loss: 0.00001371
Iteration 163/1000 | Loss: 0.00001371
Iteration 164/1000 | Loss: 0.00001371
Iteration 165/1000 | Loss: 0.00001371
Iteration 166/1000 | Loss: 0.00001371
Iteration 167/1000 | Loss: 0.00001371
Iteration 168/1000 | Loss: 0.00001371
Iteration 169/1000 | Loss: 0.00001371
Iteration 170/1000 | Loss: 0.00001371
Iteration 171/1000 | Loss: 0.00001371
Iteration 172/1000 | Loss: 0.00001371
Iteration 173/1000 | Loss: 0.00001371
Iteration 174/1000 | Loss: 0.00001371
Iteration 175/1000 | Loss: 0.00001370
Iteration 176/1000 | Loss: 0.00001370
Iteration 177/1000 | Loss: 0.00001370
Iteration 178/1000 | Loss: 0.00001370
Iteration 179/1000 | Loss: 0.00001370
Iteration 180/1000 | Loss: 0.00001370
Iteration 181/1000 | Loss: 0.00001370
Iteration 182/1000 | Loss: 0.00001370
Iteration 183/1000 | Loss: 0.00001370
Iteration 184/1000 | Loss: 0.00001370
Iteration 185/1000 | Loss: 0.00001370
Iteration 186/1000 | Loss: 0.00001370
Iteration 187/1000 | Loss: 0.00001370
Iteration 188/1000 | Loss: 0.00001370
Iteration 189/1000 | Loss: 0.00001370
Iteration 190/1000 | Loss: 0.00001370
Iteration 191/1000 | Loss: 0.00001370
Iteration 192/1000 | Loss: 0.00001370
Iteration 193/1000 | Loss: 0.00001370
Iteration 194/1000 | Loss: 0.00001370
Iteration 195/1000 | Loss: 0.00001370
Iteration 196/1000 | Loss: 0.00001369
Iteration 197/1000 | Loss: 0.00001369
Iteration 198/1000 | Loss: 0.00001369
Iteration 199/1000 | Loss: 0.00001369
Iteration 200/1000 | Loss: 0.00001369
Iteration 201/1000 | Loss: 0.00001369
Iteration 202/1000 | Loss: 0.00001369
Iteration 203/1000 | Loss: 0.00001369
Iteration 204/1000 | Loss: 0.00001369
Iteration 205/1000 | Loss: 0.00001369
Iteration 206/1000 | Loss: 0.00001369
Iteration 207/1000 | Loss: 0.00001369
Iteration 208/1000 | Loss: 0.00001369
Iteration 209/1000 | Loss: 0.00001369
Iteration 210/1000 | Loss: 0.00001369
Iteration 211/1000 | Loss: 0.00001369
Iteration 212/1000 | Loss: 0.00001369
Iteration 213/1000 | Loss: 0.00001369
Iteration 214/1000 | Loss: 0.00001369
Iteration 215/1000 | Loss: 0.00001369
Iteration 216/1000 | Loss: 0.00001369
Iteration 217/1000 | Loss: 0.00001369
Iteration 218/1000 | Loss: 0.00001369
Iteration 219/1000 | Loss: 0.00001369
Iteration 220/1000 | Loss: 0.00001369
Iteration 221/1000 | Loss: 0.00001369
Iteration 222/1000 | Loss: 0.00001369
Iteration 223/1000 | Loss: 0.00001369
Iteration 224/1000 | Loss: 0.00001369
Iteration 225/1000 | Loss: 0.00001369
Iteration 226/1000 | Loss: 0.00001369
Iteration 227/1000 | Loss: 0.00001368
Iteration 228/1000 | Loss: 0.00001368
Iteration 229/1000 | Loss: 0.00001368
Iteration 230/1000 | Loss: 0.00001368
Iteration 231/1000 | Loss: 0.00001368
Iteration 232/1000 | Loss: 0.00001368
Iteration 233/1000 | Loss: 0.00001368
Iteration 234/1000 | Loss: 0.00001368
Iteration 235/1000 | Loss: 0.00001368
Iteration 236/1000 | Loss: 0.00001368
Iteration 237/1000 | Loss: 0.00001368
Iteration 238/1000 | Loss: 0.00001368
Iteration 239/1000 | Loss: 0.00001368
Iteration 240/1000 | Loss: 0.00001368
Iteration 241/1000 | Loss: 0.00001368
Iteration 242/1000 | Loss: 0.00001368
Iteration 243/1000 | Loss: 0.00001368
Iteration 244/1000 | Loss: 0.00001368
Iteration 245/1000 | Loss: 0.00001368
Iteration 246/1000 | Loss: 0.00001368
Iteration 247/1000 | Loss: 0.00001368
Iteration 248/1000 | Loss: 0.00001368
Iteration 249/1000 | Loss: 0.00001368
Iteration 250/1000 | Loss: 0.00001368
Iteration 251/1000 | Loss: 0.00001368
Iteration 252/1000 | Loss: 0.00001368
Iteration 253/1000 | Loss: 0.00001368
Iteration 254/1000 | Loss: 0.00001368
Iteration 255/1000 | Loss: 0.00001368
Iteration 256/1000 | Loss: 0.00001368
Iteration 257/1000 | Loss: 0.00001368
Iteration 258/1000 | Loss: 0.00001368
Iteration 259/1000 | Loss: 0.00001368
Iteration 260/1000 | Loss: 0.00001368
Iteration 261/1000 | Loss: 0.00001368
Iteration 262/1000 | Loss: 0.00001368
Iteration 263/1000 | Loss: 0.00001368
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 263. Stopping optimization.
Last 5 losses: [1.3675140507984906e-05, 1.3675140507984906e-05, 1.3675140507984906e-05, 1.3675140507984906e-05, 1.3675140507984906e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3675140507984906e-05

Optimization complete. Final v2v error: 3.1225898265838623 mm

Highest mean error: 3.7126572132110596 mm for frame 49

Lowest mean error: 2.5718328952789307 mm for frame 207

Saving results

Total time: 89.00050806999207
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00361870
Iteration 2/25 | Loss: 0.00068592
Iteration 3/25 | Loss: 0.00057470
Iteration 4/25 | Loss: 0.00055791
Iteration 5/25 | Loss: 0.00055284
Iteration 6/25 | Loss: 0.00055163
Iteration 7/25 | Loss: 0.00055140
Iteration 8/25 | Loss: 0.00055140
Iteration 9/25 | Loss: 0.00055140
Iteration 10/25 | Loss: 0.00055140
Iteration 11/25 | Loss: 0.00055140
Iteration 12/25 | Loss: 0.00055140
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005514014628715813, 0.0005514014628715813, 0.0005514014628715813, 0.0005514014628715813, 0.0005514014628715813]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005514014628715813

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.02555656
Iteration 2/25 | Loss: 0.00020815
Iteration 3/25 | Loss: 0.00020815
Iteration 4/25 | Loss: 0.00020815
Iteration 5/25 | Loss: 0.00020815
Iteration 6/25 | Loss: 0.00020815
Iteration 7/25 | Loss: 0.00020815
Iteration 8/25 | Loss: 0.00020815
Iteration 9/25 | Loss: 0.00020814
Iteration 10/25 | Loss: 0.00020814
Iteration 11/25 | Loss: 0.00020814
Iteration 12/25 | Loss: 0.00020814
Iteration 13/25 | Loss: 0.00020814
Iteration 14/25 | Loss: 0.00020814
Iteration 15/25 | Loss: 0.00020814
Iteration 16/25 | Loss: 0.00020814
Iteration 17/25 | Loss: 0.00020814
Iteration 18/25 | Loss: 0.00020814
Iteration 19/25 | Loss: 0.00020814
Iteration 20/25 | Loss: 0.00020814
Iteration 21/25 | Loss: 0.00020814
Iteration 22/25 | Loss: 0.00020814
Iteration 23/25 | Loss: 0.00020814
Iteration 24/25 | Loss: 0.00020814
Iteration 25/25 | Loss: 0.00020814

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00020814
Iteration 2/1000 | Loss: 0.00001917
Iteration 3/1000 | Loss: 0.00001463
Iteration 4/1000 | Loss: 0.00001372
Iteration 5/1000 | Loss: 0.00001303
Iteration 6/1000 | Loss: 0.00001260
Iteration 7/1000 | Loss: 0.00001238
Iteration 8/1000 | Loss: 0.00001234
Iteration 9/1000 | Loss: 0.00001233
Iteration 10/1000 | Loss: 0.00001224
Iteration 11/1000 | Loss: 0.00001224
Iteration 12/1000 | Loss: 0.00001224
Iteration 13/1000 | Loss: 0.00001216
Iteration 14/1000 | Loss: 0.00001216
Iteration 15/1000 | Loss: 0.00001213
Iteration 16/1000 | Loss: 0.00001212
Iteration 17/1000 | Loss: 0.00001211
Iteration 18/1000 | Loss: 0.00001211
Iteration 19/1000 | Loss: 0.00001207
Iteration 20/1000 | Loss: 0.00001207
Iteration 21/1000 | Loss: 0.00001206
Iteration 22/1000 | Loss: 0.00001206
Iteration 23/1000 | Loss: 0.00001203
Iteration 24/1000 | Loss: 0.00001202
Iteration 25/1000 | Loss: 0.00001202
Iteration 26/1000 | Loss: 0.00001202
Iteration 27/1000 | Loss: 0.00001201
Iteration 28/1000 | Loss: 0.00001194
Iteration 29/1000 | Loss: 0.00001192
Iteration 30/1000 | Loss: 0.00001191
Iteration 31/1000 | Loss: 0.00001190
Iteration 32/1000 | Loss: 0.00001189
Iteration 33/1000 | Loss: 0.00001189
Iteration 34/1000 | Loss: 0.00001188
Iteration 35/1000 | Loss: 0.00001188
Iteration 36/1000 | Loss: 0.00001188
Iteration 37/1000 | Loss: 0.00001187
Iteration 38/1000 | Loss: 0.00001187
Iteration 39/1000 | Loss: 0.00001186
Iteration 40/1000 | Loss: 0.00001185
Iteration 41/1000 | Loss: 0.00001185
Iteration 42/1000 | Loss: 0.00001185
Iteration 43/1000 | Loss: 0.00001185
Iteration 44/1000 | Loss: 0.00001185
Iteration 45/1000 | Loss: 0.00001184
Iteration 46/1000 | Loss: 0.00001184
Iteration 47/1000 | Loss: 0.00001184
Iteration 48/1000 | Loss: 0.00001184
Iteration 49/1000 | Loss: 0.00001184
Iteration 50/1000 | Loss: 0.00001183
Iteration 51/1000 | Loss: 0.00001183
Iteration 52/1000 | Loss: 0.00001183
Iteration 53/1000 | Loss: 0.00001182
Iteration 54/1000 | Loss: 0.00001182
Iteration 55/1000 | Loss: 0.00001182
Iteration 56/1000 | Loss: 0.00001182
Iteration 57/1000 | Loss: 0.00001182
Iteration 58/1000 | Loss: 0.00001182
Iteration 59/1000 | Loss: 0.00001181
Iteration 60/1000 | Loss: 0.00001181
Iteration 61/1000 | Loss: 0.00001180
Iteration 62/1000 | Loss: 0.00001180
Iteration 63/1000 | Loss: 0.00001180
Iteration 64/1000 | Loss: 0.00001180
Iteration 65/1000 | Loss: 0.00001179
Iteration 66/1000 | Loss: 0.00001179
Iteration 67/1000 | Loss: 0.00001179
Iteration 68/1000 | Loss: 0.00001178
Iteration 69/1000 | Loss: 0.00001178
Iteration 70/1000 | Loss: 0.00001178
Iteration 71/1000 | Loss: 0.00001178
Iteration 72/1000 | Loss: 0.00001177
Iteration 73/1000 | Loss: 0.00001176
Iteration 74/1000 | Loss: 0.00001176
Iteration 75/1000 | Loss: 0.00001174
Iteration 76/1000 | Loss: 0.00001174
Iteration 77/1000 | Loss: 0.00001174
Iteration 78/1000 | Loss: 0.00001174
Iteration 79/1000 | Loss: 0.00001174
Iteration 80/1000 | Loss: 0.00001174
Iteration 81/1000 | Loss: 0.00001173
Iteration 82/1000 | Loss: 0.00001173
Iteration 83/1000 | Loss: 0.00001173
Iteration 84/1000 | Loss: 0.00001173
Iteration 85/1000 | Loss: 0.00001173
Iteration 86/1000 | Loss: 0.00001173
Iteration 87/1000 | Loss: 0.00001173
Iteration 88/1000 | Loss: 0.00001173
Iteration 89/1000 | Loss: 0.00001172
Iteration 90/1000 | Loss: 0.00001172
Iteration 91/1000 | Loss: 0.00001171
Iteration 92/1000 | Loss: 0.00001171
Iteration 93/1000 | Loss: 0.00001171
Iteration 94/1000 | Loss: 0.00001171
Iteration 95/1000 | Loss: 0.00001171
Iteration 96/1000 | Loss: 0.00001170
Iteration 97/1000 | Loss: 0.00001170
Iteration 98/1000 | Loss: 0.00001170
Iteration 99/1000 | Loss: 0.00001170
Iteration 100/1000 | Loss: 0.00001170
Iteration 101/1000 | Loss: 0.00001170
Iteration 102/1000 | Loss: 0.00001170
Iteration 103/1000 | Loss: 0.00001169
Iteration 104/1000 | Loss: 0.00001169
Iteration 105/1000 | Loss: 0.00001169
Iteration 106/1000 | Loss: 0.00001169
Iteration 107/1000 | Loss: 0.00001169
Iteration 108/1000 | Loss: 0.00001169
Iteration 109/1000 | Loss: 0.00001169
Iteration 110/1000 | Loss: 0.00001168
Iteration 111/1000 | Loss: 0.00001168
Iteration 112/1000 | Loss: 0.00001168
Iteration 113/1000 | Loss: 0.00001168
Iteration 114/1000 | Loss: 0.00001168
Iteration 115/1000 | Loss: 0.00001168
Iteration 116/1000 | Loss: 0.00001168
Iteration 117/1000 | Loss: 0.00001168
Iteration 118/1000 | Loss: 0.00001168
Iteration 119/1000 | Loss: 0.00001168
Iteration 120/1000 | Loss: 0.00001168
Iteration 121/1000 | Loss: 0.00001168
Iteration 122/1000 | Loss: 0.00001168
Iteration 123/1000 | Loss: 0.00001168
Iteration 124/1000 | Loss: 0.00001168
Iteration 125/1000 | Loss: 0.00001168
Iteration 126/1000 | Loss: 0.00001168
Iteration 127/1000 | Loss: 0.00001167
Iteration 128/1000 | Loss: 0.00001167
Iteration 129/1000 | Loss: 0.00001167
Iteration 130/1000 | Loss: 0.00001167
Iteration 131/1000 | Loss: 0.00001167
Iteration 132/1000 | Loss: 0.00001166
Iteration 133/1000 | Loss: 0.00001166
Iteration 134/1000 | Loss: 0.00001166
Iteration 135/1000 | Loss: 0.00001166
Iteration 136/1000 | Loss: 0.00001166
Iteration 137/1000 | Loss: 0.00001166
Iteration 138/1000 | Loss: 0.00001166
Iteration 139/1000 | Loss: 0.00001166
Iteration 140/1000 | Loss: 0.00001166
Iteration 141/1000 | Loss: 0.00001166
Iteration 142/1000 | Loss: 0.00001166
Iteration 143/1000 | Loss: 0.00001166
Iteration 144/1000 | Loss: 0.00001166
Iteration 145/1000 | Loss: 0.00001166
Iteration 146/1000 | Loss: 0.00001166
Iteration 147/1000 | Loss: 0.00001166
Iteration 148/1000 | Loss: 0.00001166
Iteration 149/1000 | Loss: 0.00001166
Iteration 150/1000 | Loss: 0.00001166
Iteration 151/1000 | Loss: 0.00001166
Iteration 152/1000 | Loss: 0.00001166
Iteration 153/1000 | Loss: 0.00001166
Iteration 154/1000 | Loss: 0.00001166
Iteration 155/1000 | Loss: 0.00001166
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.166106630989816e-05, 1.166106630989816e-05, 1.166106630989816e-05, 1.166106630989816e-05, 1.166106630989816e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.166106630989816e-05

Optimization complete. Final v2v error: 2.891002655029297 mm

Highest mean error: 3.1644155979156494 mm for frame 94

Lowest mean error: 2.65785551071167 mm for frame 123

Saving results

Total time: 34.27118992805481
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00381431
Iteration 2/25 | Loss: 0.00081416
Iteration 3/25 | Loss: 0.00061103
Iteration 4/25 | Loss: 0.00058628
Iteration 5/25 | Loss: 0.00057811
Iteration 6/25 | Loss: 0.00057560
Iteration 7/25 | Loss: 0.00057480
Iteration 8/25 | Loss: 0.00057477
Iteration 9/25 | Loss: 0.00057477
Iteration 10/25 | Loss: 0.00057477
Iteration 11/25 | Loss: 0.00057477
Iteration 12/25 | Loss: 0.00057477
Iteration 13/25 | Loss: 0.00057477
Iteration 14/25 | Loss: 0.00057477
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0005747732357122004, 0.0005747732357122004, 0.0005747732357122004, 0.0005747732357122004, 0.0005747732357122004]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005747732357122004

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45737791
Iteration 2/25 | Loss: 0.00020552
Iteration 3/25 | Loss: 0.00020552
Iteration 4/25 | Loss: 0.00020552
Iteration 5/25 | Loss: 0.00020552
Iteration 6/25 | Loss: 0.00020552
Iteration 7/25 | Loss: 0.00020552
Iteration 8/25 | Loss: 0.00020552
Iteration 9/25 | Loss: 0.00020552
Iteration 10/25 | Loss: 0.00020552
Iteration 11/25 | Loss: 0.00020552
Iteration 12/25 | Loss: 0.00020552
Iteration 13/25 | Loss: 0.00020552
Iteration 14/25 | Loss: 0.00020552
Iteration 15/25 | Loss: 0.00020552
Iteration 16/25 | Loss: 0.00020552
Iteration 17/25 | Loss: 0.00020552
Iteration 18/25 | Loss: 0.00020552
Iteration 19/25 | Loss: 0.00020552
Iteration 20/25 | Loss: 0.00020552
Iteration 21/25 | Loss: 0.00020552
Iteration 22/25 | Loss: 0.00020552
Iteration 23/25 | Loss: 0.00020552
Iteration 24/25 | Loss: 0.00020552
Iteration 25/25 | Loss: 0.00020552

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00020552
Iteration 2/1000 | Loss: 0.00002505
Iteration 3/1000 | Loss: 0.00001835
Iteration 4/1000 | Loss: 0.00001691
Iteration 5/1000 | Loss: 0.00001594
Iteration 6/1000 | Loss: 0.00001535
Iteration 7/1000 | Loss: 0.00001492
Iteration 8/1000 | Loss: 0.00001463
Iteration 9/1000 | Loss: 0.00001447
Iteration 10/1000 | Loss: 0.00001423
Iteration 11/1000 | Loss: 0.00001408
Iteration 12/1000 | Loss: 0.00001401
Iteration 13/1000 | Loss: 0.00001397
Iteration 14/1000 | Loss: 0.00001396
Iteration 15/1000 | Loss: 0.00001389
Iteration 16/1000 | Loss: 0.00001385
Iteration 17/1000 | Loss: 0.00001384
Iteration 18/1000 | Loss: 0.00001378
Iteration 19/1000 | Loss: 0.00001378
Iteration 20/1000 | Loss: 0.00001376
Iteration 21/1000 | Loss: 0.00001376
Iteration 22/1000 | Loss: 0.00001376
Iteration 23/1000 | Loss: 0.00001376
Iteration 24/1000 | Loss: 0.00001375
Iteration 25/1000 | Loss: 0.00001375
Iteration 26/1000 | Loss: 0.00001374
Iteration 27/1000 | Loss: 0.00001373
Iteration 28/1000 | Loss: 0.00001373
Iteration 29/1000 | Loss: 0.00001373
Iteration 30/1000 | Loss: 0.00001373
Iteration 31/1000 | Loss: 0.00001372
Iteration 32/1000 | Loss: 0.00001372
Iteration 33/1000 | Loss: 0.00001372
Iteration 34/1000 | Loss: 0.00001372
Iteration 35/1000 | Loss: 0.00001371
Iteration 36/1000 | Loss: 0.00001371
Iteration 37/1000 | Loss: 0.00001371
Iteration 38/1000 | Loss: 0.00001370
Iteration 39/1000 | Loss: 0.00001370
Iteration 40/1000 | Loss: 0.00001370
Iteration 41/1000 | Loss: 0.00001369
Iteration 42/1000 | Loss: 0.00001369
Iteration 43/1000 | Loss: 0.00001369
Iteration 44/1000 | Loss: 0.00001369
Iteration 45/1000 | Loss: 0.00001369
Iteration 46/1000 | Loss: 0.00001369
Iteration 47/1000 | Loss: 0.00001369
Iteration 48/1000 | Loss: 0.00001368
Iteration 49/1000 | Loss: 0.00001368
Iteration 50/1000 | Loss: 0.00001368
Iteration 51/1000 | Loss: 0.00001368
Iteration 52/1000 | Loss: 0.00001368
Iteration 53/1000 | Loss: 0.00001368
Iteration 54/1000 | Loss: 0.00001367
Iteration 55/1000 | Loss: 0.00001365
Iteration 56/1000 | Loss: 0.00001364
Iteration 57/1000 | Loss: 0.00001364
Iteration 58/1000 | Loss: 0.00001364
Iteration 59/1000 | Loss: 0.00001363
Iteration 60/1000 | Loss: 0.00001362
Iteration 61/1000 | Loss: 0.00001362
Iteration 62/1000 | Loss: 0.00001361
Iteration 63/1000 | Loss: 0.00001361
Iteration 64/1000 | Loss: 0.00001361
Iteration 65/1000 | Loss: 0.00001361
Iteration 66/1000 | Loss: 0.00001361
Iteration 67/1000 | Loss: 0.00001360
Iteration 68/1000 | Loss: 0.00001360
Iteration 69/1000 | Loss: 0.00001359
Iteration 70/1000 | Loss: 0.00001359
Iteration 71/1000 | Loss: 0.00001359
Iteration 72/1000 | Loss: 0.00001359
Iteration 73/1000 | Loss: 0.00001359
Iteration 74/1000 | Loss: 0.00001359
Iteration 75/1000 | Loss: 0.00001358
Iteration 76/1000 | Loss: 0.00001358
Iteration 77/1000 | Loss: 0.00001358
Iteration 78/1000 | Loss: 0.00001358
Iteration 79/1000 | Loss: 0.00001358
Iteration 80/1000 | Loss: 0.00001358
Iteration 81/1000 | Loss: 0.00001358
Iteration 82/1000 | Loss: 0.00001358
Iteration 83/1000 | Loss: 0.00001357
Iteration 84/1000 | Loss: 0.00001357
Iteration 85/1000 | Loss: 0.00001357
Iteration 86/1000 | Loss: 0.00001357
Iteration 87/1000 | Loss: 0.00001357
Iteration 88/1000 | Loss: 0.00001357
Iteration 89/1000 | Loss: 0.00001357
Iteration 90/1000 | Loss: 0.00001357
Iteration 91/1000 | Loss: 0.00001357
Iteration 92/1000 | Loss: 0.00001357
Iteration 93/1000 | Loss: 0.00001357
Iteration 94/1000 | Loss: 0.00001357
Iteration 95/1000 | Loss: 0.00001357
Iteration 96/1000 | Loss: 0.00001357
Iteration 97/1000 | Loss: 0.00001357
Iteration 98/1000 | Loss: 0.00001356
Iteration 99/1000 | Loss: 0.00001356
Iteration 100/1000 | Loss: 0.00001356
Iteration 101/1000 | Loss: 0.00001356
Iteration 102/1000 | Loss: 0.00001356
Iteration 103/1000 | Loss: 0.00001356
Iteration 104/1000 | Loss: 0.00001356
Iteration 105/1000 | Loss: 0.00001356
Iteration 106/1000 | Loss: 0.00001356
Iteration 107/1000 | Loss: 0.00001356
Iteration 108/1000 | Loss: 0.00001355
Iteration 109/1000 | Loss: 0.00001355
Iteration 110/1000 | Loss: 0.00001355
Iteration 111/1000 | Loss: 0.00001355
Iteration 112/1000 | Loss: 0.00001355
Iteration 113/1000 | Loss: 0.00001355
Iteration 114/1000 | Loss: 0.00001355
Iteration 115/1000 | Loss: 0.00001355
Iteration 116/1000 | Loss: 0.00001355
Iteration 117/1000 | Loss: 0.00001355
Iteration 118/1000 | Loss: 0.00001355
Iteration 119/1000 | Loss: 0.00001354
Iteration 120/1000 | Loss: 0.00001354
Iteration 121/1000 | Loss: 0.00001354
Iteration 122/1000 | Loss: 0.00001354
Iteration 123/1000 | Loss: 0.00001354
Iteration 124/1000 | Loss: 0.00001354
Iteration 125/1000 | Loss: 0.00001354
Iteration 126/1000 | Loss: 0.00001354
Iteration 127/1000 | Loss: 0.00001354
Iteration 128/1000 | Loss: 0.00001354
Iteration 129/1000 | Loss: 0.00001354
Iteration 130/1000 | Loss: 0.00001354
Iteration 131/1000 | Loss: 0.00001354
Iteration 132/1000 | Loss: 0.00001354
Iteration 133/1000 | Loss: 0.00001354
Iteration 134/1000 | Loss: 0.00001354
Iteration 135/1000 | Loss: 0.00001354
Iteration 136/1000 | Loss: 0.00001353
Iteration 137/1000 | Loss: 0.00001353
Iteration 138/1000 | Loss: 0.00001353
Iteration 139/1000 | Loss: 0.00001353
Iteration 140/1000 | Loss: 0.00001353
Iteration 141/1000 | Loss: 0.00001353
Iteration 142/1000 | Loss: 0.00001353
Iteration 143/1000 | Loss: 0.00001353
Iteration 144/1000 | Loss: 0.00001353
Iteration 145/1000 | Loss: 0.00001353
Iteration 146/1000 | Loss: 0.00001352
Iteration 147/1000 | Loss: 0.00001352
Iteration 148/1000 | Loss: 0.00001352
Iteration 149/1000 | Loss: 0.00001352
Iteration 150/1000 | Loss: 0.00001352
Iteration 151/1000 | Loss: 0.00001352
Iteration 152/1000 | Loss: 0.00001352
Iteration 153/1000 | Loss: 0.00001352
Iteration 154/1000 | Loss: 0.00001352
Iteration 155/1000 | Loss: 0.00001352
Iteration 156/1000 | Loss: 0.00001352
Iteration 157/1000 | Loss: 0.00001352
Iteration 158/1000 | Loss: 0.00001352
Iteration 159/1000 | Loss: 0.00001352
Iteration 160/1000 | Loss: 0.00001352
Iteration 161/1000 | Loss: 0.00001352
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.3522981134883594e-05, 1.3522981134883594e-05, 1.3522981134883594e-05, 1.3522981134883594e-05, 1.3522981134883594e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3522981134883594e-05

Optimization complete. Final v2v error: 3.0214033126831055 mm

Highest mean error: 3.2909724712371826 mm for frame 110

Lowest mean error: 2.676762104034424 mm for frame 19

Saving results

Total time: 39.30493187904358
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00501725
Iteration 2/25 | Loss: 0.00093420
Iteration 3/25 | Loss: 0.00073572
Iteration 4/25 | Loss: 0.00067959
Iteration 5/25 | Loss: 0.00067246
Iteration 6/25 | Loss: 0.00067143
Iteration 7/25 | Loss: 0.00067096
Iteration 8/25 | Loss: 0.00067094
Iteration 9/25 | Loss: 0.00067094
Iteration 10/25 | Loss: 0.00067094
Iteration 11/25 | Loss: 0.00067094
Iteration 12/25 | Loss: 0.00067094
Iteration 13/25 | Loss: 0.00067094
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006709445151500404, 0.0006709445151500404, 0.0006709445151500404, 0.0006709445151500404, 0.0006709445151500404]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006709445151500404

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.06726658
Iteration 2/25 | Loss: 0.00028340
Iteration 3/25 | Loss: 0.00028334
Iteration 4/25 | Loss: 0.00028334
Iteration 5/25 | Loss: 0.00028334
Iteration 6/25 | Loss: 0.00028334
Iteration 7/25 | Loss: 0.00028334
Iteration 8/25 | Loss: 0.00028334
Iteration 9/25 | Loss: 0.00028334
Iteration 10/25 | Loss: 0.00028334
Iteration 11/25 | Loss: 0.00028334
Iteration 12/25 | Loss: 0.00028334
Iteration 13/25 | Loss: 0.00028334
Iteration 14/25 | Loss: 0.00028334
Iteration 15/25 | Loss: 0.00028334
Iteration 16/25 | Loss: 0.00028334
Iteration 17/25 | Loss: 0.00028334
Iteration 18/25 | Loss: 0.00028334
Iteration 19/25 | Loss: 0.00028334
Iteration 20/25 | Loss: 0.00028334
Iteration 21/25 | Loss: 0.00028334
Iteration 22/25 | Loss: 0.00028334
Iteration 23/25 | Loss: 0.00028334
Iteration 24/25 | Loss: 0.00028334
Iteration 25/25 | Loss: 0.00028334

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028334
Iteration 2/1000 | Loss: 0.00004201
Iteration 3/1000 | Loss: 0.00002823
Iteration 4/1000 | Loss: 0.00002500
Iteration 5/1000 | Loss: 0.00002350
Iteration 6/1000 | Loss: 0.00002285
Iteration 7/1000 | Loss: 0.00002203
Iteration 8/1000 | Loss: 0.00002164
Iteration 9/1000 | Loss: 0.00002129
Iteration 10/1000 | Loss: 0.00002104
Iteration 11/1000 | Loss: 0.00002078
Iteration 12/1000 | Loss: 0.00002053
Iteration 13/1000 | Loss: 0.00002036
Iteration 14/1000 | Loss: 0.00002035
Iteration 15/1000 | Loss: 0.00002035
Iteration 16/1000 | Loss: 0.00002034
Iteration 17/1000 | Loss: 0.00002034
Iteration 18/1000 | Loss: 0.00002034
Iteration 19/1000 | Loss: 0.00002031
Iteration 20/1000 | Loss: 0.00002030
Iteration 21/1000 | Loss: 0.00002027
Iteration 22/1000 | Loss: 0.00002024
Iteration 23/1000 | Loss: 0.00002024
Iteration 24/1000 | Loss: 0.00002017
Iteration 25/1000 | Loss: 0.00002015
Iteration 26/1000 | Loss: 0.00002015
Iteration 27/1000 | Loss: 0.00002014
Iteration 28/1000 | Loss: 0.00002014
Iteration 29/1000 | Loss: 0.00002013
Iteration 30/1000 | Loss: 0.00002013
Iteration 31/1000 | Loss: 0.00002013
Iteration 32/1000 | Loss: 0.00002012
Iteration 33/1000 | Loss: 0.00002012
Iteration 34/1000 | Loss: 0.00002012
Iteration 35/1000 | Loss: 0.00002011
Iteration 36/1000 | Loss: 0.00002011
Iteration 37/1000 | Loss: 0.00002011
Iteration 38/1000 | Loss: 0.00002010
Iteration 39/1000 | Loss: 0.00002010
Iteration 40/1000 | Loss: 0.00002010
Iteration 41/1000 | Loss: 0.00002010
Iteration 42/1000 | Loss: 0.00002009
Iteration 43/1000 | Loss: 0.00002009
Iteration 44/1000 | Loss: 0.00002009
Iteration 45/1000 | Loss: 0.00002009
Iteration 46/1000 | Loss: 0.00002008
Iteration 47/1000 | Loss: 0.00002007
Iteration 48/1000 | Loss: 0.00002007
Iteration 49/1000 | Loss: 0.00002007
Iteration 50/1000 | Loss: 0.00002007
Iteration 51/1000 | Loss: 0.00002006
Iteration 52/1000 | Loss: 0.00002006
Iteration 53/1000 | Loss: 0.00002006
Iteration 54/1000 | Loss: 0.00002006
Iteration 55/1000 | Loss: 0.00002005
Iteration 56/1000 | Loss: 0.00002005
Iteration 57/1000 | Loss: 0.00002004
Iteration 58/1000 | Loss: 0.00002004
Iteration 59/1000 | Loss: 0.00002004
Iteration 60/1000 | Loss: 0.00002004
Iteration 61/1000 | Loss: 0.00002003
Iteration 62/1000 | Loss: 0.00002003
Iteration 63/1000 | Loss: 0.00002003
Iteration 64/1000 | Loss: 0.00002003
Iteration 65/1000 | Loss: 0.00002003
Iteration 66/1000 | Loss: 0.00002003
Iteration 67/1000 | Loss: 0.00002002
Iteration 68/1000 | Loss: 0.00002002
Iteration 69/1000 | Loss: 0.00002001
Iteration 70/1000 | Loss: 0.00002001
Iteration 71/1000 | Loss: 0.00002000
Iteration 72/1000 | Loss: 0.00002000
Iteration 73/1000 | Loss: 0.00002000
Iteration 74/1000 | Loss: 0.00001999
Iteration 75/1000 | Loss: 0.00001999
Iteration 76/1000 | Loss: 0.00001999
Iteration 77/1000 | Loss: 0.00001999
Iteration 78/1000 | Loss: 0.00001999
Iteration 79/1000 | Loss: 0.00001999
Iteration 80/1000 | Loss: 0.00001999
Iteration 81/1000 | Loss: 0.00001999
Iteration 82/1000 | Loss: 0.00001999
Iteration 83/1000 | Loss: 0.00001999
Iteration 84/1000 | Loss: 0.00001997
Iteration 85/1000 | Loss: 0.00001997
Iteration 86/1000 | Loss: 0.00001996
Iteration 87/1000 | Loss: 0.00001995
Iteration 88/1000 | Loss: 0.00001994
Iteration 89/1000 | Loss: 0.00001994
Iteration 90/1000 | Loss: 0.00001993
Iteration 91/1000 | Loss: 0.00001993
Iteration 92/1000 | Loss: 0.00001993
Iteration 93/1000 | Loss: 0.00001992
Iteration 94/1000 | Loss: 0.00001992
Iteration 95/1000 | Loss: 0.00001992
Iteration 96/1000 | Loss: 0.00001992
Iteration 97/1000 | Loss: 0.00001991
Iteration 98/1000 | Loss: 0.00001991
Iteration 99/1000 | Loss: 0.00001991
Iteration 100/1000 | Loss: 0.00001991
Iteration 101/1000 | Loss: 0.00001991
Iteration 102/1000 | Loss: 0.00001990
Iteration 103/1000 | Loss: 0.00001990
Iteration 104/1000 | Loss: 0.00001990
Iteration 105/1000 | Loss: 0.00001990
Iteration 106/1000 | Loss: 0.00001990
Iteration 107/1000 | Loss: 0.00001990
Iteration 108/1000 | Loss: 0.00001990
Iteration 109/1000 | Loss: 0.00001990
Iteration 110/1000 | Loss: 0.00001990
Iteration 111/1000 | Loss: 0.00001990
Iteration 112/1000 | Loss: 0.00001989
Iteration 113/1000 | Loss: 0.00001989
Iteration 114/1000 | Loss: 0.00001989
Iteration 115/1000 | Loss: 0.00001989
Iteration 116/1000 | Loss: 0.00001989
Iteration 117/1000 | Loss: 0.00001989
Iteration 118/1000 | Loss: 0.00001989
Iteration 119/1000 | Loss: 0.00001989
Iteration 120/1000 | Loss: 0.00001988
Iteration 121/1000 | Loss: 0.00001988
Iteration 122/1000 | Loss: 0.00001988
Iteration 123/1000 | Loss: 0.00001988
Iteration 124/1000 | Loss: 0.00001988
Iteration 125/1000 | Loss: 0.00001988
Iteration 126/1000 | Loss: 0.00001988
Iteration 127/1000 | Loss: 0.00001988
Iteration 128/1000 | Loss: 0.00001988
Iteration 129/1000 | Loss: 0.00001988
Iteration 130/1000 | Loss: 0.00001988
Iteration 131/1000 | Loss: 0.00001988
Iteration 132/1000 | Loss: 0.00001988
Iteration 133/1000 | Loss: 0.00001988
Iteration 134/1000 | Loss: 0.00001988
Iteration 135/1000 | Loss: 0.00001988
Iteration 136/1000 | Loss: 0.00001988
Iteration 137/1000 | Loss: 0.00001988
Iteration 138/1000 | Loss: 0.00001988
Iteration 139/1000 | Loss: 0.00001988
Iteration 140/1000 | Loss: 0.00001987
Iteration 141/1000 | Loss: 0.00001987
Iteration 142/1000 | Loss: 0.00001987
Iteration 143/1000 | Loss: 0.00001987
Iteration 144/1000 | Loss: 0.00001987
Iteration 145/1000 | Loss: 0.00001987
Iteration 146/1000 | Loss: 0.00001987
Iteration 147/1000 | Loss: 0.00001987
Iteration 148/1000 | Loss: 0.00001987
Iteration 149/1000 | Loss: 0.00001987
Iteration 150/1000 | Loss: 0.00001987
Iteration 151/1000 | Loss: 0.00001987
Iteration 152/1000 | Loss: 0.00001987
Iteration 153/1000 | Loss: 0.00001987
Iteration 154/1000 | Loss: 0.00001987
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.986747702176217e-05, 1.986747702176217e-05, 1.986747702176217e-05, 1.986747702176217e-05, 1.986747702176217e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.986747702176217e-05

Optimization complete. Final v2v error: 3.7091126441955566 mm

Highest mean error: 3.7499141693115234 mm for frame 41

Lowest mean error: 3.651259183883667 mm for frame 6

Saving results

Total time: 39.32661843299866
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01019001
Iteration 2/25 | Loss: 0.01019001
Iteration 3/25 | Loss: 0.01019000
Iteration 4/25 | Loss: 0.01019000
Iteration 5/25 | Loss: 0.01019000
Iteration 6/25 | Loss: 0.01019000
Iteration 7/25 | Loss: 0.01018999
Iteration 8/25 | Loss: 0.01018999
Iteration 9/25 | Loss: 0.01018999
Iteration 10/25 | Loss: 0.01018999
Iteration 11/25 | Loss: 0.01018999
Iteration 12/25 | Loss: 0.01018998
Iteration 13/25 | Loss: 0.01018998
Iteration 14/25 | Loss: 0.01018998
Iteration 15/25 | Loss: 0.01018998
Iteration 16/25 | Loss: 0.01018997
Iteration 17/25 | Loss: 0.01018997
Iteration 18/25 | Loss: 0.01018997
Iteration 19/25 | Loss: 0.01018997
Iteration 20/25 | Loss: 0.01018997
Iteration 21/25 | Loss: 0.01018996
Iteration 22/25 | Loss: 0.01018996
Iteration 23/25 | Loss: 0.01018996
Iteration 24/25 | Loss: 0.01018996
Iteration 25/25 | Loss: 0.01018996

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.71427751
Iteration 2/25 | Loss: 0.17243648
Iteration 3/25 | Loss: 0.17243545
Iteration 4/25 | Loss: 0.17243540
Iteration 5/25 | Loss: 0.17243540
Iteration 6/25 | Loss: 0.17243536
Iteration 7/25 | Loss: 0.17243536
Iteration 8/25 | Loss: 0.17243536
Iteration 9/25 | Loss: 0.17243536
Iteration 10/25 | Loss: 0.17243536
Iteration 11/25 | Loss: 0.17243536
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.17243535816669464, 0.17243535816669464, 0.17243535816669464, 0.17243535816669464, 0.17243535816669464]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.17243535816669464

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17243536
Iteration 2/1000 | Loss: 0.01196641
Iteration 3/1000 | Loss: 0.01556026
Iteration 4/1000 | Loss: 0.00506571
Iteration 5/1000 | Loss: 0.01583712
Iteration 6/1000 | Loss: 0.00109140
Iteration 7/1000 | Loss: 0.00403668
Iteration 8/1000 | Loss: 0.00240197
Iteration 9/1000 | Loss: 0.00141376
Iteration 10/1000 | Loss: 0.00074133
Iteration 11/1000 | Loss: 0.00026945
Iteration 12/1000 | Loss: 0.00048888
Iteration 13/1000 | Loss: 0.00172671
Iteration 14/1000 | Loss: 0.00150044
Iteration 15/1000 | Loss: 0.00335261
Iteration 16/1000 | Loss: 0.00051654
Iteration 17/1000 | Loss: 0.00072799
Iteration 18/1000 | Loss: 0.00007663
Iteration 19/1000 | Loss: 0.00009653
Iteration 20/1000 | Loss: 0.00036514
Iteration 21/1000 | Loss: 0.00055806
Iteration 22/1000 | Loss: 0.00017817
Iteration 23/1000 | Loss: 0.00005711
Iteration 24/1000 | Loss: 0.00004456
Iteration 25/1000 | Loss: 0.00044900
Iteration 26/1000 | Loss: 0.00003752
Iteration 27/1000 | Loss: 0.00003406
Iteration 28/1000 | Loss: 0.00014257
Iteration 29/1000 | Loss: 0.00004257
Iteration 30/1000 | Loss: 0.00004797
Iteration 31/1000 | Loss: 0.00002764
Iteration 32/1000 | Loss: 0.00025632
Iteration 33/1000 | Loss: 0.00023705
Iteration 34/1000 | Loss: 0.00003431
Iteration 35/1000 | Loss: 0.00006748
Iteration 36/1000 | Loss: 0.00002597
Iteration 37/1000 | Loss: 0.00013941
Iteration 38/1000 | Loss: 0.00080278
Iteration 39/1000 | Loss: 0.00051517
Iteration 40/1000 | Loss: 0.00040499
Iteration 41/1000 | Loss: 0.00021393
Iteration 42/1000 | Loss: 0.00025280
Iteration 43/1000 | Loss: 0.00002751
Iteration 44/1000 | Loss: 0.00002604
Iteration 45/1000 | Loss: 0.00002503
Iteration 46/1000 | Loss: 0.00018608
Iteration 47/1000 | Loss: 0.00003755
Iteration 48/1000 | Loss: 0.00002648
Iteration 49/1000 | Loss: 0.00004321
Iteration 50/1000 | Loss: 0.00027327
Iteration 51/1000 | Loss: 0.00016474
Iteration 52/1000 | Loss: 0.00018984
Iteration 53/1000 | Loss: 0.00002316
Iteration 54/1000 | Loss: 0.00002254
Iteration 55/1000 | Loss: 0.00002214
Iteration 56/1000 | Loss: 0.00002198
Iteration 57/1000 | Loss: 0.00002190
Iteration 58/1000 | Loss: 0.00002182
Iteration 59/1000 | Loss: 0.00002176
Iteration 60/1000 | Loss: 0.00002175
Iteration 61/1000 | Loss: 0.00002168
Iteration 62/1000 | Loss: 0.00002168
Iteration 63/1000 | Loss: 0.00002168
Iteration 64/1000 | Loss: 0.00002168
Iteration 65/1000 | Loss: 0.00002167
Iteration 66/1000 | Loss: 0.00002167
Iteration 67/1000 | Loss: 0.00002167
Iteration 68/1000 | Loss: 0.00002166
Iteration 69/1000 | Loss: 0.00002166
Iteration 70/1000 | Loss: 0.00002165
Iteration 71/1000 | Loss: 0.00002164
Iteration 72/1000 | Loss: 0.00002161
Iteration 73/1000 | Loss: 0.00002160
Iteration 74/1000 | Loss: 0.00002160
Iteration 75/1000 | Loss: 0.00002160
Iteration 76/1000 | Loss: 0.00002160
Iteration 77/1000 | Loss: 0.00002160
Iteration 78/1000 | Loss: 0.00002160
Iteration 79/1000 | Loss: 0.00002160
Iteration 80/1000 | Loss: 0.00002160
Iteration 81/1000 | Loss: 0.00002160
Iteration 82/1000 | Loss: 0.00002160
Iteration 83/1000 | Loss: 0.00002159
Iteration 84/1000 | Loss: 0.00002157
Iteration 85/1000 | Loss: 0.00002157
Iteration 86/1000 | Loss: 0.00002157
Iteration 87/1000 | Loss: 0.00002157
Iteration 88/1000 | Loss: 0.00002157
Iteration 89/1000 | Loss: 0.00002156
Iteration 90/1000 | Loss: 0.00002156
Iteration 91/1000 | Loss: 0.00002154
Iteration 92/1000 | Loss: 0.00002154
Iteration 93/1000 | Loss: 0.00002154
Iteration 94/1000 | Loss: 0.00002153
Iteration 95/1000 | Loss: 0.00002151
Iteration 96/1000 | Loss: 0.00002151
Iteration 97/1000 | Loss: 0.00002150
Iteration 98/1000 | Loss: 0.00002150
Iteration 99/1000 | Loss: 0.00002150
Iteration 100/1000 | Loss: 0.00002150
Iteration 101/1000 | Loss: 0.00002150
Iteration 102/1000 | Loss: 0.00002148
Iteration 103/1000 | Loss: 0.00002147
Iteration 104/1000 | Loss: 0.00002147
Iteration 105/1000 | Loss: 0.00002147
Iteration 106/1000 | Loss: 0.00002147
Iteration 107/1000 | Loss: 0.00002147
Iteration 108/1000 | Loss: 0.00002147
Iteration 109/1000 | Loss: 0.00002147
Iteration 110/1000 | Loss: 0.00002146
Iteration 111/1000 | Loss: 0.00002146
Iteration 112/1000 | Loss: 0.00002146
Iteration 113/1000 | Loss: 0.00002146
Iteration 114/1000 | Loss: 0.00002146
Iteration 115/1000 | Loss: 0.00002146
Iteration 116/1000 | Loss: 0.00002146
Iteration 117/1000 | Loss: 0.00002145
Iteration 118/1000 | Loss: 0.00002145
Iteration 119/1000 | Loss: 0.00002145
Iteration 120/1000 | Loss: 0.00002145
Iteration 121/1000 | Loss: 0.00002144
Iteration 122/1000 | Loss: 0.00002144
Iteration 123/1000 | Loss: 0.00002144
Iteration 124/1000 | Loss: 0.00002144
Iteration 125/1000 | Loss: 0.00002144
Iteration 126/1000 | Loss: 0.00002143
Iteration 127/1000 | Loss: 0.00002143
Iteration 128/1000 | Loss: 0.00002143
Iteration 129/1000 | Loss: 0.00002143
Iteration 130/1000 | Loss: 0.00002143
Iteration 131/1000 | Loss: 0.00002143
Iteration 132/1000 | Loss: 0.00002143
Iteration 133/1000 | Loss: 0.00002143
Iteration 134/1000 | Loss: 0.00002143
Iteration 135/1000 | Loss: 0.00002143
Iteration 136/1000 | Loss: 0.00002142
Iteration 137/1000 | Loss: 0.00002142
Iteration 138/1000 | Loss: 0.00002142
Iteration 139/1000 | Loss: 0.00002142
Iteration 140/1000 | Loss: 0.00002142
Iteration 141/1000 | Loss: 0.00002142
Iteration 142/1000 | Loss: 0.00002141
Iteration 143/1000 | Loss: 0.00002141
Iteration 144/1000 | Loss: 0.00002141
Iteration 145/1000 | Loss: 0.00002141
Iteration 146/1000 | Loss: 0.00002141
Iteration 147/1000 | Loss: 0.00002140
Iteration 148/1000 | Loss: 0.00002140
Iteration 149/1000 | Loss: 0.00002140
Iteration 150/1000 | Loss: 0.00002140
Iteration 151/1000 | Loss: 0.00002140
Iteration 152/1000 | Loss: 0.00002140
Iteration 153/1000 | Loss: 0.00002140
Iteration 154/1000 | Loss: 0.00002139
Iteration 155/1000 | Loss: 0.00002139
Iteration 156/1000 | Loss: 0.00002139
Iteration 157/1000 | Loss: 0.00002139
Iteration 158/1000 | Loss: 0.00002139
Iteration 159/1000 | Loss: 0.00002139
Iteration 160/1000 | Loss: 0.00002139
Iteration 161/1000 | Loss: 0.00002139
Iteration 162/1000 | Loss: 0.00002139
Iteration 163/1000 | Loss: 0.00002139
Iteration 164/1000 | Loss: 0.00002139
Iteration 165/1000 | Loss: 0.00002139
Iteration 166/1000 | Loss: 0.00002139
Iteration 167/1000 | Loss: 0.00002139
Iteration 168/1000 | Loss: 0.00002139
Iteration 169/1000 | Loss: 0.00002139
Iteration 170/1000 | Loss: 0.00002138
Iteration 171/1000 | Loss: 0.00002138
Iteration 172/1000 | Loss: 0.00002138
Iteration 173/1000 | Loss: 0.00002138
Iteration 174/1000 | Loss: 0.00002138
Iteration 175/1000 | Loss: 0.00002138
Iteration 176/1000 | Loss: 0.00002138
Iteration 177/1000 | Loss: 0.00002138
Iteration 178/1000 | Loss: 0.00002138
Iteration 179/1000 | Loss: 0.00002138
Iteration 180/1000 | Loss: 0.00002138
Iteration 181/1000 | Loss: 0.00002138
Iteration 182/1000 | Loss: 0.00002138
Iteration 183/1000 | Loss: 0.00002138
Iteration 184/1000 | Loss: 0.00002138
Iteration 185/1000 | Loss: 0.00002138
Iteration 186/1000 | Loss: 0.00002138
Iteration 187/1000 | Loss: 0.00002138
Iteration 188/1000 | Loss: 0.00002138
Iteration 189/1000 | Loss: 0.00002137
Iteration 190/1000 | Loss: 0.00002137
Iteration 191/1000 | Loss: 0.00002137
Iteration 192/1000 | Loss: 0.00002137
Iteration 193/1000 | Loss: 0.00002137
Iteration 194/1000 | Loss: 0.00002137
Iteration 195/1000 | Loss: 0.00002137
Iteration 196/1000 | Loss: 0.00002137
Iteration 197/1000 | Loss: 0.00002137
Iteration 198/1000 | Loss: 0.00002137
Iteration 199/1000 | Loss: 0.00002137
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [2.137197407137137e-05, 2.137197407137137e-05, 2.137197407137137e-05, 2.137197407137137e-05, 2.137197407137137e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.137197407137137e-05

Optimization complete. Final v2v error: 3.798395872116089 mm

Highest mean error: 11.351188659667969 mm for frame 211

Lowest mean error: 3.5210182666778564 mm for frame 177

Saving results

Total time: 110.59308552742004
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00460550
Iteration 2/25 | Loss: 0.00081801
Iteration 3/25 | Loss: 0.00064130
Iteration 4/25 | Loss: 0.00061927
Iteration 5/25 | Loss: 0.00061331
Iteration 6/25 | Loss: 0.00061243
Iteration 7/25 | Loss: 0.00061243
Iteration 8/25 | Loss: 0.00061243
Iteration 9/25 | Loss: 0.00061243
Iteration 10/25 | Loss: 0.00061243
Iteration 11/25 | Loss: 0.00061243
Iteration 12/25 | Loss: 0.00061243
Iteration 13/25 | Loss: 0.00061243
Iteration 14/25 | Loss: 0.00061243
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0006124302744865417, 0.0006124302744865417, 0.0006124302744865417, 0.0006124302744865417, 0.0006124302744865417]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006124302744865417

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.02620435
Iteration 2/25 | Loss: 0.00027562
Iteration 3/25 | Loss: 0.00027561
Iteration 4/25 | Loss: 0.00027561
Iteration 5/25 | Loss: 0.00027561
Iteration 6/25 | Loss: 0.00027561
Iteration 7/25 | Loss: 0.00027561
Iteration 8/25 | Loss: 0.00027561
Iteration 9/25 | Loss: 0.00027561
Iteration 10/25 | Loss: 0.00027561
Iteration 11/25 | Loss: 0.00027561
Iteration 12/25 | Loss: 0.00027561
Iteration 13/25 | Loss: 0.00027561
Iteration 14/25 | Loss: 0.00027561
Iteration 15/25 | Loss: 0.00027561
Iteration 16/25 | Loss: 0.00027561
Iteration 17/25 | Loss: 0.00027561
Iteration 18/25 | Loss: 0.00027561
Iteration 19/25 | Loss: 0.00027561
Iteration 20/25 | Loss: 0.00027561
Iteration 21/25 | Loss: 0.00027561
Iteration 22/25 | Loss: 0.00027561
Iteration 23/25 | Loss: 0.00027561
Iteration 24/25 | Loss: 0.00027561
Iteration 25/25 | Loss: 0.00027561

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027561
Iteration 2/1000 | Loss: 0.00002374
Iteration 3/1000 | Loss: 0.00001639
Iteration 4/1000 | Loss: 0.00001508
Iteration 5/1000 | Loss: 0.00001434
Iteration 6/1000 | Loss: 0.00001370
Iteration 7/1000 | Loss: 0.00001340
Iteration 8/1000 | Loss: 0.00001311
Iteration 9/1000 | Loss: 0.00001309
Iteration 10/1000 | Loss: 0.00001303
Iteration 11/1000 | Loss: 0.00001298
Iteration 12/1000 | Loss: 0.00001295
Iteration 13/1000 | Loss: 0.00001295
Iteration 14/1000 | Loss: 0.00001294
Iteration 15/1000 | Loss: 0.00001293
Iteration 16/1000 | Loss: 0.00001291
Iteration 17/1000 | Loss: 0.00001282
Iteration 18/1000 | Loss: 0.00001276
Iteration 19/1000 | Loss: 0.00001276
Iteration 20/1000 | Loss: 0.00001274
Iteration 21/1000 | Loss: 0.00001274
Iteration 22/1000 | Loss: 0.00001271
Iteration 23/1000 | Loss: 0.00001269
Iteration 24/1000 | Loss: 0.00001269
Iteration 25/1000 | Loss: 0.00001268
Iteration 26/1000 | Loss: 0.00001268
Iteration 27/1000 | Loss: 0.00001267
Iteration 28/1000 | Loss: 0.00001267
Iteration 29/1000 | Loss: 0.00001266
Iteration 30/1000 | Loss: 0.00001266
Iteration 31/1000 | Loss: 0.00001266
Iteration 32/1000 | Loss: 0.00001265
Iteration 33/1000 | Loss: 0.00001265
Iteration 34/1000 | Loss: 0.00001265
Iteration 35/1000 | Loss: 0.00001264
Iteration 36/1000 | Loss: 0.00001264
Iteration 37/1000 | Loss: 0.00001263
Iteration 38/1000 | Loss: 0.00001263
Iteration 39/1000 | Loss: 0.00001263
Iteration 40/1000 | Loss: 0.00001262
Iteration 41/1000 | Loss: 0.00001262
Iteration 42/1000 | Loss: 0.00001262
Iteration 43/1000 | Loss: 0.00001262
Iteration 44/1000 | Loss: 0.00001261
Iteration 45/1000 | Loss: 0.00001261
Iteration 46/1000 | Loss: 0.00001261
Iteration 47/1000 | Loss: 0.00001260
Iteration 48/1000 | Loss: 0.00001260
Iteration 49/1000 | Loss: 0.00001260
Iteration 50/1000 | Loss: 0.00001260
Iteration 51/1000 | Loss: 0.00001260
Iteration 52/1000 | Loss: 0.00001259
Iteration 53/1000 | Loss: 0.00001259
Iteration 54/1000 | Loss: 0.00001259
Iteration 55/1000 | Loss: 0.00001258
Iteration 56/1000 | Loss: 0.00001258
Iteration 57/1000 | Loss: 0.00001258
Iteration 58/1000 | Loss: 0.00001258
Iteration 59/1000 | Loss: 0.00001258
Iteration 60/1000 | Loss: 0.00001258
Iteration 61/1000 | Loss: 0.00001258
Iteration 62/1000 | Loss: 0.00001257
Iteration 63/1000 | Loss: 0.00001257
Iteration 64/1000 | Loss: 0.00001257
Iteration 65/1000 | Loss: 0.00001257
Iteration 66/1000 | Loss: 0.00001256
Iteration 67/1000 | Loss: 0.00001256
Iteration 68/1000 | Loss: 0.00001256
Iteration 69/1000 | Loss: 0.00001256
Iteration 70/1000 | Loss: 0.00001256
Iteration 71/1000 | Loss: 0.00001256
Iteration 72/1000 | Loss: 0.00001255
Iteration 73/1000 | Loss: 0.00001255
Iteration 74/1000 | Loss: 0.00001255
Iteration 75/1000 | Loss: 0.00001255
Iteration 76/1000 | Loss: 0.00001255
Iteration 77/1000 | Loss: 0.00001255
Iteration 78/1000 | Loss: 0.00001255
Iteration 79/1000 | Loss: 0.00001255
Iteration 80/1000 | Loss: 0.00001255
Iteration 81/1000 | Loss: 0.00001255
Iteration 82/1000 | Loss: 0.00001254
Iteration 83/1000 | Loss: 0.00001254
Iteration 84/1000 | Loss: 0.00001254
Iteration 85/1000 | Loss: 0.00001254
Iteration 86/1000 | Loss: 0.00001254
Iteration 87/1000 | Loss: 0.00001254
Iteration 88/1000 | Loss: 0.00001254
Iteration 89/1000 | Loss: 0.00001254
Iteration 90/1000 | Loss: 0.00001254
Iteration 91/1000 | Loss: 0.00001254
Iteration 92/1000 | Loss: 0.00001254
Iteration 93/1000 | Loss: 0.00001254
Iteration 94/1000 | Loss: 0.00001254
Iteration 95/1000 | Loss: 0.00001254
Iteration 96/1000 | Loss: 0.00001254
Iteration 97/1000 | Loss: 0.00001254
Iteration 98/1000 | Loss: 0.00001254
Iteration 99/1000 | Loss: 0.00001254
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [1.2538765986391809e-05, 1.2538765986391809e-05, 1.2538765986391809e-05, 1.2538765986391809e-05, 1.2538765986391809e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2538765986391809e-05

Optimization complete. Final v2v error: 2.9994359016418457 mm

Highest mean error: 3.7500572204589844 mm for frame 171

Lowest mean error: 2.6888582706451416 mm for frame 238

Saving results

Total time: 34.44868040084839
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00917697
Iteration 2/25 | Loss: 0.00077535
Iteration 3/25 | Loss: 0.00060307
Iteration 4/25 | Loss: 0.00056803
Iteration 5/25 | Loss: 0.00055720
Iteration 6/25 | Loss: 0.00055535
Iteration 7/25 | Loss: 0.00055497
Iteration 8/25 | Loss: 0.00055497
Iteration 9/25 | Loss: 0.00055497
Iteration 10/25 | Loss: 0.00055497
Iteration 11/25 | Loss: 0.00055497
Iteration 12/25 | Loss: 0.00055497
Iteration 13/25 | Loss: 0.00055497
Iteration 14/25 | Loss: 0.00055497
Iteration 15/25 | Loss: 0.00055497
Iteration 16/25 | Loss: 0.00055497
Iteration 17/25 | Loss: 0.00055497
Iteration 18/25 | Loss: 0.00055497
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005549689522013068, 0.0005549689522013068, 0.0005549689522013068, 0.0005549689522013068, 0.0005549689522013068]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005549689522013068

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.86296439
Iteration 2/25 | Loss: 0.00017273
Iteration 3/25 | Loss: 0.00017272
Iteration 4/25 | Loss: 0.00017272
Iteration 5/25 | Loss: 0.00017272
Iteration 6/25 | Loss: 0.00017272
Iteration 7/25 | Loss: 0.00017272
Iteration 8/25 | Loss: 0.00017272
Iteration 9/25 | Loss: 0.00017272
Iteration 10/25 | Loss: 0.00017272
Iteration 11/25 | Loss: 0.00017272
Iteration 12/25 | Loss: 0.00017272
Iteration 13/25 | Loss: 0.00017272
Iteration 14/25 | Loss: 0.00017272
Iteration 15/25 | Loss: 0.00017272
Iteration 16/25 | Loss: 0.00017272
Iteration 17/25 | Loss: 0.00017272
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00017272199329454452, 0.00017272199329454452, 0.00017272199329454452, 0.00017272199329454452, 0.00017272199329454452]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00017272199329454452

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00017272
Iteration 2/1000 | Loss: 0.00002168
Iteration 3/1000 | Loss: 0.00001393
Iteration 4/1000 | Loss: 0.00001298
Iteration 5/1000 | Loss: 0.00001235
Iteration 6/1000 | Loss: 0.00001196
Iteration 7/1000 | Loss: 0.00001170
Iteration 8/1000 | Loss: 0.00001156
Iteration 9/1000 | Loss: 0.00001155
Iteration 10/1000 | Loss: 0.00001155
Iteration 11/1000 | Loss: 0.00001153
Iteration 12/1000 | Loss: 0.00001150
Iteration 13/1000 | Loss: 0.00001146
Iteration 14/1000 | Loss: 0.00001145
Iteration 15/1000 | Loss: 0.00001143
Iteration 16/1000 | Loss: 0.00001142
Iteration 17/1000 | Loss: 0.00001142
Iteration 18/1000 | Loss: 0.00001141
Iteration 19/1000 | Loss: 0.00001140
Iteration 20/1000 | Loss: 0.00001139
Iteration 21/1000 | Loss: 0.00001138
Iteration 22/1000 | Loss: 0.00001137
Iteration 23/1000 | Loss: 0.00001136
Iteration 24/1000 | Loss: 0.00001136
Iteration 25/1000 | Loss: 0.00001136
Iteration 26/1000 | Loss: 0.00001135
Iteration 27/1000 | Loss: 0.00001135
Iteration 28/1000 | Loss: 0.00001135
Iteration 29/1000 | Loss: 0.00001134
Iteration 30/1000 | Loss: 0.00001134
Iteration 31/1000 | Loss: 0.00001133
Iteration 32/1000 | Loss: 0.00001133
Iteration 33/1000 | Loss: 0.00001133
Iteration 34/1000 | Loss: 0.00001132
Iteration 35/1000 | Loss: 0.00001132
Iteration 36/1000 | Loss: 0.00001132
Iteration 37/1000 | Loss: 0.00001132
Iteration 38/1000 | Loss: 0.00001132
Iteration 39/1000 | Loss: 0.00001131
Iteration 40/1000 | Loss: 0.00001131
Iteration 41/1000 | Loss: 0.00001131
Iteration 42/1000 | Loss: 0.00001131
Iteration 43/1000 | Loss: 0.00001131
Iteration 44/1000 | Loss: 0.00001130
Iteration 45/1000 | Loss: 0.00001130
Iteration 46/1000 | Loss: 0.00001129
Iteration 47/1000 | Loss: 0.00001129
Iteration 48/1000 | Loss: 0.00001129
Iteration 49/1000 | Loss: 0.00001127
Iteration 50/1000 | Loss: 0.00001127
Iteration 51/1000 | Loss: 0.00001127
Iteration 52/1000 | Loss: 0.00001126
Iteration 53/1000 | Loss: 0.00001126
Iteration 54/1000 | Loss: 0.00001126
Iteration 55/1000 | Loss: 0.00001126
Iteration 56/1000 | Loss: 0.00001126
Iteration 57/1000 | Loss: 0.00001126
Iteration 58/1000 | Loss: 0.00001126
Iteration 59/1000 | Loss: 0.00001126
Iteration 60/1000 | Loss: 0.00001126
Iteration 61/1000 | Loss: 0.00001125
Iteration 62/1000 | Loss: 0.00001124
Iteration 63/1000 | Loss: 0.00001124
Iteration 64/1000 | Loss: 0.00001124
Iteration 65/1000 | Loss: 0.00001123
Iteration 66/1000 | Loss: 0.00001123
Iteration 67/1000 | Loss: 0.00001122
Iteration 68/1000 | Loss: 0.00001122
Iteration 69/1000 | Loss: 0.00001122
Iteration 70/1000 | Loss: 0.00001121
Iteration 71/1000 | Loss: 0.00001121
Iteration 72/1000 | Loss: 0.00001121
Iteration 73/1000 | Loss: 0.00001121
Iteration 74/1000 | Loss: 0.00001120
Iteration 75/1000 | Loss: 0.00001120
Iteration 76/1000 | Loss: 0.00001120
Iteration 77/1000 | Loss: 0.00001119
Iteration 78/1000 | Loss: 0.00001119
Iteration 79/1000 | Loss: 0.00001119
Iteration 80/1000 | Loss: 0.00001119
Iteration 81/1000 | Loss: 0.00001119
Iteration 82/1000 | Loss: 0.00001119
Iteration 83/1000 | Loss: 0.00001119
Iteration 84/1000 | Loss: 0.00001119
Iteration 85/1000 | Loss: 0.00001119
Iteration 86/1000 | Loss: 0.00001119
Iteration 87/1000 | Loss: 0.00001118
Iteration 88/1000 | Loss: 0.00001118
Iteration 89/1000 | Loss: 0.00001118
Iteration 90/1000 | Loss: 0.00001118
Iteration 91/1000 | Loss: 0.00001118
Iteration 92/1000 | Loss: 0.00001118
Iteration 93/1000 | Loss: 0.00001118
Iteration 94/1000 | Loss: 0.00001118
Iteration 95/1000 | Loss: 0.00001117
Iteration 96/1000 | Loss: 0.00001117
Iteration 97/1000 | Loss: 0.00001117
Iteration 98/1000 | Loss: 0.00001117
Iteration 99/1000 | Loss: 0.00001117
Iteration 100/1000 | Loss: 0.00001117
Iteration 101/1000 | Loss: 0.00001117
Iteration 102/1000 | Loss: 0.00001116
Iteration 103/1000 | Loss: 0.00001116
Iteration 104/1000 | Loss: 0.00001116
Iteration 105/1000 | Loss: 0.00001116
Iteration 106/1000 | Loss: 0.00001116
Iteration 107/1000 | Loss: 0.00001116
Iteration 108/1000 | Loss: 0.00001116
Iteration 109/1000 | Loss: 0.00001116
Iteration 110/1000 | Loss: 0.00001116
Iteration 111/1000 | Loss: 0.00001116
Iteration 112/1000 | Loss: 0.00001116
Iteration 113/1000 | Loss: 0.00001116
Iteration 114/1000 | Loss: 0.00001115
Iteration 115/1000 | Loss: 0.00001115
Iteration 116/1000 | Loss: 0.00001115
Iteration 117/1000 | Loss: 0.00001115
Iteration 118/1000 | Loss: 0.00001115
Iteration 119/1000 | Loss: 0.00001115
Iteration 120/1000 | Loss: 0.00001115
Iteration 121/1000 | Loss: 0.00001115
Iteration 122/1000 | Loss: 0.00001115
Iteration 123/1000 | Loss: 0.00001114
Iteration 124/1000 | Loss: 0.00001114
Iteration 125/1000 | Loss: 0.00001114
Iteration 126/1000 | Loss: 0.00001114
Iteration 127/1000 | Loss: 0.00001114
Iteration 128/1000 | Loss: 0.00001113
Iteration 129/1000 | Loss: 0.00001113
Iteration 130/1000 | Loss: 0.00001113
Iteration 131/1000 | Loss: 0.00001112
Iteration 132/1000 | Loss: 0.00001112
Iteration 133/1000 | Loss: 0.00001112
Iteration 134/1000 | Loss: 0.00001112
Iteration 135/1000 | Loss: 0.00001111
Iteration 136/1000 | Loss: 0.00001111
Iteration 137/1000 | Loss: 0.00001111
Iteration 138/1000 | Loss: 0.00001111
Iteration 139/1000 | Loss: 0.00001111
Iteration 140/1000 | Loss: 0.00001110
Iteration 141/1000 | Loss: 0.00001110
Iteration 142/1000 | Loss: 0.00001110
Iteration 143/1000 | Loss: 0.00001110
Iteration 144/1000 | Loss: 0.00001110
Iteration 145/1000 | Loss: 0.00001110
Iteration 146/1000 | Loss: 0.00001110
Iteration 147/1000 | Loss: 0.00001110
Iteration 148/1000 | Loss: 0.00001110
Iteration 149/1000 | Loss: 0.00001110
Iteration 150/1000 | Loss: 0.00001110
Iteration 151/1000 | Loss: 0.00001109
Iteration 152/1000 | Loss: 0.00001109
Iteration 153/1000 | Loss: 0.00001109
Iteration 154/1000 | Loss: 0.00001109
Iteration 155/1000 | Loss: 0.00001108
Iteration 156/1000 | Loss: 0.00001108
Iteration 157/1000 | Loss: 0.00001108
Iteration 158/1000 | Loss: 0.00001108
Iteration 159/1000 | Loss: 0.00001108
Iteration 160/1000 | Loss: 0.00001107
Iteration 161/1000 | Loss: 0.00001107
Iteration 162/1000 | Loss: 0.00001107
Iteration 163/1000 | Loss: 0.00001107
Iteration 164/1000 | Loss: 0.00001107
Iteration 165/1000 | Loss: 0.00001107
Iteration 166/1000 | Loss: 0.00001107
Iteration 167/1000 | Loss: 0.00001107
Iteration 168/1000 | Loss: 0.00001107
Iteration 169/1000 | Loss: 0.00001107
Iteration 170/1000 | Loss: 0.00001106
Iteration 171/1000 | Loss: 0.00001106
Iteration 172/1000 | Loss: 0.00001106
Iteration 173/1000 | Loss: 0.00001106
Iteration 174/1000 | Loss: 0.00001106
Iteration 175/1000 | Loss: 0.00001106
Iteration 176/1000 | Loss: 0.00001106
Iteration 177/1000 | Loss: 0.00001106
Iteration 178/1000 | Loss: 0.00001106
Iteration 179/1000 | Loss: 0.00001106
Iteration 180/1000 | Loss: 0.00001106
Iteration 181/1000 | Loss: 0.00001106
Iteration 182/1000 | Loss: 0.00001106
Iteration 183/1000 | Loss: 0.00001105
Iteration 184/1000 | Loss: 0.00001105
Iteration 185/1000 | Loss: 0.00001105
Iteration 186/1000 | Loss: 0.00001105
Iteration 187/1000 | Loss: 0.00001105
Iteration 188/1000 | Loss: 0.00001105
Iteration 189/1000 | Loss: 0.00001105
Iteration 190/1000 | Loss: 0.00001105
Iteration 191/1000 | Loss: 0.00001105
Iteration 192/1000 | Loss: 0.00001105
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [1.1053532944060862e-05, 1.1053532944060862e-05, 1.1053532944060862e-05, 1.1053532944060862e-05, 1.1053532944060862e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1053532944060862e-05

Optimization complete. Final v2v error: 2.806126356124878 mm

Highest mean error: 2.9485485553741455 mm for frame 108

Lowest mean error: 2.631146192550659 mm for frame 32

Saving results

Total time: 39.508277893066406
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01061952
Iteration 2/25 | Loss: 0.00191320
Iteration 3/25 | Loss: 0.00112386
Iteration 4/25 | Loss: 0.00097977
Iteration 5/25 | Loss: 0.00087814
Iteration 6/25 | Loss: 0.00083132
Iteration 7/25 | Loss: 0.00077725
Iteration 8/25 | Loss: 0.00072881
Iteration 9/25 | Loss: 0.00071291
Iteration 10/25 | Loss: 0.00069002
Iteration 11/25 | Loss: 0.00067595
Iteration 12/25 | Loss: 0.00066650
Iteration 13/25 | Loss: 0.00065604
Iteration 14/25 | Loss: 0.00064810
Iteration 15/25 | Loss: 0.00063830
Iteration 16/25 | Loss: 0.00063918
Iteration 17/25 | Loss: 0.00063271
Iteration 18/25 | Loss: 0.00062089
Iteration 19/25 | Loss: 0.00061532
Iteration 20/25 | Loss: 0.00061720
Iteration 21/25 | Loss: 0.00061313
Iteration 22/25 | Loss: 0.00061080
Iteration 23/25 | Loss: 0.00061001
Iteration 24/25 | Loss: 0.00060991
Iteration 25/25 | Loss: 0.00060990

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57229471
Iteration 2/25 | Loss: 0.00031577
Iteration 3/25 | Loss: 0.00031577
Iteration 4/25 | Loss: 0.00031577
Iteration 5/25 | Loss: 0.00031577
Iteration 6/25 | Loss: 0.00031577
Iteration 7/25 | Loss: 0.00031577
Iteration 8/25 | Loss: 0.00031577
Iteration 9/25 | Loss: 0.00031577
Iteration 10/25 | Loss: 0.00031577
Iteration 11/25 | Loss: 0.00031577
Iteration 12/25 | Loss: 0.00031577
Iteration 13/25 | Loss: 0.00031577
Iteration 14/25 | Loss: 0.00031577
Iteration 15/25 | Loss: 0.00031577
Iteration 16/25 | Loss: 0.00031577
Iteration 17/25 | Loss: 0.00031577
Iteration 18/25 | Loss: 0.00031577
Iteration 19/25 | Loss: 0.00031577
Iteration 20/25 | Loss: 0.00031577
Iteration 21/25 | Loss: 0.00031577
Iteration 22/25 | Loss: 0.00031577
Iteration 23/25 | Loss: 0.00031577
Iteration 24/25 | Loss: 0.00031577
Iteration 25/25 | Loss: 0.00031577
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.00031577172921970487, 0.00031577172921970487, 0.00031577172921970487, 0.00031577172921970487, 0.00031577172921970487]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00031577172921970487

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031577
Iteration 2/1000 | Loss: 0.00003744
Iteration 3/1000 | Loss: 0.00002549
Iteration 4/1000 | Loss: 0.00002278
Iteration 5/1000 | Loss: 0.00083302
Iteration 6/1000 | Loss: 0.00028751
Iteration 7/1000 | Loss: 0.00031375
Iteration 8/1000 | Loss: 0.00002014
Iteration 9/1000 | Loss: 0.00016665
Iteration 10/1000 | Loss: 0.00010017
Iteration 11/1000 | Loss: 0.00001685
Iteration 12/1000 | Loss: 0.00001429
Iteration 13/1000 | Loss: 0.00001296
Iteration 14/1000 | Loss: 0.00001223
Iteration 15/1000 | Loss: 0.00001187
Iteration 16/1000 | Loss: 0.00001164
Iteration 17/1000 | Loss: 0.00001145
Iteration 18/1000 | Loss: 0.00001140
Iteration 19/1000 | Loss: 0.00001135
Iteration 20/1000 | Loss: 0.00001115
Iteration 21/1000 | Loss: 0.00001107
Iteration 22/1000 | Loss: 0.00001103
Iteration 23/1000 | Loss: 0.00001102
Iteration 24/1000 | Loss: 0.00001101
Iteration 25/1000 | Loss: 0.00001098
Iteration 26/1000 | Loss: 0.00001094
Iteration 27/1000 | Loss: 0.00001094
Iteration 28/1000 | Loss: 0.00001094
Iteration 29/1000 | Loss: 0.00001093
Iteration 30/1000 | Loss: 0.00001093
Iteration 31/1000 | Loss: 0.00001093
Iteration 32/1000 | Loss: 0.00001093
Iteration 33/1000 | Loss: 0.00001093
Iteration 34/1000 | Loss: 0.00001093
Iteration 35/1000 | Loss: 0.00001093
Iteration 36/1000 | Loss: 0.00001093
Iteration 37/1000 | Loss: 0.00001093
Iteration 38/1000 | Loss: 0.00001093
Iteration 39/1000 | Loss: 0.00001093
Iteration 40/1000 | Loss: 0.00001092
Iteration 41/1000 | Loss: 0.00001092
Iteration 42/1000 | Loss: 0.00001092
Iteration 43/1000 | Loss: 0.00010614
Iteration 44/1000 | Loss: 0.00001110
Iteration 45/1000 | Loss: 0.00001094
Iteration 46/1000 | Loss: 0.00001094
Iteration 47/1000 | Loss: 0.00001093
Iteration 48/1000 | Loss: 0.00001093
Iteration 49/1000 | Loss: 0.00001092
Iteration 50/1000 | Loss: 0.00001090
Iteration 51/1000 | Loss: 0.00006119
Iteration 52/1000 | Loss: 0.00001369
Iteration 53/1000 | Loss: 0.00001224
Iteration 54/1000 | Loss: 0.00001100
Iteration 55/1000 | Loss: 0.00001100
Iteration 56/1000 | Loss: 0.00001100
Iteration 57/1000 | Loss: 0.00001099
Iteration 58/1000 | Loss: 0.00001099
Iteration 59/1000 | Loss: 0.00001099
Iteration 60/1000 | Loss: 0.00001099
Iteration 61/1000 | Loss: 0.00001099
Iteration 62/1000 | Loss: 0.00001099
Iteration 63/1000 | Loss: 0.00001099
Iteration 64/1000 | Loss: 0.00001099
Iteration 65/1000 | Loss: 0.00001098
Iteration 66/1000 | Loss: 0.00001453
Iteration 67/1000 | Loss: 0.00001097
Iteration 68/1000 | Loss: 0.00001094
Iteration 69/1000 | Loss: 0.00001094
Iteration 70/1000 | Loss: 0.00001094
Iteration 71/1000 | Loss: 0.00001094
Iteration 72/1000 | Loss: 0.00001093
Iteration 73/1000 | Loss: 0.00001093
Iteration 74/1000 | Loss: 0.00001093
Iteration 75/1000 | Loss: 0.00001093
Iteration 76/1000 | Loss: 0.00001092
Iteration 77/1000 | Loss: 0.00001092
Iteration 78/1000 | Loss: 0.00001092
Iteration 79/1000 | Loss: 0.00001092
Iteration 80/1000 | Loss: 0.00001092
Iteration 81/1000 | Loss: 0.00001091
Iteration 82/1000 | Loss: 0.00001091
Iteration 83/1000 | Loss: 0.00001091
Iteration 84/1000 | Loss: 0.00001090
Iteration 85/1000 | Loss: 0.00001090
Iteration 86/1000 | Loss: 0.00001090
Iteration 87/1000 | Loss: 0.00001090
Iteration 88/1000 | Loss: 0.00001090
Iteration 89/1000 | Loss: 0.00001090
Iteration 90/1000 | Loss: 0.00001090
Iteration 91/1000 | Loss: 0.00001090
Iteration 92/1000 | Loss: 0.00001090
Iteration 93/1000 | Loss: 0.00001090
Iteration 94/1000 | Loss: 0.00001090
Iteration 95/1000 | Loss: 0.00001089
Iteration 96/1000 | Loss: 0.00001089
Iteration 97/1000 | Loss: 0.00001089
Iteration 98/1000 | Loss: 0.00001089
Iteration 99/1000 | Loss: 0.00001089
Iteration 100/1000 | Loss: 0.00001088
Iteration 101/1000 | Loss: 0.00001087
Iteration 102/1000 | Loss: 0.00001087
Iteration 103/1000 | Loss: 0.00001086
Iteration 104/1000 | Loss: 0.00001086
Iteration 105/1000 | Loss: 0.00001085
Iteration 106/1000 | Loss: 0.00001085
Iteration 107/1000 | Loss: 0.00001085
Iteration 108/1000 | Loss: 0.00001085
Iteration 109/1000 | Loss: 0.00001084
Iteration 110/1000 | Loss: 0.00001084
Iteration 111/1000 | Loss: 0.00001084
Iteration 112/1000 | Loss: 0.00001083
Iteration 113/1000 | Loss: 0.00001083
Iteration 114/1000 | Loss: 0.00001083
Iteration 115/1000 | Loss: 0.00001083
Iteration 116/1000 | Loss: 0.00001082
Iteration 117/1000 | Loss: 0.00001082
Iteration 118/1000 | Loss: 0.00001082
Iteration 119/1000 | Loss: 0.00001082
Iteration 120/1000 | Loss: 0.00001082
Iteration 121/1000 | Loss: 0.00001082
Iteration 122/1000 | Loss: 0.00001082
Iteration 123/1000 | Loss: 0.00001082
Iteration 124/1000 | Loss: 0.00001082
Iteration 125/1000 | Loss: 0.00001082
Iteration 126/1000 | Loss: 0.00001082
Iteration 127/1000 | Loss: 0.00001081
Iteration 128/1000 | Loss: 0.00001081
Iteration 129/1000 | Loss: 0.00001081
Iteration 130/1000 | Loss: 0.00001081
Iteration 131/1000 | Loss: 0.00001081
Iteration 132/1000 | Loss: 0.00001081
Iteration 133/1000 | Loss: 0.00001081
Iteration 134/1000 | Loss: 0.00001081
Iteration 135/1000 | Loss: 0.00001081
Iteration 136/1000 | Loss: 0.00001081
Iteration 137/1000 | Loss: 0.00001081
Iteration 138/1000 | Loss: 0.00001081
Iteration 139/1000 | Loss: 0.00001081
Iteration 140/1000 | Loss: 0.00001081
Iteration 141/1000 | Loss: 0.00001081
Iteration 142/1000 | Loss: 0.00001081
Iteration 143/1000 | Loss: 0.00001081
Iteration 144/1000 | Loss: 0.00001081
Iteration 145/1000 | Loss: 0.00001081
Iteration 146/1000 | Loss: 0.00001081
Iteration 147/1000 | Loss: 0.00001081
Iteration 148/1000 | Loss: 0.00001081
Iteration 149/1000 | Loss: 0.00001081
Iteration 150/1000 | Loss: 0.00001081
Iteration 151/1000 | Loss: 0.00001081
Iteration 152/1000 | Loss: 0.00001081
Iteration 153/1000 | Loss: 0.00001081
Iteration 154/1000 | Loss: 0.00001081
Iteration 155/1000 | Loss: 0.00001081
Iteration 156/1000 | Loss: 0.00001081
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.0811128049681429e-05, 1.0811128049681429e-05, 1.0811128049681429e-05, 1.0811128049681429e-05, 1.0811128049681429e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0811128049681429e-05

Optimization complete. Final v2v error: 2.8154962062835693 mm

Highest mean error: 3.2532033920288086 mm for frame 51

Lowest mean error: 2.4599199295043945 mm for frame 94

Saving results

Total time: 90.23744773864746
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00558894
Iteration 2/25 | Loss: 0.00108426
Iteration 3/25 | Loss: 0.00072823
Iteration 4/25 | Loss: 0.00067938
Iteration 5/25 | Loss: 0.00066578
Iteration 6/25 | Loss: 0.00066253
Iteration 7/25 | Loss: 0.00066192
Iteration 8/25 | Loss: 0.00066191
Iteration 9/25 | Loss: 0.00066192
Iteration 10/25 | Loss: 0.00066192
Iteration 11/25 | Loss: 0.00066192
Iteration 12/25 | Loss: 0.00066192
Iteration 13/25 | Loss: 0.00066192
Iteration 14/25 | Loss: 0.00066192
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0006619150517508388, 0.0006619150517508388, 0.0006619150517508388, 0.0006619150517508388, 0.0006619150517508388]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006619150517508388

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20666325
Iteration 2/25 | Loss: 0.00024295
Iteration 3/25 | Loss: 0.00024293
Iteration 4/25 | Loss: 0.00024293
Iteration 5/25 | Loss: 0.00024293
Iteration 6/25 | Loss: 0.00024293
Iteration 7/25 | Loss: 0.00024293
Iteration 8/25 | Loss: 0.00024293
Iteration 9/25 | Loss: 0.00024293
Iteration 10/25 | Loss: 0.00024293
Iteration 11/25 | Loss: 0.00024293
Iteration 12/25 | Loss: 0.00024293
Iteration 13/25 | Loss: 0.00024292
Iteration 14/25 | Loss: 0.00024292
Iteration 15/25 | Loss: 0.00024292
Iteration 16/25 | Loss: 0.00024292
Iteration 17/25 | Loss: 0.00024292
Iteration 18/25 | Loss: 0.00024292
Iteration 19/25 | Loss: 0.00024292
Iteration 20/25 | Loss: 0.00024292
Iteration 21/25 | Loss: 0.00024292
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00024292489979416132, 0.00024292489979416132, 0.00024292489979416132, 0.00024292489979416132, 0.00024292489979416132]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00024292489979416132

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024292
Iteration 2/1000 | Loss: 0.00002756
Iteration 3/1000 | Loss: 0.00002185
Iteration 4/1000 | Loss: 0.00002022
Iteration 5/1000 | Loss: 0.00001929
Iteration 6/1000 | Loss: 0.00001871
Iteration 7/1000 | Loss: 0.00001813
Iteration 8/1000 | Loss: 0.00001778
Iteration 9/1000 | Loss: 0.00001754
Iteration 10/1000 | Loss: 0.00001738
Iteration 11/1000 | Loss: 0.00001722
Iteration 12/1000 | Loss: 0.00001721
Iteration 13/1000 | Loss: 0.00001720
Iteration 14/1000 | Loss: 0.00001719
Iteration 15/1000 | Loss: 0.00001717
Iteration 16/1000 | Loss: 0.00001716
Iteration 17/1000 | Loss: 0.00001715
Iteration 18/1000 | Loss: 0.00001715
Iteration 19/1000 | Loss: 0.00001711
Iteration 20/1000 | Loss: 0.00001711
Iteration 21/1000 | Loss: 0.00001711
Iteration 22/1000 | Loss: 0.00001710
Iteration 23/1000 | Loss: 0.00001710
Iteration 24/1000 | Loss: 0.00001710
Iteration 25/1000 | Loss: 0.00001710
Iteration 26/1000 | Loss: 0.00001709
Iteration 27/1000 | Loss: 0.00001708
Iteration 28/1000 | Loss: 0.00001708
Iteration 29/1000 | Loss: 0.00001707
Iteration 30/1000 | Loss: 0.00001707
Iteration 31/1000 | Loss: 0.00001707
Iteration 32/1000 | Loss: 0.00001707
Iteration 33/1000 | Loss: 0.00001707
Iteration 34/1000 | Loss: 0.00001707
Iteration 35/1000 | Loss: 0.00001707
Iteration 36/1000 | Loss: 0.00001707
Iteration 37/1000 | Loss: 0.00001706
Iteration 38/1000 | Loss: 0.00001706
Iteration 39/1000 | Loss: 0.00001706
Iteration 40/1000 | Loss: 0.00001706
Iteration 41/1000 | Loss: 0.00001706
Iteration 42/1000 | Loss: 0.00001706
Iteration 43/1000 | Loss: 0.00001706
Iteration 44/1000 | Loss: 0.00001706
Iteration 45/1000 | Loss: 0.00001706
Iteration 46/1000 | Loss: 0.00001705
Iteration 47/1000 | Loss: 0.00001705
Iteration 48/1000 | Loss: 0.00001705
Iteration 49/1000 | Loss: 0.00001705
Iteration 50/1000 | Loss: 0.00001705
Iteration 51/1000 | Loss: 0.00001705
Iteration 52/1000 | Loss: 0.00001705
Iteration 53/1000 | Loss: 0.00001705
Iteration 54/1000 | Loss: 0.00001705
Iteration 55/1000 | Loss: 0.00001705
Iteration 56/1000 | Loss: 0.00001705
Iteration 57/1000 | Loss: 0.00001705
Iteration 58/1000 | Loss: 0.00001705
Iteration 59/1000 | Loss: 0.00001705
Iteration 60/1000 | Loss: 0.00001705
Iteration 61/1000 | Loss: 0.00001705
Iteration 62/1000 | Loss: 0.00001705
Iteration 63/1000 | Loss: 0.00001705
Iteration 64/1000 | Loss: 0.00001705
Iteration 65/1000 | Loss: 0.00001705
Iteration 66/1000 | Loss: 0.00001705
Iteration 67/1000 | Loss: 0.00001705
Iteration 68/1000 | Loss: 0.00001705
Iteration 69/1000 | Loss: 0.00001705
Iteration 70/1000 | Loss: 0.00001705
Iteration 71/1000 | Loss: 0.00001705
Iteration 72/1000 | Loss: 0.00001705
Iteration 73/1000 | Loss: 0.00001705
Iteration 74/1000 | Loss: 0.00001705
Iteration 75/1000 | Loss: 0.00001705
Iteration 76/1000 | Loss: 0.00001705
Iteration 77/1000 | Loss: 0.00001705
Iteration 78/1000 | Loss: 0.00001705
Iteration 79/1000 | Loss: 0.00001705
Iteration 80/1000 | Loss: 0.00001705
Iteration 81/1000 | Loss: 0.00001705
Iteration 82/1000 | Loss: 0.00001705
Iteration 83/1000 | Loss: 0.00001705
Iteration 84/1000 | Loss: 0.00001705
Iteration 85/1000 | Loss: 0.00001705
Iteration 86/1000 | Loss: 0.00001705
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 86. Stopping optimization.
Last 5 losses: [1.7054451745934784e-05, 1.7054451745934784e-05, 1.7054451745934784e-05, 1.7054451745934784e-05, 1.7054451745934784e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7054451745934784e-05

Optimization complete. Final v2v error: 3.5475540161132812 mm

Highest mean error: 3.891472339630127 mm for frame 50

Lowest mean error: 3.2181570529937744 mm for frame 76

Saving results

Total time: 33.483357667922974
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01075351
Iteration 2/25 | Loss: 0.01075351
Iteration 3/25 | Loss: 0.01075351
Iteration 4/25 | Loss: 0.01075351
Iteration 5/25 | Loss: 0.01075351
Iteration 6/25 | Loss: 0.01075350
Iteration 7/25 | Loss: 0.01075350
Iteration 8/25 | Loss: 0.01075350
Iteration 9/25 | Loss: 0.01075350
Iteration 10/25 | Loss: 0.01075350
Iteration 11/25 | Loss: 0.01075350
Iteration 12/25 | Loss: 0.01075349
Iteration 13/25 | Loss: 0.01075349
Iteration 14/25 | Loss: 0.01075349
Iteration 15/25 | Loss: 0.01075349
Iteration 16/25 | Loss: 0.01075349
Iteration 17/25 | Loss: 0.01075349
Iteration 18/25 | Loss: 0.01075349
Iteration 19/25 | Loss: 0.01075348
Iteration 20/25 | Loss: 0.01075348
Iteration 21/25 | Loss: 0.01075348
Iteration 22/25 | Loss: 0.01075348
Iteration 23/25 | Loss: 0.01075348
Iteration 24/25 | Loss: 0.01075347
Iteration 25/25 | Loss: 0.01075347

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.01146483
Iteration 2/25 | Loss: 0.08916073
Iteration 3/25 | Loss: 0.08414102
Iteration 4/25 | Loss: 0.08233839
Iteration 5/25 | Loss: 0.08246289
Iteration 6/25 | Loss: 0.08233829
Iteration 7/25 | Loss: 0.08233827
Iteration 8/25 | Loss: 0.08233825
Iteration 9/25 | Loss: 0.08233825
Iteration 10/25 | Loss: 0.08233825
Iteration 11/25 | Loss: 0.08233824
Iteration 12/25 | Loss: 0.08233825
Iteration 13/25 | Loss: 0.08233825
Iteration 14/25 | Loss: 0.08233823
Iteration 15/25 | Loss: 0.08233823
Iteration 16/25 | Loss: 0.08233823
Iteration 17/25 | Loss: 0.08233823
Iteration 18/25 | Loss: 0.08233824
Iteration 19/25 | Loss: 0.08233824
Iteration 20/25 | Loss: 0.08233824
Iteration 21/25 | Loss: 0.08233823
Iteration 22/25 | Loss: 0.08233823
Iteration 23/25 | Loss: 0.08233823
Iteration 24/25 | Loss: 0.08233823
Iteration 25/25 | Loss: 0.08233823

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08233823
Iteration 2/1000 | Loss: 0.00374619
Iteration 3/1000 | Loss: 0.00230827
Iteration 4/1000 | Loss: 0.00050249
Iteration 5/1000 | Loss: 0.00037340
Iteration 6/1000 | Loss: 0.00092579
Iteration 7/1000 | Loss: 0.00042616
Iteration 8/1000 | Loss: 0.00010127
Iteration 9/1000 | Loss: 0.00025808
Iteration 10/1000 | Loss: 0.00066717
Iteration 11/1000 | Loss: 0.00007808
Iteration 12/1000 | Loss: 0.00020893
Iteration 13/1000 | Loss: 0.00018965
Iteration 14/1000 | Loss: 0.00010614
Iteration 15/1000 | Loss: 0.00003928
Iteration 16/1000 | Loss: 0.00003561
Iteration 17/1000 | Loss: 0.00003333
Iteration 18/1000 | Loss: 0.00003116
Iteration 19/1000 | Loss: 0.00004286
Iteration 20/1000 | Loss: 0.00012556
Iteration 21/1000 | Loss: 0.00053333
Iteration 22/1000 | Loss: 0.00004336
Iteration 23/1000 | Loss: 0.00006202
Iteration 24/1000 | Loss: 0.00002539
Iteration 25/1000 | Loss: 0.00002521
Iteration 26/1000 | Loss: 0.00007498
Iteration 27/1000 | Loss: 0.00002334
Iteration 28/1000 | Loss: 0.00002212
Iteration 29/1000 | Loss: 0.00002123
Iteration 30/1000 | Loss: 0.00009303
Iteration 31/1000 | Loss: 0.00002025
Iteration 32/1000 | Loss: 0.00001965
Iteration 33/1000 | Loss: 0.00001914
Iteration 34/1000 | Loss: 0.00001856
Iteration 35/1000 | Loss: 0.00001818
Iteration 36/1000 | Loss: 0.00001795
Iteration 37/1000 | Loss: 0.00001778
Iteration 38/1000 | Loss: 0.00001774
Iteration 39/1000 | Loss: 0.00001773
Iteration 40/1000 | Loss: 0.00001772
Iteration 41/1000 | Loss: 0.00004998
Iteration 42/1000 | Loss: 0.00002208
Iteration 43/1000 | Loss: 0.00001757
Iteration 44/1000 | Loss: 0.00002859
Iteration 45/1000 | Loss: 0.00001754
Iteration 46/1000 | Loss: 0.00001754
Iteration 47/1000 | Loss: 0.00001754
Iteration 48/1000 | Loss: 0.00001754
Iteration 49/1000 | Loss: 0.00001754
Iteration 50/1000 | Loss: 0.00001754
Iteration 51/1000 | Loss: 0.00001753
Iteration 52/1000 | Loss: 0.00001753
Iteration 53/1000 | Loss: 0.00001753
Iteration 54/1000 | Loss: 0.00001752
Iteration 55/1000 | Loss: 0.00001752
Iteration 56/1000 | Loss: 0.00001751
Iteration 57/1000 | Loss: 0.00001751
Iteration 58/1000 | Loss: 0.00001750
Iteration 59/1000 | Loss: 0.00001750
Iteration 60/1000 | Loss: 0.00001750
Iteration 61/1000 | Loss: 0.00001749
Iteration 62/1000 | Loss: 0.00001749
Iteration 63/1000 | Loss: 0.00001748
Iteration 64/1000 | Loss: 0.00001748
Iteration 65/1000 | Loss: 0.00001748
Iteration 66/1000 | Loss: 0.00001747
Iteration 67/1000 | Loss: 0.00001747
Iteration 68/1000 | Loss: 0.00001746
Iteration 69/1000 | Loss: 0.00001746
Iteration 70/1000 | Loss: 0.00001745
Iteration 71/1000 | Loss: 0.00001744
Iteration 72/1000 | Loss: 0.00001744
Iteration 73/1000 | Loss: 0.00001744
Iteration 74/1000 | Loss: 0.00001743
Iteration 75/1000 | Loss: 0.00001743
Iteration 76/1000 | Loss: 0.00001741
Iteration 77/1000 | Loss: 0.00001741
Iteration 78/1000 | Loss: 0.00001740
Iteration 79/1000 | Loss: 0.00001740
Iteration 80/1000 | Loss: 0.00001740
Iteration 81/1000 | Loss: 0.00001739
Iteration 82/1000 | Loss: 0.00001739
Iteration 83/1000 | Loss: 0.00001739
Iteration 84/1000 | Loss: 0.00001738
Iteration 85/1000 | Loss: 0.00001738
Iteration 86/1000 | Loss: 0.00001738
Iteration 87/1000 | Loss: 0.00001738
Iteration 88/1000 | Loss: 0.00001738
Iteration 89/1000 | Loss: 0.00001738
Iteration 90/1000 | Loss: 0.00001738
Iteration 91/1000 | Loss: 0.00001738
Iteration 92/1000 | Loss: 0.00001738
Iteration 93/1000 | Loss: 0.00001738
Iteration 94/1000 | Loss: 0.00001737
Iteration 95/1000 | Loss: 0.00001737
Iteration 96/1000 | Loss: 0.00001737
Iteration 97/1000 | Loss: 0.00001736
Iteration 98/1000 | Loss: 0.00001736
Iteration 99/1000 | Loss: 0.00001736
Iteration 100/1000 | Loss: 0.00001735
Iteration 101/1000 | Loss: 0.00001735
Iteration 102/1000 | Loss: 0.00001735
Iteration 103/1000 | Loss: 0.00001734
Iteration 104/1000 | Loss: 0.00004821
Iteration 105/1000 | Loss: 0.00001742
Iteration 106/1000 | Loss: 0.00001765
Iteration 107/1000 | Loss: 0.00001765
Iteration 108/1000 | Loss: 0.00001739
Iteration 109/1000 | Loss: 0.00001756
Iteration 110/1000 | Loss: 0.00001732
Iteration 111/1000 | Loss: 0.00001732
Iteration 112/1000 | Loss: 0.00001731
Iteration 113/1000 | Loss: 0.00001731
Iteration 114/1000 | Loss: 0.00001731
Iteration 115/1000 | Loss: 0.00001731
Iteration 116/1000 | Loss: 0.00001752
Iteration 117/1000 | Loss: 0.00001752
Iteration 118/1000 | Loss: 0.00001751
Iteration 119/1000 | Loss: 0.00001751
Iteration 120/1000 | Loss: 0.00001751
Iteration 121/1000 | Loss: 0.00001750
Iteration 122/1000 | Loss: 0.00001739
Iteration 123/1000 | Loss: 0.00001738
Iteration 124/1000 | Loss: 0.00001737
Iteration 125/1000 | Loss: 0.00001736
Iteration 126/1000 | Loss: 0.00001736
Iteration 127/1000 | Loss: 0.00001734
Iteration 128/1000 | Loss: 0.00001734
Iteration 129/1000 | Loss: 0.00001734
Iteration 130/1000 | Loss: 0.00001733
Iteration 131/1000 | Loss: 0.00001733
Iteration 132/1000 | Loss: 0.00001733
Iteration 133/1000 | Loss: 0.00001733
Iteration 134/1000 | Loss: 0.00001732
Iteration 135/1000 | Loss: 0.00001732
Iteration 136/1000 | Loss: 0.00001732
Iteration 137/1000 | Loss: 0.00001732
Iteration 138/1000 | Loss: 0.00001732
Iteration 139/1000 | Loss: 0.00001731
Iteration 140/1000 | Loss: 0.00001731
Iteration 141/1000 | Loss: 0.00001731
Iteration 142/1000 | Loss: 0.00001730
Iteration 143/1000 | Loss: 0.00001730
Iteration 144/1000 | Loss: 0.00001730
Iteration 145/1000 | Loss: 0.00001730
Iteration 146/1000 | Loss: 0.00001730
Iteration 147/1000 | Loss: 0.00001730
Iteration 148/1000 | Loss: 0.00001730
Iteration 149/1000 | Loss: 0.00001730
Iteration 150/1000 | Loss: 0.00001730
Iteration 151/1000 | Loss: 0.00005553
Iteration 152/1000 | Loss: 0.00005092
Iteration 153/1000 | Loss: 0.00002123
Iteration 154/1000 | Loss: 0.00001767
Iteration 155/1000 | Loss: 0.00001748
Iteration 156/1000 | Loss: 0.00001746
Iteration 157/1000 | Loss: 0.00001744
Iteration 158/1000 | Loss: 0.00001731
Iteration 159/1000 | Loss: 0.00001731
Iteration 160/1000 | Loss: 0.00001724
Iteration 161/1000 | Loss: 0.00001721
Iteration 162/1000 | Loss: 0.00001721
Iteration 163/1000 | Loss: 0.00001721
Iteration 164/1000 | Loss: 0.00001721
Iteration 165/1000 | Loss: 0.00001721
Iteration 166/1000 | Loss: 0.00001721
Iteration 167/1000 | Loss: 0.00001721
Iteration 168/1000 | Loss: 0.00001721
Iteration 169/1000 | Loss: 0.00001721
Iteration 170/1000 | Loss: 0.00001721
Iteration 171/1000 | Loss: 0.00001720
Iteration 172/1000 | Loss: 0.00001720
Iteration 173/1000 | Loss: 0.00001720
Iteration 174/1000 | Loss: 0.00001720
Iteration 175/1000 | Loss: 0.00001720
Iteration 176/1000 | Loss: 0.00001720
Iteration 177/1000 | Loss: 0.00001720
Iteration 178/1000 | Loss: 0.00001720
Iteration 179/1000 | Loss: 0.00001720
Iteration 180/1000 | Loss: 0.00001720
Iteration 181/1000 | Loss: 0.00001720
Iteration 182/1000 | Loss: 0.00001719
Iteration 183/1000 | Loss: 0.00001719
Iteration 184/1000 | Loss: 0.00001719
Iteration 185/1000 | Loss: 0.00001719
Iteration 186/1000 | Loss: 0.00001719
Iteration 187/1000 | Loss: 0.00001719
Iteration 188/1000 | Loss: 0.00001719
Iteration 189/1000 | Loss: 0.00001719
Iteration 190/1000 | Loss: 0.00001719
Iteration 191/1000 | Loss: 0.00001719
Iteration 192/1000 | Loss: 0.00001718
Iteration 193/1000 | Loss: 0.00001718
Iteration 194/1000 | Loss: 0.00001718
Iteration 195/1000 | Loss: 0.00001718
Iteration 196/1000 | Loss: 0.00001718
Iteration 197/1000 | Loss: 0.00001718
Iteration 198/1000 | Loss: 0.00001718
Iteration 199/1000 | Loss: 0.00001718
Iteration 200/1000 | Loss: 0.00001718
Iteration 201/1000 | Loss: 0.00001718
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [1.718329804134555e-05, 1.718329804134555e-05, 1.718329804134555e-05, 1.718329804134555e-05, 1.718329804134555e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.718329804134555e-05

Optimization complete. Final v2v error: 3.437028646469116 mm

Highest mean error: 9.275884628295898 mm for frame 173

Lowest mean error: 3.1089565753936768 mm for frame 106

Saving results

Total time: 101.67610096931458
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01080403
Iteration 2/25 | Loss: 0.00385558
Iteration 3/25 | Loss: 0.00274372
Iteration 4/25 | Loss: 0.00233515
Iteration 5/25 | Loss: 0.00209644
Iteration 6/25 | Loss: 0.00184554
Iteration 7/25 | Loss: 0.00172464
Iteration 8/25 | Loss: 0.00166771
Iteration 9/25 | Loss: 0.00154568
Iteration 10/25 | Loss: 0.00151203
Iteration 11/25 | Loss: 0.00144659
Iteration 12/25 | Loss: 0.00145333
Iteration 13/25 | Loss: 0.00141959
Iteration 14/25 | Loss: 0.00138572
Iteration 15/25 | Loss: 0.00139802
Iteration 16/25 | Loss: 0.00137709
Iteration 17/25 | Loss: 0.00133970
Iteration 18/25 | Loss: 0.00131075
Iteration 19/25 | Loss: 0.00131153
Iteration 20/25 | Loss: 0.00129573
Iteration 21/25 | Loss: 0.00130839
Iteration 22/25 | Loss: 0.00129306
Iteration 23/25 | Loss: 0.00127797
Iteration 24/25 | Loss: 0.00128524
Iteration 25/25 | Loss: 0.00127734

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46775055
Iteration 2/25 | Loss: 0.00878465
Iteration 3/25 | Loss: 0.00419070
Iteration 4/25 | Loss: 0.00419070
Iteration 5/25 | Loss: 0.00419069
Iteration 6/25 | Loss: 0.00419069
Iteration 7/25 | Loss: 0.00419069
Iteration 8/25 | Loss: 0.00419069
Iteration 9/25 | Loss: 0.00419069
Iteration 10/25 | Loss: 0.00419069
Iteration 11/25 | Loss: 0.00419069
Iteration 12/25 | Loss: 0.00419069
Iteration 13/25 | Loss: 0.00419069
Iteration 14/25 | Loss: 0.00419069
Iteration 15/25 | Loss: 0.00419069
Iteration 16/25 | Loss: 0.00419069
Iteration 17/25 | Loss: 0.00419069
Iteration 18/25 | Loss: 0.00419069
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.004190690815448761, 0.004190690815448761, 0.004190690815448761, 0.004190690815448761, 0.004190690815448761]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004190690815448761

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00419069
Iteration 2/1000 | Loss: 0.00300947
Iteration 3/1000 | Loss: 0.00150275
Iteration 4/1000 | Loss: 0.00205497
Iteration 5/1000 | Loss: 0.00077688
Iteration 6/1000 | Loss: 0.00204210
Iteration 7/1000 | Loss: 0.00149378
Iteration 8/1000 | Loss: 0.00399149
Iteration 9/1000 | Loss: 0.00428535
Iteration 10/1000 | Loss: 0.00226889
Iteration 11/1000 | Loss: 0.00193074
Iteration 12/1000 | Loss: 0.00061832
Iteration 13/1000 | Loss: 0.00051890
Iteration 14/1000 | Loss: 0.00089040
Iteration 15/1000 | Loss: 0.00113078
Iteration 16/1000 | Loss: 0.00071758
Iteration 17/1000 | Loss: 0.00167327
Iteration 18/1000 | Loss: 0.00472431
Iteration 19/1000 | Loss: 0.00426434
Iteration 20/1000 | Loss: 0.00135317
Iteration 21/1000 | Loss: 0.00215531
Iteration 22/1000 | Loss: 0.00051903
Iteration 23/1000 | Loss: 0.00064259
Iteration 24/1000 | Loss: 0.00063965
Iteration 25/1000 | Loss: 0.00092934
Iteration 26/1000 | Loss: 0.00106596
Iteration 27/1000 | Loss: 0.00071375
Iteration 28/1000 | Loss: 0.00087875
Iteration 29/1000 | Loss: 0.00148932
Iteration 30/1000 | Loss: 0.00073552
Iteration 31/1000 | Loss: 0.00098702
Iteration 32/1000 | Loss: 0.00054374
Iteration 33/1000 | Loss: 0.00069945
Iteration 34/1000 | Loss: 0.00100931
Iteration 35/1000 | Loss: 0.00163374
Iteration 36/1000 | Loss: 0.00069704
Iteration 37/1000 | Loss: 0.00132037
Iteration 38/1000 | Loss: 0.00098981
Iteration 39/1000 | Loss: 0.00087954
Iteration 40/1000 | Loss: 0.00084738
Iteration 41/1000 | Loss: 0.00024675
Iteration 42/1000 | Loss: 0.00038926
Iteration 43/1000 | Loss: 0.00023795
Iteration 44/1000 | Loss: 0.00035862
Iteration 45/1000 | Loss: 0.00022266
Iteration 46/1000 | Loss: 0.00063151
Iteration 47/1000 | Loss: 0.00032572
Iteration 48/1000 | Loss: 0.00074980
Iteration 49/1000 | Loss: 0.00119338
Iteration 50/1000 | Loss: 0.00301922
Iteration 51/1000 | Loss: 0.00147994
Iteration 52/1000 | Loss: 0.00053017
Iteration 53/1000 | Loss: 0.00208971
Iteration 54/1000 | Loss: 0.00086944
Iteration 55/1000 | Loss: 0.00018870
Iteration 56/1000 | Loss: 0.00073279
Iteration 57/1000 | Loss: 0.00017084
Iteration 58/1000 | Loss: 0.00016372
Iteration 59/1000 | Loss: 0.00017444
Iteration 60/1000 | Loss: 0.00079716
Iteration 61/1000 | Loss: 0.00015354
Iteration 62/1000 | Loss: 0.00064239
Iteration 63/1000 | Loss: 0.00072617
Iteration 64/1000 | Loss: 0.00034893
Iteration 65/1000 | Loss: 0.00344942
Iteration 66/1000 | Loss: 0.01117146
Iteration 67/1000 | Loss: 0.00672775
Iteration 68/1000 | Loss: 0.00507333
Iteration 69/1000 | Loss: 0.00345105
Iteration 70/1000 | Loss: 0.00108048
Iteration 71/1000 | Loss: 0.00062369
Iteration 72/1000 | Loss: 0.00040856
Iteration 73/1000 | Loss: 0.00074538
Iteration 74/1000 | Loss: 0.00158260
Iteration 75/1000 | Loss: 0.00015939
Iteration 76/1000 | Loss: 0.00046131
Iteration 77/1000 | Loss: 0.00027155
Iteration 78/1000 | Loss: 0.00010043
Iteration 79/1000 | Loss: 0.00007241
Iteration 80/1000 | Loss: 0.00005975
Iteration 81/1000 | Loss: 0.00051039
Iteration 82/1000 | Loss: 0.00005057
Iteration 83/1000 | Loss: 0.00004616
Iteration 84/1000 | Loss: 0.00012649
Iteration 85/1000 | Loss: 0.00003979
Iteration 86/1000 | Loss: 0.00003732
Iteration 87/1000 | Loss: 0.00003553
Iteration 88/1000 | Loss: 0.00003401
Iteration 89/1000 | Loss: 0.00003324
Iteration 90/1000 | Loss: 0.00003229
Iteration 91/1000 | Loss: 0.00003174
Iteration 92/1000 | Loss: 0.00003137
Iteration 93/1000 | Loss: 0.00003105
Iteration 94/1000 | Loss: 0.00003084
Iteration 95/1000 | Loss: 0.00003066
Iteration 96/1000 | Loss: 0.00003059
Iteration 97/1000 | Loss: 0.00003058
Iteration 98/1000 | Loss: 0.00003058
Iteration 99/1000 | Loss: 0.00003052
Iteration 100/1000 | Loss: 0.00003046
Iteration 101/1000 | Loss: 0.00003046
Iteration 102/1000 | Loss: 0.00003045
Iteration 103/1000 | Loss: 0.00003031
Iteration 104/1000 | Loss: 0.00003030
Iteration 105/1000 | Loss: 0.00003029
Iteration 106/1000 | Loss: 0.00003028
Iteration 107/1000 | Loss: 0.00003023
Iteration 108/1000 | Loss: 0.00003011
Iteration 109/1000 | Loss: 0.00003492
Iteration 110/1000 | Loss: 0.00003492
Iteration 111/1000 | Loss: 0.00003318
Iteration 112/1000 | Loss: 0.00003484
Iteration 113/1000 | Loss: 0.00003257
Iteration 114/1000 | Loss: 0.00003400
Iteration 115/1000 | Loss: 0.00003038
Iteration 116/1000 | Loss: 0.00002969
Iteration 117/1000 | Loss: 0.00002947
Iteration 118/1000 | Loss: 0.00002942
Iteration 119/1000 | Loss: 0.00002936
Iteration 120/1000 | Loss: 0.00002913
Iteration 121/1000 | Loss: 0.00002901
Iteration 122/1000 | Loss: 0.00002900
Iteration 123/1000 | Loss: 0.00002900
Iteration 124/1000 | Loss: 0.00002895
Iteration 125/1000 | Loss: 0.00002892
Iteration 126/1000 | Loss: 0.00002892
Iteration 127/1000 | Loss: 0.00002892
Iteration 128/1000 | Loss: 0.00002891
Iteration 129/1000 | Loss: 0.00002891
Iteration 130/1000 | Loss: 0.00002890
Iteration 131/1000 | Loss: 0.00002890
Iteration 132/1000 | Loss: 0.00002889
Iteration 133/1000 | Loss: 0.00002889
Iteration 134/1000 | Loss: 0.00002889
Iteration 135/1000 | Loss: 0.00002888
Iteration 136/1000 | Loss: 0.00002888
Iteration 137/1000 | Loss: 0.00002888
Iteration 138/1000 | Loss: 0.00002887
Iteration 139/1000 | Loss: 0.00002887
Iteration 140/1000 | Loss: 0.00002887
Iteration 141/1000 | Loss: 0.00002886
Iteration 142/1000 | Loss: 0.00002886
Iteration 143/1000 | Loss: 0.00002886
Iteration 144/1000 | Loss: 0.00002885
Iteration 145/1000 | Loss: 0.00002885
Iteration 146/1000 | Loss: 0.00002885
Iteration 147/1000 | Loss: 0.00002885
Iteration 148/1000 | Loss: 0.00002885
Iteration 149/1000 | Loss: 0.00002885
Iteration 150/1000 | Loss: 0.00002885
Iteration 151/1000 | Loss: 0.00002884
Iteration 152/1000 | Loss: 0.00002884
Iteration 153/1000 | Loss: 0.00002884
Iteration 154/1000 | Loss: 0.00002884
Iteration 155/1000 | Loss: 0.00002884
Iteration 156/1000 | Loss: 0.00002884
Iteration 157/1000 | Loss: 0.00002883
Iteration 158/1000 | Loss: 0.00002883
Iteration 159/1000 | Loss: 0.00002883
Iteration 160/1000 | Loss: 0.00002883
Iteration 161/1000 | Loss: 0.00002883
Iteration 162/1000 | Loss: 0.00002883
Iteration 163/1000 | Loss: 0.00002883
Iteration 164/1000 | Loss: 0.00002883
Iteration 165/1000 | Loss: 0.00002883
Iteration 166/1000 | Loss: 0.00002883
Iteration 167/1000 | Loss: 0.00002882
Iteration 168/1000 | Loss: 0.00002882
Iteration 169/1000 | Loss: 0.00002882
Iteration 170/1000 | Loss: 0.00002882
Iteration 171/1000 | Loss: 0.00002882
Iteration 172/1000 | Loss: 0.00002882
Iteration 173/1000 | Loss: 0.00002882
Iteration 174/1000 | Loss: 0.00002882
Iteration 175/1000 | Loss: 0.00002882
Iteration 176/1000 | Loss: 0.00002882
Iteration 177/1000 | Loss: 0.00002881
Iteration 178/1000 | Loss: 0.00002881
Iteration 179/1000 | Loss: 0.00002881
Iteration 180/1000 | Loss: 0.00002881
Iteration 181/1000 | Loss: 0.00002880
Iteration 182/1000 | Loss: 0.00002880
Iteration 183/1000 | Loss: 0.00002880
Iteration 184/1000 | Loss: 0.00002880
Iteration 185/1000 | Loss: 0.00002880
Iteration 186/1000 | Loss: 0.00002879
Iteration 187/1000 | Loss: 0.00002879
Iteration 188/1000 | Loss: 0.00002879
Iteration 189/1000 | Loss: 0.00002879
Iteration 190/1000 | Loss: 0.00002879
Iteration 191/1000 | Loss: 0.00002879
Iteration 192/1000 | Loss: 0.00002879
Iteration 193/1000 | Loss: 0.00002879
Iteration 194/1000 | Loss: 0.00002879
Iteration 195/1000 | Loss: 0.00002879
Iteration 196/1000 | Loss: 0.00002879
Iteration 197/1000 | Loss: 0.00002879
Iteration 198/1000 | Loss: 0.00002879
Iteration 199/1000 | Loss: 0.00002879
Iteration 200/1000 | Loss: 0.00002879
Iteration 201/1000 | Loss: 0.00002879
Iteration 202/1000 | Loss: 0.00002879
Iteration 203/1000 | Loss: 0.00002879
Iteration 204/1000 | Loss: 0.00002879
Iteration 205/1000 | Loss: 0.00002879
Iteration 206/1000 | Loss: 0.00002879
Iteration 207/1000 | Loss: 0.00002879
Iteration 208/1000 | Loss: 0.00002879
Iteration 209/1000 | Loss: 0.00002879
Iteration 210/1000 | Loss: 0.00002879
Iteration 211/1000 | Loss: 0.00002879
Iteration 212/1000 | Loss: 0.00002879
Iteration 213/1000 | Loss: 0.00002879
Iteration 214/1000 | Loss: 0.00002879
Iteration 215/1000 | Loss: 0.00002879
Iteration 216/1000 | Loss: 0.00002879
Iteration 217/1000 | Loss: 0.00002879
Iteration 218/1000 | Loss: 0.00002879
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 218. Stopping optimization.
Last 5 losses: [2.8785218091798015e-05, 2.8785218091798015e-05, 2.8785218091798015e-05, 2.8785218091798015e-05, 2.8785218091798015e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8785218091798015e-05

Optimization complete. Final v2v error: 3.9386799335479736 mm

Highest mean error: 5.484239101409912 mm for frame 63

Lowest mean error: 3.3110783100128174 mm for frame 120

Saving results

Total time: 199.44037318229675
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00891695
Iteration 2/25 | Loss: 0.00077867
Iteration 3/25 | Loss: 0.00063888
Iteration 4/25 | Loss: 0.00059714
Iteration 5/25 | Loss: 0.00058795
Iteration 6/25 | Loss: 0.00058565
Iteration 7/25 | Loss: 0.00058543
Iteration 8/25 | Loss: 0.00058541
Iteration 9/25 | Loss: 0.00058541
Iteration 10/25 | Loss: 0.00058541
Iteration 11/25 | Loss: 0.00058541
Iteration 12/25 | Loss: 0.00058541
Iteration 13/25 | Loss: 0.00058541
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0005854112096130848, 0.0005854112096130848, 0.0005854112096130848, 0.0005854112096130848, 0.0005854112096130848]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005854112096130848

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39759314
Iteration 2/25 | Loss: 0.00017746
Iteration 3/25 | Loss: 0.00017736
Iteration 4/25 | Loss: 0.00017736
Iteration 5/25 | Loss: 0.00017735
Iteration 6/25 | Loss: 0.00017735
Iteration 7/25 | Loss: 0.00017735
Iteration 8/25 | Loss: 0.00017735
Iteration 9/25 | Loss: 0.00017735
Iteration 10/25 | Loss: 0.00017735
Iteration 11/25 | Loss: 0.00017735
Iteration 12/25 | Loss: 0.00017735
Iteration 13/25 | Loss: 0.00017735
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0001773536641849205, 0.0001773536641849205, 0.0001773536641849205, 0.0001773536641849205, 0.0001773536641849205]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0001773536641849205

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00017735
Iteration 2/1000 | Loss: 0.00001978
Iteration 3/1000 | Loss: 0.00001583
Iteration 4/1000 | Loss: 0.00001448
Iteration 5/1000 | Loss: 0.00001364
Iteration 6/1000 | Loss: 0.00001357
Iteration 7/1000 | Loss: 0.00001316
Iteration 8/1000 | Loss: 0.00001289
Iteration 9/1000 | Loss: 0.00001287
Iteration 10/1000 | Loss: 0.00001270
Iteration 11/1000 | Loss: 0.00001258
Iteration 12/1000 | Loss: 0.00001254
Iteration 13/1000 | Loss: 0.00001253
Iteration 14/1000 | Loss: 0.00001252
Iteration 15/1000 | Loss: 0.00001252
Iteration 16/1000 | Loss: 0.00001251
Iteration 17/1000 | Loss: 0.00001249
Iteration 18/1000 | Loss: 0.00001245
Iteration 19/1000 | Loss: 0.00001245
Iteration 20/1000 | Loss: 0.00001238
Iteration 21/1000 | Loss: 0.00001237
Iteration 22/1000 | Loss: 0.00001235
Iteration 23/1000 | Loss: 0.00001235
Iteration 24/1000 | Loss: 0.00001234
Iteration 25/1000 | Loss: 0.00001234
Iteration 26/1000 | Loss: 0.00001233
Iteration 27/1000 | Loss: 0.00001233
Iteration 28/1000 | Loss: 0.00001233
Iteration 29/1000 | Loss: 0.00001232
Iteration 30/1000 | Loss: 0.00001232
Iteration 31/1000 | Loss: 0.00001231
Iteration 32/1000 | Loss: 0.00001231
Iteration 33/1000 | Loss: 0.00001230
Iteration 34/1000 | Loss: 0.00001230
Iteration 35/1000 | Loss: 0.00001229
Iteration 36/1000 | Loss: 0.00001229
Iteration 37/1000 | Loss: 0.00001228
Iteration 38/1000 | Loss: 0.00001228
Iteration 39/1000 | Loss: 0.00001228
Iteration 40/1000 | Loss: 0.00001228
Iteration 41/1000 | Loss: 0.00001228
Iteration 42/1000 | Loss: 0.00001227
Iteration 43/1000 | Loss: 0.00001227
Iteration 44/1000 | Loss: 0.00001225
Iteration 45/1000 | Loss: 0.00001225
Iteration 46/1000 | Loss: 0.00001225
Iteration 47/1000 | Loss: 0.00001224
Iteration 48/1000 | Loss: 0.00001224
Iteration 49/1000 | Loss: 0.00001223
Iteration 50/1000 | Loss: 0.00001223
Iteration 51/1000 | Loss: 0.00001223
Iteration 52/1000 | Loss: 0.00001223
Iteration 53/1000 | Loss: 0.00001223
Iteration 54/1000 | Loss: 0.00001223
Iteration 55/1000 | Loss: 0.00001223
Iteration 56/1000 | Loss: 0.00001223
Iteration 57/1000 | Loss: 0.00001223
Iteration 58/1000 | Loss: 0.00001223
Iteration 59/1000 | Loss: 0.00001223
Iteration 60/1000 | Loss: 0.00001222
Iteration 61/1000 | Loss: 0.00001222
Iteration 62/1000 | Loss: 0.00001222
Iteration 63/1000 | Loss: 0.00001222
Iteration 64/1000 | Loss: 0.00001222
Iteration 65/1000 | Loss: 0.00001222
Iteration 66/1000 | Loss: 0.00001222
Iteration 67/1000 | Loss: 0.00001222
Iteration 68/1000 | Loss: 0.00001221
Iteration 69/1000 | Loss: 0.00001221
Iteration 70/1000 | Loss: 0.00001221
Iteration 71/1000 | Loss: 0.00001221
Iteration 72/1000 | Loss: 0.00001221
Iteration 73/1000 | Loss: 0.00001221
Iteration 74/1000 | Loss: 0.00001221
Iteration 75/1000 | Loss: 0.00001221
Iteration 76/1000 | Loss: 0.00001221
Iteration 77/1000 | Loss: 0.00001221
Iteration 78/1000 | Loss: 0.00001221
Iteration 79/1000 | Loss: 0.00001221
Iteration 80/1000 | Loss: 0.00001221
Iteration 81/1000 | Loss: 0.00001221
Iteration 82/1000 | Loss: 0.00001221
Iteration 83/1000 | Loss: 0.00001221
Iteration 84/1000 | Loss: 0.00001221
Iteration 85/1000 | Loss: 0.00001221
Iteration 86/1000 | Loss: 0.00001221
Iteration 87/1000 | Loss: 0.00001221
Iteration 88/1000 | Loss: 0.00001221
Iteration 89/1000 | Loss: 0.00001221
Iteration 90/1000 | Loss: 0.00001221
Iteration 91/1000 | Loss: 0.00001221
Iteration 92/1000 | Loss: 0.00001221
Iteration 93/1000 | Loss: 0.00001221
Iteration 94/1000 | Loss: 0.00001221
Iteration 95/1000 | Loss: 0.00001221
Iteration 96/1000 | Loss: 0.00001221
Iteration 97/1000 | Loss: 0.00001221
Iteration 98/1000 | Loss: 0.00001221
Iteration 99/1000 | Loss: 0.00001221
Iteration 100/1000 | Loss: 0.00001221
Iteration 101/1000 | Loss: 0.00001221
Iteration 102/1000 | Loss: 0.00001221
Iteration 103/1000 | Loss: 0.00001221
Iteration 104/1000 | Loss: 0.00001221
Iteration 105/1000 | Loss: 0.00001221
Iteration 106/1000 | Loss: 0.00001221
Iteration 107/1000 | Loss: 0.00001221
Iteration 108/1000 | Loss: 0.00001221
Iteration 109/1000 | Loss: 0.00001221
Iteration 110/1000 | Loss: 0.00001221
Iteration 111/1000 | Loss: 0.00001221
Iteration 112/1000 | Loss: 0.00001221
Iteration 113/1000 | Loss: 0.00001221
Iteration 114/1000 | Loss: 0.00001221
Iteration 115/1000 | Loss: 0.00001221
Iteration 116/1000 | Loss: 0.00001221
Iteration 117/1000 | Loss: 0.00001221
Iteration 118/1000 | Loss: 0.00001221
Iteration 119/1000 | Loss: 0.00001221
Iteration 120/1000 | Loss: 0.00001221
Iteration 121/1000 | Loss: 0.00001221
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [1.220819558511721e-05, 1.220819558511721e-05, 1.220819558511721e-05, 1.220819558511721e-05, 1.220819558511721e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.220819558511721e-05

Optimization complete. Final v2v error: 3.0283026695251465 mm

Highest mean error: 3.3242335319519043 mm for frame 205

Lowest mean error: 2.8797359466552734 mm for frame 1

Saving results

Total time: 33.44583010673523
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00469494
Iteration 2/25 | Loss: 0.00076342
Iteration 3/25 | Loss: 0.00063209
Iteration 4/25 | Loss: 0.00060453
Iteration 5/25 | Loss: 0.00059389
Iteration 6/25 | Loss: 0.00059204
Iteration 7/25 | Loss: 0.00059143
Iteration 8/25 | Loss: 0.00059137
Iteration 9/25 | Loss: 0.00059137
Iteration 10/25 | Loss: 0.00059137
Iteration 11/25 | Loss: 0.00059137
Iteration 12/25 | Loss: 0.00059137
Iteration 13/25 | Loss: 0.00059137
Iteration 14/25 | Loss: 0.00059137
Iteration 15/25 | Loss: 0.00059137
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.000591370218899101, 0.000591370218899101, 0.000591370218899101, 0.000591370218899101, 0.000591370218899101]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000591370218899101

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.60858917
Iteration 2/25 | Loss: 0.00021296
Iteration 3/25 | Loss: 0.00021296
Iteration 4/25 | Loss: 0.00021296
Iteration 5/25 | Loss: 0.00021295
Iteration 6/25 | Loss: 0.00021295
Iteration 7/25 | Loss: 0.00021295
Iteration 8/25 | Loss: 0.00021295
Iteration 9/25 | Loss: 0.00021295
Iteration 10/25 | Loss: 0.00021295
Iteration 11/25 | Loss: 0.00021295
Iteration 12/25 | Loss: 0.00021295
Iteration 13/25 | Loss: 0.00021295
Iteration 14/25 | Loss: 0.00021295
Iteration 15/25 | Loss: 0.00021295
Iteration 16/25 | Loss: 0.00021295
Iteration 17/25 | Loss: 0.00021295
Iteration 18/25 | Loss: 0.00021295
Iteration 19/25 | Loss: 0.00021295
Iteration 20/25 | Loss: 0.00021295
Iteration 21/25 | Loss: 0.00021295
Iteration 22/25 | Loss: 0.00021295
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00021295290207490325, 0.00021295290207490325, 0.00021295290207490325, 0.00021295290207490325, 0.00021295290207490325]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00021295290207490325

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00021295
Iteration 2/1000 | Loss: 0.00002896
Iteration 3/1000 | Loss: 0.00001865
Iteration 4/1000 | Loss: 0.00001631
Iteration 5/1000 | Loss: 0.00001526
Iteration 6/1000 | Loss: 0.00001479
Iteration 7/1000 | Loss: 0.00001439
Iteration 8/1000 | Loss: 0.00001423
Iteration 9/1000 | Loss: 0.00001401
Iteration 10/1000 | Loss: 0.00001399
Iteration 11/1000 | Loss: 0.00001389
Iteration 12/1000 | Loss: 0.00001389
Iteration 13/1000 | Loss: 0.00001382
Iteration 14/1000 | Loss: 0.00001382
Iteration 15/1000 | Loss: 0.00001378
Iteration 16/1000 | Loss: 0.00001377
Iteration 17/1000 | Loss: 0.00001377
Iteration 18/1000 | Loss: 0.00001376
Iteration 19/1000 | Loss: 0.00001374
Iteration 20/1000 | Loss: 0.00001373
Iteration 21/1000 | Loss: 0.00001372
Iteration 22/1000 | Loss: 0.00001372
Iteration 23/1000 | Loss: 0.00001372
Iteration 24/1000 | Loss: 0.00001371
Iteration 25/1000 | Loss: 0.00001371
Iteration 26/1000 | Loss: 0.00001370
Iteration 27/1000 | Loss: 0.00001368
Iteration 28/1000 | Loss: 0.00001368
Iteration 29/1000 | Loss: 0.00001368
Iteration 30/1000 | Loss: 0.00001368
Iteration 31/1000 | Loss: 0.00001368
Iteration 32/1000 | Loss: 0.00001368
Iteration 33/1000 | Loss: 0.00001368
Iteration 34/1000 | Loss: 0.00001367
Iteration 35/1000 | Loss: 0.00001367
Iteration 36/1000 | Loss: 0.00001367
Iteration 37/1000 | Loss: 0.00001366
Iteration 38/1000 | Loss: 0.00001366
Iteration 39/1000 | Loss: 0.00001365
Iteration 40/1000 | Loss: 0.00001365
Iteration 41/1000 | Loss: 0.00001364
Iteration 42/1000 | Loss: 0.00001364
Iteration 43/1000 | Loss: 0.00001364
Iteration 44/1000 | Loss: 0.00001364
Iteration 45/1000 | Loss: 0.00001364
Iteration 46/1000 | Loss: 0.00001363
Iteration 47/1000 | Loss: 0.00001363
Iteration 48/1000 | Loss: 0.00001363
Iteration 49/1000 | Loss: 0.00001362
Iteration 50/1000 | Loss: 0.00001362
Iteration 51/1000 | Loss: 0.00001362
Iteration 52/1000 | Loss: 0.00001362
Iteration 53/1000 | Loss: 0.00001361
Iteration 54/1000 | Loss: 0.00001361
Iteration 55/1000 | Loss: 0.00001361
Iteration 56/1000 | Loss: 0.00001361
Iteration 57/1000 | Loss: 0.00001361
Iteration 58/1000 | Loss: 0.00001361
Iteration 59/1000 | Loss: 0.00001361
Iteration 60/1000 | Loss: 0.00001361
Iteration 61/1000 | Loss: 0.00001361
Iteration 62/1000 | Loss: 0.00001361
Iteration 63/1000 | Loss: 0.00001361
Iteration 64/1000 | Loss: 0.00001361
Iteration 65/1000 | Loss: 0.00001361
Iteration 66/1000 | Loss: 0.00001360
Iteration 67/1000 | Loss: 0.00001360
Iteration 68/1000 | Loss: 0.00001360
Iteration 69/1000 | Loss: 0.00001360
Iteration 70/1000 | Loss: 0.00001359
Iteration 71/1000 | Loss: 0.00001359
Iteration 72/1000 | Loss: 0.00001359
Iteration 73/1000 | Loss: 0.00001359
Iteration 74/1000 | Loss: 0.00001359
Iteration 75/1000 | Loss: 0.00001358
Iteration 76/1000 | Loss: 0.00001358
Iteration 77/1000 | Loss: 0.00001358
Iteration 78/1000 | Loss: 0.00001358
Iteration 79/1000 | Loss: 0.00001358
Iteration 80/1000 | Loss: 0.00001358
Iteration 81/1000 | Loss: 0.00001358
Iteration 82/1000 | Loss: 0.00001358
Iteration 83/1000 | Loss: 0.00001358
Iteration 84/1000 | Loss: 0.00001358
Iteration 85/1000 | Loss: 0.00001358
Iteration 86/1000 | Loss: 0.00001358
Iteration 87/1000 | Loss: 0.00001358
Iteration 88/1000 | Loss: 0.00001358
Iteration 89/1000 | Loss: 0.00001358
Iteration 90/1000 | Loss: 0.00001358
Iteration 91/1000 | Loss: 0.00001358
Iteration 92/1000 | Loss: 0.00001358
Iteration 93/1000 | Loss: 0.00001358
Iteration 94/1000 | Loss: 0.00001358
Iteration 95/1000 | Loss: 0.00001358
Iteration 96/1000 | Loss: 0.00001358
Iteration 97/1000 | Loss: 0.00001358
Iteration 98/1000 | Loss: 0.00001358
Iteration 99/1000 | Loss: 0.00001358
Iteration 100/1000 | Loss: 0.00001358
Iteration 101/1000 | Loss: 0.00001358
Iteration 102/1000 | Loss: 0.00001358
Iteration 103/1000 | Loss: 0.00001358
Iteration 104/1000 | Loss: 0.00001358
Iteration 105/1000 | Loss: 0.00001358
Iteration 106/1000 | Loss: 0.00001358
Iteration 107/1000 | Loss: 0.00001358
Iteration 108/1000 | Loss: 0.00001358
Iteration 109/1000 | Loss: 0.00001358
Iteration 110/1000 | Loss: 0.00001358
Iteration 111/1000 | Loss: 0.00001358
Iteration 112/1000 | Loss: 0.00001358
Iteration 113/1000 | Loss: 0.00001358
Iteration 114/1000 | Loss: 0.00001358
Iteration 115/1000 | Loss: 0.00001358
Iteration 116/1000 | Loss: 0.00001358
Iteration 117/1000 | Loss: 0.00001358
Iteration 118/1000 | Loss: 0.00001358
Iteration 119/1000 | Loss: 0.00001358
Iteration 120/1000 | Loss: 0.00001358
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [1.3578927791968454e-05, 1.3578927791968454e-05, 1.3578927791968454e-05, 1.3578927791968454e-05, 1.3578927791968454e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3578927791968454e-05

Optimization complete. Final v2v error: 3.0784912109375 mm

Highest mean error: 3.7005481719970703 mm for frame 39

Lowest mean error: 2.8223390579223633 mm for frame 97

Saving results

Total time: 30.572154760360718
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00395129
Iteration 2/25 | Loss: 0.00080347
Iteration 3/25 | Loss: 0.00061835
Iteration 4/25 | Loss: 0.00060170
Iteration 5/25 | Loss: 0.00059315
Iteration 6/25 | Loss: 0.00059070
Iteration 7/25 | Loss: 0.00059023
Iteration 8/25 | Loss: 0.00059023
Iteration 9/25 | Loss: 0.00059023
Iteration 10/25 | Loss: 0.00059023
Iteration 11/25 | Loss: 0.00059023
Iteration 12/25 | Loss: 0.00059023
Iteration 13/25 | Loss: 0.00059023
Iteration 14/25 | Loss: 0.00059023
Iteration 15/25 | Loss: 0.00059023
Iteration 16/25 | Loss: 0.00059023
Iteration 17/25 | Loss: 0.00059023
Iteration 18/25 | Loss: 0.00059023
Iteration 19/25 | Loss: 0.00059023
Iteration 20/25 | Loss: 0.00059023
Iteration 21/25 | Loss: 0.00059023
Iteration 22/25 | Loss: 0.00059023
Iteration 23/25 | Loss: 0.00059007
Iteration 24/25 | Loss: 0.00059007
Iteration 25/25 | Loss: 0.00059007

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64098227
Iteration 2/25 | Loss: 0.00025514
Iteration 3/25 | Loss: 0.00025513
Iteration 4/25 | Loss: 0.00025513
Iteration 5/25 | Loss: 0.00025513
Iteration 6/25 | Loss: 0.00025513
Iteration 7/25 | Loss: 0.00025513
Iteration 8/25 | Loss: 0.00025513
Iteration 9/25 | Loss: 0.00025513
Iteration 10/25 | Loss: 0.00025513
Iteration 11/25 | Loss: 0.00025513
Iteration 12/25 | Loss: 0.00025513
Iteration 13/25 | Loss: 0.00025513
Iteration 14/25 | Loss: 0.00025513
Iteration 15/25 | Loss: 0.00025513
Iteration 16/25 | Loss: 0.00025513
Iteration 17/25 | Loss: 0.00025513
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00025512874708510935, 0.00025512874708510935, 0.00025512874708510935, 0.00025512874708510935, 0.00025512874708510935]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00025512874708510935

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025513
Iteration 2/1000 | Loss: 0.00002017
Iteration 3/1000 | Loss: 0.00001375
Iteration 4/1000 | Loss: 0.00001266
Iteration 5/1000 | Loss: 0.00001208
Iteration 6/1000 | Loss: 0.00001173
Iteration 7/1000 | Loss: 0.00001149
Iteration 8/1000 | Loss: 0.00001131
Iteration 9/1000 | Loss: 0.00001118
Iteration 10/1000 | Loss: 0.00001118
Iteration 11/1000 | Loss: 0.00001108
Iteration 12/1000 | Loss: 0.00001104
Iteration 13/1000 | Loss: 0.00001100
Iteration 14/1000 | Loss: 0.00001098
Iteration 15/1000 | Loss: 0.00001098
Iteration 16/1000 | Loss: 0.00001097
Iteration 17/1000 | Loss: 0.00001097
Iteration 18/1000 | Loss: 0.00001097
Iteration 19/1000 | Loss: 0.00001097
Iteration 20/1000 | Loss: 0.00001097
Iteration 21/1000 | Loss: 0.00001097
Iteration 22/1000 | Loss: 0.00001095
Iteration 23/1000 | Loss: 0.00001095
Iteration 24/1000 | Loss: 0.00001093
Iteration 25/1000 | Loss: 0.00001092
Iteration 26/1000 | Loss: 0.00001092
Iteration 27/1000 | Loss: 0.00001089
Iteration 28/1000 | Loss: 0.00001088
Iteration 29/1000 | Loss: 0.00001088
Iteration 30/1000 | Loss: 0.00001088
Iteration 31/1000 | Loss: 0.00001088
Iteration 32/1000 | Loss: 0.00001087
Iteration 33/1000 | Loss: 0.00001087
Iteration 34/1000 | Loss: 0.00001087
Iteration 35/1000 | Loss: 0.00001087
Iteration 36/1000 | Loss: 0.00001087
Iteration 37/1000 | Loss: 0.00001087
Iteration 38/1000 | Loss: 0.00001087
Iteration 39/1000 | Loss: 0.00001086
Iteration 40/1000 | Loss: 0.00001086
Iteration 41/1000 | Loss: 0.00001086
Iteration 42/1000 | Loss: 0.00001086
Iteration 43/1000 | Loss: 0.00001086
Iteration 44/1000 | Loss: 0.00001085
Iteration 45/1000 | Loss: 0.00001085
Iteration 46/1000 | Loss: 0.00001085
Iteration 47/1000 | Loss: 0.00001085
Iteration 48/1000 | Loss: 0.00001085
Iteration 49/1000 | Loss: 0.00001085
Iteration 50/1000 | Loss: 0.00001085
Iteration 51/1000 | Loss: 0.00001085
Iteration 52/1000 | Loss: 0.00001085
Iteration 53/1000 | Loss: 0.00001084
Iteration 54/1000 | Loss: 0.00001084
Iteration 55/1000 | Loss: 0.00001084
Iteration 56/1000 | Loss: 0.00001083
Iteration 57/1000 | Loss: 0.00001083
Iteration 58/1000 | Loss: 0.00001082
Iteration 59/1000 | Loss: 0.00001082
Iteration 60/1000 | Loss: 0.00001082
Iteration 61/1000 | Loss: 0.00001081
Iteration 62/1000 | Loss: 0.00001081
Iteration 63/1000 | Loss: 0.00001081
Iteration 64/1000 | Loss: 0.00001081
Iteration 65/1000 | Loss: 0.00001081
Iteration 66/1000 | Loss: 0.00001081
Iteration 67/1000 | Loss: 0.00001080
Iteration 68/1000 | Loss: 0.00001080
Iteration 69/1000 | Loss: 0.00001080
Iteration 70/1000 | Loss: 0.00001080
Iteration 71/1000 | Loss: 0.00001080
Iteration 72/1000 | Loss: 0.00001080
Iteration 73/1000 | Loss: 0.00001079
Iteration 74/1000 | Loss: 0.00001079
Iteration 75/1000 | Loss: 0.00001079
Iteration 76/1000 | Loss: 0.00001079
Iteration 77/1000 | Loss: 0.00001079
Iteration 78/1000 | Loss: 0.00001079
Iteration 79/1000 | Loss: 0.00001079
Iteration 80/1000 | Loss: 0.00001079
Iteration 81/1000 | Loss: 0.00001079
Iteration 82/1000 | Loss: 0.00001079
Iteration 83/1000 | Loss: 0.00001078
Iteration 84/1000 | Loss: 0.00001078
Iteration 85/1000 | Loss: 0.00001078
Iteration 86/1000 | Loss: 0.00001078
Iteration 87/1000 | Loss: 0.00001078
Iteration 88/1000 | Loss: 0.00001078
Iteration 89/1000 | Loss: 0.00001078
Iteration 90/1000 | Loss: 0.00001078
Iteration 91/1000 | Loss: 0.00001078
Iteration 92/1000 | Loss: 0.00001078
Iteration 93/1000 | Loss: 0.00001077
Iteration 94/1000 | Loss: 0.00001077
Iteration 95/1000 | Loss: 0.00001077
Iteration 96/1000 | Loss: 0.00001077
Iteration 97/1000 | Loss: 0.00001077
Iteration 98/1000 | Loss: 0.00001077
Iteration 99/1000 | Loss: 0.00001077
Iteration 100/1000 | Loss: 0.00001077
Iteration 101/1000 | Loss: 0.00001077
Iteration 102/1000 | Loss: 0.00001077
Iteration 103/1000 | Loss: 0.00001077
Iteration 104/1000 | Loss: 0.00001076
Iteration 105/1000 | Loss: 0.00001076
Iteration 106/1000 | Loss: 0.00001076
Iteration 107/1000 | Loss: 0.00001076
Iteration 108/1000 | Loss: 0.00001076
Iteration 109/1000 | Loss: 0.00001076
Iteration 110/1000 | Loss: 0.00001076
Iteration 111/1000 | Loss: 0.00001076
Iteration 112/1000 | Loss: 0.00001076
Iteration 113/1000 | Loss: 0.00001076
Iteration 114/1000 | Loss: 0.00001076
Iteration 115/1000 | Loss: 0.00001076
Iteration 116/1000 | Loss: 0.00001076
Iteration 117/1000 | Loss: 0.00001076
Iteration 118/1000 | Loss: 0.00001076
Iteration 119/1000 | Loss: 0.00001075
Iteration 120/1000 | Loss: 0.00001075
Iteration 121/1000 | Loss: 0.00001075
Iteration 122/1000 | Loss: 0.00001075
Iteration 123/1000 | Loss: 0.00001075
Iteration 124/1000 | Loss: 0.00001075
Iteration 125/1000 | Loss: 0.00001075
Iteration 126/1000 | Loss: 0.00001075
Iteration 127/1000 | Loss: 0.00001075
Iteration 128/1000 | Loss: 0.00001075
Iteration 129/1000 | Loss: 0.00001075
Iteration 130/1000 | Loss: 0.00001075
Iteration 131/1000 | Loss: 0.00001075
Iteration 132/1000 | Loss: 0.00001075
Iteration 133/1000 | Loss: 0.00001075
Iteration 134/1000 | Loss: 0.00001075
Iteration 135/1000 | Loss: 0.00001075
Iteration 136/1000 | Loss: 0.00001075
Iteration 137/1000 | Loss: 0.00001074
Iteration 138/1000 | Loss: 0.00001074
Iteration 139/1000 | Loss: 0.00001074
Iteration 140/1000 | Loss: 0.00001074
Iteration 141/1000 | Loss: 0.00001074
Iteration 142/1000 | Loss: 0.00001074
Iteration 143/1000 | Loss: 0.00001074
Iteration 144/1000 | Loss: 0.00001074
Iteration 145/1000 | Loss: 0.00001074
Iteration 146/1000 | Loss: 0.00001074
Iteration 147/1000 | Loss: 0.00001074
Iteration 148/1000 | Loss: 0.00001074
Iteration 149/1000 | Loss: 0.00001074
Iteration 150/1000 | Loss: 0.00001074
Iteration 151/1000 | Loss: 0.00001073
Iteration 152/1000 | Loss: 0.00001073
Iteration 153/1000 | Loss: 0.00001073
Iteration 154/1000 | Loss: 0.00001073
Iteration 155/1000 | Loss: 0.00001073
Iteration 156/1000 | Loss: 0.00001073
Iteration 157/1000 | Loss: 0.00001073
Iteration 158/1000 | Loss: 0.00001073
Iteration 159/1000 | Loss: 0.00001073
Iteration 160/1000 | Loss: 0.00001073
Iteration 161/1000 | Loss: 0.00001073
Iteration 162/1000 | Loss: 0.00001073
Iteration 163/1000 | Loss: 0.00001073
Iteration 164/1000 | Loss: 0.00001073
Iteration 165/1000 | Loss: 0.00001073
Iteration 166/1000 | Loss: 0.00001073
Iteration 167/1000 | Loss: 0.00001073
Iteration 168/1000 | Loss: 0.00001073
Iteration 169/1000 | Loss: 0.00001073
Iteration 170/1000 | Loss: 0.00001073
Iteration 171/1000 | Loss: 0.00001073
Iteration 172/1000 | Loss: 0.00001073
Iteration 173/1000 | Loss: 0.00001073
Iteration 174/1000 | Loss: 0.00001073
Iteration 175/1000 | Loss: 0.00001073
Iteration 176/1000 | Loss: 0.00001073
Iteration 177/1000 | Loss: 0.00001073
Iteration 178/1000 | Loss: 0.00001073
Iteration 179/1000 | Loss: 0.00001073
Iteration 180/1000 | Loss: 0.00001073
Iteration 181/1000 | Loss: 0.00001073
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [1.072517170541687e-05, 1.072517170541687e-05, 1.072517170541687e-05, 1.072517170541687e-05, 1.072517170541687e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.072517170541687e-05

Optimization complete. Final v2v error: 2.785041332244873 mm

Highest mean error: 2.949422836303711 mm for frame 30

Lowest mean error: 2.6055922508239746 mm for frame 226

Saving results

Total time: 40.91346836090088
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01061809
Iteration 2/25 | Loss: 0.00193688
Iteration 3/25 | Loss: 0.00144555
Iteration 4/25 | Loss: 0.00144686
Iteration 5/25 | Loss: 0.00122498
Iteration 6/25 | Loss: 0.00092838
Iteration 7/25 | Loss: 0.00080349
Iteration 8/25 | Loss: 0.00078192
Iteration 9/25 | Loss: 0.00077766
Iteration 10/25 | Loss: 0.00077700
Iteration 11/25 | Loss: 0.00077693
Iteration 12/25 | Loss: 0.00077693
Iteration 13/25 | Loss: 0.00077693
Iteration 14/25 | Loss: 0.00077693
Iteration 15/25 | Loss: 0.00077693
Iteration 16/25 | Loss: 0.00077693
Iteration 17/25 | Loss: 0.00077693
Iteration 18/25 | Loss: 0.00077693
Iteration 19/25 | Loss: 0.00077693
Iteration 20/25 | Loss: 0.00077693
Iteration 21/25 | Loss: 0.00077693
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007769291405566037, 0.0007769291405566037, 0.0007769291405566037, 0.0007769291405566037, 0.0007769291405566037]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007769291405566037

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45625758
Iteration 2/25 | Loss: 0.00021151
Iteration 3/25 | Loss: 0.00021150
Iteration 4/25 | Loss: 0.00021150
Iteration 5/25 | Loss: 0.00021150
Iteration 6/25 | Loss: 0.00021150
Iteration 7/25 | Loss: 0.00021150
Iteration 8/25 | Loss: 0.00021150
Iteration 9/25 | Loss: 0.00021150
Iteration 10/25 | Loss: 0.00021150
Iteration 11/25 | Loss: 0.00021150
Iteration 12/25 | Loss: 0.00021150
Iteration 13/25 | Loss: 0.00021150
Iteration 14/25 | Loss: 0.00021150
Iteration 15/25 | Loss: 0.00021150
Iteration 16/25 | Loss: 0.00021150
Iteration 17/25 | Loss: 0.00021150
Iteration 18/25 | Loss: 0.00021150
Iteration 19/25 | Loss: 0.00021150
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00021149718668311834, 0.00021149718668311834, 0.00021149718668311834, 0.00021149718668311834, 0.00021149718668311834]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00021149718668311834

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00021150
Iteration 2/1000 | Loss: 0.00003124
Iteration 3/1000 | Loss: 0.00002617
Iteration 4/1000 | Loss: 0.00002479
Iteration 5/1000 | Loss: 0.00002404
Iteration 6/1000 | Loss: 0.00002358
Iteration 7/1000 | Loss: 0.00002322
Iteration 8/1000 | Loss: 0.00002302
Iteration 9/1000 | Loss: 0.00002298
Iteration 10/1000 | Loss: 0.00002293
Iteration 11/1000 | Loss: 0.00002289
Iteration 12/1000 | Loss: 0.00002287
Iteration 13/1000 | Loss: 0.00002287
Iteration 14/1000 | Loss: 0.00002287
Iteration 15/1000 | Loss: 0.00002286
Iteration 16/1000 | Loss: 0.00002286
Iteration 17/1000 | Loss: 0.00002286
Iteration 18/1000 | Loss: 0.00002284
Iteration 19/1000 | Loss: 0.00002283
Iteration 20/1000 | Loss: 0.00002281
Iteration 21/1000 | Loss: 0.00002280
Iteration 22/1000 | Loss: 0.00002280
Iteration 23/1000 | Loss: 0.00002280
Iteration 24/1000 | Loss: 0.00002280
Iteration 25/1000 | Loss: 0.00002279
Iteration 26/1000 | Loss: 0.00002278
Iteration 27/1000 | Loss: 0.00002278
Iteration 28/1000 | Loss: 0.00002277
Iteration 29/1000 | Loss: 0.00002277
Iteration 30/1000 | Loss: 0.00002277
Iteration 31/1000 | Loss: 0.00002276
Iteration 32/1000 | Loss: 0.00002276
Iteration 33/1000 | Loss: 0.00002276
Iteration 34/1000 | Loss: 0.00002276
Iteration 35/1000 | Loss: 0.00002275
Iteration 36/1000 | Loss: 0.00002275
Iteration 37/1000 | Loss: 0.00002275
Iteration 38/1000 | Loss: 0.00002273
Iteration 39/1000 | Loss: 0.00002273
Iteration 40/1000 | Loss: 0.00002273
Iteration 41/1000 | Loss: 0.00002273
Iteration 42/1000 | Loss: 0.00002273
Iteration 43/1000 | Loss: 0.00002273
Iteration 44/1000 | Loss: 0.00002273
Iteration 45/1000 | Loss: 0.00002273
Iteration 46/1000 | Loss: 0.00002273
Iteration 47/1000 | Loss: 0.00002272
Iteration 48/1000 | Loss: 0.00002272
Iteration 49/1000 | Loss: 0.00002272
Iteration 50/1000 | Loss: 0.00002272
Iteration 51/1000 | Loss: 0.00002272
Iteration 52/1000 | Loss: 0.00002272
Iteration 53/1000 | Loss: 0.00002271
Iteration 54/1000 | Loss: 0.00002271
Iteration 55/1000 | Loss: 0.00002271
Iteration 56/1000 | Loss: 0.00002271
Iteration 57/1000 | Loss: 0.00002271
Iteration 58/1000 | Loss: 0.00002271
Iteration 59/1000 | Loss: 0.00002271
Iteration 60/1000 | Loss: 0.00002271
Iteration 61/1000 | Loss: 0.00002270
Iteration 62/1000 | Loss: 0.00002270
Iteration 63/1000 | Loss: 0.00002270
Iteration 64/1000 | Loss: 0.00002270
Iteration 65/1000 | Loss: 0.00002270
Iteration 66/1000 | Loss: 0.00002270
Iteration 67/1000 | Loss: 0.00002269
Iteration 68/1000 | Loss: 0.00002269
Iteration 69/1000 | Loss: 0.00002269
Iteration 70/1000 | Loss: 0.00002269
Iteration 71/1000 | Loss: 0.00002269
Iteration 72/1000 | Loss: 0.00002269
Iteration 73/1000 | Loss: 0.00002269
Iteration 74/1000 | Loss: 0.00002269
Iteration 75/1000 | Loss: 0.00002269
Iteration 76/1000 | Loss: 0.00002269
Iteration 77/1000 | Loss: 0.00002268
Iteration 78/1000 | Loss: 0.00002268
Iteration 79/1000 | Loss: 0.00002268
Iteration 80/1000 | Loss: 0.00002268
Iteration 81/1000 | Loss: 0.00002268
Iteration 82/1000 | Loss: 0.00002268
Iteration 83/1000 | Loss: 0.00002268
Iteration 84/1000 | Loss: 0.00002268
Iteration 85/1000 | Loss: 0.00002268
Iteration 86/1000 | Loss: 0.00002268
Iteration 87/1000 | Loss: 0.00002268
Iteration 88/1000 | Loss: 0.00002268
Iteration 89/1000 | Loss: 0.00002268
Iteration 90/1000 | Loss: 0.00002268
Iteration 91/1000 | Loss: 0.00002268
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [2.2682883354718797e-05, 2.2682883354718797e-05, 2.2682883354718797e-05, 2.2682883354718797e-05, 2.2682883354718797e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2682883354718797e-05

Optimization complete. Final v2v error: 3.9935014247894287 mm

Highest mean error: 4.362209796905518 mm for frame 28

Lowest mean error: 3.844862222671509 mm for frame 8

Saving results

Total time: 39.437546491622925
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00826776
Iteration 2/25 | Loss: 0.00075018
Iteration 3/25 | Loss: 0.00063054
Iteration 4/25 | Loss: 0.00059764
Iteration 5/25 | Loss: 0.00059380
Iteration 6/25 | Loss: 0.00059346
Iteration 7/25 | Loss: 0.00059346
Iteration 8/25 | Loss: 0.00059346
Iteration 9/25 | Loss: 0.00059346
Iteration 10/25 | Loss: 0.00059346
Iteration 11/25 | Loss: 0.00059346
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0005934601067565382, 0.0005934601067565382, 0.0005934601067565382, 0.0005934601067565382, 0.0005934601067565382]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005934601067565382

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48458660
Iteration 2/25 | Loss: 0.00019564
Iteration 3/25 | Loss: 0.00019564
Iteration 4/25 | Loss: 0.00019564
Iteration 5/25 | Loss: 0.00019564
Iteration 6/25 | Loss: 0.00019563
Iteration 7/25 | Loss: 0.00019563
Iteration 8/25 | Loss: 0.00019563
Iteration 9/25 | Loss: 0.00019563
Iteration 10/25 | Loss: 0.00019563
Iteration 11/25 | Loss: 0.00019563
Iteration 12/25 | Loss: 0.00019563
Iteration 13/25 | Loss: 0.00019563
Iteration 14/25 | Loss: 0.00019563
Iteration 15/25 | Loss: 0.00019563
Iteration 16/25 | Loss: 0.00019563
Iteration 17/25 | Loss: 0.00019563
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0001956337655428797, 0.0001956337655428797, 0.0001956337655428797, 0.0001956337655428797, 0.0001956337655428797]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0001956337655428797

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00019563
Iteration 2/1000 | Loss: 0.00002429
Iteration 3/1000 | Loss: 0.00001962
Iteration 4/1000 | Loss: 0.00001809
Iteration 5/1000 | Loss: 0.00001702
Iteration 6/1000 | Loss: 0.00001651
Iteration 7/1000 | Loss: 0.00001611
Iteration 8/1000 | Loss: 0.00001594
Iteration 9/1000 | Loss: 0.00001583
Iteration 10/1000 | Loss: 0.00001569
Iteration 11/1000 | Loss: 0.00001565
Iteration 12/1000 | Loss: 0.00001562
Iteration 13/1000 | Loss: 0.00001559
Iteration 14/1000 | Loss: 0.00001559
Iteration 15/1000 | Loss: 0.00001558
Iteration 16/1000 | Loss: 0.00001557
Iteration 17/1000 | Loss: 0.00001554
Iteration 18/1000 | Loss: 0.00001554
Iteration 19/1000 | Loss: 0.00001553
Iteration 20/1000 | Loss: 0.00001553
Iteration 21/1000 | Loss: 0.00001553
Iteration 22/1000 | Loss: 0.00001551
Iteration 23/1000 | Loss: 0.00001550
Iteration 24/1000 | Loss: 0.00001550
Iteration 25/1000 | Loss: 0.00001549
Iteration 26/1000 | Loss: 0.00001549
Iteration 27/1000 | Loss: 0.00001549
Iteration 28/1000 | Loss: 0.00001549
Iteration 29/1000 | Loss: 0.00001549
Iteration 30/1000 | Loss: 0.00001549
Iteration 31/1000 | Loss: 0.00001548
Iteration 32/1000 | Loss: 0.00001548
Iteration 33/1000 | Loss: 0.00001548
Iteration 34/1000 | Loss: 0.00001548
Iteration 35/1000 | Loss: 0.00001548
Iteration 36/1000 | Loss: 0.00001548
Iteration 37/1000 | Loss: 0.00001548
Iteration 38/1000 | Loss: 0.00001548
Iteration 39/1000 | Loss: 0.00001547
Iteration 40/1000 | Loss: 0.00001547
Iteration 41/1000 | Loss: 0.00001547
Iteration 42/1000 | Loss: 0.00001546
Iteration 43/1000 | Loss: 0.00001546
Iteration 44/1000 | Loss: 0.00001546
Iteration 45/1000 | Loss: 0.00001546
Iteration 46/1000 | Loss: 0.00001546
Iteration 47/1000 | Loss: 0.00001546
Iteration 48/1000 | Loss: 0.00001546
Iteration 49/1000 | Loss: 0.00001546
Iteration 50/1000 | Loss: 0.00001545
Iteration 51/1000 | Loss: 0.00001545
Iteration 52/1000 | Loss: 0.00001544
Iteration 53/1000 | Loss: 0.00001544
Iteration 54/1000 | Loss: 0.00001543
Iteration 55/1000 | Loss: 0.00001543
Iteration 56/1000 | Loss: 0.00001543
Iteration 57/1000 | Loss: 0.00001543
Iteration 58/1000 | Loss: 0.00001543
Iteration 59/1000 | Loss: 0.00001543
Iteration 60/1000 | Loss: 0.00001543
Iteration 61/1000 | Loss: 0.00001543
Iteration 62/1000 | Loss: 0.00001543
Iteration 63/1000 | Loss: 0.00001543
Iteration 64/1000 | Loss: 0.00001542
Iteration 65/1000 | Loss: 0.00001542
Iteration 66/1000 | Loss: 0.00001542
Iteration 67/1000 | Loss: 0.00001542
Iteration 68/1000 | Loss: 0.00001541
Iteration 69/1000 | Loss: 0.00001541
Iteration 70/1000 | Loss: 0.00001540
Iteration 71/1000 | Loss: 0.00001540
Iteration 72/1000 | Loss: 0.00001540
Iteration 73/1000 | Loss: 0.00001539
Iteration 74/1000 | Loss: 0.00001539
Iteration 75/1000 | Loss: 0.00001539
Iteration 76/1000 | Loss: 0.00001539
Iteration 77/1000 | Loss: 0.00001539
Iteration 78/1000 | Loss: 0.00001539
Iteration 79/1000 | Loss: 0.00001538
Iteration 80/1000 | Loss: 0.00001538
Iteration 81/1000 | Loss: 0.00001538
Iteration 82/1000 | Loss: 0.00001537
Iteration 83/1000 | Loss: 0.00001537
Iteration 84/1000 | Loss: 0.00001537
Iteration 85/1000 | Loss: 0.00001536
Iteration 86/1000 | Loss: 0.00001535
Iteration 87/1000 | Loss: 0.00001535
Iteration 88/1000 | Loss: 0.00001535
Iteration 89/1000 | Loss: 0.00001535
Iteration 90/1000 | Loss: 0.00001535
Iteration 91/1000 | Loss: 0.00001535
Iteration 92/1000 | Loss: 0.00001535
Iteration 93/1000 | Loss: 0.00001535
Iteration 94/1000 | Loss: 0.00001534
Iteration 95/1000 | Loss: 0.00001534
Iteration 96/1000 | Loss: 0.00001534
Iteration 97/1000 | Loss: 0.00001533
Iteration 98/1000 | Loss: 0.00001533
Iteration 99/1000 | Loss: 0.00001533
Iteration 100/1000 | Loss: 0.00001532
Iteration 101/1000 | Loss: 0.00001532
Iteration 102/1000 | Loss: 0.00001532
Iteration 103/1000 | Loss: 0.00001532
Iteration 104/1000 | Loss: 0.00001532
Iteration 105/1000 | Loss: 0.00001531
Iteration 106/1000 | Loss: 0.00001531
Iteration 107/1000 | Loss: 0.00001531
Iteration 108/1000 | Loss: 0.00001531
Iteration 109/1000 | Loss: 0.00001530
Iteration 110/1000 | Loss: 0.00001530
Iteration 111/1000 | Loss: 0.00001530
Iteration 112/1000 | Loss: 0.00001529
Iteration 113/1000 | Loss: 0.00001529
Iteration 114/1000 | Loss: 0.00001529
Iteration 115/1000 | Loss: 0.00001529
Iteration 116/1000 | Loss: 0.00001529
Iteration 117/1000 | Loss: 0.00001529
Iteration 118/1000 | Loss: 0.00001529
Iteration 119/1000 | Loss: 0.00001529
Iteration 120/1000 | Loss: 0.00001529
Iteration 121/1000 | Loss: 0.00001529
Iteration 122/1000 | Loss: 0.00001529
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.529057408333756e-05, 1.529057408333756e-05, 1.529057408333756e-05, 1.529057408333756e-05, 1.529057408333756e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.529057408333756e-05

Optimization complete. Final v2v error: 3.2333266735076904 mm

Highest mean error: 3.604219436645508 mm for frame 63

Lowest mean error: 3.1018049716949463 mm for frame 143

Saving results

Total time: 31.436909437179565
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00830667
Iteration 2/25 | Loss: 0.00073112
Iteration 3/25 | Loss: 0.00055741
Iteration 4/25 | Loss: 0.00054141
Iteration 5/25 | Loss: 0.00053610
Iteration 6/25 | Loss: 0.00053458
Iteration 7/25 | Loss: 0.00053452
Iteration 8/25 | Loss: 0.00053452
Iteration 9/25 | Loss: 0.00053452
Iteration 10/25 | Loss: 0.00053452
Iteration 11/25 | Loss: 0.00053452
Iteration 12/25 | Loss: 0.00053452
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005345182144083083, 0.0005345182144083083, 0.0005345182144083083, 0.0005345182144083083, 0.0005345182144083083]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005345182144083083

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46913135
Iteration 2/25 | Loss: 0.00018567
Iteration 3/25 | Loss: 0.00018567
Iteration 4/25 | Loss: 0.00018567
Iteration 5/25 | Loss: 0.00018567
Iteration 6/25 | Loss: 0.00018567
Iteration 7/25 | Loss: 0.00018567
Iteration 8/25 | Loss: 0.00018567
Iteration 9/25 | Loss: 0.00018566
Iteration 10/25 | Loss: 0.00018566
Iteration 11/25 | Loss: 0.00018566
Iteration 12/25 | Loss: 0.00018566
Iteration 13/25 | Loss: 0.00018566
Iteration 14/25 | Loss: 0.00018566
Iteration 15/25 | Loss: 0.00018566
Iteration 16/25 | Loss: 0.00018566
Iteration 17/25 | Loss: 0.00018566
Iteration 18/25 | Loss: 0.00018566
Iteration 19/25 | Loss: 0.00018566
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00018566449580248445, 0.00018566449580248445, 0.00018566449580248445, 0.00018566449580248445, 0.00018566449580248445]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00018566449580248445

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00018566
Iteration 2/1000 | Loss: 0.00001589
Iteration 3/1000 | Loss: 0.00001079
Iteration 4/1000 | Loss: 0.00000987
Iteration 5/1000 | Loss: 0.00000930
Iteration 6/1000 | Loss: 0.00000898
Iteration 7/1000 | Loss: 0.00000883
Iteration 8/1000 | Loss: 0.00000881
Iteration 9/1000 | Loss: 0.00000881
Iteration 10/1000 | Loss: 0.00000880
Iteration 11/1000 | Loss: 0.00000880
Iteration 12/1000 | Loss: 0.00000879
Iteration 13/1000 | Loss: 0.00000878
Iteration 14/1000 | Loss: 0.00000874
Iteration 15/1000 | Loss: 0.00000865
Iteration 16/1000 | Loss: 0.00000865
Iteration 17/1000 | Loss: 0.00000865
Iteration 18/1000 | Loss: 0.00000865
Iteration 19/1000 | Loss: 0.00000865
Iteration 20/1000 | Loss: 0.00000865
Iteration 21/1000 | Loss: 0.00000865
Iteration 22/1000 | Loss: 0.00000865
Iteration 23/1000 | Loss: 0.00000864
Iteration 24/1000 | Loss: 0.00000859
Iteration 25/1000 | Loss: 0.00000859
Iteration 26/1000 | Loss: 0.00000859
Iteration 27/1000 | Loss: 0.00000859
Iteration 28/1000 | Loss: 0.00000859
Iteration 29/1000 | Loss: 0.00000859
Iteration 30/1000 | Loss: 0.00000859
Iteration 31/1000 | Loss: 0.00000859
Iteration 32/1000 | Loss: 0.00000859
Iteration 33/1000 | Loss: 0.00000858
Iteration 34/1000 | Loss: 0.00000858
Iteration 35/1000 | Loss: 0.00000857
Iteration 36/1000 | Loss: 0.00000854
Iteration 37/1000 | Loss: 0.00000853
Iteration 38/1000 | Loss: 0.00000853
Iteration 39/1000 | Loss: 0.00000853
Iteration 40/1000 | Loss: 0.00000853
Iteration 41/1000 | Loss: 0.00000853
Iteration 42/1000 | Loss: 0.00000852
Iteration 43/1000 | Loss: 0.00000852
Iteration 44/1000 | Loss: 0.00000851
Iteration 45/1000 | Loss: 0.00000851
Iteration 46/1000 | Loss: 0.00000851
Iteration 47/1000 | Loss: 0.00000850
Iteration 48/1000 | Loss: 0.00000850
Iteration 49/1000 | Loss: 0.00000850
Iteration 50/1000 | Loss: 0.00000850
Iteration 51/1000 | Loss: 0.00000850
Iteration 52/1000 | Loss: 0.00000849
Iteration 53/1000 | Loss: 0.00000849
Iteration 54/1000 | Loss: 0.00000848
Iteration 55/1000 | Loss: 0.00000848
Iteration 56/1000 | Loss: 0.00000848
Iteration 57/1000 | Loss: 0.00000848
Iteration 58/1000 | Loss: 0.00000847
Iteration 59/1000 | Loss: 0.00000847
Iteration 60/1000 | Loss: 0.00000847
Iteration 61/1000 | Loss: 0.00000846
Iteration 62/1000 | Loss: 0.00000846
Iteration 63/1000 | Loss: 0.00000845
Iteration 64/1000 | Loss: 0.00000845
Iteration 65/1000 | Loss: 0.00000845
Iteration 66/1000 | Loss: 0.00000845
Iteration 67/1000 | Loss: 0.00000845
Iteration 68/1000 | Loss: 0.00000844
Iteration 69/1000 | Loss: 0.00000844
Iteration 70/1000 | Loss: 0.00000844
Iteration 71/1000 | Loss: 0.00000844
Iteration 72/1000 | Loss: 0.00000844
Iteration 73/1000 | Loss: 0.00000844
Iteration 74/1000 | Loss: 0.00000843
Iteration 75/1000 | Loss: 0.00000843
Iteration 76/1000 | Loss: 0.00000843
Iteration 77/1000 | Loss: 0.00000843
Iteration 78/1000 | Loss: 0.00000843
Iteration 79/1000 | Loss: 0.00000843
Iteration 80/1000 | Loss: 0.00000842
Iteration 81/1000 | Loss: 0.00000842
Iteration 82/1000 | Loss: 0.00000842
Iteration 83/1000 | Loss: 0.00000842
Iteration 84/1000 | Loss: 0.00000841
Iteration 85/1000 | Loss: 0.00000841
Iteration 86/1000 | Loss: 0.00000841
Iteration 87/1000 | Loss: 0.00000840
Iteration 88/1000 | Loss: 0.00000840
Iteration 89/1000 | Loss: 0.00000840
Iteration 90/1000 | Loss: 0.00000840
Iteration 91/1000 | Loss: 0.00000840
Iteration 92/1000 | Loss: 0.00000840
Iteration 93/1000 | Loss: 0.00000840
Iteration 94/1000 | Loss: 0.00000840
Iteration 95/1000 | Loss: 0.00000840
Iteration 96/1000 | Loss: 0.00000840
Iteration 97/1000 | Loss: 0.00000840
Iteration 98/1000 | Loss: 0.00000840
Iteration 99/1000 | Loss: 0.00000840
Iteration 100/1000 | Loss: 0.00000839
Iteration 101/1000 | Loss: 0.00000839
Iteration 102/1000 | Loss: 0.00000839
Iteration 103/1000 | Loss: 0.00000839
Iteration 104/1000 | Loss: 0.00000839
Iteration 105/1000 | Loss: 0.00000839
Iteration 106/1000 | Loss: 0.00000839
Iteration 107/1000 | Loss: 0.00000839
Iteration 108/1000 | Loss: 0.00000839
Iteration 109/1000 | Loss: 0.00000839
Iteration 110/1000 | Loss: 0.00000839
Iteration 111/1000 | Loss: 0.00000839
Iteration 112/1000 | Loss: 0.00000839
Iteration 113/1000 | Loss: 0.00000839
Iteration 114/1000 | Loss: 0.00000839
Iteration 115/1000 | Loss: 0.00000839
Iteration 116/1000 | Loss: 0.00000839
Iteration 117/1000 | Loss: 0.00000839
Iteration 118/1000 | Loss: 0.00000839
Iteration 119/1000 | Loss: 0.00000839
Iteration 120/1000 | Loss: 0.00000839
Iteration 121/1000 | Loss: 0.00000839
Iteration 122/1000 | Loss: 0.00000839
Iteration 123/1000 | Loss: 0.00000839
Iteration 124/1000 | Loss: 0.00000839
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [8.386457921005785e-06, 8.386457921005785e-06, 8.386457921005785e-06, 8.386457921005785e-06, 8.386457921005785e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.386457921005785e-06

Optimization complete. Final v2v error: 2.4581668376922607 mm

Highest mean error: 2.648925542831421 mm for frame 106

Lowest mean error: 2.277132272720337 mm for frame 156

Saving results

Total time: 32.139838218688965
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_004/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_004/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00844574
Iteration 2/25 | Loss: 0.00106349
Iteration 3/25 | Loss: 0.00068285
Iteration 4/25 | Loss: 0.00064126
Iteration 5/25 | Loss: 0.00063592
Iteration 6/25 | Loss: 0.00063360
Iteration 7/25 | Loss: 0.00063338
Iteration 8/25 | Loss: 0.00063338
Iteration 9/25 | Loss: 0.00063338
Iteration 10/25 | Loss: 0.00063338
Iteration 11/25 | Loss: 0.00063338
Iteration 12/25 | Loss: 0.00063338
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006333782221190631, 0.0006333782221190631, 0.0006333782221190631, 0.0006333782221190631, 0.0006333782221190631]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006333782221190631

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36342835
Iteration 2/25 | Loss: 0.00025894
Iteration 3/25 | Loss: 0.00025893
Iteration 4/25 | Loss: 0.00025893
Iteration 5/25 | Loss: 0.00025893
Iteration 6/25 | Loss: 0.00025893
Iteration 7/25 | Loss: 0.00025893
Iteration 8/25 | Loss: 0.00025893
Iteration 9/25 | Loss: 0.00025893
Iteration 10/25 | Loss: 0.00025893
Iteration 11/25 | Loss: 0.00025893
Iteration 12/25 | Loss: 0.00025893
Iteration 13/25 | Loss: 0.00025893
Iteration 14/25 | Loss: 0.00025893
Iteration 15/25 | Loss: 0.00025893
Iteration 16/25 | Loss: 0.00025893
Iteration 17/25 | Loss: 0.00025893
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00025892737903632224, 0.00025892737903632224, 0.00025892737903632224, 0.00025892737903632224, 0.00025892737903632224]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00025892737903632224

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025893
Iteration 2/1000 | Loss: 0.00001857
Iteration 3/1000 | Loss: 0.00001566
Iteration 4/1000 | Loss: 0.00001482
Iteration 5/1000 | Loss: 0.00001419
Iteration 6/1000 | Loss: 0.00001379
Iteration 7/1000 | Loss: 0.00001365
Iteration 8/1000 | Loss: 0.00001349
Iteration 9/1000 | Loss: 0.00001344
Iteration 10/1000 | Loss: 0.00001334
Iteration 11/1000 | Loss: 0.00001334
Iteration 12/1000 | Loss: 0.00001334
Iteration 13/1000 | Loss: 0.00001334
Iteration 14/1000 | Loss: 0.00001329
Iteration 15/1000 | Loss: 0.00001329
Iteration 16/1000 | Loss: 0.00001328
Iteration 17/1000 | Loss: 0.00001328
Iteration 18/1000 | Loss: 0.00001328
Iteration 19/1000 | Loss: 0.00001328
Iteration 20/1000 | Loss: 0.00001328
Iteration 21/1000 | Loss: 0.00001328
Iteration 22/1000 | Loss: 0.00001328
Iteration 23/1000 | Loss: 0.00001327
Iteration 24/1000 | Loss: 0.00001327
Iteration 25/1000 | Loss: 0.00001324
Iteration 26/1000 | Loss: 0.00001324
Iteration 27/1000 | Loss: 0.00001324
Iteration 28/1000 | Loss: 0.00001324
Iteration 29/1000 | Loss: 0.00001324
Iteration 30/1000 | Loss: 0.00001323
Iteration 31/1000 | Loss: 0.00001323
Iteration 32/1000 | Loss: 0.00001323
Iteration 33/1000 | Loss: 0.00001323
Iteration 34/1000 | Loss: 0.00001323
Iteration 35/1000 | Loss: 0.00001323
Iteration 36/1000 | Loss: 0.00001323
Iteration 37/1000 | Loss: 0.00001323
Iteration 38/1000 | Loss: 0.00001323
Iteration 39/1000 | Loss: 0.00001323
Iteration 40/1000 | Loss: 0.00001322
Iteration 41/1000 | Loss: 0.00001322
Iteration 42/1000 | Loss: 0.00001321
Iteration 43/1000 | Loss: 0.00001320
Iteration 44/1000 | Loss: 0.00001320
Iteration 45/1000 | Loss: 0.00001320
Iteration 46/1000 | Loss: 0.00001320
Iteration 47/1000 | Loss: 0.00001320
Iteration 48/1000 | Loss: 0.00001320
Iteration 49/1000 | Loss: 0.00001320
Iteration 50/1000 | Loss: 0.00001320
Iteration 51/1000 | Loss: 0.00001320
Iteration 52/1000 | Loss: 0.00001320
Iteration 53/1000 | Loss: 0.00001320
Iteration 54/1000 | Loss: 0.00001320
Iteration 55/1000 | Loss: 0.00001320
Iteration 56/1000 | Loss: 0.00001320
Iteration 57/1000 | Loss: 0.00001320
Iteration 58/1000 | Loss: 0.00001320
Iteration 59/1000 | Loss: 0.00001320
Iteration 60/1000 | Loss: 0.00001320
Iteration 61/1000 | Loss: 0.00001320
Iteration 62/1000 | Loss: 0.00001320
Iteration 63/1000 | Loss: 0.00001320
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 63. Stopping optimization.
Last 5 losses: [1.3201853107602801e-05, 1.3201853107602801e-05, 1.3201853107602801e-05, 1.3201853107602801e-05, 1.3201853107602801e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3201853107602801e-05

Optimization complete. Final v2v error: 3.128382921218872 mm

Highest mean error: 3.459451913833618 mm for frame 61

Lowest mean error: 2.9826951026916504 mm for frame 45

Saving results

Total time: 24.578800201416016
