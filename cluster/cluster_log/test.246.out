Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=246, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 13776-13831
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5760/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5760/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5760/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00866651
Iteration 2/25 | Loss: 0.00155843
Iteration 3/25 | Loss: 0.00136560
Iteration 4/25 | Loss: 0.00134209
Iteration 5/25 | Loss: 0.00133687
Iteration 6/25 | Loss: 0.00133643
Iteration 7/25 | Loss: 0.00133643
Iteration 8/25 | Loss: 0.00133643
Iteration 9/25 | Loss: 0.00133643
Iteration 10/25 | Loss: 0.00133643
Iteration 11/25 | Loss: 0.00133643
Iteration 12/25 | Loss: 0.00133643
Iteration 13/25 | Loss: 0.00133643
Iteration 14/25 | Loss: 0.00133643
Iteration 15/25 | Loss: 0.00133643
Iteration 16/25 | Loss: 0.00133643
Iteration 17/25 | Loss: 0.00133643
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013364322949200869, 0.0013364322949200869, 0.0013364322949200869, 0.0013364322949200869, 0.0013364322949200869]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013364322949200869

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11362815
Iteration 2/25 | Loss: 0.00360994
Iteration 3/25 | Loss: 0.00360993
Iteration 4/25 | Loss: 0.00360993
Iteration 5/25 | Loss: 0.00360993
Iteration 6/25 | Loss: 0.00360993
Iteration 7/25 | Loss: 0.00360993
Iteration 8/25 | Loss: 0.00360993
Iteration 9/25 | Loss: 0.00360993
Iteration 10/25 | Loss: 0.00360993
Iteration 11/25 | Loss: 0.00360993
Iteration 12/25 | Loss: 0.00360993
Iteration 13/25 | Loss: 0.00360993
Iteration 14/25 | Loss: 0.00360993
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0036099301651120186, 0.0036099301651120186, 0.0036099301651120186, 0.0036099301651120186, 0.0036099301651120186]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0036099301651120186

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00360993
Iteration 2/1000 | Loss: 0.00004551
Iteration 3/1000 | Loss: 0.00002882
Iteration 4/1000 | Loss: 0.00002457
Iteration 5/1000 | Loss: 0.00002247
Iteration 6/1000 | Loss: 0.00002115
Iteration 7/1000 | Loss: 0.00001995
Iteration 8/1000 | Loss: 0.00001914
Iteration 9/1000 | Loss: 0.00001855
Iteration 10/1000 | Loss: 0.00001799
Iteration 11/1000 | Loss: 0.00001764
Iteration 12/1000 | Loss: 0.00001764
Iteration 13/1000 | Loss: 0.00001755
Iteration 14/1000 | Loss: 0.00001748
Iteration 15/1000 | Loss: 0.00001744
Iteration 16/1000 | Loss: 0.00001742
Iteration 17/1000 | Loss: 0.00001739
Iteration 18/1000 | Loss: 0.00001735
Iteration 19/1000 | Loss: 0.00001734
Iteration 20/1000 | Loss: 0.00001734
Iteration 21/1000 | Loss: 0.00001733
Iteration 22/1000 | Loss: 0.00001730
Iteration 23/1000 | Loss: 0.00001727
Iteration 24/1000 | Loss: 0.00001727
Iteration 25/1000 | Loss: 0.00001726
Iteration 26/1000 | Loss: 0.00001726
Iteration 27/1000 | Loss: 0.00001725
Iteration 28/1000 | Loss: 0.00001724
Iteration 29/1000 | Loss: 0.00001723
Iteration 30/1000 | Loss: 0.00001722
Iteration 31/1000 | Loss: 0.00001722
Iteration 32/1000 | Loss: 0.00001722
Iteration 33/1000 | Loss: 0.00001722
Iteration 34/1000 | Loss: 0.00001721
Iteration 35/1000 | Loss: 0.00001721
Iteration 36/1000 | Loss: 0.00001721
Iteration 37/1000 | Loss: 0.00001721
Iteration 38/1000 | Loss: 0.00001721
Iteration 39/1000 | Loss: 0.00001720
Iteration 40/1000 | Loss: 0.00001718
Iteration 41/1000 | Loss: 0.00001718
Iteration 42/1000 | Loss: 0.00001717
Iteration 43/1000 | Loss: 0.00001717
Iteration 44/1000 | Loss: 0.00001716
Iteration 45/1000 | Loss: 0.00001716
Iteration 46/1000 | Loss: 0.00001716
Iteration 47/1000 | Loss: 0.00001716
Iteration 48/1000 | Loss: 0.00001716
Iteration 49/1000 | Loss: 0.00001716
Iteration 50/1000 | Loss: 0.00001716
Iteration 51/1000 | Loss: 0.00001715
Iteration 52/1000 | Loss: 0.00001715
Iteration 53/1000 | Loss: 0.00001714
Iteration 54/1000 | Loss: 0.00001714
Iteration 55/1000 | Loss: 0.00001714
Iteration 56/1000 | Loss: 0.00001714
Iteration 57/1000 | Loss: 0.00001714
Iteration 58/1000 | Loss: 0.00001714
Iteration 59/1000 | Loss: 0.00001714
Iteration 60/1000 | Loss: 0.00001714
Iteration 61/1000 | Loss: 0.00001713
Iteration 62/1000 | Loss: 0.00001713
Iteration 63/1000 | Loss: 0.00001713
Iteration 64/1000 | Loss: 0.00001713
Iteration 65/1000 | Loss: 0.00001712
Iteration 66/1000 | Loss: 0.00001712
Iteration 67/1000 | Loss: 0.00001712
Iteration 68/1000 | Loss: 0.00001712
Iteration 69/1000 | Loss: 0.00001711
Iteration 70/1000 | Loss: 0.00001711
Iteration 71/1000 | Loss: 0.00001711
Iteration 72/1000 | Loss: 0.00001711
Iteration 73/1000 | Loss: 0.00001710
Iteration 74/1000 | Loss: 0.00001710
Iteration 75/1000 | Loss: 0.00001710
Iteration 76/1000 | Loss: 0.00001710
Iteration 77/1000 | Loss: 0.00001710
Iteration 78/1000 | Loss: 0.00001710
Iteration 79/1000 | Loss: 0.00001709
Iteration 80/1000 | Loss: 0.00001709
Iteration 81/1000 | Loss: 0.00001709
Iteration 82/1000 | Loss: 0.00001709
Iteration 83/1000 | Loss: 0.00001708
Iteration 84/1000 | Loss: 0.00001708
Iteration 85/1000 | Loss: 0.00001708
Iteration 86/1000 | Loss: 0.00001708
Iteration 87/1000 | Loss: 0.00001708
Iteration 88/1000 | Loss: 0.00001708
Iteration 89/1000 | Loss: 0.00001708
Iteration 90/1000 | Loss: 0.00001707
Iteration 91/1000 | Loss: 0.00001707
Iteration 92/1000 | Loss: 0.00001707
Iteration 93/1000 | Loss: 0.00001707
Iteration 94/1000 | Loss: 0.00001707
Iteration 95/1000 | Loss: 0.00001707
Iteration 96/1000 | Loss: 0.00001707
Iteration 97/1000 | Loss: 0.00001707
Iteration 98/1000 | Loss: 0.00001707
Iteration 99/1000 | Loss: 0.00001707
Iteration 100/1000 | Loss: 0.00001707
Iteration 101/1000 | Loss: 0.00001707
Iteration 102/1000 | Loss: 0.00001707
Iteration 103/1000 | Loss: 0.00001707
Iteration 104/1000 | Loss: 0.00001706
Iteration 105/1000 | Loss: 0.00001706
Iteration 106/1000 | Loss: 0.00001706
Iteration 107/1000 | Loss: 0.00001706
Iteration 108/1000 | Loss: 0.00001706
Iteration 109/1000 | Loss: 0.00001706
Iteration 110/1000 | Loss: 0.00001706
Iteration 111/1000 | Loss: 0.00001706
Iteration 112/1000 | Loss: 0.00001706
Iteration 113/1000 | Loss: 0.00001706
Iteration 114/1000 | Loss: 0.00001706
Iteration 115/1000 | Loss: 0.00001706
Iteration 116/1000 | Loss: 0.00001706
Iteration 117/1000 | Loss: 0.00001706
Iteration 118/1000 | Loss: 0.00001706
Iteration 119/1000 | Loss: 0.00001706
Iteration 120/1000 | Loss: 0.00001706
Iteration 121/1000 | Loss: 0.00001706
Iteration 122/1000 | Loss: 0.00001706
Iteration 123/1000 | Loss: 0.00001706
Iteration 124/1000 | Loss: 0.00001706
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.7059868696378544e-05, 1.7059868696378544e-05, 1.7059868696378544e-05, 1.7059868696378544e-05, 1.7059868696378544e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7059868696378544e-05

Optimization complete. Final v2v error: 3.59890079498291 mm

Highest mean error: 3.8384881019592285 mm for frame 158

Lowest mean error: 3.2006194591522217 mm for frame 263

Saving results

Total time: 42.03713011741638
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5760/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5760/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5760/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00761843
Iteration 2/25 | Loss: 0.00159483
Iteration 3/25 | Loss: 0.00139140
Iteration 4/25 | Loss: 0.00136402
Iteration 5/25 | Loss: 0.00135847
Iteration 6/25 | Loss: 0.00135783
Iteration 7/25 | Loss: 0.00135783
Iteration 8/25 | Loss: 0.00135783
Iteration 9/25 | Loss: 0.00135783
Iteration 10/25 | Loss: 0.00135783
Iteration 11/25 | Loss: 0.00135783
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013578332727774978, 0.0013578332727774978, 0.0013578332727774978, 0.0013578332727774978, 0.0013578332727774978]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013578332727774978

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.51702785
Iteration 2/25 | Loss: 0.00208219
Iteration 3/25 | Loss: 0.00208213
Iteration 4/25 | Loss: 0.00208213
Iteration 5/25 | Loss: 0.00208213
Iteration 6/25 | Loss: 0.00208213
Iteration 7/25 | Loss: 0.00208213
Iteration 8/25 | Loss: 0.00208213
Iteration 9/25 | Loss: 0.00208213
Iteration 10/25 | Loss: 0.00208213
Iteration 11/25 | Loss: 0.00208213
Iteration 12/25 | Loss: 0.00208213
Iteration 13/25 | Loss: 0.00208212
Iteration 14/25 | Loss: 0.00208212
Iteration 15/25 | Loss: 0.00208212
Iteration 16/25 | Loss: 0.00208212
Iteration 17/25 | Loss: 0.00208213
Iteration 18/25 | Loss: 0.00208212
Iteration 19/25 | Loss: 0.00208212
Iteration 20/25 | Loss: 0.00208212
Iteration 21/25 | Loss: 0.00208212
Iteration 22/25 | Loss: 0.00208212
Iteration 23/25 | Loss: 0.00208212
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.002082124585285783, 0.002082124585285783, 0.002082124585285783, 0.002082124585285783, 0.002082124585285783]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002082124585285783

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00208212
Iteration 2/1000 | Loss: 0.00006067
Iteration 3/1000 | Loss: 0.00003803
Iteration 4/1000 | Loss: 0.00003019
Iteration 5/1000 | Loss: 0.00002801
Iteration 6/1000 | Loss: 0.00002678
Iteration 7/1000 | Loss: 0.00002591
Iteration 8/1000 | Loss: 0.00002519
Iteration 9/1000 | Loss: 0.00002466
Iteration 10/1000 | Loss: 0.00002433
Iteration 11/1000 | Loss: 0.00002408
Iteration 12/1000 | Loss: 0.00002394
Iteration 13/1000 | Loss: 0.00002388
Iteration 14/1000 | Loss: 0.00002388
Iteration 15/1000 | Loss: 0.00002381
Iteration 16/1000 | Loss: 0.00002378
Iteration 17/1000 | Loss: 0.00002377
Iteration 18/1000 | Loss: 0.00002371
Iteration 19/1000 | Loss: 0.00002371
Iteration 20/1000 | Loss: 0.00002371
Iteration 21/1000 | Loss: 0.00002371
Iteration 22/1000 | Loss: 0.00002371
Iteration 23/1000 | Loss: 0.00002371
Iteration 24/1000 | Loss: 0.00002370
Iteration 25/1000 | Loss: 0.00002369
Iteration 26/1000 | Loss: 0.00002369
Iteration 27/1000 | Loss: 0.00002368
Iteration 28/1000 | Loss: 0.00002368
Iteration 29/1000 | Loss: 0.00002368
Iteration 30/1000 | Loss: 0.00002368
Iteration 31/1000 | Loss: 0.00002367
Iteration 32/1000 | Loss: 0.00002367
Iteration 33/1000 | Loss: 0.00002367
Iteration 34/1000 | Loss: 0.00002366
Iteration 35/1000 | Loss: 0.00002366
Iteration 36/1000 | Loss: 0.00002366
Iteration 37/1000 | Loss: 0.00002365
Iteration 38/1000 | Loss: 0.00002365
Iteration 39/1000 | Loss: 0.00002365
Iteration 40/1000 | Loss: 0.00002365
Iteration 41/1000 | Loss: 0.00002364
Iteration 42/1000 | Loss: 0.00002364
Iteration 43/1000 | Loss: 0.00002364
Iteration 44/1000 | Loss: 0.00002363
Iteration 45/1000 | Loss: 0.00002363
Iteration 46/1000 | Loss: 0.00002363
Iteration 47/1000 | Loss: 0.00002362
Iteration 48/1000 | Loss: 0.00002362
Iteration 49/1000 | Loss: 0.00002362
Iteration 50/1000 | Loss: 0.00002361
Iteration 51/1000 | Loss: 0.00002361
Iteration 52/1000 | Loss: 0.00002361
Iteration 53/1000 | Loss: 0.00002361
Iteration 54/1000 | Loss: 0.00002360
Iteration 55/1000 | Loss: 0.00002360
Iteration 56/1000 | Loss: 0.00002360
Iteration 57/1000 | Loss: 0.00002360
Iteration 58/1000 | Loss: 0.00002360
Iteration 59/1000 | Loss: 0.00002359
Iteration 60/1000 | Loss: 0.00002359
Iteration 61/1000 | Loss: 0.00002358
Iteration 62/1000 | Loss: 0.00002358
Iteration 63/1000 | Loss: 0.00002357
Iteration 64/1000 | Loss: 0.00002357
Iteration 65/1000 | Loss: 0.00002357
Iteration 66/1000 | Loss: 0.00002357
Iteration 67/1000 | Loss: 0.00002357
Iteration 68/1000 | Loss: 0.00002356
Iteration 69/1000 | Loss: 0.00002356
Iteration 70/1000 | Loss: 0.00002356
Iteration 71/1000 | Loss: 0.00002355
Iteration 72/1000 | Loss: 0.00002355
Iteration 73/1000 | Loss: 0.00002354
Iteration 74/1000 | Loss: 0.00002354
Iteration 75/1000 | Loss: 0.00002354
Iteration 76/1000 | Loss: 0.00002354
Iteration 77/1000 | Loss: 0.00002354
Iteration 78/1000 | Loss: 0.00002354
Iteration 79/1000 | Loss: 0.00002354
Iteration 80/1000 | Loss: 0.00002353
Iteration 81/1000 | Loss: 0.00002353
Iteration 82/1000 | Loss: 0.00002353
Iteration 83/1000 | Loss: 0.00002353
Iteration 84/1000 | Loss: 0.00002353
Iteration 85/1000 | Loss: 0.00002353
Iteration 86/1000 | Loss: 0.00002352
Iteration 87/1000 | Loss: 0.00002352
Iteration 88/1000 | Loss: 0.00002352
Iteration 89/1000 | Loss: 0.00002352
Iteration 90/1000 | Loss: 0.00002352
Iteration 91/1000 | Loss: 0.00002352
Iteration 92/1000 | Loss: 0.00002351
Iteration 93/1000 | Loss: 0.00002351
Iteration 94/1000 | Loss: 0.00002351
Iteration 95/1000 | Loss: 0.00002351
Iteration 96/1000 | Loss: 0.00002351
Iteration 97/1000 | Loss: 0.00002351
Iteration 98/1000 | Loss: 0.00002351
Iteration 99/1000 | Loss: 0.00002351
Iteration 100/1000 | Loss: 0.00002351
Iteration 101/1000 | Loss: 0.00002351
Iteration 102/1000 | Loss: 0.00002351
Iteration 103/1000 | Loss: 0.00002351
Iteration 104/1000 | Loss: 0.00002351
Iteration 105/1000 | Loss: 0.00002351
Iteration 106/1000 | Loss: 0.00002351
Iteration 107/1000 | Loss: 0.00002351
Iteration 108/1000 | Loss: 0.00002351
Iteration 109/1000 | Loss: 0.00002351
Iteration 110/1000 | Loss: 0.00002351
Iteration 111/1000 | Loss: 0.00002351
Iteration 112/1000 | Loss: 0.00002351
Iteration 113/1000 | Loss: 0.00002351
Iteration 114/1000 | Loss: 0.00002351
Iteration 115/1000 | Loss: 0.00002351
Iteration 116/1000 | Loss: 0.00002351
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [2.3512473489972763e-05, 2.3512473489972763e-05, 2.3512473489972763e-05, 2.3512473489972763e-05, 2.3512473489972763e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3512473489972763e-05

Optimization complete. Final v2v error: 3.949782371520996 mm

Highest mean error: 4.767830848693848 mm for frame 166

Lowest mean error: 3.295457363128662 mm for frame 239

Saving results

Total time: 40.415257930755615
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5760/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5760/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5760/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01076241
Iteration 2/25 | Loss: 0.01076240
Iteration 3/25 | Loss: 0.00278046
Iteration 4/25 | Loss: 0.00208056
Iteration 5/25 | Loss: 0.00182821
Iteration 6/25 | Loss: 0.00170933
Iteration 7/25 | Loss: 0.00165280
Iteration 8/25 | Loss: 0.00153547
Iteration 9/25 | Loss: 0.00151699
Iteration 10/25 | Loss: 0.00148749
Iteration 11/25 | Loss: 0.00146308
Iteration 12/25 | Loss: 0.00145706
Iteration 13/25 | Loss: 0.00143358
Iteration 14/25 | Loss: 0.00143920
Iteration 15/25 | Loss: 0.00143414
Iteration 16/25 | Loss: 0.00142233
Iteration 17/25 | Loss: 0.00141847
Iteration 18/25 | Loss: 0.00141173
Iteration 19/25 | Loss: 0.00140364
Iteration 20/25 | Loss: 0.00139715
Iteration 21/25 | Loss: 0.00138579
Iteration 22/25 | Loss: 0.00138412
Iteration 23/25 | Loss: 0.00137773
Iteration 24/25 | Loss: 0.00137590
Iteration 25/25 | Loss: 0.00137465

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14295745
Iteration 2/25 | Loss: 0.00513800
Iteration 3/25 | Loss: 0.00461430
Iteration 4/25 | Loss: 0.00461430
Iteration 5/25 | Loss: 0.00461429
Iteration 6/25 | Loss: 0.00461429
Iteration 7/25 | Loss: 0.00461429
Iteration 8/25 | Loss: 0.00461429
Iteration 9/25 | Loss: 0.00461429
Iteration 10/25 | Loss: 0.00461429
Iteration 11/25 | Loss: 0.00461429
Iteration 12/25 | Loss: 0.00461429
Iteration 13/25 | Loss: 0.00461429
Iteration 14/25 | Loss: 0.00461429
Iteration 15/25 | Loss: 0.00461429
Iteration 16/25 | Loss: 0.00461429
Iteration 17/25 | Loss: 0.00461429
Iteration 18/25 | Loss: 0.00461429
Iteration 19/25 | Loss: 0.00461429
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.004614292178303003, 0.004614292178303003, 0.004614292178303003, 0.004614292178303003, 0.004614292178303003]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004614292178303003

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00461429
Iteration 2/1000 | Loss: 0.00054222
Iteration 3/1000 | Loss: 0.00047300
Iteration 4/1000 | Loss: 0.00032891
Iteration 5/1000 | Loss: 0.00047676
Iteration 6/1000 | Loss: 0.00011350
Iteration 7/1000 | Loss: 0.00047559
Iteration 8/1000 | Loss: 0.00009200
Iteration 9/1000 | Loss: 0.00014222
Iteration 10/1000 | Loss: 0.00010010
Iteration 11/1000 | Loss: 0.00036675
Iteration 12/1000 | Loss: 0.00007084
Iteration 13/1000 | Loss: 0.00016714
Iteration 14/1000 | Loss: 0.00006452
Iteration 15/1000 | Loss: 0.00005902
Iteration 16/1000 | Loss: 0.00005511
Iteration 17/1000 | Loss: 0.00021861
Iteration 18/1000 | Loss: 0.00016884
Iteration 19/1000 | Loss: 0.00016858
Iteration 20/1000 | Loss: 0.00010146
Iteration 21/1000 | Loss: 0.00005377
Iteration 22/1000 | Loss: 0.00024708
Iteration 23/1000 | Loss: 0.00007171
Iteration 24/1000 | Loss: 0.00019890
Iteration 25/1000 | Loss: 0.00017959
Iteration 26/1000 | Loss: 0.00019506
Iteration 27/1000 | Loss: 0.00027178
Iteration 28/1000 | Loss: 0.00012947
Iteration 29/1000 | Loss: 0.00007655
Iteration 30/1000 | Loss: 0.00005052
Iteration 31/1000 | Loss: 0.00004653
Iteration 32/1000 | Loss: 0.00004384
Iteration 33/1000 | Loss: 0.00004222
Iteration 34/1000 | Loss: 0.00004072
Iteration 35/1000 | Loss: 0.00004565
Iteration 36/1000 | Loss: 0.00016194
Iteration 37/1000 | Loss: 0.00037887
Iteration 38/1000 | Loss: 0.00031318
Iteration 39/1000 | Loss: 0.00010305
Iteration 40/1000 | Loss: 0.00011058
Iteration 41/1000 | Loss: 0.00004377
Iteration 42/1000 | Loss: 0.00003978
Iteration 43/1000 | Loss: 0.00011592
Iteration 44/1000 | Loss: 0.00003641
Iteration 45/1000 | Loss: 0.00003477
Iteration 46/1000 | Loss: 0.00003641
Iteration 47/1000 | Loss: 0.00003377
Iteration 48/1000 | Loss: 0.00003637
Iteration 49/1000 | Loss: 0.00003336
Iteration 50/1000 | Loss: 0.00003294
Iteration 51/1000 | Loss: 0.00003277
Iteration 52/1000 | Loss: 0.00003275
Iteration 53/1000 | Loss: 0.00003257
Iteration 54/1000 | Loss: 0.00003254
Iteration 55/1000 | Loss: 0.00003243
Iteration 56/1000 | Loss: 0.00003215
Iteration 57/1000 | Loss: 0.00003821
Iteration 58/1000 | Loss: 0.00003377
Iteration 59/1000 | Loss: 0.00003492
Iteration 60/1000 | Loss: 0.00003237
Iteration 61/1000 | Loss: 0.00003172
Iteration 62/1000 | Loss: 0.00003984
Iteration 63/1000 | Loss: 0.00003302
Iteration 64/1000 | Loss: 0.00003451
Iteration 65/1000 | Loss: 0.00003203
Iteration 66/1000 | Loss: 0.00003505
Iteration 67/1000 | Loss: 0.00003190
Iteration 68/1000 | Loss: 0.00003189
Iteration 69/1000 | Loss: 0.00003812
Iteration 70/1000 | Loss: 0.00003655
Iteration 71/1000 | Loss: 0.00003485
Iteration 72/1000 | Loss: 0.00003163
Iteration 73/1000 | Loss: 0.00003449
Iteration 74/1000 | Loss: 0.00003685
Iteration 75/1000 | Loss: 0.00003530
Iteration 76/1000 | Loss: 0.00003127
Iteration 77/1000 | Loss: 0.00003500
Iteration 78/1000 | Loss: 0.00003463
Iteration 79/1000 | Loss: 0.00003554
Iteration 80/1000 | Loss: 0.00003447
Iteration 81/1000 | Loss: 0.00003436
Iteration 82/1000 | Loss: 0.00013264
Iteration 83/1000 | Loss: 0.00031318
Iteration 84/1000 | Loss: 0.00018545
Iteration 85/1000 | Loss: 0.00004273
Iteration 86/1000 | Loss: 0.00003906
Iteration 87/1000 | Loss: 0.00003106
Iteration 88/1000 | Loss: 0.00002917
Iteration 89/1000 | Loss: 0.00002624
Iteration 90/1000 | Loss: 0.00002503
Iteration 91/1000 | Loss: 0.00002391
Iteration 92/1000 | Loss: 0.00002277
Iteration 93/1000 | Loss: 0.00002235
Iteration 94/1000 | Loss: 0.00002365
Iteration 95/1000 | Loss: 0.00002618
Iteration 96/1000 | Loss: 0.00002290
Iteration 97/1000 | Loss: 0.00002482
Iteration 98/1000 | Loss: 0.00002237
Iteration 99/1000 | Loss: 0.00002377
Iteration 100/1000 | Loss: 0.00002450
Iteration 101/1000 | Loss: 0.00002196
Iteration 102/1000 | Loss: 0.00002176
Iteration 103/1000 | Loss: 0.00002175
Iteration 104/1000 | Loss: 0.00002175
Iteration 105/1000 | Loss: 0.00002175
Iteration 106/1000 | Loss: 0.00002174
Iteration 107/1000 | Loss: 0.00002158
Iteration 108/1000 | Loss: 0.00002153
Iteration 109/1000 | Loss: 0.00002149
Iteration 110/1000 | Loss: 0.00002503
Iteration 111/1000 | Loss: 0.00002245
Iteration 112/1000 | Loss: 0.00002281
Iteration 113/1000 | Loss: 0.00002192
Iteration 114/1000 | Loss: 0.00002506
Iteration 115/1000 | Loss: 0.00002255
Iteration 116/1000 | Loss: 0.00002372
Iteration 117/1000 | Loss: 0.00002151
Iteration 118/1000 | Loss: 0.00002148
Iteration 119/1000 | Loss: 0.00002148
Iteration 120/1000 | Loss: 0.00002147
Iteration 121/1000 | Loss: 0.00002146
Iteration 122/1000 | Loss: 0.00002146
Iteration 123/1000 | Loss: 0.00002146
Iteration 124/1000 | Loss: 0.00002145
Iteration 125/1000 | Loss: 0.00002145
Iteration 126/1000 | Loss: 0.00002145
Iteration 127/1000 | Loss: 0.00002144
Iteration 128/1000 | Loss: 0.00002144
Iteration 129/1000 | Loss: 0.00002144
Iteration 130/1000 | Loss: 0.00002144
Iteration 131/1000 | Loss: 0.00002144
Iteration 132/1000 | Loss: 0.00002144
Iteration 133/1000 | Loss: 0.00002144
Iteration 134/1000 | Loss: 0.00002144
Iteration 135/1000 | Loss: 0.00002144
Iteration 136/1000 | Loss: 0.00002144
Iteration 137/1000 | Loss: 0.00002144
Iteration 138/1000 | Loss: 0.00002143
Iteration 139/1000 | Loss: 0.00002143
Iteration 140/1000 | Loss: 0.00002143
Iteration 141/1000 | Loss: 0.00002143
Iteration 142/1000 | Loss: 0.00002143
Iteration 143/1000 | Loss: 0.00002143
Iteration 144/1000 | Loss: 0.00002143
Iteration 145/1000 | Loss: 0.00002143
Iteration 146/1000 | Loss: 0.00002143
Iteration 147/1000 | Loss: 0.00002143
Iteration 148/1000 | Loss: 0.00002143
Iteration 149/1000 | Loss: 0.00002143
Iteration 150/1000 | Loss: 0.00002143
Iteration 151/1000 | Loss: 0.00002143
Iteration 152/1000 | Loss: 0.00002143
Iteration 153/1000 | Loss: 0.00002143
Iteration 154/1000 | Loss: 0.00002143
Iteration 155/1000 | Loss: 0.00002143
Iteration 156/1000 | Loss: 0.00002143
Iteration 157/1000 | Loss: 0.00002142
Iteration 158/1000 | Loss: 0.00002142
Iteration 159/1000 | Loss: 0.00002142
Iteration 160/1000 | Loss: 0.00002142
Iteration 161/1000 | Loss: 0.00002141
Iteration 162/1000 | Loss: 0.00002141
Iteration 163/1000 | Loss: 0.00002141
Iteration 164/1000 | Loss: 0.00002140
Iteration 165/1000 | Loss: 0.00002140
Iteration 166/1000 | Loss: 0.00002140
Iteration 167/1000 | Loss: 0.00002140
Iteration 168/1000 | Loss: 0.00002140
Iteration 169/1000 | Loss: 0.00002140
Iteration 170/1000 | Loss: 0.00002140
Iteration 171/1000 | Loss: 0.00002140
Iteration 172/1000 | Loss: 0.00002140
Iteration 173/1000 | Loss: 0.00002140
Iteration 174/1000 | Loss: 0.00002140
Iteration 175/1000 | Loss: 0.00002140
Iteration 176/1000 | Loss: 0.00002140
Iteration 177/1000 | Loss: 0.00002140
Iteration 178/1000 | Loss: 0.00002139
Iteration 179/1000 | Loss: 0.00002139
Iteration 180/1000 | Loss: 0.00002139
Iteration 181/1000 | Loss: 0.00002139
Iteration 182/1000 | Loss: 0.00002139
Iteration 183/1000 | Loss: 0.00002139
Iteration 184/1000 | Loss: 0.00002139
Iteration 185/1000 | Loss: 0.00002139
Iteration 186/1000 | Loss: 0.00002139
Iteration 187/1000 | Loss: 0.00002138
Iteration 188/1000 | Loss: 0.00002138
Iteration 189/1000 | Loss: 0.00002138
Iteration 190/1000 | Loss: 0.00002138
Iteration 191/1000 | Loss: 0.00002138
Iteration 192/1000 | Loss: 0.00002138
Iteration 193/1000 | Loss: 0.00002138
Iteration 194/1000 | Loss: 0.00002138
Iteration 195/1000 | Loss: 0.00002137
Iteration 196/1000 | Loss: 0.00002137
Iteration 197/1000 | Loss: 0.00002137
Iteration 198/1000 | Loss: 0.00002137
Iteration 199/1000 | Loss: 0.00002137
Iteration 200/1000 | Loss: 0.00002136
Iteration 201/1000 | Loss: 0.00002136
Iteration 202/1000 | Loss: 0.00002136
Iteration 203/1000 | Loss: 0.00002136
Iteration 204/1000 | Loss: 0.00002136
Iteration 205/1000 | Loss: 0.00002136
Iteration 206/1000 | Loss: 0.00002136
Iteration 207/1000 | Loss: 0.00002135
Iteration 208/1000 | Loss: 0.00002135
Iteration 209/1000 | Loss: 0.00002135
Iteration 210/1000 | Loss: 0.00002135
Iteration 211/1000 | Loss: 0.00002135
Iteration 212/1000 | Loss: 0.00002135
Iteration 213/1000 | Loss: 0.00002135
Iteration 214/1000 | Loss: 0.00002135
Iteration 215/1000 | Loss: 0.00002134
Iteration 216/1000 | Loss: 0.00002134
Iteration 217/1000 | Loss: 0.00002134
Iteration 218/1000 | Loss: 0.00002549
Iteration 219/1000 | Loss: 0.00002220
Iteration 220/1000 | Loss: 0.00002165
Iteration 221/1000 | Loss: 0.00002164
Iteration 222/1000 | Loss: 0.00002163
Iteration 223/1000 | Loss: 0.00002163
Iteration 224/1000 | Loss: 0.00002234
Iteration 225/1000 | Loss: 0.00002219
Iteration 226/1000 | Loss: 0.00002208
Iteration 227/1000 | Loss: 0.00002195
Iteration 228/1000 | Loss: 0.00002227
Iteration 229/1000 | Loss: 0.00002149
Iteration 230/1000 | Loss: 0.00002148
Iteration 231/1000 | Loss: 0.00002148
Iteration 232/1000 | Loss: 0.00002148
Iteration 233/1000 | Loss: 0.00002147
Iteration 234/1000 | Loss: 0.00002147
Iteration 235/1000 | Loss: 0.00002147
Iteration 236/1000 | Loss: 0.00002147
Iteration 237/1000 | Loss: 0.00002146
Iteration 238/1000 | Loss: 0.00002146
Iteration 239/1000 | Loss: 0.00002146
Iteration 240/1000 | Loss: 0.00002145
Iteration 241/1000 | Loss: 0.00002145
Iteration 242/1000 | Loss: 0.00002145
Iteration 243/1000 | Loss: 0.00002145
Iteration 244/1000 | Loss: 0.00002145
Iteration 245/1000 | Loss: 0.00002145
Iteration 246/1000 | Loss: 0.00002145
Iteration 247/1000 | Loss: 0.00002145
Iteration 248/1000 | Loss: 0.00002144
Iteration 249/1000 | Loss: 0.00002144
Iteration 250/1000 | Loss: 0.00002144
Iteration 251/1000 | Loss: 0.00002144
Iteration 252/1000 | Loss: 0.00002144
Iteration 253/1000 | Loss: 0.00002143
Iteration 254/1000 | Loss: 0.00002143
Iteration 255/1000 | Loss: 0.00002142
Iteration 256/1000 | Loss: 0.00002141
Iteration 257/1000 | Loss: 0.00002141
Iteration 258/1000 | Loss: 0.00002141
Iteration 259/1000 | Loss: 0.00002141
Iteration 260/1000 | Loss: 0.00002140
Iteration 261/1000 | Loss: 0.00002140
Iteration 262/1000 | Loss: 0.00002140
Iteration 263/1000 | Loss: 0.00002140
Iteration 264/1000 | Loss: 0.00002438
Iteration 265/1000 | Loss: 0.00002190
Iteration 266/1000 | Loss: 0.00002139
Iteration 267/1000 | Loss: 0.00002139
Iteration 268/1000 | Loss: 0.00002408
Iteration 269/1000 | Loss: 0.00002193
Iteration 270/1000 | Loss: 0.00002407
Iteration 271/1000 | Loss: 0.00002174
Iteration 272/1000 | Loss: 0.00002139
Iteration 273/1000 | Loss: 0.00002138
Iteration 274/1000 | Loss: 0.00002377
Iteration 275/1000 | Loss: 0.00002188
Iteration 276/1000 | Loss: 0.00002230
Iteration 277/1000 | Loss: 0.00002515
Iteration 278/1000 | Loss: 0.00002261
Iteration 279/1000 | Loss: 0.00002138
Iteration 280/1000 | Loss: 0.00002138
Iteration 281/1000 | Loss: 0.00002138
Iteration 282/1000 | Loss: 0.00002138
Iteration 283/1000 | Loss: 0.00002138
Iteration 284/1000 | Loss: 0.00002373
Iteration 285/1000 | Loss: 0.00002223
Iteration 286/1000 | Loss: 0.00002225
Iteration 287/1000 | Loss: 0.00002224
Iteration 288/1000 | Loss: 0.00002764
Iteration 289/1000 | Loss: 0.00002436
Iteration 290/1000 | Loss: 0.00002505
Iteration 291/1000 | Loss: 0.00002412
Iteration 292/1000 | Loss: 0.00002383
Iteration 293/1000 | Loss: 0.00002228
Iteration 294/1000 | Loss: 0.00002139
Iteration 295/1000 | Loss: 0.00002139
Iteration 296/1000 | Loss: 0.00002139
Iteration 297/1000 | Loss: 0.00002138
Iteration 298/1000 | Loss: 0.00002138
Iteration 299/1000 | Loss: 0.00002138
Iteration 300/1000 | Loss: 0.00002138
Iteration 301/1000 | Loss: 0.00002138
Iteration 302/1000 | Loss: 0.00002138
Iteration 303/1000 | Loss: 0.00002327
Iteration 304/1000 | Loss: 0.00002207
Iteration 305/1000 | Loss: 0.00002454
Iteration 306/1000 | Loss: 0.00002342
Iteration 307/1000 | Loss: 0.00002231
Iteration 308/1000 | Loss: 0.00002175
Iteration 309/1000 | Loss: 0.00002152
Iteration 310/1000 | Loss: 0.00002142
Iteration 311/1000 | Loss: 0.00002142
Iteration 312/1000 | Loss: 0.00002142
Iteration 313/1000 | Loss: 0.00002141
Iteration 314/1000 | Loss: 0.00002141
Iteration 315/1000 | Loss: 0.00002155
Iteration 316/1000 | Loss: 0.00002489
Iteration 317/1000 | Loss: 0.00002200
Iteration 318/1000 | Loss: 0.00002193
Iteration 319/1000 | Loss: 0.00002154
Iteration 320/1000 | Loss: 0.00002153
Iteration 321/1000 | Loss: 0.00002152
Iteration 322/1000 | Loss: 0.00002923
Iteration 323/1000 | Loss: 0.00002352
Iteration 324/1000 | Loss: 0.00002140
Iteration 325/1000 | Loss: 0.00002139
Iteration 326/1000 | Loss: 0.00002139
Iteration 327/1000 | Loss: 0.00002139
Iteration 328/1000 | Loss: 0.00002139
Iteration 329/1000 | Loss: 0.00002139
Iteration 330/1000 | Loss: 0.00002139
Iteration 331/1000 | Loss: 0.00002138
Iteration 332/1000 | Loss: 0.00002202
Iteration 333/1000 | Loss: 0.00002518
Iteration 334/1000 | Loss: 0.00002263
Iteration 335/1000 | Loss: 0.00002211
Iteration 336/1000 | Loss: 0.00003022
Iteration 337/1000 | Loss: 0.00002328
Iteration 338/1000 | Loss: 0.00002150
Iteration 339/1000 | Loss: 0.00002149
Iteration 340/1000 | Loss: 0.00002218
Iteration 341/1000 | Loss: 0.00002218
Iteration 342/1000 | Loss: 0.00002245
Iteration 343/1000 | Loss: 0.00003119
Iteration 344/1000 | Loss: 0.00002205
Iteration 345/1000 | Loss: 0.00002192
Iteration 346/1000 | Loss: 0.00002353
Iteration 347/1000 | Loss: 0.00002611
Iteration 348/1000 | Loss: 0.00002605
Iteration 349/1000 | Loss: 0.00002450
Iteration 350/1000 | Loss: 0.00002469
Iteration 351/1000 | Loss: 0.00002428
Iteration 352/1000 | Loss: 0.00002447
Iteration 353/1000 | Loss: 0.00002380
Iteration 354/1000 | Loss: 0.00002377
Iteration 355/1000 | Loss: 0.00002348
Iteration 356/1000 | Loss: 0.00002335
Iteration 357/1000 | Loss: 0.00002402
Iteration 358/1000 | Loss: 0.00002326
Iteration 359/1000 | Loss: 0.00002548
Iteration 360/1000 | Loss: 0.00002425
Iteration 361/1000 | Loss: 0.00002508
Iteration 362/1000 | Loss: 0.00002508
Iteration 363/1000 | Loss: 0.00002508
Iteration 364/1000 | Loss: 0.00002507
Iteration 365/1000 | Loss: 0.00002328
Iteration 366/1000 | Loss: 0.00002234
Iteration 367/1000 | Loss: 0.00002251
Iteration 368/1000 | Loss: 0.00002144
Iteration 369/1000 | Loss: 0.00002142
Iteration 370/1000 | Loss: 0.00002142
Iteration 371/1000 | Loss: 0.00002142
Iteration 372/1000 | Loss: 0.00002142
Iteration 373/1000 | Loss: 0.00002141
Iteration 374/1000 | Loss: 0.00002141
Iteration 375/1000 | Loss: 0.00002141
Iteration 376/1000 | Loss: 0.00002141
Iteration 377/1000 | Loss: 0.00002141
Iteration 378/1000 | Loss: 0.00002140
Iteration 379/1000 | Loss: 0.00002140
Iteration 380/1000 | Loss: 0.00002139
Iteration 381/1000 | Loss: 0.00002139
Iteration 382/1000 | Loss: 0.00002139
Iteration 383/1000 | Loss: 0.00002138
Iteration 384/1000 | Loss: 0.00002138
Iteration 385/1000 | Loss: 0.00002137
Iteration 386/1000 | Loss: 0.00002137
Iteration 387/1000 | Loss: 0.00002137
Iteration 388/1000 | Loss: 0.00002137
Iteration 389/1000 | Loss: 0.00002136
Iteration 390/1000 | Loss: 0.00002136
Iteration 391/1000 | Loss: 0.00002221
Iteration 392/1000 | Loss: 0.00002646
Iteration 393/1000 | Loss: 0.00002504
Iteration 394/1000 | Loss: 0.00002392
Iteration 395/1000 | Loss: 0.00002377
Iteration 396/1000 | Loss: 0.00002137
Iteration 397/1000 | Loss: 0.00002136
Iteration 398/1000 | Loss: 0.00002136
Iteration 399/1000 | Loss: 0.00002135
Iteration 400/1000 | Loss: 0.00002135
Iteration 401/1000 | Loss: 0.00002135
Iteration 402/1000 | Loss: 0.00002135
Iteration 403/1000 | Loss: 0.00002135
Iteration 404/1000 | Loss: 0.00002135
Iteration 405/1000 | Loss: 0.00002135
Iteration 406/1000 | Loss: 0.00002135
Iteration 407/1000 | Loss: 0.00002135
Iteration 408/1000 | Loss: 0.00002134
Iteration 409/1000 | Loss: 0.00002134
Iteration 410/1000 | Loss: 0.00002134
Iteration 411/1000 | Loss: 0.00002134
Iteration 412/1000 | Loss: 0.00002134
Iteration 413/1000 | Loss: 0.00002134
Iteration 414/1000 | Loss: 0.00002134
Iteration 415/1000 | Loss: 0.00002134
Iteration 416/1000 | Loss: 0.00002134
Iteration 417/1000 | Loss: 0.00002134
Iteration 418/1000 | Loss: 0.00002134
Iteration 419/1000 | Loss: 0.00002134
Iteration 420/1000 | Loss: 0.00002134
Iteration 421/1000 | Loss: 0.00002134
Iteration 422/1000 | Loss: 0.00002134
Iteration 423/1000 | Loss: 0.00002134
Iteration 424/1000 | Loss: 0.00002134
Iteration 425/1000 | Loss: 0.00002134
Iteration 426/1000 | Loss: 0.00002134
Iteration 427/1000 | Loss: 0.00002134
Iteration 428/1000 | Loss: 0.00002134
Iteration 429/1000 | Loss: 0.00002133
Iteration 430/1000 | Loss: 0.00002133
Iteration 431/1000 | Loss: 0.00002133
Iteration 432/1000 | Loss: 0.00002133
Iteration 433/1000 | Loss: 0.00002133
Iteration 434/1000 | Loss: 0.00002133
Iteration 435/1000 | Loss: 0.00002133
Iteration 436/1000 | Loss: 0.00002133
Iteration 437/1000 | Loss: 0.00002133
Iteration 438/1000 | Loss: 0.00002133
Iteration 439/1000 | Loss: 0.00002133
Iteration 440/1000 | Loss: 0.00002133
Iteration 441/1000 | Loss: 0.00002133
Iteration 442/1000 | Loss: 0.00002133
Iteration 443/1000 | Loss: 0.00002133
Iteration 444/1000 | Loss: 0.00002133
Iteration 445/1000 | Loss: 0.00002133
Iteration 446/1000 | Loss: 0.00002133
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 446. Stopping optimization.
Last 5 losses: [2.132703593815677e-05, 2.132703593815677e-05, 2.132703593815677e-05, 2.132703593815677e-05, 2.132703593815677e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.132703593815677e-05

Optimization complete. Final v2v error: 3.5072073936462402 mm

Highest mean error: 12.036277770996094 mm for frame 140

Lowest mean error: 3.0709776878356934 mm for frame 128

Saving results

Total time: 355.0126438140869
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5760/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5760/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5760/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01056697
Iteration 2/25 | Loss: 0.00272694
Iteration 3/25 | Loss: 0.00205034
Iteration 4/25 | Loss: 0.00187416
Iteration 5/25 | Loss: 0.00168414
Iteration 6/25 | Loss: 0.00153084
Iteration 7/25 | Loss: 0.00145946
Iteration 8/25 | Loss: 0.00141546
Iteration 9/25 | Loss: 0.00139585
Iteration 10/25 | Loss: 0.00138568
Iteration 11/25 | Loss: 0.00136793
Iteration 12/25 | Loss: 0.00135708
Iteration 13/25 | Loss: 0.00135225
Iteration 14/25 | Loss: 0.00135637
Iteration 15/25 | Loss: 0.00135473
Iteration 16/25 | Loss: 0.00134945
Iteration 17/25 | Loss: 0.00134790
Iteration 18/25 | Loss: 0.00134731
Iteration 19/25 | Loss: 0.00134709
Iteration 20/25 | Loss: 0.00134701
Iteration 21/25 | Loss: 0.00134701
Iteration 22/25 | Loss: 0.00134700
Iteration 23/25 | Loss: 0.00134700
Iteration 24/25 | Loss: 0.00134700
Iteration 25/25 | Loss: 0.00134699

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.03652811
Iteration 2/25 | Loss: 0.00304785
Iteration 3/25 | Loss: 0.00304784
Iteration 4/25 | Loss: 0.00304784
Iteration 5/25 | Loss: 0.00304784
Iteration 6/25 | Loss: 0.00304784
Iteration 7/25 | Loss: 0.00304784
Iteration 8/25 | Loss: 0.00304784
Iteration 9/25 | Loss: 0.00304784
Iteration 10/25 | Loss: 0.00304784
Iteration 11/25 | Loss: 0.00304784
Iteration 12/25 | Loss: 0.00304784
Iteration 13/25 | Loss: 0.00304784
Iteration 14/25 | Loss: 0.00304784
Iteration 15/25 | Loss: 0.00304784
Iteration 16/25 | Loss: 0.00304784
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0030478357803076506, 0.0030478357803076506, 0.0030478357803076506, 0.0030478357803076506, 0.0030478357803076506]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0030478357803076506

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00304784
Iteration 2/1000 | Loss: 0.00036788
Iteration 3/1000 | Loss: 0.00010427
Iteration 4/1000 | Loss: 0.00007830
Iteration 5/1000 | Loss: 0.00006295
Iteration 6/1000 | Loss: 0.00005209
Iteration 7/1000 | Loss: 0.00004620
Iteration 8/1000 | Loss: 0.00004321
Iteration 9/1000 | Loss: 0.00004140
Iteration 10/1000 | Loss: 0.00003976
Iteration 11/1000 | Loss: 0.00003882
Iteration 12/1000 | Loss: 0.00003824
Iteration 13/1000 | Loss: 0.00003776
Iteration 14/1000 | Loss: 0.00003740
Iteration 15/1000 | Loss: 0.00026961
Iteration 16/1000 | Loss: 0.00049561
Iteration 17/1000 | Loss: 0.00060718
Iteration 18/1000 | Loss: 0.00006739
Iteration 19/1000 | Loss: 0.00005288
Iteration 20/1000 | Loss: 0.00004383
Iteration 21/1000 | Loss: 0.00003377
Iteration 22/1000 | Loss: 0.00002725
Iteration 23/1000 | Loss: 0.00002395
Iteration 24/1000 | Loss: 0.00002131
Iteration 25/1000 | Loss: 0.00002003
Iteration 26/1000 | Loss: 0.00001920
Iteration 27/1000 | Loss: 0.00001852
Iteration 28/1000 | Loss: 0.00001798
Iteration 29/1000 | Loss: 0.00001768
Iteration 30/1000 | Loss: 0.00001750
Iteration 31/1000 | Loss: 0.00001741
Iteration 32/1000 | Loss: 0.00001732
Iteration 33/1000 | Loss: 0.00001725
Iteration 34/1000 | Loss: 0.00001723
Iteration 35/1000 | Loss: 0.00001722
Iteration 36/1000 | Loss: 0.00001721
Iteration 37/1000 | Loss: 0.00001721
Iteration 38/1000 | Loss: 0.00001721
Iteration 39/1000 | Loss: 0.00001720
Iteration 40/1000 | Loss: 0.00001716
Iteration 41/1000 | Loss: 0.00001715
Iteration 42/1000 | Loss: 0.00001713
Iteration 43/1000 | Loss: 0.00001713
Iteration 44/1000 | Loss: 0.00001712
Iteration 45/1000 | Loss: 0.00001712
Iteration 46/1000 | Loss: 0.00001711
Iteration 47/1000 | Loss: 0.00001711
Iteration 48/1000 | Loss: 0.00001710
Iteration 49/1000 | Loss: 0.00001710
Iteration 50/1000 | Loss: 0.00001709
Iteration 51/1000 | Loss: 0.00001709
Iteration 52/1000 | Loss: 0.00001708
Iteration 53/1000 | Loss: 0.00001708
Iteration 54/1000 | Loss: 0.00001708
Iteration 55/1000 | Loss: 0.00001708
Iteration 56/1000 | Loss: 0.00001707
Iteration 57/1000 | Loss: 0.00001707
Iteration 58/1000 | Loss: 0.00001707
Iteration 59/1000 | Loss: 0.00001707
Iteration 60/1000 | Loss: 0.00001707
Iteration 61/1000 | Loss: 0.00001707
Iteration 62/1000 | Loss: 0.00001706
Iteration 63/1000 | Loss: 0.00001706
Iteration 64/1000 | Loss: 0.00001706
Iteration 65/1000 | Loss: 0.00001706
Iteration 66/1000 | Loss: 0.00001706
Iteration 67/1000 | Loss: 0.00001706
Iteration 68/1000 | Loss: 0.00001706
Iteration 69/1000 | Loss: 0.00001706
Iteration 70/1000 | Loss: 0.00001705
Iteration 71/1000 | Loss: 0.00001705
Iteration 72/1000 | Loss: 0.00001705
Iteration 73/1000 | Loss: 0.00001705
Iteration 74/1000 | Loss: 0.00001705
Iteration 75/1000 | Loss: 0.00001705
Iteration 76/1000 | Loss: 0.00001705
Iteration 77/1000 | Loss: 0.00001705
Iteration 78/1000 | Loss: 0.00001704
Iteration 79/1000 | Loss: 0.00001704
Iteration 80/1000 | Loss: 0.00001704
Iteration 81/1000 | Loss: 0.00001704
Iteration 82/1000 | Loss: 0.00001704
Iteration 83/1000 | Loss: 0.00001704
Iteration 84/1000 | Loss: 0.00001704
Iteration 85/1000 | Loss: 0.00001704
Iteration 86/1000 | Loss: 0.00001704
Iteration 87/1000 | Loss: 0.00001704
Iteration 88/1000 | Loss: 0.00001703
Iteration 89/1000 | Loss: 0.00001703
Iteration 90/1000 | Loss: 0.00001703
Iteration 91/1000 | Loss: 0.00001703
Iteration 92/1000 | Loss: 0.00001703
Iteration 93/1000 | Loss: 0.00001702
Iteration 94/1000 | Loss: 0.00001702
Iteration 95/1000 | Loss: 0.00001702
Iteration 96/1000 | Loss: 0.00001702
Iteration 97/1000 | Loss: 0.00001702
Iteration 98/1000 | Loss: 0.00001702
Iteration 99/1000 | Loss: 0.00001702
Iteration 100/1000 | Loss: 0.00001702
Iteration 101/1000 | Loss: 0.00001702
Iteration 102/1000 | Loss: 0.00001702
Iteration 103/1000 | Loss: 0.00001702
Iteration 104/1000 | Loss: 0.00001702
Iteration 105/1000 | Loss: 0.00001702
Iteration 106/1000 | Loss: 0.00001702
Iteration 107/1000 | Loss: 0.00001702
Iteration 108/1000 | Loss: 0.00001702
Iteration 109/1000 | Loss: 0.00001702
Iteration 110/1000 | Loss: 0.00001702
Iteration 111/1000 | Loss: 0.00001702
Iteration 112/1000 | Loss: 0.00001702
Iteration 113/1000 | Loss: 0.00001702
Iteration 114/1000 | Loss: 0.00001702
Iteration 115/1000 | Loss: 0.00001702
Iteration 116/1000 | Loss: 0.00001702
Iteration 117/1000 | Loss: 0.00001702
Iteration 118/1000 | Loss: 0.00001702
Iteration 119/1000 | Loss: 0.00001702
Iteration 120/1000 | Loss: 0.00001702
Iteration 121/1000 | Loss: 0.00001702
Iteration 122/1000 | Loss: 0.00001702
Iteration 123/1000 | Loss: 0.00001702
Iteration 124/1000 | Loss: 0.00001702
Iteration 125/1000 | Loss: 0.00001702
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.702184817986563e-05, 1.702184817986563e-05, 1.702184817986563e-05, 1.702184817986563e-05, 1.702184817986563e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.702184817986563e-05

Optimization complete. Final v2v error: 3.4014995098114014 mm

Highest mean error: 9.775525093078613 mm for frame 123

Lowest mean error: 3.0378968715667725 mm for frame 110

Saving results

Total time: 87.05613327026367
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5760/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5760/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5760/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00844994
Iteration 2/25 | Loss: 0.00144132
Iteration 3/25 | Loss: 0.00131493
Iteration 4/25 | Loss: 0.00129674
Iteration 5/25 | Loss: 0.00129343
Iteration 6/25 | Loss: 0.00129340
Iteration 7/25 | Loss: 0.00129340
Iteration 8/25 | Loss: 0.00129340
Iteration 9/25 | Loss: 0.00129340
Iteration 10/25 | Loss: 0.00129340
Iteration 11/25 | Loss: 0.00129340
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012934025144204497, 0.0012934025144204497, 0.0012934025144204497, 0.0012934025144204497, 0.0012934025144204497]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012934025144204497

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.10867846
Iteration 2/25 | Loss: 0.00362186
Iteration 3/25 | Loss: 0.00362186
Iteration 4/25 | Loss: 0.00362186
Iteration 5/25 | Loss: 0.00362186
Iteration 6/25 | Loss: 0.00362186
Iteration 7/25 | Loss: 0.00362186
Iteration 8/25 | Loss: 0.00362186
Iteration 9/25 | Loss: 0.00362186
Iteration 10/25 | Loss: 0.00362186
Iteration 11/25 | Loss: 0.00362186
Iteration 12/25 | Loss: 0.00362186
Iteration 13/25 | Loss: 0.00362186
Iteration 14/25 | Loss: 0.00362186
Iteration 15/25 | Loss: 0.00362186
Iteration 16/25 | Loss: 0.00362186
Iteration 17/25 | Loss: 0.00362186
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0036218580789864063, 0.0036218580789864063, 0.0036218580789864063, 0.0036218580789864063, 0.0036218580789864063]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0036218580789864063

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00362186
Iteration 2/1000 | Loss: 0.00003815
Iteration 3/1000 | Loss: 0.00002358
Iteration 4/1000 | Loss: 0.00002082
Iteration 5/1000 | Loss: 0.00001830
Iteration 6/1000 | Loss: 0.00001713
Iteration 7/1000 | Loss: 0.00001636
Iteration 8/1000 | Loss: 0.00001579
Iteration 9/1000 | Loss: 0.00001537
Iteration 10/1000 | Loss: 0.00001513
Iteration 11/1000 | Loss: 0.00001499
Iteration 12/1000 | Loss: 0.00001489
Iteration 13/1000 | Loss: 0.00001489
Iteration 14/1000 | Loss: 0.00001488
Iteration 15/1000 | Loss: 0.00001488
Iteration 16/1000 | Loss: 0.00001485
Iteration 17/1000 | Loss: 0.00001485
Iteration 18/1000 | Loss: 0.00001484
Iteration 19/1000 | Loss: 0.00001482
Iteration 20/1000 | Loss: 0.00001481
Iteration 21/1000 | Loss: 0.00001481
Iteration 22/1000 | Loss: 0.00001481
Iteration 23/1000 | Loss: 0.00001480
Iteration 24/1000 | Loss: 0.00001480
Iteration 25/1000 | Loss: 0.00001480
Iteration 26/1000 | Loss: 0.00001480
Iteration 27/1000 | Loss: 0.00001480
Iteration 28/1000 | Loss: 0.00001479
Iteration 29/1000 | Loss: 0.00001479
Iteration 30/1000 | Loss: 0.00001479
Iteration 31/1000 | Loss: 0.00001478
Iteration 32/1000 | Loss: 0.00001478
Iteration 33/1000 | Loss: 0.00001478
Iteration 34/1000 | Loss: 0.00001478
Iteration 35/1000 | Loss: 0.00001477
Iteration 36/1000 | Loss: 0.00001477
Iteration 37/1000 | Loss: 0.00001477
Iteration 38/1000 | Loss: 0.00001477
Iteration 39/1000 | Loss: 0.00001477
Iteration 40/1000 | Loss: 0.00001477
Iteration 41/1000 | Loss: 0.00001476
Iteration 42/1000 | Loss: 0.00001476
Iteration 43/1000 | Loss: 0.00001476
Iteration 44/1000 | Loss: 0.00001476
Iteration 45/1000 | Loss: 0.00001476
Iteration 46/1000 | Loss: 0.00001476
Iteration 47/1000 | Loss: 0.00001476
Iteration 48/1000 | Loss: 0.00001476
Iteration 49/1000 | Loss: 0.00001476
Iteration 50/1000 | Loss: 0.00001476
Iteration 51/1000 | Loss: 0.00001475
Iteration 52/1000 | Loss: 0.00001475
Iteration 53/1000 | Loss: 0.00001475
Iteration 54/1000 | Loss: 0.00001475
Iteration 55/1000 | Loss: 0.00001474
Iteration 56/1000 | Loss: 0.00001474
Iteration 57/1000 | Loss: 0.00001474
Iteration 58/1000 | Loss: 0.00001474
Iteration 59/1000 | Loss: 0.00001474
Iteration 60/1000 | Loss: 0.00001474
Iteration 61/1000 | Loss: 0.00001474
Iteration 62/1000 | Loss: 0.00001474
Iteration 63/1000 | Loss: 0.00001473
Iteration 64/1000 | Loss: 0.00001473
Iteration 65/1000 | Loss: 0.00001473
Iteration 66/1000 | Loss: 0.00001473
Iteration 67/1000 | Loss: 0.00001473
Iteration 68/1000 | Loss: 0.00001473
Iteration 69/1000 | Loss: 0.00001473
Iteration 70/1000 | Loss: 0.00001473
Iteration 71/1000 | Loss: 0.00001472
Iteration 72/1000 | Loss: 0.00001472
Iteration 73/1000 | Loss: 0.00001472
Iteration 74/1000 | Loss: 0.00001472
Iteration 75/1000 | Loss: 0.00001472
Iteration 76/1000 | Loss: 0.00001471
Iteration 77/1000 | Loss: 0.00001471
Iteration 78/1000 | Loss: 0.00001471
Iteration 79/1000 | Loss: 0.00001471
Iteration 80/1000 | Loss: 0.00001471
Iteration 81/1000 | Loss: 0.00001471
Iteration 82/1000 | Loss: 0.00001471
Iteration 83/1000 | Loss: 0.00001471
Iteration 84/1000 | Loss: 0.00001471
Iteration 85/1000 | Loss: 0.00001471
Iteration 86/1000 | Loss: 0.00001471
Iteration 87/1000 | Loss: 0.00001471
Iteration 88/1000 | Loss: 0.00001470
Iteration 89/1000 | Loss: 0.00001470
Iteration 90/1000 | Loss: 0.00001470
Iteration 91/1000 | Loss: 0.00001470
Iteration 92/1000 | Loss: 0.00001470
Iteration 93/1000 | Loss: 0.00001470
Iteration 94/1000 | Loss: 0.00001469
Iteration 95/1000 | Loss: 0.00001469
Iteration 96/1000 | Loss: 0.00001469
Iteration 97/1000 | Loss: 0.00001469
Iteration 98/1000 | Loss: 0.00001469
Iteration 99/1000 | Loss: 0.00001469
Iteration 100/1000 | Loss: 0.00001469
Iteration 101/1000 | Loss: 0.00001469
Iteration 102/1000 | Loss: 0.00001469
Iteration 103/1000 | Loss: 0.00001469
Iteration 104/1000 | Loss: 0.00001469
Iteration 105/1000 | Loss: 0.00001469
Iteration 106/1000 | Loss: 0.00001469
Iteration 107/1000 | Loss: 0.00001469
Iteration 108/1000 | Loss: 0.00001468
Iteration 109/1000 | Loss: 0.00001468
Iteration 110/1000 | Loss: 0.00001468
Iteration 111/1000 | Loss: 0.00001468
Iteration 112/1000 | Loss: 0.00001468
Iteration 113/1000 | Loss: 0.00001468
Iteration 114/1000 | Loss: 0.00001468
Iteration 115/1000 | Loss: 0.00001468
Iteration 116/1000 | Loss: 0.00001468
Iteration 117/1000 | Loss: 0.00001468
Iteration 118/1000 | Loss: 0.00001468
Iteration 119/1000 | Loss: 0.00001468
Iteration 120/1000 | Loss: 0.00001468
Iteration 121/1000 | Loss: 0.00001468
Iteration 122/1000 | Loss: 0.00001468
Iteration 123/1000 | Loss: 0.00001468
Iteration 124/1000 | Loss: 0.00001467
Iteration 125/1000 | Loss: 0.00001467
Iteration 126/1000 | Loss: 0.00001467
Iteration 127/1000 | Loss: 0.00001467
Iteration 128/1000 | Loss: 0.00001467
Iteration 129/1000 | Loss: 0.00001467
Iteration 130/1000 | Loss: 0.00001467
Iteration 131/1000 | Loss: 0.00001467
Iteration 132/1000 | Loss: 0.00001467
Iteration 133/1000 | Loss: 0.00001467
Iteration 134/1000 | Loss: 0.00001467
Iteration 135/1000 | Loss: 0.00001467
Iteration 136/1000 | Loss: 0.00001467
Iteration 137/1000 | Loss: 0.00001467
Iteration 138/1000 | Loss: 0.00001467
Iteration 139/1000 | Loss: 0.00001467
Iteration 140/1000 | Loss: 0.00001467
Iteration 141/1000 | Loss: 0.00001467
Iteration 142/1000 | Loss: 0.00001467
Iteration 143/1000 | Loss: 0.00001467
Iteration 144/1000 | Loss: 0.00001467
Iteration 145/1000 | Loss: 0.00001467
Iteration 146/1000 | Loss: 0.00001467
Iteration 147/1000 | Loss: 0.00001467
Iteration 148/1000 | Loss: 0.00001467
Iteration 149/1000 | Loss: 0.00001467
Iteration 150/1000 | Loss: 0.00001467
Iteration 151/1000 | Loss: 0.00001467
Iteration 152/1000 | Loss: 0.00001467
Iteration 153/1000 | Loss: 0.00001467
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.4674636076961178e-05, 1.4674636076961178e-05, 1.4674636076961178e-05, 1.4674636076961178e-05, 1.4674636076961178e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4674636076961178e-05

Optimization complete. Final v2v error: 3.194800853729248 mm

Highest mean error: 3.4251441955566406 mm for frame 125

Lowest mean error: 2.9820754528045654 mm for frame 66

Saving results

Total time: 38.29517674446106
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5760/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5760/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5760/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00993172
Iteration 2/25 | Loss: 0.00158202
Iteration 3/25 | Loss: 0.00140069
Iteration 4/25 | Loss: 0.00136504
Iteration 5/25 | Loss: 0.00135666
Iteration 6/25 | Loss: 0.00135450
Iteration 7/25 | Loss: 0.00135438
Iteration 8/25 | Loss: 0.00135438
Iteration 9/25 | Loss: 0.00135438
Iteration 10/25 | Loss: 0.00135438
Iteration 11/25 | Loss: 0.00135438
Iteration 12/25 | Loss: 0.00135438
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013543815584853292, 0.0013543815584853292, 0.0013543815584853292, 0.0013543815584853292, 0.0013543815584853292]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013543815584853292

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.88177109
Iteration 2/25 | Loss: 0.00425991
Iteration 3/25 | Loss: 0.00425990
Iteration 4/25 | Loss: 0.00425990
Iteration 5/25 | Loss: 0.00425990
Iteration 6/25 | Loss: 0.00425990
Iteration 7/25 | Loss: 0.00425990
Iteration 8/25 | Loss: 0.00425990
Iteration 9/25 | Loss: 0.00425990
Iteration 10/25 | Loss: 0.00425990
Iteration 11/25 | Loss: 0.00425990
Iteration 12/25 | Loss: 0.00425990
Iteration 13/25 | Loss: 0.00425990
Iteration 14/25 | Loss: 0.00425990
Iteration 15/25 | Loss: 0.00425990
Iteration 16/25 | Loss: 0.00425990
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00425990205258131, 0.00425990205258131, 0.00425990205258131, 0.00425990205258131, 0.00425990205258131]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00425990205258131

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00425990
Iteration 2/1000 | Loss: 0.00006160
Iteration 3/1000 | Loss: 0.00003543
Iteration 4/1000 | Loss: 0.00002707
Iteration 5/1000 | Loss: 0.00002389
Iteration 6/1000 | Loss: 0.00002154
Iteration 7/1000 | Loss: 0.00002079
Iteration 8/1000 | Loss: 0.00002025
Iteration 9/1000 | Loss: 0.00001993
Iteration 10/1000 | Loss: 0.00001989
Iteration 11/1000 | Loss: 0.00001976
Iteration 12/1000 | Loss: 0.00001951
Iteration 13/1000 | Loss: 0.00001951
Iteration 14/1000 | Loss: 0.00001935
Iteration 15/1000 | Loss: 0.00001934
Iteration 16/1000 | Loss: 0.00001928
Iteration 17/1000 | Loss: 0.00001921
Iteration 18/1000 | Loss: 0.00001917
Iteration 19/1000 | Loss: 0.00001911
Iteration 20/1000 | Loss: 0.00001910
Iteration 21/1000 | Loss: 0.00001910
Iteration 22/1000 | Loss: 0.00001910
Iteration 23/1000 | Loss: 0.00001909
Iteration 24/1000 | Loss: 0.00001908
Iteration 25/1000 | Loss: 0.00001908
Iteration 26/1000 | Loss: 0.00001908
Iteration 27/1000 | Loss: 0.00001908
Iteration 28/1000 | Loss: 0.00001907
Iteration 29/1000 | Loss: 0.00001907
Iteration 30/1000 | Loss: 0.00001907
Iteration 31/1000 | Loss: 0.00001906
Iteration 32/1000 | Loss: 0.00001906
Iteration 33/1000 | Loss: 0.00001906
Iteration 34/1000 | Loss: 0.00001906
Iteration 35/1000 | Loss: 0.00001906
Iteration 36/1000 | Loss: 0.00001906
Iteration 37/1000 | Loss: 0.00001906
Iteration 38/1000 | Loss: 0.00001906
Iteration 39/1000 | Loss: 0.00001906
Iteration 40/1000 | Loss: 0.00001905
Iteration 41/1000 | Loss: 0.00001905
Iteration 42/1000 | Loss: 0.00001905
Iteration 43/1000 | Loss: 0.00001905
Iteration 44/1000 | Loss: 0.00001905
Iteration 45/1000 | Loss: 0.00001904
Iteration 46/1000 | Loss: 0.00001904
Iteration 47/1000 | Loss: 0.00001904
Iteration 48/1000 | Loss: 0.00001904
Iteration 49/1000 | Loss: 0.00001903
Iteration 50/1000 | Loss: 0.00001900
Iteration 51/1000 | Loss: 0.00001900
Iteration 52/1000 | Loss: 0.00001900
Iteration 53/1000 | Loss: 0.00001900
Iteration 54/1000 | Loss: 0.00001900
Iteration 55/1000 | Loss: 0.00001900
Iteration 56/1000 | Loss: 0.00001900
Iteration 57/1000 | Loss: 0.00001900
Iteration 58/1000 | Loss: 0.00001900
Iteration 59/1000 | Loss: 0.00001900
Iteration 60/1000 | Loss: 0.00001900
Iteration 61/1000 | Loss: 0.00001899
Iteration 62/1000 | Loss: 0.00001899
Iteration 63/1000 | Loss: 0.00001899
Iteration 64/1000 | Loss: 0.00001899
Iteration 65/1000 | Loss: 0.00001899
Iteration 66/1000 | Loss: 0.00001899
Iteration 67/1000 | Loss: 0.00001898
Iteration 68/1000 | Loss: 0.00001898
Iteration 69/1000 | Loss: 0.00001898
Iteration 70/1000 | Loss: 0.00001898
Iteration 71/1000 | Loss: 0.00001898
Iteration 72/1000 | Loss: 0.00001898
Iteration 73/1000 | Loss: 0.00001897
Iteration 74/1000 | Loss: 0.00001897
Iteration 75/1000 | Loss: 0.00001897
Iteration 76/1000 | Loss: 0.00001896
Iteration 77/1000 | Loss: 0.00001896
Iteration 78/1000 | Loss: 0.00001896
Iteration 79/1000 | Loss: 0.00001896
Iteration 80/1000 | Loss: 0.00001896
Iteration 81/1000 | Loss: 0.00001896
Iteration 82/1000 | Loss: 0.00001896
Iteration 83/1000 | Loss: 0.00001896
Iteration 84/1000 | Loss: 0.00001896
Iteration 85/1000 | Loss: 0.00001896
Iteration 86/1000 | Loss: 0.00001895
Iteration 87/1000 | Loss: 0.00001895
Iteration 88/1000 | Loss: 0.00001895
Iteration 89/1000 | Loss: 0.00001895
Iteration 90/1000 | Loss: 0.00001895
Iteration 91/1000 | Loss: 0.00001895
Iteration 92/1000 | Loss: 0.00001894
Iteration 93/1000 | Loss: 0.00001894
Iteration 94/1000 | Loss: 0.00001894
Iteration 95/1000 | Loss: 0.00001894
Iteration 96/1000 | Loss: 0.00001894
Iteration 97/1000 | Loss: 0.00001893
Iteration 98/1000 | Loss: 0.00001893
Iteration 99/1000 | Loss: 0.00001893
Iteration 100/1000 | Loss: 0.00001893
Iteration 101/1000 | Loss: 0.00001893
Iteration 102/1000 | Loss: 0.00001892
Iteration 103/1000 | Loss: 0.00001892
Iteration 104/1000 | Loss: 0.00001892
Iteration 105/1000 | Loss: 0.00001891
Iteration 106/1000 | Loss: 0.00001891
Iteration 107/1000 | Loss: 0.00001891
Iteration 108/1000 | Loss: 0.00001891
Iteration 109/1000 | Loss: 0.00001891
Iteration 110/1000 | Loss: 0.00001891
Iteration 111/1000 | Loss: 0.00001891
Iteration 112/1000 | Loss: 0.00001891
Iteration 113/1000 | Loss: 0.00001891
Iteration 114/1000 | Loss: 0.00001891
Iteration 115/1000 | Loss: 0.00001891
Iteration 116/1000 | Loss: 0.00001891
Iteration 117/1000 | Loss: 0.00001891
Iteration 118/1000 | Loss: 0.00001891
Iteration 119/1000 | Loss: 0.00001890
Iteration 120/1000 | Loss: 0.00001890
Iteration 121/1000 | Loss: 0.00001890
Iteration 122/1000 | Loss: 0.00001890
Iteration 123/1000 | Loss: 0.00001890
Iteration 124/1000 | Loss: 0.00001890
Iteration 125/1000 | Loss: 0.00001890
Iteration 126/1000 | Loss: 0.00001890
Iteration 127/1000 | Loss: 0.00001890
Iteration 128/1000 | Loss: 0.00001890
Iteration 129/1000 | Loss: 0.00001889
Iteration 130/1000 | Loss: 0.00001889
Iteration 131/1000 | Loss: 0.00001889
Iteration 132/1000 | Loss: 0.00001889
Iteration 133/1000 | Loss: 0.00001889
Iteration 134/1000 | Loss: 0.00001889
Iteration 135/1000 | Loss: 0.00001889
Iteration 136/1000 | Loss: 0.00001889
Iteration 137/1000 | Loss: 0.00001889
Iteration 138/1000 | Loss: 0.00001889
Iteration 139/1000 | Loss: 0.00001889
Iteration 140/1000 | Loss: 0.00001889
Iteration 141/1000 | Loss: 0.00001889
Iteration 142/1000 | Loss: 0.00001889
Iteration 143/1000 | Loss: 0.00001889
Iteration 144/1000 | Loss: 0.00001889
Iteration 145/1000 | Loss: 0.00001889
Iteration 146/1000 | Loss: 0.00001889
Iteration 147/1000 | Loss: 0.00001888
Iteration 148/1000 | Loss: 0.00001888
Iteration 149/1000 | Loss: 0.00001888
Iteration 150/1000 | Loss: 0.00001888
Iteration 151/1000 | Loss: 0.00001888
Iteration 152/1000 | Loss: 0.00001888
Iteration 153/1000 | Loss: 0.00001888
Iteration 154/1000 | Loss: 0.00001888
Iteration 155/1000 | Loss: 0.00001888
Iteration 156/1000 | Loss: 0.00001888
Iteration 157/1000 | Loss: 0.00001888
Iteration 158/1000 | Loss: 0.00001887
Iteration 159/1000 | Loss: 0.00001887
Iteration 160/1000 | Loss: 0.00001887
Iteration 161/1000 | Loss: 0.00001887
Iteration 162/1000 | Loss: 0.00001887
Iteration 163/1000 | Loss: 0.00001887
Iteration 164/1000 | Loss: 0.00001887
Iteration 165/1000 | Loss: 0.00001887
Iteration 166/1000 | Loss: 0.00001887
Iteration 167/1000 | Loss: 0.00001887
Iteration 168/1000 | Loss: 0.00001887
Iteration 169/1000 | Loss: 0.00001887
Iteration 170/1000 | Loss: 0.00001887
Iteration 171/1000 | Loss: 0.00001887
Iteration 172/1000 | Loss: 0.00001886
Iteration 173/1000 | Loss: 0.00001886
Iteration 174/1000 | Loss: 0.00001886
Iteration 175/1000 | Loss: 0.00001886
Iteration 176/1000 | Loss: 0.00001886
Iteration 177/1000 | Loss: 0.00001886
Iteration 178/1000 | Loss: 0.00001886
Iteration 179/1000 | Loss: 0.00001886
Iteration 180/1000 | Loss: 0.00001886
Iteration 181/1000 | Loss: 0.00001886
Iteration 182/1000 | Loss: 0.00001886
Iteration 183/1000 | Loss: 0.00001886
Iteration 184/1000 | Loss: 0.00001886
Iteration 185/1000 | Loss: 0.00001886
Iteration 186/1000 | Loss: 0.00001886
Iteration 187/1000 | Loss: 0.00001886
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.8856915630749427e-05, 1.8856915630749427e-05, 1.8856915630749427e-05, 1.8856915630749427e-05, 1.8856915630749427e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8856915630749427e-05

Optimization complete. Final v2v error: 3.7561779022216797 mm

Highest mean error: 4.130194664001465 mm for frame 45

Lowest mean error: 3.4410245418548584 mm for frame 16

Saving results

Total time: 39.77292084693909
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5760/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5760/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5760/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00789878
Iteration 2/25 | Loss: 0.00196315
Iteration 3/25 | Loss: 0.00151051
Iteration 4/25 | Loss: 0.00141435
Iteration 5/25 | Loss: 0.00140523
Iteration 6/25 | Loss: 0.00140262
Iteration 7/25 | Loss: 0.00138662
Iteration 8/25 | Loss: 0.00137010
Iteration 9/25 | Loss: 0.00135772
Iteration 10/25 | Loss: 0.00135451
Iteration 11/25 | Loss: 0.00135484
Iteration 12/25 | Loss: 0.00135345
Iteration 13/25 | Loss: 0.00135768
Iteration 14/25 | Loss: 0.00135435
Iteration 15/25 | Loss: 0.00135195
Iteration 16/25 | Loss: 0.00135159
Iteration 17/25 | Loss: 0.00135154
Iteration 18/25 | Loss: 0.00135154
Iteration 19/25 | Loss: 0.00135154
Iteration 20/25 | Loss: 0.00135154
Iteration 21/25 | Loss: 0.00135154
Iteration 22/25 | Loss: 0.00135153
Iteration 23/25 | Loss: 0.00135153
Iteration 24/25 | Loss: 0.00135153
Iteration 25/25 | Loss: 0.00135153

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.93422747
Iteration 2/25 | Loss: 0.00280001
Iteration 3/25 | Loss: 0.00279994
Iteration 4/25 | Loss: 0.00279994
Iteration 5/25 | Loss: 0.00279994
Iteration 6/25 | Loss: 0.00279994
Iteration 7/25 | Loss: 0.00279994
Iteration 8/25 | Loss: 0.00279994
Iteration 9/25 | Loss: 0.00279994
Iteration 10/25 | Loss: 0.00279994
Iteration 11/25 | Loss: 0.00279994
Iteration 12/25 | Loss: 0.00279994
Iteration 13/25 | Loss: 0.00279994
Iteration 14/25 | Loss: 0.00279994
Iteration 15/25 | Loss: 0.00279994
Iteration 16/25 | Loss: 0.00279994
Iteration 17/25 | Loss: 0.00279994
Iteration 18/25 | Loss: 0.00279994
Iteration 19/25 | Loss: 0.00279994
Iteration 20/25 | Loss: 0.00279994
Iteration 21/25 | Loss: 0.00279994
Iteration 22/25 | Loss: 0.00279994
Iteration 23/25 | Loss: 0.00279994
Iteration 24/25 | Loss: 0.00279994
Iteration 25/25 | Loss: 0.00279994

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00279994
Iteration 2/1000 | Loss: 0.00008113
Iteration 3/1000 | Loss: 0.00004619
Iteration 4/1000 | Loss: 0.00002753
Iteration 5/1000 | Loss: 0.00002486
Iteration 6/1000 | Loss: 0.00002343
Iteration 7/1000 | Loss: 0.00007895
Iteration 8/1000 | Loss: 0.00002200
Iteration 9/1000 | Loss: 0.00002990
Iteration 10/1000 | Loss: 0.00002098
Iteration 11/1000 | Loss: 0.00002065
Iteration 12/1000 | Loss: 0.00023079
Iteration 13/1000 | Loss: 0.00003682
Iteration 14/1000 | Loss: 0.00002400
Iteration 15/1000 | Loss: 0.00003013
Iteration 16/1000 | Loss: 0.00004268
Iteration 17/1000 | Loss: 0.00002614
Iteration 18/1000 | Loss: 0.00003964
Iteration 19/1000 | Loss: 0.00002341
Iteration 20/1000 | Loss: 0.00001869
Iteration 21/1000 | Loss: 0.00002375
Iteration 22/1000 | Loss: 0.00001846
Iteration 23/1000 | Loss: 0.00001843
Iteration 24/1000 | Loss: 0.00001842
Iteration 25/1000 | Loss: 0.00001842
Iteration 26/1000 | Loss: 0.00001838
Iteration 27/1000 | Loss: 0.00001833
Iteration 28/1000 | Loss: 0.00001832
Iteration 29/1000 | Loss: 0.00001828
Iteration 30/1000 | Loss: 0.00001828
Iteration 31/1000 | Loss: 0.00001827
Iteration 32/1000 | Loss: 0.00001827
Iteration 33/1000 | Loss: 0.00001827
Iteration 34/1000 | Loss: 0.00001827
Iteration 35/1000 | Loss: 0.00001823
Iteration 36/1000 | Loss: 0.00001823
Iteration 37/1000 | Loss: 0.00001822
Iteration 38/1000 | Loss: 0.00001822
Iteration 39/1000 | Loss: 0.00001822
Iteration 40/1000 | Loss: 0.00001822
Iteration 41/1000 | Loss: 0.00001822
Iteration 42/1000 | Loss: 0.00001822
Iteration 43/1000 | Loss: 0.00001821
Iteration 44/1000 | Loss: 0.00001821
Iteration 45/1000 | Loss: 0.00001820
Iteration 46/1000 | Loss: 0.00001820
Iteration 47/1000 | Loss: 0.00001819
Iteration 48/1000 | Loss: 0.00001818
Iteration 49/1000 | Loss: 0.00001818
Iteration 50/1000 | Loss: 0.00001818
Iteration 51/1000 | Loss: 0.00001816
Iteration 52/1000 | Loss: 0.00001814
Iteration 53/1000 | Loss: 0.00001814
Iteration 54/1000 | Loss: 0.00001813
Iteration 55/1000 | Loss: 0.00001812
Iteration 56/1000 | Loss: 0.00001809
Iteration 57/1000 | Loss: 0.00001809
Iteration 58/1000 | Loss: 0.00001809
Iteration 59/1000 | Loss: 0.00001809
Iteration 60/1000 | Loss: 0.00001809
Iteration 61/1000 | Loss: 0.00001809
Iteration 62/1000 | Loss: 0.00001809
Iteration 63/1000 | Loss: 0.00001809
Iteration 64/1000 | Loss: 0.00001808
Iteration 65/1000 | Loss: 0.00002791
Iteration 66/1000 | Loss: 0.00001807
Iteration 67/1000 | Loss: 0.00001805
Iteration 68/1000 | Loss: 0.00001805
Iteration 69/1000 | Loss: 0.00001805
Iteration 70/1000 | Loss: 0.00001805
Iteration 71/1000 | Loss: 0.00001805
Iteration 72/1000 | Loss: 0.00001805
Iteration 73/1000 | Loss: 0.00001804
Iteration 74/1000 | Loss: 0.00001804
Iteration 75/1000 | Loss: 0.00001804
Iteration 76/1000 | Loss: 0.00001804
Iteration 77/1000 | Loss: 0.00001804
Iteration 78/1000 | Loss: 0.00001804
Iteration 79/1000 | Loss: 0.00001803
Iteration 80/1000 | Loss: 0.00001802
Iteration 81/1000 | Loss: 0.00001802
Iteration 82/1000 | Loss: 0.00001802
Iteration 83/1000 | Loss: 0.00001802
Iteration 84/1000 | Loss: 0.00001802
Iteration 85/1000 | Loss: 0.00001802
Iteration 86/1000 | Loss: 0.00001801
Iteration 87/1000 | Loss: 0.00001801
Iteration 88/1000 | Loss: 0.00001801
Iteration 89/1000 | Loss: 0.00001801
Iteration 90/1000 | Loss: 0.00001801
Iteration 91/1000 | Loss: 0.00001801
Iteration 92/1000 | Loss: 0.00001801
Iteration 93/1000 | Loss: 0.00001801
Iteration 94/1000 | Loss: 0.00001801
Iteration 95/1000 | Loss: 0.00001800
Iteration 96/1000 | Loss: 0.00002571
Iteration 97/1000 | Loss: 0.00001800
Iteration 98/1000 | Loss: 0.00001796
Iteration 99/1000 | Loss: 0.00001796
Iteration 100/1000 | Loss: 0.00001796
Iteration 101/1000 | Loss: 0.00001796
Iteration 102/1000 | Loss: 0.00001796
Iteration 103/1000 | Loss: 0.00001795
Iteration 104/1000 | Loss: 0.00001795
Iteration 105/1000 | Loss: 0.00001795
Iteration 106/1000 | Loss: 0.00001795
Iteration 107/1000 | Loss: 0.00001795
Iteration 108/1000 | Loss: 0.00001795
Iteration 109/1000 | Loss: 0.00001795
Iteration 110/1000 | Loss: 0.00001795
Iteration 111/1000 | Loss: 0.00001795
Iteration 112/1000 | Loss: 0.00001795
Iteration 113/1000 | Loss: 0.00001795
Iteration 114/1000 | Loss: 0.00001795
Iteration 115/1000 | Loss: 0.00001795
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [1.795174102880992e-05, 1.795174102880992e-05, 1.795174102880992e-05, 1.795174102880992e-05, 1.795174102880992e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.795174102880992e-05

Optimization complete. Final v2v error: 3.592203140258789 mm

Highest mean error: 10.822839736938477 mm for frame 125

Lowest mean error: 3.1952340602874756 mm for frame 65

Saving results

Total time: 78.62626838684082
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5760/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5760/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5760/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00508376
Iteration 2/25 | Loss: 0.00140049
Iteration 3/25 | Loss: 0.00130724
Iteration 4/25 | Loss: 0.00129536
Iteration 5/25 | Loss: 0.00129122
Iteration 6/25 | Loss: 0.00128978
Iteration 7/25 | Loss: 0.00128978
Iteration 8/25 | Loss: 0.00128978
Iteration 9/25 | Loss: 0.00128978
Iteration 10/25 | Loss: 0.00128978
Iteration 11/25 | Loss: 0.00128978
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00128977932035923, 0.00128977932035923, 0.00128977932035923, 0.00128977932035923, 0.00128977932035923]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00128977932035923

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16331196
Iteration 2/25 | Loss: 0.00316875
Iteration 3/25 | Loss: 0.00316873
Iteration 4/25 | Loss: 0.00316873
Iteration 5/25 | Loss: 0.00316873
Iteration 6/25 | Loss: 0.00316873
Iteration 7/25 | Loss: 0.00316873
Iteration 8/25 | Loss: 0.00316873
Iteration 9/25 | Loss: 0.00316873
Iteration 10/25 | Loss: 0.00316873
Iteration 11/25 | Loss: 0.00316873
Iteration 12/25 | Loss: 0.00316873
Iteration 13/25 | Loss: 0.00316873
Iteration 14/25 | Loss: 0.00316873
Iteration 15/25 | Loss: 0.00316873
Iteration 16/25 | Loss: 0.00316873
Iteration 17/25 | Loss: 0.00316873
Iteration 18/25 | Loss: 0.00316873
Iteration 19/25 | Loss: 0.00316873
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0031687256414443254, 0.0031687256414443254, 0.0031687256414443254, 0.0031687256414443254, 0.0031687256414443254]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0031687256414443254

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00316873
Iteration 2/1000 | Loss: 0.00003169
Iteration 3/1000 | Loss: 0.00002073
Iteration 4/1000 | Loss: 0.00001768
Iteration 5/1000 | Loss: 0.00001566
Iteration 6/1000 | Loss: 0.00001495
Iteration 7/1000 | Loss: 0.00001434
Iteration 8/1000 | Loss: 0.00001387
Iteration 9/1000 | Loss: 0.00001358
Iteration 10/1000 | Loss: 0.00001356
Iteration 11/1000 | Loss: 0.00001339
Iteration 12/1000 | Loss: 0.00001337
Iteration 13/1000 | Loss: 0.00001323
Iteration 14/1000 | Loss: 0.00001309
Iteration 15/1000 | Loss: 0.00001307
Iteration 16/1000 | Loss: 0.00001304
Iteration 17/1000 | Loss: 0.00001303
Iteration 18/1000 | Loss: 0.00001303
Iteration 19/1000 | Loss: 0.00001303
Iteration 20/1000 | Loss: 0.00001302
Iteration 21/1000 | Loss: 0.00001299
Iteration 22/1000 | Loss: 0.00001299
Iteration 23/1000 | Loss: 0.00001299
Iteration 24/1000 | Loss: 0.00001299
Iteration 25/1000 | Loss: 0.00001299
Iteration 26/1000 | Loss: 0.00001299
Iteration 27/1000 | Loss: 0.00001298
Iteration 28/1000 | Loss: 0.00001298
Iteration 29/1000 | Loss: 0.00001298
Iteration 30/1000 | Loss: 0.00001298
Iteration 31/1000 | Loss: 0.00001298
Iteration 32/1000 | Loss: 0.00001297
Iteration 33/1000 | Loss: 0.00001295
Iteration 34/1000 | Loss: 0.00001295
Iteration 35/1000 | Loss: 0.00001295
Iteration 36/1000 | Loss: 0.00001295
Iteration 37/1000 | Loss: 0.00001294
Iteration 38/1000 | Loss: 0.00001294
Iteration 39/1000 | Loss: 0.00001294
Iteration 40/1000 | Loss: 0.00001294
Iteration 41/1000 | Loss: 0.00001293
Iteration 42/1000 | Loss: 0.00001293
Iteration 43/1000 | Loss: 0.00001293
Iteration 44/1000 | Loss: 0.00001293
Iteration 45/1000 | Loss: 0.00001292
Iteration 46/1000 | Loss: 0.00001292
Iteration 47/1000 | Loss: 0.00001292
Iteration 48/1000 | Loss: 0.00001291
Iteration 49/1000 | Loss: 0.00001291
Iteration 50/1000 | Loss: 0.00001291
Iteration 51/1000 | Loss: 0.00001291
Iteration 52/1000 | Loss: 0.00001291
Iteration 53/1000 | Loss: 0.00001291
Iteration 54/1000 | Loss: 0.00001291
Iteration 55/1000 | Loss: 0.00001291
Iteration 56/1000 | Loss: 0.00001291
Iteration 57/1000 | Loss: 0.00001291
Iteration 58/1000 | Loss: 0.00001291
Iteration 59/1000 | Loss: 0.00001291
Iteration 60/1000 | Loss: 0.00001290
Iteration 61/1000 | Loss: 0.00001290
Iteration 62/1000 | Loss: 0.00001290
Iteration 63/1000 | Loss: 0.00001290
Iteration 64/1000 | Loss: 0.00001290
Iteration 65/1000 | Loss: 0.00001289
Iteration 66/1000 | Loss: 0.00001289
Iteration 67/1000 | Loss: 0.00001289
Iteration 68/1000 | Loss: 0.00001289
Iteration 69/1000 | Loss: 0.00001289
Iteration 70/1000 | Loss: 0.00001289
Iteration 71/1000 | Loss: 0.00001289
Iteration 72/1000 | Loss: 0.00001288
Iteration 73/1000 | Loss: 0.00001288
Iteration 74/1000 | Loss: 0.00001287
Iteration 75/1000 | Loss: 0.00001287
Iteration 76/1000 | Loss: 0.00001287
Iteration 77/1000 | Loss: 0.00001287
Iteration 78/1000 | Loss: 0.00001287
Iteration 79/1000 | Loss: 0.00001287
Iteration 80/1000 | Loss: 0.00001287
Iteration 81/1000 | Loss: 0.00001287
Iteration 82/1000 | Loss: 0.00001287
Iteration 83/1000 | Loss: 0.00001287
Iteration 84/1000 | Loss: 0.00001286
Iteration 85/1000 | Loss: 0.00001286
Iteration 86/1000 | Loss: 0.00001286
Iteration 87/1000 | Loss: 0.00001286
Iteration 88/1000 | Loss: 0.00001285
Iteration 89/1000 | Loss: 0.00001285
Iteration 90/1000 | Loss: 0.00001285
Iteration 91/1000 | Loss: 0.00001285
Iteration 92/1000 | Loss: 0.00001285
Iteration 93/1000 | Loss: 0.00001285
Iteration 94/1000 | Loss: 0.00001284
Iteration 95/1000 | Loss: 0.00001284
Iteration 96/1000 | Loss: 0.00001284
Iteration 97/1000 | Loss: 0.00001284
Iteration 98/1000 | Loss: 0.00001284
Iteration 99/1000 | Loss: 0.00001284
Iteration 100/1000 | Loss: 0.00001284
Iteration 101/1000 | Loss: 0.00001284
Iteration 102/1000 | Loss: 0.00001284
Iteration 103/1000 | Loss: 0.00001284
Iteration 104/1000 | Loss: 0.00001284
Iteration 105/1000 | Loss: 0.00001284
Iteration 106/1000 | Loss: 0.00001284
Iteration 107/1000 | Loss: 0.00001284
Iteration 108/1000 | Loss: 0.00001284
Iteration 109/1000 | Loss: 0.00001284
Iteration 110/1000 | Loss: 0.00001284
Iteration 111/1000 | Loss: 0.00001284
Iteration 112/1000 | Loss: 0.00001284
Iteration 113/1000 | Loss: 0.00001284
Iteration 114/1000 | Loss: 0.00001284
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [1.284268273593625e-05, 1.284268273593625e-05, 1.284268273593625e-05, 1.284268273593625e-05, 1.284268273593625e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.284268273593625e-05

Optimization complete. Final v2v error: 3.0574758052825928 mm

Highest mean error: 3.328645706176758 mm for frame 22

Lowest mean error: 2.8440699577331543 mm for frame 16

Saving results

Total time: 33.76295042037964
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5760/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5760/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5760/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00375822
Iteration 2/25 | Loss: 0.00146353
Iteration 3/25 | Loss: 0.00135323
Iteration 4/25 | Loss: 0.00134338
Iteration 5/25 | Loss: 0.00133965
Iteration 6/25 | Loss: 0.00133965
Iteration 7/25 | Loss: 0.00133965
Iteration 8/25 | Loss: 0.00133965
Iteration 9/25 | Loss: 0.00133965
Iteration 10/25 | Loss: 0.00133965
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013396479189395905, 0.0013396479189395905, 0.0013396479189395905, 0.0013396479189395905, 0.0013396479189395905]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013396479189395905

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.68633163
Iteration 2/25 | Loss: 0.00336592
Iteration 3/25 | Loss: 0.00336592
Iteration 4/25 | Loss: 0.00336592
Iteration 5/25 | Loss: 0.00336592
Iteration 6/25 | Loss: 0.00336592
Iteration 7/25 | Loss: 0.00336591
Iteration 8/25 | Loss: 0.00336591
Iteration 9/25 | Loss: 0.00336591
Iteration 10/25 | Loss: 0.00336591
Iteration 11/25 | Loss: 0.00336591
Iteration 12/25 | Loss: 0.00336591
Iteration 13/25 | Loss: 0.00336591
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0033659145701676607, 0.0033659145701676607, 0.0033659145701676607, 0.0033659145701676607, 0.0033659145701676607]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0033659145701676607

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00336591
Iteration 2/1000 | Loss: 0.00006672
Iteration 3/1000 | Loss: 0.00003516
Iteration 4/1000 | Loss: 0.00002724
Iteration 5/1000 | Loss: 0.00002447
Iteration 6/1000 | Loss: 0.00002302
Iteration 7/1000 | Loss: 0.00002233
Iteration 8/1000 | Loss: 0.00002193
Iteration 9/1000 | Loss: 0.00002154
Iteration 10/1000 | Loss: 0.00002123
Iteration 11/1000 | Loss: 0.00002094
Iteration 12/1000 | Loss: 0.00002073
Iteration 13/1000 | Loss: 0.00002055
Iteration 14/1000 | Loss: 0.00002039
Iteration 15/1000 | Loss: 0.00002038
Iteration 16/1000 | Loss: 0.00002022
Iteration 17/1000 | Loss: 0.00002005
Iteration 18/1000 | Loss: 0.00001987
Iteration 19/1000 | Loss: 0.00001970
Iteration 20/1000 | Loss: 0.00001970
Iteration 21/1000 | Loss: 0.00001969
Iteration 22/1000 | Loss: 0.00001954
Iteration 23/1000 | Loss: 0.00001949
Iteration 24/1000 | Loss: 0.00001944
Iteration 25/1000 | Loss: 0.00001943
Iteration 26/1000 | Loss: 0.00001938
Iteration 27/1000 | Loss: 0.00001933
Iteration 28/1000 | Loss: 0.00001932
Iteration 29/1000 | Loss: 0.00001927
Iteration 30/1000 | Loss: 0.00001924
Iteration 31/1000 | Loss: 0.00001919
Iteration 32/1000 | Loss: 0.00001919
Iteration 33/1000 | Loss: 0.00001919
Iteration 34/1000 | Loss: 0.00001919
Iteration 35/1000 | Loss: 0.00001919
Iteration 36/1000 | Loss: 0.00001918
Iteration 37/1000 | Loss: 0.00001916
Iteration 38/1000 | Loss: 0.00001915
Iteration 39/1000 | Loss: 0.00001913
Iteration 40/1000 | Loss: 0.00001908
Iteration 41/1000 | Loss: 0.00001907
Iteration 42/1000 | Loss: 0.00001905
Iteration 43/1000 | Loss: 0.00001904
Iteration 44/1000 | Loss: 0.00001904
Iteration 45/1000 | Loss: 0.00001903
Iteration 46/1000 | Loss: 0.00001903
Iteration 47/1000 | Loss: 0.00001903
Iteration 48/1000 | Loss: 0.00001903
Iteration 49/1000 | Loss: 0.00001902
Iteration 50/1000 | Loss: 0.00001902
Iteration 51/1000 | Loss: 0.00001902
Iteration 52/1000 | Loss: 0.00001901
Iteration 53/1000 | Loss: 0.00001901
Iteration 54/1000 | Loss: 0.00001901
Iteration 55/1000 | Loss: 0.00001901
Iteration 56/1000 | Loss: 0.00001901
Iteration 57/1000 | Loss: 0.00001901
Iteration 58/1000 | Loss: 0.00001900
Iteration 59/1000 | Loss: 0.00001900
Iteration 60/1000 | Loss: 0.00001900
Iteration 61/1000 | Loss: 0.00001900
Iteration 62/1000 | Loss: 0.00001900
Iteration 63/1000 | Loss: 0.00001900
Iteration 64/1000 | Loss: 0.00001899
Iteration 65/1000 | Loss: 0.00001899
Iteration 66/1000 | Loss: 0.00001899
Iteration 67/1000 | Loss: 0.00001899
Iteration 68/1000 | Loss: 0.00001899
Iteration 69/1000 | Loss: 0.00001898
Iteration 70/1000 | Loss: 0.00001898
Iteration 71/1000 | Loss: 0.00001898
Iteration 72/1000 | Loss: 0.00001898
Iteration 73/1000 | Loss: 0.00001898
Iteration 74/1000 | Loss: 0.00001898
Iteration 75/1000 | Loss: 0.00001898
Iteration 76/1000 | Loss: 0.00001898
Iteration 77/1000 | Loss: 0.00001898
Iteration 78/1000 | Loss: 0.00001898
Iteration 79/1000 | Loss: 0.00001897
Iteration 80/1000 | Loss: 0.00001897
Iteration 81/1000 | Loss: 0.00001897
Iteration 82/1000 | Loss: 0.00001897
Iteration 83/1000 | Loss: 0.00001897
Iteration 84/1000 | Loss: 0.00001896
Iteration 85/1000 | Loss: 0.00001896
Iteration 86/1000 | Loss: 0.00001896
Iteration 87/1000 | Loss: 0.00001896
Iteration 88/1000 | Loss: 0.00001896
Iteration 89/1000 | Loss: 0.00001896
Iteration 90/1000 | Loss: 0.00001896
Iteration 91/1000 | Loss: 0.00001896
Iteration 92/1000 | Loss: 0.00001896
Iteration 93/1000 | Loss: 0.00001896
Iteration 94/1000 | Loss: 0.00001896
Iteration 95/1000 | Loss: 0.00001896
Iteration 96/1000 | Loss: 0.00001896
Iteration 97/1000 | Loss: 0.00001895
Iteration 98/1000 | Loss: 0.00001895
Iteration 99/1000 | Loss: 0.00001895
Iteration 100/1000 | Loss: 0.00001895
Iteration 101/1000 | Loss: 0.00001895
Iteration 102/1000 | Loss: 0.00001895
Iteration 103/1000 | Loss: 0.00001895
Iteration 104/1000 | Loss: 0.00001895
Iteration 105/1000 | Loss: 0.00001895
Iteration 106/1000 | Loss: 0.00001894
Iteration 107/1000 | Loss: 0.00001894
Iteration 108/1000 | Loss: 0.00001894
Iteration 109/1000 | Loss: 0.00001894
Iteration 110/1000 | Loss: 0.00001894
Iteration 111/1000 | Loss: 0.00001893
Iteration 112/1000 | Loss: 0.00001893
Iteration 113/1000 | Loss: 0.00001893
Iteration 114/1000 | Loss: 0.00001893
Iteration 115/1000 | Loss: 0.00001893
Iteration 116/1000 | Loss: 0.00001892
Iteration 117/1000 | Loss: 0.00001892
Iteration 118/1000 | Loss: 0.00001891
Iteration 119/1000 | Loss: 0.00001891
Iteration 120/1000 | Loss: 0.00001891
Iteration 121/1000 | Loss: 0.00001891
Iteration 122/1000 | Loss: 0.00001891
Iteration 123/1000 | Loss: 0.00001891
Iteration 124/1000 | Loss: 0.00001890
Iteration 125/1000 | Loss: 0.00001890
Iteration 126/1000 | Loss: 0.00001890
Iteration 127/1000 | Loss: 0.00001890
Iteration 128/1000 | Loss: 0.00001889
Iteration 129/1000 | Loss: 0.00001889
Iteration 130/1000 | Loss: 0.00001889
Iteration 131/1000 | Loss: 0.00001889
Iteration 132/1000 | Loss: 0.00001889
Iteration 133/1000 | Loss: 0.00001889
Iteration 134/1000 | Loss: 0.00001889
Iteration 135/1000 | Loss: 0.00001888
Iteration 136/1000 | Loss: 0.00001888
Iteration 137/1000 | Loss: 0.00001888
Iteration 138/1000 | Loss: 0.00001888
Iteration 139/1000 | Loss: 0.00001888
Iteration 140/1000 | Loss: 0.00001887
Iteration 141/1000 | Loss: 0.00001887
Iteration 142/1000 | Loss: 0.00001887
Iteration 143/1000 | Loss: 0.00001887
Iteration 144/1000 | Loss: 0.00001887
Iteration 145/1000 | Loss: 0.00001887
Iteration 146/1000 | Loss: 0.00001886
Iteration 147/1000 | Loss: 0.00001886
Iteration 148/1000 | Loss: 0.00001886
Iteration 149/1000 | Loss: 0.00001886
Iteration 150/1000 | Loss: 0.00001886
Iteration 151/1000 | Loss: 0.00001886
Iteration 152/1000 | Loss: 0.00001886
Iteration 153/1000 | Loss: 0.00001886
Iteration 154/1000 | Loss: 0.00001886
Iteration 155/1000 | Loss: 0.00001886
Iteration 156/1000 | Loss: 0.00001886
Iteration 157/1000 | Loss: 0.00001885
Iteration 158/1000 | Loss: 0.00001885
Iteration 159/1000 | Loss: 0.00001885
Iteration 160/1000 | Loss: 0.00001885
Iteration 161/1000 | Loss: 0.00001884
Iteration 162/1000 | Loss: 0.00001884
Iteration 163/1000 | Loss: 0.00001884
Iteration 164/1000 | Loss: 0.00001884
Iteration 165/1000 | Loss: 0.00001884
Iteration 166/1000 | Loss: 0.00001884
Iteration 167/1000 | Loss: 0.00001884
Iteration 168/1000 | Loss: 0.00001884
Iteration 169/1000 | Loss: 0.00001884
Iteration 170/1000 | Loss: 0.00001884
Iteration 171/1000 | Loss: 0.00001883
Iteration 172/1000 | Loss: 0.00001883
Iteration 173/1000 | Loss: 0.00001883
Iteration 174/1000 | Loss: 0.00001883
Iteration 175/1000 | Loss: 0.00001883
Iteration 176/1000 | Loss: 0.00001883
Iteration 177/1000 | Loss: 0.00001883
Iteration 178/1000 | Loss: 0.00001883
Iteration 179/1000 | Loss: 0.00001882
Iteration 180/1000 | Loss: 0.00001882
Iteration 181/1000 | Loss: 0.00001882
Iteration 182/1000 | Loss: 0.00001882
Iteration 183/1000 | Loss: 0.00001882
Iteration 184/1000 | Loss: 0.00001882
Iteration 185/1000 | Loss: 0.00001882
Iteration 186/1000 | Loss: 0.00001882
Iteration 187/1000 | Loss: 0.00001882
Iteration 188/1000 | Loss: 0.00001882
Iteration 189/1000 | Loss: 0.00001882
Iteration 190/1000 | Loss: 0.00001882
Iteration 191/1000 | Loss: 0.00001882
Iteration 192/1000 | Loss: 0.00001882
Iteration 193/1000 | Loss: 0.00001882
Iteration 194/1000 | Loss: 0.00001882
Iteration 195/1000 | Loss: 0.00001882
Iteration 196/1000 | Loss: 0.00001882
Iteration 197/1000 | Loss: 0.00001882
Iteration 198/1000 | Loss: 0.00001882
Iteration 199/1000 | Loss: 0.00001882
Iteration 200/1000 | Loss: 0.00001882
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [1.8821046978700906e-05, 1.8821046978700906e-05, 1.8821046978700906e-05, 1.8821046978700906e-05, 1.8821046978700906e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8821046978700906e-05

Optimization complete. Final v2v error: 3.701878547668457 mm

Highest mean error: 4.00670051574707 mm for frame 185

Lowest mean error: 3.491844654083252 mm for frame 249

Saving results

Total time: 58.80553841590881
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5760/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5760/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5760/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00690090
Iteration 2/25 | Loss: 0.00147991
Iteration 3/25 | Loss: 0.00139629
Iteration 4/25 | Loss: 0.00138638
Iteration 5/25 | Loss: 0.00138399
Iteration 6/25 | Loss: 0.00138389
Iteration 7/25 | Loss: 0.00138389
Iteration 8/25 | Loss: 0.00138389
Iteration 9/25 | Loss: 0.00138389
Iteration 10/25 | Loss: 0.00138389
Iteration 11/25 | Loss: 0.00138389
Iteration 12/25 | Loss: 0.00138389
Iteration 13/25 | Loss: 0.00138389
Iteration 14/25 | Loss: 0.00138389
Iteration 15/25 | Loss: 0.00138389
Iteration 16/25 | Loss: 0.00138389
Iteration 17/25 | Loss: 0.00138389
Iteration 18/25 | Loss: 0.00138389
Iteration 19/25 | Loss: 0.00138389
Iteration 20/25 | Loss: 0.00138389
Iteration 21/25 | Loss: 0.00138389
Iteration 22/25 | Loss: 0.00138389
Iteration 23/25 | Loss: 0.00138389
Iteration 24/25 | Loss: 0.00138389
Iteration 25/25 | Loss: 0.00138389

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.88831425
Iteration 2/25 | Loss: 0.00344725
Iteration 3/25 | Loss: 0.00344721
Iteration 4/25 | Loss: 0.00344720
Iteration 5/25 | Loss: 0.00344720
Iteration 6/25 | Loss: 0.00344720
Iteration 7/25 | Loss: 0.00344720
Iteration 8/25 | Loss: 0.00344720
Iteration 9/25 | Loss: 0.00344720
Iteration 10/25 | Loss: 0.00344720
Iteration 11/25 | Loss: 0.00344720
Iteration 12/25 | Loss: 0.00344720
Iteration 13/25 | Loss: 0.00344720
Iteration 14/25 | Loss: 0.00344720
Iteration 15/25 | Loss: 0.00344720
Iteration 16/25 | Loss: 0.00344720
Iteration 17/25 | Loss: 0.00344720
Iteration 18/25 | Loss: 0.00344720
Iteration 19/25 | Loss: 0.00344720
Iteration 20/25 | Loss: 0.00344720
Iteration 21/25 | Loss: 0.00344720
Iteration 22/25 | Loss: 0.00344720
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0034472018014639616, 0.0034472018014639616, 0.0034472018014639616, 0.0034472018014639616, 0.0034472018014639616]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0034472018014639616

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00344720
Iteration 2/1000 | Loss: 0.00004917
Iteration 3/1000 | Loss: 0.00003373
Iteration 4/1000 | Loss: 0.00002911
Iteration 5/1000 | Loss: 0.00002668
Iteration 6/1000 | Loss: 0.00002522
Iteration 7/1000 | Loss: 0.00002429
Iteration 8/1000 | Loss: 0.00002372
Iteration 9/1000 | Loss: 0.00002316
Iteration 10/1000 | Loss: 0.00002279
Iteration 11/1000 | Loss: 0.00002256
Iteration 12/1000 | Loss: 0.00002241
Iteration 13/1000 | Loss: 0.00002222
Iteration 14/1000 | Loss: 0.00002216
Iteration 15/1000 | Loss: 0.00002215
Iteration 16/1000 | Loss: 0.00002212
Iteration 17/1000 | Loss: 0.00002212
Iteration 18/1000 | Loss: 0.00002212
Iteration 19/1000 | Loss: 0.00002212
Iteration 20/1000 | Loss: 0.00002212
Iteration 21/1000 | Loss: 0.00002212
Iteration 22/1000 | Loss: 0.00002212
Iteration 23/1000 | Loss: 0.00002212
Iteration 24/1000 | Loss: 0.00002212
Iteration 25/1000 | Loss: 0.00002212
Iteration 26/1000 | Loss: 0.00002212
Iteration 27/1000 | Loss: 0.00002212
Iteration 28/1000 | Loss: 0.00002212
Iteration 29/1000 | Loss: 0.00002212
Iteration 30/1000 | Loss: 0.00002212
Iteration 31/1000 | Loss: 0.00002211
Iteration 32/1000 | Loss: 0.00002210
Iteration 33/1000 | Loss: 0.00002210
Iteration 34/1000 | Loss: 0.00002210
Iteration 35/1000 | Loss: 0.00002210
Iteration 36/1000 | Loss: 0.00002210
Iteration 37/1000 | Loss: 0.00002210
Iteration 38/1000 | Loss: 0.00002209
Iteration 39/1000 | Loss: 0.00002209
Iteration 40/1000 | Loss: 0.00002208
Iteration 41/1000 | Loss: 0.00002208
Iteration 42/1000 | Loss: 0.00002208
Iteration 43/1000 | Loss: 0.00002208
Iteration 44/1000 | Loss: 0.00002207
Iteration 45/1000 | Loss: 0.00002207
Iteration 46/1000 | Loss: 0.00002206
Iteration 47/1000 | Loss: 0.00002206
Iteration 48/1000 | Loss: 0.00002205
Iteration 49/1000 | Loss: 0.00002205
Iteration 50/1000 | Loss: 0.00002205
Iteration 51/1000 | Loss: 0.00002205
Iteration 52/1000 | Loss: 0.00002205
Iteration 53/1000 | Loss: 0.00002205
Iteration 54/1000 | Loss: 0.00002205
Iteration 55/1000 | Loss: 0.00002205
Iteration 56/1000 | Loss: 0.00002205
Iteration 57/1000 | Loss: 0.00002205
Iteration 58/1000 | Loss: 0.00002204
Iteration 59/1000 | Loss: 0.00002204
Iteration 60/1000 | Loss: 0.00002204
Iteration 61/1000 | Loss: 0.00002203
Iteration 62/1000 | Loss: 0.00002203
Iteration 63/1000 | Loss: 0.00002203
Iteration 64/1000 | Loss: 0.00002203
Iteration 65/1000 | Loss: 0.00002203
Iteration 66/1000 | Loss: 0.00002202
Iteration 67/1000 | Loss: 0.00002202
Iteration 68/1000 | Loss: 0.00002202
Iteration 69/1000 | Loss: 0.00002202
Iteration 70/1000 | Loss: 0.00002202
Iteration 71/1000 | Loss: 0.00002202
Iteration 72/1000 | Loss: 0.00002202
Iteration 73/1000 | Loss: 0.00002202
Iteration 74/1000 | Loss: 0.00002202
Iteration 75/1000 | Loss: 0.00002202
Iteration 76/1000 | Loss: 0.00002202
Iteration 77/1000 | Loss: 0.00002201
Iteration 78/1000 | Loss: 0.00002201
Iteration 79/1000 | Loss: 0.00002200
Iteration 80/1000 | Loss: 0.00002199
Iteration 81/1000 | Loss: 0.00002199
Iteration 82/1000 | Loss: 0.00002199
Iteration 83/1000 | Loss: 0.00002199
Iteration 84/1000 | Loss: 0.00002199
Iteration 85/1000 | Loss: 0.00002199
Iteration 86/1000 | Loss: 0.00002199
Iteration 87/1000 | Loss: 0.00002199
Iteration 88/1000 | Loss: 0.00002198
Iteration 89/1000 | Loss: 0.00002198
Iteration 90/1000 | Loss: 0.00002198
Iteration 91/1000 | Loss: 0.00002198
Iteration 92/1000 | Loss: 0.00002197
Iteration 93/1000 | Loss: 0.00002197
Iteration 94/1000 | Loss: 0.00002197
Iteration 95/1000 | Loss: 0.00002196
Iteration 96/1000 | Loss: 0.00002196
Iteration 97/1000 | Loss: 0.00002196
Iteration 98/1000 | Loss: 0.00002196
Iteration 99/1000 | Loss: 0.00002196
Iteration 100/1000 | Loss: 0.00002195
Iteration 101/1000 | Loss: 0.00002195
Iteration 102/1000 | Loss: 0.00002195
Iteration 103/1000 | Loss: 0.00002195
Iteration 104/1000 | Loss: 0.00002195
Iteration 105/1000 | Loss: 0.00002195
Iteration 106/1000 | Loss: 0.00002195
Iteration 107/1000 | Loss: 0.00002195
Iteration 108/1000 | Loss: 0.00002195
Iteration 109/1000 | Loss: 0.00002195
Iteration 110/1000 | Loss: 0.00002195
Iteration 111/1000 | Loss: 0.00002195
Iteration 112/1000 | Loss: 0.00002195
Iteration 113/1000 | Loss: 0.00002195
Iteration 114/1000 | Loss: 0.00002195
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [2.1953759642201476e-05, 2.1953759642201476e-05, 2.1953759642201476e-05, 2.1953759642201476e-05, 2.1953759642201476e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1953759642201476e-05

Optimization complete. Final v2v error: 4.073420524597168 mm

Highest mean error: 4.521613121032715 mm for frame 53

Lowest mean error: 3.478205680847168 mm for frame 145

Saving results

Total time: 33.00972938537598
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5760/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5760/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5760/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00978142
Iteration 2/25 | Loss: 0.00183176
Iteration 3/25 | Loss: 0.00143003
Iteration 4/25 | Loss: 0.00140365
Iteration 5/25 | Loss: 0.00139880
Iteration 6/25 | Loss: 0.00139716
Iteration 7/25 | Loss: 0.00139716
Iteration 8/25 | Loss: 0.00139716
Iteration 9/25 | Loss: 0.00139716
Iteration 10/25 | Loss: 0.00139716
Iteration 11/25 | Loss: 0.00139716
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013971634907647967, 0.0013971634907647967, 0.0013971634907647967, 0.0013971634907647967, 0.0013971634907647967]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013971634907647967

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.86663914
Iteration 2/25 | Loss: 0.00251174
Iteration 3/25 | Loss: 0.00251173
Iteration 4/25 | Loss: 0.00251173
Iteration 5/25 | Loss: 0.00251173
Iteration 6/25 | Loss: 0.00251173
Iteration 7/25 | Loss: 0.00251173
Iteration 8/25 | Loss: 0.00251173
Iteration 9/25 | Loss: 0.00251173
Iteration 10/25 | Loss: 0.00251173
Iteration 11/25 | Loss: 0.00251173
Iteration 12/25 | Loss: 0.00251173
Iteration 13/25 | Loss: 0.00251173
Iteration 14/25 | Loss: 0.00251173
Iteration 15/25 | Loss: 0.00251173
Iteration 16/25 | Loss: 0.00251173
Iteration 17/25 | Loss: 0.00251173
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0025117273908108473, 0.0025117273908108473, 0.0025117273908108473, 0.0025117273908108473, 0.0025117273908108473]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025117273908108473

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00251173
Iteration 2/1000 | Loss: 0.00005785
Iteration 3/1000 | Loss: 0.00004233
Iteration 4/1000 | Loss: 0.00003444
Iteration 5/1000 | Loss: 0.00003206
Iteration 6/1000 | Loss: 0.00002990
Iteration 7/1000 | Loss: 0.00002838
Iteration 8/1000 | Loss: 0.00002739
Iteration 9/1000 | Loss: 0.00002666
Iteration 10/1000 | Loss: 0.00002611
Iteration 11/1000 | Loss: 0.00002571
Iteration 12/1000 | Loss: 0.00002546
Iteration 13/1000 | Loss: 0.00002531
Iteration 14/1000 | Loss: 0.00002527
Iteration 15/1000 | Loss: 0.00002523
Iteration 16/1000 | Loss: 0.00002521
Iteration 17/1000 | Loss: 0.00002520
Iteration 18/1000 | Loss: 0.00002519
Iteration 19/1000 | Loss: 0.00002513
Iteration 20/1000 | Loss: 0.00002511
Iteration 21/1000 | Loss: 0.00002511
Iteration 22/1000 | Loss: 0.00002510
Iteration 23/1000 | Loss: 0.00002510
Iteration 24/1000 | Loss: 0.00002510
Iteration 25/1000 | Loss: 0.00002509
Iteration 26/1000 | Loss: 0.00002509
Iteration 27/1000 | Loss: 0.00002507
Iteration 28/1000 | Loss: 0.00002507
Iteration 29/1000 | Loss: 0.00002507
Iteration 30/1000 | Loss: 0.00002507
Iteration 31/1000 | Loss: 0.00002507
Iteration 32/1000 | Loss: 0.00002507
Iteration 33/1000 | Loss: 0.00002507
Iteration 34/1000 | Loss: 0.00002506
Iteration 35/1000 | Loss: 0.00002506
Iteration 36/1000 | Loss: 0.00002506
Iteration 37/1000 | Loss: 0.00002506
Iteration 38/1000 | Loss: 0.00002506
Iteration 39/1000 | Loss: 0.00002506
Iteration 40/1000 | Loss: 0.00002506
Iteration 41/1000 | Loss: 0.00002506
Iteration 42/1000 | Loss: 0.00002506
Iteration 43/1000 | Loss: 0.00002505
Iteration 44/1000 | Loss: 0.00002505
Iteration 45/1000 | Loss: 0.00002505
Iteration 46/1000 | Loss: 0.00002504
Iteration 47/1000 | Loss: 0.00002504
Iteration 48/1000 | Loss: 0.00002503
Iteration 49/1000 | Loss: 0.00002503
Iteration 50/1000 | Loss: 0.00002503
Iteration 51/1000 | Loss: 0.00002502
Iteration 52/1000 | Loss: 0.00002502
Iteration 53/1000 | Loss: 0.00002501
Iteration 54/1000 | Loss: 0.00002501
Iteration 55/1000 | Loss: 0.00002501
Iteration 56/1000 | Loss: 0.00002501
Iteration 57/1000 | Loss: 0.00002500
Iteration 58/1000 | Loss: 0.00002500
Iteration 59/1000 | Loss: 0.00002500
Iteration 60/1000 | Loss: 0.00002500
Iteration 61/1000 | Loss: 0.00002499
Iteration 62/1000 | Loss: 0.00002499
Iteration 63/1000 | Loss: 0.00002499
Iteration 64/1000 | Loss: 0.00002499
Iteration 65/1000 | Loss: 0.00002498
Iteration 66/1000 | Loss: 0.00002497
Iteration 67/1000 | Loss: 0.00002497
Iteration 68/1000 | Loss: 0.00002496
Iteration 69/1000 | Loss: 0.00002496
Iteration 70/1000 | Loss: 0.00002496
Iteration 71/1000 | Loss: 0.00002496
Iteration 72/1000 | Loss: 0.00002495
Iteration 73/1000 | Loss: 0.00002495
Iteration 74/1000 | Loss: 0.00002495
Iteration 75/1000 | Loss: 0.00002494
Iteration 76/1000 | Loss: 0.00002494
Iteration 77/1000 | Loss: 0.00002493
Iteration 78/1000 | Loss: 0.00002493
Iteration 79/1000 | Loss: 0.00002493
Iteration 80/1000 | Loss: 0.00002493
Iteration 81/1000 | Loss: 0.00002493
Iteration 82/1000 | Loss: 0.00002493
Iteration 83/1000 | Loss: 0.00002493
Iteration 84/1000 | Loss: 0.00002492
Iteration 85/1000 | Loss: 0.00002492
Iteration 86/1000 | Loss: 0.00002492
Iteration 87/1000 | Loss: 0.00002492
Iteration 88/1000 | Loss: 0.00002491
Iteration 89/1000 | Loss: 0.00002491
Iteration 90/1000 | Loss: 0.00002490
Iteration 91/1000 | Loss: 0.00002490
Iteration 92/1000 | Loss: 0.00002489
Iteration 93/1000 | Loss: 0.00002489
Iteration 94/1000 | Loss: 0.00002489
Iteration 95/1000 | Loss: 0.00002489
Iteration 96/1000 | Loss: 0.00002488
Iteration 97/1000 | Loss: 0.00002488
Iteration 98/1000 | Loss: 0.00002488
Iteration 99/1000 | Loss: 0.00002487
Iteration 100/1000 | Loss: 0.00002487
Iteration 101/1000 | Loss: 0.00002487
Iteration 102/1000 | Loss: 0.00002486
Iteration 103/1000 | Loss: 0.00002486
Iteration 104/1000 | Loss: 0.00002486
Iteration 105/1000 | Loss: 0.00002486
Iteration 106/1000 | Loss: 0.00002486
Iteration 107/1000 | Loss: 0.00002485
Iteration 108/1000 | Loss: 0.00002485
Iteration 109/1000 | Loss: 0.00002485
Iteration 110/1000 | Loss: 0.00002485
Iteration 111/1000 | Loss: 0.00002485
Iteration 112/1000 | Loss: 0.00002484
Iteration 113/1000 | Loss: 0.00002484
Iteration 114/1000 | Loss: 0.00002484
Iteration 115/1000 | Loss: 0.00002484
Iteration 116/1000 | Loss: 0.00002484
Iteration 117/1000 | Loss: 0.00002483
Iteration 118/1000 | Loss: 0.00002483
Iteration 119/1000 | Loss: 0.00002483
Iteration 120/1000 | Loss: 0.00002483
Iteration 121/1000 | Loss: 0.00002482
Iteration 122/1000 | Loss: 0.00002482
Iteration 123/1000 | Loss: 0.00002482
Iteration 124/1000 | Loss: 0.00002481
Iteration 125/1000 | Loss: 0.00002481
Iteration 126/1000 | Loss: 0.00002481
Iteration 127/1000 | Loss: 0.00002480
Iteration 128/1000 | Loss: 0.00002480
Iteration 129/1000 | Loss: 0.00002480
Iteration 130/1000 | Loss: 0.00002480
Iteration 131/1000 | Loss: 0.00002480
Iteration 132/1000 | Loss: 0.00002480
Iteration 133/1000 | Loss: 0.00002479
Iteration 134/1000 | Loss: 0.00002479
Iteration 135/1000 | Loss: 0.00002479
Iteration 136/1000 | Loss: 0.00002478
Iteration 137/1000 | Loss: 0.00002478
Iteration 138/1000 | Loss: 0.00002478
Iteration 139/1000 | Loss: 0.00002478
Iteration 140/1000 | Loss: 0.00002477
Iteration 141/1000 | Loss: 0.00002477
Iteration 142/1000 | Loss: 0.00002477
Iteration 143/1000 | Loss: 0.00002477
Iteration 144/1000 | Loss: 0.00002477
Iteration 145/1000 | Loss: 0.00002477
Iteration 146/1000 | Loss: 0.00002476
Iteration 147/1000 | Loss: 0.00002476
Iteration 148/1000 | Loss: 0.00002476
Iteration 149/1000 | Loss: 0.00002476
Iteration 150/1000 | Loss: 0.00002476
Iteration 151/1000 | Loss: 0.00002475
Iteration 152/1000 | Loss: 0.00002475
Iteration 153/1000 | Loss: 0.00002475
Iteration 154/1000 | Loss: 0.00002475
Iteration 155/1000 | Loss: 0.00002475
Iteration 156/1000 | Loss: 0.00002475
Iteration 157/1000 | Loss: 0.00002474
Iteration 158/1000 | Loss: 0.00002474
Iteration 159/1000 | Loss: 0.00002474
Iteration 160/1000 | Loss: 0.00002474
Iteration 161/1000 | Loss: 0.00002474
Iteration 162/1000 | Loss: 0.00002474
Iteration 163/1000 | Loss: 0.00002474
Iteration 164/1000 | Loss: 0.00002474
Iteration 165/1000 | Loss: 0.00002474
Iteration 166/1000 | Loss: 0.00002474
Iteration 167/1000 | Loss: 0.00002474
Iteration 168/1000 | Loss: 0.00002474
Iteration 169/1000 | Loss: 0.00002474
Iteration 170/1000 | Loss: 0.00002474
Iteration 171/1000 | Loss: 0.00002474
Iteration 172/1000 | Loss: 0.00002474
Iteration 173/1000 | Loss: 0.00002474
Iteration 174/1000 | Loss: 0.00002474
Iteration 175/1000 | Loss: 0.00002474
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [2.474324355716817e-05, 2.474324355716817e-05, 2.474324355716817e-05, 2.474324355716817e-05, 2.474324355716817e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.474324355716817e-05

Optimization complete. Final v2v error: 4.058963298797607 mm

Highest mean error: 4.5443339347839355 mm for frame 134

Lowest mean error: 3.457848072052002 mm for frame 28

Saving results

Total time: 41.3144268989563
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5760/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5760/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5760/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00931005
Iteration 2/25 | Loss: 0.00149427
Iteration 3/25 | Loss: 0.00131278
Iteration 4/25 | Loss: 0.00130149
Iteration 5/25 | Loss: 0.00129756
Iteration 6/25 | Loss: 0.00129623
Iteration 7/25 | Loss: 0.00129623
Iteration 8/25 | Loss: 0.00129623
Iteration 9/25 | Loss: 0.00129623
Iteration 10/25 | Loss: 0.00129623
Iteration 11/25 | Loss: 0.00129623
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012962312903255224, 0.0012962312903255224, 0.0012962312903255224, 0.0012962312903255224, 0.0012962312903255224]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012962312903255224

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.13726759
Iteration 2/25 | Loss: 0.00312723
Iteration 3/25 | Loss: 0.00312723
Iteration 4/25 | Loss: 0.00312723
Iteration 5/25 | Loss: 0.00312723
Iteration 6/25 | Loss: 0.00312723
Iteration 7/25 | Loss: 0.00312723
Iteration 8/25 | Loss: 0.00312723
Iteration 9/25 | Loss: 0.00312723
Iteration 10/25 | Loss: 0.00312723
Iteration 11/25 | Loss: 0.00312723
Iteration 12/25 | Loss: 0.00312723
Iteration 13/25 | Loss: 0.00312723
Iteration 14/25 | Loss: 0.00312723
Iteration 15/25 | Loss: 0.00312723
Iteration 16/25 | Loss: 0.00312723
Iteration 17/25 | Loss: 0.00312723
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.003127229865640402, 0.003127229865640402, 0.003127229865640402, 0.003127229865640402, 0.003127229865640402]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003127229865640402

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00312723
Iteration 2/1000 | Loss: 0.00004502
Iteration 3/1000 | Loss: 0.00002993
Iteration 4/1000 | Loss: 0.00002629
Iteration 5/1000 | Loss: 0.00002431
Iteration 6/1000 | Loss: 0.00002355
Iteration 7/1000 | Loss: 0.00002298
Iteration 8/1000 | Loss: 0.00002239
Iteration 9/1000 | Loss: 0.00002187
Iteration 10/1000 | Loss: 0.00002159
Iteration 11/1000 | Loss: 0.00002154
Iteration 12/1000 | Loss: 0.00002154
Iteration 13/1000 | Loss: 0.00002153
Iteration 14/1000 | Loss: 0.00002145
Iteration 15/1000 | Loss: 0.00002133
Iteration 16/1000 | Loss: 0.00002133
Iteration 17/1000 | Loss: 0.00002129
Iteration 18/1000 | Loss: 0.00002129
Iteration 19/1000 | Loss: 0.00002129
Iteration 20/1000 | Loss: 0.00002128
Iteration 21/1000 | Loss: 0.00002127
Iteration 22/1000 | Loss: 0.00002125
Iteration 23/1000 | Loss: 0.00002124
Iteration 24/1000 | Loss: 0.00002124
Iteration 25/1000 | Loss: 0.00002119
Iteration 26/1000 | Loss: 0.00002119
Iteration 27/1000 | Loss: 0.00002118
Iteration 28/1000 | Loss: 0.00002118
Iteration 29/1000 | Loss: 0.00002117
Iteration 30/1000 | Loss: 0.00002117
Iteration 31/1000 | Loss: 0.00002116
Iteration 32/1000 | Loss: 0.00002116
Iteration 33/1000 | Loss: 0.00002116
Iteration 34/1000 | Loss: 0.00002116
Iteration 35/1000 | Loss: 0.00002116
Iteration 36/1000 | Loss: 0.00002116
Iteration 37/1000 | Loss: 0.00002115
Iteration 38/1000 | Loss: 0.00002115
Iteration 39/1000 | Loss: 0.00002114
Iteration 40/1000 | Loss: 0.00002114
Iteration 41/1000 | Loss: 0.00002114
Iteration 42/1000 | Loss: 0.00002113
Iteration 43/1000 | Loss: 0.00002113
Iteration 44/1000 | Loss: 0.00002112
Iteration 45/1000 | Loss: 0.00002112
Iteration 46/1000 | Loss: 0.00002111
Iteration 47/1000 | Loss: 0.00002111
Iteration 48/1000 | Loss: 0.00002111
Iteration 49/1000 | Loss: 0.00002111
Iteration 50/1000 | Loss: 0.00002110
Iteration 51/1000 | Loss: 0.00002110
Iteration 52/1000 | Loss: 0.00002110
Iteration 53/1000 | Loss: 0.00002110
Iteration 54/1000 | Loss: 0.00002110
Iteration 55/1000 | Loss: 0.00002110
Iteration 56/1000 | Loss: 0.00002110
Iteration 57/1000 | Loss: 0.00002110
Iteration 58/1000 | Loss: 0.00002110
Iteration 59/1000 | Loss: 0.00002110
Iteration 60/1000 | Loss: 0.00002109
Iteration 61/1000 | Loss: 0.00002109
Iteration 62/1000 | Loss: 0.00002108
Iteration 63/1000 | Loss: 0.00002108
Iteration 64/1000 | Loss: 0.00002108
Iteration 65/1000 | Loss: 0.00002108
Iteration 66/1000 | Loss: 0.00002108
Iteration 67/1000 | Loss: 0.00002108
Iteration 68/1000 | Loss: 0.00002108
Iteration 69/1000 | Loss: 0.00002107
Iteration 70/1000 | Loss: 0.00002107
Iteration 71/1000 | Loss: 0.00002107
Iteration 72/1000 | Loss: 0.00002107
Iteration 73/1000 | Loss: 0.00002107
Iteration 74/1000 | Loss: 0.00002106
Iteration 75/1000 | Loss: 0.00002106
Iteration 76/1000 | Loss: 0.00002106
Iteration 77/1000 | Loss: 0.00002105
Iteration 78/1000 | Loss: 0.00002105
Iteration 79/1000 | Loss: 0.00002105
Iteration 80/1000 | Loss: 0.00002105
Iteration 81/1000 | Loss: 0.00002105
Iteration 82/1000 | Loss: 0.00002105
Iteration 83/1000 | Loss: 0.00002104
Iteration 84/1000 | Loss: 0.00002104
Iteration 85/1000 | Loss: 0.00002104
Iteration 86/1000 | Loss: 0.00002104
Iteration 87/1000 | Loss: 0.00002104
Iteration 88/1000 | Loss: 0.00002104
Iteration 89/1000 | Loss: 0.00002104
Iteration 90/1000 | Loss: 0.00002104
Iteration 91/1000 | Loss: 0.00002104
Iteration 92/1000 | Loss: 0.00002104
Iteration 93/1000 | Loss: 0.00002104
Iteration 94/1000 | Loss: 0.00002104
Iteration 95/1000 | Loss: 0.00002104
Iteration 96/1000 | Loss: 0.00002104
Iteration 97/1000 | Loss: 0.00002103
Iteration 98/1000 | Loss: 0.00002103
Iteration 99/1000 | Loss: 0.00002103
Iteration 100/1000 | Loss: 0.00002103
Iteration 101/1000 | Loss: 0.00002103
Iteration 102/1000 | Loss: 0.00002103
Iteration 103/1000 | Loss: 0.00002103
Iteration 104/1000 | Loss: 0.00002103
Iteration 105/1000 | Loss: 0.00002102
Iteration 106/1000 | Loss: 0.00002102
Iteration 107/1000 | Loss: 0.00002102
Iteration 108/1000 | Loss: 0.00002102
Iteration 109/1000 | Loss: 0.00002102
Iteration 110/1000 | Loss: 0.00002102
Iteration 111/1000 | Loss: 0.00002102
Iteration 112/1000 | Loss: 0.00002102
Iteration 113/1000 | Loss: 0.00002102
Iteration 114/1000 | Loss: 0.00002102
Iteration 115/1000 | Loss: 0.00002102
Iteration 116/1000 | Loss: 0.00002102
Iteration 117/1000 | Loss: 0.00002101
Iteration 118/1000 | Loss: 0.00002101
Iteration 119/1000 | Loss: 0.00002101
Iteration 120/1000 | Loss: 0.00002101
Iteration 121/1000 | Loss: 0.00002101
Iteration 122/1000 | Loss: 0.00002101
Iteration 123/1000 | Loss: 0.00002101
Iteration 124/1000 | Loss: 0.00002101
Iteration 125/1000 | Loss: 0.00002101
Iteration 126/1000 | Loss: 0.00002101
Iteration 127/1000 | Loss: 0.00002101
Iteration 128/1000 | Loss: 0.00002101
Iteration 129/1000 | Loss: 0.00002101
Iteration 130/1000 | Loss: 0.00002101
Iteration 131/1000 | Loss: 0.00002101
Iteration 132/1000 | Loss: 0.00002101
Iteration 133/1000 | Loss: 0.00002101
Iteration 134/1000 | Loss: 0.00002101
Iteration 135/1000 | Loss: 0.00002101
Iteration 136/1000 | Loss: 0.00002101
Iteration 137/1000 | Loss: 0.00002101
Iteration 138/1000 | Loss: 0.00002101
Iteration 139/1000 | Loss: 0.00002101
Iteration 140/1000 | Loss: 0.00002101
Iteration 141/1000 | Loss: 0.00002101
Iteration 142/1000 | Loss: 0.00002101
Iteration 143/1000 | Loss: 0.00002101
Iteration 144/1000 | Loss: 0.00002101
Iteration 145/1000 | Loss: 0.00002101
Iteration 146/1000 | Loss: 0.00002101
Iteration 147/1000 | Loss: 0.00002101
Iteration 148/1000 | Loss: 0.00002101
Iteration 149/1000 | Loss: 0.00002101
Iteration 150/1000 | Loss: 0.00002101
Iteration 151/1000 | Loss: 0.00002101
Iteration 152/1000 | Loss: 0.00002101
Iteration 153/1000 | Loss: 0.00002101
Iteration 154/1000 | Loss: 0.00002101
Iteration 155/1000 | Loss: 0.00002101
Iteration 156/1000 | Loss: 0.00002101
Iteration 157/1000 | Loss: 0.00002101
Iteration 158/1000 | Loss: 0.00002101
Iteration 159/1000 | Loss: 0.00002101
Iteration 160/1000 | Loss: 0.00002101
Iteration 161/1000 | Loss: 0.00002101
Iteration 162/1000 | Loss: 0.00002101
Iteration 163/1000 | Loss: 0.00002101
Iteration 164/1000 | Loss: 0.00002101
Iteration 165/1000 | Loss: 0.00002101
Iteration 166/1000 | Loss: 0.00002101
Iteration 167/1000 | Loss: 0.00002101
Iteration 168/1000 | Loss: 0.00002101
Iteration 169/1000 | Loss: 0.00002101
Iteration 170/1000 | Loss: 0.00002101
Iteration 171/1000 | Loss: 0.00002101
Iteration 172/1000 | Loss: 0.00002101
Iteration 173/1000 | Loss: 0.00002101
Iteration 174/1000 | Loss: 0.00002101
Iteration 175/1000 | Loss: 0.00002101
Iteration 176/1000 | Loss: 0.00002101
Iteration 177/1000 | Loss: 0.00002101
Iteration 178/1000 | Loss: 0.00002101
Iteration 179/1000 | Loss: 0.00002101
Iteration 180/1000 | Loss: 0.00002101
Iteration 181/1000 | Loss: 0.00002101
Iteration 182/1000 | Loss: 0.00002101
Iteration 183/1000 | Loss: 0.00002101
Iteration 184/1000 | Loss: 0.00002101
Iteration 185/1000 | Loss: 0.00002101
Iteration 186/1000 | Loss: 0.00002101
Iteration 187/1000 | Loss: 0.00002101
Iteration 188/1000 | Loss: 0.00002101
Iteration 189/1000 | Loss: 0.00002101
Iteration 190/1000 | Loss: 0.00002101
Iteration 191/1000 | Loss: 0.00002101
Iteration 192/1000 | Loss: 0.00002101
Iteration 193/1000 | Loss: 0.00002101
Iteration 194/1000 | Loss: 0.00002101
Iteration 195/1000 | Loss: 0.00002101
Iteration 196/1000 | Loss: 0.00002101
Iteration 197/1000 | Loss: 0.00002101
Iteration 198/1000 | Loss: 0.00002101
Iteration 199/1000 | Loss: 0.00002101
Iteration 200/1000 | Loss: 0.00002101
Iteration 201/1000 | Loss: 0.00002101
Iteration 202/1000 | Loss: 0.00002101
Iteration 203/1000 | Loss: 0.00002101
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [2.1007494069635868e-05, 2.1007494069635868e-05, 2.1007494069635868e-05, 2.1007494069635868e-05, 2.1007494069635868e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1007494069635868e-05

Optimization complete. Final v2v error: 3.6687543392181396 mm

Highest mean error: 4.453579902648926 mm for frame 183

Lowest mean error: 2.909170150756836 mm for frame 216

Saving results

Total time: 41.81698513031006
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5760/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5760/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5760/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00559318
Iteration 2/25 | Loss: 0.00140456
Iteration 3/25 | Loss: 0.00133318
Iteration 4/25 | Loss: 0.00132128
Iteration 5/25 | Loss: 0.00131686
Iteration 6/25 | Loss: 0.00131548
Iteration 7/25 | Loss: 0.00131548
Iteration 8/25 | Loss: 0.00131548
Iteration 9/25 | Loss: 0.00131548
Iteration 10/25 | Loss: 0.00131548
Iteration 11/25 | Loss: 0.00131548
Iteration 12/25 | Loss: 0.00131548
Iteration 13/25 | Loss: 0.00131548
Iteration 14/25 | Loss: 0.00131548
Iteration 15/25 | Loss: 0.00131548
Iteration 16/25 | Loss: 0.00131548
Iteration 17/25 | Loss: 0.00131548
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013154784683138132, 0.0013154784683138132, 0.0013154784683138132, 0.0013154784683138132, 0.0013154784683138132]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013154784683138132

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.40461063
Iteration 2/25 | Loss: 0.00362199
Iteration 3/25 | Loss: 0.00362199
Iteration 4/25 | Loss: 0.00362199
Iteration 5/25 | Loss: 0.00362198
Iteration 6/25 | Loss: 0.00362198
Iteration 7/25 | Loss: 0.00362198
Iteration 8/25 | Loss: 0.00362198
Iteration 9/25 | Loss: 0.00362198
Iteration 10/25 | Loss: 0.00362198
Iteration 11/25 | Loss: 0.00362198
Iteration 12/25 | Loss: 0.00362198
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0036219835747033358, 0.0036219835747033358, 0.0036219835747033358, 0.0036219835747033358, 0.0036219835747033358]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0036219835747033358

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00362198
Iteration 2/1000 | Loss: 0.00004730
Iteration 3/1000 | Loss: 0.00003149
Iteration 4/1000 | Loss: 0.00002689
Iteration 5/1000 | Loss: 0.00002370
Iteration 6/1000 | Loss: 0.00002173
Iteration 7/1000 | Loss: 0.00002052
Iteration 8/1000 | Loss: 0.00001966
Iteration 9/1000 | Loss: 0.00001899
Iteration 10/1000 | Loss: 0.00001855
Iteration 11/1000 | Loss: 0.00001831
Iteration 12/1000 | Loss: 0.00001815
Iteration 13/1000 | Loss: 0.00001814
Iteration 14/1000 | Loss: 0.00001806
Iteration 15/1000 | Loss: 0.00001802
Iteration 16/1000 | Loss: 0.00001801
Iteration 17/1000 | Loss: 0.00001800
Iteration 18/1000 | Loss: 0.00001794
Iteration 19/1000 | Loss: 0.00001793
Iteration 20/1000 | Loss: 0.00001790
Iteration 21/1000 | Loss: 0.00001790
Iteration 22/1000 | Loss: 0.00001788
Iteration 23/1000 | Loss: 0.00001786
Iteration 24/1000 | Loss: 0.00001786
Iteration 25/1000 | Loss: 0.00001785
Iteration 26/1000 | Loss: 0.00001784
Iteration 27/1000 | Loss: 0.00001784
Iteration 28/1000 | Loss: 0.00001784
Iteration 29/1000 | Loss: 0.00001784
Iteration 30/1000 | Loss: 0.00001784
Iteration 31/1000 | Loss: 0.00001783
Iteration 32/1000 | Loss: 0.00001783
Iteration 33/1000 | Loss: 0.00001783
Iteration 34/1000 | Loss: 0.00001783
Iteration 35/1000 | Loss: 0.00001783
Iteration 36/1000 | Loss: 0.00001783
Iteration 37/1000 | Loss: 0.00001783
Iteration 38/1000 | Loss: 0.00001783
Iteration 39/1000 | Loss: 0.00001782
Iteration 40/1000 | Loss: 0.00001781
Iteration 41/1000 | Loss: 0.00001781
Iteration 42/1000 | Loss: 0.00001781
Iteration 43/1000 | Loss: 0.00001781
Iteration 44/1000 | Loss: 0.00001781
Iteration 45/1000 | Loss: 0.00001780
Iteration 46/1000 | Loss: 0.00001780
Iteration 47/1000 | Loss: 0.00001780
Iteration 48/1000 | Loss: 0.00001780
Iteration 49/1000 | Loss: 0.00001780
Iteration 50/1000 | Loss: 0.00001780
Iteration 51/1000 | Loss: 0.00001780
Iteration 52/1000 | Loss: 0.00001779
Iteration 53/1000 | Loss: 0.00001779
Iteration 54/1000 | Loss: 0.00001779
Iteration 55/1000 | Loss: 0.00001778
Iteration 56/1000 | Loss: 0.00001778
Iteration 57/1000 | Loss: 0.00001778
Iteration 58/1000 | Loss: 0.00001778
Iteration 59/1000 | Loss: 0.00001778
Iteration 60/1000 | Loss: 0.00001777
Iteration 61/1000 | Loss: 0.00001777
Iteration 62/1000 | Loss: 0.00001777
Iteration 63/1000 | Loss: 0.00001776
Iteration 64/1000 | Loss: 0.00001776
Iteration 65/1000 | Loss: 0.00001776
Iteration 66/1000 | Loss: 0.00001776
Iteration 67/1000 | Loss: 0.00001776
Iteration 68/1000 | Loss: 0.00001776
Iteration 69/1000 | Loss: 0.00001775
Iteration 70/1000 | Loss: 0.00001775
Iteration 71/1000 | Loss: 0.00001775
Iteration 72/1000 | Loss: 0.00001775
Iteration 73/1000 | Loss: 0.00001775
Iteration 74/1000 | Loss: 0.00001775
Iteration 75/1000 | Loss: 0.00001775
Iteration 76/1000 | Loss: 0.00001775
Iteration 77/1000 | Loss: 0.00001775
Iteration 78/1000 | Loss: 0.00001775
Iteration 79/1000 | Loss: 0.00001775
Iteration 80/1000 | Loss: 0.00001774
Iteration 81/1000 | Loss: 0.00001774
Iteration 82/1000 | Loss: 0.00001774
Iteration 83/1000 | Loss: 0.00001774
Iteration 84/1000 | Loss: 0.00001773
Iteration 85/1000 | Loss: 0.00001773
Iteration 86/1000 | Loss: 0.00001773
Iteration 87/1000 | Loss: 0.00001773
Iteration 88/1000 | Loss: 0.00001773
Iteration 89/1000 | Loss: 0.00001773
Iteration 90/1000 | Loss: 0.00001773
Iteration 91/1000 | Loss: 0.00001773
Iteration 92/1000 | Loss: 0.00001773
Iteration 93/1000 | Loss: 0.00001772
Iteration 94/1000 | Loss: 0.00001772
Iteration 95/1000 | Loss: 0.00001772
Iteration 96/1000 | Loss: 0.00001772
Iteration 97/1000 | Loss: 0.00001772
Iteration 98/1000 | Loss: 0.00001771
Iteration 99/1000 | Loss: 0.00001771
Iteration 100/1000 | Loss: 0.00001771
Iteration 101/1000 | Loss: 0.00001770
Iteration 102/1000 | Loss: 0.00001770
Iteration 103/1000 | Loss: 0.00001770
Iteration 104/1000 | Loss: 0.00001770
Iteration 105/1000 | Loss: 0.00001770
Iteration 106/1000 | Loss: 0.00001770
Iteration 107/1000 | Loss: 0.00001769
Iteration 108/1000 | Loss: 0.00001769
Iteration 109/1000 | Loss: 0.00001769
Iteration 110/1000 | Loss: 0.00001769
Iteration 111/1000 | Loss: 0.00001769
Iteration 112/1000 | Loss: 0.00001769
Iteration 113/1000 | Loss: 0.00001769
Iteration 114/1000 | Loss: 0.00001769
Iteration 115/1000 | Loss: 0.00001768
Iteration 116/1000 | Loss: 0.00001768
Iteration 117/1000 | Loss: 0.00001768
Iteration 118/1000 | Loss: 0.00001768
Iteration 119/1000 | Loss: 0.00001768
Iteration 120/1000 | Loss: 0.00001768
Iteration 121/1000 | Loss: 0.00001768
Iteration 122/1000 | Loss: 0.00001768
Iteration 123/1000 | Loss: 0.00001768
Iteration 124/1000 | Loss: 0.00001768
Iteration 125/1000 | Loss: 0.00001768
Iteration 126/1000 | Loss: 0.00001768
Iteration 127/1000 | Loss: 0.00001768
Iteration 128/1000 | Loss: 0.00001768
Iteration 129/1000 | Loss: 0.00001768
Iteration 130/1000 | Loss: 0.00001768
Iteration 131/1000 | Loss: 0.00001767
Iteration 132/1000 | Loss: 0.00001767
Iteration 133/1000 | Loss: 0.00001767
Iteration 134/1000 | Loss: 0.00001767
Iteration 135/1000 | Loss: 0.00001767
Iteration 136/1000 | Loss: 0.00001767
Iteration 137/1000 | Loss: 0.00001767
Iteration 138/1000 | Loss: 0.00001767
Iteration 139/1000 | Loss: 0.00001767
Iteration 140/1000 | Loss: 0.00001767
Iteration 141/1000 | Loss: 0.00001767
Iteration 142/1000 | Loss: 0.00001767
Iteration 143/1000 | Loss: 0.00001767
Iteration 144/1000 | Loss: 0.00001767
Iteration 145/1000 | Loss: 0.00001767
Iteration 146/1000 | Loss: 0.00001767
Iteration 147/1000 | Loss: 0.00001767
Iteration 148/1000 | Loss: 0.00001767
Iteration 149/1000 | Loss: 0.00001767
Iteration 150/1000 | Loss: 0.00001767
Iteration 151/1000 | Loss: 0.00001767
Iteration 152/1000 | Loss: 0.00001767
Iteration 153/1000 | Loss: 0.00001767
Iteration 154/1000 | Loss: 0.00001767
Iteration 155/1000 | Loss: 0.00001767
Iteration 156/1000 | Loss: 0.00001767
Iteration 157/1000 | Loss: 0.00001767
Iteration 158/1000 | Loss: 0.00001767
Iteration 159/1000 | Loss: 0.00001767
Iteration 160/1000 | Loss: 0.00001767
Iteration 161/1000 | Loss: 0.00001767
Iteration 162/1000 | Loss: 0.00001767
Iteration 163/1000 | Loss: 0.00001767
Iteration 164/1000 | Loss: 0.00001767
Iteration 165/1000 | Loss: 0.00001767
Iteration 166/1000 | Loss: 0.00001767
Iteration 167/1000 | Loss: 0.00001767
Iteration 168/1000 | Loss: 0.00001767
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.766752393450588e-05, 1.766752393450588e-05, 1.766752393450588e-05, 1.766752393450588e-05, 1.766752393450588e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.766752393450588e-05

Optimization complete. Final v2v error: 3.594651699066162 mm

Highest mean error: 3.878108024597168 mm for frame 99

Lowest mean error: 3.3551509380340576 mm for frame 86

Saving results

Total time: 38.79759693145752
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5760/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5760/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5760/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00831272
Iteration 2/25 | Loss: 0.00143220
Iteration 3/25 | Loss: 0.00129818
Iteration 4/25 | Loss: 0.00128434
Iteration 5/25 | Loss: 0.00128071
Iteration 6/25 | Loss: 0.00128017
Iteration 7/25 | Loss: 0.00128017
Iteration 8/25 | Loss: 0.00128017
Iteration 9/25 | Loss: 0.00128017
Iteration 10/25 | Loss: 0.00128017
Iteration 11/25 | Loss: 0.00128017
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012801697012037039, 0.0012801697012037039, 0.0012801697012037039, 0.0012801697012037039, 0.0012801697012037039]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012801697012037039

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11717224
Iteration 2/25 | Loss: 0.00368579
Iteration 3/25 | Loss: 0.00368579
Iteration 4/25 | Loss: 0.00368579
Iteration 5/25 | Loss: 0.00368579
Iteration 6/25 | Loss: 0.00368579
Iteration 7/25 | Loss: 0.00368579
Iteration 8/25 | Loss: 0.00368579
Iteration 9/25 | Loss: 0.00368579
Iteration 10/25 | Loss: 0.00368579
Iteration 11/25 | Loss: 0.00368579
Iteration 12/25 | Loss: 0.00368579
Iteration 13/25 | Loss: 0.00368579
Iteration 14/25 | Loss: 0.00368579
Iteration 15/25 | Loss: 0.00368579
Iteration 16/25 | Loss: 0.00368579
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0036857882514595985, 0.0036857882514595985, 0.0036857882514595985, 0.0036857882514595985, 0.0036857882514595985]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0036857882514595985

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00368579
Iteration 2/1000 | Loss: 0.00003668
Iteration 3/1000 | Loss: 0.00002250
Iteration 4/1000 | Loss: 0.00001968
Iteration 5/1000 | Loss: 0.00001717
Iteration 6/1000 | Loss: 0.00001606
Iteration 7/1000 | Loss: 0.00001530
Iteration 8/1000 | Loss: 0.00001482
Iteration 9/1000 | Loss: 0.00001445
Iteration 10/1000 | Loss: 0.00001418
Iteration 11/1000 | Loss: 0.00001388
Iteration 12/1000 | Loss: 0.00001372
Iteration 13/1000 | Loss: 0.00001363
Iteration 14/1000 | Loss: 0.00001356
Iteration 15/1000 | Loss: 0.00001352
Iteration 16/1000 | Loss: 0.00001352
Iteration 17/1000 | Loss: 0.00001351
Iteration 18/1000 | Loss: 0.00001351
Iteration 19/1000 | Loss: 0.00001350
Iteration 20/1000 | Loss: 0.00001345
Iteration 21/1000 | Loss: 0.00001345
Iteration 22/1000 | Loss: 0.00001343
Iteration 23/1000 | Loss: 0.00001343
Iteration 24/1000 | Loss: 0.00001343
Iteration 25/1000 | Loss: 0.00001343
Iteration 26/1000 | Loss: 0.00001343
Iteration 27/1000 | Loss: 0.00001343
Iteration 28/1000 | Loss: 0.00001343
Iteration 29/1000 | Loss: 0.00001342
Iteration 30/1000 | Loss: 0.00001342
Iteration 31/1000 | Loss: 0.00001342
Iteration 32/1000 | Loss: 0.00001341
Iteration 33/1000 | Loss: 0.00001340
Iteration 34/1000 | Loss: 0.00001340
Iteration 35/1000 | Loss: 0.00001340
Iteration 36/1000 | Loss: 0.00001340
Iteration 37/1000 | Loss: 0.00001339
Iteration 38/1000 | Loss: 0.00001339
Iteration 39/1000 | Loss: 0.00001339
Iteration 40/1000 | Loss: 0.00001339
Iteration 41/1000 | Loss: 0.00001338
Iteration 42/1000 | Loss: 0.00001338
Iteration 43/1000 | Loss: 0.00001338
Iteration 44/1000 | Loss: 0.00001337
Iteration 45/1000 | Loss: 0.00001337
Iteration 46/1000 | Loss: 0.00001337
Iteration 47/1000 | Loss: 0.00001336
Iteration 48/1000 | Loss: 0.00001336
Iteration 49/1000 | Loss: 0.00001336
Iteration 50/1000 | Loss: 0.00001336
Iteration 51/1000 | Loss: 0.00001335
Iteration 52/1000 | Loss: 0.00001335
Iteration 53/1000 | Loss: 0.00001335
Iteration 54/1000 | Loss: 0.00001335
Iteration 55/1000 | Loss: 0.00001335
Iteration 56/1000 | Loss: 0.00001335
Iteration 57/1000 | Loss: 0.00001335
Iteration 58/1000 | Loss: 0.00001335
Iteration 59/1000 | Loss: 0.00001335
Iteration 60/1000 | Loss: 0.00001335
Iteration 61/1000 | Loss: 0.00001335
Iteration 62/1000 | Loss: 0.00001335
Iteration 63/1000 | Loss: 0.00001334
Iteration 64/1000 | Loss: 0.00001334
Iteration 65/1000 | Loss: 0.00001334
Iteration 66/1000 | Loss: 0.00001334
Iteration 67/1000 | Loss: 0.00001334
Iteration 68/1000 | Loss: 0.00001334
Iteration 69/1000 | Loss: 0.00001334
Iteration 70/1000 | Loss: 0.00001334
Iteration 71/1000 | Loss: 0.00001334
Iteration 72/1000 | Loss: 0.00001333
Iteration 73/1000 | Loss: 0.00001333
Iteration 74/1000 | Loss: 0.00001333
Iteration 75/1000 | Loss: 0.00001333
Iteration 76/1000 | Loss: 0.00001333
Iteration 77/1000 | Loss: 0.00001333
Iteration 78/1000 | Loss: 0.00001333
Iteration 79/1000 | Loss: 0.00001333
Iteration 80/1000 | Loss: 0.00001333
Iteration 81/1000 | Loss: 0.00001333
Iteration 82/1000 | Loss: 0.00001333
Iteration 83/1000 | Loss: 0.00001332
Iteration 84/1000 | Loss: 0.00001332
Iteration 85/1000 | Loss: 0.00001332
Iteration 86/1000 | Loss: 0.00001332
Iteration 87/1000 | Loss: 0.00001331
Iteration 88/1000 | Loss: 0.00001331
Iteration 89/1000 | Loss: 0.00001331
Iteration 90/1000 | Loss: 0.00001331
Iteration 91/1000 | Loss: 0.00001331
Iteration 92/1000 | Loss: 0.00001331
Iteration 93/1000 | Loss: 0.00001331
Iteration 94/1000 | Loss: 0.00001331
Iteration 95/1000 | Loss: 0.00001331
Iteration 96/1000 | Loss: 0.00001331
Iteration 97/1000 | Loss: 0.00001331
Iteration 98/1000 | Loss: 0.00001331
Iteration 99/1000 | Loss: 0.00001331
Iteration 100/1000 | Loss: 0.00001330
Iteration 101/1000 | Loss: 0.00001330
Iteration 102/1000 | Loss: 0.00001330
Iteration 103/1000 | Loss: 0.00001330
Iteration 104/1000 | Loss: 0.00001330
Iteration 105/1000 | Loss: 0.00001330
Iteration 106/1000 | Loss: 0.00001330
Iteration 107/1000 | Loss: 0.00001330
Iteration 108/1000 | Loss: 0.00001330
Iteration 109/1000 | Loss: 0.00001330
Iteration 110/1000 | Loss: 0.00001330
Iteration 111/1000 | Loss: 0.00001330
Iteration 112/1000 | Loss: 0.00001330
Iteration 113/1000 | Loss: 0.00001330
Iteration 114/1000 | Loss: 0.00001330
Iteration 115/1000 | Loss: 0.00001330
Iteration 116/1000 | Loss: 0.00001330
Iteration 117/1000 | Loss: 0.00001330
Iteration 118/1000 | Loss: 0.00001330
Iteration 119/1000 | Loss: 0.00001330
Iteration 120/1000 | Loss: 0.00001330
Iteration 121/1000 | Loss: 0.00001330
Iteration 122/1000 | Loss: 0.00001330
Iteration 123/1000 | Loss: 0.00001329
Iteration 124/1000 | Loss: 0.00001329
Iteration 125/1000 | Loss: 0.00001329
Iteration 126/1000 | Loss: 0.00001329
Iteration 127/1000 | Loss: 0.00001329
Iteration 128/1000 | Loss: 0.00001329
Iteration 129/1000 | Loss: 0.00001329
Iteration 130/1000 | Loss: 0.00001329
Iteration 131/1000 | Loss: 0.00001329
Iteration 132/1000 | Loss: 0.00001329
Iteration 133/1000 | Loss: 0.00001329
Iteration 134/1000 | Loss: 0.00001329
Iteration 135/1000 | Loss: 0.00001329
Iteration 136/1000 | Loss: 0.00001329
Iteration 137/1000 | Loss: 0.00001329
Iteration 138/1000 | Loss: 0.00001329
Iteration 139/1000 | Loss: 0.00001329
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.3294274140207563e-05, 1.3294274140207563e-05, 1.3294274140207563e-05, 1.3294274140207563e-05, 1.3294274140207563e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3294274140207563e-05

Optimization complete. Final v2v error: 3.0793895721435547 mm

Highest mean error: 3.491816759109497 mm for frame 84

Lowest mean error: 2.820824146270752 mm for frame 175

Saving results

Total time: 37.123199462890625
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5760/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5760/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5760/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00534214
Iteration 2/25 | Loss: 0.00147685
Iteration 3/25 | Loss: 0.00134881
Iteration 4/25 | Loss: 0.00133375
Iteration 5/25 | Loss: 0.00132409
Iteration 6/25 | Loss: 0.00132086
Iteration 7/25 | Loss: 0.00132026
Iteration 8/25 | Loss: 0.00132007
Iteration 9/25 | Loss: 0.00132002
Iteration 10/25 | Loss: 0.00132002
Iteration 11/25 | Loss: 0.00132002
Iteration 12/25 | Loss: 0.00132000
Iteration 13/25 | Loss: 0.00131999
Iteration 14/25 | Loss: 0.00131999
Iteration 15/25 | Loss: 0.00131999
Iteration 16/25 | Loss: 0.00131998
Iteration 17/25 | Loss: 0.00131998
Iteration 18/25 | Loss: 0.00131998
Iteration 19/25 | Loss: 0.00131998
Iteration 20/25 | Loss: 0.00131998
Iteration 21/25 | Loss: 0.00131998
Iteration 22/25 | Loss: 0.00131998
Iteration 23/25 | Loss: 0.00131997
Iteration 24/25 | Loss: 0.00131997
Iteration 25/25 | Loss: 0.00131997

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.60678387
Iteration 2/25 | Loss: 0.00359870
Iteration 3/25 | Loss: 0.00359868
Iteration 4/25 | Loss: 0.00359868
Iteration 5/25 | Loss: 0.00359868
Iteration 6/25 | Loss: 0.00359868
Iteration 7/25 | Loss: 0.00359868
Iteration 8/25 | Loss: 0.00359868
Iteration 9/25 | Loss: 0.00359868
Iteration 10/25 | Loss: 0.00359868
Iteration 11/25 | Loss: 0.00359868
Iteration 12/25 | Loss: 0.00359868
Iteration 13/25 | Loss: 0.00359868
Iteration 14/25 | Loss: 0.00359868
Iteration 15/25 | Loss: 0.00359868
Iteration 16/25 | Loss: 0.00359868
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0035986769944429398, 0.0035986769944429398, 0.0035986769944429398, 0.0035986769944429398, 0.0035986769944429398]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0035986769944429398

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00359868
Iteration 2/1000 | Loss: 0.00004012
Iteration 3/1000 | Loss: 0.00002758
Iteration 4/1000 | Loss: 0.00002350
Iteration 5/1000 | Loss: 0.00002130
Iteration 6/1000 | Loss: 0.00002028
Iteration 7/1000 | Loss: 0.00004136
Iteration 8/1000 | Loss: 0.00001950
Iteration 9/1000 | Loss: 0.00004309
Iteration 10/1000 | Loss: 0.00001857
Iteration 11/1000 | Loss: 0.00001845
Iteration 12/1000 | Loss: 0.00001821
Iteration 13/1000 | Loss: 0.00001796
Iteration 14/1000 | Loss: 0.00001796
Iteration 15/1000 | Loss: 0.00001796
Iteration 16/1000 | Loss: 0.00001795
Iteration 17/1000 | Loss: 0.00001794
Iteration 18/1000 | Loss: 0.00001794
Iteration 19/1000 | Loss: 0.00004877
Iteration 20/1000 | Loss: 0.00001780
Iteration 21/1000 | Loss: 0.00001779
Iteration 22/1000 | Loss: 0.00001774
Iteration 23/1000 | Loss: 0.00001774
Iteration 24/1000 | Loss: 0.00001773
Iteration 25/1000 | Loss: 0.00004125
Iteration 26/1000 | Loss: 0.00001771
Iteration 27/1000 | Loss: 0.00001763
Iteration 28/1000 | Loss: 0.00001762
Iteration 29/1000 | Loss: 0.00001761
Iteration 30/1000 | Loss: 0.00001761
Iteration 31/1000 | Loss: 0.00001761
Iteration 32/1000 | Loss: 0.00001760
Iteration 33/1000 | Loss: 0.00001760
Iteration 34/1000 | Loss: 0.00001760
Iteration 35/1000 | Loss: 0.00001760
Iteration 36/1000 | Loss: 0.00001760
Iteration 37/1000 | Loss: 0.00001760
Iteration 38/1000 | Loss: 0.00001760
Iteration 39/1000 | Loss: 0.00001760
Iteration 40/1000 | Loss: 0.00001760
Iteration 41/1000 | Loss: 0.00001760
Iteration 42/1000 | Loss: 0.00001760
Iteration 43/1000 | Loss: 0.00001760
Iteration 44/1000 | Loss: 0.00001759
Iteration 45/1000 | Loss: 0.00001759
Iteration 46/1000 | Loss: 0.00001759
Iteration 47/1000 | Loss: 0.00001758
Iteration 48/1000 | Loss: 0.00001758
Iteration 49/1000 | Loss: 0.00001758
Iteration 50/1000 | Loss: 0.00001758
Iteration 51/1000 | Loss: 0.00001757
Iteration 52/1000 | Loss: 0.00001757
Iteration 53/1000 | Loss: 0.00001757
Iteration 54/1000 | Loss: 0.00001757
Iteration 55/1000 | Loss: 0.00001757
Iteration 56/1000 | Loss: 0.00001757
Iteration 57/1000 | Loss: 0.00001757
Iteration 58/1000 | Loss: 0.00001757
Iteration 59/1000 | Loss: 0.00001757
Iteration 60/1000 | Loss: 0.00001756
Iteration 61/1000 | Loss: 0.00001756
Iteration 62/1000 | Loss: 0.00001756
Iteration 63/1000 | Loss: 0.00001756
Iteration 64/1000 | Loss: 0.00001756
Iteration 65/1000 | Loss: 0.00001756
Iteration 66/1000 | Loss: 0.00001756
Iteration 67/1000 | Loss: 0.00001755
Iteration 68/1000 | Loss: 0.00001755
Iteration 69/1000 | Loss: 0.00001755
Iteration 70/1000 | Loss: 0.00001755
Iteration 71/1000 | Loss: 0.00001755
Iteration 72/1000 | Loss: 0.00001755
Iteration 73/1000 | Loss: 0.00001755
Iteration 74/1000 | Loss: 0.00001755
Iteration 75/1000 | Loss: 0.00001755
Iteration 76/1000 | Loss: 0.00001755
Iteration 77/1000 | Loss: 0.00001755
Iteration 78/1000 | Loss: 0.00001754
Iteration 79/1000 | Loss: 0.00001754
Iteration 80/1000 | Loss: 0.00001754
Iteration 81/1000 | Loss: 0.00001754
Iteration 82/1000 | Loss: 0.00001754
Iteration 83/1000 | Loss: 0.00001754
Iteration 84/1000 | Loss: 0.00001753
Iteration 85/1000 | Loss: 0.00001753
Iteration 86/1000 | Loss: 0.00004961
Iteration 87/1000 | Loss: 0.00003129
Iteration 88/1000 | Loss: 0.00001753
Iteration 89/1000 | Loss: 0.00001753
Iteration 90/1000 | Loss: 0.00001753
Iteration 91/1000 | Loss: 0.00001752
Iteration 92/1000 | Loss: 0.00001752
Iteration 93/1000 | Loss: 0.00001752
Iteration 94/1000 | Loss: 0.00001752
Iteration 95/1000 | Loss: 0.00001751
Iteration 96/1000 | Loss: 0.00001751
Iteration 97/1000 | Loss: 0.00001751
Iteration 98/1000 | Loss: 0.00001751
Iteration 99/1000 | Loss: 0.00001751
Iteration 100/1000 | Loss: 0.00001751
Iteration 101/1000 | Loss: 0.00001751
Iteration 102/1000 | Loss: 0.00004800
Iteration 103/1000 | Loss: 0.00001755
Iteration 104/1000 | Loss: 0.00001751
Iteration 105/1000 | Loss: 0.00001750
Iteration 106/1000 | Loss: 0.00001750
Iteration 107/1000 | Loss: 0.00001750
Iteration 108/1000 | Loss: 0.00001749
Iteration 109/1000 | Loss: 0.00001749
Iteration 110/1000 | Loss: 0.00001749
Iteration 111/1000 | Loss: 0.00001749
Iteration 112/1000 | Loss: 0.00001749
Iteration 113/1000 | Loss: 0.00001749
Iteration 114/1000 | Loss: 0.00001749
Iteration 115/1000 | Loss: 0.00001749
Iteration 116/1000 | Loss: 0.00001749
Iteration 117/1000 | Loss: 0.00001748
Iteration 118/1000 | Loss: 0.00001748
Iteration 119/1000 | Loss: 0.00001748
Iteration 120/1000 | Loss: 0.00001748
Iteration 121/1000 | Loss: 0.00001748
Iteration 122/1000 | Loss: 0.00001748
Iteration 123/1000 | Loss: 0.00001748
Iteration 124/1000 | Loss: 0.00001748
Iteration 125/1000 | Loss: 0.00001747
Iteration 126/1000 | Loss: 0.00001747
Iteration 127/1000 | Loss: 0.00001747
Iteration 128/1000 | Loss: 0.00001747
Iteration 129/1000 | Loss: 0.00001747
Iteration 130/1000 | Loss: 0.00001747
Iteration 131/1000 | Loss: 0.00001747
Iteration 132/1000 | Loss: 0.00001747
Iteration 133/1000 | Loss: 0.00001747
Iteration 134/1000 | Loss: 0.00001747
Iteration 135/1000 | Loss: 0.00001747
Iteration 136/1000 | Loss: 0.00001747
Iteration 137/1000 | Loss: 0.00001747
Iteration 138/1000 | Loss: 0.00001747
Iteration 139/1000 | Loss: 0.00001747
Iteration 140/1000 | Loss: 0.00001747
Iteration 141/1000 | Loss: 0.00001747
Iteration 142/1000 | Loss: 0.00001747
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [1.74705455719959e-05, 1.74705455719959e-05, 1.74705455719959e-05, 1.74705455719959e-05, 1.74705455719959e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.74705455719959e-05

Optimization complete. Final v2v error: 3.5766899585723877 mm

Highest mean error: 4.280869960784912 mm for frame 200

Lowest mean error: 3.1921727657318115 mm for frame 136

Saving results

Total time: 58.50052309036255
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5760/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5760/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5760/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00411104
Iteration 2/25 | Loss: 0.00143297
Iteration 3/25 | Loss: 0.00133950
Iteration 4/25 | Loss: 0.00132936
Iteration 5/25 | Loss: 0.00132692
Iteration 6/25 | Loss: 0.00132637
Iteration 7/25 | Loss: 0.00132637
Iteration 8/25 | Loss: 0.00132637
Iteration 9/25 | Loss: 0.00132637
Iteration 10/25 | Loss: 0.00132637
Iteration 11/25 | Loss: 0.00132637
Iteration 12/25 | Loss: 0.00132637
Iteration 13/25 | Loss: 0.00132637
Iteration 14/25 | Loss: 0.00132637
Iteration 15/25 | Loss: 0.00132637
Iteration 16/25 | Loss: 0.00132637
Iteration 17/25 | Loss: 0.00132637
Iteration 18/25 | Loss: 0.00132637
Iteration 19/25 | Loss: 0.00132637
Iteration 20/25 | Loss: 0.00132637
Iteration 21/25 | Loss: 0.00132637
Iteration 22/25 | Loss: 0.00132637
Iteration 23/25 | Loss: 0.00132637
Iteration 24/25 | Loss: 0.00132637
Iteration 25/25 | Loss: 0.00132637
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0013263747096061707, 0.0013263747096061707, 0.0013263747096061707, 0.0013263747096061707, 0.0013263747096061707]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013263747096061707

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11927903
Iteration 2/25 | Loss: 0.00357227
Iteration 3/25 | Loss: 0.00357226
Iteration 4/25 | Loss: 0.00357226
Iteration 5/25 | Loss: 0.00357226
Iteration 6/25 | Loss: 0.00357226
Iteration 7/25 | Loss: 0.00357226
Iteration 8/25 | Loss: 0.00357226
Iteration 9/25 | Loss: 0.00357226
Iteration 10/25 | Loss: 0.00357226
Iteration 11/25 | Loss: 0.00357226
Iteration 12/25 | Loss: 0.00357226
Iteration 13/25 | Loss: 0.00357226
Iteration 14/25 | Loss: 0.00357226
Iteration 15/25 | Loss: 0.00357226
Iteration 16/25 | Loss: 0.00357226
Iteration 17/25 | Loss: 0.00357226
Iteration 18/25 | Loss: 0.00357226
Iteration 19/25 | Loss: 0.00357226
Iteration 20/25 | Loss: 0.00357226
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00357226119376719, 0.00357226119376719, 0.00357226119376719, 0.00357226119376719, 0.00357226119376719]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00357226119376719

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00357226
Iteration 2/1000 | Loss: 0.00006775
Iteration 3/1000 | Loss: 0.00003764
Iteration 4/1000 | Loss: 0.00002969
Iteration 5/1000 | Loss: 0.00002526
Iteration 6/1000 | Loss: 0.00002312
Iteration 7/1000 | Loss: 0.00002161
Iteration 8/1000 | Loss: 0.00002072
Iteration 9/1000 | Loss: 0.00002013
Iteration 10/1000 | Loss: 0.00001967
Iteration 11/1000 | Loss: 0.00001934
Iteration 12/1000 | Loss: 0.00001923
Iteration 13/1000 | Loss: 0.00001908
Iteration 14/1000 | Loss: 0.00001902
Iteration 15/1000 | Loss: 0.00001902
Iteration 16/1000 | Loss: 0.00001900
Iteration 17/1000 | Loss: 0.00001899
Iteration 18/1000 | Loss: 0.00001898
Iteration 19/1000 | Loss: 0.00001898
Iteration 20/1000 | Loss: 0.00001897
Iteration 21/1000 | Loss: 0.00001896
Iteration 22/1000 | Loss: 0.00001895
Iteration 23/1000 | Loss: 0.00001895
Iteration 24/1000 | Loss: 0.00001893
Iteration 25/1000 | Loss: 0.00001892
Iteration 26/1000 | Loss: 0.00001891
Iteration 27/1000 | Loss: 0.00001891
Iteration 28/1000 | Loss: 0.00001890
Iteration 29/1000 | Loss: 0.00001889
Iteration 30/1000 | Loss: 0.00001889
Iteration 31/1000 | Loss: 0.00001889
Iteration 32/1000 | Loss: 0.00001889
Iteration 33/1000 | Loss: 0.00001888
Iteration 34/1000 | Loss: 0.00001887
Iteration 35/1000 | Loss: 0.00001887
Iteration 36/1000 | Loss: 0.00001886
Iteration 37/1000 | Loss: 0.00001885
Iteration 38/1000 | Loss: 0.00001885
Iteration 39/1000 | Loss: 0.00001885
Iteration 40/1000 | Loss: 0.00001885
Iteration 41/1000 | Loss: 0.00001884
Iteration 42/1000 | Loss: 0.00001883
Iteration 43/1000 | Loss: 0.00001882
Iteration 44/1000 | Loss: 0.00001881
Iteration 45/1000 | Loss: 0.00001881
Iteration 46/1000 | Loss: 0.00001880
Iteration 47/1000 | Loss: 0.00001880
Iteration 48/1000 | Loss: 0.00001880
Iteration 49/1000 | Loss: 0.00001878
Iteration 50/1000 | Loss: 0.00001878
Iteration 51/1000 | Loss: 0.00001877
Iteration 52/1000 | Loss: 0.00001877
Iteration 53/1000 | Loss: 0.00001877
Iteration 54/1000 | Loss: 0.00001876
Iteration 55/1000 | Loss: 0.00001876
Iteration 56/1000 | Loss: 0.00001875
Iteration 57/1000 | Loss: 0.00001875
Iteration 58/1000 | Loss: 0.00001874
Iteration 59/1000 | Loss: 0.00001874
Iteration 60/1000 | Loss: 0.00001874
Iteration 61/1000 | Loss: 0.00001873
Iteration 62/1000 | Loss: 0.00001873
Iteration 63/1000 | Loss: 0.00001873
Iteration 64/1000 | Loss: 0.00001873
Iteration 65/1000 | Loss: 0.00001873
Iteration 66/1000 | Loss: 0.00001873
Iteration 67/1000 | Loss: 0.00001873
Iteration 68/1000 | Loss: 0.00001872
Iteration 69/1000 | Loss: 0.00001872
Iteration 70/1000 | Loss: 0.00001872
Iteration 71/1000 | Loss: 0.00001871
Iteration 72/1000 | Loss: 0.00001871
Iteration 73/1000 | Loss: 0.00001871
Iteration 74/1000 | Loss: 0.00001870
Iteration 75/1000 | Loss: 0.00001870
Iteration 76/1000 | Loss: 0.00001869
Iteration 77/1000 | Loss: 0.00001869
Iteration 78/1000 | Loss: 0.00001869
Iteration 79/1000 | Loss: 0.00001869
Iteration 80/1000 | Loss: 0.00001869
Iteration 81/1000 | Loss: 0.00001868
Iteration 82/1000 | Loss: 0.00001868
Iteration 83/1000 | Loss: 0.00001867
Iteration 84/1000 | Loss: 0.00001867
Iteration 85/1000 | Loss: 0.00001867
Iteration 86/1000 | Loss: 0.00001867
Iteration 87/1000 | Loss: 0.00001867
Iteration 88/1000 | Loss: 0.00001867
Iteration 89/1000 | Loss: 0.00001867
Iteration 90/1000 | Loss: 0.00001866
Iteration 91/1000 | Loss: 0.00001866
Iteration 92/1000 | Loss: 0.00001866
Iteration 93/1000 | Loss: 0.00001865
Iteration 94/1000 | Loss: 0.00001865
Iteration 95/1000 | Loss: 0.00001865
Iteration 96/1000 | Loss: 0.00001865
Iteration 97/1000 | Loss: 0.00001864
Iteration 98/1000 | Loss: 0.00001864
Iteration 99/1000 | Loss: 0.00001863
Iteration 100/1000 | Loss: 0.00001863
Iteration 101/1000 | Loss: 0.00001863
Iteration 102/1000 | Loss: 0.00001863
Iteration 103/1000 | Loss: 0.00001863
Iteration 104/1000 | Loss: 0.00001863
Iteration 105/1000 | Loss: 0.00001863
Iteration 106/1000 | Loss: 0.00001863
Iteration 107/1000 | Loss: 0.00001862
Iteration 108/1000 | Loss: 0.00001862
Iteration 109/1000 | Loss: 0.00001862
Iteration 110/1000 | Loss: 0.00001862
Iteration 111/1000 | Loss: 0.00001862
Iteration 112/1000 | Loss: 0.00001862
Iteration 113/1000 | Loss: 0.00001861
Iteration 114/1000 | Loss: 0.00001861
Iteration 115/1000 | Loss: 0.00001861
Iteration 116/1000 | Loss: 0.00001861
Iteration 117/1000 | Loss: 0.00001861
Iteration 118/1000 | Loss: 0.00001860
Iteration 119/1000 | Loss: 0.00001860
Iteration 120/1000 | Loss: 0.00001860
Iteration 121/1000 | Loss: 0.00001860
Iteration 122/1000 | Loss: 0.00001860
Iteration 123/1000 | Loss: 0.00001860
Iteration 124/1000 | Loss: 0.00001860
Iteration 125/1000 | Loss: 0.00001860
Iteration 126/1000 | Loss: 0.00001860
Iteration 127/1000 | Loss: 0.00001860
Iteration 128/1000 | Loss: 0.00001859
Iteration 129/1000 | Loss: 0.00001859
Iteration 130/1000 | Loss: 0.00001859
Iteration 131/1000 | Loss: 0.00001859
Iteration 132/1000 | Loss: 0.00001859
Iteration 133/1000 | Loss: 0.00001859
Iteration 134/1000 | Loss: 0.00001858
Iteration 135/1000 | Loss: 0.00001858
Iteration 136/1000 | Loss: 0.00001858
Iteration 137/1000 | Loss: 0.00001858
Iteration 138/1000 | Loss: 0.00001858
Iteration 139/1000 | Loss: 0.00001858
Iteration 140/1000 | Loss: 0.00001858
Iteration 141/1000 | Loss: 0.00001858
Iteration 142/1000 | Loss: 0.00001858
Iteration 143/1000 | Loss: 0.00001858
Iteration 144/1000 | Loss: 0.00001858
Iteration 145/1000 | Loss: 0.00001858
Iteration 146/1000 | Loss: 0.00001858
Iteration 147/1000 | Loss: 0.00001858
Iteration 148/1000 | Loss: 0.00001858
Iteration 149/1000 | Loss: 0.00001857
Iteration 150/1000 | Loss: 0.00001857
Iteration 151/1000 | Loss: 0.00001857
Iteration 152/1000 | Loss: 0.00001857
Iteration 153/1000 | Loss: 0.00001857
Iteration 154/1000 | Loss: 0.00001857
Iteration 155/1000 | Loss: 0.00001857
Iteration 156/1000 | Loss: 0.00001857
Iteration 157/1000 | Loss: 0.00001857
Iteration 158/1000 | Loss: 0.00001856
Iteration 159/1000 | Loss: 0.00001856
Iteration 160/1000 | Loss: 0.00001856
Iteration 161/1000 | Loss: 0.00001856
Iteration 162/1000 | Loss: 0.00001856
Iteration 163/1000 | Loss: 0.00001856
Iteration 164/1000 | Loss: 0.00001856
Iteration 165/1000 | Loss: 0.00001856
Iteration 166/1000 | Loss: 0.00001856
Iteration 167/1000 | Loss: 0.00001855
Iteration 168/1000 | Loss: 0.00001855
Iteration 169/1000 | Loss: 0.00001855
Iteration 170/1000 | Loss: 0.00001855
Iteration 171/1000 | Loss: 0.00001855
Iteration 172/1000 | Loss: 0.00001854
Iteration 173/1000 | Loss: 0.00001854
Iteration 174/1000 | Loss: 0.00001854
Iteration 175/1000 | Loss: 0.00001854
Iteration 176/1000 | Loss: 0.00001854
Iteration 177/1000 | Loss: 0.00001854
Iteration 178/1000 | Loss: 0.00001854
Iteration 179/1000 | Loss: 0.00001854
Iteration 180/1000 | Loss: 0.00001854
Iteration 181/1000 | Loss: 0.00001854
Iteration 182/1000 | Loss: 0.00001854
Iteration 183/1000 | Loss: 0.00001854
Iteration 184/1000 | Loss: 0.00001854
Iteration 185/1000 | Loss: 0.00001854
Iteration 186/1000 | Loss: 0.00001854
Iteration 187/1000 | Loss: 0.00001854
Iteration 188/1000 | Loss: 0.00001854
Iteration 189/1000 | Loss: 0.00001854
Iteration 190/1000 | Loss: 0.00001853
Iteration 191/1000 | Loss: 0.00001853
Iteration 192/1000 | Loss: 0.00001853
Iteration 193/1000 | Loss: 0.00001853
Iteration 194/1000 | Loss: 0.00001853
Iteration 195/1000 | Loss: 0.00001853
Iteration 196/1000 | Loss: 0.00001853
Iteration 197/1000 | Loss: 0.00001853
Iteration 198/1000 | Loss: 0.00001853
Iteration 199/1000 | Loss: 0.00001853
Iteration 200/1000 | Loss: 0.00001853
Iteration 201/1000 | Loss: 0.00001853
Iteration 202/1000 | Loss: 0.00001852
Iteration 203/1000 | Loss: 0.00001852
Iteration 204/1000 | Loss: 0.00001852
Iteration 205/1000 | Loss: 0.00001852
Iteration 206/1000 | Loss: 0.00001852
Iteration 207/1000 | Loss: 0.00001852
Iteration 208/1000 | Loss: 0.00001852
Iteration 209/1000 | Loss: 0.00001852
Iteration 210/1000 | Loss: 0.00001852
Iteration 211/1000 | Loss: 0.00001852
Iteration 212/1000 | Loss: 0.00001852
Iteration 213/1000 | Loss: 0.00001852
Iteration 214/1000 | Loss: 0.00001852
Iteration 215/1000 | Loss: 0.00001852
Iteration 216/1000 | Loss: 0.00001852
Iteration 217/1000 | Loss: 0.00001852
Iteration 218/1000 | Loss: 0.00001852
Iteration 219/1000 | Loss: 0.00001852
Iteration 220/1000 | Loss: 0.00001852
Iteration 221/1000 | Loss: 0.00001852
Iteration 222/1000 | Loss: 0.00001852
Iteration 223/1000 | Loss: 0.00001852
Iteration 224/1000 | Loss: 0.00001852
Iteration 225/1000 | Loss: 0.00001852
Iteration 226/1000 | Loss: 0.00001852
Iteration 227/1000 | Loss: 0.00001852
Iteration 228/1000 | Loss: 0.00001852
Iteration 229/1000 | Loss: 0.00001852
Iteration 230/1000 | Loss: 0.00001852
Iteration 231/1000 | Loss: 0.00001852
Iteration 232/1000 | Loss: 0.00001852
Iteration 233/1000 | Loss: 0.00001852
Iteration 234/1000 | Loss: 0.00001852
Iteration 235/1000 | Loss: 0.00001852
Iteration 236/1000 | Loss: 0.00001852
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 236. Stopping optimization.
Last 5 losses: [1.851664092100691e-05, 1.851664092100691e-05, 1.851664092100691e-05, 1.851664092100691e-05, 1.851664092100691e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.851664092100691e-05

Optimization complete. Final v2v error: 3.6840786933898926 mm

Highest mean error: 4.251758098602295 mm for frame 74

Lowest mean error: 3.266589879989624 mm for frame 166

Saving results

Total time: 45.00039601325989
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5760/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5760/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5760/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00869163
Iteration 2/25 | Loss: 0.00153315
Iteration 3/25 | Loss: 0.00130604
Iteration 4/25 | Loss: 0.00127866
Iteration 5/25 | Loss: 0.00127305
Iteration 6/25 | Loss: 0.00127215
Iteration 7/25 | Loss: 0.00127215
Iteration 8/25 | Loss: 0.00127215
Iteration 9/25 | Loss: 0.00127215
Iteration 10/25 | Loss: 0.00127215
Iteration 11/25 | Loss: 0.00127215
Iteration 12/25 | Loss: 0.00127215
Iteration 13/25 | Loss: 0.00127215
Iteration 14/25 | Loss: 0.00127215
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001272146706469357, 0.001272146706469357, 0.001272146706469357, 0.001272146706469357, 0.001272146706469357]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001272146706469357

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11579037
Iteration 2/25 | Loss: 0.00367861
Iteration 3/25 | Loss: 0.00367861
Iteration 4/25 | Loss: 0.00367861
Iteration 5/25 | Loss: 0.00367861
Iteration 6/25 | Loss: 0.00367860
Iteration 7/25 | Loss: 0.00367861
Iteration 8/25 | Loss: 0.00367861
Iteration 9/25 | Loss: 0.00367860
Iteration 10/25 | Loss: 0.00367860
Iteration 11/25 | Loss: 0.00367860
Iteration 12/25 | Loss: 0.00367860
Iteration 13/25 | Loss: 0.00367860
Iteration 14/25 | Loss: 0.00367860
Iteration 15/25 | Loss: 0.00367860
Iteration 16/25 | Loss: 0.00367860
Iteration 17/25 | Loss: 0.00367860
Iteration 18/25 | Loss: 0.00367860
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0036786042619496584, 0.0036786042619496584, 0.0036786042619496584, 0.0036786042619496584, 0.0036786042619496584]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0036786042619496584

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00367860
Iteration 2/1000 | Loss: 0.00003628
Iteration 3/1000 | Loss: 0.00002364
Iteration 4/1000 | Loss: 0.00001982
Iteration 5/1000 | Loss: 0.00001792
Iteration 6/1000 | Loss: 0.00001664
Iteration 7/1000 | Loss: 0.00001583
Iteration 8/1000 | Loss: 0.00001527
Iteration 9/1000 | Loss: 0.00001477
Iteration 10/1000 | Loss: 0.00001437
Iteration 11/1000 | Loss: 0.00001405
Iteration 12/1000 | Loss: 0.00001390
Iteration 13/1000 | Loss: 0.00001382
Iteration 14/1000 | Loss: 0.00001381
Iteration 15/1000 | Loss: 0.00001380
Iteration 16/1000 | Loss: 0.00001380
Iteration 17/1000 | Loss: 0.00001379
Iteration 18/1000 | Loss: 0.00001372
Iteration 19/1000 | Loss: 0.00001367
Iteration 20/1000 | Loss: 0.00001366
Iteration 21/1000 | Loss: 0.00001366
Iteration 22/1000 | Loss: 0.00001363
Iteration 23/1000 | Loss: 0.00001363
Iteration 24/1000 | Loss: 0.00001363
Iteration 25/1000 | Loss: 0.00001363
Iteration 26/1000 | Loss: 0.00001363
Iteration 27/1000 | Loss: 0.00001362
Iteration 28/1000 | Loss: 0.00001362
Iteration 29/1000 | Loss: 0.00001362
Iteration 30/1000 | Loss: 0.00001362
Iteration 31/1000 | Loss: 0.00001362
Iteration 32/1000 | Loss: 0.00001361
Iteration 33/1000 | Loss: 0.00001361
Iteration 34/1000 | Loss: 0.00001361
Iteration 35/1000 | Loss: 0.00001359
Iteration 36/1000 | Loss: 0.00001359
Iteration 37/1000 | Loss: 0.00001359
Iteration 38/1000 | Loss: 0.00001358
Iteration 39/1000 | Loss: 0.00001358
Iteration 40/1000 | Loss: 0.00001358
Iteration 41/1000 | Loss: 0.00001358
Iteration 42/1000 | Loss: 0.00001358
Iteration 43/1000 | Loss: 0.00001358
Iteration 44/1000 | Loss: 0.00001357
Iteration 45/1000 | Loss: 0.00001357
Iteration 46/1000 | Loss: 0.00001357
Iteration 47/1000 | Loss: 0.00001357
Iteration 48/1000 | Loss: 0.00001357
Iteration 49/1000 | Loss: 0.00001357
Iteration 50/1000 | Loss: 0.00001356
Iteration 51/1000 | Loss: 0.00001356
Iteration 52/1000 | Loss: 0.00001356
Iteration 53/1000 | Loss: 0.00001355
Iteration 54/1000 | Loss: 0.00001355
Iteration 55/1000 | Loss: 0.00001355
Iteration 56/1000 | Loss: 0.00001355
Iteration 57/1000 | Loss: 0.00001355
Iteration 58/1000 | Loss: 0.00001355
Iteration 59/1000 | Loss: 0.00001354
Iteration 60/1000 | Loss: 0.00001354
Iteration 61/1000 | Loss: 0.00001354
Iteration 62/1000 | Loss: 0.00001354
Iteration 63/1000 | Loss: 0.00001354
Iteration 64/1000 | Loss: 0.00001354
Iteration 65/1000 | Loss: 0.00001354
Iteration 66/1000 | Loss: 0.00001353
Iteration 67/1000 | Loss: 0.00001353
Iteration 68/1000 | Loss: 0.00001353
Iteration 69/1000 | Loss: 0.00001353
Iteration 70/1000 | Loss: 0.00001353
Iteration 71/1000 | Loss: 0.00001353
Iteration 72/1000 | Loss: 0.00001353
Iteration 73/1000 | Loss: 0.00001353
Iteration 74/1000 | Loss: 0.00001353
Iteration 75/1000 | Loss: 0.00001352
Iteration 76/1000 | Loss: 0.00001352
Iteration 77/1000 | Loss: 0.00001352
Iteration 78/1000 | Loss: 0.00001352
Iteration 79/1000 | Loss: 0.00001352
Iteration 80/1000 | Loss: 0.00001352
Iteration 81/1000 | Loss: 0.00001352
Iteration 82/1000 | Loss: 0.00001352
Iteration 83/1000 | Loss: 0.00001352
Iteration 84/1000 | Loss: 0.00001352
Iteration 85/1000 | Loss: 0.00001352
Iteration 86/1000 | Loss: 0.00001351
Iteration 87/1000 | Loss: 0.00001351
Iteration 88/1000 | Loss: 0.00001351
Iteration 89/1000 | Loss: 0.00001351
Iteration 90/1000 | Loss: 0.00001351
Iteration 91/1000 | Loss: 0.00001351
Iteration 92/1000 | Loss: 0.00001350
Iteration 93/1000 | Loss: 0.00001350
Iteration 94/1000 | Loss: 0.00001350
Iteration 95/1000 | Loss: 0.00001350
Iteration 96/1000 | Loss: 0.00001349
Iteration 97/1000 | Loss: 0.00001349
Iteration 98/1000 | Loss: 0.00001349
Iteration 99/1000 | Loss: 0.00001349
Iteration 100/1000 | Loss: 0.00001349
Iteration 101/1000 | Loss: 0.00001348
Iteration 102/1000 | Loss: 0.00001348
Iteration 103/1000 | Loss: 0.00001348
Iteration 104/1000 | Loss: 0.00001348
Iteration 105/1000 | Loss: 0.00001348
Iteration 106/1000 | Loss: 0.00001348
Iteration 107/1000 | Loss: 0.00001348
Iteration 108/1000 | Loss: 0.00001348
Iteration 109/1000 | Loss: 0.00001348
Iteration 110/1000 | Loss: 0.00001348
Iteration 111/1000 | Loss: 0.00001348
Iteration 112/1000 | Loss: 0.00001348
Iteration 113/1000 | Loss: 0.00001348
Iteration 114/1000 | Loss: 0.00001348
Iteration 115/1000 | Loss: 0.00001347
Iteration 116/1000 | Loss: 0.00001347
Iteration 117/1000 | Loss: 0.00001347
Iteration 118/1000 | Loss: 0.00001347
Iteration 119/1000 | Loss: 0.00001347
Iteration 120/1000 | Loss: 0.00001347
Iteration 121/1000 | Loss: 0.00001347
Iteration 122/1000 | Loss: 0.00001347
Iteration 123/1000 | Loss: 0.00001347
Iteration 124/1000 | Loss: 0.00001347
Iteration 125/1000 | Loss: 0.00001347
Iteration 126/1000 | Loss: 0.00001347
Iteration 127/1000 | Loss: 0.00001347
Iteration 128/1000 | Loss: 0.00001347
Iteration 129/1000 | Loss: 0.00001347
Iteration 130/1000 | Loss: 0.00001347
Iteration 131/1000 | Loss: 0.00001347
Iteration 132/1000 | Loss: 0.00001347
Iteration 133/1000 | Loss: 0.00001347
Iteration 134/1000 | Loss: 0.00001347
Iteration 135/1000 | Loss: 0.00001347
Iteration 136/1000 | Loss: 0.00001347
Iteration 137/1000 | Loss: 0.00001347
Iteration 138/1000 | Loss: 0.00001347
Iteration 139/1000 | Loss: 0.00001347
Iteration 140/1000 | Loss: 0.00001347
Iteration 141/1000 | Loss: 0.00001347
Iteration 142/1000 | Loss: 0.00001347
Iteration 143/1000 | Loss: 0.00001347
Iteration 144/1000 | Loss: 0.00001347
Iteration 145/1000 | Loss: 0.00001347
Iteration 146/1000 | Loss: 0.00001347
Iteration 147/1000 | Loss: 0.00001347
Iteration 148/1000 | Loss: 0.00001347
Iteration 149/1000 | Loss: 0.00001347
Iteration 150/1000 | Loss: 0.00001347
Iteration 151/1000 | Loss: 0.00001347
Iteration 152/1000 | Loss: 0.00001347
Iteration 153/1000 | Loss: 0.00001347
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.3470503290591296e-05, 1.3470503290591296e-05, 1.3470503290591296e-05, 1.3470503290591296e-05, 1.3470503290591296e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3470503290591296e-05

Optimization complete. Final v2v error: 3.0763232707977295 mm

Highest mean error: 3.2701025009155273 mm for frame 91

Lowest mean error: 2.8459250926971436 mm for frame 213

Saving results

Total time: 42.2758469581604
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5760/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5760/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5760/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00854809
Iteration 2/25 | Loss: 0.00149445
Iteration 3/25 | Loss: 0.00138473
Iteration 4/25 | Loss: 0.00136207
Iteration 5/25 | Loss: 0.00135838
Iteration 6/25 | Loss: 0.00135802
Iteration 7/25 | Loss: 0.00135802
Iteration 8/25 | Loss: 0.00135802
Iteration 9/25 | Loss: 0.00135802
Iteration 10/25 | Loss: 0.00135802
Iteration 11/25 | Loss: 0.00135802
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013580186059698462, 0.0013580186059698462, 0.0013580186059698462, 0.0013580186059698462, 0.0013580186059698462]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013580186059698462

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30108416
Iteration 2/25 | Loss: 0.00442582
Iteration 3/25 | Loss: 0.00442576
Iteration 4/25 | Loss: 0.00442576
Iteration 5/25 | Loss: 0.00442576
Iteration 6/25 | Loss: 0.00442576
Iteration 7/25 | Loss: 0.00442576
Iteration 8/25 | Loss: 0.00442576
Iteration 9/25 | Loss: 0.00442576
Iteration 10/25 | Loss: 0.00442576
Iteration 11/25 | Loss: 0.00442576
Iteration 12/25 | Loss: 0.00442576
Iteration 13/25 | Loss: 0.00442576
Iteration 14/25 | Loss: 0.00442576
Iteration 15/25 | Loss: 0.00442576
Iteration 16/25 | Loss: 0.00442576
Iteration 17/25 | Loss: 0.00442576
Iteration 18/25 | Loss: 0.00442576
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.004425759427249432, 0.004425759427249432, 0.004425759427249432, 0.004425759427249432, 0.004425759427249432]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004425759427249432

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00442576
Iteration 2/1000 | Loss: 0.00004166
Iteration 3/1000 | Loss: 0.00003197
Iteration 4/1000 | Loss: 0.00002705
Iteration 5/1000 | Loss: 0.00002512
Iteration 6/1000 | Loss: 0.00002312
Iteration 7/1000 | Loss: 0.00002190
Iteration 8/1000 | Loss: 0.00002114
Iteration 9/1000 | Loss: 0.00002023
Iteration 10/1000 | Loss: 0.00001946
Iteration 11/1000 | Loss: 0.00001904
Iteration 12/1000 | Loss: 0.00001870
Iteration 13/1000 | Loss: 0.00001852
Iteration 14/1000 | Loss: 0.00001849
Iteration 15/1000 | Loss: 0.00001846
Iteration 16/1000 | Loss: 0.00001845
Iteration 17/1000 | Loss: 0.00001844
Iteration 18/1000 | Loss: 0.00001844
Iteration 19/1000 | Loss: 0.00001841
Iteration 20/1000 | Loss: 0.00001841
Iteration 21/1000 | Loss: 0.00001840
Iteration 22/1000 | Loss: 0.00001840
Iteration 23/1000 | Loss: 0.00001839
Iteration 24/1000 | Loss: 0.00001839
Iteration 25/1000 | Loss: 0.00001837
Iteration 26/1000 | Loss: 0.00001837
Iteration 27/1000 | Loss: 0.00001836
Iteration 28/1000 | Loss: 0.00001836
Iteration 29/1000 | Loss: 0.00001835
Iteration 30/1000 | Loss: 0.00001835
Iteration 31/1000 | Loss: 0.00001835
Iteration 32/1000 | Loss: 0.00001834
Iteration 33/1000 | Loss: 0.00001834
Iteration 34/1000 | Loss: 0.00001834
Iteration 35/1000 | Loss: 0.00001833
Iteration 36/1000 | Loss: 0.00001833
Iteration 37/1000 | Loss: 0.00001833
Iteration 38/1000 | Loss: 0.00001832
Iteration 39/1000 | Loss: 0.00001831
Iteration 40/1000 | Loss: 0.00001831
Iteration 41/1000 | Loss: 0.00001831
Iteration 42/1000 | Loss: 0.00001830
Iteration 43/1000 | Loss: 0.00001830
Iteration 44/1000 | Loss: 0.00001830
Iteration 45/1000 | Loss: 0.00001829
Iteration 46/1000 | Loss: 0.00001829
Iteration 47/1000 | Loss: 0.00001829
Iteration 48/1000 | Loss: 0.00001829
Iteration 49/1000 | Loss: 0.00001829
Iteration 50/1000 | Loss: 0.00001828
Iteration 51/1000 | Loss: 0.00001828
Iteration 52/1000 | Loss: 0.00001828
Iteration 53/1000 | Loss: 0.00001828
Iteration 54/1000 | Loss: 0.00001828
Iteration 55/1000 | Loss: 0.00001828
Iteration 56/1000 | Loss: 0.00001828
Iteration 57/1000 | Loss: 0.00001828
Iteration 58/1000 | Loss: 0.00001828
Iteration 59/1000 | Loss: 0.00001828
Iteration 60/1000 | Loss: 0.00001827
Iteration 61/1000 | Loss: 0.00001827
Iteration 62/1000 | Loss: 0.00001827
Iteration 63/1000 | Loss: 0.00001826
Iteration 64/1000 | Loss: 0.00001826
Iteration 65/1000 | Loss: 0.00001826
Iteration 66/1000 | Loss: 0.00001826
Iteration 67/1000 | Loss: 0.00001826
Iteration 68/1000 | Loss: 0.00001825
Iteration 69/1000 | Loss: 0.00001825
Iteration 70/1000 | Loss: 0.00001825
Iteration 71/1000 | Loss: 0.00001824
Iteration 72/1000 | Loss: 0.00001824
Iteration 73/1000 | Loss: 0.00001824
Iteration 74/1000 | Loss: 0.00001823
Iteration 75/1000 | Loss: 0.00001823
Iteration 76/1000 | Loss: 0.00001823
Iteration 77/1000 | Loss: 0.00001823
Iteration 78/1000 | Loss: 0.00001822
Iteration 79/1000 | Loss: 0.00001822
Iteration 80/1000 | Loss: 0.00001822
Iteration 81/1000 | Loss: 0.00001822
Iteration 82/1000 | Loss: 0.00001821
Iteration 83/1000 | Loss: 0.00001821
Iteration 84/1000 | Loss: 0.00001821
Iteration 85/1000 | Loss: 0.00001821
Iteration 86/1000 | Loss: 0.00001821
Iteration 87/1000 | Loss: 0.00001821
Iteration 88/1000 | Loss: 0.00001821
Iteration 89/1000 | Loss: 0.00001821
Iteration 90/1000 | Loss: 0.00001821
Iteration 91/1000 | Loss: 0.00001821
Iteration 92/1000 | Loss: 0.00001821
Iteration 93/1000 | Loss: 0.00001820
Iteration 94/1000 | Loss: 0.00001820
Iteration 95/1000 | Loss: 0.00001820
Iteration 96/1000 | Loss: 0.00001820
Iteration 97/1000 | Loss: 0.00001820
Iteration 98/1000 | Loss: 0.00001820
Iteration 99/1000 | Loss: 0.00001820
Iteration 100/1000 | Loss: 0.00001820
Iteration 101/1000 | Loss: 0.00001820
Iteration 102/1000 | Loss: 0.00001820
Iteration 103/1000 | Loss: 0.00001820
Iteration 104/1000 | Loss: 0.00001820
Iteration 105/1000 | Loss: 0.00001820
Iteration 106/1000 | Loss: 0.00001820
Iteration 107/1000 | Loss: 0.00001820
Iteration 108/1000 | Loss: 0.00001820
Iteration 109/1000 | Loss: 0.00001820
Iteration 110/1000 | Loss: 0.00001820
Iteration 111/1000 | Loss: 0.00001820
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [1.819545832404401e-05, 1.819545832404401e-05, 1.819545832404401e-05, 1.819545832404401e-05, 1.819545832404401e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.819545832404401e-05

Optimization complete. Final v2v error: 3.492328405380249 mm

Highest mean error: 3.976459264755249 mm for frame 1

Lowest mean error: 3.288102865219116 mm for frame 175

Saving results

Total time: 37.407639265060425
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5760/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5760/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5760/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01090129
Iteration 2/25 | Loss: 0.01090129
Iteration 3/25 | Loss: 0.00363163
Iteration 4/25 | Loss: 0.00243752
Iteration 5/25 | Loss: 0.00205077
Iteration 6/25 | Loss: 0.00185209
Iteration 7/25 | Loss: 0.00175241
Iteration 8/25 | Loss: 0.00169561
Iteration 9/25 | Loss: 0.00164530
Iteration 10/25 | Loss: 0.00160166
Iteration 11/25 | Loss: 0.00157501
Iteration 12/25 | Loss: 0.00156544
Iteration 13/25 | Loss: 0.00155484
Iteration 14/25 | Loss: 0.00155942
Iteration 15/25 | Loss: 0.00155512
Iteration 16/25 | Loss: 0.00154981
Iteration 17/25 | Loss: 0.00154992
Iteration 18/25 | Loss: 0.00155225
Iteration 19/25 | Loss: 0.00154619
Iteration 20/25 | Loss: 0.00155523
Iteration 21/25 | Loss: 0.00155094
Iteration 22/25 | Loss: 0.00154878
Iteration 23/25 | Loss: 0.00154406
Iteration 24/25 | Loss: 0.00154598
Iteration 25/25 | Loss: 0.00154312

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11587584
Iteration 2/25 | Loss: 0.00486145
Iteration 3/25 | Loss: 0.00478233
Iteration 4/25 | Loss: 0.00478233
Iteration 5/25 | Loss: 0.00478233
Iteration 6/25 | Loss: 0.00478233
Iteration 7/25 | Loss: 0.00478233
Iteration 8/25 | Loss: 0.00478233
Iteration 9/25 | Loss: 0.00478233
Iteration 10/25 | Loss: 0.00478233
Iteration 11/25 | Loss: 0.00478233
Iteration 12/25 | Loss: 0.00478233
Iteration 13/25 | Loss: 0.00478233
Iteration 14/25 | Loss: 0.00478233
Iteration 15/25 | Loss: 0.00478233
Iteration 16/25 | Loss: 0.00478233
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.004782326519489288, 0.004782326519489288, 0.004782326519489288, 0.004782326519489288, 0.004782326519489288]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004782326519489288

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00478233
Iteration 2/1000 | Loss: 0.00062926
Iteration 3/1000 | Loss: 0.00041590
Iteration 4/1000 | Loss: 0.00032722
Iteration 5/1000 | Loss: 0.00043340
Iteration 6/1000 | Loss: 0.00022183
Iteration 7/1000 | Loss: 0.00027653
Iteration 8/1000 | Loss: 0.00019463
Iteration 9/1000 | Loss: 0.00066767
Iteration 10/1000 | Loss: 0.00038354
Iteration 11/1000 | Loss: 0.00018553
Iteration 12/1000 | Loss: 0.00020158
Iteration 13/1000 | Loss: 0.00030229
Iteration 14/1000 | Loss: 0.00020079
Iteration 15/1000 | Loss: 0.00018021
Iteration 16/1000 | Loss: 0.00017792
Iteration 17/1000 | Loss: 0.00238053
Iteration 18/1000 | Loss: 0.01269949
Iteration 19/1000 | Loss: 0.00783046
Iteration 20/1000 | Loss: 0.00865356
Iteration 21/1000 | Loss: 0.00362348
Iteration 22/1000 | Loss: 0.00092819
Iteration 23/1000 | Loss: 0.00027443
Iteration 24/1000 | Loss: 0.00061640
Iteration 25/1000 | Loss: 0.00072666
Iteration 26/1000 | Loss: 0.00166033
Iteration 27/1000 | Loss: 0.00020599
Iteration 28/1000 | Loss: 0.00045158
Iteration 29/1000 | Loss: 0.00030472
Iteration 30/1000 | Loss: 0.00039278
Iteration 31/1000 | Loss: 0.00069743
Iteration 32/1000 | Loss: 0.00015811
Iteration 33/1000 | Loss: 0.00044081
Iteration 34/1000 | Loss: 0.00053242
Iteration 35/1000 | Loss: 0.00016782
Iteration 36/1000 | Loss: 0.00017622
Iteration 37/1000 | Loss: 0.00064658
Iteration 38/1000 | Loss: 0.00117473
Iteration 39/1000 | Loss: 0.00069832
Iteration 40/1000 | Loss: 0.00056403
Iteration 41/1000 | Loss: 0.00025433
Iteration 42/1000 | Loss: 0.00031016
Iteration 43/1000 | Loss: 0.00015323
Iteration 44/1000 | Loss: 0.00024047
Iteration 45/1000 | Loss: 0.00015705
Iteration 46/1000 | Loss: 0.00066957
Iteration 47/1000 | Loss: 0.00071446
Iteration 48/1000 | Loss: 0.00058564
Iteration 49/1000 | Loss: 0.00070591
Iteration 50/1000 | Loss: 0.00017032
Iteration 51/1000 | Loss: 0.00033572
Iteration 52/1000 | Loss: 0.00016291
Iteration 53/1000 | Loss: 0.00013100
Iteration 54/1000 | Loss: 0.00012537
Iteration 55/1000 | Loss: 0.00012607
Iteration 56/1000 | Loss: 0.00012529
Iteration 57/1000 | Loss: 0.00079026
Iteration 58/1000 | Loss: 0.00056638
Iteration 59/1000 | Loss: 0.00079432
Iteration 60/1000 | Loss: 0.00110227
Iteration 61/1000 | Loss: 0.00062034
Iteration 62/1000 | Loss: 0.00072386
Iteration 63/1000 | Loss: 0.00042894
Iteration 64/1000 | Loss: 0.00039319
Iteration 65/1000 | Loss: 0.00035251
Iteration 66/1000 | Loss: 0.00049120
Iteration 67/1000 | Loss: 0.00032141
Iteration 68/1000 | Loss: 0.00053871
Iteration 69/1000 | Loss: 0.00037949
Iteration 70/1000 | Loss: 0.00034028
Iteration 71/1000 | Loss: 0.00018269
Iteration 72/1000 | Loss: 0.00021350
Iteration 73/1000 | Loss: 0.00013629
Iteration 74/1000 | Loss: 0.00015605
Iteration 75/1000 | Loss: 0.00013050
Iteration 76/1000 | Loss: 0.00018775
Iteration 77/1000 | Loss: 0.00011303
Iteration 78/1000 | Loss: 0.00011145
Iteration 79/1000 | Loss: 0.00011042
Iteration 80/1000 | Loss: 0.00011984
Iteration 81/1000 | Loss: 0.00010948
Iteration 82/1000 | Loss: 0.00010900
Iteration 83/1000 | Loss: 0.00010869
Iteration 84/1000 | Loss: 0.00010864
Iteration 85/1000 | Loss: 0.00013015
Iteration 86/1000 | Loss: 0.00012121
Iteration 87/1000 | Loss: 0.00013649
Iteration 88/1000 | Loss: 0.00010838
Iteration 89/1000 | Loss: 0.00010831
Iteration 90/1000 | Loss: 0.00012905
Iteration 91/1000 | Loss: 0.00011097
Iteration 92/1000 | Loss: 0.00010821
Iteration 93/1000 | Loss: 0.00010821
Iteration 94/1000 | Loss: 0.00010821
Iteration 95/1000 | Loss: 0.00010820
Iteration 96/1000 | Loss: 0.00010820
Iteration 97/1000 | Loss: 0.00010820
Iteration 98/1000 | Loss: 0.00010820
Iteration 99/1000 | Loss: 0.00010820
Iteration 100/1000 | Loss: 0.00010820
Iteration 101/1000 | Loss: 0.00010820
Iteration 102/1000 | Loss: 0.00010820
Iteration 103/1000 | Loss: 0.00010820
Iteration 104/1000 | Loss: 0.00010820
Iteration 105/1000 | Loss: 0.00010819
Iteration 106/1000 | Loss: 0.00010818
Iteration 107/1000 | Loss: 0.00010818
Iteration 108/1000 | Loss: 0.00011240
Iteration 109/1000 | Loss: 0.00010831
Iteration 110/1000 | Loss: 0.00010814
Iteration 111/1000 | Loss: 0.00010814
Iteration 112/1000 | Loss: 0.00010813
Iteration 113/1000 | Loss: 0.00010813
Iteration 114/1000 | Loss: 0.00010813
Iteration 115/1000 | Loss: 0.00010813
Iteration 116/1000 | Loss: 0.00010813
Iteration 117/1000 | Loss: 0.00010813
Iteration 118/1000 | Loss: 0.00010813
Iteration 119/1000 | Loss: 0.00010812
Iteration 120/1000 | Loss: 0.00010812
Iteration 121/1000 | Loss: 0.00010811
Iteration 122/1000 | Loss: 0.00010811
Iteration 123/1000 | Loss: 0.00010811
Iteration 124/1000 | Loss: 0.00010811
Iteration 125/1000 | Loss: 0.00010811
Iteration 126/1000 | Loss: 0.00010810
Iteration 127/1000 | Loss: 0.00010810
Iteration 128/1000 | Loss: 0.00010810
Iteration 129/1000 | Loss: 0.00010809
Iteration 130/1000 | Loss: 0.00010809
Iteration 131/1000 | Loss: 0.00010809
Iteration 132/1000 | Loss: 0.00010809
Iteration 133/1000 | Loss: 0.00010809
Iteration 134/1000 | Loss: 0.00010808
Iteration 135/1000 | Loss: 0.00010808
Iteration 136/1000 | Loss: 0.00010808
Iteration 137/1000 | Loss: 0.00010808
Iteration 138/1000 | Loss: 0.00010808
Iteration 139/1000 | Loss: 0.00010808
Iteration 140/1000 | Loss: 0.00010807
Iteration 141/1000 | Loss: 0.00010807
Iteration 142/1000 | Loss: 0.00010807
Iteration 143/1000 | Loss: 0.00010807
Iteration 144/1000 | Loss: 0.00010807
Iteration 145/1000 | Loss: 0.00010807
Iteration 146/1000 | Loss: 0.00010807
Iteration 147/1000 | Loss: 0.00010807
Iteration 148/1000 | Loss: 0.00010807
Iteration 149/1000 | Loss: 0.00010807
Iteration 150/1000 | Loss: 0.00010807
Iteration 151/1000 | Loss: 0.00010807
Iteration 152/1000 | Loss: 0.00010807
Iteration 153/1000 | Loss: 0.00010806
Iteration 154/1000 | Loss: 0.00010806
Iteration 155/1000 | Loss: 0.00010806
Iteration 156/1000 | Loss: 0.00010806
Iteration 157/1000 | Loss: 0.00010806
Iteration 158/1000 | Loss: 0.00010806
Iteration 159/1000 | Loss: 0.00010806
Iteration 160/1000 | Loss: 0.00010806
Iteration 161/1000 | Loss: 0.00010806
Iteration 162/1000 | Loss: 0.00010806
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [0.00010806385398609564, 0.00010806385398609564, 0.00010806385398609564, 0.00010806385398609564, 0.00010806385398609564]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00010806385398609564

Optimization complete. Final v2v error: 5.419744491577148 mm

Highest mean error: 12.238410949707031 mm for frame 55

Lowest mean error: 3.185209035873413 mm for frame 88

Saving results

Total time: 189.62277913093567
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5760/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5760/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5760/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01080127
Iteration 2/25 | Loss: 0.01080127
Iteration 3/25 | Loss: 0.01080127
Iteration 4/25 | Loss: 0.01080127
Iteration 5/25 | Loss: 0.01080127
Iteration 6/25 | Loss: 0.01080127
Iteration 7/25 | Loss: 0.01080126
Iteration 8/25 | Loss: 0.01080126
Iteration 9/25 | Loss: 0.01080126
Iteration 10/25 | Loss: 0.01080126
Iteration 11/25 | Loss: 0.01080126
Iteration 12/25 | Loss: 0.01080126
Iteration 13/25 | Loss: 0.01080125
Iteration 14/25 | Loss: 0.01080125
Iteration 15/25 | Loss: 0.01080125
Iteration 16/25 | Loss: 0.01080125
Iteration 17/25 | Loss: 0.01080125
Iteration 18/25 | Loss: 0.01080125
Iteration 19/25 | Loss: 0.01080125
Iteration 20/25 | Loss: 0.01080125
Iteration 21/25 | Loss: 0.01080124
Iteration 22/25 | Loss: 0.01080124
Iteration 23/25 | Loss: 0.01080124
Iteration 24/25 | Loss: 0.01080124
Iteration 25/25 | Loss: 0.01080124

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31517673
Iteration 2/25 | Loss: 0.12177145
Iteration 3/25 | Loss: 0.12039808
Iteration 4/25 | Loss: 0.11875951
Iteration 5/25 | Loss: 0.11869328
Iteration 6/25 | Loss: 0.11869326
Iteration 7/25 | Loss: 0.11869326
Iteration 8/25 | Loss: 0.11869326
Iteration 9/25 | Loss: 0.11869326
Iteration 10/25 | Loss: 0.11869326
Iteration 11/25 | Loss: 0.11869326
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.1186932623386383, 0.1186932623386383, 0.1186932623386383, 0.1186932623386383, 0.1186932623386383]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.1186932623386383

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.11869326
Iteration 2/1000 | Loss: 0.00404321
Iteration 3/1000 | Loss: 0.00102008
Iteration 4/1000 | Loss: 0.00104661
Iteration 5/1000 | Loss: 0.00021411
Iteration 6/1000 | Loss: 0.00009358
Iteration 7/1000 | Loss: 0.00018504
Iteration 8/1000 | Loss: 0.00005769
Iteration 9/1000 | Loss: 0.00003869
Iteration 10/1000 | Loss: 0.00003358
Iteration 11/1000 | Loss: 0.00003043
Iteration 12/1000 | Loss: 0.00002823
Iteration 13/1000 | Loss: 0.00012817
Iteration 14/1000 | Loss: 0.00002675
Iteration 15/1000 | Loss: 0.00002512
Iteration 16/1000 | Loss: 0.00002419
Iteration 17/1000 | Loss: 0.00002516
Iteration 18/1000 | Loss: 0.00002392
Iteration 19/1000 | Loss: 0.00002296
Iteration 20/1000 | Loss: 0.00002230
Iteration 21/1000 | Loss: 0.00002183
Iteration 22/1000 | Loss: 0.00002154
Iteration 23/1000 | Loss: 0.00002117
Iteration 24/1000 | Loss: 0.00002086
Iteration 25/1000 | Loss: 0.00002065
Iteration 26/1000 | Loss: 0.00002050
Iteration 27/1000 | Loss: 0.00002032
Iteration 28/1000 | Loss: 0.00002032
Iteration 29/1000 | Loss: 0.00002027
Iteration 30/1000 | Loss: 0.00002022
Iteration 31/1000 | Loss: 0.00002017
Iteration 32/1000 | Loss: 0.00002016
Iteration 33/1000 | Loss: 0.00002015
Iteration 34/1000 | Loss: 0.00002014
Iteration 35/1000 | Loss: 0.00002013
Iteration 36/1000 | Loss: 0.00002011
Iteration 37/1000 | Loss: 0.00002010
Iteration 38/1000 | Loss: 0.00002010
Iteration 39/1000 | Loss: 0.00002010
Iteration 40/1000 | Loss: 0.00002009
Iteration 41/1000 | Loss: 0.00002009
Iteration 42/1000 | Loss: 0.00002004
Iteration 43/1000 | Loss: 0.00002003
Iteration 44/1000 | Loss: 0.00002003
Iteration 45/1000 | Loss: 0.00002002
Iteration 46/1000 | Loss: 0.00002001
Iteration 47/1000 | Loss: 0.00001998
Iteration 48/1000 | Loss: 0.00001998
Iteration 49/1000 | Loss: 0.00001997
Iteration 50/1000 | Loss: 0.00001997
Iteration 51/1000 | Loss: 0.00001997
Iteration 52/1000 | Loss: 0.00001996
Iteration 53/1000 | Loss: 0.00001996
Iteration 54/1000 | Loss: 0.00001990
Iteration 55/1000 | Loss: 0.00001989
Iteration 56/1000 | Loss: 0.00001989
Iteration 57/1000 | Loss: 0.00001988
Iteration 58/1000 | Loss: 0.00001988
Iteration 59/1000 | Loss: 0.00001987
Iteration 60/1000 | Loss: 0.00001987
Iteration 61/1000 | Loss: 0.00001986
Iteration 62/1000 | Loss: 0.00001986
Iteration 63/1000 | Loss: 0.00001986
Iteration 64/1000 | Loss: 0.00001985
Iteration 65/1000 | Loss: 0.00001985
Iteration 66/1000 | Loss: 0.00001985
Iteration 67/1000 | Loss: 0.00001985
Iteration 68/1000 | Loss: 0.00001984
Iteration 69/1000 | Loss: 0.00001984
Iteration 70/1000 | Loss: 0.00001984
Iteration 71/1000 | Loss: 0.00001984
Iteration 72/1000 | Loss: 0.00001984
Iteration 73/1000 | Loss: 0.00001983
Iteration 74/1000 | Loss: 0.00001982
Iteration 75/1000 | Loss: 0.00001981
Iteration 76/1000 | Loss: 0.00001981
Iteration 77/1000 | Loss: 0.00001980
Iteration 78/1000 | Loss: 0.00001980
Iteration 79/1000 | Loss: 0.00001980
Iteration 80/1000 | Loss: 0.00001980
Iteration 81/1000 | Loss: 0.00001980
Iteration 82/1000 | Loss: 0.00001980
Iteration 83/1000 | Loss: 0.00001980
Iteration 84/1000 | Loss: 0.00001980
Iteration 85/1000 | Loss: 0.00001979
Iteration 86/1000 | Loss: 0.00001979
Iteration 87/1000 | Loss: 0.00001979
Iteration 88/1000 | Loss: 0.00001979
Iteration 89/1000 | Loss: 0.00001979
Iteration 90/1000 | Loss: 0.00001979
Iteration 91/1000 | Loss: 0.00001978
Iteration 92/1000 | Loss: 0.00001978
Iteration 93/1000 | Loss: 0.00001978
Iteration 94/1000 | Loss: 0.00001977
Iteration 95/1000 | Loss: 0.00001977
Iteration 96/1000 | Loss: 0.00001977
Iteration 97/1000 | Loss: 0.00001977
Iteration 98/1000 | Loss: 0.00001976
Iteration 99/1000 | Loss: 0.00001976
Iteration 100/1000 | Loss: 0.00001976
Iteration 101/1000 | Loss: 0.00001976
Iteration 102/1000 | Loss: 0.00001975
Iteration 103/1000 | Loss: 0.00001975
Iteration 104/1000 | Loss: 0.00001975
Iteration 105/1000 | Loss: 0.00001975
Iteration 106/1000 | Loss: 0.00001975
Iteration 107/1000 | Loss: 0.00001975
Iteration 108/1000 | Loss: 0.00001975
Iteration 109/1000 | Loss: 0.00001974
Iteration 110/1000 | Loss: 0.00001974
Iteration 111/1000 | Loss: 0.00001974
Iteration 112/1000 | Loss: 0.00001974
Iteration 113/1000 | Loss: 0.00001973
Iteration 114/1000 | Loss: 0.00001973
Iteration 115/1000 | Loss: 0.00001973
Iteration 116/1000 | Loss: 0.00001973
Iteration 117/1000 | Loss: 0.00001972
Iteration 118/1000 | Loss: 0.00001972
Iteration 119/1000 | Loss: 0.00001972
Iteration 120/1000 | Loss: 0.00001972
Iteration 121/1000 | Loss: 0.00001972
Iteration 122/1000 | Loss: 0.00001972
Iteration 123/1000 | Loss: 0.00001972
Iteration 124/1000 | Loss: 0.00001972
Iteration 125/1000 | Loss: 0.00001971
Iteration 126/1000 | Loss: 0.00001971
Iteration 127/1000 | Loss: 0.00001971
Iteration 128/1000 | Loss: 0.00001971
Iteration 129/1000 | Loss: 0.00001970
Iteration 130/1000 | Loss: 0.00001970
Iteration 131/1000 | Loss: 0.00001970
Iteration 132/1000 | Loss: 0.00001970
Iteration 133/1000 | Loss: 0.00001970
Iteration 134/1000 | Loss: 0.00001970
Iteration 135/1000 | Loss: 0.00001970
Iteration 136/1000 | Loss: 0.00001970
Iteration 137/1000 | Loss: 0.00001969
Iteration 138/1000 | Loss: 0.00001969
Iteration 139/1000 | Loss: 0.00001969
Iteration 140/1000 | Loss: 0.00001969
Iteration 141/1000 | Loss: 0.00001969
Iteration 142/1000 | Loss: 0.00001969
Iteration 143/1000 | Loss: 0.00001969
Iteration 144/1000 | Loss: 0.00001969
Iteration 145/1000 | Loss: 0.00001969
Iteration 146/1000 | Loss: 0.00001969
Iteration 147/1000 | Loss: 0.00001969
Iteration 148/1000 | Loss: 0.00001968
Iteration 149/1000 | Loss: 0.00001968
Iteration 150/1000 | Loss: 0.00001968
Iteration 151/1000 | Loss: 0.00001968
Iteration 152/1000 | Loss: 0.00001968
Iteration 153/1000 | Loss: 0.00001968
Iteration 154/1000 | Loss: 0.00001968
Iteration 155/1000 | Loss: 0.00001968
Iteration 156/1000 | Loss: 0.00001968
Iteration 157/1000 | Loss: 0.00001968
Iteration 158/1000 | Loss: 0.00001967
Iteration 159/1000 | Loss: 0.00001967
Iteration 160/1000 | Loss: 0.00001967
Iteration 161/1000 | Loss: 0.00001967
Iteration 162/1000 | Loss: 0.00001967
Iteration 163/1000 | Loss: 0.00001967
Iteration 164/1000 | Loss: 0.00001967
Iteration 165/1000 | Loss: 0.00001967
Iteration 166/1000 | Loss: 0.00001966
Iteration 167/1000 | Loss: 0.00001966
Iteration 168/1000 | Loss: 0.00001966
Iteration 169/1000 | Loss: 0.00001966
Iteration 170/1000 | Loss: 0.00001966
Iteration 171/1000 | Loss: 0.00001966
Iteration 172/1000 | Loss: 0.00001966
Iteration 173/1000 | Loss: 0.00001965
Iteration 174/1000 | Loss: 0.00001965
Iteration 175/1000 | Loss: 0.00001965
Iteration 176/1000 | Loss: 0.00001965
Iteration 177/1000 | Loss: 0.00001965
Iteration 178/1000 | Loss: 0.00001965
Iteration 179/1000 | Loss: 0.00001965
Iteration 180/1000 | Loss: 0.00001965
Iteration 181/1000 | Loss: 0.00001965
Iteration 182/1000 | Loss: 0.00001965
Iteration 183/1000 | Loss: 0.00001965
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.9654553398140706e-05, 1.9654553398140706e-05, 1.9654553398140706e-05, 1.9654553398140706e-05, 1.9654553398140706e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9654553398140706e-05

Optimization complete. Final v2v error: 3.646583318710327 mm

Highest mean error: 9.32314395904541 mm for frame 126

Lowest mean error: 3.2209675312042236 mm for frame 55

Saving results

Total time: 69.1404321193695
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5760/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5760/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5760/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00798496
Iteration 2/25 | Loss: 0.00186919
Iteration 3/25 | Loss: 0.00174180
Iteration 4/25 | Loss: 0.00144348
Iteration 5/25 | Loss: 0.00133614
Iteration 6/25 | Loss: 0.00132536
Iteration 7/25 | Loss: 0.00130452
Iteration 8/25 | Loss: 0.00130723
Iteration 9/25 | Loss: 0.00130635
Iteration 10/25 | Loss: 0.00130379
Iteration 11/25 | Loss: 0.00130132
Iteration 12/25 | Loss: 0.00130114
Iteration 13/25 | Loss: 0.00130109
Iteration 14/25 | Loss: 0.00130109
Iteration 15/25 | Loss: 0.00130109
Iteration 16/25 | Loss: 0.00130109
Iteration 17/25 | Loss: 0.00130109
Iteration 18/25 | Loss: 0.00130109
Iteration 19/25 | Loss: 0.00130109
Iteration 20/25 | Loss: 0.00130108
Iteration 21/25 | Loss: 0.00130108
Iteration 22/25 | Loss: 0.00130108
Iteration 23/25 | Loss: 0.00130108
Iteration 24/25 | Loss: 0.00130108
Iteration 25/25 | Loss: 0.00130107

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66429901
Iteration 2/25 | Loss: 0.00362580
Iteration 3/25 | Loss: 0.00362579
Iteration 4/25 | Loss: 0.00362579
Iteration 5/25 | Loss: 0.00362579
Iteration 6/25 | Loss: 0.00362579
Iteration 7/25 | Loss: 0.00362579
Iteration 8/25 | Loss: 0.00362579
Iteration 9/25 | Loss: 0.00362579
Iteration 10/25 | Loss: 0.00362579
Iteration 11/25 | Loss: 0.00362579
Iteration 12/25 | Loss: 0.00362579
Iteration 13/25 | Loss: 0.00362579
Iteration 14/25 | Loss: 0.00362579
Iteration 15/25 | Loss: 0.00362579
Iteration 16/25 | Loss: 0.00362579
Iteration 17/25 | Loss: 0.00362579
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0036257910542190075, 0.0036257910542190075, 0.0036257910542190075, 0.0036257910542190075, 0.0036257910542190075]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0036257910542190075

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00362579
Iteration 2/1000 | Loss: 0.00014629
Iteration 3/1000 | Loss: 0.00003228
Iteration 4/1000 | Loss: 0.00017987
Iteration 5/1000 | Loss: 0.00061322
Iteration 6/1000 | Loss: 0.00003188
Iteration 7/1000 | Loss: 0.00002385
Iteration 8/1000 | Loss: 0.00019915
Iteration 9/1000 | Loss: 0.00002756
Iteration 10/1000 | Loss: 0.00002258
Iteration 11/1000 | Loss: 0.00009366
Iteration 12/1000 | Loss: 0.00017606
Iteration 13/1000 | Loss: 0.00002103
Iteration 14/1000 | Loss: 0.00002047
Iteration 15/1000 | Loss: 0.00001998
Iteration 16/1000 | Loss: 0.00001951
Iteration 17/1000 | Loss: 0.00001934
Iteration 18/1000 | Loss: 0.00001933
Iteration 19/1000 | Loss: 0.00001928
Iteration 20/1000 | Loss: 0.00001921
Iteration 21/1000 | Loss: 0.00001911
Iteration 22/1000 | Loss: 0.00001907
Iteration 23/1000 | Loss: 0.00001906
Iteration 24/1000 | Loss: 0.00001906
Iteration 25/1000 | Loss: 0.00001905
Iteration 26/1000 | Loss: 0.00001905
Iteration 27/1000 | Loss: 0.00001905
Iteration 28/1000 | Loss: 0.00001905
Iteration 29/1000 | Loss: 0.00001904
Iteration 30/1000 | Loss: 0.00001904
Iteration 31/1000 | Loss: 0.00001904
Iteration 32/1000 | Loss: 0.00001904
Iteration 33/1000 | Loss: 0.00001904
Iteration 34/1000 | Loss: 0.00001904
Iteration 35/1000 | Loss: 0.00001904
Iteration 36/1000 | Loss: 0.00001903
Iteration 37/1000 | Loss: 0.00001903
Iteration 38/1000 | Loss: 0.00001903
Iteration 39/1000 | Loss: 0.00001903
Iteration 40/1000 | Loss: 0.00001903
Iteration 41/1000 | Loss: 0.00001902
Iteration 42/1000 | Loss: 0.00001902
Iteration 43/1000 | Loss: 0.00001902
Iteration 44/1000 | Loss: 0.00001900
Iteration 45/1000 | Loss: 0.00001900
Iteration 46/1000 | Loss: 0.00001900
Iteration 47/1000 | Loss: 0.00001900
Iteration 48/1000 | Loss: 0.00001900
Iteration 49/1000 | Loss: 0.00001900
Iteration 50/1000 | Loss: 0.00001900
Iteration 51/1000 | Loss: 0.00001899
Iteration 52/1000 | Loss: 0.00001899
Iteration 53/1000 | Loss: 0.00001899
Iteration 54/1000 | Loss: 0.00001899
Iteration 55/1000 | Loss: 0.00001899
Iteration 56/1000 | Loss: 0.00001899
Iteration 57/1000 | Loss: 0.00001899
Iteration 58/1000 | Loss: 0.00001899
Iteration 59/1000 | Loss: 0.00001899
Iteration 60/1000 | Loss: 0.00001899
Iteration 61/1000 | Loss: 0.00001898
Iteration 62/1000 | Loss: 0.00001898
Iteration 63/1000 | Loss: 0.00001897
Iteration 64/1000 | Loss: 0.00001897
Iteration 65/1000 | Loss: 0.00001896
Iteration 66/1000 | Loss: 0.00001896
Iteration 67/1000 | Loss: 0.00001895
Iteration 68/1000 | Loss: 0.00001895
Iteration 69/1000 | Loss: 0.00001894
Iteration 70/1000 | Loss: 0.00001894
Iteration 71/1000 | Loss: 0.00001894
Iteration 72/1000 | Loss: 0.00001894
Iteration 73/1000 | Loss: 0.00001894
Iteration 74/1000 | Loss: 0.00001894
Iteration 75/1000 | Loss: 0.00001894
Iteration 76/1000 | Loss: 0.00001894
Iteration 77/1000 | Loss: 0.00001893
Iteration 78/1000 | Loss: 0.00001893
Iteration 79/1000 | Loss: 0.00001893
Iteration 80/1000 | Loss: 0.00001893
Iteration 81/1000 | Loss: 0.00001893
Iteration 82/1000 | Loss: 0.00001892
Iteration 83/1000 | Loss: 0.00001892
Iteration 84/1000 | Loss: 0.00001892
Iteration 85/1000 | Loss: 0.00001891
Iteration 86/1000 | Loss: 0.00001891
Iteration 87/1000 | Loss: 0.00001891
Iteration 88/1000 | Loss: 0.00001891
Iteration 89/1000 | Loss: 0.00001891
Iteration 90/1000 | Loss: 0.00001891
Iteration 91/1000 | Loss: 0.00001891
Iteration 92/1000 | Loss: 0.00001891
Iteration 93/1000 | Loss: 0.00001891
Iteration 94/1000 | Loss: 0.00001891
Iteration 95/1000 | Loss: 0.00001891
Iteration 96/1000 | Loss: 0.00001890
Iteration 97/1000 | Loss: 0.00001890
Iteration 98/1000 | Loss: 0.00001890
Iteration 99/1000 | Loss: 0.00001890
Iteration 100/1000 | Loss: 0.00001890
Iteration 101/1000 | Loss: 0.00001890
Iteration 102/1000 | Loss: 0.00001890
Iteration 103/1000 | Loss: 0.00001890
Iteration 104/1000 | Loss: 0.00001890
Iteration 105/1000 | Loss: 0.00001890
Iteration 106/1000 | Loss: 0.00001890
Iteration 107/1000 | Loss: 0.00001889
Iteration 108/1000 | Loss: 0.00001889
Iteration 109/1000 | Loss: 0.00001889
Iteration 110/1000 | Loss: 0.00001889
Iteration 111/1000 | Loss: 0.00001889
Iteration 112/1000 | Loss: 0.00001889
Iteration 113/1000 | Loss: 0.00001889
Iteration 114/1000 | Loss: 0.00001889
Iteration 115/1000 | Loss: 0.00001889
Iteration 116/1000 | Loss: 0.00001889
Iteration 117/1000 | Loss: 0.00001889
Iteration 118/1000 | Loss: 0.00001889
Iteration 119/1000 | Loss: 0.00001889
Iteration 120/1000 | Loss: 0.00001889
Iteration 121/1000 | Loss: 0.00001889
Iteration 122/1000 | Loss: 0.00001889
Iteration 123/1000 | Loss: 0.00001889
Iteration 124/1000 | Loss: 0.00001889
Iteration 125/1000 | Loss: 0.00001889
Iteration 126/1000 | Loss: 0.00001889
Iteration 127/1000 | Loss: 0.00001889
Iteration 128/1000 | Loss: 0.00001889
Iteration 129/1000 | Loss: 0.00001889
Iteration 130/1000 | Loss: 0.00001889
Iteration 131/1000 | Loss: 0.00001889
Iteration 132/1000 | Loss: 0.00001889
Iteration 133/1000 | Loss: 0.00001889
Iteration 134/1000 | Loss: 0.00001889
Iteration 135/1000 | Loss: 0.00001889
Iteration 136/1000 | Loss: 0.00001889
Iteration 137/1000 | Loss: 0.00001889
Iteration 138/1000 | Loss: 0.00001889
Iteration 139/1000 | Loss: 0.00001889
Iteration 140/1000 | Loss: 0.00001889
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.889070335892029e-05, 1.889070335892029e-05, 1.889070335892029e-05, 1.889070335892029e-05, 1.889070335892029e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.889070335892029e-05

Optimization complete. Final v2v error: 3.68306303024292 mm

Highest mean error: 3.9883055686950684 mm for frame 136

Lowest mean error: 3.3211324214935303 mm for frame 191

Saving results

Total time: 58.40550684928894
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5760/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5760/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5760/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00448594
Iteration 2/25 | Loss: 0.00140150
Iteration 3/25 | Loss: 0.00130974
Iteration 4/25 | Loss: 0.00129503
Iteration 5/25 | Loss: 0.00129033
Iteration 6/25 | Loss: 0.00128874
Iteration 7/25 | Loss: 0.00128874
Iteration 8/25 | Loss: 0.00128874
Iteration 9/25 | Loss: 0.00128874
Iteration 10/25 | Loss: 0.00128874
Iteration 11/25 | Loss: 0.00128874
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012887446209788322, 0.0012887446209788322, 0.0012887446209788322, 0.0012887446209788322, 0.0012887446209788322]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012887446209788322

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.35775852
Iteration 2/25 | Loss: 0.00367943
Iteration 3/25 | Loss: 0.00367942
Iteration 4/25 | Loss: 0.00367942
Iteration 5/25 | Loss: 0.00367942
Iteration 6/25 | Loss: 0.00367942
Iteration 7/25 | Loss: 0.00367942
Iteration 8/25 | Loss: 0.00367942
Iteration 9/25 | Loss: 0.00367942
Iteration 10/25 | Loss: 0.00367942
Iteration 11/25 | Loss: 0.00367942
Iteration 12/25 | Loss: 0.00367942
Iteration 13/25 | Loss: 0.00367942
Iteration 14/25 | Loss: 0.00367942
Iteration 15/25 | Loss: 0.00367942
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0036794154439121485, 0.0036794154439121485, 0.0036794154439121485, 0.0036794154439121485, 0.0036794154439121485]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0036794154439121485

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00367942
Iteration 2/1000 | Loss: 0.00003813
Iteration 3/1000 | Loss: 0.00002659
Iteration 4/1000 | Loss: 0.00002263
Iteration 5/1000 | Loss: 0.00002046
Iteration 6/1000 | Loss: 0.00001926
Iteration 7/1000 | Loss: 0.00001825
Iteration 8/1000 | Loss: 0.00001757
Iteration 9/1000 | Loss: 0.00001696
Iteration 10/1000 | Loss: 0.00001660
Iteration 11/1000 | Loss: 0.00001636
Iteration 12/1000 | Loss: 0.00001627
Iteration 13/1000 | Loss: 0.00001621
Iteration 14/1000 | Loss: 0.00001615
Iteration 15/1000 | Loss: 0.00001611
Iteration 16/1000 | Loss: 0.00001606
Iteration 17/1000 | Loss: 0.00001606
Iteration 18/1000 | Loss: 0.00001605
Iteration 19/1000 | Loss: 0.00001603
Iteration 20/1000 | Loss: 0.00001602
Iteration 21/1000 | Loss: 0.00001601
Iteration 22/1000 | Loss: 0.00001601
Iteration 23/1000 | Loss: 0.00001600
Iteration 24/1000 | Loss: 0.00001599
Iteration 25/1000 | Loss: 0.00001599
Iteration 26/1000 | Loss: 0.00001598
Iteration 27/1000 | Loss: 0.00001598
Iteration 28/1000 | Loss: 0.00001597
Iteration 29/1000 | Loss: 0.00001597
Iteration 30/1000 | Loss: 0.00001596
Iteration 31/1000 | Loss: 0.00001596
Iteration 32/1000 | Loss: 0.00001596
Iteration 33/1000 | Loss: 0.00001595
Iteration 34/1000 | Loss: 0.00001595
Iteration 35/1000 | Loss: 0.00001595
Iteration 36/1000 | Loss: 0.00001595
Iteration 37/1000 | Loss: 0.00001595
Iteration 38/1000 | Loss: 0.00001594
Iteration 39/1000 | Loss: 0.00001594
Iteration 40/1000 | Loss: 0.00001594
Iteration 41/1000 | Loss: 0.00001593
Iteration 42/1000 | Loss: 0.00001593
Iteration 43/1000 | Loss: 0.00001593
Iteration 44/1000 | Loss: 0.00001592
Iteration 45/1000 | Loss: 0.00001592
Iteration 46/1000 | Loss: 0.00001592
Iteration 47/1000 | Loss: 0.00001592
Iteration 48/1000 | Loss: 0.00001591
Iteration 49/1000 | Loss: 0.00001591
Iteration 50/1000 | Loss: 0.00001591
Iteration 51/1000 | Loss: 0.00001591
Iteration 52/1000 | Loss: 0.00001591
Iteration 53/1000 | Loss: 0.00001591
Iteration 54/1000 | Loss: 0.00001591
Iteration 55/1000 | Loss: 0.00001591
Iteration 56/1000 | Loss: 0.00001591
Iteration 57/1000 | Loss: 0.00001591
Iteration 58/1000 | Loss: 0.00001590
Iteration 59/1000 | Loss: 0.00001590
Iteration 60/1000 | Loss: 0.00001590
Iteration 61/1000 | Loss: 0.00001590
Iteration 62/1000 | Loss: 0.00001590
Iteration 63/1000 | Loss: 0.00001590
Iteration 64/1000 | Loss: 0.00001590
Iteration 65/1000 | Loss: 0.00001590
Iteration 66/1000 | Loss: 0.00001590
Iteration 67/1000 | Loss: 0.00001589
Iteration 68/1000 | Loss: 0.00001589
Iteration 69/1000 | Loss: 0.00001589
Iteration 70/1000 | Loss: 0.00001589
Iteration 71/1000 | Loss: 0.00001589
Iteration 72/1000 | Loss: 0.00001589
Iteration 73/1000 | Loss: 0.00001589
Iteration 74/1000 | Loss: 0.00001589
Iteration 75/1000 | Loss: 0.00001589
Iteration 76/1000 | Loss: 0.00001589
Iteration 77/1000 | Loss: 0.00001589
Iteration 78/1000 | Loss: 0.00001589
Iteration 79/1000 | Loss: 0.00001588
Iteration 80/1000 | Loss: 0.00001588
Iteration 81/1000 | Loss: 0.00001588
Iteration 82/1000 | Loss: 0.00001588
Iteration 83/1000 | Loss: 0.00001588
Iteration 84/1000 | Loss: 0.00001588
Iteration 85/1000 | Loss: 0.00001588
Iteration 86/1000 | Loss: 0.00001588
Iteration 87/1000 | Loss: 0.00001588
Iteration 88/1000 | Loss: 0.00001588
Iteration 89/1000 | Loss: 0.00001588
Iteration 90/1000 | Loss: 0.00001588
Iteration 91/1000 | Loss: 0.00001588
Iteration 92/1000 | Loss: 0.00001588
Iteration 93/1000 | Loss: 0.00001588
Iteration 94/1000 | Loss: 0.00001588
Iteration 95/1000 | Loss: 0.00001587
Iteration 96/1000 | Loss: 0.00001587
Iteration 97/1000 | Loss: 0.00001587
Iteration 98/1000 | Loss: 0.00001587
Iteration 99/1000 | Loss: 0.00001587
Iteration 100/1000 | Loss: 0.00001587
Iteration 101/1000 | Loss: 0.00001587
Iteration 102/1000 | Loss: 0.00001587
Iteration 103/1000 | Loss: 0.00001587
Iteration 104/1000 | Loss: 0.00001587
Iteration 105/1000 | Loss: 0.00001587
Iteration 106/1000 | Loss: 0.00001587
Iteration 107/1000 | Loss: 0.00001586
Iteration 108/1000 | Loss: 0.00001586
Iteration 109/1000 | Loss: 0.00001586
Iteration 110/1000 | Loss: 0.00001586
Iteration 111/1000 | Loss: 0.00001586
Iteration 112/1000 | Loss: 0.00001586
Iteration 113/1000 | Loss: 0.00001586
Iteration 114/1000 | Loss: 0.00001586
Iteration 115/1000 | Loss: 0.00001586
Iteration 116/1000 | Loss: 0.00001586
Iteration 117/1000 | Loss: 0.00001586
Iteration 118/1000 | Loss: 0.00001586
Iteration 119/1000 | Loss: 0.00001586
Iteration 120/1000 | Loss: 0.00001586
Iteration 121/1000 | Loss: 0.00001586
Iteration 122/1000 | Loss: 0.00001586
Iteration 123/1000 | Loss: 0.00001586
Iteration 124/1000 | Loss: 0.00001586
Iteration 125/1000 | Loss: 0.00001586
Iteration 126/1000 | Loss: 0.00001586
Iteration 127/1000 | Loss: 0.00001585
Iteration 128/1000 | Loss: 0.00001585
Iteration 129/1000 | Loss: 0.00001585
Iteration 130/1000 | Loss: 0.00001585
Iteration 131/1000 | Loss: 0.00001585
Iteration 132/1000 | Loss: 0.00001585
Iteration 133/1000 | Loss: 0.00001585
Iteration 134/1000 | Loss: 0.00001585
Iteration 135/1000 | Loss: 0.00001585
Iteration 136/1000 | Loss: 0.00001585
Iteration 137/1000 | Loss: 0.00001585
Iteration 138/1000 | Loss: 0.00001585
Iteration 139/1000 | Loss: 0.00001584
Iteration 140/1000 | Loss: 0.00001584
Iteration 141/1000 | Loss: 0.00001584
Iteration 142/1000 | Loss: 0.00001584
Iteration 143/1000 | Loss: 0.00001584
Iteration 144/1000 | Loss: 0.00001584
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.5844852896407247e-05, 1.5844852896407247e-05, 1.5844852896407247e-05, 1.5844852896407247e-05, 1.5844852896407247e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5844852896407247e-05

Optimization complete. Final v2v error: 3.328603744506836 mm

Highest mean error: 3.6426405906677246 mm for frame 110

Lowest mean error: 3.035121440887451 mm for frame 196

Saving results

Total time: 40.86375975608826
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5760/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5760/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5760/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00997301
Iteration 2/25 | Loss: 0.00236547
Iteration 3/25 | Loss: 0.00193943
Iteration 4/25 | Loss: 0.00155179
Iteration 5/25 | Loss: 0.00153346
Iteration 6/25 | Loss: 0.00150582
Iteration 7/25 | Loss: 0.00147926
Iteration 8/25 | Loss: 0.00145401
Iteration 9/25 | Loss: 0.00143689
Iteration 10/25 | Loss: 0.00142810
Iteration 11/25 | Loss: 0.00142631
Iteration 12/25 | Loss: 0.00142538
Iteration 13/25 | Loss: 0.00142470
Iteration 14/25 | Loss: 0.00142407
Iteration 15/25 | Loss: 0.00142378
Iteration 16/25 | Loss: 0.00142359
Iteration 17/25 | Loss: 0.00142349
Iteration 18/25 | Loss: 0.00142341
Iteration 19/25 | Loss: 0.00142336
Iteration 20/25 | Loss: 0.00142330
Iteration 21/25 | Loss: 0.00142328
Iteration 22/25 | Loss: 0.00142328
Iteration 23/25 | Loss: 0.00142328
Iteration 24/25 | Loss: 0.00142328
Iteration 25/25 | Loss: 0.00142328

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.15170288
Iteration 2/25 | Loss: 0.00425600
Iteration 3/25 | Loss: 0.00390781
Iteration 4/25 | Loss: 0.00390781
Iteration 5/25 | Loss: 0.00390781
Iteration 6/25 | Loss: 0.00390781
Iteration 7/25 | Loss: 0.00390781
Iteration 8/25 | Loss: 0.00390781
Iteration 9/25 | Loss: 0.00390781
Iteration 10/25 | Loss: 0.00390781
Iteration 11/25 | Loss: 0.00390781
Iteration 12/25 | Loss: 0.00390781
Iteration 13/25 | Loss: 0.00390781
Iteration 14/25 | Loss: 0.00390781
Iteration 15/25 | Loss: 0.00390781
Iteration 16/25 | Loss: 0.00390781
Iteration 17/25 | Loss: 0.00390781
Iteration 18/25 | Loss: 0.00390781
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.003907809033989906, 0.003907809033989906, 0.003907809033989906, 0.003907809033989906, 0.003907809033989906]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003907809033989906

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00390781
Iteration 2/1000 | Loss: 0.00060812
Iteration 3/1000 | Loss: 0.00053861
Iteration 4/1000 | Loss: 0.00055376
Iteration 5/1000 | Loss: 0.00135617
Iteration 6/1000 | Loss: 0.00231800
Iteration 7/1000 | Loss: 0.00051128
Iteration 8/1000 | Loss: 0.00019682
Iteration 9/1000 | Loss: 0.00127906
Iteration 10/1000 | Loss: 0.00014765
Iteration 11/1000 | Loss: 0.00105880
Iteration 12/1000 | Loss: 0.00087954
Iteration 13/1000 | Loss: 0.00026983
Iteration 14/1000 | Loss: 0.00009260
Iteration 15/1000 | Loss: 0.00058816
Iteration 16/1000 | Loss: 0.00008630
Iteration 17/1000 | Loss: 0.00007576
Iteration 18/1000 | Loss: 0.00016501
Iteration 19/1000 | Loss: 0.00007172
Iteration 20/1000 | Loss: 0.00006446
Iteration 21/1000 | Loss: 0.00056816
Iteration 22/1000 | Loss: 0.00014571
Iteration 23/1000 | Loss: 0.00044409
Iteration 24/1000 | Loss: 0.00006584
Iteration 25/1000 | Loss: 0.00058244
Iteration 26/1000 | Loss: 0.00018595
Iteration 27/1000 | Loss: 0.00025718
Iteration 28/1000 | Loss: 0.00006296
Iteration 29/1000 | Loss: 0.00167561
Iteration 30/1000 | Loss: 0.00044541
Iteration 31/1000 | Loss: 0.00043909
Iteration 32/1000 | Loss: 0.00200646
Iteration 33/1000 | Loss: 0.00070476
Iteration 34/1000 | Loss: 0.00051068
Iteration 35/1000 | Loss: 0.00028154
Iteration 36/1000 | Loss: 0.00019149
Iteration 37/1000 | Loss: 0.00022636
Iteration 38/1000 | Loss: 0.00009695
Iteration 39/1000 | Loss: 0.00007007
Iteration 40/1000 | Loss: 0.00020184
Iteration 41/1000 | Loss: 0.00025889
Iteration 42/1000 | Loss: 0.00023869
Iteration 43/1000 | Loss: 0.00004684
Iteration 44/1000 | Loss: 0.00003978
Iteration 45/1000 | Loss: 0.00003533
Iteration 46/1000 | Loss: 0.00032786
Iteration 47/1000 | Loss: 0.00034506
Iteration 48/1000 | Loss: 0.00030410
Iteration 49/1000 | Loss: 0.00045326
Iteration 50/1000 | Loss: 0.00024109
Iteration 51/1000 | Loss: 0.00050923
Iteration 52/1000 | Loss: 0.00005763
Iteration 53/1000 | Loss: 0.00003782
Iteration 54/1000 | Loss: 0.00005245
Iteration 55/1000 | Loss: 0.00007536
Iteration 56/1000 | Loss: 0.00003999
Iteration 57/1000 | Loss: 0.00019698
Iteration 58/1000 | Loss: 0.00014135
Iteration 59/1000 | Loss: 0.00021405
Iteration 60/1000 | Loss: 0.00017529
Iteration 61/1000 | Loss: 0.00029426
Iteration 62/1000 | Loss: 0.00025319
Iteration 63/1000 | Loss: 0.00027448
Iteration 64/1000 | Loss: 0.00033478
Iteration 65/1000 | Loss: 0.00027971
Iteration 66/1000 | Loss: 0.00026135
Iteration 67/1000 | Loss: 0.00004083
Iteration 68/1000 | Loss: 0.00003350
Iteration 69/1000 | Loss: 0.00003125
Iteration 70/1000 | Loss: 0.00003054
Iteration 71/1000 | Loss: 0.00003011
Iteration 72/1000 | Loss: 0.00002962
Iteration 73/1000 | Loss: 0.00002912
Iteration 74/1000 | Loss: 0.00040174
Iteration 75/1000 | Loss: 0.00030029
Iteration 76/1000 | Loss: 0.00003522
Iteration 77/1000 | Loss: 0.00044151
Iteration 78/1000 | Loss: 0.00026275
Iteration 79/1000 | Loss: 0.00046827
Iteration 80/1000 | Loss: 0.00052399
Iteration 81/1000 | Loss: 0.00021223
Iteration 82/1000 | Loss: 0.00020405
Iteration 83/1000 | Loss: 0.00024169
Iteration 84/1000 | Loss: 0.00012752
Iteration 85/1000 | Loss: 0.00026098
Iteration 86/1000 | Loss: 0.00030489
Iteration 87/1000 | Loss: 0.00027451
Iteration 88/1000 | Loss: 0.00024291
Iteration 89/1000 | Loss: 0.00016222
Iteration 90/1000 | Loss: 0.00036011
Iteration 91/1000 | Loss: 0.00013702
Iteration 92/1000 | Loss: 0.00019050
Iteration 93/1000 | Loss: 0.00007327
Iteration 94/1000 | Loss: 0.00036043
Iteration 95/1000 | Loss: 0.00022419
Iteration 96/1000 | Loss: 0.00028294
Iteration 97/1000 | Loss: 0.00006557
Iteration 98/1000 | Loss: 0.00041968
Iteration 99/1000 | Loss: 0.00035473
Iteration 100/1000 | Loss: 0.00070381
Iteration 101/1000 | Loss: 0.00029118
Iteration 102/1000 | Loss: 0.00035905
Iteration 103/1000 | Loss: 0.00032545
Iteration 104/1000 | Loss: 0.00029881
Iteration 105/1000 | Loss: 0.00030880
Iteration 106/1000 | Loss: 0.00035834
Iteration 107/1000 | Loss: 0.00004745
Iteration 108/1000 | Loss: 0.00004061
Iteration 109/1000 | Loss: 0.00003660
Iteration 110/1000 | Loss: 0.00003412
Iteration 111/1000 | Loss: 0.00003047
Iteration 112/1000 | Loss: 0.00002750
Iteration 113/1000 | Loss: 0.00002593
Iteration 114/1000 | Loss: 0.00002539
Iteration 115/1000 | Loss: 0.00002520
Iteration 116/1000 | Loss: 0.00002512
Iteration 117/1000 | Loss: 0.00050360
Iteration 118/1000 | Loss: 0.00019145
Iteration 119/1000 | Loss: 0.00003481
Iteration 120/1000 | Loss: 0.00002558
Iteration 121/1000 | Loss: 0.00002511
Iteration 122/1000 | Loss: 0.00002494
Iteration 123/1000 | Loss: 0.00002493
Iteration 124/1000 | Loss: 0.00002487
Iteration 125/1000 | Loss: 0.00047894
Iteration 126/1000 | Loss: 0.00067192
Iteration 127/1000 | Loss: 0.00020573
Iteration 128/1000 | Loss: 0.00002998
Iteration 129/1000 | Loss: 0.00002682
Iteration 130/1000 | Loss: 0.00002528
Iteration 131/1000 | Loss: 0.00002498
Iteration 132/1000 | Loss: 0.00002485
Iteration 133/1000 | Loss: 0.00002485
Iteration 134/1000 | Loss: 0.00002484
Iteration 135/1000 | Loss: 0.00002484
Iteration 136/1000 | Loss: 0.00002481
Iteration 137/1000 | Loss: 0.00002480
Iteration 138/1000 | Loss: 0.00002480
Iteration 139/1000 | Loss: 0.00002480
Iteration 140/1000 | Loss: 0.00002480
Iteration 141/1000 | Loss: 0.00002480
Iteration 142/1000 | Loss: 0.00002480
Iteration 143/1000 | Loss: 0.00002480
Iteration 144/1000 | Loss: 0.00002480
Iteration 145/1000 | Loss: 0.00037337
Iteration 146/1000 | Loss: 0.00010231
Iteration 147/1000 | Loss: 0.00002519
Iteration 148/1000 | Loss: 0.00002483
Iteration 149/1000 | Loss: 0.00033342
Iteration 150/1000 | Loss: 0.00009467
Iteration 151/1000 | Loss: 0.00033227
Iteration 152/1000 | Loss: 0.00015082
Iteration 153/1000 | Loss: 0.00069149
Iteration 154/1000 | Loss: 0.00130631
Iteration 155/1000 | Loss: 0.00070894
Iteration 156/1000 | Loss: 0.00029420
Iteration 157/1000 | Loss: 0.00037727
Iteration 158/1000 | Loss: 0.00036410
Iteration 159/1000 | Loss: 0.00003666
Iteration 160/1000 | Loss: 0.00002940
Iteration 161/1000 | Loss: 0.00002554
Iteration 162/1000 | Loss: 0.00002388
Iteration 163/1000 | Loss: 0.00002326
Iteration 164/1000 | Loss: 0.00002284
Iteration 165/1000 | Loss: 0.00002249
Iteration 166/1000 | Loss: 0.00002226
Iteration 167/1000 | Loss: 0.00002224
Iteration 168/1000 | Loss: 0.00002210
Iteration 169/1000 | Loss: 0.00002209
Iteration 170/1000 | Loss: 0.00002208
Iteration 171/1000 | Loss: 0.00002202
Iteration 172/1000 | Loss: 0.00002202
Iteration 173/1000 | Loss: 0.00002202
Iteration 174/1000 | Loss: 0.00002202
Iteration 175/1000 | Loss: 0.00002202
Iteration 176/1000 | Loss: 0.00002202
Iteration 177/1000 | Loss: 0.00002202
Iteration 178/1000 | Loss: 0.00002202
Iteration 179/1000 | Loss: 0.00002201
Iteration 180/1000 | Loss: 0.00002201
Iteration 181/1000 | Loss: 0.00002201
Iteration 182/1000 | Loss: 0.00002199
Iteration 183/1000 | Loss: 0.00002199
Iteration 184/1000 | Loss: 0.00002198
Iteration 185/1000 | Loss: 0.00002198
Iteration 186/1000 | Loss: 0.00002197
Iteration 187/1000 | Loss: 0.00002197
Iteration 188/1000 | Loss: 0.00002196
Iteration 189/1000 | Loss: 0.00002196
Iteration 190/1000 | Loss: 0.00002196
Iteration 191/1000 | Loss: 0.00002196
Iteration 192/1000 | Loss: 0.00002196
Iteration 193/1000 | Loss: 0.00002195
Iteration 194/1000 | Loss: 0.00002195
Iteration 195/1000 | Loss: 0.00002195
Iteration 196/1000 | Loss: 0.00002193
Iteration 197/1000 | Loss: 0.00002193
Iteration 198/1000 | Loss: 0.00002193
Iteration 199/1000 | Loss: 0.00002193
Iteration 200/1000 | Loss: 0.00002193
Iteration 201/1000 | Loss: 0.00002192
Iteration 202/1000 | Loss: 0.00002192
Iteration 203/1000 | Loss: 0.00002192
Iteration 204/1000 | Loss: 0.00002192
Iteration 205/1000 | Loss: 0.00002192
Iteration 206/1000 | Loss: 0.00002192
Iteration 207/1000 | Loss: 0.00002192
Iteration 208/1000 | Loss: 0.00002192
Iteration 209/1000 | Loss: 0.00002192
Iteration 210/1000 | Loss: 0.00002192
Iteration 211/1000 | Loss: 0.00002192
Iteration 212/1000 | Loss: 0.00002192
Iteration 213/1000 | Loss: 0.00002192
Iteration 214/1000 | Loss: 0.00002192
Iteration 215/1000 | Loss: 0.00002192
Iteration 216/1000 | Loss: 0.00002191
Iteration 217/1000 | Loss: 0.00002191
Iteration 218/1000 | Loss: 0.00002191
Iteration 219/1000 | Loss: 0.00002191
Iteration 220/1000 | Loss: 0.00002191
Iteration 221/1000 | Loss: 0.00002191
Iteration 222/1000 | Loss: 0.00002191
Iteration 223/1000 | Loss: 0.00002191
Iteration 224/1000 | Loss: 0.00002191
Iteration 225/1000 | Loss: 0.00002191
Iteration 226/1000 | Loss: 0.00002191
Iteration 227/1000 | Loss: 0.00002191
Iteration 228/1000 | Loss: 0.00002191
Iteration 229/1000 | Loss: 0.00002191
Iteration 230/1000 | Loss: 0.00002191
Iteration 231/1000 | Loss: 0.00002191
Iteration 232/1000 | Loss: 0.00002191
Iteration 233/1000 | Loss: 0.00002191
Iteration 234/1000 | Loss: 0.00002190
Iteration 235/1000 | Loss: 0.00002190
Iteration 236/1000 | Loss: 0.00002190
Iteration 237/1000 | Loss: 0.00002190
Iteration 238/1000 | Loss: 0.00002190
Iteration 239/1000 | Loss: 0.00002190
Iteration 240/1000 | Loss: 0.00002190
Iteration 241/1000 | Loss: 0.00002190
Iteration 242/1000 | Loss: 0.00002190
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 242. Stopping optimization.
Last 5 losses: [2.1900588762946427e-05, 2.1900588762946427e-05, 2.1900588762946427e-05, 2.1900588762946427e-05, 2.1900588762946427e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1900588762946427e-05

Optimization complete. Final v2v error: 3.730315923690796 mm

Highest mean error: 8.67338752746582 mm for frame 9

Lowest mean error: 2.9724533557891846 mm for frame 149

Saving results

Total time: 266.45079803466797
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_37_nl_5760/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5760/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_37_nl_5760/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01003194
Iteration 2/25 | Loss: 0.00211046
Iteration 3/25 | Loss: 0.00162624
Iteration 4/25 | Loss: 0.00152786
Iteration 5/25 | Loss: 0.00145961
Iteration 6/25 | Loss: 0.00143602
Iteration 7/25 | Loss: 0.00145001
Iteration 8/25 | Loss: 0.00143490
Iteration 9/25 | Loss: 0.00140306
Iteration 10/25 | Loss: 0.00134679
Iteration 11/25 | Loss: 0.00130782
Iteration 12/25 | Loss: 0.00131166
Iteration 13/25 | Loss: 0.00129276
Iteration 14/25 | Loss: 0.00128502
Iteration 15/25 | Loss: 0.00128157
Iteration 16/25 | Loss: 0.00127962
Iteration 17/25 | Loss: 0.00127978
Iteration 18/25 | Loss: 0.00128002
Iteration 19/25 | Loss: 0.00127918
Iteration 20/25 | Loss: 0.00127943
Iteration 21/25 | Loss: 0.00128022
Iteration 22/25 | Loss: 0.00127927
Iteration 23/25 | Loss: 0.00127955
Iteration 24/25 | Loss: 0.00127900
Iteration 25/25 | Loss: 0.00127947

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19728839
Iteration 2/25 | Loss: 0.00354766
Iteration 3/25 | Loss: 0.00335372
Iteration 4/25 | Loss: 0.00335372
Iteration 5/25 | Loss: 0.00335372
Iteration 6/25 | Loss: 0.00335372
Iteration 7/25 | Loss: 0.00335372
Iteration 8/25 | Loss: 0.00335371
Iteration 9/25 | Loss: 0.00335371
Iteration 10/25 | Loss: 0.00335371
Iteration 11/25 | Loss: 0.00335371
Iteration 12/25 | Loss: 0.00335371
Iteration 13/25 | Loss: 0.00335371
Iteration 14/25 | Loss: 0.00335371
Iteration 15/25 | Loss: 0.00335371
Iteration 16/25 | Loss: 0.00335371
Iteration 17/25 | Loss: 0.00335371
Iteration 18/25 | Loss: 0.00335371
Iteration 19/25 | Loss: 0.00335371
Iteration 20/25 | Loss: 0.00335371
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0033537140116095543, 0.0033537140116095543, 0.0033537140116095543, 0.0033537140116095543, 0.0033537140116095543]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0033537140116095543

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00335371
Iteration 2/1000 | Loss: 0.00007345
Iteration 3/1000 | Loss: 0.00016070
Iteration 4/1000 | Loss: 0.00012776
Iteration 5/1000 | Loss: 0.00011855
Iteration 6/1000 | Loss: 0.00004166
Iteration 7/1000 | Loss: 0.00003585
Iteration 8/1000 | Loss: 0.00002711
Iteration 9/1000 | Loss: 0.00003408
Iteration 10/1000 | Loss: 0.00002970
Iteration 11/1000 | Loss: 0.00003029
Iteration 12/1000 | Loss: 0.00003183
Iteration 13/1000 | Loss: 0.00003107
Iteration 14/1000 | Loss: 0.00003352
Iteration 15/1000 | Loss: 0.00012484
Iteration 16/1000 | Loss: 0.00044554
Iteration 17/1000 | Loss: 0.00003614
Iteration 18/1000 | Loss: 0.00040276
Iteration 19/1000 | Loss: 0.00021141
Iteration 20/1000 | Loss: 0.00005885
Iteration 21/1000 | Loss: 0.00002946
Iteration 22/1000 | Loss: 0.00013973
Iteration 23/1000 | Loss: 0.00003736
Iteration 24/1000 | Loss: 0.00007886
Iteration 25/1000 | Loss: 0.00003749
Iteration 26/1000 | Loss: 0.00016280
Iteration 27/1000 | Loss: 0.00013709
Iteration 28/1000 | Loss: 0.00004427
Iteration 29/1000 | Loss: 0.00003045
Iteration 30/1000 | Loss: 0.00002780
Iteration 31/1000 | Loss: 0.00020748
Iteration 32/1000 | Loss: 0.00013281
Iteration 33/1000 | Loss: 0.00022010
Iteration 34/1000 | Loss: 0.00022721
Iteration 35/1000 | Loss: 0.00027637
Iteration 36/1000 | Loss: 0.00010707
Iteration 37/1000 | Loss: 0.00008480
Iteration 38/1000 | Loss: 0.00014820
Iteration 39/1000 | Loss: 0.00007469
Iteration 40/1000 | Loss: 0.00015319
Iteration 41/1000 | Loss: 0.00016975
Iteration 42/1000 | Loss: 0.00016254
Iteration 43/1000 | Loss: 0.00033714
Iteration 44/1000 | Loss: 0.00015428
Iteration 45/1000 | Loss: 0.00004783
Iteration 46/1000 | Loss: 0.00008540
Iteration 47/1000 | Loss: 0.00002462
Iteration 48/1000 | Loss: 0.00002771
Iteration 49/1000 | Loss: 0.00005779
Iteration 50/1000 | Loss: 0.00003559
Iteration 51/1000 | Loss: 0.00004085
Iteration 52/1000 | Loss: 0.00002124
Iteration 53/1000 | Loss: 0.00014586
Iteration 54/1000 | Loss: 0.00015716
Iteration 55/1000 | Loss: 0.00017670
Iteration 56/1000 | Loss: 0.00015924
Iteration 57/1000 | Loss: 0.00012151
Iteration 58/1000 | Loss: 0.00026739
Iteration 59/1000 | Loss: 0.00018382
Iteration 60/1000 | Loss: 0.00004443
Iteration 61/1000 | Loss: 0.00006975
Iteration 62/1000 | Loss: 0.00003613
Iteration 63/1000 | Loss: 0.00003462
Iteration 64/1000 | Loss: 0.00012499
Iteration 65/1000 | Loss: 0.00002465
Iteration 66/1000 | Loss: 0.00004321
Iteration 67/1000 | Loss: 0.00003340
Iteration 68/1000 | Loss: 0.00003681
Iteration 69/1000 | Loss: 0.00003095
Iteration 70/1000 | Loss: 0.00004006
Iteration 71/1000 | Loss: 0.00003035
Iteration 72/1000 | Loss: 0.00021963
Iteration 73/1000 | Loss: 0.00008842
Iteration 74/1000 | Loss: 0.00002074
Iteration 75/1000 | Loss: 0.00005400
Iteration 76/1000 | Loss: 0.00005242
Iteration 77/1000 | Loss: 0.00002768
Iteration 78/1000 | Loss: 0.00002266
Iteration 79/1000 | Loss: 0.00004849
Iteration 80/1000 | Loss: 0.00004729
Iteration 81/1000 | Loss: 0.00001877
Iteration 82/1000 | Loss: 0.00003661
Iteration 83/1000 | Loss: 0.00001779
Iteration 84/1000 | Loss: 0.00001751
Iteration 85/1000 | Loss: 0.00001739
Iteration 86/1000 | Loss: 0.00001731
Iteration 87/1000 | Loss: 0.00001726
Iteration 88/1000 | Loss: 0.00001726
Iteration 89/1000 | Loss: 0.00001726
Iteration 90/1000 | Loss: 0.00001725
Iteration 91/1000 | Loss: 0.00001725
Iteration 92/1000 | Loss: 0.00001725
Iteration 93/1000 | Loss: 0.00001724
Iteration 94/1000 | Loss: 0.00009890
Iteration 95/1000 | Loss: 0.00023751
Iteration 96/1000 | Loss: 0.00009366
Iteration 97/1000 | Loss: 0.00001756
Iteration 98/1000 | Loss: 0.00001725
Iteration 99/1000 | Loss: 0.00001720
Iteration 100/1000 | Loss: 0.00001719
Iteration 101/1000 | Loss: 0.00001719
Iteration 102/1000 | Loss: 0.00001719
Iteration 103/1000 | Loss: 0.00001718
Iteration 104/1000 | Loss: 0.00001718
Iteration 105/1000 | Loss: 0.00001718
Iteration 106/1000 | Loss: 0.00001718
Iteration 107/1000 | Loss: 0.00001718
Iteration 108/1000 | Loss: 0.00001718
Iteration 109/1000 | Loss: 0.00001718
Iteration 110/1000 | Loss: 0.00001718
Iteration 111/1000 | Loss: 0.00001718
Iteration 112/1000 | Loss: 0.00001717
Iteration 113/1000 | Loss: 0.00001717
Iteration 114/1000 | Loss: 0.00001717
Iteration 115/1000 | Loss: 0.00001716
Iteration 116/1000 | Loss: 0.00001716
Iteration 117/1000 | Loss: 0.00001715
Iteration 118/1000 | Loss: 0.00001715
Iteration 119/1000 | Loss: 0.00001714
Iteration 120/1000 | Loss: 0.00001714
Iteration 121/1000 | Loss: 0.00001713
Iteration 122/1000 | Loss: 0.00001712
Iteration 123/1000 | Loss: 0.00001712
Iteration 124/1000 | Loss: 0.00001712
Iteration 125/1000 | Loss: 0.00001712
Iteration 126/1000 | Loss: 0.00001712
Iteration 127/1000 | Loss: 0.00001712
Iteration 128/1000 | Loss: 0.00001712
Iteration 129/1000 | Loss: 0.00001711
Iteration 130/1000 | Loss: 0.00001711
Iteration 131/1000 | Loss: 0.00001711
Iteration 132/1000 | Loss: 0.00001711
Iteration 133/1000 | Loss: 0.00001711
Iteration 134/1000 | Loss: 0.00001711
Iteration 135/1000 | Loss: 0.00001711
Iteration 136/1000 | Loss: 0.00001711
Iteration 137/1000 | Loss: 0.00001711
Iteration 138/1000 | Loss: 0.00001711
Iteration 139/1000 | Loss: 0.00001710
Iteration 140/1000 | Loss: 0.00001710
Iteration 141/1000 | Loss: 0.00001710
Iteration 142/1000 | Loss: 0.00001710
Iteration 143/1000 | Loss: 0.00001710
Iteration 144/1000 | Loss: 0.00001710
Iteration 145/1000 | Loss: 0.00001710
Iteration 146/1000 | Loss: 0.00001710
Iteration 147/1000 | Loss: 0.00001710
Iteration 148/1000 | Loss: 0.00001710
Iteration 149/1000 | Loss: 0.00001710
Iteration 150/1000 | Loss: 0.00001710
Iteration 151/1000 | Loss: 0.00001710
Iteration 152/1000 | Loss: 0.00001710
Iteration 153/1000 | Loss: 0.00001710
Iteration 154/1000 | Loss: 0.00001710
Iteration 155/1000 | Loss: 0.00001710
Iteration 156/1000 | Loss: 0.00001710
Iteration 157/1000 | Loss: 0.00001710
Iteration 158/1000 | Loss: 0.00001709
Iteration 159/1000 | Loss: 0.00001709
Iteration 160/1000 | Loss: 0.00001709
Iteration 161/1000 | Loss: 0.00001709
Iteration 162/1000 | Loss: 0.00001709
Iteration 163/1000 | Loss: 0.00001709
Iteration 164/1000 | Loss: 0.00001709
Iteration 165/1000 | Loss: 0.00001709
Iteration 166/1000 | Loss: 0.00001709
Iteration 167/1000 | Loss: 0.00001709
Iteration 168/1000 | Loss: 0.00001709
Iteration 169/1000 | Loss: 0.00001709
Iteration 170/1000 | Loss: 0.00001709
Iteration 171/1000 | Loss: 0.00001708
Iteration 172/1000 | Loss: 0.00001708
Iteration 173/1000 | Loss: 0.00001708
Iteration 174/1000 | Loss: 0.00001708
Iteration 175/1000 | Loss: 0.00001708
Iteration 176/1000 | Loss: 0.00001708
Iteration 177/1000 | Loss: 0.00001708
Iteration 178/1000 | Loss: 0.00001708
Iteration 179/1000 | Loss: 0.00001708
Iteration 180/1000 | Loss: 0.00001708
Iteration 181/1000 | Loss: 0.00001708
Iteration 182/1000 | Loss: 0.00001708
Iteration 183/1000 | Loss: 0.00001708
Iteration 184/1000 | Loss: 0.00001708
Iteration 185/1000 | Loss: 0.00001708
Iteration 186/1000 | Loss: 0.00001708
Iteration 187/1000 | Loss: 0.00001708
Iteration 188/1000 | Loss: 0.00001707
Iteration 189/1000 | Loss: 0.00001707
Iteration 190/1000 | Loss: 0.00001707
Iteration 191/1000 | Loss: 0.00001707
Iteration 192/1000 | Loss: 0.00001707
Iteration 193/1000 | Loss: 0.00001707
Iteration 194/1000 | Loss: 0.00001707
Iteration 195/1000 | Loss: 0.00001707
Iteration 196/1000 | Loss: 0.00001707
Iteration 197/1000 | Loss: 0.00001707
Iteration 198/1000 | Loss: 0.00001707
Iteration 199/1000 | Loss: 0.00001707
Iteration 200/1000 | Loss: 0.00001707
Iteration 201/1000 | Loss: 0.00001707
Iteration 202/1000 | Loss: 0.00001707
Iteration 203/1000 | Loss: 0.00001707
Iteration 204/1000 | Loss: 0.00001707
Iteration 205/1000 | Loss: 0.00001707
Iteration 206/1000 | Loss: 0.00001707
Iteration 207/1000 | Loss: 0.00001706
Iteration 208/1000 | Loss: 0.00001706
Iteration 209/1000 | Loss: 0.00001706
Iteration 210/1000 | Loss: 0.00001706
Iteration 211/1000 | Loss: 0.00001706
Iteration 212/1000 | Loss: 0.00001706
Iteration 213/1000 | Loss: 0.00001706
Iteration 214/1000 | Loss: 0.00001706
Iteration 215/1000 | Loss: 0.00001706
Iteration 216/1000 | Loss: 0.00001706
Iteration 217/1000 | Loss: 0.00001706
Iteration 218/1000 | Loss: 0.00001706
Iteration 219/1000 | Loss: 0.00001706
Iteration 220/1000 | Loss: 0.00001706
Iteration 221/1000 | Loss: 0.00001706
Iteration 222/1000 | Loss: 0.00001705
Iteration 223/1000 | Loss: 0.00001705
Iteration 224/1000 | Loss: 0.00001705
Iteration 225/1000 | Loss: 0.00001705
Iteration 226/1000 | Loss: 0.00001705
Iteration 227/1000 | Loss: 0.00001705
Iteration 228/1000 | Loss: 0.00001705
Iteration 229/1000 | Loss: 0.00001704
Iteration 230/1000 | Loss: 0.00001704
Iteration 231/1000 | Loss: 0.00001704
Iteration 232/1000 | Loss: 0.00001704
Iteration 233/1000 | Loss: 0.00001704
Iteration 234/1000 | Loss: 0.00001704
Iteration 235/1000 | Loss: 0.00001704
Iteration 236/1000 | Loss: 0.00001704
Iteration 237/1000 | Loss: 0.00001704
Iteration 238/1000 | Loss: 0.00001703
Iteration 239/1000 | Loss: 0.00001703
Iteration 240/1000 | Loss: 0.00001703
Iteration 241/1000 | Loss: 0.00001703
Iteration 242/1000 | Loss: 0.00001703
Iteration 243/1000 | Loss: 0.00001703
Iteration 244/1000 | Loss: 0.00001703
Iteration 245/1000 | Loss: 0.00001703
Iteration 246/1000 | Loss: 0.00001703
Iteration 247/1000 | Loss: 0.00001703
Iteration 248/1000 | Loss: 0.00001703
Iteration 249/1000 | Loss: 0.00001703
Iteration 250/1000 | Loss: 0.00001703
Iteration 251/1000 | Loss: 0.00001703
Iteration 252/1000 | Loss: 0.00001703
Iteration 253/1000 | Loss: 0.00001703
Iteration 254/1000 | Loss: 0.00001703
Iteration 255/1000 | Loss: 0.00001703
Iteration 256/1000 | Loss: 0.00001703
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 256. Stopping optimization.
Last 5 losses: [1.7031681636581197e-05, 1.7031681636581197e-05, 1.7031681636581197e-05, 1.7031681636581197e-05, 1.7031681636581197e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7031681636581197e-05

Optimization complete. Final v2v error: 3.4169352054595947 mm

Highest mean error: 4.471316814422607 mm for frame 21

Lowest mean error: 3.062588930130005 mm for frame 61

Saving results

Total time: 189.67944645881653
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_46_us_2175/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_46_us_2175/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_46_us_2175/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00954269
Iteration 2/25 | Loss: 0.00213920
Iteration 3/25 | Loss: 0.00190425
Iteration 4/25 | Loss: 0.00187595
Iteration 5/25 | Loss: 0.00187203
Iteration 6/25 | Loss: 0.00187164
Iteration 7/25 | Loss: 0.00187164
Iteration 8/25 | Loss: 0.00187164
Iteration 9/25 | Loss: 0.00187164
Iteration 10/25 | Loss: 0.00187164
Iteration 11/25 | Loss: 0.00187164
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0018716404447332025, 0.0018716404447332025, 0.0018716404447332025, 0.0018716404447332025, 0.0018716404447332025]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018716404447332025

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55022502
Iteration 2/25 | Loss: 0.00291658
Iteration 3/25 | Loss: 0.00291658
Iteration 4/25 | Loss: 0.00291657
Iteration 5/25 | Loss: 0.00291657
Iteration 6/25 | Loss: 0.00291657
Iteration 7/25 | Loss: 0.00291657
Iteration 8/25 | Loss: 0.00291657
Iteration 9/25 | Loss: 0.00291657
Iteration 10/25 | Loss: 0.00291657
Iteration 11/25 | Loss: 0.00291657
Iteration 12/25 | Loss: 0.00291657
Iteration 13/25 | Loss: 0.00291657
Iteration 14/25 | Loss: 0.00291657
Iteration 15/25 | Loss: 0.00291657
Iteration 16/25 | Loss: 0.00291657
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0029165728483349085, 0.0029165728483349085, 0.0029165728483349085, 0.0029165728483349085, 0.0029165728483349085]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0029165728483349085

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00291657
Iteration 2/1000 | Loss: 0.00008111
Iteration 3/1000 | Loss: 0.00005836
Iteration 4/1000 | Loss: 0.00005237
Iteration 5/1000 | Loss: 0.00004857
Iteration 6/1000 | Loss: 0.00004634
Iteration 7/1000 | Loss: 0.00004513
Iteration 8/1000 | Loss: 0.00004403
Iteration 9/1000 | Loss: 0.00004320
Iteration 10/1000 | Loss: 0.00004268
Iteration 11/1000 | Loss: 0.00004232
Iteration 12/1000 | Loss: 0.00004209
Iteration 13/1000 | Loss: 0.00004192
Iteration 14/1000 | Loss: 0.00004189
Iteration 15/1000 | Loss: 0.00004186
Iteration 16/1000 | Loss: 0.00004186
Iteration 17/1000 | Loss: 0.00004186
Iteration 18/1000 | Loss: 0.00004185
Iteration 19/1000 | Loss: 0.00004185
Iteration 20/1000 | Loss: 0.00004184
Iteration 21/1000 | Loss: 0.00004180
Iteration 22/1000 | Loss: 0.00004180
Iteration 23/1000 | Loss: 0.00004180
Iteration 24/1000 | Loss: 0.00004180
Iteration 25/1000 | Loss: 0.00004179
Iteration 26/1000 | Loss: 0.00004179
Iteration 27/1000 | Loss: 0.00004179
Iteration 28/1000 | Loss: 0.00004179
Iteration 29/1000 | Loss: 0.00004176
Iteration 30/1000 | Loss: 0.00004176
Iteration 31/1000 | Loss: 0.00004176
Iteration 32/1000 | Loss: 0.00004175
Iteration 33/1000 | Loss: 0.00004175
Iteration 34/1000 | Loss: 0.00004175
Iteration 35/1000 | Loss: 0.00004174
Iteration 36/1000 | Loss: 0.00004174
Iteration 37/1000 | Loss: 0.00004172
Iteration 38/1000 | Loss: 0.00004172
Iteration 39/1000 | Loss: 0.00004171
Iteration 40/1000 | Loss: 0.00004171
Iteration 41/1000 | Loss: 0.00004171
Iteration 42/1000 | Loss: 0.00004170
Iteration 43/1000 | Loss: 0.00004170
Iteration 44/1000 | Loss: 0.00004170
Iteration 45/1000 | Loss: 0.00004169
Iteration 46/1000 | Loss: 0.00004169
Iteration 47/1000 | Loss: 0.00004168
Iteration 48/1000 | Loss: 0.00004168
Iteration 49/1000 | Loss: 0.00004168
Iteration 50/1000 | Loss: 0.00004168
Iteration 51/1000 | Loss: 0.00004168
Iteration 52/1000 | Loss: 0.00004168
Iteration 53/1000 | Loss: 0.00004168
Iteration 54/1000 | Loss: 0.00004167
Iteration 55/1000 | Loss: 0.00004167
Iteration 56/1000 | Loss: 0.00004167
Iteration 57/1000 | Loss: 0.00004166
Iteration 58/1000 | Loss: 0.00004166
Iteration 59/1000 | Loss: 0.00004166
Iteration 60/1000 | Loss: 0.00004166
Iteration 61/1000 | Loss: 0.00004166
Iteration 62/1000 | Loss: 0.00004166
Iteration 63/1000 | Loss: 0.00004165
Iteration 64/1000 | Loss: 0.00004165
Iteration 65/1000 | Loss: 0.00004165
Iteration 66/1000 | Loss: 0.00004165
Iteration 67/1000 | Loss: 0.00004165
Iteration 68/1000 | Loss: 0.00004164
Iteration 69/1000 | Loss: 0.00004164
Iteration 70/1000 | Loss: 0.00004164
Iteration 71/1000 | Loss: 0.00004164
Iteration 72/1000 | Loss: 0.00004163
Iteration 73/1000 | Loss: 0.00004163
Iteration 74/1000 | Loss: 0.00004163
Iteration 75/1000 | Loss: 0.00004163
Iteration 76/1000 | Loss: 0.00004163
Iteration 77/1000 | Loss: 0.00004162
Iteration 78/1000 | Loss: 0.00004162
Iteration 79/1000 | Loss: 0.00004162
Iteration 80/1000 | Loss: 0.00004162
Iteration 81/1000 | Loss: 0.00004162
Iteration 82/1000 | Loss: 0.00004162
Iteration 83/1000 | Loss: 0.00004161
Iteration 84/1000 | Loss: 0.00004161
Iteration 85/1000 | Loss: 0.00004161
Iteration 86/1000 | Loss: 0.00004161
Iteration 87/1000 | Loss: 0.00004161
Iteration 88/1000 | Loss: 0.00004161
Iteration 89/1000 | Loss: 0.00004161
Iteration 90/1000 | Loss: 0.00004161
Iteration 91/1000 | Loss: 0.00004161
Iteration 92/1000 | Loss: 0.00004161
Iteration 93/1000 | Loss: 0.00004161
Iteration 94/1000 | Loss: 0.00004160
Iteration 95/1000 | Loss: 0.00004160
Iteration 96/1000 | Loss: 0.00004160
Iteration 97/1000 | Loss: 0.00004160
Iteration 98/1000 | Loss: 0.00004160
Iteration 99/1000 | Loss: 0.00004160
Iteration 100/1000 | Loss: 0.00004160
Iteration 101/1000 | Loss: 0.00004160
Iteration 102/1000 | Loss: 0.00004160
Iteration 103/1000 | Loss: 0.00004160
Iteration 104/1000 | Loss: 0.00004160
Iteration 105/1000 | Loss: 0.00004160
Iteration 106/1000 | Loss: 0.00004160
Iteration 107/1000 | Loss: 0.00004160
Iteration 108/1000 | Loss: 0.00004160
Iteration 109/1000 | Loss: 0.00004160
Iteration 110/1000 | Loss: 0.00004160
Iteration 111/1000 | Loss: 0.00004160
Iteration 112/1000 | Loss: 0.00004160
Iteration 113/1000 | Loss: 0.00004160
Iteration 114/1000 | Loss: 0.00004160
Iteration 115/1000 | Loss: 0.00004160
Iteration 116/1000 | Loss: 0.00004160
Iteration 117/1000 | Loss: 0.00004160
Iteration 118/1000 | Loss: 0.00004160
Iteration 119/1000 | Loss: 0.00004160
Iteration 120/1000 | Loss: 0.00004160
Iteration 121/1000 | Loss: 0.00004160
Iteration 122/1000 | Loss: 0.00004160
Iteration 123/1000 | Loss: 0.00004160
Iteration 124/1000 | Loss: 0.00004160
Iteration 125/1000 | Loss: 0.00004160
Iteration 126/1000 | Loss: 0.00004160
Iteration 127/1000 | Loss: 0.00004160
Iteration 128/1000 | Loss: 0.00004160
Iteration 129/1000 | Loss: 0.00004160
Iteration 130/1000 | Loss: 0.00004160
Iteration 131/1000 | Loss: 0.00004160
Iteration 132/1000 | Loss: 0.00004160
Iteration 133/1000 | Loss: 0.00004160
Iteration 134/1000 | Loss: 0.00004160
Iteration 135/1000 | Loss: 0.00004160
Iteration 136/1000 | Loss: 0.00004160
Iteration 137/1000 | Loss: 0.00004160
Iteration 138/1000 | Loss: 0.00004160
Iteration 139/1000 | Loss: 0.00004160
Iteration 140/1000 | Loss: 0.00004160
Iteration 141/1000 | Loss: 0.00004160
Iteration 142/1000 | Loss: 0.00004160
Iteration 143/1000 | Loss: 0.00004160
Iteration 144/1000 | Loss: 0.00004160
Iteration 145/1000 | Loss: 0.00004160
Iteration 146/1000 | Loss: 0.00004160
Iteration 147/1000 | Loss: 0.00004160
Iteration 148/1000 | Loss: 0.00004160
Iteration 149/1000 | Loss: 0.00004160
Iteration 150/1000 | Loss: 0.00004160
Iteration 151/1000 | Loss: 0.00004160
Iteration 152/1000 | Loss: 0.00004160
Iteration 153/1000 | Loss: 0.00004160
Iteration 154/1000 | Loss: 0.00004160
Iteration 155/1000 | Loss: 0.00004160
Iteration 156/1000 | Loss: 0.00004160
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [4.15969843743369e-05, 4.15969843743369e-05, 4.15969843743369e-05, 4.15969843743369e-05, 4.15969843743369e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.15969843743369e-05

Optimization complete. Final v2v error: 5.633169651031494 mm

Highest mean error: 6.290124416351318 mm for frame 38

Lowest mean error: 5.120962619781494 mm for frame 31

Saving results

Total time: 34.92564821243286
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_46_us_2175/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_46_us_2175/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_46_us_2175/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00925044
Iteration 2/25 | Loss: 0.00209965
Iteration 3/25 | Loss: 0.00186658
Iteration 4/25 | Loss: 0.00183606
Iteration 5/25 | Loss: 0.00182682
Iteration 6/25 | Loss: 0.00182526
Iteration 7/25 | Loss: 0.00182526
Iteration 8/25 | Loss: 0.00182526
Iteration 9/25 | Loss: 0.00182526
Iteration 10/25 | Loss: 0.00182526
Iteration 11/25 | Loss: 0.00182526
Iteration 12/25 | Loss: 0.00182526
Iteration 13/25 | Loss: 0.00182526
Iteration 14/25 | Loss: 0.00182526
Iteration 15/25 | Loss: 0.00182526
Iteration 16/25 | Loss: 0.00182526
Iteration 17/25 | Loss: 0.00182526
Iteration 18/25 | Loss: 0.00182526
Iteration 19/25 | Loss: 0.00182526
Iteration 20/25 | Loss: 0.00182526
Iteration 21/25 | Loss: 0.00182526
Iteration 22/25 | Loss: 0.00182526
Iteration 23/25 | Loss: 0.00182526
Iteration 24/25 | Loss: 0.00182526
Iteration 25/25 | Loss: 0.00182526

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53143656
Iteration 2/25 | Loss: 0.00286104
Iteration 3/25 | Loss: 0.00286104
Iteration 4/25 | Loss: 0.00286103
Iteration 5/25 | Loss: 0.00286103
Iteration 6/25 | Loss: 0.00286103
Iteration 7/25 | Loss: 0.00286103
Iteration 8/25 | Loss: 0.00286103
Iteration 9/25 | Loss: 0.00286103
Iteration 10/25 | Loss: 0.00286103
Iteration 11/25 | Loss: 0.00286103
Iteration 12/25 | Loss: 0.00286103
Iteration 13/25 | Loss: 0.00286103
Iteration 14/25 | Loss: 0.00286103
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.00286103249527514, 0.00286103249527514, 0.00286103249527514, 0.00286103249527514, 0.00286103249527514]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00286103249527514

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00286103
Iteration 2/1000 | Loss: 0.00008745
Iteration 3/1000 | Loss: 0.00005618
Iteration 4/1000 | Loss: 0.00004870
Iteration 5/1000 | Loss: 0.00004490
Iteration 6/1000 | Loss: 0.00004295
Iteration 7/1000 | Loss: 0.00004134
Iteration 8/1000 | Loss: 0.00004045
Iteration 9/1000 | Loss: 0.00003982
Iteration 10/1000 | Loss: 0.00003933
Iteration 11/1000 | Loss: 0.00003893
Iteration 12/1000 | Loss: 0.00003861
Iteration 13/1000 | Loss: 0.00003836
Iteration 14/1000 | Loss: 0.00003818
Iteration 15/1000 | Loss: 0.00003816
Iteration 16/1000 | Loss: 0.00003811
Iteration 17/1000 | Loss: 0.00003804
Iteration 18/1000 | Loss: 0.00003803
Iteration 19/1000 | Loss: 0.00003798
Iteration 20/1000 | Loss: 0.00003798
Iteration 21/1000 | Loss: 0.00003795
Iteration 22/1000 | Loss: 0.00003794
Iteration 23/1000 | Loss: 0.00003794
Iteration 24/1000 | Loss: 0.00003791
Iteration 25/1000 | Loss: 0.00003789
Iteration 26/1000 | Loss: 0.00003788
Iteration 27/1000 | Loss: 0.00003787
Iteration 28/1000 | Loss: 0.00003787
Iteration 29/1000 | Loss: 0.00003787
Iteration 30/1000 | Loss: 0.00003786
Iteration 31/1000 | Loss: 0.00003786
Iteration 32/1000 | Loss: 0.00003785
Iteration 33/1000 | Loss: 0.00003785
Iteration 34/1000 | Loss: 0.00003784
Iteration 35/1000 | Loss: 0.00003784
Iteration 36/1000 | Loss: 0.00003784
Iteration 37/1000 | Loss: 0.00003783
Iteration 38/1000 | Loss: 0.00003783
Iteration 39/1000 | Loss: 0.00003783
Iteration 40/1000 | Loss: 0.00003782
Iteration 41/1000 | Loss: 0.00003782
Iteration 42/1000 | Loss: 0.00003782
Iteration 43/1000 | Loss: 0.00003781
Iteration 44/1000 | Loss: 0.00003781
Iteration 45/1000 | Loss: 0.00003780
Iteration 46/1000 | Loss: 0.00003780
Iteration 47/1000 | Loss: 0.00003779
Iteration 48/1000 | Loss: 0.00003779
Iteration 49/1000 | Loss: 0.00003779
Iteration 50/1000 | Loss: 0.00003779
Iteration 51/1000 | Loss: 0.00003778
Iteration 52/1000 | Loss: 0.00003778
Iteration 53/1000 | Loss: 0.00003778
Iteration 54/1000 | Loss: 0.00003777
Iteration 55/1000 | Loss: 0.00003777
Iteration 56/1000 | Loss: 0.00003777
Iteration 57/1000 | Loss: 0.00003776
Iteration 58/1000 | Loss: 0.00003776
Iteration 59/1000 | Loss: 0.00003776
Iteration 60/1000 | Loss: 0.00003776
Iteration 61/1000 | Loss: 0.00003775
Iteration 62/1000 | Loss: 0.00003775
Iteration 63/1000 | Loss: 0.00003775
Iteration 64/1000 | Loss: 0.00003775
Iteration 65/1000 | Loss: 0.00003775
Iteration 66/1000 | Loss: 0.00003774
Iteration 67/1000 | Loss: 0.00003774
Iteration 68/1000 | Loss: 0.00003774
Iteration 69/1000 | Loss: 0.00003774
Iteration 70/1000 | Loss: 0.00003773
Iteration 71/1000 | Loss: 0.00003773
Iteration 72/1000 | Loss: 0.00003773
Iteration 73/1000 | Loss: 0.00003773
Iteration 74/1000 | Loss: 0.00003773
Iteration 75/1000 | Loss: 0.00003773
Iteration 76/1000 | Loss: 0.00003773
Iteration 77/1000 | Loss: 0.00003773
Iteration 78/1000 | Loss: 0.00003773
Iteration 79/1000 | Loss: 0.00003773
Iteration 80/1000 | Loss: 0.00003773
Iteration 81/1000 | Loss: 0.00003772
Iteration 82/1000 | Loss: 0.00003772
Iteration 83/1000 | Loss: 0.00003772
Iteration 84/1000 | Loss: 0.00003772
Iteration 85/1000 | Loss: 0.00003772
Iteration 86/1000 | Loss: 0.00003772
Iteration 87/1000 | Loss: 0.00003772
Iteration 88/1000 | Loss: 0.00003772
Iteration 89/1000 | Loss: 0.00003772
Iteration 90/1000 | Loss: 0.00003772
Iteration 91/1000 | Loss: 0.00003772
Iteration 92/1000 | Loss: 0.00003772
Iteration 93/1000 | Loss: 0.00003772
Iteration 94/1000 | Loss: 0.00003772
Iteration 95/1000 | Loss: 0.00003772
Iteration 96/1000 | Loss: 0.00003772
Iteration 97/1000 | Loss: 0.00003772
Iteration 98/1000 | Loss: 0.00003772
Iteration 99/1000 | Loss: 0.00003772
Iteration 100/1000 | Loss: 0.00003772
Iteration 101/1000 | Loss: 0.00003772
Iteration 102/1000 | Loss: 0.00003772
Iteration 103/1000 | Loss: 0.00003772
Iteration 104/1000 | Loss: 0.00003772
Iteration 105/1000 | Loss: 0.00003772
Iteration 106/1000 | Loss: 0.00003772
Iteration 107/1000 | Loss: 0.00003772
Iteration 108/1000 | Loss: 0.00003772
Iteration 109/1000 | Loss: 0.00003772
Iteration 110/1000 | Loss: 0.00003772
Iteration 111/1000 | Loss: 0.00003772
Iteration 112/1000 | Loss: 0.00003772
Iteration 113/1000 | Loss: 0.00003772
Iteration 114/1000 | Loss: 0.00003772
Iteration 115/1000 | Loss: 0.00003772
Iteration 116/1000 | Loss: 0.00003772
Iteration 117/1000 | Loss: 0.00003772
Iteration 118/1000 | Loss: 0.00003772
Iteration 119/1000 | Loss: 0.00003772
Iteration 120/1000 | Loss: 0.00003772
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [3.772232594201341e-05, 3.772232594201341e-05, 3.772232594201341e-05, 3.772232594201341e-05, 3.772232594201341e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.772232594201341e-05

Optimization complete. Final v2v error: 5.439645767211914 mm

Highest mean error: 6.132079601287842 mm for frame 187

Lowest mean error: 4.945268154144287 mm for frame 113

Saving results

Total time: 44.10060787200928
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_46_us_2175/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_46_us_2175/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_46_us_2175/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00923340
Iteration 2/25 | Loss: 0.00208011
Iteration 3/25 | Loss: 0.00186038
Iteration 4/25 | Loss: 0.00183158
Iteration 5/25 | Loss: 0.00182602
Iteration 6/25 | Loss: 0.00182602
Iteration 7/25 | Loss: 0.00182602
Iteration 8/25 | Loss: 0.00182602
Iteration 9/25 | Loss: 0.00182602
Iteration 10/25 | Loss: 0.00182602
Iteration 11/25 | Loss: 0.00182602
Iteration 12/25 | Loss: 0.00182602
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0018260169308632612, 0.0018260169308632612, 0.0018260169308632612, 0.0018260169308632612, 0.0018260169308632612]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018260169308632612

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49734461
Iteration 2/25 | Loss: 0.00260851
Iteration 3/25 | Loss: 0.00260850
Iteration 4/25 | Loss: 0.00260850
Iteration 5/25 | Loss: 0.00260850
Iteration 6/25 | Loss: 0.00260850
Iteration 7/25 | Loss: 0.00260850
Iteration 8/25 | Loss: 0.00260850
Iteration 9/25 | Loss: 0.00260850
Iteration 10/25 | Loss: 0.00260850
Iteration 11/25 | Loss: 0.00260850
Iteration 12/25 | Loss: 0.00260850
Iteration 13/25 | Loss: 0.00260850
Iteration 14/25 | Loss: 0.00260850
Iteration 15/25 | Loss: 0.00260850
Iteration 16/25 | Loss: 0.00260850
Iteration 17/25 | Loss: 0.00260850
Iteration 18/25 | Loss: 0.00260850
Iteration 19/25 | Loss: 0.00260850
Iteration 20/25 | Loss: 0.00260850
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.002608498325571418, 0.002608498325571418, 0.002608498325571418, 0.002608498325571418, 0.002608498325571418]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002608498325571418

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00260850
Iteration 2/1000 | Loss: 0.00007408
Iteration 3/1000 | Loss: 0.00005537
Iteration 4/1000 | Loss: 0.00004878
Iteration 5/1000 | Loss: 0.00004543
Iteration 6/1000 | Loss: 0.00004306
Iteration 7/1000 | Loss: 0.00004207
Iteration 8/1000 | Loss: 0.00004151
Iteration 9/1000 | Loss: 0.00004097
Iteration 10/1000 | Loss: 0.00004049
Iteration 11/1000 | Loss: 0.00004018
Iteration 12/1000 | Loss: 0.00004011
Iteration 13/1000 | Loss: 0.00004010
Iteration 14/1000 | Loss: 0.00004009
Iteration 15/1000 | Loss: 0.00004008
Iteration 16/1000 | Loss: 0.00004005
Iteration 17/1000 | Loss: 0.00004004
Iteration 18/1000 | Loss: 0.00004004
Iteration 19/1000 | Loss: 0.00004004
Iteration 20/1000 | Loss: 0.00004003
Iteration 21/1000 | Loss: 0.00004001
Iteration 22/1000 | Loss: 0.00003999
Iteration 23/1000 | Loss: 0.00003996
Iteration 24/1000 | Loss: 0.00003994
Iteration 25/1000 | Loss: 0.00003993
Iteration 26/1000 | Loss: 0.00003992
Iteration 27/1000 | Loss: 0.00003991
Iteration 28/1000 | Loss: 0.00003991
Iteration 29/1000 | Loss: 0.00003990
Iteration 30/1000 | Loss: 0.00003989
Iteration 31/1000 | Loss: 0.00003983
Iteration 32/1000 | Loss: 0.00003981
Iteration 33/1000 | Loss: 0.00003981
Iteration 34/1000 | Loss: 0.00003979
Iteration 35/1000 | Loss: 0.00003979
Iteration 36/1000 | Loss: 0.00003979
Iteration 37/1000 | Loss: 0.00003979
Iteration 38/1000 | Loss: 0.00003978
Iteration 39/1000 | Loss: 0.00003978
Iteration 40/1000 | Loss: 0.00003978
Iteration 41/1000 | Loss: 0.00003978
Iteration 42/1000 | Loss: 0.00003978
Iteration 43/1000 | Loss: 0.00003978
Iteration 44/1000 | Loss: 0.00003977
Iteration 45/1000 | Loss: 0.00003977
Iteration 46/1000 | Loss: 0.00003976
Iteration 47/1000 | Loss: 0.00003976
Iteration 48/1000 | Loss: 0.00003976
Iteration 49/1000 | Loss: 0.00003976
Iteration 50/1000 | Loss: 0.00003976
Iteration 51/1000 | Loss: 0.00003975
Iteration 52/1000 | Loss: 0.00003975
Iteration 53/1000 | Loss: 0.00003975
Iteration 54/1000 | Loss: 0.00003975
Iteration 55/1000 | Loss: 0.00003975
Iteration 56/1000 | Loss: 0.00003975
Iteration 57/1000 | Loss: 0.00003975
Iteration 58/1000 | Loss: 0.00003974
Iteration 59/1000 | Loss: 0.00003974
Iteration 60/1000 | Loss: 0.00003974
Iteration 61/1000 | Loss: 0.00003974
Iteration 62/1000 | Loss: 0.00003974
Iteration 63/1000 | Loss: 0.00003974
Iteration 64/1000 | Loss: 0.00003974
Iteration 65/1000 | Loss: 0.00003973
Iteration 66/1000 | Loss: 0.00003973
Iteration 67/1000 | Loss: 0.00003973
Iteration 68/1000 | Loss: 0.00003973
Iteration 69/1000 | Loss: 0.00003973
Iteration 70/1000 | Loss: 0.00003973
Iteration 71/1000 | Loss: 0.00003973
Iteration 72/1000 | Loss: 0.00003973
Iteration 73/1000 | Loss: 0.00003973
Iteration 74/1000 | Loss: 0.00003973
Iteration 75/1000 | Loss: 0.00003973
Iteration 76/1000 | Loss: 0.00003973
Iteration 77/1000 | Loss: 0.00003973
Iteration 78/1000 | Loss: 0.00003973
Iteration 79/1000 | Loss: 0.00003973
Iteration 80/1000 | Loss: 0.00003973
Iteration 81/1000 | Loss: 0.00003973
Iteration 82/1000 | Loss: 0.00003973
Iteration 83/1000 | Loss: 0.00003973
Iteration 84/1000 | Loss: 0.00003973
Iteration 85/1000 | Loss: 0.00003973
Iteration 86/1000 | Loss: 0.00003973
Iteration 87/1000 | Loss: 0.00003973
Iteration 88/1000 | Loss: 0.00003973
Iteration 89/1000 | Loss: 0.00003973
Iteration 90/1000 | Loss: 0.00003973
Iteration 91/1000 | Loss: 0.00003973
Iteration 92/1000 | Loss: 0.00003973
Iteration 93/1000 | Loss: 0.00003973
Iteration 94/1000 | Loss: 0.00003973
Iteration 95/1000 | Loss: 0.00003973
Iteration 96/1000 | Loss: 0.00003973
Iteration 97/1000 | Loss: 0.00003973
Iteration 98/1000 | Loss: 0.00003973
Iteration 99/1000 | Loss: 0.00003973
Iteration 100/1000 | Loss: 0.00003973
Iteration 101/1000 | Loss: 0.00003973
Iteration 102/1000 | Loss: 0.00003973
Iteration 103/1000 | Loss: 0.00003973
Iteration 104/1000 | Loss: 0.00003973
Iteration 105/1000 | Loss: 0.00003973
Iteration 106/1000 | Loss: 0.00003973
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [3.973213824792765e-05, 3.973213824792765e-05, 3.973213824792765e-05, 3.973213824792765e-05, 3.973213824792765e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.973213824792765e-05

Optimization complete. Final v2v error: 5.516278266906738 mm

Highest mean error: 5.926173210144043 mm for frame 157

Lowest mean error: 5.216089725494385 mm for frame 55

Saving results

Total time: 35.623493909835815
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_46_us_2175/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_46_us_2175/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_46_us_2175/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00491469
Iteration 2/25 | Loss: 0.00182509
Iteration 3/25 | Loss: 0.00174084
Iteration 4/25 | Loss: 0.00173112
Iteration 5/25 | Loss: 0.00172616
Iteration 6/25 | Loss: 0.00172501
Iteration 7/25 | Loss: 0.00172501
Iteration 8/25 | Loss: 0.00172501
Iteration 9/25 | Loss: 0.00172501
Iteration 10/25 | Loss: 0.00172501
Iteration 11/25 | Loss: 0.00172501
Iteration 12/25 | Loss: 0.00172501
Iteration 13/25 | Loss: 0.00172501
Iteration 14/25 | Loss: 0.00172501
Iteration 15/25 | Loss: 0.00172501
Iteration 16/25 | Loss: 0.00172501
Iteration 17/25 | Loss: 0.00172501
Iteration 18/25 | Loss: 0.00172501
Iteration 19/25 | Loss: 0.00172501
Iteration 20/25 | Loss: 0.00172501
Iteration 21/25 | Loss: 0.00172501
Iteration 22/25 | Loss: 0.00172501
Iteration 23/25 | Loss: 0.00172501
Iteration 24/25 | Loss: 0.00172501
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.001725005335174501, 0.001725005335174501, 0.001725005335174501, 0.001725005335174501, 0.001725005335174501]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001725005335174501

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49939311
Iteration 2/25 | Loss: 0.00215687
Iteration 3/25 | Loss: 0.00215687
Iteration 4/25 | Loss: 0.00215687
Iteration 5/25 | Loss: 0.00215687
Iteration 6/25 | Loss: 0.00215687
Iteration 7/25 | Loss: 0.00215687
Iteration 8/25 | Loss: 0.00215687
Iteration 9/25 | Loss: 0.00215687
Iteration 10/25 | Loss: 0.00215687
Iteration 11/25 | Loss: 0.00215687
Iteration 12/25 | Loss: 0.00215687
Iteration 13/25 | Loss: 0.00215687
Iteration 14/25 | Loss: 0.00215687
Iteration 15/25 | Loss: 0.00215687
Iteration 16/25 | Loss: 0.00215687
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0021568681113421917, 0.0021568681113421917, 0.0021568681113421917, 0.0021568681113421917, 0.0021568681113421917]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021568681113421917

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00215687
Iteration 2/1000 | Loss: 0.00005058
Iteration 3/1000 | Loss: 0.00004000
Iteration 4/1000 | Loss: 0.00003555
Iteration 5/1000 | Loss: 0.00003361
Iteration 6/1000 | Loss: 0.00003241
Iteration 7/1000 | Loss: 0.00003194
Iteration 8/1000 | Loss: 0.00003155
Iteration 9/1000 | Loss: 0.00003119
Iteration 10/1000 | Loss: 0.00003090
Iteration 11/1000 | Loss: 0.00003078
Iteration 12/1000 | Loss: 0.00003070
Iteration 13/1000 | Loss: 0.00003064
Iteration 14/1000 | Loss: 0.00003064
Iteration 15/1000 | Loss: 0.00003064
Iteration 16/1000 | Loss: 0.00003064
Iteration 17/1000 | Loss: 0.00003064
Iteration 18/1000 | Loss: 0.00003064
Iteration 19/1000 | Loss: 0.00003064
Iteration 20/1000 | Loss: 0.00003064
Iteration 21/1000 | Loss: 0.00003061
Iteration 22/1000 | Loss: 0.00003060
Iteration 23/1000 | Loss: 0.00003060
Iteration 24/1000 | Loss: 0.00003059
Iteration 25/1000 | Loss: 0.00003059
Iteration 26/1000 | Loss: 0.00003059
Iteration 27/1000 | Loss: 0.00003058
Iteration 28/1000 | Loss: 0.00003058
Iteration 29/1000 | Loss: 0.00003058
Iteration 30/1000 | Loss: 0.00003058
Iteration 31/1000 | Loss: 0.00003057
Iteration 32/1000 | Loss: 0.00003057
Iteration 33/1000 | Loss: 0.00003057
Iteration 34/1000 | Loss: 0.00003057
Iteration 35/1000 | Loss: 0.00003057
Iteration 36/1000 | Loss: 0.00003057
Iteration 37/1000 | Loss: 0.00003056
Iteration 38/1000 | Loss: 0.00003056
Iteration 39/1000 | Loss: 0.00003055
Iteration 40/1000 | Loss: 0.00003055
Iteration 41/1000 | Loss: 0.00003055
Iteration 42/1000 | Loss: 0.00003055
Iteration 43/1000 | Loss: 0.00003055
Iteration 44/1000 | Loss: 0.00003055
Iteration 45/1000 | Loss: 0.00003054
Iteration 46/1000 | Loss: 0.00003054
Iteration 47/1000 | Loss: 0.00003054
Iteration 48/1000 | Loss: 0.00003054
Iteration 49/1000 | Loss: 0.00003054
Iteration 50/1000 | Loss: 0.00003054
Iteration 51/1000 | Loss: 0.00003054
Iteration 52/1000 | Loss: 0.00003054
Iteration 53/1000 | Loss: 0.00003054
Iteration 54/1000 | Loss: 0.00003053
Iteration 55/1000 | Loss: 0.00003053
Iteration 56/1000 | Loss: 0.00003053
Iteration 57/1000 | Loss: 0.00003053
Iteration 58/1000 | Loss: 0.00003053
Iteration 59/1000 | Loss: 0.00003053
Iteration 60/1000 | Loss: 0.00003053
Iteration 61/1000 | Loss: 0.00003053
Iteration 62/1000 | Loss: 0.00003052
Iteration 63/1000 | Loss: 0.00003052
Iteration 64/1000 | Loss: 0.00003052
Iteration 65/1000 | Loss: 0.00003051
Iteration 66/1000 | Loss: 0.00003051
Iteration 67/1000 | Loss: 0.00003051
Iteration 68/1000 | Loss: 0.00003051
Iteration 69/1000 | Loss: 0.00003051
Iteration 70/1000 | Loss: 0.00003051
Iteration 71/1000 | Loss: 0.00003051
Iteration 72/1000 | Loss: 0.00003051
Iteration 73/1000 | Loss: 0.00003051
Iteration 74/1000 | Loss: 0.00003051
Iteration 75/1000 | Loss: 0.00003051
Iteration 76/1000 | Loss: 0.00003050
Iteration 77/1000 | Loss: 0.00003050
Iteration 78/1000 | Loss: 0.00003050
Iteration 79/1000 | Loss: 0.00003050
Iteration 80/1000 | Loss: 0.00003050
Iteration 81/1000 | Loss: 0.00003049
Iteration 82/1000 | Loss: 0.00003049
Iteration 83/1000 | Loss: 0.00003049
Iteration 84/1000 | Loss: 0.00003049
Iteration 85/1000 | Loss: 0.00003049
Iteration 86/1000 | Loss: 0.00003049
Iteration 87/1000 | Loss: 0.00003049
Iteration 88/1000 | Loss: 0.00003048
Iteration 89/1000 | Loss: 0.00003048
Iteration 90/1000 | Loss: 0.00003048
Iteration 91/1000 | Loss: 0.00003048
Iteration 92/1000 | Loss: 0.00003048
Iteration 93/1000 | Loss: 0.00003048
Iteration 94/1000 | Loss: 0.00003048
Iteration 95/1000 | Loss: 0.00003048
Iteration 96/1000 | Loss: 0.00003048
Iteration 97/1000 | Loss: 0.00003047
Iteration 98/1000 | Loss: 0.00003047
Iteration 99/1000 | Loss: 0.00003047
Iteration 100/1000 | Loss: 0.00003047
Iteration 101/1000 | Loss: 0.00003047
Iteration 102/1000 | Loss: 0.00003047
Iteration 103/1000 | Loss: 0.00003047
Iteration 104/1000 | Loss: 0.00003047
Iteration 105/1000 | Loss: 0.00003047
Iteration 106/1000 | Loss: 0.00003046
Iteration 107/1000 | Loss: 0.00003046
Iteration 108/1000 | Loss: 0.00003046
Iteration 109/1000 | Loss: 0.00003046
Iteration 110/1000 | Loss: 0.00003046
Iteration 111/1000 | Loss: 0.00003045
Iteration 112/1000 | Loss: 0.00003045
Iteration 113/1000 | Loss: 0.00003045
Iteration 114/1000 | Loss: 0.00003045
Iteration 115/1000 | Loss: 0.00003045
Iteration 116/1000 | Loss: 0.00003045
Iteration 117/1000 | Loss: 0.00003045
Iteration 118/1000 | Loss: 0.00003045
Iteration 119/1000 | Loss: 0.00003045
Iteration 120/1000 | Loss: 0.00003045
Iteration 121/1000 | Loss: 0.00003045
Iteration 122/1000 | Loss: 0.00003044
Iteration 123/1000 | Loss: 0.00003044
Iteration 124/1000 | Loss: 0.00003044
Iteration 125/1000 | Loss: 0.00003044
Iteration 126/1000 | Loss: 0.00003044
Iteration 127/1000 | Loss: 0.00003044
Iteration 128/1000 | Loss: 0.00003044
Iteration 129/1000 | Loss: 0.00003044
Iteration 130/1000 | Loss: 0.00003044
Iteration 131/1000 | Loss: 0.00003043
Iteration 132/1000 | Loss: 0.00003043
Iteration 133/1000 | Loss: 0.00003043
Iteration 134/1000 | Loss: 0.00003043
Iteration 135/1000 | Loss: 0.00003043
Iteration 136/1000 | Loss: 0.00003043
Iteration 137/1000 | Loss: 0.00003043
Iteration 138/1000 | Loss: 0.00003043
Iteration 139/1000 | Loss: 0.00003043
Iteration 140/1000 | Loss: 0.00003043
Iteration 141/1000 | Loss: 0.00003043
Iteration 142/1000 | Loss: 0.00003043
Iteration 143/1000 | Loss: 0.00003043
Iteration 144/1000 | Loss: 0.00003043
Iteration 145/1000 | Loss: 0.00003043
Iteration 146/1000 | Loss: 0.00003043
Iteration 147/1000 | Loss: 0.00003043
Iteration 148/1000 | Loss: 0.00003043
Iteration 149/1000 | Loss: 0.00003043
Iteration 150/1000 | Loss: 0.00003043
Iteration 151/1000 | Loss: 0.00003043
Iteration 152/1000 | Loss: 0.00003043
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [3.0433659048867412e-05, 3.0433659048867412e-05, 3.0433659048867412e-05, 3.0433659048867412e-05, 3.0433659048867412e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0433659048867412e-05

Optimization complete. Final v2v error: 4.8703532218933105 mm

Highest mean error: 5.150825500488281 mm for frame 108

Lowest mean error: 4.677097320556641 mm for frame 25

Saving results

Total time: 34.04562783241272
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_46_us_2175/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_46_us_2175/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_46_us_2175/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00621540
Iteration 2/25 | Loss: 0.00203766
Iteration 3/25 | Loss: 0.00183889
Iteration 4/25 | Loss: 0.00181186
Iteration 5/25 | Loss: 0.00180234
Iteration 6/25 | Loss: 0.00180110
Iteration 7/25 | Loss: 0.00180110
Iteration 8/25 | Loss: 0.00180110
Iteration 9/25 | Loss: 0.00180110
Iteration 10/25 | Loss: 0.00180110
Iteration 11/25 | Loss: 0.00180110
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0018010953208431602, 0.0018010953208431602, 0.0018010953208431602, 0.0018010953208431602, 0.0018010953208431602]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018010953208431602

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.69633412
Iteration 2/25 | Loss: 0.00226588
Iteration 3/25 | Loss: 0.00226587
Iteration 4/25 | Loss: 0.00226587
Iteration 5/25 | Loss: 0.00226587
Iteration 6/25 | Loss: 0.00226587
Iteration 7/25 | Loss: 0.00226587
Iteration 8/25 | Loss: 0.00226587
Iteration 9/25 | Loss: 0.00226587
Iteration 10/25 | Loss: 0.00226587
Iteration 11/25 | Loss: 0.00226587
Iteration 12/25 | Loss: 0.00226587
Iteration 13/25 | Loss: 0.00226587
Iteration 14/25 | Loss: 0.00226587
Iteration 15/25 | Loss: 0.00226587
Iteration 16/25 | Loss: 0.00226587
Iteration 17/25 | Loss: 0.00226587
Iteration 18/25 | Loss: 0.00226587
Iteration 19/25 | Loss: 0.00226587
Iteration 20/25 | Loss: 0.00226587
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0022658687084913254, 0.0022658687084913254, 0.0022658687084913254, 0.0022658687084913254, 0.0022658687084913254]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022658687084913254

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00226587
Iteration 2/1000 | Loss: 0.00006976
Iteration 3/1000 | Loss: 0.00005199
Iteration 4/1000 | Loss: 0.00004609
Iteration 5/1000 | Loss: 0.00004299
Iteration 6/1000 | Loss: 0.00004119
Iteration 7/1000 | Loss: 0.00004002
Iteration 8/1000 | Loss: 0.00003936
Iteration 9/1000 | Loss: 0.00003872
Iteration 10/1000 | Loss: 0.00003817
Iteration 11/1000 | Loss: 0.00003782
Iteration 12/1000 | Loss: 0.00003765
Iteration 13/1000 | Loss: 0.00003761
Iteration 14/1000 | Loss: 0.00003755
Iteration 15/1000 | Loss: 0.00003751
Iteration 16/1000 | Loss: 0.00003750
Iteration 17/1000 | Loss: 0.00003743
Iteration 18/1000 | Loss: 0.00003743
Iteration 19/1000 | Loss: 0.00003734
Iteration 20/1000 | Loss: 0.00003734
Iteration 21/1000 | Loss: 0.00003734
Iteration 22/1000 | Loss: 0.00003732
Iteration 23/1000 | Loss: 0.00003732
Iteration 24/1000 | Loss: 0.00003731
Iteration 25/1000 | Loss: 0.00003731
Iteration 26/1000 | Loss: 0.00003731
Iteration 27/1000 | Loss: 0.00003731
Iteration 28/1000 | Loss: 0.00003731
Iteration 29/1000 | Loss: 0.00003731
Iteration 30/1000 | Loss: 0.00003731
Iteration 31/1000 | Loss: 0.00003731
Iteration 32/1000 | Loss: 0.00003731
Iteration 33/1000 | Loss: 0.00003730
Iteration 34/1000 | Loss: 0.00003730
Iteration 35/1000 | Loss: 0.00003730
Iteration 36/1000 | Loss: 0.00003730
Iteration 37/1000 | Loss: 0.00003728
Iteration 38/1000 | Loss: 0.00003728
Iteration 39/1000 | Loss: 0.00003728
Iteration 40/1000 | Loss: 0.00003728
Iteration 41/1000 | Loss: 0.00003728
Iteration 42/1000 | Loss: 0.00003728
Iteration 43/1000 | Loss: 0.00003728
Iteration 44/1000 | Loss: 0.00003728
Iteration 45/1000 | Loss: 0.00003727
Iteration 46/1000 | Loss: 0.00003727
Iteration 47/1000 | Loss: 0.00003727
Iteration 48/1000 | Loss: 0.00003727
Iteration 49/1000 | Loss: 0.00003727
Iteration 50/1000 | Loss: 0.00003726
Iteration 51/1000 | Loss: 0.00003726
Iteration 52/1000 | Loss: 0.00003726
Iteration 53/1000 | Loss: 0.00003726
Iteration 54/1000 | Loss: 0.00003726
Iteration 55/1000 | Loss: 0.00003725
Iteration 56/1000 | Loss: 0.00003725
Iteration 57/1000 | Loss: 0.00003725
Iteration 58/1000 | Loss: 0.00003725
Iteration 59/1000 | Loss: 0.00003725
Iteration 60/1000 | Loss: 0.00003725
Iteration 61/1000 | Loss: 0.00003725
Iteration 62/1000 | Loss: 0.00003725
Iteration 63/1000 | Loss: 0.00003725
Iteration 64/1000 | Loss: 0.00003725
Iteration 65/1000 | Loss: 0.00003724
Iteration 66/1000 | Loss: 0.00003724
Iteration 67/1000 | Loss: 0.00003724
Iteration 68/1000 | Loss: 0.00003724
Iteration 69/1000 | Loss: 0.00003724
Iteration 70/1000 | Loss: 0.00003724
Iteration 71/1000 | Loss: 0.00003724
Iteration 72/1000 | Loss: 0.00003724
Iteration 73/1000 | Loss: 0.00003723
Iteration 74/1000 | Loss: 0.00003723
Iteration 75/1000 | Loss: 0.00003723
Iteration 76/1000 | Loss: 0.00003723
Iteration 77/1000 | Loss: 0.00003723
Iteration 78/1000 | Loss: 0.00003723
Iteration 79/1000 | Loss: 0.00003723
Iteration 80/1000 | Loss: 0.00003723
Iteration 81/1000 | Loss: 0.00003723
Iteration 82/1000 | Loss: 0.00003723
Iteration 83/1000 | Loss: 0.00003723
Iteration 84/1000 | Loss: 0.00003722
Iteration 85/1000 | Loss: 0.00003722
Iteration 86/1000 | Loss: 0.00003722
Iteration 87/1000 | Loss: 0.00003722
Iteration 88/1000 | Loss: 0.00003722
Iteration 89/1000 | Loss: 0.00003722
Iteration 90/1000 | Loss: 0.00003721
Iteration 91/1000 | Loss: 0.00003721
Iteration 92/1000 | Loss: 0.00003721
Iteration 93/1000 | Loss: 0.00003721
Iteration 94/1000 | Loss: 0.00003721
Iteration 95/1000 | Loss: 0.00003721
Iteration 96/1000 | Loss: 0.00003721
Iteration 97/1000 | Loss: 0.00003721
Iteration 98/1000 | Loss: 0.00003721
Iteration 99/1000 | Loss: 0.00003721
Iteration 100/1000 | Loss: 0.00003721
Iteration 101/1000 | Loss: 0.00003721
Iteration 102/1000 | Loss: 0.00003721
Iteration 103/1000 | Loss: 0.00003721
Iteration 104/1000 | Loss: 0.00003721
Iteration 105/1000 | Loss: 0.00003721
Iteration 106/1000 | Loss: 0.00003721
Iteration 107/1000 | Loss: 0.00003721
Iteration 108/1000 | Loss: 0.00003721
Iteration 109/1000 | Loss: 0.00003721
Iteration 110/1000 | Loss: 0.00003721
Iteration 111/1000 | Loss: 0.00003721
Iteration 112/1000 | Loss: 0.00003721
Iteration 113/1000 | Loss: 0.00003721
Iteration 114/1000 | Loss: 0.00003721
Iteration 115/1000 | Loss: 0.00003721
Iteration 116/1000 | Loss: 0.00003721
Iteration 117/1000 | Loss: 0.00003721
Iteration 118/1000 | Loss: 0.00003721
Iteration 119/1000 | Loss: 0.00003721
Iteration 120/1000 | Loss: 0.00003721
Iteration 121/1000 | Loss: 0.00003721
Iteration 122/1000 | Loss: 0.00003721
Iteration 123/1000 | Loss: 0.00003721
Iteration 124/1000 | Loss: 0.00003721
Iteration 125/1000 | Loss: 0.00003721
Iteration 126/1000 | Loss: 0.00003721
Iteration 127/1000 | Loss: 0.00003721
Iteration 128/1000 | Loss: 0.00003721
Iteration 129/1000 | Loss: 0.00003721
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [3.7209614674793556e-05, 3.7209614674793556e-05, 3.7209614674793556e-05, 3.7209614674793556e-05, 3.7209614674793556e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.7209614674793556e-05

Optimization complete. Final v2v error: 5.391971111297607 mm

Highest mean error: 5.772293567657471 mm for frame 172

Lowest mean error: 5.125940799713135 mm for frame 225

Saving results

Total time: 40.78093409538269
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_46_us_2175/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_46_us_2175/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_46_us_2175/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00935560
Iteration 2/25 | Loss: 0.00182011
Iteration 3/25 | Loss: 0.00174516
Iteration 4/25 | Loss: 0.00173234
Iteration 5/25 | Loss: 0.00172839
Iteration 6/25 | Loss: 0.00172753
Iteration 7/25 | Loss: 0.00172753
Iteration 8/25 | Loss: 0.00172753
Iteration 9/25 | Loss: 0.00172753
Iteration 10/25 | Loss: 0.00172753
Iteration 11/25 | Loss: 0.00172753
Iteration 12/25 | Loss: 0.00172753
Iteration 13/25 | Loss: 0.00172753
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0017275268910452724, 0.0017275268910452724, 0.0017275268910452724, 0.0017275268910452724, 0.0017275268910452724]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017275268910452724

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49730992
Iteration 2/25 | Loss: 0.00216786
Iteration 3/25 | Loss: 0.00216786
Iteration 4/25 | Loss: 0.00216786
Iteration 5/25 | Loss: 0.00216786
Iteration 6/25 | Loss: 0.00216786
Iteration 7/25 | Loss: 0.00216786
Iteration 8/25 | Loss: 0.00216786
Iteration 9/25 | Loss: 0.00216786
Iteration 10/25 | Loss: 0.00216786
Iteration 11/25 | Loss: 0.00216786
Iteration 12/25 | Loss: 0.00216786
Iteration 13/25 | Loss: 0.00216786
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0021678581833839417, 0.0021678581833839417, 0.0021678581833839417, 0.0021678581833839417, 0.0021678581833839417]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021678581833839417

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00216786
Iteration 2/1000 | Loss: 0.00005040
Iteration 3/1000 | Loss: 0.00003726
Iteration 4/1000 | Loss: 0.00003354
Iteration 5/1000 | Loss: 0.00003173
Iteration 6/1000 | Loss: 0.00003077
Iteration 7/1000 | Loss: 0.00003011
Iteration 8/1000 | Loss: 0.00002969
Iteration 9/1000 | Loss: 0.00002940
Iteration 10/1000 | Loss: 0.00002903
Iteration 11/1000 | Loss: 0.00002878
Iteration 12/1000 | Loss: 0.00002859
Iteration 13/1000 | Loss: 0.00002849
Iteration 14/1000 | Loss: 0.00002845
Iteration 15/1000 | Loss: 0.00002845
Iteration 16/1000 | Loss: 0.00002845
Iteration 17/1000 | Loss: 0.00002845
Iteration 18/1000 | Loss: 0.00002845
Iteration 19/1000 | Loss: 0.00002845
Iteration 20/1000 | Loss: 0.00002845
Iteration 21/1000 | Loss: 0.00002845
Iteration 22/1000 | Loss: 0.00002845
Iteration 23/1000 | Loss: 0.00002845
Iteration 24/1000 | Loss: 0.00002845
Iteration 25/1000 | Loss: 0.00002844
Iteration 26/1000 | Loss: 0.00002844
Iteration 27/1000 | Loss: 0.00002844
Iteration 28/1000 | Loss: 0.00002840
Iteration 29/1000 | Loss: 0.00002840
Iteration 30/1000 | Loss: 0.00002840
Iteration 31/1000 | Loss: 0.00002832
Iteration 32/1000 | Loss: 0.00002832
Iteration 33/1000 | Loss: 0.00002830
Iteration 34/1000 | Loss: 0.00002829
Iteration 35/1000 | Loss: 0.00002829
Iteration 36/1000 | Loss: 0.00002828
Iteration 37/1000 | Loss: 0.00002827
Iteration 38/1000 | Loss: 0.00002827
Iteration 39/1000 | Loss: 0.00002826
Iteration 40/1000 | Loss: 0.00002826
Iteration 41/1000 | Loss: 0.00002826
Iteration 42/1000 | Loss: 0.00002826
Iteration 43/1000 | Loss: 0.00002824
Iteration 44/1000 | Loss: 0.00002824
Iteration 45/1000 | Loss: 0.00002824
Iteration 46/1000 | Loss: 0.00002824
Iteration 47/1000 | Loss: 0.00002824
Iteration 48/1000 | Loss: 0.00002824
Iteration 49/1000 | Loss: 0.00002823
Iteration 50/1000 | Loss: 0.00002823
Iteration 51/1000 | Loss: 0.00002823
Iteration 52/1000 | Loss: 0.00002823
Iteration 53/1000 | Loss: 0.00002823
Iteration 54/1000 | Loss: 0.00002823
Iteration 55/1000 | Loss: 0.00002823
Iteration 56/1000 | Loss: 0.00002822
Iteration 57/1000 | Loss: 0.00002822
Iteration 58/1000 | Loss: 0.00002822
Iteration 59/1000 | Loss: 0.00002822
Iteration 60/1000 | Loss: 0.00002822
Iteration 61/1000 | Loss: 0.00002822
Iteration 62/1000 | Loss: 0.00002822
Iteration 63/1000 | Loss: 0.00002822
Iteration 64/1000 | Loss: 0.00002822
Iteration 65/1000 | Loss: 0.00002821
Iteration 66/1000 | Loss: 0.00002821
Iteration 67/1000 | Loss: 0.00002821
Iteration 68/1000 | Loss: 0.00002821
Iteration 69/1000 | Loss: 0.00002821
Iteration 70/1000 | Loss: 0.00002821
Iteration 71/1000 | Loss: 0.00002821
Iteration 72/1000 | Loss: 0.00002821
Iteration 73/1000 | Loss: 0.00002821
Iteration 74/1000 | Loss: 0.00002821
Iteration 75/1000 | Loss: 0.00002821
Iteration 76/1000 | Loss: 0.00002821
Iteration 77/1000 | Loss: 0.00002821
Iteration 78/1000 | Loss: 0.00002821
Iteration 79/1000 | Loss: 0.00002820
Iteration 80/1000 | Loss: 0.00002820
Iteration 81/1000 | Loss: 0.00002820
Iteration 82/1000 | Loss: 0.00002820
Iteration 83/1000 | Loss: 0.00002820
Iteration 84/1000 | Loss: 0.00002820
Iteration 85/1000 | Loss: 0.00002820
Iteration 86/1000 | Loss: 0.00002820
Iteration 87/1000 | Loss: 0.00002820
Iteration 88/1000 | Loss: 0.00002820
Iteration 89/1000 | Loss: 0.00002820
Iteration 90/1000 | Loss: 0.00002820
Iteration 91/1000 | Loss: 0.00002820
Iteration 92/1000 | Loss: 0.00002820
Iteration 93/1000 | Loss: 0.00002820
Iteration 94/1000 | Loss: 0.00002820
Iteration 95/1000 | Loss: 0.00002820
Iteration 96/1000 | Loss: 0.00002820
Iteration 97/1000 | Loss: 0.00002820
Iteration 98/1000 | Loss: 0.00002820
Iteration 99/1000 | Loss: 0.00002820
Iteration 100/1000 | Loss: 0.00002820
Iteration 101/1000 | Loss: 0.00002820
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [2.819522524077911e-05, 2.819522524077911e-05, 2.819522524077911e-05, 2.819522524077911e-05, 2.819522524077911e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.819522524077911e-05

Optimization complete. Final v2v error: 4.591006278991699 mm

Highest mean error: 4.814435958862305 mm for frame 10

Lowest mean error: 4.467446804046631 mm for frame 121

Saving results

Total time: 33.75908446311951
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_46_us_2175/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_46_us_2175/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_46_us_2175/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01173310
Iteration 2/25 | Loss: 0.00269432
Iteration 3/25 | Loss: 0.00196072
Iteration 4/25 | Loss: 0.00192437
Iteration 5/25 | Loss: 0.00191470
Iteration 6/25 | Loss: 0.00191255
Iteration 7/25 | Loss: 0.00191248
Iteration 8/25 | Loss: 0.00191248
Iteration 9/25 | Loss: 0.00191248
Iteration 10/25 | Loss: 0.00191248
Iteration 11/25 | Loss: 0.00191248
Iteration 12/25 | Loss: 0.00191248
Iteration 13/25 | Loss: 0.00191248
Iteration 14/25 | Loss: 0.00191248
Iteration 15/25 | Loss: 0.00191248
Iteration 16/25 | Loss: 0.00191248
Iteration 17/25 | Loss: 0.00191248
Iteration 18/25 | Loss: 0.00191248
Iteration 19/25 | Loss: 0.00191248
Iteration 20/25 | Loss: 0.00191248
Iteration 21/25 | Loss: 0.00191248
Iteration 22/25 | Loss: 0.00191248
Iteration 23/25 | Loss: 0.00191248
Iteration 24/25 | Loss: 0.00191248
Iteration 25/25 | Loss: 0.00191248

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.70354813
Iteration 2/25 | Loss: 0.00139147
Iteration 3/25 | Loss: 0.00139147
Iteration 4/25 | Loss: 0.00139147
Iteration 5/25 | Loss: 0.00139147
Iteration 6/25 | Loss: 0.00139147
Iteration 7/25 | Loss: 0.00139147
Iteration 8/25 | Loss: 0.00139147
Iteration 9/25 | Loss: 0.00139147
Iteration 10/25 | Loss: 0.00139147
Iteration 11/25 | Loss: 0.00139147
Iteration 12/25 | Loss: 0.00139147
Iteration 13/25 | Loss: 0.00139147
Iteration 14/25 | Loss: 0.00139147
Iteration 15/25 | Loss: 0.00139147
Iteration 16/25 | Loss: 0.00139147
Iteration 17/25 | Loss: 0.00139147
Iteration 18/25 | Loss: 0.00139147
Iteration 19/25 | Loss: 0.00139147
Iteration 20/25 | Loss: 0.00139147
Iteration 21/25 | Loss: 0.00139147
Iteration 22/25 | Loss: 0.00139147
Iteration 23/25 | Loss: 0.00139147
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001391465775668621, 0.001391465775668621, 0.001391465775668621, 0.001391465775668621, 0.001391465775668621]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001391465775668621

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00139147
Iteration 2/1000 | Loss: 0.00011055
Iteration 3/1000 | Loss: 0.00009677
Iteration 4/1000 | Loss: 0.00009086
Iteration 5/1000 | Loss: 0.00008550
Iteration 6/1000 | Loss: 0.00008189
Iteration 7/1000 | Loss: 0.00007986
Iteration 8/1000 | Loss: 0.00007828
Iteration 9/1000 | Loss: 0.00007659
Iteration 10/1000 | Loss: 0.00007564
Iteration 11/1000 | Loss: 0.00007501
Iteration 12/1000 | Loss: 0.00007466
Iteration 13/1000 | Loss: 0.00007426
Iteration 14/1000 | Loss: 0.00007403
Iteration 15/1000 | Loss: 0.00007383
Iteration 16/1000 | Loss: 0.00007362
Iteration 17/1000 | Loss: 0.00007349
Iteration 18/1000 | Loss: 0.00007339
Iteration 19/1000 | Loss: 0.00007334
Iteration 20/1000 | Loss: 0.00007332
Iteration 21/1000 | Loss: 0.00007331
Iteration 22/1000 | Loss: 0.00007330
Iteration 23/1000 | Loss: 0.00007330
Iteration 24/1000 | Loss: 0.00007329
Iteration 25/1000 | Loss: 0.00007329
Iteration 26/1000 | Loss: 0.00007329
Iteration 27/1000 | Loss: 0.00007329
Iteration 28/1000 | Loss: 0.00007328
Iteration 29/1000 | Loss: 0.00007328
Iteration 30/1000 | Loss: 0.00007328
Iteration 31/1000 | Loss: 0.00007328
Iteration 32/1000 | Loss: 0.00007328
Iteration 33/1000 | Loss: 0.00007328
Iteration 34/1000 | Loss: 0.00007328
Iteration 35/1000 | Loss: 0.00007328
Iteration 36/1000 | Loss: 0.00007328
Iteration 37/1000 | Loss: 0.00007328
Iteration 38/1000 | Loss: 0.00007328
Iteration 39/1000 | Loss: 0.00007328
Iteration 40/1000 | Loss: 0.00007328
Iteration 41/1000 | Loss: 0.00007328
Iteration 42/1000 | Loss: 0.00007327
Iteration 43/1000 | Loss: 0.00007327
Iteration 44/1000 | Loss: 0.00007327
Iteration 45/1000 | Loss: 0.00007327
Iteration 46/1000 | Loss: 0.00007327
Iteration 47/1000 | Loss: 0.00007327
Iteration 48/1000 | Loss: 0.00007327
Iteration 49/1000 | Loss: 0.00007326
Iteration 50/1000 | Loss: 0.00007326
Iteration 51/1000 | Loss: 0.00007326
Iteration 52/1000 | Loss: 0.00007326
Iteration 53/1000 | Loss: 0.00007326
Iteration 54/1000 | Loss: 0.00007326
Iteration 55/1000 | Loss: 0.00007326
Iteration 56/1000 | Loss: 0.00007326
Iteration 57/1000 | Loss: 0.00007325
Iteration 58/1000 | Loss: 0.00007325
Iteration 59/1000 | Loss: 0.00007325
Iteration 60/1000 | Loss: 0.00007325
Iteration 61/1000 | Loss: 0.00007325
Iteration 62/1000 | Loss: 0.00007325
Iteration 63/1000 | Loss: 0.00007325
Iteration 64/1000 | Loss: 0.00007325
Iteration 65/1000 | Loss: 0.00007324
Iteration 66/1000 | Loss: 0.00007324
Iteration 67/1000 | Loss: 0.00007324
Iteration 68/1000 | Loss: 0.00007324
Iteration 69/1000 | Loss: 0.00007324
Iteration 70/1000 | Loss: 0.00007324
Iteration 71/1000 | Loss: 0.00007324
Iteration 72/1000 | Loss: 0.00007323
Iteration 73/1000 | Loss: 0.00007323
Iteration 74/1000 | Loss: 0.00007323
Iteration 75/1000 | Loss: 0.00007323
Iteration 76/1000 | Loss: 0.00007323
Iteration 77/1000 | Loss: 0.00007323
Iteration 78/1000 | Loss: 0.00007323
Iteration 79/1000 | Loss: 0.00007323
Iteration 80/1000 | Loss: 0.00007323
Iteration 81/1000 | Loss: 0.00007323
Iteration 82/1000 | Loss: 0.00007323
Iteration 83/1000 | Loss: 0.00007323
Iteration 84/1000 | Loss: 0.00007323
Iteration 85/1000 | Loss: 0.00007323
Iteration 86/1000 | Loss: 0.00007323
Iteration 87/1000 | Loss: 0.00007323
Iteration 88/1000 | Loss: 0.00007323
Iteration 89/1000 | Loss: 0.00007323
Iteration 90/1000 | Loss: 0.00007323
Iteration 91/1000 | Loss: 0.00007323
Iteration 92/1000 | Loss: 0.00007323
Iteration 93/1000 | Loss: 0.00007323
Iteration 94/1000 | Loss: 0.00007323
Iteration 95/1000 | Loss: 0.00007323
Iteration 96/1000 | Loss: 0.00007323
Iteration 97/1000 | Loss: 0.00007323
Iteration 98/1000 | Loss: 0.00007323
Iteration 99/1000 | Loss: 0.00007323
Iteration 100/1000 | Loss: 0.00007323
Iteration 101/1000 | Loss: 0.00007323
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [7.322807505261153e-05, 7.322807505261153e-05, 7.322807505261153e-05, 7.322807505261153e-05, 7.322807505261153e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.322807505261153e-05

Optimization complete. Final v2v error: 6.649875164031982 mm

Highest mean error: 7.275646209716797 mm for frame 78

Lowest mean error: 5.373141288757324 mm for frame 15

Saving results

Total time: 38.78012943267822
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_46_us_2175/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_46_us_2175/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_46_us_2175/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00422101
Iteration 2/25 | Loss: 0.00193112
Iteration 3/25 | Loss: 0.00181456
Iteration 4/25 | Loss: 0.00179402
Iteration 5/25 | Loss: 0.00178897
Iteration 6/25 | Loss: 0.00178821
Iteration 7/25 | Loss: 0.00178821
Iteration 8/25 | Loss: 0.00178821
Iteration 9/25 | Loss: 0.00178821
Iteration 10/25 | Loss: 0.00178821
Iteration 11/25 | Loss: 0.00178821
Iteration 12/25 | Loss: 0.00178821
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0017882137326523662, 0.0017882137326523662, 0.0017882137326523662, 0.0017882137326523662, 0.0017882137326523662]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017882137326523662

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.88728750
Iteration 2/25 | Loss: 0.00239121
Iteration 3/25 | Loss: 0.00239121
Iteration 4/25 | Loss: 0.00239121
Iteration 5/25 | Loss: 0.00239121
Iteration 6/25 | Loss: 0.00239121
Iteration 7/25 | Loss: 0.00239121
Iteration 8/25 | Loss: 0.00239121
Iteration 9/25 | Loss: 0.00239121
Iteration 10/25 | Loss: 0.00239121
Iteration 11/25 | Loss: 0.00239121
Iteration 12/25 | Loss: 0.00239121
Iteration 13/25 | Loss: 0.00239121
Iteration 14/25 | Loss: 0.00239121
Iteration 15/25 | Loss: 0.00239121
Iteration 16/25 | Loss: 0.00239121
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.002391206566244364, 0.002391206566244364, 0.002391206566244364, 0.002391206566244364, 0.002391206566244364]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002391206566244364

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00239121
Iteration 2/1000 | Loss: 0.00006778
Iteration 3/1000 | Loss: 0.00005248
Iteration 4/1000 | Loss: 0.00004554
Iteration 5/1000 | Loss: 0.00004179
Iteration 6/1000 | Loss: 0.00004026
Iteration 7/1000 | Loss: 0.00003891
Iteration 8/1000 | Loss: 0.00003811
Iteration 9/1000 | Loss: 0.00003758
Iteration 10/1000 | Loss: 0.00003709
Iteration 11/1000 | Loss: 0.00003673
Iteration 12/1000 | Loss: 0.00003652
Iteration 13/1000 | Loss: 0.00003632
Iteration 14/1000 | Loss: 0.00003629
Iteration 15/1000 | Loss: 0.00003627
Iteration 16/1000 | Loss: 0.00003621
Iteration 17/1000 | Loss: 0.00003618
Iteration 18/1000 | Loss: 0.00003617
Iteration 19/1000 | Loss: 0.00003616
Iteration 20/1000 | Loss: 0.00003615
Iteration 21/1000 | Loss: 0.00003614
Iteration 22/1000 | Loss: 0.00003614
Iteration 23/1000 | Loss: 0.00003611
Iteration 24/1000 | Loss: 0.00003611
Iteration 25/1000 | Loss: 0.00003611
Iteration 26/1000 | Loss: 0.00003611
Iteration 27/1000 | Loss: 0.00003611
Iteration 28/1000 | Loss: 0.00003610
Iteration 29/1000 | Loss: 0.00003609
Iteration 30/1000 | Loss: 0.00003608
Iteration 31/1000 | Loss: 0.00003607
Iteration 32/1000 | Loss: 0.00003607
Iteration 33/1000 | Loss: 0.00003605
Iteration 34/1000 | Loss: 0.00003604
Iteration 35/1000 | Loss: 0.00003604
Iteration 36/1000 | Loss: 0.00003604
Iteration 37/1000 | Loss: 0.00003601
Iteration 38/1000 | Loss: 0.00003601
Iteration 39/1000 | Loss: 0.00003601
Iteration 40/1000 | Loss: 0.00003601
Iteration 41/1000 | Loss: 0.00003600
Iteration 42/1000 | Loss: 0.00003600
Iteration 43/1000 | Loss: 0.00003600
Iteration 44/1000 | Loss: 0.00003600
Iteration 45/1000 | Loss: 0.00003600
Iteration 46/1000 | Loss: 0.00003600
Iteration 47/1000 | Loss: 0.00003600
Iteration 48/1000 | Loss: 0.00003600
Iteration 49/1000 | Loss: 0.00003600
Iteration 50/1000 | Loss: 0.00003600
Iteration 51/1000 | Loss: 0.00003600
Iteration 52/1000 | Loss: 0.00003600
Iteration 53/1000 | Loss: 0.00003600
Iteration 54/1000 | Loss: 0.00003599
Iteration 55/1000 | Loss: 0.00003598
Iteration 56/1000 | Loss: 0.00003598
Iteration 57/1000 | Loss: 0.00003598
Iteration 58/1000 | Loss: 0.00003598
Iteration 59/1000 | Loss: 0.00003598
Iteration 60/1000 | Loss: 0.00003598
Iteration 61/1000 | Loss: 0.00003598
Iteration 62/1000 | Loss: 0.00003597
Iteration 63/1000 | Loss: 0.00003597
Iteration 64/1000 | Loss: 0.00003597
Iteration 65/1000 | Loss: 0.00003597
Iteration 66/1000 | Loss: 0.00003597
Iteration 67/1000 | Loss: 0.00003597
Iteration 68/1000 | Loss: 0.00003597
Iteration 69/1000 | Loss: 0.00003597
Iteration 70/1000 | Loss: 0.00003597
Iteration 71/1000 | Loss: 0.00003597
Iteration 72/1000 | Loss: 0.00003597
Iteration 73/1000 | Loss: 0.00003596
Iteration 74/1000 | Loss: 0.00003596
Iteration 75/1000 | Loss: 0.00003595
Iteration 76/1000 | Loss: 0.00003595
Iteration 77/1000 | Loss: 0.00003595
Iteration 78/1000 | Loss: 0.00003595
Iteration 79/1000 | Loss: 0.00003595
Iteration 80/1000 | Loss: 0.00003594
Iteration 81/1000 | Loss: 0.00003594
Iteration 82/1000 | Loss: 0.00003594
Iteration 83/1000 | Loss: 0.00003594
Iteration 84/1000 | Loss: 0.00003594
Iteration 85/1000 | Loss: 0.00003593
Iteration 86/1000 | Loss: 0.00003593
Iteration 87/1000 | Loss: 0.00003593
Iteration 88/1000 | Loss: 0.00003593
Iteration 89/1000 | Loss: 0.00003592
Iteration 90/1000 | Loss: 0.00003592
Iteration 91/1000 | Loss: 0.00003592
Iteration 92/1000 | Loss: 0.00003592
Iteration 93/1000 | Loss: 0.00003592
Iteration 94/1000 | Loss: 0.00003591
Iteration 95/1000 | Loss: 0.00003591
Iteration 96/1000 | Loss: 0.00003591
Iteration 97/1000 | Loss: 0.00003591
Iteration 98/1000 | Loss: 0.00003591
Iteration 99/1000 | Loss: 0.00003591
Iteration 100/1000 | Loss: 0.00003591
Iteration 101/1000 | Loss: 0.00003591
Iteration 102/1000 | Loss: 0.00003591
Iteration 103/1000 | Loss: 0.00003591
Iteration 104/1000 | Loss: 0.00003591
Iteration 105/1000 | Loss: 0.00003591
Iteration 106/1000 | Loss: 0.00003591
Iteration 107/1000 | Loss: 0.00003591
Iteration 108/1000 | Loss: 0.00003591
Iteration 109/1000 | Loss: 0.00003591
Iteration 110/1000 | Loss: 0.00003590
Iteration 111/1000 | Loss: 0.00003590
Iteration 112/1000 | Loss: 0.00003590
Iteration 113/1000 | Loss: 0.00003590
Iteration 114/1000 | Loss: 0.00003590
Iteration 115/1000 | Loss: 0.00003590
Iteration 116/1000 | Loss: 0.00003590
Iteration 117/1000 | Loss: 0.00003590
Iteration 118/1000 | Loss: 0.00003590
Iteration 119/1000 | Loss: 0.00003590
Iteration 120/1000 | Loss: 0.00003590
Iteration 121/1000 | Loss: 0.00003590
Iteration 122/1000 | Loss: 0.00003590
Iteration 123/1000 | Loss: 0.00003590
Iteration 124/1000 | Loss: 0.00003590
Iteration 125/1000 | Loss: 0.00003590
Iteration 126/1000 | Loss: 0.00003590
Iteration 127/1000 | Loss: 0.00003590
Iteration 128/1000 | Loss: 0.00003589
Iteration 129/1000 | Loss: 0.00003589
Iteration 130/1000 | Loss: 0.00003589
Iteration 131/1000 | Loss: 0.00003589
Iteration 132/1000 | Loss: 0.00003589
Iteration 133/1000 | Loss: 0.00003589
Iteration 134/1000 | Loss: 0.00003589
Iteration 135/1000 | Loss: 0.00003589
Iteration 136/1000 | Loss: 0.00003589
Iteration 137/1000 | Loss: 0.00003589
Iteration 138/1000 | Loss: 0.00003589
Iteration 139/1000 | Loss: 0.00003589
Iteration 140/1000 | Loss: 0.00003589
Iteration 141/1000 | Loss: 0.00003589
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [3.589112020563334e-05, 3.589112020563334e-05, 3.589112020563334e-05, 3.589112020563334e-05, 3.589112020563334e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.589112020563334e-05

Optimization complete. Final v2v error: 5.22765588760376 mm

Highest mean error: 5.4239912033081055 mm for frame 238

Lowest mean error: 5.0348381996154785 mm for frame 203

Saving results

Total time: 42.89332032203674
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_46_us_2175/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_46_us_2175/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_46_us_2175/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00631320
Iteration 2/25 | Loss: 0.00209450
Iteration 3/25 | Loss: 0.00189961
Iteration 4/25 | Loss: 0.00187928
Iteration 5/25 | Loss: 0.00187479
Iteration 6/25 | Loss: 0.00187418
Iteration 7/25 | Loss: 0.00187418
Iteration 8/25 | Loss: 0.00187418
Iteration 9/25 | Loss: 0.00187418
Iteration 10/25 | Loss: 0.00187418
Iteration 11/25 | Loss: 0.00187418
Iteration 12/25 | Loss: 0.00187418
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0018741802778095007, 0.0018741802778095007, 0.0018741802778095007, 0.0018741802778095007, 0.0018741802778095007]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018741802778095007

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.88764977
Iteration 2/25 | Loss: 0.00277811
Iteration 3/25 | Loss: 0.00277807
Iteration 4/25 | Loss: 0.00277806
Iteration 5/25 | Loss: 0.00277806
Iteration 6/25 | Loss: 0.00277806
Iteration 7/25 | Loss: 0.00277806
Iteration 8/25 | Loss: 0.00277806
Iteration 9/25 | Loss: 0.00277806
Iteration 10/25 | Loss: 0.00277806
Iteration 11/25 | Loss: 0.00277806
Iteration 12/25 | Loss: 0.00277806
Iteration 13/25 | Loss: 0.00277806
Iteration 14/25 | Loss: 0.00277806
Iteration 15/25 | Loss: 0.00277806
Iteration 16/25 | Loss: 0.00277806
Iteration 17/25 | Loss: 0.00277806
Iteration 18/25 | Loss: 0.00277806
Iteration 19/25 | Loss: 0.00277806
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.002778062131255865, 0.002778062131255865, 0.002778062131255865, 0.002778062131255865, 0.002778062131255865]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002778062131255865

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00277806
Iteration 2/1000 | Loss: 0.00005753
Iteration 3/1000 | Loss: 0.00004213
Iteration 4/1000 | Loss: 0.00003840
Iteration 5/1000 | Loss: 0.00003597
Iteration 6/1000 | Loss: 0.00003470
Iteration 7/1000 | Loss: 0.00003379
Iteration 8/1000 | Loss: 0.00003316
Iteration 9/1000 | Loss: 0.00003268
Iteration 10/1000 | Loss: 0.00003226
Iteration 11/1000 | Loss: 0.00003195
Iteration 12/1000 | Loss: 0.00003168
Iteration 13/1000 | Loss: 0.00003152
Iteration 14/1000 | Loss: 0.00003149
Iteration 15/1000 | Loss: 0.00003144
Iteration 16/1000 | Loss: 0.00003144
Iteration 17/1000 | Loss: 0.00003143
Iteration 18/1000 | Loss: 0.00003143
Iteration 19/1000 | Loss: 0.00003143
Iteration 20/1000 | Loss: 0.00003143
Iteration 21/1000 | Loss: 0.00003142
Iteration 22/1000 | Loss: 0.00003142
Iteration 23/1000 | Loss: 0.00003142
Iteration 24/1000 | Loss: 0.00003142
Iteration 25/1000 | Loss: 0.00003142
Iteration 26/1000 | Loss: 0.00003142
Iteration 27/1000 | Loss: 0.00003142
Iteration 28/1000 | Loss: 0.00003141
Iteration 29/1000 | Loss: 0.00003141
Iteration 30/1000 | Loss: 0.00003140
Iteration 31/1000 | Loss: 0.00003140
Iteration 32/1000 | Loss: 0.00003140
Iteration 33/1000 | Loss: 0.00003139
Iteration 34/1000 | Loss: 0.00003138
Iteration 35/1000 | Loss: 0.00003138
Iteration 36/1000 | Loss: 0.00003138
Iteration 37/1000 | Loss: 0.00003138
Iteration 38/1000 | Loss: 0.00003138
Iteration 39/1000 | Loss: 0.00003138
Iteration 40/1000 | Loss: 0.00003138
Iteration 41/1000 | Loss: 0.00003138
Iteration 42/1000 | Loss: 0.00003138
Iteration 43/1000 | Loss: 0.00003138
Iteration 44/1000 | Loss: 0.00003137
Iteration 45/1000 | Loss: 0.00003137
Iteration 46/1000 | Loss: 0.00003137
Iteration 47/1000 | Loss: 0.00003137
Iteration 48/1000 | Loss: 0.00003137
Iteration 49/1000 | Loss: 0.00003137
Iteration 50/1000 | Loss: 0.00003136
Iteration 51/1000 | Loss: 0.00003136
Iteration 52/1000 | Loss: 0.00003136
Iteration 53/1000 | Loss: 0.00003136
Iteration 54/1000 | Loss: 0.00003136
Iteration 55/1000 | Loss: 0.00003136
Iteration 56/1000 | Loss: 0.00003136
Iteration 57/1000 | Loss: 0.00003136
Iteration 58/1000 | Loss: 0.00003136
Iteration 59/1000 | Loss: 0.00003136
Iteration 60/1000 | Loss: 0.00003136
Iteration 61/1000 | Loss: 0.00003136
Iteration 62/1000 | Loss: 0.00003136
Iteration 63/1000 | Loss: 0.00003135
Iteration 64/1000 | Loss: 0.00003135
Iteration 65/1000 | Loss: 0.00003135
Iteration 66/1000 | Loss: 0.00003134
Iteration 67/1000 | Loss: 0.00003134
Iteration 68/1000 | Loss: 0.00003134
Iteration 69/1000 | Loss: 0.00003133
Iteration 70/1000 | Loss: 0.00003133
Iteration 71/1000 | Loss: 0.00003133
Iteration 72/1000 | Loss: 0.00003133
Iteration 73/1000 | Loss: 0.00003133
Iteration 74/1000 | Loss: 0.00003132
Iteration 75/1000 | Loss: 0.00003132
Iteration 76/1000 | Loss: 0.00003132
Iteration 77/1000 | Loss: 0.00003132
Iteration 78/1000 | Loss: 0.00003132
Iteration 79/1000 | Loss: 0.00003132
Iteration 80/1000 | Loss: 0.00003132
Iteration 81/1000 | Loss: 0.00003132
Iteration 82/1000 | Loss: 0.00003132
Iteration 83/1000 | Loss: 0.00003132
Iteration 84/1000 | Loss: 0.00003132
Iteration 85/1000 | Loss: 0.00003132
Iteration 86/1000 | Loss: 0.00003132
Iteration 87/1000 | Loss: 0.00003132
Iteration 88/1000 | Loss: 0.00003131
Iteration 89/1000 | Loss: 0.00003131
Iteration 90/1000 | Loss: 0.00003131
Iteration 91/1000 | Loss: 0.00003131
Iteration 92/1000 | Loss: 0.00003131
Iteration 93/1000 | Loss: 0.00003131
Iteration 94/1000 | Loss: 0.00003131
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [3.1314808438764885e-05, 3.1314808438764885e-05, 3.1314808438764885e-05, 3.1314808438764885e-05, 3.1314808438764885e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.1314808438764885e-05

Optimization complete. Final v2v error: 4.907059192657471 mm

Highest mean error: 5.050586223602295 mm for frame 151

Lowest mean error: 4.799249649047852 mm for frame 24

Saving results

Total time: 32.90612745285034
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_46_us_2175/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_46_us_2175/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_46_us_2175/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01120054
Iteration 2/25 | Loss: 0.00476133
Iteration 3/25 | Loss: 0.00270018
Iteration 4/25 | Loss: 0.00231384
Iteration 5/25 | Loss: 0.00220379
Iteration 6/25 | Loss: 0.00216800
Iteration 7/25 | Loss: 0.00207087
Iteration 8/25 | Loss: 0.00197303
Iteration 9/25 | Loss: 0.00195295
Iteration 10/25 | Loss: 0.00190042
Iteration 11/25 | Loss: 0.00186773
Iteration 12/25 | Loss: 0.00183785
Iteration 13/25 | Loss: 0.00183362
Iteration 14/25 | Loss: 0.00181901
Iteration 15/25 | Loss: 0.00181630
Iteration 16/25 | Loss: 0.00181312
Iteration 17/25 | Loss: 0.00180660
Iteration 18/25 | Loss: 0.00180023
Iteration 19/25 | Loss: 0.00180137
Iteration 20/25 | Loss: 0.00179443
Iteration 21/25 | Loss: 0.00179035
Iteration 22/25 | Loss: 0.00179112
Iteration 23/25 | Loss: 0.00178763
Iteration 24/25 | Loss: 0.00178591
Iteration 25/25 | Loss: 0.00178367

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51983047
Iteration 2/25 | Loss: 0.00887552
Iteration 3/25 | Loss: 0.00708835
Iteration 4/25 | Loss: 0.00708835
Iteration 5/25 | Loss: 0.00708835
Iteration 6/25 | Loss: 0.00708835
Iteration 7/25 | Loss: 0.00708835
Iteration 8/25 | Loss: 0.00708835
Iteration 9/25 | Loss: 0.00708835
Iteration 10/25 | Loss: 0.00708835
Iteration 11/25 | Loss: 0.00708835
Iteration 12/25 | Loss: 0.00708835
Iteration 13/25 | Loss: 0.00708835
Iteration 14/25 | Loss: 0.00708835
Iteration 15/25 | Loss: 0.00708835
Iteration 16/25 | Loss: 0.00708835
Iteration 17/25 | Loss: 0.00708835
Iteration 18/25 | Loss: 0.00708835
Iteration 19/25 | Loss: 0.00708835
Iteration 20/25 | Loss: 0.00708835
Iteration 21/25 | Loss: 0.00708835
Iteration 22/25 | Loss: 0.00708835
Iteration 23/25 | Loss: 0.00708835
Iteration 24/25 | Loss: 0.00708835
Iteration 25/25 | Loss: 0.00708835

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00708835
Iteration 2/1000 | Loss: 0.00264050
Iteration 3/1000 | Loss: 0.00086740
Iteration 4/1000 | Loss: 0.00087290
Iteration 5/1000 | Loss: 0.00155634
Iteration 6/1000 | Loss: 0.00203713
Iteration 7/1000 | Loss: 0.00078699
Iteration 8/1000 | Loss: 0.00074034
Iteration 9/1000 | Loss: 0.00097535
Iteration 10/1000 | Loss: 0.00094527
Iteration 11/1000 | Loss: 0.00086870
Iteration 12/1000 | Loss: 0.00046470
Iteration 13/1000 | Loss: 0.00467455
Iteration 14/1000 | Loss: 0.00553322
Iteration 15/1000 | Loss: 0.00191968
Iteration 16/1000 | Loss: 0.00465828
Iteration 17/1000 | Loss: 0.00276957
Iteration 18/1000 | Loss: 0.00253378
Iteration 19/1000 | Loss: 0.00112268
Iteration 20/1000 | Loss: 0.00060076
Iteration 21/1000 | Loss: 0.00059992
Iteration 22/1000 | Loss: 0.00054225
Iteration 23/1000 | Loss: 0.00053855
Iteration 24/1000 | Loss: 0.00057043
Iteration 25/1000 | Loss: 0.00119485
Iteration 26/1000 | Loss: 0.00085763
Iteration 27/1000 | Loss: 0.00160804
Iteration 28/1000 | Loss: 0.00125049
Iteration 29/1000 | Loss: 0.00073826
Iteration 30/1000 | Loss: 0.00109558
Iteration 31/1000 | Loss: 0.00105142
Iteration 32/1000 | Loss: 0.00039415
Iteration 33/1000 | Loss: 0.00034019
Iteration 34/1000 | Loss: 0.00029909
Iteration 35/1000 | Loss: 0.00030123
Iteration 36/1000 | Loss: 0.00026804
Iteration 37/1000 | Loss: 0.00025837
Iteration 38/1000 | Loss: 0.00026021
Iteration 39/1000 | Loss: 0.00067925
Iteration 40/1000 | Loss: 0.00032214
Iteration 41/1000 | Loss: 0.00053917
Iteration 42/1000 | Loss: 0.00025571
Iteration 43/1000 | Loss: 0.00077201
Iteration 44/1000 | Loss: 0.00056316
Iteration 45/1000 | Loss: 0.00092054
Iteration 46/1000 | Loss: 0.00127293
Iteration 47/1000 | Loss: 0.00073020
Iteration 48/1000 | Loss: 0.00069780
Iteration 49/1000 | Loss: 0.00043616
Iteration 50/1000 | Loss: 0.00028576
Iteration 51/1000 | Loss: 0.00090868
Iteration 52/1000 | Loss: 0.00030738
Iteration 53/1000 | Loss: 0.00064275
Iteration 54/1000 | Loss: 0.00026307
Iteration 55/1000 | Loss: 0.00024466
Iteration 56/1000 | Loss: 0.00027185
Iteration 57/1000 | Loss: 0.00024643
Iteration 58/1000 | Loss: 0.00079060
Iteration 59/1000 | Loss: 0.00042059
Iteration 60/1000 | Loss: 0.00081239
Iteration 61/1000 | Loss: 0.00137964
Iteration 62/1000 | Loss: 0.00124468
Iteration 63/1000 | Loss: 0.00027966
Iteration 64/1000 | Loss: 0.00027595
Iteration 65/1000 | Loss: 0.00025577
Iteration 66/1000 | Loss: 0.00086778
Iteration 67/1000 | Loss: 0.00024883
Iteration 68/1000 | Loss: 0.00023455
Iteration 69/1000 | Loss: 0.00025040
Iteration 70/1000 | Loss: 0.00082772
Iteration 71/1000 | Loss: 0.00087550
Iteration 72/1000 | Loss: 0.00072369
Iteration 73/1000 | Loss: 0.00072314
Iteration 74/1000 | Loss: 0.00026742
Iteration 75/1000 | Loss: 0.00026013
Iteration 76/1000 | Loss: 0.00025476
Iteration 77/1000 | Loss: 0.00030493
Iteration 78/1000 | Loss: 0.00043121
Iteration 79/1000 | Loss: 0.00022760
Iteration 80/1000 | Loss: 0.00034510
Iteration 81/1000 | Loss: 0.00084276
Iteration 82/1000 | Loss: 0.00090880
Iteration 83/1000 | Loss: 0.00066851
Iteration 84/1000 | Loss: 0.00038838
Iteration 85/1000 | Loss: 0.00032121
Iteration 86/1000 | Loss: 0.00024091
Iteration 87/1000 | Loss: 0.00021090
Iteration 88/1000 | Loss: 0.00045784
Iteration 89/1000 | Loss: 0.00022407
Iteration 90/1000 | Loss: 0.00022046
Iteration 91/1000 | Loss: 0.00019845
Iteration 92/1000 | Loss: 0.00088184
Iteration 93/1000 | Loss: 0.00041976
Iteration 94/1000 | Loss: 0.00021711
Iteration 95/1000 | Loss: 0.00022505
Iteration 96/1000 | Loss: 0.00024431
Iteration 97/1000 | Loss: 0.00022807
Iteration 98/1000 | Loss: 0.00020959
Iteration 99/1000 | Loss: 0.00021622
Iteration 100/1000 | Loss: 0.00021460
Iteration 101/1000 | Loss: 0.00061356
Iteration 102/1000 | Loss: 0.00027715
Iteration 103/1000 | Loss: 0.00043514
Iteration 104/1000 | Loss: 0.00024932
Iteration 105/1000 | Loss: 0.00022441
Iteration 106/1000 | Loss: 0.00032238
Iteration 107/1000 | Loss: 0.00018556
Iteration 108/1000 | Loss: 0.00018072
Iteration 109/1000 | Loss: 0.00018039
Iteration 110/1000 | Loss: 0.00017958
Iteration 111/1000 | Loss: 0.00018900
Iteration 112/1000 | Loss: 0.00060855
Iteration 113/1000 | Loss: 0.00173959
Iteration 114/1000 | Loss: 0.00077346
Iteration 115/1000 | Loss: 0.00064326
Iteration 116/1000 | Loss: 0.00061694
Iteration 117/1000 | Loss: 0.00049752
Iteration 118/1000 | Loss: 0.00028388
Iteration 119/1000 | Loss: 0.00018565
Iteration 120/1000 | Loss: 0.00018273
Iteration 121/1000 | Loss: 0.00027215
Iteration 122/1000 | Loss: 0.00016660
Iteration 123/1000 | Loss: 0.00046463
Iteration 124/1000 | Loss: 0.00039070
Iteration 125/1000 | Loss: 0.00042077
Iteration 126/1000 | Loss: 0.00030598
Iteration 127/1000 | Loss: 0.00037044
Iteration 128/1000 | Loss: 0.00046546
Iteration 129/1000 | Loss: 0.00088929
Iteration 130/1000 | Loss: 0.00052393
Iteration 131/1000 | Loss: 0.00017193
Iteration 132/1000 | Loss: 0.00016730
Iteration 133/1000 | Loss: 0.00019024
Iteration 134/1000 | Loss: 0.00018005
Iteration 135/1000 | Loss: 0.00016421
Iteration 136/1000 | Loss: 0.00024101
Iteration 137/1000 | Loss: 0.00018736
Iteration 138/1000 | Loss: 0.00017881
Iteration 139/1000 | Loss: 0.00066962
Iteration 140/1000 | Loss: 0.00023835
Iteration 141/1000 | Loss: 0.00051443
Iteration 142/1000 | Loss: 0.00019306
Iteration 143/1000 | Loss: 0.00025468
Iteration 144/1000 | Loss: 0.00019507
Iteration 145/1000 | Loss: 0.00018064
Iteration 146/1000 | Loss: 0.00020726
Iteration 147/1000 | Loss: 0.00019590
Iteration 148/1000 | Loss: 0.00020792
Iteration 149/1000 | Loss: 0.00068539
Iteration 150/1000 | Loss: 0.00018524
Iteration 151/1000 | Loss: 0.00022273
Iteration 152/1000 | Loss: 0.00017166
Iteration 153/1000 | Loss: 0.00018436
Iteration 154/1000 | Loss: 0.00015837
Iteration 155/1000 | Loss: 0.00015554
Iteration 156/1000 | Loss: 0.00015499
Iteration 157/1000 | Loss: 0.00015315
Iteration 158/1000 | Loss: 0.00015278
Iteration 159/1000 | Loss: 0.00015243
Iteration 160/1000 | Loss: 0.00097685
Iteration 161/1000 | Loss: 0.00042681
Iteration 162/1000 | Loss: 0.00032574
Iteration 163/1000 | Loss: 0.00015397
Iteration 164/1000 | Loss: 0.00040209
Iteration 165/1000 | Loss: 0.00084360
Iteration 166/1000 | Loss: 0.00022891
Iteration 167/1000 | Loss: 0.00015449
Iteration 168/1000 | Loss: 0.00015206
Iteration 169/1000 | Loss: 0.00015097
Iteration 170/1000 | Loss: 0.00015054
Iteration 171/1000 | Loss: 0.00015022
Iteration 172/1000 | Loss: 0.00015003
Iteration 173/1000 | Loss: 0.00014982
Iteration 174/1000 | Loss: 0.00066431
Iteration 175/1000 | Loss: 0.00018457
Iteration 176/1000 | Loss: 0.00029323
Iteration 177/1000 | Loss: 0.00016627
Iteration 178/1000 | Loss: 0.00043719
Iteration 179/1000 | Loss: 0.00047168
Iteration 180/1000 | Loss: 0.00092475
Iteration 181/1000 | Loss: 0.00137214
Iteration 182/1000 | Loss: 0.00058366
Iteration 183/1000 | Loss: 0.00017267
Iteration 184/1000 | Loss: 0.00059381
Iteration 185/1000 | Loss: 0.00027566
Iteration 186/1000 | Loss: 0.00016245
Iteration 187/1000 | Loss: 0.00015669
Iteration 188/1000 | Loss: 0.00015255
Iteration 189/1000 | Loss: 0.00019027
Iteration 190/1000 | Loss: 0.00015092
Iteration 191/1000 | Loss: 0.00014580
Iteration 192/1000 | Loss: 0.00014478
Iteration 193/1000 | Loss: 0.00014415
Iteration 194/1000 | Loss: 0.00014369
Iteration 195/1000 | Loss: 0.00014334
Iteration 196/1000 | Loss: 0.00014325
Iteration 197/1000 | Loss: 0.00014321
Iteration 198/1000 | Loss: 0.00014321
Iteration 199/1000 | Loss: 0.00014321
Iteration 200/1000 | Loss: 0.00014320
Iteration 201/1000 | Loss: 0.00014320
Iteration 202/1000 | Loss: 0.00014320
Iteration 203/1000 | Loss: 0.00014320
Iteration 204/1000 | Loss: 0.00014319
Iteration 205/1000 | Loss: 0.00014318
Iteration 206/1000 | Loss: 0.00014318
Iteration 207/1000 | Loss: 0.00014318
Iteration 208/1000 | Loss: 0.00014318
Iteration 209/1000 | Loss: 0.00014318
Iteration 210/1000 | Loss: 0.00014318
Iteration 211/1000 | Loss: 0.00014318
Iteration 212/1000 | Loss: 0.00014318
Iteration 213/1000 | Loss: 0.00014317
Iteration 214/1000 | Loss: 0.00014317
Iteration 215/1000 | Loss: 0.00014317
Iteration 216/1000 | Loss: 0.00014316
Iteration 217/1000 | Loss: 0.00014316
Iteration 218/1000 | Loss: 0.00014315
Iteration 219/1000 | Loss: 0.00014315
Iteration 220/1000 | Loss: 0.00014314
Iteration 221/1000 | Loss: 0.00014314
Iteration 222/1000 | Loss: 0.00014312
Iteration 223/1000 | Loss: 0.00014310
Iteration 224/1000 | Loss: 0.00014310
Iteration 225/1000 | Loss: 0.00014310
Iteration 226/1000 | Loss: 0.00014309
Iteration 227/1000 | Loss: 0.00014309
Iteration 228/1000 | Loss: 0.00014309
Iteration 229/1000 | Loss: 0.00014309
Iteration 230/1000 | Loss: 0.00014309
Iteration 231/1000 | Loss: 0.00014309
Iteration 232/1000 | Loss: 0.00014309
Iteration 233/1000 | Loss: 0.00014307
Iteration 234/1000 | Loss: 0.00014307
Iteration 235/1000 | Loss: 0.00014307
Iteration 236/1000 | Loss: 0.00014307
Iteration 237/1000 | Loss: 0.00014307
Iteration 238/1000 | Loss: 0.00014306
Iteration 239/1000 | Loss: 0.00014306
Iteration 240/1000 | Loss: 0.00014306
Iteration 241/1000 | Loss: 0.00014306
Iteration 242/1000 | Loss: 0.00014306
Iteration 243/1000 | Loss: 0.00014306
Iteration 244/1000 | Loss: 0.00014306
Iteration 245/1000 | Loss: 0.00014305
Iteration 246/1000 | Loss: 0.00014305
Iteration 247/1000 | Loss: 0.00014305
Iteration 248/1000 | Loss: 0.00014305
Iteration 249/1000 | Loss: 0.00014305
Iteration 250/1000 | Loss: 0.00014304
Iteration 251/1000 | Loss: 0.00014304
Iteration 252/1000 | Loss: 0.00014303
Iteration 253/1000 | Loss: 0.00014302
Iteration 254/1000 | Loss: 0.00014302
Iteration 255/1000 | Loss: 0.00014302
Iteration 256/1000 | Loss: 0.00014302
Iteration 257/1000 | Loss: 0.00014302
Iteration 258/1000 | Loss: 0.00014301
Iteration 259/1000 | Loss: 0.00014301
Iteration 260/1000 | Loss: 0.00014301
Iteration 261/1000 | Loss: 0.00014301
Iteration 262/1000 | Loss: 0.00014301
Iteration 263/1000 | Loss: 0.00014301
Iteration 264/1000 | Loss: 0.00014300
Iteration 265/1000 | Loss: 0.00014300
Iteration 266/1000 | Loss: 0.00014300
Iteration 267/1000 | Loss: 0.00014300
Iteration 268/1000 | Loss: 0.00014300
Iteration 269/1000 | Loss: 0.00014299
Iteration 270/1000 | Loss: 0.00014299
Iteration 271/1000 | Loss: 0.00014299
Iteration 272/1000 | Loss: 0.00014299
Iteration 273/1000 | Loss: 0.00014298
Iteration 274/1000 | Loss: 0.00014298
Iteration 275/1000 | Loss: 0.00014298
Iteration 276/1000 | Loss: 0.00014298
Iteration 277/1000 | Loss: 0.00014298
Iteration 278/1000 | Loss: 0.00014298
Iteration 279/1000 | Loss: 0.00014298
Iteration 280/1000 | Loss: 0.00014298
Iteration 281/1000 | Loss: 0.00014297
Iteration 282/1000 | Loss: 0.00014297
Iteration 283/1000 | Loss: 0.00014297
Iteration 284/1000 | Loss: 0.00014297
Iteration 285/1000 | Loss: 0.00014297
Iteration 286/1000 | Loss: 0.00014297
Iteration 287/1000 | Loss: 0.00014297
Iteration 288/1000 | Loss: 0.00014296
Iteration 289/1000 | Loss: 0.00014296
Iteration 290/1000 | Loss: 0.00014296
Iteration 291/1000 | Loss: 0.00014296
Iteration 292/1000 | Loss: 0.00014296
Iteration 293/1000 | Loss: 0.00014295
Iteration 294/1000 | Loss: 0.00014295
Iteration 295/1000 | Loss: 0.00014295
Iteration 296/1000 | Loss: 0.00014295
Iteration 297/1000 | Loss: 0.00014295
Iteration 298/1000 | Loss: 0.00014294
Iteration 299/1000 | Loss: 0.00014294
Iteration 300/1000 | Loss: 0.00014294
Iteration 301/1000 | Loss: 0.00014294
Iteration 302/1000 | Loss: 0.00014294
Iteration 303/1000 | Loss: 0.00014294
Iteration 304/1000 | Loss: 0.00014294
Iteration 305/1000 | Loss: 0.00014293
Iteration 306/1000 | Loss: 0.00014293
Iteration 307/1000 | Loss: 0.00014293
Iteration 308/1000 | Loss: 0.00014293
Iteration 309/1000 | Loss: 0.00014293
Iteration 310/1000 | Loss: 0.00014293
Iteration 311/1000 | Loss: 0.00014293
Iteration 312/1000 | Loss: 0.00014293
Iteration 313/1000 | Loss: 0.00014293
Iteration 314/1000 | Loss: 0.00014293
Iteration 315/1000 | Loss: 0.00014293
Iteration 316/1000 | Loss: 0.00014293
Iteration 317/1000 | Loss: 0.00014293
Iteration 318/1000 | Loss: 0.00014293
Iteration 319/1000 | Loss: 0.00014293
Iteration 320/1000 | Loss: 0.00014293
Iteration 321/1000 | Loss: 0.00014293
Iteration 322/1000 | Loss: 0.00014293
Iteration 323/1000 | Loss: 0.00014293
Iteration 324/1000 | Loss: 0.00014293
Iteration 325/1000 | Loss: 0.00014293
Iteration 326/1000 | Loss: 0.00014293
Iteration 327/1000 | Loss: 0.00014293
Iteration 328/1000 | Loss: 0.00014293
Iteration 329/1000 | Loss: 0.00014293
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 329. Stopping optimization.
Last 5 losses: [0.00014293247659225017, 0.00014293247659225017, 0.00014293247659225017, 0.00014293247659225017, 0.00014293247659225017]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00014293247659225017

Optimization complete. Final v2v error: 6.854928493499756 mm

Highest mean error: 15.067726135253906 mm for frame 204

Lowest mean error: 4.648129463195801 mm for frame 179

Saving results

Total time: 380.36858320236206
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_46_us_2175/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_46_us_2175/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_46_us_2175/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01221802
Iteration 2/25 | Loss: 0.00270321
Iteration 3/25 | Loss: 0.00338547
Iteration 4/25 | Loss: 0.00227522
Iteration 5/25 | Loss: 0.00254320
Iteration 6/25 | Loss: 0.00266191
Iteration 7/25 | Loss: 0.00229047
Iteration 8/25 | Loss: 0.00201903
Iteration 9/25 | Loss: 0.00198794
Iteration 10/25 | Loss: 0.00189605
Iteration 11/25 | Loss: 0.00192491
Iteration 12/25 | Loss: 0.00189018
Iteration 13/25 | Loss: 0.00190260
Iteration 14/25 | Loss: 0.00182357
Iteration 15/25 | Loss: 0.00180902
Iteration 16/25 | Loss: 0.00186339
Iteration 17/25 | Loss: 0.00180613
Iteration 18/25 | Loss: 0.00184220
Iteration 19/25 | Loss: 0.00177042
Iteration 20/25 | Loss: 0.00179427
Iteration 21/25 | Loss: 0.00174226
Iteration 22/25 | Loss: 0.00173858
Iteration 23/25 | Loss: 0.00173286
Iteration 24/25 | Loss: 0.00173372
Iteration 25/25 | Loss: 0.00172682

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52402520
Iteration 2/25 | Loss: 0.00304036
Iteration 3/25 | Loss: 0.00226446
Iteration 4/25 | Loss: 0.00226446
Iteration 5/25 | Loss: 0.00226446
Iteration 6/25 | Loss: 0.00226446
Iteration 7/25 | Loss: 0.00226446
Iteration 8/25 | Loss: 0.00226446
Iteration 9/25 | Loss: 0.00226446
Iteration 10/25 | Loss: 0.00226446
Iteration 11/25 | Loss: 0.00226446
Iteration 12/25 | Loss: 0.00226446
Iteration 13/25 | Loss: 0.00226446
Iteration 14/25 | Loss: 0.00226446
Iteration 15/25 | Loss: 0.00226446
Iteration 16/25 | Loss: 0.00226446
Iteration 17/25 | Loss: 0.00226446
Iteration 18/25 | Loss: 0.00226446
Iteration 19/25 | Loss: 0.00226446
Iteration 20/25 | Loss: 0.00226446
Iteration 21/25 | Loss: 0.00226446
Iteration 22/25 | Loss: 0.00226446
Iteration 23/25 | Loss: 0.00226446
Iteration 24/25 | Loss: 0.00226446
Iteration 25/25 | Loss: 0.00226446

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00226446
Iteration 2/1000 | Loss: 0.00024215
Iteration 3/1000 | Loss: 0.00019528
Iteration 4/1000 | Loss: 0.00022076
Iteration 5/1000 | Loss: 0.00020535
Iteration 6/1000 | Loss: 0.00021319
Iteration 7/1000 | Loss: 0.00021233
Iteration 8/1000 | Loss: 0.00020011
Iteration 9/1000 | Loss: 0.00014866
Iteration 10/1000 | Loss: 0.00017352
Iteration 11/1000 | Loss: 0.00022706
Iteration 12/1000 | Loss: 0.00020460
Iteration 13/1000 | Loss: 0.00016245
Iteration 14/1000 | Loss: 0.00017308
Iteration 15/1000 | Loss: 0.00021577
Iteration 16/1000 | Loss: 0.00018349
Iteration 17/1000 | Loss: 0.00020203
Iteration 18/1000 | Loss: 0.00024834
Iteration 19/1000 | Loss: 0.00016056
Iteration 20/1000 | Loss: 0.00014592
Iteration 21/1000 | Loss: 0.00014248
Iteration 22/1000 | Loss: 0.00014450
Iteration 23/1000 | Loss: 0.00015684
Iteration 24/1000 | Loss: 0.00014561
Iteration 25/1000 | Loss: 0.00012293
Iteration 26/1000 | Loss: 0.00014227
Iteration 27/1000 | Loss: 0.00012068
Iteration 28/1000 | Loss: 0.00015520
Iteration 29/1000 | Loss: 0.00013778
Iteration 30/1000 | Loss: 0.00013017
Iteration 31/1000 | Loss: 0.00013645
Iteration 32/1000 | Loss: 0.00014930
Iteration 33/1000 | Loss: 0.00014481
Iteration 34/1000 | Loss: 0.00013960
Iteration 35/1000 | Loss: 0.00013782
Iteration 36/1000 | Loss: 0.00014263
Iteration 37/1000 | Loss: 0.00013894
Iteration 38/1000 | Loss: 0.00015977
Iteration 39/1000 | Loss: 0.00013313
Iteration 40/1000 | Loss: 0.00014512
Iteration 41/1000 | Loss: 0.00012692
Iteration 42/1000 | Loss: 0.00013612
Iteration 43/1000 | Loss: 0.00012178
Iteration 44/1000 | Loss: 0.00013461
Iteration 45/1000 | Loss: 0.00012432
Iteration 46/1000 | Loss: 0.00014550
Iteration 47/1000 | Loss: 0.00009765
Iteration 48/1000 | Loss: 0.00011373
Iteration 49/1000 | Loss: 0.00010691
Iteration 50/1000 | Loss: 0.00006981
Iteration 51/1000 | Loss: 0.00009458
Iteration 52/1000 | Loss: 0.00008094
Iteration 53/1000 | Loss: 0.00008302
Iteration 54/1000 | Loss: 0.00010187
Iteration 55/1000 | Loss: 0.00011612
Iteration 56/1000 | Loss: 0.00008772
Iteration 57/1000 | Loss: 0.00008154
Iteration 58/1000 | Loss: 0.00008939
Iteration 59/1000 | Loss: 0.00009414
Iteration 60/1000 | Loss: 0.00008830
Iteration 61/1000 | Loss: 0.00011406
Iteration 62/1000 | Loss: 0.00007816
Iteration 63/1000 | Loss: 0.00007923
Iteration 64/1000 | Loss: 0.00054478
Iteration 65/1000 | Loss: 0.00027421
Iteration 66/1000 | Loss: 0.00059543
Iteration 67/1000 | Loss: 0.00026018
Iteration 68/1000 | Loss: 0.00043009
Iteration 69/1000 | Loss: 0.00029696
Iteration 70/1000 | Loss: 0.00038812
Iteration 71/1000 | Loss: 0.00095080
Iteration 72/1000 | Loss: 0.00071904
Iteration 73/1000 | Loss: 0.00065130
Iteration 74/1000 | Loss: 0.00069500
Iteration 75/1000 | Loss: 0.00046257
Iteration 76/1000 | Loss: 0.00005114
Iteration 77/1000 | Loss: 0.00004498
Iteration 78/1000 | Loss: 0.00004210
Iteration 79/1000 | Loss: 0.00004028
Iteration 80/1000 | Loss: 0.00003937
Iteration 81/1000 | Loss: 0.00003860
Iteration 82/1000 | Loss: 0.00003823
Iteration 83/1000 | Loss: 0.00003784
Iteration 84/1000 | Loss: 0.00003762
Iteration 85/1000 | Loss: 0.00003744
Iteration 86/1000 | Loss: 0.00003731
Iteration 87/1000 | Loss: 0.00003730
Iteration 88/1000 | Loss: 0.00003719
Iteration 89/1000 | Loss: 0.00003718
Iteration 90/1000 | Loss: 0.00003718
Iteration 91/1000 | Loss: 0.00003717
Iteration 92/1000 | Loss: 0.00003717
Iteration 93/1000 | Loss: 0.00003716
Iteration 94/1000 | Loss: 0.00003716
Iteration 95/1000 | Loss: 0.00003716
Iteration 96/1000 | Loss: 0.00003715
Iteration 97/1000 | Loss: 0.00003715
Iteration 98/1000 | Loss: 0.00003715
Iteration 99/1000 | Loss: 0.00003714
Iteration 100/1000 | Loss: 0.00003713
Iteration 101/1000 | Loss: 0.00003713
Iteration 102/1000 | Loss: 0.00003713
Iteration 103/1000 | Loss: 0.00003713
Iteration 104/1000 | Loss: 0.00003713
Iteration 105/1000 | Loss: 0.00003713
Iteration 106/1000 | Loss: 0.00003713
Iteration 107/1000 | Loss: 0.00003713
Iteration 108/1000 | Loss: 0.00003713
Iteration 109/1000 | Loss: 0.00003713
Iteration 110/1000 | Loss: 0.00003712
Iteration 111/1000 | Loss: 0.00003712
Iteration 112/1000 | Loss: 0.00003712
Iteration 113/1000 | Loss: 0.00003712
Iteration 114/1000 | Loss: 0.00003711
Iteration 115/1000 | Loss: 0.00003711
Iteration 116/1000 | Loss: 0.00003711
Iteration 117/1000 | Loss: 0.00003711
Iteration 118/1000 | Loss: 0.00003711
Iteration 119/1000 | Loss: 0.00003710
Iteration 120/1000 | Loss: 0.00003710
Iteration 121/1000 | Loss: 0.00003710
Iteration 122/1000 | Loss: 0.00003710
Iteration 123/1000 | Loss: 0.00003710
Iteration 124/1000 | Loss: 0.00003710
Iteration 125/1000 | Loss: 0.00003710
Iteration 126/1000 | Loss: 0.00003710
Iteration 127/1000 | Loss: 0.00003710
Iteration 128/1000 | Loss: 0.00003710
Iteration 129/1000 | Loss: 0.00003710
Iteration 130/1000 | Loss: 0.00003709
Iteration 131/1000 | Loss: 0.00003709
Iteration 132/1000 | Loss: 0.00003709
Iteration 133/1000 | Loss: 0.00003709
Iteration 134/1000 | Loss: 0.00003709
Iteration 135/1000 | Loss: 0.00003709
Iteration 136/1000 | Loss: 0.00003709
Iteration 137/1000 | Loss: 0.00003708
Iteration 138/1000 | Loss: 0.00003708
Iteration 139/1000 | Loss: 0.00003708
Iteration 140/1000 | Loss: 0.00003708
Iteration 141/1000 | Loss: 0.00003708
Iteration 142/1000 | Loss: 0.00003707
Iteration 143/1000 | Loss: 0.00003707
Iteration 144/1000 | Loss: 0.00003707
Iteration 145/1000 | Loss: 0.00003707
Iteration 146/1000 | Loss: 0.00003707
Iteration 147/1000 | Loss: 0.00003707
Iteration 148/1000 | Loss: 0.00003707
Iteration 149/1000 | Loss: 0.00003707
Iteration 150/1000 | Loss: 0.00003706
Iteration 151/1000 | Loss: 0.00003706
Iteration 152/1000 | Loss: 0.00003706
Iteration 153/1000 | Loss: 0.00003706
Iteration 154/1000 | Loss: 0.00003706
Iteration 155/1000 | Loss: 0.00003706
Iteration 156/1000 | Loss: 0.00003706
Iteration 157/1000 | Loss: 0.00003706
Iteration 158/1000 | Loss: 0.00003706
Iteration 159/1000 | Loss: 0.00003706
Iteration 160/1000 | Loss: 0.00003706
Iteration 161/1000 | Loss: 0.00003706
Iteration 162/1000 | Loss: 0.00003706
Iteration 163/1000 | Loss: 0.00003706
Iteration 164/1000 | Loss: 0.00003705
Iteration 165/1000 | Loss: 0.00003705
Iteration 166/1000 | Loss: 0.00003705
Iteration 167/1000 | Loss: 0.00003705
Iteration 168/1000 | Loss: 0.00003705
Iteration 169/1000 | Loss: 0.00003705
Iteration 170/1000 | Loss: 0.00003705
Iteration 171/1000 | Loss: 0.00003705
Iteration 172/1000 | Loss: 0.00003705
Iteration 173/1000 | Loss: 0.00003704
Iteration 174/1000 | Loss: 0.00003704
Iteration 175/1000 | Loss: 0.00003704
Iteration 176/1000 | Loss: 0.00003704
Iteration 177/1000 | Loss: 0.00003704
Iteration 178/1000 | Loss: 0.00003704
Iteration 179/1000 | Loss: 0.00003704
Iteration 180/1000 | Loss: 0.00003704
Iteration 181/1000 | Loss: 0.00003703
Iteration 182/1000 | Loss: 0.00003703
Iteration 183/1000 | Loss: 0.00003703
Iteration 184/1000 | Loss: 0.00003703
Iteration 185/1000 | Loss: 0.00003703
Iteration 186/1000 | Loss: 0.00003703
Iteration 187/1000 | Loss: 0.00003703
Iteration 188/1000 | Loss: 0.00003703
Iteration 189/1000 | Loss: 0.00003703
Iteration 190/1000 | Loss: 0.00003703
Iteration 191/1000 | Loss: 0.00003703
Iteration 192/1000 | Loss: 0.00003703
Iteration 193/1000 | Loss: 0.00003703
Iteration 194/1000 | Loss: 0.00003703
Iteration 195/1000 | Loss: 0.00003703
Iteration 196/1000 | Loss: 0.00003703
Iteration 197/1000 | Loss: 0.00003702
Iteration 198/1000 | Loss: 0.00003702
Iteration 199/1000 | Loss: 0.00003702
Iteration 200/1000 | Loss: 0.00003702
Iteration 201/1000 | Loss: 0.00003702
Iteration 202/1000 | Loss: 0.00003702
Iteration 203/1000 | Loss: 0.00003702
Iteration 204/1000 | Loss: 0.00003702
Iteration 205/1000 | Loss: 0.00003701
Iteration 206/1000 | Loss: 0.00003701
Iteration 207/1000 | Loss: 0.00003701
Iteration 208/1000 | Loss: 0.00003701
Iteration 209/1000 | Loss: 0.00003701
Iteration 210/1000 | Loss: 0.00003701
Iteration 211/1000 | Loss: 0.00003701
Iteration 212/1000 | Loss: 0.00003701
Iteration 213/1000 | Loss: 0.00003701
Iteration 214/1000 | Loss: 0.00003701
Iteration 215/1000 | Loss: 0.00003701
Iteration 216/1000 | Loss: 0.00003700
Iteration 217/1000 | Loss: 0.00003700
Iteration 218/1000 | Loss: 0.00003700
Iteration 219/1000 | Loss: 0.00003700
Iteration 220/1000 | Loss: 0.00003700
Iteration 221/1000 | Loss: 0.00003700
Iteration 222/1000 | Loss: 0.00003700
Iteration 223/1000 | Loss: 0.00003700
Iteration 224/1000 | Loss: 0.00003700
Iteration 225/1000 | Loss: 0.00003700
Iteration 226/1000 | Loss: 0.00003700
Iteration 227/1000 | Loss: 0.00003700
Iteration 228/1000 | Loss: 0.00003700
Iteration 229/1000 | Loss: 0.00003700
Iteration 230/1000 | Loss: 0.00003700
Iteration 231/1000 | Loss: 0.00003700
Iteration 232/1000 | Loss: 0.00003700
Iteration 233/1000 | Loss: 0.00003700
Iteration 234/1000 | Loss: 0.00003700
Iteration 235/1000 | Loss: 0.00003700
Iteration 236/1000 | Loss: 0.00003700
Iteration 237/1000 | Loss: 0.00003699
Iteration 238/1000 | Loss: 0.00003699
Iteration 239/1000 | Loss: 0.00003699
Iteration 240/1000 | Loss: 0.00003699
Iteration 241/1000 | Loss: 0.00003699
Iteration 242/1000 | Loss: 0.00003699
Iteration 243/1000 | Loss: 0.00003699
Iteration 244/1000 | Loss: 0.00003699
Iteration 245/1000 | Loss: 0.00003699
Iteration 246/1000 | Loss: 0.00003699
Iteration 247/1000 | Loss: 0.00003699
Iteration 248/1000 | Loss: 0.00003699
Iteration 249/1000 | Loss: 0.00003699
Iteration 250/1000 | Loss: 0.00003699
Iteration 251/1000 | Loss: 0.00003699
Iteration 252/1000 | Loss: 0.00003699
Iteration 253/1000 | Loss: 0.00003699
Iteration 254/1000 | Loss: 0.00003699
Iteration 255/1000 | Loss: 0.00003699
Iteration 256/1000 | Loss: 0.00003699
Iteration 257/1000 | Loss: 0.00003699
Iteration 258/1000 | Loss: 0.00003699
Iteration 259/1000 | Loss: 0.00003699
Iteration 260/1000 | Loss: 0.00003699
Iteration 261/1000 | Loss: 0.00003699
Iteration 262/1000 | Loss: 0.00003699
Iteration 263/1000 | Loss: 0.00003699
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 263. Stopping optimization.
Last 5 losses: [3.69910012523178e-05, 3.69910012523178e-05, 3.69910012523178e-05, 3.69910012523178e-05, 3.69910012523178e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.69910012523178e-05

Optimization complete. Final v2v error: 5.284762382507324 mm

Highest mean error: 11.221415519714355 mm for frame 49

Lowest mean error: 4.7306084632873535 mm for frame 65

Saving results

Total time: 182.40245056152344
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_46_us_2175/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_46_us_2175/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_46_us_2175/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00520696
Iteration 2/25 | Loss: 0.00198274
Iteration 3/25 | Loss: 0.00185859
Iteration 4/25 | Loss: 0.00184288
Iteration 5/25 | Loss: 0.00183748
Iteration 6/25 | Loss: 0.00183616
Iteration 7/25 | Loss: 0.00183591
Iteration 8/25 | Loss: 0.00183591
Iteration 9/25 | Loss: 0.00183591
Iteration 10/25 | Loss: 0.00183591
Iteration 11/25 | Loss: 0.00183591
Iteration 12/25 | Loss: 0.00183591
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0018359062960371375, 0.0018359062960371375, 0.0018359062960371375, 0.0018359062960371375, 0.0018359062960371375]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018359062960371375

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26502526
Iteration 2/25 | Loss: 0.00261815
Iteration 3/25 | Loss: 0.00261815
Iteration 4/25 | Loss: 0.00261815
Iteration 5/25 | Loss: 0.00261815
Iteration 6/25 | Loss: 0.00261815
Iteration 7/25 | Loss: 0.00261815
Iteration 8/25 | Loss: 0.00261815
Iteration 9/25 | Loss: 0.00261815
Iteration 10/25 | Loss: 0.00261815
Iteration 11/25 | Loss: 0.00261815
Iteration 12/25 | Loss: 0.00261815
Iteration 13/25 | Loss: 0.00261815
Iteration 14/25 | Loss: 0.00261815
Iteration 15/25 | Loss: 0.00261815
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0026181507855653763, 0.0026181507855653763, 0.0026181507855653763, 0.0026181507855653763, 0.0026181507855653763]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026181507855653763

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00261815
Iteration 2/1000 | Loss: 0.00006574
Iteration 3/1000 | Loss: 0.00004971
Iteration 4/1000 | Loss: 0.00004162
Iteration 5/1000 | Loss: 0.00003948
Iteration 6/1000 | Loss: 0.00003840
Iteration 7/1000 | Loss: 0.00003779
Iteration 8/1000 | Loss: 0.00003724
Iteration 9/1000 | Loss: 0.00003684
Iteration 10/1000 | Loss: 0.00003651
Iteration 11/1000 | Loss: 0.00003622
Iteration 12/1000 | Loss: 0.00003607
Iteration 13/1000 | Loss: 0.00003606
Iteration 14/1000 | Loss: 0.00003599
Iteration 15/1000 | Loss: 0.00003598
Iteration 16/1000 | Loss: 0.00003597
Iteration 17/1000 | Loss: 0.00003597
Iteration 18/1000 | Loss: 0.00003596
Iteration 19/1000 | Loss: 0.00003596
Iteration 20/1000 | Loss: 0.00003596
Iteration 21/1000 | Loss: 0.00003596
Iteration 22/1000 | Loss: 0.00003596
Iteration 23/1000 | Loss: 0.00003596
Iteration 24/1000 | Loss: 0.00003596
Iteration 25/1000 | Loss: 0.00003596
Iteration 26/1000 | Loss: 0.00003596
Iteration 27/1000 | Loss: 0.00003596
Iteration 28/1000 | Loss: 0.00003595
Iteration 29/1000 | Loss: 0.00003595
Iteration 30/1000 | Loss: 0.00003595
Iteration 31/1000 | Loss: 0.00003595
Iteration 32/1000 | Loss: 0.00003594
Iteration 33/1000 | Loss: 0.00003594
Iteration 34/1000 | Loss: 0.00003594
Iteration 35/1000 | Loss: 0.00003594
Iteration 36/1000 | Loss: 0.00003594
Iteration 37/1000 | Loss: 0.00003594
Iteration 38/1000 | Loss: 0.00003594
Iteration 39/1000 | Loss: 0.00003594
Iteration 40/1000 | Loss: 0.00003594
Iteration 41/1000 | Loss: 0.00003594
Iteration 42/1000 | Loss: 0.00003594
Iteration 43/1000 | Loss: 0.00003594
Iteration 44/1000 | Loss: 0.00003594
Iteration 45/1000 | Loss: 0.00003593
Iteration 46/1000 | Loss: 0.00003593
Iteration 47/1000 | Loss: 0.00003593
Iteration 48/1000 | Loss: 0.00003593
Iteration 49/1000 | Loss: 0.00003593
Iteration 50/1000 | Loss: 0.00003593
Iteration 51/1000 | Loss: 0.00003593
Iteration 52/1000 | Loss: 0.00003593
Iteration 53/1000 | Loss: 0.00003593
Iteration 54/1000 | Loss: 0.00003593
Iteration 55/1000 | Loss: 0.00003593
Iteration 56/1000 | Loss: 0.00003593
Iteration 57/1000 | Loss: 0.00003593
Iteration 58/1000 | Loss: 0.00003593
Iteration 59/1000 | Loss: 0.00003593
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 59. Stopping optimization.
Last 5 losses: [3.593228029785678e-05, 3.593228029785678e-05, 3.593228029785678e-05, 3.593228029785678e-05, 3.593228029785678e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.593228029785678e-05

Optimization complete. Final v2v error: 5.248346328735352 mm

Highest mean error: 5.364288330078125 mm for frame 25

Lowest mean error: 5.1114349365234375 mm for frame 0

Saving results

Total time: 28.134124040603638
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_46_us_2175/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_46_us_2175/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_46_us_2175/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00515941
Iteration 2/25 | Loss: 0.00196263
Iteration 3/25 | Loss: 0.00180495
Iteration 4/25 | Loss: 0.00179270
Iteration 5/25 | Loss: 0.00178687
Iteration 6/25 | Loss: 0.00178578
Iteration 7/25 | Loss: 0.00178578
Iteration 8/25 | Loss: 0.00178578
Iteration 9/25 | Loss: 0.00178578
Iteration 10/25 | Loss: 0.00178578
Iteration 11/25 | Loss: 0.00178578
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0017857804195955396, 0.0017857804195955396, 0.0017857804195955396, 0.0017857804195955396, 0.0017857804195955396]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017857804195955396

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48474562
Iteration 2/25 | Loss: 0.00228099
Iteration 3/25 | Loss: 0.00228099
Iteration 4/25 | Loss: 0.00228099
Iteration 5/25 | Loss: 0.00228099
Iteration 6/25 | Loss: 0.00228099
Iteration 7/25 | Loss: 0.00228099
Iteration 8/25 | Loss: 0.00228099
Iteration 9/25 | Loss: 0.00228099
Iteration 10/25 | Loss: 0.00228099
Iteration 11/25 | Loss: 0.00228099
Iteration 12/25 | Loss: 0.00228098
Iteration 13/25 | Loss: 0.00228098
Iteration 14/25 | Loss: 0.00228098
Iteration 15/25 | Loss: 0.00228098
Iteration 16/25 | Loss: 0.00228098
Iteration 17/25 | Loss: 0.00228098
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0022809847723692656, 0.0022809847723692656, 0.0022809847723692656, 0.0022809847723692656, 0.0022809847723692656]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022809847723692656

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00228098
Iteration 2/1000 | Loss: 0.00007151
Iteration 3/1000 | Loss: 0.00005731
Iteration 4/1000 | Loss: 0.00005115
Iteration 5/1000 | Loss: 0.00004901
Iteration 6/1000 | Loss: 0.00004735
Iteration 7/1000 | Loss: 0.00004648
Iteration 8/1000 | Loss: 0.00004566
Iteration 9/1000 | Loss: 0.00004517
Iteration 10/1000 | Loss: 0.00004474
Iteration 11/1000 | Loss: 0.00004443
Iteration 12/1000 | Loss: 0.00004425
Iteration 13/1000 | Loss: 0.00004404
Iteration 14/1000 | Loss: 0.00004386
Iteration 15/1000 | Loss: 0.00004381
Iteration 16/1000 | Loss: 0.00004378
Iteration 17/1000 | Loss: 0.00004377
Iteration 18/1000 | Loss: 0.00004376
Iteration 19/1000 | Loss: 0.00004376
Iteration 20/1000 | Loss: 0.00004374
Iteration 21/1000 | Loss: 0.00004374
Iteration 22/1000 | Loss: 0.00004374
Iteration 23/1000 | Loss: 0.00004373
Iteration 24/1000 | Loss: 0.00004373
Iteration 25/1000 | Loss: 0.00004372
Iteration 26/1000 | Loss: 0.00004369
Iteration 27/1000 | Loss: 0.00004369
Iteration 28/1000 | Loss: 0.00004368
Iteration 29/1000 | Loss: 0.00004365
Iteration 30/1000 | Loss: 0.00004363
Iteration 31/1000 | Loss: 0.00004360
Iteration 32/1000 | Loss: 0.00004360
Iteration 33/1000 | Loss: 0.00004359
Iteration 34/1000 | Loss: 0.00004359
Iteration 35/1000 | Loss: 0.00004359
Iteration 36/1000 | Loss: 0.00004359
Iteration 37/1000 | Loss: 0.00004356
Iteration 38/1000 | Loss: 0.00004356
Iteration 39/1000 | Loss: 0.00004356
Iteration 40/1000 | Loss: 0.00004356
Iteration 41/1000 | Loss: 0.00004356
Iteration 42/1000 | Loss: 0.00004356
Iteration 43/1000 | Loss: 0.00004356
Iteration 44/1000 | Loss: 0.00004355
Iteration 45/1000 | Loss: 0.00004355
Iteration 46/1000 | Loss: 0.00004352
Iteration 47/1000 | Loss: 0.00004352
Iteration 48/1000 | Loss: 0.00004352
Iteration 49/1000 | Loss: 0.00004351
Iteration 50/1000 | Loss: 0.00004350
Iteration 51/1000 | Loss: 0.00004350
Iteration 52/1000 | Loss: 0.00004350
Iteration 53/1000 | Loss: 0.00004349
Iteration 54/1000 | Loss: 0.00004349
Iteration 55/1000 | Loss: 0.00004349
Iteration 56/1000 | Loss: 0.00004349
Iteration 57/1000 | Loss: 0.00004349
Iteration 58/1000 | Loss: 0.00004349
Iteration 59/1000 | Loss: 0.00004349
Iteration 60/1000 | Loss: 0.00004349
Iteration 61/1000 | Loss: 0.00004349
Iteration 62/1000 | Loss: 0.00004349
Iteration 63/1000 | Loss: 0.00004349
Iteration 64/1000 | Loss: 0.00004349
Iteration 65/1000 | Loss: 0.00004349
Iteration 66/1000 | Loss: 0.00004349
Iteration 67/1000 | Loss: 0.00004349
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 67. Stopping optimization.
Last 5 losses: [4.348938455223106e-05, 4.348938455223106e-05, 4.348938455223106e-05, 4.348938455223106e-05, 4.348938455223106e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.348938455223106e-05

Optimization complete. Final v2v error: 5.738282680511475 mm

Highest mean error: 6.120488166809082 mm for frame 200

Lowest mean error: 5.4307026863098145 mm for frame 100

Saving results

Total time: 39.432896852493286
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_46_us_2175/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_46_us_2175/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_46_us_2175/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01084019
Iteration 2/25 | Loss: 0.00243803
Iteration 3/25 | Loss: 0.00176794
Iteration 4/25 | Loss: 0.00165360
Iteration 5/25 | Loss: 0.00164447
Iteration 6/25 | Loss: 0.00154873
Iteration 7/25 | Loss: 0.00159322
Iteration 8/25 | Loss: 0.00153296
Iteration 9/25 | Loss: 0.00152965
Iteration 10/25 | Loss: 0.00152398
Iteration 11/25 | Loss: 0.00152178
Iteration 12/25 | Loss: 0.00151788
Iteration 13/25 | Loss: 0.00151658
Iteration 14/25 | Loss: 0.00151782
Iteration 15/25 | Loss: 0.00151667
Iteration 16/25 | Loss: 0.00151902
Iteration 17/25 | Loss: 0.00151632
Iteration 18/25 | Loss: 0.00151884
Iteration 19/25 | Loss: 0.00151682
Iteration 20/25 | Loss: 0.00151893
Iteration 21/25 | Loss: 0.00151728
Iteration 22/25 | Loss: 0.00151808
Iteration 23/25 | Loss: 0.00151581
Iteration 24/25 | Loss: 0.00151716
Iteration 25/25 | Loss: 0.00151891

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.97986758
Iteration 2/25 | Loss: 0.00177571
Iteration 3/25 | Loss: 0.00177571
Iteration 4/25 | Loss: 0.00177571
Iteration 5/25 | Loss: 0.00177571
Iteration 6/25 | Loss: 0.00177571
Iteration 7/25 | Loss: 0.00177571
Iteration 8/25 | Loss: 0.00177571
Iteration 9/25 | Loss: 0.00177571
Iteration 10/25 | Loss: 0.00177571
Iteration 11/25 | Loss: 0.00177571
Iteration 12/25 | Loss: 0.00177571
Iteration 13/25 | Loss: 0.00177571
Iteration 14/25 | Loss: 0.00177571
Iteration 15/25 | Loss: 0.00177571
Iteration 16/25 | Loss: 0.00177571
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0017757081659510732, 0.0017757081659510732, 0.0017757081659510732, 0.0017757081659510732, 0.0017757081659510732]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017757081659510732

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00177571
Iteration 2/1000 | Loss: 0.00009090
Iteration 3/1000 | Loss: 0.00006300
Iteration 4/1000 | Loss: 0.00022415
Iteration 5/1000 | Loss: 0.00005272
Iteration 6/1000 | Loss: 0.00005023
Iteration 7/1000 | Loss: 0.00018037
Iteration 8/1000 | Loss: 0.00010442
Iteration 9/1000 | Loss: 0.00006734
Iteration 10/1000 | Loss: 0.00004768
Iteration 11/1000 | Loss: 0.00007062
Iteration 12/1000 | Loss: 0.00005263
Iteration 13/1000 | Loss: 0.00006569
Iteration 14/1000 | Loss: 0.00005410
Iteration 15/1000 | Loss: 0.00004818
Iteration 16/1000 | Loss: 0.00004647
Iteration 17/1000 | Loss: 0.00004601
Iteration 18/1000 | Loss: 0.00004584
Iteration 19/1000 | Loss: 0.00004564
Iteration 20/1000 | Loss: 0.00004548
Iteration 21/1000 | Loss: 0.00004543
Iteration 22/1000 | Loss: 0.00004542
Iteration 23/1000 | Loss: 0.00004541
Iteration 24/1000 | Loss: 0.00004540
Iteration 25/1000 | Loss: 0.00004539
Iteration 26/1000 | Loss: 0.00004537
Iteration 27/1000 | Loss: 0.00004536
Iteration 28/1000 | Loss: 0.00004533
Iteration 29/1000 | Loss: 0.00004527
Iteration 30/1000 | Loss: 0.00004526
Iteration 31/1000 | Loss: 0.00004525
Iteration 32/1000 | Loss: 0.00004525
Iteration 33/1000 | Loss: 0.00004524
Iteration 34/1000 | Loss: 0.00004524
Iteration 35/1000 | Loss: 0.00004524
Iteration 36/1000 | Loss: 0.00004523
Iteration 37/1000 | Loss: 0.00004520
Iteration 38/1000 | Loss: 0.00004519
Iteration 39/1000 | Loss: 0.00004519
Iteration 40/1000 | Loss: 0.00004519
Iteration 41/1000 | Loss: 0.00004519
Iteration 42/1000 | Loss: 0.00004519
Iteration 43/1000 | Loss: 0.00004519
Iteration 44/1000 | Loss: 0.00004519
Iteration 45/1000 | Loss: 0.00004519
Iteration 46/1000 | Loss: 0.00004519
Iteration 47/1000 | Loss: 0.00004519
Iteration 48/1000 | Loss: 0.00004519
Iteration 49/1000 | Loss: 0.00004519
Iteration 50/1000 | Loss: 0.00004519
Iteration 51/1000 | Loss: 0.00004519
Iteration 52/1000 | Loss: 0.00004518
Iteration 53/1000 | Loss: 0.00004518
Iteration 54/1000 | Loss: 0.00004518
Iteration 55/1000 | Loss: 0.00004518
Iteration 56/1000 | Loss: 0.00004517
Iteration 57/1000 | Loss: 0.00004517
Iteration 58/1000 | Loss: 0.00004517
Iteration 59/1000 | Loss: 0.00004516
Iteration 60/1000 | Loss: 0.00004516
Iteration 61/1000 | Loss: 0.00004515
Iteration 62/1000 | Loss: 0.00004746
Iteration 63/1000 | Loss: 0.00004582
Iteration 64/1000 | Loss: 0.00004514
Iteration 65/1000 | Loss: 0.00004514
Iteration 66/1000 | Loss: 0.00004514
Iteration 67/1000 | Loss: 0.00004513
Iteration 68/1000 | Loss: 0.00004513
Iteration 69/1000 | Loss: 0.00004513
Iteration 70/1000 | Loss: 0.00004513
Iteration 71/1000 | Loss: 0.00004513
Iteration 72/1000 | Loss: 0.00004513
Iteration 73/1000 | Loss: 0.00004513
Iteration 74/1000 | Loss: 0.00004513
Iteration 75/1000 | Loss: 0.00004513
Iteration 76/1000 | Loss: 0.00004513
Iteration 77/1000 | Loss: 0.00004512
Iteration 78/1000 | Loss: 0.00004512
Iteration 79/1000 | Loss: 0.00004512
Iteration 80/1000 | Loss: 0.00004512
Iteration 81/1000 | Loss: 0.00004512
Iteration 82/1000 | Loss: 0.00004512
Iteration 83/1000 | Loss: 0.00004512
Iteration 84/1000 | Loss: 0.00004512
Iteration 85/1000 | Loss: 0.00004512
Iteration 86/1000 | Loss: 0.00004512
Iteration 87/1000 | Loss: 0.00004512
Iteration 88/1000 | Loss: 0.00004512
Iteration 89/1000 | Loss: 0.00004512
Iteration 90/1000 | Loss: 0.00004512
Iteration 91/1000 | Loss: 0.00004512
Iteration 92/1000 | Loss: 0.00004511
Iteration 93/1000 | Loss: 0.00004511
Iteration 94/1000 | Loss: 0.00004511
Iteration 95/1000 | Loss: 0.00004511
Iteration 96/1000 | Loss: 0.00004511
Iteration 97/1000 | Loss: 0.00004511
Iteration 98/1000 | Loss: 0.00004511
Iteration 99/1000 | Loss: 0.00004511
Iteration 100/1000 | Loss: 0.00004511
Iteration 101/1000 | Loss: 0.00004511
Iteration 102/1000 | Loss: 0.00004510
Iteration 103/1000 | Loss: 0.00004510
Iteration 104/1000 | Loss: 0.00004510
Iteration 105/1000 | Loss: 0.00004510
Iteration 106/1000 | Loss: 0.00004510
Iteration 107/1000 | Loss: 0.00004510
Iteration 108/1000 | Loss: 0.00004510
Iteration 109/1000 | Loss: 0.00004510
Iteration 110/1000 | Loss: 0.00004509
Iteration 111/1000 | Loss: 0.00004509
Iteration 112/1000 | Loss: 0.00004509
Iteration 113/1000 | Loss: 0.00004509
Iteration 114/1000 | Loss: 0.00004509
Iteration 115/1000 | Loss: 0.00004509
Iteration 116/1000 | Loss: 0.00004509
Iteration 117/1000 | Loss: 0.00004508
Iteration 118/1000 | Loss: 0.00004508
Iteration 119/1000 | Loss: 0.00004508
Iteration 120/1000 | Loss: 0.00004508
Iteration 121/1000 | Loss: 0.00004508
Iteration 122/1000 | Loss: 0.00004508
Iteration 123/1000 | Loss: 0.00004508
Iteration 124/1000 | Loss: 0.00004508
Iteration 125/1000 | Loss: 0.00004508
Iteration 126/1000 | Loss: 0.00004508
Iteration 127/1000 | Loss: 0.00004508
Iteration 128/1000 | Loss: 0.00004507
Iteration 129/1000 | Loss: 0.00004507
Iteration 130/1000 | Loss: 0.00004507
Iteration 131/1000 | Loss: 0.00004507
Iteration 132/1000 | Loss: 0.00004723
Iteration 133/1000 | Loss: 0.00004723
Iteration 134/1000 | Loss: 0.00004562
Iteration 135/1000 | Loss: 0.00004507
Iteration 136/1000 | Loss: 0.00004507
Iteration 137/1000 | Loss: 0.00004507
Iteration 138/1000 | Loss: 0.00004507
Iteration 139/1000 | Loss: 0.00004507
Iteration 140/1000 | Loss: 0.00004507
Iteration 141/1000 | Loss: 0.00004506
Iteration 142/1000 | Loss: 0.00004506
Iteration 143/1000 | Loss: 0.00004506
Iteration 144/1000 | Loss: 0.00004506
Iteration 145/1000 | Loss: 0.00004506
Iteration 146/1000 | Loss: 0.00004506
Iteration 147/1000 | Loss: 0.00004506
Iteration 148/1000 | Loss: 0.00004506
Iteration 149/1000 | Loss: 0.00004506
Iteration 150/1000 | Loss: 0.00004506
Iteration 151/1000 | Loss: 0.00004506
Iteration 152/1000 | Loss: 0.00004506
Iteration 153/1000 | Loss: 0.00004506
Iteration 154/1000 | Loss: 0.00004506
Iteration 155/1000 | Loss: 0.00004505
Iteration 156/1000 | Loss: 0.00004505
Iteration 157/1000 | Loss: 0.00004505
Iteration 158/1000 | Loss: 0.00004505
Iteration 159/1000 | Loss: 0.00004505
Iteration 160/1000 | Loss: 0.00004505
Iteration 161/1000 | Loss: 0.00004505
Iteration 162/1000 | Loss: 0.00004505
Iteration 163/1000 | Loss: 0.00004505
Iteration 164/1000 | Loss: 0.00004505
Iteration 165/1000 | Loss: 0.00004505
Iteration 166/1000 | Loss: 0.00004505
Iteration 167/1000 | Loss: 0.00004505
Iteration 168/1000 | Loss: 0.00004504
Iteration 169/1000 | Loss: 0.00004504
Iteration 170/1000 | Loss: 0.00004504
Iteration 171/1000 | Loss: 0.00004504
Iteration 172/1000 | Loss: 0.00004504
Iteration 173/1000 | Loss: 0.00004504
Iteration 174/1000 | Loss: 0.00004504
Iteration 175/1000 | Loss: 0.00004504
Iteration 176/1000 | Loss: 0.00004504
Iteration 177/1000 | Loss: 0.00004504
Iteration 178/1000 | Loss: 0.00004504
Iteration 179/1000 | Loss: 0.00004504
Iteration 180/1000 | Loss: 0.00004504
Iteration 181/1000 | Loss: 0.00004504
Iteration 182/1000 | Loss: 0.00004504
Iteration 183/1000 | Loss: 0.00004504
Iteration 184/1000 | Loss: 0.00004504
Iteration 185/1000 | Loss: 0.00004504
Iteration 186/1000 | Loss: 0.00004504
Iteration 187/1000 | Loss: 0.00004504
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [4.5041542762191966e-05, 4.5041542762191966e-05, 4.5041542762191966e-05, 4.5041542762191966e-05, 4.5041542762191966e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.5041542762191966e-05

Optimization complete. Final v2v error: 5.382260322570801 mm

Highest mean error: 15.556234359741211 mm for frame 206

Lowest mean error: 4.712759017944336 mm for frame 101

Saving results

Total time: 105.24596333503723
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_46_us_2175/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_46_us_2175/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_46_us_2175/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01078718
Iteration 2/25 | Loss: 0.01078718
Iteration 3/25 | Loss: 0.01078718
Iteration 4/25 | Loss: 0.01078718
Iteration 5/25 | Loss: 0.01078718
Iteration 6/25 | Loss: 0.01078718
Iteration 7/25 | Loss: 0.01078717
Iteration 8/25 | Loss: 0.01078717
Iteration 9/25 | Loss: 0.01078717
Iteration 10/25 | Loss: 0.01078717
Iteration 11/25 | Loss: 0.01078717
Iteration 12/25 | Loss: 0.01078717
Iteration 13/25 | Loss: 0.01078717
Iteration 14/25 | Loss: 0.01078717
Iteration 15/25 | Loss: 0.01078716
Iteration 16/25 | Loss: 0.01078716
Iteration 17/25 | Loss: 0.01078716
Iteration 18/25 | Loss: 0.01078716
Iteration 19/25 | Loss: 0.01078716
Iteration 20/25 | Loss: 0.01078716
Iteration 21/25 | Loss: 0.01078716
Iteration 22/25 | Loss: 0.01078715
Iteration 23/25 | Loss: 0.01078715
Iteration 24/25 | Loss: 0.01078715
Iteration 25/25 | Loss: 0.01078715

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.73765469
Iteration 2/25 | Loss: 0.16610862
Iteration 3/25 | Loss: 0.16610454
Iteration 4/25 | Loss: 0.16610456
Iteration 5/25 | Loss: 0.16610453
Iteration 6/25 | Loss: 0.16610453
Iteration 7/25 | Loss: 0.16610453
Iteration 8/25 | Loss: 0.16610453
Iteration 9/25 | Loss: 0.16610453
Iteration 10/25 | Loss: 0.16610450
Iteration 11/25 | Loss: 0.16610450
Iteration 12/25 | Loss: 0.16610450
Iteration 13/25 | Loss: 0.16610450
Iteration 14/25 | Loss: 0.16610450
Iteration 15/25 | Loss: 0.16610450
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.1661044955253601, 0.1661044955253601, 0.1661044955253601, 0.1661044955253601, 0.1661044955253601]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.1661044955253601

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.16610453
Iteration 2/1000 | Loss: 0.00971904
Iteration 3/1000 | Loss: 0.00469773
Iteration 4/1000 | Loss: 0.00277618
Iteration 5/1000 | Loss: 0.00184137
Iteration 6/1000 | Loss: 0.00202649
Iteration 7/1000 | Loss: 0.00099796
Iteration 8/1000 | Loss: 0.00832750
Iteration 9/1000 | Loss: 0.00044296
Iteration 10/1000 | Loss: 0.00030387
Iteration 11/1000 | Loss: 0.00049364
Iteration 12/1000 | Loss: 0.00080645
Iteration 13/1000 | Loss: 0.00148477
Iteration 14/1000 | Loss: 0.00102006
Iteration 15/1000 | Loss: 0.00029698
Iteration 16/1000 | Loss: 0.00018158
Iteration 17/1000 | Loss: 0.00131112
Iteration 18/1000 | Loss: 0.00110133
Iteration 19/1000 | Loss: 0.00158929
Iteration 20/1000 | Loss: 0.00019604
Iteration 21/1000 | Loss: 0.00012463
Iteration 22/1000 | Loss: 0.00053566
Iteration 23/1000 | Loss: 0.00115245
Iteration 24/1000 | Loss: 0.00101956
Iteration 25/1000 | Loss: 0.00081223
Iteration 26/1000 | Loss: 0.00029205
Iteration 27/1000 | Loss: 0.00015458
Iteration 28/1000 | Loss: 0.00019219
Iteration 29/1000 | Loss: 0.00016728
Iteration 30/1000 | Loss: 0.00019732
Iteration 31/1000 | Loss: 0.00043937
Iteration 32/1000 | Loss: 0.00009872
Iteration 33/1000 | Loss: 0.00048728
Iteration 34/1000 | Loss: 0.00039378
Iteration 35/1000 | Loss: 0.00009061
Iteration 36/1000 | Loss: 0.00008533
Iteration 37/1000 | Loss: 0.00027773
Iteration 38/1000 | Loss: 0.00063759
Iteration 39/1000 | Loss: 0.00009227
Iteration 40/1000 | Loss: 0.00028508
Iteration 41/1000 | Loss: 0.00108835
Iteration 42/1000 | Loss: 0.00015675
Iteration 43/1000 | Loss: 0.00010035
Iteration 44/1000 | Loss: 0.00007932
Iteration 45/1000 | Loss: 0.00007111
Iteration 46/1000 | Loss: 0.00006718
Iteration 47/1000 | Loss: 0.00006417
Iteration 48/1000 | Loss: 0.00006287
Iteration 49/1000 | Loss: 0.00006166
Iteration 50/1000 | Loss: 0.00023165
Iteration 51/1000 | Loss: 0.00006762
Iteration 52/1000 | Loss: 0.00005923
Iteration 53/1000 | Loss: 0.00005790
Iteration 54/1000 | Loss: 0.00005665
Iteration 55/1000 | Loss: 0.00005536
Iteration 56/1000 | Loss: 0.00006763
Iteration 57/1000 | Loss: 0.00005634
Iteration 58/1000 | Loss: 0.00006905
Iteration 59/1000 | Loss: 0.00009257
Iteration 60/1000 | Loss: 0.00008316
Iteration 61/1000 | Loss: 0.00005709
Iteration 62/1000 | Loss: 0.00008556
Iteration 63/1000 | Loss: 0.00007586
Iteration 64/1000 | Loss: 0.00007813
Iteration 65/1000 | Loss: 0.00012498
Iteration 66/1000 | Loss: 0.00008787
Iteration 67/1000 | Loss: 0.00011833
Iteration 68/1000 | Loss: 0.00009890
Iteration 69/1000 | Loss: 0.00014320
Iteration 70/1000 | Loss: 0.00009776
Iteration 71/1000 | Loss: 0.00006753
Iteration 72/1000 | Loss: 0.00006277
Iteration 73/1000 | Loss: 0.00005822
Iteration 74/1000 | Loss: 0.00008885
Iteration 75/1000 | Loss: 0.00006806
Iteration 76/1000 | Loss: 0.00008968
Iteration 77/1000 | Loss: 0.00006915
Iteration 78/1000 | Loss: 0.00010563
Iteration 79/1000 | Loss: 0.00007572
Iteration 80/1000 | Loss: 0.00008568
Iteration 81/1000 | Loss: 0.00006558
Iteration 82/1000 | Loss: 0.00011041
Iteration 83/1000 | Loss: 0.00007256
Iteration 84/1000 | Loss: 0.00010974
Iteration 85/1000 | Loss: 0.00007389
Iteration 86/1000 | Loss: 0.00011100
Iteration 87/1000 | Loss: 0.00007439
Iteration 88/1000 | Loss: 0.00011233
Iteration 89/1000 | Loss: 0.00007135
Iteration 90/1000 | Loss: 0.00011538
Iteration 91/1000 | Loss: 0.00009072
Iteration 92/1000 | Loss: 0.00005841
Iteration 93/1000 | Loss: 0.00007178
Iteration 94/1000 | Loss: 0.00011163
Iteration 95/1000 | Loss: 0.00006914
Iteration 96/1000 | Loss: 0.00008012
Iteration 97/1000 | Loss: 0.00006338
Iteration 98/1000 | Loss: 0.00010601
Iteration 99/1000 | Loss: 0.00007174
Iteration 100/1000 | Loss: 0.00010307
Iteration 101/1000 | Loss: 0.00010136
Iteration 102/1000 | Loss: 0.00011262
Iteration 103/1000 | Loss: 0.00009702
Iteration 104/1000 | Loss: 0.00008501
Iteration 105/1000 | Loss: 0.00008895
Iteration 106/1000 | Loss: 0.00006632
Iteration 107/1000 | Loss: 0.00008668
Iteration 108/1000 | Loss: 0.00005991
Iteration 109/1000 | Loss: 0.00005583
Iteration 110/1000 | Loss: 0.00007083
Iteration 111/1000 | Loss: 0.00005721
Iteration 112/1000 | Loss: 0.00010013
Iteration 113/1000 | Loss: 0.00006910
Iteration 114/1000 | Loss: 0.00010666
Iteration 115/1000 | Loss: 0.00007336
Iteration 116/1000 | Loss: 0.00009819
Iteration 117/1000 | Loss: 0.00006486
Iteration 118/1000 | Loss: 0.00009701
Iteration 119/1000 | Loss: 0.00006386
Iteration 120/1000 | Loss: 0.00005569
Iteration 121/1000 | Loss: 0.00005458
Iteration 122/1000 | Loss: 0.00005552
Iteration 123/1000 | Loss: 0.00005443
Iteration 124/1000 | Loss: 0.00005587
Iteration 125/1000 | Loss: 0.00005377
Iteration 126/1000 | Loss: 0.00005636
Iteration 127/1000 | Loss: 0.00005363
Iteration 128/1000 | Loss: 0.00005322
Iteration 129/1000 | Loss: 0.00005289
Iteration 130/1000 | Loss: 0.00005288
Iteration 131/1000 | Loss: 0.00005268
Iteration 132/1000 | Loss: 0.00005249
Iteration 133/1000 | Loss: 0.00005237
Iteration 134/1000 | Loss: 0.00005219
Iteration 135/1000 | Loss: 0.00005216
Iteration 136/1000 | Loss: 0.00005209
Iteration 137/1000 | Loss: 0.00005205
Iteration 138/1000 | Loss: 0.00005205
Iteration 139/1000 | Loss: 0.00005205
Iteration 140/1000 | Loss: 0.00005205
Iteration 141/1000 | Loss: 0.00005204
Iteration 142/1000 | Loss: 0.00005204
Iteration 143/1000 | Loss: 0.00005204
Iteration 144/1000 | Loss: 0.00005204
Iteration 145/1000 | Loss: 0.00005204
Iteration 146/1000 | Loss: 0.00005204
Iteration 147/1000 | Loss: 0.00005204
Iteration 148/1000 | Loss: 0.00005204
Iteration 149/1000 | Loss: 0.00005204
Iteration 150/1000 | Loss: 0.00005202
Iteration 151/1000 | Loss: 0.00005201
Iteration 152/1000 | Loss: 0.00005201
Iteration 153/1000 | Loss: 0.00005201
Iteration 154/1000 | Loss: 0.00005201
Iteration 155/1000 | Loss: 0.00005201
Iteration 156/1000 | Loss: 0.00005201
Iteration 157/1000 | Loss: 0.00005201
Iteration 158/1000 | Loss: 0.00005201
Iteration 159/1000 | Loss: 0.00005201
Iteration 160/1000 | Loss: 0.00005201
Iteration 161/1000 | Loss: 0.00005201
Iteration 162/1000 | Loss: 0.00005201
Iteration 163/1000 | Loss: 0.00005201
Iteration 164/1000 | Loss: 0.00005200
Iteration 165/1000 | Loss: 0.00005200
Iteration 166/1000 | Loss: 0.00005200
Iteration 167/1000 | Loss: 0.00005200
Iteration 168/1000 | Loss: 0.00005200
Iteration 169/1000 | Loss: 0.00005200
Iteration 170/1000 | Loss: 0.00005200
Iteration 171/1000 | Loss: 0.00005200
Iteration 172/1000 | Loss: 0.00005200
Iteration 173/1000 | Loss: 0.00005200
Iteration 174/1000 | Loss: 0.00005200
Iteration 175/1000 | Loss: 0.00005200
Iteration 176/1000 | Loss: 0.00005199
Iteration 177/1000 | Loss: 0.00005199
Iteration 178/1000 | Loss: 0.00005199
Iteration 179/1000 | Loss: 0.00005199
Iteration 180/1000 | Loss: 0.00005199
Iteration 181/1000 | Loss: 0.00005199
Iteration 182/1000 | Loss: 0.00005199
Iteration 183/1000 | Loss: 0.00005199
Iteration 184/1000 | Loss: 0.00005199
Iteration 185/1000 | Loss: 0.00005199
Iteration 186/1000 | Loss: 0.00005199
Iteration 187/1000 | Loss: 0.00005199
Iteration 188/1000 | Loss: 0.00005199
Iteration 189/1000 | Loss: 0.00005198
Iteration 190/1000 | Loss: 0.00005198
Iteration 191/1000 | Loss: 0.00005198
Iteration 192/1000 | Loss: 0.00005198
Iteration 193/1000 | Loss: 0.00005198
Iteration 194/1000 | Loss: 0.00005198
Iteration 195/1000 | Loss: 0.00005198
Iteration 196/1000 | Loss: 0.00005197
Iteration 197/1000 | Loss: 0.00005197
Iteration 198/1000 | Loss: 0.00005197
Iteration 199/1000 | Loss: 0.00005197
Iteration 200/1000 | Loss: 0.00005197
Iteration 201/1000 | Loss: 0.00005197
Iteration 202/1000 | Loss: 0.00005197
Iteration 203/1000 | Loss: 0.00005197
Iteration 204/1000 | Loss: 0.00005197
Iteration 205/1000 | Loss: 0.00005197
Iteration 206/1000 | Loss: 0.00005197
Iteration 207/1000 | Loss: 0.00005197
Iteration 208/1000 | Loss: 0.00005197
Iteration 209/1000 | Loss: 0.00005197
Iteration 210/1000 | Loss: 0.00005197
Iteration 211/1000 | Loss: 0.00005197
Iteration 212/1000 | Loss: 0.00005197
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [5.197275822865777e-05, 5.197275822865777e-05, 5.197275822865777e-05, 5.197275822865777e-05, 5.197275822865777e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.197275822865777e-05

Optimization complete. Final v2v error: 5.931395530700684 mm

Highest mean error: 12.837839126586914 mm for frame 234

Lowest mean error: 5.233762741088867 mm for frame 0

Saving results

Total time: 230.80819630622864
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_46_us_2175/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_46_us_2175/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_46_us_2175/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00971568
Iteration 2/25 | Loss: 0.00200459
Iteration 3/25 | Loss: 0.00188460
Iteration 4/25 | Loss: 0.00185962
Iteration 5/25 | Loss: 0.00185104
Iteration 6/25 | Loss: 0.00184909
Iteration 7/25 | Loss: 0.00184864
Iteration 8/25 | Loss: 0.00184864
Iteration 9/25 | Loss: 0.00184864
Iteration 10/25 | Loss: 0.00184864
Iteration 11/25 | Loss: 0.00184864
Iteration 12/25 | Loss: 0.00184864
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0018486360786482692, 0.0018486360786482692, 0.0018486360786482692, 0.0018486360786482692, 0.0018486360786482692]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018486360786482692

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66958129
Iteration 2/25 | Loss: 0.00319592
Iteration 3/25 | Loss: 0.00319592
Iteration 4/25 | Loss: 0.00319592
Iteration 5/25 | Loss: 0.00319592
Iteration 6/25 | Loss: 0.00319592
Iteration 7/25 | Loss: 0.00319592
Iteration 8/25 | Loss: 0.00319592
Iteration 9/25 | Loss: 0.00319592
Iteration 10/25 | Loss: 0.00319592
Iteration 11/25 | Loss: 0.00319592
Iteration 12/25 | Loss: 0.00319592
Iteration 13/25 | Loss: 0.00319592
Iteration 14/25 | Loss: 0.00319592
Iteration 15/25 | Loss: 0.00319592
Iteration 16/25 | Loss: 0.00319592
Iteration 17/25 | Loss: 0.00319592
Iteration 18/25 | Loss: 0.00319592
Iteration 19/25 | Loss: 0.00319592
Iteration 20/25 | Loss: 0.00319592
Iteration 21/25 | Loss: 0.00319592
Iteration 22/25 | Loss: 0.00319592
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.003195916535332799, 0.003195916535332799, 0.003195916535332799, 0.003195916535332799, 0.003195916535332799]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003195916535332799

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00319592
Iteration 2/1000 | Loss: 0.00011885
Iteration 3/1000 | Loss: 0.00006902
Iteration 4/1000 | Loss: 0.00005674
Iteration 5/1000 | Loss: 0.00005117
Iteration 6/1000 | Loss: 0.00004821
Iteration 7/1000 | Loss: 0.00004603
Iteration 8/1000 | Loss: 0.00004423
Iteration 9/1000 | Loss: 0.00004303
Iteration 10/1000 | Loss: 0.00004200
Iteration 11/1000 | Loss: 0.00004143
Iteration 12/1000 | Loss: 0.00004092
Iteration 13/1000 | Loss: 0.00004046
Iteration 14/1000 | Loss: 0.00004004
Iteration 15/1000 | Loss: 0.00003975
Iteration 16/1000 | Loss: 0.00003952
Iteration 17/1000 | Loss: 0.00003930
Iteration 18/1000 | Loss: 0.00003915
Iteration 19/1000 | Loss: 0.00003914
Iteration 20/1000 | Loss: 0.00003908
Iteration 21/1000 | Loss: 0.00003903
Iteration 22/1000 | Loss: 0.00003892
Iteration 23/1000 | Loss: 0.00003885
Iteration 24/1000 | Loss: 0.00003879
Iteration 25/1000 | Loss: 0.00003877
Iteration 26/1000 | Loss: 0.00003876
Iteration 27/1000 | Loss: 0.00003876
Iteration 28/1000 | Loss: 0.00003876
Iteration 29/1000 | Loss: 0.00003876
Iteration 30/1000 | Loss: 0.00003876
Iteration 31/1000 | Loss: 0.00003876
Iteration 32/1000 | Loss: 0.00003876
Iteration 33/1000 | Loss: 0.00003876
Iteration 34/1000 | Loss: 0.00003876
Iteration 35/1000 | Loss: 0.00003875
Iteration 36/1000 | Loss: 0.00003875
Iteration 37/1000 | Loss: 0.00003875
Iteration 38/1000 | Loss: 0.00003874
Iteration 39/1000 | Loss: 0.00003874
Iteration 40/1000 | Loss: 0.00003874
Iteration 41/1000 | Loss: 0.00003874
Iteration 42/1000 | Loss: 0.00003874
Iteration 43/1000 | Loss: 0.00003874
Iteration 44/1000 | Loss: 0.00003873
Iteration 45/1000 | Loss: 0.00003873
Iteration 46/1000 | Loss: 0.00003873
Iteration 47/1000 | Loss: 0.00003873
Iteration 48/1000 | Loss: 0.00003873
Iteration 49/1000 | Loss: 0.00003873
Iteration 50/1000 | Loss: 0.00003872
Iteration 51/1000 | Loss: 0.00003872
Iteration 52/1000 | Loss: 0.00003872
Iteration 53/1000 | Loss: 0.00003872
Iteration 54/1000 | Loss: 0.00003872
Iteration 55/1000 | Loss: 0.00003872
Iteration 56/1000 | Loss: 0.00003872
Iteration 57/1000 | Loss: 0.00003872
Iteration 58/1000 | Loss: 0.00003871
Iteration 59/1000 | Loss: 0.00003871
Iteration 60/1000 | Loss: 0.00003871
Iteration 61/1000 | Loss: 0.00003871
Iteration 62/1000 | Loss: 0.00003871
Iteration 63/1000 | Loss: 0.00003871
Iteration 64/1000 | Loss: 0.00003871
Iteration 65/1000 | Loss: 0.00003871
Iteration 66/1000 | Loss: 0.00003871
Iteration 67/1000 | Loss: 0.00003871
Iteration 68/1000 | Loss: 0.00003871
Iteration 69/1000 | Loss: 0.00003870
Iteration 70/1000 | Loss: 0.00003870
Iteration 71/1000 | Loss: 0.00003870
Iteration 72/1000 | Loss: 0.00003870
Iteration 73/1000 | Loss: 0.00003870
Iteration 74/1000 | Loss: 0.00003870
Iteration 75/1000 | Loss: 0.00003870
Iteration 76/1000 | Loss: 0.00003870
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 76. Stopping optimization.
Last 5 losses: [3.8703190512023866e-05, 3.8703190512023866e-05, 3.8703190512023866e-05, 3.8703190512023866e-05, 3.8703190512023866e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.8703190512023866e-05

Optimization complete. Final v2v error: 5.383573532104492 mm

Highest mean error: 7.705003261566162 mm for frame 94

Lowest mean error: 4.595604419708252 mm for frame 174

Saving results

Total time: 44.741533279418945
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_46_us_2175/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_46_us_2175/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_46_us_2175/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00490573
Iteration 2/25 | Loss: 0.00190448
Iteration 3/25 | Loss: 0.00181873
Iteration 4/25 | Loss: 0.00180422
Iteration 5/25 | Loss: 0.00180166
Iteration 6/25 | Loss: 0.00180166
Iteration 7/25 | Loss: 0.00180166
Iteration 8/25 | Loss: 0.00180166
Iteration 9/25 | Loss: 0.00180166
Iteration 10/25 | Loss: 0.00180166
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0018016649410128593, 0.0018016649410128593, 0.0018016649410128593, 0.0018016649410128593, 0.0018016649410128593]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018016649410128593

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.69891751
Iteration 2/25 | Loss: 0.00270564
Iteration 3/25 | Loss: 0.00270564
Iteration 4/25 | Loss: 0.00270564
Iteration 5/25 | Loss: 0.00270564
Iteration 6/25 | Loss: 0.00270564
Iteration 7/25 | Loss: 0.00270564
Iteration 8/25 | Loss: 0.00270564
Iteration 9/25 | Loss: 0.00270564
Iteration 10/25 | Loss: 0.00270564
Iteration 11/25 | Loss: 0.00270564
Iteration 12/25 | Loss: 0.00270564
Iteration 13/25 | Loss: 0.00270564
Iteration 14/25 | Loss: 0.00270564
Iteration 15/25 | Loss: 0.00270564
Iteration 16/25 | Loss: 0.00270564
Iteration 17/25 | Loss: 0.00270564
Iteration 18/25 | Loss: 0.00270564
Iteration 19/25 | Loss: 0.00270564
Iteration 20/25 | Loss: 0.00270564
Iteration 21/25 | Loss: 0.00270564
Iteration 22/25 | Loss: 0.00270564
Iteration 23/25 | Loss: 0.00270564
Iteration 24/25 | Loss: 0.00270564
Iteration 25/25 | Loss: 0.00270564

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00270564
Iteration 2/1000 | Loss: 0.00007080
Iteration 3/1000 | Loss: 0.00005169
Iteration 4/1000 | Loss: 0.00004351
Iteration 5/1000 | Loss: 0.00004056
Iteration 6/1000 | Loss: 0.00003793
Iteration 7/1000 | Loss: 0.00003616
Iteration 8/1000 | Loss: 0.00003549
Iteration 9/1000 | Loss: 0.00003494
Iteration 10/1000 | Loss: 0.00003434
Iteration 11/1000 | Loss: 0.00003391
Iteration 12/1000 | Loss: 0.00003353
Iteration 13/1000 | Loss: 0.00003327
Iteration 14/1000 | Loss: 0.00003327
Iteration 15/1000 | Loss: 0.00003308
Iteration 16/1000 | Loss: 0.00003306
Iteration 17/1000 | Loss: 0.00003304
Iteration 18/1000 | Loss: 0.00003302
Iteration 19/1000 | Loss: 0.00003294
Iteration 20/1000 | Loss: 0.00003292
Iteration 21/1000 | Loss: 0.00003291
Iteration 22/1000 | Loss: 0.00003289
Iteration 23/1000 | Loss: 0.00003289
Iteration 24/1000 | Loss: 0.00003288
Iteration 25/1000 | Loss: 0.00003287
Iteration 26/1000 | Loss: 0.00003286
Iteration 27/1000 | Loss: 0.00003286
Iteration 28/1000 | Loss: 0.00003286
Iteration 29/1000 | Loss: 0.00003285
Iteration 30/1000 | Loss: 0.00003285
Iteration 31/1000 | Loss: 0.00003285
Iteration 32/1000 | Loss: 0.00003284
Iteration 33/1000 | Loss: 0.00003284
Iteration 34/1000 | Loss: 0.00003284
Iteration 35/1000 | Loss: 0.00003282
Iteration 36/1000 | Loss: 0.00003282
Iteration 37/1000 | Loss: 0.00003282
Iteration 38/1000 | Loss: 0.00003281
Iteration 39/1000 | Loss: 0.00003281
Iteration 40/1000 | Loss: 0.00003281
Iteration 41/1000 | Loss: 0.00003281
Iteration 42/1000 | Loss: 0.00003281
Iteration 43/1000 | Loss: 0.00003281
Iteration 44/1000 | Loss: 0.00003281
Iteration 45/1000 | Loss: 0.00003281
Iteration 46/1000 | Loss: 0.00003281
Iteration 47/1000 | Loss: 0.00003281
Iteration 48/1000 | Loss: 0.00003281
Iteration 49/1000 | Loss: 0.00003280
Iteration 50/1000 | Loss: 0.00003280
Iteration 51/1000 | Loss: 0.00003280
Iteration 52/1000 | Loss: 0.00003280
Iteration 53/1000 | Loss: 0.00003280
Iteration 54/1000 | Loss: 0.00003280
Iteration 55/1000 | Loss: 0.00003280
Iteration 56/1000 | Loss: 0.00003279
Iteration 57/1000 | Loss: 0.00003279
Iteration 58/1000 | Loss: 0.00003279
Iteration 59/1000 | Loss: 0.00003279
Iteration 60/1000 | Loss: 0.00003279
Iteration 61/1000 | Loss: 0.00003278
Iteration 62/1000 | Loss: 0.00003278
Iteration 63/1000 | Loss: 0.00003278
Iteration 64/1000 | Loss: 0.00003277
Iteration 65/1000 | Loss: 0.00003277
Iteration 66/1000 | Loss: 0.00003277
Iteration 67/1000 | Loss: 0.00003276
Iteration 68/1000 | Loss: 0.00003276
Iteration 69/1000 | Loss: 0.00003276
Iteration 70/1000 | Loss: 0.00003276
Iteration 71/1000 | Loss: 0.00003276
Iteration 72/1000 | Loss: 0.00003276
Iteration 73/1000 | Loss: 0.00003276
Iteration 74/1000 | Loss: 0.00003276
Iteration 75/1000 | Loss: 0.00003276
Iteration 76/1000 | Loss: 0.00003276
Iteration 77/1000 | Loss: 0.00003276
Iteration 78/1000 | Loss: 0.00003275
Iteration 79/1000 | Loss: 0.00003275
Iteration 80/1000 | Loss: 0.00003275
Iteration 81/1000 | Loss: 0.00003275
Iteration 82/1000 | Loss: 0.00003275
Iteration 83/1000 | Loss: 0.00003275
Iteration 84/1000 | Loss: 0.00003275
Iteration 85/1000 | Loss: 0.00003275
Iteration 86/1000 | Loss: 0.00003275
Iteration 87/1000 | Loss: 0.00003275
Iteration 88/1000 | Loss: 0.00003274
Iteration 89/1000 | Loss: 0.00003274
Iteration 90/1000 | Loss: 0.00003274
Iteration 91/1000 | Loss: 0.00003274
Iteration 92/1000 | Loss: 0.00003274
Iteration 93/1000 | Loss: 0.00003274
Iteration 94/1000 | Loss: 0.00003274
Iteration 95/1000 | Loss: 0.00003274
Iteration 96/1000 | Loss: 0.00003274
Iteration 97/1000 | Loss: 0.00003274
Iteration 98/1000 | Loss: 0.00003274
Iteration 99/1000 | Loss: 0.00003274
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [3.2742122130002826e-05, 3.2742122130002826e-05, 3.2742122130002826e-05, 3.2742122130002826e-05, 3.2742122130002826e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.2742122130002826e-05

Optimization complete. Final v2v error: 5.075667381286621 mm

Highest mean error: 5.402010917663574 mm for frame 1

Lowest mean error: 4.858808994293213 mm for frame 167

Saving results

Total time: 39.68840432167053
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_46_us_2175/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_46_us_2175/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_46_us_2175/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01184082
Iteration 2/25 | Loss: 0.00377889
Iteration 3/25 | Loss: 0.00276954
Iteration 4/25 | Loss: 0.00310794
Iteration 5/25 | Loss: 0.00326333
Iteration 6/25 | Loss: 0.00272965
Iteration 7/25 | Loss: 0.00275265
Iteration 8/25 | Loss: 0.00280050
Iteration 9/25 | Loss: 0.00253772
Iteration 10/25 | Loss: 0.00226872
Iteration 11/25 | Loss: 0.00205509
Iteration 12/25 | Loss: 0.00198099
Iteration 13/25 | Loss: 0.00196231
Iteration 14/25 | Loss: 0.00193401
Iteration 15/25 | Loss: 0.00193216
Iteration 16/25 | Loss: 0.00190449
Iteration 17/25 | Loss: 0.00188276
Iteration 18/25 | Loss: 0.00189071
Iteration 19/25 | Loss: 0.00189395
Iteration 20/25 | Loss: 0.00187824
Iteration 21/25 | Loss: 0.00187333
Iteration 22/25 | Loss: 0.00185800
Iteration 23/25 | Loss: 0.00185490
Iteration 24/25 | Loss: 0.00187590
Iteration 25/25 | Loss: 0.00187028

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.68447351
Iteration 2/25 | Loss: 0.00457128
Iteration 3/25 | Loss: 0.00451574
Iteration 4/25 | Loss: 0.00450634
Iteration 5/25 | Loss: 0.00450633
Iteration 6/25 | Loss: 0.00450633
Iteration 7/25 | Loss: 0.00450633
Iteration 8/25 | Loss: 0.00450633
Iteration 9/25 | Loss: 0.00450633
Iteration 10/25 | Loss: 0.00450633
Iteration 11/25 | Loss: 0.00450633
Iteration 12/25 | Loss: 0.00450633
Iteration 13/25 | Loss: 0.00450633
Iteration 14/25 | Loss: 0.00450633
Iteration 15/25 | Loss: 0.00450633
Iteration 16/25 | Loss: 0.00450633
Iteration 17/25 | Loss: 0.00450633
Iteration 18/25 | Loss: 0.00450633
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.004506331402808428, 0.004506331402808428, 0.004506331402808428, 0.004506331402808428, 0.004506331402808428]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004506331402808428

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00450633
Iteration 2/1000 | Loss: 0.00172910
Iteration 3/1000 | Loss: 0.00181587
Iteration 4/1000 | Loss: 0.00189217
Iteration 5/1000 | Loss: 0.00186935
Iteration 6/1000 | Loss: 0.00202919
Iteration 7/1000 | Loss: 0.00147782
Iteration 8/1000 | Loss: 0.00144466
Iteration 9/1000 | Loss: 0.00179717
Iteration 10/1000 | Loss: 0.00193713
Iteration 11/1000 | Loss: 0.00144899
Iteration 12/1000 | Loss: 0.00137510
Iteration 13/1000 | Loss: 0.00129351
Iteration 14/1000 | Loss: 0.00078920
Iteration 15/1000 | Loss: 0.00101516
Iteration 16/1000 | Loss: 0.00136852
Iteration 17/1000 | Loss: 0.00086171
Iteration 18/1000 | Loss: 0.00098715
Iteration 19/1000 | Loss: 0.00095205
Iteration 20/1000 | Loss: 0.00103415
Iteration 21/1000 | Loss: 0.00082433
Iteration 22/1000 | Loss: 0.00137666
Iteration 23/1000 | Loss: 0.00146061
Iteration 24/1000 | Loss: 0.00126767
Iteration 25/1000 | Loss: 0.00149851
Iteration 26/1000 | Loss: 0.00144330
Iteration 27/1000 | Loss: 0.00105415
Iteration 28/1000 | Loss: 0.00113695
Iteration 29/1000 | Loss: 0.00094630
Iteration 30/1000 | Loss: 0.00074608
Iteration 31/1000 | Loss: 0.00102320
Iteration 32/1000 | Loss: 0.00116418
Iteration 33/1000 | Loss: 0.00138935
Iteration 34/1000 | Loss: 0.00130116
Iteration 35/1000 | Loss: 0.00082991
Iteration 36/1000 | Loss: 0.00085669
Iteration 37/1000 | Loss: 0.00101859
Iteration 38/1000 | Loss: 0.00117949
Iteration 39/1000 | Loss: 0.00092073
Iteration 40/1000 | Loss: 0.00102586
Iteration 41/1000 | Loss: 0.00122232
Iteration 42/1000 | Loss: 0.00092000
Iteration 43/1000 | Loss: 0.00191638
Iteration 44/1000 | Loss: 0.00157216
Iteration 45/1000 | Loss: 0.00116552
Iteration 46/1000 | Loss: 0.00136494
Iteration 47/1000 | Loss: 0.00155426
Iteration 48/1000 | Loss: 0.00060321
Iteration 49/1000 | Loss: 0.00051547
Iteration 50/1000 | Loss: 0.00039300
Iteration 51/1000 | Loss: 0.00061477
Iteration 52/1000 | Loss: 0.00088265
Iteration 53/1000 | Loss: 0.00121487
Iteration 54/1000 | Loss: 0.00162706
Iteration 55/1000 | Loss: 0.00125972
Iteration 56/1000 | Loss: 0.00149895
Iteration 57/1000 | Loss: 0.00127199
Iteration 58/1000 | Loss: 0.00161655
Iteration 59/1000 | Loss: 0.00073154
Iteration 60/1000 | Loss: 0.00075588
Iteration 61/1000 | Loss: 0.00038912
Iteration 62/1000 | Loss: 0.00042047
Iteration 63/1000 | Loss: 0.00073897
Iteration 64/1000 | Loss: 0.00090725
Iteration 65/1000 | Loss: 0.00062772
Iteration 66/1000 | Loss: 0.00043275
Iteration 67/1000 | Loss: 0.00044642
Iteration 68/1000 | Loss: 0.00052858
Iteration 69/1000 | Loss: 0.00075859
Iteration 70/1000 | Loss: 0.00051703
Iteration 71/1000 | Loss: 0.00068689
Iteration 72/1000 | Loss: 0.00069478
Iteration 73/1000 | Loss: 0.00059039
Iteration 74/1000 | Loss: 0.00106422
Iteration 75/1000 | Loss: 0.00046013
Iteration 76/1000 | Loss: 0.00056340
Iteration 77/1000 | Loss: 0.00116550
Iteration 78/1000 | Loss: 0.00052868
Iteration 79/1000 | Loss: 0.00038026
Iteration 80/1000 | Loss: 0.00042251
Iteration 81/1000 | Loss: 0.00039415
Iteration 82/1000 | Loss: 0.00092670
Iteration 83/1000 | Loss: 0.00054245
Iteration 84/1000 | Loss: 0.00028142
Iteration 85/1000 | Loss: 0.00083988
Iteration 86/1000 | Loss: 0.00054423
Iteration 87/1000 | Loss: 0.00063946
Iteration 88/1000 | Loss: 0.00042400
Iteration 89/1000 | Loss: 0.00035453
Iteration 90/1000 | Loss: 0.00043047
Iteration 91/1000 | Loss: 0.00045229
Iteration 92/1000 | Loss: 0.00073069
Iteration 93/1000 | Loss: 0.00104295
Iteration 94/1000 | Loss: 0.00064519
Iteration 95/1000 | Loss: 0.00052631
Iteration 96/1000 | Loss: 0.00021048
Iteration 97/1000 | Loss: 0.00039981
Iteration 98/1000 | Loss: 0.00038805
Iteration 99/1000 | Loss: 0.00029527
Iteration 100/1000 | Loss: 0.00081662
Iteration 101/1000 | Loss: 0.00048814
Iteration 102/1000 | Loss: 0.00022130
Iteration 103/1000 | Loss: 0.00046719
Iteration 104/1000 | Loss: 0.00094705
Iteration 105/1000 | Loss: 0.00046631
Iteration 106/1000 | Loss: 0.00043066
Iteration 107/1000 | Loss: 0.00033700
Iteration 108/1000 | Loss: 0.00014925
Iteration 109/1000 | Loss: 0.00016972
Iteration 110/1000 | Loss: 0.00101798
Iteration 111/1000 | Loss: 0.00077817
Iteration 112/1000 | Loss: 0.00065027
Iteration 113/1000 | Loss: 0.00021886
Iteration 114/1000 | Loss: 0.00032417
Iteration 115/1000 | Loss: 0.00020767
Iteration 116/1000 | Loss: 0.00099384
Iteration 117/1000 | Loss: 0.00059593
Iteration 118/1000 | Loss: 0.00056363
Iteration 119/1000 | Loss: 0.00059130
Iteration 120/1000 | Loss: 0.00060153
Iteration 121/1000 | Loss: 0.00048952
Iteration 122/1000 | Loss: 0.00025763
Iteration 123/1000 | Loss: 0.00071477
Iteration 124/1000 | Loss: 0.00057946
Iteration 125/1000 | Loss: 0.00045043
Iteration 126/1000 | Loss: 0.00039723
Iteration 127/1000 | Loss: 0.00022957
Iteration 128/1000 | Loss: 0.00059862
Iteration 129/1000 | Loss: 0.00046713
Iteration 130/1000 | Loss: 0.00033660
Iteration 131/1000 | Loss: 0.00106204
Iteration 132/1000 | Loss: 0.00019949
Iteration 133/1000 | Loss: 0.00042922
Iteration 134/1000 | Loss: 0.00072743
Iteration 135/1000 | Loss: 0.00013167
Iteration 136/1000 | Loss: 0.00028236
Iteration 137/1000 | Loss: 0.00035718
Iteration 138/1000 | Loss: 0.00027976
Iteration 139/1000 | Loss: 0.00030789
Iteration 140/1000 | Loss: 0.00041530
Iteration 141/1000 | Loss: 0.00034718
Iteration 142/1000 | Loss: 0.00032534
Iteration 143/1000 | Loss: 0.00049289
Iteration 144/1000 | Loss: 0.00067451
Iteration 145/1000 | Loss: 0.00033884
Iteration 146/1000 | Loss: 0.00015002
Iteration 147/1000 | Loss: 0.00037170
Iteration 148/1000 | Loss: 0.00029682
Iteration 149/1000 | Loss: 0.00033884
Iteration 150/1000 | Loss: 0.00038104
Iteration 151/1000 | Loss: 0.00043748
Iteration 152/1000 | Loss: 0.00047051
Iteration 153/1000 | Loss: 0.00044655
Iteration 154/1000 | Loss: 0.00069431
Iteration 155/1000 | Loss: 0.00083624
Iteration 156/1000 | Loss: 0.00044514
Iteration 157/1000 | Loss: 0.00060738
Iteration 158/1000 | Loss: 0.00047340
Iteration 159/1000 | Loss: 0.00042372
Iteration 160/1000 | Loss: 0.00047107
Iteration 161/1000 | Loss: 0.00032427
Iteration 162/1000 | Loss: 0.00071333
Iteration 163/1000 | Loss: 0.00019353
Iteration 164/1000 | Loss: 0.00015344
Iteration 165/1000 | Loss: 0.00013855
Iteration 166/1000 | Loss: 0.00015685
Iteration 167/1000 | Loss: 0.00015543
Iteration 168/1000 | Loss: 0.00030130
Iteration 169/1000 | Loss: 0.00024836
Iteration 170/1000 | Loss: 0.00020648
Iteration 171/1000 | Loss: 0.00014574
Iteration 172/1000 | Loss: 0.00038797
Iteration 173/1000 | Loss: 0.00017291
Iteration 174/1000 | Loss: 0.00023055
Iteration 175/1000 | Loss: 0.00015397
Iteration 176/1000 | Loss: 0.00014086
Iteration 177/1000 | Loss: 0.00015128
Iteration 178/1000 | Loss: 0.00035007
Iteration 179/1000 | Loss: 0.00013778
Iteration 180/1000 | Loss: 0.00023291
Iteration 181/1000 | Loss: 0.00017993
Iteration 182/1000 | Loss: 0.00034851
Iteration 183/1000 | Loss: 0.00016378
Iteration 184/1000 | Loss: 0.00010436
Iteration 185/1000 | Loss: 0.00012760
Iteration 186/1000 | Loss: 0.00013656
Iteration 187/1000 | Loss: 0.00013320
Iteration 188/1000 | Loss: 0.00014840
Iteration 189/1000 | Loss: 0.00012460
Iteration 190/1000 | Loss: 0.00013846
Iteration 191/1000 | Loss: 0.00015980
Iteration 192/1000 | Loss: 0.00013049
Iteration 193/1000 | Loss: 0.00010085
Iteration 194/1000 | Loss: 0.00013531
Iteration 195/1000 | Loss: 0.00012599
Iteration 196/1000 | Loss: 0.00012564
Iteration 197/1000 | Loss: 0.00014356
Iteration 198/1000 | Loss: 0.00015807
Iteration 199/1000 | Loss: 0.00014369
Iteration 200/1000 | Loss: 0.00014820
Iteration 201/1000 | Loss: 0.00014497
Iteration 202/1000 | Loss: 0.00013417
Iteration 203/1000 | Loss: 0.00014520
Iteration 204/1000 | Loss: 0.00014902
Iteration 205/1000 | Loss: 0.00012322
Iteration 206/1000 | Loss: 0.00010169
Iteration 207/1000 | Loss: 0.00007192
Iteration 208/1000 | Loss: 0.00009897
Iteration 209/1000 | Loss: 0.00023745
Iteration 210/1000 | Loss: 0.00009504
Iteration 211/1000 | Loss: 0.00010435
Iteration 212/1000 | Loss: 0.00008208
Iteration 213/1000 | Loss: 0.00009980
Iteration 214/1000 | Loss: 0.00010152
Iteration 215/1000 | Loss: 0.00012708
Iteration 216/1000 | Loss: 0.00010173
Iteration 217/1000 | Loss: 0.00011003
Iteration 218/1000 | Loss: 0.00010384
Iteration 219/1000 | Loss: 0.00012836
Iteration 220/1000 | Loss: 0.00011889
Iteration 221/1000 | Loss: 0.00011905
Iteration 222/1000 | Loss: 0.00013055
Iteration 223/1000 | Loss: 0.00035654
Iteration 224/1000 | Loss: 0.00012819
Iteration 225/1000 | Loss: 0.00012185
Iteration 226/1000 | Loss: 0.00013516
Iteration 227/1000 | Loss: 0.00012662
Iteration 228/1000 | Loss: 0.00013590
Iteration 229/1000 | Loss: 0.00012345
Iteration 230/1000 | Loss: 0.00011906
Iteration 231/1000 | Loss: 0.00010503
Iteration 232/1000 | Loss: 0.00011495
Iteration 233/1000 | Loss: 0.00009654
Iteration 234/1000 | Loss: 0.00030585
Iteration 235/1000 | Loss: 0.00008887
Iteration 236/1000 | Loss: 0.00007002
Iteration 237/1000 | Loss: 0.00005583
Iteration 238/1000 | Loss: 0.00005111
Iteration 239/1000 | Loss: 0.00004992
Iteration 240/1000 | Loss: 0.00004696
Iteration 241/1000 | Loss: 0.00004510
Iteration 242/1000 | Loss: 0.00004444
Iteration 243/1000 | Loss: 0.00004391
Iteration 244/1000 | Loss: 0.00004335
Iteration 245/1000 | Loss: 0.00004902
Iteration 246/1000 | Loss: 0.00004596
Iteration 247/1000 | Loss: 0.00004841
Iteration 248/1000 | Loss: 0.00004598
Iteration 249/1000 | Loss: 0.00004432
Iteration 250/1000 | Loss: 0.00004294
Iteration 251/1000 | Loss: 0.00004201
Iteration 252/1000 | Loss: 0.00004140
Iteration 253/1000 | Loss: 0.00004112
Iteration 254/1000 | Loss: 0.00004101
Iteration 255/1000 | Loss: 0.00004098
Iteration 256/1000 | Loss: 0.00004097
Iteration 257/1000 | Loss: 0.00004097
Iteration 258/1000 | Loss: 0.00004096
Iteration 259/1000 | Loss: 0.00004096
Iteration 260/1000 | Loss: 0.00004095
Iteration 261/1000 | Loss: 0.00004093
Iteration 262/1000 | Loss: 0.00004091
Iteration 263/1000 | Loss: 0.00005110
Iteration 264/1000 | Loss: 0.00004522
Iteration 265/1000 | Loss: 0.00004088
Iteration 266/1000 | Loss: 0.00004086
Iteration 267/1000 | Loss: 0.00004086
Iteration 268/1000 | Loss: 0.00004086
Iteration 269/1000 | Loss: 0.00004085
Iteration 270/1000 | Loss: 0.00004085
Iteration 271/1000 | Loss: 0.00004085
Iteration 272/1000 | Loss: 0.00004085
Iteration 273/1000 | Loss: 0.00004085
Iteration 274/1000 | Loss: 0.00004084
Iteration 275/1000 | Loss: 0.00004084
Iteration 276/1000 | Loss: 0.00004083
Iteration 277/1000 | Loss: 0.00004083
Iteration 278/1000 | Loss: 0.00004083
Iteration 279/1000 | Loss: 0.00004083
Iteration 280/1000 | Loss: 0.00004082
Iteration 281/1000 | Loss: 0.00004082
Iteration 282/1000 | Loss: 0.00004082
Iteration 283/1000 | Loss: 0.00004081
Iteration 284/1000 | Loss: 0.00020938
Iteration 285/1000 | Loss: 0.00004622
Iteration 286/1000 | Loss: 0.00004409
Iteration 287/1000 | Loss: 0.00004292
Iteration 288/1000 | Loss: 0.00005241
Iteration 289/1000 | Loss: 0.00004556
Iteration 290/1000 | Loss: 0.00004210
Iteration 291/1000 | Loss: 0.00020517
Iteration 292/1000 | Loss: 0.00006545
Iteration 293/1000 | Loss: 0.00004878
Iteration 294/1000 | Loss: 0.00005553
Iteration 295/1000 | Loss: 0.00004307
Iteration 296/1000 | Loss: 0.00004198
Iteration 297/1000 | Loss: 0.00004117
Iteration 298/1000 | Loss: 0.00004086
Iteration 299/1000 | Loss: 0.00004066
Iteration 300/1000 | Loss: 0.00004061
Iteration 301/1000 | Loss: 0.00004059
Iteration 302/1000 | Loss: 0.00004047
Iteration 303/1000 | Loss: 0.00004045
Iteration 304/1000 | Loss: 0.00004044
Iteration 305/1000 | Loss: 0.00004043
Iteration 306/1000 | Loss: 0.00004043
Iteration 307/1000 | Loss: 0.00004042
Iteration 308/1000 | Loss: 0.00004042
Iteration 309/1000 | Loss: 0.00004042
Iteration 310/1000 | Loss: 0.00004042
Iteration 311/1000 | Loss: 0.00004041
Iteration 312/1000 | Loss: 0.00004041
Iteration 313/1000 | Loss: 0.00004041
Iteration 314/1000 | Loss: 0.00004041
Iteration 315/1000 | Loss: 0.00004041
Iteration 316/1000 | Loss: 0.00004041
Iteration 317/1000 | Loss: 0.00004041
Iteration 318/1000 | Loss: 0.00004040
Iteration 319/1000 | Loss: 0.00004040
Iteration 320/1000 | Loss: 0.00004039
Iteration 321/1000 | Loss: 0.00004038
Iteration 322/1000 | Loss: 0.00004038
Iteration 323/1000 | Loss: 0.00004038
Iteration 324/1000 | Loss: 0.00004037
Iteration 325/1000 | Loss: 0.00004037
Iteration 326/1000 | Loss: 0.00004037
Iteration 327/1000 | Loss: 0.00004037
Iteration 328/1000 | Loss: 0.00004037
Iteration 329/1000 | Loss: 0.00004036
Iteration 330/1000 | Loss: 0.00004036
Iteration 331/1000 | Loss: 0.00004036
Iteration 332/1000 | Loss: 0.00004035
Iteration 333/1000 | Loss: 0.00004035
Iteration 334/1000 | Loss: 0.00004035
Iteration 335/1000 | Loss: 0.00004035
Iteration 336/1000 | Loss: 0.00004035
Iteration 337/1000 | Loss: 0.00004034
Iteration 338/1000 | Loss: 0.00004034
Iteration 339/1000 | Loss: 0.00004034
Iteration 340/1000 | Loss: 0.00004034
Iteration 341/1000 | Loss: 0.00004034
Iteration 342/1000 | Loss: 0.00004034
Iteration 343/1000 | Loss: 0.00004033
Iteration 344/1000 | Loss: 0.00004033
Iteration 345/1000 | Loss: 0.00004033
Iteration 346/1000 | Loss: 0.00004032
Iteration 347/1000 | Loss: 0.00004032
Iteration 348/1000 | Loss: 0.00004032
Iteration 349/1000 | Loss: 0.00004032
Iteration 350/1000 | Loss: 0.00004032
Iteration 351/1000 | Loss: 0.00004032
Iteration 352/1000 | Loss: 0.00004031
Iteration 353/1000 | Loss: 0.00004031
Iteration 354/1000 | Loss: 0.00004031
Iteration 355/1000 | Loss: 0.00004030
Iteration 356/1000 | Loss: 0.00004030
Iteration 357/1000 | Loss: 0.00004030
Iteration 358/1000 | Loss: 0.00004030
Iteration 359/1000 | Loss: 0.00004030
Iteration 360/1000 | Loss: 0.00004030
Iteration 361/1000 | Loss: 0.00004030
Iteration 362/1000 | Loss: 0.00004029
Iteration 363/1000 | Loss: 0.00004029
Iteration 364/1000 | Loss: 0.00004029
Iteration 365/1000 | Loss: 0.00004029
Iteration 366/1000 | Loss: 0.00004028
Iteration 367/1000 | Loss: 0.00004028
Iteration 368/1000 | Loss: 0.00004028
Iteration 369/1000 | Loss: 0.00004028
Iteration 370/1000 | Loss: 0.00004028
Iteration 371/1000 | Loss: 0.00004027
Iteration 372/1000 | Loss: 0.00004027
Iteration 373/1000 | Loss: 0.00004027
Iteration 374/1000 | Loss: 0.00004027
Iteration 375/1000 | Loss: 0.00004026
Iteration 376/1000 | Loss: 0.00004026
Iteration 377/1000 | Loss: 0.00004026
Iteration 378/1000 | Loss: 0.00004025
Iteration 379/1000 | Loss: 0.00004025
Iteration 380/1000 | Loss: 0.00004024
Iteration 381/1000 | Loss: 0.00004024
Iteration 382/1000 | Loss: 0.00004024
Iteration 383/1000 | Loss: 0.00004024
Iteration 384/1000 | Loss: 0.00004024
Iteration 385/1000 | Loss: 0.00004023
Iteration 386/1000 | Loss: 0.00004023
Iteration 387/1000 | Loss: 0.00004023
Iteration 388/1000 | Loss: 0.00004023
Iteration 389/1000 | Loss: 0.00004023
Iteration 390/1000 | Loss: 0.00004022
Iteration 391/1000 | Loss: 0.00004022
Iteration 392/1000 | Loss: 0.00004021
Iteration 393/1000 | Loss: 0.00004021
Iteration 394/1000 | Loss: 0.00004021
Iteration 395/1000 | Loss: 0.00004020
Iteration 396/1000 | Loss: 0.00004020
Iteration 397/1000 | Loss: 0.00004020
Iteration 398/1000 | Loss: 0.00004020
Iteration 399/1000 | Loss: 0.00004020
Iteration 400/1000 | Loss: 0.00004020
Iteration 401/1000 | Loss: 0.00004019
Iteration 402/1000 | Loss: 0.00004019
Iteration 403/1000 | Loss: 0.00004019
Iteration 404/1000 | Loss: 0.00004019
Iteration 405/1000 | Loss: 0.00004018
Iteration 406/1000 | Loss: 0.00004018
Iteration 407/1000 | Loss: 0.00004018
Iteration 408/1000 | Loss: 0.00004018
Iteration 409/1000 | Loss: 0.00004018
Iteration 410/1000 | Loss: 0.00004018
Iteration 411/1000 | Loss: 0.00004018
Iteration 412/1000 | Loss: 0.00004018
Iteration 413/1000 | Loss: 0.00004018
Iteration 414/1000 | Loss: 0.00004018
Iteration 415/1000 | Loss: 0.00004018
Iteration 416/1000 | Loss: 0.00004018
Iteration 417/1000 | Loss: 0.00004018
Iteration 418/1000 | Loss: 0.00004017
Iteration 419/1000 | Loss: 0.00004017
Iteration 420/1000 | Loss: 0.00004017
Iteration 421/1000 | Loss: 0.00004017
Iteration 422/1000 | Loss: 0.00022201
Iteration 423/1000 | Loss: 0.00004369
Iteration 424/1000 | Loss: 0.00004109
Iteration 425/1000 | Loss: 0.00004046
Iteration 426/1000 | Loss: 0.00004014
Iteration 427/1000 | Loss: 0.00004002
Iteration 428/1000 | Loss: 0.00003998
Iteration 429/1000 | Loss: 0.00003992
Iteration 430/1000 | Loss: 0.00003989
Iteration 431/1000 | Loss: 0.00003986
Iteration 432/1000 | Loss: 0.00003985
Iteration 433/1000 | Loss: 0.00003985
Iteration 434/1000 | Loss: 0.00003984
Iteration 435/1000 | Loss: 0.00004863
Iteration 436/1000 | Loss: 0.00004396
Iteration 437/1000 | Loss: 0.00003985
Iteration 438/1000 | Loss: 0.00004789
Iteration 439/1000 | Loss: 0.00004467
Iteration 440/1000 | Loss: 0.00004000
Iteration 441/1000 | Loss: 0.00003984
Iteration 442/1000 | Loss: 0.00003984
Iteration 443/1000 | Loss: 0.00003983
Iteration 444/1000 | Loss: 0.00003983
Iteration 445/1000 | Loss: 0.00003982
Iteration 446/1000 | Loss: 0.00003982
Iteration 447/1000 | Loss: 0.00004498
Iteration 448/1000 | Loss: 0.00004161
Iteration 449/1000 | Loss: 0.00004561
Iteration 450/1000 | Loss: 0.00004113
Iteration 451/1000 | Loss: 0.00004549
Iteration 452/1000 | Loss: 0.00004033
Iteration 453/1000 | Loss: 0.00004509
Iteration 454/1000 | Loss: 0.00004023
Iteration 455/1000 | Loss: 0.00004473
Iteration 456/1000 | Loss: 0.00004107
Iteration 457/1000 | Loss: 0.00004455
Iteration 458/1000 | Loss: 0.00004017
Iteration 459/1000 | Loss: 0.00004262
Iteration 460/1000 | Loss: 0.00004012
Iteration 461/1000 | Loss: 0.00004332
Iteration 462/1000 | Loss: 0.00004044
Iteration 463/1000 | Loss: 0.00004477
Iteration 464/1000 | Loss: 0.00004018
Iteration 465/1000 | Loss: 0.00004429
Iteration 466/1000 | Loss: 0.00003989
Iteration 467/1000 | Loss: 0.00003983
Iteration 468/1000 | Loss: 0.00003983
Iteration 469/1000 | Loss: 0.00003982
Iteration 470/1000 | Loss: 0.00003982
Iteration 471/1000 | Loss: 0.00003982
Iteration 472/1000 | Loss: 0.00003981
Iteration 473/1000 | Loss: 0.00003981
Iteration 474/1000 | Loss: 0.00003981
Iteration 475/1000 | Loss: 0.00003980
Iteration 476/1000 | Loss: 0.00003980
Iteration 477/1000 | Loss: 0.00003979
Iteration 478/1000 | Loss: 0.00003978
Iteration 479/1000 | Loss: 0.00003978
Iteration 480/1000 | Loss: 0.00003977
Iteration 481/1000 | Loss: 0.00003976
Iteration 482/1000 | Loss: 0.00003976
Iteration 483/1000 | Loss: 0.00003975
Iteration 484/1000 | Loss: 0.00003975
Iteration 485/1000 | Loss: 0.00003975
Iteration 486/1000 | Loss: 0.00003975
Iteration 487/1000 | Loss: 0.00003975
Iteration 488/1000 | Loss: 0.00003975
Iteration 489/1000 | Loss: 0.00003975
Iteration 490/1000 | Loss: 0.00003975
Iteration 491/1000 | Loss: 0.00003975
Iteration 492/1000 | Loss: 0.00003975
Iteration 493/1000 | Loss: 0.00003975
Iteration 494/1000 | Loss: 0.00003975
Iteration 495/1000 | Loss: 0.00003974
Iteration 496/1000 | Loss: 0.00003974
Iteration 497/1000 | Loss: 0.00003974
Iteration 498/1000 | Loss: 0.00003974
Iteration 499/1000 | Loss: 0.00003973
Iteration 500/1000 | Loss: 0.00003973
Iteration 501/1000 | Loss: 0.00003973
Iteration 502/1000 | Loss: 0.00003973
Iteration 503/1000 | Loss: 0.00003973
Iteration 504/1000 | Loss: 0.00003973
Iteration 505/1000 | Loss: 0.00003973
Iteration 506/1000 | Loss: 0.00003972
Iteration 507/1000 | Loss: 0.00003972
Iteration 508/1000 | Loss: 0.00003972
Iteration 509/1000 | Loss: 0.00003972
Iteration 510/1000 | Loss: 0.00003972
Iteration 511/1000 | Loss: 0.00003972
Iteration 512/1000 | Loss: 0.00003972
Iteration 513/1000 | Loss: 0.00003971
Iteration 514/1000 | Loss: 0.00003971
Iteration 515/1000 | Loss: 0.00003971
Iteration 516/1000 | Loss: 0.00003971
Iteration 517/1000 | Loss: 0.00003971
Iteration 518/1000 | Loss: 0.00003971
Iteration 519/1000 | Loss: 0.00003970
Iteration 520/1000 | Loss: 0.00003970
Iteration 521/1000 | Loss: 0.00003970
Iteration 522/1000 | Loss: 0.00003970
Iteration 523/1000 | Loss: 0.00003970
Iteration 524/1000 | Loss: 0.00003970
Iteration 525/1000 | Loss: 0.00003970
Iteration 526/1000 | Loss: 0.00003970
Iteration 527/1000 | Loss: 0.00003970
Iteration 528/1000 | Loss: 0.00003970
Iteration 529/1000 | Loss: 0.00003970
Iteration 530/1000 | Loss: 0.00003970
Iteration 531/1000 | Loss: 0.00003970
Iteration 532/1000 | Loss: 0.00003970
Iteration 533/1000 | Loss: 0.00003970
Iteration 534/1000 | Loss: 0.00003970
Iteration 535/1000 | Loss: 0.00003970
Iteration 536/1000 | Loss: 0.00003970
Iteration 537/1000 | Loss: 0.00003970
Iteration 538/1000 | Loss: 0.00003970
Iteration 539/1000 | Loss: 0.00003970
Iteration 540/1000 | Loss: 0.00003970
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 540. Stopping optimization.
Last 5 losses: [3.970115722040646e-05, 3.970115722040646e-05, 3.970115722040646e-05, 3.970115722040646e-05, 3.970115722040646e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.970115722040646e-05

Optimization complete. Final v2v error: 5.347865581512451 mm

Highest mean error: 11.575156211853027 mm for frame 177

Lowest mean error: 4.985978603363037 mm for frame 190

Saving results

Total time: 563.8349876403809
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_46_us_2175/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_46_us_2175/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_46_us_2175/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00959275
Iteration 2/25 | Loss: 0.00196132
Iteration 3/25 | Loss: 0.00184636
Iteration 4/25 | Loss: 0.00183019
Iteration 5/25 | Loss: 0.00182915
Iteration 6/25 | Loss: 0.00182510
Iteration 7/25 | Loss: 0.00182140
Iteration 8/25 | Loss: 0.00182053
Iteration 9/25 | Loss: 0.00182035
Iteration 10/25 | Loss: 0.00182028
Iteration 11/25 | Loss: 0.00182025
Iteration 12/25 | Loss: 0.00182025
Iteration 13/25 | Loss: 0.00182024
Iteration 14/25 | Loss: 0.00182024
Iteration 15/25 | Loss: 0.00182023
Iteration 16/25 | Loss: 0.00182023
Iteration 17/25 | Loss: 0.00182023
Iteration 18/25 | Loss: 0.00182023
Iteration 19/25 | Loss: 0.00182023
Iteration 20/25 | Loss: 0.00182023
Iteration 21/25 | Loss: 0.00182023
Iteration 22/25 | Loss: 0.00182022
Iteration 23/25 | Loss: 0.00182022
Iteration 24/25 | Loss: 0.00182022
Iteration 25/25 | Loss: 0.00182022

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.74375844
Iteration 2/25 | Loss: 0.00226311
Iteration 3/25 | Loss: 0.00226311
Iteration 4/25 | Loss: 0.00226311
Iteration 5/25 | Loss: 0.00226311
Iteration 6/25 | Loss: 0.00226311
Iteration 7/25 | Loss: 0.00226311
Iteration 8/25 | Loss: 0.00226311
Iteration 9/25 | Loss: 0.00226311
Iteration 10/25 | Loss: 0.00226311
Iteration 11/25 | Loss: 0.00226311
Iteration 12/25 | Loss: 0.00226311
Iteration 13/25 | Loss: 0.00226311
Iteration 14/25 | Loss: 0.00226311
Iteration 15/25 | Loss: 0.00226311
Iteration 16/25 | Loss: 0.00226311
Iteration 17/25 | Loss: 0.00226311
Iteration 18/25 | Loss: 0.00226311
Iteration 19/25 | Loss: 0.00226311
Iteration 20/25 | Loss: 0.00226311
Iteration 21/25 | Loss: 0.00226311
Iteration 22/25 | Loss: 0.00226311
Iteration 23/25 | Loss: 0.00226311
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0022631101310253143, 0.0022631101310253143, 0.0022631101310253143, 0.0022631101310253143, 0.0022631101310253143]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022631101310253143

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00226311
Iteration 2/1000 | Loss: 0.00006013
Iteration 3/1000 | Loss: 0.00004581
Iteration 4/1000 | Loss: 0.00004089
Iteration 5/1000 | Loss: 0.00003821
Iteration 6/1000 | Loss: 0.00003702
Iteration 7/1000 | Loss: 0.00003602
Iteration 8/1000 | Loss: 0.00003530
Iteration 9/1000 | Loss: 0.00003460
Iteration 10/1000 | Loss: 0.00003417
Iteration 11/1000 | Loss: 0.00003379
Iteration 12/1000 | Loss: 0.00003355
Iteration 13/1000 | Loss: 0.00003341
Iteration 14/1000 | Loss: 0.00003338
Iteration 15/1000 | Loss: 0.00003335
Iteration 16/1000 | Loss: 0.00003329
Iteration 17/1000 | Loss: 0.00003327
Iteration 18/1000 | Loss: 0.00003326
Iteration 19/1000 | Loss: 0.00003325
Iteration 20/1000 | Loss: 0.00003322
Iteration 21/1000 | Loss: 0.00003322
Iteration 22/1000 | Loss: 0.00003322
Iteration 23/1000 | Loss: 0.00003322
Iteration 24/1000 | Loss: 0.00003322
Iteration 25/1000 | Loss: 0.00003322
Iteration 26/1000 | Loss: 0.00003322
Iteration 27/1000 | Loss: 0.00003322
Iteration 28/1000 | Loss: 0.00003322
Iteration 29/1000 | Loss: 0.00003322
Iteration 30/1000 | Loss: 0.00003322
Iteration 31/1000 | Loss: 0.00003322
Iteration 32/1000 | Loss: 0.00003322
Iteration 33/1000 | Loss: 0.00003321
Iteration 34/1000 | Loss: 0.00003321
Iteration 35/1000 | Loss: 0.00003320
Iteration 36/1000 | Loss: 0.00003320
Iteration 37/1000 | Loss: 0.00003320
Iteration 38/1000 | Loss: 0.00003319
Iteration 39/1000 | Loss: 0.00003319
Iteration 40/1000 | Loss: 0.00003319
Iteration 41/1000 | Loss: 0.00003319
Iteration 42/1000 | Loss: 0.00003319
Iteration 43/1000 | Loss: 0.00003318
Iteration 44/1000 | Loss: 0.00003318
Iteration 45/1000 | Loss: 0.00003318
Iteration 46/1000 | Loss: 0.00003318
Iteration 47/1000 | Loss: 0.00003318
Iteration 48/1000 | Loss: 0.00003318
Iteration 49/1000 | Loss: 0.00003318
Iteration 50/1000 | Loss: 0.00003317
Iteration 51/1000 | Loss: 0.00003317
Iteration 52/1000 | Loss: 0.00003316
Iteration 53/1000 | Loss: 0.00003316
Iteration 54/1000 | Loss: 0.00003316
Iteration 55/1000 | Loss: 0.00003316
Iteration 56/1000 | Loss: 0.00003316
Iteration 57/1000 | Loss: 0.00003316
Iteration 58/1000 | Loss: 0.00003316
Iteration 59/1000 | Loss: 0.00003316
Iteration 60/1000 | Loss: 0.00003316
Iteration 61/1000 | Loss: 0.00003316
Iteration 62/1000 | Loss: 0.00003315
Iteration 63/1000 | Loss: 0.00003315
Iteration 64/1000 | Loss: 0.00003315
Iteration 65/1000 | Loss: 0.00003314
Iteration 66/1000 | Loss: 0.00003314
Iteration 67/1000 | Loss: 0.00003314
Iteration 68/1000 | Loss: 0.00003314
Iteration 69/1000 | Loss: 0.00003314
Iteration 70/1000 | Loss: 0.00003313
Iteration 71/1000 | Loss: 0.00003313
Iteration 72/1000 | Loss: 0.00003313
Iteration 73/1000 | Loss: 0.00003313
Iteration 74/1000 | Loss: 0.00003312
Iteration 75/1000 | Loss: 0.00003312
Iteration 76/1000 | Loss: 0.00003312
Iteration 77/1000 | Loss: 0.00003312
Iteration 78/1000 | Loss: 0.00003312
Iteration 79/1000 | Loss: 0.00003311
Iteration 80/1000 | Loss: 0.00003311
Iteration 81/1000 | Loss: 0.00003311
Iteration 82/1000 | Loss: 0.00003311
Iteration 83/1000 | Loss: 0.00003311
Iteration 84/1000 | Loss: 0.00003311
Iteration 85/1000 | Loss: 0.00003311
Iteration 86/1000 | Loss: 0.00003311
Iteration 87/1000 | Loss: 0.00003311
Iteration 88/1000 | Loss: 0.00003311
Iteration 89/1000 | Loss: 0.00003311
Iteration 90/1000 | Loss: 0.00003311
Iteration 91/1000 | Loss: 0.00003311
Iteration 92/1000 | Loss: 0.00003311
Iteration 93/1000 | Loss: 0.00003311
Iteration 94/1000 | Loss: 0.00003311
Iteration 95/1000 | Loss: 0.00003311
Iteration 96/1000 | Loss: 0.00003311
Iteration 97/1000 | Loss: 0.00003311
Iteration 98/1000 | Loss: 0.00003311
Iteration 99/1000 | Loss: 0.00003311
Iteration 100/1000 | Loss: 0.00003311
Iteration 101/1000 | Loss: 0.00003311
Iteration 102/1000 | Loss: 0.00003311
Iteration 103/1000 | Loss: 0.00003311
Iteration 104/1000 | Loss: 0.00003311
Iteration 105/1000 | Loss: 0.00003311
Iteration 106/1000 | Loss: 0.00003311
Iteration 107/1000 | Loss: 0.00003311
Iteration 108/1000 | Loss: 0.00003311
Iteration 109/1000 | Loss: 0.00003311
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [3.3108488423749804e-05, 3.3108488423749804e-05, 3.3108488423749804e-05, 3.3108488423749804e-05, 3.3108488423749804e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.3108488423749804e-05

Optimization complete. Final v2v error: 5.065255165100098 mm

Highest mean error: 5.461777210235596 mm for frame 78

Lowest mean error: 4.840481281280518 mm for frame 161

Saving results

Total time: 49.50054121017456
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_46_us_2175/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_46_us_2175/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_46_us_2175/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00447086
Iteration 2/25 | Loss: 0.00178197
Iteration 3/25 | Loss: 0.00172617
Iteration 4/25 | Loss: 0.00172132
Iteration 5/25 | Loss: 0.00171997
Iteration 6/25 | Loss: 0.00171994
Iteration 7/25 | Loss: 0.00171994
Iteration 8/25 | Loss: 0.00171994
Iteration 9/25 | Loss: 0.00171994
Iteration 10/25 | Loss: 0.00171994
Iteration 11/25 | Loss: 0.00171994
Iteration 12/25 | Loss: 0.00171994
Iteration 13/25 | Loss: 0.00171994
Iteration 14/25 | Loss: 0.00171994
Iteration 15/25 | Loss: 0.00171994
Iteration 16/25 | Loss: 0.00171994
Iteration 17/25 | Loss: 0.00171994
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0017199382418766618, 0.0017199382418766618, 0.0017199382418766618, 0.0017199382418766618, 0.0017199382418766618]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017199382418766618

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.93050742
Iteration 2/25 | Loss: 0.00223705
Iteration 3/25 | Loss: 0.00223704
Iteration 4/25 | Loss: 0.00223704
Iteration 5/25 | Loss: 0.00223704
Iteration 6/25 | Loss: 0.00223704
Iteration 7/25 | Loss: 0.00223704
Iteration 8/25 | Loss: 0.00223704
Iteration 9/25 | Loss: 0.00223704
Iteration 10/25 | Loss: 0.00223704
Iteration 11/25 | Loss: 0.00223704
Iteration 12/25 | Loss: 0.00223704
Iteration 13/25 | Loss: 0.00223704
Iteration 14/25 | Loss: 0.00223704
Iteration 15/25 | Loss: 0.00223704
Iteration 16/25 | Loss: 0.00223704
Iteration 17/25 | Loss: 0.00223704
Iteration 18/25 | Loss: 0.00223704
Iteration 19/25 | Loss: 0.00223704
Iteration 20/25 | Loss: 0.00223704
Iteration 21/25 | Loss: 0.00223704
Iteration 22/25 | Loss: 0.00223704
Iteration 23/25 | Loss: 0.00223704
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.002237040316686034, 0.002237040316686034, 0.002237040316686034, 0.002237040316686034, 0.002237040316686034]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002237040316686034

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00223704
Iteration 2/1000 | Loss: 0.00005567
Iteration 3/1000 | Loss: 0.00003947
Iteration 4/1000 | Loss: 0.00003537
Iteration 5/1000 | Loss: 0.00003340
Iteration 6/1000 | Loss: 0.00003227
Iteration 7/1000 | Loss: 0.00003163
Iteration 8/1000 | Loss: 0.00003136
Iteration 9/1000 | Loss: 0.00003107
Iteration 10/1000 | Loss: 0.00003087
Iteration 11/1000 | Loss: 0.00003071
Iteration 12/1000 | Loss: 0.00003064
Iteration 13/1000 | Loss: 0.00003049
Iteration 14/1000 | Loss: 0.00003046
Iteration 15/1000 | Loss: 0.00003046
Iteration 16/1000 | Loss: 0.00003045
Iteration 17/1000 | Loss: 0.00003045
Iteration 18/1000 | Loss: 0.00003042
Iteration 19/1000 | Loss: 0.00003042
Iteration 20/1000 | Loss: 0.00003041
Iteration 21/1000 | Loss: 0.00003040
Iteration 22/1000 | Loss: 0.00003039
Iteration 23/1000 | Loss: 0.00003039
Iteration 24/1000 | Loss: 0.00003038
Iteration 25/1000 | Loss: 0.00003035
Iteration 26/1000 | Loss: 0.00003035
Iteration 27/1000 | Loss: 0.00003030
Iteration 28/1000 | Loss: 0.00003028
Iteration 29/1000 | Loss: 0.00003028
Iteration 30/1000 | Loss: 0.00003028
Iteration 31/1000 | Loss: 0.00003027
Iteration 32/1000 | Loss: 0.00003027
Iteration 33/1000 | Loss: 0.00003027
Iteration 34/1000 | Loss: 0.00003026
Iteration 35/1000 | Loss: 0.00003026
Iteration 36/1000 | Loss: 0.00003026
Iteration 37/1000 | Loss: 0.00003025
Iteration 38/1000 | Loss: 0.00003025
Iteration 39/1000 | Loss: 0.00003025
Iteration 40/1000 | Loss: 0.00003024
Iteration 41/1000 | Loss: 0.00003024
Iteration 42/1000 | Loss: 0.00003024
Iteration 43/1000 | Loss: 0.00003024
Iteration 44/1000 | Loss: 0.00003023
Iteration 45/1000 | Loss: 0.00003023
Iteration 46/1000 | Loss: 0.00003023
Iteration 47/1000 | Loss: 0.00003023
Iteration 48/1000 | Loss: 0.00003023
Iteration 49/1000 | Loss: 0.00003023
Iteration 50/1000 | Loss: 0.00003023
Iteration 51/1000 | Loss: 0.00003023
Iteration 52/1000 | Loss: 0.00003023
Iteration 53/1000 | Loss: 0.00003023
Iteration 54/1000 | Loss: 0.00003023
Iteration 55/1000 | Loss: 0.00003023
Iteration 56/1000 | Loss: 0.00003023
Iteration 57/1000 | Loss: 0.00003022
Iteration 58/1000 | Loss: 0.00003022
Iteration 59/1000 | Loss: 0.00003022
Iteration 60/1000 | Loss: 0.00003022
Iteration 61/1000 | Loss: 0.00003022
Iteration 62/1000 | Loss: 0.00003022
Iteration 63/1000 | Loss: 0.00003022
Iteration 64/1000 | Loss: 0.00003022
Iteration 65/1000 | Loss: 0.00003021
Iteration 66/1000 | Loss: 0.00003021
Iteration 67/1000 | Loss: 0.00003021
Iteration 68/1000 | Loss: 0.00003021
Iteration 69/1000 | Loss: 0.00003021
Iteration 70/1000 | Loss: 0.00003021
Iteration 71/1000 | Loss: 0.00003021
Iteration 72/1000 | Loss: 0.00003021
Iteration 73/1000 | Loss: 0.00003021
Iteration 74/1000 | Loss: 0.00003021
Iteration 75/1000 | Loss: 0.00003020
Iteration 76/1000 | Loss: 0.00003020
Iteration 77/1000 | Loss: 0.00003020
Iteration 78/1000 | Loss: 0.00003020
Iteration 79/1000 | Loss: 0.00003020
Iteration 80/1000 | Loss: 0.00003020
Iteration 81/1000 | Loss: 0.00003020
Iteration 82/1000 | Loss: 0.00003020
Iteration 83/1000 | Loss: 0.00003020
Iteration 84/1000 | Loss: 0.00003020
Iteration 85/1000 | Loss: 0.00003020
Iteration 86/1000 | Loss: 0.00003020
Iteration 87/1000 | Loss: 0.00003019
Iteration 88/1000 | Loss: 0.00003019
Iteration 89/1000 | Loss: 0.00003019
Iteration 90/1000 | Loss: 0.00003019
Iteration 91/1000 | Loss: 0.00003019
Iteration 92/1000 | Loss: 0.00003019
Iteration 93/1000 | Loss: 0.00003019
Iteration 94/1000 | Loss: 0.00003019
Iteration 95/1000 | Loss: 0.00003019
Iteration 96/1000 | Loss: 0.00003019
Iteration 97/1000 | Loss: 0.00003019
Iteration 98/1000 | Loss: 0.00003019
Iteration 99/1000 | Loss: 0.00003019
Iteration 100/1000 | Loss: 0.00003019
Iteration 101/1000 | Loss: 0.00003019
Iteration 102/1000 | Loss: 0.00003019
Iteration 103/1000 | Loss: 0.00003018
Iteration 104/1000 | Loss: 0.00003018
Iteration 105/1000 | Loss: 0.00003018
Iteration 106/1000 | Loss: 0.00003018
Iteration 107/1000 | Loss: 0.00003018
Iteration 108/1000 | Loss: 0.00003018
Iteration 109/1000 | Loss: 0.00003018
Iteration 110/1000 | Loss: 0.00003018
Iteration 111/1000 | Loss: 0.00003018
Iteration 112/1000 | Loss: 0.00003018
Iteration 113/1000 | Loss: 0.00003018
Iteration 114/1000 | Loss: 0.00003018
Iteration 115/1000 | Loss: 0.00003018
Iteration 116/1000 | Loss: 0.00003018
Iteration 117/1000 | Loss: 0.00003018
Iteration 118/1000 | Loss: 0.00003018
Iteration 119/1000 | Loss: 0.00003018
Iteration 120/1000 | Loss: 0.00003017
Iteration 121/1000 | Loss: 0.00003017
Iteration 122/1000 | Loss: 0.00003017
Iteration 123/1000 | Loss: 0.00003017
Iteration 124/1000 | Loss: 0.00003017
Iteration 125/1000 | Loss: 0.00003017
Iteration 126/1000 | Loss: 0.00003017
Iteration 127/1000 | Loss: 0.00003017
Iteration 128/1000 | Loss: 0.00003017
Iteration 129/1000 | Loss: 0.00003017
Iteration 130/1000 | Loss: 0.00003017
Iteration 131/1000 | Loss: 0.00003017
Iteration 132/1000 | Loss: 0.00003017
Iteration 133/1000 | Loss: 0.00003016
Iteration 134/1000 | Loss: 0.00003016
Iteration 135/1000 | Loss: 0.00003016
Iteration 136/1000 | Loss: 0.00003016
Iteration 137/1000 | Loss: 0.00003016
Iteration 138/1000 | Loss: 0.00003016
Iteration 139/1000 | Loss: 0.00003016
Iteration 140/1000 | Loss: 0.00003016
Iteration 141/1000 | Loss: 0.00003016
Iteration 142/1000 | Loss: 0.00003016
Iteration 143/1000 | Loss: 0.00003016
Iteration 144/1000 | Loss: 0.00003016
Iteration 145/1000 | Loss: 0.00003016
Iteration 146/1000 | Loss: 0.00003016
Iteration 147/1000 | Loss: 0.00003016
Iteration 148/1000 | Loss: 0.00003016
Iteration 149/1000 | Loss: 0.00003016
Iteration 150/1000 | Loss: 0.00003016
Iteration 151/1000 | Loss: 0.00003016
Iteration 152/1000 | Loss: 0.00003015
Iteration 153/1000 | Loss: 0.00003015
Iteration 154/1000 | Loss: 0.00003015
Iteration 155/1000 | Loss: 0.00003015
Iteration 156/1000 | Loss: 0.00003015
Iteration 157/1000 | Loss: 0.00003015
Iteration 158/1000 | Loss: 0.00003015
Iteration 159/1000 | Loss: 0.00003015
Iteration 160/1000 | Loss: 0.00003015
Iteration 161/1000 | Loss: 0.00003015
Iteration 162/1000 | Loss: 0.00003015
Iteration 163/1000 | Loss: 0.00003015
Iteration 164/1000 | Loss: 0.00003015
Iteration 165/1000 | Loss: 0.00003015
Iteration 166/1000 | Loss: 0.00003015
Iteration 167/1000 | Loss: 0.00003015
Iteration 168/1000 | Loss: 0.00003014
Iteration 169/1000 | Loss: 0.00003014
Iteration 170/1000 | Loss: 0.00003014
Iteration 171/1000 | Loss: 0.00003014
Iteration 172/1000 | Loss: 0.00003014
Iteration 173/1000 | Loss: 0.00003014
Iteration 174/1000 | Loss: 0.00003014
Iteration 175/1000 | Loss: 0.00003014
Iteration 176/1000 | Loss: 0.00003014
Iteration 177/1000 | Loss: 0.00003014
Iteration 178/1000 | Loss: 0.00003014
Iteration 179/1000 | Loss: 0.00003014
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [3.0144754418870434e-05, 3.0144754418870434e-05, 3.0144754418870434e-05, 3.0144754418870434e-05, 3.0144754418870434e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0144754418870434e-05

Optimization complete. Final v2v error: 4.81954288482666 mm

Highest mean error: 5.157488822937012 mm for frame 75

Lowest mean error: 4.669404983520508 mm for frame 160

Saving results

Total time: 38.83238244056702
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_46_us_2175/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_46_us_2175/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_46_us_2175/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01187386
Iteration 2/25 | Loss: 0.00237184
Iteration 3/25 | Loss: 0.00187730
Iteration 4/25 | Loss: 0.00169050
Iteration 5/25 | Loss: 0.00177220
Iteration 6/25 | Loss: 0.00167444
Iteration 7/25 | Loss: 0.00162404
Iteration 8/25 | Loss: 0.00163959
Iteration 9/25 | Loss: 0.00163151
Iteration 10/25 | Loss: 0.00160245
Iteration 11/25 | Loss: 0.00159427
Iteration 12/25 | Loss: 0.00160031
Iteration 13/25 | Loss: 0.00157479
Iteration 14/25 | Loss: 0.00158582
Iteration 15/25 | Loss: 0.00157402
Iteration 16/25 | Loss: 0.00157577
Iteration 17/25 | Loss: 0.00157432
Iteration 18/25 | Loss: 0.00157417
Iteration 19/25 | Loss: 0.00157480
Iteration 20/25 | Loss: 0.00157376
Iteration 21/25 | Loss: 0.00157365
Iteration 22/25 | Loss: 0.00157287
Iteration 23/25 | Loss: 0.00157356
Iteration 24/25 | Loss: 0.00157270
Iteration 25/25 | Loss: 0.00157259

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46015120
Iteration 2/25 | Loss: 0.00173622
Iteration 3/25 | Loss: 0.00173622
Iteration 4/25 | Loss: 0.00173622
Iteration 5/25 | Loss: 0.00173622
Iteration 6/25 | Loss: 0.00173622
Iteration 7/25 | Loss: 0.00173622
Iteration 8/25 | Loss: 0.00173622
Iteration 9/25 | Loss: 0.00173622
Iteration 10/25 | Loss: 0.00173622
Iteration 11/25 | Loss: 0.00173622
Iteration 12/25 | Loss: 0.00173622
Iteration 13/25 | Loss: 0.00173622
Iteration 14/25 | Loss: 0.00173622
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0017362221842631698, 0.0017362221842631698, 0.0017362221842631698, 0.0017362221842631698, 0.0017362221842631698]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017362221842631698

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00173622
Iteration 2/1000 | Loss: 0.00028767
Iteration 3/1000 | Loss: 0.00015496
Iteration 4/1000 | Loss: 0.00005395
Iteration 5/1000 | Loss: 0.00005326
Iteration 6/1000 | Loss: 0.00026528
Iteration 7/1000 | Loss: 0.00004672
Iteration 8/1000 | Loss: 0.00004808
Iteration 9/1000 | Loss: 0.00005256
Iteration 10/1000 | Loss: 0.00029092
Iteration 11/1000 | Loss: 0.00011536
Iteration 12/1000 | Loss: 0.00006033
Iteration 13/1000 | Loss: 0.00036043
Iteration 14/1000 | Loss: 0.00010276
Iteration 15/1000 | Loss: 0.00006505
Iteration 16/1000 | Loss: 0.00006313
Iteration 17/1000 | Loss: 0.00006561
Iteration 18/1000 | Loss: 0.00018380
Iteration 19/1000 | Loss: 0.00008915
Iteration 20/1000 | Loss: 0.00005358
Iteration 21/1000 | Loss: 0.00015135
Iteration 22/1000 | Loss: 0.00014509
Iteration 23/1000 | Loss: 0.00005213
Iteration 24/1000 | Loss: 0.00005306
Iteration 25/1000 | Loss: 0.00005287
Iteration 26/1000 | Loss: 0.00005153
Iteration 27/1000 | Loss: 0.00005647
Iteration 28/1000 | Loss: 0.00005873
Iteration 29/1000 | Loss: 0.00005404
Iteration 30/1000 | Loss: 0.00004531
Iteration 31/1000 | Loss: 0.00033932
Iteration 32/1000 | Loss: 0.00004276
Iteration 33/1000 | Loss: 0.00004189
Iteration 34/1000 | Loss: 0.00004129
Iteration 35/1000 | Loss: 0.00045516
Iteration 36/1000 | Loss: 0.00006419
Iteration 37/1000 | Loss: 0.00031937
Iteration 38/1000 | Loss: 0.00008581
Iteration 39/1000 | Loss: 0.00022224
Iteration 40/1000 | Loss: 0.00007203
Iteration 41/1000 | Loss: 0.00004056
Iteration 42/1000 | Loss: 0.00004006
Iteration 43/1000 | Loss: 0.00003984
Iteration 44/1000 | Loss: 0.00003981
Iteration 45/1000 | Loss: 0.00003974
Iteration 46/1000 | Loss: 0.00003970
Iteration 47/1000 | Loss: 0.00003967
Iteration 48/1000 | Loss: 0.00003967
Iteration 49/1000 | Loss: 0.00003967
Iteration 50/1000 | Loss: 0.00003967
Iteration 51/1000 | Loss: 0.00003967
Iteration 52/1000 | Loss: 0.00003967
Iteration 53/1000 | Loss: 0.00003967
Iteration 54/1000 | Loss: 0.00003964
Iteration 55/1000 | Loss: 0.00003960
Iteration 56/1000 | Loss: 0.00003957
Iteration 57/1000 | Loss: 0.00003949
Iteration 58/1000 | Loss: 0.00003948
Iteration 59/1000 | Loss: 0.00003943
Iteration 60/1000 | Loss: 0.00003943
Iteration 61/1000 | Loss: 0.00003943
Iteration 62/1000 | Loss: 0.00003943
Iteration 63/1000 | Loss: 0.00003942
Iteration 64/1000 | Loss: 0.00003938
Iteration 65/1000 | Loss: 0.00003933
Iteration 66/1000 | Loss: 0.00003933
Iteration 67/1000 | Loss: 0.00003933
Iteration 68/1000 | Loss: 0.00003933
Iteration 69/1000 | Loss: 0.00003933
Iteration 70/1000 | Loss: 0.00003933
Iteration 71/1000 | Loss: 0.00003933
Iteration 72/1000 | Loss: 0.00003933
Iteration 73/1000 | Loss: 0.00003933
Iteration 74/1000 | Loss: 0.00003933
Iteration 75/1000 | Loss: 0.00003933
Iteration 76/1000 | Loss: 0.00003933
Iteration 77/1000 | Loss: 0.00003932
Iteration 78/1000 | Loss: 0.00003932
Iteration 79/1000 | Loss: 0.00003932
Iteration 80/1000 | Loss: 0.00003932
Iteration 81/1000 | Loss: 0.00003932
Iteration 82/1000 | Loss: 0.00003932
Iteration 83/1000 | Loss: 0.00003932
Iteration 84/1000 | Loss: 0.00003932
Iteration 85/1000 | Loss: 0.00003932
Iteration 86/1000 | Loss: 0.00003932
Iteration 87/1000 | Loss: 0.00003932
Iteration 88/1000 | Loss: 0.00003932
Iteration 89/1000 | Loss: 0.00003932
Iteration 90/1000 | Loss: 0.00003931
Iteration 91/1000 | Loss: 0.00003931
Iteration 92/1000 | Loss: 0.00003931
Iteration 93/1000 | Loss: 0.00003931
Iteration 94/1000 | Loss: 0.00003931
Iteration 95/1000 | Loss: 0.00003931
Iteration 96/1000 | Loss: 0.00003931
Iteration 97/1000 | Loss: 0.00003931
Iteration 98/1000 | Loss: 0.00003931
Iteration 99/1000 | Loss: 0.00003931
Iteration 100/1000 | Loss: 0.00003930
Iteration 101/1000 | Loss: 0.00003930
Iteration 102/1000 | Loss: 0.00003930
Iteration 103/1000 | Loss: 0.00003929
Iteration 104/1000 | Loss: 0.00003929
Iteration 105/1000 | Loss: 0.00003929
Iteration 106/1000 | Loss: 0.00003929
Iteration 107/1000 | Loss: 0.00003929
Iteration 108/1000 | Loss: 0.00003929
Iteration 109/1000 | Loss: 0.00003928
Iteration 110/1000 | Loss: 0.00003928
Iteration 111/1000 | Loss: 0.00003928
Iteration 112/1000 | Loss: 0.00003927
Iteration 113/1000 | Loss: 0.00003927
Iteration 114/1000 | Loss: 0.00003926
Iteration 115/1000 | Loss: 0.00003926
Iteration 116/1000 | Loss: 0.00003926
Iteration 117/1000 | Loss: 0.00003924
Iteration 118/1000 | Loss: 0.00003924
Iteration 119/1000 | Loss: 0.00003923
Iteration 120/1000 | Loss: 0.00003923
Iteration 121/1000 | Loss: 0.00003922
Iteration 122/1000 | Loss: 0.00003922
Iteration 123/1000 | Loss: 0.00003922
Iteration 124/1000 | Loss: 0.00003919
Iteration 125/1000 | Loss: 0.00003919
Iteration 126/1000 | Loss: 0.00003918
Iteration 127/1000 | Loss: 0.00003918
Iteration 128/1000 | Loss: 0.00003916
Iteration 129/1000 | Loss: 0.00003916
Iteration 130/1000 | Loss: 0.00003916
Iteration 131/1000 | Loss: 0.00003916
Iteration 132/1000 | Loss: 0.00003916
Iteration 133/1000 | Loss: 0.00003916
Iteration 134/1000 | Loss: 0.00003916
Iteration 135/1000 | Loss: 0.00003916
Iteration 136/1000 | Loss: 0.00003916
Iteration 137/1000 | Loss: 0.00003916
Iteration 138/1000 | Loss: 0.00003915
Iteration 139/1000 | Loss: 0.00003915
Iteration 140/1000 | Loss: 0.00003915
Iteration 141/1000 | Loss: 0.00003914
Iteration 142/1000 | Loss: 0.00003914
Iteration 143/1000 | Loss: 0.00003912
Iteration 144/1000 | Loss: 0.00003912
Iteration 145/1000 | Loss: 0.00003911
Iteration 146/1000 | Loss: 0.00003910
Iteration 147/1000 | Loss: 0.00003910
Iteration 148/1000 | Loss: 0.00003910
Iteration 149/1000 | Loss: 0.00003910
Iteration 150/1000 | Loss: 0.00003910
Iteration 151/1000 | Loss: 0.00003910
Iteration 152/1000 | Loss: 0.00003909
Iteration 153/1000 | Loss: 0.00003909
Iteration 154/1000 | Loss: 0.00003909
Iteration 155/1000 | Loss: 0.00003909
Iteration 156/1000 | Loss: 0.00003909
Iteration 157/1000 | Loss: 0.00003909
Iteration 158/1000 | Loss: 0.00003908
Iteration 159/1000 | Loss: 0.00003908
Iteration 160/1000 | Loss: 0.00003908
Iteration 161/1000 | Loss: 0.00003908
Iteration 162/1000 | Loss: 0.00003908
Iteration 163/1000 | Loss: 0.00003908
Iteration 164/1000 | Loss: 0.00003908
Iteration 165/1000 | Loss: 0.00003908
Iteration 166/1000 | Loss: 0.00003908
Iteration 167/1000 | Loss: 0.00003907
Iteration 168/1000 | Loss: 0.00003907
Iteration 169/1000 | Loss: 0.00003907
Iteration 170/1000 | Loss: 0.00003907
Iteration 171/1000 | Loss: 0.00003907
Iteration 172/1000 | Loss: 0.00003907
Iteration 173/1000 | Loss: 0.00003906
Iteration 174/1000 | Loss: 0.00003906
Iteration 175/1000 | Loss: 0.00003906
Iteration 176/1000 | Loss: 0.00003906
Iteration 177/1000 | Loss: 0.00003906
Iteration 178/1000 | Loss: 0.00003906
Iteration 179/1000 | Loss: 0.00003906
Iteration 180/1000 | Loss: 0.00003905
Iteration 181/1000 | Loss: 0.00003905
Iteration 182/1000 | Loss: 0.00003905
Iteration 183/1000 | Loss: 0.00003905
Iteration 184/1000 | Loss: 0.00003905
Iteration 185/1000 | Loss: 0.00003905
Iteration 186/1000 | Loss: 0.00003905
Iteration 187/1000 | Loss: 0.00003905
Iteration 188/1000 | Loss: 0.00003904
Iteration 189/1000 | Loss: 0.00003904
Iteration 190/1000 | Loss: 0.00003904
Iteration 191/1000 | Loss: 0.00003904
Iteration 192/1000 | Loss: 0.00003904
Iteration 193/1000 | Loss: 0.00003904
Iteration 194/1000 | Loss: 0.00003904
Iteration 195/1000 | Loss: 0.00003904
Iteration 196/1000 | Loss: 0.00003904
Iteration 197/1000 | Loss: 0.00003904
Iteration 198/1000 | Loss: 0.00003904
Iteration 199/1000 | Loss: 0.00003904
Iteration 200/1000 | Loss: 0.00003904
Iteration 201/1000 | Loss: 0.00003904
Iteration 202/1000 | Loss: 0.00003904
Iteration 203/1000 | Loss: 0.00003904
Iteration 204/1000 | Loss: 0.00003904
Iteration 205/1000 | Loss: 0.00003903
Iteration 206/1000 | Loss: 0.00003903
Iteration 207/1000 | Loss: 0.00003903
Iteration 208/1000 | Loss: 0.00003903
Iteration 209/1000 | Loss: 0.00003903
Iteration 210/1000 | Loss: 0.00003903
Iteration 211/1000 | Loss: 0.00003902
Iteration 212/1000 | Loss: 0.00003902
Iteration 213/1000 | Loss: 0.00003901
Iteration 214/1000 | Loss: 0.00003901
Iteration 215/1000 | Loss: 0.00003901
Iteration 216/1000 | Loss: 0.00003901
Iteration 217/1000 | Loss: 0.00003901
Iteration 218/1000 | Loss: 0.00003901
Iteration 219/1000 | Loss: 0.00003901
Iteration 220/1000 | Loss: 0.00003900
Iteration 221/1000 | Loss: 0.00003900
Iteration 222/1000 | Loss: 0.00003900
Iteration 223/1000 | Loss: 0.00003900
Iteration 224/1000 | Loss: 0.00003900
Iteration 225/1000 | Loss: 0.00003900
Iteration 226/1000 | Loss: 0.00003900
Iteration 227/1000 | Loss: 0.00003900
Iteration 228/1000 | Loss: 0.00003900
Iteration 229/1000 | Loss: 0.00003900
Iteration 230/1000 | Loss: 0.00003899
Iteration 231/1000 | Loss: 0.00003899
Iteration 232/1000 | Loss: 0.00003899
Iteration 233/1000 | Loss: 0.00003899
Iteration 234/1000 | Loss: 0.00003899
Iteration 235/1000 | Loss: 0.00003899
Iteration 236/1000 | Loss: 0.00003899
Iteration 237/1000 | Loss: 0.00003899
Iteration 238/1000 | Loss: 0.00003899
Iteration 239/1000 | Loss: 0.00003899
Iteration 240/1000 | Loss: 0.00003899
Iteration 241/1000 | Loss: 0.00003899
Iteration 242/1000 | Loss: 0.00003898
Iteration 243/1000 | Loss: 0.00003898
Iteration 244/1000 | Loss: 0.00003898
Iteration 245/1000 | Loss: 0.00003898
Iteration 246/1000 | Loss: 0.00003898
Iteration 247/1000 | Loss: 0.00003898
Iteration 248/1000 | Loss: 0.00003898
Iteration 249/1000 | Loss: 0.00003898
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 249. Stopping optimization.
Last 5 losses: [3.89844281016849e-05, 3.89844281016849e-05, 3.89844281016849e-05, 3.89844281016849e-05, 3.89844281016849e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.89844281016849e-05

Optimization complete. Final v2v error: 5.223268508911133 mm

Highest mean error: 11.007634162902832 mm for frame 45

Lowest mean error: 4.844724655151367 mm for frame 170

Saving results

Total time: 147.13397789001465
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_46_us_2175/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_46_us_2175/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_46_us_2175/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01156282
Iteration 2/25 | Loss: 0.00246814
Iteration 3/25 | Loss: 0.00212582
Iteration 4/25 | Loss: 0.00216753
Iteration 5/25 | Loss: 0.00202160
Iteration 6/25 | Loss: 0.00189847
Iteration 7/25 | Loss: 0.00182488
Iteration 8/25 | Loss: 0.00179763
Iteration 9/25 | Loss: 0.00175592
Iteration 10/25 | Loss: 0.00173779
Iteration 11/25 | Loss: 0.00172356
Iteration 12/25 | Loss: 0.00172634
Iteration 13/25 | Loss: 0.00171563
Iteration 14/25 | Loss: 0.00171083
Iteration 15/25 | Loss: 0.00171263
Iteration 16/25 | Loss: 0.00170706
Iteration 17/25 | Loss: 0.00170861
Iteration 18/25 | Loss: 0.00170615
Iteration 19/25 | Loss: 0.00170828
Iteration 20/25 | Loss: 0.00169501
Iteration 21/25 | Loss: 0.00170123
Iteration 22/25 | Loss: 0.00169636
Iteration 23/25 | Loss: 0.00169131
Iteration 24/25 | Loss: 0.00168675
Iteration 25/25 | Loss: 0.00168554

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.14608359
Iteration 2/25 | Loss: 0.00260674
Iteration 3/25 | Loss: 0.00260674
Iteration 4/25 | Loss: 0.00260674
Iteration 5/25 | Loss: 0.00260674
Iteration 6/25 | Loss: 0.00260674
Iteration 7/25 | Loss: 0.00260674
Iteration 8/25 | Loss: 0.00260674
Iteration 9/25 | Loss: 0.00260674
Iteration 10/25 | Loss: 0.00260674
Iteration 11/25 | Loss: 0.00260673
Iteration 12/25 | Loss: 0.00260673
Iteration 13/25 | Loss: 0.00260673
Iteration 14/25 | Loss: 0.00260673
Iteration 15/25 | Loss: 0.00260673
Iteration 16/25 | Loss: 0.00260673
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0026067348662763834, 0.0026067348662763834, 0.0026067348662763834, 0.0026067348662763834, 0.0026067348662763834]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026067348662763834

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00260673
Iteration 2/1000 | Loss: 0.00014881
Iteration 3/1000 | Loss: 0.00007202
Iteration 4/1000 | Loss: 0.00005412
Iteration 5/1000 | Loss: 0.00004725
Iteration 6/1000 | Loss: 0.00004384
Iteration 7/1000 | Loss: 0.00004200
Iteration 8/1000 | Loss: 0.00004057
Iteration 9/1000 | Loss: 0.00003975
Iteration 10/1000 | Loss: 0.00003927
Iteration 11/1000 | Loss: 0.00003882
Iteration 12/1000 | Loss: 0.00003846
Iteration 13/1000 | Loss: 0.00003818
Iteration 14/1000 | Loss: 0.00003795
Iteration 15/1000 | Loss: 0.00003774
Iteration 16/1000 | Loss: 0.00003762
Iteration 17/1000 | Loss: 0.00003752
Iteration 18/1000 | Loss: 0.00003751
Iteration 19/1000 | Loss: 0.00003748
Iteration 20/1000 | Loss: 0.00003794
Iteration 21/1000 | Loss: 0.00003793
Iteration 22/1000 | Loss: 0.00003793
Iteration 23/1000 | Loss: 0.00003771
Iteration 24/1000 | Loss: 0.00003743
Iteration 25/1000 | Loss: 0.00003728
Iteration 26/1000 | Loss: 0.00003731
Iteration 27/1000 | Loss: 0.00003730
Iteration 28/1000 | Loss: 0.00003730
Iteration 29/1000 | Loss: 0.00003730
Iteration 30/1000 | Loss: 0.00003729
Iteration 31/1000 | Loss: 0.00003723
Iteration 32/1000 | Loss: 0.00003719
Iteration 33/1000 | Loss: 0.00003718
Iteration 34/1000 | Loss: 0.00003715
Iteration 35/1000 | Loss: 0.00003715
Iteration 36/1000 | Loss: 0.00003715
Iteration 37/1000 | Loss: 0.00003715
Iteration 38/1000 | Loss: 0.00003715
Iteration 39/1000 | Loss: 0.00003715
Iteration 40/1000 | Loss: 0.00003714
Iteration 41/1000 | Loss: 0.00003714
Iteration 42/1000 | Loss: 0.00003714
Iteration 43/1000 | Loss: 0.00003714
Iteration 44/1000 | Loss: 0.00003714
Iteration 45/1000 | Loss: 0.00003714
Iteration 46/1000 | Loss: 0.00003714
Iteration 47/1000 | Loss: 0.00003714
Iteration 48/1000 | Loss: 0.00003714
Iteration 49/1000 | Loss: 0.00003714
Iteration 50/1000 | Loss: 0.00003713
Iteration 51/1000 | Loss: 0.00003713
Iteration 52/1000 | Loss: 0.00003713
Iteration 53/1000 | Loss: 0.00003713
Iteration 54/1000 | Loss: 0.00003713
Iteration 55/1000 | Loss: 0.00003713
Iteration 56/1000 | Loss: 0.00003713
Iteration 57/1000 | Loss: 0.00003713
Iteration 58/1000 | Loss: 0.00003713
Iteration 59/1000 | Loss: 0.00003713
Iteration 60/1000 | Loss: 0.00003713
Iteration 61/1000 | Loss: 0.00003713
Iteration 62/1000 | Loss: 0.00003712
Iteration 63/1000 | Loss: 0.00003712
Iteration 64/1000 | Loss: 0.00003712
Iteration 65/1000 | Loss: 0.00003712
Iteration 66/1000 | Loss: 0.00003712
Iteration 67/1000 | Loss: 0.00003712
Iteration 68/1000 | Loss: 0.00003712
Iteration 69/1000 | Loss: 0.00003712
Iteration 70/1000 | Loss: 0.00003712
Iteration 71/1000 | Loss: 0.00003712
Iteration 72/1000 | Loss: 0.00003712
Iteration 73/1000 | Loss: 0.00003712
Iteration 74/1000 | Loss: 0.00003712
Iteration 75/1000 | Loss: 0.00003712
Iteration 76/1000 | Loss: 0.00003712
Iteration 77/1000 | Loss: 0.00003712
Iteration 78/1000 | Loss: 0.00003712
Iteration 79/1000 | Loss: 0.00003712
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 79. Stopping optimization.
Last 5 losses: [3.712092075147666e-05, 3.712092075147666e-05, 3.712092075147666e-05, 3.712092075147666e-05, 3.712092075147666e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.712092075147666e-05

Optimization complete. Final v2v error: 5.2991461753845215 mm

Highest mean error: 11.706737518310547 mm for frame 57

Lowest mean error: 4.765268325805664 mm for frame 73

Saving results

Total time: 77.99135422706604
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_46_us_2175/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_46_us_2175/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_46_us_2175/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00497814
Iteration 2/25 | Loss: 0.00188079
Iteration 3/25 | Loss: 0.00178504
Iteration 4/25 | Loss: 0.00176588
Iteration 5/25 | Loss: 0.00175846
Iteration 6/25 | Loss: 0.00175624
Iteration 7/25 | Loss: 0.00175588
Iteration 8/25 | Loss: 0.00175588
Iteration 9/25 | Loss: 0.00175588
Iteration 10/25 | Loss: 0.00175588
Iteration 11/25 | Loss: 0.00175588
Iteration 12/25 | Loss: 0.00175588
Iteration 13/25 | Loss: 0.00175588
Iteration 14/25 | Loss: 0.00175588
Iteration 15/25 | Loss: 0.00175588
Iteration 16/25 | Loss: 0.00175588
Iteration 17/25 | Loss: 0.00175588
Iteration 18/25 | Loss: 0.00175588
Iteration 19/25 | Loss: 0.00175588
Iteration 20/25 | Loss: 0.00175588
Iteration 21/25 | Loss: 0.00175588
Iteration 22/25 | Loss: 0.00175588
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0017558758845552802, 0.0017558758845552802, 0.0017558758845552802, 0.0017558758845552802, 0.0017558758845552802]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017558758845552802

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.59186792
Iteration 2/25 | Loss: 0.00236391
Iteration 3/25 | Loss: 0.00236390
Iteration 4/25 | Loss: 0.00236390
Iteration 5/25 | Loss: 0.00236390
Iteration 6/25 | Loss: 0.00236390
Iteration 7/25 | Loss: 0.00236390
Iteration 8/25 | Loss: 0.00236390
Iteration 9/25 | Loss: 0.00236390
Iteration 10/25 | Loss: 0.00236390
Iteration 11/25 | Loss: 0.00236390
Iteration 12/25 | Loss: 0.00236390
Iteration 13/25 | Loss: 0.00236390
Iteration 14/25 | Loss: 0.00236390
Iteration 15/25 | Loss: 0.00236390
Iteration 16/25 | Loss: 0.00236390
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0023638952989131212, 0.0023638952989131212, 0.0023638952989131212, 0.0023638952989131212, 0.0023638952989131212]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023638952989131212

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00236390
Iteration 2/1000 | Loss: 0.00006016
Iteration 3/1000 | Loss: 0.00004611
Iteration 4/1000 | Loss: 0.00004133
Iteration 5/1000 | Loss: 0.00003953
Iteration 6/1000 | Loss: 0.00003832
Iteration 7/1000 | Loss: 0.00003772
Iteration 8/1000 | Loss: 0.00003722
Iteration 9/1000 | Loss: 0.00003676
Iteration 10/1000 | Loss: 0.00003673
Iteration 11/1000 | Loss: 0.00003652
Iteration 12/1000 | Loss: 0.00003630
Iteration 13/1000 | Loss: 0.00003619
Iteration 14/1000 | Loss: 0.00003618
Iteration 15/1000 | Loss: 0.00003617
Iteration 16/1000 | Loss: 0.00003617
Iteration 17/1000 | Loss: 0.00003616
Iteration 18/1000 | Loss: 0.00003612
Iteration 19/1000 | Loss: 0.00003609
Iteration 20/1000 | Loss: 0.00003608
Iteration 21/1000 | Loss: 0.00003607
Iteration 22/1000 | Loss: 0.00003604
Iteration 23/1000 | Loss: 0.00003599
Iteration 24/1000 | Loss: 0.00003598
Iteration 25/1000 | Loss: 0.00003597
Iteration 26/1000 | Loss: 0.00003597
Iteration 27/1000 | Loss: 0.00003595
Iteration 28/1000 | Loss: 0.00003594
Iteration 29/1000 | Loss: 0.00003591
Iteration 30/1000 | Loss: 0.00003590
Iteration 31/1000 | Loss: 0.00003589
Iteration 32/1000 | Loss: 0.00003589
Iteration 33/1000 | Loss: 0.00003588
Iteration 34/1000 | Loss: 0.00003588
Iteration 35/1000 | Loss: 0.00003588
Iteration 36/1000 | Loss: 0.00003588
Iteration 37/1000 | Loss: 0.00003588
Iteration 38/1000 | Loss: 0.00003588
Iteration 39/1000 | Loss: 0.00003588
Iteration 40/1000 | Loss: 0.00003586
Iteration 41/1000 | Loss: 0.00003586
Iteration 42/1000 | Loss: 0.00003585
Iteration 43/1000 | Loss: 0.00003585
Iteration 44/1000 | Loss: 0.00003585
Iteration 45/1000 | Loss: 0.00003585
Iteration 46/1000 | Loss: 0.00003584
Iteration 47/1000 | Loss: 0.00003584
Iteration 48/1000 | Loss: 0.00003584
Iteration 49/1000 | Loss: 0.00003584
Iteration 50/1000 | Loss: 0.00003584
Iteration 51/1000 | Loss: 0.00003584
Iteration 52/1000 | Loss: 0.00003584
Iteration 53/1000 | Loss: 0.00003583
Iteration 54/1000 | Loss: 0.00003583
Iteration 55/1000 | Loss: 0.00003583
Iteration 56/1000 | Loss: 0.00003583
Iteration 57/1000 | Loss: 0.00003583
Iteration 58/1000 | Loss: 0.00003582
Iteration 59/1000 | Loss: 0.00003582
Iteration 60/1000 | Loss: 0.00003582
Iteration 61/1000 | Loss: 0.00003581
Iteration 62/1000 | Loss: 0.00003581
Iteration 63/1000 | Loss: 0.00003580
Iteration 64/1000 | Loss: 0.00003580
Iteration 65/1000 | Loss: 0.00003580
Iteration 66/1000 | Loss: 0.00003580
Iteration 67/1000 | Loss: 0.00003580
Iteration 68/1000 | Loss: 0.00003580
Iteration 69/1000 | Loss: 0.00003580
Iteration 70/1000 | Loss: 0.00003580
Iteration 71/1000 | Loss: 0.00003579
Iteration 72/1000 | Loss: 0.00003579
Iteration 73/1000 | Loss: 0.00003579
Iteration 74/1000 | Loss: 0.00003579
Iteration 75/1000 | Loss: 0.00003579
Iteration 76/1000 | Loss: 0.00003579
Iteration 77/1000 | Loss: 0.00003579
Iteration 78/1000 | Loss: 0.00003579
Iteration 79/1000 | Loss: 0.00003579
Iteration 80/1000 | Loss: 0.00003579
Iteration 81/1000 | Loss: 0.00003579
Iteration 82/1000 | Loss: 0.00003578
Iteration 83/1000 | Loss: 0.00003578
Iteration 84/1000 | Loss: 0.00003578
Iteration 85/1000 | Loss: 0.00003578
Iteration 86/1000 | Loss: 0.00003578
Iteration 87/1000 | Loss: 0.00003578
Iteration 88/1000 | Loss: 0.00003578
Iteration 89/1000 | Loss: 0.00003578
Iteration 90/1000 | Loss: 0.00003578
Iteration 91/1000 | Loss: 0.00003578
Iteration 92/1000 | Loss: 0.00003578
Iteration 93/1000 | Loss: 0.00003578
Iteration 94/1000 | Loss: 0.00003578
Iteration 95/1000 | Loss: 0.00003578
Iteration 96/1000 | Loss: 0.00003578
Iteration 97/1000 | Loss: 0.00003578
Iteration 98/1000 | Loss: 0.00003578
Iteration 99/1000 | Loss: 0.00003578
Iteration 100/1000 | Loss: 0.00003578
Iteration 101/1000 | Loss: 0.00003578
Iteration 102/1000 | Loss: 0.00003578
Iteration 103/1000 | Loss: 0.00003578
Iteration 104/1000 | Loss: 0.00003578
Iteration 105/1000 | Loss: 0.00003578
Iteration 106/1000 | Loss: 0.00003578
Iteration 107/1000 | Loss: 0.00003578
Iteration 108/1000 | Loss: 0.00003578
Iteration 109/1000 | Loss: 0.00003578
Iteration 110/1000 | Loss: 0.00003578
Iteration 111/1000 | Loss: 0.00003578
Iteration 112/1000 | Loss: 0.00003578
Iteration 113/1000 | Loss: 0.00003578
Iteration 114/1000 | Loss: 0.00003578
Iteration 115/1000 | Loss: 0.00003578
Iteration 116/1000 | Loss: 0.00003578
Iteration 117/1000 | Loss: 0.00003578
Iteration 118/1000 | Loss: 0.00003578
Iteration 119/1000 | Loss: 0.00003578
Iteration 120/1000 | Loss: 0.00003578
Iteration 121/1000 | Loss: 0.00003578
Iteration 122/1000 | Loss: 0.00003578
Iteration 123/1000 | Loss: 0.00003578
Iteration 124/1000 | Loss: 0.00003578
Iteration 125/1000 | Loss: 0.00003578
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [3.5776072763837874e-05, 3.5776072763837874e-05, 3.5776072763837874e-05, 3.5776072763837874e-05, 3.5776072763837874e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.5776072763837874e-05

Optimization complete. Final v2v error: 5.245872974395752 mm

Highest mean error: 5.635282039642334 mm for frame 89

Lowest mean error: 4.899400234222412 mm for frame 0

Saving results

Total time: 36.60887861251831
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_46_us_2175/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_46_us_2175/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_46_us_2175/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00924823
Iteration 2/25 | Loss: 0.00199662
Iteration 3/25 | Loss: 0.00178311
Iteration 4/25 | Loss: 0.00176595
Iteration 5/25 | Loss: 0.00176305
Iteration 6/25 | Loss: 0.00176303
Iteration 7/25 | Loss: 0.00176303
Iteration 8/25 | Loss: 0.00176303
Iteration 9/25 | Loss: 0.00176303
Iteration 10/25 | Loss: 0.00176303
Iteration 11/25 | Loss: 0.00176303
Iteration 12/25 | Loss: 0.00176303
Iteration 13/25 | Loss: 0.00176303
Iteration 14/25 | Loss: 0.00176303
Iteration 15/25 | Loss: 0.00176303
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0017630300717428327, 0.0017630300717428327, 0.0017630300717428327, 0.0017630300717428327, 0.0017630300717428327]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017630300717428327

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48446143
Iteration 2/25 | Loss: 0.00221001
Iteration 3/25 | Loss: 0.00221001
Iteration 4/25 | Loss: 0.00221001
Iteration 5/25 | Loss: 0.00221001
Iteration 6/25 | Loss: 0.00221001
Iteration 7/25 | Loss: 0.00221001
Iteration 8/25 | Loss: 0.00221001
Iteration 9/25 | Loss: 0.00221001
Iteration 10/25 | Loss: 0.00221001
Iteration 11/25 | Loss: 0.00221001
Iteration 12/25 | Loss: 0.00221001
Iteration 13/25 | Loss: 0.00221001
Iteration 14/25 | Loss: 0.00221001
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.002210010075941682, 0.002210010075941682, 0.002210010075941682, 0.002210010075941682, 0.002210010075941682]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002210010075941682

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00221001
Iteration 2/1000 | Loss: 0.00007121
Iteration 3/1000 | Loss: 0.00004784
Iteration 4/1000 | Loss: 0.00004185
Iteration 5/1000 | Loss: 0.00003826
Iteration 6/1000 | Loss: 0.00003641
Iteration 7/1000 | Loss: 0.00003534
Iteration 8/1000 | Loss: 0.00003462
Iteration 9/1000 | Loss: 0.00003404
Iteration 10/1000 | Loss: 0.00003366
Iteration 11/1000 | Loss: 0.00003328
Iteration 12/1000 | Loss: 0.00003304
Iteration 13/1000 | Loss: 0.00003293
Iteration 14/1000 | Loss: 0.00003286
Iteration 15/1000 | Loss: 0.00003280
Iteration 16/1000 | Loss: 0.00003279
Iteration 17/1000 | Loss: 0.00003278
Iteration 18/1000 | Loss: 0.00003277
Iteration 19/1000 | Loss: 0.00003276
Iteration 20/1000 | Loss: 0.00003274
Iteration 21/1000 | Loss: 0.00003274
Iteration 22/1000 | Loss: 0.00003274
Iteration 23/1000 | Loss: 0.00003273
Iteration 24/1000 | Loss: 0.00003273
Iteration 25/1000 | Loss: 0.00003273
Iteration 26/1000 | Loss: 0.00003269
Iteration 27/1000 | Loss: 0.00003269
Iteration 28/1000 | Loss: 0.00003268
Iteration 29/1000 | Loss: 0.00003267
Iteration 30/1000 | Loss: 0.00003265
Iteration 31/1000 | Loss: 0.00003264
Iteration 32/1000 | Loss: 0.00003264
Iteration 33/1000 | Loss: 0.00003263
Iteration 34/1000 | Loss: 0.00003262
Iteration 35/1000 | Loss: 0.00003261
Iteration 36/1000 | Loss: 0.00003260
Iteration 37/1000 | Loss: 0.00003259
Iteration 38/1000 | Loss: 0.00003258
Iteration 39/1000 | Loss: 0.00003257
Iteration 40/1000 | Loss: 0.00003257
Iteration 41/1000 | Loss: 0.00003257
Iteration 42/1000 | Loss: 0.00003256
Iteration 43/1000 | Loss: 0.00003256
Iteration 44/1000 | Loss: 0.00003255
Iteration 45/1000 | Loss: 0.00003255
Iteration 46/1000 | Loss: 0.00003255
Iteration 47/1000 | Loss: 0.00003255
Iteration 48/1000 | Loss: 0.00003255
Iteration 49/1000 | Loss: 0.00003255
Iteration 50/1000 | Loss: 0.00003255
Iteration 51/1000 | Loss: 0.00003255
Iteration 52/1000 | Loss: 0.00003255
Iteration 53/1000 | Loss: 0.00003254
Iteration 54/1000 | Loss: 0.00003254
Iteration 55/1000 | Loss: 0.00003254
Iteration 56/1000 | Loss: 0.00003253
Iteration 57/1000 | Loss: 0.00003253
Iteration 58/1000 | Loss: 0.00003253
Iteration 59/1000 | Loss: 0.00003253
Iteration 60/1000 | Loss: 0.00003253
Iteration 61/1000 | Loss: 0.00003253
Iteration 62/1000 | Loss: 0.00003253
Iteration 63/1000 | Loss: 0.00003252
Iteration 64/1000 | Loss: 0.00003252
Iteration 65/1000 | Loss: 0.00003252
Iteration 66/1000 | Loss: 0.00003251
Iteration 67/1000 | Loss: 0.00003251
Iteration 68/1000 | Loss: 0.00003251
Iteration 69/1000 | Loss: 0.00003251
Iteration 70/1000 | Loss: 0.00003251
Iteration 71/1000 | Loss: 0.00003250
Iteration 72/1000 | Loss: 0.00003250
Iteration 73/1000 | Loss: 0.00003250
Iteration 74/1000 | Loss: 0.00003250
Iteration 75/1000 | Loss: 0.00003250
Iteration 76/1000 | Loss: 0.00003250
Iteration 77/1000 | Loss: 0.00003250
Iteration 78/1000 | Loss: 0.00003250
Iteration 79/1000 | Loss: 0.00003250
Iteration 80/1000 | Loss: 0.00003249
Iteration 81/1000 | Loss: 0.00003249
Iteration 82/1000 | Loss: 0.00003249
Iteration 83/1000 | Loss: 0.00003249
Iteration 84/1000 | Loss: 0.00003249
Iteration 85/1000 | Loss: 0.00003249
Iteration 86/1000 | Loss: 0.00003249
Iteration 87/1000 | Loss: 0.00003249
Iteration 88/1000 | Loss: 0.00003249
Iteration 89/1000 | Loss: 0.00003249
Iteration 90/1000 | Loss: 0.00003249
Iteration 91/1000 | Loss: 0.00003249
Iteration 92/1000 | Loss: 0.00003249
Iteration 93/1000 | Loss: 0.00003249
Iteration 94/1000 | Loss: 0.00003249
Iteration 95/1000 | Loss: 0.00003249
Iteration 96/1000 | Loss: 0.00003249
Iteration 97/1000 | Loss: 0.00003249
Iteration 98/1000 | Loss: 0.00003249
Iteration 99/1000 | Loss: 0.00003248
Iteration 100/1000 | Loss: 0.00003248
Iteration 101/1000 | Loss: 0.00003248
Iteration 102/1000 | Loss: 0.00003248
Iteration 103/1000 | Loss: 0.00003248
Iteration 104/1000 | Loss: 0.00003248
Iteration 105/1000 | Loss: 0.00003248
Iteration 106/1000 | Loss: 0.00003248
Iteration 107/1000 | Loss: 0.00003248
Iteration 108/1000 | Loss: 0.00003248
Iteration 109/1000 | Loss: 0.00003248
Iteration 110/1000 | Loss: 0.00003248
Iteration 111/1000 | Loss: 0.00003248
Iteration 112/1000 | Loss: 0.00003248
Iteration 113/1000 | Loss: 0.00003248
Iteration 114/1000 | Loss: 0.00003248
Iteration 115/1000 | Loss: 0.00003248
Iteration 116/1000 | Loss: 0.00003248
Iteration 117/1000 | Loss: 0.00003248
Iteration 118/1000 | Loss: 0.00003248
Iteration 119/1000 | Loss: 0.00003248
Iteration 120/1000 | Loss: 0.00003248
Iteration 121/1000 | Loss: 0.00003248
Iteration 122/1000 | Loss: 0.00003248
Iteration 123/1000 | Loss: 0.00003248
Iteration 124/1000 | Loss: 0.00003248
Iteration 125/1000 | Loss: 0.00003248
Iteration 126/1000 | Loss: 0.00003248
Iteration 127/1000 | Loss: 0.00003248
Iteration 128/1000 | Loss: 0.00003248
Iteration 129/1000 | Loss: 0.00003248
Iteration 130/1000 | Loss: 0.00003248
Iteration 131/1000 | Loss: 0.00003248
Iteration 132/1000 | Loss: 0.00003248
Iteration 133/1000 | Loss: 0.00003248
Iteration 134/1000 | Loss: 0.00003248
Iteration 135/1000 | Loss: 0.00003248
Iteration 136/1000 | Loss: 0.00003248
Iteration 137/1000 | Loss: 0.00003248
Iteration 138/1000 | Loss: 0.00003248
Iteration 139/1000 | Loss: 0.00003248
Iteration 140/1000 | Loss: 0.00003248
Iteration 141/1000 | Loss: 0.00003248
Iteration 142/1000 | Loss: 0.00003248
Iteration 143/1000 | Loss: 0.00003248
Iteration 144/1000 | Loss: 0.00003248
Iteration 145/1000 | Loss: 0.00003248
Iteration 146/1000 | Loss: 0.00003248
Iteration 147/1000 | Loss: 0.00003248
Iteration 148/1000 | Loss: 0.00003248
Iteration 149/1000 | Loss: 0.00003248
Iteration 150/1000 | Loss: 0.00003248
Iteration 151/1000 | Loss: 0.00003248
Iteration 152/1000 | Loss: 0.00003248
Iteration 153/1000 | Loss: 0.00003248
Iteration 154/1000 | Loss: 0.00003248
Iteration 155/1000 | Loss: 0.00003248
Iteration 156/1000 | Loss: 0.00003248
Iteration 157/1000 | Loss: 0.00003248
Iteration 158/1000 | Loss: 0.00003248
Iteration 159/1000 | Loss: 0.00003248
Iteration 160/1000 | Loss: 0.00003248
Iteration 161/1000 | Loss: 0.00003248
Iteration 162/1000 | Loss: 0.00003248
Iteration 163/1000 | Loss: 0.00003248
Iteration 164/1000 | Loss: 0.00003248
Iteration 165/1000 | Loss: 0.00003248
Iteration 166/1000 | Loss: 0.00003248
Iteration 167/1000 | Loss: 0.00003248
Iteration 168/1000 | Loss: 0.00003248
Iteration 169/1000 | Loss: 0.00003248
Iteration 170/1000 | Loss: 0.00003248
Iteration 171/1000 | Loss: 0.00003248
Iteration 172/1000 | Loss: 0.00003248
Iteration 173/1000 | Loss: 0.00003248
Iteration 174/1000 | Loss: 0.00003248
Iteration 175/1000 | Loss: 0.00003248
Iteration 176/1000 | Loss: 0.00003248
Iteration 177/1000 | Loss: 0.00003248
Iteration 178/1000 | Loss: 0.00003248
Iteration 179/1000 | Loss: 0.00003248
Iteration 180/1000 | Loss: 0.00003248
Iteration 181/1000 | Loss: 0.00003248
Iteration 182/1000 | Loss: 0.00003248
Iteration 183/1000 | Loss: 0.00003248
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [3.2481675589224324e-05, 3.2481675589224324e-05, 3.2481675589224324e-05, 3.2481675589224324e-05, 3.2481675589224324e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.2481675589224324e-05

Optimization complete. Final v2v error: 4.973222732543945 mm

Highest mean error: 5.335951805114746 mm for frame 8

Lowest mean error: 4.62735652923584 mm for frame 57

Saving results

Total time: 37.85294222831726
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_46_us_2175/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_46_us_2175/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_46_us_2175/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01174742
Iteration 2/25 | Loss: 0.00545655
Iteration 3/25 | Loss: 0.00380838
Iteration 4/25 | Loss: 0.00327408
Iteration 5/25 | Loss: 0.00316251
Iteration 6/25 | Loss: 0.00265403
Iteration 7/25 | Loss: 0.00240644
Iteration 8/25 | Loss: 0.00228942
Iteration 9/25 | Loss: 0.00224624
Iteration 10/25 | Loss: 0.00209347
Iteration 11/25 | Loss: 0.00198210
Iteration 12/25 | Loss: 0.00194069
Iteration 13/25 | Loss: 0.00189341
Iteration 14/25 | Loss: 0.00188435
Iteration 15/25 | Loss: 0.00187292
Iteration 16/25 | Loss: 0.00180873
Iteration 17/25 | Loss: 0.00175950
Iteration 18/25 | Loss: 0.00173664
Iteration 19/25 | Loss: 0.00172569
Iteration 20/25 | Loss: 0.00168992
Iteration 21/25 | Loss: 0.00167961
Iteration 22/25 | Loss: 0.00167119
Iteration 23/25 | Loss: 0.00165852
Iteration 24/25 | Loss: 0.00165568
Iteration 25/25 | Loss: 0.00164784

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.65427864
Iteration 2/25 | Loss: 0.00430423
Iteration 3/25 | Loss: 0.00419903
Iteration 4/25 | Loss: 0.00419903
Iteration 5/25 | Loss: 0.00419903
Iteration 6/25 | Loss: 0.00419903
Iteration 7/25 | Loss: 0.00419903
Iteration 8/25 | Loss: 0.00419903
Iteration 9/25 | Loss: 0.00419903
Iteration 10/25 | Loss: 0.00419903
Iteration 11/25 | Loss: 0.00419903
Iteration 12/25 | Loss: 0.00419903
Iteration 13/25 | Loss: 0.00419903
Iteration 14/25 | Loss: 0.00419903
Iteration 15/25 | Loss: 0.00419903
Iteration 16/25 | Loss: 0.00419903
Iteration 17/25 | Loss: 0.00419903
Iteration 18/25 | Loss: 0.00419903
Iteration 19/25 | Loss: 0.00419903
Iteration 20/25 | Loss: 0.00419903
Iteration 21/25 | Loss: 0.00419903
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00419903127476573, 0.00419903127476573, 0.00419903127476573, 0.00419903127476573, 0.00419903127476573]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00419903127476573

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00419903
Iteration 2/1000 | Loss: 0.00177509
Iteration 3/1000 | Loss: 0.00058196
Iteration 4/1000 | Loss: 0.00246709
Iteration 5/1000 | Loss: 0.00136979
Iteration 6/1000 | Loss: 0.00104032
Iteration 7/1000 | Loss: 0.00137342
Iteration 8/1000 | Loss: 0.00128345
Iteration 9/1000 | Loss: 0.00423808
Iteration 10/1000 | Loss: 0.00459020
Iteration 11/1000 | Loss: 0.00033551
Iteration 12/1000 | Loss: 0.00055853
Iteration 13/1000 | Loss: 0.00100242
Iteration 14/1000 | Loss: 0.00153782
Iteration 15/1000 | Loss: 0.00122420
Iteration 16/1000 | Loss: 0.00019578
Iteration 17/1000 | Loss: 0.00079320
Iteration 18/1000 | Loss: 0.00088970
Iteration 19/1000 | Loss: 0.00131377
Iteration 20/1000 | Loss: 0.00159142
Iteration 21/1000 | Loss: 0.00236462
Iteration 22/1000 | Loss: 0.00118030
Iteration 23/1000 | Loss: 0.00066262
Iteration 24/1000 | Loss: 0.00032733
Iteration 25/1000 | Loss: 0.00095546
Iteration 26/1000 | Loss: 0.00128367
Iteration 27/1000 | Loss: 0.00112920
Iteration 28/1000 | Loss: 0.00193857
Iteration 29/1000 | Loss: 0.00039076
Iteration 30/1000 | Loss: 0.00089490
Iteration 31/1000 | Loss: 0.00079470
Iteration 32/1000 | Loss: 0.00202906
Iteration 33/1000 | Loss: 0.00056234
Iteration 34/1000 | Loss: 0.00015165
Iteration 35/1000 | Loss: 0.00020187
Iteration 36/1000 | Loss: 0.00044345
Iteration 37/1000 | Loss: 0.00039896
Iteration 38/1000 | Loss: 0.00020530
Iteration 39/1000 | Loss: 0.00021641
Iteration 40/1000 | Loss: 0.00025317
Iteration 41/1000 | Loss: 0.00025692
Iteration 42/1000 | Loss: 0.00010314
Iteration 43/1000 | Loss: 0.00036742
Iteration 44/1000 | Loss: 0.00009162
Iteration 45/1000 | Loss: 0.00039645
Iteration 46/1000 | Loss: 0.00083360
Iteration 47/1000 | Loss: 0.00026055
Iteration 48/1000 | Loss: 0.00009950
Iteration 49/1000 | Loss: 0.00030791
Iteration 50/1000 | Loss: 0.00034724
Iteration 51/1000 | Loss: 0.00049002
Iteration 52/1000 | Loss: 0.00033724
Iteration 53/1000 | Loss: 0.00008589
Iteration 54/1000 | Loss: 0.00033606
Iteration 55/1000 | Loss: 0.00103560
Iteration 56/1000 | Loss: 0.00090145
Iteration 57/1000 | Loss: 0.00051016
Iteration 58/1000 | Loss: 0.00015736
Iteration 59/1000 | Loss: 0.00016092
Iteration 60/1000 | Loss: 0.00059334
Iteration 61/1000 | Loss: 0.00035245
Iteration 62/1000 | Loss: 0.00053030
Iteration 63/1000 | Loss: 0.00044411
Iteration 64/1000 | Loss: 0.00022769
Iteration 65/1000 | Loss: 0.00041109
Iteration 66/1000 | Loss: 0.00027155
Iteration 67/1000 | Loss: 0.00041764
Iteration 68/1000 | Loss: 0.00141835
Iteration 69/1000 | Loss: 0.00055453
Iteration 70/1000 | Loss: 0.00024245
Iteration 71/1000 | Loss: 0.00009365
Iteration 72/1000 | Loss: 0.00019616
Iteration 73/1000 | Loss: 0.00030819
Iteration 74/1000 | Loss: 0.00007137
Iteration 75/1000 | Loss: 0.00056173
Iteration 76/1000 | Loss: 0.00009345
Iteration 77/1000 | Loss: 0.00008544
Iteration 78/1000 | Loss: 0.00006643
Iteration 79/1000 | Loss: 0.00016743
Iteration 80/1000 | Loss: 0.00006576
Iteration 81/1000 | Loss: 0.00006445
Iteration 82/1000 | Loss: 0.00006395
Iteration 83/1000 | Loss: 0.00016821
Iteration 84/1000 | Loss: 0.00006455
Iteration 85/1000 | Loss: 0.00006333
Iteration 86/1000 | Loss: 0.00014431
Iteration 87/1000 | Loss: 0.00006352
Iteration 88/1000 | Loss: 0.00018851
Iteration 89/1000 | Loss: 0.00006296
Iteration 90/1000 | Loss: 0.00006272
Iteration 91/1000 | Loss: 0.00012609
Iteration 92/1000 | Loss: 0.00008104
Iteration 93/1000 | Loss: 0.00008195
Iteration 94/1000 | Loss: 0.00006265
Iteration 95/1000 | Loss: 0.00006261
Iteration 96/1000 | Loss: 0.00006246
Iteration 97/1000 | Loss: 0.00006246
Iteration 98/1000 | Loss: 0.00006240
Iteration 99/1000 | Loss: 0.00006238
Iteration 100/1000 | Loss: 0.00006238
Iteration 101/1000 | Loss: 0.00006237
Iteration 102/1000 | Loss: 0.00006236
Iteration 103/1000 | Loss: 0.00006236
Iteration 104/1000 | Loss: 0.00006236
Iteration 105/1000 | Loss: 0.00006235
Iteration 106/1000 | Loss: 0.00006235
Iteration 107/1000 | Loss: 0.00006235
Iteration 108/1000 | Loss: 0.00006235
Iteration 109/1000 | Loss: 0.00006235
Iteration 110/1000 | Loss: 0.00006234
Iteration 111/1000 | Loss: 0.00006234
Iteration 112/1000 | Loss: 0.00006234
Iteration 113/1000 | Loss: 0.00006234
Iteration 114/1000 | Loss: 0.00006234
Iteration 115/1000 | Loss: 0.00006234
Iteration 116/1000 | Loss: 0.00006233
Iteration 117/1000 | Loss: 0.00006233
Iteration 118/1000 | Loss: 0.00006233
Iteration 119/1000 | Loss: 0.00006233
Iteration 120/1000 | Loss: 0.00006232
Iteration 121/1000 | Loss: 0.00006232
Iteration 122/1000 | Loss: 0.00006231
Iteration 123/1000 | Loss: 0.00006231
Iteration 124/1000 | Loss: 0.00006231
Iteration 125/1000 | Loss: 0.00006231
Iteration 126/1000 | Loss: 0.00006230
Iteration 127/1000 | Loss: 0.00006229
Iteration 128/1000 | Loss: 0.00006229
Iteration 129/1000 | Loss: 0.00006228
Iteration 130/1000 | Loss: 0.00006228
Iteration 131/1000 | Loss: 0.00006228
Iteration 132/1000 | Loss: 0.00006227
Iteration 133/1000 | Loss: 0.00006227
Iteration 134/1000 | Loss: 0.00006227
Iteration 135/1000 | Loss: 0.00006226
Iteration 136/1000 | Loss: 0.00006226
Iteration 137/1000 | Loss: 0.00006226
Iteration 138/1000 | Loss: 0.00006226
Iteration 139/1000 | Loss: 0.00006225
Iteration 140/1000 | Loss: 0.00006225
Iteration 141/1000 | Loss: 0.00006225
Iteration 142/1000 | Loss: 0.00006225
Iteration 143/1000 | Loss: 0.00006225
Iteration 144/1000 | Loss: 0.00006225
Iteration 145/1000 | Loss: 0.00006225
Iteration 146/1000 | Loss: 0.00006225
Iteration 147/1000 | Loss: 0.00006225
Iteration 148/1000 | Loss: 0.00006225
Iteration 149/1000 | Loss: 0.00006225
Iteration 150/1000 | Loss: 0.00006225
Iteration 151/1000 | Loss: 0.00006224
Iteration 152/1000 | Loss: 0.00006224
Iteration 153/1000 | Loss: 0.00006224
Iteration 154/1000 | Loss: 0.00006224
Iteration 155/1000 | Loss: 0.00006224
Iteration 156/1000 | Loss: 0.00006223
Iteration 157/1000 | Loss: 0.00006223
Iteration 158/1000 | Loss: 0.00006223
Iteration 159/1000 | Loss: 0.00006223
Iteration 160/1000 | Loss: 0.00006223
Iteration 161/1000 | Loss: 0.00006223
Iteration 162/1000 | Loss: 0.00006223
Iteration 163/1000 | Loss: 0.00006223
Iteration 164/1000 | Loss: 0.00006222
Iteration 165/1000 | Loss: 0.00006222
Iteration 166/1000 | Loss: 0.00006222
Iteration 167/1000 | Loss: 0.00006222
Iteration 168/1000 | Loss: 0.00006222
Iteration 169/1000 | Loss: 0.00006222
Iteration 170/1000 | Loss: 0.00006222
Iteration 171/1000 | Loss: 0.00006222
Iteration 172/1000 | Loss: 0.00006222
Iteration 173/1000 | Loss: 0.00006222
Iteration 174/1000 | Loss: 0.00006222
Iteration 175/1000 | Loss: 0.00006221
Iteration 176/1000 | Loss: 0.00006221
Iteration 177/1000 | Loss: 0.00006221
Iteration 178/1000 | Loss: 0.00006221
Iteration 179/1000 | Loss: 0.00006221
Iteration 180/1000 | Loss: 0.00006221
Iteration 181/1000 | Loss: 0.00006221
Iteration 182/1000 | Loss: 0.00006221
Iteration 183/1000 | Loss: 0.00006221
Iteration 184/1000 | Loss: 0.00006220
Iteration 185/1000 | Loss: 0.00006220
Iteration 186/1000 | Loss: 0.00006220
Iteration 187/1000 | Loss: 0.00006220
Iteration 188/1000 | Loss: 0.00006220
Iteration 189/1000 | Loss: 0.00006220
Iteration 190/1000 | Loss: 0.00006220
Iteration 191/1000 | Loss: 0.00006220
Iteration 192/1000 | Loss: 0.00006219
Iteration 193/1000 | Loss: 0.00006219
Iteration 194/1000 | Loss: 0.00006219
Iteration 195/1000 | Loss: 0.00006219
Iteration 196/1000 | Loss: 0.00006219
Iteration 197/1000 | Loss: 0.00006219
Iteration 198/1000 | Loss: 0.00006219
Iteration 199/1000 | Loss: 0.00006219
Iteration 200/1000 | Loss: 0.00006219
Iteration 201/1000 | Loss: 0.00006219
Iteration 202/1000 | Loss: 0.00006219
Iteration 203/1000 | Loss: 0.00006219
Iteration 204/1000 | Loss: 0.00006219
Iteration 205/1000 | Loss: 0.00006219
Iteration 206/1000 | Loss: 0.00006219
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [6.219092756509781e-05, 6.219092756509781e-05, 6.219092756509781e-05, 6.219092756509781e-05, 6.219092756509781e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.219092756509781e-05

Optimization complete. Final v2v error: 5.79996395111084 mm

Highest mean error: 16.14229965209961 mm for frame 79

Lowest mean error: 4.836257457733154 mm for frame 61

Saving results

Total time: 189.9895896911621
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_1444/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1444/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1444/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00380108
Iteration 2/25 | Loss: 0.00150345
Iteration 3/25 | Loss: 0.00137694
Iteration 4/25 | Loss: 0.00135350
Iteration 5/25 | Loss: 0.00134543
Iteration 6/25 | Loss: 0.00134217
Iteration 7/25 | Loss: 0.00134089
Iteration 8/25 | Loss: 0.00134089
Iteration 9/25 | Loss: 0.00134089
Iteration 10/25 | Loss: 0.00134089
Iteration 11/25 | Loss: 0.00134089
Iteration 12/25 | Loss: 0.00134089
Iteration 13/25 | Loss: 0.00134089
Iteration 14/25 | Loss: 0.00134089
Iteration 15/25 | Loss: 0.00134089
Iteration 16/25 | Loss: 0.00134089
Iteration 17/25 | Loss: 0.00134089
Iteration 18/25 | Loss: 0.00134089
Iteration 19/25 | Loss: 0.00134089
Iteration 20/25 | Loss: 0.00134089
Iteration 21/25 | Loss: 0.00134089
Iteration 22/25 | Loss: 0.00134089
Iteration 23/25 | Loss: 0.00134089
Iteration 24/25 | Loss: 0.00134089
Iteration 25/25 | Loss: 0.00134089

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45241165
Iteration 2/25 | Loss: 0.00106739
Iteration 3/25 | Loss: 0.00106739
Iteration 4/25 | Loss: 0.00106739
Iteration 5/25 | Loss: 0.00106739
Iteration 6/25 | Loss: 0.00106739
Iteration 7/25 | Loss: 0.00106739
Iteration 8/25 | Loss: 0.00106739
Iteration 9/25 | Loss: 0.00106739
Iteration 10/25 | Loss: 0.00106739
Iteration 11/25 | Loss: 0.00106739
Iteration 12/25 | Loss: 0.00106739
Iteration 13/25 | Loss: 0.00106739
Iteration 14/25 | Loss: 0.00106739
Iteration 15/25 | Loss: 0.00106739
Iteration 16/25 | Loss: 0.00106739
Iteration 17/25 | Loss: 0.00106739
Iteration 18/25 | Loss: 0.00106739
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010673898505046964, 0.0010673898505046964, 0.0010673898505046964, 0.0010673898505046964, 0.0010673898505046964]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010673898505046964

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106739
Iteration 2/1000 | Loss: 0.00005947
Iteration 3/1000 | Loss: 0.00003867
Iteration 4/1000 | Loss: 0.00003130
Iteration 5/1000 | Loss: 0.00002957
Iteration 6/1000 | Loss: 0.00002810
Iteration 7/1000 | Loss: 0.00002706
Iteration 8/1000 | Loss: 0.00002639
Iteration 9/1000 | Loss: 0.00002596
Iteration 10/1000 | Loss: 0.00002563
Iteration 11/1000 | Loss: 0.00002541
Iteration 12/1000 | Loss: 0.00002537
Iteration 13/1000 | Loss: 0.00002520
Iteration 14/1000 | Loss: 0.00002509
Iteration 15/1000 | Loss: 0.00002503
Iteration 16/1000 | Loss: 0.00002501
Iteration 17/1000 | Loss: 0.00002501
Iteration 18/1000 | Loss: 0.00002498
Iteration 19/1000 | Loss: 0.00002497
Iteration 20/1000 | Loss: 0.00002497
Iteration 21/1000 | Loss: 0.00002491
Iteration 22/1000 | Loss: 0.00002490
Iteration 23/1000 | Loss: 0.00002489
Iteration 24/1000 | Loss: 0.00002487
Iteration 25/1000 | Loss: 0.00002486
Iteration 26/1000 | Loss: 0.00002484
Iteration 27/1000 | Loss: 0.00002484
Iteration 28/1000 | Loss: 0.00002483
Iteration 29/1000 | Loss: 0.00002482
Iteration 30/1000 | Loss: 0.00002482
Iteration 31/1000 | Loss: 0.00002481
Iteration 32/1000 | Loss: 0.00002481
Iteration 33/1000 | Loss: 0.00002481
Iteration 34/1000 | Loss: 0.00002481
Iteration 35/1000 | Loss: 0.00002481
Iteration 36/1000 | Loss: 0.00002481
Iteration 37/1000 | Loss: 0.00002480
Iteration 38/1000 | Loss: 0.00002480
Iteration 39/1000 | Loss: 0.00002479
Iteration 40/1000 | Loss: 0.00002479
Iteration 41/1000 | Loss: 0.00002479
Iteration 42/1000 | Loss: 0.00002479
Iteration 43/1000 | Loss: 0.00002478
Iteration 44/1000 | Loss: 0.00002477
Iteration 45/1000 | Loss: 0.00002477
Iteration 46/1000 | Loss: 0.00002477
Iteration 47/1000 | Loss: 0.00002476
Iteration 48/1000 | Loss: 0.00002475
Iteration 49/1000 | Loss: 0.00002474
Iteration 50/1000 | Loss: 0.00002472
Iteration 51/1000 | Loss: 0.00002471
Iteration 52/1000 | Loss: 0.00002470
Iteration 53/1000 | Loss: 0.00002470
Iteration 54/1000 | Loss: 0.00002469
Iteration 55/1000 | Loss: 0.00002469
Iteration 56/1000 | Loss: 0.00002468
Iteration 57/1000 | Loss: 0.00002468
Iteration 58/1000 | Loss: 0.00002466
Iteration 59/1000 | Loss: 0.00002466
Iteration 60/1000 | Loss: 0.00002466
Iteration 61/1000 | Loss: 0.00002466
Iteration 62/1000 | Loss: 0.00002466
Iteration 63/1000 | Loss: 0.00002466
Iteration 64/1000 | Loss: 0.00002466
Iteration 65/1000 | Loss: 0.00002466
Iteration 66/1000 | Loss: 0.00002466
Iteration 67/1000 | Loss: 0.00002466
Iteration 68/1000 | Loss: 0.00002465
Iteration 69/1000 | Loss: 0.00002465
Iteration 70/1000 | Loss: 0.00002465
Iteration 71/1000 | Loss: 0.00002465
Iteration 72/1000 | Loss: 0.00002465
Iteration 73/1000 | Loss: 0.00002464
Iteration 74/1000 | Loss: 0.00002464
Iteration 75/1000 | Loss: 0.00002464
Iteration 76/1000 | Loss: 0.00002464
Iteration 77/1000 | Loss: 0.00002464
Iteration 78/1000 | Loss: 0.00002464
Iteration 79/1000 | Loss: 0.00002464
Iteration 80/1000 | Loss: 0.00002464
Iteration 81/1000 | Loss: 0.00002464
Iteration 82/1000 | Loss: 0.00002464
Iteration 83/1000 | Loss: 0.00002463
Iteration 84/1000 | Loss: 0.00002463
Iteration 85/1000 | Loss: 0.00002463
Iteration 86/1000 | Loss: 0.00002463
Iteration 87/1000 | Loss: 0.00002463
Iteration 88/1000 | Loss: 0.00002462
Iteration 89/1000 | Loss: 0.00002462
Iteration 90/1000 | Loss: 0.00002462
Iteration 91/1000 | Loss: 0.00002462
Iteration 92/1000 | Loss: 0.00002462
Iteration 93/1000 | Loss: 0.00002462
Iteration 94/1000 | Loss: 0.00002462
Iteration 95/1000 | Loss: 0.00002462
Iteration 96/1000 | Loss: 0.00002462
Iteration 97/1000 | Loss: 0.00002461
Iteration 98/1000 | Loss: 0.00002461
Iteration 99/1000 | Loss: 0.00002461
Iteration 100/1000 | Loss: 0.00002461
Iteration 101/1000 | Loss: 0.00002461
Iteration 102/1000 | Loss: 0.00002461
Iteration 103/1000 | Loss: 0.00002461
Iteration 104/1000 | Loss: 0.00002461
Iteration 105/1000 | Loss: 0.00002461
Iteration 106/1000 | Loss: 0.00002461
Iteration 107/1000 | Loss: 0.00002461
Iteration 108/1000 | Loss: 0.00002461
Iteration 109/1000 | Loss: 0.00002461
Iteration 110/1000 | Loss: 0.00002460
Iteration 111/1000 | Loss: 0.00002460
Iteration 112/1000 | Loss: 0.00002460
Iteration 113/1000 | Loss: 0.00002460
Iteration 114/1000 | Loss: 0.00002460
Iteration 115/1000 | Loss: 0.00002460
Iteration 116/1000 | Loss: 0.00002459
Iteration 117/1000 | Loss: 0.00002459
Iteration 118/1000 | Loss: 0.00002459
Iteration 119/1000 | Loss: 0.00002459
Iteration 120/1000 | Loss: 0.00002459
Iteration 121/1000 | Loss: 0.00002458
Iteration 122/1000 | Loss: 0.00002458
Iteration 123/1000 | Loss: 0.00002458
Iteration 124/1000 | Loss: 0.00002458
Iteration 125/1000 | Loss: 0.00002458
Iteration 126/1000 | Loss: 0.00002458
Iteration 127/1000 | Loss: 0.00002458
Iteration 128/1000 | Loss: 0.00002458
Iteration 129/1000 | Loss: 0.00002458
Iteration 130/1000 | Loss: 0.00002458
Iteration 131/1000 | Loss: 0.00002457
Iteration 132/1000 | Loss: 0.00002457
Iteration 133/1000 | Loss: 0.00002457
Iteration 134/1000 | Loss: 0.00002457
Iteration 135/1000 | Loss: 0.00002457
Iteration 136/1000 | Loss: 0.00002457
Iteration 137/1000 | Loss: 0.00002457
Iteration 138/1000 | Loss: 0.00002457
Iteration 139/1000 | Loss: 0.00002457
Iteration 140/1000 | Loss: 0.00002456
Iteration 141/1000 | Loss: 0.00002456
Iteration 142/1000 | Loss: 0.00002456
Iteration 143/1000 | Loss: 0.00002456
Iteration 144/1000 | Loss: 0.00002456
Iteration 145/1000 | Loss: 0.00002456
Iteration 146/1000 | Loss: 0.00002456
Iteration 147/1000 | Loss: 0.00002456
Iteration 148/1000 | Loss: 0.00002456
Iteration 149/1000 | Loss: 0.00002456
Iteration 150/1000 | Loss: 0.00002456
Iteration 151/1000 | Loss: 0.00002456
Iteration 152/1000 | Loss: 0.00002456
Iteration 153/1000 | Loss: 0.00002456
Iteration 154/1000 | Loss: 0.00002456
Iteration 155/1000 | Loss: 0.00002456
Iteration 156/1000 | Loss: 0.00002456
Iteration 157/1000 | Loss: 0.00002456
Iteration 158/1000 | Loss: 0.00002456
Iteration 159/1000 | Loss: 0.00002456
Iteration 160/1000 | Loss: 0.00002456
Iteration 161/1000 | Loss: 0.00002456
Iteration 162/1000 | Loss: 0.00002456
Iteration 163/1000 | Loss: 0.00002456
Iteration 164/1000 | Loss: 0.00002456
Iteration 165/1000 | Loss: 0.00002456
Iteration 166/1000 | Loss: 0.00002456
Iteration 167/1000 | Loss: 0.00002456
Iteration 168/1000 | Loss: 0.00002456
Iteration 169/1000 | Loss: 0.00002456
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [2.4559834855608642e-05, 2.4559834855608642e-05, 2.4559834855608642e-05, 2.4559834855608642e-05, 2.4559834855608642e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4559834855608642e-05

Optimization complete. Final v2v error: 4.355893135070801 mm

Highest mean error: 4.732062339782715 mm for frame 55

Lowest mean error: 4.126140594482422 mm for frame 247

Saving results

Total time: 49.58114242553711
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_1444/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1444/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1444/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00502259
Iteration 2/25 | Loss: 0.00140505
Iteration 3/25 | Loss: 0.00132884
Iteration 4/25 | Loss: 0.00131650
Iteration 5/25 | Loss: 0.00131027
Iteration 6/25 | Loss: 0.00130865
Iteration 7/25 | Loss: 0.00130838
Iteration 8/25 | Loss: 0.00130838
Iteration 9/25 | Loss: 0.00130838
Iteration 10/25 | Loss: 0.00130838
Iteration 11/25 | Loss: 0.00130838
Iteration 12/25 | Loss: 0.00130838
Iteration 13/25 | Loss: 0.00130838
Iteration 14/25 | Loss: 0.00130838
Iteration 15/25 | Loss: 0.00130838
Iteration 16/25 | Loss: 0.00130838
Iteration 17/25 | Loss: 0.00130838
Iteration 18/25 | Loss: 0.00130838
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001308381324633956, 0.001308381324633956, 0.001308381324633956, 0.001308381324633956, 0.001308381324633956]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001308381324633956

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38652337
Iteration 2/25 | Loss: 0.00088879
Iteration 3/25 | Loss: 0.00088879
Iteration 4/25 | Loss: 0.00088879
Iteration 5/25 | Loss: 0.00088879
Iteration 6/25 | Loss: 0.00088879
Iteration 7/25 | Loss: 0.00088879
Iteration 8/25 | Loss: 0.00088879
Iteration 9/25 | Loss: 0.00088879
Iteration 10/25 | Loss: 0.00088879
Iteration 11/25 | Loss: 0.00088879
Iteration 12/25 | Loss: 0.00088879
Iteration 13/25 | Loss: 0.00088879
Iteration 14/25 | Loss: 0.00088879
Iteration 15/25 | Loss: 0.00088879
Iteration 16/25 | Loss: 0.00088879
Iteration 17/25 | Loss: 0.00088879
Iteration 18/25 | Loss: 0.00088879
Iteration 19/25 | Loss: 0.00088879
Iteration 20/25 | Loss: 0.00088879
Iteration 21/25 | Loss: 0.00088879
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008887857547961175, 0.0008887857547961175, 0.0008887857547961175, 0.0008887857547961175, 0.0008887857547961175]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008887857547961175

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088879
Iteration 2/1000 | Loss: 0.00004503
Iteration 3/1000 | Loss: 0.00003071
Iteration 4/1000 | Loss: 0.00002772
Iteration 5/1000 | Loss: 0.00002669
Iteration 6/1000 | Loss: 0.00002616
Iteration 7/1000 | Loss: 0.00002584
Iteration 8/1000 | Loss: 0.00002560
Iteration 9/1000 | Loss: 0.00002547
Iteration 10/1000 | Loss: 0.00002547
Iteration 11/1000 | Loss: 0.00002531
Iteration 12/1000 | Loss: 0.00002531
Iteration 13/1000 | Loss: 0.00002531
Iteration 14/1000 | Loss: 0.00002531
Iteration 15/1000 | Loss: 0.00002530
Iteration 16/1000 | Loss: 0.00002530
Iteration 17/1000 | Loss: 0.00002523
Iteration 18/1000 | Loss: 0.00002519
Iteration 19/1000 | Loss: 0.00002518
Iteration 20/1000 | Loss: 0.00002515
Iteration 21/1000 | Loss: 0.00002515
Iteration 22/1000 | Loss: 0.00002514
Iteration 23/1000 | Loss: 0.00002514
Iteration 24/1000 | Loss: 0.00002513
Iteration 25/1000 | Loss: 0.00002512
Iteration 26/1000 | Loss: 0.00002512
Iteration 27/1000 | Loss: 0.00002512
Iteration 28/1000 | Loss: 0.00002512
Iteration 29/1000 | Loss: 0.00002512
Iteration 30/1000 | Loss: 0.00002511
Iteration 31/1000 | Loss: 0.00002508
Iteration 32/1000 | Loss: 0.00002508
Iteration 33/1000 | Loss: 0.00002507
Iteration 34/1000 | Loss: 0.00002507
Iteration 35/1000 | Loss: 0.00002507
Iteration 36/1000 | Loss: 0.00002507
Iteration 37/1000 | Loss: 0.00002506
Iteration 38/1000 | Loss: 0.00002505
Iteration 39/1000 | Loss: 0.00002504
Iteration 40/1000 | Loss: 0.00002504
Iteration 41/1000 | Loss: 0.00002503
Iteration 42/1000 | Loss: 0.00002503
Iteration 43/1000 | Loss: 0.00002502
Iteration 44/1000 | Loss: 0.00002502
Iteration 45/1000 | Loss: 0.00002502
Iteration 46/1000 | Loss: 0.00002502
Iteration 47/1000 | Loss: 0.00002502
Iteration 48/1000 | Loss: 0.00002501
Iteration 49/1000 | Loss: 0.00002501
Iteration 50/1000 | Loss: 0.00002501
Iteration 51/1000 | Loss: 0.00002501
Iteration 52/1000 | Loss: 0.00002501
Iteration 53/1000 | Loss: 0.00002501
Iteration 54/1000 | Loss: 0.00002501
Iteration 55/1000 | Loss: 0.00002501
Iteration 56/1000 | Loss: 0.00002501
Iteration 57/1000 | Loss: 0.00002501
Iteration 58/1000 | Loss: 0.00002501
Iteration 59/1000 | Loss: 0.00002501
Iteration 60/1000 | Loss: 0.00002500
Iteration 61/1000 | Loss: 0.00002500
Iteration 62/1000 | Loss: 0.00002500
Iteration 63/1000 | Loss: 0.00002500
Iteration 64/1000 | Loss: 0.00002499
Iteration 65/1000 | Loss: 0.00002499
Iteration 66/1000 | Loss: 0.00002499
Iteration 67/1000 | Loss: 0.00002499
Iteration 68/1000 | Loss: 0.00002499
Iteration 69/1000 | Loss: 0.00002499
Iteration 70/1000 | Loss: 0.00002499
Iteration 71/1000 | Loss: 0.00002499
Iteration 72/1000 | Loss: 0.00002499
Iteration 73/1000 | Loss: 0.00002499
Iteration 74/1000 | Loss: 0.00002499
Iteration 75/1000 | Loss: 0.00002499
Iteration 76/1000 | Loss: 0.00002499
Iteration 77/1000 | Loss: 0.00002499
Iteration 78/1000 | Loss: 0.00002498
Iteration 79/1000 | Loss: 0.00002498
Iteration 80/1000 | Loss: 0.00002498
Iteration 81/1000 | Loss: 0.00002498
Iteration 82/1000 | Loss: 0.00002498
Iteration 83/1000 | Loss: 0.00002498
Iteration 84/1000 | Loss: 0.00002498
Iteration 85/1000 | Loss: 0.00002498
Iteration 86/1000 | Loss: 0.00002497
Iteration 87/1000 | Loss: 0.00002497
Iteration 88/1000 | Loss: 0.00002497
Iteration 89/1000 | Loss: 0.00002497
Iteration 90/1000 | Loss: 0.00002497
Iteration 91/1000 | Loss: 0.00002497
Iteration 92/1000 | Loss: 0.00002497
Iteration 93/1000 | Loss: 0.00002497
Iteration 94/1000 | Loss: 0.00002496
Iteration 95/1000 | Loss: 0.00002496
Iteration 96/1000 | Loss: 0.00002496
Iteration 97/1000 | Loss: 0.00002496
Iteration 98/1000 | Loss: 0.00002496
Iteration 99/1000 | Loss: 0.00002496
Iteration 100/1000 | Loss: 0.00002496
Iteration 101/1000 | Loss: 0.00002496
Iteration 102/1000 | Loss: 0.00002496
Iteration 103/1000 | Loss: 0.00002495
Iteration 104/1000 | Loss: 0.00002495
Iteration 105/1000 | Loss: 0.00002495
Iteration 106/1000 | Loss: 0.00002495
Iteration 107/1000 | Loss: 0.00002495
Iteration 108/1000 | Loss: 0.00002495
Iteration 109/1000 | Loss: 0.00002495
Iteration 110/1000 | Loss: 0.00002495
Iteration 111/1000 | Loss: 0.00002495
Iteration 112/1000 | Loss: 0.00002494
Iteration 113/1000 | Loss: 0.00002494
Iteration 114/1000 | Loss: 0.00002494
Iteration 115/1000 | Loss: 0.00002494
Iteration 116/1000 | Loss: 0.00002494
Iteration 117/1000 | Loss: 0.00002494
Iteration 118/1000 | Loss: 0.00002494
Iteration 119/1000 | Loss: 0.00002494
Iteration 120/1000 | Loss: 0.00002494
Iteration 121/1000 | Loss: 0.00002493
Iteration 122/1000 | Loss: 0.00002493
Iteration 123/1000 | Loss: 0.00002493
Iteration 124/1000 | Loss: 0.00002493
Iteration 125/1000 | Loss: 0.00002493
Iteration 126/1000 | Loss: 0.00002493
Iteration 127/1000 | Loss: 0.00002493
Iteration 128/1000 | Loss: 0.00002493
Iteration 129/1000 | Loss: 0.00002493
Iteration 130/1000 | Loss: 0.00002493
Iteration 131/1000 | Loss: 0.00002493
Iteration 132/1000 | Loss: 0.00002493
Iteration 133/1000 | Loss: 0.00002493
Iteration 134/1000 | Loss: 0.00002493
Iteration 135/1000 | Loss: 0.00002493
Iteration 136/1000 | Loss: 0.00002493
Iteration 137/1000 | Loss: 0.00002492
Iteration 138/1000 | Loss: 0.00002492
Iteration 139/1000 | Loss: 0.00002492
Iteration 140/1000 | Loss: 0.00002492
Iteration 141/1000 | Loss: 0.00002492
Iteration 142/1000 | Loss: 0.00002492
Iteration 143/1000 | Loss: 0.00002492
Iteration 144/1000 | Loss: 0.00002492
Iteration 145/1000 | Loss: 0.00002492
Iteration 146/1000 | Loss: 0.00002492
Iteration 147/1000 | Loss: 0.00002492
Iteration 148/1000 | Loss: 0.00002492
Iteration 149/1000 | Loss: 0.00002492
Iteration 150/1000 | Loss: 0.00002492
Iteration 151/1000 | Loss: 0.00002492
Iteration 152/1000 | Loss: 0.00002491
Iteration 153/1000 | Loss: 0.00002491
Iteration 154/1000 | Loss: 0.00002491
Iteration 155/1000 | Loss: 0.00002491
Iteration 156/1000 | Loss: 0.00002491
Iteration 157/1000 | Loss: 0.00002491
Iteration 158/1000 | Loss: 0.00002491
Iteration 159/1000 | Loss: 0.00002491
Iteration 160/1000 | Loss: 0.00002491
Iteration 161/1000 | Loss: 0.00002491
Iteration 162/1000 | Loss: 0.00002491
Iteration 163/1000 | Loss: 0.00002491
Iteration 164/1000 | Loss: 0.00002491
Iteration 165/1000 | Loss: 0.00002491
Iteration 166/1000 | Loss: 0.00002491
Iteration 167/1000 | Loss: 0.00002491
Iteration 168/1000 | Loss: 0.00002491
Iteration 169/1000 | Loss: 0.00002491
Iteration 170/1000 | Loss: 0.00002491
Iteration 171/1000 | Loss: 0.00002491
Iteration 172/1000 | Loss: 0.00002491
Iteration 173/1000 | Loss: 0.00002491
Iteration 174/1000 | Loss: 0.00002491
Iteration 175/1000 | Loss: 0.00002491
Iteration 176/1000 | Loss: 0.00002491
Iteration 177/1000 | Loss: 0.00002491
Iteration 178/1000 | Loss: 0.00002491
Iteration 179/1000 | Loss: 0.00002491
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [2.4907534680096433e-05, 2.4907534680096433e-05, 2.4907534680096433e-05, 2.4907534680096433e-05, 2.4907534680096433e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4907534680096433e-05

Optimization complete. Final v2v error: 4.364687442779541 mm

Highest mean error: 4.658825397491455 mm for frame 11

Lowest mean error: 4.086500644683838 mm for frame 82

Saving results

Total time: 36.727936029434204
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_1444/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1444/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1444/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00843655
Iteration 2/25 | Loss: 0.00158334
Iteration 3/25 | Loss: 0.00149888
Iteration 4/25 | Loss: 0.00147682
Iteration 5/25 | Loss: 0.00146785
Iteration 6/25 | Loss: 0.00146519
Iteration 7/25 | Loss: 0.00146437
Iteration 8/25 | Loss: 0.00146437
Iteration 9/25 | Loss: 0.00146437
Iteration 10/25 | Loss: 0.00146437
Iteration 11/25 | Loss: 0.00146437
Iteration 12/25 | Loss: 0.00146437
Iteration 13/25 | Loss: 0.00146437
Iteration 14/25 | Loss: 0.00146437
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0014643741305917501, 0.0014643741305917501, 0.0014643741305917501, 0.0014643741305917501, 0.0014643741305917501]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014643741305917501

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48193753
Iteration 2/25 | Loss: 0.00122995
Iteration 3/25 | Loss: 0.00122986
Iteration 4/25 | Loss: 0.00122986
Iteration 5/25 | Loss: 0.00122986
Iteration 6/25 | Loss: 0.00122986
Iteration 7/25 | Loss: 0.00122986
Iteration 8/25 | Loss: 0.00122986
Iteration 9/25 | Loss: 0.00122986
Iteration 10/25 | Loss: 0.00122986
Iteration 11/25 | Loss: 0.00122986
Iteration 12/25 | Loss: 0.00122986
Iteration 13/25 | Loss: 0.00122986
Iteration 14/25 | Loss: 0.00122986
Iteration 15/25 | Loss: 0.00122986
Iteration 16/25 | Loss: 0.00122986
Iteration 17/25 | Loss: 0.00122986
Iteration 18/25 | Loss: 0.00122986
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001229860819876194, 0.001229860819876194, 0.001229860819876194, 0.001229860819876194, 0.001229860819876194]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001229860819876194

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122986
Iteration 2/1000 | Loss: 0.00007068
Iteration 3/1000 | Loss: 0.00005250
Iteration 4/1000 | Loss: 0.00004086
Iteration 5/1000 | Loss: 0.00003861
Iteration 6/1000 | Loss: 0.00003673
Iteration 7/1000 | Loss: 0.00003586
Iteration 8/1000 | Loss: 0.00003515
Iteration 9/1000 | Loss: 0.00003474
Iteration 10/1000 | Loss: 0.00003446
Iteration 11/1000 | Loss: 0.00003423
Iteration 12/1000 | Loss: 0.00003412
Iteration 13/1000 | Loss: 0.00003412
Iteration 14/1000 | Loss: 0.00003411
Iteration 15/1000 | Loss: 0.00003411
Iteration 16/1000 | Loss: 0.00003410
Iteration 17/1000 | Loss: 0.00003410
Iteration 18/1000 | Loss: 0.00003410
Iteration 19/1000 | Loss: 0.00003410
Iteration 20/1000 | Loss: 0.00003410
Iteration 21/1000 | Loss: 0.00003409
Iteration 22/1000 | Loss: 0.00003409
Iteration 23/1000 | Loss: 0.00003409
Iteration 24/1000 | Loss: 0.00003409
Iteration 25/1000 | Loss: 0.00003409
Iteration 26/1000 | Loss: 0.00003409
Iteration 27/1000 | Loss: 0.00003409
Iteration 28/1000 | Loss: 0.00003409
Iteration 29/1000 | Loss: 0.00003408
Iteration 30/1000 | Loss: 0.00003408
Iteration 31/1000 | Loss: 0.00003408
Iteration 32/1000 | Loss: 0.00003408
Iteration 33/1000 | Loss: 0.00003408
Iteration 34/1000 | Loss: 0.00003407
Iteration 35/1000 | Loss: 0.00003407
Iteration 36/1000 | Loss: 0.00003407
Iteration 37/1000 | Loss: 0.00003407
Iteration 38/1000 | Loss: 0.00003406
Iteration 39/1000 | Loss: 0.00003406
Iteration 40/1000 | Loss: 0.00003405
Iteration 41/1000 | Loss: 0.00003405
Iteration 42/1000 | Loss: 0.00003405
Iteration 43/1000 | Loss: 0.00003404
Iteration 44/1000 | Loss: 0.00003404
Iteration 45/1000 | Loss: 0.00003404
Iteration 46/1000 | Loss: 0.00003404
Iteration 47/1000 | Loss: 0.00003404
Iteration 48/1000 | Loss: 0.00003404
Iteration 49/1000 | Loss: 0.00003403
Iteration 50/1000 | Loss: 0.00003403
Iteration 51/1000 | Loss: 0.00003403
Iteration 52/1000 | Loss: 0.00003402
Iteration 53/1000 | Loss: 0.00003401
Iteration 54/1000 | Loss: 0.00003401
Iteration 55/1000 | Loss: 0.00003401
Iteration 56/1000 | Loss: 0.00003401
Iteration 57/1000 | Loss: 0.00003400
Iteration 58/1000 | Loss: 0.00003400
Iteration 59/1000 | Loss: 0.00003400
Iteration 60/1000 | Loss: 0.00003400
Iteration 61/1000 | Loss: 0.00003400
Iteration 62/1000 | Loss: 0.00003400
Iteration 63/1000 | Loss: 0.00003399
Iteration 64/1000 | Loss: 0.00003399
Iteration 65/1000 | Loss: 0.00003399
Iteration 66/1000 | Loss: 0.00003399
Iteration 67/1000 | Loss: 0.00003399
Iteration 68/1000 | Loss: 0.00003399
Iteration 69/1000 | Loss: 0.00003399
Iteration 70/1000 | Loss: 0.00003399
Iteration 71/1000 | Loss: 0.00003399
Iteration 72/1000 | Loss: 0.00003398
Iteration 73/1000 | Loss: 0.00003398
Iteration 74/1000 | Loss: 0.00003398
Iteration 75/1000 | Loss: 0.00003398
Iteration 76/1000 | Loss: 0.00003398
Iteration 77/1000 | Loss: 0.00003398
Iteration 78/1000 | Loss: 0.00003397
Iteration 79/1000 | Loss: 0.00003397
Iteration 80/1000 | Loss: 0.00003397
Iteration 81/1000 | Loss: 0.00003397
Iteration 82/1000 | Loss: 0.00003397
Iteration 83/1000 | Loss: 0.00003397
Iteration 84/1000 | Loss: 0.00003397
Iteration 85/1000 | Loss: 0.00003397
Iteration 86/1000 | Loss: 0.00003397
Iteration 87/1000 | Loss: 0.00003397
Iteration 88/1000 | Loss: 0.00003396
Iteration 89/1000 | Loss: 0.00003396
Iteration 90/1000 | Loss: 0.00003396
Iteration 91/1000 | Loss: 0.00003396
Iteration 92/1000 | Loss: 0.00003396
Iteration 93/1000 | Loss: 0.00003396
Iteration 94/1000 | Loss: 0.00003396
Iteration 95/1000 | Loss: 0.00003396
Iteration 96/1000 | Loss: 0.00003396
Iteration 97/1000 | Loss: 0.00003396
Iteration 98/1000 | Loss: 0.00003396
Iteration 99/1000 | Loss: 0.00003395
Iteration 100/1000 | Loss: 0.00003395
Iteration 101/1000 | Loss: 0.00003395
Iteration 102/1000 | Loss: 0.00003395
Iteration 103/1000 | Loss: 0.00003395
Iteration 104/1000 | Loss: 0.00003395
Iteration 105/1000 | Loss: 0.00003395
Iteration 106/1000 | Loss: 0.00003395
Iteration 107/1000 | Loss: 0.00003395
Iteration 108/1000 | Loss: 0.00003395
Iteration 109/1000 | Loss: 0.00003395
Iteration 110/1000 | Loss: 0.00003395
Iteration 111/1000 | Loss: 0.00003395
Iteration 112/1000 | Loss: 0.00003395
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [3.39506623276975e-05, 3.39506623276975e-05, 3.39506623276975e-05, 3.39506623276975e-05, 3.39506623276975e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.39506623276975e-05

Optimization complete. Final v2v error: 4.993561267852783 mm

Highest mean error: 5.419300556182861 mm for frame 45

Lowest mean error: 4.7795729637146 mm for frame 97

Saving results

Total time: 34.25145983695984
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_1444/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1444/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1444/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01173764
Iteration 2/25 | Loss: 0.00283235
Iteration 3/25 | Loss: 0.00175966
Iteration 4/25 | Loss: 0.00161130
Iteration 5/25 | Loss: 0.00159518
Iteration 6/25 | Loss: 0.00158390
Iteration 7/25 | Loss: 0.00152139
Iteration 8/25 | Loss: 0.00147370
Iteration 9/25 | Loss: 0.00146395
Iteration 10/25 | Loss: 0.00144989
Iteration 11/25 | Loss: 0.00143756
Iteration 12/25 | Loss: 0.00143994
Iteration 13/25 | Loss: 0.00143033
Iteration 14/25 | Loss: 0.00143677
Iteration 15/25 | Loss: 0.00143272
Iteration 16/25 | Loss: 0.00142526
Iteration 17/25 | Loss: 0.00142622
Iteration 18/25 | Loss: 0.00142707
Iteration 19/25 | Loss: 0.00142415
Iteration 20/25 | Loss: 0.00142307
Iteration 21/25 | Loss: 0.00142652
Iteration 22/25 | Loss: 0.00142651
Iteration 23/25 | Loss: 0.00142521
Iteration 24/25 | Loss: 0.00142742
Iteration 25/25 | Loss: 0.00142700

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39414775
Iteration 2/25 | Loss: 0.00142466
Iteration 3/25 | Loss: 0.00133784
Iteration 4/25 | Loss: 0.00133784
Iteration 5/25 | Loss: 0.00133784
Iteration 6/25 | Loss: 0.00133784
Iteration 7/25 | Loss: 0.00133784
Iteration 8/25 | Loss: 0.00133784
Iteration 9/25 | Loss: 0.00133784
Iteration 10/25 | Loss: 0.00133784
Iteration 11/25 | Loss: 0.00133784
Iteration 12/25 | Loss: 0.00133784
Iteration 13/25 | Loss: 0.00133784
Iteration 14/25 | Loss: 0.00133784
Iteration 15/25 | Loss: 0.00133784
Iteration 16/25 | Loss: 0.00133784
Iteration 17/25 | Loss: 0.00133784
Iteration 18/25 | Loss: 0.00133784
Iteration 19/25 | Loss: 0.00133784
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0013378406874835491, 0.0013378406874835491, 0.0013378406874835491, 0.0013378406874835491, 0.0013378406874835491]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013378406874835491

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133784
Iteration 2/1000 | Loss: 0.00032367
Iteration 3/1000 | Loss: 0.00028506
Iteration 4/1000 | Loss: 0.00048908
Iteration 5/1000 | Loss: 0.00027527
Iteration 6/1000 | Loss: 0.00043598
Iteration 7/1000 | Loss: 0.00042236
Iteration 8/1000 | Loss: 0.00043668
Iteration 9/1000 | Loss: 0.00037543
Iteration 10/1000 | Loss: 0.00029266
Iteration 11/1000 | Loss: 0.00024689
Iteration 12/1000 | Loss: 0.00024853
Iteration 13/1000 | Loss: 0.00013419
Iteration 14/1000 | Loss: 0.00008090
Iteration 15/1000 | Loss: 0.00027822
Iteration 16/1000 | Loss: 0.00008055
Iteration 17/1000 | Loss: 0.00007983
Iteration 18/1000 | Loss: 0.00007345
Iteration 19/1000 | Loss: 0.00007851
Iteration 20/1000 | Loss: 0.00007145
Iteration 21/1000 | Loss: 0.00007909
Iteration 22/1000 | Loss: 0.00006853
Iteration 23/1000 | Loss: 0.00007831
Iteration 24/1000 | Loss: 0.00007347
Iteration 25/1000 | Loss: 0.00007953
Iteration 26/1000 | Loss: 0.00007347
Iteration 27/1000 | Loss: 0.00007736
Iteration 28/1000 | Loss: 0.00007120
Iteration 29/1000 | Loss: 0.00006888
Iteration 30/1000 | Loss: 0.00007197
Iteration 31/1000 | Loss: 0.00007647
Iteration 32/1000 | Loss: 0.00007436
Iteration 33/1000 | Loss: 0.00007455
Iteration 34/1000 | Loss: 0.00007323
Iteration 35/1000 | Loss: 0.00006484
Iteration 36/1000 | Loss: 0.00007048
Iteration 37/1000 | Loss: 0.00006510
Iteration 38/1000 | Loss: 0.00006080
Iteration 39/1000 | Loss: 0.00006449
Iteration 40/1000 | Loss: 0.00005940
Iteration 41/1000 | Loss: 0.00006607
Iteration 42/1000 | Loss: 0.00006506
Iteration 43/1000 | Loss: 0.00009530
Iteration 44/1000 | Loss: 0.00006278
Iteration 45/1000 | Loss: 0.00009304
Iteration 46/1000 | Loss: 0.00007940
Iteration 47/1000 | Loss: 0.00008448
Iteration 48/1000 | Loss: 0.00006919
Iteration 49/1000 | Loss: 0.00007514
Iteration 50/1000 | Loss: 0.00006752
Iteration 51/1000 | Loss: 0.00007748
Iteration 52/1000 | Loss: 0.00007529
Iteration 53/1000 | Loss: 0.00007898
Iteration 54/1000 | Loss: 0.00008082
Iteration 55/1000 | Loss: 0.00007521
Iteration 56/1000 | Loss: 0.00008497
Iteration 57/1000 | Loss: 0.00007255
Iteration 58/1000 | Loss: 0.00008363
Iteration 59/1000 | Loss: 0.00007254
Iteration 60/1000 | Loss: 0.00008180
Iteration 61/1000 | Loss: 0.00007037
Iteration 62/1000 | Loss: 0.00008175
Iteration 63/1000 | Loss: 0.00006099
Iteration 64/1000 | Loss: 0.00040527
Iteration 65/1000 | Loss: 0.00004302
Iteration 66/1000 | Loss: 0.00005932
Iteration 67/1000 | Loss: 0.00004017
Iteration 68/1000 | Loss: 0.00003991
Iteration 69/1000 | Loss: 0.00003971
Iteration 70/1000 | Loss: 0.00003961
Iteration 71/1000 | Loss: 0.00003952
Iteration 72/1000 | Loss: 0.00003947
Iteration 73/1000 | Loss: 0.00003947
Iteration 74/1000 | Loss: 0.00003945
Iteration 75/1000 | Loss: 0.00003944
Iteration 76/1000 | Loss: 0.00003933
Iteration 77/1000 | Loss: 0.00003931
Iteration 78/1000 | Loss: 0.00003930
Iteration 79/1000 | Loss: 0.00003930
Iteration 80/1000 | Loss: 0.00003925
Iteration 81/1000 | Loss: 0.00003925
Iteration 82/1000 | Loss: 0.00003924
Iteration 83/1000 | Loss: 0.00003923
Iteration 84/1000 | Loss: 0.00003923
Iteration 85/1000 | Loss: 0.00003921
Iteration 86/1000 | Loss: 0.00003919
Iteration 87/1000 | Loss: 0.00003915
Iteration 88/1000 | Loss: 0.00003915
Iteration 89/1000 | Loss: 0.00003914
Iteration 90/1000 | Loss: 0.00003914
Iteration 91/1000 | Loss: 0.00003914
Iteration 92/1000 | Loss: 0.00003913
Iteration 93/1000 | Loss: 0.00003913
Iteration 94/1000 | Loss: 0.00003913
Iteration 95/1000 | Loss: 0.00003913
Iteration 96/1000 | Loss: 0.00003913
Iteration 97/1000 | Loss: 0.00003913
Iteration 98/1000 | Loss: 0.00003913
Iteration 99/1000 | Loss: 0.00003913
Iteration 100/1000 | Loss: 0.00003913
Iteration 101/1000 | Loss: 0.00003913
Iteration 102/1000 | Loss: 0.00003913
Iteration 103/1000 | Loss: 0.00003912
Iteration 104/1000 | Loss: 0.00003912
Iteration 105/1000 | Loss: 0.00003912
Iteration 106/1000 | Loss: 0.00003912
Iteration 107/1000 | Loss: 0.00003912
Iteration 108/1000 | Loss: 0.00003912
Iteration 109/1000 | Loss: 0.00003912
Iteration 110/1000 | Loss: 0.00003912
Iteration 111/1000 | Loss: 0.00003912
Iteration 112/1000 | Loss: 0.00003912
Iteration 113/1000 | Loss: 0.00003912
Iteration 114/1000 | Loss: 0.00003912
Iteration 115/1000 | Loss: 0.00003912
Iteration 116/1000 | Loss: 0.00003912
Iteration 117/1000 | Loss: 0.00003912
Iteration 118/1000 | Loss: 0.00003912
Iteration 119/1000 | Loss: 0.00003912
Iteration 120/1000 | Loss: 0.00003912
Iteration 121/1000 | Loss: 0.00003912
Iteration 122/1000 | Loss: 0.00003912
Iteration 123/1000 | Loss: 0.00003912
Iteration 124/1000 | Loss: 0.00003912
Iteration 125/1000 | Loss: 0.00003912
Iteration 126/1000 | Loss: 0.00003912
Iteration 127/1000 | Loss: 0.00003912
Iteration 128/1000 | Loss: 0.00003912
Iteration 129/1000 | Loss: 0.00003912
Iteration 130/1000 | Loss: 0.00003912
Iteration 131/1000 | Loss: 0.00003912
Iteration 132/1000 | Loss: 0.00003912
Iteration 133/1000 | Loss: 0.00003912
Iteration 134/1000 | Loss: 0.00003912
Iteration 135/1000 | Loss: 0.00003912
Iteration 136/1000 | Loss: 0.00003912
Iteration 137/1000 | Loss: 0.00003912
Iteration 138/1000 | Loss: 0.00003912
Iteration 139/1000 | Loss: 0.00003912
Iteration 140/1000 | Loss: 0.00003912
Iteration 141/1000 | Loss: 0.00003912
Iteration 142/1000 | Loss: 0.00003912
Iteration 143/1000 | Loss: 0.00003912
Iteration 144/1000 | Loss: 0.00003912
Iteration 145/1000 | Loss: 0.00003912
Iteration 146/1000 | Loss: 0.00003912
Iteration 147/1000 | Loss: 0.00003912
Iteration 148/1000 | Loss: 0.00003912
Iteration 149/1000 | Loss: 0.00003912
Iteration 150/1000 | Loss: 0.00003912
Iteration 151/1000 | Loss: 0.00003912
Iteration 152/1000 | Loss: 0.00003912
Iteration 153/1000 | Loss: 0.00003912
Iteration 154/1000 | Loss: 0.00003912
Iteration 155/1000 | Loss: 0.00003912
Iteration 156/1000 | Loss: 0.00003912
Iteration 157/1000 | Loss: 0.00003912
Iteration 158/1000 | Loss: 0.00003912
Iteration 159/1000 | Loss: 0.00003912
Iteration 160/1000 | Loss: 0.00003912
Iteration 161/1000 | Loss: 0.00003912
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [3.912060128641315e-05, 3.912060128641315e-05, 3.912060128641315e-05, 3.912060128641315e-05, 3.912060128641315e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.912060128641315e-05

Optimization complete. Final v2v error: 5.374354362487793 mm

Highest mean error: 11.079986572265625 mm for frame 219

Lowest mean error: 4.767017841339111 mm for frame 126

Saving results

Total time: 181.24087476730347
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_1444/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1444/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1444/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00753115
Iteration 2/25 | Loss: 0.00147568
Iteration 3/25 | Loss: 0.00138158
Iteration 4/25 | Loss: 0.00136491
Iteration 5/25 | Loss: 0.00136096
Iteration 6/25 | Loss: 0.00136017
Iteration 7/25 | Loss: 0.00136017
Iteration 8/25 | Loss: 0.00136017
Iteration 9/25 | Loss: 0.00136017
Iteration 10/25 | Loss: 0.00136017
Iteration 11/25 | Loss: 0.00136017
Iteration 12/25 | Loss: 0.00136017
Iteration 13/25 | Loss: 0.00136017
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0013601667014881968, 0.0013601667014881968, 0.0013601667014881968, 0.0013601667014881968, 0.0013601667014881968]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013601667014881968

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.29922700
Iteration 2/25 | Loss: 0.00108728
Iteration 3/25 | Loss: 0.00108728
Iteration 4/25 | Loss: 0.00108727
Iteration 5/25 | Loss: 0.00108727
Iteration 6/25 | Loss: 0.00108727
Iteration 7/25 | Loss: 0.00108727
Iteration 8/25 | Loss: 0.00108727
Iteration 9/25 | Loss: 0.00108727
Iteration 10/25 | Loss: 0.00108727
Iteration 11/25 | Loss: 0.00108727
Iteration 12/25 | Loss: 0.00108727
Iteration 13/25 | Loss: 0.00108727
Iteration 14/25 | Loss: 0.00108727
Iteration 15/25 | Loss: 0.00108727
Iteration 16/25 | Loss: 0.00108727
Iteration 17/25 | Loss: 0.00108727
Iteration 18/25 | Loss: 0.00108727
Iteration 19/25 | Loss: 0.00108727
Iteration 20/25 | Loss: 0.00108727
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010872733546420932, 0.0010872733546420932, 0.0010872733546420932, 0.0010872733546420932, 0.0010872733546420932]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010872733546420932

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108727
Iteration 2/1000 | Loss: 0.00006926
Iteration 3/1000 | Loss: 0.00004085
Iteration 4/1000 | Loss: 0.00003582
Iteration 5/1000 | Loss: 0.00003347
Iteration 6/1000 | Loss: 0.00003262
Iteration 7/1000 | Loss: 0.00003175
Iteration 8/1000 | Loss: 0.00003121
Iteration 9/1000 | Loss: 0.00003092
Iteration 10/1000 | Loss: 0.00003076
Iteration 11/1000 | Loss: 0.00003075
Iteration 12/1000 | Loss: 0.00003074
Iteration 13/1000 | Loss: 0.00003073
Iteration 14/1000 | Loss: 0.00003072
Iteration 15/1000 | Loss: 0.00003068
Iteration 16/1000 | Loss: 0.00003066
Iteration 17/1000 | Loss: 0.00003066
Iteration 18/1000 | Loss: 0.00003064
Iteration 19/1000 | Loss: 0.00003058
Iteration 20/1000 | Loss: 0.00003054
Iteration 21/1000 | Loss: 0.00003051
Iteration 22/1000 | Loss: 0.00003051
Iteration 23/1000 | Loss: 0.00003046
Iteration 24/1000 | Loss: 0.00003046
Iteration 25/1000 | Loss: 0.00003042
Iteration 26/1000 | Loss: 0.00003042
Iteration 27/1000 | Loss: 0.00003042
Iteration 28/1000 | Loss: 0.00003041
Iteration 29/1000 | Loss: 0.00003041
Iteration 30/1000 | Loss: 0.00003040
Iteration 31/1000 | Loss: 0.00003040
Iteration 32/1000 | Loss: 0.00003039
Iteration 33/1000 | Loss: 0.00003037
Iteration 34/1000 | Loss: 0.00003037
Iteration 35/1000 | Loss: 0.00003037
Iteration 36/1000 | Loss: 0.00003036
Iteration 37/1000 | Loss: 0.00003036
Iteration 38/1000 | Loss: 0.00003035
Iteration 39/1000 | Loss: 0.00003035
Iteration 40/1000 | Loss: 0.00003034
Iteration 41/1000 | Loss: 0.00003034
Iteration 42/1000 | Loss: 0.00003034
Iteration 43/1000 | Loss: 0.00003034
Iteration 44/1000 | Loss: 0.00003033
Iteration 45/1000 | Loss: 0.00003033
Iteration 46/1000 | Loss: 0.00003030
Iteration 47/1000 | Loss: 0.00003030
Iteration 48/1000 | Loss: 0.00003029
Iteration 49/1000 | Loss: 0.00003029
Iteration 50/1000 | Loss: 0.00003029
Iteration 51/1000 | Loss: 0.00003028
Iteration 52/1000 | Loss: 0.00003028
Iteration 53/1000 | Loss: 0.00003028
Iteration 54/1000 | Loss: 0.00003028
Iteration 55/1000 | Loss: 0.00003026
Iteration 56/1000 | Loss: 0.00003026
Iteration 57/1000 | Loss: 0.00003026
Iteration 58/1000 | Loss: 0.00003025
Iteration 59/1000 | Loss: 0.00003025
Iteration 60/1000 | Loss: 0.00003024
Iteration 61/1000 | Loss: 0.00003023
Iteration 62/1000 | Loss: 0.00003022
Iteration 63/1000 | Loss: 0.00003022
Iteration 64/1000 | Loss: 0.00003022
Iteration 65/1000 | Loss: 0.00003022
Iteration 66/1000 | Loss: 0.00003021
Iteration 67/1000 | Loss: 0.00003021
Iteration 68/1000 | Loss: 0.00003019
Iteration 69/1000 | Loss: 0.00003019
Iteration 70/1000 | Loss: 0.00003019
Iteration 71/1000 | Loss: 0.00003019
Iteration 72/1000 | Loss: 0.00003018
Iteration 73/1000 | Loss: 0.00003018
Iteration 74/1000 | Loss: 0.00003018
Iteration 75/1000 | Loss: 0.00003018
Iteration 76/1000 | Loss: 0.00003018
Iteration 77/1000 | Loss: 0.00003018
Iteration 78/1000 | Loss: 0.00003017
Iteration 79/1000 | Loss: 0.00003016
Iteration 80/1000 | Loss: 0.00003016
Iteration 81/1000 | Loss: 0.00003016
Iteration 82/1000 | Loss: 0.00003016
Iteration 83/1000 | Loss: 0.00003016
Iteration 84/1000 | Loss: 0.00003016
Iteration 85/1000 | Loss: 0.00003015
Iteration 86/1000 | Loss: 0.00003015
Iteration 87/1000 | Loss: 0.00003015
Iteration 88/1000 | Loss: 0.00003015
Iteration 89/1000 | Loss: 0.00003015
Iteration 90/1000 | Loss: 0.00003015
Iteration 91/1000 | Loss: 0.00003015
Iteration 92/1000 | Loss: 0.00003014
Iteration 93/1000 | Loss: 0.00003014
Iteration 94/1000 | Loss: 0.00003014
Iteration 95/1000 | Loss: 0.00003014
Iteration 96/1000 | Loss: 0.00003014
Iteration 97/1000 | Loss: 0.00003014
Iteration 98/1000 | Loss: 0.00003014
Iteration 99/1000 | Loss: 0.00003014
Iteration 100/1000 | Loss: 0.00003014
Iteration 101/1000 | Loss: 0.00003014
Iteration 102/1000 | Loss: 0.00003014
Iteration 103/1000 | Loss: 0.00003014
Iteration 104/1000 | Loss: 0.00003014
Iteration 105/1000 | Loss: 0.00003014
Iteration 106/1000 | Loss: 0.00003013
Iteration 107/1000 | Loss: 0.00003013
Iteration 108/1000 | Loss: 0.00003013
Iteration 109/1000 | Loss: 0.00003013
Iteration 110/1000 | Loss: 0.00003013
Iteration 111/1000 | Loss: 0.00003013
Iteration 112/1000 | Loss: 0.00003013
Iteration 113/1000 | Loss: 0.00003013
Iteration 114/1000 | Loss: 0.00003013
Iteration 115/1000 | Loss: 0.00003013
Iteration 116/1000 | Loss: 0.00003013
Iteration 117/1000 | Loss: 0.00003013
Iteration 118/1000 | Loss: 0.00003013
Iteration 119/1000 | Loss: 0.00003013
Iteration 120/1000 | Loss: 0.00003013
Iteration 121/1000 | Loss: 0.00003013
Iteration 122/1000 | Loss: 0.00003013
Iteration 123/1000 | Loss: 0.00003013
Iteration 124/1000 | Loss: 0.00003013
Iteration 125/1000 | Loss: 0.00003012
Iteration 126/1000 | Loss: 0.00003012
Iteration 127/1000 | Loss: 0.00003012
Iteration 128/1000 | Loss: 0.00003012
Iteration 129/1000 | Loss: 0.00003012
Iteration 130/1000 | Loss: 0.00003012
Iteration 131/1000 | Loss: 0.00003012
Iteration 132/1000 | Loss: 0.00003012
Iteration 133/1000 | Loss: 0.00003012
Iteration 134/1000 | Loss: 0.00003012
Iteration 135/1000 | Loss: 0.00003012
Iteration 136/1000 | Loss: 0.00003012
Iteration 137/1000 | Loss: 0.00003012
Iteration 138/1000 | Loss: 0.00003012
Iteration 139/1000 | Loss: 0.00003012
Iteration 140/1000 | Loss: 0.00003012
Iteration 141/1000 | Loss: 0.00003012
Iteration 142/1000 | Loss: 0.00003012
Iteration 143/1000 | Loss: 0.00003012
Iteration 144/1000 | Loss: 0.00003012
Iteration 145/1000 | Loss: 0.00003012
Iteration 146/1000 | Loss: 0.00003012
Iteration 147/1000 | Loss: 0.00003012
Iteration 148/1000 | Loss: 0.00003012
Iteration 149/1000 | Loss: 0.00003012
Iteration 150/1000 | Loss: 0.00003012
Iteration 151/1000 | Loss: 0.00003012
Iteration 152/1000 | Loss: 0.00003012
Iteration 153/1000 | Loss: 0.00003012
Iteration 154/1000 | Loss: 0.00003012
Iteration 155/1000 | Loss: 0.00003012
Iteration 156/1000 | Loss: 0.00003012
Iteration 157/1000 | Loss: 0.00003012
Iteration 158/1000 | Loss: 0.00003012
Iteration 159/1000 | Loss: 0.00003012
Iteration 160/1000 | Loss: 0.00003012
Iteration 161/1000 | Loss: 0.00003012
Iteration 162/1000 | Loss: 0.00003012
Iteration 163/1000 | Loss: 0.00003012
Iteration 164/1000 | Loss: 0.00003012
Iteration 165/1000 | Loss: 0.00003012
Iteration 166/1000 | Loss: 0.00003012
Iteration 167/1000 | Loss: 0.00003012
Iteration 168/1000 | Loss: 0.00003012
Iteration 169/1000 | Loss: 0.00003012
Iteration 170/1000 | Loss: 0.00003012
Iteration 171/1000 | Loss: 0.00003012
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [3.0122533644316718e-05, 3.0122533644316718e-05, 3.0122533644316718e-05, 3.0122533644316718e-05, 3.0122533644316718e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0122533644316718e-05

Optimization complete. Final v2v error: 4.862614631652832 mm

Highest mean error: 5.0299973487854 mm for frame 79

Lowest mean error: 4.651233196258545 mm for frame 51

Saving results

Total time: 35.48687171936035
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_1444/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1444/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1444/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00790282
Iteration 2/25 | Loss: 0.00182444
Iteration 3/25 | Loss: 0.00144235
Iteration 4/25 | Loss: 0.00138565
Iteration 5/25 | Loss: 0.00137834
Iteration 6/25 | Loss: 0.00137756
Iteration 7/25 | Loss: 0.00137756
Iteration 8/25 | Loss: 0.00137756
Iteration 9/25 | Loss: 0.00137756
Iteration 10/25 | Loss: 0.00137756
Iteration 11/25 | Loss: 0.00137756
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001377557753585279, 0.001377557753585279, 0.001377557753585279, 0.001377557753585279, 0.001377557753585279]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001377557753585279

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49432170
Iteration 2/25 | Loss: 0.00110431
Iteration 3/25 | Loss: 0.00110431
Iteration 4/25 | Loss: 0.00110431
Iteration 5/25 | Loss: 0.00110431
Iteration 6/25 | Loss: 0.00110431
Iteration 7/25 | Loss: 0.00110431
Iteration 8/25 | Loss: 0.00110431
Iteration 9/25 | Loss: 0.00110430
Iteration 10/25 | Loss: 0.00110430
Iteration 11/25 | Loss: 0.00110430
Iteration 12/25 | Loss: 0.00110430
Iteration 13/25 | Loss: 0.00110430
Iteration 14/25 | Loss: 0.00110430
Iteration 15/25 | Loss: 0.00110430
Iteration 16/25 | Loss: 0.00110430
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011043045669794083, 0.0011043045669794083, 0.0011043045669794083, 0.0011043045669794083, 0.0011043045669794083]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011043045669794083

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110430
Iteration 2/1000 | Loss: 0.00007510
Iteration 3/1000 | Loss: 0.00005110
Iteration 4/1000 | Loss: 0.00004148
Iteration 5/1000 | Loss: 0.00003801
Iteration 6/1000 | Loss: 0.00003581
Iteration 7/1000 | Loss: 0.00003476
Iteration 8/1000 | Loss: 0.00003415
Iteration 9/1000 | Loss: 0.00003380
Iteration 10/1000 | Loss: 0.00003342
Iteration 11/1000 | Loss: 0.00003319
Iteration 12/1000 | Loss: 0.00003299
Iteration 13/1000 | Loss: 0.00003282
Iteration 14/1000 | Loss: 0.00003262
Iteration 15/1000 | Loss: 0.00003243
Iteration 16/1000 | Loss: 0.00003228
Iteration 17/1000 | Loss: 0.00003226
Iteration 18/1000 | Loss: 0.00003222
Iteration 19/1000 | Loss: 0.00003221
Iteration 20/1000 | Loss: 0.00003214
Iteration 21/1000 | Loss: 0.00003209
Iteration 22/1000 | Loss: 0.00003206
Iteration 23/1000 | Loss: 0.00003203
Iteration 24/1000 | Loss: 0.00003202
Iteration 25/1000 | Loss: 0.00003198
Iteration 26/1000 | Loss: 0.00003197
Iteration 27/1000 | Loss: 0.00003197
Iteration 28/1000 | Loss: 0.00003196
Iteration 29/1000 | Loss: 0.00003195
Iteration 30/1000 | Loss: 0.00003195
Iteration 31/1000 | Loss: 0.00003195
Iteration 32/1000 | Loss: 0.00003194
Iteration 33/1000 | Loss: 0.00003194
Iteration 34/1000 | Loss: 0.00003194
Iteration 35/1000 | Loss: 0.00003194
Iteration 36/1000 | Loss: 0.00003194
Iteration 37/1000 | Loss: 0.00003194
Iteration 38/1000 | Loss: 0.00003193
Iteration 39/1000 | Loss: 0.00003193
Iteration 40/1000 | Loss: 0.00003193
Iteration 41/1000 | Loss: 0.00003193
Iteration 42/1000 | Loss: 0.00003193
Iteration 43/1000 | Loss: 0.00003192
Iteration 44/1000 | Loss: 0.00003192
Iteration 45/1000 | Loss: 0.00003191
Iteration 46/1000 | Loss: 0.00003191
Iteration 47/1000 | Loss: 0.00003191
Iteration 48/1000 | Loss: 0.00003191
Iteration 49/1000 | Loss: 0.00003191
Iteration 50/1000 | Loss: 0.00003191
Iteration 51/1000 | Loss: 0.00003191
Iteration 52/1000 | Loss: 0.00003191
Iteration 53/1000 | Loss: 0.00003191
Iteration 54/1000 | Loss: 0.00003191
Iteration 55/1000 | Loss: 0.00003191
Iteration 56/1000 | Loss: 0.00003191
Iteration 57/1000 | Loss: 0.00003191
Iteration 58/1000 | Loss: 0.00003190
Iteration 59/1000 | Loss: 0.00003190
Iteration 60/1000 | Loss: 0.00003190
Iteration 61/1000 | Loss: 0.00003190
Iteration 62/1000 | Loss: 0.00003190
Iteration 63/1000 | Loss: 0.00003190
Iteration 64/1000 | Loss: 0.00003190
Iteration 65/1000 | Loss: 0.00003190
Iteration 66/1000 | Loss: 0.00003188
Iteration 67/1000 | Loss: 0.00003188
Iteration 68/1000 | Loss: 0.00003187
Iteration 69/1000 | Loss: 0.00003185
Iteration 70/1000 | Loss: 0.00003185
Iteration 71/1000 | Loss: 0.00003185
Iteration 72/1000 | Loss: 0.00003184
Iteration 73/1000 | Loss: 0.00003184
Iteration 74/1000 | Loss: 0.00003184
Iteration 75/1000 | Loss: 0.00003183
Iteration 76/1000 | Loss: 0.00003183
Iteration 77/1000 | Loss: 0.00003183
Iteration 78/1000 | Loss: 0.00003182
Iteration 79/1000 | Loss: 0.00003182
Iteration 80/1000 | Loss: 0.00003182
Iteration 81/1000 | Loss: 0.00003182
Iteration 82/1000 | Loss: 0.00003182
Iteration 83/1000 | Loss: 0.00003182
Iteration 84/1000 | Loss: 0.00003182
Iteration 85/1000 | Loss: 0.00003182
Iteration 86/1000 | Loss: 0.00003182
Iteration 87/1000 | Loss: 0.00003182
Iteration 88/1000 | Loss: 0.00003182
Iteration 89/1000 | Loss: 0.00003182
Iteration 90/1000 | Loss: 0.00003181
Iteration 91/1000 | Loss: 0.00003181
Iteration 92/1000 | Loss: 0.00003181
Iteration 93/1000 | Loss: 0.00003181
Iteration 94/1000 | Loss: 0.00003181
Iteration 95/1000 | Loss: 0.00003180
Iteration 96/1000 | Loss: 0.00003180
Iteration 97/1000 | Loss: 0.00003180
Iteration 98/1000 | Loss: 0.00003180
Iteration 99/1000 | Loss: 0.00003180
Iteration 100/1000 | Loss: 0.00003180
Iteration 101/1000 | Loss: 0.00003179
Iteration 102/1000 | Loss: 0.00003179
Iteration 103/1000 | Loss: 0.00003179
Iteration 104/1000 | Loss: 0.00003179
Iteration 105/1000 | Loss: 0.00003179
Iteration 106/1000 | Loss: 0.00003179
Iteration 107/1000 | Loss: 0.00003179
Iteration 108/1000 | Loss: 0.00003179
Iteration 109/1000 | Loss: 0.00003179
Iteration 110/1000 | Loss: 0.00003178
Iteration 111/1000 | Loss: 0.00003178
Iteration 112/1000 | Loss: 0.00003178
Iteration 113/1000 | Loss: 0.00003178
Iteration 114/1000 | Loss: 0.00003178
Iteration 115/1000 | Loss: 0.00003178
Iteration 116/1000 | Loss: 0.00003178
Iteration 117/1000 | Loss: 0.00003178
Iteration 118/1000 | Loss: 0.00003178
Iteration 119/1000 | Loss: 0.00003178
Iteration 120/1000 | Loss: 0.00003178
Iteration 121/1000 | Loss: 0.00003178
Iteration 122/1000 | Loss: 0.00003178
Iteration 123/1000 | Loss: 0.00003178
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [3.1778228731127456e-05, 3.1778228731127456e-05, 3.1778228731127456e-05, 3.1778228731127456e-05, 3.1778228731127456e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.1778228731127456e-05

Optimization complete. Final v2v error: 4.858120441436768 mm

Highest mean error: 5.265373706817627 mm for frame 19

Lowest mean error: 4.39510440826416 mm for frame 165

Saving results

Total time: 47.502803564071655
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_1444/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1444/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1444/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01171259
Iteration 2/25 | Loss: 0.00238381
Iteration 3/25 | Loss: 0.00200668
Iteration 4/25 | Loss: 0.00195410
Iteration 5/25 | Loss: 0.00191370
Iteration 6/25 | Loss: 0.00188340
Iteration 7/25 | Loss: 0.00187850
Iteration 8/25 | Loss: 0.00184625
Iteration 9/25 | Loss: 0.00180357
Iteration 10/25 | Loss: 0.00176602
Iteration 11/25 | Loss: 0.00174000
Iteration 12/25 | Loss: 0.00174361
Iteration 13/25 | Loss: 0.00175034
Iteration 14/25 | Loss: 0.00174192
Iteration 15/25 | Loss: 0.00170878
Iteration 16/25 | Loss: 0.00167805
Iteration 17/25 | Loss: 0.00168015
Iteration 18/25 | Loss: 0.00168240
Iteration 19/25 | Loss: 0.00168721
Iteration 20/25 | Loss: 0.00168643
Iteration 21/25 | Loss: 0.00168398
Iteration 22/25 | Loss: 0.00168035
Iteration 23/25 | Loss: 0.00167265
Iteration 24/25 | Loss: 0.00167271
Iteration 25/25 | Loss: 0.00167728

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25568271
Iteration 2/25 | Loss: 0.00269451
Iteration 3/25 | Loss: 0.00269446
Iteration 4/25 | Loss: 0.00269446
Iteration 5/25 | Loss: 0.00269446
Iteration 6/25 | Loss: 0.00269446
Iteration 7/25 | Loss: 0.00269446
Iteration 8/25 | Loss: 0.00269446
Iteration 9/25 | Loss: 0.00269446
Iteration 10/25 | Loss: 0.00269446
Iteration 11/25 | Loss: 0.00269446
Iteration 12/25 | Loss: 0.00269446
Iteration 13/25 | Loss: 0.00269446
Iteration 14/25 | Loss: 0.00269446
Iteration 15/25 | Loss: 0.00269446
Iteration 16/25 | Loss: 0.00269446
Iteration 17/25 | Loss: 0.00269446
Iteration 18/25 | Loss: 0.00269446
Iteration 19/25 | Loss: 0.00269446
Iteration 20/25 | Loss: 0.00269446
Iteration 21/25 | Loss: 0.00269446
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0026944612618535757, 0.0026944612618535757, 0.0026944612618535757, 0.0026944612618535757, 0.0026944612618535757]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026944612618535757

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00269446
Iteration 2/1000 | Loss: 0.00142608
Iteration 3/1000 | Loss: 0.00092174
Iteration 4/1000 | Loss: 0.00091277
Iteration 5/1000 | Loss: 0.00108318
Iteration 6/1000 | Loss: 0.00150158
Iteration 7/1000 | Loss: 0.00112436
Iteration 8/1000 | Loss: 0.00135117
Iteration 9/1000 | Loss: 0.00089844
Iteration 10/1000 | Loss: 0.00117843
Iteration 11/1000 | Loss: 0.00151067
Iteration 12/1000 | Loss: 0.00068454
Iteration 13/1000 | Loss: 0.00139698
Iteration 14/1000 | Loss: 0.00243532
Iteration 15/1000 | Loss: 0.00206956
Iteration 16/1000 | Loss: 0.00194864
Iteration 17/1000 | Loss: 0.00217649
Iteration 18/1000 | Loss: 0.00085470
Iteration 19/1000 | Loss: 0.00163658
Iteration 20/1000 | Loss: 0.00038720
Iteration 21/1000 | Loss: 0.00128775
Iteration 22/1000 | Loss: 0.00461263
Iteration 23/1000 | Loss: 0.00368496
Iteration 24/1000 | Loss: 0.00113211
Iteration 25/1000 | Loss: 0.00143212
Iteration 26/1000 | Loss: 0.00060754
Iteration 27/1000 | Loss: 0.00145126
Iteration 28/1000 | Loss: 0.00379007
Iteration 29/1000 | Loss: 0.00254314
Iteration 30/1000 | Loss: 0.00247981
Iteration 31/1000 | Loss: 0.00274532
Iteration 32/1000 | Loss: 0.00161159
Iteration 33/1000 | Loss: 0.00080477
Iteration 34/1000 | Loss: 0.00090889
Iteration 35/1000 | Loss: 0.00157647
Iteration 36/1000 | Loss: 0.00190447
Iteration 37/1000 | Loss: 0.00110685
Iteration 38/1000 | Loss: 0.00166040
Iteration 39/1000 | Loss: 0.00116572
Iteration 40/1000 | Loss: 0.00185704
Iteration 41/1000 | Loss: 0.00217657
Iteration 42/1000 | Loss: 0.00204134
Iteration 43/1000 | Loss: 0.00180460
Iteration 44/1000 | Loss: 0.00130123
Iteration 45/1000 | Loss: 0.00088374
Iteration 46/1000 | Loss: 0.00080177
Iteration 47/1000 | Loss: 0.00066458
Iteration 48/1000 | Loss: 0.00065454
Iteration 49/1000 | Loss: 0.00120114
Iteration 50/1000 | Loss: 0.00132085
Iteration 51/1000 | Loss: 0.00068565
Iteration 52/1000 | Loss: 0.00044940
Iteration 53/1000 | Loss: 0.00082738
Iteration 54/1000 | Loss: 0.00055293
Iteration 55/1000 | Loss: 0.00082972
Iteration 56/1000 | Loss: 0.00109268
Iteration 57/1000 | Loss: 0.00111633
Iteration 58/1000 | Loss: 0.00102601
Iteration 59/1000 | Loss: 0.00089742
Iteration 60/1000 | Loss: 0.00099053
Iteration 61/1000 | Loss: 0.00138275
Iteration 62/1000 | Loss: 0.00058470
Iteration 63/1000 | Loss: 0.00076697
Iteration 64/1000 | Loss: 0.00083195
Iteration 65/1000 | Loss: 0.00096142
Iteration 66/1000 | Loss: 0.00122409
Iteration 67/1000 | Loss: 0.00105914
Iteration 68/1000 | Loss: 0.00093710
Iteration 69/1000 | Loss: 0.00085390
Iteration 70/1000 | Loss: 0.00099059
Iteration 71/1000 | Loss: 0.00101205
Iteration 72/1000 | Loss: 0.00089777
Iteration 73/1000 | Loss: 0.00100175
Iteration 74/1000 | Loss: 0.00085086
Iteration 75/1000 | Loss: 0.00076829
Iteration 76/1000 | Loss: 0.00128249
Iteration 77/1000 | Loss: 0.00105302
Iteration 78/1000 | Loss: 0.00093760
Iteration 79/1000 | Loss: 0.00134868
Iteration 80/1000 | Loss: 0.00088987
Iteration 81/1000 | Loss: 0.00070439
Iteration 82/1000 | Loss: 0.00099301
Iteration 83/1000 | Loss: 0.00112510
Iteration 84/1000 | Loss: 0.00073891
Iteration 85/1000 | Loss: 0.00079327
Iteration 86/1000 | Loss: 0.00064586
Iteration 87/1000 | Loss: 0.00084002
Iteration 88/1000 | Loss: 0.00066126
Iteration 89/1000 | Loss: 0.00106731
Iteration 90/1000 | Loss: 0.00120795
Iteration 91/1000 | Loss: 0.00119976
Iteration 92/1000 | Loss: 0.00052965
Iteration 93/1000 | Loss: 0.00064486
Iteration 94/1000 | Loss: 0.00057757
Iteration 95/1000 | Loss: 0.00059730
Iteration 96/1000 | Loss: 0.00070440
Iteration 97/1000 | Loss: 0.00080647
Iteration 98/1000 | Loss: 0.00082905
Iteration 99/1000 | Loss: 0.00084986
Iteration 100/1000 | Loss: 0.00089114
Iteration 101/1000 | Loss: 0.00080430
Iteration 102/1000 | Loss: 0.00087298
Iteration 103/1000 | Loss: 0.00080300
Iteration 104/1000 | Loss: 0.00086983
Iteration 105/1000 | Loss: 0.00084624
Iteration 106/1000 | Loss: 0.00051556
Iteration 107/1000 | Loss: 0.00091717
Iteration 108/1000 | Loss: 0.00044303
Iteration 109/1000 | Loss: 0.00051202
Iteration 110/1000 | Loss: 0.00039904
Iteration 111/1000 | Loss: 0.00054997
Iteration 112/1000 | Loss: 0.00083285
Iteration 113/1000 | Loss: 0.00056203
Iteration 114/1000 | Loss: 0.00032588
Iteration 115/1000 | Loss: 0.00067450
Iteration 116/1000 | Loss: 0.00076376
Iteration 117/1000 | Loss: 0.00069142
Iteration 118/1000 | Loss: 0.00086668
Iteration 119/1000 | Loss: 0.00049885
Iteration 120/1000 | Loss: 0.00032536
Iteration 121/1000 | Loss: 0.00033520
Iteration 122/1000 | Loss: 0.00037028
Iteration 123/1000 | Loss: 0.00043761
Iteration 124/1000 | Loss: 0.00025769
Iteration 125/1000 | Loss: 0.00024425
Iteration 126/1000 | Loss: 0.00018962
Iteration 127/1000 | Loss: 0.00014584
Iteration 128/1000 | Loss: 0.00029135
Iteration 129/1000 | Loss: 0.00027163
Iteration 130/1000 | Loss: 0.00023259
Iteration 131/1000 | Loss: 0.00026308
Iteration 132/1000 | Loss: 0.00028806
Iteration 133/1000 | Loss: 0.00028795
Iteration 134/1000 | Loss: 0.00034851
Iteration 135/1000 | Loss: 0.00049710
Iteration 136/1000 | Loss: 0.00047804
Iteration 137/1000 | Loss: 0.00028489
Iteration 138/1000 | Loss: 0.00031540
Iteration 139/1000 | Loss: 0.00062249
Iteration 140/1000 | Loss: 0.00026613
Iteration 141/1000 | Loss: 0.00039141
Iteration 142/1000 | Loss: 0.00024519
Iteration 143/1000 | Loss: 0.00037416
Iteration 144/1000 | Loss: 0.00033626
Iteration 145/1000 | Loss: 0.00035235
Iteration 146/1000 | Loss: 0.00014799
Iteration 147/1000 | Loss: 0.00043611
Iteration 148/1000 | Loss: 0.00033652
Iteration 149/1000 | Loss: 0.00038259
Iteration 150/1000 | Loss: 0.00029091
Iteration 151/1000 | Loss: 0.00051834
Iteration 152/1000 | Loss: 0.00040529
Iteration 153/1000 | Loss: 0.00028833
Iteration 154/1000 | Loss: 0.00040703
Iteration 155/1000 | Loss: 0.00042380
Iteration 156/1000 | Loss: 0.00063938
Iteration 157/1000 | Loss: 0.00040687
Iteration 158/1000 | Loss: 0.00014810
Iteration 159/1000 | Loss: 0.00023616
Iteration 160/1000 | Loss: 0.00028221
Iteration 161/1000 | Loss: 0.00017489
Iteration 162/1000 | Loss: 0.00039113
Iteration 163/1000 | Loss: 0.00030955
Iteration 164/1000 | Loss: 0.00018356
Iteration 165/1000 | Loss: 0.00057487
Iteration 166/1000 | Loss: 0.00029851
Iteration 167/1000 | Loss: 0.00061442
Iteration 168/1000 | Loss: 0.00038250
Iteration 169/1000 | Loss: 0.00015427
Iteration 170/1000 | Loss: 0.00034071
Iteration 171/1000 | Loss: 0.00036451
Iteration 172/1000 | Loss: 0.00037077
Iteration 173/1000 | Loss: 0.00025272
Iteration 174/1000 | Loss: 0.00028743
Iteration 175/1000 | Loss: 0.00030582
Iteration 176/1000 | Loss: 0.00022482
Iteration 177/1000 | Loss: 0.00043849
Iteration 178/1000 | Loss: 0.00052746
Iteration 179/1000 | Loss: 0.00042434
Iteration 180/1000 | Loss: 0.00012520
Iteration 181/1000 | Loss: 0.00017616
Iteration 182/1000 | Loss: 0.00035861
Iteration 183/1000 | Loss: 0.00011099
Iteration 184/1000 | Loss: 0.00009041
Iteration 185/1000 | Loss: 0.00039097
Iteration 186/1000 | Loss: 0.00038952
Iteration 187/1000 | Loss: 0.00053599
Iteration 188/1000 | Loss: 0.00023251
Iteration 189/1000 | Loss: 0.00020889
Iteration 190/1000 | Loss: 0.00024626
Iteration 191/1000 | Loss: 0.00022149
Iteration 192/1000 | Loss: 0.00019046
Iteration 193/1000 | Loss: 0.00024513
Iteration 194/1000 | Loss: 0.00040201
Iteration 195/1000 | Loss: 0.00031067
Iteration 196/1000 | Loss: 0.00027821
Iteration 197/1000 | Loss: 0.00016286
Iteration 198/1000 | Loss: 0.00018051
Iteration 199/1000 | Loss: 0.00017636
Iteration 200/1000 | Loss: 0.00008247
Iteration 201/1000 | Loss: 0.00012138
Iteration 202/1000 | Loss: 0.00007777
Iteration 203/1000 | Loss: 0.00006328
Iteration 204/1000 | Loss: 0.00041212
Iteration 205/1000 | Loss: 0.00013062
Iteration 206/1000 | Loss: 0.00028440
Iteration 207/1000 | Loss: 0.00039515
Iteration 208/1000 | Loss: 0.00039574
Iteration 209/1000 | Loss: 0.00029016
Iteration 210/1000 | Loss: 0.00034171
Iteration 211/1000 | Loss: 0.00028924
Iteration 212/1000 | Loss: 0.00028549
Iteration 213/1000 | Loss: 0.00028524
Iteration 214/1000 | Loss: 0.00044385
Iteration 215/1000 | Loss: 0.00051215
Iteration 216/1000 | Loss: 0.00054092
Iteration 217/1000 | Loss: 0.00020784
Iteration 218/1000 | Loss: 0.00010973
Iteration 219/1000 | Loss: 0.00006980
Iteration 220/1000 | Loss: 0.00005620
Iteration 221/1000 | Loss: 0.00006206
Iteration 222/1000 | Loss: 0.00004885
Iteration 223/1000 | Loss: 0.00006683
Iteration 224/1000 | Loss: 0.00005363
Iteration 225/1000 | Loss: 0.00004662
Iteration 226/1000 | Loss: 0.00005162
Iteration 227/1000 | Loss: 0.00004543
Iteration 228/1000 | Loss: 0.00005328
Iteration 229/1000 | Loss: 0.00004247
Iteration 230/1000 | Loss: 0.00004455
Iteration 231/1000 | Loss: 0.00004247
Iteration 232/1000 | Loss: 0.00004869
Iteration 233/1000 | Loss: 0.00004316
Iteration 234/1000 | Loss: 0.00004797
Iteration 235/1000 | Loss: 0.00004961
Iteration 236/1000 | Loss: 0.00004770
Iteration 237/1000 | Loss: 0.00005021
Iteration 238/1000 | Loss: 0.00004768
Iteration 239/1000 | Loss: 0.00005565
Iteration 240/1000 | Loss: 0.00004819
Iteration 241/1000 | Loss: 0.00004546
Iteration 242/1000 | Loss: 0.00005063
Iteration 243/1000 | Loss: 0.00005259
Iteration 244/1000 | Loss: 0.00005062
Iteration 245/1000 | Loss: 0.00004773
Iteration 246/1000 | Loss: 0.00005112
Iteration 247/1000 | Loss: 0.00005104
Iteration 248/1000 | Loss: 0.00005234
Iteration 249/1000 | Loss: 0.00004906
Iteration 250/1000 | Loss: 0.00004930
Iteration 251/1000 | Loss: 0.00004993
Iteration 252/1000 | Loss: 0.00005055
Iteration 253/1000 | Loss: 0.00004734
Iteration 254/1000 | Loss: 0.00004988
Iteration 255/1000 | Loss: 0.00005303
Iteration 256/1000 | Loss: 0.00005206
Iteration 257/1000 | Loss: 0.00005367
Iteration 258/1000 | Loss: 0.00005405
Iteration 259/1000 | Loss: 0.00004229
Iteration 260/1000 | Loss: 0.00003850
Iteration 261/1000 | Loss: 0.00003650
Iteration 262/1000 | Loss: 0.00003530
Iteration 263/1000 | Loss: 0.00003464
Iteration 264/1000 | Loss: 0.00003425
Iteration 265/1000 | Loss: 0.00003403
Iteration 266/1000 | Loss: 0.00003378
Iteration 267/1000 | Loss: 0.00003376
Iteration 268/1000 | Loss: 0.00003369
Iteration 269/1000 | Loss: 0.00003369
Iteration 270/1000 | Loss: 0.00003365
Iteration 271/1000 | Loss: 0.00003365
Iteration 272/1000 | Loss: 0.00003353
Iteration 273/1000 | Loss: 0.00003346
Iteration 274/1000 | Loss: 0.00003346
Iteration 275/1000 | Loss: 0.00003346
Iteration 276/1000 | Loss: 0.00003345
Iteration 277/1000 | Loss: 0.00003345
Iteration 278/1000 | Loss: 0.00003345
Iteration 279/1000 | Loss: 0.00003345
Iteration 280/1000 | Loss: 0.00003345
Iteration 281/1000 | Loss: 0.00003344
Iteration 282/1000 | Loss: 0.00003344
Iteration 283/1000 | Loss: 0.00003343
Iteration 284/1000 | Loss: 0.00003343
Iteration 285/1000 | Loss: 0.00003343
Iteration 286/1000 | Loss: 0.00003343
Iteration 287/1000 | Loss: 0.00003343
Iteration 288/1000 | Loss: 0.00003342
Iteration 289/1000 | Loss: 0.00003342
Iteration 290/1000 | Loss: 0.00003341
Iteration 291/1000 | Loss: 0.00003341
Iteration 292/1000 | Loss: 0.00003341
Iteration 293/1000 | Loss: 0.00003341
Iteration 294/1000 | Loss: 0.00003341
Iteration 295/1000 | Loss: 0.00003340
Iteration 296/1000 | Loss: 0.00003340
Iteration 297/1000 | Loss: 0.00003340
Iteration 298/1000 | Loss: 0.00003340
Iteration 299/1000 | Loss: 0.00003340
Iteration 300/1000 | Loss: 0.00003340
Iteration 301/1000 | Loss: 0.00003339
Iteration 302/1000 | Loss: 0.00003339
Iteration 303/1000 | Loss: 0.00003339
Iteration 304/1000 | Loss: 0.00003339
Iteration 305/1000 | Loss: 0.00003339
Iteration 306/1000 | Loss: 0.00003338
Iteration 307/1000 | Loss: 0.00003338
Iteration 308/1000 | Loss: 0.00003338
Iteration 309/1000 | Loss: 0.00003338
Iteration 310/1000 | Loss: 0.00003338
Iteration 311/1000 | Loss: 0.00003337
Iteration 312/1000 | Loss: 0.00003337
Iteration 313/1000 | Loss: 0.00003337
Iteration 314/1000 | Loss: 0.00003337
Iteration 315/1000 | Loss: 0.00003337
Iteration 316/1000 | Loss: 0.00003337
Iteration 317/1000 | Loss: 0.00003337
Iteration 318/1000 | Loss: 0.00003337
Iteration 319/1000 | Loss: 0.00003337
Iteration 320/1000 | Loss: 0.00003337
Iteration 321/1000 | Loss: 0.00003336
Iteration 322/1000 | Loss: 0.00003336
Iteration 323/1000 | Loss: 0.00003336
Iteration 324/1000 | Loss: 0.00003336
Iteration 325/1000 | Loss: 0.00003336
Iteration 326/1000 | Loss: 0.00003336
Iteration 327/1000 | Loss: 0.00003336
Iteration 328/1000 | Loss: 0.00003336
Iteration 329/1000 | Loss: 0.00003336
Iteration 330/1000 | Loss: 0.00003336
Iteration 331/1000 | Loss: 0.00003336
Iteration 332/1000 | Loss: 0.00003336
Iteration 333/1000 | Loss: 0.00003336
Iteration 334/1000 | Loss: 0.00003336
Iteration 335/1000 | Loss: 0.00003336
Iteration 336/1000 | Loss: 0.00003336
Iteration 337/1000 | Loss: 0.00003336
Iteration 338/1000 | Loss: 0.00003336
Iteration 339/1000 | Loss: 0.00003336
Iteration 340/1000 | Loss: 0.00003335
Iteration 341/1000 | Loss: 0.00003335
Iteration 342/1000 | Loss: 0.00003335
Iteration 343/1000 | Loss: 0.00003335
Iteration 344/1000 | Loss: 0.00003335
Iteration 345/1000 | Loss: 0.00003335
Iteration 346/1000 | Loss: 0.00003335
Iteration 347/1000 | Loss: 0.00003335
Iteration 348/1000 | Loss: 0.00003335
Iteration 349/1000 | Loss: 0.00003335
Iteration 350/1000 | Loss: 0.00003335
Iteration 351/1000 | Loss: 0.00003335
Iteration 352/1000 | Loss: 0.00003335
Iteration 353/1000 | Loss: 0.00003335
Iteration 354/1000 | Loss: 0.00003335
Iteration 355/1000 | Loss: 0.00003335
Iteration 356/1000 | Loss: 0.00003335
Iteration 357/1000 | Loss: 0.00003335
Iteration 358/1000 | Loss: 0.00003335
Iteration 359/1000 | Loss: 0.00003335
Iteration 360/1000 | Loss: 0.00003335
Iteration 361/1000 | Loss: 0.00003335
Iteration 362/1000 | Loss: 0.00003335
Iteration 363/1000 | Loss: 0.00003335
Iteration 364/1000 | Loss: 0.00003335
Iteration 365/1000 | Loss: 0.00003335
Iteration 366/1000 | Loss: 0.00003335
Iteration 367/1000 | Loss: 0.00003335
Iteration 368/1000 | Loss: 0.00003335
Iteration 369/1000 | Loss: 0.00003335
Iteration 370/1000 | Loss: 0.00003335
Iteration 371/1000 | Loss: 0.00003335
Iteration 372/1000 | Loss: 0.00003335
Iteration 373/1000 | Loss: 0.00003335
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 373. Stopping optimization.
Last 5 losses: [3.334607754368335e-05, 3.334607754368335e-05, 3.334607754368335e-05, 3.334607754368335e-05, 3.334607754368335e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.334607754368335e-05

Optimization complete. Final v2v error: 4.905359268188477 mm

Highest mean error: 5.723950386047363 mm for frame 84

Lowest mean error: 4.274729251861572 mm for frame 262

Saving results

Total time: 505.57175946235657
