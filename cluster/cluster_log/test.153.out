Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=153, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 8568-8623
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01010644
Iteration 2/25 | Loss: 0.00287151
Iteration 3/25 | Loss: 0.00229645
Iteration 4/25 | Loss: 0.00188147
Iteration 5/25 | Loss: 0.00254626
Iteration 6/25 | Loss: 0.00243116
Iteration 7/25 | Loss: 0.00239644
Iteration 8/25 | Loss: 0.00168209
Iteration 9/25 | Loss: 0.00146263
Iteration 10/25 | Loss: 0.00139100
Iteration 11/25 | Loss: 0.00136666
Iteration 12/25 | Loss: 0.00135342
Iteration 13/25 | Loss: 0.00135588
Iteration 14/25 | Loss: 0.00135852
Iteration 15/25 | Loss: 0.00135330
Iteration 16/25 | Loss: 0.00135879
Iteration 17/25 | Loss: 0.00135212
Iteration 18/25 | Loss: 0.00134459
Iteration 19/25 | Loss: 0.00134161
Iteration 20/25 | Loss: 0.00134056
Iteration 21/25 | Loss: 0.00133917
Iteration 22/25 | Loss: 0.00133072
Iteration 23/25 | Loss: 0.00132961
Iteration 24/25 | Loss: 0.00133336
Iteration 25/25 | Loss: 0.00133056

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33280933
Iteration 2/25 | Loss: 0.00077611
Iteration 3/25 | Loss: 0.00077611
Iteration 4/25 | Loss: 0.00077611
Iteration 5/25 | Loss: 0.00077611
Iteration 6/25 | Loss: 0.00077611
Iteration 7/25 | Loss: 0.00077611
Iteration 8/25 | Loss: 0.00077611
Iteration 9/25 | Loss: 0.00077611
Iteration 10/25 | Loss: 0.00077611
Iteration 11/25 | Loss: 0.00077611
Iteration 12/25 | Loss: 0.00077611
Iteration 13/25 | Loss: 0.00077611
Iteration 14/25 | Loss: 0.00077611
Iteration 15/25 | Loss: 0.00077611
Iteration 16/25 | Loss: 0.00077611
Iteration 17/25 | Loss: 0.00077611
Iteration 18/25 | Loss: 0.00077611
Iteration 19/25 | Loss: 0.00077611
Iteration 20/25 | Loss: 0.00077611
Iteration 21/25 | Loss: 0.00077611
Iteration 22/25 | Loss: 0.00077611
Iteration 23/25 | Loss: 0.00077611
Iteration 24/25 | Loss: 0.00077611
Iteration 25/25 | Loss: 0.00077611

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077611
Iteration 2/1000 | Loss: 0.00007275
Iteration 3/1000 | Loss: 0.00014776
Iteration 4/1000 | Loss: 0.00032823
Iteration 5/1000 | Loss: 0.00013103
Iteration 6/1000 | Loss: 0.00005578
Iteration 7/1000 | Loss: 0.00003044
Iteration 8/1000 | Loss: 0.00016383
Iteration 9/1000 | Loss: 0.00004206
Iteration 10/1000 | Loss: 0.00005383
Iteration 11/1000 | Loss: 0.00005982
Iteration 12/1000 | Loss: 0.00008972
Iteration 13/1000 | Loss: 0.00004301
Iteration 14/1000 | Loss: 0.00004851
Iteration 15/1000 | Loss: 0.00003467
Iteration 16/1000 | Loss: 0.00004719
Iteration 17/1000 | Loss: 0.00004153
Iteration 18/1000 | Loss: 0.00004578
Iteration 19/1000 | Loss: 0.00012212
Iteration 20/1000 | Loss: 0.00004677
Iteration 21/1000 | Loss: 0.00004781
Iteration 22/1000 | Loss: 0.00016192
Iteration 23/1000 | Loss: 0.00004820
Iteration 24/1000 | Loss: 0.00005602
Iteration 25/1000 | Loss: 0.00006816
Iteration 26/1000 | Loss: 0.00005510
Iteration 27/1000 | Loss: 0.00006384
Iteration 28/1000 | Loss: 0.00003405
Iteration 29/1000 | Loss: 0.00002808
Iteration 30/1000 | Loss: 0.00002661
Iteration 31/1000 | Loss: 0.00002530
Iteration 32/1000 | Loss: 0.00002453
Iteration 33/1000 | Loss: 0.00010030
Iteration 34/1000 | Loss: 0.00100714
Iteration 35/1000 | Loss: 0.00015624
Iteration 36/1000 | Loss: 0.00004937
Iteration 37/1000 | Loss: 0.00005594
Iteration 38/1000 | Loss: 0.00003403
Iteration 39/1000 | Loss: 0.00002413
Iteration 40/1000 | Loss: 0.00003203
Iteration 41/1000 | Loss: 0.00002348
Iteration 42/1000 | Loss: 0.00002320
Iteration 43/1000 | Loss: 0.00002308
Iteration 44/1000 | Loss: 0.00002306
Iteration 45/1000 | Loss: 0.00002302
Iteration 46/1000 | Loss: 0.00002301
Iteration 47/1000 | Loss: 0.00007374
Iteration 48/1000 | Loss: 0.00005233
Iteration 49/1000 | Loss: 0.00008579
Iteration 50/1000 | Loss: 0.00002869
Iteration 51/1000 | Loss: 0.00002871
Iteration 52/1000 | Loss: 0.00002292
Iteration 53/1000 | Loss: 0.00002239
Iteration 54/1000 | Loss: 0.00002215
Iteration 55/1000 | Loss: 0.00002205
Iteration 56/1000 | Loss: 0.00002203
Iteration 57/1000 | Loss: 0.00002202
Iteration 58/1000 | Loss: 0.00002198
Iteration 59/1000 | Loss: 0.00002182
Iteration 60/1000 | Loss: 0.00002178
Iteration 61/1000 | Loss: 0.00010931
Iteration 62/1000 | Loss: 0.00010931
Iteration 63/1000 | Loss: 0.00002400
Iteration 64/1000 | Loss: 0.00003178
Iteration 65/1000 | Loss: 0.00002162
Iteration 66/1000 | Loss: 0.00002157
Iteration 67/1000 | Loss: 0.00002156
Iteration 68/1000 | Loss: 0.00002156
Iteration 69/1000 | Loss: 0.00002156
Iteration 70/1000 | Loss: 0.00002155
Iteration 71/1000 | Loss: 0.00002154
Iteration 72/1000 | Loss: 0.00002154
Iteration 73/1000 | Loss: 0.00002154
Iteration 74/1000 | Loss: 0.00002154
Iteration 75/1000 | Loss: 0.00002153
Iteration 76/1000 | Loss: 0.00002153
Iteration 77/1000 | Loss: 0.00002153
Iteration 78/1000 | Loss: 0.00002153
Iteration 79/1000 | Loss: 0.00002153
Iteration 80/1000 | Loss: 0.00002153
Iteration 81/1000 | Loss: 0.00002152
Iteration 82/1000 | Loss: 0.00002152
Iteration 83/1000 | Loss: 0.00002152
Iteration 84/1000 | Loss: 0.00002152
Iteration 85/1000 | Loss: 0.00002152
Iteration 86/1000 | Loss: 0.00002152
Iteration 87/1000 | Loss: 0.00002152
Iteration 88/1000 | Loss: 0.00002152
Iteration 89/1000 | Loss: 0.00002152
Iteration 90/1000 | Loss: 0.00002152
Iteration 91/1000 | Loss: 0.00002152
Iteration 92/1000 | Loss: 0.00002151
Iteration 93/1000 | Loss: 0.00002151
Iteration 94/1000 | Loss: 0.00002151
Iteration 95/1000 | Loss: 0.00002151
Iteration 96/1000 | Loss: 0.00002151
Iteration 97/1000 | Loss: 0.00002151
Iteration 98/1000 | Loss: 0.00002151
Iteration 99/1000 | Loss: 0.00002151
Iteration 100/1000 | Loss: 0.00002151
Iteration 101/1000 | Loss: 0.00002151
Iteration 102/1000 | Loss: 0.00002151
Iteration 103/1000 | Loss: 0.00002151
Iteration 104/1000 | Loss: 0.00002151
Iteration 105/1000 | Loss: 0.00002150
Iteration 106/1000 | Loss: 0.00002150
Iteration 107/1000 | Loss: 0.00002150
Iteration 108/1000 | Loss: 0.00002150
Iteration 109/1000 | Loss: 0.00002150
Iteration 110/1000 | Loss: 0.00002150
Iteration 111/1000 | Loss: 0.00002149
Iteration 112/1000 | Loss: 0.00002149
Iteration 113/1000 | Loss: 0.00002149
Iteration 114/1000 | Loss: 0.00002149
Iteration 115/1000 | Loss: 0.00002149
Iteration 116/1000 | Loss: 0.00002149
Iteration 117/1000 | Loss: 0.00002149
Iteration 118/1000 | Loss: 0.00002149
Iteration 119/1000 | Loss: 0.00002149
Iteration 120/1000 | Loss: 0.00002149
Iteration 121/1000 | Loss: 0.00002149
Iteration 122/1000 | Loss: 0.00002149
Iteration 123/1000 | Loss: 0.00002149
Iteration 124/1000 | Loss: 0.00002149
Iteration 125/1000 | Loss: 0.00002149
Iteration 126/1000 | Loss: 0.00002149
Iteration 127/1000 | Loss: 0.00002149
Iteration 128/1000 | Loss: 0.00002148
Iteration 129/1000 | Loss: 0.00002148
Iteration 130/1000 | Loss: 0.00002148
Iteration 131/1000 | Loss: 0.00002148
Iteration 132/1000 | Loss: 0.00002148
Iteration 133/1000 | Loss: 0.00002148
Iteration 134/1000 | Loss: 0.00002148
Iteration 135/1000 | Loss: 0.00002148
Iteration 136/1000 | Loss: 0.00002148
Iteration 137/1000 | Loss: 0.00002148
Iteration 138/1000 | Loss: 0.00002148
Iteration 139/1000 | Loss: 0.00002148
Iteration 140/1000 | Loss: 0.00002148
Iteration 141/1000 | Loss: 0.00002148
Iteration 142/1000 | Loss: 0.00002148
Iteration 143/1000 | Loss: 0.00002148
Iteration 144/1000 | Loss: 0.00002148
Iteration 145/1000 | Loss: 0.00002148
Iteration 146/1000 | Loss: 0.00002148
Iteration 147/1000 | Loss: 0.00002148
Iteration 148/1000 | Loss: 0.00002148
Iteration 149/1000 | Loss: 0.00002148
Iteration 150/1000 | Loss: 0.00002148
Iteration 151/1000 | Loss: 0.00002148
Iteration 152/1000 | Loss: 0.00002148
Iteration 153/1000 | Loss: 0.00002148
Iteration 154/1000 | Loss: 0.00002148
Iteration 155/1000 | Loss: 0.00002148
Iteration 156/1000 | Loss: 0.00002148
Iteration 157/1000 | Loss: 0.00002148
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [2.147701889043674e-05, 2.147701889043674e-05, 2.147701889043674e-05, 2.147701889043674e-05, 2.147701889043674e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.147701889043674e-05

Optimization complete. Final v2v error: 3.8545191287994385 mm

Highest mean error: 4.205031394958496 mm for frame 0

Lowest mean error: 3.713571548461914 mm for frame 129

Saving results

Total time: 127.84877467155457
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00428620
Iteration 2/25 | Loss: 0.00142441
Iteration 3/25 | Loss: 0.00130611
Iteration 4/25 | Loss: 0.00129041
Iteration 5/25 | Loss: 0.00128559
Iteration 6/25 | Loss: 0.00128437
Iteration 7/25 | Loss: 0.00128426
Iteration 8/25 | Loss: 0.00128426
Iteration 9/25 | Loss: 0.00128426
Iteration 10/25 | Loss: 0.00128426
Iteration 11/25 | Loss: 0.00128426
Iteration 12/25 | Loss: 0.00128426
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001284262747503817, 0.001284262747503817, 0.001284262747503817, 0.001284262747503817, 0.001284262747503817]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001284262747503817

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.96333456
Iteration 2/25 | Loss: 0.00083562
Iteration 3/25 | Loss: 0.00083559
Iteration 4/25 | Loss: 0.00083559
Iteration 5/25 | Loss: 0.00083559
Iteration 6/25 | Loss: 0.00083559
Iteration 7/25 | Loss: 0.00083559
Iteration 8/25 | Loss: 0.00083559
Iteration 9/25 | Loss: 0.00083559
Iteration 10/25 | Loss: 0.00083559
Iteration 11/25 | Loss: 0.00083559
Iteration 12/25 | Loss: 0.00083559
Iteration 13/25 | Loss: 0.00083559
Iteration 14/25 | Loss: 0.00083559
Iteration 15/25 | Loss: 0.00083559
Iteration 16/25 | Loss: 0.00083559
Iteration 17/25 | Loss: 0.00083559
Iteration 18/25 | Loss: 0.00083559
Iteration 19/25 | Loss: 0.00083559
Iteration 20/25 | Loss: 0.00083559
Iteration 21/25 | Loss: 0.00083559
Iteration 22/25 | Loss: 0.00083559
Iteration 23/25 | Loss: 0.00083559
Iteration 24/25 | Loss: 0.00083559
Iteration 25/25 | Loss: 0.00083559

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083559
Iteration 2/1000 | Loss: 0.00004189
Iteration 3/1000 | Loss: 0.00002810
Iteration 4/1000 | Loss: 0.00002604
Iteration 5/1000 | Loss: 0.00002490
Iteration 6/1000 | Loss: 0.00002411
Iteration 7/1000 | Loss: 0.00002353
Iteration 8/1000 | Loss: 0.00002304
Iteration 9/1000 | Loss: 0.00002272
Iteration 10/1000 | Loss: 0.00002248
Iteration 11/1000 | Loss: 0.00002226
Iteration 12/1000 | Loss: 0.00002216
Iteration 13/1000 | Loss: 0.00002211
Iteration 14/1000 | Loss: 0.00002204
Iteration 15/1000 | Loss: 0.00002199
Iteration 16/1000 | Loss: 0.00002195
Iteration 17/1000 | Loss: 0.00002190
Iteration 18/1000 | Loss: 0.00002184
Iteration 19/1000 | Loss: 0.00002184
Iteration 20/1000 | Loss: 0.00002183
Iteration 21/1000 | Loss: 0.00002182
Iteration 22/1000 | Loss: 0.00002182
Iteration 23/1000 | Loss: 0.00002179
Iteration 24/1000 | Loss: 0.00002179
Iteration 25/1000 | Loss: 0.00002179
Iteration 26/1000 | Loss: 0.00002179
Iteration 27/1000 | Loss: 0.00002179
Iteration 28/1000 | Loss: 0.00002179
Iteration 29/1000 | Loss: 0.00002178
Iteration 30/1000 | Loss: 0.00002178
Iteration 31/1000 | Loss: 0.00002178
Iteration 32/1000 | Loss: 0.00002178
Iteration 33/1000 | Loss: 0.00002178
Iteration 34/1000 | Loss: 0.00002178
Iteration 35/1000 | Loss: 0.00002177
Iteration 36/1000 | Loss: 0.00002177
Iteration 37/1000 | Loss: 0.00002177
Iteration 38/1000 | Loss: 0.00002177
Iteration 39/1000 | Loss: 0.00002177
Iteration 40/1000 | Loss: 0.00002177
Iteration 41/1000 | Loss: 0.00002177
Iteration 42/1000 | Loss: 0.00002177
Iteration 43/1000 | Loss: 0.00002177
Iteration 44/1000 | Loss: 0.00002175
Iteration 45/1000 | Loss: 0.00002175
Iteration 46/1000 | Loss: 0.00002175
Iteration 47/1000 | Loss: 0.00002174
Iteration 48/1000 | Loss: 0.00002174
Iteration 49/1000 | Loss: 0.00002174
Iteration 50/1000 | Loss: 0.00002173
Iteration 51/1000 | Loss: 0.00002173
Iteration 52/1000 | Loss: 0.00002173
Iteration 53/1000 | Loss: 0.00002172
Iteration 54/1000 | Loss: 0.00002172
Iteration 55/1000 | Loss: 0.00002172
Iteration 56/1000 | Loss: 0.00002171
Iteration 57/1000 | Loss: 0.00002171
Iteration 58/1000 | Loss: 0.00002171
Iteration 59/1000 | Loss: 0.00002171
Iteration 60/1000 | Loss: 0.00002171
Iteration 61/1000 | Loss: 0.00002171
Iteration 62/1000 | Loss: 0.00002171
Iteration 63/1000 | Loss: 0.00002170
Iteration 64/1000 | Loss: 0.00002170
Iteration 65/1000 | Loss: 0.00002170
Iteration 66/1000 | Loss: 0.00002170
Iteration 67/1000 | Loss: 0.00002169
Iteration 68/1000 | Loss: 0.00002169
Iteration 69/1000 | Loss: 0.00002169
Iteration 70/1000 | Loss: 0.00002168
Iteration 71/1000 | Loss: 0.00002168
Iteration 72/1000 | Loss: 0.00002168
Iteration 73/1000 | Loss: 0.00002168
Iteration 74/1000 | Loss: 0.00002168
Iteration 75/1000 | Loss: 0.00002168
Iteration 76/1000 | Loss: 0.00002167
Iteration 77/1000 | Loss: 0.00002167
Iteration 78/1000 | Loss: 0.00002167
Iteration 79/1000 | Loss: 0.00002167
Iteration 80/1000 | Loss: 0.00002167
Iteration 81/1000 | Loss: 0.00002166
Iteration 82/1000 | Loss: 0.00002166
Iteration 83/1000 | Loss: 0.00002166
Iteration 84/1000 | Loss: 0.00002166
Iteration 85/1000 | Loss: 0.00002166
Iteration 86/1000 | Loss: 0.00002165
Iteration 87/1000 | Loss: 0.00002165
Iteration 88/1000 | Loss: 0.00002165
Iteration 89/1000 | Loss: 0.00002164
Iteration 90/1000 | Loss: 0.00002164
Iteration 91/1000 | Loss: 0.00002164
Iteration 92/1000 | Loss: 0.00002164
Iteration 93/1000 | Loss: 0.00002164
Iteration 94/1000 | Loss: 0.00002163
Iteration 95/1000 | Loss: 0.00002163
Iteration 96/1000 | Loss: 0.00002163
Iteration 97/1000 | Loss: 0.00002163
Iteration 98/1000 | Loss: 0.00002162
Iteration 99/1000 | Loss: 0.00002162
Iteration 100/1000 | Loss: 0.00002162
Iteration 101/1000 | Loss: 0.00002162
Iteration 102/1000 | Loss: 0.00002162
Iteration 103/1000 | Loss: 0.00002161
Iteration 104/1000 | Loss: 0.00002161
Iteration 105/1000 | Loss: 0.00002161
Iteration 106/1000 | Loss: 0.00002161
Iteration 107/1000 | Loss: 0.00002161
Iteration 108/1000 | Loss: 0.00002161
Iteration 109/1000 | Loss: 0.00002161
Iteration 110/1000 | Loss: 0.00002160
Iteration 111/1000 | Loss: 0.00002160
Iteration 112/1000 | Loss: 0.00002160
Iteration 113/1000 | Loss: 0.00002160
Iteration 114/1000 | Loss: 0.00002160
Iteration 115/1000 | Loss: 0.00002159
Iteration 116/1000 | Loss: 0.00002159
Iteration 117/1000 | Loss: 0.00002159
Iteration 118/1000 | Loss: 0.00002159
Iteration 119/1000 | Loss: 0.00002159
Iteration 120/1000 | Loss: 0.00002159
Iteration 121/1000 | Loss: 0.00002159
Iteration 122/1000 | Loss: 0.00002159
Iteration 123/1000 | Loss: 0.00002159
Iteration 124/1000 | Loss: 0.00002159
Iteration 125/1000 | Loss: 0.00002159
Iteration 126/1000 | Loss: 0.00002158
Iteration 127/1000 | Loss: 0.00002158
Iteration 128/1000 | Loss: 0.00002158
Iteration 129/1000 | Loss: 0.00002158
Iteration 130/1000 | Loss: 0.00002158
Iteration 131/1000 | Loss: 0.00002158
Iteration 132/1000 | Loss: 0.00002158
Iteration 133/1000 | Loss: 0.00002158
Iteration 134/1000 | Loss: 0.00002158
Iteration 135/1000 | Loss: 0.00002158
Iteration 136/1000 | Loss: 0.00002158
Iteration 137/1000 | Loss: 0.00002158
Iteration 138/1000 | Loss: 0.00002158
Iteration 139/1000 | Loss: 0.00002158
Iteration 140/1000 | Loss: 0.00002158
Iteration 141/1000 | Loss: 0.00002158
Iteration 142/1000 | Loss: 0.00002157
Iteration 143/1000 | Loss: 0.00002157
Iteration 144/1000 | Loss: 0.00002157
Iteration 145/1000 | Loss: 0.00002157
Iteration 146/1000 | Loss: 0.00002157
Iteration 147/1000 | Loss: 0.00002157
Iteration 148/1000 | Loss: 0.00002157
Iteration 149/1000 | Loss: 0.00002157
Iteration 150/1000 | Loss: 0.00002157
Iteration 151/1000 | Loss: 0.00002157
Iteration 152/1000 | Loss: 0.00002157
Iteration 153/1000 | Loss: 0.00002157
Iteration 154/1000 | Loss: 0.00002157
Iteration 155/1000 | Loss: 0.00002157
Iteration 156/1000 | Loss: 0.00002157
Iteration 157/1000 | Loss: 0.00002157
Iteration 158/1000 | Loss: 0.00002157
Iteration 159/1000 | Loss: 0.00002157
Iteration 160/1000 | Loss: 0.00002157
Iteration 161/1000 | Loss: 0.00002157
Iteration 162/1000 | Loss: 0.00002157
Iteration 163/1000 | Loss: 0.00002157
Iteration 164/1000 | Loss: 0.00002157
Iteration 165/1000 | Loss: 0.00002157
Iteration 166/1000 | Loss: 0.00002157
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [2.1569181626546197e-05, 2.1569181626546197e-05, 2.1569181626546197e-05, 2.1569181626546197e-05, 2.1569181626546197e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1569181626546197e-05

Optimization complete. Final v2v error: 3.918095111846924 mm

Highest mean error: 4.6016130447387695 mm for frame 49

Lowest mean error: 3.4853017330169678 mm for frame 0

Saving results

Total time: 39.40363430976868
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00840917
Iteration 2/25 | Loss: 0.00178101
Iteration 3/25 | Loss: 0.00144043
Iteration 4/25 | Loss: 0.00138717
Iteration 5/25 | Loss: 0.00135528
Iteration 6/25 | Loss: 0.00133600
Iteration 7/25 | Loss: 0.00133675
Iteration 8/25 | Loss: 0.00133183
Iteration 9/25 | Loss: 0.00131182
Iteration 10/25 | Loss: 0.00129479
Iteration 11/25 | Loss: 0.00129733
Iteration 12/25 | Loss: 0.00128870
Iteration 13/25 | Loss: 0.00128766
Iteration 14/25 | Loss: 0.00128687
Iteration 15/25 | Loss: 0.00129028
Iteration 16/25 | Loss: 0.00128348
Iteration 17/25 | Loss: 0.00128177
Iteration 18/25 | Loss: 0.00128887
Iteration 19/25 | Loss: 0.00127826
Iteration 20/25 | Loss: 0.00127400
Iteration 21/25 | Loss: 0.00126944
Iteration 22/25 | Loss: 0.00126860
Iteration 23/25 | Loss: 0.00126840
Iteration 24/25 | Loss: 0.00126840
Iteration 25/25 | Loss: 0.00126840

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.98735565
Iteration 2/25 | Loss: 0.00062272
Iteration 3/25 | Loss: 0.00062271
Iteration 4/25 | Loss: 0.00062271
Iteration 5/25 | Loss: 0.00062271
Iteration 6/25 | Loss: 0.00062271
Iteration 7/25 | Loss: 0.00062271
Iteration 8/25 | Loss: 0.00062271
Iteration 9/25 | Loss: 0.00062271
Iteration 10/25 | Loss: 0.00062271
Iteration 11/25 | Loss: 0.00062271
Iteration 12/25 | Loss: 0.00062271
Iteration 13/25 | Loss: 0.00062271
Iteration 14/25 | Loss: 0.00062271
Iteration 15/25 | Loss: 0.00062271
Iteration 16/25 | Loss: 0.00062271
Iteration 17/25 | Loss: 0.00062271
Iteration 18/25 | Loss: 0.00062271
Iteration 19/25 | Loss: 0.00062271
Iteration 20/25 | Loss: 0.00062271
Iteration 21/25 | Loss: 0.00062271
Iteration 22/25 | Loss: 0.00062271
Iteration 23/25 | Loss: 0.00062271
Iteration 24/25 | Loss: 0.00062271
Iteration 25/25 | Loss: 0.00062271

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062271
Iteration 2/1000 | Loss: 0.00005282
Iteration 3/1000 | Loss: 0.00004079
Iteration 4/1000 | Loss: 0.00003590
Iteration 5/1000 | Loss: 0.00003386
Iteration 6/1000 | Loss: 0.00003211
Iteration 7/1000 | Loss: 0.00003082
Iteration 8/1000 | Loss: 0.00003016
Iteration 9/1000 | Loss: 0.00002948
Iteration 10/1000 | Loss: 0.00002883
Iteration 11/1000 | Loss: 0.00002831
Iteration 12/1000 | Loss: 0.00130287
Iteration 13/1000 | Loss: 0.00012034
Iteration 14/1000 | Loss: 0.00005593
Iteration 15/1000 | Loss: 0.00003316
Iteration 16/1000 | Loss: 0.00002894
Iteration 17/1000 | Loss: 0.00002624
Iteration 18/1000 | Loss: 0.00002411
Iteration 19/1000 | Loss: 0.00002251
Iteration 20/1000 | Loss: 0.00002155
Iteration 21/1000 | Loss: 0.00002106
Iteration 22/1000 | Loss: 0.00002092
Iteration 23/1000 | Loss: 0.00002083
Iteration 24/1000 | Loss: 0.00002083
Iteration 25/1000 | Loss: 0.00002080
Iteration 26/1000 | Loss: 0.00002080
Iteration 27/1000 | Loss: 0.00002080
Iteration 28/1000 | Loss: 0.00002078
Iteration 29/1000 | Loss: 0.00002075
Iteration 30/1000 | Loss: 0.00002073
Iteration 31/1000 | Loss: 0.00002073
Iteration 32/1000 | Loss: 0.00002073
Iteration 33/1000 | Loss: 0.00002073
Iteration 34/1000 | Loss: 0.00002073
Iteration 35/1000 | Loss: 0.00002073
Iteration 36/1000 | Loss: 0.00002073
Iteration 37/1000 | Loss: 0.00002073
Iteration 38/1000 | Loss: 0.00002072
Iteration 39/1000 | Loss: 0.00002072
Iteration 40/1000 | Loss: 0.00002072
Iteration 41/1000 | Loss: 0.00002072
Iteration 42/1000 | Loss: 0.00002071
Iteration 43/1000 | Loss: 0.00002071
Iteration 44/1000 | Loss: 0.00002071
Iteration 45/1000 | Loss: 0.00002071
Iteration 46/1000 | Loss: 0.00002070
Iteration 47/1000 | Loss: 0.00002070
Iteration 48/1000 | Loss: 0.00002070
Iteration 49/1000 | Loss: 0.00002069
Iteration 50/1000 | Loss: 0.00002069
Iteration 51/1000 | Loss: 0.00002069
Iteration 52/1000 | Loss: 0.00002069
Iteration 53/1000 | Loss: 0.00002069
Iteration 54/1000 | Loss: 0.00002069
Iteration 55/1000 | Loss: 0.00002068
Iteration 56/1000 | Loss: 0.00002067
Iteration 57/1000 | Loss: 0.00002066
Iteration 58/1000 | Loss: 0.00002065
Iteration 59/1000 | Loss: 0.00002064
Iteration 60/1000 | Loss: 0.00002064
Iteration 61/1000 | Loss: 0.00002064
Iteration 62/1000 | Loss: 0.00002064
Iteration 63/1000 | Loss: 0.00002064
Iteration 64/1000 | Loss: 0.00002064
Iteration 65/1000 | Loss: 0.00002064
Iteration 66/1000 | Loss: 0.00002064
Iteration 67/1000 | Loss: 0.00002064
Iteration 68/1000 | Loss: 0.00002064
Iteration 69/1000 | Loss: 0.00002064
Iteration 70/1000 | Loss: 0.00002063
Iteration 71/1000 | Loss: 0.00002063
Iteration 72/1000 | Loss: 0.00002063
Iteration 73/1000 | Loss: 0.00002063
Iteration 74/1000 | Loss: 0.00002063
Iteration 75/1000 | Loss: 0.00002063
Iteration 76/1000 | Loss: 0.00002062
Iteration 77/1000 | Loss: 0.00002062
Iteration 78/1000 | Loss: 0.00002061
Iteration 79/1000 | Loss: 0.00002061
Iteration 80/1000 | Loss: 0.00002061
Iteration 81/1000 | Loss: 0.00002061
Iteration 82/1000 | Loss: 0.00002061
Iteration 83/1000 | Loss: 0.00002061
Iteration 84/1000 | Loss: 0.00002061
Iteration 85/1000 | Loss: 0.00002060
Iteration 86/1000 | Loss: 0.00002060
Iteration 87/1000 | Loss: 0.00002060
Iteration 88/1000 | Loss: 0.00002060
Iteration 89/1000 | Loss: 0.00002060
Iteration 90/1000 | Loss: 0.00002060
Iteration 91/1000 | Loss: 0.00002060
Iteration 92/1000 | Loss: 0.00002060
Iteration 93/1000 | Loss: 0.00002059
Iteration 94/1000 | Loss: 0.00002059
Iteration 95/1000 | Loss: 0.00002059
Iteration 96/1000 | Loss: 0.00002059
Iteration 97/1000 | Loss: 0.00002058
Iteration 98/1000 | Loss: 0.00002058
Iteration 99/1000 | Loss: 0.00002058
Iteration 100/1000 | Loss: 0.00002058
Iteration 101/1000 | Loss: 0.00002058
Iteration 102/1000 | Loss: 0.00002058
Iteration 103/1000 | Loss: 0.00002058
Iteration 104/1000 | Loss: 0.00002058
Iteration 105/1000 | Loss: 0.00002058
Iteration 106/1000 | Loss: 0.00002058
Iteration 107/1000 | Loss: 0.00002058
Iteration 108/1000 | Loss: 0.00002058
Iteration 109/1000 | Loss: 0.00002057
Iteration 110/1000 | Loss: 0.00002057
Iteration 111/1000 | Loss: 0.00002057
Iteration 112/1000 | Loss: 0.00002057
Iteration 113/1000 | Loss: 0.00002057
Iteration 114/1000 | Loss: 0.00002056
Iteration 115/1000 | Loss: 0.00002056
Iteration 116/1000 | Loss: 0.00002056
Iteration 117/1000 | Loss: 0.00002056
Iteration 118/1000 | Loss: 0.00002056
Iteration 119/1000 | Loss: 0.00002056
Iteration 120/1000 | Loss: 0.00002056
Iteration 121/1000 | Loss: 0.00002056
Iteration 122/1000 | Loss: 0.00002056
Iteration 123/1000 | Loss: 0.00002055
Iteration 124/1000 | Loss: 0.00002055
Iteration 125/1000 | Loss: 0.00002055
Iteration 126/1000 | Loss: 0.00002055
Iteration 127/1000 | Loss: 0.00002055
Iteration 128/1000 | Loss: 0.00002055
Iteration 129/1000 | Loss: 0.00002055
Iteration 130/1000 | Loss: 0.00002055
Iteration 131/1000 | Loss: 0.00002055
Iteration 132/1000 | Loss: 0.00002054
Iteration 133/1000 | Loss: 0.00002054
Iteration 134/1000 | Loss: 0.00002054
Iteration 135/1000 | Loss: 0.00002054
Iteration 136/1000 | Loss: 0.00002054
Iteration 137/1000 | Loss: 0.00002054
Iteration 138/1000 | Loss: 0.00002054
Iteration 139/1000 | Loss: 0.00002054
Iteration 140/1000 | Loss: 0.00002054
Iteration 141/1000 | Loss: 0.00002054
Iteration 142/1000 | Loss: 0.00002054
Iteration 143/1000 | Loss: 0.00002054
Iteration 144/1000 | Loss: 0.00002054
Iteration 145/1000 | Loss: 0.00002054
Iteration 146/1000 | Loss: 0.00002054
Iteration 147/1000 | Loss: 0.00002054
Iteration 148/1000 | Loss: 0.00002054
Iteration 149/1000 | Loss: 0.00002054
Iteration 150/1000 | Loss: 0.00002054
Iteration 151/1000 | Loss: 0.00002054
Iteration 152/1000 | Loss: 0.00002054
Iteration 153/1000 | Loss: 0.00002054
Iteration 154/1000 | Loss: 0.00002054
Iteration 155/1000 | Loss: 0.00002054
Iteration 156/1000 | Loss: 0.00002054
Iteration 157/1000 | Loss: 0.00002054
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [2.0544053768389858e-05, 2.0544053768389858e-05, 2.0544053768389858e-05, 2.0544053768389858e-05, 2.0544053768389858e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0544053768389858e-05

Optimization complete. Final v2v error: 3.7913870811462402 mm

Highest mean error: 3.9575695991516113 mm for frame 26

Lowest mean error: 3.6491200923919678 mm for frame 1

Saving results

Total time: 80.5504539012909
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00804774
Iteration 2/25 | Loss: 0.00164624
Iteration 3/25 | Loss: 0.00137941
Iteration 4/25 | Loss: 0.00134989
Iteration 5/25 | Loss: 0.00134524
Iteration 6/25 | Loss: 0.00134382
Iteration 7/25 | Loss: 0.00134368
Iteration 8/25 | Loss: 0.00134368
Iteration 9/25 | Loss: 0.00134368
Iteration 10/25 | Loss: 0.00134368
Iteration 11/25 | Loss: 0.00134368
Iteration 12/25 | Loss: 0.00134368
Iteration 13/25 | Loss: 0.00134368
Iteration 14/25 | Loss: 0.00134368
Iteration 15/25 | Loss: 0.00134369
Iteration 16/25 | Loss: 0.00134368
Iteration 17/25 | Loss: 0.00134368
Iteration 18/25 | Loss: 0.00134368
Iteration 19/25 | Loss: 0.00134368
Iteration 20/25 | Loss: 0.00134369
Iteration 21/25 | Loss: 0.00134368
Iteration 22/25 | Loss: 0.00134368
Iteration 23/25 | Loss: 0.00134368
Iteration 24/25 | Loss: 0.00134368
Iteration 25/25 | Loss: 0.00134368

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48314464
Iteration 2/25 | Loss: 0.00075895
Iteration 3/25 | Loss: 0.00075895
Iteration 4/25 | Loss: 0.00075895
Iteration 5/25 | Loss: 0.00075895
Iteration 6/25 | Loss: 0.00075895
Iteration 7/25 | Loss: 0.00075895
Iteration 8/25 | Loss: 0.00075895
Iteration 9/25 | Loss: 0.00075895
Iteration 10/25 | Loss: 0.00075895
Iteration 11/25 | Loss: 0.00075895
Iteration 12/25 | Loss: 0.00075895
Iteration 13/25 | Loss: 0.00075895
Iteration 14/25 | Loss: 0.00075895
Iteration 15/25 | Loss: 0.00075895
Iteration 16/25 | Loss: 0.00075895
Iteration 17/25 | Loss: 0.00075895
Iteration 18/25 | Loss: 0.00075895
Iteration 19/25 | Loss: 0.00075895
Iteration 20/25 | Loss: 0.00075895
Iteration 21/25 | Loss: 0.00075895
Iteration 22/25 | Loss: 0.00075895
Iteration 23/25 | Loss: 0.00075895
Iteration 24/25 | Loss: 0.00075895
Iteration 25/25 | Loss: 0.00075895

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075895
Iteration 2/1000 | Loss: 0.00006045
Iteration 3/1000 | Loss: 0.00004222
Iteration 4/1000 | Loss: 0.00003553
Iteration 5/1000 | Loss: 0.00003353
Iteration 6/1000 | Loss: 0.00003209
Iteration 7/1000 | Loss: 0.00003128
Iteration 8/1000 | Loss: 0.00003039
Iteration 9/1000 | Loss: 0.00002983
Iteration 10/1000 | Loss: 0.00002937
Iteration 11/1000 | Loss: 0.00002909
Iteration 12/1000 | Loss: 0.00002886
Iteration 13/1000 | Loss: 0.00002884
Iteration 14/1000 | Loss: 0.00002867
Iteration 15/1000 | Loss: 0.00002862
Iteration 16/1000 | Loss: 0.00002859
Iteration 17/1000 | Loss: 0.00002850
Iteration 18/1000 | Loss: 0.00002849
Iteration 19/1000 | Loss: 0.00002846
Iteration 20/1000 | Loss: 0.00002843
Iteration 21/1000 | Loss: 0.00002837
Iteration 22/1000 | Loss: 0.00002837
Iteration 23/1000 | Loss: 0.00002832
Iteration 24/1000 | Loss: 0.00002832
Iteration 25/1000 | Loss: 0.00002832
Iteration 26/1000 | Loss: 0.00002829
Iteration 27/1000 | Loss: 0.00002828
Iteration 28/1000 | Loss: 0.00002828
Iteration 29/1000 | Loss: 0.00002828
Iteration 30/1000 | Loss: 0.00002828
Iteration 31/1000 | Loss: 0.00002828
Iteration 32/1000 | Loss: 0.00002828
Iteration 33/1000 | Loss: 0.00002828
Iteration 34/1000 | Loss: 0.00002828
Iteration 35/1000 | Loss: 0.00002828
Iteration 36/1000 | Loss: 0.00002826
Iteration 37/1000 | Loss: 0.00002826
Iteration 38/1000 | Loss: 0.00002826
Iteration 39/1000 | Loss: 0.00002826
Iteration 40/1000 | Loss: 0.00002825
Iteration 41/1000 | Loss: 0.00002825
Iteration 42/1000 | Loss: 0.00002825
Iteration 43/1000 | Loss: 0.00002825
Iteration 44/1000 | Loss: 0.00002824
Iteration 45/1000 | Loss: 0.00002824
Iteration 46/1000 | Loss: 0.00002824
Iteration 47/1000 | Loss: 0.00002824
Iteration 48/1000 | Loss: 0.00002824
Iteration 49/1000 | Loss: 0.00002824
Iteration 50/1000 | Loss: 0.00002823
Iteration 51/1000 | Loss: 0.00002823
Iteration 52/1000 | Loss: 0.00002823
Iteration 53/1000 | Loss: 0.00002823
Iteration 54/1000 | Loss: 0.00002822
Iteration 55/1000 | Loss: 0.00002822
Iteration 56/1000 | Loss: 0.00002822
Iteration 57/1000 | Loss: 0.00002822
Iteration 58/1000 | Loss: 0.00002822
Iteration 59/1000 | Loss: 0.00002822
Iteration 60/1000 | Loss: 0.00002822
Iteration 61/1000 | Loss: 0.00002821
Iteration 62/1000 | Loss: 0.00002821
Iteration 63/1000 | Loss: 0.00002820
Iteration 64/1000 | Loss: 0.00002820
Iteration 65/1000 | Loss: 0.00002820
Iteration 66/1000 | Loss: 0.00002820
Iteration 67/1000 | Loss: 0.00002820
Iteration 68/1000 | Loss: 0.00002820
Iteration 69/1000 | Loss: 0.00002819
Iteration 70/1000 | Loss: 0.00002819
Iteration 71/1000 | Loss: 0.00002819
Iteration 72/1000 | Loss: 0.00002818
Iteration 73/1000 | Loss: 0.00002818
Iteration 74/1000 | Loss: 0.00002818
Iteration 75/1000 | Loss: 0.00002818
Iteration 76/1000 | Loss: 0.00002818
Iteration 77/1000 | Loss: 0.00002818
Iteration 78/1000 | Loss: 0.00002817
Iteration 79/1000 | Loss: 0.00002817
Iteration 80/1000 | Loss: 0.00002817
Iteration 81/1000 | Loss: 0.00002817
Iteration 82/1000 | Loss: 0.00002817
Iteration 83/1000 | Loss: 0.00002816
Iteration 84/1000 | Loss: 0.00002816
Iteration 85/1000 | Loss: 0.00002816
Iteration 86/1000 | Loss: 0.00002816
Iteration 87/1000 | Loss: 0.00002816
Iteration 88/1000 | Loss: 0.00002816
Iteration 89/1000 | Loss: 0.00002816
Iteration 90/1000 | Loss: 0.00002815
Iteration 91/1000 | Loss: 0.00002815
Iteration 92/1000 | Loss: 0.00002815
Iteration 93/1000 | Loss: 0.00002815
Iteration 94/1000 | Loss: 0.00002815
Iteration 95/1000 | Loss: 0.00002815
Iteration 96/1000 | Loss: 0.00002815
Iteration 97/1000 | Loss: 0.00002815
Iteration 98/1000 | Loss: 0.00002815
Iteration 99/1000 | Loss: 0.00002815
Iteration 100/1000 | Loss: 0.00002815
Iteration 101/1000 | Loss: 0.00002814
Iteration 102/1000 | Loss: 0.00002814
Iteration 103/1000 | Loss: 0.00002814
Iteration 104/1000 | Loss: 0.00002814
Iteration 105/1000 | Loss: 0.00002814
Iteration 106/1000 | Loss: 0.00002814
Iteration 107/1000 | Loss: 0.00002814
Iteration 108/1000 | Loss: 0.00002814
Iteration 109/1000 | Loss: 0.00002814
Iteration 110/1000 | Loss: 0.00002814
Iteration 111/1000 | Loss: 0.00002814
Iteration 112/1000 | Loss: 0.00002814
Iteration 113/1000 | Loss: 0.00002814
Iteration 114/1000 | Loss: 0.00002814
Iteration 115/1000 | Loss: 0.00002814
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [2.814388608385343e-05, 2.814388608385343e-05, 2.814388608385343e-05, 2.814388608385343e-05, 2.814388608385343e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.814388608385343e-05

Optimization complete. Final v2v error: 4.436488628387451 mm

Highest mean error: 5.178224086761475 mm for frame 36

Lowest mean error: 3.9766557216644287 mm for frame 46

Saving results

Total time: 37.88110041618347
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00650072
Iteration 2/25 | Loss: 0.00166441
Iteration 3/25 | Loss: 0.00146609
Iteration 4/25 | Loss: 0.00144298
Iteration 5/25 | Loss: 0.00143956
Iteration 6/25 | Loss: 0.00143956
Iteration 7/25 | Loss: 0.00143956
Iteration 8/25 | Loss: 0.00143956
Iteration 9/25 | Loss: 0.00143956
Iteration 10/25 | Loss: 0.00143956
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001439563580788672, 0.001439563580788672, 0.001439563580788672, 0.001439563580788672, 0.001439563580788672]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001439563580788672

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50235713
Iteration 2/25 | Loss: 0.00091481
Iteration 3/25 | Loss: 0.00091478
Iteration 4/25 | Loss: 0.00091478
Iteration 5/25 | Loss: 0.00091478
Iteration 6/25 | Loss: 0.00091478
Iteration 7/25 | Loss: 0.00091478
Iteration 8/25 | Loss: 0.00091478
Iteration 9/25 | Loss: 0.00091478
Iteration 10/25 | Loss: 0.00091478
Iteration 11/25 | Loss: 0.00091478
Iteration 12/25 | Loss: 0.00091478
Iteration 13/25 | Loss: 0.00091478
Iteration 14/25 | Loss: 0.00091478
Iteration 15/25 | Loss: 0.00091478
Iteration 16/25 | Loss: 0.00091478
Iteration 17/25 | Loss: 0.00091478
Iteration 18/25 | Loss: 0.00091478
Iteration 19/25 | Loss: 0.00091478
Iteration 20/25 | Loss: 0.00091478
Iteration 21/25 | Loss: 0.00091478
Iteration 22/25 | Loss: 0.00091478
Iteration 23/25 | Loss: 0.00091478
Iteration 24/25 | Loss: 0.00091478
Iteration 25/25 | Loss: 0.00091478

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091478
Iteration 2/1000 | Loss: 0.00007103
Iteration 3/1000 | Loss: 0.00004296
Iteration 4/1000 | Loss: 0.00003643
Iteration 5/1000 | Loss: 0.00003382
Iteration 6/1000 | Loss: 0.00003246
Iteration 7/1000 | Loss: 0.00003142
Iteration 8/1000 | Loss: 0.00003096
Iteration 9/1000 | Loss: 0.00003045
Iteration 10/1000 | Loss: 0.00003008
Iteration 11/1000 | Loss: 0.00002973
Iteration 12/1000 | Loss: 0.00002956
Iteration 13/1000 | Loss: 0.00002937
Iteration 14/1000 | Loss: 0.00002931
Iteration 15/1000 | Loss: 0.00002929
Iteration 16/1000 | Loss: 0.00002924
Iteration 17/1000 | Loss: 0.00002921
Iteration 18/1000 | Loss: 0.00002921
Iteration 19/1000 | Loss: 0.00002920
Iteration 20/1000 | Loss: 0.00002918
Iteration 21/1000 | Loss: 0.00002918
Iteration 22/1000 | Loss: 0.00002918
Iteration 23/1000 | Loss: 0.00002918
Iteration 24/1000 | Loss: 0.00002918
Iteration 25/1000 | Loss: 0.00002918
Iteration 26/1000 | Loss: 0.00002917
Iteration 27/1000 | Loss: 0.00002917
Iteration 28/1000 | Loss: 0.00002917
Iteration 29/1000 | Loss: 0.00002917
Iteration 30/1000 | Loss: 0.00002917
Iteration 31/1000 | Loss: 0.00002917
Iteration 32/1000 | Loss: 0.00002917
Iteration 33/1000 | Loss: 0.00002917
Iteration 34/1000 | Loss: 0.00002916
Iteration 35/1000 | Loss: 0.00002915
Iteration 36/1000 | Loss: 0.00002914
Iteration 37/1000 | Loss: 0.00002914
Iteration 38/1000 | Loss: 0.00002914
Iteration 39/1000 | Loss: 0.00002914
Iteration 40/1000 | Loss: 0.00002913
Iteration 41/1000 | Loss: 0.00002913
Iteration 42/1000 | Loss: 0.00002913
Iteration 43/1000 | Loss: 0.00002913
Iteration 44/1000 | Loss: 0.00002912
Iteration 45/1000 | Loss: 0.00002912
Iteration 46/1000 | Loss: 0.00002912
Iteration 47/1000 | Loss: 0.00002911
Iteration 48/1000 | Loss: 0.00002911
Iteration 49/1000 | Loss: 0.00002911
Iteration 50/1000 | Loss: 0.00002911
Iteration 51/1000 | Loss: 0.00002911
Iteration 52/1000 | Loss: 0.00002911
Iteration 53/1000 | Loss: 0.00002910
Iteration 54/1000 | Loss: 0.00002910
Iteration 55/1000 | Loss: 0.00002910
Iteration 56/1000 | Loss: 0.00002909
Iteration 57/1000 | Loss: 0.00002909
Iteration 58/1000 | Loss: 0.00002909
Iteration 59/1000 | Loss: 0.00002908
Iteration 60/1000 | Loss: 0.00002908
Iteration 61/1000 | Loss: 0.00002908
Iteration 62/1000 | Loss: 0.00002907
Iteration 63/1000 | Loss: 0.00002906
Iteration 64/1000 | Loss: 0.00002906
Iteration 65/1000 | Loss: 0.00002906
Iteration 66/1000 | Loss: 0.00002906
Iteration 67/1000 | Loss: 0.00002906
Iteration 68/1000 | Loss: 0.00002905
Iteration 69/1000 | Loss: 0.00002905
Iteration 70/1000 | Loss: 0.00002905
Iteration 71/1000 | Loss: 0.00002905
Iteration 72/1000 | Loss: 0.00002905
Iteration 73/1000 | Loss: 0.00002905
Iteration 74/1000 | Loss: 0.00002905
Iteration 75/1000 | Loss: 0.00002905
Iteration 76/1000 | Loss: 0.00002904
Iteration 77/1000 | Loss: 0.00002904
Iteration 78/1000 | Loss: 0.00002904
Iteration 79/1000 | Loss: 0.00002904
Iteration 80/1000 | Loss: 0.00002904
Iteration 81/1000 | Loss: 0.00002904
Iteration 82/1000 | Loss: 0.00002904
Iteration 83/1000 | Loss: 0.00002904
Iteration 84/1000 | Loss: 0.00002904
Iteration 85/1000 | Loss: 0.00002904
Iteration 86/1000 | Loss: 0.00002903
Iteration 87/1000 | Loss: 0.00002903
Iteration 88/1000 | Loss: 0.00002903
Iteration 89/1000 | Loss: 0.00002903
Iteration 90/1000 | Loss: 0.00002903
Iteration 91/1000 | Loss: 0.00002903
Iteration 92/1000 | Loss: 0.00002902
Iteration 93/1000 | Loss: 0.00002902
Iteration 94/1000 | Loss: 0.00002902
Iteration 95/1000 | Loss: 0.00002902
Iteration 96/1000 | Loss: 0.00002902
Iteration 97/1000 | Loss: 0.00002902
Iteration 98/1000 | Loss: 0.00002902
Iteration 99/1000 | Loss: 0.00002902
Iteration 100/1000 | Loss: 0.00002902
Iteration 101/1000 | Loss: 0.00002902
Iteration 102/1000 | Loss: 0.00002902
Iteration 103/1000 | Loss: 0.00002902
Iteration 104/1000 | Loss: 0.00002902
Iteration 105/1000 | Loss: 0.00002902
Iteration 106/1000 | Loss: 0.00002902
Iteration 107/1000 | Loss: 0.00002901
Iteration 108/1000 | Loss: 0.00002901
Iteration 109/1000 | Loss: 0.00002901
Iteration 110/1000 | Loss: 0.00002901
Iteration 111/1000 | Loss: 0.00002901
Iteration 112/1000 | Loss: 0.00002901
Iteration 113/1000 | Loss: 0.00002901
Iteration 114/1000 | Loss: 0.00002901
Iteration 115/1000 | Loss: 0.00002901
Iteration 116/1000 | Loss: 0.00002901
Iteration 117/1000 | Loss: 0.00002901
Iteration 118/1000 | Loss: 0.00002901
Iteration 119/1000 | Loss: 0.00002901
Iteration 120/1000 | Loss: 0.00002901
Iteration 121/1000 | Loss: 0.00002901
Iteration 122/1000 | Loss: 0.00002901
Iteration 123/1000 | Loss: 0.00002901
Iteration 124/1000 | Loss: 0.00002901
Iteration 125/1000 | Loss: 0.00002901
Iteration 126/1000 | Loss: 0.00002901
Iteration 127/1000 | Loss: 0.00002901
Iteration 128/1000 | Loss: 0.00002901
Iteration 129/1000 | Loss: 0.00002901
Iteration 130/1000 | Loss: 0.00002901
Iteration 131/1000 | Loss: 0.00002901
Iteration 132/1000 | Loss: 0.00002901
Iteration 133/1000 | Loss: 0.00002901
Iteration 134/1000 | Loss: 0.00002901
Iteration 135/1000 | Loss: 0.00002901
Iteration 136/1000 | Loss: 0.00002901
Iteration 137/1000 | Loss: 0.00002901
Iteration 138/1000 | Loss: 0.00002901
Iteration 139/1000 | Loss: 0.00002901
Iteration 140/1000 | Loss: 0.00002901
Iteration 141/1000 | Loss: 0.00002901
Iteration 142/1000 | Loss: 0.00002901
Iteration 143/1000 | Loss: 0.00002901
Iteration 144/1000 | Loss: 0.00002901
Iteration 145/1000 | Loss: 0.00002901
Iteration 146/1000 | Loss: 0.00002901
Iteration 147/1000 | Loss: 0.00002901
Iteration 148/1000 | Loss: 0.00002901
Iteration 149/1000 | Loss: 0.00002901
Iteration 150/1000 | Loss: 0.00002901
Iteration 151/1000 | Loss: 0.00002901
Iteration 152/1000 | Loss: 0.00002901
Iteration 153/1000 | Loss: 0.00002901
Iteration 154/1000 | Loss: 0.00002901
Iteration 155/1000 | Loss: 0.00002901
Iteration 156/1000 | Loss: 0.00002901
Iteration 157/1000 | Loss: 0.00002901
Iteration 158/1000 | Loss: 0.00002901
Iteration 159/1000 | Loss: 0.00002901
Iteration 160/1000 | Loss: 0.00002901
Iteration 161/1000 | Loss: 0.00002901
Iteration 162/1000 | Loss: 0.00002901
Iteration 163/1000 | Loss: 0.00002901
Iteration 164/1000 | Loss: 0.00002901
Iteration 165/1000 | Loss: 0.00002901
Iteration 166/1000 | Loss: 0.00002901
Iteration 167/1000 | Loss: 0.00002901
Iteration 168/1000 | Loss: 0.00002901
Iteration 169/1000 | Loss: 0.00002901
Iteration 170/1000 | Loss: 0.00002901
Iteration 171/1000 | Loss: 0.00002901
Iteration 172/1000 | Loss: 0.00002901
Iteration 173/1000 | Loss: 0.00002901
Iteration 174/1000 | Loss: 0.00002901
Iteration 175/1000 | Loss: 0.00002901
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [2.9013386665610597e-05, 2.9013386665610597e-05, 2.9013386665610597e-05, 2.9013386665610597e-05, 2.9013386665610597e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9013386665610597e-05

Optimization complete. Final v2v error: 4.458965301513672 mm

Highest mean error: 4.8801374435424805 mm for frame 63

Lowest mean error: 3.9631285667419434 mm for frame 175

Saving results

Total time: 35.95016956329346
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00407316
Iteration 2/25 | Loss: 0.00127973
Iteration 3/25 | Loss: 0.00121060
Iteration 4/25 | Loss: 0.00119734
Iteration 5/25 | Loss: 0.00119278
Iteration 6/25 | Loss: 0.00119207
Iteration 7/25 | Loss: 0.00119207
Iteration 8/25 | Loss: 0.00119207
Iteration 9/25 | Loss: 0.00119207
Iteration 10/25 | Loss: 0.00119207
Iteration 11/25 | Loss: 0.00119207
Iteration 12/25 | Loss: 0.00119207
Iteration 13/25 | Loss: 0.00119207
Iteration 14/25 | Loss: 0.00119207
Iteration 15/25 | Loss: 0.00119207
Iteration 16/25 | Loss: 0.00119207
Iteration 17/25 | Loss: 0.00119207
Iteration 18/25 | Loss: 0.00119207
Iteration 19/25 | Loss: 0.00119207
Iteration 20/25 | Loss: 0.00119207
Iteration 21/25 | Loss: 0.00119207
Iteration 22/25 | Loss: 0.00119207
Iteration 23/25 | Loss: 0.00119207
Iteration 24/25 | Loss: 0.00119207
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0011920746183022857, 0.0011920746183022857, 0.0011920746183022857, 0.0011920746183022857, 0.0011920746183022857]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011920746183022857

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.87865889
Iteration 2/25 | Loss: 0.00071836
Iteration 3/25 | Loss: 0.00071836
Iteration 4/25 | Loss: 0.00071836
Iteration 5/25 | Loss: 0.00071836
Iteration 6/25 | Loss: 0.00071836
Iteration 7/25 | Loss: 0.00071836
Iteration 8/25 | Loss: 0.00071836
Iteration 9/25 | Loss: 0.00071836
Iteration 10/25 | Loss: 0.00071836
Iteration 11/25 | Loss: 0.00071836
Iteration 12/25 | Loss: 0.00071836
Iteration 13/25 | Loss: 0.00071836
Iteration 14/25 | Loss: 0.00071836
Iteration 15/25 | Loss: 0.00071836
Iteration 16/25 | Loss: 0.00071836
Iteration 17/25 | Loss: 0.00071836
Iteration 18/25 | Loss: 0.00071836
Iteration 19/25 | Loss: 0.00071836
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007183551788330078, 0.0007183551788330078, 0.0007183551788330078, 0.0007183551788330078, 0.0007183551788330078]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007183551788330078

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071836
Iteration 2/1000 | Loss: 0.00003218
Iteration 3/1000 | Loss: 0.00002032
Iteration 4/1000 | Loss: 0.00001771
Iteration 5/1000 | Loss: 0.00001645
Iteration 6/1000 | Loss: 0.00001572
Iteration 7/1000 | Loss: 0.00001526
Iteration 8/1000 | Loss: 0.00001485
Iteration 9/1000 | Loss: 0.00001465
Iteration 10/1000 | Loss: 0.00001450
Iteration 11/1000 | Loss: 0.00001429
Iteration 12/1000 | Loss: 0.00001421
Iteration 13/1000 | Loss: 0.00001418
Iteration 14/1000 | Loss: 0.00001415
Iteration 15/1000 | Loss: 0.00001413
Iteration 16/1000 | Loss: 0.00001410
Iteration 17/1000 | Loss: 0.00001408
Iteration 18/1000 | Loss: 0.00001405
Iteration 19/1000 | Loss: 0.00001398
Iteration 20/1000 | Loss: 0.00001394
Iteration 21/1000 | Loss: 0.00001394
Iteration 22/1000 | Loss: 0.00001391
Iteration 23/1000 | Loss: 0.00001390
Iteration 24/1000 | Loss: 0.00001388
Iteration 25/1000 | Loss: 0.00001388
Iteration 26/1000 | Loss: 0.00001387
Iteration 27/1000 | Loss: 0.00001386
Iteration 28/1000 | Loss: 0.00001382
Iteration 29/1000 | Loss: 0.00001382
Iteration 30/1000 | Loss: 0.00001380
Iteration 31/1000 | Loss: 0.00001379
Iteration 32/1000 | Loss: 0.00001378
Iteration 33/1000 | Loss: 0.00001377
Iteration 34/1000 | Loss: 0.00001377
Iteration 35/1000 | Loss: 0.00001376
Iteration 36/1000 | Loss: 0.00001376
Iteration 37/1000 | Loss: 0.00001376
Iteration 38/1000 | Loss: 0.00001375
Iteration 39/1000 | Loss: 0.00001375
Iteration 40/1000 | Loss: 0.00001374
Iteration 41/1000 | Loss: 0.00001374
Iteration 42/1000 | Loss: 0.00001374
Iteration 43/1000 | Loss: 0.00001373
Iteration 44/1000 | Loss: 0.00001373
Iteration 45/1000 | Loss: 0.00001373
Iteration 46/1000 | Loss: 0.00001373
Iteration 47/1000 | Loss: 0.00001373
Iteration 48/1000 | Loss: 0.00001372
Iteration 49/1000 | Loss: 0.00001372
Iteration 50/1000 | Loss: 0.00001372
Iteration 51/1000 | Loss: 0.00001372
Iteration 52/1000 | Loss: 0.00001372
Iteration 53/1000 | Loss: 0.00001371
Iteration 54/1000 | Loss: 0.00001371
Iteration 55/1000 | Loss: 0.00001371
Iteration 56/1000 | Loss: 0.00001371
Iteration 57/1000 | Loss: 0.00001371
Iteration 58/1000 | Loss: 0.00001370
Iteration 59/1000 | Loss: 0.00001370
Iteration 60/1000 | Loss: 0.00001370
Iteration 61/1000 | Loss: 0.00001370
Iteration 62/1000 | Loss: 0.00001370
Iteration 63/1000 | Loss: 0.00001370
Iteration 64/1000 | Loss: 0.00001369
Iteration 65/1000 | Loss: 0.00001369
Iteration 66/1000 | Loss: 0.00001369
Iteration 67/1000 | Loss: 0.00001368
Iteration 68/1000 | Loss: 0.00001368
Iteration 69/1000 | Loss: 0.00001367
Iteration 70/1000 | Loss: 0.00001367
Iteration 71/1000 | Loss: 0.00001367
Iteration 72/1000 | Loss: 0.00001366
Iteration 73/1000 | Loss: 0.00001366
Iteration 74/1000 | Loss: 0.00001366
Iteration 75/1000 | Loss: 0.00001366
Iteration 76/1000 | Loss: 0.00001366
Iteration 77/1000 | Loss: 0.00001365
Iteration 78/1000 | Loss: 0.00001365
Iteration 79/1000 | Loss: 0.00001364
Iteration 80/1000 | Loss: 0.00001364
Iteration 81/1000 | Loss: 0.00001363
Iteration 82/1000 | Loss: 0.00001363
Iteration 83/1000 | Loss: 0.00001363
Iteration 84/1000 | Loss: 0.00001363
Iteration 85/1000 | Loss: 0.00001362
Iteration 86/1000 | Loss: 0.00001361
Iteration 87/1000 | Loss: 0.00001360
Iteration 88/1000 | Loss: 0.00001359
Iteration 89/1000 | Loss: 0.00001359
Iteration 90/1000 | Loss: 0.00001358
Iteration 91/1000 | Loss: 0.00001358
Iteration 92/1000 | Loss: 0.00001358
Iteration 93/1000 | Loss: 0.00001358
Iteration 94/1000 | Loss: 0.00001356
Iteration 95/1000 | Loss: 0.00001356
Iteration 96/1000 | Loss: 0.00001356
Iteration 97/1000 | Loss: 0.00001355
Iteration 98/1000 | Loss: 0.00001355
Iteration 99/1000 | Loss: 0.00001355
Iteration 100/1000 | Loss: 0.00001354
Iteration 101/1000 | Loss: 0.00001354
Iteration 102/1000 | Loss: 0.00001354
Iteration 103/1000 | Loss: 0.00001354
Iteration 104/1000 | Loss: 0.00001354
Iteration 105/1000 | Loss: 0.00001354
Iteration 106/1000 | Loss: 0.00001354
Iteration 107/1000 | Loss: 0.00001354
Iteration 108/1000 | Loss: 0.00001353
Iteration 109/1000 | Loss: 0.00001353
Iteration 110/1000 | Loss: 0.00001353
Iteration 111/1000 | Loss: 0.00001353
Iteration 112/1000 | Loss: 0.00001353
Iteration 113/1000 | Loss: 0.00001353
Iteration 114/1000 | Loss: 0.00001353
Iteration 115/1000 | Loss: 0.00001353
Iteration 116/1000 | Loss: 0.00001352
Iteration 117/1000 | Loss: 0.00001352
Iteration 118/1000 | Loss: 0.00001352
Iteration 119/1000 | Loss: 0.00001352
Iteration 120/1000 | Loss: 0.00001352
Iteration 121/1000 | Loss: 0.00001352
Iteration 122/1000 | Loss: 0.00001352
Iteration 123/1000 | Loss: 0.00001352
Iteration 124/1000 | Loss: 0.00001352
Iteration 125/1000 | Loss: 0.00001352
Iteration 126/1000 | Loss: 0.00001351
Iteration 127/1000 | Loss: 0.00001351
Iteration 128/1000 | Loss: 0.00001351
Iteration 129/1000 | Loss: 0.00001351
Iteration 130/1000 | Loss: 0.00001351
Iteration 131/1000 | Loss: 0.00001351
Iteration 132/1000 | Loss: 0.00001351
Iteration 133/1000 | Loss: 0.00001351
Iteration 134/1000 | Loss: 0.00001351
Iteration 135/1000 | Loss: 0.00001350
Iteration 136/1000 | Loss: 0.00001350
Iteration 137/1000 | Loss: 0.00001350
Iteration 138/1000 | Loss: 0.00001350
Iteration 139/1000 | Loss: 0.00001350
Iteration 140/1000 | Loss: 0.00001350
Iteration 141/1000 | Loss: 0.00001350
Iteration 142/1000 | Loss: 0.00001350
Iteration 143/1000 | Loss: 0.00001350
Iteration 144/1000 | Loss: 0.00001350
Iteration 145/1000 | Loss: 0.00001350
Iteration 146/1000 | Loss: 0.00001350
Iteration 147/1000 | Loss: 0.00001350
Iteration 148/1000 | Loss: 0.00001350
Iteration 149/1000 | Loss: 0.00001350
Iteration 150/1000 | Loss: 0.00001350
Iteration 151/1000 | Loss: 0.00001350
Iteration 152/1000 | Loss: 0.00001350
Iteration 153/1000 | Loss: 0.00001350
Iteration 154/1000 | Loss: 0.00001350
Iteration 155/1000 | Loss: 0.00001350
Iteration 156/1000 | Loss: 0.00001350
Iteration 157/1000 | Loss: 0.00001350
Iteration 158/1000 | Loss: 0.00001350
Iteration 159/1000 | Loss: 0.00001350
Iteration 160/1000 | Loss: 0.00001350
Iteration 161/1000 | Loss: 0.00001350
Iteration 162/1000 | Loss: 0.00001350
Iteration 163/1000 | Loss: 0.00001350
Iteration 164/1000 | Loss: 0.00001350
Iteration 165/1000 | Loss: 0.00001350
Iteration 166/1000 | Loss: 0.00001350
Iteration 167/1000 | Loss: 0.00001350
Iteration 168/1000 | Loss: 0.00001350
Iteration 169/1000 | Loss: 0.00001350
Iteration 170/1000 | Loss: 0.00001350
Iteration 171/1000 | Loss: 0.00001350
Iteration 172/1000 | Loss: 0.00001350
Iteration 173/1000 | Loss: 0.00001350
Iteration 174/1000 | Loss: 0.00001350
Iteration 175/1000 | Loss: 0.00001350
Iteration 176/1000 | Loss: 0.00001350
Iteration 177/1000 | Loss: 0.00001350
Iteration 178/1000 | Loss: 0.00001350
Iteration 179/1000 | Loss: 0.00001350
Iteration 180/1000 | Loss: 0.00001350
Iteration 181/1000 | Loss: 0.00001350
Iteration 182/1000 | Loss: 0.00001350
Iteration 183/1000 | Loss: 0.00001350
Iteration 184/1000 | Loss: 0.00001350
Iteration 185/1000 | Loss: 0.00001350
Iteration 186/1000 | Loss: 0.00001350
Iteration 187/1000 | Loss: 0.00001350
Iteration 188/1000 | Loss: 0.00001350
Iteration 189/1000 | Loss: 0.00001350
Iteration 190/1000 | Loss: 0.00001350
Iteration 191/1000 | Loss: 0.00001350
Iteration 192/1000 | Loss: 0.00001350
Iteration 193/1000 | Loss: 0.00001350
Iteration 194/1000 | Loss: 0.00001350
Iteration 195/1000 | Loss: 0.00001350
Iteration 196/1000 | Loss: 0.00001350
Iteration 197/1000 | Loss: 0.00001350
Iteration 198/1000 | Loss: 0.00001350
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [1.3504371963790618e-05, 1.3504371963790618e-05, 1.3504371963790618e-05, 1.3504371963790618e-05, 1.3504371963790618e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3504371963790618e-05

Optimization complete. Final v2v error: 3.13960599899292 mm

Highest mean error: 3.682234048843384 mm for frame 63

Lowest mean error: 2.957228183746338 mm for frame 106

Saving results

Total time: 39.278186559677124
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00866240
Iteration 2/25 | Loss: 0.00316719
Iteration 3/25 | Loss: 0.00228873
Iteration 4/25 | Loss: 0.00187224
Iteration 5/25 | Loss: 0.00178749
Iteration 6/25 | Loss: 0.00180857
Iteration 7/25 | Loss: 0.00173119
Iteration 8/25 | Loss: 0.00172099
Iteration 9/25 | Loss: 0.00162314
Iteration 10/25 | Loss: 0.00162890
Iteration 11/25 | Loss: 0.00160049
Iteration 12/25 | Loss: 0.00156781
Iteration 13/25 | Loss: 0.00156548
Iteration 14/25 | Loss: 0.00153370
Iteration 15/25 | Loss: 0.00152194
Iteration 16/25 | Loss: 0.00151053
Iteration 17/25 | Loss: 0.00154187
Iteration 18/25 | Loss: 0.00150738
Iteration 19/25 | Loss: 0.00152280
Iteration 20/25 | Loss: 0.00153272
Iteration 21/25 | Loss: 0.00149011
Iteration 22/25 | Loss: 0.00150091
Iteration 23/25 | Loss: 0.00149407
Iteration 24/25 | Loss: 0.00149641
Iteration 25/25 | Loss: 0.00148384

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.45682144
Iteration 2/25 | Loss: 0.00499643
Iteration 3/25 | Loss: 0.00363284
Iteration 4/25 | Loss: 0.00348938
Iteration 5/25 | Loss: 0.00348938
Iteration 6/25 | Loss: 0.00348938
Iteration 7/25 | Loss: 0.00348938
Iteration 8/25 | Loss: 0.00348938
Iteration 9/25 | Loss: 0.00348938
Iteration 10/25 | Loss: 0.00348938
Iteration 11/25 | Loss: 0.00348938
Iteration 12/25 | Loss: 0.00348938
Iteration 13/25 | Loss: 0.00348938
Iteration 14/25 | Loss: 0.00348938
Iteration 15/25 | Loss: 0.00348938
Iteration 16/25 | Loss: 0.00348938
Iteration 17/25 | Loss: 0.00348938
Iteration 18/25 | Loss: 0.00348938
Iteration 19/25 | Loss: 0.00348938
Iteration 20/25 | Loss: 0.00348938
Iteration 21/25 | Loss: 0.00348938
Iteration 22/25 | Loss: 0.00348938
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.003489377209916711, 0.003489377209916711, 0.003489377209916711, 0.003489377209916711, 0.003489377209916711]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003489377209916711

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00348938
Iteration 2/1000 | Loss: 0.00073630
Iteration 3/1000 | Loss: 0.00152156
Iteration 4/1000 | Loss: 0.00541397
Iteration 5/1000 | Loss: 0.00040740
Iteration 6/1000 | Loss: 0.00389247
Iteration 7/1000 | Loss: 0.00063578
Iteration 8/1000 | Loss: 0.00206159
Iteration 9/1000 | Loss: 0.00264672
Iteration 10/1000 | Loss: 0.00219176
Iteration 11/1000 | Loss: 0.00146892
Iteration 12/1000 | Loss: 0.00307617
Iteration 13/1000 | Loss: 0.00240651
Iteration 14/1000 | Loss: 0.00079453
Iteration 15/1000 | Loss: 0.00018388
Iteration 16/1000 | Loss: 0.00022927
Iteration 17/1000 | Loss: 0.00115604
Iteration 18/1000 | Loss: 0.00042283
Iteration 19/1000 | Loss: 0.00037732
Iteration 20/1000 | Loss: 0.00025461
Iteration 21/1000 | Loss: 0.00042742
Iteration 22/1000 | Loss: 0.00085089
Iteration 23/1000 | Loss: 0.00029262
Iteration 24/1000 | Loss: 0.00150876
Iteration 25/1000 | Loss: 0.00126231
Iteration 26/1000 | Loss: 0.00435344
Iteration 27/1000 | Loss: 0.00323136
Iteration 28/1000 | Loss: 0.00032355
Iteration 29/1000 | Loss: 0.00301298
Iteration 30/1000 | Loss: 0.00737835
Iteration 31/1000 | Loss: 0.00534387
Iteration 32/1000 | Loss: 0.00323020
Iteration 33/1000 | Loss: 0.00271318
Iteration 34/1000 | Loss: 0.00271350
Iteration 35/1000 | Loss: 0.00112716
Iteration 36/1000 | Loss: 0.00117235
Iteration 37/1000 | Loss: 0.00029883
Iteration 38/1000 | Loss: 0.00120757
Iteration 39/1000 | Loss: 0.00010906
Iteration 40/1000 | Loss: 0.00070294
Iteration 41/1000 | Loss: 0.00051583
Iteration 42/1000 | Loss: 0.00020002
Iteration 43/1000 | Loss: 0.00009880
Iteration 44/1000 | Loss: 0.00054650
Iteration 45/1000 | Loss: 0.00050717
Iteration 46/1000 | Loss: 0.00011826
Iteration 47/1000 | Loss: 0.00042084
Iteration 48/1000 | Loss: 0.00012164
Iteration 49/1000 | Loss: 0.00018830
Iteration 50/1000 | Loss: 0.00018855
Iteration 51/1000 | Loss: 0.00075295
Iteration 52/1000 | Loss: 0.00018569
Iteration 53/1000 | Loss: 0.00013192
Iteration 54/1000 | Loss: 0.00013713
Iteration 55/1000 | Loss: 0.00005604
Iteration 56/1000 | Loss: 0.00004952
Iteration 57/1000 | Loss: 0.00004435
Iteration 58/1000 | Loss: 0.00004111
Iteration 59/1000 | Loss: 0.00003908
Iteration 60/1000 | Loss: 0.00011933
Iteration 61/1000 | Loss: 0.00004219
Iteration 62/1000 | Loss: 0.00003710
Iteration 63/1000 | Loss: 0.00003547
Iteration 64/1000 | Loss: 0.00003428
Iteration 65/1000 | Loss: 0.00003339
Iteration 66/1000 | Loss: 0.00003621
Iteration 67/1000 | Loss: 0.00003483
Iteration 68/1000 | Loss: 0.00003324
Iteration 69/1000 | Loss: 0.00003255
Iteration 70/1000 | Loss: 0.00025679
Iteration 71/1000 | Loss: 0.00064910
Iteration 72/1000 | Loss: 0.00036501
Iteration 73/1000 | Loss: 0.00003377
Iteration 74/1000 | Loss: 0.00003238
Iteration 75/1000 | Loss: 0.00003145
Iteration 76/1000 | Loss: 0.00003055
Iteration 77/1000 | Loss: 0.00104399
Iteration 78/1000 | Loss: 0.00038306
Iteration 79/1000 | Loss: 0.00037447
Iteration 80/1000 | Loss: 0.00064432
Iteration 81/1000 | Loss: 0.00047453
Iteration 82/1000 | Loss: 0.00004061
Iteration 83/1000 | Loss: 0.00016459
Iteration 84/1000 | Loss: 0.00003324
Iteration 85/1000 | Loss: 0.00003088
Iteration 86/1000 | Loss: 0.00002936
Iteration 87/1000 | Loss: 0.00002875
Iteration 88/1000 | Loss: 0.00040316
Iteration 89/1000 | Loss: 0.00045494
Iteration 90/1000 | Loss: 0.00042507
Iteration 91/1000 | Loss: 0.00059478
Iteration 92/1000 | Loss: 0.00050237
Iteration 93/1000 | Loss: 0.00070173
Iteration 94/1000 | Loss: 0.00049054
Iteration 95/1000 | Loss: 0.00017799
Iteration 96/1000 | Loss: 0.00025271
Iteration 97/1000 | Loss: 0.00034273
Iteration 98/1000 | Loss: 0.00022296
Iteration 99/1000 | Loss: 0.00022348
Iteration 100/1000 | Loss: 0.00030466
Iteration 101/1000 | Loss: 0.00003791
Iteration 102/1000 | Loss: 0.00043672
Iteration 103/1000 | Loss: 0.00048180
Iteration 104/1000 | Loss: 0.00004102
Iteration 105/1000 | Loss: 0.00003266
Iteration 106/1000 | Loss: 0.00002751
Iteration 107/1000 | Loss: 0.00002531
Iteration 108/1000 | Loss: 0.00002444
Iteration 109/1000 | Loss: 0.00002377
Iteration 110/1000 | Loss: 0.00002334
Iteration 111/1000 | Loss: 0.00002310
Iteration 112/1000 | Loss: 0.00002289
Iteration 113/1000 | Loss: 0.00002289
Iteration 114/1000 | Loss: 0.00002275
Iteration 115/1000 | Loss: 0.00002272
Iteration 116/1000 | Loss: 0.00002272
Iteration 117/1000 | Loss: 0.00002270
Iteration 118/1000 | Loss: 0.00002270
Iteration 119/1000 | Loss: 0.00002268
Iteration 120/1000 | Loss: 0.00002268
Iteration 121/1000 | Loss: 0.00002267
Iteration 122/1000 | Loss: 0.00002267
Iteration 123/1000 | Loss: 0.00002267
Iteration 124/1000 | Loss: 0.00002267
Iteration 125/1000 | Loss: 0.00002267
Iteration 126/1000 | Loss: 0.00002267
Iteration 127/1000 | Loss: 0.00002267
Iteration 128/1000 | Loss: 0.00002267
Iteration 129/1000 | Loss: 0.00002267
Iteration 130/1000 | Loss: 0.00002267
Iteration 131/1000 | Loss: 0.00002267
Iteration 132/1000 | Loss: 0.00002266
Iteration 133/1000 | Loss: 0.00002266
Iteration 134/1000 | Loss: 0.00002266
Iteration 135/1000 | Loss: 0.00002266
Iteration 136/1000 | Loss: 0.00002266
Iteration 137/1000 | Loss: 0.00002266
Iteration 138/1000 | Loss: 0.00002266
Iteration 139/1000 | Loss: 0.00002266
Iteration 140/1000 | Loss: 0.00002265
Iteration 141/1000 | Loss: 0.00002264
Iteration 142/1000 | Loss: 0.00002262
Iteration 143/1000 | Loss: 0.00002261
Iteration 144/1000 | Loss: 0.00002260
Iteration 145/1000 | Loss: 0.00002260
Iteration 146/1000 | Loss: 0.00002259
Iteration 147/1000 | Loss: 0.00002259
Iteration 148/1000 | Loss: 0.00002258
Iteration 149/1000 | Loss: 0.00002258
Iteration 150/1000 | Loss: 0.00002258
Iteration 151/1000 | Loss: 0.00002257
Iteration 152/1000 | Loss: 0.00002257
Iteration 153/1000 | Loss: 0.00002257
Iteration 154/1000 | Loss: 0.00002257
Iteration 155/1000 | Loss: 0.00002257
Iteration 156/1000 | Loss: 0.00002256
Iteration 157/1000 | Loss: 0.00002256
Iteration 158/1000 | Loss: 0.00002256
Iteration 159/1000 | Loss: 0.00002255
Iteration 160/1000 | Loss: 0.00002255
Iteration 161/1000 | Loss: 0.00002255
Iteration 162/1000 | Loss: 0.00002254
Iteration 163/1000 | Loss: 0.00002254
Iteration 164/1000 | Loss: 0.00002254
Iteration 165/1000 | Loss: 0.00002254
Iteration 166/1000 | Loss: 0.00002254
Iteration 167/1000 | Loss: 0.00002253
Iteration 168/1000 | Loss: 0.00002253
Iteration 169/1000 | Loss: 0.00002253
Iteration 170/1000 | Loss: 0.00002253
Iteration 171/1000 | Loss: 0.00002253
Iteration 172/1000 | Loss: 0.00002253
Iteration 173/1000 | Loss: 0.00002253
Iteration 174/1000 | Loss: 0.00002252
Iteration 175/1000 | Loss: 0.00002252
Iteration 176/1000 | Loss: 0.00002252
Iteration 177/1000 | Loss: 0.00002252
Iteration 178/1000 | Loss: 0.00002252
Iteration 179/1000 | Loss: 0.00002252
Iteration 180/1000 | Loss: 0.00002251
Iteration 181/1000 | Loss: 0.00002251
Iteration 182/1000 | Loss: 0.00002251
Iteration 183/1000 | Loss: 0.00002251
Iteration 184/1000 | Loss: 0.00002250
Iteration 185/1000 | Loss: 0.00002250
Iteration 186/1000 | Loss: 0.00002250
Iteration 187/1000 | Loss: 0.00002250
Iteration 188/1000 | Loss: 0.00002249
Iteration 189/1000 | Loss: 0.00002249
Iteration 190/1000 | Loss: 0.00002249
Iteration 191/1000 | Loss: 0.00002248
Iteration 192/1000 | Loss: 0.00002248
Iteration 193/1000 | Loss: 0.00002247
Iteration 194/1000 | Loss: 0.00002247
Iteration 195/1000 | Loss: 0.00002247
Iteration 196/1000 | Loss: 0.00002246
Iteration 197/1000 | Loss: 0.00002246
Iteration 198/1000 | Loss: 0.00002245
Iteration 199/1000 | Loss: 0.00002244
Iteration 200/1000 | Loss: 0.00002243
Iteration 201/1000 | Loss: 0.00002242
Iteration 202/1000 | Loss: 0.00002242
Iteration 203/1000 | Loss: 0.00002242
Iteration 204/1000 | Loss: 0.00002241
Iteration 205/1000 | Loss: 0.00002241
Iteration 206/1000 | Loss: 0.00002241
Iteration 207/1000 | Loss: 0.00002240
Iteration 208/1000 | Loss: 0.00002240
Iteration 209/1000 | Loss: 0.00002240
Iteration 210/1000 | Loss: 0.00002240
Iteration 211/1000 | Loss: 0.00002239
Iteration 212/1000 | Loss: 0.00002239
Iteration 213/1000 | Loss: 0.00002239
Iteration 214/1000 | Loss: 0.00002238
Iteration 215/1000 | Loss: 0.00002238
Iteration 216/1000 | Loss: 0.00002238
Iteration 217/1000 | Loss: 0.00002238
Iteration 218/1000 | Loss: 0.00002238
Iteration 219/1000 | Loss: 0.00002238
Iteration 220/1000 | Loss: 0.00002237
Iteration 221/1000 | Loss: 0.00002237
Iteration 222/1000 | Loss: 0.00002237
Iteration 223/1000 | Loss: 0.00002237
Iteration 224/1000 | Loss: 0.00002237
Iteration 225/1000 | Loss: 0.00002237
Iteration 226/1000 | Loss: 0.00002237
Iteration 227/1000 | Loss: 0.00002236
Iteration 228/1000 | Loss: 0.00002236
Iteration 229/1000 | Loss: 0.00002236
Iteration 230/1000 | Loss: 0.00002236
Iteration 231/1000 | Loss: 0.00002236
Iteration 232/1000 | Loss: 0.00002235
Iteration 233/1000 | Loss: 0.00002235
Iteration 234/1000 | Loss: 0.00002235
Iteration 235/1000 | Loss: 0.00002235
Iteration 236/1000 | Loss: 0.00002235
Iteration 237/1000 | Loss: 0.00002234
Iteration 238/1000 | Loss: 0.00002234
Iteration 239/1000 | Loss: 0.00002234
Iteration 240/1000 | Loss: 0.00053503
Iteration 241/1000 | Loss: 0.00019718
Iteration 242/1000 | Loss: 0.00002474
Iteration 243/1000 | Loss: 0.00002258
Iteration 244/1000 | Loss: 0.00002242
Iteration 245/1000 | Loss: 0.00002240
Iteration 246/1000 | Loss: 0.00002240
Iteration 247/1000 | Loss: 0.00002234
Iteration 248/1000 | Loss: 0.00002232
Iteration 249/1000 | Loss: 0.00002232
Iteration 250/1000 | Loss: 0.00002232
Iteration 251/1000 | Loss: 0.00002231
Iteration 252/1000 | Loss: 0.00002231
Iteration 253/1000 | Loss: 0.00002231
Iteration 254/1000 | Loss: 0.00002231
Iteration 255/1000 | Loss: 0.00002230
Iteration 256/1000 | Loss: 0.00002230
Iteration 257/1000 | Loss: 0.00002230
Iteration 258/1000 | Loss: 0.00002230
Iteration 259/1000 | Loss: 0.00002230
Iteration 260/1000 | Loss: 0.00002230
Iteration 261/1000 | Loss: 0.00002229
Iteration 262/1000 | Loss: 0.00002229
Iteration 263/1000 | Loss: 0.00002229
Iteration 264/1000 | Loss: 0.00002228
Iteration 265/1000 | Loss: 0.00002228
Iteration 266/1000 | Loss: 0.00002228
Iteration 267/1000 | Loss: 0.00002227
Iteration 268/1000 | Loss: 0.00002227
Iteration 269/1000 | Loss: 0.00002226
Iteration 270/1000 | Loss: 0.00002226
Iteration 271/1000 | Loss: 0.00002226
Iteration 272/1000 | Loss: 0.00002226
Iteration 273/1000 | Loss: 0.00002226
Iteration 274/1000 | Loss: 0.00002226
Iteration 275/1000 | Loss: 0.00002225
Iteration 276/1000 | Loss: 0.00002225
Iteration 277/1000 | Loss: 0.00002225
Iteration 278/1000 | Loss: 0.00002225
Iteration 279/1000 | Loss: 0.00052895
Iteration 280/1000 | Loss: 0.00109904
Iteration 281/1000 | Loss: 0.00101636
Iteration 282/1000 | Loss: 0.00055049
Iteration 283/1000 | Loss: 0.00004150
Iteration 284/1000 | Loss: 0.00002575
Iteration 285/1000 | Loss: 0.00002230
Iteration 286/1000 | Loss: 0.00002026
Iteration 287/1000 | Loss: 0.00001909
Iteration 288/1000 | Loss: 0.00001811
Iteration 289/1000 | Loss: 0.00001768
Iteration 290/1000 | Loss: 0.00001746
Iteration 291/1000 | Loss: 0.00001745
Iteration 292/1000 | Loss: 0.00001742
Iteration 293/1000 | Loss: 0.00001726
Iteration 294/1000 | Loss: 0.00001712
Iteration 295/1000 | Loss: 0.00001710
Iteration 296/1000 | Loss: 0.00001707
Iteration 297/1000 | Loss: 0.00001703
Iteration 298/1000 | Loss: 0.00001703
Iteration 299/1000 | Loss: 0.00001702
Iteration 300/1000 | Loss: 0.00001701
Iteration 301/1000 | Loss: 0.00001695
Iteration 302/1000 | Loss: 0.00001695
Iteration 303/1000 | Loss: 0.00001693
Iteration 304/1000 | Loss: 0.00001692
Iteration 305/1000 | Loss: 0.00001692
Iteration 306/1000 | Loss: 0.00001692
Iteration 307/1000 | Loss: 0.00001692
Iteration 308/1000 | Loss: 0.00001692
Iteration 309/1000 | Loss: 0.00001692
Iteration 310/1000 | Loss: 0.00001691
Iteration 311/1000 | Loss: 0.00001691
Iteration 312/1000 | Loss: 0.00001690
Iteration 313/1000 | Loss: 0.00001690
Iteration 314/1000 | Loss: 0.00001689
Iteration 315/1000 | Loss: 0.00001689
Iteration 316/1000 | Loss: 0.00001688
Iteration 317/1000 | Loss: 0.00001688
Iteration 318/1000 | Loss: 0.00001688
Iteration 319/1000 | Loss: 0.00001688
Iteration 320/1000 | Loss: 0.00001688
Iteration 321/1000 | Loss: 0.00001687
Iteration 322/1000 | Loss: 0.00001687
Iteration 323/1000 | Loss: 0.00001687
Iteration 324/1000 | Loss: 0.00001687
Iteration 325/1000 | Loss: 0.00001687
Iteration 326/1000 | Loss: 0.00001686
Iteration 327/1000 | Loss: 0.00001686
Iteration 328/1000 | Loss: 0.00001686
Iteration 329/1000 | Loss: 0.00001686
Iteration 330/1000 | Loss: 0.00001686
Iteration 331/1000 | Loss: 0.00001685
Iteration 332/1000 | Loss: 0.00001685
Iteration 333/1000 | Loss: 0.00001685
Iteration 334/1000 | Loss: 0.00001684
Iteration 335/1000 | Loss: 0.00001684
Iteration 336/1000 | Loss: 0.00001684
Iteration 337/1000 | Loss: 0.00001684
Iteration 338/1000 | Loss: 0.00001683
Iteration 339/1000 | Loss: 0.00001683
Iteration 340/1000 | Loss: 0.00001683
Iteration 341/1000 | Loss: 0.00001683
Iteration 342/1000 | Loss: 0.00001683
Iteration 343/1000 | Loss: 0.00001683
Iteration 344/1000 | Loss: 0.00001683
Iteration 345/1000 | Loss: 0.00001683
Iteration 346/1000 | Loss: 0.00001683
Iteration 347/1000 | Loss: 0.00001683
Iteration 348/1000 | Loss: 0.00001683
Iteration 349/1000 | Loss: 0.00001683
Iteration 350/1000 | Loss: 0.00001683
Iteration 351/1000 | Loss: 0.00001683
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 351. Stopping optimization.
Last 5 losses: [1.6826996215968393e-05, 1.6826996215968393e-05, 1.6826996215968393e-05, 1.6826996215968393e-05, 1.6826996215968393e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6826996215968393e-05

Optimization complete. Final v2v error: 3.2933952808380127 mm

Highest mean error: 10.08425521850586 mm for frame 71

Lowest mean error: 2.757387638092041 mm for frame 59

Saving results

Total time: 279.28437972068787
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00509895
Iteration 2/25 | Loss: 0.00135400
Iteration 3/25 | Loss: 0.00128177
Iteration 4/25 | Loss: 0.00127292
Iteration 5/25 | Loss: 0.00127020
Iteration 6/25 | Loss: 0.00127020
Iteration 7/25 | Loss: 0.00127020
Iteration 8/25 | Loss: 0.00127020
Iteration 9/25 | Loss: 0.00127020
Iteration 10/25 | Loss: 0.00127020
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001270204083994031, 0.001270204083994031, 0.001270204083994031, 0.001270204083994031, 0.001270204083994031]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001270204083994031

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.87604433
Iteration 2/25 | Loss: 0.00073855
Iteration 3/25 | Loss: 0.00073855
Iteration 4/25 | Loss: 0.00073855
Iteration 5/25 | Loss: 0.00073855
Iteration 6/25 | Loss: 0.00073855
Iteration 7/25 | Loss: 0.00073855
Iteration 8/25 | Loss: 0.00073855
Iteration 9/25 | Loss: 0.00073855
Iteration 10/25 | Loss: 0.00073855
Iteration 11/25 | Loss: 0.00073855
Iteration 12/25 | Loss: 0.00073855
Iteration 13/25 | Loss: 0.00073855
Iteration 14/25 | Loss: 0.00073855
Iteration 15/25 | Loss: 0.00073855
Iteration 16/25 | Loss: 0.00073855
Iteration 17/25 | Loss: 0.00073855
Iteration 18/25 | Loss: 0.00073855
Iteration 19/25 | Loss: 0.00073855
Iteration 20/25 | Loss: 0.00073855
Iteration 21/25 | Loss: 0.00073855
Iteration 22/25 | Loss: 0.00073855
Iteration 23/25 | Loss: 0.00073855
Iteration 24/25 | Loss: 0.00073855
Iteration 25/25 | Loss: 0.00073855

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073855
Iteration 2/1000 | Loss: 0.00004404
Iteration 3/1000 | Loss: 0.00002584
Iteration 4/1000 | Loss: 0.00002354
Iteration 5/1000 | Loss: 0.00002231
Iteration 6/1000 | Loss: 0.00002147
Iteration 7/1000 | Loss: 0.00002089
Iteration 8/1000 | Loss: 0.00002055
Iteration 9/1000 | Loss: 0.00002018
Iteration 10/1000 | Loss: 0.00001993
Iteration 11/1000 | Loss: 0.00001967
Iteration 12/1000 | Loss: 0.00001935
Iteration 13/1000 | Loss: 0.00001915
Iteration 14/1000 | Loss: 0.00001897
Iteration 15/1000 | Loss: 0.00001874
Iteration 16/1000 | Loss: 0.00001860
Iteration 17/1000 | Loss: 0.00001855
Iteration 18/1000 | Loss: 0.00001844
Iteration 19/1000 | Loss: 0.00001837
Iteration 20/1000 | Loss: 0.00001829
Iteration 21/1000 | Loss: 0.00001829
Iteration 22/1000 | Loss: 0.00001828
Iteration 23/1000 | Loss: 0.00001826
Iteration 24/1000 | Loss: 0.00001824
Iteration 25/1000 | Loss: 0.00001824
Iteration 26/1000 | Loss: 0.00001824
Iteration 27/1000 | Loss: 0.00001820
Iteration 28/1000 | Loss: 0.00001818
Iteration 29/1000 | Loss: 0.00001818
Iteration 30/1000 | Loss: 0.00001816
Iteration 31/1000 | Loss: 0.00001816
Iteration 32/1000 | Loss: 0.00001816
Iteration 33/1000 | Loss: 0.00001816
Iteration 34/1000 | Loss: 0.00001816
Iteration 35/1000 | Loss: 0.00001816
Iteration 36/1000 | Loss: 0.00001816
Iteration 37/1000 | Loss: 0.00001816
Iteration 38/1000 | Loss: 0.00001815
Iteration 39/1000 | Loss: 0.00001815
Iteration 40/1000 | Loss: 0.00001815
Iteration 41/1000 | Loss: 0.00001814
Iteration 42/1000 | Loss: 0.00001814
Iteration 43/1000 | Loss: 0.00001814
Iteration 44/1000 | Loss: 0.00001813
Iteration 45/1000 | Loss: 0.00001813
Iteration 46/1000 | Loss: 0.00001813
Iteration 47/1000 | Loss: 0.00001813
Iteration 48/1000 | Loss: 0.00001813
Iteration 49/1000 | Loss: 0.00001812
Iteration 50/1000 | Loss: 0.00001812
Iteration 51/1000 | Loss: 0.00001812
Iteration 52/1000 | Loss: 0.00001811
Iteration 53/1000 | Loss: 0.00001811
Iteration 54/1000 | Loss: 0.00001810
Iteration 55/1000 | Loss: 0.00001810
Iteration 56/1000 | Loss: 0.00001810
Iteration 57/1000 | Loss: 0.00001810
Iteration 58/1000 | Loss: 0.00001810
Iteration 59/1000 | Loss: 0.00001810
Iteration 60/1000 | Loss: 0.00001809
Iteration 61/1000 | Loss: 0.00001809
Iteration 62/1000 | Loss: 0.00001809
Iteration 63/1000 | Loss: 0.00001809
Iteration 64/1000 | Loss: 0.00001809
Iteration 65/1000 | Loss: 0.00001809
Iteration 66/1000 | Loss: 0.00001809
Iteration 67/1000 | Loss: 0.00001808
Iteration 68/1000 | Loss: 0.00001808
Iteration 69/1000 | Loss: 0.00001808
Iteration 70/1000 | Loss: 0.00001808
Iteration 71/1000 | Loss: 0.00001808
Iteration 72/1000 | Loss: 0.00001807
Iteration 73/1000 | Loss: 0.00001807
Iteration 74/1000 | Loss: 0.00001807
Iteration 75/1000 | Loss: 0.00001807
Iteration 76/1000 | Loss: 0.00001807
Iteration 77/1000 | Loss: 0.00001807
Iteration 78/1000 | Loss: 0.00001806
Iteration 79/1000 | Loss: 0.00001806
Iteration 80/1000 | Loss: 0.00001806
Iteration 81/1000 | Loss: 0.00001806
Iteration 82/1000 | Loss: 0.00001806
Iteration 83/1000 | Loss: 0.00001806
Iteration 84/1000 | Loss: 0.00001806
Iteration 85/1000 | Loss: 0.00001806
Iteration 86/1000 | Loss: 0.00001806
Iteration 87/1000 | Loss: 0.00001806
Iteration 88/1000 | Loss: 0.00001805
Iteration 89/1000 | Loss: 0.00001805
Iteration 90/1000 | Loss: 0.00001805
Iteration 91/1000 | Loss: 0.00001805
Iteration 92/1000 | Loss: 0.00001805
Iteration 93/1000 | Loss: 0.00001805
Iteration 94/1000 | Loss: 0.00001805
Iteration 95/1000 | Loss: 0.00001804
Iteration 96/1000 | Loss: 0.00001804
Iteration 97/1000 | Loss: 0.00001804
Iteration 98/1000 | Loss: 0.00001804
Iteration 99/1000 | Loss: 0.00001804
Iteration 100/1000 | Loss: 0.00001803
Iteration 101/1000 | Loss: 0.00001803
Iteration 102/1000 | Loss: 0.00001803
Iteration 103/1000 | Loss: 0.00001803
Iteration 104/1000 | Loss: 0.00001803
Iteration 105/1000 | Loss: 0.00001802
Iteration 106/1000 | Loss: 0.00001802
Iteration 107/1000 | Loss: 0.00001802
Iteration 108/1000 | Loss: 0.00001802
Iteration 109/1000 | Loss: 0.00001802
Iteration 110/1000 | Loss: 0.00001801
Iteration 111/1000 | Loss: 0.00001801
Iteration 112/1000 | Loss: 0.00001801
Iteration 113/1000 | Loss: 0.00001801
Iteration 114/1000 | Loss: 0.00001801
Iteration 115/1000 | Loss: 0.00001801
Iteration 116/1000 | Loss: 0.00001801
Iteration 117/1000 | Loss: 0.00001801
Iteration 118/1000 | Loss: 0.00001801
Iteration 119/1000 | Loss: 0.00001801
Iteration 120/1000 | Loss: 0.00001800
Iteration 121/1000 | Loss: 0.00001800
Iteration 122/1000 | Loss: 0.00001800
Iteration 123/1000 | Loss: 0.00001800
Iteration 124/1000 | Loss: 0.00001800
Iteration 125/1000 | Loss: 0.00001800
Iteration 126/1000 | Loss: 0.00001800
Iteration 127/1000 | Loss: 0.00001800
Iteration 128/1000 | Loss: 0.00001800
Iteration 129/1000 | Loss: 0.00001800
Iteration 130/1000 | Loss: 0.00001800
Iteration 131/1000 | Loss: 0.00001799
Iteration 132/1000 | Loss: 0.00001799
Iteration 133/1000 | Loss: 0.00001799
Iteration 134/1000 | Loss: 0.00001799
Iteration 135/1000 | Loss: 0.00001799
Iteration 136/1000 | Loss: 0.00001799
Iteration 137/1000 | Loss: 0.00001799
Iteration 138/1000 | Loss: 0.00001799
Iteration 139/1000 | Loss: 0.00001799
Iteration 140/1000 | Loss: 0.00001799
Iteration 141/1000 | Loss: 0.00001799
Iteration 142/1000 | Loss: 0.00001799
Iteration 143/1000 | Loss: 0.00001799
Iteration 144/1000 | Loss: 0.00001799
Iteration 145/1000 | Loss: 0.00001799
Iteration 146/1000 | Loss: 0.00001799
Iteration 147/1000 | Loss: 0.00001799
Iteration 148/1000 | Loss: 0.00001799
Iteration 149/1000 | Loss: 0.00001799
Iteration 150/1000 | Loss: 0.00001799
Iteration 151/1000 | Loss: 0.00001799
Iteration 152/1000 | Loss: 0.00001799
Iteration 153/1000 | Loss: 0.00001799
Iteration 154/1000 | Loss: 0.00001799
Iteration 155/1000 | Loss: 0.00001799
Iteration 156/1000 | Loss: 0.00001799
Iteration 157/1000 | Loss: 0.00001799
Iteration 158/1000 | Loss: 0.00001799
Iteration 159/1000 | Loss: 0.00001799
Iteration 160/1000 | Loss: 0.00001799
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.799040546757169e-05, 1.799040546757169e-05, 1.799040546757169e-05, 1.799040546757169e-05, 1.799040546757169e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.799040546757169e-05

Optimization complete. Final v2v error: 3.5405871868133545 mm

Highest mean error: 3.889077663421631 mm for frame 0

Lowest mean error: 3.4928553104400635 mm for frame 115

Saving results

Total time: 49.884734869003296
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00500649
Iteration 2/25 | Loss: 0.00140730
Iteration 3/25 | Loss: 0.00130000
Iteration 4/25 | Loss: 0.00127632
Iteration 5/25 | Loss: 0.00127046
Iteration 6/25 | Loss: 0.00126872
Iteration 7/25 | Loss: 0.00126815
Iteration 8/25 | Loss: 0.00126792
Iteration 9/25 | Loss: 0.00126788
Iteration 10/25 | Loss: 0.00126788
Iteration 11/25 | Loss: 0.00126788
Iteration 12/25 | Loss: 0.00126788
Iteration 13/25 | Loss: 0.00126788
Iteration 14/25 | Loss: 0.00126788
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012678814819082618, 0.0012678814819082618, 0.0012678814819082618, 0.0012678814819082618, 0.0012678814819082618]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012678814819082618

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48188496
Iteration 2/25 | Loss: 0.00103960
Iteration 3/25 | Loss: 0.00103960
Iteration 4/25 | Loss: 0.00103960
Iteration 5/25 | Loss: 0.00103960
Iteration 6/25 | Loss: 0.00103959
Iteration 7/25 | Loss: 0.00103959
Iteration 8/25 | Loss: 0.00103959
Iteration 9/25 | Loss: 0.00103959
Iteration 10/25 | Loss: 0.00103959
Iteration 11/25 | Loss: 0.00103959
Iteration 12/25 | Loss: 0.00103959
Iteration 13/25 | Loss: 0.00103959
Iteration 14/25 | Loss: 0.00103959
Iteration 15/25 | Loss: 0.00103959
Iteration 16/25 | Loss: 0.00103959
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010395935969427228, 0.0010395935969427228, 0.0010395935969427228, 0.0010395935969427228, 0.0010395935969427228]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010395935969427228

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103959
Iteration 2/1000 | Loss: 0.00008896
Iteration 3/1000 | Loss: 0.00008630
Iteration 4/1000 | Loss: 0.00004011
Iteration 5/1000 | Loss: 0.00003425
Iteration 6/1000 | Loss: 0.00003125
Iteration 7/1000 | Loss: 0.00006143
Iteration 8/1000 | Loss: 0.00006593
Iteration 9/1000 | Loss: 0.00005895
Iteration 10/1000 | Loss: 0.00003288
Iteration 11/1000 | Loss: 0.00002650
Iteration 12/1000 | Loss: 0.00002498
Iteration 13/1000 | Loss: 0.00002425
Iteration 14/1000 | Loss: 0.00002394
Iteration 15/1000 | Loss: 0.00002370
Iteration 16/1000 | Loss: 0.00002340
Iteration 17/1000 | Loss: 0.00002317
Iteration 18/1000 | Loss: 0.00002316
Iteration 19/1000 | Loss: 0.00002307
Iteration 20/1000 | Loss: 0.00002304
Iteration 21/1000 | Loss: 0.00002304
Iteration 22/1000 | Loss: 0.00002304
Iteration 23/1000 | Loss: 0.00002303
Iteration 24/1000 | Loss: 0.00002303
Iteration 25/1000 | Loss: 0.00002302
Iteration 26/1000 | Loss: 0.00002296
Iteration 27/1000 | Loss: 0.00002295
Iteration 28/1000 | Loss: 0.00002292
Iteration 29/1000 | Loss: 0.00002291
Iteration 30/1000 | Loss: 0.00002291
Iteration 31/1000 | Loss: 0.00002291
Iteration 32/1000 | Loss: 0.00002291
Iteration 33/1000 | Loss: 0.00002290
Iteration 34/1000 | Loss: 0.00002290
Iteration 35/1000 | Loss: 0.00002289
Iteration 36/1000 | Loss: 0.00002288
Iteration 37/1000 | Loss: 0.00002288
Iteration 38/1000 | Loss: 0.00002287
Iteration 39/1000 | Loss: 0.00002287
Iteration 40/1000 | Loss: 0.00002286
Iteration 41/1000 | Loss: 0.00002286
Iteration 42/1000 | Loss: 0.00002284
Iteration 43/1000 | Loss: 0.00002284
Iteration 44/1000 | Loss: 0.00002284
Iteration 45/1000 | Loss: 0.00002283
Iteration 46/1000 | Loss: 0.00002281
Iteration 47/1000 | Loss: 0.00002278
Iteration 48/1000 | Loss: 0.00002278
Iteration 49/1000 | Loss: 0.00002278
Iteration 50/1000 | Loss: 0.00002277
Iteration 51/1000 | Loss: 0.00002276
Iteration 52/1000 | Loss: 0.00002276
Iteration 53/1000 | Loss: 0.00002276
Iteration 54/1000 | Loss: 0.00002276
Iteration 55/1000 | Loss: 0.00002276
Iteration 56/1000 | Loss: 0.00002275
Iteration 57/1000 | Loss: 0.00002275
Iteration 58/1000 | Loss: 0.00002275
Iteration 59/1000 | Loss: 0.00002275
Iteration 60/1000 | Loss: 0.00002275
Iteration 61/1000 | Loss: 0.00002275
Iteration 62/1000 | Loss: 0.00002275
Iteration 63/1000 | Loss: 0.00002275
Iteration 64/1000 | Loss: 0.00002274
Iteration 65/1000 | Loss: 0.00002273
Iteration 66/1000 | Loss: 0.00002273
Iteration 67/1000 | Loss: 0.00002272
Iteration 68/1000 | Loss: 0.00002272
Iteration 69/1000 | Loss: 0.00002272
Iteration 70/1000 | Loss: 0.00002271
Iteration 71/1000 | Loss: 0.00002271
Iteration 72/1000 | Loss: 0.00002270
Iteration 73/1000 | Loss: 0.00002270
Iteration 74/1000 | Loss: 0.00002270
Iteration 75/1000 | Loss: 0.00002269
Iteration 76/1000 | Loss: 0.00002269
Iteration 77/1000 | Loss: 0.00002269
Iteration 78/1000 | Loss: 0.00002268
Iteration 79/1000 | Loss: 0.00002268
Iteration 80/1000 | Loss: 0.00002268
Iteration 81/1000 | Loss: 0.00002268
Iteration 82/1000 | Loss: 0.00002268
Iteration 83/1000 | Loss: 0.00002268
Iteration 84/1000 | Loss: 0.00002268
Iteration 85/1000 | Loss: 0.00002268
Iteration 86/1000 | Loss: 0.00002268
Iteration 87/1000 | Loss: 0.00002267
Iteration 88/1000 | Loss: 0.00002267
Iteration 89/1000 | Loss: 0.00002267
Iteration 90/1000 | Loss: 0.00002267
Iteration 91/1000 | Loss: 0.00002267
Iteration 92/1000 | Loss: 0.00002267
Iteration 93/1000 | Loss: 0.00002267
Iteration 94/1000 | Loss: 0.00002266
Iteration 95/1000 | Loss: 0.00002266
Iteration 96/1000 | Loss: 0.00002266
Iteration 97/1000 | Loss: 0.00002266
Iteration 98/1000 | Loss: 0.00002266
Iteration 99/1000 | Loss: 0.00002266
Iteration 100/1000 | Loss: 0.00002266
Iteration 101/1000 | Loss: 0.00002265
Iteration 102/1000 | Loss: 0.00002265
Iteration 103/1000 | Loss: 0.00002265
Iteration 104/1000 | Loss: 0.00002264
Iteration 105/1000 | Loss: 0.00002264
Iteration 106/1000 | Loss: 0.00002264
Iteration 107/1000 | Loss: 0.00002264
Iteration 108/1000 | Loss: 0.00002264
Iteration 109/1000 | Loss: 0.00002264
Iteration 110/1000 | Loss: 0.00002264
Iteration 111/1000 | Loss: 0.00002264
Iteration 112/1000 | Loss: 0.00002264
Iteration 113/1000 | Loss: 0.00002264
Iteration 114/1000 | Loss: 0.00002264
Iteration 115/1000 | Loss: 0.00002264
Iteration 116/1000 | Loss: 0.00002263
Iteration 117/1000 | Loss: 0.00002263
Iteration 118/1000 | Loss: 0.00002263
Iteration 119/1000 | Loss: 0.00002263
Iteration 120/1000 | Loss: 0.00002263
Iteration 121/1000 | Loss: 0.00002263
Iteration 122/1000 | Loss: 0.00002263
Iteration 123/1000 | Loss: 0.00002263
Iteration 124/1000 | Loss: 0.00002263
Iteration 125/1000 | Loss: 0.00002263
Iteration 126/1000 | Loss: 0.00002263
Iteration 127/1000 | Loss: 0.00002263
Iteration 128/1000 | Loss: 0.00002263
Iteration 129/1000 | Loss: 0.00002263
Iteration 130/1000 | Loss: 0.00002263
Iteration 131/1000 | Loss: 0.00002263
Iteration 132/1000 | Loss: 0.00002263
Iteration 133/1000 | Loss: 0.00002262
Iteration 134/1000 | Loss: 0.00002262
Iteration 135/1000 | Loss: 0.00002262
Iteration 136/1000 | Loss: 0.00002262
Iteration 137/1000 | Loss: 0.00002262
Iteration 138/1000 | Loss: 0.00002262
Iteration 139/1000 | Loss: 0.00002262
Iteration 140/1000 | Loss: 0.00002262
Iteration 141/1000 | Loss: 0.00002262
Iteration 142/1000 | Loss: 0.00002262
Iteration 143/1000 | Loss: 0.00002262
Iteration 144/1000 | Loss: 0.00002262
Iteration 145/1000 | Loss: 0.00002262
Iteration 146/1000 | Loss: 0.00002262
Iteration 147/1000 | Loss: 0.00002262
Iteration 148/1000 | Loss: 0.00002262
Iteration 149/1000 | Loss: 0.00002262
Iteration 150/1000 | Loss: 0.00002262
Iteration 151/1000 | Loss: 0.00002262
Iteration 152/1000 | Loss: 0.00002262
Iteration 153/1000 | Loss: 0.00002262
Iteration 154/1000 | Loss: 0.00002262
Iteration 155/1000 | Loss: 0.00002262
Iteration 156/1000 | Loss: 0.00002262
Iteration 157/1000 | Loss: 0.00002262
Iteration 158/1000 | Loss: 0.00002262
Iteration 159/1000 | Loss: 0.00002262
Iteration 160/1000 | Loss: 0.00002262
Iteration 161/1000 | Loss: 0.00002262
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [2.2615526177105494e-05, 2.2615526177105494e-05, 2.2615526177105494e-05, 2.2615526177105494e-05, 2.2615526177105494e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2615526177105494e-05

Optimization complete. Final v2v error: 3.8620355129241943 mm

Highest mean error: 5.469878196716309 mm for frame 94

Lowest mean error: 2.916240930557251 mm for frame 49

Saving results

Total time: 47.677956342697144
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00486230
Iteration 2/25 | Loss: 0.00155780
Iteration 3/25 | Loss: 0.00141684
Iteration 4/25 | Loss: 0.00139749
Iteration 5/25 | Loss: 0.00139128
Iteration 6/25 | Loss: 0.00138960
Iteration 7/25 | Loss: 0.00138890
Iteration 8/25 | Loss: 0.00138857
Iteration 9/25 | Loss: 0.00138857
Iteration 10/25 | Loss: 0.00138857
Iteration 11/25 | Loss: 0.00138857
Iteration 12/25 | Loss: 0.00138857
Iteration 13/25 | Loss: 0.00138857
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001388569246046245, 0.001388569246046245, 0.001388569246046245, 0.001388569246046245, 0.001388569246046245]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001388569246046245

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41500473
Iteration 2/25 | Loss: 0.00248147
Iteration 3/25 | Loss: 0.00248147
Iteration 4/25 | Loss: 0.00248147
Iteration 5/25 | Loss: 0.00248147
Iteration 6/25 | Loss: 0.00248147
Iteration 7/25 | Loss: 0.00248147
Iteration 8/25 | Loss: 0.00248147
Iteration 9/25 | Loss: 0.00248147
Iteration 10/25 | Loss: 0.00248147
Iteration 11/25 | Loss: 0.00248147
Iteration 12/25 | Loss: 0.00248147
Iteration 13/25 | Loss: 0.00248147
Iteration 14/25 | Loss: 0.00248147
Iteration 15/25 | Loss: 0.00248147
Iteration 16/25 | Loss: 0.00248147
Iteration 17/25 | Loss: 0.00248147
Iteration 18/25 | Loss: 0.00248147
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0024814694188535213, 0.0024814694188535213, 0.0024814694188535213, 0.0024814694188535213, 0.0024814694188535213]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024814694188535213

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00248147
Iteration 2/1000 | Loss: 0.00022848
Iteration 3/1000 | Loss: 0.00015134
Iteration 4/1000 | Loss: 0.00013473
Iteration 5/1000 | Loss: 0.00012219
Iteration 6/1000 | Loss: 0.00011652
Iteration 7/1000 | Loss: 0.00011114
Iteration 8/1000 | Loss: 0.00010723
Iteration 9/1000 | Loss: 0.00010226
Iteration 10/1000 | Loss: 0.00009968
Iteration 11/1000 | Loss: 0.00009798
Iteration 12/1000 | Loss: 0.00009681
Iteration 13/1000 | Loss: 0.00009588
Iteration 14/1000 | Loss: 0.00009488
Iteration 15/1000 | Loss: 0.00009395
Iteration 16/1000 | Loss: 0.00009315
Iteration 17/1000 | Loss: 0.00034439
Iteration 18/1000 | Loss: 0.00009619
Iteration 19/1000 | Loss: 0.00029161
Iteration 20/1000 | Loss: 0.00021447
Iteration 21/1000 | Loss: 0.00025647
Iteration 22/1000 | Loss: 0.00017724
Iteration 23/1000 | Loss: 0.00020383
Iteration 24/1000 | Loss: 0.00016044
Iteration 25/1000 | Loss: 0.00018271
Iteration 26/1000 | Loss: 0.00010509
Iteration 27/1000 | Loss: 0.00024841
Iteration 28/1000 | Loss: 0.00032550
Iteration 29/1000 | Loss: 0.00030535
Iteration 30/1000 | Loss: 0.00014769
Iteration 31/1000 | Loss: 0.00009277
Iteration 32/1000 | Loss: 0.00009145
Iteration 33/1000 | Loss: 0.00008941
Iteration 34/1000 | Loss: 0.00039980
Iteration 35/1000 | Loss: 0.00008918
Iteration 36/1000 | Loss: 0.00008686
Iteration 37/1000 | Loss: 0.00008540
Iteration 38/1000 | Loss: 0.00044456
Iteration 39/1000 | Loss: 0.00037062
Iteration 40/1000 | Loss: 0.00050811
Iteration 41/1000 | Loss: 0.00008692
Iteration 42/1000 | Loss: 0.00008400
Iteration 43/1000 | Loss: 0.00008265
Iteration 44/1000 | Loss: 0.00153475
Iteration 45/1000 | Loss: 0.00135031
Iteration 46/1000 | Loss: 0.00011842
Iteration 47/1000 | Loss: 0.00084472
Iteration 48/1000 | Loss: 0.00009467
Iteration 49/1000 | Loss: 0.00062190
Iteration 50/1000 | Loss: 0.00259277
Iteration 51/1000 | Loss: 0.00009849
Iteration 52/1000 | Loss: 0.00008374
Iteration 53/1000 | Loss: 0.00108115
Iteration 54/1000 | Loss: 0.00270950
Iteration 55/1000 | Loss: 0.00010047
Iteration 56/1000 | Loss: 0.00007875
Iteration 57/1000 | Loss: 0.00007117
Iteration 58/1000 | Loss: 0.00006681
Iteration 59/1000 | Loss: 0.00006383
Iteration 60/1000 | Loss: 0.00056364
Iteration 61/1000 | Loss: 0.00008303
Iteration 62/1000 | Loss: 0.00006060
Iteration 63/1000 | Loss: 0.00005793
Iteration 64/1000 | Loss: 0.00005632
Iteration 65/1000 | Loss: 0.00057802
Iteration 66/1000 | Loss: 0.00008457
Iteration 67/1000 | Loss: 0.00005508
Iteration 68/1000 | Loss: 0.00005272
Iteration 69/1000 | Loss: 0.00005154
Iteration 70/1000 | Loss: 0.00005037
Iteration 71/1000 | Loss: 0.00004971
Iteration 72/1000 | Loss: 0.00004919
Iteration 73/1000 | Loss: 0.00004868
Iteration 74/1000 | Loss: 0.00004819
Iteration 75/1000 | Loss: 0.00062054
Iteration 76/1000 | Loss: 0.00027160
Iteration 77/1000 | Loss: 0.00004796
Iteration 78/1000 | Loss: 0.00004772
Iteration 79/1000 | Loss: 0.00120293
Iteration 80/1000 | Loss: 0.00070933
Iteration 81/1000 | Loss: 0.00017386
Iteration 82/1000 | Loss: 0.00004987
Iteration 83/1000 | Loss: 0.00031433
Iteration 84/1000 | Loss: 0.00015018
Iteration 85/1000 | Loss: 0.00004634
Iteration 86/1000 | Loss: 0.00004535
Iteration 87/1000 | Loss: 0.00004478
Iteration 88/1000 | Loss: 0.00004429
Iteration 89/1000 | Loss: 0.00004395
Iteration 90/1000 | Loss: 0.00004371
Iteration 91/1000 | Loss: 0.00004346
Iteration 92/1000 | Loss: 0.00004341
Iteration 93/1000 | Loss: 0.00004336
Iteration 94/1000 | Loss: 0.00004336
Iteration 95/1000 | Loss: 0.00004335
Iteration 96/1000 | Loss: 0.00004335
Iteration 97/1000 | Loss: 0.00004335
Iteration 98/1000 | Loss: 0.00004335
Iteration 99/1000 | Loss: 0.00004334
Iteration 100/1000 | Loss: 0.00004334
Iteration 101/1000 | Loss: 0.00004333
Iteration 102/1000 | Loss: 0.00004333
Iteration 103/1000 | Loss: 0.00004333
Iteration 104/1000 | Loss: 0.00004333
Iteration 105/1000 | Loss: 0.00004332
Iteration 106/1000 | Loss: 0.00004332
Iteration 107/1000 | Loss: 0.00004330
Iteration 108/1000 | Loss: 0.00004330
Iteration 109/1000 | Loss: 0.00004328
Iteration 110/1000 | Loss: 0.00004327
Iteration 111/1000 | Loss: 0.00004325
Iteration 112/1000 | Loss: 0.00004324
Iteration 113/1000 | Loss: 0.00004324
Iteration 114/1000 | Loss: 0.00004320
Iteration 115/1000 | Loss: 0.00004314
Iteration 116/1000 | Loss: 0.00004311
Iteration 117/1000 | Loss: 0.00004311
Iteration 118/1000 | Loss: 0.00004311
Iteration 119/1000 | Loss: 0.00004310
Iteration 120/1000 | Loss: 0.00004307
Iteration 121/1000 | Loss: 0.00004307
Iteration 122/1000 | Loss: 0.00004307
Iteration 123/1000 | Loss: 0.00004306
Iteration 124/1000 | Loss: 0.00004306
Iteration 125/1000 | Loss: 0.00004304
Iteration 126/1000 | Loss: 0.00004304
Iteration 127/1000 | Loss: 0.00004304
Iteration 128/1000 | Loss: 0.00004304
Iteration 129/1000 | Loss: 0.00004304
Iteration 130/1000 | Loss: 0.00004304
Iteration 131/1000 | Loss: 0.00004304
Iteration 132/1000 | Loss: 0.00004303
Iteration 133/1000 | Loss: 0.00004303
Iteration 134/1000 | Loss: 0.00004303
Iteration 135/1000 | Loss: 0.00004303
Iteration 136/1000 | Loss: 0.00004300
Iteration 137/1000 | Loss: 0.00004299
Iteration 138/1000 | Loss: 0.00004299
Iteration 139/1000 | Loss: 0.00004298
Iteration 140/1000 | Loss: 0.00004298
Iteration 141/1000 | Loss: 0.00004298
Iteration 142/1000 | Loss: 0.00004298
Iteration 143/1000 | Loss: 0.00004298
Iteration 144/1000 | Loss: 0.00004297
Iteration 145/1000 | Loss: 0.00004297
Iteration 146/1000 | Loss: 0.00004297
Iteration 147/1000 | Loss: 0.00004296
Iteration 148/1000 | Loss: 0.00004296
Iteration 149/1000 | Loss: 0.00004296
Iteration 150/1000 | Loss: 0.00004296
Iteration 151/1000 | Loss: 0.00004296
Iteration 152/1000 | Loss: 0.00004295
Iteration 153/1000 | Loss: 0.00004295
Iteration 154/1000 | Loss: 0.00004295
Iteration 155/1000 | Loss: 0.00004295
Iteration 156/1000 | Loss: 0.00004295
Iteration 157/1000 | Loss: 0.00004295
Iteration 158/1000 | Loss: 0.00004294
Iteration 159/1000 | Loss: 0.00004294
Iteration 160/1000 | Loss: 0.00004292
Iteration 161/1000 | Loss: 0.00004291
Iteration 162/1000 | Loss: 0.00004290
Iteration 163/1000 | Loss: 0.00004289
Iteration 164/1000 | Loss: 0.00004288
Iteration 165/1000 | Loss: 0.00004287
Iteration 166/1000 | Loss: 0.00004287
Iteration 167/1000 | Loss: 0.00004286
Iteration 168/1000 | Loss: 0.00004286
Iteration 169/1000 | Loss: 0.00004286
Iteration 170/1000 | Loss: 0.00004285
Iteration 171/1000 | Loss: 0.00004285
Iteration 172/1000 | Loss: 0.00004285
Iteration 173/1000 | Loss: 0.00004285
Iteration 174/1000 | Loss: 0.00004285
Iteration 175/1000 | Loss: 0.00004284
Iteration 176/1000 | Loss: 0.00004284
Iteration 177/1000 | Loss: 0.00004284
Iteration 178/1000 | Loss: 0.00004284
Iteration 179/1000 | Loss: 0.00004284
Iteration 180/1000 | Loss: 0.00004283
Iteration 181/1000 | Loss: 0.00004283
Iteration 182/1000 | Loss: 0.00004282
Iteration 183/1000 | Loss: 0.00004282
Iteration 184/1000 | Loss: 0.00004282
Iteration 185/1000 | Loss: 0.00004281
Iteration 186/1000 | Loss: 0.00004281
Iteration 187/1000 | Loss: 0.00004281
Iteration 188/1000 | Loss: 0.00004280
Iteration 189/1000 | Loss: 0.00004280
Iteration 190/1000 | Loss: 0.00004280
Iteration 191/1000 | Loss: 0.00004279
Iteration 192/1000 | Loss: 0.00004279
Iteration 193/1000 | Loss: 0.00004278
Iteration 194/1000 | Loss: 0.00004277
Iteration 195/1000 | Loss: 0.00004277
Iteration 196/1000 | Loss: 0.00062923
Iteration 197/1000 | Loss: 0.00021715
Iteration 198/1000 | Loss: 0.00004400
Iteration 199/1000 | Loss: 0.00004305
Iteration 200/1000 | Loss: 0.00004286
Iteration 201/1000 | Loss: 0.00004284
Iteration 202/1000 | Loss: 0.00004281
Iteration 203/1000 | Loss: 0.00004281
Iteration 204/1000 | Loss: 0.00004277
Iteration 205/1000 | Loss: 0.00004277
Iteration 206/1000 | Loss: 0.00004276
Iteration 207/1000 | Loss: 0.00004276
Iteration 208/1000 | Loss: 0.00004276
Iteration 209/1000 | Loss: 0.00004275
Iteration 210/1000 | Loss: 0.00004275
Iteration 211/1000 | Loss: 0.00004275
Iteration 212/1000 | Loss: 0.00004275
Iteration 213/1000 | Loss: 0.00004274
Iteration 214/1000 | Loss: 0.00004274
Iteration 215/1000 | Loss: 0.00004274
Iteration 216/1000 | Loss: 0.00004274
Iteration 217/1000 | Loss: 0.00004274
Iteration 218/1000 | Loss: 0.00004273
Iteration 219/1000 | Loss: 0.00004273
Iteration 220/1000 | Loss: 0.00004273
Iteration 221/1000 | Loss: 0.00004273
Iteration 222/1000 | Loss: 0.00004273
Iteration 223/1000 | Loss: 0.00004272
Iteration 224/1000 | Loss: 0.00004272
Iteration 225/1000 | Loss: 0.00004272
Iteration 226/1000 | Loss: 0.00004272
Iteration 227/1000 | Loss: 0.00004272
Iteration 228/1000 | Loss: 0.00004272
Iteration 229/1000 | Loss: 0.00004271
Iteration 230/1000 | Loss: 0.00004271
Iteration 231/1000 | Loss: 0.00004271
Iteration 232/1000 | Loss: 0.00004270
Iteration 233/1000 | Loss: 0.00004270
Iteration 234/1000 | Loss: 0.00004270
Iteration 235/1000 | Loss: 0.00004270
Iteration 236/1000 | Loss: 0.00004269
Iteration 237/1000 | Loss: 0.00004269
Iteration 238/1000 | Loss: 0.00004269
Iteration 239/1000 | Loss: 0.00004269
Iteration 240/1000 | Loss: 0.00004269
Iteration 241/1000 | Loss: 0.00004268
Iteration 242/1000 | Loss: 0.00004268
Iteration 243/1000 | Loss: 0.00004268
Iteration 244/1000 | Loss: 0.00004268
Iteration 245/1000 | Loss: 0.00004268
Iteration 246/1000 | Loss: 0.00004268
Iteration 247/1000 | Loss: 0.00004268
Iteration 248/1000 | Loss: 0.00004268
Iteration 249/1000 | Loss: 0.00004267
Iteration 250/1000 | Loss: 0.00004267
Iteration 251/1000 | Loss: 0.00004267
Iteration 252/1000 | Loss: 0.00004267
Iteration 253/1000 | Loss: 0.00004267
Iteration 254/1000 | Loss: 0.00004267
Iteration 255/1000 | Loss: 0.00004267
Iteration 256/1000 | Loss: 0.00076119
Iteration 257/1000 | Loss: 0.00139195
Iteration 258/1000 | Loss: 0.00027082
Iteration 259/1000 | Loss: 0.00036452
Iteration 260/1000 | Loss: 0.00006590
Iteration 261/1000 | Loss: 0.00006308
Iteration 262/1000 | Loss: 0.00004218
Iteration 263/1000 | Loss: 0.00004130
Iteration 264/1000 | Loss: 0.00004063
Iteration 265/1000 | Loss: 0.00004000
Iteration 266/1000 | Loss: 0.00003947
Iteration 267/1000 | Loss: 0.00003905
Iteration 268/1000 | Loss: 0.00003833
Iteration 269/1000 | Loss: 0.00003801
Iteration 270/1000 | Loss: 0.00003775
Iteration 271/1000 | Loss: 0.00003769
Iteration 272/1000 | Loss: 0.00003747
Iteration 273/1000 | Loss: 0.00003746
Iteration 274/1000 | Loss: 0.00003743
Iteration 275/1000 | Loss: 0.00003743
Iteration 276/1000 | Loss: 0.00003742
Iteration 277/1000 | Loss: 0.00003741
Iteration 278/1000 | Loss: 0.00003740
Iteration 279/1000 | Loss: 0.00003739
Iteration 280/1000 | Loss: 0.00003737
Iteration 281/1000 | Loss: 0.00003736
Iteration 282/1000 | Loss: 0.00003735
Iteration 283/1000 | Loss: 0.00003728
Iteration 284/1000 | Loss: 0.00003727
Iteration 285/1000 | Loss: 0.00003726
Iteration 286/1000 | Loss: 0.00003726
Iteration 287/1000 | Loss: 0.00003725
Iteration 288/1000 | Loss: 0.00003725
Iteration 289/1000 | Loss: 0.00003725
Iteration 290/1000 | Loss: 0.00003724
Iteration 291/1000 | Loss: 0.00003724
Iteration 292/1000 | Loss: 0.00003724
Iteration 293/1000 | Loss: 0.00003723
Iteration 294/1000 | Loss: 0.00003723
Iteration 295/1000 | Loss: 0.00003723
Iteration 296/1000 | Loss: 0.00003722
Iteration 297/1000 | Loss: 0.00003721
Iteration 298/1000 | Loss: 0.00003721
Iteration 299/1000 | Loss: 0.00003720
Iteration 300/1000 | Loss: 0.00003720
Iteration 301/1000 | Loss: 0.00003720
Iteration 302/1000 | Loss: 0.00003720
Iteration 303/1000 | Loss: 0.00003719
Iteration 304/1000 | Loss: 0.00003719
Iteration 305/1000 | Loss: 0.00003719
Iteration 306/1000 | Loss: 0.00003718
Iteration 307/1000 | Loss: 0.00003718
Iteration 308/1000 | Loss: 0.00003718
Iteration 309/1000 | Loss: 0.00003718
Iteration 310/1000 | Loss: 0.00003718
Iteration 311/1000 | Loss: 0.00003718
Iteration 312/1000 | Loss: 0.00003717
Iteration 313/1000 | Loss: 0.00003717
Iteration 314/1000 | Loss: 0.00003717
Iteration 315/1000 | Loss: 0.00003717
Iteration 316/1000 | Loss: 0.00003717
Iteration 317/1000 | Loss: 0.00003716
Iteration 318/1000 | Loss: 0.00003716
Iteration 319/1000 | Loss: 0.00003716
Iteration 320/1000 | Loss: 0.00003716
Iteration 321/1000 | Loss: 0.00003715
Iteration 322/1000 | Loss: 0.00003715
Iteration 323/1000 | Loss: 0.00003715
Iteration 324/1000 | Loss: 0.00003714
Iteration 325/1000 | Loss: 0.00003714
Iteration 326/1000 | Loss: 0.00003714
Iteration 327/1000 | Loss: 0.00003713
Iteration 328/1000 | Loss: 0.00003713
Iteration 329/1000 | Loss: 0.00003713
Iteration 330/1000 | Loss: 0.00003712
Iteration 331/1000 | Loss: 0.00003712
Iteration 332/1000 | Loss: 0.00003712
Iteration 333/1000 | Loss: 0.00003710
Iteration 334/1000 | Loss: 0.00003710
Iteration 335/1000 | Loss: 0.00003710
Iteration 336/1000 | Loss: 0.00003709
Iteration 337/1000 | Loss: 0.00003709
Iteration 338/1000 | Loss: 0.00003708
Iteration 339/1000 | Loss: 0.00003708
Iteration 340/1000 | Loss: 0.00003708
Iteration 341/1000 | Loss: 0.00003708
Iteration 342/1000 | Loss: 0.00003708
Iteration 343/1000 | Loss: 0.00003708
Iteration 344/1000 | Loss: 0.00003708
Iteration 345/1000 | Loss: 0.00003708
Iteration 346/1000 | Loss: 0.00003708
Iteration 347/1000 | Loss: 0.00003708
Iteration 348/1000 | Loss: 0.00003708
Iteration 349/1000 | Loss: 0.00003707
Iteration 350/1000 | Loss: 0.00003707
Iteration 351/1000 | Loss: 0.00003707
Iteration 352/1000 | Loss: 0.00003707
Iteration 353/1000 | Loss: 0.00003707
Iteration 354/1000 | Loss: 0.00003706
Iteration 355/1000 | Loss: 0.00003706
Iteration 356/1000 | Loss: 0.00003706
Iteration 357/1000 | Loss: 0.00003706
Iteration 358/1000 | Loss: 0.00003706
Iteration 359/1000 | Loss: 0.00003706
Iteration 360/1000 | Loss: 0.00003706
Iteration 361/1000 | Loss: 0.00003706
Iteration 362/1000 | Loss: 0.00003706
Iteration 363/1000 | Loss: 0.00003706
Iteration 364/1000 | Loss: 0.00003706
Iteration 365/1000 | Loss: 0.00003705
Iteration 366/1000 | Loss: 0.00003705
Iteration 367/1000 | Loss: 0.00003705
Iteration 368/1000 | Loss: 0.00003705
Iteration 369/1000 | Loss: 0.00003705
Iteration 370/1000 | Loss: 0.00003705
Iteration 371/1000 | Loss: 0.00003705
Iteration 372/1000 | Loss: 0.00003705
Iteration 373/1000 | Loss: 0.00003705
Iteration 374/1000 | Loss: 0.00003705
Iteration 375/1000 | Loss: 0.00003705
Iteration 376/1000 | Loss: 0.00003704
Iteration 377/1000 | Loss: 0.00003704
Iteration 378/1000 | Loss: 0.00003704
Iteration 379/1000 | Loss: 0.00003704
Iteration 380/1000 | Loss: 0.00003704
Iteration 381/1000 | Loss: 0.00003704
Iteration 382/1000 | Loss: 0.00003704
Iteration 383/1000 | Loss: 0.00003704
Iteration 384/1000 | Loss: 0.00003704
Iteration 385/1000 | Loss: 0.00003703
Iteration 386/1000 | Loss: 0.00003703
Iteration 387/1000 | Loss: 0.00003703
Iteration 388/1000 | Loss: 0.00003703
Iteration 389/1000 | Loss: 0.00003703
Iteration 390/1000 | Loss: 0.00003703
Iteration 391/1000 | Loss: 0.00003703
Iteration 392/1000 | Loss: 0.00003703
Iteration 393/1000 | Loss: 0.00003703
Iteration 394/1000 | Loss: 0.00003702
Iteration 395/1000 | Loss: 0.00003702
Iteration 396/1000 | Loss: 0.00003702
Iteration 397/1000 | Loss: 0.00003702
Iteration 398/1000 | Loss: 0.00003701
Iteration 399/1000 | Loss: 0.00003701
Iteration 400/1000 | Loss: 0.00003701
Iteration 401/1000 | Loss: 0.00003701
Iteration 402/1000 | Loss: 0.00003701
Iteration 403/1000 | Loss: 0.00003700
Iteration 404/1000 | Loss: 0.00003700
Iteration 405/1000 | Loss: 0.00003700
Iteration 406/1000 | Loss: 0.00003700
Iteration 407/1000 | Loss: 0.00003700
Iteration 408/1000 | Loss: 0.00003700
Iteration 409/1000 | Loss: 0.00003700
Iteration 410/1000 | Loss: 0.00003700
Iteration 411/1000 | Loss: 0.00003700
Iteration 412/1000 | Loss: 0.00003700
Iteration 413/1000 | Loss: 0.00003700
Iteration 414/1000 | Loss: 0.00003699
Iteration 415/1000 | Loss: 0.00003699
Iteration 416/1000 | Loss: 0.00003699
Iteration 417/1000 | Loss: 0.00003699
Iteration 418/1000 | Loss: 0.00003699
Iteration 419/1000 | Loss: 0.00003699
Iteration 420/1000 | Loss: 0.00003698
Iteration 421/1000 | Loss: 0.00003698
Iteration 422/1000 | Loss: 0.00003698
Iteration 423/1000 | Loss: 0.00003698
Iteration 424/1000 | Loss: 0.00003698
Iteration 425/1000 | Loss: 0.00003698
Iteration 426/1000 | Loss: 0.00003698
Iteration 427/1000 | Loss: 0.00003698
Iteration 428/1000 | Loss: 0.00003697
Iteration 429/1000 | Loss: 0.00003697
Iteration 430/1000 | Loss: 0.00003697
Iteration 431/1000 | Loss: 0.00003697
Iteration 432/1000 | Loss: 0.00003697
Iteration 433/1000 | Loss: 0.00003697
Iteration 434/1000 | Loss: 0.00003697
Iteration 435/1000 | Loss: 0.00003697
Iteration 436/1000 | Loss: 0.00003697
Iteration 437/1000 | Loss: 0.00003697
Iteration 438/1000 | Loss: 0.00003697
Iteration 439/1000 | Loss: 0.00003697
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 439. Stopping optimization.
Last 5 losses: [3.697202919283882e-05, 3.697202919283882e-05, 3.697202919283882e-05, 3.697202919283882e-05, 3.697202919283882e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.697202919283882e-05

Optimization complete. Final v2v error: 4.007958889007568 mm

Highest mean error: 11.462174415588379 mm for frame 58

Lowest mean error: 3.0540194511413574 mm for frame 26

Saving results

Total time: 202.78379440307617
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00849441
Iteration 2/25 | Loss: 0.00129178
Iteration 3/25 | Loss: 0.00123055
Iteration 4/25 | Loss: 0.00122135
Iteration 5/25 | Loss: 0.00122064
Iteration 6/25 | Loss: 0.00122064
Iteration 7/25 | Loss: 0.00122064
Iteration 8/25 | Loss: 0.00122064
Iteration 9/25 | Loss: 0.00122064
Iteration 10/25 | Loss: 0.00122064
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001220638514496386, 0.001220638514496386, 0.001220638514496386, 0.001220638514496386, 0.001220638514496386]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001220638514496386

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42256260
Iteration 2/25 | Loss: 0.00084205
Iteration 3/25 | Loss: 0.00084204
Iteration 4/25 | Loss: 0.00084204
Iteration 5/25 | Loss: 0.00084204
Iteration 6/25 | Loss: 0.00084204
Iteration 7/25 | Loss: 0.00084204
Iteration 8/25 | Loss: 0.00084204
Iteration 9/25 | Loss: 0.00084204
Iteration 10/25 | Loss: 0.00084204
Iteration 11/25 | Loss: 0.00084204
Iteration 12/25 | Loss: 0.00084204
Iteration 13/25 | Loss: 0.00084204
Iteration 14/25 | Loss: 0.00084204
Iteration 15/25 | Loss: 0.00084204
Iteration 16/25 | Loss: 0.00084204
Iteration 17/25 | Loss: 0.00084204
Iteration 18/25 | Loss: 0.00084204
Iteration 19/25 | Loss: 0.00084204
Iteration 20/25 | Loss: 0.00084204
Iteration 21/25 | Loss: 0.00084204
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.000842040462885052, 0.000842040462885052, 0.000842040462885052, 0.000842040462885052, 0.000842040462885052]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000842040462885052

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084204
Iteration 2/1000 | Loss: 0.00002956
Iteration 3/1000 | Loss: 0.00001841
Iteration 4/1000 | Loss: 0.00001562
Iteration 5/1000 | Loss: 0.00001473
Iteration 6/1000 | Loss: 0.00001407
Iteration 7/1000 | Loss: 0.00001362
Iteration 8/1000 | Loss: 0.00001361
Iteration 9/1000 | Loss: 0.00001354
Iteration 10/1000 | Loss: 0.00001352
Iteration 11/1000 | Loss: 0.00001352
Iteration 12/1000 | Loss: 0.00001332
Iteration 13/1000 | Loss: 0.00001311
Iteration 14/1000 | Loss: 0.00001300
Iteration 15/1000 | Loss: 0.00001295
Iteration 16/1000 | Loss: 0.00001294
Iteration 17/1000 | Loss: 0.00001293
Iteration 18/1000 | Loss: 0.00001290
Iteration 19/1000 | Loss: 0.00001289
Iteration 20/1000 | Loss: 0.00001289
Iteration 21/1000 | Loss: 0.00001288
Iteration 22/1000 | Loss: 0.00001288
Iteration 23/1000 | Loss: 0.00001287
Iteration 24/1000 | Loss: 0.00001287
Iteration 25/1000 | Loss: 0.00001282
Iteration 26/1000 | Loss: 0.00001282
Iteration 27/1000 | Loss: 0.00001281
Iteration 28/1000 | Loss: 0.00001281
Iteration 29/1000 | Loss: 0.00001281
Iteration 30/1000 | Loss: 0.00001278
Iteration 31/1000 | Loss: 0.00001277
Iteration 32/1000 | Loss: 0.00001277
Iteration 33/1000 | Loss: 0.00001276
Iteration 34/1000 | Loss: 0.00001276
Iteration 35/1000 | Loss: 0.00001275
Iteration 36/1000 | Loss: 0.00001273
Iteration 37/1000 | Loss: 0.00001273
Iteration 38/1000 | Loss: 0.00001273
Iteration 39/1000 | Loss: 0.00001272
Iteration 40/1000 | Loss: 0.00001272
Iteration 41/1000 | Loss: 0.00001272
Iteration 42/1000 | Loss: 0.00001272
Iteration 43/1000 | Loss: 0.00001271
Iteration 44/1000 | Loss: 0.00001271
Iteration 45/1000 | Loss: 0.00001271
Iteration 46/1000 | Loss: 0.00001271
Iteration 47/1000 | Loss: 0.00001271
Iteration 48/1000 | Loss: 0.00001271
Iteration 49/1000 | Loss: 0.00001271
Iteration 50/1000 | Loss: 0.00001271
Iteration 51/1000 | Loss: 0.00001271
Iteration 52/1000 | Loss: 0.00001270
Iteration 53/1000 | Loss: 0.00001270
Iteration 54/1000 | Loss: 0.00001270
Iteration 55/1000 | Loss: 0.00001269
Iteration 56/1000 | Loss: 0.00001269
Iteration 57/1000 | Loss: 0.00001269
Iteration 58/1000 | Loss: 0.00001269
Iteration 59/1000 | Loss: 0.00001268
Iteration 60/1000 | Loss: 0.00001268
Iteration 61/1000 | Loss: 0.00001268
Iteration 62/1000 | Loss: 0.00001267
Iteration 63/1000 | Loss: 0.00001267
Iteration 64/1000 | Loss: 0.00001265
Iteration 65/1000 | Loss: 0.00001265
Iteration 66/1000 | Loss: 0.00001264
Iteration 67/1000 | Loss: 0.00001263
Iteration 68/1000 | Loss: 0.00001263
Iteration 69/1000 | Loss: 0.00001263
Iteration 70/1000 | Loss: 0.00001263
Iteration 71/1000 | Loss: 0.00001263
Iteration 72/1000 | Loss: 0.00001263
Iteration 73/1000 | Loss: 0.00001263
Iteration 74/1000 | Loss: 0.00001263
Iteration 75/1000 | Loss: 0.00001262
Iteration 76/1000 | Loss: 0.00001262
Iteration 77/1000 | Loss: 0.00001262
Iteration 78/1000 | Loss: 0.00001262
Iteration 79/1000 | Loss: 0.00001262
Iteration 80/1000 | Loss: 0.00001261
Iteration 81/1000 | Loss: 0.00001261
Iteration 82/1000 | Loss: 0.00001260
Iteration 83/1000 | Loss: 0.00001260
Iteration 84/1000 | Loss: 0.00001259
Iteration 85/1000 | Loss: 0.00001259
Iteration 86/1000 | Loss: 0.00001259
Iteration 87/1000 | Loss: 0.00001258
Iteration 88/1000 | Loss: 0.00001258
Iteration 89/1000 | Loss: 0.00001257
Iteration 90/1000 | Loss: 0.00001256
Iteration 91/1000 | Loss: 0.00001255
Iteration 92/1000 | Loss: 0.00001255
Iteration 93/1000 | Loss: 0.00001254
Iteration 94/1000 | Loss: 0.00001254
Iteration 95/1000 | Loss: 0.00001254
Iteration 96/1000 | Loss: 0.00001254
Iteration 97/1000 | Loss: 0.00001253
Iteration 98/1000 | Loss: 0.00001253
Iteration 99/1000 | Loss: 0.00001253
Iteration 100/1000 | Loss: 0.00001253
Iteration 101/1000 | Loss: 0.00001253
Iteration 102/1000 | Loss: 0.00001253
Iteration 103/1000 | Loss: 0.00001253
Iteration 104/1000 | Loss: 0.00001253
Iteration 105/1000 | Loss: 0.00001253
Iteration 106/1000 | Loss: 0.00001253
Iteration 107/1000 | Loss: 0.00001252
Iteration 108/1000 | Loss: 0.00001252
Iteration 109/1000 | Loss: 0.00001252
Iteration 110/1000 | Loss: 0.00001252
Iteration 111/1000 | Loss: 0.00001252
Iteration 112/1000 | Loss: 0.00001252
Iteration 113/1000 | Loss: 0.00001252
Iteration 114/1000 | Loss: 0.00001252
Iteration 115/1000 | Loss: 0.00001252
Iteration 116/1000 | Loss: 0.00001251
Iteration 117/1000 | Loss: 0.00001251
Iteration 118/1000 | Loss: 0.00001251
Iteration 119/1000 | Loss: 0.00001251
Iteration 120/1000 | Loss: 0.00001251
Iteration 121/1000 | Loss: 0.00001251
Iteration 122/1000 | Loss: 0.00001251
Iteration 123/1000 | Loss: 0.00001250
Iteration 124/1000 | Loss: 0.00001250
Iteration 125/1000 | Loss: 0.00001250
Iteration 126/1000 | Loss: 0.00001250
Iteration 127/1000 | Loss: 0.00001250
Iteration 128/1000 | Loss: 0.00001249
Iteration 129/1000 | Loss: 0.00001249
Iteration 130/1000 | Loss: 0.00001249
Iteration 131/1000 | Loss: 0.00001249
Iteration 132/1000 | Loss: 0.00001249
Iteration 133/1000 | Loss: 0.00001249
Iteration 134/1000 | Loss: 0.00001249
Iteration 135/1000 | Loss: 0.00001249
Iteration 136/1000 | Loss: 0.00001249
Iteration 137/1000 | Loss: 0.00001249
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.2489556866057683e-05, 1.2489556866057683e-05, 1.2489556866057683e-05, 1.2489556866057683e-05, 1.2489556866057683e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2489556866057683e-05

Optimization complete. Final v2v error: 3.0069046020507812 mm

Highest mean error: 3.4093053340911865 mm for frame 192

Lowest mean error: 2.671780824661255 mm for frame 106

Saving results

Total time: 33.737732887268066
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00451763
Iteration 2/25 | Loss: 0.00140731
Iteration 3/25 | Loss: 0.00126224
Iteration 4/25 | Loss: 0.00124704
Iteration 5/25 | Loss: 0.00124222
Iteration 6/25 | Loss: 0.00124187
Iteration 7/25 | Loss: 0.00124187
Iteration 8/25 | Loss: 0.00124187
Iteration 9/25 | Loss: 0.00124187
Iteration 10/25 | Loss: 0.00124187
Iteration 11/25 | Loss: 0.00124187
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012418734841048717, 0.0012418734841048717, 0.0012418734841048717, 0.0012418734841048717, 0.0012418734841048717]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012418734841048717

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45730555
Iteration 2/25 | Loss: 0.00076420
Iteration 3/25 | Loss: 0.00076420
Iteration 4/25 | Loss: 0.00076420
Iteration 5/25 | Loss: 0.00076420
Iteration 6/25 | Loss: 0.00076420
Iteration 7/25 | Loss: 0.00076420
Iteration 8/25 | Loss: 0.00076420
Iteration 9/25 | Loss: 0.00076420
Iteration 10/25 | Loss: 0.00076420
Iteration 11/25 | Loss: 0.00076420
Iteration 12/25 | Loss: 0.00076420
Iteration 13/25 | Loss: 0.00076420
Iteration 14/25 | Loss: 0.00076420
Iteration 15/25 | Loss: 0.00076420
Iteration 16/25 | Loss: 0.00076420
Iteration 17/25 | Loss: 0.00076420
Iteration 18/25 | Loss: 0.00076420
Iteration 19/25 | Loss: 0.00076420
Iteration 20/25 | Loss: 0.00076420
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007641954580321908, 0.0007641954580321908, 0.0007641954580321908, 0.0007641954580321908, 0.0007641954580321908]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007641954580321908

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076420
Iteration 2/1000 | Loss: 0.00003323
Iteration 3/1000 | Loss: 0.00002646
Iteration 4/1000 | Loss: 0.00002499
Iteration 5/1000 | Loss: 0.00002418
Iteration 6/1000 | Loss: 0.00002331
Iteration 7/1000 | Loss: 0.00002277
Iteration 8/1000 | Loss: 0.00002234
Iteration 9/1000 | Loss: 0.00002189
Iteration 10/1000 | Loss: 0.00002167
Iteration 11/1000 | Loss: 0.00002150
Iteration 12/1000 | Loss: 0.00002139
Iteration 13/1000 | Loss: 0.00002128
Iteration 14/1000 | Loss: 0.00002127
Iteration 15/1000 | Loss: 0.00002124
Iteration 16/1000 | Loss: 0.00002123
Iteration 17/1000 | Loss: 0.00002122
Iteration 18/1000 | Loss: 0.00002121
Iteration 19/1000 | Loss: 0.00002118
Iteration 20/1000 | Loss: 0.00002118
Iteration 21/1000 | Loss: 0.00002117
Iteration 22/1000 | Loss: 0.00002117
Iteration 23/1000 | Loss: 0.00002116
Iteration 24/1000 | Loss: 0.00002116
Iteration 25/1000 | Loss: 0.00002115
Iteration 26/1000 | Loss: 0.00002115
Iteration 27/1000 | Loss: 0.00002115
Iteration 28/1000 | Loss: 0.00002115
Iteration 29/1000 | Loss: 0.00002115
Iteration 30/1000 | Loss: 0.00002115
Iteration 31/1000 | Loss: 0.00002115
Iteration 32/1000 | Loss: 0.00002114
Iteration 33/1000 | Loss: 0.00002114
Iteration 34/1000 | Loss: 0.00002114
Iteration 35/1000 | Loss: 0.00002114
Iteration 36/1000 | Loss: 0.00002114
Iteration 37/1000 | Loss: 0.00002114
Iteration 38/1000 | Loss: 0.00002113
Iteration 39/1000 | Loss: 0.00002113
Iteration 40/1000 | Loss: 0.00002113
Iteration 41/1000 | Loss: 0.00002112
Iteration 42/1000 | Loss: 0.00002112
Iteration 43/1000 | Loss: 0.00002112
Iteration 44/1000 | Loss: 0.00002112
Iteration 45/1000 | Loss: 0.00002112
Iteration 46/1000 | Loss: 0.00002112
Iteration 47/1000 | Loss: 0.00002112
Iteration 48/1000 | Loss: 0.00002111
Iteration 49/1000 | Loss: 0.00002111
Iteration 50/1000 | Loss: 0.00002111
Iteration 51/1000 | Loss: 0.00002111
Iteration 52/1000 | Loss: 0.00002111
Iteration 53/1000 | Loss: 0.00002111
Iteration 54/1000 | Loss: 0.00002111
Iteration 55/1000 | Loss: 0.00002110
Iteration 56/1000 | Loss: 0.00002110
Iteration 57/1000 | Loss: 0.00002110
Iteration 58/1000 | Loss: 0.00002110
Iteration 59/1000 | Loss: 0.00002110
Iteration 60/1000 | Loss: 0.00002110
Iteration 61/1000 | Loss: 0.00002109
Iteration 62/1000 | Loss: 0.00002109
Iteration 63/1000 | Loss: 0.00002109
Iteration 64/1000 | Loss: 0.00002109
Iteration 65/1000 | Loss: 0.00002109
Iteration 66/1000 | Loss: 0.00002108
Iteration 67/1000 | Loss: 0.00002108
Iteration 68/1000 | Loss: 0.00002108
Iteration 69/1000 | Loss: 0.00002108
Iteration 70/1000 | Loss: 0.00002108
Iteration 71/1000 | Loss: 0.00002107
Iteration 72/1000 | Loss: 0.00002107
Iteration 73/1000 | Loss: 0.00002107
Iteration 74/1000 | Loss: 0.00002107
Iteration 75/1000 | Loss: 0.00002106
Iteration 76/1000 | Loss: 0.00002106
Iteration 77/1000 | Loss: 0.00002106
Iteration 78/1000 | Loss: 0.00002105
Iteration 79/1000 | Loss: 0.00002105
Iteration 80/1000 | Loss: 0.00002104
Iteration 81/1000 | Loss: 0.00002104
Iteration 82/1000 | Loss: 0.00002101
Iteration 83/1000 | Loss: 0.00002098
Iteration 84/1000 | Loss: 0.00002098
Iteration 85/1000 | Loss: 0.00002098
Iteration 86/1000 | Loss: 0.00002098
Iteration 87/1000 | Loss: 0.00002098
Iteration 88/1000 | Loss: 0.00002098
Iteration 89/1000 | Loss: 0.00002098
Iteration 90/1000 | Loss: 0.00002098
Iteration 91/1000 | Loss: 0.00002098
Iteration 92/1000 | Loss: 0.00002097
Iteration 93/1000 | Loss: 0.00002097
Iteration 94/1000 | Loss: 0.00002097
Iteration 95/1000 | Loss: 0.00002097
Iteration 96/1000 | Loss: 0.00002097
Iteration 97/1000 | Loss: 0.00002096
Iteration 98/1000 | Loss: 0.00002095
Iteration 99/1000 | Loss: 0.00002095
Iteration 100/1000 | Loss: 0.00002094
Iteration 101/1000 | Loss: 0.00002094
Iteration 102/1000 | Loss: 0.00002094
Iteration 103/1000 | Loss: 0.00002094
Iteration 104/1000 | Loss: 0.00002092
Iteration 105/1000 | Loss: 0.00002092
Iteration 106/1000 | Loss: 0.00002092
Iteration 107/1000 | Loss: 0.00002091
Iteration 108/1000 | Loss: 0.00002091
Iteration 109/1000 | Loss: 0.00002089
Iteration 110/1000 | Loss: 0.00002089
Iteration 111/1000 | Loss: 0.00002089
Iteration 112/1000 | Loss: 0.00002088
Iteration 113/1000 | Loss: 0.00002088
Iteration 114/1000 | Loss: 0.00002088
Iteration 115/1000 | Loss: 0.00002088
Iteration 116/1000 | Loss: 0.00002088
Iteration 117/1000 | Loss: 0.00002087
Iteration 118/1000 | Loss: 0.00002087
Iteration 119/1000 | Loss: 0.00002087
Iteration 120/1000 | Loss: 0.00002086
Iteration 121/1000 | Loss: 0.00002086
Iteration 122/1000 | Loss: 0.00002086
Iteration 123/1000 | Loss: 0.00002086
Iteration 124/1000 | Loss: 0.00002086
Iteration 125/1000 | Loss: 0.00002086
Iteration 126/1000 | Loss: 0.00002086
Iteration 127/1000 | Loss: 0.00002086
Iteration 128/1000 | Loss: 0.00002085
Iteration 129/1000 | Loss: 0.00002085
Iteration 130/1000 | Loss: 0.00002085
Iteration 131/1000 | Loss: 0.00002085
Iteration 132/1000 | Loss: 0.00002085
Iteration 133/1000 | Loss: 0.00002085
Iteration 134/1000 | Loss: 0.00002085
Iteration 135/1000 | Loss: 0.00002085
Iteration 136/1000 | Loss: 0.00002085
Iteration 137/1000 | Loss: 0.00002085
Iteration 138/1000 | Loss: 0.00002085
Iteration 139/1000 | Loss: 0.00002085
Iteration 140/1000 | Loss: 0.00002085
Iteration 141/1000 | Loss: 0.00002085
Iteration 142/1000 | Loss: 0.00002085
Iteration 143/1000 | Loss: 0.00002085
Iteration 144/1000 | Loss: 0.00002085
Iteration 145/1000 | Loss: 0.00002085
Iteration 146/1000 | Loss: 0.00002085
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [2.0849967768299393e-05, 2.0849967768299393e-05, 2.0849967768299393e-05, 2.0849967768299393e-05, 2.0849967768299393e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0849967768299393e-05

Optimization complete. Final v2v error: 3.8596246242523193 mm

Highest mean error: 4.408663272857666 mm for frame 143

Lowest mean error: 3.4495983123779297 mm for frame 5

Saving results

Total time: 40.88081741333008
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00837620
Iteration 2/25 | Loss: 0.00129983
Iteration 3/25 | Loss: 0.00121394
Iteration 4/25 | Loss: 0.00120680
Iteration 5/25 | Loss: 0.00120581
Iteration 6/25 | Loss: 0.00120581
Iteration 7/25 | Loss: 0.00120581
Iteration 8/25 | Loss: 0.00120581
Iteration 9/25 | Loss: 0.00120581
Iteration 10/25 | Loss: 0.00120581
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012058106949552894, 0.0012058106949552894, 0.0012058106949552894, 0.0012058106949552894, 0.0012058106949552894]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012058106949552894

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.88294983
Iteration 2/25 | Loss: 0.00076357
Iteration 3/25 | Loss: 0.00076357
Iteration 4/25 | Loss: 0.00076357
Iteration 5/25 | Loss: 0.00076357
Iteration 6/25 | Loss: 0.00076357
Iteration 7/25 | Loss: 0.00076357
Iteration 8/25 | Loss: 0.00076357
Iteration 9/25 | Loss: 0.00076357
Iteration 10/25 | Loss: 0.00076357
Iteration 11/25 | Loss: 0.00076357
Iteration 12/25 | Loss: 0.00076357
Iteration 13/25 | Loss: 0.00076357
Iteration 14/25 | Loss: 0.00076357
Iteration 15/25 | Loss: 0.00076357
Iteration 16/25 | Loss: 0.00076357
Iteration 17/25 | Loss: 0.00076357
Iteration 18/25 | Loss: 0.00076357
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007635695510543883, 0.0007635695510543883, 0.0007635695510543883, 0.0007635695510543883, 0.0007635695510543883]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007635695510543883

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076357
Iteration 2/1000 | Loss: 0.00002226
Iteration 3/1000 | Loss: 0.00001563
Iteration 4/1000 | Loss: 0.00001412
Iteration 5/1000 | Loss: 0.00001323
Iteration 6/1000 | Loss: 0.00001272
Iteration 7/1000 | Loss: 0.00001247
Iteration 8/1000 | Loss: 0.00001221
Iteration 9/1000 | Loss: 0.00001199
Iteration 10/1000 | Loss: 0.00001190
Iteration 11/1000 | Loss: 0.00001186
Iteration 12/1000 | Loss: 0.00001186
Iteration 13/1000 | Loss: 0.00001179
Iteration 14/1000 | Loss: 0.00001177
Iteration 15/1000 | Loss: 0.00001175
Iteration 16/1000 | Loss: 0.00001175
Iteration 17/1000 | Loss: 0.00001172
Iteration 18/1000 | Loss: 0.00001169
Iteration 19/1000 | Loss: 0.00001168
Iteration 20/1000 | Loss: 0.00001167
Iteration 21/1000 | Loss: 0.00001167
Iteration 22/1000 | Loss: 0.00001166
Iteration 23/1000 | Loss: 0.00001165
Iteration 24/1000 | Loss: 0.00001165
Iteration 25/1000 | Loss: 0.00001165
Iteration 26/1000 | Loss: 0.00001164
Iteration 27/1000 | Loss: 0.00001163
Iteration 28/1000 | Loss: 0.00001162
Iteration 29/1000 | Loss: 0.00001162
Iteration 30/1000 | Loss: 0.00001161
Iteration 31/1000 | Loss: 0.00001161
Iteration 32/1000 | Loss: 0.00001160
Iteration 33/1000 | Loss: 0.00001158
Iteration 34/1000 | Loss: 0.00001154
Iteration 35/1000 | Loss: 0.00001154
Iteration 36/1000 | Loss: 0.00001154
Iteration 37/1000 | Loss: 0.00001154
Iteration 38/1000 | Loss: 0.00001152
Iteration 39/1000 | Loss: 0.00001145
Iteration 40/1000 | Loss: 0.00001145
Iteration 41/1000 | Loss: 0.00001137
Iteration 42/1000 | Loss: 0.00001135
Iteration 43/1000 | Loss: 0.00001135
Iteration 44/1000 | Loss: 0.00001134
Iteration 45/1000 | Loss: 0.00001133
Iteration 46/1000 | Loss: 0.00001132
Iteration 47/1000 | Loss: 0.00001131
Iteration 48/1000 | Loss: 0.00001130
Iteration 49/1000 | Loss: 0.00001129
Iteration 50/1000 | Loss: 0.00001129
Iteration 51/1000 | Loss: 0.00001129
Iteration 52/1000 | Loss: 0.00001128
Iteration 53/1000 | Loss: 0.00001128
Iteration 54/1000 | Loss: 0.00001127
Iteration 55/1000 | Loss: 0.00001126
Iteration 56/1000 | Loss: 0.00001125
Iteration 57/1000 | Loss: 0.00001124
Iteration 58/1000 | Loss: 0.00001124
Iteration 59/1000 | Loss: 0.00001124
Iteration 60/1000 | Loss: 0.00001123
Iteration 61/1000 | Loss: 0.00001123
Iteration 62/1000 | Loss: 0.00001122
Iteration 63/1000 | Loss: 0.00001122
Iteration 64/1000 | Loss: 0.00001121
Iteration 65/1000 | Loss: 0.00001120
Iteration 66/1000 | Loss: 0.00001119
Iteration 67/1000 | Loss: 0.00001119
Iteration 68/1000 | Loss: 0.00001119
Iteration 69/1000 | Loss: 0.00001118
Iteration 70/1000 | Loss: 0.00001118
Iteration 71/1000 | Loss: 0.00001117
Iteration 72/1000 | Loss: 0.00001117
Iteration 73/1000 | Loss: 0.00001115
Iteration 74/1000 | Loss: 0.00001115
Iteration 75/1000 | Loss: 0.00001115
Iteration 76/1000 | Loss: 0.00001114
Iteration 77/1000 | Loss: 0.00001114
Iteration 78/1000 | Loss: 0.00001113
Iteration 79/1000 | Loss: 0.00001113
Iteration 80/1000 | Loss: 0.00001113
Iteration 81/1000 | Loss: 0.00001113
Iteration 82/1000 | Loss: 0.00001113
Iteration 83/1000 | Loss: 0.00001113
Iteration 84/1000 | Loss: 0.00001113
Iteration 85/1000 | Loss: 0.00001113
Iteration 86/1000 | Loss: 0.00001113
Iteration 87/1000 | Loss: 0.00001113
Iteration 88/1000 | Loss: 0.00001113
Iteration 89/1000 | Loss: 0.00001112
Iteration 90/1000 | Loss: 0.00001112
Iteration 91/1000 | Loss: 0.00001112
Iteration 92/1000 | Loss: 0.00001112
Iteration 93/1000 | Loss: 0.00001112
Iteration 94/1000 | Loss: 0.00001112
Iteration 95/1000 | Loss: 0.00001112
Iteration 96/1000 | Loss: 0.00001111
Iteration 97/1000 | Loss: 0.00001111
Iteration 98/1000 | Loss: 0.00001111
Iteration 99/1000 | Loss: 0.00001110
Iteration 100/1000 | Loss: 0.00001110
Iteration 101/1000 | Loss: 0.00001110
Iteration 102/1000 | Loss: 0.00001110
Iteration 103/1000 | Loss: 0.00001110
Iteration 104/1000 | Loss: 0.00001109
Iteration 105/1000 | Loss: 0.00001109
Iteration 106/1000 | Loss: 0.00001109
Iteration 107/1000 | Loss: 0.00001108
Iteration 108/1000 | Loss: 0.00001108
Iteration 109/1000 | Loss: 0.00001108
Iteration 110/1000 | Loss: 0.00001108
Iteration 111/1000 | Loss: 0.00001108
Iteration 112/1000 | Loss: 0.00001108
Iteration 113/1000 | Loss: 0.00001108
Iteration 114/1000 | Loss: 0.00001108
Iteration 115/1000 | Loss: 0.00001108
Iteration 116/1000 | Loss: 0.00001108
Iteration 117/1000 | Loss: 0.00001108
Iteration 118/1000 | Loss: 0.00001108
Iteration 119/1000 | Loss: 0.00001107
Iteration 120/1000 | Loss: 0.00001107
Iteration 121/1000 | Loss: 0.00001107
Iteration 122/1000 | Loss: 0.00001107
Iteration 123/1000 | Loss: 0.00001107
Iteration 124/1000 | Loss: 0.00001107
Iteration 125/1000 | Loss: 0.00001107
Iteration 126/1000 | Loss: 0.00001107
Iteration 127/1000 | Loss: 0.00001106
Iteration 128/1000 | Loss: 0.00001106
Iteration 129/1000 | Loss: 0.00001106
Iteration 130/1000 | Loss: 0.00001106
Iteration 131/1000 | Loss: 0.00001106
Iteration 132/1000 | Loss: 0.00001106
Iteration 133/1000 | Loss: 0.00001106
Iteration 134/1000 | Loss: 0.00001106
Iteration 135/1000 | Loss: 0.00001106
Iteration 136/1000 | Loss: 0.00001106
Iteration 137/1000 | Loss: 0.00001105
Iteration 138/1000 | Loss: 0.00001105
Iteration 139/1000 | Loss: 0.00001105
Iteration 140/1000 | Loss: 0.00001105
Iteration 141/1000 | Loss: 0.00001105
Iteration 142/1000 | Loss: 0.00001105
Iteration 143/1000 | Loss: 0.00001105
Iteration 144/1000 | Loss: 0.00001105
Iteration 145/1000 | Loss: 0.00001105
Iteration 146/1000 | Loss: 0.00001105
Iteration 147/1000 | Loss: 0.00001105
Iteration 148/1000 | Loss: 0.00001105
Iteration 149/1000 | Loss: 0.00001105
Iteration 150/1000 | Loss: 0.00001104
Iteration 151/1000 | Loss: 0.00001104
Iteration 152/1000 | Loss: 0.00001104
Iteration 153/1000 | Loss: 0.00001104
Iteration 154/1000 | Loss: 0.00001103
Iteration 155/1000 | Loss: 0.00001103
Iteration 156/1000 | Loss: 0.00001103
Iteration 157/1000 | Loss: 0.00001102
Iteration 158/1000 | Loss: 0.00001102
Iteration 159/1000 | Loss: 0.00001102
Iteration 160/1000 | Loss: 0.00001102
Iteration 161/1000 | Loss: 0.00001101
Iteration 162/1000 | Loss: 0.00001101
Iteration 163/1000 | Loss: 0.00001101
Iteration 164/1000 | Loss: 0.00001100
Iteration 165/1000 | Loss: 0.00001100
Iteration 166/1000 | Loss: 0.00001100
Iteration 167/1000 | Loss: 0.00001100
Iteration 168/1000 | Loss: 0.00001100
Iteration 169/1000 | Loss: 0.00001100
Iteration 170/1000 | Loss: 0.00001100
Iteration 171/1000 | Loss: 0.00001100
Iteration 172/1000 | Loss: 0.00001100
Iteration 173/1000 | Loss: 0.00001100
Iteration 174/1000 | Loss: 0.00001100
Iteration 175/1000 | Loss: 0.00001100
Iteration 176/1000 | Loss: 0.00001100
Iteration 177/1000 | Loss: 0.00001100
Iteration 178/1000 | Loss: 0.00001100
Iteration 179/1000 | Loss: 0.00001100
Iteration 180/1000 | Loss: 0.00001100
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.0999595360772219e-05, 1.0999595360772219e-05, 1.0999595360772219e-05, 1.0999595360772219e-05, 1.0999595360772219e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0999595360772219e-05

Optimization complete. Final v2v error: 2.8457260131835938 mm

Highest mean error: 3.0504825115203857 mm for frame 235

Lowest mean error: 2.674022912979126 mm for frame 181

Saving results

Total time: 42.77801847457886
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00620002
Iteration 2/25 | Loss: 0.00149517
Iteration 3/25 | Loss: 0.00131651
Iteration 4/25 | Loss: 0.00128722
Iteration 5/25 | Loss: 0.00126802
Iteration 6/25 | Loss: 0.00125752
Iteration 7/25 | Loss: 0.00125739
Iteration 8/25 | Loss: 0.00125496
Iteration 9/25 | Loss: 0.00126051
Iteration 10/25 | Loss: 0.00125940
Iteration 11/25 | Loss: 0.00123875
Iteration 12/25 | Loss: 0.00123789
Iteration 13/25 | Loss: 0.00123766
Iteration 14/25 | Loss: 0.00123758
Iteration 15/25 | Loss: 0.00123737
Iteration 16/25 | Loss: 0.00123712
Iteration 17/25 | Loss: 0.00123686
Iteration 18/25 | Loss: 0.00123682
Iteration 19/25 | Loss: 0.00123681
Iteration 20/25 | Loss: 0.00123681
Iteration 21/25 | Loss: 0.00123681
Iteration 22/25 | Loss: 0.00123681
Iteration 23/25 | Loss: 0.00123681
Iteration 24/25 | Loss: 0.00123681
Iteration 25/25 | Loss: 0.00123681

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63192034
Iteration 2/25 | Loss: 0.00082581
Iteration 3/25 | Loss: 0.00082581
Iteration 4/25 | Loss: 0.00082581
Iteration 5/25 | Loss: 0.00082581
Iteration 6/25 | Loss: 0.00082581
Iteration 7/25 | Loss: 0.00082581
Iteration 8/25 | Loss: 0.00082581
Iteration 9/25 | Loss: 0.00082581
Iteration 10/25 | Loss: 0.00082581
Iteration 11/25 | Loss: 0.00082580
Iteration 12/25 | Loss: 0.00082580
Iteration 13/25 | Loss: 0.00082580
Iteration 14/25 | Loss: 0.00082580
Iteration 15/25 | Loss: 0.00082580
Iteration 16/25 | Loss: 0.00082580
Iteration 17/25 | Loss: 0.00082580
Iteration 18/25 | Loss: 0.00082580
Iteration 19/25 | Loss: 0.00082580
Iteration 20/25 | Loss: 0.00082580
Iteration 21/25 | Loss: 0.00082580
Iteration 22/25 | Loss: 0.00082580
Iteration 23/25 | Loss: 0.00082580
Iteration 24/25 | Loss: 0.00082580
Iteration 25/25 | Loss: 0.00082580

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082580
Iteration 2/1000 | Loss: 0.00003615
Iteration 3/1000 | Loss: 0.00002596
Iteration 4/1000 | Loss: 0.00002346
Iteration 5/1000 | Loss: 0.00002202
Iteration 6/1000 | Loss: 0.00002080
Iteration 7/1000 | Loss: 0.00002016
Iteration 8/1000 | Loss: 0.00001969
Iteration 9/1000 | Loss: 0.00001937
Iteration 10/1000 | Loss: 0.00001903
Iteration 11/1000 | Loss: 0.00001880
Iteration 12/1000 | Loss: 0.00001864
Iteration 13/1000 | Loss: 0.00001856
Iteration 14/1000 | Loss: 0.00001850
Iteration 15/1000 | Loss: 0.00001845
Iteration 16/1000 | Loss: 0.00001838
Iteration 17/1000 | Loss: 0.00001838
Iteration 18/1000 | Loss: 0.00001836
Iteration 19/1000 | Loss: 0.00001835
Iteration 20/1000 | Loss: 0.00001832
Iteration 21/1000 | Loss: 0.00001832
Iteration 22/1000 | Loss: 0.00001830
Iteration 23/1000 | Loss: 0.00001830
Iteration 24/1000 | Loss: 0.00001830
Iteration 25/1000 | Loss: 0.00001829
Iteration 26/1000 | Loss: 0.00001829
Iteration 27/1000 | Loss: 0.00001829
Iteration 28/1000 | Loss: 0.00001828
Iteration 29/1000 | Loss: 0.00001828
Iteration 30/1000 | Loss: 0.00001827
Iteration 31/1000 | Loss: 0.00001827
Iteration 32/1000 | Loss: 0.00001827
Iteration 33/1000 | Loss: 0.00001826
Iteration 34/1000 | Loss: 0.00001826
Iteration 35/1000 | Loss: 0.00001825
Iteration 36/1000 | Loss: 0.00001825
Iteration 37/1000 | Loss: 0.00001824
Iteration 38/1000 | Loss: 0.00001824
Iteration 39/1000 | Loss: 0.00001824
Iteration 40/1000 | Loss: 0.00001824
Iteration 41/1000 | Loss: 0.00001823
Iteration 42/1000 | Loss: 0.00001823
Iteration 43/1000 | Loss: 0.00001823
Iteration 44/1000 | Loss: 0.00001823
Iteration 45/1000 | Loss: 0.00001822
Iteration 46/1000 | Loss: 0.00001822
Iteration 47/1000 | Loss: 0.00001822
Iteration 48/1000 | Loss: 0.00001822
Iteration 49/1000 | Loss: 0.00001822
Iteration 50/1000 | Loss: 0.00001822
Iteration 51/1000 | Loss: 0.00001822
Iteration 52/1000 | Loss: 0.00001822
Iteration 53/1000 | Loss: 0.00001822
Iteration 54/1000 | Loss: 0.00001822
Iteration 55/1000 | Loss: 0.00001821
Iteration 56/1000 | Loss: 0.00001820
Iteration 57/1000 | Loss: 0.00001820
Iteration 58/1000 | Loss: 0.00001820
Iteration 59/1000 | Loss: 0.00001819
Iteration 60/1000 | Loss: 0.00001819
Iteration 61/1000 | Loss: 0.00001819
Iteration 62/1000 | Loss: 0.00001818
Iteration 63/1000 | Loss: 0.00001818
Iteration 64/1000 | Loss: 0.00001817
Iteration 65/1000 | Loss: 0.00001817
Iteration 66/1000 | Loss: 0.00001817
Iteration 67/1000 | Loss: 0.00001816
Iteration 68/1000 | Loss: 0.00001816
Iteration 69/1000 | Loss: 0.00001816
Iteration 70/1000 | Loss: 0.00001816
Iteration 71/1000 | Loss: 0.00001815
Iteration 72/1000 | Loss: 0.00001815
Iteration 73/1000 | Loss: 0.00001815
Iteration 74/1000 | Loss: 0.00001814
Iteration 75/1000 | Loss: 0.00001814
Iteration 76/1000 | Loss: 0.00001814
Iteration 77/1000 | Loss: 0.00001814
Iteration 78/1000 | Loss: 0.00001814
Iteration 79/1000 | Loss: 0.00001814
Iteration 80/1000 | Loss: 0.00001814
Iteration 81/1000 | Loss: 0.00001814
Iteration 82/1000 | Loss: 0.00001814
Iteration 83/1000 | Loss: 0.00001813
Iteration 84/1000 | Loss: 0.00001813
Iteration 85/1000 | Loss: 0.00001813
Iteration 86/1000 | Loss: 0.00001813
Iteration 87/1000 | Loss: 0.00001813
Iteration 88/1000 | Loss: 0.00001813
Iteration 89/1000 | Loss: 0.00001813
Iteration 90/1000 | Loss: 0.00001813
Iteration 91/1000 | Loss: 0.00001812
Iteration 92/1000 | Loss: 0.00001812
Iteration 93/1000 | Loss: 0.00001812
Iteration 94/1000 | Loss: 0.00001812
Iteration 95/1000 | Loss: 0.00001811
Iteration 96/1000 | Loss: 0.00001811
Iteration 97/1000 | Loss: 0.00001811
Iteration 98/1000 | Loss: 0.00001811
Iteration 99/1000 | Loss: 0.00001811
Iteration 100/1000 | Loss: 0.00001811
Iteration 101/1000 | Loss: 0.00001811
Iteration 102/1000 | Loss: 0.00001811
Iteration 103/1000 | Loss: 0.00001811
Iteration 104/1000 | Loss: 0.00001810
Iteration 105/1000 | Loss: 0.00001810
Iteration 106/1000 | Loss: 0.00001810
Iteration 107/1000 | Loss: 0.00001810
Iteration 108/1000 | Loss: 0.00001810
Iteration 109/1000 | Loss: 0.00001810
Iteration 110/1000 | Loss: 0.00001810
Iteration 111/1000 | Loss: 0.00001810
Iteration 112/1000 | Loss: 0.00001810
Iteration 113/1000 | Loss: 0.00001810
Iteration 114/1000 | Loss: 0.00001809
Iteration 115/1000 | Loss: 0.00001809
Iteration 116/1000 | Loss: 0.00001809
Iteration 117/1000 | Loss: 0.00001809
Iteration 118/1000 | Loss: 0.00001809
Iteration 119/1000 | Loss: 0.00001809
Iteration 120/1000 | Loss: 0.00001809
Iteration 121/1000 | Loss: 0.00001809
Iteration 122/1000 | Loss: 0.00001809
Iteration 123/1000 | Loss: 0.00001809
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [1.8092288883053698e-05, 1.8092288883053698e-05, 1.8092288883053698e-05, 1.8092288883053698e-05, 1.8092288883053698e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8092288883053698e-05

Optimization complete. Final v2v error: 3.511167287826538 mm

Highest mean error: 4.429389476776123 mm for frame 126

Lowest mean error: 2.967075824737549 mm for frame 68

Saving results

Total time: 65.00713205337524
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00427450
Iteration 2/25 | Loss: 0.00129156
Iteration 3/25 | Loss: 0.00121347
Iteration 4/25 | Loss: 0.00120640
Iteration 5/25 | Loss: 0.00120471
Iteration 6/25 | Loss: 0.00120464
Iteration 7/25 | Loss: 0.00120464
Iteration 8/25 | Loss: 0.00120464
Iteration 9/25 | Loss: 0.00120464
Iteration 10/25 | Loss: 0.00120464
Iteration 11/25 | Loss: 0.00120464
Iteration 12/25 | Loss: 0.00120464
Iteration 13/25 | Loss: 0.00120464
Iteration 14/25 | Loss: 0.00120464
Iteration 15/25 | Loss: 0.00120464
Iteration 16/25 | Loss: 0.00120464
Iteration 17/25 | Loss: 0.00120464
Iteration 18/25 | Loss: 0.00120464
Iteration 19/25 | Loss: 0.00120464
Iteration 20/25 | Loss: 0.00120464
Iteration 21/25 | Loss: 0.00120464
Iteration 22/25 | Loss: 0.00120464
Iteration 23/25 | Loss: 0.00120464
Iteration 24/25 | Loss: 0.00120464
Iteration 25/25 | Loss: 0.00120464

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21850622
Iteration 2/25 | Loss: 0.00054064
Iteration 3/25 | Loss: 0.00054064
Iteration 4/25 | Loss: 0.00054064
Iteration 5/25 | Loss: 0.00054064
Iteration 6/25 | Loss: 0.00054064
Iteration 7/25 | Loss: 0.00054064
Iteration 8/25 | Loss: 0.00054064
Iteration 9/25 | Loss: 0.00054064
Iteration 10/25 | Loss: 0.00054064
Iteration 11/25 | Loss: 0.00054064
Iteration 12/25 | Loss: 0.00054064
Iteration 13/25 | Loss: 0.00054064
Iteration 14/25 | Loss: 0.00054064
Iteration 15/25 | Loss: 0.00054064
Iteration 16/25 | Loss: 0.00054064
Iteration 17/25 | Loss: 0.00054064
Iteration 18/25 | Loss: 0.00054064
Iteration 19/25 | Loss: 0.00054064
Iteration 20/25 | Loss: 0.00054064
Iteration 21/25 | Loss: 0.00054064
Iteration 22/25 | Loss: 0.00054064
Iteration 23/25 | Loss: 0.00054064
Iteration 24/25 | Loss: 0.00054064
Iteration 25/25 | Loss: 0.00054064

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054064
Iteration 2/1000 | Loss: 0.00004113
Iteration 3/1000 | Loss: 0.00002574
Iteration 4/1000 | Loss: 0.00002070
Iteration 5/1000 | Loss: 0.00001942
Iteration 6/1000 | Loss: 0.00001829
Iteration 7/1000 | Loss: 0.00001738
Iteration 8/1000 | Loss: 0.00001680
Iteration 9/1000 | Loss: 0.00001643
Iteration 10/1000 | Loss: 0.00001608
Iteration 11/1000 | Loss: 0.00001579
Iteration 12/1000 | Loss: 0.00001563
Iteration 13/1000 | Loss: 0.00001545
Iteration 14/1000 | Loss: 0.00001541
Iteration 15/1000 | Loss: 0.00001532
Iteration 16/1000 | Loss: 0.00001531
Iteration 17/1000 | Loss: 0.00001526
Iteration 18/1000 | Loss: 0.00001513
Iteration 19/1000 | Loss: 0.00001512
Iteration 20/1000 | Loss: 0.00001512
Iteration 21/1000 | Loss: 0.00001511
Iteration 22/1000 | Loss: 0.00001511
Iteration 23/1000 | Loss: 0.00001508
Iteration 24/1000 | Loss: 0.00001508
Iteration 25/1000 | Loss: 0.00001508
Iteration 26/1000 | Loss: 0.00001508
Iteration 27/1000 | Loss: 0.00001507
Iteration 28/1000 | Loss: 0.00001506
Iteration 29/1000 | Loss: 0.00001506
Iteration 30/1000 | Loss: 0.00001506
Iteration 31/1000 | Loss: 0.00001506
Iteration 32/1000 | Loss: 0.00001505
Iteration 33/1000 | Loss: 0.00001505
Iteration 34/1000 | Loss: 0.00001505
Iteration 35/1000 | Loss: 0.00001504
Iteration 36/1000 | Loss: 0.00001503
Iteration 37/1000 | Loss: 0.00001502
Iteration 38/1000 | Loss: 0.00001502
Iteration 39/1000 | Loss: 0.00001502
Iteration 40/1000 | Loss: 0.00001502
Iteration 41/1000 | Loss: 0.00001501
Iteration 42/1000 | Loss: 0.00001501
Iteration 43/1000 | Loss: 0.00001501
Iteration 44/1000 | Loss: 0.00001501
Iteration 45/1000 | Loss: 0.00001501
Iteration 46/1000 | Loss: 0.00001501
Iteration 47/1000 | Loss: 0.00001500
Iteration 48/1000 | Loss: 0.00001500
Iteration 49/1000 | Loss: 0.00001499
Iteration 50/1000 | Loss: 0.00001499
Iteration 51/1000 | Loss: 0.00001499
Iteration 52/1000 | Loss: 0.00001498
Iteration 53/1000 | Loss: 0.00001498
Iteration 54/1000 | Loss: 0.00001498
Iteration 55/1000 | Loss: 0.00001498
Iteration 56/1000 | Loss: 0.00001498
Iteration 57/1000 | Loss: 0.00001498
Iteration 58/1000 | Loss: 0.00001498
Iteration 59/1000 | Loss: 0.00001498
Iteration 60/1000 | Loss: 0.00001498
Iteration 61/1000 | Loss: 0.00001498
Iteration 62/1000 | Loss: 0.00001498
Iteration 63/1000 | Loss: 0.00001498
Iteration 64/1000 | Loss: 0.00001498
Iteration 65/1000 | Loss: 0.00001498
Iteration 66/1000 | Loss: 0.00001498
Iteration 67/1000 | Loss: 0.00001498
Iteration 68/1000 | Loss: 0.00001498
Iteration 69/1000 | Loss: 0.00001498
Iteration 70/1000 | Loss: 0.00001498
Iteration 71/1000 | Loss: 0.00001498
Iteration 72/1000 | Loss: 0.00001498
Iteration 73/1000 | Loss: 0.00001498
Iteration 74/1000 | Loss: 0.00001498
Iteration 75/1000 | Loss: 0.00001498
Iteration 76/1000 | Loss: 0.00001498
Iteration 77/1000 | Loss: 0.00001498
Iteration 78/1000 | Loss: 0.00001498
Iteration 79/1000 | Loss: 0.00001498
Iteration 80/1000 | Loss: 0.00001498
Iteration 81/1000 | Loss: 0.00001498
Iteration 82/1000 | Loss: 0.00001498
Iteration 83/1000 | Loss: 0.00001498
Iteration 84/1000 | Loss: 0.00001498
Iteration 85/1000 | Loss: 0.00001498
Iteration 86/1000 | Loss: 0.00001498
Iteration 87/1000 | Loss: 0.00001498
Iteration 88/1000 | Loss: 0.00001498
Iteration 89/1000 | Loss: 0.00001498
Iteration 90/1000 | Loss: 0.00001498
Iteration 91/1000 | Loss: 0.00001498
Iteration 92/1000 | Loss: 0.00001498
Iteration 93/1000 | Loss: 0.00001498
Iteration 94/1000 | Loss: 0.00001498
Iteration 95/1000 | Loss: 0.00001498
Iteration 96/1000 | Loss: 0.00001498
Iteration 97/1000 | Loss: 0.00001498
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [1.497564426244935e-05, 1.497564426244935e-05, 1.497564426244935e-05, 1.497564426244935e-05, 1.497564426244935e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.497564426244935e-05

Optimization complete. Final v2v error: 3.323453664779663 mm

Highest mean error: 3.3569884300231934 mm for frame 17

Lowest mean error: 3.3085572719573975 mm for frame 119

Saving results

Total time: 31.51771879196167
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00545329
Iteration 2/25 | Loss: 0.00139185
Iteration 3/25 | Loss: 0.00129216
Iteration 4/25 | Loss: 0.00127570
Iteration 5/25 | Loss: 0.00126914
Iteration 6/25 | Loss: 0.00126838
Iteration 7/25 | Loss: 0.00126838
Iteration 8/25 | Loss: 0.00126838
Iteration 9/25 | Loss: 0.00126838
Iteration 10/25 | Loss: 0.00126838
Iteration 11/25 | Loss: 0.00126838
Iteration 12/25 | Loss: 0.00126838
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012683846289291978, 0.0012683846289291978, 0.0012683846289291978, 0.0012683846289291978, 0.0012683846289291978]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012683846289291978

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.80073434
Iteration 2/25 | Loss: 0.00070203
Iteration 3/25 | Loss: 0.00070203
Iteration 4/25 | Loss: 0.00070203
Iteration 5/25 | Loss: 0.00070203
Iteration 6/25 | Loss: 0.00070203
Iteration 7/25 | Loss: 0.00070203
Iteration 8/25 | Loss: 0.00070203
Iteration 9/25 | Loss: 0.00070203
Iteration 10/25 | Loss: 0.00070203
Iteration 11/25 | Loss: 0.00070203
Iteration 12/25 | Loss: 0.00070203
Iteration 13/25 | Loss: 0.00070203
Iteration 14/25 | Loss: 0.00070203
Iteration 15/25 | Loss: 0.00070203
Iteration 16/25 | Loss: 0.00070203
Iteration 17/25 | Loss: 0.00070203
Iteration 18/25 | Loss: 0.00070203
Iteration 19/25 | Loss: 0.00070203
Iteration 20/25 | Loss: 0.00070203
Iteration 21/25 | Loss: 0.00070203
Iteration 22/25 | Loss: 0.00070203
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007020264747552574, 0.0007020264747552574, 0.0007020264747552574, 0.0007020264747552574, 0.0007020264747552574]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007020264747552574

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070203
Iteration 2/1000 | Loss: 0.00003568
Iteration 3/1000 | Loss: 0.00002806
Iteration 4/1000 | Loss: 0.00002619
Iteration 5/1000 | Loss: 0.00002507
Iteration 6/1000 | Loss: 0.00002428
Iteration 7/1000 | Loss: 0.00002391
Iteration 8/1000 | Loss: 0.00002348
Iteration 9/1000 | Loss: 0.00002284
Iteration 10/1000 | Loss: 0.00002257
Iteration 11/1000 | Loss: 0.00002214
Iteration 12/1000 | Loss: 0.00002183
Iteration 13/1000 | Loss: 0.00002158
Iteration 14/1000 | Loss: 0.00002134
Iteration 15/1000 | Loss: 0.00002120
Iteration 16/1000 | Loss: 0.00002101
Iteration 17/1000 | Loss: 0.00002091
Iteration 18/1000 | Loss: 0.00002091
Iteration 19/1000 | Loss: 0.00002079
Iteration 20/1000 | Loss: 0.00002079
Iteration 21/1000 | Loss: 0.00002077
Iteration 22/1000 | Loss: 0.00002077
Iteration 23/1000 | Loss: 0.00002077
Iteration 24/1000 | Loss: 0.00002077
Iteration 25/1000 | Loss: 0.00002077
Iteration 26/1000 | Loss: 0.00002077
Iteration 27/1000 | Loss: 0.00002077
Iteration 28/1000 | Loss: 0.00002077
Iteration 29/1000 | Loss: 0.00002077
Iteration 30/1000 | Loss: 0.00002077
Iteration 31/1000 | Loss: 0.00002077
Iteration 32/1000 | Loss: 0.00002076
Iteration 33/1000 | Loss: 0.00002076
Iteration 34/1000 | Loss: 0.00002076
Iteration 35/1000 | Loss: 0.00002076
Iteration 36/1000 | Loss: 0.00002075
Iteration 37/1000 | Loss: 0.00002075
Iteration 38/1000 | Loss: 0.00002075
Iteration 39/1000 | Loss: 0.00002071
Iteration 40/1000 | Loss: 0.00002070
Iteration 41/1000 | Loss: 0.00002069
Iteration 42/1000 | Loss: 0.00002069
Iteration 43/1000 | Loss: 0.00002068
Iteration 44/1000 | Loss: 0.00002068
Iteration 45/1000 | Loss: 0.00002068
Iteration 46/1000 | Loss: 0.00002068
Iteration 47/1000 | Loss: 0.00002067
Iteration 48/1000 | Loss: 0.00002067
Iteration 49/1000 | Loss: 0.00002067
Iteration 50/1000 | Loss: 0.00002066
Iteration 51/1000 | Loss: 0.00002065
Iteration 52/1000 | Loss: 0.00002065
Iteration 53/1000 | Loss: 0.00002064
Iteration 54/1000 | Loss: 0.00002063
Iteration 55/1000 | Loss: 0.00002062
Iteration 56/1000 | Loss: 0.00002062
Iteration 57/1000 | Loss: 0.00002062
Iteration 58/1000 | Loss: 0.00002061
Iteration 59/1000 | Loss: 0.00002061
Iteration 60/1000 | Loss: 0.00002061
Iteration 61/1000 | Loss: 0.00002061
Iteration 62/1000 | Loss: 0.00002060
Iteration 63/1000 | Loss: 0.00002060
Iteration 64/1000 | Loss: 0.00002059
Iteration 65/1000 | Loss: 0.00002059
Iteration 66/1000 | Loss: 0.00002058
Iteration 67/1000 | Loss: 0.00002058
Iteration 68/1000 | Loss: 0.00002058
Iteration 69/1000 | Loss: 0.00002058
Iteration 70/1000 | Loss: 0.00002058
Iteration 71/1000 | Loss: 0.00002058
Iteration 72/1000 | Loss: 0.00002058
Iteration 73/1000 | Loss: 0.00002055
Iteration 74/1000 | Loss: 0.00002055
Iteration 75/1000 | Loss: 0.00002054
Iteration 76/1000 | Loss: 0.00002054
Iteration 77/1000 | Loss: 0.00002053
Iteration 78/1000 | Loss: 0.00002051
Iteration 79/1000 | Loss: 0.00002051
Iteration 80/1000 | Loss: 0.00002051
Iteration 81/1000 | Loss: 0.00002051
Iteration 82/1000 | Loss: 0.00002051
Iteration 83/1000 | Loss: 0.00002051
Iteration 84/1000 | Loss: 0.00002050
Iteration 85/1000 | Loss: 0.00002050
Iteration 86/1000 | Loss: 0.00002050
Iteration 87/1000 | Loss: 0.00002050
Iteration 88/1000 | Loss: 0.00002050
Iteration 89/1000 | Loss: 0.00002050
Iteration 90/1000 | Loss: 0.00002049
Iteration 91/1000 | Loss: 0.00002049
Iteration 92/1000 | Loss: 0.00002049
Iteration 93/1000 | Loss: 0.00002049
Iteration 94/1000 | Loss: 0.00002049
Iteration 95/1000 | Loss: 0.00002049
Iteration 96/1000 | Loss: 0.00002049
Iteration 97/1000 | Loss: 0.00002048
Iteration 98/1000 | Loss: 0.00002048
Iteration 99/1000 | Loss: 0.00002048
Iteration 100/1000 | Loss: 0.00002048
Iteration 101/1000 | Loss: 0.00002048
Iteration 102/1000 | Loss: 0.00002047
Iteration 103/1000 | Loss: 0.00002047
Iteration 104/1000 | Loss: 0.00002047
Iteration 105/1000 | Loss: 0.00002046
Iteration 106/1000 | Loss: 0.00002046
Iteration 107/1000 | Loss: 0.00002046
Iteration 108/1000 | Loss: 0.00002046
Iteration 109/1000 | Loss: 0.00002046
Iteration 110/1000 | Loss: 0.00002046
Iteration 111/1000 | Loss: 0.00002046
Iteration 112/1000 | Loss: 0.00002045
Iteration 113/1000 | Loss: 0.00002045
Iteration 114/1000 | Loss: 0.00002045
Iteration 115/1000 | Loss: 0.00002045
Iteration 116/1000 | Loss: 0.00002045
Iteration 117/1000 | Loss: 0.00002045
Iteration 118/1000 | Loss: 0.00002045
Iteration 119/1000 | Loss: 0.00002044
Iteration 120/1000 | Loss: 0.00002044
Iteration 121/1000 | Loss: 0.00002044
Iteration 122/1000 | Loss: 0.00002043
Iteration 123/1000 | Loss: 0.00002043
Iteration 124/1000 | Loss: 0.00002042
Iteration 125/1000 | Loss: 0.00002042
Iteration 126/1000 | Loss: 0.00002042
Iteration 127/1000 | Loss: 0.00002042
Iteration 128/1000 | Loss: 0.00002042
Iteration 129/1000 | Loss: 0.00002042
Iteration 130/1000 | Loss: 0.00002042
Iteration 131/1000 | Loss: 0.00002041
Iteration 132/1000 | Loss: 0.00002041
Iteration 133/1000 | Loss: 0.00002041
Iteration 134/1000 | Loss: 0.00002041
Iteration 135/1000 | Loss: 0.00002040
Iteration 136/1000 | Loss: 0.00002040
Iteration 137/1000 | Loss: 0.00002040
Iteration 138/1000 | Loss: 0.00002040
Iteration 139/1000 | Loss: 0.00002040
Iteration 140/1000 | Loss: 0.00002040
Iteration 141/1000 | Loss: 0.00002040
Iteration 142/1000 | Loss: 0.00002040
Iteration 143/1000 | Loss: 0.00002040
Iteration 144/1000 | Loss: 0.00002040
Iteration 145/1000 | Loss: 0.00002039
Iteration 146/1000 | Loss: 0.00002039
Iteration 147/1000 | Loss: 0.00002039
Iteration 148/1000 | Loss: 0.00002039
Iteration 149/1000 | Loss: 0.00002039
Iteration 150/1000 | Loss: 0.00002039
Iteration 151/1000 | Loss: 0.00002039
Iteration 152/1000 | Loss: 0.00002039
Iteration 153/1000 | Loss: 0.00002039
Iteration 154/1000 | Loss: 0.00002039
Iteration 155/1000 | Loss: 0.00002039
Iteration 156/1000 | Loss: 0.00002039
Iteration 157/1000 | Loss: 0.00002039
Iteration 158/1000 | Loss: 0.00002039
Iteration 159/1000 | Loss: 0.00002038
Iteration 160/1000 | Loss: 0.00002038
Iteration 161/1000 | Loss: 0.00002038
Iteration 162/1000 | Loss: 0.00002038
Iteration 163/1000 | Loss: 0.00002038
Iteration 164/1000 | Loss: 0.00002038
Iteration 165/1000 | Loss: 0.00002038
Iteration 166/1000 | Loss: 0.00002038
Iteration 167/1000 | Loss: 0.00002038
Iteration 168/1000 | Loss: 0.00002038
Iteration 169/1000 | Loss: 0.00002038
Iteration 170/1000 | Loss: 0.00002038
Iteration 171/1000 | Loss: 0.00002038
Iteration 172/1000 | Loss: 0.00002038
Iteration 173/1000 | Loss: 0.00002038
Iteration 174/1000 | Loss: 0.00002038
Iteration 175/1000 | Loss: 0.00002038
Iteration 176/1000 | Loss: 0.00002038
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [2.038153979810886e-05, 2.038153979810886e-05, 2.038153979810886e-05, 2.038153979810886e-05, 2.038153979810886e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.038153979810886e-05

Optimization complete. Final v2v error: 3.8092963695526123 mm

Highest mean error: 3.8572146892547607 mm for frame 111

Lowest mean error: 3.735130548477173 mm for frame 34

Saving results

Total time: 46.63560104370117
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00875726
Iteration 2/25 | Loss: 0.00131089
Iteration 3/25 | Loss: 0.00121758
Iteration 4/25 | Loss: 0.00120574
Iteration 5/25 | Loss: 0.00120206
Iteration 6/25 | Loss: 0.00120167
Iteration 7/25 | Loss: 0.00120167
Iteration 8/25 | Loss: 0.00120167
Iteration 9/25 | Loss: 0.00120167
Iteration 10/25 | Loss: 0.00120167
Iteration 11/25 | Loss: 0.00120167
Iteration 12/25 | Loss: 0.00120167
Iteration 13/25 | Loss: 0.00120167
Iteration 14/25 | Loss: 0.00120167
Iteration 15/25 | Loss: 0.00120167
Iteration 16/25 | Loss: 0.00120167
Iteration 17/25 | Loss: 0.00120167
Iteration 18/25 | Loss: 0.00120167
Iteration 19/25 | Loss: 0.00120167
Iteration 20/25 | Loss: 0.00120167
Iteration 21/25 | Loss: 0.00120167
Iteration 22/25 | Loss: 0.00120167
Iteration 23/25 | Loss: 0.00120167
Iteration 24/25 | Loss: 0.00120167
Iteration 25/25 | Loss: 0.00120167

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.86410093
Iteration 2/25 | Loss: 0.00077226
Iteration 3/25 | Loss: 0.00077225
Iteration 4/25 | Loss: 0.00077225
Iteration 5/25 | Loss: 0.00077225
Iteration 6/25 | Loss: 0.00077225
Iteration 7/25 | Loss: 0.00077225
Iteration 8/25 | Loss: 0.00077225
Iteration 9/25 | Loss: 0.00077225
Iteration 10/25 | Loss: 0.00077225
Iteration 11/25 | Loss: 0.00077225
Iteration 12/25 | Loss: 0.00077225
Iteration 13/25 | Loss: 0.00077225
Iteration 14/25 | Loss: 0.00077225
Iteration 15/25 | Loss: 0.00077225
Iteration 16/25 | Loss: 0.00077225
Iteration 17/25 | Loss: 0.00077225
Iteration 18/25 | Loss: 0.00077225
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007722500595264137, 0.0007722500595264137, 0.0007722500595264137, 0.0007722500595264137, 0.0007722500595264137]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007722500595264137

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077225
Iteration 2/1000 | Loss: 0.00002311
Iteration 3/1000 | Loss: 0.00001539
Iteration 4/1000 | Loss: 0.00001387
Iteration 5/1000 | Loss: 0.00001308
Iteration 6/1000 | Loss: 0.00001245
Iteration 7/1000 | Loss: 0.00001214
Iteration 8/1000 | Loss: 0.00001209
Iteration 9/1000 | Loss: 0.00001179
Iteration 10/1000 | Loss: 0.00001156
Iteration 11/1000 | Loss: 0.00001150
Iteration 12/1000 | Loss: 0.00001150
Iteration 13/1000 | Loss: 0.00001145
Iteration 14/1000 | Loss: 0.00001141
Iteration 15/1000 | Loss: 0.00001135
Iteration 16/1000 | Loss: 0.00001132
Iteration 17/1000 | Loss: 0.00001132
Iteration 18/1000 | Loss: 0.00001131
Iteration 19/1000 | Loss: 0.00001131
Iteration 20/1000 | Loss: 0.00001131
Iteration 21/1000 | Loss: 0.00001131
Iteration 22/1000 | Loss: 0.00001131
Iteration 23/1000 | Loss: 0.00001129
Iteration 24/1000 | Loss: 0.00001129
Iteration 25/1000 | Loss: 0.00001127
Iteration 26/1000 | Loss: 0.00001127
Iteration 27/1000 | Loss: 0.00001126
Iteration 28/1000 | Loss: 0.00001126
Iteration 29/1000 | Loss: 0.00001126
Iteration 30/1000 | Loss: 0.00001126
Iteration 31/1000 | Loss: 0.00001126
Iteration 32/1000 | Loss: 0.00001125
Iteration 33/1000 | Loss: 0.00001125
Iteration 34/1000 | Loss: 0.00001125
Iteration 35/1000 | Loss: 0.00001124
Iteration 36/1000 | Loss: 0.00001124
Iteration 37/1000 | Loss: 0.00001124
Iteration 38/1000 | Loss: 0.00001123
Iteration 39/1000 | Loss: 0.00001123
Iteration 40/1000 | Loss: 0.00001123
Iteration 41/1000 | Loss: 0.00001123
Iteration 42/1000 | Loss: 0.00001123
Iteration 43/1000 | Loss: 0.00001122
Iteration 44/1000 | Loss: 0.00001122
Iteration 45/1000 | Loss: 0.00001122
Iteration 46/1000 | Loss: 0.00001122
Iteration 47/1000 | Loss: 0.00001122
Iteration 48/1000 | Loss: 0.00001122
Iteration 49/1000 | Loss: 0.00001122
Iteration 50/1000 | Loss: 0.00001122
Iteration 51/1000 | Loss: 0.00001122
Iteration 52/1000 | Loss: 0.00001122
Iteration 53/1000 | Loss: 0.00001121
Iteration 54/1000 | Loss: 0.00001121
Iteration 55/1000 | Loss: 0.00001118
Iteration 56/1000 | Loss: 0.00001117
Iteration 57/1000 | Loss: 0.00001117
Iteration 58/1000 | Loss: 0.00001117
Iteration 59/1000 | Loss: 0.00001117
Iteration 60/1000 | Loss: 0.00001117
Iteration 61/1000 | Loss: 0.00001117
Iteration 62/1000 | Loss: 0.00001117
Iteration 63/1000 | Loss: 0.00001116
Iteration 64/1000 | Loss: 0.00001116
Iteration 65/1000 | Loss: 0.00001116
Iteration 66/1000 | Loss: 0.00001112
Iteration 67/1000 | Loss: 0.00001111
Iteration 68/1000 | Loss: 0.00001111
Iteration 69/1000 | Loss: 0.00001110
Iteration 70/1000 | Loss: 0.00001110
Iteration 71/1000 | Loss: 0.00001109
Iteration 72/1000 | Loss: 0.00001107
Iteration 73/1000 | Loss: 0.00001106
Iteration 74/1000 | Loss: 0.00001105
Iteration 75/1000 | Loss: 0.00001104
Iteration 76/1000 | Loss: 0.00001104
Iteration 77/1000 | Loss: 0.00001103
Iteration 78/1000 | Loss: 0.00001103
Iteration 79/1000 | Loss: 0.00001102
Iteration 80/1000 | Loss: 0.00001101
Iteration 81/1000 | Loss: 0.00001101
Iteration 82/1000 | Loss: 0.00001100
Iteration 83/1000 | Loss: 0.00001100
Iteration 84/1000 | Loss: 0.00001099
Iteration 85/1000 | Loss: 0.00001099
Iteration 86/1000 | Loss: 0.00001098
Iteration 87/1000 | Loss: 0.00001098
Iteration 88/1000 | Loss: 0.00001097
Iteration 89/1000 | Loss: 0.00001097
Iteration 90/1000 | Loss: 0.00001097
Iteration 91/1000 | Loss: 0.00001097
Iteration 92/1000 | Loss: 0.00001096
Iteration 93/1000 | Loss: 0.00001096
Iteration 94/1000 | Loss: 0.00001095
Iteration 95/1000 | Loss: 0.00001094
Iteration 96/1000 | Loss: 0.00001094
Iteration 97/1000 | Loss: 0.00001094
Iteration 98/1000 | Loss: 0.00001094
Iteration 99/1000 | Loss: 0.00001094
Iteration 100/1000 | Loss: 0.00001094
Iteration 101/1000 | Loss: 0.00001094
Iteration 102/1000 | Loss: 0.00001094
Iteration 103/1000 | Loss: 0.00001094
Iteration 104/1000 | Loss: 0.00001094
Iteration 105/1000 | Loss: 0.00001093
Iteration 106/1000 | Loss: 0.00001093
Iteration 107/1000 | Loss: 0.00001093
Iteration 108/1000 | Loss: 0.00001093
Iteration 109/1000 | Loss: 0.00001093
Iteration 110/1000 | Loss: 0.00001092
Iteration 111/1000 | Loss: 0.00001092
Iteration 112/1000 | Loss: 0.00001091
Iteration 113/1000 | Loss: 0.00001091
Iteration 114/1000 | Loss: 0.00001091
Iteration 115/1000 | Loss: 0.00001090
Iteration 116/1000 | Loss: 0.00001090
Iteration 117/1000 | Loss: 0.00001090
Iteration 118/1000 | Loss: 0.00001090
Iteration 119/1000 | Loss: 0.00001089
Iteration 120/1000 | Loss: 0.00001089
Iteration 121/1000 | Loss: 0.00001089
Iteration 122/1000 | Loss: 0.00001089
Iteration 123/1000 | Loss: 0.00001088
Iteration 124/1000 | Loss: 0.00001088
Iteration 125/1000 | Loss: 0.00001088
Iteration 126/1000 | Loss: 0.00001087
Iteration 127/1000 | Loss: 0.00001087
Iteration 128/1000 | Loss: 0.00001086
Iteration 129/1000 | Loss: 0.00001086
Iteration 130/1000 | Loss: 0.00001086
Iteration 131/1000 | Loss: 0.00001086
Iteration 132/1000 | Loss: 0.00001085
Iteration 133/1000 | Loss: 0.00001085
Iteration 134/1000 | Loss: 0.00001085
Iteration 135/1000 | Loss: 0.00001085
Iteration 136/1000 | Loss: 0.00001085
Iteration 137/1000 | Loss: 0.00001085
Iteration 138/1000 | Loss: 0.00001085
Iteration 139/1000 | Loss: 0.00001085
Iteration 140/1000 | Loss: 0.00001084
Iteration 141/1000 | Loss: 0.00001084
Iteration 142/1000 | Loss: 0.00001084
Iteration 143/1000 | Loss: 0.00001083
Iteration 144/1000 | Loss: 0.00001083
Iteration 145/1000 | Loss: 0.00001083
Iteration 146/1000 | Loss: 0.00001083
Iteration 147/1000 | Loss: 0.00001083
Iteration 148/1000 | Loss: 0.00001083
Iteration 149/1000 | Loss: 0.00001083
Iteration 150/1000 | Loss: 0.00001083
Iteration 151/1000 | Loss: 0.00001083
Iteration 152/1000 | Loss: 0.00001083
Iteration 153/1000 | Loss: 0.00001083
Iteration 154/1000 | Loss: 0.00001083
Iteration 155/1000 | Loss: 0.00001083
Iteration 156/1000 | Loss: 0.00001083
Iteration 157/1000 | Loss: 0.00001082
Iteration 158/1000 | Loss: 0.00001082
Iteration 159/1000 | Loss: 0.00001082
Iteration 160/1000 | Loss: 0.00001082
Iteration 161/1000 | Loss: 0.00001081
Iteration 162/1000 | Loss: 0.00001081
Iteration 163/1000 | Loss: 0.00001081
Iteration 164/1000 | Loss: 0.00001081
Iteration 165/1000 | Loss: 0.00001081
Iteration 166/1000 | Loss: 0.00001081
Iteration 167/1000 | Loss: 0.00001080
Iteration 168/1000 | Loss: 0.00001080
Iteration 169/1000 | Loss: 0.00001080
Iteration 170/1000 | Loss: 0.00001080
Iteration 171/1000 | Loss: 0.00001080
Iteration 172/1000 | Loss: 0.00001080
Iteration 173/1000 | Loss: 0.00001079
Iteration 174/1000 | Loss: 0.00001079
Iteration 175/1000 | Loss: 0.00001079
Iteration 176/1000 | Loss: 0.00001079
Iteration 177/1000 | Loss: 0.00001079
Iteration 178/1000 | Loss: 0.00001079
Iteration 179/1000 | Loss: 0.00001079
Iteration 180/1000 | Loss: 0.00001079
Iteration 181/1000 | Loss: 0.00001079
Iteration 182/1000 | Loss: 0.00001079
Iteration 183/1000 | Loss: 0.00001079
Iteration 184/1000 | Loss: 0.00001079
Iteration 185/1000 | Loss: 0.00001079
Iteration 186/1000 | Loss: 0.00001079
Iteration 187/1000 | Loss: 0.00001079
Iteration 188/1000 | Loss: 0.00001079
Iteration 189/1000 | Loss: 0.00001079
Iteration 190/1000 | Loss: 0.00001079
Iteration 191/1000 | Loss: 0.00001079
Iteration 192/1000 | Loss: 0.00001079
Iteration 193/1000 | Loss: 0.00001079
Iteration 194/1000 | Loss: 0.00001079
Iteration 195/1000 | Loss: 0.00001079
Iteration 196/1000 | Loss: 0.00001079
Iteration 197/1000 | Loss: 0.00001079
Iteration 198/1000 | Loss: 0.00001079
Iteration 199/1000 | Loss: 0.00001079
Iteration 200/1000 | Loss: 0.00001079
Iteration 201/1000 | Loss: 0.00001079
Iteration 202/1000 | Loss: 0.00001079
Iteration 203/1000 | Loss: 0.00001079
Iteration 204/1000 | Loss: 0.00001079
Iteration 205/1000 | Loss: 0.00001079
Iteration 206/1000 | Loss: 0.00001079
Iteration 207/1000 | Loss: 0.00001079
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [1.0791018212330528e-05, 1.0791018212330528e-05, 1.0791018212330528e-05, 1.0791018212330528e-05, 1.0791018212330528e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0791018212330528e-05

Optimization complete. Final v2v error: 2.8057308197021484 mm

Highest mean error: 3.1341328620910645 mm for frame 231

Lowest mean error: 2.6331686973571777 mm for frame 128

Saving results

Total time: 43.65519094467163
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00750519
Iteration 2/25 | Loss: 0.00155969
Iteration 3/25 | Loss: 0.00129217
Iteration 4/25 | Loss: 0.00127709
Iteration 5/25 | Loss: 0.00127493
Iteration 6/25 | Loss: 0.00127418
Iteration 7/25 | Loss: 0.00127411
Iteration 8/25 | Loss: 0.00127411
Iteration 9/25 | Loss: 0.00127411
Iteration 10/25 | Loss: 0.00127411
Iteration 11/25 | Loss: 0.00127411
Iteration 12/25 | Loss: 0.00127411
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001274106907658279, 0.001274106907658279, 0.001274106907658279, 0.001274106907658279, 0.001274106907658279]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001274106907658279

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42411840
Iteration 2/25 | Loss: 0.00078712
Iteration 3/25 | Loss: 0.00078711
Iteration 4/25 | Loss: 0.00078711
Iteration 5/25 | Loss: 0.00078710
Iteration 6/25 | Loss: 0.00078710
Iteration 7/25 | Loss: 0.00078710
Iteration 8/25 | Loss: 0.00078710
Iteration 9/25 | Loss: 0.00078710
Iteration 10/25 | Loss: 0.00078710
Iteration 11/25 | Loss: 0.00078710
Iteration 12/25 | Loss: 0.00078710
Iteration 13/25 | Loss: 0.00078710
Iteration 14/25 | Loss: 0.00078710
Iteration 15/25 | Loss: 0.00078710
Iteration 16/25 | Loss: 0.00078710
Iteration 17/25 | Loss: 0.00078710
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007871024427004158, 0.0007871024427004158, 0.0007871024427004158, 0.0007871024427004158, 0.0007871024427004158]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007871024427004158

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078710
Iteration 2/1000 | Loss: 0.00003652
Iteration 3/1000 | Loss: 0.00002335
Iteration 4/1000 | Loss: 0.00001909
Iteration 5/1000 | Loss: 0.00001786
Iteration 6/1000 | Loss: 0.00001706
Iteration 7/1000 | Loss: 0.00001661
Iteration 8/1000 | Loss: 0.00001619
Iteration 9/1000 | Loss: 0.00001596
Iteration 10/1000 | Loss: 0.00001589
Iteration 11/1000 | Loss: 0.00001584
Iteration 12/1000 | Loss: 0.00001574
Iteration 13/1000 | Loss: 0.00001571
Iteration 14/1000 | Loss: 0.00001570
Iteration 15/1000 | Loss: 0.00001569
Iteration 16/1000 | Loss: 0.00001568
Iteration 17/1000 | Loss: 0.00001567
Iteration 18/1000 | Loss: 0.00001562
Iteration 19/1000 | Loss: 0.00001558
Iteration 20/1000 | Loss: 0.00001557
Iteration 21/1000 | Loss: 0.00001554
Iteration 22/1000 | Loss: 0.00001553
Iteration 23/1000 | Loss: 0.00001552
Iteration 24/1000 | Loss: 0.00001551
Iteration 25/1000 | Loss: 0.00001551
Iteration 26/1000 | Loss: 0.00001551
Iteration 27/1000 | Loss: 0.00001549
Iteration 28/1000 | Loss: 0.00001549
Iteration 29/1000 | Loss: 0.00001549
Iteration 30/1000 | Loss: 0.00001549
Iteration 31/1000 | Loss: 0.00001549
Iteration 32/1000 | Loss: 0.00001549
Iteration 33/1000 | Loss: 0.00001549
Iteration 34/1000 | Loss: 0.00001549
Iteration 35/1000 | Loss: 0.00001549
Iteration 36/1000 | Loss: 0.00001548
Iteration 37/1000 | Loss: 0.00001548
Iteration 38/1000 | Loss: 0.00001548
Iteration 39/1000 | Loss: 0.00001548
Iteration 40/1000 | Loss: 0.00001548
Iteration 41/1000 | Loss: 0.00001548
Iteration 42/1000 | Loss: 0.00001548
Iteration 43/1000 | Loss: 0.00001548
Iteration 44/1000 | Loss: 0.00001548
Iteration 45/1000 | Loss: 0.00001548
Iteration 46/1000 | Loss: 0.00001548
Iteration 47/1000 | Loss: 0.00001547
Iteration 48/1000 | Loss: 0.00001547
Iteration 49/1000 | Loss: 0.00001547
Iteration 50/1000 | Loss: 0.00001547
Iteration 51/1000 | Loss: 0.00001547
Iteration 52/1000 | Loss: 0.00001546
Iteration 53/1000 | Loss: 0.00001545
Iteration 54/1000 | Loss: 0.00001545
Iteration 55/1000 | Loss: 0.00001545
Iteration 56/1000 | Loss: 0.00001545
Iteration 57/1000 | Loss: 0.00001545
Iteration 58/1000 | Loss: 0.00001544
Iteration 59/1000 | Loss: 0.00001544
Iteration 60/1000 | Loss: 0.00001543
Iteration 61/1000 | Loss: 0.00001543
Iteration 62/1000 | Loss: 0.00001541
Iteration 63/1000 | Loss: 0.00001537
Iteration 64/1000 | Loss: 0.00001534
Iteration 65/1000 | Loss: 0.00001533
Iteration 66/1000 | Loss: 0.00001533
Iteration 67/1000 | Loss: 0.00001533
Iteration 68/1000 | Loss: 0.00001533
Iteration 69/1000 | Loss: 0.00001533
Iteration 70/1000 | Loss: 0.00001532
Iteration 71/1000 | Loss: 0.00001532
Iteration 72/1000 | Loss: 0.00001532
Iteration 73/1000 | Loss: 0.00001532
Iteration 74/1000 | Loss: 0.00001532
Iteration 75/1000 | Loss: 0.00001532
Iteration 76/1000 | Loss: 0.00001530
Iteration 77/1000 | Loss: 0.00001530
Iteration 78/1000 | Loss: 0.00001530
Iteration 79/1000 | Loss: 0.00001530
Iteration 80/1000 | Loss: 0.00001530
Iteration 81/1000 | Loss: 0.00001530
Iteration 82/1000 | Loss: 0.00001529
Iteration 83/1000 | Loss: 0.00001529
Iteration 84/1000 | Loss: 0.00001529
Iteration 85/1000 | Loss: 0.00001528
Iteration 86/1000 | Loss: 0.00001528
Iteration 87/1000 | Loss: 0.00001528
Iteration 88/1000 | Loss: 0.00001527
Iteration 89/1000 | Loss: 0.00001527
Iteration 90/1000 | Loss: 0.00001527
Iteration 91/1000 | Loss: 0.00001526
Iteration 92/1000 | Loss: 0.00001526
Iteration 93/1000 | Loss: 0.00001526
Iteration 94/1000 | Loss: 0.00001526
Iteration 95/1000 | Loss: 0.00001526
Iteration 96/1000 | Loss: 0.00001526
Iteration 97/1000 | Loss: 0.00001526
Iteration 98/1000 | Loss: 0.00001525
Iteration 99/1000 | Loss: 0.00001525
Iteration 100/1000 | Loss: 0.00001525
Iteration 101/1000 | Loss: 0.00001525
Iteration 102/1000 | Loss: 0.00001525
Iteration 103/1000 | Loss: 0.00001524
Iteration 104/1000 | Loss: 0.00001524
Iteration 105/1000 | Loss: 0.00001524
Iteration 106/1000 | Loss: 0.00001524
Iteration 107/1000 | Loss: 0.00001523
Iteration 108/1000 | Loss: 0.00001523
Iteration 109/1000 | Loss: 0.00001523
Iteration 110/1000 | Loss: 0.00001523
Iteration 111/1000 | Loss: 0.00001523
Iteration 112/1000 | Loss: 0.00001523
Iteration 113/1000 | Loss: 0.00001523
Iteration 114/1000 | Loss: 0.00001523
Iteration 115/1000 | Loss: 0.00001523
Iteration 116/1000 | Loss: 0.00001523
Iteration 117/1000 | Loss: 0.00001523
Iteration 118/1000 | Loss: 0.00001523
Iteration 119/1000 | Loss: 0.00001523
Iteration 120/1000 | Loss: 0.00001522
Iteration 121/1000 | Loss: 0.00001522
Iteration 122/1000 | Loss: 0.00001522
Iteration 123/1000 | Loss: 0.00001522
Iteration 124/1000 | Loss: 0.00001522
Iteration 125/1000 | Loss: 0.00001522
Iteration 126/1000 | Loss: 0.00001521
Iteration 127/1000 | Loss: 0.00001521
Iteration 128/1000 | Loss: 0.00001521
Iteration 129/1000 | Loss: 0.00001521
Iteration 130/1000 | Loss: 0.00001521
Iteration 131/1000 | Loss: 0.00001521
Iteration 132/1000 | Loss: 0.00001521
Iteration 133/1000 | Loss: 0.00001521
Iteration 134/1000 | Loss: 0.00001521
Iteration 135/1000 | Loss: 0.00001521
Iteration 136/1000 | Loss: 0.00001521
Iteration 137/1000 | Loss: 0.00001521
Iteration 138/1000 | Loss: 0.00001520
Iteration 139/1000 | Loss: 0.00001520
Iteration 140/1000 | Loss: 0.00001520
Iteration 141/1000 | Loss: 0.00001520
Iteration 142/1000 | Loss: 0.00001520
Iteration 143/1000 | Loss: 0.00001520
Iteration 144/1000 | Loss: 0.00001520
Iteration 145/1000 | Loss: 0.00001520
Iteration 146/1000 | Loss: 0.00001520
Iteration 147/1000 | Loss: 0.00001520
Iteration 148/1000 | Loss: 0.00001520
Iteration 149/1000 | Loss: 0.00001520
Iteration 150/1000 | Loss: 0.00001520
Iteration 151/1000 | Loss: 0.00001520
Iteration 152/1000 | Loss: 0.00001520
Iteration 153/1000 | Loss: 0.00001520
Iteration 154/1000 | Loss: 0.00001519
Iteration 155/1000 | Loss: 0.00001519
Iteration 156/1000 | Loss: 0.00001519
Iteration 157/1000 | Loss: 0.00001519
Iteration 158/1000 | Loss: 0.00001519
Iteration 159/1000 | Loss: 0.00001519
Iteration 160/1000 | Loss: 0.00001519
Iteration 161/1000 | Loss: 0.00001519
Iteration 162/1000 | Loss: 0.00001519
Iteration 163/1000 | Loss: 0.00001519
Iteration 164/1000 | Loss: 0.00001519
Iteration 165/1000 | Loss: 0.00001519
Iteration 166/1000 | Loss: 0.00001519
Iteration 167/1000 | Loss: 0.00001518
Iteration 168/1000 | Loss: 0.00001518
Iteration 169/1000 | Loss: 0.00001518
Iteration 170/1000 | Loss: 0.00001518
Iteration 171/1000 | Loss: 0.00001518
Iteration 172/1000 | Loss: 0.00001518
Iteration 173/1000 | Loss: 0.00001518
Iteration 174/1000 | Loss: 0.00001518
Iteration 175/1000 | Loss: 0.00001518
Iteration 176/1000 | Loss: 0.00001518
Iteration 177/1000 | Loss: 0.00001518
Iteration 178/1000 | Loss: 0.00001518
Iteration 179/1000 | Loss: 0.00001518
Iteration 180/1000 | Loss: 0.00001518
Iteration 181/1000 | Loss: 0.00001518
Iteration 182/1000 | Loss: 0.00001518
Iteration 183/1000 | Loss: 0.00001518
Iteration 184/1000 | Loss: 0.00001518
Iteration 185/1000 | Loss: 0.00001518
Iteration 186/1000 | Loss: 0.00001518
Iteration 187/1000 | Loss: 0.00001518
Iteration 188/1000 | Loss: 0.00001518
Iteration 189/1000 | Loss: 0.00001518
Iteration 190/1000 | Loss: 0.00001518
Iteration 191/1000 | Loss: 0.00001518
Iteration 192/1000 | Loss: 0.00001518
Iteration 193/1000 | Loss: 0.00001518
Iteration 194/1000 | Loss: 0.00001518
Iteration 195/1000 | Loss: 0.00001518
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [1.5179954061750323e-05, 1.5179954061750323e-05, 1.5179954061750323e-05, 1.5179954061750323e-05, 1.5179954061750323e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5179954061750323e-05

Optimization complete. Final v2v error: 3.2870078086853027 mm

Highest mean error: 3.6940290927886963 mm for frame 109

Lowest mean error: 3.0349268913269043 mm for frame 18

Saving results

Total time: 37.41924977302551
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00677508
Iteration 2/25 | Loss: 0.00174296
Iteration 3/25 | Loss: 0.00159764
Iteration 4/25 | Loss: 0.00133698
Iteration 5/25 | Loss: 0.00131978
Iteration 6/25 | Loss: 0.00130640
Iteration 7/25 | Loss: 0.00131917
Iteration 8/25 | Loss: 0.00129764
Iteration 9/25 | Loss: 0.00129644
Iteration 10/25 | Loss: 0.00129618
Iteration 11/25 | Loss: 0.00129586
Iteration 12/25 | Loss: 0.00129564
Iteration 13/25 | Loss: 0.00129559
Iteration 14/25 | Loss: 0.00129558
Iteration 15/25 | Loss: 0.00129558
Iteration 16/25 | Loss: 0.00129558
Iteration 17/25 | Loss: 0.00129558
Iteration 18/25 | Loss: 0.00129558
Iteration 19/25 | Loss: 0.00129558
Iteration 20/25 | Loss: 0.00129558
Iteration 21/25 | Loss: 0.00129558
Iteration 22/25 | Loss: 0.00129558
Iteration 23/25 | Loss: 0.00129558
Iteration 24/25 | Loss: 0.00129558
Iteration 25/25 | Loss: 0.00129558

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.20680189
Iteration 2/25 | Loss: 0.00096934
Iteration 3/25 | Loss: 0.00096801
Iteration 4/25 | Loss: 0.00096801
Iteration 5/25 | Loss: 0.00096801
Iteration 6/25 | Loss: 0.00096801
Iteration 7/25 | Loss: 0.00096801
Iteration 8/25 | Loss: 0.00096801
Iteration 9/25 | Loss: 0.00096801
Iteration 10/25 | Loss: 0.00096801
Iteration 11/25 | Loss: 0.00096801
Iteration 12/25 | Loss: 0.00096801
Iteration 13/25 | Loss: 0.00096801
Iteration 14/25 | Loss: 0.00096801
Iteration 15/25 | Loss: 0.00096801
Iteration 16/25 | Loss: 0.00096801
Iteration 17/25 | Loss: 0.00096801
Iteration 18/25 | Loss: 0.00096801
Iteration 19/25 | Loss: 0.00096801
Iteration 20/25 | Loss: 0.00096801
Iteration 21/25 | Loss: 0.00096801
Iteration 22/25 | Loss: 0.00096801
Iteration 23/25 | Loss: 0.00096801
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0009680072544142604, 0.0009680072544142604, 0.0009680072544142604, 0.0009680072544142604, 0.0009680072544142604]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009680072544142604

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096801
Iteration 2/1000 | Loss: 0.00005119
Iteration 3/1000 | Loss: 0.00004063
Iteration 4/1000 | Loss: 0.00003261
Iteration 5/1000 | Loss: 0.00002909
Iteration 6/1000 | Loss: 0.00002753
Iteration 7/1000 | Loss: 0.00002638
Iteration 8/1000 | Loss: 0.00003196
Iteration 9/1000 | Loss: 0.00002501
Iteration 10/1000 | Loss: 0.00003116
Iteration 11/1000 | Loss: 0.00002430
Iteration 12/1000 | Loss: 0.00002382
Iteration 13/1000 | Loss: 0.00041189
Iteration 14/1000 | Loss: 0.00005766
Iteration 15/1000 | Loss: 0.00002556
Iteration 16/1000 | Loss: 0.00002881
Iteration 17/1000 | Loss: 0.00002299
Iteration 18/1000 | Loss: 0.00002254
Iteration 19/1000 | Loss: 0.00002217
Iteration 20/1000 | Loss: 0.00002188
Iteration 21/1000 | Loss: 0.00002161
Iteration 22/1000 | Loss: 0.00002144
Iteration 23/1000 | Loss: 0.00002143
Iteration 24/1000 | Loss: 0.00002143
Iteration 25/1000 | Loss: 0.00002142
Iteration 26/1000 | Loss: 0.00002141
Iteration 27/1000 | Loss: 0.00002141
Iteration 28/1000 | Loss: 0.00002139
Iteration 29/1000 | Loss: 0.00002138
Iteration 30/1000 | Loss: 0.00002137
Iteration 31/1000 | Loss: 0.00002136
Iteration 32/1000 | Loss: 0.00002133
Iteration 33/1000 | Loss: 0.00002133
Iteration 34/1000 | Loss: 0.00002133
Iteration 35/1000 | Loss: 0.00002131
Iteration 36/1000 | Loss: 0.00002130
Iteration 37/1000 | Loss: 0.00002130
Iteration 38/1000 | Loss: 0.00002129
Iteration 39/1000 | Loss: 0.00002129
Iteration 40/1000 | Loss: 0.00002128
Iteration 41/1000 | Loss: 0.00002128
Iteration 42/1000 | Loss: 0.00002127
Iteration 43/1000 | Loss: 0.00002126
Iteration 44/1000 | Loss: 0.00002126
Iteration 45/1000 | Loss: 0.00002125
Iteration 46/1000 | Loss: 0.00002113
Iteration 47/1000 | Loss: 0.00002112
Iteration 48/1000 | Loss: 0.00002112
Iteration 49/1000 | Loss: 0.00002111
Iteration 50/1000 | Loss: 0.00002111
Iteration 51/1000 | Loss: 0.00002111
Iteration 52/1000 | Loss: 0.00002110
Iteration 53/1000 | Loss: 0.00002108
Iteration 54/1000 | Loss: 0.00002107
Iteration 55/1000 | Loss: 0.00002107
Iteration 56/1000 | Loss: 0.00002103
Iteration 57/1000 | Loss: 0.00002098
Iteration 58/1000 | Loss: 0.00002097
Iteration 59/1000 | Loss: 0.00002097
Iteration 60/1000 | Loss: 0.00002097
Iteration 61/1000 | Loss: 0.00002097
Iteration 62/1000 | Loss: 0.00002097
Iteration 63/1000 | Loss: 0.00002097
Iteration 64/1000 | Loss: 0.00002097
Iteration 65/1000 | Loss: 0.00002097
Iteration 66/1000 | Loss: 0.00002096
Iteration 67/1000 | Loss: 0.00002096
Iteration 68/1000 | Loss: 0.00002095
Iteration 69/1000 | Loss: 0.00002095
Iteration 70/1000 | Loss: 0.00002095
Iteration 71/1000 | Loss: 0.00002095
Iteration 72/1000 | Loss: 0.00002094
Iteration 73/1000 | Loss: 0.00002094
Iteration 74/1000 | Loss: 0.00002094
Iteration 75/1000 | Loss: 0.00002094
Iteration 76/1000 | Loss: 0.00002094
Iteration 77/1000 | Loss: 0.00002094
Iteration 78/1000 | Loss: 0.00002093
Iteration 79/1000 | Loss: 0.00002093
Iteration 80/1000 | Loss: 0.00002093
Iteration 81/1000 | Loss: 0.00002093
Iteration 82/1000 | Loss: 0.00002093
Iteration 83/1000 | Loss: 0.00002093
Iteration 84/1000 | Loss: 0.00002093
Iteration 85/1000 | Loss: 0.00002093
Iteration 86/1000 | Loss: 0.00002093
Iteration 87/1000 | Loss: 0.00002093
Iteration 88/1000 | Loss: 0.00002093
Iteration 89/1000 | Loss: 0.00002093
Iteration 90/1000 | Loss: 0.00002093
Iteration 91/1000 | Loss: 0.00002093
Iteration 92/1000 | Loss: 0.00002093
Iteration 93/1000 | Loss: 0.00002093
Iteration 94/1000 | Loss: 0.00002093
Iteration 95/1000 | Loss: 0.00002092
Iteration 96/1000 | Loss: 0.00002092
Iteration 97/1000 | Loss: 0.00002092
Iteration 98/1000 | Loss: 0.00002092
Iteration 99/1000 | Loss: 0.00002092
Iteration 100/1000 | Loss: 0.00002092
Iteration 101/1000 | Loss: 0.00002092
Iteration 102/1000 | Loss: 0.00002092
Iteration 103/1000 | Loss: 0.00002092
Iteration 104/1000 | Loss: 0.00002092
Iteration 105/1000 | Loss: 0.00002092
Iteration 106/1000 | Loss: 0.00002092
Iteration 107/1000 | Loss: 0.00002092
Iteration 108/1000 | Loss: 0.00002092
Iteration 109/1000 | Loss: 0.00002091
Iteration 110/1000 | Loss: 0.00002091
Iteration 111/1000 | Loss: 0.00002091
Iteration 112/1000 | Loss: 0.00002091
Iteration 113/1000 | Loss: 0.00002091
Iteration 114/1000 | Loss: 0.00002091
Iteration 115/1000 | Loss: 0.00002091
Iteration 116/1000 | Loss: 0.00002091
Iteration 117/1000 | Loss: 0.00002091
Iteration 118/1000 | Loss: 0.00002091
Iteration 119/1000 | Loss: 0.00002090
Iteration 120/1000 | Loss: 0.00002090
Iteration 121/1000 | Loss: 0.00002090
Iteration 122/1000 | Loss: 0.00002090
Iteration 123/1000 | Loss: 0.00002090
Iteration 124/1000 | Loss: 0.00002090
Iteration 125/1000 | Loss: 0.00002090
Iteration 126/1000 | Loss: 0.00002090
Iteration 127/1000 | Loss: 0.00002090
Iteration 128/1000 | Loss: 0.00002090
Iteration 129/1000 | Loss: 0.00002090
Iteration 130/1000 | Loss: 0.00002089
Iteration 131/1000 | Loss: 0.00002089
Iteration 132/1000 | Loss: 0.00002089
Iteration 133/1000 | Loss: 0.00002089
Iteration 134/1000 | Loss: 0.00002089
Iteration 135/1000 | Loss: 0.00002089
Iteration 136/1000 | Loss: 0.00002089
Iteration 137/1000 | Loss: 0.00002089
Iteration 138/1000 | Loss: 0.00002089
Iteration 139/1000 | Loss: 0.00002088
Iteration 140/1000 | Loss: 0.00002088
Iteration 141/1000 | Loss: 0.00002088
Iteration 142/1000 | Loss: 0.00002088
Iteration 143/1000 | Loss: 0.00002088
Iteration 144/1000 | Loss: 0.00002088
Iteration 145/1000 | Loss: 0.00002088
Iteration 146/1000 | Loss: 0.00002088
Iteration 147/1000 | Loss: 0.00002088
Iteration 148/1000 | Loss: 0.00002088
Iteration 149/1000 | Loss: 0.00002088
Iteration 150/1000 | Loss: 0.00002087
Iteration 151/1000 | Loss: 0.00002087
Iteration 152/1000 | Loss: 0.00002087
Iteration 153/1000 | Loss: 0.00002087
Iteration 154/1000 | Loss: 0.00002087
Iteration 155/1000 | Loss: 0.00002087
Iteration 156/1000 | Loss: 0.00002087
Iteration 157/1000 | Loss: 0.00002087
Iteration 158/1000 | Loss: 0.00002086
Iteration 159/1000 | Loss: 0.00002086
Iteration 160/1000 | Loss: 0.00002086
Iteration 161/1000 | Loss: 0.00002086
Iteration 162/1000 | Loss: 0.00002086
Iteration 163/1000 | Loss: 0.00002086
Iteration 164/1000 | Loss: 0.00002086
Iteration 165/1000 | Loss: 0.00002086
Iteration 166/1000 | Loss: 0.00002086
Iteration 167/1000 | Loss: 0.00002086
Iteration 168/1000 | Loss: 0.00002086
Iteration 169/1000 | Loss: 0.00002086
Iteration 170/1000 | Loss: 0.00002086
Iteration 171/1000 | Loss: 0.00002085
Iteration 172/1000 | Loss: 0.00002085
Iteration 173/1000 | Loss: 0.00002085
Iteration 174/1000 | Loss: 0.00002085
Iteration 175/1000 | Loss: 0.00002085
Iteration 176/1000 | Loss: 0.00002085
Iteration 177/1000 | Loss: 0.00002084
Iteration 178/1000 | Loss: 0.00002084
Iteration 179/1000 | Loss: 0.00002084
Iteration 180/1000 | Loss: 0.00002084
Iteration 181/1000 | Loss: 0.00002084
Iteration 182/1000 | Loss: 0.00002084
Iteration 183/1000 | Loss: 0.00002084
Iteration 184/1000 | Loss: 0.00002084
Iteration 185/1000 | Loss: 0.00002083
Iteration 186/1000 | Loss: 0.00002083
Iteration 187/1000 | Loss: 0.00002083
Iteration 188/1000 | Loss: 0.00002083
Iteration 189/1000 | Loss: 0.00002083
Iteration 190/1000 | Loss: 0.00002082
Iteration 191/1000 | Loss: 0.00002082
Iteration 192/1000 | Loss: 0.00002082
Iteration 193/1000 | Loss: 0.00002082
Iteration 194/1000 | Loss: 0.00002082
Iteration 195/1000 | Loss: 0.00002082
Iteration 196/1000 | Loss: 0.00002082
Iteration 197/1000 | Loss: 0.00002081
Iteration 198/1000 | Loss: 0.00002081
Iteration 199/1000 | Loss: 0.00002081
Iteration 200/1000 | Loss: 0.00002081
Iteration 201/1000 | Loss: 0.00002081
Iteration 202/1000 | Loss: 0.00002081
Iteration 203/1000 | Loss: 0.00002081
Iteration 204/1000 | Loss: 0.00002081
Iteration 205/1000 | Loss: 0.00002081
Iteration 206/1000 | Loss: 0.00002081
Iteration 207/1000 | Loss: 0.00002081
Iteration 208/1000 | Loss: 0.00002081
Iteration 209/1000 | Loss: 0.00002081
Iteration 210/1000 | Loss: 0.00002081
Iteration 211/1000 | Loss: 0.00002081
Iteration 212/1000 | Loss: 0.00002081
Iteration 213/1000 | Loss: 0.00002081
Iteration 214/1000 | Loss: 0.00002081
Iteration 215/1000 | Loss: 0.00002081
Iteration 216/1000 | Loss: 0.00002081
Iteration 217/1000 | Loss: 0.00002081
Iteration 218/1000 | Loss: 0.00002081
Iteration 219/1000 | Loss: 0.00002081
Iteration 220/1000 | Loss: 0.00002081
Iteration 221/1000 | Loss: 0.00002081
Iteration 222/1000 | Loss: 0.00002081
Iteration 223/1000 | Loss: 0.00002081
Iteration 224/1000 | Loss: 0.00002081
Iteration 225/1000 | Loss: 0.00002081
Iteration 226/1000 | Loss: 0.00002081
Iteration 227/1000 | Loss: 0.00002081
Iteration 228/1000 | Loss: 0.00002081
Iteration 229/1000 | Loss: 0.00002081
Iteration 230/1000 | Loss: 0.00002081
Iteration 231/1000 | Loss: 0.00002081
Iteration 232/1000 | Loss: 0.00002081
Iteration 233/1000 | Loss: 0.00002081
Iteration 234/1000 | Loss: 0.00002081
Iteration 235/1000 | Loss: 0.00002081
Iteration 236/1000 | Loss: 0.00002081
Iteration 237/1000 | Loss: 0.00002081
Iteration 238/1000 | Loss: 0.00002081
Iteration 239/1000 | Loss: 0.00002081
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 239. Stopping optimization.
Last 5 losses: [2.0807376131415367e-05, 2.0807376131415367e-05, 2.0807376131415367e-05, 2.0807376131415367e-05, 2.0807376131415367e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0807376131415367e-05

Optimization complete. Final v2v error: 3.7408175468444824 mm

Highest mean error: 5.5877814292907715 mm for frame 12

Lowest mean error: 2.765913963317871 mm for frame 115

Saving results

Total time: 80.34953808784485
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00537948
Iteration 2/25 | Loss: 0.00144210
Iteration 3/25 | Loss: 0.00132135
Iteration 4/25 | Loss: 0.00130952
Iteration 5/25 | Loss: 0.00130565
Iteration 6/25 | Loss: 0.00130476
Iteration 7/25 | Loss: 0.00130476
Iteration 8/25 | Loss: 0.00130476
Iteration 9/25 | Loss: 0.00130476
Iteration 10/25 | Loss: 0.00130476
Iteration 11/25 | Loss: 0.00130476
Iteration 12/25 | Loss: 0.00130476
Iteration 13/25 | Loss: 0.00130476
Iteration 14/25 | Loss: 0.00130476
Iteration 15/25 | Loss: 0.00130476
Iteration 16/25 | Loss: 0.00130476
Iteration 17/25 | Loss: 0.00130476
Iteration 18/25 | Loss: 0.00130476
Iteration 19/25 | Loss: 0.00130476
Iteration 20/25 | Loss: 0.00130476
Iteration 21/25 | Loss: 0.00130476
Iteration 22/25 | Loss: 0.00130476
Iteration 23/25 | Loss: 0.00130476
Iteration 24/25 | Loss: 0.00130476
Iteration 25/25 | Loss: 0.00130476

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.67640090
Iteration 2/25 | Loss: 0.00087711
Iteration 3/25 | Loss: 0.00087710
Iteration 4/25 | Loss: 0.00087710
Iteration 5/25 | Loss: 0.00087710
Iteration 6/25 | Loss: 0.00087710
Iteration 7/25 | Loss: 0.00087710
Iteration 8/25 | Loss: 0.00087710
Iteration 9/25 | Loss: 0.00087710
Iteration 10/25 | Loss: 0.00087710
Iteration 11/25 | Loss: 0.00087710
Iteration 12/25 | Loss: 0.00087710
Iteration 13/25 | Loss: 0.00087710
Iteration 14/25 | Loss: 0.00087710
Iteration 15/25 | Loss: 0.00087710
Iteration 16/25 | Loss: 0.00087710
Iteration 17/25 | Loss: 0.00087710
Iteration 18/25 | Loss: 0.00087710
Iteration 19/25 | Loss: 0.00087710
Iteration 20/25 | Loss: 0.00087710
Iteration 21/25 | Loss: 0.00087710
Iteration 22/25 | Loss: 0.00087710
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008771008579060435, 0.0008771008579060435, 0.0008771008579060435, 0.0008771008579060435, 0.0008771008579060435]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008771008579060435

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087710
Iteration 2/1000 | Loss: 0.00005490
Iteration 3/1000 | Loss: 0.00003489
Iteration 4/1000 | Loss: 0.00003108
Iteration 5/1000 | Loss: 0.00002962
Iteration 6/1000 | Loss: 0.00002839
Iteration 7/1000 | Loss: 0.00002758
Iteration 8/1000 | Loss: 0.00002671
Iteration 9/1000 | Loss: 0.00002618
Iteration 10/1000 | Loss: 0.00002554
Iteration 11/1000 | Loss: 0.00002516
Iteration 12/1000 | Loss: 0.00002477
Iteration 13/1000 | Loss: 0.00002448
Iteration 14/1000 | Loss: 0.00002419
Iteration 15/1000 | Loss: 0.00002394
Iteration 16/1000 | Loss: 0.00002373
Iteration 17/1000 | Loss: 0.00002351
Iteration 18/1000 | Loss: 0.00002331
Iteration 19/1000 | Loss: 0.00002326
Iteration 20/1000 | Loss: 0.00002315
Iteration 21/1000 | Loss: 0.00002312
Iteration 22/1000 | Loss: 0.00002305
Iteration 23/1000 | Loss: 0.00002303
Iteration 24/1000 | Loss: 0.00002302
Iteration 25/1000 | Loss: 0.00002301
Iteration 26/1000 | Loss: 0.00002301
Iteration 27/1000 | Loss: 0.00002297
Iteration 28/1000 | Loss: 0.00002297
Iteration 29/1000 | Loss: 0.00002294
Iteration 30/1000 | Loss: 0.00002292
Iteration 31/1000 | Loss: 0.00002292
Iteration 32/1000 | Loss: 0.00002289
Iteration 33/1000 | Loss: 0.00002289
Iteration 34/1000 | Loss: 0.00002288
Iteration 35/1000 | Loss: 0.00002287
Iteration 36/1000 | Loss: 0.00002287
Iteration 37/1000 | Loss: 0.00002287
Iteration 38/1000 | Loss: 0.00002287
Iteration 39/1000 | Loss: 0.00002287
Iteration 40/1000 | Loss: 0.00002287
Iteration 41/1000 | Loss: 0.00002287
Iteration 42/1000 | Loss: 0.00002287
Iteration 43/1000 | Loss: 0.00002287
Iteration 44/1000 | Loss: 0.00002286
Iteration 45/1000 | Loss: 0.00002286
Iteration 46/1000 | Loss: 0.00002285
Iteration 47/1000 | Loss: 0.00002285
Iteration 48/1000 | Loss: 0.00002284
Iteration 49/1000 | Loss: 0.00002284
Iteration 50/1000 | Loss: 0.00002283
Iteration 51/1000 | Loss: 0.00002282
Iteration 52/1000 | Loss: 0.00002282
Iteration 53/1000 | Loss: 0.00002282
Iteration 54/1000 | Loss: 0.00002282
Iteration 55/1000 | Loss: 0.00002282
Iteration 56/1000 | Loss: 0.00002282
Iteration 57/1000 | Loss: 0.00002282
Iteration 58/1000 | Loss: 0.00002282
Iteration 59/1000 | Loss: 0.00002282
Iteration 60/1000 | Loss: 0.00002282
Iteration 61/1000 | Loss: 0.00002282
Iteration 62/1000 | Loss: 0.00002282
Iteration 63/1000 | Loss: 0.00002281
Iteration 64/1000 | Loss: 0.00002281
Iteration 65/1000 | Loss: 0.00002280
Iteration 66/1000 | Loss: 0.00002280
Iteration 67/1000 | Loss: 0.00002279
Iteration 68/1000 | Loss: 0.00002279
Iteration 69/1000 | Loss: 0.00002279
Iteration 70/1000 | Loss: 0.00002279
Iteration 71/1000 | Loss: 0.00002279
Iteration 72/1000 | Loss: 0.00002279
Iteration 73/1000 | Loss: 0.00002278
Iteration 74/1000 | Loss: 0.00002278
Iteration 75/1000 | Loss: 0.00002278
Iteration 76/1000 | Loss: 0.00002277
Iteration 77/1000 | Loss: 0.00002277
Iteration 78/1000 | Loss: 0.00002276
Iteration 79/1000 | Loss: 0.00002276
Iteration 80/1000 | Loss: 0.00002276
Iteration 81/1000 | Loss: 0.00002276
Iteration 82/1000 | Loss: 0.00002276
Iteration 83/1000 | Loss: 0.00002276
Iteration 84/1000 | Loss: 0.00002276
Iteration 85/1000 | Loss: 0.00002276
Iteration 86/1000 | Loss: 0.00002276
Iteration 87/1000 | Loss: 0.00002276
Iteration 88/1000 | Loss: 0.00002276
Iteration 89/1000 | Loss: 0.00002275
Iteration 90/1000 | Loss: 0.00002275
Iteration 91/1000 | Loss: 0.00002275
Iteration 92/1000 | Loss: 0.00002275
Iteration 93/1000 | Loss: 0.00002274
Iteration 94/1000 | Loss: 0.00002274
Iteration 95/1000 | Loss: 0.00002274
Iteration 96/1000 | Loss: 0.00002274
Iteration 97/1000 | Loss: 0.00002274
Iteration 98/1000 | Loss: 0.00002274
Iteration 99/1000 | Loss: 0.00002274
Iteration 100/1000 | Loss: 0.00002274
Iteration 101/1000 | Loss: 0.00002274
Iteration 102/1000 | Loss: 0.00002274
Iteration 103/1000 | Loss: 0.00002272
Iteration 104/1000 | Loss: 0.00002272
Iteration 105/1000 | Loss: 0.00002271
Iteration 106/1000 | Loss: 0.00002271
Iteration 107/1000 | Loss: 0.00002271
Iteration 108/1000 | Loss: 0.00002271
Iteration 109/1000 | Loss: 0.00002270
Iteration 110/1000 | Loss: 0.00002270
Iteration 111/1000 | Loss: 0.00002270
Iteration 112/1000 | Loss: 0.00002270
Iteration 113/1000 | Loss: 0.00002269
Iteration 114/1000 | Loss: 0.00002269
Iteration 115/1000 | Loss: 0.00002269
Iteration 116/1000 | Loss: 0.00002268
Iteration 117/1000 | Loss: 0.00002268
Iteration 118/1000 | Loss: 0.00002267
Iteration 119/1000 | Loss: 0.00002267
Iteration 120/1000 | Loss: 0.00002267
Iteration 121/1000 | Loss: 0.00002267
Iteration 122/1000 | Loss: 0.00002267
Iteration 123/1000 | Loss: 0.00002266
Iteration 124/1000 | Loss: 0.00002266
Iteration 125/1000 | Loss: 0.00002265
Iteration 126/1000 | Loss: 0.00002265
Iteration 127/1000 | Loss: 0.00002265
Iteration 128/1000 | Loss: 0.00002264
Iteration 129/1000 | Loss: 0.00002264
Iteration 130/1000 | Loss: 0.00002264
Iteration 131/1000 | Loss: 0.00002264
Iteration 132/1000 | Loss: 0.00002264
Iteration 133/1000 | Loss: 0.00002263
Iteration 134/1000 | Loss: 0.00002263
Iteration 135/1000 | Loss: 0.00002263
Iteration 136/1000 | Loss: 0.00002263
Iteration 137/1000 | Loss: 0.00002263
Iteration 138/1000 | Loss: 0.00002263
Iteration 139/1000 | Loss: 0.00002263
Iteration 140/1000 | Loss: 0.00002263
Iteration 141/1000 | Loss: 0.00002263
Iteration 142/1000 | Loss: 0.00002263
Iteration 143/1000 | Loss: 0.00002263
Iteration 144/1000 | Loss: 0.00002263
Iteration 145/1000 | Loss: 0.00002263
Iteration 146/1000 | Loss: 0.00002263
Iteration 147/1000 | Loss: 0.00002263
Iteration 148/1000 | Loss: 0.00002263
Iteration 149/1000 | Loss: 0.00002263
Iteration 150/1000 | Loss: 0.00002263
Iteration 151/1000 | Loss: 0.00002263
Iteration 152/1000 | Loss: 0.00002263
Iteration 153/1000 | Loss: 0.00002263
Iteration 154/1000 | Loss: 0.00002263
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [2.262782072648406e-05, 2.262782072648406e-05, 2.262782072648406e-05, 2.262782072648406e-05, 2.262782072648406e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.262782072648406e-05

Optimization complete. Final v2v error: 3.929037094116211 mm

Highest mean error: 4.265779972076416 mm for frame 23

Lowest mean error: 3.847031354904175 mm for frame 164

Saving results

Total time: 56.912376165390015
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00768224
Iteration 2/25 | Loss: 0.00163631
Iteration 3/25 | Loss: 0.00130461
Iteration 4/25 | Loss: 0.00126174
Iteration 5/25 | Loss: 0.00125391
Iteration 6/25 | Loss: 0.00125207
Iteration 7/25 | Loss: 0.00125207
Iteration 8/25 | Loss: 0.00125207
Iteration 9/25 | Loss: 0.00125207
Iteration 10/25 | Loss: 0.00125207
Iteration 11/25 | Loss: 0.00125207
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012520652962848544, 0.0012520652962848544, 0.0012520652962848544, 0.0012520652962848544, 0.0012520652962848544]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012520652962848544

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49003339
Iteration 2/25 | Loss: 0.00079930
Iteration 3/25 | Loss: 0.00079930
Iteration 4/25 | Loss: 0.00079930
Iteration 5/25 | Loss: 0.00079929
Iteration 6/25 | Loss: 0.00079929
Iteration 7/25 | Loss: 0.00079929
Iteration 8/25 | Loss: 0.00079929
Iteration 9/25 | Loss: 0.00079929
Iteration 10/25 | Loss: 0.00079929
Iteration 11/25 | Loss: 0.00079929
Iteration 12/25 | Loss: 0.00079929
Iteration 13/25 | Loss: 0.00079929
Iteration 14/25 | Loss: 0.00079929
Iteration 15/25 | Loss: 0.00079929
Iteration 16/25 | Loss: 0.00079929
Iteration 17/25 | Loss: 0.00079929
Iteration 18/25 | Loss: 0.00079929
Iteration 19/25 | Loss: 0.00079929
Iteration 20/25 | Loss: 0.00079929
Iteration 21/25 | Loss: 0.00079929
Iteration 22/25 | Loss: 0.00079929
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007992928731255233, 0.0007992928731255233, 0.0007992928731255233, 0.0007992928731255233, 0.0007992928731255233]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007992928731255233

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079929
Iteration 2/1000 | Loss: 0.00005141
Iteration 3/1000 | Loss: 0.00002730
Iteration 4/1000 | Loss: 0.00002134
Iteration 5/1000 | Loss: 0.00001968
Iteration 6/1000 | Loss: 0.00001859
Iteration 7/1000 | Loss: 0.00001783
Iteration 8/1000 | Loss: 0.00001723
Iteration 9/1000 | Loss: 0.00001693
Iteration 10/1000 | Loss: 0.00001665
Iteration 11/1000 | Loss: 0.00001641
Iteration 12/1000 | Loss: 0.00001628
Iteration 13/1000 | Loss: 0.00001626
Iteration 14/1000 | Loss: 0.00001625
Iteration 15/1000 | Loss: 0.00001624
Iteration 16/1000 | Loss: 0.00001624
Iteration 17/1000 | Loss: 0.00001624
Iteration 18/1000 | Loss: 0.00001624
Iteration 19/1000 | Loss: 0.00001623
Iteration 20/1000 | Loss: 0.00001622
Iteration 21/1000 | Loss: 0.00001621
Iteration 22/1000 | Loss: 0.00001621
Iteration 23/1000 | Loss: 0.00001620
Iteration 24/1000 | Loss: 0.00001620
Iteration 25/1000 | Loss: 0.00001619
Iteration 26/1000 | Loss: 0.00001616
Iteration 27/1000 | Loss: 0.00001616
Iteration 28/1000 | Loss: 0.00001613
Iteration 29/1000 | Loss: 0.00001613
Iteration 30/1000 | Loss: 0.00001613
Iteration 31/1000 | Loss: 0.00001613
Iteration 32/1000 | Loss: 0.00001613
Iteration 33/1000 | Loss: 0.00001613
Iteration 34/1000 | Loss: 0.00001613
Iteration 35/1000 | Loss: 0.00001612
Iteration 36/1000 | Loss: 0.00001612
Iteration 37/1000 | Loss: 0.00001612
Iteration 38/1000 | Loss: 0.00001612
Iteration 39/1000 | Loss: 0.00001610
Iteration 40/1000 | Loss: 0.00001609
Iteration 41/1000 | Loss: 0.00001609
Iteration 42/1000 | Loss: 0.00001609
Iteration 43/1000 | Loss: 0.00001608
Iteration 44/1000 | Loss: 0.00001608
Iteration 45/1000 | Loss: 0.00001608
Iteration 46/1000 | Loss: 0.00001608
Iteration 47/1000 | Loss: 0.00001607
Iteration 48/1000 | Loss: 0.00001607
Iteration 49/1000 | Loss: 0.00001607
Iteration 50/1000 | Loss: 0.00001606
Iteration 51/1000 | Loss: 0.00001606
Iteration 52/1000 | Loss: 0.00001606
Iteration 53/1000 | Loss: 0.00001606
Iteration 54/1000 | Loss: 0.00001606
Iteration 55/1000 | Loss: 0.00001606
Iteration 56/1000 | Loss: 0.00001606
Iteration 57/1000 | Loss: 0.00001606
Iteration 58/1000 | Loss: 0.00001605
Iteration 59/1000 | Loss: 0.00001605
Iteration 60/1000 | Loss: 0.00001605
Iteration 61/1000 | Loss: 0.00001605
Iteration 62/1000 | Loss: 0.00001605
Iteration 63/1000 | Loss: 0.00001605
Iteration 64/1000 | Loss: 0.00001605
Iteration 65/1000 | Loss: 0.00001604
Iteration 66/1000 | Loss: 0.00001604
Iteration 67/1000 | Loss: 0.00001604
Iteration 68/1000 | Loss: 0.00001604
Iteration 69/1000 | Loss: 0.00001604
Iteration 70/1000 | Loss: 0.00001604
Iteration 71/1000 | Loss: 0.00001604
Iteration 72/1000 | Loss: 0.00001603
Iteration 73/1000 | Loss: 0.00001603
Iteration 74/1000 | Loss: 0.00001603
Iteration 75/1000 | Loss: 0.00001603
Iteration 76/1000 | Loss: 0.00001603
Iteration 77/1000 | Loss: 0.00001603
Iteration 78/1000 | Loss: 0.00001603
Iteration 79/1000 | Loss: 0.00001602
Iteration 80/1000 | Loss: 0.00001602
Iteration 81/1000 | Loss: 0.00001602
Iteration 82/1000 | Loss: 0.00001602
Iteration 83/1000 | Loss: 0.00001602
Iteration 84/1000 | Loss: 0.00001602
Iteration 85/1000 | Loss: 0.00001602
Iteration 86/1000 | Loss: 0.00001602
Iteration 87/1000 | Loss: 0.00001602
Iteration 88/1000 | Loss: 0.00001602
Iteration 89/1000 | Loss: 0.00001602
Iteration 90/1000 | Loss: 0.00001602
Iteration 91/1000 | Loss: 0.00001602
Iteration 92/1000 | Loss: 0.00001601
Iteration 93/1000 | Loss: 0.00001601
Iteration 94/1000 | Loss: 0.00001601
Iteration 95/1000 | Loss: 0.00001601
Iteration 96/1000 | Loss: 0.00001601
Iteration 97/1000 | Loss: 0.00001601
Iteration 98/1000 | Loss: 0.00001601
Iteration 99/1000 | Loss: 0.00001601
Iteration 100/1000 | Loss: 0.00001601
Iteration 101/1000 | Loss: 0.00001601
Iteration 102/1000 | Loss: 0.00001601
Iteration 103/1000 | Loss: 0.00001601
Iteration 104/1000 | Loss: 0.00001600
Iteration 105/1000 | Loss: 0.00001600
Iteration 106/1000 | Loss: 0.00001600
Iteration 107/1000 | Loss: 0.00001600
Iteration 108/1000 | Loss: 0.00001600
Iteration 109/1000 | Loss: 0.00001600
Iteration 110/1000 | Loss: 0.00001600
Iteration 111/1000 | Loss: 0.00001600
Iteration 112/1000 | Loss: 0.00001600
Iteration 113/1000 | Loss: 0.00001600
Iteration 114/1000 | Loss: 0.00001599
Iteration 115/1000 | Loss: 0.00001599
Iteration 116/1000 | Loss: 0.00001599
Iteration 117/1000 | Loss: 0.00001599
Iteration 118/1000 | Loss: 0.00001599
Iteration 119/1000 | Loss: 0.00001599
Iteration 120/1000 | Loss: 0.00001599
Iteration 121/1000 | Loss: 0.00001598
Iteration 122/1000 | Loss: 0.00001598
Iteration 123/1000 | Loss: 0.00001598
Iteration 124/1000 | Loss: 0.00001598
Iteration 125/1000 | Loss: 0.00001598
Iteration 126/1000 | Loss: 0.00001598
Iteration 127/1000 | Loss: 0.00001598
Iteration 128/1000 | Loss: 0.00001598
Iteration 129/1000 | Loss: 0.00001598
Iteration 130/1000 | Loss: 0.00001598
Iteration 131/1000 | Loss: 0.00001598
Iteration 132/1000 | Loss: 0.00001597
Iteration 133/1000 | Loss: 0.00001597
Iteration 134/1000 | Loss: 0.00001597
Iteration 135/1000 | Loss: 0.00001597
Iteration 136/1000 | Loss: 0.00001597
Iteration 137/1000 | Loss: 0.00001597
Iteration 138/1000 | Loss: 0.00001597
Iteration 139/1000 | Loss: 0.00001597
Iteration 140/1000 | Loss: 0.00001597
Iteration 141/1000 | Loss: 0.00001596
Iteration 142/1000 | Loss: 0.00001596
Iteration 143/1000 | Loss: 0.00001596
Iteration 144/1000 | Loss: 0.00001596
Iteration 145/1000 | Loss: 0.00001596
Iteration 146/1000 | Loss: 0.00001596
Iteration 147/1000 | Loss: 0.00001596
Iteration 148/1000 | Loss: 0.00001596
Iteration 149/1000 | Loss: 0.00001596
Iteration 150/1000 | Loss: 0.00001596
Iteration 151/1000 | Loss: 0.00001596
Iteration 152/1000 | Loss: 0.00001596
Iteration 153/1000 | Loss: 0.00001596
Iteration 154/1000 | Loss: 0.00001596
Iteration 155/1000 | Loss: 0.00001596
Iteration 156/1000 | Loss: 0.00001596
Iteration 157/1000 | Loss: 0.00001596
Iteration 158/1000 | Loss: 0.00001596
Iteration 159/1000 | Loss: 0.00001596
Iteration 160/1000 | Loss: 0.00001596
Iteration 161/1000 | Loss: 0.00001596
Iteration 162/1000 | Loss: 0.00001596
Iteration 163/1000 | Loss: 0.00001596
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.5957746654748917e-05, 1.5957746654748917e-05, 1.5957746654748917e-05, 1.5957746654748917e-05, 1.5957746654748917e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5957746654748917e-05

Optimization complete. Final v2v error: 3.409470319747925 mm

Highest mean error: 3.7152318954467773 mm for frame 139

Lowest mean error: 2.9849677085876465 mm for frame 38

Saving results

Total time: 37.481048822402954
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00506805
Iteration 2/25 | Loss: 0.00149261
Iteration 3/25 | Loss: 0.00127547
Iteration 4/25 | Loss: 0.00124835
Iteration 5/25 | Loss: 0.00124489
Iteration 6/25 | Loss: 0.00124422
Iteration 7/25 | Loss: 0.00124422
Iteration 8/25 | Loss: 0.00124422
Iteration 9/25 | Loss: 0.00124422
Iteration 10/25 | Loss: 0.00124422
Iteration 11/25 | Loss: 0.00124422
Iteration 12/25 | Loss: 0.00124422
Iteration 13/25 | Loss: 0.00124422
Iteration 14/25 | Loss: 0.00124422
Iteration 15/25 | Loss: 0.00124422
Iteration 16/25 | Loss: 0.00124422
Iteration 17/25 | Loss: 0.00124422
Iteration 18/25 | Loss: 0.00124422
Iteration 19/25 | Loss: 0.00124422
Iteration 20/25 | Loss: 0.00124422
Iteration 21/25 | Loss: 0.00124422
Iteration 22/25 | Loss: 0.00124422
Iteration 23/25 | Loss: 0.00124422
Iteration 24/25 | Loss: 0.00124422
Iteration 25/25 | Loss: 0.00124422

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45845771
Iteration 2/25 | Loss: 0.00079044
Iteration 3/25 | Loss: 0.00079044
Iteration 4/25 | Loss: 0.00079044
Iteration 5/25 | Loss: 0.00079044
Iteration 6/25 | Loss: 0.00079044
Iteration 7/25 | Loss: 0.00079044
Iteration 8/25 | Loss: 0.00079043
Iteration 9/25 | Loss: 0.00079043
Iteration 10/25 | Loss: 0.00079043
Iteration 11/25 | Loss: 0.00079043
Iteration 12/25 | Loss: 0.00079043
Iteration 13/25 | Loss: 0.00079043
Iteration 14/25 | Loss: 0.00079043
Iteration 15/25 | Loss: 0.00079043
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0007904344820417464, 0.0007904344820417464, 0.0007904344820417464, 0.0007904344820417464, 0.0007904344820417464]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007904344820417464

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079043
Iteration 2/1000 | Loss: 0.00002784
Iteration 3/1000 | Loss: 0.00002009
Iteration 4/1000 | Loss: 0.00001775
Iteration 5/1000 | Loss: 0.00001673
Iteration 6/1000 | Loss: 0.00001601
Iteration 7/1000 | Loss: 0.00001553
Iteration 8/1000 | Loss: 0.00001529
Iteration 9/1000 | Loss: 0.00001526
Iteration 10/1000 | Loss: 0.00001499
Iteration 11/1000 | Loss: 0.00001485
Iteration 12/1000 | Loss: 0.00001473
Iteration 13/1000 | Loss: 0.00001471
Iteration 14/1000 | Loss: 0.00001463
Iteration 15/1000 | Loss: 0.00001458
Iteration 16/1000 | Loss: 0.00001455
Iteration 17/1000 | Loss: 0.00001455
Iteration 18/1000 | Loss: 0.00001455
Iteration 19/1000 | Loss: 0.00001455
Iteration 20/1000 | Loss: 0.00001454
Iteration 21/1000 | Loss: 0.00001453
Iteration 22/1000 | Loss: 0.00001453
Iteration 23/1000 | Loss: 0.00001452
Iteration 24/1000 | Loss: 0.00001451
Iteration 25/1000 | Loss: 0.00001451
Iteration 26/1000 | Loss: 0.00001451
Iteration 27/1000 | Loss: 0.00001451
Iteration 28/1000 | Loss: 0.00001451
Iteration 29/1000 | Loss: 0.00001451
Iteration 30/1000 | Loss: 0.00001451
Iteration 31/1000 | Loss: 0.00001451
Iteration 32/1000 | Loss: 0.00001451
Iteration 33/1000 | Loss: 0.00001451
Iteration 34/1000 | Loss: 0.00001451
Iteration 35/1000 | Loss: 0.00001450
Iteration 36/1000 | Loss: 0.00001450
Iteration 37/1000 | Loss: 0.00001450
Iteration 38/1000 | Loss: 0.00001450
Iteration 39/1000 | Loss: 0.00001450
Iteration 40/1000 | Loss: 0.00001450
Iteration 41/1000 | Loss: 0.00001450
Iteration 42/1000 | Loss: 0.00001448
Iteration 43/1000 | Loss: 0.00001448
Iteration 44/1000 | Loss: 0.00001448
Iteration 45/1000 | Loss: 0.00001447
Iteration 46/1000 | Loss: 0.00001447
Iteration 47/1000 | Loss: 0.00001446
Iteration 48/1000 | Loss: 0.00001446
Iteration 49/1000 | Loss: 0.00001446
Iteration 50/1000 | Loss: 0.00001446
Iteration 51/1000 | Loss: 0.00001446
Iteration 52/1000 | Loss: 0.00001446
Iteration 53/1000 | Loss: 0.00001445
Iteration 54/1000 | Loss: 0.00001445
Iteration 55/1000 | Loss: 0.00001445
Iteration 56/1000 | Loss: 0.00001444
Iteration 57/1000 | Loss: 0.00001444
Iteration 58/1000 | Loss: 0.00001444
Iteration 59/1000 | Loss: 0.00001444
Iteration 60/1000 | Loss: 0.00001444
Iteration 61/1000 | Loss: 0.00001444
Iteration 62/1000 | Loss: 0.00001444
Iteration 63/1000 | Loss: 0.00001444
Iteration 64/1000 | Loss: 0.00001443
Iteration 65/1000 | Loss: 0.00001443
Iteration 66/1000 | Loss: 0.00001442
Iteration 67/1000 | Loss: 0.00001442
Iteration 68/1000 | Loss: 0.00001441
Iteration 69/1000 | Loss: 0.00001441
Iteration 70/1000 | Loss: 0.00001440
Iteration 71/1000 | Loss: 0.00001440
Iteration 72/1000 | Loss: 0.00001439
Iteration 73/1000 | Loss: 0.00001439
Iteration 74/1000 | Loss: 0.00001438
Iteration 75/1000 | Loss: 0.00001438
Iteration 76/1000 | Loss: 0.00001438
Iteration 77/1000 | Loss: 0.00001438
Iteration 78/1000 | Loss: 0.00001437
Iteration 79/1000 | Loss: 0.00001437
Iteration 80/1000 | Loss: 0.00001437
Iteration 81/1000 | Loss: 0.00001437
Iteration 82/1000 | Loss: 0.00001437
Iteration 83/1000 | Loss: 0.00001436
Iteration 84/1000 | Loss: 0.00001436
Iteration 85/1000 | Loss: 0.00001436
Iteration 86/1000 | Loss: 0.00001435
Iteration 87/1000 | Loss: 0.00001435
Iteration 88/1000 | Loss: 0.00001435
Iteration 89/1000 | Loss: 0.00001435
Iteration 90/1000 | Loss: 0.00001435
Iteration 91/1000 | Loss: 0.00001435
Iteration 92/1000 | Loss: 0.00001434
Iteration 93/1000 | Loss: 0.00001434
Iteration 94/1000 | Loss: 0.00001434
Iteration 95/1000 | Loss: 0.00001434
Iteration 96/1000 | Loss: 0.00001434
Iteration 97/1000 | Loss: 0.00001434
Iteration 98/1000 | Loss: 0.00001434
Iteration 99/1000 | Loss: 0.00001433
Iteration 100/1000 | Loss: 0.00001433
Iteration 101/1000 | Loss: 0.00001433
Iteration 102/1000 | Loss: 0.00001433
Iteration 103/1000 | Loss: 0.00001433
Iteration 104/1000 | Loss: 0.00001433
Iteration 105/1000 | Loss: 0.00001433
Iteration 106/1000 | Loss: 0.00001432
Iteration 107/1000 | Loss: 0.00001432
Iteration 108/1000 | Loss: 0.00001432
Iteration 109/1000 | Loss: 0.00001432
Iteration 110/1000 | Loss: 0.00001432
Iteration 111/1000 | Loss: 0.00001432
Iteration 112/1000 | Loss: 0.00001431
Iteration 113/1000 | Loss: 0.00001431
Iteration 114/1000 | Loss: 0.00001431
Iteration 115/1000 | Loss: 0.00001431
Iteration 116/1000 | Loss: 0.00001430
Iteration 117/1000 | Loss: 0.00001430
Iteration 118/1000 | Loss: 0.00001429
Iteration 119/1000 | Loss: 0.00001429
Iteration 120/1000 | Loss: 0.00001428
Iteration 121/1000 | Loss: 0.00001428
Iteration 122/1000 | Loss: 0.00001428
Iteration 123/1000 | Loss: 0.00001428
Iteration 124/1000 | Loss: 0.00001428
Iteration 125/1000 | Loss: 0.00001428
Iteration 126/1000 | Loss: 0.00001428
Iteration 127/1000 | Loss: 0.00001428
Iteration 128/1000 | Loss: 0.00001427
Iteration 129/1000 | Loss: 0.00001427
Iteration 130/1000 | Loss: 0.00001427
Iteration 131/1000 | Loss: 0.00001427
Iteration 132/1000 | Loss: 0.00001427
Iteration 133/1000 | Loss: 0.00001427
Iteration 134/1000 | Loss: 0.00001425
Iteration 135/1000 | Loss: 0.00001425
Iteration 136/1000 | Loss: 0.00001425
Iteration 137/1000 | Loss: 0.00001425
Iteration 138/1000 | Loss: 0.00001425
Iteration 139/1000 | Loss: 0.00001424
Iteration 140/1000 | Loss: 0.00001424
Iteration 141/1000 | Loss: 0.00001423
Iteration 142/1000 | Loss: 0.00001423
Iteration 143/1000 | Loss: 0.00001422
Iteration 144/1000 | Loss: 0.00001422
Iteration 145/1000 | Loss: 0.00001422
Iteration 146/1000 | Loss: 0.00001422
Iteration 147/1000 | Loss: 0.00001422
Iteration 148/1000 | Loss: 0.00001422
Iteration 149/1000 | Loss: 0.00001422
Iteration 150/1000 | Loss: 0.00001421
Iteration 151/1000 | Loss: 0.00001421
Iteration 152/1000 | Loss: 0.00001421
Iteration 153/1000 | Loss: 0.00001421
Iteration 154/1000 | Loss: 0.00001421
Iteration 155/1000 | Loss: 0.00001421
Iteration 156/1000 | Loss: 0.00001421
Iteration 157/1000 | Loss: 0.00001421
Iteration 158/1000 | Loss: 0.00001420
Iteration 159/1000 | Loss: 0.00001420
Iteration 160/1000 | Loss: 0.00001420
Iteration 161/1000 | Loss: 0.00001419
Iteration 162/1000 | Loss: 0.00001419
Iteration 163/1000 | Loss: 0.00001419
Iteration 164/1000 | Loss: 0.00001419
Iteration 165/1000 | Loss: 0.00001419
Iteration 166/1000 | Loss: 0.00001419
Iteration 167/1000 | Loss: 0.00001419
Iteration 168/1000 | Loss: 0.00001419
Iteration 169/1000 | Loss: 0.00001419
Iteration 170/1000 | Loss: 0.00001419
Iteration 171/1000 | Loss: 0.00001418
Iteration 172/1000 | Loss: 0.00001418
Iteration 173/1000 | Loss: 0.00001418
Iteration 174/1000 | Loss: 0.00001418
Iteration 175/1000 | Loss: 0.00001418
Iteration 176/1000 | Loss: 0.00001418
Iteration 177/1000 | Loss: 0.00001418
Iteration 178/1000 | Loss: 0.00001418
Iteration 179/1000 | Loss: 0.00001418
Iteration 180/1000 | Loss: 0.00001418
Iteration 181/1000 | Loss: 0.00001417
Iteration 182/1000 | Loss: 0.00001417
Iteration 183/1000 | Loss: 0.00001417
Iteration 184/1000 | Loss: 0.00001417
Iteration 185/1000 | Loss: 0.00001417
Iteration 186/1000 | Loss: 0.00001416
Iteration 187/1000 | Loss: 0.00001416
Iteration 188/1000 | Loss: 0.00001416
Iteration 189/1000 | Loss: 0.00001416
Iteration 190/1000 | Loss: 0.00001416
Iteration 191/1000 | Loss: 0.00001416
Iteration 192/1000 | Loss: 0.00001416
Iteration 193/1000 | Loss: 0.00001415
Iteration 194/1000 | Loss: 0.00001415
Iteration 195/1000 | Loss: 0.00001415
Iteration 196/1000 | Loss: 0.00001415
Iteration 197/1000 | Loss: 0.00001415
Iteration 198/1000 | Loss: 0.00001415
Iteration 199/1000 | Loss: 0.00001415
Iteration 200/1000 | Loss: 0.00001415
Iteration 201/1000 | Loss: 0.00001415
Iteration 202/1000 | Loss: 0.00001414
Iteration 203/1000 | Loss: 0.00001414
Iteration 204/1000 | Loss: 0.00001414
Iteration 205/1000 | Loss: 0.00001414
Iteration 206/1000 | Loss: 0.00001413
Iteration 207/1000 | Loss: 0.00001413
Iteration 208/1000 | Loss: 0.00001413
Iteration 209/1000 | Loss: 0.00001413
Iteration 210/1000 | Loss: 0.00001413
Iteration 211/1000 | Loss: 0.00001413
Iteration 212/1000 | Loss: 0.00001412
Iteration 213/1000 | Loss: 0.00001412
Iteration 214/1000 | Loss: 0.00001412
Iteration 215/1000 | Loss: 0.00001412
Iteration 216/1000 | Loss: 0.00001412
Iteration 217/1000 | Loss: 0.00001412
Iteration 218/1000 | Loss: 0.00001412
Iteration 219/1000 | Loss: 0.00001411
Iteration 220/1000 | Loss: 0.00001411
Iteration 221/1000 | Loss: 0.00001411
Iteration 222/1000 | Loss: 0.00001411
Iteration 223/1000 | Loss: 0.00001411
Iteration 224/1000 | Loss: 0.00001411
Iteration 225/1000 | Loss: 0.00001411
Iteration 226/1000 | Loss: 0.00001411
Iteration 227/1000 | Loss: 0.00001410
Iteration 228/1000 | Loss: 0.00001410
Iteration 229/1000 | Loss: 0.00001410
Iteration 230/1000 | Loss: 0.00001410
Iteration 231/1000 | Loss: 0.00001410
Iteration 232/1000 | Loss: 0.00001410
Iteration 233/1000 | Loss: 0.00001410
Iteration 234/1000 | Loss: 0.00001410
Iteration 235/1000 | Loss: 0.00001410
Iteration 236/1000 | Loss: 0.00001410
Iteration 237/1000 | Loss: 0.00001410
Iteration 238/1000 | Loss: 0.00001410
Iteration 239/1000 | Loss: 0.00001409
Iteration 240/1000 | Loss: 0.00001409
Iteration 241/1000 | Loss: 0.00001409
Iteration 242/1000 | Loss: 0.00001409
Iteration 243/1000 | Loss: 0.00001409
Iteration 244/1000 | Loss: 0.00001409
Iteration 245/1000 | Loss: 0.00001409
Iteration 246/1000 | Loss: 0.00001409
Iteration 247/1000 | Loss: 0.00001409
Iteration 248/1000 | Loss: 0.00001409
Iteration 249/1000 | Loss: 0.00001409
Iteration 250/1000 | Loss: 0.00001409
Iteration 251/1000 | Loss: 0.00001409
Iteration 252/1000 | Loss: 0.00001409
Iteration 253/1000 | Loss: 0.00001409
Iteration 254/1000 | Loss: 0.00001409
Iteration 255/1000 | Loss: 0.00001409
Iteration 256/1000 | Loss: 0.00001409
Iteration 257/1000 | Loss: 0.00001409
Iteration 258/1000 | Loss: 0.00001409
Iteration 259/1000 | Loss: 0.00001409
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 259. Stopping optimization.
Last 5 losses: [1.409311607858399e-05, 1.409311607858399e-05, 1.409311607858399e-05, 1.409311607858399e-05, 1.409311607858399e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.409311607858399e-05

Optimization complete. Final v2v error: 3.074934244155884 mm

Highest mean error: 4.656804084777832 mm for frame 83

Lowest mean error: 2.692530632019043 mm for frame 163

Saving results

Total time: 46.99442100524902
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00367969
Iteration 2/25 | Loss: 0.00125935
Iteration 3/25 | Loss: 0.00119190
Iteration 4/25 | Loss: 0.00118195
Iteration 5/25 | Loss: 0.00117844
Iteration 6/25 | Loss: 0.00117844
Iteration 7/25 | Loss: 0.00117844
Iteration 8/25 | Loss: 0.00117844
Iteration 9/25 | Loss: 0.00117844
Iteration 10/25 | Loss: 0.00117844
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001178441452793777, 0.001178441452793777, 0.001178441452793777, 0.001178441452793777, 0.001178441452793777]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001178441452793777

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.69467783
Iteration 2/25 | Loss: 0.00074889
Iteration 3/25 | Loss: 0.00074888
Iteration 4/25 | Loss: 0.00074888
Iteration 5/25 | Loss: 0.00074888
Iteration 6/25 | Loss: 0.00074888
Iteration 7/25 | Loss: 0.00074888
Iteration 8/25 | Loss: 0.00074888
Iteration 9/25 | Loss: 0.00074888
Iteration 10/25 | Loss: 0.00074888
Iteration 11/25 | Loss: 0.00074888
Iteration 12/25 | Loss: 0.00074888
Iteration 13/25 | Loss: 0.00074888
Iteration 14/25 | Loss: 0.00074888
Iteration 15/25 | Loss: 0.00074888
Iteration 16/25 | Loss: 0.00074888
Iteration 17/25 | Loss: 0.00074888
Iteration 18/25 | Loss: 0.00074888
Iteration 19/25 | Loss: 0.00074888
Iteration 20/25 | Loss: 0.00074888
Iteration 21/25 | Loss: 0.00074888
Iteration 22/25 | Loss: 0.00074888
Iteration 23/25 | Loss: 0.00074888
Iteration 24/25 | Loss: 0.00074888
Iteration 25/25 | Loss: 0.00074888

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074888
Iteration 2/1000 | Loss: 0.00001830
Iteration 3/1000 | Loss: 0.00001352
Iteration 4/1000 | Loss: 0.00001240
Iteration 5/1000 | Loss: 0.00001181
Iteration 6/1000 | Loss: 0.00001151
Iteration 7/1000 | Loss: 0.00001133
Iteration 8/1000 | Loss: 0.00001133
Iteration 9/1000 | Loss: 0.00001108
Iteration 10/1000 | Loss: 0.00001083
Iteration 11/1000 | Loss: 0.00001082
Iteration 12/1000 | Loss: 0.00001070
Iteration 13/1000 | Loss: 0.00001062
Iteration 14/1000 | Loss: 0.00001058
Iteration 15/1000 | Loss: 0.00001055
Iteration 16/1000 | Loss: 0.00001055
Iteration 17/1000 | Loss: 0.00001055
Iteration 18/1000 | Loss: 0.00001055
Iteration 19/1000 | Loss: 0.00001055
Iteration 20/1000 | Loss: 0.00001054
Iteration 21/1000 | Loss: 0.00001054
Iteration 22/1000 | Loss: 0.00001054
Iteration 23/1000 | Loss: 0.00001054
Iteration 24/1000 | Loss: 0.00001053
Iteration 25/1000 | Loss: 0.00001051
Iteration 26/1000 | Loss: 0.00001050
Iteration 27/1000 | Loss: 0.00001050
Iteration 28/1000 | Loss: 0.00001049
Iteration 29/1000 | Loss: 0.00001048
Iteration 30/1000 | Loss: 0.00001046
Iteration 31/1000 | Loss: 0.00001046
Iteration 32/1000 | Loss: 0.00001046
Iteration 33/1000 | Loss: 0.00001046
Iteration 34/1000 | Loss: 0.00001045
Iteration 35/1000 | Loss: 0.00001045
Iteration 36/1000 | Loss: 0.00001045
Iteration 37/1000 | Loss: 0.00001043
Iteration 38/1000 | Loss: 0.00001043
Iteration 39/1000 | Loss: 0.00001041
Iteration 40/1000 | Loss: 0.00001040
Iteration 41/1000 | Loss: 0.00001039
Iteration 42/1000 | Loss: 0.00001039
Iteration 43/1000 | Loss: 0.00001039
Iteration 44/1000 | Loss: 0.00001039
Iteration 45/1000 | Loss: 0.00001038
Iteration 46/1000 | Loss: 0.00001038
Iteration 47/1000 | Loss: 0.00001035
Iteration 48/1000 | Loss: 0.00001035
Iteration 49/1000 | Loss: 0.00001033
Iteration 50/1000 | Loss: 0.00001032
Iteration 51/1000 | Loss: 0.00001032
Iteration 52/1000 | Loss: 0.00001032
Iteration 53/1000 | Loss: 0.00001031
Iteration 54/1000 | Loss: 0.00001031
Iteration 55/1000 | Loss: 0.00001030
Iteration 56/1000 | Loss: 0.00001029
Iteration 57/1000 | Loss: 0.00001028
Iteration 58/1000 | Loss: 0.00001028
Iteration 59/1000 | Loss: 0.00001027
Iteration 60/1000 | Loss: 0.00001027
Iteration 61/1000 | Loss: 0.00001026
Iteration 62/1000 | Loss: 0.00001025
Iteration 63/1000 | Loss: 0.00001025
Iteration 64/1000 | Loss: 0.00001024
Iteration 65/1000 | Loss: 0.00001024
Iteration 66/1000 | Loss: 0.00001024
Iteration 67/1000 | Loss: 0.00001023
Iteration 68/1000 | Loss: 0.00001023
Iteration 69/1000 | Loss: 0.00001023
Iteration 70/1000 | Loss: 0.00001023
Iteration 71/1000 | Loss: 0.00001023
Iteration 72/1000 | Loss: 0.00001023
Iteration 73/1000 | Loss: 0.00001023
Iteration 74/1000 | Loss: 0.00001023
Iteration 75/1000 | Loss: 0.00001023
Iteration 76/1000 | Loss: 0.00001023
Iteration 77/1000 | Loss: 0.00001023
Iteration 78/1000 | Loss: 0.00001022
Iteration 79/1000 | Loss: 0.00001022
Iteration 80/1000 | Loss: 0.00001022
Iteration 81/1000 | Loss: 0.00001022
Iteration 82/1000 | Loss: 0.00001022
Iteration 83/1000 | Loss: 0.00001021
Iteration 84/1000 | Loss: 0.00001021
Iteration 85/1000 | Loss: 0.00001021
Iteration 86/1000 | Loss: 0.00001020
Iteration 87/1000 | Loss: 0.00001020
Iteration 88/1000 | Loss: 0.00001020
Iteration 89/1000 | Loss: 0.00001020
Iteration 90/1000 | Loss: 0.00001020
Iteration 91/1000 | Loss: 0.00001019
Iteration 92/1000 | Loss: 0.00001019
Iteration 93/1000 | Loss: 0.00001019
Iteration 94/1000 | Loss: 0.00001019
Iteration 95/1000 | Loss: 0.00001019
Iteration 96/1000 | Loss: 0.00001019
Iteration 97/1000 | Loss: 0.00001018
Iteration 98/1000 | Loss: 0.00001018
Iteration 99/1000 | Loss: 0.00001018
Iteration 100/1000 | Loss: 0.00001017
Iteration 101/1000 | Loss: 0.00001017
Iteration 102/1000 | Loss: 0.00001017
Iteration 103/1000 | Loss: 0.00001017
Iteration 104/1000 | Loss: 0.00001017
Iteration 105/1000 | Loss: 0.00001017
Iteration 106/1000 | Loss: 0.00001016
Iteration 107/1000 | Loss: 0.00001016
Iteration 108/1000 | Loss: 0.00001016
Iteration 109/1000 | Loss: 0.00001016
Iteration 110/1000 | Loss: 0.00001016
Iteration 111/1000 | Loss: 0.00001016
Iteration 112/1000 | Loss: 0.00001016
Iteration 113/1000 | Loss: 0.00001016
Iteration 114/1000 | Loss: 0.00001016
Iteration 115/1000 | Loss: 0.00001016
Iteration 116/1000 | Loss: 0.00001016
Iteration 117/1000 | Loss: 0.00001016
Iteration 118/1000 | Loss: 0.00001015
Iteration 119/1000 | Loss: 0.00001015
Iteration 120/1000 | Loss: 0.00001015
Iteration 121/1000 | Loss: 0.00001015
Iteration 122/1000 | Loss: 0.00001014
Iteration 123/1000 | Loss: 0.00001014
Iteration 124/1000 | Loss: 0.00001014
Iteration 125/1000 | Loss: 0.00001014
Iteration 126/1000 | Loss: 0.00001014
Iteration 127/1000 | Loss: 0.00001013
Iteration 128/1000 | Loss: 0.00001013
Iteration 129/1000 | Loss: 0.00001013
Iteration 130/1000 | Loss: 0.00001013
Iteration 131/1000 | Loss: 0.00001013
Iteration 132/1000 | Loss: 0.00001013
Iteration 133/1000 | Loss: 0.00001013
Iteration 134/1000 | Loss: 0.00001013
Iteration 135/1000 | Loss: 0.00001013
Iteration 136/1000 | Loss: 0.00001013
Iteration 137/1000 | Loss: 0.00001012
Iteration 138/1000 | Loss: 0.00001012
Iteration 139/1000 | Loss: 0.00001012
Iteration 140/1000 | Loss: 0.00001012
Iteration 141/1000 | Loss: 0.00001012
Iteration 142/1000 | Loss: 0.00001012
Iteration 143/1000 | Loss: 0.00001012
Iteration 144/1000 | Loss: 0.00001012
Iteration 145/1000 | Loss: 0.00001012
Iteration 146/1000 | Loss: 0.00001012
Iteration 147/1000 | Loss: 0.00001011
Iteration 148/1000 | Loss: 0.00001011
Iteration 149/1000 | Loss: 0.00001011
Iteration 150/1000 | Loss: 0.00001011
Iteration 151/1000 | Loss: 0.00001011
Iteration 152/1000 | Loss: 0.00001010
Iteration 153/1000 | Loss: 0.00001010
Iteration 154/1000 | Loss: 0.00001010
Iteration 155/1000 | Loss: 0.00001010
Iteration 156/1000 | Loss: 0.00001009
Iteration 157/1000 | Loss: 0.00001009
Iteration 158/1000 | Loss: 0.00001009
Iteration 159/1000 | Loss: 0.00001009
Iteration 160/1000 | Loss: 0.00001009
Iteration 161/1000 | Loss: 0.00001009
Iteration 162/1000 | Loss: 0.00001009
Iteration 163/1000 | Loss: 0.00001009
Iteration 164/1000 | Loss: 0.00001008
Iteration 165/1000 | Loss: 0.00001008
Iteration 166/1000 | Loss: 0.00001008
Iteration 167/1000 | Loss: 0.00001008
Iteration 168/1000 | Loss: 0.00001008
Iteration 169/1000 | Loss: 0.00001007
Iteration 170/1000 | Loss: 0.00001007
Iteration 171/1000 | Loss: 0.00001007
Iteration 172/1000 | Loss: 0.00001007
Iteration 173/1000 | Loss: 0.00001007
Iteration 174/1000 | Loss: 0.00001007
Iteration 175/1000 | Loss: 0.00001006
Iteration 176/1000 | Loss: 0.00001005
Iteration 177/1000 | Loss: 0.00001005
Iteration 178/1000 | Loss: 0.00001005
Iteration 179/1000 | Loss: 0.00001005
Iteration 180/1000 | Loss: 0.00001004
Iteration 181/1000 | Loss: 0.00001004
Iteration 182/1000 | Loss: 0.00001004
Iteration 183/1000 | Loss: 0.00001004
Iteration 184/1000 | Loss: 0.00001004
Iteration 185/1000 | Loss: 0.00001004
Iteration 186/1000 | Loss: 0.00001004
Iteration 187/1000 | Loss: 0.00001004
Iteration 188/1000 | Loss: 0.00001004
Iteration 189/1000 | Loss: 0.00001004
Iteration 190/1000 | Loss: 0.00001004
Iteration 191/1000 | Loss: 0.00001004
Iteration 192/1000 | Loss: 0.00001004
Iteration 193/1000 | Loss: 0.00001004
Iteration 194/1000 | Loss: 0.00001004
Iteration 195/1000 | Loss: 0.00001004
Iteration 196/1000 | Loss: 0.00001004
Iteration 197/1000 | Loss: 0.00001004
Iteration 198/1000 | Loss: 0.00001004
Iteration 199/1000 | Loss: 0.00001004
Iteration 200/1000 | Loss: 0.00001004
Iteration 201/1000 | Loss: 0.00001004
Iteration 202/1000 | Loss: 0.00001004
Iteration 203/1000 | Loss: 0.00001003
Iteration 204/1000 | Loss: 0.00001003
Iteration 205/1000 | Loss: 0.00001003
Iteration 206/1000 | Loss: 0.00001003
Iteration 207/1000 | Loss: 0.00001003
Iteration 208/1000 | Loss: 0.00001003
Iteration 209/1000 | Loss: 0.00001003
Iteration 210/1000 | Loss: 0.00001003
Iteration 211/1000 | Loss: 0.00001003
Iteration 212/1000 | Loss: 0.00001003
Iteration 213/1000 | Loss: 0.00001003
Iteration 214/1000 | Loss: 0.00001003
Iteration 215/1000 | Loss: 0.00001003
Iteration 216/1000 | Loss: 0.00001003
Iteration 217/1000 | Loss: 0.00001003
Iteration 218/1000 | Loss: 0.00001003
Iteration 219/1000 | Loss: 0.00001003
Iteration 220/1000 | Loss: 0.00001002
Iteration 221/1000 | Loss: 0.00001002
Iteration 222/1000 | Loss: 0.00001002
Iteration 223/1000 | Loss: 0.00001002
Iteration 224/1000 | Loss: 0.00001002
Iteration 225/1000 | Loss: 0.00001002
Iteration 226/1000 | Loss: 0.00001002
Iteration 227/1000 | Loss: 0.00001002
Iteration 228/1000 | Loss: 0.00001002
Iteration 229/1000 | Loss: 0.00001002
Iteration 230/1000 | Loss: 0.00001002
Iteration 231/1000 | Loss: 0.00001002
Iteration 232/1000 | Loss: 0.00001002
Iteration 233/1000 | Loss: 0.00001002
Iteration 234/1000 | Loss: 0.00001002
Iteration 235/1000 | Loss: 0.00001002
Iteration 236/1000 | Loss: 0.00001002
Iteration 237/1000 | Loss: 0.00001002
Iteration 238/1000 | Loss: 0.00001002
Iteration 239/1000 | Loss: 0.00001002
Iteration 240/1000 | Loss: 0.00001002
Iteration 241/1000 | Loss: 0.00001002
Iteration 242/1000 | Loss: 0.00001002
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 242. Stopping optimization.
Last 5 losses: [1.0022040441981517e-05, 1.0022040441981517e-05, 1.0022040441981517e-05, 1.0022040441981517e-05, 1.0022040441981517e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0022040441981517e-05

Optimization complete. Final v2v error: 2.7268588542938232 mm

Highest mean error: 3.1618733406066895 mm for frame 133

Lowest mean error: 2.639855146408081 mm for frame 211

Saving results

Total time: 45.15386986732483
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01021429
Iteration 2/25 | Loss: 0.00232812
Iteration 3/25 | Loss: 0.00191510
Iteration 4/25 | Loss: 0.00208731
Iteration 5/25 | Loss: 0.00196906
Iteration 6/25 | Loss: 0.00162787
Iteration 7/25 | Loss: 0.00149951
Iteration 8/25 | Loss: 0.00144662
Iteration 9/25 | Loss: 0.00142592
Iteration 10/25 | Loss: 0.00142199
Iteration 11/25 | Loss: 0.00141423
Iteration 12/25 | Loss: 0.00141167
Iteration 13/25 | Loss: 0.00140890
Iteration 14/25 | Loss: 0.00140665
Iteration 15/25 | Loss: 0.00140121
Iteration 16/25 | Loss: 0.00139320
Iteration 17/25 | Loss: 0.00138880
Iteration 18/25 | Loss: 0.00138789
Iteration 19/25 | Loss: 0.00138760
Iteration 20/25 | Loss: 0.00138750
Iteration 21/25 | Loss: 0.00138750
Iteration 22/25 | Loss: 0.00138750
Iteration 23/25 | Loss: 0.00138749
Iteration 24/25 | Loss: 0.00138749
Iteration 25/25 | Loss: 0.00138749

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.94395453
Iteration 2/25 | Loss: 0.00109091
Iteration 3/25 | Loss: 0.00109089
Iteration 4/25 | Loss: 0.00109089
Iteration 5/25 | Loss: 0.00109089
Iteration 6/25 | Loss: 0.00109088
Iteration 7/25 | Loss: 0.00109088
Iteration 8/25 | Loss: 0.00109088
Iteration 9/25 | Loss: 0.00109088
Iteration 10/25 | Loss: 0.00109088
Iteration 11/25 | Loss: 0.00109088
Iteration 12/25 | Loss: 0.00109088
Iteration 13/25 | Loss: 0.00109088
Iteration 14/25 | Loss: 0.00109088
Iteration 15/25 | Loss: 0.00109088
Iteration 16/25 | Loss: 0.00109088
Iteration 17/25 | Loss: 0.00109088
Iteration 18/25 | Loss: 0.00109088
Iteration 19/25 | Loss: 0.00109088
Iteration 20/25 | Loss: 0.00109088
Iteration 21/25 | Loss: 0.00109088
Iteration 22/25 | Loss: 0.00109088
Iteration 23/25 | Loss: 0.00109088
Iteration 24/25 | Loss: 0.00109088
Iteration 25/25 | Loss: 0.00109088

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109088
Iteration 2/1000 | Loss: 0.00007081
Iteration 3/1000 | Loss: 0.00004588
Iteration 4/1000 | Loss: 0.00004037
Iteration 5/1000 | Loss: 0.00003792
Iteration 6/1000 | Loss: 0.00003633
Iteration 7/1000 | Loss: 0.00104929
Iteration 8/1000 | Loss: 0.00073085
Iteration 9/1000 | Loss: 0.00005109
Iteration 10/1000 | Loss: 0.00003796
Iteration 11/1000 | Loss: 0.00003571
Iteration 12/1000 | Loss: 0.00003435
Iteration 13/1000 | Loss: 0.00031002
Iteration 14/1000 | Loss: 0.00055438
Iteration 15/1000 | Loss: 0.00155307
Iteration 16/1000 | Loss: 0.00013262
Iteration 17/1000 | Loss: 0.00022948
Iteration 18/1000 | Loss: 0.00049517
Iteration 19/1000 | Loss: 0.00027813
Iteration 20/1000 | Loss: 0.00004254
Iteration 21/1000 | Loss: 0.00003637
Iteration 22/1000 | Loss: 0.00066492
Iteration 23/1000 | Loss: 0.00098526
Iteration 24/1000 | Loss: 0.00106470
Iteration 25/1000 | Loss: 0.00051132
Iteration 26/1000 | Loss: 0.00043063
Iteration 27/1000 | Loss: 0.00028597
Iteration 28/1000 | Loss: 0.00004060
Iteration 29/1000 | Loss: 0.00003471
Iteration 30/1000 | Loss: 0.00003245
Iteration 31/1000 | Loss: 0.00003100
Iteration 32/1000 | Loss: 0.00026665
Iteration 33/1000 | Loss: 0.00030990
Iteration 34/1000 | Loss: 0.00004424
Iteration 35/1000 | Loss: 0.00003494
Iteration 36/1000 | Loss: 0.00003088
Iteration 37/1000 | Loss: 0.00031316
Iteration 38/1000 | Loss: 0.00013208
Iteration 39/1000 | Loss: 0.00003037
Iteration 40/1000 | Loss: 0.00002969
Iteration 41/1000 | Loss: 0.00002943
Iteration 42/1000 | Loss: 0.00020410
Iteration 43/1000 | Loss: 0.00003412
Iteration 44/1000 | Loss: 0.00002959
Iteration 45/1000 | Loss: 0.00013054
Iteration 46/1000 | Loss: 0.00003317
Iteration 47/1000 | Loss: 0.00009620
Iteration 48/1000 | Loss: 0.00003449
Iteration 49/1000 | Loss: 0.00004897
Iteration 50/1000 | Loss: 0.00003851
Iteration 51/1000 | Loss: 0.00003941
Iteration 52/1000 | Loss: 0.00002921
Iteration 53/1000 | Loss: 0.00002832
Iteration 54/1000 | Loss: 0.00002760
Iteration 55/1000 | Loss: 0.00002725
Iteration 56/1000 | Loss: 0.00002698
Iteration 57/1000 | Loss: 0.00002676
Iteration 58/1000 | Loss: 0.00002668
Iteration 59/1000 | Loss: 0.00002667
Iteration 60/1000 | Loss: 0.00002665
Iteration 61/1000 | Loss: 0.00002665
Iteration 62/1000 | Loss: 0.00002665
Iteration 63/1000 | Loss: 0.00002664
Iteration 64/1000 | Loss: 0.00002664
Iteration 65/1000 | Loss: 0.00002663
Iteration 66/1000 | Loss: 0.00002662
Iteration 67/1000 | Loss: 0.00002662
Iteration 68/1000 | Loss: 0.00002661
Iteration 69/1000 | Loss: 0.00002659
Iteration 70/1000 | Loss: 0.00002658
Iteration 71/1000 | Loss: 0.00002658
Iteration 72/1000 | Loss: 0.00002658
Iteration 73/1000 | Loss: 0.00002657
Iteration 74/1000 | Loss: 0.00002657
Iteration 75/1000 | Loss: 0.00002657
Iteration 76/1000 | Loss: 0.00002657
Iteration 77/1000 | Loss: 0.00002657
Iteration 78/1000 | Loss: 0.00002657
Iteration 79/1000 | Loss: 0.00002657
Iteration 80/1000 | Loss: 0.00002657
Iteration 81/1000 | Loss: 0.00002656
Iteration 82/1000 | Loss: 0.00002656
Iteration 83/1000 | Loss: 0.00002655
Iteration 84/1000 | Loss: 0.00002654
Iteration 85/1000 | Loss: 0.00002653
Iteration 86/1000 | Loss: 0.00002648
Iteration 87/1000 | Loss: 0.00002647
Iteration 88/1000 | Loss: 0.00002645
Iteration 89/1000 | Loss: 0.00002645
Iteration 90/1000 | Loss: 0.00002644
Iteration 91/1000 | Loss: 0.00002644
Iteration 92/1000 | Loss: 0.00002644
Iteration 93/1000 | Loss: 0.00002644
Iteration 94/1000 | Loss: 0.00002644
Iteration 95/1000 | Loss: 0.00002644
Iteration 96/1000 | Loss: 0.00002644
Iteration 97/1000 | Loss: 0.00002637
Iteration 98/1000 | Loss: 0.00002636
Iteration 99/1000 | Loss: 0.00002636
Iteration 100/1000 | Loss: 0.00002636
Iteration 101/1000 | Loss: 0.00002636
Iteration 102/1000 | Loss: 0.00002636
Iteration 103/1000 | Loss: 0.00002636
Iteration 104/1000 | Loss: 0.00002635
Iteration 105/1000 | Loss: 0.00002635
Iteration 106/1000 | Loss: 0.00002635
Iteration 107/1000 | Loss: 0.00002635
Iteration 108/1000 | Loss: 0.00002635
Iteration 109/1000 | Loss: 0.00002635
Iteration 110/1000 | Loss: 0.00002635
Iteration 111/1000 | Loss: 0.00002634
Iteration 112/1000 | Loss: 0.00002634
Iteration 113/1000 | Loss: 0.00002634
Iteration 114/1000 | Loss: 0.00002634
Iteration 115/1000 | Loss: 0.00002634
Iteration 116/1000 | Loss: 0.00002634
Iteration 117/1000 | Loss: 0.00002633
Iteration 118/1000 | Loss: 0.00002633
Iteration 119/1000 | Loss: 0.00002633
Iteration 120/1000 | Loss: 0.00002633
Iteration 121/1000 | Loss: 0.00002633
Iteration 122/1000 | Loss: 0.00002633
Iteration 123/1000 | Loss: 0.00002632
Iteration 124/1000 | Loss: 0.00002632
Iteration 125/1000 | Loss: 0.00002632
Iteration 126/1000 | Loss: 0.00002632
Iteration 127/1000 | Loss: 0.00002632
Iteration 128/1000 | Loss: 0.00002632
Iteration 129/1000 | Loss: 0.00002631
Iteration 130/1000 | Loss: 0.00002631
Iteration 131/1000 | Loss: 0.00002631
Iteration 132/1000 | Loss: 0.00002631
Iteration 133/1000 | Loss: 0.00002631
Iteration 134/1000 | Loss: 0.00002631
Iteration 135/1000 | Loss: 0.00002631
Iteration 136/1000 | Loss: 0.00002630
Iteration 137/1000 | Loss: 0.00002630
Iteration 138/1000 | Loss: 0.00002630
Iteration 139/1000 | Loss: 0.00002630
Iteration 140/1000 | Loss: 0.00002630
Iteration 141/1000 | Loss: 0.00002630
Iteration 142/1000 | Loss: 0.00002630
Iteration 143/1000 | Loss: 0.00002630
Iteration 144/1000 | Loss: 0.00002630
Iteration 145/1000 | Loss: 0.00002630
Iteration 146/1000 | Loss: 0.00002630
Iteration 147/1000 | Loss: 0.00002630
Iteration 148/1000 | Loss: 0.00002630
Iteration 149/1000 | Loss: 0.00002630
Iteration 150/1000 | Loss: 0.00002630
Iteration 151/1000 | Loss: 0.00002630
Iteration 152/1000 | Loss: 0.00002630
Iteration 153/1000 | Loss: 0.00002630
Iteration 154/1000 | Loss: 0.00002630
Iteration 155/1000 | Loss: 0.00002630
Iteration 156/1000 | Loss: 0.00002630
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [2.6300669560441747e-05, 2.6300669560441747e-05, 2.6300669560441747e-05, 2.6300669560441747e-05, 2.6300669560441747e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6300669560441747e-05

Optimization complete. Final v2v error: 4.278413772583008 mm

Highest mean error: 5.3849263191223145 mm for frame 125

Lowest mean error: 3.6698105335235596 mm for frame 29

Saving results

Total time: 135.00913071632385
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00811300
Iteration 2/25 | Loss: 0.00158912
Iteration 3/25 | Loss: 0.00132492
Iteration 4/25 | Loss: 0.00127816
Iteration 5/25 | Loss: 0.00126608
Iteration 6/25 | Loss: 0.00126225
Iteration 7/25 | Loss: 0.00125823
Iteration 8/25 | Loss: 0.00126433
Iteration 9/25 | Loss: 0.00126786
Iteration 10/25 | Loss: 0.00127009
Iteration 11/25 | Loss: 0.00127660
Iteration 12/25 | Loss: 0.00127265
Iteration 13/25 | Loss: 0.00126843
Iteration 14/25 | Loss: 0.00127009
Iteration 15/25 | Loss: 0.00127123
Iteration 16/25 | Loss: 0.00127041
Iteration 17/25 | Loss: 0.00127353
Iteration 18/25 | Loss: 0.00126858
Iteration 19/25 | Loss: 0.00126989
Iteration 20/25 | Loss: 0.00126845
Iteration 21/25 | Loss: 0.00127149
Iteration 22/25 | Loss: 0.00126663
Iteration 23/25 | Loss: 0.00126926
Iteration 24/25 | Loss: 0.00126693
Iteration 25/25 | Loss: 0.00126512

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44702566
Iteration 2/25 | Loss: 0.00088641
Iteration 3/25 | Loss: 0.00088641
Iteration 4/25 | Loss: 0.00088641
Iteration 5/25 | Loss: 0.00088641
Iteration 6/25 | Loss: 0.00088641
Iteration 7/25 | Loss: 0.00088641
Iteration 8/25 | Loss: 0.00088641
Iteration 9/25 | Loss: 0.00088641
Iteration 10/25 | Loss: 0.00088641
Iteration 11/25 | Loss: 0.00088641
Iteration 12/25 | Loss: 0.00088641
Iteration 13/25 | Loss: 0.00088641
Iteration 14/25 | Loss: 0.00088641
Iteration 15/25 | Loss: 0.00088641
Iteration 16/25 | Loss: 0.00088641
Iteration 17/25 | Loss: 0.00088641
Iteration 18/25 | Loss: 0.00088641
Iteration 19/25 | Loss: 0.00088641
Iteration 20/25 | Loss: 0.00088641
Iteration 21/25 | Loss: 0.00088641
Iteration 22/25 | Loss: 0.00088641
Iteration 23/25 | Loss: 0.00088641
Iteration 24/25 | Loss: 0.00088641
Iteration 25/25 | Loss: 0.00088641

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088641
Iteration 2/1000 | Loss: 0.00009967
Iteration 3/1000 | Loss: 0.00018856
Iteration 4/1000 | Loss: 0.00016437
Iteration 5/1000 | Loss: 0.00011910
Iteration 6/1000 | Loss: 0.00014732
Iteration 7/1000 | Loss: 0.00016168
Iteration 8/1000 | Loss: 0.00011779
Iteration 9/1000 | Loss: 0.00018653
Iteration 10/1000 | Loss: 0.00012969
Iteration 11/1000 | Loss: 0.00013471
Iteration 12/1000 | Loss: 0.00012302
Iteration 13/1000 | Loss: 0.00016421
Iteration 14/1000 | Loss: 0.00016216
Iteration 15/1000 | Loss: 0.00018739
Iteration 16/1000 | Loss: 0.00018085
Iteration 17/1000 | Loss: 0.00017203
Iteration 18/1000 | Loss: 0.00017631
Iteration 19/1000 | Loss: 0.00015230
Iteration 20/1000 | Loss: 0.00019288
Iteration 21/1000 | Loss: 0.00018806
Iteration 22/1000 | Loss: 0.00022950
Iteration 23/1000 | Loss: 0.00018254
Iteration 24/1000 | Loss: 0.00016933
Iteration 25/1000 | Loss: 0.00020654
Iteration 26/1000 | Loss: 0.00020585
Iteration 27/1000 | Loss: 0.00020383
Iteration 28/1000 | Loss: 0.00027083
Iteration 29/1000 | Loss: 0.00018972
Iteration 30/1000 | Loss: 0.00026633
Iteration 31/1000 | Loss: 0.00026879
Iteration 32/1000 | Loss: 0.00020930
Iteration 33/1000 | Loss: 0.00017656
Iteration 34/1000 | Loss: 0.00016762
Iteration 35/1000 | Loss: 0.00015351
Iteration 36/1000 | Loss: 0.00015462
Iteration 37/1000 | Loss: 0.00018718
Iteration 38/1000 | Loss: 0.00014366
Iteration 39/1000 | Loss: 0.00018341
Iteration 40/1000 | Loss: 0.00022188
Iteration 41/1000 | Loss: 0.00020406
Iteration 42/1000 | Loss: 0.00013755
Iteration 43/1000 | Loss: 0.00016348
Iteration 44/1000 | Loss: 0.00019479
Iteration 45/1000 | Loss: 0.00016559
Iteration 46/1000 | Loss: 0.00007264
Iteration 47/1000 | Loss: 0.00011556
Iteration 48/1000 | Loss: 0.00029136
Iteration 49/1000 | Loss: 0.00014648
Iteration 50/1000 | Loss: 0.00012049
Iteration 51/1000 | Loss: 0.00010766
Iteration 52/1000 | Loss: 0.00011702
Iteration 53/1000 | Loss: 0.00008207
Iteration 54/1000 | Loss: 0.00008473
Iteration 55/1000 | Loss: 0.00008800
Iteration 56/1000 | Loss: 0.00012880
Iteration 57/1000 | Loss: 0.00008019
Iteration 58/1000 | Loss: 0.00007024
Iteration 59/1000 | Loss: 0.00008396
Iteration 60/1000 | Loss: 0.00010939
Iteration 61/1000 | Loss: 0.00010197
Iteration 62/1000 | Loss: 0.00011767
Iteration 63/1000 | Loss: 0.00008683
Iteration 64/1000 | Loss: 0.00010918
Iteration 65/1000 | Loss: 0.00010243
Iteration 66/1000 | Loss: 0.00007738
Iteration 67/1000 | Loss: 0.00008720
Iteration 68/1000 | Loss: 0.00006973
Iteration 69/1000 | Loss: 0.00012628
Iteration 70/1000 | Loss: 0.00009758
Iteration 71/1000 | Loss: 0.00013239
Iteration 72/1000 | Loss: 0.00004043
Iteration 73/1000 | Loss: 0.00007461
Iteration 74/1000 | Loss: 0.00010008
Iteration 75/1000 | Loss: 0.00007492
Iteration 76/1000 | Loss: 0.00007638
Iteration 77/1000 | Loss: 0.00011202
Iteration 78/1000 | Loss: 0.00010974
Iteration 79/1000 | Loss: 0.00009019
Iteration 80/1000 | Loss: 0.00010817
Iteration 81/1000 | Loss: 0.00011709
Iteration 82/1000 | Loss: 0.00008796
Iteration 83/1000 | Loss: 0.00010247
Iteration 84/1000 | Loss: 0.00007754
Iteration 85/1000 | Loss: 0.00007581
Iteration 86/1000 | Loss: 0.00008121
Iteration 87/1000 | Loss: 0.00007833
Iteration 88/1000 | Loss: 0.00008466
Iteration 89/1000 | Loss: 0.00010122
Iteration 90/1000 | Loss: 0.00008482
Iteration 91/1000 | Loss: 0.00008966
Iteration 92/1000 | Loss: 0.00007152
Iteration 93/1000 | Loss: 0.00009800
Iteration 94/1000 | Loss: 0.00008202
Iteration 95/1000 | Loss: 0.00008009
Iteration 96/1000 | Loss: 0.00008373
Iteration 97/1000 | Loss: 0.00010947
Iteration 98/1000 | Loss: 0.00009616
Iteration 99/1000 | Loss: 0.00008299
Iteration 100/1000 | Loss: 0.00010565
Iteration 101/1000 | Loss: 0.00009907
Iteration 102/1000 | Loss: 0.00010569
Iteration 103/1000 | Loss: 0.00010113
Iteration 104/1000 | Loss: 0.00010385
Iteration 105/1000 | Loss: 0.00007105
Iteration 106/1000 | Loss: 0.00007812
Iteration 107/1000 | Loss: 0.00008423
Iteration 108/1000 | Loss: 0.00008217
Iteration 109/1000 | Loss: 0.00006074
Iteration 110/1000 | Loss: 0.00004715
Iteration 111/1000 | Loss: 0.00006472
Iteration 112/1000 | Loss: 0.00008888
Iteration 113/1000 | Loss: 0.00008172
Iteration 114/1000 | Loss: 0.00008354
Iteration 115/1000 | Loss: 0.00007985
Iteration 116/1000 | Loss: 0.00008406
Iteration 117/1000 | Loss: 0.00004547
Iteration 118/1000 | Loss: 0.00006773
Iteration 119/1000 | Loss: 0.00004770
Iteration 120/1000 | Loss: 0.00004058
Iteration 121/1000 | Loss: 0.00006688
Iteration 122/1000 | Loss: 0.00005729
Iteration 123/1000 | Loss: 0.00007355
Iteration 124/1000 | Loss: 0.00006115
Iteration 125/1000 | Loss: 0.00007614
Iteration 126/1000 | Loss: 0.00006814
Iteration 127/1000 | Loss: 0.00007897
Iteration 128/1000 | Loss: 0.00008509
Iteration 129/1000 | Loss: 0.00008491
Iteration 130/1000 | Loss: 0.00008597
Iteration 131/1000 | Loss: 0.00008793
Iteration 132/1000 | Loss: 0.00008125
Iteration 133/1000 | Loss: 0.00005716
Iteration 134/1000 | Loss: 0.00003701
Iteration 135/1000 | Loss: 0.00004360
Iteration 136/1000 | Loss: 0.00004774
Iteration 137/1000 | Loss: 0.00004307
Iteration 138/1000 | Loss: 0.00004560
Iteration 139/1000 | Loss: 0.00004509
Iteration 140/1000 | Loss: 0.00005127
Iteration 141/1000 | Loss: 0.00006027
Iteration 142/1000 | Loss: 0.00004025
Iteration 143/1000 | Loss: 0.00005133
Iteration 144/1000 | Loss: 0.00006827
Iteration 145/1000 | Loss: 0.00006967
Iteration 146/1000 | Loss: 0.00004013
Iteration 147/1000 | Loss: 0.00005194
Iteration 148/1000 | Loss: 0.00005308
Iteration 149/1000 | Loss: 0.00005154
Iteration 150/1000 | Loss: 0.00005404
Iteration 151/1000 | Loss: 0.00004158
Iteration 152/1000 | Loss: 0.00005329
Iteration 153/1000 | Loss: 0.00005147
Iteration 154/1000 | Loss: 0.00005272
Iteration 155/1000 | Loss: 0.00005428
Iteration 156/1000 | Loss: 0.00005546
Iteration 157/1000 | Loss: 0.00005312
Iteration 158/1000 | Loss: 0.00005630
Iteration 159/1000 | Loss: 0.00005266
Iteration 160/1000 | Loss: 0.00005311
Iteration 161/1000 | Loss: 0.00005867
Iteration 162/1000 | Loss: 0.00004808
Iteration 163/1000 | Loss: 0.00006731
Iteration 164/1000 | Loss: 0.00005010
Iteration 165/1000 | Loss: 0.00006282
Iteration 166/1000 | Loss: 0.00004349
Iteration 167/1000 | Loss: 0.00003464
Iteration 168/1000 | Loss: 0.00005711
Iteration 169/1000 | Loss: 0.00003963
Iteration 170/1000 | Loss: 0.00004893
Iteration 171/1000 | Loss: 0.00003914
Iteration 172/1000 | Loss: 0.00004043
Iteration 173/1000 | Loss: 0.00004261
Iteration 174/1000 | Loss: 0.00004552
Iteration 175/1000 | Loss: 0.00005077
Iteration 176/1000 | Loss: 0.00004766
Iteration 177/1000 | Loss: 0.00005209
Iteration 178/1000 | Loss: 0.00005507
Iteration 179/1000 | Loss: 0.00004783
Iteration 180/1000 | Loss: 0.00006014
Iteration 181/1000 | Loss: 0.00005788
Iteration 182/1000 | Loss: 0.00006281
Iteration 183/1000 | Loss: 0.00005164
Iteration 184/1000 | Loss: 0.00004434
Iteration 185/1000 | Loss: 0.00003750
Iteration 186/1000 | Loss: 0.00003920
Iteration 187/1000 | Loss: 0.00002558
Iteration 188/1000 | Loss: 0.00003334
Iteration 189/1000 | Loss: 0.00003294
Iteration 190/1000 | Loss: 0.00003413
Iteration 191/1000 | Loss: 0.00003482
Iteration 192/1000 | Loss: 0.00003443
Iteration 193/1000 | Loss: 0.00004332
Iteration 194/1000 | Loss: 0.00003339
Iteration 195/1000 | Loss: 0.00003609
Iteration 196/1000 | Loss: 0.00004636
Iteration 197/1000 | Loss: 0.00004381
Iteration 198/1000 | Loss: 0.00002088
Iteration 199/1000 | Loss: 0.00004504
Iteration 200/1000 | Loss: 0.00002101
Iteration 201/1000 | Loss: 0.00002753
Iteration 202/1000 | Loss: 0.00002296
Iteration 203/1000 | Loss: 0.00004174
Iteration 204/1000 | Loss: 0.00003046
Iteration 205/1000 | Loss: 0.00002471
Iteration 206/1000 | Loss: 0.00001917
Iteration 207/1000 | Loss: 0.00003809
Iteration 208/1000 | Loss: 0.00004359
Iteration 209/1000 | Loss: 0.00002650
Iteration 210/1000 | Loss: 0.00003987
Iteration 211/1000 | Loss: 0.00003356
Iteration 212/1000 | Loss: 0.00003906
Iteration 213/1000 | Loss: 0.00003104
Iteration 214/1000 | Loss: 0.00005182
Iteration 215/1000 | Loss: 0.00002801
Iteration 216/1000 | Loss: 0.00003582
Iteration 217/1000 | Loss: 0.00002061
Iteration 218/1000 | Loss: 0.00003206
Iteration 219/1000 | Loss: 0.00002184
Iteration 220/1000 | Loss: 0.00004106
Iteration 221/1000 | Loss: 0.00002804
Iteration 222/1000 | Loss: 0.00003621
Iteration 223/1000 | Loss: 0.00003078
Iteration 224/1000 | Loss: 0.00005621
Iteration 225/1000 | Loss: 0.00003677
Iteration 226/1000 | Loss: 0.00004929
Iteration 227/1000 | Loss: 0.00003372
Iteration 228/1000 | Loss: 0.00004679
Iteration 229/1000 | Loss: 0.00002774
Iteration 230/1000 | Loss: 0.00004117
Iteration 231/1000 | Loss: 0.00003551
Iteration 232/1000 | Loss: 0.00003725
Iteration 233/1000 | Loss: 0.00003927
Iteration 234/1000 | Loss: 0.00004168
Iteration 235/1000 | Loss: 0.00003904
Iteration 236/1000 | Loss: 0.00005045
Iteration 237/1000 | Loss: 0.00002427
Iteration 238/1000 | Loss: 0.00002135
Iteration 239/1000 | Loss: 0.00004592
Iteration 240/1000 | Loss: 0.00002035
Iteration 241/1000 | Loss: 0.00001712
Iteration 242/1000 | Loss: 0.00001551
Iteration 243/1000 | Loss: 0.00001455
Iteration 244/1000 | Loss: 0.00001415
Iteration 245/1000 | Loss: 0.00001413
Iteration 246/1000 | Loss: 0.00001401
Iteration 247/1000 | Loss: 0.00001394
Iteration 248/1000 | Loss: 0.00001390
Iteration 249/1000 | Loss: 0.00001383
Iteration 250/1000 | Loss: 0.00001381
Iteration 251/1000 | Loss: 0.00001378
Iteration 252/1000 | Loss: 0.00001378
Iteration 253/1000 | Loss: 0.00001376
Iteration 254/1000 | Loss: 0.00001375
Iteration 255/1000 | Loss: 0.00001374
Iteration 256/1000 | Loss: 0.00001372
Iteration 257/1000 | Loss: 0.00001372
Iteration 258/1000 | Loss: 0.00001371
Iteration 259/1000 | Loss: 0.00001371
Iteration 260/1000 | Loss: 0.00001356
Iteration 261/1000 | Loss: 0.00001353
Iteration 262/1000 | Loss: 0.00001345
Iteration 263/1000 | Loss: 0.00001341
Iteration 264/1000 | Loss: 0.00001340
Iteration 265/1000 | Loss: 0.00001339
Iteration 266/1000 | Loss: 0.00001338
Iteration 267/1000 | Loss: 0.00001338
Iteration 268/1000 | Loss: 0.00001337
Iteration 269/1000 | Loss: 0.00001335
Iteration 270/1000 | Loss: 0.00001334
Iteration 271/1000 | Loss: 0.00001334
Iteration 272/1000 | Loss: 0.00001333
Iteration 273/1000 | Loss: 0.00001332
Iteration 274/1000 | Loss: 0.00001332
Iteration 275/1000 | Loss: 0.00001330
Iteration 276/1000 | Loss: 0.00001329
Iteration 277/1000 | Loss: 0.00001329
Iteration 278/1000 | Loss: 0.00001328
Iteration 279/1000 | Loss: 0.00001328
Iteration 280/1000 | Loss: 0.00001327
Iteration 281/1000 | Loss: 0.00001327
Iteration 282/1000 | Loss: 0.00001327
Iteration 283/1000 | Loss: 0.00001327
Iteration 284/1000 | Loss: 0.00001326
Iteration 285/1000 | Loss: 0.00001326
Iteration 286/1000 | Loss: 0.00001326
Iteration 287/1000 | Loss: 0.00001326
Iteration 288/1000 | Loss: 0.00001325
Iteration 289/1000 | Loss: 0.00001325
Iteration 290/1000 | Loss: 0.00001325
Iteration 291/1000 | Loss: 0.00001324
Iteration 292/1000 | Loss: 0.00001324
Iteration 293/1000 | Loss: 0.00001324
Iteration 294/1000 | Loss: 0.00001324
Iteration 295/1000 | Loss: 0.00001324
Iteration 296/1000 | Loss: 0.00001323
Iteration 297/1000 | Loss: 0.00001323
Iteration 298/1000 | Loss: 0.00001321
Iteration 299/1000 | Loss: 0.00001320
Iteration 300/1000 | Loss: 0.00001319
Iteration 301/1000 | Loss: 0.00001319
Iteration 302/1000 | Loss: 0.00001319
Iteration 303/1000 | Loss: 0.00001318
Iteration 304/1000 | Loss: 0.00001318
Iteration 305/1000 | Loss: 0.00001317
Iteration 306/1000 | Loss: 0.00001317
Iteration 307/1000 | Loss: 0.00001317
Iteration 308/1000 | Loss: 0.00001316
Iteration 309/1000 | Loss: 0.00001316
Iteration 310/1000 | Loss: 0.00001316
Iteration 311/1000 | Loss: 0.00001315
Iteration 312/1000 | Loss: 0.00001315
Iteration 313/1000 | Loss: 0.00001314
Iteration 314/1000 | Loss: 0.00001313
Iteration 315/1000 | Loss: 0.00001313
Iteration 316/1000 | Loss: 0.00001311
Iteration 317/1000 | Loss: 0.00001311
Iteration 318/1000 | Loss: 0.00001311
Iteration 319/1000 | Loss: 0.00001311
Iteration 320/1000 | Loss: 0.00001306
Iteration 321/1000 | Loss: 0.00001304
Iteration 322/1000 | Loss: 0.00001304
Iteration 323/1000 | Loss: 0.00001304
Iteration 324/1000 | Loss: 0.00001304
Iteration 325/1000 | Loss: 0.00001304
Iteration 326/1000 | Loss: 0.00001304
Iteration 327/1000 | Loss: 0.00001303
Iteration 328/1000 | Loss: 0.00001303
Iteration 329/1000 | Loss: 0.00001303
Iteration 330/1000 | Loss: 0.00001301
Iteration 331/1000 | Loss: 0.00001301
Iteration 332/1000 | Loss: 0.00001301
Iteration 333/1000 | Loss: 0.00001301
Iteration 334/1000 | Loss: 0.00001301
Iteration 335/1000 | Loss: 0.00001301
Iteration 336/1000 | Loss: 0.00001301
Iteration 337/1000 | Loss: 0.00001301
Iteration 338/1000 | Loss: 0.00001301
Iteration 339/1000 | Loss: 0.00001300
Iteration 340/1000 | Loss: 0.00001300
Iteration 341/1000 | Loss: 0.00001299
Iteration 342/1000 | Loss: 0.00001299
Iteration 343/1000 | Loss: 0.00001298
Iteration 344/1000 | Loss: 0.00001298
Iteration 345/1000 | Loss: 0.00001297
Iteration 346/1000 | Loss: 0.00001297
Iteration 347/1000 | Loss: 0.00001297
Iteration 348/1000 | Loss: 0.00001297
Iteration 349/1000 | Loss: 0.00001297
Iteration 350/1000 | Loss: 0.00001297
Iteration 351/1000 | Loss: 0.00001297
Iteration 352/1000 | Loss: 0.00001296
Iteration 353/1000 | Loss: 0.00001296
Iteration 354/1000 | Loss: 0.00001296
Iteration 355/1000 | Loss: 0.00001295
Iteration 356/1000 | Loss: 0.00001295
Iteration 357/1000 | Loss: 0.00001295
Iteration 358/1000 | Loss: 0.00001295
Iteration 359/1000 | Loss: 0.00001295
Iteration 360/1000 | Loss: 0.00001294
Iteration 361/1000 | Loss: 0.00001294
Iteration 362/1000 | Loss: 0.00001294
Iteration 363/1000 | Loss: 0.00001294
Iteration 364/1000 | Loss: 0.00001294
Iteration 365/1000 | Loss: 0.00001294
Iteration 366/1000 | Loss: 0.00001294
Iteration 367/1000 | Loss: 0.00001293
Iteration 368/1000 | Loss: 0.00001292
Iteration 369/1000 | Loss: 0.00001292
Iteration 370/1000 | Loss: 0.00001292
Iteration 371/1000 | Loss: 0.00001292
Iteration 372/1000 | Loss: 0.00001291
Iteration 373/1000 | Loss: 0.00001291
Iteration 374/1000 | Loss: 0.00001291
Iteration 375/1000 | Loss: 0.00001291
Iteration 376/1000 | Loss: 0.00001291
Iteration 377/1000 | Loss: 0.00001291
Iteration 378/1000 | Loss: 0.00001291
Iteration 379/1000 | Loss: 0.00001291
Iteration 380/1000 | Loss: 0.00001291
Iteration 381/1000 | Loss: 0.00001291
Iteration 382/1000 | Loss: 0.00001291
Iteration 383/1000 | Loss: 0.00001290
Iteration 384/1000 | Loss: 0.00001290
Iteration 385/1000 | Loss: 0.00001290
Iteration 386/1000 | Loss: 0.00001290
Iteration 387/1000 | Loss: 0.00001290
Iteration 388/1000 | Loss: 0.00001289
Iteration 389/1000 | Loss: 0.00001289
Iteration 390/1000 | Loss: 0.00001289
Iteration 391/1000 | Loss: 0.00001289
Iteration 392/1000 | Loss: 0.00001289
Iteration 393/1000 | Loss: 0.00001289
Iteration 394/1000 | Loss: 0.00001289
Iteration 395/1000 | Loss: 0.00001289
Iteration 396/1000 | Loss: 0.00001289
Iteration 397/1000 | Loss: 0.00001289
Iteration 398/1000 | Loss: 0.00001288
Iteration 399/1000 | Loss: 0.00001288
Iteration 400/1000 | Loss: 0.00001288
Iteration 401/1000 | Loss: 0.00001288
Iteration 402/1000 | Loss: 0.00001288
Iteration 403/1000 | Loss: 0.00001288
Iteration 404/1000 | Loss: 0.00001288
Iteration 405/1000 | Loss: 0.00001288
Iteration 406/1000 | Loss: 0.00001288
Iteration 407/1000 | Loss: 0.00001288
Iteration 408/1000 | Loss: 0.00001288
Iteration 409/1000 | Loss: 0.00001288
Iteration 410/1000 | Loss: 0.00001288
Iteration 411/1000 | Loss: 0.00001288
Iteration 412/1000 | Loss: 0.00001288
Iteration 413/1000 | Loss: 0.00001287
Iteration 414/1000 | Loss: 0.00001287
Iteration 415/1000 | Loss: 0.00001287
Iteration 416/1000 | Loss: 0.00001287
Iteration 417/1000 | Loss: 0.00001287
Iteration 418/1000 | Loss: 0.00001287
Iteration 419/1000 | Loss: 0.00001287
Iteration 420/1000 | Loss: 0.00001287
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 420. Stopping optimization.
Last 5 losses: [1.2874249478045385e-05, 1.2874249478045385e-05, 1.2874249478045385e-05, 1.2874249478045385e-05, 1.2874249478045385e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2874249478045385e-05

Optimization complete. Final v2v error: 3.0760793685913086 mm

Highest mean error: 3.9292588233947754 mm for frame 145

Lowest mean error: 2.9218177795410156 mm for frame 2

Saving results

Total time: 463.55410075187683
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00443184
Iteration 2/25 | Loss: 0.00130763
Iteration 3/25 | Loss: 0.00123984
Iteration 4/25 | Loss: 0.00122312
Iteration 5/25 | Loss: 0.00121760
Iteration 6/25 | Loss: 0.00121666
Iteration 7/25 | Loss: 0.00121666
Iteration 8/25 | Loss: 0.00121666
Iteration 9/25 | Loss: 0.00121666
Iteration 10/25 | Loss: 0.00121666
Iteration 11/25 | Loss: 0.00121666
Iteration 12/25 | Loss: 0.00121666
Iteration 13/25 | Loss: 0.00121666
Iteration 14/25 | Loss: 0.00121666
Iteration 15/25 | Loss: 0.00121666
Iteration 16/25 | Loss: 0.00121666
Iteration 17/25 | Loss: 0.00121666
Iteration 18/25 | Loss: 0.00121666
Iteration 19/25 | Loss: 0.00121666
Iteration 20/25 | Loss: 0.00121666
Iteration 21/25 | Loss: 0.00121666
Iteration 22/25 | Loss: 0.00121666
Iteration 23/25 | Loss: 0.00121666
Iteration 24/25 | Loss: 0.00121666
Iteration 25/25 | Loss: 0.00121666

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51462948
Iteration 2/25 | Loss: 0.00076957
Iteration 3/25 | Loss: 0.00076957
Iteration 4/25 | Loss: 0.00076957
Iteration 5/25 | Loss: 0.00076957
Iteration 6/25 | Loss: 0.00076957
Iteration 7/25 | Loss: 0.00076957
Iteration 8/25 | Loss: 0.00076956
Iteration 9/25 | Loss: 0.00076956
Iteration 10/25 | Loss: 0.00076956
Iteration 11/25 | Loss: 0.00076956
Iteration 12/25 | Loss: 0.00076956
Iteration 13/25 | Loss: 0.00076956
Iteration 14/25 | Loss: 0.00076956
Iteration 15/25 | Loss: 0.00076956
Iteration 16/25 | Loss: 0.00076956
Iteration 17/25 | Loss: 0.00076956
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007695642416365445, 0.0007695642416365445, 0.0007695642416365445, 0.0007695642416365445, 0.0007695642416365445]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007695642416365445

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076956
Iteration 2/1000 | Loss: 0.00002668
Iteration 3/1000 | Loss: 0.00001967
Iteration 4/1000 | Loss: 0.00001814
Iteration 5/1000 | Loss: 0.00001734
Iteration 6/1000 | Loss: 0.00001679
Iteration 7/1000 | Loss: 0.00001641
Iteration 8/1000 | Loss: 0.00001615
Iteration 9/1000 | Loss: 0.00001613
Iteration 10/1000 | Loss: 0.00001594
Iteration 11/1000 | Loss: 0.00001585
Iteration 12/1000 | Loss: 0.00001575
Iteration 13/1000 | Loss: 0.00001568
Iteration 14/1000 | Loss: 0.00001567
Iteration 15/1000 | Loss: 0.00001566
Iteration 16/1000 | Loss: 0.00001566
Iteration 17/1000 | Loss: 0.00001565
Iteration 18/1000 | Loss: 0.00001565
Iteration 19/1000 | Loss: 0.00001563
Iteration 20/1000 | Loss: 0.00001561
Iteration 21/1000 | Loss: 0.00001560
Iteration 22/1000 | Loss: 0.00001559
Iteration 23/1000 | Loss: 0.00001559
Iteration 24/1000 | Loss: 0.00001557
Iteration 25/1000 | Loss: 0.00001552
Iteration 26/1000 | Loss: 0.00001547
Iteration 27/1000 | Loss: 0.00001546
Iteration 28/1000 | Loss: 0.00001545
Iteration 29/1000 | Loss: 0.00001543
Iteration 30/1000 | Loss: 0.00001543
Iteration 31/1000 | Loss: 0.00001540
Iteration 32/1000 | Loss: 0.00001540
Iteration 33/1000 | Loss: 0.00001540
Iteration 34/1000 | Loss: 0.00001539
Iteration 35/1000 | Loss: 0.00001538
Iteration 36/1000 | Loss: 0.00001537
Iteration 37/1000 | Loss: 0.00001537
Iteration 38/1000 | Loss: 0.00001537
Iteration 39/1000 | Loss: 0.00001536
Iteration 40/1000 | Loss: 0.00001536
Iteration 41/1000 | Loss: 0.00001535
Iteration 42/1000 | Loss: 0.00001534
Iteration 43/1000 | Loss: 0.00001533
Iteration 44/1000 | Loss: 0.00001533
Iteration 45/1000 | Loss: 0.00001532
Iteration 46/1000 | Loss: 0.00001531
Iteration 47/1000 | Loss: 0.00001531
Iteration 48/1000 | Loss: 0.00001531
Iteration 49/1000 | Loss: 0.00001531
Iteration 50/1000 | Loss: 0.00001531
Iteration 51/1000 | Loss: 0.00001531
Iteration 52/1000 | Loss: 0.00001531
Iteration 53/1000 | Loss: 0.00001531
Iteration 54/1000 | Loss: 0.00001531
Iteration 55/1000 | Loss: 0.00001530
Iteration 56/1000 | Loss: 0.00001530
Iteration 57/1000 | Loss: 0.00001530
Iteration 58/1000 | Loss: 0.00001530
Iteration 59/1000 | Loss: 0.00001530
Iteration 60/1000 | Loss: 0.00001530
Iteration 61/1000 | Loss: 0.00001530
Iteration 62/1000 | Loss: 0.00001530
Iteration 63/1000 | Loss: 0.00001530
Iteration 64/1000 | Loss: 0.00001530
Iteration 65/1000 | Loss: 0.00001530
Iteration 66/1000 | Loss: 0.00001530
Iteration 67/1000 | Loss: 0.00001530
Iteration 68/1000 | Loss: 0.00001530
Iteration 69/1000 | Loss: 0.00001530
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 69. Stopping optimization.
Last 5 losses: [1.529950532130897e-05, 1.529950532130897e-05, 1.529950532130897e-05, 1.529950532130897e-05, 1.529950532130897e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.529950532130897e-05

Optimization complete. Final v2v error: 3.3320536613464355 mm

Highest mean error: 3.846252918243408 mm for frame 77

Lowest mean error: 3.1732189655303955 mm for frame 25

Saving results

Total time: 29.93187189102173
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00384957
Iteration 2/25 | Loss: 0.00126085
Iteration 3/25 | Loss: 0.00119517
Iteration 4/25 | Loss: 0.00118498
Iteration 5/25 | Loss: 0.00118100
Iteration 6/25 | Loss: 0.00118062
Iteration 7/25 | Loss: 0.00118062
Iteration 8/25 | Loss: 0.00118062
Iteration 9/25 | Loss: 0.00118062
Iteration 10/25 | Loss: 0.00118062
Iteration 11/25 | Loss: 0.00118062
Iteration 12/25 | Loss: 0.00118062
Iteration 13/25 | Loss: 0.00118062
Iteration 14/25 | Loss: 0.00118062
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0011806172551587224, 0.0011806172551587224, 0.0011806172551587224, 0.0011806172551587224, 0.0011806172551587224]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011806172551587224

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.00724983
Iteration 2/25 | Loss: 0.00077777
Iteration 3/25 | Loss: 0.00077777
Iteration 4/25 | Loss: 0.00077777
Iteration 5/25 | Loss: 0.00077777
Iteration 6/25 | Loss: 0.00077777
Iteration 7/25 | Loss: 0.00077777
Iteration 8/25 | Loss: 0.00077777
Iteration 9/25 | Loss: 0.00077777
Iteration 10/25 | Loss: 0.00077777
Iteration 11/25 | Loss: 0.00077777
Iteration 12/25 | Loss: 0.00077777
Iteration 13/25 | Loss: 0.00077777
Iteration 14/25 | Loss: 0.00077777
Iteration 15/25 | Loss: 0.00077777
Iteration 16/25 | Loss: 0.00077777
Iteration 17/25 | Loss: 0.00077777
Iteration 18/25 | Loss: 0.00077777
Iteration 19/25 | Loss: 0.00077777
Iteration 20/25 | Loss: 0.00077777
Iteration 21/25 | Loss: 0.00077777
Iteration 22/25 | Loss: 0.00077777
Iteration 23/25 | Loss: 0.00077777
Iteration 24/25 | Loss: 0.00077777
Iteration 25/25 | Loss: 0.00077777

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077777
Iteration 2/1000 | Loss: 0.00002416
Iteration 3/1000 | Loss: 0.00001629
Iteration 4/1000 | Loss: 0.00001394
Iteration 5/1000 | Loss: 0.00001320
Iteration 6/1000 | Loss: 0.00001250
Iteration 7/1000 | Loss: 0.00001205
Iteration 8/1000 | Loss: 0.00001182
Iteration 9/1000 | Loss: 0.00001161
Iteration 10/1000 | Loss: 0.00001139
Iteration 11/1000 | Loss: 0.00001133
Iteration 12/1000 | Loss: 0.00001131
Iteration 13/1000 | Loss: 0.00001128
Iteration 14/1000 | Loss: 0.00001119
Iteration 15/1000 | Loss: 0.00001119
Iteration 16/1000 | Loss: 0.00001118
Iteration 17/1000 | Loss: 0.00001108
Iteration 18/1000 | Loss: 0.00001106
Iteration 19/1000 | Loss: 0.00001105
Iteration 20/1000 | Loss: 0.00001104
Iteration 21/1000 | Loss: 0.00001104
Iteration 22/1000 | Loss: 0.00001103
Iteration 23/1000 | Loss: 0.00001102
Iteration 24/1000 | Loss: 0.00001101
Iteration 25/1000 | Loss: 0.00001101
Iteration 26/1000 | Loss: 0.00001100
Iteration 27/1000 | Loss: 0.00001100
Iteration 28/1000 | Loss: 0.00001099
Iteration 29/1000 | Loss: 0.00001096
Iteration 30/1000 | Loss: 0.00001096
Iteration 31/1000 | Loss: 0.00001096
Iteration 32/1000 | Loss: 0.00001095
Iteration 33/1000 | Loss: 0.00001095
Iteration 34/1000 | Loss: 0.00001095
Iteration 35/1000 | Loss: 0.00001094
Iteration 36/1000 | Loss: 0.00001094
Iteration 37/1000 | Loss: 0.00001094
Iteration 38/1000 | Loss: 0.00001094
Iteration 39/1000 | Loss: 0.00001093
Iteration 40/1000 | Loss: 0.00001093
Iteration 41/1000 | Loss: 0.00001091
Iteration 42/1000 | Loss: 0.00001091
Iteration 43/1000 | Loss: 0.00001091
Iteration 44/1000 | Loss: 0.00001091
Iteration 45/1000 | Loss: 0.00001091
Iteration 46/1000 | Loss: 0.00001091
Iteration 47/1000 | Loss: 0.00001091
Iteration 48/1000 | Loss: 0.00001090
Iteration 49/1000 | Loss: 0.00001089
Iteration 50/1000 | Loss: 0.00001088
Iteration 51/1000 | Loss: 0.00001088
Iteration 52/1000 | Loss: 0.00001087
Iteration 53/1000 | Loss: 0.00001087
Iteration 54/1000 | Loss: 0.00001087
Iteration 55/1000 | Loss: 0.00001087
Iteration 56/1000 | Loss: 0.00001086
Iteration 57/1000 | Loss: 0.00001086
Iteration 58/1000 | Loss: 0.00001086
Iteration 59/1000 | Loss: 0.00001085
Iteration 60/1000 | Loss: 0.00001084
Iteration 61/1000 | Loss: 0.00001080
Iteration 62/1000 | Loss: 0.00001080
Iteration 63/1000 | Loss: 0.00001080
Iteration 64/1000 | Loss: 0.00001080
Iteration 65/1000 | Loss: 0.00001080
Iteration 66/1000 | Loss: 0.00001080
Iteration 67/1000 | Loss: 0.00001080
Iteration 68/1000 | Loss: 0.00001080
Iteration 69/1000 | Loss: 0.00001080
Iteration 70/1000 | Loss: 0.00001080
Iteration 71/1000 | Loss: 0.00001079
Iteration 72/1000 | Loss: 0.00001079
Iteration 73/1000 | Loss: 0.00001078
Iteration 74/1000 | Loss: 0.00001076
Iteration 75/1000 | Loss: 0.00001076
Iteration 76/1000 | Loss: 0.00001076
Iteration 77/1000 | Loss: 0.00001076
Iteration 78/1000 | Loss: 0.00001076
Iteration 79/1000 | Loss: 0.00001076
Iteration 80/1000 | Loss: 0.00001075
Iteration 81/1000 | Loss: 0.00001075
Iteration 82/1000 | Loss: 0.00001075
Iteration 83/1000 | Loss: 0.00001075
Iteration 84/1000 | Loss: 0.00001075
Iteration 85/1000 | Loss: 0.00001075
Iteration 86/1000 | Loss: 0.00001074
Iteration 87/1000 | Loss: 0.00001074
Iteration 88/1000 | Loss: 0.00001073
Iteration 89/1000 | Loss: 0.00001073
Iteration 90/1000 | Loss: 0.00001073
Iteration 91/1000 | Loss: 0.00001073
Iteration 92/1000 | Loss: 0.00001072
Iteration 93/1000 | Loss: 0.00001072
Iteration 94/1000 | Loss: 0.00001072
Iteration 95/1000 | Loss: 0.00001072
Iteration 96/1000 | Loss: 0.00001071
Iteration 97/1000 | Loss: 0.00001071
Iteration 98/1000 | Loss: 0.00001070
Iteration 99/1000 | Loss: 0.00001070
Iteration 100/1000 | Loss: 0.00001069
Iteration 101/1000 | Loss: 0.00001069
Iteration 102/1000 | Loss: 0.00001069
Iteration 103/1000 | Loss: 0.00001069
Iteration 104/1000 | Loss: 0.00001069
Iteration 105/1000 | Loss: 0.00001069
Iteration 106/1000 | Loss: 0.00001068
Iteration 107/1000 | Loss: 0.00001068
Iteration 108/1000 | Loss: 0.00001068
Iteration 109/1000 | Loss: 0.00001068
Iteration 110/1000 | Loss: 0.00001067
Iteration 111/1000 | Loss: 0.00001067
Iteration 112/1000 | Loss: 0.00001067
Iteration 113/1000 | Loss: 0.00001067
Iteration 114/1000 | Loss: 0.00001066
Iteration 115/1000 | Loss: 0.00001065
Iteration 116/1000 | Loss: 0.00001065
Iteration 117/1000 | Loss: 0.00001065
Iteration 118/1000 | Loss: 0.00001064
Iteration 119/1000 | Loss: 0.00001064
Iteration 120/1000 | Loss: 0.00001063
Iteration 121/1000 | Loss: 0.00001063
Iteration 122/1000 | Loss: 0.00001063
Iteration 123/1000 | Loss: 0.00001062
Iteration 124/1000 | Loss: 0.00001062
Iteration 125/1000 | Loss: 0.00001062
Iteration 126/1000 | Loss: 0.00001062
Iteration 127/1000 | Loss: 0.00001061
Iteration 128/1000 | Loss: 0.00001061
Iteration 129/1000 | Loss: 0.00001061
Iteration 130/1000 | Loss: 0.00001061
Iteration 131/1000 | Loss: 0.00001061
Iteration 132/1000 | Loss: 0.00001061
Iteration 133/1000 | Loss: 0.00001061
Iteration 134/1000 | Loss: 0.00001061
Iteration 135/1000 | Loss: 0.00001060
Iteration 136/1000 | Loss: 0.00001060
Iteration 137/1000 | Loss: 0.00001060
Iteration 138/1000 | Loss: 0.00001060
Iteration 139/1000 | Loss: 0.00001060
Iteration 140/1000 | Loss: 0.00001060
Iteration 141/1000 | Loss: 0.00001060
Iteration 142/1000 | Loss: 0.00001059
Iteration 143/1000 | Loss: 0.00001059
Iteration 144/1000 | Loss: 0.00001059
Iteration 145/1000 | Loss: 0.00001059
Iteration 146/1000 | Loss: 0.00001059
Iteration 147/1000 | Loss: 0.00001059
Iteration 148/1000 | Loss: 0.00001059
Iteration 149/1000 | Loss: 0.00001059
Iteration 150/1000 | Loss: 0.00001059
Iteration 151/1000 | Loss: 0.00001059
Iteration 152/1000 | Loss: 0.00001059
Iteration 153/1000 | Loss: 0.00001059
Iteration 154/1000 | Loss: 0.00001059
Iteration 155/1000 | Loss: 0.00001059
Iteration 156/1000 | Loss: 0.00001059
Iteration 157/1000 | Loss: 0.00001059
Iteration 158/1000 | Loss: 0.00001059
Iteration 159/1000 | Loss: 0.00001059
Iteration 160/1000 | Loss: 0.00001059
Iteration 161/1000 | Loss: 0.00001059
Iteration 162/1000 | Loss: 0.00001058
Iteration 163/1000 | Loss: 0.00001058
Iteration 164/1000 | Loss: 0.00001058
Iteration 165/1000 | Loss: 0.00001058
Iteration 166/1000 | Loss: 0.00001058
Iteration 167/1000 | Loss: 0.00001058
Iteration 168/1000 | Loss: 0.00001058
Iteration 169/1000 | Loss: 0.00001058
Iteration 170/1000 | Loss: 0.00001058
Iteration 171/1000 | Loss: 0.00001058
Iteration 172/1000 | Loss: 0.00001058
Iteration 173/1000 | Loss: 0.00001058
Iteration 174/1000 | Loss: 0.00001058
Iteration 175/1000 | Loss: 0.00001058
Iteration 176/1000 | Loss: 0.00001058
Iteration 177/1000 | Loss: 0.00001058
Iteration 178/1000 | Loss: 0.00001058
Iteration 179/1000 | Loss: 0.00001058
Iteration 180/1000 | Loss: 0.00001058
Iteration 181/1000 | Loss: 0.00001057
Iteration 182/1000 | Loss: 0.00001057
Iteration 183/1000 | Loss: 0.00001057
Iteration 184/1000 | Loss: 0.00001057
Iteration 185/1000 | Loss: 0.00001057
Iteration 186/1000 | Loss: 0.00001057
Iteration 187/1000 | Loss: 0.00001057
Iteration 188/1000 | Loss: 0.00001057
Iteration 189/1000 | Loss: 0.00001057
Iteration 190/1000 | Loss: 0.00001057
Iteration 191/1000 | Loss: 0.00001057
Iteration 192/1000 | Loss: 0.00001057
Iteration 193/1000 | Loss: 0.00001057
Iteration 194/1000 | Loss: 0.00001057
Iteration 195/1000 | Loss: 0.00001057
Iteration 196/1000 | Loss: 0.00001057
Iteration 197/1000 | Loss: 0.00001057
Iteration 198/1000 | Loss: 0.00001056
Iteration 199/1000 | Loss: 0.00001056
Iteration 200/1000 | Loss: 0.00001056
Iteration 201/1000 | Loss: 0.00001056
Iteration 202/1000 | Loss: 0.00001056
Iteration 203/1000 | Loss: 0.00001056
Iteration 204/1000 | Loss: 0.00001056
Iteration 205/1000 | Loss: 0.00001056
Iteration 206/1000 | Loss: 0.00001056
Iteration 207/1000 | Loss: 0.00001056
Iteration 208/1000 | Loss: 0.00001056
Iteration 209/1000 | Loss: 0.00001056
Iteration 210/1000 | Loss: 0.00001056
Iteration 211/1000 | Loss: 0.00001056
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [1.055980465025641e-05, 1.055980465025641e-05, 1.055980465025641e-05, 1.055980465025641e-05, 1.055980465025641e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.055980465025641e-05

Optimization complete. Final v2v error: 2.7967536449432373 mm

Highest mean error: 3.190352201461792 mm for frame 96

Lowest mean error: 2.6319527626037598 mm for frame 28

Saving results

Total time: 41.16130781173706
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00841477
Iteration 2/25 | Loss: 0.00288174
Iteration 3/25 | Loss: 0.00198927
Iteration 4/25 | Loss: 0.00166397
Iteration 5/25 | Loss: 0.00154081
Iteration 6/25 | Loss: 0.00151492
Iteration 7/25 | Loss: 0.00149082
Iteration 8/25 | Loss: 0.00148827
Iteration 9/25 | Loss: 0.00148366
Iteration 10/25 | Loss: 0.00148231
Iteration 11/25 | Loss: 0.00146844
Iteration 12/25 | Loss: 0.00146721
Iteration 13/25 | Loss: 0.00146652
Iteration 14/25 | Loss: 0.00146877
Iteration 15/25 | Loss: 0.00146553
Iteration 16/25 | Loss: 0.00146828
Iteration 17/25 | Loss: 0.00146463
Iteration 18/25 | Loss: 0.00147239
Iteration 19/25 | Loss: 0.00146546
Iteration 20/25 | Loss: 0.00146413
Iteration 21/25 | Loss: 0.00146332
Iteration 22/25 | Loss: 0.00146319
Iteration 23/25 | Loss: 0.00146308
Iteration 24/25 | Loss: 0.00146628
Iteration 25/25 | Loss: 0.00146289

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.50478697
Iteration 2/25 | Loss: 0.00407709
Iteration 3/25 | Loss: 0.00298784
Iteration 4/25 | Loss: 0.00298783
Iteration 5/25 | Loss: 0.00298783
Iteration 6/25 | Loss: 0.00298783
Iteration 7/25 | Loss: 0.00298783
Iteration 8/25 | Loss: 0.00298783
Iteration 9/25 | Loss: 0.00298783
Iteration 10/25 | Loss: 0.00298783
Iteration 11/25 | Loss: 0.00298783
Iteration 12/25 | Loss: 0.00298783
Iteration 13/25 | Loss: 0.00298783
Iteration 14/25 | Loss: 0.00298783
Iteration 15/25 | Loss: 0.00298783
Iteration 16/25 | Loss: 0.00298783
Iteration 17/25 | Loss: 0.00298783
Iteration 18/25 | Loss: 0.00298783
Iteration 19/25 | Loss: 0.00298783
Iteration 20/25 | Loss: 0.00298783
Iteration 21/25 | Loss: 0.00298783
Iteration 22/25 | Loss: 0.00298783
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.002987826243042946, 0.002987826243042946, 0.002987826243042946, 0.002987826243042946, 0.002987826243042946]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002987826243042946

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00298783
Iteration 2/1000 | Loss: 0.00625659
Iteration 3/1000 | Loss: 0.00109568
Iteration 4/1000 | Loss: 0.00024559
Iteration 5/1000 | Loss: 0.00026673
Iteration 6/1000 | Loss: 0.00019188
Iteration 7/1000 | Loss: 0.00170326
Iteration 8/1000 | Loss: 0.00055537
Iteration 9/1000 | Loss: 0.00181802
Iteration 10/1000 | Loss: 0.00440942
Iteration 11/1000 | Loss: 0.00024425
Iteration 12/1000 | Loss: 0.00035722
Iteration 13/1000 | Loss: 0.00016620
Iteration 14/1000 | Loss: 0.00108748
Iteration 15/1000 | Loss: 0.00008416
Iteration 16/1000 | Loss: 0.00007489
Iteration 17/1000 | Loss: 0.00006899
Iteration 18/1000 | Loss: 0.00006253
Iteration 19/1000 | Loss: 0.00323216
Iteration 20/1000 | Loss: 0.00239408
Iteration 21/1000 | Loss: 0.00177803
Iteration 22/1000 | Loss: 0.00046766
Iteration 23/1000 | Loss: 0.00016199
Iteration 24/1000 | Loss: 0.00005368
Iteration 25/1000 | Loss: 0.00004411
Iteration 26/1000 | Loss: 0.00023857
Iteration 27/1000 | Loss: 0.00003843
Iteration 28/1000 | Loss: 0.00003350
Iteration 29/1000 | Loss: 0.00003075
Iteration 30/1000 | Loss: 0.00031781
Iteration 31/1000 | Loss: 0.00003070
Iteration 32/1000 | Loss: 0.00002639
Iteration 33/1000 | Loss: 0.00002456
Iteration 34/1000 | Loss: 0.00002357
Iteration 35/1000 | Loss: 0.00002260
Iteration 36/1000 | Loss: 0.00031920
Iteration 37/1000 | Loss: 0.00002672
Iteration 38/1000 | Loss: 0.00002203
Iteration 39/1000 | Loss: 0.00002362
Iteration 40/1000 | Loss: 0.00002074
Iteration 41/1000 | Loss: 0.00001920
Iteration 42/1000 | Loss: 0.00001820
Iteration 43/1000 | Loss: 0.00001780
Iteration 44/1000 | Loss: 0.00001770
Iteration 45/1000 | Loss: 0.00001783
Iteration 46/1000 | Loss: 0.00001748
Iteration 47/1000 | Loss: 0.00001727
Iteration 48/1000 | Loss: 0.00001817
Iteration 49/1000 | Loss: 0.00001709
Iteration 50/1000 | Loss: 0.00001709
Iteration 51/1000 | Loss: 0.00001709
Iteration 52/1000 | Loss: 0.00001708
Iteration 53/1000 | Loss: 0.00001708
Iteration 54/1000 | Loss: 0.00001708
Iteration 55/1000 | Loss: 0.00001708
Iteration 56/1000 | Loss: 0.00001707
Iteration 57/1000 | Loss: 0.00001707
Iteration 58/1000 | Loss: 0.00001706
Iteration 59/1000 | Loss: 0.00001706
Iteration 60/1000 | Loss: 0.00001706
Iteration 61/1000 | Loss: 0.00001706
Iteration 62/1000 | Loss: 0.00001705
Iteration 63/1000 | Loss: 0.00001705
Iteration 64/1000 | Loss: 0.00001704
Iteration 65/1000 | Loss: 0.00001704
Iteration 66/1000 | Loss: 0.00001704
Iteration 67/1000 | Loss: 0.00001703
Iteration 68/1000 | Loss: 0.00001703
Iteration 69/1000 | Loss: 0.00001702
Iteration 70/1000 | Loss: 0.00001702
Iteration 71/1000 | Loss: 0.00001750
Iteration 72/1000 | Loss: 0.00001750
Iteration 73/1000 | Loss: 0.00001888
Iteration 74/1000 | Loss: 0.00001708
Iteration 75/1000 | Loss: 0.00001693
Iteration 76/1000 | Loss: 0.00001693
Iteration 77/1000 | Loss: 0.00001693
Iteration 78/1000 | Loss: 0.00001693
Iteration 79/1000 | Loss: 0.00001693
Iteration 80/1000 | Loss: 0.00001693
Iteration 81/1000 | Loss: 0.00001693
Iteration 82/1000 | Loss: 0.00001693
Iteration 83/1000 | Loss: 0.00001693
Iteration 84/1000 | Loss: 0.00001693
Iteration 85/1000 | Loss: 0.00001693
Iteration 86/1000 | Loss: 0.00001693
Iteration 87/1000 | Loss: 0.00001693
Iteration 88/1000 | Loss: 0.00001693
Iteration 89/1000 | Loss: 0.00001693
Iteration 90/1000 | Loss: 0.00001693
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [1.6925014278967865e-05, 1.6925014278967865e-05, 1.6925014278967865e-05, 1.6925014278967865e-05, 1.6925014278967865e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6925014278967865e-05

Optimization complete. Final v2v error: 3.4477381706237793 mm

Highest mean error: 4.878151893615723 mm for frame 86

Lowest mean error: 2.931210994720459 mm for frame 153

Saving results

Total time: 123.19727444648743
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01064926
Iteration 2/25 | Loss: 0.01064926
Iteration 3/25 | Loss: 0.00252442
Iteration 4/25 | Loss: 0.00237498
Iteration 5/25 | Loss: 0.00212974
Iteration 6/25 | Loss: 0.00189000
Iteration 7/25 | Loss: 0.00168619
Iteration 8/25 | Loss: 0.00159196
Iteration 9/25 | Loss: 0.00153159
Iteration 10/25 | Loss: 0.00151569
Iteration 11/25 | Loss: 0.00149985
Iteration 12/25 | Loss: 0.00148434
Iteration 13/25 | Loss: 0.00147003
Iteration 14/25 | Loss: 0.00146715
Iteration 15/25 | Loss: 0.00144534
Iteration 16/25 | Loss: 0.00143117
Iteration 17/25 | Loss: 0.00141381
Iteration 18/25 | Loss: 0.00139346
Iteration 19/25 | Loss: 0.00138180
Iteration 20/25 | Loss: 0.00138177
Iteration 21/25 | Loss: 0.00138065
Iteration 22/25 | Loss: 0.00137336
Iteration 23/25 | Loss: 0.00137513
Iteration 24/25 | Loss: 0.00137357
Iteration 25/25 | Loss: 0.00137322

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38595366
Iteration 2/25 | Loss: 0.00635232
Iteration 3/25 | Loss: 0.00188531
Iteration 4/25 | Loss: 0.00188531
Iteration 5/25 | Loss: 0.00188531
Iteration 6/25 | Loss: 0.00188531
Iteration 7/25 | Loss: 0.00188531
Iteration 8/25 | Loss: 0.00188531
Iteration 9/25 | Loss: 0.00188531
Iteration 10/25 | Loss: 0.00188531
Iteration 11/25 | Loss: 0.00188531
Iteration 12/25 | Loss: 0.00188531
Iteration 13/25 | Loss: 0.00188531
Iteration 14/25 | Loss: 0.00188531
Iteration 15/25 | Loss: 0.00188531
Iteration 16/25 | Loss: 0.00188531
Iteration 17/25 | Loss: 0.00188531
Iteration 18/25 | Loss: 0.00188531
Iteration 19/25 | Loss: 0.00188531
Iteration 20/25 | Loss: 0.00188531
Iteration 21/25 | Loss: 0.00188531
Iteration 22/25 | Loss: 0.00188531
Iteration 23/25 | Loss: 0.00188531
Iteration 24/25 | Loss: 0.00188531
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.001885308651253581, 0.001885308651253581, 0.001885308651253581, 0.001885308651253581, 0.001885308651253581]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001885308651253581

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00188531
Iteration 2/1000 | Loss: 0.00452880
Iteration 3/1000 | Loss: 0.00045112
Iteration 4/1000 | Loss: 0.00063530
Iteration 5/1000 | Loss: 0.00036258
Iteration 6/1000 | Loss: 0.00021532
Iteration 7/1000 | Loss: 0.00219768
Iteration 8/1000 | Loss: 0.00061262
Iteration 9/1000 | Loss: 0.00018763
Iteration 10/1000 | Loss: 0.00117694
Iteration 11/1000 | Loss: 0.00009973
Iteration 12/1000 | Loss: 0.00016788
Iteration 13/1000 | Loss: 0.00120794
Iteration 14/1000 | Loss: 0.00081421
Iteration 15/1000 | Loss: 0.00015934
Iteration 16/1000 | Loss: 0.00015688
Iteration 17/1000 | Loss: 0.00009088
Iteration 18/1000 | Loss: 0.00057503
Iteration 19/1000 | Loss: 0.00044113
Iteration 20/1000 | Loss: 0.00038724
Iteration 21/1000 | Loss: 0.00091121
Iteration 22/1000 | Loss: 0.00055566
Iteration 23/1000 | Loss: 0.00042138
Iteration 24/1000 | Loss: 0.00017537
Iteration 25/1000 | Loss: 0.00017629
Iteration 26/1000 | Loss: 0.00045678
Iteration 27/1000 | Loss: 0.00037599
Iteration 28/1000 | Loss: 0.00015825
Iteration 29/1000 | Loss: 0.00024639
Iteration 30/1000 | Loss: 0.00027319
Iteration 31/1000 | Loss: 0.00023755
Iteration 32/1000 | Loss: 0.00017141
Iteration 33/1000 | Loss: 0.00023764
Iteration 34/1000 | Loss: 0.00023734
Iteration 35/1000 | Loss: 0.00043969
Iteration 36/1000 | Loss: 0.00025012
Iteration 37/1000 | Loss: 0.00024251
Iteration 38/1000 | Loss: 0.00023482
Iteration 39/1000 | Loss: 0.00018107
Iteration 40/1000 | Loss: 0.00025804
Iteration 41/1000 | Loss: 0.00021888
Iteration 42/1000 | Loss: 0.00021328
Iteration 43/1000 | Loss: 0.00027046
Iteration 44/1000 | Loss: 0.00022678
Iteration 45/1000 | Loss: 0.00053125
Iteration 46/1000 | Loss: 0.00028884
Iteration 47/1000 | Loss: 0.00015910
Iteration 48/1000 | Loss: 0.00016724
Iteration 49/1000 | Loss: 0.00025703
Iteration 50/1000 | Loss: 0.00022031
Iteration 51/1000 | Loss: 0.00023598
Iteration 52/1000 | Loss: 0.00022610
Iteration 53/1000 | Loss: 0.00018442
Iteration 54/1000 | Loss: 0.00010917
Iteration 55/1000 | Loss: 0.00022160
Iteration 56/1000 | Loss: 0.00012482
Iteration 57/1000 | Loss: 0.00020765
Iteration 58/1000 | Loss: 0.00021469
Iteration 59/1000 | Loss: 0.00022164
Iteration 60/1000 | Loss: 0.00018643
Iteration 61/1000 | Loss: 0.00011499
Iteration 62/1000 | Loss: 0.00019726
Iteration 63/1000 | Loss: 0.00011140
Iteration 64/1000 | Loss: 0.00016629
Iteration 65/1000 | Loss: 0.00012722
Iteration 66/1000 | Loss: 0.00017261
Iteration 67/1000 | Loss: 0.00015374
Iteration 68/1000 | Loss: 0.00050138
Iteration 69/1000 | Loss: 0.00037343
Iteration 70/1000 | Loss: 0.00013015
Iteration 71/1000 | Loss: 0.00015337
Iteration 72/1000 | Loss: 0.00014501
Iteration 73/1000 | Loss: 0.00020601
Iteration 74/1000 | Loss: 0.00014429
Iteration 75/1000 | Loss: 0.00042465
Iteration 76/1000 | Loss: 0.00011549
Iteration 77/1000 | Loss: 0.00026583
Iteration 78/1000 | Loss: 0.00021492
Iteration 79/1000 | Loss: 0.00018234
Iteration 80/1000 | Loss: 0.00016847
Iteration 81/1000 | Loss: 0.00025424
Iteration 82/1000 | Loss: 0.00025444
Iteration 83/1000 | Loss: 0.00019500
Iteration 84/1000 | Loss: 0.00034793
Iteration 85/1000 | Loss: 0.00025038
Iteration 86/1000 | Loss: 0.00017459
Iteration 87/1000 | Loss: 0.00006633
Iteration 88/1000 | Loss: 0.00040276
Iteration 89/1000 | Loss: 0.00051907
Iteration 90/1000 | Loss: 0.00042123
Iteration 91/1000 | Loss: 0.00025627
Iteration 92/1000 | Loss: 0.00036601
Iteration 93/1000 | Loss: 0.00057864
Iteration 94/1000 | Loss: 0.00019426
Iteration 95/1000 | Loss: 0.00017565
Iteration 96/1000 | Loss: 0.00007874
Iteration 97/1000 | Loss: 0.00014777
Iteration 98/1000 | Loss: 0.00007262
Iteration 99/1000 | Loss: 0.00012718
Iteration 100/1000 | Loss: 0.00014302
Iteration 101/1000 | Loss: 0.00010987
Iteration 102/1000 | Loss: 0.00020782
Iteration 103/1000 | Loss: 0.00018894
Iteration 104/1000 | Loss: 0.00006416
Iteration 105/1000 | Loss: 0.00022711
Iteration 106/1000 | Loss: 0.00011788
Iteration 107/1000 | Loss: 0.00005618
Iteration 108/1000 | Loss: 0.00016713
Iteration 109/1000 | Loss: 0.00012261
Iteration 110/1000 | Loss: 0.00010864
Iteration 111/1000 | Loss: 0.00012657
Iteration 112/1000 | Loss: 0.00009257
Iteration 113/1000 | Loss: 0.00019081
Iteration 114/1000 | Loss: 0.00050957
Iteration 115/1000 | Loss: 0.00028678
Iteration 116/1000 | Loss: 0.00015465
Iteration 117/1000 | Loss: 0.00004988
Iteration 118/1000 | Loss: 0.00005227
Iteration 119/1000 | Loss: 0.00004784
Iteration 120/1000 | Loss: 0.00023347
Iteration 121/1000 | Loss: 0.00032593
Iteration 122/1000 | Loss: 0.00020703
Iteration 123/1000 | Loss: 0.00021402
Iteration 124/1000 | Loss: 0.00005092
Iteration 125/1000 | Loss: 0.00012721
Iteration 126/1000 | Loss: 0.00007995
Iteration 127/1000 | Loss: 0.00006229
Iteration 128/1000 | Loss: 0.00016006
Iteration 129/1000 | Loss: 0.00013734
Iteration 130/1000 | Loss: 0.00021549
Iteration 131/1000 | Loss: 0.00018251
Iteration 132/1000 | Loss: 0.00014117
Iteration 133/1000 | Loss: 0.00020905
Iteration 134/1000 | Loss: 0.00013373
Iteration 135/1000 | Loss: 0.00014140
Iteration 136/1000 | Loss: 0.00016796
Iteration 137/1000 | Loss: 0.00036788
Iteration 138/1000 | Loss: 0.00011866
Iteration 139/1000 | Loss: 0.00014227
Iteration 140/1000 | Loss: 0.00011554
Iteration 141/1000 | Loss: 0.00009545
Iteration 142/1000 | Loss: 0.00006586
Iteration 143/1000 | Loss: 0.00004009
Iteration 144/1000 | Loss: 0.00003679
Iteration 145/1000 | Loss: 0.00017253
Iteration 146/1000 | Loss: 0.00011253
Iteration 147/1000 | Loss: 0.00055192
Iteration 148/1000 | Loss: 0.00025976
Iteration 149/1000 | Loss: 0.00047207
Iteration 150/1000 | Loss: 0.00029602
Iteration 151/1000 | Loss: 0.00028442
Iteration 152/1000 | Loss: 0.00006664
Iteration 153/1000 | Loss: 0.00011537
Iteration 154/1000 | Loss: 0.00003728
Iteration 155/1000 | Loss: 0.00003502
Iteration 156/1000 | Loss: 0.00017783
Iteration 157/1000 | Loss: 0.00004023
Iteration 158/1000 | Loss: 0.00015094
Iteration 159/1000 | Loss: 0.00016398
Iteration 160/1000 | Loss: 0.00004646
Iteration 161/1000 | Loss: 0.00004558
Iteration 162/1000 | Loss: 0.00003562
Iteration 163/1000 | Loss: 0.00003401
Iteration 164/1000 | Loss: 0.00003286
Iteration 165/1000 | Loss: 0.00003223
Iteration 166/1000 | Loss: 0.00003182
Iteration 167/1000 | Loss: 0.00003149
Iteration 168/1000 | Loss: 0.00003117
Iteration 169/1000 | Loss: 0.00003083
Iteration 170/1000 | Loss: 0.00003043
Iteration 171/1000 | Loss: 0.00017154
Iteration 172/1000 | Loss: 0.00101204
Iteration 173/1000 | Loss: 0.00070163
Iteration 174/1000 | Loss: 0.00004865
Iteration 175/1000 | Loss: 0.00019608
Iteration 176/1000 | Loss: 0.00011556
Iteration 177/1000 | Loss: 0.00019451
Iteration 178/1000 | Loss: 0.00017220
Iteration 179/1000 | Loss: 0.00012699
Iteration 180/1000 | Loss: 0.00066814
Iteration 181/1000 | Loss: 0.00044421
Iteration 182/1000 | Loss: 0.00003550
Iteration 183/1000 | Loss: 0.00047713
Iteration 184/1000 | Loss: 0.00008242
Iteration 185/1000 | Loss: 0.00030766
Iteration 186/1000 | Loss: 0.00011990
Iteration 187/1000 | Loss: 0.00025723
Iteration 188/1000 | Loss: 0.00010170
Iteration 189/1000 | Loss: 0.00020188
Iteration 190/1000 | Loss: 0.00021954
Iteration 191/1000 | Loss: 0.00005545
Iteration 192/1000 | Loss: 0.00010935
Iteration 193/1000 | Loss: 0.00006534
Iteration 194/1000 | Loss: 0.00032927
Iteration 195/1000 | Loss: 0.00005964
Iteration 196/1000 | Loss: 0.00002910
Iteration 197/1000 | Loss: 0.00002613
Iteration 198/1000 | Loss: 0.00002510
Iteration 199/1000 | Loss: 0.00003173
Iteration 200/1000 | Loss: 0.00002460
Iteration 201/1000 | Loss: 0.00002283
Iteration 202/1000 | Loss: 0.00002231
Iteration 203/1000 | Loss: 0.00002334
Iteration 204/1000 | Loss: 0.00002162
Iteration 205/1000 | Loss: 0.00002120
Iteration 206/1000 | Loss: 0.00002090
Iteration 207/1000 | Loss: 0.00002065
Iteration 208/1000 | Loss: 0.00002034
Iteration 209/1000 | Loss: 0.00002006
Iteration 210/1000 | Loss: 0.00002000
Iteration 211/1000 | Loss: 0.00001994
Iteration 212/1000 | Loss: 0.00001990
Iteration 213/1000 | Loss: 0.00001986
Iteration 214/1000 | Loss: 0.00001979
Iteration 215/1000 | Loss: 0.00001976
Iteration 216/1000 | Loss: 0.00001956
Iteration 217/1000 | Loss: 0.00001956
Iteration 218/1000 | Loss: 0.00001934
Iteration 219/1000 | Loss: 0.00001903
Iteration 220/1000 | Loss: 0.00001878
Iteration 221/1000 | Loss: 0.00001856
Iteration 222/1000 | Loss: 0.00027702
Iteration 223/1000 | Loss: 0.00001899
Iteration 224/1000 | Loss: 0.00001790
Iteration 225/1000 | Loss: 0.00001748
Iteration 226/1000 | Loss: 0.00001696
Iteration 227/1000 | Loss: 0.00001693
Iteration 228/1000 | Loss: 0.00014774
Iteration 229/1000 | Loss: 0.00012930
Iteration 230/1000 | Loss: 0.00013068
Iteration 231/1000 | Loss: 0.00014096
Iteration 232/1000 | Loss: 0.00014511
Iteration 233/1000 | Loss: 0.00002098
Iteration 234/1000 | Loss: 0.00001970
Iteration 235/1000 | Loss: 0.00001855
Iteration 236/1000 | Loss: 0.00001783
Iteration 237/1000 | Loss: 0.00001742
Iteration 238/1000 | Loss: 0.00001709
Iteration 239/1000 | Loss: 0.00018612
Iteration 240/1000 | Loss: 0.00018004
Iteration 241/1000 | Loss: 0.00015907
Iteration 242/1000 | Loss: 0.00010051
Iteration 243/1000 | Loss: 0.00004747
Iteration 244/1000 | Loss: 0.00002134
Iteration 245/1000 | Loss: 0.00001885
Iteration 246/1000 | Loss: 0.00001682
Iteration 247/1000 | Loss: 0.00001635
Iteration 248/1000 | Loss: 0.00001618
Iteration 249/1000 | Loss: 0.00001616
Iteration 250/1000 | Loss: 0.00001608
Iteration 251/1000 | Loss: 0.00001606
Iteration 252/1000 | Loss: 0.00001606
Iteration 253/1000 | Loss: 0.00001606
Iteration 254/1000 | Loss: 0.00001605
Iteration 255/1000 | Loss: 0.00001605
Iteration 256/1000 | Loss: 0.00001605
Iteration 257/1000 | Loss: 0.00001605
Iteration 258/1000 | Loss: 0.00001605
Iteration 259/1000 | Loss: 0.00001604
Iteration 260/1000 | Loss: 0.00001602
Iteration 261/1000 | Loss: 0.00001601
Iteration 262/1000 | Loss: 0.00001601
Iteration 263/1000 | Loss: 0.00001601
Iteration 264/1000 | Loss: 0.00001601
Iteration 265/1000 | Loss: 0.00001600
Iteration 266/1000 | Loss: 0.00001600
Iteration 267/1000 | Loss: 0.00001600
Iteration 268/1000 | Loss: 0.00001599
Iteration 269/1000 | Loss: 0.00001599
Iteration 270/1000 | Loss: 0.00001599
Iteration 271/1000 | Loss: 0.00001598
Iteration 272/1000 | Loss: 0.00001598
Iteration 273/1000 | Loss: 0.00001598
Iteration 274/1000 | Loss: 0.00001598
Iteration 275/1000 | Loss: 0.00001598
Iteration 276/1000 | Loss: 0.00001598
Iteration 277/1000 | Loss: 0.00001597
Iteration 278/1000 | Loss: 0.00001597
Iteration 279/1000 | Loss: 0.00001597
Iteration 280/1000 | Loss: 0.00001597
Iteration 281/1000 | Loss: 0.00001596
Iteration 282/1000 | Loss: 0.00001596
Iteration 283/1000 | Loss: 0.00001595
Iteration 284/1000 | Loss: 0.00001595
Iteration 285/1000 | Loss: 0.00001595
Iteration 286/1000 | Loss: 0.00001595
Iteration 287/1000 | Loss: 0.00001595
Iteration 288/1000 | Loss: 0.00001594
Iteration 289/1000 | Loss: 0.00001594
Iteration 290/1000 | Loss: 0.00001594
Iteration 291/1000 | Loss: 0.00001594
Iteration 292/1000 | Loss: 0.00001593
Iteration 293/1000 | Loss: 0.00001593
Iteration 294/1000 | Loss: 0.00001593
Iteration 295/1000 | Loss: 0.00001593
Iteration 296/1000 | Loss: 0.00001593
Iteration 297/1000 | Loss: 0.00001592
Iteration 298/1000 | Loss: 0.00001592
Iteration 299/1000 | Loss: 0.00001592
Iteration 300/1000 | Loss: 0.00001592
Iteration 301/1000 | Loss: 0.00001591
Iteration 302/1000 | Loss: 0.00001591
Iteration 303/1000 | Loss: 0.00001591
Iteration 304/1000 | Loss: 0.00001591
Iteration 305/1000 | Loss: 0.00001591
Iteration 306/1000 | Loss: 0.00001591
Iteration 307/1000 | Loss: 0.00001591
Iteration 308/1000 | Loss: 0.00001591
Iteration 309/1000 | Loss: 0.00001591
Iteration 310/1000 | Loss: 0.00001591
Iteration 311/1000 | Loss: 0.00001591
Iteration 312/1000 | Loss: 0.00001591
Iteration 313/1000 | Loss: 0.00001590
Iteration 314/1000 | Loss: 0.00001590
Iteration 315/1000 | Loss: 0.00001590
Iteration 316/1000 | Loss: 0.00001590
Iteration 317/1000 | Loss: 0.00001590
Iteration 318/1000 | Loss: 0.00001590
Iteration 319/1000 | Loss: 0.00001590
Iteration 320/1000 | Loss: 0.00001590
Iteration 321/1000 | Loss: 0.00001590
Iteration 322/1000 | Loss: 0.00001590
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 322. Stopping optimization.
Last 5 losses: [1.590363353898283e-05, 1.590363353898283e-05, 1.590363353898283e-05, 1.590363353898283e-05, 1.590363353898283e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.590363353898283e-05

Optimization complete. Final v2v error: 3.3838980197906494 mm

Highest mean error: 5.468707084655762 mm for frame 45

Lowest mean error: 3.195493221282959 mm for frame 197

Saving results

Total time: 447.1625328063965
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01000891
Iteration 2/25 | Loss: 0.01000891
Iteration 3/25 | Loss: 0.01000891
Iteration 4/25 | Loss: 0.01000891
Iteration 5/25 | Loss: 0.00280590
Iteration 6/25 | Loss: 0.00192573
Iteration 7/25 | Loss: 0.00173963
Iteration 8/25 | Loss: 0.00169485
Iteration 9/25 | Loss: 0.00162163
Iteration 10/25 | Loss: 0.00153055
Iteration 11/25 | Loss: 0.00150111
Iteration 12/25 | Loss: 0.00146082
Iteration 13/25 | Loss: 0.00145234
Iteration 14/25 | Loss: 0.00144523
Iteration 15/25 | Loss: 0.00144150
Iteration 16/25 | Loss: 0.00143795
Iteration 17/25 | Loss: 0.00143135
Iteration 18/25 | Loss: 0.00143276
Iteration 19/25 | Loss: 0.00142763
Iteration 20/25 | Loss: 0.00142545
Iteration 21/25 | Loss: 0.00142570
Iteration 22/25 | Loss: 0.00142158
Iteration 23/25 | Loss: 0.00142277
Iteration 24/25 | Loss: 0.00141615
Iteration 25/25 | Loss: 0.00141310

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40723431
Iteration 2/25 | Loss: 0.00228731
Iteration 3/25 | Loss: 0.00137720
Iteration 4/25 | Loss: 0.00137716
Iteration 5/25 | Loss: 0.00137716
Iteration 6/25 | Loss: 0.00137716
Iteration 7/25 | Loss: 0.00137716
Iteration 8/25 | Loss: 0.00137716
Iteration 9/25 | Loss: 0.00137716
Iteration 10/25 | Loss: 0.00137716
Iteration 11/25 | Loss: 0.00137716
Iteration 12/25 | Loss: 0.00137716
Iteration 13/25 | Loss: 0.00137715
Iteration 14/25 | Loss: 0.00137715
Iteration 15/25 | Loss: 0.00137715
Iteration 16/25 | Loss: 0.00137715
Iteration 17/25 | Loss: 0.00137715
Iteration 18/25 | Loss: 0.00137715
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0013771547237411141, 0.0013771547237411141, 0.0013771547237411141, 0.0013771547237411141, 0.0013771547237411141]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013771547237411141

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137715
Iteration 2/1000 | Loss: 0.00117749
Iteration 3/1000 | Loss: 0.00018692
Iteration 4/1000 | Loss: 0.00012414
Iteration 5/1000 | Loss: 0.00011109
Iteration 6/1000 | Loss: 0.00013607
Iteration 7/1000 | Loss: 0.00010974
Iteration 8/1000 | Loss: 0.00019397
Iteration 9/1000 | Loss: 0.00018746
Iteration 10/1000 | Loss: 0.00010934
Iteration 11/1000 | Loss: 0.00013599
Iteration 12/1000 | Loss: 0.00036724
Iteration 13/1000 | Loss: 0.00015247
Iteration 14/1000 | Loss: 0.00014835
Iteration 15/1000 | Loss: 0.00101537
Iteration 16/1000 | Loss: 0.00123576
Iteration 17/1000 | Loss: 0.00027235
Iteration 18/1000 | Loss: 0.00030598
Iteration 19/1000 | Loss: 0.00033912
Iteration 20/1000 | Loss: 0.00012960
Iteration 21/1000 | Loss: 0.00018587
Iteration 22/1000 | Loss: 0.00054120
Iteration 23/1000 | Loss: 0.00120324
Iteration 24/1000 | Loss: 0.00224933
Iteration 25/1000 | Loss: 0.00268262
Iteration 26/1000 | Loss: 0.00217477
Iteration 27/1000 | Loss: 0.00015304
Iteration 28/1000 | Loss: 0.00009329
Iteration 29/1000 | Loss: 0.00008562
Iteration 30/1000 | Loss: 0.00019419
Iteration 31/1000 | Loss: 0.00018893
Iteration 32/1000 | Loss: 0.00007468
Iteration 33/1000 | Loss: 0.00004330
Iteration 34/1000 | Loss: 0.00029480
Iteration 35/1000 | Loss: 0.00005358
Iteration 36/1000 | Loss: 0.00013861
Iteration 37/1000 | Loss: 0.00014877
Iteration 38/1000 | Loss: 0.00017245
Iteration 39/1000 | Loss: 0.00013659
Iteration 40/1000 | Loss: 0.00012538
Iteration 41/1000 | Loss: 0.00035829
Iteration 42/1000 | Loss: 0.00008102
Iteration 43/1000 | Loss: 0.00004953
Iteration 44/1000 | Loss: 0.00003809
Iteration 45/1000 | Loss: 0.00014538
Iteration 46/1000 | Loss: 0.00003652
Iteration 47/1000 | Loss: 0.00004334
Iteration 48/1000 | Loss: 0.00004437
Iteration 49/1000 | Loss: 0.00003621
Iteration 50/1000 | Loss: 0.00002468
Iteration 51/1000 | Loss: 0.00003914
Iteration 52/1000 | Loss: 0.00003630
Iteration 53/1000 | Loss: 0.00002902
Iteration 54/1000 | Loss: 0.00002851
Iteration 55/1000 | Loss: 0.00002083
Iteration 56/1000 | Loss: 0.00002263
Iteration 57/1000 | Loss: 0.00003338
Iteration 58/1000 | Loss: 0.00002512
Iteration 59/1000 | Loss: 0.00002323
Iteration 60/1000 | Loss: 0.00006224
Iteration 61/1000 | Loss: 0.00002067
Iteration 62/1000 | Loss: 0.00003853
Iteration 63/1000 | Loss: 0.00003080
Iteration 64/1000 | Loss: 0.00003447
Iteration 65/1000 | Loss: 0.00003189
Iteration 66/1000 | Loss: 0.00003347
Iteration 67/1000 | Loss: 0.00002041
Iteration 68/1000 | Loss: 0.00003865
Iteration 69/1000 | Loss: 0.00002190
Iteration 70/1000 | Loss: 0.00002846
Iteration 71/1000 | Loss: 0.00002171
Iteration 72/1000 | Loss: 0.00002025
Iteration 73/1000 | Loss: 0.00005845
Iteration 74/1000 | Loss: 0.00002995
Iteration 75/1000 | Loss: 0.00002345
Iteration 76/1000 | Loss: 0.00002157
Iteration 77/1000 | Loss: 0.00002005
Iteration 78/1000 | Loss: 0.00002156
Iteration 79/1000 | Loss: 0.00001891
Iteration 80/1000 | Loss: 0.00001885
Iteration 81/1000 | Loss: 0.00001879
Iteration 82/1000 | Loss: 0.00001858
Iteration 83/1000 | Loss: 0.00001854
Iteration 84/1000 | Loss: 0.00001849
Iteration 85/1000 | Loss: 0.00001848
Iteration 86/1000 | Loss: 0.00001848
Iteration 87/1000 | Loss: 0.00001848
Iteration 88/1000 | Loss: 0.00001847
Iteration 89/1000 | Loss: 0.00001847
Iteration 90/1000 | Loss: 0.00001847
Iteration 91/1000 | Loss: 0.00001846
Iteration 92/1000 | Loss: 0.00001846
Iteration 93/1000 | Loss: 0.00001846
Iteration 94/1000 | Loss: 0.00001844
Iteration 95/1000 | Loss: 0.00001844
Iteration 96/1000 | Loss: 0.00001843
Iteration 97/1000 | Loss: 0.00001842
Iteration 98/1000 | Loss: 0.00001842
Iteration 99/1000 | Loss: 0.00001842
Iteration 100/1000 | Loss: 0.00001842
Iteration 101/1000 | Loss: 0.00001842
Iteration 102/1000 | Loss: 0.00001842
Iteration 103/1000 | Loss: 0.00001841
Iteration 104/1000 | Loss: 0.00001841
Iteration 105/1000 | Loss: 0.00001841
Iteration 106/1000 | Loss: 0.00001840
Iteration 107/1000 | Loss: 0.00001840
Iteration 108/1000 | Loss: 0.00001840
Iteration 109/1000 | Loss: 0.00001840
Iteration 110/1000 | Loss: 0.00001840
Iteration 111/1000 | Loss: 0.00001840
Iteration 112/1000 | Loss: 0.00001839
Iteration 113/1000 | Loss: 0.00001839
Iteration 114/1000 | Loss: 0.00001839
Iteration 115/1000 | Loss: 0.00001839
Iteration 116/1000 | Loss: 0.00001839
Iteration 117/1000 | Loss: 0.00001838
Iteration 118/1000 | Loss: 0.00001838
Iteration 119/1000 | Loss: 0.00001838
Iteration 120/1000 | Loss: 0.00001838
Iteration 121/1000 | Loss: 0.00001838
Iteration 122/1000 | Loss: 0.00001838
Iteration 123/1000 | Loss: 0.00001838
Iteration 124/1000 | Loss: 0.00001838
Iteration 125/1000 | Loss: 0.00001838
Iteration 126/1000 | Loss: 0.00001837
Iteration 127/1000 | Loss: 0.00001837
Iteration 128/1000 | Loss: 0.00001837
Iteration 129/1000 | Loss: 0.00001837
Iteration 130/1000 | Loss: 0.00001837
Iteration 131/1000 | Loss: 0.00001837
Iteration 132/1000 | Loss: 0.00001836
Iteration 133/1000 | Loss: 0.00001836
Iteration 134/1000 | Loss: 0.00001836
Iteration 135/1000 | Loss: 0.00001836
Iteration 136/1000 | Loss: 0.00001836
Iteration 137/1000 | Loss: 0.00001836
Iteration 138/1000 | Loss: 0.00001835
Iteration 139/1000 | Loss: 0.00001835
Iteration 140/1000 | Loss: 0.00001835
Iteration 141/1000 | Loss: 0.00001835
Iteration 142/1000 | Loss: 0.00001835
Iteration 143/1000 | Loss: 0.00001835
Iteration 144/1000 | Loss: 0.00001835
Iteration 145/1000 | Loss: 0.00001835
Iteration 146/1000 | Loss: 0.00001834
Iteration 147/1000 | Loss: 0.00001834
Iteration 148/1000 | Loss: 0.00001834
Iteration 149/1000 | Loss: 0.00001834
Iteration 150/1000 | Loss: 0.00001834
Iteration 151/1000 | Loss: 0.00001834
Iteration 152/1000 | Loss: 0.00001834
Iteration 153/1000 | Loss: 0.00001833
Iteration 154/1000 | Loss: 0.00001833
Iteration 155/1000 | Loss: 0.00001833
Iteration 156/1000 | Loss: 0.00001833
Iteration 157/1000 | Loss: 0.00001833
Iteration 158/1000 | Loss: 0.00001833
Iteration 159/1000 | Loss: 0.00001833
Iteration 160/1000 | Loss: 0.00001833
Iteration 161/1000 | Loss: 0.00001833
Iteration 162/1000 | Loss: 0.00001833
Iteration 163/1000 | Loss: 0.00001833
Iteration 164/1000 | Loss: 0.00001832
Iteration 165/1000 | Loss: 0.00001832
Iteration 166/1000 | Loss: 0.00001832
Iteration 167/1000 | Loss: 0.00001832
Iteration 168/1000 | Loss: 0.00001832
Iteration 169/1000 | Loss: 0.00001832
Iteration 170/1000 | Loss: 0.00001831
Iteration 171/1000 | Loss: 0.00001831
Iteration 172/1000 | Loss: 0.00001831
Iteration 173/1000 | Loss: 0.00001831
Iteration 174/1000 | Loss: 0.00001831
Iteration 175/1000 | Loss: 0.00001831
Iteration 176/1000 | Loss: 0.00001831
Iteration 177/1000 | Loss: 0.00001831
Iteration 178/1000 | Loss: 0.00001831
Iteration 179/1000 | Loss: 0.00001831
Iteration 180/1000 | Loss: 0.00001830
Iteration 181/1000 | Loss: 0.00001830
Iteration 182/1000 | Loss: 0.00001830
Iteration 183/1000 | Loss: 0.00001830
Iteration 184/1000 | Loss: 0.00001829
Iteration 185/1000 | Loss: 0.00001829
Iteration 186/1000 | Loss: 0.00001829
Iteration 187/1000 | Loss: 0.00001829
Iteration 188/1000 | Loss: 0.00001829
Iteration 189/1000 | Loss: 0.00001829
Iteration 190/1000 | Loss: 0.00001829
Iteration 191/1000 | Loss: 0.00001829
Iteration 192/1000 | Loss: 0.00001829
Iteration 193/1000 | Loss: 0.00001828
Iteration 194/1000 | Loss: 0.00001828
Iteration 195/1000 | Loss: 0.00001828
Iteration 196/1000 | Loss: 0.00001828
Iteration 197/1000 | Loss: 0.00001828
Iteration 198/1000 | Loss: 0.00001827
Iteration 199/1000 | Loss: 0.00001827
Iteration 200/1000 | Loss: 0.00001827
Iteration 201/1000 | Loss: 0.00001827
Iteration 202/1000 | Loss: 0.00001827
Iteration 203/1000 | Loss: 0.00001827
Iteration 204/1000 | Loss: 0.00001827
Iteration 205/1000 | Loss: 0.00001827
Iteration 206/1000 | Loss: 0.00001827
Iteration 207/1000 | Loss: 0.00001827
Iteration 208/1000 | Loss: 0.00001827
Iteration 209/1000 | Loss: 0.00001826
Iteration 210/1000 | Loss: 0.00001826
Iteration 211/1000 | Loss: 0.00001826
Iteration 212/1000 | Loss: 0.00001826
Iteration 213/1000 | Loss: 0.00001825
Iteration 214/1000 | Loss: 0.00001825
Iteration 215/1000 | Loss: 0.00001825
Iteration 216/1000 | Loss: 0.00001825
Iteration 217/1000 | Loss: 0.00001825
Iteration 218/1000 | Loss: 0.00001824
Iteration 219/1000 | Loss: 0.00001824
Iteration 220/1000 | Loss: 0.00001824
Iteration 221/1000 | Loss: 0.00001824
Iteration 222/1000 | Loss: 0.00001824
Iteration 223/1000 | Loss: 0.00001824
Iteration 224/1000 | Loss: 0.00001824
Iteration 225/1000 | Loss: 0.00001824
Iteration 226/1000 | Loss: 0.00001824
Iteration 227/1000 | Loss: 0.00001824
Iteration 228/1000 | Loss: 0.00001824
Iteration 229/1000 | Loss: 0.00001824
Iteration 230/1000 | Loss: 0.00001824
Iteration 231/1000 | Loss: 0.00001823
Iteration 232/1000 | Loss: 0.00001823
Iteration 233/1000 | Loss: 0.00001823
Iteration 234/1000 | Loss: 0.00001823
Iteration 235/1000 | Loss: 0.00001823
Iteration 236/1000 | Loss: 0.00001822
Iteration 237/1000 | Loss: 0.00001822
Iteration 238/1000 | Loss: 0.00001822
Iteration 239/1000 | Loss: 0.00001822
Iteration 240/1000 | Loss: 0.00001822
Iteration 241/1000 | Loss: 0.00001822
Iteration 242/1000 | Loss: 0.00001822
Iteration 243/1000 | Loss: 0.00001822
Iteration 244/1000 | Loss: 0.00001821
Iteration 245/1000 | Loss: 0.00001821
Iteration 246/1000 | Loss: 0.00001821
Iteration 247/1000 | Loss: 0.00001821
Iteration 248/1000 | Loss: 0.00001821
Iteration 249/1000 | Loss: 0.00001821
Iteration 250/1000 | Loss: 0.00001821
Iteration 251/1000 | Loss: 0.00001821
Iteration 252/1000 | Loss: 0.00001820
Iteration 253/1000 | Loss: 0.00001820
Iteration 254/1000 | Loss: 0.00001820
Iteration 255/1000 | Loss: 0.00001820
Iteration 256/1000 | Loss: 0.00001820
Iteration 257/1000 | Loss: 0.00001819
Iteration 258/1000 | Loss: 0.00001819
Iteration 259/1000 | Loss: 0.00001819
Iteration 260/1000 | Loss: 0.00001819
Iteration 261/1000 | Loss: 0.00001819
Iteration 262/1000 | Loss: 0.00001819
Iteration 263/1000 | Loss: 0.00001819
Iteration 264/1000 | Loss: 0.00001819
Iteration 265/1000 | Loss: 0.00001819
Iteration 266/1000 | Loss: 0.00001819
Iteration 267/1000 | Loss: 0.00001818
Iteration 268/1000 | Loss: 0.00001818
Iteration 269/1000 | Loss: 0.00001817
Iteration 270/1000 | Loss: 0.00001817
Iteration 271/1000 | Loss: 0.00001817
Iteration 272/1000 | Loss: 0.00001817
Iteration 273/1000 | Loss: 0.00001817
Iteration 274/1000 | Loss: 0.00001817
Iteration 275/1000 | Loss: 0.00001817
Iteration 276/1000 | Loss: 0.00001816
Iteration 277/1000 | Loss: 0.00001816
Iteration 278/1000 | Loss: 0.00001816
Iteration 279/1000 | Loss: 0.00001816
Iteration 280/1000 | Loss: 0.00001815
Iteration 281/1000 | Loss: 0.00001815
Iteration 282/1000 | Loss: 0.00001815
Iteration 283/1000 | Loss: 0.00001815
Iteration 284/1000 | Loss: 0.00001814
Iteration 285/1000 | Loss: 0.00001814
Iteration 286/1000 | Loss: 0.00001814
Iteration 287/1000 | Loss: 0.00001813
Iteration 288/1000 | Loss: 0.00004859
Iteration 289/1000 | Loss: 0.00006597
Iteration 290/1000 | Loss: 0.00002739
Iteration 291/1000 | Loss: 0.00002685
Iteration 292/1000 | Loss: 0.00001820
Iteration 293/1000 | Loss: 0.00001808
Iteration 294/1000 | Loss: 0.00001807
Iteration 295/1000 | Loss: 0.00001807
Iteration 296/1000 | Loss: 0.00001806
Iteration 297/1000 | Loss: 0.00001806
Iteration 298/1000 | Loss: 0.00001806
Iteration 299/1000 | Loss: 0.00001806
Iteration 300/1000 | Loss: 0.00001806
Iteration 301/1000 | Loss: 0.00001806
Iteration 302/1000 | Loss: 0.00001806
Iteration 303/1000 | Loss: 0.00001806
Iteration 304/1000 | Loss: 0.00001805
Iteration 305/1000 | Loss: 0.00001805
Iteration 306/1000 | Loss: 0.00001804
Iteration 307/1000 | Loss: 0.00001804
Iteration 308/1000 | Loss: 0.00001804
Iteration 309/1000 | Loss: 0.00001804
Iteration 310/1000 | Loss: 0.00001803
Iteration 311/1000 | Loss: 0.00001803
Iteration 312/1000 | Loss: 0.00001802
Iteration 313/1000 | Loss: 0.00001802
Iteration 314/1000 | Loss: 0.00001802
Iteration 315/1000 | Loss: 0.00001802
Iteration 316/1000 | Loss: 0.00001801
Iteration 317/1000 | Loss: 0.00001801
Iteration 318/1000 | Loss: 0.00001800
Iteration 319/1000 | Loss: 0.00001800
Iteration 320/1000 | Loss: 0.00001800
Iteration 321/1000 | Loss: 0.00001800
Iteration 322/1000 | Loss: 0.00001800
Iteration 323/1000 | Loss: 0.00001799
Iteration 324/1000 | Loss: 0.00001799
Iteration 325/1000 | Loss: 0.00001799
Iteration 326/1000 | Loss: 0.00001799
Iteration 327/1000 | Loss: 0.00001798
Iteration 328/1000 | Loss: 0.00001798
Iteration 329/1000 | Loss: 0.00001798
Iteration 330/1000 | Loss: 0.00001797
Iteration 331/1000 | Loss: 0.00001797
Iteration 332/1000 | Loss: 0.00001796
Iteration 333/1000 | Loss: 0.00001796
Iteration 334/1000 | Loss: 0.00001796
Iteration 335/1000 | Loss: 0.00001796
Iteration 336/1000 | Loss: 0.00001795
Iteration 337/1000 | Loss: 0.00001795
Iteration 338/1000 | Loss: 0.00003465
Iteration 339/1000 | Loss: 0.00004475
Iteration 340/1000 | Loss: 0.00002030
Iteration 341/1000 | Loss: 0.00003678
Iteration 342/1000 | Loss: 0.00002423
Iteration 343/1000 | Loss: 0.00004110
Iteration 344/1000 | Loss: 0.00002248
Iteration 345/1000 | Loss: 0.00001838
Iteration 346/1000 | Loss: 0.00003340
Iteration 347/1000 | Loss: 0.00004483
Iteration 348/1000 | Loss: 0.00002224
Iteration 349/1000 | Loss: 0.00002726
Iteration 350/1000 | Loss: 0.00001910
Iteration 351/1000 | Loss: 0.00001828
Iteration 352/1000 | Loss: 0.00003940
Iteration 353/1000 | Loss: 0.00003714
Iteration 354/1000 | Loss: 0.00002848
Iteration 355/1000 | Loss: 0.00003218
Iteration 356/1000 | Loss: 0.00002271
Iteration 357/1000 | Loss: 0.00001847
Iteration 358/1000 | Loss: 0.00001989
Iteration 359/1000 | Loss: 0.00001805
Iteration 360/1000 | Loss: 0.00001804
Iteration 361/1000 | Loss: 0.00001804
Iteration 362/1000 | Loss: 0.00001800
Iteration 363/1000 | Loss: 0.00001797
Iteration 364/1000 | Loss: 0.00003251
Iteration 365/1000 | Loss: 0.00002932
Iteration 366/1000 | Loss: 0.00002143
Iteration 367/1000 | Loss: 0.00001799
Iteration 368/1000 | Loss: 0.00001799
Iteration 369/1000 | Loss: 0.00001799
Iteration 370/1000 | Loss: 0.00001798
Iteration 371/1000 | Loss: 0.00001798
Iteration 372/1000 | Loss: 0.00001797
Iteration 373/1000 | Loss: 0.00003328
Iteration 374/1000 | Loss: 0.00002603
Iteration 375/1000 | Loss: 0.00002009
Iteration 376/1000 | Loss: 0.00003397
Iteration 377/1000 | Loss: 0.00002009
Iteration 378/1000 | Loss: 0.00001829
Iteration 379/1000 | Loss: 0.00003647
Iteration 380/1000 | Loss: 0.00004063
Iteration 381/1000 | Loss: 0.00002021
Iteration 382/1000 | Loss: 0.00003066
Iteration 383/1000 | Loss: 0.00003487
Iteration 384/1000 | Loss: 0.00001979
Iteration 385/1000 | Loss: 0.00003120
Iteration 386/1000 | Loss: 0.00002177
Iteration 387/1000 | Loss: 0.00002981
Iteration 388/1000 | Loss: 0.00002206
Iteration 389/1000 | Loss: 0.00002007
Iteration 390/1000 | Loss: 0.00004908
Iteration 391/1000 | Loss: 0.00001864
Iteration 392/1000 | Loss: 0.00002318
Iteration 393/1000 | Loss: 0.00003106
Iteration 394/1000 | Loss: 0.00002052
Iteration 395/1000 | Loss: 0.00001987
Iteration 396/1000 | Loss: 0.00003105
Iteration 397/1000 | Loss: 0.00002271
Iteration 398/1000 | Loss: 0.00001813
Iteration 399/1000 | Loss: 0.00003592
Iteration 400/1000 | Loss: 0.00002180
Iteration 401/1000 | Loss: 0.00003713
Iteration 402/1000 | Loss: 0.00003299
Iteration 403/1000 | Loss: 0.00002647
Iteration 404/1000 | Loss: 0.00001869
Iteration 405/1000 | Loss: 0.00003480
Iteration 406/1000 | Loss: 0.00002827
Iteration 407/1000 | Loss: 0.00003013
Iteration 408/1000 | Loss: 0.00003291
Iteration 409/1000 | Loss: 0.00003002
Iteration 410/1000 | Loss: 0.00003331
Iteration 411/1000 | Loss: 0.00002108
Iteration 412/1000 | Loss: 0.00003001
Iteration 413/1000 | Loss: 0.00002035
Iteration 414/1000 | Loss: 0.00003233
Iteration 415/1000 | Loss: 0.00002461
Iteration 416/1000 | Loss: 0.00003038
Iteration 417/1000 | Loss: 0.00002087
Iteration 418/1000 | Loss: 0.00002894
Iteration 419/1000 | Loss: 0.00002221
Iteration 420/1000 | Loss: 0.00002826
Iteration 421/1000 | Loss: 0.00002312
Iteration 422/1000 | Loss: 0.00002684
Iteration 423/1000 | Loss: 0.00001835
Iteration 424/1000 | Loss: 0.00001808
Iteration 425/1000 | Loss: 0.00001798
Iteration 426/1000 | Loss: 0.00001798
Iteration 427/1000 | Loss: 0.00001797
Iteration 428/1000 | Loss: 0.00001795
Iteration 429/1000 | Loss: 0.00001794
Iteration 430/1000 | Loss: 0.00001793
Iteration 431/1000 | Loss: 0.00001793
Iteration 432/1000 | Loss: 0.00001793
Iteration 433/1000 | Loss: 0.00001793
Iteration 434/1000 | Loss: 0.00001793
Iteration 435/1000 | Loss: 0.00001793
Iteration 436/1000 | Loss: 0.00001792
Iteration 437/1000 | Loss: 0.00001792
Iteration 438/1000 | Loss: 0.00001792
Iteration 439/1000 | Loss: 0.00001792
Iteration 440/1000 | Loss: 0.00001792
Iteration 441/1000 | Loss: 0.00001792
Iteration 442/1000 | Loss: 0.00001792
Iteration 443/1000 | Loss: 0.00001792
Iteration 444/1000 | Loss: 0.00001791
Iteration 445/1000 | Loss: 0.00001791
Iteration 446/1000 | Loss: 0.00001791
Iteration 447/1000 | Loss: 0.00001791
Iteration 448/1000 | Loss: 0.00001791
Iteration 449/1000 | Loss: 0.00001791
Iteration 450/1000 | Loss: 0.00001790
Iteration 451/1000 | Loss: 0.00001790
Iteration 452/1000 | Loss: 0.00001790
Iteration 453/1000 | Loss: 0.00001789
Iteration 454/1000 | Loss: 0.00001789
Iteration 455/1000 | Loss: 0.00001789
Iteration 456/1000 | Loss: 0.00001789
Iteration 457/1000 | Loss: 0.00001788
Iteration 458/1000 | Loss: 0.00001788
Iteration 459/1000 | Loss: 0.00001788
Iteration 460/1000 | Loss: 0.00001788
Iteration 461/1000 | Loss: 0.00001788
Iteration 462/1000 | Loss: 0.00001787
Iteration 463/1000 | Loss: 0.00001787
Iteration 464/1000 | Loss: 0.00001786
Iteration 465/1000 | Loss: 0.00001786
Iteration 466/1000 | Loss: 0.00001786
Iteration 467/1000 | Loss: 0.00001786
Iteration 468/1000 | Loss: 0.00001786
Iteration 469/1000 | Loss: 0.00001786
Iteration 470/1000 | Loss: 0.00001785
Iteration 471/1000 | Loss: 0.00001785
Iteration 472/1000 | Loss: 0.00001785
Iteration 473/1000 | Loss: 0.00001785
Iteration 474/1000 | Loss: 0.00001785
Iteration 475/1000 | Loss: 0.00001785
Iteration 476/1000 | Loss: 0.00001785
Iteration 477/1000 | Loss: 0.00001785
Iteration 478/1000 | Loss: 0.00001785
Iteration 479/1000 | Loss: 0.00001785
Iteration 480/1000 | Loss: 0.00001785
Iteration 481/1000 | Loss: 0.00001784
Iteration 482/1000 | Loss: 0.00001784
Iteration 483/1000 | Loss: 0.00001784
Iteration 484/1000 | Loss: 0.00001784
Iteration 485/1000 | Loss: 0.00001784
Iteration 486/1000 | Loss: 0.00001784
Iteration 487/1000 | Loss: 0.00001784
Iteration 488/1000 | Loss: 0.00001783
Iteration 489/1000 | Loss: 0.00001783
Iteration 490/1000 | Loss: 0.00001783
Iteration 491/1000 | Loss: 0.00001783
Iteration 492/1000 | Loss: 0.00001783
Iteration 493/1000 | Loss: 0.00001783
Iteration 494/1000 | Loss: 0.00001783
Iteration 495/1000 | Loss: 0.00001783
Iteration 496/1000 | Loss: 0.00001782
Iteration 497/1000 | Loss: 0.00001782
Iteration 498/1000 | Loss: 0.00001782
Iteration 499/1000 | Loss: 0.00001782
Iteration 500/1000 | Loss: 0.00001782
Iteration 501/1000 | Loss: 0.00001782
Iteration 502/1000 | Loss: 0.00001782
Iteration 503/1000 | Loss: 0.00001782
Iteration 504/1000 | Loss: 0.00001782
Iteration 505/1000 | Loss: 0.00001782
Iteration 506/1000 | Loss: 0.00001782
Iteration 507/1000 | Loss: 0.00001782
Iteration 508/1000 | Loss: 0.00001782
Iteration 509/1000 | Loss: 0.00001782
Iteration 510/1000 | Loss: 0.00001782
Iteration 511/1000 | Loss: 0.00001782
Iteration 512/1000 | Loss: 0.00001782
Iteration 513/1000 | Loss: 0.00001781
Iteration 514/1000 | Loss: 0.00001781
Iteration 515/1000 | Loss: 0.00001781
Iteration 516/1000 | Loss: 0.00001781
Iteration 517/1000 | Loss: 0.00001781
Iteration 518/1000 | Loss: 0.00001781
Iteration 519/1000 | Loss: 0.00001781
Iteration 520/1000 | Loss: 0.00001781
Iteration 521/1000 | Loss: 0.00001781
Iteration 522/1000 | Loss: 0.00001781
Iteration 523/1000 | Loss: 0.00001781
Iteration 524/1000 | Loss: 0.00001781
Iteration 525/1000 | Loss: 0.00001781
Iteration 526/1000 | Loss: 0.00001781
Iteration 527/1000 | Loss: 0.00001781
Iteration 528/1000 | Loss: 0.00001781
Iteration 529/1000 | Loss: 0.00001781
Iteration 530/1000 | Loss: 0.00001781
Iteration 531/1000 | Loss: 0.00001781
Iteration 532/1000 | Loss: 0.00001781
Iteration 533/1000 | Loss: 0.00001781
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 533. Stopping optimization.
Last 5 losses: [1.7813363228924572e-05, 1.7813363228924572e-05, 1.7813363228924572e-05, 1.7813363228924572e-05, 1.7813363228924572e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7813363228924572e-05

Optimization complete. Final v2v error: 3.5998260974884033 mm

Highest mean error: 5.615850448608398 mm for frame 102

Lowest mean error: 3.2279558181762695 mm for frame 231

Saving results

Total time: 335.2165069580078
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01038764
Iteration 2/25 | Loss: 0.01038764
Iteration 3/25 | Loss: 0.01038764
Iteration 4/25 | Loss: 0.00271603
Iteration 5/25 | Loss: 0.00199371
Iteration 6/25 | Loss: 0.00168164
Iteration 7/25 | Loss: 0.00161298
Iteration 8/25 | Loss: 0.00152464
Iteration 9/25 | Loss: 0.00144464
Iteration 10/25 | Loss: 0.00134690
Iteration 11/25 | Loss: 0.00133817
Iteration 12/25 | Loss: 0.00127144
Iteration 13/25 | Loss: 0.00125205
Iteration 14/25 | Loss: 0.00124897
Iteration 15/25 | Loss: 0.00123704
Iteration 16/25 | Loss: 0.00122465
Iteration 17/25 | Loss: 0.00122287
Iteration 18/25 | Loss: 0.00122584
Iteration 19/25 | Loss: 0.00122343
Iteration 20/25 | Loss: 0.00122480
Iteration 21/25 | Loss: 0.00122084
Iteration 22/25 | Loss: 0.00121924
Iteration 23/25 | Loss: 0.00121878
Iteration 24/25 | Loss: 0.00121873
Iteration 25/25 | Loss: 0.00121872

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.79031658
Iteration 2/25 | Loss: 0.00099048
Iteration 3/25 | Loss: 0.00099047
Iteration 4/25 | Loss: 0.00081091
Iteration 5/25 | Loss: 0.00081091
Iteration 6/25 | Loss: 0.00081091
Iteration 7/25 | Loss: 0.00081091
Iteration 8/25 | Loss: 0.00081091
Iteration 9/25 | Loss: 0.00081091
Iteration 10/25 | Loss: 0.00081091
Iteration 11/25 | Loss: 0.00081091
Iteration 12/25 | Loss: 0.00081091
Iteration 13/25 | Loss: 0.00081090
Iteration 14/25 | Loss: 0.00081090
Iteration 15/25 | Loss: 0.00081090
Iteration 16/25 | Loss: 0.00081090
Iteration 17/25 | Loss: 0.00081090
Iteration 18/25 | Loss: 0.00081090
Iteration 19/25 | Loss: 0.00081090
Iteration 20/25 | Loss: 0.00081090
Iteration 21/25 | Loss: 0.00081090
Iteration 22/25 | Loss: 0.00081090
Iteration 23/25 | Loss: 0.00081090
Iteration 24/25 | Loss: 0.00081090
Iteration 25/25 | Loss: 0.00081090

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081090
Iteration 2/1000 | Loss: 0.00016507
Iteration 3/1000 | Loss: 0.00007024
Iteration 4/1000 | Loss: 0.00016789
Iteration 5/1000 | Loss: 0.00002819
Iteration 6/1000 | Loss: 0.00002660
Iteration 7/1000 | Loss: 0.00004482
Iteration 8/1000 | Loss: 0.00005310
Iteration 9/1000 | Loss: 0.00002468
Iteration 10/1000 | Loss: 0.00002406
Iteration 11/1000 | Loss: 0.00002350
Iteration 12/1000 | Loss: 0.00179880
Iteration 13/1000 | Loss: 0.00021945
Iteration 14/1000 | Loss: 0.00002450
Iteration 15/1000 | Loss: 0.00007144
Iteration 16/1000 | Loss: 0.00019296
Iteration 17/1000 | Loss: 0.00009712
Iteration 18/1000 | Loss: 0.00005244
Iteration 19/1000 | Loss: 0.00001912
Iteration 20/1000 | Loss: 0.00009634
Iteration 21/1000 | Loss: 0.00003979
Iteration 22/1000 | Loss: 0.00005781
Iteration 23/1000 | Loss: 0.00001931
Iteration 24/1000 | Loss: 0.00001794
Iteration 25/1000 | Loss: 0.00002037
Iteration 26/1000 | Loss: 0.00005256
Iteration 27/1000 | Loss: 0.00004256
Iteration 28/1000 | Loss: 0.00001540
Iteration 29/1000 | Loss: 0.00011747
Iteration 30/1000 | Loss: 0.00004709
Iteration 31/1000 | Loss: 0.00004926
Iteration 32/1000 | Loss: 0.00001483
Iteration 33/1000 | Loss: 0.00004128
Iteration 34/1000 | Loss: 0.00001456
Iteration 35/1000 | Loss: 0.00002551
Iteration 36/1000 | Loss: 0.00001440
Iteration 37/1000 | Loss: 0.00002035
Iteration 38/1000 | Loss: 0.00001433
Iteration 39/1000 | Loss: 0.00001433
Iteration 40/1000 | Loss: 0.00001433
Iteration 41/1000 | Loss: 0.00001433
Iteration 42/1000 | Loss: 0.00001433
Iteration 43/1000 | Loss: 0.00001433
Iteration 44/1000 | Loss: 0.00001433
Iteration 45/1000 | Loss: 0.00001433
Iteration 46/1000 | Loss: 0.00001433
Iteration 47/1000 | Loss: 0.00001433
Iteration 48/1000 | Loss: 0.00001433
Iteration 49/1000 | Loss: 0.00001432
Iteration 50/1000 | Loss: 0.00001425
Iteration 51/1000 | Loss: 0.00001424
Iteration 52/1000 | Loss: 0.00001424
Iteration 53/1000 | Loss: 0.00001424
Iteration 54/1000 | Loss: 0.00001423
Iteration 55/1000 | Loss: 0.00001423
Iteration 56/1000 | Loss: 0.00001423
Iteration 57/1000 | Loss: 0.00001423
Iteration 58/1000 | Loss: 0.00001423
Iteration 59/1000 | Loss: 0.00001423
Iteration 60/1000 | Loss: 0.00001423
Iteration 61/1000 | Loss: 0.00001423
Iteration 62/1000 | Loss: 0.00001423
Iteration 63/1000 | Loss: 0.00001423
Iteration 64/1000 | Loss: 0.00001423
Iteration 65/1000 | Loss: 0.00001422
Iteration 66/1000 | Loss: 0.00001422
Iteration 67/1000 | Loss: 0.00001422
Iteration 68/1000 | Loss: 0.00001421
Iteration 69/1000 | Loss: 0.00001421
Iteration 70/1000 | Loss: 0.00001421
Iteration 71/1000 | Loss: 0.00001421
Iteration 72/1000 | Loss: 0.00001421
Iteration 73/1000 | Loss: 0.00001420
Iteration 74/1000 | Loss: 0.00001420
Iteration 75/1000 | Loss: 0.00001420
Iteration 76/1000 | Loss: 0.00001420
Iteration 77/1000 | Loss: 0.00001420
Iteration 78/1000 | Loss: 0.00001420
Iteration 79/1000 | Loss: 0.00001420
Iteration 80/1000 | Loss: 0.00001418
Iteration 81/1000 | Loss: 0.00001418
Iteration 82/1000 | Loss: 0.00001418
Iteration 83/1000 | Loss: 0.00001417
Iteration 84/1000 | Loss: 0.00001417
Iteration 85/1000 | Loss: 0.00001417
Iteration 86/1000 | Loss: 0.00001417
Iteration 87/1000 | Loss: 0.00001417
Iteration 88/1000 | Loss: 0.00001417
Iteration 89/1000 | Loss: 0.00001417
Iteration 90/1000 | Loss: 0.00001417
Iteration 91/1000 | Loss: 0.00001416
Iteration 92/1000 | Loss: 0.00001416
Iteration 93/1000 | Loss: 0.00001416
Iteration 94/1000 | Loss: 0.00001416
Iteration 95/1000 | Loss: 0.00001416
Iteration 96/1000 | Loss: 0.00001415
Iteration 97/1000 | Loss: 0.00001415
Iteration 98/1000 | Loss: 0.00001415
Iteration 99/1000 | Loss: 0.00001415
Iteration 100/1000 | Loss: 0.00001415
Iteration 101/1000 | Loss: 0.00001415
Iteration 102/1000 | Loss: 0.00001415
Iteration 103/1000 | Loss: 0.00001415
Iteration 104/1000 | Loss: 0.00001415
Iteration 105/1000 | Loss: 0.00001415
Iteration 106/1000 | Loss: 0.00001415
Iteration 107/1000 | Loss: 0.00001415
Iteration 108/1000 | Loss: 0.00001415
Iteration 109/1000 | Loss: 0.00001415
Iteration 110/1000 | Loss: 0.00001415
Iteration 111/1000 | Loss: 0.00001414
Iteration 112/1000 | Loss: 0.00002980
Iteration 113/1000 | Loss: 0.00004112
Iteration 114/1000 | Loss: 0.00001417
Iteration 115/1000 | Loss: 0.00001414
Iteration 116/1000 | Loss: 0.00001413
Iteration 117/1000 | Loss: 0.00001412
Iteration 118/1000 | Loss: 0.00001412
Iteration 119/1000 | Loss: 0.00001412
Iteration 120/1000 | Loss: 0.00001412
Iteration 121/1000 | Loss: 0.00001412
Iteration 122/1000 | Loss: 0.00001412
Iteration 123/1000 | Loss: 0.00001412
Iteration 124/1000 | Loss: 0.00001412
Iteration 125/1000 | Loss: 0.00001412
Iteration 126/1000 | Loss: 0.00001412
Iteration 127/1000 | Loss: 0.00001412
Iteration 128/1000 | Loss: 0.00001412
Iteration 129/1000 | Loss: 0.00001412
Iteration 130/1000 | Loss: 0.00001412
Iteration 131/1000 | Loss: 0.00001411
Iteration 132/1000 | Loss: 0.00001411
Iteration 133/1000 | Loss: 0.00001411
Iteration 134/1000 | Loss: 0.00001411
Iteration 135/1000 | Loss: 0.00001411
Iteration 136/1000 | Loss: 0.00001411
Iteration 137/1000 | Loss: 0.00001411
Iteration 138/1000 | Loss: 0.00001411
Iteration 139/1000 | Loss: 0.00001411
Iteration 140/1000 | Loss: 0.00001411
Iteration 141/1000 | Loss: 0.00001411
Iteration 142/1000 | Loss: 0.00001411
Iteration 143/1000 | Loss: 0.00001411
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.411498487868812e-05, 1.411498487868812e-05, 1.411498487868812e-05, 1.411498487868812e-05, 1.411498487868812e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.411498487868812e-05

Optimization complete. Final v2v error: 3.207484245300293 mm

Highest mean error: 3.7317166328430176 mm for frame 90

Lowest mean error: 2.959362268447876 mm for frame 119

Saving results

Total time: 105.8940896987915
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_044/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_044/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00551910
Iteration 2/25 | Loss: 0.00146981
Iteration 3/25 | Loss: 0.00131327
Iteration 4/25 | Loss: 0.00129569
Iteration 5/25 | Loss: 0.00129015
Iteration 6/25 | Loss: 0.00128930
Iteration 7/25 | Loss: 0.00128930
Iteration 8/25 | Loss: 0.00128930
Iteration 9/25 | Loss: 0.00128930
Iteration 10/25 | Loss: 0.00128930
Iteration 11/25 | Loss: 0.00128930
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012892960803583264, 0.0012892960803583264, 0.0012892960803583264, 0.0012892960803583264, 0.0012892960803583264]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012892960803583264

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34893930
Iteration 2/25 | Loss: 0.00090762
Iteration 3/25 | Loss: 0.00090761
Iteration 4/25 | Loss: 0.00090761
Iteration 5/25 | Loss: 0.00090760
Iteration 6/25 | Loss: 0.00090760
Iteration 7/25 | Loss: 0.00090760
Iteration 8/25 | Loss: 0.00090760
Iteration 9/25 | Loss: 0.00090760
Iteration 10/25 | Loss: 0.00090760
Iteration 11/25 | Loss: 0.00090760
Iteration 12/25 | Loss: 0.00090760
Iteration 13/25 | Loss: 0.00090760
Iteration 14/25 | Loss: 0.00090760
Iteration 15/25 | Loss: 0.00090760
Iteration 16/25 | Loss: 0.00090760
Iteration 17/25 | Loss: 0.00090760
Iteration 18/25 | Loss: 0.00090760
Iteration 19/25 | Loss: 0.00090760
Iteration 20/25 | Loss: 0.00090760
Iteration 21/25 | Loss: 0.00090760
Iteration 22/25 | Loss: 0.00090760
Iteration 23/25 | Loss: 0.00090760
Iteration 24/25 | Loss: 0.00090760
Iteration 25/25 | Loss: 0.00090760

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090760
Iteration 2/1000 | Loss: 0.00004710
Iteration 3/1000 | Loss: 0.00002705
Iteration 4/1000 | Loss: 0.00002387
Iteration 5/1000 | Loss: 0.00002263
Iteration 6/1000 | Loss: 0.00002152
Iteration 7/1000 | Loss: 0.00002085
Iteration 8/1000 | Loss: 0.00002032
Iteration 9/1000 | Loss: 0.00001994
Iteration 10/1000 | Loss: 0.00001955
Iteration 11/1000 | Loss: 0.00001930
Iteration 12/1000 | Loss: 0.00001912
Iteration 13/1000 | Loss: 0.00001900
Iteration 14/1000 | Loss: 0.00001899
Iteration 15/1000 | Loss: 0.00001893
Iteration 16/1000 | Loss: 0.00001890
Iteration 17/1000 | Loss: 0.00001889
Iteration 18/1000 | Loss: 0.00001889
Iteration 19/1000 | Loss: 0.00001886
Iteration 20/1000 | Loss: 0.00001884
Iteration 21/1000 | Loss: 0.00001880
Iteration 22/1000 | Loss: 0.00001880
Iteration 23/1000 | Loss: 0.00001877
Iteration 24/1000 | Loss: 0.00001876
Iteration 25/1000 | Loss: 0.00001876
Iteration 26/1000 | Loss: 0.00001876
Iteration 27/1000 | Loss: 0.00001874
Iteration 28/1000 | Loss: 0.00001873
Iteration 29/1000 | Loss: 0.00001871
Iteration 30/1000 | Loss: 0.00001871
Iteration 31/1000 | Loss: 0.00001871
Iteration 32/1000 | Loss: 0.00001871
Iteration 33/1000 | Loss: 0.00001871
Iteration 34/1000 | Loss: 0.00001869
Iteration 35/1000 | Loss: 0.00001869
Iteration 36/1000 | Loss: 0.00001868
Iteration 37/1000 | Loss: 0.00001867
Iteration 38/1000 | Loss: 0.00001867
Iteration 39/1000 | Loss: 0.00001867
Iteration 40/1000 | Loss: 0.00001866
Iteration 41/1000 | Loss: 0.00001866
Iteration 42/1000 | Loss: 0.00001865
Iteration 43/1000 | Loss: 0.00001865
Iteration 44/1000 | Loss: 0.00001865
Iteration 45/1000 | Loss: 0.00001865
Iteration 46/1000 | Loss: 0.00001865
Iteration 47/1000 | Loss: 0.00001865
Iteration 48/1000 | Loss: 0.00001864
Iteration 49/1000 | Loss: 0.00001864
Iteration 50/1000 | Loss: 0.00001864
Iteration 51/1000 | Loss: 0.00001864
Iteration 52/1000 | Loss: 0.00001863
Iteration 53/1000 | Loss: 0.00001863
Iteration 54/1000 | Loss: 0.00001863
Iteration 55/1000 | Loss: 0.00001862
Iteration 56/1000 | Loss: 0.00001862
Iteration 57/1000 | Loss: 0.00001862
Iteration 58/1000 | Loss: 0.00001862
Iteration 59/1000 | Loss: 0.00001862
Iteration 60/1000 | Loss: 0.00001861
Iteration 61/1000 | Loss: 0.00001861
Iteration 62/1000 | Loss: 0.00001860
Iteration 63/1000 | Loss: 0.00001860
Iteration 64/1000 | Loss: 0.00001860
Iteration 65/1000 | Loss: 0.00001860
Iteration 66/1000 | Loss: 0.00001859
Iteration 67/1000 | Loss: 0.00001859
Iteration 68/1000 | Loss: 0.00001858
Iteration 69/1000 | Loss: 0.00001858
Iteration 70/1000 | Loss: 0.00001858
Iteration 71/1000 | Loss: 0.00001857
Iteration 72/1000 | Loss: 0.00001857
Iteration 73/1000 | Loss: 0.00001857
Iteration 74/1000 | Loss: 0.00001856
Iteration 75/1000 | Loss: 0.00001856
Iteration 76/1000 | Loss: 0.00001856
Iteration 77/1000 | Loss: 0.00001855
Iteration 78/1000 | Loss: 0.00001855
Iteration 79/1000 | Loss: 0.00001855
Iteration 80/1000 | Loss: 0.00001854
Iteration 81/1000 | Loss: 0.00001854
Iteration 82/1000 | Loss: 0.00001854
Iteration 83/1000 | Loss: 0.00001854
Iteration 84/1000 | Loss: 0.00001853
Iteration 85/1000 | Loss: 0.00001853
Iteration 86/1000 | Loss: 0.00001853
Iteration 87/1000 | Loss: 0.00001853
Iteration 88/1000 | Loss: 0.00001852
Iteration 89/1000 | Loss: 0.00001852
Iteration 90/1000 | Loss: 0.00001852
Iteration 91/1000 | Loss: 0.00001851
Iteration 92/1000 | Loss: 0.00001851
Iteration 93/1000 | Loss: 0.00001851
Iteration 94/1000 | Loss: 0.00001851
Iteration 95/1000 | Loss: 0.00001851
Iteration 96/1000 | Loss: 0.00001851
Iteration 97/1000 | Loss: 0.00001851
Iteration 98/1000 | Loss: 0.00001851
Iteration 99/1000 | Loss: 0.00001851
Iteration 100/1000 | Loss: 0.00001850
Iteration 101/1000 | Loss: 0.00001850
Iteration 102/1000 | Loss: 0.00001850
Iteration 103/1000 | Loss: 0.00001850
Iteration 104/1000 | Loss: 0.00001850
Iteration 105/1000 | Loss: 0.00001850
Iteration 106/1000 | Loss: 0.00001850
Iteration 107/1000 | Loss: 0.00001850
Iteration 108/1000 | Loss: 0.00001850
Iteration 109/1000 | Loss: 0.00001850
Iteration 110/1000 | Loss: 0.00001850
Iteration 111/1000 | Loss: 0.00001850
Iteration 112/1000 | Loss: 0.00001850
Iteration 113/1000 | Loss: 0.00001850
Iteration 114/1000 | Loss: 0.00001850
Iteration 115/1000 | Loss: 0.00001850
Iteration 116/1000 | Loss: 0.00001850
Iteration 117/1000 | Loss: 0.00001850
Iteration 118/1000 | Loss: 0.00001850
Iteration 119/1000 | Loss: 0.00001850
Iteration 120/1000 | Loss: 0.00001850
Iteration 121/1000 | Loss: 0.00001850
Iteration 122/1000 | Loss: 0.00001850
Iteration 123/1000 | Loss: 0.00001850
Iteration 124/1000 | Loss: 0.00001850
Iteration 125/1000 | Loss: 0.00001850
Iteration 126/1000 | Loss: 0.00001850
Iteration 127/1000 | Loss: 0.00001850
Iteration 128/1000 | Loss: 0.00001850
Iteration 129/1000 | Loss: 0.00001850
Iteration 130/1000 | Loss: 0.00001850
Iteration 131/1000 | Loss: 0.00001850
Iteration 132/1000 | Loss: 0.00001850
Iteration 133/1000 | Loss: 0.00001850
Iteration 134/1000 | Loss: 0.00001850
Iteration 135/1000 | Loss: 0.00001850
Iteration 136/1000 | Loss: 0.00001850
Iteration 137/1000 | Loss: 0.00001850
Iteration 138/1000 | Loss: 0.00001850
Iteration 139/1000 | Loss: 0.00001850
Iteration 140/1000 | Loss: 0.00001850
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.8498529243515804e-05, 1.8498529243515804e-05, 1.8498529243515804e-05, 1.8498529243515804e-05, 1.8498529243515804e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8498529243515804e-05

Optimization complete. Final v2v error: 3.590724468231201 mm

Highest mean error: 4.497659683227539 mm for frame 143

Lowest mean error: 2.877342462539673 mm for frame 7

Saving results

Total time: 42.90039134025574
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_it_4274/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_it_4274/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_it_4274/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01123493
Iteration 2/25 | Loss: 0.01123493
Iteration 3/25 | Loss: 0.00152631
Iteration 4/25 | Loss: 0.00100906
Iteration 5/25 | Loss: 0.00088683
Iteration 6/25 | Loss: 0.00083033
Iteration 7/25 | Loss: 0.00086793
Iteration 8/25 | Loss: 0.00081598
Iteration 9/25 | Loss: 0.00074339
Iteration 10/25 | Loss: 0.00071829
Iteration 11/25 | Loss: 0.00071090
Iteration 12/25 | Loss: 0.00070979
Iteration 13/25 | Loss: 0.00070947
Iteration 14/25 | Loss: 0.00070941
Iteration 15/25 | Loss: 0.00070941
Iteration 16/25 | Loss: 0.00070941
Iteration 17/25 | Loss: 0.00070940
Iteration 18/25 | Loss: 0.00070940
Iteration 19/25 | Loss: 0.00070940
Iteration 20/25 | Loss: 0.00070940
Iteration 21/25 | Loss: 0.00070940
Iteration 22/25 | Loss: 0.00070940
Iteration 23/25 | Loss: 0.00070940
Iteration 24/25 | Loss: 0.00070940
Iteration 25/25 | Loss: 0.00070940

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34932947
Iteration 2/25 | Loss: 0.00017090
Iteration 3/25 | Loss: 0.00017090
Iteration 4/25 | Loss: 0.00017090
Iteration 5/25 | Loss: 0.00017090
Iteration 6/25 | Loss: 0.00017090
Iteration 7/25 | Loss: 0.00017090
Iteration 8/25 | Loss: 0.00017090
Iteration 9/25 | Loss: 0.00017090
Iteration 10/25 | Loss: 0.00017090
Iteration 11/25 | Loss: 0.00017090
Iteration 12/25 | Loss: 0.00017090
Iteration 13/25 | Loss: 0.00017090
Iteration 14/25 | Loss: 0.00017090
Iteration 15/25 | Loss: 0.00017090
Iteration 16/25 | Loss: 0.00017090
Iteration 17/25 | Loss: 0.00017090
Iteration 18/25 | Loss: 0.00017090
Iteration 19/25 | Loss: 0.00017090
Iteration 20/25 | Loss: 0.00017090
Iteration 21/25 | Loss: 0.00017090
Iteration 22/25 | Loss: 0.00017090
Iteration 23/25 | Loss: 0.00017090
Iteration 24/25 | Loss: 0.00017090
Iteration 25/25 | Loss: 0.00017090

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00017090
Iteration 2/1000 | Loss: 0.00003328
Iteration 3/1000 | Loss: 0.00002829
Iteration 4/1000 | Loss: 0.00002582
Iteration 5/1000 | Loss: 0.00002453
Iteration 6/1000 | Loss: 0.00002391
Iteration 7/1000 | Loss: 0.00002346
Iteration 8/1000 | Loss: 0.00002338
Iteration 9/1000 | Loss: 0.00002337
Iteration 10/1000 | Loss: 0.00002327
Iteration 11/1000 | Loss: 0.00002324
Iteration 12/1000 | Loss: 0.00002323
Iteration 13/1000 | Loss: 0.00002322
Iteration 14/1000 | Loss: 0.00002320
Iteration 15/1000 | Loss: 0.00002316
Iteration 16/1000 | Loss: 0.00002315
Iteration 17/1000 | Loss: 0.00002314
Iteration 18/1000 | Loss: 0.00002313
Iteration 19/1000 | Loss: 0.00002313
Iteration 20/1000 | Loss: 0.00002310
Iteration 21/1000 | Loss: 0.00002310
Iteration 22/1000 | Loss: 0.00002310
Iteration 23/1000 | Loss: 0.00002310
Iteration 24/1000 | Loss: 0.00002310
Iteration 25/1000 | Loss: 0.00002310
Iteration 26/1000 | Loss: 0.00002310
Iteration 27/1000 | Loss: 0.00002310
Iteration 28/1000 | Loss: 0.00002310
Iteration 29/1000 | Loss: 0.00002310
Iteration 30/1000 | Loss: 0.00002310
Iteration 31/1000 | Loss: 0.00002309
Iteration 32/1000 | Loss: 0.00002309
Iteration 33/1000 | Loss: 0.00002309
Iteration 34/1000 | Loss: 0.00002309
Iteration 35/1000 | Loss: 0.00002309
Iteration 36/1000 | Loss: 0.00002309
Iteration 37/1000 | Loss: 0.00002309
Iteration 38/1000 | Loss: 0.00002309
Iteration 39/1000 | Loss: 0.00002307
Iteration 40/1000 | Loss: 0.00002307
Iteration 41/1000 | Loss: 0.00002306
Iteration 42/1000 | Loss: 0.00002305
Iteration 43/1000 | Loss: 0.00002305
Iteration 44/1000 | Loss: 0.00002305
Iteration 45/1000 | Loss: 0.00002304
Iteration 46/1000 | Loss: 0.00002304
Iteration 47/1000 | Loss: 0.00002304
Iteration 48/1000 | Loss: 0.00002304
Iteration 49/1000 | Loss: 0.00002304
Iteration 50/1000 | Loss: 0.00002303
Iteration 51/1000 | Loss: 0.00002303
Iteration 52/1000 | Loss: 0.00002303
Iteration 53/1000 | Loss: 0.00002303
Iteration 54/1000 | Loss: 0.00002303
Iteration 55/1000 | Loss: 0.00002303
Iteration 56/1000 | Loss: 0.00002303
Iteration 57/1000 | Loss: 0.00002302
Iteration 58/1000 | Loss: 0.00002302
Iteration 59/1000 | Loss: 0.00002302
Iteration 60/1000 | Loss: 0.00002302
Iteration 61/1000 | Loss: 0.00002302
Iteration 62/1000 | Loss: 0.00002302
Iteration 63/1000 | Loss: 0.00002302
Iteration 64/1000 | Loss: 0.00002302
Iteration 65/1000 | Loss: 0.00002302
Iteration 66/1000 | Loss: 0.00002302
Iteration 67/1000 | Loss: 0.00002301
Iteration 68/1000 | Loss: 0.00002301
Iteration 69/1000 | Loss: 0.00002301
Iteration 70/1000 | Loss: 0.00002301
Iteration 71/1000 | Loss: 0.00002300
Iteration 72/1000 | Loss: 0.00002300
Iteration 73/1000 | Loss: 0.00002300
Iteration 74/1000 | Loss: 0.00002300
Iteration 75/1000 | Loss: 0.00002300
Iteration 76/1000 | Loss: 0.00002300
Iteration 77/1000 | Loss: 0.00002300
Iteration 78/1000 | Loss: 0.00002300
Iteration 79/1000 | Loss: 0.00002300
Iteration 80/1000 | Loss: 0.00002299
Iteration 81/1000 | Loss: 0.00002299
Iteration 82/1000 | Loss: 0.00002299
Iteration 83/1000 | Loss: 0.00002298
Iteration 84/1000 | Loss: 0.00002298
Iteration 85/1000 | Loss: 0.00002298
Iteration 86/1000 | Loss: 0.00002298
Iteration 87/1000 | Loss: 0.00002298
Iteration 88/1000 | Loss: 0.00002298
Iteration 89/1000 | Loss: 0.00002298
Iteration 90/1000 | Loss: 0.00002297
Iteration 91/1000 | Loss: 0.00002297
Iteration 92/1000 | Loss: 0.00002297
Iteration 93/1000 | Loss: 0.00002297
Iteration 94/1000 | Loss: 0.00002296
Iteration 95/1000 | Loss: 0.00002296
Iteration 96/1000 | Loss: 0.00002296
Iteration 97/1000 | Loss: 0.00002295
Iteration 98/1000 | Loss: 0.00002295
Iteration 99/1000 | Loss: 0.00002295
Iteration 100/1000 | Loss: 0.00002295
Iteration 101/1000 | Loss: 0.00002295
Iteration 102/1000 | Loss: 0.00002295
Iteration 103/1000 | Loss: 0.00002295
Iteration 104/1000 | Loss: 0.00002295
Iteration 105/1000 | Loss: 0.00002295
Iteration 106/1000 | Loss: 0.00002294
Iteration 107/1000 | Loss: 0.00002294
Iteration 108/1000 | Loss: 0.00002294
Iteration 109/1000 | Loss: 0.00002294
Iteration 110/1000 | Loss: 0.00002294
Iteration 111/1000 | Loss: 0.00002294
Iteration 112/1000 | Loss: 0.00002294
Iteration 113/1000 | Loss: 0.00002293
Iteration 114/1000 | Loss: 0.00002293
Iteration 115/1000 | Loss: 0.00002293
Iteration 116/1000 | Loss: 0.00002293
Iteration 117/1000 | Loss: 0.00002293
Iteration 118/1000 | Loss: 0.00002292
Iteration 119/1000 | Loss: 0.00002292
Iteration 120/1000 | Loss: 0.00002292
Iteration 121/1000 | Loss: 0.00002292
Iteration 122/1000 | Loss: 0.00002292
Iteration 123/1000 | Loss: 0.00002292
Iteration 124/1000 | Loss: 0.00002292
Iteration 125/1000 | Loss: 0.00002292
Iteration 126/1000 | Loss: 0.00002291
Iteration 127/1000 | Loss: 0.00002291
Iteration 128/1000 | Loss: 0.00002291
Iteration 129/1000 | Loss: 0.00002291
Iteration 130/1000 | Loss: 0.00002291
Iteration 131/1000 | Loss: 0.00002291
Iteration 132/1000 | Loss: 0.00002291
Iteration 133/1000 | Loss: 0.00002291
Iteration 134/1000 | Loss: 0.00002291
Iteration 135/1000 | Loss: 0.00002291
Iteration 136/1000 | Loss: 0.00002291
Iteration 137/1000 | Loss: 0.00002291
Iteration 138/1000 | Loss: 0.00002291
Iteration 139/1000 | Loss: 0.00002291
Iteration 140/1000 | Loss: 0.00002290
Iteration 141/1000 | Loss: 0.00002290
Iteration 142/1000 | Loss: 0.00002290
Iteration 143/1000 | Loss: 0.00002290
Iteration 144/1000 | Loss: 0.00002290
Iteration 145/1000 | Loss: 0.00002289
Iteration 146/1000 | Loss: 0.00002289
Iteration 147/1000 | Loss: 0.00002289
Iteration 148/1000 | Loss: 0.00002289
Iteration 149/1000 | Loss: 0.00002289
Iteration 150/1000 | Loss: 0.00002289
Iteration 151/1000 | Loss: 0.00002289
Iteration 152/1000 | Loss: 0.00002288
Iteration 153/1000 | Loss: 0.00002288
Iteration 154/1000 | Loss: 0.00002288
Iteration 155/1000 | Loss: 0.00002288
Iteration 156/1000 | Loss: 0.00002288
Iteration 157/1000 | Loss: 0.00002288
Iteration 158/1000 | Loss: 0.00002288
Iteration 159/1000 | Loss: 0.00002288
Iteration 160/1000 | Loss: 0.00002288
Iteration 161/1000 | Loss: 0.00002288
Iteration 162/1000 | Loss: 0.00002288
Iteration 163/1000 | Loss: 0.00002288
Iteration 164/1000 | Loss: 0.00002287
Iteration 165/1000 | Loss: 0.00002287
Iteration 166/1000 | Loss: 0.00002287
Iteration 167/1000 | Loss: 0.00002287
Iteration 168/1000 | Loss: 0.00002287
Iteration 169/1000 | Loss: 0.00002287
Iteration 170/1000 | Loss: 0.00002287
Iteration 171/1000 | Loss: 0.00002287
Iteration 172/1000 | Loss: 0.00002287
Iteration 173/1000 | Loss: 0.00002287
Iteration 174/1000 | Loss: 0.00002286
Iteration 175/1000 | Loss: 0.00002286
Iteration 176/1000 | Loss: 0.00002286
Iteration 177/1000 | Loss: 0.00002286
Iteration 178/1000 | Loss: 0.00002286
Iteration 179/1000 | Loss: 0.00002286
Iteration 180/1000 | Loss: 0.00002286
Iteration 181/1000 | Loss: 0.00002286
Iteration 182/1000 | Loss: 0.00002286
Iteration 183/1000 | Loss: 0.00002286
Iteration 184/1000 | Loss: 0.00002286
Iteration 185/1000 | Loss: 0.00002286
Iteration 186/1000 | Loss: 0.00002286
Iteration 187/1000 | Loss: 0.00002286
Iteration 188/1000 | Loss: 0.00002286
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [2.285869231855031e-05, 2.285869231855031e-05, 2.285869231855031e-05, 2.285869231855031e-05, 2.285869231855031e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.285869231855031e-05

Optimization complete. Final v2v error: 4.051680088043213 mm

Highest mean error: 4.631641864776611 mm for frame 47

Lowest mean error: 3.426043748855591 mm for frame 237

Saving results

Total time: 51.20944690704346
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_it_4274/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_it_4274/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_it_4274/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00611146
Iteration 2/25 | Loss: 0.00089950
Iteration 3/25 | Loss: 0.00078503
Iteration 4/25 | Loss: 0.00075377
Iteration 5/25 | Loss: 0.00073886
Iteration 6/25 | Loss: 0.00073661
Iteration 7/25 | Loss: 0.00073641
Iteration 8/25 | Loss: 0.00073641
Iteration 9/25 | Loss: 0.00073641
Iteration 10/25 | Loss: 0.00073641
Iteration 11/25 | Loss: 0.00073641
Iteration 12/25 | Loss: 0.00073641
Iteration 13/25 | Loss: 0.00073641
Iteration 14/25 | Loss: 0.00073641
Iteration 15/25 | Loss: 0.00073641
Iteration 16/25 | Loss: 0.00073641
Iteration 17/25 | Loss: 0.00073641
Iteration 18/25 | Loss: 0.00073641
Iteration 19/25 | Loss: 0.00073641
Iteration 20/25 | Loss: 0.00073641
Iteration 21/25 | Loss: 0.00073641
Iteration 22/25 | Loss: 0.00073641
Iteration 23/25 | Loss: 0.00073641
Iteration 24/25 | Loss: 0.00073641
Iteration 25/25 | Loss: 0.00073641

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.10705352
Iteration 2/25 | Loss: 0.00023503
Iteration 3/25 | Loss: 0.00023502
Iteration 4/25 | Loss: 0.00023502
Iteration 5/25 | Loss: 0.00023502
Iteration 6/25 | Loss: 0.00023502
Iteration 7/25 | Loss: 0.00023502
Iteration 8/25 | Loss: 0.00023502
Iteration 9/25 | Loss: 0.00023502
Iteration 10/25 | Loss: 0.00023502
Iteration 11/25 | Loss: 0.00023502
Iteration 12/25 | Loss: 0.00023502
Iteration 13/25 | Loss: 0.00023502
Iteration 14/25 | Loss: 0.00023502
Iteration 15/25 | Loss: 0.00023502
Iteration 16/25 | Loss: 0.00023502
Iteration 17/25 | Loss: 0.00023502
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00023502149269916117, 0.00023502149269916117, 0.00023502149269916117, 0.00023502149269916117, 0.00023502149269916117]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00023502149269916117

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00023502
Iteration 2/1000 | Loss: 0.00004660
Iteration 3/1000 | Loss: 0.00003910
Iteration 4/1000 | Loss: 0.00003643
Iteration 5/1000 | Loss: 0.00003445
Iteration 6/1000 | Loss: 0.00003306
Iteration 7/1000 | Loss: 0.00003223
Iteration 8/1000 | Loss: 0.00003178
Iteration 9/1000 | Loss: 0.00003143
Iteration 10/1000 | Loss: 0.00003127
Iteration 11/1000 | Loss: 0.00003124
Iteration 12/1000 | Loss: 0.00003120
Iteration 13/1000 | Loss: 0.00003120
Iteration 14/1000 | Loss: 0.00003119
Iteration 15/1000 | Loss: 0.00003118
Iteration 16/1000 | Loss: 0.00003118
Iteration 17/1000 | Loss: 0.00003114
Iteration 18/1000 | Loss: 0.00003114
Iteration 19/1000 | Loss: 0.00003113
Iteration 20/1000 | Loss: 0.00003113
Iteration 21/1000 | Loss: 0.00003113
Iteration 22/1000 | Loss: 0.00003113
Iteration 23/1000 | Loss: 0.00003112
Iteration 24/1000 | Loss: 0.00003111
Iteration 25/1000 | Loss: 0.00003110
Iteration 26/1000 | Loss: 0.00003109
Iteration 27/1000 | Loss: 0.00003109
Iteration 28/1000 | Loss: 0.00003109
Iteration 29/1000 | Loss: 0.00003109
Iteration 30/1000 | Loss: 0.00003109
Iteration 31/1000 | Loss: 0.00003109
Iteration 32/1000 | Loss: 0.00003109
Iteration 33/1000 | Loss: 0.00003109
Iteration 34/1000 | Loss: 0.00003108
Iteration 35/1000 | Loss: 0.00003108
Iteration 36/1000 | Loss: 0.00003107
Iteration 37/1000 | Loss: 0.00003106
Iteration 38/1000 | Loss: 0.00003105
Iteration 39/1000 | Loss: 0.00003105
Iteration 40/1000 | Loss: 0.00003099
Iteration 41/1000 | Loss: 0.00003099
Iteration 42/1000 | Loss: 0.00003096
Iteration 43/1000 | Loss: 0.00003095
Iteration 44/1000 | Loss: 0.00003095
Iteration 45/1000 | Loss: 0.00003095
Iteration 46/1000 | Loss: 0.00003093
Iteration 47/1000 | Loss: 0.00003091
Iteration 48/1000 | Loss: 0.00003090
Iteration 49/1000 | Loss: 0.00003090
Iteration 50/1000 | Loss: 0.00003090
Iteration 51/1000 | Loss: 0.00003090
Iteration 52/1000 | Loss: 0.00003089
Iteration 53/1000 | Loss: 0.00003089
Iteration 54/1000 | Loss: 0.00003089
Iteration 55/1000 | Loss: 0.00003089
Iteration 56/1000 | Loss: 0.00003089
Iteration 57/1000 | Loss: 0.00003089
Iteration 58/1000 | Loss: 0.00003089
Iteration 59/1000 | Loss: 0.00003089
Iteration 60/1000 | Loss: 0.00003088
Iteration 61/1000 | Loss: 0.00003087
Iteration 62/1000 | Loss: 0.00003087
Iteration 63/1000 | Loss: 0.00003087
Iteration 64/1000 | Loss: 0.00003087
Iteration 65/1000 | Loss: 0.00003087
Iteration 66/1000 | Loss: 0.00003087
Iteration 67/1000 | Loss: 0.00003087
Iteration 68/1000 | Loss: 0.00003086
Iteration 69/1000 | Loss: 0.00003086
Iteration 70/1000 | Loss: 0.00003086
Iteration 71/1000 | Loss: 0.00003086
Iteration 72/1000 | Loss: 0.00003086
Iteration 73/1000 | Loss: 0.00003086
Iteration 74/1000 | Loss: 0.00003086
Iteration 75/1000 | Loss: 0.00003085
Iteration 76/1000 | Loss: 0.00003085
Iteration 77/1000 | Loss: 0.00003084
Iteration 78/1000 | Loss: 0.00003084
Iteration 79/1000 | Loss: 0.00003084
Iteration 80/1000 | Loss: 0.00003084
Iteration 81/1000 | Loss: 0.00003084
Iteration 82/1000 | Loss: 0.00003084
Iteration 83/1000 | Loss: 0.00003084
Iteration 84/1000 | Loss: 0.00003084
Iteration 85/1000 | Loss: 0.00003084
Iteration 86/1000 | Loss: 0.00003083
Iteration 87/1000 | Loss: 0.00003083
Iteration 88/1000 | Loss: 0.00003083
Iteration 89/1000 | Loss: 0.00003083
Iteration 90/1000 | Loss: 0.00003083
Iteration 91/1000 | Loss: 0.00003083
Iteration 92/1000 | Loss: 0.00003083
Iteration 93/1000 | Loss: 0.00003082
Iteration 94/1000 | Loss: 0.00003082
Iteration 95/1000 | Loss: 0.00003082
Iteration 96/1000 | Loss: 0.00003082
Iteration 97/1000 | Loss: 0.00003082
Iteration 98/1000 | Loss: 0.00003082
Iteration 99/1000 | Loss: 0.00003082
Iteration 100/1000 | Loss: 0.00003082
Iteration 101/1000 | Loss: 0.00003082
Iteration 102/1000 | Loss: 0.00003082
Iteration 103/1000 | Loss: 0.00003082
Iteration 104/1000 | Loss: 0.00003082
Iteration 105/1000 | Loss: 0.00003082
Iteration 106/1000 | Loss: 0.00003082
Iteration 107/1000 | Loss: 0.00003082
Iteration 108/1000 | Loss: 0.00003082
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [3.0815710488241166e-05, 3.0815710488241166e-05, 3.0815710488241166e-05, 3.0815710488241166e-05, 3.0815710488241166e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0815710488241166e-05

Optimization complete. Final v2v error: 4.634112358093262 mm

Highest mean error: 4.967495441436768 mm for frame 95

Lowest mean error: 4.335078239440918 mm for frame 81

Saving results

Total time: 33.51454305648804
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_it_4274/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_it_4274/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_it_4274/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01023265
Iteration 2/25 | Loss: 0.00274498
Iteration 3/25 | Loss: 0.00182843
Iteration 4/25 | Loss: 0.00150014
Iteration 5/25 | Loss: 0.00139565
Iteration 6/25 | Loss: 0.00144174
Iteration 7/25 | Loss: 0.00124786
Iteration 8/25 | Loss: 0.00113912
Iteration 9/25 | Loss: 0.00111471
Iteration 10/25 | Loss: 0.00108621
Iteration 11/25 | Loss: 0.00108298
Iteration 12/25 | Loss: 0.00102975
Iteration 13/25 | Loss: 0.00102291
Iteration 14/25 | Loss: 0.00102285
Iteration 15/25 | Loss: 0.00099323
Iteration 16/25 | Loss: 0.00098388
Iteration 17/25 | Loss: 0.00098033
Iteration 18/25 | Loss: 0.00097622
Iteration 19/25 | Loss: 0.00098443
Iteration 20/25 | Loss: 0.00096645
Iteration 21/25 | Loss: 0.00096337
Iteration 22/25 | Loss: 0.00095219
Iteration 23/25 | Loss: 0.00095312
Iteration 24/25 | Loss: 0.00094779
Iteration 25/25 | Loss: 0.00094527

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35521483
Iteration 2/25 | Loss: 0.00303115
Iteration 3/25 | Loss: 0.00199062
Iteration 4/25 | Loss: 0.00199062
Iteration 5/25 | Loss: 0.00199062
Iteration 6/25 | Loss: 0.00199062
Iteration 7/25 | Loss: 0.00199062
Iteration 8/25 | Loss: 0.00199062
Iteration 9/25 | Loss: 0.00199062
Iteration 10/25 | Loss: 0.00199062
Iteration 11/25 | Loss: 0.00199062
Iteration 12/25 | Loss: 0.00199062
Iteration 13/25 | Loss: 0.00199062
Iteration 14/25 | Loss: 0.00199062
Iteration 15/25 | Loss: 0.00199062
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0019906191155314445, 0.0019906191155314445, 0.0019906191155314445, 0.0019906191155314445, 0.0019906191155314445]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019906191155314445

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00199062
Iteration 2/1000 | Loss: 0.00376924
Iteration 3/1000 | Loss: 0.00215689
Iteration 4/1000 | Loss: 0.00049223
Iteration 5/1000 | Loss: 0.00029017
Iteration 6/1000 | Loss: 0.00037943
Iteration 7/1000 | Loss: 0.00107372
Iteration 8/1000 | Loss: 0.00093853
Iteration 9/1000 | Loss: 0.00069347
Iteration 10/1000 | Loss: 0.00077305
Iteration 11/1000 | Loss: 0.00101088
Iteration 12/1000 | Loss: 0.00329089
Iteration 13/1000 | Loss: 0.00089123
Iteration 14/1000 | Loss: 0.00026508
Iteration 15/1000 | Loss: 0.00035365
Iteration 16/1000 | Loss: 0.00146434
Iteration 17/1000 | Loss: 0.00012563
Iteration 18/1000 | Loss: 0.00015790
Iteration 19/1000 | Loss: 0.00009978
Iteration 20/1000 | Loss: 0.00051549
Iteration 21/1000 | Loss: 0.00030009
Iteration 22/1000 | Loss: 0.00018022
Iteration 23/1000 | Loss: 0.00013362
Iteration 24/1000 | Loss: 0.00016218
Iteration 25/1000 | Loss: 0.00045822
Iteration 26/1000 | Loss: 0.00060957
Iteration 27/1000 | Loss: 0.00076778
Iteration 28/1000 | Loss: 0.00068850
Iteration 29/1000 | Loss: 0.00059286
Iteration 30/1000 | Loss: 0.00039716
Iteration 31/1000 | Loss: 0.00052441
Iteration 32/1000 | Loss: 0.00060949
Iteration 33/1000 | Loss: 0.00016353
Iteration 34/1000 | Loss: 0.00034299
Iteration 35/1000 | Loss: 0.00019331
Iteration 36/1000 | Loss: 0.00038730
Iteration 37/1000 | Loss: 0.00067091
Iteration 38/1000 | Loss: 0.00074011
Iteration 39/1000 | Loss: 0.00024376
Iteration 40/1000 | Loss: 0.00013569
Iteration 41/1000 | Loss: 0.00007735
Iteration 42/1000 | Loss: 0.00013044
Iteration 43/1000 | Loss: 0.00013042
Iteration 44/1000 | Loss: 0.00006933
Iteration 45/1000 | Loss: 0.00015826
Iteration 46/1000 | Loss: 0.00029842
Iteration 47/1000 | Loss: 0.00021272
Iteration 48/1000 | Loss: 0.00041083
Iteration 49/1000 | Loss: 0.00028412
Iteration 50/1000 | Loss: 0.00020462
Iteration 51/1000 | Loss: 0.00012944
Iteration 52/1000 | Loss: 0.00019011
Iteration 53/1000 | Loss: 0.00017776
Iteration 54/1000 | Loss: 0.00021219
Iteration 55/1000 | Loss: 0.00023163
Iteration 56/1000 | Loss: 0.00006401
Iteration 57/1000 | Loss: 0.00015247
Iteration 58/1000 | Loss: 0.00004742
Iteration 59/1000 | Loss: 0.00005290
Iteration 60/1000 | Loss: 0.00004918
Iteration 61/1000 | Loss: 0.00011367
Iteration 62/1000 | Loss: 0.00006073
Iteration 63/1000 | Loss: 0.00005023
Iteration 64/1000 | Loss: 0.00007311
Iteration 65/1000 | Loss: 0.00011822
Iteration 66/1000 | Loss: 0.00009302
Iteration 67/1000 | Loss: 0.00005120
Iteration 68/1000 | Loss: 0.00006832
Iteration 69/1000 | Loss: 0.00005698
Iteration 70/1000 | Loss: 0.00007016
Iteration 71/1000 | Loss: 0.00006351
Iteration 72/1000 | Loss: 0.00007684
Iteration 73/1000 | Loss: 0.00009193
Iteration 74/1000 | Loss: 0.00007247
Iteration 75/1000 | Loss: 0.00014339
Iteration 76/1000 | Loss: 0.00008679
Iteration 77/1000 | Loss: 0.00008055
Iteration 78/1000 | Loss: 0.00011526
Iteration 79/1000 | Loss: 0.00005099
Iteration 80/1000 | Loss: 0.00006123
Iteration 81/1000 | Loss: 0.00007530
Iteration 82/1000 | Loss: 0.00006459
Iteration 83/1000 | Loss: 0.00006102
Iteration 84/1000 | Loss: 0.00010847
Iteration 85/1000 | Loss: 0.00010282
Iteration 86/1000 | Loss: 0.00008408
Iteration 87/1000 | Loss: 0.00051256
Iteration 88/1000 | Loss: 0.00007108
Iteration 89/1000 | Loss: 0.00005658
Iteration 90/1000 | Loss: 0.00006176
Iteration 91/1000 | Loss: 0.00005758
Iteration 92/1000 | Loss: 0.00007458
Iteration 93/1000 | Loss: 0.00006203
Iteration 94/1000 | Loss: 0.00005232
Iteration 95/1000 | Loss: 0.00007139
Iteration 96/1000 | Loss: 0.00012111
Iteration 97/1000 | Loss: 0.00008803
Iteration 98/1000 | Loss: 0.00007222
Iteration 99/1000 | Loss: 0.00006237
Iteration 100/1000 | Loss: 0.00006607
Iteration 101/1000 | Loss: 0.00006001
Iteration 102/1000 | Loss: 0.00012446
Iteration 103/1000 | Loss: 0.00006737
Iteration 104/1000 | Loss: 0.00009051
Iteration 105/1000 | Loss: 0.00004791
Iteration 106/1000 | Loss: 0.00022215
Iteration 107/1000 | Loss: 0.00005200
Iteration 108/1000 | Loss: 0.00007446
Iteration 109/1000 | Loss: 0.00003406
Iteration 110/1000 | Loss: 0.00008140
Iteration 111/1000 | Loss: 0.00006951
Iteration 112/1000 | Loss: 0.00003216
Iteration 113/1000 | Loss: 0.00004731
Iteration 114/1000 | Loss: 0.00004502
Iteration 115/1000 | Loss: 0.00006012
Iteration 116/1000 | Loss: 0.00003131
Iteration 117/1000 | Loss: 0.00003102
Iteration 118/1000 | Loss: 0.00003090
Iteration 119/1000 | Loss: 0.00003087
Iteration 120/1000 | Loss: 0.00006498
Iteration 121/1000 | Loss: 0.00007412
Iteration 122/1000 | Loss: 0.00006019
Iteration 123/1000 | Loss: 0.00005164
Iteration 124/1000 | Loss: 0.00004196
Iteration 125/1000 | Loss: 0.00005587
Iteration 126/1000 | Loss: 0.00004606
Iteration 127/1000 | Loss: 0.00005987
Iteration 128/1000 | Loss: 0.00004862
Iteration 129/1000 | Loss: 0.00004532
Iteration 130/1000 | Loss: 0.00005022
Iteration 131/1000 | Loss: 0.00005014
Iteration 132/1000 | Loss: 0.00006891
Iteration 133/1000 | Loss: 0.00003077
Iteration 134/1000 | Loss: 0.00004582
Iteration 135/1000 | Loss: 0.00004535
Iteration 136/1000 | Loss: 0.00004302
Iteration 137/1000 | Loss: 0.00004614
Iteration 138/1000 | Loss: 0.00004508
Iteration 139/1000 | Loss: 0.00004544
Iteration 140/1000 | Loss: 0.00006108
Iteration 141/1000 | Loss: 0.00004464
Iteration 142/1000 | Loss: 0.00004065
Iteration 143/1000 | Loss: 0.00004064
Iteration 144/1000 | Loss: 0.00006562
Iteration 145/1000 | Loss: 0.00004146
Iteration 146/1000 | Loss: 0.00004773
Iteration 147/1000 | Loss: 0.00004054
Iteration 148/1000 | Loss: 0.00007235
Iteration 149/1000 | Loss: 0.00005089
Iteration 150/1000 | Loss: 0.00004057
Iteration 151/1000 | Loss: 0.00004860
Iteration 152/1000 | Loss: 0.00003665
Iteration 153/1000 | Loss: 0.00038315
Iteration 154/1000 | Loss: 0.00003379
Iteration 155/1000 | Loss: 0.00003266
Iteration 156/1000 | Loss: 0.00003143
Iteration 157/1000 | Loss: 0.00003080
Iteration 158/1000 | Loss: 0.00003046
Iteration 159/1000 | Loss: 0.00003014
Iteration 160/1000 | Loss: 0.00004771
Iteration 161/1000 | Loss: 0.00004771
Iteration 162/1000 | Loss: 0.00012760
Iteration 163/1000 | Loss: 0.00020796
Iteration 164/1000 | Loss: 0.00003070
Iteration 165/1000 | Loss: 0.00003781
Iteration 166/1000 | Loss: 0.00005522
Iteration 167/1000 | Loss: 0.00002991
Iteration 168/1000 | Loss: 0.00002986
Iteration 169/1000 | Loss: 0.00002982
Iteration 170/1000 | Loss: 0.00002981
Iteration 171/1000 | Loss: 0.00002981
Iteration 172/1000 | Loss: 0.00002980
Iteration 173/1000 | Loss: 0.00003883
Iteration 174/1000 | Loss: 0.00003028
Iteration 175/1000 | Loss: 0.00003036
Iteration 176/1000 | Loss: 0.00004156
Iteration 177/1000 | Loss: 0.00003031
Iteration 178/1000 | Loss: 0.00003485
Iteration 179/1000 | Loss: 0.00002972
Iteration 180/1000 | Loss: 0.00002971
Iteration 181/1000 | Loss: 0.00002970
Iteration 182/1000 | Loss: 0.00002970
Iteration 183/1000 | Loss: 0.00002970
Iteration 184/1000 | Loss: 0.00002970
Iteration 185/1000 | Loss: 0.00002970
Iteration 186/1000 | Loss: 0.00002970
Iteration 187/1000 | Loss: 0.00002970
Iteration 188/1000 | Loss: 0.00002969
Iteration 189/1000 | Loss: 0.00002969
Iteration 190/1000 | Loss: 0.00002969
Iteration 191/1000 | Loss: 0.00002969
Iteration 192/1000 | Loss: 0.00002969
Iteration 193/1000 | Loss: 0.00002969
Iteration 194/1000 | Loss: 0.00002969
Iteration 195/1000 | Loss: 0.00002969
Iteration 196/1000 | Loss: 0.00002969
Iteration 197/1000 | Loss: 0.00002969
Iteration 198/1000 | Loss: 0.00002969
Iteration 199/1000 | Loss: 0.00002969
Iteration 200/1000 | Loss: 0.00002969
Iteration 201/1000 | Loss: 0.00002969
Iteration 202/1000 | Loss: 0.00002969
Iteration 203/1000 | Loss: 0.00002969
Iteration 204/1000 | Loss: 0.00002969
Iteration 205/1000 | Loss: 0.00002969
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [2.969035085698124e-05, 2.969035085698124e-05, 2.969035085698124e-05, 2.969035085698124e-05, 2.969035085698124e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.969035085698124e-05

Optimization complete. Final v2v error: 4.592555522918701 mm

Highest mean error: 6.672549247741699 mm for frame 236

Lowest mean error: 4.093531131744385 mm for frame 38

Saving results

Total time: 320.31619477272034
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_it_4274/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_it_4274/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_it_4274/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00444248
Iteration 2/25 | Loss: 0.00080325
Iteration 3/25 | Loss: 0.00065674
Iteration 4/25 | Loss: 0.00063748
Iteration 5/25 | Loss: 0.00063068
Iteration 6/25 | Loss: 0.00062952
Iteration 7/25 | Loss: 0.00062947
Iteration 8/25 | Loss: 0.00062947
Iteration 9/25 | Loss: 0.00062947
Iteration 10/25 | Loss: 0.00062947
Iteration 11/25 | Loss: 0.00062947
Iteration 12/25 | Loss: 0.00062947
Iteration 13/25 | Loss: 0.00062947
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.000629468122497201, 0.000629468122497201, 0.000629468122497201, 0.000629468122497201, 0.000629468122497201]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000629468122497201

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34595823
Iteration 2/25 | Loss: 0.00015529
Iteration 3/25 | Loss: 0.00015529
Iteration 4/25 | Loss: 0.00015529
Iteration 5/25 | Loss: 0.00015529
Iteration 6/25 | Loss: 0.00015529
Iteration 7/25 | Loss: 0.00015529
Iteration 8/25 | Loss: 0.00015529
Iteration 9/25 | Loss: 0.00015529
Iteration 10/25 | Loss: 0.00015529
Iteration 11/25 | Loss: 0.00015529
Iteration 12/25 | Loss: 0.00015529
Iteration 13/25 | Loss: 0.00015529
Iteration 14/25 | Loss: 0.00015529
Iteration 15/25 | Loss: 0.00015529
Iteration 16/25 | Loss: 0.00015529
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00015528962831012905, 0.00015528962831012905, 0.00015528962831012905, 0.00015528962831012905, 0.00015528962831012905]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00015528962831012905

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00015529
Iteration 2/1000 | Loss: 0.00002457
Iteration 3/1000 | Loss: 0.00002064
Iteration 4/1000 | Loss: 0.00001936
Iteration 5/1000 | Loss: 0.00001858
Iteration 6/1000 | Loss: 0.00001800
Iteration 7/1000 | Loss: 0.00001780
Iteration 8/1000 | Loss: 0.00001756
Iteration 9/1000 | Loss: 0.00001747
Iteration 10/1000 | Loss: 0.00001745
Iteration 11/1000 | Loss: 0.00001743
Iteration 12/1000 | Loss: 0.00001743
Iteration 13/1000 | Loss: 0.00001742
Iteration 14/1000 | Loss: 0.00001742
Iteration 15/1000 | Loss: 0.00001742
Iteration 16/1000 | Loss: 0.00001741
Iteration 17/1000 | Loss: 0.00001741
Iteration 18/1000 | Loss: 0.00001741
Iteration 19/1000 | Loss: 0.00001741
Iteration 20/1000 | Loss: 0.00001741
Iteration 21/1000 | Loss: 0.00001740
Iteration 22/1000 | Loss: 0.00001740
Iteration 23/1000 | Loss: 0.00001740
Iteration 24/1000 | Loss: 0.00001739
Iteration 25/1000 | Loss: 0.00001739
Iteration 26/1000 | Loss: 0.00001738
Iteration 27/1000 | Loss: 0.00001738
Iteration 28/1000 | Loss: 0.00001738
Iteration 29/1000 | Loss: 0.00001738
Iteration 30/1000 | Loss: 0.00001737
Iteration 31/1000 | Loss: 0.00001737
Iteration 32/1000 | Loss: 0.00001737
Iteration 33/1000 | Loss: 0.00001737
Iteration 34/1000 | Loss: 0.00001737
Iteration 35/1000 | Loss: 0.00001737
Iteration 36/1000 | Loss: 0.00001737
Iteration 37/1000 | Loss: 0.00001737
Iteration 38/1000 | Loss: 0.00001737
Iteration 39/1000 | Loss: 0.00001737
Iteration 40/1000 | Loss: 0.00001736
Iteration 41/1000 | Loss: 0.00001736
Iteration 42/1000 | Loss: 0.00001736
Iteration 43/1000 | Loss: 0.00001736
Iteration 44/1000 | Loss: 0.00001736
Iteration 45/1000 | Loss: 0.00001736
Iteration 46/1000 | Loss: 0.00001736
Iteration 47/1000 | Loss: 0.00001736
Iteration 48/1000 | Loss: 0.00001736
Iteration 49/1000 | Loss: 0.00001735
Iteration 50/1000 | Loss: 0.00001735
Iteration 51/1000 | Loss: 0.00001735
Iteration 52/1000 | Loss: 0.00001735
Iteration 53/1000 | Loss: 0.00001735
Iteration 54/1000 | Loss: 0.00001735
Iteration 55/1000 | Loss: 0.00001735
Iteration 56/1000 | Loss: 0.00001735
Iteration 57/1000 | Loss: 0.00001734
Iteration 58/1000 | Loss: 0.00001734
Iteration 59/1000 | Loss: 0.00001734
Iteration 60/1000 | Loss: 0.00001734
Iteration 61/1000 | Loss: 0.00001734
Iteration 62/1000 | Loss: 0.00001734
Iteration 63/1000 | Loss: 0.00001734
Iteration 64/1000 | Loss: 0.00001734
Iteration 65/1000 | Loss: 0.00001733
Iteration 66/1000 | Loss: 0.00001733
Iteration 67/1000 | Loss: 0.00001733
Iteration 68/1000 | Loss: 0.00001733
Iteration 69/1000 | Loss: 0.00001733
Iteration 70/1000 | Loss: 0.00001733
Iteration 71/1000 | Loss: 0.00001733
Iteration 72/1000 | Loss: 0.00001732
Iteration 73/1000 | Loss: 0.00001732
Iteration 74/1000 | Loss: 0.00001732
Iteration 75/1000 | Loss: 0.00001732
Iteration 76/1000 | Loss: 0.00001732
Iteration 77/1000 | Loss: 0.00001732
Iteration 78/1000 | Loss: 0.00001732
Iteration 79/1000 | Loss: 0.00001732
Iteration 80/1000 | Loss: 0.00001732
Iteration 81/1000 | Loss: 0.00001731
Iteration 82/1000 | Loss: 0.00001731
Iteration 83/1000 | Loss: 0.00001731
Iteration 84/1000 | Loss: 0.00001730
Iteration 85/1000 | Loss: 0.00001730
Iteration 86/1000 | Loss: 0.00001730
Iteration 87/1000 | Loss: 0.00001730
Iteration 88/1000 | Loss: 0.00001730
Iteration 89/1000 | Loss: 0.00001730
Iteration 90/1000 | Loss: 0.00001730
Iteration 91/1000 | Loss: 0.00001730
Iteration 92/1000 | Loss: 0.00001730
Iteration 93/1000 | Loss: 0.00001730
Iteration 94/1000 | Loss: 0.00001729
Iteration 95/1000 | Loss: 0.00001728
Iteration 96/1000 | Loss: 0.00001728
Iteration 97/1000 | Loss: 0.00001728
Iteration 98/1000 | Loss: 0.00001728
Iteration 99/1000 | Loss: 0.00001727
Iteration 100/1000 | Loss: 0.00001727
Iteration 101/1000 | Loss: 0.00001727
Iteration 102/1000 | Loss: 0.00001727
Iteration 103/1000 | Loss: 0.00001727
Iteration 104/1000 | Loss: 0.00001727
Iteration 105/1000 | Loss: 0.00001726
Iteration 106/1000 | Loss: 0.00001725
Iteration 107/1000 | Loss: 0.00001725
Iteration 108/1000 | Loss: 0.00001725
Iteration 109/1000 | Loss: 0.00001725
Iteration 110/1000 | Loss: 0.00001724
Iteration 111/1000 | Loss: 0.00001724
Iteration 112/1000 | Loss: 0.00001724
Iteration 113/1000 | Loss: 0.00001724
Iteration 114/1000 | Loss: 0.00001724
Iteration 115/1000 | Loss: 0.00001724
Iteration 116/1000 | Loss: 0.00001724
Iteration 117/1000 | Loss: 0.00001724
Iteration 118/1000 | Loss: 0.00001723
Iteration 119/1000 | Loss: 0.00001723
Iteration 120/1000 | Loss: 0.00001722
Iteration 121/1000 | Loss: 0.00001722
Iteration 122/1000 | Loss: 0.00001722
Iteration 123/1000 | Loss: 0.00001722
Iteration 124/1000 | Loss: 0.00001722
Iteration 125/1000 | Loss: 0.00001722
Iteration 126/1000 | Loss: 0.00001722
Iteration 127/1000 | Loss: 0.00001721
Iteration 128/1000 | Loss: 0.00001721
Iteration 129/1000 | Loss: 0.00001721
Iteration 130/1000 | Loss: 0.00001721
Iteration 131/1000 | Loss: 0.00001720
Iteration 132/1000 | Loss: 0.00001720
Iteration 133/1000 | Loss: 0.00001719
Iteration 134/1000 | Loss: 0.00001719
Iteration 135/1000 | Loss: 0.00001719
Iteration 136/1000 | Loss: 0.00001719
Iteration 137/1000 | Loss: 0.00001719
Iteration 138/1000 | Loss: 0.00001719
Iteration 139/1000 | Loss: 0.00001719
Iteration 140/1000 | Loss: 0.00001719
Iteration 141/1000 | Loss: 0.00001719
Iteration 142/1000 | Loss: 0.00001719
Iteration 143/1000 | Loss: 0.00001719
Iteration 144/1000 | Loss: 0.00001719
Iteration 145/1000 | Loss: 0.00001719
Iteration 146/1000 | Loss: 0.00001718
Iteration 147/1000 | Loss: 0.00001718
Iteration 148/1000 | Loss: 0.00001718
Iteration 149/1000 | Loss: 0.00001717
Iteration 150/1000 | Loss: 0.00001717
Iteration 151/1000 | Loss: 0.00001717
Iteration 152/1000 | Loss: 0.00001717
Iteration 153/1000 | Loss: 0.00001717
Iteration 154/1000 | Loss: 0.00001717
Iteration 155/1000 | Loss: 0.00001717
Iteration 156/1000 | Loss: 0.00001717
Iteration 157/1000 | Loss: 0.00001717
Iteration 158/1000 | Loss: 0.00001717
Iteration 159/1000 | Loss: 0.00001717
Iteration 160/1000 | Loss: 0.00001716
Iteration 161/1000 | Loss: 0.00001716
Iteration 162/1000 | Loss: 0.00001716
Iteration 163/1000 | Loss: 0.00001716
Iteration 164/1000 | Loss: 0.00001716
Iteration 165/1000 | Loss: 0.00001716
Iteration 166/1000 | Loss: 0.00001716
Iteration 167/1000 | Loss: 0.00001716
Iteration 168/1000 | Loss: 0.00001716
Iteration 169/1000 | Loss: 0.00001716
Iteration 170/1000 | Loss: 0.00001716
Iteration 171/1000 | Loss: 0.00001716
Iteration 172/1000 | Loss: 0.00001716
Iteration 173/1000 | Loss: 0.00001716
Iteration 174/1000 | Loss: 0.00001716
Iteration 175/1000 | Loss: 0.00001716
Iteration 176/1000 | Loss: 0.00001716
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.7163369193440303e-05, 1.7163369193440303e-05, 1.7163369193440303e-05, 1.7163369193440303e-05, 1.7163369193440303e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7163369193440303e-05

Optimization complete. Final v2v error: 3.510688543319702 mm

Highest mean error: 4.055810451507568 mm for frame 197

Lowest mean error: 2.975198984146118 mm for frame 219

Saving results

Total time: 37.054123401641846
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_it_4274/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_it_4274/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_it_4274/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01125252
Iteration 2/25 | Loss: 0.00374340
Iteration 3/25 | Loss: 0.00205194
Iteration 4/25 | Loss: 0.00180834
Iteration 5/25 | Loss: 0.00162796
Iteration 6/25 | Loss: 0.00152674
Iteration 7/25 | Loss: 0.00152293
Iteration 8/25 | Loss: 0.00144501
Iteration 9/25 | Loss: 0.00137531
Iteration 10/25 | Loss: 0.00135158
Iteration 11/25 | Loss: 0.00132343
Iteration 12/25 | Loss: 0.00123521
Iteration 13/25 | Loss: 0.00118021
Iteration 14/25 | Loss: 0.00114562
Iteration 15/25 | Loss: 0.00113022
Iteration 16/25 | Loss: 0.00112381
Iteration 17/25 | Loss: 0.00112580
Iteration 18/25 | Loss: 0.00111149
Iteration 19/25 | Loss: 0.00110925
Iteration 20/25 | Loss: 0.00110882
Iteration 21/25 | Loss: 0.00113156
Iteration 22/25 | Loss: 0.00112567
Iteration 23/25 | Loss: 0.00109781
Iteration 24/25 | Loss: 0.00108967
Iteration 25/25 | Loss: 0.00108385

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.12658858
Iteration 2/25 | Loss: 0.00260359
Iteration 3/25 | Loss: 0.00260359
Iteration 4/25 | Loss: 0.00260359
Iteration 5/25 | Loss: 0.00244857
Iteration 6/25 | Loss: 0.00244857
Iteration 7/25 | Loss: 0.00244857
Iteration 8/25 | Loss: 0.00244856
Iteration 9/25 | Loss: 0.00244856
Iteration 10/25 | Loss: 0.00244856
Iteration 11/25 | Loss: 0.00244856
Iteration 12/25 | Loss: 0.00244856
Iteration 13/25 | Loss: 0.00244856
Iteration 14/25 | Loss: 0.00244856
Iteration 15/25 | Loss: 0.00244856
Iteration 16/25 | Loss: 0.00244856
Iteration 17/25 | Loss: 0.00244856
Iteration 18/25 | Loss: 0.00244856
Iteration 19/25 | Loss: 0.00244856
Iteration 20/25 | Loss: 0.00244856
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0024485636968165636, 0.0024485636968165636, 0.0024485636968165636, 0.0024485636968165636, 0.0024485636968165636]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024485636968165636

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00244856
Iteration 2/1000 | Loss: 0.00079863
Iteration 3/1000 | Loss: 0.00042772
Iteration 4/1000 | Loss: 0.00076268
Iteration 5/1000 | Loss: 0.00070277
Iteration 6/1000 | Loss: 0.00041985
Iteration 7/1000 | Loss: 0.00078239
Iteration 8/1000 | Loss: 0.00021827
Iteration 9/1000 | Loss: 0.00019261
Iteration 10/1000 | Loss: 0.00018058
Iteration 11/1000 | Loss: 0.00034327
Iteration 12/1000 | Loss: 0.00069701
Iteration 13/1000 | Loss: 0.00176006
Iteration 14/1000 | Loss: 0.00029511
Iteration 15/1000 | Loss: 0.00039875
Iteration 16/1000 | Loss: 0.00017524
Iteration 17/1000 | Loss: 0.00038293
Iteration 18/1000 | Loss: 0.00027557
Iteration 19/1000 | Loss: 0.00035442
Iteration 20/1000 | Loss: 0.00119390
Iteration 21/1000 | Loss: 0.00049146
Iteration 22/1000 | Loss: 0.00081279
Iteration 23/1000 | Loss: 0.00014829
Iteration 24/1000 | Loss: 0.00014534
Iteration 25/1000 | Loss: 0.00024281
Iteration 26/1000 | Loss: 0.00013892
Iteration 27/1000 | Loss: 0.00015959
Iteration 28/1000 | Loss: 0.00051453
Iteration 29/1000 | Loss: 0.00038307
Iteration 30/1000 | Loss: 0.00087286
Iteration 31/1000 | Loss: 0.00055380
Iteration 32/1000 | Loss: 0.00010955
Iteration 33/1000 | Loss: 0.00010089
Iteration 34/1000 | Loss: 0.00012826
Iteration 35/1000 | Loss: 0.00047115
Iteration 36/1000 | Loss: 0.00329235
Iteration 37/1000 | Loss: 0.00036682
Iteration 38/1000 | Loss: 0.00055611
Iteration 39/1000 | Loss: 0.00014753
Iteration 40/1000 | Loss: 0.00034531
Iteration 41/1000 | Loss: 0.00016515
Iteration 42/1000 | Loss: 0.00016023
Iteration 43/1000 | Loss: 0.00006618
Iteration 44/1000 | Loss: 0.00008704
Iteration 45/1000 | Loss: 0.00040165
Iteration 46/1000 | Loss: 0.00010763
Iteration 47/1000 | Loss: 0.00006175
Iteration 48/1000 | Loss: 0.00005041
Iteration 49/1000 | Loss: 0.00007816
Iteration 50/1000 | Loss: 0.00023971
Iteration 51/1000 | Loss: 0.00005954
Iteration 52/1000 | Loss: 0.00004719
Iteration 53/1000 | Loss: 0.00015797
Iteration 54/1000 | Loss: 0.00007366
Iteration 55/1000 | Loss: 0.00004655
Iteration 56/1000 | Loss: 0.00008276
Iteration 57/1000 | Loss: 0.00004614
Iteration 58/1000 | Loss: 0.00019963
Iteration 59/1000 | Loss: 0.00019963
Iteration 60/1000 | Loss: 0.00009089
Iteration 61/1000 | Loss: 0.00019883
Iteration 62/1000 | Loss: 0.00004770
Iteration 63/1000 | Loss: 0.00004603
Iteration 64/1000 | Loss: 0.00004576
Iteration 65/1000 | Loss: 0.00004565
Iteration 66/1000 | Loss: 0.00004565
Iteration 67/1000 | Loss: 0.00004564
Iteration 68/1000 | Loss: 0.00004564
Iteration 69/1000 | Loss: 0.00004563
Iteration 70/1000 | Loss: 0.00004563
Iteration 71/1000 | Loss: 0.00004561
Iteration 72/1000 | Loss: 0.00004561
Iteration 73/1000 | Loss: 0.00004561
Iteration 74/1000 | Loss: 0.00004560
Iteration 75/1000 | Loss: 0.00004560
Iteration 76/1000 | Loss: 0.00004560
Iteration 77/1000 | Loss: 0.00004560
Iteration 78/1000 | Loss: 0.00004560
Iteration 79/1000 | Loss: 0.00004559
Iteration 80/1000 | Loss: 0.00004559
Iteration 81/1000 | Loss: 0.00004559
Iteration 82/1000 | Loss: 0.00004558
Iteration 83/1000 | Loss: 0.00004557
Iteration 84/1000 | Loss: 0.00004557
Iteration 85/1000 | Loss: 0.00004556
Iteration 86/1000 | Loss: 0.00008322
Iteration 87/1000 | Loss: 0.00004881
Iteration 88/1000 | Loss: 0.00005272
Iteration 89/1000 | Loss: 0.00005013
Iteration 90/1000 | Loss: 0.00004550
Iteration 91/1000 | Loss: 0.00004925
Iteration 92/1000 | Loss: 0.00004547
Iteration 93/1000 | Loss: 0.00004544
Iteration 94/1000 | Loss: 0.00004544
Iteration 95/1000 | Loss: 0.00004543
Iteration 96/1000 | Loss: 0.00004543
Iteration 97/1000 | Loss: 0.00004543
Iteration 98/1000 | Loss: 0.00004543
Iteration 99/1000 | Loss: 0.00004543
Iteration 100/1000 | Loss: 0.00004543
Iteration 101/1000 | Loss: 0.00004543
Iteration 102/1000 | Loss: 0.00004543
Iteration 103/1000 | Loss: 0.00004542
Iteration 104/1000 | Loss: 0.00004542
Iteration 105/1000 | Loss: 0.00004542
Iteration 106/1000 | Loss: 0.00004542
Iteration 107/1000 | Loss: 0.00004542
Iteration 108/1000 | Loss: 0.00004542
Iteration 109/1000 | Loss: 0.00004541
Iteration 110/1000 | Loss: 0.00004541
Iteration 111/1000 | Loss: 0.00004537
Iteration 112/1000 | Loss: 0.00004534
Iteration 113/1000 | Loss: 0.00004533
Iteration 114/1000 | Loss: 0.00004533
Iteration 115/1000 | Loss: 0.00004533
Iteration 116/1000 | Loss: 0.00004532
Iteration 117/1000 | Loss: 0.00004532
Iteration 118/1000 | Loss: 0.00004532
Iteration 119/1000 | Loss: 0.00004532
Iteration 120/1000 | Loss: 0.00004532
Iteration 121/1000 | Loss: 0.00004532
Iteration 122/1000 | Loss: 0.00004532
Iteration 123/1000 | Loss: 0.00004532
Iteration 124/1000 | Loss: 0.00004532
Iteration 125/1000 | Loss: 0.00004532
Iteration 126/1000 | Loss: 0.00004531
Iteration 127/1000 | Loss: 0.00004531
Iteration 128/1000 | Loss: 0.00004531
Iteration 129/1000 | Loss: 0.00004531
Iteration 130/1000 | Loss: 0.00004531
Iteration 131/1000 | Loss: 0.00004531
Iteration 132/1000 | Loss: 0.00004530
Iteration 133/1000 | Loss: 0.00004530
Iteration 134/1000 | Loss: 0.00004530
Iteration 135/1000 | Loss: 0.00004530
Iteration 136/1000 | Loss: 0.00004530
Iteration 137/1000 | Loss: 0.00004530
Iteration 138/1000 | Loss: 0.00004529
Iteration 139/1000 | Loss: 0.00004529
Iteration 140/1000 | Loss: 0.00004529
Iteration 141/1000 | Loss: 0.00004529
Iteration 142/1000 | Loss: 0.00004528
Iteration 143/1000 | Loss: 0.00004528
Iteration 144/1000 | Loss: 0.00004528
Iteration 145/1000 | Loss: 0.00004528
Iteration 146/1000 | Loss: 0.00004528
Iteration 147/1000 | Loss: 0.00004528
Iteration 148/1000 | Loss: 0.00004527
Iteration 149/1000 | Loss: 0.00004527
Iteration 150/1000 | Loss: 0.00004527
Iteration 151/1000 | Loss: 0.00004527
Iteration 152/1000 | Loss: 0.00004527
Iteration 153/1000 | Loss: 0.00004527
Iteration 154/1000 | Loss: 0.00004527
Iteration 155/1000 | Loss: 0.00004527
Iteration 156/1000 | Loss: 0.00004527
Iteration 157/1000 | Loss: 0.00004527
Iteration 158/1000 | Loss: 0.00004527
Iteration 159/1000 | Loss: 0.00004527
Iteration 160/1000 | Loss: 0.00004527
Iteration 161/1000 | Loss: 0.00004527
Iteration 162/1000 | Loss: 0.00004527
Iteration 163/1000 | Loss: 0.00004527
Iteration 164/1000 | Loss: 0.00004527
Iteration 165/1000 | Loss: 0.00004527
Iteration 166/1000 | Loss: 0.00004527
Iteration 167/1000 | Loss: 0.00004527
Iteration 168/1000 | Loss: 0.00004527
Iteration 169/1000 | Loss: 0.00004527
Iteration 170/1000 | Loss: 0.00004527
Iteration 171/1000 | Loss: 0.00004527
Iteration 172/1000 | Loss: 0.00004527
Iteration 173/1000 | Loss: 0.00004527
Iteration 174/1000 | Loss: 0.00004527
Iteration 175/1000 | Loss: 0.00004527
Iteration 176/1000 | Loss: 0.00004527
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [4.5267555833561346e-05, 4.5267555833561346e-05, 4.5267555833561346e-05, 4.5267555833561346e-05, 4.5267555833561346e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.5267555833561346e-05

Optimization complete. Final v2v error: 5.088318824768066 mm

Highest mean error: 21.89409065246582 mm for frame 4

Lowest mean error: 4.4510016441345215 mm for frame 80

Saving results

Total time: 146.87366104125977
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_it_4274/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_it_4274/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_it_4274/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00801309
Iteration 2/25 | Loss: 0.00091260
Iteration 3/25 | Loss: 0.00075470
Iteration 4/25 | Loss: 0.00072273
Iteration 5/25 | Loss: 0.00071203
Iteration 6/25 | Loss: 0.00071017
Iteration 7/25 | Loss: 0.00071005
Iteration 8/25 | Loss: 0.00071005
Iteration 9/25 | Loss: 0.00071005
Iteration 10/25 | Loss: 0.00071005
Iteration 11/25 | Loss: 0.00071005
Iteration 12/25 | Loss: 0.00071005
Iteration 13/25 | Loss: 0.00071005
Iteration 14/25 | Loss: 0.00071005
Iteration 15/25 | Loss: 0.00071005
Iteration 16/25 | Loss: 0.00071005
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007100473158061504, 0.0007100473158061504, 0.0007100473158061504, 0.0007100473158061504, 0.0007100473158061504]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007100473158061504

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30059218
Iteration 2/25 | Loss: 0.00023727
Iteration 3/25 | Loss: 0.00023725
Iteration 4/25 | Loss: 0.00023725
Iteration 5/25 | Loss: 0.00023725
Iteration 6/25 | Loss: 0.00023725
Iteration 7/25 | Loss: 0.00023725
Iteration 8/25 | Loss: 0.00023725
Iteration 9/25 | Loss: 0.00023725
Iteration 10/25 | Loss: 0.00023725
Iteration 11/25 | Loss: 0.00023725
Iteration 12/25 | Loss: 0.00023725
Iteration 13/25 | Loss: 0.00023725
Iteration 14/25 | Loss: 0.00023725
Iteration 15/25 | Loss: 0.00023725
Iteration 16/25 | Loss: 0.00023725
Iteration 17/25 | Loss: 0.00023725
Iteration 18/25 | Loss: 0.00023725
Iteration 19/25 | Loss: 0.00023725
Iteration 20/25 | Loss: 0.00023725
Iteration 21/25 | Loss: 0.00023725
Iteration 22/25 | Loss: 0.00023725
Iteration 23/25 | Loss: 0.00023725
Iteration 24/25 | Loss: 0.00023725
Iteration 25/25 | Loss: 0.00023725

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00023725
Iteration 2/1000 | Loss: 0.00003439
Iteration 3/1000 | Loss: 0.00002748
Iteration 4/1000 | Loss: 0.00002406
Iteration 5/1000 | Loss: 0.00002313
Iteration 6/1000 | Loss: 0.00002223
Iteration 7/1000 | Loss: 0.00002179
Iteration 8/1000 | Loss: 0.00002126
Iteration 9/1000 | Loss: 0.00002099
Iteration 10/1000 | Loss: 0.00002079
Iteration 11/1000 | Loss: 0.00002074
Iteration 12/1000 | Loss: 0.00002066
Iteration 13/1000 | Loss: 0.00002065
Iteration 14/1000 | Loss: 0.00002061
Iteration 15/1000 | Loss: 0.00002060
Iteration 16/1000 | Loss: 0.00002058
Iteration 17/1000 | Loss: 0.00002057
Iteration 18/1000 | Loss: 0.00002054
Iteration 19/1000 | Loss: 0.00002054
Iteration 20/1000 | Loss: 0.00002054
Iteration 21/1000 | Loss: 0.00002054
Iteration 22/1000 | Loss: 0.00002054
Iteration 23/1000 | Loss: 0.00002054
Iteration 24/1000 | Loss: 0.00002054
Iteration 25/1000 | Loss: 0.00002054
Iteration 26/1000 | Loss: 0.00002054
Iteration 27/1000 | Loss: 0.00002053
Iteration 28/1000 | Loss: 0.00002053
Iteration 29/1000 | Loss: 0.00002053
Iteration 30/1000 | Loss: 0.00002053
Iteration 31/1000 | Loss: 0.00002053
Iteration 32/1000 | Loss: 0.00002053
Iteration 33/1000 | Loss: 0.00002053
Iteration 34/1000 | Loss: 0.00002053
Iteration 35/1000 | Loss: 0.00002052
Iteration 36/1000 | Loss: 0.00002052
Iteration 37/1000 | Loss: 0.00002052
Iteration 38/1000 | Loss: 0.00002051
Iteration 39/1000 | Loss: 0.00002051
Iteration 40/1000 | Loss: 0.00002051
Iteration 41/1000 | Loss: 0.00002050
Iteration 42/1000 | Loss: 0.00002050
Iteration 43/1000 | Loss: 0.00002049
Iteration 44/1000 | Loss: 0.00002049
Iteration 45/1000 | Loss: 0.00002049
Iteration 46/1000 | Loss: 0.00002049
Iteration 47/1000 | Loss: 0.00002049
Iteration 48/1000 | Loss: 0.00002048
Iteration 49/1000 | Loss: 0.00002048
Iteration 50/1000 | Loss: 0.00002048
Iteration 51/1000 | Loss: 0.00002048
Iteration 52/1000 | Loss: 0.00002048
Iteration 53/1000 | Loss: 0.00002047
Iteration 54/1000 | Loss: 0.00002047
Iteration 55/1000 | Loss: 0.00002047
Iteration 56/1000 | Loss: 0.00002046
Iteration 57/1000 | Loss: 0.00002046
Iteration 58/1000 | Loss: 0.00002046
Iteration 59/1000 | Loss: 0.00002046
Iteration 60/1000 | Loss: 0.00002046
Iteration 61/1000 | Loss: 0.00002045
Iteration 62/1000 | Loss: 0.00002045
Iteration 63/1000 | Loss: 0.00002045
Iteration 64/1000 | Loss: 0.00002044
Iteration 65/1000 | Loss: 0.00002044
Iteration 66/1000 | Loss: 0.00002044
Iteration 67/1000 | Loss: 0.00002044
Iteration 68/1000 | Loss: 0.00002044
Iteration 69/1000 | Loss: 0.00002044
Iteration 70/1000 | Loss: 0.00002044
Iteration 71/1000 | Loss: 0.00002044
Iteration 72/1000 | Loss: 0.00002044
Iteration 73/1000 | Loss: 0.00002043
Iteration 74/1000 | Loss: 0.00002043
Iteration 75/1000 | Loss: 0.00002043
Iteration 76/1000 | Loss: 0.00002043
Iteration 77/1000 | Loss: 0.00002043
Iteration 78/1000 | Loss: 0.00002043
Iteration 79/1000 | Loss: 0.00002042
Iteration 80/1000 | Loss: 0.00002042
Iteration 81/1000 | Loss: 0.00002042
Iteration 82/1000 | Loss: 0.00002042
Iteration 83/1000 | Loss: 0.00002042
Iteration 84/1000 | Loss: 0.00002042
Iteration 85/1000 | Loss: 0.00002042
Iteration 86/1000 | Loss: 0.00002042
Iteration 87/1000 | Loss: 0.00002042
Iteration 88/1000 | Loss: 0.00002042
Iteration 89/1000 | Loss: 0.00002042
Iteration 90/1000 | Loss: 0.00002042
Iteration 91/1000 | Loss: 0.00002042
Iteration 92/1000 | Loss: 0.00002041
Iteration 93/1000 | Loss: 0.00002041
Iteration 94/1000 | Loss: 0.00002041
Iteration 95/1000 | Loss: 0.00002040
Iteration 96/1000 | Loss: 0.00002040
Iteration 97/1000 | Loss: 0.00002040
Iteration 98/1000 | Loss: 0.00002040
Iteration 99/1000 | Loss: 0.00002040
Iteration 100/1000 | Loss: 0.00002040
Iteration 101/1000 | Loss: 0.00002040
Iteration 102/1000 | Loss: 0.00002040
Iteration 103/1000 | Loss: 0.00002040
Iteration 104/1000 | Loss: 0.00002040
Iteration 105/1000 | Loss: 0.00002040
Iteration 106/1000 | Loss: 0.00002040
Iteration 107/1000 | Loss: 0.00002040
Iteration 108/1000 | Loss: 0.00002040
Iteration 109/1000 | Loss: 0.00002040
Iteration 110/1000 | Loss: 0.00002040
Iteration 111/1000 | Loss: 0.00002040
Iteration 112/1000 | Loss: 0.00002040
Iteration 113/1000 | Loss: 0.00002040
Iteration 114/1000 | Loss: 0.00002040
Iteration 115/1000 | Loss: 0.00002040
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [2.0395642422954552e-05, 2.0395642422954552e-05, 2.0395642422954552e-05, 2.0395642422954552e-05, 2.0395642422954552e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0395642422954552e-05

Optimization complete. Final v2v error: 3.8761210441589355 mm

Highest mean error: 4.395035266876221 mm for frame 216

Lowest mean error: 3.561599016189575 mm for frame 113

Saving results

Total time: 37.14889597892761
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_it_4274/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_it_4274/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_it_4274/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00633388
Iteration 2/25 | Loss: 0.00098814
Iteration 3/25 | Loss: 0.00068095
Iteration 4/25 | Loss: 0.00064410
Iteration 5/25 | Loss: 0.00063839
Iteration 6/25 | Loss: 0.00063672
Iteration 7/25 | Loss: 0.00063635
Iteration 8/25 | Loss: 0.00063635
Iteration 9/25 | Loss: 0.00063635
Iteration 10/25 | Loss: 0.00063635
Iteration 11/25 | Loss: 0.00063635
Iteration 12/25 | Loss: 0.00063635
Iteration 13/25 | Loss: 0.00063635
Iteration 14/25 | Loss: 0.00063635
Iteration 15/25 | Loss: 0.00063634
Iteration 16/25 | Loss: 0.00063634
Iteration 17/25 | Loss: 0.00063634
Iteration 18/25 | Loss: 0.00063634
Iteration 19/25 | Loss: 0.00063634
Iteration 20/25 | Loss: 0.00063634
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006363439606502652, 0.0006363439606502652, 0.0006363439606502652, 0.0006363439606502652, 0.0006363439606502652]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006363439606502652

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.10967398
Iteration 2/25 | Loss: 0.00013278
Iteration 3/25 | Loss: 0.00013271
Iteration 4/25 | Loss: 0.00013271
Iteration 5/25 | Loss: 0.00013270
Iteration 6/25 | Loss: 0.00013270
Iteration 7/25 | Loss: 0.00013270
Iteration 8/25 | Loss: 0.00013270
Iteration 9/25 | Loss: 0.00013270
Iteration 10/25 | Loss: 0.00013270
Iteration 11/25 | Loss: 0.00013270
Iteration 12/25 | Loss: 0.00013270
Iteration 13/25 | Loss: 0.00013270
Iteration 14/25 | Loss: 0.00013270
Iteration 15/25 | Loss: 0.00013270
Iteration 16/25 | Loss: 0.00013270
Iteration 17/25 | Loss: 0.00013270
Iteration 18/25 | Loss: 0.00013270
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00013270271301735193, 0.00013270271301735193, 0.00013270271301735193, 0.00013270271301735193, 0.00013270271301735193]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00013270271301735193

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00013270
Iteration 2/1000 | Loss: 0.00002094
Iteration 3/1000 | Loss: 0.00001663
Iteration 4/1000 | Loss: 0.00001473
Iteration 5/1000 | Loss: 0.00001395
Iteration 6/1000 | Loss: 0.00001347
Iteration 7/1000 | Loss: 0.00001315
Iteration 8/1000 | Loss: 0.00001291
Iteration 9/1000 | Loss: 0.00001282
Iteration 10/1000 | Loss: 0.00001280
Iteration 11/1000 | Loss: 0.00001280
Iteration 12/1000 | Loss: 0.00001276
Iteration 13/1000 | Loss: 0.00001273
Iteration 14/1000 | Loss: 0.00001272
Iteration 15/1000 | Loss: 0.00001272
Iteration 16/1000 | Loss: 0.00001270
Iteration 17/1000 | Loss: 0.00001270
Iteration 18/1000 | Loss: 0.00001269
Iteration 19/1000 | Loss: 0.00001269
Iteration 20/1000 | Loss: 0.00001269
Iteration 21/1000 | Loss: 0.00001268
Iteration 22/1000 | Loss: 0.00001268
Iteration 23/1000 | Loss: 0.00001268
Iteration 24/1000 | Loss: 0.00001267
Iteration 25/1000 | Loss: 0.00001267
Iteration 26/1000 | Loss: 0.00001266
Iteration 27/1000 | Loss: 0.00001266
Iteration 28/1000 | Loss: 0.00001265
Iteration 29/1000 | Loss: 0.00001265
Iteration 30/1000 | Loss: 0.00001265
Iteration 31/1000 | Loss: 0.00001265
Iteration 32/1000 | Loss: 0.00001264
Iteration 33/1000 | Loss: 0.00001264
Iteration 34/1000 | Loss: 0.00001264
Iteration 35/1000 | Loss: 0.00001264
Iteration 36/1000 | Loss: 0.00001264
Iteration 37/1000 | Loss: 0.00001264
Iteration 38/1000 | Loss: 0.00001264
Iteration 39/1000 | Loss: 0.00001264
Iteration 40/1000 | Loss: 0.00001263
Iteration 41/1000 | Loss: 0.00001263
Iteration 42/1000 | Loss: 0.00001262
Iteration 43/1000 | Loss: 0.00001262
Iteration 44/1000 | Loss: 0.00001262
Iteration 45/1000 | Loss: 0.00001262
Iteration 46/1000 | Loss: 0.00001262
Iteration 47/1000 | Loss: 0.00001262
Iteration 48/1000 | Loss: 0.00001262
Iteration 49/1000 | Loss: 0.00001261
Iteration 50/1000 | Loss: 0.00001261
Iteration 51/1000 | Loss: 0.00001261
Iteration 52/1000 | Loss: 0.00001261
Iteration 53/1000 | Loss: 0.00001261
Iteration 54/1000 | Loss: 0.00001261
Iteration 55/1000 | Loss: 0.00001261
Iteration 56/1000 | Loss: 0.00001260
Iteration 57/1000 | Loss: 0.00001260
Iteration 58/1000 | Loss: 0.00001260
Iteration 59/1000 | Loss: 0.00001260
Iteration 60/1000 | Loss: 0.00001260
Iteration 61/1000 | Loss: 0.00001260
Iteration 62/1000 | Loss: 0.00001259
Iteration 63/1000 | Loss: 0.00001259
Iteration 64/1000 | Loss: 0.00001259
Iteration 65/1000 | Loss: 0.00001259
Iteration 66/1000 | Loss: 0.00001259
Iteration 67/1000 | Loss: 0.00001259
Iteration 68/1000 | Loss: 0.00001258
Iteration 69/1000 | Loss: 0.00001258
Iteration 70/1000 | Loss: 0.00001258
Iteration 71/1000 | Loss: 0.00001258
Iteration 72/1000 | Loss: 0.00001257
Iteration 73/1000 | Loss: 0.00001257
Iteration 74/1000 | Loss: 0.00001257
Iteration 75/1000 | Loss: 0.00001257
Iteration 76/1000 | Loss: 0.00001257
Iteration 77/1000 | Loss: 0.00001257
Iteration 78/1000 | Loss: 0.00001257
Iteration 79/1000 | Loss: 0.00001257
Iteration 80/1000 | Loss: 0.00001257
Iteration 81/1000 | Loss: 0.00001257
Iteration 82/1000 | Loss: 0.00001257
Iteration 83/1000 | Loss: 0.00001257
Iteration 84/1000 | Loss: 0.00001257
Iteration 85/1000 | Loss: 0.00001257
Iteration 86/1000 | Loss: 0.00001257
Iteration 87/1000 | Loss: 0.00001257
Iteration 88/1000 | Loss: 0.00001257
Iteration 89/1000 | Loss: 0.00001257
Iteration 90/1000 | Loss: 0.00001257
Iteration 91/1000 | Loss: 0.00001257
Iteration 92/1000 | Loss: 0.00001257
Iteration 93/1000 | Loss: 0.00001257
Iteration 94/1000 | Loss: 0.00001257
Iteration 95/1000 | Loss: 0.00001257
Iteration 96/1000 | Loss: 0.00001257
Iteration 97/1000 | Loss: 0.00001257
Iteration 98/1000 | Loss: 0.00001257
Iteration 99/1000 | Loss: 0.00001257
Iteration 100/1000 | Loss: 0.00001257
Iteration 101/1000 | Loss: 0.00001257
Iteration 102/1000 | Loss: 0.00001257
Iteration 103/1000 | Loss: 0.00001257
Iteration 104/1000 | Loss: 0.00001257
Iteration 105/1000 | Loss: 0.00001257
Iteration 106/1000 | Loss: 0.00001257
Iteration 107/1000 | Loss: 0.00001257
Iteration 108/1000 | Loss: 0.00001257
Iteration 109/1000 | Loss: 0.00001257
Iteration 110/1000 | Loss: 0.00001257
Iteration 111/1000 | Loss: 0.00001257
Iteration 112/1000 | Loss: 0.00001257
Iteration 113/1000 | Loss: 0.00001257
Iteration 114/1000 | Loss: 0.00001257
Iteration 115/1000 | Loss: 0.00001257
Iteration 116/1000 | Loss: 0.00001257
Iteration 117/1000 | Loss: 0.00001257
Iteration 118/1000 | Loss: 0.00001257
Iteration 119/1000 | Loss: 0.00001257
Iteration 120/1000 | Loss: 0.00001257
Iteration 121/1000 | Loss: 0.00001257
Iteration 122/1000 | Loss: 0.00001257
Iteration 123/1000 | Loss: 0.00001257
Iteration 124/1000 | Loss: 0.00001257
Iteration 125/1000 | Loss: 0.00001257
Iteration 126/1000 | Loss: 0.00001257
Iteration 127/1000 | Loss: 0.00001257
Iteration 128/1000 | Loss: 0.00001257
Iteration 129/1000 | Loss: 0.00001257
Iteration 130/1000 | Loss: 0.00001257
Iteration 131/1000 | Loss: 0.00001257
Iteration 132/1000 | Loss: 0.00001257
Iteration 133/1000 | Loss: 0.00001257
Iteration 134/1000 | Loss: 0.00001257
Iteration 135/1000 | Loss: 0.00001257
Iteration 136/1000 | Loss: 0.00001257
Iteration 137/1000 | Loss: 0.00001257
Iteration 138/1000 | Loss: 0.00001257
Iteration 139/1000 | Loss: 0.00001257
Iteration 140/1000 | Loss: 0.00001257
Iteration 141/1000 | Loss: 0.00001257
Iteration 142/1000 | Loss: 0.00001257
Iteration 143/1000 | Loss: 0.00001257
Iteration 144/1000 | Loss: 0.00001257
Iteration 145/1000 | Loss: 0.00001257
Iteration 146/1000 | Loss: 0.00001257
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.2569127648021095e-05, 1.2569127648021095e-05, 1.2569127648021095e-05, 1.2569127648021095e-05, 1.2569127648021095e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2569127648021095e-05

Optimization complete. Final v2v error: 3.0684938430786133 mm

Highest mean error: 3.8650875091552734 mm for frame 34

Lowest mean error: 2.688100814819336 mm for frame 119

Saving results

Total time: 30.593599319458008
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_it_4274/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_it_4274/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_it_4274/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00560909
Iteration 2/25 | Loss: 0.00094576
Iteration 3/25 | Loss: 0.00078406
Iteration 4/25 | Loss: 0.00074266
Iteration 5/25 | Loss: 0.00072898
Iteration 6/25 | Loss: 0.00072663
Iteration 7/25 | Loss: 0.00072617
Iteration 8/25 | Loss: 0.00072617
Iteration 9/25 | Loss: 0.00072617
Iteration 10/25 | Loss: 0.00072617
Iteration 11/25 | Loss: 0.00072617
Iteration 12/25 | Loss: 0.00072617
Iteration 13/25 | Loss: 0.00072617
Iteration 14/25 | Loss: 0.00072617
Iteration 15/25 | Loss: 0.00072617
Iteration 16/25 | Loss: 0.00072617
Iteration 17/25 | Loss: 0.00072617
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007261695573106408, 0.0007261695573106408, 0.0007261695573106408, 0.0007261695573106408, 0.0007261695573106408]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007261695573106408

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.91977668
Iteration 2/25 | Loss: 0.00022465
Iteration 3/25 | Loss: 0.00022464
Iteration 4/25 | Loss: 0.00022464
Iteration 5/25 | Loss: 0.00022464
Iteration 6/25 | Loss: 0.00022464
Iteration 7/25 | Loss: 0.00022464
Iteration 8/25 | Loss: 0.00022463
Iteration 9/25 | Loss: 0.00022463
Iteration 10/25 | Loss: 0.00022463
Iteration 11/25 | Loss: 0.00022463
Iteration 12/25 | Loss: 0.00022463
Iteration 13/25 | Loss: 0.00022463
Iteration 14/25 | Loss: 0.00022463
Iteration 15/25 | Loss: 0.00022463
Iteration 16/25 | Loss: 0.00022463
Iteration 17/25 | Loss: 0.00022463
Iteration 18/25 | Loss: 0.00022463
Iteration 19/25 | Loss: 0.00022463
Iteration 20/25 | Loss: 0.00022463
Iteration 21/25 | Loss: 0.00022463
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00022463408822659403, 0.00022463408822659403, 0.00022463408822659403, 0.00022463408822659403, 0.00022463408822659403]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00022463408822659403

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00022463
Iteration 2/1000 | Loss: 0.00004218
Iteration 3/1000 | Loss: 0.00003542
Iteration 4/1000 | Loss: 0.00003227
Iteration 5/1000 | Loss: 0.00003044
Iteration 6/1000 | Loss: 0.00002922
Iteration 7/1000 | Loss: 0.00002809
Iteration 8/1000 | Loss: 0.00002754
Iteration 9/1000 | Loss: 0.00002715
Iteration 10/1000 | Loss: 0.00002688
Iteration 11/1000 | Loss: 0.00002673
Iteration 12/1000 | Loss: 0.00002672
Iteration 13/1000 | Loss: 0.00002672
Iteration 14/1000 | Loss: 0.00002670
Iteration 15/1000 | Loss: 0.00002667
Iteration 16/1000 | Loss: 0.00002667
Iteration 17/1000 | Loss: 0.00002666
Iteration 18/1000 | Loss: 0.00002666
Iteration 19/1000 | Loss: 0.00002665
Iteration 20/1000 | Loss: 0.00002662
Iteration 21/1000 | Loss: 0.00002661
Iteration 22/1000 | Loss: 0.00002661
Iteration 23/1000 | Loss: 0.00002661
Iteration 24/1000 | Loss: 0.00002660
Iteration 25/1000 | Loss: 0.00002660
Iteration 26/1000 | Loss: 0.00002660
Iteration 27/1000 | Loss: 0.00002660
Iteration 28/1000 | Loss: 0.00002657
Iteration 29/1000 | Loss: 0.00002656
Iteration 30/1000 | Loss: 0.00002655
Iteration 31/1000 | Loss: 0.00002655
Iteration 32/1000 | Loss: 0.00002653
Iteration 33/1000 | Loss: 0.00002651
Iteration 34/1000 | Loss: 0.00002650
Iteration 35/1000 | Loss: 0.00002649
Iteration 36/1000 | Loss: 0.00002649
Iteration 37/1000 | Loss: 0.00002649
Iteration 38/1000 | Loss: 0.00002648
Iteration 39/1000 | Loss: 0.00002648
Iteration 40/1000 | Loss: 0.00002646
Iteration 41/1000 | Loss: 0.00002643
Iteration 42/1000 | Loss: 0.00002643
Iteration 43/1000 | Loss: 0.00002642
Iteration 44/1000 | Loss: 0.00002642
Iteration 45/1000 | Loss: 0.00002642
Iteration 46/1000 | Loss: 0.00002642
Iteration 47/1000 | Loss: 0.00002642
Iteration 48/1000 | Loss: 0.00002640
Iteration 49/1000 | Loss: 0.00002640
Iteration 50/1000 | Loss: 0.00002640
Iteration 51/1000 | Loss: 0.00002640
Iteration 52/1000 | Loss: 0.00002640
Iteration 53/1000 | Loss: 0.00002639
Iteration 54/1000 | Loss: 0.00002639
Iteration 55/1000 | Loss: 0.00002639
Iteration 56/1000 | Loss: 0.00002639
Iteration 57/1000 | Loss: 0.00002639
Iteration 58/1000 | Loss: 0.00002639
Iteration 59/1000 | Loss: 0.00002639
Iteration 60/1000 | Loss: 0.00002639
Iteration 61/1000 | Loss: 0.00002638
Iteration 62/1000 | Loss: 0.00002638
Iteration 63/1000 | Loss: 0.00002638
Iteration 64/1000 | Loss: 0.00002638
Iteration 65/1000 | Loss: 0.00002638
Iteration 66/1000 | Loss: 0.00002638
Iteration 67/1000 | Loss: 0.00002638
Iteration 68/1000 | Loss: 0.00002638
Iteration 69/1000 | Loss: 0.00002638
Iteration 70/1000 | Loss: 0.00002638
Iteration 71/1000 | Loss: 0.00002638
Iteration 72/1000 | Loss: 0.00002637
Iteration 73/1000 | Loss: 0.00002637
Iteration 74/1000 | Loss: 0.00002637
Iteration 75/1000 | Loss: 0.00002636
Iteration 76/1000 | Loss: 0.00002636
Iteration 77/1000 | Loss: 0.00002636
Iteration 78/1000 | Loss: 0.00002636
Iteration 79/1000 | Loss: 0.00002636
Iteration 80/1000 | Loss: 0.00002636
Iteration 81/1000 | Loss: 0.00002636
Iteration 82/1000 | Loss: 0.00002636
Iteration 83/1000 | Loss: 0.00002636
Iteration 84/1000 | Loss: 0.00002636
Iteration 85/1000 | Loss: 0.00002636
Iteration 86/1000 | Loss: 0.00002636
Iteration 87/1000 | Loss: 0.00002636
Iteration 88/1000 | Loss: 0.00002636
Iteration 89/1000 | Loss: 0.00002636
Iteration 90/1000 | Loss: 0.00002635
Iteration 91/1000 | Loss: 0.00002635
Iteration 92/1000 | Loss: 0.00002635
Iteration 93/1000 | Loss: 0.00002635
Iteration 94/1000 | Loss: 0.00002635
Iteration 95/1000 | Loss: 0.00002635
Iteration 96/1000 | Loss: 0.00002635
Iteration 97/1000 | Loss: 0.00002634
Iteration 98/1000 | Loss: 0.00002634
Iteration 99/1000 | Loss: 0.00002634
Iteration 100/1000 | Loss: 0.00002634
Iteration 101/1000 | Loss: 0.00002634
Iteration 102/1000 | Loss: 0.00002634
Iteration 103/1000 | Loss: 0.00002633
Iteration 104/1000 | Loss: 0.00002633
Iteration 105/1000 | Loss: 0.00002633
Iteration 106/1000 | Loss: 0.00002633
Iteration 107/1000 | Loss: 0.00002633
Iteration 108/1000 | Loss: 0.00002633
Iteration 109/1000 | Loss: 0.00002633
Iteration 110/1000 | Loss: 0.00002633
Iteration 111/1000 | Loss: 0.00002633
Iteration 112/1000 | Loss: 0.00002633
Iteration 113/1000 | Loss: 0.00002633
Iteration 114/1000 | Loss: 0.00002633
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [2.6332536435802467e-05, 2.6332536435802467e-05, 2.6332536435802467e-05, 2.6332536435802467e-05, 2.6332536435802467e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6332536435802467e-05

Optimization complete. Final v2v error: 4.295531749725342 mm

Highest mean error: 5.007991790771484 mm for frame 0

Lowest mean error: 3.546060085296631 mm for frame 89

Saving results

Total time: 36.65018606185913
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_it_4274/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_it_4274/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_it_4274/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00446836
Iteration 2/25 | Loss: 0.00083517
Iteration 3/25 | Loss: 0.00073018
Iteration 4/25 | Loss: 0.00069881
Iteration 5/25 | Loss: 0.00069409
Iteration 6/25 | Loss: 0.00069371
Iteration 7/25 | Loss: 0.00069371
Iteration 8/25 | Loss: 0.00069371
Iteration 9/25 | Loss: 0.00069371
Iteration 10/25 | Loss: 0.00069371
Iteration 11/25 | Loss: 0.00069371
Iteration 12/25 | Loss: 0.00069371
Iteration 13/25 | Loss: 0.00069371
Iteration 14/25 | Loss: 0.00069371
Iteration 15/25 | Loss: 0.00069371
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0006937134894542396, 0.0006937134894542396, 0.0006937134894542396, 0.0006937134894542396, 0.0006937134894542396]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006937134894542396

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39956605
Iteration 2/25 | Loss: 0.00020600
Iteration 3/25 | Loss: 0.00020600
Iteration 4/25 | Loss: 0.00020600
Iteration 5/25 | Loss: 0.00020600
Iteration 6/25 | Loss: 0.00020600
Iteration 7/25 | Loss: 0.00020600
Iteration 8/25 | Loss: 0.00020600
Iteration 9/25 | Loss: 0.00020600
Iteration 10/25 | Loss: 0.00020600
Iteration 11/25 | Loss: 0.00020600
Iteration 12/25 | Loss: 0.00020600
Iteration 13/25 | Loss: 0.00020600
Iteration 14/25 | Loss: 0.00020600
Iteration 15/25 | Loss: 0.00020600
Iteration 16/25 | Loss: 0.00020600
Iteration 17/25 | Loss: 0.00020600
Iteration 18/25 | Loss: 0.00020600
Iteration 19/25 | Loss: 0.00020600
Iteration 20/25 | Loss: 0.00020600
Iteration 21/25 | Loss: 0.00020600
Iteration 22/25 | Loss: 0.00020600
Iteration 23/25 | Loss: 0.00020600
Iteration 24/25 | Loss: 0.00020600
Iteration 25/25 | Loss: 0.00020600

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00020600
Iteration 2/1000 | Loss: 0.00003719
Iteration 3/1000 | Loss: 0.00003108
Iteration 4/1000 | Loss: 0.00002896
Iteration 5/1000 | Loss: 0.00002732
Iteration 6/1000 | Loss: 0.00002646
Iteration 7/1000 | Loss: 0.00002557
Iteration 8/1000 | Loss: 0.00002528
Iteration 9/1000 | Loss: 0.00002504
Iteration 10/1000 | Loss: 0.00002494
Iteration 11/1000 | Loss: 0.00002493
Iteration 12/1000 | Loss: 0.00002482
Iteration 13/1000 | Loss: 0.00002482
Iteration 14/1000 | Loss: 0.00002480
Iteration 15/1000 | Loss: 0.00002479
Iteration 16/1000 | Loss: 0.00002479
Iteration 17/1000 | Loss: 0.00002478
Iteration 18/1000 | Loss: 0.00002478
Iteration 19/1000 | Loss: 0.00002477
Iteration 20/1000 | Loss: 0.00002477
Iteration 21/1000 | Loss: 0.00002474
Iteration 22/1000 | Loss: 0.00002474
Iteration 23/1000 | Loss: 0.00002474
Iteration 24/1000 | Loss: 0.00002473
Iteration 25/1000 | Loss: 0.00002473
Iteration 26/1000 | Loss: 0.00002472
Iteration 27/1000 | Loss: 0.00002471
Iteration 28/1000 | Loss: 0.00002470
Iteration 29/1000 | Loss: 0.00002467
Iteration 30/1000 | Loss: 0.00002467
Iteration 31/1000 | Loss: 0.00002467
Iteration 32/1000 | Loss: 0.00002467
Iteration 33/1000 | Loss: 0.00002465
Iteration 34/1000 | Loss: 0.00002463
Iteration 35/1000 | Loss: 0.00002462
Iteration 36/1000 | Loss: 0.00002462
Iteration 37/1000 | Loss: 0.00002462
Iteration 38/1000 | Loss: 0.00002462
Iteration 39/1000 | Loss: 0.00002462
Iteration 40/1000 | Loss: 0.00002462
Iteration 41/1000 | Loss: 0.00002462
Iteration 42/1000 | Loss: 0.00002461
Iteration 43/1000 | Loss: 0.00002461
Iteration 44/1000 | Loss: 0.00002461
Iteration 45/1000 | Loss: 0.00002461
Iteration 46/1000 | Loss: 0.00002461
Iteration 47/1000 | Loss: 0.00002461
Iteration 48/1000 | Loss: 0.00002460
Iteration 49/1000 | Loss: 0.00002460
Iteration 50/1000 | Loss: 0.00002460
Iteration 51/1000 | Loss: 0.00002459
Iteration 52/1000 | Loss: 0.00002459
Iteration 53/1000 | Loss: 0.00002459
Iteration 54/1000 | Loss: 0.00002458
Iteration 55/1000 | Loss: 0.00002458
Iteration 56/1000 | Loss: 0.00002458
Iteration 57/1000 | Loss: 0.00002458
Iteration 58/1000 | Loss: 0.00002457
Iteration 59/1000 | Loss: 0.00002457
Iteration 60/1000 | Loss: 0.00002456
Iteration 61/1000 | Loss: 0.00002456
Iteration 62/1000 | Loss: 0.00002456
Iteration 63/1000 | Loss: 0.00002456
Iteration 64/1000 | Loss: 0.00002456
Iteration 65/1000 | Loss: 0.00002456
Iteration 66/1000 | Loss: 0.00002456
Iteration 67/1000 | Loss: 0.00002456
Iteration 68/1000 | Loss: 0.00002456
Iteration 69/1000 | Loss: 0.00002456
Iteration 70/1000 | Loss: 0.00002455
Iteration 71/1000 | Loss: 0.00002455
Iteration 72/1000 | Loss: 0.00002455
Iteration 73/1000 | Loss: 0.00002455
Iteration 74/1000 | Loss: 0.00002454
Iteration 75/1000 | Loss: 0.00002453
Iteration 76/1000 | Loss: 0.00002453
Iteration 77/1000 | Loss: 0.00002452
Iteration 78/1000 | Loss: 0.00002452
Iteration 79/1000 | Loss: 0.00002452
Iteration 80/1000 | Loss: 0.00002451
Iteration 81/1000 | Loss: 0.00002451
Iteration 82/1000 | Loss: 0.00002451
Iteration 83/1000 | Loss: 0.00002450
Iteration 84/1000 | Loss: 0.00002450
Iteration 85/1000 | Loss: 0.00002449
Iteration 86/1000 | Loss: 0.00002449
Iteration 87/1000 | Loss: 0.00002449
Iteration 88/1000 | Loss: 0.00002449
Iteration 89/1000 | Loss: 0.00002448
Iteration 90/1000 | Loss: 0.00002448
Iteration 91/1000 | Loss: 0.00002448
Iteration 92/1000 | Loss: 0.00002448
Iteration 93/1000 | Loss: 0.00002448
Iteration 94/1000 | Loss: 0.00002448
Iteration 95/1000 | Loss: 0.00002448
Iteration 96/1000 | Loss: 0.00002448
Iteration 97/1000 | Loss: 0.00002448
Iteration 98/1000 | Loss: 0.00002448
Iteration 99/1000 | Loss: 0.00002448
Iteration 100/1000 | Loss: 0.00002448
Iteration 101/1000 | Loss: 0.00002448
Iteration 102/1000 | Loss: 0.00002447
Iteration 103/1000 | Loss: 0.00002447
Iteration 104/1000 | Loss: 0.00002447
Iteration 105/1000 | Loss: 0.00002447
Iteration 106/1000 | Loss: 0.00002447
Iteration 107/1000 | Loss: 0.00002447
Iteration 108/1000 | Loss: 0.00002447
Iteration 109/1000 | Loss: 0.00002447
Iteration 110/1000 | Loss: 0.00002447
Iteration 111/1000 | Loss: 0.00002447
Iteration 112/1000 | Loss: 0.00002447
Iteration 113/1000 | Loss: 0.00002447
Iteration 114/1000 | Loss: 0.00002447
Iteration 115/1000 | Loss: 0.00002447
Iteration 116/1000 | Loss: 0.00002447
Iteration 117/1000 | Loss: 0.00002447
Iteration 118/1000 | Loss: 0.00002447
Iteration 119/1000 | Loss: 0.00002446
Iteration 120/1000 | Loss: 0.00002446
Iteration 121/1000 | Loss: 0.00002446
Iteration 122/1000 | Loss: 0.00002446
Iteration 123/1000 | Loss: 0.00002446
Iteration 124/1000 | Loss: 0.00002445
Iteration 125/1000 | Loss: 0.00002445
Iteration 126/1000 | Loss: 0.00002445
Iteration 127/1000 | Loss: 0.00002445
Iteration 128/1000 | Loss: 0.00002445
Iteration 129/1000 | Loss: 0.00002445
Iteration 130/1000 | Loss: 0.00002445
Iteration 131/1000 | Loss: 0.00002445
Iteration 132/1000 | Loss: 0.00002445
Iteration 133/1000 | Loss: 0.00002445
Iteration 134/1000 | Loss: 0.00002445
Iteration 135/1000 | Loss: 0.00002445
Iteration 136/1000 | Loss: 0.00002445
Iteration 137/1000 | Loss: 0.00002445
Iteration 138/1000 | Loss: 0.00002445
Iteration 139/1000 | Loss: 0.00002445
Iteration 140/1000 | Loss: 0.00002445
Iteration 141/1000 | Loss: 0.00002445
Iteration 142/1000 | Loss: 0.00002445
Iteration 143/1000 | Loss: 0.00002445
Iteration 144/1000 | Loss: 0.00002445
Iteration 145/1000 | Loss: 0.00002445
Iteration 146/1000 | Loss: 0.00002445
Iteration 147/1000 | Loss: 0.00002445
Iteration 148/1000 | Loss: 0.00002445
Iteration 149/1000 | Loss: 0.00002445
Iteration 150/1000 | Loss: 0.00002445
Iteration 151/1000 | Loss: 0.00002445
Iteration 152/1000 | Loss: 0.00002445
Iteration 153/1000 | Loss: 0.00002445
Iteration 154/1000 | Loss: 0.00002445
Iteration 155/1000 | Loss: 0.00002445
Iteration 156/1000 | Loss: 0.00002445
Iteration 157/1000 | Loss: 0.00002445
Iteration 158/1000 | Loss: 0.00002445
Iteration 159/1000 | Loss: 0.00002445
Iteration 160/1000 | Loss: 0.00002445
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [2.4445358576485887e-05, 2.4445358576485887e-05, 2.4445358576485887e-05, 2.4445358576485887e-05, 2.4445358576485887e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4445358576485887e-05

Optimization complete. Final v2v error: 4.21303129196167 mm

Highest mean error: 4.65946102142334 mm for frame 80

Lowest mean error: 3.8809256553649902 mm for frame 30

Saving results

Total time: 33.81894588470459
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_it_4274/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_it_4274/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_it_4274/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00888435
Iteration 2/25 | Loss: 0.00129639
Iteration 3/25 | Loss: 0.00090064
Iteration 4/25 | Loss: 0.00081621
Iteration 5/25 | Loss: 0.00079546
Iteration 6/25 | Loss: 0.00079224
Iteration 7/25 | Loss: 0.00079158
Iteration 8/25 | Loss: 0.00079158
Iteration 9/25 | Loss: 0.00079158
Iteration 10/25 | Loss: 0.00079158
Iteration 11/25 | Loss: 0.00079158
Iteration 12/25 | Loss: 0.00079158
Iteration 13/25 | Loss: 0.00079158
Iteration 14/25 | Loss: 0.00079158
Iteration 15/25 | Loss: 0.00079158
Iteration 16/25 | Loss: 0.00079158
Iteration 17/25 | Loss: 0.00079158
Iteration 18/25 | Loss: 0.00079158
Iteration 19/25 | Loss: 0.00079158
Iteration 20/25 | Loss: 0.00079158
Iteration 21/25 | Loss: 0.00079158
Iteration 22/25 | Loss: 0.00079158
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007915776805020869, 0.0007915776805020869, 0.0007915776805020869, 0.0007915776805020869, 0.0007915776805020869]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007915776805020869

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36022019
Iteration 2/25 | Loss: 0.00021972
Iteration 3/25 | Loss: 0.00021969
Iteration 4/25 | Loss: 0.00021969
Iteration 5/25 | Loss: 0.00021969
Iteration 6/25 | Loss: 0.00021969
Iteration 7/25 | Loss: 0.00021968
Iteration 8/25 | Loss: 0.00021968
Iteration 9/25 | Loss: 0.00021968
Iteration 10/25 | Loss: 0.00021968
Iteration 11/25 | Loss: 0.00021968
Iteration 12/25 | Loss: 0.00021968
Iteration 13/25 | Loss: 0.00021968
Iteration 14/25 | Loss: 0.00021968
Iteration 15/25 | Loss: 0.00021968
Iteration 16/25 | Loss: 0.00021968
Iteration 17/25 | Loss: 0.00021968
Iteration 18/25 | Loss: 0.00021968
Iteration 19/25 | Loss: 0.00021968
Iteration 20/25 | Loss: 0.00021968
Iteration 21/25 | Loss: 0.00021968
Iteration 22/25 | Loss: 0.00021968
Iteration 23/25 | Loss: 0.00021968
Iteration 24/25 | Loss: 0.00021968
Iteration 25/25 | Loss: 0.00021968

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00021968
Iteration 2/1000 | Loss: 0.00004155
Iteration 3/1000 | Loss: 0.00003370
Iteration 4/1000 | Loss: 0.00003134
Iteration 5/1000 | Loss: 0.00003022
Iteration 6/1000 | Loss: 0.00002908
Iteration 7/1000 | Loss: 0.00002841
Iteration 8/1000 | Loss: 0.00002764
Iteration 9/1000 | Loss: 0.00002728
Iteration 10/1000 | Loss: 0.00002708
Iteration 11/1000 | Loss: 0.00002705
Iteration 12/1000 | Loss: 0.00002698
Iteration 13/1000 | Loss: 0.00002684
Iteration 14/1000 | Loss: 0.00002684
Iteration 15/1000 | Loss: 0.00002683
Iteration 16/1000 | Loss: 0.00002676
Iteration 17/1000 | Loss: 0.00002672
Iteration 18/1000 | Loss: 0.00002670
Iteration 19/1000 | Loss: 0.00002668
Iteration 20/1000 | Loss: 0.00002668
Iteration 21/1000 | Loss: 0.00002667
Iteration 22/1000 | Loss: 0.00002667
Iteration 23/1000 | Loss: 0.00002666
Iteration 24/1000 | Loss: 0.00002665
Iteration 25/1000 | Loss: 0.00002663
Iteration 26/1000 | Loss: 0.00002663
Iteration 27/1000 | Loss: 0.00002663
Iteration 28/1000 | Loss: 0.00002662
Iteration 29/1000 | Loss: 0.00002662
Iteration 30/1000 | Loss: 0.00002661
Iteration 31/1000 | Loss: 0.00002661
Iteration 32/1000 | Loss: 0.00002660
Iteration 33/1000 | Loss: 0.00002660
Iteration 34/1000 | Loss: 0.00002660
Iteration 35/1000 | Loss: 0.00002659
Iteration 36/1000 | Loss: 0.00002659
Iteration 37/1000 | Loss: 0.00002659
Iteration 38/1000 | Loss: 0.00002659
Iteration 39/1000 | Loss: 0.00002659
Iteration 40/1000 | Loss: 0.00002659
Iteration 41/1000 | Loss: 0.00002659
Iteration 42/1000 | Loss: 0.00002659
Iteration 43/1000 | Loss: 0.00002659
Iteration 44/1000 | Loss: 0.00002658
Iteration 45/1000 | Loss: 0.00002658
Iteration 46/1000 | Loss: 0.00002658
Iteration 47/1000 | Loss: 0.00002658
Iteration 48/1000 | Loss: 0.00002658
Iteration 49/1000 | Loss: 0.00002658
Iteration 50/1000 | Loss: 0.00002657
Iteration 51/1000 | Loss: 0.00002656
Iteration 52/1000 | Loss: 0.00002656
Iteration 53/1000 | Loss: 0.00002656
Iteration 54/1000 | Loss: 0.00002655
Iteration 55/1000 | Loss: 0.00002655
Iteration 56/1000 | Loss: 0.00002655
Iteration 57/1000 | Loss: 0.00002654
Iteration 58/1000 | Loss: 0.00002654
Iteration 59/1000 | Loss: 0.00002653
Iteration 60/1000 | Loss: 0.00002653
Iteration 61/1000 | Loss: 0.00002653
Iteration 62/1000 | Loss: 0.00002653
Iteration 63/1000 | Loss: 0.00002653
Iteration 64/1000 | Loss: 0.00002653
Iteration 65/1000 | Loss: 0.00002653
Iteration 66/1000 | Loss: 0.00002652
Iteration 67/1000 | Loss: 0.00002652
Iteration 68/1000 | Loss: 0.00002652
Iteration 69/1000 | Loss: 0.00002652
Iteration 70/1000 | Loss: 0.00002652
Iteration 71/1000 | Loss: 0.00002651
Iteration 72/1000 | Loss: 0.00002651
Iteration 73/1000 | Loss: 0.00002651
Iteration 74/1000 | Loss: 0.00002651
Iteration 75/1000 | Loss: 0.00002650
Iteration 76/1000 | Loss: 0.00002650
Iteration 77/1000 | Loss: 0.00002650
Iteration 78/1000 | Loss: 0.00002650
Iteration 79/1000 | Loss: 0.00002650
Iteration 80/1000 | Loss: 0.00002650
Iteration 81/1000 | Loss: 0.00002649
Iteration 82/1000 | Loss: 0.00002649
Iteration 83/1000 | Loss: 0.00002649
Iteration 84/1000 | Loss: 0.00002649
Iteration 85/1000 | Loss: 0.00002649
Iteration 86/1000 | Loss: 0.00002649
Iteration 87/1000 | Loss: 0.00002649
Iteration 88/1000 | Loss: 0.00002649
Iteration 89/1000 | Loss: 0.00002649
Iteration 90/1000 | Loss: 0.00002649
Iteration 91/1000 | Loss: 0.00002648
Iteration 92/1000 | Loss: 0.00002648
Iteration 93/1000 | Loss: 0.00002648
Iteration 94/1000 | Loss: 0.00002647
Iteration 95/1000 | Loss: 0.00002647
Iteration 96/1000 | Loss: 0.00002647
Iteration 97/1000 | Loss: 0.00002647
Iteration 98/1000 | Loss: 0.00002647
Iteration 99/1000 | Loss: 0.00002646
Iteration 100/1000 | Loss: 0.00002646
Iteration 101/1000 | Loss: 0.00002646
Iteration 102/1000 | Loss: 0.00002646
Iteration 103/1000 | Loss: 0.00002645
Iteration 104/1000 | Loss: 0.00002645
Iteration 105/1000 | Loss: 0.00002644
Iteration 106/1000 | Loss: 0.00002644
Iteration 107/1000 | Loss: 0.00002644
Iteration 108/1000 | Loss: 0.00002643
Iteration 109/1000 | Loss: 0.00002643
Iteration 110/1000 | Loss: 0.00002641
Iteration 111/1000 | Loss: 0.00002641
Iteration 112/1000 | Loss: 0.00002641
Iteration 113/1000 | Loss: 0.00002640
Iteration 114/1000 | Loss: 0.00002640
Iteration 115/1000 | Loss: 0.00002640
Iteration 116/1000 | Loss: 0.00002640
Iteration 117/1000 | Loss: 0.00002640
Iteration 118/1000 | Loss: 0.00002640
Iteration 119/1000 | Loss: 0.00002640
Iteration 120/1000 | Loss: 0.00002640
Iteration 121/1000 | Loss: 0.00002640
Iteration 122/1000 | Loss: 0.00002640
Iteration 123/1000 | Loss: 0.00002640
Iteration 124/1000 | Loss: 0.00002640
Iteration 125/1000 | Loss: 0.00002639
Iteration 126/1000 | Loss: 0.00002639
Iteration 127/1000 | Loss: 0.00002639
Iteration 128/1000 | Loss: 0.00002639
Iteration 129/1000 | Loss: 0.00002639
Iteration 130/1000 | Loss: 0.00002639
Iteration 131/1000 | Loss: 0.00002639
Iteration 132/1000 | Loss: 0.00002639
Iteration 133/1000 | Loss: 0.00002638
Iteration 134/1000 | Loss: 0.00002638
Iteration 135/1000 | Loss: 0.00002638
Iteration 136/1000 | Loss: 0.00002638
Iteration 137/1000 | Loss: 0.00002637
Iteration 138/1000 | Loss: 0.00002637
Iteration 139/1000 | Loss: 0.00002637
Iteration 140/1000 | Loss: 0.00002637
Iteration 141/1000 | Loss: 0.00002637
Iteration 142/1000 | Loss: 0.00002637
Iteration 143/1000 | Loss: 0.00002637
Iteration 144/1000 | Loss: 0.00002637
Iteration 145/1000 | Loss: 0.00002637
Iteration 146/1000 | Loss: 0.00002637
Iteration 147/1000 | Loss: 0.00002637
Iteration 148/1000 | Loss: 0.00002636
Iteration 149/1000 | Loss: 0.00002636
Iteration 150/1000 | Loss: 0.00002636
Iteration 151/1000 | Loss: 0.00002636
Iteration 152/1000 | Loss: 0.00002636
Iteration 153/1000 | Loss: 0.00002636
Iteration 154/1000 | Loss: 0.00002635
Iteration 155/1000 | Loss: 0.00002635
Iteration 156/1000 | Loss: 0.00002635
Iteration 157/1000 | Loss: 0.00002635
Iteration 158/1000 | Loss: 0.00002635
Iteration 159/1000 | Loss: 0.00002635
Iteration 160/1000 | Loss: 0.00002635
Iteration 161/1000 | Loss: 0.00002635
Iteration 162/1000 | Loss: 0.00002635
Iteration 163/1000 | Loss: 0.00002635
Iteration 164/1000 | Loss: 0.00002635
Iteration 165/1000 | Loss: 0.00002635
Iteration 166/1000 | Loss: 0.00002635
Iteration 167/1000 | Loss: 0.00002635
Iteration 168/1000 | Loss: 0.00002635
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [2.6351191991125233e-05, 2.6351191991125233e-05, 2.6351191991125233e-05, 2.6351191991125233e-05, 2.6351191991125233e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6351191991125233e-05

Optimization complete. Final v2v error: 4.237478256225586 mm

Highest mean error: 4.942403316497803 mm for frame 32

Lowest mean error: 3.534507989883423 mm for frame 71

Saving results

Total time: 45.43582105636597
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_it_4274/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_it_4274/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_it_4274/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00507208
Iteration 2/25 | Loss: 0.00112362
Iteration 3/25 | Loss: 0.00086040
Iteration 4/25 | Loss: 0.00081325
Iteration 5/25 | Loss: 0.00080405
Iteration 6/25 | Loss: 0.00080307
Iteration 7/25 | Loss: 0.00080287
Iteration 8/25 | Loss: 0.00080287
Iteration 9/25 | Loss: 0.00080287
Iteration 10/25 | Loss: 0.00080287
Iteration 11/25 | Loss: 0.00080287
Iteration 12/25 | Loss: 0.00080287
Iteration 13/25 | Loss: 0.00080287
Iteration 14/25 | Loss: 0.00080287
Iteration 15/25 | Loss: 0.00080287
Iteration 16/25 | Loss: 0.00080287
Iteration 17/25 | Loss: 0.00080287
Iteration 18/25 | Loss: 0.00080287
Iteration 19/25 | Loss: 0.00080287
Iteration 20/25 | Loss: 0.00080287
Iteration 21/25 | Loss: 0.00080287
Iteration 22/25 | Loss: 0.00080287
Iteration 23/25 | Loss: 0.00080287
Iteration 24/25 | Loss: 0.00080287
Iteration 25/25 | Loss: 0.00080287

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.97270709
Iteration 2/25 | Loss: 0.00021975
Iteration 3/25 | Loss: 0.00021970
Iteration 4/25 | Loss: 0.00021970
Iteration 5/25 | Loss: 0.00021970
Iteration 6/25 | Loss: 0.00021970
Iteration 7/25 | Loss: 0.00021970
Iteration 8/25 | Loss: 0.00021970
Iteration 9/25 | Loss: 0.00021970
Iteration 10/25 | Loss: 0.00021970
Iteration 11/25 | Loss: 0.00021970
Iteration 12/25 | Loss: 0.00021970
Iteration 13/25 | Loss: 0.00021970
Iteration 14/25 | Loss: 0.00021970
Iteration 15/25 | Loss: 0.00021970
Iteration 16/25 | Loss: 0.00021970
Iteration 17/25 | Loss: 0.00021970
Iteration 18/25 | Loss: 0.00021970
Iteration 19/25 | Loss: 0.00021970
Iteration 20/25 | Loss: 0.00021970
Iteration 21/25 | Loss: 0.00021970
Iteration 22/25 | Loss: 0.00021970
Iteration 23/25 | Loss: 0.00021970
Iteration 24/25 | Loss: 0.00021970
Iteration 25/25 | Loss: 0.00021970
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0002196991554228589, 0.0002196991554228589, 0.0002196991554228589, 0.0002196991554228589, 0.0002196991554228589]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002196991554228589

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00021970
Iteration 2/1000 | Loss: 0.00004622
Iteration 3/1000 | Loss: 0.00003295
Iteration 4/1000 | Loss: 0.00002692
Iteration 5/1000 | Loss: 0.00002576
Iteration 6/1000 | Loss: 0.00002512
Iteration 7/1000 | Loss: 0.00002449
Iteration 8/1000 | Loss: 0.00002388
Iteration 9/1000 | Loss: 0.00002347
Iteration 10/1000 | Loss: 0.00002318
Iteration 11/1000 | Loss: 0.00002303
Iteration 12/1000 | Loss: 0.00002302
Iteration 13/1000 | Loss: 0.00002275
Iteration 14/1000 | Loss: 0.00002260
Iteration 15/1000 | Loss: 0.00002259
Iteration 16/1000 | Loss: 0.00002256
Iteration 17/1000 | Loss: 0.00002255
Iteration 18/1000 | Loss: 0.00002254
Iteration 19/1000 | Loss: 0.00002253
Iteration 20/1000 | Loss: 0.00002253
Iteration 21/1000 | Loss: 0.00002253
Iteration 22/1000 | Loss: 0.00002252
Iteration 23/1000 | Loss: 0.00002251
Iteration 24/1000 | Loss: 0.00002251
Iteration 25/1000 | Loss: 0.00002250
Iteration 26/1000 | Loss: 0.00002250
Iteration 27/1000 | Loss: 0.00002250
Iteration 28/1000 | Loss: 0.00002250
Iteration 29/1000 | Loss: 0.00002250
Iteration 30/1000 | Loss: 0.00002250
Iteration 31/1000 | Loss: 0.00002250
Iteration 32/1000 | Loss: 0.00002249
Iteration 33/1000 | Loss: 0.00002249
Iteration 34/1000 | Loss: 0.00002249
Iteration 35/1000 | Loss: 0.00002249
Iteration 36/1000 | Loss: 0.00002249
Iteration 37/1000 | Loss: 0.00002249
Iteration 38/1000 | Loss: 0.00002249
Iteration 39/1000 | Loss: 0.00002248
Iteration 40/1000 | Loss: 0.00002248
Iteration 41/1000 | Loss: 0.00002248
Iteration 42/1000 | Loss: 0.00002248
Iteration 43/1000 | Loss: 0.00002248
Iteration 44/1000 | Loss: 0.00002248
Iteration 45/1000 | Loss: 0.00002248
Iteration 46/1000 | Loss: 0.00002248
Iteration 47/1000 | Loss: 0.00002248
Iteration 48/1000 | Loss: 0.00002248
Iteration 49/1000 | Loss: 0.00002247
Iteration 50/1000 | Loss: 0.00002247
Iteration 51/1000 | Loss: 0.00002247
Iteration 52/1000 | Loss: 0.00002247
Iteration 53/1000 | Loss: 0.00002246
Iteration 54/1000 | Loss: 0.00002246
Iteration 55/1000 | Loss: 0.00002246
Iteration 56/1000 | Loss: 0.00002245
Iteration 57/1000 | Loss: 0.00002243
Iteration 58/1000 | Loss: 0.00002242
Iteration 59/1000 | Loss: 0.00002242
Iteration 60/1000 | Loss: 0.00002242
Iteration 61/1000 | Loss: 0.00002241
Iteration 62/1000 | Loss: 0.00002241
Iteration 63/1000 | Loss: 0.00002241
Iteration 64/1000 | Loss: 0.00002238
Iteration 65/1000 | Loss: 0.00002238
Iteration 66/1000 | Loss: 0.00002238
Iteration 67/1000 | Loss: 0.00002238
Iteration 68/1000 | Loss: 0.00002238
Iteration 69/1000 | Loss: 0.00002238
Iteration 70/1000 | Loss: 0.00002238
Iteration 71/1000 | Loss: 0.00002237
Iteration 72/1000 | Loss: 0.00002237
Iteration 73/1000 | Loss: 0.00002237
Iteration 74/1000 | Loss: 0.00002237
Iteration 75/1000 | Loss: 0.00002237
Iteration 76/1000 | Loss: 0.00002237
Iteration 77/1000 | Loss: 0.00002236
Iteration 78/1000 | Loss: 0.00002236
Iteration 79/1000 | Loss: 0.00002236
Iteration 80/1000 | Loss: 0.00002236
Iteration 81/1000 | Loss: 0.00002236
Iteration 82/1000 | Loss: 0.00002236
Iteration 83/1000 | Loss: 0.00002236
Iteration 84/1000 | Loss: 0.00002236
Iteration 85/1000 | Loss: 0.00002236
Iteration 86/1000 | Loss: 0.00002236
Iteration 87/1000 | Loss: 0.00002236
Iteration 88/1000 | Loss: 0.00002236
Iteration 89/1000 | Loss: 0.00002235
Iteration 90/1000 | Loss: 0.00002235
Iteration 91/1000 | Loss: 0.00002235
Iteration 92/1000 | Loss: 0.00002234
Iteration 93/1000 | Loss: 0.00002234
Iteration 94/1000 | Loss: 0.00002233
Iteration 95/1000 | Loss: 0.00002232
Iteration 96/1000 | Loss: 0.00002232
Iteration 97/1000 | Loss: 0.00002232
Iteration 98/1000 | Loss: 0.00002232
Iteration 99/1000 | Loss: 0.00002232
Iteration 100/1000 | Loss: 0.00002232
Iteration 101/1000 | Loss: 0.00002231
Iteration 102/1000 | Loss: 0.00002231
Iteration 103/1000 | Loss: 0.00002231
Iteration 104/1000 | Loss: 0.00002231
Iteration 105/1000 | Loss: 0.00002231
Iteration 106/1000 | Loss: 0.00002231
Iteration 107/1000 | Loss: 0.00002231
Iteration 108/1000 | Loss: 0.00002230
Iteration 109/1000 | Loss: 0.00002230
Iteration 110/1000 | Loss: 0.00002230
Iteration 111/1000 | Loss: 0.00002230
Iteration 112/1000 | Loss: 0.00002230
Iteration 113/1000 | Loss: 0.00002230
Iteration 114/1000 | Loss: 0.00002230
Iteration 115/1000 | Loss: 0.00002230
Iteration 116/1000 | Loss: 0.00002229
Iteration 117/1000 | Loss: 0.00002229
Iteration 118/1000 | Loss: 0.00002229
Iteration 119/1000 | Loss: 0.00002229
Iteration 120/1000 | Loss: 0.00002229
Iteration 121/1000 | Loss: 0.00002229
Iteration 122/1000 | Loss: 0.00002229
Iteration 123/1000 | Loss: 0.00002229
Iteration 124/1000 | Loss: 0.00002229
Iteration 125/1000 | Loss: 0.00002229
Iteration 126/1000 | Loss: 0.00002228
Iteration 127/1000 | Loss: 0.00002228
Iteration 128/1000 | Loss: 0.00002228
Iteration 129/1000 | Loss: 0.00002228
Iteration 130/1000 | Loss: 0.00002228
Iteration 131/1000 | Loss: 0.00002228
Iteration 132/1000 | Loss: 0.00002228
Iteration 133/1000 | Loss: 0.00002228
Iteration 134/1000 | Loss: 0.00002228
Iteration 135/1000 | Loss: 0.00002228
Iteration 136/1000 | Loss: 0.00002228
Iteration 137/1000 | Loss: 0.00002228
Iteration 138/1000 | Loss: 0.00002228
Iteration 139/1000 | Loss: 0.00002228
Iteration 140/1000 | Loss: 0.00002228
Iteration 141/1000 | Loss: 0.00002228
Iteration 142/1000 | Loss: 0.00002228
Iteration 143/1000 | Loss: 0.00002227
Iteration 144/1000 | Loss: 0.00002227
Iteration 145/1000 | Loss: 0.00002227
Iteration 146/1000 | Loss: 0.00002227
Iteration 147/1000 | Loss: 0.00002227
Iteration 148/1000 | Loss: 0.00002227
Iteration 149/1000 | Loss: 0.00002227
Iteration 150/1000 | Loss: 0.00002227
Iteration 151/1000 | Loss: 0.00002227
Iteration 152/1000 | Loss: 0.00002227
Iteration 153/1000 | Loss: 0.00002227
Iteration 154/1000 | Loss: 0.00002227
Iteration 155/1000 | Loss: 0.00002227
Iteration 156/1000 | Loss: 0.00002227
Iteration 157/1000 | Loss: 0.00002227
Iteration 158/1000 | Loss: 0.00002227
Iteration 159/1000 | Loss: 0.00002227
Iteration 160/1000 | Loss: 0.00002227
Iteration 161/1000 | Loss: 0.00002227
Iteration 162/1000 | Loss: 0.00002227
Iteration 163/1000 | Loss: 0.00002227
Iteration 164/1000 | Loss: 0.00002227
Iteration 165/1000 | Loss: 0.00002227
Iteration 166/1000 | Loss: 0.00002227
Iteration 167/1000 | Loss: 0.00002227
Iteration 168/1000 | Loss: 0.00002227
Iteration 169/1000 | Loss: 0.00002227
Iteration 170/1000 | Loss: 0.00002227
Iteration 171/1000 | Loss: 0.00002227
Iteration 172/1000 | Loss: 0.00002227
Iteration 173/1000 | Loss: 0.00002227
Iteration 174/1000 | Loss: 0.00002227
Iteration 175/1000 | Loss: 0.00002227
Iteration 176/1000 | Loss: 0.00002227
Iteration 177/1000 | Loss: 0.00002227
Iteration 178/1000 | Loss: 0.00002227
Iteration 179/1000 | Loss: 0.00002227
Iteration 180/1000 | Loss: 0.00002227
Iteration 181/1000 | Loss: 0.00002227
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [2.2270311092142947e-05, 2.2270311092142947e-05, 2.2270311092142947e-05, 2.2270311092142947e-05, 2.2270311092142947e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2270311092142947e-05

Optimization complete. Final v2v error: 3.9000561237335205 mm

Highest mean error: 4.227899074554443 mm for frame 1

Lowest mean error: 3.727322816848755 mm for frame 88

Saving results

Total time: 38.19008016586304
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_it_4274/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_it_4274/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_it_4274/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01114092
Iteration 2/25 | Loss: 0.01114092
Iteration 3/25 | Loss: 0.00273031
Iteration 4/25 | Loss: 0.00150661
Iteration 5/25 | Loss: 0.00112562
Iteration 6/25 | Loss: 0.00107559
Iteration 7/25 | Loss: 0.00092232
Iteration 8/25 | Loss: 0.00091285
Iteration 9/25 | Loss: 0.00091612
Iteration 10/25 | Loss: 0.00086075
Iteration 11/25 | Loss: 0.00078157
Iteration 12/25 | Loss: 0.00075506
Iteration 13/25 | Loss: 0.00072204
Iteration 14/25 | Loss: 0.00071403
Iteration 15/25 | Loss: 0.00071653
Iteration 16/25 | Loss: 0.00069579
Iteration 17/25 | Loss: 0.00068378
Iteration 18/25 | Loss: 0.00068008
Iteration 19/25 | Loss: 0.00067941
Iteration 20/25 | Loss: 0.00067887
Iteration 21/25 | Loss: 0.00067863
Iteration 22/25 | Loss: 0.00068788
Iteration 23/25 | Loss: 0.00068649
Iteration 24/25 | Loss: 0.00068351
Iteration 25/25 | Loss: 0.00068328

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.83914137
Iteration 2/25 | Loss: 0.00029460
Iteration 3/25 | Loss: 0.00025215
Iteration 4/25 | Loss: 0.00025215
Iteration 5/25 | Loss: 0.00025215
Iteration 6/25 | Loss: 0.00025215
Iteration 7/25 | Loss: 0.00025215
Iteration 8/25 | Loss: 0.00025215
Iteration 9/25 | Loss: 0.00025215
Iteration 10/25 | Loss: 0.00025214
Iteration 11/25 | Loss: 0.00025214
Iteration 12/25 | Loss: 0.00025214
Iteration 13/25 | Loss: 0.00025214
Iteration 14/25 | Loss: 0.00025214
Iteration 15/25 | Loss: 0.00025214
Iteration 16/25 | Loss: 0.00025214
Iteration 17/25 | Loss: 0.00025214
Iteration 18/25 | Loss: 0.00025214
Iteration 19/25 | Loss: 0.00025214
Iteration 20/25 | Loss: 0.00025214
Iteration 21/25 | Loss: 0.00025214
Iteration 22/25 | Loss: 0.00025214
Iteration 23/25 | Loss: 0.00025214
Iteration 24/25 | Loss: 0.00025214
Iteration 25/25 | Loss: 0.00025214

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025214
Iteration 2/1000 | Loss: 0.00362978
Iteration 3/1000 | Loss: 0.00405892
Iteration 4/1000 | Loss: 0.00145046
Iteration 5/1000 | Loss: 0.00128029
Iteration 6/1000 | Loss: 0.00006546
Iteration 7/1000 | Loss: 0.00011730
Iteration 8/1000 | Loss: 0.00011785
Iteration 9/1000 | Loss: 0.00010976
Iteration 10/1000 | Loss: 0.00011183
Iteration 11/1000 | Loss: 0.00011342
Iteration 12/1000 | Loss: 0.00014485
Iteration 13/1000 | Loss: 0.00015173
Iteration 14/1000 | Loss: 0.00012593
Iteration 15/1000 | Loss: 0.00011417
Iteration 16/1000 | Loss: 0.00014813
Iteration 17/1000 | Loss: 0.00011319
Iteration 18/1000 | Loss: 0.00118801
Iteration 19/1000 | Loss: 0.00075428
Iteration 20/1000 | Loss: 0.00111072
Iteration 21/1000 | Loss: 0.00053000
Iteration 22/1000 | Loss: 0.00072870
Iteration 23/1000 | Loss: 0.00204002
Iteration 24/1000 | Loss: 0.00172470
Iteration 25/1000 | Loss: 0.00093997
Iteration 26/1000 | Loss: 0.00256510
Iteration 27/1000 | Loss: 0.00158744
Iteration 28/1000 | Loss: 0.00158055
Iteration 29/1000 | Loss: 0.00154952
Iteration 30/1000 | Loss: 0.00104144
Iteration 31/1000 | Loss: 0.00097944
Iteration 32/1000 | Loss: 0.00027861
Iteration 33/1000 | Loss: 0.00209978
Iteration 34/1000 | Loss: 0.00126696
Iteration 35/1000 | Loss: 0.00072182
Iteration 36/1000 | Loss: 0.00011377
Iteration 37/1000 | Loss: 0.00110299
Iteration 38/1000 | Loss: 0.00101288
Iteration 39/1000 | Loss: 0.00119115
Iteration 40/1000 | Loss: 0.00186339
Iteration 41/1000 | Loss: 0.00117591
Iteration 42/1000 | Loss: 0.00119633
Iteration 43/1000 | Loss: 0.00115268
Iteration 44/1000 | Loss: 0.00129291
Iteration 45/1000 | Loss: 0.00152650
Iteration 46/1000 | Loss: 0.00161644
Iteration 47/1000 | Loss: 0.00088617
Iteration 48/1000 | Loss: 0.00149849
Iteration 49/1000 | Loss: 0.00076975
Iteration 50/1000 | Loss: 0.00148915
Iteration 51/1000 | Loss: 0.00117182
Iteration 52/1000 | Loss: 0.00179011
Iteration 53/1000 | Loss: 0.00125595
Iteration 54/1000 | Loss: 0.00113053
Iteration 55/1000 | Loss: 0.00122786
Iteration 56/1000 | Loss: 0.00135077
Iteration 57/1000 | Loss: 0.00127840
Iteration 58/1000 | Loss: 0.00249541
Iteration 59/1000 | Loss: 0.00204555
Iteration 60/1000 | Loss: 0.00110307
Iteration 61/1000 | Loss: 0.00009689
Iteration 62/1000 | Loss: 0.00067657
Iteration 63/1000 | Loss: 0.00109412
Iteration 64/1000 | Loss: 0.00119160
Iteration 65/1000 | Loss: 0.00116446
Iteration 66/1000 | Loss: 0.00111552
Iteration 67/1000 | Loss: 0.00098344
Iteration 68/1000 | Loss: 0.00109898
Iteration 69/1000 | Loss: 0.00023321
Iteration 70/1000 | Loss: 0.00183238
Iteration 71/1000 | Loss: 0.00232232
Iteration 72/1000 | Loss: 0.00136289
Iteration 73/1000 | Loss: 0.00105158
Iteration 74/1000 | Loss: 0.00114643
Iteration 75/1000 | Loss: 0.00156380
Iteration 76/1000 | Loss: 0.00094546
Iteration 77/1000 | Loss: 0.00092734
Iteration 78/1000 | Loss: 0.00111382
Iteration 79/1000 | Loss: 0.00082522
Iteration 80/1000 | Loss: 0.00101112
Iteration 81/1000 | Loss: 0.00193546
Iteration 82/1000 | Loss: 0.00087497
Iteration 83/1000 | Loss: 0.00106568
Iteration 84/1000 | Loss: 0.00140513
Iteration 85/1000 | Loss: 0.00072094
Iteration 86/1000 | Loss: 0.00096806
Iteration 87/1000 | Loss: 0.00060416
Iteration 88/1000 | Loss: 0.00092527
Iteration 89/1000 | Loss: 0.00122789
Iteration 90/1000 | Loss: 0.00099801
Iteration 91/1000 | Loss: 0.00202782
Iteration 92/1000 | Loss: 0.00013815
Iteration 93/1000 | Loss: 0.00004264
Iteration 94/1000 | Loss: 0.00003560
Iteration 95/1000 | Loss: 0.00003775
Iteration 96/1000 | Loss: 0.00076902
Iteration 97/1000 | Loss: 0.00077855
Iteration 98/1000 | Loss: 0.00092233
Iteration 99/1000 | Loss: 0.00112583
Iteration 100/1000 | Loss: 0.00138263
Iteration 101/1000 | Loss: 0.00103319
Iteration 102/1000 | Loss: 0.00163207
Iteration 103/1000 | Loss: 0.00114931
Iteration 104/1000 | Loss: 0.00121842
Iteration 105/1000 | Loss: 0.00079596
Iteration 106/1000 | Loss: 0.00177861
Iteration 107/1000 | Loss: 0.00126974
Iteration 108/1000 | Loss: 0.00106761
Iteration 109/1000 | Loss: 0.00133316
Iteration 110/1000 | Loss: 0.00178579
Iteration 111/1000 | Loss: 0.00141586
Iteration 112/1000 | Loss: 0.00196628
Iteration 113/1000 | Loss: 0.00013077
Iteration 114/1000 | Loss: 0.00010386
Iteration 115/1000 | Loss: 0.00004581
Iteration 116/1000 | Loss: 0.00003118
Iteration 117/1000 | Loss: 0.00003188
Iteration 118/1000 | Loss: 0.00002105
Iteration 119/1000 | Loss: 0.00002014
Iteration 120/1000 | Loss: 0.00001974
Iteration 121/1000 | Loss: 0.00001941
Iteration 122/1000 | Loss: 0.00003536
Iteration 123/1000 | Loss: 0.00002125
Iteration 124/1000 | Loss: 0.00001909
Iteration 125/1000 | Loss: 0.00001908
Iteration 126/1000 | Loss: 0.00001908
Iteration 127/1000 | Loss: 0.00001908
Iteration 128/1000 | Loss: 0.00001908
Iteration 129/1000 | Loss: 0.00001908
Iteration 130/1000 | Loss: 0.00001908
Iteration 131/1000 | Loss: 0.00001908
Iteration 132/1000 | Loss: 0.00001908
Iteration 133/1000 | Loss: 0.00001908
Iteration 134/1000 | Loss: 0.00001908
Iteration 135/1000 | Loss: 0.00001907
Iteration 136/1000 | Loss: 0.00001898
Iteration 137/1000 | Loss: 0.00001898
Iteration 138/1000 | Loss: 0.00001898
Iteration 139/1000 | Loss: 0.00001890
Iteration 140/1000 | Loss: 0.00001888
Iteration 141/1000 | Loss: 0.00001876
Iteration 142/1000 | Loss: 0.00001875
Iteration 143/1000 | Loss: 0.00001875
Iteration 144/1000 | Loss: 0.00001875
Iteration 145/1000 | Loss: 0.00002258
Iteration 146/1000 | Loss: 0.00001867
Iteration 147/1000 | Loss: 0.00001867
Iteration 148/1000 | Loss: 0.00001866
Iteration 149/1000 | Loss: 0.00001866
Iteration 150/1000 | Loss: 0.00001866
Iteration 151/1000 | Loss: 0.00001866
Iteration 152/1000 | Loss: 0.00001865
Iteration 153/1000 | Loss: 0.00001865
Iteration 154/1000 | Loss: 0.00001865
Iteration 155/1000 | Loss: 0.00001865
Iteration 156/1000 | Loss: 0.00001864
Iteration 157/1000 | Loss: 0.00001864
Iteration 158/1000 | Loss: 0.00001864
Iteration 159/1000 | Loss: 0.00001863
Iteration 160/1000 | Loss: 0.00001863
Iteration 161/1000 | Loss: 0.00001862
Iteration 162/1000 | Loss: 0.00001862
Iteration 163/1000 | Loss: 0.00001862
Iteration 164/1000 | Loss: 0.00003009
Iteration 165/1000 | Loss: 0.00001863
Iteration 166/1000 | Loss: 0.00001857
Iteration 167/1000 | Loss: 0.00001851
Iteration 168/1000 | Loss: 0.00001850
Iteration 169/1000 | Loss: 0.00001850
Iteration 170/1000 | Loss: 0.00001849
Iteration 171/1000 | Loss: 0.00001848
Iteration 172/1000 | Loss: 0.00001847
Iteration 173/1000 | Loss: 0.00001843
Iteration 174/1000 | Loss: 0.00001826
Iteration 175/1000 | Loss: 0.00001819
Iteration 176/1000 | Loss: 0.00001818
Iteration 177/1000 | Loss: 0.00004110
Iteration 178/1000 | Loss: 0.00001943
Iteration 179/1000 | Loss: 0.00001811
Iteration 180/1000 | Loss: 0.00001791
Iteration 181/1000 | Loss: 0.00001790
Iteration 182/1000 | Loss: 0.00001790
Iteration 183/1000 | Loss: 0.00001790
Iteration 184/1000 | Loss: 0.00001790
Iteration 185/1000 | Loss: 0.00001790
Iteration 186/1000 | Loss: 0.00001789
Iteration 187/1000 | Loss: 0.00001789
Iteration 188/1000 | Loss: 0.00001788
Iteration 189/1000 | Loss: 0.00001788
Iteration 190/1000 | Loss: 0.00001787
Iteration 191/1000 | Loss: 0.00001787
Iteration 192/1000 | Loss: 0.00001786
Iteration 193/1000 | Loss: 0.00001786
Iteration 194/1000 | Loss: 0.00001786
Iteration 195/1000 | Loss: 0.00001786
Iteration 196/1000 | Loss: 0.00001786
Iteration 197/1000 | Loss: 0.00001786
Iteration 198/1000 | Loss: 0.00001785
Iteration 199/1000 | Loss: 0.00001785
Iteration 200/1000 | Loss: 0.00001785
Iteration 201/1000 | Loss: 0.00001784
Iteration 202/1000 | Loss: 0.00001783
Iteration 203/1000 | Loss: 0.00001783
Iteration 204/1000 | Loss: 0.00001782
Iteration 205/1000 | Loss: 0.00001782
Iteration 206/1000 | Loss: 0.00001781
Iteration 207/1000 | Loss: 0.00001781
Iteration 208/1000 | Loss: 0.00001781
Iteration 209/1000 | Loss: 0.00001781
Iteration 210/1000 | Loss: 0.00001781
Iteration 211/1000 | Loss: 0.00001781
Iteration 212/1000 | Loss: 0.00003939
Iteration 213/1000 | Loss: 0.00003939
Iteration 214/1000 | Loss: 0.00012498
Iteration 215/1000 | Loss: 0.00001797
Iteration 216/1000 | Loss: 0.00004079
Iteration 217/1000 | Loss: 0.00001978
Iteration 218/1000 | Loss: 0.00002045
Iteration 219/1000 | Loss: 0.00004422
Iteration 220/1000 | Loss: 0.00002048
Iteration 221/1000 | Loss: 0.00001883
Iteration 222/1000 | Loss: 0.00001815
Iteration 223/1000 | Loss: 0.00001774
Iteration 224/1000 | Loss: 0.00001773
Iteration 225/1000 | Loss: 0.00001773
Iteration 226/1000 | Loss: 0.00001773
Iteration 227/1000 | Loss: 0.00001773
Iteration 228/1000 | Loss: 0.00001773
Iteration 229/1000 | Loss: 0.00001773
Iteration 230/1000 | Loss: 0.00001773
Iteration 231/1000 | Loss: 0.00001773
Iteration 232/1000 | Loss: 0.00001773
Iteration 233/1000 | Loss: 0.00001772
Iteration 234/1000 | Loss: 0.00001772
Iteration 235/1000 | Loss: 0.00001772
Iteration 236/1000 | Loss: 0.00001772
Iteration 237/1000 | Loss: 0.00001772
Iteration 238/1000 | Loss: 0.00001772
Iteration 239/1000 | Loss: 0.00001772
Iteration 240/1000 | Loss: 0.00001771
Iteration 241/1000 | Loss: 0.00001771
Iteration 242/1000 | Loss: 0.00001771
Iteration 243/1000 | Loss: 0.00001771
Iteration 244/1000 | Loss: 0.00001771
Iteration 245/1000 | Loss: 0.00001771
Iteration 246/1000 | Loss: 0.00001771
Iteration 247/1000 | Loss: 0.00001771
Iteration 248/1000 | Loss: 0.00001771
Iteration 249/1000 | Loss: 0.00001771
Iteration 250/1000 | Loss: 0.00001771
Iteration 251/1000 | Loss: 0.00001771
Iteration 252/1000 | Loss: 0.00001771
Iteration 253/1000 | Loss: 0.00001771
Iteration 254/1000 | Loss: 0.00001771
Iteration 255/1000 | Loss: 0.00001771
Iteration 256/1000 | Loss: 0.00001771
Iteration 257/1000 | Loss: 0.00001771
Iteration 258/1000 | Loss: 0.00001771
Iteration 259/1000 | Loss: 0.00001771
Iteration 260/1000 | Loss: 0.00001771
Iteration 261/1000 | Loss: 0.00001771
Iteration 262/1000 | Loss: 0.00001771
Iteration 263/1000 | Loss: 0.00001771
Iteration 264/1000 | Loss: 0.00001771
Iteration 265/1000 | Loss: 0.00001771
Iteration 266/1000 | Loss: 0.00001771
Iteration 267/1000 | Loss: 0.00001771
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 267. Stopping optimization.
Last 5 losses: [1.770805465639569e-05, 1.770805465639569e-05, 1.770805465639569e-05, 1.770805465639569e-05, 1.770805465639569e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.770805465639569e-05

Optimization complete. Final v2v error: 3.363008499145508 mm

Highest mean error: 8.753582000732422 mm for frame 53

Lowest mean error: 2.9202539920806885 mm for frame 27

Saving results

Total time: 246.65354442596436
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_it_4274/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_it_4274/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_it_4274/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00448405
Iteration 2/25 | Loss: 0.00092321
Iteration 3/25 | Loss: 0.00079671
Iteration 4/25 | Loss: 0.00075602
Iteration 5/25 | Loss: 0.00074124
Iteration 6/25 | Loss: 0.00073887
Iteration 7/25 | Loss: 0.00073799
Iteration 8/25 | Loss: 0.00073782
Iteration 9/25 | Loss: 0.00073782
Iteration 10/25 | Loss: 0.00073782
Iteration 11/25 | Loss: 0.00073782
Iteration 12/25 | Loss: 0.00073782
Iteration 13/25 | Loss: 0.00073782
Iteration 14/25 | Loss: 0.00073782
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0007378157461062074, 0.0007378157461062074, 0.0007378157461062074, 0.0007378157461062074, 0.0007378157461062074]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007378157461062074

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22200751
Iteration 2/25 | Loss: 0.00022985
Iteration 3/25 | Loss: 0.00022982
Iteration 4/25 | Loss: 0.00022982
Iteration 5/25 | Loss: 0.00022982
Iteration 6/25 | Loss: 0.00022982
Iteration 7/25 | Loss: 0.00022982
Iteration 8/25 | Loss: 0.00022982
Iteration 9/25 | Loss: 0.00022982
Iteration 10/25 | Loss: 0.00022982
Iteration 11/25 | Loss: 0.00022982
Iteration 12/25 | Loss: 0.00022982
Iteration 13/25 | Loss: 0.00022982
Iteration 14/25 | Loss: 0.00022982
Iteration 15/25 | Loss: 0.00022982
Iteration 16/25 | Loss: 0.00022982
Iteration 17/25 | Loss: 0.00022982
Iteration 18/25 | Loss: 0.00022982
Iteration 19/25 | Loss: 0.00022982
Iteration 20/25 | Loss: 0.00022982
Iteration 21/25 | Loss: 0.00022982
Iteration 22/25 | Loss: 0.00022982
Iteration 23/25 | Loss: 0.00022982
Iteration 24/25 | Loss: 0.00022982
Iteration 25/25 | Loss: 0.00022982

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00022982
Iteration 2/1000 | Loss: 0.00004003
Iteration 3/1000 | Loss: 0.00003001
Iteration 4/1000 | Loss: 0.00002581
Iteration 5/1000 | Loss: 0.00002477
Iteration 6/1000 | Loss: 0.00002388
Iteration 7/1000 | Loss: 0.00002329
Iteration 8/1000 | Loss: 0.00002284
Iteration 9/1000 | Loss: 0.00002259
Iteration 10/1000 | Loss: 0.00002235
Iteration 11/1000 | Loss: 0.00002226
Iteration 12/1000 | Loss: 0.00002225
Iteration 13/1000 | Loss: 0.00002225
Iteration 14/1000 | Loss: 0.00002224
Iteration 15/1000 | Loss: 0.00002209
Iteration 16/1000 | Loss: 0.00002203
Iteration 17/1000 | Loss: 0.00002201
Iteration 18/1000 | Loss: 0.00002200
Iteration 19/1000 | Loss: 0.00002199
Iteration 20/1000 | Loss: 0.00002197
Iteration 21/1000 | Loss: 0.00002196
Iteration 22/1000 | Loss: 0.00002196
Iteration 23/1000 | Loss: 0.00002196
Iteration 24/1000 | Loss: 0.00002195
Iteration 25/1000 | Loss: 0.00002195
Iteration 26/1000 | Loss: 0.00002195
Iteration 27/1000 | Loss: 0.00002194
Iteration 28/1000 | Loss: 0.00002194
Iteration 29/1000 | Loss: 0.00002194
Iteration 30/1000 | Loss: 0.00002193
Iteration 31/1000 | Loss: 0.00002192
Iteration 32/1000 | Loss: 0.00002190
Iteration 33/1000 | Loss: 0.00002190
Iteration 34/1000 | Loss: 0.00002190
Iteration 35/1000 | Loss: 0.00002187
Iteration 36/1000 | Loss: 0.00002187
Iteration 37/1000 | Loss: 0.00002186
Iteration 38/1000 | Loss: 0.00002186
Iteration 39/1000 | Loss: 0.00002186
Iteration 40/1000 | Loss: 0.00002186
Iteration 41/1000 | Loss: 0.00002186
Iteration 42/1000 | Loss: 0.00002186
Iteration 43/1000 | Loss: 0.00002186
Iteration 44/1000 | Loss: 0.00002186
Iteration 45/1000 | Loss: 0.00002186
Iteration 46/1000 | Loss: 0.00002186
Iteration 47/1000 | Loss: 0.00002185
Iteration 48/1000 | Loss: 0.00002185
Iteration 49/1000 | Loss: 0.00002185
Iteration 50/1000 | Loss: 0.00002185
Iteration 51/1000 | Loss: 0.00002185
Iteration 52/1000 | Loss: 0.00002184
Iteration 53/1000 | Loss: 0.00002183
Iteration 54/1000 | Loss: 0.00002183
Iteration 55/1000 | Loss: 0.00002183
Iteration 56/1000 | Loss: 0.00002183
Iteration 57/1000 | Loss: 0.00002183
Iteration 58/1000 | Loss: 0.00002182
Iteration 59/1000 | Loss: 0.00002182
Iteration 60/1000 | Loss: 0.00002182
Iteration 61/1000 | Loss: 0.00002182
Iteration 62/1000 | Loss: 0.00002182
Iteration 63/1000 | Loss: 0.00002182
Iteration 64/1000 | Loss: 0.00002182
Iteration 65/1000 | Loss: 0.00002182
Iteration 66/1000 | Loss: 0.00002182
Iteration 67/1000 | Loss: 0.00002182
Iteration 68/1000 | Loss: 0.00002181
Iteration 69/1000 | Loss: 0.00002181
Iteration 70/1000 | Loss: 0.00002181
Iteration 71/1000 | Loss: 0.00002181
Iteration 72/1000 | Loss: 0.00002180
Iteration 73/1000 | Loss: 0.00002180
Iteration 74/1000 | Loss: 0.00002180
Iteration 75/1000 | Loss: 0.00002180
Iteration 76/1000 | Loss: 0.00002179
Iteration 77/1000 | Loss: 0.00002179
Iteration 78/1000 | Loss: 0.00002179
Iteration 79/1000 | Loss: 0.00002179
Iteration 80/1000 | Loss: 0.00002179
Iteration 81/1000 | Loss: 0.00002179
Iteration 82/1000 | Loss: 0.00002178
Iteration 83/1000 | Loss: 0.00002178
Iteration 84/1000 | Loss: 0.00002178
Iteration 85/1000 | Loss: 0.00002178
Iteration 86/1000 | Loss: 0.00002178
Iteration 87/1000 | Loss: 0.00002178
Iteration 88/1000 | Loss: 0.00002178
Iteration 89/1000 | Loss: 0.00002178
Iteration 90/1000 | Loss: 0.00002178
Iteration 91/1000 | Loss: 0.00002178
Iteration 92/1000 | Loss: 0.00002178
Iteration 93/1000 | Loss: 0.00002178
Iteration 94/1000 | Loss: 0.00002178
Iteration 95/1000 | Loss: 0.00002178
Iteration 96/1000 | Loss: 0.00002178
Iteration 97/1000 | Loss: 0.00002178
Iteration 98/1000 | Loss: 0.00002178
Iteration 99/1000 | Loss: 0.00002178
Iteration 100/1000 | Loss: 0.00002178
Iteration 101/1000 | Loss: 0.00002178
Iteration 102/1000 | Loss: 0.00002178
Iteration 103/1000 | Loss: 0.00002178
Iteration 104/1000 | Loss: 0.00002178
Iteration 105/1000 | Loss: 0.00002178
Iteration 106/1000 | Loss: 0.00002178
Iteration 107/1000 | Loss: 0.00002178
Iteration 108/1000 | Loss: 0.00002178
Iteration 109/1000 | Loss: 0.00002178
Iteration 110/1000 | Loss: 0.00002178
Iteration 111/1000 | Loss: 0.00002178
Iteration 112/1000 | Loss: 0.00002178
Iteration 113/1000 | Loss: 0.00002178
Iteration 114/1000 | Loss: 0.00002178
Iteration 115/1000 | Loss: 0.00002178
Iteration 116/1000 | Loss: 0.00002178
Iteration 117/1000 | Loss: 0.00002178
Iteration 118/1000 | Loss: 0.00002178
Iteration 119/1000 | Loss: 0.00002178
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [2.177667738578748e-05, 2.177667738578748e-05, 2.177667738578748e-05, 2.177667738578748e-05, 2.177667738578748e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.177667738578748e-05

Optimization complete. Final v2v error: 4.042532444000244 mm

Highest mean error: 4.340820789337158 mm for frame 6

Lowest mean error: 3.851959705352783 mm for frame 100

Saving results

Total time: 33.93185257911682
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_it_4274/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_it_4274/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_it_4274/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01067326
Iteration 2/25 | Loss: 0.00479372
Iteration 3/25 | Loss: 0.00317853
Iteration 4/25 | Loss: 0.00230805
Iteration 5/25 | Loss: 0.00189917
Iteration 6/25 | Loss: 0.00173368
Iteration 7/25 | Loss: 0.00171792
Iteration 8/25 | Loss: 0.00164131
Iteration 9/25 | Loss: 0.00148641
Iteration 10/25 | Loss: 0.00139110
Iteration 11/25 | Loss: 0.00134681
Iteration 12/25 | Loss: 0.00131665
Iteration 13/25 | Loss: 0.00126264
Iteration 14/25 | Loss: 0.00118714
Iteration 15/25 | Loss: 0.00113713
Iteration 16/25 | Loss: 0.00111732
Iteration 17/25 | Loss: 0.00110987
Iteration 18/25 | Loss: 0.00110777
Iteration 19/25 | Loss: 0.00110728
Iteration 20/25 | Loss: 0.00110706
Iteration 21/25 | Loss: 0.00110681
Iteration 22/25 | Loss: 0.00110659
Iteration 23/25 | Loss: 0.00110637
Iteration 24/25 | Loss: 0.00110620
Iteration 25/25 | Loss: 0.00110610

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32044256
Iteration 2/25 | Loss: 0.00258518
Iteration 3/25 | Loss: 0.00258518
Iteration 4/25 | Loss: 0.00258518
Iteration 5/25 | Loss: 0.00258518
Iteration 6/25 | Loss: 0.00258518
Iteration 7/25 | Loss: 0.00258518
Iteration 8/25 | Loss: 0.00258518
Iteration 9/25 | Loss: 0.00258517
Iteration 10/25 | Loss: 0.00258517
Iteration 11/25 | Loss: 0.00258517
Iteration 12/25 | Loss: 0.00258517
Iteration 13/25 | Loss: 0.00258517
Iteration 14/25 | Loss: 0.00258517
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0025851749815046787, 0.0025851749815046787, 0.0025851749815046787, 0.0025851749815046787, 0.0025851749815046787]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025851749815046787

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00258517
Iteration 2/1000 | Loss: 0.00056410
Iteration 3/1000 | Loss: 0.00039148
Iteration 4/1000 | Loss: 0.00032130
Iteration 5/1000 | Loss: 0.00028929
Iteration 6/1000 | Loss: 0.00027257
Iteration 7/1000 | Loss: 0.00025571
Iteration 8/1000 | Loss: 0.00166685
Iteration 9/1000 | Loss: 0.03782569
Iteration 10/1000 | Loss: 0.01929029
Iteration 11/1000 | Loss: 0.02443593
Iteration 12/1000 | Loss: 0.00153354
Iteration 13/1000 | Loss: 0.00095545
Iteration 14/1000 | Loss: 0.00490925
Iteration 15/1000 | Loss: 0.00029384
Iteration 16/1000 | Loss: 0.00054536
Iteration 17/1000 | Loss: 0.00019903
Iteration 18/1000 | Loss: 0.00030911
Iteration 19/1000 | Loss: 0.00006038
Iteration 20/1000 | Loss: 0.00042935
Iteration 21/1000 | Loss: 0.00005931
Iteration 22/1000 | Loss: 0.00005166
Iteration 23/1000 | Loss: 0.00015264
Iteration 24/1000 | Loss: 0.00003297
Iteration 25/1000 | Loss: 0.00002800
Iteration 26/1000 | Loss: 0.00002498
Iteration 27/1000 | Loss: 0.00002346
Iteration 28/1000 | Loss: 0.00002235
Iteration 29/1000 | Loss: 0.00002145
Iteration 30/1000 | Loss: 0.00002082
Iteration 31/1000 | Loss: 0.00002040
Iteration 32/1000 | Loss: 0.00002036
Iteration 33/1000 | Loss: 0.00002012
Iteration 34/1000 | Loss: 0.00001995
Iteration 35/1000 | Loss: 0.00001992
Iteration 36/1000 | Loss: 0.00001990
Iteration 37/1000 | Loss: 0.00001989
Iteration 38/1000 | Loss: 0.00001989
Iteration 39/1000 | Loss: 0.00001989
Iteration 40/1000 | Loss: 0.00001988
Iteration 41/1000 | Loss: 0.00001988
Iteration 42/1000 | Loss: 0.00001986
Iteration 43/1000 | Loss: 0.00001985
Iteration 44/1000 | Loss: 0.00001984
Iteration 45/1000 | Loss: 0.00001984
Iteration 46/1000 | Loss: 0.00001982
Iteration 47/1000 | Loss: 0.00001977
Iteration 48/1000 | Loss: 0.00001974
Iteration 49/1000 | Loss: 0.00001973
Iteration 50/1000 | Loss: 0.00001971
Iteration 51/1000 | Loss: 0.00001971
Iteration 52/1000 | Loss: 0.00001970
Iteration 53/1000 | Loss: 0.00001970
Iteration 54/1000 | Loss: 0.00001970
Iteration 55/1000 | Loss: 0.00001969
Iteration 56/1000 | Loss: 0.00001969
Iteration 57/1000 | Loss: 0.00001969
Iteration 58/1000 | Loss: 0.00001969
Iteration 59/1000 | Loss: 0.00001969
Iteration 60/1000 | Loss: 0.00001968
Iteration 61/1000 | Loss: 0.00001968
Iteration 62/1000 | Loss: 0.00001968
Iteration 63/1000 | Loss: 0.00001967
Iteration 64/1000 | Loss: 0.00001967
Iteration 65/1000 | Loss: 0.00001967
Iteration 66/1000 | Loss: 0.00001967
Iteration 67/1000 | Loss: 0.00001967
Iteration 68/1000 | Loss: 0.00001967
Iteration 69/1000 | Loss: 0.00001967
Iteration 70/1000 | Loss: 0.00001967
Iteration 71/1000 | Loss: 0.00001967
Iteration 72/1000 | Loss: 0.00001967
Iteration 73/1000 | Loss: 0.00001966
Iteration 74/1000 | Loss: 0.00001966
Iteration 75/1000 | Loss: 0.00001966
Iteration 76/1000 | Loss: 0.00001966
Iteration 77/1000 | Loss: 0.00001965
Iteration 78/1000 | Loss: 0.00001965
Iteration 79/1000 | Loss: 0.00001964
Iteration 80/1000 | Loss: 0.00001964
Iteration 81/1000 | Loss: 0.00001964
Iteration 82/1000 | Loss: 0.00001964
Iteration 83/1000 | Loss: 0.00001963
Iteration 84/1000 | Loss: 0.00001963
Iteration 85/1000 | Loss: 0.00001963
Iteration 86/1000 | Loss: 0.00001963
Iteration 87/1000 | Loss: 0.00001963
Iteration 88/1000 | Loss: 0.00001963
Iteration 89/1000 | Loss: 0.00001963
Iteration 90/1000 | Loss: 0.00001962
Iteration 91/1000 | Loss: 0.00001962
Iteration 92/1000 | Loss: 0.00001962
Iteration 93/1000 | Loss: 0.00001962
Iteration 94/1000 | Loss: 0.00001962
Iteration 95/1000 | Loss: 0.00001962
Iteration 96/1000 | Loss: 0.00001962
Iteration 97/1000 | Loss: 0.00001962
Iteration 98/1000 | Loss: 0.00001962
Iteration 99/1000 | Loss: 0.00001961
Iteration 100/1000 | Loss: 0.00001961
Iteration 101/1000 | Loss: 0.00001961
Iteration 102/1000 | Loss: 0.00001961
Iteration 103/1000 | Loss: 0.00001961
Iteration 104/1000 | Loss: 0.00001961
Iteration 105/1000 | Loss: 0.00001961
Iteration 106/1000 | Loss: 0.00001961
Iteration 107/1000 | Loss: 0.00001961
Iteration 108/1000 | Loss: 0.00001961
Iteration 109/1000 | Loss: 0.00001960
Iteration 110/1000 | Loss: 0.00001960
Iteration 111/1000 | Loss: 0.00001960
Iteration 112/1000 | Loss: 0.00001960
Iteration 113/1000 | Loss: 0.00001960
Iteration 114/1000 | Loss: 0.00001960
Iteration 115/1000 | Loss: 0.00001960
Iteration 116/1000 | Loss: 0.00001960
Iteration 117/1000 | Loss: 0.00001960
Iteration 118/1000 | Loss: 0.00001960
Iteration 119/1000 | Loss: 0.00001960
Iteration 120/1000 | Loss: 0.00001960
Iteration 121/1000 | Loss: 0.00001960
Iteration 122/1000 | Loss: 0.00001960
Iteration 123/1000 | Loss: 0.00001960
Iteration 124/1000 | Loss: 0.00001960
Iteration 125/1000 | Loss: 0.00001960
Iteration 126/1000 | Loss: 0.00001960
Iteration 127/1000 | Loss: 0.00001960
Iteration 128/1000 | Loss: 0.00001960
Iteration 129/1000 | Loss: 0.00001960
Iteration 130/1000 | Loss: 0.00001960
Iteration 131/1000 | Loss: 0.00001960
Iteration 132/1000 | Loss: 0.00001960
Iteration 133/1000 | Loss: 0.00001960
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [1.959562359843403e-05, 1.959562359843403e-05, 1.959562359843403e-05, 1.959562359843403e-05, 1.959562359843403e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.959562359843403e-05

Optimization complete. Final v2v error: 3.770562171936035 mm

Highest mean error: 4.072316646575928 mm for frame 11

Lowest mean error: 3.4901623725891113 mm for frame 76

Saving results

Total time: 95.37364459037781
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_it_4274/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_it_4274/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_it_4274/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00834684
Iteration 2/25 | Loss: 0.00083706
Iteration 3/25 | Loss: 0.00063394
Iteration 4/25 | Loss: 0.00060359
Iteration 5/25 | Loss: 0.00059460
Iteration 6/25 | Loss: 0.00059251
Iteration 7/25 | Loss: 0.00059223
Iteration 8/25 | Loss: 0.00059223
Iteration 9/25 | Loss: 0.00059223
Iteration 10/25 | Loss: 0.00059223
Iteration 11/25 | Loss: 0.00059223
Iteration 12/25 | Loss: 0.00059223
Iteration 13/25 | Loss: 0.00059223
Iteration 14/25 | Loss: 0.00059223
Iteration 15/25 | Loss: 0.00059223
Iteration 16/25 | Loss: 0.00059223
Iteration 17/25 | Loss: 0.00059223
Iteration 18/25 | Loss: 0.00059223
Iteration 19/25 | Loss: 0.00059223
Iteration 20/25 | Loss: 0.00059223
Iteration 21/25 | Loss: 0.00059223
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005922311102040112, 0.0005922311102040112, 0.0005922311102040112, 0.0005922311102040112, 0.0005922311102040112]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005922311102040112

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34848964
Iteration 2/25 | Loss: 0.00016180
Iteration 3/25 | Loss: 0.00016180
Iteration 4/25 | Loss: 0.00016180
Iteration 5/25 | Loss: 0.00016180
Iteration 6/25 | Loss: 0.00016180
Iteration 7/25 | Loss: 0.00016180
Iteration 8/25 | Loss: 0.00016180
Iteration 9/25 | Loss: 0.00016180
Iteration 10/25 | Loss: 0.00016180
Iteration 11/25 | Loss: 0.00016180
Iteration 12/25 | Loss: 0.00016180
Iteration 13/25 | Loss: 0.00016180
Iteration 14/25 | Loss: 0.00016180
Iteration 15/25 | Loss: 0.00016180
Iteration 16/25 | Loss: 0.00016180
Iteration 17/25 | Loss: 0.00016180
Iteration 18/25 | Loss: 0.00016180
Iteration 19/25 | Loss: 0.00016180
Iteration 20/25 | Loss: 0.00016180
Iteration 21/25 | Loss: 0.00016180
Iteration 22/25 | Loss: 0.00016180
Iteration 23/25 | Loss: 0.00016180
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00016179787053260952, 0.00016179787053260952, 0.00016179787053260952, 0.00016179787053260952, 0.00016179787053260952]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00016179787053260952

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00016180
Iteration 2/1000 | Loss: 0.00001911
Iteration 3/1000 | Loss: 0.00001465
Iteration 4/1000 | Loss: 0.00001370
Iteration 5/1000 | Loss: 0.00001311
Iteration 6/1000 | Loss: 0.00001272
Iteration 7/1000 | Loss: 0.00001234
Iteration 8/1000 | Loss: 0.00001226
Iteration 9/1000 | Loss: 0.00001220
Iteration 10/1000 | Loss: 0.00001220
Iteration 11/1000 | Loss: 0.00001219
Iteration 12/1000 | Loss: 0.00001219
Iteration 13/1000 | Loss: 0.00001217
Iteration 14/1000 | Loss: 0.00001217
Iteration 15/1000 | Loss: 0.00001214
Iteration 16/1000 | Loss: 0.00001214
Iteration 17/1000 | Loss: 0.00001214
Iteration 18/1000 | Loss: 0.00001214
Iteration 19/1000 | Loss: 0.00001214
Iteration 20/1000 | Loss: 0.00001214
Iteration 21/1000 | Loss: 0.00001210
Iteration 22/1000 | Loss: 0.00001209
Iteration 23/1000 | Loss: 0.00001209
Iteration 24/1000 | Loss: 0.00001209
Iteration 25/1000 | Loss: 0.00001209
Iteration 26/1000 | Loss: 0.00001209
Iteration 27/1000 | Loss: 0.00001208
Iteration 28/1000 | Loss: 0.00001208
Iteration 29/1000 | Loss: 0.00001205
Iteration 30/1000 | Loss: 0.00001205
Iteration 31/1000 | Loss: 0.00001205
Iteration 32/1000 | Loss: 0.00001205
Iteration 33/1000 | Loss: 0.00001205
Iteration 34/1000 | Loss: 0.00001205
Iteration 35/1000 | Loss: 0.00001205
Iteration 36/1000 | Loss: 0.00001204
Iteration 37/1000 | Loss: 0.00001204
Iteration 38/1000 | Loss: 0.00001204
Iteration 39/1000 | Loss: 0.00001204
Iteration 40/1000 | Loss: 0.00001204
Iteration 41/1000 | Loss: 0.00001204
Iteration 42/1000 | Loss: 0.00001204
Iteration 43/1000 | Loss: 0.00001203
Iteration 44/1000 | Loss: 0.00001203
Iteration 45/1000 | Loss: 0.00001202
Iteration 46/1000 | Loss: 0.00001202
Iteration 47/1000 | Loss: 0.00001202
Iteration 48/1000 | Loss: 0.00001202
Iteration 49/1000 | Loss: 0.00001201
Iteration 50/1000 | Loss: 0.00001201
Iteration 51/1000 | Loss: 0.00001201
Iteration 52/1000 | Loss: 0.00001200
Iteration 53/1000 | Loss: 0.00001200
Iteration 54/1000 | Loss: 0.00001200
Iteration 55/1000 | Loss: 0.00001199
Iteration 56/1000 | Loss: 0.00001199
Iteration 57/1000 | Loss: 0.00001199
Iteration 58/1000 | Loss: 0.00001198
Iteration 59/1000 | Loss: 0.00001198
Iteration 60/1000 | Loss: 0.00001198
Iteration 61/1000 | Loss: 0.00001198
Iteration 62/1000 | Loss: 0.00001198
Iteration 63/1000 | Loss: 0.00001197
Iteration 64/1000 | Loss: 0.00001197
Iteration 65/1000 | Loss: 0.00001197
Iteration 66/1000 | Loss: 0.00001196
Iteration 67/1000 | Loss: 0.00001196
Iteration 68/1000 | Loss: 0.00001196
Iteration 69/1000 | Loss: 0.00001196
Iteration 70/1000 | Loss: 0.00001196
Iteration 71/1000 | Loss: 0.00001196
Iteration 72/1000 | Loss: 0.00001196
Iteration 73/1000 | Loss: 0.00001195
Iteration 74/1000 | Loss: 0.00001195
Iteration 75/1000 | Loss: 0.00001195
Iteration 76/1000 | Loss: 0.00001195
Iteration 77/1000 | Loss: 0.00001195
Iteration 78/1000 | Loss: 0.00001195
Iteration 79/1000 | Loss: 0.00001195
Iteration 80/1000 | Loss: 0.00001195
Iteration 81/1000 | Loss: 0.00001195
Iteration 82/1000 | Loss: 0.00001195
Iteration 83/1000 | Loss: 0.00001195
Iteration 84/1000 | Loss: 0.00001195
Iteration 85/1000 | Loss: 0.00001194
Iteration 86/1000 | Loss: 0.00001194
Iteration 87/1000 | Loss: 0.00001194
Iteration 88/1000 | Loss: 0.00001194
Iteration 89/1000 | Loss: 0.00001193
Iteration 90/1000 | Loss: 0.00001193
Iteration 91/1000 | Loss: 0.00001193
Iteration 92/1000 | Loss: 0.00001193
Iteration 93/1000 | Loss: 0.00001193
Iteration 94/1000 | Loss: 0.00001193
Iteration 95/1000 | Loss: 0.00001193
Iteration 96/1000 | Loss: 0.00001193
Iteration 97/1000 | Loss: 0.00001192
Iteration 98/1000 | Loss: 0.00001192
Iteration 99/1000 | Loss: 0.00001192
Iteration 100/1000 | Loss: 0.00001192
Iteration 101/1000 | Loss: 0.00001192
Iteration 102/1000 | Loss: 0.00001192
Iteration 103/1000 | Loss: 0.00001191
Iteration 104/1000 | Loss: 0.00001191
Iteration 105/1000 | Loss: 0.00001191
Iteration 106/1000 | Loss: 0.00001191
Iteration 107/1000 | Loss: 0.00001191
Iteration 108/1000 | Loss: 0.00001191
Iteration 109/1000 | Loss: 0.00001190
Iteration 110/1000 | Loss: 0.00001190
Iteration 111/1000 | Loss: 0.00001190
Iteration 112/1000 | Loss: 0.00001190
Iteration 113/1000 | Loss: 0.00001189
Iteration 114/1000 | Loss: 0.00001189
Iteration 115/1000 | Loss: 0.00001189
Iteration 116/1000 | Loss: 0.00001189
Iteration 117/1000 | Loss: 0.00001189
Iteration 118/1000 | Loss: 0.00001189
Iteration 119/1000 | Loss: 0.00001188
Iteration 120/1000 | Loss: 0.00001188
Iteration 121/1000 | Loss: 0.00001188
Iteration 122/1000 | Loss: 0.00001188
Iteration 123/1000 | Loss: 0.00001188
Iteration 124/1000 | Loss: 0.00001187
Iteration 125/1000 | Loss: 0.00001187
Iteration 126/1000 | Loss: 0.00001187
Iteration 127/1000 | Loss: 0.00001187
Iteration 128/1000 | Loss: 0.00001187
Iteration 129/1000 | Loss: 0.00001186
Iteration 130/1000 | Loss: 0.00001186
Iteration 131/1000 | Loss: 0.00001186
Iteration 132/1000 | Loss: 0.00001186
Iteration 133/1000 | Loss: 0.00001186
Iteration 134/1000 | Loss: 0.00001186
Iteration 135/1000 | Loss: 0.00001186
Iteration 136/1000 | Loss: 0.00001186
Iteration 137/1000 | Loss: 0.00001186
Iteration 138/1000 | Loss: 0.00001186
Iteration 139/1000 | Loss: 0.00001186
Iteration 140/1000 | Loss: 0.00001185
Iteration 141/1000 | Loss: 0.00001185
Iteration 142/1000 | Loss: 0.00001185
Iteration 143/1000 | Loss: 0.00001185
Iteration 144/1000 | Loss: 0.00001184
Iteration 145/1000 | Loss: 0.00001184
Iteration 146/1000 | Loss: 0.00001184
Iteration 147/1000 | Loss: 0.00001184
Iteration 148/1000 | Loss: 0.00001184
Iteration 149/1000 | Loss: 0.00001184
Iteration 150/1000 | Loss: 0.00001184
Iteration 151/1000 | Loss: 0.00001184
Iteration 152/1000 | Loss: 0.00001183
Iteration 153/1000 | Loss: 0.00001183
Iteration 154/1000 | Loss: 0.00001183
Iteration 155/1000 | Loss: 0.00001183
Iteration 156/1000 | Loss: 0.00001183
Iteration 157/1000 | Loss: 0.00001183
Iteration 158/1000 | Loss: 0.00001183
Iteration 159/1000 | Loss: 0.00001183
Iteration 160/1000 | Loss: 0.00001183
Iteration 161/1000 | Loss: 0.00001183
Iteration 162/1000 | Loss: 0.00001182
Iteration 163/1000 | Loss: 0.00001182
Iteration 164/1000 | Loss: 0.00001182
Iteration 165/1000 | Loss: 0.00001182
Iteration 166/1000 | Loss: 0.00001182
Iteration 167/1000 | Loss: 0.00001182
Iteration 168/1000 | Loss: 0.00001182
Iteration 169/1000 | Loss: 0.00001182
Iteration 170/1000 | Loss: 0.00001182
Iteration 171/1000 | Loss: 0.00001182
Iteration 172/1000 | Loss: 0.00001182
Iteration 173/1000 | Loss: 0.00001181
Iteration 174/1000 | Loss: 0.00001181
Iteration 175/1000 | Loss: 0.00001181
Iteration 176/1000 | Loss: 0.00001181
Iteration 177/1000 | Loss: 0.00001181
Iteration 178/1000 | Loss: 0.00001181
Iteration 179/1000 | Loss: 0.00001181
Iteration 180/1000 | Loss: 0.00001181
Iteration 181/1000 | Loss: 0.00001181
Iteration 182/1000 | Loss: 0.00001181
Iteration 183/1000 | Loss: 0.00001181
Iteration 184/1000 | Loss: 0.00001181
Iteration 185/1000 | Loss: 0.00001181
Iteration 186/1000 | Loss: 0.00001181
Iteration 187/1000 | Loss: 0.00001181
Iteration 188/1000 | Loss: 0.00001181
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [1.181261905003339e-05, 1.181261905003339e-05, 1.181261905003339e-05, 1.181261905003339e-05, 1.181261905003339e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.181261905003339e-05

Optimization complete. Final v2v error: 2.944840908050537 mm

Highest mean error: 3.2117817401885986 mm for frame 53

Lowest mean error: 2.7082650661468506 mm for frame 208

Saving results

Total time: 35.475385665893555
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_it_4274/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_it_4274/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_it_4274/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00409291
Iteration 2/25 | Loss: 0.00084822
Iteration 3/25 | Loss: 0.00069978
Iteration 4/25 | Loss: 0.00067466
Iteration 5/25 | Loss: 0.00066599
Iteration 6/25 | Loss: 0.00066376
Iteration 7/25 | Loss: 0.00066343
Iteration 8/25 | Loss: 0.00066343
Iteration 9/25 | Loss: 0.00066343
Iteration 10/25 | Loss: 0.00066343
Iteration 11/25 | Loss: 0.00066343
Iteration 12/25 | Loss: 0.00066343
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006634278106503189, 0.0006634278106503189, 0.0006634278106503189, 0.0006634278106503189, 0.0006634278106503189]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006634278106503189

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.71152353
Iteration 2/25 | Loss: 0.00016797
Iteration 3/25 | Loss: 0.00016795
Iteration 4/25 | Loss: 0.00016795
Iteration 5/25 | Loss: 0.00016795
Iteration 6/25 | Loss: 0.00016795
Iteration 7/25 | Loss: 0.00016795
Iteration 8/25 | Loss: 0.00016795
Iteration 9/25 | Loss: 0.00016795
Iteration 10/25 | Loss: 0.00016795
Iteration 11/25 | Loss: 0.00016795
Iteration 12/25 | Loss: 0.00016795
Iteration 13/25 | Loss: 0.00016795
Iteration 14/25 | Loss: 0.00016795
Iteration 15/25 | Loss: 0.00016795
Iteration 16/25 | Loss: 0.00016795
Iteration 17/25 | Loss: 0.00016795
Iteration 18/25 | Loss: 0.00016795
Iteration 19/25 | Loss: 0.00016795
Iteration 20/25 | Loss: 0.00016795
Iteration 21/25 | Loss: 0.00016795
Iteration 22/25 | Loss: 0.00016795
Iteration 23/25 | Loss: 0.00016795
Iteration 24/25 | Loss: 0.00016795
Iteration 25/25 | Loss: 0.00016795

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00016795
Iteration 2/1000 | Loss: 0.00002977
Iteration 3/1000 | Loss: 0.00002427
Iteration 4/1000 | Loss: 0.00002255
Iteration 5/1000 | Loss: 0.00002156
Iteration 6/1000 | Loss: 0.00002103
Iteration 7/1000 | Loss: 0.00002034
Iteration 8/1000 | Loss: 0.00002006
Iteration 9/1000 | Loss: 0.00001999
Iteration 10/1000 | Loss: 0.00001993
Iteration 11/1000 | Loss: 0.00001984
Iteration 12/1000 | Loss: 0.00001980
Iteration 13/1000 | Loss: 0.00001975
Iteration 14/1000 | Loss: 0.00001972
Iteration 15/1000 | Loss: 0.00001971
Iteration 16/1000 | Loss: 0.00001971
Iteration 17/1000 | Loss: 0.00001970
Iteration 18/1000 | Loss: 0.00001970
Iteration 19/1000 | Loss: 0.00001969
Iteration 20/1000 | Loss: 0.00001969
Iteration 21/1000 | Loss: 0.00001968
Iteration 22/1000 | Loss: 0.00001968
Iteration 23/1000 | Loss: 0.00001967
Iteration 24/1000 | Loss: 0.00001967
Iteration 25/1000 | Loss: 0.00001967
Iteration 26/1000 | Loss: 0.00001967
Iteration 27/1000 | Loss: 0.00001967
Iteration 28/1000 | Loss: 0.00001966
Iteration 29/1000 | Loss: 0.00001966
Iteration 30/1000 | Loss: 0.00001966
Iteration 31/1000 | Loss: 0.00001966
Iteration 32/1000 | Loss: 0.00001966
Iteration 33/1000 | Loss: 0.00001965
Iteration 34/1000 | Loss: 0.00001965
Iteration 35/1000 | Loss: 0.00001965
Iteration 36/1000 | Loss: 0.00001964
Iteration 37/1000 | Loss: 0.00001964
Iteration 38/1000 | Loss: 0.00001964
Iteration 39/1000 | Loss: 0.00001963
Iteration 40/1000 | Loss: 0.00001963
Iteration 41/1000 | Loss: 0.00001962
Iteration 42/1000 | Loss: 0.00001962
Iteration 43/1000 | Loss: 0.00001961
Iteration 44/1000 | Loss: 0.00001961
Iteration 45/1000 | Loss: 0.00001960
Iteration 46/1000 | Loss: 0.00001960
Iteration 47/1000 | Loss: 0.00001959
Iteration 48/1000 | Loss: 0.00001959
Iteration 49/1000 | Loss: 0.00001959
Iteration 50/1000 | Loss: 0.00001958
Iteration 51/1000 | Loss: 0.00001958
Iteration 52/1000 | Loss: 0.00001958
Iteration 53/1000 | Loss: 0.00001956
Iteration 54/1000 | Loss: 0.00001956
Iteration 55/1000 | Loss: 0.00001956
Iteration 56/1000 | Loss: 0.00001956
Iteration 57/1000 | Loss: 0.00001955
Iteration 58/1000 | Loss: 0.00001955
Iteration 59/1000 | Loss: 0.00001955
Iteration 60/1000 | Loss: 0.00001955
Iteration 61/1000 | Loss: 0.00001955
Iteration 62/1000 | Loss: 0.00001955
Iteration 63/1000 | Loss: 0.00001955
Iteration 64/1000 | Loss: 0.00001955
Iteration 65/1000 | Loss: 0.00001955
Iteration 66/1000 | Loss: 0.00001955
Iteration 67/1000 | Loss: 0.00001954
Iteration 68/1000 | Loss: 0.00001954
Iteration 69/1000 | Loss: 0.00001954
Iteration 70/1000 | Loss: 0.00001953
Iteration 71/1000 | Loss: 0.00001953
Iteration 72/1000 | Loss: 0.00001953
Iteration 73/1000 | Loss: 0.00001952
Iteration 74/1000 | Loss: 0.00001952
Iteration 75/1000 | Loss: 0.00001952
Iteration 76/1000 | Loss: 0.00001952
Iteration 77/1000 | Loss: 0.00001952
Iteration 78/1000 | Loss: 0.00001952
Iteration 79/1000 | Loss: 0.00001951
Iteration 80/1000 | Loss: 0.00001951
Iteration 81/1000 | Loss: 0.00001951
Iteration 82/1000 | Loss: 0.00001951
Iteration 83/1000 | Loss: 0.00001951
Iteration 84/1000 | Loss: 0.00001951
Iteration 85/1000 | Loss: 0.00001951
Iteration 86/1000 | Loss: 0.00001951
Iteration 87/1000 | Loss: 0.00001951
Iteration 88/1000 | Loss: 0.00001950
Iteration 89/1000 | Loss: 0.00001950
Iteration 90/1000 | Loss: 0.00001950
Iteration 91/1000 | Loss: 0.00001950
Iteration 92/1000 | Loss: 0.00001950
Iteration 93/1000 | Loss: 0.00001950
Iteration 94/1000 | Loss: 0.00001950
Iteration 95/1000 | Loss: 0.00001950
Iteration 96/1000 | Loss: 0.00001950
Iteration 97/1000 | Loss: 0.00001949
Iteration 98/1000 | Loss: 0.00001949
Iteration 99/1000 | Loss: 0.00001949
Iteration 100/1000 | Loss: 0.00001949
Iteration 101/1000 | Loss: 0.00001949
Iteration 102/1000 | Loss: 0.00001948
Iteration 103/1000 | Loss: 0.00001948
Iteration 104/1000 | Loss: 0.00001948
Iteration 105/1000 | Loss: 0.00001948
Iteration 106/1000 | Loss: 0.00001948
Iteration 107/1000 | Loss: 0.00001948
Iteration 108/1000 | Loss: 0.00001948
Iteration 109/1000 | Loss: 0.00001948
Iteration 110/1000 | Loss: 0.00001948
Iteration 111/1000 | Loss: 0.00001948
Iteration 112/1000 | Loss: 0.00001948
Iteration 113/1000 | Loss: 0.00001948
Iteration 114/1000 | Loss: 0.00001948
Iteration 115/1000 | Loss: 0.00001948
Iteration 116/1000 | Loss: 0.00001948
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [1.948234967130702e-05, 1.948234967130702e-05, 1.948234967130702e-05, 1.948234967130702e-05, 1.948234967130702e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.948234967130702e-05

Optimization complete. Final v2v error: 3.8231263160705566 mm

Highest mean error: 4.290553092956543 mm for frame 89

Lowest mean error: 3.422365188598633 mm for frame 39

Saving results

Total time: 32.11311364173889
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_it_4274/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_it_4274/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_it_4274/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00886077
Iteration 2/25 | Loss: 0.00108627
Iteration 3/25 | Loss: 0.00067281
Iteration 4/25 | Loss: 0.00063325
Iteration 5/25 | Loss: 0.00062225
Iteration 6/25 | Loss: 0.00061877
Iteration 7/25 | Loss: 0.00061796
Iteration 8/25 | Loss: 0.00061794
Iteration 9/25 | Loss: 0.00061794
Iteration 10/25 | Loss: 0.00061794
Iteration 11/25 | Loss: 0.00061794
Iteration 12/25 | Loss: 0.00061794
Iteration 13/25 | Loss: 0.00061794
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006179421325214207, 0.0006179421325214207, 0.0006179421325214207, 0.0006179421325214207, 0.0006179421325214207]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006179421325214207

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.82027698
Iteration 2/25 | Loss: 0.00014821
Iteration 3/25 | Loss: 0.00014821
Iteration 4/25 | Loss: 0.00014821
Iteration 5/25 | Loss: 0.00014820
Iteration 6/25 | Loss: 0.00014820
Iteration 7/25 | Loss: 0.00014820
Iteration 8/25 | Loss: 0.00014820
Iteration 9/25 | Loss: 0.00014820
Iteration 10/25 | Loss: 0.00014820
Iteration 11/25 | Loss: 0.00014820
Iteration 12/25 | Loss: 0.00014820
Iteration 13/25 | Loss: 0.00014820
Iteration 14/25 | Loss: 0.00014820
Iteration 15/25 | Loss: 0.00014820
Iteration 16/25 | Loss: 0.00014820
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00014820342767052352, 0.00014820342767052352, 0.00014820342767052352, 0.00014820342767052352, 0.00014820342767052352]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00014820342767052352

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00014820
Iteration 2/1000 | Loss: 0.00002120
Iteration 3/1000 | Loss: 0.00001701
Iteration 4/1000 | Loss: 0.00001565
Iteration 5/1000 | Loss: 0.00001485
Iteration 6/1000 | Loss: 0.00001434
Iteration 7/1000 | Loss: 0.00001422
Iteration 8/1000 | Loss: 0.00001421
Iteration 9/1000 | Loss: 0.00001405
Iteration 10/1000 | Loss: 0.00001398
Iteration 11/1000 | Loss: 0.00001391
Iteration 12/1000 | Loss: 0.00001390
Iteration 13/1000 | Loss: 0.00001389
Iteration 14/1000 | Loss: 0.00001388
Iteration 15/1000 | Loss: 0.00001385
Iteration 16/1000 | Loss: 0.00001383
Iteration 17/1000 | Loss: 0.00001382
Iteration 18/1000 | Loss: 0.00001382
Iteration 19/1000 | Loss: 0.00001382
Iteration 20/1000 | Loss: 0.00001382
Iteration 21/1000 | Loss: 0.00001382
Iteration 22/1000 | Loss: 0.00001382
Iteration 23/1000 | Loss: 0.00001382
Iteration 24/1000 | Loss: 0.00001382
Iteration 25/1000 | Loss: 0.00001382
Iteration 26/1000 | Loss: 0.00001382
Iteration 27/1000 | Loss: 0.00001382
Iteration 28/1000 | Loss: 0.00001382
Iteration 29/1000 | Loss: 0.00001381
Iteration 30/1000 | Loss: 0.00001381
Iteration 31/1000 | Loss: 0.00001379
Iteration 32/1000 | Loss: 0.00001378
Iteration 33/1000 | Loss: 0.00001378
Iteration 34/1000 | Loss: 0.00001378
Iteration 35/1000 | Loss: 0.00001378
Iteration 36/1000 | Loss: 0.00001378
Iteration 37/1000 | Loss: 0.00001377
Iteration 38/1000 | Loss: 0.00001377
Iteration 39/1000 | Loss: 0.00001376
Iteration 40/1000 | Loss: 0.00001376
Iteration 41/1000 | Loss: 0.00001376
Iteration 42/1000 | Loss: 0.00001375
Iteration 43/1000 | Loss: 0.00001374
Iteration 44/1000 | Loss: 0.00001374
Iteration 45/1000 | Loss: 0.00001374
Iteration 46/1000 | Loss: 0.00001374
Iteration 47/1000 | Loss: 0.00001374
Iteration 48/1000 | Loss: 0.00001373
Iteration 49/1000 | Loss: 0.00001373
Iteration 50/1000 | Loss: 0.00001372
Iteration 51/1000 | Loss: 0.00001372
Iteration 52/1000 | Loss: 0.00001372
Iteration 53/1000 | Loss: 0.00001371
Iteration 54/1000 | Loss: 0.00001371
Iteration 55/1000 | Loss: 0.00001371
Iteration 56/1000 | Loss: 0.00001371
Iteration 57/1000 | Loss: 0.00001371
Iteration 58/1000 | Loss: 0.00001371
Iteration 59/1000 | Loss: 0.00001371
Iteration 60/1000 | Loss: 0.00001370
Iteration 61/1000 | Loss: 0.00001370
Iteration 62/1000 | Loss: 0.00001370
Iteration 63/1000 | Loss: 0.00001370
Iteration 64/1000 | Loss: 0.00001369
Iteration 65/1000 | Loss: 0.00001369
Iteration 66/1000 | Loss: 0.00001369
Iteration 67/1000 | Loss: 0.00001369
Iteration 68/1000 | Loss: 0.00001368
Iteration 69/1000 | Loss: 0.00001368
Iteration 70/1000 | Loss: 0.00001368
Iteration 71/1000 | Loss: 0.00001368
Iteration 72/1000 | Loss: 0.00001368
Iteration 73/1000 | Loss: 0.00001368
Iteration 74/1000 | Loss: 0.00001368
Iteration 75/1000 | Loss: 0.00001368
Iteration 76/1000 | Loss: 0.00001367
Iteration 77/1000 | Loss: 0.00001367
Iteration 78/1000 | Loss: 0.00001367
Iteration 79/1000 | Loss: 0.00001367
Iteration 80/1000 | Loss: 0.00001367
Iteration 81/1000 | Loss: 0.00001366
Iteration 82/1000 | Loss: 0.00001366
Iteration 83/1000 | Loss: 0.00001366
Iteration 84/1000 | Loss: 0.00001366
Iteration 85/1000 | Loss: 0.00001366
Iteration 86/1000 | Loss: 0.00001366
Iteration 87/1000 | Loss: 0.00001366
Iteration 88/1000 | Loss: 0.00001365
Iteration 89/1000 | Loss: 0.00001365
Iteration 90/1000 | Loss: 0.00001364
Iteration 91/1000 | Loss: 0.00001364
Iteration 92/1000 | Loss: 0.00001364
Iteration 93/1000 | Loss: 0.00001364
Iteration 94/1000 | Loss: 0.00001364
Iteration 95/1000 | Loss: 0.00001364
Iteration 96/1000 | Loss: 0.00001364
Iteration 97/1000 | Loss: 0.00001364
Iteration 98/1000 | Loss: 0.00001363
Iteration 99/1000 | Loss: 0.00001363
Iteration 100/1000 | Loss: 0.00001363
Iteration 101/1000 | Loss: 0.00001363
Iteration 102/1000 | Loss: 0.00001363
Iteration 103/1000 | Loss: 0.00001363
Iteration 104/1000 | Loss: 0.00001362
Iteration 105/1000 | Loss: 0.00001362
Iteration 106/1000 | Loss: 0.00001362
Iteration 107/1000 | Loss: 0.00001362
Iteration 108/1000 | Loss: 0.00001362
Iteration 109/1000 | Loss: 0.00001362
Iteration 110/1000 | Loss: 0.00001361
Iteration 111/1000 | Loss: 0.00001361
Iteration 112/1000 | Loss: 0.00001361
Iteration 113/1000 | Loss: 0.00001361
Iteration 114/1000 | Loss: 0.00001361
Iteration 115/1000 | Loss: 0.00001361
Iteration 116/1000 | Loss: 0.00001361
Iteration 117/1000 | Loss: 0.00001361
Iteration 118/1000 | Loss: 0.00001361
Iteration 119/1000 | Loss: 0.00001361
Iteration 120/1000 | Loss: 0.00001360
Iteration 121/1000 | Loss: 0.00001360
Iteration 122/1000 | Loss: 0.00001360
Iteration 123/1000 | Loss: 0.00001360
Iteration 124/1000 | Loss: 0.00001360
Iteration 125/1000 | Loss: 0.00001360
Iteration 126/1000 | Loss: 0.00001360
Iteration 127/1000 | Loss: 0.00001360
Iteration 128/1000 | Loss: 0.00001360
Iteration 129/1000 | Loss: 0.00001360
Iteration 130/1000 | Loss: 0.00001360
Iteration 131/1000 | Loss: 0.00001360
Iteration 132/1000 | Loss: 0.00001360
Iteration 133/1000 | Loss: 0.00001360
Iteration 134/1000 | Loss: 0.00001360
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.3603226761915721e-05, 1.3603226761915721e-05, 1.3603226761915721e-05, 1.3603226761915721e-05, 1.3603226761915721e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3603226761915721e-05

Optimization complete. Final v2v error: 3.1205074787139893 mm

Highest mean error: 3.648790121078491 mm for frame 25

Lowest mean error: 2.687990427017212 mm for frame 75

Saving results

Total time: 33.61668157577515
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_it_4274/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_it_4274/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_it_4274/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00284092
Iteration 2/25 | Loss: 0.00099131
Iteration 3/25 | Loss: 0.00074290
Iteration 4/25 | Loss: 0.00070021
Iteration 5/25 | Loss: 0.00068515
Iteration 6/25 | Loss: 0.00068034
Iteration 7/25 | Loss: 0.00067857
Iteration 8/25 | Loss: 0.00067777
Iteration 9/25 | Loss: 0.00067763
Iteration 10/25 | Loss: 0.00067763
Iteration 11/25 | Loss: 0.00067763
Iteration 12/25 | Loss: 0.00067763
Iteration 13/25 | Loss: 0.00067763
Iteration 14/25 | Loss: 0.00067763
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0006776311784051359, 0.0006776311784051359, 0.0006776311784051359, 0.0006776311784051359, 0.0006776311784051359]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006776311784051359

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34220648
Iteration 2/25 | Loss: 0.00018534
Iteration 3/25 | Loss: 0.00018534
Iteration 4/25 | Loss: 0.00018534
Iteration 5/25 | Loss: 0.00018534
Iteration 6/25 | Loss: 0.00018534
Iteration 7/25 | Loss: 0.00018534
Iteration 8/25 | Loss: 0.00018534
Iteration 9/25 | Loss: 0.00018534
Iteration 10/25 | Loss: 0.00018534
Iteration 11/25 | Loss: 0.00018534
Iteration 12/25 | Loss: 0.00018534
Iteration 13/25 | Loss: 0.00018534
Iteration 14/25 | Loss: 0.00018534
Iteration 15/25 | Loss: 0.00018534
Iteration 16/25 | Loss: 0.00018534
Iteration 17/25 | Loss: 0.00018534
Iteration 18/25 | Loss: 0.00018534
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00018534011906012893, 0.00018534011906012893, 0.00018534011906012893, 0.00018534011906012893, 0.00018534011906012893]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00018534011906012893

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00018534
Iteration 2/1000 | Loss: 0.00003378
Iteration 3/1000 | Loss: 0.00002457
Iteration 4/1000 | Loss: 0.00002136
Iteration 5/1000 | Loss: 0.00001974
Iteration 6/1000 | Loss: 0.00001913
Iteration 7/1000 | Loss: 0.00001833
Iteration 8/1000 | Loss: 0.00001789
Iteration 9/1000 | Loss: 0.00001759
Iteration 10/1000 | Loss: 0.00001732
Iteration 11/1000 | Loss: 0.00001714
Iteration 12/1000 | Loss: 0.00001702
Iteration 13/1000 | Loss: 0.00001695
Iteration 14/1000 | Loss: 0.00001694
Iteration 15/1000 | Loss: 0.00001692
Iteration 16/1000 | Loss: 0.00001691
Iteration 17/1000 | Loss: 0.00001689
Iteration 18/1000 | Loss: 0.00001685
Iteration 19/1000 | Loss: 0.00001679
Iteration 20/1000 | Loss: 0.00001673
Iteration 21/1000 | Loss: 0.00001670
Iteration 22/1000 | Loss: 0.00001668
Iteration 23/1000 | Loss: 0.00001667
Iteration 24/1000 | Loss: 0.00001665
Iteration 25/1000 | Loss: 0.00001664
Iteration 26/1000 | Loss: 0.00001663
Iteration 27/1000 | Loss: 0.00001662
Iteration 28/1000 | Loss: 0.00001662
Iteration 29/1000 | Loss: 0.00001660
Iteration 30/1000 | Loss: 0.00001658
Iteration 31/1000 | Loss: 0.00001657
Iteration 32/1000 | Loss: 0.00001657
Iteration 33/1000 | Loss: 0.00001655
Iteration 34/1000 | Loss: 0.00001654
Iteration 35/1000 | Loss: 0.00001654
Iteration 36/1000 | Loss: 0.00001654
Iteration 37/1000 | Loss: 0.00001653
Iteration 38/1000 | Loss: 0.00001652
Iteration 39/1000 | Loss: 0.00001652
Iteration 40/1000 | Loss: 0.00001651
Iteration 41/1000 | Loss: 0.00001651
Iteration 42/1000 | Loss: 0.00001651
Iteration 43/1000 | Loss: 0.00001650
Iteration 44/1000 | Loss: 0.00001650
Iteration 45/1000 | Loss: 0.00001650
Iteration 46/1000 | Loss: 0.00001650
Iteration 47/1000 | Loss: 0.00001649
Iteration 48/1000 | Loss: 0.00001649
Iteration 49/1000 | Loss: 0.00001649
Iteration 50/1000 | Loss: 0.00001649
Iteration 51/1000 | Loss: 0.00001649
Iteration 52/1000 | Loss: 0.00001649
Iteration 53/1000 | Loss: 0.00001648
Iteration 54/1000 | Loss: 0.00001648
Iteration 55/1000 | Loss: 0.00001648
Iteration 56/1000 | Loss: 0.00001648
Iteration 57/1000 | Loss: 0.00001648
Iteration 58/1000 | Loss: 0.00001648
Iteration 59/1000 | Loss: 0.00001648
Iteration 60/1000 | Loss: 0.00001648
Iteration 61/1000 | Loss: 0.00001648
Iteration 62/1000 | Loss: 0.00001648
Iteration 63/1000 | Loss: 0.00001648
Iteration 64/1000 | Loss: 0.00001648
Iteration 65/1000 | Loss: 0.00001648
Iteration 66/1000 | Loss: 0.00001648
Iteration 67/1000 | Loss: 0.00001648
Iteration 68/1000 | Loss: 0.00001647
Iteration 69/1000 | Loss: 0.00001647
Iteration 70/1000 | Loss: 0.00001646
Iteration 71/1000 | Loss: 0.00001646
Iteration 72/1000 | Loss: 0.00001646
Iteration 73/1000 | Loss: 0.00001645
Iteration 74/1000 | Loss: 0.00001645
Iteration 75/1000 | Loss: 0.00001645
Iteration 76/1000 | Loss: 0.00001645
Iteration 77/1000 | Loss: 0.00001645
Iteration 78/1000 | Loss: 0.00001645
Iteration 79/1000 | Loss: 0.00001645
Iteration 80/1000 | Loss: 0.00001645
Iteration 81/1000 | Loss: 0.00001644
Iteration 82/1000 | Loss: 0.00001644
Iteration 83/1000 | Loss: 0.00001644
Iteration 84/1000 | Loss: 0.00001644
Iteration 85/1000 | Loss: 0.00001644
Iteration 86/1000 | Loss: 0.00001644
Iteration 87/1000 | Loss: 0.00001643
Iteration 88/1000 | Loss: 0.00001643
Iteration 89/1000 | Loss: 0.00001643
Iteration 90/1000 | Loss: 0.00001643
Iteration 91/1000 | Loss: 0.00001643
Iteration 92/1000 | Loss: 0.00001643
Iteration 93/1000 | Loss: 0.00001642
Iteration 94/1000 | Loss: 0.00001642
Iteration 95/1000 | Loss: 0.00001642
Iteration 96/1000 | Loss: 0.00001641
Iteration 97/1000 | Loss: 0.00001641
Iteration 98/1000 | Loss: 0.00001641
Iteration 99/1000 | Loss: 0.00001641
Iteration 100/1000 | Loss: 0.00001641
Iteration 101/1000 | Loss: 0.00001641
Iteration 102/1000 | Loss: 0.00001640
Iteration 103/1000 | Loss: 0.00001640
Iteration 104/1000 | Loss: 0.00001640
Iteration 105/1000 | Loss: 0.00001640
Iteration 106/1000 | Loss: 0.00001640
Iteration 107/1000 | Loss: 0.00001640
Iteration 108/1000 | Loss: 0.00001640
Iteration 109/1000 | Loss: 0.00001640
Iteration 110/1000 | Loss: 0.00001640
Iteration 111/1000 | Loss: 0.00001639
Iteration 112/1000 | Loss: 0.00001639
Iteration 113/1000 | Loss: 0.00001639
Iteration 114/1000 | Loss: 0.00001639
Iteration 115/1000 | Loss: 0.00001638
Iteration 116/1000 | Loss: 0.00001638
Iteration 117/1000 | Loss: 0.00001638
Iteration 118/1000 | Loss: 0.00001638
Iteration 119/1000 | Loss: 0.00001638
Iteration 120/1000 | Loss: 0.00001638
Iteration 121/1000 | Loss: 0.00001638
Iteration 122/1000 | Loss: 0.00001638
Iteration 123/1000 | Loss: 0.00001638
Iteration 124/1000 | Loss: 0.00001638
Iteration 125/1000 | Loss: 0.00001637
Iteration 126/1000 | Loss: 0.00001637
Iteration 127/1000 | Loss: 0.00001637
Iteration 128/1000 | Loss: 0.00001637
Iteration 129/1000 | Loss: 0.00001637
Iteration 130/1000 | Loss: 0.00001636
Iteration 131/1000 | Loss: 0.00001636
Iteration 132/1000 | Loss: 0.00001636
Iteration 133/1000 | Loss: 0.00001636
Iteration 134/1000 | Loss: 0.00001636
Iteration 135/1000 | Loss: 0.00001636
Iteration 136/1000 | Loss: 0.00001636
Iteration 137/1000 | Loss: 0.00001636
Iteration 138/1000 | Loss: 0.00001636
Iteration 139/1000 | Loss: 0.00001636
Iteration 140/1000 | Loss: 0.00001636
Iteration 141/1000 | Loss: 0.00001636
Iteration 142/1000 | Loss: 0.00001636
Iteration 143/1000 | Loss: 0.00001636
Iteration 144/1000 | Loss: 0.00001635
Iteration 145/1000 | Loss: 0.00001635
Iteration 146/1000 | Loss: 0.00001635
Iteration 147/1000 | Loss: 0.00001635
Iteration 148/1000 | Loss: 0.00001635
Iteration 149/1000 | Loss: 0.00001635
Iteration 150/1000 | Loss: 0.00001635
Iteration 151/1000 | Loss: 0.00001635
Iteration 152/1000 | Loss: 0.00001635
Iteration 153/1000 | Loss: 0.00001635
Iteration 154/1000 | Loss: 0.00001635
Iteration 155/1000 | Loss: 0.00001635
Iteration 156/1000 | Loss: 0.00001635
Iteration 157/1000 | Loss: 0.00001635
Iteration 158/1000 | Loss: 0.00001635
Iteration 159/1000 | Loss: 0.00001635
Iteration 160/1000 | Loss: 0.00001634
Iteration 161/1000 | Loss: 0.00001634
Iteration 162/1000 | Loss: 0.00001634
Iteration 163/1000 | Loss: 0.00001634
Iteration 164/1000 | Loss: 0.00001634
Iteration 165/1000 | Loss: 0.00001634
Iteration 166/1000 | Loss: 0.00001634
Iteration 167/1000 | Loss: 0.00001634
Iteration 168/1000 | Loss: 0.00001634
Iteration 169/1000 | Loss: 0.00001634
Iteration 170/1000 | Loss: 0.00001634
Iteration 171/1000 | Loss: 0.00001634
Iteration 172/1000 | Loss: 0.00001634
Iteration 173/1000 | Loss: 0.00001634
Iteration 174/1000 | Loss: 0.00001634
Iteration 175/1000 | Loss: 0.00001634
Iteration 176/1000 | Loss: 0.00001634
Iteration 177/1000 | Loss: 0.00001634
Iteration 178/1000 | Loss: 0.00001634
Iteration 179/1000 | Loss: 0.00001634
Iteration 180/1000 | Loss: 0.00001634
Iteration 181/1000 | Loss: 0.00001634
Iteration 182/1000 | Loss: 0.00001634
Iteration 183/1000 | Loss: 0.00001634
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.633625288377516e-05, 1.633625288377516e-05, 1.633625288377516e-05, 1.633625288377516e-05, 1.633625288377516e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.633625288377516e-05

Optimization complete. Final v2v error: 3.460329532623291 mm

Highest mean error: 4.171807765960693 mm for frame 107

Lowest mean error: 3.063013792037964 mm for frame 139

Saving results

Total time: 44.09381461143494
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_it_4274/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_it_4274/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_it_4274/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00610966
Iteration 2/25 | Loss: 0.00128360
Iteration 3/25 | Loss: 0.00085146
Iteration 4/25 | Loss: 0.00076918
Iteration 5/25 | Loss: 0.00075714
Iteration 6/25 | Loss: 0.00075364
Iteration 7/25 | Loss: 0.00075288
Iteration 8/25 | Loss: 0.00075277
Iteration 9/25 | Loss: 0.00075277
Iteration 10/25 | Loss: 0.00075277
Iteration 11/25 | Loss: 0.00075277
Iteration 12/25 | Loss: 0.00075276
Iteration 13/25 | Loss: 0.00075276
Iteration 14/25 | Loss: 0.00075276
Iteration 15/25 | Loss: 0.00075276
Iteration 16/25 | Loss: 0.00075276
Iteration 17/25 | Loss: 0.00075276
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007527627749368548, 0.0007527627749368548, 0.0007527627749368548, 0.0007527627749368548, 0.0007527627749368548]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007527627749368548

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36835730
Iteration 2/25 | Loss: 0.00020104
Iteration 3/25 | Loss: 0.00020104
Iteration 4/25 | Loss: 0.00020104
Iteration 5/25 | Loss: 0.00020104
Iteration 6/25 | Loss: 0.00020104
Iteration 7/25 | Loss: 0.00020103
Iteration 8/25 | Loss: 0.00020103
Iteration 9/25 | Loss: 0.00020103
Iteration 10/25 | Loss: 0.00020103
Iteration 11/25 | Loss: 0.00020103
Iteration 12/25 | Loss: 0.00020103
Iteration 13/25 | Loss: 0.00020103
Iteration 14/25 | Loss: 0.00020103
Iteration 15/25 | Loss: 0.00020103
Iteration 16/25 | Loss: 0.00020103
Iteration 17/25 | Loss: 0.00020103
Iteration 18/25 | Loss: 0.00020103
Iteration 19/25 | Loss: 0.00020103
Iteration 20/25 | Loss: 0.00020103
Iteration 21/25 | Loss: 0.00020103
Iteration 22/25 | Loss: 0.00020103
Iteration 23/25 | Loss: 0.00020103
Iteration 24/25 | Loss: 0.00020103
Iteration 25/25 | Loss: 0.00020103

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00020103
Iteration 2/1000 | Loss: 0.00004008
Iteration 3/1000 | Loss: 0.00003429
Iteration 4/1000 | Loss: 0.00003022
Iteration 5/1000 | Loss: 0.00002833
Iteration 6/1000 | Loss: 0.00002712
Iteration 7/1000 | Loss: 0.00002637
Iteration 8/1000 | Loss: 0.00002578
Iteration 9/1000 | Loss: 0.00002537
Iteration 10/1000 | Loss: 0.00002511
Iteration 11/1000 | Loss: 0.00002496
Iteration 12/1000 | Loss: 0.00002485
Iteration 13/1000 | Loss: 0.00002471
Iteration 14/1000 | Loss: 0.00002467
Iteration 15/1000 | Loss: 0.00002467
Iteration 16/1000 | Loss: 0.00002462
Iteration 17/1000 | Loss: 0.00002460
Iteration 18/1000 | Loss: 0.00002459
Iteration 19/1000 | Loss: 0.00002459
Iteration 20/1000 | Loss: 0.00002455
Iteration 21/1000 | Loss: 0.00002454
Iteration 22/1000 | Loss: 0.00002449
Iteration 23/1000 | Loss: 0.00002448
Iteration 24/1000 | Loss: 0.00002447
Iteration 25/1000 | Loss: 0.00002447
Iteration 26/1000 | Loss: 0.00002446
Iteration 27/1000 | Loss: 0.00002446
Iteration 28/1000 | Loss: 0.00002446
Iteration 29/1000 | Loss: 0.00002445
Iteration 30/1000 | Loss: 0.00002445
Iteration 31/1000 | Loss: 0.00002444
Iteration 32/1000 | Loss: 0.00002440
Iteration 33/1000 | Loss: 0.00002439
Iteration 34/1000 | Loss: 0.00002439
Iteration 35/1000 | Loss: 0.00002438
Iteration 36/1000 | Loss: 0.00002438
Iteration 37/1000 | Loss: 0.00002438
Iteration 38/1000 | Loss: 0.00002437
Iteration 39/1000 | Loss: 0.00002437
Iteration 40/1000 | Loss: 0.00002437
Iteration 41/1000 | Loss: 0.00002437
Iteration 42/1000 | Loss: 0.00002437
Iteration 43/1000 | Loss: 0.00002437
Iteration 44/1000 | Loss: 0.00002436
Iteration 45/1000 | Loss: 0.00002436
Iteration 46/1000 | Loss: 0.00002436
Iteration 47/1000 | Loss: 0.00002435
Iteration 48/1000 | Loss: 0.00002435
Iteration 49/1000 | Loss: 0.00002435
Iteration 50/1000 | Loss: 0.00002435
Iteration 51/1000 | Loss: 0.00002434
Iteration 52/1000 | Loss: 0.00002434
Iteration 53/1000 | Loss: 0.00002434
Iteration 54/1000 | Loss: 0.00002433
Iteration 55/1000 | Loss: 0.00002433
Iteration 56/1000 | Loss: 0.00002433
Iteration 57/1000 | Loss: 0.00002432
Iteration 58/1000 | Loss: 0.00002432
Iteration 59/1000 | Loss: 0.00002431
Iteration 60/1000 | Loss: 0.00002431
Iteration 61/1000 | Loss: 0.00002430
Iteration 62/1000 | Loss: 0.00002430
Iteration 63/1000 | Loss: 0.00002430
Iteration 64/1000 | Loss: 0.00002429
Iteration 65/1000 | Loss: 0.00002429
Iteration 66/1000 | Loss: 0.00002429
Iteration 67/1000 | Loss: 0.00002429
Iteration 68/1000 | Loss: 0.00002428
Iteration 69/1000 | Loss: 0.00002428
Iteration 70/1000 | Loss: 0.00002427
Iteration 71/1000 | Loss: 0.00002427
Iteration 72/1000 | Loss: 0.00002427
Iteration 73/1000 | Loss: 0.00002427
Iteration 74/1000 | Loss: 0.00002427
Iteration 75/1000 | Loss: 0.00002427
Iteration 76/1000 | Loss: 0.00002426
Iteration 77/1000 | Loss: 0.00002426
Iteration 78/1000 | Loss: 0.00002426
Iteration 79/1000 | Loss: 0.00002426
Iteration 80/1000 | Loss: 0.00002425
Iteration 81/1000 | Loss: 0.00002425
Iteration 82/1000 | Loss: 0.00002425
Iteration 83/1000 | Loss: 0.00002425
Iteration 84/1000 | Loss: 0.00002425
Iteration 85/1000 | Loss: 0.00002425
Iteration 86/1000 | Loss: 0.00002425
Iteration 87/1000 | Loss: 0.00002425
Iteration 88/1000 | Loss: 0.00002425
Iteration 89/1000 | Loss: 0.00002425
Iteration 90/1000 | Loss: 0.00002424
Iteration 91/1000 | Loss: 0.00002424
Iteration 92/1000 | Loss: 0.00002424
Iteration 93/1000 | Loss: 0.00002424
Iteration 94/1000 | Loss: 0.00002424
Iteration 95/1000 | Loss: 0.00002424
Iteration 96/1000 | Loss: 0.00002424
Iteration 97/1000 | Loss: 0.00002423
Iteration 98/1000 | Loss: 0.00002423
Iteration 99/1000 | Loss: 0.00002423
Iteration 100/1000 | Loss: 0.00002423
Iteration 101/1000 | Loss: 0.00002423
Iteration 102/1000 | Loss: 0.00002423
Iteration 103/1000 | Loss: 0.00002423
Iteration 104/1000 | Loss: 0.00002423
Iteration 105/1000 | Loss: 0.00002423
Iteration 106/1000 | Loss: 0.00002423
Iteration 107/1000 | Loss: 0.00002423
Iteration 108/1000 | Loss: 0.00002423
Iteration 109/1000 | Loss: 0.00002423
Iteration 110/1000 | Loss: 0.00002423
Iteration 111/1000 | Loss: 0.00002423
Iteration 112/1000 | Loss: 0.00002423
Iteration 113/1000 | Loss: 0.00002423
Iteration 114/1000 | Loss: 0.00002423
Iteration 115/1000 | Loss: 0.00002423
Iteration 116/1000 | Loss: 0.00002423
Iteration 117/1000 | Loss: 0.00002423
Iteration 118/1000 | Loss: 0.00002423
Iteration 119/1000 | Loss: 0.00002423
Iteration 120/1000 | Loss: 0.00002423
Iteration 121/1000 | Loss: 0.00002423
Iteration 122/1000 | Loss: 0.00002423
Iteration 123/1000 | Loss: 0.00002423
Iteration 124/1000 | Loss: 0.00002423
Iteration 125/1000 | Loss: 0.00002423
Iteration 126/1000 | Loss: 0.00002423
Iteration 127/1000 | Loss: 0.00002423
Iteration 128/1000 | Loss: 0.00002423
Iteration 129/1000 | Loss: 0.00002423
Iteration 130/1000 | Loss: 0.00002423
Iteration 131/1000 | Loss: 0.00002423
Iteration 132/1000 | Loss: 0.00002423
Iteration 133/1000 | Loss: 0.00002423
Iteration 134/1000 | Loss: 0.00002423
Iteration 135/1000 | Loss: 0.00002423
Iteration 136/1000 | Loss: 0.00002423
Iteration 137/1000 | Loss: 0.00002423
Iteration 138/1000 | Loss: 0.00002423
Iteration 139/1000 | Loss: 0.00002423
Iteration 140/1000 | Loss: 0.00002423
Iteration 141/1000 | Loss: 0.00002423
Iteration 142/1000 | Loss: 0.00002423
Iteration 143/1000 | Loss: 0.00002423
Iteration 144/1000 | Loss: 0.00002423
Iteration 145/1000 | Loss: 0.00002423
Iteration 146/1000 | Loss: 0.00002423
Iteration 147/1000 | Loss: 0.00002423
Iteration 148/1000 | Loss: 0.00002423
Iteration 149/1000 | Loss: 0.00002423
Iteration 150/1000 | Loss: 0.00002423
Iteration 151/1000 | Loss: 0.00002423
Iteration 152/1000 | Loss: 0.00002423
Iteration 153/1000 | Loss: 0.00002423
Iteration 154/1000 | Loss: 0.00002423
Iteration 155/1000 | Loss: 0.00002423
Iteration 156/1000 | Loss: 0.00002423
Iteration 157/1000 | Loss: 0.00002423
Iteration 158/1000 | Loss: 0.00002423
Iteration 159/1000 | Loss: 0.00002423
Iteration 160/1000 | Loss: 0.00002423
Iteration 161/1000 | Loss: 0.00002423
Iteration 162/1000 | Loss: 0.00002423
Iteration 163/1000 | Loss: 0.00002423
Iteration 164/1000 | Loss: 0.00002423
Iteration 165/1000 | Loss: 0.00002423
Iteration 166/1000 | Loss: 0.00002423
Iteration 167/1000 | Loss: 0.00002423
Iteration 168/1000 | Loss: 0.00002423
Iteration 169/1000 | Loss: 0.00002423
Iteration 170/1000 | Loss: 0.00002423
Iteration 171/1000 | Loss: 0.00002423
Iteration 172/1000 | Loss: 0.00002423
Iteration 173/1000 | Loss: 0.00002423
Iteration 174/1000 | Loss: 0.00002423
Iteration 175/1000 | Loss: 0.00002423
Iteration 176/1000 | Loss: 0.00002423
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [2.422662146273069e-05, 2.422662146273069e-05, 2.422662146273069e-05, 2.422662146273069e-05, 2.422662146273069e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.422662146273069e-05

Optimization complete. Final v2v error: 4.275203704833984 mm

Highest mean error: 4.928896903991699 mm for frame 0

Lowest mean error: 3.629781723022461 mm for frame 119

Saving results

Total time: 39.00799536705017
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_it_4274/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_it_4274/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_it_4274/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01079979
Iteration 2/25 | Loss: 0.00355924
Iteration 3/25 | Loss: 0.00241251
Iteration 4/25 | Loss: 0.00215752
Iteration 5/25 | Loss: 0.00186551
Iteration 6/25 | Loss: 0.00196572
Iteration 7/25 | Loss: 0.00170843
Iteration 8/25 | Loss: 0.00167026
Iteration 9/25 | Loss: 0.00149202
Iteration 10/25 | Loss: 0.00138475
Iteration 11/25 | Loss: 0.00128168
Iteration 12/25 | Loss: 0.00121308
Iteration 13/25 | Loss: 0.00118550
Iteration 14/25 | Loss: 0.00121239
Iteration 15/25 | Loss: 0.00115250
Iteration 16/25 | Loss: 0.00109745
Iteration 17/25 | Loss: 0.00108438
Iteration 18/25 | Loss: 0.00107707
Iteration 19/25 | Loss: 0.00106453
Iteration 20/25 | Loss: 0.00103866
Iteration 21/25 | Loss: 0.00103483
Iteration 22/25 | Loss: 0.00103425
Iteration 23/25 | Loss: 0.00103575
Iteration 24/25 | Loss: 0.00102309
Iteration 25/25 | Loss: 0.00102356

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47050989
Iteration 2/25 | Loss: 0.00403534
Iteration 3/25 | Loss: 0.00302574
Iteration 4/25 | Loss: 0.00291121
Iteration 5/25 | Loss: 0.00291119
Iteration 6/25 | Loss: 0.00291119
Iteration 7/25 | Loss: 0.00291118
Iteration 8/25 | Loss: 0.00291118
Iteration 9/25 | Loss: 0.00291118
Iteration 10/25 | Loss: 0.00291118
Iteration 11/25 | Loss: 0.00291118
Iteration 12/25 | Loss: 0.00291118
Iteration 13/25 | Loss: 0.00291118
Iteration 14/25 | Loss: 0.00291118
Iteration 15/25 | Loss: 0.00291118
Iteration 16/25 | Loss: 0.00291118
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0029111832845956087, 0.0029111832845956087, 0.0029111832845956087, 0.0029111832845956087, 0.0029111832845956087]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0029111832845956087

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00291118
Iteration 2/1000 | Loss: 0.00093461
Iteration 3/1000 | Loss: 0.00051164
Iteration 4/1000 | Loss: 0.00194052
Iteration 5/1000 | Loss: 0.00041293
Iteration 6/1000 | Loss: 0.00209851
Iteration 7/1000 | Loss: 0.00062773
Iteration 8/1000 | Loss: 0.00028305
Iteration 9/1000 | Loss: 0.00037311
Iteration 10/1000 | Loss: 0.00047122
Iteration 11/1000 | Loss: 0.00046388
Iteration 12/1000 | Loss: 0.00021315
Iteration 13/1000 | Loss: 0.00060829
Iteration 14/1000 | Loss: 0.00035852
Iteration 15/1000 | Loss: 0.00249302
Iteration 16/1000 | Loss: 0.00221744
Iteration 17/1000 | Loss: 0.00023458
Iteration 18/1000 | Loss: 0.00019356
Iteration 19/1000 | Loss: 0.00056411
Iteration 20/1000 | Loss: 0.00018578
Iteration 21/1000 | Loss: 0.00082732
Iteration 22/1000 | Loss: 0.00174767
Iteration 23/1000 | Loss: 0.00099845
Iteration 24/1000 | Loss: 0.00204661
Iteration 25/1000 | Loss: 0.00056224
Iteration 26/1000 | Loss: 0.00140807
Iteration 27/1000 | Loss: 0.00024058
Iteration 28/1000 | Loss: 0.00031581
Iteration 29/1000 | Loss: 0.00031207
Iteration 30/1000 | Loss: 0.00072294
Iteration 31/1000 | Loss: 0.00019643
Iteration 32/1000 | Loss: 0.00073841
Iteration 33/1000 | Loss: 0.00022731
Iteration 34/1000 | Loss: 0.00017302
Iteration 35/1000 | Loss: 0.00064035
Iteration 36/1000 | Loss: 0.00030594
Iteration 37/1000 | Loss: 0.00030109
Iteration 38/1000 | Loss: 0.00168115
Iteration 39/1000 | Loss: 0.00023321
Iteration 40/1000 | Loss: 0.00030219
Iteration 41/1000 | Loss: 0.00018745
Iteration 42/1000 | Loss: 0.00017733
Iteration 43/1000 | Loss: 0.00042291
Iteration 44/1000 | Loss: 0.00049603
Iteration 45/1000 | Loss: 0.00086840
Iteration 46/1000 | Loss: 0.00019058
Iteration 47/1000 | Loss: 0.00020339
Iteration 48/1000 | Loss: 0.00017028
Iteration 49/1000 | Loss: 0.00024597
Iteration 50/1000 | Loss: 0.00029624
Iteration 51/1000 | Loss: 0.00017705
Iteration 52/1000 | Loss: 0.00018671
Iteration 53/1000 | Loss: 0.00016855
Iteration 54/1000 | Loss: 0.00053797
Iteration 55/1000 | Loss: 0.00151359
Iteration 56/1000 | Loss: 0.00114571
Iteration 57/1000 | Loss: 0.00043330
Iteration 58/1000 | Loss: 0.00088800
Iteration 59/1000 | Loss: 0.00114644
Iteration 60/1000 | Loss: 0.00022410
Iteration 61/1000 | Loss: 0.00056030
Iteration 62/1000 | Loss: 0.00066273
Iteration 63/1000 | Loss: 0.00034616
Iteration 64/1000 | Loss: 0.00022006
Iteration 65/1000 | Loss: 0.00076019
Iteration 66/1000 | Loss: 0.00017168
Iteration 67/1000 | Loss: 0.00026269
Iteration 68/1000 | Loss: 0.00016380
Iteration 69/1000 | Loss: 0.00029018
Iteration 70/1000 | Loss: 0.00029670
Iteration 71/1000 | Loss: 0.00015709
Iteration 72/1000 | Loss: 0.00015605
Iteration 73/1000 | Loss: 0.00051303
Iteration 74/1000 | Loss: 0.00030600
Iteration 75/1000 | Loss: 0.00022846
Iteration 76/1000 | Loss: 0.00039108
Iteration 77/1000 | Loss: 0.00039261
Iteration 78/1000 | Loss: 0.00017056
Iteration 79/1000 | Loss: 0.00037274
Iteration 80/1000 | Loss: 0.00016179
Iteration 81/1000 | Loss: 0.00026755
Iteration 82/1000 | Loss: 0.00015557
Iteration 83/1000 | Loss: 0.00042574
Iteration 84/1000 | Loss: 0.00036169
Iteration 85/1000 | Loss: 0.00033120
Iteration 86/1000 | Loss: 0.00053886
Iteration 87/1000 | Loss: 0.00022753
Iteration 88/1000 | Loss: 0.00040079
Iteration 89/1000 | Loss: 0.00054342
Iteration 90/1000 | Loss: 0.00017003
Iteration 91/1000 | Loss: 0.00015927
Iteration 92/1000 | Loss: 0.00015587
Iteration 93/1000 | Loss: 0.00015472
Iteration 94/1000 | Loss: 0.00066577
Iteration 95/1000 | Loss: 0.00070754
Iteration 96/1000 | Loss: 0.00037678
Iteration 97/1000 | Loss: 0.00044741
Iteration 98/1000 | Loss: 0.00027451
Iteration 99/1000 | Loss: 0.00059373
Iteration 100/1000 | Loss: 0.00037237
Iteration 101/1000 | Loss: 0.00036641
Iteration 102/1000 | Loss: 0.00053435
Iteration 103/1000 | Loss: 0.00031390
Iteration 104/1000 | Loss: 0.00048372
Iteration 105/1000 | Loss: 0.00040205
Iteration 106/1000 | Loss: 0.00051427
Iteration 107/1000 | Loss: 0.00101099
Iteration 108/1000 | Loss: 0.00058636
Iteration 109/1000 | Loss: 0.00039509
Iteration 110/1000 | Loss: 0.00017066
Iteration 111/1000 | Loss: 0.00021804
Iteration 112/1000 | Loss: 0.00016673
Iteration 113/1000 | Loss: 0.00040777
Iteration 114/1000 | Loss: 0.00017474
Iteration 115/1000 | Loss: 0.00015093
Iteration 116/1000 | Loss: 0.00014832
Iteration 117/1000 | Loss: 0.00014708
Iteration 118/1000 | Loss: 0.00014616
Iteration 119/1000 | Loss: 0.00014581
Iteration 120/1000 | Loss: 0.00014556
Iteration 121/1000 | Loss: 0.00014538
Iteration 122/1000 | Loss: 0.00014527
Iteration 123/1000 | Loss: 0.00014523
Iteration 124/1000 | Loss: 0.00014517
Iteration 125/1000 | Loss: 0.00014517
Iteration 126/1000 | Loss: 0.00014517
Iteration 127/1000 | Loss: 0.00014517
Iteration 128/1000 | Loss: 0.00014517
Iteration 129/1000 | Loss: 0.00014517
Iteration 130/1000 | Loss: 0.00014517
Iteration 131/1000 | Loss: 0.00014517
Iteration 132/1000 | Loss: 0.00014517
Iteration 133/1000 | Loss: 0.00014517
Iteration 134/1000 | Loss: 0.00014517
Iteration 135/1000 | Loss: 0.00014516
Iteration 136/1000 | Loss: 0.00014516
Iteration 137/1000 | Loss: 0.00014516
Iteration 138/1000 | Loss: 0.00014516
Iteration 139/1000 | Loss: 0.00014516
Iteration 140/1000 | Loss: 0.00014516
Iteration 141/1000 | Loss: 0.00014516
Iteration 142/1000 | Loss: 0.00014515
Iteration 143/1000 | Loss: 0.00014515
Iteration 144/1000 | Loss: 0.00014515
Iteration 145/1000 | Loss: 0.00014515
Iteration 146/1000 | Loss: 0.00014515
Iteration 147/1000 | Loss: 0.00014515
Iteration 148/1000 | Loss: 0.00014515
Iteration 149/1000 | Loss: 0.00014515
Iteration 150/1000 | Loss: 0.00014515
Iteration 151/1000 | Loss: 0.00014515
Iteration 152/1000 | Loss: 0.00014515
Iteration 153/1000 | Loss: 0.00014514
Iteration 154/1000 | Loss: 0.00014514
Iteration 155/1000 | Loss: 0.00014514
Iteration 156/1000 | Loss: 0.00014514
Iteration 157/1000 | Loss: 0.00014514
Iteration 158/1000 | Loss: 0.00014514
Iteration 159/1000 | Loss: 0.00014514
Iteration 160/1000 | Loss: 0.00014513
Iteration 161/1000 | Loss: 0.00014513
Iteration 162/1000 | Loss: 0.00014513
Iteration 163/1000 | Loss: 0.00014513
Iteration 164/1000 | Loss: 0.00014513
Iteration 165/1000 | Loss: 0.00014513
Iteration 166/1000 | Loss: 0.00014513
Iteration 167/1000 | Loss: 0.00014512
Iteration 168/1000 | Loss: 0.00014512
Iteration 169/1000 | Loss: 0.00014512
Iteration 170/1000 | Loss: 0.00014512
Iteration 171/1000 | Loss: 0.00014511
Iteration 172/1000 | Loss: 0.00014511
Iteration 173/1000 | Loss: 0.00014511
Iteration 174/1000 | Loss: 0.00014511
Iteration 175/1000 | Loss: 0.00014511
Iteration 176/1000 | Loss: 0.00014511
Iteration 177/1000 | Loss: 0.00014511
Iteration 178/1000 | Loss: 0.00014511
Iteration 179/1000 | Loss: 0.00014510
Iteration 180/1000 | Loss: 0.00014510
Iteration 181/1000 | Loss: 0.00014509
Iteration 182/1000 | Loss: 0.00014509
Iteration 183/1000 | Loss: 0.00014509
Iteration 184/1000 | Loss: 0.00014509
Iteration 185/1000 | Loss: 0.00014505
Iteration 186/1000 | Loss: 0.00014505
Iteration 187/1000 | Loss: 0.00014505
Iteration 188/1000 | Loss: 0.00014505
Iteration 189/1000 | Loss: 0.00014505
Iteration 190/1000 | Loss: 0.00014504
Iteration 191/1000 | Loss: 0.00014504
Iteration 192/1000 | Loss: 0.00014504
Iteration 193/1000 | Loss: 0.00014504
Iteration 194/1000 | Loss: 0.00014504
Iteration 195/1000 | Loss: 0.00014504
Iteration 196/1000 | Loss: 0.00014504
Iteration 197/1000 | Loss: 0.00014503
Iteration 198/1000 | Loss: 0.00014503
Iteration 199/1000 | Loss: 0.00014503
Iteration 200/1000 | Loss: 0.00014503
Iteration 201/1000 | Loss: 0.00014503
Iteration 202/1000 | Loss: 0.00014503
Iteration 203/1000 | Loss: 0.00014503
Iteration 204/1000 | Loss: 0.00014503
Iteration 205/1000 | Loss: 0.00014503
Iteration 206/1000 | Loss: 0.00014503
Iteration 207/1000 | Loss: 0.00014503
Iteration 208/1000 | Loss: 0.00014503
Iteration 209/1000 | Loss: 0.00014503
Iteration 210/1000 | Loss: 0.00014503
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [0.00014502604608424008, 0.00014502604608424008, 0.00014502604608424008, 0.00014502604608424008, 0.00014502604608424008]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00014502604608424008

Optimization complete. Final v2v error: 6.284531116485596 mm

Highest mean error: 12.552101135253906 mm for frame 104

Lowest mean error: 4.083765029907227 mm for frame 114

Saving results

Total time: 221.2590913772583
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_it_4274/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_it_4274/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_it_4274/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00927673
Iteration 2/25 | Loss: 0.00104330
Iteration 3/25 | Loss: 0.00083804
Iteration 4/25 | Loss: 0.00078925
Iteration 5/25 | Loss: 0.00076724
Iteration 6/25 | Loss: 0.00075925
Iteration 7/25 | Loss: 0.00075714
Iteration 8/25 | Loss: 0.00075655
Iteration 9/25 | Loss: 0.00075652
Iteration 10/25 | Loss: 0.00075652
Iteration 11/25 | Loss: 0.00075652
Iteration 12/25 | Loss: 0.00075652
Iteration 13/25 | Loss: 0.00075652
Iteration 14/25 | Loss: 0.00075652
Iteration 15/25 | Loss: 0.00075652
Iteration 16/25 | Loss: 0.00075652
Iteration 17/25 | Loss: 0.00075652
Iteration 18/25 | Loss: 0.00075652
Iteration 19/25 | Loss: 0.00075652
Iteration 20/25 | Loss: 0.00075652
Iteration 21/25 | Loss: 0.00075652
Iteration 22/25 | Loss: 0.00075652
Iteration 23/25 | Loss: 0.00075652
Iteration 24/25 | Loss: 0.00075652
Iteration 25/25 | Loss: 0.00075652

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33444178
Iteration 2/25 | Loss: 0.00025822
Iteration 3/25 | Loss: 0.00025818
Iteration 4/25 | Loss: 0.00025818
Iteration 5/25 | Loss: 0.00025818
Iteration 6/25 | Loss: 0.00025818
Iteration 7/25 | Loss: 0.00025818
Iteration 8/25 | Loss: 0.00025818
Iteration 9/25 | Loss: 0.00025818
Iteration 10/25 | Loss: 0.00025818
Iteration 11/25 | Loss: 0.00025818
Iteration 12/25 | Loss: 0.00025818
Iteration 13/25 | Loss: 0.00025818
Iteration 14/25 | Loss: 0.00025818
Iteration 15/25 | Loss: 0.00025818
Iteration 16/25 | Loss: 0.00025818
Iteration 17/25 | Loss: 0.00025818
Iteration 18/25 | Loss: 0.00025818
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00025817996356636286, 0.00025817996356636286, 0.00025817996356636286, 0.00025817996356636286, 0.00025817996356636286]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00025817996356636286

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025818
Iteration 2/1000 | Loss: 0.00005178
Iteration 3/1000 | Loss: 0.00004133
Iteration 4/1000 | Loss: 0.00003662
Iteration 5/1000 | Loss: 0.00003441
Iteration 6/1000 | Loss: 0.00003307
Iteration 7/1000 | Loss: 0.00003195
Iteration 8/1000 | Loss: 0.00003114
Iteration 9/1000 | Loss: 0.00003061
Iteration 10/1000 | Loss: 0.00003025
Iteration 11/1000 | Loss: 0.00003002
Iteration 12/1000 | Loss: 0.00002985
Iteration 13/1000 | Loss: 0.00002976
Iteration 14/1000 | Loss: 0.00002970
Iteration 15/1000 | Loss: 0.00002969
Iteration 16/1000 | Loss: 0.00002969
Iteration 17/1000 | Loss: 0.00002969
Iteration 18/1000 | Loss: 0.00002968
Iteration 19/1000 | Loss: 0.00002968
Iteration 20/1000 | Loss: 0.00002967
Iteration 21/1000 | Loss: 0.00002965
Iteration 22/1000 | Loss: 0.00002965
Iteration 23/1000 | Loss: 0.00002965
Iteration 24/1000 | Loss: 0.00002964
Iteration 25/1000 | Loss: 0.00002964
Iteration 26/1000 | Loss: 0.00002964
Iteration 27/1000 | Loss: 0.00002962
Iteration 28/1000 | Loss: 0.00002961
Iteration 29/1000 | Loss: 0.00002960
Iteration 30/1000 | Loss: 0.00002960
Iteration 31/1000 | Loss: 0.00002959
Iteration 32/1000 | Loss: 0.00002959
Iteration 33/1000 | Loss: 0.00002958
Iteration 34/1000 | Loss: 0.00002958
Iteration 35/1000 | Loss: 0.00002958
Iteration 36/1000 | Loss: 0.00002958
Iteration 37/1000 | Loss: 0.00002958
Iteration 38/1000 | Loss: 0.00002958
Iteration 39/1000 | Loss: 0.00002958
Iteration 40/1000 | Loss: 0.00002957
Iteration 41/1000 | Loss: 0.00002957
Iteration 42/1000 | Loss: 0.00002957
Iteration 43/1000 | Loss: 0.00002956
Iteration 44/1000 | Loss: 0.00002956
Iteration 45/1000 | Loss: 0.00002956
Iteration 46/1000 | Loss: 0.00002955
Iteration 47/1000 | Loss: 0.00002955
Iteration 48/1000 | Loss: 0.00002955
Iteration 49/1000 | Loss: 0.00002955
Iteration 50/1000 | Loss: 0.00002955
Iteration 51/1000 | Loss: 0.00002955
Iteration 52/1000 | Loss: 0.00002955
Iteration 53/1000 | Loss: 0.00002955
Iteration 54/1000 | Loss: 0.00002954
Iteration 55/1000 | Loss: 0.00002954
Iteration 56/1000 | Loss: 0.00002954
Iteration 57/1000 | Loss: 0.00002954
Iteration 58/1000 | Loss: 0.00002954
Iteration 59/1000 | Loss: 0.00002954
Iteration 60/1000 | Loss: 0.00002954
Iteration 61/1000 | Loss: 0.00002954
Iteration 62/1000 | Loss: 0.00002953
Iteration 63/1000 | Loss: 0.00002953
Iteration 64/1000 | Loss: 0.00002953
Iteration 65/1000 | Loss: 0.00002953
Iteration 66/1000 | Loss: 0.00002953
Iteration 67/1000 | Loss: 0.00002953
Iteration 68/1000 | Loss: 0.00002953
Iteration 69/1000 | Loss: 0.00002953
Iteration 70/1000 | Loss: 0.00002953
Iteration 71/1000 | Loss: 0.00002952
Iteration 72/1000 | Loss: 0.00002952
Iteration 73/1000 | Loss: 0.00002952
Iteration 74/1000 | Loss: 0.00002952
Iteration 75/1000 | Loss: 0.00002952
Iteration 76/1000 | Loss: 0.00002952
Iteration 77/1000 | Loss: 0.00002951
Iteration 78/1000 | Loss: 0.00002951
Iteration 79/1000 | Loss: 0.00002951
Iteration 80/1000 | Loss: 0.00002951
Iteration 81/1000 | Loss: 0.00002951
Iteration 82/1000 | Loss: 0.00002951
Iteration 83/1000 | Loss: 0.00002951
Iteration 84/1000 | Loss: 0.00002951
Iteration 85/1000 | Loss: 0.00002951
Iteration 86/1000 | Loss: 0.00002951
Iteration 87/1000 | Loss: 0.00002951
Iteration 88/1000 | Loss: 0.00002951
Iteration 89/1000 | Loss: 0.00002951
Iteration 90/1000 | Loss: 0.00002951
Iteration 91/1000 | Loss: 0.00002951
Iteration 92/1000 | Loss: 0.00002951
Iteration 93/1000 | Loss: 0.00002950
Iteration 94/1000 | Loss: 0.00002950
Iteration 95/1000 | Loss: 0.00002950
Iteration 96/1000 | Loss: 0.00002950
Iteration 97/1000 | Loss: 0.00002950
Iteration 98/1000 | Loss: 0.00002950
Iteration 99/1000 | Loss: 0.00002950
Iteration 100/1000 | Loss: 0.00002950
Iteration 101/1000 | Loss: 0.00002950
Iteration 102/1000 | Loss: 0.00002950
Iteration 103/1000 | Loss: 0.00002950
Iteration 104/1000 | Loss: 0.00002950
Iteration 105/1000 | Loss: 0.00002950
Iteration 106/1000 | Loss: 0.00002950
Iteration 107/1000 | Loss: 0.00002950
Iteration 108/1000 | Loss: 0.00002950
Iteration 109/1000 | Loss: 0.00002949
Iteration 110/1000 | Loss: 0.00002949
Iteration 111/1000 | Loss: 0.00002949
Iteration 112/1000 | Loss: 0.00002949
Iteration 113/1000 | Loss: 0.00002949
Iteration 114/1000 | Loss: 0.00002949
Iteration 115/1000 | Loss: 0.00002949
Iteration 116/1000 | Loss: 0.00002949
Iteration 117/1000 | Loss: 0.00002949
Iteration 118/1000 | Loss: 0.00002949
Iteration 119/1000 | Loss: 0.00002949
Iteration 120/1000 | Loss: 0.00002949
Iteration 121/1000 | Loss: 0.00002949
Iteration 122/1000 | Loss: 0.00002949
Iteration 123/1000 | Loss: 0.00002949
Iteration 124/1000 | Loss: 0.00002949
Iteration 125/1000 | Loss: 0.00002948
Iteration 126/1000 | Loss: 0.00002948
Iteration 127/1000 | Loss: 0.00002948
Iteration 128/1000 | Loss: 0.00002948
Iteration 129/1000 | Loss: 0.00002948
Iteration 130/1000 | Loss: 0.00002948
Iteration 131/1000 | Loss: 0.00002948
Iteration 132/1000 | Loss: 0.00002948
Iteration 133/1000 | Loss: 0.00002948
Iteration 134/1000 | Loss: 0.00002948
Iteration 135/1000 | Loss: 0.00002948
Iteration 136/1000 | Loss: 0.00002948
Iteration 137/1000 | Loss: 0.00002948
Iteration 138/1000 | Loss: 0.00002948
Iteration 139/1000 | Loss: 0.00002948
Iteration 140/1000 | Loss: 0.00002948
Iteration 141/1000 | Loss: 0.00002948
Iteration 142/1000 | Loss: 0.00002947
Iteration 143/1000 | Loss: 0.00002947
Iteration 144/1000 | Loss: 0.00002947
Iteration 145/1000 | Loss: 0.00002947
Iteration 146/1000 | Loss: 0.00002947
Iteration 147/1000 | Loss: 0.00002947
Iteration 148/1000 | Loss: 0.00002947
Iteration 149/1000 | Loss: 0.00002947
Iteration 150/1000 | Loss: 0.00002947
Iteration 151/1000 | Loss: 0.00002947
Iteration 152/1000 | Loss: 0.00002947
Iteration 153/1000 | Loss: 0.00002947
Iteration 154/1000 | Loss: 0.00002947
Iteration 155/1000 | Loss: 0.00002947
Iteration 156/1000 | Loss: 0.00002947
Iteration 157/1000 | Loss: 0.00002947
Iteration 158/1000 | Loss: 0.00002947
Iteration 159/1000 | Loss: 0.00002947
Iteration 160/1000 | Loss: 0.00002947
Iteration 161/1000 | Loss: 0.00002947
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [2.9473343602148816e-05, 2.9473343602148816e-05, 2.9473343602148816e-05, 2.9473343602148816e-05, 2.9473343602148816e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9473343602148816e-05

Optimization complete. Final v2v error: 4.444891929626465 mm

Highest mean error: 5.921871662139893 mm for frame 71

Lowest mean error: 3.713873863220215 mm for frame 101

Saving results

Total time: 39.25403594970703
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_it_4274/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_it_4274/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_it_4274/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01110478
Iteration 2/25 | Loss: 0.01110478
Iteration 3/25 | Loss: 0.01110478
Iteration 4/25 | Loss: 0.01110478
Iteration 5/25 | Loss: 0.01110477
Iteration 6/25 | Loss: 0.01110477
Iteration 7/25 | Loss: 0.01110477
Iteration 8/25 | Loss: 0.01110477
Iteration 9/25 | Loss: 0.01110477
Iteration 10/25 | Loss: 0.01110476
Iteration 11/25 | Loss: 0.01110476
Iteration 12/25 | Loss: 0.01110476
Iteration 13/25 | Loss: 0.01110476
Iteration 14/25 | Loss: 0.01110476
Iteration 15/25 | Loss: 0.01110475
Iteration 16/25 | Loss: 0.01110475
Iteration 17/25 | Loss: 0.01110475
Iteration 18/25 | Loss: 0.01110475
Iteration 19/25 | Loss: 0.01110475
Iteration 20/25 | Loss: 0.01110475
Iteration 21/25 | Loss: 0.01110475
Iteration 22/25 | Loss: 0.01110475
Iteration 23/25 | Loss: 0.01110474
Iteration 24/25 | Loss: 0.01110474
Iteration 25/25 | Loss: 0.01110474

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.05701280
Iteration 2/25 | Loss: 0.09824014
Iteration 3/25 | Loss: 0.09789339
Iteration 4/25 | Loss: 0.09733862
Iteration 5/25 | Loss: 0.09728221
Iteration 6/25 | Loss: 0.09728215
Iteration 7/25 | Loss: 0.09728213
Iteration 8/25 | Loss: 0.09728213
Iteration 9/25 | Loss: 0.09728213
Iteration 10/25 | Loss: 0.09728213
Iteration 11/25 | Loss: 0.09728213
Iteration 12/25 | Loss: 0.09728213
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.09728212654590607, 0.09728212654590607, 0.09728212654590607, 0.09728212654590607, 0.09728212654590607]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.09728212654590607

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.09728212
Iteration 2/1000 | Loss: 0.00163194
Iteration 3/1000 | Loss: 0.00113899
Iteration 4/1000 | Loss: 0.00031973
Iteration 5/1000 | Loss: 0.00085119
Iteration 6/1000 | Loss: 0.00009797
Iteration 7/1000 | Loss: 0.00028142
Iteration 8/1000 | Loss: 0.00031772
Iteration 9/1000 | Loss: 0.00041171
Iteration 10/1000 | Loss: 0.00006637
Iteration 11/1000 | Loss: 0.00010216
Iteration 12/1000 | Loss: 0.00013140
Iteration 13/1000 | Loss: 0.00009464
Iteration 14/1000 | Loss: 0.00042927
Iteration 15/1000 | Loss: 0.00005554
Iteration 16/1000 | Loss: 0.00039619
Iteration 17/1000 | Loss: 0.00013944
Iteration 18/1000 | Loss: 0.00006871
Iteration 19/1000 | Loss: 0.00014306
Iteration 20/1000 | Loss: 0.00016992
Iteration 21/1000 | Loss: 0.00048611
Iteration 22/1000 | Loss: 0.00004307
Iteration 23/1000 | Loss: 0.00010725
Iteration 24/1000 | Loss: 0.00004200
Iteration 25/1000 | Loss: 0.00006695
Iteration 26/1000 | Loss: 0.00009027
Iteration 27/1000 | Loss: 0.00009175
Iteration 28/1000 | Loss: 0.00003842
Iteration 29/1000 | Loss: 0.00011354
Iteration 30/1000 | Loss: 0.00006222
Iteration 31/1000 | Loss: 0.00013312
Iteration 32/1000 | Loss: 0.00003050
Iteration 33/1000 | Loss: 0.00010823
Iteration 34/1000 | Loss: 0.00007771
Iteration 35/1000 | Loss: 0.00003088
Iteration 36/1000 | Loss: 0.00015359
Iteration 37/1000 | Loss: 0.00002857
Iteration 38/1000 | Loss: 0.00002748
Iteration 39/1000 | Loss: 0.00014327
Iteration 40/1000 | Loss: 0.00034165
Iteration 41/1000 | Loss: 0.00007359
Iteration 42/1000 | Loss: 0.00003531
Iteration 43/1000 | Loss: 0.00002942
Iteration 44/1000 | Loss: 0.00002678
Iteration 45/1000 | Loss: 0.00002675
Iteration 46/1000 | Loss: 0.00002673
Iteration 47/1000 | Loss: 0.00013392
Iteration 48/1000 | Loss: 0.00003178
Iteration 49/1000 | Loss: 0.00005987
Iteration 50/1000 | Loss: 0.00011462
Iteration 51/1000 | Loss: 0.00003144
Iteration 52/1000 | Loss: 0.00002944
Iteration 53/1000 | Loss: 0.00003384
Iteration 54/1000 | Loss: 0.00002622
Iteration 55/1000 | Loss: 0.00004714
Iteration 56/1000 | Loss: 0.00002873
Iteration 57/1000 | Loss: 0.00002647
Iteration 58/1000 | Loss: 0.00006768
Iteration 59/1000 | Loss: 0.00025439
Iteration 60/1000 | Loss: 0.00002723
Iteration 61/1000 | Loss: 0.00005993
Iteration 62/1000 | Loss: 0.00005218
Iteration 63/1000 | Loss: 0.00002777
Iteration 64/1000 | Loss: 0.00002574
Iteration 65/1000 | Loss: 0.00002572
Iteration 66/1000 | Loss: 0.00002572
Iteration 67/1000 | Loss: 0.00002572
Iteration 68/1000 | Loss: 0.00002572
Iteration 69/1000 | Loss: 0.00002623
Iteration 70/1000 | Loss: 0.00002622
Iteration 71/1000 | Loss: 0.00002576
Iteration 72/1000 | Loss: 0.00002565
Iteration 73/1000 | Loss: 0.00002565
Iteration 74/1000 | Loss: 0.00002565
Iteration 75/1000 | Loss: 0.00002564
Iteration 76/1000 | Loss: 0.00002564
Iteration 77/1000 | Loss: 0.00002564
Iteration 78/1000 | Loss: 0.00002564
Iteration 79/1000 | Loss: 0.00002563
Iteration 80/1000 | Loss: 0.00002608
Iteration 81/1000 | Loss: 0.00007413
Iteration 82/1000 | Loss: 0.00002602
Iteration 83/1000 | Loss: 0.00002554
Iteration 84/1000 | Loss: 0.00002554
Iteration 85/1000 | Loss: 0.00002553
Iteration 86/1000 | Loss: 0.00002553
Iteration 87/1000 | Loss: 0.00002553
Iteration 88/1000 | Loss: 0.00002553
Iteration 89/1000 | Loss: 0.00002553
Iteration 90/1000 | Loss: 0.00002553
Iteration 91/1000 | Loss: 0.00002553
Iteration 92/1000 | Loss: 0.00002553
Iteration 93/1000 | Loss: 0.00002553
Iteration 94/1000 | Loss: 0.00002553
Iteration 95/1000 | Loss: 0.00002553
Iteration 96/1000 | Loss: 0.00002553
Iteration 97/1000 | Loss: 0.00002553
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [2.5532290237606503e-05, 2.5532290237606503e-05, 2.5532290237606503e-05, 2.5532290237606503e-05, 2.5532290237606503e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5532290237606503e-05

Optimization complete. Final v2v error: 4.305482387542725 mm

Highest mean error: 6.005049705505371 mm for frame 192

Lowest mean error: 3.5342018604278564 mm for frame 10

Saving results

Total time: 116.2654402256012
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_it_4274/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_it_4274/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_it_4274/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00473431
Iteration 2/25 | Loss: 0.00096628
Iteration 3/25 | Loss: 0.00075685
Iteration 4/25 | Loss: 0.00072137
Iteration 5/25 | Loss: 0.00071435
Iteration 6/25 | Loss: 0.00071281
Iteration 7/25 | Loss: 0.00071274
Iteration 8/25 | Loss: 0.00071274
Iteration 9/25 | Loss: 0.00071274
Iteration 10/25 | Loss: 0.00071274
Iteration 11/25 | Loss: 0.00071274
Iteration 12/25 | Loss: 0.00071274
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007127351127564907, 0.0007127351127564907, 0.0007127351127564907, 0.0007127351127564907, 0.0007127351127564907]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007127351127564907

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35893750
Iteration 2/25 | Loss: 0.00019947
Iteration 3/25 | Loss: 0.00019947
Iteration 4/25 | Loss: 0.00019947
Iteration 5/25 | Loss: 0.00019947
Iteration 6/25 | Loss: 0.00019947
Iteration 7/25 | Loss: 0.00019947
Iteration 8/25 | Loss: 0.00019947
Iteration 9/25 | Loss: 0.00019947
Iteration 10/25 | Loss: 0.00019947
Iteration 11/25 | Loss: 0.00019947
Iteration 12/25 | Loss: 0.00019947
Iteration 13/25 | Loss: 0.00019947
Iteration 14/25 | Loss: 0.00019947
Iteration 15/25 | Loss: 0.00019947
Iteration 16/25 | Loss: 0.00019947
Iteration 17/25 | Loss: 0.00019947
Iteration 18/25 | Loss: 0.00019947
Iteration 19/25 | Loss: 0.00019947
Iteration 20/25 | Loss: 0.00019947
Iteration 21/25 | Loss: 0.00019947
Iteration 22/25 | Loss: 0.00019947
Iteration 23/25 | Loss: 0.00019947
Iteration 24/25 | Loss: 0.00019947
Iteration 25/25 | Loss: 0.00019947

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00019947
Iteration 2/1000 | Loss: 0.00003699
Iteration 3/1000 | Loss: 0.00003221
Iteration 4/1000 | Loss: 0.00003035
Iteration 5/1000 | Loss: 0.00002895
Iteration 6/1000 | Loss: 0.00002824
Iteration 7/1000 | Loss: 0.00002762
Iteration 8/1000 | Loss: 0.00002726
Iteration 9/1000 | Loss: 0.00002692
Iteration 10/1000 | Loss: 0.00002676
Iteration 11/1000 | Loss: 0.00002665
Iteration 12/1000 | Loss: 0.00002658
Iteration 13/1000 | Loss: 0.00002657
Iteration 14/1000 | Loss: 0.00002657
Iteration 15/1000 | Loss: 0.00002654
Iteration 16/1000 | Loss: 0.00002653
Iteration 17/1000 | Loss: 0.00002653
Iteration 18/1000 | Loss: 0.00002653
Iteration 19/1000 | Loss: 0.00002653
Iteration 20/1000 | Loss: 0.00002652
Iteration 21/1000 | Loss: 0.00002652
Iteration 22/1000 | Loss: 0.00002649
Iteration 23/1000 | Loss: 0.00002648
Iteration 24/1000 | Loss: 0.00002648
Iteration 25/1000 | Loss: 0.00002648
Iteration 26/1000 | Loss: 0.00002645
Iteration 27/1000 | Loss: 0.00002642
Iteration 28/1000 | Loss: 0.00002640
Iteration 29/1000 | Loss: 0.00002638
Iteration 30/1000 | Loss: 0.00002638
Iteration 31/1000 | Loss: 0.00002637
Iteration 32/1000 | Loss: 0.00002636
Iteration 33/1000 | Loss: 0.00002635
Iteration 34/1000 | Loss: 0.00002635
Iteration 35/1000 | Loss: 0.00002635
Iteration 36/1000 | Loss: 0.00002635
Iteration 37/1000 | Loss: 0.00002634
Iteration 38/1000 | Loss: 0.00002634
Iteration 39/1000 | Loss: 0.00002634
Iteration 40/1000 | Loss: 0.00002634
Iteration 41/1000 | Loss: 0.00002634
Iteration 42/1000 | Loss: 0.00002634
Iteration 43/1000 | Loss: 0.00002634
Iteration 44/1000 | Loss: 0.00002634
Iteration 45/1000 | Loss: 0.00002634
Iteration 46/1000 | Loss: 0.00002634
Iteration 47/1000 | Loss: 0.00002634
Iteration 48/1000 | Loss: 0.00002633
Iteration 49/1000 | Loss: 0.00002633
Iteration 50/1000 | Loss: 0.00002633
Iteration 51/1000 | Loss: 0.00002633
Iteration 52/1000 | Loss: 0.00002633
Iteration 53/1000 | Loss: 0.00002633
Iteration 54/1000 | Loss: 0.00002633
Iteration 55/1000 | Loss: 0.00002633
Iteration 56/1000 | Loss: 0.00002633
Iteration 57/1000 | Loss: 0.00002633
Iteration 58/1000 | Loss: 0.00002633
Iteration 59/1000 | Loss: 0.00002633
Iteration 60/1000 | Loss: 0.00002633
Iteration 61/1000 | Loss: 0.00002632
Iteration 62/1000 | Loss: 0.00002632
Iteration 63/1000 | Loss: 0.00002632
Iteration 64/1000 | Loss: 0.00002632
Iteration 65/1000 | Loss: 0.00002632
Iteration 66/1000 | Loss: 0.00002631
Iteration 67/1000 | Loss: 0.00002631
Iteration 68/1000 | Loss: 0.00002631
Iteration 69/1000 | Loss: 0.00002631
Iteration 70/1000 | Loss: 0.00002631
Iteration 71/1000 | Loss: 0.00002631
Iteration 72/1000 | Loss: 0.00002631
Iteration 73/1000 | Loss: 0.00002631
Iteration 74/1000 | Loss: 0.00002630
Iteration 75/1000 | Loss: 0.00002630
Iteration 76/1000 | Loss: 0.00002630
Iteration 77/1000 | Loss: 0.00002630
Iteration 78/1000 | Loss: 0.00002630
Iteration 79/1000 | Loss: 0.00002629
Iteration 80/1000 | Loss: 0.00002629
Iteration 81/1000 | Loss: 0.00002629
Iteration 82/1000 | Loss: 0.00002629
Iteration 83/1000 | Loss: 0.00002629
Iteration 84/1000 | Loss: 0.00002629
Iteration 85/1000 | Loss: 0.00002629
Iteration 86/1000 | Loss: 0.00002629
Iteration 87/1000 | Loss: 0.00002629
Iteration 88/1000 | Loss: 0.00002629
Iteration 89/1000 | Loss: 0.00002629
Iteration 90/1000 | Loss: 0.00002629
Iteration 91/1000 | Loss: 0.00002629
Iteration 92/1000 | Loss: 0.00002629
Iteration 93/1000 | Loss: 0.00002628
Iteration 94/1000 | Loss: 0.00002628
Iteration 95/1000 | Loss: 0.00002628
Iteration 96/1000 | Loss: 0.00002628
Iteration 97/1000 | Loss: 0.00002628
Iteration 98/1000 | Loss: 0.00002628
Iteration 99/1000 | Loss: 0.00002628
Iteration 100/1000 | Loss: 0.00002628
Iteration 101/1000 | Loss: 0.00002628
Iteration 102/1000 | Loss: 0.00002628
Iteration 103/1000 | Loss: 0.00002628
Iteration 104/1000 | Loss: 0.00002628
Iteration 105/1000 | Loss: 0.00002628
Iteration 106/1000 | Loss: 0.00002628
Iteration 107/1000 | Loss: 0.00002628
Iteration 108/1000 | Loss: 0.00002628
Iteration 109/1000 | Loss: 0.00002628
Iteration 110/1000 | Loss: 0.00002628
Iteration 111/1000 | Loss: 0.00002628
Iteration 112/1000 | Loss: 0.00002628
Iteration 113/1000 | Loss: 0.00002628
Iteration 114/1000 | Loss: 0.00002628
Iteration 115/1000 | Loss: 0.00002628
Iteration 116/1000 | Loss: 0.00002628
Iteration 117/1000 | Loss: 0.00002628
Iteration 118/1000 | Loss: 0.00002628
Iteration 119/1000 | Loss: 0.00002628
Iteration 120/1000 | Loss: 0.00002628
Iteration 121/1000 | Loss: 0.00002628
Iteration 122/1000 | Loss: 0.00002628
Iteration 123/1000 | Loss: 0.00002628
Iteration 124/1000 | Loss: 0.00002628
Iteration 125/1000 | Loss: 0.00002628
Iteration 126/1000 | Loss: 0.00002628
Iteration 127/1000 | Loss: 0.00002628
Iteration 128/1000 | Loss: 0.00002628
Iteration 129/1000 | Loss: 0.00002628
Iteration 130/1000 | Loss: 0.00002628
Iteration 131/1000 | Loss: 0.00002628
Iteration 132/1000 | Loss: 0.00002628
Iteration 133/1000 | Loss: 0.00002628
Iteration 134/1000 | Loss: 0.00002628
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [2.627511821629014e-05, 2.627511821629014e-05, 2.627511821629014e-05, 2.627511821629014e-05, 2.627511821629014e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.627511821629014e-05

Optimization complete. Final v2v error: 4.359659671783447 mm

Highest mean error: 4.928460121154785 mm for frame 57

Lowest mean error: 3.8416271209716797 mm for frame 229

Saving results

Total time: 39.77373957633972
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_it_4274/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_it_4274/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_it_4274/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01155339
Iteration 2/25 | Loss: 0.00275803
Iteration 3/25 | Loss: 0.00162290
Iteration 4/25 | Loss: 0.00144701
Iteration 5/25 | Loss: 0.00131784
Iteration 6/25 | Loss: 0.00132515
Iteration 7/25 | Loss: 0.00117777
Iteration 8/25 | Loss: 0.00106717
Iteration 9/25 | Loss: 0.00107564
Iteration 10/25 | Loss: 0.00109274
Iteration 11/25 | Loss: 0.00103123
Iteration 12/25 | Loss: 0.00104004
Iteration 13/25 | Loss: 0.00100155
Iteration 14/25 | Loss: 0.00097724
Iteration 15/25 | Loss: 0.00097027
Iteration 16/25 | Loss: 0.00094461
Iteration 17/25 | Loss: 0.00093157
Iteration 18/25 | Loss: 0.00092772
Iteration 19/25 | Loss: 0.00092930
Iteration 20/25 | Loss: 0.00090832
Iteration 21/25 | Loss: 0.00089663
Iteration 22/25 | Loss: 0.00088939
Iteration 23/25 | Loss: 0.00088513
Iteration 24/25 | Loss: 0.00087486
Iteration 25/25 | Loss: 0.00088025

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.06574798
Iteration 2/25 | Loss: 0.00210104
Iteration 3/25 | Loss: 0.00202844
Iteration 4/25 | Loss: 0.00202844
Iteration 5/25 | Loss: 0.00202844
Iteration 6/25 | Loss: 0.00202844
Iteration 7/25 | Loss: 0.00202844
Iteration 8/25 | Loss: 0.00202844
Iteration 9/25 | Loss: 0.00202844
Iteration 10/25 | Loss: 0.00202844
Iteration 11/25 | Loss: 0.00202844
Iteration 12/25 | Loss: 0.00202844
Iteration 13/25 | Loss: 0.00202844
Iteration 14/25 | Loss: 0.00202844
Iteration 15/25 | Loss: 0.00202844
Iteration 16/25 | Loss: 0.00202844
Iteration 17/25 | Loss: 0.00202844
Iteration 18/25 | Loss: 0.00202844
Iteration 19/25 | Loss: 0.00202844
Iteration 20/25 | Loss: 0.00202844
Iteration 21/25 | Loss: 0.00202844
Iteration 22/25 | Loss: 0.00202844
Iteration 23/25 | Loss: 0.00202844
Iteration 24/25 | Loss: 0.00202844
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0020284363999962807, 0.0020284363999962807, 0.0020284363999962807, 0.0020284363999962807, 0.0020284363999962807]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020284363999962807

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00202844
Iteration 2/1000 | Loss: 0.00108724
Iteration 3/1000 | Loss: 0.00057343
Iteration 4/1000 | Loss: 0.00032749
Iteration 5/1000 | Loss: 0.00018640
Iteration 6/1000 | Loss: 0.00020135
Iteration 7/1000 | Loss: 0.00019641
Iteration 8/1000 | Loss: 0.00160096
Iteration 9/1000 | Loss: 0.00084935
Iteration 10/1000 | Loss: 0.00056289
Iteration 11/1000 | Loss: 0.00054593
Iteration 12/1000 | Loss: 0.00043236
Iteration 13/1000 | Loss: 0.00026118
Iteration 14/1000 | Loss: 0.00033059
Iteration 15/1000 | Loss: 0.00036559
Iteration 16/1000 | Loss: 0.00019688
Iteration 17/1000 | Loss: 0.00011967
Iteration 18/1000 | Loss: 0.00019792
Iteration 19/1000 | Loss: 0.00054924
Iteration 20/1000 | Loss: 0.00028404
Iteration 21/1000 | Loss: 0.00018188
Iteration 22/1000 | Loss: 0.00015882
Iteration 23/1000 | Loss: 0.00060903
Iteration 24/1000 | Loss: 0.00070616
Iteration 25/1000 | Loss: 0.00032646
Iteration 26/1000 | Loss: 0.00024844
Iteration 27/1000 | Loss: 0.00150903
Iteration 28/1000 | Loss: 0.00041329
Iteration 29/1000 | Loss: 0.00119341
Iteration 30/1000 | Loss: 0.00081840
Iteration 31/1000 | Loss: 0.00048310
Iteration 32/1000 | Loss: 0.00046095
Iteration 33/1000 | Loss: 0.00025344
Iteration 34/1000 | Loss: 0.00021820
Iteration 35/1000 | Loss: 0.00019635
Iteration 36/1000 | Loss: 0.00131670
Iteration 37/1000 | Loss: 0.00097473
Iteration 38/1000 | Loss: 0.00070091
Iteration 39/1000 | Loss: 0.00049380
Iteration 40/1000 | Loss: 0.00016476
Iteration 41/1000 | Loss: 0.00012557
Iteration 42/1000 | Loss: 0.00017164
Iteration 43/1000 | Loss: 0.00015124
Iteration 44/1000 | Loss: 0.00013638
Iteration 45/1000 | Loss: 0.00027636
Iteration 46/1000 | Loss: 0.00017227
Iteration 47/1000 | Loss: 0.00013709
Iteration 48/1000 | Loss: 0.00016415
Iteration 49/1000 | Loss: 0.00014774
Iteration 50/1000 | Loss: 0.00014643
Iteration 51/1000 | Loss: 0.00015010
Iteration 52/1000 | Loss: 0.00014564
Iteration 53/1000 | Loss: 0.00014075
Iteration 54/1000 | Loss: 0.00012982
Iteration 55/1000 | Loss: 0.00023315
Iteration 56/1000 | Loss: 0.00041198
Iteration 57/1000 | Loss: 0.00012786
Iteration 58/1000 | Loss: 0.00022948
Iteration 59/1000 | Loss: 0.00012317
Iteration 60/1000 | Loss: 0.00013342
Iteration 61/1000 | Loss: 0.00013838
Iteration 62/1000 | Loss: 0.00013939
Iteration 63/1000 | Loss: 0.00012495
Iteration 64/1000 | Loss: 0.00018227
Iteration 65/1000 | Loss: 0.00013883
Iteration 66/1000 | Loss: 0.00027311
Iteration 67/1000 | Loss: 0.00014215
Iteration 68/1000 | Loss: 0.00025605
Iteration 69/1000 | Loss: 0.00014198
Iteration 70/1000 | Loss: 0.00014250
Iteration 71/1000 | Loss: 0.00013814
Iteration 72/1000 | Loss: 0.00014086
Iteration 73/1000 | Loss: 0.00013543
Iteration 74/1000 | Loss: 0.00013550
Iteration 75/1000 | Loss: 0.00028107
Iteration 76/1000 | Loss: 0.00020034
Iteration 77/1000 | Loss: 0.00014739
Iteration 78/1000 | Loss: 0.00016800
Iteration 79/1000 | Loss: 0.00014693
Iteration 80/1000 | Loss: 0.00016140
Iteration 81/1000 | Loss: 0.00013352
Iteration 82/1000 | Loss: 0.00020683
Iteration 83/1000 | Loss: 0.00018509
Iteration 84/1000 | Loss: 0.00016807
Iteration 85/1000 | Loss: 0.00013429
Iteration 86/1000 | Loss: 0.00012793
Iteration 87/1000 | Loss: 0.00014475
Iteration 88/1000 | Loss: 0.00013943
Iteration 89/1000 | Loss: 0.00014498
Iteration 90/1000 | Loss: 0.00007252
Iteration 91/1000 | Loss: 0.00006399
Iteration 92/1000 | Loss: 0.00009275
Iteration 93/1000 | Loss: 0.00008755
Iteration 94/1000 | Loss: 0.00012008
Iteration 95/1000 | Loss: 0.00011169
Iteration 96/1000 | Loss: 0.00013475
Iteration 97/1000 | Loss: 0.00010415
Iteration 98/1000 | Loss: 0.00015475
Iteration 99/1000 | Loss: 0.00011742
Iteration 100/1000 | Loss: 0.00016794
Iteration 101/1000 | Loss: 0.00011081
Iteration 102/1000 | Loss: 0.00015501
Iteration 103/1000 | Loss: 0.00015951
Iteration 104/1000 | Loss: 0.00018370
Iteration 105/1000 | Loss: 0.00006891
Iteration 106/1000 | Loss: 0.00004762
Iteration 107/1000 | Loss: 0.00004292
Iteration 108/1000 | Loss: 0.00003994
Iteration 109/1000 | Loss: 0.00003793
Iteration 110/1000 | Loss: 0.00003663
Iteration 111/1000 | Loss: 0.00004879
Iteration 112/1000 | Loss: 0.00003576
Iteration 113/1000 | Loss: 0.00003425
Iteration 114/1000 | Loss: 0.00003348
Iteration 115/1000 | Loss: 0.00003300
Iteration 116/1000 | Loss: 0.00003274
Iteration 117/1000 | Loss: 0.00003222
Iteration 118/1000 | Loss: 0.00003171
Iteration 119/1000 | Loss: 0.00003141
Iteration 120/1000 | Loss: 0.00003119
Iteration 121/1000 | Loss: 0.00003114
Iteration 122/1000 | Loss: 0.00003114
Iteration 123/1000 | Loss: 0.00003113
Iteration 124/1000 | Loss: 0.00003113
Iteration 125/1000 | Loss: 0.00003112
Iteration 126/1000 | Loss: 0.00003101
Iteration 127/1000 | Loss: 0.00037308
Iteration 128/1000 | Loss: 0.00054299
Iteration 129/1000 | Loss: 0.00004816
Iteration 130/1000 | Loss: 0.00003634
Iteration 131/1000 | Loss: 0.00003235
Iteration 132/1000 | Loss: 0.00003068
Iteration 133/1000 | Loss: 0.00003001
Iteration 134/1000 | Loss: 0.00002948
Iteration 135/1000 | Loss: 0.00002902
Iteration 136/1000 | Loss: 0.00002880
Iteration 137/1000 | Loss: 0.00002872
Iteration 138/1000 | Loss: 0.00002871
Iteration 139/1000 | Loss: 0.00002867
Iteration 140/1000 | Loss: 0.00002864
Iteration 141/1000 | Loss: 0.00002864
Iteration 142/1000 | Loss: 0.00002864
Iteration 143/1000 | Loss: 0.00002863
Iteration 144/1000 | Loss: 0.00002863
Iteration 145/1000 | Loss: 0.00002863
Iteration 146/1000 | Loss: 0.00002863
Iteration 147/1000 | Loss: 0.00002863
Iteration 148/1000 | Loss: 0.00002862
Iteration 149/1000 | Loss: 0.00002862
Iteration 150/1000 | Loss: 0.00002861
Iteration 151/1000 | Loss: 0.00002860
Iteration 152/1000 | Loss: 0.00002860
Iteration 153/1000 | Loss: 0.00002860
Iteration 154/1000 | Loss: 0.00002860
Iteration 155/1000 | Loss: 0.00002860
Iteration 156/1000 | Loss: 0.00002859
Iteration 157/1000 | Loss: 0.00002859
Iteration 158/1000 | Loss: 0.00002859
Iteration 159/1000 | Loss: 0.00002859
Iteration 160/1000 | Loss: 0.00002858
Iteration 161/1000 | Loss: 0.00002858
Iteration 162/1000 | Loss: 0.00002857
Iteration 163/1000 | Loss: 0.00002857
Iteration 164/1000 | Loss: 0.00002857
Iteration 165/1000 | Loss: 0.00002857
Iteration 166/1000 | Loss: 0.00002857
Iteration 167/1000 | Loss: 0.00002857
Iteration 168/1000 | Loss: 0.00002857
Iteration 169/1000 | Loss: 0.00002856
Iteration 170/1000 | Loss: 0.00002856
Iteration 171/1000 | Loss: 0.00002856
Iteration 172/1000 | Loss: 0.00002855
Iteration 173/1000 | Loss: 0.00002855
Iteration 174/1000 | Loss: 0.00002854
Iteration 175/1000 | Loss: 0.00002854
Iteration 176/1000 | Loss: 0.00002854
Iteration 177/1000 | Loss: 0.00002854
Iteration 178/1000 | Loss: 0.00002853
Iteration 179/1000 | Loss: 0.00002853
Iteration 180/1000 | Loss: 0.00002853
Iteration 181/1000 | Loss: 0.00002852
Iteration 182/1000 | Loss: 0.00002852
Iteration 183/1000 | Loss: 0.00002852
Iteration 184/1000 | Loss: 0.00002852
Iteration 185/1000 | Loss: 0.00002852
Iteration 186/1000 | Loss: 0.00002852
Iteration 187/1000 | Loss: 0.00002851
Iteration 188/1000 | Loss: 0.00002851
Iteration 189/1000 | Loss: 0.00002850
Iteration 190/1000 | Loss: 0.00002850
Iteration 191/1000 | Loss: 0.00002850
Iteration 192/1000 | Loss: 0.00002850
Iteration 193/1000 | Loss: 0.00002850
Iteration 194/1000 | Loss: 0.00002850
Iteration 195/1000 | Loss: 0.00002849
Iteration 196/1000 | Loss: 0.00002849
Iteration 197/1000 | Loss: 0.00002849
Iteration 198/1000 | Loss: 0.00002849
Iteration 199/1000 | Loss: 0.00002849
Iteration 200/1000 | Loss: 0.00002849
Iteration 201/1000 | Loss: 0.00002849
Iteration 202/1000 | Loss: 0.00002849
Iteration 203/1000 | Loss: 0.00002849
Iteration 204/1000 | Loss: 0.00002849
Iteration 205/1000 | Loss: 0.00002848
Iteration 206/1000 | Loss: 0.00002848
Iteration 207/1000 | Loss: 0.00002848
Iteration 208/1000 | Loss: 0.00002847
Iteration 209/1000 | Loss: 0.00002847
Iteration 210/1000 | Loss: 0.00002847
Iteration 211/1000 | Loss: 0.00002847
Iteration 212/1000 | Loss: 0.00002847
Iteration 213/1000 | Loss: 0.00002847
Iteration 214/1000 | Loss: 0.00002847
Iteration 215/1000 | Loss: 0.00002847
Iteration 216/1000 | Loss: 0.00002846
Iteration 217/1000 | Loss: 0.00002846
Iteration 218/1000 | Loss: 0.00002846
Iteration 219/1000 | Loss: 0.00002846
Iteration 220/1000 | Loss: 0.00002846
Iteration 221/1000 | Loss: 0.00002846
Iteration 222/1000 | Loss: 0.00002846
Iteration 223/1000 | Loss: 0.00002846
Iteration 224/1000 | Loss: 0.00002846
Iteration 225/1000 | Loss: 0.00002846
Iteration 226/1000 | Loss: 0.00002845
Iteration 227/1000 | Loss: 0.00002845
Iteration 228/1000 | Loss: 0.00002845
Iteration 229/1000 | Loss: 0.00002845
Iteration 230/1000 | Loss: 0.00002845
Iteration 231/1000 | Loss: 0.00002845
Iteration 232/1000 | Loss: 0.00002845
Iteration 233/1000 | Loss: 0.00002845
Iteration 234/1000 | Loss: 0.00002845
Iteration 235/1000 | Loss: 0.00002845
Iteration 236/1000 | Loss: 0.00002845
Iteration 237/1000 | Loss: 0.00002845
Iteration 238/1000 | Loss: 0.00002845
Iteration 239/1000 | Loss: 0.00002845
Iteration 240/1000 | Loss: 0.00002845
Iteration 241/1000 | Loss: 0.00002845
Iteration 242/1000 | Loss: 0.00002845
Iteration 243/1000 | Loss: 0.00002845
Iteration 244/1000 | Loss: 0.00002845
Iteration 245/1000 | Loss: 0.00002845
Iteration 246/1000 | Loss: 0.00002845
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 246. Stopping optimization.
Last 5 losses: [2.8447864679037593e-05, 2.8447864679037593e-05, 2.8447864679037593e-05, 2.8447864679037593e-05, 2.8447864679037593e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8447864679037593e-05

Optimization complete. Final v2v error: 4.423607349395752 mm

Highest mean error: 9.797196388244629 mm for frame 12

Lowest mean error: 3.8290553092956543 mm for frame 0

Saving results

Total time: 236.23434567451477
