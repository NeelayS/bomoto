Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=45, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 2520-2575
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00961583
Iteration 2/25 | Loss: 0.00961583
Iteration 3/25 | Loss: 0.00961583
Iteration 4/25 | Loss: 0.00961582
Iteration 5/25 | Loss: 0.00961582
Iteration 6/25 | Loss: 0.00269398
Iteration 7/25 | Loss: 0.00211324
Iteration 8/25 | Loss: 0.00191964
Iteration 9/25 | Loss: 0.00197015
Iteration 10/25 | Loss: 0.00176129
Iteration 11/25 | Loss: 0.00169458
Iteration 12/25 | Loss: 0.00163863
Iteration 13/25 | Loss: 0.00158631
Iteration 14/25 | Loss: 0.00154494
Iteration 15/25 | Loss: 0.00150522
Iteration 16/25 | Loss: 0.00143878
Iteration 17/25 | Loss: 0.00139564
Iteration 18/25 | Loss: 0.00138001
Iteration 19/25 | Loss: 0.00137610
Iteration 20/25 | Loss: 0.00136603
Iteration 21/25 | Loss: 0.00135570
Iteration 22/25 | Loss: 0.00136024
Iteration 23/25 | Loss: 0.00135095
Iteration 24/25 | Loss: 0.00135023
Iteration 25/25 | Loss: 0.00134984

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21129847
Iteration 2/25 | Loss: 0.00245331
Iteration 3/25 | Loss: 0.00245106
Iteration 4/25 | Loss: 0.00245105
Iteration 5/25 | Loss: 0.00245066
Iteration 6/25 | Loss: 0.00245066
Iteration 7/25 | Loss: 0.00245066
Iteration 8/25 | Loss: 0.00245066
Iteration 9/25 | Loss: 0.00245066
Iteration 10/25 | Loss: 0.00245066
Iteration 11/25 | Loss: 0.00245066
Iteration 12/25 | Loss: 0.00245066
Iteration 13/25 | Loss: 0.00245066
Iteration 14/25 | Loss: 0.00245066
Iteration 15/25 | Loss: 0.00245066
Iteration 16/25 | Loss: 0.00245066
Iteration 17/25 | Loss: 0.00245066
Iteration 18/25 | Loss: 0.00245066
Iteration 19/25 | Loss: 0.00245066
Iteration 20/25 | Loss: 0.00245066
Iteration 21/25 | Loss: 0.00245066
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0024506591726094484, 0.0024506591726094484, 0.0024506591726094484, 0.0024506591726094484, 0.0024506591726094484]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024506591726094484

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00245066
Iteration 2/1000 | Loss: 0.00020752
Iteration 3/1000 | Loss: 0.00014770
Iteration 4/1000 | Loss: 0.00012444
Iteration 5/1000 | Loss: 0.00011059
Iteration 6/1000 | Loss: 0.00011101
Iteration 7/1000 | Loss: 0.00010115
Iteration 8/1000 | Loss: 0.00019700
Iteration 9/1000 | Loss: 0.00045794
Iteration 10/1000 | Loss: 0.00104695
Iteration 11/1000 | Loss: 0.00056231
Iteration 12/1000 | Loss: 0.00025119
Iteration 13/1000 | Loss: 0.00017420
Iteration 14/1000 | Loss: 0.00016158
Iteration 15/1000 | Loss: 0.00009616
Iteration 16/1000 | Loss: 0.00007379
Iteration 17/1000 | Loss: 0.00003270
Iteration 18/1000 | Loss: 0.00004590
Iteration 19/1000 | Loss: 0.00006916
Iteration 20/1000 | Loss: 0.00003644
Iteration 21/1000 | Loss: 0.00007032
Iteration 22/1000 | Loss: 0.00008854
Iteration 23/1000 | Loss: 0.00004407
Iteration 24/1000 | Loss: 0.00002961
Iteration 25/1000 | Loss: 0.00001845
Iteration 26/1000 | Loss: 0.00001884
Iteration 27/1000 | Loss: 0.00004193
Iteration 28/1000 | Loss: 0.00001614
Iteration 29/1000 | Loss: 0.00001581
Iteration 30/1000 | Loss: 0.00001552
Iteration 31/1000 | Loss: 0.00001550
Iteration 32/1000 | Loss: 0.00001546
Iteration 33/1000 | Loss: 0.00001532
Iteration 34/1000 | Loss: 0.00001524
Iteration 35/1000 | Loss: 0.00001512
Iteration 36/1000 | Loss: 0.00001511
Iteration 37/1000 | Loss: 0.00001508
Iteration 38/1000 | Loss: 0.00001507
Iteration 39/1000 | Loss: 0.00001506
Iteration 40/1000 | Loss: 0.00001505
Iteration 41/1000 | Loss: 0.00001505
Iteration 42/1000 | Loss: 0.00001504
Iteration 43/1000 | Loss: 0.00001504
Iteration 44/1000 | Loss: 0.00001503
Iteration 45/1000 | Loss: 0.00001503
Iteration 46/1000 | Loss: 0.00001499
Iteration 47/1000 | Loss: 0.00001498
Iteration 48/1000 | Loss: 0.00001497
Iteration 49/1000 | Loss: 0.00001497
Iteration 50/1000 | Loss: 0.00001495
Iteration 51/1000 | Loss: 0.00001495
Iteration 52/1000 | Loss: 0.00001495
Iteration 53/1000 | Loss: 0.00001495
Iteration 54/1000 | Loss: 0.00001495
Iteration 55/1000 | Loss: 0.00001495
Iteration 56/1000 | Loss: 0.00001494
Iteration 57/1000 | Loss: 0.00001494
Iteration 58/1000 | Loss: 0.00001494
Iteration 59/1000 | Loss: 0.00001494
Iteration 60/1000 | Loss: 0.00001493
Iteration 61/1000 | Loss: 0.00001493
Iteration 62/1000 | Loss: 0.00001493
Iteration 63/1000 | Loss: 0.00001493
Iteration 64/1000 | Loss: 0.00001493
Iteration 65/1000 | Loss: 0.00001493
Iteration 66/1000 | Loss: 0.00001493
Iteration 67/1000 | Loss: 0.00001493
Iteration 68/1000 | Loss: 0.00001493
Iteration 69/1000 | Loss: 0.00001492
Iteration 70/1000 | Loss: 0.00001492
Iteration 71/1000 | Loss: 0.00001492
Iteration 72/1000 | Loss: 0.00001492
Iteration 73/1000 | Loss: 0.00001491
Iteration 74/1000 | Loss: 0.00001491
Iteration 75/1000 | Loss: 0.00001491
Iteration 76/1000 | Loss: 0.00001491
Iteration 77/1000 | Loss: 0.00001490
Iteration 78/1000 | Loss: 0.00001490
Iteration 79/1000 | Loss: 0.00001490
Iteration 80/1000 | Loss: 0.00001490
Iteration 81/1000 | Loss: 0.00001490
Iteration 82/1000 | Loss: 0.00001490
Iteration 83/1000 | Loss: 0.00001490
Iteration 84/1000 | Loss: 0.00001490
Iteration 85/1000 | Loss: 0.00001489
Iteration 86/1000 | Loss: 0.00001489
Iteration 87/1000 | Loss: 0.00001489
Iteration 88/1000 | Loss: 0.00001489
Iteration 89/1000 | Loss: 0.00001489
Iteration 90/1000 | Loss: 0.00001489
Iteration 91/1000 | Loss: 0.00001488
Iteration 92/1000 | Loss: 0.00001488
Iteration 93/1000 | Loss: 0.00001487
Iteration 94/1000 | Loss: 0.00001487
Iteration 95/1000 | Loss: 0.00001487
Iteration 96/1000 | Loss: 0.00001487
Iteration 97/1000 | Loss: 0.00001487
Iteration 98/1000 | Loss: 0.00001487
Iteration 99/1000 | Loss: 0.00001487
Iteration 100/1000 | Loss: 0.00001487
Iteration 101/1000 | Loss: 0.00001487
Iteration 102/1000 | Loss: 0.00001487
Iteration 103/1000 | Loss: 0.00001487
Iteration 104/1000 | Loss: 0.00001487
Iteration 105/1000 | Loss: 0.00001487
Iteration 106/1000 | Loss: 0.00001486
Iteration 107/1000 | Loss: 0.00001486
Iteration 108/1000 | Loss: 0.00001486
Iteration 109/1000 | Loss: 0.00001486
Iteration 110/1000 | Loss: 0.00001486
Iteration 111/1000 | Loss: 0.00001486
Iteration 112/1000 | Loss: 0.00001486
Iteration 113/1000 | Loss: 0.00001486
Iteration 114/1000 | Loss: 0.00001486
Iteration 115/1000 | Loss: 0.00001486
Iteration 116/1000 | Loss: 0.00001486
Iteration 117/1000 | Loss: 0.00001486
Iteration 118/1000 | Loss: 0.00001486
Iteration 119/1000 | Loss: 0.00001485
Iteration 120/1000 | Loss: 0.00001485
Iteration 121/1000 | Loss: 0.00001485
Iteration 122/1000 | Loss: 0.00001485
Iteration 123/1000 | Loss: 0.00001485
Iteration 124/1000 | Loss: 0.00001485
Iteration 125/1000 | Loss: 0.00001485
Iteration 126/1000 | Loss: 0.00001485
Iteration 127/1000 | Loss: 0.00001485
Iteration 128/1000 | Loss: 0.00001485
Iteration 129/1000 | Loss: 0.00001485
Iteration 130/1000 | Loss: 0.00001485
Iteration 131/1000 | Loss: 0.00001485
Iteration 132/1000 | Loss: 0.00001485
Iteration 133/1000 | Loss: 0.00001485
Iteration 134/1000 | Loss: 0.00001485
Iteration 135/1000 | Loss: 0.00001485
Iteration 136/1000 | Loss: 0.00001485
Iteration 137/1000 | Loss: 0.00001485
Iteration 138/1000 | Loss: 0.00001485
Iteration 139/1000 | Loss: 0.00001485
Iteration 140/1000 | Loss: 0.00001485
Iteration 141/1000 | Loss: 0.00001485
Iteration 142/1000 | Loss: 0.00001485
Iteration 143/1000 | Loss: 0.00001485
Iteration 144/1000 | Loss: 0.00001485
Iteration 145/1000 | Loss: 0.00001485
Iteration 146/1000 | Loss: 0.00001485
Iteration 147/1000 | Loss: 0.00001485
Iteration 148/1000 | Loss: 0.00001485
Iteration 149/1000 | Loss: 0.00001485
Iteration 150/1000 | Loss: 0.00001485
Iteration 151/1000 | Loss: 0.00001485
Iteration 152/1000 | Loss: 0.00001485
Iteration 153/1000 | Loss: 0.00001485
Iteration 154/1000 | Loss: 0.00001485
Iteration 155/1000 | Loss: 0.00001485
Iteration 156/1000 | Loss: 0.00001485
Iteration 157/1000 | Loss: 0.00001485
Iteration 158/1000 | Loss: 0.00001485
Iteration 159/1000 | Loss: 0.00001485
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [1.4854895198368467e-05, 1.4854895198368467e-05, 1.4854895198368467e-05, 1.4854895198368467e-05, 1.4854895198368467e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4854895198368467e-05

Optimization complete. Final v2v error: 3.3231613636016846 mm

Highest mean error: 3.639970064163208 mm for frame 96

Lowest mean error: 2.954554319381714 mm for frame 166

Saving results

Total time: 108.50749206542969
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01068571
Iteration 2/25 | Loss: 0.01068571
Iteration 3/25 | Loss: 0.01068571
Iteration 4/25 | Loss: 0.01068571
Iteration 5/25 | Loss: 0.01068570
Iteration 6/25 | Loss: 0.01068570
Iteration 7/25 | Loss: 0.01068570
Iteration 8/25 | Loss: 0.01068570
Iteration 9/25 | Loss: 0.01068570
Iteration 10/25 | Loss: 0.01068570
Iteration 11/25 | Loss: 0.01068569
Iteration 12/25 | Loss: 0.01068569
Iteration 13/25 | Loss: 0.01068569
Iteration 14/25 | Loss: 0.01068568
Iteration 15/25 | Loss: 0.01068568
Iteration 16/25 | Loss: 0.01068568
Iteration 17/25 | Loss: 0.01068567
Iteration 18/25 | Loss: 0.01068567
Iteration 19/25 | Loss: 0.01068567
Iteration 20/25 | Loss: 0.01068567
Iteration 21/25 | Loss: 0.01068567
Iteration 22/25 | Loss: 0.01068567
Iteration 23/25 | Loss: 0.01068566
Iteration 24/25 | Loss: 0.01068566
Iteration 25/25 | Loss: 0.01068566

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.00308228
Iteration 2/25 | Loss: 0.14536172
Iteration 3/25 | Loss: 0.14535424
Iteration 4/25 | Loss: 0.14535421
Iteration 5/25 | Loss: 0.14535420
Iteration 6/25 | Loss: 0.14535420
Iteration 7/25 | Loss: 0.14535420
Iteration 8/25 | Loss: 0.14535420
Iteration 9/25 | Loss: 0.14535420
Iteration 10/25 | Loss: 0.14535420
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.14535419642925262, 0.14535419642925262, 0.14535419642925262, 0.14535419642925262, 0.14535419642925262]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.14535419642925262

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.14535420
Iteration 2/1000 | Loss: 0.00440157
Iteration 3/1000 | Loss: 0.00067349
Iteration 4/1000 | Loss: 0.00227393
Iteration 5/1000 | Loss: 0.00014884
Iteration 6/1000 | Loss: 0.00022587
Iteration 7/1000 | Loss: 0.00038731
Iteration 8/1000 | Loss: 0.00003248
Iteration 9/1000 | Loss: 0.00004286
Iteration 10/1000 | Loss: 0.00007732
Iteration 11/1000 | Loss: 0.00018100
Iteration 12/1000 | Loss: 0.00003404
Iteration 13/1000 | Loss: 0.00004460
Iteration 14/1000 | Loss: 0.00002388
Iteration 15/1000 | Loss: 0.00008814
Iteration 16/1000 | Loss: 0.00003360
Iteration 17/1000 | Loss: 0.00001688
Iteration 18/1000 | Loss: 0.00004404
Iteration 19/1000 | Loss: 0.00001665
Iteration 20/1000 | Loss: 0.00001533
Iteration 21/1000 | Loss: 0.00004290
Iteration 22/1000 | Loss: 0.00001462
Iteration 23/1000 | Loss: 0.00010701
Iteration 24/1000 | Loss: 0.00010586
Iteration 25/1000 | Loss: 0.00005392
Iteration 26/1000 | Loss: 0.00004050
Iteration 27/1000 | Loss: 0.00001384
Iteration 28/1000 | Loss: 0.00001354
Iteration 29/1000 | Loss: 0.00005082
Iteration 30/1000 | Loss: 0.00009186
Iteration 31/1000 | Loss: 0.00002033
Iteration 32/1000 | Loss: 0.00004099
Iteration 33/1000 | Loss: 0.00001308
Iteration 34/1000 | Loss: 0.00006200
Iteration 35/1000 | Loss: 0.00004222
Iteration 36/1000 | Loss: 0.00003203
Iteration 37/1000 | Loss: 0.00001262
Iteration 38/1000 | Loss: 0.00001353
Iteration 39/1000 | Loss: 0.00001238
Iteration 40/1000 | Loss: 0.00001235
Iteration 41/1000 | Loss: 0.00001235
Iteration 42/1000 | Loss: 0.00001235
Iteration 43/1000 | Loss: 0.00001234
Iteration 44/1000 | Loss: 0.00001234
Iteration 45/1000 | Loss: 0.00001234
Iteration 46/1000 | Loss: 0.00001233
Iteration 47/1000 | Loss: 0.00001233
Iteration 48/1000 | Loss: 0.00001233
Iteration 49/1000 | Loss: 0.00001232
Iteration 50/1000 | Loss: 0.00001232
Iteration 51/1000 | Loss: 0.00003577
Iteration 52/1000 | Loss: 0.00003636
Iteration 53/1000 | Loss: 0.00001224
Iteration 54/1000 | Loss: 0.00001949
Iteration 55/1000 | Loss: 0.00001210
Iteration 56/1000 | Loss: 0.00001208
Iteration 57/1000 | Loss: 0.00001208
Iteration 58/1000 | Loss: 0.00001208
Iteration 59/1000 | Loss: 0.00001208
Iteration 60/1000 | Loss: 0.00001208
Iteration 61/1000 | Loss: 0.00001207
Iteration 62/1000 | Loss: 0.00001207
Iteration 63/1000 | Loss: 0.00001207
Iteration 64/1000 | Loss: 0.00001206
Iteration 65/1000 | Loss: 0.00001206
Iteration 66/1000 | Loss: 0.00001205
Iteration 67/1000 | Loss: 0.00001205
Iteration 68/1000 | Loss: 0.00001205
Iteration 69/1000 | Loss: 0.00001205
Iteration 70/1000 | Loss: 0.00001205
Iteration 71/1000 | Loss: 0.00001204
Iteration 72/1000 | Loss: 0.00001204
Iteration 73/1000 | Loss: 0.00001204
Iteration 74/1000 | Loss: 0.00001204
Iteration 75/1000 | Loss: 0.00001203
Iteration 76/1000 | Loss: 0.00001202
Iteration 77/1000 | Loss: 0.00001202
Iteration 78/1000 | Loss: 0.00001202
Iteration 79/1000 | Loss: 0.00001202
Iteration 80/1000 | Loss: 0.00001202
Iteration 81/1000 | Loss: 0.00001202
Iteration 82/1000 | Loss: 0.00001202
Iteration 83/1000 | Loss: 0.00001202
Iteration 84/1000 | Loss: 0.00001202
Iteration 85/1000 | Loss: 0.00001201
Iteration 86/1000 | Loss: 0.00001201
Iteration 87/1000 | Loss: 0.00003817
Iteration 88/1000 | Loss: 0.00001985
Iteration 89/1000 | Loss: 0.00001201
Iteration 90/1000 | Loss: 0.00002548
Iteration 91/1000 | Loss: 0.00001200
Iteration 92/1000 | Loss: 0.00001200
Iteration 93/1000 | Loss: 0.00001199
Iteration 94/1000 | Loss: 0.00001199
Iteration 95/1000 | Loss: 0.00001199
Iteration 96/1000 | Loss: 0.00001199
Iteration 97/1000 | Loss: 0.00001199
Iteration 98/1000 | Loss: 0.00001199
Iteration 99/1000 | Loss: 0.00001199
Iteration 100/1000 | Loss: 0.00001199
Iteration 101/1000 | Loss: 0.00001199
Iteration 102/1000 | Loss: 0.00001199
Iteration 103/1000 | Loss: 0.00001198
Iteration 104/1000 | Loss: 0.00001198
Iteration 105/1000 | Loss: 0.00001198
Iteration 106/1000 | Loss: 0.00001197
Iteration 107/1000 | Loss: 0.00001197
Iteration 108/1000 | Loss: 0.00001196
Iteration 109/1000 | Loss: 0.00001196
Iteration 110/1000 | Loss: 0.00001196
Iteration 111/1000 | Loss: 0.00001196
Iteration 112/1000 | Loss: 0.00001196
Iteration 113/1000 | Loss: 0.00001196
Iteration 114/1000 | Loss: 0.00001196
Iteration 115/1000 | Loss: 0.00001196
Iteration 116/1000 | Loss: 0.00001195
Iteration 117/1000 | Loss: 0.00001195
Iteration 118/1000 | Loss: 0.00001195
Iteration 119/1000 | Loss: 0.00001195
Iteration 120/1000 | Loss: 0.00001195
Iteration 121/1000 | Loss: 0.00001195
Iteration 122/1000 | Loss: 0.00001195
Iteration 123/1000 | Loss: 0.00001194
Iteration 124/1000 | Loss: 0.00001194
Iteration 125/1000 | Loss: 0.00001194
Iteration 126/1000 | Loss: 0.00001193
Iteration 127/1000 | Loss: 0.00001193
Iteration 128/1000 | Loss: 0.00001193
Iteration 129/1000 | Loss: 0.00001193
Iteration 130/1000 | Loss: 0.00001193
Iteration 131/1000 | Loss: 0.00001193
Iteration 132/1000 | Loss: 0.00001192
Iteration 133/1000 | Loss: 0.00001192
Iteration 134/1000 | Loss: 0.00001191
Iteration 135/1000 | Loss: 0.00001191
Iteration 136/1000 | Loss: 0.00001191
Iteration 137/1000 | Loss: 0.00001191
Iteration 138/1000 | Loss: 0.00001191
Iteration 139/1000 | Loss: 0.00001191
Iteration 140/1000 | Loss: 0.00001190
Iteration 141/1000 | Loss: 0.00001189
Iteration 142/1000 | Loss: 0.00001188
Iteration 143/1000 | Loss: 0.00001188
Iteration 144/1000 | Loss: 0.00001187
Iteration 145/1000 | Loss: 0.00001187
Iteration 146/1000 | Loss: 0.00001187
Iteration 147/1000 | Loss: 0.00001187
Iteration 148/1000 | Loss: 0.00001187
Iteration 149/1000 | Loss: 0.00001187
Iteration 150/1000 | Loss: 0.00001187
Iteration 151/1000 | Loss: 0.00001187
Iteration 152/1000 | Loss: 0.00001187
Iteration 153/1000 | Loss: 0.00001187
Iteration 154/1000 | Loss: 0.00001187
Iteration 155/1000 | Loss: 0.00001187
Iteration 156/1000 | Loss: 0.00001187
Iteration 157/1000 | Loss: 0.00001187
Iteration 158/1000 | Loss: 0.00001187
Iteration 159/1000 | Loss: 0.00001187
Iteration 160/1000 | Loss: 0.00001187
Iteration 161/1000 | Loss: 0.00001187
Iteration 162/1000 | Loss: 0.00001187
Iteration 163/1000 | Loss: 0.00001187
Iteration 164/1000 | Loss: 0.00001187
Iteration 165/1000 | Loss: 0.00001187
Iteration 166/1000 | Loss: 0.00001187
Iteration 167/1000 | Loss: 0.00001187
Iteration 168/1000 | Loss: 0.00001187
Iteration 169/1000 | Loss: 0.00001187
Iteration 170/1000 | Loss: 0.00001187
Iteration 171/1000 | Loss: 0.00001187
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.1870377420564182e-05, 1.1870377420564182e-05, 1.1870377420564182e-05, 1.1870377420564182e-05, 1.1870377420564182e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1870377420564182e-05

Optimization complete. Final v2v error: 2.9684433937072754 mm

Highest mean error: 3.669837713241577 mm for frame 228

Lowest mean error: 2.515406847000122 mm for frame 8

Saving results

Total time: 93.4937469959259
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00867578
Iteration 2/25 | Loss: 0.00154123
Iteration 3/25 | Loss: 0.00126059
Iteration 4/25 | Loss: 0.00122816
Iteration 5/25 | Loss: 0.00122387
Iteration 6/25 | Loss: 0.00122309
Iteration 7/25 | Loss: 0.00121861
Iteration 8/25 | Loss: 0.00121703
Iteration 9/25 | Loss: 0.00121467
Iteration 10/25 | Loss: 0.00121473
Iteration 11/25 | Loss: 0.00120677
Iteration 12/25 | Loss: 0.00120013
Iteration 13/25 | Loss: 0.00119865
Iteration 14/25 | Loss: 0.00119829
Iteration 15/25 | Loss: 0.00119818
Iteration 16/25 | Loss: 0.00119818
Iteration 17/25 | Loss: 0.00119817
Iteration 18/25 | Loss: 0.00119815
Iteration 19/25 | Loss: 0.00119814
Iteration 20/25 | Loss: 0.00119814
Iteration 21/25 | Loss: 0.00119814
Iteration 22/25 | Loss: 0.00119814
Iteration 23/25 | Loss: 0.00119814
Iteration 24/25 | Loss: 0.00119814
Iteration 25/25 | Loss: 0.00119814

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.83251894
Iteration 2/25 | Loss: 0.00097317
Iteration 3/25 | Loss: 0.00097317
Iteration 4/25 | Loss: 0.00097317
Iteration 5/25 | Loss: 0.00097317
Iteration 6/25 | Loss: 0.00097317
Iteration 7/25 | Loss: 0.00097317
Iteration 8/25 | Loss: 0.00097317
Iteration 9/25 | Loss: 0.00097317
Iteration 10/25 | Loss: 0.00097317
Iteration 11/25 | Loss: 0.00097317
Iteration 12/25 | Loss: 0.00097317
Iteration 13/25 | Loss: 0.00097317
Iteration 14/25 | Loss: 0.00097317
Iteration 15/25 | Loss: 0.00097317
Iteration 16/25 | Loss: 0.00097317
Iteration 17/25 | Loss: 0.00097317
Iteration 18/25 | Loss: 0.00097317
Iteration 19/25 | Loss: 0.00097317
Iteration 20/25 | Loss: 0.00097317
Iteration 21/25 | Loss: 0.00097317
Iteration 22/25 | Loss: 0.00097317
Iteration 23/25 | Loss: 0.00097317
Iteration 24/25 | Loss: 0.00097317
Iteration 25/25 | Loss: 0.00097317

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097317
Iteration 2/1000 | Loss: 0.00003568
Iteration 3/1000 | Loss: 0.00002993
Iteration 4/1000 | Loss: 0.00002772
Iteration 5/1000 | Loss: 0.00002658
Iteration 6/1000 | Loss: 0.00002587
Iteration 7/1000 | Loss: 0.00002519
Iteration 8/1000 | Loss: 0.00002482
Iteration 9/1000 | Loss: 0.00002451
Iteration 10/1000 | Loss: 0.00002442
Iteration 11/1000 | Loss: 0.00002438
Iteration 12/1000 | Loss: 0.00002426
Iteration 13/1000 | Loss: 0.00002419
Iteration 14/1000 | Loss: 0.00002406
Iteration 15/1000 | Loss: 0.00002404
Iteration 16/1000 | Loss: 0.00002403
Iteration 17/1000 | Loss: 0.00002403
Iteration 18/1000 | Loss: 0.00002403
Iteration 19/1000 | Loss: 0.00002403
Iteration 20/1000 | Loss: 0.00002403
Iteration 21/1000 | Loss: 0.00002402
Iteration 22/1000 | Loss: 0.00002402
Iteration 23/1000 | Loss: 0.00002402
Iteration 24/1000 | Loss: 0.00002402
Iteration 25/1000 | Loss: 0.00002402
Iteration 26/1000 | Loss: 0.00002402
Iteration 27/1000 | Loss: 0.00002402
Iteration 28/1000 | Loss: 0.00002400
Iteration 29/1000 | Loss: 0.00002400
Iteration 30/1000 | Loss: 0.00002399
Iteration 31/1000 | Loss: 0.00002397
Iteration 32/1000 | Loss: 0.00002395
Iteration 33/1000 | Loss: 0.00002394
Iteration 34/1000 | Loss: 0.00002391
Iteration 35/1000 | Loss: 0.00002391
Iteration 36/1000 | Loss: 0.00002391
Iteration 37/1000 | Loss: 0.00002391
Iteration 38/1000 | Loss: 0.00002390
Iteration 39/1000 | Loss: 0.00002390
Iteration 40/1000 | Loss: 0.00002390
Iteration 41/1000 | Loss: 0.00002390
Iteration 42/1000 | Loss: 0.00002390
Iteration 43/1000 | Loss: 0.00002389
Iteration 44/1000 | Loss: 0.00002389
Iteration 45/1000 | Loss: 0.00002389
Iteration 46/1000 | Loss: 0.00002388
Iteration 47/1000 | Loss: 0.00002388
Iteration 48/1000 | Loss: 0.00002388
Iteration 49/1000 | Loss: 0.00002388
Iteration 50/1000 | Loss: 0.00002388
Iteration 51/1000 | Loss: 0.00002388
Iteration 52/1000 | Loss: 0.00002388
Iteration 53/1000 | Loss: 0.00002388
Iteration 54/1000 | Loss: 0.00002388
Iteration 55/1000 | Loss: 0.00002388
Iteration 56/1000 | Loss: 0.00002388
Iteration 57/1000 | Loss: 0.00002388
Iteration 58/1000 | Loss: 0.00002388
Iteration 59/1000 | Loss: 0.00002388
Iteration 60/1000 | Loss: 0.00002387
Iteration 61/1000 | Loss: 0.00002387
Iteration 62/1000 | Loss: 0.00002387
Iteration 63/1000 | Loss: 0.00002386
Iteration 64/1000 | Loss: 0.00002386
Iteration 65/1000 | Loss: 0.00002386
Iteration 66/1000 | Loss: 0.00002386
Iteration 67/1000 | Loss: 0.00002386
Iteration 68/1000 | Loss: 0.00002386
Iteration 69/1000 | Loss: 0.00002386
Iteration 70/1000 | Loss: 0.00002386
Iteration 71/1000 | Loss: 0.00002386
Iteration 72/1000 | Loss: 0.00002386
Iteration 73/1000 | Loss: 0.00002386
Iteration 74/1000 | Loss: 0.00002386
Iteration 75/1000 | Loss: 0.00002386
Iteration 76/1000 | Loss: 0.00002386
Iteration 77/1000 | Loss: 0.00002386
Iteration 78/1000 | Loss: 0.00002386
Iteration 79/1000 | Loss: 0.00002386
Iteration 80/1000 | Loss: 0.00002386
Iteration 81/1000 | Loss: 0.00002386
Iteration 82/1000 | Loss: 0.00002386
Iteration 83/1000 | Loss: 0.00002386
Iteration 84/1000 | Loss: 0.00002386
Iteration 85/1000 | Loss: 0.00002386
Iteration 86/1000 | Loss: 0.00002386
Iteration 87/1000 | Loss: 0.00002386
Iteration 88/1000 | Loss: 0.00002386
Iteration 89/1000 | Loss: 0.00002386
Iteration 90/1000 | Loss: 0.00002386
Iteration 91/1000 | Loss: 0.00002386
Iteration 92/1000 | Loss: 0.00002386
Iteration 93/1000 | Loss: 0.00002386
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [2.3857277483330108e-05, 2.3857277483330108e-05, 2.3857277483330108e-05, 2.3857277483330108e-05, 2.3857277483330108e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3857277483330108e-05

Optimization complete. Final v2v error: 4.129696369171143 mm

Highest mean error: 4.257560729980469 mm for frame 135

Lowest mean error: 4.039785861968994 mm for frame 14

Saving results

Total time: 49.08270883560181
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00390177
Iteration 2/25 | Loss: 0.00121063
Iteration 3/25 | Loss: 0.00113303
Iteration 4/25 | Loss: 0.00112463
Iteration 5/25 | Loss: 0.00112252
Iteration 6/25 | Loss: 0.00112241
Iteration 7/25 | Loss: 0.00112241
Iteration 8/25 | Loss: 0.00112241
Iteration 9/25 | Loss: 0.00112241
Iteration 10/25 | Loss: 0.00112241
Iteration 11/25 | Loss: 0.00112241
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011224065674468875, 0.0011224065674468875, 0.0011224065674468875, 0.0011224065674468875, 0.0011224065674468875]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011224065674468875

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25575149
Iteration 2/25 | Loss: 0.00197475
Iteration 3/25 | Loss: 0.00197475
Iteration 4/25 | Loss: 0.00197475
Iteration 5/25 | Loss: 0.00197475
Iteration 6/25 | Loss: 0.00197475
Iteration 7/25 | Loss: 0.00197475
Iteration 8/25 | Loss: 0.00197475
Iteration 9/25 | Loss: 0.00197475
Iteration 10/25 | Loss: 0.00197475
Iteration 11/25 | Loss: 0.00197475
Iteration 12/25 | Loss: 0.00197475
Iteration 13/25 | Loss: 0.00197475
Iteration 14/25 | Loss: 0.00197475
Iteration 15/25 | Loss: 0.00197475
Iteration 16/25 | Loss: 0.00197475
Iteration 17/25 | Loss: 0.00197475
Iteration 18/25 | Loss: 0.00197475
Iteration 19/25 | Loss: 0.00197475
Iteration 20/25 | Loss: 0.00197475
Iteration 21/25 | Loss: 0.00197475
Iteration 22/25 | Loss: 0.00197475
Iteration 23/25 | Loss: 0.00197475
Iteration 24/25 | Loss: 0.00197475
Iteration 25/25 | Loss: 0.00197475

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00197475
Iteration 2/1000 | Loss: 0.00002227
Iteration 3/1000 | Loss: 0.00001460
Iteration 4/1000 | Loss: 0.00001214
Iteration 5/1000 | Loss: 0.00001076
Iteration 6/1000 | Loss: 0.00001000
Iteration 7/1000 | Loss: 0.00000949
Iteration 8/1000 | Loss: 0.00000909
Iteration 9/1000 | Loss: 0.00000892
Iteration 10/1000 | Loss: 0.00000874
Iteration 11/1000 | Loss: 0.00000854
Iteration 12/1000 | Loss: 0.00000844
Iteration 13/1000 | Loss: 0.00000840
Iteration 14/1000 | Loss: 0.00000838
Iteration 15/1000 | Loss: 0.00000837
Iteration 16/1000 | Loss: 0.00000836
Iteration 17/1000 | Loss: 0.00000835
Iteration 18/1000 | Loss: 0.00000834
Iteration 19/1000 | Loss: 0.00000830
Iteration 20/1000 | Loss: 0.00000829
Iteration 21/1000 | Loss: 0.00000828
Iteration 22/1000 | Loss: 0.00000827
Iteration 23/1000 | Loss: 0.00000826
Iteration 24/1000 | Loss: 0.00000822
Iteration 25/1000 | Loss: 0.00000819
Iteration 26/1000 | Loss: 0.00000818
Iteration 27/1000 | Loss: 0.00000817
Iteration 28/1000 | Loss: 0.00000817
Iteration 29/1000 | Loss: 0.00000816
Iteration 30/1000 | Loss: 0.00000815
Iteration 31/1000 | Loss: 0.00000815
Iteration 32/1000 | Loss: 0.00000815
Iteration 33/1000 | Loss: 0.00000814
Iteration 34/1000 | Loss: 0.00000814
Iteration 35/1000 | Loss: 0.00000812
Iteration 36/1000 | Loss: 0.00000812
Iteration 37/1000 | Loss: 0.00000811
Iteration 38/1000 | Loss: 0.00000811
Iteration 39/1000 | Loss: 0.00000810
Iteration 40/1000 | Loss: 0.00000810
Iteration 41/1000 | Loss: 0.00000809
Iteration 42/1000 | Loss: 0.00000808
Iteration 43/1000 | Loss: 0.00000807
Iteration 44/1000 | Loss: 0.00000807
Iteration 45/1000 | Loss: 0.00000807
Iteration 46/1000 | Loss: 0.00000806
Iteration 47/1000 | Loss: 0.00000805
Iteration 48/1000 | Loss: 0.00000805
Iteration 49/1000 | Loss: 0.00000804
Iteration 50/1000 | Loss: 0.00000804
Iteration 51/1000 | Loss: 0.00000804
Iteration 52/1000 | Loss: 0.00000803
Iteration 53/1000 | Loss: 0.00000803
Iteration 54/1000 | Loss: 0.00000802
Iteration 55/1000 | Loss: 0.00000801
Iteration 56/1000 | Loss: 0.00000800
Iteration 57/1000 | Loss: 0.00000800
Iteration 58/1000 | Loss: 0.00000798
Iteration 59/1000 | Loss: 0.00000797
Iteration 60/1000 | Loss: 0.00000794
Iteration 61/1000 | Loss: 0.00000794
Iteration 62/1000 | Loss: 0.00000794
Iteration 63/1000 | Loss: 0.00000794
Iteration 64/1000 | Loss: 0.00000794
Iteration 65/1000 | Loss: 0.00000794
Iteration 66/1000 | Loss: 0.00000794
Iteration 67/1000 | Loss: 0.00000794
Iteration 68/1000 | Loss: 0.00000793
Iteration 69/1000 | Loss: 0.00000793
Iteration 70/1000 | Loss: 0.00000793
Iteration 71/1000 | Loss: 0.00000792
Iteration 72/1000 | Loss: 0.00000792
Iteration 73/1000 | Loss: 0.00000792
Iteration 74/1000 | Loss: 0.00000791
Iteration 75/1000 | Loss: 0.00000791
Iteration 76/1000 | Loss: 0.00000791
Iteration 77/1000 | Loss: 0.00000790
Iteration 78/1000 | Loss: 0.00000790
Iteration 79/1000 | Loss: 0.00000790
Iteration 80/1000 | Loss: 0.00000789
Iteration 81/1000 | Loss: 0.00000789
Iteration 82/1000 | Loss: 0.00000789
Iteration 83/1000 | Loss: 0.00000789
Iteration 84/1000 | Loss: 0.00000789
Iteration 85/1000 | Loss: 0.00000789
Iteration 86/1000 | Loss: 0.00000789
Iteration 87/1000 | Loss: 0.00000788
Iteration 88/1000 | Loss: 0.00000788
Iteration 89/1000 | Loss: 0.00000788
Iteration 90/1000 | Loss: 0.00000788
Iteration 91/1000 | Loss: 0.00000788
Iteration 92/1000 | Loss: 0.00000788
Iteration 93/1000 | Loss: 0.00000787
Iteration 94/1000 | Loss: 0.00000786
Iteration 95/1000 | Loss: 0.00000786
Iteration 96/1000 | Loss: 0.00000786
Iteration 97/1000 | Loss: 0.00000786
Iteration 98/1000 | Loss: 0.00000786
Iteration 99/1000 | Loss: 0.00000786
Iteration 100/1000 | Loss: 0.00000785
Iteration 101/1000 | Loss: 0.00000785
Iteration 102/1000 | Loss: 0.00000785
Iteration 103/1000 | Loss: 0.00000785
Iteration 104/1000 | Loss: 0.00000785
Iteration 105/1000 | Loss: 0.00000785
Iteration 106/1000 | Loss: 0.00000784
Iteration 107/1000 | Loss: 0.00000784
Iteration 108/1000 | Loss: 0.00000783
Iteration 109/1000 | Loss: 0.00000783
Iteration 110/1000 | Loss: 0.00000783
Iteration 111/1000 | Loss: 0.00000783
Iteration 112/1000 | Loss: 0.00000783
Iteration 113/1000 | Loss: 0.00000782
Iteration 114/1000 | Loss: 0.00000782
Iteration 115/1000 | Loss: 0.00000782
Iteration 116/1000 | Loss: 0.00000782
Iteration 117/1000 | Loss: 0.00000782
Iteration 118/1000 | Loss: 0.00000782
Iteration 119/1000 | Loss: 0.00000782
Iteration 120/1000 | Loss: 0.00000782
Iteration 121/1000 | Loss: 0.00000782
Iteration 122/1000 | Loss: 0.00000781
Iteration 123/1000 | Loss: 0.00000781
Iteration 124/1000 | Loss: 0.00000781
Iteration 125/1000 | Loss: 0.00000781
Iteration 126/1000 | Loss: 0.00000781
Iteration 127/1000 | Loss: 0.00000781
Iteration 128/1000 | Loss: 0.00000781
Iteration 129/1000 | Loss: 0.00000780
Iteration 130/1000 | Loss: 0.00000780
Iteration 131/1000 | Loss: 0.00000780
Iteration 132/1000 | Loss: 0.00000780
Iteration 133/1000 | Loss: 0.00000780
Iteration 134/1000 | Loss: 0.00000780
Iteration 135/1000 | Loss: 0.00000780
Iteration 136/1000 | Loss: 0.00000780
Iteration 137/1000 | Loss: 0.00000780
Iteration 138/1000 | Loss: 0.00000780
Iteration 139/1000 | Loss: 0.00000779
Iteration 140/1000 | Loss: 0.00000779
Iteration 141/1000 | Loss: 0.00000779
Iteration 142/1000 | Loss: 0.00000779
Iteration 143/1000 | Loss: 0.00000779
Iteration 144/1000 | Loss: 0.00000779
Iteration 145/1000 | Loss: 0.00000779
Iteration 146/1000 | Loss: 0.00000779
Iteration 147/1000 | Loss: 0.00000779
Iteration 148/1000 | Loss: 0.00000779
Iteration 149/1000 | Loss: 0.00000778
Iteration 150/1000 | Loss: 0.00000778
Iteration 151/1000 | Loss: 0.00000778
Iteration 152/1000 | Loss: 0.00000778
Iteration 153/1000 | Loss: 0.00000778
Iteration 154/1000 | Loss: 0.00000778
Iteration 155/1000 | Loss: 0.00000778
Iteration 156/1000 | Loss: 0.00000778
Iteration 157/1000 | Loss: 0.00000778
Iteration 158/1000 | Loss: 0.00000778
Iteration 159/1000 | Loss: 0.00000777
Iteration 160/1000 | Loss: 0.00000777
Iteration 161/1000 | Loss: 0.00000777
Iteration 162/1000 | Loss: 0.00000777
Iteration 163/1000 | Loss: 0.00000777
Iteration 164/1000 | Loss: 0.00000777
Iteration 165/1000 | Loss: 0.00000777
Iteration 166/1000 | Loss: 0.00000777
Iteration 167/1000 | Loss: 0.00000777
Iteration 168/1000 | Loss: 0.00000777
Iteration 169/1000 | Loss: 0.00000777
Iteration 170/1000 | Loss: 0.00000777
Iteration 171/1000 | Loss: 0.00000777
Iteration 172/1000 | Loss: 0.00000777
Iteration 173/1000 | Loss: 0.00000777
Iteration 174/1000 | Loss: 0.00000777
Iteration 175/1000 | Loss: 0.00000777
Iteration 176/1000 | Loss: 0.00000777
Iteration 177/1000 | Loss: 0.00000777
Iteration 178/1000 | Loss: 0.00000777
Iteration 179/1000 | Loss: 0.00000777
Iteration 180/1000 | Loss: 0.00000777
Iteration 181/1000 | Loss: 0.00000777
Iteration 182/1000 | Loss: 0.00000777
Iteration 183/1000 | Loss: 0.00000777
Iteration 184/1000 | Loss: 0.00000777
Iteration 185/1000 | Loss: 0.00000777
Iteration 186/1000 | Loss: 0.00000777
Iteration 187/1000 | Loss: 0.00000777
Iteration 188/1000 | Loss: 0.00000777
Iteration 189/1000 | Loss: 0.00000777
Iteration 190/1000 | Loss: 0.00000777
Iteration 191/1000 | Loss: 0.00000777
Iteration 192/1000 | Loss: 0.00000777
Iteration 193/1000 | Loss: 0.00000777
Iteration 194/1000 | Loss: 0.00000777
Iteration 195/1000 | Loss: 0.00000777
Iteration 196/1000 | Loss: 0.00000777
Iteration 197/1000 | Loss: 0.00000777
Iteration 198/1000 | Loss: 0.00000777
Iteration 199/1000 | Loss: 0.00000777
Iteration 200/1000 | Loss: 0.00000777
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [7.766334420011844e-06, 7.766334420011844e-06, 7.766334420011844e-06, 7.766334420011844e-06, 7.766334420011844e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.766334420011844e-06

Optimization complete. Final v2v error: 2.3942642211914062 mm

Highest mean error: 3.393946886062622 mm for frame 71

Lowest mean error: 2.1863436698913574 mm for frame 111

Saving results

Total time: 40.95269536972046
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00813176
Iteration 2/25 | Loss: 0.00147496
Iteration 3/25 | Loss: 0.00119659
Iteration 4/25 | Loss: 0.00116408
Iteration 5/25 | Loss: 0.00115918
Iteration 6/25 | Loss: 0.00115791
Iteration 7/25 | Loss: 0.00115732
Iteration 8/25 | Loss: 0.00115698
Iteration 9/25 | Loss: 0.00115968
Iteration 10/25 | Loss: 0.00115852
Iteration 11/25 | Loss: 0.00115965
Iteration 12/25 | Loss: 0.00115893
Iteration 13/25 | Loss: 0.00115943
Iteration 14/25 | Loss: 0.00115833
Iteration 15/25 | Loss: 0.00115925
Iteration 16/25 | Loss: 0.00115938
Iteration 17/25 | Loss: 0.00115855
Iteration 18/25 | Loss: 0.00115877
Iteration 19/25 | Loss: 0.00115874
Iteration 20/25 | Loss: 0.00115902
Iteration 21/25 | Loss: 0.00115824
Iteration 22/25 | Loss: 0.00115868
Iteration 23/25 | Loss: 0.00115894
Iteration 24/25 | Loss: 0.00115867
Iteration 25/25 | Loss: 0.00115930

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24097538
Iteration 2/25 | Loss: 0.00193157
Iteration 3/25 | Loss: 0.00193156
Iteration 4/25 | Loss: 0.00193156
Iteration 5/25 | Loss: 0.00193156
Iteration 6/25 | Loss: 0.00193156
Iteration 7/25 | Loss: 0.00193156
Iteration 8/25 | Loss: 0.00193156
Iteration 9/25 | Loss: 0.00193156
Iteration 10/25 | Loss: 0.00193156
Iteration 11/25 | Loss: 0.00193156
Iteration 12/25 | Loss: 0.00193156
Iteration 13/25 | Loss: 0.00193156
Iteration 14/25 | Loss: 0.00193156
Iteration 15/25 | Loss: 0.00193156
Iteration 16/25 | Loss: 0.00193156
Iteration 17/25 | Loss: 0.00193156
Iteration 18/25 | Loss: 0.00193156
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0019315603422001004, 0.0019315603422001004, 0.0019315603422001004, 0.0019315603422001004, 0.0019315603422001004]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019315603422001004

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00193156
Iteration 2/1000 | Loss: 0.00004207
Iteration 3/1000 | Loss: 0.00002121
Iteration 4/1000 | Loss: 0.00003757
Iteration 5/1000 | Loss: 0.00001609
Iteration 6/1000 | Loss: 0.00002824
Iteration 7/1000 | Loss: 0.00003394
Iteration 8/1000 | Loss: 0.00002973
Iteration 9/1000 | Loss: 0.00003758
Iteration 10/1000 | Loss: 0.00004182
Iteration 11/1000 | Loss: 0.00003641
Iteration 12/1000 | Loss: 0.00004418
Iteration 13/1000 | Loss: 0.00003197
Iteration 14/1000 | Loss: 0.00004282
Iteration 15/1000 | Loss: 0.00003402
Iteration 16/1000 | Loss: 0.00005029
Iteration 17/1000 | Loss: 0.00003928
Iteration 18/1000 | Loss: 0.00004342
Iteration 19/1000 | Loss: 0.00002752
Iteration 20/1000 | Loss: 0.00002476
Iteration 21/1000 | Loss: 0.00001806
Iteration 22/1000 | Loss: 0.00002317
Iteration 23/1000 | Loss: 0.00003523
Iteration 24/1000 | Loss: 0.00003574
Iteration 25/1000 | Loss: 0.00003772
Iteration 26/1000 | Loss: 0.00002998
Iteration 27/1000 | Loss: 0.00002801
Iteration 28/1000 | Loss: 0.00003021
Iteration 29/1000 | Loss: 0.00004155
Iteration 30/1000 | Loss: 0.00003243
Iteration 31/1000 | Loss: 0.00002842
Iteration 32/1000 | Loss: 0.00002101
Iteration 33/1000 | Loss: 0.00002197
Iteration 34/1000 | Loss: 0.00003429
Iteration 35/1000 | Loss: 0.00002180
Iteration 36/1000 | Loss: 0.00002470
Iteration 37/1000 | Loss: 0.00003050
Iteration 38/1000 | Loss: 0.00003804
Iteration 39/1000 | Loss: 0.00003175
Iteration 40/1000 | Loss: 0.00002004
Iteration 41/1000 | Loss: 0.00003108
Iteration 42/1000 | Loss: 0.00003896
Iteration 43/1000 | Loss: 0.00003976
Iteration 44/1000 | Loss: 0.00004106
Iteration 45/1000 | Loss: 0.00003940
Iteration 46/1000 | Loss: 0.00004006
Iteration 47/1000 | Loss: 0.00003849
Iteration 48/1000 | Loss: 0.00004009
Iteration 49/1000 | Loss: 0.00003846
Iteration 50/1000 | Loss: 0.00003366
Iteration 51/1000 | Loss: 0.00002514
Iteration 52/1000 | Loss: 0.00003148
Iteration 53/1000 | Loss: 0.00001445
Iteration 54/1000 | Loss: 0.00002407
Iteration 55/1000 | Loss: 0.00002686
Iteration 56/1000 | Loss: 0.00001881
Iteration 57/1000 | Loss: 0.00001768
Iteration 58/1000 | Loss: 0.00001684
Iteration 59/1000 | Loss: 0.00002344
Iteration 60/1000 | Loss: 0.00002456
Iteration 61/1000 | Loss: 0.00002320
Iteration 62/1000 | Loss: 0.00001608
Iteration 63/1000 | Loss: 0.00001333
Iteration 64/1000 | Loss: 0.00001140
Iteration 65/1000 | Loss: 0.00001045
Iteration 66/1000 | Loss: 0.00000991
Iteration 67/1000 | Loss: 0.00000957
Iteration 68/1000 | Loss: 0.00000929
Iteration 69/1000 | Loss: 0.00000924
Iteration 70/1000 | Loss: 0.00000923
Iteration 71/1000 | Loss: 0.00000922
Iteration 72/1000 | Loss: 0.00000920
Iteration 73/1000 | Loss: 0.00000920
Iteration 74/1000 | Loss: 0.00000919
Iteration 75/1000 | Loss: 0.00000919
Iteration 76/1000 | Loss: 0.00000918
Iteration 77/1000 | Loss: 0.00000917
Iteration 78/1000 | Loss: 0.00000916
Iteration 79/1000 | Loss: 0.00000908
Iteration 80/1000 | Loss: 0.00000897
Iteration 81/1000 | Loss: 0.00000893
Iteration 82/1000 | Loss: 0.00000892
Iteration 83/1000 | Loss: 0.00000891
Iteration 84/1000 | Loss: 0.00000891
Iteration 85/1000 | Loss: 0.00000890
Iteration 86/1000 | Loss: 0.00000890
Iteration 87/1000 | Loss: 0.00000889
Iteration 88/1000 | Loss: 0.00000889
Iteration 89/1000 | Loss: 0.00000888
Iteration 90/1000 | Loss: 0.00000887
Iteration 91/1000 | Loss: 0.00000886
Iteration 92/1000 | Loss: 0.00000886
Iteration 93/1000 | Loss: 0.00000877
Iteration 94/1000 | Loss: 0.00000877
Iteration 95/1000 | Loss: 0.00000876
Iteration 96/1000 | Loss: 0.00000875
Iteration 97/1000 | Loss: 0.00000872
Iteration 98/1000 | Loss: 0.00000872
Iteration 99/1000 | Loss: 0.00000871
Iteration 100/1000 | Loss: 0.00000871
Iteration 101/1000 | Loss: 0.00000867
Iteration 102/1000 | Loss: 0.00000865
Iteration 103/1000 | Loss: 0.00000864
Iteration 104/1000 | Loss: 0.00000864
Iteration 105/1000 | Loss: 0.00000864
Iteration 106/1000 | Loss: 0.00000863
Iteration 107/1000 | Loss: 0.00000861
Iteration 108/1000 | Loss: 0.00000860
Iteration 109/1000 | Loss: 0.00000860
Iteration 110/1000 | Loss: 0.00000860
Iteration 111/1000 | Loss: 0.00000859
Iteration 112/1000 | Loss: 0.00000859
Iteration 113/1000 | Loss: 0.00000858
Iteration 114/1000 | Loss: 0.00000857
Iteration 115/1000 | Loss: 0.00000857
Iteration 116/1000 | Loss: 0.00000857
Iteration 117/1000 | Loss: 0.00000857
Iteration 118/1000 | Loss: 0.00000857
Iteration 119/1000 | Loss: 0.00000856
Iteration 120/1000 | Loss: 0.00000856
Iteration 121/1000 | Loss: 0.00000856
Iteration 122/1000 | Loss: 0.00000856
Iteration 123/1000 | Loss: 0.00000855
Iteration 124/1000 | Loss: 0.00000855
Iteration 125/1000 | Loss: 0.00000855
Iteration 126/1000 | Loss: 0.00000851
Iteration 127/1000 | Loss: 0.00000850
Iteration 128/1000 | Loss: 0.00000850
Iteration 129/1000 | Loss: 0.00000850
Iteration 130/1000 | Loss: 0.00000849
Iteration 131/1000 | Loss: 0.00000849
Iteration 132/1000 | Loss: 0.00000848
Iteration 133/1000 | Loss: 0.00000848
Iteration 134/1000 | Loss: 0.00000848
Iteration 135/1000 | Loss: 0.00000848
Iteration 136/1000 | Loss: 0.00000848
Iteration 137/1000 | Loss: 0.00000848
Iteration 138/1000 | Loss: 0.00000848
Iteration 139/1000 | Loss: 0.00000848
Iteration 140/1000 | Loss: 0.00000848
Iteration 141/1000 | Loss: 0.00000848
Iteration 142/1000 | Loss: 0.00000847
Iteration 143/1000 | Loss: 0.00000847
Iteration 144/1000 | Loss: 0.00000847
Iteration 145/1000 | Loss: 0.00000847
Iteration 146/1000 | Loss: 0.00000847
Iteration 147/1000 | Loss: 0.00000847
Iteration 148/1000 | Loss: 0.00000847
Iteration 149/1000 | Loss: 0.00000847
Iteration 150/1000 | Loss: 0.00000847
Iteration 151/1000 | Loss: 0.00000846
Iteration 152/1000 | Loss: 0.00000846
Iteration 153/1000 | Loss: 0.00000846
Iteration 154/1000 | Loss: 0.00000846
Iteration 155/1000 | Loss: 0.00000846
Iteration 156/1000 | Loss: 0.00000846
Iteration 157/1000 | Loss: 0.00000846
Iteration 158/1000 | Loss: 0.00000846
Iteration 159/1000 | Loss: 0.00000845
Iteration 160/1000 | Loss: 0.00000845
Iteration 161/1000 | Loss: 0.00000845
Iteration 162/1000 | Loss: 0.00000845
Iteration 163/1000 | Loss: 0.00000845
Iteration 164/1000 | Loss: 0.00000845
Iteration 165/1000 | Loss: 0.00000845
Iteration 166/1000 | Loss: 0.00000845
Iteration 167/1000 | Loss: 0.00000845
Iteration 168/1000 | Loss: 0.00000845
Iteration 169/1000 | Loss: 0.00000845
Iteration 170/1000 | Loss: 0.00000845
Iteration 171/1000 | Loss: 0.00000845
Iteration 172/1000 | Loss: 0.00000845
Iteration 173/1000 | Loss: 0.00000845
Iteration 174/1000 | Loss: 0.00000844
Iteration 175/1000 | Loss: 0.00000844
Iteration 176/1000 | Loss: 0.00000844
Iteration 177/1000 | Loss: 0.00000844
Iteration 178/1000 | Loss: 0.00000844
Iteration 179/1000 | Loss: 0.00000844
Iteration 180/1000 | Loss: 0.00000844
Iteration 181/1000 | Loss: 0.00000843
Iteration 182/1000 | Loss: 0.00000843
Iteration 183/1000 | Loss: 0.00000843
Iteration 184/1000 | Loss: 0.00000842
Iteration 185/1000 | Loss: 0.00000842
Iteration 186/1000 | Loss: 0.00000842
Iteration 187/1000 | Loss: 0.00000842
Iteration 188/1000 | Loss: 0.00000842
Iteration 189/1000 | Loss: 0.00000842
Iteration 190/1000 | Loss: 0.00000842
Iteration 191/1000 | Loss: 0.00000842
Iteration 192/1000 | Loss: 0.00000842
Iteration 193/1000 | Loss: 0.00000842
Iteration 194/1000 | Loss: 0.00000842
Iteration 195/1000 | Loss: 0.00000842
Iteration 196/1000 | Loss: 0.00000842
Iteration 197/1000 | Loss: 0.00000842
Iteration 198/1000 | Loss: 0.00000842
Iteration 199/1000 | Loss: 0.00000842
Iteration 200/1000 | Loss: 0.00000841
Iteration 201/1000 | Loss: 0.00000840
Iteration 202/1000 | Loss: 0.00000840
Iteration 203/1000 | Loss: 0.00000839
Iteration 204/1000 | Loss: 0.00000839
Iteration 205/1000 | Loss: 0.00000839
Iteration 206/1000 | Loss: 0.00000839
Iteration 207/1000 | Loss: 0.00000839
Iteration 208/1000 | Loss: 0.00000839
Iteration 209/1000 | Loss: 0.00000839
Iteration 210/1000 | Loss: 0.00000839
Iteration 211/1000 | Loss: 0.00000839
Iteration 212/1000 | Loss: 0.00000839
Iteration 213/1000 | Loss: 0.00000838
Iteration 214/1000 | Loss: 0.00000838
Iteration 215/1000 | Loss: 0.00000838
Iteration 216/1000 | Loss: 0.00000838
Iteration 217/1000 | Loss: 0.00000838
Iteration 218/1000 | Loss: 0.00000838
Iteration 219/1000 | Loss: 0.00000838
Iteration 220/1000 | Loss: 0.00000838
Iteration 221/1000 | Loss: 0.00000838
Iteration 222/1000 | Loss: 0.00000838
Iteration 223/1000 | Loss: 0.00000838
Iteration 224/1000 | Loss: 0.00000838
Iteration 225/1000 | Loss: 0.00000838
Iteration 226/1000 | Loss: 0.00000837
Iteration 227/1000 | Loss: 0.00000837
Iteration 228/1000 | Loss: 0.00000837
Iteration 229/1000 | Loss: 0.00000837
Iteration 230/1000 | Loss: 0.00000837
Iteration 231/1000 | Loss: 0.00000837
Iteration 232/1000 | Loss: 0.00000837
Iteration 233/1000 | Loss: 0.00000837
Iteration 234/1000 | Loss: 0.00000837
Iteration 235/1000 | Loss: 0.00000837
Iteration 236/1000 | Loss: 0.00000837
Iteration 237/1000 | Loss: 0.00000837
Iteration 238/1000 | Loss: 0.00000837
Iteration 239/1000 | Loss: 0.00000837
Iteration 240/1000 | Loss: 0.00000837
Iteration 241/1000 | Loss: 0.00000837
Iteration 242/1000 | Loss: 0.00000837
Iteration 243/1000 | Loss: 0.00000836
Iteration 244/1000 | Loss: 0.00000836
Iteration 245/1000 | Loss: 0.00000836
Iteration 246/1000 | Loss: 0.00000836
Iteration 247/1000 | Loss: 0.00000836
Iteration 248/1000 | Loss: 0.00000836
Iteration 249/1000 | Loss: 0.00000836
Iteration 250/1000 | Loss: 0.00000836
Iteration 251/1000 | Loss: 0.00000836
Iteration 252/1000 | Loss: 0.00000836
Iteration 253/1000 | Loss: 0.00000836
Iteration 254/1000 | Loss: 0.00000836
Iteration 255/1000 | Loss: 0.00000836
Iteration 256/1000 | Loss: 0.00000836
Iteration 257/1000 | Loss: 0.00000836
Iteration 258/1000 | Loss: 0.00000836
Iteration 259/1000 | Loss: 0.00000836
Iteration 260/1000 | Loss: 0.00000836
Iteration 261/1000 | Loss: 0.00000836
Iteration 262/1000 | Loss: 0.00000836
Iteration 263/1000 | Loss: 0.00000836
Iteration 264/1000 | Loss: 0.00000836
Iteration 265/1000 | Loss: 0.00000836
Iteration 266/1000 | Loss: 0.00000836
Iteration 267/1000 | Loss: 0.00000836
Iteration 268/1000 | Loss: 0.00000836
Iteration 269/1000 | Loss: 0.00000836
Iteration 270/1000 | Loss: 0.00000836
Iteration 271/1000 | Loss: 0.00000836
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 271. Stopping optimization.
Last 5 losses: [8.356336365977768e-06, 8.356336365977768e-06, 8.356336365977768e-06, 8.356336365977768e-06, 8.356336365977768e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.356336365977768e-06

Optimization complete. Final v2v error: 2.4976906776428223 mm

Highest mean error: 3.518773317337036 mm for frame 89

Lowest mean error: 2.337977886199951 mm for frame 182

Saving results

Total time: 166.67282962799072
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00432789
Iteration 2/25 | Loss: 0.00140655
Iteration 3/25 | Loss: 0.00119890
Iteration 4/25 | Loss: 0.00118295
Iteration 5/25 | Loss: 0.00117976
Iteration 6/25 | Loss: 0.00117941
Iteration 7/25 | Loss: 0.00117941
Iteration 8/25 | Loss: 0.00117941
Iteration 9/25 | Loss: 0.00117941
Iteration 10/25 | Loss: 0.00117941
Iteration 11/25 | Loss: 0.00117941
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011794096790254116, 0.0011794096790254116, 0.0011794096790254116, 0.0011794096790254116, 0.0011794096790254116]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011794096790254116

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22363544
Iteration 2/25 | Loss: 0.00180996
Iteration 3/25 | Loss: 0.00180996
Iteration 4/25 | Loss: 0.00180996
Iteration 5/25 | Loss: 0.00180996
Iteration 6/25 | Loss: 0.00180996
Iteration 7/25 | Loss: 0.00180995
Iteration 8/25 | Loss: 0.00180995
Iteration 9/25 | Loss: 0.00180995
Iteration 10/25 | Loss: 0.00180995
Iteration 11/25 | Loss: 0.00180995
Iteration 12/25 | Loss: 0.00180995
Iteration 13/25 | Loss: 0.00180995
Iteration 14/25 | Loss: 0.00180995
Iteration 15/25 | Loss: 0.00180995
Iteration 16/25 | Loss: 0.00180995
Iteration 17/25 | Loss: 0.00180995
Iteration 18/25 | Loss: 0.00180995
Iteration 19/25 | Loss: 0.00180995
Iteration 20/25 | Loss: 0.00180995
Iteration 21/25 | Loss: 0.00180995
Iteration 22/25 | Loss: 0.00180995
Iteration 23/25 | Loss: 0.00180995
Iteration 24/25 | Loss: 0.00180995
Iteration 25/25 | Loss: 0.00180995

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00180995
Iteration 2/1000 | Loss: 0.00003281
Iteration 3/1000 | Loss: 0.00002088
Iteration 4/1000 | Loss: 0.00001834
Iteration 5/1000 | Loss: 0.00001742
Iteration 6/1000 | Loss: 0.00001649
Iteration 7/1000 | Loss: 0.00001596
Iteration 8/1000 | Loss: 0.00001559
Iteration 9/1000 | Loss: 0.00001537
Iteration 10/1000 | Loss: 0.00001515
Iteration 11/1000 | Loss: 0.00001490
Iteration 12/1000 | Loss: 0.00001477
Iteration 13/1000 | Loss: 0.00001475
Iteration 14/1000 | Loss: 0.00001473
Iteration 15/1000 | Loss: 0.00001466
Iteration 16/1000 | Loss: 0.00001454
Iteration 17/1000 | Loss: 0.00001453
Iteration 18/1000 | Loss: 0.00001448
Iteration 19/1000 | Loss: 0.00001448
Iteration 20/1000 | Loss: 0.00001447
Iteration 21/1000 | Loss: 0.00001447
Iteration 22/1000 | Loss: 0.00001445
Iteration 23/1000 | Loss: 0.00001444
Iteration 24/1000 | Loss: 0.00001443
Iteration 25/1000 | Loss: 0.00001443
Iteration 26/1000 | Loss: 0.00001442
Iteration 27/1000 | Loss: 0.00001441
Iteration 28/1000 | Loss: 0.00001440
Iteration 29/1000 | Loss: 0.00001439
Iteration 30/1000 | Loss: 0.00001439
Iteration 31/1000 | Loss: 0.00001438
Iteration 32/1000 | Loss: 0.00001438
Iteration 33/1000 | Loss: 0.00001437
Iteration 34/1000 | Loss: 0.00001436
Iteration 35/1000 | Loss: 0.00001436
Iteration 36/1000 | Loss: 0.00001435
Iteration 37/1000 | Loss: 0.00001435
Iteration 38/1000 | Loss: 0.00001435
Iteration 39/1000 | Loss: 0.00001434
Iteration 40/1000 | Loss: 0.00001432
Iteration 41/1000 | Loss: 0.00001431
Iteration 42/1000 | Loss: 0.00001430
Iteration 43/1000 | Loss: 0.00001429
Iteration 44/1000 | Loss: 0.00001426
Iteration 45/1000 | Loss: 0.00001426
Iteration 46/1000 | Loss: 0.00001424
Iteration 47/1000 | Loss: 0.00001424
Iteration 48/1000 | Loss: 0.00001423
Iteration 49/1000 | Loss: 0.00001423
Iteration 50/1000 | Loss: 0.00001423
Iteration 51/1000 | Loss: 0.00001422
Iteration 52/1000 | Loss: 0.00001422
Iteration 53/1000 | Loss: 0.00001421
Iteration 54/1000 | Loss: 0.00001421
Iteration 55/1000 | Loss: 0.00001420
Iteration 56/1000 | Loss: 0.00001419
Iteration 57/1000 | Loss: 0.00001419
Iteration 58/1000 | Loss: 0.00001419
Iteration 59/1000 | Loss: 0.00001419
Iteration 60/1000 | Loss: 0.00001419
Iteration 61/1000 | Loss: 0.00001419
Iteration 62/1000 | Loss: 0.00001418
Iteration 63/1000 | Loss: 0.00001418
Iteration 64/1000 | Loss: 0.00001418
Iteration 65/1000 | Loss: 0.00001418
Iteration 66/1000 | Loss: 0.00001418
Iteration 67/1000 | Loss: 0.00001417
Iteration 68/1000 | Loss: 0.00001417
Iteration 69/1000 | Loss: 0.00001417
Iteration 70/1000 | Loss: 0.00001417
Iteration 71/1000 | Loss: 0.00001416
Iteration 72/1000 | Loss: 0.00001416
Iteration 73/1000 | Loss: 0.00001416
Iteration 74/1000 | Loss: 0.00001416
Iteration 75/1000 | Loss: 0.00001416
Iteration 76/1000 | Loss: 0.00001416
Iteration 77/1000 | Loss: 0.00001416
Iteration 78/1000 | Loss: 0.00001416
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 78. Stopping optimization.
Last 5 losses: [1.4158588783175219e-05, 1.4158588783175219e-05, 1.4158588783175219e-05, 1.4158588783175219e-05, 1.4158588783175219e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4158588783175219e-05

Optimization complete. Final v2v error: 3.2082834243774414 mm

Highest mean error: 4.43524169921875 mm for frame 203

Lowest mean error: 2.7112414836883545 mm for frame 97

Saving results

Total time: 39.304237365722656
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00392945
Iteration 2/25 | Loss: 0.00128536
Iteration 3/25 | Loss: 0.00115711
Iteration 4/25 | Loss: 0.00113951
Iteration 5/25 | Loss: 0.00113723
Iteration 6/25 | Loss: 0.00113706
Iteration 7/25 | Loss: 0.00113706
Iteration 8/25 | Loss: 0.00113706
Iteration 9/25 | Loss: 0.00113706
Iteration 10/25 | Loss: 0.00113706
Iteration 11/25 | Loss: 0.00113706
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011370606953278184, 0.0011370606953278184, 0.0011370606953278184, 0.0011370606953278184, 0.0011370606953278184]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011370606953278184

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23028684
Iteration 2/25 | Loss: 0.00165322
Iteration 3/25 | Loss: 0.00165322
Iteration 4/25 | Loss: 0.00165322
Iteration 5/25 | Loss: 0.00165321
Iteration 6/25 | Loss: 0.00165321
Iteration 7/25 | Loss: 0.00165321
Iteration 8/25 | Loss: 0.00165321
Iteration 9/25 | Loss: 0.00165321
Iteration 10/25 | Loss: 0.00165321
Iteration 11/25 | Loss: 0.00165321
Iteration 12/25 | Loss: 0.00165321
Iteration 13/25 | Loss: 0.00165321
Iteration 14/25 | Loss: 0.00165321
Iteration 15/25 | Loss: 0.00165321
Iteration 16/25 | Loss: 0.00165321
Iteration 17/25 | Loss: 0.00165321
Iteration 18/25 | Loss: 0.00165321
Iteration 19/25 | Loss: 0.00165321
Iteration 20/25 | Loss: 0.00165321
Iteration 21/25 | Loss: 0.00165321
Iteration 22/25 | Loss: 0.00165321
Iteration 23/25 | Loss: 0.00165321
Iteration 24/25 | Loss: 0.00165321
Iteration 25/25 | Loss: 0.00165321

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00165321
Iteration 2/1000 | Loss: 0.00001967
Iteration 3/1000 | Loss: 0.00001496
Iteration 4/1000 | Loss: 0.00001330
Iteration 5/1000 | Loss: 0.00001224
Iteration 6/1000 | Loss: 0.00001173
Iteration 7/1000 | Loss: 0.00001124
Iteration 8/1000 | Loss: 0.00001100
Iteration 9/1000 | Loss: 0.00001072
Iteration 10/1000 | Loss: 0.00001049
Iteration 11/1000 | Loss: 0.00001040
Iteration 12/1000 | Loss: 0.00001034
Iteration 13/1000 | Loss: 0.00001033
Iteration 14/1000 | Loss: 0.00001016
Iteration 15/1000 | Loss: 0.00001013
Iteration 16/1000 | Loss: 0.00000999
Iteration 17/1000 | Loss: 0.00000999
Iteration 18/1000 | Loss: 0.00000997
Iteration 19/1000 | Loss: 0.00000995
Iteration 20/1000 | Loss: 0.00000993
Iteration 21/1000 | Loss: 0.00000993
Iteration 22/1000 | Loss: 0.00000987
Iteration 23/1000 | Loss: 0.00000986
Iteration 24/1000 | Loss: 0.00000981
Iteration 25/1000 | Loss: 0.00000980
Iteration 26/1000 | Loss: 0.00000978
Iteration 27/1000 | Loss: 0.00000968
Iteration 28/1000 | Loss: 0.00000960
Iteration 29/1000 | Loss: 0.00000959
Iteration 30/1000 | Loss: 0.00000957
Iteration 31/1000 | Loss: 0.00000957
Iteration 32/1000 | Loss: 0.00000956
Iteration 33/1000 | Loss: 0.00000956
Iteration 34/1000 | Loss: 0.00000955
Iteration 35/1000 | Loss: 0.00000954
Iteration 36/1000 | Loss: 0.00000953
Iteration 37/1000 | Loss: 0.00000952
Iteration 38/1000 | Loss: 0.00000952
Iteration 39/1000 | Loss: 0.00000951
Iteration 40/1000 | Loss: 0.00000951
Iteration 41/1000 | Loss: 0.00000947
Iteration 42/1000 | Loss: 0.00000947
Iteration 43/1000 | Loss: 0.00000947
Iteration 44/1000 | Loss: 0.00000947
Iteration 45/1000 | Loss: 0.00000947
Iteration 46/1000 | Loss: 0.00000947
Iteration 47/1000 | Loss: 0.00000947
Iteration 48/1000 | Loss: 0.00000947
Iteration 49/1000 | Loss: 0.00000947
Iteration 50/1000 | Loss: 0.00000947
Iteration 51/1000 | Loss: 0.00000946
Iteration 52/1000 | Loss: 0.00000946
Iteration 53/1000 | Loss: 0.00000945
Iteration 54/1000 | Loss: 0.00000945
Iteration 55/1000 | Loss: 0.00000945
Iteration 56/1000 | Loss: 0.00000945
Iteration 57/1000 | Loss: 0.00000945
Iteration 58/1000 | Loss: 0.00000945
Iteration 59/1000 | Loss: 0.00000945
Iteration 60/1000 | Loss: 0.00000945
Iteration 61/1000 | Loss: 0.00000944
Iteration 62/1000 | Loss: 0.00000944
Iteration 63/1000 | Loss: 0.00000944
Iteration 64/1000 | Loss: 0.00000943
Iteration 65/1000 | Loss: 0.00000943
Iteration 66/1000 | Loss: 0.00000942
Iteration 67/1000 | Loss: 0.00000942
Iteration 68/1000 | Loss: 0.00000942
Iteration 69/1000 | Loss: 0.00000941
Iteration 70/1000 | Loss: 0.00000941
Iteration 71/1000 | Loss: 0.00000941
Iteration 72/1000 | Loss: 0.00000941
Iteration 73/1000 | Loss: 0.00000941
Iteration 74/1000 | Loss: 0.00000941
Iteration 75/1000 | Loss: 0.00000941
Iteration 76/1000 | Loss: 0.00000941
Iteration 77/1000 | Loss: 0.00000941
Iteration 78/1000 | Loss: 0.00000941
Iteration 79/1000 | Loss: 0.00000941
Iteration 80/1000 | Loss: 0.00000940
Iteration 81/1000 | Loss: 0.00000940
Iteration 82/1000 | Loss: 0.00000940
Iteration 83/1000 | Loss: 0.00000940
Iteration 84/1000 | Loss: 0.00000939
Iteration 85/1000 | Loss: 0.00000939
Iteration 86/1000 | Loss: 0.00000939
Iteration 87/1000 | Loss: 0.00000939
Iteration 88/1000 | Loss: 0.00000939
Iteration 89/1000 | Loss: 0.00000938
Iteration 90/1000 | Loss: 0.00000938
Iteration 91/1000 | Loss: 0.00000938
Iteration 92/1000 | Loss: 0.00000937
Iteration 93/1000 | Loss: 0.00000937
Iteration 94/1000 | Loss: 0.00000937
Iteration 95/1000 | Loss: 0.00000937
Iteration 96/1000 | Loss: 0.00000937
Iteration 97/1000 | Loss: 0.00000936
Iteration 98/1000 | Loss: 0.00000936
Iteration 99/1000 | Loss: 0.00000936
Iteration 100/1000 | Loss: 0.00000936
Iteration 101/1000 | Loss: 0.00000936
Iteration 102/1000 | Loss: 0.00000936
Iteration 103/1000 | Loss: 0.00000936
Iteration 104/1000 | Loss: 0.00000936
Iteration 105/1000 | Loss: 0.00000936
Iteration 106/1000 | Loss: 0.00000936
Iteration 107/1000 | Loss: 0.00000936
Iteration 108/1000 | Loss: 0.00000936
Iteration 109/1000 | Loss: 0.00000936
Iteration 110/1000 | Loss: 0.00000936
Iteration 111/1000 | Loss: 0.00000936
Iteration 112/1000 | Loss: 0.00000936
Iteration 113/1000 | Loss: 0.00000936
Iteration 114/1000 | Loss: 0.00000936
Iteration 115/1000 | Loss: 0.00000936
Iteration 116/1000 | Loss: 0.00000936
Iteration 117/1000 | Loss: 0.00000936
Iteration 118/1000 | Loss: 0.00000936
Iteration 119/1000 | Loss: 0.00000936
Iteration 120/1000 | Loss: 0.00000936
Iteration 121/1000 | Loss: 0.00000936
Iteration 122/1000 | Loss: 0.00000936
Iteration 123/1000 | Loss: 0.00000936
Iteration 124/1000 | Loss: 0.00000936
Iteration 125/1000 | Loss: 0.00000936
Iteration 126/1000 | Loss: 0.00000936
Iteration 127/1000 | Loss: 0.00000936
Iteration 128/1000 | Loss: 0.00000936
Iteration 129/1000 | Loss: 0.00000936
Iteration 130/1000 | Loss: 0.00000936
Iteration 131/1000 | Loss: 0.00000936
Iteration 132/1000 | Loss: 0.00000936
Iteration 133/1000 | Loss: 0.00000936
Iteration 134/1000 | Loss: 0.00000936
Iteration 135/1000 | Loss: 0.00000936
Iteration 136/1000 | Loss: 0.00000936
Iteration 137/1000 | Loss: 0.00000936
Iteration 138/1000 | Loss: 0.00000936
Iteration 139/1000 | Loss: 0.00000936
Iteration 140/1000 | Loss: 0.00000936
Iteration 141/1000 | Loss: 0.00000936
Iteration 142/1000 | Loss: 0.00000936
Iteration 143/1000 | Loss: 0.00000936
Iteration 144/1000 | Loss: 0.00000936
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [9.356594091514125e-06, 9.356594091514125e-06, 9.356594091514125e-06, 9.356594091514125e-06, 9.356594091514125e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.356594091514125e-06

Optimization complete. Final v2v error: 2.631040573120117 mm

Highest mean error: 2.9514756202697754 mm for frame 74

Lowest mean error: 2.421523094177246 mm for frame 4

Saving results

Total time: 38.28662586212158
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00381114
Iteration 2/25 | Loss: 0.00119939
Iteration 3/25 | Loss: 0.00113984
Iteration 4/25 | Loss: 0.00113098
Iteration 5/25 | Loss: 0.00112802
Iteration 6/25 | Loss: 0.00112710
Iteration 7/25 | Loss: 0.00112710
Iteration 8/25 | Loss: 0.00112710
Iteration 9/25 | Loss: 0.00112710
Iteration 10/25 | Loss: 0.00112710
Iteration 11/25 | Loss: 0.00112710
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001127102063037455, 0.001127102063037455, 0.001127102063037455, 0.001127102063037455, 0.001127102063037455]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001127102063037455

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.75257266
Iteration 2/25 | Loss: 0.00191104
Iteration 3/25 | Loss: 0.00191104
Iteration 4/25 | Loss: 0.00191104
Iteration 5/25 | Loss: 0.00191104
Iteration 6/25 | Loss: 0.00191104
Iteration 7/25 | Loss: 0.00191104
Iteration 8/25 | Loss: 0.00191104
Iteration 9/25 | Loss: 0.00191104
Iteration 10/25 | Loss: 0.00191104
Iteration 11/25 | Loss: 0.00191104
Iteration 12/25 | Loss: 0.00191104
Iteration 13/25 | Loss: 0.00191104
Iteration 14/25 | Loss: 0.00191104
Iteration 15/25 | Loss: 0.00191104
Iteration 16/25 | Loss: 0.00191104
Iteration 17/25 | Loss: 0.00191104
Iteration 18/25 | Loss: 0.00191104
Iteration 19/25 | Loss: 0.00191104
Iteration 20/25 | Loss: 0.00191104
Iteration 21/25 | Loss: 0.00191104
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0019110353896394372, 0.0019110353896394372, 0.0019110353896394372, 0.0019110353896394372, 0.0019110353896394372]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019110353896394372

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00191104
Iteration 2/1000 | Loss: 0.00002011
Iteration 3/1000 | Loss: 0.00001408
Iteration 4/1000 | Loss: 0.00001184
Iteration 5/1000 | Loss: 0.00001087
Iteration 6/1000 | Loss: 0.00001028
Iteration 7/1000 | Loss: 0.00000986
Iteration 8/1000 | Loss: 0.00000942
Iteration 9/1000 | Loss: 0.00000931
Iteration 10/1000 | Loss: 0.00000918
Iteration 11/1000 | Loss: 0.00000918
Iteration 12/1000 | Loss: 0.00000904
Iteration 13/1000 | Loss: 0.00000890
Iteration 14/1000 | Loss: 0.00000887
Iteration 15/1000 | Loss: 0.00000886
Iteration 16/1000 | Loss: 0.00000885
Iteration 17/1000 | Loss: 0.00000875
Iteration 18/1000 | Loss: 0.00000873
Iteration 19/1000 | Loss: 0.00000866
Iteration 20/1000 | Loss: 0.00000864
Iteration 21/1000 | Loss: 0.00000861
Iteration 22/1000 | Loss: 0.00000860
Iteration 23/1000 | Loss: 0.00000859
Iteration 24/1000 | Loss: 0.00000858
Iteration 25/1000 | Loss: 0.00000858
Iteration 26/1000 | Loss: 0.00000858
Iteration 27/1000 | Loss: 0.00000857
Iteration 28/1000 | Loss: 0.00000857
Iteration 29/1000 | Loss: 0.00000856
Iteration 30/1000 | Loss: 0.00000856
Iteration 31/1000 | Loss: 0.00000855
Iteration 32/1000 | Loss: 0.00000855
Iteration 33/1000 | Loss: 0.00000855
Iteration 34/1000 | Loss: 0.00000854
Iteration 35/1000 | Loss: 0.00000854
Iteration 36/1000 | Loss: 0.00000854
Iteration 37/1000 | Loss: 0.00000854
Iteration 38/1000 | Loss: 0.00000854
Iteration 39/1000 | Loss: 0.00000854
Iteration 40/1000 | Loss: 0.00000854
Iteration 41/1000 | Loss: 0.00000854
Iteration 42/1000 | Loss: 0.00000853
Iteration 43/1000 | Loss: 0.00000853
Iteration 44/1000 | Loss: 0.00000852
Iteration 45/1000 | Loss: 0.00000851
Iteration 46/1000 | Loss: 0.00000850
Iteration 47/1000 | Loss: 0.00000850
Iteration 48/1000 | Loss: 0.00000850
Iteration 49/1000 | Loss: 0.00000850
Iteration 50/1000 | Loss: 0.00000849
Iteration 51/1000 | Loss: 0.00000849
Iteration 52/1000 | Loss: 0.00000848
Iteration 53/1000 | Loss: 0.00000848
Iteration 54/1000 | Loss: 0.00000847
Iteration 55/1000 | Loss: 0.00000844
Iteration 56/1000 | Loss: 0.00000844
Iteration 57/1000 | Loss: 0.00000844
Iteration 58/1000 | Loss: 0.00000844
Iteration 59/1000 | Loss: 0.00000844
Iteration 60/1000 | Loss: 0.00000844
Iteration 61/1000 | Loss: 0.00000844
Iteration 62/1000 | Loss: 0.00000844
Iteration 63/1000 | Loss: 0.00000844
Iteration 64/1000 | Loss: 0.00000844
Iteration 65/1000 | Loss: 0.00000844
Iteration 66/1000 | Loss: 0.00000843
Iteration 67/1000 | Loss: 0.00000843
Iteration 68/1000 | Loss: 0.00000843
Iteration 69/1000 | Loss: 0.00000841
Iteration 70/1000 | Loss: 0.00000838
Iteration 71/1000 | Loss: 0.00000838
Iteration 72/1000 | Loss: 0.00000838
Iteration 73/1000 | Loss: 0.00000837
Iteration 74/1000 | Loss: 0.00000837
Iteration 75/1000 | Loss: 0.00000837
Iteration 76/1000 | Loss: 0.00000836
Iteration 77/1000 | Loss: 0.00000836
Iteration 78/1000 | Loss: 0.00000835
Iteration 79/1000 | Loss: 0.00000835
Iteration 80/1000 | Loss: 0.00000834
Iteration 81/1000 | Loss: 0.00000834
Iteration 82/1000 | Loss: 0.00000833
Iteration 83/1000 | Loss: 0.00000833
Iteration 84/1000 | Loss: 0.00000833
Iteration 85/1000 | Loss: 0.00000833
Iteration 86/1000 | Loss: 0.00000833
Iteration 87/1000 | Loss: 0.00000833
Iteration 88/1000 | Loss: 0.00000832
Iteration 89/1000 | Loss: 0.00000832
Iteration 90/1000 | Loss: 0.00000832
Iteration 91/1000 | Loss: 0.00000831
Iteration 92/1000 | Loss: 0.00000831
Iteration 93/1000 | Loss: 0.00000831
Iteration 94/1000 | Loss: 0.00000831
Iteration 95/1000 | Loss: 0.00000831
Iteration 96/1000 | Loss: 0.00000831
Iteration 97/1000 | Loss: 0.00000831
Iteration 98/1000 | Loss: 0.00000831
Iteration 99/1000 | Loss: 0.00000831
Iteration 100/1000 | Loss: 0.00000831
Iteration 101/1000 | Loss: 0.00000831
Iteration 102/1000 | Loss: 0.00000831
Iteration 103/1000 | Loss: 0.00000831
Iteration 104/1000 | Loss: 0.00000830
Iteration 105/1000 | Loss: 0.00000830
Iteration 106/1000 | Loss: 0.00000830
Iteration 107/1000 | Loss: 0.00000829
Iteration 108/1000 | Loss: 0.00000828
Iteration 109/1000 | Loss: 0.00000828
Iteration 110/1000 | Loss: 0.00000828
Iteration 111/1000 | Loss: 0.00000828
Iteration 112/1000 | Loss: 0.00000828
Iteration 113/1000 | Loss: 0.00000828
Iteration 114/1000 | Loss: 0.00000828
Iteration 115/1000 | Loss: 0.00000828
Iteration 116/1000 | Loss: 0.00000827
Iteration 117/1000 | Loss: 0.00000827
Iteration 118/1000 | Loss: 0.00000827
Iteration 119/1000 | Loss: 0.00000826
Iteration 120/1000 | Loss: 0.00000826
Iteration 121/1000 | Loss: 0.00000826
Iteration 122/1000 | Loss: 0.00000826
Iteration 123/1000 | Loss: 0.00000826
Iteration 124/1000 | Loss: 0.00000826
Iteration 125/1000 | Loss: 0.00000825
Iteration 126/1000 | Loss: 0.00000825
Iteration 127/1000 | Loss: 0.00000825
Iteration 128/1000 | Loss: 0.00000825
Iteration 129/1000 | Loss: 0.00000825
Iteration 130/1000 | Loss: 0.00000825
Iteration 131/1000 | Loss: 0.00000825
Iteration 132/1000 | Loss: 0.00000825
Iteration 133/1000 | Loss: 0.00000825
Iteration 134/1000 | Loss: 0.00000824
Iteration 135/1000 | Loss: 0.00000824
Iteration 136/1000 | Loss: 0.00000824
Iteration 137/1000 | Loss: 0.00000824
Iteration 138/1000 | Loss: 0.00000824
Iteration 139/1000 | Loss: 0.00000824
Iteration 140/1000 | Loss: 0.00000824
Iteration 141/1000 | Loss: 0.00000824
Iteration 142/1000 | Loss: 0.00000824
Iteration 143/1000 | Loss: 0.00000824
Iteration 144/1000 | Loss: 0.00000824
Iteration 145/1000 | Loss: 0.00000824
Iteration 146/1000 | Loss: 0.00000824
Iteration 147/1000 | Loss: 0.00000824
Iteration 148/1000 | Loss: 0.00000824
Iteration 149/1000 | Loss: 0.00000824
Iteration 150/1000 | Loss: 0.00000824
Iteration 151/1000 | Loss: 0.00000824
Iteration 152/1000 | Loss: 0.00000824
Iteration 153/1000 | Loss: 0.00000824
Iteration 154/1000 | Loss: 0.00000824
Iteration 155/1000 | Loss: 0.00000824
Iteration 156/1000 | Loss: 0.00000824
Iteration 157/1000 | Loss: 0.00000824
Iteration 158/1000 | Loss: 0.00000824
Iteration 159/1000 | Loss: 0.00000824
Iteration 160/1000 | Loss: 0.00000824
Iteration 161/1000 | Loss: 0.00000824
Iteration 162/1000 | Loss: 0.00000824
Iteration 163/1000 | Loss: 0.00000824
Iteration 164/1000 | Loss: 0.00000824
Iteration 165/1000 | Loss: 0.00000824
Iteration 166/1000 | Loss: 0.00000824
Iteration 167/1000 | Loss: 0.00000824
Iteration 168/1000 | Loss: 0.00000824
Iteration 169/1000 | Loss: 0.00000824
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [8.244225682574324e-06, 8.244225682574324e-06, 8.244225682574324e-06, 8.244225682574324e-06, 8.244225682574324e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.244225682574324e-06

Optimization complete. Final v2v error: 2.4921109676361084 mm

Highest mean error: 2.9023211002349854 mm for frame 55

Lowest mean error: 2.411590814590454 mm for frame 3

Saving results

Total time: 36.17325019836426
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00411227
Iteration 2/25 | Loss: 0.00125647
Iteration 3/25 | Loss: 0.00118601
Iteration 4/25 | Loss: 0.00117867
Iteration 5/25 | Loss: 0.00117629
Iteration 6/25 | Loss: 0.00117590
Iteration 7/25 | Loss: 0.00117590
Iteration 8/25 | Loss: 0.00117590
Iteration 9/25 | Loss: 0.00117590
Iteration 10/25 | Loss: 0.00117590
Iteration 11/25 | Loss: 0.00117590
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011759005719795823, 0.0011759005719795823, 0.0011759005719795823, 0.0011759005719795823, 0.0011759005719795823]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011759005719795823

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28216004
Iteration 2/25 | Loss: 0.00184162
Iteration 3/25 | Loss: 0.00184162
Iteration 4/25 | Loss: 0.00184162
Iteration 5/25 | Loss: 0.00184162
Iteration 6/25 | Loss: 0.00184162
Iteration 7/25 | Loss: 0.00184162
Iteration 8/25 | Loss: 0.00184162
Iteration 9/25 | Loss: 0.00184162
Iteration 10/25 | Loss: 0.00184162
Iteration 11/25 | Loss: 0.00184162
Iteration 12/25 | Loss: 0.00184162
Iteration 13/25 | Loss: 0.00184162
Iteration 14/25 | Loss: 0.00184162
Iteration 15/25 | Loss: 0.00184162
Iteration 16/25 | Loss: 0.00184162
Iteration 17/25 | Loss: 0.00184162
Iteration 18/25 | Loss: 0.00184162
Iteration 19/25 | Loss: 0.00184162
Iteration 20/25 | Loss: 0.00184162
Iteration 21/25 | Loss: 0.00184162
Iteration 22/25 | Loss: 0.00184162
Iteration 23/25 | Loss: 0.00184162
Iteration 24/25 | Loss: 0.00184162
Iteration 25/25 | Loss: 0.00184162
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0018416199600324035, 0.0018416199600324035, 0.0018416199600324035, 0.0018416199600324035, 0.0018416199600324035]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018416199600324035

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00184162
Iteration 2/1000 | Loss: 0.00002838
Iteration 3/1000 | Loss: 0.00002212
Iteration 4/1000 | Loss: 0.00001939
Iteration 5/1000 | Loss: 0.00001821
Iteration 6/1000 | Loss: 0.00001745
Iteration 7/1000 | Loss: 0.00001683
Iteration 8/1000 | Loss: 0.00001639
Iteration 9/1000 | Loss: 0.00001598
Iteration 10/1000 | Loss: 0.00001571
Iteration 11/1000 | Loss: 0.00001545
Iteration 12/1000 | Loss: 0.00001524
Iteration 13/1000 | Loss: 0.00001521
Iteration 14/1000 | Loss: 0.00001501
Iteration 15/1000 | Loss: 0.00001492
Iteration 16/1000 | Loss: 0.00001492
Iteration 17/1000 | Loss: 0.00001492
Iteration 18/1000 | Loss: 0.00001492
Iteration 19/1000 | Loss: 0.00001491
Iteration 20/1000 | Loss: 0.00001488
Iteration 21/1000 | Loss: 0.00001487
Iteration 22/1000 | Loss: 0.00001487
Iteration 23/1000 | Loss: 0.00001479
Iteration 24/1000 | Loss: 0.00001478
Iteration 25/1000 | Loss: 0.00001477
Iteration 26/1000 | Loss: 0.00001475
Iteration 27/1000 | Loss: 0.00001471
Iteration 28/1000 | Loss: 0.00001471
Iteration 29/1000 | Loss: 0.00001470
Iteration 30/1000 | Loss: 0.00001469
Iteration 31/1000 | Loss: 0.00001468
Iteration 32/1000 | Loss: 0.00001467
Iteration 33/1000 | Loss: 0.00001466
Iteration 34/1000 | Loss: 0.00001464
Iteration 35/1000 | Loss: 0.00001463
Iteration 36/1000 | Loss: 0.00001463
Iteration 37/1000 | Loss: 0.00001463
Iteration 38/1000 | Loss: 0.00001462
Iteration 39/1000 | Loss: 0.00001461
Iteration 40/1000 | Loss: 0.00001461
Iteration 41/1000 | Loss: 0.00001461
Iteration 42/1000 | Loss: 0.00001460
Iteration 43/1000 | Loss: 0.00001460
Iteration 44/1000 | Loss: 0.00001460
Iteration 45/1000 | Loss: 0.00001460
Iteration 46/1000 | Loss: 0.00001459
Iteration 47/1000 | Loss: 0.00001459
Iteration 48/1000 | Loss: 0.00001459
Iteration 49/1000 | Loss: 0.00001459
Iteration 50/1000 | Loss: 0.00001459
Iteration 51/1000 | Loss: 0.00001459
Iteration 52/1000 | Loss: 0.00001459
Iteration 53/1000 | Loss: 0.00001459
Iteration 54/1000 | Loss: 0.00001459
Iteration 55/1000 | Loss: 0.00001458
Iteration 56/1000 | Loss: 0.00001458
Iteration 57/1000 | Loss: 0.00001458
Iteration 58/1000 | Loss: 0.00001458
Iteration 59/1000 | Loss: 0.00001458
Iteration 60/1000 | Loss: 0.00001457
Iteration 61/1000 | Loss: 0.00001457
Iteration 62/1000 | Loss: 0.00001456
Iteration 63/1000 | Loss: 0.00001456
Iteration 64/1000 | Loss: 0.00001456
Iteration 65/1000 | Loss: 0.00001456
Iteration 66/1000 | Loss: 0.00001455
Iteration 67/1000 | Loss: 0.00001454
Iteration 68/1000 | Loss: 0.00001454
Iteration 69/1000 | Loss: 0.00001453
Iteration 70/1000 | Loss: 0.00001453
Iteration 71/1000 | Loss: 0.00001453
Iteration 72/1000 | Loss: 0.00001453
Iteration 73/1000 | Loss: 0.00001453
Iteration 74/1000 | Loss: 0.00001453
Iteration 75/1000 | Loss: 0.00001453
Iteration 76/1000 | Loss: 0.00001453
Iteration 77/1000 | Loss: 0.00001452
Iteration 78/1000 | Loss: 0.00001452
Iteration 79/1000 | Loss: 0.00001452
Iteration 80/1000 | Loss: 0.00001452
Iteration 81/1000 | Loss: 0.00001451
Iteration 82/1000 | Loss: 0.00001451
Iteration 83/1000 | Loss: 0.00001451
Iteration 84/1000 | Loss: 0.00001451
Iteration 85/1000 | Loss: 0.00001451
Iteration 86/1000 | Loss: 0.00001450
Iteration 87/1000 | Loss: 0.00001450
Iteration 88/1000 | Loss: 0.00001450
Iteration 89/1000 | Loss: 0.00001450
Iteration 90/1000 | Loss: 0.00001450
Iteration 91/1000 | Loss: 0.00001450
Iteration 92/1000 | Loss: 0.00001450
Iteration 93/1000 | Loss: 0.00001449
Iteration 94/1000 | Loss: 0.00001449
Iteration 95/1000 | Loss: 0.00001448
Iteration 96/1000 | Loss: 0.00001448
Iteration 97/1000 | Loss: 0.00001448
Iteration 98/1000 | Loss: 0.00001448
Iteration 99/1000 | Loss: 0.00001448
Iteration 100/1000 | Loss: 0.00001448
Iteration 101/1000 | Loss: 0.00001448
Iteration 102/1000 | Loss: 0.00001448
Iteration 103/1000 | Loss: 0.00001447
Iteration 104/1000 | Loss: 0.00001447
Iteration 105/1000 | Loss: 0.00001447
Iteration 106/1000 | Loss: 0.00001447
Iteration 107/1000 | Loss: 0.00001447
Iteration 108/1000 | Loss: 0.00001447
Iteration 109/1000 | Loss: 0.00001447
Iteration 110/1000 | Loss: 0.00001447
Iteration 111/1000 | Loss: 0.00001447
Iteration 112/1000 | Loss: 0.00001447
Iteration 113/1000 | Loss: 0.00001447
Iteration 114/1000 | Loss: 0.00001447
Iteration 115/1000 | Loss: 0.00001447
Iteration 116/1000 | Loss: 0.00001447
Iteration 117/1000 | Loss: 0.00001447
Iteration 118/1000 | Loss: 0.00001447
Iteration 119/1000 | Loss: 0.00001447
Iteration 120/1000 | Loss: 0.00001447
Iteration 121/1000 | Loss: 0.00001447
Iteration 122/1000 | Loss: 0.00001447
Iteration 123/1000 | Loss: 0.00001447
Iteration 124/1000 | Loss: 0.00001447
Iteration 125/1000 | Loss: 0.00001447
Iteration 126/1000 | Loss: 0.00001447
Iteration 127/1000 | Loss: 0.00001447
Iteration 128/1000 | Loss: 0.00001447
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.4469250345428009e-05, 1.4469250345428009e-05, 1.4469250345428009e-05, 1.4469250345428009e-05, 1.4469250345428009e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4469250345428009e-05

Optimization complete. Final v2v error: 3.257972002029419 mm

Highest mean error: 3.6413016319274902 mm for frame 13

Lowest mean error: 3.013420343399048 mm for frame 33

Saving results

Total time: 37.44146156311035
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00796891
Iteration 2/25 | Loss: 0.00164967
Iteration 3/25 | Loss: 0.00138094
Iteration 4/25 | Loss: 0.00126678
Iteration 5/25 | Loss: 0.00120971
Iteration 6/25 | Loss: 0.00119391
Iteration 7/25 | Loss: 0.00118658
Iteration 8/25 | Loss: 0.00118730
Iteration 9/25 | Loss: 0.00118346
Iteration 10/25 | Loss: 0.00118762
Iteration 11/25 | Loss: 0.00118324
Iteration 12/25 | Loss: 0.00118160
Iteration 13/25 | Loss: 0.00118157
Iteration 14/25 | Loss: 0.00118157
Iteration 15/25 | Loss: 0.00118157
Iteration 16/25 | Loss: 0.00118157
Iteration 17/25 | Loss: 0.00118157
Iteration 18/25 | Loss: 0.00118157
Iteration 19/25 | Loss: 0.00118156
Iteration 20/25 | Loss: 0.00118156
Iteration 21/25 | Loss: 0.00118156
Iteration 22/25 | Loss: 0.00118156
Iteration 23/25 | Loss: 0.00118156
Iteration 24/25 | Loss: 0.00118156
Iteration 25/25 | Loss: 0.00118155

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.72597694
Iteration 2/25 | Loss: 0.00229743
Iteration 3/25 | Loss: 0.00229743
Iteration 4/25 | Loss: 0.00197638
Iteration 5/25 | Loss: 0.00197638
Iteration 6/25 | Loss: 0.00197638
Iteration 7/25 | Loss: 0.00201143
Iteration 8/25 | Loss: 0.00201143
Iteration 9/25 | Loss: 0.00201143
Iteration 10/25 | Loss: 0.00201143
Iteration 11/25 | Loss: 0.00201143
Iteration 12/25 | Loss: 0.00201143
Iteration 13/25 | Loss: 0.00201143
Iteration 14/25 | Loss: 0.00201143
Iteration 15/25 | Loss: 0.00201143
Iteration 16/25 | Loss: 0.00201143
Iteration 17/25 | Loss: 0.00201143
Iteration 18/25 | Loss: 0.00201143
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0020114292856305838, 0.0020114292856305838, 0.0020114292856305838, 0.0020114292856305838, 0.0020114292856305838]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020114292856305838

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00201143
Iteration 2/1000 | Loss: 0.00014614
Iteration 3/1000 | Loss: 0.00035153
Iteration 4/1000 | Loss: 0.00245720
Iteration 5/1000 | Loss: 0.00006513
Iteration 6/1000 | Loss: 0.00007415
Iteration 7/1000 | Loss: 0.00050842
Iteration 8/1000 | Loss: 0.00046968
Iteration 9/1000 | Loss: 0.00002335
Iteration 10/1000 | Loss: 0.00002876
Iteration 11/1000 | Loss: 0.00001808
Iteration 12/1000 | Loss: 0.00001765
Iteration 13/1000 | Loss: 0.00001710
Iteration 14/1000 | Loss: 0.00001673
Iteration 15/1000 | Loss: 0.00012049
Iteration 16/1000 | Loss: 0.00006698
Iteration 17/1000 | Loss: 0.00001632
Iteration 18/1000 | Loss: 0.00005272
Iteration 19/1000 | Loss: 0.00003343
Iteration 20/1000 | Loss: 0.00006112
Iteration 21/1000 | Loss: 0.00001604
Iteration 22/1000 | Loss: 0.00001595
Iteration 23/1000 | Loss: 0.00003407
Iteration 24/1000 | Loss: 0.00001585
Iteration 25/1000 | Loss: 0.00004661
Iteration 26/1000 | Loss: 0.00001616
Iteration 27/1000 | Loss: 0.00006664
Iteration 28/1000 | Loss: 0.00001568
Iteration 29/1000 | Loss: 0.00001560
Iteration 30/1000 | Loss: 0.00001560
Iteration 31/1000 | Loss: 0.00001555
Iteration 32/1000 | Loss: 0.00001555
Iteration 33/1000 | Loss: 0.00001555
Iteration 34/1000 | Loss: 0.00001555
Iteration 35/1000 | Loss: 0.00001553
Iteration 36/1000 | Loss: 0.00001553
Iteration 37/1000 | Loss: 0.00001552
Iteration 38/1000 | Loss: 0.00001552
Iteration 39/1000 | Loss: 0.00001551
Iteration 40/1000 | Loss: 0.00001551
Iteration 41/1000 | Loss: 0.00001551
Iteration 42/1000 | Loss: 0.00001551
Iteration 43/1000 | Loss: 0.00001551
Iteration 44/1000 | Loss: 0.00001551
Iteration 45/1000 | Loss: 0.00001551
Iteration 46/1000 | Loss: 0.00001551
Iteration 47/1000 | Loss: 0.00001550
Iteration 48/1000 | Loss: 0.00001550
Iteration 49/1000 | Loss: 0.00001549
Iteration 50/1000 | Loss: 0.00001548
Iteration 51/1000 | Loss: 0.00001548
Iteration 52/1000 | Loss: 0.00001547
Iteration 53/1000 | Loss: 0.00001546
Iteration 54/1000 | Loss: 0.00001546
Iteration 55/1000 | Loss: 0.00001545
Iteration 56/1000 | Loss: 0.00001544
Iteration 57/1000 | Loss: 0.00001544
Iteration 58/1000 | Loss: 0.00001543
Iteration 59/1000 | Loss: 0.00001543
Iteration 60/1000 | Loss: 0.00001543
Iteration 61/1000 | Loss: 0.00001543
Iteration 62/1000 | Loss: 0.00001543
Iteration 63/1000 | Loss: 0.00001543
Iteration 64/1000 | Loss: 0.00001543
Iteration 65/1000 | Loss: 0.00001543
Iteration 66/1000 | Loss: 0.00001543
Iteration 67/1000 | Loss: 0.00001542
Iteration 68/1000 | Loss: 0.00001542
Iteration 69/1000 | Loss: 0.00001542
Iteration 70/1000 | Loss: 0.00001542
Iteration 71/1000 | Loss: 0.00001542
Iteration 72/1000 | Loss: 0.00001542
Iteration 73/1000 | Loss: 0.00001542
Iteration 74/1000 | Loss: 0.00001541
Iteration 75/1000 | Loss: 0.00001541
Iteration 76/1000 | Loss: 0.00001541
Iteration 77/1000 | Loss: 0.00001540
Iteration 78/1000 | Loss: 0.00001540
Iteration 79/1000 | Loss: 0.00001540
Iteration 80/1000 | Loss: 0.00001540
Iteration 81/1000 | Loss: 0.00001540
Iteration 82/1000 | Loss: 0.00001540
Iteration 83/1000 | Loss: 0.00001540
Iteration 84/1000 | Loss: 0.00001540
Iteration 85/1000 | Loss: 0.00001540
Iteration 86/1000 | Loss: 0.00001539
Iteration 87/1000 | Loss: 0.00001539
Iteration 88/1000 | Loss: 0.00001539
Iteration 89/1000 | Loss: 0.00001539
Iteration 90/1000 | Loss: 0.00001539
Iteration 91/1000 | Loss: 0.00001539
Iteration 92/1000 | Loss: 0.00001538
Iteration 93/1000 | Loss: 0.00001538
Iteration 94/1000 | Loss: 0.00001538
Iteration 95/1000 | Loss: 0.00001538
Iteration 96/1000 | Loss: 0.00001538
Iteration 97/1000 | Loss: 0.00001538
Iteration 98/1000 | Loss: 0.00001538
Iteration 99/1000 | Loss: 0.00001538
Iteration 100/1000 | Loss: 0.00001537
Iteration 101/1000 | Loss: 0.00001537
Iteration 102/1000 | Loss: 0.00001537
Iteration 103/1000 | Loss: 0.00001537
Iteration 104/1000 | Loss: 0.00001537
Iteration 105/1000 | Loss: 0.00001537
Iteration 106/1000 | Loss: 0.00001536
Iteration 107/1000 | Loss: 0.00001536
Iteration 108/1000 | Loss: 0.00001536
Iteration 109/1000 | Loss: 0.00001536
Iteration 110/1000 | Loss: 0.00001536
Iteration 111/1000 | Loss: 0.00001536
Iteration 112/1000 | Loss: 0.00001536
Iteration 113/1000 | Loss: 0.00001535
Iteration 114/1000 | Loss: 0.00001535
Iteration 115/1000 | Loss: 0.00001535
Iteration 116/1000 | Loss: 0.00001535
Iteration 117/1000 | Loss: 0.00001535
Iteration 118/1000 | Loss: 0.00001534
Iteration 119/1000 | Loss: 0.00001534
Iteration 120/1000 | Loss: 0.00001534
Iteration 121/1000 | Loss: 0.00001534
Iteration 122/1000 | Loss: 0.00001534
Iteration 123/1000 | Loss: 0.00001533
Iteration 124/1000 | Loss: 0.00001533
Iteration 125/1000 | Loss: 0.00001533
Iteration 126/1000 | Loss: 0.00001533
Iteration 127/1000 | Loss: 0.00001533
Iteration 128/1000 | Loss: 0.00001533
Iteration 129/1000 | Loss: 0.00001533
Iteration 130/1000 | Loss: 0.00001533
Iteration 131/1000 | Loss: 0.00001533
Iteration 132/1000 | Loss: 0.00001533
Iteration 133/1000 | Loss: 0.00001532
Iteration 134/1000 | Loss: 0.00001532
Iteration 135/1000 | Loss: 0.00001532
Iteration 136/1000 | Loss: 0.00001532
Iteration 137/1000 | Loss: 0.00001532
Iteration 138/1000 | Loss: 0.00001532
Iteration 139/1000 | Loss: 0.00001532
Iteration 140/1000 | Loss: 0.00001532
Iteration 141/1000 | Loss: 0.00001532
Iteration 142/1000 | Loss: 0.00001532
Iteration 143/1000 | Loss: 0.00001531
Iteration 144/1000 | Loss: 0.00001531
Iteration 145/1000 | Loss: 0.00001531
Iteration 146/1000 | Loss: 0.00001531
Iteration 147/1000 | Loss: 0.00001531
Iteration 148/1000 | Loss: 0.00001531
Iteration 149/1000 | Loss: 0.00001530
Iteration 150/1000 | Loss: 0.00001530
Iteration 151/1000 | Loss: 0.00001530
Iteration 152/1000 | Loss: 0.00001530
Iteration 153/1000 | Loss: 0.00001529
Iteration 154/1000 | Loss: 0.00001529
Iteration 155/1000 | Loss: 0.00001529
Iteration 156/1000 | Loss: 0.00001529
Iteration 157/1000 | Loss: 0.00001529
Iteration 158/1000 | Loss: 0.00001528
Iteration 159/1000 | Loss: 0.00001528
Iteration 160/1000 | Loss: 0.00001528
Iteration 161/1000 | Loss: 0.00001528
Iteration 162/1000 | Loss: 0.00001528
Iteration 163/1000 | Loss: 0.00001528
Iteration 164/1000 | Loss: 0.00001528
Iteration 165/1000 | Loss: 0.00001528
Iteration 166/1000 | Loss: 0.00001528
Iteration 167/1000 | Loss: 0.00001528
Iteration 168/1000 | Loss: 0.00001527
Iteration 169/1000 | Loss: 0.00001527
Iteration 170/1000 | Loss: 0.00001527
Iteration 171/1000 | Loss: 0.00001527
Iteration 172/1000 | Loss: 0.00001527
Iteration 173/1000 | Loss: 0.00001527
Iteration 174/1000 | Loss: 0.00006998
Iteration 175/1000 | Loss: 0.00007727
Iteration 176/1000 | Loss: 0.00003130
Iteration 177/1000 | Loss: 0.00005801
Iteration 178/1000 | Loss: 0.00001553
Iteration 179/1000 | Loss: 0.00007837
Iteration 180/1000 | Loss: 0.00003948
Iteration 181/1000 | Loss: 0.00002753
Iteration 182/1000 | Loss: 0.00001548
Iteration 183/1000 | Loss: 0.00001532
Iteration 184/1000 | Loss: 0.00003185
Iteration 185/1000 | Loss: 0.00001618
Iteration 186/1000 | Loss: 0.00001593
Iteration 187/1000 | Loss: 0.00001532
Iteration 188/1000 | Loss: 0.00001531
Iteration 189/1000 | Loss: 0.00001531
Iteration 190/1000 | Loss: 0.00001531
Iteration 191/1000 | Loss: 0.00001531
Iteration 192/1000 | Loss: 0.00001531
Iteration 193/1000 | Loss: 0.00001531
Iteration 194/1000 | Loss: 0.00001530
Iteration 195/1000 | Loss: 0.00001530
Iteration 196/1000 | Loss: 0.00001530
Iteration 197/1000 | Loss: 0.00001530
Iteration 198/1000 | Loss: 0.00001530
Iteration 199/1000 | Loss: 0.00001530
Iteration 200/1000 | Loss: 0.00001530
Iteration 201/1000 | Loss: 0.00001530
Iteration 202/1000 | Loss: 0.00001530
Iteration 203/1000 | Loss: 0.00001530
Iteration 204/1000 | Loss: 0.00001529
Iteration 205/1000 | Loss: 0.00001529
Iteration 206/1000 | Loss: 0.00001529
Iteration 207/1000 | Loss: 0.00001529
Iteration 208/1000 | Loss: 0.00001529
Iteration 209/1000 | Loss: 0.00001529
Iteration 210/1000 | Loss: 0.00001528
Iteration 211/1000 | Loss: 0.00001528
Iteration 212/1000 | Loss: 0.00001528
Iteration 213/1000 | Loss: 0.00001527
Iteration 214/1000 | Loss: 0.00001527
Iteration 215/1000 | Loss: 0.00001527
Iteration 216/1000 | Loss: 0.00001526
Iteration 217/1000 | Loss: 0.00001526
Iteration 218/1000 | Loss: 0.00001526
Iteration 219/1000 | Loss: 0.00001526
Iteration 220/1000 | Loss: 0.00001526
Iteration 221/1000 | Loss: 0.00001526
Iteration 222/1000 | Loss: 0.00001526
Iteration 223/1000 | Loss: 0.00001525
Iteration 224/1000 | Loss: 0.00001525
Iteration 225/1000 | Loss: 0.00001525
Iteration 226/1000 | Loss: 0.00001525
Iteration 227/1000 | Loss: 0.00001525
Iteration 228/1000 | Loss: 0.00001525
Iteration 229/1000 | Loss: 0.00001525
Iteration 230/1000 | Loss: 0.00001525
Iteration 231/1000 | Loss: 0.00001525
Iteration 232/1000 | Loss: 0.00001525
Iteration 233/1000 | Loss: 0.00001525
Iteration 234/1000 | Loss: 0.00001525
Iteration 235/1000 | Loss: 0.00001525
Iteration 236/1000 | Loss: 0.00001525
Iteration 237/1000 | Loss: 0.00001525
Iteration 238/1000 | Loss: 0.00001525
Iteration 239/1000 | Loss: 0.00001525
Iteration 240/1000 | Loss: 0.00001525
Iteration 241/1000 | Loss: 0.00001525
Iteration 242/1000 | Loss: 0.00001525
Iteration 243/1000 | Loss: 0.00001525
Iteration 244/1000 | Loss: 0.00001525
Iteration 245/1000 | Loss: 0.00001525
Iteration 246/1000 | Loss: 0.00001525
Iteration 247/1000 | Loss: 0.00001525
Iteration 248/1000 | Loss: 0.00001525
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 248. Stopping optimization.
Last 5 losses: [1.5250214346451685e-05, 1.5250214346451685e-05, 1.5250214346451685e-05, 1.5250214346451685e-05, 1.5250214346451685e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5250214346451685e-05

Optimization complete. Final v2v error: 3.3238203525543213 mm

Highest mean error: 3.853614091873169 mm for frame 147

Lowest mean error: 2.9796853065490723 mm for frame 136

Saving results

Total time: 90.55563926696777
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00608465
Iteration 2/25 | Loss: 0.00120144
Iteration 3/25 | Loss: 0.00113417
Iteration 4/25 | Loss: 0.00112245
Iteration 5/25 | Loss: 0.00111890
Iteration 6/25 | Loss: 0.00111847
Iteration 7/25 | Loss: 0.00111847
Iteration 8/25 | Loss: 0.00111847
Iteration 9/25 | Loss: 0.00111847
Iteration 10/25 | Loss: 0.00111847
Iteration 11/25 | Loss: 0.00111847
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001118472428061068, 0.001118472428061068, 0.001118472428061068, 0.001118472428061068, 0.001118472428061068]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001118472428061068

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.52880883
Iteration 2/25 | Loss: 0.00181982
Iteration 3/25 | Loss: 0.00181982
Iteration 4/25 | Loss: 0.00181982
Iteration 5/25 | Loss: 0.00181982
Iteration 6/25 | Loss: 0.00181982
Iteration 7/25 | Loss: 0.00181982
Iteration 8/25 | Loss: 0.00181982
Iteration 9/25 | Loss: 0.00181982
Iteration 10/25 | Loss: 0.00181982
Iteration 11/25 | Loss: 0.00181982
Iteration 12/25 | Loss: 0.00181982
Iteration 13/25 | Loss: 0.00181982
Iteration 14/25 | Loss: 0.00181982
Iteration 15/25 | Loss: 0.00181982
Iteration 16/25 | Loss: 0.00181982
Iteration 17/25 | Loss: 0.00181982
Iteration 18/25 | Loss: 0.00181982
Iteration 19/25 | Loss: 0.00181982
Iteration 20/25 | Loss: 0.00181982
Iteration 21/25 | Loss: 0.00181982
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0018198214238509536, 0.0018198214238509536, 0.0018198214238509536, 0.0018198214238509536, 0.0018198214238509536]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018198214238509536

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00181982
Iteration 2/1000 | Loss: 0.00002184
Iteration 3/1000 | Loss: 0.00001626
Iteration 4/1000 | Loss: 0.00001437
Iteration 5/1000 | Loss: 0.00001307
Iteration 6/1000 | Loss: 0.00001229
Iteration 7/1000 | Loss: 0.00001176
Iteration 8/1000 | Loss: 0.00001135
Iteration 9/1000 | Loss: 0.00001101
Iteration 10/1000 | Loss: 0.00001066
Iteration 11/1000 | Loss: 0.00001049
Iteration 12/1000 | Loss: 0.00001046
Iteration 13/1000 | Loss: 0.00001032
Iteration 14/1000 | Loss: 0.00001019
Iteration 15/1000 | Loss: 0.00001011
Iteration 16/1000 | Loss: 0.00001003
Iteration 17/1000 | Loss: 0.00001001
Iteration 18/1000 | Loss: 0.00001001
Iteration 19/1000 | Loss: 0.00001000
Iteration 20/1000 | Loss: 0.00000996
Iteration 21/1000 | Loss: 0.00000982
Iteration 22/1000 | Loss: 0.00000980
Iteration 23/1000 | Loss: 0.00000976
Iteration 24/1000 | Loss: 0.00000973
Iteration 25/1000 | Loss: 0.00000972
Iteration 26/1000 | Loss: 0.00000964
Iteration 27/1000 | Loss: 0.00000962
Iteration 28/1000 | Loss: 0.00000962
Iteration 29/1000 | Loss: 0.00000962
Iteration 30/1000 | Loss: 0.00000960
Iteration 31/1000 | Loss: 0.00000960
Iteration 32/1000 | Loss: 0.00000960
Iteration 33/1000 | Loss: 0.00000959
Iteration 34/1000 | Loss: 0.00000958
Iteration 35/1000 | Loss: 0.00000958
Iteration 36/1000 | Loss: 0.00000957
Iteration 37/1000 | Loss: 0.00000957
Iteration 38/1000 | Loss: 0.00000957
Iteration 39/1000 | Loss: 0.00000957
Iteration 40/1000 | Loss: 0.00000956
Iteration 41/1000 | Loss: 0.00000956
Iteration 42/1000 | Loss: 0.00000956
Iteration 43/1000 | Loss: 0.00000956
Iteration 44/1000 | Loss: 0.00000956
Iteration 45/1000 | Loss: 0.00000955
Iteration 46/1000 | Loss: 0.00000955
Iteration 47/1000 | Loss: 0.00000955
Iteration 48/1000 | Loss: 0.00000955
Iteration 49/1000 | Loss: 0.00000954
Iteration 50/1000 | Loss: 0.00000954
Iteration 51/1000 | Loss: 0.00000954
Iteration 52/1000 | Loss: 0.00000954
Iteration 53/1000 | Loss: 0.00000954
Iteration 54/1000 | Loss: 0.00000954
Iteration 55/1000 | Loss: 0.00000954
Iteration 56/1000 | Loss: 0.00000954
Iteration 57/1000 | Loss: 0.00000954
Iteration 58/1000 | Loss: 0.00000954
Iteration 59/1000 | Loss: 0.00000954
Iteration 60/1000 | Loss: 0.00000954
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 60. Stopping optimization.
Last 5 losses: [9.535589924780652e-06, 9.535589924780652e-06, 9.535589924780652e-06, 9.535589924780652e-06, 9.535589924780652e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.535589924780652e-06

Optimization complete. Final v2v error: 2.7074995040893555 mm

Highest mean error: 3.01784610748291 mm for frame 54

Lowest mean error: 2.536159038543701 mm for frame 158

Saving results

Total time: 34.530539989471436
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01014154
Iteration 2/25 | Loss: 0.00186912
Iteration 3/25 | Loss: 0.00150178
Iteration 4/25 | Loss: 0.00138536
Iteration 5/25 | Loss: 0.00133252
Iteration 6/25 | Loss: 0.00132138
Iteration 7/25 | Loss: 0.00131284
Iteration 8/25 | Loss: 0.00128942
Iteration 9/25 | Loss: 0.00127171
Iteration 10/25 | Loss: 0.00126771
Iteration 11/25 | Loss: 0.00126332
Iteration 12/25 | Loss: 0.00125645
Iteration 13/25 | Loss: 0.00125463
Iteration 14/25 | Loss: 0.00125267
Iteration 15/25 | Loss: 0.00126772
Iteration 16/25 | Loss: 0.00125402
Iteration 17/25 | Loss: 0.00125086
Iteration 18/25 | Loss: 0.00124069
Iteration 19/25 | Loss: 0.00122890
Iteration 20/25 | Loss: 0.00121935
Iteration 21/25 | Loss: 0.00122322
Iteration 22/25 | Loss: 0.00122880
Iteration 23/25 | Loss: 0.00122407
Iteration 24/25 | Loss: 0.00121980
Iteration 25/25 | Loss: 0.00122228

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25951850
Iteration 2/25 | Loss: 0.00310135
Iteration 3/25 | Loss: 0.00190190
Iteration 4/25 | Loss: 0.00190189
Iteration 5/25 | Loss: 0.00190189
Iteration 6/25 | Loss: 0.00190189
Iteration 7/25 | Loss: 0.00190189
Iteration 8/25 | Loss: 0.00190189
Iteration 9/25 | Loss: 0.00190189
Iteration 10/25 | Loss: 0.00190189
Iteration 11/25 | Loss: 0.00190189
Iteration 12/25 | Loss: 0.00190189
Iteration 13/25 | Loss: 0.00190189
Iteration 14/25 | Loss: 0.00190189
Iteration 15/25 | Loss: 0.00190189
Iteration 16/25 | Loss: 0.00190189
Iteration 17/25 | Loss: 0.00190189
Iteration 18/25 | Loss: 0.00190189
Iteration 19/25 | Loss: 0.00190189
Iteration 20/25 | Loss: 0.00190189
Iteration 21/25 | Loss: 0.00190189
Iteration 22/25 | Loss: 0.00190189
Iteration 23/25 | Loss: 0.00190189
Iteration 24/25 | Loss: 0.00190189
Iteration 25/25 | Loss: 0.00190189

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00190189
Iteration 2/1000 | Loss: 0.00050857
Iteration 3/1000 | Loss: 0.00176569
Iteration 4/1000 | Loss: 0.00038831
Iteration 5/1000 | Loss: 0.00010617
Iteration 6/1000 | Loss: 0.00059445
Iteration 7/1000 | Loss: 0.00047111
Iteration 8/1000 | Loss: 0.00029641
Iteration 9/1000 | Loss: 0.00066873
Iteration 10/1000 | Loss: 0.00070128
Iteration 11/1000 | Loss: 0.00069937
Iteration 12/1000 | Loss: 0.00139915
Iteration 13/1000 | Loss: 0.00117763
Iteration 14/1000 | Loss: 0.00123307
Iteration 15/1000 | Loss: 0.00091000
Iteration 16/1000 | Loss: 0.00034620
Iteration 17/1000 | Loss: 0.00084854
Iteration 18/1000 | Loss: 0.00033989
Iteration 19/1000 | Loss: 0.00043404
Iteration 20/1000 | Loss: 0.00016819
Iteration 21/1000 | Loss: 0.00034739
Iteration 22/1000 | Loss: 0.00027401
Iteration 23/1000 | Loss: 0.00021321
Iteration 24/1000 | Loss: 0.00020397
Iteration 25/1000 | Loss: 0.00078290
Iteration 26/1000 | Loss: 0.00023985
Iteration 27/1000 | Loss: 0.00027215
Iteration 28/1000 | Loss: 0.00019942
Iteration 29/1000 | Loss: 0.00018996
Iteration 30/1000 | Loss: 0.00016706
Iteration 31/1000 | Loss: 0.00018012
Iteration 32/1000 | Loss: 0.00016981
Iteration 33/1000 | Loss: 0.00009395
Iteration 34/1000 | Loss: 0.00015058
Iteration 35/1000 | Loss: 0.00028828
Iteration 36/1000 | Loss: 0.00027913
Iteration 37/1000 | Loss: 0.00024584
Iteration 38/1000 | Loss: 0.00022816
Iteration 39/1000 | Loss: 0.00029602
Iteration 40/1000 | Loss: 0.00021945
Iteration 41/1000 | Loss: 0.00026563
Iteration 42/1000 | Loss: 0.00021849
Iteration 43/1000 | Loss: 0.00039170
Iteration 44/1000 | Loss: 0.00032634
Iteration 45/1000 | Loss: 0.00031004
Iteration 46/1000 | Loss: 0.00019956
Iteration 47/1000 | Loss: 0.00015981
Iteration 48/1000 | Loss: 0.00009329
Iteration 49/1000 | Loss: 0.00033202
Iteration 50/1000 | Loss: 0.00029272
Iteration 51/1000 | Loss: 0.00017361
Iteration 52/1000 | Loss: 0.00019061
Iteration 53/1000 | Loss: 0.00024357
Iteration 54/1000 | Loss: 0.00021454
Iteration 55/1000 | Loss: 0.00030156
Iteration 56/1000 | Loss: 0.00023718
Iteration 57/1000 | Loss: 0.00023417
Iteration 58/1000 | Loss: 0.00044687
Iteration 59/1000 | Loss: 0.00025437
Iteration 60/1000 | Loss: 0.00014512
Iteration 61/1000 | Loss: 0.00013625
Iteration 62/1000 | Loss: 0.00013587
Iteration 63/1000 | Loss: 0.00034816
Iteration 64/1000 | Loss: 0.00045045
Iteration 65/1000 | Loss: 0.00022655
Iteration 66/1000 | Loss: 0.00027360
Iteration 67/1000 | Loss: 0.00024756
Iteration 68/1000 | Loss: 0.00033398
Iteration 69/1000 | Loss: 0.00024450
Iteration 70/1000 | Loss: 0.00006283
Iteration 71/1000 | Loss: 0.00003956
Iteration 72/1000 | Loss: 0.00002954
Iteration 73/1000 | Loss: 0.00003272
Iteration 74/1000 | Loss: 0.00003475
Iteration 75/1000 | Loss: 0.00003578
Iteration 76/1000 | Loss: 0.00003341
Iteration 77/1000 | Loss: 0.00003748
Iteration 78/1000 | Loss: 0.00003032
Iteration 79/1000 | Loss: 0.00040764
Iteration 80/1000 | Loss: 0.00003712
Iteration 81/1000 | Loss: 0.00002278
Iteration 82/1000 | Loss: 0.00003287
Iteration 83/1000 | Loss: 0.00003875
Iteration 84/1000 | Loss: 0.00003394
Iteration 85/1000 | Loss: 0.00003505
Iteration 86/1000 | Loss: 0.00003285
Iteration 87/1000 | Loss: 0.00003489
Iteration 88/1000 | Loss: 0.00003128
Iteration 89/1000 | Loss: 0.00003321
Iteration 90/1000 | Loss: 0.00003352
Iteration 91/1000 | Loss: 0.00003013
Iteration 92/1000 | Loss: 0.00003653
Iteration 93/1000 | Loss: 0.00003008
Iteration 94/1000 | Loss: 0.00003544
Iteration 95/1000 | Loss: 0.00003418
Iteration 96/1000 | Loss: 0.00003190
Iteration 97/1000 | Loss: 0.00003380
Iteration 98/1000 | Loss: 0.00003196
Iteration 99/1000 | Loss: 0.00003312
Iteration 100/1000 | Loss: 0.00003129
Iteration 101/1000 | Loss: 0.00003303
Iteration 102/1000 | Loss: 0.00003165
Iteration 103/1000 | Loss: 0.00003248
Iteration 104/1000 | Loss: 0.00002235
Iteration 105/1000 | Loss: 0.00003237
Iteration 106/1000 | Loss: 0.00004892
Iteration 107/1000 | Loss: 0.00004677
Iteration 108/1000 | Loss: 0.00004520
Iteration 109/1000 | Loss: 0.00004723
Iteration 110/1000 | Loss: 0.00002141
Iteration 111/1000 | Loss: 0.00001609
Iteration 112/1000 | Loss: 0.00001490
Iteration 113/1000 | Loss: 0.00001397
Iteration 114/1000 | Loss: 0.00001348
Iteration 115/1000 | Loss: 0.00001304
Iteration 116/1000 | Loss: 0.00001284
Iteration 117/1000 | Loss: 0.00001282
Iteration 118/1000 | Loss: 0.00001271
Iteration 119/1000 | Loss: 0.00001258
Iteration 120/1000 | Loss: 0.00001255
Iteration 121/1000 | Loss: 0.00001250
Iteration 122/1000 | Loss: 0.00001250
Iteration 123/1000 | Loss: 0.00001249
Iteration 124/1000 | Loss: 0.00001249
Iteration 125/1000 | Loss: 0.00001246
Iteration 126/1000 | Loss: 0.00001246
Iteration 127/1000 | Loss: 0.00001245
Iteration 128/1000 | Loss: 0.00001244
Iteration 129/1000 | Loss: 0.00001242
Iteration 130/1000 | Loss: 0.00001241
Iteration 131/1000 | Loss: 0.00001241
Iteration 132/1000 | Loss: 0.00001240
Iteration 133/1000 | Loss: 0.00001240
Iteration 134/1000 | Loss: 0.00001240
Iteration 135/1000 | Loss: 0.00001240
Iteration 136/1000 | Loss: 0.00001239
Iteration 137/1000 | Loss: 0.00001239
Iteration 138/1000 | Loss: 0.00001239
Iteration 139/1000 | Loss: 0.00001238
Iteration 140/1000 | Loss: 0.00001238
Iteration 141/1000 | Loss: 0.00001238
Iteration 142/1000 | Loss: 0.00001238
Iteration 143/1000 | Loss: 0.00001237
Iteration 144/1000 | Loss: 0.00001237
Iteration 145/1000 | Loss: 0.00001237
Iteration 146/1000 | Loss: 0.00001237
Iteration 147/1000 | Loss: 0.00001236
Iteration 148/1000 | Loss: 0.00001236
Iteration 149/1000 | Loss: 0.00001235
Iteration 150/1000 | Loss: 0.00001234
Iteration 151/1000 | Loss: 0.00001234
Iteration 152/1000 | Loss: 0.00001233
Iteration 153/1000 | Loss: 0.00001232
Iteration 154/1000 | Loss: 0.00001232
Iteration 155/1000 | Loss: 0.00001231
Iteration 156/1000 | Loss: 0.00001229
Iteration 157/1000 | Loss: 0.00001229
Iteration 158/1000 | Loss: 0.00001229
Iteration 159/1000 | Loss: 0.00001228
Iteration 160/1000 | Loss: 0.00001227
Iteration 161/1000 | Loss: 0.00001227
Iteration 162/1000 | Loss: 0.00001227
Iteration 163/1000 | Loss: 0.00001226
Iteration 164/1000 | Loss: 0.00001226
Iteration 165/1000 | Loss: 0.00001222
Iteration 166/1000 | Loss: 0.00001221
Iteration 167/1000 | Loss: 0.00001219
Iteration 168/1000 | Loss: 0.00001218
Iteration 169/1000 | Loss: 0.00001218
Iteration 170/1000 | Loss: 0.00001218
Iteration 171/1000 | Loss: 0.00001217
Iteration 172/1000 | Loss: 0.00001217
Iteration 173/1000 | Loss: 0.00001217
Iteration 174/1000 | Loss: 0.00001217
Iteration 175/1000 | Loss: 0.00001217
Iteration 176/1000 | Loss: 0.00001217
Iteration 177/1000 | Loss: 0.00001217
Iteration 178/1000 | Loss: 0.00001216
Iteration 179/1000 | Loss: 0.00001216
Iteration 180/1000 | Loss: 0.00001216
Iteration 181/1000 | Loss: 0.00001215
Iteration 182/1000 | Loss: 0.00001215
Iteration 183/1000 | Loss: 0.00001215
Iteration 184/1000 | Loss: 0.00001215
Iteration 185/1000 | Loss: 0.00001214
Iteration 186/1000 | Loss: 0.00001214
Iteration 187/1000 | Loss: 0.00001214
Iteration 188/1000 | Loss: 0.00001214
Iteration 189/1000 | Loss: 0.00001213
Iteration 190/1000 | Loss: 0.00001213
Iteration 191/1000 | Loss: 0.00001212
Iteration 192/1000 | Loss: 0.00001212
Iteration 193/1000 | Loss: 0.00001212
Iteration 194/1000 | Loss: 0.00001212
Iteration 195/1000 | Loss: 0.00001212
Iteration 196/1000 | Loss: 0.00001212
Iteration 197/1000 | Loss: 0.00001212
Iteration 198/1000 | Loss: 0.00001212
Iteration 199/1000 | Loss: 0.00001212
Iteration 200/1000 | Loss: 0.00001211
Iteration 201/1000 | Loss: 0.00001211
Iteration 202/1000 | Loss: 0.00001211
Iteration 203/1000 | Loss: 0.00001211
Iteration 204/1000 | Loss: 0.00001211
Iteration 205/1000 | Loss: 0.00001211
Iteration 206/1000 | Loss: 0.00001211
Iteration 207/1000 | Loss: 0.00001210
Iteration 208/1000 | Loss: 0.00001210
Iteration 209/1000 | Loss: 0.00001210
Iteration 210/1000 | Loss: 0.00001210
Iteration 211/1000 | Loss: 0.00001209
Iteration 212/1000 | Loss: 0.00001209
Iteration 213/1000 | Loss: 0.00001209
Iteration 214/1000 | Loss: 0.00001209
Iteration 215/1000 | Loss: 0.00001209
Iteration 216/1000 | Loss: 0.00001209
Iteration 217/1000 | Loss: 0.00001209
Iteration 218/1000 | Loss: 0.00001209
Iteration 219/1000 | Loss: 0.00001209
Iteration 220/1000 | Loss: 0.00001208
Iteration 221/1000 | Loss: 0.00001208
Iteration 222/1000 | Loss: 0.00001208
Iteration 223/1000 | Loss: 0.00001208
Iteration 224/1000 | Loss: 0.00001208
Iteration 225/1000 | Loss: 0.00001208
Iteration 226/1000 | Loss: 0.00001208
Iteration 227/1000 | Loss: 0.00001208
Iteration 228/1000 | Loss: 0.00001208
Iteration 229/1000 | Loss: 0.00001208
Iteration 230/1000 | Loss: 0.00001208
Iteration 231/1000 | Loss: 0.00001208
Iteration 232/1000 | Loss: 0.00001208
Iteration 233/1000 | Loss: 0.00001208
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 233. Stopping optimization.
Last 5 losses: [1.2083866749890149e-05, 1.2083866749890149e-05, 1.2083866749890149e-05, 1.2083866749890149e-05, 1.2083866749890149e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2083866749890149e-05

Optimization complete. Final v2v error: 2.8827438354492188 mm

Highest mean error: 4.570762634277344 mm for frame 67

Lowest mean error: 2.4180784225463867 mm for frame 3

Saving results

Total time: 221.1595380306244
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01070868
Iteration 2/25 | Loss: 0.00184810
Iteration 3/25 | Loss: 0.00143585
Iteration 4/25 | Loss: 0.00138698
Iteration 5/25 | Loss: 0.00137102
Iteration 6/25 | Loss: 0.00137814
Iteration 7/25 | Loss: 0.00136030
Iteration 8/25 | Loss: 0.00132913
Iteration 9/25 | Loss: 0.00132111
Iteration 10/25 | Loss: 0.00131166
Iteration 11/25 | Loss: 0.00130227
Iteration 12/25 | Loss: 0.00129898
Iteration 13/25 | Loss: 0.00129593
Iteration 14/25 | Loss: 0.00129883
Iteration 15/25 | Loss: 0.00129350
Iteration 16/25 | Loss: 0.00129363
Iteration 17/25 | Loss: 0.00129621
Iteration 18/25 | Loss: 0.00129150
Iteration 19/25 | Loss: 0.00129112
Iteration 20/25 | Loss: 0.00129101
Iteration 21/25 | Loss: 0.00129101
Iteration 22/25 | Loss: 0.00129101
Iteration 23/25 | Loss: 0.00129101
Iteration 24/25 | Loss: 0.00129100
Iteration 25/25 | Loss: 0.00129100

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.64861393
Iteration 2/25 | Loss: 0.00159529
Iteration 3/25 | Loss: 0.00159529
Iteration 4/25 | Loss: 0.00159529
Iteration 5/25 | Loss: 0.00159529
Iteration 6/25 | Loss: 0.00159529
Iteration 7/25 | Loss: 0.00159529
Iteration 8/25 | Loss: 0.00159529
Iteration 9/25 | Loss: 0.00159529
Iteration 10/25 | Loss: 0.00159529
Iteration 11/25 | Loss: 0.00159529
Iteration 12/25 | Loss: 0.00159529
Iteration 13/25 | Loss: 0.00159529
Iteration 14/25 | Loss: 0.00159529
Iteration 15/25 | Loss: 0.00159529
Iteration 16/25 | Loss: 0.00159529
Iteration 17/25 | Loss: 0.00159529
Iteration 18/25 | Loss: 0.00159529
Iteration 19/25 | Loss: 0.00159529
Iteration 20/25 | Loss: 0.00159529
Iteration 21/25 | Loss: 0.00159529
Iteration 22/25 | Loss: 0.00159529
Iteration 23/25 | Loss: 0.00159529
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0015952851390466094, 0.0015952851390466094, 0.0015952851390466094, 0.0015952851390466094, 0.0015952851390466094]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015952851390466094

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00159529
Iteration 2/1000 | Loss: 0.00005961
Iteration 3/1000 | Loss: 0.00004252
Iteration 4/1000 | Loss: 0.00003704
Iteration 5/1000 | Loss: 0.00003501
Iteration 6/1000 | Loss: 0.00003322
Iteration 7/1000 | Loss: 0.00003235
Iteration 8/1000 | Loss: 0.00003154
Iteration 9/1000 | Loss: 0.00003107
Iteration 10/1000 | Loss: 0.00003069
Iteration 11/1000 | Loss: 0.00003026
Iteration 12/1000 | Loss: 0.00002993
Iteration 13/1000 | Loss: 0.00002958
Iteration 14/1000 | Loss: 0.00002931
Iteration 15/1000 | Loss: 0.00002902
Iteration 16/1000 | Loss: 0.00002881
Iteration 17/1000 | Loss: 0.00002864
Iteration 18/1000 | Loss: 0.00002850
Iteration 19/1000 | Loss: 0.00002847
Iteration 20/1000 | Loss: 0.00002844
Iteration 21/1000 | Loss: 0.00002843
Iteration 22/1000 | Loss: 0.00002841
Iteration 23/1000 | Loss: 0.00002841
Iteration 24/1000 | Loss: 0.00002839
Iteration 25/1000 | Loss: 0.00002839
Iteration 26/1000 | Loss: 0.00002835
Iteration 27/1000 | Loss: 0.00002835
Iteration 28/1000 | Loss: 0.00002834
Iteration 29/1000 | Loss: 0.00002829
Iteration 30/1000 | Loss: 0.00002827
Iteration 31/1000 | Loss: 0.00002824
Iteration 32/1000 | Loss: 0.00002823
Iteration 33/1000 | Loss: 0.00002823
Iteration 34/1000 | Loss: 0.00002823
Iteration 35/1000 | Loss: 0.00002819
Iteration 36/1000 | Loss: 0.00002818
Iteration 37/1000 | Loss: 0.00002817
Iteration 38/1000 | Loss: 0.00002817
Iteration 39/1000 | Loss: 0.00002817
Iteration 40/1000 | Loss: 0.00002817
Iteration 41/1000 | Loss: 0.00002812
Iteration 42/1000 | Loss: 0.00002811
Iteration 43/1000 | Loss: 0.00002811
Iteration 44/1000 | Loss: 0.00002810
Iteration 45/1000 | Loss: 0.00002809
Iteration 46/1000 | Loss: 0.00002808
Iteration 47/1000 | Loss: 0.00002808
Iteration 48/1000 | Loss: 0.00002808
Iteration 49/1000 | Loss: 0.00002806
Iteration 50/1000 | Loss: 0.00002805
Iteration 51/1000 | Loss: 0.00002805
Iteration 52/1000 | Loss: 0.00002804
Iteration 53/1000 | Loss: 0.00002804
Iteration 54/1000 | Loss: 0.00002804
Iteration 55/1000 | Loss: 0.00002803
Iteration 56/1000 | Loss: 0.00002803
Iteration 57/1000 | Loss: 0.00002803
Iteration 58/1000 | Loss: 0.00002802
Iteration 59/1000 | Loss: 0.00002802
Iteration 60/1000 | Loss: 0.00002802
Iteration 61/1000 | Loss: 0.00002802
Iteration 62/1000 | Loss: 0.00002802
Iteration 63/1000 | Loss: 0.00002802
Iteration 64/1000 | Loss: 0.00002801
Iteration 65/1000 | Loss: 0.00002801
Iteration 66/1000 | Loss: 0.00002800
Iteration 67/1000 | Loss: 0.00002800
Iteration 68/1000 | Loss: 0.00002800
Iteration 69/1000 | Loss: 0.00002800
Iteration 70/1000 | Loss: 0.00002800
Iteration 71/1000 | Loss: 0.00002799
Iteration 72/1000 | Loss: 0.00002799
Iteration 73/1000 | Loss: 0.00002798
Iteration 74/1000 | Loss: 0.00002798
Iteration 75/1000 | Loss: 0.00002798
Iteration 76/1000 | Loss: 0.00002798
Iteration 77/1000 | Loss: 0.00002798
Iteration 78/1000 | Loss: 0.00002798
Iteration 79/1000 | Loss: 0.00002798
Iteration 80/1000 | Loss: 0.00002797
Iteration 81/1000 | Loss: 0.00002797
Iteration 82/1000 | Loss: 0.00002797
Iteration 83/1000 | Loss: 0.00002797
Iteration 84/1000 | Loss: 0.00002797
Iteration 85/1000 | Loss: 0.00002797
Iteration 86/1000 | Loss: 0.00002797
Iteration 87/1000 | Loss: 0.00002797
Iteration 88/1000 | Loss: 0.00002797
Iteration 89/1000 | Loss: 0.00002797
Iteration 90/1000 | Loss: 0.00002796
Iteration 91/1000 | Loss: 0.00002795
Iteration 92/1000 | Loss: 0.00002795
Iteration 93/1000 | Loss: 0.00002794
Iteration 94/1000 | Loss: 0.00002794
Iteration 95/1000 | Loss: 0.00002794
Iteration 96/1000 | Loss: 0.00002793
Iteration 97/1000 | Loss: 0.00002793
Iteration 98/1000 | Loss: 0.00002793
Iteration 99/1000 | Loss: 0.00002792
Iteration 100/1000 | Loss: 0.00002792
Iteration 101/1000 | Loss: 0.00002792
Iteration 102/1000 | Loss: 0.00002792
Iteration 103/1000 | Loss: 0.00002792
Iteration 104/1000 | Loss: 0.00002792
Iteration 105/1000 | Loss: 0.00002792
Iteration 106/1000 | Loss: 0.00002792
Iteration 107/1000 | Loss: 0.00002792
Iteration 108/1000 | Loss: 0.00002792
Iteration 109/1000 | Loss: 0.00002792
Iteration 110/1000 | Loss: 0.00002792
Iteration 111/1000 | Loss: 0.00002791
Iteration 112/1000 | Loss: 0.00002791
Iteration 113/1000 | Loss: 0.00002791
Iteration 114/1000 | Loss: 0.00002790
Iteration 115/1000 | Loss: 0.00002790
Iteration 116/1000 | Loss: 0.00002790
Iteration 117/1000 | Loss: 0.00002790
Iteration 118/1000 | Loss: 0.00002790
Iteration 119/1000 | Loss: 0.00002790
Iteration 120/1000 | Loss: 0.00002790
Iteration 121/1000 | Loss: 0.00002789
Iteration 122/1000 | Loss: 0.00002789
Iteration 123/1000 | Loss: 0.00002789
Iteration 124/1000 | Loss: 0.00002789
Iteration 125/1000 | Loss: 0.00002789
Iteration 126/1000 | Loss: 0.00002789
Iteration 127/1000 | Loss: 0.00002789
Iteration 128/1000 | Loss: 0.00002789
Iteration 129/1000 | Loss: 0.00002788
Iteration 130/1000 | Loss: 0.00002788
Iteration 131/1000 | Loss: 0.00002788
Iteration 132/1000 | Loss: 0.00002788
Iteration 133/1000 | Loss: 0.00002788
Iteration 134/1000 | Loss: 0.00002788
Iteration 135/1000 | Loss: 0.00002788
Iteration 136/1000 | Loss: 0.00002788
Iteration 137/1000 | Loss: 0.00002787
Iteration 138/1000 | Loss: 0.00002787
Iteration 139/1000 | Loss: 0.00002787
Iteration 140/1000 | Loss: 0.00002787
Iteration 141/1000 | Loss: 0.00002787
Iteration 142/1000 | Loss: 0.00002787
Iteration 143/1000 | Loss: 0.00002787
Iteration 144/1000 | Loss: 0.00002786
Iteration 145/1000 | Loss: 0.00002786
Iteration 146/1000 | Loss: 0.00002786
Iteration 147/1000 | Loss: 0.00002786
Iteration 148/1000 | Loss: 0.00002786
Iteration 149/1000 | Loss: 0.00002786
Iteration 150/1000 | Loss: 0.00002786
Iteration 151/1000 | Loss: 0.00002786
Iteration 152/1000 | Loss: 0.00002786
Iteration 153/1000 | Loss: 0.00002786
Iteration 154/1000 | Loss: 0.00002786
Iteration 155/1000 | Loss: 0.00002786
Iteration 156/1000 | Loss: 0.00002786
Iteration 157/1000 | Loss: 0.00002786
Iteration 158/1000 | Loss: 0.00002786
Iteration 159/1000 | Loss: 0.00002786
Iteration 160/1000 | Loss: 0.00002786
Iteration 161/1000 | Loss: 0.00002786
Iteration 162/1000 | Loss: 0.00002786
Iteration 163/1000 | Loss: 0.00002786
Iteration 164/1000 | Loss: 0.00002786
Iteration 165/1000 | Loss: 0.00002786
Iteration 166/1000 | Loss: 0.00002786
Iteration 167/1000 | Loss: 0.00002786
Iteration 168/1000 | Loss: 0.00002786
Iteration 169/1000 | Loss: 0.00002786
Iteration 170/1000 | Loss: 0.00002786
Iteration 171/1000 | Loss: 0.00002786
Iteration 172/1000 | Loss: 0.00002786
Iteration 173/1000 | Loss: 0.00002786
Iteration 174/1000 | Loss: 0.00002786
Iteration 175/1000 | Loss: 0.00002786
Iteration 176/1000 | Loss: 0.00002786
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [2.7856198357767425e-05, 2.7856198357767425e-05, 2.7856198357767425e-05, 2.7856198357767425e-05, 2.7856198357767425e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7856198357767425e-05

Optimization complete. Final v2v error: 4.329307556152344 mm

Highest mean error: 5.3205790519714355 mm for frame 171

Lowest mean error: 3.2920315265655518 mm for frame 205

Saving results

Total time: 84.19049119949341
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00521567
Iteration 2/25 | Loss: 0.00138395
Iteration 3/25 | Loss: 0.00119936
Iteration 4/25 | Loss: 0.00117932
Iteration 5/25 | Loss: 0.00117621
Iteration 6/25 | Loss: 0.00117567
Iteration 7/25 | Loss: 0.00117567
Iteration 8/25 | Loss: 0.00117567
Iteration 9/25 | Loss: 0.00117567
Iteration 10/25 | Loss: 0.00117567
Iteration 11/25 | Loss: 0.00117567
Iteration 12/25 | Loss: 0.00117567
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001175670069642365, 0.001175670069642365, 0.001175670069642365, 0.001175670069642365, 0.001175670069642365]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001175670069642365

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44348514
Iteration 2/25 | Loss: 0.00162529
Iteration 3/25 | Loss: 0.00162528
Iteration 4/25 | Loss: 0.00162528
Iteration 5/25 | Loss: 0.00162528
Iteration 6/25 | Loss: 0.00162528
Iteration 7/25 | Loss: 0.00162528
Iteration 8/25 | Loss: 0.00162528
Iteration 9/25 | Loss: 0.00162528
Iteration 10/25 | Loss: 0.00162528
Iteration 11/25 | Loss: 0.00162528
Iteration 12/25 | Loss: 0.00162528
Iteration 13/25 | Loss: 0.00162528
Iteration 14/25 | Loss: 0.00162528
Iteration 15/25 | Loss: 0.00162528
Iteration 16/25 | Loss: 0.00162528
Iteration 17/25 | Loss: 0.00162528
Iteration 18/25 | Loss: 0.00162528
Iteration 19/25 | Loss: 0.00162528
Iteration 20/25 | Loss: 0.00162528
Iteration 21/25 | Loss: 0.00162528
Iteration 22/25 | Loss: 0.00162528
Iteration 23/25 | Loss: 0.00162528
Iteration 24/25 | Loss: 0.00162528
Iteration 25/25 | Loss: 0.00162528

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00162528
Iteration 2/1000 | Loss: 0.00002159
Iteration 3/1000 | Loss: 0.00001526
Iteration 4/1000 | Loss: 0.00001394
Iteration 5/1000 | Loss: 0.00001290
Iteration 6/1000 | Loss: 0.00001232
Iteration 7/1000 | Loss: 0.00001200
Iteration 8/1000 | Loss: 0.00001162
Iteration 9/1000 | Loss: 0.00001131
Iteration 10/1000 | Loss: 0.00001095
Iteration 11/1000 | Loss: 0.00001093
Iteration 12/1000 | Loss: 0.00001073
Iteration 13/1000 | Loss: 0.00001052
Iteration 14/1000 | Loss: 0.00001045
Iteration 15/1000 | Loss: 0.00001044
Iteration 16/1000 | Loss: 0.00001043
Iteration 17/1000 | Loss: 0.00001037
Iteration 18/1000 | Loss: 0.00001028
Iteration 19/1000 | Loss: 0.00001020
Iteration 20/1000 | Loss: 0.00001018
Iteration 21/1000 | Loss: 0.00001017
Iteration 22/1000 | Loss: 0.00001016
Iteration 23/1000 | Loss: 0.00001015
Iteration 24/1000 | Loss: 0.00001014
Iteration 25/1000 | Loss: 0.00001012
Iteration 26/1000 | Loss: 0.00001011
Iteration 27/1000 | Loss: 0.00001010
Iteration 28/1000 | Loss: 0.00001007
Iteration 29/1000 | Loss: 0.00001001
Iteration 30/1000 | Loss: 0.00001000
Iteration 31/1000 | Loss: 0.00000999
Iteration 32/1000 | Loss: 0.00000999
Iteration 33/1000 | Loss: 0.00000997
Iteration 34/1000 | Loss: 0.00000996
Iteration 35/1000 | Loss: 0.00000995
Iteration 36/1000 | Loss: 0.00000994
Iteration 37/1000 | Loss: 0.00000994
Iteration 38/1000 | Loss: 0.00000994
Iteration 39/1000 | Loss: 0.00000993
Iteration 40/1000 | Loss: 0.00000993
Iteration 41/1000 | Loss: 0.00000992
Iteration 42/1000 | Loss: 0.00000992
Iteration 43/1000 | Loss: 0.00000991
Iteration 44/1000 | Loss: 0.00000989
Iteration 45/1000 | Loss: 0.00000989
Iteration 46/1000 | Loss: 0.00000984
Iteration 47/1000 | Loss: 0.00000984
Iteration 48/1000 | Loss: 0.00000983
Iteration 49/1000 | Loss: 0.00000982
Iteration 50/1000 | Loss: 0.00000982
Iteration 51/1000 | Loss: 0.00000981
Iteration 52/1000 | Loss: 0.00000981
Iteration 53/1000 | Loss: 0.00000980
Iteration 54/1000 | Loss: 0.00000980
Iteration 55/1000 | Loss: 0.00000979
Iteration 56/1000 | Loss: 0.00000979
Iteration 57/1000 | Loss: 0.00000979
Iteration 58/1000 | Loss: 0.00000979
Iteration 59/1000 | Loss: 0.00000979
Iteration 60/1000 | Loss: 0.00000978
Iteration 61/1000 | Loss: 0.00000978
Iteration 62/1000 | Loss: 0.00000977
Iteration 63/1000 | Loss: 0.00000977
Iteration 64/1000 | Loss: 0.00000976
Iteration 65/1000 | Loss: 0.00000975
Iteration 66/1000 | Loss: 0.00000975
Iteration 67/1000 | Loss: 0.00000975
Iteration 68/1000 | Loss: 0.00000975
Iteration 69/1000 | Loss: 0.00000974
Iteration 70/1000 | Loss: 0.00000974
Iteration 71/1000 | Loss: 0.00000974
Iteration 72/1000 | Loss: 0.00000973
Iteration 73/1000 | Loss: 0.00000972
Iteration 74/1000 | Loss: 0.00000971
Iteration 75/1000 | Loss: 0.00000971
Iteration 76/1000 | Loss: 0.00000970
Iteration 77/1000 | Loss: 0.00000970
Iteration 78/1000 | Loss: 0.00000969
Iteration 79/1000 | Loss: 0.00000969
Iteration 80/1000 | Loss: 0.00000968
Iteration 81/1000 | Loss: 0.00000967
Iteration 82/1000 | Loss: 0.00000967
Iteration 83/1000 | Loss: 0.00000967
Iteration 84/1000 | Loss: 0.00000967
Iteration 85/1000 | Loss: 0.00000966
Iteration 86/1000 | Loss: 0.00000966
Iteration 87/1000 | Loss: 0.00000966
Iteration 88/1000 | Loss: 0.00000965
Iteration 89/1000 | Loss: 0.00000965
Iteration 90/1000 | Loss: 0.00000965
Iteration 91/1000 | Loss: 0.00000964
Iteration 92/1000 | Loss: 0.00000964
Iteration 93/1000 | Loss: 0.00000964
Iteration 94/1000 | Loss: 0.00000964
Iteration 95/1000 | Loss: 0.00000963
Iteration 96/1000 | Loss: 0.00000963
Iteration 97/1000 | Loss: 0.00000963
Iteration 98/1000 | Loss: 0.00000962
Iteration 99/1000 | Loss: 0.00000962
Iteration 100/1000 | Loss: 0.00000962
Iteration 101/1000 | Loss: 0.00000962
Iteration 102/1000 | Loss: 0.00000961
Iteration 103/1000 | Loss: 0.00000961
Iteration 104/1000 | Loss: 0.00000961
Iteration 105/1000 | Loss: 0.00000961
Iteration 106/1000 | Loss: 0.00000961
Iteration 107/1000 | Loss: 0.00000961
Iteration 108/1000 | Loss: 0.00000961
Iteration 109/1000 | Loss: 0.00000961
Iteration 110/1000 | Loss: 0.00000961
Iteration 111/1000 | Loss: 0.00000961
Iteration 112/1000 | Loss: 0.00000961
Iteration 113/1000 | Loss: 0.00000961
Iteration 114/1000 | Loss: 0.00000961
Iteration 115/1000 | Loss: 0.00000961
Iteration 116/1000 | Loss: 0.00000961
Iteration 117/1000 | Loss: 0.00000961
Iteration 118/1000 | Loss: 0.00000961
Iteration 119/1000 | Loss: 0.00000961
Iteration 120/1000 | Loss: 0.00000961
Iteration 121/1000 | Loss: 0.00000961
Iteration 122/1000 | Loss: 0.00000961
Iteration 123/1000 | Loss: 0.00000961
Iteration 124/1000 | Loss: 0.00000961
Iteration 125/1000 | Loss: 0.00000961
Iteration 126/1000 | Loss: 0.00000961
Iteration 127/1000 | Loss: 0.00000961
Iteration 128/1000 | Loss: 0.00000961
Iteration 129/1000 | Loss: 0.00000961
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [9.613553629606031e-06, 9.613553629606031e-06, 9.613553629606031e-06, 9.613553629606031e-06, 9.613553629606031e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.613553629606031e-06

Optimization complete. Final v2v error: 2.6845340728759766 mm

Highest mean error: 2.963113784790039 mm for frame 174

Lowest mean error: 2.3477895259857178 mm for frame 120

Saving results

Total time: 44.08163619041443
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01115519
Iteration 2/25 | Loss: 0.01115519
Iteration 3/25 | Loss: 0.01115519
Iteration 4/25 | Loss: 0.01115519
Iteration 5/25 | Loss: 0.01115519
Iteration 6/25 | Loss: 0.01115519
Iteration 7/25 | Loss: 0.01115519
Iteration 8/25 | Loss: 0.01115519
Iteration 9/25 | Loss: 0.01115519
Iteration 10/25 | Loss: 0.01115518
Iteration 11/25 | Loss: 0.01115518
Iteration 12/25 | Loss: 0.01115518
Iteration 13/25 | Loss: 0.01115518
Iteration 14/25 | Loss: 0.01115518
Iteration 15/25 | Loss: 0.01115518
Iteration 16/25 | Loss: 0.01115518
Iteration 17/25 | Loss: 0.01115518
Iteration 18/25 | Loss: 0.01115518
Iteration 19/25 | Loss: 0.01115518
Iteration 20/25 | Loss: 0.01115518
Iteration 21/25 | Loss: 0.01115518
Iteration 22/25 | Loss: 0.01115518
Iteration 23/25 | Loss: 0.01115518
Iteration 24/25 | Loss: 0.01115518
Iteration 25/25 | Loss: 0.01115518

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 10.57395649
Iteration 2/25 | Loss: 0.19482252
Iteration 3/25 | Loss: 0.19023420
Iteration 4/25 | Loss: 0.18866669
Iteration 5/25 | Loss: 0.18861777
Iteration 6/25 | Loss: 0.18855849
Iteration 7/25 | Loss: 0.18855849
Iteration 8/25 | Loss: 0.18855844
Iteration 9/25 | Loss: 0.18855841
Iteration 10/25 | Loss: 0.18855841
Iteration 11/25 | Loss: 0.18855837
Iteration 12/25 | Loss: 0.18855837
Iteration 13/25 | Loss: 0.18855837
Iteration 14/25 | Loss: 0.18855837
Iteration 15/25 | Loss: 0.18855837
Iteration 16/25 | Loss: 0.18855837
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.18855836987495422, 0.18855836987495422, 0.18855836987495422, 0.18855836987495422, 0.18855836987495422]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.18855836987495422

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.18855837
Iteration 2/1000 | Loss: 0.00676966
Iteration 3/1000 | Loss: 0.00177654
Iteration 4/1000 | Loss: 0.00042636
Iteration 5/1000 | Loss: 0.00035057
Iteration 6/1000 | Loss: 0.00052273
Iteration 7/1000 | Loss: 0.00014065
Iteration 8/1000 | Loss: 0.00022062
Iteration 9/1000 | Loss: 0.00008608
Iteration 10/1000 | Loss: 0.00010567
Iteration 11/1000 | Loss: 0.00013913
Iteration 12/1000 | Loss: 0.00006562
Iteration 13/1000 | Loss: 0.00005205
Iteration 14/1000 | Loss: 0.00004206
Iteration 15/1000 | Loss: 0.00009808
Iteration 16/1000 | Loss: 0.00003782
Iteration 17/1000 | Loss: 0.00005921
Iteration 18/1000 | Loss: 0.00003374
Iteration 19/1000 | Loss: 0.00003116
Iteration 20/1000 | Loss: 0.00014092
Iteration 21/1000 | Loss: 0.00014512
Iteration 22/1000 | Loss: 0.00012658
Iteration 23/1000 | Loss: 0.00006263
Iteration 24/1000 | Loss: 0.00003291
Iteration 25/1000 | Loss: 0.00003061
Iteration 26/1000 | Loss: 0.00002839
Iteration 27/1000 | Loss: 0.00002776
Iteration 28/1000 | Loss: 0.00002684
Iteration 29/1000 | Loss: 0.00002894
Iteration 30/1000 | Loss: 0.00006902
Iteration 31/1000 | Loss: 0.00002765
Iteration 32/1000 | Loss: 0.00002530
Iteration 33/1000 | Loss: 0.00002469
Iteration 34/1000 | Loss: 0.00002430
Iteration 35/1000 | Loss: 0.00002442
Iteration 36/1000 | Loss: 0.00002370
Iteration 37/1000 | Loss: 0.00002330
Iteration 38/1000 | Loss: 0.00002308
Iteration 39/1000 | Loss: 0.00002288
Iteration 40/1000 | Loss: 0.00002286
Iteration 41/1000 | Loss: 0.00002290
Iteration 42/1000 | Loss: 0.00002293
Iteration 43/1000 | Loss: 0.00002267
Iteration 44/1000 | Loss: 0.00002219
Iteration 45/1000 | Loss: 0.00002207
Iteration 46/1000 | Loss: 0.00002202
Iteration 47/1000 | Loss: 0.00002202
Iteration 48/1000 | Loss: 0.00002194
Iteration 49/1000 | Loss: 0.00002194
Iteration 50/1000 | Loss: 0.00002177
Iteration 51/1000 | Loss: 0.00002170
Iteration 52/1000 | Loss: 0.00002169
Iteration 53/1000 | Loss: 0.00002168
Iteration 54/1000 | Loss: 0.00002168
Iteration 55/1000 | Loss: 0.00002168
Iteration 56/1000 | Loss: 0.00002168
Iteration 57/1000 | Loss: 0.00002167
Iteration 58/1000 | Loss: 0.00002167
Iteration 59/1000 | Loss: 0.00002166
Iteration 60/1000 | Loss: 0.00002166
Iteration 61/1000 | Loss: 0.00002166
Iteration 62/1000 | Loss: 0.00002165
Iteration 63/1000 | Loss: 0.00002165
Iteration 64/1000 | Loss: 0.00002165
Iteration 65/1000 | Loss: 0.00002164
Iteration 66/1000 | Loss: 0.00002164
Iteration 67/1000 | Loss: 0.00002163
Iteration 68/1000 | Loss: 0.00002163
Iteration 69/1000 | Loss: 0.00002163
Iteration 70/1000 | Loss: 0.00002163
Iteration 71/1000 | Loss: 0.00002163
Iteration 72/1000 | Loss: 0.00002163
Iteration 73/1000 | Loss: 0.00002162
Iteration 74/1000 | Loss: 0.00002162
Iteration 75/1000 | Loss: 0.00002161
Iteration 76/1000 | Loss: 0.00002161
Iteration 77/1000 | Loss: 0.00002161
Iteration 78/1000 | Loss: 0.00002161
Iteration 79/1000 | Loss: 0.00002161
Iteration 80/1000 | Loss: 0.00002161
Iteration 81/1000 | Loss: 0.00002161
Iteration 82/1000 | Loss: 0.00002161
Iteration 83/1000 | Loss: 0.00002160
Iteration 84/1000 | Loss: 0.00002160
Iteration 85/1000 | Loss: 0.00002160
Iteration 86/1000 | Loss: 0.00002160
Iteration 87/1000 | Loss: 0.00002160
Iteration 88/1000 | Loss: 0.00002160
Iteration 89/1000 | Loss: 0.00002160
Iteration 90/1000 | Loss: 0.00002160
Iteration 91/1000 | Loss: 0.00002160
Iteration 92/1000 | Loss: 0.00002160
Iteration 93/1000 | Loss: 0.00002160
Iteration 94/1000 | Loss: 0.00002160
Iteration 95/1000 | Loss: 0.00002160
Iteration 96/1000 | Loss: 0.00002160
Iteration 97/1000 | Loss: 0.00002160
Iteration 98/1000 | Loss: 0.00002159
Iteration 99/1000 | Loss: 0.00002159
Iteration 100/1000 | Loss: 0.00002159
Iteration 101/1000 | Loss: 0.00002159
Iteration 102/1000 | Loss: 0.00002159
Iteration 103/1000 | Loss: 0.00002159
Iteration 104/1000 | Loss: 0.00002159
Iteration 105/1000 | Loss: 0.00002159
Iteration 106/1000 | Loss: 0.00002159
Iteration 107/1000 | Loss: 0.00002159
Iteration 108/1000 | Loss: 0.00002159
Iteration 109/1000 | Loss: 0.00002158
Iteration 110/1000 | Loss: 0.00002158
Iteration 111/1000 | Loss: 0.00002158
Iteration 112/1000 | Loss: 0.00002158
Iteration 113/1000 | Loss: 0.00002157
Iteration 114/1000 | Loss: 0.00002157
Iteration 115/1000 | Loss: 0.00002157
Iteration 116/1000 | Loss: 0.00002157
Iteration 117/1000 | Loss: 0.00002156
Iteration 118/1000 | Loss: 0.00002156
Iteration 119/1000 | Loss: 0.00002156
Iteration 120/1000 | Loss: 0.00002156
Iteration 121/1000 | Loss: 0.00002156
Iteration 122/1000 | Loss: 0.00002156
Iteration 123/1000 | Loss: 0.00002156
Iteration 124/1000 | Loss: 0.00002156
Iteration 125/1000 | Loss: 0.00002156
Iteration 126/1000 | Loss: 0.00002156
Iteration 127/1000 | Loss: 0.00002156
Iteration 128/1000 | Loss: 0.00002156
Iteration 129/1000 | Loss: 0.00002156
Iteration 130/1000 | Loss: 0.00002156
Iteration 131/1000 | Loss: 0.00002156
Iteration 132/1000 | Loss: 0.00002156
Iteration 133/1000 | Loss: 0.00002156
Iteration 134/1000 | Loss: 0.00002156
Iteration 135/1000 | Loss: 0.00002156
Iteration 136/1000 | Loss: 0.00002156
Iteration 137/1000 | Loss: 0.00002156
Iteration 138/1000 | Loss: 0.00002156
Iteration 139/1000 | Loss: 0.00002156
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [2.155776383006014e-05, 2.155776383006014e-05, 2.155776383006014e-05, 2.155776383006014e-05, 2.155776383006014e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.155776383006014e-05

Optimization complete. Final v2v error: 3.5050878524780273 mm

Highest mean error: 19.620756149291992 mm for frame 110

Lowest mean error: 3.0713858604431152 mm for frame 211

Saving results

Total time: 91.1654143333435
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00415517
Iteration 2/25 | Loss: 0.00125394
Iteration 3/25 | Loss: 0.00117114
Iteration 4/25 | Loss: 0.00115512
Iteration 5/25 | Loss: 0.00115066
Iteration 6/25 | Loss: 0.00114955
Iteration 7/25 | Loss: 0.00114955
Iteration 8/25 | Loss: 0.00114955
Iteration 9/25 | Loss: 0.00114955
Iteration 10/25 | Loss: 0.00114955
Iteration 11/25 | Loss: 0.00114955
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011495498474687338, 0.0011495498474687338, 0.0011495498474687338, 0.0011495498474687338, 0.0011495498474687338]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011495498474687338

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.72150373
Iteration 2/25 | Loss: 0.00182243
Iteration 3/25 | Loss: 0.00182243
Iteration 4/25 | Loss: 0.00182243
Iteration 5/25 | Loss: 0.00182243
Iteration 6/25 | Loss: 0.00182243
Iteration 7/25 | Loss: 0.00182243
Iteration 8/25 | Loss: 0.00182243
Iteration 9/25 | Loss: 0.00182243
Iteration 10/25 | Loss: 0.00182243
Iteration 11/25 | Loss: 0.00182243
Iteration 12/25 | Loss: 0.00182243
Iteration 13/25 | Loss: 0.00182243
Iteration 14/25 | Loss: 0.00182243
Iteration 15/25 | Loss: 0.00182243
Iteration 16/25 | Loss: 0.00182243
Iteration 17/25 | Loss: 0.00182243
Iteration 18/25 | Loss: 0.00182243
Iteration 19/25 | Loss: 0.00182243
Iteration 20/25 | Loss: 0.00182243
Iteration 21/25 | Loss: 0.00182243
Iteration 22/25 | Loss: 0.00182243
Iteration 23/25 | Loss: 0.00182243
Iteration 24/25 | Loss: 0.00182243
Iteration 25/25 | Loss: 0.00182243

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00182243
Iteration 2/1000 | Loss: 0.00002383
Iteration 3/1000 | Loss: 0.00001660
Iteration 4/1000 | Loss: 0.00001457
Iteration 5/1000 | Loss: 0.00001383
Iteration 6/1000 | Loss: 0.00001326
Iteration 7/1000 | Loss: 0.00001279
Iteration 8/1000 | Loss: 0.00001244
Iteration 9/1000 | Loss: 0.00001218
Iteration 10/1000 | Loss: 0.00001194
Iteration 11/1000 | Loss: 0.00001179
Iteration 12/1000 | Loss: 0.00001172
Iteration 13/1000 | Loss: 0.00001167
Iteration 14/1000 | Loss: 0.00001160
Iteration 15/1000 | Loss: 0.00001159
Iteration 16/1000 | Loss: 0.00001159
Iteration 17/1000 | Loss: 0.00001158
Iteration 18/1000 | Loss: 0.00001151
Iteration 19/1000 | Loss: 0.00001151
Iteration 20/1000 | Loss: 0.00001150
Iteration 21/1000 | Loss: 0.00001149
Iteration 22/1000 | Loss: 0.00001147
Iteration 23/1000 | Loss: 0.00001143
Iteration 24/1000 | Loss: 0.00001143
Iteration 25/1000 | Loss: 0.00001137
Iteration 26/1000 | Loss: 0.00001137
Iteration 27/1000 | Loss: 0.00001130
Iteration 28/1000 | Loss: 0.00001130
Iteration 29/1000 | Loss: 0.00001128
Iteration 30/1000 | Loss: 0.00001128
Iteration 31/1000 | Loss: 0.00001128
Iteration 32/1000 | Loss: 0.00001127
Iteration 33/1000 | Loss: 0.00001127
Iteration 34/1000 | Loss: 0.00001127
Iteration 35/1000 | Loss: 0.00001126
Iteration 36/1000 | Loss: 0.00001125
Iteration 37/1000 | Loss: 0.00001125
Iteration 38/1000 | Loss: 0.00001125
Iteration 39/1000 | Loss: 0.00001125
Iteration 40/1000 | Loss: 0.00001124
Iteration 41/1000 | Loss: 0.00001124
Iteration 42/1000 | Loss: 0.00001124
Iteration 43/1000 | Loss: 0.00001124
Iteration 44/1000 | Loss: 0.00001124
Iteration 45/1000 | Loss: 0.00001123
Iteration 46/1000 | Loss: 0.00001123
Iteration 47/1000 | Loss: 0.00001123
Iteration 48/1000 | Loss: 0.00001123
Iteration 49/1000 | Loss: 0.00001123
Iteration 50/1000 | Loss: 0.00001122
Iteration 51/1000 | Loss: 0.00001122
Iteration 52/1000 | Loss: 0.00001121
Iteration 53/1000 | Loss: 0.00001121
Iteration 54/1000 | Loss: 0.00001121
Iteration 55/1000 | Loss: 0.00001121
Iteration 56/1000 | Loss: 0.00001121
Iteration 57/1000 | Loss: 0.00001121
Iteration 58/1000 | Loss: 0.00001121
Iteration 59/1000 | Loss: 0.00001121
Iteration 60/1000 | Loss: 0.00001120
Iteration 61/1000 | Loss: 0.00001120
Iteration 62/1000 | Loss: 0.00001120
Iteration 63/1000 | Loss: 0.00001119
Iteration 64/1000 | Loss: 0.00001119
Iteration 65/1000 | Loss: 0.00001119
Iteration 66/1000 | Loss: 0.00001119
Iteration 67/1000 | Loss: 0.00001118
Iteration 68/1000 | Loss: 0.00001118
Iteration 69/1000 | Loss: 0.00001118
Iteration 70/1000 | Loss: 0.00001118
Iteration 71/1000 | Loss: 0.00001118
Iteration 72/1000 | Loss: 0.00001118
Iteration 73/1000 | Loss: 0.00001118
Iteration 74/1000 | Loss: 0.00001118
Iteration 75/1000 | Loss: 0.00001118
Iteration 76/1000 | Loss: 0.00001118
Iteration 77/1000 | Loss: 0.00001118
Iteration 78/1000 | Loss: 0.00001118
Iteration 79/1000 | Loss: 0.00001118
Iteration 80/1000 | Loss: 0.00001118
Iteration 81/1000 | Loss: 0.00001118
Iteration 82/1000 | Loss: 0.00001118
Iteration 83/1000 | Loss: 0.00001118
Iteration 84/1000 | Loss: 0.00001118
Iteration 85/1000 | Loss: 0.00001118
Iteration 86/1000 | Loss: 0.00001118
Iteration 87/1000 | Loss: 0.00001118
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 87. Stopping optimization.
Last 5 losses: [1.1175074178026989e-05, 1.1175074178026989e-05, 1.1175074178026989e-05, 1.1175074178026989e-05, 1.1175074178026989e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1175074178026989e-05

Optimization complete. Final v2v error: 2.8922977447509766 mm

Highest mean error: 3.390115976333618 mm for frame 108

Lowest mean error: 2.639143705368042 mm for frame 129

Saving results

Total time: 34.09134912490845
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00810532
Iteration 2/25 | Loss: 0.00125121
Iteration 3/25 | Loss: 0.00116348
Iteration 4/25 | Loss: 0.00115497
Iteration 5/25 | Loss: 0.00114236
Iteration 6/25 | Loss: 0.00114076
Iteration 7/25 | Loss: 0.00114045
Iteration 8/25 | Loss: 0.00114030
Iteration 9/25 | Loss: 0.00114021
Iteration 10/25 | Loss: 0.00114019
Iteration 11/25 | Loss: 0.00114019
Iteration 12/25 | Loss: 0.00114019
Iteration 13/25 | Loss: 0.00114019
Iteration 14/25 | Loss: 0.00114019
Iteration 15/25 | Loss: 0.00114019
Iteration 16/25 | Loss: 0.00114018
Iteration 17/25 | Loss: 0.00114018
Iteration 18/25 | Loss: 0.00114018
Iteration 19/25 | Loss: 0.00114018
Iteration 20/25 | Loss: 0.00114018
Iteration 21/25 | Loss: 0.00114018
Iteration 22/25 | Loss: 0.00114018
Iteration 23/25 | Loss: 0.00114018
Iteration 24/25 | Loss: 0.00114018
Iteration 25/25 | Loss: 0.00114018

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.36752033
Iteration 2/25 | Loss: 0.00191802
Iteration 3/25 | Loss: 0.00191802
Iteration 4/25 | Loss: 0.00191802
Iteration 5/25 | Loss: 0.00191802
Iteration 6/25 | Loss: 0.00191802
Iteration 7/25 | Loss: 0.00191802
Iteration 8/25 | Loss: 0.00191802
Iteration 9/25 | Loss: 0.00191802
Iteration 10/25 | Loss: 0.00191802
Iteration 11/25 | Loss: 0.00191802
Iteration 12/25 | Loss: 0.00191802
Iteration 13/25 | Loss: 0.00191802
Iteration 14/25 | Loss: 0.00191802
Iteration 15/25 | Loss: 0.00191802
Iteration 16/25 | Loss: 0.00191802
Iteration 17/25 | Loss: 0.00191802
Iteration 18/25 | Loss: 0.00191802
Iteration 19/25 | Loss: 0.00191802
Iteration 20/25 | Loss: 0.00191802
Iteration 21/25 | Loss: 0.00191802
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0019180170493200421, 0.0019180170493200421, 0.0019180170493200421, 0.0019180170493200421, 0.0019180170493200421]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019180170493200421

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00191802
Iteration 2/1000 | Loss: 0.00002137
Iteration 3/1000 | Loss: 0.00001435
Iteration 4/1000 | Loss: 0.00001288
Iteration 5/1000 | Loss: 0.00001225
Iteration 6/1000 | Loss: 0.00001167
Iteration 7/1000 | Loss: 0.00001121
Iteration 8/1000 | Loss: 0.00001094
Iteration 9/1000 | Loss: 0.00001070
Iteration 10/1000 | Loss: 0.00001059
Iteration 11/1000 | Loss: 0.00001044
Iteration 12/1000 | Loss: 0.00001040
Iteration 13/1000 | Loss: 0.00001039
Iteration 14/1000 | Loss: 0.00001037
Iteration 15/1000 | Loss: 0.00001024
Iteration 16/1000 | Loss: 0.00001019
Iteration 17/1000 | Loss: 0.00001018
Iteration 18/1000 | Loss: 0.00001016
Iteration 19/1000 | Loss: 0.00001013
Iteration 20/1000 | Loss: 0.00001012
Iteration 21/1000 | Loss: 0.00001007
Iteration 22/1000 | Loss: 0.00001003
Iteration 23/1000 | Loss: 0.00001002
Iteration 24/1000 | Loss: 0.00001001
Iteration 25/1000 | Loss: 0.00000998
Iteration 26/1000 | Loss: 0.00000998
Iteration 27/1000 | Loss: 0.00000998
Iteration 28/1000 | Loss: 0.00000997
Iteration 29/1000 | Loss: 0.00000996
Iteration 30/1000 | Loss: 0.00000995
Iteration 31/1000 | Loss: 0.00000994
Iteration 32/1000 | Loss: 0.00000993
Iteration 33/1000 | Loss: 0.00000991
Iteration 34/1000 | Loss: 0.00000988
Iteration 35/1000 | Loss: 0.00000988
Iteration 36/1000 | Loss: 0.00000987
Iteration 37/1000 | Loss: 0.00000987
Iteration 38/1000 | Loss: 0.00000978
Iteration 39/1000 | Loss: 0.00000978
Iteration 40/1000 | Loss: 0.00000977
Iteration 41/1000 | Loss: 0.00000976
Iteration 42/1000 | Loss: 0.00000976
Iteration 43/1000 | Loss: 0.00000975
Iteration 44/1000 | Loss: 0.00000975
Iteration 45/1000 | Loss: 0.00000974
Iteration 46/1000 | Loss: 0.00000974
Iteration 47/1000 | Loss: 0.00000972
Iteration 48/1000 | Loss: 0.00000972
Iteration 49/1000 | Loss: 0.00000972
Iteration 50/1000 | Loss: 0.00000972
Iteration 51/1000 | Loss: 0.00000972
Iteration 52/1000 | Loss: 0.00000972
Iteration 53/1000 | Loss: 0.00000971
Iteration 54/1000 | Loss: 0.00000971
Iteration 55/1000 | Loss: 0.00000970
Iteration 56/1000 | Loss: 0.00000970
Iteration 57/1000 | Loss: 0.00000970
Iteration 58/1000 | Loss: 0.00000970
Iteration 59/1000 | Loss: 0.00000970
Iteration 60/1000 | Loss: 0.00000970
Iteration 61/1000 | Loss: 0.00000970
Iteration 62/1000 | Loss: 0.00000970
Iteration 63/1000 | Loss: 0.00000970
Iteration 64/1000 | Loss: 0.00000969
Iteration 65/1000 | Loss: 0.00000969
Iteration 66/1000 | Loss: 0.00000969
Iteration 67/1000 | Loss: 0.00000968
Iteration 68/1000 | Loss: 0.00000968
Iteration 69/1000 | Loss: 0.00000968
Iteration 70/1000 | Loss: 0.00000968
Iteration 71/1000 | Loss: 0.00000968
Iteration 72/1000 | Loss: 0.00000968
Iteration 73/1000 | Loss: 0.00000968
Iteration 74/1000 | Loss: 0.00000968
Iteration 75/1000 | Loss: 0.00000968
Iteration 76/1000 | Loss: 0.00000968
Iteration 77/1000 | Loss: 0.00000967
Iteration 78/1000 | Loss: 0.00000967
Iteration 79/1000 | Loss: 0.00000967
Iteration 80/1000 | Loss: 0.00000967
Iteration 81/1000 | Loss: 0.00000966
Iteration 82/1000 | Loss: 0.00000966
Iteration 83/1000 | Loss: 0.00000966
Iteration 84/1000 | Loss: 0.00000965
Iteration 85/1000 | Loss: 0.00000965
Iteration 86/1000 | Loss: 0.00000965
Iteration 87/1000 | Loss: 0.00000965
Iteration 88/1000 | Loss: 0.00000964
Iteration 89/1000 | Loss: 0.00000964
Iteration 90/1000 | Loss: 0.00000964
Iteration 91/1000 | Loss: 0.00000964
Iteration 92/1000 | Loss: 0.00000964
Iteration 93/1000 | Loss: 0.00000964
Iteration 94/1000 | Loss: 0.00000964
Iteration 95/1000 | Loss: 0.00000964
Iteration 96/1000 | Loss: 0.00000964
Iteration 97/1000 | Loss: 0.00000963
Iteration 98/1000 | Loss: 0.00000963
Iteration 99/1000 | Loss: 0.00000963
Iteration 100/1000 | Loss: 0.00000963
Iteration 101/1000 | Loss: 0.00000963
Iteration 102/1000 | Loss: 0.00000962
Iteration 103/1000 | Loss: 0.00000962
Iteration 104/1000 | Loss: 0.00000962
Iteration 105/1000 | Loss: 0.00000962
Iteration 106/1000 | Loss: 0.00000962
Iteration 107/1000 | Loss: 0.00000962
Iteration 108/1000 | Loss: 0.00000962
Iteration 109/1000 | Loss: 0.00000961
Iteration 110/1000 | Loss: 0.00000961
Iteration 111/1000 | Loss: 0.00000961
Iteration 112/1000 | Loss: 0.00000961
Iteration 113/1000 | Loss: 0.00000961
Iteration 114/1000 | Loss: 0.00000961
Iteration 115/1000 | Loss: 0.00000961
Iteration 116/1000 | Loss: 0.00000961
Iteration 117/1000 | Loss: 0.00000961
Iteration 118/1000 | Loss: 0.00000961
Iteration 119/1000 | Loss: 0.00000961
Iteration 120/1000 | Loss: 0.00000961
Iteration 121/1000 | Loss: 0.00000961
Iteration 122/1000 | Loss: 0.00000961
Iteration 123/1000 | Loss: 0.00000961
Iteration 124/1000 | Loss: 0.00000961
Iteration 125/1000 | Loss: 0.00000961
Iteration 126/1000 | Loss: 0.00000960
Iteration 127/1000 | Loss: 0.00000960
Iteration 128/1000 | Loss: 0.00000960
Iteration 129/1000 | Loss: 0.00000960
Iteration 130/1000 | Loss: 0.00000960
Iteration 131/1000 | Loss: 0.00000960
Iteration 132/1000 | Loss: 0.00000960
Iteration 133/1000 | Loss: 0.00000960
Iteration 134/1000 | Loss: 0.00000960
Iteration 135/1000 | Loss: 0.00000960
Iteration 136/1000 | Loss: 0.00000959
Iteration 137/1000 | Loss: 0.00000959
Iteration 138/1000 | Loss: 0.00000959
Iteration 139/1000 | Loss: 0.00000959
Iteration 140/1000 | Loss: 0.00000959
Iteration 141/1000 | Loss: 0.00000959
Iteration 142/1000 | Loss: 0.00000959
Iteration 143/1000 | Loss: 0.00000959
Iteration 144/1000 | Loss: 0.00000959
Iteration 145/1000 | Loss: 0.00000959
Iteration 146/1000 | Loss: 0.00000959
Iteration 147/1000 | Loss: 0.00000959
Iteration 148/1000 | Loss: 0.00000959
Iteration 149/1000 | Loss: 0.00000959
Iteration 150/1000 | Loss: 0.00000958
Iteration 151/1000 | Loss: 0.00000958
Iteration 152/1000 | Loss: 0.00000958
Iteration 153/1000 | Loss: 0.00000958
Iteration 154/1000 | Loss: 0.00000958
Iteration 155/1000 | Loss: 0.00000958
Iteration 156/1000 | Loss: 0.00000958
Iteration 157/1000 | Loss: 0.00000958
Iteration 158/1000 | Loss: 0.00000958
Iteration 159/1000 | Loss: 0.00000958
Iteration 160/1000 | Loss: 0.00000958
Iteration 161/1000 | Loss: 0.00000958
Iteration 162/1000 | Loss: 0.00000958
Iteration 163/1000 | Loss: 0.00000958
Iteration 164/1000 | Loss: 0.00000958
Iteration 165/1000 | Loss: 0.00000958
Iteration 166/1000 | Loss: 0.00000958
Iteration 167/1000 | Loss: 0.00000958
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [9.578820936440025e-06, 9.578820936440025e-06, 9.578820936440025e-06, 9.578820936440025e-06, 9.578820936440025e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.578820936440025e-06

Optimization complete. Final v2v error: 2.6953535079956055 mm

Highest mean error: 2.962123394012451 mm for frame 49

Lowest mean error: 2.493574619293213 mm for frame 36

Saving results

Total time: 45.138736963272095
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01082652
Iteration 2/25 | Loss: 0.00159689
Iteration 3/25 | Loss: 0.00128540
Iteration 4/25 | Loss: 0.00127158
Iteration 5/25 | Loss: 0.00126166
Iteration 6/25 | Loss: 0.00125006
Iteration 7/25 | Loss: 0.00124218
Iteration 8/25 | Loss: 0.00124629
Iteration 9/25 | Loss: 0.00125044
Iteration 10/25 | Loss: 0.00124536
Iteration 11/25 | Loss: 0.00123914
Iteration 12/25 | Loss: 0.00123534
Iteration 13/25 | Loss: 0.00123779
Iteration 14/25 | Loss: 0.00123628
Iteration 15/25 | Loss: 0.00123555
Iteration 16/25 | Loss: 0.00123389
Iteration 17/25 | Loss: 0.00122534
Iteration 18/25 | Loss: 0.00123480
Iteration 19/25 | Loss: 0.00122778
Iteration 20/25 | Loss: 0.00121949
Iteration 21/25 | Loss: 0.00122085
Iteration 22/25 | Loss: 0.00121835
Iteration 23/25 | Loss: 0.00121835
Iteration 24/25 | Loss: 0.00121835
Iteration 25/25 | Loss: 0.00121835

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.97892207
Iteration 2/25 | Loss: 0.00155089
Iteration 3/25 | Loss: 0.00153040
Iteration 4/25 | Loss: 0.00153040
Iteration 5/25 | Loss: 0.00153040
Iteration 6/25 | Loss: 0.00153040
Iteration 7/25 | Loss: 0.00153040
Iteration 8/25 | Loss: 0.00153040
Iteration 9/25 | Loss: 0.00153040
Iteration 10/25 | Loss: 0.00153040
Iteration 11/25 | Loss: 0.00153040
Iteration 12/25 | Loss: 0.00153040
Iteration 13/25 | Loss: 0.00153040
Iteration 14/25 | Loss: 0.00153040
Iteration 15/25 | Loss: 0.00153040
Iteration 16/25 | Loss: 0.00153040
Iteration 17/25 | Loss: 0.00153040
Iteration 18/25 | Loss: 0.00153040
Iteration 19/25 | Loss: 0.00153040
Iteration 20/25 | Loss: 0.00153040
Iteration 21/25 | Loss: 0.00153040
Iteration 22/25 | Loss: 0.00153040
Iteration 23/25 | Loss: 0.00153040
Iteration 24/25 | Loss: 0.00153040
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0015303960535675287, 0.0015303960535675287, 0.0015303960535675287, 0.0015303960535675287, 0.0015303960535675287]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015303960535675287

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00153040
Iteration 2/1000 | Loss: 0.00023645
Iteration 3/1000 | Loss: 0.00030347
Iteration 4/1000 | Loss: 0.00019004
Iteration 5/1000 | Loss: 0.00017765
Iteration 6/1000 | Loss: 0.00021058
Iteration 7/1000 | Loss: 0.00004372
Iteration 8/1000 | Loss: 0.00004139
Iteration 9/1000 | Loss: 0.00003421
Iteration 10/1000 | Loss: 0.00002917
Iteration 11/1000 | Loss: 0.00002607
Iteration 12/1000 | Loss: 0.00002764
Iteration 13/1000 | Loss: 0.00002300
Iteration 14/1000 | Loss: 0.00002222
Iteration 15/1000 | Loss: 0.00005380
Iteration 16/1000 | Loss: 0.00002082
Iteration 17/1000 | Loss: 0.00002053
Iteration 18/1000 | Loss: 0.00004891
Iteration 19/1000 | Loss: 0.00003511
Iteration 20/1000 | Loss: 0.00020402
Iteration 21/1000 | Loss: 0.00002432
Iteration 22/1000 | Loss: 0.00002115
Iteration 23/1000 | Loss: 0.00002215
Iteration 24/1000 | Loss: 0.00002716
Iteration 25/1000 | Loss: 0.00002836
Iteration 26/1000 | Loss: 0.00001806
Iteration 27/1000 | Loss: 0.00001778
Iteration 28/1000 | Loss: 0.00002890
Iteration 29/1000 | Loss: 0.00001758
Iteration 30/1000 | Loss: 0.00001756
Iteration 31/1000 | Loss: 0.00001754
Iteration 32/1000 | Loss: 0.00001754
Iteration 33/1000 | Loss: 0.00001748
Iteration 34/1000 | Loss: 0.00003961
Iteration 35/1000 | Loss: 0.00001739
Iteration 36/1000 | Loss: 0.00001737
Iteration 37/1000 | Loss: 0.00001734
Iteration 38/1000 | Loss: 0.00001730
Iteration 39/1000 | Loss: 0.00003074
Iteration 40/1000 | Loss: 0.00003074
Iteration 41/1000 | Loss: 0.00007673
Iteration 42/1000 | Loss: 0.00001982
Iteration 43/1000 | Loss: 0.00001709
Iteration 44/1000 | Loss: 0.00001709
Iteration 45/1000 | Loss: 0.00001706
Iteration 46/1000 | Loss: 0.00001706
Iteration 47/1000 | Loss: 0.00001705
Iteration 48/1000 | Loss: 0.00001705
Iteration 49/1000 | Loss: 0.00001705
Iteration 50/1000 | Loss: 0.00001705
Iteration 51/1000 | Loss: 0.00001705
Iteration 52/1000 | Loss: 0.00001705
Iteration 53/1000 | Loss: 0.00001705
Iteration 54/1000 | Loss: 0.00001705
Iteration 55/1000 | Loss: 0.00001705
Iteration 56/1000 | Loss: 0.00001704
Iteration 57/1000 | Loss: 0.00001704
Iteration 58/1000 | Loss: 0.00001704
Iteration 59/1000 | Loss: 0.00001704
Iteration 60/1000 | Loss: 0.00001704
Iteration 61/1000 | Loss: 0.00001704
Iteration 62/1000 | Loss: 0.00001704
Iteration 63/1000 | Loss: 0.00001704
Iteration 64/1000 | Loss: 0.00001704
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 64. Stopping optimization.
Last 5 losses: [1.704154237813782e-05, 1.704154237813782e-05, 1.704154237813782e-05, 1.704154237813782e-05, 1.704154237813782e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.704154237813782e-05

Optimization complete. Final v2v error: 3.4641082286834717 mm

Highest mean error: 4.5023603439331055 mm for frame 19

Lowest mean error: 3.2842209339141846 mm for frame 1

Saving results

Total time: 91.37459564208984
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00797624
Iteration 2/25 | Loss: 0.00157544
Iteration 3/25 | Loss: 0.00126776
Iteration 4/25 | Loss: 0.00124070
Iteration 5/25 | Loss: 0.00123269
Iteration 6/25 | Loss: 0.00123067
Iteration 7/25 | Loss: 0.00123067
Iteration 8/25 | Loss: 0.00123067
Iteration 9/25 | Loss: 0.00123067
Iteration 10/25 | Loss: 0.00123067
Iteration 11/25 | Loss: 0.00123067
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012306717690080404, 0.0012306717690080404, 0.0012306717690080404, 0.0012306717690080404, 0.0012306717690080404]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012306717690080404

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11227036
Iteration 2/25 | Loss: 0.00199142
Iteration 3/25 | Loss: 0.00199142
Iteration 4/25 | Loss: 0.00199142
Iteration 5/25 | Loss: 0.00199142
Iteration 6/25 | Loss: 0.00199142
Iteration 7/25 | Loss: 0.00199142
Iteration 8/25 | Loss: 0.00199142
Iteration 9/25 | Loss: 0.00199142
Iteration 10/25 | Loss: 0.00199142
Iteration 11/25 | Loss: 0.00199142
Iteration 12/25 | Loss: 0.00199142
Iteration 13/25 | Loss: 0.00199142
Iteration 14/25 | Loss: 0.00199142
Iteration 15/25 | Loss: 0.00199142
Iteration 16/25 | Loss: 0.00199142
Iteration 17/25 | Loss: 0.00199142
Iteration 18/25 | Loss: 0.00199142
Iteration 19/25 | Loss: 0.00199142
Iteration 20/25 | Loss: 0.00199142
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001991420052945614, 0.001991420052945614, 0.001991420052945614, 0.001991420052945614, 0.001991420052945614]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001991420052945614

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00199142
Iteration 2/1000 | Loss: 0.00007389
Iteration 3/1000 | Loss: 0.00004664
Iteration 4/1000 | Loss: 0.00003571
Iteration 5/1000 | Loss: 0.00003334
Iteration 6/1000 | Loss: 0.00003176
Iteration 7/1000 | Loss: 0.00003056
Iteration 8/1000 | Loss: 0.00002970
Iteration 9/1000 | Loss: 0.00002918
Iteration 10/1000 | Loss: 0.00002880
Iteration 11/1000 | Loss: 0.00002843
Iteration 12/1000 | Loss: 0.00002807
Iteration 13/1000 | Loss: 0.00002784
Iteration 14/1000 | Loss: 0.00002762
Iteration 15/1000 | Loss: 0.00002761
Iteration 16/1000 | Loss: 0.00002745
Iteration 17/1000 | Loss: 0.00002731
Iteration 18/1000 | Loss: 0.00002716
Iteration 19/1000 | Loss: 0.00002707
Iteration 20/1000 | Loss: 0.00002705
Iteration 21/1000 | Loss: 0.00002705
Iteration 22/1000 | Loss: 0.00002702
Iteration 23/1000 | Loss: 0.00002699
Iteration 24/1000 | Loss: 0.00002693
Iteration 25/1000 | Loss: 0.00002692
Iteration 26/1000 | Loss: 0.00002691
Iteration 27/1000 | Loss: 0.00002688
Iteration 28/1000 | Loss: 0.00002688
Iteration 29/1000 | Loss: 0.00002688
Iteration 30/1000 | Loss: 0.00002688
Iteration 31/1000 | Loss: 0.00002687
Iteration 32/1000 | Loss: 0.00002682
Iteration 33/1000 | Loss: 0.00002682
Iteration 34/1000 | Loss: 0.00002680
Iteration 35/1000 | Loss: 0.00002679
Iteration 36/1000 | Loss: 0.00002678
Iteration 37/1000 | Loss: 0.00002678
Iteration 38/1000 | Loss: 0.00002677
Iteration 39/1000 | Loss: 0.00002677
Iteration 40/1000 | Loss: 0.00002677
Iteration 41/1000 | Loss: 0.00002676
Iteration 42/1000 | Loss: 0.00002675
Iteration 43/1000 | Loss: 0.00002675
Iteration 44/1000 | Loss: 0.00002675
Iteration 45/1000 | Loss: 0.00002674
Iteration 46/1000 | Loss: 0.00002674
Iteration 47/1000 | Loss: 0.00002674
Iteration 48/1000 | Loss: 0.00002673
Iteration 49/1000 | Loss: 0.00002673
Iteration 50/1000 | Loss: 0.00002673
Iteration 51/1000 | Loss: 0.00002673
Iteration 52/1000 | Loss: 0.00002672
Iteration 53/1000 | Loss: 0.00002672
Iteration 54/1000 | Loss: 0.00002672
Iteration 55/1000 | Loss: 0.00002671
Iteration 56/1000 | Loss: 0.00002671
Iteration 57/1000 | Loss: 0.00002671
Iteration 58/1000 | Loss: 0.00002671
Iteration 59/1000 | Loss: 0.00002670
Iteration 60/1000 | Loss: 0.00002670
Iteration 61/1000 | Loss: 0.00002670
Iteration 62/1000 | Loss: 0.00002670
Iteration 63/1000 | Loss: 0.00002669
Iteration 64/1000 | Loss: 0.00002669
Iteration 65/1000 | Loss: 0.00002669
Iteration 66/1000 | Loss: 0.00002668
Iteration 67/1000 | Loss: 0.00002668
Iteration 68/1000 | Loss: 0.00002668
Iteration 69/1000 | Loss: 0.00002667
Iteration 70/1000 | Loss: 0.00002667
Iteration 71/1000 | Loss: 0.00002667
Iteration 72/1000 | Loss: 0.00002667
Iteration 73/1000 | Loss: 0.00002667
Iteration 74/1000 | Loss: 0.00002667
Iteration 75/1000 | Loss: 0.00002666
Iteration 76/1000 | Loss: 0.00002666
Iteration 77/1000 | Loss: 0.00002666
Iteration 78/1000 | Loss: 0.00002666
Iteration 79/1000 | Loss: 0.00002666
Iteration 80/1000 | Loss: 0.00002666
Iteration 81/1000 | Loss: 0.00002665
Iteration 82/1000 | Loss: 0.00002665
Iteration 83/1000 | Loss: 0.00002665
Iteration 84/1000 | Loss: 0.00002665
Iteration 85/1000 | Loss: 0.00002665
Iteration 86/1000 | Loss: 0.00002664
Iteration 87/1000 | Loss: 0.00002664
Iteration 88/1000 | Loss: 0.00002664
Iteration 89/1000 | Loss: 0.00002663
Iteration 90/1000 | Loss: 0.00002663
Iteration 91/1000 | Loss: 0.00002663
Iteration 92/1000 | Loss: 0.00002663
Iteration 93/1000 | Loss: 0.00002663
Iteration 94/1000 | Loss: 0.00002663
Iteration 95/1000 | Loss: 0.00002662
Iteration 96/1000 | Loss: 0.00002662
Iteration 97/1000 | Loss: 0.00002662
Iteration 98/1000 | Loss: 0.00002662
Iteration 99/1000 | Loss: 0.00002662
Iteration 100/1000 | Loss: 0.00002662
Iteration 101/1000 | Loss: 0.00002662
Iteration 102/1000 | Loss: 0.00002662
Iteration 103/1000 | Loss: 0.00002662
Iteration 104/1000 | Loss: 0.00002661
Iteration 105/1000 | Loss: 0.00002661
Iteration 106/1000 | Loss: 0.00002661
Iteration 107/1000 | Loss: 0.00002661
Iteration 108/1000 | Loss: 0.00002661
Iteration 109/1000 | Loss: 0.00002661
Iteration 110/1000 | Loss: 0.00002661
Iteration 111/1000 | Loss: 0.00002661
Iteration 112/1000 | Loss: 0.00002660
Iteration 113/1000 | Loss: 0.00002660
Iteration 114/1000 | Loss: 0.00002660
Iteration 115/1000 | Loss: 0.00002660
Iteration 116/1000 | Loss: 0.00002660
Iteration 117/1000 | Loss: 0.00002660
Iteration 118/1000 | Loss: 0.00002660
Iteration 119/1000 | Loss: 0.00002660
Iteration 120/1000 | Loss: 0.00002660
Iteration 121/1000 | Loss: 0.00002660
Iteration 122/1000 | Loss: 0.00002660
Iteration 123/1000 | Loss: 0.00002660
Iteration 124/1000 | Loss: 0.00002660
Iteration 125/1000 | Loss: 0.00002660
Iteration 126/1000 | Loss: 0.00002660
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [2.6603493097354658e-05, 2.6603493097354658e-05, 2.6603493097354658e-05, 2.6603493097354658e-05, 2.6603493097354658e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6603493097354658e-05

Optimization complete. Final v2v error: 4.161512851715088 mm

Highest mean error: 5.148519992828369 mm for frame 158

Lowest mean error: 2.649766683578491 mm for frame 198

Saving results

Total time: 46.18472194671631
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00765285
Iteration 2/25 | Loss: 0.00195046
Iteration 3/25 | Loss: 0.00138054
Iteration 4/25 | Loss: 0.00124141
Iteration 5/25 | Loss: 0.00123430
Iteration 6/25 | Loss: 0.00119606
Iteration 7/25 | Loss: 0.00117658
Iteration 8/25 | Loss: 0.00118396
Iteration 9/25 | Loss: 0.00117264
Iteration 10/25 | Loss: 0.00116113
Iteration 11/25 | Loss: 0.00116558
Iteration 12/25 | Loss: 0.00116548
Iteration 13/25 | Loss: 0.00115342
Iteration 14/25 | Loss: 0.00115424
Iteration 15/25 | Loss: 0.00115212
Iteration 16/25 | Loss: 0.00115208
Iteration 17/25 | Loss: 0.00115208
Iteration 18/25 | Loss: 0.00115207
Iteration 19/25 | Loss: 0.00115207
Iteration 20/25 | Loss: 0.00115207
Iteration 21/25 | Loss: 0.00115207
Iteration 22/25 | Loss: 0.00115206
Iteration 23/25 | Loss: 0.00115383
Iteration 24/25 | Loss: 0.00115221
Iteration 25/25 | Loss: 0.00115249

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.93477130
Iteration 2/25 | Loss: 0.00194804
Iteration 3/25 | Loss: 0.00194804
Iteration 4/25 | Loss: 0.00187883
Iteration 5/25 | Loss: 0.00187883
Iteration 6/25 | Loss: 0.00187882
Iteration 7/25 | Loss: 0.00187882
Iteration 8/25 | Loss: 0.00187882
Iteration 9/25 | Loss: 0.00187882
Iteration 10/25 | Loss: 0.00187882
Iteration 11/25 | Loss: 0.00187882
Iteration 12/25 | Loss: 0.00187882
Iteration 13/25 | Loss: 0.00187882
Iteration 14/25 | Loss: 0.00187882
Iteration 15/25 | Loss: 0.00187882
Iteration 16/25 | Loss: 0.00187882
Iteration 17/25 | Loss: 0.00187882
Iteration 18/25 | Loss: 0.00187882
Iteration 19/25 | Loss: 0.00187882
Iteration 20/25 | Loss: 0.00187882
Iteration 21/25 | Loss: 0.00187882
Iteration 22/25 | Loss: 0.00187882
Iteration 23/25 | Loss: 0.00187882
Iteration 24/25 | Loss: 0.00187882
Iteration 25/25 | Loss: 0.00187882

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00187882
Iteration 2/1000 | Loss: 0.00003194
Iteration 3/1000 | Loss: 0.00008405
Iteration 4/1000 | Loss: 0.00028640
Iteration 5/1000 | Loss: 0.00002901
Iteration 6/1000 | Loss: 0.00001915
Iteration 7/1000 | Loss: 0.00001684
Iteration 8/1000 | Loss: 0.00002212
Iteration 9/1000 | Loss: 0.00004969
Iteration 10/1000 | Loss: 0.00002094
Iteration 11/1000 | Loss: 0.00018769
Iteration 12/1000 | Loss: 0.00002941
Iteration 13/1000 | Loss: 0.00001530
Iteration 14/1000 | Loss: 0.00001750
Iteration 15/1000 | Loss: 0.00005555
Iteration 16/1000 | Loss: 0.00001510
Iteration 17/1000 | Loss: 0.00002354
Iteration 18/1000 | Loss: 0.00007139
Iteration 19/1000 | Loss: 0.00012315
Iteration 20/1000 | Loss: 0.00003325
Iteration 21/1000 | Loss: 0.00003461
Iteration 22/1000 | Loss: 0.00002328
Iteration 23/1000 | Loss: 0.00002256
Iteration 24/1000 | Loss: 0.00001768
Iteration 25/1000 | Loss: 0.00001646
Iteration 26/1000 | Loss: 0.00001672
Iteration 27/1000 | Loss: 0.00001458
Iteration 28/1000 | Loss: 0.00001943
Iteration 29/1000 | Loss: 0.00001895
Iteration 30/1000 | Loss: 0.00002013
Iteration 31/1000 | Loss: 0.00001610
Iteration 32/1000 | Loss: 0.00002819
Iteration 33/1000 | Loss: 0.00001737
Iteration 34/1000 | Loss: 0.00001853
Iteration 35/1000 | Loss: 0.00001445
Iteration 36/1000 | Loss: 0.00001433
Iteration 37/1000 | Loss: 0.00001433
Iteration 38/1000 | Loss: 0.00001433
Iteration 39/1000 | Loss: 0.00002578
Iteration 40/1000 | Loss: 0.00002124
Iteration 41/1000 | Loss: 0.00001508
Iteration 42/1000 | Loss: 0.00001422
Iteration 43/1000 | Loss: 0.00001422
Iteration 44/1000 | Loss: 0.00001422
Iteration 45/1000 | Loss: 0.00001422
Iteration 46/1000 | Loss: 0.00001422
Iteration 47/1000 | Loss: 0.00001421
Iteration 48/1000 | Loss: 0.00001480
Iteration 49/1000 | Loss: 0.00001419
Iteration 50/1000 | Loss: 0.00001419
Iteration 51/1000 | Loss: 0.00001419
Iteration 52/1000 | Loss: 0.00001419
Iteration 53/1000 | Loss: 0.00001419
Iteration 54/1000 | Loss: 0.00001419
Iteration 55/1000 | Loss: 0.00001419
Iteration 56/1000 | Loss: 0.00001419
Iteration 57/1000 | Loss: 0.00001418
Iteration 58/1000 | Loss: 0.00001418
Iteration 59/1000 | Loss: 0.00001418
Iteration 60/1000 | Loss: 0.00001418
Iteration 61/1000 | Loss: 0.00001418
Iteration 62/1000 | Loss: 0.00001418
Iteration 63/1000 | Loss: 0.00001418
Iteration 64/1000 | Loss: 0.00001417
Iteration 65/1000 | Loss: 0.00001417
Iteration 66/1000 | Loss: 0.00001417
Iteration 67/1000 | Loss: 0.00001417
Iteration 68/1000 | Loss: 0.00001419
Iteration 69/1000 | Loss: 0.00001694
Iteration 70/1000 | Loss: 0.00003835
Iteration 71/1000 | Loss: 0.00002101
Iteration 72/1000 | Loss: 0.00001917
Iteration 73/1000 | Loss: 0.00002088
Iteration 74/1000 | Loss: 0.00001407
Iteration 75/1000 | Loss: 0.00001564
Iteration 76/1000 | Loss: 0.00002317
Iteration 77/1000 | Loss: 0.00001624
Iteration 78/1000 | Loss: 0.00001618
Iteration 79/1000 | Loss: 0.00001518
Iteration 80/1000 | Loss: 0.00001976
Iteration 81/1000 | Loss: 0.00001404
Iteration 82/1000 | Loss: 0.00001404
Iteration 83/1000 | Loss: 0.00001404
Iteration 84/1000 | Loss: 0.00001404
Iteration 85/1000 | Loss: 0.00001404
Iteration 86/1000 | Loss: 0.00001404
Iteration 87/1000 | Loss: 0.00001403
Iteration 88/1000 | Loss: 0.00001403
Iteration 89/1000 | Loss: 0.00001403
Iteration 90/1000 | Loss: 0.00001403
Iteration 91/1000 | Loss: 0.00001403
Iteration 92/1000 | Loss: 0.00001402
Iteration 93/1000 | Loss: 0.00001402
Iteration 94/1000 | Loss: 0.00001402
Iteration 95/1000 | Loss: 0.00001402
Iteration 96/1000 | Loss: 0.00001401
Iteration 97/1000 | Loss: 0.00001401
Iteration 98/1000 | Loss: 0.00001401
Iteration 99/1000 | Loss: 0.00001401
Iteration 100/1000 | Loss: 0.00001401
Iteration 101/1000 | Loss: 0.00001401
Iteration 102/1000 | Loss: 0.00001401
Iteration 103/1000 | Loss: 0.00001402
Iteration 104/1000 | Loss: 0.00001401
Iteration 105/1000 | Loss: 0.00001401
Iteration 106/1000 | Loss: 0.00001400
Iteration 107/1000 | Loss: 0.00001400
Iteration 108/1000 | Loss: 0.00001400
Iteration 109/1000 | Loss: 0.00001400
Iteration 110/1000 | Loss: 0.00001400
Iteration 111/1000 | Loss: 0.00001400
Iteration 112/1000 | Loss: 0.00001400
Iteration 113/1000 | Loss: 0.00001400
Iteration 114/1000 | Loss: 0.00001400
Iteration 115/1000 | Loss: 0.00001400
Iteration 116/1000 | Loss: 0.00001400
Iteration 117/1000 | Loss: 0.00001400
Iteration 118/1000 | Loss: 0.00001400
Iteration 119/1000 | Loss: 0.00001400
Iteration 120/1000 | Loss: 0.00001400
Iteration 121/1000 | Loss: 0.00001400
Iteration 122/1000 | Loss: 0.00001400
Iteration 123/1000 | Loss: 0.00001400
Iteration 124/1000 | Loss: 0.00001399
Iteration 125/1000 | Loss: 0.00001412
Iteration 126/1000 | Loss: 0.00001400
Iteration 127/1000 | Loss: 0.00001399
Iteration 128/1000 | Loss: 0.00001399
Iteration 129/1000 | Loss: 0.00001399
Iteration 130/1000 | Loss: 0.00001398
Iteration 131/1000 | Loss: 0.00001398
Iteration 132/1000 | Loss: 0.00001398
Iteration 133/1000 | Loss: 0.00001398
Iteration 134/1000 | Loss: 0.00001398
Iteration 135/1000 | Loss: 0.00001398
Iteration 136/1000 | Loss: 0.00001398
Iteration 137/1000 | Loss: 0.00001398
Iteration 138/1000 | Loss: 0.00001568
Iteration 139/1000 | Loss: 0.00001407
Iteration 140/1000 | Loss: 0.00001398
Iteration 141/1000 | Loss: 0.00001397
Iteration 142/1000 | Loss: 0.00001397
Iteration 143/1000 | Loss: 0.00001397
Iteration 144/1000 | Loss: 0.00001397
Iteration 145/1000 | Loss: 0.00001397
Iteration 146/1000 | Loss: 0.00001397
Iteration 147/1000 | Loss: 0.00001397
Iteration 148/1000 | Loss: 0.00001397
Iteration 149/1000 | Loss: 0.00001397
Iteration 150/1000 | Loss: 0.00001397
Iteration 151/1000 | Loss: 0.00001397
Iteration 152/1000 | Loss: 0.00001397
Iteration 153/1000 | Loss: 0.00001397
Iteration 154/1000 | Loss: 0.00001397
Iteration 155/1000 | Loss: 0.00001397
Iteration 156/1000 | Loss: 0.00001397
Iteration 157/1000 | Loss: 0.00001397
Iteration 158/1000 | Loss: 0.00001397
Iteration 159/1000 | Loss: 0.00001397
Iteration 160/1000 | Loss: 0.00001397
Iteration 161/1000 | Loss: 0.00001397
Iteration 162/1000 | Loss: 0.00001397
Iteration 163/1000 | Loss: 0.00001397
Iteration 164/1000 | Loss: 0.00001397
Iteration 165/1000 | Loss: 0.00001397
Iteration 166/1000 | Loss: 0.00001397
Iteration 167/1000 | Loss: 0.00001397
Iteration 168/1000 | Loss: 0.00001397
Iteration 169/1000 | Loss: 0.00001397
Iteration 170/1000 | Loss: 0.00001397
Iteration 171/1000 | Loss: 0.00001397
Iteration 172/1000 | Loss: 0.00001397
Iteration 173/1000 | Loss: 0.00001397
Iteration 174/1000 | Loss: 0.00001397
Iteration 175/1000 | Loss: 0.00001397
Iteration 176/1000 | Loss: 0.00001397
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.3965240214020014e-05, 1.3965240214020014e-05, 1.3965240214020014e-05, 1.3965240214020014e-05, 1.3965240214020014e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3965240214020014e-05

Optimization complete. Final v2v error: 2.6569578647613525 mm

Highest mean error: 21.67445945739746 mm for frame 131

Lowest mean error: 2.3439836502075195 mm for frame 147

Saving results

Total time: 109.06001138687134
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00459365
Iteration 2/25 | Loss: 0.00124460
Iteration 3/25 | Loss: 0.00118732
Iteration 4/25 | Loss: 0.00117665
Iteration 5/25 | Loss: 0.00117393
Iteration 6/25 | Loss: 0.00117364
Iteration 7/25 | Loss: 0.00117364
Iteration 8/25 | Loss: 0.00117364
Iteration 9/25 | Loss: 0.00117364
Iteration 10/25 | Loss: 0.00117364
Iteration 11/25 | Loss: 0.00117364
Iteration 12/25 | Loss: 0.00117364
Iteration 13/25 | Loss: 0.00117364
Iteration 14/25 | Loss: 0.00117364
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0011736367596313357, 0.0011736367596313357, 0.0011736367596313357, 0.0011736367596313357, 0.0011736367596313357]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011736367596313357

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26216495
Iteration 2/25 | Loss: 0.00191564
Iteration 3/25 | Loss: 0.00191563
Iteration 4/25 | Loss: 0.00191563
Iteration 5/25 | Loss: 0.00191563
Iteration 6/25 | Loss: 0.00191563
Iteration 7/25 | Loss: 0.00191563
Iteration 8/25 | Loss: 0.00191563
Iteration 9/25 | Loss: 0.00191563
Iteration 10/25 | Loss: 0.00191563
Iteration 11/25 | Loss: 0.00191563
Iteration 12/25 | Loss: 0.00191563
Iteration 13/25 | Loss: 0.00191563
Iteration 14/25 | Loss: 0.00191563
Iteration 15/25 | Loss: 0.00191563
Iteration 16/25 | Loss: 0.00191563
Iteration 17/25 | Loss: 0.00191563
Iteration 18/25 | Loss: 0.00191563
Iteration 19/25 | Loss: 0.00191563
Iteration 20/25 | Loss: 0.00191563
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0019156289054080844, 0.0019156289054080844, 0.0019156289054080844, 0.0019156289054080844, 0.0019156289054080844]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019156289054080844

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00191563
Iteration 2/1000 | Loss: 0.00003126
Iteration 3/1000 | Loss: 0.00002195
Iteration 4/1000 | Loss: 0.00001911
Iteration 5/1000 | Loss: 0.00001813
Iteration 6/1000 | Loss: 0.00001736
Iteration 7/1000 | Loss: 0.00001686
Iteration 8/1000 | Loss: 0.00001643
Iteration 9/1000 | Loss: 0.00001605
Iteration 10/1000 | Loss: 0.00001566
Iteration 11/1000 | Loss: 0.00001543
Iteration 12/1000 | Loss: 0.00001523
Iteration 13/1000 | Loss: 0.00001520
Iteration 14/1000 | Loss: 0.00001505
Iteration 15/1000 | Loss: 0.00001501
Iteration 16/1000 | Loss: 0.00001501
Iteration 17/1000 | Loss: 0.00001500
Iteration 18/1000 | Loss: 0.00001500
Iteration 19/1000 | Loss: 0.00001497
Iteration 20/1000 | Loss: 0.00001491
Iteration 21/1000 | Loss: 0.00001486
Iteration 22/1000 | Loss: 0.00001483
Iteration 23/1000 | Loss: 0.00001482
Iteration 24/1000 | Loss: 0.00001480
Iteration 25/1000 | Loss: 0.00001479
Iteration 26/1000 | Loss: 0.00001478
Iteration 27/1000 | Loss: 0.00001476
Iteration 28/1000 | Loss: 0.00001475
Iteration 29/1000 | Loss: 0.00001475
Iteration 30/1000 | Loss: 0.00001474
Iteration 31/1000 | Loss: 0.00001474
Iteration 32/1000 | Loss: 0.00001473
Iteration 33/1000 | Loss: 0.00001473
Iteration 34/1000 | Loss: 0.00001470
Iteration 35/1000 | Loss: 0.00001470
Iteration 36/1000 | Loss: 0.00001469
Iteration 37/1000 | Loss: 0.00001467
Iteration 38/1000 | Loss: 0.00001467
Iteration 39/1000 | Loss: 0.00001466
Iteration 40/1000 | Loss: 0.00001466
Iteration 41/1000 | Loss: 0.00001466
Iteration 42/1000 | Loss: 0.00001466
Iteration 43/1000 | Loss: 0.00001465
Iteration 44/1000 | Loss: 0.00001465
Iteration 45/1000 | Loss: 0.00001464
Iteration 46/1000 | Loss: 0.00001460
Iteration 47/1000 | Loss: 0.00001460
Iteration 48/1000 | Loss: 0.00001460
Iteration 49/1000 | Loss: 0.00001460
Iteration 50/1000 | Loss: 0.00001458
Iteration 51/1000 | Loss: 0.00001457
Iteration 52/1000 | Loss: 0.00001457
Iteration 53/1000 | Loss: 0.00001457
Iteration 54/1000 | Loss: 0.00001456
Iteration 55/1000 | Loss: 0.00001456
Iteration 56/1000 | Loss: 0.00001455
Iteration 57/1000 | Loss: 0.00001455
Iteration 58/1000 | Loss: 0.00001455
Iteration 59/1000 | Loss: 0.00001454
Iteration 60/1000 | Loss: 0.00001454
Iteration 61/1000 | Loss: 0.00001453
Iteration 62/1000 | Loss: 0.00001453
Iteration 63/1000 | Loss: 0.00001451
Iteration 64/1000 | Loss: 0.00001451
Iteration 65/1000 | Loss: 0.00001451
Iteration 66/1000 | Loss: 0.00001451
Iteration 67/1000 | Loss: 0.00001451
Iteration 68/1000 | Loss: 0.00001451
Iteration 69/1000 | Loss: 0.00001451
Iteration 70/1000 | Loss: 0.00001451
Iteration 71/1000 | Loss: 0.00001450
Iteration 72/1000 | Loss: 0.00001450
Iteration 73/1000 | Loss: 0.00001450
Iteration 74/1000 | Loss: 0.00001449
Iteration 75/1000 | Loss: 0.00001449
Iteration 76/1000 | Loss: 0.00001449
Iteration 77/1000 | Loss: 0.00001448
Iteration 78/1000 | Loss: 0.00001448
Iteration 79/1000 | Loss: 0.00001448
Iteration 80/1000 | Loss: 0.00001447
Iteration 81/1000 | Loss: 0.00001447
Iteration 82/1000 | Loss: 0.00001447
Iteration 83/1000 | Loss: 0.00001447
Iteration 84/1000 | Loss: 0.00001447
Iteration 85/1000 | Loss: 0.00001446
Iteration 86/1000 | Loss: 0.00001446
Iteration 87/1000 | Loss: 0.00001446
Iteration 88/1000 | Loss: 0.00001446
Iteration 89/1000 | Loss: 0.00001446
Iteration 90/1000 | Loss: 0.00001445
Iteration 91/1000 | Loss: 0.00001445
Iteration 92/1000 | Loss: 0.00001445
Iteration 93/1000 | Loss: 0.00001444
Iteration 94/1000 | Loss: 0.00001444
Iteration 95/1000 | Loss: 0.00001444
Iteration 96/1000 | Loss: 0.00001444
Iteration 97/1000 | Loss: 0.00001444
Iteration 98/1000 | Loss: 0.00001444
Iteration 99/1000 | Loss: 0.00001444
Iteration 100/1000 | Loss: 0.00001444
Iteration 101/1000 | Loss: 0.00001444
Iteration 102/1000 | Loss: 0.00001443
Iteration 103/1000 | Loss: 0.00001443
Iteration 104/1000 | Loss: 0.00001443
Iteration 105/1000 | Loss: 0.00001443
Iteration 106/1000 | Loss: 0.00001443
Iteration 107/1000 | Loss: 0.00001442
Iteration 108/1000 | Loss: 0.00001442
Iteration 109/1000 | Loss: 0.00001442
Iteration 110/1000 | Loss: 0.00001441
Iteration 111/1000 | Loss: 0.00001441
Iteration 112/1000 | Loss: 0.00001441
Iteration 113/1000 | Loss: 0.00001441
Iteration 114/1000 | Loss: 0.00001441
Iteration 115/1000 | Loss: 0.00001441
Iteration 116/1000 | Loss: 0.00001441
Iteration 117/1000 | Loss: 0.00001440
Iteration 118/1000 | Loss: 0.00001440
Iteration 119/1000 | Loss: 0.00001440
Iteration 120/1000 | Loss: 0.00001440
Iteration 121/1000 | Loss: 0.00001440
Iteration 122/1000 | Loss: 0.00001440
Iteration 123/1000 | Loss: 0.00001440
Iteration 124/1000 | Loss: 0.00001440
Iteration 125/1000 | Loss: 0.00001440
Iteration 126/1000 | Loss: 0.00001440
Iteration 127/1000 | Loss: 0.00001440
Iteration 128/1000 | Loss: 0.00001440
Iteration 129/1000 | Loss: 0.00001440
Iteration 130/1000 | Loss: 0.00001440
Iteration 131/1000 | Loss: 0.00001440
Iteration 132/1000 | Loss: 0.00001440
Iteration 133/1000 | Loss: 0.00001440
Iteration 134/1000 | Loss: 0.00001440
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.439593415852869e-05, 1.439593415852869e-05, 1.439593415852869e-05, 1.439593415852869e-05, 1.439593415852869e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.439593415852869e-05

Optimization complete. Final v2v error: 3.209007978439331 mm

Highest mean error: 3.596224069595337 mm for frame 193

Lowest mean error: 2.602674722671509 mm for frame 65

Saving results

Total time: 44.16583442687988
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00793608
Iteration 2/25 | Loss: 0.00136762
Iteration 3/25 | Loss: 0.00115615
Iteration 4/25 | Loss: 0.00114064
Iteration 5/25 | Loss: 0.00113595
Iteration 6/25 | Loss: 0.00113440
Iteration 7/25 | Loss: 0.00113440
Iteration 8/25 | Loss: 0.00113440
Iteration 9/25 | Loss: 0.00113440
Iteration 10/25 | Loss: 0.00113440
Iteration 11/25 | Loss: 0.00113440
Iteration 12/25 | Loss: 0.00113440
Iteration 13/25 | Loss: 0.00113440
Iteration 14/25 | Loss: 0.00113440
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0011343964142724872, 0.0011343964142724872, 0.0011343964142724872, 0.0011343964142724872, 0.0011343964142724872]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011343964142724872

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.12020576
Iteration 2/25 | Loss: 0.00193191
Iteration 3/25 | Loss: 0.00193191
Iteration 4/25 | Loss: 0.00193190
Iteration 5/25 | Loss: 0.00193190
Iteration 6/25 | Loss: 0.00193190
Iteration 7/25 | Loss: 0.00193190
Iteration 8/25 | Loss: 0.00193190
Iteration 9/25 | Loss: 0.00193190
Iteration 10/25 | Loss: 0.00193190
Iteration 11/25 | Loss: 0.00193190
Iteration 12/25 | Loss: 0.00193190
Iteration 13/25 | Loss: 0.00193190
Iteration 14/25 | Loss: 0.00193190
Iteration 15/25 | Loss: 0.00193190
Iteration 16/25 | Loss: 0.00193190
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001931900274939835, 0.001931900274939835, 0.001931900274939835, 0.001931900274939835, 0.001931900274939835]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001931900274939835

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00193190
Iteration 2/1000 | Loss: 0.00003968
Iteration 3/1000 | Loss: 0.00002452
Iteration 4/1000 | Loss: 0.00001980
Iteration 5/1000 | Loss: 0.00001697
Iteration 6/1000 | Loss: 0.00001549
Iteration 7/1000 | Loss: 0.00001441
Iteration 8/1000 | Loss: 0.00001368
Iteration 9/1000 | Loss: 0.00001312
Iteration 10/1000 | Loss: 0.00001268
Iteration 11/1000 | Loss: 0.00001238
Iteration 12/1000 | Loss: 0.00001219
Iteration 13/1000 | Loss: 0.00001216
Iteration 14/1000 | Loss: 0.00001193
Iteration 15/1000 | Loss: 0.00001181
Iteration 16/1000 | Loss: 0.00001181
Iteration 17/1000 | Loss: 0.00001180
Iteration 18/1000 | Loss: 0.00001171
Iteration 19/1000 | Loss: 0.00001170
Iteration 20/1000 | Loss: 0.00001166
Iteration 21/1000 | Loss: 0.00001162
Iteration 22/1000 | Loss: 0.00001155
Iteration 23/1000 | Loss: 0.00001155
Iteration 24/1000 | Loss: 0.00001154
Iteration 25/1000 | Loss: 0.00001153
Iteration 26/1000 | Loss: 0.00001153
Iteration 27/1000 | Loss: 0.00001152
Iteration 28/1000 | Loss: 0.00001151
Iteration 29/1000 | Loss: 0.00001147
Iteration 30/1000 | Loss: 0.00001144
Iteration 31/1000 | Loss: 0.00001143
Iteration 32/1000 | Loss: 0.00001143
Iteration 33/1000 | Loss: 0.00001142
Iteration 34/1000 | Loss: 0.00001142
Iteration 35/1000 | Loss: 0.00001140
Iteration 36/1000 | Loss: 0.00001140
Iteration 37/1000 | Loss: 0.00001138
Iteration 38/1000 | Loss: 0.00001138
Iteration 39/1000 | Loss: 0.00001138
Iteration 40/1000 | Loss: 0.00001138
Iteration 41/1000 | Loss: 0.00001138
Iteration 42/1000 | Loss: 0.00001138
Iteration 43/1000 | Loss: 0.00001138
Iteration 44/1000 | Loss: 0.00001138
Iteration 45/1000 | Loss: 0.00001138
Iteration 46/1000 | Loss: 0.00001138
Iteration 47/1000 | Loss: 0.00001138
Iteration 48/1000 | Loss: 0.00001137
Iteration 49/1000 | Loss: 0.00001135
Iteration 50/1000 | Loss: 0.00001131
Iteration 51/1000 | Loss: 0.00001131
Iteration 52/1000 | Loss: 0.00001129
Iteration 53/1000 | Loss: 0.00001128
Iteration 54/1000 | Loss: 0.00001128
Iteration 55/1000 | Loss: 0.00001127
Iteration 56/1000 | Loss: 0.00001127
Iteration 57/1000 | Loss: 0.00001127
Iteration 58/1000 | Loss: 0.00001126
Iteration 59/1000 | Loss: 0.00001126
Iteration 60/1000 | Loss: 0.00001126
Iteration 61/1000 | Loss: 0.00001126
Iteration 62/1000 | Loss: 0.00001126
Iteration 63/1000 | Loss: 0.00001126
Iteration 64/1000 | Loss: 0.00001125
Iteration 65/1000 | Loss: 0.00001125
Iteration 66/1000 | Loss: 0.00001125
Iteration 67/1000 | Loss: 0.00001125
Iteration 68/1000 | Loss: 0.00001125
Iteration 69/1000 | Loss: 0.00001125
Iteration 70/1000 | Loss: 0.00001124
Iteration 71/1000 | Loss: 0.00001124
Iteration 72/1000 | Loss: 0.00001124
Iteration 73/1000 | Loss: 0.00001123
Iteration 74/1000 | Loss: 0.00001123
Iteration 75/1000 | Loss: 0.00001123
Iteration 76/1000 | Loss: 0.00001123
Iteration 77/1000 | Loss: 0.00001123
Iteration 78/1000 | Loss: 0.00001123
Iteration 79/1000 | Loss: 0.00001123
Iteration 80/1000 | Loss: 0.00001123
Iteration 81/1000 | Loss: 0.00001123
Iteration 82/1000 | Loss: 0.00001123
Iteration 83/1000 | Loss: 0.00001122
Iteration 84/1000 | Loss: 0.00001122
Iteration 85/1000 | Loss: 0.00001122
Iteration 86/1000 | Loss: 0.00001122
Iteration 87/1000 | Loss: 0.00001122
Iteration 88/1000 | Loss: 0.00001122
Iteration 89/1000 | Loss: 0.00001121
Iteration 90/1000 | Loss: 0.00001121
Iteration 91/1000 | Loss: 0.00001121
Iteration 92/1000 | Loss: 0.00001121
Iteration 93/1000 | Loss: 0.00001121
Iteration 94/1000 | Loss: 0.00001121
Iteration 95/1000 | Loss: 0.00001121
Iteration 96/1000 | Loss: 0.00001120
Iteration 97/1000 | Loss: 0.00001120
Iteration 98/1000 | Loss: 0.00001120
Iteration 99/1000 | Loss: 0.00001120
Iteration 100/1000 | Loss: 0.00001120
Iteration 101/1000 | Loss: 0.00001120
Iteration 102/1000 | Loss: 0.00001120
Iteration 103/1000 | Loss: 0.00001119
Iteration 104/1000 | Loss: 0.00001119
Iteration 105/1000 | Loss: 0.00001119
Iteration 106/1000 | Loss: 0.00001119
Iteration 107/1000 | Loss: 0.00001119
Iteration 108/1000 | Loss: 0.00001119
Iteration 109/1000 | Loss: 0.00001119
Iteration 110/1000 | Loss: 0.00001118
Iteration 111/1000 | Loss: 0.00001118
Iteration 112/1000 | Loss: 0.00001118
Iteration 113/1000 | Loss: 0.00001118
Iteration 114/1000 | Loss: 0.00001118
Iteration 115/1000 | Loss: 0.00001118
Iteration 116/1000 | Loss: 0.00001118
Iteration 117/1000 | Loss: 0.00001117
Iteration 118/1000 | Loss: 0.00001117
Iteration 119/1000 | Loss: 0.00001117
Iteration 120/1000 | Loss: 0.00001117
Iteration 121/1000 | Loss: 0.00001117
Iteration 122/1000 | Loss: 0.00001116
Iteration 123/1000 | Loss: 0.00001116
Iteration 124/1000 | Loss: 0.00001116
Iteration 125/1000 | Loss: 0.00001116
Iteration 126/1000 | Loss: 0.00001116
Iteration 127/1000 | Loss: 0.00001116
Iteration 128/1000 | Loss: 0.00001116
Iteration 129/1000 | Loss: 0.00001116
Iteration 130/1000 | Loss: 0.00001116
Iteration 131/1000 | Loss: 0.00001116
Iteration 132/1000 | Loss: 0.00001116
Iteration 133/1000 | Loss: 0.00001116
Iteration 134/1000 | Loss: 0.00001115
Iteration 135/1000 | Loss: 0.00001115
Iteration 136/1000 | Loss: 0.00001115
Iteration 137/1000 | Loss: 0.00001115
Iteration 138/1000 | Loss: 0.00001115
Iteration 139/1000 | Loss: 0.00001114
Iteration 140/1000 | Loss: 0.00001114
Iteration 141/1000 | Loss: 0.00001114
Iteration 142/1000 | Loss: 0.00001114
Iteration 143/1000 | Loss: 0.00001114
Iteration 144/1000 | Loss: 0.00001113
Iteration 145/1000 | Loss: 0.00001113
Iteration 146/1000 | Loss: 0.00001113
Iteration 147/1000 | Loss: 0.00001113
Iteration 148/1000 | Loss: 0.00001113
Iteration 149/1000 | Loss: 0.00001113
Iteration 150/1000 | Loss: 0.00001113
Iteration 151/1000 | Loss: 0.00001113
Iteration 152/1000 | Loss: 0.00001113
Iteration 153/1000 | Loss: 0.00001113
Iteration 154/1000 | Loss: 0.00001113
Iteration 155/1000 | Loss: 0.00001113
Iteration 156/1000 | Loss: 0.00001113
Iteration 157/1000 | Loss: 0.00001113
Iteration 158/1000 | Loss: 0.00001113
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [1.1128606274724007e-05, 1.1128606274724007e-05, 1.1128606274724007e-05, 1.1128606274724007e-05, 1.1128606274724007e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1128606274724007e-05

Optimization complete. Final v2v error: 2.755925178527832 mm

Highest mean error: 4.097780227661133 mm for frame 77

Lowest mean error: 2.2499921321868896 mm for frame 108

Saving results

Total time: 42.05794310569763
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00873104
Iteration 2/25 | Loss: 0.00151454
Iteration 3/25 | Loss: 0.00122383
Iteration 4/25 | Loss: 0.00119456
Iteration 5/25 | Loss: 0.00118989
Iteration 6/25 | Loss: 0.00118921
Iteration 7/25 | Loss: 0.00118921
Iteration 8/25 | Loss: 0.00118921
Iteration 9/25 | Loss: 0.00118921
Iteration 10/25 | Loss: 0.00118921
Iteration 11/25 | Loss: 0.00118921
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011892092879861593, 0.0011892092879861593, 0.0011892092879861593, 0.0011892092879861593, 0.0011892092879861593]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011892092879861593

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.82740909
Iteration 2/25 | Loss: 0.00107219
Iteration 3/25 | Loss: 0.00107218
Iteration 4/25 | Loss: 0.00107218
Iteration 5/25 | Loss: 0.00107218
Iteration 6/25 | Loss: 0.00107218
Iteration 7/25 | Loss: 0.00107218
Iteration 8/25 | Loss: 0.00107218
Iteration 9/25 | Loss: 0.00107218
Iteration 10/25 | Loss: 0.00107218
Iteration 11/25 | Loss: 0.00107218
Iteration 12/25 | Loss: 0.00107218
Iteration 13/25 | Loss: 0.00107218
Iteration 14/25 | Loss: 0.00107218
Iteration 15/25 | Loss: 0.00107218
Iteration 16/25 | Loss: 0.00107218
Iteration 17/25 | Loss: 0.00107218
Iteration 18/25 | Loss: 0.00107218
Iteration 19/25 | Loss: 0.00107218
Iteration 20/25 | Loss: 0.00107218
Iteration 21/25 | Loss: 0.00107218
Iteration 22/25 | Loss: 0.00107218
Iteration 23/25 | Loss: 0.00107218
Iteration 24/25 | Loss: 0.00107218
Iteration 25/25 | Loss: 0.00107218

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107218
Iteration 2/1000 | Loss: 0.00003854
Iteration 3/1000 | Loss: 0.00003264
Iteration 4/1000 | Loss: 0.00002942
Iteration 5/1000 | Loss: 0.00002846
Iteration 6/1000 | Loss: 0.00002742
Iteration 7/1000 | Loss: 0.00002687
Iteration 8/1000 | Loss: 0.00002653
Iteration 9/1000 | Loss: 0.00002627
Iteration 10/1000 | Loss: 0.00002611
Iteration 11/1000 | Loss: 0.00002600
Iteration 12/1000 | Loss: 0.00002583
Iteration 13/1000 | Loss: 0.00002570
Iteration 14/1000 | Loss: 0.00002563
Iteration 15/1000 | Loss: 0.00002557
Iteration 16/1000 | Loss: 0.00002553
Iteration 17/1000 | Loss: 0.00002553
Iteration 18/1000 | Loss: 0.00002553
Iteration 19/1000 | Loss: 0.00002553
Iteration 20/1000 | Loss: 0.00002552
Iteration 21/1000 | Loss: 0.00002549
Iteration 22/1000 | Loss: 0.00002549
Iteration 23/1000 | Loss: 0.00002549
Iteration 24/1000 | Loss: 0.00002549
Iteration 25/1000 | Loss: 0.00002549
Iteration 26/1000 | Loss: 0.00002549
Iteration 27/1000 | Loss: 0.00002549
Iteration 28/1000 | Loss: 0.00002549
Iteration 29/1000 | Loss: 0.00002549
Iteration 30/1000 | Loss: 0.00002549
Iteration 31/1000 | Loss: 0.00002548
Iteration 32/1000 | Loss: 0.00002548
Iteration 33/1000 | Loss: 0.00002548
Iteration 34/1000 | Loss: 0.00002548
Iteration 35/1000 | Loss: 0.00002548
Iteration 36/1000 | Loss: 0.00002547
Iteration 37/1000 | Loss: 0.00002547
Iteration 38/1000 | Loss: 0.00002547
Iteration 39/1000 | Loss: 0.00002547
Iteration 40/1000 | Loss: 0.00002547
Iteration 41/1000 | Loss: 0.00002547
Iteration 42/1000 | Loss: 0.00002547
Iteration 43/1000 | Loss: 0.00002547
Iteration 44/1000 | Loss: 0.00002547
Iteration 45/1000 | Loss: 0.00002547
Iteration 46/1000 | Loss: 0.00002547
Iteration 47/1000 | Loss: 0.00002547
Iteration 48/1000 | Loss: 0.00002547
Iteration 49/1000 | Loss: 0.00002547
Iteration 50/1000 | Loss: 0.00002547
Iteration 51/1000 | Loss: 0.00002547
Iteration 52/1000 | Loss: 0.00002547
Iteration 53/1000 | Loss: 0.00002547
Iteration 54/1000 | Loss: 0.00002547
Iteration 55/1000 | Loss: 0.00002547
Iteration 56/1000 | Loss: 0.00002547
Iteration 57/1000 | Loss: 0.00002547
Iteration 58/1000 | Loss: 0.00002547
Iteration 59/1000 | Loss: 0.00002547
Iteration 60/1000 | Loss: 0.00002547
Iteration 61/1000 | Loss: 0.00002547
Iteration 62/1000 | Loss: 0.00002547
Iteration 63/1000 | Loss: 0.00002547
Iteration 64/1000 | Loss: 0.00002547
Iteration 65/1000 | Loss: 0.00002547
Iteration 66/1000 | Loss: 0.00002547
Iteration 67/1000 | Loss: 0.00002547
Iteration 68/1000 | Loss: 0.00002547
Iteration 69/1000 | Loss: 0.00002547
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 69. Stopping optimization.
Last 5 losses: [2.5472521883784793e-05, 2.5472521883784793e-05, 2.5472521883784793e-05, 2.5472521883784793e-05, 2.5472521883784793e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5472521883784793e-05

Optimization complete. Final v2v error: 4.334980487823486 mm

Highest mean error: 4.620730876922607 mm for frame 123

Lowest mean error: 4.027337074279785 mm for frame 81

Saving results

Total time: 30.05677080154419
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00523008
Iteration 2/25 | Loss: 0.00142663
Iteration 3/25 | Loss: 0.00130260
Iteration 4/25 | Loss: 0.00129122
Iteration 5/25 | Loss: 0.00129070
Iteration 6/25 | Loss: 0.00129070
Iteration 7/25 | Loss: 0.00129070
Iteration 8/25 | Loss: 0.00129070
Iteration 9/25 | Loss: 0.00129070
Iteration 10/25 | Loss: 0.00129070
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012907012132927775, 0.0012907012132927775, 0.0012907012132927775, 0.0012907012132927775, 0.0012907012132927775]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012907012132927775

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.69060111
Iteration 2/25 | Loss: 0.00138721
Iteration 3/25 | Loss: 0.00138720
Iteration 4/25 | Loss: 0.00138720
Iteration 5/25 | Loss: 0.00138720
Iteration 6/25 | Loss: 0.00138720
Iteration 7/25 | Loss: 0.00138720
Iteration 8/25 | Loss: 0.00138720
Iteration 9/25 | Loss: 0.00138720
Iteration 10/25 | Loss: 0.00138720
Iteration 11/25 | Loss: 0.00138720
Iteration 12/25 | Loss: 0.00138720
Iteration 13/25 | Loss: 0.00138720
Iteration 14/25 | Loss: 0.00138720
Iteration 15/25 | Loss: 0.00138720
Iteration 16/25 | Loss: 0.00138720
Iteration 17/25 | Loss: 0.00138720
Iteration 18/25 | Loss: 0.00138720
Iteration 19/25 | Loss: 0.00138720
Iteration 20/25 | Loss: 0.00138720
Iteration 21/25 | Loss: 0.00138720
Iteration 22/25 | Loss: 0.00138720
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0013871986884623766, 0.0013871986884623766, 0.0013871986884623766, 0.0013871986884623766, 0.0013871986884623766]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013871986884623766

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00138720
Iteration 2/1000 | Loss: 0.00004399
Iteration 3/1000 | Loss: 0.00003833
Iteration 4/1000 | Loss: 0.00003583
Iteration 5/1000 | Loss: 0.00003481
Iteration 6/1000 | Loss: 0.00003437
Iteration 7/1000 | Loss: 0.00003365
Iteration 8/1000 | Loss: 0.00003313
Iteration 9/1000 | Loss: 0.00003256
Iteration 10/1000 | Loss: 0.00003209
Iteration 11/1000 | Loss: 0.00003171
Iteration 12/1000 | Loss: 0.00003119
Iteration 13/1000 | Loss: 0.00003080
Iteration 14/1000 | Loss: 0.00003059
Iteration 15/1000 | Loss: 0.00003033
Iteration 16/1000 | Loss: 0.00003012
Iteration 17/1000 | Loss: 0.00003006
Iteration 18/1000 | Loss: 0.00003003
Iteration 19/1000 | Loss: 0.00003002
Iteration 20/1000 | Loss: 0.00003001
Iteration 21/1000 | Loss: 0.00003000
Iteration 22/1000 | Loss: 0.00002990
Iteration 23/1000 | Loss: 0.00002990
Iteration 24/1000 | Loss: 0.00002990
Iteration 25/1000 | Loss: 0.00002987
Iteration 26/1000 | Loss: 0.00002983
Iteration 27/1000 | Loss: 0.00002982
Iteration 28/1000 | Loss: 0.00002981
Iteration 29/1000 | Loss: 0.00002981
Iteration 30/1000 | Loss: 0.00002981
Iteration 31/1000 | Loss: 0.00002981
Iteration 32/1000 | Loss: 0.00002981
Iteration 33/1000 | Loss: 0.00002981
Iteration 34/1000 | Loss: 0.00002980
Iteration 35/1000 | Loss: 0.00002980
Iteration 36/1000 | Loss: 0.00002973
Iteration 37/1000 | Loss: 0.00002973
Iteration 38/1000 | Loss: 0.00002973
Iteration 39/1000 | Loss: 0.00002972
Iteration 40/1000 | Loss: 0.00002972
Iteration 41/1000 | Loss: 0.00002972
Iteration 42/1000 | Loss: 0.00002972
Iteration 43/1000 | Loss: 0.00002972
Iteration 44/1000 | Loss: 0.00002972
Iteration 45/1000 | Loss: 0.00002972
Iteration 46/1000 | Loss: 0.00002971
Iteration 47/1000 | Loss: 0.00002971
Iteration 48/1000 | Loss: 0.00002971
Iteration 49/1000 | Loss: 0.00002971
Iteration 50/1000 | Loss: 0.00002971
Iteration 51/1000 | Loss: 0.00002970
Iteration 52/1000 | Loss: 0.00002970
Iteration 53/1000 | Loss: 0.00002970
Iteration 54/1000 | Loss: 0.00002970
Iteration 55/1000 | Loss: 0.00002970
Iteration 56/1000 | Loss: 0.00002969
Iteration 57/1000 | Loss: 0.00002969
Iteration 58/1000 | Loss: 0.00002969
Iteration 59/1000 | Loss: 0.00002969
Iteration 60/1000 | Loss: 0.00002968
Iteration 61/1000 | Loss: 0.00002968
Iteration 62/1000 | Loss: 0.00002968
Iteration 63/1000 | Loss: 0.00002965
Iteration 64/1000 | Loss: 0.00002964
Iteration 65/1000 | Loss: 0.00002964
Iteration 66/1000 | Loss: 0.00002964
Iteration 67/1000 | Loss: 0.00002964
Iteration 68/1000 | Loss: 0.00002964
Iteration 69/1000 | Loss: 0.00002964
Iteration 70/1000 | Loss: 0.00002964
Iteration 71/1000 | Loss: 0.00002964
Iteration 72/1000 | Loss: 0.00002964
Iteration 73/1000 | Loss: 0.00002964
Iteration 74/1000 | Loss: 0.00002964
Iteration 75/1000 | Loss: 0.00002964
Iteration 76/1000 | Loss: 0.00002964
Iteration 77/1000 | Loss: 0.00002964
Iteration 78/1000 | Loss: 0.00002964
Iteration 79/1000 | Loss: 0.00002964
Iteration 80/1000 | Loss: 0.00002964
Iteration 81/1000 | Loss: 0.00002964
Iteration 82/1000 | Loss: 0.00002964
Iteration 83/1000 | Loss: 0.00002964
Iteration 84/1000 | Loss: 0.00002964
Iteration 85/1000 | Loss: 0.00002964
Iteration 86/1000 | Loss: 0.00002964
Iteration 87/1000 | Loss: 0.00002964
Iteration 88/1000 | Loss: 0.00002964
Iteration 89/1000 | Loss: 0.00002964
Iteration 90/1000 | Loss: 0.00002964
Iteration 91/1000 | Loss: 0.00002964
Iteration 92/1000 | Loss: 0.00002964
Iteration 93/1000 | Loss: 0.00002964
Iteration 94/1000 | Loss: 0.00002964
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [2.9637178158736788e-05, 2.9637178158736788e-05, 2.9637178158736788e-05, 2.9637178158736788e-05, 2.9637178158736788e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9637178158736788e-05

Optimization complete. Final v2v error: 4.272449016571045 mm

Highest mean error: 4.290920257568359 mm for frame 227

Lowest mean error: 4.246443271636963 mm for frame 46

Saving results

Total time: 43.58407711982727
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00768863
Iteration 2/25 | Loss: 0.00157760
Iteration 3/25 | Loss: 0.00129588
Iteration 4/25 | Loss: 0.00124162
Iteration 5/25 | Loss: 0.00122939
Iteration 6/25 | Loss: 0.00120472
Iteration 7/25 | Loss: 0.00119509
Iteration 8/25 | Loss: 0.00119157
Iteration 9/25 | Loss: 0.00119021
Iteration 10/25 | Loss: 0.00118935
Iteration 11/25 | Loss: 0.00118915
Iteration 12/25 | Loss: 0.00118902
Iteration 13/25 | Loss: 0.00118889
Iteration 14/25 | Loss: 0.00118858
Iteration 15/25 | Loss: 0.00118694
Iteration 16/25 | Loss: 0.00119000
Iteration 17/25 | Loss: 0.00118539
Iteration 18/25 | Loss: 0.00118469
Iteration 19/25 | Loss: 0.00118421
Iteration 20/25 | Loss: 0.00118350
Iteration 21/25 | Loss: 0.00118625
Iteration 22/25 | Loss: 0.00118465
Iteration 23/25 | Loss: 0.00118388
Iteration 24/25 | Loss: 0.00118110
Iteration 25/25 | Loss: 0.00118039

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.14773107
Iteration 2/25 | Loss: 0.00290521
Iteration 3/25 | Loss: 0.00290521
Iteration 4/25 | Loss: 0.00290521
Iteration 5/25 | Loss: 0.00290521
Iteration 6/25 | Loss: 0.00290520
Iteration 7/25 | Loss: 0.00290520
Iteration 8/25 | Loss: 0.00290520
Iteration 9/25 | Loss: 0.00290520
Iteration 10/25 | Loss: 0.00290520
Iteration 11/25 | Loss: 0.00290520
Iteration 12/25 | Loss: 0.00290520
Iteration 13/25 | Loss: 0.00290520
Iteration 14/25 | Loss: 0.00290520
Iteration 15/25 | Loss: 0.00290520
Iteration 16/25 | Loss: 0.00290520
Iteration 17/25 | Loss: 0.00290520
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002905203029513359, 0.002905203029513359, 0.002905203029513359, 0.002905203029513359, 0.002905203029513359]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002905203029513359

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00290520
Iteration 2/1000 | Loss: 0.00026127
Iteration 3/1000 | Loss: 0.00013233
Iteration 4/1000 | Loss: 0.00032869
Iteration 5/1000 | Loss: 0.00005797
Iteration 6/1000 | Loss: 0.00026748
Iteration 7/1000 | Loss: 0.00015626
Iteration 8/1000 | Loss: 0.00014046
Iteration 9/1000 | Loss: 0.00003292
Iteration 10/1000 | Loss: 0.00002938
Iteration 11/1000 | Loss: 0.00064200
Iteration 12/1000 | Loss: 0.00003750
Iteration 13/1000 | Loss: 0.00002749
Iteration 14/1000 | Loss: 0.00002420
Iteration 15/1000 | Loss: 0.00002169
Iteration 16/1000 | Loss: 0.00002002
Iteration 17/1000 | Loss: 0.00001884
Iteration 18/1000 | Loss: 0.00001828
Iteration 19/1000 | Loss: 0.00001793
Iteration 20/1000 | Loss: 0.00001769
Iteration 21/1000 | Loss: 0.00001751
Iteration 22/1000 | Loss: 0.00001733
Iteration 23/1000 | Loss: 0.00001717
Iteration 24/1000 | Loss: 0.00001716
Iteration 25/1000 | Loss: 0.00001696
Iteration 26/1000 | Loss: 0.00001687
Iteration 27/1000 | Loss: 0.00001682
Iteration 28/1000 | Loss: 0.00001676
Iteration 29/1000 | Loss: 0.00001665
Iteration 30/1000 | Loss: 0.00001663
Iteration 31/1000 | Loss: 0.00001662
Iteration 32/1000 | Loss: 0.00001657
Iteration 33/1000 | Loss: 0.00001657
Iteration 34/1000 | Loss: 0.00001656
Iteration 35/1000 | Loss: 0.00001656
Iteration 36/1000 | Loss: 0.00001655
Iteration 37/1000 | Loss: 0.00001655
Iteration 38/1000 | Loss: 0.00001655
Iteration 39/1000 | Loss: 0.00001655
Iteration 40/1000 | Loss: 0.00001655
Iteration 41/1000 | Loss: 0.00001655
Iteration 42/1000 | Loss: 0.00001655
Iteration 43/1000 | Loss: 0.00001654
Iteration 44/1000 | Loss: 0.00001654
Iteration 45/1000 | Loss: 0.00001654
Iteration 46/1000 | Loss: 0.00001654
Iteration 47/1000 | Loss: 0.00001654
Iteration 48/1000 | Loss: 0.00001653
Iteration 49/1000 | Loss: 0.00001653
Iteration 50/1000 | Loss: 0.00001653
Iteration 51/1000 | Loss: 0.00001653
Iteration 52/1000 | Loss: 0.00001653
Iteration 53/1000 | Loss: 0.00001652
Iteration 54/1000 | Loss: 0.00001652
Iteration 55/1000 | Loss: 0.00001651
Iteration 56/1000 | Loss: 0.00001651
Iteration 57/1000 | Loss: 0.00001651
Iteration 58/1000 | Loss: 0.00001650
Iteration 59/1000 | Loss: 0.00001650
Iteration 60/1000 | Loss: 0.00001650
Iteration 61/1000 | Loss: 0.00001650
Iteration 62/1000 | Loss: 0.00001650
Iteration 63/1000 | Loss: 0.00001650
Iteration 64/1000 | Loss: 0.00001649
Iteration 65/1000 | Loss: 0.00001649
Iteration 66/1000 | Loss: 0.00001649
Iteration 67/1000 | Loss: 0.00001648
Iteration 68/1000 | Loss: 0.00001648
Iteration 69/1000 | Loss: 0.00001647
Iteration 70/1000 | Loss: 0.00001647
Iteration 71/1000 | Loss: 0.00001647
Iteration 72/1000 | Loss: 0.00001646
Iteration 73/1000 | Loss: 0.00001646
Iteration 74/1000 | Loss: 0.00001646
Iteration 75/1000 | Loss: 0.00001646
Iteration 76/1000 | Loss: 0.00001645
Iteration 77/1000 | Loss: 0.00001645
Iteration 78/1000 | Loss: 0.00001645
Iteration 79/1000 | Loss: 0.00001645
Iteration 80/1000 | Loss: 0.00001645
Iteration 81/1000 | Loss: 0.00001645
Iteration 82/1000 | Loss: 0.00001645
Iteration 83/1000 | Loss: 0.00001645
Iteration 84/1000 | Loss: 0.00001644
Iteration 85/1000 | Loss: 0.00001644
Iteration 86/1000 | Loss: 0.00001644
Iteration 87/1000 | Loss: 0.00001644
Iteration 88/1000 | Loss: 0.00001644
Iteration 89/1000 | Loss: 0.00001643
Iteration 90/1000 | Loss: 0.00001643
Iteration 91/1000 | Loss: 0.00001642
Iteration 92/1000 | Loss: 0.00001642
Iteration 93/1000 | Loss: 0.00001642
Iteration 94/1000 | Loss: 0.00001642
Iteration 95/1000 | Loss: 0.00001642
Iteration 96/1000 | Loss: 0.00001641
Iteration 97/1000 | Loss: 0.00001641
Iteration 98/1000 | Loss: 0.00001640
Iteration 99/1000 | Loss: 0.00001638
Iteration 100/1000 | Loss: 0.00001638
Iteration 101/1000 | Loss: 0.00001638
Iteration 102/1000 | Loss: 0.00001638
Iteration 103/1000 | Loss: 0.00001638
Iteration 104/1000 | Loss: 0.00001638
Iteration 105/1000 | Loss: 0.00001638
Iteration 106/1000 | Loss: 0.00001637
Iteration 107/1000 | Loss: 0.00001637
Iteration 108/1000 | Loss: 0.00001637
Iteration 109/1000 | Loss: 0.00001636
Iteration 110/1000 | Loss: 0.00001636
Iteration 111/1000 | Loss: 0.00001635
Iteration 112/1000 | Loss: 0.00001635
Iteration 113/1000 | Loss: 0.00001635
Iteration 114/1000 | Loss: 0.00001635
Iteration 115/1000 | Loss: 0.00001634
Iteration 116/1000 | Loss: 0.00001634
Iteration 117/1000 | Loss: 0.00001634
Iteration 118/1000 | Loss: 0.00001634
Iteration 119/1000 | Loss: 0.00001634
Iteration 120/1000 | Loss: 0.00001634
Iteration 121/1000 | Loss: 0.00001634
Iteration 122/1000 | Loss: 0.00001634
Iteration 123/1000 | Loss: 0.00001633
Iteration 124/1000 | Loss: 0.00001633
Iteration 125/1000 | Loss: 0.00001633
Iteration 126/1000 | Loss: 0.00001633
Iteration 127/1000 | Loss: 0.00001633
Iteration 128/1000 | Loss: 0.00001633
Iteration 129/1000 | Loss: 0.00001633
Iteration 130/1000 | Loss: 0.00001633
Iteration 131/1000 | Loss: 0.00001633
Iteration 132/1000 | Loss: 0.00001633
Iteration 133/1000 | Loss: 0.00001633
Iteration 134/1000 | Loss: 0.00001632
Iteration 135/1000 | Loss: 0.00001632
Iteration 136/1000 | Loss: 0.00001632
Iteration 137/1000 | Loss: 0.00001632
Iteration 138/1000 | Loss: 0.00001632
Iteration 139/1000 | Loss: 0.00001632
Iteration 140/1000 | Loss: 0.00001632
Iteration 141/1000 | Loss: 0.00001632
Iteration 142/1000 | Loss: 0.00001631
Iteration 143/1000 | Loss: 0.00001631
Iteration 144/1000 | Loss: 0.00001631
Iteration 145/1000 | Loss: 0.00001631
Iteration 146/1000 | Loss: 0.00001631
Iteration 147/1000 | Loss: 0.00001631
Iteration 148/1000 | Loss: 0.00001630
Iteration 149/1000 | Loss: 0.00001630
Iteration 150/1000 | Loss: 0.00001630
Iteration 151/1000 | Loss: 0.00001630
Iteration 152/1000 | Loss: 0.00001630
Iteration 153/1000 | Loss: 0.00001630
Iteration 154/1000 | Loss: 0.00001630
Iteration 155/1000 | Loss: 0.00001630
Iteration 156/1000 | Loss: 0.00001630
Iteration 157/1000 | Loss: 0.00001630
Iteration 158/1000 | Loss: 0.00001630
Iteration 159/1000 | Loss: 0.00001629
Iteration 160/1000 | Loss: 0.00001629
Iteration 161/1000 | Loss: 0.00001629
Iteration 162/1000 | Loss: 0.00001629
Iteration 163/1000 | Loss: 0.00001629
Iteration 164/1000 | Loss: 0.00001629
Iteration 165/1000 | Loss: 0.00001629
Iteration 166/1000 | Loss: 0.00001629
Iteration 167/1000 | Loss: 0.00001629
Iteration 168/1000 | Loss: 0.00001629
Iteration 169/1000 | Loss: 0.00001629
Iteration 170/1000 | Loss: 0.00001629
Iteration 171/1000 | Loss: 0.00001629
Iteration 172/1000 | Loss: 0.00001629
Iteration 173/1000 | Loss: 0.00001629
Iteration 174/1000 | Loss: 0.00001629
Iteration 175/1000 | Loss: 0.00001629
Iteration 176/1000 | Loss: 0.00001629
Iteration 177/1000 | Loss: 0.00001629
Iteration 178/1000 | Loss: 0.00001629
Iteration 179/1000 | Loss: 0.00001629
Iteration 180/1000 | Loss: 0.00001629
Iteration 181/1000 | Loss: 0.00001629
Iteration 182/1000 | Loss: 0.00001629
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [1.628617974347435e-05, 1.628617974347435e-05, 1.628617974347435e-05, 1.628617974347435e-05, 1.628617974347435e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.628617974347435e-05

Optimization complete. Final v2v error: 3.4238553047180176 mm

Highest mean error: 3.930798292160034 mm for frame 183

Lowest mean error: 3.0045392513275146 mm for frame 20

Saving results

Total time: 103.81882119178772
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01017180
Iteration 2/25 | Loss: 0.00164285
Iteration 3/25 | Loss: 0.00146752
Iteration 4/25 | Loss: 0.00120453
Iteration 5/25 | Loss: 0.00119812
Iteration 6/25 | Loss: 0.00119602
Iteration 7/25 | Loss: 0.00120112
Iteration 8/25 | Loss: 0.00120685
Iteration 9/25 | Loss: 0.00120813
Iteration 10/25 | Loss: 0.00120043
Iteration 11/25 | Loss: 0.00119674
Iteration 12/25 | Loss: 0.00119580
Iteration 13/25 | Loss: 0.00120166
Iteration 14/25 | Loss: 0.00120523
Iteration 15/25 | Loss: 0.00120098
Iteration 16/25 | Loss: 0.00119570
Iteration 17/25 | Loss: 0.00118937
Iteration 18/25 | Loss: 0.00118741
Iteration 19/25 | Loss: 0.00118452
Iteration 20/25 | Loss: 0.00118409
Iteration 21/25 | Loss: 0.00118396
Iteration 22/25 | Loss: 0.00118386
Iteration 23/25 | Loss: 0.00118383
Iteration 24/25 | Loss: 0.00118383
Iteration 25/25 | Loss: 0.00118383

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31512809
Iteration 2/25 | Loss: 0.00184995
Iteration 3/25 | Loss: 0.00184995
Iteration 4/25 | Loss: 0.00184995
Iteration 5/25 | Loss: 0.00184995
Iteration 6/25 | Loss: 0.00184995
Iteration 7/25 | Loss: 0.00184995
Iteration 8/25 | Loss: 0.00184995
Iteration 9/25 | Loss: 0.00184995
Iteration 10/25 | Loss: 0.00184995
Iteration 11/25 | Loss: 0.00184995
Iteration 12/25 | Loss: 0.00184995
Iteration 13/25 | Loss: 0.00184995
Iteration 14/25 | Loss: 0.00184995
Iteration 15/25 | Loss: 0.00184995
Iteration 16/25 | Loss: 0.00184995
Iteration 17/25 | Loss: 0.00184995
Iteration 18/25 | Loss: 0.00184995
Iteration 19/25 | Loss: 0.00184995
Iteration 20/25 | Loss: 0.00184995
Iteration 21/25 | Loss: 0.00184995
Iteration 22/25 | Loss: 0.00184995
Iteration 23/25 | Loss: 0.00184995
Iteration 24/25 | Loss: 0.00184995
Iteration 25/25 | Loss: 0.00184995

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00184995
Iteration 2/1000 | Loss: 0.00002360
Iteration 3/1000 | Loss: 0.00001593
Iteration 4/1000 | Loss: 0.00001401
Iteration 5/1000 | Loss: 0.00001326
Iteration 6/1000 | Loss: 0.00001255
Iteration 7/1000 | Loss: 0.00001184
Iteration 8/1000 | Loss: 0.00001151
Iteration 9/1000 | Loss: 0.00001138
Iteration 10/1000 | Loss: 0.00001117
Iteration 11/1000 | Loss: 0.00001101
Iteration 12/1000 | Loss: 0.00001092
Iteration 13/1000 | Loss: 0.00001079
Iteration 14/1000 | Loss: 0.00001078
Iteration 15/1000 | Loss: 0.00001068
Iteration 16/1000 | Loss: 0.00001066
Iteration 17/1000 | Loss: 0.00001064
Iteration 18/1000 | Loss: 0.00001064
Iteration 19/1000 | Loss: 0.00001063
Iteration 20/1000 | Loss: 0.00001062
Iteration 21/1000 | Loss: 0.00001058
Iteration 22/1000 | Loss: 0.00001058
Iteration 23/1000 | Loss: 0.00001058
Iteration 24/1000 | Loss: 0.00001057
Iteration 25/1000 | Loss: 0.00001057
Iteration 26/1000 | Loss: 0.00001053
Iteration 27/1000 | Loss: 0.00001053
Iteration 28/1000 | Loss: 0.00001052
Iteration 29/1000 | Loss: 0.00001052
Iteration 30/1000 | Loss: 0.00001051
Iteration 31/1000 | Loss: 0.00001050
Iteration 32/1000 | Loss: 0.00001049
Iteration 33/1000 | Loss: 0.00001048
Iteration 34/1000 | Loss: 0.00001048
Iteration 35/1000 | Loss: 0.00001048
Iteration 36/1000 | Loss: 0.00001044
Iteration 37/1000 | Loss: 0.00001043
Iteration 38/1000 | Loss: 0.00001043
Iteration 39/1000 | Loss: 0.00001042
Iteration 40/1000 | Loss: 0.00001042
Iteration 41/1000 | Loss: 0.00001042
Iteration 42/1000 | Loss: 0.00001041
Iteration 43/1000 | Loss: 0.00001041
Iteration 44/1000 | Loss: 0.00001039
Iteration 45/1000 | Loss: 0.00001039
Iteration 46/1000 | Loss: 0.00001039
Iteration 47/1000 | Loss: 0.00001038
Iteration 48/1000 | Loss: 0.00001038
Iteration 49/1000 | Loss: 0.00001038
Iteration 50/1000 | Loss: 0.00001037
Iteration 51/1000 | Loss: 0.00001037
Iteration 52/1000 | Loss: 0.00001037
Iteration 53/1000 | Loss: 0.00001037
Iteration 54/1000 | Loss: 0.00001036
Iteration 55/1000 | Loss: 0.00001036
Iteration 56/1000 | Loss: 0.00001036
Iteration 57/1000 | Loss: 0.00001036
Iteration 58/1000 | Loss: 0.00001035
Iteration 59/1000 | Loss: 0.00001035
Iteration 60/1000 | Loss: 0.00001035
Iteration 61/1000 | Loss: 0.00001035
Iteration 62/1000 | Loss: 0.00001035
Iteration 63/1000 | Loss: 0.00001035
Iteration 64/1000 | Loss: 0.00001035
Iteration 65/1000 | Loss: 0.00001035
Iteration 66/1000 | Loss: 0.00001035
Iteration 67/1000 | Loss: 0.00001035
Iteration 68/1000 | Loss: 0.00001035
Iteration 69/1000 | Loss: 0.00001035
Iteration 70/1000 | Loss: 0.00001035
Iteration 71/1000 | Loss: 0.00001035
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 71. Stopping optimization.
Last 5 losses: [1.0348010619054548e-05, 1.0348010619054548e-05, 1.0348010619054548e-05, 1.0348010619054548e-05, 1.0348010619054548e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0348010619054548e-05

Optimization complete. Final v2v error: 2.7654058933258057 mm

Highest mean error: 3.298912763595581 mm for frame 72

Lowest mean error: 2.497530937194824 mm for frame 111

Saving results

Total time: 61.32539367675781
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00798321
Iteration 2/25 | Loss: 0.00130265
Iteration 3/25 | Loss: 0.00120564
Iteration 4/25 | Loss: 0.00119181
Iteration 5/25 | Loss: 0.00118850
Iteration 6/25 | Loss: 0.00118785
Iteration 7/25 | Loss: 0.00118785
Iteration 8/25 | Loss: 0.00118785
Iteration 9/25 | Loss: 0.00118785
Iteration 10/25 | Loss: 0.00118785
Iteration 11/25 | Loss: 0.00118785
Iteration 12/25 | Loss: 0.00118785
Iteration 13/25 | Loss: 0.00118785
Iteration 14/25 | Loss: 0.00118785
Iteration 15/25 | Loss: 0.00118785
Iteration 16/25 | Loss: 0.00118785
Iteration 17/25 | Loss: 0.00118785
Iteration 18/25 | Loss: 0.00118785
Iteration 19/25 | Loss: 0.00118785
Iteration 20/25 | Loss: 0.00118785
Iteration 21/25 | Loss: 0.00118785
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0011878479272127151, 0.0011878479272127151, 0.0011878479272127151, 0.0011878479272127151, 0.0011878479272127151]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011878479272127151

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35449648
Iteration 2/25 | Loss: 0.00194443
Iteration 3/25 | Loss: 0.00194443
Iteration 4/25 | Loss: 0.00194443
Iteration 5/25 | Loss: 0.00194443
Iteration 6/25 | Loss: 0.00194443
Iteration 7/25 | Loss: 0.00194443
Iteration 8/25 | Loss: 0.00194443
Iteration 9/25 | Loss: 0.00194443
Iteration 10/25 | Loss: 0.00194443
Iteration 11/25 | Loss: 0.00194443
Iteration 12/25 | Loss: 0.00194443
Iteration 13/25 | Loss: 0.00194443
Iteration 14/25 | Loss: 0.00194443
Iteration 15/25 | Loss: 0.00194443
Iteration 16/25 | Loss: 0.00194443
Iteration 17/25 | Loss: 0.00194443
Iteration 18/25 | Loss: 0.00194443
Iteration 19/25 | Loss: 0.00194443
Iteration 20/25 | Loss: 0.00194443
Iteration 21/25 | Loss: 0.00194443
Iteration 22/25 | Loss: 0.00194443
Iteration 23/25 | Loss: 0.00194443
Iteration 24/25 | Loss: 0.00194443
Iteration 25/25 | Loss: 0.00194443

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00194443
Iteration 2/1000 | Loss: 0.00003566
Iteration 3/1000 | Loss: 0.00002461
Iteration 4/1000 | Loss: 0.00001901
Iteration 5/1000 | Loss: 0.00001770
Iteration 6/1000 | Loss: 0.00001703
Iteration 7/1000 | Loss: 0.00001638
Iteration 8/1000 | Loss: 0.00001601
Iteration 9/1000 | Loss: 0.00001570
Iteration 10/1000 | Loss: 0.00001549
Iteration 11/1000 | Loss: 0.00001523
Iteration 12/1000 | Loss: 0.00001509
Iteration 13/1000 | Loss: 0.00001492
Iteration 14/1000 | Loss: 0.00001492
Iteration 15/1000 | Loss: 0.00001488
Iteration 16/1000 | Loss: 0.00001486
Iteration 17/1000 | Loss: 0.00001486
Iteration 18/1000 | Loss: 0.00001485
Iteration 19/1000 | Loss: 0.00001484
Iteration 20/1000 | Loss: 0.00001482
Iteration 21/1000 | Loss: 0.00001481
Iteration 22/1000 | Loss: 0.00001480
Iteration 23/1000 | Loss: 0.00001480
Iteration 24/1000 | Loss: 0.00001479
Iteration 25/1000 | Loss: 0.00001479
Iteration 26/1000 | Loss: 0.00001478
Iteration 27/1000 | Loss: 0.00001473
Iteration 28/1000 | Loss: 0.00001472
Iteration 29/1000 | Loss: 0.00001471
Iteration 30/1000 | Loss: 0.00001471
Iteration 31/1000 | Loss: 0.00001470
Iteration 32/1000 | Loss: 0.00001462
Iteration 33/1000 | Loss: 0.00001459
Iteration 34/1000 | Loss: 0.00001458
Iteration 35/1000 | Loss: 0.00001458
Iteration 36/1000 | Loss: 0.00001457
Iteration 37/1000 | Loss: 0.00001455
Iteration 38/1000 | Loss: 0.00001454
Iteration 39/1000 | Loss: 0.00001453
Iteration 40/1000 | Loss: 0.00001453
Iteration 41/1000 | Loss: 0.00001453
Iteration 42/1000 | Loss: 0.00001453
Iteration 43/1000 | Loss: 0.00001453
Iteration 44/1000 | Loss: 0.00001453
Iteration 45/1000 | Loss: 0.00001453
Iteration 46/1000 | Loss: 0.00001453
Iteration 47/1000 | Loss: 0.00001453
Iteration 48/1000 | Loss: 0.00001453
Iteration 49/1000 | Loss: 0.00001453
Iteration 50/1000 | Loss: 0.00001452
Iteration 51/1000 | Loss: 0.00001451
Iteration 52/1000 | Loss: 0.00001450
Iteration 53/1000 | Loss: 0.00001450
Iteration 54/1000 | Loss: 0.00001450
Iteration 55/1000 | Loss: 0.00001450
Iteration 56/1000 | Loss: 0.00001450
Iteration 57/1000 | Loss: 0.00001450
Iteration 58/1000 | Loss: 0.00001450
Iteration 59/1000 | Loss: 0.00001450
Iteration 60/1000 | Loss: 0.00001450
Iteration 61/1000 | Loss: 0.00001450
Iteration 62/1000 | Loss: 0.00001450
Iteration 63/1000 | Loss: 0.00001450
Iteration 64/1000 | Loss: 0.00001450
Iteration 65/1000 | Loss: 0.00001450
Iteration 66/1000 | Loss: 0.00001450
Iteration 67/1000 | Loss: 0.00001450
Iteration 68/1000 | Loss: 0.00001450
Iteration 69/1000 | Loss: 0.00001450
Iteration 70/1000 | Loss: 0.00001450
Iteration 71/1000 | Loss: 0.00001450
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 71. Stopping optimization.
Last 5 losses: [1.4499884855467826e-05, 1.4499884855467826e-05, 1.4499884855467826e-05, 1.4499884855467826e-05, 1.4499884855467826e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4499884855467826e-05

Optimization complete. Final v2v error: 3.1985087394714355 mm

Highest mean error: 4.12731409072876 mm for frame 121

Lowest mean error: 2.4658589363098145 mm for frame 148

Saving results

Total time: 32.541789531707764
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00949440
Iteration 2/25 | Loss: 0.00174949
Iteration 3/25 | Loss: 0.00140541
Iteration 4/25 | Loss: 0.00139516
Iteration 5/25 | Loss: 0.00139317
Iteration 6/25 | Loss: 0.00139274
Iteration 7/25 | Loss: 0.00139274
Iteration 8/25 | Loss: 0.00139274
Iteration 9/25 | Loss: 0.00139274
Iteration 10/25 | Loss: 0.00139274
Iteration 11/25 | Loss: 0.00139274
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013927372638136148, 0.0013927372638136148, 0.0013927372638136148, 0.0013927372638136148, 0.0013927372638136148]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013927372638136148

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62141097
Iteration 2/25 | Loss: 0.00144845
Iteration 3/25 | Loss: 0.00144845
Iteration 4/25 | Loss: 0.00144845
Iteration 5/25 | Loss: 0.00144845
Iteration 6/25 | Loss: 0.00144845
Iteration 7/25 | Loss: 0.00144845
Iteration 8/25 | Loss: 0.00144845
Iteration 9/25 | Loss: 0.00144845
Iteration 10/25 | Loss: 0.00144845
Iteration 11/25 | Loss: 0.00144845
Iteration 12/25 | Loss: 0.00144845
Iteration 13/25 | Loss: 0.00144845
Iteration 14/25 | Loss: 0.00144845
Iteration 15/25 | Loss: 0.00144845
Iteration 16/25 | Loss: 0.00144845
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0014484465355053544, 0.0014484465355053544, 0.0014484465355053544, 0.0014484465355053544, 0.0014484465355053544]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014484465355053544

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00144845
Iteration 2/1000 | Loss: 0.00006959
Iteration 3/1000 | Loss: 0.00004922
Iteration 4/1000 | Loss: 0.00004177
Iteration 5/1000 | Loss: 0.00003920
Iteration 6/1000 | Loss: 0.00003802
Iteration 7/1000 | Loss: 0.00003729
Iteration 8/1000 | Loss: 0.00003663
Iteration 9/1000 | Loss: 0.00003587
Iteration 10/1000 | Loss: 0.00003522
Iteration 11/1000 | Loss: 0.00003465
Iteration 12/1000 | Loss: 0.00003430
Iteration 13/1000 | Loss: 0.00003391
Iteration 14/1000 | Loss: 0.00003355
Iteration 15/1000 | Loss: 0.00003330
Iteration 16/1000 | Loss: 0.00003312
Iteration 17/1000 | Loss: 0.00003310
Iteration 18/1000 | Loss: 0.00003297
Iteration 19/1000 | Loss: 0.00003289
Iteration 20/1000 | Loss: 0.00003285
Iteration 21/1000 | Loss: 0.00003272
Iteration 22/1000 | Loss: 0.00003252
Iteration 23/1000 | Loss: 0.00003237
Iteration 24/1000 | Loss: 0.00003234
Iteration 25/1000 | Loss: 0.00003226
Iteration 26/1000 | Loss: 0.00003217
Iteration 27/1000 | Loss: 0.00003216
Iteration 28/1000 | Loss: 0.00003213
Iteration 29/1000 | Loss: 0.00003213
Iteration 30/1000 | Loss: 0.00003213
Iteration 31/1000 | Loss: 0.00003213
Iteration 32/1000 | Loss: 0.00003213
Iteration 33/1000 | Loss: 0.00003213
Iteration 34/1000 | Loss: 0.00003213
Iteration 35/1000 | Loss: 0.00003213
Iteration 36/1000 | Loss: 0.00003213
Iteration 37/1000 | Loss: 0.00003211
Iteration 38/1000 | Loss: 0.00003211
Iteration 39/1000 | Loss: 0.00003211
Iteration 40/1000 | Loss: 0.00003211
Iteration 41/1000 | Loss: 0.00003210
Iteration 42/1000 | Loss: 0.00003210
Iteration 43/1000 | Loss: 0.00003209
Iteration 44/1000 | Loss: 0.00003207
Iteration 45/1000 | Loss: 0.00003205
Iteration 46/1000 | Loss: 0.00003205
Iteration 47/1000 | Loss: 0.00003204
Iteration 48/1000 | Loss: 0.00003203
Iteration 49/1000 | Loss: 0.00003203
Iteration 50/1000 | Loss: 0.00003203
Iteration 51/1000 | Loss: 0.00003202
Iteration 52/1000 | Loss: 0.00003202
Iteration 53/1000 | Loss: 0.00003202
Iteration 54/1000 | Loss: 0.00003202
Iteration 55/1000 | Loss: 0.00003202
Iteration 56/1000 | Loss: 0.00003202
Iteration 57/1000 | Loss: 0.00003202
Iteration 58/1000 | Loss: 0.00003202
Iteration 59/1000 | Loss: 0.00003202
Iteration 60/1000 | Loss: 0.00003202
Iteration 61/1000 | Loss: 0.00003202
Iteration 62/1000 | Loss: 0.00003201
Iteration 63/1000 | Loss: 0.00003201
Iteration 64/1000 | Loss: 0.00003201
Iteration 65/1000 | Loss: 0.00003201
Iteration 66/1000 | Loss: 0.00003201
Iteration 67/1000 | Loss: 0.00003201
Iteration 68/1000 | Loss: 0.00003200
Iteration 69/1000 | Loss: 0.00003200
Iteration 70/1000 | Loss: 0.00003199
Iteration 71/1000 | Loss: 0.00003199
Iteration 72/1000 | Loss: 0.00003199
Iteration 73/1000 | Loss: 0.00003199
Iteration 74/1000 | Loss: 0.00003198
Iteration 75/1000 | Loss: 0.00003198
Iteration 76/1000 | Loss: 0.00003198
Iteration 77/1000 | Loss: 0.00003198
Iteration 78/1000 | Loss: 0.00003198
Iteration 79/1000 | Loss: 0.00003198
Iteration 80/1000 | Loss: 0.00003198
Iteration 81/1000 | Loss: 0.00003198
Iteration 82/1000 | Loss: 0.00003198
Iteration 83/1000 | Loss: 0.00003198
Iteration 84/1000 | Loss: 0.00003198
Iteration 85/1000 | Loss: 0.00003198
Iteration 86/1000 | Loss: 0.00003198
Iteration 87/1000 | Loss: 0.00003198
Iteration 88/1000 | Loss: 0.00003197
Iteration 89/1000 | Loss: 0.00003197
Iteration 90/1000 | Loss: 0.00003197
Iteration 91/1000 | Loss: 0.00003197
Iteration 92/1000 | Loss: 0.00003197
Iteration 93/1000 | Loss: 0.00003197
Iteration 94/1000 | Loss: 0.00003197
Iteration 95/1000 | Loss: 0.00003197
Iteration 96/1000 | Loss: 0.00003197
Iteration 97/1000 | Loss: 0.00003196
Iteration 98/1000 | Loss: 0.00003196
Iteration 99/1000 | Loss: 0.00003196
Iteration 100/1000 | Loss: 0.00003196
Iteration 101/1000 | Loss: 0.00003196
Iteration 102/1000 | Loss: 0.00003196
Iteration 103/1000 | Loss: 0.00003196
Iteration 104/1000 | Loss: 0.00003196
Iteration 105/1000 | Loss: 0.00003196
Iteration 106/1000 | Loss: 0.00003196
Iteration 107/1000 | Loss: 0.00003196
Iteration 108/1000 | Loss: 0.00003196
Iteration 109/1000 | Loss: 0.00003196
Iteration 110/1000 | Loss: 0.00003196
Iteration 111/1000 | Loss: 0.00003196
Iteration 112/1000 | Loss: 0.00003196
Iteration 113/1000 | Loss: 0.00003196
Iteration 114/1000 | Loss: 0.00003195
Iteration 115/1000 | Loss: 0.00003195
Iteration 116/1000 | Loss: 0.00003195
Iteration 117/1000 | Loss: 0.00003195
Iteration 118/1000 | Loss: 0.00003195
Iteration 119/1000 | Loss: 0.00003195
Iteration 120/1000 | Loss: 0.00003195
Iteration 121/1000 | Loss: 0.00003195
Iteration 122/1000 | Loss: 0.00003195
Iteration 123/1000 | Loss: 0.00003195
Iteration 124/1000 | Loss: 0.00003195
Iteration 125/1000 | Loss: 0.00003195
Iteration 126/1000 | Loss: 0.00003195
Iteration 127/1000 | Loss: 0.00003195
Iteration 128/1000 | Loss: 0.00003194
Iteration 129/1000 | Loss: 0.00003194
Iteration 130/1000 | Loss: 0.00003194
Iteration 131/1000 | Loss: 0.00003194
Iteration 132/1000 | Loss: 0.00003194
Iteration 133/1000 | Loss: 0.00003194
Iteration 134/1000 | Loss: 0.00003194
Iteration 135/1000 | Loss: 0.00003194
Iteration 136/1000 | Loss: 0.00003194
Iteration 137/1000 | Loss: 0.00003194
Iteration 138/1000 | Loss: 0.00003194
Iteration 139/1000 | Loss: 0.00003194
Iteration 140/1000 | Loss: 0.00003194
Iteration 141/1000 | Loss: 0.00003194
Iteration 142/1000 | Loss: 0.00003194
Iteration 143/1000 | Loss: 0.00003194
Iteration 144/1000 | Loss: 0.00003194
Iteration 145/1000 | Loss: 0.00003194
Iteration 146/1000 | Loss: 0.00003194
Iteration 147/1000 | Loss: 0.00003194
Iteration 148/1000 | Loss: 0.00003194
Iteration 149/1000 | Loss: 0.00003194
Iteration 150/1000 | Loss: 0.00003194
Iteration 151/1000 | Loss: 0.00003194
Iteration 152/1000 | Loss: 0.00003194
Iteration 153/1000 | Loss: 0.00003194
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [3.1942512578098103e-05, 3.1942512578098103e-05, 3.1942512578098103e-05, 3.1942512578098103e-05, 3.1942512578098103e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.1942512578098103e-05

Optimization complete. Final v2v error: 4.615719318389893 mm

Highest mean error: 5.26353120803833 mm for frame 58

Lowest mean error: 4.1887993812561035 mm for frame 114

Saving results

Total time: 48.06655693054199
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00495369
Iteration 2/25 | Loss: 0.00159110
Iteration 3/25 | Loss: 0.00139540
Iteration 4/25 | Loss: 0.00135478
Iteration 5/25 | Loss: 0.00134993
Iteration 6/25 | Loss: 0.00133415
Iteration 7/25 | Loss: 0.00132594
Iteration 8/25 | Loss: 0.00131140
Iteration 9/25 | Loss: 0.00130734
Iteration 10/25 | Loss: 0.00129350
Iteration 11/25 | Loss: 0.00128360
Iteration 12/25 | Loss: 0.00128037
Iteration 13/25 | Loss: 0.00128140
Iteration 14/25 | Loss: 0.00128577
Iteration 15/25 | Loss: 0.00128062
Iteration 16/25 | Loss: 0.00127264
Iteration 17/25 | Loss: 0.00127085
Iteration 18/25 | Loss: 0.00127013
Iteration 19/25 | Loss: 0.00127266
Iteration 20/25 | Loss: 0.00127087
Iteration 21/25 | Loss: 0.00126864
Iteration 22/25 | Loss: 0.00126741
Iteration 23/25 | Loss: 0.00126724
Iteration 24/25 | Loss: 0.00126718
Iteration 25/25 | Loss: 0.00126718

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23135185
Iteration 2/25 | Loss: 0.00450887
Iteration 3/25 | Loss: 0.00450887
Iteration 4/25 | Loss: 0.00450887
Iteration 5/25 | Loss: 0.00450887
Iteration 6/25 | Loss: 0.00450887
Iteration 7/25 | Loss: 0.00450887
Iteration 8/25 | Loss: 0.00450887
Iteration 9/25 | Loss: 0.00450887
Iteration 10/25 | Loss: 0.00450887
Iteration 11/25 | Loss: 0.00450887
Iteration 12/25 | Loss: 0.00450887
Iteration 13/25 | Loss: 0.00450887
Iteration 14/25 | Loss: 0.00450887
Iteration 15/25 | Loss: 0.00450887
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0045088669285178185, 0.0045088669285178185, 0.0045088669285178185, 0.0045088669285178185, 0.0045088669285178185]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0045088669285178185

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00450887
Iteration 2/1000 | Loss: 0.00024360
Iteration 3/1000 | Loss: 0.00012577
Iteration 4/1000 | Loss: 0.00009883
Iteration 5/1000 | Loss: 0.00008431
Iteration 6/1000 | Loss: 0.00007702
Iteration 7/1000 | Loss: 0.00007288
Iteration 8/1000 | Loss: 0.00007003
Iteration 9/1000 | Loss: 0.00006804
Iteration 10/1000 | Loss: 0.00006623
Iteration 11/1000 | Loss: 0.00006472
Iteration 12/1000 | Loss: 0.00006331
Iteration 13/1000 | Loss: 0.00006207
Iteration 14/1000 | Loss: 0.00027237
Iteration 15/1000 | Loss: 0.00027234
Iteration 16/1000 | Loss: 0.00016568
Iteration 17/1000 | Loss: 0.00007272
Iteration 18/1000 | Loss: 0.00006641
Iteration 19/1000 | Loss: 0.00006319
Iteration 20/1000 | Loss: 0.00006121
Iteration 21/1000 | Loss: 0.00028503
Iteration 22/1000 | Loss: 0.00052259
Iteration 23/1000 | Loss: 0.00007695
Iteration 24/1000 | Loss: 0.00006909
Iteration 25/1000 | Loss: 0.00006532
Iteration 26/1000 | Loss: 0.00006201
Iteration 27/1000 | Loss: 0.00005959
Iteration 28/1000 | Loss: 0.00005714
Iteration 29/1000 | Loss: 0.00005579
Iteration 30/1000 | Loss: 0.00005515
Iteration 31/1000 | Loss: 0.00005479
Iteration 32/1000 | Loss: 0.00005422
Iteration 33/1000 | Loss: 0.00005355
Iteration 34/1000 | Loss: 0.00005303
Iteration 35/1000 | Loss: 0.00049288
Iteration 36/1000 | Loss: 0.00006920
Iteration 37/1000 | Loss: 0.00007272
Iteration 38/1000 | Loss: 0.00005871
Iteration 39/1000 | Loss: 0.00005679
Iteration 40/1000 | Loss: 0.00005491
Iteration 41/1000 | Loss: 0.00005342
Iteration 42/1000 | Loss: 0.00005452
Iteration 43/1000 | Loss: 0.00005149
Iteration 44/1000 | Loss: 0.00005088
Iteration 45/1000 | Loss: 0.00005037
Iteration 46/1000 | Loss: 0.00029139
Iteration 47/1000 | Loss: 0.00006148
Iteration 48/1000 | Loss: 0.00005419
Iteration 49/1000 | Loss: 0.00005209
Iteration 50/1000 | Loss: 0.00005045
Iteration 51/1000 | Loss: 0.00004905
Iteration 52/1000 | Loss: 0.00004849
Iteration 53/1000 | Loss: 0.00004826
Iteration 54/1000 | Loss: 0.00004823
Iteration 55/1000 | Loss: 0.00004822
Iteration 56/1000 | Loss: 0.00004819
Iteration 57/1000 | Loss: 0.00004817
Iteration 58/1000 | Loss: 0.00004816
Iteration 59/1000 | Loss: 0.00004816
Iteration 60/1000 | Loss: 0.00004793
Iteration 61/1000 | Loss: 0.00004766
Iteration 62/1000 | Loss: 0.00004764
Iteration 63/1000 | Loss: 0.00004761
Iteration 64/1000 | Loss: 0.00004753
Iteration 65/1000 | Loss: 0.00004739
Iteration 66/1000 | Loss: 0.00004739
Iteration 67/1000 | Loss: 0.00004738
Iteration 68/1000 | Loss: 0.00004737
Iteration 69/1000 | Loss: 0.00004737
Iteration 70/1000 | Loss: 0.00004737
Iteration 71/1000 | Loss: 0.00004737
Iteration 72/1000 | Loss: 0.00004737
Iteration 73/1000 | Loss: 0.00004737
Iteration 74/1000 | Loss: 0.00004737
Iteration 75/1000 | Loss: 0.00004737
Iteration 76/1000 | Loss: 0.00004737
Iteration 77/1000 | Loss: 0.00004736
Iteration 78/1000 | Loss: 0.00004736
Iteration 79/1000 | Loss: 0.00004736
Iteration 80/1000 | Loss: 0.00004736
Iteration 81/1000 | Loss: 0.00004736
Iteration 82/1000 | Loss: 0.00004735
Iteration 83/1000 | Loss: 0.00004735
Iteration 84/1000 | Loss: 0.00004735
Iteration 85/1000 | Loss: 0.00004734
Iteration 86/1000 | Loss: 0.00004734
Iteration 87/1000 | Loss: 0.00004734
Iteration 88/1000 | Loss: 0.00004734
Iteration 89/1000 | Loss: 0.00004733
Iteration 90/1000 | Loss: 0.00004733
Iteration 91/1000 | Loss: 0.00004733
Iteration 92/1000 | Loss: 0.00004732
Iteration 93/1000 | Loss: 0.00004732
Iteration 94/1000 | Loss: 0.00004732
Iteration 95/1000 | Loss: 0.00004732
Iteration 96/1000 | Loss: 0.00004731
Iteration 97/1000 | Loss: 0.00004731
Iteration 98/1000 | Loss: 0.00004729
Iteration 99/1000 | Loss: 0.00004726
Iteration 100/1000 | Loss: 0.00004726
Iteration 101/1000 | Loss: 0.00004725
Iteration 102/1000 | Loss: 0.00004725
Iteration 103/1000 | Loss: 0.00004724
Iteration 104/1000 | Loss: 0.00004721
Iteration 105/1000 | Loss: 0.00004721
Iteration 106/1000 | Loss: 0.00004719
Iteration 107/1000 | Loss: 0.00004719
Iteration 108/1000 | Loss: 0.00004718
Iteration 109/1000 | Loss: 0.00004718
Iteration 110/1000 | Loss: 0.00004718
Iteration 111/1000 | Loss: 0.00004718
Iteration 112/1000 | Loss: 0.00004718
Iteration 113/1000 | Loss: 0.00004718
Iteration 114/1000 | Loss: 0.00004717
Iteration 115/1000 | Loss: 0.00004716
Iteration 116/1000 | Loss: 0.00004715
Iteration 117/1000 | Loss: 0.00004715
Iteration 118/1000 | Loss: 0.00004714
Iteration 119/1000 | Loss: 0.00004714
Iteration 120/1000 | Loss: 0.00004714
Iteration 121/1000 | Loss: 0.00004714
Iteration 122/1000 | Loss: 0.00004712
Iteration 123/1000 | Loss: 0.00004712
Iteration 124/1000 | Loss: 0.00004712
Iteration 125/1000 | Loss: 0.00004711
Iteration 126/1000 | Loss: 0.00004711
Iteration 127/1000 | Loss: 0.00004711
Iteration 128/1000 | Loss: 0.00004710
Iteration 129/1000 | Loss: 0.00004710
Iteration 130/1000 | Loss: 0.00004709
Iteration 131/1000 | Loss: 0.00004709
Iteration 132/1000 | Loss: 0.00004709
Iteration 133/1000 | Loss: 0.00004709
Iteration 134/1000 | Loss: 0.00004709
Iteration 135/1000 | Loss: 0.00004709
Iteration 136/1000 | Loss: 0.00004708
Iteration 137/1000 | Loss: 0.00004708
Iteration 138/1000 | Loss: 0.00004708
Iteration 139/1000 | Loss: 0.00004708
Iteration 140/1000 | Loss: 0.00004708
Iteration 141/1000 | Loss: 0.00004708
Iteration 142/1000 | Loss: 0.00004708
Iteration 143/1000 | Loss: 0.00004708
Iteration 144/1000 | Loss: 0.00004707
Iteration 145/1000 | Loss: 0.00004707
Iteration 146/1000 | Loss: 0.00004707
Iteration 147/1000 | Loss: 0.00004707
Iteration 148/1000 | Loss: 0.00004707
Iteration 149/1000 | Loss: 0.00004707
Iteration 150/1000 | Loss: 0.00004707
Iteration 151/1000 | Loss: 0.00004707
Iteration 152/1000 | Loss: 0.00004707
Iteration 153/1000 | Loss: 0.00004707
Iteration 154/1000 | Loss: 0.00004707
Iteration 155/1000 | Loss: 0.00004707
Iteration 156/1000 | Loss: 0.00004707
Iteration 157/1000 | Loss: 0.00004707
Iteration 158/1000 | Loss: 0.00004707
Iteration 159/1000 | Loss: 0.00004707
Iteration 160/1000 | Loss: 0.00004706
Iteration 161/1000 | Loss: 0.00004706
Iteration 162/1000 | Loss: 0.00004706
Iteration 163/1000 | Loss: 0.00004706
Iteration 164/1000 | Loss: 0.00004706
Iteration 165/1000 | Loss: 0.00004706
Iteration 166/1000 | Loss: 0.00004706
Iteration 167/1000 | Loss: 0.00004706
Iteration 168/1000 | Loss: 0.00004706
Iteration 169/1000 | Loss: 0.00004706
Iteration 170/1000 | Loss: 0.00004706
Iteration 171/1000 | Loss: 0.00004706
Iteration 172/1000 | Loss: 0.00004706
Iteration 173/1000 | Loss: 0.00004706
Iteration 174/1000 | Loss: 0.00004706
Iteration 175/1000 | Loss: 0.00004706
Iteration 176/1000 | Loss: 0.00004705
Iteration 177/1000 | Loss: 0.00004705
Iteration 178/1000 | Loss: 0.00004705
Iteration 179/1000 | Loss: 0.00004705
Iteration 180/1000 | Loss: 0.00004705
Iteration 181/1000 | Loss: 0.00004705
Iteration 182/1000 | Loss: 0.00004705
Iteration 183/1000 | Loss: 0.00004705
Iteration 184/1000 | Loss: 0.00004705
Iteration 185/1000 | Loss: 0.00004705
Iteration 186/1000 | Loss: 0.00004705
Iteration 187/1000 | Loss: 0.00004705
Iteration 188/1000 | Loss: 0.00004705
Iteration 189/1000 | Loss: 0.00004705
Iteration 190/1000 | Loss: 0.00004705
Iteration 191/1000 | Loss: 0.00004705
Iteration 192/1000 | Loss: 0.00004705
Iteration 193/1000 | Loss: 0.00004705
Iteration 194/1000 | Loss: 0.00004705
Iteration 195/1000 | Loss: 0.00004705
Iteration 196/1000 | Loss: 0.00004705
Iteration 197/1000 | Loss: 0.00004705
Iteration 198/1000 | Loss: 0.00004705
Iteration 199/1000 | Loss: 0.00004705
Iteration 200/1000 | Loss: 0.00004705
Iteration 201/1000 | Loss: 0.00004705
Iteration 202/1000 | Loss: 0.00004705
Iteration 203/1000 | Loss: 0.00004705
Iteration 204/1000 | Loss: 0.00004705
Iteration 205/1000 | Loss: 0.00004705
Iteration 206/1000 | Loss: 0.00004705
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [4.7046076360857114e-05, 4.7046076360857114e-05, 4.7046076360857114e-05, 4.7046076360857114e-05, 4.7046076360857114e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.7046076360857114e-05

Optimization complete. Final v2v error: 4.092601776123047 mm

Highest mean error: 11.545246124267578 mm for frame 107

Lowest mean error: 2.4222681522369385 mm for frame 170

Saving results

Total time: 137.37710809707642
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00709984
Iteration 2/25 | Loss: 0.00148263
Iteration 3/25 | Loss: 0.00130062
Iteration 4/25 | Loss: 0.00127154
Iteration 5/25 | Loss: 0.00126366
Iteration 6/25 | Loss: 0.00126142
Iteration 7/25 | Loss: 0.00126142
Iteration 8/25 | Loss: 0.00126142
Iteration 9/25 | Loss: 0.00126142
Iteration 10/25 | Loss: 0.00126135
Iteration 11/25 | Loss: 0.00126135
Iteration 12/25 | Loss: 0.00126135
Iteration 13/25 | Loss: 0.00126135
Iteration 14/25 | Loss: 0.00126135
Iteration 15/25 | Loss: 0.00126135
Iteration 16/25 | Loss: 0.00126135
Iteration 17/25 | Loss: 0.00126135
Iteration 18/25 | Loss: 0.00126135
Iteration 19/25 | Loss: 0.00126135
Iteration 20/25 | Loss: 0.00126135
Iteration 21/25 | Loss: 0.00126135
Iteration 22/25 | Loss: 0.00126135
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012613458093255758, 0.0012613458093255758, 0.0012613458093255758, 0.0012613458093255758, 0.0012613458093255758]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012613458093255758

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.97029257
Iteration 2/25 | Loss: 0.00202608
Iteration 3/25 | Loss: 0.00202608
Iteration 4/25 | Loss: 0.00202608
Iteration 5/25 | Loss: 0.00202608
Iteration 6/25 | Loss: 0.00202608
Iteration 7/25 | Loss: 0.00202608
Iteration 8/25 | Loss: 0.00202608
Iteration 9/25 | Loss: 0.00202607
Iteration 10/25 | Loss: 0.00202607
Iteration 11/25 | Loss: 0.00202607
Iteration 12/25 | Loss: 0.00202607
Iteration 13/25 | Loss: 0.00202607
Iteration 14/25 | Loss: 0.00202607
Iteration 15/25 | Loss: 0.00202607
Iteration 16/25 | Loss: 0.00202607
Iteration 17/25 | Loss: 0.00202607
Iteration 18/25 | Loss: 0.00202607
Iteration 19/25 | Loss: 0.00202607
Iteration 20/25 | Loss: 0.00202607
Iteration 21/25 | Loss: 0.00202607
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.002026073867455125, 0.002026073867455125, 0.002026073867455125, 0.002026073867455125, 0.002026073867455125]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002026073867455125

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00202607
Iteration 2/1000 | Loss: 0.00002812
Iteration 3/1000 | Loss: 0.00002338
Iteration 4/1000 | Loss: 0.00002212
Iteration 5/1000 | Loss: 0.00002129
Iteration 6/1000 | Loss: 0.00002074
Iteration 7/1000 | Loss: 0.00002030
Iteration 8/1000 | Loss: 0.00001982
Iteration 9/1000 | Loss: 0.00001935
Iteration 10/1000 | Loss: 0.00001901
Iteration 11/1000 | Loss: 0.00001876
Iteration 12/1000 | Loss: 0.00001871
Iteration 13/1000 | Loss: 0.00001851
Iteration 14/1000 | Loss: 0.00001828
Iteration 15/1000 | Loss: 0.00001817
Iteration 16/1000 | Loss: 0.00001815
Iteration 17/1000 | Loss: 0.00001811
Iteration 18/1000 | Loss: 0.00001808
Iteration 19/1000 | Loss: 0.00001804
Iteration 20/1000 | Loss: 0.00001803
Iteration 21/1000 | Loss: 0.00001802
Iteration 22/1000 | Loss: 0.00001802
Iteration 23/1000 | Loss: 0.00001801
Iteration 24/1000 | Loss: 0.00001801
Iteration 25/1000 | Loss: 0.00001801
Iteration 26/1000 | Loss: 0.00001800
Iteration 27/1000 | Loss: 0.00001800
Iteration 28/1000 | Loss: 0.00001800
Iteration 29/1000 | Loss: 0.00001800
Iteration 30/1000 | Loss: 0.00001800
Iteration 31/1000 | Loss: 0.00001800
Iteration 32/1000 | Loss: 0.00001800
Iteration 33/1000 | Loss: 0.00001799
Iteration 34/1000 | Loss: 0.00001799
Iteration 35/1000 | Loss: 0.00001799
Iteration 36/1000 | Loss: 0.00001798
Iteration 37/1000 | Loss: 0.00001798
Iteration 38/1000 | Loss: 0.00001798
Iteration 39/1000 | Loss: 0.00001798
Iteration 40/1000 | Loss: 0.00001797
Iteration 41/1000 | Loss: 0.00001797
Iteration 42/1000 | Loss: 0.00001797
Iteration 43/1000 | Loss: 0.00001796
Iteration 44/1000 | Loss: 0.00001796
Iteration 45/1000 | Loss: 0.00001796
Iteration 46/1000 | Loss: 0.00001795
Iteration 47/1000 | Loss: 0.00001795
Iteration 48/1000 | Loss: 0.00001795
Iteration 49/1000 | Loss: 0.00001794
Iteration 50/1000 | Loss: 0.00001794
Iteration 51/1000 | Loss: 0.00001794
Iteration 52/1000 | Loss: 0.00001794
Iteration 53/1000 | Loss: 0.00001794
Iteration 54/1000 | Loss: 0.00001793
Iteration 55/1000 | Loss: 0.00001793
Iteration 56/1000 | Loss: 0.00001793
Iteration 57/1000 | Loss: 0.00001793
Iteration 58/1000 | Loss: 0.00001793
Iteration 59/1000 | Loss: 0.00001793
Iteration 60/1000 | Loss: 0.00001793
Iteration 61/1000 | Loss: 0.00001793
Iteration 62/1000 | Loss: 0.00001793
Iteration 63/1000 | Loss: 0.00001793
Iteration 64/1000 | Loss: 0.00001793
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 64. Stopping optimization.
Last 5 losses: [1.792813782230951e-05, 1.792813782230951e-05, 1.792813782230951e-05, 1.792813782230951e-05, 1.792813782230951e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.792813782230951e-05

Optimization complete. Final v2v error: 3.566169261932373 mm

Highest mean error: 4.221566677093506 mm for frame 180

Lowest mean error: 3.0076446533203125 mm for frame 134

Saving results

Total time: 38.19364523887634
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00422499
Iteration 2/25 | Loss: 0.00131288
Iteration 3/25 | Loss: 0.00118896
Iteration 4/25 | Loss: 0.00117300
Iteration 5/25 | Loss: 0.00117132
Iteration 6/25 | Loss: 0.00117132
Iteration 7/25 | Loss: 0.00117132
Iteration 8/25 | Loss: 0.00117132
Iteration 9/25 | Loss: 0.00117132
Iteration 10/25 | Loss: 0.00117132
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011713154381141067, 0.0011713154381141067, 0.0011713154381141067, 0.0011713154381141067, 0.0011713154381141067]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011713154381141067

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22548461
Iteration 2/25 | Loss: 0.00179353
Iteration 3/25 | Loss: 0.00179353
Iteration 4/25 | Loss: 0.00179353
Iteration 5/25 | Loss: 0.00179353
Iteration 6/25 | Loss: 0.00179352
Iteration 7/25 | Loss: 0.00179352
Iteration 8/25 | Loss: 0.00179352
Iteration 9/25 | Loss: 0.00179352
Iteration 10/25 | Loss: 0.00179352
Iteration 11/25 | Loss: 0.00179352
Iteration 12/25 | Loss: 0.00179352
Iteration 13/25 | Loss: 0.00179352
Iteration 14/25 | Loss: 0.00179352
Iteration 15/25 | Loss: 0.00179352
Iteration 16/25 | Loss: 0.00179352
Iteration 17/25 | Loss: 0.00179352
Iteration 18/25 | Loss: 0.00179352
Iteration 19/25 | Loss: 0.00179352
Iteration 20/25 | Loss: 0.00179352
Iteration 21/25 | Loss: 0.00179352
Iteration 22/25 | Loss: 0.00179352
Iteration 23/25 | Loss: 0.00179352
Iteration 24/25 | Loss: 0.00179352
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0017935226205736399, 0.0017935226205736399, 0.0017935226205736399, 0.0017935226205736399, 0.0017935226205736399]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017935226205736399

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00179352
Iteration 2/1000 | Loss: 0.00002298
Iteration 3/1000 | Loss: 0.00001734
Iteration 4/1000 | Loss: 0.00001558
Iteration 5/1000 | Loss: 0.00001449
Iteration 6/1000 | Loss: 0.00001386
Iteration 7/1000 | Loss: 0.00001338
Iteration 8/1000 | Loss: 0.00001298
Iteration 9/1000 | Loss: 0.00001262
Iteration 10/1000 | Loss: 0.00001244
Iteration 11/1000 | Loss: 0.00001237
Iteration 12/1000 | Loss: 0.00001223
Iteration 13/1000 | Loss: 0.00001212
Iteration 14/1000 | Loss: 0.00001209
Iteration 15/1000 | Loss: 0.00001199
Iteration 16/1000 | Loss: 0.00001198
Iteration 17/1000 | Loss: 0.00001198
Iteration 18/1000 | Loss: 0.00001197
Iteration 19/1000 | Loss: 0.00001188
Iteration 20/1000 | Loss: 0.00001185
Iteration 21/1000 | Loss: 0.00001175
Iteration 22/1000 | Loss: 0.00001171
Iteration 23/1000 | Loss: 0.00001171
Iteration 24/1000 | Loss: 0.00001170
Iteration 25/1000 | Loss: 0.00001169
Iteration 26/1000 | Loss: 0.00001168
Iteration 27/1000 | Loss: 0.00001168
Iteration 28/1000 | Loss: 0.00001168
Iteration 29/1000 | Loss: 0.00001167
Iteration 30/1000 | Loss: 0.00001167
Iteration 31/1000 | Loss: 0.00001167
Iteration 32/1000 | Loss: 0.00001167
Iteration 33/1000 | Loss: 0.00001166
Iteration 34/1000 | Loss: 0.00001166
Iteration 35/1000 | Loss: 0.00001164
Iteration 36/1000 | Loss: 0.00001164
Iteration 37/1000 | Loss: 0.00001163
Iteration 38/1000 | Loss: 0.00001163
Iteration 39/1000 | Loss: 0.00001163
Iteration 40/1000 | Loss: 0.00001163
Iteration 41/1000 | Loss: 0.00001162
Iteration 42/1000 | Loss: 0.00001162
Iteration 43/1000 | Loss: 0.00001161
Iteration 44/1000 | Loss: 0.00001161
Iteration 45/1000 | Loss: 0.00001161
Iteration 46/1000 | Loss: 0.00001161
Iteration 47/1000 | Loss: 0.00001161
Iteration 48/1000 | Loss: 0.00001160
Iteration 49/1000 | Loss: 0.00001160
Iteration 50/1000 | Loss: 0.00001160
Iteration 51/1000 | Loss: 0.00001160
Iteration 52/1000 | Loss: 0.00001160
Iteration 53/1000 | Loss: 0.00001159
Iteration 54/1000 | Loss: 0.00001159
Iteration 55/1000 | Loss: 0.00001158
Iteration 56/1000 | Loss: 0.00001154
Iteration 57/1000 | Loss: 0.00001154
Iteration 58/1000 | Loss: 0.00001154
Iteration 59/1000 | Loss: 0.00001154
Iteration 60/1000 | Loss: 0.00001154
Iteration 61/1000 | Loss: 0.00001154
Iteration 62/1000 | Loss: 0.00001154
Iteration 63/1000 | Loss: 0.00001154
Iteration 64/1000 | Loss: 0.00001153
Iteration 65/1000 | Loss: 0.00001153
Iteration 66/1000 | Loss: 0.00001153
Iteration 67/1000 | Loss: 0.00001151
Iteration 68/1000 | Loss: 0.00001151
Iteration 69/1000 | Loss: 0.00001150
Iteration 70/1000 | Loss: 0.00001150
Iteration 71/1000 | Loss: 0.00001150
Iteration 72/1000 | Loss: 0.00001150
Iteration 73/1000 | Loss: 0.00001150
Iteration 74/1000 | Loss: 0.00001150
Iteration 75/1000 | Loss: 0.00001150
Iteration 76/1000 | Loss: 0.00001150
Iteration 77/1000 | Loss: 0.00001150
Iteration 78/1000 | Loss: 0.00001149
Iteration 79/1000 | Loss: 0.00001149
Iteration 80/1000 | Loss: 0.00001149
Iteration 81/1000 | Loss: 0.00001143
Iteration 82/1000 | Loss: 0.00001142
Iteration 83/1000 | Loss: 0.00001141
Iteration 84/1000 | Loss: 0.00001141
Iteration 85/1000 | Loss: 0.00001140
Iteration 86/1000 | Loss: 0.00001140
Iteration 87/1000 | Loss: 0.00001140
Iteration 88/1000 | Loss: 0.00001140
Iteration 89/1000 | Loss: 0.00001140
Iteration 90/1000 | Loss: 0.00001139
Iteration 91/1000 | Loss: 0.00001139
Iteration 92/1000 | Loss: 0.00001139
Iteration 93/1000 | Loss: 0.00001138
Iteration 94/1000 | Loss: 0.00001138
Iteration 95/1000 | Loss: 0.00001138
Iteration 96/1000 | Loss: 0.00001137
Iteration 97/1000 | Loss: 0.00001137
Iteration 98/1000 | Loss: 0.00001136
Iteration 99/1000 | Loss: 0.00001136
Iteration 100/1000 | Loss: 0.00001135
Iteration 101/1000 | Loss: 0.00001135
Iteration 102/1000 | Loss: 0.00001135
Iteration 103/1000 | Loss: 0.00001135
Iteration 104/1000 | Loss: 0.00001134
Iteration 105/1000 | Loss: 0.00001134
Iteration 106/1000 | Loss: 0.00001134
Iteration 107/1000 | Loss: 0.00001133
Iteration 108/1000 | Loss: 0.00001132
Iteration 109/1000 | Loss: 0.00001132
Iteration 110/1000 | Loss: 0.00001132
Iteration 111/1000 | Loss: 0.00001132
Iteration 112/1000 | Loss: 0.00001132
Iteration 113/1000 | Loss: 0.00001132
Iteration 114/1000 | Loss: 0.00001132
Iteration 115/1000 | Loss: 0.00001132
Iteration 116/1000 | Loss: 0.00001132
Iteration 117/1000 | Loss: 0.00001132
Iteration 118/1000 | Loss: 0.00001132
Iteration 119/1000 | Loss: 0.00001132
Iteration 120/1000 | Loss: 0.00001131
Iteration 121/1000 | Loss: 0.00001131
Iteration 122/1000 | Loss: 0.00001131
Iteration 123/1000 | Loss: 0.00001131
Iteration 124/1000 | Loss: 0.00001131
Iteration 125/1000 | Loss: 0.00001130
Iteration 126/1000 | Loss: 0.00001130
Iteration 127/1000 | Loss: 0.00001130
Iteration 128/1000 | Loss: 0.00001130
Iteration 129/1000 | Loss: 0.00001130
Iteration 130/1000 | Loss: 0.00001130
Iteration 131/1000 | Loss: 0.00001130
Iteration 132/1000 | Loss: 0.00001130
Iteration 133/1000 | Loss: 0.00001129
Iteration 134/1000 | Loss: 0.00001129
Iteration 135/1000 | Loss: 0.00001129
Iteration 136/1000 | Loss: 0.00001129
Iteration 137/1000 | Loss: 0.00001129
Iteration 138/1000 | Loss: 0.00001128
Iteration 139/1000 | Loss: 0.00001128
Iteration 140/1000 | Loss: 0.00001128
Iteration 141/1000 | Loss: 0.00001128
Iteration 142/1000 | Loss: 0.00001128
Iteration 143/1000 | Loss: 0.00001128
Iteration 144/1000 | Loss: 0.00001128
Iteration 145/1000 | Loss: 0.00001128
Iteration 146/1000 | Loss: 0.00001128
Iteration 147/1000 | Loss: 0.00001127
Iteration 148/1000 | Loss: 0.00001127
Iteration 149/1000 | Loss: 0.00001127
Iteration 150/1000 | Loss: 0.00001127
Iteration 151/1000 | Loss: 0.00001127
Iteration 152/1000 | Loss: 0.00001127
Iteration 153/1000 | Loss: 0.00001126
Iteration 154/1000 | Loss: 0.00001126
Iteration 155/1000 | Loss: 0.00001126
Iteration 156/1000 | Loss: 0.00001126
Iteration 157/1000 | Loss: 0.00001126
Iteration 158/1000 | Loss: 0.00001125
Iteration 159/1000 | Loss: 0.00001125
Iteration 160/1000 | Loss: 0.00001125
Iteration 161/1000 | Loss: 0.00001125
Iteration 162/1000 | Loss: 0.00001125
Iteration 163/1000 | Loss: 0.00001125
Iteration 164/1000 | Loss: 0.00001125
Iteration 165/1000 | Loss: 0.00001125
Iteration 166/1000 | Loss: 0.00001124
Iteration 167/1000 | Loss: 0.00001124
Iteration 168/1000 | Loss: 0.00001124
Iteration 169/1000 | Loss: 0.00001124
Iteration 170/1000 | Loss: 0.00001124
Iteration 171/1000 | Loss: 0.00001124
Iteration 172/1000 | Loss: 0.00001124
Iteration 173/1000 | Loss: 0.00001124
Iteration 174/1000 | Loss: 0.00001124
Iteration 175/1000 | Loss: 0.00001124
Iteration 176/1000 | Loss: 0.00001124
Iteration 177/1000 | Loss: 0.00001124
Iteration 178/1000 | Loss: 0.00001124
Iteration 179/1000 | Loss: 0.00001124
Iteration 180/1000 | Loss: 0.00001123
Iteration 181/1000 | Loss: 0.00001123
Iteration 182/1000 | Loss: 0.00001123
Iteration 183/1000 | Loss: 0.00001123
Iteration 184/1000 | Loss: 0.00001123
Iteration 185/1000 | Loss: 0.00001123
Iteration 186/1000 | Loss: 0.00001123
Iteration 187/1000 | Loss: 0.00001123
Iteration 188/1000 | Loss: 0.00001123
Iteration 189/1000 | Loss: 0.00001123
Iteration 190/1000 | Loss: 0.00001123
Iteration 191/1000 | Loss: 0.00001123
Iteration 192/1000 | Loss: 0.00001123
Iteration 193/1000 | Loss: 0.00001123
Iteration 194/1000 | Loss: 0.00001123
Iteration 195/1000 | Loss: 0.00001123
Iteration 196/1000 | Loss: 0.00001123
Iteration 197/1000 | Loss: 0.00001123
Iteration 198/1000 | Loss: 0.00001123
Iteration 199/1000 | Loss: 0.00001123
Iteration 200/1000 | Loss: 0.00001123
Iteration 201/1000 | Loss: 0.00001123
Iteration 202/1000 | Loss: 0.00001123
Iteration 203/1000 | Loss: 0.00001123
Iteration 204/1000 | Loss: 0.00001123
Iteration 205/1000 | Loss: 0.00001123
Iteration 206/1000 | Loss: 0.00001123
Iteration 207/1000 | Loss: 0.00001123
Iteration 208/1000 | Loss: 0.00001123
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [1.1227033610339276e-05, 1.1227033610339276e-05, 1.1227033610339276e-05, 1.1227033610339276e-05, 1.1227033610339276e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1227033610339276e-05

Optimization complete. Final v2v error: 2.907416820526123 mm

Highest mean error: 3.2435004711151123 mm for frame 111

Lowest mean error: 2.576472520828247 mm for frame 13

Saving results

Total time: 44.90428304672241
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00401970
Iteration 2/25 | Loss: 0.00120361
Iteration 3/25 | Loss: 0.00113348
Iteration 4/25 | Loss: 0.00112673
Iteration 5/25 | Loss: 0.00112514
Iteration 6/25 | Loss: 0.00112463
Iteration 7/25 | Loss: 0.00112463
Iteration 8/25 | Loss: 0.00112463
Iteration 9/25 | Loss: 0.00112463
Iteration 10/25 | Loss: 0.00112463
Iteration 11/25 | Loss: 0.00112463
Iteration 12/25 | Loss: 0.00112463
Iteration 13/25 | Loss: 0.00112463
Iteration 14/25 | Loss: 0.00112463
Iteration 15/25 | Loss: 0.00112463
Iteration 16/25 | Loss: 0.00112463
Iteration 17/25 | Loss: 0.00112463
Iteration 18/25 | Loss: 0.00112463
Iteration 19/25 | Loss: 0.00112463
Iteration 20/25 | Loss: 0.00112463
Iteration 21/25 | Loss: 0.00112463
Iteration 22/25 | Loss: 0.00112463
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0011246281210333109, 0.0011246281210333109, 0.0011246281210333109, 0.0011246281210333109, 0.0011246281210333109]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011246281210333109

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25661039
Iteration 2/25 | Loss: 0.00198036
Iteration 3/25 | Loss: 0.00198036
Iteration 4/25 | Loss: 0.00198036
Iteration 5/25 | Loss: 0.00198036
Iteration 6/25 | Loss: 0.00198036
Iteration 7/25 | Loss: 0.00198036
Iteration 8/25 | Loss: 0.00198036
Iteration 9/25 | Loss: 0.00198036
Iteration 10/25 | Loss: 0.00198036
Iteration 11/25 | Loss: 0.00198036
Iteration 12/25 | Loss: 0.00198036
Iteration 13/25 | Loss: 0.00198036
Iteration 14/25 | Loss: 0.00198036
Iteration 15/25 | Loss: 0.00198036
Iteration 16/25 | Loss: 0.00198036
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001980355242267251, 0.001980355242267251, 0.001980355242267251, 0.001980355242267251, 0.001980355242267251]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001980355242267251

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00198036
Iteration 2/1000 | Loss: 0.00002175
Iteration 3/1000 | Loss: 0.00001433
Iteration 4/1000 | Loss: 0.00001175
Iteration 5/1000 | Loss: 0.00001053
Iteration 6/1000 | Loss: 0.00000986
Iteration 7/1000 | Loss: 0.00000974
Iteration 8/1000 | Loss: 0.00000934
Iteration 9/1000 | Loss: 0.00000903
Iteration 10/1000 | Loss: 0.00000888
Iteration 11/1000 | Loss: 0.00000878
Iteration 12/1000 | Loss: 0.00000858
Iteration 13/1000 | Loss: 0.00000852
Iteration 14/1000 | Loss: 0.00000848
Iteration 15/1000 | Loss: 0.00000840
Iteration 16/1000 | Loss: 0.00000834
Iteration 17/1000 | Loss: 0.00000822
Iteration 18/1000 | Loss: 0.00000819
Iteration 19/1000 | Loss: 0.00000818
Iteration 20/1000 | Loss: 0.00000818
Iteration 21/1000 | Loss: 0.00000817
Iteration 22/1000 | Loss: 0.00000816
Iteration 23/1000 | Loss: 0.00000816
Iteration 24/1000 | Loss: 0.00000815
Iteration 25/1000 | Loss: 0.00000814
Iteration 26/1000 | Loss: 0.00000814
Iteration 27/1000 | Loss: 0.00000814
Iteration 28/1000 | Loss: 0.00000813
Iteration 29/1000 | Loss: 0.00000813
Iteration 30/1000 | Loss: 0.00000813
Iteration 31/1000 | Loss: 0.00000813
Iteration 32/1000 | Loss: 0.00000812
Iteration 33/1000 | Loss: 0.00000812
Iteration 34/1000 | Loss: 0.00000811
Iteration 35/1000 | Loss: 0.00000810
Iteration 36/1000 | Loss: 0.00000810
Iteration 37/1000 | Loss: 0.00000809
Iteration 38/1000 | Loss: 0.00000809
Iteration 39/1000 | Loss: 0.00000809
Iteration 40/1000 | Loss: 0.00000808
Iteration 41/1000 | Loss: 0.00000808
Iteration 42/1000 | Loss: 0.00000808
Iteration 43/1000 | Loss: 0.00000807
Iteration 44/1000 | Loss: 0.00000806
Iteration 45/1000 | Loss: 0.00000806
Iteration 46/1000 | Loss: 0.00000804
Iteration 47/1000 | Loss: 0.00000804
Iteration 48/1000 | Loss: 0.00000802
Iteration 49/1000 | Loss: 0.00000802
Iteration 50/1000 | Loss: 0.00000802
Iteration 51/1000 | Loss: 0.00000802
Iteration 52/1000 | Loss: 0.00000802
Iteration 53/1000 | Loss: 0.00000802
Iteration 54/1000 | Loss: 0.00000801
Iteration 55/1000 | Loss: 0.00000800
Iteration 56/1000 | Loss: 0.00000799
Iteration 57/1000 | Loss: 0.00000797
Iteration 58/1000 | Loss: 0.00000793
Iteration 59/1000 | Loss: 0.00000793
Iteration 60/1000 | Loss: 0.00000792
Iteration 61/1000 | Loss: 0.00000792
Iteration 62/1000 | Loss: 0.00000792
Iteration 63/1000 | Loss: 0.00000792
Iteration 64/1000 | Loss: 0.00000792
Iteration 65/1000 | Loss: 0.00000792
Iteration 66/1000 | Loss: 0.00000792
Iteration 67/1000 | Loss: 0.00000792
Iteration 68/1000 | Loss: 0.00000791
Iteration 69/1000 | Loss: 0.00000791
Iteration 70/1000 | Loss: 0.00000791
Iteration 71/1000 | Loss: 0.00000790
Iteration 72/1000 | Loss: 0.00000790
Iteration 73/1000 | Loss: 0.00000790
Iteration 74/1000 | Loss: 0.00000790
Iteration 75/1000 | Loss: 0.00000790
Iteration 76/1000 | Loss: 0.00000789
Iteration 77/1000 | Loss: 0.00000789
Iteration 78/1000 | Loss: 0.00000789
Iteration 79/1000 | Loss: 0.00000789
Iteration 80/1000 | Loss: 0.00000789
Iteration 81/1000 | Loss: 0.00000789
Iteration 82/1000 | Loss: 0.00000788
Iteration 83/1000 | Loss: 0.00000788
Iteration 84/1000 | Loss: 0.00000787
Iteration 85/1000 | Loss: 0.00000787
Iteration 86/1000 | Loss: 0.00000787
Iteration 87/1000 | Loss: 0.00000786
Iteration 88/1000 | Loss: 0.00000786
Iteration 89/1000 | Loss: 0.00000786
Iteration 90/1000 | Loss: 0.00000786
Iteration 91/1000 | Loss: 0.00000785
Iteration 92/1000 | Loss: 0.00000785
Iteration 93/1000 | Loss: 0.00000785
Iteration 94/1000 | Loss: 0.00000785
Iteration 95/1000 | Loss: 0.00000785
Iteration 96/1000 | Loss: 0.00000784
Iteration 97/1000 | Loss: 0.00000784
Iteration 98/1000 | Loss: 0.00000784
Iteration 99/1000 | Loss: 0.00000784
Iteration 100/1000 | Loss: 0.00000784
Iteration 101/1000 | Loss: 0.00000784
Iteration 102/1000 | Loss: 0.00000784
Iteration 103/1000 | Loss: 0.00000784
Iteration 104/1000 | Loss: 0.00000783
Iteration 105/1000 | Loss: 0.00000783
Iteration 106/1000 | Loss: 0.00000783
Iteration 107/1000 | Loss: 0.00000782
Iteration 108/1000 | Loss: 0.00000782
Iteration 109/1000 | Loss: 0.00000782
Iteration 110/1000 | Loss: 0.00000782
Iteration 111/1000 | Loss: 0.00000782
Iteration 112/1000 | Loss: 0.00000781
Iteration 113/1000 | Loss: 0.00000781
Iteration 114/1000 | Loss: 0.00000781
Iteration 115/1000 | Loss: 0.00000781
Iteration 116/1000 | Loss: 0.00000781
Iteration 117/1000 | Loss: 0.00000781
Iteration 118/1000 | Loss: 0.00000781
Iteration 119/1000 | Loss: 0.00000780
Iteration 120/1000 | Loss: 0.00000780
Iteration 121/1000 | Loss: 0.00000780
Iteration 122/1000 | Loss: 0.00000780
Iteration 123/1000 | Loss: 0.00000780
Iteration 124/1000 | Loss: 0.00000780
Iteration 125/1000 | Loss: 0.00000780
Iteration 126/1000 | Loss: 0.00000780
Iteration 127/1000 | Loss: 0.00000780
Iteration 128/1000 | Loss: 0.00000780
Iteration 129/1000 | Loss: 0.00000780
Iteration 130/1000 | Loss: 0.00000780
Iteration 131/1000 | Loss: 0.00000780
Iteration 132/1000 | Loss: 0.00000780
Iteration 133/1000 | Loss: 0.00000780
Iteration 134/1000 | Loss: 0.00000780
Iteration 135/1000 | Loss: 0.00000779
Iteration 136/1000 | Loss: 0.00000779
Iteration 137/1000 | Loss: 0.00000779
Iteration 138/1000 | Loss: 0.00000779
Iteration 139/1000 | Loss: 0.00000779
Iteration 140/1000 | Loss: 0.00000779
Iteration 141/1000 | Loss: 0.00000779
Iteration 142/1000 | Loss: 0.00000779
Iteration 143/1000 | Loss: 0.00000779
Iteration 144/1000 | Loss: 0.00000779
Iteration 145/1000 | Loss: 0.00000779
Iteration 146/1000 | Loss: 0.00000779
Iteration 147/1000 | Loss: 0.00000779
Iteration 148/1000 | Loss: 0.00000779
Iteration 149/1000 | Loss: 0.00000779
Iteration 150/1000 | Loss: 0.00000779
Iteration 151/1000 | Loss: 0.00000778
Iteration 152/1000 | Loss: 0.00000778
Iteration 153/1000 | Loss: 0.00000778
Iteration 154/1000 | Loss: 0.00000778
Iteration 155/1000 | Loss: 0.00000778
Iteration 156/1000 | Loss: 0.00000778
Iteration 157/1000 | Loss: 0.00000778
Iteration 158/1000 | Loss: 0.00000778
Iteration 159/1000 | Loss: 0.00000778
Iteration 160/1000 | Loss: 0.00000778
Iteration 161/1000 | Loss: 0.00000778
Iteration 162/1000 | Loss: 0.00000778
Iteration 163/1000 | Loss: 0.00000778
Iteration 164/1000 | Loss: 0.00000778
Iteration 165/1000 | Loss: 0.00000778
Iteration 166/1000 | Loss: 0.00000778
Iteration 167/1000 | Loss: 0.00000778
Iteration 168/1000 | Loss: 0.00000778
Iteration 169/1000 | Loss: 0.00000778
Iteration 170/1000 | Loss: 0.00000777
Iteration 171/1000 | Loss: 0.00000777
Iteration 172/1000 | Loss: 0.00000777
Iteration 173/1000 | Loss: 0.00000777
Iteration 174/1000 | Loss: 0.00000777
Iteration 175/1000 | Loss: 0.00000777
Iteration 176/1000 | Loss: 0.00000777
Iteration 177/1000 | Loss: 0.00000777
Iteration 178/1000 | Loss: 0.00000777
Iteration 179/1000 | Loss: 0.00000777
Iteration 180/1000 | Loss: 0.00000777
Iteration 181/1000 | Loss: 0.00000777
Iteration 182/1000 | Loss: 0.00000777
Iteration 183/1000 | Loss: 0.00000777
Iteration 184/1000 | Loss: 0.00000777
Iteration 185/1000 | Loss: 0.00000777
Iteration 186/1000 | Loss: 0.00000777
Iteration 187/1000 | Loss: 0.00000777
Iteration 188/1000 | Loss: 0.00000777
Iteration 189/1000 | Loss: 0.00000777
Iteration 190/1000 | Loss: 0.00000777
Iteration 191/1000 | Loss: 0.00000777
Iteration 192/1000 | Loss: 0.00000777
Iteration 193/1000 | Loss: 0.00000777
Iteration 194/1000 | Loss: 0.00000777
Iteration 195/1000 | Loss: 0.00000777
Iteration 196/1000 | Loss: 0.00000777
Iteration 197/1000 | Loss: 0.00000777
Iteration 198/1000 | Loss: 0.00000777
Iteration 199/1000 | Loss: 0.00000777
Iteration 200/1000 | Loss: 0.00000777
Iteration 201/1000 | Loss: 0.00000777
Iteration 202/1000 | Loss: 0.00000777
Iteration 203/1000 | Loss: 0.00000777
Iteration 204/1000 | Loss: 0.00000777
Iteration 205/1000 | Loss: 0.00000777
Iteration 206/1000 | Loss: 0.00000777
Iteration 207/1000 | Loss: 0.00000777
Iteration 208/1000 | Loss: 0.00000777
Iteration 209/1000 | Loss: 0.00000777
Iteration 210/1000 | Loss: 0.00000777
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [7.770563570375089e-06, 7.770563570375089e-06, 7.770563570375089e-06, 7.770563570375089e-06, 7.770563570375089e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.770563570375089e-06

Optimization complete. Final v2v error: 2.397019147872925 mm

Highest mean error: 3.364492177963257 mm for frame 53

Lowest mean error: 2.225593328475952 mm for frame 146

Saving results

Total time: 41.583606004714966
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00988283
Iteration 2/25 | Loss: 0.00212390
Iteration 3/25 | Loss: 0.00173842
Iteration 4/25 | Loss: 0.00171891
Iteration 5/25 | Loss: 0.00159501
Iteration 6/25 | Loss: 0.00138803
Iteration 7/25 | Loss: 0.00135312
Iteration 8/25 | Loss: 0.00133378
Iteration 9/25 | Loss: 0.00132108
Iteration 10/25 | Loss: 0.00133438
Iteration 11/25 | Loss: 0.00132304
Iteration 12/25 | Loss: 0.00132874
Iteration 13/25 | Loss: 0.00132839
Iteration 14/25 | Loss: 0.00133020
Iteration 15/25 | Loss: 0.00133174
Iteration 16/25 | Loss: 0.00132910
Iteration 17/25 | Loss: 0.00132848
Iteration 18/25 | Loss: 0.00133552
Iteration 19/25 | Loss: 0.00132997
Iteration 20/25 | Loss: 0.00133398
Iteration 21/25 | Loss: 0.00133492
Iteration 22/25 | Loss: 0.00132519
Iteration 23/25 | Loss: 0.00132723
Iteration 24/25 | Loss: 0.00131875
Iteration 25/25 | Loss: 0.00131394

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20099616
Iteration 2/25 | Loss: 0.00332318
Iteration 3/25 | Loss: 0.00332318
Iteration 4/25 | Loss: 0.00332318
Iteration 5/25 | Loss: 0.00332318
Iteration 6/25 | Loss: 0.00332317
Iteration 7/25 | Loss: 0.00332317
Iteration 8/25 | Loss: 0.00332317
Iteration 9/25 | Loss: 0.00332317
Iteration 10/25 | Loss: 0.00332317
Iteration 11/25 | Loss: 0.00332317
Iteration 12/25 | Loss: 0.00332317
Iteration 13/25 | Loss: 0.00332317
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.003323173150420189, 0.003323173150420189, 0.003323173150420189, 0.003323173150420189, 0.003323173150420189]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003323173150420189

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00332317
Iteration 2/1000 | Loss: 0.00017111
Iteration 3/1000 | Loss: 0.00015126
Iteration 4/1000 | Loss: 0.00017153
Iteration 5/1000 | Loss: 0.00009078
Iteration 6/1000 | Loss: 0.00011439
Iteration 7/1000 | Loss: 0.00040673
Iteration 8/1000 | Loss: 0.00016621
Iteration 9/1000 | Loss: 0.00031609
Iteration 10/1000 | Loss: 0.00009839
Iteration 11/1000 | Loss: 0.00008732
Iteration 12/1000 | Loss: 0.00008788
Iteration 13/1000 | Loss: 0.00007851
Iteration 14/1000 | Loss: 0.00007611
Iteration 15/1000 | Loss: 0.00033092
Iteration 16/1000 | Loss: 0.00021188
Iteration 17/1000 | Loss: 0.00015205
Iteration 18/1000 | Loss: 0.00024870
Iteration 19/1000 | Loss: 0.00041896
Iteration 20/1000 | Loss: 0.00008452
Iteration 21/1000 | Loss: 0.00014659
Iteration 22/1000 | Loss: 0.00024616
Iteration 23/1000 | Loss: 0.00007522
Iteration 24/1000 | Loss: 0.00018321
Iteration 25/1000 | Loss: 0.00019389
Iteration 26/1000 | Loss: 0.00020539
Iteration 27/1000 | Loss: 0.00007656
Iteration 28/1000 | Loss: 0.00019665
Iteration 29/1000 | Loss: 0.00020222
Iteration 30/1000 | Loss: 0.00038512
Iteration 31/1000 | Loss: 0.00029960
Iteration 32/1000 | Loss: 0.00026678
Iteration 33/1000 | Loss: 0.00033297
Iteration 34/1000 | Loss: 0.00114127
Iteration 35/1000 | Loss: 0.00028728
Iteration 36/1000 | Loss: 0.00022201
Iteration 37/1000 | Loss: 0.00028110
Iteration 38/1000 | Loss: 0.00101629
Iteration 39/1000 | Loss: 0.00012833
Iteration 40/1000 | Loss: 0.00008257
Iteration 41/1000 | Loss: 0.00007316
Iteration 42/1000 | Loss: 0.00006629
Iteration 43/1000 | Loss: 0.00006124
Iteration 44/1000 | Loss: 0.00092183
Iteration 45/1000 | Loss: 0.00006989
Iteration 46/1000 | Loss: 0.00006217
Iteration 47/1000 | Loss: 0.00005848
Iteration 48/1000 | Loss: 0.00005531
Iteration 49/1000 | Loss: 0.00005296
Iteration 50/1000 | Loss: 0.00006201
Iteration 51/1000 | Loss: 0.00005740
Iteration 52/1000 | Loss: 0.00005461
Iteration 53/1000 | Loss: 0.00005237
Iteration 54/1000 | Loss: 0.00127665
Iteration 55/1000 | Loss: 0.00475071
Iteration 56/1000 | Loss: 0.00126468
Iteration 57/1000 | Loss: 0.00008753
Iteration 58/1000 | Loss: 0.00006188
Iteration 59/1000 | Loss: 0.00005398
Iteration 60/1000 | Loss: 0.00004820
Iteration 61/1000 | Loss: 0.00004226
Iteration 62/1000 | Loss: 0.00004292
Iteration 63/1000 | Loss: 0.00003533
Iteration 64/1000 | Loss: 0.00003908
Iteration 65/1000 | Loss: 0.00003233
Iteration 66/1000 | Loss: 0.00003063
Iteration 67/1000 | Loss: 0.00002866
Iteration 68/1000 | Loss: 0.00002685
Iteration 69/1000 | Loss: 0.00002604
Iteration 70/1000 | Loss: 0.00002529
Iteration 71/1000 | Loss: 0.00002457
Iteration 72/1000 | Loss: 0.00002402
Iteration 73/1000 | Loss: 0.00002365
Iteration 74/1000 | Loss: 0.00002328
Iteration 75/1000 | Loss: 0.00002313
Iteration 76/1000 | Loss: 0.00002311
Iteration 77/1000 | Loss: 0.00002308
Iteration 78/1000 | Loss: 0.00002290
Iteration 79/1000 | Loss: 0.00002280
Iteration 80/1000 | Loss: 0.00002280
Iteration 81/1000 | Loss: 0.00002279
Iteration 82/1000 | Loss: 0.00002279
Iteration 83/1000 | Loss: 0.00002278
Iteration 84/1000 | Loss: 0.00002278
Iteration 85/1000 | Loss: 0.00002277
Iteration 86/1000 | Loss: 0.00002276
Iteration 87/1000 | Loss: 0.00002275
Iteration 88/1000 | Loss: 0.00002275
Iteration 89/1000 | Loss: 0.00002275
Iteration 90/1000 | Loss: 0.00002275
Iteration 91/1000 | Loss: 0.00002273
Iteration 92/1000 | Loss: 0.00002273
Iteration 93/1000 | Loss: 0.00002273
Iteration 94/1000 | Loss: 0.00002273
Iteration 95/1000 | Loss: 0.00002273
Iteration 96/1000 | Loss: 0.00002273
Iteration 97/1000 | Loss: 0.00002273
Iteration 98/1000 | Loss: 0.00002273
Iteration 99/1000 | Loss: 0.00002272
Iteration 100/1000 | Loss: 0.00002272
Iteration 101/1000 | Loss: 0.00002272
Iteration 102/1000 | Loss: 0.00002271
Iteration 103/1000 | Loss: 0.00002271
Iteration 104/1000 | Loss: 0.00002271
Iteration 105/1000 | Loss: 0.00002271
Iteration 106/1000 | Loss: 0.00002271
Iteration 107/1000 | Loss: 0.00002271
Iteration 108/1000 | Loss: 0.00002271
Iteration 109/1000 | Loss: 0.00002271
Iteration 110/1000 | Loss: 0.00002271
Iteration 111/1000 | Loss: 0.00016607
Iteration 112/1000 | Loss: 0.00002724
Iteration 113/1000 | Loss: 0.00002570
Iteration 114/1000 | Loss: 0.00002472
Iteration 115/1000 | Loss: 0.00018743
Iteration 116/1000 | Loss: 0.00024149
Iteration 117/1000 | Loss: 0.00012270
Iteration 118/1000 | Loss: 0.00024003
Iteration 119/1000 | Loss: 0.00012737
Iteration 120/1000 | Loss: 0.00020875
Iteration 121/1000 | Loss: 0.00002333
Iteration 122/1000 | Loss: 0.00002293
Iteration 123/1000 | Loss: 0.00002274
Iteration 124/1000 | Loss: 0.00002267
Iteration 125/1000 | Loss: 0.00002267
Iteration 126/1000 | Loss: 0.00002267
Iteration 127/1000 | Loss: 0.00002267
Iteration 128/1000 | Loss: 0.00002267
Iteration 129/1000 | Loss: 0.00002266
Iteration 130/1000 | Loss: 0.00002266
Iteration 131/1000 | Loss: 0.00002266
Iteration 132/1000 | Loss: 0.00002266
Iteration 133/1000 | Loss: 0.00002266
Iteration 134/1000 | Loss: 0.00002266
Iteration 135/1000 | Loss: 0.00002265
Iteration 136/1000 | Loss: 0.00002265
Iteration 137/1000 | Loss: 0.00002265
Iteration 138/1000 | Loss: 0.00002265
Iteration 139/1000 | Loss: 0.00002265
Iteration 140/1000 | Loss: 0.00002265
Iteration 141/1000 | Loss: 0.00002265
Iteration 142/1000 | Loss: 0.00002265
Iteration 143/1000 | Loss: 0.00002265
Iteration 144/1000 | Loss: 0.00002264
Iteration 145/1000 | Loss: 0.00002264
Iteration 146/1000 | Loss: 0.00002264
Iteration 147/1000 | Loss: 0.00002264
Iteration 148/1000 | Loss: 0.00002263
Iteration 149/1000 | Loss: 0.00002263
Iteration 150/1000 | Loss: 0.00002263
Iteration 151/1000 | Loss: 0.00002263
Iteration 152/1000 | Loss: 0.00002263
Iteration 153/1000 | Loss: 0.00002263
Iteration 154/1000 | Loss: 0.00002263
Iteration 155/1000 | Loss: 0.00002263
Iteration 156/1000 | Loss: 0.00002263
Iteration 157/1000 | Loss: 0.00002263
Iteration 158/1000 | Loss: 0.00002263
Iteration 159/1000 | Loss: 0.00002263
Iteration 160/1000 | Loss: 0.00002263
Iteration 161/1000 | Loss: 0.00002263
Iteration 162/1000 | Loss: 0.00002263
Iteration 163/1000 | Loss: 0.00002263
Iteration 164/1000 | Loss: 0.00002262
Iteration 165/1000 | Loss: 0.00002262
Iteration 166/1000 | Loss: 0.00002262
Iteration 167/1000 | Loss: 0.00002262
Iteration 168/1000 | Loss: 0.00002262
Iteration 169/1000 | Loss: 0.00002262
Iteration 170/1000 | Loss: 0.00002262
Iteration 171/1000 | Loss: 0.00002262
Iteration 172/1000 | Loss: 0.00002262
Iteration 173/1000 | Loss: 0.00002262
Iteration 174/1000 | Loss: 0.00002262
Iteration 175/1000 | Loss: 0.00002262
Iteration 176/1000 | Loss: 0.00002262
Iteration 177/1000 | Loss: 0.00002262
Iteration 178/1000 | Loss: 0.00002262
Iteration 179/1000 | Loss: 0.00002262
Iteration 180/1000 | Loss: 0.00002262
Iteration 181/1000 | Loss: 0.00002262
Iteration 182/1000 | Loss: 0.00002262
Iteration 183/1000 | Loss: 0.00002262
Iteration 184/1000 | Loss: 0.00002262
Iteration 185/1000 | Loss: 0.00002262
Iteration 186/1000 | Loss: 0.00002262
Iteration 187/1000 | Loss: 0.00002262
Iteration 188/1000 | Loss: 0.00002262
Iteration 189/1000 | Loss: 0.00002262
Iteration 190/1000 | Loss: 0.00002262
Iteration 191/1000 | Loss: 0.00002262
Iteration 192/1000 | Loss: 0.00002262
Iteration 193/1000 | Loss: 0.00002262
Iteration 194/1000 | Loss: 0.00002262
Iteration 195/1000 | Loss: 0.00002262
Iteration 196/1000 | Loss: 0.00002262
Iteration 197/1000 | Loss: 0.00002262
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [2.2615959096583538e-05, 2.2615959096583538e-05, 2.2615959096583538e-05, 2.2615959096583538e-05, 2.2615959096583538e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2615959096583538e-05

Optimization complete. Final v2v error: 3.6789634227752686 mm

Highest mean error: 8.094871520996094 mm for frame 116

Lowest mean error: 2.4316248893737793 mm for frame 43

Saving results

Total time: 170.23508167266846
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00718584
Iteration 2/25 | Loss: 0.00135272
Iteration 3/25 | Loss: 0.00119649
Iteration 4/25 | Loss: 0.00115888
Iteration 5/25 | Loss: 0.00115101
Iteration 6/25 | Loss: 0.00115239
Iteration 7/25 | Loss: 0.00114745
Iteration 8/25 | Loss: 0.00114700
Iteration 9/25 | Loss: 0.00114689
Iteration 10/25 | Loss: 0.00114681
Iteration 11/25 | Loss: 0.00114680
Iteration 12/25 | Loss: 0.00114680
Iteration 13/25 | Loss: 0.00114680
Iteration 14/25 | Loss: 0.00114680
Iteration 15/25 | Loss: 0.00114680
Iteration 16/25 | Loss: 0.00114680
Iteration 17/25 | Loss: 0.00114680
Iteration 18/25 | Loss: 0.00114680
Iteration 19/25 | Loss: 0.00114680
Iteration 20/25 | Loss: 0.00114680
Iteration 21/25 | Loss: 0.00114680
Iteration 22/25 | Loss: 0.00114679
Iteration 23/25 | Loss: 0.00114679
Iteration 24/25 | Loss: 0.00114679
Iteration 25/25 | Loss: 0.00114679

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.82112682
Iteration 2/25 | Loss: 0.00196951
Iteration 3/25 | Loss: 0.00196950
Iteration 4/25 | Loss: 0.00196950
Iteration 5/25 | Loss: 0.00196950
Iteration 6/25 | Loss: 0.00196950
Iteration 7/25 | Loss: 0.00196950
Iteration 8/25 | Loss: 0.00196950
Iteration 9/25 | Loss: 0.00196950
Iteration 10/25 | Loss: 0.00196950
Iteration 11/25 | Loss: 0.00196950
Iteration 12/25 | Loss: 0.00196950
Iteration 13/25 | Loss: 0.00196950
Iteration 14/25 | Loss: 0.00196950
Iteration 15/25 | Loss: 0.00196950
Iteration 16/25 | Loss: 0.00196950
Iteration 17/25 | Loss: 0.00196950
Iteration 18/25 | Loss: 0.00196950
Iteration 19/25 | Loss: 0.00196950
Iteration 20/25 | Loss: 0.00196950
Iteration 21/25 | Loss: 0.00196950
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0019694995135068893, 0.0019694995135068893, 0.0019694995135068893, 0.0019694995135068893, 0.0019694995135068893]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019694995135068893

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00196950
Iteration 2/1000 | Loss: 0.00001552
Iteration 3/1000 | Loss: 0.00001274
Iteration 4/1000 | Loss: 0.00001181
Iteration 5/1000 | Loss: 0.00001125
Iteration 6/1000 | Loss: 0.00001083
Iteration 7/1000 | Loss: 0.00001058
Iteration 8/1000 | Loss: 0.00001032
Iteration 9/1000 | Loss: 0.00006034
Iteration 10/1000 | Loss: 0.00006034
Iteration 11/1000 | Loss: 0.00001754
Iteration 12/1000 | Loss: 0.00001173
Iteration 13/1000 | Loss: 0.00001034
Iteration 14/1000 | Loss: 0.00000985
Iteration 15/1000 | Loss: 0.00000982
Iteration 16/1000 | Loss: 0.00000978
Iteration 17/1000 | Loss: 0.00000974
Iteration 18/1000 | Loss: 0.00000973
Iteration 19/1000 | Loss: 0.00005167
Iteration 20/1000 | Loss: 0.00001227
Iteration 21/1000 | Loss: 0.00001069
Iteration 22/1000 | Loss: 0.00000972
Iteration 23/1000 | Loss: 0.00000961
Iteration 24/1000 | Loss: 0.00000958
Iteration 25/1000 | Loss: 0.00000957
Iteration 26/1000 | Loss: 0.00000955
Iteration 27/1000 | Loss: 0.00000955
Iteration 28/1000 | Loss: 0.00000954
Iteration 29/1000 | Loss: 0.00000954
Iteration 30/1000 | Loss: 0.00000953
Iteration 31/1000 | Loss: 0.00000953
Iteration 32/1000 | Loss: 0.00000953
Iteration 33/1000 | Loss: 0.00000952
Iteration 34/1000 | Loss: 0.00000952
Iteration 35/1000 | Loss: 0.00000952
Iteration 36/1000 | Loss: 0.00000951
Iteration 37/1000 | Loss: 0.00000947
Iteration 38/1000 | Loss: 0.00000947
Iteration 39/1000 | Loss: 0.00000944
Iteration 40/1000 | Loss: 0.00000938
Iteration 41/1000 | Loss: 0.00000937
Iteration 42/1000 | Loss: 0.00000937
Iteration 43/1000 | Loss: 0.00000936
Iteration 44/1000 | Loss: 0.00000936
Iteration 45/1000 | Loss: 0.00000932
Iteration 46/1000 | Loss: 0.00000931
Iteration 47/1000 | Loss: 0.00000930
Iteration 48/1000 | Loss: 0.00000928
Iteration 49/1000 | Loss: 0.00000927
Iteration 50/1000 | Loss: 0.00000926
Iteration 51/1000 | Loss: 0.00000925
Iteration 52/1000 | Loss: 0.00000925
Iteration 53/1000 | Loss: 0.00000924
Iteration 54/1000 | Loss: 0.00000924
Iteration 55/1000 | Loss: 0.00000923
Iteration 56/1000 | Loss: 0.00000923
Iteration 57/1000 | Loss: 0.00000923
Iteration 58/1000 | Loss: 0.00000922
Iteration 59/1000 | Loss: 0.00000922
Iteration 60/1000 | Loss: 0.00000922
Iteration 61/1000 | Loss: 0.00000922
Iteration 62/1000 | Loss: 0.00000922
Iteration 63/1000 | Loss: 0.00000922
Iteration 64/1000 | Loss: 0.00000922
Iteration 65/1000 | Loss: 0.00000922
Iteration 66/1000 | Loss: 0.00000921
Iteration 67/1000 | Loss: 0.00000921
Iteration 68/1000 | Loss: 0.00000920
Iteration 69/1000 | Loss: 0.00000920
Iteration 70/1000 | Loss: 0.00000920
Iteration 71/1000 | Loss: 0.00000920
Iteration 72/1000 | Loss: 0.00000920
Iteration 73/1000 | Loss: 0.00000920
Iteration 74/1000 | Loss: 0.00000920
Iteration 75/1000 | Loss: 0.00000920
Iteration 76/1000 | Loss: 0.00000920
Iteration 77/1000 | Loss: 0.00000919
Iteration 78/1000 | Loss: 0.00000919
Iteration 79/1000 | Loss: 0.00000919
Iteration 80/1000 | Loss: 0.00000919
Iteration 81/1000 | Loss: 0.00000919
Iteration 82/1000 | Loss: 0.00000918
Iteration 83/1000 | Loss: 0.00000918
Iteration 84/1000 | Loss: 0.00000918
Iteration 85/1000 | Loss: 0.00000917
Iteration 86/1000 | Loss: 0.00000917
Iteration 87/1000 | Loss: 0.00000917
Iteration 88/1000 | Loss: 0.00000917
Iteration 89/1000 | Loss: 0.00000917
Iteration 90/1000 | Loss: 0.00000916
Iteration 91/1000 | Loss: 0.00000916
Iteration 92/1000 | Loss: 0.00000916
Iteration 93/1000 | Loss: 0.00000916
Iteration 94/1000 | Loss: 0.00000916
Iteration 95/1000 | Loss: 0.00000916
Iteration 96/1000 | Loss: 0.00000915
Iteration 97/1000 | Loss: 0.00000915
Iteration 98/1000 | Loss: 0.00000915
Iteration 99/1000 | Loss: 0.00000915
Iteration 100/1000 | Loss: 0.00000915
Iteration 101/1000 | Loss: 0.00000915
Iteration 102/1000 | Loss: 0.00000915
Iteration 103/1000 | Loss: 0.00000914
Iteration 104/1000 | Loss: 0.00000914
Iteration 105/1000 | Loss: 0.00000914
Iteration 106/1000 | Loss: 0.00000914
Iteration 107/1000 | Loss: 0.00000913
Iteration 108/1000 | Loss: 0.00000913
Iteration 109/1000 | Loss: 0.00000913
Iteration 110/1000 | Loss: 0.00000913
Iteration 111/1000 | Loss: 0.00000913
Iteration 112/1000 | Loss: 0.00000912
Iteration 113/1000 | Loss: 0.00000912
Iteration 114/1000 | Loss: 0.00000912
Iteration 115/1000 | Loss: 0.00000912
Iteration 116/1000 | Loss: 0.00000912
Iteration 117/1000 | Loss: 0.00000912
Iteration 118/1000 | Loss: 0.00000912
Iteration 119/1000 | Loss: 0.00000912
Iteration 120/1000 | Loss: 0.00000912
Iteration 121/1000 | Loss: 0.00000911
Iteration 122/1000 | Loss: 0.00000911
Iteration 123/1000 | Loss: 0.00000911
Iteration 124/1000 | Loss: 0.00000910
Iteration 125/1000 | Loss: 0.00000910
Iteration 126/1000 | Loss: 0.00000910
Iteration 127/1000 | Loss: 0.00000910
Iteration 128/1000 | Loss: 0.00000910
Iteration 129/1000 | Loss: 0.00000910
Iteration 130/1000 | Loss: 0.00000910
Iteration 131/1000 | Loss: 0.00000910
Iteration 132/1000 | Loss: 0.00000910
Iteration 133/1000 | Loss: 0.00000910
Iteration 134/1000 | Loss: 0.00000910
Iteration 135/1000 | Loss: 0.00000910
Iteration 136/1000 | Loss: 0.00000909
Iteration 137/1000 | Loss: 0.00000909
Iteration 138/1000 | Loss: 0.00000909
Iteration 139/1000 | Loss: 0.00000909
Iteration 140/1000 | Loss: 0.00000908
Iteration 141/1000 | Loss: 0.00000908
Iteration 142/1000 | Loss: 0.00000908
Iteration 143/1000 | Loss: 0.00000908
Iteration 144/1000 | Loss: 0.00000908
Iteration 145/1000 | Loss: 0.00000908
Iteration 146/1000 | Loss: 0.00000908
Iteration 147/1000 | Loss: 0.00000908
Iteration 148/1000 | Loss: 0.00000908
Iteration 149/1000 | Loss: 0.00000908
Iteration 150/1000 | Loss: 0.00000908
Iteration 151/1000 | Loss: 0.00000908
Iteration 152/1000 | Loss: 0.00000907
Iteration 153/1000 | Loss: 0.00000907
Iteration 154/1000 | Loss: 0.00000907
Iteration 155/1000 | Loss: 0.00000907
Iteration 156/1000 | Loss: 0.00000907
Iteration 157/1000 | Loss: 0.00000907
Iteration 158/1000 | Loss: 0.00000907
Iteration 159/1000 | Loss: 0.00000907
Iteration 160/1000 | Loss: 0.00000907
Iteration 161/1000 | Loss: 0.00000907
Iteration 162/1000 | Loss: 0.00000907
Iteration 163/1000 | Loss: 0.00000907
Iteration 164/1000 | Loss: 0.00000907
Iteration 165/1000 | Loss: 0.00000907
Iteration 166/1000 | Loss: 0.00000907
Iteration 167/1000 | Loss: 0.00000907
Iteration 168/1000 | Loss: 0.00000907
Iteration 169/1000 | Loss: 0.00000907
Iteration 170/1000 | Loss: 0.00000907
Iteration 171/1000 | Loss: 0.00000907
Iteration 172/1000 | Loss: 0.00000907
Iteration 173/1000 | Loss: 0.00000907
Iteration 174/1000 | Loss: 0.00000907
Iteration 175/1000 | Loss: 0.00000907
Iteration 176/1000 | Loss: 0.00000907
Iteration 177/1000 | Loss: 0.00000907
Iteration 178/1000 | Loss: 0.00000907
Iteration 179/1000 | Loss: 0.00000907
Iteration 180/1000 | Loss: 0.00000907
Iteration 181/1000 | Loss: 0.00000907
Iteration 182/1000 | Loss: 0.00000907
Iteration 183/1000 | Loss: 0.00000907
Iteration 184/1000 | Loss: 0.00000907
Iteration 185/1000 | Loss: 0.00000907
Iteration 186/1000 | Loss: 0.00000907
Iteration 187/1000 | Loss: 0.00000907
Iteration 188/1000 | Loss: 0.00000907
Iteration 189/1000 | Loss: 0.00000907
Iteration 190/1000 | Loss: 0.00000907
Iteration 191/1000 | Loss: 0.00000907
Iteration 192/1000 | Loss: 0.00000907
Iteration 193/1000 | Loss: 0.00000907
Iteration 194/1000 | Loss: 0.00000907
Iteration 195/1000 | Loss: 0.00000907
Iteration 196/1000 | Loss: 0.00000907
Iteration 197/1000 | Loss: 0.00000907
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [9.07102548808325e-06, 9.07102548808325e-06, 9.07102548808325e-06, 9.07102548808325e-06, 9.07102548808325e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.07102548808325e-06

Optimization complete. Final v2v error: 2.629237651824951 mm

Highest mean error: 3.016188859939575 mm for frame 149

Lowest mean error: 2.4819865226745605 mm for frame 26

Saving results

Total time: 64.06820964813232
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00972205
Iteration 2/25 | Loss: 0.00178384
Iteration 3/25 | Loss: 0.00139443
Iteration 4/25 | Loss: 0.00137069
Iteration 5/25 | Loss: 0.00136364
Iteration 6/25 | Loss: 0.00136263
Iteration 7/25 | Loss: 0.00136263
Iteration 8/25 | Loss: 0.00136263
Iteration 9/25 | Loss: 0.00136263
Iteration 10/25 | Loss: 0.00136263
Iteration 11/25 | Loss: 0.00136263
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013626337749883533, 0.0013626337749883533, 0.0013626337749883533, 0.0013626337749883533, 0.0013626337749883533]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013626337749883533

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.48180157
Iteration 2/25 | Loss: 0.00182393
Iteration 3/25 | Loss: 0.00182393
Iteration 4/25 | Loss: 0.00182393
Iteration 5/25 | Loss: 0.00182393
Iteration 6/25 | Loss: 0.00182393
Iteration 7/25 | Loss: 0.00182393
Iteration 8/25 | Loss: 0.00182393
Iteration 9/25 | Loss: 0.00182393
Iteration 10/25 | Loss: 0.00182393
Iteration 11/25 | Loss: 0.00182393
Iteration 12/25 | Loss: 0.00182393
Iteration 13/25 | Loss: 0.00182392
Iteration 14/25 | Loss: 0.00182392
Iteration 15/25 | Loss: 0.00182392
Iteration 16/25 | Loss: 0.00182392
Iteration 17/25 | Loss: 0.00182392
Iteration 18/25 | Loss: 0.00182393
Iteration 19/25 | Loss: 0.00182392
Iteration 20/25 | Loss: 0.00182392
Iteration 21/25 | Loss: 0.00182392
Iteration 22/25 | Loss: 0.00182392
Iteration 23/25 | Loss: 0.00182392
Iteration 24/25 | Loss: 0.00182392
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0018239248311147094, 0.0018239248311147094, 0.0018239248311147094, 0.0018239248311147094, 0.0018239248311147094]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018239248311147094

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00182392
Iteration 2/1000 | Loss: 0.00006016
Iteration 3/1000 | Loss: 0.00004256
Iteration 4/1000 | Loss: 0.00003891
Iteration 5/1000 | Loss: 0.00003683
Iteration 6/1000 | Loss: 0.00003578
Iteration 7/1000 | Loss: 0.00003492
Iteration 8/1000 | Loss: 0.00003442
Iteration 9/1000 | Loss: 0.00003407
Iteration 10/1000 | Loss: 0.00003370
Iteration 11/1000 | Loss: 0.00003337
Iteration 12/1000 | Loss: 0.00003299
Iteration 13/1000 | Loss: 0.00003273
Iteration 14/1000 | Loss: 0.00003247
Iteration 15/1000 | Loss: 0.00003218
Iteration 16/1000 | Loss: 0.00003194
Iteration 17/1000 | Loss: 0.00003174
Iteration 18/1000 | Loss: 0.00003169
Iteration 19/1000 | Loss: 0.00003169
Iteration 20/1000 | Loss: 0.00003160
Iteration 21/1000 | Loss: 0.00003156
Iteration 22/1000 | Loss: 0.00003144
Iteration 23/1000 | Loss: 0.00003140
Iteration 24/1000 | Loss: 0.00003140
Iteration 25/1000 | Loss: 0.00003131
Iteration 26/1000 | Loss: 0.00003129
Iteration 27/1000 | Loss: 0.00003127
Iteration 28/1000 | Loss: 0.00003125
Iteration 29/1000 | Loss: 0.00003124
Iteration 30/1000 | Loss: 0.00003124
Iteration 31/1000 | Loss: 0.00003118
Iteration 32/1000 | Loss: 0.00003118
Iteration 33/1000 | Loss: 0.00003118
Iteration 34/1000 | Loss: 0.00003117
Iteration 35/1000 | Loss: 0.00003117
Iteration 36/1000 | Loss: 0.00003117
Iteration 37/1000 | Loss: 0.00003117
Iteration 38/1000 | Loss: 0.00003117
Iteration 39/1000 | Loss: 0.00003117
Iteration 40/1000 | Loss: 0.00003116
Iteration 41/1000 | Loss: 0.00003115
Iteration 42/1000 | Loss: 0.00003115
Iteration 43/1000 | Loss: 0.00003115
Iteration 44/1000 | Loss: 0.00003115
Iteration 45/1000 | Loss: 0.00003115
Iteration 46/1000 | Loss: 0.00003115
Iteration 47/1000 | Loss: 0.00003115
Iteration 48/1000 | Loss: 0.00003115
Iteration 49/1000 | Loss: 0.00003115
Iteration 50/1000 | Loss: 0.00003115
Iteration 51/1000 | Loss: 0.00003115
Iteration 52/1000 | Loss: 0.00003114
Iteration 53/1000 | Loss: 0.00003114
Iteration 54/1000 | Loss: 0.00003114
Iteration 55/1000 | Loss: 0.00003114
Iteration 56/1000 | Loss: 0.00003114
Iteration 57/1000 | Loss: 0.00003114
Iteration 58/1000 | Loss: 0.00003113
Iteration 59/1000 | Loss: 0.00003113
Iteration 60/1000 | Loss: 0.00003113
Iteration 61/1000 | Loss: 0.00003113
Iteration 62/1000 | Loss: 0.00003113
Iteration 63/1000 | Loss: 0.00003113
Iteration 64/1000 | Loss: 0.00003113
Iteration 65/1000 | Loss: 0.00003112
Iteration 66/1000 | Loss: 0.00003112
Iteration 67/1000 | Loss: 0.00003112
Iteration 68/1000 | Loss: 0.00003112
Iteration 69/1000 | Loss: 0.00003112
Iteration 70/1000 | Loss: 0.00003111
Iteration 71/1000 | Loss: 0.00003111
Iteration 72/1000 | Loss: 0.00003111
Iteration 73/1000 | Loss: 0.00003111
Iteration 74/1000 | Loss: 0.00003111
Iteration 75/1000 | Loss: 0.00003111
Iteration 76/1000 | Loss: 0.00003111
Iteration 77/1000 | Loss: 0.00003111
Iteration 78/1000 | Loss: 0.00003111
Iteration 79/1000 | Loss: 0.00003111
Iteration 80/1000 | Loss: 0.00003110
Iteration 81/1000 | Loss: 0.00003110
Iteration 82/1000 | Loss: 0.00003110
Iteration 83/1000 | Loss: 0.00003110
Iteration 84/1000 | Loss: 0.00003110
Iteration 85/1000 | Loss: 0.00003110
Iteration 86/1000 | Loss: 0.00003109
Iteration 87/1000 | Loss: 0.00003109
Iteration 88/1000 | Loss: 0.00003109
Iteration 89/1000 | Loss: 0.00003109
Iteration 90/1000 | Loss: 0.00003109
Iteration 91/1000 | Loss: 0.00003108
Iteration 92/1000 | Loss: 0.00003108
Iteration 93/1000 | Loss: 0.00003108
Iteration 94/1000 | Loss: 0.00003108
Iteration 95/1000 | Loss: 0.00003108
Iteration 96/1000 | Loss: 0.00003108
Iteration 97/1000 | Loss: 0.00003108
Iteration 98/1000 | Loss: 0.00003108
Iteration 99/1000 | Loss: 0.00003107
Iteration 100/1000 | Loss: 0.00003107
Iteration 101/1000 | Loss: 0.00003107
Iteration 102/1000 | Loss: 0.00003107
Iteration 103/1000 | Loss: 0.00003107
Iteration 104/1000 | Loss: 0.00003107
Iteration 105/1000 | Loss: 0.00003107
Iteration 106/1000 | Loss: 0.00003107
Iteration 107/1000 | Loss: 0.00003107
Iteration 108/1000 | Loss: 0.00003107
Iteration 109/1000 | Loss: 0.00003107
Iteration 110/1000 | Loss: 0.00003107
Iteration 111/1000 | Loss: 0.00003107
Iteration 112/1000 | Loss: 0.00003106
Iteration 113/1000 | Loss: 0.00003106
Iteration 114/1000 | Loss: 0.00003106
Iteration 115/1000 | Loss: 0.00003106
Iteration 116/1000 | Loss: 0.00003106
Iteration 117/1000 | Loss: 0.00003106
Iteration 118/1000 | Loss: 0.00003106
Iteration 119/1000 | Loss: 0.00003106
Iteration 120/1000 | Loss: 0.00003106
Iteration 121/1000 | Loss: 0.00003105
Iteration 122/1000 | Loss: 0.00003105
Iteration 123/1000 | Loss: 0.00003105
Iteration 124/1000 | Loss: 0.00003105
Iteration 125/1000 | Loss: 0.00003105
Iteration 126/1000 | Loss: 0.00003105
Iteration 127/1000 | Loss: 0.00003105
Iteration 128/1000 | Loss: 0.00003105
Iteration 129/1000 | Loss: 0.00003105
Iteration 130/1000 | Loss: 0.00003105
Iteration 131/1000 | Loss: 0.00003105
Iteration 132/1000 | Loss: 0.00003105
Iteration 133/1000 | Loss: 0.00003105
Iteration 134/1000 | Loss: 0.00003105
Iteration 135/1000 | Loss: 0.00003104
Iteration 136/1000 | Loss: 0.00003104
Iteration 137/1000 | Loss: 0.00003104
Iteration 138/1000 | Loss: 0.00003104
Iteration 139/1000 | Loss: 0.00003104
Iteration 140/1000 | Loss: 0.00003104
Iteration 141/1000 | Loss: 0.00003104
Iteration 142/1000 | Loss: 0.00003104
Iteration 143/1000 | Loss: 0.00003104
Iteration 144/1000 | Loss: 0.00003104
Iteration 145/1000 | Loss: 0.00003104
Iteration 146/1000 | Loss: 0.00003103
Iteration 147/1000 | Loss: 0.00003103
Iteration 148/1000 | Loss: 0.00003103
Iteration 149/1000 | Loss: 0.00003103
Iteration 150/1000 | Loss: 0.00003103
Iteration 151/1000 | Loss: 0.00003103
Iteration 152/1000 | Loss: 0.00003103
Iteration 153/1000 | Loss: 0.00003103
Iteration 154/1000 | Loss: 0.00003103
Iteration 155/1000 | Loss: 0.00003103
Iteration 156/1000 | Loss: 0.00003102
Iteration 157/1000 | Loss: 0.00003102
Iteration 158/1000 | Loss: 0.00003102
Iteration 159/1000 | Loss: 0.00003102
Iteration 160/1000 | Loss: 0.00003102
Iteration 161/1000 | Loss: 0.00003102
Iteration 162/1000 | Loss: 0.00003102
Iteration 163/1000 | Loss: 0.00003102
Iteration 164/1000 | Loss: 0.00003101
Iteration 165/1000 | Loss: 0.00003101
Iteration 166/1000 | Loss: 0.00003101
Iteration 167/1000 | Loss: 0.00003101
Iteration 168/1000 | Loss: 0.00003101
Iteration 169/1000 | Loss: 0.00003101
Iteration 170/1000 | Loss: 0.00003101
Iteration 171/1000 | Loss: 0.00003101
Iteration 172/1000 | Loss: 0.00003101
Iteration 173/1000 | Loss: 0.00003101
Iteration 174/1000 | Loss: 0.00003101
Iteration 175/1000 | Loss: 0.00003101
Iteration 176/1000 | Loss: 0.00003101
Iteration 177/1000 | Loss: 0.00003100
Iteration 178/1000 | Loss: 0.00003100
Iteration 179/1000 | Loss: 0.00003100
Iteration 180/1000 | Loss: 0.00003100
Iteration 181/1000 | Loss: 0.00003100
Iteration 182/1000 | Loss: 0.00003100
Iteration 183/1000 | Loss: 0.00003100
Iteration 184/1000 | Loss: 0.00003100
Iteration 185/1000 | Loss: 0.00003100
Iteration 186/1000 | Loss: 0.00003100
Iteration 187/1000 | Loss: 0.00003100
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [3.100301182712428e-05, 3.100301182712428e-05, 3.100301182712428e-05, 3.100301182712428e-05, 3.100301182712428e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.100301182712428e-05

Optimization complete. Final v2v error: 4.632287979125977 mm

Highest mean error: 5.442101001739502 mm for frame 15

Lowest mean error: 4.284900665283203 mm for frame 43

Saving results

Total time: 51.208863258361816
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01021192
Iteration 2/25 | Loss: 0.01021192
Iteration 3/25 | Loss: 0.01021192
Iteration 4/25 | Loss: 0.01021191
Iteration 5/25 | Loss: 0.01021191
Iteration 6/25 | Loss: 0.01021191
Iteration 7/25 | Loss: 0.01021191
Iteration 8/25 | Loss: 0.01021191
Iteration 9/25 | Loss: 0.01021191
Iteration 10/25 | Loss: 0.01021191
Iteration 11/25 | Loss: 0.01021191
Iteration 12/25 | Loss: 0.01021191
Iteration 13/25 | Loss: 0.01021191
Iteration 14/25 | Loss: 0.01021190
Iteration 15/25 | Loss: 0.01021190
Iteration 16/25 | Loss: 0.01021190
Iteration 17/25 | Loss: 0.01021190
Iteration 18/25 | Loss: 0.01021190
Iteration 19/25 | Loss: 0.01021190
Iteration 20/25 | Loss: 0.01021190
Iteration 21/25 | Loss: 0.01021190
Iteration 22/25 | Loss: 0.01021190
Iteration 23/25 | Loss: 0.01021190
Iteration 24/25 | Loss: 0.01021189
Iteration 25/25 | Loss: 0.01021189

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34982193
Iteration 2/25 | Loss: 0.17548402
Iteration 3/25 | Loss: 0.17415491
Iteration 4/25 | Loss: 0.17325826
Iteration 5/25 | Loss: 0.17307881
Iteration 6/25 | Loss: 0.17305787
Iteration 7/25 | Loss: 0.17305781
Iteration 8/25 | Loss: 0.17305779
Iteration 9/25 | Loss: 0.17305779
Iteration 10/25 | Loss: 0.17305779
Iteration 11/25 | Loss: 0.17305778
Iteration 12/25 | Loss: 0.17305778
Iteration 13/25 | Loss: 0.17305778
Iteration 14/25 | Loss: 0.17305778
Iteration 15/25 | Loss: 0.17305778
Iteration 16/25 | Loss: 0.17305778
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.17305777966976166, 0.17305777966976166, 0.17305777966976166, 0.17305777966976166, 0.17305777966976166]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.17305777966976166

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17305778
Iteration 2/1000 | Loss: 0.01404569
Iteration 3/1000 | Loss: 0.00254061
Iteration 4/1000 | Loss: 0.00738258
Iteration 5/1000 | Loss: 0.00171802
Iteration 6/1000 | Loss: 0.00157038
Iteration 7/1000 | Loss: 0.00031572
Iteration 8/1000 | Loss: 0.00033721
Iteration 9/1000 | Loss: 0.00023319
Iteration 10/1000 | Loss: 0.00007317
Iteration 11/1000 | Loss: 0.00015669
Iteration 12/1000 | Loss: 0.00004731
Iteration 13/1000 | Loss: 0.00016603
Iteration 14/1000 | Loss: 0.00003412
Iteration 15/1000 | Loss: 0.00004536
Iteration 16/1000 | Loss: 0.00004382
Iteration 17/1000 | Loss: 0.00002420
Iteration 18/1000 | Loss: 0.00002243
Iteration 19/1000 | Loss: 0.00003534
Iteration 20/1000 | Loss: 0.00002019
Iteration 21/1000 | Loss: 0.00002861
Iteration 22/1000 | Loss: 0.00001878
Iteration 23/1000 | Loss: 0.00002856
Iteration 24/1000 | Loss: 0.00019501
Iteration 25/1000 | Loss: 0.00006973
Iteration 26/1000 | Loss: 0.00005100
Iteration 27/1000 | Loss: 0.00001715
Iteration 28/1000 | Loss: 0.00002624
Iteration 29/1000 | Loss: 0.00002581
Iteration 30/1000 | Loss: 0.00011429
Iteration 31/1000 | Loss: 0.00003036
Iteration 32/1000 | Loss: 0.00002810
Iteration 33/1000 | Loss: 0.00002690
Iteration 34/1000 | Loss: 0.00004349
Iteration 35/1000 | Loss: 0.00001913
Iteration 36/1000 | Loss: 0.00001659
Iteration 37/1000 | Loss: 0.00001895
Iteration 38/1000 | Loss: 0.00001570
Iteration 39/1000 | Loss: 0.00001566
Iteration 40/1000 | Loss: 0.00001547
Iteration 41/1000 | Loss: 0.00001540
Iteration 42/1000 | Loss: 0.00001539
Iteration 43/1000 | Loss: 0.00001537
Iteration 44/1000 | Loss: 0.00001522
Iteration 45/1000 | Loss: 0.00001522
Iteration 46/1000 | Loss: 0.00001521
Iteration 47/1000 | Loss: 0.00001520
Iteration 48/1000 | Loss: 0.00001520
Iteration 49/1000 | Loss: 0.00001520
Iteration 50/1000 | Loss: 0.00001519
Iteration 51/1000 | Loss: 0.00001519
Iteration 52/1000 | Loss: 0.00001519
Iteration 53/1000 | Loss: 0.00001518
Iteration 54/1000 | Loss: 0.00001517
Iteration 55/1000 | Loss: 0.00001516
Iteration 56/1000 | Loss: 0.00001516
Iteration 57/1000 | Loss: 0.00001516
Iteration 58/1000 | Loss: 0.00001515
Iteration 59/1000 | Loss: 0.00001515
Iteration 60/1000 | Loss: 0.00001515
Iteration 61/1000 | Loss: 0.00001514
Iteration 62/1000 | Loss: 0.00001514
Iteration 63/1000 | Loss: 0.00001514
Iteration 64/1000 | Loss: 0.00001514
Iteration 65/1000 | Loss: 0.00001514
Iteration 66/1000 | Loss: 0.00001514
Iteration 67/1000 | Loss: 0.00001514
Iteration 68/1000 | Loss: 0.00001513
Iteration 69/1000 | Loss: 0.00001512
Iteration 70/1000 | Loss: 0.00001512
Iteration 71/1000 | Loss: 0.00001512
Iteration 72/1000 | Loss: 0.00001512
Iteration 73/1000 | Loss: 0.00001512
Iteration 74/1000 | Loss: 0.00001512
Iteration 75/1000 | Loss: 0.00001512
Iteration 76/1000 | Loss: 0.00001511
Iteration 77/1000 | Loss: 0.00001510
Iteration 78/1000 | Loss: 0.00001510
Iteration 79/1000 | Loss: 0.00001510
Iteration 80/1000 | Loss: 0.00001510
Iteration 81/1000 | Loss: 0.00001509
Iteration 82/1000 | Loss: 0.00001509
Iteration 83/1000 | Loss: 0.00001509
Iteration 84/1000 | Loss: 0.00001509
Iteration 85/1000 | Loss: 0.00001509
Iteration 86/1000 | Loss: 0.00001508
Iteration 87/1000 | Loss: 0.00001508
Iteration 88/1000 | Loss: 0.00001507
Iteration 89/1000 | Loss: 0.00001507
Iteration 90/1000 | Loss: 0.00001506
Iteration 91/1000 | Loss: 0.00001506
Iteration 92/1000 | Loss: 0.00001506
Iteration 93/1000 | Loss: 0.00001506
Iteration 94/1000 | Loss: 0.00001505
Iteration 95/1000 | Loss: 0.00001505
Iteration 96/1000 | Loss: 0.00001505
Iteration 97/1000 | Loss: 0.00001505
Iteration 98/1000 | Loss: 0.00001505
Iteration 99/1000 | Loss: 0.00001505
Iteration 100/1000 | Loss: 0.00001505
Iteration 101/1000 | Loss: 0.00001505
Iteration 102/1000 | Loss: 0.00001505
Iteration 103/1000 | Loss: 0.00001505
Iteration 104/1000 | Loss: 0.00001505
Iteration 105/1000 | Loss: 0.00001505
Iteration 106/1000 | Loss: 0.00001505
Iteration 107/1000 | Loss: 0.00001505
Iteration 108/1000 | Loss: 0.00001505
Iteration 109/1000 | Loss: 0.00001505
Iteration 110/1000 | Loss: 0.00001505
Iteration 111/1000 | Loss: 0.00001505
Iteration 112/1000 | Loss: 0.00001505
Iteration 113/1000 | Loss: 0.00001505
Iteration 114/1000 | Loss: 0.00001505
Iteration 115/1000 | Loss: 0.00001505
Iteration 116/1000 | Loss: 0.00001505
Iteration 117/1000 | Loss: 0.00001505
Iteration 118/1000 | Loss: 0.00001505
Iteration 119/1000 | Loss: 0.00001505
Iteration 120/1000 | Loss: 0.00001505
Iteration 121/1000 | Loss: 0.00001505
Iteration 122/1000 | Loss: 0.00001505
Iteration 123/1000 | Loss: 0.00001505
Iteration 124/1000 | Loss: 0.00001505
Iteration 125/1000 | Loss: 0.00001505
Iteration 126/1000 | Loss: 0.00001505
Iteration 127/1000 | Loss: 0.00001505
Iteration 128/1000 | Loss: 0.00001505
Iteration 129/1000 | Loss: 0.00001505
Iteration 130/1000 | Loss: 0.00001505
Iteration 131/1000 | Loss: 0.00001505
Iteration 132/1000 | Loss: 0.00001505
Iteration 133/1000 | Loss: 0.00001505
Iteration 134/1000 | Loss: 0.00001505
Iteration 135/1000 | Loss: 0.00001505
Iteration 136/1000 | Loss: 0.00001505
Iteration 137/1000 | Loss: 0.00001505
Iteration 138/1000 | Loss: 0.00001505
Iteration 139/1000 | Loss: 0.00001505
Iteration 140/1000 | Loss: 0.00001505
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.504622105130693e-05, 1.504622105130693e-05, 1.504622105130693e-05, 1.504622105130693e-05, 1.504622105130693e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.504622105130693e-05

Optimization complete. Final v2v error: 3.2974791526794434 mm

Highest mean error: 3.8050308227539062 mm for frame 73

Lowest mean error: 2.8199098110198975 mm for frame 223

Saving results

Total time: 82.46506333351135
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00417776
Iteration 2/25 | Loss: 0.00125335
Iteration 3/25 | Loss: 0.00117407
Iteration 4/25 | Loss: 0.00115836
Iteration 5/25 | Loss: 0.00115454
Iteration 6/25 | Loss: 0.00115406
Iteration 7/25 | Loss: 0.00115406
Iteration 8/25 | Loss: 0.00115406
Iteration 9/25 | Loss: 0.00115406
Iteration 10/25 | Loss: 0.00115406
Iteration 11/25 | Loss: 0.00115406
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011540645500645041, 0.0011540645500645041, 0.0011540645500645041, 0.0011540645500645041, 0.0011540645500645041]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011540645500645041

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.00004816
Iteration 2/25 | Loss: 0.00183398
Iteration 3/25 | Loss: 0.00183397
Iteration 4/25 | Loss: 0.00183397
Iteration 5/25 | Loss: 0.00183397
Iteration 6/25 | Loss: 0.00183397
Iteration 7/25 | Loss: 0.00183397
Iteration 8/25 | Loss: 0.00183397
Iteration 9/25 | Loss: 0.00183397
Iteration 10/25 | Loss: 0.00183397
Iteration 11/25 | Loss: 0.00183396
Iteration 12/25 | Loss: 0.00183396
Iteration 13/25 | Loss: 0.00183396
Iteration 14/25 | Loss: 0.00183396
Iteration 15/25 | Loss: 0.00183396
Iteration 16/25 | Loss: 0.00183396
Iteration 17/25 | Loss: 0.00183396
Iteration 18/25 | Loss: 0.00183396
Iteration 19/25 | Loss: 0.00183396
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0018339648377150297, 0.0018339648377150297, 0.0018339648377150297, 0.0018339648377150297, 0.0018339648377150297]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018339648377150297

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00183396
Iteration 2/1000 | Loss: 0.00001956
Iteration 3/1000 | Loss: 0.00001604
Iteration 4/1000 | Loss: 0.00001480
Iteration 5/1000 | Loss: 0.00001400
Iteration 6/1000 | Loss: 0.00001337
Iteration 7/1000 | Loss: 0.00001297
Iteration 8/1000 | Loss: 0.00001260
Iteration 9/1000 | Loss: 0.00001231
Iteration 10/1000 | Loss: 0.00001205
Iteration 11/1000 | Loss: 0.00001187
Iteration 12/1000 | Loss: 0.00001173
Iteration 13/1000 | Loss: 0.00001170
Iteration 14/1000 | Loss: 0.00001163
Iteration 15/1000 | Loss: 0.00001158
Iteration 16/1000 | Loss: 0.00001158
Iteration 17/1000 | Loss: 0.00001153
Iteration 18/1000 | Loss: 0.00001151
Iteration 19/1000 | Loss: 0.00001145
Iteration 20/1000 | Loss: 0.00001144
Iteration 21/1000 | Loss: 0.00001143
Iteration 22/1000 | Loss: 0.00001142
Iteration 23/1000 | Loss: 0.00001140
Iteration 24/1000 | Loss: 0.00001140
Iteration 25/1000 | Loss: 0.00001140
Iteration 26/1000 | Loss: 0.00001140
Iteration 27/1000 | Loss: 0.00001140
Iteration 28/1000 | Loss: 0.00001140
Iteration 29/1000 | Loss: 0.00001139
Iteration 30/1000 | Loss: 0.00001139
Iteration 31/1000 | Loss: 0.00001138
Iteration 32/1000 | Loss: 0.00001138
Iteration 33/1000 | Loss: 0.00001138
Iteration 34/1000 | Loss: 0.00001135
Iteration 35/1000 | Loss: 0.00001135
Iteration 36/1000 | Loss: 0.00001135
Iteration 37/1000 | Loss: 0.00001134
Iteration 38/1000 | Loss: 0.00001134
Iteration 39/1000 | Loss: 0.00001134
Iteration 40/1000 | Loss: 0.00001133
Iteration 41/1000 | Loss: 0.00001131
Iteration 42/1000 | Loss: 0.00001131
Iteration 43/1000 | Loss: 0.00001131
Iteration 44/1000 | Loss: 0.00001130
Iteration 45/1000 | Loss: 0.00001130
Iteration 46/1000 | Loss: 0.00001129
Iteration 47/1000 | Loss: 0.00001129
Iteration 48/1000 | Loss: 0.00001129
Iteration 49/1000 | Loss: 0.00001129
Iteration 50/1000 | Loss: 0.00001128
Iteration 51/1000 | Loss: 0.00001128
Iteration 52/1000 | Loss: 0.00001127
Iteration 53/1000 | Loss: 0.00001127
Iteration 54/1000 | Loss: 0.00001126
Iteration 55/1000 | Loss: 0.00001126
Iteration 56/1000 | Loss: 0.00001125
Iteration 57/1000 | Loss: 0.00001125
Iteration 58/1000 | Loss: 0.00001125
Iteration 59/1000 | Loss: 0.00001124
Iteration 60/1000 | Loss: 0.00001123
Iteration 61/1000 | Loss: 0.00001123
Iteration 62/1000 | Loss: 0.00001123
Iteration 63/1000 | Loss: 0.00001123
Iteration 64/1000 | Loss: 0.00001122
Iteration 65/1000 | Loss: 0.00001122
Iteration 66/1000 | Loss: 0.00001122
Iteration 67/1000 | Loss: 0.00001122
Iteration 68/1000 | Loss: 0.00001121
Iteration 69/1000 | Loss: 0.00001121
Iteration 70/1000 | Loss: 0.00001120
Iteration 71/1000 | Loss: 0.00001120
Iteration 72/1000 | Loss: 0.00001120
Iteration 73/1000 | Loss: 0.00001119
Iteration 74/1000 | Loss: 0.00001118
Iteration 75/1000 | Loss: 0.00001118
Iteration 76/1000 | Loss: 0.00001118
Iteration 77/1000 | Loss: 0.00001118
Iteration 78/1000 | Loss: 0.00001117
Iteration 79/1000 | Loss: 0.00001117
Iteration 80/1000 | Loss: 0.00001117
Iteration 81/1000 | Loss: 0.00001117
Iteration 82/1000 | Loss: 0.00001116
Iteration 83/1000 | Loss: 0.00001116
Iteration 84/1000 | Loss: 0.00001115
Iteration 85/1000 | Loss: 0.00001115
Iteration 86/1000 | Loss: 0.00001115
Iteration 87/1000 | Loss: 0.00001115
Iteration 88/1000 | Loss: 0.00001114
Iteration 89/1000 | Loss: 0.00001114
Iteration 90/1000 | Loss: 0.00001114
Iteration 91/1000 | Loss: 0.00001113
Iteration 92/1000 | Loss: 0.00001113
Iteration 93/1000 | Loss: 0.00001113
Iteration 94/1000 | Loss: 0.00001112
Iteration 95/1000 | Loss: 0.00001112
Iteration 96/1000 | Loss: 0.00001112
Iteration 97/1000 | Loss: 0.00001111
Iteration 98/1000 | Loss: 0.00001111
Iteration 99/1000 | Loss: 0.00001111
Iteration 100/1000 | Loss: 0.00001111
Iteration 101/1000 | Loss: 0.00001111
Iteration 102/1000 | Loss: 0.00001111
Iteration 103/1000 | Loss: 0.00001111
Iteration 104/1000 | Loss: 0.00001110
Iteration 105/1000 | Loss: 0.00001110
Iteration 106/1000 | Loss: 0.00001110
Iteration 107/1000 | Loss: 0.00001110
Iteration 108/1000 | Loss: 0.00001109
Iteration 109/1000 | Loss: 0.00001109
Iteration 110/1000 | Loss: 0.00001109
Iteration 111/1000 | Loss: 0.00001109
Iteration 112/1000 | Loss: 0.00001109
Iteration 113/1000 | Loss: 0.00001109
Iteration 114/1000 | Loss: 0.00001109
Iteration 115/1000 | Loss: 0.00001109
Iteration 116/1000 | Loss: 0.00001109
Iteration 117/1000 | Loss: 0.00001109
Iteration 118/1000 | Loss: 0.00001109
Iteration 119/1000 | Loss: 0.00001109
Iteration 120/1000 | Loss: 0.00001109
Iteration 121/1000 | Loss: 0.00001109
Iteration 122/1000 | Loss: 0.00001109
Iteration 123/1000 | Loss: 0.00001109
Iteration 124/1000 | Loss: 0.00001109
Iteration 125/1000 | Loss: 0.00001109
Iteration 126/1000 | Loss: 0.00001109
Iteration 127/1000 | Loss: 0.00001108
Iteration 128/1000 | Loss: 0.00001108
Iteration 129/1000 | Loss: 0.00001108
Iteration 130/1000 | Loss: 0.00001108
Iteration 131/1000 | Loss: 0.00001108
Iteration 132/1000 | Loss: 0.00001107
Iteration 133/1000 | Loss: 0.00001107
Iteration 134/1000 | Loss: 0.00001107
Iteration 135/1000 | Loss: 0.00001107
Iteration 136/1000 | Loss: 0.00001107
Iteration 137/1000 | Loss: 0.00001107
Iteration 138/1000 | Loss: 0.00001107
Iteration 139/1000 | Loss: 0.00001107
Iteration 140/1000 | Loss: 0.00001107
Iteration 141/1000 | Loss: 0.00001107
Iteration 142/1000 | Loss: 0.00001107
Iteration 143/1000 | Loss: 0.00001107
Iteration 144/1000 | Loss: 0.00001107
Iteration 145/1000 | Loss: 0.00001107
Iteration 146/1000 | Loss: 0.00001107
Iteration 147/1000 | Loss: 0.00001107
Iteration 148/1000 | Loss: 0.00001107
Iteration 149/1000 | Loss: 0.00001106
Iteration 150/1000 | Loss: 0.00001106
Iteration 151/1000 | Loss: 0.00001106
Iteration 152/1000 | Loss: 0.00001106
Iteration 153/1000 | Loss: 0.00001106
Iteration 154/1000 | Loss: 0.00001106
Iteration 155/1000 | Loss: 0.00001106
Iteration 156/1000 | Loss: 0.00001106
Iteration 157/1000 | Loss: 0.00001105
Iteration 158/1000 | Loss: 0.00001105
Iteration 159/1000 | Loss: 0.00001105
Iteration 160/1000 | Loss: 0.00001105
Iteration 161/1000 | Loss: 0.00001105
Iteration 162/1000 | Loss: 0.00001105
Iteration 163/1000 | Loss: 0.00001105
Iteration 164/1000 | Loss: 0.00001105
Iteration 165/1000 | Loss: 0.00001105
Iteration 166/1000 | Loss: 0.00001105
Iteration 167/1000 | Loss: 0.00001105
Iteration 168/1000 | Loss: 0.00001105
Iteration 169/1000 | Loss: 0.00001105
Iteration 170/1000 | Loss: 0.00001105
Iteration 171/1000 | Loss: 0.00001105
Iteration 172/1000 | Loss: 0.00001105
Iteration 173/1000 | Loss: 0.00001105
Iteration 174/1000 | Loss: 0.00001105
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [1.1053502930735704e-05, 1.1053502930735704e-05, 1.1053502930735704e-05, 1.1053502930735704e-05, 1.1053502930735704e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1053502930735704e-05

Optimization complete. Final v2v error: 2.8717668056488037 mm

Highest mean error: 3.676492214202881 mm for frame 235

Lowest mean error: 2.725713014602661 mm for frame 137

Saving results

Total time: 46.14904761314392
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01023543
Iteration 2/25 | Loss: 0.00177865
Iteration 3/25 | Loss: 0.00132457
Iteration 4/25 | Loss: 0.00126909
Iteration 5/25 | Loss: 0.00125848
Iteration 6/25 | Loss: 0.00125553
Iteration 7/25 | Loss: 0.00125467
Iteration 8/25 | Loss: 0.00125455
Iteration 9/25 | Loss: 0.00125455
Iteration 10/25 | Loss: 0.00125455
Iteration 11/25 | Loss: 0.00125455
Iteration 12/25 | Loss: 0.00125455
Iteration 13/25 | Loss: 0.00125455
Iteration 14/25 | Loss: 0.00125455
Iteration 15/25 | Loss: 0.00125455
Iteration 16/25 | Loss: 0.00125455
Iteration 17/25 | Loss: 0.00125455
Iteration 18/25 | Loss: 0.00125455
Iteration 19/25 | Loss: 0.00125455
Iteration 20/25 | Loss: 0.00125455
Iteration 21/25 | Loss: 0.00125455
Iteration 22/25 | Loss: 0.00125455
Iteration 23/25 | Loss: 0.00125455
Iteration 24/25 | Loss: 0.00125455
Iteration 25/25 | Loss: 0.00125455

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.87435758
Iteration 2/25 | Loss: 0.00171930
Iteration 3/25 | Loss: 0.00171930
Iteration 4/25 | Loss: 0.00171930
Iteration 5/25 | Loss: 0.00171930
Iteration 6/25 | Loss: 0.00171930
Iteration 7/25 | Loss: 0.00171930
Iteration 8/25 | Loss: 0.00171930
Iteration 9/25 | Loss: 0.00171930
Iteration 10/25 | Loss: 0.00171930
Iteration 11/25 | Loss: 0.00171930
Iteration 12/25 | Loss: 0.00171930
Iteration 13/25 | Loss: 0.00171930
Iteration 14/25 | Loss: 0.00171930
Iteration 15/25 | Loss: 0.00171930
Iteration 16/25 | Loss: 0.00171930
Iteration 17/25 | Loss: 0.00171930
Iteration 18/25 | Loss: 0.00171930
Iteration 19/25 | Loss: 0.00171930
Iteration 20/25 | Loss: 0.00171930
Iteration 21/25 | Loss: 0.00171930
Iteration 22/25 | Loss: 0.00171930
Iteration 23/25 | Loss: 0.00171930
Iteration 24/25 | Loss: 0.00171930
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0017192983068525791, 0.0017192983068525791, 0.0017192983068525791, 0.0017192983068525791, 0.0017192983068525791]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017192983068525791

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00171930
Iteration 2/1000 | Loss: 0.00006506
Iteration 3/1000 | Loss: 0.00004375
Iteration 4/1000 | Loss: 0.00003220
Iteration 5/1000 | Loss: 0.00002928
Iteration 6/1000 | Loss: 0.00002810
Iteration 7/1000 | Loss: 0.00002722
Iteration 8/1000 | Loss: 0.00002645
Iteration 9/1000 | Loss: 0.00002597
Iteration 10/1000 | Loss: 0.00002562
Iteration 11/1000 | Loss: 0.00002523
Iteration 12/1000 | Loss: 0.00002492
Iteration 13/1000 | Loss: 0.00002469
Iteration 14/1000 | Loss: 0.00002447
Iteration 15/1000 | Loss: 0.00002431
Iteration 16/1000 | Loss: 0.00002411
Iteration 17/1000 | Loss: 0.00002395
Iteration 18/1000 | Loss: 0.00002392
Iteration 19/1000 | Loss: 0.00002388
Iteration 20/1000 | Loss: 0.00002383
Iteration 21/1000 | Loss: 0.00002376
Iteration 22/1000 | Loss: 0.00002371
Iteration 23/1000 | Loss: 0.00002371
Iteration 24/1000 | Loss: 0.00002371
Iteration 25/1000 | Loss: 0.00002371
Iteration 26/1000 | Loss: 0.00002371
Iteration 27/1000 | Loss: 0.00002371
Iteration 28/1000 | Loss: 0.00002371
Iteration 29/1000 | Loss: 0.00002371
Iteration 30/1000 | Loss: 0.00002371
Iteration 31/1000 | Loss: 0.00002368
Iteration 32/1000 | Loss: 0.00002368
Iteration 33/1000 | Loss: 0.00002368
Iteration 34/1000 | Loss: 0.00002367
Iteration 35/1000 | Loss: 0.00002366
Iteration 36/1000 | Loss: 0.00002366
Iteration 37/1000 | Loss: 0.00002366
Iteration 38/1000 | Loss: 0.00002366
Iteration 39/1000 | Loss: 0.00002366
Iteration 40/1000 | Loss: 0.00002366
Iteration 41/1000 | Loss: 0.00002366
Iteration 42/1000 | Loss: 0.00002366
Iteration 43/1000 | Loss: 0.00002366
Iteration 44/1000 | Loss: 0.00002366
Iteration 45/1000 | Loss: 0.00002366
Iteration 46/1000 | Loss: 0.00002365
Iteration 47/1000 | Loss: 0.00002365
Iteration 48/1000 | Loss: 0.00002364
Iteration 49/1000 | Loss: 0.00002364
Iteration 50/1000 | Loss: 0.00002363
Iteration 51/1000 | Loss: 0.00002363
Iteration 52/1000 | Loss: 0.00002362
Iteration 53/1000 | Loss: 0.00002362
Iteration 54/1000 | Loss: 0.00002361
Iteration 55/1000 | Loss: 0.00002361
Iteration 56/1000 | Loss: 0.00002360
Iteration 57/1000 | Loss: 0.00002360
Iteration 58/1000 | Loss: 0.00002356
Iteration 59/1000 | Loss: 0.00002355
Iteration 60/1000 | Loss: 0.00002355
Iteration 61/1000 | Loss: 0.00002355
Iteration 62/1000 | Loss: 0.00002355
Iteration 63/1000 | Loss: 0.00002354
Iteration 64/1000 | Loss: 0.00002354
Iteration 65/1000 | Loss: 0.00002353
Iteration 66/1000 | Loss: 0.00002353
Iteration 67/1000 | Loss: 0.00002353
Iteration 68/1000 | Loss: 0.00002353
Iteration 69/1000 | Loss: 0.00002352
Iteration 70/1000 | Loss: 0.00002352
Iteration 71/1000 | Loss: 0.00002351
Iteration 72/1000 | Loss: 0.00002351
Iteration 73/1000 | Loss: 0.00002351
Iteration 74/1000 | Loss: 0.00002350
Iteration 75/1000 | Loss: 0.00002350
Iteration 76/1000 | Loss: 0.00002350
Iteration 77/1000 | Loss: 0.00002349
Iteration 78/1000 | Loss: 0.00002349
Iteration 79/1000 | Loss: 0.00002349
Iteration 80/1000 | Loss: 0.00002349
Iteration 81/1000 | Loss: 0.00002349
Iteration 82/1000 | Loss: 0.00002349
Iteration 83/1000 | Loss: 0.00002349
Iteration 84/1000 | Loss: 0.00002349
Iteration 85/1000 | Loss: 0.00002349
Iteration 86/1000 | Loss: 0.00002349
Iteration 87/1000 | Loss: 0.00002348
Iteration 88/1000 | Loss: 0.00002348
Iteration 89/1000 | Loss: 0.00002348
Iteration 90/1000 | Loss: 0.00002347
Iteration 91/1000 | Loss: 0.00002347
Iteration 92/1000 | Loss: 0.00002347
Iteration 93/1000 | Loss: 0.00002347
Iteration 94/1000 | Loss: 0.00002347
Iteration 95/1000 | Loss: 0.00002347
Iteration 96/1000 | Loss: 0.00002347
Iteration 97/1000 | Loss: 0.00002347
Iteration 98/1000 | Loss: 0.00002347
Iteration 99/1000 | Loss: 0.00002347
Iteration 100/1000 | Loss: 0.00002346
Iteration 101/1000 | Loss: 0.00002346
Iteration 102/1000 | Loss: 0.00002346
Iteration 103/1000 | Loss: 0.00002346
Iteration 104/1000 | Loss: 0.00002346
Iteration 105/1000 | Loss: 0.00002346
Iteration 106/1000 | Loss: 0.00002345
Iteration 107/1000 | Loss: 0.00002345
Iteration 108/1000 | Loss: 0.00002345
Iteration 109/1000 | Loss: 0.00002345
Iteration 110/1000 | Loss: 0.00002344
Iteration 111/1000 | Loss: 0.00002344
Iteration 112/1000 | Loss: 0.00002344
Iteration 113/1000 | Loss: 0.00002344
Iteration 114/1000 | Loss: 0.00002344
Iteration 115/1000 | Loss: 0.00002344
Iteration 116/1000 | Loss: 0.00002344
Iteration 117/1000 | Loss: 0.00002344
Iteration 118/1000 | Loss: 0.00002344
Iteration 119/1000 | Loss: 0.00002343
Iteration 120/1000 | Loss: 0.00002343
Iteration 121/1000 | Loss: 0.00002343
Iteration 122/1000 | Loss: 0.00002342
Iteration 123/1000 | Loss: 0.00002342
Iteration 124/1000 | Loss: 0.00002342
Iteration 125/1000 | Loss: 0.00002342
Iteration 126/1000 | Loss: 0.00002342
Iteration 127/1000 | Loss: 0.00002342
Iteration 128/1000 | Loss: 0.00002342
Iteration 129/1000 | Loss: 0.00002342
Iteration 130/1000 | Loss: 0.00002342
Iteration 131/1000 | Loss: 0.00002342
Iteration 132/1000 | Loss: 0.00002342
Iteration 133/1000 | Loss: 0.00002341
Iteration 134/1000 | Loss: 0.00002341
Iteration 135/1000 | Loss: 0.00002341
Iteration 136/1000 | Loss: 0.00002341
Iteration 137/1000 | Loss: 0.00002341
Iteration 138/1000 | Loss: 0.00002341
Iteration 139/1000 | Loss: 0.00002340
Iteration 140/1000 | Loss: 0.00002340
Iteration 141/1000 | Loss: 0.00002340
Iteration 142/1000 | Loss: 0.00002340
Iteration 143/1000 | Loss: 0.00002340
Iteration 144/1000 | Loss: 0.00002340
Iteration 145/1000 | Loss: 0.00002340
Iteration 146/1000 | Loss: 0.00002339
Iteration 147/1000 | Loss: 0.00002339
Iteration 148/1000 | Loss: 0.00002339
Iteration 149/1000 | Loss: 0.00002338
Iteration 150/1000 | Loss: 0.00002338
Iteration 151/1000 | Loss: 0.00002338
Iteration 152/1000 | Loss: 0.00002338
Iteration 153/1000 | Loss: 0.00002338
Iteration 154/1000 | Loss: 0.00002338
Iteration 155/1000 | Loss: 0.00002338
Iteration 156/1000 | Loss: 0.00002338
Iteration 157/1000 | Loss: 0.00002338
Iteration 158/1000 | Loss: 0.00002338
Iteration 159/1000 | Loss: 0.00002338
Iteration 160/1000 | Loss: 0.00002338
Iteration 161/1000 | Loss: 0.00002338
Iteration 162/1000 | Loss: 0.00002338
Iteration 163/1000 | Loss: 0.00002337
Iteration 164/1000 | Loss: 0.00002337
Iteration 165/1000 | Loss: 0.00002337
Iteration 166/1000 | Loss: 0.00002337
Iteration 167/1000 | Loss: 0.00002337
Iteration 168/1000 | Loss: 0.00002337
Iteration 169/1000 | Loss: 0.00002337
Iteration 170/1000 | Loss: 0.00002337
Iteration 171/1000 | Loss: 0.00002337
Iteration 172/1000 | Loss: 0.00002336
Iteration 173/1000 | Loss: 0.00002336
Iteration 174/1000 | Loss: 0.00002336
Iteration 175/1000 | Loss: 0.00002336
Iteration 176/1000 | Loss: 0.00002336
Iteration 177/1000 | Loss: 0.00002336
Iteration 178/1000 | Loss: 0.00002336
Iteration 179/1000 | Loss: 0.00002336
Iteration 180/1000 | Loss: 0.00002336
Iteration 181/1000 | Loss: 0.00002336
Iteration 182/1000 | Loss: 0.00002336
Iteration 183/1000 | Loss: 0.00002335
Iteration 184/1000 | Loss: 0.00002335
Iteration 185/1000 | Loss: 0.00002335
Iteration 186/1000 | Loss: 0.00002335
Iteration 187/1000 | Loss: 0.00002335
Iteration 188/1000 | Loss: 0.00002335
Iteration 189/1000 | Loss: 0.00002335
Iteration 190/1000 | Loss: 0.00002335
Iteration 191/1000 | Loss: 0.00002335
Iteration 192/1000 | Loss: 0.00002335
Iteration 193/1000 | Loss: 0.00002335
Iteration 194/1000 | Loss: 0.00002335
Iteration 195/1000 | Loss: 0.00002335
Iteration 196/1000 | Loss: 0.00002335
Iteration 197/1000 | Loss: 0.00002335
Iteration 198/1000 | Loss: 0.00002335
Iteration 199/1000 | Loss: 0.00002335
Iteration 200/1000 | Loss: 0.00002334
Iteration 201/1000 | Loss: 0.00002334
Iteration 202/1000 | Loss: 0.00002334
Iteration 203/1000 | Loss: 0.00002334
Iteration 204/1000 | Loss: 0.00002334
Iteration 205/1000 | Loss: 0.00002334
Iteration 206/1000 | Loss: 0.00002334
Iteration 207/1000 | Loss: 0.00002334
Iteration 208/1000 | Loss: 0.00002334
Iteration 209/1000 | Loss: 0.00002334
Iteration 210/1000 | Loss: 0.00002334
Iteration 211/1000 | Loss: 0.00002334
Iteration 212/1000 | Loss: 0.00002334
Iteration 213/1000 | Loss: 0.00002334
Iteration 214/1000 | Loss: 0.00002334
Iteration 215/1000 | Loss: 0.00002334
Iteration 216/1000 | Loss: 0.00002334
Iteration 217/1000 | Loss: 0.00002334
Iteration 218/1000 | Loss: 0.00002334
Iteration 219/1000 | Loss: 0.00002334
Iteration 220/1000 | Loss: 0.00002334
Iteration 221/1000 | Loss: 0.00002334
Iteration 222/1000 | Loss: 0.00002334
Iteration 223/1000 | Loss: 0.00002334
Iteration 224/1000 | Loss: 0.00002334
Iteration 225/1000 | Loss: 0.00002334
Iteration 226/1000 | Loss: 0.00002334
Iteration 227/1000 | Loss: 0.00002334
Iteration 228/1000 | Loss: 0.00002334
Iteration 229/1000 | Loss: 0.00002334
Iteration 230/1000 | Loss: 0.00002334
Iteration 231/1000 | Loss: 0.00002334
Iteration 232/1000 | Loss: 0.00002334
Iteration 233/1000 | Loss: 0.00002334
Iteration 234/1000 | Loss: 0.00002334
Iteration 235/1000 | Loss: 0.00002334
Iteration 236/1000 | Loss: 0.00002334
Iteration 237/1000 | Loss: 0.00002334
Iteration 238/1000 | Loss: 0.00002334
Iteration 239/1000 | Loss: 0.00002334
Iteration 240/1000 | Loss: 0.00002334
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 240. Stopping optimization.
Last 5 losses: [2.3338134269579314e-05, 2.3338134269579314e-05, 2.3338134269579314e-05, 2.3338134269579314e-05, 2.3338134269579314e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3338134269579314e-05

Optimization complete. Final v2v error: 3.973421096801758 mm

Highest mean error: 4.716443061828613 mm for frame 79

Lowest mean error: 3.5314836502075195 mm for frame 137

Saving results

Total time: 53.43580198287964
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00722311
Iteration 2/25 | Loss: 0.00170731
Iteration 3/25 | Loss: 0.00134057
Iteration 4/25 | Loss: 0.00127875
Iteration 5/25 | Loss: 0.00126354
Iteration 6/25 | Loss: 0.00125910
Iteration 7/25 | Loss: 0.00126006
Iteration 8/25 | Loss: 0.00125681
Iteration 9/25 | Loss: 0.00125510
Iteration 10/25 | Loss: 0.00125428
Iteration 11/25 | Loss: 0.00125399
Iteration 12/25 | Loss: 0.00125397
Iteration 13/25 | Loss: 0.00125393
Iteration 14/25 | Loss: 0.00125393
Iteration 15/25 | Loss: 0.00125393
Iteration 16/25 | Loss: 0.00125392
Iteration 17/25 | Loss: 0.00125392
Iteration 18/25 | Loss: 0.00125392
Iteration 19/25 | Loss: 0.00125392
Iteration 20/25 | Loss: 0.00125391
Iteration 21/25 | Loss: 0.00125391
Iteration 22/25 | Loss: 0.00125391
Iteration 23/25 | Loss: 0.00125391
Iteration 24/25 | Loss: 0.00125390
Iteration 25/25 | Loss: 0.00125388

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.20197248
Iteration 2/25 | Loss: 0.00240909
Iteration 3/25 | Loss: 0.00240909
Iteration 4/25 | Loss: 0.00240908
Iteration 5/25 | Loss: 0.00240908
Iteration 6/25 | Loss: 0.00240908
Iteration 7/25 | Loss: 0.00240908
Iteration 8/25 | Loss: 0.00240908
Iteration 9/25 | Loss: 0.00240908
Iteration 10/25 | Loss: 0.00240908
Iteration 11/25 | Loss: 0.00240908
Iteration 12/25 | Loss: 0.00240908
Iteration 13/25 | Loss: 0.00240908
Iteration 14/25 | Loss: 0.00240908
Iteration 15/25 | Loss: 0.00240908
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.002409081906080246, 0.002409081906080246, 0.002409081906080246, 0.002409081906080246, 0.002409081906080246]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002409081906080246

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00240908
Iteration 2/1000 | Loss: 0.00281238
Iteration 3/1000 | Loss: 0.00013947
Iteration 4/1000 | Loss: 0.00006007
Iteration 5/1000 | Loss: 0.00004435
Iteration 6/1000 | Loss: 0.00003868
Iteration 7/1000 | Loss: 0.00003530
Iteration 8/1000 | Loss: 0.00061420
Iteration 9/1000 | Loss: 0.00104227
Iteration 10/1000 | Loss: 0.00045250
Iteration 11/1000 | Loss: 0.00003542
Iteration 12/1000 | Loss: 0.00003110
Iteration 13/1000 | Loss: 0.00002898
Iteration 14/1000 | Loss: 0.00002735
Iteration 15/1000 | Loss: 0.00002573
Iteration 16/1000 | Loss: 0.00002438
Iteration 17/1000 | Loss: 0.00002357
Iteration 18/1000 | Loss: 0.00002299
Iteration 19/1000 | Loss: 0.00002263
Iteration 20/1000 | Loss: 0.00002232
Iteration 21/1000 | Loss: 0.00002203
Iteration 22/1000 | Loss: 0.00002191
Iteration 23/1000 | Loss: 0.00002175
Iteration 24/1000 | Loss: 0.00002153
Iteration 25/1000 | Loss: 0.00002142
Iteration 26/1000 | Loss: 0.00002139
Iteration 27/1000 | Loss: 0.00002139
Iteration 28/1000 | Loss: 0.00002138
Iteration 29/1000 | Loss: 0.00002131
Iteration 30/1000 | Loss: 0.00002130
Iteration 31/1000 | Loss: 0.00002128
Iteration 32/1000 | Loss: 0.00002127
Iteration 33/1000 | Loss: 0.00002127
Iteration 34/1000 | Loss: 0.00002127
Iteration 35/1000 | Loss: 0.00002126
Iteration 36/1000 | Loss: 0.00002124
Iteration 37/1000 | Loss: 0.00002124
Iteration 38/1000 | Loss: 0.00002120
Iteration 39/1000 | Loss: 0.00002115
Iteration 40/1000 | Loss: 0.00002113
Iteration 41/1000 | Loss: 0.00002112
Iteration 42/1000 | Loss: 0.00002112
Iteration 43/1000 | Loss: 0.00002112
Iteration 44/1000 | Loss: 0.00002112
Iteration 45/1000 | Loss: 0.00002111
Iteration 46/1000 | Loss: 0.00002111
Iteration 47/1000 | Loss: 0.00002111
Iteration 48/1000 | Loss: 0.00002111
Iteration 49/1000 | Loss: 0.00002111
Iteration 50/1000 | Loss: 0.00002110
Iteration 51/1000 | Loss: 0.00002109
Iteration 52/1000 | Loss: 0.00002109
Iteration 53/1000 | Loss: 0.00002109
Iteration 54/1000 | Loss: 0.00002109
Iteration 55/1000 | Loss: 0.00002108
Iteration 56/1000 | Loss: 0.00002108
Iteration 57/1000 | Loss: 0.00002108
Iteration 58/1000 | Loss: 0.00002108
Iteration 59/1000 | Loss: 0.00002108
Iteration 60/1000 | Loss: 0.00002107
Iteration 61/1000 | Loss: 0.00002107
Iteration 62/1000 | Loss: 0.00002107
Iteration 63/1000 | Loss: 0.00002107
Iteration 64/1000 | Loss: 0.00002106
Iteration 65/1000 | Loss: 0.00002106
Iteration 66/1000 | Loss: 0.00002106
Iteration 67/1000 | Loss: 0.00002105
Iteration 68/1000 | Loss: 0.00002104
Iteration 69/1000 | Loss: 0.00002104
Iteration 70/1000 | Loss: 0.00002103
Iteration 71/1000 | Loss: 0.00002100
Iteration 72/1000 | Loss: 0.00002099
Iteration 73/1000 | Loss: 0.00002098
Iteration 74/1000 | Loss: 0.00002097
Iteration 75/1000 | Loss: 0.00002097
Iteration 76/1000 | Loss: 0.00002097
Iteration 77/1000 | Loss: 0.00002096
Iteration 78/1000 | Loss: 0.00002096
Iteration 79/1000 | Loss: 0.00002096
Iteration 80/1000 | Loss: 0.00002095
Iteration 81/1000 | Loss: 0.00002095
Iteration 82/1000 | Loss: 0.00002095
Iteration 83/1000 | Loss: 0.00002094
Iteration 84/1000 | Loss: 0.00002094
Iteration 85/1000 | Loss: 0.00002094
Iteration 86/1000 | Loss: 0.00002093
Iteration 87/1000 | Loss: 0.00002093
Iteration 88/1000 | Loss: 0.00002093
Iteration 89/1000 | Loss: 0.00002092
Iteration 90/1000 | Loss: 0.00002092
Iteration 91/1000 | Loss: 0.00002092
Iteration 92/1000 | Loss: 0.00002091
Iteration 93/1000 | Loss: 0.00002090
Iteration 94/1000 | Loss: 0.00002089
Iteration 95/1000 | Loss: 0.00002089
Iteration 96/1000 | Loss: 0.00002089
Iteration 97/1000 | Loss: 0.00002088
Iteration 98/1000 | Loss: 0.00002088
Iteration 99/1000 | Loss: 0.00002088
Iteration 100/1000 | Loss: 0.00002087
Iteration 101/1000 | Loss: 0.00002087
Iteration 102/1000 | Loss: 0.00002087
Iteration 103/1000 | Loss: 0.00002087
Iteration 104/1000 | Loss: 0.00002086
Iteration 105/1000 | Loss: 0.00002086
Iteration 106/1000 | Loss: 0.00002086
Iteration 107/1000 | Loss: 0.00002085
Iteration 108/1000 | Loss: 0.00002085
Iteration 109/1000 | Loss: 0.00002085
Iteration 110/1000 | Loss: 0.00002085
Iteration 111/1000 | Loss: 0.00002084
Iteration 112/1000 | Loss: 0.00002084
Iteration 113/1000 | Loss: 0.00002084
Iteration 114/1000 | Loss: 0.00002083
Iteration 115/1000 | Loss: 0.00002083
Iteration 116/1000 | Loss: 0.00002083
Iteration 117/1000 | Loss: 0.00002083
Iteration 118/1000 | Loss: 0.00002083
Iteration 119/1000 | Loss: 0.00002082
Iteration 120/1000 | Loss: 0.00002082
Iteration 121/1000 | Loss: 0.00002082
Iteration 122/1000 | Loss: 0.00002082
Iteration 123/1000 | Loss: 0.00002082
Iteration 124/1000 | Loss: 0.00002082
Iteration 125/1000 | Loss: 0.00002082
Iteration 126/1000 | Loss: 0.00002082
Iteration 127/1000 | Loss: 0.00002082
Iteration 128/1000 | Loss: 0.00002082
Iteration 129/1000 | Loss: 0.00002082
Iteration 130/1000 | Loss: 0.00002082
Iteration 131/1000 | Loss: 0.00002081
Iteration 132/1000 | Loss: 0.00002081
Iteration 133/1000 | Loss: 0.00002081
Iteration 134/1000 | Loss: 0.00002081
Iteration 135/1000 | Loss: 0.00002081
Iteration 136/1000 | Loss: 0.00002081
Iteration 137/1000 | Loss: 0.00002081
Iteration 138/1000 | Loss: 0.00002081
Iteration 139/1000 | Loss: 0.00002081
Iteration 140/1000 | Loss: 0.00002081
Iteration 141/1000 | Loss: 0.00002081
Iteration 142/1000 | Loss: 0.00002081
Iteration 143/1000 | Loss: 0.00002080
Iteration 144/1000 | Loss: 0.00002080
Iteration 145/1000 | Loss: 0.00002080
Iteration 146/1000 | Loss: 0.00002080
Iteration 147/1000 | Loss: 0.00002080
Iteration 148/1000 | Loss: 0.00002080
Iteration 149/1000 | Loss: 0.00002080
Iteration 150/1000 | Loss: 0.00002080
Iteration 151/1000 | Loss: 0.00002080
Iteration 152/1000 | Loss: 0.00002080
Iteration 153/1000 | Loss: 0.00002079
Iteration 154/1000 | Loss: 0.00002079
Iteration 155/1000 | Loss: 0.00002079
Iteration 156/1000 | Loss: 0.00002079
Iteration 157/1000 | Loss: 0.00002079
Iteration 158/1000 | Loss: 0.00002078
Iteration 159/1000 | Loss: 0.00002078
Iteration 160/1000 | Loss: 0.00002078
Iteration 161/1000 | Loss: 0.00002078
Iteration 162/1000 | Loss: 0.00002078
Iteration 163/1000 | Loss: 0.00002078
Iteration 164/1000 | Loss: 0.00002078
Iteration 165/1000 | Loss: 0.00002078
Iteration 166/1000 | Loss: 0.00002078
Iteration 167/1000 | Loss: 0.00002078
Iteration 168/1000 | Loss: 0.00002078
Iteration 169/1000 | Loss: 0.00002078
Iteration 170/1000 | Loss: 0.00002078
Iteration 171/1000 | Loss: 0.00002078
Iteration 172/1000 | Loss: 0.00002078
Iteration 173/1000 | Loss: 0.00002078
Iteration 174/1000 | Loss: 0.00002078
Iteration 175/1000 | Loss: 0.00002078
Iteration 176/1000 | Loss: 0.00002078
Iteration 177/1000 | Loss: 0.00002078
Iteration 178/1000 | Loss: 0.00002078
Iteration 179/1000 | Loss: 0.00002078
Iteration 180/1000 | Loss: 0.00002078
Iteration 181/1000 | Loss: 0.00002078
Iteration 182/1000 | Loss: 0.00002078
Iteration 183/1000 | Loss: 0.00002078
Iteration 184/1000 | Loss: 0.00002078
Iteration 185/1000 | Loss: 0.00002078
Iteration 186/1000 | Loss: 0.00002078
Iteration 187/1000 | Loss: 0.00002078
Iteration 188/1000 | Loss: 0.00002078
Iteration 189/1000 | Loss: 0.00002078
Iteration 190/1000 | Loss: 0.00002078
Iteration 191/1000 | Loss: 0.00002078
Iteration 192/1000 | Loss: 0.00002078
Iteration 193/1000 | Loss: 0.00002078
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [2.0776760720764287e-05, 2.0776760720764287e-05, 2.0776760720764287e-05, 2.0776760720764287e-05, 2.0776760720764287e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0776760720764287e-05

Optimization complete. Final v2v error: 3.730487585067749 mm

Highest mean error: 5.789340972900391 mm for frame 65

Lowest mean error: 2.6789138317108154 mm for frame 106

Saving results

Total time: 71.51922178268433
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00804752
Iteration 2/25 | Loss: 0.00134756
Iteration 3/25 | Loss: 0.00118119
Iteration 4/25 | Loss: 0.00116179
Iteration 5/25 | Loss: 0.00115837
Iteration 6/25 | Loss: 0.00115779
Iteration 7/25 | Loss: 0.00115779
Iteration 8/25 | Loss: 0.00115779
Iteration 9/25 | Loss: 0.00115779
Iteration 10/25 | Loss: 0.00115779
Iteration 11/25 | Loss: 0.00115779
Iteration 12/25 | Loss: 0.00115779
Iteration 13/25 | Loss: 0.00115779
Iteration 14/25 | Loss: 0.00115779
Iteration 15/25 | Loss: 0.00115779
Iteration 16/25 | Loss: 0.00115779
Iteration 17/25 | Loss: 0.00115779
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001157785882242024, 0.001157785882242024, 0.001157785882242024, 0.001157785882242024, 0.001157785882242024]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001157785882242024

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15717876
Iteration 2/25 | Loss: 0.00153523
Iteration 3/25 | Loss: 0.00153519
Iteration 4/25 | Loss: 0.00153519
Iteration 5/25 | Loss: 0.00153519
Iteration 6/25 | Loss: 0.00153519
Iteration 7/25 | Loss: 0.00153519
Iteration 8/25 | Loss: 0.00153519
Iteration 9/25 | Loss: 0.00153519
Iteration 10/25 | Loss: 0.00153519
Iteration 11/25 | Loss: 0.00153519
Iteration 12/25 | Loss: 0.00153519
Iteration 13/25 | Loss: 0.00153519
Iteration 14/25 | Loss: 0.00153519
Iteration 15/25 | Loss: 0.00153519
Iteration 16/25 | Loss: 0.00153519
Iteration 17/25 | Loss: 0.00153519
Iteration 18/25 | Loss: 0.00153519
Iteration 19/25 | Loss: 0.00153519
Iteration 20/25 | Loss: 0.00153519
Iteration 21/25 | Loss: 0.00153519
Iteration 22/25 | Loss: 0.00153519
Iteration 23/25 | Loss: 0.00153519
Iteration 24/25 | Loss: 0.00153519
Iteration 25/25 | Loss: 0.00153519

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00153519
Iteration 2/1000 | Loss: 0.00003545
Iteration 3/1000 | Loss: 0.00002409
Iteration 4/1000 | Loss: 0.00001836
Iteration 5/1000 | Loss: 0.00001669
Iteration 6/1000 | Loss: 0.00001574
Iteration 7/1000 | Loss: 0.00001485
Iteration 8/1000 | Loss: 0.00001431
Iteration 9/1000 | Loss: 0.00001392
Iteration 10/1000 | Loss: 0.00001355
Iteration 11/1000 | Loss: 0.00001326
Iteration 12/1000 | Loss: 0.00001297
Iteration 13/1000 | Loss: 0.00001296
Iteration 14/1000 | Loss: 0.00001284
Iteration 15/1000 | Loss: 0.00001262
Iteration 16/1000 | Loss: 0.00001247
Iteration 17/1000 | Loss: 0.00001245
Iteration 18/1000 | Loss: 0.00001228
Iteration 19/1000 | Loss: 0.00001227
Iteration 20/1000 | Loss: 0.00001227
Iteration 21/1000 | Loss: 0.00001226
Iteration 22/1000 | Loss: 0.00001225
Iteration 23/1000 | Loss: 0.00001218
Iteration 24/1000 | Loss: 0.00001216
Iteration 25/1000 | Loss: 0.00001216
Iteration 26/1000 | Loss: 0.00001215
Iteration 27/1000 | Loss: 0.00001214
Iteration 28/1000 | Loss: 0.00001212
Iteration 29/1000 | Loss: 0.00001211
Iteration 30/1000 | Loss: 0.00001208
Iteration 31/1000 | Loss: 0.00001208
Iteration 32/1000 | Loss: 0.00001208
Iteration 33/1000 | Loss: 0.00001208
Iteration 34/1000 | Loss: 0.00001208
Iteration 35/1000 | Loss: 0.00001208
Iteration 36/1000 | Loss: 0.00001207
Iteration 37/1000 | Loss: 0.00001205
Iteration 38/1000 | Loss: 0.00001205
Iteration 39/1000 | Loss: 0.00001204
Iteration 40/1000 | Loss: 0.00001203
Iteration 41/1000 | Loss: 0.00001203
Iteration 42/1000 | Loss: 0.00001202
Iteration 43/1000 | Loss: 0.00001202
Iteration 44/1000 | Loss: 0.00001202
Iteration 45/1000 | Loss: 0.00001202
Iteration 46/1000 | Loss: 0.00001202
Iteration 47/1000 | Loss: 0.00001201
Iteration 48/1000 | Loss: 0.00001201
Iteration 49/1000 | Loss: 0.00001201
Iteration 50/1000 | Loss: 0.00001201
Iteration 51/1000 | Loss: 0.00001201
Iteration 52/1000 | Loss: 0.00001200
Iteration 53/1000 | Loss: 0.00001200
Iteration 54/1000 | Loss: 0.00001200
Iteration 55/1000 | Loss: 0.00001199
Iteration 56/1000 | Loss: 0.00001199
Iteration 57/1000 | Loss: 0.00001199
Iteration 58/1000 | Loss: 0.00001199
Iteration 59/1000 | Loss: 0.00001199
Iteration 60/1000 | Loss: 0.00001199
Iteration 61/1000 | Loss: 0.00001199
Iteration 62/1000 | Loss: 0.00001199
Iteration 63/1000 | Loss: 0.00001198
Iteration 64/1000 | Loss: 0.00001198
Iteration 65/1000 | Loss: 0.00001198
Iteration 66/1000 | Loss: 0.00001198
Iteration 67/1000 | Loss: 0.00001197
Iteration 68/1000 | Loss: 0.00001197
Iteration 69/1000 | Loss: 0.00001197
Iteration 70/1000 | Loss: 0.00001196
Iteration 71/1000 | Loss: 0.00001196
Iteration 72/1000 | Loss: 0.00001196
Iteration 73/1000 | Loss: 0.00001196
Iteration 74/1000 | Loss: 0.00001196
Iteration 75/1000 | Loss: 0.00001195
Iteration 76/1000 | Loss: 0.00001195
Iteration 77/1000 | Loss: 0.00001195
Iteration 78/1000 | Loss: 0.00001195
Iteration 79/1000 | Loss: 0.00001195
Iteration 80/1000 | Loss: 0.00001194
Iteration 81/1000 | Loss: 0.00001194
Iteration 82/1000 | Loss: 0.00001194
Iteration 83/1000 | Loss: 0.00001194
Iteration 84/1000 | Loss: 0.00001193
Iteration 85/1000 | Loss: 0.00001193
Iteration 86/1000 | Loss: 0.00001193
Iteration 87/1000 | Loss: 0.00001193
Iteration 88/1000 | Loss: 0.00001192
Iteration 89/1000 | Loss: 0.00001192
Iteration 90/1000 | Loss: 0.00001192
Iteration 91/1000 | Loss: 0.00001192
Iteration 92/1000 | Loss: 0.00001192
Iteration 93/1000 | Loss: 0.00001192
Iteration 94/1000 | Loss: 0.00001192
Iteration 95/1000 | Loss: 0.00001192
Iteration 96/1000 | Loss: 0.00001191
Iteration 97/1000 | Loss: 0.00001191
Iteration 98/1000 | Loss: 0.00001191
Iteration 99/1000 | Loss: 0.00001191
Iteration 100/1000 | Loss: 0.00001191
Iteration 101/1000 | Loss: 0.00001190
Iteration 102/1000 | Loss: 0.00001190
Iteration 103/1000 | Loss: 0.00001190
Iteration 104/1000 | Loss: 0.00001190
Iteration 105/1000 | Loss: 0.00001190
Iteration 106/1000 | Loss: 0.00001189
Iteration 107/1000 | Loss: 0.00001189
Iteration 108/1000 | Loss: 0.00001189
Iteration 109/1000 | Loss: 0.00001189
Iteration 110/1000 | Loss: 0.00001189
Iteration 111/1000 | Loss: 0.00001189
Iteration 112/1000 | Loss: 0.00001189
Iteration 113/1000 | Loss: 0.00001189
Iteration 114/1000 | Loss: 0.00001188
Iteration 115/1000 | Loss: 0.00001188
Iteration 116/1000 | Loss: 0.00001188
Iteration 117/1000 | Loss: 0.00001188
Iteration 118/1000 | Loss: 0.00001188
Iteration 119/1000 | Loss: 0.00001187
Iteration 120/1000 | Loss: 0.00001187
Iteration 121/1000 | Loss: 0.00001187
Iteration 122/1000 | Loss: 0.00001187
Iteration 123/1000 | Loss: 0.00001187
Iteration 124/1000 | Loss: 0.00001187
Iteration 125/1000 | Loss: 0.00001186
Iteration 126/1000 | Loss: 0.00001186
Iteration 127/1000 | Loss: 0.00001186
Iteration 128/1000 | Loss: 0.00001186
Iteration 129/1000 | Loss: 0.00001186
Iteration 130/1000 | Loss: 0.00001186
Iteration 131/1000 | Loss: 0.00001186
Iteration 132/1000 | Loss: 0.00001186
Iteration 133/1000 | Loss: 0.00001186
Iteration 134/1000 | Loss: 0.00001186
Iteration 135/1000 | Loss: 0.00001185
Iteration 136/1000 | Loss: 0.00001185
Iteration 137/1000 | Loss: 0.00001185
Iteration 138/1000 | Loss: 0.00001184
Iteration 139/1000 | Loss: 0.00001184
Iteration 140/1000 | Loss: 0.00001184
Iteration 141/1000 | Loss: 0.00001184
Iteration 142/1000 | Loss: 0.00001184
Iteration 143/1000 | Loss: 0.00001183
Iteration 144/1000 | Loss: 0.00001183
Iteration 145/1000 | Loss: 0.00001183
Iteration 146/1000 | Loss: 0.00001182
Iteration 147/1000 | Loss: 0.00001182
Iteration 148/1000 | Loss: 0.00001182
Iteration 149/1000 | Loss: 0.00001182
Iteration 150/1000 | Loss: 0.00001182
Iteration 151/1000 | Loss: 0.00001182
Iteration 152/1000 | Loss: 0.00001182
Iteration 153/1000 | Loss: 0.00001182
Iteration 154/1000 | Loss: 0.00001181
Iteration 155/1000 | Loss: 0.00001181
Iteration 156/1000 | Loss: 0.00001181
Iteration 157/1000 | Loss: 0.00001181
Iteration 158/1000 | Loss: 0.00001181
Iteration 159/1000 | Loss: 0.00001181
Iteration 160/1000 | Loss: 0.00001181
Iteration 161/1000 | Loss: 0.00001181
Iteration 162/1000 | Loss: 0.00001181
Iteration 163/1000 | Loss: 0.00001181
Iteration 164/1000 | Loss: 0.00001181
Iteration 165/1000 | Loss: 0.00001181
Iteration 166/1000 | Loss: 0.00001181
Iteration 167/1000 | Loss: 0.00001181
Iteration 168/1000 | Loss: 0.00001181
Iteration 169/1000 | Loss: 0.00001181
Iteration 170/1000 | Loss: 0.00001181
Iteration 171/1000 | Loss: 0.00001181
Iteration 172/1000 | Loss: 0.00001181
Iteration 173/1000 | Loss: 0.00001181
Iteration 174/1000 | Loss: 0.00001181
Iteration 175/1000 | Loss: 0.00001181
Iteration 176/1000 | Loss: 0.00001181
Iteration 177/1000 | Loss: 0.00001181
Iteration 178/1000 | Loss: 0.00001181
Iteration 179/1000 | Loss: 0.00001181
Iteration 180/1000 | Loss: 0.00001181
Iteration 181/1000 | Loss: 0.00001181
Iteration 182/1000 | Loss: 0.00001181
Iteration 183/1000 | Loss: 0.00001181
Iteration 184/1000 | Loss: 0.00001181
Iteration 185/1000 | Loss: 0.00001181
Iteration 186/1000 | Loss: 0.00001181
Iteration 187/1000 | Loss: 0.00001181
Iteration 188/1000 | Loss: 0.00001181
Iteration 189/1000 | Loss: 0.00001181
Iteration 190/1000 | Loss: 0.00001181
Iteration 191/1000 | Loss: 0.00001181
Iteration 192/1000 | Loss: 0.00001181
Iteration 193/1000 | Loss: 0.00001181
Iteration 194/1000 | Loss: 0.00001181
Iteration 195/1000 | Loss: 0.00001181
Iteration 196/1000 | Loss: 0.00001181
Iteration 197/1000 | Loss: 0.00001181
Iteration 198/1000 | Loss: 0.00001181
Iteration 199/1000 | Loss: 0.00001181
Iteration 200/1000 | Loss: 0.00001181
Iteration 201/1000 | Loss: 0.00001181
Iteration 202/1000 | Loss: 0.00001181
Iteration 203/1000 | Loss: 0.00001181
Iteration 204/1000 | Loss: 0.00001181
Iteration 205/1000 | Loss: 0.00001181
Iteration 206/1000 | Loss: 0.00001181
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [1.1814122444775421e-05, 1.1814122444775421e-05, 1.1814122444775421e-05, 1.1814122444775421e-05, 1.1814122444775421e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1814122444775421e-05

Optimization complete. Final v2v error: 2.9349522590637207 mm

Highest mean error: 3.4134538173675537 mm for frame 92

Lowest mean error: 2.6376616954803467 mm for frame 171

Saving results

Total time: 44.469560623168945
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00451316
Iteration 2/25 | Loss: 0.00122455
Iteration 3/25 | Loss: 0.00116567
Iteration 4/25 | Loss: 0.00115574
Iteration 5/25 | Loss: 0.00115301
Iteration 6/25 | Loss: 0.00115289
Iteration 7/25 | Loss: 0.00115289
Iteration 8/25 | Loss: 0.00115289
Iteration 9/25 | Loss: 0.00115289
Iteration 10/25 | Loss: 0.00115289
Iteration 11/25 | Loss: 0.00115289
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001152888755314052, 0.001152888755314052, 0.001152888755314052, 0.001152888755314052, 0.001152888755314052]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001152888755314052

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21133471
Iteration 2/25 | Loss: 0.00181491
Iteration 3/25 | Loss: 0.00181491
Iteration 4/25 | Loss: 0.00181491
Iteration 5/25 | Loss: 0.00181491
Iteration 6/25 | Loss: 0.00181491
Iteration 7/25 | Loss: 0.00181491
Iteration 8/25 | Loss: 0.00181491
Iteration 9/25 | Loss: 0.00181491
Iteration 10/25 | Loss: 0.00181491
Iteration 11/25 | Loss: 0.00181490
Iteration 12/25 | Loss: 0.00181490
Iteration 13/25 | Loss: 0.00181490
Iteration 14/25 | Loss: 0.00181490
Iteration 15/25 | Loss: 0.00181490
Iteration 16/25 | Loss: 0.00181490
Iteration 17/25 | Loss: 0.00181490
Iteration 18/25 | Loss: 0.00181490
Iteration 19/25 | Loss: 0.00181490
Iteration 20/25 | Loss: 0.00181490
Iteration 21/25 | Loss: 0.00181490
Iteration 22/25 | Loss: 0.00181490
Iteration 23/25 | Loss: 0.00181490
Iteration 24/25 | Loss: 0.00181490
Iteration 25/25 | Loss: 0.00181490

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00181490
Iteration 2/1000 | Loss: 0.00002324
Iteration 3/1000 | Loss: 0.00001586
Iteration 4/1000 | Loss: 0.00001426
Iteration 5/1000 | Loss: 0.00001334
Iteration 6/1000 | Loss: 0.00001285
Iteration 7/1000 | Loss: 0.00001214
Iteration 8/1000 | Loss: 0.00001180
Iteration 9/1000 | Loss: 0.00001147
Iteration 10/1000 | Loss: 0.00001140
Iteration 11/1000 | Loss: 0.00001107
Iteration 12/1000 | Loss: 0.00001090
Iteration 13/1000 | Loss: 0.00001086
Iteration 14/1000 | Loss: 0.00001069
Iteration 15/1000 | Loss: 0.00001062
Iteration 16/1000 | Loss: 0.00001061
Iteration 17/1000 | Loss: 0.00001061
Iteration 18/1000 | Loss: 0.00001059
Iteration 19/1000 | Loss: 0.00001053
Iteration 20/1000 | Loss: 0.00001049
Iteration 21/1000 | Loss: 0.00001049
Iteration 22/1000 | Loss: 0.00001045
Iteration 23/1000 | Loss: 0.00001041
Iteration 24/1000 | Loss: 0.00001039
Iteration 25/1000 | Loss: 0.00001034
Iteration 26/1000 | Loss: 0.00001032
Iteration 27/1000 | Loss: 0.00001023
Iteration 28/1000 | Loss: 0.00001023
Iteration 29/1000 | Loss: 0.00001022
Iteration 30/1000 | Loss: 0.00001015
Iteration 31/1000 | Loss: 0.00001014
Iteration 32/1000 | Loss: 0.00001014
Iteration 33/1000 | Loss: 0.00001014
Iteration 34/1000 | Loss: 0.00001013
Iteration 35/1000 | Loss: 0.00001010
Iteration 36/1000 | Loss: 0.00001010
Iteration 37/1000 | Loss: 0.00001010
Iteration 38/1000 | Loss: 0.00001010
Iteration 39/1000 | Loss: 0.00001010
Iteration 40/1000 | Loss: 0.00001010
Iteration 41/1000 | Loss: 0.00001009
Iteration 42/1000 | Loss: 0.00001009
Iteration 43/1000 | Loss: 0.00001009
Iteration 44/1000 | Loss: 0.00001009
Iteration 45/1000 | Loss: 0.00001009
Iteration 46/1000 | Loss: 0.00001009
Iteration 47/1000 | Loss: 0.00001008
Iteration 48/1000 | Loss: 0.00001006
Iteration 49/1000 | Loss: 0.00001006
Iteration 50/1000 | Loss: 0.00001006
Iteration 51/1000 | Loss: 0.00001006
Iteration 52/1000 | Loss: 0.00001005
Iteration 53/1000 | Loss: 0.00001005
Iteration 54/1000 | Loss: 0.00001005
Iteration 55/1000 | Loss: 0.00001004
Iteration 56/1000 | Loss: 0.00001004
Iteration 57/1000 | Loss: 0.00001004
Iteration 58/1000 | Loss: 0.00001003
Iteration 59/1000 | Loss: 0.00001002
Iteration 60/1000 | Loss: 0.00001002
Iteration 61/1000 | Loss: 0.00001001
Iteration 62/1000 | Loss: 0.00001001
Iteration 63/1000 | Loss: 0.00001001
Iteration 64/1000 | Loss: 0.00001001
Iteration 65/1000 | Loss: 0.00001001
Iteration 66/1000 | Loss: 0.00001001
Iteration 67/1000 | Loss: 0.00001001
Iteration 68/1000 | Loss: 0.00001001
Iteration 69/1000 | Loss: 0.00001000
Iteration 70/1000 | Loss: 0.00001000
Iteration 71/1000 | Loss: 0.00001000
Iteration 72/1000 | Loss: 0.00001000
Iteration 73/1000 | Loss: 0.00000999
Iteration 74/1000 | Loss: 0.00000999
Iteration 75/1000 | Loss: 0.00000999
Iteration 76/1000 | Loss: 0.00000999
Iteration 77/1000 | Loss: 0.00000999
Iteration 78/1000 | Loss: 0.00000999
Iteration 79/1000 | Loss: 0.00000999
Iteration 80/1000 | Loss: 0.00000999
Iteration 81/1000 | Loss: 0.00000999
Iteration 82/1000 | Loss: 0.00000999
Iteration 83/1000 | Loss: 0.00000998
Iteration 84/1000 | Loss: 0.00000998
Iteration 85/1000 | Loss: 0.00000998
Iteration 86/1000 | Loss: 0.00000998
Iteration 87/1000 | Loss: 0.00000998
Iteration 88/1000 | Loss: 0.00000998
Iteration 89/1000 | Loss: 0.00000997
Iteration 90/1000 | Loss: 0.00000997
Iteration 91/1000 | Loss: 0.00000996
Iteration 92/1000 | Loss: 0.00000996
Iteration 93/1000 | Loss: 0.00000996
Iteration 94/1000 | Loss: 0.00000995
Iteration 95/1000 | Loss: 0.00000995
Iteration 96/1000 | Loss: 0.00000995
Iteration 97/1000 | Loss: 0.00000995
Iteration 98/1000 | Loss: 0.00000995
Iteration 99/1000 | Loss: 0.00000995
Iteration 100/1000 | Loss: 0.00000995
Iteration 101/1000 | Loss: 0.00000995
Iteration 102/1000 | Loss: 0.00000995
Iteration 103/1000 | Loss: 0.00000995
Iteration 104/1000 | Loss: 0.00000995
Iteration 105/1000 | Loss: 0.00000995
Iteration 106/1000 | Loss: 0.00000995
Iteration 107/1000 | Loss: 0.00000994
Iteration 108/1000 | Loss: 0.00000994
Iteration 109/1000 | Loss: 0.00000994
Iteration 110/1000 | Loss: 0.00000994
Iteration 111/1000 | Loss: 0.00000994
Iteration 112/1000 | Loss: 0.00000994
Iteration 113/1000 | Loss: 0.00000993
Iteration 114/1000 | Loss: 0.00000993
Iteration 115/1000 | Loss: 0.00000993
Iteration 116/1000 | Loss: 0.00000993
Iteration 117/1000 | Loss: 0.00000993
Iteration 118/1000 | Loss: 0.00000993
Iteration 119/1000 | Loss: 0.00000993
Iteration 120/1000 | Loss: 0.00000992
Iteration 121/1000 | Loss: 0.00000992
Iteration 122/1000 | Loss: 0.00000992
Iteration 123/1000 | Loss: 0.00000992
Iteration 124/1000 | Loss: 0.00000992
Iteration 125/1000 | Loss: 0.00000992
Iteration 126/1000 | Loss: 0.00000992
Iteration 127/1000 | Loss: 0.00000992
Iteration 128/1000 | Loss: 0.00000992
Iteration 129/1000 | Loss: 0.00000992
Iteration 130/1000 | Loss: 0.00000992
Iteration 131/1000 | Loss: 0.00000991
Iteration 132/1000 | Loss: 0.00000991
Iteration 133/1000 | Loss: 0.00000991
Iteration 134/1000 | Loss: 0.00000991
Iteration 135/1000 | Loss: 0.00000991
Iteration 136/1000 | Loss: 0.00000991
Iteration 137/1000 | Loss: 0.00000991
Iteration 138/1000 | Loss: 0.00000991
Iteration 139/1000 | Loss: 0.00000991
Iteration 140/1000 | Loss: 0.00000991
Iteration 141/1000 | Loss: 0.00000990
Iteration 142/1000 | Loss: 0.00000990
Iteration 143/1000 | Loss: 0.00000990
Iteration 144/1000 | Loss: 0.00000990
Iteration 145/1000 | Loss: 0.00000990
Iteration 146/1000 | Loss: 0.00000990
Iteration 147/1000 | Loss: 0.00000989
Iteration 148/1000 | Loss: 0.00000989
Iteration 149/1000 | Loss: 0.00000989
Iteration 150/1000 | Loss: 0.00000988
Iteration 151/1000 | Loss: 0.00000988
Iteration 152/1000 | Loss: 0.00000988
Iteration 153/1000 | Loss: 0.00000988
Iteration 154/1000 | Loss: 0.00000988
Iteration 155/1000 | Loss: 0.00000988
Iteration 156/1000 | Loss: 0.00000988
Iteration 157/1000 | Loss: 0.00000988
Iteration 158/1000 | Loss: 0.00000988
Iteration 159/1000 | Loss: 0.00000988
Iteration 160/1000 | Loss: 0.00000987
Iteration 161/1000 | Loss: 0.00000987
Iteration 162/1000 | Loss: 0.00000987
Iteration 163/1000 | Loss: 0.00000987
Iteration 164/1000 | Loss: 0.00000987
Iteration 165/1000 | Loss: 0.00000987
Iteration 166/1000 | Loss: 0.00000987
Iteration 167/1000 | Loss: 0.00000987
Iteration 168/1000 | Loss: 0.00000987
Iteration 169/1000 | Loss: 0.00000986
Iteration 170/1000 | Loss: 0.00000986
Iteration 171/1000 | Loss: 0.00000986
Iteration 172/1000 | Loss: 0.00000986
Iteration 173/1000 | Loss: 0.00000986
Iteration 174/1000 | Loss: 0.00000986
Iteration 175/1000 | Loss: 0.00000986
Iteration 176/1000 | Loss: 0.00000986
Iteration 177/1000 | Loss: 0.00000986
Iteration 178/1000 | Loss: 0.00000986
Iteration 179/1000 | Loss: 0.00000986
Iteration 180/1000 | Loss: 0.00000986
Iteration 181/1000 | Loss: 0.00000986
Iteration 182/1000 | Loss: 0.00000986
Iteration 183/1000 | Loss: 0.00000986
Iteration 184/1000 | Loss: 0.00000986
Iteration 185/1000 | Loss: 0.00000986
Iteration 186/1000 | Loss: 0.00000986
Iteration 187/1000 | Loss: 0.00000986
Iteration 188/1000 | Loss: 0.00000986
Iteration 189/1000 | Loss: 0.00000986
Iteration 190/1000 | Loss: 0.00000986
Iteration 191/1000 | Loss: 0.00000986
Iteration 192/1000 | Loss: 0.00000986
Iteration 193/1000 | Loss: 0.00000986
Iteration 194/1000 | Loss: 0.00000986
Iteration 195/1000 | Loss: 0.00000986
Iteration 196/1000 | Loss: 0.00000986
Iteration 197/1000 | Loss: 0.00000986
Iteration 198/1000 | Loss: 0.00000986
Iteration 199/1000 | Loss: 0.00000986
Iteration 200/1000 | Loss: 0.00000986
Iteration 201/1000 | Loss: 0.00000986
Iteration 202/1000 | Loss: 0.00000986
Iteration 203/1000 | Loss: 0.00000986
Iteration 204/1000 | Loss: 0.00000986
Iteration 205/1000 | Loss: 0.00000986
Iteration 206/1000 | Loss: 0.00000986
Iteration 207/1000 | Loss: 0.00000986
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [9.861190847004764e-06, 9.861190847004764e-06, 9.861190847004764e-06, 9.861190847004764e-06, 9.861190847004764e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.861190847004764e-06

Optimization complete. Final v2v error: 2.6998841762542725 mm

Highest mean error: 2.820162296295166 mm for frame 107

Lowest mean error: 2.5677096843719482 mm for frame 45

Saving results

Total time: 42.43061351776123
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00976066
Iteration 2/25 | Loss: 0.00319968
Iteration 3/25 | Loss: 0.00207336
Iteration 4/25 | Loss: 0.00195729
Iteration 5/25 | Loss: 0.00177972
Iteration 6/25 | Loss: 0.00169113
Iteration 7/25 | Loss: 0.00163488
Iteration 8/25 | Loss: 0.00157307
Iteration 9/25 | Loss: 0.00155606
Iteration 10/25 | Loss: 0.00153503
Iteration 11/25 | Loss: 0.00151970
Iteration 12/25 | Loss: 0.00150737
Iteration 13/25 | Loss: 0.00149388
Iteration 14/25 | Loss: 0.00149107
Iteration 15/25 | Loss: 0.00148185
Iteration 16/25 | Loss: 0.00147307
Iteration 17/25 | Loss: 0.00147518
Iteration 18/25 | Loss: 0.00147177
Iteration 19/25 | Loss: 0.00146987
Iteration 20/25 | Loss: 0.00146914
Iteration 21/25 | Loss: 0.00146859
Iteration 22/25 | Loss: 0.00146823
Iteration 23/25 | Loss: 0.00147261
Iteration 24/25 | Loss: 0.00148345
Iteration 25/25 | Loss: 0.00148247

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21370935
Iteration 2/25 | Loss: 0.00506477
Iteration 3/25 | Loss: 0.00418532
Iteration 4/25 | Loss: 0.00418532
Iteration 5/25 | Loss: 0.00418532
Iteration 6/25 | Loss: 0.00418532
Iteration 7/25 | Loss: 0.00418532
Iteration 8/25 | Loss: 0.00418532
Iteration 9/25 | Loss: 0.00418532
Iteration 10/25 | Loss: 0.00418532
Iteration 11/25 | Loss: 0.00418532
Iteration 12/25 | Loss: 0.00418532
Iteration 13/25 | Loss: 0.00418532
Iteration 14/25 | Loss: 0.00418532
Iteration 15/25 | Loss: 0.00418532
Iteration 16/25 | Loss: 0.00418532
Iteration 17/25 | Loss: 0.00418532
Iteration 18/25 | Loss: 0.00418532
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.004185318015515804, 0.004185318015515804, 0.004185318015515804, 0.004185318015515804, 0.004185318015515804]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004185318015515804

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00418532
Iteration 2/1000 | Loss: 0.00199179
Iteration 3/1000 | Loss: 0.00603927
Iteration 4/1000 | Loss: 0.00070049
Iteration 5/1000 | Loss: 0.00042568
Iteration 6/1000 | Loss: 0.00022841
Iteration 7/1000 | Loss: 0.00064346
Iteration 8/1000 | Loss: 0.00058505
Iteration 9/1000 | Loss: 0.00284101
Iteration 10/1000 | Loss: 0.00030530
Iteration 11/1000 | Loss: 0.00062073
Iteration 12/1000 | Loss: 0.00024684
Iteration 13/1000 | Loss: 0.00037718
Iteration 14/1000 | Loss: 0.00097203
Iteration 15/1000 | Loss: 0.00024460
Iteration 16/1000 | Loss: 0.00056048
Iteration 17/1000 | Loss: 0.00081830
Iteration 18/1000 | Loss: 0.00017524
Iteration 19/1000 | Loss: 0.00019361
Iteration 20/1000 | Loss: 0.00080425
Iteration 21/1000 | Loss: 0.00157166
Iteration 22/1000 | Loss: 0.00060126
Iteration 23/1000 | Loss: 0.00049174
Iteration 24/1000 | Loss: 0.00077442
Iteration 25/1000 | Loss: 0.00117293
Iteration 26/1000 | Loss: 0.00026599
Iteration 27/1000 | Loss: 0.00026347
Iteration 28/1000 | Loss: 0.00168283
Iteration 29/1000 | Loss: 0.00059954
Iteration 30/1000 | Loss: 0.00084439
Iteration 31/1000 | Loss: 0.00023330
Iteration 32/1000 | Loss: 0.00047198
Iteration 33/1000 | Loss: 0.00030586
Iteration 34/1000 | Loss: 0.00023939
Iteration 35/1000 | Loss: 0.00022529
Iteration 36/1000 | Loss: 0.00026384
Iteration 37/1000 | Loss: 0.00021841
Iteration 38/1000 | Loss: 0.00018125
Iteration 39/1000 | Loss: 0.00019268
Iteration 40/1000 | Loss: 0.00089365
Iteration 41/1000 | Loss: 0.00010119
Iteration 42/1000 | Loss: 0.00020218
Iteration 43/1000 | Loss: 0.00023467
Iteration 44/1000 | Loss: 0.00144596
Iteration 45/1000 | Loss: 0.00194053
Iteration 46/1000 | Loss: 0.00039083
Iteration 47/1000 | Loss: 0.00016859
Iteration 48/1000 | Loss: 0.00025054
Iteration 49/1000 | Loss: 0.00018091
Iteration 50/1000 | Loss: 0.00009793
Iteration 51/1000 | Loss: 0.00018353
Iteration 52/1000 | Loss: 0.00009050
Iteration 53/1000 | Loss: 0.00027846
Iteration 54/1000 | Loss: 0.00008840
Iteration 55/1000 | Loss: 0.00008702
Iteration 56/1000 | Loss: 0.00038582
Iteration 57/1000 | Loss: 0.00028392
Iteration 58/1000 | Loss: 0.00008557
Iteration 59/1000 | Loss: 0.00038482
Iteration 60/1000 | Loss: 0.00017666
Iteration 61/1000 | Loss: 0.00045961
Iteration 62/1000 | Loss: 0.00115829
Iteration 63/1000 | Loss: 0.00578606
Iteration 64/1000 | Loss: 0.00207397
Iteration 65/1000 | Loss: 0.00056459
Iteration 66/1000 | Loss: 0.00036333
Iteration 67/1000 | Loss: 0.00035447
Iteration 68/1000 | Loss: 0.00047499
Iteration 69/1000 | Loss: 0.00065838
Iteration 70/1000 | Loss: 0.00026433
Iteration 71/1000 | Loss: 0.00013502
Iteration 72/1000 | Loss: 0.00055261
Iteration 73/1000 | Loss: 0.00053175
Iteration 74/1000 | Loss: 0.00036486
Iteration 75/1000 | Loss: 0.00027163
Iteration 76/1000 | Loss: 0.00038440
Iteration 77/1000 | Loss: 0.00029839
Iteration 78/1000 | Loss: 0.00014651
Iteration 79/1000 | Loss: 0.00004599
Iteration 80/1000 | Loss: 0.00013703
Iteration 81/1000 | Loss: 0.00004857
Iteration 82/1000 | Loss: 0.00003715
Iteration 83/1000 | Loss: 0.00028242
Iteration 84/1000 | Loss: 0.00005154
Iteration 85/1000 | Loss: 0.00004542
Iteration 86/1000 | Loss: 0.00019296
Iteration 87/1000 | Loss: 0.00032639
Iteration 88/1000 | Loss: 0.00038081
Iteration 89/1000 | Loss: 0.00003695
Iteration 90/1000 | Loss: 0.00002659
Iteration 91/1000 | Loss: 0.00002516
Iteration 92/1000 | Loss: 0.00009091
Iteration 93/1000 | Loss: 0.00002408
Iteration 94/1000 | Loss: 0.00002352
Iteration 95/1000 | Loss: 0.00017525
Iteration 96/1000 | Loss: 0.00002832
Iteration 97/1000 | Loss: 0.00002392
Iteration 98/1000 | Loss: 0.00002230
Iteration 99/1000 | Loss: 0.00002115
Iteration 100/1000 | Loss: 0.00002049
Iteration 101/1000 | Loss: 0.00002019
Iteration 102/1000 | Loss: 0.00008159
Iteration 103/1000 | Loss: 0.00027047
Iteration 104/1000 | Loss: 0.00002415
Iteration 105/1000 | Loss: 0.00002004
Iteration 106/1000 | Loss: 0.00001886
Iteration 107/1000 | Loss: 0.00001841
Iteration 108/1000 | Loss: 0.00001801
Iteration 109/1000 | Loss: 0.00001765
Iteration 110/1000 | Loss: 0.00001729
Iteration 111/1000 | Loss: 0.00001704
Iteration 112/1000 | Loss: 0.00016667
Iteration 113/1000 | Loss: 0.00002577
Iteration 114/1000 | Loss: 0.00002053
Iteration 115/1000 | Loss: 0.00007129
Iteration 116/1000 | Loss: 0.00011350
Iteration 117/1000 | Loss: 0.00013645
Iteration 118/1000 | Loss: 0.00011241
Iteration 119/1000 | Loss: 0.00013436
Iteration 120/1000 | Loss: 0.00015843
Iteration 121/1000 | Loss: 0.00003149
Iteration 122/1000 | Loss: 0.00002182
Iteration 123/1000 | Loss: 0.00002059
Iteration 124/1000 | Loss: 0.00002003
Iteration 125/1000 | Loss: 0.00020958
Iteration 126/1000 | Loss: 0.00037568
Iteration 127/1000 | Loss: 0.00015848
Iteration 128/1000 | Loss: 0.00017905
Iteration 129/1000 | Loss: 0.00002083
Iteration 130/1000 | Loss: 0.00001872
Iteration 131/1000 | Loss: 0.00001802
Iteration 132/1000 | Loss: 0.00001739
Iteration 133/1000 | Loss: 0.00001703
Iteration 134/1000 | Loss: 0.00001689
Iteration 135/1000 | Loss: 0.00001680
Iteration 136/1000 | Loss: 0.00001678
Iteration 137/1000 | Loss: 0.00001676
Iteration 138/1000 | Loss: 0.00001676
Iteration 139/1000 | Loss: 0.00001675
Iteration 140/1000 | Loss: 0.00001675
Iteration 141/1000 | Loss: 0.00001675
Iteration 142/1000 | Loss: 0.00001674
Iteration 143/1000 | Loss: 0.00001673
Iteration 144/1000 | Loss: 0.00001673
Iteration 145/1000 | Loss: 0.00001673
Iteration 146/1000 | Loss: 0.00001673
Iteration 147/1000 | Loss: 0.00001673
Iteration 148/1000 | Loss: 0.00001672
Iteration 149/1000 | Loss: 0.00001672
Iteration 150/1000 | Loss: 0.00001672
Iteration 151/1000 | Loss: 0.00001672
Iteration 152/1000 | Loss: 0.00001672
Iteration 153/1000 | Loss: 0.00001672
Iteration 154/1000 | Loss: 0.00001672
Iteration 155/1000 | Loss: 0.00001672
Iteration 156/1000 | Loss: 0.00001672
Iteration 157/1000 | Loss: 0.00001671
Iteration 158/1000 | Loss: 0.00001671
Iteration 159/1000 | Loss: 0.00001671
Iteration 160/1000 | Loss: 0.00001671
Iteration 161/1000 | Loss: 0.00001671
Iteration 162/1000 | Loss: 0.00001671
Iteration 163/1000 | Loss: 0.00001671
Iteration 164/1000 | Loss: 0.00001670
Iteration 165/1000 | Loss: 0.00001669
Iteration 166/1000 | Loss: 0.00001668
Iteration 167/1000 | Loss: 0.00001667
Iteration 168/1000 | Loss: 0.00001667
Iteration 169/1000 | Loss: 0.00001667
Iteration 170/1000 | Loss: 0.00001667
Iteration 171/1000 | Loss: 0.00001667
Iteration 172/1000 | Loss: 0.00001667
Iteration 173/1000 | Loss: 0.00001666
Iteration 174/1000 | Loss: 0.00001666
Iteration 175/1000 | Loss: 0.00001666
Iteration 176/1000 | Loss: 0.00001666
Iteration 177/1000 | Loss: 0.00001666
Iteration 178/1000 | Loss: 0.00001665
Iteration 179/1000 | Loss: 0.00001665
Iteration 180/1000 | Loss: 0.00001665
Iteration 181/1000 | Loss: 0.00001664
Iteration 182/1000 | Loss: 0.00001664
Iteration 183/1000 | Loss: 0.00001663
Iteration 184/1000 | Loss: 0.00001663
Iteration 185/1000 | Loss: 0.00001662
Iteration 186/1000 | Loss: 0.00001662
Iteration 187/1000 | Loss: 0.00001662
Iteration 188/1000 | Loss: 0.00001662
Iteration 189/1000 | Loss: 0.00001662
Iteration 190/1000 | Loss: 0.00001662
Iteration 191/1000 | Loss: 0.00001662
Iteration 192/1000 | Loss: 0.00001661
Iteration 193/1000 | Loss: 0.00001661
Iteration 194/1000 | Loss: 0.00001661
Iteration 195/1000 | Loss: 0.00001661
Iteration 196/1000 | Loss: 0.00001661
Iteration 197/1000 | Loss: 0.00001661
Iteration 198/1000 | Loss: 0.00001661
Iteration 199/1000 | Loss: 0.00001661
Iteration 200/1000 | Loss: 0.00001661
Iteration 201/1000 | Loss: 0.00001661
Iteration 202/1000 | Loss: 0.00001660
Iteration 203/1000 | Loss: 0.00001659
Iteration 204/1000 | Loss: 0.00001659
Iteration 205/1000 | Loss: 0.00001659
Iteration 206/1000 | Loss: 0.00001658
Iteration 207/1000 | Loss: 0.00001658
Iteration 208/1000 | Loss: 0.00001658
Iteration 209/1000 | Loss: 0.00001658
Iteration 210/1000 | Loss: 0.00001658
Iteration 211/1000 | Loss: 0.00001657
Iteration 212/1000 | Loss: 0.00001657
Iteration 213/1000 | Loss: 0.00001657
Iteration 214/1000 | Loss: 0.00001657
Iteration 215/1000 | Loss: 0.00001657
Iteration 216/1000 | Loss: 0.00001657
Iteration 217/1000 | Loss: 0.00001657
Iteration 218/1000 | Loss: 0.00001657
Iteration 219/1000 | Loss: 0.00001657
Iteration 220/1000 | Loss: 0.00001657
Iteration 221/1000 | Loss: 0.00001657
Iteration 222/1000 | Loss: 0.00001656
Iteration 223/1000 | Loss: 0.00001656
Iteration 224/1000 | Loss: 0.00001656
Iteration 225/1000 | Loss: 0.00001656
Iteration 226/1000 | Loss: 0.00001656
Iteration 227/1000 | Loss: 0.00001656
Iteration 228/1000 | Loss: 0.00001656
Iteration 229/1000 | Loss: 0.00001656
Iteration 230/1000 | Loss: 0.00001655
Iteration 231/1000 | Loss: 0.00001654
Iteration 232/1000 | Loss: 0.00001654
Iteration 233/1000 | Loss: 0.00001654
Iteration 234/1000 | Loss: 0.00001654
Iteration 235/1000 | Loss: 0.00001654
Iteration 236/1000 | Loss: 0.00001653
Iteration 237/1000 | Loss: 0.00001653
Iteration 238/1000 | Loss: 0.00001653
Iteration 239/1000 | Loss: 0.00001652
Iteration 240/1000 | Loss: 0.00001652
Iteration 241/1000 | Loss: 0.00001652
Iteration 242/1000 | Loss: 0.00001652
Iteration 243/1000 | Loss: 0.00001652
Iteration 244/1000 | Loss: 0.00001652
Iteration 245/1000 | Loss: 0.00001651
Iteration 246/1000 | Loss: 0.00001651
Iteration 247/1000 | Loss: 0.00001651
Iteration 248/1000 | Loss: 0.00001651
Iteration 249/1000 | Loss: 0.00001650
Iteration 250/1000 | Loss: 0.00001650
Iteration 251/1000 | Loss: 0.00001650
Iteration 252/1000 | Loss: 0.00001650
Iteration 253/1000 | Loss: 0.00001650
Iteration 254/1000 | Loss: 0.00001650
Iteration 255/1000 | Loss: 0.00001650
Iteration 256/1000 | Loss: 0.00001650
Iteration 257/1000 | Loss: 0.00001650
Iteration 258/1000 | Loss: 0.00001650
Iteration 259/1000 | Loss: 0.00001649
Iteration 260/1000 | Loss: 0.00001649
Iteration 261/1000 | Loss: 0.00001649
Iteration 262/1000 | Loss: 0.00001649
Iteration 263/1000 | Loss: 0.00001649
Iteration 264/1000 | Loss: 0.00001649
Iteration 265/1000 | Loss: 0.00001649
Iteration 266/1000 | Loss: 0.00001649
Iteration 267/1000 | Loss: 0.00001649
Iteration 268/1000 | Loss: 0.00001649
Iteration 269/1000 | Loss: 0.00001649
Iteration 270/1000 | Loss: 0.00001649
Iteration 271/1000 | Loss: 0.00001649
Iteration 272/1000 | Loss: 0.00001649
Iteration 273/1000 | Loss: 0.00001649
Iteration 274/1000 | Loss: 0.00001648
Iteration 275/1000 | Loss: 0.00001648
Iteration 276/1000 | Loss: 0.00001648
Iteration 277/1000 | Loss: 0.00001648
Iteration 278/1000 | Loss: 0.00001648
Iteration 279/1000 | Loss: 0.00001648
Iteration 280/1000 | Loss: 0.00001648
Iteration 281/1000 | Loss: 0.00001648
Iteration 282/1000 | Loss: 0.00001648
Iteration 283/1000 | Loss: 0.00001648
Iteration 284/1000 | Loss: 0.00001648
Iteration 285/1000 | Loss: 0.00001648
Iteration 286/1000 | Loss: 0.00001648
Iteration 287/1000 | Loss: 0.00001648
Iteration 288/1000 | Loss: 0.00001648
Iteration 289/1000 | Loss: 0.00001648
Iteration 290/1000 | Loss: 0.00001648
Iteration 291/1000 | Loss: 0.00001648
Iteration 292/1000 | Loss: 0.00001648
Iteration 293/1000 | Loss: 0.00001648
Iteration 294/1000 | Loss: 0.00001648
Iteration 295/1000 | Loss: 0.00001648
Iteration 296/1000 | Loss: 0.00001648
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 296. Stopping optimization.
Last 5 losses: [1.6484878869960085e-05, 1.6484878869960085e-05, 1.6484878869960085e-05, 1.6484878869960085e-05, 1.6484878869960085e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6484878869960085e-05

Optimization complete. Final v2v error: 3.2847912311553955 mm

Highest mean error: 6.467535018920898 mm for frame 2

Lowest mean error: 2.899226188659668 mm for frame 127

Saving results

Total time: 261.97638607025146
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00804924
Iteration 2/25 | Loss: 0.00170051
Iteration 3/25 | Loss: 0.00150367
Iteration 4/25 | Loss: 0.00145782
Iteration 5/25 | Loss: 0.00144729
Iteration 6/25 | Loss: 0.00145173
Iteration 7/25 | Loss: 0.00141755
Iteration 8/25 | Loss: 0.00139912
Iteration 9/25 | Loss: 0.00139171
Iteration 10/25 | Loss: 0.00139425
Iteration 11/25 | Loss: 0.00138901
Iteration 12/25 | Loss: 0.00138218
Iteration 13/25 | Loss: 0.00136181
Iteration 14/25 | Loss: 0.00135404
Iteration 15/25 | Loss: 0.00135181
Iteration 16/25 | Loss: 0.00134913
Iteration 17/25 | Loss: 0.00134830
Iteration 18/25 | Loss: 0.00135517
Iteration 19/25 | Loss: 0.00134850
Iteration 20/25 | Loss: 0.00134578
Iteration 21/25 | Loss: 0.00134527
Iteration 22/25 | Loss: 0.00134516
Iteration 23/25 | Loss: 0.00134515
Iteration 24/25 | Loss: 0.00134515
Iteration 25/25 | Loss: 0.00134515

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17880428
Iteration 2/25 | Loss: 0.00443699
Iteration 3/25 | Loss: 0.00443698
Iteration 4/25 | Loss: 0.00443698
Iteration 5/25 | Loss: 0.00443697
Iteration 6/25 | Loss: 0.00443697
Iteration 7/25 | Loss: 0.00443697
Iteration 8/25 | Loss: 0.00443697
Iteration 9/25 | Loss: 0.00443697
Iteration 10/25 | Loss: 0.00443697
Iteration 11/25 | Loss: 0.00443697
Iteration 12/25 | Loss: 0.00443697
Iteration 13/25 | Loss: 0.00443697
Iteration 14/25 | Loss: 0.00443697
Iteration 15/25 | Loss: 0.00443697
Iteration 16/25 | Loss: 0.00443697
Iteration 17/25 | Loss: 0.00443697
Iteration 18/25 | Loss: 0.00443697
Iteration 19/25 | Loss: 0.00443697
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.004436972551047802, 0.004436972551047802, 0.004436972551047802, 0.004436972551047802, 0.004436972551047802]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004436972551047802

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00443697
Iteration 2/1000 | Loss: 0.00025686
Iteration 3/1000 | Loss: 0.00018062
Iteration 4/1000 | Loss: 0.00015869
Iteration 5/1000 | Loss: 0.00013974
Iteration 6/1000 | Loss: 0.00013255
Iteration 7/1000 | Loss: 0.00012467
Iteration 8/1000 | Loss: 0.00012025
Iteration 9/1000 | Loss: 0.00011612
Iteration 10/1000 | Loss: 0.00011347
Iteration 11/1000 | Loss: 0.00049576
Iteration 12/1000 | Loss: 0.00186496
Iteration 13/1000 | Loss: 0.00349539
Iteration 14/1000 | Loss: 0.00156021
Iteration 15/1000 | Loss: 0.00147528
Iteration 16/1000 | Loss: 0.00033178
Iteration 17/1000 | Loss: 0.00046390
Iteration 18/1000 | Loss: 0.00015142
Iteration 19/1000 | Loss: 0.00047520
Iteration 20/1000 | Loss: 0.00010187
Iteration 21/1000 | Loss: 0.00009332
Iteration 22/1000 | Loss: 0.00008464
Iteration 23/1000 | Loss: 0.00013186
Iteration 24/1000 | Loss: 0.00007489
Iteration 25/1000 | Loss: 0.00007179
Iteration 26/1000 | Loss: 0.00006935
Iteration 27/1000 | Loss: 0.00006728
Iteration 28/1000 | Loss: 0.00056594
Iteration 29/1000 | Loss: 0.00006898
Iteration 30/1000 | Loss: 0.00006509
Iteration 31/1000 | Loss: 0.00006351
Iteration 32/1000 | Loss: 0.00058653
Iteration 33/1000 | Loss: 0.00006520
Iteration 34/1000 | Loss: 0.00006092
Iteration 35/1000 | Loss: 0.00061976
Iteration 36/1000 | Loss: 0.00006161
Iteration 37/1000 | Loss: 0.00005874
Iteration 38/1000 | Loss: 0.00005697
Iteration 39/1000 | Loss: 0.00064084
Iteration 40/1000 | Loss: 0.00006047
Iteration 41/1000 | Loss: 0.00005542
Iteration 42/1000 | Loss: 0.00005374
Iteration 43/1000 | Loss: 0.00067090
Iteration 44/1000 | Loss: 0.00115183
Iteration 45/1000 | Loss: 0.00019255
Iteration 46/1000 | Loss: 0.00064328
Iteration 47/1000 | Loss: 0.00006847
Iteration 48/1000 | Loss: 0.00005453
Iteration 49/1000 | Loss: 0.00004997
Iteration 50/1000 | Loss: 0.00004629
Iteration 51/1000 | Loss: 0.00057570
Iteration 52/1000 | Loss: 0.00004468
Iteration 53/1000 | Loss: 0.00128280
Iteration 54/1000 | Loss: 0.00153178
Iteration 55/1000 | Loss: 0.00119843
Iteration 56/1000 | Loss: 0.00004959
Iteration 57/1000 | Loss: 0.00004077
Iteration 58/1000 | Loss: 0.00003635
Iteration 59/1000 | Loss: 0.00003269
Iteration 60/1000 | Loss: 0.00065847
Iteration 61/1000 | Loss: 0.00003427
Iteration 62/1000 | Loss: 0.00002959
Iteration 63/1000 | Loss: 0.00118613
Iteration 64/1000 | Loss: 0.00003468
Iteration 65/1000 | Loss: 0.00002716
Iteration 66/1000 | Loss: 0.00002352
Iteration 67/1000 | Loss: 0.00001986
Iteration 68/1000 | Loss: 0.00001804
Iteration 69/1000 | Loss: 0.00001631
Iteration 70/1000 | Loss: 0.00001568
Iteration 71/1000 | Loss: 0.00001531
Iteration 72/1000 | Loss: 0.00001489
Iteration 73/1000 | Loss: 0.00001452
Iteration 74/1000 | Loss: 0.00001420
Iteration 75/1000 | Loss: 0.00001402
Iteration 76/1000 | Loss: 0.00001399
Iteration 77/1000 | Loss: 0.00001394
Iteration 78/1000 | Loss: 0.00001383
Iteration 79/1000 | Loss: 0.00001381
Iteration 80/1000 | Loss: 0.00001375
Iteration 81/1000 | Loss: 0.00001368
Iteration 82/1000 | Loss: 0.00001365
Iteration 83/1000 | Loss: 0.00001365
Iteration 84/1000 | Loss: 0.00001365
Iteration 85/1000 | Loss: 0.00001363
Iteration 86/1000 | Loss: 0.00001362
Iteration 87/1000 | Loss: 0.00001361
Iteration 88/1000 | Loss: 0.00001361
Iteration 89/1000 | Loss: 0.00001360
Iteration 90/1000 | Loss: 0.00001359
Iteration 91/1000 | Loss: 0.00001359
Iteration 92/1000 | Loss: 0.00001358
Iteration 93/1000 | Loss: 0.00001358
Iteration 94/1000 | Loss: 0.00001358
Iteration 95/1000 | Loss: 0.00001358
Iteration 96/1000 | Loss: 0.00001358
Iteration 97/1000 | Loss: 0.00001358
Iteration 98/1000 | Loss: 0.00001357
Iteration 99/1000 | Loss: 0.00001356
Iteration 100/1000 | Loss: 0.00001356
Iteration 101/1000 | Loss: 0.00001355
Iteration 102/1000 | Loss: 0.00001355
Iteration 103/1000 | Loss: 0.00001355
Iteration 104/1000 | Loss: 0.00001355
Iteration 105/1000 | Loss: 0.00001354
Iteration 106/1000 | Loss: 0.00001354
Iteration 107/1000 | Loss: 0.00001354
Iteration 108/1000 | Loss: 0.00001354
Iteration 109/1000 | Loss: 0.00001354
Iteration 110/1000 | Loss: 0.00001354
Iteration 111/1000 | Loss: 0.00001353
Iteration 112/1000 | Loss: 0.00001353
Iteration 113/1000 | Loss: 0.00001352
Iteration 114/1000 | Loss: 0.00001351
Iteration 115/1000 | Loss: 0.00001351
Iteration 116/1000 | Loss: 0.00001350
Iteration 117/1000 | Loss: 0.00001350
Iteration 118/1000 | Loss: 0.00001350
Iteration 119/1000 | Loss: 0.00001350
Iteration 120/1000 | Loss: 0.00001350
Iteration 121/1000 | Loss: 0.00001350
Iteration 122/1000 | Loss: 0.00001350
Iteration 123/1000 | Loss: 0.00001350
Iteration 124/1000 | Loss: 0.00001350
Iteration 125/1000 | Loss: 0.00001350
Iteration 126/1000 | Loss: 0.00001350
Iteration 127/1000 | Loss: 0.00001350
Iteration 128/1000 | Loss: 0.00001350
Iteration 129/1000 | Loss: 0.00001350
Iteration 130/1000 | Loss: 0.00001350
Iteration 131/1000 | Loss: 0.00001350
Iteration 132/1000 | Loss: 0.00001350
Iteration 133/1000 | Loss: 0.00001350
Iteration 134/1000 | Loss: 0.00001350
Iteration 135/1000 | Loss: 0.00001350
Iteration 136/1000 | Loss: 0.00001350
Iteration 137/1000 | Loss: 0.00001350
Iteration 138/1000 | Loss: 0.00001350
Iteration 139/1000 | Loss: 0.00001350
Iteration 140/1000 | Loss: 0.00001350
Iteration 141/1000 | Loss: 0.00001350
Iteration 142/1000 | Loss: 0.00001350
Iteration 143/1000 | Loss: 0.00001350
Iteration 144/1000 | Loss: 0.00001350
Iteration 145/1000 | Loss: 0.00001350
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [1.3495094208337832e-05, 1.3495094208337832e-05, 1.3495094208337832e-05, 1.3495094208337832e-05, 1.3495094208337832e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3495094208337832e-05

Optimization complete. Final v2v error: 3.0770723819732666 mm

Highest mean error: 3.717811346054077 mm for frame 81

Lowest mean error: 2.437023401260376 mm for frame 3

Saving results

Total time: 149.6340618133545
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00504436
Iteration 2/25 | Loss: 0.00128603
Iteration 3/25 | Loss: 0.00118332
Iteration 4/25 | Loss: 0.00116942
Iteration 5/25 | Loss: 0.00116587
Iteration 6/25 | Loss: 0.00116571
Iteration 7/25 | Loss: 0.00116571
Iteration 8/25 | Loss: 0.00116571
Iteration 9/25 | Loss: 0.00116571
Iteration 10/25 | Loss: 0.00116571
Iteration 11/25 | Loss: 0.00116571
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011657061986625195, 0.0011657061986625195, 0.0011657061986625195, 0.0011657061986625195, 0.0011657061986625195]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011657061986625195

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.06695080
Iteration 2/25 | Loss: 0.00190434
Iteration 3/25 | Loss: 0.00190431
Iteration 4/25 | Loss: 0.00190431
Iteration 5/25 | Loss: 0.00190431
Iteration 6/25 | Loss: 0.00190431
Iteration 7/25 | Loss: 0.00190431
Iteration 8/25 | Loss: 0.00190431
Iteration 9/25 | Loss: 0.00190431
Iteration 10/25 | Loss: 0.00190431
Iteration 11/25 | Loss: 0.00190431
Iteration 12/25 | Loss: 0.00190431
Iteration 13/25 | Loss: 0.00190431
Iteration 14/25 | Loss: 0.00190431
Iteration 15/25 | Loss: 0.00190431
Iteration 16/25 | Loss: 0.00190431
Iteration 17/25 | Loss: 0.00190431
Iteration 18/25 | Loss: 0.00190431
Iteration 19/25 | Loss: 0.00190431
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0019043058855459094, 0.0019043058855459094, 0.0019043058855459094, 0.0019043058855459094, 0.0019043058855459094]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019043058855459094

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00190431
Iteration 2/1000 | Loss: 0.00002375
Iteration 3/1000 | Loss: 0.00001719
Iteration 4/1000 | Loss: 0.00001554
Iteration 5/1000 | Loss: 0.00001455
Iteration 6/1000 | Loss: 0.00001397
Iteration 7/1000 | Loss: 0.00001348
Iteration 8/1000 | Loss: 0.00001307
Iteration 9/1000 | Loss: 0.00001271
Iteration 10/1000 | Loss: 0.00001252
Iteration 11/1000 | Loss: 0.00001225
Iteration 12/1000 | Loss: 0.00001207
Iteration 13/1000 | Loss: 0.00001199
Iteration 14/1000 | Loss: 0.00001191
Iteration 15/1000 | Loss: 0.00001186
Iteration 16/1000 | Loss: 0.00001186
Iteration 17/1000 | Loss: 0.00001183
Iteration 18/1000 | Loss: 0.00001181
Iteration 19/1000 | Loss: 0.00001180
Iteration 20/1000 | Loss: 0.00001178
Iteration 21/1000 | Loss: 0.00001177
Iteration 22/1000 | Loss: 0.00001175
Iteration 23/1000 | Loss: 0.00001171
Iteration 24/1000 | Loss: 0.00001171
Iteration 25/1000 | Loss: 0.00001171
Iteration 26/1000 | Loss: 0.00001170
Iteration 27/1000 | Loss: 0.00001170
Iteration 28/1000 | Loss: 0.00001169
Iteration 29/1000 | Loss: 0.00001167
Iteration 30/1000 | Loss: 0.00001167
Iteration 31/1000 | Loss: 0.00001166
Iteration 32/1000 | Loss: 0.00001166
Iteration 33/1000 | Loss: 0.00001166
Iteration 34/1000 | Loss: 0.00001165
Iteration 35/1000 | Loss: 0.00001163
Iteration 36/1000 | Loss: 0.00001161
Iteration 37/1000 | Loss: 0.00001161
Iteration 38/1000 | Loss: 0.00001161
Iteration 39/1000 | Loss: 0.00001161
Iteration 40/1000 | Loss: 0.00001160
Iteration 41/1000 | Loss: 0.00001159
Iteration 42/1000 | Loss: 0.00001159
Iteration 43/1000 | Loss: 0.00001158
Iteration 44/1000 | Loss: 0.00001158
Iteration 45/1000 | Loss: 0.00001158
Iteration 46/1000 | Loss: 0.00001158
Iteration 47/1000 | Loss: 0.00001158
Iteration 48/1000 | Loss: 0.00001157
Iteration 49/1000 | Loss: 0.00001157
Iteration 50/1000 | Loss: 0.00001157
Iteration 51/1000 | Loss: 0.00001157
Iteration 52/1000 | Loss: 0.00001157
Iteration 53/1000 | Loss: 0.00001157
Iteration 54/1000 | Loss: 0.00001157
Iteration 55/1000 | Loss: 0.00001157
Iteration 56/1000 | Loss: 0.00001156
Iteration 57/1000 | Loss: 0.00001156
Iteration 58/1000 | Loss: 0.00001155
Iteration 59/1000 | Loss: 0.00001155
Iteration 60/1000 | Loss: 0.00001155
Iteration 61/1000 | Loss: 0.00001155
Iteration 62/1000 | Loss: 0.00001155
Iteration 63/1000 | Loss: 0.00001154
Iteration 64/1000 | Loss: 0.00001154
Iteration 65/1000 | Loss: 0.00001154
Iteration 66/1000 | Loss: 0.00001154
Iteration 67/1000 | Loss: 0.00001154
Iteration 68/1000 | Loss: 0.00001154
Iteration 69/1000 | Loss: 0.00001153
Iteration 70/1000 | Loss: 0.00001153
Iteration 71/1000 | Loss: 0.00001153
Iteration 72/1000 | Loss: 0.00001153
Iteration 73/1000 | Loss: 0.00001152
Iteration 74/1000 | Loss: 0.00001152
Iteration 75/1000 | Loss: 0.00001151
Iteration 76/1000 | Loss: 0.00001151
Iteration 77/1000 | Loss: 0.00001151
Iteration 78/1000 | Loss: 0.00001151
Iteration 79/1000 | Loss: 0.00001150
Iteration 80/1000 | Loss: 0.00001150
Iteration 81/1000 | Loss: 0.00001150
Iteration 82/1000 | Loss: 0.00001150
Iteration 83/1000 | Loss: 0.00001150
Iteration 84/1000 | Loss: 0.00001149
Iteration 85/1000 | Loss: 0.00001149
Iteration 86/1000 | Loss: 0.00001149
Iteration 87/1000 | Loss: 0.00001149
Iteration 88/1000 | Loss: 0.00001149
Iteration 89/1000 | Loss: 0.00001149
Iteration 90/1000 | Loss: 0.00001149
Iteration 91/1000 | Loss: 0.00001149
Iteration 92/1000 | Loss: 0.00001149
Iteration 93/1000 | Loss: 0.00001149
Iteration 94/1000 | Loss: 0.00001148
Iteration 95/1000 | Loss: 0.00001148
Iteration 96/1000 | Loss: 0.00001148
Iteration 97/1000 | Loss: 0.00001147
Iteration 98/1000 | Loss: 0.00001147
Iteration 99/1000 | Loss: 0.00001147
Iteration 100/1000 | Loss: 0.00001147
Iteration 101/1000 | Loss: 0.00001147
Iteration 102/1000 | Loss: 0.00001147
Iteration 103/1000 | Loss: 0.00001147
Iteration 104/1000 | Loss: 0.00001146
Iteration 105/1000 | Loss: 0.00001146
Iteration 106/1000 | Loss: 0.00001146
Iteration 107/1000 | Loss: 0.00001146
Iteration 108/1000 | Loss: 0.00001146
Iteration 109/1000 | Loss: 0.00001146
Iteration 110/1000 | Loss: 0.00001146
Iteration 111/1000 | Loss: 0.00001146
Iteration 112/1000 | Loss: 0.00001146
Iteration 113/1000 | Loss: 0.00001145
Iteration 114/1000 | Loss: 0.00001145
Iteration 115/1000 | Loss: 0.00001145
Iteration 116/1000 | Loss: 0.00001145
Iteration 117/1000 | Loss: 0.00001145
Iteration 118/1000 | Loss: 0.00001145
Iteration 119/1000 | Loss: 0.00001145
Iteration 120/1000 | Loss: 0.00001145
Iteration 121/1000 | Loss: 0.00001144
Iteration 122/1000 | Loss: 0.00001144
Iteration 123/1000 | Loss: 0.00001144
Iteration 124/1000 | Loss: 0.00001144
Iteration 125/1000 | Loss: 0.00001144
Iteration 126/1000 | Loss: 0.00001143
Iteration 127/1000 | Loss: 0.00001143
Iteration 128/1000 | Loss: 0.00001143
Iteration 129/1000 | Loss: 0.00001143
Iteration 130/1000 | Loss: 0.00001143
Iteration 131/1000 | Loss: 0.00001143
Iteration 132/1000 | Loss: 0.00001143
Iteration 133/1000 | Loss: 0.00001143
Iteration 134/1000 | Loss: 0.00001143
Iteration 135/1000 | Loss: 0.00001143
Iteration 136/1000 | Loss: 0.00001143
Iteration 137/1000 | Loss: 0.00001142
Iteration 138/1000 | Loss: 0.00001142
Iteration 139/1000 | Loss: 0.00001142
Iteration 140/1000 | Loss: 0.00001142
Iteration 141/1000 | Loss: 0.00001142
Iteration 142/1000 | Loss: 0.00001142
Iteration 143/1000 | Loss: 0.00001142
Iteration 144/1000 | Loss: 0.00001142
Iteration 145/1000 | Loss: 0.00001142
Iteration 146/1000 | Loss: 0.00001142
Iteration 147/1000 | Loss: 0.00001142
Iteration 148/1000 | Loss: 0.00001142
Iteration 149/1000 | Loss: 0.00001142
Iteration 150/1000 | Loss: 0.00001142
Iteration 151/1000 | Loss: 0.00001142
Iteration 152/1000 | Loss: 0.00001141
Iteration 153/1000 | Loss: 0.00001141
Iteration 154/1000 | Loss: 0.00001141
Iteration 155/1000 | Loss: 0.00001141
Iteration 156/1000 | Loss: 0.00001141
Iteration 157/1000 | Loss: 0.00001141
Iteration 158/1000 | Loss: 0.00001141
Iteration 159/1000 | Loss: 0.00001141
Iteration 160/1000 | Loss: 0.00001141
Iteration 161/1000 | Loss: 0.00001141
Iteration 162/1000 | Loss: 0.00001141
Iteration 163/1000 | Loss: 0.00001141
Iteration 164/1000 | Loss: 0.00001141
Iteration 165/1000 | Loss: 0.00001141
Iteration 166/1000 | Loss: 0.00001141
Iteration 167/1000 | Loss: 0.00001141
Iteration 168/1000 | Loss: 0.00001141
Iteration 169/1000 | Loss: 0.00001141
Iteration 170/1000 | Loss: 0.00001141
Iteration 171/1000 | Loss: 0.00001141
Iteration 172/1000 | Loss: 0.00001141
Iteration 173/1000 | Loss: 0.00001141
Iteration 174/1000 | Loss: 0.00001141
Iteration 175/1000 | Loss: 0.00001141
Iteration 176/1000 | Loss: 0.00001141
Iteration 177/1000 | Loss: 0.00001141
Iteration 178/1000 | Loss: 0.00001141
Iteration 179/1000 | Loss: 0.00001141
Iteration 180/1000 | Loss: 0.00001141
Iteration 181/1000 | Loss: 0.00001141
Iteration 182/1000 | Loss: 0.00001141
Iteration 183/1000 | Loss: 0.00001141
Iteration 184/1000 | Loss: 0.00001141
Iteration 185/1000 | Loss: 0.00001141
Iteration 186/1000 | Loss: 0.00001141
Iteration 187/1000 | Loss: 0.00001141
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.1408919817768037e-05, 1.1408919817768037e-05, 1.1408919817768037e-05, 1.1408919817768037e-05, 1.1408919817768037e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1408919817768037e-05

Optimization complete. Final v2v error: 2.8971545696258545 mm

Highest mean error: 3.290966272354126 mm for frame 155

Lowest mean error: 2.386988401412964 mm for frame 32

Saving results

Total time: 41.45700240135193
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00851445
Iteration 2/25 | Loss: 0.00189413
Iteration 3/25 | Loss: 0.00140131
Iteration 4/25 | Loss: 0.00133104
Iteration 5/25 | Loss: 0.00131433
Iteration 6/25 | Loss: 0.00133744
Iteration 7/25 | Loss: 0.00131770
Iteration 8/25 | Loss: 0.00129163
Iteration 9/25 | Loss: 0.00128814
Iteration 10/25 | Loss: 0.00129456
Iteration 11/25 | Loss: 0.00128326
Iteration 12/25 | Loss: 0.00127199
Iteration 13/25 | Loss: 0.00127266
Iteration 14/25 | Loss: 0.00126756
Iteration 15/25 | Loss: 0.00126553
Iteration 16/25 | Loss: 0.00126523
Iteration 17/25 | Loss: 0.00126494
Iteration 18/25 | Loss: 0.00126407
Iteration 19/25 | Loss: 0.00126405
Iteration 20/25 | Loss: 0.00126404
Iteration 21/25 | Loss: 0.00126404
Iteration 22/25 | Loss: 0.00126404
Iteration 23/25 | Loss: 0.00126404
Iteration 24/25 | Loss: 0.00126404
Iteration 25/25 | Loss: 0.00126402

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.32988834
Iteration 2/25 | Loss: 0.00123706
Iteration 3/25 | Loss: 0.00123706
Iteration 4/25 | Loss: 0.00123706
Iteration 5/25 | Loss: 0.00123706
Iteration 6/25 | Loss: 0.00123706
Iteration 7/25 | Loss: 0.00123706
Iteration 8/25 | Loss: 0.00123706
Iteration 9/25 | Loss: 0.00123706
Iteration 10/25 | Loss: 0.00123705
Iteration 11/25 | Loss: 0.00123705
Iteration 12/25 | Loss: 0.00123705
Iteration 13/25 | Loss: 0.00123705
Iteration 14/25 | Loss: 0.00123705
Iteration 15/25 | Loss: 0.00123705
Iteration 16/25 | Loss: 0.00123705
Iteration 17/25 | Loss: 0.00123705
Iteration 18/25 | Loss: 0.00123705
Iteration 19/25 | Loss: 0.00123705
Iteration 20/25 | Loss: 0.00123705
Iteration 21/25 | Loss: 0.00123705
Iteration 22/25 | Loss: 0.00123705
Iteration 23/25 | Loss: 0.00123705
Iteration 24/25 | Loss: 0.00123705
Iteration 25/25 | Loss: 0.00123705

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123705
Iteration 2/1000 | Loss: 0.00003381
Iteration 3/1000 | Loss: 0.00002708
Iteration 4/1000 | Loss: 0.00002514
Iteration 5/1000 | Loss: 0.00002427
Iteration 6/1000 | Loss: 0.00002356
Iteration 7/1000 | Loss: 0.00002304
Iteration 8/1000 | Loss: 0.00002266
Iteration 9/1000 | Loss: 0.00002245
Iteration 10/1000 | Loss: 0.00002222
Iteration 11/1000 | Loss: 0.00002212
Iteration 12/1000 | Loss: 0.00002202
Iteration 13/1000 | Loss: 0.00002197
Iteration 14/1000 | Loss: 0.00002185
Iteration 15/1000 | Loss: 0.00002182
Iteration 16/1000 | Loss: 0.00002179
Iteration 17/1000 | Loss: 0.00002178
Iteration 18/1000 | Loss: 0.00002173
Iteration 19/1000 | Loss: 0.00002173
Iteration 20/1000 | Loss: 0.00002173
Iteration 21/1000 | Loss: 0.00002172
Iteration 22/1000 | Loss: 0.00002172
Iteration 23/1000 | Loss: 0.00002172
Iteration 24/1000 | Loss: 0.00002172
Iteration 25/1000 | Loss: 0.00002172
Iteration 26/1000 | Loss: 0.00002172
Iteration 27/1000 | Loss: 0.00002167
Iteration 28/1000 | Loss: 0.00002167
Iteration 29/1000 | Loss: 0.00002159
Iteration 30/1000 | Loss: 0.00002159
Iteration 31/1000 | Loss: 0.00002158
Iteration 32/1000 | Loss: 0.00002157
Iteration 33/1000 | Loss: 0.00002156
Iteration 34/1000 | Loss: 0.00002154
Iteration 35/1000 | Loss: 0.00002154
Iteration 36/1000 | Loss: 0.00002152
Iteration 37/1000 | Loss: 0.00002152
Iteration 38/1000 | Loss: 0.00002151
Iteration 39/1000 | Loss: 0.00002151
Iteration 40/1000 | Loss: 0.00002149
Iteration 41/1000 | Loss: 0.00002148
Iteration 42/1000 | Loss: 0.00002148
Iteration 43/1000 | Loss: 0.00002147
Iteration 44/1000 | Loss: 0.00002147
Iteration 45/1000 | Loss: 0.00002147
Iteration 46/1000 | Loss: 0.00002147
Iteration 47/1000 | Loss: 0.00002147
Iteration 48/1000 | Loss: 0.00002146
Iteration 49/1000 | Loss: 0.00002146
Iteration 50/1000 | Loss: 0.00002146
Iteration 51/1000 | Loss: 0.00002146
Iteration 52/1000 | Loss: 0.00002145
Iteration 53/1000 | Loss: 0.00002144
Iteration 54/1000 | Loss: 0.00002143
Iteration 55/1000 | Loss: 0.00002143
Iteration 56/1000 | Loss: 0.00002142
Iteration 57/1000 | Loss: 0.00002141
Iteration 58/1000 | Loss: 0.00013531
Iteration 59/1000 | Loss: 0.00002837
Iteration 60/1000 | Loss: 0.00006377
Iteration 61/1000 | Loss: 0.00011909
Iteration 62/1000 | Loss: 0.00006956
Iteration 63/1000 | Loss: 0.00002149
Iteration 64/1000 | Loss: 0.00012036
Iteration 65/1000 | Loss: 0.00005430
Iteration 66/1000 | Loss: 0.00011157
Iteration 67/1000 | Loss: 0.00005894
Iteration 68/1000 | Loss: 0.00010471
Iteration 69/1000 | Loss: 0.00005659
Iteration 70/1000 | Loss: 0.00009264
Iteration 71/1000 | Loss: 0.00007064
Iteration 72/1000 | Loss: 0.00007643
Iteration 73/1000 | Loss: 0.00002206
Iteration 74/1000 | Loss: 0.00002164
Iteration 75/1000 | Loss: 0.00002123
Iteration 76/1000 | Loss: 0.00002113
Iteration 77/1000 | Loss: 0.00002097
Iteration 78/1000 | Loss: 0.00002094
Iteration 79/1000 | Loss: 0.00002094
Iteration 80/1000 | Loss: 0.00002087
Iteration 81/1000 | Loss: 0.00002087
Iteration 82/1000 | Loss: 0.00002085
Iteration 83/1000 | Loss: 0.00002085
Iteration 84/1000 | Loss: 0.00002085
Iteration 85/1000 | Loss: 0.00002085
Iteration 86/1000 | Loss: 0.00002084
Iteration 87/1000 | Loss: 0.00002084
Iteration 88/1000 | Loss: 0.00002084
Iteration 89/1000 | Loss: 0.00002084
Iteration 90/1000 | Loss: 0.00002084
Iteration 91/1000 | Loss: 0.00002084
Iteration 92/1000 | Loss: 0.00002084
Iteration 93/1000 | Loss: 0.00002083
Iteration 94/1000 | Loss: 0.00002083
Iteration 95/1000 | Loss: 0.00002083
Iteration 96/1000 | Loss: 0.00002083
Iteration 97/1000 | Loss: 0.00002083
Iteration 98/1000 | Loss: 0.00002083
Iteration 99/1000 | Loss: 0.00002083
Iteration 100/1000 | Loss: 0.00002082
Iteration 101/1000 | Loss: 0.00002082
Iteration 102/1000 | Loss: 0.00002082
Iteration 103/1000 | Loss: 0.00002081
Iteration 104/1000 | Loss: 0.00002081
Iteration 105/1000 | Loss: 0.00002081
Iteration 106/1000 | Loss: 0.00002081
Iteration 107/1000 | Loss: 0.00002081
Iteration 108/1000 | Loss: 0.00002080
Iteration 109/1000 | Loss: 0.00002080
Iteration 110/1000 | Loss: 0.00002080
Iteration 111/1000 | Loss: 0.00002079
Iteration 112/1000 | Loss: 0.00002079
Iteration 113/1000 | Loss: 0.00002079
Iteration 114/1000 | Loss: 0.00002079
Iteration 115/1000 | Loss: 0.00002079
Iteration 116/1000 | Loss: 0.00002078
Iteration 117/1000 | Loss: 0.00002078
Iteration 118/1000 | Loss: 0.00002078
Iteration 119/1000 | Loss: 0.00002077
Iteration 120/1000 | Loss: 0.00002077
Iteration 121/1000 | Loss: 0.00002077
Iteration 122/1000 | Loss: 0.00002077
Iteration 123/1000 | Loss: 0.00002077
Iteration 124/1000 | Loss: 0.00002077
Iteration 125/1000 | Loss: 0.00002077
Iteration 126/1000 | Loss: 0.00002076
Iteration 127/1000 | Loss: 0.00002076
Iteration 128/1000 | Loss: 0.00002076
Iteration 129/1000 | Loss: 0.00002076
Iteration 130/1000 | Loss: 0.00002076
Iteration 131/1000 | Loss: 0.00002076
Iteration 132/1000 | Loss: 0.00002076
Iteration 133/1000 | Loss: 0.00002076
Iteration 134/1000 | Loss: 0.00002076
Iteration 135/1000 | Loss: 0.00002076
Iteration 136/1000 | Loss: 0.00002075
Iteration 137/1000 | Loss: 0.00002075
Iteration 138/1000 | Loss: 0.00002075
Iteration 139/1000 | Loss: 0.00002075
Iteration 140/1000 | Loss: 0.00002074
Iteration 141/1000 | Loss: 0.00002074
Iteration 142/1000 | Loss: 0.00002074
Iteration 143/1000 | Loss: 0.00002074
Iteration 144/1000 | Loss: 0.00002074
Iteration 145/1000 | Loss: 0.00002074
Iteration 146/1000 | Loss: 0.00002074
Iteration 147/1000 | Loss: 0.00002073
Iteration 148/1000 | Loss: 0.00002073
Iteration 149/1000 | Loss: 0.00002073
Iteration 150/1000 | Loss: 0.00002073
Iteration 151/1000 | Loss: 0.00002073
Iteration 152/1000 | Loss: 0.00002073
Iteration 153/1000 | Loss: 0.00002073
Iteration 154/1000 | Loss: 0.00002073
Iteration 155/1000 | Loss: 0.00002073
Iteration 156/1000 | Loss: 0.00002072
Iteration 157/1000 | Loss: 0.00002072
Iteration 158/1000 | Loss: 0.00002072
Iteration 159/1000 | Loss: 0.00002072
Iteration 160/1000 | Loss: 0.00002072
Iteration 161/1000 | Loss: 0.00002072
Iteration 162/1000 | Loss: 0.00002071
Iteration 163/1000 | Loss: 0.00002071
Iteration 164/1000 | Loss: 0.00002070
Iteration 165/1000 | Loss: 0.00002070
Iteration 166/1000 | Loss: 0.00002069
Iteration 167/1000 | Loss: 0.00002069
Iteration 168/1000 | Loss: 0.00002069
Iteration 169/1000 | Loss: 0.00002068
Iteration 170/1000 | Loss: 0.00002068
Iteration 171/1000 | Loss: 0.00002068
Iteration 172/1000 | Loss: 0.00002068
Iteration 173/1000 | Loss: 0.00002068
Iteration 174/1000 | Loss: 0.00002068
Iteration 175/1000 | Loss: 0.00002067
Iteration 176/1000 | Loss: 0.00002067
Iteration 177/1000 | Loss: 0.00002067
Iteration 178/1000 | Loss: 0.00002066
Iteration 179/1000 | Loss: 0.00002066
Iteration 180/1000 | Loss: 0.00002066
Iteration 181/1000 | Loss: 0.00002065
Iteration 182/1000 | Loss: 0.00002065
Iteration 183/1000 | Loss: 0.00002065
Iteration 184/1000 | Loss: 0.00002065
Iteration 185/1000 | Loss: 0.00002064
Iteration 186/1000 | Loss: 0.00002064
Iteration 187/1000 | Loss: 0.00002064
Iteration 188/1000 | Loss: 0.00002064
Iteration 189/1000 | Loss: 0.00002064
Iteration 190/1000 | Loss: 0.00002064
Iteration 191/1000 | Loss: 0.00002064
Iteration 192/1000 | Loss: 0.00002064
Iteration 193/1000 | Loss: 0.00002063
Iteration 194/1000 | Loss: 0.00002063
Iteration 195/1000 | Loss: 0.00002063
Iteration 196/1000 | Loss: 0.00002062
Iteration 197/1000 | Loss: 0.00002062
Iteration 198/1000 | Loss: 0.00002062
Iteration 199/1000 | Loss: 0.00002062
Iteration 200/1000 | Loss: 0.00002062
Iteration 201/1000 | Loss: 0.00002061
Iteration 202/1000 | Loss: 0.00002061
Iteration 203/1000 | Loss: 0.00002061
Iteration 204/1000 | Loss: 0.00002061
Iteration 205/1000 | Loss: 0.00002061
Iteration 206/1000 | Loss: 0.00002061
Iteration 207/1000 | Loss: 0.00002061
Iteration 208/1000 | Loss: 0.00002061
Iteration 209/1000 | Loss: 0.00002061
Iteration 210/1000 | Loss: 0.00002060
Iteration 211/1000 | Loss: 0.00002060
Iteration 212/1000 | Loss: 0.00002059
Iteration 213/1000 | Loss: 0.00002059
Iteration 214/1000 | Loss: 0.00002059
Iteration 215/1000 | Loss: 0.00002058
Iteration 216/1000 | Loss: 0.00002956
Iteration 217/1000 | Loss: 0.00002330
Iteration 218/1000 | Loss: 0.00002164
Iteration 219/1000 | Loss: 0.00002253
Iteration 220/1000 | Loss: 0.00002774
Iteration 221/1000 | Loss: 0.00002211
Iteration 222/1000 | Loss: 0.00002709
Iteration 223/1000 | Loss: 0.00002142
Iteration 224/1000 | Loss: 0.00002501
Iteration 225/1000 | Loss: 0.00002136
Iteration 226/1000 | Loss: 0.00002495
Iteration 227/1000 | Loss: 0.00002107
Iteration 228/1000 | Loss: 0.00002144
Iteration 229/1000 | Loss: 0.00002060
Iteration 230/1000 | Loss: 0.00002050
Iteration 231/1000 | Loss: 0.00002050
Iteration 232/1000 | Loss: 0.00002044
Iteration 233/1000 | Loss: 0.00002044
Iteration 234/1000 | Loss: 0.00002043
Iteration 235/1000 | Loss: 0.00002039
Iteration 236/1000 | Loss: 0.00002036
Iteration 237/1000 | Loss: 0.00002035
Iteration 238/1000 | Loss: 0.00002035
Iteration 239/1000 | Loss: 0.00002035
Iteration 240/1000 | Loss: 0.00002034
Iteration 241/1000 | Loss: 0.00002034
Iteration 242/1000 | Loss: 0.00002034
Iteration 243/1000 | Loss: 0.00002033
Iteration 244/1000 | Loss: 0.00002033
Iteration 245/1000 | Loss: 0.00002033
Iteration 246/1000 | Loss: 0.00002033
Iteration 247/1000 | Loss: 0.00002033
Iteration 248/1000 | Loss: 0.00002033
Iteration 249/1000 | Loss: 0.00002032
Iteration 250/1000 | Loss: 0.00002032
Iteration 251/1000 | Loss: 0.00002032
Iteration 252/1000 | Loss: 0.00002031
Iteration 253/1000 | Loss: 0.00002031
Iteration 254/1000 | Loss: 0.00002031
Iteration 255/1000 | Loss: 0.00002031
Iteration 256/1000 | Loss: 0.00002031
Iteration 257/1000 | Loss: 0.00002031
Iteration 258/1000 | Loss: 0.00002031
Iteration 259/1000 | Loss: 0.00002031
Iteration 260/1000 | Loss: 0.00002031
Iteration 261/1000 | Loss: 0.00002030
Iteration 262/1000 | Loss: 0.00002030
Iteration 263/1000 | Loss: 0.00002030
Iteration 264/1000 | Loss: 0.00002030
Iteration 265/1000 | Loss: 0.00002030
Iteration 266/1000 | Loss: 0.00002030
Iteration 267/1000 | Loss: 0.00002030
Iteration 268/1000 | Loss: 0.00002030
Iteration 269/1000 | Loss: 0.00002030
Iteration 270/1000 | Loss: 0.00002029
Iteration 271/1000 | Loss: 0.00002029
Iteration 272/1000 | Loss: 0.00002029
Iteration 273/1000 | Loss: 0.00002029
Iteration 274/1000 | Loss: 0.00002029
Iteration 275/1000 | Loss: 0.00002029
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 275. Stopping optimization.
Last 5 losses: [2.0294821297284216e-05, 2.0294821297284216e-05, 2.0294821297284216e-05, 2.0294821297284216e-05, 2.0294821297284216e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0294821297284216e-05

Optimization complete. Final v2v error: 3.69720196723938 mm

Highest mean error: 6.157230377197266 mm for frame 12

Lowest mean error: 3.190338134765625 mm for frame 204

Saving results

Total time: 132.3670892715454
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00444103
Iteration 2/25 | Loss: 0.00117618
Iteration 3/25 | Loss: 0.00112562
Iteration 4/25 | Loss: 0.00112181
Iteration 5/25 | Loss: 0.00112147
Iteration 6/25 | Loss: 0.00112147
Iteration 7/25 | Loss: 0.00112147
Iteration 8/25 | Loss: 0.00112147
Iteration 9/25 | Loss: 0.00112147
Iteration 10/25 | Loss: 0.00112147
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011214683763682842, 0.0011214683763682842, 0.0011214683763682842, 0.0011214683763682842, 0.0011214683763682842]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011214683763682842

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.84104800
Iteration 2/25 | Loss: 0.00154404
Iteration 3/25 | Loss: 0.00154401
Iteration 4/25 | Loss: 0.00154401
Iteration 5/25 | Loss: 0.00154401
Iteration 6/25 | Loss: 0.00154401
Iteration 7/25 | Loss: 0.00154401
Iteration 8/25 | Loss: 0.00154401
Iteration 9/25 | Loss: 0.00154401
Iteration 10/25 | Loss: 0.00154401
Iteration 11/25 | Loss: 0.00154401
Iteration 12/25 | Loss: 0.00154401
Iteration 13/25 | Loss: 0.00154401
Iteration 14/25 | Loss: 0.00154401
Iteration 15/25 | Loss: 0.00154401
Iteration 16/25 | Loss: 0.00154401
Iteration 17/25 | Loss: 0.00154401
Iteration 18/25 | Loss: 0.00154401
Iteration 19/25 | Loss: 0.00154401
Iteration 20/25 | Loss: 0.00154401
Iteration 21/25 | Loss: 0.00154401
Iteration 22/25 | Loss: 0.00154401
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0015440053539350629, 0.0015440053539350629, 0.0015440053539350629, 0.0015440053539350629, 0.0015440053539350629]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015440053539350629

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00154401
Iteration 2/1000 | Loss: 0.00001704
Iteration 3/1000 | Loss: 0.00001339
Iteration 4/1000 | Loss: 0.00001166
Iteration 5/1000 | Loss: 0.00001064
Iteration 6/1000 | Loss: 0.00001018
Iteration 7/1000 | Loss: 0.00000978
Iteration 8/1000 | Loss: 0.00000943
Iteration 9/1000 | Loss: 0.00000925
Iteration 10/1000 | Loss: 0.00000900
Iteration 11/1000 | Loss: 0.00000879
Iteration 12/1000 | Loss: 0.00000862
Iteration 13/1000 | Loss: 0.00000854
Iteration 14/1000 | Loss: 0.00000854
Iteration 15/1000 | Loss: 0.00000839
Iteration 16/1000 | Loss: 0.00000833
Iteration 17/1000 | Loss: 0.00000820
Iteration 18/1000 | Loss: 0.00000820
Iteration 19/1000 | Loss: 0.00000819
Iteration 20/1000 | Loss: 0.00000819
Iteration 21/1000 | Loss: 0.00000818
Iteration 22/1000 | Loss: 0.00000818
Iteration 23/1000 | Loss: 0.00000818
Iteration 24/1000 | Loss: 0.00000817
Iteration 25/1000 | Loss: 0.00000817
Iteration 26/1000 | Loss: 0.00000816
Iteration 27/1000 | Loss: 0.00000816
Iteration 28/1000 | Loss: 0.00000816
Iteration 29/1000 | Loss: 0.00000815
Iteration 30/1000 | Loss: 0.00000815
Iteration 31/1000 | Loss: 0.00000811
Iteration 32/1000 | Loss: 0.00000811
Iteration 33/1000 | Loss: 0.00000809
Iteration 34/1000 | Loss: 0.00000809
Iteration 35/1000 | Loss: 0.00000809
Iteration 36/1000 | Loss: 0.00000804
Iteration 37/1000 | Loss: 0.00000802
Iteration 38/1000 | Loss: 0.00000801
Iteration 39/1000 | Loss: 0.00000796
Iteration 40/1000 | Loss: 0.00000795
Iteration 41/1000 | Loss: 0.00000795
Iteration 42/1000 | Loss: 0.00000790
Iteration 43/1000 | Loss: 0.00000789
Iteration 44/1000 | Loss: 0.00000784
Iteration 45/1000 | Loss: 0.00000784
Iteration 46/1000 | Loss: 0.00000783
Iteration 47/1000 | Loss: 0.00000783
Iteration 48/1000 | Loss: 0.00000783
Iteration 49/1000 | Loss: 0.00000783
Iteration 50/1000 | Loss: 0.00000783
Iteration 51/1000 | Loss: 0.00000783
Iteration 52/1000 | Loss: 0.00000783
Iteration 53/1000 | Loss: 0.00000783
Iteration 54/1000 | Loss: 0.00000782
Iteration 55/1000 | Loss: 0.00000782
Iteration 56/1000 | Loss: 0.00000782
Iteration 57/1000 | Loss: 0.00000781
Iteration 58/1000 | Loss: 0.00000781
Iteration 59/1000 | Loss: 0.00000780
Iteration 60/1000 | Loss: 0.00000780
Iteration 61/1000 | Loss: 0.00000780
Iteration 62/1000 | Loss: 0.00000780
Iteration 63/1000 | Loss: 0.00000780
Iteration 64/1000 | Loss: 0.00000780
Iteration 65/1000 | Loss: 0.00000780
Iteration 66/1000 | Loss: 0.00000780
Iteration 67/1000 | Loss: 0.00000780
Iteration 68/1000 | Loss: 0.00000780
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 68. Stopping optimization.
Last 5 losses: [7.796687896188814e-06, 7.796687896188814e-06, 7.796687896188814e-06, 7.796687896188814e-06, 7.796687896188814e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.796687896188814e-06

Optimization complete. Final v2v error: 2.4831480979919434 mm

Highest mean error: 2.821394920349121 mm for frame 143

Lowest mean error: 2.3704469203948975 mm for frame 21

Saving results

Total time: 38.1493923664093
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00932365
Iteration 2/25 | Loss: 0.00932365
Iteration 3/25 | Loss: 0.00932365
Iteration 4/25 | Loss: 0.00363019
Iteration 5/25 | Loss: 0.00308517
Iteration 6/25 | Loss: 0.00254168
Iteration 7/25 | Loss: 0.00221989
Iteration 8/25 | Loss: 0.00207118
Iteration 9/25 | Loss: 0.00204755
Iteration 10/25 | Loss: 0.00200496
Iteration 11/25 | Loss: 0.00202465
Iteration 12/25 | Loss: 0.00199294
Iteration 13/25 | Loss: 0.00166070
Iteration 14/25 | Loss: 0.00143540
Iteration 15/25 | Loss: 0.00130526
Iteration 16/25 | Loss: 0.00126762
Iteration 17/25 | Loss: 0.00124963
Iteration 18/25 | Loss: 0.00125357
Iteration 19/25 | Loss: 0.00124542
Iteration 20/25 | Loss: 0.00124524
Iteration 21/25 | Loss: 0.00124514
Iteration 22/25 | Loss: 0.00124511
Iteration 23/25 | Loss: 0.00124511
Iteration 24/25 | Loss: 0.00124511
Iteration 25/25 | Loss: 0.00124511

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20656884
Iteration 2/25 | Loss: 0.00233870
Iteration 3/25 | Loss: 0.00138857
Iteration 4/25 | Loss: 0.00138856
Iteration 5/25 | Loss: 0.00138856
Iteration 6/25 | Loss: 0.00138855
Iteration 7/25 | Loss: 0.00138855
Iteration 8/25 | Loss: 0.00138855
Iteration 9/25 | Loss: 0.00138855
Iteration 10/25 | Loss: 0.00138855
Iteration 11/25 | Loss: 0.00138855
Iteration 12/25 | Loss: 0.00138855
Iteration 13/25 | Loss: 0.00138855
Iteration 14/25 | Loss: 0.00138855
Iteration 15/25 | Loss: 0.00138855
Iteration 16/25 | Loss: 0.00138855
Iteration 17/25 | Loss: 0.00138855
Iteration 18/25 | Loss: 0.00138855
Iteration 19/25 | Loss: 0.00138855
Iteration 20/25 | Loss: 0.00138855
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0013885529479011893, 0.0013885529479011893, 0.0013885529479011893, 0.0013885529479011893, 0.0013885529479011893]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013885529479011893

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00138855
Iteration 2/1000 | Loss: 0.00050790
Iteration 3/1000 | Loss: 0.00003797
Iteration 4/1000 | Loss: 0.00003146
Iteration 5/1000 | Loss: 0.00074118
Iteration 6/1000 | Loss: 0.00002908
Iteration 7/1000 | Loss: 0.00002623
Iteration 8/1000 | Loss: 0.00002498
Iteration 9/1000 | Loss: 0.00002399
Iteration 10/1000 | Loss: 0.00002344
Iteration 11/1000 | Loss: 0.00002292
Iteration 12/1000 | Loss: 0.00002235
Iteration 13/1000 | Loss: 0.00002188
Iteration 14/1000 | Loss: 0.00002163
Iteration 15/1000 | Loss: 0.00002137
Iteration 16/1000 | Loss: 0.00002114
Iteration 17/1000 | Loss: 0.00002094
Iteration 18/1000 | Loss: 0.00002076
Iteration 19/1000 | Loss: 0.00027307
Iteration 20/1000 | Loss: 0.00223727
Iteration 21/1000 | Loss: 0.00006578
Iteration 22/1000 | Loss: 0.00003262
Iteration 23/1000 | Loss: 0.00017783
Iteration 24/1000 | Loss: 0.00002428
Iteration 25/1000 | Loss: 0.00002071
Iteration 26/1000 | Loss: 0.00001644
Iteration 27/1000 | Loss: 0.00001396
Iteration 28/1000 | Loss: 0.00001303
Iteration 29/1000 | Loss: 0.00001205
Iteration 30/1000 | Loss: 0.00001148
Iteration 31/1000 | Loss: 0.00046794
Iteration 32/1000 | Loss: 0.00202908
Iteration 33/1000 | Loss: 0.00066442
Iteration 34/1000 | Loss: 0.00031596
Iteration 35/1000 | Loss: 0.00002229
Iteration 36/1000 | Loss: 0.00050006
Iteration 37/1000 | Loss: 0.00013917
Iteration 38/1000 | Loss: 0.00001274
Iteration 39/1000 | Loss: 0.00011052
Iteration 40/1000 | Loss: 0.00133740
Iteration 41/1000 | Loss: 0.00011014
Iteration 42/1000 | Loss: 0.00002050
Iteration 43/1000 | Loss: 0.00023051
Iteration 44/1000 | Loss: 0.00004471
Iteration 45/1000 | Loss: 0.00001142
Iteration 46/1000 | Loss: 0.00001092
Iteration 47/1000 | Loss: 0.00001062
Iteration 48/1000 | Loss: 0.00001056
Iteration 49/1000 | Loss: 0.00001055
Iteration 50/1000 | Loss: 0.00001051
Iteration 51/1000 | Loss: 0.00001050
Iteration 52/1000 | Loss: 0.00001050
Iteration 53/1000 | Loss: 0.00001049
Iteration 54/1000 | Loss: 0.00001048
Iteration 55/1000 | Loss: 0.00001047
Iteration 56/1000 | Loss: 0.00001047
Iteration 57/1000 | Loss: 0.00001047
Iteration 58/1000 | Loss: 0.00001047
Iteration 59/1000 | Loss: 0.00001046
Iteration 60/1000 | Loss: 0.00001046
Iteration 61/1000 | Loss: 0.00001046
Iteration 62/1000 | Loss: 0.00001045
Iteration 63/1000 | Loss: 0.00001045
Iteration 64/1000 | Loss: 0.00001045
Iteration 65/1000 | Loss: 0.00001045
Iteration 66/1000 | Loss: 0.00001045
Iteration 67/1000 | Loss: 0.00001045
Iteration 68/1000 | Loss: 0.00001045
Iteration 69/1000 | Loss: 0.00001044
Iteration 70/1000 | Loss: 0.00001044
Iteration 71/1000 | Loss: 0.00001044
Iteration 72/1000 | Loss: 0.00001044
Iteration 73/1000 | Loss: 0.00001044
Iteration 74/1000 | Loss: 0.00001044
Iteration 75/1000 | Loss: 0.00001044
Iteration 76/1000 | Loss: 0.00001044
Iteration 77/1000 | Loss: 0.00001044
Iteration 78/1000 | Loss: 0.00001044
Iteration 79/1000 | Loss: 0.00001043
Iteration 80/1000 | Loss: 0.00001043
Iteration 81/1000 | Loss: 0.00001043
Iteration 82/1000 | Loss: 0.00001043
Iteration 83/1000 | Loss: 0.00001043
Iteration 84/1000 | Loss: 0.00001043
Iteration 85/1000 | Loss: 0.00001043
Iteration 86/1000 | Loss: 0.00001043
Iteration 87/1000 | Loss: 0.00001043
Iteration 88/1000 | Loss: 0.00001042
Iteration 89/1000 | Loss: 0.00001042
Iteration 90/1000 | Loss: 0.00001042
Iteration 91/1000 | Loss: 0.00001042
Iteration 92/1000 | Loss: 0.00001041
Iteration 93/1000 | Loss: 0.00001041
Iteration 94/1000 | Loss: 0.00001041
Iteration 95/1000 | Loss: 0.00001040
Iteration 96/1000 | Loss: 0.00001040
Iteration 97/1000 | Loss: 0.00001040
Iteration 98/1000 | Loss: 0.00001039
Iteration 99/1000 | Loss: 0.00001039
Iteration 100/1000 | Loss: 0.00001039
Iteration 101/1000 | Loss: 0.00001039
Iteration 102/1000 | Loss: 0.00001038
Iteration 103/1000 | Loss: 0.00001038
Iteration 104/1000 | Loss: 0.00001037
Iteration 105/1000 | Loss: 0.00001037
Iteration 106/1000 | Loss: 0.00001037
Iteration 107/1000 | Loss: 0.00001037
Iteration 108/1000 | Loss: 0.00001036
Iteration 109/1000 | Loss: 0.00001036
Iteration 110/1000 | Loss: 0.00001036
Iteration 111/1000 | Loss: 0.00001036
Iteration 112/1000 | Loss: 0.00001032
Iteration 113/1000 | Loss: 0.00001032
Iteration 114/1000 | Loss: 0.00001031
Iteration 115/1000 | Loss: 0.00001031
Iteration 116/1000 | Loss: 0.00001031
Iteration 117/1000 | Loss: 0.00001030
Iteration 118/1000 | Loss: 0.00001029
Iteration 119/1000 | Loss: 0.00001029
Iteration 120/1000 | Loss: 0.00001029
Iteration 121/1000 | Loss: 0.00001029
Iteration 122/1000 | Loss: 0.00001029
Iteration 123/1000 | Loss: 0.00001028
Iteration 124/1000 | Loss: 0.00001028
Iteration 125/1000 | Loss: 0.00001028
Iteration 126/1000 | Loss: 0.00001027
Iteration 127/1000 | Loss: 0.00001027
Iteration 128/1000 | Loss: 0.00001027
Iteration 129/1000 | Loss: 0.00001027
Iteration 130/1000 | Loss: 0.00001027
Iteration 131/1000 | Loss: 0.00001026
Iteration 132/1000 | Loss: 0.00001026
Iteration 133/1000 | Loss: 0.00001026
Iteration 134/1000 | Loss: 0.00001025
Iteration 135/1000 | Loss: 0.00001025
Iteration 136/1000 | Loss: 0.00001025
Iteration 137/1000 | Loss: 0.00001025
Iteration 138/1000 | Loss: 0.00001025
Iteration 139/1000 | Loss: 0.00001025
Iteration 140/1000 | Loss: 0.00001025
Iteration 141/1000 | Loss: 0.00001025
Iteration 142/1000 | Loss: 0.00001025
Iteration 143/1000 | Loss: 0.00001024
Iteration 144/1000 | Loss: 0.00001024
Iteration 145/1000 | Loss: 0.00001024
Iteration 146/1000 | Loss: 0.00001024
Iteration 147/1000 | Loss: 0.00001024
Iteration 148/1000 | Loss: 0.00001024
Iteration 149/1000 | Loss: 0.00001024
Iteration 150/1000 | Loss: 0.00001024
Iteration 151/1000 | Loss: 0.00001024
Iteration 152/1000 | Loss: 0.00001024
Iteration 153/1000 | Loss: 0.00001024
Iteration 154/1000 | Loss: 0.00001024
Iteration 155/1000 | Loss: 0.00001023
Iteration 156/1000 | Loss: 0.00001023
Iteration 157/1000 | Loss: 0.00001023
Iteration 158/1000 | Loss: 0.00001023
Iteration 159/1000 | Loss: 0.00001023
Iteration 160/1000 | Loss: 0.00001023
Iteration 161/1000 | Loss: 0.00001023
Iteration 162/1000 | Loss: 0.00001023
Iteration 163/1000 | Loss: 0.00001023
Iteration 164/1000 | Loss: 0.00001023
Iteration 165/1000 | Loss: 0.00001023
Iteration 166/1000 | Loss: 0.00001023
Iteration 167/1000 | Loss: 0.00001023
Iteration 168/1000 | Loss: 0.00001023
Iteration 169/1000 | Loss: 0.00001023
Iteration 170/1000 | Loss: 0.00001023
Iteration 171/1000 | Loss: 0.00001023
Iteration 172/1000 | Loss: 0.00001023
Iteration 173/1000 | Loss: 0.00001022
Iteration 174/1000 | Loss: 0.00001022
Iteration 175/1000 | Loss: 0.00001022
Iteration 176/1000 | Loss: 0.00001022
Iteration 177/1000 | Loss: 0.00001022
Iteration 178/1000 | Loss: 0.00001022
Iteration 179/1000 | Loss: 0.00001022
Iteration 180/1000 | Loss: 0.00001022
Iteration 181/1000 | Loss: 0.00001022
Iteration 182/1000 | Loss: 0.00001022
Iteration 183/1000 | Loss: 0.00001022
Iteration 184/1000 | Loss: 0.00001022
Iteration 185/1000 | Loss: 0.00001022
Iteration 186/1000 | Loss: 0.00001022
Iteration 187/1000 | Loss: 0.00001022
Iteration 188/1000 | Loss: 0.00001022
Iteration 189/1000 | Loss: 0.00001022
Iteration 190/1000 | Loss: 0.00001022
Iteration 191/1000 | Loss: 0.00001021
Iteration 192/1000 | Loss: 0.00001021
Iteration 193/1000 | Loss: 0.00001021
Iteration 194/1000 | Loss: 0.00001021
Iteration 195/1000 | Loss: 0.00001021
Iteration 196/1000 | Loss: 0.00001021
Iteration 197/1000 | Loss: 0.00001021
Iteration 198/1000 | Loss: 0.00001021
Iteration 199/1000 | Loss: 0.00001021
Iteration 200/1000 | Loss: 0.00001021
Iteration 201/1000 | Loss: 0.00001021
Iteration 202/1000 | Loss: 0.00001021
Iteration 203/1000 | Loss: 0.00001021
Iteration 204/1000 | Loss: 0.00001021
Iteration 205/1000 | Loss: 0.00001021
Iteration 206/1000 | Loss: 0.00001021
Iteration 207/1000 | Loss: 0.00001021
Iteration 208/1000 | Loss: 0.00001021
Iteration 209/1000 | Loss: 0.00001021
Iteration 210/1000 | Loss: 0.00001021
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [1.0210236723651178e-05, 1.0210236723651178e-05, 1.0210236723651178e-05, 1.0210236723651178e-05, 1.0210236723651178e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0210236723651178e-05

Optimization complete. Final v2v error: 2.744605541229248 mm

Highest mean error: 2.932053565979004 mm for frame 1

Lowest mean error: 2.7104852199554443 mm for frame 150

Saving results

Total time: 116.97485971450806
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00984115
Iteration 2/25 | Loss: 0.00336987
Iteration 3/25 | Loss: 0.00220554
Iteration 4/25 | Loss: 0.00197261
Iteration 5/25 | Loss: 0.00178344
Iteration 6/25 | Loss: 0.00170191
Iteration 7/25 | Loss: 0.00162493
Iteration 8/25 | Loss: 0.00156603
Iteration 9/25 | Loss: 0.00153874
Iteration 10/25 | Loss: 0.00153168
Iteration 11/25 | Loss: 0.00150902
Iteration 12/25 | Loss: 0.00148749
Iteration 13/25 | Loss: 0.00150811
Iteration 14/25 | Loss: 0.00147740
Iteration 15/25 | Loss: 0.00145566
Iteration 16/25 | Loss: 0.00144788
Iteration 17/25 | Loss: 0.00143515
Iteration 18/25 | Loss: 0.00142805
Iteration 19/25 | Loss: 0.00141988
Iteration 20/25 | Loss: 0.00142420
Iteration 21/25 | Loss: 0.00141197
Iteration 22/25 | Loss: 0.00140311
Iteration 23/25 | Loss: 0.00139306
Iteration 24/25 | Loss: 0.00139077
Iteration 25/25 | Loss: 0.00138868

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25941849
Iteration 2/25 | Loss: 0.01467745
Iteration 3/25 | Loss: 0.05444532
Iteration 4/25 | Loss: 0.03945665
Iteration 5/25 | Loss: 0.00960229
Iteration 6/25 | Loss: 0.01988128
Iteration 7/25 | Loss: 0.01798750
Iteration 8/25 | Loss: 0.02832405
Iteration 9/25 | Loss: 0.02605470
Iteration 10/25 | Loss: 0.02157078
Iteration 11/25 | Loss: 0.02090444
Iteration 12/25 | Loss: 0.00907701
Iteration 13/25 | Loss: 0.01229116
Iteration 14/25 | Loss: 0.01164191
Iteration 15/25 | Loss: 0.00560228
Iteration 16/25 | Loss: 0.00415221
Iteration 17/25 | Loss: 0.00429835
Iteration 18/25 | Loss: 0.00397354
Iteration 19/25 | Loss: 0.00397354
Iteration 20/25 | Loss: 0.00397354
Iteration 21/25 | Loss: 0.00397354
Iteration 22/25 | Loss: 0.00397353
Iteration 23/25 | Loss: 0.00397353
Iteration 24/25 | Loss: 0.00397353
Iteration 25/25 | Loss: 0.00397353

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00397353
Iteration 2/1000 | Loss: 0.00067769
Iteration 3/1000 | Loss: 0.00346923
Iteration 4/1000 | Loss: 0.00066377
Iteration 5/1000 | Loss: 0.00070536
Iteration 6/1000 | Loss: 0.00092856
Iteration 7/1000 | Loss: 0.00040199
Iteration 8/1000 | Loss: 0.00037459
Iteration 9/1000 | Loss: 0.00096728
Iteration 10/1000 | Loss: 0.00052145
Iteration 11/1000 | Loss: 0.00085127
Iteration 12/1000 | Loss: 0.00075654
Iteration 13/1000 | Loss: 0.00025824
Iteration 14/1000 | Loss: 0.00058650
Iteration 15/1000 | Loss: 0.00065545
Iteration 16/1000 | Loss: 0.00105556
Iteration 17/1000 | Loss: 0.00015825
Iteration 18/1000 | Loss: 0.00032669
Iteration 19/1000 | Loss: 0.00109363
Iteration 20/1000 | Loss: 0.00028708
Iteration 21/1000 | Loss: 0.00026474
Iteration 22/1000 | Loss: 0.00016014
Iteration 23/1000 | Loss: 0.00028721
Iteration 24/1000 | Loss: 0.00024208
Iteration 25/1000 | Loss: 0.00014100
Iteration 26/1000 | Loss: 0.00034937
Iteration 27/1000 | Loss: 0.00034617
Iteration 28/1000 | Loss: 0.00043670
Iteration 29/1000 | Loss: 0.00148518
Iteration 30/1000 | Loss: 0.00376584
Iteration 31/1000 | Loss: 0.00140377
Iteration 32/1000 | Loss: 0.00077606
Iteration 33/1000 | Loss: 0.00024981
Iteration 34/1000 | Loss: 0.00014963
Iteration 35/1000 | Loss: 0.00013649
Iteration 36/1000 | Loss: 0.00028857
Iteration 37/1000 | Loss: 0.00111059
Iteration 38/1000 | Loss: 0.00055601
Iteration 39/1000 | Loss: 0.00026013
Iteration 40/1000 | Loss: 0.00027075
Iteration 41/1000 | Loss: 0.00014429
Iteration 42/1000 | Loss: 0.00036471
Iteration 43/1000 | Loss: 0.00012814
Iteration 44/1000 | Loss: 0.00013890
Iteration 45/1000 | Loss: 0.00039009
Iteration 46/1000 | Loss: 0.00088468
Iteration 47/1000 | Loss: 0.00053373
Iteration 48/1000 | Loss: 0.00016893
Iteration 49/1000 | Loss: 0.00056882
Iteration 50/1000 | Loss: 0.00153250
Iteration 51/1000 | Loss: 0.00235295
Iteration 52/1000 | Loss: 0.00099441
Iteration 53/1000 | Loss: 0.00039046
Iteration 54/1000 | Loss: 0.00013938
Iteration 55/1000 | Loss: 0.00040055
Iteration 56/1000 | Loss: 0.00063705
Iteration 57/1000 | Loss: 0.00045117
Iteration 58/1000 | Loss: 0.00011901
Iteration 59/1000 | Loss: 0.00063565
Iteration 60/1000 | Loss: 0.00125156
Iteration 61/1000 | Loss: 0.00021183
Iteration 62/1000 | Loss: 0.00137869
Iteration 63/1000 | Loss: 0.00080482
Iteration 64/1000 | Loss: 0.00120155
Iteration 65/1000 | Loss: 0.00014080
Iteration 66/1000 | Loss: 0.00014500
Iteration 67/1000 | Loss: 0.00030917
Iteration 68/1000 | Loss: 0.00018224
Iteration 69/1000 | Loss: 0.00038691
Iteration 70/1000 | Loss: 0.00292324
Iteration 71/1000 | Loss: 0.00038681
Iteration 72/1000 | Loss: 0.00038619
Iteration 73/1000 | Loss: 0.00051719
Iteration 74/1000 | Loss: 0.00019805
Iteration 75/1000 | Loss: 0.00011596
Iteration 76/1000 | Loss: 0.00010953
Iteration 77/1000 | Loss: 0.00033479
Iteration 78/1000 | Loss: 0.00120618
Iteration 79/1000 | Loss: 0.00034566
Iteration 80/1000 | Loss: 0.00035678
Iteration 81/1000 | Loss: 0.00036152
Iteration 82/1000 | Loss: 0.00012828
Iteration 83/1000 | Loss: 0.00011895
Iteration 84/1000 | Loss: 0.00022456
Iteration 85/1000 | Loss: 0.00184941
Iteration 86/1000 | Loss: 0.00069393
Iteration 87/1000 | Loss: 0.00023797
Iteration 88/1000 | Loss: 0.00033909
Iteration 89/1000 | Loss: 0.00099132
Iteration 90/1000 | Loss: 0.00033388
Iteration 91/1000 | Loss: 0.00198641
Iteration 92/1000 | Loss: 0.00016030
Iteration 93/1000 | Loss: 0.00026542
Iteration 94/1000 | Loss: 0.00009161
Iteration 95/1000 | Loss: 0.00009415
Iteration 96/1000 | Loss: 0.00029917
Iteration 97/1000 | Loss: 0.00008705
Iteration 98/1000 | Loss: 0.00043039
Iteration 99/1000 | Loss: 0.00012920
Iteration 100/1000 | Loss: 0.00010606
Iteration 101/1000 | Loss: 0.00007933
Iteration 102/1000 | Loss: 0.00006928
Iteration 103/1000 | Loss: 0.00031322
Iteration 104/1000 | Loss: 0.00065280
Iteration 105/1000 | Loss: 0.00009793
Iteration 106/1000 | Loss: 0.00109444
Iteration 107/1000 | Loss: 0.00241917
Iteration 108/1000 | Loss: 0.00202498
Iteration 109/1000 | Loss: 0.00104401
Iteration 110/1000 | Loss: 0.00136732
Iteration 111/1000 | Loss: 0.00020015
Iteration 112/1000 | Loss: 0.00074651
Iteration 113/1000 | Loss: 0.00136231
Iteration 114/1000 | Loss: 0.00032595
Iteration 115/1000 | Loss: 0.00013287
Iteration 116/1000 | Loss: 0.00009065
Iteration 117/1000 | Loss: 0.00012393
Iteration 118/1000 | Loss: 0.00014302
Iteration 119/1000 | Loss: 0.00017638
Iteration 120/1000 | Loss: 0.00006062
Iteration 121/1000 | Loss: 0.00006150
Iteration 122/1000 | Loss: 0.00008240
Iteration 123/1000 | Loss: 0.00012329
Iteration 124/1000 | Loss: 0.00020766
Iteration 125/1000 | Loss: 0.00005737
Iteration 126/1000 | Loss: 0.00021626
Iteration 127/1000 | Loss: 0.00010833
Iteration 128/1000 | Loss: 0.00024505
Iteration 129/1000 | Loss: 0.00014199
Iteration 130/1000 | Loss: 0.00021740
Iteration 131/1000 | Loss: 0.00008211
Iteration 132/1000 | Loss: 0.00012291
Iteration 133/1000 | Loss: 0.00007441
Iteration 134/1000 | Loss: 0.00008481
Iteration 135/1000 | Loss: 0.00017427
Iteration 136/1000 | Loss: 0.00010182
Iteration 137/1000 | Loss: 0.00017927
Iteration 138/1000 | Loss: 0.00012475
Iteration 139/1000 | Loss: 0.00018497
Iteration 140/1000 | Loss: 0.00012320
Iteration 141/1000 | Loss: 0.00017183
Iteration 142/1000 | Loss: 0.00007040
Iteration 143/1000 | Loss: 0.00049236
Iteration 144/1000 | Loss: 0.00018322
Iteration 145/1000 | Loss: 0.00020455
Iteration 146/1000 | Loss: 0.00009799
Iteration 147/1000 | Loss: 0.00005648
Iteration 148/1000 | Loss: 0.00022194
Iteration 149/1000 | Loss: 0.00014190
Iteration 150/1000 | Loss: 0.00009533
Iteration 151/1000 | Loss: 0.00006403
Iteration 152/1000 | Loss: 0.00009300
Iteration 153/1000 | Loss: 0.00008619
Iteration 154/1000 | Loss: 0.00006868
Iteration 155/1000 | Loss: 0.00006189
Iteration 156/1000 | Loss: 0.00006909
Iteration 157/1000 | Loss: 0.00017179
Iteration 158/1000 | Loss: 0.00009403
Iteration 159/1000 | Loss: 0.00014824
Iteration 160/1000 | Loss: 0.00016326
Iteration 161/1000 | Loss: 0.00009244
Iteration 162/1000 | Loss: 0.00016069
Iteration 163/1000 | Loss: 0.00040366
Iteration 164/1000 | Loss: 0.00034278
Iteration 165/1000 | Loss: 0.00021038
Iteration 166/1000 | Loss: 0.00009911
Iteration 167/1000 | Loss: 0.00011491
Iteration 168/1000 | Loss: 0.00013804
Iteration 169/1000 | Loss: 0.00011033
Iteration 170/1000 | Loss: 0.00007826
Iteration 171/1000 | Loss: 0.00021178
Iteration 172/1000 | Loss: 0.00015929
Iteration 173/1000 | Loss: 0.00007712
Iteration 174/1000 | Loss: 0.00007350
Iteration 175/1000 | Loss: 0.00008477
Iteration 176/1000 | Loss: 0.00045539
Iteration 177/1000 | Loss: 0.00013870
Iteration 178/1000 | Loss: 0.00018647
Iteration 179/1000 | Loss: 0.00007326
Iteration 180/1000 | Loss: 0.00009228
Iteration 181/1000 | Loss: 0.00013037
Iteration 182/1000 | Loss: 0.00010251
Iteration 183/1000 | Loss: 0.00017358
Iteration 184/1000 | Loss: 0.00020114
Iteration 185/1000 | Loss: 0.00011048
Iteration 186/1000 | Loss: 0.00006111
Iteration 187/1000 | Loss: 0.00010566
Iteration 188/1000 | Loss: 0.00012499
Iteration 189/1000 | Loss: 0.00019312
Iteration 190/1000 | Loss: 0.00030957
Iteration 191/1000 | Loss: 0.00005007
Iteration 192/1000 | Loss: 0.00008232
Iteration 193/1000 | Loss: 0.00004569
Iteration 194/1000 | Loss: 0.00008350
Iteration 195/1000 | Loss: 0.00005607
Iteration 196/1000 | Loss: 0.00004371
Iteration 197/1000 | Loss: 0.00004321
Iteration 198/1000 | Loss: 0.00004294
Iteration 199/1000 | Loss: 0.00004266
Iteration 200/1000 | Loss: 0.00004263
Iteration 201/1000 | Loss: 0.00004256
Iteration 202/1000 | Loss: 0.00004237
Iteration 203/1000 | Loss: 0.00004215
Iteration 204/1000 | Loss: 0.00004200
Iteration 205/1000 | Loss: 0.00004198
Iteration 206/1000 | Loss: 0.00004188
Iteration 207/1000 | Loss: 0.00004186
Iteration 208/1000 | Loss: 0.00004183
Iteration 209/1000 | Loss: 0.00004180
Iteration 210/1000 | Loss: 0.00004166
Iteration 211/1000 | Loss: 0.00004163
Iteration 212/1000 | Loss: 0.00004162
Iteration 213/1000 | Loss: 0.00074797
Iteration 214/1000 | Loss: 0.00051783
Iteration 215/1000 | Loss: 0.00015641
Iteration 216/1000 | Loss: 0.00004330
Iteration 217/1000 | Loss: 0.00012357
Iteration 218/1000 | Loss: 0.00020216
Iteration 219/1000 | Loss: 0.00003933
Iteration 220/1000 | Loss: 0.00008680
Iteration 221/1000 | Loss: 0.00017296
Iteration 222/1000 | Loss: 0.00003773
Iteration 223/1000 | Loss: 0.00011465
Iteration 224/1000 | Loss: 0.00003707
Iteration 225/1000 | Loss: 0.00003822
Iteration 226/1000 | Loss: 0.00003648
Iteration 227/1000 | Loss: 0.00003627
Iteration 228/1000 | Loss: 0.00003620
Iteration 229/1000 | Loss: 0.00003618
Iteration 230/1000 | Loss: 0.00003617
Iteration 231/1000 | Loss: 0.00003617
Iteration 232/1000 | Loss: 0.00003616
Iteration 233/1000 | Loss: 0.00003615
Iteration 234/1000 | Loss: 0.00003615
Iteration 235/1000 | Loss: 0.00003614
Iteration 236/1000 | Loss: 0.00003614
Iteration 237/1000 | Loss: 0.00003613
Iteration 238/1000 | Loss: 0.00003612
Iteration 239/1000 | Loss: 0.00003612
Iteration 240/1000 | Loss: 0.00003599
Iteration 241/1000 | Loss: 0.00003597
Iteration 242/1000 | Loss: 0.00003597
Iteration 243/1000 | Loss: 0.00003597
Iteration 244/1000 | Loss: 0.00003596
Iteration 245/1000 | Loss: 0.00003595
Iteration 246/1000 | Loss: 0.00003592
Iteration 247/1000 | Loss: 0.00003591
Iteration 248/1000 | Loss: 0.00003589
Iteration 249/1000 | Loss: 0.00003588
Iteration 250/1000 | Loss: 0.00003588
Iteration 251/1000 | Loss: 0.00003588
Iteration 252/1000 | Loss: 0.00003588
Iteration 253/1000 | Loss: 0.00003588
Iteration 254/1000 | Loss: 0.00003588
Iteration 255/1000 | Loss: 0.00003587
Iteration 256/1000 | Loss: 0.00003587
Iteration 257/1000 | Loss: 0.00003585
Iteration 258/1000 | Loss: 0.00003584
Iteration 259/1000 | Loss: 0.00003584
Iteration 260/1000 | Loss: 0.00003583
Iteration 261/1000 | Loss: 0.00003583
Iteration 262/1000 | Loss: 0.00003583
Iteration 263/1000 | Loss: 0.00003583
Iteration 264/1000 | Loss: 0.00003583
Iteration 265/1000 | Loss: 0.00003583
Iteration 266/1000 | Loss: 0.00003583
Iteration 267/1000 | Loss: 0.00003583
Iteration 268/1000 | Loss: 0.00003583
Iteration 269/1000 | Loss: 0.00003582
Iteration 270/1000 | Loss: 0.00003582
Iteration 271/1000 | Loss: 0.00003582
Iteration 272/1000 | Loss: 0.00003581
Iteration 273/1000 | Loss: 0.00003581
Iteration 274/1000 | Loss: 0.00003581
Iteration 275/1000 | Loss: 0.00003580
Iteration 276/1000 | Loss: 0.00003580
Iteration 277/1000 | Loss: 0.00003580
Iteration 278/1000 | Loss: 0.00003580
Iteration 279/1000 | Loss: 0.00003580
Iteration 280/1000 | Loss: 0.00003580
Iteration 281/1000 | Loss: 0.00003580
Iteration 282/1000 | Loss: 0.00003579
Iteration 283/1000 | Loss: 0.00003579
Iteration 284/1000 | Loss: 0.00003579
Iteration 285/1000 | Loss: 0.00003579
Iteration 286/1000 | Loss: 0.00003579
Iteration 287/1000 | Loss: 0.00003579
Iteration 288/1000 | Loss: 0.00003579
Iteration 289/1000 | Loss: 0.00003579
Iteration 290/1000 | Loss: 0.00003579
Iteration 291/1000 | Loss: 0.00003579
Iteration 292/1000 | Loss: 0.00003578
Iteration 293/1000 | Loss: 0.00003578
Iteration 294/1000 | Loss: 0.00003578
Iteration 295/1000 | Loss: 0.00003578
Iteration 296/1000 | Loss: 0.00003578
Iteration 297/1000 | Loss: 0.00003577
Iteration 298/1000 | Loss: 0.00003577
Iteration 299/1000 | Loss: 0.00003577
Iteration 300/1000 | Loss: 0.00003577
Iteration 301/1000 | Loss: 0.00003577
Iteration 302/1000 | Loss: 0.00003577
Iteration 303/1000 | Loss: 0.00003577
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 303. Stopping optimization.
Last 5 losses: [3.5771856346400455e-05, 3.5771856346400455e-05, 3.5771856346400455e-05, 3.5771856346400455e-05, 3.5771856346400455e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.5771856346400455e-05

Optimization complete. Final v2v error: 4.495935916900635 mm

Highest mean error: 12.051373481750488 mm for frame 232

Lowest mean error: 2.9946515560150146 mm for frame 108

Saving results

Total time: 434.40732645988464
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00534432
Iteration 2/25 | Loss: 0.00122335
Iteration 3/25 | Loss: 0.00115322
Iteration 4/25 | Loss: 0.00114279
Iteration 5/25 | Loss: 0.00113980
Iteration 6/25 | Loss: 0.00113944
Iteration 7/25 | Loss: 0.00113944
Iteration 8/25 | Loss: 0.00113944
Iteration 9/25 | Loss: 0.00113944
Iteration 10/25 | Loss: 0.00113944
Iteration 11/25 | Loss: 0.00113944
Iteration 12/25 | Loss: 0.00113944
Iteration 13/25 | Loss: 0.00113944
Iteration 14/25 | Loss: 0.00113944
Iteration 15/25 | Loss: 0.00113944
Iteration 16/25 | Loss: 0.00113944
Iteration 17/25 | Loss: 0.00113944
Iteration 18/25 | Loss: 0.00113944
Iteration 19/25 | Loss: 0.00113944
Iteration 20/25 | Loss: 0.00113944
Iteration 21/25 | Loss: 0.00113944
Iteration 22/25 | Loss: 0.00113944
Iteration 23/25 | Loss: 0.00113944
Iteration 24/25 | Loss: 0.00113944
Iteration 25/25 | Loss: 0.00113944

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.27887106
Iteration 2/25 | Loss: 0.00179536
Iteration 3/25 | Loss: 0.00179535
Iteration 4/25 | Loss: 0.00179535
Iteration 5/25 | Loss: 0.00179535
Iteration 6/25 | Loss: 0.00179535
Iteration 7/25 | Loss: 0.00179535
Iteration 8/25 | Loss: 0.00179535
Iteration 9/25 | Loss: 0.00179535
Iteration 10/25 | Loss: 0.00179535
Iteration 11/25 | Loss: 0.00179535
Iteration 12/25 | Loss: 0.00179535
Iteration 13/25 | Loss: 0.00179535
Iteration 14/25 | Loss: 0.00179535
Iteration 15/25 | Loss: 0.00179535
Iteration 16/25 | Loss: 0.00179535
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001795346848666668, 0.001795346848666668, 0.001795346848666668, 0.001795346848666668, 0.001795346848666668]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001795346848666668

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00179535
Iteration 2/1000 | Loss: 0.00002202
Iteration 3/1000 | Loss: 0.00001540
Iteration 4/1000 | Loss: 0.00001361
Iteration 5/1000 | Loss: 0.00001267
Iteration 6/1000 | Loss: 0.00001205
Iteration 7/1000 | Loss: 0.00001157
Iteration 8/1000 | Loss: 0.00001123
Iteration 9/1000 | Loss: 0.00001086
Iteration 10/1000 | Loss: 0.00001064
Iteration 11/1000 | Loss: 0.00001056
Iteration 12/1000 | Loss: 0.00001042
Iteration 13/1000 | Loss: 0.00001029
Iteration 14/1000 | Loss: 0.00001029
Iteration 15/1000 | Loss: 0.00001026
Iteration 16/1000 | Loss: 0.00001026
Iteration 17/1000 | Loss: 0.00001020
Iteration 18/1000 | Loss: 0.00001019
Iteration 19/1000 | Loss: 0.00001016
Iteration 20/1000 | Loss: 0.00001015
Iteration 21/1000 | Loss: 0.00001015
Iteration 22/1000 | Loss: 0.00001014
Iteration 23/1000 | Loss: 0.00001013
Iteration 24/1000 | Loss: 0.00001013
Iteration 25/1000 | Loss: 0.00001011
Iteration 26/1000 | Loss: 0.00001011
Iteration 27/1000 | Loss: 0.00001010
Iteration 28/1000 | Loss: 0.00001010
Iteration 29/1000 | Loss: 0.00001006
Iteration 30/1000 | Loss: 0.00001005
Iteration 31/1000 | Loss: 0.00001005
Iteration 32/1000 | Loss: 0.00000997
Iteration 33/1000 | Loss: 0.00000996
Iteration 34/1000 | Loss: 0.00000996
Iteration 35/1000 | Loss: 0.00000994
Iteration 36/1000 | Loss: 0.00000993
Iteration 37/1000 | Loss: 0.00000993
Iteration 38/1000 | Loss: 0.00000992
Iteration 39/1000 | Loss: 0.00000992
Iteration 40/1000 | Loss: 0.00000992
Iteration 41/1000 | Loss: 0.00000991
Iteration 42/1000 | Loss: 0.00000991
Iteration 43/1000 | Loss: 0.00000991
Iteration 44/1000 | Loss: 0.00000990
Iteration 45/1000 | Loss: 0.00000990
Iteration 46/1000 | Loss: 0.00000988
Iteration 47/1000 | Loss: 0.00000987
Iteration 48/1000 | Loss: 0.00000987
Iteration 49/1000 | Loss: 0.00000986
Iteration 50/1000 | Loss: 0.00000986
Iteration 51/1000 | Loss: 0.00000985
Iteration 52/1000 | Loss: 0.00000985
Iteration 53/1000 | Loss: 0.00000985
Iteration 54/1000 | Loss: 0.00000984
Iteration 55/1000 | Loss: 0.00000984
Iteration 56/1000 | Loss: 0.00000983
Iteration 57/1000 | Loss: 0.00000983
Iteration 58/1000 | Loss: 0.00000982
Iteration 59/1000 | Loss: 0.00000982
Iteration 60/1000 | Loss: 0.00000982
Iteration 61/1000 | Loss: 0.00000981
Iteration 62/1000 | Loss: 0.00000981
Iteration 63/1000 | Loss: 0.00000980
Iteration 64/1000 | Loss: 0.00000980
Iteration 65/1000 | Loss: 0.00000980
Iteration 66/1000 | Loss: 0.00000979
Iteration 67/1000 | Loss: 0.00000979
Iteration 68/1000 | Loss: 0.00000979
Iteration 69/1000 | Loss: 0.00000979
Iteration 70/1000 | Loss: 0.00000979
Iteration 71/1000 | Loss: 0.00000979
Iteration 72/1000 | Loss: 0.00000979
Iteration 73/1000 | Loss: 0.00000979
Iteration 74/1000 | Loss: 0.00000979
Iteration 75/1000 | Loss: 0.00000978
Iteration 76/1000 | Loss: 0.00000978
Iteration 77/1000 | Loss: 0.00000978
Iteration 78/1000 | Loss: 0.00000978
Iteration 79/1000 | Loss: 0.00000978
Iteration 80/1000 | Loss: 0.00000978
Iteration 81/1000 | Loss: 0.00000978
Iteration 82/1000 | Loss: 0.00000978
Iteration 83/1000 | Loss: 0.00000977
Iteration 84/1000 | Loss: 0.00000977
Iteration 85/1000 | Loss: 0.00000977
Iteration 86/1000 | Loss: 0.00000977
Iteration 87/1000 | Loss: 0.00000977
Iteration 88/1000 | Loss: 0.00000977
Iteration 89/1000 | Loss: 0.00000976
Iteration 90/1000 | Loss: 0.00000976
Iteration 91/1000 | Loss: 0.00000976
Iteration 92/1000 | Loss: 0.00000976
Iteration 93/1000 | Loss: 0.00000976
Iteration 94/1000 | Loss: 0.00000976
Iteration 95/1000 | Loss: 0.00000976
Iteration 96/1000 | Loss: 0.00000975
Iteration 97/1000 | Loss: 0.00000975
Iteration 98/1000 | Loss: 0.00000975
Iteration 99/1000 | Loss: 0.00000974
Iteration 100/1000 | Loss: 0.00000974
Iteration 101/1000 | Loss: 0.00000974
Iteration 102/1000 | Loss: 0.00000974
Iteration 103/1000 | Loss: 0.00000973
Iteration 104/1000 | Loss: 0.00000973
Iteration 105/1000 | Loss: 0.00000973
Iteration 106/1000 | Loss: 0.00000973
Iteration 107/1000 | Loss: 0.00000973
Iteration 108/1000 | Loss: 0.00000973
Iteration 109/1000 | Loss: 0.00000973
Iteration 110/1000 | Loss: 0.00000972
Iteration 111/1000 | Loss: 0.00000972
Iteration 112/1000 | Loss: 0.00000972
Iteration 113/1000 | Loss: 0.00000972
Iteration 114/1000 | Loss: 0.00000972
Iteration 115/1000 | Loss: 0.00000972
Iteration 116/1000 | Loss: 0.00000972
Iteration 117/1000 | Loss: 0.00000971
Iteration 118/1000 | Loss: 0.00000971
Iteration 119/1000 | Loss: 0.00000971
Iteration 120/1000 | Loss: 0.00000971
Iteration 121/1000 | Loss: 0.00000971
Iteration 122/1000 | Loss: 0.00000971
Iteration 123/1000 | Loss: 0.00000971
Iteration 124/1000 | Loss: 0.00000971
Iteration 125/1000 | Loss: 0.00000971
Iteration 126/1000 | Loss: 0.00000971
Iteration 127/1000 | Loss: 0.00000971
Iteration 128/1000 | Loss: 0.00000971
Iteration 129/1000 | Loss: 0.00000971
Iteration 130/1000 | Loss: 0.00000971
Iteration 131/1000 | Loss: 0.00000971
Iteration 132/1000 | Loss: 0.00000971
Iteration 133/1000 | Loss: 0.00000971
Iteration 134/1000 | Loss: 0.00000971
Iteration 135/1000 | Loss: 0.00000971
Iteration 136/1000 | Loss: 0.00000971
Iteration 137/1000 | Loss: 0.00000971
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [9.71273948380258e-06, 9.71273948380258e-06, 9.71273948380258e-06, 9.71273948380258e-06, 9.71273948380258e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.71273948380258e-06

Optimization complete. Final v2v error: 2.682244300842285 mm

Highest mean error: 3.4964075088500977 mm for frame 61

Lowest mean error: 2.4293105602264404 mm for frame 158

Saving results

Total time: 37.70028829574585
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00390365
Iteration 2/25 | Loss: 0.00123118
Iteration 3/25 | Loss: 0.00116371
Iteration 4/25 | Loss: 0.00115480
Iteration 5/25 | Loss: 0.00115285
Iteration 6/25 | Loss: 0.00115285
Iteration 7/25 | Loss: 0.00115285
Iteration 8/25 | Loss: 0.00115285
Iteration 9/25 | Loss: 0.00115285
Iteration 10/25 | Loss: 0.00115285
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011528461473062634, 0.0011528461473062634, 0.0011528461473062634, 0.0011528461473062634, 0.0011528461473062634]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011528461473062634

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25323880
Iteration 2/25 | Loss: 0.00182037
Iteration 3/25 | Loss: 0.00182036
Iteration 4/25 | Loss: 0.00182036
Iteration 5/25 | Loss: 0.00182036
Iteration 6/25 | Loss: 0.00182036
Iteration 7/25 | Loss: 0.00182036
Iteration 8/25 | Loss: 0.00182036
Iteration 9/25 | Loss: 0.00182036
Iteration 10/25 | Loss: 0.00182036
Iteration 11/25 | Loss: 0.00182036
Iteration 12/25 | Loss: 0.00182036
Iteration 13/25 | Loss: 0.00182036
Iteration 14/25 | Loss: 0.00182036
Iteration 15/25 | Loss: 0.00182036
Iteration 16/25 | Loss: 0.00182036
Iteration 17/25 | Loss: 0.00182036
Iteration 18/25 | Loss: 0.00182036
Iteration 19/25 | Loss: 0.00182036
Iteration 20/25 | Loss: 0.00182036
Iteration 21/25 | Loss: 0.00182036
Iteration 22/25 | Loss: 0.00182036
Iteration 23/25 | Loss: 0.00182036
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001820360659621656, 0.001820360659621656, 0.001820360659621656, 0.001820360659621656, 0.001820360659621656]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001820360659621656

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00182036
Iteration 2/1000 | Loss: 0.00001836
Iteration 3/1000 | Loss: 0.00001285
Iteration 4/1000 | Loss: 0.00001178
Iteration 5/1000 | Loss: 0.00001121
Iteration 6/1000 | Loss: 0.00001061
Iteration 7/1000 | Loss: 0.00001027
Iteration 8/1000 | Loss: 0.00000987
Iteration 9/1000 | Loss: 0.00000974
Iteration 10/1000 | Loss: 0.00000971
Iteration 11/1000 | Loss: 0.00000966
Iteration 12/1000 | Loss: 0.00000965
Iteration 13/1000 | Loss: 0.00000952
Iteration 14/1000 | Loss: 0.00000941
Iteration 15/1000 | Loss: 0.00000928
Iteration 16/1000 | Loss: 0.00000926
Iteration 17/1000 | Loss: 0.00000924
Iteration 18/1000 | Loss: 0.00000924
Iteration 19/1000 | Loss: 0.00000923
Iteration 20/1000 | Loss: 0.00000922
Iteration 21/1000 | Loss: 0.00000916
Iteration 22/1000 | Loss: 0.00000911
Iteration 23/1000 | Loss: 0.00000901
Iteration 24/1000 | Loss: 0.00000901
Iteration 25/1000 | Loss: 0.00000898
Iteration 26/1000 | Loss: 0.00000895
Iteration 27/1000 | Loss: 0.00000894
Iteration 28/1000 | Loss: 0.00000894
Iteration 29/1000 | Loss: 0.00000894
Iteration 30/1000 | Loss: 0.00000894
Iteration 31/1000 | Loss: 0.00000892
Iteration 32/1000 | Loss: 0.00000887
Iteration 33/1000 | Loss: 0.00000887
Iteration 34/1000 | Loss: 0.00000887
Iteration 35/1000 | Loss: 0.00000885
Iteration 36/1000 | Loss: 0.00000885
Iteration 37/1000 | Loss: 0.00000884
Iteration 38/1000 | Loss: 0.00000882
Iteration 39/1000 | Loss: 0.00000882
Iteration 40/1000 | Loss: 0.00000882
Iteration 41/1000 | Loss: 0.00000882
Iteration 42/1000 | Loss: 0.00000882
Iteration 43/1000 | Loss: 0.00000881
Iteration 44/1000 | Loss: 0.00000881
Iteration 45/1000 | Loss: 0.00000881
Iteration 46/1000 | Loss: 0.00000881
Iteration 47/1000 | Loss: 0.00000881
Iteration 48/1000 | Loss: 0.00000881
Iteration 49/1000 | Loss: 0.00000881
Iteration 50/1000 | Loss: 0.00000881
Iteration 51/1000 | Loss: 0.00000881
Iteration 52/1000 | Loss: 0.00000881
Iteration 53/1000 | Loss: 0.00000881
Iteration 54/1000 | Loss: 0.00000881
Iteration 55/1000 | Loss: 0.00000881
Iteration 56/1000 | Loss: 0.00000881
Iteration 57/1000 | Loss: 0.00000881
Iteration 58/1000 | Loss: 0.00000881
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 58. Stopping optimization.
Last 5 losses: [8.805328434391413e-06, 8.805328434391413e-06, 8.805328434391413e-06, 8.805328434391413e-06, 8.805328434391413e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.805328434391413e-06

Optimization complete. Final v2v error: 2.5803396701812744 mm

Highest mean error: 2.77535343170166 mm for frame 127

Lowest mean error: 2.468773603439331 mm for frame 227

Saving results

Total time: 32.807164430618286
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00656246
Iteration 2/25 | Loss: 0.00125064
Iteration 3/25 | Loss: 0.00116316
Iteration 4/25 | Loss: 0.00114772
Iteration 5/25 | Loss: 0.00114298
Iteration 6/25 | Loss: 0.00114184
Iteration 7/25 | Loss: 0.00114184
Iteration 8/25 | Loss: 0.00114184
Iteration 9/25 | Loss: 0.00114184
Iteration 10/25 | Loss: 0.00114184
Iteration 11/25 | Loss: 0.00114184
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011418354697525501, 0.0011418354697525501, 0.0011418354697525501, 0.0011418354697525501, 0.0011418354697525501]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011418354697525501

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.20286274
Iteration 2/25 | Loss: 0.00176374
Iteration 3/25 | Loss: 0.00176373
Iteration 4/25 | Loss: 0.00176373
Iteration 5/25 | Loss: 0.00176373
Iteration 6/25 | Loss: 0.00176373
Iteration 7/25 | Loss: 0.00176373
Iteration 8/25 | Loss: 0.00176373
Iteration 9/25 | Loss: 0.00176373
Iteration 10/25 | Loss: 0.00176373
Iteration 11/25 | Loss: 0.00176373
Iteration 12/25 | Loss: 0.00176373
Iteration 13/25 | Loss: 0.00176373
Iteration 14/25 | Loss: 0.00176373
Iteration 15/25 | Loss: 0.00176373
Iteration 16/25 | Loss: 0.00176373
Iteration 17/25 | Loss: 0.00176373
Iteration 18/25 | Loss: 0.00176373
Iteration 19/25 | Loss: 0.00176373
Iteration 20/25 | Loss: 0.00176373
Iteration 21/25 | Loss: 0.00176373
Iteration 22/25 | Loss: 0.00176373
Iteration 23/25 | Loss: 0.00176373
Iteration 24/25 | Loss: 0.00176373
Iteration 25/25 | Loss: 0.00176373

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00176373
Iteration 2/1000 | Loss: 0.00002159
Iteration 3/1000 | Loss: 0.00001459
Iteration 4/1000 | Loss: 0.00001328
Iteration 5/1000 | Loss: 0.00001265
Iteration 6/1000 | Loss: 0.00001208
Iteration 7/1000 | Loss: 0.00001154
Iteration 8/1000 | Loss: 0.00001131
Iteration 9/1000 | Loss: 0.00001098
Iteration 10/1000 | Loss: 0.00001068
Iteration 11/1000 | Loss: 0.00001045
Iteration 12/1000 | Loss: 0.00001035
Iteration 13/1000 | Loss: 0.00001024
Iteration 14/1000 | Loss: 0.00001019
Iteration 15/1000 | Loss: 0.00001010
Iteration 16/1000 | Loss: 0.00001006
Iteration 17/1000 | Loss: 0.00000996
Iteration 18/1000 | Loss: 0.00000994
Iteration 19/1000 | Loss: 0.00000988
Iteration 20/1000 | Loss: 0.00000988
Iteration 21/1000 | Loss: 0.00000988
Iteration 22/1000 | Loss: 0.00000987
Iteration 23/1000 | Loss: 0.00000987
Iteration 24/1000 | Loss: 0.00000985
Iteration 25/1000 | Loss: 0.00000985
Iteration 26/1000 | Loss: 0.00000983
Iteration 27/1000 | Loss: 0.00000983
Iteration 28/1000 | Loss: 0.00000983
Iteration 29/1000 | Loss: 0.00000983
Iteration 30/1000 | Loss: 0.00000983
Iteration 31/1000 | Loss: 0.00000983
Iteration 32/1000 | Loss: 0.00000983
Iteration 33/1000 | Loss: 0.00000983
Iteration 34/1000 | Loss: 0.00000982
Iteration 35/1000 | Loss: 0.00000982
Iteration 36/1000 | Loss: 0.00000982
Iteration 37/1000 | Loss: 0.00000982
Iteration 38/1000 | Loss: 0.00000982
Iteration 39/1000 | Loss: 0.00000982
Iteration 40/1000 | Loss: 0.00000982
Iteration 41/1000 | Loss: 0.00000982
Iteration 42/1000 | Loss: 0.00000982
Iteration 43/1000 | Loss: 0.00000982
Iteration 44/1000 | Loss: 0.00000982
Iteration 45/1000 | Loss: 0.00000982
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 45. Stopping optimization.
Last 5 losses: [9.824082553677727e-06, 9.824082553677727e-06, 9.824082553677727e-06, 9.824082553677727e-06, 9.824082553677727e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.824082553677727e-06

Optimization complete. Final v2v error: 2.71815824508667 mm

Highest mean error: 2.9523746967315674 mm for frame 71

Lowest mean error: 2.5474770069122314 mm for frame 11

Saving results

Total time: 30.524868488311768
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01034769
Iteration 2/25 | Loss: 0.00397936
Iteration 3/25 | Loss: 0.00321534
Iteration 4/25 | Loss: 0.00308154
Iteration 5/25 | Loss: 0.00298040
Iteration 6/25 | Loss: 0.00284412
Iteration 7/25 | Loss: 0.00250305
Iteration 8/25 | Loss: 0.00236598
Iteration 9/25 | Loss: 0.00243442
Iteration 10/25 | Loss: 0.00226898
Iteration 11/25 | Loss: 0.00225517
Iteration 12/25 | Loss: 0.00228389
Iteration 13/25 | Loss: 0.00205000
Iteration 14/25 | Loss: 0.00200009
Iteration 15/25 | Loss: 0.00189085
Iteration 16/25 | Loss: 0.00187238
Iteration 17/25 | Loss: 0.00183760
Iteration 18/25 | Loss: 0.00185328
Iteration 19/25 | Loss: 0.00182255
Iteration 20/25 | Loss: 0.00182776
Iteration 21/25 | Loss: 0.00182099
Iteration 22/25 | Loss: 0.00181462
Iteration 23/25 | Loss: 0.00181307
Iteration 24/25 | Loss: 0.00181236
Iteration 25/25 | Loss: 0.00181176

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.98185682
Iteration 2/25 | Loss: 0.00416764
Iteration 3/25 | Loss: 0.00416764
Iteration 4/25 | Loss: 0.00416764
Iteration 5/25 | Loss: 0.00416764
Iteration 6/25 | Loss: 0.00416764
Iteration 7/25 | Loss: 0.00416764
Iteration 8/25 | Loss: 0.00416764
Iteration 9/25 | Loss: 0.00416764
Iteration 10/25 | Loss: 0.00416764
Iteration 11/25 | Loss: 0.00416764
Iteration 12/25 | Loss: 0.00416764
Iteration 13/25 | Loss: 0.00416764
Iteration 14/25 | Loss: 0.00416764
Iteration 15/25 | Loss: 0.00416764
Iteration 16/25 | Loss: 0.00416764
Iteration 17/25 | Loss: 0.00416764
Iteration 18/25 | Loss: 0.00416764
Iteration 19/25 | Loss: 0.00416764
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.004167635925114155, 0.004167635925114155, 0.004167635925114155, 0.004167635925114155, 0.004167635925114155]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004167635925114155

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00416764
Iteration 2/1000 | Loss: 0.00031408
Iteration 3/1000 | Loss: 0.00023712
Iteration 4/1000 | Loss: 0.00021122
Iteration 5/1000 | Loss: 0.00019796
Iteration 6/1000 | Loss: 0.00018732
Iteration 7/1000 | Loss: 0.00017909
Iteration 8/1000 | Loss: 0.00017374
Iteration 9/1000 | Loss: 0.00043063
Iteration 10/1000 | Loss: 0.00016844
Iteration 11/1000 | Loss: 0.00207706
Iteration 12/1000 | Loss: 0.00785487
Iteration 13/1000 | Loss: 0.01431732
Iteration 14/1000 | Loss: 0.00753141
Iteration 15/1000 | Loss: 0.01002593
Iteration 16/1000 | Loss: 0.00876175
Iteration 17/1000 | Loss: 0.00106629
Iteration 18/1000 | Loss: 0.00041706
Iteration 19/1000 | Loss: 0.00094326
Iteration 20/1000 | Loss: 0.00156335
Iteration 21/1000 | Loss: 0.00085596
Iteration 22/1000 | Loss: 0.00062611
Iteration 23/1000 | Loss: 0.00022705
Iteration 24/1000 | Loss: 0.00018723
Iteration 25/1000 | Loss: 0.00135402
Iteration 26/1000 | Loss: 0.00179297
Iteration 27/1000 | Loss: 0.00061878
Iteration 28/1000 | Loss: 0.00083021
Iteration 29/1000 | Loss: 0.00165433
Iteration 30/1000 | Loss: 0.00292882
Iteration 31/1000 | Loss: 0.00239752
Iteration 32/1000 | Loss: 0.00316702
Iteration 33/1000 | Loss: 0.00190357
Iteration 34/1000 | Loss: 0.00088394
Iteration 35/1000 | Loss: 0.00057249
Iteration 36/1000 | Loss: 0.00017299
Iteration 37/1000 | Loss: 0.00013692
Iteration 38/1000 | Loss: 0.00011724
Iteration 39/1000 | Loss: 0.00020975
Iteration 40/1000 | Loss: 0.00022570
Iteration 41/1000 | Loss: 0.00009007
Iteration 42/1000 | Loss: 0.00008000
Iteration 43/1000 | Loss: 0.00007413
Iteration 44/1000 | Loss: 0.00006988
Iteration 45/1000 | Loss: 0.00019920
Iteration 46/1000 | Loss: 0.00019553
Iteration 47/1000 | Loss: 0.00007823
Iteration 48/1000 | Loss: 0.00008402
Iteration 49/1000 | Loss: 0.00011205
Iteration 50/1000 | Loss: 0.00009527
Iteration 51/1000 | Loss: 0.00010923
Iteration 52/1000 | Loss: 0.00007630
Iteration 53/1000 | Loss: 0.00008411
Iteration 54/1000 | Loss: 0.00006641
Iteration 55/1000 | Loss: 0.00006470
Iteration 56/1000 | Loss: 0.00007602
Iteration 57/1000 | Loss: 0.00007403
Iteration 58/1000 | Loss: 0.00006197
Iteration 59/1000 | Loss: 0.00006054
Iteration 60/1000 | Loss: 0.00005931
Iteration 61/1000 | Loss: 0.00005859
Iteration 62/1000 | Loss: 0.00022751
Iteration 63/1000 | Loss: 0.00044582
Iteration 64/1000 | Loss: 0.00042007
Iteration 65/1000 | Loss: 0.00038353
Iteration 66/1000 | Loss: 0.00029823
Iteration 67/1000 | Loss: 0.00010916
Iteration 68/1000 | Loss: 0.00024863
Iteration 69/1000 | Loss: 0.00007591
Iteration 70/1000 | Loss: 0.00009313
Iteration 71/1000 | Loss: 0.00008553
Iteration 72/1000 | Loss: 0.00007869
Iteration 73/1000 | Loss: 0.00007536
Iteration 74/1000 | Loss: 0.00005873
Iteration 75/1000 | Loss: 0.00005691
Iteration 76/1000 | Loss: 0.00005620
Iteration 77/1000 | Loss: 0.00005571
Iteration 78/1000 | Loss: 0.00005514
Iteration 79/1000 | Loss: 0.00005470
Iteration 80/1000 | Loss: 0.00008634
Iteration 81/1000 | Loss: 0.00006202
Iteration 82/1000 | Loss: 0.00008069
Iteration 83/1000 | Loss: 0.00008579
Iteration 84/1000 | Loss: 0.00007314
Iteration 85/1000 | Loss: 0.00007310
Iteration 86/1000 | Loss: 0.00007043
Iteration 87/1000 | Loss: 0.00007527
Iteration 88/1000 | Loss: 0.00005803
Iteration 89/1000 | Loss: 0.00005615
Iteration 90/1000 | Loss: 0.00005523
Iteration 91/1000 | Loss: 0.00005473
Iteration 92/1000 | Loss: 0.00005404
Iteration 93/1000 | Loss: 0.00005364
Iteration 94/1000 | Loss: 0.00005330
Iteration 95/1000 | Loss: 0.00005306
Iteration 96/1000 | Loss: 0.00005280
Iteration 97/1000 | Loss: 0.00005258
Iteration 98/1000 | Loss: 0.00008506
Iteration 99/1000 | Loss: 0.00052466
Iteration 100/1000 | Loss: 0.00041884
Iteration 101/1000 | Loss: 0.00027455
Iteration 102/1000 | Loss: 0.00006342
Iteration 103/1000 | Loss: 0.00005683
Iteration 104/1000 | Loss: 0.00005418
Iteration 105/1000 | Loss: 0.00005167
Iteration 106/1000 | Loss: 0.00005023
Iteration 107/1000 | Loss: 0.00004953
Iteration 108/1000 | Loss: 0.00004913
Iteration 109/1000 | Loss: 0.00004882
Iteration 110/1000 | Loss: 0.00004866
Iteration 111/1000 | Loss: 0.00004857
Iteration 112/1000 | Loss: 0.00004856
Iteration 113/1000 | Loss: 0.00004855
Iteration 114/1000 | Loss: 0.00004855
Iteration 115/1000 | Loss: 0.00004853
Iteration 116/1000 | Loss: 0.00004847
Iteration 117/1000 | Loss: 0.00004844
Iteration 118/1000 | Loss: 0.00004844
Iteration 119/1000 | Loss: 0.00004843
Iteration 120/1000 | Loss: 0.00004843
Iteration 121/1000 | Loss: 0.00004842
Iteration 122/1000 | Loss: 0.00004840
Iteration 123/1000 | Loss: 0.00004840
Iteration 124/1000 | Loss: 0.00004840
Iteration 125/1000 | Loss: 0.00004840
Iteration 126/1000 | Loss: 0.00004839
Iteration 127/1000 | Loss: 0.00004839
Iteration 128/1000 | Loss: 0.00004839
Iteration 129/1000 | Loss: 0.00004839
Iteration 130/1000 | Loss: 0.00004839
Iteration 131/1000 | Loss: 0.00004839
Iteration 132/1000 | Loss: 0.00004839
Iteration 133/1000 | Loss: 0.00004838
Iteration 134/1000 | Loss: 0.00004838
Iteration 135/1000 | Loss: 0.00004838
Iteration 136/1000 | Loss: 0.00004838
Iteration 137/1000 | Loss: 0.00004837
Iteration 138/1000 | Loss: 0.00004837
Iteration 139/1000 | Loss: 0.00004836
Iteration 140/1000 | Loss: 0.00004836
Iteration 141/1000 | Loss: 0.00004835
Iteration 142/1000 | Loss: 0.00004835
Iteration 143/1000 | Loss: 0.00004835
Iteration 144/1000 | Loss: 0.00004835
Iteration 145/1000 | Loss: 0.00004835
Iteration 146/1000 | Loss: 0.00004835
Iteration 147/1000 | Loss: 0.00004835
Iteration 148/1000 | Loss: 0.00004835
Iteration 149/1000 | Loss: 0.00004835
Iteration 150/1000 | Loss: 0.00004835
Iteration 151/1000 | Loss: 0.00004835
Iteration 152/1000 | Loss: 0.00004834
Iteration 153/1000 | Loss: 0.00004834
Iteration 154/1000 | Loss: 0.00004834
Iteration 155/1000 | Loss: 0.00004834
Iteration 156/1000 | Loss: 0.00004834
Iteration 157/1000 | Loss: 0.00004834
Iteration 158/1000 | Loss: 0.00004834
Iteration 159/1000 | Loss: 0.00004834
Iteration 160/1000 | Loss: 0.00004834
Iteration 161/1000 | Loss: 0.00004834
Iteration 162/1000 | Loss: 0.00004834
Iteration 163/1000 | Loss: 0.00004834
Iteration 164/1000 | Loss: 0.00004834
Iteration 165/1000 | Loss: 0.00004834
Iteration 166/1000 | Loss: 0.00004833
Iteration 167/1000 | Loss: 0.00004833
Iteration 168/1000 | Loss: 0.00004833
Iteration 169/1000 | Loss: 0.00004832
Iteration 170/1000 | Loss: 0.00004832
Iteration 171/1000 | Loss: 0.00004832
Iteration 172/1000 | Loss: 0.00004832
Iteration 173/1000 | Loss: 0.00004830
Iteration 174/1000 | Loss: 0.00004830
Iteration 175/1000 | Loss: 0.00004830
Iteration 176/1000 | Loss: 0.00004830
Iteration 177/1000 | Loss: 0.00004829
Iteration 178/1000 | Loss: 0.00004829
Iteration 179/1000 | Loss: 0.00004829
Iteration 180/1000 | Loss: 0.00004829
Iteration 181/1000 | Loss: 0.00004829
Iteration 182/1000 | Loss: 0.00004829
Iteration 183/1000 | Loss: 0.00004829
Iteration 184/1000 | Loss: 0.00004829
Iteration 185/1000 | Loss: 0.00004829
Iteration 186/1000 | Loss: 0.00004828
Iteration 187/1000 | Loss: 0.00004828
Iteration 188/1000 | Loss: 0.00004828
Iteration 189/1000 | Loss: 0.00004828
Iteration 190/1000 | Loss: 0.00004828
Iteration 191/1000 | Loss: 0.00004828
Iteration 192/1000 | Loss: 0.00004828
Iteration 193/1000 | Loss: 0.00004828
Iteration 194/1000 | Loss: 0.00004828
Iteration 195/1000 | Loss: 0.00004828
Iteration 196/1000 | Loss: 0.00004828
Iteration 197/1000 | Loss: 0.00004828
Iteration 198/1000 | Loss: 0.00004827
Iteration 199/1000 | Loss: 0.00004827
Iteration 200/1000 | Loss: 0.00004827
Iteration 201/1000 | Loss: 0.00004827
Iteration 202/1000 | Loss: 0.00004827
Iteration 203/1000 | Loss: 0.00004827
Iteration 204/1000 | Loss: 0.00004827
Iteration 205/1000 | Loss: 0.00004827
Iteration 206/1000 | Loss: 0.00004827
Iteration 207/1000 | Loss: 0.00004826
Iteration 208/1000 | Loss: 0.00004826
Iteration 209/1000 | Loss: 0.00004826
Iteration 210/1000 | Loss: 0.00004826
Iteration 211/1000 | Loss: 0.00004826
Iteration 212/1000 | Loss: 0.00004826
Iteration 213/1000 | Loss: 0.00004826
Iteration 214/1000 | Loss: 0.00004826
Iteration 215/1000 | Loss: 0.00004826
Iteration 216/1000 | Loss: 0.00004826
Iteration 217/1000 | Loss: 0.00004826
Iteration 218/1000 | Loss: 0.00004826
Iteration 219/1000 | Loss: 0.00004825
Iteration 220/1000 | Loss: 0.00004825
Iteration 221/1000 | Loss: 0.00004825
Iteration 222/1000 | Loss: 0.00004825
Iteration 223/1000 | Loss: 0.00004825
Iteration 224/1000 | Loss: 0.00004825
Iteration 225/1000 | Loss: 0.00004825
Iteration 226/1000 | Loss: 0.00004825
Iteration 227/1000 | Loss: 0.00004825
Iteration 228/1000 | Loss: 0.00020619
Iteration 229/1000 | Loss: 0.00037518
Iteration 230/1000 | Loss: 0.00013718
Iteration 231/1000 | Loss: 0.00005322
Iteration 232/1000 | Loss: 0.00036137
Iteration 233/1000 | Loss: 0.00005115
Iteration 234/1000 | Loss: 0.00004872
Iteration 235/1000 | Loss: 0.00004711
Iteration 236/1000 | Loss: 0.00004612
Iteration 237/1000 | Loss: 0.00004554
Iteration 238/1000 | Loss: 0.00004528
Iteration 239/1000 | Loss: 0.00004506
Iteration 240/1000 | Loss: 0.00004497
Iteration 241/1000 | Loss: 0.00004494
Iteration 242/1000 | Loss: 0.00004494
Iteration 243/1000 | Loss: 0.00004493
Iteration 244/1000 | Loss: 0.00004491
Iteration 245/1000 | Loss: 0.00004487
Iteration 246/1000 | Loss: 0.00004487
Iteration 247/1000 | Loss: 0.00004487
Iteration 248/1000 | Loss: 0.00004487
Iteration 249/1000 | Loss: 0.00004487
Iteration 250/1000 | Loss: 0.00004487
Iteration 251/1000 | Loss: 0.00004487
Iteration 252/1000 | Loss: 0.00004487
Iteration 253/1000 | Loss: 0.00004487
Iteration 254/1000 | Loss: 0.00004487
Iteration 255/1000 | Loss: 0.00004486
Iteration 256/1000 | Loss: 0.00004486
Iteration 257/1000 | Loss: 0.00004486
Iteration 258/1000 | Loss: 0.00004486
Iteration 259/1000 | Loss: 0.00004486
Iteration 260/1000 | Loss: 0.00004486
Iteration 261/1000 | Loss: 0.00004486
Iteration 262/1000 | Loss: 0.00004486
Iteration 263/1000 | Loss: 0.00004485
Iteration 264/1000 | Loss: 0.00004485
Iteration 265/1000 | Loss: 0.00004484
Iteration 266/1000 | Loss: 0.00004483
Iteration 267/1000 | Loss: 0.00004483
Iteration 268/1000 | Loss: 0.00004483
Iteration 269/1000 | Loss: 0.00004483
Iteration 270/1000 | Loss: 0.00004483
Iteration 271/1000 | Loss: 0.00004483
Iteration 272/1000 | Loss: 0.00004483
Iteration 273/1000 | Loss: 0.00004483
Iteration 274/1000 | Loss: 0.00004483
Iteration 275/1000 | Loss: 0.00004483
Iteration 276/1000 | Loss: 0.00004482
Iteration 277/1000 | Loss: 0.00004481
Iteration 278/1000 | Loss: 0.00004481
Iteration 279/1000 | Loss: 0.00004481
Iteration 280/1000 | Loss: 0.00004481
Iteration 281/1000 | Loss: 0.00004481
Iteration 282/1000 | Loss: 0.00004481
Iteration 283/1000 | Loss: 0.00004480
Iteration 284/1000 | Loss: 0.00004480
Iteration 285/1000 | Loss: 0.00004480
Iteration 286/1000 | Loss: 0.00004479
Iteration 287/1000 | Loss: 0.00004478
Iteration 288/1000 | Loss: 0.00004478
Iteration 289/1000 | Loss: 0.00004476
Iteration 290/1000 | Loss: 0.00004476
Iteration 291/1000 | Loss: 0.00004476
Iteration 292/1000 | Loss: 0.00004476
Iteration 293/1000 | Loss: 0.00004476
Iteration 294/1000 | Loss: 0.00004476
Iteration 295/1000 | Loss: 0.00004476
Iteration 296/1000 | Loss: 0.00004476
Iteration 297/1000 | Loss: 0.00004476
Iteration 298/1000 | Loss: 0.00004476
Iteration 299/1000 | Loss: 0.00004476
Iteration 300/1000 | Loss: 0.00004475
Iteration 301/1000 | Loss: 0.00004475
Iteration 302/1000 | Loss: 0.00004475
Iteration 303/1000 | Loss: 0.00004474
Iteration 304/1000 | Loss: 0.00004474
Iteration 305/1000 | Loss: 0.00004474
Iteration 306/1000 | Loss: 0.00004474
Iteration 307/1000 | Loss: 0.00004474
Iteration 308/1000 | Loss: 0.00004473
Iteration 309/1000 | Loss: 0.00004473
Iteration 310/1000 | Loss: 0.00004472
Iteration 311/1000 | Loss: 0.00004472
Iteration 312/1000 | Loss: 0.00004472
Iteration 313/1000 | Loss: 0.00004472
Iteration 314/1000 | Loss: 0.00004472
Iteration 315/1000 | Loss: 0.00004472
Iteration 316/1000 | Loss: 0.00004472
Iteration 317/1000 | Loss: 0.00004471
Iteration 318/1000 | Loss: 0.00004471
Iteration 319/1000 | Loss: 0.00004471
Iteration 320/1000 | Loss: 0.00004470
Iteration 321/1000 | Loss: 0.00004470
Iteration 322/1000 | Loss: 0.00004470
Iteration 323/1000 | Loss: 0.00004470
Iteration 324/1000 | Loss: 0.00004470
Iteration 325/1000 | Loss: 0.00004470
Iteration 326/1000 | Loss: 0.00004470
Iteration 327/1000 | Loss: 0.00004469
Iteration 328/1000 | Loss: 0.00004469
Iteration 329/1000 | Loss: 0.00004469
Iteration 330/1000 | Loss: 0.00004469
Iteration 331/1000 | Loss: 0.00004468
Iteration 332/1000 | Loss: 0.00004468
Iteration 333/1000 | Loss: 0.00004468
Iteration 334/1000 | Loss: 0.00004468
Iteration 335/1000 | Loss: 0.00004468
Iteration 336/1000 | Loss: 0.00004468
Iteration 337/1000 | Loss: 0.00004468
Iteration 338/1000 | Loss: 0.00004468
Iteration 339/1000 | Loss: 0.00004468
Iteration 340/1000 | Loss: 0.00004468
Iteration 341/1000 | Loss: 0.00004468
Iteration 342/1000 | Loss: 0.00004468
Iteration 343/1000 | Loss: 0.00004468
Iteration 344/1000 | Loss: 0.00004468
Iteration 345/1000 | Loss: 0.00004468
Iteration 346/1000 | Loss: 0.00004468
Iteration 347/1000 | Loss: 0.00004467
Iteration 348/1000 | Loss: 0.00004467
Iteration 349/1000 | Loss: 0.00004467
Iteration 350/1000 | Loss: 0.00004467
Iteration 351/1000 | Loss: 0.00004467
Iteration 352/1000 | Loss: 0.00004467
Iteration 353/1000 | Loss: 0.00004466
Iteration 354/1000 | Loss: 0.00004466
Iteration 355/1000 | Loss: 0.00004466
Iteration 356/1000 | Loss: 0.00004466
Iteration 357/1000 | Loss: 0.00004466
Iteration 358/1000 | Loss: 0.00004466
Iteration 359/1000 | Loss: 0.00004466
Iteration 360/1000 | Loss: 0.00004466
Iteration 361/1000 | Loss: 0.00004466
Iteration 362/1000 | Loss: 0.00004466
Iteration 363/1000 | Loss: 0.00004466
Iteration 364/1000 | Loss: 0.00004466
Iteration 365/1000 | Loss: 0.00004466
Iteration 366/1000 | Loss: 0.00004466
Iteration 367/1000 | Loss: 0.00004465
Iteration 368/1000 | Loss: 0.00004465
Iteration 369/1000 | Loss: 0.00004465
Iteration 370/1000 | Loss: 0.00004465
Iteration 371/1000 | Loss: 0.00004465
Iteration 372/1000 | Loss: 0.00004465
Iteration 373/1000 | Loss: 0.00004465
Iteration 374/1000 | Loss: 0.00004465
Iteration 375/1000 | Loss: 0.00004465
Iteration 376/1000 | Loss: 0.00004465
Iteration 377/1000 | Loss: 0.00004465
Iteration 378/1000 | Loss: 0.00004465
Iteration 379/1000 | Loss: 0.00004465
Iteration 380/1000 | Loss: 0.00004465
Iteration 381/1000 | Loss: 0.00004465
Iteration 382/1000 | Loss: 0.00004465
Iteration 383/1000 | Loss: 0.00004465
Iteration 384/1000 | Loss: 0.00004465
Iteration 385/1000 | Loss: 0.00004465
Iteration 386/1000 | Loss: 0.00004465
Iteration 387/1000 | Loss: 0.00004465
Iteration 388/1000 | Loss: 0.00004465
Iteration 389/1000 | Loss: 0.00004465
Iteration 390/1000 | Loss: 0.00004465
Iteration 391/1000 | Loss: 0.00004465
Iteration 392/1000 | Loss: 0.00004465
Iteration 393/1000 | Loss: 0.00004465
Iteration 394/1000 | Loss: 0.00004465
Iteration 395/1000 | Loss: 0.00004465
Iteration 396/1000 | Loss: 0.00004465
Iteration 397/1000 | Loss: 0.00004465
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 397. Stopping optimization.
Last 5 losses: [4.464859375730157e-05, 4.464859375730157e-05, 4.464859375730157e-05, 4.464859375730157e-05, 4.464859375730157e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.464859375730157e-05

Optimization complete. Final v2v error: 4.665724754333496 mm

Highest mean error: 10.61384105682373 mm for frame 44

Lowest mean error: 4.011376857757568 mm for frame 163

Saving results

Total time: 238.82158279418945
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00790839
Iteration 2/25 | Loss: 0.00156354
Iteration 3/25 | Loss: 0.00128763
Iteration 4/25 | Loss: 0.00125531
Iteration 5/25 | Loss: 0.00124749
Iteration 6/25 | Loss: 0.00124555
Iteration 7/25 | Loss: 0.00124546
Iteration 8/25 | Loss: 0.00124546
Iteration 9/25 | Loss: 0.00124546
Iteration 10/25 | Loss: 0.00124546
Iteration 11/25 | Loss: 0.00124546
Iteration 12/25 | Loss: 0.00124546
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012454581446945667, 0.0012454581446945667, 0.0012454581446945667, 0.0012454581446945667, 0.0012454581446945667]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012454581446945667

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.08885062
Iteration 2/25 | Loss: 0.00215126
Iteration 3/25 | Loss: 0.00215126
Iteration 4/25 | Loss: 0.00215126
Iteration 5/25 | Loss: 0.00215126
Iteration 6/25 | Loss: 0.00215126
Iteration 7/25 | Loss: 0.00215126
Iteration 8/25 | Loss: 0.00215126
Iteration 9/25 | Loss: 0.00215125
Iteration 10/25 | Loss: 0.00215125
Iteration 11/25 | Loss: 0.00215125
Iteration 12/25 | Loss: 0.00215125
Iteration 13/25 | Loss: 0.00215125
Iteration 14/25 | Loss: 0.00215125
Iteration 15/25 | Loss: 0.00215125
Iteration 16/25 | Loss: 0.00215125
Iteration 17/25 | Loss: 0.00215125
Iteration 18/25 | Loss: 0.00215125
Iteration 19/25 | Loss: 0.00215125
Iteration 20/25 | Loss: 0.00215125
Iteration 21/25 | Loss: 0.00215125
Iteration 22/25 | Loss: 0.00215125
Iteration 23/25 | Loss: 0.00215125
Iteration 24/25 | Loss: 0.00215125
Iteration 25/25 | Loss: 0.00215125

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00215125
Iteration 2/1000 | Loss: 0.00008435
Iteration 3/1000 | Loss: 0.00005310
Iteration 4/1000 | Loss: 0.00004103
Iteration 5/1000 | Loss: 0.00003686
Iteration 6/1000 | Loss: 0.00003525
Iteration 7/1000 | Loss: 0.00003410
Iteration 8/1000 | Loss: 0.00003313
Iteration 9/1000 | Loss: 0.00003251
Iteration 10/1000 | Loss: 0.00003199
Iteration 11/1000 | Loss: 0.00003160
Iteration 12/1000 | Loss: 0.00003124
Iteration 13/1000 | Loss: 0.00003094
Iteration 14/1000 | Loss: 0.00003073
Iteration 15/1000 | Loss: 0.00003052
Iteration 16/1000 | Loss: 0.00003028
Iteration 17/1000 | Loss: 0.00003011
Iteration 18/1000 | Loss: 0.00003001
Iteration 19/1000 | Loss: 0.00003000
Iteration 20/1000 | Loss: 0.00002994
Iteration 21/1000 | Loss: 0.00002988
Iteration 22/1000 | Loss: 0.00002984
Iteration 23/1000 | Loss: 0.00002981
Iteration 24/1000 | Loss: 0.00002975
Iteration 25/1000 | Loss: 0.00002972
Iteration 26/1000 | Loss: 0.00002971
Iteration 27/1000 | Loss: 0.00002971
Iteration 28/1000 | Loss: 0.00002970
Iteration 29/1000 | Loss: 0.00002970
Iteration 30/1000 | Loss: 0.00002968
Iteration 31/1000 | Loss: 0.00002967
Iteration 32/1000 | Loss: 0.00002967
Iteration 33/1000 | Loss: 0.00002966
Iteration 34/1000 | Loss: 0.00002966
Iteration 35/1000 | Loss: 0.00002966
Iteration 36/1000 | Loss: 0.00002966
Iteration 37/1000 | Loss: 0.00002965
Iteration 38/1000 | Loss: 0.00002964
Iteration 39/1000 | Loss: 0.00002964
Iteration 40/1000 | Loss: 0.00002964
Iteration 41/1000 | Loss: 0.00002963
Iteration 42/1000 | Loss: 0.00002963
Iteration 43/1000 | Loss: 0.00002962
Iteration 44/1000 | Loss: 0.00002961
Iteration 45/1000 | Loss: 0.00002960
Iteration 46/1000 | Loss: 0.00002960
Iteration 47/1000 | Loss: 0.00002960
Iteration 48/1000 | Loss: 0.00002959
Iteration 49/1000 | Loss: 0.00002958
Iteration 50/1000 | Loss: 0.00002957
Iteration 51/1000 | Loss: 0.00002956
Iteration 52/1000 | Loss: 0.00002956
Iteration 53/1000 | Loss: 0.00002956
Iteration 54/1000 | Loss: 0.00002956
Iteration 55/1000 | Loss: 0.00002956
Iteration 56/1000 | Loss: 0.00002956
Iteration 57/1000 | Loss: 0.00002955
Iteration 58/1000 | Loss: 0.00002955
Iteration 59/1000 | Loss: 0.00002954
Iteration 60/1000 | Loss: 0.00002953
Iteration 61/1000 | Loss: 0.00002953
Iteration 62/1000 | Loss: 0.00002953
Iteration 63/1000 | Loss: 0.00002952
Iteration 64/1000 | Loss: 0.00002952
Iteration 65/1000 | Loss: 0.00002951
Iteration 66/1000 | Loss: 0.00002951
Iteration 67/1000 | Loss: 0.00002951
Iteration 68/1000 | Loss: 0.00002951
Iteration 69/1000 | Loss: 0.00002950
Iteration 70/1000 | Loss: 0.00002950
Iteration 71/1000 | Loss: 0.00002950
Iteration 72/1000 | Loss: 0.00002949
Iteration 73/1000 | Loss: 0.00002949
Iteration 74/1000 | Loss: 0.00002949
Iteration 75/1000 | Loss: 0.00002948
Iteration 76/1000 | Loss: 0.00002948
Iteration 77/1000 | Loss: 0.00002947
Iteration 78/1000 | Loss: 0.00002947
Iteration 79/1000 | Loss: 0.00002947
Iteration 80/1000 | Loss: 0.00002947
Iteration 81/1000 | Loss: 0.00002947
Iteration 82/1000 | Loss: 0.00002947
Iteration 83/1000 | Loss: 0.00002947
Iteration 84/1000 | Loss: 0.00002947
Iteration 85/1000 | Loss: 0.00002946
Iteration 86/1000 | Loss: 0.00002946
Iteration 87/1000 | Loss: 0.00002946
Iteration 88/1000 | Loss: 0.00002946
Iteration 89/1000 | Loss: 0.00002945
Iteration 90/1000 | Loss: 0.00002945
Iteration 91/1000 | Loss: 0.00002945
Iteration 92/1000 | Loss: 0.00002944
Iteration 93/1000 | Loss: 0.00002944
Iteration 94/1000 | Loss: 0.00002944
Iteration 95/1000 | Loss: 0.00002944
Iteration 96/1000 | Loss: 0.00002944
Iteration 97/1000 | Loss: 0.00002944
Iteration 98/1000 | Loss: 0.00002943
Iteration 99/1000 | Loss: 0.00002943
Iteration 100/1000 | Loss: 0.00002943
Iteration 101/1000 | Loss: 0.00002943
Iteration 102/1000 | Loss: 0.00002943
Iteration 103/1000 | Loss: 0.00002943
Iteration 104/1000 | Loss: 0.00002943
Iteration 105/1000 | Loss: 0.00002943
Iteration 106/1000 | Loss: 0.00002943
Iteration 107/1000 | Loss: 0.00002942
Iteration 108/1000 | Loss: 0.00002942
Iteration 109/1000 | Loss: 0.00002942
Iteration 110/1000 | Loss: 0.00002942
Iteration 111/1000 | Loss: 0.00002942
Iteration 112/1000 | Loss: 0.00002942
Iteration 113/1000 | Loss: 0.00002942
Iteration 114/1000 | Loss: 0.00002941
Iteration 115/1000 | Loss: 0.00002941
Iteration 116/1000 | Loss: 0.00002941
Iteration 117/1000 | Loss: 0.00002941
Iteration 118/1000 | Loss: 0.00002940
Iteration 119/1000 | Loss: 0.00002940
Iteration 120/1000 | Loss: 0.00002940
Iteration 121/1000 | Loss: 0.00002940
Iteration 122/1000 | Loss: 0.00002940
Iteration 123/1000 | Loss: 0.00002940
Iteration 124/1000 | Loss: 0.00002940
Iteration 125/1000 | Loss: 0.00002940
Iteration 126/1000 | Loss: 0.00002940
Iteration 127/1000 | Loss: 0.00002940
Iteration 128/1000 | Loss: 0.00002940
Iteration 129/1000 | Loss: 0.00002940
Iteration 130/1000 | Loss: 0.00002940
Iteration 131/1000 | Loss: 0.00002940
Iteration 132/1000 | Loss: 0.00002940
Iteration 133/1000 | Loss: 0.00002940
Iteration 134/1000 | Loss: 0.00002940
Iteration 135/1000 | Loss: 0.00002940
Iteration 136/1000 | Loss: 0.00002940
Iteration 137/1000 | Loss: 0.00002940
Iteration 138/1000 | Loss: 0.00002940
Iteration 139/1000 | Loss: 0.00002940
Iteration 140/1000 | Loss: 0.00002940
Iteration 141/1000 | Loss: 0.00002940
Iteration 142/1000 | Loss: 0.00002940
Iteration 143/1000 | Loss: 0.00002940
Iteration 144/1000 | Loss: 0.00002940
Iteration 145/1000 | Loss: 0.00002940
Iteration 146/1000 | Loss: 0.00002940
Iteration 147/1000 | Loss: 0.00002940
Iteration 148/1000 | Loss: 0.00002940
Iteration 149/1000 | Loss: 0.00002940
Iteration 150/1000 | Loss: 0.00002940
Iteration 151/1000 | Loss: 0.00002940
Iteration 152/1000 | Loss: 0.00002940
Iteration 153/1000 | Loss: 0.00002940
Iteration 154/1000 | Loss: 0.00002940
Iteration 155/1000 | Loss: 0.00002940
Iteration 156/1000 | Loss: 0.00002940
Iteration 157/1000 | Loss: 0.00002940
Iteration 158/1000 | Loss: 0.00002940
Iteration 159/1000 | Loss: 0.00002940
Iteration 160/1000 | Loss: 0.00002940
Iteration 161/1000 | Loss: 0.00002940
Iteration 162/1000 | Loss: 0.00002940
Iteration 163/1000 | Loss: 0.00002940
Iteration 164/1000 | Loss: 0.00002940
Iteration 165/1000 | Loss: 0.00002940
Iteration 166/1000 | Loss: 0.00002940
Iteration 167/1000 | Loss: 0.00002940
Iteration 168/1000 | Loss: 0.00002940
Iteration 169/1000 | Loss: 0.00002940
Iteration 170/1000 | Loss: 0.00002940
Iteration 171/1000 | Loss: 0.00002940
Iteration 172/1000 | Loss: 0.00002940
Iteration 173/1000 | Loss: 0.00002940
Iteration 174/1000 | Loss: 0.00002940
Iteration 175/1000 | Loss: 0.00002940
Iteration 176/1000 | Loss: 0.00002940
Iteration 177/1000 | Loss: 0.00002940
Iteration 178/1000 | Loss: 0.00002940
Iteration 179/1000 | Loss: 0.00002940
Iteration 180/1000 | Loss: 0.00002940
Iteration 181/1000 | Loss: 0.00002940
Iteration 182/1000 | Loss: 0.00002940
Iteration 183/1000 | Loss: 0.00002940
Iteration 184/1000 | Loss: 0.00002940
Iteration 185/1000 | Loss: 0.00002940
Iteration 186/1000 | Loss: 0.00002940
Iteration 187/1000 | Loss: 0.00002940
Iteration 188/1000 | Loss: 0.00002940
Iteration 189/1000 | Loss: 0.00002940
Iteration 190/1000 | Loss: 0.00002940
Iteration 191/1000 | Loss: 0.00002940
Iteration 192/1000 | Loss: 0.00002940
Iteration 193/1000 | Loss: 0.00002940
Iteration 194/1000 | Loss: 0.00002940
Iteration 195/1000 | Loss: 0.00002940
Iteration 196/1000 | Loss: 0.00002940
Iteration 197/1000 | Loss: 0.00002940
Iteration 198/1000 | Loss: 0.00002940
Iteration 199/1000 | Loss: 0.00002940
Iteration 200/1000 | Loss: 0.00002940
Iteration 201/1000 | Loss: 0.00002940
Iteration 202/1000 | Loss: 0.00002940
Iteration 203/1000 | Loss: 0.00002940
Iteration 204/1000 | Loss: 0.00002940
Iteration 205/1000 | Loss: 0.00002940
Iteration 206/1000 | Loss: 0.00002940
Iteration 207/1000 | Loss: 0.00002940
Iteration 208/1000 | Loss: 0.00002940
Iteration 209/1000 | Loss: 0.00002940
Iteration 210/1000 | Loss: 0.00002940
Iteration 211/1000 | Loss: 0.00002940
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [2.9398277547443286e-05, 2.9398277547443286e-05, 2.9398277547443286e-05, 2.9398277547443286e-05, 2.9398277547443286e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9398277547443286e-05

Optimization complete. Final v2v error: 4.3512187004089355 mm

Highest mean error: 5.30633544921875 mm for frame 81

Lowest mean error: 3.607848882675171 mm for frame 12

Saving results

Total time: 50.820308685302734
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01030331
Iteration 2/25 | Loss: 0.00166391
Iteration 3/25 | Loss: 0.00129767
Iteration 4/25 | Loss: 0.00128170
Iteration 5/25 | Loss: 0.00127847
Iteration 6/25 | Loss: 0.00127780
Iteration 7/25 | Loss: 0.00127780
Iteration 8/25 | Loss: 0.00127780
Iteration 9/25 | Loss: 0.00127780
Iteration 10/25 | Loss: 0.00127780
Iteration 11/25 | Loss: 0.00127780
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012778034433722496, 0.0012778034433722496, 0.0012778034433722496, 0.0012778034433722496, 0.0012778034433722496]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012778034433722496

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.39649785
Iteration 2/25 | Loss: 0.00153610
Iteration 3/25 | Loss: 0.00153610
Iteration 4/25 | Loss: 0.00153610
Iteration 5/25 | Loss: 0.00153610
Iteration 6/25 | Loss: 0.00153610
Iteration 7/25 | Loss: 0.00153610
Iteration 8/25 | Loss: 0.00153610
Iteration 9/25 | Loss: 0.00153610
Iteration 10/25 | Loss: 0.00153610
Iteration 11/25 | Loss: 0.00153610
Iteration 12/25 | Loss: 0.00153610
Iteration 13/25 | Loss: 0.00153610
Iteration 14/25 | Loss: 0.00153610
Iteration 15/25 | Loss: 0.00153610
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0015360985416918993, 0.0015360985416918993, 0.0015360985416918993, 0.0015360985416918993, 0.0015360985416918993]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015360985416918993

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00153610
Iteration 2/1000 | Loss: 0.00004475
Iteration 3/1000 | Loss: 0.00003034
Iteration 4/1000 | Loss: 0.00002565
Iteration 5/1000 | Loss: 0.00002388
Iteration 6/1000 | Loss: 0.00002297
Iteration 7/1000 | Loss: 0.00002230
Iteration 8/1000 | Loss: 0.00002185
Iteration 9/1000 | Loss: 0.00002150
Iteration 10/1000 | Loss: 0.00002121
Iteration 11/1000 | Loss: 0.00002104
Iteration 12/1000 | Loss: 0.00002088
Iteration 13/1000 | Loss: 0.00002073
Iteration 14/1000 | Loss: 0.00002072
Iteration 15/1000 | Loss: 0.00002071
Iteration 16/1000 | Loss: 0.00002062
Iteration 17/1000 | Loss: 0.00002057
Iteration 18/1000 | Loss: 0.00002055
Iteration 19/1000 | Loss: 0.00002052
Iteration 20/1000 | Loss: 0.00002048
Iteration 21/1000 | Loss: 0.00002046
Iteration 22/1000 | Loss: 0.00002045
Iteration 23/1000 | Loss: 0.00002044
Iteration 24/1000 | Loss: 0.00002043
Iteration 25/1000 | Loss: 0.00002042
Iteration 26/1000 | Loss: 0.00002042
Iteration 27/1000 | Loss: 0.00002041
Iteration 28/1000 | Loss: 0.00002041
Iteration 29/1000 | Loss: 0.00002041
Iteration 30/1000 | Loss: 0.00002041
Iteration 31/1000 | Loss: 0.00002041
Iteration 32/1000 | Loss: 0.00002041
Iteration 33/1000 | Loss: 0.00002039
Iteration 34/1000 | Loss: 0.00002038
Iteration 35/1000 | Loss: 0.00002037
Iteration 36/1000 | Loss: 0.00002036
Iteration 37/1000 | Loss: 0.00002035
Iteration 38/1000 | Loss: 0.00002034
Iteration 39/1000 | Loss: 0.00002033
Iteration 40/1000 | Loss: 0.00002033
Iteration 41/1000 | Loss: 0.00002033
Iteration 42/1000 | Loss: 0.00002033
Iteration 43/1000 | Loss: 0.00002033
Iteration 44/1000 | Loss: 0.00002032
Iteration 45/1000 | Loss: 0.00002032
Iteration 46/1000 | Loss: 0.00002031
Iteration 47/1000 | Loss: 0.00002031
Iteration 48/1000 | Loss: 0.00002031
Iteration 49/1000 | Loss: 0.00002031
Iteration 50/1000 | Loss: 0.00002030
Iteration 51/1000 | Loss: 0.00002030
Iteration 52/1000 | Loss: 0.00002030
Iteration 53/1000 | Loss: 0.00002026
Iteration 54/1000 | Loss: 0.00002026
Iteration 55/1000 | Loss: 0.00002026
Iteration 56/1000 | Loss: 0.00002026
Iteration 57/1000 | Loss: 0.00002026
Iteration 58/1000 | Loss: 0.00002026
Iteration 59/1000 | Loss: 0.00002026
Iteration 60/1000 | Loss: 0.00002025
Iteration 61/1000 | Loss: 0.00002025
Iteration 62/1000 | Loss: 0.00002025
Iteration 63/1000 | Loss: 0.00002025
Iteration 64/1000 | Loss: 0.00002025
Iteration 65/1000 | Loss: 0.00002025
Iteration 66/1000 | Loss: 0.00002025
Iteration 67/1000 | Loss: 0.00002025
Iteration 68/1000 | Loss: 0.00002025
Iteration 69/1000 | Loss: 0.00002025
Iteration 70/1000 | Loss: 0.00002025
Iteration 71/1000 | Loss: 0.00002024
Iteration 72/1000 | Loss: 0.00002023
Iteration 73/1000 | Loss: 0.00002023
Iteration 74/1000 | Loss: 0.00002023
Iteration 75/1000 | Loss: 0.00002023
Iteration 76/1000 | Loss: 0.00002023
Iteration 77/1000 | Loss: 0.00002022
Iteration 78/1000 | Loss: 0.00002022
Iteration 79/1000 | Loss: 0.00002022
Iteration 80/1000 | Loss: 0.00002022
Iteration 81/1000 | Loss: 0.00002022
Iteration 82/1000 | Loss: 0.00002022
Iteration 83/1000 | Loss: 0.00002022
Iteration 84/1000 | Loss: 0.00002022
Iteration 85/1000 | Loss: 0.00002021
Iteration 86/1000 | Loss: 0.00002021
Iteration 87/1000 | Loss: 0.00002020
Iteration 88/1000 | Loss: 0.00002020
Iteration 89/1000 | Loss: 0.00002018
Iteration 90/1000 | Loss: 0.00002018
Iteration 91/1000 | Loss: 0.00002017
Iteration 92/1000 | Loss: 0.00002017
Iteration 93/1000 | Loss: 0.00002016
Iteration 94/1000 | Loss: 0.00002016
Iteration 95/1000 | Loss: 0.00002016
Iteration 96/1000 | Loss: 0.00002014
Iteration 97/1000 | Loss: 0.00002014
Iteration 98/1000 | Loss: 0.00002014
Iteration 99/1000 | Loss: 0.00002014
Iteration 100/1000 | Loss: 0.00002014
Iteration 101/1000 | Loss: 0.00002014
Iteration 102/1000 | Loss: 0.00002014
Iteration 103/1000 | Loss: 0.00002014
Iteration 104/1000 | Loss: 0.00002013
Iteration 105/1000 | Loss: 0.00002013
Iteration 106/1000 | Loss: 0.00002013
Iteration 107/1000 | Loss: 0.00002013
Iteration 108/1000 | Loss: 0.00002013
Iteration 109/1000 | Loss: 0.00002012
Iteration 110/1000 | Loss: 0.00002012
Iteration 111/1000 | Loss: 0.00002012
Iteration 112/1000 | Loss: 0.00002012
Iteration 113/1000 | Loss: 0.00002012
Iteration 114/1000 | Loss: 0.00002012
Iteration 115/1000 | Loss: 0.00002011
Iteration 116/1000 | Loss: 0.00002011
Iteration 117/1000 | Loss: 0.00002011
Iteration 118/1000 | Loss: 0.00002011
Iteration 119/1000 | Loss: 0.00002011
Iteration 120/1000 | Loss: 0.00002011
Iteration 121/1000 | Loss: 0.00002011
Iteration 122/1000 | Loss: 0.00002011
Iteration 123/1000 | Loss: 0.00002010
Iteration 124/1000 | Loss: 0.00002010
Iteration 125/1000 | Loss: 0.00002010
Iteration 126/1000 | Loss: 0.00002009
Iteration 127/1000 | Loss: 0.00002009
Iteration 128/1000 | Loss: 0.00002009
Iteration 129/1000 | Loss: 0.00002009
Iteration 130/1000 | Loss: 0.00002009
Iteration 131/1000 | Loss: 0.00002009
Iteration 132/1000 | Loss: 0.00002009
Iteration 133/1000 | Loss: 0.00002009
Iteration 134/1000 | Loss: 0.00002008
Iteration 135/1000 | Loss: 0.00002008
Iteration 136/1000 | Loss: 0.00002008
Iteration 137/1000 | Loss: 0.00002008
Iteration 138/1000 | Loss: 0.00002008
Iteration 139/1000 | Loss: 0.00002008
Iteration 140/1000 | Loss: 0.00002008
Iteration 141/1000 | Loss: 0.00002007
Iteration 142/1000 | Loss: 0.00002007
Iteration 143/1000 | Loss: 0.00002007
Iteration 144/1000 | Loss: 0.00002007
Iteration 145/1000 | Loss: 0.00002007
Iteration 146/1000 | Loss: 0.00002007
Iteration 147/1000 | Loss: 0.00002007
Iteration 148/1000 | Loss: 0.00002007
Iteration 149/1000 | Loss: 0.00002007
Iteration 150/1000 | Loss: 0.00002007
Iteration 151/1000 | Loss: 0.00002007
Iteration 152/1000 | Loss: 0.00002007
Iteration 153/1000 | Loss: 0.00002007
Iteration 154/1000 | Loss: 0.00002007
Iteration 155/1000 | Loss: 0.00002007
Iteration 156/1000 | Loss: 0.00002007
Iteration 157/1000 | Loss: 0.00002007
Iteration 158/1000 | Loss: 0.00002007
Iteration 159/1000 | Loss: 0.00002007
Iteration 160/1000 | Loss: 0.00002007
Iteration 161/1000 | Loss: 0.00002007
Iteration 162/1000 | Loss: 0.00002007
Iteration 163/1000 | Loss: 0.00002007
Iteration 164/1000 | Loss: 0.00002007
Iteration 165/1000 | Loss: 0.00002007
Iteration 166/1000 | Loss: 0.00002007
Iteration 167/1000 | Loss: 0.00002007
Iteration 168/1000 | Loss: 0.00002007
Iteration 169/1000 | Loss: 0.00002007
Iteration 170/1000 | Loss: 0.00002007
Iteration 171/1000 | Loss: 0.00002007
Iteration 172/1000 | Loss: 0.00002007
Iteration 173/1000 | Loss: 0.00002007
Iteration 174/1000 | Loss: 0.00002007
Iteration 175/1000 | Loss: 0.00002007
Iteration 176/1000 | Loss: 0.00002007
Iteration 177/1000 | Loss: 0.00002007
Iteration 178/1000 | Loss: 0.00002007
Iteration 179/1000 | Loss: 0.00002007
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [2.006862087000627e-05, 2.006862087000627e-05, 2.006862087000627e-05, 2.006862087000627e-05, 2.006862087000627e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.006862087000627e-05

Optimization complete. Final v2v error: 3.6797568798065186 mm

Highest mean error: 4.46729850769043 mm for frame 25

Lowest mean error: 2.761852979660034 mm for frame 0

Saving results

Total time: 43.961867809295654
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_felice_posed_011/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_felice_posed_011/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00396692
Iteration 2/25 | Loss: 0.00121165
Iteration 3/25 | Loss: 0.00114465
Iteration 4/25 | Loss: 0.00113384
Iteration 5/25 | Loss: 0.00113042
Iteration 6/25 | Loss: 0.00112955
Iteration 7/25 | Loss: 0.00112955
Iteration 8/25 | Loss: 0.00112955
Iteration 9/25 | Loss: 0.00112955
Iteration 10/25 | Loss: 0.00112955
Iteration 11/25 | Loss: 0.00112955
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011295503936707973, 0.0011295503936707973, 0.0011295503936707973, 0.0011295503936707973, 0.0011295503936707973]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011295503936707973

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.72656488
Iteration 2/25 | Loss: 0.00191172
Iteration 3/25 | Loss: 0.00191171
Iteration 4/25 | Loss: 0.00191171
Iteration 5/25 | Loss: 0.00191171
Iteration 6/25 | Loss: 0.00191171
Iteration 7/25 | Loss: 0.00191171
Iteration 8/25 | Loss: 0.00191171
Iteration 9/25 | Loss: 0.00191171
Iteration 10/25 | Loss: 0.00191171
Iteration 11/25 | Loss: 0.00191171
Iteration 12/25 | Loss: 0.00191171
Iteration 13/25 | Loss: 0.00191171
Iteration 14/25 | Loss: 0.00191171
Iteration 15/25 | Loss: 0.00191171
Iteration 16/25 | Loss: 0.00191171
Iteration 17/25 | Loss: 0.00191171
Iteration 18/25 | Loss: 0.00191171
Iteration 19/25 | Loss: 0.00191171
Iteration 20/25 | Loss: 0.00191171
Iteration 21/25 | Loss: 0.00191171
Iteration 22/25 | Loss: 0.00191171
Iteration 23/25 | Loss: 0.00191171
Iteration 24/25 | Loss: 0.00191171
Iteration 25/25 | Loss: 0.00191171

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00191171
Iteration 2/1000 | Loss: 0.00002125
Iteration 3/1000 | Loss: 0.00001541
Iteration 4/1000 | Loss: 0.00001307
Iteration 5/1000 | Loss: 0.00001215
Iteration 6/1000 | Loss: 0.00001141
Iteration 7/1000 | Loss: 0.00001100
Iteration 8/1000 | Loss: 0.00001056
Iteration 9/1000 | Loss: 0.00001029
Iteration 10/1000 | Loss: 0.00001017
Iteration 11/1000 | Loss: 0.00001004
Iteration 12/1000 | Loss: 0.00001003
Iteration 13/1000 | Loss: 0.00000987
Iteration 14/1000 | Loss: 0.00000980
Iteration 15/1000 | Loss: 0.00000980
Iteration 16/1000 | Loss: 0.00000976
Iteration 17/1000 | Loss: 0.00000975
Iteration 18/1000 | Loss: 0.00000973
Iteration 19/1000 | Loss: 0.00000969
Iteration 20/1000 | Loss: 0.00000968
Iteration 21/1000 | Loss: 0.00000966
Iteration 22/1000 | Loss: 0.00000959
Iteration 23/1000 | Loss: 0.00000956
Iteration 24/1000 | Loss: 0.00000956
Iteration 25/1000 | Loss: 0.00000953
Iteration 26/1000 | Loss: 0.00000953
Iteration 27/1000 | Loss: 0.00000953
Iteration 28/1000 | Loss: 0.00000953
Iteration 29/1000 | Loss: 0.00000952
Iteration 30/1000 | Loss: 0.00000951
Iteration 31/1000 | Loss: 0.00000951
Iteration 32/1000 | Loss: 0.00000950
Iteration 33/1000 | Loss: 0.00000950
Iteration 34/1000 | Loss: 0.00000949
Iteration 35/1000 | Loss: 0.00000949
Iteration 36/1000 | Loss: 0.00000949
Iteration 37/1000 | Loss: 0.00000948
Iteration 38/1000 | Loss: 0.00000948
Iteration 39/1000 | Loss: 0.00000947
Iteration 40/1000 | Loss: 0.00000947
Iteration 41/1000 | Loss: 0.00000947
Iteration 42/1000 | Loss: 0.00000947
Iteration 43/1000 | Loss: 0.00000947
Iteration 44/1000 | Loss: 0.00000947
Iteration 45/1000 | Loss: 0.00000947
Iteration 46/1000 | Loss: 0.00000947
Iteration 47/1000 | Loss: 0.00000947
Iteration 48/1000 | Loss: 0.00000947
Iteration 49/1000 | Loss: 0.00000947
Iteration 50/1000 | Loss: 0.00000947
Iteration 51/1000 | Loss: 0.00000946
Iteration 52/1000 | Loss: 0.00000946
Iteration 53/1000 | Loss: 0.00000946
Iteration 54/1000 | Loss: 0.00000946
Iteration 55/1000 | Loss: 0.00000946
Iteration 56/1000 | Loss: 0.00000946
Iteration 57/1000 | Loss: 0.00000945
Iteration 58/1000 | Loss: 0.00000945
Iteration 59/1000 | Loss: 0.00000939
Iteration 60/1000 | Loss: 0.00000937
Iteration 61/1000 | Loss: 0.00000936
Iteration 62/1000 | Loss: 0.00000936
Iteration 63/1000 | Loss: 0.00000935
Iteration 64/1000 | Loss: 0.00000935
Iteration 65/1000 | Loss: 0.00000934
Iteration 66/1000 | Loss: 0.00000934
Iteration 67/1000 | Loss: 0.00000934
Iteration 68/1000 | Loss: 0.00000933
Iteration 69/1000 | Loss: 0.00000933
Iteration 70/1000 | Loss: 0.00000932
Iteration 71/1000 | Loss: 0.00000932
Iteration 72/1000 | Loss: 0.00000932
Iteration 73/1000 | Loss: 0.00000931
Iteration 74/1000 | Loss: 0.00000931
Iteration 75/1000 | Loss: 0.00000931
Iteration 76/1000 | Loss: 0.00000931
Iteration 77/1000 | Loss: 0.00000930
Iteration 78/1000 | Loss: 0.00000930
Iteration 79/1000 | Loss: 0.00000930
Iteration 80/1000 | Loss: 0.00000930
Iteration 81/1000 | Loss: 0.00000929
Iteration 82/1000 | Loss: 0.00000929
Iteration 83/1000 | Loss: 0.00000929
Iteration 84/1000 | Loss: 0.00000929
Iteration 85/1000 | Loss: 0.00000928
Iteration 86/1000 | Loss: 0.00000928
Iteration 87/1000 | Loss: 0.00000928
Iteration 88/1000 | Loss: 0.00000928
Iteration 89/1000 | Loss: 0.00000928
Iteration 90/1000 | Loss: 0.00000928
Iteration 91/1000 | Loss: 0.00000928
Iteration 92/1000 | Loss: 0.00000928
Iteration 93/1000 | Loss: 0.00000927
Iteration 94/1000 | Loss: 0.00000927
Iteration 95/1000 | Loss: 0.00000927
Iteration 96/1000 | Loss: 0.00000927
Iteration 97/1000 | Loss: 0.00000927
Iteration 98/1000 | Loss: 0.00000927
Iteration 99/1000 | Loss: 0.00000926
Iteration 100/1000 | Loss: 0.00000926
Iteration 101/1000 | Loss: 0.00000925
Iteration 102/1000 | Loss: 0.00000925
Iteration 103/1000 | Loss: 0.00000924
Iteration 104/1000 | Loss: 0.00000924
Iteration 105/1000 | Loss: 0.00000924
Iteration 106/1000 | Loss: 0.00000924
Iteration 107/1000 | Loss: 0.00000924
Iteration 108/1000 | Loss: 0.00000923
Iteration 109/1000 | Loss: 0.00000923
Iteration 110/1000 | Loss: 0.00000923
Iteration 111/1000 | Loss: 0.00000923
Iteration 112/1000 | Loss: 0.00000922
Iteration 113/1000 | Loss: 0.00000921
Iteration 114/1000 | Loss: 0.00000921
Iteration 115/1000 | Loss: 0.00000921
Iteration 116/1000 | Loss: 0.00000921
Iteration 117/1000 | Loss: 0.00000921
Iteration 118/1000 | Loss: 0.00000920
Iteration 119/1000 | Loss: 0.00000920
Iteration 120/1000 | Loss: 0.00000920
Iteration 121/1000 | Loss: 0.00000920
Iteration 122/1000 | Loss: 0.00000920
Iteration 123/1000 | Loss: 0.00000920
Iteration 124/1000 | Loss: 0.00000920
Iteration 125/1000 | Loss: 0.00000919
Iteration 126/1000 | Loss: 0.00000919
Iteration 127/1000 | Loss: 0.00000919
Iteration 128/1000 | Loss: 0.00000918
Iteration 129/1000 | Loss: 0.00000918
Iteration 130/1000 | Loss: 0.00000918
Iteration 131/1000 | Loss: 0.00000918
Iteration 132/1000 | Loss: 0.00000917
Iteration 133/1000 | Loss: 0.00000917
Iteration 134/1000 | Loss: 0.00000917
Iteration 135/1000 | Loss: 0.00000917
Iteration 136/1000 | Loss: 0.00000917
Iteration 137/1000 | Loss: 0.00000917
Iteration 138/1000 | Loss: 0.00000917
Iteration 139/1000 | Loss: 0.00000916
Iteration 140/1000 | Loss: 0.00000916
Iteration 141/1000 | Loss: 0.00000916
Iteration 142/1000 | Loss: 0.00000916
Iteration 143/1000 | Loss: 0.00000916
Iteration 144/1000 | Loss: 0.00000916
Iteration 145/1000 | Loss: 0.00000916
Iteration 146/1000 | Loss: 0.00000915
Iteration 147/1000 | Loss: 0.00000915
Iteration 148/1000 | Loss: 0.00000915
Iteration 149/1000 | Loss: 0.00000915
Iteration 150/1000 | Loss: 0.00000915
Iteration 151/1000 | Loss: 0.00000915
Iteration 152/1000 | Loss: 0.00000915
Iteration 153/1000 | Loss: 0.00000915
Iteration 154/1000 | Loss: 0.00000915
Iteration 155/1000 | Loss: 0.00000914
Iteration 156/1000 | Loss: 0.00000914
Iteration 157/1000 | Loss: 0.00000914
Iteration 158/1000 | Loss: 0.00000914
Iteration 159/1000 | Loss: 0.00000914
Iteration 160/1000 | Loss: 0.00000914
Iteration 161/1000 | Loss: 0.00000914
Iteration 162/1000 | Loss: 0.00000914
Iteration 163/1000 | Loss: 0.00000914
Iteration 164/1000 | Loss: 0.00000914
Iteration 165/1000 | Loss: 0.00000914
Iteration 166/1000 | Loss: 0.00000914
Iteration 167/1000 | Loss: 0.00000914
Iteration 168/1000 | Loss: 0.00000913
Iteration 169/1000 | Loss: 0.00000913
Iteration 170/1000 | Loss: 0.00000913
Iteration 171/1000 | Loss: 0.00000913
Iteration 172/1000 | Loss: 0.00000913
Iteration 173/1000 | Loss: 0.00000913
Iteration 174/1000 | Loss: 0.00000913
Iteration 175/1000 | Loss: 0.00000913
Iteration 176/1000 | Loss: 0.00000913
Iteration 177/1000 | Loss: 0.00000913
Iteration 178/1000 | Loss: 0.00000913
Iteration 179/1000 | Loss: 0.00000913
Iteration 180/1000 | Loss: 0.00000913
Iteration 181/1000 | Loss: 0.00000913
Iteration 182/1000 | Loss: 0.00000913
Iteration 183/1000 | Loss: 0.00000913
Iteration 184/1000 | Loss: 0.00000913
Iteration 185/1000 | Loss: 0.00000913
Iteration 186/1000 | Loss: 0.00000912
Iteration 187/1000 | Loss: 0.00000912
Iteration 188/1000 | Loss: 0.00000912
Iteration 189/1000 | Loss: 0.00000912
Iteration 190/1000 | Loss: 0.00000912
Iteration 191/1000 | Loss: 0.00000912
Iteration 192/1000 | Loss: 0.00000912
Iteration 193/1000 | Loss: 0.00000912
Iteration 194/1000 | Loss: 0.00000912
Iteration 195/1000 | Loss: 0.00000912
Iteration 196/1000 | Loss: 0.00000912
Iteration 197/1000 | Loss: 0.00000912
Iteration 198/1000 | Loss: 0.00000912
Iteration 199/1000 | Loss: 0.00000912
Iteration 200/1000 | Loss: 0.00000912
Iteration 201/1000 | Loss: 0.00000912
Iteration 202/1000 | Loss: 0.00000912
Iteration 203/1000 | Loss: 0.00000911
Iteration 204/1000 | Loss: 0.00000911
Iteration 205/1000 | Loss: 0.00000911
Iteration 206/1000 | Loss: 0.00000911
Iteration 207/1000 | Loss: 0.00000911
Iteration 208/1000 | Loss: 0.00000911
Iteration 209/1000 | Loss: 0.00000911
Iteration 210/1000 | Loss: 0.00000911
Iteration 211/1000 | Loss: 0.00000911
Iteration 212/1000 | Loss: 0.00000911
Iteration 213/1000 | Loss: 0.00000911
Iteration 214/1000 | Loss: 0.00000911
Iteration 215/1000 | Loss: 0.00000911
Iteration 216/1000 | Loss: 0.00000911
Iteration 217/1000 | Loss: 0.00000911
Iteration 218/1000 | Loss: 0.00000911
Iteration 219/1000 | Loss: 0.00000911
Iteration 220/1000 | Loss: 0.00000910
Iteration 221/1000 | Loss: 0.00000910
Iteration 222/1000 | Loss: 0.00000910
Iteration 223/1000 | Loss: 0.00000910
Iteration 224/1000 | Loss: 0.00000910
Iteration 225/1000 | Loss: 0.00000910
Iteration 226/1000 | Loss: 0.00000910
Iteration 227/1000 | Loss: 0.00000910
Iteration 228/1000 | Loss: 0.00000910
Iteration 229/1000 | Loss: 0.00000910
Iteration 230/1000 | Loss: 0.00000910
Iteration 231/1000 | Loss: 0.00000910
Iteration 232/1000 | Loss: 0.00000910
Iteration 233/1000 | Loss: 0.00000909
Iteration 234/1000 | Loss: 0.00000909
Iteration 235/1000 | Loss: 0.00000909
Iteration 236/1000 | Loss: 0.00000909
Iteration 237/1000 | Loss: 0.00000909
Iteration 238/1000 | Loss: 0.00000909
Iteration 239/1000 | Loss: 0.00000909
Iteration 240/1000 | Loss: 0.00000909
Iteration 241/1000 | Loss: 0.00000909
Iteration 242/1000 | Loss: 0.00000909
Iteration 243/1000 | Loss: 0.00000909
Iteration 244/1000 | Loss: 0.00000908
Iteration 245/1000 | Loss: 0.00000908
Iteration 246/1000 | Loss: 0.00000908
Iteration 247/1000 | Loss: 0.00000908
Iteration 248/1000 | Loss: 0.00000908
Iteration 249/1000 | Loss: 0.00000908
Iteration 250/1000 | Loss: 0.00000908
Iteration 251/1000 | Loss: 0.00000908
Iteration 252/1000 | Loss: 0.00000908
Iteration 253/1000 | Loss: 0.00000908
Iteration 254/1000 | Loss: 0.00000908
Iteration 255/1000 | Loss: 0.00000908
Iteration 256/1000 | Loss: 0.00000908
Iteration 257/1000 | Loss: 0.00000908
Iteration 258/1000 | Loss: 0.00000907
Iteration 259/1000 | Loss: 0.00000907
Iteration 260/1000 | Loss: 0.00000907
Iteration 261/1000 | Loss: 0.00000907
Iteration 262/1000 | Loss: 0.00000907
Iteration 263/1000 | Loss: 0.00000907
Iteration 264/1000 | Loss: 0.00000907
Iteration 265/1000 | Loss: 0.00000907
Iteration 266/1000 | Loss: 0.00000907
Iteration 267/1000 | Loss: 0.00000906
Iteration 268/1000 | Loss: 0.00000906
Iteration 269/1000 | Loss: 0.00000906
Iteration 270/1000 | Loss: 0.00000906
Iteration 271/1000 | Loss: 0.00000906
Iteration 272/1000 | Loss: 0.00000906
Iteration 273/1000 | Loss: 0.00000906
Iteration 274/1000 | Loss: 0.00000906
Iteration 275/1000 | Loss: 0.00000906
Iteration 276/1000 | Loss: 0.00000906
Iteration 277/1000 | Loss: 0.00000906
Iteration 278/1000 | Loss: 0.00000906
Iteration 279/1000 | Loss: 0.00000906
Iteration 280/1000 | Loss: 0.00000906
Iteration 281/1000 | Loss: 0.00000906
Iteration 282/1000 | Loss: 0.00000906
Iteration 283/1000 | Loss: 0.00000906
Iteration 284/1000 | Loss: 0.00000906
Iteration 285/1000 | Loss: 0.00000906
Iteration 286/1000 | Loss: 0.00000906
Iteration 287/1000 | Loss: 0.00000906
Iteration 288/1000 | Loss: 0.00000906
Iteration 289/1000 | Loss: 0.00000906
Iteration 290/1000 | Loss: 0.00000906
Iteration 291/1000 | Loss: 0.00000906
Iteration 292/1000 | Loss: 0.00000906
Iteration 293/1000 | Loss: 0.00000906
Iteration 294/1000 | Loss: 0.00000906
Iteration 295/1000 | Loss: 0.00000906
Iteration 296/1000 | Loss: 0.00000906
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 296. Stopping optimization.
Last 5 losses: [9.063718607649207e-06, 9.063718607649207e-06, 9.063718607649207e-06, 9.063718607649207e-06, 9.063718607649207e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.063718607649207e-06

Optimization complete. Final v2v error: 2.626415729522705 mm

Highest mean error: 2.9721109867095947 mm for frame 93

Lowest mean error: 2.3823349475860596 mm for frame 29

Saving results

Total time: 44.7640380859375
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_29_us_0610/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0610/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_29_us_0610/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00893508
Iteration 2/25 | Loss: 0.00112087
Iteration 3/25 | Loss: 0.00076553
Iteration 4/25 | Loss: 0.00069391
Iteration 5/25 | Loss: 0.00067933
Iteration 6/25 | Loss: 0.00067666
Iteration 7/25 | Loss: 0.00067626
Iteration 8/25 | Loss: 0.00067626
Iteration 9/25 | Loss: 0.00067626
Iteration 10/25 | Loss: 0.00067626
Iteration 11/25 | Loss: 0.00067626
Iteration 12/25 | Loss: 0.00067626
Iteration 13/25 | Loss: 0.00067626
Iteration 14/25 | Loss: 0.00067626
Iteration 15/25 | Loss: 0.00067626
Iteration 16/25 | Loss: 0.00067626
Iteration 17/25 | Loss: 0.00067626
Iteration 18/25 | Loss: 0.00067626
Iteration 19/25 | Loss: 0.00067626
Iteration 20/25 | Loss: 0.00067626
Iteration 21/25 | Loss: 0.00067626
Iteration 22/25 | Loss: 0.00067626
Iteration 23/25 | Loss: 0.00067626
Iteration 24/25 | Loss: 0.00067626
Iteration 25/25 | Loss: 0.00067626

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47858977
Iteration 2/25 | Loss: 0.00045402
Iteration 3/25 | Loss: 0.00045402
Iteration 4/25 | Loss: 0.00045402
Iteration 5/25 | Loss: 0.00045402
Iteration 6/25 | Loss: 0.00045402
Iteration 7/25 | Loss: 0.00045402
Iteration 8/25 | Loss: 0.00045402
Iteration 9/25 | Loss: 0.00045402
Iteration 10/25 | Loss: 0.00045402
Iteration 11/25 | Loss: 0.00045402
Iteration 12/25 | Loss: 0.00045402
Iteration 13/25 | Loss: 0.00045402
Iteration 14/25 | Loss: 0.00045402
Iteration 15/25 | Loss: 0.00045402
Iteration 16/25 | Loss: 0.00045402
Iteration 17/25 | Loss: 0.00045402
Iteration 18/25 | Loss: 0.00045402
Iteration 19/25 | Loss: 0.00045402
Iteration 20/25 | Loss: 0.00045402
Iteration 21/25 | Loss: 0.00045402
Iteration 22/25 | Loss: 0.00045402
Iteration 23/25 | Loss: 0.00045402
Iteration 24/25 | Loss: 0.00045402
Iteration 25/25 | Loss: 0.00045402

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045402
Iteration 2/1000 | Loss: 0.00003604
Iteration 3/1000 | Loss: 0.00002198
Iteration 4/1000 | Loss: 0.00001829
Iteration 5/1000 | Loss: 0.00001709
Iteration 6/1000 | Loss: 0.00001642
Iteration 7/1000 | Loss: 0.00001596
Iteration 8/1000 | Loss: 0.00001557
Iteration 9/1000 | Loss: 0.00001533
Iteration 10/1000 | Loss: 0.00001521
Iteration 11/1000 | Loss: 0.00001520
Iteration 12/1000 | Loss: 0.00001516
Iteration 13/1000 | Loss: 0.00001515
Iteration 14/1000 | Loss: 0.00001515
Iteration 15/1000 | Loss: 0.00001514
Iteration 16/1000 | Loss: 0.00001514
Iteration 17/1000 | Loss: 0.00001514
Iteration 18/1000 | Loss: 0.00001514
Iteration 19/1000 | Loss: 0.00001512
Iteration 20/1000 | Loss: 0.00001512
Iteration 21/1000 | Loss: 0.00001512
Iteration 22/1000 | Loss: 0.00001511
Iteration 23/1000 | Loss: 0.00001511
Iteration 24/1000 | Loss: 0.00001510
Iteration 25/1000 | Loss: 0.00001508
Iteration 26/1000 | Loss: 0.00001505
Iteration 27/1000 | Loss: 0.00001504
Iteration 28/1000 | Loss: 0.00001504
Iteration 29/1000 | Loss: 0.00001502
Iteration 30/1000 | Loss: 0.00001501
Iteration 31/1000 | Loss: 0.00001499
Iteration 32/1000 | Loss: 0.00001498
Iteration 33/1000 | Loss: 0.00001498
Iteration 34/1000 | Loss: 0.00001497
Iteration 35/1000 | Loss: 0.00001495
Iteration 36/1000 | Loss: 0.00001495
Iteration 37/1000 | Loss: 0.00001495
Iteration 38/1000 | Loss: 0.00001494
Iteration 39/1000 | Loss: 0.00001494
Iteration 40/1000 | Loss: 0.00001494
Iteration 41/1000 | Loss: 0.00001494
Iteration 42/1000 | Loss: 0.00001494
Iteration 43/1000 | Loss: 0.00001494
Iteration 44/1000 | Loss: 0.00001494
Iteration 45/1000 | Loss: 0.00001494
Iteration 46/1000 | Loss: 0.00001494
Iteration 47/1000 | Loss: 0.00001493
Iteration 48/1000 | Loss: 0.00001492
Iteration 49/1000 | Loss: 0.00001492
Iteration 50/1000 | Loss: 0.00001492
Iteration 51/1000 | Loss: 0.00001491
Iteration 52/1000 | Loss: 0.00001491
Iteration 53/1000 | Loss: 0.00001491
Iteration 54/1000 | Loss: 0.00001490
Iteration 55/1000 | Loss: 0.00001490
Iteration 56/1000 | Loss: 0.00001490
Iteration 57/1000 | Loss: 0.00001489
Iteration 58/1000 | Loss: 0.00001489
Iteration 59/1000 | Loss: 0.00001488
Iteration 60/1000 | Loss: 0.00001487
Iteration 61/1000 | Loss: 0.00001487
Iteration 62/1000 | Loss: 0.00001487
Iteration 63/1000 | Loss: 0.00001486
Iteration 64/1000 | Loss: 0.00001486
Iteration 65/1000 | Loss: 0.00001485
Iteration 66/1000 | Loss: 0.00001485
Iteration 67/1000 | Loss: 0.00001484
Iteration 68/1000 | Loss: 0.00001484
Iteration 69/1000 | Loss: 0.00001484
Iteration 70/1000 | Loss: 0.00001484
Iteration 71/1000 | Loss: 0.00001483
Iteration 72/1000 | Loss: 0.00001483
Iteration 73/1000 | Loss: 0.00001482
Iteration 74/1000 | Loss: 0.00001482
Iteration 75/1000 | Loss: 0.00001482
Iteration 76/1000 | Loss: 0.00001482
Iteration 77/1000 | Loss: 0.00001482
Iteration 78/1000 | Loss: 0.00001481
Iteration 79/1000 | Loss: 0.00001481
Iteration 80/1000 | Loss: 0.00001481
Iteration 81/1000 | Loss: 0.00001481
Iteration 82/1000 | Loss: 0.00001481
Iteration 83/1000 | Loss: 0.00001481
Iteration 84/1000 | Loss: 0.00001481
Iteration 85/1000 | Loss: 0.00001480
Iteration 86/1000 | Loss: 0.00001480
Iteration 87/1000 | Loss: 0.00001480
Iteration 88/1000 | Loss: 0.00001480
Iteration 89/1000 | Loss: 0.00001480
Iteration 90/1000 | Loss: 0.00001480
Iteration 91/1000 | Loss: 0.00001480
Iteration 92/1000 | Loss: 0.00001480
Iteration 93/1000 | Loss: 0.00001480
Iteration 94/1000 | Loss: 0.00001480
Iteration 95/1000 | Loss: 0.00001479
Iteration 96/1000 | Loss: 0.00001479
Iteration 97/1000 | Loss: 0.00001479
Iteration 98/1000 | Loss: 0.00001479
Iteration 99/1000 | Loss: 0.00001479
Iteration 100/1000 | Loss: 0.00001479
Iteration 101/1000 | Loss: 0.00001479
Iteration 102/1000 | Loss: 0.00001478
Iteration 103/1000 | Loss: 0.00001478
Iteration 104/1000 | Loss: 0.00001478
Iteration 105/1000 | Loss: 0.00001478
Iteration 106/1000 | Loss: 0.00001478
Iteration 107/1000 | Loss: 0.00001478
Iteration 108/1000 | Loss: 0.00001478
Iteration 109/1000 | Loss: 0.00001478
Iteration 110/1000 | Loss: 0.00001478
Iteration 111/1000 | Loss: 0.00001478
Iteration 112/1000 | Loss: 0.00001478
Iteration 113/1000 | Loss: 0.00001478
Iteration 114/1000 | Loss: 0.00001478
Iteration 115/1000 | Loss: 0.00001478
Iteration 116/1000 | Loss: 0.00001478
Iteration 117/1000 | Loss: 0.00001478
Iteration 118/1000 | Loss: 0.00001478
Iteration 119/1000 | Loss: 0.00001478
Iteration 120/1000 | Loss: 0.00001478
Iteration 121/1000 | Loss: 0.00001478
Iteration 122/1000 | Loss: 0.00001478
Iteration 123/1000 | Loss: 0.00001478
Iteration 124/1000 | Loss: 0.00001478
Iteration 125/1000 | Loss: 0.00001478
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.4778162949369289e-05, 1.4778162949369289e-05, 1.4778162949369289e-05, 1.4778162949369289e-05, 1.4778162949369289e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4778162949369289e-05

Optimization complete. Final v2v error: 3.3409175872802734 mm

Highest mean error: 3.6909425258636475 mm for frame 191

Lowest mean error: 3.0749576091766357 mm for frame 55

Saving results

Total time: 38.744303941726685
