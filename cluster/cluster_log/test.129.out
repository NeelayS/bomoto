Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=129, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 7224-7279
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00838167
Iteration 2/25 | Loss: 0.00118603
Iteration 3/25 | Loss: 0.00077049
Iteration 4/25 | Loss: 0.00073504
Iteration 5/25 | Loss: 0.00072611
Iteration 6/25 | Loss: 0.00072294
Iteration 7/25 | Loss: 0.00072149
Iteration 8/25 | Loss: 0.00072129
Iteration 9/25 | Loss: 0.00072129
Iteration 10/25 | Loss: 0.00072129
Iteration 11/25 | Loss: 0.00072129
Iteration 12/25 | Loss: 0.00072129
Iteration 13/25 | Loss: 0.00072129
Iteration 14/25 | Loss: 0.00072129
Iteration 15/25 | Loss: 0.00072129
Iteration 16/25 | Loss: 0.00072129
Iteration 17/25 | Loss: 0.00072129
Iteration 18/25 | Loss: 0.00072129
Iteration 19/25 | Loss: 0.00072129
Iteration 20/25 | Loss: 0.00072129
Iteration 21/25 | Loss: 0.00072129
Iteration 22/25 | Loss: 0.00072129
Iteration 23/25 | Loss: 0.00072129
Iteration 24/25 | Loss: 0.00072129
Iteration 25/25 | Loss: 0.00072129

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32695532
Iteration 2/25 | Loss: 0.00025099
Iteration 3/25 | Loss: 0.00025099
Iteration 4/25 | Loss: 0.00025099
Iteration 5/25 | Loss: 0.00025099
Iteration 6/25 | Loss: 0.00025099
Iteration 7/25 | Loss: 0.00025099
Iteration 8/25 | Loss: 0.00025099
Iteration 9/25 | Loss: 0.00025099
Iteration 10/25 | Loss: 0.00025099
Iteration 11/25 | Loss: 0.00025099
Iteration 12/25 | Loss: 0.00025099
Iteration 13/25 | Loss: 0.00025099
Iteration 14/25 | Loss: 0.00025099
Iteration 15/25 | Loss: 0.00025099
Iteration 16/25 | Loss: 0.00025099
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00025099189952015877, 0.00025099189952015877, 0.00025099189952015877, 0.00025099189952015877, 0.00025099189952015877]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00025099189952015877

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025099
Iteration 2/1000 | Loss: 0.00005098
Iteration 3/1000 | Loss: 0.00003528
Iteration 4/1000 | Loss: 0.00003230
Iteration 5/1000 | Loss: 0.00003127
Iteration 6/1000 | Loss: 0.00003054
Iteration 7/1000 | Loss: 0.00003003
Iteration 8/1000 | Loss: 0.00002974
Iteration 9/1000 | Loss: 0.00002934
Iteration 10/1000 | Loss: 0.00002915
Iteration 11/1000 | Loss: 0.00002894
Iteration 12/1000 | Loss: 0.00002875
Iteration 13/1000 | Loss: 0.00002870
Iteration 14/1000 | Loss: 0.00002870
Iteration 15/1000 | Loss: 0.00002869
Iteration 16/1000 | Loss: 0.00002869
Iteration 17/1000 | Loss: 0.00002853
Iteration 18/1000 | Loss: 0.00002850
Iteration 19/1000 | Loss: 0.00002850
Iteration 20/1000 | Loss: 0.00002850
Iteration 21/1000 | Loss: 0.00002850
Iteration 22/1000 | Loss: 0.00002850
Iteration 23/1000 | Loss: 0.00002850
Iteration 24/1000 | Loss: 0.00002850
Iteration 25/1000 | Loss: 0.00002850
Iteration 26/1000 | Loss: 0.00002850
Iteration 27/1000 | Loss: 0.00002850
Iteration 28/1000 | Loss: 0.00002849
Iteration 29/1000 | Loss: 0.00002849
Iteration 30/1000 | Loss: 0.00002849
Iteration 31/1000 | Loss: 0.00002849
Iteration 32/1000 | Loss: 0.00002849
Iteration 33/1000 | Loss: 0.00002848
Iteration 34/1000 | Loss: 0.00002848
Iteration 35/1000 | Loss: 0.00002847
Iteration 36/1000 | Loss: 0.00002846
Iteration 37/1000 | Loss: 0.00002845
Iteration 38/1000 | Loss: 0.00002845
Iteration 39/1000 | Loss: 0.00002845
Iteration 40/1000 | Loss: 0.00002845
Iteration 41/1000 | Loss: 0.00002845
Iteration 42/1000 | Loss: 0.00002845
Iteration 43/1000 | Loss: 0.00002845
Iteration 44/1000 | Loss: 0.00002845
Iteration 45/1000 | Loss: 0.00002845
Iteration 46/1000 | Loss: 0.00002845
Iteration 47/1000 | Loss: 0.00002845
Iteration 48/1000 | Loss: 0.00002845
Iteration 49/1000 | Loss: 0.00002844
Iteration 50/1000 | Loss: 0.00002844
Iteration 51/1000 | Loss: 0.00002842
Iteration 52/1000 | Loss: 0.00002842
Iteration 53/1000 | Loss: 0.00002842
Iteration 54/1000 | Loss: 0.00002842
Iteration 55/1000 | Loss: 0.00002842
Iteration 56/1000 | Loss: 0.00002842
Iteration 57/1000 | Loss: 0.00002842
Iteration 58/1000 | Loss: 0.00002842
Iteration 59/1000 | Loss: 0.00002842
Iteration 60/1000 | Loss: 0.00002842
Iteration 61/1000 | Loss: 0.00002842
Iteration 62/1000 | Loss: 0.00002841
Iteration 63/1000 | Loss: 0.00002841
Iteration 64/1000 | Loss: 0.00002840
Iteration 65/1000 | Loss: 0.00002840
Iteration 66/1000 | Loss: 0.00002839
Iteration 67/1000 | Loss: 0.00002838
Iteration 68/1000 | Loss: 0.00002838
Iteration 69/1000 | Loss: 0.00002838
Iteration 70/1000 | Loss: 0.00002837
Iteration 71/1000 | Loss: 0.00002834
Iteration 72/1000 | Loss: 0.00002834
Iteration 73/1000 | Loss: 0.00002834
Iteration 74/1000 | Loss: 0.00002834
Iteration 75/1000 | Loss: 0.00002832
Iteration 76/1000 | Loss: 0.00002832
Iteration 77/1000 | Loss: 0.00002832
Iteration 78/1000 | Loss: 0.00002830
Iteration 79/1000 | Loss: 0.00002830
Iteration 80/1000 | Loss: 0.00002830
Iteration 81/1000 | Loss: 0.00002830
Iteration 82/1000 | Loss: 0.00002830
Iteration 83/1000 | Loss: 0.00002830
Iteration 84/1000 | Loss: 0.00002830
Iteration 85/1000 | Loss: 0.00002830
Iteration 86/1000 | Loss: 0.00002830
Iteration 87/1000 | Loss: 0.00002829
Iteration 88/1000 | Loss: 0.00002828
Iteration 89/1000 | Loss: 0.00002827
Iteration 90/1000 | Loss: 0.00002825
Iteration 91/1000 | Loss: 0.00002825
Iteration 92/1000 | Loss: 0.00002825
Iteration 93/1000 | Loss: 0.00002825
Iteration 94/1000 | Loss: 0.00002825
Iteration 95/1000 | Loss: 0.00002825
Iteration 96/1000 | Loss: 0.00002824
Iteration 97/1000 | Loss: 0.00002824
Iteration 98/1000 | Loss: 0.00002822
Iteration 99/1000 | Loss: 0.00002822
Iteration 100/1000 | Loss: 0.00002820
Iteration 101/1000 | Loss: 0.00002819
Iteration 102/1000 | Loss: 0.00002819
Iteration 103/1000 | Loss: 0.00002819
Iteration 104/1000 | Loss: 0.00002819
Iteration 105/1000 | Loss: 0.00002819
Iteration 106/1000 | Loss: 0.00002818
Iteration 107/1000 | Loss: 0.00002818
Iteration 108/1000 | Loss: 0.00002818
Iteration 109/1000 | Loss: 0.00002817
Iteration 110/1000 | Loss: 0.00002816
Iteration 111/1000 | Loss: 0.00002813
Iteration 112/1000 | Loss: 0.00002813
Iteration 113/1000 | Loss: 0.00002813
Iteration 114/1000 | Loss: 0.00002813
Iteration 115/1000 | Loss: 0.00002813
Iteration 116/1000 | Loss: 0.00002813
Iteration 117/1000 | Loss: 0.00002813
Iteration 118/1000 | Loss: 0.00002813
Iteration 119/1000 | Loss: 0.00002813
Iteration 120/1000 | Loss: 0.00002812
Iteration 121/1000 | Loss: 0.00002812
Iteration 122/1000 | Loss: 0.00002812
Iteration 123/1000 | Loss: 0.00002812
Iteration 124/1000 | Loss: 0.00002811
Iteration 125/1000 | Loss: 0.00002811
Iteration 126/1000 | Loss: 0.00002811
Iteration 127/1000 | Loss: 0.00002811
Iteration 128/1000 | Loss: 0.00002811
Iteration 129/1000 | Loss: 0.00002811
Iteration 130/1000 | Loss: 0.00002811
Iteration 131/1000 | Loss: 0.00002811
Iteration 132/1000 | Loss: 0.00002811
Iteration 133/1000 | Loss: 0.00002811
Iteration 134/1000 | Loss: 0.00002811
Iteration 135/1000 | Loss: 0.00002811
Iteration 136/1000 | Loss: 0.00002811
Iteration 137/1000 | Loss: 0.00002810
Iteration 138/1000 | Loss: 0.00002810
Iteration 139/1000 | Loss: 0.00002810
Iteration 140/1000 | Loss: 0.00002810
Iteration 141/1000 | Loss: 0.00002810
Iteration 142/1000 | Loss: 0.00002810
Iteration 143/1000 | Loss: 0.00002810
Iteration 144/1000 | Loss: 0.00002810
Iteration 145/1000 | Loss: 0.00002810
Iteration 146/1000 | Loss: 0.00002810
Iteration 147/1000 | Loss: 0.00002810
Iteration 148/1000 | Loss: 0.00002810
Iteration 149/1000 | Loss: 0.00002810
Iteration 150/1000 | Loss: 0.00002809
Iteration 151/1000 | Loss: 0.00002809
Iteration 152/1000 | Loss: 0.00002809
Iteration 153/1000 | Loss: 0.00002809
Iteration 154/1000 | Loss: 0.00002809
Iteration 155/1000 | Loss: 0.00002809
Iteration 156/1000 | Loss: 0.00002809
Iteration 157/1000 | Loss: 0.00002809
Iteration 158/1000 | Loss: 0.00002809
Iteration 159/1000 | Loss: 0.00002809
Iteration 160/1000 | Loss: 0.00002809
Iteration 161/1000 | Loss: 0.00002809
Iteration 162/1000 | Loss: 0.00002809
Iteration 163/1000 | Loss: 0.00002809
Iteration 164/1000 | Loss: 0.00002809
Iteration 165/1000 | Loss: 0.00002809
Iteration 166/1000 | Loss: 0.00002809
Iteration 167/1000 | Loss: 0.00002808
Iteration 168/1000 | Loss: 0.00002808
Iteration 169/1000 | Loss: 0.00002808
Iteration 170/1000 | Loss: 0.00002808
Iteration 171/1000 | Loss: 0.00002808
Iteration 172/1000 | Loss: 0.00002808
Iteration 173/1000 | Loss: 0.00002808
Iteration 174/1000 | Loss: 0.00002808
Iteration 175/1000 | Loss: 0.00002808
Iteration 176/1000 | Loss: 0.00002808
Iteration 177/1000 | Loss: 0.00002808
Iteration 178/1000 | Loss: 0.00002808
Iteration 179/1000 | Loss: 0.00002808
Iteration 180/1000 | Loss: 0.00002808
Iteration 181/1000 | Loss: 0.00002808
Iteration 182/1000 | Loss: 0.00002808
Iteration 183/1000 | Loss: 0.00002808
Iteration 184/1000 | Loss: 0.00002808
Iteration 185/1000 | Loss: 0.00002808
Iteration 186/1000 | Loss: 0.00002808
Iteration 187/1000 | Loss: 0.00002808
Iteration 188/1000 | Loss: 0.00002808
Iteration 189/1000 | Loss: 0.00002808
Iteration 190/1000 | Loss: 0.00002808
Iteration 191/1000 | Loss: 0.00002808
Iteration 192/1000 | Loss: 0.00002808
Iteration 193/1000 | Loss: 0.00002808
Iteration 194/1000 | Loss: 0.00002808
Iteration 195/1000 | Loss: 0.00002808
Iteration 196/1000 | Loss: 0.00002808
Iteration 197/1000 | Loss: 0.00002808
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [2.8075070076738484e-05, 2.8075070076738484e-05, 2.8075070076738484e-05, 2.8075070076738484e-05, 2.8075070076738484e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8075070076738484e-05

Optimization complete. Final v2v error: 4.296424388885498 mm

Highest mean error: 4.395906925201416 mm for frame 23

Lowest mean error: 4.131472110748291 mm for frame 89

Saving results

Total time: 43.69597887992859
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00696288
Iteration 2/25 | Loss: 0.00089065
Iteration 3/25 | Loss: 0.00063331
Iteration 4/25 | Loss: 0.00058267
Iteration 5/25 | Loss: 0.00056119
Iteration 6/25 | Loss: 0.00055527
Iteration 7/25 | Loss: 0.00055377
Iteration 8/25 | Loss: 0.00055334
Iteration 9/25 | Loss: 0.00055325
Iteration 10/25 | Loss: 0.00055325
Iteration 11/25 | Loss: 0.00055325
Iteration 12/25 | Loss: 0.00055325
Iteration 13/25 | Loss: 0.00055325
Iteration 14/25 | Loss: 0.00055325
Iteration 15/25 | Loss: 0.00055325
Iteration 16/25 | Loss: 0.00055325
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005532473442144692, 0.0005532473442144692, 0.0005532473442144692, 0.0005532473442144692, 0.0005532473442144692]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005532473442144692

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.38243389
Iteration 2/25 | Loss: 0.00017088
Iteration 3/25 | Loss: 0.00017088
Iteration 4/25 | Loss: 0.00017088
Iteration 5/25 | Loss: 0.00017088
Iteration 6/25 | Loss: 0.00017088
Iteration 7/25 | Loss: 0.00017088
Iteration 8/25 | Loss: 0.00017088
Iteration 9/25 | Loss: 0.00017088
Iteration 10/25 | Loss: 0.00017088
Iteration 11/25 | Loss: 0.00017088
Iteration 12/25 | Loss: 0.00017088
Iteration 13/25 | Loss: 0.00017088
Iteration 14/25 | Loss: 0.00017088
Iteration 15/25 | Loss: 0.00017088
Iteration 16/25 | Loss: 0.00017088
Iteration 17/25 | Loss: 0.00017088
Iteration 18/25 | Loss: 0.00017088
Iteration 19/25 | Loss: 0.00017088
Iteration 20/25 | Loss: 0.00017088
Iteration 21/25 | Loss: 0.00017088
Iteration 22/25 | Loss: 0.00017088
Iteration 23/25 | Loss: 0.00017088
Iteration 24/25 | Loss: 0.00017088
Iteration 25/25 | Loss: 0.00017088

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00017088
Iteration 2/1000 | Loss: 0.00001697
Iteration 3/1000 | Loss: 0.00001315
Iteration 4/1000 | Loss: 0.00001253
Iteration 5/1000 | Loss: 0.00001208
Iteration 6/1000 | Loss: 0.00001177
Iteration 7/1000 | Loss: 0.00001160
Iteration 8/1000 | Loss: 0.00001157
Iteration 9/1000 | Loss: 0.00001149
Iteration 10/1000 | Loss: 0.00001149
Iteration 11/1000 | Loss: 0.00001148
Iteration 12/1000 | Loss: 0.00001148
Iteration 13/1000 | Loss: 0.00001148
Iteration 14/1000 | Loss: 0.00001147
Iteration 15/1000 | Loss: 0.00001146
Iteration 16/1000 | Loss: 0.00001143
Iteration 17/1000 | Loss: 0.00001139
Iteration 18/1000 | Loss: 0.00001139
Iteration 19/1000 | Loss: 0.00001137
Iteration 20/1000 | Loss: 0.00001137
Iteration 21/1000 | Loss: 0.00001136
Iteration 22/1000 | Loss: 0.00001134
Iteration 23/1000 | Loss: 0.00001132
Iteration 24/1000 | Loss: 0.00001128
Iteration 25/1000 | Loss: 0.00001128
Iteration 26/1000 | Loss: 0.00001127
Iteration 27/1000 | Loss: 0.00001127
Iteration 28/1000 | Loss: 0.00001126
Iteration 29/1000 | Loss: 0.00001125
Iteration 30/1000 | Loss: 0.00001121
Iteration 31/1000 | Loss: 0.00001120
Iteration 32/1000 | Loss: 0.00001117
Iteration 33/1000 | Loss: 0.00001116
Iteration 34/1000 | Loss: 0.00001113
Iteration 35/1000 | Loss: 0.00001112
Iteration 36/1000 | Loss: 0.00001110
Iteration 37/1000 | Loss: 0.00001110
Iteration 38/1000 | Loss: 0.00001108
Iteration 39/1000 | Loss: 0.00001108
Iteration 40/1000 | Loss: 0.00001108
Iteration 41/1000 | Loss: 0.00001108
Iteration 42/1000 | Loss: 0.00001107
Iteration 43/1000 | Loss: 0.00001106
Iteration 44/1000 | Loss: 0.00001106
Iteration 45/1000 | Loss: 0.00001105
Iteration 46/1000 | Loss: 0.00001105
Iteration 47/1000 | Loss: 0.00001104
Iteration 48/1000 | Loss: 0.00001104
Iteration 49/1000 | Loss: 0.00001103
Iteration 50/1000 | Loss: 0.00001102
Iteration 51/1000 | Loss: 0.00001102
Iteration 52/1000 | Loss: 0.00001102
Iteration 53/1000 | Loss: 0.00001102
Iteration 54/1000 | Loss: 0.00001101
Iteration 55/1000 | Loss: 0.00001101
Iteration 56/1000 | Loss: 0.00001101
Iteration 57/1000 | Loss: 0.00001100
Iteration 58/1000 | Loss: 0.00001100
Iteration 59/1000 | Loss: 0.00001100
Iteration 60/1000 | Loss: 0.00001099
Iteration 61/1000 | Loss: 0.00001099
Iteration 62/1000 | Loss: 0.00001099
Iteration 63/1000 | Loss: 0.00001099
Iteration 64/1000 | Loss: 0.00001099
Iteration 65/1000 | Loss: 0.00001098
Iteration 66/1000 | Loss: 0.00001098
Iteration 67/1000 | Loss: 0.00001098
Iteration 68/1000 | Loss: 0.00001097
Iteration 69/1000 | Loss: 0.00001097
Iteration 70/1000 | Loss: 0.00001097
Iteration 71/1000 | Loss: 0.00001097
Iteration 72/1000 | Loss: 0.00001097
Iteration 73/1000 | Loss: 0.00001097
Iteration 74/1000 | Loss: 0.00001096
Iteration 75/1000 | Loss: 0.00001096
Iteration 76/1000 | Loss: 0.00001096
Iteration 77/1000 | Loss: 0.00001095
Iteration 78/1000 | Loss: 0.00001094
Iteration 79/1000 | Loss: 0.00001094
Iteration 80/1000 | Loss: 0.00001093
Iteration 81/1000 | Loss: 0.00001093
Iteration 82/1000 | Loss: 0.00001093
Iteration 83/1000 | Loss: 0.00001093
Iteration 84/1000 | Loss: 0.00001092
Iteration 85/1000 | Loss: 0.00001092
Iteration 86/1000 | Loss: 0.00001092
Iteration 87/1000 | Loss: 0.00001092
Iteration 88/1000 | Loss: 0.00001091
Iteration 89/1000 | Loss: 0.00001091
Iteration 90/1000 | Loss: 0.00001091
Iteration 91/1000 | Loss: 0.00001091
Iteration 92/1000 | Loss: 0.00001091
Iteration 93/1000 | Loss: 0.00001091
Iteration 94/1000 | Loss: 0.00001091
Iteration 95/1000 | Loss: 0.00001091
Iteration 96/1000 | Loss: 0.00001091
Iteration 97/1000 | Loss: 0.00001091
Iteration 98/1000 | Loss: 0.00001091
Iteration 99/1000 | Loss: 0.00001091
Iteration 100/1000 | Loss: 0.00001091
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [1.0908172043855302e-05, 1.0908172043855302e-05, 1.0908172043855302e-05, 1.0908172043855302e-05, 1.0908172043855302e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0908172043855302e-05

Optimization complete. Final v2v error: 2.814537286758423 mm

Highest mean error: 3.145498514175415 mm for frame 180

Lowest mean error: 2.631139039993286 mm for frame 23

Saving results

Total time: 40.654815435409546
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00794880
Iteration 2/25 | Loss: 0.00171217
Iteration 3/25 | Loss: 0.00088053
Iteration 4/25 | Loss: 0.00076147
Iteration 5/25 | Loss: 0.00067297
Iteration 6/25 | Loss: 0.00064966
Iteration 7/25 | Loss: 0.00065412
Iteration 8/25 | Loss: 0.00063035
Iteration 9/25 | Loss: 0.00062089
Iteration 10/25 | Loss: 0.00062213
Iteration 11/25 | Loss: 0.00061244
Iteration 12/25 | Loss: 0.00061273
Iteration 13/25 | Loss: 0.00060970
Iteration 14/25 | Loss: 0.00060902
Iteration 15/25 | Loss: 0.00060886
Iteration 16/25 | Loss: 0.00060875
Iteration 17/25 | Loss: 0.00060875
Iteration 18/25 | Loss: 0.00060875
Iteration 19/25 | Loss: 0.00060874
Iteration 20/25 | Loss: 0.00060874
Iteration 21/25 | Loss: 0.00060874
Iteration 22/25 | Loss: 0.00060874
Iteration 23/25 | Loss: 0.00060874
Iteration 24/25 | Loss: 0.00060874
Iteration 25/25 | Loss: 0.00060874

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.96857369
Iteration 2/25 | Loss: 0.00017502
Iteration 3/25 | Loss: 0.00017502
Iteration 4/25 | Loss: 0.00017502
Iteration 5/25 | Loss: 0.00017502
Iteration 6/25 | Loss: 0.00017502
Iteration 7/25 | Loss: 0.00017502
Iteration 8/25 | Loss: 0.00017502
Iteration 9/25 | Loss: 0.00017502
Iteration 10/25 | Loss: 0.00017502
Iteration 11/25 | Loss: 0.00017502
Iteration 12/25 | Loss: 0.00017502
Iteration 13/25 | Loss: 0.00017502
Iteration 14/25 | Loss: 0.00017502
Iteration 15/25 | Loss: 0.00017502
Iteration 16/25 | Loss: 0.00017502
Iteration 17/25 | Loss: 0.00017502
Iteration 18/25 | Loss: 0.00017502
Iteration 19/25 | Loss: 0.00017502
Iteration 20/25 | Loss: 0.00017502
Iteration 21/25 | Loss: 0.00017502
Iteration 22/25 | Loss: 0.00017502
Iteration 23/25 | Loss: 0.00017502
Iteration 24/25 | Loss: 0.00017502
Iteration 25/25 | Loss: 0.00017502

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00017502
Iteration 2/1000 | Loss: 0.00006459
Iteration 3/1000 | Loss: 0.00001728
Iteration 4/1000 | Loss: 0.00001591
Iteration 5/1000 | Loss: 0.00001511
Iteration 6/1000 | Loss: 0.00003803
Iteration 7/1000 | Loss: 0.00001626
Iteration 8/1000 | Loss: 0.00001430
Iteration 9/1000 | Loss: 0.00001422
Iteration 10/1000 | Loss: 0.00001411
Iteration 11/1000 | Loss: 0.00001452
Iteration 12/1000 | Loss: 0.00001451
Iteration 13/1000 | Loss: 0.00001396
Iteration 14/1000 | Loss: 0.00001395
Iteration 15/1000 | Loss: 0.00002159
Iteration 16/1000 | Loss: 0.00001433
Iteration 17/1000 | Loss: 0.00001398
Iteration 18/1000 | Loss: 0.00001382
Iteration 19/1000 | Loss: 0.00001381
Iteration 20/1000 | Loss: 0.00001378
Iteration 21/1000 | Loss: 0.00001376
Iteration 22/1000 | Loss: 0.00001376
Iteration 23/1000 | Loss: 0.00001373
Iteration 24/1000 | Loss: 0.00001372
Iteration 25/1000 | Loss: 0.00001372
Iteration 26/1000 | Loss: 0.00001371
Iteration 27/1000 | Loss: 0.00001371
Iteration 28/1000 | Loss: 0.00001371
Iteration 29/1000 | Loss: 0.00001370
Iteration 30/1000 | Loss: 0.00001370
Iteration 31/1000 | Loss: 0.00001369
Iteration 32/1000 | Loss: 0.00001369
Iteration 33/1000 | Loss: 0.00001368
Iteration 34/1000 | Loss: 0.00003546
Iteration 35/1000 | Loss: 0.00001376
Iteration 36/1000 | Loss: 0.00001364
Iteration 37/1000 | Loss: 0.00001364
Iteration 38/1000 | Loss: 0.00001363
Iteration 39/1000 | Loss: 0.00001363
Iteration 40/1000 | Loss: 0.00001362
Iteration 41/1000 | Loss: 0.00001362
Iteration 42/1000 | Loss: 0.00001362
Iteration 43/1000 | Loss: 0.00001362
Iteration 44/1000 | Loss: 0.00001362
Iteration 45/1000 | Loss: 0.00001362
Iteration 46/1000 | Loss: 0.00001362
Iteration 47/1000 | Loss: 0.00001361
Iteration 48/1000 | Loss: 0.00001361
Iteration 49/1000 | Loss: 0.00001361
Iteration 50/1000 | Loss: 0.00001361
Iteration 51/1000 | Loss: 0.00001361
Iteration 52/1000 | Loss: 0.00001409
Iteration 53/1000 | Loss: 0.00001409
Iteration 54/1000 | Loss: 0.00002542
Iteration 55/1000 | Loss: 0.00001372
Iteration 56/1000 | Loss: 0.00001370
Iteration 57/1000 | Loss: 0.00001369
Iteration 58/1000 | Loss: 0.00001364
Iteration 59/1000 | Loss: 0.00001354
Iteration 60/1000 | Loss: 0.00001354
Iteration 61/1000 | Loss: 0.00001353
Iteration 62/1000 | Loss: 0.00001353
Iteration 63/1000 | Loss: 0.00001353
Iteration 64/1000 | Loss: 0.00001353
Iteration 65/1000 | Loss: 0.00001352
Iteration 66/1000 | Loss: 0.00001351
Iteration 67/1000 | Loss: 0.00001351
Iteration 68/1000 | Loss: 0.00001351
Iteration 69/1000 | Loss: 0.00001350
Iteration 70/1000 | Loss: 0.00001350
Iteration 71/1000 | Loss: 0.00001350
Iteration 72/1000 | Loss: 0.00001350
Iteration 73/1000 | Loss: 0.00001350
Iteration 74/1000 | Loss: 0.00001350
Iteration 75/1000 | Loss: 0.00001350
Iteration 76/1000 | Loss: 0.00001350
Iteration 77/1000 | Loss: 0.00001350
Iteration 78/1000 | Loss: 0.00001350
Iteration 79/1000 | Loss: 0.00001350
Iteration 80/1000 | Loss: 0.00001349
Iteration 81/1000 | Loss: 0.00001349
Iteration 82/1000 | Loss: 0.00001349
Iteration 83/1000 | Loss: 0.00001349
Iteration 84/1000 | Loss: 0.00001349
Iteration 85/1000 | Loss: 0.00001349
Iteration 86/1000 | Loss: 0.00001349
Iteration 87/1000 | Loss: 0.00001349
Iteration 88/1000 | Loss: 0.00001349
Iteration 89/1000 | Loss: 0.00001349
Iteration 90/1000 | Loss: 0.00001349
Iteration 91/1000 | Loss: 0.00001349
Iteration 92/1000 | Loss: 0.00001349
Iteration 93/1000 | Loss: 0.00001349
Iteration 94/1000 | Loss: 0.00001349
Iteration 95/1000 | Loss: 0.00001349
Iteration 96/1000 | Loss: 0.00001349
Iteration 97/1000 | Loss: 0.00001349
Iteration 98/1000 | Loss: 0.00001349
Iteration 99/1000 | Loss: 0.00001349
Iteration 100/1000 | Loss: 0.00001349
Iteration 101/1000 | Loss: 0.00001349
Iteration 102/1000 | Loss: 0.00001349
Iteration 103/1000 | Loss: 0.00001349
Iteration 104/1000 | Loss: 0.00001349
Iteration 105/1000 | Loss: 0.00001349
Iteration 106/1000 | Loss: 0.00001349
Iteration 107/1000 | Loss: 0.00001349
Iteration 108/1000 | Loss: 0.00001349
Iteration 109/1000 | Loss: 0.00001349
Iteration 110/1000 | Loss: 0.00001349
Iteration 111/1000 | Loss: 0.00001349
Iteration 112/1000 | Loss: 0.00001349
Iteration 113/1000 | Loss: 0.00001349
Iteration 114/1000 | Loss: 0.00001349
Iteration 115/1000 | Loss: 0.00001349
Iteration 116/1000 | Loss: 0.00001349
Iteration 117/1000 | Loss: 0.00001349
Iteration 118/1000 | Loss: 0.00001349
Iteration 119/1000 | Loss: 0.00001349
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.3485364434018265e-05, 1.3485364434018265e-05, 1.3485364434018265e-05, 1.3485364434018265e-05, 1.3485364434018265e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3485364434018265e-05

Optimization complete. Final v2v error: 3.0795884132385254 mm

Highest mean error: 9.068219184875488 mm for frame 108

Lowest mean error: 2.7996273040771484 mm for frame 117

Saving results

Total time: 61.30403542518616
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01010510
Iteration 2/25 | Loss: 0.00216407
Iteration 3/25 | Loss: 0.00139835
Iteration 4/25 | Loss: 0.00121726
Iteration 5/25 | Loss: 0.00107699
Iteration 6/25 | Loss: 0.00099115
Iteration 7/25 | Loss: 0.00089681
Iteration 8/25 | Loss: 0.00084686
Iteration 9/25 | Loss: 0.00083087
Iteration 10/25 | Loss: 0.00079658
Iteration 11/25 | Loss: 0.00075197
Iteration 12/25 | Loss: 0.00069835
Iteration 13/25 | Loss: 0.00066214
Iteration 14/25 | Loss: 0.00065734
Iteration 15/25 | Loss: 0.00062909
Iteration 16/25 | Loss: 0.00062405
Iteration 17/25 | Loss: 0.00061088
Iteration 18/25 | Loss: 0.00059503
Iteration 19/25 | Loss: 0.00058736
Iteration 20/25 | Loss: 0.00059318
Iteration 21/25 | Loss: 0.00058941
Iteration 22/25 | Loss: 0.00058383
Iteration 23/25 | Loss: 0.00058171
Iteration 24/25 | Loss: 0.00057966
Iteration 25/25 | Loss: 0.00057867

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48262239
Iteration 2/25 | Loss: 0.00031814
Iteration 3/25 | Loss: 0.00020052
Iteration 4/25 | Loss: 0.00020052
Iteration 5/25 | Loss: 0.00020052
Iteration 6/25 | Loss: 0.00020052
Iteration 7/25 | Loss: 0.00020051
Iteration 8/25 | Loss: 0.00020051
Iteration 9/25 | Loss: 0.00020051
Iteration 10/25 | Loss: 0.00020051
Iteration 11/25 | Loss: 0.00020051
Iteration 12/25 | Loss: 0.00020051
Iteration 13/25 | Loss: 0.00020051
Iteration 14/25 | Loss: 0.00020051
Iteration 15/25 | Loss: 0.00020051
Iteration 16/25 | Loss: 0.00020051
Iteration 17/25 | Loss: 0.00020051
Iteration 18/25 | Loss: 0.00020051
Iteration 19/25 | Loss: 0.00020051
Iteration 20/25 | Loss: 0.00020051
Iteration 21/25 | Loss: 0.00020051
Iteration 22/25 | Loss: 0.00020051
Iteration 23/25 | Loss: 0.00020051
Iteration 24/25 | Loss: 0.00020051
Iteration 25/25 | Loss: 0.00020051

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00020051
Iteration 2/1000 | Loss: 0.00009006
Iteration 3/1000 | Loss: 0.00008623
Iteration 4/1000 | Loss: 0.00009204
Iteration 5/1000 | Loss: 0.00006795
Iteration 6/1000 | Loss: 0.00004428
Iteration 7/1000 | Loss: 0.00001418
Iteration 8/1000 | Loss: 0.00001295
Iteration 9/1000 | Loss: 0.00018424
Iteration 10/1000 | Loss: 0.00002590
Iteration 11/1000 | Loss: 0.00001446
Iteration 12/1000 | Loss: 0.00002288
Iteration 13/1000 | Loss: 0.00002765
Iteration 14/1000 | Loss: 0.00018701
Iteration 15/1000 | Loss: 0.00025072
Iteration 16/1000 | Loss: 0.00001787
Iteration 17/1000 | Loss: 0.00002675
Iteration 18/1000 | Loss: 0.00027989
Iteration 19/1000 | Loss: 0.00023074
Iteration 20/1000 | Loss: 0.00017826
Iteration 21/1000 | Loss: 0.00005198
Iteration 22/1000 | Loss: 0.00005492
Iteration 23/1000 | Loss: 0.00016693
Iteration 24/1000 | Loss: 0.00001949
Iteration 25/1000 | Loss: 0.00004376
Iteration 26/1000 | Loss: 0.00001747
Iteration 27/1000 | Loss: 0.00019938
Iteration 28/1000 | Loss: 0.00014371
Iteration 29/1000 | Loss: 0.00010687
Iteration 30/1000 | Loss: 0.00017129
Iteration 31/1000 | Loss: 0.00018273
Iteration 32/1000 | Loss: 0.00041455
Iteration 33/1000 | Loss: 0.00064844
Iteration 34/1000 | Loss: 0.00023994
Iteration 35/1000 | Loss: 0.00067011
Iteration 36/1000 | Loss: 0.00002970
Iteration 37/1000 | Loss: 0.00005130
Iteration 38/1000 | Loss: 0.00007971
Iteration 39/1000 | Loss: 0.00001911
Iteration 40/1000 | Loss: 0.00002126
Iteration 41/1000 | Loss: 0.00017593
Iteration 42/1000 | Loss: 0.00002314
Iteration 43/1000 | Loss: 0.00006810
Iteration 44/1000 | Loss: 0.00003242
Iteration 45/1000 | Loss: 0.00001845
Iteration 46/1000 | Loss: 0.00006030
Iteration 47/1000 | Loss: 0.00003593
Iteration 48/1000 | Loss: 0.00007779
Iteration 49/1000 | Loss: 0.00005938
Iteration 50/1000 | Loss: 0.00007269
Iteration 51/1000 | Loss: 0.00014568
Iteration 52/1000 | Loss: 0.00003092
Iteration 53/1000 | Loss: 0.00003827
Iteration 54/1000 | Loss: 0.00002052
Iteration 55/1000 | Loss: 0.00002556
Iteration 56/1000 | Loss: 0.00003857
Iteration 57/1000 | Loss: 0.00004438
Iteration 58/1000 | Loss: 0.00003296
Iteration 59/1000 | Loss: 0.00005924
Iteration 60/1000 | Loss: 0.00017838
Iteration 61/1000 | Loss: 0.00004785
Iteration 62/1000 | Loss: 0.00009101
Iteration 63/1000 | Loss: 0.00002944
Iteration 64/1000 | Loss: 0.00011292
Iteration 65/1000 | Loss: 0.00004108
Iteration 66/1000 | Loss: 0.00003816
Iteration 67/1000 | Loss: 0.00003749
Iteration 68/1000 | Loss: 0.00003643
Iteration 69/1000 | Loss: 0.00011951
Iteration 70/1000 | Loss: 0.00004020
Iteration 71/1000 | Loss: 0.00003704
Iteration 72/1000 | Loss: 0.00003116
Iteration 73/1000 | Loss: 0.00004668
Iteration 74/1000 | Loss: 0.00004058
Iteration 75/1000 | Loss: 0.00004218
Iteration 76/1000 | Loss: 0.00003757
Iteration 77/1000 | Loss: 0.00003174
Iteration 78/1000 | Loss: 0.00005023
Iteration 79/1000 | Loss: 0.00017318
Iteration 80/1000 | Loss: 0.00003446
Iteration 81/1000 | Loss: 0.00004268
Iteration 82/1000 | Loss: 0.00002670
Iteration 83/1000 | Loss: 0.00004012
Iteration 84/1000 | Loss: 0.00003128
Iteration 85/1000 | Loss: 0.00003232
Iteration 86/1000 | Loss: 0.00003491
Iteration 87/1000 | Loss: 0.00002773
Iteration 88/1000 | Loss: 0.00004760
Iteration 89/1000 | Loss: 0.00002451
Iteration 90/1000 | Loss: 0.00002734
Iteration 91/1000 | Loss: 0.00004585
Iteration 92/1000 | Loss: 0.00003977
Iteration 93/1000 | Loss: 0.00003084
Iteration 94/1000 | Loss: 0.00006541
Iteration 95/1000 | Loss: 0.00019465
Iteration 96/1000 | Loss: 0.00009660
Iteration 97/1000 | Loss: 0.00006841
Iteration 98/1000 | Loss: 0.00002047
Iteration 99/1000 | Loss: 0.00002882
Iteration 100/1000 | Loss: 0.00001223
Iteration 101/1000 | Loss: 0.00001401
Iteration 102/1000 | Loss: 0.00004208
Iteration 103/1000 | Loss: 0.00005340
Iteration 104/1000 | Loss: 0.00002963
Iteration 105/1000 | Loss: 0.00001489
Iteration 106/1000 | Loss: 0.00001167
Iteration 107/1000 | Loss: 0.00002983
Iteration 108/1000 | Loss: 0.00001089
Iteration 109/1000 | Loss: 0.00003697
Iteration 110/1000 | Loss: 0.00005475
Iteration 111/1000 | Loss: 0.00001261
Iteration 112/1000 | Loss: 0.00002278
Iteration 113/1000 | Loss: 0.00013144
Iteration 114/1000 | Loss: 0.00002416
Iteration 115/1000 | Loss: 0.00001365
Iteration 116/1000 | Loss: 0.00001070
Iteration 117/1000 | Loss: 0.00001050
Iteration 118/1000 | Loss: 0.00001050
Iteration 119/1000 | Loss: 0.00001050
Iteration 120/1000 | Loss: 0.00001050
Iteration 121/1000 | Loss: 0.00001050
Iteration 122/1000 | Loss: 0.00001050
Iteration 123/1000 | Loss: 0.00001050
Iteration 124/1000 | Loss: 0.00001048
Iteration 125/1000 | Loss: 0.00001045
Iteration 126/1000 | Loss: 0.00001045
Iteration 127/1000 | Loss: 0.00001045
Iteration 128/1000 | Loss: 0.00001044
Iteration 129/1000 | Loss: 0.00001093
Iteration 130/1000 | Loss: 0.00001056
Iteration 131/1000 | Loss: 0.00002008
Iteration 132/1000 | Loss: 0.00002382
Iteration 133/1000 | Loss: 0.00001039
Iteration 134/1000 | Loss: 0.00001038
Iteration 135/1000 | Loss: 0.00001038
Iteration 136/1000 | Loss: 0.00001038
Iteration 137/1000 | Loss: 0.00001037
Iteration 138/1000 | Loss: 0.00001037
Iteration 139/1000 | Loss: 0.00001037
Iteration 140/1000 | Loss: 0.00001037
Iteration 141/1000 | Loss: 0.00001037
Iteration 142/1000 | Loss: 0.00001037
Iteration 143/1000 | Loss: 0.00001037
Iteration 144/1000 | Loss: 0.00001037
Iteration 145/1000 | Loss: 0.00001037
Iteration 146/1000 | Loss: 0.00001036
Iteration 147/1000 | Loss: 0.00001036
Iteration 148/1000 | Loss: 0.00001036
Iteration 149/1000 | Loss: 0.00001036
Iteration 150/1000 | Loss: 0.00001036
Iteration 151/1000 | Loss: 0.00001036
Iteration 152/1000 | Loss: 0.00001036
Iteration 153/1000 | Loss: 0.00001036
Iteration 154/1000 | Loss: 0.00001241
Iteration 155/1000 | Loss: 0.00001035
Iteration 156/1000 | Loss: 0.00001035
Iteration 157/1000 | Loss: 0.00001035
Iteration 158/1000 | Loss: 0.00001035
Iteration 159/1000 | Loss: 0.00001035
Iteration 160/1000 | Loss: 0.00001035
Iteration 161/1000 | Loss: 0.00001035
Iteration 162/1000 | Loss: 0.00001035
Iteration 163/1000 | Loss: 0.00001035
Iteration 164/1000 | Loss: 0.00001035
Iteration 165/1000 | Loss: 0.00001035
Iteration 166/1000 | Loss: 0.00001035
Iteration 167/1000 | Loss: 0.00001035
Iteration 168/1000 | Loss: 0.00001035
Iteration 169/1000 | Loss: 0.00001035
Iteration 170/1000 | Loss: 0.00001034
Iteration 171/1000 | Loss: 0.00001034
Iteration 172/1000 | Loss: 0.00001034
Iteration 173/1000 | Loss: 0.00001034
Iteration 174/1000 | Loss: 0.00001034
Iteration 175/1000 | Loss: 0.00001034
Iteration 176/1000 | Loss: 0.00001034
Iteration 177/1000 | Loss: 0.00001034
Iteration 178/1000 | Loss: 0.00001034
Iteration 179/1000 | Loss: 0.00001034
Iteration 180/1000 | Loss: 0.00001034
Iteration 181/1000 | Loss: 0.00001034
Iteration 182/1000 | Loss: 0.00001034
Iteration 183/1000 | Loss: 0.00001034
Iteration 184/1000 | Loss: 0.00001034
Iteration 185/1000 | Loss: 0.00001034
Iteration 186/1000 | Loss: 0.00001034
Iteration 187/1000 | Loss: 0.00001034
Iteration 188/1000 | Loss: 0.00001034
Iteration 189/1000 | Loss: 0.00001034
Iteration 190/1000 | Loss: 0.00001034
Iteration 191/1000 | Loss: 0.00001034
Iteration 192/1000 | Loss: 0.00001034
Iteration 193/1000 | Loss: 0.00001034
Iteration 194/1000 | Loss: 0.00001034
Iteration 195/1000 | Loss: 0.00001034
Iteration 196/1000 | Loss: 0.00001034
Iteration 197/1000 | Loss: 0.00001034
Iteration 198/1000 | Loss: 0.00001034
Iteration 199/1000 | Loss: 0.00001034
Iteration 200/1000 | Loss: 0.00001034
Iteration 201/1000 | Loss: 0.00001034
Iteration 202/1000 | Loss: 0.00001034
Iteration 203/1000 | Loss: 0.00001034
Iteration 204/1000 | Loss: 0.00001034
Iteration 205/1000 | Loss: 0.00001034
Iteration 206/1000 | Loss: 0.00001034
Iteration 207/1000 | Loss: 0.00001034
Iteration 208/1000 | Loss: 0.00001034
Iteration 209/1000 | Loss: 0.00001034
Iteration 210/1000 | Loss: 0.00001034
Iteration 211/1000 | Loss: 0.00001034
Iteration 212/1000 | Loss: 0.00001034
Iteration 213/1000 | Loss: 0.00001034
Iteration 214/1000 | Loss: 0.00001034
Iteration 215/1000 | Loss: 0.00001034
Iteration 216/1000 | Loss: 0.00001034
Iteration 217/1000 | Loss: 0.00001034
Iteration 218/1000 | Loss: 0.00001034
Iteration 219/1000 | Loss: 0.00001034
Iteration 220/1000 | Loss: 0.00001034
Iteration 221/1000 | Loss: 0.00001034
Iteration 222/1000 | Loss: 0.00001034
Iteration 223/1000 | Loss: 0.00001034
Iteration 224/1000 | Loss: 0.00001034
Iteration 225/1000 | Loss: 0.00001034
Iteration 226/1000 | Loss: 0.00001034
Iteration 227/1000 | Loss: 0.00001034
Iteration 228/1000 | Loss: 0.00001034
Iteration 229/1000 | Loss: 0.00001034
Iteration 230/1000 | Loss: 0.00001034
Iteration 231/1000 | Loss: 0.00001034
Iteration 232/1000 | Loss: 0.00001034
Iteration 233/1000 | Loss: 0.00001034
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 233. Stopping optimization.
Last 5 losses: [1.034118849929655e-05, 1.034118849929655e-05, 1.034118849929655e-05, 1.034118849929655e-05, 1.034118849929655e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.034118849929655e-05

Optimization complete. Final v2v error: 2.723752975463867 mm

Highest mean error: 4.255438327789307 mm for frame 12

Lowest mean error: 2.2690300941467285 mm for frame 83

Saving results

Total time: 247.53208684921265
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00824484
Iteration 2/25 | Loss: 0.00097886
Iteration 3/25 | Loss: 0.00066385
Iteration 4/25 | Loss: 0.00060731
Iteration 5/25 | Loss: 0.00059098
Iteration 6/25 | Loss: 0.00058811
Iteration 7/25 | Loss: 0.00058759
Iteration 8/25 | Loss: 0.00058759
Iteration 9/25 | Loss: 0.00058759
Iteration 10/25 | Loss: 0.00058759
Iteration 11/25 | Loss: 0.00058759
Iteration 12/25 | Loss: 0.00058759
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005875934730283916, 0.0005875934730283916, 0.0005875934730283916, 0.0005875934730283916, 0.0005875934730283916]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005875934730283916

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26207447
Iteration 2/25 | Loss: 0.00014247
Iteration 3/25 | Loss: 0.00014246
Iteration 4/25 | Loss: 0.00014246
Iteration 5/25 | Loss: 0.00014246
Iteration 6/25 | Loss: 0.00014246
Iteration 7/25 | Loss: 0.00014246
Iteration 8/25 | Loss: 0.00014246
Iteration 9/25 | Loss: 0.00014246
Iteration 10/25 | Loss: 0.00014246
Iteration 11/25 | Loss: 0.00014246
Iteration 12/25 | Loss: 0.00014246
Iteration 13/25 | Loss: 0.00014246
Iteration 14/25 | Loss: 0.00014246
Iteration 15/25 | Loss: 0.00014246
Iteration 16/25 | Loss: 0.00014246
Iteration 17/25 | Loss: 0.00014246
Iteration 18/25 | Loss: 0.00014246
Iteration 19/25 | Loss: 0.00014246
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00014245703641790897, 0.00014245703641790897, 0.00014245703641790897, 0.00014245703641790897, 0.00014245703641790897]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00014245703641790897

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00014246
Iteration 2/1000 | Loss: 0.00003017
Iteration 3/1000 | Loss: 0.00002253
Iteration 4/1000 | Loss: 0.00002007
Iteration 5/1000 | Loss: 0.00001912
Iteration 6/1000 | Loss: 0.00001823
Iteration 7/1000 | Loss: 0.00001766
Iteration 8/1000 | Loss: 0.00001725
Iteration 9/1000 | Loss: 0.00001696
Iteration 10/1000 | Loss: 0.00001681
Iteration 11/1000 | Loss: 0.00001673
Iteration 12/1000 | Loss: 0.00001667
Iteration 13/1000 | Loss: 0.00001665
Iteration 14/1000 | Loss: 0.00001664
Iteration 15/1000 | Loss: 0.00001664
Iteration 16/1000 | Loss: 0.00001657
Iteration 17/1000 | Loss: 0.00001656
Iteration 18/1000 | Loss: 0.00001651
Iteration 19/1000 | Loss: 0.00001651
Iteration 20/1000 | Loss: 0.00001650
Iteration 21/1000 | Loss: 0.00001650
Iteration 22/1000 | Loss: 0.00001650
Iteration 23/1000 | Loss: 0.00001649
Iteration 24/1000 | Loss: 0.00001648
Iteration 25/1000 | Loss: 0.00001645
Iteration 26/1000 | Loss: 0.00001644
Iteration 27/1000 | Loss: 0.00001643
Iteration 28/1000 | Loss: 0.00001642
Iteration 29/1000 | Loss: 0.00001642
Iteration 30/1000 | Loss: 0.00001641
Iteration 31/1000 | Loss: 0.00001641
Iteration 32/1000 | Loss: 0.00001640
Iteration 33/1000 | Loss: 0.00001639
Iteration 34/1000 | Loss: 0.00001637
Iteration 35/1000 | Loss: 0.00001636
Iteration 36/1000 | Loss: 0.00001635
Iteration 37/1000 | Loss: 0.00001635
Iteration 38/1000 | Loss: 0.00001635
Iteration 39/1000 | Loss: 0.00001635
Iteration 40/1000 | Loss: 0.00001635
Iteration 41/1000 | Loss: 0.00001635
Iteration 42/1000 | Loss: 0.00001635
Iteration 43/1000 | Loss: 0.00001635
Iteration 44/1000 | Loss: 0.00001635
Iteration 45/1000 | Loss: 0.00001634
Iteration 46/1000 | Loss: 0.00001634
Iteration 47/1000 | Loss: 0.00001634
Iteration 48/1000 | Loss: 0.00001633
Iteration 49/1000 | Loss: 0.00001632
Iteration 50/1000 | Loss: 0.00001632
Iteration 51/1000 | Loss: 0.00001631
Iteration 52/1000 | Loss: 0.00001630
Iteration 53/1000 | Loss: 0.00001630
Iteration 54/1000 | Loss: 0.00001629
Iteration 55/1000 | Loss: 0.00001629
Iteration 56/1000 | Loss: 0.00001629
Iteration 57/1000 | Loss: 0.00001629
Iteration 58/1000 | Loss: 0.00001629
Iteration 59/1000 | Loss: 0.00001628
Iteration 60/1000 | Loss: 0.00001628
Iteration 61/1000 | Loss: 0.00001628
Iteration 62/1000 | Loss: 0.00001628
Iteration 63/1000 | Loss: 0.00001627
Iteration 64/1000 | Loss: 0.00001627
Iteration 65/1000 | Loss: 0.00001626
Iteration 66/1000 | Loss: 0.00001626
Iteration 67/1000 | Loss: 0.00001626
Iteration 68/1000 | Loss: 0.00001625
Iteration 69/1000 | Loss: 0.00001625
Iteration 70/1000 | Loss: 0.00001625
Iteration 71/1000 | Loss: 0.00001625
Iteration 72/1000 | Loss: 0.00001624
Iteration 73/1000 | Loss: 0.00001624
Iteration 74/1000 | Loss: 0.00001624
Iteration 75/1000 | Loss: 0.00001624
Iteration 76/1000 | Loss: 0.00001624
Iteration 77/1000 | Loss: 0.00001623
Iteration 78/1000 | Loss: 0.00001623
Iteration 79/1000 | Loss: 0.00001623
Iteration 80/1000 | Loss: 0.00001623
Iteration 81/1000 | Loss: 0.00001623
Iteration 82/1000 | Loss: 0.00001622
Iteration 83/1000 | Loss: 0.00001622
Iteration 84/1000 | Loss: 0.00001622
Iteration 85/1000 | Loss: 0.00001622
Iteration 86/1000 | Loss: 0.00001622
Iteration 87/1000 | Loss: 0.00001622
Iteration 88/1000 | Loss: 0.00001622
Iteration 89/1000 | Loss: 0.00001622
Iteration 90/1000 | Loss: 0.00001621
Iteration 91/1000 | Loss: 0.00001621
Iteration 92/1000 | Loss: 0.00001621
Iteration 93/1000 | Loss: 0.00001621
Iteration 94/1000 | Loss: 0.00001621
Iteration 95/1000 | Loss: 0.00001621
Iteration 96/1000 | Loss: 0.00001620
Iteration 97/1000 | Loss: 0.00001620
Iteration 98/1000 | Loss: 0.00001620
Iteration 99/1000 | Loss: 0.00001620
Iteration 100/1000 | Loss: 0.00001620
Iteration 101/1000 | Loss: 0.00001620
Iteration 102/1000 | Loss: 0.00001619
Iteration 103/1000 | Loss: 0.00001619
Iteration 104/1000 | Loss: 0.00001619
Iteration 105/1000 | Loss: 0.00001619
Iteration 106/1000 | Loss: 0.00001619
Iteration 107/1000 | Loss: 0.00001619
Iteration 108/1000 | Loss: 0.00001619
Iteration 109/1000 | Loss: 0.00001619
Iteration 110/1000 | Loss: 0.00001618
Iteration 111/1000 | Loss: 0.00001618
Iteration 112/1000 | Loss: 0.00001618
Iteration 113/1000 | Loss: 0.00001618
Iteration 114/1000 | Loss: 0.00001618
Iteration 115/1000 | Loss: 0.00001618
Iteration 116/1000 | Loss: 0.00001618
Iteration 117/1000 | Loss: 0.00001618
Iteration 118/1000 | Loss: 0.00001618
Iteration 119/1000 | Loss: 0.00001618
Iteration 120/1000 | Loss: 0.00001618
Iteration 121/1000 | Loss: 0.00001618
Iteration 122/1000 | Loss: 0.00001618
Iteration 123/1000 | Loss: 0.00001617
Iteration 124/1000 | Loss: 0.00001617
Iteration 125/1000 | Loss: 0.00001617
Iteration 126/1000 | Loss: 0.00001617
Iteration 127/1000 | Loss: 0.00001617
Iteration 128/1000 | Loss: 0.00001616
Iteration 129/1000 | Loss: 0.00001616
Iteration 130/1000 | Loss: 0.00001615
Iteration 131/1000 | Loss: 0.00001615
Iteration 132/1000 | Loss: 0.00001614
Iteration 133/1000 | Loss: 0.00001614
Iteration 134/1000 | Loss: 0.00001614
Iteration 135/1000 | Loss: 0.00001614
Iteration 136/1000 | Loss: 0.00001613
Iteration 137/1000 | Loss: 0.00001613
Iteration 138/1000 | Loss: 0.00001613
Iteration 139/1000 | Loss: 0.00001613
Iteration 140/1000 | Loss: 0.00001613
Iteration 141/1000 | Loss: 0.00001613
Iteration 142/1000 | Loss: 0.00001613
Iteration 143/1000 | Loss: 0.00001613
Iteration 144/1000 | Loss: 0.00001613
Iteration 145/1000 | Loss: 0.00001613
Iteration 146/1000 | Loss: 0.00001613
Iteration 147/1000 | Loss: 0.00001613
Iteration 148/1000 | Loss: 0.00001613
Iteration 149/1000 | Loss: 0.00001613
Iteration 150/1000 | Loss: 0.00001613
Iteration 151/1000 | Loss: 0.00001613
Iteration 152/1000 | Loss: 0.00001613
Iteration 153/1000 | Loss: 0.00001612
Iteration 154/1000 | Loss: 0.00001612
Iteration 155/1000 | Loss: 0.00001612
Iteration 156/1000 | Loss: 0.00001612
Iteration 157/1000 | Loss: 0.00001612
Iteration 158/1000 | Loss: 0.00001612
Iteration 159/1000 | Loss: 0.00001611
Iteration 160/1000 | Loss: 0.00001611
Iteration 161/1000 | Loss: 0.00001611
Iteration 162/1000 | Loss: 0.00001611
Iteration 163/1000 | Loss: 0.00001611
Iteration 164/1000 | Loss: 0.00001610
Iteration 165/1000 | Loss: 0.00001610
Iteration 166/1000 | Loss: 0.00001610
Iteration 167/1000 | Loss: 0.00001610
Iteration 168/1000 | Loss: 0.00001610
Iteration 169/1000 | Loss: 0.00001610
Iteration 170/1000 | Loss: 0.00001609
Iteration 171/1000 | Loss: 0.00001609
Iteration 172/1000 | Loss: 0.00001609
Iteration 173/1000 | Loss: 0.00001609
Iteration 174/1000 | Loss: 0.00001609
Iteration 175/1000 | Loss: 0.00001609
Iteration 176/1000 | Loss: 0.00001609
Iteration 177/1000 | Loss: 0.00001609
Iteration 178/1000 | Loss: 0.00001609
Iteration 179/1000 | Loss: 0.00001609
Iteration 180/1000 | Loss: 0.00001608
Iteration 181/1000 | Loss: 0.00001608
Iteration 182/1000 | Loss: 0.00001608
Iteration 183/1000 | Loss: 0.00001608
Iteration 184/1000 | Loss: 0.00001608
Iteration 185/1000 | Loss: 0.00001608
Iteration 186/1000 | Loss: 0.00001607
Iteration 187/1000 | Loss: 0.00001607
Iteration 188/1000 | Loss: 0.00001607
Iteration 189/1000 | Loss: 0.00001607
Iteration 190/1000 | Loss: 0.00001607
Iteration 191/1000 | Loss: 0.00001607
Iteration 192/1000 | Loss: 0.00001607
Iteration 193/1000 | Loss: 0.00001607
Iteration 194/1000 | Loss: 0.00001607
Iteration 195/1000 | Loss: 0.00001607
Iteration 196/1000 | Loss: 0.00001607
Iteration 197/1000 | Loss: 0.00001606
Iteration 198/1000 | Loss: 0.00001606
Iteration 199/1000 | Loss: 0.00001606
Iteration 200/1000 | Loss: 0.00001606
Iteration 201/1000 | Loss: 0.00001606
Iteration 202/1000 | Loss: 0.00001606
Iteration 203/1000 | Loss: 0.00001606
Iteration 204/1000 | Loss: 0.00001606
Iteration 205/1000 | Loss: 0.00001606
Iteration 206/1000 | Loss: 0.00001606
Iteration 207/1000 | Loss: 0.00001606
Iteration 208/1000 | Loss: 0.00001605
Iteration 209/1000 | Loss: 0.00001605
Iteration 210/1000 | Loss: 0.00001605
Iteration 211/1000 | Loss: 0.00001605
Iteration 212/1000 | Loss: 0.00001605
Iteration 213/1000 | Loss: 0.00001605
Iteration 214/1000 | Loss: 0.00001605
Iteration 215/1000 | Loss: 0.00001605
Iteration 216/1000 | Loss: 0.00001605
Iteration 217/1000 | Loss: 0.00001605
Iteration 218/1000 | Loss: 0.00001605
Iteration 219/1000 | Loss: 0.00001605
Iteration 220/1000 | Loss: 0.00001605
Iteration 221/1000 | Loss: 0.00001605
Iteration 222/1000 | Loss: 0.00001605
Iteration 223/1000 | Loss: 0.00001605
Iteration 224/1000 | Loss: 0.00001605
Iteration 225/1000 | Loss: 0.00001605
Iteration 226/1000 | Loss: 0.00001605
Iteration 227/1000 | Loss: 0.00001605
Iteration 228/1000 | Loss: 0.00001605
Iteration 229/1000 | Loss: 0.00001605
Iteration 230/1000 | Loss: 0.00001605
Iteration 231/1000 | Loss: 0.00001605
Iteration 232/1000 | Loss: 0.00001605
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 232. Stopping optimization.
Last 5 losses: [1.6048392353695817e-05, 1.6048392353695817e-05, 1.6048392353695817e-05, 1.6048392353695817e-05, 1.6048392353695817e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6048392353695817e-05

Optimization complete. Final v2v error: 3.3109474182128906 mm

Highest mean error: 5.0252685546875 mm for frame 76

Lowest mean error: 2.685459613800049 mm for frame 106

Saving results

Total time: 43.26486396789551
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00906258
Iteration 2/25 | Loss: 0.00112312
Iteration 3/25 | Loss: 0.00075696
Iteration 4/25 | Loss: 0.00071046
Iteration 5/25 | Loss: 0.00070159
Iteration 6/25 | Loss: 0.00069984
Iteration 7/25 | Loss: 0.00069963
Iteration 8/25 | Loss: 0.00069963
Iteration 9/25 | Loss: 0.00069963
Iteration 10/25 | Loss: 0.00069963
Iteration 11/25 | Loss: 0.00069963
Iteration 12/25 | Loss: 0.00069963
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006996344891376793, 0.0006996344891376793, 0.0006996344891376793, 0.0006996344891376793, 0.0006996344891376793]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006996344891376793

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.98777294
Iteration 2/25 | Loss: 0.00022729
Iteration 3/25 | Loss: 0.00022728
Iteration 4/25 | Loss: 0.00022728
Iteration 5/25 | Loss: 0.00022728
Iteration 6/25 | Loss: 0.00022728
Iteration 7/25 | Loss: 0.00022728
Iteration 8/25 | Loss: 0.00022728
Iteration 9/25 | Loss: 0.00022728
Iteration 10/25 | Loss: 0.00022728
Iteration 11/25 | Loss: 0.00022728
Iteration 12/25 | Loss: 0.00022728
Iteration 13/25 | Loss: 0.00022728
Iteration 14/25 | Loss: 0.00022728
Iteration 15/25 | Loss: 0.00022728
Iteration 16/25 | Loss: 0.00022728
Iteration 17/25 | Loss: 0.00022728
Iteration 18/25 | Loss: 0.00022728
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00022728250769432634, 0.00022728250769432634, 0.00022728250769432634, 0.00022728250769432634, 0.00022728250769432634]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00022728250769432634

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00022728
Iteration 2/1000 | Loss: 0.00003088
Iteration 3/1000 | Loss: 0.00002459
Iteration 4/1000 | Loss: 0.00002272
Iteration 5/1000 | Loss: 0.00002160
Iteration 6/1000 | Loss: 0.00002081
Iteration 7/1000 | Loss: 0.00002013
Iteration 8/1000 | Loss: 0.00001957
Iteration 9/1000 | Loss: 0.00001934
Iteration 10/1000 | Loss: 0.00001923
Iteration 11/1000 | Loss: 0.00001922
Iteration 12/1000 | Loss: 0.00001921
Iteration 13/1000 | Loss: 0.00001920
Iteration 14/1000 | Loss: 0.00001920
Iteration 15/1000 | Loss: 0.00001918
Iteration 16/1000 | Loss: 0.00001903
Iteration 17/1000 | Loss: 0.00001899
Iteration 18/1000 | Loss: 0.00001899
Iteration 19/1000 | Loss: 0.00001897
Iteration 20/1000 | Loss: 0.00001896
Iteration 21/1000 | Loss: 0.00001895
Iteration 22/1000 | Loss: 0.00001895
Iteration 23/1000 | Loss: 0.00001894
Iteration 24/1000 | Loss: 0.00001894
Iteration 25/1000 | Loss: 0.00001894
Iteration 26/1000 | Loss: 0.00001893
Iteration 27/1000 | Loss: 0.00001892
Iteration 28/1000 | Loss: 0.00001892
Iteration 29/1000 | Loss: 0.00001892
Iteration 30/1000 | Loss: 0.00001892
Iteration 31/1000 | Loss: 0.00001892
Iteration 32/1000 | Loss: 0.00001892
Iteration 33/1000 | Loss: 0.00001892
Iteration 34/1000 | Loss: 0.00001892
Iteration 35/1000 | Loss: 0.00001892
Iteration 36/1000 | Loss: 0.00001892
Iteration 37/1000 | Loss: 0.00001891
Iteration 38/1000 | Loss: 0.00001891
Iteration 39/1000 | Loss: 0.00001891
Iteration 40/1000 | Loss: 0.00001891
Iteration 41/1000 | Loss: 0.00001891
Iteration 42/1000 | Loss: 0.00001890
Iteration 43/1000 | Loss: 0.00001890
Iteration 44/1000 | Loss: 0.00001890
Iteration 45/1000 | Loss: 0.00001890
Iteration 46/1000 | Loss: 0.00001889
Iteration 47/1000 | Loss: 0.00001889
Iteration 48/1000 | Loss: 0.00001889
Iteration 49/1000 | Loss: 0.00001889
Iteration 50/1000 | Loss: 0.00001889
Iteration 51/1000 | Loss: 0.00001889
Iteration 52/1000 | Loss: 0.00001889
Iteration 53/1000 | Loss: 0.00001889
Iteration 54/1000 | Loss: 0.00001889
Iteration 55/1000 | Loss: 0.00001889
Iteration 56/1000 | Loss: 0.00001889
Iteration 57/1000 | Loss: 0.00001889
Iteration 58/1000 | Loss: 0.00001889
Iteration 59/1000 | Loss: 0.00001889
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 59. Stopping optimization.
Last 5 losses: [1.8885792087530717e-05, 1.8885792087530717e-05, 1.8885792087530717e-05, 1.8885792087530717e-05, 1.8885792087530717e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8885792087530717e-05

Optimization complete. Final v2v error: 3.676786184310913 mm

Highest mean error: 3.9704251289367676 mm for frame 5

Lowest mean error: 3.434513807296753 mm for frame 130

Saving results

Total time: 29.16050934791565
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01057484
Iteration 2/25 | Loss: 0.00115076
Iteration 3/25 | Loss: 0.00077578
Iteration 4/25 | Loss: 0.00070951
Iteration 5/25 | Loss: 0.00069215
Iteration 6/25 | Loss: 0.00068870
Iteration 7/25 | Loss: 0.00068768
Iteration 8/25 | Loss: 0.00068768
Iteration 9/25 | Loss: 0.00068768
Iteration 10/25 | Loss: 0.00068768
Iteration 11/25 | Loss: 0.00068768
Iteration 12/25 | Loss: 0.00068768
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006876798579469323, 0.0006876798579469323, 0.0006876798579469323, 0.0006876798579469323, 0.0006876798579469323]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006876798579469323

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57450116
Iteration 2/25 | Loss: 0.00023289
Iteration 3/25 | Loss: 0.00023289
Iteration 4/25 | Loss: 0.00023289
Iteration 5/25 | Loss: 0.00023289
Iteration 6/25 | Loss: 0.00023289
Iteration 7/25 | Loss: 0.00023289
Iteration 8/25 | Loss: 0.00023289
Iteration 9/25 | Loss: 0.00023289
Iteration 10/25 | Loss: 0.00023289
Iteration 11/25 | Loss: 0.00023289
Iteration 12/25 | Loss: 0.00023289
Iteration 13/25 | Loss: 0.00023289
Iteration 14/25 | Loss: 0.00023289
Iteration 15/25 | Loss: 0.00023289
Iteration 16/25 | Loss: 0.00023289
Iteration 17/25 | Loss: 0.00023289
Iteration 18/25 | Loss: 0.00023289
Iteration 19/25 | Loss: 0.00023289
Iteration 20/25 | Loss: 0.00023289
Iteration 21/25 | Loss: 0.00023289
Iteration 22/25 | Loss: 0.00023289
Iteration 23/25 | Loss: 0.00023289
Iteration 24/25 | Loss: 0.00023289
Iteration 25/25 | Loss: 0.00023289

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00023289
Iteration 2/1000 | Loss: 0.00003349
Iteration 3/1000 | Loss: 0.00002433
Iteration 4/1000 | Loss: 0.00002268
Iteration 5/1000 | Loss: 0.00002142
Iteration 6/1000 | Loss: 0.00002068
Iteration 7/1000 | Loss: 0.00002016
Iteration 8/1000 | Loss: 0.00001973
Iteration 9/1000 | Loss: 0.00001942
Iteration 10/1000 | Loss: 0.00001920
Iteration 11/1000 | Loss: 0.00001904
Iteration 12/1000 | Loss: 0.00001893
Iteration 13/1000 | Loss: 0.00001891
Iteration 14/1000 | Loss: 0.00001890
Iteration 15/1000 | Loss: 0.00001889
Iteration 16/1000 | Loss: 0.00001889
Iteration 17/1000 | Loss: 0.00001888
Iteration 18/1000 | Loss: 0.00001887
Iteration 19/1000 | Loss: 0.00001886
Iteration 20/1000 | Loss: 0.00001886
Iteration 21/1000 | Loss: 0.00001883
Iteration 22/1000 | Loss: 0.00001883
Iteration 23/1000 | Loss: 0.00001883
Iteration 24/1000 | Loss: 0.00001882
Iteration 25/1000 | Loss: 0.00001882
Iteration 26/1000 | Loss: 0.00001882
Iteration 27/1000 | Loss: 0.00001882
Iteration 28/1000 | Loss: 0.00001882
Iteration 29/1000 | Loss: 0.00001882
Iteration 30/1000 | Loss: 0.00001882
Iteration 31/1000 | Loss: 0.00001881
Iteration 32/1000 | Loss: 0.00001881
Iteration 33/1000 | Loss: 0.00001879
Iteration 34/1000 | Loss: 0.00001879
Iteration 35/1000 | Loss: 0.00001878
Iteration 36/1000 | Loss: 0.00001878
Iteration 37/1000 | Loss: 0.00001878
Iteration 38/1000 | Loss: 0.00001877
Iteration 39/1000 | Loss: 0.00001876
Iteration 40/1000 | Loss: 0.00001876
Iteration 41/1000 | Loss: 0.00001876
Iteration 42/1000 | Loss: 0.00001876
Iteration 43/1000 | Loss: 0.00001876
Iteration 44/1000 | Loss: 0.00001876
Iteration 45/1000 | Loss: 0.00001876
Iteration 46/1000 | Loss: 0.00001876
Iteration 47/1000 | Loss: 0.00001876
Iteration 48/1000 | Loss: 0.00001876
Iteration 49/1000 | Loss: 0.00001875
Iteration 50/1000 | Loss: 0.00001875
Iteration 51/1000 | Loss: 0.00001875
Iteration 52/1000 | Loss: 0.00001875
Iteration 53/1000 | Loss: 0.00001875
Iteration 54/1000 | Loss: 0.00001874
Iteration 55/1000 | Loss: 0.00001874
Iteration 56/1000 | Loss: 0.00001874
Iteration 57/1000 | Loss: 0.00001873
Iteration 58/1000 | Loss: 0.00001873
Iteration 59/1000 | Loss: 0.00001873
Iteration 60/1000 | Loss: 0.00001873
Iteration 61/1000 | Loss: 0.00001873
Iteration 62/1000 | Loss: 0.00001873
Iteration 63/1000 | Loss: 0.00001872
Iteration 64/1000 | Loss: 0.00001871
Iteration 65/1000 | Loss: 0.00001871
Iteration 66/1000 | Loss: 0.00001871
Iteration 67/1000 | Loss: 0.00001871
Iteration 68/1000 | Loss: 0.00001871
Iteration 69/1000 | Loss: 0.00001871
Iteration 70/1000 | Loss: 0.00001871
Iteration 71/1000 | Loss: 0.00001871
Iteration 72/1000 | Loss: 0.00001870
Iteration 73/1000 | Loss: 0.00001870
Iteration 74/1000 | Loss: 0.00001870
Iteration 75/1000 | Loss: 0.00001870
Iteration 76/1000 | Loss: 0.00001870
Iteration 77/1000 | Loss: 0.00001869
Iteration 78/1000 | Loss: 0.00001869
Iteration 79/1000 | Loss: 0.00001869
Iteration 80/1000 | Loss: 0.00001868
Iteration 81/1000 | Loss: 0.00001868
Iteration 82/1000 | Loss: 0.00001868
Iteration 83/1000 | Loss: 0.00001868
Iteration 84/1000 | Loss: 0.00001868
Iteration 85/1000 | Loss: 0.00001868
Iteration 86/1000 | Loss: 0.00001868
Iteration 87/1000 | Loss: 0.00001868
Iteration 88/1000 | Loss: 0.00001867
Iteration 89/1000 | Loss: 0.00001867
Iteration 90/1000 | Loss: 0.00001867
Iteration 91/1000 | Loss: 0.00001867
Iteration 92/1000 | Loss: 0.00001867
Iteration 93/1000 | Loss: 0.00001867
Iteration 94/1000 | Loss: 0.00001867
Iteration 95/1000 | Loss: 0.00001867
Iteration 96/1000 | Loss: 0.00001866
Iteration 97/1000 | Loss: 0.00001866
Iteration 98/1000 | Loss: 0.00001866
Iteration 99/1000 | Loss: 0.00001866
Iteration 100/1000 | Loss: 0.00001866
Iteration 101/1000 | Loss: 0.00001866
Iteration 102/1000 | Loss: 0.00001866
Iteration 103/1000 | Loss: 0.00001866
Iteration 104/1000 | Loss: 0.00001866
Iteration 105/1000 | Loss: 0.00001866
Iteration 106/1000 | Loss: 0.00001866
Iteration 107/1000 | Loss: 0.00001866
Iteration 108/1000 | Loss: 0.00001866
Iteration 109/1000 | Loss: 0.00001866
Iteration 110/1000 | Loss: 0.00001865
Iteration 111/1000 | Loss: 0.00001865
Iteration 112/1000 | Loss: 0.00001865
Iteration 113/1000 | Loss: 0.00001865
Iteration 114/1000 | Loss: 0.00001865
Iteration 115/1000 | Loss: 0.00001865
Iteration 116/1000 | Loss: 0.00001865
Iteration 117/1000 | Loss: 0.00001865
Iteration 118/1000 | Loss: 0.00001865
Iteration 119/1000 | Loss: 0.00001865
Iteration 120/1000 | Loss: 0.00001865
Iteration 121/1000 | Loss: 0.00001865
Iteration 122/1000 | Loss: 0.00001865
Iteration 123/1000 | Loss: 0.00001865
Iteration 124/1000 | Loss: 0.00001864
Iteration 125/1000 | Loss: 0.00001864
Iteration 126/1000 | Loss: 0.00001864
Iteration 127/1000 | Loss: 0.00001864
Iteration 128/1000 | Loss: 0.00001864
Iteration 129/1000 | Loss: 0.00001864
Iteration 130/1000 | Loss: 0.00001864
Iteration 131/1000 | Loss: 0.00001864
Iteration 132/1000 | Loss: 0.00001864
Iteration 133/1000 | Loss: 0.00001864
Iteration 134/1000 | Loss: 0.00001864
Iteration 135/1000 | Loss: 0.00001864
Iteration 136/1000 | Loss: 0.00001864
Iteration 137/1000 | Loss: 0.00001864
Iteration 138/1000 | Loss: 0.00001864
Iteration 139/1000 | Loss: 0.00001864
Iteration 140/1000 | Loss: 0.00001864
Iteration 141/1000 | Loss: 0.00001864
Iteration 142/1000 | Loss: 0.00001864
Iteration 143/1000 | Loss: 0.00001864
Iteration 144/1000 | Loss: 0.00001864
Iteration 145/1000 | Loss: 0.00001863
Iteration 146/1000 | Loss: 0.00001863
Iteration 147/1000 | Loss: 0.00001863
Iteration 148/1000 | Loss: 0.00001863
Iteration 149/1000 | Loss: 0.00001863
Iteration 150/1000 | Loss: 0.00001863
Iteration 151/1000 | Loss: 0.00001863
Iteration 152/1000 | Loss: 0.00001863
Iteration 153/1000 | Loss: 0.00001863
Iteration 154/1000 | Loss: 0.00001863
Iteration 155/1000 | Loss: 0.00001863
Iteration 156/1000 | Loss: 0.00001863
Iteration 157/1000 | Loss: 0.00001863
Iteration 158/1000 | Loss: 0.00001863
Iteration 159/1000 | Loss: 0.00001863
Iteration 160/1000 | Loss: 0.00001863
Iteration 161/1000 | Loss: 0.00001863
Iteration 162/1000 | Loss: 0.00001863
Iteration 163/1000 | Loss: 0.00001863
Iteration 164/1000 | Loss: 0.00001863
Iteration 165/1000 | Loss: 0.00001863
Iteration 166/1000 | Loss: 0.00001863
Iteration 167/1000 | Loss: 0.00001863
Iteration 168/1000 | Loss: 0.00001863
Iteration 169/1000 | Loss: 0.00001863
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.86285706149647e-05, 1.86285706149647e-05, 1.86285706149647e-05, 1.86285706149647e-05, 1.86285706149647e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.86285706149647e-05

Optimization complete. Final v2v error: 3.661627769470215 mm

Highest mean error: 4.161275386810303 mm for frame 124

Lowest mean error: 3.1674399375915527 mm for frame 40

Saving results

Total time: 40.8068265914917
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01093699
Iteration 2/25 | Loss: 0.00093682
Iteration 3/25 | Loss: 0.00066528
Iteration 4/25 | Loss: 0.00061054
Iteration 5/25 | Loss: 0.00060357
Iteration 6/25 | Loss: 0.00060372
Iteration 7/25 | Loss: 0.00059640
Iteration 8/25 | Loss: 0.00059191
Iteration 9/25 | Loss: 0.00059056
Iteration 10/25 | Loss: 0.00059008
Iteration 11/25 | Loss: 0.00059000
Iteration 12/25 | Loss: 0.00059000
Iteration 13/25 | Loss: 0.00059000
Iteration 14/25 | Loss: 0.00059000
Iteration 15/25 | Loss: 0.00059000
Iteration 16/25 | Loss: 0.00059000
Iteration 17/25 | Loss: 0.00059000
Iteration 18/25 | Loss: 0.00059000
Iteration 19/25 | Loss: 0.00059000
Iteration 20/25 | Loss: 0.00058999
Iteration 21/25 | Loss: 0.00058999
Iteration 22/25 | Loss: 0.00058999
Iteration 23/25 | Loss: 0.00058999
Iteration 24/25 | Loss: 0.00058999
Iteration 25/25 | Loss: 0.00058999

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.39134908
Iteration 2/25 | Loss: 0.00013170
Iteration 3/25 | Loss: 0.00013170
Iteration 4/25 | Loss: 0.00013170
Iteration 5/25 | Loss: 0.00013170
Iteration 6/25 | Loss: 0.00013170
Iteration 7/25 | Loss: 0.00013170
Iteration 8/25 | Loss: 0.00013170
Iteration 9/25 | Loss: 0.00013170
Iteration 10/25 | Loss: 0.00013170
Iteration 11/25 | Loss: 0.00013170
Iteration 12/25 | Loss: 0.00013170
Iteration 13/25 | Loss: 0.00013170
Iteration 14/25 | Loss: 0.00013170
Iteration 15/25 | Loss: 0.00013170
Iteration 16/25 | Loss: 0.00013170
Iteration 17/25 | Loss: 0.00013170
Iteration 18/25 | Loss: 0.00013170
Iteration 19/25 | Loss: 0.00013170
Iteration 20/25 | Loss: 0.00013170
Iteration 21/25 | Loss: 0.00013170
Iteration 22/25 | Loss: 0.00013170
Iteration 23/25 | Loss: 0.00013170
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00013169959129299968, 0.00013169959129299968, 0.00013169959129299968, 0.00013169959129299968, 0.00013169959129299968]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00013169959129299968

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00013170
Iteration 2/1000 | Loss: 0.00001817
Iteration 3/1000 | Loss: 0.00001437
Iteration 4/1000 | Loss: 0.00001342
Iteration 5/1000 | Loss: 0.00001271
Iteration 6/1000 | Loss: 0.00001233
Iteration 7/1000 | Loss: 0.00001203
Iteration 8/1000 | Loss: 0.00001175
Iteration 9/1000 | Loss: 0.00001160
Iteration 10/1000 | Loss: 0.00001157
Iteration 11/1000 | Loss: 0.00001156
Iteration 12/1000 | Loss: 0.00001148
Iteration 13/1000 | Loss: 0.00001144
Iteration 14/1000 | Loss: 0.00001141
Iteration 15/1000 | Loss: 0.00001138
Iteration 16/1000 | Loss: 0.00001137
Iteration 17/1000 | Loss: 0.00001136
Iteration 18/1000 | Loss: 0.00001135
Iteration 19/1000 | Loss: 0.00001135
Iteration 20/1000 | Loss: 0.00001134
Iteration 21/1000 | Loss: 0.00001134
Iteration 22/1000 | Loss: 0.00001133
Iteration 23/1000 | Loss: 0.00001133
Iteration 24/1000 | Loss: 0.00001132
Iteration 25/1000 | Loss: 0.00001132
Iteration 26/1000 | Loss: 0.00001131
Iteration 27/1000 | Loss: 0.00001131
Iteration 28/1000 | Loss: 0.00001130
Iteration 29/1000 | Loss: 0.00001130
Iteration 30/1000 | Loss: 0.00001129
Iteration 31/1000 | Loss: 0.00001129
Iteration 32/1000 | Loss: 0.00001129
Iteration 33/1000 | Loss: 0.00001129
Iteration 34/1000 | Loss: 0.00001129
Iteration 35/1000 | Loss: 0.00001129
Iteration 36/1000 | Loss: 0.00001129
Iteration 37/1000 | Loss: 0.00001129
Iteration 38/1000 | Loss: 0.00001129
Iteration 39/1000 | Loss: 0.00001128
Iteration 40/1000 | Loss: 0.00001128
Iteration 41/1000 | Loss: 0.00001128
Iteration 42/1000 | Loss: 0.00001128
Iteration 43/1000 | Loss: 0.00001127
Iteration 44/1000 | Loss: 0.00001127
Iteration 45/1000 | Loss: 0.00001126
Iteration 46/1000 | Loss: 0.00001126
Iteration 47/1000 | Loss: 0.00001126
Iteration 48/1000 | Loss: 0.00001126
Iteration 49/1000 | Loss: 0.00001126
Iteration 50/1000 | Loss: 0.00001126
Iteration 51/1000 | Loss: 0.00001126
Iteration 52/1000 | Loss: 0.00001126
Iteration 53/1000 | Loss: 0.00001126
Iteration 54/1000 | Loss: 0.00001126
Iteration 55/1000 | Loss: 0.00001125
Iteration 56/1000 | Loss: 0.00001125
Iteration 57/1000 | Loss: 0.00001125
Iteration 58/1000 | Loss: 0.00001124
Iteration 59/1000 | Loss: 0.00001124
Iteration 60/1000 | Loss: 0.00001124
Iteration 61/1000 | Loss: 0.00001123
Iteration 62/1000 | Loss: 0.00001123
Iteration 63/1000 | Loss: 0.00001123
Iteration 64/1000 | Loss: 0.00001123
Iteration 65/1000 | Loss: 0.00001123
Iteration 66/1000 | Loss: 0.00001122
Iteration 67/1000 | Loss: 0.00001122
Iteration 68/1000 | Loss: 0.00001122
Iteration 69/1000 | Loss: 0.00001121
Iteration 70/1000 | Loss: 0.00001121
Iteration 71/1000 | Loss: 0.00001121
Iteration 72/1000 | Loss: 0.00001121
Iteration 73/1000 | Loss: 0.00001120
Iteration 74/1000 | Loss: 0.00001120
Iteration 75/1000 | Loss: 0.00001120
Iteration 76/1000 | Loss: 0.00001120
Iteration 77/1000 | Loss: 0.00001120
Iteration 78/1000 | Loss: 0.00001120
Iteration 79/1000 | Loss: 0.00001120
Iteration 80/1000 | Loss: 0.00001120
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 80. Stopping optimization.
Last 5 losses: [1.1196438208571635e-05, 1.1196438208571635e-05, 1.1196438208571635e-05, 1.1196438208571635e-05, 1.1196438208571635e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1196438208571635e-05

Optimization complete. Final v2v error: 2.8503170013427734 mm

Highest mean error: 3.021425485610962 mm for frame 145

Lowest mean error: 2.5743393898010254 mm for frame 84

Saving results

Total time: 45.21665549278259
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00607166
Iteration 2/25 | Loss: 0.00138291
Iteration 3/25 | Loss: 0.00074794
Iteration 4/25 | Loss: 0.00069158
Iteration 5/25 | Loss: 0.00068129
Iteration 6/25 | Loss: 0.00067975
Iteration 7/25 | Loss: 0.00067929
Iteration 8/25 | Loss: 0.00067929
Iteration 9/25 | Loss: 0.00067929
Iteration 10/25 | Loss: 0.00067929
Iteration 11/25 | Loss: 0.00067929
Iteration 12/25 | Loss: 0.00067929
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006792937638238072, 0.0006792937638238072, 0.0006792937638238072, 0.0006792937638238072, 0.0006792937638238072]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006792937638238072

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44260955
Iteration 2/25 | Loss: 0.00025485
Iteration 3/25 | Loss: 0.00025483
Iteration 4/25 | Loss: 0.00025482
Iteration 5/25 | Loss: 0.00025482
Iteration 6/25 | Loss: 0.00025482
Iteration 7/25 | Loss: 0.00025482
Iteration 8/25 | Loss: 0.00025482
Iteration 9/25 | Loss: 0.00025482
Iteration 10/25 | Loss: 0.00025482
Iteration 11/25 | Loss: 0.00025482
Iteration 12/25 | Loss: 0.00025482
Iteration 13/25 | Loss: 0.00025482
Iteration 14/25 | Loss: 0.00025482
Iteration 15/25 | Loss: 0.00025482
Iteration 16/25 | Loss: 0.00025482
Iteration 17/25 | Loss: 0.00025482
Iteration 18/25 | Loss: 0.00025482
Iteration 19/25 | Loss: 0.00025482
Iteration 20/25 | Loss: 0.00025482
Iteration 21/25 | Loss: 0.00025482
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0002548231277614832, 0.0002548231277614832, 0.0002548231277614832, 0.0002548231277614832, 0.0002548231277614832]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002548231277614832

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025482
Iteration 2/1000 | Loss: 0.00003109
Iteration 3/1000 | Loss: 0.00002186
Iteration 4/1000 | Loss: 0.00001786
Iteration 5/1000 | Loss: 0.00001689
Iteration 6/1000 | Loss: 0.00001615
Iteration 7/1000 | Loss: 0.00001579
Iteration 8/1000 | Loss: 0.00001556
Iteration 9/1000 | Loss: 0.00001544
Iteration 10/1000 | Loss: 0.00001543
Iteration 11/1000 | Loss: 0.00001541
Iteration 12/1000 | Loss: 0.00001541
Iteration 13/1000 | Loss: 0.00001540
Iteration 14/1000 | Loss: 0.00001540
Iteration 15/1000 | Loss: 0.00001540
Iteration 16/1000 | Loss: 0.00001540
Iteration 17/1000 | Loss: 0.00001540
Iteration 18/1000 | Loss: 0.00001540
Iteration 19/1000 | Loss: 0.00001538
Iteration 20/1000 | Loss: 0.00001538
Iteration 21/1000 | Loss: 0.00001531
Iteration 22/1000 | Loss: 0.00001528
Iteration 23/1000 | Loss: 0.00001525
Iteration 24/1000 | Loss: 0.00001524
Iteration 25/1000 | Loss: 0.00001523
Iteration 26/1000 | Loss: 0.00001522
Iteration 27/1000 | Loss: 0.00001521
Iteration 28/1000 | Loss: 0.00001521
Iteration 29/1000 | Loss: 0.00001520
Iteration 30/1000 | Loss: 0.00001520
Iteration 31/1000 | Loss: 0.00001519
Iteration 32/1000 | Loss: 0.00001519
Iteration 33/1000 | Loss: 0.00001519
Iteration 34/1000 | Loss: 0.00001519
Iteration 35/1000 | Loss: 0.00001518
Iteration 36/1000 | Loss: 0.00001516
Iteration 37/1000 | Loss: 0.00001516
Iteration 38/1000 | Loss: 0.00001516
Iteration 39/1000 | Loss: 0.00001516
Iteration 40/1000 | Loss: 0.00001516
Iteration 41/1000 | Loss: 0.00001516
Iteration 42/1000 | Loss: 0.00001515
Iteration 43/1000 | Loss: 0.00001515
Iteration 44/1000 | Loss: 0.00001515
Iteration 45/1000 | Loss: 0.00001515
Iteration 46/1000 | Loss: 0.00001515
Iteration 47/1000 | Loss: 0.00001515
Iteration 48/1000 | Loss: 0.00001515
Iteration 49/1000 | Loss: 0.00001515
Iteration 50/1000 | Loss: 0.00001515
Iteration 51/1000 | Loss: 0.00001514
Iteration 52/1000 | Loss: 0.00001514
Iteration 53/1000 | Loss: 0.00001514
Iteration 54/1000 | Loss: 0.00001513
Iteration 55/1000 | Loss: 0.00001513
Iteration 56/1000 | Loss: 0.00001512
Iteration 57/1000 | Loss: 0.00001512
Iteration 58/1000 | Loss: 0.00001512
Iteration 59/1000 | Loss: 0.00001511
Iteration 60/1000 | Loss: 0.00001511
Iteration 61/1000 | Loss: 0.00001510
Iteration 62/1000 | Loss: 0.00001510
Iteration 63/1000 | Loss: 0.00001510
Iteration 64/1000 | Loss: 0.00001509
Iteration 65/1000 | Loss: 0.00001509
Iteration 66/1000 | Loss: 0.00001509
Iteration 67/1000 | Loss: 0.00001508
Iteration 68/1000 | Loss: 0.00001507
Iteration 69/1000 | Loss: 0.00001507
Iteration 70/1000 | Loss: 0.00001507
Iteration 71/1000 | Loss: 0.00001506
Iteration 72/1000 | Loss: 0.00001506
Iteration 73/1000 | Loss: 0.00001506
Iteration 74/1000 | Loss: 0.00001506
Iteration 75/1000 | Loss: 0.00001505
Iteration 76/1000 | Loss: 0.00001505
Iteration 77/1000 | Loss: 0.00001505
Iteration 78/1000 | Loss: 0.00001505
Iteration 79/1000 | Loss: 0.00001505
Iteration 80/1000 | Loss: 0.00001505
Iteration 81/1000 | Loss: 0.00001505
Iteration 82/1000 | Loss: 0.00001504
Iteration 83/1000 | Loss: 0.00001504
Iteration 84/1000 | Loss: 0.00001504
Iteration 85/1000 | Loss: 0.00001504
Iteration 86/1000 | Loss: 0.00001504
Iteration 87/1000 | Loss: 0.00001504
Iteration 88/1000 | Loss: 0.00001504
Iteration 89/1000 | Loss: 0.00001504
Iteration 90/1000 | Loss: 0.00001504
Iteration 91/1000 | Loss: 0.00001504
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [1.5040543075883761e-05, 1.5040543075883761e-05, 1.5040543075883761e-05, 1.5040543075883761e-05, 1.5040543075883761e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5040543075883761e-05

Optimization complete. Final v2v error: 3.299499988555908 mm

Highest mean error: 3.495634078979492 mm for frame 12

Lowest mean error: 2.959848403930664 mm for frame 5

Saving results

Total time: 31.72144389152527
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01075032
Iteration 2/25 | Loss: 0.00324842
Iteration 3/25 | Loss: 0.00271980
Iteration 4/25 | Loss: 0.00251122
Iteration 5/25 | Loss: 0.00246199
Iteration 6/25 | Loss: 0.00230066
Iteration 7/25 | Loss: 0.00221911
Iteration 8/25 | Loss: 0.00214259
Iteration 9/25 | Loss: 0.00208058
Iteration 10/25 | Loss: 0.00209380
Iteration 11/25 | Loss: 0.00206726
Iteration 12/25 | Loss: 0.00201434
Iteration 13/25 | Loss: 0.00200601
Iteration 14/25 | Loss: 0.00197150
Iteration 15/25 | Loss: 0.00195901
Iteration 16/25 | Loss: 0.00195118
Iteration 17/25 | Loss: 0.00194359
Iteration 18/25 | Loss: 0.00194681
Iteration 19/25 | Loss: 0.00195255
Iteration 20/25 | Loss: 0.00193832
Iteration 21/25 | Loss: 0.00195232
Iteration 22/25 | Loss: 0.00197186
Iteration 23/25 | Loss: 0.00195592
Iteration 24/25 | Loss: 0.00197469
Iteration 25/25 | Loss: 0.00187310

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14660120
Iteration 2/25 | Loss: 0.00778956
Iteration 3/25 | Loss: 0.00778953
Iteration 4/25 | Loss: 0.00778952
Iteration 5/25 | Loss: 0.00778952
Iteration 6/25 | Loss: 0.00778952
Iteration 7/25 | Loss: 0.00778952
Iteration 8/25 | Loss: 0.00778952
Iteration 9/25 | Loss: 0.00778952
Iteration 10/25 | Loss: 0.00778952
Iteration 11/25 | Loss: 0.00778952
Iteration 12/25 | Loss: 0.00778952
Iteration 13/25 | Loss: 0.00778952
Iteration 14/25 | Loss: 0.00778952
Iteration 15/25 | Loss: 0.00778952
Iteration 16/25 | Loss: 0.00778952
Iteration 17/25 | Loss: 0.00778952
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.007789519615471363, 0.007789519615471363, 0.007789519615471363, 0.007789519615471363, 0.007789519615471363]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.007789519615471363

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00778952
Iteration 2/1000 | Loss: 0.00969824
Iteration 3/1000 | Loss: 0.00202515
Iteration 4/1000 | Loss: 0.00133333
Iteration 5/1000 | Loss: 0.00074877
Iteration 6/1000 | Loss: 0.00057040
Iteration 7/1000 | Loss: 0.00126023
Iteration 8/1000 | Loss: 0.00068264
Iteration 9/1000 | Loss: 0.00112115
Iteration 10/1000 | Loss: 0.00163255
Iteration 11/1000 | Loss: 0.00093006
Iteration 12/1000 | Loss: 0.00256160
Iteration 13/1000 | Loss: 0.00247327
Iteration 14/1000 | Loss: 0.00535670
Iteration 15/1000 | Loss: 0.00443004
Iteration 16/1000 | Loss: 0.00336270
Iteration 17/1000 | Loss: 0.00333113
Iteration 18/1000 | Loss: 0.00339530
Iteration 19/1000 | Loss: 0.00243726
Iteration 20/1000 | Loss: 0.00284992
Iteration 21/1000 | Loss: 0.00236238
Iteration 22/1000 | Loss: 0.00147107
Iteration 23/1000 | Loss: 0.00199049
Iteration 24/1000 | Loss: 0.00102040
Iteration 25/1000 | Loss: 0.00089298
Iteration 26/1000 | Loss: 0.00099539
Iteration 27/1000 | Loss: 0.00166588
Iteration 28/1000 | Loss: 0.00124940
Iteration 29/1000 | Loss: 0.00108519
Iteration 30/1000 | Loss: 0.00093906
Iteration 31/1000 | Loss: 0.00140156
Iteration 32/1000 | Loss: 0.00070043
Iteration 33/1000 | Loss: 0.00060827
Iteration 34/1000 | Loss: 0.00148113
Iteration 35/1000 | Loss: 0.00169691
Iteration 36/1000 | Loss: 0.00424559
Iteration 37/1000 | Loss: 0.00309420
Iteration 38/1000 | Loss: 0.00273706
Iteration 39/1000 | Loss: 0.00201726
Iteration 40/1000 | Loss: 0.00166196
Iteration 41/1000 | Loss: 0.00056178
Iteration 42/1000 | Loss: 0.00175087
Iteration 43/1000 | Loss: 0.00147341
Iteration 44/1000 | Loss: 0.00148709
Iteration 45/1000 | Loss: 0.00082463
Iteration 46/1000 | Loss: 0.00351625
Iteration 47/1000 | Loss: 0.01487356
Iteration 48/1000 | Loss: 0.01122028
Iteration 49/1000 | Loss: 0.01270218
Iteration 50/1000 | Loss: 0.00886349
Iteration 51/1000 | Loss: 0.01129680
Iteration 52/1000 | Loss: 0.00937219
Iteration 53/1000 | Loss: 0.01111474
Iteration 54/1000 | Loss: 0.00718517
Iteration 55/1000 | Loss: 0.01055539
Iteration 56/1000 | Loss: 0.00369984
Iteration 57/1000 | Loss: 0.00377607
Iteration 58/1000 | Loss: 0.00328881
Iteration 59/1000 | Loss: 0.00310571
Iteration 60/1000 | Loss: 0.00230898
Iteration 61/1000 | Loss: 0.00207543
Iteration 62/1000 | Loss: 0.00243423
Iteration 63/1000 | Loss: 0.00473142
Iteration 64/1000 | Loss: 0.00190727
Iteration 65/1000 | Loss: 0.00055958
Iteration 66/1000 | Loss: 0.00130620
Iteration 67/1000 | Loss: 0.00159134
Iteration 68/1000 | Loss: 0.00072964
Iteration 69/1000 | Loss: 0.00278185
Iteration 70/1000 | Loss: 0.00188881
Iteration 71/1000 | Loss: 0.00217296
Iteration 72/1000 | Loss: 0.00345699
Iteration 73/1000 | Loss: 0.00260391
Iteration 74/1000 | Loss: 0.00274473
Iteration 75/1000 | Loss: 0.00318143
Iteration 76/1000 | Loss: 0.00270984
Iteration 77/1000 | Loss: 0.00333290
Iteration 78/1000 | Loss: 0.00205609
Iteration 79/1000 | Loss: 0.00104905
Iteration 80/1000 | Loss: 0.00044493
Iteration 81/1000 | Loss: 0.00046375
Iteration 82/1000 | Loss: 0.00078083
Iteration 83/1000 | Loss: 0.00156491
Iteration 84/1000 | Loss: 0.00167273
Iteration 85/1000 | Loss: 0.00193053
Iteration 86/1000 | Loss: 0.00211440
Iteration 87/1000 | Loss: 0.00199932
Iteration 88/1000 | Loss: 0.00256031
Iteration 89/1000 | Loss: 0.00325559
Iteration 90/1000 | Loss: 0.00168725
Iteration 91/1000 | Loss: 0.00130006
Iteration 92/1000 | Loss: 0.00103749
Iteration 93/1000 | Loss: 0.00110225
Iteration 94/1000 | Loss: 0.00094203
Iteration 95/1000 | Loss: 0.00090054
Iteration 96/1000 | Loss: 0.00045439
Iteration 97/1000 | Loss: 0.00136836
Iteration 98/1000 | Loss: 0.00073272
Iteration 99/1000 | Loss: 0.00018699
Iteration 100/1000 | Loss: 0.00060620
Iteration 101/1000 | Loss: 0.00028307
Iteration 102/1000 | Loss: 0.00227072
Iteration 103/1000 | Loss: 0.00346531
Iteration 104/1000 | Loss: 0.00264526
Iteration 105/1000 | Loss: 0.00154371
Iteration 106/1000 | Loss: 0.00097625
Iteration 107/1000 | Loss: 0.00088489
Iteration 108/1000 | Loss: 0.00071796
Iteration 109/1000 | Loss: 0.00036454
Iteration 110/1000 | Loss: 0.00022070
Iteration 111/1000 | Loss: 0.00057519
Iteration 112/1000 | Loss: 0.00030227
Iteration 113/1000 | Loss: 0.00026103
Iteration 114/1000 | Loss: 0.00056486
Iteration 115/1000 | Loss: 0.00068637
Iteration 116/1000 | Loss: 0.00073924
Iteration 117/1000 | Loss: 0.00013417
Iteration 118/1000 | Loss: 0.00011399
Iteration 119/1000 | Loss: 0.00011343
Iteration 120/1000 | Loss: 0.00011039
Iteration 121/1000 | Loss: 0.00010879
Iteration 122/1000 | Loss: 0.00009897
Iteration 123/1000 | Loss: 0.00011085
Iteration 124/1000 | Loss: 0.00008900
Iteration 125/1000 | Loss: 0.00009237
Iteration 126/1000 | Loss: 0.00009394
Iteration 127/1000 | Loss: 0.00010542
Iteration 128/1000 | Loss: 0.00031544
Iteration 129/1000 | Loss: 0.00009405
Iteration 130/1000 | Loss: 0.00021559
Iteration 131/1000 | Loss: 0.00106703
Iteration 132/1000 | Loss: 0.00077487
Iteration 133/1000 | Loss: 0.00052736
Iteration 134/1000 | Loss: 0.00010323
Iteration 135/1000 | Loss: 0.00007745
Iteration 136/1000 | Loss: 0.00007970
Iteration 137/1000 | Loss: 0.00006730
Iteration 138/1000 | Loss: 0.00008252
Iteration 139/1000 | Loss: 0.00007092
Iteration 140/1000 | Loss: 0.00006296
Iteration 141/1000 | Loss: 0.00005938
Iteration 142/1000 | Loss: 0.00006239
Iteration 143/1000 | Loss: 0.00007272
Iteration 144/1000 | Loss: 0.00005915
Iteration 145/1000 | Loss: 0.00006524
Iteration 146/1000 | Loss: 0.00006240
Iteration 147/1000 | Loss: 0.00006909
Iteration 148/1000 | Loss: 0.00006610
Iteration 149/1000 | Loss: 0.00005279
Iteration 150/1000 | Loss: 0.00006744
Iteration 151/1000 | Loss: 0.00006168
Iteration 152/1000 | Loss: 0.00006770
Iteration 153/1000 | Loss: 0.00006526
Iteration 154/1000 | Loss: 0.00006499
Iteration 155/1000 | Loss: 0.00006215
Iteration 156/1000 | Loss: 0.00021636
Iteration 157/1000 | Loss: 0.00017846
Iteration 158/1000 | Loss: 0.00033157
Iteration 159/1000 | Loss: 0.00019580
Iteration 160/1000 | Loss: 0.00016899
Iteration 161/1000 | Loss: 0.00015913
Iteration 162/1000 | Loss: 0.00006130
Iteration 163/1000 | Loss: 0.00006337
Iteration 164/1000 | Loss: 0.00006209
Iteration 165/1000 | Loss: 0.00005909
Iteration 166/1000 | Loss: 0.00005189
Iteration 167/1000 | Loss: 0.00006197
Iteration 168/1000 | Loss: 0.00004997
Iteration 169/1000 | Loss: 0.00005862
Iteration 170/1000 | Loss: 0.00005841
Iteration 171/1000 | Loss: 0.00005187
Iteration 172/1000 | Loss: 0.00005975
Iteration 173/1000 | Loss: 0.00006151
Iteration 174/1000 | Loss: 0.00006065
Iteration 175/1000 | Loss: 0.00004815
Iteration 176/1000 | Loss: 0.00006263
Iteration 177/1000 | Loss: 0.00006660
Iteration 178/1000 | Loss: 0.00005362
Iteration 179/1000 | Loss: 0.00005692
Iteration 180/1000 | Loss: 0.00006099
Iteration 181/1000 | Loss: 0.00006008
Iteration 182/1000 | Loss: 0.00006837
Iteration 183/1000 | Loss: 0.00006601
Iteration 184/1000 | Loss: 0.00005682
Iteration 185/1000 | Loss: 0.00006145
Iteration 186/1000 | Loss: 0.00006158
Iteration 187/1000 | Loss: 0.00006616
Iteration 188/1000 | Loss: 0.00006391
Iteration 189/1000 | Loss: 0.00005780
Iteration 190/1000 | Loss: 0.00006768
Iteration 191/1000 | Loss: 0.00006663
Iteration 192/1000 | Loss: 0.00006667
Iteration 193/1000 | Loss: 0.00005882
Iteration 194/1000 | Loss: 0.00006260
Iteration 195/1000 | Loss: 0.00005798
Iteration 196/1000 | Loss: 0.00006096
Iteration 197/1000 | Loss: 0.00006202
Iteration 198/1000 | Loss: 0.00006235
Iteration 199/1000 | Loss: 0.00006070
Iteration 200/1000 | Loss: 0.00006065
Iteration 201/1000 | Loss: 0.00006084
Iteration 202/1000 | Loss: 0.00005927
Iteration 203/1000 | Loss: 0.00005712
Iteration 204/1000 | Loss: 0.00006172
Iteration 205/1000 | Loss: 0.00005190
Iteration 206/1000 | Loss: 0.00005773
Iteration 207/1000 | Loss: 0.00006587
Iteration 208/1000 | Loss: 0.00005670
Iteration 209/1000 | Loss: 0.00005537
Iteration 210/1000 | Loss: 0.00005751
Iteration 211/1000 | Loss: 0.00006127
Iteration 212/1000 | Loss: 0.00006484
Iteration 213/1000 | Loss: 0.00006251
Iteration 214/1000 | Loss: 0.00006117
Iteration 215/1000 | Loss: 0.00006386
Iteration 216/1000 | Loss: 0.00005768
Iteration 217/1000 | Loss: 0.00006331
Iteration 218/1000 | Loss: 0.00006994
Iteration 219/1000 | Loss: 0.00006710
Iteration 220/1000 | Loss: 0.00006921
Iteration 221/1000 | Loss: 0.00005652
Iteration 222/1000 | Loss: 0.00006041
Iteration 223/1000 | Loss: 0.00006057
Iteration 224/1000 | Loss: 0.00006366
Iteration 225/1000 | Loss: 0.00006050
Iteration 226/1000 | Loss: 0.00006409
Iteration 227/1000 | Loss: 0.00006887
Iteration 228/1000 | Loss: 0.00005782
Iteration 229/1000 | Loss: 0.00005472
Iteration 230/1000 | Loss: 0.00005016
Iteration 231/1000 | Loss: 0.00005809
Iteration 232/1000 | Loss: 0.00006659
Iteration 233/1000 | Loss: 0.00005810
Iteration 234/1000 | Loss: 0.00006310
Iteration 235/1000 | Loss: 0.00005612
Iteration 236/1000 | Loss: 0.00006402
Iteration 237/1000 | Loss: 0.00007333
Iteration 238/1000 | Loss: 0.00005984
Iteration 239/1000 | Loss: 0.00005659
Iteration 240/1000 | Loss: 0.00006777
Iteration 241/1000 | Loss: 0.00005456
Iteration 242/1000 | Loss: 0.00006108
Iteration 243/1000 | Loss: 0.00005713
Iteration 244/1000 | Loss: 0.00006204
Iteration 245/1000 | Loss: 0.00005104
Iteration 246/1000 | Loss: 0.00006605
Iteration 247/1000 | Loss: 0.00006101
Iteration 248/1000 | Loss: 0.00006309
Iteration 249/1000 | Loss: 0.00006526
Iteration 250/1000 | Loss: 0.00005366
Iteration 251/1000 | Loss: 0.00005647
Iteration 252/1000 | Loss: 0.00005842
Iteration 253/1000 | Loss: 0.00005327
Iteration 254/1000 | Loss: 0.00004934
Iteration 255/1000 | Loss: 0.00006689
Iteration 256/1000 | Loss: 0.00005856
Iteration 257/1000 | Loss: 0.00005819
Iteration 258/1000 | Loss: 0.00008164
Iteration 259/1000 | Loss: 0.00005262
Iteration 260/1000 | Loss: 0.00006724
Iteration 261/1000 | Loss: 0.00005836
Iteration 262/1000 | Loss: 0.00005698
Iteration 263/1000 | Loss: 0.00007515
Iteration 264/1000 | Loss: 0.00006151
Iteration 265/1000 | Loss: 0.00005752
Iteration 266/1000 | Loss: 0.00005851
Iteration 267/1000 | Loss: 0.00005799
Iteration 268/1000 | Loss: 0.00005684
Iteration 269/1000 | Loss: 0.00007413
Iteration 270/1000 | Loss: 0.00006119
Iteration 271/1000 | Loss: 0.00007473
Iteration 272/1000 | Loss: 0.00005982
Iteration 273/1000 | Loss: 0.00007682
Iteration 274/1000 | Loss: 0.00005591
Iteration 275/1000 | Loss: 0.00006446
Iteration 276/1000 | Loss: 0.00006622
Iteration 277/1000 | Loss: 0.00005438
Iteration 278/1000 | Loss: 0.00005789
Iteration 279/1000 | Loss: 0.00004842
Iteration 280/1000 | Loss: 0.00004983
Iteration 281/1000 | Loss: 0.00005242
Iteration 282/1000 | Loss: 0.00005167
Iteration 283/1000 | Loss: 0.00005236
Iteration 284/1000 | Loss: 0.00004807
Iteration 285/1000 | Loss: 0.00005140
Iteration 286/1000 | Loss: 0.00005684
Iteration 287/1000 | Loss: 0.00005727
Iteration 288/1000 | Loss: 0.00005622
Iteration 289/1000 | Loss: 0.00005190
Iteration 290/1000 | Loss: 0.00005497
Iteration 291/1000 | Loss: 0.00005191
Iteration 292/1000 | Loss: 0.00005552
Iteration 293/1000 | Loss: 0.00005161
Iteration 294/1000 | Loss: 0.00005669
Iteration 295/1000 | Loss: 0.00005640
Iteration 296/1000 | Loss: 0.00005781
Iteration 297/1000 | Loss: 0.00004999
Iteration 298/1000 | Loss: 0.00005101
Iteration 299/1000 | Loss: 0.00005857
Iteration 300/1000 | Loss: 0.00005626
Iteration 301/1000 | Loss: 0.00005657
Iteration 302/1000 | Loss: 0.00005252
Iteration 303/1000 | Loss: 0.00005882
Iteration 304/1000 | Loss: 0.00005328
Iteration 305/1000 | Loss: 0.00005709
Iteration 306/1000 | Loss: 0.00004782
Iteration 307/1000 | Loss: 0.00005150
Iteration 308/1000 | Loss: 0.00005802
Iteration 309/1000 | Loss: 0.00005545
Iteration 310/1000 | Loss: 0.00005772
Iteration 311/1000 | Loss: 0.00004490
Iteration 312/1000 | Loss: 0.00005717
Iteration 313/1000 | Loss: 0.00005576
Iteration 314/1000 | Loss: 0.00005540
Iteration 315/1000 | Loss: 0.00006256
Iteration 316/1000 | Loss: 0.00005539
Iteration 317/1000 | Loss: 0.00005539
Iteration 318/1000 | Loss: 0.00005539
Iteration 319/1000 | Loss: 0.00006270
Iteration 320/1000 | Loss: 0.00005241
Iteration 321/1000 | Loss: 0.00004799
Iteration 322/1000 | Loss: 0.00004629
Iteration 323/1000 | Loss: 0.00004570
Iteration 324/1000 | Loss: 0.00004512
Iteration 325/1000 | Loss: 0.00004462
Iteration 326/1000 | Loss: 0.00004432
Iteration 327/1000 | Loss: 0.00004409
Iteration 328/1000 | Loss: 0.00004390
Iteration 329/1000 | Loss: 0.00004376
Iteration 330/1000 | Loss: 0.00004374
Iteration 331/1000 | Loss: 0.00004374
Iteration 332/1000 | Loss: 0.00004373
Iteration 333/1000 | Loss: 0.00004372
Iteration 334/1000 | Loss: 0.00004370
Iteration 335/1000 | Loss: 0.00004368
Iteration 336/1000 | Loss: 0.00004368
Iteration 337/1000 | Loss: 0.00004367
Iteration 338/1000 | Loss: 0.00004365
Iteration 339/1000 | Loss: 0.00004364
Iteration 340/1000 | Loss: 0.00004352
Iteration 341/1000 | Loss: 0.00004351
Iteration 342/1000 | Loss: 0.00004351
Iteration 343/1000 | Loss: 0.00004350
Iteration 344/1000 | Loss: 0.00004350
Iteration 345/1000 | Loss: 0.00004347
Iteration 346/1000 | Loss: 0.00004341
Iteration 347/1000 | Loss: 0.00004341
Iteration 348/1000 | Loss: 0.00004340
Iteration 349/1000 | Loss: 0.00004339
Iteration 350/1000 | Loss: 0.00004338
Iteration 351/1000 | Loss: 0.00004335
Iteration 352/1000 | Loss: 0.00004333
Iteration 353/1000 | Loss: 0.00004333
Iteration 354/1000 | Loss: 0.00004332
Iteration 355/1000 | Loss: 0.00004332
Iteration 356/1000 | Loss: 0.00004332
Iteration 357/1000 | Loss: 0.00004331
Iteration 358/1000 | Loss: 0.00021305
Iteration 359/1000 | Loss: 0.00015458
Iteration 360/1000 | Loss: 0.00004568
Iteration 361/1000 | Loss: 0.00004422
Iteration 362/1000 | Loss: 0.00004351
Iteration 363/1000 | Loss: 0.00004332
Iteration 364/1000 | Loss: 0.00004331
Iteration 365/1000 | Loss: 0.00004331
Iteration 366/1000 | Loss: 0.00004331
Iteration 367/1000 | Loss: 0.00004331
Iteration 368/1000 | Loss: 0.00004331
Iteration 369/1000 | Loss: 0.00004330
Iteration 370/1000 | Loss: 0.00004330
Iteration 371/1000 | Loss: 0.00004330
Iteration 372/1000 | Loss: 0.00004330
Iteration 373/1000 | Loss: 0.00004330
Iteration 374/1000 | Loss: 0.00004330
Iteration 375/1000 | Loss: 0.00004329
Iteration 376/1000 | Loss: 0.00004329
Iteration 377/1000 | Loss: 0.00004329
Iteration 378/1000 | Loss: 0.00004329
Iteration 379/1000 | Loss: 0.00004329
Iteration 380/1000 | Loss: 0.00004328
Iteration 381/1000 | Loss: 0.00004328
Iteration 382/1000 | Loss: 0.00004328
Iteration 383/1000 | Loss: 0.00004328
Iteration 384/1000 | Loss: 0.00004328
Iteration 385/1000 | Loss: 0.00004328
Iteration 386/1000 | Loss: 0.00004328
Iteration 387/1000 | Loss: 0.00004327
Iteration 388/1000 | Loss: 0.00004327
Iteration 389/1000 | Loss: 0.00004327
Iteration 390/1000 | Loss: 0.00004327
Iteration 391/1000 | Loss: 0.00004327
Iteration 392/1000 | Loss: 0.00004327
Iteration 393/1000 | Loss: 0.00004327
Iteration 394/1000 | Loss: 0.00004327
Iteration 395/1000 | Loss: 0.00004327
Iteration 396/1000 | Loss: 0.00004327
Iteration 397/1000 | Loss: 0.00004326
Iteration 398/1000 | Loss: 0.00004326
Iteration 399/1000 | Loss: 0.00004326
Iteration 400/1000 | Loss: 0.00004326
Iteration 401/1000 | Loss: 0.00004326
Iteration 402/1000 | Loss: 0.00004326
Iteration 403/1000 | Loss: 0.00004326
Iteration 404/1000 | Loss: 0.00004325
Iteration 405/1000 | Loss: 0.00004325
Iteration 406/1000 | Loss: 0.00004325
Iteration 407/1000 | Loss: 0.00004325
Iteration 408/1000 | Loss: 0.00004325
Iteration 409/1000 | Loss: 0.00004325
Iteration 410/1000 | Loss: 0.00004324
Iteration 411/1000 | Loss: 0.00004324
Iteration 412/1000 | Loss: 0.00004324
Iteration 413/1000 | Loss: 0.00004324
Iteration 414/1000 | Loss: 0.00004324
Iteration 415/1000 | Loss: 0.00004324
Iteration 416/1000 | Loss: 0.00004324
Iteration 417/1000 | Loss: 0.00004324
Iteration 418/1000 | Loss: 0.00004324
Iteration 419/1000 | Loss: 0.00004324
Iteration 420/1000 | Loss: 0.00004324
Iteration 421/1000 | Loss: 0.00004324
Iteration 422/1000 | Loss: 0.00004324
Iteration 423/1000 | Loss: 0.00004324
Iteration 424/1000 | Loss: 0.00004324
Iteration 425/1000 | Loss: 0.00004324
Iteration 426/1000 | Loss: 0.00004324
Iteration 427/1000 | Loss: 0.00004324
Iteration 428/1000 | Loss: 0.00004324
Iteration 429/1000 | Loss: 0.00004324
Iteration 430/1000 | Loss: 0.00004324
Iteration 431/1000 | Loss: 0.00004324
Iteration 432/1000 | Loss: 0.00004324
Iteration 433/1000 | Loss: 0.00004324
Iteration 434/1000 | Loss: 0.00004324
Iteration 435/1000 | Loss: 0.00004324
Iteration 436/1000 | Loss: 0.00004324
Iteration 437/1000 | Loss: 0.00004324
Iteration 438/1000 | Loss: 0.00004324
Iteration 439/1000 | Loss: 0.00004324
Iteration 440/1000 | Loss: 0.00004324
Iteration 441/1000 | Loss: 0.00004324
Iteration 442/1000 | Loss: 0.00004324
Iteration 443/1000 | Loss: 0.00004324
Iteration 444/1000 | Loss: 0.00004324
Iteration 445/1000 | Loss: 0.00004324
Iteration 446/1000 | Loss: 0.00004324
Iteration 447/1000 | Loss: 0.00004324
Iteration 448/1000 | Loss: 0.00004324
Iteration 449/1000 | Loss: 0.00004324
Iteration 450/1000 | Loss: 0.00004324
Iteration 451/1000 | Loss: 0.00004324
Iteration 452/1000 | Loss: 0.00004324
Iteration 453/1000 | Loss: 0.00004324
Iteration 454/1000 | Loss: 0.00004324
Iteration 455/1000 | Loss: 0.00004324
Iteration 456/1000 | Loss: 0.00004324
Iteration 457/1000 | Loss: 0.00004324
Iteration 458/1000 | Loss: 0.00004324
Iteration 459/1000 | Loss: 0.00004324
Iteration 460/1000 | Loss: 0.00004324
Iteration 461/1000 | Loss: 0.00004324
Iteration 462/1000 | Loss: 0.00004324
Iteration 463/1000 | Loss: 0.00004324
Iteration 464/1000 | Loss: 0.00004324
Iteration 465/1000 | Loss: 0.00004324
Iteration 466/1000 | Loss: 0.00004324
Iteration 467/1000 | Loss: 0.00004324
Iteration 468/1000 | Loss: 0.00004324
Iteration 469/1000 | Loss: 0.00004324
Iteration 470/1000 | Loss: 0.00004324
Iteration 471/1000 | Loss: 0.00004324
Iteration 472/1000 | Loss: 0.00004324
Iteration 473/1000 | Loss: 0.00004324
Iteration 474/1000 | Loss: 0.00004324
Iteration 475/1000 | Loss: 0.00004324
Iteration 476/1000 | Loss: 0.00004324
Iteration 477/1000 | Loss: 0.00004324
Iteration 478/1000 | Loss: 0.00004324
Iteration 479/1000 | Loss: 0.00004324
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 479. Stopping optimization.
Last 5 losses: [4.323548637330532e-05, 4.323548637330532e-05, 4.323548637330532e-05, 4.323548637330532e-05, 4.323548637330532e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.323548637330532e-05

Optimization complete. Final v2v error: 5.4120259284973145 mm

Highest mean error: 8.27837085723877 mm for frame 111

Lowest mean error: 4.273039817810059 mm for frame 0

Saving results

Total time: 604.0852127075195
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00729321
Iteration 2/25 | Loss: 0.00150644
Iteration 3/25 | Loss: 0.00078136
Iteration 4/25 | Loss: 0.00077362
Iteration 5/25 | Loss: 0.00062460
Iteration 6/25 | Loss: 0.00061276
Iteration 7/25 | Loss: 0.00060762
Iteration 8/25 | Loss: 0.00060598
Iteration 9/25 | Loss: 0.00059413
Iteration 10/25 | Loss: 0.00059976
Iteration 11/25 | Loss: 0.00058518
Iteration 12/25 | Loss: 0.00058301
Iteration 13/25 | Loss: 0.00057652
Iteration 14/25 | Loss: 0.00057620
Iteration 15/25 | Loss: 0.00057611
Iteration 16/25 | Loss: 0.00057609
Iteration 17/25 | Loss: 0.00057609
Iteration 18/25 | Loss: 0.00057609
Iteration 19/25 | Loss: 0.00057609
Iteration 20/25 | Loss: 0.00057609
Iteration 21/25 | Loss: 0.00057608
Iteration 22/25 | Loss: 0.00057608
Iteration 23/25 | Loss: 0.00057608
Iteration 24/25 | Loss: 0.00057608
Iteration 25/25 | Loss: 0.00057608

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.82375908
Iteration 2/25 | Loss: 0.00015716
Iteration 3/25 | Loss: 0.00015716
Iteration 4/25 | Loss: 0.00015716
Iteration 5/25 | Loss: 0.00015716
Iteration 6/25 | Loss: 0.00015716
Iteration 7/25 | Loss: 0.00015716
Iteration 8/25 | Loss: 0.00015716
Iteration 9/25 | Loss: 0.00015716
Iteration 10/25 | Loss: 0.00015716
Iteration 11/25 | Loss: 0.00015716
Iteration 12/25 | Loss: 0.00015716
Iteration 13/25 | Loss: 0.00015716
Iteration 14/25 | Loss: 0.00015716
Iteration 15/25 | Loss: 0.00015716
Iteration 16/25 | Loss: 0.00015716
Iteration 17/25 | Loss: 0.00015716
Iteration 18/25 | Loss: 0.00015716
Iteration 19/25 | Loss: 0.00015716
Iteration 20/25 | Loss: 0.00015716
Iteration 21/25 | Loss: 0.00015716
Iteration 22/25 | Loss: 0.00015716
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0001571553439134732, 0.0001571553439134732, 0.0001571553439134732, 0.0001571553439134732, 0.0001571553439134732]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0001571553439134732

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00015716
Iteration 2/1000 | Loss: 0.00003934
Iteration 3/1000 | Loss: 0.00001695
Iteration 4/1000 | Loss: 0.00001604
Iteration 5/1000 | Loss: 0.00002961
Iteration 6/1000 | Loss: 0.00002959
Iteration 7/1000 | Loss: 0.00002133
Iteration 8/1000 | Loss: 0.00003676
Iteration 9/1000 | Loss: 0.00001770
Iteration 10/1000 | Loss: 0.00001450
Iteration 11/1000 | Loss: 0.00001428
Iteration 12/1000 | Loss: 0.00001411
Iteration 13/1000 | Loss: 0.00001409
Iteration 14/1000 | Loss: 0.00003047
Iteration 15/1000 | Loss: 0.00001561
Iteration 16/1000 | Loss: 0.00001663
Iteration 17/1000 | Loss: 0.00001397
Iteration 18/1000 | Loss: 0.00001386
Iteration 19/1000 | Loss: 0.00001385
Iteration 20/1000 | Loss: 0.00001383
Iteration 21/1000 | Loss: 0.00001371
Iteration 22/1000 | Loss: 0.00001370
Iteration 23/1000 | Loss: 0.00001369
Iteration 24/1000 | Loss: 0.00005056
Iteration 25/1000 | Loss: 0.00003451
Iteration 26/1000 | Loss: 0.00001367
Iteration 27/1000 | Loss: 0.00001365
Iteration 28/1000 | Loss: 0.00001363
Iteration 29/1000 | Loss: 0.00001363
Iteration 30/1000 | Loss: 0.00001363
Iteration 31/1000 | Loss: 0.00001362
Iteration 32/1000 | Loss: 0.00001362
Iteration 33/1000 | Loss: 0.00001362
Iteration 34/1000 | Loss: 0.00001362
Iteration 35/1000 | Loss: 0.00001362
Iteration 36/1000 | Loss: 0.00001361
Iteration 37/1000 | Loss: 0.00001361
Iteration 38/1000 | Loss: 0.00001361
Iteration 39/1000 | Loss: 0.00001361
Iteration 40/1000 | Loss: 0.00001361
Iteration 41/1000 | Loss: 0.00002249
Iteration 42/1000 | Loss: 0.00001504
Iteration 43/1000 | Loss: 0.00001360
Iteration 44/1000 | Loss: 0.00001359
Iteration 45/1000 | Loss: 0.00001359
Iteration 46/1000 | Loss: 0.00001359
Iteration 47/1000 | Loss: 0.00001359
Iteration 48/1000 | Loss: 0.00001359
Iteration 49/1000 | Loss: 0.00001359
Iteration 50/1000 | Loss: 0.00001359
Iteration 51/1000 | Loss: 0.00001359
Iteration 52/1000 | Loss: 0.00001359
Iteration 53/1000 | Loss: 0.00001359
Iteration 54/1000 | Loss: 0.00001358
Iteration 55/1000 | Loss: 0.00001358
Iteration 56/1000 | Loss: 0.00001358
Iteration 57/1000 | Loss: 0.00001357
Iteration 58/1000 | Loss: 0.00001796
Iteration 59/1000 | Loss: 0.00003622
Iteration 60/1000 | Loss: 0.00001364
Iteration 61/1000 | Loss: 0.00001352
Iteration 62/1000 | Loss: 0.00001352
Iteration 63/1000 | Loss: 0.00001351
Iteration 64/1000 | Loss: 0.00001350
Iteration 65/1000 | Loss: 0.00001350
Iteration 66/1000 | Loss: 0.00001350
Iteration 67/1000 | Loss: 0.00001349
Iteration 68/1000 | Loss: 0.00001349
Iteration 69/1000 | Loss: 0.00001349
Iteration 70/1000 | Loss: 0.00001349
Iteration 71/1000 | Loss: 0.00001349
Iteration 72/1000 | Loss: 0.00001349
Iteration 73/1000 | Loss: 0.00001349
Iteration 74/1000 | Loss: 0.00001348
Iteration 75/1000 | Loss: 0.00001348
Iteration 76/1000 | Loss: 0.00001348
Iteration 77/1000 | Loss: 0.00001348
Iteration 78/1000 | Loss: 0.00001348
Iteration 79/1000 | Loss: 0.00001348
Iteration 80/1000 | Loss: 0.00001348
Iteration 81/1000 | Loss: 0.00001348
Iteration 82/1000 | Loss: 0.00001347
Iteration 83/1000 | Loss: 0.00001347
Iteration 84/1000 | Loss: 0.00001347
Iteration 85/1000 | Loss: 0.00001347
Iteration 86/1000 | Loss: 0.00001347
Iteration 87/1000 | Loss: 0.00001347
Iteration 88/1000 | Loss: 0.00001347
Iteration 89/1000 | Loss: 0.00001347
Iteration 90/1000 | Loss: 0.00001347
Iteration 91/1000 | Loss: 0.00001347
Iteration 92/1000 | Loss: 0.00001347
Iteration 93/1000 | Loss: 0.00001346
Iteration 94/1000 | Loss: 0.00001346
Iteration 95/1000 | Loss: 0.00001346
Iteration 96/1000 | Loss: 0.00001346
Iteration 97/1000 | Loss: 0.00001346
Iteration 98/1000 | Loss: 0.00001346
Iteration 99/1000 | Loss: 0.00001346
Iteration 100/1000 | Loss: 0.00001346
Iteration 101/1000 | Loss: 0.00001345
Iteration 102/1000 | Loss: 0.00001345
Iteration 103/1000 | Loss: 0.00001345
Iteration 104/1000 | Loss: 0.00001344
Iteration 105/1000 | Loss: 0.00001344
Iteration 106/1000 | Loss: 0.00001344
Iteration 107/1000 | Loss: 0.00001344
Iteration 108/1000 | Loss: 0.00001343
Iteration 109/1000 | Loss: 0.00001343
Iteration 110/1000 | Loss: 0.00001343
Iteration 111/1000 | Loss: 0.00001343
Iteration 112/1000 | Loss: 0.00001343
Iteration 113/1000 | Loss: 0.00001343
Iteration 114/1000 | Loss: 0.00001342
Iteration 115/1000 | Loss: 0.00001342
Iteration 116/1000 | Loss: 0.00001342
Iteration 117/1000 | Loss: 0.00001342
Iteration 118/1000 | Loss: 0.00001341
Iteration 119/1000 | Loss: 0.00001341
Iteration 120/1000 | Loss: 0.00001341
Iteration 121/1000 | Loss: 0.00001340
Iteration 122/1000 | Loss: 0.00001340
Iteration 123/1000 | Loss: 0.00001340
Iteration 124/1000 | Loss: 0.00001340
Iteration 125/1000 | Loss: 0.00001340
Iteration 126/1000 | Loss: 0.00001340
Iteration 127/1000 | Loss: 0.00001340
Iteration 128/1000 | Loss: 0.00001340
Iteration 129/1000 | Loss: 0.00001340
Iteration 130/1000 | Loss: 0.00001340
Iteration 131/1000 | Loss: 0.00001340
Iteration 132/1000 | Loss: 0.00001340
Iteration 133/1000 | Loss: 0.00001340
Iteration 134/1000 | Loss: 0.00001340
Iteration 135/1000 | Loss: 0.00001340
Iteration 136/1000 | Loss: 0.00001339
Iteration 137/1000 | Loss: 0.00001339
Iteration 138/1000 | Loss: 0.00001339
Iteration 139/1000 | Loss: 0.00001339
Iteration 140/1000 | Loss: 0.00001339
Iteration 141/1000 | Loss: 0.00001339
Iteration 142/1000 | Loss: 0.00001339
Iteration 143/1000 | Loss: 0.00001339
Iteration 144/1000 | Loss: 0.00001339
Iteration 145/1000 | Loss: 0.00001339
Iteration 146/1000 | Loss: 0.00001339
Iteration 147/1000 | Loss: 0.00001339
Iteration 148/1000 | Loss: 0.00001339
Iteration 149/1000 | Loss: 0.00001339
Iteration 150/1000 | Loss: 0.00001339
Iteration 151/1000 | Loss: 0.00001339
Iteration 152/1000 | Loss: 0.00001339
Iteration 153/1000 | Loss: 0.00001339
Iteration 154/1000 | Loss: 0.00001339
Iteration 155/1000 | Loss: 0.00001339
Iteration 156/1000 | Loss: 0.00001339
Iteration 157/1000 | Loss: 0.00001339
Iteration 158/1000 | Loss: 0.00001339
Iteration 159/1000 | Loss: 0.00001339
Iteration 160/1000 | Loss: 0.00001339
Iteration 161/1000 | Loss: 0.00001339
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.3393347217061091e-05, 1.3393347217061091e-05, 1.3393347217061091e-05, 1.3393347217061091e-05, 1.3393347217061091e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3393347217061091e-05

Optimization complete. Final v2v error: 3.0693776607513428 mm

Highest mean error: 3.7614872455596924 mm for frame 49

Lowest mean error: 2.8068764209747314 mm for frame 225

Saving results

Total time: 77.71582102775574
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00838970
Iteration 2/25 | Loss: 0.00147106
Iteration 3/25 | Loss: 0.00098070
Iteration 4/25 | Loss: 0.00086224
Iteration 5/25 | Loss: 0.00081414
Iteration 6/25 | Loss: 0.00080584
Iteration 7/25 | Loss: 0.00080553
Iteration 8/25 | Loss: 0.00079281
Iteration 9/25 | Loss: 0.00078825
Iteration 10/25 | Loss: 0.00078240
Iteration 11/25 | Loss: 0.00078622
Iteration 12/25 | Loss: 0.00078529
Iteration 13/25 | Loss: 0.00078117
Iteration 14/25 | Loss: 0.00077651
Iteration 15/25 | Loss: 0.00077286
Iteration 16/25 | Loss: 0.00077414
Iteration 17/25 | Loss: 0.00077131
Iteration 18/25 | Loss: 0.00077274
Iteration 19/25 | Loss: 0.00077306
Iteration 20/25 | Loss: 0.00077086
Iteration 21/25 | Loss: 0.00077454
Iteration 22/25 | Loss: 0.00077033
Iteration 23/25 | Loss: 0.00076682
Iteration 24/25 | Loss: 0.00076550
Iteration 25/25 | Loss: 0.00076332

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.59465027
Iteration 2/25 | Loss: 0.00098538
Iteration 3/25 | Loss: 0.00098523
Iteration 4/25 | Loss: 0.00098523
Iteration 5/25 | Loss: 0.00098523
Iteration 6/25 | Loss: 0.00098523
Iteration 7/25 | Loss: 0.00098522
Iteration 8/25 | Loss: 0.00098522
Iteration 9/25 | Loss: 0.00098522
Iteration 10/25 | Loss: 0.00098522
Iteration 11/25 | Loss: 0.00098522
Iteration 12/25 | Loss: 0.00098522
Iteration 13/25 | Loss: 0.00098522
Iteration 14/25 | Loss: 0.00098522
Iteration 15/25 | Loss: 0.00098522
Iteration 16/25 | Loss: 0.00098522
Iteration 17/25 | Loss: 0.00098522
Iteration 18/25 | Loss: 0.00098522
Iteration 19/25 | Loss: 0.00098522
Iteration 20/25 | Loss: 0.00098522
Iteration 21/25 | Loss: 0.00098522
Iteration 22/25 | Loss: 0.00098522
Iteration 23/25 | Loss: 0.00098522
Iteration 24/25 | Loss: 0.00098522
Iteration 25/25 | Loss: 0.00098522

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098522
Iteration 2/1000 | Loss: 0.00017445
Iteration 3/1000 | Loss: 0.00043605
Iteration 4/1000 | Loss: 0.00185274
Iteration 5/1000 | Loss: 0.00046977
Iteration 6/1000 | Loss: 0.00045674
Iteration 7/1000 | Loss: 0.00010700
Iteration 8/1000 | Loss: 0.00008552
Iteration 9/1000 | Loss: 0.00102634
Iteration 10/1000 | Loss: 0.00071910
Iteration 11/1000 | Loss: 0.00007401
Iteration 12/1000 | Loss: 0.00006636
Iteration 13/1000 | Loss: 0.00006120
Iteration 14/1000 | Loss: 0.00005703
Iteration 15/1000 | Loss: 0.00119587
Iteration 16/1000 | Loss: 0.00096652
Iteration 17/1000 | Loss: 0.00007986
Iteration 18/1000 | Loss: 0.00005439
Iteration 19/1000 | Loss: 0.00355676
Iteration 20/1000 | Loss: 0.00010770
Iteration 21/1000 | Loss: 0.00005576
Iteration 22/1000 | Loss: 0.00004672
Iteration 23/1000 | Loss: 0.00004203
Iteration 24/1000 | Loss: 0.00003879
Iteration 25/1000 | Loss: 0.00003652
Iteration 26/1000 | Loss: 0.00003534
Iteration 27/1000 | Loss: 0.00003441
Iteration 28/1000 | Loss: 0.00003373
Iteration 29/1000 | Loss: 0.00003327
Iteration 30/1000 | Loss: 0.00003272
Iteration 31/1000 | Loss: 0.00003226
Iteration 32/1000 | Loss: 0.00003182
Iteration 33/1000 | Loss: 0.00003152
Iteration 34/1000 | Loss: 0.00003128
Iteration 35/1000 | Loss: 0.00003105
Iteration 36/1000 | Loss: 0.00003087
Iteration 37/1000 | Loss: 0.00003069
Iteration 38/1000 | Loss: 0.00003057
Iteration 39/1000 | Loss: 0.00003057
Iteration 40/1000 | Loss: 0.00003053
Iteration 41/1000 | Loss: 0.00003048
Iteration 42/1000 | Loss: 0.00003035
Iteration 43/1000 | Loss: 0.00003035
Iteration 44/1000 | Loss: 0.00003034
Iteration 45/1000 | Loss: 0.00003034
Iteration 46/1000 | Loss: 0.00003033
Iteration 47/1000 | Loss: 0.00003033
Iteration 48/1000 | Loss: 0.00003033
Iteration 49/1000 | Loss: 0.00003032
Iteration 50/1000 | Loss: 0.00003028
Iteration 51/1000 | Loss: 0.00003025
Iteration 52/1000 | Loss: 0.00003024
Iteration 53/1000 | Loss: 0.00003023
Iteration 54/1000 | Loss: 0.00003023
Iteration 55/1000 | Loss: 0.00003022
Iteration 56/1000 | Loss: 0.00003022
Iteration 57/1000 | Loss: 0.00003021
Iteration 58/1000 | Loss: 0.00003021
Iteration 59/1000 | Loss: 0.00003020
Iteration 60/1000 | Loss: 0.00003020
Iteration 61/1000 | Loss: 0.00003020
Iteration 62/1000 | Loss: 0.00003020
Iteration 63/1000 | Loss: 0.00003020
Iteration 64/1000 | Loss: 0.00003020
Iteration 65/1000 | Loss: 0.00003020
Iteration 66/1000 | Loss: 0.00003020
Iteration 67/1000 | Loss: 0.00003020
Iteration 68/1000 | Loss: 0.00003019
Iteration 69/1000 | Loss: 0.00003018
Iteration 70/1000 | Loss: 0.00003018
Iteration 71/1000 | Loss: 0.00003017
Iteration 72/1000 | Loss: 0.00003017
Iteration 73/1000 | Loss: 0.00003017
Iteration 74/1000 | Loss: 0.00003017
Iteration 75/1000 | Loss: 0.00003017
Iteration 76/1000 | Loss: 0.00003016
Iteration 77/1000 | Loss: 0.00003016
Iteration 78/1000 | Loss: 0.00003016
Iteration 79/1000 | Loss: 0.00003015
Iteration 80/1000 | Loss: 0.00003015
Iteration 81/1000 | Loss: 0.00003015
Iteration 82/1000 | Loss: 0.00003015
Iteration 83/1000 | Loss: 0.00003015
Iteration 84/1000 | Loss: 0.00003015
Iteration 85/1000 | Loss: 0.00003015
Iteration 86/1000 | Loss: 0.00003015
Iteration 87/1000 | Loss: 0.00003015
Iteration 88/1000 | Loss: 0.00003015
Iteration 89/1000 | Loss: 0.00003015
Iteration 90/1000 | Loss: 0.00003014
Iteration 91/1000 | Loss: 0.00003014
Iteration 92/1000 | Loss: 0.00003014
Iteration 93/1000 | Loss: 0.00003013
Iteration 94/1000 | Loss: 0.00003013
Iteration 95/1000 | Loss: 0.00003013
Iteration 96/1000 | Loss: 0.00003013
Iteration 97/1000 | Loss: 0.00003012
Iteration 98/1000 | Loss: 0.00003012
Iteration 99/1000 | Loss: 0.00003012
Iteration 100/1000 | Loss: 0.00003011
Iteration 101/1000 | Loss: 0.00003011
Iteration 102/1000 | Loss: 0.00003011
Iteration 103/1000 | Loss: 0.00003011
Iteration 104/1000 | Loss: 0.00003011
Iteration 105/1000 | Loss: 0.00003011
Iteration 106/1000 | Loss: 0.00003010
Iteration 107/1000 | Loss: 0.00003010
Iteration 108/1000 | Loss: 0.00003010
Iteration 109/1000 | Loss: 0.00003010
Iteration 110/1000 | Loss: 0.00003009
Iteration 111/1000 | Loss: 0.00003009
Iteration 112/1000 | Loss: 0.00003009
Iteration 113/1000 | Loss: 0.00003009
Iteration 114/1000 | Loss: 0.00003009
Iteration 115/1000 | Loss: 0.00003008
Iteration 116/1000 | Loss: 0.00003008
Iteration 117/1000 | Loss: 0.00003008
Iteration 118/1000 | Loss: 0.00003008
Iteration 119/1000 | Loss: 0.00003007
Iteration 120/1000 | Loss: 0.00003007
Iteration 121/1000 | Loss: 0.00003007
Iteration 122/1000 | Loss: 0.00003007
Iteration 123/1000 | Loss: 0.00003007
Iteration 124/1000 | Loss: 0.00003006
Iteration 125/1000 | Loss: 0.00003006
Iteration 126/1000 | Loss: 0.00003006
Iteration 127/1000 | Loss: 0.00003006
Iteration 128/1000 | Loss: 0.00003006
Iteration 129/1000 | Loss: 0.00003005
Iteration 130/1000 | Loss: 0.00003005
Iteration 131/1000 | Loss: 0.00003005
Iteration 132/1000 | Loss: 0.00003005
Iteration 133/1000 | Loss: 0.00003005
Iteration 134/1000 | Loss: 0.00003004
Iteration 135/1000 | Loss: 0.00003004
Iteration 136/1000 | Loss: 0.00003004
Iteration 137/1000 | Loss: 0.00003004
Iteration 138/1000 | Loss: 0.00003004
Iteration 139/1000 | Loss: 0.00003003
Iteration 140/1000 | Loss: 0.00003003
Iteration 141/1000 | Loss: 0.00003003
Iteration 142/1000 | Loss: 0.00003003
Iteration 143/1000 | Loss: 0.00003003
Iteration 144/1000 | Loss: 0.00003003
Iteration 145/1000 | Loss: 0.00003003
Iteration 146/1000 | Loss: 0.00003003
Iteration 147/1000 | Loss: 0.00003003
Iteration 148/1000 | Loss: 0.00003003
Iteration 149/1000 | Loss: 0.00003003
Iteration 150/1000 | Loss: 0.00003003
Iteration 151/1000 | Loss: 0.00003003
Iteration 152/1000 | Loss: 0.00003003
Iteration 153/1000 | Loss: 0.00003003
Iteration 154/1000 | Loss: 0.00003002
Iteration 155/1000 | Loss: 0.00003002
Iteration 156/1000 | Loss: 0.00003002
Iteration 157/1000 | Loss: 0.00003002
Iteration 158/1000 | Loss: 0.00003002
Iteration 159/1000 | Loss: 0.00003001
Iteration 160/1000 | Loss: 0.00003001
Iteration 161/1000 | Loss: 0.00003001
Iteration 162/1000 | Loss: 0.00003001
Iteration 163/1000 | Loss: 0.00003001
Iteration 164/1000 | Loss: 0.00003001
Iteration 165/1000 | Loss: 0.00003001
Iteration 166/1000 | Loss: 0.00003001
Iteration 167/1000 | Loss: 0.00003001
Iteration 168/1000 | Loss: 0.00003001
Iteration 169/1000 | Loss: 0.00003001
Iteration 170/1000 | Loss: 0.00003000
Iteration 171/1000 | Loss: 0.00003000
Iteration 172/1000 | Loss: 0.00003000
Iteration 173/1000 | Loss: 0.00003000
Iteration 174/1000 | Loss: 0.00003000
Iteration 175/1000 | Loss: 0.00003000
Iteration 176/1000 | Loss: 0.00003000
Iteration 177/1000 | Loss: 0.00003000
Iteration 178/1000 | Loss: 0.00003000
Iteration 179/1000 | Loss: 0.00003000
Iteration 180/1000 | Loss: 0.00003000
Iteration 181/1000 | Loss: 0.00003000
Iteration 182/1000 | Loss: 0.00003000
Iteration 183/1000 | Loss: 0.00003000
Iteration 184/1000 | Loss: 0.00003000
Iteration 185/1000 | Loss: 0.00003000
Iteration 186/1000 | Loss: 0.00003000
Iteration 187/1000 | Loss: 0.00003000
Iteration 188/1000 | Loss: 0.00003000
Iteration 189/1000 | Loss: 0.00003000
Iteration 190/1000 | Loss: 0.00003000
Iteration 191/1000 | Loss: 0.00003000
Iteration 192/1000 | Loss: 0.00003000
Iteration 193/1000 | Loss: 0.00003000
Iteration 194/1000 | Loss: 0.00003000
Iteration 195/1000 | Loss: 0.00003000
Iteration 196/1000 | Loss: 0.00003000
Iteration 197/1000 | Loss: 0.00003000
Iteration 198/1000 | Loss: 0.00003000
Iteration 199/1000 | Loss: 0.00003000
Iteration 200/1000 | Loss: 0.00003000
Iteration 201/1000 | Loss: 0.00003000
Iteration 202/1000 | Loss: 0.00003000
Iteration 203/1000 | Loss: 0.00003000
Iteration 204/1000 | Loss: 0.00003000
Iteration 205/1000 | Loss: 0.00003000
Iteration 206/1000 | Loss: 0.00003000
Iteration 207/1000 | Loss: 0.00003000
Iteration 208/1000 | Loss: 0.00003000
Iteration 209/1000 | Loss: 0.00003000
Iteration 210/1000 | Loss: 0.00003000
Iteration 211/1000 | Loss: 0.00003000
Iteration 212/1000 | Loss: 0.00003000
Iteration 213/1000 | Loss: 0.00003000
Iteration 214/1000 | Loss: 0.00003000
Iteration 215/1000 | Loss: 0.00003000
Iteration 216/1000 | Loss: 0.00003000
Iteration 217/1000 | Loss: 0.00003000
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [2.9999946491443552e-05, 2.9999946491443552e-05, 2.9999946491443552e-05, 2.9999946491443552e-05, 2.9999946491443552e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9999946491443552e-05

Optimization complete. Final v2v error: 4.343744277954102 mm

Highest mean error: 6.942177772521973 mm for frame 60

Lowest mean error: 2.969360828399658 mm for frame 100

Saving results

Total time: 114.23729681968689
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00801918
Iteration 2/25 | Loss: 0.00146456
Iteration 3/25 | Loss: 0.00089189
Iteration 4/25 | Loss: 0.00074458
Iteration 5/25 | Loss: 0.00069499
Iteration 6/25 | Loss: 0.00067882
Iteration 7/25 | Loss: 0.00068202
Iteration 8/25 | Loss: 0.00065042
Iteration 9/25 | Loss: 0.00064641
Iteration 10/25 | Loss: 0.00064157
Iteration 11/25 | Loss: 0.00063599
Iteration 12/25 | Loss: 0.00063368
Iteration 13/25 | Loss: 0.00063343
Iteration 14/25 | Loss: 0.00063332
Iteration 15/25 | Loss: 0.00063331
Iteration 16/25 | Loss: 0.00063331
Iteration 17/25 | Loss: 0.00063331
Iteration 18/25 | Loss: 0.00063331
Iteration 19/25 | Loss: 0.00063331
Iteration 20/25 | Loss: 0.00063331
Iteration 21/25 | Loss: 0.00063330
Iteration 22/25 | Loss: 0.00063330
Iteration 23/25 | Loss: 0.00063330
Iteration 24/25 | Loss: 0.00063330
Iteration 25/25 | Loss: 0.00063330

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39942455
Iteration 2/25 | Loss: 0.00020422
Iteration 3/25 | Loss: 0.00020421
Iteration 4/25 | Loss: 0.00020421
Iteration 5/25 | Loss: 0.00020421
Iteration 6/25 | Loss: 0.00020421
Iteration 7/25 | Loss: 0.00020421
Iteration 8/25 | Loss: 0.00020421
Iteration 9/25 | Loss: 0.00020421
Iteration 10/25 | Loss: 0.00020420
Iteration 11/25 | Loss: 0.00020420
Iteration 12/25 | Loss: 0.00020420
Iteration 13/25 | Loss: 0.00020420
Iteration 14/25 | Loss: 0.00020420
Iteration 15/25 | Loss: 0.00020420
Iteration 16/25 | Loss: 0.00020420
Iteration 17/25 | Loss: 0.00020420
Iteration 18/25 | Loss: 0.00020420
Iteration 19/25 | Loss: 0.00020420
Iteration 20/25 | Loss: 0.00020420
Iteration 21/25 | Loss: 0.00020420
Iteration 22/25 | Loss: 0.00020420
Iteration 23/25 | Loss: 0.00020420
Iteration 24/25 | Loss: 0.00020420
Iteration 25/25 | Loss: 0.00020420
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.000204204858164303, 0.000204204858164303, 0.000204204858164303, 0.000204204858164303, 0.000204204858164303]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000204204858164303

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00020420
Iteration 2/1000 | Loss: 0.00002313
Iteration 3/1000 | Loss: 0.00001766
Iteration 4/1000 | Loss: 0.00001621
Iteration 5/1000 | Loss: 0.00001533
Iteration 6/1000 | Loss: 0.00001486
Iteration 7/1000 | Loss: 0.00001459
Iteration 8/1000 | Loss: 0.00001457
Iteration 9/1000 | Loss: 0.00001437
Iteration 10/1000 | Loss: 0.00001430
Iteration 11/1000 | Loss: 0.00001420
Iteration 12/1000 | Loss: 0.00001419
Iteration 13/1000 | Loss: 0.00001419
Iteration 14/1000 | Loss: 0.00001419
Iteration 15/1000 | Loss: 0.00001417
Iteration 16/1000 | Loss: 0.00001410
Iteration 17/1000 | Loss: 0.00001408
Iteration 18/1000 | Loss: 0.00001408
Iteration 19/1000 | Loss: 0.00001408
Iteration 20/1000 | Loss: 0.00001408
Iteration 21/1000 | Loss: 0.00001408
Iteration 22/1000 | Loss: 0.00001408
Iteration 23/1000 | Loss: 0.00001407
Iteration 24/1000 | Loss: 0.00001407
Iteration 25/1000 | Loss: 0.00001406
Iteration 26/1000 | Loss: 0.00001405
Iteration 27/1000 | Loss: 0.00001405
Iteration 28/1000 | Loss: 0.00001405
Iteration 29/1000 | Loss: 0.00001404
Iteration 30/1000 | Loss: 0.00001403
Iteration 31/1000 | Loss: 0.00001401
Iteration 32/1000 | Loss: 0.00001400
Iteration 33/1000 | Loss: 0.00001400
Iteration 34/1000 | Loss: 0.00001400
Iteration 35/1000 | Loss: 0.00001400
Iteration 36/1000 | Loss: 0.00001399
Iteration 37/1000 | Loss: 0.00001399
Iteration 38/1000 | Loss: 0.00001399
Iteration 39/1000 | Loss: 0.00001399
Iteration 40/1000 | Loss: 0.00001398
Iteration 41/1000 | Loss: 0.00001398
Iteration 42/1000 | Loss: 0.00001398
Iteration 43/1000 | Loss: 0.00001398
Iteration 44/1000 | Loss: 0.00001397
Iteration 45/1000 | Loss: 0.00001397
Iteration 46/1000 | Loss: 0.00001397
Iteration 47/1000 | Loss: 0.00001397
Iteration 48/1000 | Loss: 0.00001396
Iteration 49/1000 | Loss: 0.00001396
Iteration 50/1000 | Loss: 0.00001396
Iteration 51/1000 | Loss: 0.00001396
Iteration 52/1000 | Loss: 0.00001395
Iteration 53/1000 | Loss: 0.00001395
Iteration 54/1000 | Loss: 0.00001395
Iteration 55/1000 | Loss: 0.00001395
Iteration 56/1000 | Loss: 0.00001395
Iteration 57/1000 | Loss: 0.00001395
Iteration 58/1000 | Loss: 0.00001395
Iteration 59/1000 | Loss: 0.00001395
Iteration 60/1000 | Loss: 0.00001395
Iteration 61/1000 | Loss: 0.00001394
Iteration 62/1000 | Loss: 0.00001394
Iteration 63/1000 | Loss: 0.00001394
Iteration 64/1000 | Loss: 0.00001394
Iteration 65/1000 | Loss: 0.00001394
Iteration 66/1000 | Loss: 0.00001394
Iteration 67/1000 | Loss: 0.00001394
Iteration 68/1000 | Loss: 0.00001394
Iteration 69/1000 | Loss: 0.00001394
Iteration 70/1000 | Loss: 0.00001394
Iteration 71/1000 | Loss: 0.00001394
Iteration 72/1000 | Loss: 0.00001394
Iteration 73/1000 | Loss: 0.00001394
Iteration 74/1000 | Loss: 0.00001394
Iteration 75/1000 | Loss: 0.00001394
Iteration 76/1000 | Loss: 0.00001394
Iteration 77/1000 | Loss: 0.00001394
Iteration 78/1000 | Loss: 0.00001394
Iteration 79/1000 | Loss: 0.00001394
Iteration 80/1000 | Loss: 0.00001394
Iteration 81/1000 | Loss: 0.00001394
Iteration 82/1000 | Loss: 0.00001394
Iteration 83/1000 | Loss: 0.00001394
Iteration 84/1000 | Loss: 0.00001394
Iteration 85/1000 | Loss: 0.00001394
Iteration 86/1000 | Loss: 0.00001394
Iteration 87/1000 | Loss: 0.00001394
Iteration 88/1000 | Loss: 0.00001394
Iteration 89/1000 | Loss: 0.00001394
Iteration 90/1000 | Loss: 0.00001394
Iteration 91/1000 | Loss: 0.00001394
Iteration 92/1000 | Loss: 0.00001394
Iteration 93/1000 | Loss: 0.00001394
Iteration 94/1000 | Loss: 0.00001394
Iteration 95/1000 | Loss: 0.00001394
Iteration 96/1000 | Loss: 0.00001394
Iteration 97/1000 | Loss: 0.00001394
Iteration 98/1000 | Loss: 0.00001394
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [1.3941587894805707e-05, 1.3941587894805707e-05, 1.3941587894805707e-05, 1.3941587894805707e-05, 1.3941587894805707e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3941587894805707e-05

Optimization complete. Final v2v error: 3.167130947113037 mm

Highest mean error: 3.8129165172576904 mm for frame 182

Lowest mean error: 2.917909860610962 mm for frame 96

Saving results

Total time: 50.6754047870636
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01046214
Iteration 2/25 | Loss: 0.00291396
Iteration 3/25 | Loss: 0.00228234
Iteration 4/25 | Loss: 0.00215727
Iteration 5/25 | Loss: 0.00209169
Iteration 6/25 | Loss: 0.00200184
Iteration 7/25 | Loss: 0.00175264
Iteration 8/25 | Loss: 0.00163754
Iteration 9/25 | Loss: 0.00159413
Iteration 10/25 | Loss: 0.00157908
Iteration 11/25 | Loss: 0.00157205
Iteration 12/25 | Loss: 0.00157741
Iteration 13/25 | Loss: 0.00157567
Iteration 14/25 | Loss: 0.00157262
Iteration 15/25 | Loss: 0.00157463
Iteration 16/25 | Loss: 0.00157062
Iteration 17/25 | Loss: 0.00156856
Iteration 18/25 | Loss: 0.00156517
Iteration 19/25 | Loss: 0.00156410
Iteration 20/25 | Loss: 0.00156361
Iteration 21/25 | Loss: 0.00156346
Iteration 22/25 | Loss: 0.00156344
Iteration 23/25 | Loss: 0.00156344
Iteration 24/25 | Loss: 0.00156344
Iteration 25/25 | Loss: 0.00156343

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42361963
Iteration 2/25 | Loss: 0.00619194
Iteration 3/25 | Loss: 0.00619194
Iteration 4/25 | Loss: 0.00619194
Iteration 5/25 | Loss: 0.00619193
Iteration 6/25 | Loss: 0.00619193
Iteration 7/25 | Loss: 0.00619193
Iteration 8/25 | Loss: 0.00619193
Iteration 9/25 | Loss: 0.00619193
Iteration 10/25 | Loss: 0.00619193
Iteration 11/25 | Loss: 0.00619193
Iteration 12/25 | Loss: 0.00619193
Iteration 13/25 | Loss: 0.00619193
Iteration 14/25 | Loss: 0.00619193
Iteration 15/25 | Loss: 0.00619193
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.006191932130604982, 0.006191932130604982, 0.006191932130604982, 0.006191932130604982, 0.006191932130604982]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.006191932130604982

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00619193
Iteration 2/1000 | Loss: 0.01654366
Iteration 3/1000 | Loss: 0.00183758
Iteration 4/1000 | Loss: 0.00064985
Iteration 5/1000 | Loss: 0.00036704
Iteration 6/1000 | Loss: 0.00017828
Iteration 7/1000 | Loss: 0.00010376
Iteration 8/1000 | Loss: 0.00006791
Iteration 9/1000 | Loss: 0.00004979
Iteration 10/1000 | Loss: 0.00013244
Iteration 11/1000 | Loss: 0.00012101
Iteration 12/1000 | Loss: 0.00009237
Iteration 13/1000 | Loss: 0.00004959
Iteration 14/1000 | Loss: 0.00003581
Iteration 15/1000 | Loss: 0.00011081
Iteration 16/1000 | Loss: 0.00003508
Iteration 17/1000 | Loss: 0.00002934
Iteration 18/1000 | Loss: 0.00002602
Iteration 19/1000 | Loss: 0.00002884
Iteration 20/1000 | Loss: 0.00002527
Iteration 21/1000 | Loss: 0.00002307
Iteration 22/1000 | Loss: 0.00002207
Iteration 23/1000 | Loss: 0.00002114
Iteration 24/1000 | Loss: 0.00002032
Iteration 25/1000 | Loss: 0.00001986
Iteration 26/1000 | Loss: 0.00001937
Iteration 27/1000 | Loss: 0.00001894
Iteration 28/1000 | Loss: 0.00001859
Iteration 29/1000 | Loss: 0.00001842
Iteration 30/1000 | Loss: 0.00001821
Iteration 31/1000 | Loss: 0.00001808
Iteration 32/1000 | Loss: 0.00001798
Iteration 33/1000 | Loss: 0.00001797
Iteration 34/1000 | Loss: 0.00001796
Iteration 35/1000 | Loss: 0.00001795
Iteration 36/1000 | Loss: 0.00001794
Iteration 37/1000 | Loss: 0.00001794
Iteration 38/1000 | Loss: 0.00001790
Iteration 39/1000 | Loss: 0.00001789
Iteration 40/1000 | Loss: 0.00001788
Iteration 41/1000 | Loss: 0.00001787
Iteration 42/1000 | Loss: 0.00001787
Iteration 43/1000 | Loss: 0.00001786
Iteration 44/1000 | Loss: 0.00001786
Iteration 45/1000 | Loss: 0.00001785
Iteration 46/1000 | Loss: 0.00001785
Iteration 47/1000 | Loss: 0.00001785
Iteration 48/1000 | Loss: 0.00001784
Iteration 49/1000 | Loss: 0.00001784
Iteration 50/1000 | Loss: 0.00001784
Iteration 51/1000 | Loss: 0.00001784
Iteration 52/1000 | Loss: 0.00001783
Iteration 53/1000 | Loss: 0.00001783
Iteration 54/1000 | Loss: 0.00001783
Iteration 55/1000 | Loss: 0.00001782
Iteration 56/1000 | Loss: 0.00001782
Iteration 57/1000 | Loss: 0.00001782
Iteration 58/1000 | Loss: 0.00001781
Iteration 59/1000 | Loss: 0.00001781
Iteration 60/1000 | Loss: 0.00001781
Iteration 61/1000 | Loss: 0.00001781
Iteration 62/1000 | Loss: 0.00001781
Iteration 63/1000 | Loss: 0.00001781
Iteration 64/1000 | Loss: 0.00001781
Iteration 65/1000 | Loss: 0.00001781
Iteration 66/1000 | Loss: 0.00001780
Iteration 67/1000 | Loss: 0.00001780
Iteration 68/1000 | Loss: 0.00001780
Iteration 69/1000 | Loss: 0.00001779
Iteration 70/1000 | Loss: 0.00001779
Iteration 71/1000 | Loss: 0.00001779
Iteration 72/1000 | Loss: 0.00001779
Iteration 73/1000 | Loss: 0.00001779
Iteration 74/1000 | Loss: 0.00001779
Iteration 75/1000 | Loss: 0.00001778
Iteration 76/1000 | Loss: 0.00001778
Iteration 77/1000 | Loss: 0.00001778
Iteration 78/1000 | Loss: 0.00001778
Iteration 79/1000 | Loss: 0.00001778
Iteration 80/1000 | Loss: 0.00001778
Iteration 81/1000 | Loss: 0.00001777
Iteration 82/1000 | Loss: 0.00001777
Iteration 83/1000 | Loss: 0.00001777
Iteration 84/1000 | Loss: 0.00001777
Iteration 85/1000 | Loss: 0.00001777
Iteration 86/1000 | Loss: 0.00001776
Iteration 87/1000 | Loss: 0.00001776
Iteration 88/1000 | Loss: 0.00001776
Iteration 89/1000 | Loss: 0.00001776
Iteration 90/1000 | Loss: 0.00001776
Iteration 91/1000 | Loss: 0.00001776
Iteration 92/1000 | Loss: 0.00001776
Iteration 93/1000 | Loss: 0.00001775
Iteration 94/1000 | Loss: 0.00001775
Iteration 95/1000 | Loss: 0.00001775
Iteration 96/1000 | Loss: 0.00001775
Iteration 97/1000 | Loss: 0.00001774
Iteration 98/1000 | Loss: 0.00001774
Iteration 99/1000 | Loss: 0.00001774
Iteration 100/1000 | Loss: 0.00001774
Iteration 101/1000 | Loss: 0.00001774
Iteration 102/1000 | Loss: 0.00001774
Iteration 103/1000 | Loss: 0.00001774
Iteration 104/1000 | Loss: 0.00001774
Iteration 105/1000 | Loss: 0.00001774
Iteration 106/1000 | Loss: 0.00001774
Iteration 107/1000 | Loss: 0.00001774
Iteration 108/1000 | Loss: 0.00001773
Iteration 109/1000 | Loss: 0.00001773
Iteration 110/1000 | Loss: 0.00001773
Iteration 111/1000 | Loss: 0.00001773
Iteration 112/1000 | Loss: 0.00001773
Iteration 113/1000 | Loss: 0.00001773
Iteration 114/1000 | Loss: 0.00001773
Iteration 115/1000 | Loss: 0.00001773
Iteration 116/1000 | Loss: 0.00001773
Iteration 117/1000 | Loss: 0.00001773
Iteration 118/1000 | Loss: 0.00001773
Iteration 119/1000 | Loss: 0.00001773
Iteration 120/1000 | Loss: 0.00001772
Iteration 121/1000 | Loss: 0.00001772
Iteration 122/1000 | Loss: 0.00001772
Iteration 123/1000 | Loss: 0.00001772
Iteration 124/1000 | Loss: 0.00001772
Iteration 125/1000 | Loss: 0.00001772
Iteration 126/1000 | Loss: 0.00001772
Iteration 127/1000 | Loss: 0.00001772
Iteration 128/1000 | Loss: 0.00001772
Iteration 129/1000 | Loss: 0.00001772
Iteration 130/1000 | Loss: 0.00001772
Iteration 131/1000 | Loss: 0.00001772
Iteration 132/1000 | Loss: 0.00001772
Iteration 133/1000 | Loss: 0.00001771
Iteration 134/1000 | Loss: 0.00001771
Iteration 135/1000 | Loss: 0.00001771
Iteration 136/1000 | Loss: 0.00001771
Iteration 137/1000 | Loss: 0.00001771
Iteration 138/1000 | Loss: 0.00001771
Iteration 139/1000 | Loss: 0.00001771
Iteration 140/1000 | Loss: 0.00001771
Iteration 141/1000 | Loss: 0.00001771
Iteration 142/1000 | Loss: 0.00001771
Iteration 143/1000 | Loss: 0.00001771
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.771290772012435e-05, 1.771290772012435e-05, 1.771290772012435e-05, 1.771290772012435e-05, 1.771290772012435e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.771290772012435e-05

Optimization complete. Final v2v error: 3.5379831790924072 mm

Highest mean error: 4.056304931640625 mm for frame 238

Lowest mean error: 3.392678737640381 mm for frame 95

Saving results

Total time: 104.155264377594
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01049593
Iteration 2/25 | Loss: 0.00184329
Iteration 3/25 | Loss: 0.00104721
Iteration 4/25 | Loss: 0.00100258
Iteration 5/25 | Loss: 0.00085099
Iteration 6/25 | Loss: 0.00079544
Iteration 7/25 | Loss: 0.00077679
Iteration 8/25 | Loss: 0.00075492
Iteration 9/25 | Loss: 0.00073397
Iteration 10/25 | Loss: 0.00070404
Iteration 11/25 | Loss: 0.00070307
Iteration 12/25 | Loss: 0.00068658
Iteration 13/25 | Loss: 0.00066802
Iteration 14/25 | Loss: 0.00066505
Iteration 15/25 | Loss: 0.00065479
Iteration 16/25 | Loss: 0.00064725
Iteration 17/25 | Loss: 0.00064200
Iteration 18/25 | Loss: 0.00063847
Iteration 19/25 | Loss: 0.00063436
Iteration 20/25 | Loss: 0.00063435
Iteration 21/25 | Loss: 0.00063523
Iteration 22/25 | Loss: 0.00063367
Iteration 23/25 | Loss: 0.00063362
Iteration 24/25 | Loss: 0.00063421
Iteration 25/25 | Loss: 0.00063187

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47660816
Iteration 2/25 | Loss: 0.00037355
Iteration 3/25 | Loss: 0.00037355
Iteration 4/25 | Loss: 0.00037355
Iteration 5/25 | Loss: 0.00037355
Iteration 6/25 | Loss: 0.00037355
Iteration 7/25 | Loss: 0.00037355
Iteration 8/25 | Loss: 0.00037355
Iteration 9/25 | Loss: 0.00037355
Iteration 10/25 | Loss: 0.00037355
Iteration 11/25 | Loss: 0.00037355
Iteration 12/25 | Loss: 0.00037355
Iteration 13/25 | Loss: 0.00037355
Iteration 14/25 | Loss: 0.00037355
Iteration 15/25 | Loss: 0.00037355
Iteration 16/25 | Loss: 0.00037355
Iteration 17/25 | Loss: 0.00037355
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0003735478385351598, 0.0003735478385351598, 0.0003735478385351598, 0.0003735478385351598, 0.0003735478385351598]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003735478385351598

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037355
Iteration 2/1000 | Loss: 0.00023061
Iteration 3/1000 | Loss: 0.00046971
Iteration 4/1000 | Loss: 0.00097592
Iteration 5/1000 | Loss: 0.00091928
Iteration 6/1000 | Loss: 0.00103720
Iteration 7/1000 | Loss: 0.00056602
Iteration 8/1000 | Loss: 0.00076887
Iteration 9/1000 | Loss: 0.00040175
Iteration 10/1000 | Loss: 0.00050743
Iteration 11/1000 | Loss: 0.00056229
Iteration 12/1000 | Loss: 0.00109002
Iteration 13/1000 | Loss: 0.00053752
Iteration 14/1000 | Loss: 0.00055266
Iteration 15/1000 | Loss: 0.00056079
Iteration 16/1000 | Loss: 0.00118268
Iteration 17/1000 | Loss: 0.00063538
Iteration 18/1000 | Loss: 0.00092267
Iteration 19/1000 | Loss: 0.00093739
Iteration 20/1000 | Loss: 0.00152101
Iteration 21/1000 | Loss: 0.00243074
Iteration 22/1000 | Loss: 0.00049083
Iteration 23/1000 | Loss: 0.00058397
Iteration 24/1000 | Loss: 0.00115821
Iteration 25/1000 | Loss: 0.00047092
Iteration 26/1000 | Loss: 0.00019122
Iteration 27/1000 | Loss: 0.00027290
Iteration 28/1000 | Loss: 0.00065768
Iteration 29/1000 | Loss: 0.00007612
Iteration 30/1000 | Loss: 0.00016022
Iteration 31/1000 | Loss: 0.00023515
Iteration 32/1000 | Loss: 0.00026862
Iteration 33/1000 | Loss: 0.00026915
Iteration 34/1000 | Loss: 0.00031781
Iteration 35/1000 | Loss: 0.00026009
Iteration 36/1000 | Loss: 0.00060629
Iteration 37/1000 | Loss: 0.00038883
Iteration 38/1000 | Loss: 0.00031454
Iteration 39/1000 | Loss: 0.00054164
Iteration 40/1000 | Loss: 0.00041166
Iteration 41/1000 | Loss: 0.00043403
Iteration 42/1000 | Loss: 0.00023888
Iteration 43/1000 | Loss: 0.00026183
Iteration 44/1000 | Loss: 0.00037697
Iteration 45/1000 | Loss: 0.00043915
Iteration 46/1000 | Loss: 0.00041372
Iteration 47/1000 | Loss: 0.00038751
Iteration 48/1000 | Loss: 0.00051649
Iteration 49/1000 | Loss: 0.00053271
Iteration 50/1000 | Loss: 0.00047058
Iteration 51/1000 | Loss: 0.00056161
Iteration 52/1000 | Loss: 0.00055602
Iteration 53/1000 | Loss: 0.00006743
Iteration 54/1000 | Loss: 0.00065203
Iteration 55/1000 | Loss: 0.00055978
Iteration 56/1000 | Loss: 0.00046498
Iteration 57/1000 | Loss: 0.00019243
Iteration 58/1000 | Loss: 0.00027709
Iteration 59/1000 | Loss: 0.00038897
Iteration 60/1000 | Loss: 0.00073486
Iteration 61/1000 | Loss: 0.00018164
Iteration 62/1000 | Loss: 0.00039890
Iteration 63/1000 | Loss: 0.00077503
Iteration 64/1000 | Loss: 0.00029352
Iteration 65/1000 | Loss: 0.00018168
Iteration 66/1000 | Loss: 0.00009212
Iteration 67/1000 | Loss: 0.00021755
Iteration 68/1000 | Loss: 0.00024975
Iteration 69/1000 | Loss: 0.00019510
Iteration 70/1000 | Loss: 0.00006599
Iteration 71/1000 | Loss: 0.00033796
Iteration 72/1000 | Loss: 0.00019765
Iteration 73/1000 | Loss: 0.00023478
Iteration 74/1000 | Loss: 0.00004059
Iteration 75/1000 | Loss: 0.00002861
Iteration 76/1000 | Loss: 0.00005507
Iteration 77/1000 | Loss: 0.00050393
Iteration 78/1000 | Loss: 0.00056393
Iteration 79/1000 | Loss: 0.00011294
Iteration 80/1000 | Loss: 0.00014657
Iteration 81/1000 | Loss: 0.00025759
Iteration 82/1000 | Loss: 0.00027451
Iteration 83/1000 | Loss: 0.00022190
Iteration 84/1000 | Loss: 0.00032277
Iteration 85/1000 | Loss: 0.00028537
Iteration 86/1000 | Loss: 0.00033641
Iteration 87/1000 | Loss: 0.00007758
Iteration 88/1000 | Loss: 0.00098999
Iteration 89/1000 | Loss: 0.00004455
Iteration 90/1000 | Loss: 0.00002796
Iteration 91/1000 | Loss: 0.00002187
Iteration 92/1000 | Loss: 0.00001997
Iteration 93/1000 | Loss: 0.00001823
Iteration 94/1000 | Loss: 0.00001680
Iteration 95/1000 | Loss: 0.00032648
Iteration 96/1000 | Loss: 0.00046956
Iteration 97/1000 | Loss: 0.00027372
Iteration 98/1000 | Loss: 0.00003419
Iteration 99/1000 | Loss: 0.00002583
Iteration 100/1000 | Loss: 0.00001638
Iteration 101/1000 | Loss: 0.00001438
Iteration 102/1000 | Loss: 0.00001387
Iteration 103/1000 | Loss: 0.00001334
Iteration 104/1000 | Loss: 0.00001294
Iteration 105/1000 | Loss: 0.00001277
Iteration 106/1000 | Loss: 0.00001270
Iteration 107/1000 | Loss: 0.00001265
Iteration 108/1000 | Loss: 0.00001264
Iteration 109/1000 | Loss: 0.00001262
Iteration 110/1000 | Loss: 0.00001261
Iteration 111/1000 | Loss: 0.00001261
Iteration 112/1000 | Loss: 0.00001260
Iteration 113/1000 | Loss: 0.00001259
Iteration 114/1000 | Loss: 0.00001259
Iteration 115/1000 | Loss: 0.00001258
Iteration 116/1000 | Loss: 0.00001258
Iteration 117/1000 | Loss: 0.00001257
Iteration 118/1000 | Loss: 0.00001256
Iteration 119/1000 | Loss: 0.00001256
Iteration 120/1000 | Loss: 0.00001255
Iteration 121/1000 | Loss: 0.00001255
Iteration 122/1000 | Loss: 0.00001255
Iteration 123/1000 | Loss: 0.00001254
Iteration 124/1000 | Loss: 0.00001254
Iteration 125/1000 | Loss: 0.00001254
Iteration 126/1000 | Loss: 0.00001254
Iteration 127/1000 | Loss: 0.00001253
Iteration 128/1000 | Loss: 0.00001253
Iteration 129/1000 | Loss: 0.00001253
Iteration 130/1000 | Loss: 0.00001253
Iteration 131/1000 | Loss: 0.00001253
Iteration 132/1000 | Loss: 0.00001253
Iteration 133/1000 | Loss: 0.00001252
Iteration 134/1000 | Loss: 0.00001252
Iteration 135/1000 | Loss: 0.00001252
Iteration 136/1000 | Loss: 0.00001252
Iteration 137/1000 | Loss: 0.00001252
Iteration 138/1000 | Loss: 0.00001252
Iteration 139/1000 | Loss: 0.00001251
Iteration 140/1000 | Loss: 0.00001251
Iteration 141/1000 | Loss: 0.00001251
Iteration 142/1000 | Loss: 0.00001251
Iteration 143/1000 | Loss: 0.00001251
Iteration 144/1000 | Loss: 0.00001251
Iteration 145/1000 | Loss: 0.00001251
Iteration 146/1000 | Loss: 0.00001250
Iteration 147/1000 | Loss: 0.00001250
Iteration 148/1000 | Loss: 0.00001250
Iteration 149/1000 | Loss: 0.00001249
Iteration 150/1000 | Loss: 0.00001249
Iteration 151/1000 | Loss: 0.00001248
Iteration 152/1000 | Loss: 0.00001247
Iteration 153/1000 | Loss: 0.00001247
Iteration 154/1000 | Loss: 0.00001247
Iteration 155/1000 | Loss: 0.00001246
Iteration 156/1000 | Loss: 0.00001246
Iteration 157/1000 | Loss: 0.00001246
Iteration 158/1000 | Loss: 0.00001245
Iteration 159/1000 | Loss: 0.00001245
Iteration 160/1000 | Loss: 0.00001244
Iteration 161/1000 | Loss: 0.00001244
Iteration 162/1000 | Loss: 0.00001243
Iteration 163/1000 | Loss: 0.00001243
Iteration 164/1000 | Loss: 0.00001243
Iteration 165/1000 | Loss: 0.00001242
Iteration 166/1000 | Loss: 0.00001242
Iteration 167/1000 | Loss: 0.00001241
Iteration 168/1000 | Loss: 0.00001241
Iteration 169/1000 | Loss: 0.00001240
Iteration 170/1000 | Loss: 0.00001240
Iteration 171/1000 | Loss: 0.00001240
Iteration 172/1000 | Loss: 0.00001240
Iteration 173/1000 | Loss: 0.00001239
Iteration 174/1000 | Loss: 0.00001239
Iteration 175/1000 | Loss: 0.00001239
Iteration 176/1000 | Loss: 0.00001238
Iteration 177/1000 | Loss: 0.00001238
Iteration 178/1000 | Loss: 0.00001238
Iteration 179/1000 | Loss: 0.00001237
Iteration 180/1000 | Loss: 0.00001237
Iteration 181/1000 | Loss: 0.00001237
Iteration 182/1000 | Loss: 0.00001236
Iteration 183/1000 | Loss: 0.00001236
Iteration 184/1000 | Loss: 0.00001236
Iteration 185/1000 | Loss: 0.00001235
Iteration 186/1000 | Loss: 0.00001235
Iteration 187/1000 | Loss: 0.00001234
Iteration 188/1000 | Loss: 0.00001234
Iteration 189/1000 | Loss: 0.00001234
Iteration 190/1000 | Loss: 0.00001233
Iteration 191/1000 | Loss: 0.00001233
Iteration 192/1000 | Loss: 0.00001233
Iteration 193/1000 | Loss: 0.00001232
Iteration 194/1000 | Loss: 0.00001232
Iteration 195/1000 | Loss: 0.00001231
Iteration 196/1000 | Loss: 0.00001231
Iteration 197/1000 | Loss: 0.00001231
Iteration 198/1000 | Loss: 0.00001230
Iteration 199/1000 | Loss: 0.00001230
Iteration 200/1000 | Loss: 0.00001230
Iteration 201/1000 | Loss: 0.00001230
Iteration 202/1000 | Loss: 0.00001229
Iteration 203/1000 | Loss: 0.00001229
Iteration 204/1000 | Loss: 0.00001229
Iteration 205/1000 | Loss: 0.00001228
Iteration 206/1000 | Loss: 0.00001228
Iteration 207/1000 | Loss: 0.00001228
Iteration 208/1000 | Loss: 0.00001228
Iteration 209/1000 | Loss: 0.00001228
Iteration 210/1000 | Loss: 0.00001228
Iteration 211/1000 | Loss: 0.00001227
Iteration 212/1000 | Loss: 0.00001227
Iteration 213/1000 | Loss: 0.00001227
Iteration 214/1000 | Loss: 0.00001227
Iteration 215/1000 | Loss: 0.00001226
Iteration 216/1000 | Loss: 0.00001226
Iteration 217/1000 | Loss: 0.00001226
Iteration 218/1000 | Loss: 0.00001226
Iteration 219/1000 | Loss: 0.00001226
Iteration 220/1000 | Loss: 0.00001226
Iteration 221/1000 | Loss: 0.00001226
Iteration 222/1000 | Loss: 0.00001225
Iteration 223/1000 | Loss: 0.00001225
Iteration 224/1000 | Loss: 0.00001225
Iteration 225/1000 | Loss: 0.00001225
Iteration 226/1000 | Loss: 0.00001225
Iteration 227/1000 | Loss: 0.00001225
Iteration 228/1000 | Loss: 0.00001225
Iteration 229/1000 | Loss: 0.00001225
Iteration 230/1000 | Loss: 0.00001225
Iteration 231/1000 | Loss: 0.00001225
Iteration 232/1000 | Loss: 0.00001224
Iteration 233/1000 | Loss: 0.00001224
Iteration 234/1000 | Loss: 0.00001224
Iteration 235/1000 | Loss: 0.00001224
Iteration 236/1000 | Loss: 0.00001224
Iteration 237/1000 | Loss: 0.00001224
Iteration 238/1000 | Loss: 0.00001224
Iteration 239/1000 | Loss: 0.00001224
Iteration 240/1000 | Loss: 0.00001224
Iteration 241/1000 | Loss: 0.00001224
Iteration 242/1000 | Loss: 0.00001224
Iteration 243/1000 | Loss: 0.00001224
Iteration 244/1000 | Loss: 0.00001224
Iteration 245/1000 | Loss: 0.00001224
Iteration 246/1000 | Loss: 0.00001224
Iteration 247/1000 | Loss: 0.00001224
Iteration 248/1000 | Loss: 0.00001224
Iteration 249/1000 | Loss: 0.00001224
Iteration 250/1000 | Loss: 0.00001223
Iteration 251/1000 | Loss: 0.00001223
Iteration 252/1000 | Loss: 0.00001223
Iteration 253/1000 | Loss: 0.00001223
Iteration 254/1000 | Loss: 0.00001223
Iteration 255/1000 | Loss: 0.00001223
Iteration 256/1000 | Loss: 0.00001223
Iteration 257/1000 | Loss: 0.00001223
Iteration 258/1000 | Loss: 0.00001223
Iteration 259/1000 | Loss: 0.00001223
Iteration 260/1000 | Loss: 0.00001223
Iteration 261/1000 | Loss: 0.00001223
Iteration 262/1000 | Loss: 0.00001223
Iteration 263/1000 | Loss: 0.00001223
Iteration 264/1000 | Loss: 0.00001223
Iteration 265/1000 | Loss: 0.00001223
Iteration 266/1000 | Loss: 0.00001223
Iteration 267/1000 | Loss: 0.00001223
Iteration 268/1000 | Loss: 0.00001223
Iteration 269/1000 | Loss: 0.00001222
Iteration 270/1000 | Loss: 0.00001222
Iteration 271/1000 | Loss: 0.00001222
Iteration 272/1000 | Loss: 0.00001222
Iteration 273/1000 | Loss: 0.00001222
Iteration 274/1000 | Loss: 0.00001222
Iteration 275/1000 | Loss: 0.00001222
Iteration 276/1000 | Loss: 0.00001222
Iteration 277/1000 | Loss: 0.00001222
Iteration 278/1000 | Loss: 0.00001221
Iteration 279/1000 | Loss: 0.00001221
Iteration 280/1000 | Loss: 0.00001221
Iteration 281/1000 | Loss: 0.00001221
Iteration 282/1000 | Loss: 0.00001221
Iteration 283/1000 | Loss: 0.00001221
Iteration 284/1000 | Loss: 0.00001221
Iteration 285/1000 | Loss: 0.00001221
Iteration 286/1000 | Loss: 0.00001221
Iteration 287/1000 | Loss: 0.00001221
Iteration 288/1000 | Loss: 0.00001221
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 288. Stopping optimization.
Last 5 losses: [1.2210525710543152e-05, 1.2210525710543152e-05, 1.2210525710543152e-05, 1.2210525710543152e-05, 1.2210525710543152e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2210525710543152e-05

Optimization complete. Final v2v error: 2.9324405193328857 mm

Highest mean error: 4.132326602935791 mm for frame 23

Lowest mean error: 2.5720274448394775 mm for frame 127

Saving results

Total time: 208.76673126220703
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00999233
Iteration 2/25 | Loss: 0.00203481
Iteration 3/25 | Loss: 0.00111024
Iteration 4/25 | Loss: 0.00089301
Iteration 5/25 | Loss: 0.00087141
Iteration 6/25 | Loss: 0.00082933
Iteration 7/25 | Loss: 0.00076215
Iteration 8/25 | Loss: 0.00069616
Iteration 9/25 | Loss: 0.00067119
Iteration 10/25 | Loss: 0.00064828
Iteration 11/25 | Loss: 0.00064695
Iteration 12/25 | Loss: 0.00063690
Iteration 13/25 | Loss: 0.00063228
Iteration 14/25 | Loss: 0.00063420
Iteration 15/25 | Loss: 0.00062947
Iteration 16/25 | Loss: 0.00063268
Iteration 17/25 | Loss: 0.00063326
Iteration 18/25 | Loss: 0.00063237
Iteration 19/25 | Loss: 0.00062866
Iteration 20/25 | Loss: 0.00062656
Iteration 21/25 | Loss: 0.00062498
Iteration 22/25 | Loss: 0.00062367
Iteration 23/25 | Loss: 0.00062341
Iteration 24/25 | Loss: 0.00062317
Iteration 25/25 | Loss: 0.00062297

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42260563
Iteration 2/25 | Loss: 0.00017360
Iteration 3/25 | Loss: 0.00017360
Iteration 4/25 | Loss: 0.00017360
Iteration 5/25 | Loss: 0.00017360
Iteration 6/25 | Loss: 0.00017360
Iteration 7/25 | Loss: 0.00017360
Iteration 8/25 | Loss: 0.00017360
Iteration 9/25 | Loss: 0.00017360
Iteration 10/25 | Loss: 0.00017360
Iteration 11/25 | Loss: 0.00017360
Iteration 12/25 | Loss: 0.00017360
Iteration 13/25 | Loss: 0.00017360
Iteration 14/25 | Loss: 0.00017360
Iteration 15/25 | Loss: 0.00017360
Iteration 16/25 | Loss: 0.00017360
Iteration 17/25 | Loss: 0.00017360
Iteration 18/25 | Loss: 0.00017360
Iteration 19/25 | Loss: 0.00017360
Iteration 20/25 | Loss: 0.00017360
Iteration 21/25 | Loss: 0.00017360
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00017359681078232825, 0.00017359681078232825, 0.00017359681078232825, 0.00017359681078232825, 0.00017359681078232825]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00017359681078232825

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00017360
Iteration 2/1000 | Loss: 0.00002138
Iteration 3/1000 | Loss: 0.00001697
Iteration 4/1000 | Loss: 0.00001573
Iteration 5/1000 | Loss: 0.00001486
Iteration 6/1000 | Loss: 0.00001434
Iteration 7/1000 | Loss: 0.00001400
Iteration 8/1000 | Loss: 0.00001372
Iteration 9/1000 | Loss: 0.00001351
Iteration 10/1000 | Loss: 0.00001334
Iteration 11/1000 | Loss: 0.00001333
Iteration 12/1000 | Loss: 0.00001333
Iteration 13/1000 | Loss: 0.00001326
Iteration 14/1000 | Loss: 0.00001322
Iteration 15/1000 | Loss: 0.00001316
Iteration 16/1000 | Loss: 0.00001312
Iteration 17/1000 | Loss: 0.00001311
Iteration 18/1000 | Loss: 0.00001311
Iteration 19/1000 | Loss: 0.00001309
Iteration 20/1000 | Loss: 0.00001305
Iteration 21/1000 | Loss: 0.00001304
Iteration 22/1000 | Loss: 0.00001303
Iteration 23/1000 | Loss: 0.00001303
Iteration 24/1000 | Loss: 0.00001303
Iteration 25/1000 | Loss: 0.00001302
Iteration 26/1000 | Loss: 0.00001302
Iteration 27/1000 | Loss: 0.00001301
Iteration 28/1000 | Loss: 0.00001301
Iteration 29/1000 | Loss: 0.00001300
Iteration 30/1000 | Loss: 0.00001300
Iteration 31/1000 | Loss: 0.00001300
Iteration 32/1000 | Loss: 0.00001299
Iteration 33/1000 | Loss: 0.00001298
Iteration 34/1000 | Loss: 0.00001298
Iteration 35/1000 | Loss: 0.00001298
Iteration 36/1000 | Loss: 0.00001297
Iteration 37/1000 | Loss: 0.00001297
Iteration 38/1000 | Loss: 0.00001297
Iteration 39/1000 | Loss: 0.00001296
Iteration 40/1000 | Loss: 0.00001296
Iteration 41/1000 | Loss: 0.00001296
Iteration 42/1000 | Loss: 0.00001296
Iteration 43/1000 | Loss: 0.00001296
Iteration 44/1000 | Loss: 0.00001296
Iteration 45/1000 | Loss: 0.00001296
Iteration 46/1000 | Loss: 0.00001296
Iteration 47/1000 | Loss: 0.00001296
Iteration 48/1000 | Loss: 0.00001296
Iteration 49/1000 | Loss: 0.00001295
Iteration 50/1000 | Loss: 0.00001295
Iteration 51/1000 | Loss: 0.00001295
Iteration 52/1000 | Loss: 0.00001294
Iteration 53/1000 | Loss: 0.00001294
Iteration 54/1000 | Loss: 0.00001294
Iteration 55/1000 | Loss: 0.00001293
Iteration 56/1000 | Loss: 0.00001293
Iteration 57/1000 | Loss: 0.00001293
Iteration 58/1000 | Loss: 0.00001293
Iteration 59/1000 | Loss: 0.00001293
Iteration 60/1000 | Loss: 0.00001292
Iteration 61/1000 | Loss: 0.00001292
Iteration 62/1000 | Loss: 0.00001292
Iteration 63/1000 | Loss: 0.00001292
Iteration 64/1000 | Loss: 0.00001292
Iteration 65/1000 | Loss: 0.00001292
Iteration 66/1000 | Loss: 0.00001292
Iteration 67/1000 | Loss: 0.00001292
Iteration 68/1000 | Loss: 0.00001291
Iteration 69/1000 | Loss: 0.00001291
Iteration 70/1000 | Loss: 0.00001291
Iteration 71/1000 | Loss: 0.00001291
Iteration 72/1000 | Loss: 0.00001290
Iteration 73/1000 | Loss: 0.00001290
Iteration 74/1000 | Loss: 0.00001290
Iteration 75/1000 | Loss: 0.00001290
Iteration 76/1000 | Loss: 0.00001290
Iteration 77/1000 | Loss: 0.00001290
Iteration 78/1000 | Loss: 0.00001289
Iteration 79/1000 | Loss: 0.00001289
Iteration 80/1000 | Loss: 0.00001289
Iteration 81/1000 | Loss: 0.00001289
Iteration 82/1000 | Loss: 0.00001289
Iteration 83/1000 | Loss: 0.00001289
Iteration 84/1000 | Loss: 0.00001289
Iteration 85/1000 | Loss: 0.00001289
Iteration 86/1000 | Loss: 0.00001289
Iteration 87/1000 | Loss: 0.00001288
Iteration 88/1000 | Loss: 0.00001288
Iteration 89/1000 | Loss: 0.00001288
Iteration 90/1000 | Loss: 0.00001288
Iteration 91/1000 | Loss: 0.00001288
Iteration 92/1000 | Loss: 0.00001288
Iteration 93/1000 | Loss: 0.00001288
Iteration 94/1000 | Loss: 0.00001288
Iteration 95/1000 | Loss: 0.00001288
Iteration 96/1000 | Loss: 0.00001288
Iteration 97/1000 | Loss: 0.00001288
Iteration 98/1000 | Loss: 0.00001288
Iteration 99/1000 | Loss: 0.00001288
Iteration 100/1000 | Loss: 0.00001288
Iteration 101/1000 | Loss: 0.00001288
Iteration 102/1000 | Loss: 0.00001288
Iteration 103/1000 | Loss: 0.00001288
Iteration 104/1000 | Loss: 0.00001287
Iteration 105/1000 | Loss: 0.00001287
Iteration 106/1000 | Loss: 0.00001287
Iteration 107/1000 | Loss: 0.00001287
Iteration 108/1000 | Loss: 0.00001287
Iteration 109/1000 | Loss: 0.00001287
Iteration 110/1000 | Loss: 0.00001287
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [1.2874546882812865e-05, 1.2874546882812865e-05, 1.2874546882812865e-05, 1.2874546882812865e-05, 1.2874546882812865e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2874546882812865e-05

Optimization complete. Final v2v error: 3.0075650215148926 mm

Highest mean error: 9.383381843566895 mm for frame 261

Lowest mean error: 2.4748454093933105 mm for frame 182

Saving results

Total time: 81.48772811889648
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00419422
Iteration 2/25 | Loss: 0.00081406
Iteration 3/25 | Loss: 0.00064368
Iteration 4/25 | Loss: 0.00061454
Iteration 5/25 | Loss: 0.00060459
Iteration 6/25 | Loss: 0.00060380
Iteration 7/25 | Loss: 0.00060352
Iteration 8/25 | Loss: 0.00060352
Iteration 9/25 | Loss: 0.00060352
Iteration 10/25 | Loss: 0.00060352
Iteration 11/25 | Loss: 0.00060352
Iteration 12/25 | Loss: 0.00060352
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006035201367922127, 0.0006035201367922127, 0.0006035201367922127, 0.0006035201367922127, 0.0006035201367922127]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006035201367922127

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21885598
Iteration 2/25 | Loss: 0.00018543
Iteration 3/25 | Loss: 0.00018541
Iteration 4/25 | Loss: 0.00018541
Iteration 5/25 | Loss: 0.00018541
Iteration 6/25 | Loss: 0.00018541
Iteration 7/25 | Loss: 0.00018541
Iteration 8/25 | Loss: 0.00018541
Iteration 9/25 | Loss: 0.00018541
Iteration 10/25 | Loss: 0.00018541
Iteration 11/25 | Loss: 0.00018541
Iteration 12/25 | Loss: 0.00018541
Iteration 13/25 | Loss: 0.00018541
Iteration 14/25 | Loss: 0.00018541
Iteration 15/25 | Loss: 0.00018541
Iteration 16/25 | Loss: 0.00018541
Iteration 17/25 | Loss: 0.00018541
Iteration 18/25 | Loss: 0.00018541
Iteration 19/25 | Loss: 0.00018541
Iteration 20/25 | Loss: 0.00018541
Iteration 21/25 | Loss: 0.00018541
Iteration 22/25 | Loss: 0.00018541
Iteration 23/25 | Loss: 0.00018541
Iteration 24/25 | Loss: 0.00018541
Iteration 25/25 | Loss: 0.00018541

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00018541
Iteration 2/1000 | Loss: 0.00002619
Iteration 3/1000 | Loss: 0.00001789
Iteration 4/1000 | Loss: 0.00001639
Iteration 5/1000 | Loss: 0.00001561
Iteration 6/1000 | Loss: 0.00001493
Iteration 7/1000 | Loss: 0.00001462
Iteration 8/1000 | Loss: 0.00001453
Iteration 9/1000 | Loss: 0.00001452
Iteration 10/1000 | Loss: 0.00001444
Iteration 11/1000 | Loss: 0.00001424
Iteration 12/1000 | Loss: 0.00001417
Iteration 13/1000 | Loss: 0.00001413
Iteration 14/1000 | Loss: 0.00001402
Iteration 15/1000 | Loss: 0.00001401
Iteration 16/1000 | Loss: 0.00001400
Iteration 17/1000 | Loss: 0.00001395
Iteration 18/1000 | Loss: 0.00001393
Iteration 19/1000 | Loss: 0.00001393
Iteration 20/1000 | Loss: 0.00001393
Iteration 21/1000 | Loss: 0.00001392
Iteration 22/1000 | Loss: 0.00001392
Iteration 23/1000 | Loss: 0.00001392
Iteration 24/1000 | Loss: 0.00001392
Iteration 25/1000 | Loss: 0.00001391
Iteration 26/1000 | Loss: 0.00001391
Iteration 27/1000 | Loss: 0.00001391
Iteration 28/1000 | Loss: 0.00001391
Iteration 29/1000 | Loss: 0.00001391
Iteration 30/1000 | Loss: 0.00001391
Iteration 31/1000 | Loss: 0.00001391
Iteration 32/1000 | Loss: 0.00001390
Iteration 33/1000 | Loss: 0.00001390
Iteration 34/1000 | Loss: 0.00001390
Iteration 35/1000 | Loss: 0.00001390
Iteration 36/1000 | Loss: 0.00001390
Iteration 37/1000 | Loss: 0.00001390
Iteration 38/1000 | Loss: 0.00001389
Iteration 39/1000 | Loss: 0.00001389
Iteration 40/1000 | Loss: 0.00001389
Iteration 41/1000 | Loss: 0.00001389
Iteration 42/1000 | Loss: 0.00001389
Iteration 43/1000 | Loss: 0.00001389
Iteration 44/1000 | Loss: 0.00001389
Iteration 45/1000 | Loss: 0.00001389
Iteration 46/1000 | Loss: 0.00001389
Iteration 47/1000 | Loss: 0.00001389
Iteration 48/1000 | Loss: 0.00001388
Iteration 49/1000 | Loss: 0.00001388
Iteration 50/1000 | Loss: 0.00001388
Iteration 51/1000 | Loss: 0.00001388
Iteration 52/1000 | Loss: 0.00001388
Iteration 53/1000 | Loss: 0.00001388
Iteration 54/1000 | Loss: 0.00001387
Iteration 55/1000 | Loss: 0.00001387
Iteration 56/1000 | Loss: 0.00001387
Iteration 57/1000 | Loss: 0.00001387
Iteration 58/1000 | Loss: 0.00001387
Iteration 59/1000 | Loss: 0.00001387
Iteration 60/1000 | Loss: 0.00001387
Iteration 61/1000 | Loss: 0.00001387
Iteration 62/1000 | Loss: 0.00001387
Iteration 63/1000 | Loss: 0.00001387
Iteration 64/1000 | Loss: 0.00001387
Iteration 65/1000 | Loss: 0.00001387
Iteration 66/1000 | Loss: 0.00001387
Iteration 67/1000 | Loss: 0.00001387
Iteration 68/1000 | Loss: 0.00001387
Iteration 69/1000 | Loss: 0.00001386
Iteration 70/1000 | Loss: 0.00001386
Iteration 71/1000 | Loss: 0.00001386
Iteration 72/1000 | Loss: 0.00001386
Iteration 73/1000 | Loss: 0.00001386
Iteration 74/1000 | Loss: 0.00001386
Iteration 75/1000 | Loss: 0.00001385
Iteration 76/1000 | Loss: 0.00001385
Iteration 77/1000 | Loss: 0.00001385
Iteration 78/1000 | Loss: 0.00001384
Iteration 79/1000 | Loss: 0.00001384
Iteration 80/1000 | Loss: 0.00001384
Iteration 81/1000 | Loss: 0.00001384
Iteration 82/1000 | Loss: 0.00001383
Iteration 83/1000 | Loss: 0.00001383
Iteration 84/1000 | Loss: 0.00001383
Iteration 85/1000 | Loss: 0.00001383
Iteration 86/1000 | Loss: 0.00001383
Iteration 87/1000 | Loss: 0.00001383
Iteration 88/1000 | Loss: 0.00001382
Iteration 89/1000 | Loss: 0.00001382
Iteration 90/1000 | Loss: 0.00001382
Iteration 91/1000 | Loss: 0.00001382
Iteration 92/1000 | Loss: 0.00001382
Iteration 93/1000 | Loss: 0.00001382
Iteration 94/1000 | Loss: 0.00001382
Iteration 95/1000 | Loss: 0.00001382
Iteration 96/1000 | Loss: 0.00001382
Iteration 97/1000 | Loss: 0.00001382
Iteration 98/1000 | Loss: 0.00001382
Iteration 99/1000 | Loss: 0.00001382
Iteration 100/1000 | Loss: 0.00001382
Iteration 101/1000 | Loss: 0.00001382
Iteration 102/1000 | Loss: 0.00001382
Iteration 103/1000 | Loss: 0.00001382
Iteration 104/1000 | Loss: 0.00001382
Iteration 105/1000 | Loss: 0.00001382
Iteration 106/1000 | Loss: 0.00001382
Iteration 107/1000 | Loss: 0.00001382
Iteration 108/1000 | Loss: 0.00001382
Iteration 109/1000 | Loss: 0.00001382
Iteration 110/1000 | Loss: 0.00001382
Iteration 111/1000 | Loss: 0.00001382
Iteration 112/1000 | Loss: 0.00001382
Iteration 113/1000 | Loss: 0.00001382
Iteration 114/1000 | Loss: 0.00001382
Iteration 115/1000 | Loss: 0.00001382
Iteration 116/1000 | Loss: 0.00001382
Iteration 117/1000 | Loss: 0.00001382
Iteration 118/1000 | Loss: 0.00001382
Iteration 119/1000 | Loss: 0.00001382
Iteration 120/1000 | Loss: 0.00001382
Iteration 121/1000 | Loss: 0.00001382
Iteration 122/1000 | Loss: 0.00001382
Iteration 123/1000 | Loss: 0.00001382
Iteration 124/1000 | Loss: 0.00001382
Iteration 125/1000 | Loss: 0.00001382
Iteration 126/1000 | Loss: 0.00001382
Iteration 127/1000 | Loss: 0.00001382
Iteration 128/1000 | Loss: 0.00001382
Iteration 129/1000 | Loss: 0.00001382
Iteration 130/1000 | Loss: 0.00001382
Iteration 131/1000 | Loss: 0.00001382
Iteration 132/1000 | Loss: 0.00001382
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [1.381973379466217e-05, 1.381973379466217e-05, 1.381973379466217e-05, 1.381973379466217e-05, 1.381973379466217e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.381973379466217e-05

Optimization complete. Final v2v error: 3.217444896697998 mm

Highest mean error: 3.248375415802002 mm for frame 90

Lowest mean error: 3.182925224304199 mm for frame 69

Saving results

Total time: 32.505876541137695
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00422131
Iteration 2/25 | Loss: 0.00073165
Iteration 3/25 | Loss: 0.00061092
Iteration 4/25 | Loss: 0.00058712
Iteration 5/25 | Loss: 0.00057907
Iteration 6/25 | Loss: 0.00057713
Iteration 7/25 | Loss: 0.00057640
Iteration 8/25 | Loss: 0.00057633
Iteration 9/25 | Loss: 0.00057633
Iteration 10/25 | Loss: 0.00057633
Iteration 11/25 | Loss: 0.00057633
Iteration 12/25 | Loss: 0.00057633
Iteration 13/25 | Loss: 0.00057633
Iteration 14/25 | Loss: 0.00057633
Iteration 15/25 | Loss: 0.00057633
Iteration 16/25 | Loss: 0.00057633
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005763323279097676, 0.0005763323279097676, 0.0005763323279097676, 0.0005763323279097676, 0.0005763323279097676]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005763323279097676

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.88492274
Iteration 2/25 | Loss: 0.00018812
Iteration 3/25 | Loss: 0.00018809
Iteration 4/25 | Loss: 0.00018809
Iteration 5/25 | Loss: 0.00018809
Iteration 6/25 | Loss: 0.00018809
Iteration 7/25 | Loss: 0.00018809
Iteration 8/25 | Loss: 0.00018809
Iteration 9/25 | Loss: 0.00018809
Iteration 10/25 | Loss: 0.00018809
Iteration 11/25 | Loss: 0.00018809
Iteration 12/25 | Loss: 0.00018809
Iteration 13/25 | Loss: 0.00018809
Iteration 14/25 | Loss: 0.00018809
Iteration 15/25 | Loss: 0.00018809
Iteration 16/25 | Loss: 0.00018809
Iteration 17/25 | Loss: 0.00018809
Iteration 18/25 | Loss: 0.00018809
Iteration 19/25 | Loss: 0.00018809
Iteration 20/25 | Loss: 0.00018809
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00018809123139362782, 0.00018809123139362782, 0.00018809123139362782, 0.00018809123139362782, 0.00018809123139362782]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00018809123139362782

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00018809
Iteration 2/1000 | Loss: 0.00002816
Iteration 3/1000 | Loss: 0.00001823
Iteration 4/1000 | Loss: 0.00001662
Iteration 5/1000 | Loss: 0.00001567
Iteration 6/1000 | Loss: 0.00001515
Iteration 7/1000 | Loss: 0.00001471
Iteration 8/1000 | Loss: 0.00001430
Iteration 9/1000 | Loss: 0.00001408
Iteration 10/1000 | Loss: 0.00001406
Iteration 11/1000 | Loss: 0.00001403
Iteration 12/1000 | Loss: 0.00001398
Iteration 13/1000 | Loss: 0.00001386
Iteration 14/1000 | Loss: 0.00001384
Iteration 15/1000 | Loss: 0.00001384
Iteration 16/1000 | Loss: 0.00001383
Iteration 17/1000 | Loss: 0.00001383
Iteration 18/1000 | Loss: 0.00001382
Iteration 19/1000 | Loss: 0.00001378
Iteration 20/1000 | Loss: 0.00001375
Iteration 21/1000 | Loss: 0.00001372
Iteration 22/1000 | Loss: 0.00001368
Iteration 23/1000 | Loss: 0.00001368
Iteration 24/1000 | Loss: 0.00001368
Iteration 25/1000 | Loss: 0.00001367
Iteration 26/1000 | Loss: 0.00001366
Iteration 27/1000 | Loss: 0.00001364
Iteration 28/1000 | Loss: 0.00001358
Iteration 29/1000 | Loss: 0.00001355
Iteration 30/1000 | Loss: 0.00001355
Iteration 31/1000 | Loss: 0.00001354
Iteration 32/1000 | Loss: 0.00001354
Iteration 33/1000 | Loss: 0.00001352
Iteration 34/1000 | Loss: 0.00001352
Iteration 35/1000 | Loss: 0.00001352
Iteration 36/1000 | Loss: 0.00001351
Iteration 37/1000 | Loss: 0.00001351
Iteration 38/1000 | Loss: 0.00001351
Iteration 39/1000 | Loss: 0.00001350
Iteration 40/1000 | Loss: 0.00001350
Iteration 41/1000 | Loss: 0.00001349
Iteration 42/1000 | Loss: 0.00001349
Iteration 43/1000 | Loss: 0.00001348
Iteration 44/1000 | Loss: 0.00001348
Iteration 45/1000 | Loss: 0.00001348
Iteration 46/1000 | Loss: 0.00001348
Iteration 47/1000 | Loss: 0.00001348
Iteration 48/1000 | Loss: 0.00001348
Iteration 49/1000 | Loss: 0.00001347
Iteration 50/1000 | Loss: 0.00001347
Iteration 51/1000 | Loss: 0.00001347
Iteration 52/1000 | Loss: 0.00001347
Iteration 53/1000 | Loss: 0.00001347
Iteration 54/1000 | Loss: 0.00001346
Iteration 55/1000 | Loss: 0.00001346
Iteration 56/1000 | Loss: 0.00001345
Iteration 57/1000 | Loss: 0.00001345
Iteration 58/1000 | Loss: 0.00001345
Iteration 59/1000 | Loss: 0.00001344
Iteration 60/1000 | Loss: 0.00001344
Iteration 61/1000 | Loss: 0.00001344
Iteration 62/1000 | Loss: 0.00001344
Iteration 63/1000 | Loss: 0.00001343
Iteration 64/1000 | Loss: 0.00001343
Iteration 65/1000 | Loss: 0.00001342
Iteration 66/1000 | Loss: 0.00001342
Iteration 67/1000 | Loss: 0.00001341
Iteration 68/1000 | Loss: 0.00001341
Iteration 69/1000 | Loss: 0.00001340
Iteration 70/1000 | Loss: 0.00001340
Iteration 71/1000 | Loss: 0.00001339
Iteration 72/1000 | Loss: 0.00001339
Iteration 73/1000 | Loss: 0.00001339
Iteration 74/1000 | Loss: 0.00001339
Iteration 75/1000 | Loss: 0.00001339
Iteration 76/1000 | Loss: 0.00001339
Iteration 77/1000 | Loss: 0.00001338
Iteration 78/1000 | Loss: 0.00001338
Iteration 79/1000 | Loss: 0.00001338
Iteration 80/1000 | Loss: 0.00001338
Iteration 81/1000 | Loss: 0.00001337
Iteration 82/1000 | Loss: 0.00001336
Iteration 83/1000 | Loss: 0.00001336
Iteration 84/1000 | Loss: 0.00001335
Iteration 85/1000 | Loss: 0.00001335
Iteration 86/1000 | Loss: 0.00001335
Iteration 87/1000 | Loss: 0.00001335
Iteration 88/1000 | Loss: 0.00001335
Iteration 89/1000 | Loss: 0.00001334
Iteration 90/1000 | Loss: 0.00001334
Iteration 91/1000 | Loss: 0.00001334
Iteration 92/1000 | Loss: 0.00001333
Iteration 93/1000 | Loss: 0.00001333
Iteration 94/1000 | Loss: 0.00001333
Iteration 95/1000 | Loss: 0.00001333
Iteration 96/1000 | Loss: 0.00001332
Iteration 97/1000 | Loss: 0.00001331
Iteration 98/1000 | Loss: 0.00001331
Iteration 99/1000 | Loss: 0.00001331
Iteration 100/1000 | Loss: 0.00001331
Iteration 101/1000 | Loss: 0.00001331
Iteration 102/1000 | Loss: 0.00001331
Iteration 103/1000 | Loss: 0.00001331
Iteration 104/1000 | Loss: 0.00001331
Iteration 105/1000 | Loss: 0.00001331
Iteration 106/1000 | Loss: 0.00001331
Iteration 107/1000 | Loss: 0.00001330
Iteration 108/1000 | Loss: 0.00001330
Iteration 109/1000 | Loss: 0.00001330
Iteration 110/1000 | Loss: 0.00001330
Iteration 111/1000 | Loss: 0.00001330
Iteration 112/1000 | Loss: 0.00001330
Iteration 113/1000 | Loss: 0.00001330
Iteration 114/1000 | Loss: 0.00001330
Iteration 115/1000 | Loss: 0.00001330
Iteration 116/1000 | Loss: 0.00001330
Iteration 117/1000 | Loss: 0.00001329
Iteration 118/1000 | Loss: 0.00001329
Iteration 119/1000 | Loss: 0.00001329
Iteration 120/1000 | Loss: 0.00001329
Iteration 121/1000 | Loss: 0.00001328
Iteration 122/1000 | Loss: 0.00001328
Iteration 123/1000 | Loss: 0.00001328
Iteration 124/1000 | Loss: 0.00001328
Iteration 125/1000 | Loss: 0.00001328
Iteration 126/1000 | Loss: 0.00001328
Iteration 127/1000 | Loss: 0.00001328
Iteration 128/1000 | Loss: 0.00001328
Iteration 129/1000 | Loss: 0.00001328
Iteration 130/1000 | Loss: 0.00001328
Iteration 131/1000 | Loss: 0.00001327
Iteration 132/1000 | Loss: 0.00001327
Iteration 133/1000 | Loss: 0.00001327
Iteration 134/1000 | Loss: 0.00001327
Iteration 135/1000 | Loss: 0.00001327
Iteration 136/1000 | Loss: 0.00001327
Iteration 137/1000 | Loss: 0.00001327
Iteration 138/1000 | Loss: 0.00001327
Iteration 139/1000 | Loss: 0.00001327
Iteration 140/1000 | Loss: 0.00001327
Iteration 141/1000 | Loss: 0.00001327
Iteration 142/1000 | Loss: 0.00001327
Iteration 143/1000 | Loss: 0.00001327
Iteration 144/1000 | Loss: 0.00001327
Iteration 145/1000 | Loss: 0.00001327
Iteration 146/1000 | Loss: 0.00001326
Iteration 147/1000 | Loss: 0.00001326
Iteration 148/1000 | Loss: 0.00001326
Iteration 149/1000 | Loss: 0.00001326
Iteration 150/1000 | Loss: 0.00001326
Iteration 151/1000 | Loss: 0.00001326
Iteration 152/1000 | Loss: 0.00001326
Iteration 153/1000 | Loss: 0.00001326
Iteration 154/1000 | Loss: 0.00001326
Iteration 155/1000 | Loss: 0.00001326
Iteration 156/1000 | Loss: 0.00001326
Iteration 157/1000 | Loss: 0.00001326
Iteration 158/1000 | Loss: 0.00001326
Iteration 159/1000 | Loss: 0.00001325
Iteration 160/1000 | Loss: 0.00001325
Iteration 161/1000 | Loss: 0.00001325
Iteration 162/1000 | Loss: 0.00001325
Iteration 163/1000 | Loss: 0.00001325
Iteration 164/1000 | Loss: 0.00001325
Iteration 165/1000 | Loss: 0.00001325
Iteration 166/1000 | Loss: 0.00001325
Iteration 167/1000 | Loss: 0.00001325
Iteration 168/1000 | Loss: 0.00001325
Iteration 169/1000 | Loss: 0.00001325
Iteration 170/1000 | Loss: 0.00001325
Iteration 171/1000 | Loss: 0.00001325
Iteration 172/1000 | Loss: 0.00001325
Iteration 173/1000 | Loss: 0.00001325
Iteration 174/1000 | Loss: 0.00001325
Iteration 175/1000 | Loss: 0.00001325
Iteration 176/1000 | Loss: 0.00001325
Iteration 177/1000 | Loss: 0.00001325
Iteration 178/1000 | Loss: 0.00001325
Iteration 179/1000 | Loss: 0.00001325
Iteration 180/1000 | Loss: 0.00001325
Iteration 181/1000 | Loss: 0.00001325
Iteration 182/1000 | Loss: 0.00001325
Iteration 183/1000 | Loss: 0.00001325
Iteration 184/1000 | Loss: 0.00001325
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [1.324651384493336e-05, 1.324651384493336e-05, 1.324651384493336e-05, 1.324651384493336e-05, 1.324651384493336e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.324651384493336e-05

Optimization complete. Final v2v error: 3.076909065246582 mm

Highest mean error: 3.4341602325439453 mm for frame 116

Lowest mean error: 2.7132182121276855 mm for frame 8

Saving results

Total time: 40.96522569656372
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00856553
Iteration 2/25 | Loss: 0.00116632
Iteration 3/25 | Loss: 0.00074193
Iteration 4/25 | Loss: 0.00067487
Iteration 5/25 | Loss: 0.00066670
Iteration 6/25 | Loss: 0.00066576
Iteration 7/25 | Loss: 0.00066576
Iteration 8/25 | Loss: 0.00066576
Iteration 9/25 | Loss: 0.00066576
Iteration 10/25 | Loss: 0.00066576
Iteration 11/25 | Loss: 0.00066576
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0006657601334154606, 0.0006657601334154606, 0.0006657601334154606, 0.0006657601334154606, 0.0006657601334154606]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006657601334154606

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.02830577
Iteration 2/25 | Loss: 0.00018600
Iteration 3/25 | Loss: 0.00018599
Iteration 4/25 | Loss: 0.00018599
Iteration 5/25 | Loss: 0.00018599
Iteration 6/25 | Loss: 0.00018599
Iteration 7/25 | Loss: 0.00018599
Iteration 8/25 | Loss: 0.00018599
Iteration 9/25 | Loss: 0.00018599
Iteration 10/25 | Loss: 0.00018599
Iteration 11/25 | Loss: 0.00018599
Iteration 12/25 | Loss: 0.00018599
Iteration 13/25 | Loss: 0.00018599
Iteration 14/25 | Loss: 0.00018599
Iteration 15/25 | Loss: 0.00018599
Iteration 16/25 | Loss: 0.00018599
Iteration 17/25 | Loss: 0.00018599
Iteration 18/25 | Loss: 0.00018599
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00018598891620058566, 0.00018598891620058566, 0.00018598891620058566, 0.00018598891620058566, 0.00018598891620058566]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00018598891620058566

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00018599
Iteration 2/1000 | Loss: 0.00002654
Iteration 3/1000 | Loss: 0.00002213
Iteration 4/1000 | Loss: 0.00002089
Iteration 5/1000 | Loss: 0.00001990
Iteration 6/1000 | Loss: 0.00001933
Iteration 7/1000 | Loss: 0.00001879
Iteration 8/1000 | Loss: 0.00001857
Iteration 9/1000 | Loss: 0.00001841
Iteration 10/1000 | Loss: 0.00001837
Iteration 11/1000 | Loss: 0.00001837
Iteration 12/1000 | Loss: 0.00001833
Iteration 13/1000 | Loss: 0.00001818
Iteration 14/1000 | Loss: 0.00001818
Iteration 15/1000 | Loss: 0.00001818
Iteration 16/1000 | Loss: 0.00001818
Iteration 17/1000 | Loss: 0.00001818
Iteration 18/1000 | Loss: 0.00001818
Iteration 19/1000 | Loss: 0.00001818
Iteration 20/1000 | Loss: 0.00001818
Iteration 21/1000 | Loss: 0.00001818
Iteration 22/1000 | Loss: 0.00001817
Iteration 23/1000 | Loss: 0.00001817
Iteration 24/1000 | Loss: 0.00001817
Iteration 25/1000 | Loss: 0.00001816
Iteration 26/1000 | Loss: 0.00001816
Iteration 27/1000 | Loss: 0.00001814
Iteration 28/1000 | Loss: 0.00001812
Iteration 29/1000 | Loss: 0.00001811
Iteration 30/1000 | Loss: 0.00001811
Iteration 31/1000 | Loss: 0.00001811
Iteration 32/1000 | Loss: 0.00001811
Iteration 33/1000 | Loss: 0.00001811
Iteration 34/1000 | Loss: 0.00001811
Iteration 35/1000 | Loss: 0.00001811
Iteration 36/1000 | Loss: 0.00001811
Iteration 37/1000 | Loss: 0.00001810
Iteration 38/1000 | Loss: 0.00001809
Iteration 39/1000 | Loss: 0.00001809
Iteration 40/1000 | Loss: 0.00001808
Iteration 41/1000 | Loss: 0.00001808
Iteration 42/1000 | Loss: 0.00001808
Iteration 43/1000 | Loss: 0.00001808
Iteration 44/1000 | Loss: 0.00001807
Iteration 45/1000 | Loss: 0.00001807
Iteration 46/1000 | Loss: 0.00001807
Iteration 47/1000 | Loss: 0.00001807
Iteration 48/1000 | Loss: 0.00001807
Iteration 49/1000 | Loss: 0.00001807
Iteration 50/1000 | Loss: 0.00001806
Iteration 51/1000 | Loss: 0.00001806
Iteration 52/1000 | Loss: 0.00001806
Iteration 53/1000 | Loss: 0.00001806
Iteration 54/1000 | Loss: 0.00001806
Iteration 55/1000 | Loss: 0.00001805
Iteration 56/1000 | Loss: 0.00001805
Iteration 57/1000 | Loss: 0.00001805
Iteration 58/1000 | Loss: 0.00001805
Iteration 59/1000 | Loss: 0.00001805
Iteration 60/1000 | Loss: 0.00001804
Iteration 61/1000 | Loss: 0.00001803
Iteration 62/1000 | Loss: 0.00001803
Iteration 63/1000 | Loss: 0.00001803
Iteration 64/1000 | Loss: 0.00001803
Iteration 65/1000 | Loss: 0.00001803
Iteration 66/1000 | Loss: 0.00001803
Iteration 67/1000 | Loss: 0.00001803
Iteration 68/1000 | Loss: 0.00001803
Iteration 69/1000 | Loss: 0.00001802
Iteration 70/1000 | Loss: 0.00001802
Iteration 71/1000 | Loss: 0.00001802
Iteration 72/1000 | Loss: 0.00001802
Iteration 73/1000 | Loss: 0.00001802
Iteration 74/1000 | Loss: 0.00001802
Iteration 75/1000 | Loss: 0.00001801
Iteration 76/1000 | Loss: 0.00001801
Iteration 77/1000 | Loss: 0.00001801
Iteration 78/1000 | Loss: 0.00001801
Iteration 79/1000 | Loss: 0.00001801
Iteration 80/1000 | Loss: 0.00001801
Iteration 81/1000 | Loss: 0.00001801
Iteration 82/1000 | Loss: 0.00001801
Iteration 83/1000 | Loss: 0.00001801
Iteration 84/1000 | Loss: 0.00001801
Iteration 85/1000 | Loss: 0.00001801
Iteration 86/1000 | Loss: 0.00001801
Iteration 87/1000 | Loss: 0.00001801
Iteration 88/1000 | Loss: 0.00001801
Iteration 89/1000 | Loss: 0.00001801
Iteration 90/1000 | Loss: 0.00001801
Iteration 91/1000 | Loss: 0.00001801
Iteration 92/1000 | Loss: 0.00001800
Iteration 93/1000 | Loss: 0.00001800
Iteration 94/1000 | Loss: 0.00001800
Iteration 95/1000 | Loss: 0.00001800
Iteration 96/1000 | Loss: 0.00001800
Iteration 97/1000 | Loss: 0.00001799
Iteration 98/1000 | Loss: 0.00001799
Iteration 99/1000 | Loss: 0.00001799
Iteration 100/1000 | Loss: 0.00001799
Iteration 101/1000 | Loss: 0.00001799
Iteration 102/1000 | Loss: 0.00001799
Iteration 103/1000 | Loss: 0.00001799
Iteration 104/1000 | Loss: 0.00001799
Iteration 105/1000 | Loss: 0.00001799
Iteration 106/1000 | Loss: 0.00001799
Iteration 107/1000 | Loss: 0.00001799
Iteration 108/1000 | Loss: 0.00001799
Iteration 109/1000 | Loss: 0.00001799
Iteration 110/1000 | Loss: 0.00001799
Iteration 111/1000 | Loss: 0.00001799
Iteration 112/1000 | Loss: 0.00001799
Iteration 113/1000 | Loss: 0.00001799
Iteration 114/1000 | Loss: 0.00001799
Iteration 115/1000 | Loss: 0.00001799
Iteration 116/1000 | Loss: 0.00001799
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [1.798546509235166e-05, 1.798546509235166e-05, 1.798546509235166e-05, 1.798546509235166e-05, 1.798546509235166e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.798546509235166e-05

Optimization complete. Final v2v error: 3.627659797668457 mm

Highest mean error: 4.206221103668213 mm for frame 1

Lowest mean error: 3.2963409423828125 mm for frame 51

Saving results

Total time: 31.08008313179016
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00844163
Iteration 2/25 | Loss: 0.00080783
Iteration 3/25 | Loss: 0.00066647
Iteration 4/25 | Loss: 0.00062255
Iteration 5/25 | Loss: 0.00061437
Iteration 6/25 | Loss: 0.00061265
Iteration 7/25 | Loss: 0.00061241
Iteration 8/25 | Loss: 0.00061241
Iteration 9/25 | Loss: 0.00061241
Iteration 10/25 | Loss: 0.00061241
Iteration 11/25 | Loss: 0.00061241
Iteration 12/25 | Loss: 0.00061241
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006124108913354576, 0.0006124108913354576, 0.0006124108913354576, 0.0006124108913354576, 0.0006124108913354576]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006124108913354576

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.56098557
Iteration 2/25 | Loss: 0.00018336
Iteration 3/25 | Loss: 0.00018331
Iteration 4/25 | Loss: 0.00018331
Iteration 5/25 | Loss: 0.00018331
Iteration 6/25 | Loss: 0.00018331
Iteration 7/25 | Loss: 0.00018330
Iteration 8/25 | Loss: 0.00018330
Iteration 9/25 | Loss: 0.00018330
Iteration 10/25 | Loss: 0.00018330
Iteration 11/25 | Loss: 0.00018330
Iteration 12/25 | Loss: 0.00018330
Iteration 13/25 | Loss: 0.00018330
Iteration 14/25 | Loss: 0.00018330
Iteration 15/25 | Loss: 0.00018330
Iteration 16/25 | Loss: 0.00018330
Iteration 17/25 | Loss: 0.00018330
Iteration 18/25 | Loss: 0.00018330
Iteration 19/25 | Loss: 0.00018330
Iteration 20/25 | Loss: 0.00018330
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00018330385501030833, 0.00018330385501030833, 0.00018330385501030833, 0.00018330385501030833, 0.00018330385501030833]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00018330385501030833

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00018330
Iteration 2/1000 | Loss: 0.00002942
Iteration 3/1000 | Loss: 0.00002018
Iteration 4/1000 | Loss: 0.00001810
Iteration 5/1000 | Loss: 0.00001724
Iteration 6/1000 | Loss: 0.00001675
Iteration 7/1000 | Loss: 0.00001649
Iteration 8/1000 | Loss: 0.00001610
Iteration 9/1000 | Loss: 0.00001582
Iteration 10/1000 | Loss: 0.00001562
Iteration 11/1000 | Loss: 0.00001549
Iteration 12/1000 | Loss: 0.00001549
Iteration 13/1000 | Loss: 0.00001548
Iteration 14/1000 | Loss: 0.00001542
Iteration 15/1000 | Loss: 0.00001539
Iteration 16/1000 | Loss: 0.00001538
Iteration 17/1000 | Loss: 0.00001538
Iteration 18/1000 | Loss: 0.00001535
Iteration 19/1000 | Loss: 0.00001535
Iteration 20/1000 | Loss: 0.00001532
Iteration 21/1000 | Loss: 0.00001531
Iteration 22/1000 | Loss: 0.00001531
Iteration 23/1000 | Loss: 0.00001531
Iteration 24/1000 | Loss: 0.00001530
Iteration 25/1000 | Loss: 0.00001530
Iteration 26/1000 | Loss: 0.00001530
Iteration 27/1000 | Loss: 0.00001530
Iteration 28/1000 | Loss: 0.00001529
Iteration 29/1000 | Loss: 0.00001529
Iteration 30/1000 | Loss: 0.00001529
Iteration 31/1000 | Loss: 0.00001528
Iteration 32/1000 | Loss: 0.00001528
Iteration 33/1000 | Loss: 0.00001528
Iteration 34/1000 | Loss: 0.00001528
Iteration 35/1000 | Loss: 0.00001528
Iteration 36/1000 | Loss: 0.00001528
Iteration 37/1000 | Loss: 0.00001527
Iteration 38/1000 | Loss: 0.00001527
Iteration 39/1000 | Loss: 0.00001527
Iteration 40/1000 | Loss: 0.00001527
Iteration 41/1000 | Loss: 0.00001527
Iteration 42/1000 | Loss: 0.00001527
Iteration 43/1000 | Loss: 0.00001526
Iteration 44/1000 | Loss: 0.00001526
Iteration 45/1000 | Loss: 0.00001526
Iteration 46/1000 | Loss: 0.00001526
Iteration 47/1000 | Loss: 0.00001525
Iteration 48/1000 | Loss: 0.00001525
Iteration 49/1000 | Loss: 0.00001525
Iteration 50/1000 | Loss: 0.00001525
Iteration 51/1000 | Loss: 0.00001524
Iteration 52/1000 | Loss: 0.00001524
Iteration 53/1000 | Loss: 0.00001524
Iteration 54/1000 | Loss: 0.00001524
Iteration 55/1000 | Loss: 0.00001524
Iteration 56/1000 | Loss: 0.00001524
Iteration 57/1000 | Loss: 0.00001524
Iteration 58/1000 | Loss: 0.00001524
Iteration 59/1000 | Loss: 0.00001523
Iteration 60/1000 | Loss: 0.00001523
Iteration 61/1000 | Loss: 0.00001523
Iteration 62/1000 | Loss: 0.00001523
Iteration 63/1000 | Loss: 0.00001523
Iteration 64/1000 | Loss: 0.00001523
Iteration 65/1000 | Loss: 0.00001523
Iteration 66/1000 | Loss: 0.00001523
Iteration 67/1000 | Loss: 0.00001523
Iteration 68/1000 | Loss: 0.00001523
Iteration 69/1000 | Loss: 0.00001523
Iteration 70/1000 | Loss: 0.00001523
Iteration 71/1000 | Loss: 0.00001523
Iteration 72/1000 | Loss: 0.00001523
Iteration 73/1000 | Loss: 0.00001523
Iteration 74/1000 | Loss: 0.00001523
Iteration 75/1000 | Loss: 0.00001523
Iteration 76/1000 | Loss: 0.00001523
Iteration 77/1000 | Loss: 0.00001523
Iteration 78/1000 | Loss: 0.00001523
Iteration 79/1000 | Loss: 0.00001523
Iteration 80/1000 | Loss: 0.00001523
Iteration 81/1000 | Loss: 0.00001523
Iteration 82/1000 | Loss: 0.00001523
Iteration 83/1000 | Loss: 0.00001523
Iteration 84/1000 | Loss: 0.00001523
Iteration 85/1000 | Loss: 0.00001523
Iteration 86/1000 | Loss: 0.00001523
Iteration 87/1000 | Loss: 0.00001523
Iteration 88/1000 | Loss: 0.00001523
Iteration 89/1000 | Loss: 0.00001523
Iteration 90/1000 | Loss: 0.00001523
Iteration 91/1000 | Loss: 0.00001523
Iteration 92/1000 | Loss: 0.00001523
Iteration 93/1000 | Loss: 0.00001523
Iteration 94/1000 | Loss: 0.00001523
Iteration 95/1000 | Loss: 0.00001523
Iteration 96/1000 | Loss: 0.00001523
Iteration 97/1000 | Loss: 0.00001523
Iteration 98/1000 | Loss: 0.00001523
Iteration 99/1000 | Loss: 0.00001523
Iteration 100/1000 | Loss: 0.00001523
Iteration 101/1000 | Loss: 0.00001523
Iteration 102/1000 | Loss: 0.00001523
Iteration 103/1000 | Loss: 0.00001523
Iteration 104/1000 | Loss: 0.00001523
Iteration 105/1000 | Loss: 0.00001523
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [1.5226669347612187e-05, 1.5226669347612187e-05, 1.5226669347612187e-05, 1.5226669347612187e-05, 1.5226669347612187e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5226669347612187e-05

Optimization complete. Final v2v error: 3.300912857055664 mm

Highest mean error: 3.787412166595459 mm for frame 49

Lowest mean error: 2.8899056911468506 mm for frame 194

Saving results

Total time: 37.03923988342285
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01019908
Iteration 2/25 | Loss: 0.01019908
Iteration 3/25 | Loss: 0.01019908
Iteration 4/25 | Loss: 0.01019907
Iteration 5/25 | Loss: 0.01019907
Iteration 6/25 | Loss: 0.01019907
Iteration 7/25 | Loss: 0.01019907
Iteration 8/25 | Loss: 0.01019907
Iteration 9/25 | Loss: 0.01019906
Iteration 10/25 | Loss: 0.01019906
Iteration 11/25 | Loss: 0.01019906
Iteration 12/25 | Loss: 0.01019906
Iteration 13/25 | Loss: 0.01019906
Iteration 14/25 | Loss: 0.01019906
Iteration 15/25 | Loss: 0.01019905
Iteration 16/25 | Loss: 0.01019905
Iteration 17/25 | Loss: 0.01019905
Iteration 18/25 | Loss: 0.01019905
Iteration 19/25 | Loss: 0.01019905
Iteration 20/25 | Loss: 0.01019905
Iteration 21/25 | Loss: 0.01019904
Iteration 22/25 | Loss: 0.01019904
Iteration 23/25 | Loss: 0.01019904
Iteration 24/25 | Loss: 0.01019904
Iteration 25/25 | Loss: 0.01019904

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.69202399
Iteration 2/25 | Loss: 0.17274129
Iteration 3/25 | Loss: 0.17273532
Iteration 4/25 | Loss: 0.17273527
Iteration 5/25 | Loss: 0.17273524
Iteration 6/25 | Loss: 0.17273524
Iteration 7/25 | Loss: 0.17273524
Iteration 8/25 | Loss: 0.17273524
Iteration 9/25 | Loss: 0.17273524
Iteration 10/25 | Loss: 0.17273524
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.17273524403572083, 0.17273524403572083, 0.17273524403572083, 0.17273524403572083, 0.17273524403572083]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.17273524403572083

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17273524
Iteration 2/1000 | Loss: 0.01472171
Iteration 3/1000 | Loss: 0.01035047
Iteration 4/1000 | Loss: 0.01543364
Iteration 5/1000 | Loss: 0.00188504
Iteration 6/1000 | Loss: 0.00197865
Iteration 7/1000 | Loss: 0.00217646
Iteration 8/1000 | Loss: 0.00226710
Iteration 9/1000 | Loss: 0.00310740
Iteration 10/1000 | Loss: 0.00032958
Iteration 11/1000 | Loss: 0.00054544
Iteration 12/1000 | Loss: 0.00081243
Iteration 13/1000 | Loss: 0.00159946
Iteration 14/1000 | Loss: 0.00011426
Iteration 15/1000 | Loss: 0.00180275
Iteration 16/1000 | Loss: 0.00079070
Iteration 17/1000 | Loss: 0.00110174
Iteration 18/1000 | Loss: 0.00132274
Iteration 19/1000 | Loss: 0.00018351
Iteration 20/1000 | Loss: 0.00030784
Iteration 21/1000 | Loss: 0.00099088
Iteration 22/1000 | Loss: 0.00036112
Iteration 23/1000 | Loss: 0.00031615
Iteration 24/1000 | Loss: 0.00100882
Iteration 25/1000 | Loss: 0.00034054
Iteration 26/1000 | Loss: 0.00009247
Iteration 27/1000 | Loss: 0.00018154
Iteration 28/1000 | Loss: 0.00008545
Iteration 29/1000 | Loss: 0.00021503
Iteration 30/1000 | Loss: 0.00142991
Iteration 31/1000 | Loss: 0.00043093
Iteration 32/1000 | Loss: 0.00110873
Iteration 33/1000 | Loss: 0.00049965
Iteration 34/1000 | Loss: 0.00087483
Iteration 35/1000 | Loss: 0.00129780
Iteration 36/1000 | Loss: 0.00174782
Iteration 37/1000 | Loss: 0.00024463
Iteration 38/1000 | Loss: 0.00005333
Iteration 39/1000 | Loss: 0.00006336
Iteration 40/1000 | Loss: 0.00004368
Iteration 41/1000 | Loss: 0.00003723
Iteration 42/1000 | Loss: 0.00081539
Iteration 43/1000 | Loss: 0.00188059
Iteration 44/1000 | Loss: 0.00023569
Iteration 45/1000 | Loss: 0.00184002
Iteration 46/1000 | Loss: 0.00030610
Iteration 47/1000 | Loss: 0.00004834
Iteration 48/1000 | Loss: 0.00002604
Iteration 49/1000 | Loss: 0.00012738
Iteration 50/1000 | Loss: 0.00002391
Iteration 51/1000 | Loss: 0.00008747
Iteration 52/1000 | Loss: 0.00002765
Iteration 53/1000 | Loss: 0.00002862
Iteration 54/1000 | Loss: 0.00020420
Iteration 55/1000 | Loss: 0.00045781
Iteration 56/1000 | Loss: 0.00002791
Iteration 57/1000 | Loss: 0.00018331
Iteration 58/1000 | Loss: 0.00002199
Iteration 59/1000 | Loss: 0.00002155
Iteration 60/1000 | Loss: 0.00014655
Iteration 61/1000 | Loss: 0.00002117
Iteration 62/1000 | Loss: 0.00008461
Iteration 63/1000 | Loss: 0.00002873
Iteration 64/1000 | Loss: 0.00004466
Iteration 65/1000 | Loss: 0.00002083
Iteration 66/1000 | Loss: 0.00002071
Iteration 67/1000 | Loss: 0.00002057
Iteration 68/1000 | Loss: 0.00002054
Iteration 69/1000 | Loss: 0.00002046
Iteration 70/1000 | Loss: 0.00002044
Iteration 71/1000 | Loss: 0.00002044
Iteration 72/1000 | Loss: 0.00002042
Iteration 73/1000 | Loss: 0.00002042
Iteration 74/1000 | Loss: 0.00021064
Iteration 75/1000 | Loss: 0.00017857
Iteration 76/1000 | Loss: 0.00005038
Iteration 77/1000 | Loss: 0.00003102
Iteration 78/1000 | Loss: 0.00002051
Iteration 79/1000 | Loss: 0.00012211
Iteration 80/1000 | Loss: 0.00002063
Iteration 81/1000 | Loss: 0.00002040
Iteration 82/1000 | Loss: 0.00002040
Iteration 83/1000 | Loss: 0.00002040
Iteration 84/1000 | Loss: 0.00002035
Iteration 85/1000 | Loss: 0.00002035
Iteration 86/1000 | Loss: 0.00002035
Iteration 87/1000 | Loss: 0.00002035
Iteration 88/1000 | Loss: 0.00002035
Iteration 89/1000 | Loss: 0.00002035
Iteration 90/1000 | Loss: 0.00002035
Iteration 91/1000 | Loss: 0.00002035
Iteration 92/1000 | Loss: 0.00002035
Iteration 93/1000 | Loss: 0.00002034
Iteration 94/1000 | Loss: 0.00002034
Iteration 95/1000 | Loss: 0.00002034
Iteration 96/1000 | Loss: 0.00002033
Iteration 97/1000 | Loss: 0.00002033
Iteration 98/1000 | Loss: 0.00002033
Iteration 99/1000 | Loss: 0.00002033
Iteration 100/1000 | Loss: 0.00002033
Iteration 101/1000 | Loss: 0.00002033
Iteration 102/1000 | Loss: 0.00002033
Iteration 103/1000 | Loss: 0.00002033
Iteration 104/1000 | Loss: 0.00002033
Iteration 105/1000 | Loss: 0.00002032
Iteration 106/1000 | Loss: 0.00002032
Iteration 107/1000 | Loss: 0.00002032
Iteration 108/1000 | Loss: 0.00002032
Iteration 109/1000 | Loss: 0.00002032
Iteration 110/1000 | Loss: 0.00002032
Iteration 111/1000 | Loss: 0.00002032
Iteration 112/1000 | Loss: 0.00002032
Iteration 113/1000 | Loss: 0.00002032
Iteration 114/1000 | Loss: 0.00002032
Iteration 115/1000 | Loss: 0.00002032
Iteration 116/1000 | Loss: 0.00002032
Iteration 117/1000 | Loss: 0.00002032
Iteration 118/1000 | Loss: 0.00002032
Iteration 119/1000 | Loss: 0.00002032
Iteration 120/1000 | Loss: 0.00002032
Iteration 121/1000 | Loss: 0.00002031
Iteration 122/1000 | Loss: 0.00002031
Iteration 123/1000 | Loss: 0.00002031
Iteration 124/1000 | Loss: 0.00002031
Iteration 125/1000 | Loss: 0.00002031
Iteration 126/1000 | Loss: 0.00002031
Iteration 127/1000 | Loss: 0.00002031
Iteration 128/1000 | Loss: 0.00002031
Iteration 129/1000 | Loss: 0.00002031
Iteration 130/1000 | Loss: 0.00002031
Iteration 131/1000 | Loss: 0.00002030
Iteration 132/1000 | Loss: 0.00002030
Iteration 133/1000 | Loss: 0.00002030
Iteration 134/1000 | Loss: 0.00002030
Iteration 135/1000 | Loss: 0.00002030
Iteration 136/1000 | Loss: 0.00002030
Iteration 137/1000 | Loss: 0.00002030
Iteration 138/1000 | Loss: 0.00002030
Iteration 139/1000 | Loss: 0.00002030
Iteration 140/1000 | Loss: 0.00002030
Iteration 141/1000 | Loss: 0.00002029
Iteration 142/1000 | Loss: 0.00002029
Iteration 143/1000 | Loss: 0.00002029
Iteration 144/1000 | Loss: 0.00002029
Iteration 145/1000 | Loss: 0.00002029
Iteration 146/1000 | Loss: 0.00002029
Iteration 147/1000 | Loss: 0.00002029
Iteration 148/1000 | Loss: 0.00002029
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [2.029356073762756e-05, 2.029356073762756e-05, 2.029356073762756e-05, 2.029356073762756e-05, 2.029356073762756e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.029356073762756e-05

Optimization complete. Final v2v error: 3.768723249435425 mm

Highest mean error: 4.239276885986328 mm for frame 73

Lowest mean error: 3.53961181640625 mm for frame 3

Saving results

Total time: 133.63949990272522
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00793681
Iteration 2/25 | Loss: 0.00110177
Iteration 3/25 | Loss: 0.00077062
Iteration 4/25 | Loss: 0.00069841
Iteration 5/25 | Loss: 0.00067076
Iteration 6/25 | Loss: 0.00065177
Iteration 7/25 | Loss: 0.00066123
Iteration 8/25 | Loss: 0.00066161
Iteration 9/25 | Loss: 0.00065372
Iteration 10/25 | Loss: 0.00064578
Iteration 11/25 | Loss: 0.00063592
Iteration 12/25 | Loss: 0.00063040
Iteration 13/25 | Loss: 0.00062785
Iteration 14/25 | Loss: 0.00062646
Iteration 15/25 | Loss: 0.00062602
Iteration 16/25 | Loss: 0.00062587
Iteration 17/25 | Loss: 0.00062581
Iteration 18/25 | Loss: 0.00062581
Iteration 19/25 | Loss: 0.00062580
Iteration 20/25 | Loss: 0.00062580
Iteration 21/25 | Loss: 0.00062580
Iteration 22/25 | Loss: 0.00062580
Iteration 23/25 | Loss: 0.00062580
Iteration 24/25 | Loss: 0.00062580
Iteration 25/25 | Loss: 0.00062580

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.89169478
Iteration 2/25 | Loss: 0.00020647
Iteration 3/25 | Loss: 0.00020646
Iteration 4/25 | Loss: 0.00020646
Iteration 5/25 | Loss: 0.00020646
Iteration 6/25 | Loss: 0.00020646
Iteration 7/25 | Loss: 0.00020646
Iteration 8/25 | Loss: 0.00020646
Iteration 9/25 | Loss: 0.00020646
Iteration 10/25 | Loss: 0.00020646
Iteration 11/25 | Loss: 0.00020646
Iteration 12/25 | Loss: 0.00020646
Iteration 13/25 | Loss: 0.00020646
Iteration 14/25 | Loss: 0.00020646
Iteration 15/25 | Loss: 0.00020646
Iteration 16/25 | Loss: 0.00020646
Iteration 17/25 | Loss: 0.00020646
Iteration 18/25 | Loss: 0.00020646
Iteration 19/25 | Loss: 0.00020646
Iteration 20/25 | Loss: 0.00020646
Iteration 21/25 | Loss: 0.00020646
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00020645689801312983, 0.00020645689801312983, 0.00020645689801312983, 0.00020645689801312983, 0.00020645689801312983]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00020645689801312983

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00020646
Iteration 2/1000 | Loss: 0.00003274
Iteration 3/1000 | Loss: 0.00002554
Iteration 4/1000 | Loss: 0.00002273
Iteration 5/1000 | Loss: 0.00002153
Iteration 6/1000 | Loss: 0.00002074
Iteration 7/1000 | Loss: 0.00002025
Iteration 8/1000 | Loss: 0.00064201
Iteration 9/1000 | Loss: 0.00002168
Iteration 10/1000 | Loss: 0.00001938
Iteration 11/1000 | Loss: 0.00001831
Iteration 12/1000 | Loss: 0.00001764
Iteration 13/1000 | Loss: 0.00001724
Iteration 14/1000 | Loss: 0.00001706
Iteration 15/1000 | Loss: 0.00001703
Iteration 16/1000 | Loss: 0.00001694
Iteration 17/1000 | Loss: 0.00001693
Iteration 18/1000 | Loss: 0.00001692
Iteration 19/1000 | Loss: 0.00001687
Iteration 20/1000 | Loss: 0.00001684
Iteration 21/1000 | Loss: 0.00001682
Iteration 22/1000 | Loss: 0.00001682
Iteration 23/1000 | Loss: 0.00001681
Iteration 24/1000 | Loss: 0.00001681
Iteration 25/1000 | Loss: 0.00001680
Iteration 26/1000 | Loss: 0.00001680
Iteration 27/1000 | Loss: 0.00001668
Iteration 28/1000 | Loss: 0.00001667
Iteration 29/1000 | Loss: 0.00001667
Iteration 30/1000 | Loss: 0.00001665
Iteration 31/1000 | Loss: 0.00001664
Iteration 32/1000 | Loss: 0.00001662
Iteration 33/1000 | Loss: 0.00001661
Iteration 34/1000 | Loss: 0.00001661
Iteration 35/1000 | Loss: 0.00001660
Iteration 36/1000 | Loss: 0.00001660
Iteration 37/1000 | Loss: 0.00001660
Iteration 38/1000 | Loss: 0.00001660
Iteration 39/1000 | Loss: 0.00001659
Iteration 40/1000 | Loss: 0.00001659
Iteration 41/1000 | Loss: 0.00001658
Iteration 42/1000 | Loss: 0.00001655
Iteration 43/1000 | Loss: 0.00001655
Iteration 44/1000 | Loss: 0.00001653
Iteration 45/1000 | Loss: 0.00001653
Iteration 46/1000 | Loss: 0.00001652
Iteration 47/1000 | Loss: 0.00001652
Iteration 48/1000 | Loss: 0.00001651
Iteration 49/1000 | Loss: 0.00001651
Iteration 50/1000 | Loss: 0.00001651
Iteration 51/1000 | Loss: 0.00001650
Iteration 52/1000 | Loss: 0.00001650
Iteration 53/1000 | Loss: 0.00001650
Iteration 54/1000 | Loss: 0.00001650
Iteration 55/1000 | Loss: 0.00001649
Iteration 56/1000 | Loss: 0.00001649
Iteration 57/1000 | Loss: 0.00001649
Iteration 58/1000 | Loss: 0.00001648
Iteration 59/1000 | Loss: 0.00001648
Iteration 60/1000 | Loss: 0.00001648
Iteration 61/1000 | Loss: 0.00001647
Iteration 62/1000 | Loss: 0.00001647
Iteration 63/1000 | Loss: 0.00001647
Iteration 64/1000 | Loss: 0.00001646
Iteration 65/1000 | Loss: 0.00001646
Iteration 66/1000 | Loss: 0.00001646
Iteration 67/1000 | Loss: 0.00001646
Iteration 68/1000 | Loss: 0.00001646
Iteration 69/1000 | Loss: 0.00001646
Iteration 70/1000 | Loss: 0.00001646
Iteration 71/1000 | Loss: 0.00001645
Iteration 72/1000 | Loss: 0.00001645
Iteration 73/1000 | Loss: 0.00001645
Iteration 74/1000 | Loss: 0.00001645
Iteration 75/1000 | Loss: 0.00001645
Iteration 76/1000 | Loss: 0.00001645
Iteration 77/1000 | Loss: 0.00001644
Iteration 78/1000 | Loss: 0.00001644
Iteration 79/1000 | Loss: 0.00001644
Iteration 80/1000 | Loss: 0.00001644
Iteration 81/1000 | Loss: 0.00001644
Iteration 82/1000 | Loss: 0.00001644
Iteration 83/1000 | Loss: 0.00001644
Iteration 84/1000 | Loss: 0.00001644
Iteration 85/1000 | Loss: 0.00001644
Iteration 86/1000 | Loss: 0.00001644
Iteration 87/1000 | Loss: 0.00001644
Iteration 88/1000 | Loss: 0.00001644
Iteration 89/1000 | Loss: 0.00001644
Iteration 90/1000 | Loss: 0.00001644
Iteration 91/1000 | Loss: 0.00001644
Iteration 92/1000 | Loss: 0.00001644
Iteration 93/1000 | Loss: 0.00001644
Iteration 94/1000 | Loss: 0.00001644
Iteration 95/1000 | Loss: 0.00001644
Iteration 96/1000 | Loss: 0.00001644
Iteration 97/1000 | Loss: 0.00001644
Iteration 98/1000 | Loss: 0.00001644
Iteration 99/1000 | Loss: 0.00001644
Iteration 100/1000 | Loss: 0.00001644
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [1.6441008483525366e-05, 1.6441008483525366e-05, 1.6441008483525366e-05, 1.6441008483525366e-05, 1.6441008483525366e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6441008483525366e-05

Optimization complete. Final v2v error: 3.369699716567993 mm

Highest mean error: 4.210498809814453 mm for frame 41

Lowest mean error: 2.8704094886779785 mm for frame 75

Saving results

Total time: 60.624969482421875
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00422920
Iteration 2/25 | Loss: 0.00080627
Iteration 3/25 | Loss: 0.00064423
Iteration 4/25 | Loss: 0.00061283
Iteration 5/25 | Loss: 0.00060301
Iteration 6/25 | Loss: 0.00060071
Iteration 7/25 | Loss: 0.00060004
Iteration 8/25 | Loss: 0.00060004
Iteration 9/25 | Loss: 0.00060004
Iteration 10/25 | Loss: 0.00060004
Iteration 11/25 | Loss: 0.00060004
Iteration 12/25 | Loss: 0.00060004
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006000423454679549, 0.0006000423454679549, 0.0006000423454679549, 0.0006000423454679549, 0.0006000423454679549]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006000423454679549

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43910754
Iteration 2/25 | Loss: 0.00020816
Iteration 3/25 | Loss: 0.00020815
Iteration 4/25 | Loss: 0.00020815
Iteration 5/25 | Loss: 0.00020815
Iteration 6/25 | Loss: 0.00020815
Iteration 7/25 | Loss: 0.00020815
Iteration 8/25 | Loss: 0.00020815
Iteration 9/25 | Loss: 0.00020815
Iteration 10/25 | Loss: 0.00020815
Iteration 11/25 | Loss: 0.00020815
Iteration 12/25 | Loss: 0.00020815
Iteration 13/25 | Loss: 0.00020815
Iteration 14/25 | Loss: 0.00020815
Iteration 15/25 | Loss: 0.00020815
Iteration 16/25 | Loss: 0.00020815
Iteration 17/25 | Loss: 0.00020815
Iteration 18/25 | Loss: 0.00020815
Iteration 19/25 | Loss: 0.00020815
Iteration 20/25 | Loss: 0.00020815
Iteration 21/25 | Loss: 0.00020815
Iteration 22/25 | Loss: 0.00020815
Iteration 23/25 | Loss: 0.00020815
Iteration 24/25 | Loss: 0.00020815
Iteration 25/25 | Loss: 0.00020815

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00020815
Iteration 2/1000 | Loss: 0.00002664
Iteration 3/1000 | Loss: 0.00001702
Iteration 4/1000 | Loss: 0.00001562
Iteration 5/1000 | Loss: 0.00001477
Iteration 6/1000 | Loss: 0.00001431
Iteration 7/1000 | Loss: 0.00001397
Iteration 8/1000 | Loss: 0.00001372
Iteration 9/1000 | Loss: 0.00001351
Iteration 10/1000 | Loss: 0.00001336
Iteration 11/1000 | Loss: 0.00001335
Iteration 12/1000 | Loss: 0.00001334
Iteration 13/1000 | Loss: 0.00001334
Iteration 14/1000 | Loss: 0.00001333
Iteration 15/1000 | Loss: 0.00001327
Iteration 16/1000 | Loss: 0.00001324
Iteration 17/1000 | Loss: 0.00001324
Iteration 18/1000 | Loss: 0.00001323
Iteration 19/1000 | Loss: 0.00001323
Iteration 20/1000 | Loss: 0.00001322
Iteration 21/1000 | Loss: 0.00001318
Iteration 22/1000 | Loss: 0.00001318
Iteration 23/1000 | Loss: 0.00001316
Iteration 24/1000 | Loss: 0.00001316
Iteration 25/1000 | Loss: 0.00001315
Iteration 26/1000 | Loss: 0.00001315
Iteration 27/1000 | Loss: 0.00001315
Iteration 28/1000 | Loss: 0.00001315
Iteration 29/1000 | Loss: 0.00001315
Iteration 30/1000 | Loss: 0.00001314
Iteration 31/1000 | Loss: 0.00001314
Iteration 32/1000 | Loss: 0.00001314
Iteration 33/1000 | Loss: 0.00001314
Iteration 34/1000 | Loss: 0.00001313
Iteration 35/1000 | Loss: 0.00001313
Iteration 36/1000 | Loss: 0.00001313
Iteration 37/1000 | Loss: 0.00001313
Iteration 38/1000 | Loss: 0.00001313
Iteration 39/1000 | Loss: 0.00001312
Iteration 40/1000 | Loss: 0.00001312
Iteration 41/1000 | Loss: 0.00001312
Iteration 42/1000 | Loss: 0.00001312
Iteration 43/1000 | Loss: 0.00001312
Iteration 44/1000 | Loss: 0.00001312
Iteration 45/1000 | Loss: 0.00001312
Iteration 46/1000 | Loss: 0.00001312
Iteration 47/1000 | Loss: 0.00001312
Iteration 48/1000 | Loss: 0.00001311
Iteration 49/1000 | Loss: 0.00001311
Iteration 50/1000 | Loss: 0.00001311
Iteration 51/1000 | Loss: 0.00001311
Iteration 52/1000 | Loss: 0.00001310
Iteration 53/1000 | Loss: 0.00001310
Iteration 54/1000 | Loss: 0.00001310
Iteration 55/1000 | Loss: 0.00001310
Iteration 56/1000 | Loss: 0.00001309
Iteration 57/1000 | Loss: 0.00001309
Iteration 58/1000 | Loss: 0.00001309
Iteration 59/1000 | Loss: 0.00001309
Iteration 60/1000 | Loss: 0.00001309
Iteration 61/1000 | Loss: 0.00001309
Iteration 62/1000 | Loss: 0.00001309
Iteration 63/1000 | Loss: 0.00001309
Iteration 64/1000 | Loss: 0.00001309
Iteration 65/1000 | Loss: 0.00001309
Iteration 66/1000 | Loss: 0.00001309
Iteration 67/1000 | Loss: 0.00001309
Iteration 68/1000 | Loss: 0.00001309
Iteration 69/1000 | Loss: 0.00001309
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 69. Stopping optimization.
Last 5 losses: [1.308934861299349e-05, 1.308934861299349e-05, 1.308934861299349e-05, 1.308934861299349e-05, 1.308934861299349e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.308934861299349e-05

Optimization complete. Final v2v error: 3.0235679149627686 mm

Highest mean error: 3.5907394886016846 mm for frame 61

Lowest mean error: 2.586240768432617 mm for frame 118

Saving results

Total time: 34.98181343078613
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00811606
Iteration 2/25 | Loss: 0.00142500
Iteration 3/25 | Loss: 0.00091158
Iteration 4/25 | Loss: 0.00079824
Iteration 5/25 | Loss: 0.00077654
Iteration 6/25 | Loss: 0.00077010
Iteration 7/25 | Loss: 0.00076792
Iteration 8/25 | Loss: 0.00077212
Iteration 9/25 | Loss: 0.00076582
Iteration 10/25 | Loss: 0.00075819
Iteration 11/25 | Loss: 0.00074961
Iteration 12/25 | Loss: 0.00074289
Iteration 13/25 | Loss: 0.00074094
Iteration 14/25 | Loss: 0.00073995
Iteration 15/25 | Loss: 0.00073951
Iteration 16/25 | Loss: 0.00073933
Iteration 17/25 | Loss: 0.00073927
Iteration 18/25 | Loss: 0.00073926
Iteration 19/25 | Loss: 0.00073926
Iteration 20/25 | Loss: 0.00073926
Iteration 21/25 | Loss: 0.00073926
Iteration 22/25 | Loss: 0.00073926
Iteration 23/25 | Loss: 0.00073926
Iteration 24/25 | Loss: 0.00073926
Iteration 25/25 | Loss: 0.00073926

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.88544464
Iteration 2/25 | Loss: 0.00031981
Iteration 3/25 | Loss: 0.00031977
Iteration 4/25 | Loss: 0.00031977
Iteration 5/25 | Loss: 0.00031977
Iteration 6/25 | Loss: 0.00031977
Iteration 7/25 | Loss: 0.00031977
Iteration 8/25 | Loss: 0.00031977
Iteration 9/25 | Loss: 0.00031977
Iteration 10/25 | Loss: 0.00031977
Iteration 11/25 | Loss: 0.00031977
Iteration 12/25 | Loss: 0.00031977
Iteration 13/25 | Loss: 0.00031977
Iteration 14/25 | Loss: 0.00031977
Iteration 15/25 | Loss: 0.00031977
Iteration 16/25 | Loss: 0.00031977
Iteration 17/25 | Loss: 0.00031977
Iteration 18/25 | Loss: 0.00031977
Iteration 19/25 | Loss: 0.00031977
Iteration 20/25 | Loss: 0.00031977
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00031976992613635957, 0.00031976992613635957, 0.00031976992613635957, 0.00031976992613635957, 0.00031976992613635957]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00031976992613635957

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031977
Iteration 2/1000 | Loss: 0.00004625
Iteration 3/1000 | Loss: 0.00003797
Iteration 4/1000 | Loss: 0.00003458
Iteration 5/1000 | Loss: 0.00003302
Iteration 6/1000 | Loss: 0.00003207
Iteration 7/1000 | Loss: 0.00034190
Iteration 8/1000 | Loss: 0.00040004
Iteration 9/1000 | Loss: 0.00003912
Iteration 10/1000 | Loss: 0.00003085
Iteration 11/1000 | Loss: 0.00002812
Iteration 12/1000 | Loss: 0.00002697
Iteration 13/1000 | Loss: 0.00002602
Iteration 14/1000 | Loss: 0.00002553
Iteration 15/1000 | Loss: 0.00002527
Iteration 16/1000 | Loss: 0.00002506
Iteration 17/1000 | Loss: 0.00002490
Iteration 18/1000 | Loss: 0.00002485
Iteration 19/1000 | Loss: 0.00002485
Iteration 20/1000 | Loss: 0.00002471
Iteration 21/1000 | Loss: 0.00002462
Iteration 22/1000 | Loss: 0.00002462
Iteration 23/1000 | Loss: 0.00002461
Iteration 24/1000 | Loss: 0.00002461
Iteration 25/1000 | Loss: 0.00002460
Iteration 26/1000 | Loss: 0.00002459
Iteration 27/1000 | Loss: 0.00002459
Iteration 28/1000 | Loss: 0.00002456
Iteration 29/1000 | Loss: 0.00002450
Iteration 30/1000 | Loss: 0.00002450
Iteration 31/1000 | Loss: 0.00002448
Iteration 32/1000 | Loss: 0.00002448
Iteration 33/1000 | Loss: 0.00002448
Iteration 34/1000 | Loss: 0.00002448
Iteration 35/1000 | Loss: 0.00002448
Iteration 36/1000 | Loss: 0.00002448
Iteration 37/1000 | Loss: 0.00002448
Iteration 38/1000 | Loss: 0.00002448
Iteration 39/1000 | Loss: 0.00002448
Iteration 40/1000 | Loss: 0.00002447
Iteration 41/1000 | Loss: 0.00002447
Iteration 42/1000 | Loss: 0.00002447
Iteration 43/1000 | Loss: 0.00002447
Iteration 44/1000 | Loss: 0.00002447
Iteration 45/1000 | Loss: 0.00002447
Iteration 46/1000 | Loss: 0.00002447
Iteration 47/1000 | Loss: 0.00002446
Iteration 48/1000 | Loss: 0.00002446
Iteration 49/1000 | Loss: 0.00002445
Iteration 50/1000 | Loss: 0.00002445
Iteration 51/1000 | Loss: 0.00002445
Iteration 52/1000 | Loss: 0.00002445
Iteration 53/1000 | Loss: 0.00002445
Iteration 54/1000 | Loss: 0.00002445
Iteration 55/1000 | Loss: 0.00002444
Iteration 56/1000 | Loss: 0.00002444
Iteration 57/1000 | Loss: 0.00002444
Iteration 58/1000 | Loss: 0.00002444
Iteration 59/1000 | Loss: 0.00002444
Iteration 60/1000 | Loss: 0.00002444
Iteration 61/1000 | Loss: 0.00002444
Iteration 62/1000 | Loss: 0.00002443
Iteration 63/1000 | Loss: 0.00002443
Iteration 64/1000 | Loss: 0.00002443
Iteration 65/1000 | Loss: 0.00002443
Iteration 66/1000 | Loss: 0.00002442
Iteration 67/1000 | Loss: 0.00002442
Iteration 68/1000 | Loss: 0.00002442
Iteration 69/1000 | Loss: 0.00002442
Iteration 70/1000 | Loss: 0.00002442
Iteration 71/1000 | Loss: 0.00002442
Iteration 72/1000 | Loss: 0.00002442
Iteration 73/1000 | Loss: 0.00002442
Iteration 74/1000 | Loss: 0.00002442
Iteration 75/1000 | Loss: 0.00002442
Iteration 76/1000 | Loss: 0.00002442
Iteration 77/1000 | Loss: 0.00002441
Iteration 78/1000 | Loss: 0.00002441
Iteration 79/1000 | Loss: 0.00002441
Iteration 80/1000 | Loss: 0.00002441
Iteration 81/1000 | Loss: 0.00002441
Iteration 82/1000 | Loss: 0.00002441
Iteration 83/1000 | Loss: 0.00002441
Iteration 84/1000 | Loss: 0.00002441
Iteration 85/1000 | Loss: 0.00002440
Iteration 86/1000 | Loss: 0.00002440
Iteration 87/1000 | Loss: 0.00002440
Iteration 88/1000 | Loss: 0.00002440
Iteration 89/1000 | Loss: 0.00002440
Iteration 90/1000 | Loss: 0.00002440
Iteration 91/1000 | Loss: 0.00002440
Iteration 92/1000 | Loss: 0.00002440
Iteration 93/1000 | Loss: 0.00002440
Iteration 94/1000 | Loss: 0.00002440
Iteration 95/1000 | Loss: 0.00002440
Iteration 96/1000 | Loss: 0.00002440
Iteration 97/1000 | Loss: 0.00002440
Iteration 98/1000 | Loss: 0.00002439
Iteration 99/1000 | Loss: 0.00002439
Iteration 100/1000 | Loss: 0.00002439
Iteration 101/1000 | Loss: 0.00002439
Iteration 102/1000 | Loss: 0.00002439
Iteration 103/1000 | Loss: 0.00002439
Iteration 104/1000 | Loss: 0.00002439
Iteration 105/1000 | Loss: 0.00002439
Iteration 106/1000 | Loss: 0.00002439
Iteration 107/1000 | Loss: 0.00002439
Iteration 108/1000 | Loss: 0.00002439
Iteration 109/1000 | Loss: 0.00002439
Iteration 110/1000 | Loss: 0.00002439
Iteration 111/1000 | Loss: 0.00002438
Iteration 112/1000 | Loss: 0.00002438
Iteration 113/1000 | Loss: 0.00002438
Iteration 114/1000 | Loss: 0.00002438
Iteration 115/1000 | Loss: 0.00002438
Iteration 116/1000 | Loss: 0.00002438
Iteration 117/1000 | Loss: 0.00002438
Iteration 118/1000 | Loss: 0.00002437
Iteration 119/1000 | Loss: 0.00002437
Iteration 120/1000 | Loss: 0.00002437
Iteration 121/1000 | Loss: 0.00002437
Iteration 122/1000 | Loss: 0.00002437
Iteration 123/1000 | Loss: 0.00002437
Iteration 124/1000 | Loss: 0.00002437
Iteration 125/1000 | Loss: 0.00002437
Iteration 126/1000 | Loss: 0.00002437
Iteration 127/1000 | Loss: 0.00002437
Iteration 128/1000 | Loss: 0.00002437
Iteration 129/1000 | Loss: 0.00002437
Iteration 130/1000 | Loss: 0.00002437
Iteration 131/1000 | Loss: 0.00002437
Iteration 132/1000 | Loss: 0.00002437
Iteration 133/1000 | Loss: 0.00002437
Iteration 134/1000 | Loss: 0.00002436
Iteration 135/1000 | Loss: 0.00002436
Iteration 136/1000 | Loss: 0.00002436
Iteration 137/1000 | Loss: 0.00002436
Iteration 138/1000 | Loss: 0.00002436
Iteration 139/1000 | Loss: 0.00002436
Iteration 140/1000 | Loss: 0.00002436
Iteration 141/1000 | Loss: 0.00002436
Iteration 142/1000 | Loss: 0.00002436
Iteration 143/1000 | Loss: 0.00002436
Iteration 144/1000 | Loss: 0.00002436
Iteration 145/1000 | Loss: 0.00002436
Iteration 146/1000 | Loss: 0.00002436
Iteration 147/1000 | Loss: 0.00002436
Iteration 148/1000 | Loss: 0.00002436
Iteration 149/1000 | Loss: 0.00002436
Iteration 150/1000 | Loss: 0.00002436
Iteration 151/1000 | Loss: 0.00002436
Iteration 152/1000 | Loss: 0.00002436
Iteration 153/1000 | Loss: 0.00002436
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [2.4358889277209528e-05, 2.4358889277209528e-05, 2.4358889277209528e-05, 2.4358889277209528e-05, 2.4358889277209528e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4358889277209528e-05

Optimization complete. Final v2v error: 4.0320143699646 mm

Highest mean error: 4.612419605255127 mm for frame 68

Lowest mean error: 3.5263919830322266 mm for frame 99

Saving results

Total time: 75.05564665794373
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00617774
Iteration 2/25 | Loss: 0.00130143
Iteration 3/25 | Loss: 0.00100864
Iteration 4/25 | Loss: 0.00091340
Iteration 5/25 | Loss: 0.00089432
Iteration 6/25 | Loss: 0.00090164
Iteration 7/25 | Loss: 0.00089936
Iteration 8/25 | Loss: 0.00084599
Iteration 9/25 | Loss: 0.00080586
Iteration 10/25 | Loss: 0.00078122
Iteration 11/25 | Loss: 0.00077265
Iteration 12/25 | Loss: 0.00076932
Iteration 13/25 | Loss: 0.00076838
Iteration 14/25 | Loss: 0.00076815
Iteration 15/25 | Loss: 0.00076809
Iteration 16/25 | Loss: 0.00076809
Iteration 17/25 | Loss: 0.00076809
Iteration 18/25 | Loss: 0.00076809
Iteration 19/25 | Loss: 0.00076809
Iteration 20/25 | Loss: 0.00076809
Iteration 21/25 | Loss: 0.00076809
Iteration 22/25 | Loss: 0.00076809
Iteration 23/25 | Loss: 0.00076808
Iteration 24/25 | Loss: 0.00076808
Iteration 25/25 | Loss: 0.00076808

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33517075
Iteration 2/25 | Loss: 0.00096946
Iteration 3/25 | Loss: 0.00096938
Iteration 4/25 | Loss: 0.00096938
Iteration 5/25 | Loss: 0.00096938
Iteration 6/25 | Loss: 0.00096938
Iteration 7/25 | Loss: 0.00096938
Iteration 8/25 | Loss: 0.00096938
Iteration 9/25 | Loss: 0.00096938
Iteration 10/25 | Loss: 0.00096938
Iteration 11/25 | Loss: 0.00096938
Iteration 12/25 | Loss: 0.00096938
Iteration 13/25 | Loss: 0.00096938
Iteration 14/25 | Loss: 0.00096938
Iteration 15/25 | Loss: 0.00096938
Iteration 16/25 | Loss: 0.00096938
Iteration 17/25 | Loss: 0.00096938
Iteration 18/25 | Loss: 0.00096938
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009693751344457269, 0.0009693751344457269, 0.0009693751344457269, 0.0009693751344457269, 0.0009693751344457269]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009693751344457269

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096938
Iteration 2/1000 | Loss: 0.00017070
Iteration 3/1000 | Loss: 0.00007195
Iteration 4/1000 | Loss: 0.00004836
Iteration 5/1000 | Loss: 0.00003619
Iteration 6/1000 | Loss: 0.00003101
Iteration 7/1000 | Loss: 0.00002957
Iteration 8/1000 | Loss: 0.00002840
Iteration 9/1000 | Loss: 0.00002741
Iteration 10/1000 | Loss: 0.00002686
Iteration 11/1000 | Loss: 0.00002641
Iteration 12/1000 | Loss: 0.00002614
Iteration 13/1000 | Loss: 0.00002588
Iteration 14/1000 | Loss: 0.00002568
Iteration 15/1000 | Loss: 0.00002548
Iteration 16/1000 | Loss: 0.00002541
Iteration 17/1000 | Loss: 0.00002536
Iteration 18/1000 | Loss: 0.00002536
Iteration 19/1000 | Loss: 0.00002534
Iteration 20/1000 | Loss: 0.00002529
Iteration 21/1000 | Loss: 0.00002522
Iteration 22/1000 | Loss: 0.00002519
Iteration 23/1000 | Loss: 0.00002518
Iteration 24/1000 | Loss: 0.00002518
Iteration 25/1000 | Loss: 0.00002517
Iteration 26/1000 | Loss: 0.00002517
Iteration 27/1000 | Loss: 0.00002517
Iteration 28/1000 | Loss: 0.00002516
Iteration 29/1000 | Loss: 0.00002515
Iteration 30/1000 | Loss: 0.00002515
Iteration 31/1000 | Loss: 0.00002511
Iteration 32/1000 | Loss: 0.00002510
Iteration 33/1000 | Loss: 0.00002509
Iteration 34/1000 | Loss: 0.00002509
Iteration 35/1000 | Loss: 0.00002509
Iteration 36/1000 | Loss: 0.00002508
Iteration 37/1000 | Loss: 0.00002508
Iteration 38/1000 | Loss: 0.00002507
Iteration 39/1000 | Loss: 0.00002507
Iteration 40/1000 | Loss: 0.00002507
Iteration 41/1000 | Loss: 0.00002505
Iteration 42/1000 | Loss: 0.00002505
Iteration 43/1000 | Loss: 0.00002505
Iteration 44/1000 | Loss: 0.00002502
Iteration 45/1000 | Loss: 0.00002502
Iteration 46/1000 | Loss: 0.00002500
Iteration 47/1000 | Loss: 0.00002499
Iteration 48/1000 | Loss: 0.00002499
Iteration 49/1000 | Loss: 0.00002499
Iteration 50/1000 | Loss: 0.00002498
Iteration 51/1000 | Loss: 0.00002498
Iteration 52/1000 | Loss: 0.00002498
Iteration 53/1000 | Loss: 0.00002498
Iteration 54/1000 | Loss: 0.00002497
Iteration 55/1000 | Loss: 0.00002497
Iteration 56/1000 | Loss: 0.00002497
Iteration 57/1000 | Loss: 0.00002497
Iteration 58/1000 | Loss: 0.00002497
Iteration 59/1000 | Loss: 0.00002497
Iteration 60/1000 | Loss: 0.00002497
Iteration 61/1000 | Loss: 0.00002497
Iteration 62/1000 | Loss: 0.00002496
Iteration 63/1000 | Loss: 0.00002495
Iteration 64/1000 | Loss: 0.00002495
Iteration 65/1000 | Loss: 0.00002494
Iteration 66/1000 | Loss: 0.00002494
Iteration 67/1000 | Loss: 0.00002494
Iteration 68/1000 | Loss: 0.00002493
Iteration 69/1000 | Loss: 0.00002492
Iteration 70/1000 | Loss: 0.00002492
Iteration 71/1000 | Loss: 0.00002491
Iteration 72/1000 | Loss: 0.00002491
Iteration 73/1000 | Loss: 0.00002491
Iteration 74/1000 | Loss: 0.00002490
Iteration 75/1000 | Loss: 0.00002490
Iteration 76/1000 | Loss: 0.00002490
Iteration 77/1000 | Loss: 0.00002490
Iteration 78/1000 | Loss: 0.00002489
Iteration 79/1000 | Loss: 0.00002489
Iteration 80/1000 | Loss: 0.00002489
Iteration 81/1000 | Loss: 0.00002488
Iteration 82/1000 | Loss: 0.00002488
Iteration 83/1000 | Loss: 0.00002488
Iteration 84/1000 | Loss: 0.00002488
Iteration 85/1000 | Loss: 0.00002488
Iteration 86/1000 | Loss: 0.00002487
Iteration 87/1000 | Loss: 0.00002487
Iteration 88/1000 | Loss: 0.00002487
Iteration 89/1000 | Loss: 0.00002487
Iteration 90/1000 | Loss: 0.00002487
Iteration 91/1000 | Loss: 0.00002487
Iteration 92/1000 | Loss: 0.00002486
Iteration 93/1000 | Loss: 0.00002486
Iteration 94/1000 | Loss: 0.00002485
Iteration 95/1000 | Loss: 0.00002485
Iteration 96/1000 | Loss: 0.00002485
Iteration 97/1000 | Loss: 0.00002484
Iteration 98/1000 | Loss: 0.00002484
Iteration 99/1000 | Loss: 0.00002484
Iteration 100/1000 | Loss: 0.00002484
Iteration 101/1000 | Loss: 0.00002484
Iteration 102/1000 | Loss: 0.00002484
Iteration 103/1000 | Loss: 0.00002484
Iteration 104/1000 | Loss: 0.00002484
Iteration 105/1000 | Loss: 0.00002483
Iteration 106/1000 | Loss: 0.00002483
Iteration 107/1000 | Loss: 0.00002483
Iteration 108/1000 | Loss: 0.00002483
Iteration 109/1000 | Loss: 0.00002483
Iteration 110/1000 | Loss: 0.00002483
Iteration 111/1000 | Loss: 0.00002483
Iteration 112/1000 | Loss: 0.00002483
Iteration 113/1000 | Loss: 0.00002482
Iteration 114/1000 | Loss: 0.00002482
Iteration 115/1000 | Loss: 0.00002482
Iteration 116/1000 | Loss: 0.00002482
Iteration 117/1000 | Loss: 0.00002482
Iteration 118/1000 | Loss: 0.00002482
Iteration 119/1000 | Loss: 0.00002481
Iteration 120/1000 | Loss: 0.00002481
Iteration 121/1000 | Loss: 0.00002481
Iteration 122/1000 | Loss: 0.00002481
Iteration 123/1000 | Loss: 0.00002481
Iteration 124/1000 | Loss: 0.00002481
Iteration 125/1000 | Loss: 0.00002481
Iteration 126/1000 | Loss: 0.00002480
Iteration 127/1000 | Loss: 0.00002480
Iteration 128/1000 | Loss: 0.00002480
Iteration 129/1000 | Loss: 0.00002480
Iteration 130/1000 | Loss: 0.00002480
Iteration 131/1000 | Loss: 0.00002480
Iteration 132/1000 | Loss: 0.00002480
Iteration 133/1000 | Loss: 0.00002480
Iteration 134/1000 | Loss: 0.00002480
Iteration 135/1000 | Loss: 0.00002480
Iteration 136/1000 | Loss: 0.00002480
Iteration 137/1000 | Loss: 0.00002480
Iteration 138/1000 | Loss: 0.00002480
Iteration 139/1000 | Loss: 0.00002480
Iteration 140/1000 | Loss: 0.00002480
Iteration 141/1000 | Loss: 0.00002479
Iteration 142/1000 | Loss: 0.00002479
Iteration 143/1000 | Loss: 0.00002479
Iteration 144/1000 | Loss: 0.00002479
Iteration 145/1000 | Loss: 0.00002479
Iteration 146/1000 | Loss: 0.00002479
Iteration 147/1000 | Loss: 0.00002478
Iteration 148/1000 | Loss: 0.00002478
Iteration 149/1000 | Loss: 0.00002478
Iteration 150/1000 | Loss: 0.00002478
Iteration 151/1000 | Loss: 0.00002478
Iteration 152/1000 | Loss: 0.00002478
Iteration 153/1000 | Loss: 0.00002478
Iteration 154/1000 | Loss: 0.00002478
Iteration 155/1000 | Loss: 0.00002478
Iteration 156/1000 | Loss: 0.00002478
Iteration 157/1000 | Loss: 0.00002477
Iteration 158/1000 | Loss: 0.00002477
Iteration 159/1000 | Loss: 0.00002477
Iteration 160/1000 | Loss: 0.00002477
Iteration 161/1000 | Loss: 0.00002477
Iteration 162/1000 | Loss: 0.00002477
Iteration 163/1000 | Loss: 0.00002477
Iteration 164/1000 | Loss: 0.00002477
Iteration 165/1000 | Loss: 0.00002477
Iteration 166/1000 | Loss: 0.00002477
Iteration 167/1000 | Loss: 0.00002477
Iteration 168/1000 | Loss: 0.00002477
Iteration 169/1000 | Loss: 0.00002477
Iteration 170/1000 | Loss: 0.00002477
Iteration 171/1000 | Loss: 0.00002477
Iteration 172/1000 | Loss: 0.00002477
Iteration 173/1000 | Loss: 0.00002477
Iteration 174/1000 | Loss: 0.00002477
Iteration 175/1000 | Loss: 0.00002477
Iteration 176/1000 | Loss: 0.00002477
Iteration 177/1000 | Loss: 0.00002477
Iteration 178/1000 | Loss: 0.00002477
Iteration 179/1000 | Loss: 0.00002477
Iteration 180/1000 | Loss: 0.00002477
Iteration 181/1000 | Loss: 0.00002477
Iteration 182/1000 | Loss: 0.00002477
Iteration 183/1000 | Loss: 0.00002477
Iteration 184/1000 | Loss: 0.00002477
Iteration 185/1000 | Loss: 0.00002477
Iteration 186/1000 | Loss: 0.00002477
Iteration 187/1000 | Loss: 0.00002477
Iteration 188/1000 | Loss: 0.00002477
Iteration 189/1000 | Loss: 0.00002477
Iteration 190/1000 | Loss: 0.00002477
Iteration 191/1000 | Loss: 0.00002477
Iteration 192/1000 | Loss: 0.00002477
Iteration 193/1000 | Loss: 0.00002477
Iteration 194/1000 | Loss: 0.00002477
Iteration 195/1000 | Loss: 0.00002477
Iteration 196/1000 | Loss: 0.00002477
Iteration 197/1000 | Loss: 0.00002477
Iteration 198/1000 | Loss: 0.00002477
Iteration 199/1000 | Loss: 0.00002477
Iteration 200/1000 | Loss: 0.00002477
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [2.4766432034084573e-05, 2.4766432034084573e-05, 2.4766432034084573e-05, 2.4766432034084573e-05, 2.4766432034084573e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4766432034084573e-05

Optimization complete. Final v2v error: 3.8473055362701416 mm

Highest mean error: 5.785297393798828 mm for frame 61

Lowest mean error: 2.9765920639038086 mm for frame 153

Saving results

Total time: 64.07874321937561
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_ben_posed_008/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_ben_posed_008/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00842553
Iteration 2/25 | Loss: 0.00094505
Iteration 3/25 | Loss: 0.00072577
Iteration 4/25 | Loss: 0.00068143
Iteration 5/25 | Loss: 0.00066887
Iteration 6/25 | Loss: 0.00066404
Iteration 7/25 | Loss: 0.00066559
Iteration 8/25 | Loss: 0.00066428
Iteration 9/25 | Loss: 0.00066336
Iteration 10/25 | Loss: 0.00066686
Iteration 11/25 | Loss: 0.00066651
Iteration 12/25 | Loss: 0.00066092
Iteration 13/25 | Loss: 0.00065899
Iteration 14/25 | Loss: 0.00065727
Iteration 15/25 | Loss: 0.00065611
Iteration 16/25 | Loss: 0.00066354
Iteration 17/25 | Loss: 0.00065955
Iteration 18/25 | Loss: 0.00065535
Iteration 19/25 | Loss: 0.00065023
Iteration 20/25 | Loss: 0.00064924
Iteration 21/25 | Loss: 0.00064889
Iteration 22/25 | Loss: 0.00064876
Iteration 23/25 | Loss: 0.00064875
Iteration 24/25 | Loss: 0.00064875
Iteration 25/25 | Loss: 0.00064875

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45079958
Iteration 2/25 | Loss: 0.00110136
Iteration 3/25 | Loss: 0.00110135
Iteration 4/25 | Loss: 0.00110135
Iteration 5/25 | Loss: 0.00110135
Iteration 6/25 | Loss: 0.00110135
Iteration 7/25 | Loss: 0.00110135
Iteration 8/25 | Loss: 0.00110135
Iteration 9/25 | Loss: 0.00110134
Iteration 10/25 | Loss: 0.00110134
Iteration 11/25 | Loss: 0.00110134
Iteration 12/25 | Loss: 0.00110134
Iteration 13/25 | Loss: 0.00110134
Iteration 14/25 | Loss: 0.00110134
Iteration 15/25 | Loss: 0.00110134
Iteration 16/25 | Loss: 0.00110134
Iteration 17/25 | Loss: 0.00110134
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001101344358175993, 0.001101344358175993, 0.001101344358175993, 0.001101344358175993, 0.001101344358175993]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001101344358175993

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110134
Iteration 2/1000 | Loss: 0.00012246
Iteration 3/1000 | Loss: 0.00009013
Iteration 4/1000 | Loss: 0.00007764
Iteration 5/1000 | Loss: 0.00007101
Iteration 6/1000 | Loss: 0.00006710
Iteration 7/1000 | Loss: 0.00006509
Iteration 8/1000 | Loss: 0.00006264
Iteration 9/1000 | Loss: 0.00006047
Iteration 10/1000 | Loss: 0.00005941
Iteration 11/1000 | Loss: 0.00005865
Iteration 12/1000 | Loss: 0.00005800
Iteration 13/1000 | Loss: 0.00005743
Iteration 14/1000 | Loss: 0.00005690
Iteration 15/1000 | Loss: 0.00005646
Iteration 16/1000 | Loss: 0.00120404
Iteration 17/1000 | Loss: 0.00060871
Iteration 18/1000 | Loss: 0.00008064
Iteration 19/1000 | Loss: 0.00006992
Iteration 20/1000 | Loss: 0.00006155
Iteration 21/1000 | Loss: 0.00005721
Iteration 22/1000 | Loss: 0.00053960
Iteration 23/1000 | Loss: 0.00010430
Iteration 24/1000 | Loss: 0.00005867
Iteration 25/1000 | Loss: 0.00044961
Iteration 26/1000 | Loss: 0.00009773
Iteration 27/1000 | Loss: 0.00005889
Iteration 28/1000 | Loss: 0.00084688
Iteration 29/1000 | Loss: 0.00014263
Iteration 30/1000 | Loss: 0.00005929
Iteration 31/1000 | Loss: 0.00005831
Iteration 32/1000 | Loss: 0.00005652
Iteration 33/1000 | Loss: 0.00134750
Iteration 34/1000 | Loss: 0.00137973
Iteration 35/1000 | Loss: 0.00027929
Iteration 36/1000 | Loss: 0.00006237
Iteration 37/1000 | Loss: 0.00005658
Iteration 38/1000 | Loss: 0.00005287
Iteration 39/1000 | Loss: 0.00005191
Iteration 40/1000 | Loss: 0.00005082
Iteration 41/1000 | Loss: 0.00004978
Iteration 42/1000 | Loss: 0.00004896
Iteration 43/1000 | Loss: 0.00004824
Iteration 44/1000 | Loss: 0.00061449
Iteration 45/1000 | Loss: 0.00103970
Iteration 46/1000 | Loss: 0.00102989
Iteration 47/1000 | Loss: 0.00058195
Iteration 48/1000 | Loss: 0.00041049
Iteration 49/1000 | Loss: 0.00004737
Iteration 50/1000 | Loss: 0.00004389
Iteration 51/1000 | Loss: 0.00004137
Iteration 52/1000 | Loss: 0.00003970
Iteration 53/1000 | Loss: 0.00048203
Iteration 54/1000 | Loss: 0.00109878
Iteration 55/1000 | Loss: 0.00058825
Iteration 56/1000 | Loss: 0.00105116
Iteration 57/1000 | Loss: 0.00042640
Iteration 58/1000 | Loss: 0.00009000
Iteration 59/1000 | Loss: 0.00004179
Iteration 60/1000 | Loss: 0.00003721
Iteration 61/1000 | Loss: 0.00112617
Iteration 62/1000 | Loss: 0.00003638
Iteration 63/1000 | Loss: 0.00050203
Iteration 64/1000 | Loss: 0.00003335
Iteration 65/1000 | Loss: 0.00002928
Iteration 66/1000 | Loss: 0.00002776
Iteration 67/1000 | Loss: 0.00057650
Iteration 68/1000 | Loss: 0.00002768
Iteration 69/1000 | Loss: 0.00002548
Iteration 70/1000 | Loss: 0.00002408
Iteration 71/1000 | Loss: 0.00002314
Iteration 72/1000 | Loss: 0.00002246
Iteration 73/1000 | Loss: 0.00002203
Iteration 74/1000 | Loss: 0.00002164
Iteration 75/1000 | Loss: 0.00002133
Iteration 76/1000 | Loss: 0.00002107
Iteration 77/1000 | Loss: 0.00002092
Iteration 78/1000 | Loss: 0.00002072
Iteration 79/1000 | Loss: 0.00002064
Iteration 80/1000 | Loss: 0.00002060
Iteration 81/1000 | Loss: 0.00002042
Iteration 82/1000 | Loss: 0.00002041
Iteration 83/1000 | Loss: 0.00002033
Iteration 84/1000 | Loss: 0.00002033
Iteration 85/1000 | Loss: 0.00002027
Iteration 86/1000 | Loss: 0.00002027
Iteration 87/1000 | Loss: 0.00002027
Iteration 88/1000 | Loss: 0.00052092
Iteration 89/1000 | Loss: 0.00005856
Iteration 90/1000 | Loss: 0.00002459
Iteration 91/1000 | Loss: 0.00002032
Iteration 92/1000 | Loss: 0.00001885
Iteration 93/1000 | Loss: 0.00001856
Iteration 94/1000 | Loss: 0.00001849
Iteration 95/1000 | Loss: 0.00001838
Iteration 96/1000 | Loss: 0.00001836
Iteration 97/1000 | Loss: 0.00001832
Iteration 98/1000 | Loss: 0.00001831
Iteration 99/1000 | Loss: 0.00001831
Iteration 100/1000 | Loss: 0.00001830
Iteration 101/1000 | Loss: 0.00001830
Iteration 102/1000 | Loss: 0.00001829
Iteration 103/1000 | Loss: 0.00001829
Iteration 104/1000 | Loss: 0.00001829
Iteration 105/1000 | Loss: 0.00001828
Iteration 106/1000 | Loss: 0.00001827
Iteration 107/1000 | Loss: 0.00001825
Iteration 108/1000 | Loss: 0.00001825
Iteration 109/1000 | Loss: 0.00001822
Iteration 110/1000 | Loss: 0.00001822
Iteration 111/1000 | Loss: 0.00001821
Iteration 112/1000 | Loss: 0.00001820
Iteration 113/1000 | Loss: 0.00001820
Iteration 114/1000 | Loss: 0.00001819
Iteration 115/1000 | Loss: 0.00001819
Iteration 116/1000 | Loss: 0.00001819
Iteration 117/1000 | Loss: 0.00001819
Iteration 118/1000 | Loss: 0.00001818
Iteration 119/1000 | Loss: 0.00001818
Iteration 120/1000 | Loss: 0.00001817
Iteration 121/1000 | Loss: 0.00001817
Iteration 122/1000 | Loss: 0.00001817
Iteration 123/1000 | Loss: 0.00001816
Iteration 124/1000 | Loss: 0.00001816
Iteration 125/1000 | Loss: 0.00001816
Iteration 126/1000 | Loss: 0.00001816
Iteration 127/1000 | Loss: 0.00001815
Iteration 128/1000 | Loss: 0.00001815
Iteration 129/1000 | Loss: 0.00001815
Iteration 130/1000 | Loss: 0.00001815
Iteration 131/1000 | Loss: 0.00001814
Iteration 132/1000 | Loss: 0.00001814
Iteration 133/1000 | Loss: 0.00001814
Iteration 134/1000 | Loss: 0.00001814
Iteration 135/1000 | Loss: 0.00001814
Iteration 136/1000 | Loss: 0.00001814
Iteration 137/1000 | Loss: 0.00001814
Iteration 138/1000 | Loss: 0.00001814
Iteration 139/1000 | Loss: 0.00001814
Iteration 140/1000 | Loss: 0.00001813
Iteration 141/1000 | Loss: 0.00001813
Iteration 142/1000 | Loss: 0.00001813
Iteration 143/1000 | Loss: 0.00001813
Iteration 144/1000 | Loss: 0.00001813
Iteration 145/1000 | Loss: 0.00001813
Iteration 146/1000 | Loss: 0.00001813
Iteration 147/1000 | Loss: 0.00001813
Iteration 148/1000 | Loss: 0.00001813
Iteration 149/1000 | Loss: 0.00001813
Iteration 150/1000 | Loss: 0.00001812
Iteration 151/1000 | Loss: 0.00001812
Iteration 152/1000 | Loss: 0.00001812
Iteration 153/1000 | Loss: 0.00001812
Iteration 154/1000 | Loss: 0.00001812
Iteration 155/1000 | Loss: 0.00001812
Iteration 156/1000 | Loss: 0.00001812
Iteration 157/1000 | Loss: 0.00001812
Iteration 158/1000 | Loss: 0.00001811
Iteration 159/1000 | Loss: 0.00001811
Iteration 160/1000 | Loss: 0.00001811
Iteration 161/1000 | Loss: 0.00001811
Iteration 162/1000 | Loss: 0.00001811
Iteration 163/1000 | Loss: 0.00001811
Iteration 164/1000 | Loss: 0.00001811
Iteration 165/1000 | Loss: 0.00001811
Iteration 166/1000 | Loss: 0.00001811
Iteration 167/1000 | Loss: 0.00001810
Iteration 168/1000 | Loss: 0.00001810
Iteration 169/1000 | Loss: 0.00001810
Iteration 170/1000 | Loss: 0.00001810
Iteration 171/1000 | Loss: 0.00001810
Iteration 172/1000 | Loss: 0.00001810
Iteration 173/1000 | Loss: 0.00001810
Iteration 174/1000 | Loss: 0.00001810
Iteration 175/1000 | Loss: 0.00001810
Iteration 176/1000 | Loss: 0.00001810
Iteration 177/1000 | Loss: 0.00001810
Iteration 178/1000 | Loss: 0.00001810
Iteration 179/1000 | Loss: 0.00001810
Iteration 180/1000 | Loss: 0.00001810
Iteration 181/1000 | Loss: 0.00001810
Iteration 182/1000 | Loss: 0.00001810
Iteration 183/1000 | Loss: 0.00001809
Iteration 184/1000 | Loss: 0.00001809
Iteration 185/1000 | Loss: 0.00001809
Iteration 186/1000 | Loss: 0.00001809
Iteration 187/1000 | Loss: 0.00001809
Iteration 188/1000 | Loss: 0.00001809
Iteration 189/1000 | Loss: 0.00001809
Iteration 190/1000 | Loss: 0.00001809
Iteration 191/1000 | Loss: 0.00001809
Iteration 192/1000 | Loss: 0.00001809
Iteration 193/1000 | Loss: 0.00001809
Iteration 194/1000 | Loss: 0.00001809
Iteration 195/1000 | Loss: 0.00001809
Iteration 196/1000 | Loss: 0.00001809
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [1.8091262973030098e-05, 1.8091262973030098e-05, 1.8091262973030098e-05, 1.8091262973030098e-05, 1.8091262973030098e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8091262973030098e-05

Optimization complete. Final v2v error: 2.837928295135498 mm

Highest mean error: 11.185585975646973 mm for frame 94

Lowest mean error: 2.3574788570404053 mm for frame 52

Saving results

Total time: 182.31618404388428
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_42_us_0617/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_42_us_0617/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_42_us_0617/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00883495
Iteration 2/25 | Loss: 0.00168187
Iteration 3/25 | Loss: 0.00157659
Iteration 4/25 | Loss: 0.00154936
Iteration 5/25 | Loss: 0.00152737
Iteration 6/25 | Loss: 0.00151474
Iteration 7/25 | Loss: 0.00150874
Iteration 8/25 | Loss: 0.00150823
Iteration 9/25 | Loss: 0.00150767
Iteration 10/25 | Loss: 0.00150640
Iteration 11/25 | Loss: 0.00150607
Iteration 12/25 | Loss: 0.00150603
Iteration 13/25 | Loss: 0.00150603
Iteration 14/25 | Loss: 0.00150603
Iteration 15/25 | Loss: 0.00150603
Iteration 16/25 | Loss: 0.00150603
Iteration 17/25 | Loss: 0.00150603
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001506030559539795, 0.001506030559539795, 0.001506030559539795, 0.001506030559539795, 0.001506030559539795]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001506030559539795

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50473237
Iteration 2/25 | Loss: 0.00168076
Iteration 3/25 | Loss: 0.00168075
Iteration 4/25 | Loss: 0.00168075
Iteration 5/25 | Loss: 0.00168075
Iteration 6/25 | Loss: 0.00168075
Iteration 7/25 | Loss: 0.00168075
Iteration 8/25 | Loss: 0.00168075
Iteration 9/25 | Loss: 0.00168075
Iteration 10/25 | Loss: 0.00168075
Iteration 11/25 | Loss: 0.00168075
Iteration 12/25 | Loss: 0.00168075
Iteration 13/25 | Loss: 0.00168075
Iteration 14/25 | Loss: 0.00168075
Iteration 15/25 | Loss: 0.00168075
Iteration 16/25 | Loss: 0.00168075
Iteration 17/25 | Loss: 0.00168075
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001680751913227141, 0.001680751913227141, 0.001680751913227141, 0.001680751913227141, 0.001680751913227141]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001680751913227141

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00168075
Iteration 2/1000 | Loss: 0.00006275
Iteration 3/1000 | Loss: 0.00004509
Iteration 4/1000 | Loss: 0.00003952
Iteration 5/1000 | Loss: 0.00003763
Iteration 6/1000 | Loss: 0.00003638
Iteration 7/1000 | Loss: 0.00003540
Iteration 8/1000 | Loss: 0.00003454
Iteration 9/1000 | Loss: 0.00003395
Iteration 10/1000 | Loss: 0.00003344
Iteration 11/1000 | Loss: 0.00003314
Iteration 12/1000 | Loss: 0.00003312
Iteration 13/1000 | Loss: 0.00003293
Iteration 14/1000 | Loss: 0.00003289
Iteration 15/1000 | Loss: 0.00003284
Iteration 16/1000 | Loss: 0.00003284
Iteration 17/1000 | Loss: 0.00003274
Iteration 18/1000 | Loss: 0.00003270
Iteration 19/1000 | Loss: 0.00003270
Iteration 20/1000 | Loss: 0.00003270
Iteration 21/1000 | Loss: 0.00003270
Iteration 22/1000 | Loss: 0.00003270
Iteration 23/1000 | Loss: 0.00003270
Iteration 24/1000 | Loss: 0.00003269
Iteration 25/1000 | Loss: 0.00003269
Iteration 26/1000 | Loss: 0.00003269
Iteration 27/1000 | Loss: 0.00003266
Iteration 28/1000 | Loss: 0.00003265
Iteration 29/1000 | Loss: 0.00003265
Iteration 30/1000 | Loss: 0.00003265
Iteration 31/1000 | Loss: 0.00003265
Iteration 32/1000 | Loss: 0.00003265
Iteration 33/1000 | Loss: 0.00003265
Iteration 34/1000 | Loss: 0.00003265
Iteration 35/1000 | Loss: 0.00003264
Iteration 36/1000 | Loss: 0.00003264
Iteration 37/1000 | Loss: 0.00003263
Iteration 38/1000 | Loss: 0.00003263
Iteration 39/1000 | Loss: 0.00003263
Iteration 40/1000 | Loss: 0.00003262
Iteration 41/1000 | Loss: 0.00003262
Iteration 42/1000 | Loss: 0.00003262
Iteration 43/1000 | Loss: 0.00003262
Iteration 44/1000 | Loss: 0.00003262
Iteration 45/1000 | Loss: 0.00003261
Iteration 46/1000 | Loss: 0.00003261
Iteration 47/1000 | Loss: 0.00003261
Iteration 48/1000 | Loss: 0.00003261
Iteration 49/1000 | Loss: 0.00003261
Iteration 50/1000 | Loss: 0.00003261
Iteration 51/1000 | Loss: 0.00003261
Iteration 52/1000 | Loss: 0.00003261
Iteration 53/1000 | Loss: 0.00003260
Iteration 54/1000 | Loss: 0.00003260
Iteration 55/1000 | Loss: 0.00003260
Iteration 56/1000 | Loss: 0.00003260
Iteration 57/1000 | Loss: 0.00003260
Iteration 58/1000 | Loss: 0.00003260
Iteration 59/1000 | Loss: 0.00003260
Iteration 60/1000 | Loss: 0.00003260
Iteration 61/1000 | Loss: 0.00003260
Iteration 62/1000 | Loss: 0.00003260
Iteration 63/1000 | Loss: 0.00003260
Iteration 64/1000 | Loss: 0.00003259
Iteration 65/1000 | Loss: 0.00003259
Iteration 66/1000 | Loss: 0.00003259
Iteration 67/1000 | Loss: 0.00003259
Iteration 68/1000 | Loss: 0.00003259
Iteration 69/1000 | Loss: 0.00003258
Iteration 70/1000 | Loss: 0.00003258
Iteration 71/1000 | Loss: 0.00003258
Iteration 72/1000 | Loss: 0.00003257
Iteration 73/1000 | Loss: 0.00003257
Iteration 74/1000 | Loss: 0.00003257
Iteration 75/1000 | Loss: 0.00003257
Iteration 76/1000 | Loss: 0.00003256
Iteration 77/1000 | Loss: 0.00003256
Iteration 78/1000 | Loss: 0.00003256
Iteration 79/1000 | Loss: 0.00003256
Iteration 80/1000 | Loss: 0.00003256
Iteration 81/1000 | Loss: 0.00003256
Iteration 82/1000 | Loss: 0.00003256
Iteration 83/1000 | Loss: 0.00003256
Iteration 84/1000 | Loss: 0.00003256
Iteration 85/1000 | Loss: 0.00003256
Iteration 86/1000 | Loss: 0.00003255
Iteration 87/1000 | Loss: 0.00003255
Iteration 88/1000 | Loss: 0.00003255
Iteration 89/1000 | Loss: 0.00003255
Iteration 90/1000 | Loss: 0.00003255
Iteration 91/1000 | Loss: 0.00003255
Iteration 92/1000 | Loss: 0.00003254
Iteration 93/1000 | Loss: 0.00003254
Iteration 94/1000 | Loss: 0.00003254
Iteration 95/1000 | Loss: 0.00003254
Iteration 96/1000 | Loss: 0.00003254
Iteration 97/1000 | Loss: 0.00003254
Iteration 98/1000 | Loss: 0.00003253
Iteration 99/1000 | Loss: 0.00003253
Iteration 100/1000 | Loss: 0.00003253
Iteration 101/1000 | Loss: 0.00003253
Iteration 102/1000 | Loss: 0.00003253
Iteration 103/1000 | Loss: 0.00003253
Iteration 104/1000 | Loss: 0.00003253
Iteration 105/1000 | Loss: 0.00003253
Iteration 106/1000 | Loss: 0.00003253
Iteration 107/1000 | Loss: 0.00003253
Iteration 108/1000 | Loss: 0.00003253
Iteration 109/1000 | Loss: 0.00003253
Iteration 110/1000 | Loss: 0.00003252
Iteration 111/1000 | Loss: 0.00003252
Iteration 112/1000 | Loss: 0.00003252
Iteration 113/1000 | Loss: 0.00003252
Iteration 114/1000 | Loss: 0.00003252
Iteration 115/1000 | Loss: 0.00003252
Iteration 116/1000 | Loss: 0.00003252
Iteration 117/1000 | Loss: 0.00003252
Iteration 118/1000 | Loss: 0.00003252
Iteration 119/1000 | Loss: 0.00003252
Iteration 120/1000 | Loss: 0.00003252
Iteration 121/1000 | Loss: 0.00003252
Iteration 122/1000 | Loss: 0.00003252
Iteration 123/1000 | Loss: 0.00003252
Iteration 124/1000 | Loss: 0.00003252
Iteration 125/1000 | Loss: 0.00003252
Iteration 126/1000 | Loss: 0.00003252
Iteration 127/1000 | Loss: 0.00003251
Iteration 128/1000 | Loss: 0.00003251
Iteration 129/1000 | Loss: 0.00003251
Iteration 130/1000 | Loss: 0.00003251
Iteration 131/1000 | Loss: 0.00003251
Iteration 132/1000 | Loss: 0.00003251
Iteration 133/1000 | Loss: 0.00003251
Iteration 134/1000 | Loss: 0.00003251
Iteration 135/1000 | Loss: 0.00003251
Iteration 136/1000 | Loss: 0.00003251
Iteration 137/1000 | Loss: 0.00003251
Iteration 138/1000 | Loss: 0.00003251
Iteration 139/1000 | Loss: 0.00003251
Iteration 140/1000 | Loss: 0.00003251
Iteration 141/1000 | Loss: 0.00003251
Iteration 142/1000 | Loss: 0.00003251
Iteration 143/1000 | Loss: 0.00003251
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [3.251355155953206e-05, 3.251355155953206e-05, 3.251355155953206e-05, 3.251355155953206e-05, 3.251355155953206e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.251355155953206e-05

Optimization complete. Final v2v error: 4.941041469573975 mm

Highest mean error: 5.210959434509277 mm for frame 72

Lowest mean error: 4.701175212860107 mm for frame 172

Saving results

Total time: 53.834747314453125
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_42_us_0617/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_42_us_0617/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_42_us_0617/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01102087
Iteration 2/25 | Loss: 0.00180602
Iteration 3/25 | Loss: 0.00155672
Iteration 4/25 | Loss: 0.00153127
Iteration 5/25 | Loss: 0.00152597
Iteration 6/25 | Loss: 0.00152382
Iteration 7/25 | Loss: 0.00152341
Iteration 8/25 | Loss: 0.00152341
Iteration 9/25 | Loss: 0.00152341
Iteration 10/25 | Loss: 0.00152341
Iteration 11/25 | Loss: 0.00152341
Iteration 12/25 | Loss: 0.00152341
Iteration 13/25 | Loss: 0.00152341
Iteration 14/25 | Loss: 0.00152341
Iteration 15/25 | Loss: 0.00152341
Iteration 16/25 | Loss: 0.00152341
Iteration 17/25 | Loss: 0.00152341
Iteration 18/25 | Loss: 0.00152341
Iteration 19/25 | Loss: 0.00152341
Iteration 20/25 | Loss: 0.00152341
Iteration 21/25 | Loss: 0.00152341
Iteration 22/25 | Loss: 0.00152341
Iteration 23/25 | Loss: 0.00152341
Iteration 24/25 | Loss: 0.00152341
Iteration 25/25 | Loss: 0.00152341

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49344015
Iteration 2/25 | Loss: 0.00149377
Iteration 3/25 | Loss: 0.00149377
Iteration 4/25 | Loss: 0.00149377
Iteration 5/25 | Loss: 0.00149377
Iteration 6/25 | Loss: 0.00149377
Iteration 7/25 | Loss: 0.00149377
Iteration 8/25 | Loss: 0.00149377
Iteration 9/25 | Loss: 0.00149377
Iteration 10/25 | Loss: 0.00149377
Iteration 11/25 | Loss: 0.00149377
Iteration 12/25 | Loss: 0.00149377
Iteration 13/25 | Loss: 0.00149377
Iteration 14/25 | Loss: 0.00149377
Iteration 15/25 | Loss: 0.00149377
Iteration 16/25 | Loss: 0.00149377
Iteration 17/25 | Loss: 0.00149377
Iteration 18/25 | Loss: 0.00149377
Iteration 19/25 | Loss: 0.00149377
Iteration 20/25 | Loss: 0.00149377
Iteration 21/25 | Loss: 0.00149377
Iteration 22/25 | Loss: 0.00149377
Iteration 23/25 | Loss: 0.00149377
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0014937670202925801, 0.0014937670202925801, 0.0014937670202925801, 0.0014937670202925801, 0.0014937670202925801]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014937670202925801

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00149377
Iteration 2/1000 | Loss: 0.00006890
Iteration 3/1000 | Loss: 0.00004962
Iteration 4/1000 | Loss: 0.00004533
Iteration 5/1000 | Loss: 0.00004295
Iteration 6/1000 | Loss: 0.00004158
Iteration 7/1000 | Loss: 0.00004032
Iteration 8/1000 | Loss: 0.00003967
Iteration 9/1000 | Loss: 0.00003936
Iteration 10/1000 | Loss: 0.00003925
Iteration 11/1000 | Loss: 0.00003919
Iteration 12/1000 | Loss: 0.00003917
Iteration 13/1000 | Loss: 0.00003913
Iteration 14/1000 | Loss: 0.00003910
Iteration 15/1000 | Loss: 0.00003907
Iteration 16/1000 | Loss: 0.00003899
Iteration 17/1000 | Loss: 0.00003894
Iteration 18/1000 | Loss: 0.00003894
Iteration 19/1000 | Loss: 0.00003894
Iteration 20/1000 | Loss: 0.00003894
Iteration 21/1000 | Loss: 0.00003893
Iteration 22/1000 | Loss: 0.00003893
Iteration 23/1000 | Loss: 0.00003893
Iteration 24/1000 | Loss: 0.00003892
Iteration 25/1000 | Loss: 0.00003892
Iteration 26/1000 | Loss: 0.00003892
Iteration 27/1000 | Loss: 0.00003891
Iteration 28/1000 | Loss: 0.00003891
Iteration 29/1000 | Loss: 0.00003891
Iteration 30/1000 | Loss: 0.00003891
Iteration 31/1000 | Loss: 0.00003891
Iteration 32/1000 | Loss: 0.00003891
Iteration 33/1000 | Loss: 0.00003891
Iteration 34/1000 | Loss: 0.00003890
Iteration 35/1000 | Loss: 0.00003890
Iteration 36/1000 | Loss: 0.00003890
Iteration 37/1000 | Loss: 0.00003890
Iteration 38/1000 | Loss: 0.00003890
Iteration 39/1000 | Loss: 0.00003890
Iteration 40/1000 | Loss: 0.00003890
Iteration 41/1000 | Loss: 0.00003889
Iteration 42/1000 | Loss: 0.00003889
Iteration 43/1000 | Loss: 0.00003889
Iteration 44/1000 | Loss: 0.00003889
Iteration 45/1000 | Loss: 0.00003889
Iteration 46/1000 | Loss: 0.00003888
Iteration 47/1000 | Loss: 0.00003888
Iteration 48/1000 | Loss: 0.00003888
Iteration 49/1000 | Loss: 0.00003888
Iteration 50/1000 | Loss: 0.00003888
Iteration 51/1000 | Loss: 0.00003888
Iteration 52/1000 | Loss: 0.00003888
Iteration 53/1000 | Loss: 0.00003888
Iteration 54/1000 | Loss: 0.00003887
Iteration 55/1000 | Loss: 0.00003887
Iteration 56/1000 | Loss: 0.00003887
Iteration 57/1000 | Loss: 0.00003887
Iteration 58/1000 | Loss: 0.00003887
Iteration 59/1000 | Loss: 0.00003887
Iteration 60/1000 | Loss: 0.00003887
Iteration 61/1000 | Loss: 0.00003887
Iteration 62/1000 | Loss: 0.00003886
Iteration 63/1000 | Loss: 0.00003886
Iteration 64/1000 | Loss: 0.00003886
Iteration 65/1000 | Loss: 0.00003886
Iteration 66/1000 | Loss: 0.00003886
Iteration 67/1000 | Loss: 0.00003886
Iteration 68/1000 | Loss: 0.00003886
Iteration 69/1000 | Loss: 0.00003886
Iteration 70/1000 | Loss: 0.00003886
Iteration 71/1000 | Loss: 0.00003886
Iteration 72/1000 | Loss: 0.00003886
Iteration 73/1000 | Loss: 0.00003886
Iteration 74/1000 | Loss: 0.00003886
Iteration 75/1000 | Loss: 0.00003886
Iteration 76/1000 | Loss: 0.00003886
Iteration 77/1000 | Loss: 0.00003886
Iteration 78/1000 | Loss: 0.00003885
Iteration 79/1000 | Loss: 0.00003885
Iteration 80/1000 | Loss: 0.00003885
Iteration 81/1000 | Loss: 0.00003885
Iteration 82/1000 | Loss: 0.00003885
Iteration 83/1000 | Loss: 0.00003885
Iteration 84/1000 | Loss: 0.00003885
Iteration 85/1000 | Loss: 0.00003885
Iteration 86/1000 | Loss: 0.00003885
Iteration 87/1000 | Loss: 0.00003885
Iteration 88/1000 | Loss: 0.00003885
Iteration 89/1000 | Loss: 0.00003885
Iteration 90/1000 | Loss: 0.00003885
Iteration 91/1000 | Loss: 0.00003885
Iteration 92/1000 | Loss: 0.00003884
Iteration 93/1000 | Loss: 0.00003884
Iteration 94/1000 | Loss: 0.00003884
Iteration 95/1000 | Loss: 0.00003884
Iteration 96/1000 | Loss: 0.00003884
Iteration 97/1000 | Loss: 0.00003884
Iteration 98/1000 | Loss: 0.00003884
Iteration 99/1000 | Loss: 0.00003884
Iteration 100/1000 | Loss: 0.00003884
Iteration 101/1000 | Loss: 0.00003883
Iteration 102/1000 | Loss: 0.00003883
Iteration 103/1000 | Loss: 0.00003883
Iteration 104/1000 | Loss: 0.00003883
Iteration 105/1000 | Loss: 0.00003883
Iteration 106/1000 | Loss: 0.00003883
Iteration 107/1000 | Loss: 0.00003883
Iteration 108/1000 | Loss: 0.00003883
Iteration 109/1000 | Loss: 0.00003883
Iteration 110/1000 | Loss: 0.00003883
Iteration 111/1000 | Loss: 0.00003883
Iteration 112/1000 | Loss: 0.00003882
Iteration 113/1000 | Loss: 0.00003882
Iteration 114/1000 | Loss: 0.00003882
Iteration 115/1000 | Loss: 0.00003882
Iteration 116/1000 | Loss: 0.00003882
Iteration 117/1000 | Loss: 0.00003882
Iteration 118/1000 | Loss: 0.00003882
Iteration 119/1000 | Loss: 0.00003881
Iteration 120/1000 | Loss: 0.00003881
Iteration 121/1000 | Loss: 0.00003881
Iteration 122/1000 | Loss: 0.00003881
Iteration 123/1000 | Loss: 0.00003881
Iteration 124/1000 | Loss: 0.00003880
Iteration 125/1000 | Loss: 0.00003880
Iteration 126/1000 | Loss: 0.00003880
Iteration 127/1000 | Loss: 0.00003880
Iteration 128/1000 | Loss: 0.00003880
Iteration 129/1000 | Loss: 0.00003880
Iteration 130/1000 | Loss: 0.00003880
Iteration 131/1000 | Loss: 0.00003879
Iteration 132/1000 | Loss: 0.00003879
Iteration 133/1000 | Loss: 0.00003879
Iteration 134/1000 | Loss: 0.00003879
Iteration 135/1000 | Loss: 0.00003878
Iteration 136/1000 | Loss: 0.00003878
Iteration 137/1000 | Loss: 0.00003878
Iteration 138/1000 | Loss: 0.00003878
Iteration 139/1000 | Loss: 0.00003878
Iteration 140/1000 | Loss: 0.00003878
Iteration 141/1000 | Loss: 0.00003878
Iteration 142/1000 | Loss: 0.00003878
Iteration 143/1000 | Loss: 0.00003878
Iteration 144/1000 | Loss: 0.00003878
Iteration 145/1000 | Loss: 0.00003878
Iteration 146/1000 | Loss: 0.00003878
Iteration 147/1000 | Loss: 0.00003877
Iteration 148/1000 | Loss: 0.00003877
Iteration 149/1000 | Loss: 0.00003877
Iteration 150/1000 | Loss: 0.00003877
Iteration 151/1000 | Loss: 0.00003877
Iteration 152/1000 | Loss: 0.00003877
Iteration 153/1000 | Loss: 0.00003877
Iteration 154/1000 | Loss: 0.00003877
Iteration 155/1000 | Loss: 0.00003877
Iteration 156/1000 | Loss: 0.00003877
Iteration 157/1000 | Loss: 0.00003877
Iteration 158/1000 | Loss: 0.00003877
Iteration 159/1000 | Loss: 0.00003877
Iteration 160/1000 | Loss: 0.00003877
Iteration 161/1000 | Loss: 0.00003877
Iteration 162/1000 | Loss: 0.00003877
Iteration 163/1000 | Loss: 0.00003877
Iteration 164/1000 | Loss: 0.00003877
Iteration 165/1000 | Loss: 0.00003877
Iteration 166/1000 | Loss: 0.00003877
Iteration 167/1000 | Loss: 0.00003877
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [3.8769278035033494e-05, 3.8769278035033494e-05, 3.8769278035033494e-05, 3.8769278035033494e-05, 3.8769278035033494e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.8769278035033494e-05

Optimization complete. Final v2v error: 5.216343879699707 mm

Highest mean error: 5.399871826171875 mm for frame 99

Lowest mean error: 4.933784484863281 mm for frame 151

Saving results

Total time: 36.634721517562866
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_42_us_0617/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_42_us_0617/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_42_us_0617/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01113399
Iteration 2/25 | Loss: 0.00184667
Iteration 3/25 | Loss: 0.00158378
Iteration 4/25 | Loss: 0.00154742
Iteration 5/25 | Loss: 0.00153275
Iteration 6/25 | Loss: 0.00152395
Iteration 7/25 | Loss: 0.00152039
Iteration 8/25 | Loss: 0.00151668
Iteration 9/25 | Loss: 0.00151799
Iteration 10/25 | Loss: 0.00151578
Iteration 11/25 | Loss: 0.00151564
Iteration 12/25 | Loss: 0.00151644
Iteration 13/25 | Loss: 0.00151476
Iteration 14/25 | Loss: 0.00151419
Iteration 15/25 | Loss: 0.00151408
Iteration 16/25 | Loss: 0.00151403
Iteration 17/25 | Loss: 0.00151403
Iteration 18/25 | Loss: 0.00151403
Iteration 19/25 | Loss: 0.00151402
Iteration 20/25 | Loss: 0.00151402
Iteration 21/25 | Loss: 0.00151402
Iteration 22/25 | Loss: 0.00151402
Iteration 23/25 | Loss: 0.00151402
Iteration 24/25 | Loss: 0.00151402
Iteration 25/25 | Loss: 0.00151402

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.02298546
Iteration 2/25 | Loss: 0.00185300
Iteration 3/25 | Loss: 0.00185291
Iteration 4/25 | Loss: 0.00185291
Iteration 5/25 | Loss: 0.00185291
Iteration 6/25 | Loss: 0.00185291
Iteration 7/25 | Loss: 0.00185291
Iteration 8/25 | Loss: 0.00185291
Iteration 9/25 | Loss: 0.00185291
Iteration 10/25 | Loss: 0.00185291
Iteration 11/25 | Loss: 0.00185291
Iteration 12/25 | Loss: 0.00185291
Iteration 13/25 | Loss: 0.00185291
Iteration 14/25 | Loss: 0.00185291
Iteration 15/25 | Loss: 0.00185291
Iteration 16/25 | Loss: 0.00185291
Iteration 17/25 | Loss: 0.00185291
Iteration 18/25 | Loss: 0.00185291
Iteration 19/25 | Loss: 0.00185291
Iteration 20/25 | Loss: 0.00185291
Iteration 21/25 | Loss: 0.00185291
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001852909685112536, 0.001852909685112536, 0.001852909685112536, 0.001852909685112536, 0.001852909685112536]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001852909685112536

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00185291
Iteration 2/1000 | Loss: 0.00005299
Iteration 3/1000 | Loss: 0.00003978
Iteration 4/1000 | Loss: 0.00003450
Iteration 5/1000 | Loss: 0.00003165
Iteration 6/1000 | Loss: 0.00003025
Iteration 7/1000 | Loss: 0.00002913
Iteration 8/1000 | Loss: 0.00002821
Iteration 9/1000 | Loss: 0.00002749
Iteration 10/1000 | Loss: 0.00002695
Iteration 11/1000 | Loss: 0.00002663
Iteration 12/1000 | Loss: 0.00002632
Iteration 13/1000 | Loss: 0.00002616
Iteration 14/1000 | Loss: 0.00002607
Iteration 15/1000 | Loss: 0.00002606
Iteration 16/1000 | Loss: 0.00002602
Iteration 17/1000 | Loss: 0.00002598
Iteration 18/1000 | Loss: 0.00002597
Iteration 19/1000 | Loss: 0.00002597
Iteration 20/1000 | Loss: 0.00002595
Iteration 21/1000 | Loss: 0.00002595
Iteration 22/1000 | Loss: 0.00002595
Iteration 23/1000 | Loss: 0.00002595
Iteration 24/1000 | Loss: 0.00002594
Iteration 25/1000 | Loss: 0.00002594
Iteration 26/1000 | Loss: 0.00002594
Iteration 27/1000 | Loss: 0.00002593
Iteration 28/1000 | Loss: 0.00002591
Iteration 29/1000 | Loss: 0.00002590
Iteration 30/1000 | Loss: 0.00002589
Iteration 31/1000 | Loss: 0.00002589
Iteration 32/1000 | Loss: 0.00002588
Iteration 33/1000 | Loss: 0.00002588
Iteration 34/1000 | Loss: 0.00002587
Iteration 35/1000 | Loss: 0.00002587
Iteration 36/1000 | Loss: 0.00002586
Iteration 37/1000 | Loss: 0.00002585
Iteration 38/1000 | Loss: 0.00002585
Iteration 39/1000 | Loss: 0.00002585
Iteration 40/1000 | Loss: 0.00002584
Iteration 41/1000 | Loss: 0.00002584
Iteration 42/1000 | Loss: 0.00002584
Iteration 43/1000 | Loss: 0.00002583
Iteration 44/1000 | Loss: 0.00002583
Iteration 45/1000 | Loss: 0.00002583
Iteration 46/1000 | Loss: 0.00002582
Iteration 47/1000 | Loss: 0.00002582
Iteration 48/1000 | Loss: 0.00002581
Iteration 49/1000 | Loss: 0.00002581
Iteration 50/1000 | Loss: 0.00002580
Iteration 51/1000 | Loss: 0.00002580
Iteration 52/1000 | Loss: 0.00002580
Iteration 53/1000 | Loss: 0.00002579
Iteration 54/1000 | Loss: 0.00002579
Iteration 55/1000 | Loss: 0.00002578
Iteration 56/1000 | Loss: 0.00002578
Iteration 57/1000 | Loss: 0.00002578
Iteration 58/1000 | Loss: 0.00002577
Iteration 59/1000 | Loss: 0.00002577
Iteration 60/1000 | Loss: 0.00002577
Iteration 61/1000 | Loss: 0.00002577
Iteration 62/1000 | Loss: 0.00002576
Iteration 63/1000 | Loss: 0.00002576
Iteration 64/1000 | Loss: 0.00002576
Iteration 65/1000 | Loss: 0.00002576
Iteration 66/1000 | Loss: 0.00002576
Iteration 67/1000 | Loss: 0.00002576
Iteration 68/1000 | Loss: 0.00002576
Iteration 69/1000 | Loss: 0.00002575
Iteration 70/1000 | Loss: 0.00002575
Iteration 71/1000 | Loss: 0.00002575
Iteration 72/1000 | Loss: 0.00002575
Iteration 73/1000 | Loss: 0.00002575
Iteration 74/1000 | Loss: 0.00002574
Iteration 75/1000 | Loss: 0.00002574
Iteration 76/1000 | Loss: 0.00002573
Iteration 77/1000 | Loss: 0.00002573
Iteration 78/1000 | Loss: 0.00002573
Iteration 79/1000 | Loss: 0.00002573
Iteration 80/1000 | Loss: 0.00002573
Iteration 81/1000 | Loss: 0.00002572
Iteration 82/1000 | Loss: 0.00002572
Iteration 83/1000 | Loss: 0.00002572
Iteration 84/1000 | Loss: 0.00002572
Iteration 85/1000 | Loss: 0.00002572
Iteration 86/1000 | Loss: 0.00002572
Iteration 87/1000 | Loss: 0.00002571
Iteration 88/1000 | Loss: 0.00002571
Iteration 89/1000 | Loss: 0.00002571
Iteration 90/1000 | Loss: 0.00002571
Iteration 91/1000 | Loss: 0.00002571
Iteration 92/1000 | Loss: 0.00002571
Iteration 93/1000 | Loss: 0.00002571
Iteration 94/1000 | Loss: 0.00002570
Iteration 95/1000 | Loss: 0.00002570
Iteration 96/1000 | Loss: 0.00002570
Iteration 97/1000 | Loss: 0.00002570
Iteration 98/1000 | Loss: 0.00002570
Iteration 99/1000 | Loss: 0.00002570
Iteration 100/1000 | Loss: 0.00002570
Iteration 101/1000 | Loss: 0.00002570
Iteration 102/1000 | Loss: 0.00002570
Iteration 103/1000 | Loss: 0.00002569
Iteration 104/1000 | Loss: 0.00002569
Iteration 105/1000 | Loss: 0.00002569
Iteration 106/1000 | Loss: 0.00002569
Iteration 107/1000 | Loss: 0.00002569
Iteration 108/1000 | Loss: 0.00002569
Iteration 109/1000 | Loss: 0.00002569
Iteration 110/1000 | Loss: 0.00002569
Iteration 111/1000 | Loss: 0.00002569
Iteration 112/1000 | Loss: 0.00002569
Iteration 113/1000 | Loss: 0.00002569
Iteration 114/1000 | Loss: 0.00002568
Iteration 115/1000 | Loss: 0.00002568
Iteration 116/1000 | Loss: 0.00002568
Iteration 117/1000 | Loss: 0.00002568
Iteration 118/1000 | Loss: 0.00002568
Iteration 119/1000 | Loss: 0.00002568
Iteration 120/1000 | Loss: 0.00002568
Iteration 121/1000 | Loss: 0.00002568
Iteration 122/1000 | Loss: 0.00002568
Iteration 123/1000 | Loss: 0.00002568
Iteration 124/1000 | Loss: 0.00002567
Iteration 125/1000 | Loss: 0.00002567
Iteration 126/1000 | Loss: 0.00002567
Iteration 127/1000 | Loss: 0.00002567
Iteration 128/1000 | Loss: 0.00002567
Iteration 129/1000 | Loss: 0.00002567
Iteration 130/1000 | Loss: 0.00002567
Iteration 131/1000 | Loss: 0.00002567
Iteration 132/1000 | Loss: 0.00002566
Iteration 133/1000 | Loss: 0.00002566
Iteration 134/1000 | Loss: 0.00002566
Iteration 135/1000 | Loss: 0.00002566
Iteration 136/1000 | Loss: 0.00002566
Iteration 137/1000 | Loss: 0.00002566
Iteration 138/1000 | Loss: 0.00002566
Iteration 139/1000 | Loss: 0.00002565
Iteration 140/1000 | Loss: 0.00002565
Iteration 141/1000 | Loss: 0.00002565
Iteration 142/1000 | Loss: 0.00002565
Iteration 143/1000 | Loss: 0.00002565
Iteration 144/1000 | Loss: 0.00002565
Iteration 145/1000 | Loss: 0.00002565
Iteration 146/1000 | Loss: 0.00002565
Iteration 147/1000 | Loss: 0.00002565
Iteration 148/1000 | Loss: 0.00002565
Iteration 149/1000 | Loss: 0.00002564
Iteration 150/1000 | Loss: 0.00002564
Iteration 151/1000 | Loss: 0.00002564
Iteration 152/1000 | Loss: 0.00002564
Iteration 153/1000 | Loss: 0.00002564
Iteration 154/1000 | Loss: 0.00002564
Iteration 155/1000 | Loss: 0.00002564
Iteration 156/1000 | Loss: 0.00002563
Iteration 157/1000 | Loss: 0.00002563
Iteration 158/1000 | Loss: 0.00002563
Iteration 159/1000 | Loss: 0.00002563
Iteration 160/1000 | Loss: 0.00002563
Iteration 161/1000 | Loss: 0.00002563
Iteration 162/1000 | Loss: 0.00002563
Iteration 163/1000 | Loss: 0.00002563
Iteration 164/1000 | Loss: 0.00002563
Iteration 165/1000 | Loss: 0.00002563
Iteration 166/1000 | Loss: 0.00002563
Iteration 167/1000 | Loss: 0.00002563
Iteration 168/1000 | Loss: 0.00002563
Iteration 169/1000 | Loss: 0.00002563
Iteration 170/1000 | Loss: 0.00002563
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [2.563180350989569e-05, 2.563180350989569e-05, 2.563180350989569e-05, 2.563180350989569e-05, 2.563180350989569e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.563180350989569e-05

Optimization complete. Final v2v error: 4.365331172943115 mm

Highest mean error: 5.220798492431641 mm for frame 239

Lowest mean error: 3.9008309841156006 mm for frame 48

Saving results

Total time: 68.01033425331116
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_42_us_0617/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_42_us_0617/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_42_us_0617/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00601493
Iteration 2/25 | Loss: 0.00179770
Iteration 3/25 | Loss: 0.00156037
Iteration 4/25 | Loss: 0.00152820
Iteration 5/25 | Loss: 0.00151758
Iteration 6/25 | Loss: 0.00151532
Iteration 7/25 | Loss: 0.00151488
Iteration 8/25 | Loss: 0.00151488
Iteration 9/25 | Loss: 0.00151488
Iteration 10/25 | Loss: 0.00151488
Iteration 11/25 | Loss: 0.00151488
Iteration 12/25 | Loss: 0.00151488
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0015148845268413424, 0.0015148845268413424, 0.0015148845268413424, 0.0015148845268413424, 0.0015148845268413424]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015148845268413424

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47930372
Iteration 2/25 | Loss: 0.00147686
Iteration 3/25 | Loss: 0.00147684
Iteration 4/25 | Loss: 0.00147684
Iteration 5/25 | Loss: 0.00147684
Iteration 6/25 | Loss: 0.00147684
Iteration 7/25 | Loss: 0.00147684
Iteration 8/25 | Loss: 0.00147684
Iteration 9/25 | Loss: 0.00147684
Iteration 10/25 | Loss: 0.00147684
Iteration 11/25 | Loss: 0.00147684
Iteration 12/25 | Loss: 0.00147684
Iteration 13/25 | Loss: 0.00147684
Iteration 14/25 | Loss: 0.00147684
Iteration 15/25 | Loss: 0.00147684
Iteration 16/25 | Loss: 0.00147684
Iteration 17/25 | Loss: 0.00147684
Iteration 18/25 | Loss: 0.00147684
Iteration 19/25 | Loss: 0.00147684
Iteration 20/25 | Loss: 0.00147684
Iteration 21/25 | Loss: 0.00147684
Iteration 22/25 | Loss: 0.00147684
Iteration 23/25 | Loss: 0.00147684
Iteration 24/25 | Loss: 0.00147684
Iteration 25/25 | Loss: 0.00147684

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00147684
Iteration 2/1000 | Loss: 0.00005309
Iteration 3/1000 | Loss: 0.00004264
Iteration 4/1000 | Loss: 0.00003756
Iteration 5/1000 | Loss: 0.00003580
Iteration 6/1000 | Loss: 0.00003456
Iteration 7/1000 | Loss: 0.00003347
Iteration 8/1000 | Loss: 0.00003271
Iteration 9/1000 | Loss: 0.00003213
Iteration 10/1000 | Loss: 0.00003171
Iteration 11/1000 | Loss: 0.00003141
Iteration 12/1000 | Loss: 0.00003123
Iteration 13/1000 | Loss: 0.00003116
Iteration 14/1000 | Loss: 0.00003112
Iteration 15/1000 | Loss: 0.00003110
Iteration 16/1000 | Loss: 0.00003110
Iteration 17/1000 | Loss: 0.00003109
Iteration 18/1000 | Loss: 0.00003108
Iteration 19/1000 | Loss: 0.00003108
Iteration 20/1000 | Loss: 0.00003107
Iteration 21/1000 | Loss: 0.00003106
Iteration 22/1000 | Loss: 0.00003106
Iteration 23/1000 | Loss: 0.00003105
Iteration 24/1000 | Loss: 0.00003105
Iteration 25/1000 | Loss: 0.00003105
Iteration 26/1000 | Loss: 0.00003103
Iteration 27/1000 | Loss: 0.00003102
Iteration 28/1000 | Loss: 0.00003102
Iteration 29/1000 | Loss: 0.00003102
Iteration 30/1000 | Loss: 0.00003102
Iteration 31/1000 | Loss: 0.00003101
Iteration 32/1000 | Loss: 0.00003101
Iteration 33/1000 | Loss: 0.00003101
Iteration 34/1000 | Loss: 0.00003101
Iteration 35/1000 | Loss: 0.00003101
Iteration 36/1000 | Loss: 0.00003100
Iteration 37/1000 | Loss: 0.00003100
Iteration 38/1000 | Loss: 0.00003100
Iteration 39/1000 | Loss: 0.00003100
Iteration 40/1000 | Loss: 0.00003100
Iteration 41/1000 | Loss: 0.00003100
Iteration 42/1000 | Loss: 0.00003100
Iteration 43/1000 | Loss: 0.00003100
Iteration 44/1000 | Loss: 0.00003100
Iteration 45/1000 | Loss: 0.00003100
Iteration 46/1000 | Loss: 0.00003100
Iteration 47/1000 | Loss: 0.00003100
Iteration 48/1000 | Loss: 0.00003099
Iteration 49/1000 | Loss: 0.00003099
Iteration 50/1000 | Loss: 0.00003099
Iteration 51/1000 | Loss: 0.00003099
Iteration 52/1000 | Loss: 0.00003099
Iteration 53/1000 | Loss: 0.00003099
Iteration 54/1000 | Loss: 0.00003099
Iteration 55/1000 | Loss: 0.00003098
Iteration 56/1000 | Loss: 0.00003098
Iteration 57/1000 | Loss: 0.00003098
Iteration 58/1000 | Loss: 0.00003098
Iteration 59/1000 | Loss: 0.00003097
Iteration 60/1000 | Loss: 0.00003097
Iteration 61/1000 | Loss: 0.00003097
Iteration 62/1000 | Loss: 0.00003096
Iteration 63/1000 | Loss: 0.00003096
Iteration 64/1000 | Loss: 0.00003095
Iteration 65/1000 | Loss: 0.00003095
Iteration 66/1000 | Loss: 0.00003095
Iteration 67/1000 | Loss: 0.00003095
Iteration 68/1000 | Loss: 0.00003094
Iteration 69/1000 | Loss: 0.00003094
Iteration 70/1000 | Loss: 0.00003094
Iteration 71/1000 | Loss: 0.00003094
Iteration 72/1000 | Loss: 0.00003094
Iteration 73/1000 | Loss: 0.00003093
Iteration 74/1000 | Loss: 0.00003093
Iteration 75/1000 | Loss: 0.00003093
Iteration 76/1000 | Loss: 0.00003092
Iteration 77/1000 | Loss: 0.00003092
Iteration 78/1000 | Loss: 0.00003092
Iteration 79/1000 | Loss: 0.00003092
Iteration 80/1000 | Loss: 0.00003092
Iteration 81/1000 | Loss: 0.00003091
Iteration 82/1000 | Loss: 0.00003091
Iteration 83/1000 | Loss: 0.00003091
Iteration 84/1000 | Loss: 0.00003091
Iteration 85/1000 | Loss: 0.00003091
Iteration 86/1000 | Loss: 0.00003091
Iteration 87/1000 | Loss: 0.00003091
Iteration 88/1000 | Loss: 0.00003091
Iteration 89/1000 | Loss: 0.00003091
Iteration 90/1000 | Loss: 0.00003091
Iteration 91/1000 | Loss: 0.00003091
Iteration 92/1000 | Loss: 0.00003090
Iteration 93/1000 | Loss: 0.00003090
Iteration 94/1000 | Loss: 0.00003090
Iteration 95/1000 | Loss: 0.00003090
Iteration 96/1000 | Loss: 0.00003090
Iteration 97/1000 | Loss: 0.00003090
Iteration 98/1000 | Loss: 0.00003090
Iteration 99/1000 | Loss: 0.00003090
Iteration 100/1000 | Loss: 0.00003090
Iteration 101/1000 | Loss: 0.00003090
Iteration 102/1000 | Loss: 0.00003089
Iteration 103/1000 | Loss: 0.00003089
Iteration 104/1000 | Loss: 0.00003089
Iteration 105/1000 | Loss: 0.00003089
Iteration 106/1000 | Loss: 0.00003089
Iteration 107/1000 | Loss: 0.00003089
Iteration 108/1000 | Loss: 0.00003089
Iteration 109/1000 | Loss: 0.00003089
Iteration 110/1000 | Loss: 0.00003089
Iteration 111/1000 | Loss: 0.00003089
Iteration 112/1000 | Loss: 0.00003089
Iteration 113/1000 | Loss: 0.00003089
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [3.089020538027398e-05, 3.089020538027398e-05, 3.089020538027398e-05, 3.089020538027398e-05, 3.089020538027398e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.089020538027398e-05

Optimization complete. Final v2v error: 4.74502420425415 mm

Highest mean error: 5.4221062660217285 mm for frame 47

Lowest mean error: 4.258989334106445 mm for frame 96

Saving results

Total time: 36.340625524520874
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_42_us_0617/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_42_us_0617/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_42_us_0617/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00344297
Iteration 2/25 | Loss: 0.00165387
Iteration 3/25 | Loss: 0.00156488
Iteration 4/25 | Loss: 0.00153183
Iteration 5/25 | Loss: 0.00152321
Iteration 6/25 | Loss: 0.00152092
Iteration 7/25 | Loss: 0.00151942
Iteration 8/25 | Loss: 0.00151911
Iteration 9/25 | Loss: 0.00151911
Iteration 10/25 | Loss: 0.00151911
Iteration 11/25 | Loss: 0.00151911
Iteration 12/25 | Loss: 0.00151911
Iteration 13/25 | Loss: 0.00151911
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0015191115671768785, 0.0015191115671768785, 0.0015191115671768785, 0.0015191115671768785, 0.0015191115671768785]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015191115671768785

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37965703
Iteration 2/25 | Loss: 0.00227174
Iteration 3/25 | Loss: 0.00227174
Iteration 4/25 | Loss: 0.00227174
Iteration 5/25 | Loss: 0.00227174
Iteration 6/25 | Loss: 0.00227174
Iteration 7/25 | Loss: 0.00227174
Iteration 8/25 | Loss: 0.00227174
Iteration 9/25 | Loss: 0.00227174
Iteration 10/25 | Loss: 0.00227173
Iteration 11/25 | Loss: 0.00227173
Iteration 12/25 | Loss: 0.00227173
Iteration 13/25 | Loss: 0.00227173
Iteration 14/25 | Loss: 0.00227173
Iteration 15/25 | Loss: 0.00227173
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0022717348765581846, 0.0022717348765581846, 0.0022717348765581846, 0.0022717348765581846, 0.0022717348765581846]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022717348765581846

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00227173
Iteration 2/1000 | Loss: 0.00006667
Iteration 3/1000 | Loss: 0.00003898
Iteration 4/1000 | Loss: 0.00003417
Iteration 5/1000 | Loss: 0.00003128
Iteration 6/1000 | Loss: 0.00002998
Iteration 7/1000 | Loss: 0.00002879
Iteration 8/1000 | Loss: 0.00002779
Iteration 9/1000 | Loss: 0.00002717
Iteration 10/1000 | Loss: 0.00002681
Iteration 11/1000 | Loss: 0.00002646
Iteration 12/1000 | Loss: 0.00002612
Iteration 13/1000 | Loss: 0.00002594
Iteration 14/1000 | Loss: 0.00002588
Iteration 15/1000 | Loss: 0.00002578
Iteration 16/1000 | Loss: 0.00002575
Iteration 17/1000 | Loss: 0.00002571
Iteration 18/1000 | Loss: 0.00002571
Iteration 19/1000 | Loss: 0.00002571
Iteration 20/1000 | Loss: 0.00002570
Iteration 21/1000 | Loss: 0.00002570
Iteration 22/1000 | Loss: 0.00002569
Iteration 23/1000 | Loss: 0.00002569
Iteration 24/1000 | Loss: 0.00002567
Iteration 25/1000 | Loss: 0.00002565
Iteration 26/1000 | Loss: 0.00002565
Iteration 27/1000 | Loss: 0.00002565
Iteration 28/1000 | Loss: 0.00002564
Iteration 29/1000 | Loss: 0.00002564
Iteration 30/1000 | Loss: 0.00002563
Iteration 31/1000 | Loss: 0.00002562
Iteration 32/1000 | Loss: 0.00002562
Iteration 33/1000 | Loss: 0.00002562
Iteration 34/1000 | Loss: 0.00002561
Iteration 35/1000 | Loss: 0.00002561
Iteration 36/1000 | Loss: 0.00002561
Iteration 37/1000 | Loss: 0.00002560
Iteration 38/1000 | Loss: 0.00002560
Iteration 39/1000 | Loss: 0.00002559
Iteration 40/1000 | Loss: 0.00002559
Iteration 41/1000 | Loss: 0.00002558
Iteration 42/1000 | Loss: 0.00002558
Iteration 43/1000 | Loss: 0.00002558
Iteration 44/1000 | Loss: 0.00002557
Iteration 45/1000 | Loss: 0.00002557
Iteration 46/1000 | Loss: 0.00002557
Iteration 47/1000 | Loss: 0.00002556
Iteration 48/1000 | Loss: 0.00002556
Iteration 49/1000 | Loss: 0.00002555
Iteration 50/1000 | Loss: 0.00002555
Iteration 51/1000 | Loss: 0.00002555
Iteration 52/1000 | Loss: 0.00002555
Iteration 53/1000 | Loss: 0.00002554
Iteration 54/1000 | Loss: 0.00002554
Iteration 55/1000 | Loss: 0.00002553
Iteration 56/1000 | Loss: 0.00002552
Iteration 57/1000 | Loss: 0.00002551
Iteration 58/1000 | Loss: 0.00002551
Iteration 59/1000 | Loss: 0.00002551
Iteration 60/1000 | Loss: 0.00002551
Iteration 61/1000 | Loss: 0.00002551
Iteration 62/1000 | Loss: 0.00002550
Iteration 63/1000 | Loss: 0.00002550
Iteration 64/1000 | Loss: 0.00002550
Iteration 65/1000 | Loss: 0.00002549
Iteration 66/1000 | Loss: 0.00002549
Iteration 67/1000 | Loss: 0.00002549
Iteration 68/1000 | Loss: 0.00002549
Iteration 69/1000 | Loss: 0.00002549
Iteration 70/1000 | Loss: 0.00002549
Iteration 71/1000 | Loss: 0.00002549
Iteration 72/1000 | Loss: 0.00002549
Iteration 73/1000 | Loss: 0.00002548
Iteration 74/1000 | Loss: 0.00002548
Iteration 75/1000 | Loss: 0.00002548
Iteration 76/1000 | Loss: 0.00002548
Iteration 77/1000 | Loss: 0.00002547
Iteration 78/1000 | Loss: 0.00002547
Iteration 79/1000 | Loss: 0.00002547
Iteration 80/1000 | Loss: 0.00002547
Iteration 81/1000 | Loss: 0.00002547
Iteration 82/1000 | Loss: 0.00002547
Iteration 83/1000 | Loss: 0.00002547
Iteration 84/1000 | Loss: 0.00002547
Iteration 85/1000 | Loss: 0.00002547
Iteration 86/1000 | Loss: 0.00002547
Iteration 87/1000 | Loss: 0.00002546
Iteration 88/1000 | Loss: 0.00002546
Iteration 89/1000 | Loss: 0.00002546
Iteration 90/1000 | Loss: 0.00002546
Iteration 91/1000 | Loss: 0.00002546
Iteration 92/1000 | Loss: 0.00002546
Iteration 93/1000 | Loss: 0.00002546
Iteration 94/1000 | Loss: 0.00002545
Iteration 95/1000 | Loss: 0.00002545
Iteration 96/1000 | Loss: 0.00002545
Iteration 97/1000 | Loss: 0.00002545
Iteration 98/1000 | Loss: 0.00002545
Iteration 99/1000 | Loss: 0.00002544
Iteration 100/1000 | Loss: 0.00002544
Iteration 101/1000 | Loss: 0.00002544
Iteration 102/1000 | Loss: 0.00002544
Iteration 103/1000 | Loss: 0.00002544
Iteration 104/1000 | Loss: 0.00002543
Iteration 105/1000 | Loss: 0.00002543
Iteration 106/1000 | Loss: 0.00002543
Iteration 107/1000 | Loss: 0.00002543
Iteration 108/1000 | Loss: 0.00002542
Iteration 109/1000 | Loss: 0.00002542
Iteration 110/1000 | Loss: 0.00002542
Iteration 111/1000 | Loss: 0.00002542
Iteration 112/1000 | Loss: 0.00002542
Iteration 113/1000 | Loss: 0.00002541
Iteration 114/1000 | Loss: 0.00002541
Iteration 115/1000 | Loss: 0.00002541
Iteration 116/1000 | Loss: 0.00002541
Iteration 117/1000 | Loss: 0.00002541
Iteration 118/1000 | Loss: 0.00002540
Iteration 119/1000 | Loss: 0.00002540
Iteration 120/1000 | Loss: 0.00002540
Iteration 121/1000 | Loss: 0.00002539
Iteration 122/1000 | Loss: 0.00002539
Iteration 123/1000 | Loss: 0.00002539
Iteration 124/1000 | Loss: 0.00002539
Iteration 125/1000 | Loss: 0.00002539
Iteration 126/1000 | Loss: 0.00002538
Iteration 127/1000 | Loss: 0.00002538
Iteration 128/1000 | Loss: 0.00002538
Iteration 129/1000 | Loss: 0.00002538
Iteration 130/1000 | Loss: 0.00002537
Iteration 131/1000 | Loss: 0.00002537
Iteration 132/1000 | Loss: 0.00002537
Iteration 133/1000 | Loss: 0.00002536
Iteration 134/1000 | Loss: 0.00002536
Iteration 135/1000 | Loss: 0.00002536
Iteration 136/1000 | Loss: 0.00002535
Iteration 137/1000 | Loss: 0.00002535
Iteration 138/1000 | Loss: 0.00002535
Iteration 139/1000 | Loss: 0.00002535
Iteration 140/1000 | Loss: 0.00002535
Iteration 141/1000 | Loss: 0.00002535
Iteration 142/1000 | Loss: 0.00002535
Iteration 143/1000 | Loss: 0.00002535
Iteration 144/1000 | Loss: 0.00002535
Iteration 145/1000 | Loss: 0.00002535
Iteration 146/1000 | Loss: 0.00002534
Iteration 147/1000 | Loss: 0.00002534
Iteration 148/1000 | Loss: 0.00002534
Iteration 149/1000 | Loss: 0.00002534
Iteration 150/1000 | Loss: 0.00002534
Iteration 151/1000 | Loss: 0.00002534
Iteration 152/1000 | Loss: 0.00002534
Iteration 153/1000 | Loss: 0.00002534
Iteration 154/1000 | Loss: 0.00002534
Iteration 155/1000 | Loss: 0.00002533
Iteration 156/1000 | Loss: 0.00002533
Iteration 157/1000 | Loss: 0.00002533
Iteration 158/1000 | Loss: 0.00002533
Iteration 159/1000 | Loss: 0.00002533
Iteration 160/1000 | Loss: 0.00002533
Iteration 161/1000 | Loss: 0.00002532
Iteration 162/1000 | Loss: 0.00002532
Iteration 163/1000 | Loss: 0.00002532
Iteration 164/1000 | Loss: 0.00002532
Iteration 165/1000 | Loss: 0.00002532
Iteration 166/1000 | Loss: 0.00002531
Iteration 167/1000 | Loss: 0.00002531
Iteration 168/1000 | Loss: 0.00002531
Iteration 169/1000 | Loss: 0.00002531
Iteration 170/1000 | Loss: 0.00002531
Iteration 171/1000 | Loss: 0.00002531
Iteration 172/1000 | Loss: 0.00002531
Iteration 173/1000 | Loss: 0.00002531
Iteration 174/1000 | Loss: 0.00002531
Iteration 175/1000 | Loss: 0.00002531
Iteration 176/1000 | Loss: 0.00002531
Iteration 177/1000 | Loss: 0.00002531
Iteration 178/1000 | Loss: 0.00002531
Iteration 179/1000 | Loss: 0.00002531
Iteration 180/1000 | Loss: 0.00002531
Iteration 181/1000 | Loss: 0.00002531
Iteration 182/1000 | Loss: 0.00002531
Iteration 183/1000 | Loss: 0.00002531
Iteration 184/1000 | Loss: 0.00002531
Iteration 185/1000 | Loss: 0.00002531
Iteration 186/1000 | Loss: 0.00002531
Iteration 187/1000 | Loss: 0.00002531
Iteration 188/1000 | Loss: 0.00002531
Iteration 189/1000 | Loss: 0.00002531
Iteration 190/1000 | Loss: 0.00002531
Iteration 191/1000 | Loss: 0.00002531
Iteration 192/1000 | Loss: 0.00002531
Iteration 193/1000 | Loss: 0.00002531
Iteration 194/1000 | Loss: 0.00002531
Iteration 195/1000 | Loss: 0.00002531
Iteration 196/1000 | Loss: 0.00002531
Iteration 197/1000 | Loss: 0.00002531
Iteration 198/1000 | Loss: 0.00002531
Iteration 199/1000 | Loss: 0.00002531
Iteration 200/1000 | Loss: 0.00002531
Iteration 201/1000 | Loss: 0.00002531
Iteration 202/1000 | Loss: 0.00002531
Iteration 203/1000 | Loss: 0.00002531
Iteration 204/1000 | Loss: 0.00002531
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [2.5307866962975822e-05, 2.5307866962975822e-05, 2.5307866962975822e-05, 2.5307866962975822e-05, 2.5307866962975822e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5307866962975822e-05

Optimization complete. Final v2v error: 4.312455654144287 mm

Highest mean error: 4.599966049194336 mm for frame 139

Lowest mean error: 4.136009693145752 mm for frame 88

Saving results

Total time: 46.35838508605957
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_42_us_0617/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_42_us_0617/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_42_us_0617/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00511366
Iteration 2/25 | Loss: 0.00161524
Iteration 3/25 | Loss: 0.00147171
Iteration 4/25 | Loss: 0.00145240
Iteration 5/25 | Loss: 0.00144809
Iteration 6/25 | Loss: 0.00144750
Iteration 7/25 | Loss: 0.00144750
Iteration 8/25 | Loss: 0.00144750
Iteration 9/25 | Loss: 0.00144750
Iteration 10/25 | Loss: 0.00144750
Iteration 11/25 | Loss: 0.00144750
Iteration 12/25 | Loss: 0.00144750
Iteration 13/25 | Loss: 0.00144750
Iteration 14/25 | Loss: 0.00144750
Iteration 15/25 | Loss: 0.00144750
Iteration 16/25 | Loss: 0.00144750
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0014475033385679126, 0.0014475033385679126, 0.0014475033385679126, 0.0014475033385679126, 0.0014475033385679126]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014475033385679126

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.32371807
Iteration 2/25 | Loss: 0.00155182
Iteration 3/25 | Loss: 0.00155178
Iteration 4/25 | Loss: 0.00155177
Iteration 5/25 | Loss: 0.00155177
Iteration 6/25 | Loss: 0.00155177
Iteration 7/25 | Loss: 0.00155177
Iteration 8/25 | Loss: 0.00155177
Iteration 9/25 | Loss: 0.00155177
Iteration 10/25 | Loss: 0.00155177
Iteration 11/25 | Loss: 0.00155177
Iteration 12/25 | Loss: 0.00155177
Iteration 13/25 | Loss: 0.00155177
Iteration 14/25 | Loss: 0.00155177
Iteration 15/25 | Loss: 0.00155177
Iteration 16/25 | Loss: 0.00155177
Iteration 17/25 | Loss: 0.00155177
Iteration 18/25 | Loss: 0.00155177
Iteration 19/25 | Loss: 0.00155177
Iteration 20/25 | Loss: 0.00155177
Iteration 21/25 | Loss: 0.00155177
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0015517728170379996, 0.0015517728170379996, 0.0015517728170379996, 0.0015517728170379996, 0.0015517728170379996]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015517728170379996

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00155177
Iteration 2/1000 | Loss: 0.00004057
Iteration 3/1000 | Loss: 0.00003356
Iteration 4/1000 | Loss: 0.00003062
Iteration 5/1000 | Loss: 0.00002924
Iteration 6/1000 | Loss: 0.00002832
Iteration 7/1000 | Loss: 0.00002749
Iteration 8/1000 | Loss: 0.00002690
Iteration 9/1000 | Loss: 0.00002656
Iteration 10/1000 | Loss: 0.00002631
Iteration 11/1000 | Loss: 0.00002624
Iteration 12/1000 | Loss: 0.00002612
Iteration 13/1000 | Loss: 0.00002610
Iteration 14/1000 | Loss: 0.00002602
Iteration 15/1000 | Loss: 0.00002599
Iteration 16/1000 | Loss: 0.00002598
Iteration 17/1000 | Loss: 0.00002597
Iteration 18/1000 | Loss: 0.00002597
Iteration 19/1000 | Loss: 0.00002597
Iteration 20/1000 | Loss: 0.00002596
Iteration 21/1000 | Loss: 0.00002595
Iteration 22/1000 | Loss: 0.00002595
Iteration 23/1000 | Loss: 0.00002594
Iteration 24/1000 | Loss: 0.00002593
Iteration 25/1000 | Loss: 0.00002589
Iteration 26/1000 | Loss: 0.00002577
Iteration 27/1000 | Loss: 0.00002569
Iteration 28/1000 | Loss: 0.00002569
Iteration 29/1000 | Loss: 0.00002568
Iteration 30/1000 | Loss: 0.00002567
Iteration 31/1000 | Loss: 0.00002566
Iteration 32/1000 | Loss: 0.00002564
Iteration 33/1000 | Loss: 0.00002563
Iteration 34/1000 | Loss: 0.00002563
Iteration 35/1000 | Loss: 0.00002563
Iteration 36/1000 | Loss: 0.00002562
Iteration 37/1000 | Loss: 0.00002560
Iteration 38/1000 | Loss: 0.00002559
Iteration 39/1000 | Loss: 0.00002558
Iteration 40/1000 | Loss: 0.00002558
Iteration 41/1000 | Loss: 0.00002557
Iteration 42/1000 | Loss: 0.00002557
Iteration 43/1000 | Loss: 0.00002557
Iteration 44/1000 | Loss: 0.00002557
Iteration 45/1000 | Loss: 0.00002556
Iteration 46/1000 | Loss: 0.00002556
Iteration 47/1000 | Loss: 0.00002556
Iteration 48/1000 | Loss: 0.00002555
Iteration 49/1000 | Loss: 0.00002555
Iteration 50/1000 | Loss: 0.00002555
Iteration 51/1000 | Loss: 0.00002555
Iteration 52/1000 | Loss: 0.00002555
Iteration 53/1000 | Loss: 0.00002555
Iteration 54/1000 | Loss: 0.00002555
Iteration 55/1000 | Loss: 0.00002555
Iteration 56/1000 | Loss: 0.00002554
Iteration 57/1000 | Loss: 0.00002554
Iteration 58/1000 | Loss: 0.00002554
Iteration 59/1000 | Loss: 0.00002554
Iteration 60/1000 | Loss: 0.00002554
Iteration 61/1000 | Loss: 0.00002553
Iteration 62/1000 | Loss: 0.00002553
Iteration 63/1000 | Loss: 0.00002552
Iteration 64/1000 | Loss: 0.00002552
Iteration 65/1000 | Loss: 0.00002552
Iteration 66/1000 | Loss: 0.00002551
Iteration 67/1000 | Loss: 0.00002551
Iteration 68/1000 | Loss: 0.00002551
Iteration 69/1000 | Loss: 0.00002551
Iteration 70/1000 | Loss: 0.00002550
Iteration 71/1000 | Loss: 0.00002550
Iteration 72/1000 | Loss: 0.00002549
Iteration 73/1000 | Loss: 0.00002549
Iteration 74/1000 | Loss: 0.00002548
Iteration 75/1000 | Loss: 0.00002548
Iteration 76/1000 | Loss: 0.00002548
Iteration 77/1000 | Loss: 0.00002548
Iteration 78/1000 | Loss: 0.00002548
Iteration 79/1000 | Loss: 0.00002548
Iteration 80/1000 | Loss: 0.00002548
Iteration 81/1000 | Loss: 0.00002548
Iteration 82/1000 | Loss: 0.00002548
Iteration 83/1000 | Loss: 0.00002548
Iteration 84/1000 | Loss: 0.00002548
Iteration 85/1000 | Loss: 0.00002548
Iteration 86/1000 | Loss: 0.00002548
Iteration 87/1000 | Loss: 0.00002547
Iteration 88/1000 | Loss: 0.00002547
Iteration 89/1000 | Loss: 0.00002547
Iteration 90/1000 | Loss: 0.00002547
Iteration 91/1000 | Loss: 0.00002547
Iteration 92/1000 | Loss: 0.00002546
Iteration 93/1000 | Loss: 0.00002546
Iteration 94/1000 | Loss: 0.00002546
Iteration 95/1000 | Loss: 0.00002546
Iteration 96/1000 | Loss: 0.00002546
Iteration 97/1000 | Loss: 0.00002546
Iteration 98/1000 | Loss: 0.00002546
Iteration 99/1000 | Loss: 0.00002546
Iteration 100/1000 | Loss: 0.00002546
Iteration 101/1000 | Loss: 0.00002546
Iteration 102/1000 | Loss: 0.00002545
Iteration 103/1000 | Loss: 0.00002545
Iteration 104/1000 | Loss: 0.00002545
Iteration 105/1000 | Loss: 0.00002545
Iteration 106/1000 | Loss: 0.00002545
Iteration 107/1000 | Loss: 0.00002545
Iteration 108/1000 | Loss: 0.00002545
Iteration 109/1000 | Loss: 0.00002545
Iteration 110/1000 | Loss: 0.00002545
Iteration 111/1000 | Loss: 0.00002545
Iteration 112/1000 | Loss: 0.00002545
Iteration 113/1000 | Loss: 0.00002545
Iteration 114/1000 | Loss: 0.00002545
Iteration 115/1000 | Loss: 0.00002545
Iteration 116/1000 | Loss: 0.00002545
Iteration 117/1000 | Loss: 0.00002544
Iteration 118/1000 | Loss: 0.00002544
Iteration 119/1000 | Loss: 0.00002544
Iteration 120/1000 | Loss: 0.00002544
Iteration 121/1000 | Loss: 0.00002544
Iteration 122/1000 | Loss: 0.00002544
Iteration 123/1000 | Loss: 0.00002544
Iteration 124/1000 | Loss: 0.00002544
Iteration 125/1000 | Loss: 0.00002543
Iteration 126/1000 | Loss: 0.00002543
Iteration 127/1000 | Loss: 0.00002543
Iteration 128/1000 | Loss: 0.00002543
Iteration 129/1000 | Loss: 0.00002543
Iteration 130/1000 | Loss: 0.00002543
Iteration 131/1000 | Loss: 0.00002543
Iteration 132/1000 | Loss: 0.00002543
Iteration 133/1000 | Loss: 0.00002543
Iteration 134/1000 | Loss: 0.00002543
Iteration 135/1000 | Loss: 0.00002543
Iteration 136/1000 | Loss: 0.00002543
Iteration 137/1000 | Loss: 0.00002543
Iteration 138/1000 | Loss: 0.00002543
Iteration 139/1000 | Loss: 0.00002543
Iteration 140/1000 | Loss: 0.00002543
Iteration 141/1000 | Loss: 0.00002543
Iteration 142/1000 | Loss: 0.00002543
Iteration 143/1000 | Loss: 0.00002543
Iteration 144/1000 | Loss: 0.00002543
Iteration 145/1000 | Loss: 0.00002543
Iteration 146/1000 | Loss: 0.00002543
Iteration 147/1000 | Loss: 0.00002543
Iteration 148/1000 | Loss: 0.00002543
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [2.542561196605675e-05, 2.542561196605675e-05, 2.542561196605675e-05, 2.542561196605675e-05, 2.542561196605675e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.542561196605675e-05

Optimization complete. Final v2v error: 4.345334529876709 mm

Highest mean error: 4.802220344543457 mm for frame 85

Lowest mean error: 3.87275767326355 mm for frame 0

Saving results

Total time: 44.521970987319946
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_42_us_0617/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_42_us_0617/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_42_us_0617/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00619215
Iteration 2/25 | Loss: 0.00170510
Iteration 3/25 | Loss: 0.00157587
Iteration 4/25 | Loss: 0.00155910
Iteration 5/25 | Loss: 0.00153524
Iteration 6/25 | Loss: 0.00152079
Iteration 7/25 | Loss: 0.00151666
Iteration 8/25 | Loss: 0.00150360
Iteration 9/25 | Loss: 0.00150301
Iteration 10/25 | Loss: 0.00150285
Iteration 11/25 | Loss: 0.00150279
Iteration 12/25 | Loss: 0.00150279
Iteration 13/25 | Loss: 0.00150279
Iteration 14/25 | Loss: 0.00150278
Iteration 15/25 | Loss: 0.00150278
Iteration 16/25 | Loss: 0.00150278
Iteration 17/25 | Loss: 0.00150278
Iteration 18/25 | Loss: 0.00150278
Iteration 19/25 | Loss: 0.00150278
Iteration 20/25 | Loss: 0.00150278
Iteration 21/25 | Loss: 0.00150278
Iteration 22/25 | Loss: 0.00150277
Iteration 23/25 | Loss: 0.00150277
Iteration 24/25 | Loss: 0.00150277
Iteration 25/25 | Loss: 0.00150277

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.93009460
Iteration 2/25 | Loss: 0.00188637
Iteration 3/25 | Loss: 0.00188635
Iteration 4/25 | Loss: 0.00188635
Iteration 5/25 | Loss: 0.00188635
Iteration 6/25 | Loss: 0.00188635
Iteration 7/25 | Loss: 0.00188635
Iteration 8/25 | Loss: 0.00188635
Iteration 9/25 | Loss: 0.00188635
Iteration 10/25 | Loss: 0.00188635
Iteration 11/25 | Loss: 0.00188635
Iteration 12/25 | Loss: 0.00188635
Iteration 13/25 | Loss: 0.00188635
Iteration 14/25 | Loss: 0.00188635
Iteration 15/25 | Loss: 0.00188635
Iteration 16/25 | Loss: 0.00188635
Iteration 17/25 | Loss: 0.00188635
Iteration 18/25 | Loss: 0.00188635
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0018863523146137595, 0.0018863523146137595, 0.0018863523146137595, 0.0018863523146137595, 0.0018863523146137595]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018863523146137595

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00188635
Iteration 2/1000 | Loss: 0.00006090
Iteration 3/1000 | Loss: 0.00004350
Iteration 4/1000 | Loss: 0.00003763
Iteration 5/1000 | Loss: 0.00003564
Iteration 6/1000 | Loss: 0.00003412
Iteration 7/1000 | Loss: 0.00003315
Iteration 8/1000 | Loss: 0.00003240
Iteration 9/1000 | Loss: 0.00003181
Iteration 10/1000 | Loss: 0.00003136
Iteration 11/1000 | Loss: 0.00003115
Iteration 12/1000 | Loss: 0.00003111
Iteration 13/1000 | Loss: 0.00003094
Iteration 14/1000 | Loss: 0.00003091
Iteration 15/1000 | Loss: 0.00003085
Iteration 16/1000 | Loss: 0.00003076
Iteration 17/1000 | Loss: 0.00003069
Iteration 18/1000 | Loss: 0.00003069
Iteration 19/1000 | Loss: 0.00003064
Iteration 20/1000 | Loss: 0.00003064
Iteration 21/1000 | Loss: 0.00003061
Iteration 22/1000 | Loss: 0.00003061
Iteration 23/1000 | Loss: 0.00003060
Iteration 24/1000 | Loss: 0.00003060
Iteration 25/1000 | Loss: 0.00003060
Iteration 26/1000 | Loss: 0.00003059
Iteration 27/1000 | Loss: 0.00003059
Iteration 28/1000 | Loss: 0.00003059
Iteration 29/1000 | Loss: 0.00003058
Iteration 30/1000 | Loss: 0.00003058
Iteration 31/1000 | Loss: 0.00003058
Iteration 32/1000 | Loss: 0.00003058
Iteration 33/1000 | Loss: 0.00003058
Iteration 34/1000 | Loss: 0.00003058
Iteration 35/1000 | Loss: 0.00003058
Iteration 36/1000 | Loss: 0.00003058
Iteration 37/1000 | Loss: 0.00003058
Iteration 38/1000 | Loss: 0.00003057
Iteration 39/1000 | Loss: 0.00003057
Iteration 40/1000 | Loss: 0.00003057
Iteration 41/1000 | Loss: 0.00003056
Iteration 42/1000 | Loss: 0.00003055
Iteration 43/1000 | Loss: 0.00003055
Iteration 44/1000 | Loss: 0.00003055
Iteration 45/1000 | Loss: 0.00003054
Iteration 46/1000 | Loss: 0.00003054
Iteration 47/1000 | Loss: 0.00003054
Iteration 48/1000 | Loss: 0.00003053
Iteration 49/1000 | Loss: 0.00003053
Iteration 50/1000 | Loss: 0.00003053
Iteration 51/1000 | Loss: 0.00003052
Iteration 52/1000 | Loss: 0.00003051
Iteration 53/1000 | Loss: 0.00003051
Iteration 54/1000 | Loss: 0.00003051
Iteration 55/1000 | Loss: 0.00003051
Iteration 56/1000 | Loss: 0.00003050
Iteration 57/1000 | Loss: 0.00003050
Iteration 58/1000 | Loss: 0.00003050
Iteration 59/1000 | Loss: 0.00003049
Iteration 60/1000 | Loss: 0.00003049
Iteration 61/1000 | Loss: 0.00003049
Iteration 62/1000 | Loss: 0.00003048
Iteration 63/1000 | Loss: 0.00003048
Iteration 64/1000 | Loss: 0.00003048
Iteration 65/1000 | Loss: 0.00003047
Iteration 66/1000 | Loss: 0.00003047
Iteration 67/1000 | Loss: 0.00003047
Iteration 68/1000 | Loss: 0.00003047
Iteration 69/1000 | Loss: 0.00003046
Iteration 70/1000 | Loss: 0.00003046
Iteration 71/1000 | Loss: 0.00003046
Iteration 72/1000 | Loss: 0.00003045
Iteration 73/1000 | Loss: 0.00003045
Iteration 74/1000 | Loss: 0.00003045
Iteration 75/1000 | Loss: 0.00003045
Iteration 76/1000 | Loss: 0.00003045
Iteration 77/1000 | Loss: 0.00003045
Iteration 78/1000 | Loss: 0.00003044
Iteration 79/1000 | Loss: 0.00003044
Iteration 80/1000 | Loss: 0.00003044
Iteration 81/1000 | Loss: 0.00003044
Iteration 82/1000 | Loss: 0.00003044
Iteration 83/1000 | Loss: 0.00003044
Iteration 84/1000 | Loss: 0.00003044
Iteration 85/1000 | Loss: 0.00003043
Iteration 86/1000 | Loss: 0.00003043
Iteration 87/1000 | Loss: 0.00003043
Iteration 88/1000 | Loss: 0.00003043
Iteration 89/1000 | Loss: 0.00003043
Iteration 90/1000 | Loss: 0.00003043
Iteration 91/1000 | Loss: 0.00003043
Iteration 92/1000 | Loss: 0.00003043
Iteration 93/1000 | Loss: 0.00003043
Iteration 94/1000 | Loss: 0.00003043
Iteration 95/1000 | Loss: 0.00003043
Iteration 96/1000 | Loss: 0.00003042
Iteration 97/1000 | Loss: 0.00003042
Iteration 98/1000 | Loss: 0.00003042
Iteration 99/1000 | Loss: 0.00003042
Iteration 100/1000 | Loss: 0.00003042
Iteration 101/1000 | Loss: 0.00003042
Iteration 102/1000 | Loss: 0.00003042
Iteration 103/1000 | Loss: 0.00003042
Iteration 104/1000 | Loss: 0.00003041
Iteration 105/1000 | Loss: 0.00003041
Iteration 106/1000 | Loss: 0.00003041
Iteration 107/1000 | Loss: 0.00003041
Iteration 108/1000 | Loss: 0.00003041
Iteration 109/1000 | Loss: 0.00003041
Iteration 110/1000 | Loss: 0.00003041
Iteration 111/1000 | Loss: 0.00003041
Iteration 112/1000 | Loss: 0.00003041
Iteration 113/1000 | Loss: 0.00003041
Iteration 114/1000 | Loss: 0.00003040
Iteration 115/1000 | Loss: 0.00003040
Iteration 116/1000 | Loss: 0.00003040
Iteration 117/1000 | Loss: 0.00003040
Iteration 118/1000 | Loss: 0.00003040
Iteration 119/1000 | Loss: 0.00003040
Iteration 120/1000 | Loss: 0.00003040
Iteration 121/1000 | Loss: 0.00003040
Iteration 122/1000 | Loss: 0.00003040
Iteration 123/1000 | Loss: 0.00003039
Iteration 124/1000 | Loss: 0.00003039
Iteration 125/1000 | Loss: 0.00003039
Iteration 126/1000 | Loss: 0.00003039
Iteration 127/1000 | Loss: 0.00003039
Iteration 128/1000 | Loss: 0.00003038
Iteration 129/1000 | Loss: 0.00003038
Iteration 130/1000 | Loss: 0.00003038
Iteration 131/1000 | Loss: 0.00003038
Iteration 132/1000 | Loss: 0.00003038
Iteration 133/1000 | Loss: 0.00003038
Iteration 134/1000 | Loss: 0.00003037
Iteration 135/1000 | Loss: 0.00003037
Iteration 136/1000 | Loss: 0.00003037
Iteration 137/1000 | Loss: 0.00003037
Iteration 138/1000 | Loss: 0.00003037
Iteration 139/1000 | Loss: 0.00003037
Iteration 140/1000 | Loss: 0.00003037
Iteration 141/1000 | Loss: 0.00003037
Iteration 142/1000 | Loss: 0.00003037
Iteration 143/1000 | Loss: 0.00003037
Iteration 144/1000 | Loss: 0.00003037
Iteration 145/1000 | Loss: 0.00003036
Iteration 146/1000 | Loss: 0.00003036
Iteration 147/1000 | Loss: 0.00003036
Iteration 148/1000 | Loss: 0.00003036
Iteration 149/1000 | Loss: 0.00003036
Iteration 150/1000 | Loss: 0.00003036
Iteration 151/1000 | Loss: 0.00003036
Iteration 152/1000 | Loss: 0.00003036
Iteration 153/1000 | Loss: 0.00003035
Iteration 154/1000 | Loss: 0.00003035
Iteration 155/1000 | Loss: 0.00003035
Iteration 156/1000 | Loss: 0.00003035
Iteration 157/1000 | Loss: 0.00003035
Iteration 158/1000 | Loss: 0.00003035
Iteration 159/1000 | Loss: 0.00003035
Iteration 160/1000 | Loss: 0.00003035
Iteration 161/1000 | Loss: 0.00003035
Iteration 162/1000 | Loss: 0.00003035
Iteration 163/1000 | Loss: 0.00003035
Iteration 164/1000 | Loss: 0.00003035
Iteration 165/1000 | Loss: 0.00003035
Iteration 166/1000 | Loss: 0.00003035
Iteration 167/1000 | Loss: 0.00003035
Iteration 168/1000 | Loss: 0.00003035
Iteration 169/1000 | Loss: 0.00003035
Iteration 170/1000 | Loss: 0.00003035
Iteration 171/1000 | Loss: 0.00003035
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [3.034720430150628e-05, 3.034720430150628e-05, 3.034720430150628e-05, 3.034720430150628e-05, 3.034720430150628e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.034720430150628e-05

Optimization complete. Final v2v error: 4.695703983306885 mm

Highest mean error: 5.4326887130737305 mm for frame 87

Lowest mean error: 4.101995944976807 mm for frame 8

Saving results

Total time: 57.307008504867554
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_42_us_0617/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_42_us_0617/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_42_us_0617/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01160375
Iteration 2/25 | Loss: 0.00278906
Iteration 3/25 | Loss: 0.00327893
Iteration 4/25 | Loss: 0.00184916
Iteration 5/25 | Loss: 0.00175950
Iteration 6/25 | Loss: 0.00156458
Iteration 7/25 | Loss: 0.00149429
Iteration 8/25 | Loss: 0.00147976
Iteration 9/25 | Loss: 0.00147469
Iteration 10/25 | Loss: 0.00145676
Iteration 11/25 | Loss: 0.00145311
Iteration 12/25 | Loss: 0.00144020
Iteration 13/25 | Loss: 0.00143619
Iteration 14/25 | Loss: 0.00143343
Iteration 15/25 | Loss: 0.00142848
Iteration 16/25 | Loss: 0.00142437
Iteration 17/25 | Loss: 0.00142343
Iteration 18/25 | Loss: 0.00142317
Iteration 19/25 | Loss: 0.00142304
Iteration 20/25 | Loss: 0.00142296
Iteration 21/25 | Loss: 0.00142296
Iteration 22/25 | Loss: 0.00142295
Iteration 23/25 | Loss: 0.00142295
Iteration 24/25 | Loss: 0.00142295
Iteration 25/25 | Loss: 0.00142295

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58403599
Iteration 2/25 | Loss: 0.00152499
Iteration 3/25 | Loss: 0.00152498
Iteration 4/25 | Loss: 0.00152498
Iteration 5/25 | Loss: 0.00152498
Iteration 6/25 | Loss: 0.00152498
Iteration 7/25 | Loss: 0.00152498
Iteration 8/25 | Loss: 0.00152498
Iteration 9/25 | Loss: 0.00152498
Iteration 10/25 | Loss: 0.00152498
Iteration 11/25 | Loss: 0.00152498
Iteration 12/25 | Loss: 0.00152498
Iteration 13/25 | Loss: 0.00152498
Iteration 14/25 | Loss: 0.00152498
Iteration 15/25 | Loss: 0.00152498
Iteration 16/25 | Loss: 0.00152498
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001524982857517898, 0.001524982857517898, 0.001524982857517898, 0.001524982857517898, 0.001524982857517898]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001524982857517898

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00152498
Iteration 2/1000 | Loss: 0.00004514
Iteration 3/1000 | Loss: 0.00003373
Iteration 4/1000 | Loss: 0.00003038
Iteration 5/1000 | Loss: 0.00002866
Iteration 6/1000 | Loss: 0.00002767
Iteration 7/1000 | Loss: 0.00002694
Iteration 8/1000 | Loss: 0.00002646
Iteration 9/1000 | Loss: 0.00042739
Iteration 10/1000 | Loss: 0.00046131
Iteration 11/1000 | Loss: 0.00010700
Iteration 12/1000 | Loss: 0.00004544
Iteration 13/1000 | Loss: 0.00003640
Iteration 14/1000 | Loss: 0.00003187
Iteration 15/1000 | Loss: 0.00002921
Iteration 16/1000 | Loss: 0.00002839
Iteration 17/1000 | Loss: 0.00081299
Iteration 18/1000 | Loss: 0.00079090
Iteration 19/1000 | Loss: 0.00004338
Iteration 20/1000 | Loss: 0.00002837
Iteration 21/1000 | Loss: 0.00002503
Iteration 22/1000 | Loss: 0.00002377
Iteration 23/1000 | Loss: 0.00002309
Iteration 24/1000 | Loss: 0.00002265
Iteration 25/1000 | Loss: 0.00002258
Iteration 26/1000 | Loss: 0.00002257
Iteration 27/1000 | Loss: 0.00002239
Iteration 28/1000 | Loss: 0.00002238
Iteration 29/1000 | Loss: 0.00002237
Iteration 30/1000 | Loss: 0.00002237
Iteration 31/1000 | Loss: 0.00002237
Iteration 32/1000 | Loss: 0.00002236
Iteration 33/1000 | Loss: 0.00002236
Iteration 34/1000 | Loss: 0.00002236
Iteration 35/1000 | Loss: 0.00002236
Iteration 36/1000 | Loss: 0.00002236
Iteration 37/1000 | Loss: 0.00002236
Iteration 38/1000 | Loss: 0.00002235
Iteration 39/1000 | Loss: 0.00002235
Iteration 40/1000 | Loss: 0.00002235
Iteration 41/1000 | Loss: 0.00002235
Iteration 42/1000 | Loss: 0.00002235
Iteration 43/1000 | Loss: 0.00002235
Iteration 44/1000 | Loss: 0.00002234
Iteration 45/1000 | Loss: 0.00002234
Iteration 46/1000 | Loss: 0.00002234
Iteration 47/1000 | Loss: 0.00002234
Iteration 48/1000 | Loss: 0.00002234
Iteration 49/1000 | Loss: 0.00002234
Iteration 50/1000 | Loss: 0.00002234
Iteration 51/1000 | Loss: 0.00002234
Iteration 52/1000 | Loss: 0.00002234
Iteration 53/1000 | Loss: 0.00002234
Iteration 54/1000 | Loss: 0.00002234
Iteration 55/1000 | Loss: 0.00002234
Iteration 56/1000 | Loss: 0.00002234
Iteration 57/1000 | Loss: 0.00002234
Iteration 58/1000 | Loss: 0.00002234
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 58. Stopping optimization.
Last 5 losses: [2.233885788882617e-05, 2.233885788882617e-05, 2.233885788882617e-05, 2.233885788882617e-05, 2.233885788882617e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.233885788882617e-05

Optimization complete. Final v2v error: 4.121829986572266 mm

Highest mean error: 4.831613063812256 mm for frame 76

Lowest mean error: 3.8162341117858887 mm for frame 41

Saving results

Total time: 72.32010579109192
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_42_us_0617/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_42_us_0617/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_42_us_0617/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00484221
Iteration 2/25 | Loss: 0.00155272
Iteration 3/25 | Loss: 0.00145381
Iteration 4/25 | Loss: 0.00144181
Iteration 5/25 | Loss: 0.00143757
Iteration 6/25 | Loss: 0.00143631
Iteration 7/25 | Loss: 0.00143631
Iteration 8/25 | Loss: 0.00143631
Iteration 9/25 | Loss: 0.00143631
Iteration 10/25 | Loss: 0.00143631
Iteration 11/25 | Loss: 0.00143631
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001436305814422667, 0.001436305814422667, 0.001436305814422667, 0.001436305814422667, 0.001436305814422667]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001436305814422667

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50673926
Iteration 2/25 | Loss: 0.00152779
Iteration 3/25 | Loss: 0.00152779
Iteration 4/25 | Loss: 0.00152779
Iteration 5/25 | Loss: 0.00152779
Iteration 6/25 | Loss: 0.00152779
Iteration 7/25 | Loss: 0.00152779
Iteration 8/25 | Loss: 0.00152779
Iteration 9/25 | Loss: 0.00152779
Iteration 10/25 | Loss: 0.00152779
Iteration 11/25 | Loss: 0.00152779
Iteration 12/25 | Loss: 0.00152779
Iteration 13/25 | Loss: 0.00152779
Iteration 14/25 | Loss: 0.00152779
Iteration 15/25 | Loss: 0.00152779
Iteration 16/25 | Loss: 0.00152779
Iteration 17/25 | Loss: 0.00152779
Iteration 18/25 | Loss: 0.00152779
Iteration 19/25 | Loss: 0.00152779
Iteration 20/25 | Loss: 0.00152779
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0015277893980965018, 0.0015277893980965018, 0.0015277893980965018, 0.0015277893980965018, 0.0015277893980965018]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015277893980965018

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00152779
Iteration 2/1000 | Loss: 0.00003935
Iteration 3/1000 | Loss: 0.00003094
Iteration 4/1000 | Loss: 0.00002764
Iteration 5/1000 | Loss: 0.00002617
Iteration 6/1000 | Loss: 0.00002534
Iteration 7/1000 | Loss: 0.00002490
Iteration 8/1000 | Loss: 0.00002434
Iteration 9/1000 | Loss: 0.00002396
Iteration 10/1000 | Loss: 0.00002369
Iteration 11/1000 | Loss: 0.00002367
Iteration 12/1000 | Loss: 0.00002360
Iteration 13/1000 | Loss: 0.00002348
Iteration 14/1000 | Loss: 0.00002341
Iteration 15/1000 | Loss: 0.00002340
Iteration 16/1000 | Loss: 0.00002332
Iteration 17/1000 | Loss: 0.00002331
Iteration 18/1000 | Loss: 0.00002323
Iteration 19/1000 | Loss: 0.00002319
Iteration 20/1000 | Loss: 0.00002319
Iteration 21/1000 | Loss: 0.00002318
Iteration 22/1000 | Loss: 0.00002317
Iteration 23/1000 | Loss: 0.00002314
Iteration 24/1000 | Loss: 0.00002313
Iteration 25/1000 | Loss: 0.00002313
Iteration 26/1000 | Loss: 0.00002312
Iteration 27/1000 | Loss: 0.00002312
Iteration 28/1000 | Loss: 0.00002310
Iteration 29/1000 | Loss: 0.00002309
Iteration 30/1000 | Loss: 0.00002308
Iteration 31/1000 | Loss: 0.00002307
Iteration 32/1000 | Loss: 0.00002307
Iteration 33/1000 | Loss: 0.00002306
Iteration 34/1000 | Loss: 0.00002306
Iteration 35/1000 | Loss: 0.00002305
Iteration 36/1000 | Loss: 0.00002304
Iteration 37/1000 | Loss: 0.00002304
Iteration 38/1000 | Loss: 0.00002304
Iteration 39/1000 | Loss: 0.00002303
Iteration 40/1000 | Loss: 0.00002303
Iteration 41/1000 | Loss: 0.00002302
Iteration 42/1000 | Loss: 0.00002302
Iteration 43/1000 | Loss: 0.00002302
Iteration 44/1000 | Loss: 0.00002302
Iteration 45/1000 | Loss: 0.00002302
Iteration 46/1000 | Loss: 0.00002301
Iteration 47/1000 | Loss: 0.00002301
Iteration 48/1000 | Loss: 0.00002301
Iteration 49/1000 | Loss: 0.00002301
Iteration 50/1000 | Loss: 0.00002301
Iteration 51/1000 | Loss: 0.00002301
Iteration 52/1000 | Loss: 0.00002300
Iteration 53/1000 | Loss: 0.00002300
Iteration 54/1000 | Loss: 0.00002300
Iteration 55/1000 | Loss: 0.00002300
Iteration 56/1000 | Loss: 0.00002299
Iteration 57/1000 | Loss: 0.00002299
Iteration 58/1000 | Loss: 0.00002298
Iteration 59/1000 | Loss: 0.00002298
Iteration 60/1000 | Loss: 0.00002298
Iteration 61/1000 | Loss: 0.00002298
Iteration 62/1000 | Loss: 0.00002298
Iteration 63/1000 | Loss: 0.00002298
Iteration 64/1000 | Loss: 0.00002298
Iteration 65/1000 | Loss: 0.00002298
Iteration 66/1000 | Loss: 0.00002298
Iteration 67/1000 | Loss: 0.00002298
Iteration 68/1000 | Loss: 0.00002298
Iteration 69/1000 | Loss: 0.00002298
Iteration 70/1000 | Loss: 0.00002298
Iteration 71/1000 | Loss: 0.00002298
Iteration 72/1000 | Loss: 0.00002298
Iteration 73/1000 | Loss: 0.00002298
Iteration 74/1000 | Loss: 0.00002298
Iteration 75/1000 | Loss: 0.00002298
Iteration 76/1000 | Loss: 0.00002298
Iteration 77/1000 | Loss: 0.00002298
Iteration 78/1000 | Loss: 0.00002298
Iteration 79/1000 | Loss: 0.00002298
Iteration 80/1000 | Loss: 0.00002298
Iteration 81/1000 | Loss: 0.00002298
Iteration 82/1000 | Loss: 0.00002298
Iteration 83/1000 | Loss: 0.00002298
Iteration 84/1000 | Loss: 0.00002298
Iteration 85/1000 | Loss: 0.00002298
Iteration 86/1000 | Loss: 0.00002298
Iteration 87/1000 | Loss: 0.00002298
Iteration 88/1000 | Loss: 0.00002298
Iteration 89/1000 | Loss: 0.00002298
Iteration 90/1000 | Loss: 0.00002298
Iteration 91/1000 | Loss: 0.00002298
Iteration 92/1000 | Loss: 0.00002298
Iteration 93/1000 | Loss: 0.00002298
Iteration 94/1000 | Loss: 0.00002298
Iteration 95/1000 | Loss: 0.00002298
Iteration 96/1000 | Loss: 0.00002298
Iteration 97/1000 | Loss: 0.00002298
Iteration 98/1000 | Loss: 0.00002298
Iteration 99/1000 | Loss: 0.00002298
Iteration 100/1000 | Loss: 0.00002298
Iteration 101/1000 | Loss: 0.00002298
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [2.298035724379588e-05, 2.298035724379588e-05, 2.298035724379588e-05, 2.298035724379588e-05, 2.298035724379588e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.298035724379588e-05

Optimization complete. Final v2v error: 4.140566825866699 mm

Highest mean error: 4.753115653991699 mm for frame 66

Lowest mean error: 3.8263440132141113 mm for frame 95

Saving results

Total time: 37.42731332778931
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_42_us_0617/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_42_us_0617/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_42_us_0617/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00531493
Iteration 2/25 | Loss: 0.00152613
Iteration 3/25 | Loss: 0.00146882
Iteration 4/25 | Loss: 0.00146047
Iteration 5/25 | Loss: 0.00145834
Iteration 6/25 | Loss: 0.00145807
Iteration 7/25 | Loss: 0.00145807
Iteration 8/25 | Loss: 0.00145807
Iteration 9/25 | Loss: 0.00145807
Iteration 10/25 | Loss: 0.00145807
Iteration 11/25 | Loss: 0.00145807
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014580725692212582, 0.0014580725692212582, 0.0014580725692212582, 0.0014580725692212582, 0.0014580725692212582]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014580725692212582

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.88178992
Iteration 2/25 | Loss: 0.00156189
Iteration 3/25 | Loss: 0.00156189
Iteration 4/25 | Loss: 0.00156189
Iteration 5/25 | Loss: 0.00156189
Iteration 6/25 | Loss: 0.00156189
Iteration 7/25 | Loss: 0.00156189
Iteration 8/25 | Loss: 0.00156189
Iteration 9/25 | Loss: 0.00156189
Iteration 10/25 | Loss: 0.00156189
Iteration 11/25 | Loss: 0.00156189
Iteration 12/25 | Loss: 0.00156189
Iteration 13/25 | Loss: 0.00156189
Iteration 14/25 | Loss: 0.00156189
Iteration 15/25 | Loss: 0.00156189
Iteration 16/25 | Loss: 0.00156189
Iteration 17/25 | Loss: 0.00156189
Iteration 18/25 | Loss: 0.00156189
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001561889424920082, 0.001561889424920082, 0.001561889424920082, 0.001561889424920082, 0.001561889424920082]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001561889424920082

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00156189
Iteration 2/1000 | Loss: 0.00003888
Iteration 3/1000 | Loss: 0.00003277
Iteration 4/1000 | Loss: 0.00003048
Iteration 5/1000 | Loss: 0.00002938
Iteration 6/1000 | Loss: 0.00002862
Iteration 7/1000 | Loss: 0.00002809
Iteration 8/1000 | Loss: 0.00002773
Iteration 9/1000 | Loss: 0.00002751
Iteration 10/1000 | Loss: 0.00002731
Iteration 11/1000 | Loss: 0.00002723
Iteration 12/1000 | Loss: 0.00002718
Iteration 13/1000 | Loss: 0.00002712
Iteration 14/1000 | Loss: 0.00002709
Iteration 15/1000 | Loss: 0.00002709
Iteration 16/1000 | Loss: 0.00002708
Iteration 17/1000 | Loss: 0.00002708
Iteration 18/1000 | Loss: 0.00002708
Iteration 19/1000 | Loss: 0.00002708
Iteration 20/1000 | Loss: 0.00002707
Iteration 21/1000 | Loss: 0.00002704
Iteration 22/1000 | Loss: 0.00002704
Iteration 23/1000 | Loss: 0.00002703
Iteration 24/1000 | Loss: 0.00002702
Iteration 25/1000 | Loss: 0.00002702
Iteration 26/1000 | Loss: 0.00002702
Iteration 27/1000 | Loss: 0.00002702
Iteration 28/1000 | Loss: 0.00002701
Iteration 29/1000 | Loss: 0.00002701
Iteration 30/1000 | Loss: 0.00002700
Iteration 31/1000 | Loss: 0.00002700
Iteration 32/1000 | Loss: 0.00002700
Iteration 33/1000 | Loss: 0.00002699
Iteration 34/1000 | Loss: 0.00002699
Iteration 35/1000 | Loss: 0.00002699
Iteration 36/1000 | Loss: 0.00002699
Iteration 37/1000 | Loss: 0.00002699
Iteration 38/1000 | Loss: 0.00002699
Iteration 39/1000 | Loss: 0.00002699
Iteration 40/1000 | Loss: 0.00002699
Iteration 41/1000 | Loss: 0.00002699
Iteration 42/1000 | Loss: 0.00002698
Iteration 43/1000 | Loss: 0.00002698
Iteration 44/1000 | Loss: 0.00002698
Iteration 45/1000 | Loss: 0.00002697
Iteration 46/1000 | Loss: 0.00002697
Iteration 47/1000 | Loss: 0.00002697
Iteration 48/1000 | Loss: 0.00002697
Iteration 49/1000 | Loss: 0.00002697
Iteration 50/1000 | Loss: 0.00002697
Iteration 51/1000 | Loss: 0.00002697
Iteration 52/1000 | Loss: 0.00002697
Iteration 53/1000 | Loss: 0.00002697
Iteration 54/1000 | Loss: 0.00002697
Iteration 55/1000 | Loss: 0.00002696
Iteration 56/1000 | Loss: 0.00002696
Iteration 57/1000 | Loss: 0.00002696
Iteration 58/1000 | Loss: 0.00002695
Iteration 59/1000 | Loss: 0.00002695
Iteration 60/1000 | Loss: 0.00002695
Iteration 61/1000 | Loss: 0.00002694
Iteration 62/1000 | Loss: 0.00002694
Iteration 63/1000 | Loss: 0.00002693
Iteration 64/1000 | Loss: 0.00002693
Iteration 65/1000 | Loss: 0.00002692
Iteration 66/1000 | Loss: 0.00002692
Iteration 67/1000 | Loss: 0.00002692
Iteration 68/1000 | Loss: 0.00002692
Iteration 69/1000 | Loss: 0.00002692
Iteration 70/1000 | Loss: 0.00002691
Iteration 71/1000 | Loss: 0.00002691
Iteration 72/1000 | Loss: 0.00002691
Iteration 73/1000 | Loss: 0.00002691
Iteration 74/1000 | Loss: 0.00002691
Iteration 75/1000 | Loss: 0.00002691
Iteration 76/1000 | Loss: 0.00002691
Iteration 77/1000 | Loss: 0.00002691
Iteration 78/1000 | Loss: 0.00002691
Iteration 79/1000 | Loss: 0.00002691
Iteration 80/1000 | Loss: 0.00002691
Iteration 81/1000 | Loss: 0.00002691
Iteration 82/1000 | Loss: 0.00002691
Iteration 83/1000 | Loss: 0.00002691
Iteration 84/1000 | Loss: 0.00002691
Iteration 85/1000 | Loss: 0.00002691
Iteration 86/1000 | Loss: 0.00002691
Iteration 87/1000 | Loss: 0.00002691
Iteration 88/1000 | Loss: 0.00002691
Iteration 89/1000 | Loss: 0.00002691
Iteration 90/1000 | Loss: 0.00002691
Iteration 91/1000 | Loss: 0.00002691
Iteration 92/1000 | Loss: 0.00002691
Iteration 93/1000 | Loss: 0.00002691
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [2.691465851967223e-05, 2.691465851967223e-05, 2.691465851967223e-05, 2.691465851967223e-05, 2.691465851967223e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.691465851967223e-05

Optimization complete. Final v2v error: 4.520472049713135 mm

Highest mean error: 4.824664115905762 mm for frame 86

Lowest mean error: 4.2036285400390625 mm for frame 213

Saving results

Total time: 32.792731285095215
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_42_us_0617/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_42_us_0617/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_42_us_0617/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00941484
Iteration 2/25 | Loss: 0.00160913
Iteration 3/25 | Loss: 0.00149826
Iteration 4/25 | Loss: 0.00148143
Iteration 5/25 | Loss: 0.00147553
Iteration 6/25 | Loss: 0.00147493
Iteration 7/25 | Loss: 0.00147493
Iteration 8/25 | Loss: 0.00147493
Iteration 9/25 | Loss: 0.00147493
Iteration 10/25 | Loss: 0.00147493
Iteration 11/25 | Loss: 0.00147493
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014749262481927872, 0.0014749262481927872, 0.0014749262481927872, 0.0014749262481927872, 0.0014749262481927872]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014749262481927872

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.83939922
Iteration 2/25 | Loss: 0.00150557
Iteration 3/25 | Loss: 0.00150557
Iteration 4/25 | Loss: 0.00150557
Iteration 5/25 | Loss: 0.00150557
Iteration 6/25 | Loss: 0.00150557
Iteration 7/25 | Loss: 0.00150557
Iteration 8/25 | Loss: 0.00150557
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 8. Stopping optimization.
Last 5 losses: [0.00150556571315974, 0.00150556571315974, 0.00150556571315974, 0.00150556571315974, 0.00150556571315974]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00150556571315974

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00150557
Iteration 2/1000 | Loss: 0.00004322
Iteration 3/1000 | Loss: 0.00003445
Iteration 4/1000 | Loss: 0.00003146
Iteration 5/1000 | Loss: 0.00002990
Iteration 6/1000 | Loss: 0.00002890
Iteration 7/1000 | Loss: 0.00002832
Iteration 8/1000 | Loss: 0.00002785
Iteration 9/1000 | Loss: 0.00002757
Iteration 10/1000 | Loss: 0.00002734
Iteration 11/1000 | Loss: 0.00002716
Iteration 12/1000 | Loss: 0.00002708
Iteration 13/1000 | Loss: 0.00002702
Iteration 14/1000 | Loss: 0.00002694
Iteration 15/1000 | Loss: 0.00002693
Iteration 16/1000 | Loss: 0.00002690
Iteration 17/1000 | Loss: 0.00002689
Iteration 18/1000 | Loss: 0.00002687
Iteration 19/1000 | Loss: 0.00002685
Iteration 20/1000 | Loss: 0.00002678
Iteration 21/1000 | Loss: 0.00002678
Iteration 22/1000 | Loss: 0.00002674
Iteration 23/1000 | Loss: 0.00002674
Iteration 24/1000 | Loss: 0.00002673
Iteration 25/1000 | Loss: 0.00002673
Iteration 26/1000 | Loss: 0.00002672
Iteration 27/1000 | Loss: 0.00002671
Iteration 28/1000 | Loss: 0.00002670
Iteration 29/1000 | Loss: 0.00002670
Iteration 30/1000 | Loss: 0.00002669
Iteration 31/1000 | Loss: 0.00002669
Iteration 32/1000 | Loss: 0.00002667
Iteration 33/1000 | Loss: 0.00002666
Iteration 34/1000 | Loss: 0.00002666
Iteration 35/1000 | Loss: 0.00002665
Iteration 36/1000 | Loss: 0.00002665
Iteration 37/1000 | Loss: 0.00002665
Iteration 38/1000 | Loss: 0.00002664
Iteration 39/1000 | Loss: 0.00002664
Iteration 40/1000 | Loss: 0.00002664
Iteration 41/1000 | Loss: 0.00002664
Iteration 42/1000 | Loss: 0.00002663
Iteration 43/1000 | Loss: 0.00002662
Iteration 44/1000 | Loss: 0.00002662
Iteration 45/1000 | Loss: 0.00002661
Iteration 46/1000 | Loss: 0.00002661
Iteration 47/1000 | Loss: 0.00002661
Iteration 48/1000 | Loss: 0.00002661
Iteration 49/1000 | Loss: 0.00002660
Iteration 50/1000 | Loss: 0.00002660
Iteration 51/1000 | Loss: 0.00002660
Iteration 52/1000 | Loss: 0.00002660
Iteration 53/1000 | Loss: 0.00002660
Iteration 54/1000 | Loss: 0.00002660
Iteration 55/1000 | Loss: 0.00002660
Iteration 56/1000 | Loss: 0.00002660
Iteration 57/1000 | Loss: 0.00002660
Iteration 58/1000 | Loss: 0.00002660
Iteration 59/1000 | Loss: 0.00002660
Iteration 60/1000 | Loss: 0.00002660
Iteration 61/1000 | Loss: 0.00002659
Iteration 62/1000 | Loss: 0.00002659
Iteration 63/1000 | Loss: 0.00002659
Iteration 64/1000 | Loss: 0.00002659
Iteration 65/1000 | Loss: 0.00002659
Iteration 66/1000 | Loss: 0.00002659
Iteration 67/1000 | Loss: 0.00002659
Iteration 68/1000 | Loss: 0.00002659
Iteration 69/1000 | Loss: 0.00002658
Iteration 70/1000 | Loss: 0.00002658
Iteration 71/1000 | Loss: 0.00002658
Iteration 72/1000 | Loss: 0.00002658
Iteration 73/1000 | Loss: 0.00002658
Iteration 74/1000 | Loss: 0.00002658
Iteration 75/1000 | Loss: 0.00002658
Iteration 76/1000 | Loss: 0.00002658
Iteration 77/1000 | Loss: 0.00002657
Iteration 78/1000 | Loss: 0.00002657
Iteration 79/1000 | Loss: 0.00002657
Iteration 80/1000 | Loss: 0.00002657
Iteration 81/1000 | Loss: 0.00002657
Iteration 82/1000 | Loss: 0.00002657
Iteration 83/1000 | Loss: 0.00002657
Iteration 84/1000 | Loss: 0.00002657
Iteration 85/1000 | Loss: 0.00002657
Iteration 86/1000 | Loss: 0.00002656
Iteration 87/1000 | Loss: 0.00002656
Iteration 88/1000 | Loss: 0.00002656
Iteration 89/1000 | Loss: 0.00002656
Iteration 90/1000 | Loss: 0.00002656
Iteration 91/1000 | Loss: 0.00002656
Iteration 92/1000 | Loss: 0.00002656
Iteration 93/1000 | Loss: 0.00002656
Iteration 94/1000 | Loss: 0.00002656
Iteration 95/1000 | Loss: 0.00002656
Iteration 96/1000 | Loss: 0.00002656
Iteration 97/1000 | Loss: 0.00002655
Iteration 98/1000 | Loss: 0.00002655
Iteration 99/1000 | Loss: 0.00002655
Iteration 100/1000 | Loss: 0.00002655
Iteration 101/1000 | Loss: 0.00002655
Iteration 102/1000 | Loss: 0.00002655
Iteration 103/1000 | Loss: 0.00002655
Iteration 104/1000 | Loss: 0.00002655
Iteration 105/1000 | Loss: 0.00002655
Iteration 106/1000 | Loss: 0.00002655
Iteration 107/1000 | Loss: 0.00002655
Iteration 108/1000 | Loss: 0.00002655
Iteration 109/1000 | Loss: 0.00002655
Iteration 110/1000 | Loss: 0.00002655
Iteration 111/1000 | Loss: 0.00002655
Iteration 112/1000 | Loss: 0.00002655
Iteration 113/1000 | Loss: 0.00002655
Iteration 114/1000 | Loss: 0.00002655
Iteration 115/1000 | Loss: 0.00002655
Iteration 116/1000 | Loss: 0.00002655
Iteration 117/1000 | Loss: 0.00002655
Iteration 118/1000 | Loss: 0.00002655
Iteration 119/1000 | Loss: 0.00002655
Iteration 120/1000 | Loss: 0.00002655
Iteration 121/1000 | Loss: 0.00002655
Iteration 122/1000 | Loss: 0.00002655
Iteration 123/1000 | Loss: 0.00002655
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [2.655018943187315e-05, 2.655018943187315e-05, 2.655018943187315e-05, 2.655018943187315e-05, 2.655018943187315e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.655018943187315e-05

Optimization complete. Final v2v error: 4.364490032196045 mm

Highest mean error: 4.946152210235596 mm for frame 114

Lowest mean error: 3.9829602241516113 mm for frame 89

Saving results

Total time: 35.263283252716064
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_42_us_0617/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_42_us_0617/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_42_us_0617/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00483470
Iteration 2/25 | Loss: 0.00155184
Iteration 3/25 | Loss: 0.00145091
Iteration 4/25 | Loss: 0.00144207
Iteration 5/25 | Loss: 0.00143914
Iteration 6/25 | Loss: 0.00143855
Iteration 7/25 | Loss: 0.00143855
Iteration 8/25 | Loss: 0.00143855
Iteration 9/25 | Loss: 0.00143855
Iteration 10/25 | Loss: 0.00143855
Iteration 11/25 | Loss: 0.00143855
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014385469257831573, 0.0014385469257831573, 0.0014385469257831573, 0.0014385469257831573, 0.0014385469257831573]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014385469257831573

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48725116
Iteration 2/25 | Loss: 0.00155772
Iteration 3/25 | Loss: 0.00155772
Iteration 4/25 | Loss: 0.00155772
Iteration 5/25 | Loss: 0.00155772
Iteration 6/25 | Loss: 0.00155772
Iteration 7/25 | Loss: 0.00155772
Iteration 8/25 | Loss: 0.00155772
Iteration 9/25 | Loss: 0.00155772
Iteration 10/25 | Loss: 0.00155772
Iteration 11/25 | Loss: 0.00155772
Iteration 12/25 | Loss: 0.00155772
Iteration 13/25 | Loss: 0.00155772
Iteration 14/25 | Loss: 0.00155772
Iteration 15/25 | Loss: 0.00155772
Iteration 16/25 | Loss: 0.00155772
Iteration 17/25 | Loss: 0.00155772
Iteration 18/25 | Loss: 0.00155772
Iteration 19/25 | Loss: 0.00155772
Iteration 20/25 | Loss: 0.00155772
Iteration 21/25 | Loss: 0.00155772
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0015577177982777357, 0.0015577177982777357, 0.0015577177982777357, 0.0015577177982777357, 0.0015577177982777357]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015577177982777357

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00155772
Iteration 2/1000 | Loss: 0.00003848
Iteration 3/1000 | Loss: 0.00003013
Iteration 4/1000 | Loss: 0.00002761
Iteration 5/1000 | Loss: 0.00002636
Iteration 6/1000 | Loss: 0.00002560
Iteration 7/1000 | Loss: 0.00002500
Iteration 8/1000 | Loss: 0.00002450
Iteration 9/1000 | Loss: 0.00002419
Iteration 10/1000 | Loss: 0.00002394
Iteration 11/1000 | Loss: 0.00002391
Iteration 12/1000 | Loss: 0.00002387
Iteration 13/1000 | Loss: 0.00002386
Iteration 14/1000 | Loss: 0.00002381
Iteration 15/1000 | Loss: 0.00002374
Iteration 16/1000 | Loss: 0.00002370
Iteration 17/1000 | Loss: 0.00002368
Iteration 18/1000 | Loss: 0.00002368
Iteration 19/1000 | Loss: 0.00002367
Iteration 20/1000 | Loss: 0.00002361
Iteration 21/1000 | Loss: 0.00002359
Iteration 22/1000 | Loss: 0.00002358
Iteration 23/1000 | Loss: 0.00002358
Iteration 24/1000 | Loss: 0.00002355
Iteration 25/1000 | Loss: 0.00002354
Iteration 26/1000 | Loss: 0.00002351
Iteration 27/1000 | Loss: 0.00002348
Iteration 28/1000 | Loss: 0.00002348
Iteration 29/1000 | Loss: 0.00002348
Iteration 30/1000 | Loss: 0.00002348
Iteration 31/1000 | Loss: 0.00002348
Iteration 32/1000 | Loss: 0.00002347
Iteration 33/1000 | Loss: 0.00002347
Iteration 34/1000 | Loss: 0.00002345
Iteration 35/1000 | Loss: 0.00002341
Iteration 36/1000 | Loss: 0.00002339
Iteration 37/1000 | Loss: 0.00002339
Iteration 38/1000 | Loss: 0.00002338
Iteration 39/1000 | Loss: 0.00002338
Iteration 40/1000 | Loss: 0.00002337
Iteration 41/1000 | Loss: 0.00002337
Iteration 42/1000 | Loss: 0.00002336
Iteration 43/1000 | Loss: 0.00002336
Iteration 44/1000 | Loss: 0.00002336
Iteration 45/1000 | Loss: 0.00002336
Iteration 46/1000 | Loss: 0.00002336
Iteration 47/1000 | Loss: 0.00002335
Iteration 48/1000 | Loss: 0.00002335
Iteration 49/1000 | Loss: 0.00002335
Iteration 50/1000 | Loss: 0.00002334
Iteration 51/1000 | Loss: 0.00002334
Iteration 52/1000 | Loss: 0.00002333
Iteration 53/1000 | Loss: 0.00002333
Iteration 54/1000 | Loss: 0.00002333
Iteration 55/1000 | Loss: 0.00002333
Iteration 56/1000 | Loss: 0.00002333
Iteration 57/1000 | Loss: 0.00002332
Iteration 58/1000 | Loss: 0.00002332
Iteration 59/1000 | Loss: 0.00002332
Iteration 60/1000 | Loss: 0.00002332
Iteration 61/1000 | Loss: 0.00002332
Iteration 62/1000 | Loss: 0.00002332
Iteration 63/1000 | Loss: 0.00002332
Iteration 64/1000 | Loss: 0.00002332
Iteration 65/1000 | Loss: 0.00002332
Iteration 66/1000 | Loss: 0.00002332
Iteration 67/1000 | Loss: 0.00002332
Iteration 68/1000 | Loss: 0.00002331
Iteration 69/1000 | Loss: 0.00002331
Iteration 70/1000 | Loss: 0.00002331
Iteration 71/1000 | Loss: 0.00002330
Iteration 72/1000 | Loss: 0.00002330
Iteration 73/1000 | Loss: 0.00002330
Iteration 74/1000 | Loss: 0.00002330
Iteration 75/1000 | Loss: 0.00002329
Iteration 76/1000 | Loss: 0.00002329
Iteration 77/1000 | Loss: 0.00002329
Iteration 78/1000 | Loss: 0.00002329
Iteration 79/1000 | Loss: 0.00002329
Iteration 80/1000 | Loss: 0.00002329
Iteration 81/1000 | Loss: 0.00002329
Iteration 82/1000 | Loss: 0.00002329
Iteration 83/1000 | Loss: 0.00002329
Iteration 84/1000 | Loss: 0.00002329
Iteration 85/1000 | Loss: 0.00002328
Iteration 86/1000 | Loss: 0.00002328
Iteration 87/1000 | Loss: 0.00002328
Iteration 88/1000 | Loss: 0.00002328
Iteration 89/1000 | Loss: 0.00002327
Iteration 90/1000 | Loss: 0.00002327
Iteration 91/1000 | Loss: 0.00002327
Iteration 92/1000 | Loss: 0.00002327
Iteration 93/1000 | Loss: 0.00002327
Iteration 94/1000 | Loss: 0.00002327
Iteration 95/1000 | Loss: 0.00002327
Iteration 96/1000 | Loss: 0.00002327
Iteration 97/1000 | Loss: 0.00002327
Iteration 98/1000 | Loss: 0.00002327
Iteration 99/1000 | Loss: 0.00002327
Iteration 100/1000 | Loss: 0.00002327
Iteration 101/1000 | Loss: 0.00002327
Iteration 102/1000 | Loss: 0.00002327
Iteration 103/1000 | Loss: 0.00002327
Iteration 104/1000 | Loss: 0.00002327
Iteration 105/1000 | Loss: 0.00002327
Iteration 106/1000 | Loss: 0.00002327
Iteration 107/1000 | Loss: 0.00002327
Iteration 108/1000 | Loss: 0.00002327
Iteration 109/1000 | Loss: 0.00002327
Iteration 110/1000 | Loss: 0.00002327
Iteration 111/1000 | Loss: 0.00002327
Iteration 112/1000 | Loss: 0.00002327
Iteration 113/1000 | Loss: 0.00002327
Iteration 114/1000 | Loss: 0.00002327
Iteration 115/1000 | Loss: 0.00002327
Iteration 116/1000 | Loss: 0.00002327
Iteration 117/1000 | Loss: 0.00002327
Iteration 118/1000 | Loss: 0.00002327
Iteration 119/1000 | Loss: 0.00002327
Iteration 120/1000 | Loss: 0.00002327
Iteration 121/1000 | Loss: 0.00002327
Iteration 122/1000 | Loss: 0.00002327
Iteration 123/1000 | Loss: 0.00002327
Iteration 124/1000 | Loss: 0.00002327
Iteration 125/1000 | Loss: 0.00002327
Iteration 126/1000 | Loss: 0.00002327
Iteration 127/1000 | Loss: 0.00002327
Iteration 128/1000 | Loss: 0.00002327
Iteration 129/1000 | Loss: 0.00002327
Iteration 130/1000 | Loss: 0.00002327
Iteration 131/1000 | Loss: 0.00002327
Iteration 132/1000 | Loss: 0.00002327
Iteration 133/1000 | Loss: 0.00002327
Iteration 134/1000 | Loss: 0.00002327
Iteration 135/1000 | Loss: 0.00002327
Iteration 136/1000 | Loss: 0.00002327
Iteration 137/1000 | Loss: 0.00002327
Iteration 138/1000 | Loss: 0.00002327
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [2.3268921722774394e-05, 2.3268921722774394e-05, 2.3268921722774394e-05, 2.3268921722774394e-05, 2.3268921722774394e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3268921722774394e-05

Optimization complete. Final v2v error: 4.183125019073486 mm

Highest mean error: 4.772981643676758 mm for frame 57

Lowest mean error: 3.898146390914917 mm for frame 211

Saving results

Total time: 39.588605642318726
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_42_us_0617/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_42_us_0617/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_42_us_0617/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00979093
Iteration 2/25 | Loss: 0.00185654
Iteration 3/25 | Loss: 0.00164217
Iteration 4/25 | Loss: 0.00161811
Iteration 5/25 | Loss: 0.00161009
Iteration 6/25 | Loss: 0.00160826
Iteration 7/25 | Loss: 0.00160826
Iteration 8/25 | Loss: 0.00160826
Iteration 9/25 | Loss: 0.00160826
Iteration 10/25 | Loss: 0.00160826
Iteration 11/25 | Loss: 0.00160826
Iteration 12/25 | Loss: 0.00160826
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001608263817615807, 0.001608263817615807, 0.001608263817615807, 0.001608263817615807, 0.001608263817615807]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001608263817615807

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42270827
Iteration 2/25 | Loss: 0.00178911
Iteration 3/25 | Loss: 0.00178910
Iteration 4/25 | Loss: 0.00178910
Iteration 5/25 | Loss: 0.00178910
Iteration 6/25 | Loss: 0.00178910
Iteration 7/25 | Loss: 0.00178910
Iteration 8/25 | Loss: 0.00178910
Iteration 9/25 | Loss: 0.00178910
Iteration 10/25 | Loss: 0.00178910
Iteration 11/25 | Loss: 0.00178910
Iteration 12/25 | Loss: 0.00178910
Iteration 13/25 | Loss: 0.00178910
Iteration 14/25 | Loss: 0.00178910
Iteration 15/25 | Loss: 0.00178910
Iteration 16/25 | Loss: 0.00178910
Iteration 17/25 | Loss: 0.00178910
Iteration 18/25 | Loss: 0.00178910
Iteration 19/25 | Loss: 0.00178910
Iteration 20/25 | Loss: 0.00178910
Iteration 21/25 | Loss: 0.00178910
Iteration 22/25 | Loss: 0.00178910
Iteration 23/25 | Loss: 0.00178910
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001789100468158722, 0.001789100468158722, 0.001789100468158722, 0.001789100468158722, 0.001789100468158722]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001789100468158722

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00178910
Iteration 2/1000 | Loss: 0.00008247
Iteration 3/1000 | Loss: 0.00005862
Iteration 4/1000 | Loss: 0.00004992
Iteration 5/1000 | Loss: 0.00004676
Iteration 6/1000 | Loss: 0.00004417
Iteration 7/1000 | Loss: 0.00004242
Iteration 8/1000 | Loss: 0.00004107
Iteration 9/1000 | Loss: 0.00004009
Iteration 10/1000 | Loss: 0.00003931
Iteration 11/1000 | Loss: 0.00003876
Iteration 12/1000 | Loss: 0.00003824
Iteration 13/1000 | Loss: 0.00003783
Iteration 14/1000 | Loss: 0.00003749
Iteration 15/1000 | Loss: 0.00003722
Iteration 16/1000 | Loss: 0.00003702
Iteration 17/1000 | Loss: 0.00003689
Iteration 18/1000 | Loss: 0.00003686
Iteration 19/1000 | Loss: 0.00003682
Iteration 20/1000 | Loss: 0.00003676
Iteration 21/1000 | Loss: 0.00003676
Iteration 22/1000 | Loss: 0.00003674
Iteration 23/1000 | Loss: 0.00003673
Iteration 24/1000 | Loss: 0.00003673
Iteration 25/1000 | Loss: 0.00003672
Iteration 26/1000 | Loss: 0.00003672
Iteration 27/1000 | Loss: 0.00003671
Iteration 28/1000 | Loss: 0.00003671
Iteration 29/1000 | Loss: 0.00003670
Iteration 30/1000 | Loss: 0.00003670
Iteration 31/1000 | Loss: 0.00003670
Iteration 32/1000 | Loss: 0.00003669
Iteration 33/1000 | Loss: 0.00003668
Iteration 34/1000 | Loss: 0.00003668
Iteration 35/1000 | Loss: 0.00003668
Iteration 36/1000 | Loss: 0.00003668
Iteration 37/1000 | Loss: 0.00003668
Iteration 38/1000 | Loss: 0.00003668
Iteration 39/1000 | Loss: 0.00003668
Iteration 40/1000 | Loss: 0.00003668
Iteration 41/1000 | Loss: 0.00003667
Iteration 42/1000 | Loss: 0.00003667
Iteration 43/1000 | Loss: 0.00003667
Iteration 44/1000 | Loss: 0.00003667
Iteration 45/1000 | Loss: 0.00003667
Iteration 46/1000 | Loss: 0.00003667
Iteration 47/1000 | Loss: 0.00003666
Iteration 48/1000 | Loss: 0.00003666
Iteration 49/1000 | Loss: 0.00003666
Iteration 50/1000 | Loss: 0.00003665
Iteration 51/1000 | Loss: 0.00003665
Iteration 52/1000 | Loss: 0.00003665
Iteration 53/1000 | Loss: 0.00003664
Iteration 54/1000 | Loss: 0.00003664
Iteration 55/1000 | Loss: 0.00003664
Iteration 56/1000 | Loss: 0.00003664
Iteration 57/1000 | Loss: 0.00003663
Iteration 58/1000 | Loss: 0.00003663
Iteration 59/1000 | Loss: 0.00003663
Iteration 60/1000 | Loss: 0.00003663
Iteration 61/1000 | Loss: 0.00003662
Iteration 62/1000 | Loss: 0.00003662
Iteration 63/1000 | Loss: 0.00003662
Iteration 64/1000 | Loss: 0.00003662
Iteration 65/1000 | Loss: 0.00003662
Iteration 66/1000 | Loss: 0.00003662
Iteration 67/1000 | Loss: 0.00003662
Iteration 68/1000 | Loss: 0.00003661
Iteration 69/1000 | Loss: 0.00003661
Iteration 70/1000 | Loss: 0.00003661
Iteration 71/1000 | Loss: 0.00003661
Iteration 72/1000 | Loss: 0.00003661
Iteration 73/1000 | Loss: 0.00003661
Iteration 74/1000 | Loss: 0.00003660
Iteration 75/1000 | Loss: 0.00003660
Iteration 76/1000 | Loss: 0.00003660
Iteration 77/1000 | Loss: 0.00003660
Iteration 78/1000 | Loss: 0.00003660
Iteration 79/1000 | Loss: 0.00003659
Iteration 80/1000 | Loss: 0.00003659
Iteration 81/1000 | Loss: 0.00003659
Iteration 82/1000 | Loss: 0.00003659
Iteration 83/1000 | Loss: 0.00003658
Iteration 84/1000 | Loss: 0.00003658
Iteration 85/1000 | Loss: 0.00003657
Iteration 86/1000 | Loss: 0.00003657
Iteration 87/1000 | Loss: 0.00003657
Iteration 88/1000 | Loss: 0.00003657
Iteration 89/1000 | Loss: 0.00003657
Iteration 90/1000 | Loss: 0.00003656
Iteration 91/1000 | Loss: 0.00003656
Iteration 92/1000 | Loss: 0.00003656
Iteration 93/1000 | Loss: 0.00003656
Iteration 94/1000 | Loss: 0.00003656
Iteration 95/1000 | Loss: 0.00003656
Iteration 96/1000 | Loss: 0.00003656
Iteration 97/1000 | Loss: 0.00003656
Iteration 98/1000 | Loss: 0.00003656
Iteration 99/1000 | Loss: 0.00003656
Iteration 100/1000 | Loss: 0.00003656
Iteration 101/1000 | Loss: 0.00003656
Iteration 102/1000 | Loss: 0.00003655
Iteration 103/1000 | Loss: 0.00003655
Iteration 104/1000 | Loss: 0.00003655
Iteration 105/1000 | Loss: 0.00003655
Iteration 106/1000 | Loss: 0.00003655
Iteration 107/1000 | Loss: 0.00003655
Iteration 108/1000 | Loss: 0.00003655
Iteration 109/1000 | Loss: 0.00003655
Iteration 110/1000 | Loss: 0.00003654
Iteration 111/1000 | Loss: 0.00003654
Iteration 112/1000 | Loss: 0.00003654
Iteration 113/1000 | Loss: 0.00003654
Iteration 114/1000 | Loss: 0.00003654
Iteration 115/1000 | Loss: 0.00003654
Iteration 116/1000 | Loss: 0.00003654
Iteration 117/1000 | Loss: 0.00003654
Iteration 118/1000 | Loss: 0.00003653
Iteration 119/1000 | Loss: 0.00003653
Iteration 120/1000 | Loss: 0.00003653
Iteration 121/1000 | Loss: 0.00003653
Iteration 122/1000 | Loss: 0.00003653
Iteration 123/1000 | Loss: 0.00003653
Iteration 124/1000 | Loss: 0.00003653
Iteration 125/1000 | Loss: 0.00003653
Iteration 126/1000 | Loss: 0.00003653
Iteration 127/1000 | Loss: 0.00003653
Iteration 128/1000 | Loss: 0.00003653
Iteration 129/1000 | Loss: 0.00003653
Iteration 130/1000 | Loss: 0.00003653
Iteration 131/1000 | Loss: 0.00003653
Iteration 132/1000 | Loss: 0.00003653
Iteration 133/1000 | Loss: 0.00003653
Iteration 134/1000 | Loss: 0.00003653
Iteration 135/1000 | Loss: 0.00003653
Iteration 136/1000 | Loss: 0.00003653
Iteration 137/1000 | Loss: 0.00003653
Iteration 138/1000 | Loss: 0.00003652
Iteration 139/1000 | Loss: 0.00003652
Iteration 140/1000 | Loss: 0.00003652
Iteration 141/1000 | Loss: 0.00003652
Iteration 142/1000 | Loss: 0.00003652
Iteration 143/1000 | Loss: 0.00003652
Iteration 144/1000 | Loss: 0.00003652
Iteration 145/1000 | Loss: 0.00003652
Iteration 146/1000 | Loss: 0.00003652
Iteration 147/1000 | Loss: 0.00003652
Iteration 148/1000 | Loss: 0.00003652
Iteration 149/1000 | Loss: 0.00003652
Iteration 150/1000 | Loss: 0.00003652
Iteration 151/1000 | Loss: 0.00003652
Iteration 152/1000 | Loss: 0.00003652
Iteration 153/1000 | Loss: 0.00003652
Iteration 154/1000 | Loss: 0.00003651
Iteration 155/1000 | Loss: 0.00003651
Iteration 156/1000 | Loss: 0.00003651
Iteration 157/1000 | Loss: 0.00003651
Iteration 158/1000 | Loss: 0.00003651
Iteration 159/1000 | Loss: 0.00003651
Iteration 160/1000 | Loss: 0.00003651
Iteration 161/1000 | Loss: 0.00003651
Iteration 162/1000 | Loss: 0.00003651
Iteration 163/1000 | Loss: 0.00003651
Iteration 164/1000 | Loss: 0.00003651
Iteration 165/1000 | Loss: 0.00003651
Iteration 166/1000 | Loss: 0.00003651
Iteration 167/1000 | Loss: 0.00003651
Iteration 168/1000 | Loss: 0.00003651
Iteration 169/1000 | Loss: 0.00003651
Iteration 170/1000 | Loss: 0.00003651
Iteration 171/1000 | Loss: 0.00003651
Iteration 172/1000 | Loss: 0.00003651
Iteration 173/1000 | Loss: 0.00003651
Iteration 174/1000 | Loss: 0.00003651
Iteration 175/1000 | Loss: 0.00003651
Iteration 176/1000 | Loss: 0.00003650
Iteration 177/1000 | Loss: 0.00003650
Iteration 178/1000 | Loss: 0.00003650
Iteration 179/1000 | Loss: 0.00003650
Iteration 180/1000 | Loss: 0.00003650
Iteration 181/1000 | Loss: 0.00003650
Iteration 182/1000 | Loss: 0.00003650
Iteration 183/1000 | Loss: 0.00003650
Iteration 184/1000 | Loss: 0.00003650
Iteration 185/1000 | Loss: 0.00003650
Iteration 186/1000 | Loss: 0.00003650
Iteration 187/1000 | Loss: 0.00003649
Iteration 188/1000 | Loss: 0.00003649
Iteration 189/1000 | Loss: 0.00003649
Iteration 190/1000 | Loss: 0.00003649
Iteration 191/1000 | Loss: 0.00003649
Iteration 192/1000 | Loss: 0.00003649
Iteration 193/1000 | Loss: 0.00003649
Iteration 194/1000 | Loss: 0.00003649
Iteration 195/1000 | Loss: 0.00003649
Iteration 196/1000 | Loss: 0.00003649
Iteration 197/1000 | Loss: 0.00003649
Iteration 198/1000 | Loss: 0.00003649
Iteration 199/1000 | Loss: 0.00003649
Iteration 200/1000 | Loss: 0.00003649
Iteration 201/1000 | Loss: 0.00003649
Iteration 202/1000 | Loss: 0.00003649
Iteration 203/1000 | Loss: 0.00003649
Iteration 204/1000 | Loss: 0.00003649
Iteration 205/1000 | Loss: 0.00003649
Iteration 206/1000 | Loss: 0.00003649
Iteration 207/1000 | Loss: 0.00003649
Iteration 208/1000 | Loss: 0.00003649
Iteration 209/1000 | Loss: 0.00003649
Iteration 210/1000 | Loss: 0.00003649
Iteration 211/1000 | Loss: 0.00003649
Iteration 212/1000 | Loss: 0.00003649
Iteration 213/1000 | Loss: 0.00003649
Iteration 214/1000 | Loss: 0.00003649
Iteration 215/1000 | Loss: 0.00003649
Iteration 216/1000 | Loss: 0.00003649
Iteration 217/1000 | Loss: 0.00003649
Iteration 218/1000 | Loss: 0.00003649
Iteration 219/1000 | Loss: 0.00003649
Iteration 220/1000 | Loss: 0.00003649
Iteration 221/1000 | Loss: 0.00003649
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 221. Stopping optimization.
Last 5 losses: [3.649430800578557e-05, 3.649430800578557e-05, 3.649430800578557e-05, 3.649430800578557e-05, 3.649430800578557e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.649430800578557e-05

Optimization complete. Final v2v error: 5.075262069702148 mm

Highest mean error: 6.682981967926025 mm for frame 103

Lowest mean error: 4.223595142364502 mm for frame 137

Saving results

Total time: 51.90217709541321
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_42_us_0617/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_42_us_0617/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_42_us_0617/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00585746
Iteration 2/25 | Loss: 0.00180482
Iteration 3/25 | Loss: 0.00170494
Iteration 4/25 | Loss: 0.00167753
Iteration 5/25 | Loss: 0.00166905
Iteration 6/25 | Loss: 0.00166617
Iteration 7/25 | Loss: 0.00166590
Iteration 8/25 | Loss: 0.00166590
Iteration 9/25 | Loss: 0.00166590
Iteration 10/25 | Loss: 0.00166590
Iteration 11/25 | Loss: 0.00166590
Iteration 12/25 | Loss: 0.00166590
Iteration 13/25 | Loss: 0.00166590
Iteration 14/25 | Loss: 0.00166590
Iteration 15/25 | Loss: 0.00166590
Iteration 16/25 | Loss: 0.00166590
Iteration 17/25 | Loss: 0.00166590
Iteration 18/25 | Loss: 0.00166590
Iteration 19/25 | Loss: 0.00166590
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0016658984823152423, 0.0016658984823152423, 0.0016658984823152423, 0.0016658984823152423, 0.0016658984823152423]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016658984823152423

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.08962476
Iteration 2/25 | Loss: 0.00178772
Iteration 3/25 | Loss: 0.00178770
Iteration 4/25 | Loss: 0.00178770
Iteration 5/25 | Loss: 0.00178770
Iteration 6/25 | Loss: 0.00178770
Iteration 7/25 | Loss: 0.00178770
Iteration 8/25 | Loss: 0.00178770
Iteration 9/25 | Loss: 0.00178770
Iteration 10/25 | Loss: 0.00178770
Iteration 11/25 | Loss: 0.00178770
Iteration 12/25 | Loss: 0.00178770
Iteration 13/25 | Loss: 0.00178770
Iteration 14/25 | Loss: 0.00178770
Iteration 15/25 | Loss: 0.00178770
Iteration 16/25 | Loss: 0.00178770
Iteration 17/25 | Loss: 0.00178770
Iteration 18/25 | Loss: 0.00178770
Iteration 19/25 | Loss: 0.00178770
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001787699875421822, 0.001787699875421822, 0.001787699875421822, 0.001787699875421822, 0.001787699875421822]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001787699875421822

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00178770
Iteration 2/1000 | Loss: 0.00006985
Iteration 3/1000 | Loss: 0.00005345
Iteration 4/1000 | Loss: 0.00004779
Iteration 5/1000 | Loss: 0.00004421
Iteration 6/1000 | Loss: 0.00004262
Iteration 7/1000 | Loss: 0.00004121
Iteration 8/1000 | Loss: 0.00004063
Iteration 9/1000 | Loss: 0.00004005
Iteration 10/1000 | Loss: 0.00003976
Iteration 11/1000 | Loss: 0.00003960
Iteration 12/1000 | Loss: 0.00003941
Iteration 13/1000 | Loss: 0.00003931
Iteration 14/1000 | Loss: 0.00003930
Iteration 15/1000 | Loss: 0.00003929
Iteration 16/1000 | Loss: 0.00003927
Iteration 17/1000 | Loss: 0.00003923
Iteration 18/1000 | Loss: 0.00003917
Iteration 19/1000 | Loss: 0.00003916
Iteration 20/1000 | Loss: 0.00003916
Iteration 21/1000 | Loss: 0.00003916
Iteration 22/1000 | Loss: 0.00003914
Iteration 23/1000 | Loss: 0.00003913
Iteration 24/1000 | Loss: 0.00003911
Iteration 25/1000 | Loss: 0.00003911
Iteration 26/1000 | Loss: 0.00003907
Iteration 27/1000 | Loss: 0.00003906
Iteration 28/1000 | Loss: 0.00003904
Iteration 29/1000 | Loss: 0.00003903
Iteration 30/1000 | Loss: 0.00003902
Iteration 31/1000 | Loss: 0.00003902
Iteration 32/1000 | Loss: 0.00003902
Iteration 33/1000 | Loss: 0.00003901
Iteration 34/1000 | Loss: 0.00003901
Iteration 35/1000 | Loss: 0.00003901
Iteration 36/1000 | Loss: 0.00003901
Iteration 37/1000 | Loss: 0.00003900
Iteration 38/1000 | Loss: 0.00003899
Iteration 39/1000 | Loss: 0.00003899
Iteration 40/1000 | Loss: 0.00003899
Iteration 41/1000 | Loss: 0.00003899
Iteration 42/1000 | Loss: 0.00003898
Iteration 43/1000 | Loss: 0.00003898
Iteration 44/1000 | Loss: 0.00003898
Iteration 45/1000 | Loss: 0.00003898
Iteration 46/1000 | Loss: 0.00003898
Iteration 47/1000 | Loss: 0.00003898
Iteration 48/1000 | Loss: 0.00003898
Iteration 49/1000 | Loss: 0.00003898
Iteration 50/1000 | Loss: 0.00003898
Iteration 51/1000 | Loss: 0.00003897
Iteration 52/1000 | Loss: 0.00003897
Iteration 53/1000 | Loss: 0.00003897
Iteration 54/1000 | Loss: 0.00003897
Iteration 55/1000 | Loss: 0.00003897
Iteration 56/1000 | Loss: 0.00003897
Iteration 57/1000 | Loss: 0.00003897
Iteration 58/1000 | Loss: 0.00003897
Iteration 59/1000 | Loss: 0.00003897
Iteration 60/1000 | Loss: 0.00003897
Iteration 61/1000 | Loss: 0.00003896
Iteration 62/1000 | Loss: 0.00003896
Iteration 63/1000 | Loss: 0.00003896
Iteration 64/1000 | Loss: 0.00003895
Iteration 65/1000 | Loss: 0.00003895
Iteration 66/1000 | Loss: 0.00003895
Iteration 67/1000 | Loss: 0.00003895
Iteration 68/1000 | Loss: 0.00003893
Iteration 69/1000 | Loss: 0.00003892
Iteration 70/1000 | Loss: 0.00003892
Iteration 71/1000 | Loss: 0.00003892
Iteration 72/1000 | Loss: 0.00003892
Iteration 73/1000 | Loss: 0.00003892
Iteration 74/1000 | Loss: 0.00003892
Iteration 75/1000 | Loss: 0.00003892
Iteration 76/1000 | Loss: 0.00003892
Iteration 77/1000 | Loss: 0.00003892
Iteration 78/1000 | Loss: 0.00003892
Iteration 79/1000 | Loss: 0.00003892
Iteration 80/1000 | Loss: 0.00003892
Iteration 81/1000 | Loss: 0.00003892
Iteration 82/1000 | Loss: 0.00003892
Iteration 83/1000 | Loss: 0.00003892
Iteration 84/1000 | Loss: 0.00003892
Iteration 85/1000 | Loss: 0.00003892
Iteration 86/1000 | Loss: 0.00003892
Iteration 87/1000 | Loss: 0.00003892
Iteration 88/1000 | Loss: 0.00003892
Iteration 89/1000 | Loss: 0.00003892
Iteration 90/1000 | Loss: 0.00003892
Iteration 91/1000 | Loss: 0.00003892
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [3.8915655750315636e-05, 3.8915655750315636e-05, 3.8915655750315636e-05, 3.8915655750315636e-05, 3.8915655750315636e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.8915655750315636e-05

Optimization complete. Final v2v error: 5.180437088012695 mm

Highest mean error: 5.515462398529053 mm for frame 83

Lowest mean error: 5.084570407867432 mm for frame 160

Saving results

Total time: 40.44081258773804
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_42_us_0617/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_42_us_0617/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_42_us_0617/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00558716
Iteration 2/25 | Loss: 0.00165413
Iteration 3/25 | Loss: 0.00154042
Iteration 4/25 | Loss: 0.00151165
Iteration 5/25 | Loss: 0.00150616
Iteration 6/25 | Loss: 0.00150520
Iteration 7/25 | Loss: 0.00150520
Iteration 8/25 | Loss: 0.00150519
Iteration 9/25 | Loss: 0.00150519
Iteration 10/25 | Loss: 0.00150519
Iteration 11/25 | Loss: 0.00150519
Iteration 12/25 | Loss: 0.00150519
Iteration 13/25 | Loss: 0.00150519
Iteration 14/25 | Loss: 0.00150519
Iteration 15/25 | Loss: 0.00150519
Iteration 16/25 | Loss: 0.00150519
Iteration 17/25 | Loss: 0.00150519
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0015051857335492969, 0.0015051857335492969, 0.0015051857335492969, 0.0015051857335492969, 0.0015051857335492969]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015051857335492969

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46849167
Iteration 2/25 | Loss: 0.00144482
Iteration 3/25 | Loss: 0.00144481
Iteration 4/25 | Loss: 0.00144481
Iteration 5/25 | Loss: 0.00144481
Iteration 6/25 | Loss: 0.00144481
Iteration 7/25 | Loss: 0.00144481
Iteration 8/25 | Loss: 0.00144481
Iteration 9/25 | Loss: 0.00144481
Iteration 10/25 | Loss: 0.00144481
Iteration 11/25 | Loss: 0.00144481
Iteration 12/25 | Loss: 0.00144481
Iteration 13/25 | Loss: 0.00144481
Iteration 14/25 | Loss: 0.00144481
Iteration 15/25 | Loss: 0.00144481
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001444812398403883, 0.001444812398403883, 0.001444812398403883, 0.001444812398403883, 0.001444812398403883]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001444812398403883

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00144481
Iteration 2/1000 | Loss: 0.00006619
Iteration 3/1000 | Loss: 0.00005509
Iteration 4/1000 | Loss: 0.00005145
Iteration 5/1000 | Loss: 0.00004951
Iteration 6/1000 | Loss: 0.00004824
Iteration 7/1000 | Loss: 0.00004719
Iteration 8/1000 | Loss: 0.00004643
Iteration 9/1000 | Loss: 0.00004601
Iteration 10/1000 | Loss: 0.00004587
Iteration 11/1000 | Loss: 0.00004585
Iteration 12/1000 | Loss: 0.00004583
Iteration 13/1000 | Loss: 0.00004578
Iteration 14/1000 | Loss: 0.00004578
Iteration 15/1000 | Loss: 0.00004578
Iteration 16/1000 | Loss: 0.00004576
Iteration 17/1000 | Loss: 0.00004569
Iteration 18/1000 | Loss: 0.00004566
Iteration 19/1000 | Loss: 0.00004564
Iteration 20/1000 | Loss: 0.00004563
Iteration 21/1000 | Loss: 0.00004562
Iteration 22/1000 | Loss: 0.00004562
Iteration 23/1000 | Loss: 0.00004562
Iteration 24/1000 | Loss: 0.00004562
Iteration 25/1000 | Loss: 0.00004561
Iteration 26/1000 | Loss: 0.00004561
Iteration 27/1000 | Loss: 0.00004560
Iteration 28/1000 | Loss: 0.00004556
Iteration 29/1000 | Loss: 0.00004556
Iteration 30/1000 | Loss: 0.00004555
Iteration 31/1000 | Loss: 0.00004554
Iteration 32/1000 | Loss: 0.00004554
Iteration 33/1000 | Loss: 0.00004553
Iteration 34/1000 | Loss: 0.00004553
Iteration 35/1000 | Loss: 0.00004552
Iteration 36/1000 | Loss: 0.00004552
Iteration 37/1000 | Loss: 0.00004552
Iteration 38/1000 | Loss: 0.00004551
Iteration 39/1000 | Loss: 0.00004551
Iteration 40/1000 | Loss: 0.00004551
Iteration 41/1000 | Loss: 0.00004551
Iteration 42/1000 | Loss: 0.00004551
Iteration 43/1000 | Loss: 0.00004550
Iteration 44/1000 | Loss: 0.00004550
Iteration 45/1000 | Loss: 0.00004549
Iteration 46/1000 | Loss: 0.00004549
Iteration 47/1000 | Loss: 0.00004549
Iteration 48/1000 | Loss: 0.00004549
Iteration 49/1000 | Loss: 0.00004549
Iteration 50/1000 | Loss: 0.00004549
Iteration 51/1000 | Loss: 0.00004549
Iteration 52/1000 | Loss: 0.00004548
Iteration 53/1000 | Loss: 0.00004548
Iteration 54/1000 | Loss: 0.00004548
Iteration 55/1000 | Loss: 0.00004548
Iteration 56/1000 | Loss: 0.00004548
Iteration 57/1000 | Loss: 0.00004547
Iteration 58/1000 | Loss: 0.00004547
Iteration 59/1000 | Loss: 0.00004547
Iteration 60/1000 | Loss: 0.00004547
Iteration 61/1000 | Loss: 0.00004547
Iteration 62/1000 | Loss: 0.00004547
Iteration 63/1000 | Loss: 0.00004547
Iteration 64/1000 | Loss: 0.00004547
Iteration 65/1000 | Loss: 0.00004546
Iteration 66/1000 | Loss: 0.00004546
Iteration 67/1000 | Loss: 0.00004546
Iteration 68/1000 | Loss: 0.00004546
Iteration 69/1000 | Loss: 0.00004546
Iteration 70/1000 | Loss: 0.00004546
Iteration 71/1000 | Loss: 0.00004546
Iteration 72/1000 | Loss: 0.00004546
Iteration 73/1000 | Loss: 0.00004546
Iteration 74/1000 | Loss: 0.00004546
Iteration 75/1000 | Loss: 0.00004546
Iteration 76/1000 | Loss: 0.00004546
Iteration 77/1000 | Loss: 0.00004545
Iteration 78/1000 | Loss: 0.00004545
Iteration 79/1000 | Loss: 0.00004545
Iteration 80/1000 | Loss: 0.00004545
Iteration 81/1000 | Loss: 0.00004545
Iteration 82/1000 | Loss: 0.00004545
Iteration 83/1000 | Loss: 0.00004545
Iteration 84/1000 | Loss: 0.00004545
Iteration 85/1000 | Loss: 0.00004545
Iteration 86/1000 | Loss: 0.00004545
Iteration 87/1000 | Loss: 0.00004545
Iteration 88/1000 | Loss: 0.00004545
Iteration 89/1000 | Loss: 0.00004545
Iteration 90/1000 | Loss: 0.00004544
Iteration 91/1000 | Loss: 0.00004544
Iteration 92/1000 | Loss: 0.00004544
Iteration 93/1000 | Loss: 0.00004544
Iteration 94/1000 | Loss: 0.00004544
Iteration 95/1000 | Loss: 0.00004544
Iteration 96/1000 | Loss: 0.00004544
Iteration 97/1000 | Loss: 0.00004544
Iteration 98/1000 | Loss: 0.00004543
Iteration 99/1000 | Loss: 0.00004543
Iteration 100/1000 | Loss: 0.00004543
Iteration 101/1000 | Loss: 0.00004543
Iteration 102/1000 | Loss: 0.00004543
Iteration 103/1000 | Loss: 0.00004543
Iteration 104/1000 | Loss: 0.00004543
Iteration 105/1000 | Loss: 0.00004542
Iteration 106/1000 | Loss: 0.00004542
Iteration 107/1000 | Loss: 0.00004542
Iteration 108/1000 | Loss: 0.00004542
Iteration 109/1000 | Loss: 0.00004542
Iteration 110/1000 | Loss: 0.00004542
Iteration 111/1000 | Loss: 0.00004542
Iteration 112/1000 | Loss: 0.00004542
Iteration 113/1000 | Loss: 0.00004542
Iteration 114/1000 | Loss: 0.00004542
Iteration 115/1000 | Loss: 0.00004542
Iteration 116/1000 | Loss: 0.00004542
Iteration 117/1000 | Loss: 0.00004541
Iteration 118/1000 | Loss: 0.00004541
Iteration 119/1000 | Loss: 0.00004541
Iteration 120/1000 | Loss: 0.00004541
Iteration 121/1000 | Loss: 0.00004541
Iteration 122/1000 | Loss: 0.00004541
Iteration 123/1000 | Loss: 0.00004541
Iteration 124/1000 | Loss: 0.00004541
Iteration 125/1000 | Loss: 0.00004541
Iteration 126/1000 | Loss: 0.00004541
Iteration 127/1000 | Loss: 0.00004541
Iteration 128/1000 | Loss: 0.00004541
Iteration 129/1000 | Loss: 0.00004541
Iteration 130/1000 | Loss: 0.00004541
Iteration 131/1000 | Loss: 0.00004541
Iteration 132/1000 | Loss: 0.00004541
Iteration 133/1000 | Loss: 0.00004541
Iteration 134/1000 | Loss: 0.00004540
Iteration 135/1000 | Loss: 0.00004540
Iteration 136/1000 | Loss: 0.00004540
Iteration 137/1000 | Loss: 0.00004540
Iteration 138/1000 | Loss: 0.00004540
Iteration 139/1000 | Loss: 0.00004540
Iteration 140/1000 | Loss: 0.00004540
Iteration 141/1000 | Loss: 0.00004540
Iteration 142/1000 | Loss: 0.00004540
Iteration 143/1000 | Loss: 0.00004540
Iteration 144/1000 | Loss: 0.00004540
Iteration 145/1000 | Loss: 0.00004540
Iteration 146/1000 | Loss: 0.00004540
Iteration 147/1000 | Loss: 0.00004540
Iteration 148/1000 | Loss: 0.00004540
Iteration 149/1000 | Loss: 0.00004540
Iteration 150/1000 | Loss: 0.00004540
Iteration 151/1000 | Loss: 0.00004540
Iteration 152/1000 | Loss: 0.00004540
Iteration 153/1000 | Loss: 0.00004539
Iteration 154/1000 | Loss: 0.00004539
Iteration 155/1000 | Loss: 0.00004539
Iteration 156/1000 | Loss: 0.00004539
Iteration 157/1000 | Loss: 0.00004539
Iteration 158/1000 | Loss: 0.00004539
Iteration 159/1000 | Loss: 0.00004539
Iteration 160/1000 | Loss: 0.00004539
Iteration 161/1000 | Loss: 0.00004539
Iteration 162/1000 | Loss: 0.00004539
Iteration 163/1000 | Loss: 0.00004539
Iteration 164/1000 | Loss: 0.00004539
Iteration 165/1000 | Loss: 0.00004539
Iteration 166/1000 | Loss: 0.00004539
Iteration 167/1000 | Loss: 0.00004539
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [4.539404471870512e-05, 4.539404471870512e-05, 4.539404471870512e-05, 4.539404471870512e-05, 4.539404471870512e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.539404471870512e-05

Optimization complete. Final v2v error: 5.494830131530762 mm

Highest mean error: 6.673887729644775 mm for frame 77

Lowest mean error: 4.814276695251465 mm for frame 114

Saving results

Total time: 36.40371489524841
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_42_us_0617/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_42_us_0617/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_42_us_0617/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00832740
Iteration 2/25 | Loss: 0.00181083
Iteration 3/25 | Loss: 0.00147262
Iteration 4/25 | Loss: 0.00143863
Iteration 5/25 | Loss: 0.00143484
Iteration 6/25 | Loss: 0.00141976
Iteration 7/25 | Loss: 0.00142308
Iteration 8/25 | Loss: 0.00141921
Iteration 9/25 | Loss: 0.00141759
Iteration 10/25 | Loss: 0.00142071
Iteration 11/25 | Loss: 0.00141894
Iteration 12/25 | Loss: 0.00141711
Iteration 13/25 | Loss: 0.00141675
Iteration 14/25 | Loss: 0.00141647
Iteration 15/25 | Loss: 0.00141644
Iteration 16/25 | Loss: 0.00141688
Iteration 17/25 | Loss: 0.00141581
Iteration 18/25 | Loss: 0.00141650
Iteration 19/25 | Loss: 0.00141618
Iteration 20/25 | Loss: 0.00141540
Iteration 21/25 | Loss: 0.00141526
Iteration 22/25 | Loss: 0.00141517
Iteration 23/25 | Loss: 0.00141516
Iteration 24/25 | Loss: 0.00141515
Iteration 25/25 | Loss: 0.00141514

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.15666389
Iteration 2/25 | Loss: 0.00143444
Iteration 3/25 | Loss: 0.00143441
Iteration 4/25 | Loss: 0.00143441
Iteration 5/25 | Loss: 0.00143441
Iteration 6/25 | Loss: 0.00143441
Iteration 7/25 | Loss: 0.00143441
Iteration 8/25 | Loss: 0.00143441
Iteration 9/25 | Loss: 0.00143441
Iteration 10/25 | Loss: 0.00143441
Iteration 11/25 | Loss: 0.00143441
Iteration 12/25 | Loss: 0.00143441
Iteration 13/25 | Loss: 0.00143441
Iteration 14/25 | Loss: 0.00143441
Iteration 15/25 | Loss: 0.00143441
Iteration 16/25 | Loss: 0.00143441
Iteration 17/25 | Loss: 0.00143441
Iteration 18/25 | Loss: 0.00143441
Iteration 19/25 | Loss: 0.00143441
Iteration 20/25 | Loss: 0.00143441
Iteration 21/25 | Loss: 0.00143441
Iteration 22/25 | Loss: 0.00143441
Iteration 23/25 | Loss: 0.00143441
Iteration 24/25 | Loss: 0.00143441
Iteration 25/25 | Loss: 0.00143441

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00143441
Iteration 2/1000 | Loss: 0.00005427
Iteration 3/1000 | Loss: 0.00004201
Iteration 4/1000 | Loss: 0.00003845
Iteration 5/1000 | Loss: 0.00003654
Iteration 6/1000 | Loss: 0.00003547
Iteration 7/1000 | Loss: 0.00003480
Iteration 8/1000 | Loss: 0.00003435
Iteration 9/1000 | Loss: 0.00003398
Iteration 10/1000 | Loss: 0.00003370
Iteration 11/1000 | Loss: 0.00003349
Iteration 12/1000 | Loss: 0.00003344
Iteration 13/1000 | Loss: 0.00003344
Iteration 14/1000 | Loss: 0.00003343
Iteration 15/1000 | Loss: 0.00003342
Iteration 16/1000 | Loss: 0.00003342
Iteration 17/1000 | Loss: 0.00003341
Iteration 18/1000 | Loss: 0.00003341
Iteration 19/1000 | Loss: 0.00003339
Iteration 20/1000 | Loss: 0.00003339
Iteration 21/1000 | Loss: 0.00003338
Iteration 22/1000 | Loss: 0.00003337
Iteration 23/1000 | Loss: 0.00003337
Iteration 24/1000 | Loss: 0.00003336
Iteration 25/1000 | Loss: 0.00003336
Iteration 26/1000 | Loss: 0.00003336
Iteration 27/1000 | Loss: 0.00003335
Iteration 28/1000 | Loss: 0.00003335
Iteration 29/1000 | Loss: 0.00003335
Iteration 30/1000 | Loss: 0.00003334
Iteration 31/1000 | Loss: 0.00003334
Iteration 32/1000 | Loss: 0.00003333
Iteration 33/1000 | Loss: 0.00003333
Iteration 34/1000 | Loss: 0.00003333
Iteration 35/1000 | Loss: 0.00003332
Iteration 36/1000 | Loss: 0.00003332
Iteration 37/1000 | Loss: 0.00003332
Iteration 38/1000 | Loss: 0.00003332
Iteration 39/1000 | Loss: 0.00003331
Iteration 40/1000 | Loss: 0.00003331
Iteration 41/1000 | Loss: 0.00003331
Iteration 42/1000 | Loss: 0.00003331
Iteration 43/1000 | Loss: 0.00003331
Iteration 44/1000 | Loss: 0.00003331
Iteration 45/1000 | Loss: 0.00003330
Iteration 46/1000 | Loss: 0.00003330
Iteration 47/1000 | Loss: 0.00003330
Iteration 48/1000 | Loss: 0.00003330
Iteration 49/1000 | Loss: 0.00003330
Iteration 50/1000 | Loss: 0.00003330
Iteration 51/1000 | Loss: 0.00003330
Iteration 52/1000 | Loss: 0.00003329
Iteration 53/1000 | Loss: 0.00003328
Iteration 54/1000 | Loss: 0.00003328
Iteration 55/1000 | Loss: 0.00003328
Iteration 56/1000 | Loss: 0.00003328
Iteration 57/1000 | Loss: 0.00003327
Iteration 58/1000 | Loss: 0.00003327
Iteration 59/1000 | Loss: 0.00003327
Iteration 60/1000 | Loss: 0.00003326
Iteration 61/1000 | Loss: 0.00003326
Iteration 62/1000 | Loss: 0.00003326
Iteration 63/1000 | Loss: 0.00003325
Iteration 64/1000 | Loss: 0.00003325
Iteration 65/1000 | Loss: 0.00003325
Iteration 66/1000 | Loss: 0.00003325
Iteration 67/1000 | Loss: 0.00003325
Iteration 68/1000 | Loss: 0.00003324
Iteration 69/1000 | Loss: 0.00003324
Iteration 70/1000 | Loss: 0.00003324
Iteration 71/1000 | Loss: 0.00003324
Iteration 72/1000 | Loss: 0.00003324
Iteration 73/1000 | Loss: 0.00003324
Iteration 74/1000 | Loss: 0.00003324
Iteration 75/1000 | Loss: 0.00003324
Iteration 76/1000 | Loss: 0.00003324
Iteration 77/1000 | Loss: 0.00003324
Iteration 78/1000 | Loss: 0.00003323
Iteration 79/1000 | Loss: 0.00003323
Iteration 80/1000 | Loss: 0.00003323
Iteration 81/1000 | Loss: 0.00003323
Iteration 82/1000 | Loss: 0.00003323
Iteration 83/1000 | Loss: 0.00003323
Iteration 84/1000 | Loss: 0.00003323
Iteration 85/1000 | Loss: 0.00003323
Iteration 86/1000 | Loss: 0.00003323
Iteration 87/1000 | Loss: 0.00003323
Iteration 88/1000 | Loss: 0.00003323
Iteration 89/1000 | Loss: 0.00003323
Iteration 90/1000 | Loss: 0.00003323
Iteration 91/1000 | Loss: 0.00003322
Iteration 92/1000 | Loss: 0.00003322
Iteration 93/1000 | Loss: 0.00003322
Iteration 94/1000 | Loss: 0.00003322
Iteration 95/1000 | Loss: 0.00003322
Iteration 96/1000 | Loss: 0.00003322
Iteration 97/1000 | Loss: 0.00003322
Iteration 98/1000 | Loss: 0.00003322
Iteration 99/1000 | Loss: 0.00003322
Iteration 100/1000 | Loss: 0.00003322
Iteration 101/1000 | Loss: 0.00003322
Iteration 102/1000 | Loss: 0.00003322
Iteration 103/1000 | Loss: 0.00003322
Iteration 104/1000 | Loss: 0.00003322
Iteration 105/1000 | Loss: 0.00003322
Iteration 106/1000 | Loss: 0.00003322
Iteration 107/1000 | Loss: 0.00003322
Iteration 108/1000 | Loss: 0.00003322
Iteration 109/1000 | Loss: 0.00003322
Iteration 110/1000 | Loss: 0.00003322
Iteration 111/1000 | Loss: 0.00003322
Iteration 112/1000 | Loss: 0.00003322
Iteration 113/1000 | Loss: 0.00003322
Iteration 114/1000 | Loss: 0.00003322
Iteration 115/1000 | Loss: 0.00003322
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [3.3217893360415474e-05, 3.3217893360415474e-05, 3.3217893360415474e-05, 3.3217893360415474e-05, 3.3217893360415474e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.3217893360415474e-05

Optimization complete. Final v2v error: 4.930357933044434 mm

Highest mean error: 11.138643264770508 mm for frame 113

Lowest mean error: 4.351141929626465 mm for frame 188

Saving results

Total time: 71.17646980285645
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_42_us_0617/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_42_us_0617/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_42_us_0617/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00493832
Iteration 2/25 | Loss: 0.00154407
Iteration 3/25 | Loss: 0.00146308
Iteration 4/25 | Loss: 0.00145292
Iteration 5/25 | Loss: 0.00144866
Iteration 6/25 | Loss: 0.00144768
Iteration 7/25 | Loss: 0.00144768
Iteration 8/25 | Loss: 0.00144768
Iteration 9/25 | Loss: 0.00144768
Iteration 10/25 | Loss: 0.00144768
Iteration 11/25 | Loss: 0.00144768
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001447676564566791, 0.001447676564566791, 0.001447676564566791, 0.001447676564566791, 0.001447676564566791]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001447676564566791

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.45219231
Iteration 2/25 | Loss: 0.00140279
Iteration 3/25 | Loss: 0.00140278
Iteration 4/25 | Loss: 0.00140278
Iteration 5/25 | Loss: 0.00140278
Iteration 6/25 | Loss: 0.00140278
Iteration 7/25 | Loss: 0.00140278
Iteration 8/25 | Loss: 0.00140278
Iteration 9/25 | Loss: 0.00140278
Iteration 10/25 | Loss: 0.00140278
Iteration 11/25 | Loss: 0.00140278
Iteration 12/25 | Loss: 0.00140278
Iteration 13/25 | Loss: 0.00140278
Iteration 14/25 | Loss: 0.00140278
Iteration 15/25 | Loss: 0.00140278
Iteration 16/25 | Loss: 0.00140278
Iteration 17/25 | Loss: 0.00140278
Iteration 18/25 | Loss: 0.00140278
Iteration 19/25 | Loss: 0.00140278
Iteration 20/25 | Loss: 0.00140278
Iteration 21/25 | Loss: 0.00140278
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001402781461365521, 0.001402781461365521, 0.001402781461365521, 0.001402781461365521, 0.001402781461365521]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001402781461365521

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00140278
Iteration 2/1000 | Loss: 0.00004142
Iteration 3/1000 | Loss: 0.00003454
Iteration 4/1000 | Loss: 0.00003184
Iteration 5/1000 | Loss: 0.00003068
Iteration 6/1000 | Loss: 0.00002977
Iteration 7/1000 | Loss: 0.00002916
Iteration 8/1000 | Loss: 0.00002914
Iteration 9/1000 | Loss: 0.00002875
Iteration 10/1000 | Loss: 0.00002851
Iteration 11/1000 | Loss: 0.00002830
Iteration 12/1000 | Loss: 0.00002823
Iteration 13/1000 | Loss: 0.00002820
Iteration 14/1000 | Loss: 0.00002812
Iteration 15/1000 | Loss: 0.00002808
Iteration 16/1000 | Loss: 0.00002808
Iteration 17/1000 | Loss: 0.00002808
Iteration 18/1000 | Loss: 0.00002807
Iteration 19/1000 | Loss: 0.00002807
Iteration 20/1000 | Loss: 0.00002806
Iteration 21/1000 | Loss: 0.00002806
Iteration 22/1000 | Loss: 0.00002805
Iteration 23/1000 | Loss: 0.00002804
Iteration 24/1000 | Loss: 0.00002804
Iteration 25/1000 | Loss: 0.00002804
Iteration 26/1000 | Loss: 0.00002804
Iteration 27/1000 | Loss: 0.00002804
Iteration 28/1000 | Loss: 0.00002804
Iteration 29/1000 | Loss: 0.00002804
Iteration 30/1000 | Loss: 0.00002803
Iteration 31/1000 | Loss: 0.00002803
Iteration 32/1000 | Loss: 0.00002803
Iteration 33/1000 | Loss: 0.00002803
Iteration 34/1000 | Loss: 0.00002803
Iteration 35/1000 | Loss: 0.00002803
Iteration 36/1000 | Loss: 0.00002803
Iteration 37/1000 | Loss: 0.00002803
Iteration 38/1000 | Loss: 0.00002803
Iteration 39/1000 | Loss: 0.00002803
Iteration 40/1000 | Loss: 0.00002803
Iteration 41/1000 | Loss: 0.00002803
Iteration 42/1000 | Loss: 0.00002802
Iteration 43/1000 | Loss: 0.00002802
Iteration 44/1000 | Loss: 0.00002802
Iteration 45/1000 | Loss: 0.00002801
Iteration 46/1000 | Loss: 0.00002801
Iteration 47/1000 | Loss: 0.00002801
Iteration 48/1000 | Loss: 0.00002800
Iteration 49/1000 | Loss: 0.00002800
Iteration 50/1000 | Loss: 0.00002799
Iteration 51/1000 | Loss: 0.00002799
Iteration 52/1000 | Loss: 0.00002798
Iteration 53/1000 | Loss: 0.00002798
Iteration 54/1000 | Loss: 0.00002798
Iteration 55/1000 | Loss: 0.00002798
Iteration 56/1000 | Loss: 0.00002798
Iteration 57/1000 | Loss: 0.00002797
Iteration 58/1000 | Loss: 0.00002797
Iteration 59/1000 | Loss: 0.00002797
Iteration 60/1000 | Loss: 0.00002797
Iteration 61/1000 | Loss: 0.00002797
Iteration 62/1000 | Loss: 0.00002797
Iteration 63/1000 | Loss: 0.00002796
Iteration 64/1000 | Loss: 0.00002796
Iteration 65/1000 | Loss: 0.00002796
Iteration 66/1000 | Loss: 0.00002795
Iteration 67/1000 | Loss: 0.00002795
Iteration 68/1000 | Loss: 0.00002795
Iteration 69/1000 | Loss: 0.00002795
Iteration 70/1000 | Loss: 0.00002795
Iteration 71/1000 | Loss: 0.00002795
Iteration 72/1000 | Loss: 0.00002795
Iteration 73/1000 | Loss: 0.00002795
Iteration 74/1000 | Loss: 0.00002795
Iteration 75/1000 | Loss: 0.00002794
Iteration 76/1000 | Loss: 0.00002794
Iteration 77/1000 | Loss: 0.00002794
Iteration 78/1000 | Loss: 0.00002794
Iteration 79/1000 | Loss: 0.00002793
Iteration 80/1000 | Loss: 0.00002793
Iteration 81/1000 | Loss: 0.00002793
Iteration 82/1000 | Loss: 0.00002793
Iteration 83/1000 | Loss: 0.00002793
Iteration 84/1000 | Loss: 0.00002793
Iteration 85/1000 | Loss: 0.00002793
Iteration 86/1000 | Loss: 0.00002793
Iteration 87/1000 | Loss: 0.00002793
Iteration 88/1000 | Loss: 0.00002793
Iteration 89/1000 | Loss: 0.00002792
Iteration 90/1000 | Loss: 0.00002792
Iteration 91/1000 | Loss: 0.00002792
Iteration 92/1000 | Loss: 0.00002792
Iteration 93/1000 | Loss: 0.00002792
Iteration 94/1000 | Loss: 0.00002792
Iteration 95/1000 | Loss: 0.00002792
Iteration 96/1000 | Loss: 0.00002792
Iteration 97/1000 | Loss: 0.00002792
Iteration 98/1000 | Loss: 0.00002792
Iteration 99/1000 | Loss: 0.00002792
Iteration 100/1000 | Loss: 0.00002792
Iteration 101/1000 | Loss: 0.00002792
Iteration 102/1000 | Loss: 0.00002792
Iteration 103/1000 | Loss: 0.00002792
Iteration 104/1000 | Loss: 0.00002791
Iteration 105/1000 | Loss: 0.00002791
Iteration 106/1000 | Loss: 0.00002791
Iteration 107/1000 | Loss: 0.00002791
Iteration 108/1000 | Loss: 0.00002791
Iteration 109/1000 | Loss: 0.00002791
Iteration 110/1000 | Loss: 0.00002791
Iteration 111/1000 | Loss: 0.00002791
Iteration 112/1000 | Loss: 0.00002791
Iteration 113/1000 | Loss: 0.00002791
Iteration 114/1000 | Loss: 0.00002791
Iteration 115/1000 | Loss: 0.00002791
Iteration 116/1000 | Loss: 0.00002790
Iteration 117/1000 | Loss: 0.00002790
Iteration 118/1000 | Loss: 0.00002790
Iteration 119/1000 | Loss: 0.00002790
Iteration 120/1000 | Loss: 0.00002790
Iteration 121/1000 | Loss: 0.00002790
Iteration 122/1000 | Loss: 0.00002790
Iteration 123/1000 | Loss: 0.00002790
Iteration 124/1000 | Loss: 0.00002790
Iteration 125/1000 | Loss: 0.00002790
Iteration 126/1000 | Loss: 0.00002790
Iteration 127/1000 | Loss: 0.00002790
Iteration 128/1000 | Loss: 0.00002790
Iteration 129/1000 | Loss: 0.00002790
Iteration 130/1000 | Loss: 0.00002790
Iteration 131/1000 | Loss: 0.00002790
Iteration 132/1000 | Loss: 0.00002789
Iteration 133/1000 | Loss: 0.00002789
Iteration 134/1000 | Loss: 0.00002789
Iteration 135/1000 | Loss: 0.00002789
Iteration 136/1000 | Loss: 0.00002789
Iteration 137/1000 | Loss: 0.00002789
Iteration 138/1000 | Loss: 0.00002789
Iteration 139/1000 | Loss: 0.00002789
Iteration 140/1000 | Loss: 0.00002789
Iteration 141/1000 | Loss: 0.00002789
Iteration 142/1000 | Loss: 0.00002789
Iteration 143/1000 | Loss: 0.00002789
Iteration 144/1000 | Loss: 0.00002789
Iteration 145/1000 | Loss: 0.00002789
Iteration 146/1000 | Loss: 0.00002789
Iteration 147/1000 | Loss: 0.00002789
Iteration 148/1000 | Loss: 0.00002789
Iteration 149/1000 | Loss: 0.00002789
Iteration 150/1000 | Loss: 0.00002789
Iteration 151/1000 | Loss: 0.00002789
Iteration 152/1000 | Loss: 0.00002789
Iteration 153/1000 | Loss: 0.00002789
Iteration 154/1000 | Loss: 0.00002789
Iteration 155/1000 | Loss: 0.00002789
Iteration 156/1000 | Loss: 0.00002789
Iteration 157/1000 | Loss: 0.00002789
Iteration 158/1000 | Loss: 0.00002789
Iteration 159/1000 | Loss: 0.00002789
Iteration 160/1000 | Loss: 0.00002789
Iteration 161/1000 | Loss: 0.00002789
Iteration 162/1000 | Loss: 0.00002789
Iteration 163/1000 | Loss: 0.00002789
Iteration 164/1000 | Loss: 0.00002789
Iteration 165/1000 | Loss: 0.00002788
Iteration 166/1000 | Loss: 0.00002788
Iteration 167/1000 | Loss: 0.00002788
Iteration 168/1000 | Loss: 0.00002788
Iteration 169/1000 | Loss: 0.00002788
Iteration 170/1000 | Loss: 0.00002788
Iteration 171/1000 | Loss: 0.00002788
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [2.788494930427987e-05, 2.788494930427987e-05, 2.788494930427987e-05, 2.788494930427987e-05, 2.788494930427987e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.788494930427987e-05

Optimization complete. Final v2v error: 4.581275463104248 mm

Highest mean error: 4.90966796875 mm for frame 74

Lowest mean error: 4.050581932067871 mm for frame 95

Saving results

Total time: 36.40845608711243
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_42_us_0617/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_42_us_0617/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_42_us_0617/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00495245
Iteration 2/25 | Loss: 0.00158866
Iteration 3/25 | Loss: 0.00149235
Iteration 4/25 | Loss: 0.00147137
Iteration 5/25 | Loss: 0.00146317
Iteration 6/25 | Loss: 0.00146102
Iteration 7/25 | Loss: 0.00146085
Iteration 8/25 | Loss: 0.00146085
Iteration 9/25 | Loss: 0.00146085
Iteration 10/25 | Loss: 0.00146085
Iteration 11/25 | Loss: 0.00146085
Iteration 12/25 | Loss: 0.00146085
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001460848143324256, 0.001460848143324256, 0.001460848143324256, 0.001460848143324256, 0.001460848143324256]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001460848143324256

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.70390129
Iteration 2/25 | Loss: 0.00145892
Iteration 3/25 | Loss: 0.00145890
Iteration 4/25 | Loss: 0.00145890
Iteration 5/25 | Loss: 0.00145890
Iteration 6/25 | Loss: 0.00145890
Iteration 7/25 | Loss: 0.00145890
Iteration 8/25 | Loss: 0.00145890
Iteration 9/25 | Loss: 0.00145890
Iteration 10/25 | Loss: 0.00145890
Iteration 11/25 | Loss: 0.00145890
Iteration 12/25 | Loss: 0.00145890
Iteration 13/25 | Loss: 0.00145890
Iteration 14/25 | Loss: 0.00145890
Iteration 15/25 | Loss: 0.00145890
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0014589008642360568, 0.0014589008642360568, 0.0014589008642360568, 0.0014589008642360568, 0.0014589008642360568]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014589008642360568

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00145890
Iteration 2/1000 | Loss: 0.00005527
Iteration 3/1000 | Loss: 0.00004001
Iteration 4/1000 | Loss: 0.00003566
Iteration 5/1000 | Loss: 0.00003369
Iteration 6/1000 | Loss: 0.00003249
Iteration 7/1000 | Loss: 0.00003171
Iteration 8/1000 | Loss: 0.00003104
Iteration 9/1000 | Loss: 0.00003052
Iteration 10/1000 | Loss: 0.00003021
Iteration 11/1000 | Loss: 0.00003002
Iteration 12/1000 | Loss: 0.00002988
Iteration 13/1000 | Loss: 0.00002987
Iteration 14/1000 | Loss: 0.00002986
Iteration 15/1000 | Loss: 0.00002986
Iteration 16/1000 | Loss: 0.00002983
Iteration 17/1000 | Loss: 0.00002975
Iteration 18/1000 | Loss: 0.00002972
Iteration 19/1000 | Loss: 0.00002972
Iteration 20/1000 | Loss: 0.00002971
Iteration 21/1000 | Loss: 0.00002971
Iteration 22/1000 | Loss: 0.00002970
Iteration 23/1000 | Loss: 0.00002966
Iteration 24/1000 | Loss: 0.00002966
Iteration 25/1000 | Loss: 0.00002966
Iteration 26/1000 | Loss: 0.00002966
Iteration 27/1000 | Loss: 0.00002964
Iteration 28/1000 | Loss: 0.00002964
Iteration 29/1000 | Loss: 0.00002964
Iteration 30/1000 | Loss: 0.00002964
Iteration 31/1000 | Loss: 0.00002963
Iteration 32/1000 | Loss: 0.00002963
Iteration 33/1000 | Loss: 0.00002962
Iteration 34/1000 | Loss: 0.00002962
Iteration 35/1000 | Loss: 0.00002961
Iteration 36/1000 | Loss: 0.00002961
Iteration 37/1000 | Loss: 0.00002961
Iteration 38/1000 | Loss: 0.00002960
Iteration 39/1000 | Loss: 0.00002959
Iteration 40/1000 | Loss: 0.00002959
Iteration 41/1000 | Loss: 0.00002959
Iteration 42/1000 | Loss: 0.00002959
Iteration 43/1000 | Loss: 0.00002958
Iteration 44/1000 | Loss: 0.00002958
Iteration 45/1000 | Loss: 0.00002957
Iteration 46/1000 | Loss: 0.00002957
Iteration 47/1000 | Loss: 0.00002957
Iteration 48/1000 | Loss: 0.00002957
Iteration 49/1000 | Loss: 0.00002956
Iteration 50/1000 | Loss: 0.00002956
Iteration 51/1000 | Loss: 0.00002956
Iteration 52/1000 | Loss: 0.00002955
Iteration 53/1000 | Loss: 0.00002954
Iteration 54/1000 | Loss: 0.00002954
Iteration 55/1000 | Loss: 0.00002954
Iteration 56/1000 | Loss: 0.00002953
Iteration 57/1000 | Loss: 0.00002953
Iteration 58/1000 | Loss: 0.00002953
Iteration 59/1000 | Loss: 0.00002952
Iteration 60/1000 | Loss: 0.00002952
Iteration 61/1000 | Loss: 0.00002952
Iteration 62/1000 | Loss: 0.00002951
Iteration 63/1000 | Loss: 0.00002951
Iteration 64/1000 | Loss: 0.00002950
Iteration 65/1000 | Loss: 0.00002950
Iteration 66/1000 | Loss: 0.00002950
Iteration 67/1000 | Loss: 0.00002950
Iteration 68/1000 | Loss: 0.00002950
Iteration 69/1000 | Loss: 0.00002949
Iteration 70/1000 | Loss: 0.00002949
Iteration 71/1000 | Loss: 0.00002949
Iteration 72/1000 | Loss: 0.00002948
Iteration 73/1000 | Loss: 0.00002948
Iteration 74/1000 | Loss: 0.00002948
Iteration 75/1000 | Loss: 0.00002947
Iteration 76/1000 | Loss: 0.00002947
Iteration 77/1000 | Loss: 0.00002947
Iteration 78/1000 | Loss: 0.00002947
Iteration 79/1000 | Loss: 0.00002946
Iteration 80/1000 | Loss: 0.00002946
Iteration 81/1000 | Loss: 0.00002946
Iteration 82/1000 | Loss: 0.00002946
Iteration 83/1000 | Loss: 0.00002946
Iteration 84/1000 | Loss: 0.00002946
Iteration 85/1000 | Loss: 0.00002946
Iteration 86/1000 | Loss: 0.00002945
Iteration 87/1000 | Loss: 0.00002945
Iteration 88/1000 | Loss: 0.00002945
Iteration 89/1000 | Loss: 0.00002945
Iteration 90/1000 | Loss: 0.00002945
Iteration 91/1000 | Loss: 0.00002945
Iteration 92/1000 | Loss: 0.00002944
Iteration 93/1000 | Loss: 0.00002944
Iteration 94/1000 | Loss: 0.00002944
Iteration 95/1000 | Loss: 0.00002944
Iteration 96/1000 | Loss: 0.00002944
Iteration 97/1000 | Loss: 0.00002944
Iteration 98/1000 | Loss: 0.00002944
Iteration 99/1000 | Loss: 0.00002944
Iteration 100/1000 | Loss: 0.00002944
Iteration 101/1000 | Loss: 0.00002944
Iteration 102/1000 | Loss: 0.00002944
Iteration 103/1000 | Loss: 0.00002944
Iteration 104/1000 | Loss: 0.00002944
Iteration 105/1000 | Loss: 0.00002944
Iteration 106/1000 | Loss: 0.00002944
Iteration 107/1000 | Loss: 0.00002944
Iteration 108/1000 | Loss: 0.00002944
Iteration 109/1000 | Loss: 0.00002944
Iteration 110/1000 | Loss: 0.00002944
Iteration 111/1000 | Loss: 0.00002944
Iteration 112/1000 | Loss: 0.00002944
Iteration 113/1000 | Loss: 0.00002944
Iteration 114/1000 | Loss: 0.00002944
Iteration 115/1000 | Loss: 0.00002944
Iteration 116/1000 | Loss: 0.00002944
Iteration 117/1000 | Loss: 0.00002944
Iteration 118/1000 | Loss: 0.00002944
Iteration 119/1000 | Loss: 0.00002944
Iteration 120/1000 | Loss: 0.00002944
Iteration 121/1000 | Loss: 0.00002944
Iteration 122/1000 | Loss: 0.00002944
Iteration 123/1000 | Loss: 0.00002944
Iteration 124/1000 | Loss: 0.00002944
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [2.9441604056046344e-05, 2.9441604056046344e-05, 2.9441604056046344e-05, 2.9441604056046344e-05, 2.9441604056046344e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9441604056046344e-05

Optimization complete. Final v2v error: 4.6864447593688965 mm

Highest mean error: 4.931117534637451 mm for frame 212

Lowest mean error: 4.277340412139893 mm for frame 85

Saving results

Total time: 39.94444990158081
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_42_us_0617/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_42_us_0617/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_42_us_0617/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00924113
Iteration 2/25 | Loss: 0.00151740
Iteration 3/25 | Loss: 0.00142248
Iteration 4/25 | Loss: 0.00141376
Iteration 5/25 | Loss: 0.00141113
Iteration 6/25 | Loss: 0.00141016
Iteration 7/25 | Loss: 0.00141016
Iteration 8/25 | Loss: 0.00141016
Iteration 9/25 | Loss: 0.00141016
Iteration 10/25 | Loss: 0.00141016
Iteration 11/25 | Loss: 0.00141016
Iteration 12/25 | Loss: 0.00141016
Iteration 13/25 | Loss: 0.00141016
Iteration 14/25 | Loss: 0.00141016
Iteration 15/25 | Loss: 0.00141016
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001410158583894372, 0.001410158583894372, 0.001410158583894372, 0.001410158583894372, 0.001410158583894372]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001410158583894372

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51120126
Iteration 2/25 | Loss: 0.00142816
Iteration 3/25 | Loss: 0.00142816
Iteration 4/25 | Loss: 0.00142816
Iteration 5/25 | Loss: 0.00142815
Iteration 6/25 | Loss: 0.00142815
Iteration 7/25 | Loss: 0.00142815
Iteration 8/25 | Loss: 0.00142815
Iteration 9/25 | Loss: 0.00142815
Iteration 10/25 | Loss: 0.00142815
Iteration 11/25 | Loss: 0.00142815
Iteration 12/25 | Loss: 0.00142815
Iteration 13/25 | Loss: 0.00142815
Iteration 14/25 | Loss: 0.00142815
Iteration 15/25 | Loss: 0.00142815
Iteration 16/25 | Loss: 0.00142815
Iteration 17/25 | Loss: 0.00142815
Iteration 18/25 | Loss: 0.00142815
Iteration 19/25 | Loss: 0.00142815
Iteration 20/25 | Loss: 0.00142815
Iteration 21/25 | Loss: 0.00142815
Iteration 22/25 | Loss: 0.00142815
Iteration 23/25 | Loss: 0.00142815
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0014281542971730232, 0.0014281542971730232, 0.0014281542971730232, 0.0014281542971730232, 0.0014281542971730232]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014281542971730232

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00142815
Iteration 2/1000 | Loss: 0.00003854
Iteration 3/1000 | Loss: 0.00002829
Iteration 4/1000 | Loss: 0.00002500
Iteration 5/1000 | Loss: 0.00002339
Iteration 6/1000 | Loss: 0.00002263
Iteration 7/1000 | Loss: 0.00002192
Iteration 8/1000 | Loss: 0.00002159
Iteration 9/1000 | Loss: 0.00002128
Iteration 10/1000 | Loss: 0.00002100
Iteration 11/1000 | Loss: 0.00002080
Iteration 12/1000 | Loss: 0.00002063
Iteration 13/1000 | Loss: 0.00002062
Iteration 14/1000 | Loss: 0.00002052
Iteration 15/1000 | Loss: 0.00002052
Iteration 16/1000 | Loss: 0.00002051
Iteration 17/1000 | Loss: 0.00002050
Iteration 18/1000 | Loss: 0.00002050
Iteration 19/1000 | Loss: 0.00002048
Iteration 20/1000 | Loss: 0.00002048
Iteration 21/1000 | Loss: 0.00002047
Iteration 22/1000 | Loss: 0.00002047
Iteration 23/1000 | Loss: 0.00002047
Iteration 24/1000 | Loss: 0.00002047
Iteration 25/1000 | Loss: 0.00002047
Iteration 26/1000 | Loss: 0.00002047
Iteration 27/1000 | Loss: 0.00002047
Iteration 28/1000 | Loss: 0.00002047
Iteration 29/1000 | Loss: 0.00002047
Iteration 30/1000 | Loss: 0.00002046
Iteration 31/1000 | Loss: 0.00002045
Iteration 32/1000 | Loss: 0.00002045
Iteration 33/1000 | Loss: 0.00002045
Iteration 34/1000 | Loss: 0.00002044
Iteration 35/1000 | Loss: 0.00002044
Iteration 36/1000 | Loss: 0.00002043
Iteration 37/1000 | Loss: 0.00002043
Iteration 38/1000 | Loss: 0.00002043
Iteration 39/1000 | Loss: 0.00002043
Iteration 40/1000 | Loss: 0.00002043
Iteration 41/1000 | Loss: 0.00002043
Iteration 42/1000 | Loss: 0.00002043
Iteration 43/1000 | Loss: 0.00002042
Iteration 44/1000 | Loss: 0.00002042
Iteration 45/1000 | Loss: 0.00002042
Iteration 46/1000 | Loss: 0.00002041
Iteration 47/1000 | Loss: 0.00002041
Iteration 48/1000 | Loss: 0.00002041
Iteration 49/1000 | Loss: 0.00002041
Iteration 50/1000 | Loss: 0.00002041
Iteration 51/1000 | Loss: 0.00002040
Iteration 52/1000 | Loss: 0.00002040
Iteration 53/1000 | Loss: 0.00002040
Iteration 54/1000 | Loss: 0.00002040
Iteration 55/1000 | Loss: 0.00002039
Iteration 56/1000 | Loss: 0.00002039
Iteration 57/1000 | Loss: 0.00002039
Iteration 58/1000 | Loss: 0.00002039
Iteration 59/1000 | Loss: 0.00002039
Iteration 60/1000 | Loss: 0.00002038
Iteration 61/1000 | Loss: 0.00002038
Iteration 62/1000 | Loss: 0.00002038
Iteration 63/1000 | Loss: 0.00002038
Iteration 64/1000 | Loss: 0.00002038
Iteration 65/1000 | Loss: 0.00002038
Iteration 66/1000 | Loss: 0.00002038
Iteration 67/1000 | Loss: 0.00002038
Iteration 68/1000 | Loss: 0.00002038
Iteration 69/1000 | Loss: 0.00002037
Iteration 70/1000 | Loss: 0.00002037
Iteration 71/1000 | Loss: 0.00002037
Iteration 72/1000 | Loss: 0.00002036
Iteration 73/1000 | Loss: 0.00002036
Iteration 74/1000 | Loss: 0.00002036
Iteration 75/1000 | Loss: 0.00002036
Iteration 76/1000 | Loss: 0.00002036
Iteration 77/1000 | Loss: 0.00002036
Iteration 78/1000 | Loss: 0.00002036
Iteration 79/1000 | Loss: 0.00002035
Iteration 80/1000 | Loss: 0.00002035
Iteration 81/1000 | Loss: 0.00002035
Iteration 82/1000 | Loss: 0.00002035
Iteration 83/1000 | Loss: 0.00002035
Iteration 84/1000 | Loss: 0.00002035
Iteration 85/1000 | Loss: 0.00002035
Iteration 86/1000 | Loss: 0.00002035
Iteration 87/1000 | Loss: 0.00002035
Iteration 88/1000 | Loss: 0.00002034
Iteration 89/1000 | Loss: 0.00002034
Iteration 90/1000 | Loss: 0.00002034
Iteration 91/1000 | Loss: 0.00002034
Iteration 92/1000 | Loss: 0.00002034
Iteration 93/1000 | Loss: 0.00002034
Iteration 94/1000 | Loss: 0.00002034
Iteration 95/1000 | Loss: 0.00002034
Iteration 96/1000 | Loss: 0.00002034
Iteration 97/1000 | Loss: 0.00002033
Iteration 98/1000 | Loss: 0.00002033
Iteration 99/1000 | Loss: 0.00002033
Iteration 100/1000 | Loss: 0.00002033
Iteration 101/1000 | Loss: 0.00002033
Iteration 102/1000 | Loss: 0.00002033
Iteration 103/1000 | Loss: 0.00002033
Iteration 104/1000 | Loss: 0.00002033
Iteration 105/1000 | Loss: 0.00002033
Iteration 106/1000 | Loss: 0.00002033
Iteration 107/1000 | Loss: 0.00002033
Iteration 108/1000 | Loss: 0.00002033
Iteration 109/1000 | Loss: 0.00002033
Iteration 110/1000 | Loss: 0.00002033
Iteration 111/1000 | Loss: 0.00002033
Iteration 112/1000 | Loss: 0.00002033
Iteration 113/1000 | Loss: 0.00002033
Iteration 114/1000 | Loss: 0.00002033
Iteration 115/1000 | Loss: 0.00002033
Iteration 116/1000 | Loss: 0.00002033
Iteration 117/1000 | Loss: 0.00002033
Iteration 118/1000 | Loss: 0.00002033
Iteration 119/1000 | Loss: 0.00002033
Iteration 120/1000 | Loss: 0.00002033
Iteration 121/1000 | Loss: 0.00002032
Iteration 122/1000 | Loss: 0.00002032
Iteration 123/1000 | Loss: 0.00002032
Iteration 124/1000 | Loss: 0.00002032
Iteration 125/1000 | Loss: 0.00002032
Iteration 126/1000 | Loss: 0.00002032
Iteration 127/1000 | Loss: 0.00002032
Iteration 128/1000 | Loss: 0.00002032
Iteration 129/1000 | Loss: 0.00002032
Iteration 130/1000 | Loss: 0.00002032
Iteration 131/1000 | Loss: 0.00002032
Iteration 132/1000 | Loss: 0.00002032
Iteration 133/1000 | Loss: 0.00002032
Iteration 134/1000 | Loss: 0.00002032
Iteration 135/1000 | Loss: 0.00002032
Iteration 136/1000 | Loss: 0.00002032
Iteration 137/1000 | Loss: 0.00002032
Iteration 138/1000 | Loss: 0.00002032
Iteration 139/1000 | Loss: 0.00002032
Iteration 140/1000 | Loss: 0.00002032
Iteration 141/1000 | Loss: 0.00002031
Iteration 142/1000 | Loss: 0.00002031
Iteration 143/1000 | Loss: 0.00002031
Iteration 144/1000 | Loss: 0.00002031
Iteration 145/1000 | Loss: 0.00002031
Iteration 146/1000 | Loss: 0.00002031
Iteration 147/1000 | Loss: 0.00002031
Iteration 148/1000 | Loss: 0.00002031
Iteration 149/1000 | Loss: 0.00002031
Iteration 150/1000 | Loss: 0.00002031
Iteration 151/1000 | Loss: 0.00002030
Iteration 152/1000 | Loss: 0.00002030
Iteration 153/1000 | Loss: 0.00002030
Iteration 154/1000 | Loss: 0.00002030
Iteration 155/1000 | Loss: 0.00002030
Iteration 156/1000 | Loss: 0.00002030
Iteration 157/1000 | Loss: 0.00002030
Iteration 158/1000 | Loss: 0.00002030
Iteration 159/1000 | Loss: 0.00002030
Iteration 160/1000 | Loss: 0.00002030
Iteration 161/1000 | Loss: 0.00002029
Iteration 162/1000 | Loss: 0.00002029
Iteration 163/1000 | Loss: 0.00002029
Iteration 164/1000 | Loss: 0.00002029
Iteration 165/1000 | Loss: 0.00002029
Iteration 166/1000 | Loss: 0.00002029
Iteration 167/1000 | Loss: 0.00002029
Iteration 168/1000 | Loss: 0.00002029
Iteration 169/1000 | Loss: 0.00002029
Iteration 170/1000 | Loss: 0.00002029
Iteration 171/1000 | Loss: 0.00002029
Iteration 172/1000 | Loss: 0.00002029
Iteration 173/1000 | Loss: 0.00002029
Iteration 174/1000 | Loss: 0.00002029
Iteration 175/1000 | Loss: 0.00002029
Iteration 176/1000 | Loss: 0.00002029
Iteration 177/1000 | Loss: 0.00002029
Iteration 178/1000 | Loss: 0.00002029
Iteration 179/1000 | Loss: 0.00002029
Iteration 180/1000 | Loss: 0.00002029
Iteration 181/1000 | Loss: 0.00002029
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [2.0292480257921852e-05, 2.0292480257921852e-05, 2.0292480257921852e-05, 2.0292480257921852e-05, 2.0292480257921852e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0292480257921852e-05

Optimization complete. Final v2v error: 3.898925542831421 mm

Highest mean error: 4.2175140380859375 mm for frame 27

Lowest mean error: 3.5775246620178223 mm for frame 114

Saving results

Total time: 37.36459136009216
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_42_us_0617/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_42_us_0617/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_42_us_0617/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01181098
Iteration 2/25 | Loss: 0.01181098
Iteration 3/25 | Loss: 0.00255729
Iteration 4/25 | Loss: 0.00202719
Iteration 5/25 | Loss: 0.00178590
Iteration 6/25 | Loss: 0.00159576
Iteration 7/25 | Loss: 0.00155323
Iteration 8/25 | Loss: 0.00150724
Iteration 9/25 | Loss: 0.00151631
Iteration 10/25 | Loss: 0.00144514
Iteration 11/25 | Loss: 0.00140552
Iteration 12/25 | Loss: 0.00134417
Iteration 13/25 | Loss: 0.00133524
Iteration 14/25 | Loss: 0.00133250
Iteration 15/25 | Loss: 0.00132864
Iteration 16/25 | Loss: 0.00132788
Iteration 17/25 | Loss: 0.00132690
Iteration 18/25 | Loss: 0.00132653
Iteration 19/25 | Loss: 0.00132856
Iteration 20/25 | Loss: 0.00132587
Iteration 21/25 | Loss: 0.00132287
Iteration 22/25 | Loss: 0.00132222
Iteration 23/25 | Loss: 0.00132196
Iteration 24/25 | Loss: 0.00132191
Iteration 25/25 | Loss: 0.00132191

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.68453407
Iteration 2/25 | Loss: 0.00154396
Iteration 3/25 | Loss: 0.00133405
Iteration 4/25 | Loss: 0.00133405
Iteration 5/25 | Loss: 0.00133405
Iteration 6/25 | Loss: 0.00133405
Iteration 7/25 | Loss: 0.00133404
Iteration 8/25 | Loss: 0.00133404
Iteration 9/25 | Loss: 0.00133404
Iteration 10/25 | Loss: 0.00133404
Iteration 11/25 | Loss: 0.00133404
Iteration 12/25 | Loss: 0.00133404
Iteration 13/25 | Loss: 0.00133404
Iteration 14/25 | Loss: 0.00133404
Iteration 15/25 | Loss: 0.00133404
Iteration 16/25 | Loss: 0.00133404
Iteration 17/25 | Loss: 0.00133404
Iteration 18/25 | Loss: 0.00133404
Iteration 19/25 | Loss: 0.00133404
Iteration 20/25 | Loss: 0.00133404
Iteration 21/25 | Loss: 0.00133404
Iteration 22/25 | Loss: 0.00133404
Iteration 23/25 | Loss: 0.00133404
Iteration 24/25 | Loss: 0.00133404
Iteration 25/25 | Loss: 0.00133404

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133404
Iteration 2/1000 | Loss: 0.00007029
Iteration 3/1000 | Loss: 0.00010798
Iteration 4/1000 | Loss: 0.00003597
Iteration 5/1000 | Loss: 0.00012857
Iteration 6/1000 | Loss: 0.00004414
Iteration 7/1000 | Loss: 0.00003664
Iteration 8/1000 | Loss: 0.00003254
Iteration 9/1000 | Loss: 0.00003184
Iteration 10/1000 | Loss: 0.00003140
Iteration 11/1000 | Loss: 0.00042199
Iteration 12/1000 | Loss: 0.00030331
Iteration 13/1000 | Loss: 0.00010892
Iteration 14/1000 | Loss: 0.00029103
Iteration 15/1000 | Loss: 0.00003244
Iteration 16/1000 | Loss: 0.00008946
Iteration 17/1000 | Loss: 0.00003874
Iteration 18/1000 | Loss: 0.00002972
Iteration 19/1000 | Loss: 0.00002917
Iteration 20/1000 | Loss: 0.00002888
Iteration 21/1000 | Loss: 0.00007925
Iteration 22/1000 | Loss: 0.00015479
Iteration 23/1000 | Loss: 0.00003150
Iteration 24/1000 | Loss: 0.00003116
Iteration 25/1000 | Loss: 0.00002846
Iteration 26/1000 | Loss: 0.00002846
Iteration 27/1000 | Loss: 0.00004218
Iteration 28/1000 | Loss: 0.00006313
Iteration 29/1000 | Loss: 0.00002844
Iteration 30/1000 | Loss: 0.00008042
Iteration 31/1000 | Loss: 0.00004706
Iteration 32/1000 | Loss: 0.00002828
Iteration 33/1000 | Loss: 0.00002826
Iteration 34/1000 | Loss: 0.00002826
Iteration 35/1000 | Loss: 0.00002825
Iteration 36/1000 | Loss: 0.00002825
Iteration 37/1000 | Loss: 0.00002824
Iteration 38/1000 | Loss: 0.00002824
Iteration 39/1000 | Loss: 0.00002824
Iteration 40/1000 | Loss: 0.00002823
Iteration 41/1000 | Loss: 0.00002821
Iteration 42/1000 | Loss: 0.00006837
Iteration 43/1000 | Loss: 0.00003617
Iteration 44/1000 | Loss: 0.00002833
Iteration 45/1000 | Loss: 0.00002821
Iteration 46/1000 | Loss: 0.00002821
Iteration 47/1000 | Loss: 0.00002829
Iteration 48/1000 | Loss: 0.00002829
Iteration 49/1000 | Loss: 0.00002828
Iteration 50/1000 | Loss: 0.00002828
Iteration 51/1000 | Loss: 0.00002828
Iteration 52/1000 | Loss: 0.00004805
Iteration 53/1000 | Loss: 0.00002820
Iteration 54/1000 | Loss: 0.00002819
Iteration 55/1000 | Loss: 0.00002819
Iteration 56/1000 | Loss: 0.00002816
Iteration 57/1000 | Loss: 0.00002816
Iteration 58/1000 | Loss: 0.00002815
Iteration 59/1000 | Loss: 0.00002814
Iteration 60/1000 | Loss: 0.00002814
Iteration 61/1000 | Loss: 0.00002813
Iteration 62/1000 | Loss: 0.00002813
Iteration 63/1000 | Loss: 0.00002812
Iteration 64/1000 | Loss: 0.00002812
Iteration 65/1000 | Loss: 0.00002812
Iteration 66/1000 | Loss: 0.00002812
Iteration 67/1000 | Loss: 0.00002812
Iteration 68/1000 | Loss: 0.00002812
Iteration 69/1000 | Loss: 0.00002811
Iteration 70/1000 | Loss: 0.00002811
Iteration 71/1000 | Loss: 0.00002811
Iteration 72/1000 | Loss: 0.00002811
Iteration 73/1000 | Loss: 0.00002811
Iteration 74/1000 | Loss: 0.00002811
Iteration 75/1000 | Loss: 0.00002810
Iteration 76/1000 | Loss: 0.00002810
Iteration 77/1000 | Loss: 0.00002810
Iteration 78/1000 | Loss: 0.00002810
Iteration 79/1000 | Loss: 0.00004326
Iteration 80/1000 | Loss: 0.00002969
Iteration 81/1000 | Loss: 0.00002815
Iteration 82/1000 | Loss: 0.00002811
Iteration 83/1000 | Loss: 0.00002811
Iteration 84/1000 | Loss: 0.00002810
Iteration 85/1000 | Loss: 0.00002810
Iteration 86/1000 | Loss: 0.00002807
Iteration 87/1000 | Loss: 0.00002807
Iteration 88/1000 | Loss: 0.00002810
Iteration 89/1000 | Loss: 0.00002810
Iteration 90/1000 | Loss: 0.00002810
Iteration 91/1000 | Loss: 0.00002805
Iteration 92/1000 | Loss: 0.00002805
Iteration 93/1000 | Loss: 0.00002805
Iteration 94/1000 | Loss: 0.00002805
Iteration 95/1000 | Loss: 0.00002805
Iteration 96/1000 | Loss: 0.00002808
Iteration 97/1000 | Loss: 0.00002807
Iteration 98/1000 | Loss: 0.00002807
Iteration 99/1000 | Loss: 0.00002807
Iteration 100/1000 | Loss: 0.00003071
Iteration 101/1000 | Loss: 0.00010454
Iteration 102/1000 | Loss: 0.00003047
Iteration 103/1000 | Loss: 0.00008900
Iteration 104/1000 | Loss: 0.00002906
Iteration 105/1000 | Loss: 0.00002812
Iteration 106/1000 | Loss: 0.00002803
Iteration 107/1000 | Loss: 0.00002803
Iteration 108/1000 | Loss: 0.00002803
Iteration 109/1000 | Loss: 0.00002803
Iteration 110/1000 | Loss: 0.00002803
Iteration 111/1000 | Loss: 0.00002803
Iteration 112/1000 | Loss: 0.00002803
Iteration 113/1000 | Loss: 0.00002803
Iteration 114/1000 | Loss: 0.00002803
Iteration 115/1000 | Loss: 0.00002803
Iteration 116/1000 | Loss: 0.00002803
Iteration 117/1000 | Loss: 0.00002803
Iteration 118/1000 | Loss: 0.00002803
Iteration 119/1000 | Loss: 0.00008170
Iteration 120/1000 | Loss: 0.00002804
Iteration 121/1000 | Loss: 0.00002803
Iteration 122/1000 | Loss: 0.00002803
Iteration 123/1000 | Loss: 0.00002803
Iteration 124/1000 | Loss: 0.00002801
Iteration 125/1000 | Loss: 0.00002801
Iteration 126/1000 | Loss: 0.00005021
Iteration 127/1000 | Loss: 0.00002806
Iteration 128/1000 | Loss: 0.00002801
Iteration 129/1000 | Loss: 0.00002801
Iteration 130/1000 | Loss: 0.00002801
Iteration 131/1000 | Loss: 0.00002801
Iteration 132/1000 | Loss: 0.00002801
Iteration 133/1000 | Loss: 0.00002801
Iteration 134/1000 | Loss: 0.00002801
Iteration 135/1000 | Loss: 0.00002801
Iteration 136/1000 | Loss: 0.00002801
Iteration 137/1000 | Loss: 0.00002800
Iteration 138/1000 | Loss: 0.00002800
Iteration 139/1000 | Loss: 0.00002800
Iteration 140/1000 | Loss: 0.00002800
Iteration 141/1000 | Loss: 0.00002800
Iteration 142/1000 | Loss: 0.00002800
Iteration 143/1000 | Loss: 0.00002799
Iteration 144/1000 | Loss: 0.00002799
Iteration 145/1000 | Loss: 0.00002799
Iteration 146/1000 | Loss: 0.00002798
Iteration 147/1000 | Loss: 0.00002798
Iteration 148/1000 | Loss: 0.00002798
Iteration 149/1000 | Loss: 0.00002798
Iteration 150/1000 | Loss: 0.00002798
Iteration 151/1000 | Loss: 0.00002798
Iteration 152/1000 | Loss: 0.00002798
Iteration 153/1000 | Loss: 0.00002798
Iteration 154/1000 | Loss: 0.00002798
Iteration 155/1000 | Loss: 0.00002798
Iteration 156/1000 | Loss: 0.00002798
Iteration 157/1000 | Loss: 0.00002798
Iteration 158/1000 | Loss: 0.00002798
Iteration 159/1000 | Loss: 0.00002798
Iteration 160/1000 | Loss: 0.00002798
Iteration 161/1000 | Loss: 0.00002798
Iteration 162/1000 | Loss: 0.00002798
Iteration 163/1000 | Loss: 0.00002798
Iteration 164/1000 | Loss: 0.00002798
Iteration 165/1000 | Loss: 0.00002797
Iteration 166/1000 | Loss: 0.00002797
Iteration 167/1000 | Loss: 0.00002797
Iteration 168/1000 | Loss: 0.00002797
Iteration 169/1000 | Loss: 0.00002797
Iteration 170/1000 | Loss: 0.00002797
Iteration 171/1000 | Loss: 0.00002797
Iteration 172/1000 | Loss: 0.00002797
Iteration 173/1000 | Loss: 0.00002797
Iteration 174/1000 | Loss: 0.00002797
Iteration 175/1000 | Loss: 0.00002797
Iteration 176/1000 | Loss: 0.00002797
Iteration 177/1000 | Loss: 0.00002797
Iteration 178/1000 | Loss: 0.00002797
Iteration 179/1000 | Loss: 0.00002797
Iteration 180/1000 | Loss: 0.00002797
Iteration 181/1000 | Loss: 0.00002797
Iteration 182/1000 | Loss: 0.00002797
Iteration 183/1000 | Loss: 0.00002797
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [2.7972429961664602e-05, 2.7972429961664602e-05, 2.7972429961664602e-05, 2.7972429961664602e-05, 2.7972429961664602e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7972429961664602e-05

Optimization complete. Final v2v error: 4.444031238555908 mm

Highest mean error: 11.253522872924805 mm for frame 87

Lowest mean error: 3.9080419540405273 mm for frame 216

Saving results

Total time: 130.06130027770996
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_42_us_0617/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_42_us_0617/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_42_us_0617/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01164633
Iteration 2/25 | Loss: 0.00287984
Iteration 3/25 | Loss: 0.00217683
Iteration 4/25 | Loss: 0.00205003
Iteration 5/25 | Loss: 0.00182640
Iteration 6/25 | Loss: 0.00178912
Iteration 7/25 | Loss: 0.00166343
Iteration 8/25 | Loss: 0.00165325
Iteration 9/25 | Loss: 0.00162932
Iteration 10/25 | Loss: 0.00161834
Iteration 11/25 | Loss: 0.00157207
Iteration 12/25 | Loss: 0.00155328
Iteration 13/25 | Loss: 0.00155270
Iteration 14/25 | Loss: 0.00155272
Iteration 15/25 | Loss: 0.00155098
Iteration 16/25 | Loss: 0.00154837
Iteration 17/25 | Loss: 0.00155238
Iteration 18/25 | Loss: 0.00154120
Iteration 19/25 | Loss: 0.00154011
Iteration 20/25 | Loss: 0.00153983
Iteration 21/25 | Loss: 0.00153952
Iteration 22/25 | Loss: 0.00153944
Iteration 23/25 | Loss: 0.00153944
Iteration 24/25 | Loss: 0.00153943
Iteration 25/25 | Loss: 0.00153943

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48849964
Iteration 2/25 | Loss: 0.00333561
Iteration 3/25 | Loss: 0.00313210
Iteration 4/25 | Loss: 0.00313210
Iteration 5/25 | Loss: 0.00313210
Iteration 6/25 | Loss: 0.00313210
Iteration 7/25 | Loss: 0.00313209
Iteration 8/25 | Loss: 0.00313209
Iteration 9/25 | Loss: 0.00313209
Iteration 10/25 | Loss: 0.00313209
Iteration 11/25 | Loss: 0.00313209
Iteration 12/25 | Loss: 0.00313209
Iteration 13/25 | Loss: 0.00313209
Iteration 14/25 | Loss: 0.00313209
Iteration 15/25 | Loss: 0.00313209
Iteration 16/25 | Loss: 0.00313209
Iteration 17/25 | Loss: 0.00313209
Iteration 18/25 | Loss: 0.00313209
Iteration 19/25 | Loss: 0.00313209
Iteration 20/25 | Loss: 0.00313209
Iteration 21/25 | Loss: 0.00313209
Iteration 22/25 | Loss: 0.00313209
Iteration 23/25 | Loss: 0.00313209
Iteration 24/25 | Loss: 0.00313209
Iteration 25/25 | Loss: 0.00313209

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00313209
Iteration 2/1000 | Loss: 0.00040852
Iteration 3/1000 | Loss: 0.00025348
Iteration 4/1000 | Loss: 0.00052840
Iteration 5/1000 | Loss: 0.00046960
Iteration 6/1000 | Loss: 0.00060048
Iteration 7/1000 | Loss: 0.00067128
Iteration 8/1000 | Loss: 0.00048240
Iteration 9/1000 | Loss: 0.00079823
Iteration 10/1000 | Loss: 0.00019316
Iteration 11/1000 | Loss: 0.00030827
Iteration 12/1000 | Loss: 0.00023786
Iteration 13/1000 | Loss: 0.00089142
Iteration 14/1000 | Loss: 0.00013107
Iteration 15/1000 | Loss: 0.00011319
Iteration 16/1000 | Loss: 0.00039482
Iteration 17/1000 | Loss: 0.00013301
Iteration 18/1000 | Loss: 0.00009218
Iteration 19/1000 | Loss: 0.00071018
Iteration 20/1000 | Loss: 0.00010086
Iteration 21/1000 | Loss: 0.00011952
Iteration 22/1000 | Loss: 0.00016370
Iteration 23/1000 | Loss: 0.00008437
Iteration 24/1000 | Loss: 0.00008079
Iteration 25/1000 | Loss: 0.00007876
Iteration 26/1000 | Loss: 0.00007742
Iteration 27/1000 | Loss: 0.00014008
Iteration 28/1000 | Loss: 0.00034956
Iteration 29/1000 | Loss: 0.00029285
Iteration 30/1000 | Loss: 0.00018086
Iteration 31/1000 | Loss: 0.00012590
Iteration 32/1000 | Loss: 0.00010424
Iteration 33/1000 | Loss: 0.00015142
Iteration 34/1000 | Loss: 0.00007289
Iteration 35/1000 | Loss: 0.00027963
Iteration 36/1000 | Loss: 0.00015306
Iteration 37/1000 | Loss: 0.00082644
Iteration 38/1000 | Loss: 0.00006834
Iteration 39/1000 | Loss: 0.00034824
Iteration 40/1000 | Loss: 0.00006161
Iteration 41/1000 | Loss: 0.00006057
Iteration 42/1000 | Loss: 0.00005976
Iteration 43/1000 | Loss: 0.00005922
Iteration 44/1000 | Loss: 0.00005899
Iteration 45/1000 | Loss: 0.00005877
Iteration 46/1000 | Loss: 0.00005872
Iteration 47/1000 | Loss: 0.00005869
Iteration 48/1000 | Loss: 0.00005869
Iteration 49/1000 | Loss: 0.00005869
Iteration 50/1000 | Loss: 0.00005868
Iteration 51/1000 | Loss: 0.00005868
Iteration 52/1000 | Loss: 0.00005868
Iteration 53/1000 | Loss: 0.00005866
Iteration 54/1000 | Loss: 0.00005866
Iteration 55/1000 | Loss: 0.00005866
Iteration 56/1000 | Loss: 0.00005866
Iteration 57/1000 | Loss: 0.00005866
Iteration 58/1000 | Loss: 0.00005866
Iteration 59/1000 | Loss: 0.00005866
Iteration 60/1000 | Loss: 0.00005866
Iteration 61/1000 | Loss: 0.00005865
Iteration 62/1000 | Loss: 0.00005865
Iteration 63/1000 | Loss: 0.00005864
Iteration 64/1000 | Loss: 0.00005864
Iteration 65/1000 | Loss: 0.00005864
Iteration 66/1000 | Loss: 0.00005864
Iteration 67/1000 | Loss: 0.00005863
Iteration 68/1000 | Loss: 0.00005863
Iteration 69/1000 | Loss: 0.00005863
Iteration 70/1000 | Loss: 0.00005863
Iteration 71/1000 | Loss: 0.00005863
Iteration 72/1000 | Loss: 0.00005863
Iteration 73/1000 | Loss: 0.00005863
Iteration 74/1000 | Loss: 0.00005863
Iteration 75/1000 | Loss: 0.00005863
Iteration 76/1000 | Loss: 0.00005863
Iteration 77/1000 | Loss: 0.00005863
Iteration 78/1000 | Loss: 0.00005863
Iteration 79/1000 | Loss: 0.00005862
Iteration 80/1000 | Loss: 0.00005862
Iteration 81/1000 | Loss: 0.00005862
Iteration 82/1000 | Loss: 0.00005862
Iteration 83/1000 | Loss: 0.00005862
Iteration 84/1000 | Loss: 0.00005862
Iteration 85/1000 | Loss: 0.00005861
Iteration 86/1000 | Loss: 0.00005861
Iteration 87/1000 | Loss: 0.00005861
Iteration 88/1000 | Loss: 0.00005861
Iteration 89/1000 | Loss: 0.00005860
Iteration 90/1000 | Loss: 0.00005860
Iteration 91/1000 | Loss: 0.00005860
Iteration 92/1000 | Loss: 0.00005860
Iteration 93/1000 | Loss: 0.00005860
Iteration 94/1000 | Loss: 0.00005860
Iteration 95/1000 | Loss: 0.00005860
Iteration 96/1000 | Loss: 0.00005860
Iteration 97/1000 | Loss: 0.00005859
Iteration 98/1000 | Loss: 0.00005859
Iteration 99/1000 | Loss: 0.00005859
Iteration 100/1000 | Loss: 0.00005859
Iteration 101/1000 | Loss: 0.00005859
Iteration 102/1000 | Loss: 0.00005859
Iteration 103/1000 | Loss: 0.00005859
Iteration 104/1000 | Loss: 0.00005859
Iteration 105/1000 | Loss: 0.00005859
Iteration 106/1000 | Loss: 0.00005859
Iteration 107/1000 | Loss: 0.00005859
Iteration 108/1000 | Loss: 0.00005859
Iteration 109/1000 | Loss: 0.00005858
Iteration 110/1000 | Loss: 0.00005858
Iteration 111/1000 | Loss: 0.00005858
Iteration 112/1000 | Loss: 0.00005858
Iteration 113/1000 | Loss: 0.00005858
Iteration 114/1000 | Loss: 0.00005858
Iteration 115/1000 | Loss: 0.00005858
Iteration 116/1000 | Loss: 0.00005858
Iteration 117/1000 | Loss: 0.00005858
Iteration 118/1000 | Loss: 0.00005858
Iteration 119/1000 | Loss: 0.00005858
Iteration 120/1000 | Loss: 0.00005858
Iteration 121/1000 | Loss: 0.00005858
Iteration 122/1000 | Loss: 0.00005858
Iteration 123/1000 | Loss: 0.00005858
Iteration 124/1000 | Loss: 0.00005858
Iteration 125/1000 | Loss: 0.00005858
Iteration 126/1000 | Loss: 0.00005858
Iteration 127/1000 | Loss: 0.00005858
Iteration 128/1000 | Loss: 0.00005858
Iteration 129/1000 | Loss: 0.00005858
Iteration 130/1000 | Loss: 0.00005857
Iteration 131/1000 | Loss: 0.00005857
Iteration 132/1000 | Loss: 0.00005857
Iteration 133/1000 | Loss: 0.00005857
Iteration 134/1000 | Loss: 0.00005857
Iteration 135/1000 | Loss: 0.00005857
Iteration 136/1000 | Loss: 0.00005857
Iteration 137/1000 | Loss: 0.00005857
Iteration 138/1000 | Loss: 0.00005856
Iteration 139/1000 | Loss: 0.00005856
Iteration 140/1000 | Loss: 0.00005856
Iteration 141/1000 | Loss: 0.00005856
Iteration 142/1000 | Loss: 0.00005856
Iteration 143/1000 | Loss: 0.00005856
Iteration 144/1000 | Loss: 0.00005856
Iteration 145/1000 | Loss: 0.00005856
Iteration 146/1000 | Loss: 0.00005856
Iteration 147/1000 | Loss: 0.00005855
Iteration 148/1000 | Loss: 0.00005855
Iteration 149/1000 | Loss: 0.00005855
Iteration 150/1000 | Loss: 0.00005855
Iteration 151/1000 | Loss: 0.00005855
Iteration 152/1000 | Loss: 0.00005855
Iteration 153/1000 | Loss: 0.00005855
Iteration 154/1000 | Loss: 0.00005855
Iteration 155/1000 | Loss: 0.00005855
Iteration 156/1000 | Loss: 0.00005855
Iteration 157/1000 | Loss: 0.00005855
Iteration 158/1000 | Loss: 0.00005855
Iteration 159/1000 | Loss: 0.00005855
Iteration 160/1000 | Loss: 0.00005855
Iteration 161/1000 | Loss: 0.00005855
Iteration 162/1000 | Loss: 0.00005855
Iteration 163/1000 | Loss: 0.00005855
Iteration 164/1000 | Loss: 0.00005855
Iteration 165/1000 | Loss: 0.00005855
Iteration 166/1000 | Loss: 0.00005855
Iteration 167/1000 | Loss: 0.00005855
Iteration 168/1000 | Loss: 0.00005855
Iteration 169/1000 | Loss: 0.00005855
Iteration 170/1000 | Loss: 0.00005855
Iteration 171/1000 | Loss: 0.00005855
Iteration 172/1000 | Loss: 0.00005855
Iteration 173/1000 | Loss: 0.00005855
Iteration 174/1000 | Loss: 0.00005855
Iteration 175/1000 | Loss: 0.00005855
Iteration 176/1000 | Loss: 0.00005855
Iteration 177/1000 | Loss: 0.00005855
Iteration 178/1000 | Loss: 0.00005855
Iteration 179/1000 | Loss: 0.00005855
Iteration 180/1000 | Loss: 0.00005855
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [5.855489507666789e-05, 5.855489507666789e-05, 5.855489507666789e-05, 5.855489507666789e-05, 5.855489507666789e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.855489507666789e-05

Optimization complete. Final v2v error: 5.108338832855225 mm

Highest mean error: 14.46127700805664 mm for frame 30

Lowest mean error: 4.358311653137207 mm for frame 67

Saving results

Total time: 112.31471180915833
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_42_us_0617/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_42_us_0617/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_42_us_0617/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00966111
Iteration 2/25 | Loss: 0.00192212
Iteration 3/25 | Loss: 0.00168531
Iteration 4/25 | Loss: 0.00163929
Iteration 5/25 | Loss: 0.00162779
Iteration 6/25 | Loss: 0.00162650
Iteration 7/25 | Loss: 0.00162649
Iteration 8/25 | Loss: 0.00162649
Iteration 9/25 | Loss: 0.00162649
Iteration 10/25 | Loss: 0.00162649
Iteration 11/25 | Loss: 0.00162649
Iteration 12/25 | Loss: 0.00162649
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.00162648712284863, 0.00162648712284863, 0.00162648712284863, 0.00162648712284863, 0.00162648712284863]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00162648712284863

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.03487802
Iteration 2/25 | Loss: 0.00179444
Iteration 3/25 | Loss: 0.00179443
Iteration 4/25 | Loss: 0.00179443
Iteration 5/25 | Loss: 0.00179443
Iteration 6/25 | Loss: 0.00179442
Iteration 7/25 | Loss: 0.00179442
Iteration 8/25 | Loss: 0.00179442
Iteration 9/25 | Loss: 0.00179442
Iteration 10/25 | Loss: 0.00179442
Iteration 11/25 | Loss: 0.00179442
Iteration 12/25 | Loss: 0.00179442
Iteration 13/25 | Loss: 0.00179442
Iteration 14/25 | Loss: 0.00179442
Iteration 15/25 | Loss: 0.00179442
Iteration 16/25 | Loss: 0.00179442
Iteration 17/25 | Loss: 0.00179442
Iteration 18/25 | Loss: 0.00179442
Iteration 19/25 | Loss: 0.00179442
Iteration 20/25 | Loss: 0.00179442
Iteration 21/25 | Loss: 0.00179442
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0017944240244105458, 0.0017944240244105458, 0.0017944240244105458, 0.0017944240244105458, 0.0017944240244105458]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017944240244105458

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00179442
Iteration 2/1000 | Loss: 0.00007418
Iteration 3/1000 | Loss: 0.00005008
Iteration 4/1000 | Loss: 0.00004355
Iteration 5/1000 | Loss: 0.00004104
Iteration 6/1000 | Loss: 0.00003917
Iteration 7/1000 | Loss: 0.00003821
Iteration 8/1000 | Loss: 0.00003760
Iteration 9/1000 | Loss: 0.00003720
Iteration 10/1000 | Loss: 0.00003692
Iteration 11/1000 | Loss: 0.00003676
Iteration 12/1000 | Loss: 0.00003676
Iteration 13/1000 | Loss: 0.00003675
Iteration 14/1000 | Loss: 0.00003675
Iteration 15/1000 | Loss: 0.00003674
Iteration 16/1000 | Loss: 0.00003673
Iteration 17/1000 | Loss: 0.00003673
Iteration 18/1000 | Loss: 0.00003673
Iteration 19/1000 | Loss: 0.00003672
Iteration 20/1000 | Loss: 0.00003672
Iteration 21/1000 | Loss: 0.00003672
Iteration 22/1000 | Loss: 0.00003671
Iteration 23/1000 | Loss: 0.00003671
Iteration 24/1000 | Loss: 0.00003671
Iteration 25/1000 | Loss: 0.00003671
Iteration 26/1000 | Loss: 0.00003670
Iteration 27/1000 | Loss: 0.00003670
Iteration 28/1000 | Loss: 0.00003670
Iteration 29/1000 | Loss: 0.00003670
Iteration 30/1000 | Loss: 0.00003670
Iteration 31/1000 | Loss: 0.00003670
Iteration 32/1000 | Loss: 0.00003670
Iteration 33/1000 | Loss: 0.00003670
Iteration 34/1000 | Loss: 0.00003670
Iteration 35/1000 | Loss: 0.00003670
Iteration 36/1000 | Loss: 0.00003670
Iteration 37/1000 | Loss: 0.00003670
Iteration 38/1000 | Loss: 0.00003669
Iteration 39/1000 | Loss: 0.00003669
Iteration 40/1000 | Loss: 0.00003669
Iteration 41/1000 | Loss: 0.00003669
Iteration 42/1000 | Loss: 0.00003669
Iteration 43/1000 | Loss: 0.00003669
Iteration 44/1000 | Loss: 0.00003668
Iteration 45/1000 | Loss: 0.00003668
Iteration 46/1000 | Loss: 0.00003668
Iteration 47/1000 | Loss: 0.00003667
Iteration 48/1000 | Loss: 0.00003667
Iteration 49/1000 | Loss: 0.00003666
Iteration 50/1000 | Loss: 0.00003666
Iteration 51/1000 | Loss: 0.00003666
Iteration 52/1000 | Loss: 0.00003666
Iteration 53/1000 | Loss: 0.00003666
Iteration 54/1000 | Loss: 0.00003666
Iteration 55/1000 | Loss: 0.00003666
Iteration 56/1000 | Loss: 0.00003666
Iteration 57/1000 | Loss: 0.00003665
Iteration 58/1000 | Loss: 0.00003665
Iteration 59/1000 | Loss: 0.00003664
Iteration 60/1000 | Loss: 0.00003664
Iteration 61/1000 | Loss: 0.00003664
Iteration 62/1000 | Loss: 0.00003663
Iteration 63/1000 | Loss: 0.00003662
Iteration 64/1000 | Loss: 0.00003662
Iteration 65/1000 | Loss: 0.00003662
Iteration 66/1000 | Loss: 0.00003662
Iteration 67/1000 | Loss: 0.00003662
Iteration 68/1000 | Loss: 0.00003661
Iteration 69/1000 | Loss: 0.00003661
Iteration 70/1000 | Loss: 0.00003661
Iteration 71/1000 | Loss: 0.00003661
Iteration 72/1000 | Loss: 0.00003661
Iteration 73/1000 | Loss: 0.00003661
Iteration 74/1000 | Loss: 0.00003661
Iteration 75/1000 | Loss: 0.00003661
Iteration 76/1000 | Loss: 0.00003661
Iteration 77/1000 | Loss: 0.00003661
Iteration 78/1000 | Loss: 0.00003661
Iteration 79/1000 | Loss: 0.00003661
Iteration 80/1000 | Loss: 0.00003661
Iteration 81/1000 | Loss: 0.00003661
Iteration 82/1000 | Loss: 0.00003661
Iteration 83/1000 | Loss: 0.00003661
Iteration 84/1000 | Loss: 0.00003661
Iteration 85/1000 | Loss: 0.00003661
Iteration 86/1000 | Loss: 0.00003661
Iteration 87/1000 | Loss: 0.00003661
Iteration 88/1000 | Loss: 0.00003661
Iteration 89/1000 | Loss: 0.00003661
Iteration 90/1000 | Loss: 0.00003661
Iteration 91/1000 | Loss: 0.00003661
Iteration 92/1000 | Loss: 0.00003661
Iteration 93/1000 | Loss: 0.00003661
Iteration 94/1000 | Loss: 0.00003661
Iteration 95/1000 | Loss: 0.00003661
Iteration 96/1000 | Loss: 0.00003661
Iteration 97/1000 | Loss: 0.00003661
Iteration 98/1000 | Loss: 0.00003661
Iteration 99/1000 | Loss: 0.00003661
Iteration 100/1000 | Loss: 0.00003661
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [3.660731454147026e-05, 3.660731454147026e-05, 3.660731454147026e-05, 3.660731454147026e-05, 3.660731454147026e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.660731454147026e-05

Optimization complete. Final v2v error: 5.1080641746521 mm

Highest mean error: 5.19983434677124 mm for frame 131

Lowest mean error: 4.98907995223999 mm for frame 122

Saving results

Total time: 30.550231218338013
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_42_us_0617/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_42_us_0617/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_42_us_0617/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00974859
Iteration 2/25 | Loss: 0.00158839
Iteration 3/25 | Loss: 0.00148335
Iteration 4/25 | Loss: 0.00146873
Iteration 5/25 | Loss: 0.00146382
Iteration 6/25 | Loss: 0.00146252
Iteration 7/25 | Loss: 0.00146212
Iteration 8/25 | Loss: 0.00146212
Iteration 9/25 | Loss: 0.00146212
Iteration 10/25 | Loss: 0.00146212
Iteration 11/25 | Loss: 0.00146212
Iteration 12/25 | Loss: 0.00146212
Iteration 13/25 | Loss: 0.00146212
Iteration 14/25 | Loss: 0.00146212
Iteration 15/25 | Loss: 0.00146212
Iteration 16/25 | Loss: 0.00146212
Iteration 17/25 | Loss: 0.00146212
Iteration 18/25 | Loss: 0.00146212
Iteration 19/25 | Loss: 0.00146212
Iteration 20/25 | Loss: 0.00146212
Iteration 21/25 | Loss: 0.00146212
Iteration 22/25 | Loss: 0.00146212
Iteration 23/25 | Loss: 0.00146212
Iteration 24/25 | Loss: 0.00146212
Iteration 25/25 | Loss: 0.00146212

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53415120
Iteration 2/25 | Loss: 0.00146768
Iteration 3/25 | Loss: 0.00146768
Iteration 4/25 | Loss: 0.00146768
Iteration 5/25 | Loss: 0.00146768
Iteration 6/25 | Loss: 0.00146768
Iteration 7/25 | Loss: 0.00146768
Iteration 8/25 | Loss: 0.00146768
Iteration 9/25 | Loss: 0.00146768
Iteration 10/25 | Loss: 0.00146768
Iteration 11/25 | Loss: 0.00146768
Iteration 12/25 | Loss: 0.00146768
Iteration 13/25 | Loss: 0.00146768
Iteration 14/25 | Loss: 0.00146768
Iteration 15/25 | Loss: 0.00146768
Iteration 16/25 | Loss: 0.00146768
Iteration 17/25 | Loss: 0.00146768
Iteration 18/25 | Loss: 0.00146768
Iteration 19/25 | Loss: 0.00146768
Iteration 20/25 | Loss: 0.00146768
Iteration 21/25 | Loss: 0.00146768
Iteration 22/25 | Loss: 0.00146768
Iteration 23/25 | Loss: 0.00146768
Iteration 24/25 | Loss: 0.00146768
Iteration 25/25 | Loss: 0.00146768

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00146768
Iteration 2/1000 | Loss: 0.00004047
Iteration 3/1000 | Loss: 0.00002919
Iteration 4/1000 | Loss: 0.00002695
Iteration 5/1000 | Loss: 0.00002571
Iteration 6/1000 | Loss: 0.00002476
Iteration 7/1000 | Loss: 0.00002434
Iteration 8/1000 | Loss: 0.00002374
Iteration 9/1000 | Loss: 0.00002342
Iteration 10/1000 | Loss: 0.00002315
Iteration 11/1000 | Loss: 0.00002293
Iteration 12/1000 | Loss: 0.00002277
Iteration 13/1000 | Loss: 0.00002258
Iteration 14/1000 | Loss: 0.00002255
Iteration 15/1000 | Loss: 0.00002253
Iteration 16/1000 | Loss: 0.00002252
Iteration 17/1000 | Loss: 0.00002252
Iteration 18/1000 | Loss: 0.00002251
Iteration 19/1000 | Loss: 0.00002251
Iteration 20/1000 | Loss: 0.00002250
Iteration 21/1000 | Loss: 0.00002250
Iteration 22/1000 | Loss: 0.00002246
Iteration 23/1000 | Loss: 0.00002243
Iteration 24/1000 | Loss: 0.00002242
Iteration 25/1000 | Loss: 0.00002242
Iteration 26/1000 | Loss: 0.00002242
Iteration 27/1000 | Loss: 0.00002242
Iteration 28/1000 | Loss: 0.00002242
Iteration 29/1000 | Loss: 0.00002241
Iteration 30/1000 | Loss: 0.00002241
Iteration 31/1000 | Loss: 0.00002241
Iteration 32/1000 | Loss: 0.00002240
Iteration 33/1000 | Loss: 0.00002240
Iteration 34/1000 | Loss: 0.00002239
Iteration 35/1000 | Loss: 0.00002239
Iteration 36/1000 | Loss: 0.00002238
Iteration 37/1000 | Loss: 0.00002238
Iteration 38/1000 | Loss: 0.00002237
Iteration 39/1000 | Loss: 0.00002237
Iteration 40/1000 | Loss: 0.00002237
Iteration 41/1000 | Loss: 0.00002236
Iteration 42/1000 | Loss: 0.00002236
Iteration 43/1000 | Loss: 0.00002236
Iteration 44/1000 | Loss: 0.00002235
Iteration 45/1000 | Loss: 0.00002235
Iteration 46/1000 | Loss: 0.00002235
Iteration 47/1000 | Loss: 0.00002235
Iteration 48/1000 | Loss: 0.00002235
Iteration 49/1000 | Loss: 0.00002235
Iteration 50/1000 | Loss: 0.00002234
Iteration 51/1000 | Loss: 0.00002234
Iteration 52/1000 | Loss: 0.00002234
Iteration 53/1000 | Loss: 0.00002234
Iteration 54/1000 | Loss: 0.00002233
Iteration 55/1000 | Loss: 0.00002233
Iteration 56/1000 | Loss: 0.00002233
Iteration 57/1000 | Loss: 0.00002232
Iteration 58/1000 | Loss: 0.00002232
Iteration 59/1000 | Loss: 0.00002232
Iteration 60/1000 | Loss: 0.00002232
Iteration 61/1000 | Loss: 0.00002232
Iteration 62/1000 | Loss: 0.00002231
Iteration 63/1000 | Loss: 0.00002231
Iteration 64/1000 | Loss: 0.00002231
Iteration 65/1000 | Loss: 0.00002230
Iteration 66/1000 | Loss: 0.00002230
Iteration 67/1000 | Loss: 0.00002230
Iteration 68/1000 | Loss: 0.00002230
Iteration 69/1000 | Loss: 0.00002229
Iteration 70/1000 | Loss: 0.00002229
Iteration 71/1000 | Loss: 0.00002229
Iteration 72/1000 | Loss: 0.00002229
Iteration 73/1000 | Loss: 0.00002228
Iteration 74/1000 | Loss: 0.00002228
Iteration 75/1000 | Loss: 0.00002228
Iteration 76/1000 | Loss: 0.00002228
Iteration 77/1000 | Loss: 0.00002227
Iteration 78/1000 | Loss: 0.00002227
Iteration 79/1000 | Loss: 0.00002227
Iteration 80/1000 | Loss: 0.00002227
Iteration 81/1000 | Loss: 0.00002227
Iteration 82/1000 | Loss: 0.00002227
Iteration 83/1000 | Loss: 0.00002226
Iteration 84/1000 | Loss: 0.00002226
Iteration 85/1000 | Loss: 0.00002226
Iteration 86/1000 | Loss: 0.00002226
Iteration 87/1000 | Loss: 0.00002226
Iteration 88/1000 | Loss: 0.00002226
Iteration 89/1000 | Loss: 0.00002226
Iteration 90/1000 | Loss: 0.00002226
Iteration 91/1000 | Loss: 0.00002226
Iteration 92/1000 | Loss: 0.00002226
Iteration 93/1000 | Loss: 0.00002226
Iteration 94/1000 | Loss: 0.00002225
Iteration 95/1000 | Loss: 0.00002225
Iteration 96/1000 | Loss: 0.00002225
Iteration 97/1000 | Loss: 0.00002225
Iteration 98/1000 | Loss: 0.00002225
Iteration 99/1000 | Loss: 0.00002225
Iteration 100/1000 | Loss: 0.00002225
Iteration 101/1000 | Loss: 0.00002225
Iteration 102/1000 | Loss: 0.00002225
Iteration 103/1000 | Loss: 0.00002225
Iteration 104/1000 | Loss: 0.00002225
Iteration 105/1000 | Loss: 0.00002224
Iteration 106/1000 | Loss: 0.00002224
Iteration 107/1000 | Loss: 0.00002224
Iteration 108/1000 | Loss: 0.00002224
Iteration 109/1000 | Loss: 0.00002224
Iteration 110/1000 | Loss: 0.00002224
Iteration 111/1000 | Loss: 0.00002224
Iteration 112/1000 | Loss: 0.00002224
Iteration 113/1000 | Loss: 0.00002224
Iteration 114/1000 | Loss: 0.00002224
Iteration 115/1000 | Loss: 0.00002224
Iteration 116/1000 | Loss: 0.00002224
Iteration 117/1000 | Loss: 0.00002223
Iteration 118/1000 | Loss: 0.00002223
Iteration 119/1000 | Loss: 0.00002223
Iteration 120/1000 | Loss: 0.00002223
Iteration 121/1000 | Loss: 0.00002223
Iteration 122/1000 | Loss: 0.00002223
Iteration 123/1000 | Loss: 0.00002223
Iteration 124/1000 | Loss: 0.00002223
Iteration 125/1000 | Loss: 0.00002223
Iteration 126/1000 | Loss: 0.00002223
Iteration 127/1000 | Loss: 0.00002223
Iteration 128/1000 | Loss: 0.00002223
Iteration 129/1000 | Loss: 0.00002223
Iteration 130/1000 | Loss: 0.00002223
Iteration 131/1000 | Loss: 0.00002223
Iteration 132/1000 | Loss: 0.00002223
Iteration 133/1000 | Loss: 0.00002223
Iteration 134/1000 | Loss: 0.00002223
Iteration 135/1000 | Loss: 0.00002223
Iteration 136/1000 | Loss: 0.00002223
Iteration 137/1000 | Loss: 0.00002223
Iteration 138/1000 | Loss: 0.00002223
Iteration 139/1000 | Loss: 0.00002223
Iteration 140/1000 | Loss: 0.00002223
Iteration 141/1000 | Loss: 0.00002223
Iteration 142/1000 | Loss: 0.00002223
Iteration 143/1000 | Loss: 0.00002223
Iteration 144/1000 | Loss: 0.00002223
Iteration 145/1000 | Loss: 0.00002223
Iteration 146/1000 | Loss: 0.00002223
Iteration 147/1000 | Loss: 0.00002223
Iteration 148/1000 | Loss: 0.00002223
Iteration 149/1000 | Loss: 0.00002223
Iteration 150/1000 | Loss: 0.00002223
Iteration 151/1000 | Loss: 0.00002223
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [2.2228148736758158e-05, 2.2228148736758158e-05, 2.2228148736758158e-05, 2.2228148736758158e-05, 2.2228148736758158e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2228148736758158e-05

Optimization complete. Final v2v error: 4.0381269454956055 mm

Highest mean error: 4.674356460571289 mm for frame 45

Lowest mean error: 3.7835605144500732 mm for frame 111

Saving results

Total time: 37.7894184589386
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_42_us_0617/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_42_us_0617/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_42_us_0617/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00953804
Iteration 2/25 | Loss: 0.00154766
Iteration 3/25 | Loss: 0.00148207
Iteration 4/25 | Loss: 0.00146917
Iteration 5/25 | Loss: 0.00146456
Iteration 6/25 | Loss: 0.00146359
Iteration 7/25 | Loss: 0.00146359
Iteration 8/25 | Loss: 0.00146359
Iteration 9/25 | Loss: 0.00146359
Iteration 10/25 | Loss: 0.00146359
Iteration 11/25 | Loss: 0.00146359
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001463586580939591, 0.001463586580939591, 0.001463586580939591, 0.001463586580939591, 0.001463586580939591]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001463586580939591

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.54179573
Iteration 2/25 | Loss: 0.00147254
Iteration 3/25 | Loss: 0.00147253
Iteration 4/25 | Loss: 0.00147253
Iteration 5/25 | Loss: 0.00147253
Iteration 6/25 | Loss: 0.00147253
Iteration 7/25 | Loss: 0.00147253
Iteration 8/25 | Loss: 0.00147253
Iteration 9/25 | Loss: 0.00147253
Iteration 10/25 | Loss: 0.00147252
Iteration 11/25 | Loss: 0.00147252
Iteration 12/25 | Loss: 0.00147252
Iteration 13/25 | Loss: 0.00147252
Iteration 14/25 | Loss: 0.00147252
Iteration 15/25 | Loss: 0.00147252
Iteration 16/25 | Loss: 0.00147252
Iteration 17/25 | Loss: 0.00147252
Iteration 18/25 | Loss: 0.00147252
Iteration 19/25 | Loss: 0.00147252
Iteration 20/25 | Loss: 0.00147252
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0014725246001034975, 0.0014725246001034975, 0.0014725246001034975, 0.0014725246001034975, 0.0014725246001034975]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014725246001034975

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00147252
Iteration 2/1000 | Loss: 0.00004936
Iteration 3/1000 | Loss: 0.00003588
Iteration 4/1000 | Loss: 0.00003216
Iteration 5/1000 | Loss: 0.00003043
Iteration 6/1000 | Loss: 0.00002959
Iteration 7/1000 | Loss: 0.00002885
Iteration 8/1000 | Loss: 0.00002826
Iteration 9/1000 | Loss: 0.00002790
Iteration 10/1000 | Loss: 0.00002763
Iteration 11/1000 | Loss: 0.00002745
Iteration 12/1000 | Loss: 0.00002744
Iteration 13/1000 | Loss: 0.00002744
Iteration 14/1000 | Loss: 0.00002738
Iteration 15/1000 | Loss: 0.00002737
Iteration 16/1000 | Loss: 0.00002734
Iteration 17/1000 | Loss: 0.00002732
Iteration 18/1000 | Loss: 0.00002726
Iteration 19/1000 | Loss: 0.00002725
Iteration 20/1000 | Loss: 0.00002725
Iteration 21/1000 | Loss: 0.00002725
Iteration 22/1000 | Loss: 0.00002725
Iteration 23/1000 | Loss: 0.00002724
Iteration 24/1000 | Loss: 0.00002723
Iteration 25/1000 | Loss: 0.00002723
Iteration 26/1000 | Loss: 0.00002720
Iteration 27/1000 | Loss: 0.00002720
Iteration 28/1000 | Loss: 0.00002720
Iteration 29/1000 | Loss: 0.00002719
Iteration 30/1000 | Loss: 0.00002719
Iteration 31/1000 | Loss: 0.00002718
Iteration 32/1000 | Loss: 0.00002718
Iteration 33/1000 | Loss: 0.00002717
Iteration 34/1000 | Loss: 0.00002717
Iteration 35/1000 | Loss: 0.00002717
Iteration 36/1000 | Loss: 0.00002717
Iteration 37/1000 | Loss: 0.00002717
Iteration 38/1000 | Loss: 0.00002717
Iteration 39/1000 | Loss: 0.00002717
Iteration 40/1000 | Loss: 0.00002717
Iteration 41/1000 | Loss: 0.00002717
Iteration 42/1000 | Loss: 0.00002716
Iteration 43/1000 | Loss: 0.00002716
Iteration 44/1000 | Loss: 0.00002714
Iteration 45/1000 | Loss: 0.00002714
Iteration 46/1000 | Loss: 0.00002714
Iteration 47/1000 | Loss: 0.00002714
Iteration 48/1000 | Loss: 0.00002714
Iteration 49/1000 | Loss: 0.00002713
Iteration 50/1000 | Loss: 0.00002713
Iteration 51/1000 | Loss: 0.00002713
Iteration 52/1000 | Loss: 0.00002713
Iteration 53/1000 | Loss: 0.00002712
Iteration 54/1000 | Loss: 0.00002712
Iteration 55/1000 | Loss: 0.00002712
Iteration 56/1000 | Loss: 0.00002711
Iteration 57/1000 | Loss: 0.00002711
Iteration 58/1000 | Loss: 0.00002711
Iteration 59/1000 | Loss: 0.00002711
Iteration 60/1000 | Loss: 0.00002710
Iteration 61/1000 | Loss: 0.00002710
Iteration 62/1000 | Loss: 0.00002710
Iteration 63/1000 | Loss: 0.00002709
Iteration 64/1000 | Loss: 0.00002709
Iteration 65/1000 | Loss: 0.00002709
Iteration 66/1000 | Loss: 0.00002708
Iteration 67/1000 | Loss: 0.00002708
Iteration 68/1000 | Loss: 0.00002708
Iteration 69/1000 | Loss: 0.00002708
Iteration 70/1000 | Loss: 0.00002708
Iteration 71/1000 | Loss: 0.00002708
Iteration 72/1000 | Loss: 0.00002707
Iteration 73/1000 | Loss: 0.00002707
Iteration 74/1000 | Loss: 0.00002707
Iteration 75/1000 | Loss: 0.00002707
Iteration 76/1000 | Loss: 0.00002707
Iteration 77/1000 | Loss: 0.00002707
Iteration 78/1000 | Loss: 0.00002707
Iteration 79/1000 | Loss: 0.00002707
Iteration 80/1000 | Loss: 0.00002706
Iteration 81/1000 | Loss: 0.00002706
Iteration 82/1000 | Loss: 0.00002706
Iteration 83/1000 | Loss: 0.00002706
Iteration 84/1000 | Loss: 0.00002706
Iteration 85/1000 | Loss: 0.00002705
Iteration 86/1000 | Loss: 0.00002705
Iteration 87/1000 | Loss: 0.00002705
Iteration 88/1000 | Loss: 0.00002705
Iteration 89/1000 | Loss: 0.00002705
Iteration 90/1000 | Loss: 0.00002705
Iteration 91/1000 | Loss: 0.00002705
Iteration 92/1000 | Loss: 0.00002705
Iteration 93/1000 | Loss: 0.00002705
Iteration 94/1000 | Loss: 0.00002704
Iteration 95/1000 | Loss: 0.00002704
Iteration 96/1000 | Loss: 0.00002704
Iteration 97/1000 | Loss: 0.00002704
Iteration 98/1000 | Loss: 0.00002704
Iteration 99/1000 | Loss: 0.00002704
Iteration 100/1000 | Loss: 0.00002704
Iteration 101/1000 | Loss: 0.00002704
Iteration 102/1000 | Loss: 0.00002704
Iteration 103/1000 | Loss: 0.00002703
Iteration 104/1000 | Loss: 0.00002703
Iteration 105/1000 | Loss: 0.00002703
Iteration 106/1000 | Loss: 0.00002703
Iteration 107/1000 | Loss: 0.00002703
Iteration 108/1000 | Loss: 0.00002703
Iteration 109/1000 | Loss: 0.00002703
Iteration 110/1000 | Loss: 0.00002703
Iteration 111/1000 | Loss: 0.00002702
Iteration 112/1000 | Loss: 0.00002702
Iteration 113/1000 | Loss: 0.00002702
Iteration 114/1000 | Loss: 0.00002702
Iteration 115/1000 | Loss: 0.00002702
Iteration 116/1000 | Loss: 0.00002702
Iteration 117/1000 | Loss: 0.00002702
Iteration 118/1000 | Loss: 0.00002702
Iteration 119/1000 | Loss: 0.00002702
Iteration 120/1000 | Loss: 0.00002702
Iteration 121/1000 | Loss: 0.00002702
Iteration 122/1000 | Loss: 0.00002702
Iteration 123/1000 | Loss: 0.00002702
Iteration 124/1000 | Loss: 0.00002702
Iteration 125/1000 | Loss: 0.00002701
Iteration 126/1000 | Loss: 0.00002701
Iteration 127/1000 | Loss: 0.00002701
Iteration 128/1000 | Loss: 0.00002701
Iteration 129/1000 | Loss: 0.00002701
Iteration 130/1000 | Loss: 0.00002701
Iteration 131/1000 | Loss: 0.00002701
Iteration 132/1000 | Loss: 0.00002701
Iteration 133/1000 | Loss: 0.00002701
Iteration 134/1000 | Loss: 0.00002701
Iteration 135/1000 | Loss: 0.00002700
Iteration 136/1000 | Loss: 0.00002700
Iteration 137/1000 | Loss: 0.00002700
Iteration 138/1000 | Loss: 0.00002700
Iteration 139/1000 | Loss: 0.00002700
Iteration 140/1000 | Loss: 0.00002700
Iteration 141/1000 | Loss: 0.00002700
Iteration 142/1000 | Loss: 0.00002700
Iteration 143/1000 | Loss: 0.00002700
Iteration 144/1000 | Loss: 0.00002699
Iteration 145/1000 | Loss: 0.00002699
Iteration 146/1000 | Loss: 0.00002699
Iteration 147/1000 | Loss: 0.00002699
Iteration 148/1000 | Loss: 0.00002699
Iteration 149/1000 | Loss: 0.00002699
Iteration 150/1000 | Loss: 0.00002699
Iteration 151/1000 | Loss: 0.00002699
Iteration 152/1000 | Loss: 0.00002699
Iteration 153/1000 | Loss: 0.00002699
Iteration 154/1000 | Loss: 0.00002699
Iteration 155/1000 | Loss: 0.00002699
Iteration 156/1000 | Loss: 0.00002699
Iteration 157/1000 | Loss: 0.00002699
Iteration 158/1000 | Loss: 0.00002698
Iteration 159/1000 | Loss: 0.00002698
Iteration 160/1000 | Loss: 0.00002698
Iteration 161/1000 | Loss: 0.00002698
Iteration 162/1000 | Loss: 0.00002698
Iteration 163/1000 | Loss: 0.00002698
Iteration 164/1000 | Loss: 0.00002698
Iteration 165/1000 | Loss: 0.00002698
Iteration 166/1000 | Loss: 0.00002698
Iteration 167/1000 | Loss: 0.00002698
Iteration 168/1000 | Loss: 0.00002698
Iteration 169/1000 | Loss: 0.00002698
Iteration 170/1000 | Loss: 0.00002698
Iteration 171/1000 | Loss: 0.00002698
Iteration 172/1000 | Loss: 0.00002698
Iteration 173/1000 | Loss: 0.00002698
Iteration 174/1000 | Loss: 0.00002698
Iteration 175/1000 | Loss: 0.00002698
Iteration 176/1000 | Loss: 0.00002698
Iteration 177/1000 | Loss: 0.00002698
Iteration 178/1000 | Loss: 0.00002698
Iteration 179/1000 | Loss: 0.00002698
Iteration 180/1000 | Loss: 0.00002698
Iteration 181/1000 | Loss: 0.00002698
Iteration 182/1000 | Loss: 0.00002698
Iteration 183/1000 | Loss: 0.00002698
Iteration 184/1000 | Loss: 0.00002698
Iteration 185/1000 | Loss: 0.00002698
Iteration 186/1000 | Loss: 0.00002698
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [2.6978570531355217e-05, 2.6978570531355217e-05, 2.6978570531355217e-05, 2.6978570531355217e-05, 2.6978570531355217e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6978570531355217e-05

Optimization complete. Final v2v error: 4.4803667068481445 mm

Highest mean error: 5.039170742034912 mm for frame 106

Lowest mean error: 3.9868125915527344 mm for frame 12

Saving results

Total time: 37.071129322052
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_42_us_0617/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_42_us_0617/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_42_us_0617/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00589806
Iteration 2/25 | Loss: 0.00150638
Iteration 3/25 | Loss: 0.00144058
Iteration 4/25 | Loss: 0.00143138
Iteration 5/25 | Loss: 0.00142794
Iteration 6/25 | Loss: 0.00142766
Iteration 7/25 | Loss: 0.00142766
Iteration 8/25 | Loss: 0.00142766
Iteration 9/25 | Loss: 0.00142766
Iteration 10/25 | Loss: 0.00142766
Iteration 11/25 | Loss: 0.00142766
Iteration 12/25 | Loss: 0.00142766
Iteration 13/25 | Loss: 0.00142766
Iteration 14/25 | Loss: 0.00142766
Iteration 15/25 | Loss: 0.00142766
Iteration 16/25 | Loss: 0.00142766
Iteration 17/25 | Loss: 0.00142766
Iteration 18/25 | Loss: 0.00142766
Iteration 19/25 | Loss: 0.00142766
Iteration 20/25 | Loss: 0.00142766
Iteration 21/25 | Loss: 0.00142766
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0014276638394221663, 0.0014276638394221663, 0.0014276638394221663, 0.0014276638394221663, 0.0014276638394221663]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014276638394221663

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 18.78131294
Iteration 2/25 | Loss: 0.00140221
Iteration 3/25 | Loss: 0.00140211
Iteration 4/25 | Loss: 0.00140211
Iteration 5/25 | Loss: 0.00140211
Iteration 6/25 | Loss: 0.00140211
Iteration 7/25 | Loss: 0.00140211
Iteration 8/25 | Loss: 0.00140211
Iteration 9/25 | Loss: 0.00140211
Iteration 10/25 | Loss: 0.00140211
Iteration 11/25 | Loss: 0.00140210
Iteration 12/25 | Loss: 0.00140210
Iteration 13/25 | Loss: 0.00140210
Iteration 14/25 | Loss: 0.00140210
Iteration 15/25 | Loss: 0.00140210
Iteration 16/25 | Loss: 0.00140210
Iteration 17/25 | Loss: 0.00140210
Iteration 18/25 | Loss: 0.00140210
Iteration 19/25 | Loss: 0.00140210
Iteration 20/25 | Loss: 0.00140210
Iteration 21/25 | Loss: 0.00140210
Iteration 22/25 | Loss: 0.00140210
Iteration 23/25 | Loss: 0.00140210
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001402104739099741, 0.001402104739099741, 0.001402104739099741, 0.001402104739099741, 0.001402104739099741]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001402104739099741

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00140210
Iteration 2/1000 | Loss: 0.00003988
Iteration 3/1000 | Loss: 0.00003132
Iteration 4/1000 | Loss: 0.00002863
Iteration 5/1000 | Loss: 0.00002715
Iteration 6/1000 | Loss: 0.00002629
Iteration 7/1000 | Loss: 0.00002571
Iteration 8/1000 | Loss: 0.00002531
Iteration 9/1000 | Loss: 0.00002503
Iteration 10/1000 | Loss: 0.00002481
Iteration 11/1000 | Loss: 0.00002470
Iteration 12/1000 | Loss: 0.00002462
Iteration 13/1000 | Loss: 0.00002452
Iteration 14/1000 | Loss: 0.00002449
Iteration 15/1000 | Loss: 0.00002448
Iteration 16/1000 | Loss: 0.00002447
Iteration 17/1000 | Loss: 0.00002446
Iteration 18/1000 | Loss: 0.00002445
Iteration 19/1000 | Loss: 0.00002445
Iteration 20/1000 | Loss: 0.00002445
Iteration 21/1000 | Loss: 0.00002444
Iteration 22/1000 | Loss: 0.00002442
Iteration 23/1000 | Loss: 0.00002442
Iteration 24/1000 | Loss: 0.00002442
Iteration 25/1000 | Loss: 0.00002442
Iteration 26/1000 | Loss: 0.00002442
Iteration 27/1000 | Loss: 0.00002442
Iteration 28/1000 | Loss: 0.00002442
Iteration 29/1000 | Loss: 0.00002442
Iteration 30/1000 | Loss: 0.00002442
Iteration 31/1000 | Loss: 0.00002441
Iteration 32/1000 | Loss: 0.00002441
Iteration 33/1000 | Loss: 0.00002441
Iteration 34/1000 | Loss: 0.00002441
Iteration 35/1000 | Loss: 0.00002441
Iteration 36/1000 | Loss: 0.00002441
Iteration 37/1000 | Loss: 0.00002441
Iteration 38/1000 | Loss: 0.00002440
Iteration 39/1000 | Loss: 0.00002440
Iteration 40/1000 | Loss: 0.00002440
Iteration 41/1000 | Loss: 0.00002440
Iteration 42/1000 | Loss: 0.00002440
Iteration 43/1000 | Loss: 0.00002440
Iteration 44/1000 | Loss: 0.00002439
Iteration 45/1000 | Loss: 0.00002439
Iteration 46/1000 | Loss: 0.00002438
Iteration 47/1000 | Loss: 0.00002438
Iteration 48/1000 | Loss: 0.00002438
Iteration 49/1000 | Loss: 0.00002437
Iteration 50/1000 | Loss: 0.00002437
Iteration 51/1000 | Loss: 0.00002436
Iteration 52/1000 | Loss: 0.00002436
Iteration 53/1000 | Loss: 0.00002435
Iteration 54/1000 | Loss: 0.00002435
Iteration 55/1000 | Loss: 0.00002435
Iteration 56/1000 | Loss: 0.00002434
Iteration 57/1000 | Loss: 0.00002434
Iteration 58/1000 | Loss: 0.00002434
Iteration 59/1000 | Loss: 0.00002434
Iteration 60/1000 | Loss: 0.00002434
Iteration 61/1000 | Loss: 0.00002434
Iteration 62/1000 | Loss: 0.00002433
Iteration 63/1000 | Loss: 0.00002433
Iteration 64/1000 | Loss: 0.00002433
Iteration 65/1000 | Loss: 0.00002432
Iteration 66/1000 | Loss: 0.00002432
Iteration 67/1000 | Loss: 0.00002432
Iteration 68/1000 | Loss: 0.00002431
Iteration 69/1000 | Loss: 0.00002431
Iteration 70/1000 | Loss: 0.00002431
Iteration 71/1000 | Loss: 0.00002431
Iteration 72/1000 | Loss: 0.00002431
Iteration 73/1000 | Loss: 0.00002431
Iteration 74/1000 | Loss: 0.00002430
Iteration 75/1000 | Loss: 0.00002430
Iteration 76/1000 | Loss: 0.00002430
Iteration 77/1000 | Loss: 0.00002430
Iteration 78/1000 | Loss: 0.00002430
Iteration 79/1000 | Loss: 0.00002430
Iteration 80/1000 | Loss: 0.00002430
Iteration 81/1000 | Loss: 0.00002430
Iteration 82/1000 | Loss: 0.00002430
Iteration 83/1000 | Loss: 0.00002430
Iteration 84/1000 | Loss: 0.00002430
Iteration 85/1000 | Loss: 0.00002430
Iteration 86/1000 | Loss: 0.00002430
Iteration 87/1000 | Loss: 0.00002430
Iteration 88/1000 | Loss: 0.00002430
Iteration 89/1000 | Loss: 0.00002429
Iteration 90/1000 | Loss: 0.00002429
Iteration 91/1000 | Loss: 0.00002429
Iteration 92/1000 | Loss: 0.00002429
Iteration 93/1000 | Loss: 0.00002429
Iteration 94/1000 | Loss: 0.00002429
Iteration 95/1000 | Loss: 0.00002429
Iteration 96/1000 | Loss: 0.00002429
Iteration 97/1000 | Loss: 0.00002429
Iteration 98/1000 | Loss: 0.00002429
Iteration 99/1000 | Loss: 0.00002429
Iteration 100/1000 | Loss: 0.00002429
Iteration 101/1000 | Loss: 0.00002429
Iteration 102/1000 | Loss: 0.00002429
Iteration 103/1000 | Loss: 0.00002429
Iteration 104/1000 | Loss: 0.00002428
Iteration 105/1000 | Loss: 0.00002428
Iteration 106/1000 | Loss: 0.00002428
Iteration 107/1000 | Loss: 0.00002428
Iteration 108/1000 | Loss: 0.00002428
Iteration 109/1000 | Loss: 0.00002428
Iteration 110/1000 | Loss: 0.00002428
Iteration 111/1000 | Loss: 0.00002428
Iteration 112/1000 | Loss: 0.00002428
Iteration 113/1000 | Loss: 0.00002428
Iteration 114/1000 | Loss: 0.00002428
Iteration 115/1000 | Loss: 0.00002428
Iteration 116/1000 | Loss: 0.00002428
Iteration 117/1000 | Loss: 0.00002428
Iteration 118/1000 | Loss: 0.00002428
Iteration 119/1000 | Loss: 0.00002427
Iteration 120/1000 | Loss: 0.00002427
Iteration 121/1000 | Loss: 0.00002427
Iteration 122/1000 | Loss: 0.00002427
Iteration 123/1000 | Loss: 0.00002427
Iteration 124/1000 | Loss: 0.00002427
Iteration 125/1000 | Loss: 0.00002427
Iteration 126/1000 | Loss: 0.00002427
Iteration 127/1000 | Loss: 0.00002427
Iteration 128/1000 | Loss: 0.00002427
Iteration 129/1000 | Loss: 0.00002427
Iteration 130/1000 | Loss: 0.00002427
Iteration 131/1000 | Loss: 0.00002427
Iteration 132/1000 | Loss: 0.00002427
Iteration 133/1000 | Loss: 0.00002427
Iteration 134/1000 | Loss: 0.00002427
Iteration 135/1000 | Loss: 0.00002427
Iteration 136/1000 | Loss: 0.00002427
Iteration 137/1000 | Loss: 0.00002427
Iteration 138/1000 | Loss: 0.00002426
Iteration 139/1000 | Loss: 0.00002426
Iteration 140/1000 | Loss: 0.00002426
Iteration 141/1000 | Loss: 0.00002426
Iteration 142/1000 | Loss: 0.00002426
Iteration 143/1000 | Loss: 0.00002426
Iteration 144/1000 | Loss: 0.00002426
Iteration 145/1000 | Loss: 0.00002426
Iteration 146/1000 | Loss: 0.00002426
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [2.4264001694973558e-05, 2.4264001694973558e-05, 2.4264001694973558e-05, 2.4264001694973558e-05, 2.4264001694973558e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4264001694973558e-05

Optimization complete. Final v2v error: 4.307931900024414 mm

Highest mean error: 4.607532024383545 mm for frame 60

Lowest mean error: 3.887666940689087 mm for frame 144

Saving results

Total time: 39.76866888999939
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_2081/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_2081/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_2081/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00907665
Iteration 2/25 | Loss: 0.00178378
Iteration 3/25 | Loss: 0.00139982
Iteration 4/25 | Loss: 0.00132353
Iteration 5/25 | Loss: 0.00130489
Iteration 6/25 | Loss: 0.00129132
Iteration 7/25 | Loss: 0.00128525
Iteration 8/25 | Loss: 0.00128286
Iteration 9/25 | Loss: 0.00127999
Iteration 10/25 | Loss: 0.00127958
Iteration 11/25 | Loss: 0.00127937
Iteration 12/25 | Loss: 0.00127929
Iteration 13/25 | Loss: 0.00127929
Iteration 14/25 | Loss: 0.00127929
Iteration 15/25 | Loss: 0.00127928
Iteration 16/25 | Loss: 0.00127928
Iteration 17/25 | Loss: 0.00127928
Iteration 18/25 | Loss: 0.00127928
Iteration 19/25 | Loss: 0.00127928
Iteration 20/25 | Loss: 0.00127928
Iteration 21/25 | Loss: 0.00127927
Iteration 22/25 | Loss: 0.00127927
Iteration 23/25 | Loss: 0.00127927
Iteration 24/25 | Loss: 0.00127927
Iteration 25/25 | Loss: 0.00127927

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.13925159
Iteration 2/25 | Loss: 0.00146771
Iteration 3/25 | Loss: 0.00146770
Iteration 4/25 | Loss: 0.00146770
Iteration 5/25 | Loss: 0.00146770
Iteration 6/25 | Loss: 0.00146770
Iteration 7/25 | Loss: 0.00146770
Iteration 8/25 | Loss: 0.00146770
Iteration 9/25 | Loss: 0.00146770
Iteration 10/25 | Loss: 0.00146770
Iteration 11/25 | Loss: 0.00146770
Iteration 12/25 | Loss: 0.00146770
Iteration 13/25 | Loss: 0.00146770
Iteration 14/25 | Loss: 0.00146770
Iteration 15/25 | Loss: 0.00146770
Iteration 16/25 | Loss: 0.00146770
Iteration 17/25 | Loss: 0.00146770
Iteration 18/25 | Loss: 0.00146770
Iteration 19/25 | Loss: 0.00146770
Iteration 20/25 | Loss: 0.00146770
Iteration 21/25 | Loss: 0.00146770
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0014676954597234726, 0.0014676954597234726, 0.0014676954597234726, 0.0014676954597234726, 0.0014676954597234726]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014676954597234726

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00146770
Iteration 2/1000 | Loss: 0.00006545
Iteration 3/1000 | Loss: 0.00004253
Iteration 4/1000 | Loss: 0.00003055
Iteration 5/1000 | Loss: 0.00002731
Iteration 6/1000 | Loss: 0.00002473
Iteration 7/1000 | Loss: 0.00002377
Iteration 8/1000 | Loss: 0.00002281
Iteration 9/1000 | Loss: 0.00002222
Iteration 10/1000 | Loss: 0.00002168
Iteration 11/1000 | Loss: 0.00002129
Iteration 12/1000 | Loss: 0.00002100
Iteration 13/1000 | Loss: 0.00002100
Iteration 14/1000 | Loss: 0.00002082
Iteration 15/1000 | Loss: 0.00002065
Iteration 16/1000 | Loss: 0.00002060
Iteration 17/1000 | Loss: 0.00002056
Iteration 18/1000 | Loss: 0.00002055
Iteration 19/1000 | Loss: 0.00002053
Iteration 20/1000 | Loss: 0.00002053
Iteration 21/1000 | Loss: 0.00002053
Iteration 22/1000 | Loss: 0.00002053
Iteration 23/1000 | Loss: 0.00002052
Iteration 24/1000 | Loss: 0.00002052
Iteration 25/1000 | Loss: 0.00002052
Iteration 26/1000 | Loss: 0.00002052
Iteration 27/1000 | Loss: 0.00002052
Iteration 28/1000 | Loss: 0.00002052
Iteration 29/1000 | Loss: 0.00002052
Iteration 30/1000 | Loss: 0.00002052
Iteration 31/1000 | Loss: 0.00002052
Iteration 32/1000 | Loss: 0.00002052
Iteration 33/1000 | Loss: 0.00002052
Iteration 34/1000 | Loss: 0.00002052
Iteration 35/1000 | Loss: 0.00002052
Iteration 36/1000 | Loss: 0.00002051
Iteration 37/1000 | Loss: 0.00002051
Iteration 38/1000 | Loss: 0.00002051
Iteration 39/1000 | Loss: 0.00002051
Iteration 40/1000 | Loss: 0.00002051
Iteration 41/1000 | Loss: 0.00002050
Iteration 42/1000 | Loss: 0.00002050
Iteration 43/1000 | Loss: 0.00002049
Iteration 44/1000 | Loss: 0.00002049
Iteration 45/1000 | Loss: 0.00002049
Iteration 46/1000 | Loss: 0.00002049
Iteration 47/1000 | Loss: 0.00002049
Iteration 48/1000 | Loss: 0.00002048
Iteration 49/1000 | Loss: 0.00002048
Iteration 50/1000 | Loss: 0.00002048
Iteration 51/1000 | Loss: 0.00002047
Iteration 52/1000 | Loss: 0.00002046
Iteration 53/1000 | Loss: 0.00002046
Iteration 54/1000 | Loss: 0.00002046
Iteration 55/1000 | Loss: 0.00002046
Iteration 56/1000 | Loss: 0.00002045
Iteration 57/1000 | Loss: 0.00002045
Iteration 58/1000 | Loss: 0.00002045
Iteration 59/1000 | Loss: 0.00002045
Iteration 60/1000 | Loss: 0.00002045
Iteration 61/1000 | Loss: 0.00002044
Iteration 62/1000 | Loss: 0.00002043
Iteration 63/1000 | Loss: 0.00002043
Iteration 64/1000 | Loss: 0.00002043
Iteration 65/1000 | Loss: 0.00002043
Iteration 66/1000 | Loss: 0.00002043
Iteration 67/1000 | Loss: 0.00002043
Iteration 68/1000 | Loss: 0.00002042
Iteration 69/1000 | Loss: 0.00002042
Iteration 70/1000 | Loss: 0.00002042
Iteration 71/1000 | Loss: 0.00002042
Iteration 72/1000 | Loss: 0.00002042
Iteration 73/1000 | Loss: 0.00002042
Iteration 74/1000 | Loss: 0.00002042
Iteration 75/1000 | Loss: 0.00002042
Iteration 76/1000 | Loss: 0.00002042
Iteration 77/1000 | Loss: 0.00002042
Iteration 78/1000 | Loss: 0.00002042
Iteration 79/1000 | Loss: 0.00002042
Iteration 80/1000 | Loss: 0.00002041
Iteration 81/1000 | Loss: 0.00002041
Iteration 82/1000 | Loss: 0.00002041
Iteration 83/1000 | Loss: 0.00002041
Iteration 84/1000 | Loss: 0.00002041
Iteration 85/1000 | Loss: 0.00002041
Iteration 86/1000 | Loss: 0.00002041
Iteration 87/1000 | Loss: 0.00002041
Iteration 88/1000 | Loss: 0.00002040
Iteration 89/1000 | Loss: 0.00002040
Iteration 90/1000 | Loss: 0.00002040
Iteration 91/1000 | Loss: 0.00002040
Iteration 92/1000 | Loss: 0.00002040
Iteration 93/1000 | Loss: 0.00002040
Iteration 94/1000 | Loss: 0.00002040
Iteration 95/1000 | Loss: 0.00002040
Iteration 96/1000 | Loss: 0.00002040
Iteration 97/1000 | Loss: 0.00002039
Iteration 98/1000 | Loss: 0.00002039
Iteration 99/1000 | Loss: 0.00002039
Iteration 100/1000 | Loss: 0.00002039
Iteration 101/1000 | Loss: 0.00002039
Iteration 102/1000 | Loss: 0.00002039
Iteration 103/1000 | Loss: 0.00002039
Iteration 104/1000 | Loss: 0.00002039
Iteration 105/1000 | Loss: 0.00002039
Iteration 106/1000 | Loss: 0.00002039
Iteration 107/1000 | Loss: 0.00002038
Iteration 108/1000 | Loss: 0.00002038
Iteration 109/1000 | Loss: 0.00002038
Iteration 110/1000 | Loss: 0.00002038
Iteration 111/1000 | Loss: 0.00002037
Iteration 112/1000 | Loss: 0.00002037
Iteration 113/1000 | Loss: 0.00002037
Iteration 114/1000 | Loss: 0.00002037
Iteration 115/1000 | Loss: 0.00002037
Iteration 116/1000 | Loss: 0.00002037
Iteration 117/1000 | Loss: 0.00002037
Iteration 118/1000 | Loss: 0.00002037
Iteration 119/1000 | Loss: 0.00002037
Iteration 120/1000 | Loss: 0.00002036
Iteration 121/1000 | Loss: 0.00002036
Iteration 122/1000 | Loss: 0.00002036
Iteration 123/1000 | Loss: 0.00002036
Iteration 124/1000 | Loss: 0.00002036
Iteration 125/1000 | Loss: 0.00002036
Iteration 126/1000 | Loss: 0.00002036
Iteration 127/1000 | Loss: 0.00002035
Iteration 128/1000 | Loss: 0.00002035
Iteration 129/1000 | Loss: 0.00002035
Iteration 130/1000 | Loss: 0.00002035
Iteration 131/1000 | Loss: 0.00002034
Iteration 132/1000 | Loss: 0.00002034
Iteration 133/1000 | Loss: 0.00002034
Iteration 134/1000 | Loss: 0.00002034
Iteration 135/1000 | Loss: 0.00002034
Iteration 136/1000 | Loss: 0.00002034
Iteration 137/1000 | Loss: 0.00002034
Iteration 138/1000 | Loss: 0.00002034
Iteration 139/1000 | Loss: 0.00002034
Iteration 140/1000 | Loss: 0.00002033
Iteration 141/1000 | Loss: 0.00002033
Iteration 142/1000 | Loss: 0.00002033
Iteration 143/1000 | Loss: 0.00002033
Iteration 144/1000 | Loss: 0.00002033
Iteration 145/1000 | Loss: 0.00002033
Iteration 146/1000 | Loss: 0.00002033
Iteration 147/1000 | Loss: 0.00002033
Iteration 148/1000 | Loss: 0.00002033
Iteration 149/1000 | Loss: 0.00002033
Iteration 150/1000 | Loss: 0.00002033
Iteration 151/1000 | Loss: 0.00002033
Iteration 152/1000 | Loss: 0.00002032
Iteration 153/1000 | Loss: 0.00002032
Iteration 154/1000 | Loss: 0.00002032
Iteration 155/1000 | Loss: 0.00002032
Iteration 156/1000 | Loss: 0.00002032
Iteration 157/1000 | Loss: 0.00002032
Iteration 158/1000 | Loss: 0.00002032
Iteration 159/1000 | Loss: 0.00002032
Iteration 160/1000 | Loss: 0.00002032
Iteration 161/1000 | Loss: 0.00002032
Iteration 162/1000 | Loss: 0.00002031
Iteration 163/1000 | Loss: 0.00002031
Iteration 164/1000 | Loss: 0.00002031
Iteration 165/1000 | Loss: 0.00002031
Iteration 166/1000 | Loss: 0.00002031
Iteration 167/1000 | Loss: 0.00002031
Iteration 168/1000 | Loss: 0.00002031
Iteration 169/1000 | Loss: 0.00002031
Iteration 170/1000 | Loss: 0.00002031
Iteration 171/1000 | Loss: 0.00002031
Iteration 172/1000 | Loss: 0.00002031
Iteration 173/1000 | Loss: 0.00002031
Iteration 174/1000 | Loss: 0.00002031
Iteration 175/1000 | Loss: 0.00002031
Iteration 176/1000 | Loss: 0.00002030
Iteration 177/1000 | Loss: 0.00002030
Iteration 178/1000 | Loss: 0.00002030
Iteration 179/1000 | Loss: 0.00002030
Iteration 180/1000 | Loss: 0.00002030
Iteration 181/1000 | Loss: 0.00002030
Iteration 182/1000 | Loss: 0.00002030
Iteration 183/1000 | Loss: 0.00002029
Iteration 184/1000 | Loss: 0.00002029
Iteration 185/1000 | Loss: 0.00002029
Iteration 186/1000 | Loss: 0.00002029
Iteration 187/1000 | Loss: 0.00002029
Iteration 188/1000 | Loss: 0.00002029
Iteration 189/1000 | Loss: 0.00002029
Iteration 190/1000 | Loss: 0.00002029
Iteration 191/1000 | Loss: 0.00002029
Iteration 192/1000 | Loss: 0.00002029
Iteration 193/1000 | Loss: 0.00002029
Iteration 194/1000 | Loss: 0.00002029
Iteration 195/1000 | Loss: 0.00002029
Iteration 196/1000 | Loss: 0.00002029
Iteration 197/1000 | Loss: 0.00002029
Iteration 198/1000 | Loss: 0.00002029
Iteration 199/1000 | Loss: 0.00002029
Iteration 200/1000 | Loss: 0.00002029
Iteration 201/1000 | Loss: 0.00002029
Iteration 202/1000 | Loss: 0.00002029
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [2.0292149201850407e-05, 2.0292149201850407e-05, 2.0292149201850407e-05, 2.0292149201850407e-05, 2.0292149201850407e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0292149201850407e-05

Optimization complete. Final v2v error: 3.9166862964630127 mm

Highest mean error: 4.119913578033447 mm for frame 113

Lowest mean error: 3.5395140647888184 mm for frame 35

Saving results

Total time: 52.85037159919739
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_2081/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_2081/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_2081/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00838857
Iteration 2/25 | Loss: 0.00191037
Iteration 3/25 | Loss: 0.00133875
Iteration 4/25 | Loss: 0.00126588
Iteration 5/25 | Loss: 0.00126257
Iteration 6/25 | Loss: 0.00126213
Iteration 7/25 | Loss: 0.00126213
Iteration 8/25 | Loss: 0.00126213
Iteration 9/25 | Loss: 0.00126213
Iteration 10/25 | Loss: 0.00126213
Iteration 11/25 | Loss: 0.00126213
Iteration 12/25 | Loss: 0.00126213
Iteration 13/25 | Loss: 0.00126213
Iteration 14/25 | Loss: 0.00126213
Iteration 15/25 | Loss: 0.00126213
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012621323112398386, 0.0012621323112398386, 0.0012621323112398386, 0.0012621323112398386, 0.0012621323112398386]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012621323112398386

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24554491
Iteration 2/25 | Loss: 0.00136476
Iteration 3/25 | Loss: 0.00136475
Iteration 4/25 | Loss: 0.00136475
Iteration 5/25 | Loss: 0.00136475
Iteration 6/25 | Loss: 0.00136475
Iteration 7/25 | Loss: 0.00136475
Iteration 8/25 | Loss: 0.00136475
Iteration 9/25 | Loss: 0.00136475
Iteration 10/25 | Loss: 0.00136475
Iteration 11/25 | Loss: 0.00136475
Iteration 12/25 | Loss: 0.00136475
Iteration 13/25 | Loss: 0.00136475
Iteration 14/25 | Loss: 0.00136475
Iteration 15/25 | Loss: 0.00136475
Iteration 16/25 | Loss: 0.00136475
Iteration 17/25 | Loss: 0.00136475
Iteration 18/25 | Loss: 0.00136475
Iteration 19/25 | Loss: 0.00136475
Iteration 20/25 | Loss: 0.00136475
Iteration 21/25 | Loss: 0.00136475
Iteration 22/25 | Loss: 0.00136475
Iteration 23/25 | Loss: 0.00136475
Iteration 24/25 | Loss: 0.00136475
Iteration 25/25 | Loss: 0.00136475

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00136475
Iteration 2/1000 | Loss: 0.00004117
Iteration 3/1000 | Loss: 0.00002676
Iteration 4/1000 | Loss: 0.00002281
Iteration 5/1000 | Loss: 0.00002137
Iteration 6/1000 | Loss: 0.00002063
Iteration 7/1000 | Loss: 0.00002013
Iteration 8/1000 | Loss: 0.00001966
Iteration 9/1000 | Loss: 0.00001938
Iteration 10/1000 | Loss: 0.00001906
Iteration 11/1000 | Loss: 0.00001873
Iteration 12/1000 | Loss: 0.00001860
Iteration 13/1000 | Loss: 0.00001860
Iteration 14/1000 | Loss: 0.00001859
Iteration 15/1000 | Loss: 0.00001859
Iteration 16/1000 | Loss: 0.00001841
Iteration 17/1000 | Loss: 0.00001826
Iteration 18/1000 | Loss: 0.00001820
Iteration 19/1000 | Loss: 0.00001813
Iteration 20/1000 | Loss: 0.00001813
Iteration 21/1000 | Loss: 0.00001813
Iteration 22/1000 | Loss: 0.00001813
Iteration 23/1000 | Loss: 0.00001812
Iteration 24/1000 | Loss: 0.00001812
Iteration 25/1000 | Loss: 0.00001812
Iteration 26/1000 | Loss: 0.00001811
Iteration 27/1000 | Loss: 0.00001811
Iteration 28/1000 | Loss: 0.00001810
Iteration 29/1000 | Loss: 0.00001810
Iteration 30/1000 | Loss: 0.00001810
Iteration 31/1000 | Loss: 0.00001810
Iteration 32/1000 | Loss: 0.00001810
Iteration 33/1000 | Loss: 0.00001810
Iteration 34/1000 | Loss: 0.00001810
Iteration 35/1000 | Loss: 0.00001810
Iteration 36/1000 | Loss: 0.00001810
Iteration 37/1000 | Loss: 0.00001810
Iteration 38/1000 | Loss: 0.00001810
Iteration 39/1000 | Loss: 0.00001810
Iteration 40/1000 | Loss: 0.00001809
Iteration 41/1000 | Loss: 0.00001809
Iteration 42/1000 | Loss: 0.00001809
Iteration 43/1000 | Loss: 0.00001809
Iteration 44/1000 | Loss: 0.00001809
Iteration 45/1000 | Loss: 0.00001809
Iteration 46/1000 | Loss: 0.00001809
Iteration 47/1000 | Loss: 0.00001809
Iteration 48/1000 | Loss: 0.00001809
Iteration 49/1000 | Loss: 0.00001808
Iteration 50/1000 | Loss: 0.00001808
Iteration 51/1000 | Loss: 0.00001806
Iteration 52/1000 | Loss: 0.00001806
Iteration 53/1000 | Loss: 0.00001806
Iteration 54/1000 | Loss: 0.00001805
Iteration 55/1000 | Loss: 0.00001804
Iteration 56/1000 | Loss: 0.00001804
Iteration 57/1000 | Loss: 0.00001804
Iteration 58/1000 | Loss: 0.00001804
Iteration 59/1000 | Loss: 0.00001804
Iteration 60/1000 | Loss: 0.00001804
Iteration 61/1000 | Loss: 0.00001804
Iteration 62/1000 | Loss: 0.00001804
Iteration 63/1000 | Loss: 0.00001804
Iteration 64/1000 | Loss: 0.00001803
Iteration 65/1000 | Loss: 0.00001803
Iteration 66/1000 | Loss: 0.00001803
Iteration 67/1000 | Loss: 0.00001803
Iteration 68/1000 | Loss: 0.00001803
Iteration 69/1000 | Loss: 0.00001802
Iteration 70/1000 | Loss: 0.00001802
Iteration 71/1000 | Loss: 0.00001802
Iteration 72/1000 | Loss: 0.00001802
Iteration 73/1000 | Loss: 0.00001802
Iteration 74/1000 | Loss: 0.00001802
Iteration 75/1000 | Loss: 0.00001802
Iteration 76/1000 | Loss: 0.00001801
Iteration 77/1000 | Loss: 0.00001801
Iteration 78/1000 | Loss: 0.00001801
Iteration 79/1000 | Loss: 0.00001801
Iteration 80/1000 | Loss: 0.00001801
Iteration 81/1000 | Loss: 0.00001801
Iteration 82/1000 | Loss: 0.00001801
Iteration 83/1000 | Loss: 0.00001801
Iteration 84/1000 | Loss: 0.00001801
Iteration 85/1000 | Loss: 0.00001801
Iteration 86/1000 | Loss: 0.00001799
Iteration 87/1000 | Loss: 0.00001799
Iteration 88/1000 | Loss: 0.00001799
Iteration 89/1000 | Loss: 0.00001799
Iteration 90/1000 | Loss: 0.00001799
Iteration 91/1000 | Loss: 0.00001798
Iteration 92/1000 | Loss: 0.00001798
Iteration 93/1000 | Loss: 0.00001798
Iteration 94/1000 | Loss: 0.00001798
Iteration 95/1000 | Loss: 0.00001798
Iteration 96/1000 | Loss: 0.00001798
Iteration 97/1000 | Loss: 0.00001798
Iteration 98/1000 | Loss: 0.00001798
Iteration 99/1000 | Loss: 0.00001798
Iteration 100/1000 | Loss: 0.00001798
Iteration 101/1000 | Loss: 0.00001798
Iteration 102/1000 | Loss: 0.00001798
Iteration 103/1000 | Loss: 0.00001798
Iteration 104/1000 | Loss: 0.00001797
Iteration 105/1000 | Loss: 0.00001797
Iteration 106/1000 | Loss: 0.00001797
Iteration 107/1000 | Loss: 0.00001797
Iteration 108/1000 | Loss: 0.00001797
Iteration 109/1000 | Loss: 0.00001796
Iteration 110/1000 | Loss: 0.00001796
Iteration 111/1000 | Loss: 0.00001796
Iteration 112/1000 | Loss: 0.00001796
Iteration 113/1000 | Loss: 0.00001796
Iteration 114/1000 | Loss: 0.00001796
Iteration 115/1000 | Loss: 0.00001796
Iteration 116/1000 | Loss: 0.00001796
Iteration 117/1000 | Loss: 0.00001796
Iteration 118/1000 | Loss: 0.00001796
Iteration 119/1000 | Loss: 0.00001796
Iteration 120/1000 | Loss: 0.00001796
Iteration 121/1000 | Loss: 0.00001796
Iteration 122/1000 | Loss: 0.00001796
Iteration 123/1000 | Loss: 0.00001795
Iteration 124/1000 | Loss: 0.00001795
Iteration 125/1000 | Loss: 0.00001795
Iteration 126/1000 | Loss: 0.00001795
Iteration 127/1000 | Loss: 0.00001795
Iteration 128/1000 | Loss: 0.00001795
Iteration 129/1000 | Loss: 0.00001795
Iteration 130/1000 | Loss: 0.00001794
Iteration 131/1000 | Loss: 0.00001794
Iteration 132/1000 | Loss: 0.00001794
Iteration 133/1000 | Loss: 0.00001794
Iteration 134/1000 | Loss: 0.00001794
Iteration 135/1000 | Loss: 0.00001794
Iteration 136/1000 | Loss: 0.00001794
Iteration 137/1000 | Loss: 0.00001794
Iteration 138/1000 | Loss: 0.00001794
Iteration 139/1000 | Loss: 0.00001794
Iteration 140/1000 | Loss: 0.00001793
Iteration 141/1000 | Loss: 0.00001793
Iteration 142/1000 | Loss: 0.00001793
Iteration 143/1000 | Loss: 0.00001793
Iteration 144/1000 | Loss: 0.00001793
Iteration 145/1000 | Loss: 0.00001793
Iteration 146/1000 | Loss: 0.00001793
Iteration 147/1000 | Loss: 0.00001793
Iteration 148/1000 | Loss: 0.00001792
Iteration 149/1000 | Loss: 0.00001792
Iteration 150/1000 | Loss: 0.00001792
Iteration 151/1000 | Loss: 0.00001792
Iteration 152/1000 | Loss: 0.00001792
Iteration 153/1000 | Loss: 0.00001791
Iteration 154/1000 | Loss: 0.00001791
Iteration 155/1000 | Loss: 0.00001791
Iteration 156/1000 | Loss: 0.00001791
Iteration 157/1000 | Loss: 0.00001791
Iteration 158/1000 | Loss: 0.00001791
Iteration 159/1000 | Loss: 0.00001791
Iteration 160/1000 | Loss: 0.00001791
Iteration 161/1000 | Loss: 0.00001791
Iteration 162/1000 | Loss: 0.00001791
Iteration 163/1000 | Loss: 0.00001791
Iteration 164/1000 | Loss: 0.00001791
Iteration 165/1000 | Loss: 0.00001791
Iteration 166/1000 | Loss: 0.00001791
Iteration 167/1000 | Loss: 0.00001791
Iteration 168/1000 | Loss: 0.00001791
Iteration 169/1000 | Loss: 0.00001791
Iteration 170/1000 | Loss: 0.00001791
Iteration 171/1000 | Loss: 0.00001791
Iteration 172/1000 | Loss: 0.00001791
Iteration 173/1000 | Loss: 0.00001791
Iteration 174/1000 | Loss: 0.00001791
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [1.790568421711214e-05, 1.790568421711214e-05, 1.790568421711214e-05, 1.790568421711214e-05, 1.790568421711214e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.790568421711214e-05

Optimization complete. Final v2v error: 3.6219701766967773 mm

Highest mean error: 4.020394325256348 mm for frame 133

Lowest mean error: 3.235635995864868 mm for frame 112

Saving results

Total time: 39.668036222457886
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_2081/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_2081/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_2081/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01064157
Iteration 2/25 | Loss: 0.00194590
Iteration 3/25 | Loss: 0.00157058
Iteration 4/25 | Loss: 0.00148136
Iteration 5/25 | Loss: 0.00152301
Iteration 6/25 | Loss: 0.00149554
Iteration 7/25 | Loss: 0.00144653
Iteration 8/25 | Loss: 0.00141123
Iteration 9/25 | Loss: 0.00139565
Iteration 10/25 | Loss: 0.00138982
Iteration 11/25 | Loss: 0.00138840
Iteration 12/25 | Loss: 0.00139043
Iteration 13/25 | Loss: 0.00138898
Iteration 14/25 | Loss: 0.00138681
Iteration 15/25 | Loss: 0.00137997
Iteration 16/25 | Loss: 0.00138736
Iteration 17/25 | Loss: 0.00142358
Iteration 18/25 | Loss: 0.00143129
Iteration 19/25 | Loss: 0.00139385
Iteration 20/25 | Loss: 0.00138624
Iteration 21/25 | Loss: 0.00136836
Iteration 22/25 | Loss: 0.00135668
Iteration 23/25 | Loss: 0.00135716
Iteration 24/25 | Loss: 0.00135875
Iteration 25/25 | Loss: 0.00135091

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.83761764
Iteration 2/25 | Loss: 0.00254418
Iteration 3/25 | Loss: 0.00254417
Iteration 4/25 | Loss: 0.00254417
Iteration 5/25 | Loss: 0.00254417
Iteration 6/25 | Loss: 0.00254416
Iteration 7/25 | Loss: 0.00254416
Iteration 8/25 | Loss: 0.00254416
Iteration 9/25 | Loss: 0.00254416
Iteration 10/25 | Loss: 0.00254416
Iteration 11/25 | Loss: 0.00254416
Iteration 12/25 | Loss: 0.00254416
Iteration 13/25 | Loss: 0.00254416
Iteration 14/25 | Loss: 0.00254416
Iteration 15/25 | Loss: 0.00254416
Iteration 16/25 | Loss: 0.00254416
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.002544163726270199, 0.002544163726270199, 0.002544163726270199, 0.002544163726270199, 0.002544163726270199]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002544163726270199

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00254416
Iteration 2/1000 | Loss: 0.00048219
Iteration 3/1000 | Loss: 0.00131958
Iteration 4/1000 | Loss: 0.00113257
Iteration 5/1000 | Loss: 0.00132480
Iteration 6/1000 | Loss: 0.00125921
Iteration 7/1000 | Loss: 0.00126325
Iteration 8/1000 | Loss: 0.00133965
Iteration 9/1000 | Loss: 0.00138475
Iteration 10/1000 | Loss: 0.00121621
Iteration 11/1000 | Loss: 0.00139636
Iteration 12/1000 | Loss: 0.00129391
Iteration 13/1000 | Loss: 0.00122547
Iteration 14/1000 | Loss: 0.00119462
Iteration 15/1000 | Loss: 0.00113410
Iteration 16/1000 | Loss: 0.00291453
Iteration 17/1000 | Loss: 0.00133883
Iteration 18/1000 | Loss: 0.00241300
Iteration 19/1000 | Loss: 0.00313570
Iteration 20/1000 | Loss: 0.00124597
Iteration 21/1000 | Loss: 0.00116654
Iteration 22/1000 | Loss: 0.00107189
Iteration 23/1000 | Loss: 0.00085746
Iteration 24/1000 | Loss: 0.00118005
Iteration 25/1000 | Loss: 0.00095426
Iteration 26/1000 | Loss: 0.00075939
Iteration 27/1000 | Loss: 0.00071618
Iteration 28/1000 | Loss: 0.00062906
Iteration 29/1000 | Loss: 0.00085199
Iteration 30/1000 | Loss: 0.00101121
Iteration 31/1000 | Loss: 0.00161509
Iteration 32/1000 | Loss: 0.00134736
Iteration 33/1000 | Loss: 0.00123703
Iteration 34/1000 | Loss: 0.00090309
Iteration 35/1000 | Loss: 0.00096734
Iteration 36/1000 | Loss: 0.00068315
Iteration 37/1000 | Loss: 0.00083493
Iteration 38/1000 | Loss: 0.00118926
Iteration 39/1000 | Loss: 0.00160687
Iteration 40/1000 | Loss: 0.00141093
Iteration 41/1000 | Loss: 0.00120462
Iteration 42/1000 | Loss: 0.00152056
Iteration 43/1000 | Loss: 0.00093542
Iteration 44/1000 | Loss: 0.00118020
Iteration 45/1000 | Loss: 0.00110944
Iteration 46/1000 | Loss: 0.00124584
Iteration 47/1000 | Loss: 0.00113010
Iteration 48/1000 | Loss: 0.00099704
Iteration 49/1000 | Loss: 0.00103683
Iteration 50/1000 | Loss: 0.00110309
Iteration 51/1000 | Loss: 0.00115712
Iteration 52/1000 | Loss: 0.00105946
Iteration 53/1000 | Loss: 0.00118994
Iteration 54/1000 | Loss: 0.00141563
Iteration 55/1000 | Loss: 0.00126218
Iteration 56/1000 | Loss: 0.00079110
Iteration 57/1000 | Loss: 0.00168657
Iteration 58/1000 | Loss: 0.00122210
Iteration 59/1000 | Loss: 0.00111251
Iteration 60/1000 | Loss: 0.00090526
Iteration 61/1000 | Loss: 0.00100519
Iteration 62/1000 | Loss: 0.00092360
Iteration 63/1000 | Loss: 0.00119040
Iteration 64/1000 | Loss: 0.00101421
Iteration 65/1000 | Loss: 0.00111092
Iteration 66/1000 | Loss: 0.00098039
Iteration 67/1000 | Loss: 0.00087762
Iteration 68/1000 | Loss: 0.00089355
Iteration 69/1000 | Loss: 0.00141298
Iteration 70/1000 | Loss: 0.00191020
Iteration 71/1000 | Loss: 0.00093211
Iteration 72/1000 | Loss: 0.00101064
Iteration 73/1000 | Loss: 0.00098303
Iteration 74/1000 | Loss: 0.00134459
Iteration 75/1000 | Loss: 0.00115759
Iteration 76/1000 | Loss: 0.00074862
Iteration 77/1000 | Loss: 0.00060097
Iteration 78/1000 | Loss: 0.00089695
Iteration 79/1000 | Loss: 0.00093749
Iteration 80/1000 | Loss: 0.00106542
Iteration 81/1000 | Loss: 0.00106697
Iteration 82/1000 | Loss: 0.00157234
Iteration 83/1000 | Loss: 0.00113153
Iteration 84/1000 | Loss: 0.00112432
Iteration 85/1000 | Loss: 0.00095477
Iteration 86/1000 | Loss: 0.00087988
Iteration 87/1000 | Loss: 0.00089859
Iteration 88/1000 | Loss: 0.00104438
Iteration 89/1000 | Loss: 0.00114178
Iteration 90/1000 | Loss: 0.00125924
Iteration 91/1000 | Loss: 0.00120166
Iteration 92/1000 | Loss: 0.00157264
Iteration 93/1000 | Loss: 0.00090552
Iteration 94/1000 | Loss: 0.00086273
Iteration 95/1000 | Loss: 0.00177110
Iteration 96/1000 | Loss: 0.00325768
Iteration 97/1000 | Loss: 0.00262802
Iteration 98/1000 | Loss: 0.00273224
Iteration 99/1000 | Loss: 0.00106171
Iteration 100/1000 | Loss: 0.00094179
Iteration 101/1000 | Loss: 0.00103708
Iteration 102/1000 | Loss: 0.00459489
Iteration 103/1000 | Loss: 0.00199385
Iteration 104/1000 | Loss: 0.00111787
Iteration 105/1000 | Loss: 0.00099176
Iteration 106/1000 | Loss: 0.00321484
Iteration 107/1000 | Loss: 0.00119078
Iteration 108/1000 | Loss: 0.00073008
Iteration 109/1000 | Loss: 0.00107497
Iteration 110/1000 | Loss: 0.00084100
Iteration 111/1000 | Loss: 0.00101448
Iteration 112/1000 | Loss: 0.00106091
Iteration 113/1000 | Loss: 0.00119443
Iteration 114/1000 | Loss: 0.00117875
Iteration 115/1000 | Loss: 0.00114968
Iteration 116/1000 | Loss: 0.00116776
Iteration 117/1000 | Loss: 0.00175319
Iteration 118/1000 | Loss: 0.00114543
Iteration 119/1000 | Loss: 0.00114125
Iteration 120/1000 | Loss: 0.00105042
Iteration 121/1000 | Loss: 0.00094129
Iteration 122/1000 | Loss: 0.00082443
Iteration 123/1000 | Loss: 0.00086521
Iteration 124/1000 | Loss: 0.00105319
Iteration 125/1000 | Loss: 0.00132322
Iteration 126/1000 | Loss: 0.00103945
Iteration 127/1000 | Loss: 0.00096752
Iteration 128/1000 | Loss: 0.00114851
Iteration 129/1000 | Loss: 0.00063598
Iteration 130/1000 | Loss: 0.00087617
Iteration 131/1000 | Loss: 0.00078892
Iteration 132/1000 | Loss: 0.00075944
Iteration 133/1000 | Loss: 0.00071265
Iteration 134/1000 | Loss: 0.00060730
Iteration 135/1000 | Loss: 0.00080360
Iteration 136/1000 | Loss: 0.00119524
Iteration 137/1000 | Loss: 0.00070451
Iteration 138/1000 | Loss: 0.00066593
Iteration 139/1000 | Loss: 0.00080579
Iteration 140/1000 | Loss: 0.00077814
Iteration 141/1000 | Loss: 0.00067729
Iteration 142/1000 | Loss: 0.00040628
Iteration 143/1000 | Loss: 0.00070945
Iteration 144/1000 | Loss: 0.00060540
Iteration 145/1000 | Loss: 0.00071416
Iteration 146/1000 | Loss: 0.00075704
Iteration 147/1000 | Loss: 0.00085050
Iteration 148/1000 | Loss: 0.00083526
Iteration 149/1000 | Loss: 0.00082939
Iteration 150/1000 | Loss: 0.00090003
Iteration 151/1000 | Loss: 0.00132713
Iteration 152/1000 | Loss: 0.00118917
Iteration 153/1000 | Loss: 0.00080503
Iteration 154/1000 | Loss: 0.00077624
Iteration 155/1000 | Loss: 0.00060492
Iteration 156/1000 | Loss: 0.00054176
Iteration 157/1000 | Loss: 0.00076361
Iteration 158/1000 | Loss: 0.00102489
Iteration 159/1000 | Loss: 0.00087742
Iteration 160/1000 | Loss: 0.00098524
Iteration 161/1000 | Loss: 0.00096082
Iteration 162/1000 | Loss: 0.00085019
Iteration 163/1000 | Loss: 0.00110263
Iteration 164/1000 | Loss: 0.00089389
Iteration 165/1000 | Loss: 0.00053179
Iteration 166/1000 | Loss: 0.00065228
Iteration 167/1000 | Loss: 0.00056857
Iteration 168/1000 | Loss: 0.00065297
Iteration 169/1000 | Loss: 0.00080423
Iteration 170/1000 | Loss: 0.00079848
Iteration 171/1000 | Loss: 0.00083888
Iteration 172/1000 | Loss: 0.00061245
Iteration 173/1000 | Loss: 0.00052225
Iteration 174/1000 | Loss: 0.00060034
Iteration 175/1000 | Loss: 0.00053138
Iteration 176/1000 | Loss: 0.00049152
Iteration 177/1000 | Loss: 0.00103534
Iteration 178/1000 | Loss: 0.00072501
Iteration 179/1000 | Loss: 0.00070252
Iteration 180/1000 | Loss: 0.00064529
Iteration 181/1000 | Loss: 0.00050252
Iteration 182/1000 | Loss: 0.00037931
Iteration 183/1000 | Loss: 0.00047065
Iteration 184/1000 | Loss: 0.00039137
Iteration 185/1000 | Loss: 0.00046236
Iteration 186/1000 | Loss: 0.00059092
Iteration 187/1000 | Loss: 0.00071143
Iteration 188/1000 | Loss: 0.00075348
Iteration 189/1000 | Loss: 0.00069419
Iteration 190/1000 | Loss: 0.00066507
Iteration 191/1000 | Loss: 0.00065183
Iteration 192/1000 | Loss: 0.00066838
Iteration 193/1000 | Loss: 0.00072813
Iteration 194/1000 | Loss: 0.00075825
Iteration 195/1000 | Loss: 0.00089108
Iteration 196/1000 | Loss: 0.00075567
Iteration 197/1000 | Loss: 0.00065429
Iteration 198/1000 | Loss: 0.00072274
Iteration 199/1000 | Loss: 0.00056799
Iteration 200/1000 | Loss: 0.00050445
Iteration 201/1000 | Loss: 0.00071558
Iteration 202/1000 | Loss: 0.00057157
Iteration 203/1000 | Loss: 0.00047050
Iteration 204/1000 | Loss: 0.00058247
Iteration 205/1000 | Loss: 0.00079618
Iteration 206/1000 | Loss: 0.00072753
Iteration 207/1000 | Loss: 0.00080092
Iteration 208/1000 | Loss: 0.00098865
Iteration 209/1000 | Loss: 0.00070531
Iteration 210/1000 | Loss: 0.00076586
Iteration 211/1000 | Loss: 0.00070147
Iteration 212/1000 | Loss: 0.00070780
Iteration 213/1000 | Loss: 0.00074768
Iteration 214/1000 | Loss: 0.00082473
Iteration 215/1000 | Loss: 0.00079757
Iteration 216/1000 | Loss: 0.00085045
Iteration 217/1000 | Loss: 0.00078716
Iteration 218/1000 | Loss: 0.00071085
Iteration 219/1000 | Loss: 0.00121513
Iteration 220/1000 | Loss: 0.00182278
Iteration 221/1000 | Loss: 0.00068407
Iteration 222/1000 | Loss: 0.00074109
Iteration 223/1000 | Loss: 0.00060301
Iteration 224/1000 | Loss: 0.00066635
Iteration 225/1000 | Loss: 0.00072629
Iteration 226/1000 | Loss: 0.00064671
Iteration 227/1000 | Loss: 0.00028501
Iteration 228/1000 | Loss: 0.00050820
Iteration 229/1000 | Loss: 0.00061340
Iteration 230/1000 | Loss: 0.00046845
Iteration 231/1000 | Loss: 0.00046620
Iteration 232/1000 | Loss: 0.00046152
Iteration 233/1000 | Loss: 0.00065114
Iteration 234/1000 | Loss: 0.00065727
Iteration 235/1000 | Loss: 0.00072526
Iteration 236/1000 | Loss: 0.00071884
Iteration 237/1000 | Loss: 0.00069738
Iteration 238/1000 | Loss: 0.00065408
Iteration 239/1000 | Loss: 0.00054223
Iteration 240/1000 | Loss: 0.00054277
Iteration 241/1000 | Loss: 0.00066926
Iteration 242/1000 | Loss: 0.00076536
Iteration 243/1000 | Loss: 0.00049599
Iteration 244/1000 | Loss: 0.00110335
Iteration 245/1000 | Loss: 0.00064855
Iteration 246/1000 | Loss: 0.00066773
Iteration 247/1000 | Loss: 0.00071820
Iteration 248/1000 | Loss: 0.00078319
Iteration 249/1000 | Loss: 0.00054332
Iteration 250/1000 | Loss: 0.00062159
Iteration 251/1000 | Loss: 0.00056498
Iteration 252/1000 | Loss: 0.00059581
Iteration 253/1000 | Loss: 0.00055943
Iteration 254/1000 | Loss: 0.00062398
Iteration 255/1000 | Loss: 0.00076912
Iteration 256/1000 | Loss: 0.00160297
Iteration 257/1000 | Loss: 0.00061831
Iteration 258/1000 | Loss: 0.00103147
Iteration 259/1000 | Loss: 0.00057680
Iteration 260/1000 | Loss: 0.00051102
Iteration 261/1000 | Loss: 0.00068629
Iteration 262/1000 | Loss: 0.00077407
Iteration 263/1000 | Loss: 0.00058427
Iteration 264/1000 | Loss: 0.00052261
Iteration 265/1000 | Loss: 0.00058717
Iteration 266/1000 | Loss: 0.00063813
Iteration 267/1000 | Loss: 0.00065984
Iteration 268/1000 | Loss: 0.00060179
Iteration 269/1000 | Loss: 0.00091679
Iteration 270/1000 | Loss: 0.00037621
Iteration 271/1000 | Loss: 0.00075350
Iteration 272/1000 | Loss: 0.00061622
Iteration 273/1000 | Loss: 0.00063135
Iteration 274/1000 | Loss: 0.00067452
Iteration 275/1000 | Loss: 0.00058764
Iteration 276/1000 | Loss: 0.00045560
Iteration 277/1000 | Loss: 0.00055015
Iteration 278/1000 | Loss: 0.00073595
Iteration 279/1000 | Loss: 0.00072258
Iteration 280/1000 | Loss: 0.00070676
Iteration 281/1000 | Loss: 0.00061115
Iteration 282/1000 | Loss: 0.00054437
Iteration 283/1000 | Loss: 0.00050566
Iteration 284/1000 | Loss: 0.00068212
Iteration 285/1000 | Loss: 0.00084714
Iteration 286/1000 | Loss: 0.00064760
Iteration 287/1000 | Loss: 0.00054672
Iteration 288/1000 | Loss: 0.00055564
Iteration 289/1000 | Loss: 0.00047174
Iteration 290/1000 | Loss: 0.00045403
Iteration 291/1000 | Loss: 0.00077860
Iteration 292/1000 | Loss: 0.00076321
Iteration 293/1000 | Loss: 0.00091366
Iteration 294/1000 | Loss: 0.00058048
Iteration 295/1000 | Loss: 0.00064926
Iteration 296/1000 | Loss: 0.00063315
Iteration 297/1000 | Loss: 0.00063143
Iteration 298/1000 | Loss: 0.00065874
Iteration 299/1000 | Loss: 0.00062270
Iteration 300/1000 | Loss: 0.00065082
Iteration 301/1000 | Loss: 0.00079550
Iteration 302/1000 | Loss: 0.00071728
Iteration 303/1000 | Loss: 0.00052629
Iteration 304/1000 | Loss: 0.00042560
Iteration 305/1000 | Loss: 0.00064885
Iteration 306/1000 | Loss: 0.00069052
Iteration 307/1000 | Loss: 0.00080987
Iteration 308/1000 | Loss: 0.00094923
Iteration 309/1000 | Loss: 0.00204270
Iteration 310/1000 | Loss: 0.00094903
Iteration 311/1000 | Loss: 0.00135944
Iteration 312/1000 | Loss: 0.00066676
Iteration 313/1000 | Loss: 0.00041691
Iteration 314/1000 | Loss: 0.00117927
Iteration 315/1000 | Loss: 0.00046553
Iteration 316/1000 | Loss: 0.00041210
Iteration 317/1000 | Loss: 0.00043669
Iteration 318/1000 | Loss: 0.00081491
Iteration 319/1000 | Loss: 0.00053872
Iteration 320/1000 | Loss: 0.00050956
Iteration 321/1000 | Loss: 0.00152878
Iteration 322/1000 | Loss: 0.00041371
Iteration 323/1000 | Loss: 0.00040783
Iteration 324/1000 | Loss: 0.00049141
Iteration 325/1000 | Loss: 0.00089710
Iteration 326/1000 | Loss: 0.00096492
Iteration 327/1000 | Loss: 0.00024103
Iteration 328/1000 | Loss: 0.00023107
Iteration 329/1000 | Loss: 0.00039705
Iteration 330/1000 | Loss: 0.00024000
Iteration 331/1000 | Loss: 0.00035202
Iteration 332/1000 | Loss: 0.00026436
Iteration 333/1000 | Loss: 0.00037813
Iteration 334/1000 | Loss: 0.00022209
Iteration 335/1000 | Loss: 0.00095751
Iteration 336/1000 | Loss: 0.00034492
Iteration 337/1000 | Loss: 0.00060577
Iteration 338/1000 | Loss: 0.00077665
Iteration 339/1000 | Loss: 0.00060928
Iteration 340/1000 | Loss: 0.00067563
Iteration 341/1000 | Loss: 0.00010964
Iteration 342/1000 | Loss: 0.00028028
Iteration 343/1000 | Loss: 0.00030855
Iteration 344/1000 | Loss: 0.00033657
Iteration 345/1000 | Loss: 0.00027659
Iteration 346/1000 | Loss: 0.00034992
Iteration 347/1000 | Loss: 0.00020963
Iteration 348/1000 | Loss: 0.00021777
Iteration 349/1000 | Loss: 0.00025670
Iteration 350/1000 | Loss: 0.00026146
Iteration 351/1000 | Loss: 0.00031471
Iteration 352/1000 | Loss: 0.00027100
Iteration 353/1000 | Loss: 0.00025147
Iteration 354/1000 | Loss: 0.00024070
Iteration 355/1000 | Loss: 0.00057397
Iteration 356/1000 | Loss: 0.00029195
Iteration 357/1000 | Loss: 0.00051656
Iteration 358/1000 | Loss: 0.00045971
Iteration 359/1000 | Loss: 0.00025242
Iteration 360/1000 | Loss: 0.00049399
Iteration 361/1000 | Loss: 0.00041424
Iteration 362/1000 | Loss: 0.00030503
Iteration 363/1000 | Loss: 0.00077003
Iteration 364/1000 | Loss: 0.00041939
Iteration 365/1000 | Loss: 0.00063774
Iteration 366/1000 | Loss: 0.00034608
Iteration 367/1000 | Loss: 0.00040653
Iteration 368/1000 | Loss: 0.00021373
Iteration 369/1000 | Loss: 0.00044264
Iteration 370/1000 | Loss: 0.00030188
Iteration 371/1000 | Loss: 0.00020754
Iteration 372/1000 | Loss: 0.00015619
Iteration 373/1000 | Loss: 0.00028938
Iteration 374/1000 | Loss: 0.00015128
Iteration 375/1000 | Loss: 0.00018442
Iteration 376/1000 | Loss: 0.00062887
Iteration 377/1000 | Loss: 0.00042046
Iteration 378/1000 | Loss: 0.00041489
Iteration 379/1000 | Loss: 0.00037305
Iteration 380/1000 | Loss: 0.00032465
Iteration 381/1000 | Loss: 0.00024061
Iteration 382/1000 | Loss: 0.00015102
Iteration 383/1000 | Loss: 0.00018423
Iteration 384/1000 | Loss: 0.00025720
Iteration 385/1000 | Loss: 0.00024262
Iteration 386/1000 | Loss: 0.00025021
Iteration 387/1000 | Loss: 0.00024798
Iteration 388/1000 | Loss: 0.00027386
Iteration 389/1000 | Loss: 0.00018123
Iteration 390/1000 | Loss: 0.00029198
Iteration 391/1000 | Loss: 0.00028972
Iteration 392/1000 | Loss: 0.00021606
Iteration 393/1000 | Loss: 0.00025209
Iteration 394/1000 | Loss: 0.00019657
Iteration 395/1000 | Loss: 0.00016855
Iteration 396/1000 | Loss: 0.00015836
Iteration 397/1000 | Loss: 0.00015666
Iteration 398/1000 | Loss: 0.00021883
Iteration 399/1000 | Loss: 0.00028148
Iteration 400/1000 | Loss: 0.00014337
Iteration 401/1000 | Loss: 0.00029781
Iteration 402/1000 | Loss: 0.00035325
Iteration 403/1000 | Loss: 0.00009261
Iteration 404/1000 | Loss: 0.00014750
Iteration 405/1000 | Loss: 0.00027761
Iteration 406/1000 | Loss: 0.00025912
Iteration 407/1000 | Loss: 0.00023286
Iteration 408/1000 | Loss: 0.00023857
Iteration 409/1000 | Loss: 0.00023508
Iteration 410/1000 | Loss: 0.00022763
Iteration 411/1000 | Loss: 0.00027353
Iteration 412/1000 | Loss: 0.00031437
Iteration 413/1000 | Loss: 0.00027313
Iteration 414/1000 | Loss: 0.00038072
Iteration 415/1000 | Loss: 0.00042303
Iteration 416/1000 | Loss: 0.00032107
Iteration 417/1000 | Loss: 0.00023165
Iteration 418/1000 | Loss: 0.00024202
Iteration 419/1000 | Loss: 0.00029625
Iteration 420/1000 | Loss: 0.00053681
Iteration 421/1000 | Loss: 0.00033167
Iteration 422/1000 | Loss: 0.00023384
Iteration 423/1000 | Loss: 0.00019392
Iteration 424/1000 | Loss: 0.00026724
Iteration 425/1000 | Loss: 0.00026781
Iteration 426/1000 | Loss: 0.00028363
Iteration 427/1000 | Loss: 0.00012974
Iteration 428/1000 | Loss: 0.00018900
Iteration 429/1000 | Loss: 0.00011890
Iteration 430/1000 | Loss: 0.00016204
Iteration 431/1000 | Loss: 0.00019774
Iteration 432/1000 | Loss: 0.00021528
Iteration 433/1000 | Loss: 0.00018624
Iteration 434/1000 | Loss: 0.00028186
Iteration 435/1000 | Loss: 0.00020668
Iteration 436/1000 | Loss: 0.00023164
Iteration 437/1000 | Loss: 0.00166881
Iteration 438/1000 | Loss: 0.00079783
Iteration 439/1000 | Loss: 0.00042441
Iteration 440/1000 | Loss: 0.00041106
Iteration 441/1000 | Loss: 0.00040607
Iteration 442/1000 | Loss: 0.00036004
Iteration 443/1000 | Loss: 0.00037520
Iteration 444/1000 | Loss: 0.00034482
Iteration 445/1000 | Loss: 0.00040759
Iteration 446/1000 | Loss: 0.00038913
Iteration 447/1000 | Loss: 0.00035955
Iteration 448/1000 | Loss: 0.00026517
Iteration 449/1000 | Loss: 0.00032952
Iteration 450/1000 | Loss: 0.00026360
Iteration 451/1000 | Loss: 0.00170053
Iteration 452/1000 | Loss: 0.00043669
Iteration 453/1000 | Loss: 0.00015878
Iteration 454/1000 | Loss: 0.00015946
Iteration 455/1000 | Loss: 0.00021052
Iteration 456/1000 | Loss: 0.00025569
Iteration 457/1000 | Loss: 0.00030529
Iteration 458/1000 | Loss: 0.00034839
Iteration 459/1000 | Loss: 0.00029256
Iteration 460/1000 | Loss: 0.00029070
Iteration 461/1000 | Loss: 0.00037711
Iteration 462/1000 | Loss: 0.00034700
Iteration 463/1000 | Loss: 0.00031868
Iteration 464/1000 | Loss: 0.00026768
Iteration 465/1000 | Loss: 0.00027594
Iteration 466/1000 | Loss: 0.00022869
Iteration 467/1000 | Loss: 0.00022060
Iteration 468/1000 | Loss: 0.00027512
Iteration 469/1000 | Loss: 0.00023276
Iteration 470/1000 | Loss: 0.00031315
Iteration 471/1000 | Loss: 0.00024693
Iteration 472/1000 | Loss: 0.00030299
Iteration 473/1000 | Loss: 0.00024722
Iteration 474/1000 | Loss: 0.00022006
Iteration 475/1000 | Loss: 0.00020821
Iteration 476/1000 | Loss: 0.00019984
Iteration 477/1000 | Loss: 0.00029127
Iteration 478/1000 | Loss: 0.00033422
Iteration 479/1000 | Loss: 0.00063800
Iteration 480/1000 | Loss: 0.00068935
Iteration 481/1000 | Loss: 0.00041078
Iteration 482/1000 | Loss: 0.00044342
Iteration 483/1000 | Loss: 0.00165837
Iteration 484/1000 | Loss: 0.00096998
Iteration 485/1000 | Loss: 0.00022624
Iteration 486/1000 | Loss: 0.00031767
Iteration 487/1000 | Loss: 0.00026637
Iteration 488/1000 | Loss: 0.00018008
Iteration 489/1000 | Loss: 0.00034789
Iteration 490/1000 | Loss: 0.00018087
Iteration 491/1000 | Loss: 0.00083592
Iteration 492/1000 | Loss: 0.00028348
Iteration 493/1000 | Loss: 0.00039002
Iteration 494/1000 | Loss: 0.00024150
Iteration 495/1000 | Loss: 0.00021218
Iteration 496/1000 | Loss: 0.00019969
Iteration 497/1000 | Loss: 0.00022309
Iteration 498/1000 | Loss: 0.00027120
Iteration 499/1000 | Loss: 0.00026791
Iteration 500/1000 | Loss: 0.00034735
Iteration 501/1000 | Loss: 0.00039123
Iteration 502/1000 | Loss: 0.00027399
Iteration 503/1000 | Loss: 0.00025452
Iteration 504/1000 | Loss: 0.00025374
Iteration 505/1000 | Loss: 0.00026785
Iteration 506/1000 | Loss: 0.00048095
Iteration 507/1000 | Loss: 0.00027327
Iteration 508/1000 | Loss: 0.00017149
Iteration 509/1000 | Loss: 0.00027349
Iteration 510/1000 | Loss: 0.00016961
Iteration 511/1000 | Loss: 0.00023516
Iteration 512/1000 | Loss: 0.00021976
Iteration 513/1000 | Loss: 0.00024142
Iteration 514/1000 | Loss: 0.00021556
Iteration 515/1000 | Loss: 0.00020782
Iteration 516/1000 | Loss: 0.00027346
Iteration 517/1000 | Loss: 0.00023122
Iteration 518/1000 | Loss: 0.00019342
Iteration 519/1000 | Loss: 0.00028041
Iteration 520/1000 | Loss: 0.00026449
Iteration 521/1000 | Loss: 0.00016691
Iteration 522/1000 | Loss: 0.00011947
Iteration 523/1000 | Loss: 0.00016148
Iteration 524/1000 | Loss: 0.00015533
Iteration 525/1000 | Loss: 0.00021175
Iteration 526/1000 | Loss: 0.00027961
Iteration 527/1000 | Loss: 0.00009870
Iteration 528/1000 | Loss: 0.00015771
Iteration 529/1000 | Loss: 0.00021936
Iteration 530/1000 | Loss: 0.00017276
Iteration 531/1000 | Loss: 0.00020585
Iteration 532/1000 | Loss: 0.00019887
Iteration 533/1000 | Loss: 0.00018705
Iteration 534/1000 | Loss: 0.00011729
Iteration 535/1000 | Loss: 0.00017791
Iteration 536/1000 | Loss: 0.00020642
Iteration 537/1000 | Loss: 0.00024782
Iteration 538/1000 | Loss: 0.00015297
Iteration 539/1000 | Loss: 0.00014075
Iteration 540/1000 | Loss: 0.00020989
Iteration 541/1000 | Loss: 0.00026207
Iteration 542/1000 | Loss: 0.00017618
Iteration 543/1000 | Loss: 0.00016027
Iteration 544/1000 | Loss: 0.00033245
Iteration 545/1000 | Loss: 0.00029534
Iteration 546/1000 | Loss: 0.00040307
Iteration 547/1000 | Loss: 0.00020338
Iteration 548/1000 | Loss: 0.00010966
Iteration 549/1000 | Loss: 0.00013900
Iteration 550/1000 | Loss: 0.00017087
Iteration 551/1000 | Loss: 0.00011420
Iteration 552/1000 | Loss: 0.00022557
Iteration 553/1000 | Loss: 0.00028804
Iteration 554/1000 | Loss: 0.00015221
Iteration 555/1000 | Loss: 0.00015564
Iteration 556/1000 | Loss: 0.00020237
Iteration 557/1000 | Loss: 0.00034998
Iteration 558/1000 | Loss: 0.00021765
Iteration 559/1000 | Loss: 0.00043722
Iteration 560/1000 | Loss: 0.00027830
Iteration 561/1000 | Loss: 0.00014533
Iteration 562/1000 | Loss: 0.00027344
Iteration 563/1000 | Loss: 0.00035078
Iteration 564/1000 | Loss: 0.00018616
Iteration 565/1000 | Loss: 0.00006962
Iteration 566/1000 | Loss: 0.00016021
Iteration 567/1000 | Loss: 0.00014987
Iteration 568/1000 | Loss: 0.00010794
Iteration 569/1000 | Loss: 0.00082605
Iteration 570/1000 | Loss: 0.00013448
Iteration 571/1000 | Loss: 0.00014292
Iteration 572/1000 | Loss: 0.00014832
Iteration 573/1000 | Loss: 0.00013833
Iteration 574/1000 | Loss: 0.00012258
Iteration 575/1000 | Loss: 0.00073047
Iteration 576/1000 | Loss: 0.00012514
Iteration 577/1000 | Loss: 0.00013877
Iteration 578/1000 | Loss: 0.00013253
Iteration 579/1000 | Loss: 0.00009797
Iteration 580/1000 | Loss: 0.00008887
Iteration 581/1000 | Loss: 0.00012299
Iteration 582/1000 | Loss: 0.00012493
Iteration 583/1000 | Loss: 0.00007804
Iteration 584/1000 | Loss: 0.00009455
Iteration 585/1000 | Loss: 0.00010181
Iteration 586/1000 | Loss: 0.00010576
Iteration 587/1000 | Loss: 0.00013505
Iteration 588/1000 | Loss: 0.00011413
Iteration 589/1000 | Loss: 0.00005200
Iteration 590/1000 | Loss: 0.00010193
Iteration 591/1000 | Loss: 0.00009031
Iteration 592/1000 | Loss: 0.00011942
Iteration 593/1000 | Loss: 0.00010819
Iteration 594/1000 | Loss: 0.00011262
Iteration 595/1000 | Loss: 0.00014546
Iteration 596/1000 | Loss: 0.00014234
Iteration 597/1000 | Loss: 0.00012795
Iteration 598/1000 | Loss: 0.00014128
Iteration 599/1000 | Loss: 0.00011905
Iteration 600/1000 | Loss: 0.00014049
Iteration 601/1000 | Loss: 0.00012432
Iteration 602/1000 | Loss: 0.00010059
Iteration 603/1000 | Loss: 0.00004576
Iteration 604/1000 | Loss: 0.00014391
Iteration 605/1000 | Loss: 0.00010215
Iteration 606/1000 | Loss: 0.00010522
Iteration 607/1000 | Loss: 0.00013022
Iteration 608/1000 | Loss: 0.00012699
Iteration 609/1000 | Loss: 0.00008395
Iteration 610/1000 | Loss: 0.00010305
Iteration 611/1000 | Loss: 0.00011255
Iteration 612/1000 | Loss: 0.00012764
Iteration 613/1000 | Loss: 0.00008765
Iteration 614/1000 | Loss: 0.00016161
Iteration 615/1000 | Loss: 0.00008935
Iteration 616/1000 | Loss: 0.00012642
Iteration 617/1000 | Loss: 0.00015624
Iteration 618/1000 | Loss: 0.00018953
Iteration 619/1000 | Loss: 0.00017045
Iteration 620/1000 | Loss: 0.00006664
Iteration 621/1000 | Loss: 0.00009630
Iteration 622/1000 | Loss: 0.00013484
Iteration 623/1000 | Loss: 0.00010819
Iteration 624/1000 | Loss: 0.00013401
Iteration 625/1000 | Loss: 0.00031764
Iteration 626/1000 | Loss: 0.00007305
Iteration 627/1000 | Loss: 0.00012949
Iteration 628/1000 | Loss: 0.00014347
Iteration 629/1000 | Loss: 0.00017174
Iteration 630/1000 | Loss: 0.00006721
Iteration 631/1000 | Loss: 0.00009640
Iteration 632/1000 | Loss: 0.00012929
Iteration 633/1000 | Loss: 0.00013730
Iteration 634/1000 | Loss: 0.00018217
Iteration 635/1000 | Loss: 0.00017752
Iteration 636/1000 | Loss: 0.00027078
Iteration 637/1000 | Loss: 0.00026814
Iteration 638/1000 | Loss: 0.00017103
Iteration 639/1000 | Loss: 0.00010096
Iteration 640/1000 | Loss: 0.00009590
Iteration 641/1000 | Loss: 0.00005212
Iteration 642/1000 | Loss: 0.00004904
Iteration 643/1000 | Loss: 0.00005631
Iteration 644/1000 | Loss: 0.00006434
Iteration 645/1000 | Loss: 0.00008485
Iteration 646/1000 | Loss: 0.00010035
Iteration 647/1000 | Loss: 0.00014244
Iteration 648/1000 | Loss: 0.00013700
Iteration 649/1000 | Loss: 0.00011568
Iteration 650/1000 | Loss: 0.00014306
Iteration 651/1000 | Loss: 0.00011682
Iteration 652/1000 | Loss: 0.00012939
Iteration 653/1000 | Loss: 0.00037859
Iteration 654/1000 | Loss: 0.00012420
Iteration 655/1000 | Loss: 0.00010527
Iteration 656/1000 | Loss: 0.00013813
Iteration 657/1000 | Loss: 0.00009186
Iteration 658/1000 | Loss: 0.00005492
Iteration 659/1000 | Loss: 0.00047784
Iteration 660/1000 | Loss: 0.00024176
Iteration 661/1000 | Loss: 0.00057459
Iteration 662/1000 | Loss: 0.00018815
Iteration 663/1000 | Loss: 0.00092856
Iteration 664/1000 | Loss: 0.00008869
Iteration 665/1000 | Loss: 0.00030334
Iteration 666/1000 | Loss: 0.00012480
Iteration 667/1000 | Loss: 0.00007485
Iteration 668/1000 | Loss: 0.00051759
Iteration 669/1000 | Loss: 0.00019048
Iteration 670/1000 | Loss: 0.00009469
Iteration 671/1000 | Loss: 0.00017012
Iteration 672/1000 | Loss: 0.00011111
Iteration 673/1000 | Loss: 0.00016464
Iteration 674/1000 | Loss: 0.00011901
Iteration 675/1000 | Loss: 0.00015895
Iteration 676/1000 | Loss: 0.00031445
Iteration 677/1000 | Loss: 0.00011709
Iteration 678/1000 | Loss: 0.00012419
Iteration 679/1000 | Loss: 0.00014190
Iteration 680/1000 | Loss: 0.00013926
Iteration 681/1000 | Loss: 0.00014173
Iteration 682/1000 | Loss: 0.00013322
Iteration 683/1000 | Loss: 0.00033180
Iteration 684/1000 | Loss: 0.00013922
Iteration 685/1000 | Loss: 0.00014164
Iteration 686/1000 | Loss: 0.00010986
Iteration 687/1000 | Loss: 0.00011460
Iteration 688/1000 | Loss: 0.00013964
Iteration 689/1000 | Loss: 0.00011297
Iteration 690/1000 | Loss: 0.00014679
Iteration 691/1000 | Loss: 0.00006044
Iteration 692/1000 | Loss: 0.00007916
Iteration 693/1000 | Loss: 0.00005286
Iteration 694/1000 | Loss: 0.00006866
Iteration 695/1000 | Loss: 0.00014373
Iteration 696/1000 | Loss: 0.00005521
Iteration 697/1000 | Loss: 0.00006980
Iteration 698/1000 | Loss: 0.00003870
Iteration 699/1000 | Loss: 0.00012319
Iteration 700/1000 | Loss: 0.00010682
Iteration 701/1000 | Loss: 0.00062415
Iteration 702/1000 | Loss: 0.00012947
Iteration 703/1000 | Loss: 0.00004003
Iteration 704/1000 | Loss: 0.00004857
Iteration 705/1000 | Loss: 0.00004434
Iteration 706/1000 | Loss: 0.00004422
Iteration 707/1000 | Loss: 0.00004809
Iteration 708/1000 | Loss: 0.00004605
Iteration 709/1000 | Loss: 0.00005408
Iteration 710/1000 | Loss: 0.00004805
Iteration 711/1000 | Loss: 0.00004746
Iteration 712/1000 | Loss: 0.00004261
Iteration 713/1000 | Loss: 0.00004822
Iteration 714/1000 | Loss: 0.00004266
Iteration 715/1000 | Loss: 0.00005029
Iteration 716/1000 | Loss: 0.00004944
Iteration 717/1000 | Loss: 0.00005800
Iteration 718/1000 | Loss: 0.00096960
Iteration 719/1000 | Loss: 0.00010614
Iteration 720/1000 | Loss: 0.00004654
Iteration 721/1000 | Loss: 0.00005067
Iteration 722/1000 | Loss: 0.00006242
Iteration 723/1000 | Loss: 0.00006766
Iteration 724/1000 | Loss: 0.00003795
Iteration 725/1000 | Loss: 0.00004463
Iteration 726/1000 | Loss: 0.00005374
Iteration 727/1000 | Loss: 0.00005715
Iteration 728/1000 | Loss: 0.00005894
Iteration 729/1000 | Loss: 0.00004677
Iteration 730/1000 | Loss: 0.00004935
Iteration 731/1000 | Loss: 0.00004751
Iteration 732/1000 | Loss: 0.00004765
Iteration 733/1000 | Loss: 0.00006402
Iteration 734/1000 | Loss: 0.00006055
Iteration 735/1000 | Loss: 0.00004627
Iteration 736/1000 | Loss: 0.00004912
Iteration 737/1000 | Loss: 0.00004760
Iteration 738/1000 | Loss: 0.00004863
Iteration 739/1000 | Loss: 0.00004571
Iteration 740/1000 | Loss: 0.00004771
Iteration 741/1000 | Loss: 0.00004551
Iteration 742/1000 | Loss: 0.00011412
Iteration 743/1000 | Loss: 0.00005272
Iteration 744/1000 | Loss: 0.00009505
Iteration 745/1000 | Loss: 0.00005461
Iteration 746/1000 | Loss: 0.00008894
Iteration 747/1000 | Loss: 0.00005686
Iteration 748/1000 | Loss: 0.00004719
Iteration 749/1000 | Loss: 0.00006770
Iteration 750/1000 | Loss: 0.00005614
Iteration 751/1000 | Loss: 0.00006752
Iteration 752/1000 | Loss: 0.00005627
Iteration 753/1000 | Loss: 0.00004615
Iteration 754/1000 | Loss: 0.00004845
Iteration 755/1000 | Loss: 0.00004644
Iteration 756/1000 | Loss: 0.00004700
Iteration 757/1000 | Loss: 0.00004537
Iteration 758/1000 | Loss: 0.00009676
Iteration 759/1000 | Loss: 0.00005964
Iteration 760/1000 | Loss: 0.00006629
Iteration 761/1000 | Loss: 0.00006485
Iteration 762/1000 | Loss: 0.00003714
Iteration 763/1000 | Loss: 0.00006663
Iteration 764/1000 | Loss: 0.00004274
Iteration 765/1000 | Loss: 0.00003398
Iteration 766/1000 | Loss: 0.00003323
Iteration 767/1000 | Loss: 0.00003287
Iteration 768/1000 | Loss: 0.00003255
Iteration 769/1000 | Loss: 0.00008286
Iteration 770/1000 | Loss: 0.00016006
Iteration 771/1000 | Loss: 0.00008024
Iteration 772/1000 | Loss: 0.00006920
Iteration 773/1000 | Loss: 0.00015500
Iteration 774/1000 | Loss: 0.00003307
Iteration 775/1000 | Loss: 0.00003097
Iteration 776/1000 | Loss: 0.00003041
Iteration 777/1000 | Loss: 0.00002998
Iteration 778/1000 | Loss: 0.00002997
Iteration 779/1000 | Loss: 0.00002988
Iteration 780/1000 | Loss: 0.00004652
Iteration 781/1000 | Loss: 0.00003110
Iteration 782/1000 | Loss: 0.00003031
Iteration 783/1000 | Loss: 0.00002981
Iteration 784/1000 | Loss: 0.00002950
Iteration 785/1000 | Loss: 0.00002939
Iteration 786/1000 | Loss: 0.00002935
Iteration 787/1000 | Loss: 0.00002935
Iteration 788/1000 | Loss: 0.00002933
Iteration 789/1000 | Loss: 0.00002931
Iteration 790/1000 | Loss: 0.00002931
Iteration 791/1000 | Loss: 0.00002930
Iteration 792/1000 | Loss: 0.00002930
Iteration 793/1000 | Loss: 0.00002929
Iteration 794/1000 | Loss: 0.00002931
Iteration 795/1000 | Loss: 0.00002931
Iteration 796/1000 | Loss: 0.00002931
Iteration 797/1000 | Loss: 0.00002930
Iteration 798/1000 | Loss: 0.00002930
Iteration 799/1000 | Loss: 0.00002930
Iteration 800/1000 | Loss: 0.00002929
Iteration 801/1000 | Loss: 0.00002928
Iteration 802/1000 | Loss: 0.00002925
Iteration 803/1000 | Loss: 0.00002925
Iteration 804/1000 | Loss: 0.00002923
Iteration 805/1000 | Loss: 0.00002923
Iteration 806/1000 | Loss: 0.00002922
Iteration 807/1000 | Loss: 0.00002922
Iteration 808/1000 | Loss: 0.00002922
Iteration 809/1000 | Loss: 0.00002922
Iteration 810/1000 | Loss: 0.00002922
Iteration 811/1000 | Loss: 0.00002922
Iteration 812/1000 | Loss: 0.00002922
Iteration 813/1000 | Loss: 0.00002921
Iteration 814/1000 | Loss: 0.00002921
Iteration 815/1000 | Loss: 0.00002921
Iteration 816/1000 | Loss: 0.00002921
Iteration 817/1000 | Loss: 0.00002921
Iteration 818/1000 | Loss: 0.00002921
Iteration 819/1000 | Loss: 0.00002949
Iteration 820/1000 | Loss: 0.00002949
Iteration 821/1000 | Loss: 0.00002949
Iteration 822/1000 | Loss: 0.00002948
Iteration 823/1000 | Loss: 0.00002948
Iteration 824/1000 | Loss: 0.00002940
Iteration 825/1000 | Loss: 0.00002925
Iteration 826/1000 | Loss: 0.00002925
Iteration 827/1000 | Loss: 0.00002919
Iteration 828/1000 | Loss: 0.00002919
Iteration 829/1000 | Loss: 0.00002919
Iteration 830/1000 | Loss: 0.00002919
Iteration 831/1000 | Loss: 0.00002919
Iteration 832/1000 | Loss: 0.00002919
Iteration 833/1000 | Loss: 0.00002919
Iteration 834/1000 | Loss: 0.00002919
Iteration 835/1000 | Loss: 0.00002918
Iteration 836/1000 | Loss: 0.00002918
Iteration 837/1000 | Loss: 0.00002918
Iteration 838/1000 | Loss: 0.00002918
Iteration 839/1000 | Loss: 0.00002918
Iteration 840/1000 | Loss: 0.00002918
Iteration 841/1000 | Loss: 0.00002918
Iteration 842/1000 | Loss: 0.00002917
Iteration 843/1000 | Loss: 0.00002917
Iteration 844/1000 | Loss: 0.00002917
Iteration 845/1000 | Loss: 0.00002917
Iteration 846/1000 | Loss: 0.00002916
Iteration 847/1000 | Loss: 0.00002916
Iteration 848/1000 | Loss: 0.00002916
Iteration 849/1000 | Loss: 0.00002916
Iteration 850/1000 | Loss: 0.00002916
Iteration 851/1000 | Loss: 0.00002915
Iteration 852/1000 | Loss: 0.00002915
Iteration 853/1000 | Loss: 0.00002915
Iteration 854/1000 | Loss: 0.00002915
Iteration 855/1000 | Loss: 0.00002915
Iteration 856/1000 | Loss: 0.00002915
Iteration 857/1000 | Loss: 0.00002915
Iteration 858/1000 | Loss: 0.00002915
Iteration 859/1000 | Loss: 0.00002915
Iteration 860/1000 | Loss: 0.00002915
Iteration 861/1000 | Loss: 0.00002915
Iteration 862/1000 | Loss: 0.00002915
Iteration 863/1000 | Loss: 0.00002915
Iteration 864/1000 | Loss: 0.00002915
Iteration 865/1000 | Loss: 0.00002914
Iteration 866/1000 | Loss: 0.00002914
Iteration 867/1000 | Loss: 0.00002914
Iteration 868/1000 | Loss: 0.00002914
Iteration 869/1000 | Loss: 0.00002914
Iteration 870/1000 | Loss: 0.00002914
Iteration 871/1000 | Loss: 0.00002914
Iteration 872/1000 | Loss: 0.00002914
Iteration 873/1000 | Loss: 0.00002914
Iteration 874/1000 | Loss: 0.00002914
Iteration 875/1000 | Loss: 0.00002914
Iteration 876/1000 | Loss: 0.00002914
Iteration 877/1000 | Loss: 0.00002914
Iteration 878/1000 | Loss: 0.00002914
Iteration 879/1000 | Loss: 0.00002913
Iteration 880/1000 | Loss: 0.00002913
Iteration 881/1000 | Loss: 0.00002913
Iteration 882/1000 | Loss: 0.00002913
Iteration 883/1000 | Loss: 0.00002913
Iteration 884/1000 | Loss: 0.00002913
Iteration 885/1000 | Loss: 0.00002913
Iteration 886/1000 | Loss: 0.00002913
Iteration 887/1000 | Loss: 0.00002913
Iteration 888/1000 | Loss: 0.00002913
Iteration 889/1000 | Loss: 0.00002913
Iteration 890/1000 | Loss: 0.00002913
Iteration 891/1000 | Loss: 0.00002912
Iteration 892/1000 | Loss: 0.00002912
Iteration 893/1000 | Loss: 0.00002912
Iteration 894/1000 | Loss: 0.00002912
Iteration 895/1000 | Loss: 0.00002912
Iteration 896/1000 | Loss: 0.00002912
Iteration 897/1000 | Loss: 0.00002912
Iteration 898/1000 | Loss: 0.00002912
Iteration 899/1000 | Loss: 0.00002912
Iteration 900/1000 | Loss: 0.00002912
Iteration 901/1000 | Loss: 0.00002912
Iteration 902/1000 | Loss: 0.00002912
Iteration 903/1000 | Loss: 0.00002912
Iteration 904/1000 | Loss: 0.00002912
Iteration 905/1000 | Loss: 0.00002912
Iteration 906/1000 | Loss: 0.00002911
Iteration 907/1000 | Loss: 0.00002911
Iteration 908/1000 | Loss: 0.00002911
Iteration 909/1000 | Loss: 0.00002911
Iteration 910/1000 | Loss: 0.00002911
Iteration 911/1000 | Loss: 0.00002911
Iteration 912/1000 | Loss: 0.00002911
Iteration 913/1000 | Loss: 0.00002911
Iteration 914/1000 | Loss: 0.00002910
Iteration 915/1000 | Loss: 0.00002910
Iteration 916/1000 | Loss: 0.00002910
Iteration 917/1000 | Loss: 0.00002910
Iteration 918/1000 | Loss: 0.00002910
Iteration 919/1000 | Loss: 0.00002910
Iteration 920/1000 | Loss: 0.00002910
Iteration 921/1000 | Loss: 0.00002910
Iteration 922/1000 | Loss: 0.00002910
Iteration 923/1000 | Loss: 0.00002910
Iteration 924/1000 | Loss: 0.00002909
Iteration 925/1000 | Loss: 0.00002909
Iteration 926/1000 | Loss: 0.00002909
Iteration 927/1000 | Loss: 0.00002909
Iteration 928/1000 | Loss: 0.00002908
Iteration 929/1000 | Loss: 0.00002908
Iteration 930/1000 | Loss: 0.00002908
Iteration 931/1000 | Loss: 0.00002908
Iteration 932/1000 | Loss: 0.00002908
Iteration 933/1000 | Loss: 0.00002908
Iteration 934/1000 | Loss: 0.00002907
Iteration 935/1000 | Loss: 0.00002907
Iteration 936/1000 | Loss: 0.00002907
Iteration 937/1000 | Loss: 0.00002907
Iteration 938/1000 | Loss: 0.00002907
Iteration 939/1000 | Loss: 0.00002907
Iteration 940/1000 | Loss: 0.00002907
Iteration 941/1000 | Loss: 0.00002907
Iteration 942/1000 | Loss: 0.00002907
Iteration 943/1000 | Loss: 0.00002906
Iteration 944/1000 | Loss: 0.00002906
Iteration 945/1000 | Loss: 0.00002906
Iteration 946/1000 | Loss: 0.00002906
Iteration 947/1000 | Loss: 0.00002906
Iteration 948/1000 | Loss: 0.00002906
Iteration 949/1000 | Loss: 0.00002906
Iteration 950/1000 | Loss: 0.00002906
Iteration 951/1000 | Loss: 0.00002906
Iteration 952/1000 | Loss: 0.00002906
Iteration 953/1000 | Loss: 0.00002906
Iteration 954/1000 | Loss: 0.00002906
Iteration 955/1000 | Loss: 0.00002906
Iteration 956/1000 | Loss: 0.00002906
Iteration 957/1000 | Loss: 0.00002906
Iteration 958/1000 | Loss: 0.00002906
Iteration 959/1000 | Loss: 0.00002906
Iteration 960/1000 | Loss: 0.00002906
Iteration 961/1000 | Loss: 0.00002906
Iteration 962/1000 | Loss: 0.00002906
Iteration 963/1000 | Loss: 0.00002906
Iteration 964/1000 | Loss: 0.00002906
Iteration 965/1000 | Loss: 0.00002906
Iteration 966/1000 | Loss: 0.00002906
Iteration 967/1000 | Loss: 0.00002906
Iteration 968/1000 | Loss: 0.00002906
Iteration 969/1000 | Loss: 0.00002906
Iteration 970/1000 | Loss: 0.00002906
Iteration 971/1000 | Loss: 0.00002906
Iteration 972/1000 | Loss: 0.00002906
Iteration 973/1000 | Loss: 0.00002906
Iteration 974/1000 | Loss: 0.00002906
Iteration 975/1000 | Loss: 0.00002906
Iteration 976/1000 | Loss: 0.00002906
Iteration 977/1000 | Loss: 0.00002906
Iteration 978/1000 | Loss: 0.00002906
Iteration 979/1000 | Loss: 0.00002906
Iteration 980/1000 | Loss: 0.00002906
Iteration 981/1000 | Loss: 0.00002906
Iteration 982/1000 | Loss: 0.00002906
Iteration 983/1000 | Loss: 0.00002906
Iteration 984/1000 | Loss: 0.00002905
Iteration 985/1000 | Loss: 0.00002905
Iteration 986/1000 | Loss: 0.00002905
Iteration 987/1000 | Loss: 0.00002905
Iteration 988/1000 | Loss: 0.00002905
Iteration 989/1000 | Loss: 0.00002905
Iteration 990/1000 | Loss: 0.00002905
Iteration 991/1000 | Loss: 0.00002905
Iteration 992/1000 | Loss: 0.00002905
Iteration 993/1000 | Loss: 0.00002905
Iteration 994/1000 | Loss: 0.00002905
Iteration 995/1000 | Loss: 0.00002905
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 995. Stopping optimization.
Last 5 losses: [2.905466317315586e-05, 2.905466317315586e-05, 2.905466317315586e-05, 2.905466317315586e-05, 2.905466317315586e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.905466317315586e-05

Optimization complete. Final v2v error: 4.340786933898926 mm

Highest mean error: 20.53550910949707 mm for frame 190

Lowest mean error: 3.795530319213867 mm for frame 1

Saving results

Total time: 1329.5517570972443
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_2081/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_2081/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_2081/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00488737
Iteration 2/25 | Loss: 0.00135160
Iteration 3/25 | Loss: 0.00115593
Iteration 4/25 | Loss: 0.00113535
Iteration 5/25 | Loss: 0.00113311
Iteration 6/25 | Loss: 0.00113259
Iteration 7/25 | Loss: 0.00113259
Iteration 8/25 | Loss: 0.00113259
Iteration 9/25 | Loss: 0.00113259
Iteration 10/25 | Loss: 0.00113259
Iteration 11/25 | Loss: 0.00113259
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011325916275382042, 0.0011325916275382042, 0.0011325916275382042, 0.0011325916275382042, 0.0011325916275382042]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011325916275382042

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25141644
Iteration 2/25 | Loss: 0.00136777
Iteration 3/25 | Loss: 0.00136776
Iteration 4/25 | Loss: 0.00136776
Iteration 5/25 | Loss: 0.00136776
Iteration 6/25 | Loss: 0.00136776
Iteration 7/25 | Loss: 0.00136776
Iteration 8/25 | Loss: 0.00136776
Iteration 9/25 | Loss: 0.00136776
Iteration 10/25 | Loss: 0.00136776
Iteration 11/25 | Loss: 0.00136776
Iteration 12/25 | Loss: 0.00136776
Iteration 13/25 | Loss: 0.00136776
Iteration 14/25 | Loss: 0.00136776
Iteration 15/25 | Loss: 0.00136776
Iteration 16/25 | Loss: 0.00136776
Iteration 17/25 | Loss: 0.00136776
Iteration 18/25 | Loss: 0.00136776
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0013677600072696805, 0.0013677600072696805, 0.0013677600072696805, 0.0013677600072696805, 0.0013677600072696805]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013677600072696805

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00136776
Iteration 2/1000 | Loss: 0.00003678
Iteration 3/1000 | Loss: 0.00002449
Iteration 4/1000 | Loss: 0.00002031
Iteration 5/1000 | Loss: 0.00001812
Iteration 6/1000 | Loss: 0.00001720
Iteration 7/1000 | Loss: 0.00001672
Iteration 8/1000 | Loss: 0.00001630
Iteration 9/1000 | Loss: 0.00001600
Iteration 10/1000 | Loss: 0.00001578
Iteration 11/1000 | Loss: 0.00001567
Iteration 12/1000 | Loss: 0.00001565
Iteration 13/1000 | Loss: 0.00001564
Iteration 14/1000 | Loss: 0.00001563
Iteration 15/1000 | Loss: 0.00001563
Iteration 16/1000 | Loss: 0.00001563
Iteration 17/1000 | Loss: 0.00001563
Iteration 18/1000 | Loss: 0.00001563
Iteration 19/1000 | Loss: 0.00001563
Iteration 20/1000 | Loss: 0.00001563
Iteration 21/1000 | Loss: 0.00001563
Iteration 22/1000 | Loss: 0.00001563
Iteration 23/1000 | Loss: 0.00001563
Iteration 24/1000 | Loss: 0.00001563
Iteration 25/1000 | Loss: 0.00001562
Iteration 26/1000 | Loss: 0.00001562
Iteration 27/1000 | Loss: 0.00001559
Iteration 28/1000 | Loss: 0.00001559
Iteration 29/1000 | Loss: 0.00001558
Iteration 30/1000 | Loss: 0.00001558
Iteration 31/1000 | Loss: 0.00001557
Iteration 32/1000 | Loss: 0.00001557
Iteration 33/1000 | Loss: 0.00001556
Iteration 34/1000 | Loss: 0.00001555
Iteration 35/1000 | Loss: 0.00001555
Iteration 36/1000 | Loss: 0.00001553
Iteration 37/1000 | Loss: 0.00001553
Iteration 38/1000 | Loss: 0.00001553
Iteration 39/1000 | Loss: 0.00001552
Iteration 40/1000 | Loss: 0.00001552
Iteration 41/1000 | Loss: 0.00001551
Iteration 42/1000 | Loss: 0.00001551
Iteration 43/1000 | Loss: 0.00001550
Iteration 44/1000 | Loss: 0.00001549
Iteration 45/1000 | Loss: 0.00001549
Iteration 46/1000 | Loss: 0.00001549
Iteration 47/1000 | Loss: 0.00001549
Iteration 48/1000 | Loss: 0.00001548
Iteration 49/1000 | Loss: 0.00001548
Iteration 50/1000 | Loss: 0.00001548
Iteration 51/1000 | Loss: 0.00001548
Iteration 52/1000 | Loss: 0.00001548
Iteration 53/1000 | Loss: 0.00001548
Iteration 54/1000 | Loss: 0.00001548
Iteration 55/1000 | Loss: 0.00001548
Iteration 56/1000 | Loss: 0.00001548
Iteration 57/1000 | Loss: 0.00001548
Iteration 58/1000 | Loss: 0.00001548
Iteration 59/1000 | Loss: 0.00001548
Iteration 60/1000 | Loss: 0.00001546
Iteration 61/1000 | Loss: 0.00001546
Iteration 62/1000 | Loss: 0.00001545
Iteration 63/1000 | Loss: 0.00001545
Iteration 64/1000 | Loss: 0.00001545
Iteration 65/1000 | Loss: 0.00001545
Iteration 66/1000 | Loss: 0.00001544
Iteration 67/1000 | Loss: 0.00001544
Iteration 68/1000 | Loss: 0.00001544
Iteration 69/1000 | Loss: 0.00001543
Iteration 70/1000 | Loss: 0.00001543
Iteration 71/1000 | Loss: 0.00001542
Iteration 72/1000 | Loss: 0.00001542
Iteration 73/1000 | Loss: 0.00001540
Iteration 74/1000 | Loss: 0.00001540
Iteration 75/1000 | Loss: 0.00001540
Iteration 76/1000 | Loss: 0.00001540
Iteration 77/1000 | Loss: 0.00001540
Iteration 78/1000 | Loss: 0.00001540
Iteration 79/1000 | Loss: 0.00001540
Iteration 80/1000 | Loss: 0.00001540
Iteration 81/1000 | Loss: 0.00001540
Iteration 82/1000 | Loss: 0.00001539
Iteration 83/1000 | Loss: 0.00001539
Iteration 84/1000 | Loss: 0.00001539
Iteration 85/1000 | Loss: 0.00001539
Iteration 86/1000 | Loss: 0.00001539
Iteration 87/1000 | Loss: 0.00001538
Iteration 88/1000 | Loss: 0.00001538
Iteration 89/1000 | Loss: 0.00001538
Iteration 90/1000 | Loss: 0.00001538
Iteration 91/1000 | Loss: 0.00001537
Iteration 92/1000 | Loss: 0.00001537
Iteration 93/1000 | Loss: 0.00001537
Iteration 94/1000 | Loss: 0.00001537
Iteration 95/1000 | Loss: 0.00001536
Iteration 96/1000 | Loss: 0.00001536
Iteration 97/1000 | Loss: 0.00001536
Iteration 98/1000 | Loss: 0.00001536
Iteration 99/1000 | Loss: 0.00001536
Iteration 100/1000 | Loss: 0.00001536
Iteration 101/1000 | Loss: 0.00001536
Iteration 102/1000 | Loss: 0.00001536
Iteration 103/1000 | Loss: 0.00001536
Iteration 104/1000 | Loss: 0.00001535
Iteration 105/1000 | Loss: 0.00001535
Iteration 106/1000 | Loss: 0.00001535
Iteration 107/1000 | Loss: 0.00001535
Iteration 108/1000 | Loss: 0.00001535
Iteration 109/1000 | Loss: 0.00001534
Iteration 110/1000 | Loss: 0.00001534
Iteration 111/1000 | Loss: 0.00001534
Iteration 112/1000 | Loss: 0.00001534
Iteration 113/1000 | Loss: 0.00001534
Iteration 114/1000 | Loss: 0.00001533
Iteration 115/1000 | Loss: 0.00001533
Iteration 116/1000 | Loss: 0.00001533
Iteration 117/1000 | Loss: 0.00001533
Iteration 118/1000 | Loss: 0.00001533
Iteration 119/1000 | Loss: 0.00001532
Iteration 120/1000 | Loss: 0.00001532
Iteration 121/1000 | Loss: 0.00001532
Iteration 122/1000 | Loss: 0.00001532
Iteration 123/1000 | Loss: 0.00001531
Iteration 124/1000 | Loss: 0.00001531
Iteration 125/1000 | Loss: 0.00001530
Iteration 126/1000 | Loss: 0.00001530
Iteration 127/1000 | Loss: 0.00001530
Iteration 128/1000 | Loss: 0.00001530
Iteration 129/1000 | Loss: 0.00001529
Iteration 130/1000 | Loss: 0.00001529
Iteration 131/1000 | Loss: 0.00001529
Iteration 132/1000 | Loss: 0.00001529
Iteration 133/1000 | Loss: 0.00001529
Iteration 134/1000 | Loss: 0.00001529
Iteration 135/1000 | Loss: 0.00001528
Iteration 136/1000 | Loss: 0.00001528
Iteration 137/1000 | Loss: 0.00001528
Iteration 138/1000 | Loss: 0.00001528
Iteration 139/1000 | Loss: 0.00001528
Iteration 140/1000 | Loss: 0.00001528
Iteration 141/1000 | Loss: 0.00001527
Iteration 142/1000 | Loss: 0.00001527
Iteration 143/1000 | Loss: 0.00001527
Iteration 144/1000 | Loss: 0.00001527
Iteration 145/1000 | Loss: 0.00001527
Iteration 146/1000 | Loss: 0.00001526
Iteration 147/1000 | Loss: 0.00001526
Iteration 148/1000 | Loss: 0.00001526
Iteration 149/1000 | Loss: 0.00001526
Iteration 150/1000 | Loss: 0.00001525
Iteration 151/1000 | Loss: 0.00001525
Iteration 152/1000 | Loss: 0.00001525
Iteration 153/1000 | Loss: 0.00001525
Iteration 154/1000 | Loss: 0.00001524
Iteration 155/1000 | Loss: 0.00001524
Iteration 156/1000 | Loss: 0.00001524
Iteration 157/1000 | Loss: 0.00001524
Iteration 158/1000 | Loss: 0.00001524
Iteration 159/1000 | Loss: 0.00001524
Iteration 160/1000 | Loss: 0.00001524
Iteration 161/1000 | Loss: 0.00001524
Iteration 162/1000 | Loss: 0.00001524
Iteration 163/1000 | Loss: 0.00001524
Iteration 164/1000 | Loss: 0.00001523
Iteration 165/1000 | Loss: 0.00001523
Iteration 166/1000 | Loss: 0.00001523
Iteration 167/1000 | Loss: 0.00001523
Iteration 168/1000 | Loss: 0.00001523
Iteration 169/1000 | Loss: 0.00001523
Iteration 170/1000 | Loss: 0.00001523
Iteration 171/1000 | Loss: 0.00001522
Iteration 172/1000 | Loss: 0.00001522
Iteration 173/1000 | Loss: 0.00001522
Iteration 174/1000 | Loss: 0.00001522
Iteration 175/1000 | Loss: 0.00001522
Iteration 176/1000 | Loss: 0.00001521
Iteration 177/1000 | Loss: 0.00001521
Iteration 178/1000 | Loss: 0.00001521
Iteration 179/1000 | Loss: 0.00001521
Iteration 180/1000 | Loss: 0.00001521
Iteration 181/1000 | Loss: 0.00001521
Iteration 182/1000 | Loss: 0.00001521
Iteration 183/1000 | Loss: 0.00001521
Iteration 184/1000 | Loss: 0.00001520
Iteration 185/1000 | Loss: 0.00001520
Iteration 186/1000 | Loss: 0.00001520
Iteration 187/1000 | Loss: 0.00001520
Iteration 188/1000 | Loss: 0.00001520
Iteration 189/1000 | Loss: 0.00001520
Iteration 190/1000 | Loss: 0.00001520
Iteration 191/1000 | Loss: 0.00001520
Iteration 192/1000 | Loss: 0.00001520
Iteration 193/1000 | Loss: 0.00001520
Iteration 194/1000 | Loss: 0.00001520
Iteration 195/1000 | Loss: 0.00001520
Iteration 196/1000 | Loss: 0.00001519
Iteration 197/1000 | Loss: 0.00001519
Iteration 198/1000 | Loss: 0.00001519
Iteration 199/1000 | Loss: 0.00001519
Iteration 200/1000 | Loss: 0.00001519
Iteration 201/1000 | Loss: 0.00001519
Iteration 202/1000 | Loss: 0.00001519
Iteration 203/1000 | Loss: 0.00001519
Iteration 204/1000 | Loss: 0.00001519
Iteration 205/1000 | Loss: 0.00001519
Iteration 206/1000 | Loss: 0.00001519
Iteration 207/1000 | Loss: 0.00001519
Iteration 208/1000 | Loss: 0.00001519
Iteration 209/1000 | Loss: 0.00001519
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [1.519386023574043e-05, 1.519386023574043e-05, 1.519386023574043e-05, 1.519386023574043e-05, 1.519386023574043e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.519386023574043e-05

Optimization complete. Final v2v error: 3.1878585815429688 mm

Highest mean error: 4.212808609008789 mm for frame 85

Lowest mean error: 2.759920835494995 mm for frame 130

Saving results

Total time: 38.44881296157837
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_36_us_2081/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_2081/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_36_us_2081/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00401900
Iteration 2/25 | Loss: 0.00120041
Iteration 3/25 | Loss: 0.00112213
Iteration 4/25 | Loss: 0.00111470
Iteration 5/25 | Loss: 0.00111179
Iteration 6/25 | Loss: 0.00111152
Iteration 7/25 | Loss: 0.00111152
Iteration 8/25 | Loss: 0.00111152
Iteration 9/25 | Loss: 0.00111152
Iteration 10/25 | Loss: 0.00111152
Iteration 11/25 | Loss: 0.00111152
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001111519173718989, 0.001111519173718989, 0.001111519173718989, 0.001111519173718989, 0.001111519173718989]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001111519173718989

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.86236143
Iteration 2/25 | Loss: 0.00157606
Iteration 3/25 | Loss: 0.00157606
Iteration 4/25 | Loss: 0.00157606
Iteration 5/25 | Loss: 0.00157606
Iteration 6/25 | Loss: 0.00157606
Iteration 7/25 | Loss: 0.00157606
Iteration 8/25 | Loss: 0.00157606
Iteration 9/25 | Loss: 0.00157606
Iteration 10/25 | Loss: 0.00157606
Iteration 11/25 | Loss: 0.00157606
Iteration 12/25 | Loss: 0.00157606
Iteration 13/25 | Loss: 0.00157606
Iteration 14/25 | Loss: 0.00157606
Iteration 15/25 | Loss: 0.00157606
Iteration 16/25 | Loss: 0.00157606
Iteration 17/25 | Loss: 0.00157606
Iteration 18/25 | Loss: 0.00157606
Iteration 19/25 | Loss: 0.00157606
Iteration 20/25 | Loss: 0.00157606
Iteration 21/25 | Loss: 0.00157606
Iteration 22/25 | Loss: 0.00157606
Iteration 23/25 | Loss: 0.00157606
Iteration 24/25 | Loss: 0.00157606
Iteration 25/25 | Loss: 0.00157606

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00157606
Iteration 2/1000 | Loss: 0.00002746
Iteration 3/1000 | Loss: 0.00001761
Iteration 4/1000 | Loss: 0.00001563
Iteration 5/1000 | Loss: 0.00001481
Iteration 6/1000 | Loss: 0.00001471
Iteration 7/1000 | Loss: 0.00001414
Iteration 8/1000 | Loss: 0.00001380
Iteration 9/1000 | Loss: 0.00001361
Iteration 10/1000 | Loss: 0.00001361
Iteration 11/1000 | Loss: 0.00001353
Iteration 12/1000 | Loss: 0.00001352
Iteration 13/1000 | Loss: 0.00001351
Iteration 14/1000 | Loss: 0.00001351
Iteration 15/1000 | Loss: 0.00001350
Iteration 16/1000 | Loss: 0.00001350
Iteration 17/1000 | Loss: 0.00001350
Iteration 18/1000 | Loss: 0.00001349
Iteration 19/1000 | Loss: 0.00001349
Iteration 20/1000 | Loss: 0.00001349
Iteration 21/1000 | Loss: 0.00001349
Iteration 22/1000 | Loss: 0.00001349
Iteration 23/1000 | Loss: 0.00001348
Iteration 24/1000 | Loss: 0.00001348
Iteration 25/1000 | Loss: 0.00001347
Iteration 26/1000 | Loss: 0.00001346
Iteration 27/1000 | Loss: 0.00001346
Iteration 28/1000 | Loss: 0.00001346
Iteration 29/1000 | Loss: 0.00001346
Iteration 30/1000 | Loss: 0.00001346
Iteration 31/1000 | Loss: 0.00001346
Iteration 32/1000 | Loss: 0.00001345
Iteration 33/1000 | Loss: 0.00001345
Iteration 34/1000 | Loss: 0.00001345
Iteration 35/1000 | Loss: 0.00001345
Iteration 36/1000 | Loss: 0.00001345
Iteration 37/1000 | Loss: 0.00001344
Iteration 38/1000 | Loss: 0.00001344
Iteration 39/1000 | Loss: 0.00001343
Iteration 40/1000 | Loss: 0.00001343
Iteration 41/1000 | Loss: 0.00001342
Iteration 42/1000 | Loss: 0.00001342
Iteration 43/1000 | Loss: 0.00001342
Iteration 44/1000 | Loss: 0.00001342
Iteration 45/1000 | Loss: 0.00001342
Iteration 46/1000 | Loss: 0.00001342
Iteration 47/1000 | Loss: 0.00001342
Iteration 48/1000 | Loss: 0.00001342
Iteration 49/1000 | Loss: 0.00001342
Iteration 50/1000 | Loss: 0.00001342
Iteration 51/1000 | Loss: 0.00001341
Iteration 52/1000 | Loss: 0.00001341
Iteration 53/1000 | Loss: 0.00001338
Iteration 54/1000 | Loss: 0.00001337
Iteration 55/1000 | Loss: 0.00001337
Iteration 56/1000 | Loss: 0.00001337
Iteration 57/1000 | Loss: 0.00001337
Iteration 58/1000 | Loss: 0.00001337
Iteration 59/1000 | Loss: 0.00001337
Iteration 60/1000 | Loss: 0.00001337
Iteration 61/1000 | Loss: 0.00001337
Iteration 62/1000 | Loss: 0.00001337
Iteration 63/1000 | Loss: 0.00001337
Iteration 64/1000 | Loss: 0.00001337
Iteration 65/1000 | Loss: 0.00001337
Iteration 66/1000 | Loss: 0.00001336
Iteration 67/1000 | Loss: 0.00001336
Iteration 68/1000 | Loss: 0.00001335
Iteration 69/1000 | Loss: 0.00001333
Iteration 70/1000 | Loss: 0.00001333
Iteration 71/1000 | Loss: 0.00001333
Iteration 72/1000 | Loss: 0.00001333
Iteration 73/1000 | Loss: 0.00001333
Iteration 74/1000 | Loss: 0.00001333
Iteration 75/1000 | Loss: 0.00001332
Iteration 76/1000 | Loss: 0.00001332
Iteration 77/1000 | Loss: 0.00001332
Iteration 78/1000 | Loss: 0.00001331
Iteration 79/1000 | Loss: 0.00001331
Iteration 80/1000 | Loss: 0.00001330
Iteration 81/1000 | Loss: 0.00001330
Iteration 82/1000 | Loss: 0.00001330
Iteration 83/1000 | Loss: 0.00001330
Iteration 84/1000 | Loss: 0.00001329
Iteration 85/1000 | Loss: 0.00001329
Iteration 86/1000 | Loss: 0.00001329
Iteration 87/1000 | Loss: 0.00001329
Iteration 88/1000 | Loss: 0.00001329
Iteration 89/1000 | Loss: 0.00001329
Iteration 90/1000 | Loss: 0.00001328
Iteration 91/1000 | Loss: 0.00001328
Iteration 92/1000 | Loss: 0.00001328
Iteration 93/1000 | Loss: 0.00001328
Iteration 94/1000 | Loss: 0.00001328
Iteration 95/1000 | Loss: 0.00001328
Iteration 96/1000 | Loss: 0.00001327
Iteration 97/1000 | Loss: 0.00001327
Iteration 98/1000 | Loss: 0.00001327
Iteration 99/1000 | Loss: 0.00001327
Iteration 100/1000 | Loss: 0.00001327
Iteration 101/1000 | Loss: 0.00001327
Iteration 102/1000 | Loss: 0.00001327
Iteration 103/1000 | Loss: 0.00001326
Iteration 104/1000 | Loss: 0.00001326
Iteration 105/1000 | Loss: 0.00001326
Iteration 106/1000 | Loss: 0.00001326
Iteration 107/1000 | Loss: 0.00001326
Iteration 108/1000 | Loss: 0.00001326
Iteration 109/1000 | Loss: 0.00001326
Iteration 110/1000 | Loss: 0.00001326
Iteration 111/1000 | Loss: 0.00001326
Iteration 112/1000 | Loss: 0.00001326
Iteration 113/1000 | Loss: 0.00001326
Iteration 114/1000 | Loss: 0.00001326
Iteration 115/1000 | Loss: 0.00001326
Iteration 116/1000 | Loss: 0.00001326
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [1.3263950677355751e-05, 1.3263950677355751e-05, 1.3263950677355751e-05, 1.3263950677355751e-05, 1.3263950677355751e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3263950677355751e-05

Optimization complete. Final v2v error: 3.120965003967285 mm

Highest mean error: 3.458723545074463 mm for frame 148

Lowest mean error: 2.7479684352874756 mm for frame 7

Saving results

Total time: 32.703187227249146
