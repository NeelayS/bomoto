Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=107, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 5992-6047
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00714734
Iteration 2/25 | Loss: 0.00137541
Iteration 3/25 | Loss: 0.00128479
Iteration 4/25 | Loss: 0.00127294
Iteration 5/25 | Loss: 0.00126840
Iteration 6/25 | Loss: 0.00126774
Iteration 7/25 | Loss: 0.00126774
Iteration 8/25 | Loss: 0.00126774
Iteration 9/25 | Loss: 0.00126774
Iteration 10/25 | Loss: 0.00126774
Iteration 11/25 | Loss: 0.00126774
Iteration 12/25 | Loss: 0.00126774
Iteration 13/25 | Loss: 0.00126774
Iteration 14/25 | Loss: 0.00126774
Iteration 15/25 | Loss: 0.00126774
Iteration 16/25 | Loss: 0.00126774
Iteration 17/25 | Loss: 0.00126774
Iteration 18/25 | Loss: 0.00126774
Iteration 19/25 | Loss: 0.00126774
Iteration 20/25 | Loss: 0.00126774
Iteration 21/25 | Loss: 0.00126774
Iteration 22/25 | Loss: 0.00126774
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012677391059696674, 0.0012677391059696674, 0.0012677391059696674, 0.0012677391059696674, 0.0012677391059696674]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012677391059696674

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.15704226
Iteration 2/25 | Loss: 0.00090106
Iteration 3/25 | Loss: 0.00090106
Iteration 4/25 | Loss: 0.00090106
Iteration 5/25 | Loss: 0.00090105
Iteration 6/25 | Loss: 0.00090105
Iteration 7/25 | Loss: 0.00090105
Iteration 8/25 | Loss: 0.00090105
Iteration 9/25 | Loss: 0.00090105
Iteration 10/25 | Loss: 0.00090105
Iteration 11/25 | Loss: 0.00090105
Iteration 12/25 | Loss: 0.00090105
Iteration 13/25 | Loss: 0.00090105
Iteration 14/25 | Loss: 0.00090105
Iteration 15/25 | Loss: 0.00090105
Iteration 16/25 | Loss: 0.00090105
Iteration 17/25 | Loss: 0.00090105
Iteration 18/25 | Loss: 0.00090105
Iteration 19/25 | Loss: 0.00090105
Iteration 20/25 | Loss: 0.00090105
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009010529611259699, 0.0009010529611259699, 0.0009010529611259699, 0.0009010529611259699, 0.0009010529611259699]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009010529611259699

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090105
Iteration 2/1000 | Loss: 0.00003651
Iteration 3/1000 | Loss: 0.00002523
Iteration 4/1000 | Loss: 0.00002239
Iteration 5/1000 | Loss: 0.00002128
Iteration 6/1000 | Loss: 0.00002039
Iteration 7/1000 | Loss: 0.00001979
Iteration 8/1000 | Loss: 0.00001928
Iteration 9/1000 | Loss: 0.00001899
Iteration 10/1000 | Loss: 0.00001873
Iteration 11/1000 | Loss: 0.00001849
Iteration 12/1000 | Loss: 0.00001834
Iteration 13/1000 | Loss: 0.00001816
Iteration 14/1000 | Loss: 0.00001805
Iteration 15/1000 | Loss: 0.00001801
Iteration 16/1000 | Loss: 0.00001798
Iteration 17/1000 | Loss: 0.00001797
Iteration 18/1000 | Loss: 0.00001796
Iteration 19/1000 | Loss: 0.00001795
Iteration 20/1000 | Loss: 0.00001795
Iteration 21/1000 | Loss: 0.00001794
Iteration 22/1000 | Loss: 0.00001794
Iteration 23/1000 | Loss: 0.00001793
Iteration 24/1000 | Loss: 0.00001792
Iteration 25/1000 | Loss: 0.00001792
Iteration 26/1000 | Loss: 0.00001791
Iteration 27/1000 | Loss: 0.00001791
Iteration 28/1000 | Loss: 0.00001790
Iteration 29/1000 | Loss: 0.00001789
Iteration 30/1000 | Loss: 0.00001789
Iteration 31/1000 | Loss: 0.00001789
Iteration 32/1000 | Loss: 0.00001788
Iteration 33/1000 | Loss: 0.00001788
Iteration 34/1000 | Loss: 0.00001787
Iteration 35/1000 | Loss: 0.00001787
Iteration 36/1000 | Loss: 0.00001786
Iteration 37/1000 | Loss: 0.00001785
Iteration 38/1000 | Loss: 0.00001785
Iteration 39/1000 | Loss: 0.00001784
Iteration 40/1000 | Loss: 0.00001783
Iteration 41/1000 | Loss: 0.00001783
Iteration 42/1000 | Loss: 0.00001783
Iteration 43/1000 | Loss: 0.00001782
Iteration 44/1000 | Loss: 0.00001782
Iteration 45/1000 | Loss: 0.00001781
Iteration 46/1000 | Loss: 0.00001781
Iteration 47/1000 | Loss: 0.00001781
Iteration 48/1000 | Loss: 0.00001780
Iteration 49/1000 | Loss: 0.00001780
Iteration 50/1000 | Loss: 0.00001779
Iteration 51/1000 | Loss: 0.00001779
Iteration 52/1000 | Loss: 0.00001778
Iteration 53/1000 | Loss: 0.00001776
Iteration 54/1000 | Loss: 0.00001776
Iteration 55/1000 | Loss: 0.00001776
Iteration 56/1000 | Loss: 0.00001775
Iteration 57/1000 | Loss: 0.00001775
Iteration 58/1000 | Loss: 0.00001774
Iteration 59/1000 | Loss: 0.00001773
Iteration 60/1000 | Loss: 0.00001773
Iteration 61/1000 | Loss: 0.00001773
Iteration 62/1000 | Loss: 0.00001773
Iteration 63/1000 | Loss: 0.00001772
Iteration 64/1000 | Loss: 0.00001772
Iteration 65/1000 | Loss: 0.00001771
Iteration 66/1000 | Loss: 0.00001771
Iteration 67/1000 | Loss: 0.00001770
Iteration 68/1000 | Loss: 0.00001769
Iteration 69/1000 | Loss: 0.00001767
Iteration 70/1000 | Loss: 0.00001767
Iteration 71/1000 | Loss: 0.00001766
Iteration 72/1000 | Loss: 0.00001766
Iteration 73/1000 | Loss: 0.00001765
Iteration 74/1000 | Loss: 0.00001765
Iteration 75/1000 | Loss: 0.00001764
Iteration 76/1000 | Loss: 0.00001764
Iteration 77/1000 | Loss: 0.00001764
Iteration 78/1000 | Loss: 0.00001763
Iteration 79/1000 | Loss: 0.00001763
Iteration 80/1000 | Loss: 0.00001763
Iteration 81/1000 | Loss: 0.00001762
Iteration 82/1000 | Loss: 0.00001762
Iteration 83/1000 | Loss: 0.00001762
Iteration 84/1000 | Loss: 0.00001762
Iteration 85/1000 | Loss: 0.00001762
Iteration 86/1000 | Loss: 0.00001762
Iteration 87/1000 | Loss: 0.00001761
Iteration 88/1000 | Loss: 0.00001761
Iteration 89/1000 | Loss: 0.00001760
Iteration 90/1000 | Loss: 0.00001760
Iteration 91/1000 | Loss: 0.00001760
Iteration 92/1000 | Loss: 0.00001760
Iteration 93/1000 | Loss: 0.00001759
Iteration 94/1000 | Loss: 0.00001759
Iteration 95/1000 | Loss: 0.00001759
Iteration 96/1000 | Loss: 0.00001759
Iteration 97/1000 | Loss: 0.00001759
Iteration 98/1000 | Loss: 0.00001758
Iteration 99/1000 | Loss: 0.00001758
Iteration 100/1000 | Loss: 0.00001758
Iteration 101/1000 | Loss: 0.00001757
Iteration 102/1000 | Loss: 0.00001757
Iteration 103/1000 | Loss: 0.00001757
Iteration 104/1000 | Loss: 0.00001757
Iteration 105/1000 | Loss: 0.00001757
Iteration 106/1000 | Loss: 0.00001756
Iteration 107/1000 | Loss: 0.00001756
Iteration 108/1000 | Loss: 0.00001756
Iteration 109/1000 | Loss: 0.00001756
Iteration 110/1000 | Loss: 0.00001756
Iteration 111/1000 | Loss: 0.00001755
Iteration 112/1000 | Loss: 0.00001755
Iteration 113/1000 | Loss: 0.00001755
Iteration 114/1000 | Loss: 0.00001755
Iteration 115/1000 | Loss: 0.00001754
Iteration 116/1000 | Loss: 0.00001754
Iteration 117/1000 | Loss: 0.00001754
Iteration 118/1000 | Loss: 0.00001753
Iteration 119/1000 | Loss: 0.00001753
Iteration 120/1000 | Loss: 0.00001753
Iteration 121/1000 | Loss: 0.00001753
Iteration 122/1000 | Loss: 0.00001753
Iteration 123/1000 | Loss: 0.00001752
Iteration 124/1000 | Loss: 0.00001752
Iteration 125/1000 | Loss: 0.00001752
Iteration 126/1000 | Loss: 0.00001752
Iteration 127/1000 | Loss: 0.00001752
Iteration 128/1000 | Loss: 0.00001752
Iteration 129/1000 | Loss: 0.00001752
Iteration 130/1000 | Loss: 0.00001752
Iteration 131/1000 | Loss: 0.00001752
Iteration 132/1000 | Loss: 0.00001752
Iteration 133/1000 | Loss: 0.00001751
Iteration 134/1000 | Loss: 0.00001751
Iteration 135/1000 | Loss: 0.00001751
Iteration 136/1000 | Loss: 0.00001751
Iteration 137/1000 | Loss: 0.00001751
Iteration 138/1000 | Loss: 0.00001750
Iteration 139/1000 | Loss: 0.00001750
Iteration 140/1000 | Loss: 0.00001750
Iteration 141/1000 | Loss: 0.00001750
Iteration 142/1000 | Loss: 0.00001750
Iteration 143/1000 | Loss: 0.00001750
Iteration 144/1000 | Loss: 0.00001750
Iteration 145/1000 | Loss: 0.00001750
Iteration 146/1000 | Loss: 0.00001750
Iteration 147/1000 | Loss: 0.00001749
Iteration 148/1000 | Loss: 0.00001749
Iteration 149/1000 | Loss: 0.00001749
Iteration 150/1000 | Loss: 0.00001749
Iteration 151/1000 | Loss: 0.00001749
Iteration 152/1000 | Loss: 0.00001749
Iteration 153/1000 | Loss: 0.00001748
Iteration 154/1000 | Loss: 0.00001748
Iteration 155/1000 | Loss: 0.00001748
Iteration 156/1000 | Loss: 0.00001748
Iteration 157/1000 | Loss: 0.00001748
Iteration 158/1000 | Loss: 0.00001748
Iteration 159/1000 | Loss: 0.00001748
Iteration 160/1000 | Loss: 0.00001748
Iteration 161/1000 | Loss: 0.00001747
Iteration 162/1000 | Loss: 0.00001747
Iteration 163/1000 | Loss: 0.00001747
Iteration 164/1000 | Loss: 0.00001747
Iteration 165/1000 | Loss: 0.00001747
Iteration 166/1000 | Loss: 0.00001747
Iteration 167/1000 | Loss: 0.00001747
Iteration 168/1000 | Loss: 0.00001747
Iteration 169/1000 | Loss: 0.00001747
Iteration 170/1000 | Loss: 0.00001747
Iteration 171/1000 | Loss: 0.00001747
Iteration 172/1000 | Loss: 0.00001747
Iteration 173/1000 | Loss: 0.00001747
Iteration 174/1000 | Loss: 0.00001747
Iteration 175/1000 | Loss: 0.00001746
Iteration 176/1000 | Loss: 0.00001746
Iteration 177/1000 | Loss: 0.00001746
Iteration 178/1000 | Loss: 0.00001746
Iteration 179/1000 | Loss: 0.00001746
Iteration 180/1000 | Loss: 0.00001746
Iteration 181/1000 | Loss: 0.00001746
Iteration 182/1000 | Loss: 0.00001746
Iteration 183/1000 | Loss: 0.00001746
Iteration 184/1000 | Loss: 0.00001746
Iteration 185/1000 | Loss: 0.00001746
Iteration 186/1000 | Loss: 0.00001746
Iteration 187/1000 | Loss: 0.00001746
Iteration 188/1000 | Loss: 0.00001746
Iteration 189/1000 | Loss: 0.00001746
Iteration 190/1000 | Loss: 0.00001746
Iteration 191/1000 | Loss: 0.00001746
Iteration 192/1000 | Loss: 0.00001746
Iteration 193/1000 | Loss: 0.00001746
Iteration 194/1000 | Loss: 0.00001745
Iteration 195/1000 | Loss: 0.00001745
Iteration 196/1000 | Loss: 0.00001745
Iteration 197/1000 | Loss: 0.00001745
Iteration 198/1000 | Loss: 0.00001745
Iteration 199/1000 | Loss: 0.00001745
Iteration 200/1000 | Loss: 0.00001745
Iteration 201/1000 | Loss: 0.00001745
Iteration 202/1000 | Loss: 0.00001745
Iteration 203/1000 | Loss: 0.00001745
Iteration 204/1000 | Loss: 0.00001745
Iteration 205/1000 | Loss: 0.00001745
Iteration 206/1000 | Loss: 0.00001745
Iteration 207/1000 | Loss: 0.00001745
Iteration 208/1000 | Loss: 0.00001745
Iteration 209/1000 | Loss: 0.00001745
Iteration 210/1000 | Loss: 0.00001745
Iteration 211/1000 | Loss: 0.00001745
Iteration 212/1000 | Loss: 0.00001745
Iteration 213/1000 | Loss: 0.00001745
Iteration 214/1000 | Loss: 0.00001745
Iteration 215/1000 | Loss: 0.00001745
Iteration 216/1000 | Loss: 0.00001745
Iteration 217/1000 | Loss: 0.00001745
Iteration 218/1000 | Loss: 0.00001745
Iteration 219/1000 | Loss: 0.00001745
Iteration 220/1000 | Loss: 0.00001745
Iteration 221/1000 | Loss: 0.00001745
Iteration 222/1000 | Loss: 0.00001745
Iteration 223/1000 | Loss: 0.00001745
Iteration 224/1000 | Loss: 0.00001745
Iteration 225/1000 | Loss: 0.00001745
Iteration 226/1000 | Loss: 0.00001745
Iteration 227/1000 | Loss: 0.00001745
Iteration 228/1000 | Loss: 0.00001745
Iteration 229/1000 | Loss: 0.00001745
Iteration 230/1000 | Loss: 0.00001745
Iteration 231/1000 | Loss: 0.00001745
Iteration 232/1000 | Loss: 0.00001745
Iteration 233/1000 | Loss: 0.00001745
Iteration 234/1000 | Loss: 0.00001745
Iteration 235/1000 | Loss: 0.00001745
Iteration 236/1000 | Loss: 0.00001745
Iteration 237/1000 | Loss: 0.00001745
Iteration 238/1000 | Loss: 0.00001745
Iteration 239/1000 | Loss: 0.00001745
Iteration 240/1000 | Loss: 0.00001745
Iteration 241/1000 | Loss: 0.00001745
Iteration 242/1000 | Loss: 0.00001745
Iteration 243/1000 | Loss: 0.00001745
Iteration 244/1000 | Loss: 0.00001745
Iteration 245/1000 | Loss: 0.00001745
Iteration 246/1000 | Loss: 0.00001745
Iteration 247/1000 | Loss: 0.00001745
Iteration 248/1000 | Loss: 0.00001745
Iteration 249/1000 | Loss: 0.00001745
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 249. Stopping optimization.
Last 5 losses: [1.7454980479669757e-05, 1.7454980479669757e-05, 1.7454980479669757e-05, 1.7454980479669757e-05, 1.7454980479669757e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7454980479669757e-05

Optimization complete. Final v2v error: 3.4840800762176514 mm

Highest mean error: 5.14664888381958 mm for frame 98

Lowest mean error: 3.0770199298858643 mm for frame 142

Saving results

Total time: 44.78511333465576
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00807776
Iteration 2/25 | Loss: 0.00129447
Iteration 3/25 | Loss: 0.00121325
Iteration 4/25 | Loss: 0.00120652
Iteration 5/25 | Loss: 0.00120467
Iteration 6/25 | Loss: 0.00120467
Iteration 7/25 | Loss: 0.00120467
Iteration 8/25 | Loss: 0.00120467
Iteration 9/25 | Loss: 0.00120467
Iteration 10/25 | Loss: 0.00120467
Iteration 11/25 | Loss: 0.00120467
Iteration 12/25 | Loss: 0.00120467
Iteration 13/25 | Loss: 0.00120467
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012046709889546037, 0.0012046709889546037, 0.0012046709889546037, 0.0012046709889546037, 0.0012046709889546037]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012046709889546037

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43627870
Iteration 2/25 | Loss: 0.00072725
Iteration 3/25 | Loss: 0.00072724
Iteration 4/25 | Loss: 0.00072724
Iteration 5/25 | Loss: 0.00072724
Iteration 6/25 | Loss: 0.00072724
Iteration 7/25 | Loss: 0.00072724
Iteration 8/25 | Loss: 0.00072724
Iteration 9/25 | Loss: 0.00072724
Iteration 10/25 | Loss: 0.00072724
Iteration 11/25 | Loss: 0.00072724
Iteration 12/25 | Loss: 0.00072724
Iteration 13/25 | Loss: 0.00072724
Iteration 14/25 | Loss: 0.00072724
Iteration 15/25 | Loss: 0.00072724
Iteration 16/25 | Loss: 0.00072724
Iteration 17/25 | Loss: 0.00072724
Iteration 18/25 | Loss: 0.00072724
Iteration 19/25 | Loss: 0.00072724
Iteration 20/25 | Loss: 0.00072724
Iteration 21/25 | Loss: 0.00072724
Iteration 22/25 | Loss: 0.00072724
Iteration 23/25 | Loss: 0.00072724
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007272397051565349, 0.0007272397051565349, 0.0007272397051565349, 0.0007272397051565349, 0.0007272397051565349]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007272397051565349

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072724
Iteration 2/1000 | Loss: 0.00002754
Iteration 3/1000 | Loss: 0.00001882
Iteration 4/1000 | Loss: 0.00001605
Iteration 5/1000 | Loss: 0.00001487
Iteration 6/1000 | Loss: 0.00001401
Iteration 7/1000 | Loss: 0.00001340
Iteration 8/1000 | Loss: 0.00001298
Iteration 9/1000 | Loss: 0.00001276
Iteration 10/1000 | Loss: 0.00001253
Iteration 11/1000 | Loss: 0.00001238
Iteration 12/1000 | Loss: 0.00001235
Iteration 13/1000 | Loss: 0.00001234
Iteration 14/1000 | Loss: 0.00001234
Iteration 15/1000 | Loss: 0.00001233
Iteration 16/1000 | Loss: 0.00001232
Iteration 17/1000 | Loss: 0.00001231
Iteration 18/1000 | Loss: 0.00001228
Iteration 19/1000 | Loss: 0.00001224
Iteration 20/1000 | Loss: 0.00001224
Iteration 21/1000 | Loss: 0.00001221
Iteration 22/1000 | Loss: 0.00001219
Iteration 23/1000 | Loss: 0.00001219
Iteration 24/1000 | Loss: 0.00001219
Iteration 25/1000 | Loss: 0.00001218
Iteration 26/1000 | Loss: 0.00001215
Iteration 27/1000 | Loss: 0.00001211
Iteration 28/1000 | Loss: 0.00001211
Iteration 29/1000 | Loss: 0.00001207
Iteration 30/1000 | Loss: 0.00001207
Iteration 31/1000 | Loss: 0.00001207
Iteration 32/1000 | Loss: 0.00001207
Iteration 33/1000 | Loss: 0.00001207
Iteration 34/1000 | Loss: 0.00001206
Iteration 35/1000 | Loss: 0.00001206
Iteration 36/1000 | Loss: 0.00001206
Iteration 37/1000 | Loss: 0.00001205
Iteration 38/1000 | Loss: 0.00001204
Iteration 39/1000 | Loss: 0.00001204
Iteration 40/1000 | Loss: 0.00001203
Iteration 41/1000 | Loss: 0.00001203
Iteration 42/1000 | Loss: 0.00001202
Iteration 43/1000 | Loss: 0.00001202
Iteration 44/1000 | Loss: 0.00001202
Iteration 45/1000 | Loss: 0.00001201
Iteration 46/1000 | Loss: 0.00001201
Iteration 47/1000 | Loss: 0.00001201
Iteration 48/1000 | Loss: 0.00001201
Iteration 49/1000 | Loss: 0.00001201
Iteration 50/1000 | Loss: 0.00001201
Iteration 51/1000 | Loss: 0.00001200
Iteration 52/1000 | Loss: 0.00001200
Iteration 53/1000 | Loss: 0.00001200
Iteration 54/1000 | Loss: 0.00001199
Iteration 55/1000 | Loss: 0.00001199
Iteration 56/1000 | Loss: 0.00001198
Iteration 57/1000 | Loss: 0.00001198
Iteration 58/1000 | Loss: 0.00001198
Iteration 59/1000 | Loss: 0.00001198
Iteration 60/1000 | Loss: 0.00001197
Iteration 61/1000 | Loss: 0.00001197
Iteration 62/1000 | Loss: 0.00001196
Iteration 63/1000 | Loss: 0.00001196
Iteration 64/1000 | Loss: 0.00001196
Iteration 65/1000 | Loss: 0.00001195
Iteration 66/1000 | Loss: 0.00001195
Iteration 67/1000 | Loss: 0.00001195
Iteration 68/1000 | Loss: 0.00001195
Iteration 69/1000 | Loss: 0.00001194
Iteration 70/1000 | Loss: 0.00001194
Iteration 71/1000 | Loss: 0.00001194
Iteration 72/1000 | Loss: 0.00001193
Iteration 73/1000 | Loss: 0.00001193
Iteration 74/1000 | Loss: 0.00001193
Iteration 75/1000 | Loss: 0.00001192
Iteration 76/1000 | Loss: 0.00001190
Iteration 77/1000 | Loss: 0.00001190
Iteration 78/1000 | Loss: 0.00001190
Iteration 79/1000 | Loss: 0.00001189
Iteration 80/1000 | Loss: 0.00001189
Iteration 81/1000 | Loss: 0.00001188
Iteration 82/1000 | Loss: 0.00001187
Iteration 83/1000 | Loss: 0.00001186
Iteration 84/1000 | Loss: 0.00001186
Iteration 85/1000 | Loss: 0.00001186
Iteration 86/1000 | Loss: 0.00001185
Iteration 87/1000 | Loss: 0.00001182
Iteration 88/1000 | Loss: 0.00001182
Iteration 89/1000 | Loss: 0.00001182
Iteration 90/1000 | Loss: 0.00001182
Iteration 91/1000 | Loss: 0.00001182
Iteration 92/1000 | Loss: 0.00001181
Iteration 93/1000 | Loss: 0.00001181
Iteration 94/1000 | Loss: 0.00001181
Iteration 95/1000 | Loss: 0.00001181
Iteration 96/1000 | Loss: 0.00001181
Iteration 97/1000 | Loss: 0.00001181
Iteration 98/1000 | Loss: 0.00001181
Iteration 99/1000 | Loss: 0.00001181
Iteration 100/1000 | Loss: 0.00001181
Iteration 101/1000 | Loss: 0.00001181
Iteration 102/1000 | Loss: 0.00001180
Iteration 103/1000 | Loss: 0.00001179
Iteration 104/1000 | Loss: 0.00001179
Iteration 105/1000 | Loss: 0.00001178
Iteration 106/1000 | Loss: 0.00001177
Iteration 107/1000 | Loss: 0.00001177
Iteration 108/1000 | Loss: 0.00001177
Iteration 109/1000 | Loss: 0.00001176
Iteration 110/1000 | Loss: 0.00001176
Iteration 111/1000 | Loss: 0.00001176
Iteration 112/1000 | Loss: 0.00001176
Iteration 113/1000 | Loss: 0.00001176
Iteration 114/1000 | Loss: 0.00001175
Iteration 115/1000 | Loss: 0.00001175
Iteration 116/1000 | Loss: 0.00001175
Iteration 117/1000 | Loss: 0.00001175
Iteration 118/1000 | Loss: 0.00001174
Iteration 119/1000 | Loss: 0.00001174
Iteration 120/1000 | Loss: 0.00001174
Iteration 121/1000 | Loss: 0.00001174
Iteration 122/1000 | Loss: 0.00001173
Iteration 123/1000 | Loss: 0.00001173
Iteration 124/1000 | Loss: 0.00001173
Iteration 125/1000 | Loss: 0.00001172
Iteration 126/1000 | Loss: 0.00001172
Iteration 127/1000 | Loss: 0.00001172
Iteration 128/1000 | Loss: 0.00001172
Iteration 129/1000 | Loss: 0.00001172
Iteration 130/1000 | Loss: 0.00001171
Iteration 131/1000 | Loss: 0.00001171
Iteration 132/1000 | Loss: 0.00001171
Iteration 133/1000 | Loss: 0.00001171
Iteration 134/1000 | Loss: 0.00001171
Iteration 135/1000 | Loss: 0.00001171
Iteration 136/1000 | Loss: 0.00001171
Iteration 137/1000 | Loss: 0.00001171
Iteration 138/1000 | Loss: 0.00001171
Iteration 139/1000 | Loss: 0.00001170
Iteration 140/1000 | Loss: 0.00001170
Iteration 141/1000 | Loss: 0.00001170
Iteration 142/1000 | Loss: 0.00001170
Iteration 143/1000 | Loss: 0.00001170
Iteration 144/1000 | Loss: 0.00001170
Iteration 145/1000 | Loss: 0.00001170
Iteration 146/1000 | Loss: 0.00001170
Iteration 147/1000 | Loss: 0.00001169
Iteration 148/1000 | Loss: 0.00001169
Iteration 149/1000 | Loss: 0.00001169
Iteration 150/1000 | Loss: 0.00001169
Iteration 151/1000 | Loss: 0.00001169
Iteration 152/1000 | Loss: 0.00001169
Iteration 153/1000 | Loss: 0.00001169
Iteration 154/1000 | Loss: 0.00001169
Iteration 155/1000 | Loss: 0.00001169
Iteration 156/1000 | Loss: 0.00001168
Iteration 157/1000 | Loss: 0.00001168
Iteration 158/1000 | Loss: 0.00001168
Iteration 159/1000 | Loss: 0.00001168
Iteration 160/1000 | Loss: 0.00001168
Iteration 161/1000 | Loss: 0.00001168
Iteration 162/1000 | Loss: 0.00001168
Iteration 163/1000 | Loss: 0.00001168
Iteration 164/1000 | Loss: 0.00001168
Iteration 165/1000 | Loss: 0.00001168
Iteration 166/1000 | Loss: 0.00001168
Iteration 167/1000 | Loss: 0.00001168
Iteration 168/1000 | Loss: 0.00001168
Iteration 169/1000 | Loss: 0.00001167
Iteration 170/1000 | Loss: 0.00001167
Iteration 171/1000 | Loss: 0.00001167
Iteration 172/1000 | Loss: 0.00001167
Iteration 173/1000 | Loss: 0.00001167
Iteration 174/1000 | Loss: 0.00001167
Iteration 175/1000 | Loss: 0.00001167
Iteration 176/1000 | Loss: 0.00001167
Iteration 177/1000 | Loss: 0.00001167
Iteration 178/1000 | Loss: 0.00001167
Iteration 179/1000 | Loss: 0.00001166
Iteration 180/1000 | Loss: 0.00001166
Iteration 181/1000 | Loss: 0.00001166
Iteration 182/1000 | Loss: 0.00001166
Iteration 183/1000 | Loss: 0.00001166
Iteration 184/1000 | Loss: 0.00001166
Iteration 185/1000 | Loss: 0.00001166
Iteration 186/1000 | Loss: 0.00001166
Iteration 187/1000 | Loss: 0.00001166
Iteration 188/1000 | Loss: 0.00001166
Iteration 189/1000 | Loss: 0.00001166
Iteration 190/1000 | Loss: 0.00001166
Iteration 191/1000 | Loss: 0.00001166
Iteration 192/1000 | Loss: 0.00001166
Iteration 193/1000 | Loss: 0.00001166
Iteration 194/1000 | Loss: 0.00001166
Iteration 195/1000 | Loss: 0.00001166
Iteration 196/1000 | Loss: 0.00001166
Iteration 197/1000 | Loss: 0.00001166
Iteration 198/1000 | Loss: 0.00001166
Iteration 199/1000 | Loss: 0.00001166
Iteration 200/1000 | Loss: 0.00001166
Iteration 201/1000 | Loss: 0.00001166
Iteration 202/1000 | Loss: 0.00001166
Iteration 203/1000 | Loss: 0.00001166
Iteration 204/1000 | Loss: 0.00001166
Iteration 205/1000 | Loss: 0.00001166
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [1.1657779396045953e-05, 1.1657779396045953e-05, 1.1657779396045953e-05, 1.1657779396045953e-05, 1.1657779396045953e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1657779396045953e-05

Optimization complete. Final v2v error: 2.9113314151763916 mm

Highest mean error: 3.10528826713562 mm for frame 55

Lowest mean error: 2.741689682006836 mm for frame 27

Saving results

Total time: 40.099263429641724
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00404973
Iteration 2/25 | Loss: 0.00125819
Iteration 3/25 | Loss: 0.00119849
Iteration 4/25 | Loss: 0.00119126
Iteration 5/25 | Loss: 0.00118992
Iteration 6/25 | Loss: 0.00118992
Iteration 7/25 | Loss: 0.00118992
Iteration 8/25 | Loss: 0.00118992
Iteration 9/25 | Loss: 0.00118992
Iteration 10/25 | Loss: 0.00118992
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011899173259735107, 0.0011899173259735107, 0.0011899173259735107, 0.0011899173259735107, 0.0011899173259735107]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011899173259735107

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.60155606
Iteration 2/25 | Loss: 0.00074167
Iteration 3/25 | Loss: 0.00074165
Iteration 4/25 | Loss: 0.00074165
Iteration 5/25 | Loss: 0.00074165
Iteration 6/25 | Loss: 0.00074165
Iteration 7/25 | Loss: 0.00074165
Iteration 8/25 | Loss: 0.00074165
Iteration 9/25 | Loss: 0.00074165
Iteration 10/25 | Loss: 0.00074165
Iteration 11/25 | Loss: 0.00074165
Iteration 12/25 | Loss: 0.00074165
Iteration 13/25 | Loss: 0.00074165
Iteration 14/25 | Loss: 0.00074165
Iteration 15/25 | Loss: 0.00074165
Iteration 16/25 | Loss: 0.00074165
Iteration 17/25 | Loss: 0.00074165
Iteration 18/25 | Loss: 0.00074165
Iteration 19/25 | Loss: 0.00074165
Iteration 20/25 | Loss: 0.00074165
Iteration 21/25 | Loss: 0.00074165
Iteration 22/25 | Loss: 0.00074165
Iteration 23/25 | Loss: 0.00074165
Iteration 24/25 | Loss: 0.00074165
Iteration 25/25 | Loss: 0.00074165

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074165
Iteration 2/1000 | Loss: 0.00002644
Iteration 3/1000 | Loss: 0.00001823
Iteration 4/1000 | Loss: 0.00001591
Iteration 5/1000 | Loss: 0.00001474
Iteration 6/1000 | Loss: 0.00001417
Iteration 7/1000 | Loss: 0.00001364
Iteration 8/1000 | Loss: 0.00001364
Iteration 9/1000 | Loss: 0.00001332
Iteration 10/1000 | Loss: 0.00001311
Iteration 11/1000 | Loss: 0.00001287
Iteration 12/1000 | Loss: 0.00001270
Iteration 13/1000 | Loss: 0.00001260
Iteration 14/1000 | Loss: 0.00001247
Iteration 15/1000 | Loss: 0.00001243
Iteration 16/1000 | Loss: 0.00001242
Iteration 17/1000 | Loss: 0.00001241
Iteration 18/1000 | Loss: 0.00001240
Iteration 19/1000 | Loss: 0.00001239
Iteration 20/1000 | Loss: 0.00001239
Iteration 21/1000 | Loss: 0.00001239
Iteration 22/1000 | Loss: 0.00001238
Iteration 23/1000 | Loss: 0.00001237
Iteration 24/1000 | Loss: 0.00001236
Iteration 25/1000 | Loss: 0.00001235
Iteration 26/1000 | Loss: 0.00001235
Iteration 27/1000 | Loss: 0.00001234
Iteration 28/1000 | Loss: 0.00001234
Iteration 29/1000 | Loss: 0.00001233
Iteration 30/1000 | Loss: 0.00001232
Iteration 31/1000 | Loss: 0.00001232
Iteration 32/1000 | Loss: 0.00001229
Iteration 33/1000 | Loss: 0.00001228
Iteration 34/1000 | Loss: 0.00001228
Iteration 35/1000 | Loss: 0.00001227
Iteration 36/1000 | Loss: 0.00001226
Iteration 37/1000 | Loss: 0.00001226
Iteration 38/1000 | Loss: 0.00001225
Iteration 39/1000 | Loss: 0.00001225
Iteration 40/1000 | Loss: 0.00001224
Iteration 41/1000 | Loss: 0.00001223
Iteration 42/1000 | Loss: 0.00001223
Iteration 43/1000 | Loss: 0.00001223
Iteration 44/1000 | Loss: 0.00001223
Iteration 45/1000 | Loss: 0.00001223
Iteration 46/1000 | Loss: 0.00001222
Iteration 47/1000 | Loss: 0.00001222
Iteration 48/1000 | Loss: 0.00001222
Iteration 49/1000 | Loss: 0.00001222
Iteration 50/1000 | Loss: 0.00001221
Iteration 51/1000 | Loss: 0.00001221
Iteration 52/1000 | Loss: 0.00001221
Iteration 53/1000 | Loss: 0.00001221
Iteration 54/1000 | Loss: 0.00001220
Iteration 55/1000 | Loss: 0.00001220
Iteration 56/1000 | Loss: 0.00001220
Iteration 57/1000 | Loss: 0.00001219
Iteration 58/1000 | Loss: 0.00001219
Iteration 59/1000 | Loss: 0.00001218
Iteration 60/1000 | Loss: 0.00001218
Iteration 61/1000 | Loss: 0.00001218
Iteration 62/1000 | Loss: 0.00001217
Iteration 63/1000 | Loss: 0.00001216
Iteration 64/1000 | Loss: 0.00001214
Iteration 65/1000 | Loss: 0.00001214
Iteration 66/1000 | Loss: 0.00001213
Iteration 67/1000 | Loss: 0.00001212
Iteration 68/1000 | Loss: 0.00001212
Iteration 69/1000 | Loss: 0.00001211
Iteration 70/1000 | Loss: 0.00001211
Iteration 71/1000 | Loss: 0.00001211
Iteration 72/1000 | Loss: 0.00001211
Iteration 73/1000 | Loss: 0.00001211
Iteration 74/1000 | Loss: 0.00001211
Iteration 75/1000 | Loss: 0.00001211
Iteration 76/1000 | Loss: 0.00001211
Iteration 77/1000 | Loss: 0.00001211
Iteration 78/1000 | Loss: 0.00001209
Iteration 79/1000 | Loss: 0.00001209
Iteration 80/1000 | Loss: 0.00001209
Iteration 81/1000 | Loss: 0.00001209
Iteration 82/1000 | Loss: 0.00001208
Iteration 83/1000 | Loss: 0.00001208
Iteration 84/1000 | Loss: 0.00001208
Iteration 85/1000 | Loss: 0.00001208
Iteration 86/1000 | Loss: 0.00001208
Iteration 87/1000 | Loss: 0.00001207
Iteration 88/1000 | Loss: 0.00001207
Iteration 89/1000 | Loss: 0.00001207
Iteration 90/1000 | Loss: 0.00001207
Iteration 91/1000 | Loss: 0.00001206
Iteration 92/1000 | Loss: 0.00001205
Iteration 93/1000 | Loss: 0.00001205
Iteration 94/1000 | Loss: 0.00001205
Iteration 95/1000 | Loss: 0.00001205
Iteration 96/1000 | Loss: 0.00001205
Iteration 97/1000 | Loss: 0.00001205
Iteration 98/1000 | Loss: 0.00001204
Iteration 99/1000 | Loss: 0.00001204
Iteration 100/1000 | Loss: 0.00001204
Iteration 101/1000 | Loss: 0.00001204
Iteration 102/1000 | Loss: 0.00001204
Iteration 103/1000 | Loss: 0.00001204
Iteration 104/1000 | Loss: 0.00001203
Iteration 105/1000 | Loss: 0.00001203
Iteration 106/1000 | Loss: 0.00001203
Iteration 107/1000 | Loss: 0.00001202
Iteration 108/1000 | Loss: 0.00001202
Iteration 109/1000 | Loss: 0.00001202
Iteration 110/1000 | Loss: 0.00001202
Iteration 111/1000 | Loss: 0.00001201
Iteration 112/1000 | Loss: 0.00001201
Iteration 113/1000 | Loss: 0.00001201
Iteration 114/1000 | Loss: 0.00001200
Iteration 115/1000 | Loss: 0.00001200
Iteration 116/1000 | Loss: 0.00001199
Iteration 117/1000 | Loss: 0.00001199
Iteration 118/1000 | Loss: 0.00001199
Iteration 119/1000 | Loss: 0.00001199
Iteration 120/1000 | Loss: 0.00001197
Iteration 121/1000 | Loss: 0.00001197
Iteration 122/1000 | Loss: 0.00001196
Iteration 123/1000 | Loss: 0.00001196
Iteration 124/1000 | Loss: 0.00001196
Iteration 125/1000 | Loss: 0.00001196
Iteration 126/1000 | Loss: 0.00001196
Iteration 127/1000 | Loss: 0.00001196
Iteration 128/1000 | Loss: 0.00001195
Iteration 129/1000 | Loss: 0.00001195
Iteration 130/1000 | Loss: 0.00001195
Iteration 131/1000 | Loss: 0.00001195
Iteration 132/1000 | Loss: 0.00001194
Iteration 133/1000 | Loss: 0.00001194
Iteration 134/1000 | Loss: 0.00001194
Iteration 135/1000 | Loss: 0.00001193
Iteration 136/1000 | Loss: 0.00001193
Iteration 137/1000 | Loss: 0.00001193
Iteration 138/1000 | Loss: 0.00001193
Iteration 139/1000 | Loss: 0.00001193
Iteration 140/1000 | Loss: 0.00001193
Iteration 141/1000 | Loss: 0.00001193
Iteration 142/1000 | Loss: 0.00001193
Iteration 143/1000 | Loss: 0.00001192
Iteration 144/1000 | Loss: 0.00001192
Iteration 145/1000 | Loss: 0.00001192
Iteration 146/1000 | Loss: 0.00001192
Iteration 147/1000 | Loss: 0.00001192
Iteration 148/1000 | Loss: 0.00001192
Iteration 149/1000 | Loss: 0.00001192
Iteration 150/1000 | Loss: 0.00001191
Iteration 151/1000 | Loss: 0.00001191
Iteration 152/1000 | Loss: 0.00001191
Iteration 153/1000 | Loss: 0.00001191
Iteration 154/1000 | Loss: 0.00001191
Iteration 155/1000 | Loss: 0.00001191
Iteration 156/1000 | Loss: 0.00001191
Iteration 157/1000 | Loss: 0.00001191
Iteration 158/1000 | Loss: 0.00001191
Iteration 159/1000 | Loss: 0.00001191
Iteration 160/1000 | Loss: 0.00001191
Iteration 161/1000 | Loss: 0.00001191
Iteration 162/1000 | Loss: 0.00001191
Iteration 163/1000 | Loss: 0.00001190
Iteration 164/1000 | Loss: 0.00001190
Iteration 165/1000 | Loss: 0.00001190
Iteration 166/1000 | Loss: 0.00001190
Iteration 167/1000 | Loss: 0.00001190
Iteration 168/1000 | Loss: 0.00001190
Iteration 169/1000 | Loss: 0.00001190
Iteration 170/1000 | Loss: 0.00001190
Iteration 171/1000 | Loss: 0.00001190
Iteration 172/1000 | Loss: 0.00001190
Iteration 173/1000 | Loss: 0.00001190
Iteration 174/1000 | Loss: 0.00001190
Iteration 175/1000 | Loss: 0.00001190
Iteration 176/1000 | Loss: 0.00001190
Iteration 177/1000 | Loss: 0.00001190
Iteration 178/1000 | Loss: 0.00001189
Iteration 179/1000 | Loss: 0.00001189
Iteration 180/1000 | Loss: 0.00001189
Iteration 181/1000 | Loss: 0.00001189
Iteration 182/1000 | Loss: 0.00001189
Iteration 183/1000 | Loss: 0.00001189
Iteration 184/1000 | Loss: 0.00001189
Iteration 185/1000 | Loss: 0.00001189
Iteration 186/1000 | Loss: 0.00001189
Iteration 187/1000 | Loss: 0.00001189
Iteration 188/1000 | Loss: 0.00001189
Iteration 189/1000 | Loss: 0.00001189
Iteration 190/1000 | Loss: 0.00001188
Iteration 191/1000 | Loss: 0.00001188
Iteration 192/1000 | Loss: 0.00001188
Iteration 193/1000 | Loss: 0.00001188
Iteration 194/1000 | Loss: 0.00001188
Iteration 195/1000 | Loss: 0.00001188
Iteration 196/1000 | Loss: 0.00001188
Iteration 197/1000 | Loss: 0.00001188
Iteration 198/1000 | Loss: 0.00001188
Iteration 199/1000 | Loss: 0.00001188
Iteration 200/1000 | Loss: 0.00001188
Iteration 201/1000 | Loss: 0.00001188
Iteration 202/1000 | Loss: 0.00001187
Iteration 203/1000 | Loss: 0.00001187
Iteration 204/1000 | Loss: 0.00001187
Iteration 205/1000 | Loss: 0.00001187
Iteration 206/1000 | Loss: 0.00001187
Iteration 207/1000 | Loss: 0.00001187
Iteration 208/1000 | Loss: 0.00001187
Iteration 209/1000 | Loss: 0.00001187
Iteration 210/1000 | Loss: 0.00001187
Iteration 211/1000 | Loss: 0.00001187
Iteration 212/1000 | Loss: 0.00001187
Iteration 213/1000 | Loss: 0.00001187
Iteration 214/1000 | Loss: 0.00001187
Iteration 215/1000 | Loss: 0.00001187
Iteration 216/1000 | Loss: 0.00001187
Iteration 217/1000 | Loss: 0.00001187
Iteration 218/1000 | Loss: 0.00001187
Iteration 219/1000 | Loss: 0.00001187
Iteration 220/1000 | Loss: 0.00001186
Iteration 221/1000 | Loss: 0.00001186
Iteration 222/1000 | Loss: 0.00001186
Iteration 223/1000 | Loss: 0.00001186
Iteration 224/1000 | Loss: 0.00001186
Iteration 225/1000 | Loss: 0.00001186
Iteration 226/1000 | Loss: 0.00001186
Iteration 227/1000 | Loss: 0.00001186
Iteration 228/1000 | Loss: 0.00001186
Iteration 229/1000 | Loss: 0.00001186
Iteration 230/1000 | Loss: 0.00001186
Iteration 231/1000 | Loss: 0.00001186
Iteration 232/1000 | Loss: 0.00001186
Iteration 233/1000 | Loss: 0.00001186
Iteration 234/1000 | Loss: 0.00001186
Iteration 235/1000 | Loss: 0.00001186
Iteration 236/1000 | Loss: 0.00001186
Iteration 237/1000 | Loss: 0.00001186
Iteration 238/1000 | Loss: 0.00001186
Iteration 239/1000 | Loss: 0.00001186
Iteration 240/1000 | Loss: 0.00001186
Iteration 241/1000 | Loss: 0.00001186
Iteration 242/1000 | Loss: 0.00001186
Iteration 243/1000 | Loss: 0.00001185
Iteration 244/1000 | Loss: 0.00001185
Iteration 245/1000 | Loss: 0.00001185
Iteration 246/1000 | Loss: 0.00001185
Iteration 247/1000 | Loss: 0.00001185
Iteration 248/1000 | Loss: 0.00001185
Iteration 249/1000 | Loss: 0.00001185
Iteration 250/1000 | Loss: 0.00001185
Iteration 251/1000 | Loss: 0.00001185
Iteration 252/1000 | Loss: 0.00001185
Iteration 253/1000 | Loss: 0.00001185
Iteration 254/1000 | Loss: 0.00001185
Iteration 255/1000 | Loss: 0.00001185
Iteration 256/1000 | Loss: 0.00001185
Iteration 257/1000 | Loss: 0.00001185
Iteration 258/1000 | Loss: 0.00001185
Iteration 259/1000 | Loss: 0.00001185
Iteration 260/1000 | Loss: 0.00001185
Iteration 261/1000 | Loss: 0.00001185
Iteration 262/1000 | Loss: 0.00001185
Iteration 263/1000 | Loss: 0.00001185
Iteration 264/1000 | Loss: 0.00001185
Iteration 265/1000 | Loss: 0.00001185
Iteration 266/1000 | Loss: 0.00001185
Iteration 267/1000 | Loss: 0.00001185
Iteration 268/1000 | Loss: 0.00001185
Iteration 269/1000 | Loss: 0.00001185
Iteration 270/1000 | Loss: 0.00001185
Iteration 271/1000 | Loss: 0.00001185
Iteration 272/1000 | Loss: 0.00001185
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 272. Stopping optimization.
Last 5 losses: [1.1850670489366166e-05, 1.1850670489366166e-05, 1.1850670489366166e-05, 1.1850670489366166e-05, 1.1850670489366166e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1850670489366166e-05

Optimization complete. Final v2v error: 2.945639133453369 mm

Highest mean error: 3.2861268520355225 mm for frame 79

Lowest mean error: 2.803478240966797 mm for frame 3

Saving results

Total time: 43.09740662574768
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00339840
Iteration 2/25 | Loss: 0.00136443
Iteration 3/25 | Loss: 0.00124296
Iteration 4/25 | Loss: 0.00122282
Iteration 5/25 | Loss: 0.00121663
Iteration 6/25 | Loss: 0.00121508
Iteration 7/25 | Loss: 0.00121508
Iteration 8/25 | Loss: 0.00121508
Iteration 9/25 | Loss: 0.00121508
Iteration 10/25 | Loss: 0.00121508
Iteration 11/25 | Loss: 0.00121508
Iteration 12/25 | Loss: 0.00121508
Iteration 13/25 | Loss: 0.00121508
Iteration 14/25 | Loss: 0.00121508
Iteration 15/25 | Loss: 0.00121508
Iteration 16/25 | Loss: 0.00121508
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012150821276009083, 0.0012150821276009083, 0.0012150821276009083, 0.0012150821276009083, 0.0012150821276009083]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012150821276009083

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37799108
Iteration 2/25 | Loss: 0.00074245
Iteration 3/25 | Loss: 0.00074244
Iteration 4/25 | Loss: 0.00074244
Iteration 5/25 | Loss: 0.00074244
Iteration 6/25 | Loss: 0.00074244
Iteration 7/25 | Loss: 0.00074244
Iteration 8/25 | Loss: 0.00074244
Iteration 9/25 | Loss: 0.00074244
Iteration 10/25 | Loss: 0.00074244
Iteration 11/25 | Loss: 0.00074244
Iteration 12/25 | Loss: 0.00074244
Iteration 13/25 | Loss: 0.00074244
Iteration 14/25 | Loss: 0.00074244
Iteration 15/25 | Loss: 0.00074244
Iteration 16/25 | Loss: 0.00074244
Iteration 17/25 | Loss: 0.00074244
Iteration 18/25 | Loss: 0.00074244
Iteration 19/25 | Loss: 0.00074244
Iteration 20/25 | Loss: 0.00074244
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007424409850500524, 0.0007424409850500524, 0.0007424409850500524, 0.0007424409850500524, 0.0007424409850500524]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007424409850500524

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074244
Iteration 2/1000 | Loss: 0.00004241
Iteration 3/1000 | Loss: 0.00002779
Iteration 4/1000 | Loss: 0.00002440
Iteration 5/1000 | Loss: 0.00002289
Iteration 6/1000 | Loss: 0.00002163
Iteration 7/1000 | Loss: 0.00002083
Iteration 8/1000 | Loss: 0.00002032
Iteration 9/1000 | Loss: 0.00001992
Iteration 10/1000 | Loss: 0.00001953
Iteration 11/1000 | Loss: 0.00001927
Iteration 12/1000 | Loss: 0.00001907
Iteration 13/1000 | Loss: 0.00001900
Iteration 14/1000 | Loss: 0.00001896
Iteration 15/1000 | Loss: 0.00001893
Iteration 16/1000 | Loss: 0.00001892
Iteration 17/1000 | Loss: 0.00001891
Iteration 18/1000 | Loss: 0.00001891
Iteration 19/1000 | Loss: 0.00001890
Iteration 20/1000 | Loss: 0.00001889
Iteration 21/1000 | Loss: 0.00001888
Iteration 22/1000 | Loss: 0.00001887
Iteration 23/1000 | Loss: 0.00001887
Iteration 24/1000 | Loss: 0.00001887
Iteration 25/1000 | Loss: 0.00001886
Iteration 26/1000 | Loss: 0.00001882
Iteration 27/1000 | Loss: 0.00001880
Iteration 28/1000 | Loss: 0.00001879
Iteration 29/1000 | Loss: 0.00001879
Iteration 30/1000 | Loss: 0.00001878
Iteration 31/1000 | Loss: 0.00001878
Iteration 32/1000 | Loss: 0.00001877
Iteration 33/1000 | Loss: 0.00001876
Iteration 34/1000 | Loss: 0.00001874
Iteration 35/1000 | Loss: 0.00001874
Iteration 36/1000 | Loss: 0.00001873
Iteration 37/1000 | Loss: 0.00001872
Iteration 38/1000 | Loss: 0.00001872
Iteration 39/1000 | Loss: 0.00001871
Iteration 40/1000 | Loss: 0.00001871
Iteration 41/1000 | Loss: 0.00001871
Iteration 42/1000 | Loss: 0.00001871
Iteration 43/1000 | Loss: 0.00001871
Iteration 44/1000 | Loss: 0.00001868
Iteration 45/1000 | Loss: 0.00001868
Iteration 46/1000 | Loss: 0.00001864
Iteration 47/1000 | Loss: 0.00001863
Iteration 48/1000 | Loss: 0.00001862
Iteration 49/1000 | Loss: 0.00001862
Iteration 50/1000 | Loss: 0.00001861
Iteration 51/1000 | Loss: 0.00001861
Iteration 52/1000 | Loss: 0.00001861
Iteration 53/1000 | Loss: 0.00001860
Iteration 54/1000 | Loss: 0.00001860
Iteration 55/1000 | Loss: 0.00001860
Iteration 56/1000 | Loss: 0.00001860
Iteration 57/1000 | Loss: 0.00001859
Iteration 58/1000 | Loss: 0.00001859
Iteration 59/1000 | Loss: 0.00001859
Iteration 60/1000 | Loss: 0.00001859
Iteration 61/1000 | Loss: 0.00001858
Iteration 62/1000 | Loss: 0.00001858
Iteration 63/1000 | Loss: 0.00001858
Iteration 64/1000 | Loss: 0.00001858
Iteration 65/1000 | Loss: 0.00001858
Iteration 66/1000 | Loss: 0.00001857
Iteration 67/1000 | Loss: 0.00001857
Iteration 68/1000 | Loss: 0.00001857
Iteration 69/1000 | Loss: 0.00001857
Iteration 70/1000 | Loss: 0.00001857
Iteration 71/1000 | Loss: 0.00001857
Iteration 72/1000 | Loss: 0.00001857
Iteration 73/1000 | Loss: 0.00001857
Iteration 74/1000 | Loss: 0.00001857
Iteration 75/1000 | Loss: 0.00001857
Iteration 76/1000 | Loss: 0.00001856
Iteration 77/1000 | Loss: 0.00001856
Iteration 78/1000 | Loss: 0.00001856
Iteration 79/1000 | Loss: 0.00001856
Iteration 80/1000 | Loss: 0.00001856
Iteration 81/1000 | Loss: 0.00001856
Iteration 82/1000 | Loss: 0.00001856
Iteration 83/1000 | Loss: 0.00001856
Iteration 84/1000 | Loss: 0.00001856
Iteration 85/1000 | Loss: 0.00001856
Iteration 86/1000 | Loss: 0.00001855
Iteration 87/1000 | Loss: 0.00001855
Iteration 88/1000 | Loss: 0.00001855
Iteration 89/1000 | Loss: 0.00001855
Iteration 90/1000 | Loss: 0.00001855
Iteration 91/1000 | Loss: 0.00001855
Iteration 92/1000 | Loss: 0.00001855
Iteration 93/1000 | Loss: 0.00001855
Iteration 94/1000 | Loss: 0.00001855
Iteration 95/1000 | Loss: 0.00001855
Iteration 96/1000 | Loss: 0.00001855
Iteration 97/1000 | Loss: 0.00001855
Iteration 98/1000 | Loss: 0.00001854
Iteration 99/1000 | Loss: 0.00001854
Iteration 100/1000 | Loss: 0.00001854
Iteration 101/1000 | Loss: 0.00001854
Iteration 102/1000 | Loss: 0.00001854
Iteration 103/1000 | Loss: 0.00001854
Iteration 104/1000 | Loss: 0.00001853
Iteration 105/1000 | Loss: 0.00001853
Iteration 106/1000 | Loss: 0.00001853
Iteration 107/1000 | Loss: 0.00001853
Iteration 108/1000 | Loss: 0.00001853
Iteration 109/1000 | Loss: 0.00001853
Iteration 110/1000 | Loss: 0.00001853
Iteration 111/1000 | Loss: 0.00001853
Iteration 112/1000 | Loss: 0.00001853
Iteration 113/1000 | Loss: 0.00001853
Iteration 114/1000 | Loss: 0.00001853
Iteration 115/1000 | Loss: 0.00001853
Iteration 116/1000 | Loss: 0.00001853
Iteration 117/1000 | Loss: 0.00001853
Iteration 118/1000 | Loss: 0.00001853
Iteration 119/1000 | Loss: 0.00001853
Iteration 120/1000 | Loss: 0.00001853
Iteration 121/1000 | Loss: 0.00001853
Iteration 122/1000 | Loss: 0.00001853
Iteration 123/1000 | Loss: 0.00001853
Iteration 124/1000 | Loss: 0.00001853
Iteration 125/1000 | Loss: 0.00001853
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.8530248780734837e-05, 1.8530248780734837e-05, 1.8530248780734837e-05, 1.8530248780734837e-05, 1.8530248780734837e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8530248780734837e-05

Optimization complete. Final v2v error: 3.648287773132324 mm

Highest mean error: 4.349603176116943 mm for frame 210

Lowest mean error: 2.9312336444854736 mm for frame 112

Saving results

Total time: 41.44152879714966
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01042732
Iteration 2/25 | Loss: 0.00217854
Iteration 3/25 | Loss: 0.00173151
Iteration 4/25 | Loss: 0.00162312
Iteration 5/25 | Loss: 0.00164409
Iteration 6/25 | Loss: 0.00161293
Iteration 7/25 | Loss: 0.00145276
Iteration 8/25 | Loss: 0.00139505
Iteration 9/25 | Loss: 0.00136661
Iteration 10/25 | Loss: 0.00135130
Iteration 11/25 | Loss: 0.00133868
Iteration 12/25 | Loss: 0.00134353
Iteration 13/25 | Loss: 0.00133468
Iteration 14/25 | Loss: 0.00133809
Iteration 15/25 | Loss: 0.00133317
Iteration 16/25 | Loss: 0.00133196
Iteration 17/25 | Loss: 0.00133680
Iteration 18/25 | Loss: 0.00133506
Iteration 19/25 | Loss: 0.00133347
Iteration 20/25 | Loss: 0.00132064
Iteration 21/25 | Loss: 0.00132144
Iteration 22/25 | Loss: 0.00132060
Iteration 23/25 | Loss: 0.00131811
Iteration 24/25 | Loss: 0.00131347
Iteration 25/25 | Loss: 0.00131428

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42863715
Iteration 2/25 | Loss: 0.00161364
Iteration 3/25 | Loss: 0.00121973
Iteration 4/25 | Loss: 0.00121973
Iteration 5/25 | Loss: 0.00121973
Iteration 6/25 | Loss: 0.00121973
Iteration 7/25 | Loss: 0.00121973
Iteration 8/25 | Loss: 0.00121973
Iteration 9/25 | Loss: 0.00121973
Iteration 10/25 | Loss: 0.00121973
Iteration 11/25 | Loss: 0.00121973
Iteration 12/25 | Loss: 0.00121973
Iteration 13/25 | Loss: 0.00121973
Iteration 14/25 | Loss: 0.00121973
Iteration 15/25 | Loss: 0.00121972
Iteration 16/25 | Loss: 0.00121972
Iteration 17/25 | Loss: 0.00121972
Iteration 18/25 | Loss: 0.00121972
Iteration 19/25 | Loss: 0.00121972
Iteration 20/25 | Loss: 0.00121972
Iteration 21/25 | Loss: 0.00121972
Iteration 22/25 | Loss: 0.00121972
Iteration 23/25 | Loss: 0.00121972
Iteration 24/25 | Loss: 0.00121972
Iteration 25/25 | Loss: 0.00121972

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121972
Iteration 2/1000 | Loss: 0.00071796
Iteration 3/1000 | Loss: 0.00059885
Iteration 4/1000 | Loss: 0.00044162
Iteration 5/1000 | Loss: 0.00063847
Iteration 6/1000 | Loss: 0.00045583
Iteration 7/1000 | Loss: 0.00042004
Iteration 8/1000 | Loss: 0.00049337
Iteration 9/1000 | Loss: 0.00063982
Iteration 10/1000 | Loss: 0.00047386
Iteration 11/1000 | Loss: 0.00035993
Iteration 12/1000 | Loss: 0.00039672
Iteration 13/1000 | Loss: 0.00045582
Iteration 14/1000 | Loss: 0.00047550
Iteration 15/1000 | Loss: 0.00051268
Iteration 16/1000 | Loss: 0.00115426
Iteration 17/1000 | Loss: 0.00126684
Iteration 18/1000 | Loss: 0.00058306
Iteration 19/1000 | Loss: 0.00021336
Iteration 20/1000 | Loss: 0.00073819
Iteration 21/1000 | Loss: 0.00067939
Iteration 22/1000 | Loss: 0.00084334
Iteration 23/1000 | Loss: 0.00091583
Iteration 24/1000 | Loss: 0.00077788
Iteration 25/1000 | Loss: 0.00060193
Iteration 26/1000 | Loss: 0.00053804
Iteration 27/1000 | Loss: 0.00032574
Iteration 28/1000 | Loss: 0.00063606
Iteration 29/1000 | Loss: 0.00053058
Iteration 30/1000 | Loss: 0.00066393
Iteration 31/1000 | Loss: 0.00066929
Iteration 32/1000 | Loss: 0.00062660
Iteration 33/1000 | Loss: 0.00042903
Iteration 34/1000 | Loss: 0.00052256
Iteration 35/1000 | Loss: 0.00064854
Iteration 36/1000 | Loss: 0.00052572
Iteration 37/1000 | Loss: 0.00060833
Iteration 38/1000 | Loss: 0.00058179
Iteration 39/1000 | Loss: 0.00049287
Iteration 40/1000 | Loss: 0.00055804
Iteration 41/1000 | Loss: 0.00030517
Iteration 42/1000 | Loss: 0.00030830
Iteration 43/1000 | Loss: 0.00052182
Iteration 44/1000 | Loss: 0.00048985
Iteration 45/1000 | Loss: 0.00038897
Iteration 46/1000 | Loss: 0.00058713
Iteration 47/1000 | Loss: 0.00033642
Iteration 48/1000 | Loss: 0.00044849
Iteration 49/1000 | Loss: 0.00057553
Iteration 50/1000 | Loss: 0.00063227
Iteration 51/1000 | Loss: 0.00051548
Iteration 52/1000 | Loss: 0.00034183
Iteration 53/1000 | Loss: 0.00083106
Iteration 54/1000 | Loss: 0.00068047
Iteration 55/1000 | Loss: 0.00048926
Iteration 56/1000 | Loss: 0.00042824
Iteration 57/1000 | Loss: 0.00072851
Iteration 58/1000 | Loss: 0.00042122
Iteration 59/1000 | Loss: 0.00041258
Iteration 60/1000 | Loss: 0.00040380
Iteration 61/1000 | Loss: 0.00032369
Iteration 62/1000 | Loss: 0.00051480
Iteration 63/1000 | Loss: 0.00036605
Iteration 64/1000 | Loss: 0.00043123
Iteration 65/1000 | Loss: 0.00056604
Iteration 66/1000 | Loss: 0.00053464
Iteration 67/1000 | Loss: 0.00038802
Iteration 68/1000 | Loss: 0.00018846
Iteration 69/1000 | Loss: 0.00008895
Iteration 70/1000 | Loss: 0.00018661
Iteration 71/1000 | Loss: 0.00009306
Iteration 72/1000 | Loss: 0.00015197
Iteration 73/1000 | Loss: 0.00023061
Iteration 74/1000 | Loss: 0.00021350
Iteration 75/1000 | Loss: 0.00026002
Iteration 76/1000 | Loss: 0.00083229
Iteration 77/1000 | Loss: 0.00055336
Iteration 78/1000 | Loss: 0.00009051
Iteration 79/1000 | Loss: 0.00006974
Iteration 80/1000 | Loss: 0.00021154
Iteration 81/1000 | Loss: 0.00016702
Iteration 82/1000 | Loss: 0.00014566
Iteration 83/1000 | Loss: 0.00022324
Iteration 84/1000 | Loss: 0.00024907
Iteration 85/1000 | Loss: 0.00036748
Iteration 86/1000 | Loss: 0.00025059
Iteration 87/1000 | Loss: 0.00022070
Iteration 88/1000 | Loss: 0.00017329
Iteration 89/1000 | Loss: 0.00016649
Iteration 90/1000 | Loss: 0.00019906
Iteration 91/1000 | Loss: 0.00015197
Iteration 92/1000 | Loss: 0.00012390
Iteration 93/1000 | Loss: 0.00011017
Iteration 94/1000 | Loss: 0.00021426
Iteration 95/1000 | Loss: 0.00020525
Iteration 96/1000 | Loss: 0.00014931
Iteration 97/1000 | Loss: 0.00025132
Iteration 98/1000 | Loss: 0.00023652
Iteration 99/1000 | Loss: 0.00015298
Iteration 100/1000 | Loss: 0.00017384
Iteration 101/1000 | Loss: 0.00026939
Iteration 102/1000 | Loss: 0.00027840
Iteration 103/1000 | Loss: 0.00008277
Iteration 104/1000 | Loss: 0.00012906
Iteration 105/1000 | Loss: 0.00016993
Iteration 106/1000 | Loss: 0.00023345
Iteration 107/1000 | Loss: 0.00014639
Iteration 108/1000 | Loss: 0.00015952
Iteration 109/1000 | Loss: 0.00018151
Iteration 110/1000 | Loss: 0.00014256
Iteration 111/1000 | Loss: 0.00024115
Iteration 112/1000 | Loss: 0.00030008
Iteration 113/1000 | Loss: 0.00017370
Iteration 114/1000 | Loss: 0.00027840
Iteration 115/1000 | Loss: 0.00020415
Iteration 116/1000 | Loss: 0.00026807
Iteration 117/1000 | Loss: 0.00018676
Iteration 118/1000 | Loss: 0.00023742
Iteration 119/1000 | Loss: 0.00021006
Iteration 120/1000 | Loss: 0.00022763
Iteration 121/1000 | Loss: 0.00010489
Iteration 122/1000 | Loss: 0.00020265
Iteration 123/1000 | Loss: 0.00023773
Iteration 124/1000 | Loss: 0.00021909
Iteration 125/1000 | Loss: 0.00025274
Iteration 126/1000 | Loss: 0.00016831
Iteration 127/1000 | Loss: 0.00007204
Iteration 128/1000 | Loss: 0.00008736
Iteration 129/1000 | Loss: 0.00003667
Iteration 130/1000 | Loss: 0.00003501
Iteration 131/1000 | Loss: 0.00020349
Iteration 132/1000 | Loss: 0.00009023
Iteration 133/1000 | Loss: 0.00022124
Iteration 134/1000 | Loss: 0.00027569
Iteration 135/1000 | Loss: 0.00016120
Iteration 136/1000 | Loss: 0.00021295
Iteration 137/1000 | Loss: 0.00019908
Iteration 138/1000 | Loss: 0.00019601
Iteration 139/1000 | Loss: 0.00016671
Iteration 140/1000 | Loss: 0.00012005
Iteration 141/1000 | Loss: 0.00015593
Iteration 142/1000 | Loss: 0.00009652
Iteration 143/1000 | Loss: 0.00013105
Iteration 144/1000 | Loss: 0.00009508
Iteration 145/1000 | Loss: 0.00003952
Iteration 146/1000 | Loss: 0.00003508
Iteration 147/1000 | Loss: 0.00020614
Iteration 148/1000 | Loss: 0.00021113
Iteration 149/1000 | Loss: 0.00039707
Iteration 150/1000 | Loss: 0.00025965
Iteration 151/1000 | Loss: 0.00013339
Iteration 152/1000 | Loss: 0.00010716
Iteration 153/1000 | Loss: 0.00026334
Iteration 154/1000 | Loss: 0.00029643
Iteration 155/1000 | Loss: 0.00003916
Iteration 156/1000 | Loss: 0.00013330
Iteration 157/1000 | Loss: 0.00003347
Iteration 158/1000 | Loss: 0.00003020
Iteration 159/1000 | Loss: 0.00020493
Iteration 160/1000 | Loss: 0.00013806
Iteration 161/1000 | Loss: 0.00009882
Iteration 162/1000 | Loss: 0.00009233
Iteration 163/1000 | Loss: 0.00008114
Iteration 164/1000 | Loss: 0.00007236
Iteration 165/1000 | Loss: 0.00008090
Iteration 166/1000 | Loss: 0.00007578
Iteration 167/1000 | Loss: 0.00008600
Iteration 168/1000 | Loss: 0.00015831
Iteration 169/1000 | Loss: 0.00012865
Iteration 170/1000 | Loss: 0.00006695
Iteration 171/1000 | Loss: 0.00019364
Iteration 172/1000 | Loss: 0.00018287
Iteration 173/1000 | Loss: 0.00007291
Iteration 174/1000 | Loss: 0.00004573
Iteration 175/1000 | Loss: 0.00003563
Iteration 176/1000 | Loss: 0.00015320
Iteration 177/1000 | Loss: 0.00011426
Iteration 178/1000 | Loss: 0.00013308
Iteration 179/1000 | Loss: 0.00004913
Iteration 180/1000 | Loss: 0.00013739
Iteration 181/1000 | Loss: 0.00011880
Iteration 182/1000 | Loss: 0.00011626
Iteration 183/1000 | Loss: 0.00010696
Iteration 184/1000 | Loss: 0.00015174
Iteration 185/1000 | Loss: 0.00004444
Iteration 186/1000 | Loss: 0.00003576
Iteration 187/1000 | Loss: 0.00005146
Iteration 188/1000 | Loss: 0.00013995
Iteration 189/1000 | Loss: 0.00014242
Iteration 190/1000 | Loss: 0.00022058
Iteration 191/1000 | Loss: 0.00008652
Iteration 192/1000 | Loss: 0.00013405
Iteration 193/1000 | Loss: 0.00010229
Iteration 194/1000 | Loss: 0.00003919
Iteration 195/1000 | Loss: 0.00010491
Iteration 196/1000 | Loss: 0.00010742
Iteration 197/1000 | Loss: 0.00010891
Iteration 198/1000 | Loss: 0.00007231
Iteration 199/1000 | Loss: 0.00003949
Iteration 200/1000 | Loss: 0.00018061
Iteration 201/1000 | Loss: 0.00007652
Iteration 202/1000 | Loss: 0.00013414
Iteration 203/1000 | Loss: 0.00012522
Iteration 204/1000 | Loss: 0.00007783
Iteration 205/1000 | Loss: 0.00017171
Iteration 206/1000 | Loss: 0.00026902
Iteration 207/1000 | Loss: 0.00016468
Iteration 208/1000 | Loss: 0.00022717
Iteration 209/1000 | Loss: 0.00024069
Iteration 210/1000 | Loss: 0.00021904
Iteration 211/1000 | Loss: 0.00024030
Iteration 212/1000 | Loss: 0.00023057
Iteration 213/1000 | Loss: 0.00024505
Iteration 214/1000 | Loss: 0.00032754
Iteration 215/1000 | Loss: 0.00016827
Iteration 216/1000 | Loss: 0.00029256
Iteration 217/1000 | Loss: 0.00017014
Iteration 218/1000 | Loss: 0.00011346
Iteration 219/1000 | Loss: 0.00017337
Iteration 220/1000 | Loss: 0.00021782
Iteration 221/1000 | Loss: 0.00014182
Iteration 222/1000 | Loss: 0.00025902
Iteration 223/1000 | Loss: 0.00032641
Iteration 224/1000 | Loss: 0.00007644
Iteration 225/1000 | Loss: 0.00018206
Iteration 226/1000 | Loss: 0.00003798
Iteration 227/1000 | Loss: 0.00013173
Iteration 228/1000 | Loss: 0.00003471
Iteration 229/1000 | Loss: 0.00003060
Iteration 230/1000 | Loss: 0.00014899
Iteration 231/1000 | Loss: 0.00009987
Iteration 232/1000 | Loss: 0.00018566
Iteration 233/1000 | Loss: 0.00024419
Iteration 234/1000 | Loss: 0.00005342
Iteration 235/1000 | Loss: 0.00052629
Iteration 236/1000 | Loss: 0.00014117
Iteration 237/1000 | Loss: 0.00016038
Iteration 238/1000 | Loss: 0.00041431
Iteration 239/1000 | Loss: 0.00013347
Iteration 240/1000 | Loss: 0.00009463
Iteration 241/1000 | Loss: 0.00006256
Iteration 242/1000 | Loss: 0.00020794
Iteration 243/1000 | Loss: 0.00008591
Iteration 244/1000 | Loss: 0.00015009
Iteration 245/1000 | Loss: 0.00015201
Iteration 246/1000 | Loss: 0.00013729
Iteration 247/1000 | Loss: 0.00012586
Iteration 248/1000 | Loss: 0.00033577
Iteration 249/1000 | Loss: 0.00006871
Iteration 250/1000 | Loss: 0.00005978
Iteration 251/1000 | Loss: 0.00005739
Iteration 252/1000 | Loss: 0.00003032
Iteration 253/1000 | Loss: 0.00003411
Iteration 254/1000 | Loss: 0.00018052
Iteration 255/1000 | Loss: 0.00003197
Iteration 256/1000 | Loss: 0.00005820
Iteration 257/1000 | Loss: 0.00002601
Iteration 258/1000 | Loss: 0.00002532
Iteration 259/1000 | Loss: 0.00002479
Iteration 260/1000 | Loss: 0.00013286
Iteration 261/1000 | Loss: 0.00005284
Iteration 262/1000 | Loss: 0.00009783
Iteration 263/1000 | Loss: 0.00002790
Iteration 264/1000 | Loss: 0.00005259
Iteration 265/1000 | Loss: 0.00002673
Iteration 266/1000 | Loss: 0.00002436
Iteration 267/1000 | Loss: 0.00002379
Iteration 268/1000 | Loss: 0.00002325
Iteration 269/1000 | Loss: 0.00002488
Iteration 270/1000 | Loss: 0.00002326
Iteration 271/1000 | Loss: 0.00002439
Iteration 272/1000 | Loss: 0.00002298
Iteration 273/1000 | Loss: 0.00002285
Iteration 274/1000 | Loss: 0.00002272
Iteration 275/1000 | Loss: 0.00002269
Iteration 276/1000 | Loss: 0.00002267
Iteration 277/1000 | Loss: 0.00002254
Iteration 278/1000 | Loss: 0.00002249
Iteration 279/1000 | Loss: 0.00002246
Iteration 280/1000 | Loss: 0.00002245
Iteration 281/1000 | Loss: 0.00002245
Iteration 282/1000 | Loss: 0.00002244
Iteration 283/1000 | Loss: 0.00002243
Iteration 284/1000 | Loss: 0.00002243
Iteration 285/1000 | Loss: 0.00002242
Iteration 286/1000 | Loss: 0.00002241
Iteration 287/1000 | Loss: 0.00002241
Iteration 288/1000 | Loss: 0.00002236
Iteration 289/1000 | Loss: 0.00002234
Iteration 290/1000 | Loss: 0.00002233
Iteration 291/1000 | Loss: 0.00002233
Iteration 292/1000 | Loss: 0.00002233
Iteration 293/1000 | Loss: 0.00002233
Iteration 294/1000 | Loss: 0.00002233
Iteration 295/1000 | Loss: 0.00002232
Iteration 296/1000 | Loss: 0.00002232
Iteration 297/1000 | Loss: 0.00002231
Iteration 298/1000 | Loss: 0.00002231
Iteration 299/1000 | Loss: 0.00002231
Iteration 300/1000 | Loss: 0.00002231
Iteration 301/1000 | Loss: 0.00002231
Iteration 302/1000 | Loss: 0.00002231
Iteration 303/1000 | Loss: 0.00002231
Iteration 304/1000 | Loss: 0.00002231
Iteration 305/1000 | Loss: 0.00002231
Iteration 306/1000 | Loss: 0.00002231
Iteration 307/1000 | Loss: 0.00002231
Iteration 308/1000 | Loss: 0.00002231
Iteration 309/1000 | Loss: 0.00002230
Iteration 310/1000 | Loss: 0.00002230
Iteration 311/1000 | Loss: 0.00002230
Iteration 312/1000 | Loss: 0.00002230
Iteration 313/1000 | Loss: 0.00002230
Iteration 314/1000 | Loss: 0.00002229
Iteration 315/1000 | Loss: 0.00002229
Iteration 316/1000 | Loss: 0.00002229
Iteration 317/1000 | Loss: 0.00002228
Iteration 318/1000 | Loss: 0.00002228
Iteration 319/1000 | Loss: 0.00002228
Iteration 320/1000 | Loss: 0.00002227
Iteration 321/1000 | Loss: 0.00002227
Iteration 322/1000 | Loss: 0.00002227
Iteration 323/1000 | Loss: 0.00002227
Iteration 324/1000 | Loss: 0.00002227
Iteration 325/1000 | Loss: 0.00002227
Iteration 326/1000 | Loss: 0.00002227
Iteration 327/1000 | Loss: 0.00002227
Iteration 328/1000 | Loss: 0.00002227
Iteration 329/1000 | Loss: 0.00002227
Iteration 330/1000 | Loss: 0.00002227
Iteration 331/1000 | Loss: 0.00002227
Iteration 332/1000 | Loss: 0.00002227
Iteration 333/1000 | Loss: 0.00002226
Iteration 334/1000 | Loss: 0.00002226
Iteration 335/1000 | Loss: 0.00002226
Iteration 336/1000 | Loss: 0.00002226
Iteration 337/1000 | Loss: 0.00002226
Iteration 338/1000 | Loss: 0.00002225
Iteration 339/1000 | Loss: 0.00002225
Iteration 340/1000 | Loss: 0.00002225
Iteration 341/1000 | Loss: 0.00002225
Iteration 342/1000 | Loss: 0.00002225
Iteration 343/1000 | Loss: 0.00002225
Iteration 344/1000 | Loss: 0.00002225
Iteration 345/1000 | Loss: 0.00002225
Iteration 346/1000 | Loss: 0.00002225
Iteration 347/1000 | Loss: 0.00002225
Iteration 348/1000 | Loss: 0.00002225
Iteration 349/1000 | Loss: 0.00002225
Iteration 350/1000 | Loss: 0.00002225
Iteration 351/1000 | Loss: 0.00002225
Iteration 352/1000 | Loss: 0.00002224
Iteration 353/1000 | Loss: 0.00002224
Iteration 354/1000 | Loss: 0.00002224
Iteration 355/1000 | Loss: 0.00002224
Iteration 356/1000 | Loss: 0.00002224
Iteration 357/1000 | Loss: 0.00002224
Iteration 358/1000 | Loss: 0.00002224
Iteration 359/1000 | Loss: 0.00002223
Iteration 360/1000 | Loss: 0.00002223
Iteration 361/1000 | Loss: 0.00002223
Iteration 362/1000 | Loss: 0.00002223
Iteration 363/1000 | Loss: 0.00002223
Iteration 364/1000 | Loss: 0.00002223
Iteration 365/1000 | Loss: 0.00002223
Iteration 366/1000 | Loss: 0.00002223
Iteration 367/1000 | Loss: 0.00002223
Iteration 368/1000 | Loss: 0.00002223
Iteration 369/1000 | Loss: 0.00002222
Iteration 370/1000 | Loss: 0.00002222
Iteration 371/1000 | Loss: 0.00002222
Iteration 372/1000 | Loss: 0.00002222
Iteration 373/1000 | Loss: 0.00002222
Iteration 374/1000 | Loss: 0.00002222
Iteration 375/1000 | Loss: 0.00010504
Iteration 376/1000 | Loss: 0.00064610
Iteration 377/1000 | Loss: 0.00005383
Iteration 378/1000 | Loss: 0.00005212
Iteration 379/1000 | Loss: 0.00012272
Iteration 380/1000 | Loss: 0.00010831
Iteration 381/1000 | Loss: 0.00003725
Iteration 382/1000 | Loss: 0.00031243
Iteration 383/1000 | Loss: 0.00014359
Iteration 384/1000 | Loss: 0.00008016
Iteration 385/1000 | Loss: 0.00024442
Iteration 386/1000 | Loss: 0.00003015
Iteration 387/1000 | Loss: 0.00009343
Iteration 388/1000 | Loss: 0.00009108
Iteration 389/1000 | Loss: 0.00009195
Iteration 390/1000 | Loss: 0.00003899
Iteration 391/1000 | Loss: 0.00007180
Iteration 392/1000 | Loss: 0.00004580
Iteration 393/1000 | Loss: 0.00005669
Iteration 394/1000 | Loss: 0.00004551
Iteration 395/1000 | Loss: 0.00003721
Iteration 396/1000 | Loss: 0.00012444
Iteration 397/1000 | Loss: 0.00007814
Iteration 398/1000 | Loss: 0.00010926
Iteration 399/1000 | Loss: 0.00011369
Iteration 400/1000 | Loss: 0.00009217
Iteration 401/1000 | Loss: 0.00002686
Iteration 402/1000 | Loss: 0.00009824
Iteration 403/1000 | Loss: 0.00005695
Iteration 404/1000 | Loss: 0.00010590
Iteration 405/1000 | Loss: 0.00002906
Iteration 406/1000 | Loss: 0.00002675
Iteration 407/1000 | Loss: 0.00017341
Iteration 408/1000 | Loss: 0.00007145
Iteration 409/1000 | Loss: 0.00002973
Iteration 410/1000 | Loss: 0.00004701
Iteration 411/1000 | Loss: 0.00010690
Iteration 412/1000 | Loss: 0.00007103
Iteration 413/1000 | Loss: 0.00039084
Iteration 414/1000 | Loss: 0.00023976
Iteration 415/1000 | Loss: 0.00007351
Iteration 416/1000 | Loss: 0.00009196
Iteration 417/1000 | Loss: 0.00036713
Iteration 418/1000 | Loss: 0.00015781
Iteration 419/1000 | Loss: 0.00010496
Iteration 420/1000 | Loss: 0.00038818
Iteration 421/1000 | Loss: 0.00046459
Iteration 422/1000 | Loss: 0.00012593
Iteration 423/1000 | Loss: 0.00002837
Iteration 424/1000 | Loss: 0.00002343
Iteration 425/1000 | Loss: 0.00002194
Iteration 426/1000 | Loss: 0.00002117
Iteration 427/1000 | Loss: 0.00002061
Iteration 428/1000 | Loss: 0.00002018
Iteration 429/1000 | Loss: 0.00002001
Iteration 430/1000 | Loss: 0.00002001
Iteration 431/1000 | Loss: 0.00002000
Iteration 432/1000 | Loss: 0.00001996
Iteration 433/1000 | Loss: 0.00001995
Iteration 434/1000 | Loss: 0.00001988
Iteration 435/1000 | Loss: 0.00001988
Iteration 436/1000 | Loss: 0.00001985
Iteration 437/1000 | Loss: 0.00001979
Iteration 438/1000 | Loss: 0.00001978
Iteration 439/1000 | Loss: 0.00001978
Iteration 440/1000 | Loss: 0.00001977
Iteration 441/1000 | Loss: 0.00001977
Iteration 442/1000 | Loss: 0.00001975
Iteration 443/1000 | Loss: 0.00001974
Iteration 444/1000 | Loss: 0.00001970
Iteration 445/1000 | Loss: 0.00001970
Iteration 446/1000 | Loss: 0.00001970
Iteration 447/1000 | Loss: 0.00001970
Iteration 448/1000 | Loss: 0.00001970
Iteration 449/1000 | Loss: 0.00001970
Iteration 450/1000 | Loss: 0.00001970
Iteration 451/1000 | Loss: 0.00001970
Iteration 452/1000 | Loss: 0.00001970
Iteration 453/1000 | Loss: 0.00001970
Iteration 454/1000 | Loss: 0.00001970
Iteration 455/1000 | Loss: 0.00001969
Iteration 456/1000 | Loss: 0.00001969
Iteration 457/1000 | Loss: 0.00001968
Iteration 458/1000 | Loss: 0.00001968
Iteration 459/1000 | Loss: 0.00001967
Iteration 460/1000 | Loss: 0.00001967
Iteration 461/1000 | Loss: 0.00001967
Iteration 462/1000 | Loss: 0.00001967
Iteration 463/1000 | Loss: 0.00001967
Iteration 464/1000 | Loss: 0.00001967
Iteration 465/1000 | Loss: 0.00001967
Iteration 466/1000 | Loss: 0.00001967
Iteration 467/1000 | Loss: 0.00001966
Iteration 468/1000 | Loss: 0.00001966
Iteration 469/1000 | Loss: 0.00001966
Iteration 470/1000 | Loss: 0.00001966
Iteration 471/1000 | Loss: 0.00001966
Iteration 472/1000 | Loss: 0.00001965
Iteration 473/1000 | Loss: 0.00001965
Iteration 474/1000 | Loss: 0.00001965
Iteration 475/1000 | Loss: 0.00001965
Iteration 476/1000 | Loss: 0.00001964
Iteration 477/1000 | Loss: 0.00001964
Iteration 478/1000 | Loss: 0.00001964
Iteration 479/1000 | Loss: 0.00001964
Iteration 480/1000 | Loss: 0.00001964
Iteration 481/1000 | Loss: 0.00001964
Iteration 482/1000 | Loss: 0.00001964
Iteration 483/1000 | Loss: 0.00001964
Iteration 484/1000 | Loss: 0.00001964
Iteration 485/1000 | Loss: 0.00001964
Iteration 486/1000 | Loss: 0.00001964
Iteration 487/1000 | Loss: 0.00001964
Iteration 488/1000 | Loss: 0.00001963
Iteration 489/1000 | Loss: 0.00001963
Iteration 490/1000 | Loss: 0.00001963
Iteration 491/1000 | Loss: 0.00001963
Iteration 492/1000 | Loss: 0.00001963
Iteration 493/1000 | Loss: 0.00001963
Iteration 494/1000 | Loss: 0.00001963
Iteration 495/1000 | Loss: 0.00001963
Iteration 496/1000 | Loss: 0.00001963
Iteration 497/1000 | Loss: 0.00001963
Iteration 498/1000 | Loss: 0.00001963
Iteration 499/1000 | Loss: 0.00001962
Iteration 500/1000 | Loss: 0.00001962
Iteration 501/1000 | Loss: 0.00001962
Iteration 502/1000 | Loss: 0.00001962
Iteration 503/1000 | Loss: 0.00001962
Iteration 504/1000 | Loss: 0.00001962
Iteration 505/1000 | Loss: 0.00001962
Iteration 506/1000 | Loss: 0.00001962
Iteration 507/1000 | Loss: 0.00001962
Iteration 508/1000 | Loss: 0.00001962
Iteration 509/1000 | Loss: 0.00001962
Iteration 510/1000 | Loss: 0.00001962
Iteration 511/1000 | Loss: 0.00001962
Iteration 512/1000 | Loss: 0.00001962
Iteration 513/1000 | Loss: 0.00001962
Iteration 514/1000 | Loss: 0.00001962
Iteration 515/1000 | Loss: 0.00001961
Iteration 516/1000 | Loss: 0.00001961
Iteration 517/1000 | Loss: 0.00001961
Iteration 518/1000 | Loss: 0.00001961
Iteration 519/1000 | Loss: 0.00001961
Iteration 520/1000 | Loss: 0.00001960
Iteration 521/1000 | Loss: 0.00001960
Iteration 522/1000 | Loss: 0.00001960
Iteration 523/1000 | Loss: 0.00001960
Iteration 524/1000 | Loss: 0.00001960
Iteration 525/1000 | Loss: 0.00001960
Iteration 526/1000 | Loss: 0.00001960
Iteration 527/1000 | Loss: 0.00001960
Iteration 528/1000 | Loss: 0.00001960
Iteration 529/1000 | Loss: 0.00001960
Iteration 530/1000 | Loss: 0.00001960
Iteration 531/1000 | Loss: 0.00001960
Iteration 532/1000 | Loss: 0.00001960
Iteration 533/1000 | Loss: 0.00001960
Iteration 534/1000 | Loss: 0.00001960
Iteration 535/1000 | Loss: 0.00001960
Iteration 536/1000 | Loss: 0.00001960
Iteration 537/1000 | Loss: 0.00001960
Iteration 538/1000 | Loss: 0.00001959
Iteration 539/1000 | Loss: 0.00001959
Iteration 540/1000 | Loss: 0.00001959
Iteration 541/1000 | Loss: 0.00001959
Iteration 542/1000 | Loss: 0.00001959
Iteration 543/1000 | Loss: 0.00001959
Iteration 544/1000 | Loss: 0.00001959
Iteration 545/1000 | Loss: 0.00001959
Iteration 546/1000 | Loss: 0.00001959
Iteration 547/1000 | Loss: 0.00001959
Iteration 548/1000 | Loss: 0.00001959
Iteration 549/1000 | Loss: 0.00001959
Iteration 550/1000 | Loss: 0.00001959
Iteration 551/1000 | Loss: 0.00001959
Iteration 552/1000 | Loss: 0.00001959
Iteration 553/1000 | Loss: 0.00001959
Iteration 554/1000 | Loss: 0.00001959
Iteration 555/1000 | Loss: 0.00001959
Iteration 556/1000 | Loss: 0.00001959
Iteration 557/1000 | Loss: 0.00001959
Iteration 558/1000 | Loss: 0.00001959
Iteration 559/1000 | Loss: 0.00001959
Iteration 560/1000 | Loss: 0.00001959
Iteration 561/1000 | Loss: 0.00001959
Iteration 562/1000 | Loss: 0.00001959
Iteration 563/1000 | Loss: 0.00001959
Iteration 564/1000 | Loss: 0.00001959
Iteration 565/1000 | Loss: 0.00001959
Iteration 566/1000 | Loss: 0.00001959
Iteration 567/1000 | Loss: 0.00001959
Iteration 568/1000 | Loss: 0.00001959
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 568. Stopping optimization.
Last 5 losses: [1.9585628251661547e-05, 1.9585628251661547e-05, 1.9585628251661547e-05, 1.9585628251661547e-05, 1.9585628251661547e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9585628251661547e-05

Optimization complete. Final v2v error: 3.5809082984924316 mm

Highest mean error: 7.061325550079346 mm for frame 56

Lowest mean error: 3.1296770572662354 mm for frame 133

Saving results

Total time: 601.6170773506165
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00406787
Iteration 2/25 | Loss: 0.00131939
Iteration 3/25 | Loss: 0.00122650
Iteration 4/25 | Loss: 0.00121924
Iteration 5/25 | Loss: 0.00121680
Iteration 6/25 | Loss: 0.00121628
Iteration 7/25 | Loss: 0.00121628
Iteration 8/25 | Loss: 0.00121628
Iteration 9/25 | Loss: 0.00121628
Iteration 10/25 | Loss: 0.00121628
Iteration 11/25 | Loss: 0.00121628
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012162752682343125, 0.0012162752682343125, 0.0012162752682343125, 0.0012162752682343125, 0.0012162752682343125]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012162752682343125

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42854786
Iteration 2/25 | Loss: 0.00060588
Iteration 3/25 | Loss: 0.00060588
Iteration 4/25 | Loss: 0.00060587
Iteration 5/25 | Loss: 0.00060587
Iteration 6/25 | Loss: 0.00060587
Iteration 7/25 | Loss: 0.00060587
Iteration 8/25 | Loss: 0.00060587
Iteration 9/25 | Loss: 0.00060587
Iteration 10/25 | Loss: 0.00060587
Iteration 11/25 | Loss: 0.00060587
Iteration 12/25 | Loss: 0.00060587
Iteration 13/25 | Loss: 0.00060587
Iteration 14/25 | Loss: 0.00060587
Iteration 15/25 | Loss: 0.00060587
Iteration 16/25 | Loss: 0.00060587
Iteration 17/25 | Loss: 0.00060587
Iteration 18/25 | Loss: 0.00060587
Iteration 19/25 | Loss: 0.00060587
Iteration 20/25 | Loss: 0.00060587
Iteration 21/25 | Loss: 0.00060587
Iteration 22/25 | Loss: 0.00060587
Iteration 23/25 | Loss: 0.00060587
Iteration 24/25 | Loss: 0.00060587
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0006058714934624732, 0.0006058714934624732, 0.0006058714934624732, 0.0006058714934624732, 0.0006058714934624732]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006058714934624732

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060587
Iteration 2/1000 | Loss: 0.00003952
Iteration 3/1000 | Loss: 0.00002551
Iteration 4/1000 | Loss: 0.00002131
Iteration 5/1000 | Loss: 0.00001959
Iteration 6/1000 | Loss: 0.00001849
Iteration 7/1000 | Loss: 0.00001771
Iteration 8/1000 | Loss: 0.00001715
Iteration 9/1000 | Loss: 0.00001670
Iteration 10/1000 | Loss: 0.00001646
Iteration 11/1000 | Loss: 0.00001626
Iteration 12/1000 | Loss: 0.00001617
Iteration 13/1000 | Loss: 0.00001614
Iteration 14/1000 | Loss: 0.00001613
Iteration 15/1000 | Loss: 0.00001613
Iteration 16/1000 | Loss: 0.00001612
Iteration 17/1000 | Loss: 0.00001608
Iteration 18/1000 | Loss: 0.00001604
Iteration 19/1000 | Loss: 0.00001601
Iteration 20/1000 | Loss: 0.00001600
Iteration 21/1000 | Loss: 0.00001599
Iteration 22/1000 | Loss: 0.00001598
Iteration 23/1000 | Loss: 0.00001598
Iteration 24/1000 | Loss: 0.00001597
Iteration 25/1000 | Loss: 0.00001595
Iteration 26/1000 | Loss: 0.00001595
Iteration 27/1000 | Loss: 0.00001595
Iteration 28/1000 | Loss: 0.00001594
Iteration 29/1000 | Loss: 0.00001594
Iteration 30/1000 | Loss: 0.00001593
Iteration 31/1000 | Loss: 0.00001593
Iteration 32/1000 | Loss: 0.00001593
Iteration 33/1000 | Loss: 0.00001592
Iteration 34/1000 | Loss: 0.00001592
Iteration 35/1000 | Loss: 0.00001591
Iteration 36/1000 | Loss: 0.00001591
Iteration 37/1000 | Loss: 0.00001591
Iteration 38/1000 | Loss: 0.00001591
Iteration 39/1000 | Loss: 0.00001590
Iteration 40/1000 | Loss: 0.00001587
Iteration 41/1000 | Loss: 0.00001586
Iteration 42/1000 | Loss: 0.00001585
Iteration 43/1000 | Loss: 0.00001584
Iteration 44/1000 | Loss: 0.00001584
Iteration 45/1000 | Loss: 0.00001584
Iteration 46/1000 | Loss: 0.00001583
Iteration 47/1000 | Loss: 0.00001583
Iteration 48/1000 | Loss: 0.00001583
Iteration 49/1000 | Loss: 0.00001582
Iteration 50/1000 | Loss: 0.00001582
Iteration 51/1000 | Loss: 0.00001582
Iteration 52/1000 | Loss: 0.00001581
Iteration 53/1000 | Loss: 0.00001581
Iteration 54/1000 | Loss: 0.00001581
Iteration 55/1000 | Loss: 0.00001581
Iteration 56/1000 | Loss: 0.00001581
Iteration 57/1000 | Loss: 0.00001581
Iteration 58/1000 | Loss: 0.00001581
Iteration 59/1000 | Loss: 0.00001581
Iteration 60/1000 | Loss: 0.00001581
Iteration 61/1000 | Loss: 0.00001581
Iteration 62/1000 | Loss: 0.00001580
Iteration 63/1000 | Loss: 0.00001580
Iteration 64/1000 | Loss: 0.00001580
Iteration 65/1000 | Loss: 0.00001580
Iteration 66/1000 | Loss: 0.00001580
Iteration 67/1000 | Loss: 0.00001579
Iteration 68/1000 | Loss: 0.00001579
Iteration 69/1000 | Loss: 0.00001579
Iteration 70/1000 | Loss: 0.00001579
Iteration 71/1000 | Loss: 0.00001579
Iteration 72/1000 | Loss: 0.00001579
Iteration 73/1000 | Loss: 0.00001579
Iteration 74/1000 | Loss: 0.00001579
Iteration 75/1000 | Loss: 0.00001579
Iteration 76/1000 | Loss: 0.00001579
Iteration 77/1000 | Loss: 0.00001579
Iteration 78/1000 | Loss: 0.00001578
Iteration 79/1000 | Loss: 0.00001578
Iteration 80/1000 | Loss: 0.00001578
Iteration 81/1000 | Loss: 0.00001578
Iteration 82/1000 | Loss: 0.00001578
Iteration 83/1000 | Loss: 0.00001578
Iteration 84/1000 | Loss: 0.00001578
Iteration 85/1000 | Loss: 0.00001578
Iteration 86/1000 | Loss: 0.00001578
Iteration 87/1000 | Loss: 0.00001578
Iteration 88/1000 | Loss: 0.00001577
Iteration 89/1000 | Loss: 0.00001577
Iteration 90/1000 | Loss: 0.00001577
Iteration 91/1000 | Loss: 0.00001577
Iteration 92/1000 | Loss: 0.00001577
Iteration 93/1000 | Loss: 0.00001577
Iteration 94/1000 | Loss: 0.00001577
Iteration 95/1000 | Loss: 0.00001577
Iteration 96/1000 | Loss: 0.00001577
Iteration 97/1000 | Loss: 0.00001577
Iteration 98/1000 | Loss: 0.00001577
Iteration 99/1000 | Loss: 0.00001577
Iteration 100/1000 | Loss: 0.00001577
Iteration 101/1000 | Loss: 0.00001577
Iteration 102/1000 | Loss: 0.00001577
Iteration 103/1000 | Loss: 0.00001577
Iteration 104/1000 | Loss: 0.00001577
Iteration 105/1000 | Loss: 0.00001577
Iteration 106/1000 | Loss: 0.00001577
Iteration 107/1000 | Loss: 0.00001577
Iteration 108/1000 | Loss: 0.00001577
Iteration 109/1000 | Loss: 0.00001577
Iteration 110/1000 | Loss: 0.00001577
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [1.5765375792398117e-05, 1.5765375792398117e-05, 1.5765375792398117e-05, 1.5765375792398117e-05, 1.5765375792398117e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5765375792398117e-05

Optimization complete. Final v2v error: 3.3358802795410156 mm

Highest mean error: 3.9201741218566895 mm for frame 67

Lowest mean error: 2.9696121215820312 mm for frame 12

Saving results

Total time: 32.5953106880188
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00576143
Iteration 2/25 | Loss: 0.00160997
Iteration 3/25 | Loss: 0.00135198
Iteration 4/25 | Loss: 0.00133107
Iteration 5/25 | Loss: 0.00132694
Iteration 6/25 | Loss: 0.00132593
Iteration 7/25 | Loss: 0.00132593
Iteration 8/25 | Loss: 0.00132593
Iteration 9/25 | Loss: 0.00132593
Iteration 10/25 | Loss: 0.00132593
Iteration 11/25 | Loss: 0.00132593
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013259256957098842, 0.0013259256957098842, 0.0013259256957098842, 0.0013259256957098842, 0.0013259256957098842]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013259256957098842

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65471411
Iteration 2/25 | Loss: 0.00080051
Iteration 3/25 | Loss: 0.00080048
Iteration 4/25 | Loss: 0.00080048
Iteration 5/25 | Loss: 0.00080048
Iteration 6/25 | Loss: 0.00080048
Iteration 7/25 | Loss: 0.00080048
Iteration 8/25 | Loss: 0.00080048
Iteration 9/25 | Loss: 0.00080048
Iteration 10/25 | Loss: 0.00080048
Iteration 11/25 | Loss: 0.00080048
Iteration 12/25 | Loss: 0.00080048
Iteration 13/25 | Loss: 0.00080048
Iteration 14/25 | Loss: 0.00080048
Iteration 15/25 | Loss: 0.00080048
Iteration 16/25 | Loss: 0.00080048
Iteration 17/25 | Loss: 0.00080048
Iteration 18/25 | Loss: 0.00080048
Iteration 19/25 | Loss: 0.00080048
Iteration 20/25 | Loss: 0.00080048
Iteration 21/25 | Loss: 0.00080048
Iteration 22/25 | Loss: 0.00080048
Iteration 23/25 | Loss: 0.00080048
Iteration 24/25 | Loss: 0.00080048
Iteration 25/25 | Loss: 0.00080048
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.000800478970631957, 0.000800478970631957, 0.000800478970631957, 0.000800478970631957, 0.000800478970631957]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000800478970631957

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080048
Iteration 2/1000 | Loss: 0.00004697
Iteration 3/1000 | Loss: 0.00003409
Iteration 4/1000 | Loss: 0.00002941
Iteration 5/1000 | Loss: 0.00002752
Iteration 6/1000 | Loss: 0.00002610
Iteration 7/1000 | Loss: 0.00002519
Iteration 8/1000 | Loss: 0.00002458
Iteration 9/1000 | Loss: 0.00002416
Iteration 10/1000 | Loss: 0.00002373
Iteration 11/1000 | Loss: 0.00002339
Iteration 12/1000 | Loss: 0.00002311
Iteration 13/1000 | Loss: 0.00002289
Iteration 14/1000 | Loss: 0.00002265
Iteration 15/1000 | Loss: 0.00002248
Iteration 16/1000 | Loss: 0.00002243
Iteration 17/1000 | Loss: 0.00002234
Iteration 18/1000 | Loss: 0.00002220
Iteration 19/1000 | Loss: 0.00002216
Iteration 20/1000 | Loss: 0.00002212
Iteration 21/1000 | Loss: 0.00002207
Iteration 22/1000 | Loss: 0.00002206
Iteration 23/1000 | Loss: 0.00002205
Iteration 24/1000 | Loss: 0.00002204
Iteration 25/1000 | Loss: 0.00002203
Iteration 26/1000 | Loss: 0.00002203
Iteration 27/1000 | Loss: 0.00002202
Iteration 28/1000 | Loss: 0.00002202
Iteration 29/1000 | Loss: 0.00002202
Iteration 30/1000 | Loss: 0.00002201
Iteration 31/1000 | Loss: 0.00002201
Iteration 32/1000 | Loss: 0.00002200
Iteration 33/1000 | Loss: 0.00002200
Iteration 34/1000 | Loss: 0.00002200
Iteration 35/1000 | Loss: 0.00002200
Iteration 36/1000 | Loss: 0.00002200
Iteration 37/1000 | Loss: 0.00002200
Iteration 38/1000 | Loss: 0.00002200
Iteration 39/1000 | Loss: 0.00002199
Iteration 40/1000 | Loss: 0.00002199
Iteration 41/1000 | Loss: 0.00002199
Iteration 42/1000 | Loss: 0.00002199
Iteration 43/1000 | Loss: 0.00002199
Iteration 44/1000 | Loss: 0.00002198
Iteration 45/1000 | Loss: 0.00002198
Iteration 46/1000 | Loss: 0.00002198
Iteration 47/1000 | Loss: 0.00002197
Iteration 48/1000 | Loss: 0.00002197
Iteration 49/1000 | Loss: 0.00002197
Iteration 50/1000 | Loss: 0.00002196
Iteration 51/1000 | Loss: 0.00002196
Iteration 52/1000 | Loss: 0.00002196
Iteration 53/1000 | Loss: 0.00002196
Iteration 54/1000 | Loss: 0.00002196
Iteration 55/1000 | Loss: 0.00002195
Iteration 56/1000 | Loss: 0.00002195
Iteration 57/1000 | Loss: 0.00002195
Iteration 58/1000 | Loss: 0.00002195
Iteration 59/1000 | Loss: 0.00002195
Iteration 60/1000 | Loss: 0.00002194
Iteration 61/1000 | Loss: 0.00002194
Iteration 62/1000 | Loss: 0.00002194
Iteration 63/1000 | Loss: 0.00002194
Iteration 64/1000 | Loss: 0.00002194
Iteration 65/1000 | Loss: 0.00002194
Iteration 66/1000 | Loss: 0.00002194
Iteration 67/1000 | Loss: 0.00002194
Iteration 68/1000 | Loss: 0.00002194
Iteration 69/1000 | Loss: 0.00002194
Iteration 70/1000 | Loss: 0.00002193
Iteration 71/1000 | Loss: 0.00002193
Iteration 72/1000 | Loss: 0.00002193
Iteration 73/1000 | Loss: 0.00002193
Iteration 74/1000 | Loss: 0.00002193
Iteration 75/1000 | Loss: 0.00002193
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 75. Stopping optimization.
Last 5 losses: [2.1934556571068242e-05, 2.1934556571068242e-05, 2.1934556571068242e-05, 2.1934556571068242e-05, 2.1934556571068242e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1934556571068242e-05

Optimization complete. Final v2v error: 3.8840651512145996 mm

Highest mean error: 4.385139465332031 mm for frame 117

Lowest mean error: 3.15946888923645 mm for frame 11

Saving results

Total time: 40.74143600463867
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00979917
Iteration 2/25 | Loss: 0.00361244
Iteration 3/25 | Loss: 0.00233899
Iteration 4/25 | Loss: 0.00209334
Iteration 5/25 | Loss: 0.00187359
Iteration 6/25 | Loss: 0.00179470
Iteration 7/25 | Loss: 0.00176201
Iteration 8/25 | Loss: 0.00166198
Iteration 9/25 | Loss: 0.00162786
Iteration 10/25 | Loss: 0.00158470
Iteration 11/25 | Loss: 0.00160064
Iteration 12/25 | Loss: 0.00157995
Iteration 13/25 | Loss: 0.00153947
Iteration 14/25 | Loss: 0.00151666
Iteration 15/25 | Loss: 0.00152119
Iteration 16/25 | Loss: 0.00149624
Iteration 17/25 | Loss: 0.00148285
Iteration 18/25 | Loss: 0.00147198
Iteration 19/25 | Loss: 0.00146413
Iteration 20/25 | Loss: 0.00145501
Iteration 21/25 | Loss: 0.00145260
Iteration 22/25 | Loss: 0.00145060
Iteration 23/25 | Loss: 0.00144409
Iteration 24/25 | Loss: 0.00143681
Iteration 25/25 | Loss: 0.00143227

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48947036
Iteration 2/25 | Loss: 0.00937504
Iteration 3/25 | Loss: 0.03268576
Iteration 4/25 | Loss: 0.03394654
Iteration 5/25 | Loss: 0.01849123
Iteration 6/25 | Loss: 0.00341692
Iteration 7/25 | Loss: 0.00238093
Iteration 8/25 | Loss: 0.00200728
Iteration 9/25 | Loss: 0.00200728
Iteration 10/25 | Loss: 0.00200728
Iteration 11/25 | Loss: 0.00200728
Iteration 12/25 | Loss: 0.00200728
Iteration 13/25 | Loss: 0.00200728
Iteration 14/25 | Loss: 0.00200728
Iteration 15/25 | Loss: 0.00200728
Iteration 16/25 | Loss: 0.00200728
Iteration 17/25 | Loss: 0.00200728
Iteration 18/25 | Loss: 0.00200728
Iteration 19/25 | Loss: 0.00200728
Iteration 20/25 | Loss: 0.00200728
Iteration 21/25 | Loss: 0.00200728
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.002007280243560672, 0.002007280243560672, 0.002007280243560672, 0.002007280243560672, 0.002007280243560672]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002007280243560672

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00200728
Iteration 2/1000 | Loss: 0.00063357
Iteration 3/1000 | Loss: 0.00281468
Iteration 4/1000 | Loss: 0.00057251
Iteration 5/1000 | Loss: 0.00227676
Iteration 6/1000 | Loss: 0.00021147
Iteration 7/1000 | Loss: 0.00036670
Iteration 8/1000 | Loss: 0.00078756
Iteration 9/1000 | Loss: 0.00018354
Iteration 10/1000 | Loss: 0.00059101
Iteration 11/1000 | Loss: 0.00087330
Iteration 12/1000 | Loss: 0.00074376
Iteration 13/1000 | Loss: 0.00036331
Iteration 14/1000 | Loss: 0.00052042
Iteration 15/1000 | Loss: 0.00038655
Iteration 16/1000 | Loss: 0.00046058
Iteration 17/1000 | Loss: 0.00064227
Iteration 18/1000 | Loss: 0.00029976
Iteration 19/1000 | Loss: 0.00053466
Iteration 20/1000 | Loss: 0.00033707
Iteration 21/1000 | Loss: 0.00023847
Iteration 22/1000 | Loss: 0.00024587
Iteration 23/1000 | Loss: 0.00031763
Iteration 24/1000 | Loss: 0.00037820
Iteration 25/1000 | Loss: 0.00031351
Iteration 26/1000 | Loss: 0.00053873
Iteration 27/1000 | Loss: 0.00069499
Iteration 28/1000 | Loss: 0.00014985
Iteration 29/1000 | Loss: 0.00024088
Iteration 30/1000 | Loss: 0.00102586
Iteration 31/1000 | Loss: 0.00054445
Iteration 32/1000 | Loss: 0.00088668
Iteration 33/1000 | Loss: 0.00218481
Iteration 34/1000 | Loss: 0.00034471
Iteration 35/1000 | Loss: 0.00039496
Iteration 36/1000 | Loss: 0.00034240
Iteration 37/1000 | Loss: 0.00043830
Iteration 38/1000 | Loss: 0.00031565
Iteration 39/1000 | Loss: 0.00012079
Iteration 40/1000 | Loss: 0.00011609
Iteration 41/1000 | Loss: 0.00135625
Iteration 42/1000 | Loss: 0.00352405
Iteration 43/1000 | Loss: 0.00463677
Iteration 44/1000 | Loss: 0.00129844
Iteration 45/1000 | Loss: 0.00120878
Iteration 46/1000 | Loss: 0.00103767
Iteration 47/1000 | Loss: 0.00195928
Iteration 48/1000 | Loss: 0.00140022
Iteration 49/1000 | Loss: 0.00079229
Iteration 50/1000 | Loss: 0.00020484
Iteration 51/1000 | Loss: 0.00029868
Iteration 52/1000 | Loss: 0.00106200
Iteration 53/1000 | Loss: 0.00056700
Iteration 54/1000 | Loss: 0.00027918
Iteration 55/1000 | Loss: 0.00089946
Iteration 56/1000 | Loss: 0.00070835
Iteration 57/1000 | Loss: 0.00101075
Iteration 58/1000 | Loss: 0.00048520
Iteration 59/1000 | Loss: 0.00065108
Iteration 60/1000 | Loss: 0.00046640
Iteration 61/1000 | Loss: 0.00069817
Iteration 62/1000 | Loss: 0.00044554
Iteration 63/1000 | Loss: 0.00064636
Iteration 64/1000 | Loss: 0.00042277
Iteration 65/1000 | Loss: 0.00048426
Iteration 66/1000 | Loss: 0.00044140
Iteration 67/1000 | Loss: 0.00061327
Iteration 68/1000 | Loss: 0.00013623
Iteration 69/1000 | Loss: 0.00125748
Iteration 70/1000 | Loss: 0.00051335
Iteration 71/1000 | Loss: 0.00160791
Iteration 72/1000 | Loss: 0.00048937
Iteration 73/1000 | Loss: 0.00042318
Iteration 74/1000 | Loss: 0.00021823
Iteration 75/1000 | Loss: 0.00041122
Iteration 76/1000 | Loss: 0.00047218
Iteration 77/1000 | Loss: 0.00025054
Iteration 78/1000 | Loss: 0.00041444
Iteration 79/1000 | Loss: 0.00128922
Iteration 80/1000 | Loss: 0.00133907
Iteration 81/1000 | Loss: 0.00050609
Iteration 82/1000 | Loss: 0.00131637
Iteration 83/1000 | Loss: 0.00089872
Iteration 84/1000 | Loss: 0.00041488
Iteration 85/1000 | Loss: 0.00012625
Iteration 86/1000 | Loss: 0.00021417
Iteration 87/1000 | Loss: 0.00076438
Iteration 88/1000 | Loss: 0.00107297
Iteration 89/1000 | Loss: 0.00045571
Iteration 90/1000 | Loss: 0.00039796
Iteration 91/1000 | Loss: 0.00057605
Iteration 92/1000 | Loss: 0.00037785
Iteration 93/1000 | Loss: 0.00042671
Iteration 94/1000 | Loss: 0.00041383
Iteration 95/1000 | Loss: 0.00046141
Iteration 96/1000 | Loss: 0.00065679
Iteration 97/1000 | Loss: 0.00041859
Iteration 98/1000 | Loss: 0.00020904
Iteration 99/1000 | Loss: 0.00009816
Iteration 100/1000 | Loss: 0.00021763
Iteration 101/1000 | Loss: 0.00019674
Iteration 102/1000 | Loss: 0.00263029
Iteration 103/1000 | Loss: 0.00153610
Iteration 104/1000 | Loss: 0.00085393
Iteration 105/1000 | Loss: 0.00227549
Iteration 106/1000 | Loss: 0.00055187
Iteration 107/1000 | Loss: 0.00038686
Iteration 108/1000 | Loss: 0.00030596
Iteration 109/1000 | Loss: 0.00010550
Iteration 110/1000 | Loss: 0.00032168
Iteration 111/1000 | Loss: 0.00055209
Iteration 112/1000 | Loss: 0.00014019
Iteration 113/1000 | Loss: 0.00030267
Iteration 114/1000 | Loss: 0.00043261
Iteration 115/1000 | Loss: 0.00017008
Iteration 116/1000 | Loss: 0.00030321
Iteration 117/1000 | Loss: 0.00008978
Iteration 118/1000 | Loss: 0.00019973
Iteration 119/1000 | Loss: 0.00012334
Iteration 120/1000 | Loss: 0.00008203
Iteration 121/1000 | Loss: 0.00013143
Iteration 122/1000 | Loss: 0.00138777
Iteration 123/1000 | Loss: 0.00166067
Iteration 124/1000 | Loss: 0.00050229
Iteration 125/1000 | Loss: 0.00080914
Iteration 126/1000 | Loss: 0.00049294
Iteration 127/1000 | Loss: 0.00087581
Iteration 128/1000 | Loss: 0.00072558
Iteration 129/1000 | Loss: 0.00007714
Iteration 130/1000 | Loss: 0.00021888
Iteration 131/1000 | Loss: 0.00070072
Iteration 132/1000 | Loss: 0.00007487
Iteration 133/1000 | Loss: 0.00007121
Iteration 134/1000 | Loss: 0.00009222
Iteration 135/1000 | Loss: 0.00017652
Iteration 136/1000 | Loss: 0.00006814
Iteration 137/1000 | Loss: 0.00066657
Iteration 138/1000 | Loss: 0.00020031
Iteration 139/1000 | Loss: 0.00023912
Iteration 140/1000 | Loss: 0.00130604
Iteration 141/1000 | Loss: 0.00029690
Iteration 142/1000 | Loss: 0.00009813
Iteration 143/1000 | Loss: 0.00064069
Iteration 144/1000 | Loss: 0.00011027
Iteration 145/1000 | Loss: 0.00007065
Iteration 146/1000 | Loss: 0.00165726
Iteration 147/1000 | Loss: 0.00006003
Iteration 148/1000 | Loss: 0.00013213
Iteration 149/1000 | Loss: 0.00005634
Iteration 150/1000 | Loss: 0.00028341
Iteration 151/1000 | Loss: 0.00021654
Iteration 152/1000 | Loss: 0.00005802
Iteration 153/1000 | Loss: 0.00006817
Iteration 154/1000 | Loss: 0.00004976
Iteration 155/1000 | Loss: 0.00004885
Iteration 156/1000 | Loss: 0.00004820
Iteration 157/1000 | Loss: 0.00004774
Iteration 158/1000 | Loss: 0.00007326
Iteration 159/1000 | Loss: 0.00041863
Iteration 160/1000 | Loss: 0.00061881
Iteration 161/1000 | Loss: 0.00004811
Iteration 162/1000 | Loss: 0.00004712
Iteration 163/1000 | Loss: 0.00004673
Iteration 164/1000 | Loss: 0.00015028
Iteration 165/1000 | Loss: 0.00061002
Iteration 166/1000 | Loss: 0.00010478
Iteration 167/1000 | Loss: 0.00004731
Iteration 168/1000 | Loss: 0.00004618
Iteration 169/1000 | Loss: 0.00004565
Iteration 170/1000 | Loss: 0.00010787
Iteration 171/1000 | Loss: 0.00004525
Iteration 172/1000 | Loss: 0.00006512
Iteration 173/1000 | Loss: 0.00004497
Iteration 174/1000 | Loss: 0.00004464
Iteration 175/1000 | Loss: 0.00004453
Iteration 176/1000 | Loss: 0.00004441
Iteration 177/1000 | Loss: 0.00004427
Iteration 178/1000 | Loss: 0.00004416
Iteration 179/1000 | Loss: 0.00004410
Iteration 180/1000 | Loss: 0.00004406
Iteration 181/1000 | Loss: 0.00004405
Iteration 182/1000 | Loss: 0.00004404
Iteration 183/1000 | Loss: 0.00006253
Iteration 184/1000 | Loss: 0.00018711
Iteration 185/1000 | Loss: 0.00004552
Iteration 186/1000 | Loss: 0.00004353
Iteration 187/1000 | Loss: 0.00004298
Iteration 188/1000 | Loss: 0.00004265
Iteration 189/1000 | Loss: 0.00012047
Iteration 190/1000 | Loss: 0.00004438
Iteration 191/1000 | Loss: 0.00004545
Iteration 192/1000 | Loss: 0.00004189
Iteration 193/1000 | Loss: 0.00004177
Iteration 194/1000 | Loss: 0.00004422
Iteration 195/1000 | Loss: 0.00004172
Iteration 196/1000 | Loss: 0.00004172
Iteration 197/1000 | Loss: 0.00004172
Iteration 198/1000 | Loss: 0.00004172
Iteration 199/1000 | Loss: 0.00004172
Iteration 200/1000 | Loss: 0.00004171
Iteration 201/1000 | Loss: 0.00004171
Iteration 202/1000 | Loss: 0.00004171
Iteration 203/1000 | Loss: 0.00004171
Iteration 204/1000 | Loss: 0.00004169
Iteration 205/1000 | Loss: 0.00004168
Iteration 206/1000 | Loss: 0.00004159
Iteration 207/1000 | Loss: 0.00004158
Iteration 208/1000 | Loss: 0.00004158
Iteration 209/1000 | Loss: 0.00005880
Iteration 210/1000 | Loss: 0.00004154
Iteration 211/1000 | Loss: 0.00004153
Iteration 212/1000 | Loss: 0.00004152
Iteration 213/1000 | Loss: 0.00004151
Iteration 214/1000 | Loss: 0.00004151
Iteration 215/1000 | Loss: 0.00012690
Iteration 216/1000 | Loss: 0.00011016
Iteration 217/1000 | Loss: 0.00004188
Iteration 218/1000 | Loss: 0.00010502
Iteration 219/1000 | Loss: 0.00058954
Iteration 220/1000 | Loss: 0.00004850
Iteration 221/1000 | Loss: 0.00004247
Iteration 222/1000 | Loss: 0.00004088
Iteration 223/1000 | Loss: 0.00009233
Iteration 224/1000 | Loss: 0.00050451
Iteration 225/1000 | Loss: 0.00003996
Iteration 226/1000 | Loss: 0.00003990
Iteration 227/1000 | Loss: 0.00008426
Iteration 228/1000 | Loss: 0.00004027
Iteration 229/1000 | Loss: 0.00003973
Iteration 230/1000 | Loss: 0.00003968
Iteration 231/1000 | Loss: 0.00003967
Iteration 232/1000 | Loss: 0.00003965
Iteration 233/1000 | Loss: 0.00003964
Iteration 234/1000 | Loss: 0.00003964
Iteration 235/1000 | Loss: 0.00003964
Iteration 236/1000 | Loss: 0.00003963
Iteration 237/1000 | Loss: 0.00004665
Iteration 238/1000 | Loss: 0.00004026
Iteration 239/1000 | Loss: 0.00004020
Iteration 240/1000 | Loss: 0.00003961
Iteration 241/1000 | Loss: 0.00003961
Iteration 242/1000 | Loss: 0.00003961
Iteration 243/1000 | Loss: 0.00003961
Iteration 244/1000 | Loss: 0.00003961
Iteration 245/1000 | Loss: 0.00003961
Iteration 246/1000 | Loss: 0.00003961
Iteration 247/1000 | Loss: 0.00003961
Iteration 248/1000 | Loss: 0.00003961
Iteration 249/1000 | Loss: 0.00003961
Iteration 250/1000 | Loss: 0.00003961
Iteration 251/1000 | Loss: 0.00003961
Iteration 252/1000 | Loss: 0.00003961
Iteration 253/1000 | Loss: 0.00003960
Iteration 254/1000 | Loss: 0.00003960
Iteration 255/1000 | Loss: 0.00003960
Iteration 256/1000 | Loss: 0.00003960
Iteration 257/1000 | Loss: 0.00003960
Iteration 258/1000 | Loss: 0.00003960
Iteration 259/1000 | Loss: 0.00003960
Iteration 260/1000 | Loss: 0.00003959
Iteration 261/1000 | Loss: 0.00003959
Iteration 262/1000 | Loss: 0.00003959
Iteration 263/1000 | Loss: 0.00003959
Iteration 264/1000 | Loss: 0.00003959
Iteration 265/1000 | Loss: 0.00003959
Iteration 266/1000 | Loss: 0.00003959
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 266. Stopping optimization.
Last 5 losses: [3.959451714763418e-05, 3.959451714763418e-05, 3.959451714763418e-05, 3.959451714763418e-05, 3.959451714763418e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.959451714763418e-05

Optimization complete. Final v2v error: 4.924416542053223 mm

Highest mean error: 12.09028148651123 mm for frame 128

Lowest mean error: 3.485456705093384 mm for frame 110

Saving results

Total time: 400.6706371307373
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00396362
Iteration 2/25 | Loss: 0.00127990
Iteration 3/25 | Loss: 0.00121011
Iteration 4/25 | Loss: 0.00120042
Iteration 5/25 | Loss: 0.00119758
Iteration 6/25 | Loss: 0.00119730
Iteration 7/25 | Loss: 0.00119730
Iteration 8/25 | Loss: 0.00119730
Iteration 9/25 | Loss: 0.00119730
Iteration 10/25 | Loss: 0.00119730
Iteration 11/25 | Loss: 0.00119730
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011972957290709019, 0.0011972957290709019, 0.0011972957290709019, 0.0011972957290709019, 0.0011972957290709019]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011972957290709019

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45365596
Iteration 2/25 | Loss: 0.00077108
Iteration 3/25 | Loss: 0.00077108
Iteration 4/25 | Loss: 0.00077108
Iteration 5/25 | Loss: 0.00077108
Iteration 6/25 | Loss: 0.00077108
Iteration 7/25 | Loss: 0.00077108
Iteration 8/25 | Loss: 0.00077108
Iteration 9/25 | Loss: 0.00077108
Iteration 10/25 | Loss: 0.00077108
Iteration 11/25 | Loss: 0.00077108
Iteration 12/25 | Loss: 0.00077108
Iteration 13/25 | Loss: 0.00077108
Iteration 14/25 | Loss: 0.00077108
Iteration 15/25 | Loss: 0.00077108
Iteration 16/25 | Loss: 0.00077108
Iteration 17/25 | Loss: 0.00077108
Iteration 18/25 | Loss: 0.00077108
Iteration 19/25 | Loss: 0.00077108
Iteration 20/25 | Loss: 0.00077108
Iteration 21/25 | Loss: 0.00077108
Iteration 22/25 | Loss: 0.00077108
Iteration 23/25 | Loss: 0.00077108
Iteration 24/25 | Loss: 0.00077108
Iteration 25/25 | Loss: 0.00077108

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077108
Iteration 2/1000 | Loss: 0.00002449
Iteration 3/1000 | Loss: 0.00001733
Iteration 4/1000 | Loss: 0.00001468
Iteration 5/1000 | Loss: 0.00001374
Iteration 6/1000 | Loss: 0.00001312
Iteration 7/1000 | Loss: 0.00001263
Iteration 8/1000 | Loss: 0.00001237
Iteration 9/1000 | Loss: 0.00001206
Iteration 10/1000 | Loss: 0.00001192
Iteration 11/1000 | Loss: 0.00001183
Iteration 12/1000 | Loss: 0.00001179
Iteration 13/1000 | Loss: 0.00001174
Iteration 14/1000 | Loss: 0.00001173
Iteration 15/1000 | Loss: 0.00001167
Iteration 16/1000 | Loss: 0.00001165
Iteration 17/1000 | Loss: 0.00001164
Iteration 18/1000 | Loss: 0.00001164
Iteration 19/1000 | Loss: 0.00001163
Iteration 20/1000 | Loss: 0.00001158
Iteration 21/1000 | Loss: 0.00001158
Iteration 22/1000 | Loss: 0.00001157
Iteration 23/1000 | Loss: 0.00001157
Iteration 24/1000 | Loss: 0.00001156
Iteration 25/1000 | Loss: 0.00001155
Iteration 26/1000 | Loss: 0.00001154
Iteration 27/1000 | Loss: 0.00001154
Iteration 28/1000 | Loss: 0.00001153
Iteration 29/1000 | Loss: 0.00001152
Iteration 30/1000 | Loss: 0.00001151
Iteration 31/1000 | Loss: 0.00001151
Iteration 32/1000 | Loss: 0.00001151
Iteration 33/1000 | Loss: 0.00001151
Iteration 34/1000 | Loss: 0.00001150
Iteration 35/1000 | Loss: 0.00001150
Iteration 36/1000 | Loss: 0.00001150
Iteration 37/1000 | Loss: 0.00001149
Iteration 38/1000 | Loss: 0.00001149
Iteration 39/1000 | Loss: 0.00001148
Iteration 40/1000 | Loss: 0.00001148
Iteration 41/1000 | Loss: 0.00001147
Iteration 42/1000 | Loss: 0.00001147
Iteration 43/1000 | Loss: 0.00001147
Iteration 44/1000 | Loss: 0.00001146
Iteration 45/1000 | Loss: 0.00001146
Iteration 46/1000 | Loss: 0.00001146
Iteration 47/1000 | Loss: 0.00001145
Iteration 48/1000 | Loss: 0.00001145
Iteration 49/1000 | Loss: 0.00001145
Iteration 50/1000 | Loss: 0.00001143
Iteration 51/1000 | Loss: 0.00001143
Iteration 52/1000 | Loss: 0.00001143
Iteration 53/1000 | Loss: 0.00001143
Iteration 54/1000 | Loss: 0.00001142
Iteration 55/1000 | Loss: 0.00001142
Iteration 56/1000 | Loss: 0.00001142
Iteration 57/1000 | Loss: 0.00001142
Iteration 58/1000 | Loss: 0.00001142
Iteration 59/1000 | Loss: 0.00001142
Iteration 60/1000 | Loss: 0.00001142
Iteration 61/1000 | Loss: 0.00001142
Iteration 62/1000 | Loss: 0.00001141
Iteration 63/1000 | Loss: 0.00001141
Iteration 64/1000 | Loss: 0.00001141
Iteration 65/1000 | Loss: 0.00001140
Iteration 66/1000 | Loss: 0.00001140
Iteration 67/1000 | Loss: 0.00001140
Iteration 68/1000 | Loss: 0.00001140
Iteration 69/1000 | Loss: 0.00001139
Iteration 70/1000 | Loss: 0.00001139
Iteration 71/1000 | Loss: 0.00001138
Iteration 72/1000 | Loss: 0.00001138
Iteration 73/1000 | Loss: 0.00001138
Iteration 74/1000 | Loss: 0.00001137
Iteration 75/1000 | Loss: 0.00001137
Iteration 76/1000 | Loss: 0.00001137
Iteration 77/1000 | Loss: 0.00001137
Iteration 78/1000 | Loss: 0.00001136
Iteration 79/1000 | Loss: 0.00001135
Iteration 80/1000 | Loss: 0.00001135
Iteration 81/1000 | Loss: 0.00001134
Iteration 82/1000 | Loss: 0.00001134
Iteration 83/1000 | Loss: 0.00001134
Iteration 84/1000 | Loss: 0.00001133
Iteration 85/1000 | Loss: 0.00001132
Iteration 86/1000 | Loss: 0.00001131
Iteration 87/1000 | Loss: 0.00001131
Iteration 88/1000 | Loss: 0.00001130
Iteration 89/1000 | Loss: 0.00001130
Iteration 90/1000 | Loss: 0.00001130
Iteration 91/1000 | Loss: 0.00001129
Iteration 92/1000 | Loss: 0.00001129
Iteration 93/1000 | Loss: 0.00001128
Iteration 94/1000 | Loss: 0.00001127
Iteration 95/1000 | Loss: 0.00001126
Iteration 96/1000 | Loss: 0.00001126
Iteration 97/1000 | Loss: 0.00001126
Iteration 98/1000 | Loss: 0.00001126
Iteration 99/1000 | Loss: 0.00001126
Iteration 100/1000 | Loss: 0.00001126
Iteration 101/1000 | Loss: 0.00001125
Iteration 102/1000 | Loss: 0.00001124
Iteration 103/1000 | Loss: 0.00001124
Iteration 104/1000 | Loss: 0.00001124
Iteration 105/1000 | Loss: 0.00001123
Iteration 106/1000 | Loss: 0.00001123
Iteration 107/1000 | Loss: 0.00001123
Iteration 108/1000 | Loss: 0.00001122
Iteration 109/1000 | Loss: 0.00001122
Iteration 110/1000 | Loss: 0.00001122
Iteration 111/1000 | Loss: 0.00001122
Iteration 112/1000 | Loss: 0.00001121
Iteration 113/1000 | Loss: 0.00001121
Iteration 114/1000 | Loss: 0.00001120
Iteration 115/1000 | Loss: 0.00001120
Iteration 116/1000 | Loss: 0.00001120
Iteration 117/1000 | Loss: 0.00001119
Iteration 118/1000 | Loss: 0.00001119
Iteration 119/1000 | Loss: 0.00001119
Iteration 120/1000 | Loss: 0.00001118
Iteration 121/1000 | Loss: 0.00001118
Iteration 122/1000 | Loss: 0.00001118
Iteration 123/1000 | Loss: 0.00001118
Iteration 124/1000 | Loss: 0.00001117
Iteration 125/1000 | Loss: 0.00001117
Iteration 126/1000 | Loss: 0.00001117
Iteration 127/1000 | Loss: 0.00001117
Iteration 128/1000 | Loss: 0.00001117
Iteration 129/1000 | Loss: 0.00001117
Iteration 130/1000 | Loss: 0.00001117
Iteration 131/1000 | Loss: 0.00001116
Iteration 132/1000 | Loss: 0.00001116
Iteration 133/1000 | Loss: 0.00001116
Iteration 134/1000 | Loss: 0.00001116
Iteration 135/1000 | Loss: 0.00001116
Iteration 136/1000 | Loss: 0.00001116
Iteration 137/1000 | Loss: 0.00001116
Iteration 138/1000 | Loss: 0.00001116
Iteration 139/1000 | Loss: 0.00001115
Iteration 140/1000 | Loss: 0.00001115
Iteration 141/1000 | Loss: 0.00001115
Iteration 142/1000 | Loss: 0.00001115
Iteration 143/1000 | Loss: 0.00001114
Iteration 144/1000 | Loss: 0.00001114
Iteration 145/1000 | Loss: 0.00001114
Iteration 146/1000 | Loss: 0.00001114
Iteration 147/1000 | Loss: 0.00001114
Iteration 148/1000 | Loss: 0.00001114
Iteration 149/1000 | Loss: 0.00001114
Iteration 150/1000 | Loss: 0.00001114
Iteration 151/1000 | Loss: 0.00001114
Iteration 152/1000 | Loss: 0.00001113
Iteration 153/1000 | Loss: 0.00001113
Iteration 154/1000 | Loss: 0.00001113
Iteration 155/1000 | Loss: 0.00001113
Iteration 156/1000 | Loss: 0.00001113
Iteration 157/1000 | Loss: 0.00001113
Iteration 158/1000 | Loss: 0.00001113
Iteration 159/1000 | Loss: 0.00001113
Iteration 160/1000 | Loss: 0.00001113
Iteration 161/1000 | Loss: 0.00001113
Iteration 162/1000 | Loss: 0.00001113
Iteration 163/1000 | Loss: 0.00001113
Iteration 164/1000 | Loss: 0.00001113
Iteration 165/1000 | Loss: 0.00001113
Iteration 166/1000 | Loss: 0.00001113
Iteration 167/1000 | Loss: 0.00001113
Iteration 168/1000 | Loss: 0.00001113
Iteration 169/1000 | Loss: 0.00001113
Iteration 170/1000 | Loss: 0.00001113
Iteration 171/1000 | Loss: 0.00001113
Iteration 172/1000 | Loss: 0.00001113
Iteration 173/1000 | Loss: 0.00001113
Iteration 174/1000 | Loss: 0.00001113
Iteration 175/1000 | Loss: 0.00001113
Iteration 176/1000 | Loss: 0.00001113
Iteration 177/1000 | Loss: 0.00001113
Iteration 178/1000 | Loss: 0.00001113
Iteration 179/1000 | Loss: 0.00001113
Iteration 180/1000 | Loss: 0.00001113
Iteration 181/1000 | Loss: 0.00001113
Iteration 182/1000 | Loss: 0.00001113
Iteration 183/1000 | Loss: 0.00001113
Iteration 184/1000 | Loss: 0.00001113
Iteration 185/1000 | Loss: 0.00001113
Iteration 186/1000 | Loss: 0.00001113
Iteration 187/1000 | Loss: 0.00001113
Iteration 188/1000 | Loss: 0.00001113
Iteration 189/1000 | Loss: 0.00001113
Iteration 190/1000 | Loss: 0.00001113
Iteration 191/1000 | Loss: 0.00001113
Iteration 192/1000 | Loss: 0.00001113
Iteration 193/1000 | Loss: 0.00001113
Iteration 194/1000 | Loss: 0.00001113
Iteration 195/1000 | Loss: 0.00001113
Iteration 196/1000 | Loss: 0.00001113
Iteration 197/1000 | Loss: 0.00001113
Iteration 198/1000 | Loss: 0.00001113
Iteration 199/1000 | Loss: 0.00001113
Iteration 200/1000 | Loss: 0.00001113
Iteration 201/1000 | Loss: 0.00001113
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [1.1125588571303524e-05, 1.1125588571303524e-05, 1.1125588571303524e-05, 1.1125588571303524e-05, 1.1125588571303524e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1125588571303524e-05

Optimization complete. Final v2v error: 2.8736212253570557 mm

Highest mean error: 3.0805506706237793 mm for frame 105

Lowest mean error: 2.7016398906707764 mm for frame 138

Saving results

Total time: 37.286614418029785
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00783874
Iteration 2/25 | Loss: 0.00131112
Iteration 3/25 | Loss: 0.00122951
Iteration 4/25 | Loss: 0.00121903
Iteration 5/25 | Loss: 0.00121517
Iteration 6/25 | Loss: 0.00121444
Iteration 7/25 | Loss: 0.00121442
Iteration 8/25 | Loss: 0.00121442
Iteration 9/25 | Loss: 0.00121442
Iteration 10/25 | Loss: 0.00121442
Iteration 11/25 | Loss: 0.00121442
Iteration 12/25 | Loss: 0.00121442
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001214419724419713, 0.001214419724419713, 0.001214419724419713, 0.001214419724419713, 0.001214419724419713]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001214419724419713

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51836479
Iteration 2/25 | Loss: 0.00079130
Iteration 3/25 | Loss: 0.00079130
Iteration 4/25 | Loss: 0.00079129
Iteration 5/25 | Loss: 0.00079129
Iteration 6/25 | Loss: 0.00079129
Iteration 7/25 | Loss: 0.00079129
Iteration 8/25 | Loss: 0.00079129
Iteration 9/25 | Loss: 0.00079129
Iteration 10/25 | Loss: 0.00079129
Iteration 11/25 | Loss: 0.00079129
Iteration 12/25 | Loss: 0.00079129
Iteration 13/25 | Loss: 0.00079129
Iteration 14/25 | Loss: 0.00079129
Iteration 15/25 | Loss: 0.00079129
Iteration 16/25 | Loss: 0.00079129
Iteration 17/25 | Loss: 0.00079129
Iteration 18/25 | Loss: 0.00079129
Iteration 19/25 | Loss: 0.00079129
Iteration 20/25 | Loss: 0.00079129
Iteration 21/25 | Loss: 0.00079129
Iteration 22/25 | Loss: 0.00079129
Iteration 23/25 | Loss: 0.00079129
Iteration 24/25 | Loss: 0.00079129
Iteration 25/25 | Loss: 0.00079129

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079129
Iteration 2/1000 | Loss: 0.00002845
Iteration 3/1000 | Loss: 0.00001932
Iteration 4/1000 | Loss: 0.00001586
Iteration 5/1000 | Loss: 0.00001484
Iteration 6/1000 | Loss: 0.00001437
Iteration 7/1000 | Loss: 0.00001394
Iteration 8/1000 | Loss: 0.00001354
Iteration 9/1000 | Loss: 0.00001349
Iteration 10/1000 | Loss: 0.00001337
Iteration 11/1000 | Loss: 0.00001327
Iteration 12/1000 | Loss: 0.00001319
Iteration 13/1000 | Loss: 0.00001312
Iteration 14/1000 | Loss: 0.00001309
Iteration 15/1000 | Loss: 0.00001306
Iteration 16/1000 | Loss: 0.00001305
Iteration 17/1000 | Loss: 0.00001304
Iteration 18/1000 | Loss: 0.00001301
Iteration 19/1000 | Loss: 0.00001299
Iteration 20/1000 | Loss: 0.00001295
Iteration 21/1000 | Loss: 0.00001290
Iteration 22/1000 | Loss: 0.00001290
Iteration 23/1000 | Loss: 0.00001289
Iteration 24/1000 | Loss: 0.00001289
Iteration 25/1000 | Loss: 0.00001288
Iteration 26/1000 | Loss: 0.00001287
Iteration 27/1000 | Loss: 0.00001280
Iteration 28/1000 | Loss: 0.00001279
Iteration 29/1000 | Loss: 0.00001279
Iteration 30/1000 | Loss: 0.00001273
Iteration 31/1000 | Loss: 0.00001273
Iteration 32/1000 | Loss: 0.00001271
Iteration 33/1000 | Loss: 0.00001271
Iteration 34/1000 | Loss: 0.00001271
Iteration 35/1000 | Loss: 0.00001270
Iteration 36/1000 | Loss: 0.00001270
Iteration 37/1000 | Loss: 0.00001270
Iteration 38/1000 | Loss: 0.00001268
Iteration 39/1000 | Loss: 0.00001267
Iteration 40/1000 | Loss: 0.00001267
Iteration 41/1000 | Loss: 0.00001266
Iteration 42/1000 | Loss: 0.00001266
Iteration 43/1000 | Loss: 0.00001266
Iteration 44/1000 | Loss: 0.00001265
Iteration 45/1000 | Loss: 0.00001265
Iteration 46/1000 | Loss: 0.00001264
Iteration 47/1000 | Loss: 0.00001264
Iteration 48/1000 | Loss: 0.00001264
Iteration 49/1000 | Loss: 0.00001263
Iteration 50/1000 | Loss: 0.00001262
Iteration 51/1000 | Loss: 0.00001262
Iteration 52/1000 | Loss: 0.00001262
Iteration 53/1000 | Loss: 0.00001261
Iteration 54/1000 | Loss: 0.00001261
Iteration 55/1000 | Loss: 0.00001261
Iteration 56/1000 | Loss: 0.00001260
Iteration 57/1000 | Loss: 0.00001260
Iteration 58/1000 | Loss: 0.00001260
Iteration 59/1000 | Loss: 0.00001260
Iteration 60/1000 | Loss: 0.00001260
Iteration 61/1000 | Loss: 0.00001259
Iteration 62/1000 | Loss: 0.00001259
Iteration 63/1000 | Loss: 0.00001258
Iteration 64/1000 | Loss: 0.00001258
Iteration 65/1000 | Loss: 0.00001257
Iteration 66/1000 | Loss: 0.00001257
Iteration 67/1000 | Loss: 0.00001257
Iteration 68/1000 | Loss: 0.00001257
Iteration 69/1000 | Loss: 0.00001257
Iteration 70/1000 | Loss: 0.00001256
Iteration 71/1000 | Loss: 0.00001256
Iteration 72/1000 | Loss: 0.00001256
Iteration 73/1000 | Loss: 0.00001256
Iteration 74/1000 | Loss: 0.00001256
Iteration 75/1000 | Loss: 0.00001256
Iteration 76/1000 | Loss: 0.00001256
Iteration 77/1000 | Loss: 0.00001255
Iteration 78/1000 | Loss: 0.00001255
Iteration 79/1000 | Loss: 0.00001255
Iteration 80/1000 | Loss: 0.00001254
Iteration 81/1000 | Loss: 0.00001254
Iteration 82/1000 | Loss: 0.00001254
Iteration 83/1000 | Loss: 0.00001254
Iteration 84/1000 | Loss: 0.00001254
Iteration 85/1000 | Loss: 0.00001254
Iteration 86/1000 | Loss: 0.00001254
Iteration 87/1000 | Loss: 0.00001253
Iteration 88/1000 | Loss: 0.00001253
Iteration 89/1000 | Loss: 0.00001253
Iteration 90/1000 | Loss: 0.00001252
Iteration 91/1000 | Loss: 0.00001252
Iteration 92/1000 | Loss: 0.00001251
Iteration 93/1000 | Loss: 0.00001251
Iteration 94/1000 | Loss: 0.00001251
Iteration 95/1000 | Loss: 0.00001251
Iteration 96/1000 | Loss: 0.00001250
Iteration 97/1000 | Loss: 0.00001250
Iteration 98/1000 | Loss: 0.00001250
Iteration 99/1000 | Loss: 0.00001250
Iteration 100/1000 | Loss: 0.00001250
Iteration 101/1000 | Loss: 0.00001249
Iteration 102/1000 | Loss: 0.00001249
Iteration 103/1000 | Loss: 0.00001249
Iteration 104/1000 | Loss: 0.00001249
Iteration 105/1000 | Loss: 0.00001248
Iteration 106/1000 | Loss: 0.00001248
Iteration 107/1000 | Loss: 0.00001248
Iteration 108/1000 | Loss: 0.00001248
Iteration 109/1000 | Loss: 0.00001248
Iteration 110/1000 | Loss: 0.00001248
Iteration 111/1000 | Loss: 0.00001248
Iteration 112/1000 | Loss: 0.00001248
Iteration 113/1000 | Loss: 0.00001247
Iteration 114/1000 | Loss: 0.00001247
Iteration 115/1000 | Loss: 0.00001247
Iteration 116/1000 | Loss: 0.00001247
Iteration 117/1000 | Loss: 0.00001247
Iteration 118/1000 | Loss: 0.00001247
Iteration 119/1000 | Loss: 0.00001246
Iteration 120/1000 | Loss: 0.00001246
Iteration 121/1000 | Loss: 0.00001246
Iteration 122/1000 | Loss: 0.00001245
Iteration 123/1000 | Loss: 0.00001245
Iteration 124/1000 | Loss: 0.00001245
Iteration 125/1000 | Loss: 0.00001245
Iteration 126/1000 | Loss: 0.00001245
Iteration 127/1000 | Loss: 0.00001245
Iteration 128/1000 | Loss: 0.00001245
Iteration 129/1000 | Loss: 0.00001244
Iteration 130/1000 | Loss: 0.00001244
Iteration 131/1000 | Loss: 0.00001244
Iteration 132/1000 | Loss: 0.00001244
Iteration 133/1000 | Loss: 0.00001244
Iteration 134/1000 | Loss: 0.00001244
Iteration 135/1000 | Loss: 0.00001244
Iteration 136/1000 | Loss: 0.00001244
Iteration 137/1000 | Loss: 0.00001244
Iteration 138/1000 | Loss: 0.00001244
Iteration 139/1000 | Loss: 0.00001244
Iteration 140/1000 | Loss: 0.00001243
Iteration 141/1000 | Loss: 0.00001243
Iteration 142/1000 | Loss: 0.00001242
Iteration 143/1000 | Loss: 0.00001242
Iteration 144/1000 | Loss: 0.00001242
Iteration 145/1000 | Loss: 0.00001242
Iteration 146/1000 | Loss: 0.00001242
Iteration 147/1000 | Loss: 0.00001242
Iteration 148/1000 | Loss: 0.00001242
Iteration 149/1000 | Loss: 0.00001242
Iteration 150/1000 | Loss: 0.00001242
Iteration 151/1000 | Loss: 0.00001242
Iteration 152/1000 | Loss: 0.00001242
Iteration 153/1000 | Loss: 0.00001242
Iteration 154/1000 | Loss: 0.00001242
Iteration 155/1000 | Loss: 0.00001242
Iteration 156/1000 | Loss: 0.00001242
Iteration 157/1000 | Loss: 0.00001242
Iteration 158/1000 | Loss: 0.00001242
Iteration 159/1000 | Loss: 0.00001242
Iteration 160/1000 | Loss: 0.00001241
Iteration 161/1000 | Loss: 0.00001241
Iteration 162/1000 | Loss: 0.00001241
Iteration 163/1000 | Loss: 0.00001241
Iteration 164/1000 | Loss: 0.00001241
Iteration 165/1000 | Loss: 0.00001241
Iteration 166/1000 | Loss: 0.00001241
Iteration 167/1000 | Loss: 0.00001241
Iteration 168/1000 | Loss: 0.00001241
Iteration 169/1000 | Loss: 0.00001241
Iteration 170/1000 | Loss: 0.00001241
Iteration 171/1000 | Loss: 0.00001241
Iteration 172/1000 | Loss: 0.00001241
Iteration 173/1000 | Loss: 0.00001240
Iteration 174/1000 | Loss: 0.00001240
Iteration 175/1000 | Loss: 0.00001240
Iteration 176/1000 | Loss: 0.00001240
Iteration 177/1000 | Loss: 0.00001240
Iteration 178/1000 | Loss: 0.00001240
Iteration 179/1000 | Loss: 0.00001240
Iteration 180/1000 | Loss: 0.00001240
Iteration 181/1000 | Loss: 0.00001240
Iteration 182/1000 | Loss: 0.00001240
Iteration 183/1000 | Loss: 0.00001240
Iteration 184/1000 | Loss: 0.00001240
Iteration 185/1000 | Loss: 0.00001240
Iteration 186/1000 | Loss: 0.00001240
Iteration 187/1000 | Loss: 0.00001240
Iteration 188/1000 | Loss: 0.00001240
Iteration 189/1000 | Loss: 0.00001240
Iteration 190/1000 | Loss: 0.00001240
Iteration 191/1000 | Loss: 0.00001240
Iteration 192/1000 | Loss: 0.00001240
Iteration 193/1000 | Loss: 0.00001240
Iteration 194/1000 | Loss: 0.00001239
Iteration 195/1000 | Loss: 0.00001239
Iteration 196/1000 | Loss: 0.00001239
Iteration 197/1000 | Loss: 0.00001239
Iteration 198/1000 | Loss: 0.00001239
Iteration 199/1000 | Loss: 0.00001239
Iteration 200/1000 | Loss: 0.00001239
Iteration 201/1000 | Loss: 0.00001239
Iteration 202/1000 | Loss: 0.00001239
Iteration 203/1000 | Loss: 0.00001239
Iteration 204/1000 | Loss: 0.00001239
Iteration 205/1000 | Loss: 0.00001238
Iteration 206/1000 | Loss: 0.00001238
Iteration 207/1000 | Loss: 0.00001238
Iteration 208/1000 | Loss: 0.00001238
Iteration 209/1000 | Loss: 0.00001238
Iteration 210/1000 | Loss: 0.00001238
Iteration 211/1000 | Loss: 0.00001238
Iteration 212/1000 | Loss: 0.00001238
Iteration 213/1000 | Loss: 0.00001238
Iteration 214/1000 | Loss: 0.00001238
Iteration 215/1000 | Loss: 0.00001238
Iteration 216/1000 | Loss: 0.00001238
Iteration 217/1000 | Loss: 0.00001238
Iteration 218/1000 | Loss: 0.00001238
Iteration 219/1000 | Loss: 0.00001238
Iteration 220/1000 | Loss: 0.00001238
Iteration 221/1000 | Loss: 0.00001238
Iteration 222/1000 | Loss: 0.00001238
Iteration 223/1000 | Loss: 0.00001238
Iteration 224/1000 | Loss: 0.00001238
Iteration 225/1000 | Loss: 0.00001238
Iteration 226/1000 | Loss: 0.00001237
Iteration 227/1000 | Loss: 0.00001237
Iteration 228/1000 | Loss: 0.00001237
Iteration 229/1000 | Loss: 0.00001237
Iteration 230/1000 | Loss: 0.00001237
Iteration 231/1000 | Loss: 0.00001237
Iteration 232/1000 | Loss: 0.00001237
Iteration 233/1000 | Loss: 0.00001237
Iteration 234/1000 | Loss: 0.00001237
Iteration 235/1000 | Loss: 0.00001237
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 235. Stopping optimization.
Last 5 losses: [1.2373550816846546e-05, 1.2373550816846546e-05, 1.2373550816846546e-05, 1.2373550816846546e-05, 1.2373550816846546e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2373550816846546e-05

Optimization complete. Final v2v error: 2.967958927154541 mm

Highest mean error: 3.8602654933929443 mm for frame 59

Lowest mean error: 2.689955234527588 mm for frame 33

Saving results

Total time: 41.15022134780884
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00494323
Iteration 2/25 | Loss: 0.00140786
Iteration 3/25 | Loss: 0.00128496
Iteration 4/25 | Loss: 0.00127345
Iteration 5/25 | Loss: 0.00127019
Iteration 6/25 | Loss: 0.00126978
Iteration 7/25 | Loss: 0.00126978
Iteration 8/25 | Loss: 0.00126978
Iteration 9/25 | Loss: 0.00126978
Iteration 10/25 | Loss: 0.00126978
Iteration 11/25 | Loss: 0.00126978
Iteration 12/25 | Loss: 0.00126978
Iteration 13/25 | Loss: 0.00126978
Iteration 14/25 | Loss: 0.00126978
Iteration 15/25 | Loss: 0.00126978
Iteration 16/25 | Loss: 0.00126978
Iteration 17/25 | Loss: 0.00126978
Iteration 18/25 | Loss: 0.00126978
Iteration 19/25 | Loss: 0.00126978
Iteration 20/25 | Loss: 0.00126978
Iteration 21/25 | Loss: 0.00126978
Iteration 22/25 | Loss: 0.00126978
Iteration 23/25 | Loss: 0.00126978
Iteration 24/25 | Loss: 0.00126978
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0012697753263637424, 0.0012697753263637424, 0.0012697753263637424, 0.0012697753263637424, 0.0012697753263637424]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012697753263637424

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53697383
Iteration 2/25 | Loss: 0.00075763
Iteration 3/25 | Loss: 0.00075762
Iteration 4/25 | Loss: 0.00075761
Iteration 5/25 | Loss: 0.00075761
Iteration 6/25 | Loss: 0.00075761
Iteration 7/25 | Loss: 0.00075761
Iteration 8/25 | Loss: 0.00075761
Iteration 9/25 | Loss: 0.00075761
Iteration 10/25 | Loss: 0.00075761
Iteration 11/25 | Loss: 0.00075761
Iteration 12/25 | Loss: 0.00075761
Iteration 13/25 | Loss: 0.00075761
Iteration 14/25 | Loss: 0.00075761
Iteration 15/25 | Loss: 0.00075761
Iteration 16/25 | Loss: 0.00075761
Iteration 17/25 | Loss: 0.00075761
Iteration 18/25 | Loss: 0.00075761
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007576118223369122, 0.0007576118223369122, 0.0007576118223369122, 0.0007576118223369122, 0.0007576118223369122]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007576118223369122

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075761
Iteration 2/1000 | Loss: 0.00003848
Iteration 3/1000 | Loss: 0.00002733
Iteration 4/1000 | Loss: 0.00002215
Iteration 5/1000 | Loss: 0.00002067
Iteration 6/1000 | Loss: 0.00001965
Iteration 7/1000 | Loss: 0.00001916
Iteration 8/1000 | Loss: 0.00001854
Iteration 9/1000 | Loss: 0.00001815
Iteration 10/1000 | Loss: 0.00001782
Iteration 11/1000 | Loss: 0.00001756
Iteration 12/1000 | Loss: 0.00001743
Iteration 13/1000 | Loss: 0.00001726
Iteration 14/1000 | Loss: 0.00001713
Iteration 15/1000 | Loss: 0.00001710
Iteration 16/1000 | Loss: 0.00001709
Iteration 17/1000 | Loss: 0.00001709
Iteration 18/1000 | Loss: 0.00001708
Iteration 19/1000 | Loss: 0.00001708
Iteration 20/1000 | Loss: 0.00001708
Iteration 21/1000 | Loss: 0.00001707
Iteration 22/1000 | Loss: 0.00001706
Iteration 23/1000 | Loss: 0.00001706
Iteration 24/1000 | Loss: 0.00001703
Iteration 25/1000 | Loss: 0.00001703
Iteration 26/1000 | Loss: 0.00001701
Iteration 27/1000 | Loss: 0.00001701
Iteration 28/1000 | Loss: 0.00001701
Iteration 29/1000 | Loss: 0.00001700
Iteration 30/1000 | Loss: 0.00001700
Iteration 31/1000 | Loss: 0.00001699
Iteration 32/1000 | Loss: 0.00001698
Iteration 33/1000 | Loss: 0.00001697
Iteration 34/1000 | Loss: 0.00001697
Iteration 35/1000 | Loss: 0.00001697
Iteration 36/1000 | Loss: 0.00001696
Iteration 37/1000 | Loss: 0.00001696
Iteration 38/1000 | Loss: 0.00001695
Iteration 39/1000 | Loss: 0.00001695
Iteration 40/1000 | Loss: 0.00001695
Iteration 41/1000 | Loss: 0.00001694
Iteration 42/1000 | Loss: 0.00001694
Iteration 43/1000 | Loss: 0.00001694
Iteration 44/1000 | Loss: 0.00001693
Iteration 45/1000 | Loss: 0.00001693
Iteration 46/1000 | Loss: 0.00001693
Iteration 47/1000 | Loss: 0.00001692
Iteration 48/1000 | Loss: 0.00001692
Iteration 49/1000 | Loss: 0.00001691
Iteration 50/1000 | Loss: 0.00001691
Iteration 51/1000 | Loss: 0.00001691
Iteration 52/1000 | Loss: 0.00001690
Iteration 53/1000 | Loss: 0.00001690
Iteration 54/1000 | Loss: 0.00001690
Iteration 55/1000 | Loss: 0.00001689
Iteration 56/1000 | Loss: 0.00001689
Iteration 57/1000 | Loss: 0.00001689
Iteration 58/1000 | Loss: 0.00001688
Iteration 59/1000 | Loss: 0.00001688
Iteration 60/1000 | Loss: 0.00001687
Iteration 61/1000 | Loss: 0.00001687
Iteration 62/1000 | Loss: 0.00001687
Iteration 63/1000 | Loss: 0.00001687
Iteration 64/1000 | Loss: 0.00001686
Iteration 65/1000 | Loss: 0.00001685
Iteration 66/1000 | Loss: 0.00001685
Iteration 67/1000 | Loss: 0.00001685
Iteration 68/1000 | Loss: 0.00001684
Iteration 69/1000 | Loss: 0.00001683
Iteration 70/1000 | Loss: 0.00001683
Iteration 71/1000 | Loss: 0.00001683
Iteration 72/1000 | Loss: 0.00001682
Iteration 73/1000 | Loss: 0.00001681
Iteration 74/1000 | Loss: 0.00001681
Iteration 75/1000 | Loss: 0.00001681
Iteration 76/1000 | Loss: 0.00001681
Iteration 77/1000 | Loss: 0.00001680
Iteration 78/1000 | Loss: 0.00001680
Iteration 79/1000 | Loss: 0.00001680
Iteration 80/1000 | Loss: 0.00001678
Iteration 81/1000 | Loss: 0.00001678
Iteration 82/1000 | Loss: 0.00001678
Iteration 83/1000 | Loss: 0.00001678
Iteration 84/1000 | Loss: 0.00001678
Iteration 85/1000 | Loss: 0.00001678
Iteration 86/1000 | Loss: 0.00001678
Iteration 87/1000 | Loss: 0.00001678
Iteration 88/1000 | Loss: 0.00001678
Iteration 89/1000 | Loss: 0.00001677
Iteration 90/1000 | Loss: 0.00001677
Iteration 91/1000 | Loss: 0.00001676
Iteration 92/1000 | Loss: 0.00001675
Iteration 93/1000 | Loss: 0.00001675
Iteration 94/1000 | Loss: 0.00001675
Iteration 95/1000 | Loss: 0.00001674
Iteration 96/1000 | Loss: 0.00001674
Iteration 97/1000 | Loss: 0.00001674
Iteration 98/1000 | Loss: 0.00001674
Iteration 99/1000 | Loss: 0.00001674
Iteration 100/1000 | Loss: 0.00001674
Iteration 101/1000 | Loss: 0.00001674
Iteration 102/1000 | Loss: 0.00001674
Iteration 103/1000 | Loss: 0.00001674
Iteration 104/1000 | Loss: 0.00001674
Iteration 105/1000 | Loss: 0.00001673
Iteration 106/1000 | Loss: 0.00001673
Iteration 107/1000 | Loss: 0.00001673
Iteration 108/1000 | Loss: 0.00001672
Iteration 109/1000 | Loss: 0.00001672
Iteration 110/1000 | Loss: 0.00001672
Iteration 111/1000 | Loss: 0.00001672
Iteration 112/1000 | Loss: 0.00001672
Iteration 113/1000 | Loss: 0.00001672
Iteration 114/1000 | Loss: 0.00001671
Iteration 115/1000 | Loss: 0.00001671
Iteration 116/1000 | Loss: 0.00001671
Iteration 117/1000 | Loss: 0.00001671
Iteration 118/1000 | Loss: 0.00001671
Iteration 119/1000 | Loss: 0.00001671
Iteration 120/1000 | Loss: 0.00001671
Iteration 121/1000 | Loss: 0.00001671
Iteration 122/1000 | Loss: 0.00001670
Iteration 123/1000 | Loss: 0.00001670
Iteration 124/1000 | Loss: 0.00001670
Iteration 125/1000 | Loss: 0.00001670
Iteration 126/1000 | Loss: 0.00001669
Iteration 127/1000 | Loss: 0.00001669
Iteration 128/1000 | Loss: 0.00001669
Iteration 129/1000 | Loss: 0.00001669
Iteration 130/1000 | Loss: 0.00001669
Iteration 131/1000 | Loss: 0.00001669
Iteration 132/1000 | Loss: 0.00001669
Iteration 133/1000 | Loss: 0.00001669
Iteration 134/1000 | Loss: 0.00001668
Iteration 135/1000 | Loss: 0.00001668
Iteration 136/1000 | Loss: 0.00001668
Iteration 137/1000 | Loss: 0.00001668
Iteration 138/1000 | Loss: 0.00001668
Iteration 139/1000 | Loss: 0.00001668
Iteration 140/1000 | Loss: 0.00001668
Iteration 141/1000 | Loss: 0.00001668
Iteration 142/1000 | Loss: 0.00001668
Iteration 143/1000 | Loss: 0.00001668
Iteration 144/1000 | Loss: 0.00001668
Iteration 145/1000 | Loss: 0.00001668
Iteration 146/1000 | Loss: 0.00001668
Iteration 147/1000 | Loss: 0.00001668
Iteration 148/1000 | Loss: 0.00001668
Iteration 149/1000 | Loss: 0.00001668
Iteration 150/1000 | Loss: 0.00001668
Iteration 151/1000 | Loss: 0.00001667
Iteration 152/1000 | Loss: 0.00001667
Iteration 153/1000 | Loss: 0.00001667
Iteration 154/1000 | Loss: 0.00001667
Iteration 155/1000 | Loss: 0.00001667
Iteration 156/1000 | Loss: 0.00001667
Iteration 157/1000 | Loss: 0.00001667
Iteration 158/1000 | Loss: 0.00001667
Iteration 159/1000 | Loss: 0.00001667
Iteration 160/1000 | Loss: 0.00001667
Iteration 161/1000 | Loss: 0.00001667
Iteration 162/1000 | Loss: 0.00001667
Iteration 163/1000 | Loss: 0.00001667
Iteration 164/1000 | Loss: 0.00001666
Iteration 165/1000 | Loss: 0.00001666
Iteration 166/1000 | Loss: 0.00001666
Iteration 167/1000 | Loss: 0.00001666
Iteration 168/1000 | Loss: 0.00001666
Iteration 169/1000 | Loss: 0.00001666
Iteration 170/1000 | Loss: 0.00001666
Iteration 171/1000 | Loss: 0.00001666
Iteration 172/1000 | Loss: 0.00001666
Iteration 173/1000 | Loss: 0.00001666
Iteration 174/1000 | Loss: 0.00001666
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [1.6662583220750093e-05, 1.6662583220750093e-05, 1.6662583220750093e-05, 1.6662583220750093e-05, 1.6662583220750093e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6662583220750093e-05

Optimization complete. Final v2v error: 3.3659985065460205 mm

Highest mean error: 4.498973369598389 mm for frame 110

Lowest mean error: 2.766216993331909 mm for frame 166

Saving results

Total time: 40.62097096443176
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00982844
Iteration 2/25 | Loss: 0.00982844
Iteration 3/25 | Loss: 0.00982844
Iteration 4/25 | Loss: 0.00982843
Iteration 5/25 | Loss: 0.00982843
Iteration 6/25 | Loss: 0.00982843
Iteration 7/25 | Loss: 0.00982843
Iteration 8/25 | Loss: 0.00982843
Iteration 9/25 | Loss: 0.00982843
Iteration 10/25 | Loss: 0.00982843
Iteration 11/25 | Loss: 0.00982842
Iteration 12/25 | Loss: 0.00982842
Iteration 13/25 | Loss: 0.00982842
Iteration 14/25 | Loss: 0.00982842
Iteration 15/25 | Loss: 0.00982842
Iteration 16/25 | Loss: 0.00982842
Iteration 17/25 | Loss: 0.00982841
Iteration 18/25 | Loss: 0.00982841
Iteration 19/25 | Loss: 0.00982841
Iteration 20/25 | Loss: 0.00982841
Iteration 21/25 | Loss: 0.00982841
Iteration 22/25 | Loss: 0.00982841
Iteration 23/25 | Loss: 0.00982841
Iteration 24/25 | Loss: 0.00982840
Iteration 25/25 | Loss: 0.00982840

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52100492
Iteration 2/25 | Loss: 0.18633313
Iteration 3/25 | Loss: 0.18632570
Iteration 4/25 | Loss: 0.18632567
Iteration 5/25 | Loss: 0.18632564
Iteration 6/25 | Loss: 0.18632564
Iteration 7/25 | Loss: 0.18632564
Iteration 8/25 | Loss: 0.18632564
Iteration 9/25 | Loss: 0.18632562
Iteration 10/25 | Loss: 0.18632562
Iteration 11/25 | Loss: 0.18632562
Iteration 12/25 | Loss: 0.18632559
Iteration 13/25 | Loss: 0.18632559
Iteration 14/25 | Loss: 0.18632559
Iteration 15/25 | Loss: 0.18632559
Iteration 16/25 | Loss: 0.18632559
Iteration 17/25 | Loss: 0.18632561
Iteration 18/25 | Loss: 0.18632561
Iteration 19/25 | Loss: 0.18632561
Iteration 20/25 | Loss: 0.18632561
Iteration 21/25 | Loss: 0.18632561
Iteration 22/25 | Loss: 0.18632561
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.18632560968399048, 0.18632560968399048, 0.18632560968399048, 0.18632560968399048, 0.18632560968399048]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.18632560968399048

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.18632561
Iteration 2/1000 | Loss: 0.00599173
Iteration 3/1000 | Loss: 0.00217190
Iteration 4/1000 | Loss: 0.00081244
Iteration 5/1000 | Loss: 0.00055337
Iteration 6/1000 | Loss: 0.00056832
Iteration 7/1000 | Loss: 0.00040052
Iteration 8/1000 | Loss: 0.00077464
Iteration 9/1000 | Loss: 0.00047783
Iteration 10/1000 | Loss: 0.00007476
Iteration 11/1000 | Loss: 0.00055731
Iteration 12/1000 | Loss: 0.00091702
Iteration 13/1000 | Loss: 0.00107726
Iteration 14/1000 | Loss: 0.00006473
Iteration 15/1000 | Loss: 0.00010024
Iteration 16/1000 | Loss: 0.00037700
Iteration 17/1000 | Loss: 0.00041436
Iteration 18/1000 | Loss: 0.00020051
Iteration 19/1000 | Loss: 0.00009662
Iteration 20/1000 | Loss: 0.00034475
Iteration 21/1000 | Loss: 0.00011464
Iteration 22/1000 | Loss: 0.00003442
Iteration 23/1000 | Loss: 0.00025815
Iteration 24/1000 | Loss: 0.00061689
Iteration 25/1000 | Loss: 0.00016318
Iteration 26/1000 | Loss: 0.00003334
Iteration 27/1000 | Loss: 0.00003440
Iteration 28/1000 | Loss: 0.00002855
Iteration 29/1000 | Loss: 0.00010167
Iteration 30/1000 | Loss: 0.00033693
Iteration 31/1000 | Loss: 0.00144185
Iteration 32/1000 | Loss: 0.00012254
Iteration 33/1000 | Loss: 0.00004551
Iteration 34/1000 | Loss: 0.00002650
Iteration 35/1000 | Loss: 0.00024958
Iteration 36/1000 | Loss: 0.00004182
Iteration 37/1000 | Loss: 0.00037766
Iteration 38/1000 | Loss: 0.00002622
Iteration 39/1000 | Loss: 0.00002488
Iteration 40/1000 | Loss: 0.00023799
Iteration 41/1000 | Loss: 0.00002429
Iteration 42/1000 | Loss: 0.00014365
Iteration 43/1000 | Loss: 0.00081700
Iteration 44/1000 | Loss: 0.00030805
Iteration 45/1000 | Loss: 0.00028496
Iteration 46/1000 | Loss: 0.00003174
Iteration 47/1000 | Loss: 0.00004129
Iteration 48/1000 | Loss: 0.00002346
Iteration 49/1000 | Loss: 0.00002314
Iteration 50/1000 | Loss: 0.00014401
Iteration 51/1000 | Loss: 0.00003818
Iteration 52/1000 | Loss: 0.00004898
Iteration 53/1000 | Loss: 0.00002264
Iteration 54/1000 | Loss: 0.00006967
Iteration 55/1000 | Loss: 0.00002266
Iteration 56/1000 | Loss: 0.00002241
Iteration 57/1000 | Loss: 0.00002240
Iteration 58/1000 | Loss: 0.00002237
Iteration 59/1000 | Loss: 0.00002231
Iteration 60/1000 | Loss: 0.00002230
Iteration 61/1000 | Loss: 0.00002229
Iteration 62/1000 | Loss: 0.00002228
Iteration 63/1000 | Loss: 0.00002225
Iteration 64/1000 | Loss: 0.00007502
Iteration 65/1000 | Loss: 0.00002223
Iteration 66/1000 | Loss: 0.00002222
Iteration 67/1000 | Loss: 0.00002217
Iteration 68/1000 | Loss: 0.00002217
Iteration 69/1000 | Loss: 0.00002216
Iteration 70/1000 | Loss: 0.00002216
Iteration 71/1000 | Loss: 0.00002215
Iteration 72/1000 | Loss: 0.00002215
Iteration 73/1000 | Loss: 0.00002214
Iteration 74/1000 | Loss: 0.00002214
Iteration 75/1000 | Loss: 0.00002214
Iteration 76/1000 | Loss: 0.00002214
Iteration 77/1000 | Loss: 0.00002213
Iteration 78/1000 | Loss: 0.00002213
Iteration 79/1000 | Loss: 0.00002213
Iteration 80/1000 | Loss: 0.00002211
Iteration 81/1000 | Loss: 0.00002208
Iteration 82/1000 | Loss: 0.00002208
Iteration 83/1000 | Loss: 0.00002207
Iteration 84/1000 | Loss: 0.00002207
Iteration 85/1000 | Loss: 0.00002207
Iteration 86/1000 | Loss: 0.00002207
Iteration 87/1000 | Loss: 0.00002207
Iteration 88/1000 | Loss: 0.00002207
Iteration 89/1000 | Loss: 0.00002207
Iteration 90/1000 | Loss: 0.00002207
Iteration 91/1000 | Loss: 0.00002207
Iteration 92/1000 | Loss: 0.00002207
Iteration 93/1000 | Loss: 0.00002207
Iteration 94/1000 | Loss: 0.00002207
Iteration 95/1000 | Loss: 0.00002207
Iteration 96/1000 | Loss: 0.00002207
Iteration 97/1000 | Loss: 0.00002206
Iteration 98/1000 | Loss: 0.00002206
Iteration 99/1000 | Loss: 0.00002205
Iteration 100/1000 | Loss: 0.00002205
Iteration 101/1000 | Loss: 0.00002204
Iteration 102/1000 | Loss: 0.00002203
Iteration 103/1000 | Loss: 0.00002203
Iteration 104/1000 | Loss: 0.00002202
Iteration 105/1000 | Loss: 0.00002202
Iteration 106/1000 | Loss: 0.00002202
Iteration 107/1000 | Loss: 0.00002201
Iteration 108/1000 | Loss: 0.00002201
Iteration 109/1000 | Loss: 0.00002201
Iteration 110/1000 | Loss: 0.00002201
Iteration 111/1000 | Loss: 0.00002200
Iteration 112/1000 | Loss: 0.00002200
Iteration 113/1000 | Loss: 0.00002200
Iteration 114/1000 | Loss: 0.00002199
Iteration 115/1000 | Loss: 0.00002199
Iteration 116/1000 | Loss: 0.00002199
Iteration 117/1000 | Loss: 0.00002199
Iteration 118/1000 | Loss: 0.00002199
Iteration 119/1000 | Loss: 0.00002199
Iteration 120/1000 | Loss: 0.00002199
Iteration 121/1000 | Loss: 0.00002199
Iteration 122/1000 | Loss: 0.00002199
Iteration 123/1000 | Loss: 0.00002199
Iteration 124/1000 | Loss: 0.00002199
Iteration 125/1000 | Loss: 0.00002199
Iteration 126/1000 | Loss: 0.00002199
Iteration 127/1000 | Loss: 0.00002199
Iteration 128/1000 | Loss: 0.00002199
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [2.1985655621392652e-05, 2.1985655621392652e-05, 2.1985655621392652e-05, 2.1985655621392652e-05, 2.1985655621392652e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1985655621392652e-05

Optimization complete. Final v2v error: 3.994253396987915 mm

Highest mean error: 4.419401168823242 mm for frame 18

Lowest mean error: 3.7129580974578857 mm for frame 128

Saving results

Total time: 104.91715788841248
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00397189
Iteration 2/25 | Loss: 0.00128720
Iteration 3/25 | Loss: 0.00122262
Iteration 4/25 | Loss: 0.00121330
Iteration 5/25 | Loss: 0.00121163
Iteration 6/25 | Loss: 0.00121163
Iteration 7/25 | Loss: 0.00121163
Iteration 8/25 | Loss: 0.00121163
Iteration 9/25 | Loss: 0.00121163
Iteration 10/25 | Loss: 0.00121163
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012116280850023031, 0.0012116280850023031, 0.0012116280850023031, 0.0012116280850023031, 0.0012116280850023031]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012116280850023031

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42392385
Iteration 2/25 | Loss: 0.00073917
Iteration 3/25 | Loss: 0.00073917
Iteration 4/25 | Loss: 0.00073917
Iteration 5/25 | Loss: 0.00073917
Iteration 6/25 | Loss: 0.00073917
Iteration 7/25 | Loss: 0.00073917
Iteration 8/25 | Loss: 0.00073917
Iteration 9/25 | Loss: 0.00073917
Iteration 10/25 | Loss: 0.00073917
Iteration 11/25 | Loss: 0.00073917
Iteration 12/25 | Loss: 0.00073917
Iteration 13/25 | Loss: 0.00073917
Iteration 14/25 | Loss: 0.00073917
Iteration 15/25 | Loss: 0.00073917
Iteration 16/25 | Loss: 0.00073917
Iteration 17/25 | Loss: 0.00073917
Iteration 18/25 | Loss: 0.00073917
Iteration 19/25 | Loss: 0.00073917
Iteration 20/25 | Loss: 0.00073917
Iteration 21/25 | Loss: 0.00073917
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.000739166687708348, 0.000739166687708348, 0.000739166687708348, 0.000739166687708348, 0.000739166687708348]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000739166687708348

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073917
Iteration 2/1000 | Loss: 0.00002185
Iteration 3/1000 | Loss: 0.00001719
Iteration 4/1000 | Loss: 0.00001606
Iteration 5/1000 | Loss: 0.00001528
Iteration 6/1000 | Loss: 0.00001507
Iteration 7/1000 | Loss: 0.00001506
Iteration 8/1000 | Loss: 0.00001497
Iteration 9/1000 | Loss: 0.00001465
Iteration 10/1000 | Loss: 0.00001450
Iteration 11/1000 | Loss: 0.00001429
Iteration 12/1000 | Loss: 0.00001426
Iteration 13/1000 | Loss: 0.00001418
Iteration 14/1000 | Loss: 0.00001415
Iteration 15/1000 | Loss: 0.00001415
Iteration 16/1000 | Loss: 0.00001406
Iteration 17/1000 | Loss: 0.00001406
Iteration 18/1000 | Loss: 0.00001406
Iteration 19/1000 | Loss: 0.00001405
Iteration 20/1000 | Loss: 0.00001405
Iteration 21/1000 | Loss: 0.00001405
Iteration 22/1000 | Loss: 0.00001404
Iteration 23/1000 | Loss: 0.00001404
Iteration 24/1000 | Loss: 0.00001403
Iteration 25/1000 | Loss: 0.00001391
Iteration 26/1000 | Loss: 0.00001391
Iteration 27/1000 | Loss: 0.00001391
Iteration 28/1000 | Loss: 0.00001391
Iteration 29/1000 | Loss: 0.00001391
Iteration 30/1000 | Loss: 0.00001390
Iteration 31/1000 | Loss: 0.00001390
Iteration 32/1000 | Loss: 0.00001389
Iteration 33/1000 | Loss: 0.00001389
Iteration 34/1000 | Loss: 0.00001389
Iteration 35/1000 | Loss: 0.00001388
Iteration 36/1000 | Loss: 0.00001377
Iteration 37/1000 | Loss: 0.00001376
Iteration 38/1000 | Loss: 0.00001375
Iteration 39/1000 | Loss: 0.00001374
Iteration 40/1000 | Loss: 0.00001373
Iteration 41/1000 | Loss: 0.00001367
Iteration 42/1000 | Loss: 0.00001364
Iteration 43/1000 | Loss: 0.00001363
Iteration 44/1000 | Loss: 0.00001362
Iteration 45/1000 | Loss: 0.00001362
Iteration 46/1000 | Loss: 0.00001361
Iteration 47/1000 | Loss: 0.00001361
Iteration 48/1000 | Loss: 0.00001360
Iteration 49/1000 | Loss: 0.00001359
Iteration 50/1000 | Loss: 0.00001359
Iteration 51/1000 | Loss: 0.00001358
Iteration 52/1000 | Loss: 0.00001358
Iteration 53/1000 | Loss: 0.00001358
Iteration 54/1000 | Loss: 0.00001358
Iteration 55/1000 | Loss: 0.00001358
Iteration 56/1000 | Loss: 0.00001358
Iteration 57/1000 | Loss: 0.00001358
Iteration 58/1000 | Loss: 0.00001358
Iteration 59/1000 | Loss: 0.00001357
Iteration 60/1000 | Loss: 0.00001356
Iteration 61/1000 | Loss: 0.00001356
Iteration 62/1000 | Loss: 0.00001356
Iteration 63/1000 | Loss: 0.00001355
Iteration 64/1000 | Loss: 0.00001354
Iteration 65/1000 | Loss: 0.00001354
Iteration 66/1000 | Loss: 0.00001354
Iteration 67/1000 | Loss: 0.00001354
Iteration 68/1000 | Loss: 0.00001353
Iteration 69/1000 | Loss: 0.00001353
Iteration 70/1000 | Loss: 0.00001351
Iteration 71/1000 | Loss: 0.00001350
Iteration 72/1000 | Loss: 0.00001350
Iteration 73/1000 | Loss: 0.00001350
Iteration 74/1000 | Loss: 0.00001349
Iteration 75/1000 | Loss: 0.00001349
Iteration 76/1000 | Loss: 0.00001349
Iteration 77/1000 | Loss: 0.00001349
Iteration 78/1000 | Loss: 0.00001348
Iteration 79/1000 | Loss: 0.00001348
Iteration 80/1000 | Loss: 0.00001348
Iteration 81/1000 | Loss: 0.00001348
Iteration 82/1000 | Loss: 0.00001347
Iteration 83/1000 | Loss: 0.00001347
Iteration 84/1000 | Loss: 0.00001347
Iteration 85/1000 | Loss: 0.00001347
Iteration 86/1000 | Loss: 0.00001347
Iteration 87/1000 | Loss: 0.00001347
Iteration 88/1000 | Loss: 0.00001347
Iteration 89/1000 | Loss: 0.00001347
Iteration 90/1000 | Loss: 0.00001347
Iteration 91/1000 | Loss: 0.00001347
Iteration 92/1000 | Loss: 0.00001347
Iteration 93/1000 | Loss: 0.00001346
Iteration 94/1000 | Loss: 0.00001344
Iteration 95/1000 | Loss: 0.00001344
Iteration 96/1000 | Loss: 0.00001344
Iteration 97/1000 | Loss: 0.00001344
Iteration 98/1000 | Loss: 0.00001343
Iteration 99/1000 | Loss: 0.00001343
Iteration 100/1000 | Loss: 0.00001343
Iteration 101/1000 | Loss: 0.00001342
Iteration 102/1000 | Loss: 0.00001341
Iteration 103/1000 | Loss: 0.00001341
Iteration 104/1000 | Loss: 0.00001340
Iteration 105/1000 | Loss: 0.00001340
Iteration 106/1000 | Loss: 0.00001340
Iteration 107/1000 | Loss: 0.00001339
Iteration 108/1000 | Loss: 0.00001339
Iteration 109/1000 | Loss: 0.00001339
Iteration 110/1000 | Loss: 0.00001339
Iteration 111/1000 | Loss: 0.00001338
Iteration 112/1000 | Loss: 0.00001338
Iteration 113/1000 | Loss: 0.00001336
Iteration 114/1000 | Loss: 0.00001334
Iteration 115/1000 | Loss: 0.00001333
Iteration 116/1000 | Loss: 0.00001333
Iteration 117/1000 | Loss: 0.00001333
Iteration 118/1000 | Loss: 0.00001333
Iteration 119/1000 | Loss: 0.00001333
Iteration 120/1000 | Loss: 0.00001332
Iteration 121/1000 | Loss: 0.00001332
Iteration 122/1000 | Loss: 0.00001332
Iteration 123/1000 | Loss: 0.00001331
Iteration 124/1000 | Loss: 0.00001331
Iteration 125/1000 | Loss: 0.00001331
Iteration 126/1000 | Loss: 0.00001330
Iteration 127/1000 | Loss: 0.00001330
Iteration 128/1000 | Loss: 0.00001330
Iteration 129/1000 | Loss: 0.00001330
Iteration 130/1000 | Loss: 0.00001329
Iteration 131/1000 | Loss: 0.00001329
Iteration 132/1000 | Loss: 0.00001329
Iteration 133/1000 | Loss: 0.00001329
Iteration 134/1000 | Loss: 0.00001329
Iteration 135/1000 | Loss: 0.00001329
Iteration 136/1000 | Loss: 0.00001329
Iteration 137/1000 | Loss: 0.00001329
Iteration 138/1000 | Loss: 0.00001329
Iteration 139/1000 | Loss: 0.00001329
Iteration 140/1000 | Loss: 0.00001329
Iteration 141/1000 | Loss: 0.00001328
Iteration 142/1000 | Loss: 0.00001328
Iteration 143/1000 | Loss: 0.00001328
Iteration 144/1000 | Loss: 0.00001328
Iteration 145/1000 | Loss: 0.00001328
Iteration 146/1000 | Loss: 0.00001328
Iteration 147/1000 | Loss: 0.00001328
Iteration 148/1000 | Loss: 0.00001328
Iteration 149/1000 | Loss: 0.00001328
Iteration 150/1000 | Loss: 0.00001328
Iteration 151/1000 | Loss: 0.00001328
Iteration 152/1000 | Loss: 0.00001327
Iteration 153/1000 | Loss: 0.00001327
Iteration 154/1000 | Loss: 0.00001327
Iteration 155/1000 | Loss: 0.00001327
Iteration 156/1000 | Loss: 0.00001327
Iteration 157/1000 | Loss: 0.00001327
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.3274760021886323e-05, 1.3274760021886323e-05, 1.3274760021886323e-05, 1.3274760021886323e-05, 1.3274760021886323e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3274760021886323e-05

Optimization complete. Final v2v error: 3.091661214828491 mm

Highest mean error: 3.177137613296509 mm for frame 266

Lowest mean error: 3.072249174118042 mm for frame 43

Saving results

Total time: 40.16768217086792
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00448079
Iteration 2/25 | Loss: 0.00128143
Iteration 3/25 | Loss: 0.00123742
Iteration 4/25 | Loss: 0.00122706
Iteration 5/25 | Loss: 0.00122405
Iteration 6/25 | Loss: 0.00122398
Iteration 7/25 | Loss: 0.00122398
Iteration 8/25 | Loss: 0.00122398
Iteration 9/25 | Loss: 0.00122398
Iteration 10/25 | Loss: 0.00122398
Iteration 11/25 | Loss: 0.00122398
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001223981729708612, 0.001223981729708612, 0.001223981729708612, 0.001223981729708612, 0.001223981729708612]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001223981729708612

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44602287
Iteration 2/25 | Loss: 0.00075712
Iteration 3/25 | Loss: 0.00075712
Iteration 4/25 | Loss: 0.00075712
Iteration 5/25 | Loss: 0.00075712
Iteration 6/25 | Loss: 0.00075712
Iteration 7/25 | Loss: 0.00075712
Iteration 8/25 | Loss: 0.00075712
Iteration 9/25 | Loss: 0.00075712
Iteration 10/25 | Loss: 0.00075712
Iteration 11/25 | Loss: 0.00075712
Iteration 12/25 | Loss: 0.00075712
Iteration 13/25 | Loss: 0.00075712
Iteration 14/25 | Loss: 0.00075712
Iteration 15/25 | Loss: 0.00075712
Iteration 16/25 | Loss: 0.00075712
Iteration 17/25 | Loss: 0.00075712
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007571171736344695, 0.0007571171736344695, 0.0007571171736344695, 0.0007571171736344695, 0.0007571171736344695]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007571171736344695

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075712
Iteration 2/1000 | Loss: 0.00003039
Iteration 3/1000 | Loss: 0.00002155
Iteration 4/1000 | Loss: 0.00001985
Iteration 5/1000 | Loss: 0.00001870
Iteration 6/1000 | Loss: 0.00001796
Iteration 7/1000 | Loss: 0.00001752
Iteration 8/1000 | Loss: 0.00001718
Iteration 9/1000 | Loss: 0.00001683
Iteration 10/1000 | Loss: 0.00001682
Iteration 11/1000 | Loss: 0.00001662
Iteration 12/1000 | Loss: 0.00001653
Iteration 13/1000 | Loss: 0.00001644
Iteration 14/1000 | Loss: 0.00001636
Iteration 15/1000 | Loss: 0.00001630
Iteration 16/1000 | Loss: 0.00001630
Iteration 17/1000 | Loss: 0.00001629
Iteration 18/1000 | Loss: 0.00001629
Iteration 19/1000 | Loss: 0.00001629
Iteration 20/1000 | Loss: 0.00001628
Iteration 21/1000 | Loss: 0.00001627
Iteration 22/1000 | Loss: 0.00001627
Iteration 23/1000 | Loss: 0.00001625
Iteration 24/1000 | Loss: 0.00001624
Iteration 25/1000 | Loss: 0.00001624
Iteration 26/1000 | Loss: 0.00001623
Iteration 27/1000 | Loss: 0.00001623
Iteration 28/1000 | Loss: 0.00001623
Iteration 29/1000 | Loss: 0.00001623
Iteration 30/1000 | Loss: 0.00001623
Iteration 31/1000 | Loss: 0.00001623
Iteration 32/1000 | Loss: 0.00001622
Iteration 33/1000 | Loss: 0.00001621
Iteration 34/1000 | Loss: 0.00001621
Iteration 35/1000 | Loss: 0.00001619
Iteration 36/1000 | Loss: 0.00001618
Iteration 37/1000 | Loss: 0.00001617
Iteration 38/1000 | Loss: 0.00001617
Iteration 39/1000 | Loss: 0.00001617
Iteration 40/1000 | Loss: 0.00001616
Iteration 41/1000 | Loss: 0.00001613
Iteration 42/1000 | Loss: 0.00001613
Iteration 43/1000 | Loss: 0.00001613
Iteration 44/1000 | Loss: 0.00001613
Iteration 45/1000 | Loss: 0.00001613
Iteration 46/1000 | Loss: 0.00001613
Iteration 47/1000 | Loss: 0.00001612
Iteration 48/1000 | Loss: 0.00001612
Iteration 49/1000 | Loss: 0.00001610
Iteration 50/1000 | Loss: 0.00001610
Iteration 51/1000 | Loss: 0.00001610
Iteration 52/1000 | Loss: 0.00001609
Iteration 53/1000 | Loss: 0.00001609
Iteration 54/1000 | Loss: 0.00001609
Iteration 55/1000 | Loss: 0.00001609
Iteration 56/1000 | Loss: 0.00001609
Iteration 57/1000 | Loss: 0.00001608
Iteration 58/1000 | Loss: 0.00001608
Iteration 59/1000 | Loss: 0.00001608
Iteration 60/1000 | Loss: 0.00001608
Iteration 61/1000 | Loss: 0.00001607
Iteration 62/1000 | Loss: 0.00001607
Iteration 63/1000 | Loss: 0.00001606
Iteration 64/1000 | Loss: 0.00001606
Iteration 65/1000 | Loss: 0.00001606
Iteration 66/1000 | Loss: 0.00001602
Iteration 67/1000 | Loss: 0.00001598
Iteration 68/1000 | Loss: 0.00001597
Iteration 69/1000 | Loss: 0.00001596
Iteration 70/1000 | Loss: 0.00001594
Iteration 71/1000 | Loss: 0.00001594
Iteration 72/1000 | Loss: 0.00001594
Iteration 73/1000 | Loss: 0.00001594
Iteration 74/1000 | Loss: 0.00001594
Iteration 75/1000 | Loss: 0.00001593
Iteration 76/1000 | Loss: 0.00001593
Iteration 77/1000 | Loss: 0.00001593
Iteration 78/1000 | Loss: 0.00001593
Iteration 79/1000 | Loss: 0.00001592
Iteration 80/1000 | Loss: 0.00001591
Iteration 81/1000 | Loss: 0.00001591
Iteration 82/1000 | Loss: 0.00001591
Iteration 83/1000 | Loss: 0.00001591
Iteration 84/1000 | Loss: 0.00001591
Iteration 85/1000 | Loss: 0.00001590
Iteration 86/1000 | Loss: 0.00001590
Iteration 87/1000 | Loss: 0.00001590
Iteration 88/1000 | Loss: 0.00001590
Iteration 89/1000 | Loss: 0.00001590
Iteration 90/1000 | Loss: 0.00001590
Iteration 91/1000 | Loss: 0.00001589
Iteration 92/1000 | Loss: 0.00001589
Iteration 93/1000 | Loss: 0.00001588
Iteration 94/1000 | Loss: 0.00001588
Iteration 95/1000 | Loss: 0.00001588
Iteration 96/1000 | Loss: 0.00001588
Iteration 97/1000 | Loss: 0.00001588
Iteration 98/1000 | Loss: 0.00001587
Iteration 99/1000 | Loss: 0.00001587
Iteration 100/1000 | Loss: 0.00001587
Iteration 101/1000 | Loss: 0.00001587
Iteration 102/1000 | Loss: 0.00001587
Iteration 103/1000 | Loss: 0.00001587
Iteration 104/1000 | Loss: 0.00001586
Iteration 105/1000 | Loss: 0.00001586
Iteration 106/1000 | Loss: 0.00001586
Iteration 107/1000 | Loss: 0.00001586
Iteration 108/1000 | Loss: 0.00001586
Iteration 109/1000 | Loss: 0.00001586
Iteration 110/1000 | Loss: 0.00001586
Iteration 111/1000 | Loss: 0.00001586
Iteration 112/1000 | Loss: 0.00001585
Iteration 113/1000 | Loss: 0.00001585
Iteration 114/1000 | Loss: 0.00001585
Iteration 115/1000 | Loss: 0.00001585
Iteration 116/1000 | Loss: 0.00001585
Iteration 117/1000 | Loss: 0.00001585
Iteration 118/1000 | Loss: 0.00001585
Iteration 119/1000 | Loss: 0.00001585
Iteration 120/1000 | Loss: 0.00001585
Iteration 121/1000 | Loss: 0.00001585
Iteration 122/1000 | Loss: 0.00001585
Iteration 123/1000 | Loss: 0.00001585
Iteration 124/1000 | Loss: 0.00001585
Iteration 125/1000 | Loss: 0.00001585
Iteration 126/1000 | Loss: 0.00001585
Iteration 127/1000 | Loss: 0.00001585
Iteration 128/1000 | Loss: 0.00001585
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.5849602277739905e-05, 1.5849602277739905e-05, 1.5849602277739905e-05, 1.5849602277739905e-05, 1.5849602277739905e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5849602277739905e-05

Optimization complete. Final v2v error: 3.3833060264587402 mm

Highest mean error: 3.520317554473877 mm for frame 139

Lowest mean error: 3.2410905361175537 mm for frame 17

Saving results

Total time: 35.74721074104309
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00800892
Iteration 2/25 | Loss: 0.00145648
Iteration 3/25 | Loss: 0.00125587
Iteration 4/25 | Loss: 0.00123260
Iteration 5/25 | Loss: 0.00122581
Iteration 6/25 | Loss: 0.00122446
Iteration 7/25 | Loss: 0.00122446
Iteration 8/25 | Loss: 0.00122446
Iteration 9/25 | Loss: 0.00122446
Iteration 10/25 | Loss: 0.00122446
Iteration 11/25 | Loss: 0.00122446
Iteration 12/25 | Loss: 0.00122446
Iteration 13/25 | Loss: 0.00122446
Iteration 14/25 | Loss: 0.00122446
Iteration 15/25 | Loss: 0.00122446
Iteration 16/25 | Loss: 0.00122446
Iteration 17/25 | Loss: 0.00122446
Iteration 18/25 | Loss: 0.00122446
Iteration 19/25 | Loss: 0.00122446
Iteration 20/25 | Loss: 0.00122446
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0012244594981893897, 0.0012244594981893897, 0.0012244594981893897, 0.0012244594981893897, 0.0012244594981893897]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012244594981893897

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46936584
Iteration 2/25 | Loss: 0.00078677
Iteration 3/25 | Loss: 0.00078677
Iteration 4/25 | Loss: 0.00078677
Iteration 5/25 | Loss: 0.00078677
Iteration 6/25 | Loss: 0.00078677
Iteration 7/25 | Loss: 0.00078677
Iteration 8/25 | Loss: 0.00078677
Iteration 9/25 | Loss: 0.00078677
Iteration 10/25 | Loss: 0.00078677
Iteration 11/25 | Loss: 0.00078677
Iteration 12/25 | Loss: 0.00078677
Iteration 13/25 | Loss: 0.00078677
Iteration 14/25 | Loss: 0.00078677
Iteration 15/25 | Loss: 0.00078677
Iteration 16/25 | Loss: 0.00078677
Iteration 17/25 | Loss: 0.00078677
Iteration 18/25 | Loss: 0.00078677
Iteration 19/25 | Loss: 0.00078677
Iteration 20/25 | Loss: 0.00078677
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007867681561037898, 0.0007867681561037898, 0.0007867681561037898, 0.0007867681561037898, 0.0007867681561037898]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007867681561037898

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078677
Iteration 2/1000 | Loss: 0.00003786
Iteration 3/1000 | Loss: 0.00002543
Iteration 4/1000 | Loss: 0.00002130
Iteration 5/1000 | Loss: 0.00001962
Iteration 6/1000 | Loss: 0.00001826
Iteration 7/1000 | Loss: 0.00001739
Iteration 8/1000 | Loss: 0.00001678
Iteration 9/1000 | Loss: 0.00001633
Iteration 10/1000 | Loss: 0.00001606
Iteration 11/1000 | Loss: 0.00001581
Iteration 12/1000 | Loss: 0.00001559
Iteration 13/1000 | Loss: 0.00001553
Iteration 14/1000 | Loss: 0.00001548
Iteration 15/1000 | Loss: 0.00001548
Iteration 16/1000 | Loss: 0.00001546
Iteration 17/1000 | Loss: 0.00001545
Iteration 18/1000 | Loss: 0.00001539
Iteration 19/1000 | Loss: 0.00001536
Iteration 20/1000 | Loss: 0.00001535
Iteration 21/1000 | Loss: 0.00001532
Iteration 22/1000 | Loss: 0.00001532
Iteration 23/1000 | Loss: 0.00001527
Iteration 24/1000 | Loss: 0.00001526
Iteration 25/1000 | Loss: 0.00001522
Iteration 26/1000 | Loss: 0.00001522
Iteration 27/1000 | Loss: 0.00001520
Iteration 28/1000 | Loss: 0.00001520
Iteration 29/1000 | Loss: 0.00001519
Iteration 30/1000 | Loss: 0.00001519
Iteration 31/1000 | Loss: 0.00001519
Iteration 32/1000 | Loss: 0.00001518
Iteration 33/1000 | Loss: 0.00001518
Iteration 34/1000 | Loss: 0.00001518
Iteration 35/1000 | Loss: 0.00001517
Iteration 36/1000 | Loss: 0.00001517
Iteration 37/1000 | Loss: 0.00001516
Iteration 38/1000 | Loss: 0.00001516
Iteration 39/1000 | Loss: 0.00001516
Iteration 40/1000 | Loss: 0.00001515
Iteration 41/1000 | Loss: 0.00001515
Iteration 42/1000 | Loss: 0.00001515
Iteration 43/1000 | Loss: 0.00001515
Iteration 44/1000 | Loss: 0.00001515
Iteration 45/1000 | Loss: 0.00001514
Iteration 46/1000 | Loss: 0.00001514
Iteration 47/1000 | Loss: 0.00001514
Iteration 48/1000 | Loss: 0.00001514
Iteration 49/1000 | Loss: 0.00001514
Iteration 50/1000 | Loss: 0.00001514
Iteration 51/1000 | Loss: 0.00001514
Iteration 52/1000 | Loss: 0.00001513
Iteration 53/1000 | Loss: 0.00001513
Iteration 54/1000 | Loss: 0.00001512
Iteration 55/1000 | Loss: 0.00001512
Iteration 56/1000 | Loss: 0.00001512
Iteration 57/1000 | Loss: 0.00001512
Iteration 58/1000 | Loss: 0.00001512
Iteration 59/1000 | Loss: 0.00001511
Iteration 60/1000 | Loss: 0.00001511
Iteration 61/1000 | Loss: 0.00001511
Iteration 62/1000 | Loss: 0.00001511
Iteration 63/1000 | Loss: 0.00001511
Iteration 64/1000 | Loss: 0.00001511
Iteration 65/1000 | Loss: 0.00001511
Iteration 66/1000 | Loss: 0.00001510
Iteration 67/1000 | Loss: 0.00001510
Iteration 68/1000 | Loss: 0.00001510
Iteration 69/1000 | Loss: 0.00001510
Iteration 70/1000 | Loss: 0.00001510
Iteration 71/1000 | Loss: 0.00001509
Iteration 72/1000 | Loss: 0.00001509
Iteration 73/1000 | Loss: 0.00001509
Iteration 74/1000 | Loss: 0.00001509
Iteration 75/1000 | Loss: 0.00001509
Iteration 76/1000 | Loss: 0.00001509
Iteration 77/1000 | Loss: 0.00001509
Iteration 78/1000 | Loss: 0.00001508
Iteration 79/1000 | Loss: 0.00001508
Iteration 80/1000 | Loss: 0.00001508
Iteration 81/1000 | Loss: 0.00001507
Iteration 82/1000 | Loss: 0.00001507
Iteration 83/1000 | Loss: 0.00001507
Iteration 84/1000 | Loss: 0.00001507
Iteration 85/1000 | Loss: 0.00001506
Iteration 86/1000 | Loss: 0.00001506
Iteration 87/1000 | Loss: 0.00001506
Iteration 88/1000 | Loss: 0.00001506
Iteration 89/1000 | Loss: 0.00001505
Iteration 90/1000 | Loss: 0.00001505
Iteration 91/1000 | Loss: 0.00001504
Iteration 92/1000 | Loss: 0.00001504
Iteration 93/1000 | Loss: 0.00001504
Iteration 94/1000 | Loss: 0.00001504
Iteration 95/1000 | Loss: 0.00001504
Iteration 96/1000 | Loss: 0.00001504
Iteration 97/1000 | Loss: 0.00001504
Iteration 98/1000 | Loss: 0.00001504
Iteration 99/1000 | Loss: 0.00001504
Iteration 100/1000 | Loss: 0.00001503
Iteration 101/1000 | Loss: 0.00001503
Iteration 102/1000 | Loss: 0.00001503
Iteration 103/1000 | Loss: 0.00001503
Iteration 104/1000 | Loss: 0.00001502
Iteration 105/1000 | Loss: 0.00001502
Iteration 106/1000 | Loss: 0.00001502
Iteration 107/1000 | Loss: 0.00001502
Iteration 108/1000 | Loss: 0.00001501
Iteration 109/1000 | Loss: 0.00001501
Iteration 110/1000 | Loss: 0.00001501
Iteration 111/1000 | Loss: 0.00001501
Iteration 112/1000 | Loss: 0.00001501
Iteration 113/1000 | Loss: 0.00001501
Iteration 114/1000 | Loss: 0.00001501
Iteration 115/1000 | Loss: 0.00001500
Iteration 116/1000 | Loss: 0.00001500
Iteration 117/1000 | Loss: 0.00001500
Iteration 118/1000 | Loss: 0.00001500
Iteration 119/1000 | Loss: 0.00001500
Iteration 120/1000 | Loss: 0.00001500
Iteration 121/1000 | Loss: 0.00001500
Iteration 122/1000 | Loss: 0.00001500
Iteration 123/1000 | Loss: 0.00001500
Iteration 124/1000 | Loss: 0.00001500
Iteration 125/1000 | Loss: 0.00001499
Iteration 126/1000 | Loss: 0.00001499
Iteration 127/1000 | Loss: 0.00001499
Iteration 128/1000 | Loss: 0.00001499
Iteration 129/1000 | Loss: 0.00001499
Iteration 130/1000 | Loss: 0.00001499
Iteration 131/1000 | Loss: 0.00001498
Iteration 132/1000 | Loss: 0.00001498
Iteration 133/1000 | Loss: 0.00001498
Iteration 134/1000 | Loss: 0.00001498
Iteration 135/1000 | Loss: 0.00001498
Iteration 136/1000 | Loss: 0.00001497
Iteration 137/1000 | Loss: 0.00001497
Iteration 138/1000 | Loss: 0.00001497
Iteration 139/1000 | Loss: 0.00001497
Iteration 140/1000 | Loss: 0.00001497
Iteration 141/1000 | Loss: 0.00001497
Iteration 142/1000 | Loss: 0.00001497
Iteration 143/1000 | Loss: 0.00001497
Iteration 144/1000 | Loss: 0.00001497
Iteration 145/1000 | Loss: 0.00001497
Iteration 146/1000 | Loss: 0.00001497
Iteration 147/1000 | Loss: 0.00001496
Iteration 148/1000 | Loss: 0.00001496
Iteration 149/1000 | Loss: 0.00001496
Iteration 150/1000 | Loss: 0.00001496
Iteration 151/1000 | Loss: 0.00001496
Iteration 152/1000 | Loss: 0.00001496
Iteration 153/1000 | Loss: 0.00001496
Iteration 154/1000 | Loss: 0.00001496
Iteration 155/1000 | Loss: 0.00001496
Iteration 156/1000 | Loss: 0.00001496
Iteration 157/1000 | Loss: 0.00001496
Iteration 158/1000 | Loss: 0.00001496
Iteration 159/1000 | Loss: 0.00001496
Iteration 160/1000 | Loss: 0.00001496
Iteration 161/1000 | Loss: 0.00001496
Iteration 162/1000 | Loss: 0.00001496
Iteration 163/1000 | Loss: 0.00001496
Iteration 164/1000 | Loss: 0.00001496
Iteration 165/1000 | Loss: 0.00001496
Iteration 166/1000 | Loss: 0.00001496
Iteration 167/1000 | Loss: 0.00001496
Iteration 168/1000 | Loss: 0.00001496
Iteration 169/1000 | Loss: 0.00001496
Iteration 170/1000 | Loss: 0.00001496
Iteration 171/1000 | Loss: 0.00001496
Iteration 172/1000 | Loss: 0.00001496
Iteration 173/1000 | Loss: 0.00001496
Iteration 174/1000 | Loss: 0.00001496
Iteration 175/1000 | Loss: 0.00001496
Iteration 176/1000 | Loss: 0.00001496
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.4963923604227602e-05, 1.4963923604227602e-05, 1.4963923604227602e-05, 1.4963923604227602e-05, 1.4963923604227602e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4963923604227602e-05

Optimization complete. Final v2v error: 3.2966384887695312 mm

Highest mean error: 3.7293543815612793 mm for frame 79

Lowest mean error: 2.942601442337036 mm for frame 154

Saving results

Total time: 41.89668560028076
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00994284
Iteration 2/25 | Loss: 0.00994283
Iteration 3/25 | Loss: 0.00269977
Iteration 4/25 | Loss: 0.00211599
Iteration 5/25 | Loss: 0.00203531
Iteration 6/25 | Loss: 0.00209731
Iteration 7/25 | Loss: 0.00200392
Iteration 8/25 | Loss: 0.00194098
Iteration 9/25 | Loss: 0.00190129
Iteration 10/25 | Loss: 0.00193097
Iteration 11/25 | Loss: 0.00177252
Iteration 12/25 | Loss: 0.00173757
Iteration 13/25 | Loss: 0.00169030
Iteration 14/25 | Loss: 0.00171251
Iteration 15/25 | Loss: 0.00169449
Iteration 16/25 | Loss: 0.00165668
Iteration 17/25 | Loss: 0.00159245
Iteration 18/25 | Loss: 0.00158627
Iteration 19/25 | Loss: 0.00157741
Iteration 20/25 | Loss: 0.00157331
Iteration 21/25 | Loss: 0.00157220
Iteration 22/25 | Loss: 0.00157565
Iteration 23/25 | Loss: 0.00157097
Iteration 24/25 | Loss: 0.00156844
Iteration 25/25 | Loss: 0.00157524

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42411733
Iteration 2/25 | Loss: 0.00624193
Iteration 3/25 | Loss: 0.00197348
Iteration 4/25 | Loss: 0.00197346
Iteration 5/25 | Loss: 0.00197346
Iteration 6/25 | Loss: 0.00197346
Iteration 7/25 | Loss: 0.00197346
Iteration 8/25 | Loss: 0.00197346
Iteration 9/25 | Loss: 0.00197346
Iteration 10/25 | Loss: 0.00197346
Iteration 11/25 | Loss: 0.00197346
Iteration 12/25 | Loss: 0.00197346
Iteration 13/25 | Loss: 0.00197346
Iteration 14/25 | Loss: 0.00197346
Iteration 15/25 | Loss: 0.00197346
Iteration 16/25 | Loss: 0.00197346
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0019734599627554417, 0.0019734599627554417, 0.0019734599627554417, 0.0019734599627554417, 0.0019734599627554417]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019734599627554417

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00197346
Iteration 2/1000 | Loss: 0.00316606
Iteration 3/1000 | Loss: 0.00498335
Iteration 4/1000 | Loss: 0.00134950
Iteration 5/1000 | Loss: 0.00239570
Iteration 6/1000 | Loss: 0.00179686
Iteration 7/1000 | Loss: 0.00096194
Iteration 8/1000 | Loss: 0.00118112
Iteration 9/1000 | Loss: 0.00173702
Iteration 10/1000 | Loss: 0.00200027
Iteration 11/1000 | Loss: 0.00193009
Iteration 12/1000 | Loss: 0.00185648
Iteration 13/1000 | Loss: 0.00185604
Iteration 14/1000 | Loss: 0.00116334
Iteration 15/1000 | Loss: 0.00137990
Iteration 16/1000 | Loss: 0.00312016
Iteration 17/1000 | Loss: 0.00199787
Iteration 18/1000 | Loss: 0.00147709
Iteration 19/1000 | Loss: 0.00167621
Iteration 20/1000 | Loss: 0.00105415
Iteration 21/1000 | Loss: 0.00091520
Iteration 22/1000 | Loss: 0.00231744
Iteration 23/1000 | Loss: 0.00439496
Iteration 24/1000 | Loss: 0.00122920
Iteration 25/1000 | Loss: 0.00060166
Iteration 26/1000 | Loss: 0.00052350
Iteration 27/1000 | Loss: 0.00036215
Iteration 28/1000 | Loss: 0.00035868
Iteration 29/1000 | Loss: 0.00011796
Iteration 30/1000 | Loss: 0.00030296
Iteration 31/1000 | Loss: 0.00051490
Iteration 32/1000 | Loss: 0.00091929
Iteration 33/1000 | Loss: 0.00018531
Iteration 34/1000 | Loss: 0.00096776
Iteration 35/1000 | Loss: 0.00109152
Iteration 36/1000 | Loss: 0.00136056
Iteration 37/1000 | Loss: 0.00094469
Iteration 38/1000 | Loss: 0.00015621
Iteration 39/1000 | Loss: 0.00009345
Iteration 40/1000 | Loss: 0.00035741
Iteration 41/1000 | Loss: 0.00024127
Iteration 42/1000 | Loss: 0.00008447
Iteration 43/1000 | Loss: 0.00016161
Iteration 44/1000 | Loss: 0.00039065
Iteration 45/1000 | Loss: 0.00016058
Iteration 46/1000 | Loss: 0.00008302
Iteration 47/1000 | Loss: 0.00054382
Iteration 48/1000 | Loss: 0.00012158
Iteration 49/1000 | Loss: 0.00013839
Iteration 50/1000 | Loss: 0.00009129
Iteration 51/1000 | Loss: 0.00054894
Iteration 52/1000 | Loss: 0.00020775
Iteration 53/1000 | Loss: 0.00038373
Iteration 54/1000 | Loss: 0.00006603
Iteration 55/1000 | Loss: 0.00003905
Iteration 56/1000 | Loss: 0.00010317
Iteration 57/1000 | Loss: 0.00004457
Iteration 58/1000 | Loss: 0.00062187
Iteration 59/1000 | Loss: 0.00067587
Iteration 60/1000 | Loss: 0.00062541
Iteration 61/1000 | Loss: 0.00076838
Iteration 62/1000 | Loss: 0.00043235
Iteration 63/1000 | Loss: 0.00017085
Iteration 64/1000 | Loss: 0.00004259
Iteration 65/1000 | Loss: 0.00005171
Iteration 66/1000 | Loss: 0.00006161
Iteration 67/1000 | Loss: 0.00003082
Iteration 68/1000 | Loss: 0.00006881
Iteration 69/1000 | Loss: 0.00024844
Iteration 70/1000 | Loss: 0.00003244
Iteration 71/1000 | Loss: 0.00006935
Iteration 72/1000 | Loss: 0.00002836
Iteration 73/1000 | Loss: 0.00002799
Iteration 74/1000 | Loss: 0.00018459
Iteration 75/1000 | Loss: 0.00006337
Iteration 76/1000 | Loss: 0.00035544
Iteration 77/1000 | Loss: 0.00004138
Iteration 78/1000 | Loss: 0.00002753
Iteration 79/1000 | Loss: 0.00007678
Iteration 80/1000 | Loss: 0.00012070
Iteration 81/1000 | Loss: 0.00003460
Iteration 82/1000 | Loss: 0.00003587
Iteration 83/1000 | Loss: 0.00002710
Iteration 84/1000 | Loss: 0.00002708
Iteration 85/1000 | Loss: 0.00002700
Iteration 86/1000 | Loss: 0.00002700
Iteration 87/1000 | Loss: 0.00002699
Iteration 88/1000 | Loss: 0.00002698
Iteration 89/1000 | Loss: 0.00002697
Iteration 90/1000 | Loss: 0.00002694
Iteration 91/1000 | Loss: 0.00002693
Iteration 92/1000 | Loss: 0.00002692
Iteration 93/1000 | Loss: 0.00002691
Iteration 94/1000 | Loss: 0.00002690
Iteration 95/1000 | Loss: 0.00002689
Iteration 96/1000 | Loss: 0.00002685
Iteration 97/1000 | Loss: 0.00002684
Iteration 98/1000 | Loss: 0.00042237
Iteration 99/1000 | Loss: 0.00045002
Iteration 100/1000 | Loss: 0.00010803
Iteration 101/1000 | Loss: 0.00021927
Iteration 102/1000 | Loss: 0.00008122
Iteration 103/1000 | Loss: 0.00002907
Iteration 104/1000 | Loss: 0.00005017
Iteration 105/1000 | Loss: 0.00002990
Iteration 106/1000 | Loss: 0.00002600
Iteration 107/1000 | Loss: 0.00008199
Iteration 108/1000 | Loss: 0.00014241
Iteration 109/1000 | Loss: 0.00002502
Iteration 110/1000 | Loss: 0.00003394
Iteration 111/1000 | Loss: 0.00002466
Iteration 112/1000 | Loss: 0.00002465
Iteration 113/1000 | Loss: 0.00002449
Iteration 114/1000 | Loss: 0.00002436
Iteration 115/1000 | Loss: 0.00007019
Iteration 116/1000 | Loss: 0.00002438
Iteration 117/1000 | Loss: 0.00002413
Iteration 118/1000 | Loss: 0.00002411
Iteration 119/1000 | Loss: 0.00002411
Iteration 120/1000 | Loss: 0.00002410
Iteration 121/1000 | Loss: 0.00002409
Iteration 122/1000 | Loss: 0.00002408
Iteration 123/1000 | Loss: 0.00002407
Iteration 124/1000 | Loss: 0.00002403
Iteration 125/1000 | Loss: 0.00002400
Iteration 126/1000 | Loss: 0.00002399
Iteration 127/1000 | Loss: 0.00002398
Iteration 128/1000 | Loss: 0.00002397
Iteration 129/1000 | Loss: 0.00002397
Iteration 130/1000 | Loss: 0.00002396
Iteration 131/1000 | Loss: 0.00002396
Iteration 132/1000 | Loss: 0.00002396
Iteration 133/1000 | Loss: 0.00002395
Iteration 134/1000 | Loss: 0.00002395
Iteration 135/1000 | Loss: 0.00002394
Iteration 136/1000 | Loss: 0.00002394
Iteration 137/1000 | Loss: 0.00002393
Iteration 138/1000 | Loss: 0.00002392
Iteration 139/1000 | Loss: 0.00002392
Iteration 140/1000 | Loss: 0.00002392
Iteration 141/1000 | Loss: 0.00002391
Iteration 142/1000 | Loss: 0.00002391
Iteration 143/1000 | Loss: 0.00002391
Iteration 144/1000 | Loss: 0.00002391
Iteration 145/1000 | Loss: 0.00002391
Iteration 146/1000 | Loss: 0.00002390
Iteration 147/1000 | Loss: 0.00002390
Iteration 148/1000 | Loss: 0.00002390
Iteration 149/1000 | Loss: 0.00002390
Iteration 150/1000 | Loss: 0.00002390
Iteration 151/1000 | Loss: 0.00002389
Iteration 152/1000 | Loss: 0.00002389
Iteration 153/1000 | Loss: 0.00002389
Iteration 154/1000 | Loss: 0.00002389
Iteration 155/1000 | Loss: 0.00002388
Iteration 156/1000 | Loss: 0.00002388
Iteration 157/1000 | Loss: 0.00002388
Iteration 158/1000 | Loss: 0.00002388
Iteration 159/1000 | Loss: 0.00002388
Iteration 160/1000 | Loss: 0.00002387
Iteration 161/1000 | Loss: 0.00002387
Iteration 162/1000 | Loss: 0.00002387
Iteration 163/1000 | Loss: 0.00002387
Iteration 164/1000 | Loss: 0.00002387
Iteration 165/1000 | Loss: 0.00002387
Iteration 166/1000 | Loss: 0.00002386
Iteration 167/1000 | Loss: 0.00002386
Iteration 168/1000 | Loss: 0.00002386
Iteration 169/1000 | Loss: 0.00002385
Iteration 170/1000 | Loss: 0.00002385
Iteration 171/1000 | Loss: 0.00002385
Iteration 172/1000 | Loss: 0.00002385
Iteration 173/1000 | Loss: 0.00002385
Iteration 174/1000 | Loss: 0.00002385
Iteration 175/1000 | Loss: 0.00002385
Iteration 176/1000 | Loss: 0.00002385
Iteration 177/1000 | Loss: 0.00002385
Iteration 178/1000 | Loss: 0.00002384
Iteration 179/1000 | Loss: 0.00002384
Iteration 180/1000 | Loss: 0.00002384
Iteration 181/1000 | Loss: 0.00002384
Iteration 182/1000 | Loss: 0.00002384
Iteration 183/1000 | Loss: 0.00002384
Iteration 184/1000 | Loss: 0.00002384
Iteration 185/1000 | Loss: 0.00002384
Iteration 186/1000 | Loss: 0.00002384
Iteration 187/1000 | Loss: 0.00002384
Iteration 188/1000 | Loss: 0.00002384
Iteration 189/1000 | Loss: 0.00002384
Iteration 190/1000 | Loss: 0.00002383
Iteration 191/1000 | Loss: 0.00002383
Iteration 192/1000 | Loss: 0.00002383
Iteration 193/1000 | Loss: 0.00002383
Iteration 194/1000 | Loss: 0.00002383
Iteration 195/1000 | Loss: 0.00002383
Iteration 196/1000 | Loss: 0.00002383
Iteration 197/1000 | Loss: 0.00002383
Iteration 198/1000 | Loss: 0.00002383
Iteration 199/1000 | Loss: 0.00002383
Iteration 200/1000 | Loss: 0.00007073
Iteration 201/1000 | Loss: 0.00002385
Iteration 202/1000 | Loss: 0.00002383
Iteration 203/1000 | Loss: 0.00002383
Iteration 204/1000 | Loss: 0.00002383
Iteration 205/1000 | Loss: 0.00002383
Iteration 206/1000 | Loss: 0.00002383
Iteration 207/1000 | Loss: 0.00002382
Iteration 208/1000 | Loss: 0.00002382
Iteration 209/1000 | Loss: 0.00002382
Iteration 210/1000 | Loss: 0.00002382
Iteration 211/1000 | Loss: 0.00002382
Iteration 212/1000 | Loss: 0.00002382
Iteration 213/1000 | Loss: 0.00002382
Iteration 214/1000 | Loss: 0.00002382
Iteration 215/1000 | Loss: 0.00002382
Iteration 216/1000 | Loss: 0.00002381
Iteration 217/1000 | Loss: 0.00002381
Iteration 218/1000 | Loss: 0.00002381
Iteration 219/1000 | Loss: 0.00002381
Iteration 220/1000 | Loss: 0.00002381
Iteration 221/1000 | Loss: 0.00002381
Iteration 222/1000 | Loss: 0.00002381
Iteration 223/1000 | Loss: 0.00002381
Iteration 224/1000 | Loss: 0.00002381
Iteration 225/1000 | Loss: 0.00002381
Iteration 226/1000 | Loss: 0.00003766
Iteration 227/1000 | Loss: 0.00003158
Iteration 228/1000 | Loss: 0.00002382
Iteration 229/1000 | Loss: 0.00002382
Iteration 230/1000 | Loss: 0.00002382
Iteration 231/1000 | Loss: 0.00002382
Iteration 232/1000 | Loss: 0.00002382
Iteration 233/1000 | Loss: 0.00002382
Iteration 234/1000 | Loss: 0.00002382
Iteration 235/1000 | Loss: 0.00002382
Iteration 236/1000 | Loss: 0.00002382
Iteration 237/1000 | Loss: 0.00002382
Iteration 238/1000 | Loss: 0.00002381
Iteration 239/1000 | Loss: 0.00002381
Iteration 240/1000 | Loss: 0.00002381
Iteration 241/1000 | Loss: 0.00002381
Iteration 242/1000 | Loss: 0.00002381
Iteration 243/1000 | Loss: 0.00002381
Iteration 244/1000 | Loss: 0.00002381
Iteration 245/1000 | Loss: 0.00002381
Iteration 246/1000 | Loss: 0.00002381
Iteration 247/1000 | Loss: 0.00002381
Iteration 248/1000 | Loss: 0.00002381
Iteration 249/1000 | Loss: 0.00002381
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 249. Stopping optimization.
Last 5 losses: [2.381494596193079e-05, 2.381494596193079e-05, 2.381494596193079e-05, 2.381494596193079e-05, 2.381494596193079e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.381494596193079e-05

Optimization complete. Final v2v error: 3.429739475250244 mm

Highest mean error: 10.173270225524902 mm for frame 36

Lowest mean error: 2.8491880893707275 mm for frame 162

Saving results

Total time: 231.1657531261444
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00826994
Iteration 2/25 | Loss: 0.00147585
Iteration 3/25 | Loss: 0.00127312
Iteration 4/25 | Loss: 0.00124372
Iteration 5/25 | Loss: 0.00123757
Iteration 6/25 | Loss: 0.00123650
Iteration 7/25 | Loss: 0.00123650
Iteration 8/25 | Loss: 0.00123650
Iteration 9/25 | Loss: 0.00123650
Iteration 10/25 | Loss: 0.00123650
Iteration 11/25 | Loss: 0.00123650
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012364975409582257, 0.0012364975409582257, 0.0012364975409582257, 0.0012364975409582257, 0.0012364975409582257]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012364975409582257

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.03072596
Iteration 2/25 | Loss: 0.00052295
Iteration 3/25 | Loss: 0.00052294
Iteration 4/25 | Loss: 0.00052294
Iteration 5/25 | Loss: 0.00052294
Iteration 6/25 | Loss: 0.00052294
Iteration 7/25 | Loss: 0.00052294
Iteration 8/25 | Loss: 0.00052294
Iteration 9/25 | Loss: 0.00052294
Iteration 10/25 | Loss: 0.00052294
Iteration 11/25 | Loss: 0.00052294
Iteration 12/25 | Loss: 0.00052294
Iteration 13/25 | Loss: 0.00052294
Iteration 14/25 | Loss: 0.00052294
Iteration 15/25 | Loss: 0.00052294
Iteration 16/25 | Loss: 0.00052294
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005229389062151313, 0.0005229389062151313, 0.0005229389062151313, 0.0005229389062151313, 0.0005229389062151313]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005229389062151313

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052294
Iteration 2/1000 | Loss: 0.00004624
Iteration 3/1000 | Loss: 0.00003359
Iteration 4/1000 | Loss: 0.00002966
Iteration 5/1000 | Loss: 0.00002829
Iteration 6/1000 | Loss: 0.00002659
Iteration 7/1000 | Loss: 0.00002568
Iteration 8/1000 | Loss: 0.00002499
Iteration 9/1000 | Loss: 0.00002462
Iteration 10/1000 | Loss: 0.00002430
Iteration 11/1000 | Loss: 0.00002407
Iteration 12/1000 | Loss: 0.00002382
Iteration 13/1000 | Loss: 0.00002358
Iteration 14/1000 | Loss: 0.00002357
Iteration 15/1000 | Loss: 0.00002336
Iteration 16/1000 | Loss: 0.00002336
Iteration 17/1000 | Loss: 0.00002327
Iteration 18/1000 | Loss: 0.00002313
Iteration 19/1000 | Loss: 0.00002311
Iteration 20/1000 | Loss: 0.00002311
Iteration 21/1000 | Loss: 0.00002311
Iteration 22/1000 | Loss: 0.00002311
Iteration 23/1000 | Loss: 0.00002311
Iteration 24/1000 | Loss: 0.00002311
Iteration 25/1000 | Loss: 0.00002310
Iteration 26/1000 | Loss: 0.00002310
Iteration 27/1000 | Loss: 0.00002310
Iteration 28/1000 | Loss: 0.00002310
Iteration 29/1000 | Loss: 0.00002306
Iteration 30/1000 | Loss: 0.00002305
Iteration 31/1000 | Loss: 0.00002305
Iteration 32/1000 | Loss: 0.00002305
Iteration 33/1000 | Loss: 0.00002304
Iteration 34/1000 | Loss: 0.00002303
Iteration 35/1000 | Loss: 0.00002303
Iteration 36/1000 | Loss: 0.00002303
Iteration 37/1000 | Loss: 0.00002303
Iteration 38/1000 | Loss: 0.00002303
Iteration 39/1000 | Loss: 0.00002303
Iteration 40/1000 | Loss: 0.00002303
Iteration 41/1000 | Loss: 0.00002303
Iteration 42/1000 | Loss: 0.00002302
Iteration 43/1000 | Loss: 0.00002302
Iteration 44/1000 | Loss: 0.00002302
Iteration 45/1000 | Loss: 0.00002302
Iteration 46/1000 | Loss: 0.00002302
Iteration 47/1000 | Loss: 0.00002302
Iteration 48/1000 | Loss: 0.00002301
Iteration 49/1000 | Loss: 0.00002301
Iteration 50/1000 | Loss: 0.00002301
Iteration 51/1000 | Loss: 0.00002300
Iteration 52/1000 | Loss: 0.00002300
Iteration 53/1000 | Loss: 0.00002298
Iteration 54/1000 | Loss: 0.00002298
Iteration 55/1000 | Loss: 0.00002298
Iteration 56/1000 | Loss: 0.00002297
Iteration 57/1000 | Loss: 0.00002297
Iteration 58/1000 | Loss: 0.00002296
Iteration 59/1000 | Loss: 0.00002296
Iteration 60/1000 | Loss: 0.00002295
Iteration 61/1000 | Loss: 0.00002295
Iteration 62/1000 | Loss: 0.00002294
Iteration 63/1000 | Loss: 0.00002294
Iteration 64/1000 | Loss: 0.00002294
Iteration 65/1000 | Loss: 0.00002294
Iteration 66/1000 | Loss: 0.00002294
Iteration 67/1000 | Loss: 0.00002294
Iteration 68/1000 | Loss: 0.00002293
Iteration 69/1000 | Loss: 0.00002293
Iteration 70/1000 | Loss: 0.00002293
Iteration 71/1000 | Loss: 0.00002292
Iteration 72/1000 | Loss: 0.00002292
Iteration 73/1000 | Loss: 0.00002292
Iteration 74/1000 | Loss: 0.00002292
Iteration 75/1000 | Loss: 0.00002291
Iteration 76/1000 | Loss: 0.00002290
Iteration 77/1000 | Loss: 0.00002290
Iteration 78/1000 | Loss: 0.00002288
Iteration 79/1000 | Loss: 0.00002287
Iteration 80/1000 | Loss: 0.00002287
Iteration 81/1000 | Loss: 0.00002286
Iteration 82/1000 | Loss: 0.00002285
Iteration 83/1000 | Loss: 0.00002285
Iteration 84/1000 | Loss: 0.00002285
Iteration 85/1000 | Loss: 0.00002285
Iteration 86/1000 | Loss: 0.00002284
Iteration 87/1000 | Loss: 0.00002284
Iteration 88/1000 | Loss: 0.00002284
Iteration 89/1000 | Loss: 0.00002284
Iteration 90/1000 | Loss: 0.00002284
Iteration 91/1000 | Loss: 0.00002284
Iteration 92/1000 | Loss: 0.00002284
Iteration 93/1000 | Loss: 0.00002284
Iteration 94/1000 | Loss: 0.00002284
Iteration 95/1000 | Loss: 0.00002284
Iteration 96/1000 | Loss: 0.00002284
Iteration 97/1000 | Loss: 0.00002283
Iteration 98/1000 | Loss: 0.00002283
Iteration 99/1000 | Loss: 0.00002283
Iteration 100/1000 | Loss: 0.00002283
Iteration 101/1000 | Loss: 0.00002283
Iteration 102/1000 | Loss: 0.00002283
Iteration 103/1000 | Loss: 0.00002282
Iteration 104/1000 | Loss: 0.00002282
Iteration 105/1000 | Loss: 0.00002282
Iteration 106/1000 | Loss: 0.00002282
Iteration 107/1000 | Loss: 0.00002282
Iteration 108/1000 | Loss: 0.00002282
Iteration 109/1000 | Loss: 0.00002282
Iteration 110/1000 | Loss: 0.00002282
Iteration 111/1000 | Loss: 0.00002282
Iteration 112/1000 | Loss: 0.00002282
Iteration 113/1000 | Loss: 0.00002282
Iteration 114/1000 | Loss: 0.00002281
Iteration 115/1000 | Loss: 0.00002281
Iteration 116/1000 | Loss: 0.00002281
Iteration 117/1000 | Loss: 0.00002281
Iteration 118/1000 | Loss: 0.00002281
Iteration 119/1000 | Loss: 0.00002281
Iteration 120/1000 | Loss: 0.00002281
Iteration 121/1000 | Loss: 0.00002281
Iteration 122/1000 | Loss: 0.00002281
Iteration 123/1000 | Loss: 0.00002281
Iteration 124/1000 | Loss: 0.00002281
Iteration 125/1000 | Loss: 0.00002280
Iteration 126/1000 | Loss: 0.00002280
Iteration 127/1000 | Loss: 0.00002280
Iteration 128/1000 | Loss: 0.00002280
Iteration 129/1000 | Loss: 0.00002280
Iteration 130/1000 | Loss: 0.00002280
Iteration 131/1000 | Loss: 0.00002280
Iteration 132/1000 | Loss: 0.00002280
Iteration 133/1000 | Loss: 0.00002279
Iteration 134/1000 | Loss: 0.00002279
Iteration 135/1000 | Loss: 0.00002279
Iteration 136/1000 | Loss: 0.00002279
Iteration 137/1000 | Loss: 0.00002279
Iteration 138/1000 | Loss: 0.00002279
Iteration 139/1000 | Loss: 0.00002279
Iteration 140/1000 | Loss: 0.00002279
Iteration 141/1000 | Loss: 0.00002278
Iteration 142/1000 | Loss: 0.00002278
Iteration 143/1000 | Loss: 0.00002278
Iteration 144/1000 | Loss: 0.00002278
Iteration 145/1000 | Loss: 0.00002278
Iteration 146/1000 | Loss: 0.00002278
Iteration 147/1000 | Loss: 0.00002278
Iteration 148/1000 | Loss: 0.00002278
Iteration 149/1000 | Loss: 0.00002278
Iteration 150/1000 | Loss: 0.00002278
Iteration 151/1000 | Loss: 0.00002278
Iteration 152/1000 | Loss: 0.00002278
Iteration 153/1000 | Loss: 0.00002278
Iteration 154/1000 | Loss: 0.00002278
Iteration 155/1000 | Loss: 0.00002278
Iteration 156/1000 | Loss: 0.00002278
Iteration 157/1000 | Loss: 0.00002278
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [2.2780290237278678e-05, 2.2780290237278678e-05, 2.2780290237278678e-05, 2.2780290237278678e-05, 2.2780290237278678e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2780290237278678e-05

Optimization complete. Final v2v error: 4.1116790771484375 mm

Highest mean error: 4.369020938873291 mm for frame 42

Lowest mean error: 3.973796844482422 mm for frame 119

Saving results

Total time: 40.09368634223938
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00971544
Iteration 2/25 | Loss: 0.00218803
Iteration 3/25 | Loss: 0.00177482
Iteration 4/25 | Loss: 0.00158421
Iteration 5/25 | Loss: 0.00159567
Iteration 6/25 | Loss: 0.00157839
Iteration 7/25 | Loss: 0.00141294
Iteration 8/25 | Loss: 0.00149679
Iteration 9/25 | Loss: 0.00135140
Iteration 10/25 | Loss: 0.00133555
Iteration 11/25 | Loss: 0.00133163
Iteration 12/25 | Loss: 0.00132950
Iteration 13/25 | Loss: 0.00133217
Iteration 14/25 | Loss: 0.00133042
Iteration 15/25 | Loss: 0.00132807
Iteration 16/25 | Loss: 0.00132710
Iteration 17/25 | Loss: 0.00132647
Iteration 18/25 | Loss: 0.00132480
Iteration 19/25 | Loss: 0.00132690
Iteration 20/25 | Loss: 0.00132464
Iteration 21/25 | Loss: 0.00132234
Iteration 22/25 | Loss: 0.00132170
Iteration 23/25 | Loss: 0.00132152
Iteration 24/25 | Loss: 0.00132151
Iteration 25/25 | Loss: 0.00132151

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.87164044
Iteration 2/25 | Loss: 0.00148831
Iteration 3/25 | Loss: 0.00148830
Iteration 4/25 | Loss: 0.00148830
Iteration 5/25 | Loss: 0.00148830
Iteration 6/25 | Loss: 0.00148830
Iteration 7/25 | Loss: 0.00148830
Iteration 8/25 | Loss: 0.00148830
Iteration 9/25 | Loss: 0.00148830
Iteration 10/25 | Loss: 0.00148830
Iteration 11/25 | Loss: 0.00148830
Iteration 12/25 | Loss: 0.00148830
Iteration 13/25 | Loss: 0.00148830
Iteration 14/25 | Loss: 0.00148830
Iteration 15/25 | Loss: 0.00148830
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001488301670178771, 0.001488301670178771, 0.001488301670178771, 0.001488301670178771, 0.001488301670178771]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001488301670178771

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00148830
Iteration 2/1000 | Loss: 0.00014764
Iteration 3/1000 | Loss: 0.00010135
Iteration 4/1000 | Loss: 0.00007299
Iteration 5/1000 | Loss: 0.00006150
Iteration 6/1000 | Loss: 0.00005445
Iteration 7/1000 | Loss: 0.00005194
Iteration 8/1000 | Loss: 0.00029117
Iteration 9/1000 | Loss: 0.00013646
Iteration 10/1000 | Loss: 0.00033147
Iteration 11/1000 | Loss: 0.00008268
Iteration 12/1000 | Loss: 0.00012344
Iteration 13/1000 | Loss: 0.00010012
Iteration 14/1000 | Loss: 0.00004367
Iteration 15/1000 | Loss: 0.00071850
Iteration 16/1000 | Loss: 0.00019203
Iteration 17/1000 | Loss: 0.00006270
Iteration 18/1000 | Loss: 0.00003967
Iteration 19/1000 | Loss: 0.00003365
Iteration 20/1000 | Loss: 0.00003055
Iteration 21/1000 | Loss: 0.00002787
Iteration 22/1000 | Loss: 0.00004920
Iteration 23/1000 | Loss: 0.00002488
Iteration 24/1000 | Loss: 0.00002284
Iteration 25/1000 | Loss: 0.00002163
Iteration 26/1000 | Loss: 0.00002065
Iteration 27/1000 | Loss: 0.00001975
Iteration 28/1000 | Loss: 0.00001918
Iteration 29/1000 | Loss: 0.00024780
Iteration 30/1000 | Loss: 0.00015282
Iteration 31/1000 | Loss: 0.00042402
Iteration 32/1000 | Loss: 0.00017228
Iteration 33/1000 | Loss: 0.00019179
Iteration 34/1000 | Loss: 0.00020134
Iteration 35/1000 | Loss: 0.00002623
Iteration 36/1000 | Loss: 0.00002330
Iteration 37/1000 | Loss: 0.00002187
Iteration 38/1000 | Loss: 0.00031684
Iteration 39/1000 | Loss: 0.00010416
Iteration 40/1000 | Loss: 0.00030196
Iteration 41/1000 | Loss: 0.00005430
Iteration 42/1000 | Loss: 0.00003063
Iteration 43/1000 | Loss: 0.00002207
Iteration 44/1000 | Loss: 0.00001941
Iteration 45/1000 | Loss: 0.00001835
Iteration 46/1000 | Loss: 0.00001788
Iteration 47/1000 | Loss: 0.00001774
Iteration 48/1000 | Loss: 0.00001751
Iteration 49/1000 | Loss: 0.00001731
Iteration 50/1000 | Loss: 0.00001730
Iteration 51/1000 | Loss: 0.00001723
Iteration 52/1000 | Loss: 0.00001722
Iteration 53/1000 | Loss: 0.00001722
Iteration 54/1000 | Loss: 0.00001718
Iteration 55/1000 | Loss: 0.00001718
Iteration 56/1000 | Loss: 0.00001716
Iteration 57/1000 | Loss: 0.00001713
Iteration 58/1000 | Loss: 0.00001713
Iteration 59/1000 | Loss: 0.00001713
Iteration 60/1000 | Loss: 0.00001712
Iteration 61/1000 | Loss: 0.00001711
Iteration 62/1000 | Loss: 0.00001711
Iteration 63/1000 | Loss: 0.00001711
Iteration 64/1000 | Loss: 0.00001710
Iteration 65/1000 | Loss: 0.00001710
Iteration 66/1000 | Loss: 0.00001709
Iteration 67/1000 | Loss: 0.00001709
Iteration 68/1000 | Loss: 0.00001709
Iteration 69/1000 | Loss: 0.00001709
Iteration 70/1000 | Loss: 0.00001708
Iteration 71/1000 | Loss: 0.00001708
Iteration 72/1000 | Loss: 0.00001708
Iteration 73/1000 | Loss: 0.00001708
Iteration 74/1000 | Loss: 0.00001708
Iteration 75/1000 | Loss: 0.00001707
Iteration 76/1000 | Loss: 0.00001707
Iteration 77/1000 | Loss: 0.00001707
Iteration 78/1000 | Loss: 0.00001707
Iteration 79/1000 | Loss: 0.00001706
Iteration 80/1000 | Loss: 0.00001706
Iteration 81/1000 | Loss: 0.00001705
Iteration 82/1000 | Loss: 0.00001705
Iteration 83/1000 | Loss: 0.00001705
Iteration 84/1000 | Loss: 0.00001705
Iteration 85/1000 | Loss: 0.00001704
Iteration 86/1000 | Loss: 0.00001704
Iteration 87/1000 | Loss: 0.00001703
Iteration 88/1000 | Loss: 0.00001703
Iteration 89/1000 | Loss: 0.00001703
Iteration 90/1000 | Loss: 0.00001702
Iteration 91/1000 | Loss: 0.00001702
Iteration 92/1000 | Loss: 0.00001702
Iteration 93/1000 | Loss: 0.00001702
Iteration 94/1000 | Loss: 0.00001702
Iteration 95/1000 | Loss: 0.00001702
Iteration 96/1000 | Loss: 0.00001701
Iteration 97/1000 | Loss: 0.00001701
Iteration 98/1000 | Loss: 0.00001701
Iteration 99/1000 | Loss: 0.00001701
Iteration 100/1000 | Loss: 0.00001701
Iteration 101/1000 | Loss: 0.00001701
Iteration 102/1000 | Loss: 0.00001701
Iteration 103/1000 | Loss: 0.00001701
Iteration 104/1000 | Loss: 0.00001701
Iteration 105/1000 | Loss: 0.00001701
Iteration 106/1000 | Loss: 0.00001701
Iteration 107/1000 | Loss: 0.00001700
Iteration 108/1000 | Loss: 0.00001700
Iteration 109/1000 | Loss: 0.00001700
Iteration 110/1000 | Loss: 0.00001700
Iteration 111/1000 | Loss: 0.00001700
Iteration 112/1000 | Loss: 0.00001700
Iteration 113/1000 | Loss: 0.00001700
Iteration 114/1000 | Loss: 0.00001699
Iteration 115/1000 | Loss: 0.00001699
Iteration 116/1000 | Loss: 0.00001699
Iteration 117/1000 | Loss: 0.00001699
Iteration 118/1000 | Loss: 0.00001699
Iteration 119/1000 | Loss: 0.00001699
Iteration 120/1000 | Loss: 0.00001699
Iteration 121/1000 | Loss: 0.00001699
Iteration 122/1000 | Loss: 0.00001699
Iteration 123/1000 | Loss: 0.00001698
Iteration 124/1000 | Loss: 0.00001698
Iteration 125/1000 | Loss: 0.00001698
Iteration 126/1000 | Loss: 0.00001697
Iteration 127/1000 | Loss: 0.00001697
Iteration 128/1000 | Loss: 0.00001697
Iteration 129/1000 | Loss: 0.00001696
Iteration 130/1000 | Loss: 0.00001696
Iteration 131/1000 | Loss: 0.00001696
Iteration 132/1000 | Loss: 0.00001696
Iteration 133/1000 | Loss: 0.00001696
Iteration 134/1000 | Loss: 0.00001696
Iteration 135/1000 | Loss: 0.00001696
Iteration 136/1000 | Loss: 0.00001696
Iteration 137/1000 | Loss: 0.00001696
Iteration 138/1000 | Loss: 0.00001696
Iteration 139/1000 | Loss: 0.00001696
Iteration 140/1000 | Loss: 0.00001696
Iteration 141/1000 | Loss: 0.00001696
Iteration 142/1000 | Loss: 0.00001696
Iteration 143/1000 | Loss: 0.00001696
Iteration 144/1000 | Loss: 0.00001695
Iteration 145/1000 | Loss: 0.00001695
Iteration 146/1000 | Loss: 0.00001695
Iteration 147/1000 | Loss: 0.00001695
Iteration 148/1000 | Loss: 0.00001695
Iteration 149/1000 | Loss: 0.00001695
Iteration 150/1000 | Loss: 0.00001695
Iteration 151/1000 | Loss: 0.00001695
Iteration 152/1000 | Loss: 0.00001695
Iteration 153/1000 | Loss: 0.00001695
Iteration 154/1000 | Loss: 0.00001695
Iteration 155/1000 | Loss: 0.00001695
Iteration 156/1000 | Loss: 0.00001695
Iteration 157/1000 | Loss: 0.00001695
Iteration 158/1000 | Loss: 0.00001695
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [1.6954158127191477e-05, 1.6954158127191477e-05, 1.6954158127191477e-05, 1.6954158127191477e-05, 1.6954158127191477e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6954158127191477e-05

Optimization complete. Final v2v error: 3.4451539516448975 mm

Highest mean error: 4.517367362976074 mm for frame 65

Lowest mean error: 3.0367002487182617 mm for frame 107

Saving results

Total time: 114.05112195014954
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00864948
Iteration 2/25 | Loss: 0.00256068
Iteration 3/25 | Loss: 0.00202608
Iteration 4/25 | Loss: 0.00201154
Iteration 5/25 | Loss: 0.00176605
Iteration 6/25 | Loss: 0.00167559
Iteration 7/25 | Loss: 0.00166320
Iteration 8/25 | Loss: 0.00165779
Iteration 9/25 | Loss: 0.00165280
Iteration 10/25 | Loss: 0.00165064
Iteration 11/25 | Loss: 0.00164780
Iteration 12/25 | Loss: 0.00164559
Iteration 13/25 | Loss: 0.00164524
Iteration 14/25 | Loss: 0.00164511
Iteration 15/25 | Loss: 0.00166364
Iteration 16/25 | Loss: 0.00168483
Iteration 17/25 | Loss: 0.00169586
Iteration 18/25 | Loss: 0.00162123
Iteration 19/25 | Loss: 0.00161555
Iteration 20/25 | Loss: 0.00160498
Iteration 21/25 | Loss: 0.00160392
Iteration 22/25 | Loss: 0.00160355
Iteration 23/25 | Loss: 0.00160353
Iteration 24/25 | Loss: 0.00160353
Iteration 25/25 | Loss: 0.00160353

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59319150
Iteration 2/25 | Loss: 0.00220063
Iteration 3/25 | Loss: 0.00220019
Iteration 4/25 | Loss: 0.00220019
Iteration 5/25 | Loss: 0.00220018
Iteration 6/25 | Loss: 0.00220018
Iteration 7/25 | Loss: 0.00220018
Iteration 8/25 | Loss: 0.00220018
Iteration 9/25 | Loss: 0.00220018
Iteration 10/25 | Loss: 0.00220018
Iteration 11/25 | Loss: 0.00220018
Iteration 12/25 | Loss: 0.00220018
Iteration 13/25 | Loss: 0.00220018
Iteration 14/25 | Loss: 0.00220018
Iteration 15/25 | Loss: 0.00220018
Iteration 16/25 | Loss: 0.00220018
Iteration 17/25 | Loss: 0.00220018
Iteration 18/25 | Loss: 0.00220018
Iteration 19/25 | Loss: 0.00220018
Iteration 20/25 | Loss: 0.00220018
Iteration 21/25 | Loss: 0.00220018
Iteration 22/25 | Loss: 0.00220018
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0022001820616424084, 0.0022001820616424084, 0.0022001820616424084, 0.0022001820616424084, 0.0022001820616424084]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022001820616424084

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00220018
Iteration 2/1000 | Loss: 0.00368010
Iteration 3/1000 | Loss: 0.00020129
Iteration 4/1000 | Loss: 0.00011657
Iteration 5/1000 | Loss: 0.00009056
Iteration 6/1000 | Loss: 0.00007765
Iteration 7/1000 | Loss: 0.00007198
Iteration 8/1000 | Loss: 0.00006766
Iteration 9/1000 | Loss: 0.00006498
Iteration 10/1000 | Loss: 0.00006335
Iteration 11/1000 | Loss: 0.00006180
Iteration 12/1000 | Loss: 0.00006084
Iteration 13/1000 | Loss: 0.00005994
Iteration 14/1000 | Loss: 0.00005920
Iteration 15/1000 | Loss: 0.00012149
Iteration 16/1000 | Loss: 0.00007199
Iteration 17/1000 | Loss: 0.00007008
Iteration 18/1000 | Loss: 0.00006304
Iteration 19/1000 | Loss: 0.00006599
Iteration 20/1000 | Loss: 0.00006126
Iteration 21/1000 | Loss: 0.00005828
Iteration 22/1000 | Loss: 0.00005776
Iteration 23/1000 | Loss: 0.00005747
Iteration 24/1000 | Loss: 0.00005711
Iteration 25/1000 | Loss: 0.00006324
Iteration 26/1000 | Loss: 0.00005845
Iteration 27/1000 | Loss: 0.00005873
Iteration 28/1000 | Loss: 0.00005692
Iteration 29/1000 | Loss: 0.00005616
Iteration 30/1000 | Loss: 0.00005599
Iteration 31/1000 | Loss: 0.00007038
Iteration 32/1000 | Loss: 0.00007036
Iteration 33/1000 | Loss: 0.00006946
Iteration 34/1000 | Loss: 0.00005939
Iteration 35/1000 | Loss: 0.00005722
Iteration 36/1000 | Loss: 0.00005607
Iteration 37/1000 | Loss: 0.00005557
Iteration 38/1000 | Loss: 0.00005492
Iteration 39/1000 | Loss: 0.00005467
Iteration 40/1000 | Loss: 0.00005463
Iteration 41/1000 | Loss: 0.00005450
Iteration 42/1000 | Loss: 0.00005449
Iteration 43/1000 | Loss: 0.00005449
Iteration 44/1000 | Loss: 0.00005449
Iteration 45/1000 | Loss: 0.00005449
Iteration 46/1000 | Loss: 0.00005447
Iteration 47/1000 | Loss: 0.00005446
Iteration 48/1000 | Loss: 0.00005433
Iteration 49/1000 | Loss: 0.00005428
Iteration 50/1000 | Loss: 0.00005427
Iteration 51/1000 | Loss: 0.00005426
Iteration 52/1000 | Loss: 0.00005425
Iteration 53/1000 | Loss: 0.00005425
Iteration 54/1000 | Loss: 0.00005425
Iteration 55/1000 | Loss: 0.00005425
Iteration 56/1000 | Loss: 0.00005425
Iteration 57/1000 | Loss: 0.00005425
Iteration 58/1000 | Loss: 0.00005425
Iteration 59/1000 | Loss: 0.00005425
Iteration 60/1000 | Loss: 0.00005424
Iteration 61/1000 | Loss: 0.00005424
Iteration 62/1000 | Loss: 0.00005424
Iteration 63/1000 | Loss: 0.00005424
Iteration 64/1000 | Loss: 0.00005424
Iteration 65/1000 | Loss: 0.00005423
Iteration 66/1000 | Loss: 0.00005423
Iteration 67/1000 | Loss: 0.00005423
Iteration 68/1000 | Loss: 0.00005422
Iteration 69/1000 | Loss: 0.00005422
Iteration 70/1000 | Loss: 0.00005422
Iteration 71/1000 | Loss: 0.00005422
Iteration 72/1000 | Loss: 0.00005421
Iteration 73/1000 | Loss: 0.00005421
Iteration 74/1000 | Loss: 0.00005421
Iteration 75/1000 | Loss: 0.00005421
Iteration 76/1000 | Loss: 0.00005420
Iteration 77/1000 | Loss: 0.00005420
Iteration 78/1000 | Loss: 0.00005420
Iteration 79/1000 | Loss: 0.00005419
Iteration 80/1000 | Loss: 0.00005419
Iteration 81/1000 | Loss: 0.00005419
Iteration 82/1000 | Loss: 0.00005419
Iteration 83/1000 | Loss: 0.00005419
Iteration 84/1000 | Loss: 0.00005418
Iteration 85/1000 | Loss: 0.00005418
Iteration 86/1000 | Loss: 0.00005418
Iteration 87/1000 | Loss: 0.00005416
Iteration 88/1000 | Loss: 0.00005416
Iteration 89/1000 | Loss: 0.00005415
Iteration 90/1000 | Loss: 0.00005415
Iteration 91/1000 | Loss: 0.00005415
Iteration 92/1000 | Loss: 0.00005414
Iteration 93/1000 | Loss: 0.00005414
Iteration 94/1000 | Loss: 0.00005414
Iteration 95/1000 | Loss: 0.00005414
Iteration 96/1000 | Loss: 0.00005414
Iteration 97/1000 | Loss: 0.00005414
Iteration 98/1000 | Loss: 0.00005414
Iteration 99/1000 | Loss: 0.00005413
Iteration 100/1000 | Loss: 0.00005413
Iteration 101/1000 | Loss: 0.00005413
Iteration 102/1000 | Loss: 0.00005412
Iteration 103/1000 | Loss: 0.00005412
Iteration 104/1000 | Loss: 0.00005412
Iteration 105/1000 | Loss: 0.00005412
Iteration 106/1000 | Loss: 0.00005411
Iteration 107/1000 | Loss: 0.00005411
Iteration 108/1000 | Loss: 0.00005411
Iteration 109/1000 | Loss: 0.00005411
Iteration 110/1000 | Loss: 0.00005410
Iteration 111/1000 | Loss: 0.00005410
Iteration 112/1000 | Loss: 0.00005410
Iteration 113/1000 | Loss: 0.00005410
Iteration 114/1000 | Loss: 0.00005409
Iteration 115/1000 | Loss: 0.00005409
Iteration 116/1000 | Loss: 0.00005409
Iteration 117/1000 | Loss: 0.00005409
Iteration 118/1000 | Loss: 0.00005408
Iteration 119/1000 | Loss: 0.00005408
Iteration 120/1000 | Loss: 0.00005408
Iteration 121/1000 | Loss: 0.00005408
Iteration 122/1000 | Loss: 0.00005407
Iteration 123/1000 | Loss: 0.00005407
Iteration 124/1000 | Loss: 0.00005407
Iteration 125/1000 | Loss: 0.00005407
Iteration 126/1000 | Loss: 0.00005406
Iteration 127/1000 | Loss: 0.00005406
Iteration 128/1000 | Loss: 0.00005406
Iteration 129/1000 | Loss: 0.00005406
Iteration 130/1000 | Loss: 0.00005406
Iteration 131/1000 | Loss: 0.00005406
Iteration 132/1000 | Loss: 0.00005405
Iteration 133/1000 | Loss: 0.00005405
Iteration 134/1000 | Loss: 0.00005405
Iteration 135/1000 | Loss: 0.00005405
Iteration 136/1000 | Loss: 0.00005405
Iteration 137/1000 | Loss: 0.00005405
Iteration 138/1000 | Loss: 0.00005405
Iteration 139/1000 | Loss: 0.00005405
Iteration 140/1000 | Loss: 0.00005405
Iteration 141/1000 | Loss: 0.00005405
Iteration 142/1000 | Loss: 0.00005404
Iteration 143/1000 | Loss: 0.00005404
Iteration 144/1000 | Loss: 0.00005404
Iteration 145/1000 | Loss: 0.00005404
Iteration 146/1000 | Loss: 0.00005404
Iteration 147/1000 | Loss: 0.00005404
Iteration 148/1000 | Loss: 0.00005404
Iteration 149/1000 | Loss: 0.00005404
Iteration 150/1000 | Loss: 0.00005404
Iteration 151/1000 | Loss: 0.00005404
Iteration 152/1000 | Loss: 0.00005403
Iteration 153/1000 | Loss: 0.00005403
Iteration 154/1000 | Loss: 0.00005403
Iteration 155/1000 | Loss: 0.00005403
Iteration 156/1000 | Loss: 0.00005403
Iteration 157/1000 | Loss: 0.00005403
Iteration 158/1000 | Loss: 0.00005403
Iteration 159/1000 | Loss: 0.00005403
Iteration 160/1000 | Loss: 0.00005403
Iteration 161/1000 | Loss: 0.00005403
Iteration 162/1000 | Loss: 0.00005403
Iteration 163/1000 | Loss: 0.00005403
Iteration 164/1000 | Loss: 0.00005403
Iteration 165/1000 | Loss: 0.00005403
Iteration 166/1000 | Loss: 0.00005403
Iteration 167/1000 | Loss: 0.00005403
Iteration 168/1000 | Loss: 0.00005403
Iteration 169/1000 | Loss: 0.00005403
Iteration 170/1000 | Loss: 0.00005402
Iteration 171/1000 | Loss: 0.00005402
Iteration 172/1000 | Loss: 0.00005402
Iteration 173/1000 | Loss: 0.00005402
Iteration 174/1000 | Loss: 0.00005402
Iteration 175/1000 | Loss: 0.00005402
Iteration 176/1000 | Loss: 0.00005402
Iteration 177/1000 | Loss: 0.00005402
Iteration 178/1000 | Loss: 0.00005402
Iteration 179/1000 | Loss: 0.00005402
Iteration 180/1000 | Loss: 0.00005402
Iteration 181/1000 | Loss: 0.00005402
Iteration 182/1000 | Loss: 0.00005402
Iteration 183/1000 | Loss: 0.00005402
Iteration 184/1000 | Loss: 0.00005402
Iteration 185/1000 | Loss: 0.00005402
Iteration 186/1000 | Loss: 0.00005402
Iteration 187/1000 | Loss: 0.00005402
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [5.4022988479118794e-05, 5.4022988479118794e-05, 5.4022988479118794e-05, 5.4022988479118794e-05, 5.4022988479118794e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.4022988479118794e-05

Optimization complete. Final v2v error: 5.55581521987915 mm

Highest mean error: 11.434161186218262 mm for frame 42

Lowest mean error: 3.5345966815948486 mm for frame 83

Saving results

Total time: 122.65664720535278
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00839825
Iteration 2/25 | Loss: 0.00167335
Iteration 3/25 | Loss: 0.00144259
Iteration 4/25 | Loss: 0.00140396
Iteration 5/25 | Loss: 0.00138345
Iteration 6/25 | Loss: 0.00138349
Iteration 7/25 | Loss: 0.00137709
Iteration 8/25 | Loss: 0.00132455
Iteration 9/25 | Loss: 0.00132457
Iteration 10/25 | Loss: 0.00131375
Iteration 11/25 | Loss: 0.00131001
Iteration 12/25 | Loss: 0.00130793
Iteration 13/25 | Loss: 0.00130698
Iteration 14/25 | Loss: 0.00129996
Iteration 15/25 | Loss: 0.00129793
Iteration 16/25 | Loss: 0.00129868
Iteration 17/25 | Loss: 0.00129534
Iteration 18/25 | Loss: 0.00129662
Iteration 19/25 | Loss: 0.00128202
Iteration 20/25 | Loss: 0.00127877
Iteration 21/25 | Loss: 0.00128297
Iteration 22/25 | Loss: 0.00127581
Iteration 23/25 | Loss: 0.00127519
Iteration 24/25 | Loss: 0.00127517
Iteration 25/25 | Loss: 0.00127517

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.98407888
Iteration 2/25 | Loss: 0.00059659
Iteration 3/25 | Loss: 0.00059658
Iteration 4/25 | Loss: 0.00059658
Iteration 5/25 | Loss: 0.00059658
Iteration 6/25 | Loss: 0.00059658
Iteration 7/25 | Loss: 0.00059658
Iteration 8/25 | Loss: 0.00059658
Iteration 9/25 | Loss: 0.00059658
Iteration 10/25 | Loss: 0.00059658
Iteration 11/25 | Loss: 0.00059658
Iteration 12/25 | Loss: 0.00059658
Iteration 13/25 | Loss: 0.00059658
Iteration 14/25 | Loss: 0.00059658
Iteration 15/25 | Loss: 0.00059658
Iteration 16/25 | Loss: 0.00059658
Iteration 17/25 | Loss: 0.00059658
Iteration 18/25 | Loss: 0.00059658
Iteration 19/25 | Loss: 0.00059658
Iteration 20/25 | Loss: 0.00059658
Iteration 21/25 | Loss: 0.00059658
Iteration 22/25 | Loss: 0.00059658
Iteration 23/25 | Loss: 0.00059658
Iteration 24/25 | Loss: 0.00059658
Iteration 25/25 | Loss: 0.00059658

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059658
Iteration 2/1000 | Loss: 0.00004827
Iteration 3/1000 | Loss: 0.00003613
Iteration 4/1000 | Loss: 0.00003201
Iteration 5/1000 | Loss: 0.00003038
Iteration 6/1000 | Loss: 0.00002902
Iteration 7/1000 | Loss: 0.00002813
Iteration 8/1000 | Loss: 0.00002748
Iteration 9/1000 | Loss: 0.00002692
Iteration 10/1000 | Loss: 0.00002633
Iteration 11/1000 | Loss: 0.00085232
Iteration 12/1000 | Loss: 0.00003579
Iteration 13/1000 | Loss: 0.00002905
Iteration 14/1000 | Loss: 0.00002526
Iteration 15/1000 | Loss: 0.00002340
Iteration 16/1000 | Loss: 0.00002245
Iteration 17/1000 | Loss: 0.00002165
Iteration 18/1000 | Loss: 0.00002132
Iteration 19/1000 | Loss: 0.00002118
Iteration 20/1000 | Loss: 0.00002116
Iteration 21/1000 | Loss: 0.00002113
Iteration 22/1000 | Loss: 0.00002113
Iteration 23/1000 | Loss: 0.00002112
Iteration 24/1000 | Loss: 0.00002110
Iteration 25/1000 | Loss: 0.00002106
Iteration 26/1000 | Loss: 0.00002103
Iteration 27/1000 | Loss: 0.00002103
Iteration 28/1000 | Loss: 0.00002103
Iteration 29/1000 | Loss: 0.00002100
Iteration 30/1000 | Loss: 0.00002094
Iteration 31/1000 | Loss: 0.00002092
Iteration 32/1000 | Loss: 0.00002091
Iteration 33/1000 | Loss: 0.00002091
Iteration 34/1000 | Loss: 0.00002091
Iteration 35/1000 | Loss: 0.00002091
Iteration 36/1000 | Loss: 0.00002090
Iteration 37/1000 | Loss: 0.00002090
Iteration 38/1000 | Loss: 0.00002090
Iteration 39/1000 | Loss: 0.00002088
Iteration 40/1000 | Loss: 0.00002084
Iteration 41/1000 | Loss: 0.00002084
Iteration 42/1000 | Loss: 0.00002082
Iteration 43/1000 | Loss: 0.00002082
Iteration 44/1000 | Loss: 0.00002081
Iteration 45/1000 | Loss: 0.00002081
Iteration 46/1000 | Loss: 0.00002081
Iteration 47/1000 | Loss: 0.00002081
Iteration 48/1000 | Loss: 0.00002080
Iteration 49/1000 | Loss: 0.00002080
Iteration 50/1000 | Loss: 0.00002080
Iteration 51/1000 | Loss: 0.00002080
Iteration 52/1000 | Loss: 0.00002079
Iteration 53/1000 | Loss: 0.00002079
Iteration 54/1000 | Loss: 0.00002079
Iteration 55/1000 | Loss: 0.00002079
Iteration 56/1000 | Loss: 0.00002078
Iteration 57/1000 | Loss: 0.00002076
Iteration 58/1000 | Loss: 0.00002076
Iteration 59/1000 | Loss: 0.00002075
Iteration 60/1000 | Loss: 0.00002075
Iteration 61/1000 | Loss: 0.00002074
Iteration 62/1000 | Loss: 0.00002074
Iteration 63/1000 | Loss: 0.00002074
Iteration 64/1000 | Loss: 0.00002074
Iteration 65/1000 | Loss: 0.00002074
Iteration 66/1000 | Loss: 0.00002074
Iteration 67/1000 | Loss: 0.00002073
Iteration 68/1000 | Loss: 0.00002073
Iteration 69/1000 | Loss: 0.00002073
Iteration 70/1000 | Loss: 0.00002072
Iteration 71/1000 | Loss: 0.00002072
Iteration 72/1000 | Loss: 0.00002072
Iteration 73/1000 | Loss: 0.00002072
Iteration 74/1000 | Loss: 0.00002072
Iteration 75/1000 | Loss: 0.00002072
Iteration 76/1000 | Loss: 0.00002072
Iteration 77/1000 | Loss: 0.00002072
Iteration 78/1000 | Loss: 0.00002072
Iteration 79/1000 | Loss: 0.00002072
Iteration 80/1000 | Loss: 0.00002072
Iteration 81/1000 | Loss: 0.00002071
Iteration 82/1000 | Loss: 0.00002071
Iteration 83/1000 | Loss: 0.00002071
Iteration 84/1000 | Loss: 0.00002071
Iteration 85/1000 | Loss: 0.00002071
Iteration 86/1000 | Loss: 0.00002071
Iteration 87/1000 | Loss: 0.00002071
Iteration 88/1000 | Loss: 0.00002071
Iteration 89/1000 | Loss: 0.00002071
Iteration 90/1000 | Loss: 0.00002071
Iteration 91/1000 | Loss: 0.00002071
Iteration 92/1000 | Loss: 0.00002071
Iteration 93/1000 | Loss: 0.00002071
Iteration 94/1000 | Loss: 0.00002071
Iteration 95/1000 | Loss: 0.00002071
Iteration 96/1000 | Loss: 0.00002071
Iteration 97/1000 | Loss: 0.00002071
Iteration 98/1000 | Loss: 0.00002071
Iteration 99/1000 | Loss: 0.00002071
Iteration 100/1000 | Loss: 0.00002071
Iteration 101/1000 | Loss: 0.00002071
Iteration 102/1000 | Loss: 0.00002071
Iteration 103/1000 | Loss: 0.00002071
Iteration 104/1000 | Loss: 0.00002071
Iteration 105/1000 | Loss: 0.00002071
Iteration 106/1000 | Loss: 0.00002071
Iteration 107/1000 | Loss: 0.00002071
Iteration 108/1000 | Loss: 0.00002071
Iteration 109/1000 | Loss: 0.00002071
Iteration 110/1000 | Loss: 0.00002071
Iteration 111/1000 | Loss: 0.00002071
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [2.0707648218376562e-05, 2.0707648218376562e-05, 2.0707648218376562e-05, 2.0707648218376562e-05, 2.0707648218376562e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0707648218376562e-05

Optimization complete. Final v2v error: 3.8133671283721924 mm

Highest mean error: 4.075161457061768 mm for frame 129

Lowest mean error: 3.6991162300109863 mm for frame 0

Saving results

Total time: 76.47147917747498
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00805745
Iteration 2/25 | Loss: 0.00169647
Iteration 3/25 | Loss: 0.00137458
Iteration 4/25 | Loss: 0.00134533
Iteration 5/25 | Loss: 0.00134069
Iteration 6/25 | Loss: 0.00133963
Iteration 7/25 | Loss: 0.00133963
Iteration 8/25 | Loss: 0.00133963
Iteration 9/25 | Loss: 0.00133963
Iteration 10/25 | Loss: 0.00133963
Iteration 11/25 | Loss: 0.00133963
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013396264985203743, 0.0013396264985203743, 0.0013396264985203743, 0.0013396264985203743, 0.0013396264985203743]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013396264985203743

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44296408
Iteration 2/25 | Loss: 0.00049662
Iteration 3/25 | Loss: 0.00049662
Iteration 4/25 | Loss: 0.00049662
Iteration 5/25 | Loss: 0.00049662
Iteration 6/25 | Loss: 0.00049662
Iteration 7/25 | Loss: 0.00049662
Iteration 8/25 | Loss: 0.00049662
Iteration 9/25 | Loss: 0.00049662
Iteration 10/25 | Loss: 0.00049662
Iteration 11/25 | Loss: 0.00049662
Iteration 12/25 | Loss: 0.00049662
Iteration 13/25 | Loss: 0.00049662
Iteration 14/25 | Loss: 0.00049662
Iteration 15/25 | Loss: 0.00049662
Iteration 16/25 | Loss: 0.00049662
Iteration 17/25 | Loss: 0.00049662
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0004966183332726359, 0.0004966183332726359, 0.0004966183332726359, 0.0004966183332726359, 0.0004966183332726359]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004966183332726359

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049662
Iteration 2/1000 | Loss: 0.00005540
Iteration 3/1000 | Loss: 0.00004625
Iteration 4/1000 | Loss: 0.00004377
Iteration 5/1000 | Loss: 0.00004200
Iteration 6/1000 | Loss: 0.00004079
Iteration 7/1000 | Loss: 0.00003991
Iteration 8/1000 | Loss: 0.00003939
Iteration 9/1000 | Loss: 0.00003910
Iteration 10/1000 | Loss: 0.00003883
Iteration 11/1000 | Loss: 0.00003862
Iteration 12/1000 | Loss: 0.00003858
Iteration 13/1000 | Loss: 0.00003855
Iteration 14/1000 | Loss: 0.00003852
Iteration 15/1000 | Loss: 0.00003852
Iteration 16/1000 | Loss: 0.00003852
Iteration 17/1000 | Loss: 0.00003852
Iteration 18/1000 | Loss: 0.00003852
Iteration 19/1000 | Loss: 0.00003851
Iteration 20/1000 | Loss: 0.00003842
Iteration 21/1000 | Loss: 0.00003842
Iteration 22/1000 | Loss: 0.00003842
Iteration 23/1000 | Loss: 0.00003842
Iteration 24/1000 | Loss: 0.00003842
Iteration 25/1000 | Loss: 0.00003841
Iteration 26/1000 | Loss: 0.00003841
Iteration 27/1000 | Loss: 0.00003839
Iteration 28/1000 | Loss: 0.00003837
Iteration 29/1000 | Loss: 0.00003837
Iteration 30/1000 | Loss: 0.00003837
Iteration 31/1000 | Loss: 0.00003837
Iteration 32/1000 | Loss: 0.00003837
Iteration 33/1000 | Loss: 0.00003836
Iteration 34/1000 | Loss: 0.00003836
Iteration 35/1000 | Loss: 0.00003836
Iteration 36/1000 | Loss: 0.00003836
Iteration 37/1000 | Loss: 0.00003835
Iteration 38/1000 | Loss: 0.00003834
Iteration 39/1000 | Loss: 0.00003834
Iteration 40/1000 | Loss: 0.00003834
Iteration 41/1000 | Loss: 0.00003833
Iteration 42/1000 | Loss: 0.00003833
Iteration 43/1000 | Loss: 0.00003833
Iteration 44/1000 | Loss: 0.00003833
Iteration 45/1000 | Loss: 0.00003833
Iteration 46/1000 | Loss: 0.00003833
Iteration 47/1000 | Loss: 0.00003833
Iteration 48/1000 | Loss: 0.00003832
Iteration 49/1000 | Loss: 0.00003832
Iteration 50/1000 | Loss: 0.00003831
Iteration 51/1000 | Loss: 0.00003831
Iteration 52/1000 | Loss: 0.00003831
Iteration 53/1000 | Loss: 0.00003830
Iteration 54/1000 | Loss: 0.00003830
Iteration 55/1000 | Loss: 0.00003830
Iteration 56/1000 | Loss: 0.00003830
Iteration 57/1000 | Loss: 0.00003830
Iteration 58/1000 | Loss: 0.00003830
Iteration 59/1000 | Loss: 0.00003830
Iteration 60/1000 | Loss: 0.00003830
Iteration 61/1000 | Loss: 0.00003830
Iteration 62/1000 | Loss: 0.00003830
Iteration 63/1000 | Loss: 0.00003829
Iteration 64/1000 | Loss: 0.00003829
Iteration 65/1000 | Loss: 0.00003829
Iteration 66/1000 | Loss: 0.00003829
Iteration 67/1000 | Loss: 0.00003829
Iteration 68/1000 | Loss: 0.00003829
Iteration 69/1000 | Loss: 0.00003829
Iteration 70/1000 | Loss: 0.00003829
Iteration 71/1000 | Loss: 0.00003828
Iteration 72/1000 | Loss: 0.00003828
Iteration 73/1000 | Loss: 0.00003828
Iteration 74/1000 | Loss: 0.00003828
Iteration 75/1000 | Loss: 0.00003828
Iteration 76/1000 | Loss: 0.00003828
Iteration 77/1000 | Loss: 0.00003828
Iteration 78/1000 | Loss: 0.00003828
Iteration 79/1000 | Loss: 0.00003828
Iteration 80/1000 | Loss: 0.00003828
Iteration 81/1000 | Loss: 0.00003828
Iteration 82/1000 | Loss: 0.00003828
Iteration 83/1000 | Loss: 0.00003828
Iteration 84/1000 | Loss: 0.00003828
Iteration 85/1000 | Loss: 0.00003828
Iteration 86/1000 | Loss: 0.00003828
Iteration 87/1000 | Loss: 0.00003828
Iteration 88/1000 | Loss: 0.00003828
Iteration 89/1000 | Loss: 0.00003828
Iteration 90/1000 | Loss: 0.00003828
Iteration 91/1000 | Loss: 0.00003828
Iteration 92/1000 | Loss: 0.00003828
Iteration 93/1000 | Loss: 0.00003828
Iteration 94/1000 | Loss: 0.00003828
Iteration 95/1000 | Loss: 0.00003828
Iteration 96/1000 | Loss: 0.00003828
Iteration 97/1000 | Loss: 0.00003828
Iteration 98/1000 | Loss: 0.00003828
Iteration 99/1000 | Loss: 0.00003828
Iteration 100/1000 | Loss: 0.00003828
Iteration 101/1000 | Loss: 0.00003828
Iteration 102/1000 | Loss: 0.00003828
Iteration 103/1000 | Loss: 0.00003828
Iteration 104/1000 | Loss: 0.00003828
Iteration 105/1000 | Loss: 0.00003828
Iteration 106/1000 | Loss: 0.00003828
Iteration 107/1000 | Loss: 0.00003828
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [3.828258195426315e-05, 3.828258195426315e-05, 3.828258195426315e-05, 3.828258195426315e-05, 3.828258195426315e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.828258195426315e-05

Optimization complete. Final v2v error: 5.068205833435059 mm

Highest mean error: 5.405426979064941 mm for frame 42

Lowest mean error: 4.540119171142578 mm for frame 120

Saving results

Total time: 29.765478372573853
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01013210
Iteration 2/25 | Loss: 0.00214533
Iteration 3/25 | Loss: 0.00154986
Iteration 4/25 | Loss: 0.00146100
Iteration 5/25 | Loss: 0.00141892
Iteration 6/25 | Loss: 0.00149828
Iteration 7/25 | Loss: 0.00141570
Iteration 8/25 | Loss: 0.00142758
Iteration 9/25 | Loss: 0.00131787
Iteration 10/25 | Loss: 0.00129042
Iteration 11/25 | Loss: 0.00129003
Iteration 12/25 | Loss: 0.00128932
Iteration 13/25 | Loss: 0.00129014
Iteration 14/25 | Loss: 0.00128300
Iteration 15/25 | Loss: 0.00128264
Iteration 16/25 | Loss: 0.00127225
Iteration 17/25 | Loss: 0.00126762
Iteration 18/25 | Loss: 0.00126874
Iteration 19/25 | Loss: 0.00126901
Iteration 20/25 | Loss: 0.00126706
Iteration 21/25 | Loss: 0.00126357
Iteration 22/25 | Loss: 0.00126319
Iteration 23/25 | Loss: 0.00126300
Iteration 24/25 | Loss: 0.00126297
Iteration 25/25 | Loss: 0.00126297

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49205530
Iteration 2/25 | Loss: 0.00098167
Iteration 3/25 | Loss: 0.00090526
Iteration 4/25 | Loss: 0.00090526
Iteration 5/25 | Loss: 0.00090526
Iteration 6/25 | Loss: 0.00090526
Iteration 7/25 | Loss: 0.00090526
Iteration 8/25 | Loss: 0.00090526
Iteration 9/25 | Loss: 0.00090526
Iteration 10/25 | Loss: 0.00090526
Iteration 11/25 | Loss: 0.00090526
Iteration 12/25 | Loss: 0.00090526
Iteration 13/25 | Loss: 0.00090526
Iteration 14/25 | Loss: 0.00090526
Iteration 15/25 | Loss: 0.00090526
Iteration 16/25 | Loss: 0.00090526
Iteration 17/25 | Loss: 0.00090526
Iteration 18/25 | Loss: 0.00090526
Iteration 19/25 | Loss: 0.00090526
Iteration 20/25 | Loss: 0.00090526
Iteration 21/25 | Loss: 0.00090526
Iteration 22/25 | Loss: 0.00090526
Iteration 23/25 | Loss: 0.00090526
Iteration 24/25 | Loss: 0.00090526
Iteration 25/25 | Loss: 0.00090526

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090526
Iteration 2/1000 | Loss: 0.00003241
Iteration 3/1000 | Loss: 0.00002304
Iteration 4/1000 | Loss: 0.00002046
Iteration 5/1000 | Loss: 0.00001913
Iteration 6/1000 | Loss: 0.00020463
Iteration 7/1000 | Loss: 0.00011402
Iteration 8/1000 | Loss: 0.00010503
Iteration 9/1000 | Loss: 0.00001998
Iteration 10/1000 | Loss: 0.00024680
Iteration 11/1000 | Loss: 0.00002562
Iteration 12/1000 | Loss: 0.00002224
Iteration 13/1000 | Loss: 0.00002056
Iteration 14/1000 | Loss: 0.00023899
Iteration 15/1000 | Loss: 0.00399897
Iteration 16/1000 | Loss: 0.00055705
Iteration 17/1000 | Loss: 0.00088405
Iteration 18/1000 | Loss: 0.00002568
Iteration 19/1000 | Loss: 0.00002060
Iteration 20/1000 | Loss: 0.00001931
Iteration 21/1000 | Loss: 0.00001848
Iteration 22/1000 | Loss: 0.00017661
Iteration 23/1000 | Loss: 0.00002234
Iteration 24/1000 | Loss: 0.00002051
Iteration 25/1000 | Loss: 0.00001963
Iteration 26/1000 | Loss: 0.00001917
Iteration 27/1000 | Loss: 0.00015184
Iteration 28/1000 | Loss: 0.00047654
Iteration 29/1000 | Loss: 0.00112858
Iteration 30/1000 | Loss: 0.00006063
Iteration 31/1000 | Loss: 0.00005575
Iteration 32/1000 | Loss: 0.00002440
Iteration 33/1000 | Loss: 0.00052991
Iteration 34/1000 | Loss: 0.00003754
Iteration 35/1000 | Loss: 0.00002807
Iteration 36/1000 | Loss: 0.00002515
Iteration 37/1000 | Loss: 0.00012388
Iteration 38/1000 | Loss: 0.00002169
Iteration 39/1000 | Loss: 0.00005815
Iteration 40/1000 | Loss: 0.00024408
Iteration 41/1000 | Loss: 0.00013203
Iteration 42/1000 | Loss: 0.00042182
Iteration 43/1000 | Loss: 0.00020256
Iteration 44/1000 | Loss: 0.00093624
Iteration 45/1000 | Loss: 0.00011545
Iteration 46/1000 | Loss: 0.00003148
Iteration 47/1000 | Loss: 0.00013122
Iteration 48/1000 | Loss: 0.00002381
Iteration 49/1000 | Loss: 0.00026289
Iteration 50/1000 | Loss: 0.00002536
Iteration 51/1000 | Loss: 0.00002264
Iteration 52/1000 | Loss: 0.00002021
Iteration 53/1000 | Loss: 0.00001876
Iteration 54/1000 | Loss: 0.00001818
Iteration 55/1000 | Loss: 0.00034605
Iteration 56/1000 | Loss: 0.00024491
Iteration 57/1000 | Loss: 0.00011117
Iteration 58/1000 | Loss: 0.00004609
Iteration 59/1000 | Loss: 0.00027943
Iteration 60/1000 | Loss: 0.00013896
Iteration 61/1000 | Loss: 0.00002617
Iteration 62/1000 | Loss: 0.00029734
Iteration 63/1000 | Loss: 0.00009544
Iteration 64/1000 | Loss: 0.00002402
Iteration 65/1000 | Loss: 0.00001841
Iteration 66/1000 | Loss: 0.00001684
Iteration 67/1000 | Loss: 0.00001614
Iteration 68/1000 | Loss: 0.00001572
Iteration 69/1000 | Loss: 0.00001515
Iteration 70/1000 | Loss: 0.00001474
Iteration 71/1000 | Loss: 0.00001435
Iteration 72/1000 | Loss: 0.00007820
Iteration 73/1000 | Loss: 0.00001411
Iteration 74/1000 | Loss: 0.00001391
Iteration 75/1000 | Loss: 0.00001390
Iteration 76/1000 | Loss: 0.00001388
Iteration 77/1000 | Loss: 0.00001388
Iteration 78/1000 | Loss: 0.00001387
Iteration 79/1000 | Loss: 0.00001387
Iteration 80/1000 | Loss: 0.00001387
Iteration 81/1000 | Loss: 0.00001387
Iteration 82/1000 | Loss: 0.00001387
Iteration 83/1000 | Loss: 0.00001386
Iteration 84/1000 | Loss: 0.00001386
Iteration 85/1000 | Loss: 0.00001386
Iteration 86/1000 | Loss: 0.00001386
Iteration 87/1000 | Loss: 0.00001386
Iteration 88/1000 | Loss: 0.00001386
Iteration 89/1000 | Loss: 0.00001386
Iteration 90/1000 | Loss: 0.00001386
Iteration 91/1000 | Loss: 0.00001385
Iteration 92/1000 | Loss: 0.00001385
Iteration 93/1000 | Loss: 0.00001385
Iteration 94/1000 | Loss: 0.00001385
Iteration 95/1000 | Loss: 0.00001385
Iteration 96/1000 | Loss: 0.00001385
Iteration 97/1000 | Loss: 0.00001385
Iteration 98/1000 | Loss: 0.00001384
Iteration 99/1000 | Loss: 0.00001384
Iteration 100/1000 | Loss: 0.00001384
Iteration 101/1000 | Loss: 0.00001384
Iteration 102/1000 | Loss: 0.00001384
Iteration 103/1000 | Loss: 0.00001384
Iteration 104/1000 | Loss: 0.00001383
Iteration 105/1000 | Loss: 0.00001383
Iteration 106/1000 | Loss: 0.00001383
Iteration 107/1000 | Loss: 0.00001383
Iteration 108/1000 | Loss: 0.00001383
Iteration 109/1000 | Loss: 0.00001383
Iteration 110/1000 | Loss: 0.00001382
Iteration 111/1000 | Loss: 0.00001382
Iteration 112/1000 | Loss: 0.00001382
Iteration 113/1000 | Loss: 0.00001382
Iteration 114/1000 | Loss: 0.00001382
Iteration 115/1000 | Loss: 0.00001382
Iteration 116/1000 | Loss: 0.00001382
Iteration 117/1000 | Loss: 0.00001381
Iteration 118/1000 | Loss: 0.00001381
Iteration 119/1000 | Loss: 0.00001381
Iteration 120/1000 | Loss: 0.00001381
Iteration 121/1000 | Loss: 0.00001381
Iteration 122/1000 | Loss: 0.00001381
Iteration 123/1000 | Loss: 0.00001380
Iteration 124/1000 | Loss: 0.00001380
Iteration 125/1000 | Loss: 0.00001380
Iteration 126/1000 | Loss: 0.00001380
Iteration 127/1000 | Loss: 0.00001380
Iteration 128/1000 | Loss: 0.00001380
Iteration 129/1000 | Loss: 0.00001380
Iteration 130/1000 | Loss: 0.00001380
Iteration 131/1000 | Loss: 0.00001380
Iteration 132/1000 | Loss: 0.00001380
Iteration 133/1000 | Loss: 0.00001380
Iteration 134/1000 | Loss: 0.00001380
Iteration 135/1000 | Loss: 0.00001380
Iteration 136/1000 | Loss: 0.00001380
Iteration 137/1000 | Loss: 0.00001380
Iteration 138/1000 | Loss: 0.00001380
Iteration 139/1000 | Loss: 0.00001380
Iteration 140/1000 | Loss: 0.00001380
Iteration 141/1000 | Loss: 0.00001380
Iteration 142/1000 | Loss: 0.00001380
Iteration 143/1000 | Loss: 0.00001380
Iteration 144/1000 | Loss: 0.00001380
Iteration 145/1000 | Loss: 0.00001380
Iteration 146/1000 | Loss: 0.00001380
Iteration 147/1000 | Loss: 0.00001380
Iteration 148/1000 | Loss: 0.00001380
Iteration 149/1000 | Loss: 0.00001380
Iteration 150/1000 | Loss: 0.00001380
Iteration 151/1000 | Loss: 0.00001380
Iteration 152/1000 | Loss: 0.00001380
Iteration 153/1000 | Loss: 0.00001380
Iteration 154/1000 | Loss: 0.00001380
Iteration 155/1000 | Loss: 0.00001380
Iteration 156/1000 | Loss: 0.00001380
Iteration 157/1000 | Loss: 0.00001380
Iteration 158/1000 | Loss: 0.00001380
Iteration 159/1000 | Loss: 0.00001380
Iteration 160/1000 | Loss: 0.00001380
Iteration 161/1000 | Loss: 0.00001380
Iteration 162/1000 | Loss: 0.00001380
Iteration 163/1000 | Loss: 0.00001380
Iteration 164/1000 | Loss: 0.00001380
Iteration 165/1000 | Loss: 0.00001380
Iteration 166/1000 | Loss: 0.00001380
Iteration 167/1000 | Loss: 0.00001380
Iteration 168/1000 | Loss: 0.00001380
Iteration 169/1000 | Loss: 0.00001380
Iteration 170/1000 | Loss: 0.00001380
Iteration 171/1000 | Loss: 0.00001380
Iteration 172/1000 | Loss: 0.00001380
Iteration 173/1000 | Loss: 0.00001380
Iteration 174/1000 | Loss: 0.00001380
Iteration 175/1000 | Loss: 0.00001380
Iteration 176/1000 | Loss: 0.00001380
Iteration 177/1000 | Loss: 0.00001380
Iteration 178/1000 | Loss: 0.00001380
Iteration 179/1000 | Loss: 0.00001380
Iteration 180/1000 | Loss: 0.00001380
Iteration 181/1000 | Loss: 0.00001380
Iteration 182/1000 | Loss: 0.00001380
Iteration 183/1000 | Loss: 0.00001380
Iteration 184/1000 | Loss: 0.00001380
Iteration 185/1000 | Loss: 0.00001380
Iteration 186/1000 | Loss: 0.00001380
Iteration 187/1000 | Loss: 0.00001380
Iteration 188/1000 | Loss: 0.00001380
Iteration 189/1000 | Loss: 0.00001380
Iteration 190/1000 | Loss: 0.00001380
Iteration 191/1000 | Loss: 0.00001380
Iteration 192/1000 | Loss: 0.00001380
Iteration 193/1000 | Loss: 0.00001380
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [1.3798250620311592e-05, 1.3798250620311592e-05, 1.3798250620311592e-05, 1.3798250620311592e-05, 1.3798250620311592e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3798250620311592e-05

Optimization complete. Final v2v error: 3.119999408721924 mm

Highest mean error: 4.265431880950928 mm for frame 64

Lowest mean error: 2.752614974975586 mm for frame 9

Saving results

Total time: 151.4587504863739
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00795803
Iteration 2/25 | Loss: 0.00144900
Iteration 3/25 | Loss: 0.00135439
Iteration 4/25 | Loss: 0.00133074
Iteration 5/25 | Loss: 0.00132349
Iteration 6/25 | Loss: 0.00132274
Iteration 7/25 | Loss: 0.00132274
Iteration 8/25 | Loss: 0.00132274
Iteration 9/25 | Loss: 0.00132274
Iteration 10/25 | Loss: 0.00132274
Iteration 11/25 | Loss: 0.00132274
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001322741387411952, 0.001322741387411952, 0.001322741387411952, 0.001322741387411952, 0.001322741387411952]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001322741387411952

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.08442020
Iteration 2/25 | Loss: 0.00098556
Iteration 3/25 | Loss: 0.00098556
Iteration 4/25 | Loss: 0.00098556
Iteration 5/25 | Loss: 0.00098556
Iteration 6/25 | Loss: 0.00098556
Iteration 7/25 | Loss: 0.00098556
Iteration 8/25 | Loss: 0.00098556
Iteration 9/25 | Loss: 0.00098556
Iteration 10/25 | Loss: 0.00098556
Iteration 11/25 | Loss: 0.00098556
Iteration 12/25 | Loss: 0.00098556
Iteration 13/25 | Loss: 0.00098556
Iteration 14/25 | Loss: 0.00098556
Iteration 15/25 | Loss: 0.00098556
Iteration 16/25 | Loss: 0.00098556
Iteration 17/25 | Loss: 0.00098556
Iteration 18/25 | Loss: 0.00098556
Iteration 19/25 | Loss: 0.00098556
Iteration 20/25 | Loss: 0.00098556
Iteration 21/25 | Loss: 0.00098556
Iteration 22/25 | Loss: 0.00098556
Iteration 23/25 | Loss: 0.00098556
Iteration 24/25 | Loss: 0.00098556
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.000985560123808682, 0.000985560123808682, 0.000985560123808682, 0.000985560123808682, 0.000985560123808682]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000985560123808682

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098556
Iteration 2/1000 | Loss: 0.00004262
Iteration 3/1000 | Loss: 0.00003126
Iteration 4/1000 | Loss: 0.00002859
Iteration 5/1000 | Loss: 0.00002778
Iteration 6/1000 | Loss: 0.00002718
Iteration 7/1000 | Loss: 0.00002696
Iteration 8/1000 | Loss: 0.00002672
Iteration 9/1000 | Loss: 0.00002643
Iteration 10/1000 | Loss: 0.00002622
Iteration 11/1000 | Loss: 0.00002617
Iteration 12/1000 | Loss: 0.00002616
Iteration 13/1000 | Loss: 0.00002615
Iteration 14/1000 | Loss: 0.00002600
Iteration 15/1000 | Loss: 0.00002587
Iteration 16/1000 | Loss: 0.00002586
Iteration 17/1000 | Loss: 0.00002585
Iteration 18/1000 | Loss: 0.00002582
Iteration 19/1000 | Loss: 0.00002581
Iteration 20/1000 | Loss: 0.00002579
Iteration 21/1000 | Loss: 0.00002578
Iteration 22/1000 | Loss: 0.00002577
Iteration 23/1000 | Loss: 0.00002577
Iteration 24/1000 | Loss: 0.00002577
Iteration 25/1000 | Loss: 0.00002576
Iteration 26/1000 | Loss: 0.00002575
Iteration 27/1000 | Loss: 0.00002574
Iteration 28/1000 | Loss: 0.00002574
Iteration 29/1000 | Loss: 0.00002574
Iteration 30/1000 | Loss: 0.00002573
Iteration 31/1000 | Loss: 0.00002573
Iteration 32/1000 | Loss: 0.00002572
Iteration 33/1000 | Loss: 0.00002571
Iteration 34/1000 | Loss: 0.00002571
Iteration 35/1000 | Loss: 0.00002570
Iteration 36/1000 | Loss: 0.00002570
Iteration 37/1000 | Loss: 0.00002565
Iteration 38/1000 | Loss: 0.00002563
Iteration 39/1000 | Loss: 0.00002563
Iteration 40/1000 | Loss: 0.00002563
Iteration 41/1000 | Loss: 0.00002563
Iteration 42/1000 | Loss: 0.00002563
Iteration 43/1000 | Loss: 0.00002563
Iteration 44/1000 | Loss: 0.00002563
Iteration 45/1000 | Loss: 0.00002562
Iteration 46/1000 | Loss: 0.00002562
Iteration 47/1000 | Loss: 0.00002560
Iteration 48/1000 | Loss: 0.00002560
Iteration 49/1000 | Loss: 0.00002560
Iteration 50/1000 | Loss: 0.00002560
Iteration 51/1000 | Loss: 0.00002559
Iteration 52/1000 | Loss: 0.00002559
Iteration 53/1000 | Loss: 0.00002559
Iteration 54/1000 | Loss: 0.00002559
Iteration 55/1000 | Loss: 0.00002559
Iteration 56/1000 | Loss: 0.00002559
Iteration 57/1000 | Loss: 0.00002559
Iteration 58/1000 | Loss: 0.00002559
Iteration 59/1000 | Loss: 0.00002559
Iteration 60/1000 | Loss: 0.00002559
Iteration 61/1000 | Loss: 0.00002559
Iteration 62/1000 | Loss: 0.00002559
Iteration 63/1000 | Loss: 0.00002559
Iteration 64/1000 | Loss: 0.00002559
Iteration 65/1000 | Loss: 0.00002559
Iteration 66/1000 | Loss: 0.00002559
Iteration 67/1000 | Loss: 0.00002559
Iteration 68/1000 | Loss: 0.00002559
Iteration 69/1000 | Loss: 0.00002559
Iteration 70/1000 | Loss: 0.00002559
Iteration 71/1000 | Loss: 0.00002559
Iteration 72/1000 | Loss: 0.00002558
Iteration 73/1000 | Loss: 0.00002558
Iteration 74/1000 | Loss: 0.00002557
Iteration 75/1000 | Loss: 0.00002557
Iteration 76/1000 | Loss: 0.00002557
Iteration 77/1000 | Loss: 0.00002557
Iteration 78/1000 | Loss: 0.00002557
Iteration 79/1000 | Loss: 0.00002557
Iteration 80/1000 | Loss: 0.00002556
Iteration 81/1000 | Loss: 0.00002556
Iteration 82/1000 | Loss: 0.00002556
Iteration 83/1000 | Loss: 0.00002556
Iteration 84/1000 | Loss: 0.00002556
Iteration 85/1000 | Loss: 0.00002555
Iteration 86/1000 | Loss: 0.00002555
Iteration 87/1000 | Loss: 0.00002555
Iteration 88/1000 | Loss: 0.00002555
Iteration 89/1000 | Loss: 0.00002555
Iteration 90/1000 | Loss: 0.00002555
Iteration 91/1000 | Loss: 0.00002555
Iteration 92/1000 | Loss: 0.00002555
Iteration 93/1000 | Loss: 0.00002555
Iteration 94/1000 | Loss: 0.00002554
Iteration 95/1000 | Loss: 0.00002554
Iteration 96/1000 | Loss: 0.00002554
Iteration 97/1000 | Loss: 0.00002553
Iteration 98/1000 | Loss: 0.00002553
Iteration 99/1000 | Loss: 0.00002553
Iteration 100/1000 | Loss: 0.00002553
Iteration 101/1000 | Loss: 0.00002552
Iteration 102/1000 | Loss: 0.00002552
Iteration 103/1000 | Loss: 0.00002552
Iteration 104/1000 | Loss: 0.00002552
Iteration 105/1000 | Loss: 0.00002552
Iteration 106/1000 | Loss: 0.00002552
Iteration 107/1000 | Loss: 0.00002551
Iteration 108/1000 | Loss: 0.00002551
Iteration 109/1000 | Loss: 0.00002551
Iteration 110/1000 | Loss: 0.00002551
Iteration 111/1000 | Loss: 0.00002551
Iteration 112/1000 | Loss: 0.00002551
Iteration 113/1000 | Loss: 0.00002551
Iteration 114/1000 | Loss: 0.00002550
Iteration 115/1000 | Loss: 0.00002550
Iteration 116/1000 | Loss: 0.00002550
Iteration 117/1000 | Loss: 0.00002550
Iteration 118/1000 | Loss: 0.00002550
Iteration 119/1000 | Loss: 0.00002550
Iteration 120/1000 | Loss: 0.00002550
Iteration 121/1000 | Loss: 0.00002550
Iteration 122/1000 | Loss: 0.00002549
Iteration 123/1000 | Loss: 0.00002549
Iteration 124/1000 | Loss: 0.00002549
Iteration 125/1000 | Loss: 0.00002549
Iteration 126/1000 | Loss: 0.00002549
Iteration 127/1000 | Loss: 0.00002548
Iteration 128/1000 | Loss: 0.00002548
Iteration 129/1000 | Loss: 0.00002548
Iteration 130/1000 | Loss: 0.00002548
Iteration 131/1000 | Loss: 0.00002548
Iteration 132/1000 | Loss: 0.00002548
Iteration 133/1000 | Loss: 0.00002548
Iteration 134/1000 | Loss: 0.00002548
Iteration 135/1000 | Loss: 0.00002548
Iteration 136/1000 | Loss: 0.00002548
Iteration 137/1000 | Loss: 0.00002548
Iteration 138/1000 | Loss: 0.00002548
Iteration 139/1000 | Loss: 0.00002548
Iteration 140/1000 | Loss: 0.00002547
Iteration 141/1000 | Loss: 0.00002547
Iteration 142/1000 | Loss: 0.00002547
Iteration 143/1000 | Loss: 0.00002547
Iteration 144/1000 | Loss: 0.00002547
Iteration 145/1000 | Loss: 0.00002547
Iteration 146/1000 | Loss: 0.00002547
Iteration 147/1000 | Loss: 0.00002547
Iteration 148/1000 | Loss: 0.00002547
Iteration 149/1000 | Loss: 0.00002546
Iteration 150/1000 | Loss: 0.00002546
Iteration 151/1000 | Loss: 0.00002546
Iteration 152/1000 | Loss: 0.00002546
Iteration 153/1000 | Loss: 0.00002546
Iteration 154/1000 | Loss: 0.00002546
Iteration 155/1000 | Loss: 0.00002546
Iteration 156/1000 | Loss: 0.00002545
Iteration 157/1000 | Loss: 0.00002545
Iteration 158/1000 | Loss: 0.00002545
Iteration 159/1000 | Loss: 0.00002545
Iteration 160/1000 | Loss: 0.00002545
Iteration 161/1000 | Loss: 0.00002545
Iteration 162/1000 | Loss: 0.00002545
Iteration 163/1000 | Loss: 0.00002545
Iteration 164/1000 | Loss: 0.00002545
Iteration 165/1000 | Loss: 0.00002545
Iteration 166/1000 | Loss: 0.00002545
Iteration 167/1000 | Loss: 0.00002545
Iteration 168/1000 | Loss: 0.00002545
Iteration 169/1000 | Loss: 0.00002545
Iteration 170/1000 | Loss: 0.00002545
Iteration 171/1000 | Loss: 0.00002545
Iteration 172/1000 | Loss: 0.00002545
Iteration 173/1000 | Loss: 0.00002545
Iteration 174/1000 | Loss: 0.00002545
Iteration 175/1000 | Loss: 0.00002544
Iteration 176/1000 | Loss: 0.00002544
Iteration 177/1000 | Loss: 0.00002544
Iteration 178/1000 | Loss: 0.00002544
Iteration 179/1000 | Loss: 0.00002544
Iteration 180/1000 | Loss: 0.00002544
Iteration 181/1000 | Loss: 0.00002544
Iteration 182/1000 | Loss: 0.00002544
Iteration 183/1000 | Loss: 0.00002544
Iteration 184/1000 | Loss: 0.00002544
Iteration 185/1000 | Loss: 0.00002544
Iteration 186/1000 | Loss: 0.00002544
Iteration 187/1000 | Loss: 0.00002544
Iteration 188/1000 | Loss: 0.00002544
Iteration 189/1000 | Loss: 0.00002544
Iteration 190/1000 | Loss: 0.00002543
Iteration 191/1000 | Loss: 0.00002543
Iteration 192/1000 | Loss: 0.00002543
Iteration 193/1000 | Loss: 0.00002543
Iteration 194/1000 | Loss: 0.00002543
Iteration 195/1000 | Loss: 0.00002543
Iteration 196/1000 | Loss: 0.00002543
Iteration 197/1000 | Loss: 0.00002543
Iteration 198/1000 | Loss: 0.00002543
Iteration 199/1000 | Loss: 0.00002543
Iteration 200/1000 | Loss: 0.00002543
Iteration 201/1000 | Loss: 0.00002543
Iteration 202/1000 | Loss: 0.00002543
Iteration 203/1000 | Loss: 0.00002542
Iteration 204/1000 | Loss: 0.00002542
Iteration 205/1000 | Loss: 0.00002542
Iteration 206/1000 | Loss: 0.00002542
Iteration 207/1000 | Loss: 0.00002542
Iteration 208/1000 | Loss: 0.00002542
Iteration 209/1000 | Loss: 0.00002542
Iteration 210/1000 | Loss: 0.00002542
Iteration 211/1000 | Loss: 0.00002542
Iteration 212/1000 | Loss: 0.00002542
Iteration 213/1000 | Loss: 0.00002542
Iteration 214/1000 | Loss: 0.00002542
Iteration 215/1000 | Loss: 0.00002542
Iteration 216/1000 | Loss: 0.00002542
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [2.542200127209071e-05, 2.542200127209071e-05, 2.542200127209071e-05, 2.542200127209071e-05, 2.542200127209071e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.542200127209071e-05

Optimization complete. Final v2v error: 4.088555812835693 mm

Highest mean error: 4.852775573730469 mm for frame 130

Lowest mean error: 3.8734562397003174 mm for frame 121

Saving results

Total time: 46.11428236961365
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01091389
Iteration 2/25 | Loss: 0.00174648
Iteration 3/25 | Loss: 0.00142878
Iteration 4/25 | Loss: 0.00140248
Iteration 5/25 | Loss: 0.00139588
Iteration 6/25 | Loss: 0.00139513
Iteration 7/25 | Loss: 0.00139513
Iteration 8/25 | Loss: 0.00139513
Iteration 9/25 | Loss: 0.00139513
Iteration 10/25 | Loss: 0.00139513
Iteration 11/25 | Loss: 0.00139513
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013951252913102508, 0.0013951252913102508, 0.0013951252913102508, 0.0013951252913102508, 0.0013951252913102508]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013951252913102508

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17737043
Iteration 2/25 | Loss: 0.00097606
Iteration 3/25 | Loss: 0.00097604
Iteration 4/25 | Loss: 0.00097604
Iteration 5/25 | Loss: 0.00097604
Iteration 6/25 | Loss: 0.00097604
Iteration 7/25 | Loss: 0.00097604
Iteration 8/25 | Loss: 0.00097603
Iteration 9/25 | Loss: 0.00097603
Iteration 10/25 | Loss: 0.00097603
Iteration 11/25 | Loss: 0.00097603
Iteration 12/25 | Loss: 0.00097603
Iteration 13/25 | Loss: 0.00097603
Iteration 14/25 | Loss: 0.00097603
Iteration 15/25 | Loss: 0.00097603
Iteration 16/25 | Loss: 0.00097603
Iteration 17/25 | Loss: 0.00097603
Iteration 18/25 | Loss: 0.00097603
Iteration 19/25 | Loss: 0.00097603
Iteration 20/25 | Loss: 0.00097603
Iteration 21/25 | Loss: 0.00097603
Iteration 22/25 | Loss: 0.00097603
Iteration 23/25 | Loss: 0.00097603
Iteration 24/25 | Loss: 0.00097603
Iteration 25/25 | Loss: 0.00097603

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097603
Iteration 2/1000 | Loss: 0.00008345
Iteration 3/1000 | Loss: 0.00004426
Iteration 4/1000 | Loss: 0.00003585
Iteration 5/1000 | Loss: 0.00003373
Iteration 6/1000 | Loss: 0.00003212
Iteration 7/1000 | Loss: 0.00003128
Iteration 8/1000 | Loss: 0.00003059
Iteration 9/1000 | Loss: 0.00003007
Iteration 10/1000 | Loss: 0.00002965
Iteration 11/1000 | Loss: 0.00002933
Iteration 12/1000 | Loss: 0.00002911
Iteration 13/1000 | Loss: 0.00002890
Iteration 14/1000 | Loss: 0.00002888
Iteration 15/1000 | Loss: 0.00002864
Iteration 16/1000 | Loss: 0.00002852
Iteration 17/1000 | Loss: 0.00002845
Iteration 18/1000 | Loss: 0.00002831
Iteration 19/1000 | Loss: 0.00002817
Iteration 20/1000 | Loss: 0.00002815
Iteration 21/1000 | Loss: 0.00002811
Iteration 22/1000 | Loss: 0.00002810
Iteration 23/1000 | Loss: 0.00002810
Iteration 24/1000 | Loss: 0.00002810
Iteration 25/1000 | Loss: 0.00002809
Iteration 26/1000 | Loss: 0.00002809
Iteration 27/1000 | Loss: 0.00002809
Iteration 28/1000 | Loss: 0.00002809
Iteration 29/1000 | Loss: 0.00002809
Iteration 30/1000 | Loss: 0.00002806
Iteration 31/1000 | Loss: 0.00002806
Iteration 32/1000 | Loss: 0.00002804
Iteration 33/1000 | Loss: 0.00002803
Iteration 34/1000 | Loss: 0.00002800
Iteration 35/1000 | Loss: 0.00002800
Iteration 36/1000 | Loss: 0.00002797
Iteration 37/1000 | Loss: 0.00002797
Iteration 38/1000 | Loss: 0.00002795
Iteration 39/1000 | Loss: 0.00002792
Iteration 40/1000 | Loss: 0.00002788
Iteration 41/1000 | Loss: 0.00002788
Iteration 42/1000 | Loss: 0.00002786
Iteration 43/1000 | Loss: 0.00002785
Iteration 44/1000 | Loss: 0.00002785
Iteration 45/1000 | Loss: 0.00002783
Iteration 46/1000 | Loss: 0.00002782
Iteration 47/1000 | Loss: 0.00002782
Iteration 48/1000 | Loss: 0.00002780
Iteration 49/1000 | Loss: 0.00002779
Iteration 50/1000 | Loss: 0.00002779
Iteration 51/1000 | Loss: 0.00002779
Iteration 52/1000 | Loss: 0.00002778
Iteration 53/1000 | Loss: 0.00002778
Iteration 54/1000 | Loss: 0.00002778
Iteration 55/1000 | Loss: 0.00002777
Iteration 56/1000 | Loss: 0.00002777
Iteration 57/1000 | Loss: 0.00002777
Iteration 58/1000 | Loss: 0.00002777
Iteration 59/1000 | Loss: 0.00002777
Iteration 60/1000 | Loss: 0.00002777
Iteration 61/1000 | Loss: 0.00002776
Iteration 62/1000 | Loss: 0.00002776
Iteration 63/1000 | Loss: 0.00002776
Iteration 64/1000 | Loss: 0.00002776
Iteration 65/1000 | Loss: 0.00002776
Iteration 66/1000 | Loss: 0.00002776
Iteration 67/1000 | Loss: 0.00002776
Iteration 68/1000 | Loss: 0.00002774
Iteration 69/1000 | Loss: 0.00002774
Iteration 70/1000 | Loss: 0.00002773
Iteration 71/1000 | Loss: 0.00002773
Iteration 72/1000 | Loss: 0.00002773
Iteration 73/1000 | Loss: 0.00002772
Iteration 74/1000 | Loss: 0.00002772
Iteration 75/1000 | Loss: 0.00002772
Iteration 76/1000 | Loss: 0.00002771
Iteration 77/1000 | Loss: 0.00002771
Iteration 78/1000 | Loss: 0.00002771
Iteration 79/1000 | Loss: 0.00002770
Iteration 80/1000 | Loss: 0.00002770
Iteration 81/1000 | Loss: 0.00002770
Iteration 82/1000 | Loss: 0.00002770
Iteration 83/1000 | Loss: 0.00002770
Iteration 84/1000 | Loss: 0.00002770
Iteration 85/1000 | Loss: 0.00002770
Iteration 86/1000 | Loss: 0.00002770
Iteration 87/1000 | Loss: 0.00002770
Iteration 88/1000 | Loss: 0.00002769
Iteration 89/1000 | Loss: 0.00002769
Iteration 90/1000 | Loss: 0.00002769
Iteration 91/1000 | Loss: 0.00002769
Iteration 92/1000 | Loss: 0.00002769
Iteration 93/1000 | Loss: 0.00002769
Iteration 94/1000 | Loss: 0.00002768
Iteration 95/1000 | Loss: 0.00002768
Iteration 96/1000 | Loss: 0.00002768
Iteration 97/1000 | Loss: 0.00002768
Iteration 98/1000 | Loss: 0.00002767
Iteration 99/1000 | Loss: 0.00002767
Iteration 100/1000 | Loss: 0.00002767
Iteration 101/1000 | Loss: 0.00002767
Iteration 102/1000 | Loss: 0.00002767
Iteration 103/1000 | Loss: 0.00002766
Iteration 104/1000 | Loss: 0.00002766
Iteration 105/1000 | Loss: 0.00002766
Iteration 106/1000 | Loss: 0.00002766
Iteration 107/1000 | Loss: 0.00002766
Iteration 108/1000 | Loss: 0.00002766
Iteration 109/1000 | Loss: 0.00002766
Iteration 110/1000 | Loss: 0.00002766
Iteration 111/1000 | Loss: 0.00002766
Iteration 112/1000 | Loss: 0.00002766
Iteration 113/1000 | Loss: 0.00002766
Iteration 114/1000 | Loss: 0.00002766
Iteration 115/1000 | Loss: 0.00002766
Iteration 116/1000 | Loss: 0.00002766
Iteration 117/1000 | Loss: 0.00002766
Iteration 118/1000 | Loss: 0.00002766
Iteration 119/1000 | Loss: 0.00002766
Iteration 120/1000 | Loss: 0.00002766
Iteration 121/1000 | Loss: 0.00002766
Iteration 122/1000 | Loss: 0.00002766
Iteration 123/1000 | Loss: 0.00002766
Iteration 124/1000 | Loss: 0.00002766
Iteration 125/1000 | Loss: 0.00002766
Iteration 126/1000 | Loss: 0.00002766
Iteration 127/1000 | Loss: 0.00002766
Iteration 128/1000 | Loss: 0.00002766
Iteration 129/1000 | Loss: 0.00002766
Iteration 130/1000 | Loss: 0.00002766
Iteration 131/1000 | Loss: 0.00002766
Iteration 132/1000 | Loss: 0.00002766
Iteration 133/1000 | Loss: 0.00002766
Iteration 134/1000 | Loss: 0.00002766
Iteration 135/1000 | Loss: 0.00002766
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [2.7656615202431567e-05, 2.7656615202431567e-05, 2.7656615202431567e-05, 2.7656615202431567e-05, 2.7656615202431567e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7656615202431567e-05

Optimization complete. Final v2v error: 4.325986862182617 mm

Highest mean error: 5.631467342376709 mm for frame 149

Lowest mean error: 3.581904411315918 mm for frame 54

Saving results

Total time: 49.8233528137207
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00997782
Iteration 2/25 | Loss: 0.00185806
Iteration 3/25 | Loss: 0.00156949
Iteration 4/25 | Loss: 0.00144380
Iteration 5/25 | Loss: 0.00142039
Iteration 6/25 | Loss: 0.00145532
Iteration 7/25 | Loss: 0.00140673
Iteration 8/25 | Loss: 0.00137356
Iteration 9/25 | Loss: 0.00135065
Iteration 10/25 | Loss: 0.00134559
Iteration 11/25 | Loss: 0.00134367
Iteration 12/25 | Loss: 0.00134040
Iteration 13/25 | Loss: 0.00133363
Iteration 14/25 | Loss: 0.00133550
Iteration 15/25 | Loss: 0.00133615
Iteration 16/25 | Loss: 0.00133719
Iteration 17/25 | Loss: 0.00133107
Iteration 18/25 | Loss: 0.00132829
Iteration 19/25 | Loss: 0.00133028
Iteration 20/25 | Loss: 0.00132971
Iteration 21/25 | Loss: 0.00132522
Iteration 22/25 | Loss: 0.00132487
Iteration 23/25 | Loss: 0.00132478
Iteration 24/25 | Loss: 0.00132475
Iteration 25/25 | Loss: 0.00132475

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60217381
Iteration 2/25 | Loss: 0.00141129
Iteration 3/25 | Loss: 0.00141128
Iteration 4/25 | Loss: 0.00141128
Iteration 5/25 | Loss: 0.00141128
Iteration 6/25 | Loss: 0.00141128
Iteration 7/25 | Loss: 0.00141128
Iteration 8/25 | Loss: 0.00141128
Iteration 9/25 | Loss: 0.00140145
Iteration 10/25 | Loss: 0.00140145
Iteration 11/25 | Loss: 0.00140145
Iteration 12/25 | Loss: 0.00140145
Iteration 13/25 | Loss: 0.00140145
Iteration 14/25 | Loss: 0.00140145
Iteration 15/25 | Loss: 0.00140145
Iteration 16/25 | Loss: 0.00140144
Iteration 17/25 | Loss: 0.00140144
Iteration 18/25 | Loss: 0.00140144
Iteration 19/25 | Loss: 0.00140144
Iteration 20/25 | Loss: 0.00140144
Iteration 21/25 | Loss: 0.00140144
Iteration 22/25 | Loss: 0.00140144
Iteration 23/25 | Loss: 0.00140145
Iteration 24/25 | Loss: 0.00140145
Iteration 25/25 | Loss: 0.00140144

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00140144
Iteration 2/1000 | Loss: 0.00092644
Iteration 3/1000 | Loss: 0.00012072
Iteration 4/1000 | Loss: 0.00014643
Iteration 5/1000 | Loss: 0.00025222
Iteration 6/1000 | Loss: 0.00022103
Iteration 7/1000 | Loss: 0.00004379
Iteration 8/1000 | Loss: 0.00015886
Iteration 9/1000 | Loss: 0.00007300
Iteration 10/1000 | Loss: 0.00008327
Iteration 11/1000 | Loss: 0.00061165
Iteration 12/1000 | Loss: 0.00019282
Iteration 13/1000 | Loss: 0.00021603
Iteration 14/1000 | Loss: 0.00017602
Iteration 15/1000 | Loss: 0.00003721
Iteration 16/1000 | Loss: 0.00003219
Iteration 17/1000 | Loss: 0.00004149
Iteration 18/1000 | Loss: 0.00005125
Iteration 19/1000 | Loss: 0.00003804
Iteration 20/1000 | Loss: 0.00003973
Iteration 21/1000 | Loss: 0.00004018
Iteration 22/1000 | Loss: 0.00004207
Iteration 23/1000 | Loss: 0.00004461
Iteration 24/1000 | Loss: 0.00005014
Iteration 25/1000 | Loss: 0.00004510
Iteration 26/1000 | Loss: 0.00005349
Iteration 27/1000 | Loss: 0.00005273
Iteration 28/1000 | Loss: 0.00005140
Iteration 29/1000 | Loss: 0.00005328
Iteration 30/1000 | Loss: 0.00005471
Iteration 31/1000 | Loss: 0.00005715
Iteration 32/1000 | Loss: 0.00005670
Iteration 33/1000 | Loss: 0.00005290
Iteration 34/1000 | Loss: 0.00005477
Iteration 35/1000 | Loss: 0.00005174
Iteration 36/1000 | Loss: 0.00005407
Iteration 37/1000 | Loss: 0.00025703
Iteration 38/1000 | Loss: 0.00021368
Iteration 39/1000 | Loss: 0.00005199
Iteration 40/1000 | Loss: 0.00005402
Iteration 41/1000 | Loss: 0.00006120
Iteration 42/1000 | Loss: 0.00005383
Iteration 43/1000 | Loss: 0.00005774
Iteration 44/1000 | Loss: 0.00005330
Iteration 45/1000 | Loss: 0.00005418
Iteration 46/1000 | Loss: 0.00005431
Iteration 47/1000 | Loss: 0.00005339
Iteration 48/1000 | Loss: 0.00005812
Iteration 49/1000 | Loss: 0.00006048
Iteration 50/1000 | Loss: 0.00005154
Iteration 51/1000 | Loss: 0.00005236
Iteration 52/1000 | Loss: 0.00005536
Iteration 53/1000 | Loss: 0.00005251
Iteration 54/1000 | Loss: 0.00005370
Iteration 55/1000 | Loss: 0.00005203
Iteration 56/1000 | Loss: 0.00004805
Iteration 57/1000 | Loss: 0.00005431
Iteration 58/1000 | Loss: 0.00005303
Iteration 59/1000 | Loss: 0.00005014
Iteration 60/1000 | Loss: 0.00005169
Iteration 61/1000 | Loss: 0.00039012
Iteration 62/1000 | Loss: 0.00024442
Iteration 63/1000 | Loss: 0.00010212
Iteration 64/1000 | Loss: 0.00035941
Iteration 65/1000 | Loss: 0.00016106
Iteration 66/1000 | Loss: 0.00002824
Iteration 67/1000 | Loss: 0.00002482
Iteration 68/1000 | Loss: 0.00002296
Iteration 69/1000 | Loss: 0.00002209
Iteration 70/1000 | Loss: 0.00002124
Iteration 71/1000 | Loss: 0.00002072
Iteration 72/1000 | Loss: 0.00002050
Iteration 73/1000 | Loss: 0.00002029
Iteration 74/1000 | Loss: 0.00002027
Iteration 75/1000 | Loss: 0.00002018
Iteration 76/1000 | Loss: 0.00002015
Iteration 77/1000 | Loss: 0.00002015
Iteration 78/1000 | Loss: 0.00002012
Iteration 79/1000 | Loss: 0.00002012
Iteration 80/1000 | Loss: 0.00002010
Iteration 81/1000 | Loss: 0.00001999
Iteration 82/1000 | Loss: 0.00001993
Iteration 83/1000 | Loss: 0.00001992
Iteration 84/1000 | Loss: 0.00001990
Iteration 85/1000 | Loss: 0.00001990
Iteration 86/1000 | Loss: 0.00001989
Iteration 87/1000 | Loss: 0.00001988
Iteration 88/1000 | Loss: 0.00001987
Iteration 89/1000 | Loss: 0.00001986
Iteration 90/1000 | Loss: 0.00001985
Iteration 91/1000 | Loss: 0.00001985
Iteration 92/1000 | Loss: 0.00001984
Iteration 93/1000 | Loss: 0.00001984
Iteration 94/1000 | Loss: 0.00001983
Iteration 95/1000 | Loss: 0.00001983
Iteration 96/1000 | Loss: 0.00001983
Iteration 97/1000 | Loss: 0.00001982
Iteration 98/1000 | Loss: 0.00001981
Iteration 99/1000 | Loss: 0.00001981
Iteration 100/1000 | Loss: 0.00001980
Iteration 101/1000 | Loss: 0.00001980
Iteration 102/1000 | Loss: 0.00001980
Iteration 103/1000 | Loss: 0.00001979
Iteration 104/1000 | Loss: 0.00001979
Iteration 105/1000 | Loss: 0.00001979
Iteration 106/1000 | Loss: 0.00001978
Iteration 107/1000 | Loss: 0.00001978
Iteration 108/1000 | Loss: 0.00001977
Iteration 109/1000 | Loss: 0.00001977
Iteration 110/1000 | Loss: 0.00001977
Iteration 111/1000 | Loss: 0.00001977
Iteration 112/1000 | Loss: 0.00001976
Iteration 113/1000 | Loss: 0.00001976
Iteration 114/1000 | Loss: 0.00001976
Iteration 115/1000 | Loss: 0.00001976
Iteration 116/1000 | Loss: 0.00001975
Iteration 117/1000 | Loss: 0.00001975
Iteration 118/1000 | Loss: 0.00001975
Iteration 119/1000 | Loss: 0.00001975
Iteration 120/1000 | Loss: 0.00001975
Iteration 121/1000 | Loss: 0.00001975
Iteration 122/1000 | Loss: 0.00001975
Iteration 123/1000 | Loss: 0.00001975
Iteration 124/1000 | Loss: 0.00001975
Iteration 125/1000 | Loss: 0.00001974
Iteration 126/1000 | Loss: 0.00001974
Iteration 127/1000 | Loss: 0.00001974
Iteration 128/1000 | Loss: 0.00001974
Iteration 129/1000 | Loss: 0.00001974
Iteration 130/1000 | Loss: 0.00001974
Iteration 131/1000 | Loss: 0.00001974
Iteration 132/1000 | Loss: 0.00001974
Iteration 133/1000 | Loss: 0.00001974
Iteration 134/1000 | Loss: 0.00001974
Iteration 135/1000 | Loss: 0.00001974
Iteration 136/1000 | Loss: 0.00001974
Iteration 137/1000 | Loss: 0.00001974
Iteration 138/1000 | Loss: 0.00001974
Iteration 139/1000 | Loss: 0.00001974
Iteration 140/1000 | Loss: 0.00001974
Iteration 141/1000 | Loss: 0.00001974
Iteration 142/1000 | Loss: 0.00001973
Iteration 143/1000 | Loss: 0.00001973
Iteration 144/1000 | Loss: 0.00001973
Iteration 145/1000 | Loss: 0.00001973
Iteration 146/1000 | Loss: 0.00001973
Iteration 147/1000 | Loss: 0.00001973
Iteration 148/1000 | Loss: 0.00001973
Iteration 149/1000 | Loss: 0.00001973
Iteration 150/1000 | Loss: 0.00001973
Iteration 151/1000 | Loss: 0.00001972
Iteration 152/1000 | Loss: 0.00001972
Iteration 153/1000 | Loss: 0.00001972
Iteration 154/1000 | Loss: 0.00001972
Iteration 155/1000 | Loss: 0.00001972
Iteration 156/1000 | Loss: 0.00001971
Iteration 157/1000 | Loss: 0.00001971
Iteration 158/1000 | Loss: 0.00001971
Iteration 159/1000 | Loss: 0.00001971
Iteration 160/1000 | Loss: 0.00001970
Iteration 161/1000 | Loss: 0.00001970
Iteration 162/1000 | Loss: 0.00001970
Iteration 163/1000 | Loss: 0.00001970
Iteration 164/1000 | Loss: 0.00001969
Iteration 165/1000 | Loss: 0.00001969
Iteration 166/1000 | Loss: 0.00001969
Iteration 167/1000 | Loss: 0.00001969
Iteration 168/1000 | Loss: 0.00001969
Iteration 169/1000 | Loss: 0.00001969
Iteration 170/1000 | Loss: 0.00001969
Iteration 171/1000 | Loss: 0.00001968
Iteration 172/1000 | Loss: 0.00001968
Iteration 173/1000 | Loss: 0.00001968
Iteration 174/1000 | Loss: 0.00001968
Iteration 175/1000 | Loss: 0.00001968
Iteration 176/1000 | Loss: 0.00001968
Iteration 177/1000 | Loss: 0.00001967
Iteration 178/1000 | Loss: 0.00001967
Iteration 179/1000 | Loss: 0.00001967
Iteration 180/1000 | Loss: 0.00001967
Iteration 181/1000 | Loss: 0.00001967
Iteration 182/1000 | Loss: 0.00001967
Iteration 183/1000 | Loss: 0.00001967
Iteration 184/1000 | Loss: 0.00001967
Iteration 185/1000 | Loss: 0.00001967
Iteration 186/1000 | Loss: 0.00001967
Iteration 187/1000 | Loss: 0.00001967
Iteration 188/1000 | Loss: 0.00001967
Iteration 189/1000 | Loss: 0.00001967
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.9670418623718433e-05, 1.9670418623718433e-05, 1.9670418623718433e-05, 1.9670418623718433e-05, 1.9670418623718433e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9670418623718433e-05

Optimization complete. Final v2v error: 3.5049164295196533 mm

Highest mean error: 6.203792572021484 mm for frame 50

Lowest mean error: 2.8799660205841064 mm for frame 1

Saving results

Total time: 151.9731137752533
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00911017
Iteration 2/25 | Loss: 0.00305585
Iteration 3/25 | Loss: 0.00195449
Iteration 4/25 | Loss: 0.00189994
Iteration 5/25 | Loss: 0.00181762
Iteration 6/25 | Loss: 0.00184087
Iteration 7/25 | Loss: 0.00177970
Iteration 8/25 | Loss: 0.00167934
Iteration 9/25 | Loss: 0.00164789
Iteration 10/25 | Loss: 0.00161553
Iteration 11/25 | Loss: 0.00162779
Iteration 12/25 | Loss: 0.00160722
Iteration 13/25 | Loss: 0.00162371
Iteration 14/25 | Loss: 0.00160593
Iteration 15/25 | Loss: 0.00161101
Iteration 16/25 | Loss: 0.00160376
Iteration 17/25 | Loss: 0.00160525
Iteration 18/25 | Loss: 0.00159355
Iteration 19/25 | Loss: 0.00159281
Iteration 20/25 | Loss: 0.00159222
Iteration 21/25 | Loss: 0.00159190
Iteration 22/25 | Loss: 0.00159545
Iteration 23/25 | Loss: 0.00158976
Iteration 24/25 | Loss: 0.00158693
Iteration 25/25 | Loss: 0.00158605

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.23307514
Iteration 2/25 | Loss: 0.00626857
Iteration 3/25 | Loss: 0.00626857
Iteration 4/25 | Loss: 0.00626857
Iteration 5/25 | Loss: 0.00626857
Iteration 6/25 | Loss: 0.00626857
Iteration 7/25 | Loss: 0.00626857
Iteration 8/25 | Loss: 0.00626857
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 8. Stopping optimization.
Last 5 losses: [0.0062685683369636536, 0.0062685683369636536, 0.0062685683369636536, 0.0062685683369636536, 0.0062685683369636536]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0062685683369636536

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00626857
Iteration 2/1000 | Loss: 0.00846209
Iteration 3/1000 | Loss: 0.00330740
Iteration 4/1000 | Loss: 0.00061027
Iteration 5/1000 | Loss: 0.00083622
Iteration 6/1000 | Loss: 0.00064428
Iteration 7/1000 | Loss: 0.00082443
Iteration 8/1000 | Loss: 0.00083741
Iteration 9/1000 | Loss: 0.00042867
Iteration 10/1000 | Loss: 0.00071183
Iteration 11/1000 | Loss: 0.00110715
Iteration 12/1000 | Loss: 0.00063915
Iteration 13/1000 | Loss: 0.00104532
Iteration 14/1000 | Loss: 0.00097465
Iteration 15/1000 | Loss: 0.00196684
Iteration 16/1000 | Loss: 0.00255499
Iteration 17/1000 | Loss: 0.00540101
Iteration 18/1000 | Loss: 0.00248885
Iteration 19/1000 | Loss: 0.00744558
Iteration 20/1000 | Loss: 0.00945806
Iteration 21/1000 | Loss: 0.00853532
Iteration 22/1000 | Loss: 0.00565298
Iteration 23/1000 | Loss: 0.00293257
Iteration 24/1000 | Loss: 0.00235477
Iteration 25/1000 | Loss: 0.00152622
Iteration 26/1000 | Loss: 0.00215864
Iteration 27/1000 | Loss: 0.00156492
Iteration 28/1000 | Loss: 0.00177484
Iteration 29/1000 | Loss: 0.00211131
Iteration 30/1000 | Loss: 0.00133385
Iteration 31/1000 | Loss: 0.00080055
Iteration 32/1000 | Loss: 0.00094895
Iteration 33/1000 | Loss: 0.00144497
Iteration 34/1000 | Loss: 0.00135409
Iteration 35/1000 | Loss: 0.00122693
Iteration 36/1000 | Loss: 0.00126383
Iteration 37/1000 | Loss: 0.00090238
Iteration 38/1000 | Loss: 0.00053908
Iteration 39/1000 | Loss: 0.00384682
Iteration 40/1000 | Loss: 0.00181272
Iteration 41/1000 | Loss: 0.00100362
Iteration 42/1000 | Loss: 0.00098662
Iteration 43/1000 | Loss: 0.00059801
Iteration 44/1000 | Loss: 0.00162424
Iteration 45/1000 | Loss: 0.00190106
Iteration 46/1000 | Loss: 0.00136539
Iteration 47/1000 | Loss: 0.00092292
Iteration 48/1000 | Loss: 0.00131755
Iteration 49/1000 | Loss: 0.00106741
Iteration 50/1000 | Loss: 0.00119685
Iteration 51/1000 | Loss: 0.00167393
Iteration 52/1000 | Loss: 0.00091446
Iteration 53/1000 | Loss: 0.00147723
Iteration 54/1000 | Loss: 0.00181472
Iteration 55/1000 | Loss: 0.00112887
Iteration 56/1000 | Loss: 0.00040594
Iteration 57/1000 | Loss: 0.00119158
Iteration 58/1000 | Loss: 0.00071984
Iteration 59/1000 | Loss: 0.00105072
Iteration 60/1000 | Loss: 0.00099695
Iteration 61/1000 | Loss: 0.00180205
Iteration 62/1000 | Loss: 0.00150468
Iteration 63/1000 | Loss: 0.00177548
Iteration 64/1000 | Loss: 0.00228900
Iteration 65/1000 | Loss: 0.00162034
Iteration 66/1000 | Loss: 0.00162136
Iteration 67/1000 | Loss: 0.00111976
Iteration 68/1000 | Loss: 0.00066999
Iteration 69/1000 | Loss: 0.00088450
Iteration 70/1000 | Loss: 0.00063255
Iteration 71/1000 | Loss: 0.00112304
Iteration 72/1000 | Loss: 0.00233031
Iteration 73/1000 | Loss: 0.00150236
Iteration 74/1000 | Loss: 0.00143943
Iteration 75/1000 | Loss: 0.00167190
Iteration 76/1000 | Loss: 0.00331387
Iteration 77/1000 | Loss: 0.00059492
Iteration 78/1000 | Loss: 0.00061246
Iteration 79/1000 | Loss: 0.00185198
Iteration 80/1000 | Loss: 0.00130167
Iteration 81/1000 | Loss: 0.00123660
Iteration 82/1000 | Loss: 0.00346787
Iteration 83/1000 | Loss: 0.00183211
Iteration 84/1000 | Loss: 0.00069624
Iteration 85/1000 | Loss: 0.00059577
Iteration 86/1000 | Loss: 0.00046910
Iteration 87/1000 | Loss: 0.00017912
Iteration 88/1000 | Loss: 0.00085059
Iteration 89/1000 | Loss: 0.00098754
Iteration 90/1000 | Loss: 0.00023385
Iteration 91/1000 | Loss: 0.00018418
Iteration 92/1000 | Loss: 0.00042277
Iteration 93/1000 | Loss: 0.00046704
Iteration 94/1000 | Loss: 0.00024553
Iteration 95/1000 | Loss: 0.00107696
Iteration 96/1000 | Loss: 0.00060524
Iteration 97/1000 | Loss: 0.00067283
Iteration 98/1000 | Loss: 0.00043895
Iteration 99/1000 | Loss: 0.00045982
Iteration 100/1000 | Loss: 0.00041230
Iteration 101/1000 | Loss: 0.00055425
Iteration 102/1000 | Loss: 0.00052040
Iteration 103/1000 | Loss: 0.00027932
Iteration 104/1000 | Loss: 0.00078505
Iteration 105/1000 | Loss: 0.00069581
Iteration 106/1000 | Loss: 0.00072023
Iteration 107/1000 | Loss: 0.00122773
Iteration 108/1000 | Loss: 0.00196023
Iteration 109/1000 | Loss: 0.00052307
Iteration 110/1000 | Loss: 0.00078561
Iteration 111/1000 | Loss: 0.00063112
Iteration 112/1000 | Loss: 0.00065726
Iteration 113/1000 | Loss: 0.00055959
Iteration 114/1000 | Loss: 0.00086635
Iteration 115/1000 | Loss: 0.00101377
Iteration 116/1000 | Loss: 0.00070120
Iteration 117/1000 | Loss: 0.00052910
Iteration 118/1000 | Loss: 0.00055343
Iteration 119/1000 | Loss: 0.00054262
Iteration 120/1000 | Loss: 0.00070618
Iteration 121/1000 | Loss: 0.00179407
Iteration 122/1000 | Loss: 0.00067373
Iteration 123/1000 | Loss: 0.00136250
Iteration 124/1000 | Loss: 0.00092814
Iteration 125/1000 | Loss: 0.00109478
Iteration 126/1000 | Loss: 0.00084367
Iteration 127/1000 | Loss: 0.00103223
Iteration 128/1000 | Loss: 0.00156421
Iteration 129/1000 | Loss: 0.00036328
Iteration 130/1000 | Loss: 0.00044625
Iteration 131/1000 | Loss: 0.00040080
Iteration 132/1000 | Loss: 0.00059165
Iteration 133/1000 | Loss: 0.00121225
Iteration 134/1000 | Loss: 0.00115930
Iteration 135/1000 | Loss: 0.00073273
Iteration 136/1000 | Loss: 0.00086285
Iteration 137/1000 | Loss: 0.00046360
Iteration 138/1000 | Loss: 0.00041481
Iteration 139/1000 | Loss: 0.00081065
Iteration 140/1000 | Loss: 0.00033850
Iteration 141/1000 | Loss: 0.00058587
Iteration 142/1000 | Loss: 0.00051536
Iteration 143/1000 | Loss: 0.00064480
Iteration 144/1000 | Loss: 0.00095701
Iteration 145/1000 | Loss: 0.00086504
Iteration 146/1000 | Loss: 0.00076241
Iteration 147/1000 | Loss: 0.00050460
Iteration 148/1000 | Loss: 0.00115952
Iteration 149/1000 | Loss: 0.00146098
Iteration 150/1000 | Loss: 0.00153142
Iteration 151/1000 | Loss: 0.00093574
Iteration 152/1000 | Loss: 0.00033404
Iteration 153/1000 | Loss: 0.00071494
Iteration 154/1000 | Loss: 0.00046642
Iteration 155/1000 | Loss: 0.00013364
Iteration 156/1000 | Loss: 0.00018241
Iteration 157/1000 | Loss: 0.00017938
Iteration 158/1000 | Loss: 0.00017204
Iteration 159/1000 | Loss: 0.00060480
Iteration 160/1000 | Loss: 0.00051717
Iteration 161/1000 | Loss: 0.00044043
Iteration 162/1000 | Loss: 0.00069046
Iteration 163/1000 | Loss: 0.00095460
Iteration 164/1000 | Loss: 0.00028066
Iteration 165/1000 | Loss: 0.00049701
Iteration 166/1000 | Loss: 0.00013616
Iteration 167/1000 | Loss: 0.00053877
Iteration 168/1000 | Loss: 0.00052248
Iteration 169/1000 | Loss: 0.00115371
Iteration 170/1000 | Loss: 0.00020780
Iteration 171/1000 | Loss: 0.00052516
Iteration 172/1000 | Loss: 0.00048185
Iteration 173/1000 | Loss: 0.00089512
Iteration 174/1000 | Loss: 0.00052150
Iteration 175/1000 | Loss: 0.00060420
Iteration 176/1000 | Loss: 0.00068359
Iteration 177/1000 | Loss: 0.00018732
Iteration 178/1000 | Loss: 0.00027857
Iteration 179/1000 | Loss: 0.00074528
Iteration 180/1000 | Loss: 0.00077916
Iteration 181/1000 | Loss: 0.00036077
Iteration 182/1000 | Loss: 0.00011993
Iteration 183/1000 | Loss: 0.00041221
Iteration 184/1000 | Loss: 0.00020157
Iteration 185/1000 | Loss: 0.00012298
Iteration 186/1000 | Loss: 0.00011322
Iteration 187/1000 | Loss: 0.00012822
Iteration 188/1000 | Loss: 0.00066544
Iteration 189/1000 | Loss: 0.00034074
Iteration 190/1000 | Loss: 0.00068795
Iteration 191/1000 | Loss: 0.00045333
Iteration 192/1000 | Loss: 0.00032509
Iteration 193/1000 | Loss: 0.00090565
Iteration 194/1000 | Loss: 0.00048223
Iteration 195/1000 | Loss: 0.00046711
Iteration 196/1000 | Loss: 0.00024331
Iteration 197/1000 | Loss: 0.00004897
Iteration 198/1000 | Loss: 0.00011429
Iteration 199/1000 | Loss: 0.00106380
Iteration 200/1000 | Loss: 0.00044600
Iteration 201/1000 | Loss: 0.00047475
Iteration 202/1000 | Loss: 0.00114333
Iteration 203/1000 | Loss: 0.00028378
Iteration 204/1000 | Loss: 0.00029281
Iteration 205/1000 | Loss: 0.00007484
Iteration 206/1000 | Loss: 0.00041785
Iteration 207/1000 | Loss: 0.00021277
Iteration 208/1000 | Loss: 0.00191019
Iteration 209/1000 | Loss: 0.00039920
Iteration 210/1000 | Loss: 0.00042037
Iteration 211/1000 | Loss: 0.00025798
Iteration 212/1000 | Loss: 0.00045168
Iteration 213/1000 | Loss: 0.00075872
Iteration 214/1000 | Loss: 0.00115040
Iteration 215/1000 | Loss: 0.00033194
Iteration 216/1000 | Loss: 0.00116232
Iteration 217/1000 | Loss: 0.00088386
Iteration 218/1000 | Loss: 0.00159995
Iteration 219/1000 | Loss: 0.00121742
Iteration 220/1000 | Loss: 0.00076337
Iteration 221/1000 | Loss: 0.00006436
Iteration 222/1000 | Loss: 0.00004556
Iteration 223/1000 | Loss: 0.00033717
Iteration 224/1000 | Loss: 0.00003598
Iteration 225/1000 | Loss: 0.00003311
Iteration 226/1000 | Loss: 0.00003124
Iteration 227/1000 | Loss: 0.00030403
Iteration 228/1000 | Loss: 0.00002936
Iteration 229/1000 | Loss: 0.00002753
Iteration 230/1000 | Loss: 0.00044445
Iteration 231/1000 | Loss: 0.00006823
Iteration 232/1000 | Loss: 0.00002592
Iteration 233/1000 | Loss: 0.00082762
Iteration 234/1000 | Loss: 0.00035705
Iteration 235/1000 | Loss: 0.00029567
Iteration 236/1000 | Loss: 0.00052830
Iteration 237/1000 | Loss: 0.00019987
Iteration 238/1000 | Loss: 0.00004536
Iteration 239/1000 | Loss: 0.00002515
Iteration 240/1000 | Loss: 0.00002359
Iteration 241/1000 | Loss: 0.00002283
Iteration 242/1000 | Loss: 0.00002191
Iteration 243/1000 | Loss: 0.00002143
Iteration 244/1000 | Loss: 0.00002114
Iteration 245/1000 | Loss: 0.00002096
Iteration 246/1000 | Loss: 0.00002093
Iteration 247/1000 | Loss: 0.00002080
Iteration 248/1000 | Loss: 0.00002071
Iteration 249/1000 | Loss: 0.00002071
Iteration 250/1000 | Loss: 0.00002068
Iteration 251/1000 | Loss: 0.00002052
Iteration 252/1000 | Loss: 0.00002051
Iteration 253/1000 | Loss: 0.00002051
Iteration 254/1000 | Loss: 0.00002051
Iteration 255/1000 | Loss: 0.00002047
Iteration 256/1000 | Loss: 0.00002047
Iteration 257/1000 | Loss: 0.00002047
Iteration 258/1000 | Loss: 0.00002046
Iteration 259/1000 | Loss: 0.00002046
Iteration 260/1000 | Loss: 0.00002046
Iteration 261/1000 | Loss: 0.00002046
Iteration 262/1000 | Loss: 0.00002041
Iteration 263/1000 | Loss: 0.00002039
Iteration 264/1000 | Loss: 0.00002036
Iteration 265/1000 | Loss: 0.00002033
Iteration 266/1000 | Loss: 0.00002032
Iteration 267/1000 | Loss: 0.00002026
Iteration 268/1000 | Loss: 0.00002025
Iteration 269/1000 | Loss: 0.00002025
Iteration 270/1000 | Loss: 0.00002024
Iteration 271/1000 | Loss: 0.00002024
Iteration 272/1000 | Loss: 0.00002024
Iteration 273/1000 | Loss: 0.00002024
Iteration 274/1000 | Loss: 0.00002023
Iteration 275/1000 | Loss: 0.00002023
Iteration 276/1000 | Loss: 0.00002023
Iteration 277/1000 | Loss: 0.00002023
Iteration 278/1000 | Loss: 0.00002023
Iteration 279/1000 | Loss: 0.00002022
Iteration 280/1000 | Loss: 0.00002022
Iteration 281/1000 | Loss: 0.00002022
Iteration 282/1000 | Loss: 0.00002022
Iteration 283/1000 | Loss: 0.00002021
Iteration 284/1000 | Loss: 0.00002021
Iteration 285/1000 | Loss: 0.00002021
Iteration 286/1000 | Loss: 0.00002021
Iteration 287/1000 | Loss: 0.00002021
Iteration 288/1000 | Loss: 0.00002021
Iteration 289/1000 | Loss: 0.00002020
Iteration 290/1000 | Loss: 0.00002020
Iteration 291/1000 | Loss: 0.00002020
Iteration 292/1000 | Loss: 0.00002020
Iteration 293/1000 | Loss: 0.00002020
Iteration 294/1000 | Loss: 0.00002020
Iteration 295/1000 | Loss: 0.00002020
Iteration 296/1000 | Loss: 0.00002020
Iteration 297/1000 | Loss: 0.00002020
Iteration 298/1000 | Loss: 0.00002019
Iteration 299/1000 | Loss: 0.00002019
Iteration 300/1000 | Loss: 0.00002019
Iteration 301/1000 | Loss: 0.00002019
Iteration 302/1000 | Loss: 0.00002019
Iteration 303/1000 | Loss: 0.00002019
Iteration 304/1000 | Loss: 0.00002019
Iteration 305/1000 | Loss: 0.00002019
Iteration 306/1000 | Loss: 0.00002019
Iteration 307/1000 | Loss: 0.00002019
Iteration 308/1000 | Loss: 0.00002019
Iteration 309/1000 | Loss: 0.00002019
Iteration 310/1000 | Loss: 0.00002019
Iteration 311/1000 | Loss: 0.00002018
Iteration 312/1000 | Loss: 0.00002018
Iteration 313/1000 | Loss: 0.00002018
Iteration 314/1000 | Loss: 0.00002018
Iteration 315/1000 | Loss: 0.00002018
Iteration 316/1000 | Loss: 0.00002018
Iteration 317/1000 | Loss: 0.00002018
Iteration 318/1000 | Loss: 0.00002018
Iteration 319/1000 | Loss: 0.00002018
Iteration 320/1000 | Loss: 0.00002018
Iteration 321/1000 | Loss: 0.00002018
Iteration 322/1000 | Loss: 0.00002018
Iteration 323/1000 | Loss: 0.00002018
Iteration 324/1000 | Loss: 0.00002018
Iteration 325/1000 | Loss: 0.00002018
Iteration 326/1000 | Loss: 0.00002018
Iteration 327/1000 | Loss: 0.00002018
Iteration 328/1000 | Loss: 0.00002018
Iteration 329/1000 | Loss: 0.00002018
Iteration 330/1000 | Loss: 0.00002018
Iteration 331/1000 | Loss: 0.00002018
Iteration 332/1000 | Loss: 0.00002018
Iteration 333/1000 | Loss: 0.00002018
Iteration 334/1000 | Loss: 0.00002018
Iteration 335/1000 | Loss: 0.00002018
Iteration 336/1000 | Loss: 0.00002018
Iteration 337/1000 | Loss: 0.00002018
Iteration 338/1000 | Loss: 0.00002018
Iteration 339/1000 | Loss: 0.00002018
Iteration 340/1000 | Loss: 0.00002018
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 340. Stopping optimization.
Last 5 losses: [2.01750299311243e-05, 2.01750299311243e-05, 2.01750299311243e-05, 2.01750299311243e-05, 2.01750299311243e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.01750299311243e-05

Optimization complete. Final v2v error: 3.6071834564208984 mm

Highest mean error: 5.315823078155518 mm for frame 69

Lowest mean error: 2.935025691986084 mm for frame 163

Saving results

Total time: 419.0872166156769
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00490580
Iteration 2/25 | Loss: 0.00137117
Iteration 3/25 | Loss: 0.00128046
Iteration 4/25 | Loss: 0.00126518
Iteration 5/25 | Loss: 0.00125904
Iteration 6/25 | Loss: 0.00125863
Iteration 7/25 | Loss: 0.00125863
Iteration 8/25 | Loss: 0.00125863
Iteration 9/25 | Loss: 0.00125863
Iteration 10/25 | Loss: 0.00125863
Iteration 11/25 | Loss: 0.00125863
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012586285592988133, 0.0012586285592988133, 0.0012586285592988133, 0.0012586285592988133, 0.0012586285592988133]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012586285592988133

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.83925748
Iteration 2/25 | Loss: 0.00076322
Iteration 3/25 | Loss: 0.00076322
Iteration 4/25 | Loss: 0.00076322
Iteration 5/25 | Loss: 0.00076322
Iteration 6/25 | Loss: 0.00076321
Iteration 7/25 | Loss: 0.00076321
Iteration 8/25 | Loss: 0.00076321
Iteration 9/25 | Loss: 0.00076321
Iteration 10/25 | Loss: 0.00076321
Iteration 11/25 | Loss: 0.00076321
Iteration 12/25 | Loss: 0.00076321
Iteration 13/25 | Loss: 0.00076321
Iteration 14/25 | Loss: 0.00076321
Iteration 15/25 | Loss: 0.00076321
Iteration 16/25 | Loss: 0.00076321
Iteration 17/25 | Loss: 0.00076321
Iteration 18/25 | Loss: 0.00076321
Iteration 19/25 | Loss: 0.00076321
Iteration 20/25 | Loss: 0.00076321
Iteration 21/25 | Loss: 0.00076321
Iteration 22/25 | Loss: 0.00076321
Iteration 23/25 | Loss: 0.00076321
Iteration 24/25 | Loss: 0.00076321
Iteration 25/25 | Loss: 0.00076321

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076321
Iteration 2/1000 | Loss: 0.00004584
Iteration 3/1000 | Loss: 0.00002999
Iteration 4/1000 | Loss: 0.00002757
Iteration 5/1000 | Loss: 0.00002596
Iteration 6/1000 | Loss: 0.00002462
Iteration 7/1000 | Loss: 0.00002364
Iteration 8/1000 | Loss: 0.00002316
Iteration 9/1000 | Loss: 0.00002263
Iteration 10/1000 | Loss: 0.00002220
Iteration 11/1000 | Loss: 0.00002185
Iteration 12/1000 | Loss: 0.00002156
Iteration 13/1000 | Loss: 0.00002129
Iteration 14/1000 | Loss: 0.00002122
Iteration 15/1000 | Loss: 0.00002121
Iteration 16/1000 | Loss: 0.00002119
Iteration 17/1000 | Loss: 0.00002101
Iteration 18/1000 | Loss: 0.00002084
Iteration 19/1000 | Loss: 0.00002081
Iteration 20/1000 | Loss: 0.00002078
Iteration 21/1000 | Loss: 0.00002070
Iteration 22/1000 | Loss: 0.00002070
Iteration 23/1000 | Loss: 0.00002064
Iteration 24/1000 | Loss: 0.00002057
Iteration 25/1000 | Loss: 0.00002056
Iteration 26/1000 | Loss: 0.00002055
Iteration 27/1000 | Loss: 0.00002051
Iteration 28/1000 | Loss: 0.00002044
Iteration 29/1000 | Loss: 0.00002041
Iteration 30/1000 | Loss: 0.00002038
Iteration 31/1000 | Loss: 0.00002037
Iteration 32/1000 | Loss: 0.00002037
Iteration 33/1000 | Loss: 0.00002036
Iteration 34/1000 | Loss: 0.00002036
Iteration 35/1000 | Loss: 0.00002036
Iteration 36/1000 | Loss: 0.00002036
Iteration 37/1000 | Loss: 0.00002036
Iteration 38/1000 | Loss: 0.00002036
Iteration 39/1000 | Loss: 0.00002036
Iteration 40/1000 | Loss: 0.00002036
Iteration 41/1000 | Loss: 0.00002036
Iteration 42/1000 | Loss: 0.00002036
Iteration 43/1000 | Loss: 0.00002036
Iteration 44/1000 | Loss: 0.00002035
Iteration 45/1000 | Loss: 0.00002035
Iteration 46/1000 | Loss: 0.00002035
Iteration 47/1000 | Loss: 0.00002035
Iteration 48/1000 | Loss: 0.00002032
Iteration 49/1000 | Loss: 0.00002032
Iteration 50/1000 | Loss: 0.00002032
Iteration 51/1000 | Loss: 0.00002032
Iteration 52/1000 | Loss: 0.00002031
Iteration 53/1000 | Loss: 0.00002031
Iteration 54/1000 | Loss: 0.00002031
Iteration 55/1000 | Loss: 0.00002031
Iteration 56/1000 | Loss: 0.00002031
Iteration 57/1000 | Loss: 0.00002031
Iteration 58/1000 | Loss: 0.00002031
Iteration 59/1000 | Loss: 0.00002031
Iteration 60/1000 | Loss: 0.00002031
Iteration 61/1000 | Loss: 0.00002031
Iteration 62/1000 | Loss: 0.00002031
Iteration 63/1000 | Loss: 0.00002030
Iteration 64/1000 | Loss: 0.00002030
Iteration 65/1000 | Loss: 0.00002030
Iteration 66/1000 | Loss: 0.00002030
Iteration 67/1000 | Loss: 0.00002029
Iteration 68/1000 | Loss: 0.00002029
Iteration 69/1000 | Loss: 0.00002029
Iteration 70/1000 | Loss: 0.00002029
Iteration 71/1000 | Loss: 0.00002029
Iteration 72/1000 | Loss: 0.00002028
Iteration 73/1000 | Loss: 0.00002028
Iteration 74/1000 | Loss: 0.00002028
Iteration 75/1000 | Loss: 0.00002028
Iteration 76/1000 | Loss: 0.00002027
Iteration 77/1000 | Loss: 0.00002027
Iteration 78/1000 | Loss: 0.00002027
Iteration 79/1000 | Loss: 0.00002026
Iteration 80/1000 | Loss: 0.00002026
Iteration 81/1000 | Loss: 0.00002026
Iteration 82/1000 | Loss: 0.00002026
Iteration 83/1000 | Loss: 0.00002025
Iteration 84/1000 | Loss: 0.00002025
Iteration 85/1000 | Loss: 0.00002025
Iteration 86/1000 | Loss: 0.00002025
Iteration 87/1000 | Loss: 0.00002024
Iteration 88/1000 | Loss: 0.00002024
Iteration 89/1000 | Loss: 0.00002024
Iteration 90/1000 | Loss: 0.00002024
Iteration 91/1000 | Loss: 0.00002023
Iteration 92/1000 | Loss: 0.00002023
Iteration 93/1000 | Loss: 0.00002023
Iteration 94/1000 | Loss: 0.00002022
Iteration 95/1000 | Loss: 0.00002021
Iteration 96/1000 | Loss: 0.00002020
Iteration 97/1000 | Loss: 0.00002020
Iteration 98/1000 | Loss: 0.00002020
Iteration 99/1000 | Loss: 0.00002019
Iteration 100/1000 | Loss: 0.00002019
Iteration 101/1000 | Loss: 0.00002019
Iteration 102/1000 | Loss: 0.00002018
Iteration 103/1000 | Loss: 0.00002018
Iteration 104/1000 | Loss: 0.00002018
Iteration 105/1000 | Loss: 0.00002018
Iteration 106/1000 | Loss: 0.00002017
Iteration 107/1000 | Loss: 0.00002017
Iteration 108/1000 | Loss: 0.00002017
Iteration 109/1000 | Loss: 0.00002017
Iteration 110/1000 | Loss: 0.00002016
Iteration 111/1000 | Loss: 0.00002016
Iteration 112/1000 | Loss: 0.00002016
Iteration 113/1000 | Loss: 0.00002016
Iteration 114/1000 | Loss: 0.00002016
Iteration 115/1000 | Loss: 0.00002016
Iteration 116/1000 | Loss: 0.00002016
Iteration 117/1000 | Loss: 0.00002016
Iteration 118/1000 | Loss: 0.00002016
Iteration 119/1000 | Loss: 0.00002016
Iteration 120/1000 | Loss: 0.00002015
Iteration 121/1000 | Loss: 0.00002015
Iteration 122/1000 | Loss: 0.00002014
Iteration 123/1000 | Loss: 0.00002014
Iteration 124/1000 | Loss: 0.00002014
Iteration 125/1000 | Loss: 0.00002014
Iteration 126/1000 | Loss: 0.00002014
Iteration 127/1000 | Loss: 0.00002014
Iteration 128/1000 | Loss: 0.00002014
Iteration 129/1000 | Loss: 0.00002014
Iteration 130/1000 | Loss: 0.00002013
Iteration 131/1000 | Loss: 0.00002013
Iteration 132/1000 | Loss: 0.00002013
Iteration 133/1000 | Loss: 0.00002013
Iteration 134/1000 | Loss: 0.00002013
Iteration 135/1000 | Loss: 0.00002013
Iteration 136/1000 | Loss: 0.00002013
Iteration 137/1000 | Loss: 0.00002012
Iteration 138/1000 | Loss: 0.00002011
Iteration 139/1000 | Loss: 0.00002011
Iteration 140/1000 | Loss: 0.00002011
Iteration 141/1000 | Loss: 0.00002011
Iteration 142/1000 | Loss: 0.00002011
Iteration 143/1000 | Loss: 0.00002011
Iteration 144/1000 | Loss: 0.00002011
Iteration 145/1000 | Loss: 0.00002011
Iteration 146/1000 | Loss: 0.00002011
Iteration 147/1000 | Loss: 0.00002011
Iteration 148/1000 | Loss: 0.00002011
Iteration 149/1000 | Loss: 0.00002011
Iteration 150/1000 | Loss: 0.00002011
Iteration 151/1000 | Loss: 0.00002010
Iteration 152/1000 | Loss: 0.00002010
Iteration 153/1000 | Loss: 0.00002010
Iteration 154/1000 | Loss: 0.00002010
Iteration 155/1000 | Loss: 0.00002010
Iteration 156/1000 | Loss: 0.00002009
Iteration 157/1000 | Loss: 0.00002009
Iteration 158/1000 | Loss: 0.00002008
Iteration 159/1000 | Loss: 0.00002008
Iteration 160/1000 | Loss: 0.00002008
Iteration 161/1000 | Loss: 0.00002008
Iteration 162/1000 | Loss: 0.00002007
Iteration 163/1000 | Loss: 0.00002007
Iteration 164/1000 | Loss: 0.00002007
Iteration 165/1000 | Loss: 0.00002007
Iteration 166/1000 | Loss: 0.00002006
Iteration 167/1000 | Loss: 0.00002006
Iteration 168/1000 | Loss: 0.00002006
Iteration 169/1000 | Loss: 0.00002005
Iteration 170/1000 | Loss: 0.00002005
Iteration 171/1000 | Loss: 0.00002005
Iteration 172/1000 | Loss: 0.00002005
Iteration 173/1000 | Loss: 0.00002005
Iteration 174/1000 | Loss: 0.00002005
Iteration 175/1000 | Loss: 0.00002005
Iteration 176/1000 | Loss: 0.00002004
Iteration 177/1000 | Loss: 0.00002004
Iteration 178/1000 | Loss: 0.00002004
Iteration 179/1000 | Loss: 0.00002004
Iteration 180/1000 | Loss: 0.00002004
Iteration 181/1000 | Loss: 0.00002003
Iteration 182/1000 | Loss: 0.00002003
Iteration 183/1000 | Loss: 0.00002003
Iteration 184/1000 | Loss: 0.00002003
Iteration 185/1000 | Loss: 0.00002003
Iteration 186/1000 | Loss: 0.00002003
Iteration 187/1000 | Loss: 0.00002003
Iteration 188/1000 | Loss: 0.00002003
Iteration 189/1000 | Loss: 0.00002003
Iteration 190/1000 | Loss: 0.00002003
Iteration 191/1000 | Loss: 0.00002003
Iteration 192/1000 | Loss: 0.00002002
Iteration 193/1000 | Loss: 0.00002002
Iteration 194/1000 | Loss: 0.00002002
Iteration 195/1000 | Loss: 0.00002002
Iteration 196/1000 | Loss: 0.00002002
Iteration 197/1000 | Loss: 0.00002002
Iteration 198/1000 | Loss: 0.00002002
Iteration 199/1000 | Loss: 0.00002002
Iteration 200/1000 | Loss: 0.00002001
Iteration 201/1000 | Loss: 0.00002001
Iteration 202/1000 | Loss: 0.00002001
Iteration 203/1000 | Loss: 0.00002001
Iteration 204/1000 | Loss: 0.00002001
Iteration 205/1000 | Loss: 0.00002000
Iteration 206/1000 | Loss: 0.00002000
Iteration 207/1000 | Loss: 0.00002000
Iteration 208/1000 | Loss: 0.00002000
Iteration 209/1000 | Loss: 0.00002000
Iteration 210/1000 | Loss: 0.00002000
Iteration 211/1000 | Loss: 0.00002000
Iteration 212/1000 | Loss: 0.00002000
Iteration 213/1000 | Loss: 0.00001999
Iteration 214/1000 | Loss: 0.00001999
Iteration 215/1000 | Loss: 0.00001999
Iteration 216/1000 | Loss: 0.00001999
Iteration 217/1000 | Loss: 0.00001999
Iteration 218/1000 | Loss: 0.00001999
Iteration 219/1000 | Loss: 0.00001999
Iteration 220/1000 | Loss: 0.00001999
Iteration 221/1000 | Loss: 0.00001999
Iteration 222/1000 | Loss: 0.00001999
Iteration 223/1000 | Loss: 0.00001999
Iteration 224/1000 | Loss: 0.00001999
Iteration 225/1000 | Loss: 0.00001999
Iteration 226/1000 | Loss: 0.00001999
Iteration 227/1000 | Loss: 0.00001999
Iteration 228/1000 | Loss: 0.00001998
Iteration 229/1000 | Loss: 0.00001998
Iteration 230/1000 | Loss: 0.00001998
Iteration 231/1000 | Loss: 0.00001998
Iteration 232/1000 | Loss: 0.00001998
Iteration 233/1000 | Loss: 0.00001998
Iteration 234/1000 | Loss: 0.00001998
Iteration 235/1000 | Loss: 0.00001998
Iteration 236/1000 | Loss: 0.00001998
Iteration 237/1000 | Loss: 0.00001998
Iteration 238/1000 | Loss: 0.00001998
Iteration 239/1000 | Loss: 0.00001998
Iteration 240/1000 | Loss: 0.00001998
Iteration 241/1000 | Loss: 0.00001998
Iteration 242/1000 | Loss: 0.00001998
Iteration 243/1000 | Loss: 0.00001998
Iteration 244/1000 | Loss: 0.00001998
Iteration 245/1000 | Loss: 0.00001998
Iteration 246/1000 | Loss: 0.00001998
Iteration 247/1000 | Loss: 0.00001998
Iteration 248/1000 | Loss: 0.00001998
Iteration 249/1000 | Loss: 0.00001998
Iteration 250/1000 | Loss: 0.00001998
Iteration 251/1000 | Loss: 0.00001998
Iteration 252/1000 | Loss: 0.00001998
Iteration 253/1000 | Loss: 0.00001998
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 253. Stopping optimization.
Last 5 losses: [1.9978459022240713e-05, 1.9978459022240713e-05, 1.9978459022240713e-05, 1.9978459022240713e-05, 1.9978459022240713e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9978459022240713e-05

Optimization complete. Final v2v error: 3.7728328704833984 mm

Highest mean error: 4.4700236320495605 mm for frame 6

Lowest mean error: 3.588636875152588 mm for frame 60

Saving results

Total time: 58.077083587646484
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00437768
Iteration 2/25 | Loss: 0.00134537
Iteration 3/25 | Loss: 0.00127209
Iteration 4/25 | Loss: 0.00126025
Iteration 5/25 | Loss: 0.00125628
Iteration 6/25 | Loss: 0.00125598
Iteration 7/25 | Loss: 0.00125598
Iteration 8/25 | Loss: 0.00125598
Iteration 9/25 | Loss: 0.00125598
Iteration 10/25 | Loss: 0.00125598
Iteration 11/25 | Loss: 0.00125598
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012559762690216303, 0.0012559762690216303, 0.0012559762690216303, 0.0012559762690216303, 0.0012559762690216303]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012559762690216303

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.01514745
Iteration 2/25 | Loss: 0.00075352
Iteration 3/25 | Loss: 0.00075352
Iteration 4/25 | Loss: 0.00075352
Iteration 5/25 | Loss: 0.00075352
Iteration 6/25 | Loss: 0.00075352
Iteration 7/25 | Loss: 0.00075352
Iteration 8/25 | Loss: 0.00075352
Iteration 9/25 | Loss: 0.00075352
Iteration 10/25 | Loss: 0.00075352
Iteration 11/25 | Loss: 0.00075352
Iteration 12/25 | Loss: 0.00075352
Iteration 13/25 | Loss: 0.00075352
Iteration 14/25 | Loss: 0.00075352
Iteration 15/25 | Loss: 0.00075352
Iteration 16/25 | Loss: 0.00075352
Iteration 17/25 | Loss: 0.00075352
Iteration 18/25 | Loss: 0.00075352
Iteration 19/25 | Loss: 0.00075352
Iteration 20/25 | Loss: 0.00075352
Iteration 21/25 | Loss: 0.00075352
Iteration 22/25 | Loss: 0.00075352
Iteration 23/25 | Loss: 0.00075352
Iteration 24/25 | Loss: 0.00075352
Iteration 25/25 | Loss: 0.00075352

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075352
Iteration 2/1000 | Loss: 0.00003314
Iteration 3/1000 | Loss: 0.00002671
Iteration 4/1000 | Loss: 0.00002407
Iteration 5/1000 | Loss: 0.00002299
Iteration 6/1000 | Loss: 0.00002232
Iteration 7/1000 | Loss: 0.00002167
Iteration 8/1000 | Loss: 0.00002117
Iteration 9/1000 | Loss: 0.00002087
Iteration 10/1000 | Loss: 0.00002053
Iteration 11/1000 | Loss: 0.00002028
Iteration 12/1000 | Loss: 0.00002024
Iteration 13/1000 | Loss: 0.00002017
Iteration 14/1000 | Loss: 0.00002003
Iteration 15/1000 | Loss: 0.00002002
Iteration 16/1000 | Loss: 0.00002002
Iteration 17/1000 | Loss: 0.00001995
Iteration 18/1000 | Loss: 0.00001994
Iteration 19/1000 | Loss: 0.00001994
Iteration 20/1000 | Loss: 0.00001988
Iteration 21/1000 | Loss: 0.00001984
Iteration 22/1000 | Loss: 0.00001983
Iteration 23/1000 | Loss: 0.00001980
Iteration 24/1000 | Loss: 0.00001977
Iteration 25/1000 | Loss: 0.00001977
Iteration 26/1000 | Loss: 0.00001975
Iteration 27/1000 | Loss: 0.00001974
Iteration 28/1000 | Loss: 0.00001973
Iteration 29/1000 | Loss: 0.00001972
Iteration 30/1000 | Loss: 0.00001972
Iteration 31/1000 | Loss: 0.00001971
Iteration 32/1000 | Loss: 0.00001970
Iteration 33/1000 | Loss: 0.00001969
Iteration 34/1000 | Loss: 0.00001969
Iteration 35/1000 | Loss: 0.00001968
Iteration 36/1000 | Loss: 0.00001966
Iteration 37/1000 | Loss: 0.00001966
Iteration 38/1000 | Loss: 0.00001966
Iteration 39/1000 | Loss: 0.00001966
Iteration 40/1000 | Loss: 0.00001966
Iteration 41/1000 | Loss: 0.00001966
Iteration 42/1000 | Loss: 0.00001966
Iteration 43/1000 | Loss: 0.00001966
Iteration 44/1000 | Loss: 0.00001965
Iteration 45/1000 | Loss: 0.00001965
Iteration 46/1000 | Loss: 0.00001965
Iteration 47/1000 | Loss: 0.00001965
Iteration 48/1000 | Loss: 0.00001965
Iteration 49/1000 | Loss: 0.00001965
Iteration 50/1000 | Loss: 0.00001965
Iteration 51/1000 | Loss: 0.00001965
Iteration 52/1000 | Loss: 0.00001964
Iteration 53/1000 | Loss: 0.00001964
Iteration 54/1000 | Loss: 0.00001964
Iteration 55/1000 | Loss: 0.00001964
Iteration 56/1000 | Loss: 0.00001964
Iteration 57/1000 | Loss: 0.00001962
Iteration 58/1000 | Loss: 0.00001962
Iteration 59/1000 | Loss: 0.00001962
Iteration 60/1000 | Loss: 0.00001961
Iteration 61/1000 | Loss: 0.00001961
Iteration 62/1000 | Loss: 0.00001960
Iteration 63/1000 | Loss: 0.00001960
Iteration 64/1000 | Loss: 0.00001960
Iteration 65/1000 | Loss: 0.00001959
Iteration 66/1000 | Loss: 0.00001959
Iteration 67/1000 | Loss: 0.00001958
Iteration 68/1000 | Loss: 0.00001957
Iteration 69/1000 | Loss: 0.00001957
Iteration 70/1000 | Loss: 0.00001957
Iteration 71/1000 | Loss: 0.00001957
Iteration 72/1000 | Loss: 0.00001957
Iteration 73/1000 | Loss: 0.00001956
Iteration 74/1000 | Loss: 0.00001956
Iteration 75/1000 | Loss: 0.00001956
Iteration 76/1000 | Loss: 0.00001956
Iteration 77/1000 | Loss: 0.00001955
Iteration 78/1000 | Loss: 0.00001955
Iteration 79/1000 | Loss: 0.00001954
Iteration 80/1000 | Loss: 0.00001954
Iteration 81/1000 | Loss: 0.00001954
Iteration 82/1000 | Loss: 0.00001953
Iteration 83/1000 | Loss: 0.00001953
Iteration 84/1000 | Loss: 0.00001953
Iteration 85/1000 | Loss: 0.00001952
Iteration 86/1000 | Loss: 0.00001952
Iteration 87/1000 | Loss: 0.00001952
Iteration 88/1000 | Loss: 0.00001952
Iteration 89/1000 | Loss: 0.00001952
Iteration 90/1000 | Loss: 0.00001951
Iteration 91/1000 | Loss: 0.00001951
Iteration 92/1000 | Loss: 0.00001951
Iteration 93/1000 | Loss: 0.00001951
Iteration 94/1000 | Loss: 0.00001951
Iteration 95/1000 | Loss: 0.00001950
Iteration 96/1000 | Loss: 0.00001950
Iteration 97/1000 | Loss: 0.00001950
Iteration 98/1000 | Loss: 0.00001950
Iteration 99/1000 | Loss: 0.00001950
Iteration 100/1000 | Loss: 0.00001949
Iteration 101/1000 | Loss: 0.00001949
Iteration 102/1000 | Loss: 0.00001949
Iteration 103/1000 | Loss: 0.00001949
Iteration 104/1000 | Loss: 0.00001949
Iteration 105/1000 | Loss: 0.00001949
Iteration 106/1000 | Loss: 0.00001949
Iteration 107/1000 | Loss: 0.00001949
Iteration 108/1000 | Loss: 0.00001948
Iteration 109/1000 | Loss: 0.00001948
Iteration 110/1000 | Loss: 0.00001948
Iteration 111/1000 | Loss: 0.00001948
Iteration 112/1000 | Loss: 0.00001948
Iteration 113/1000 | Loss: 0.00001948
Iteration 114/1000 | Loss: 0.00001948
Iteration 115/1000 | Loss: 0.00001948
Iteration 116/1000 | Loss: 0.00001947
Iteration 117/1000 | Loss: 0.00001947
Iteration 118/1000 | Loss: 0.00001947
Iteration 119/1000 | Loss: 0.00001947
Iteration 120/1000 | Loss: 0.00001947
Iteration 121/1000 | Loss: 0.00001947
Iteration 122/1000 | Loss: 0.00001947
Iteration 123/1000 | Loss: 0.00001947
Iteration 124/1000 | Loss: 0.00001947
Iteration 125/1000 | Loss: 0.00001947
Iteration 126/1000 | Loss: 0.00001947
Iteration 127/1000 | Loss: 0.00001947
Iteration 128/1000 | Loss: 0.00001946
Iteration 129/1000 | Loss: 0.00001946
Iteration 130/1000 | Loss: 0.00001946
Iteration 131/1000 | Loss: 0.00001946
Iteration 132/1000 | Loss: 0.00001946
Iteration 133/1000 | Loss: 0.00001946
Iteration 134/1000 | Loss: 0.00001946
Iteration 135/1000 | Loss: 0.00001946
Iteration 136/1000 | Loss: 0.00001946
Iteration 137/1000 | Loss: 0.00001946
Iteration 138/1000 | Loss: 0.00001946
Iteration 139/1000 | Loss: 0.00001946
Iteration 140/1000 | Loss: 0.00001946
Iteration 141/1000 | Loss: 0.00001945
Iteration 142/1000 | Loss: 0.00001945
Iteration 143/1000 | Loss: 0.00001945
Iteration 144/1000 | Loss: 0.00001945
Iteration 145/1000 | Loss: 0.00001945
Iteration 146/1000 | Loss: 0.00001945
Iteration 147/1000 | Loss: 0.00001945
Iteration 148/1000 | Loss: 0.00001945
Iteration 149/1000 | Loss: 0.00001945
Iteration 150/1000 | Loss: 0.00001944
Iteration 151/1000 | Loss: 0.00001944
Iteration 152/1000 | Loss: 0.00001944
Iteration 153/1000 | Loss: 0.00001944
Iteration 154/1000 | Loss: 0.00001944
Iteration 155/1000 | Loss: 0.00001944
Iteration 156/1000 | Loss: 0.00001944
Iteration 157/1000 | Loss: 0.00001943
Iteration 158/1000 | Loss: 0.00001943
Iteration 159/1000 | Loss: 0.00001943
Iteration 160/1000 | Loss: 0.00001943
Iteration 161/1000 | Loss: 0.00001943
Iteration 162/1000 | Loss: 0.00001943
Iteration 163/1000 | Loss: 0.00001943
Iteration 164/1000 | Loss: 0.00001943
Iteration 165/1000 | Loss: 0.00001943
Iteration 166/1000 | Loss: 0.00001943
Iteration 167/1000 | Loss: 0.00001943
Iteration 168/1000 | Loss: 0.00001943
Iteration 169/1000 | Loss: 0.00001943
Iteration 170/1000 | Loss: 0.00001943
Iteration 171/1000 | Loss: 0.00001943
Iteration 172/1000 | Loss: 0.00001943
Iteration 173/1000 | Loss: 0.00001942
Iteration 174/1000 | Loss: 0.00001942
Iteration 175/1000 | Loss: 0.00001942
Iteration 176/1000 | Loss: 0.00001942
Iteration 177/1000 | Loss: 0.00001942
Iteration 178/1000 | Loss: 0.00001942
Iteration 179/1000 | Loss: 0.00001942
Iteration 180/1000 | Loss: 0.00001942
Iteration 181/1000 | Loss: 0.00001942
Iteration 182/1000 | Loss: 0.00001942
Iteration 183/1000 | Loss: 0.00001942
Iteration 184/1000 | Loss: 0.00001942
Iteration 185/1000 | Loss: 0.00001942
Iteration 186/1000 | Loss: 0.00001942
Iteration 187/1000 | Loss: 0.00001942
Iteration 188/1000 | Loss: 0.00001942
Iteration 189/1000 | Loss: 0.00001941
Iteration 190/1000 | Loss: 0.00001941
Iteration 191/1000 | Loss: 0.00001941
Iteration 192/1000 | Loss: 0.00001941
Iteration 193/1000 | Loss: 0.00001941
Iteration 194/1000 | Loss: 0.00001941
Iteration 195/1000 | Loss: 0.00001941
Iteration 196/1000 | Loss: 0.00001941
Iteration 197/1000 | Loss: 0.00001941
Iteration 198/1000 | Loss: 0.00001941
Iteration 199/1000 | Loss: 0.00001940
Iteration 200/1000 | Loss: 0.00001940
Iteration 201/1000 | Loss: 0.00001940
Iteration 202/1000 | Loss: 0.00001940
Iteration 203/1000 | Loss: 0.00001940
Iteration 204/1000 | Loss: 0.00001940
Iteration 205/1000 | Loss: 0.00001940
Iteration 206/1000 | Loss: 0.00001940
Iteration 207/1000 | Loss: 0.00001940
Iteration 208/1000 | Loss: 0.00001940
Iteration 209/1000 | Loss: 0.00001940
Iteration 210/1000 | Loss: 0.00001940
Iteration 211/1000 | Loss: 0.00001940
Iteration 212/1000 | Loss: 0.00001940
Iteration 213/1000 | Loss: 0.00001940
Iteration 214/1000 | Loss: 0.00001940
Iteration 215/1000 | Loss: 0.00001940
Iteration 216/1000 | Loss: 0.00001940
Iteration 217/1000 | Loss: 0.00001940
Iteration 218/1000 | Loss: 0.00001940
Iteration 219/1000 | Loss: 0.00001940
Iteration 220/1000 | Loss: 0.00001940
Iteration 221/1000 | Loss: 0.00001940
Iteration 222/1000 | Loss: 0.00001940
Iteration 223/1000 | Loss: 0.00001940
Iteration 224/1000 | Loss: 0.00001940
Iteration 225/1000 | Loss: 0.00001940
Iteration 226/1000 | Loss: 0.00001940
Iteration 227/1000 | Loss: 0.00001940
Iteration 228/1000 | Loss: 0.00001940
Iteration 229/1000 | Loss: 0.00001940
Iteration 230/1000 | Loss: 0.00001940
Iteration 231/1000 | Loss: 0.00001940
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 231. Stopping optimization.
Last 5 losses: [1.9398186850594357e-05, 1.9398186850594357e-05, 1.9398186850594357e-05, 1.9398186850594357e-05, 1.9398186850594357e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9398186850594357e-05

Optimization complete. Final v2v error: 3.7266924381256104 mm

Highest mean error: 4.104653835296631 mm for frame 17

Lowest mean error: 3.429583787918091 mm for frame 43

Saving results

Total time: 41.71681475639343
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00392618
Iteration 2/25 | Loss: 0.00134197
Iteration 3/25 | Loss: 0.00126149
Iteration 4/25 | Loss: 0.00124746
Iteration 5/25 | Loss: 0.00124123
Iteration 6/25 | Loss: 0.00124029
Iteration 7/25 | Loss: 0.00124029
Iteration 8/25 | Loss: 0.00124029
Iteration 9/25 | Loss: 0.00124029
Iteration 10/25 | Loss: 0.00124029
Iteration 11/25 | Loss: 0.00124029
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012402869760990143, 0.0012402869760990143, 0.0012402869760990143, 0.0012402869760990143, 0.0012402869760990143]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012402869760990143

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.47699118
Iteration 2/25 | Loss: 0.00092034
Iteration 3/25 | Loss: 0.00092033
Iteration 4/25 | Loss: 0.00092033
Iteration 5/25 | Loss: 0.00092032
Iteration 6/25 | Loss: 0.00092032
Iteration 7/25 | Loss: 0.00092032
Iteration 8/25 | Loss: 0.00092032
Iteration 9/25 | Loss: 0.00092032
Iteration 10/25 | Loss: 0.00092032
Iteration 11/25 | Loss: 0.00092032
Iteration 12/25 | Loss: 0.00092032
Iteration 13/25 | Loss: 0.00092032
Iteration 14/25 | Loss: 0.00092032
Iteration 15/25 | Loss: 0.00092032
Iteration 16/25 | Loss: 0.00092032
Iteration 17/25 | Loss: 0.00092032
Iteration 18/25 | Loss: 0.00092032
Iteration 19/25 | Loss: 0.00092032
Iteration 20/25 | Loss: 0.00092032
Iteration 21/25 | Loss: 0.00092032
Iteration 22/25 | Loss: 0.00092032
Iteration 23/25 | Loss: 0.00092032
Iteration 24/25 | Loss: 0.00092032
Iteration 25/25 | Loss: 0.00092032

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092032
Iteration 2/1000 | Loss: 0.00002788
Iteration 3/1000 | Loss: 0.00001868
Iteration 4/1000 | Loss: 0.00001727
Iteration 5/1000 | Loss: 0.00001645
Iteration 6/1000 | Loss: 0.00001599
Iteration 7/1000 | Loss: 0.00001567
Iteration 8/1000 | Loss: 0.00001547
Iteration 9/1000 | Loss: 0.00001523
Iteration 10/1000 | Loss: 0.00001498
Iteration 11/1000 | Loss: 0.00001490
Iteration 12/1000 | Loss: 0.00001486
Iteration 13/1000 | Loss: 0.00001482
Iteration 14/1000 | Loss: 0.00001481
Iteration 15/1000 | Loss: 0.00001476
Iteration 16/1000 | Loss: 0.00001471
Iteration 17/1000 | Loss: 0.00001469
Iteration 18/1000 | Loss: 0.00001468
Iteration 19/1000 | Loss: 0.00001466
Iteration 20/1000 | Loss: 0.00001464
Iteration 21/1000 | Loss: 0.00001463
Iteration 22/1000 | Loss: 0.00001463
Iteration 23/1000 | Loss: 0.00001460
Iteration 24/1000 | Loss: 0.00001459
Iteration 25/1000 | Loss: 0.00001458
Iteration 26/1000 | Loss: 0.00001457
Iteration 27/1000 | Loss: 0.00001455
Iteration 28/1000 | Loss: 0.00001454
Iteration 29/1000 | Loss: 0.00001454
Iteration 30/1000 | Loss: 0.00001454
Iteration 31/1000 | Loss: 0.00001454
Iteration 32/1000 | Loss: 0.00001454
Iteration 33/1000 | Loss: 0.00001453
Iteration 34/1000 | Loss: 0.00001453
Iteration 35/1000 | Loss: 0.00001453
Iteration 36/1000 | Loss: 0.00001453
Iteration 37/1000 | Loss: 0.00001453
Iteration 38/1000 | Loss: 0.00001452
Iteration 39/1000 | Loss: 0.00001452
Iteration 40/1000 | Loss: 0.00001451
Iteration 41/1000 | Loss: 0.00001450
Iteration 42/1000 | Loss: 0.00001450
Iteration 43/1000 | Loss: 0.00001450
Iteration 44/1000 | Loss: 0.00001450
Iteration 45/1000 | Loss: 0.00001450
Iteration 46/1000 | Loss: 0.00001450
Iteration 47/1000 | Loss: 0.00001450
Iteration 48/1000 | Loss: 0.00001450
Iteration 49/1000 | Loss: 0.00001449
Iteration 50/1000 | Loss: 0.00001449
Iteration 51/1000 | Loss: 0.00001449
Iteration 52/1000 | Loss: 0.00001449
Iteration 53/1000 | Loss: 0.00001449
Iteration 54/1000 | Loss: 0.00001449
Iteration 55/1000 | Loss: 0.00001449
Iteration 56/1000 | Loss: 0.00001448
Iteration 57/1000 | Loss: 0.00001448
Iteration 58/1000 | Loss: 0.00001448
Iteration 59/1000 | Loss: 0.00001448
Iteration 60/1000 | Loss: 0.00001448
Iteration 61/1000 | Loss: 0.00001448
Iteration 62/1000 | Loss: 0.00001448
Iteration 63/1000 | Loss: 0.00001448
Iteration 64/1000 | Loss: 0.00001448
Iteration 65/1000 | Loss: 0.00001448
Iteration 66/1000 | Loss: 0.00001448
Iteration 67/1000 | Loss: 0.00001448
Iteration 68/1000 | Loss: 0.00001448
Iteration 69/1000 | Loss: 0.00001448
Iteration 70/1000 | Loss: 0.00001448
Iteration 71/1000 | Loss: 0.00001448
Iteration 72/1000 | Loss: 0.00001448
Iteration 73/1000 | Loss: 0.00001448
Iteration 74/1000 | Loss: 0.00001448
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 74. Stopping optimization.
Last 5 losses: [1.4475132957159076e-05, 1.4475132957159076e-05, 1.4475132957159076e-05, 1.4475132957159076e-05, 1.4475132957159076e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4475132957159076e-05

Optimization complete. Final v2v error: 3.227367877960205 mm

Highest mean error: 3.8269870281219482 mm for frame 77

Lowest mean error: 2.8409228324890137 mm for frame 27

Saving results

Total time: 29.96877431869507
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00715134
Iteration 2/25 | Loss: 0.00146156
Iteration 3/25 | Loss: 0.00134999
Iteration 4/25 | Loss: 0.00133962
Iteration 5/25 | Loss: 0.00133602
Iteration 6/25 | Loss: 0.00133555
Iteration 7/25 | Loss: 0.00133555
Iteration 8/25 | Loss: 0.00133555
Iteration 9/25 | Loss: 0.00133555
Iteration 10/25 | Loss: 0.00133555
Iteration 11/25 | Loss: 0.00133555
Iteration 12/25 | Loss: 0.00133555
Iteration 13/25 | Loss: 0.00133555
Iteration 14/25 | Loss: 0.00133555
Iteration 15/25 | Loss: 0.00133555
Iteration 16/25 | Loss: 0.00133555
Iteration 17/25 | Loss: 0.00133555
Iteration 18/25 | Loss: 0.00133555
Iteration 19/25 | Loss: 0.00133555
Iteration 20/25 | Loss: 0.00133555
Iteration 21/25 | Loss: 0.00133555
Iteration 22/25 | Loss: 0.00133555
Iteration 23/25 | Loss: 0.00133555
Iteration 24/25 | Loss: 0.00133555
Iteration 25/25 | Loss: 0.00133555

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45084691
Iteration 2/25 | Loss: 0.00076098
Iteration 3/25 | Loss: 0.00076098
Iteration 4/25 | Loss: 0.00076098
Iteration 5/25 | Loss: 0.00076098
Iteration 6/25 | Loss: 0.00076098
Iteration 7/25 | Loss: 0.00076098
Iteration 8/25 | Loss: 0.00076098
Iteration 9/25 | Loss: 0.00076098
Iteration 10/25 | Loss: 0.00076098
Iteration 11/25 | Loss: 0.00076098
Iteration 12/25 | Loss: 0.00076098
Iteration 13/25 | Loss: 0.00076098
Iteration 14/25 | Loss: 0.00076098
Iteration 15/25 | Loss: 0.00076098
Iteration 16/25 | Loss: 0.00076098
Iteration 17/25 | Loss: 0.00076098
Iteration 18/25 | Loss: 0.00076098
Iteration 19/25 | Loss: 0.00076098
Iteration 20/25 | Loss: 0.00076098
Iteration 21/25 | Loss: 0.00076098
Iteration 22/25 | Loss: 0.00076098
Iteration 23/25 | Loss: 0.00076098
Iteration 24/25 | Loss: 0.00076098
Iteration 25/25 | Loss: 0.00076098

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076098
Iteration 2/1000 | Loss: 0.00004175
Iteration 3/1000 | Loss: 0.00003217
Iteration 4/1000 | Loss: 0.00002987
Iteration 5/1000 | Loss: 0.00002853
Iteration 6/1000 | Loss: 0.00002765
Iteration 7/1000 | Loss: 0.00002696
Iteration 8/1000 | Loss: 0.00002667
Iteration 9/1000 | Loss: 0.00002636
Iteration 10/1000 | Loss: 0.00002610
Iteration 11/1000 | Loss: 0.00002584
Iteration 12/1000 | Loss: 0.00002571
Iteration 13/1000 | Loss: 0.00002552
Iteration 14/1000 | Loss: 0.00002548
Iteration 15/1000 | Loss: 0.00002541
Iteration 16/1000 | Loss: 0.00002541
Iteration 17/1000 | Loss: 0.00002538
Iteration 18/1000 | Loss: 0.00002535
Iteration 19/1000 | Loss: 0.00002531
Iteration 20/1000 | Loss: 0.00002530
Iteration 21/1000 | Loss: 0.00002529
Iteration 22/1000 | Loss: 0.00002528
Iteration 23/1000 | Loss: 0.00002527
Iteration 24/1000 | Loss: 0.00002522
Iteration 25/1000 | Loss: 0.00002522
Iteration 26/1000 | Loss: 0.00002518
Iteration 27/1000 | Loss: 0.00002518
Iteration 28/1000 | Loss: 0.00002515
Iteration 29/1000 | Loss: 0.00002515
Iteration 30/1000 | Loss: 0.00002515
Iteration 31/1000 | Loss: 0.00002514
Iteration 32/1000 | Loss: 0.00002511
Iteration 33/1000 | Loss: 0.00002510
Iteration 34/1000 | Loss: 0.00002509
Iteration 35/1000 | Loss: 0.00002509
Iteration 36/1000 | Loss: 0.00002508
Iteration 37/1000 | Loss: 0.00002508
Iteration 38/1000 | Loss: 0.00002507
Iteration 39/1000 | Loss: 0.00002507
Iteration 40/1000 | Loss: 0.00002507
Iteration 41/1000 | Loss: 0.00002506
Iteration 42/1000 | Loss: 0.00002506
Iteration 43/1000 | Loss: 0.00002506
Iteration 44/1000 | Loss: 0.00002505
Iteration 45/1000 | Loss: 0.00002505
Iteration 46/1000 | Loss: 0.00002505
Iteration 47/1000 | Loss: 0.00002505
Iteration 48/1000 | Loss: 0.00002505
Iteration 49/1000 | Loss: 0.00002504
Iteration 50/1000 | Loss: 0.00002504
Iteration 51/1000 | Loss: 0.00002504
Iteration 52/1000 | Loss: 0.00002504
Iteration 53/1000 | Loss: 0.00002504
Iteration 54/1000 | Loss: 0.00002503
Iteration 55/1000 | Loss: 0.00002503
Iteration 56/1000 | Loss: 0.00002503
Iteration 57/1000 | Loss: 0.00002502
Iteration 58/1000 | Loss: 0.00002502
Iteration 59/1000 | Loss: 0.00002502
Iteration 60/1000 | Loss: 0.00002502
Iteration 61/1000 | Loss: 0.00002502
Iteration 62/1000 | Loss: 0.00002502
Iteration 63/1000 | Loss: 0.00002502
Iteration 64/1000 | Loss: 0.00002502
Iteration 65/1000 | Loss: 0.00002502
Iteration 66/1000 | Loss: 0.00002501
Iteration 67/1000 | Loss: 0.00002501
Iteration 68/1000 | Loss: 0.00002501
Iteration 69/1000 | Loss: 0.00002501
Iteration 70/1000 | Loss: 0.00002500
Iteration 71/1000 | Loss: 0.00002500
Iteration 72/1000 | Loss: 0.00002499
Iteration 73/1000 | Loss: 0.00002499
Iteration 74/1000 | Loss: 0.00002499
Iteration 75/1000 | Loss: 0.00002499
Iteration 76/1000 | Loss: 0.00002499
Iteration 77/1000 | Loss: 0.00002498
Iteration 78/1000 | Loss: 0.00002498
Iteration 79/1000 | Loss: 0.00002498
Iteration 80/1000 | Loss: 0.00002498
Iteration 81/1000 | Loss: 0.00002498
Iteration 82/1000 | Loss: 0.00002497
Iteration 83/1000 | Loss: 0.00002497
Iteration 84/1000 | Loss: 0.00002497
Iteration 85/1000 | Loss: 0.00002497
Iteration 86/1000 | Loss: 0.00002496
Iteration 87/1000 | Loss: 0.00002496
Iteration 88/1000 | Loss: 0.00002496
Iteration 89/1000 | Loss: 0.00002496
Iteration 90/1000 | Loss: 0.00002495
Iteration 91/1000 | Loss: 0.00002495
Iteration 92/1000 | Loss: 0.00002495
Iteration 93/1000 | Loss: 0.00002495
Iteration 94/1000 | Loss: 0.00002495
Iteration 95/1000 | Loss: 0.00002495
Iteration 96/1000 | Loss: 0.00002494
Iteration 97/1000 | Loss: 0.00002494
Iteration 98/1000 | Loss: 0.00002494
Iteration 99/1000 | Loss: 0.00002494
Iteration 100/1000 | Loss: 0.00002494
Iteration 101/1000 | Loss: 0.00002494
Iteration 102/1000 | Loss: 0.00002493
Iteration 103/1000 | Loss: 0.00002493
Iteration 104/1000 | Loss: 0.00002493
Iteration 105/1000 | Loss: 0.00002493
Iteration 106/1000 | Loss: 0.00002493
Iteration 107/1000 | Loss: 0.00002493
Iteration 108/1000 | Loss: 0.00002493
Iteration 109/1000 | Loss: 0.00002492
Iteration 110/1000 | Loss: 0.00002492
Iteration 111/1000 | Loss: 0.00002492
Iteration 112/1000 | Loss: 0.00002492
Iteration 113/1000 | Loss: 0.00002491
Iteration 114/1000 | Loss: 0.00002491
Iteration 115/1000 | Loss: 0.00002491
Iteration 116/1000 | Loss: 0.00002491
Iteration 117/1000 | Loss: 0.00002491
Iteration 118/1000 | Loss: 0.00002491
Iteration 119/1000 | Loss: 0.00002491
Iteration 120/1000 | Loss: 0.00002490
Iteration 121/1000 | Loss: 0.00002490
Iteration 122/1000 | Loss: 0.00002490
Iteration 123/1000 | Loss: 0.00002490
Iteration 124/1000 | Loss: 0.00002490
Iteration 125/1000 | Loss: 0.00002489
Iteration 126/1000 | Loss: 0.00002489
Iteration 127/1000 | Loss: 0.00002489
Iteration 128/1000 | Loss: 0.00002489
Iteration 129/1000 | Loss: 0.00002489
Iteration 130/1000 | Loss: 0.00002489
Iteration 131/1000 | Loss: 0.00002489
Iteration 132/1000 | Loss: 0.00002489
Iteration 133/1000 | Loss: 0.00002488
Iteration 134/1000 | Loss: 0.00002488
Iteration 135/1000 | Loss: 0.00002488
Iteration 136/1000 | Loss: 0.00002488
Iteration 137/1000 | Loss: 0.00002488
Iteration 138/1000 | Loss: 0.00002488
Iteration 139/1000 | Loss: 0.00002488
Iteration 140/1000 | Loss: 0.00002488
Iteration 141/1000 | Loss: 0.00002488
Iteration 142/1000 | Loss: 0.00002487
Iteration 143/1000 | Loss: 0.00002487
Iteration 144/1000 | Loss: 0.00002487
Iteration 145/1000 | Loss: 0.00002487
Iteration 146/1000 | Loss: 0.00002487
Iteration 147/1000 | Loss: 0.00002487
Iteration 148/1000 | Loss: 0.00002487
Iteration 149/1000 | Loss: 0.00002487
Iteration 150/1000 | Loss: 0.00002487
Iteration 151/1000 | Loss: 0.00002487
Iteration 152/1000 | Loss: 0.00002486
Iteration 153/1000 | Loss: 0.00002486
Iteration 154/1000 | Loss: 0.00002486
Iteration 155/1000 | Loss: 0.00002486
Iteration 156/1000 | Loss: 0.00002486
Iteration 157/1000 | Loss: 0.00002486
Iteration 158/1000 | Loss: 0.00002486
Iteration 159/1000 | Loss: 0.00002486
Iteration 160/1000 | Loss: 0.00002485
Iteration 161/1000 | Loss: 0.00002485
Iteration 162/1000 | Loss: 0.00002485
Iteration 163/1000 | Loss: 0.00002485
Iteration 164/1000 | Loss: 0.00002485
Iteration 165/1000 | Loss: 0.00002485
Iteration 166/1000 | Loss: 0.00002484
Iteration 167/1000 | Loss: 0.00002484
Iteration 168/1000 | Loss: 0.00002484
Iteration 169/1000 | Loss: 0.00002484
Iteration 170/1000 | Loss: 0.00002484
Iteration 171/1000 | Loss: 0.00002484
Iteration 172/1000 | Loss: 0.00002484
Iteration 173/1000 | Loss: 0.00002484
Iteration 174/1000 | Loss: 0.00002484
Iteration 175/1000 | Loss: 0.00002484
Iteration 176/1000 | Loss: 0.00002483
Iteration 177/1000 | Loss: 0.00002483
Iteration 178/1000 | Loss: 0.00002483
Iteration 179/1000 | Loss: 0.00002483
Iteration 180/1000 | Loss: 0.00002483
Iteration 181/1000 | Loss: 0.00002483
Iteration 182/1000 | Loss: 0.00002483
Iteration 183/1000 | Loss: 0.00002483
Iteration 184/1000 | Loss: 0.00002483
Iteration 185/1000 | Loss: 0.00002483
Iteration 186/1000 | Loss: 0.00002483
Iteration 187/1000 | Loss: 0.00002483
Iteration 188/1000 | Loss: 0.00002483
Iteration 189/1000 | Loss: 0.00002483
Iteration 190/1000 | Loss: 0.00002482
Iteration 191/1000 | Loss: 0.00002482
Iteration 192/1000 | Loss: 0.00002482
Iteration 193/1000 | Loss: 0.00002482
Iteration 194/1000 | Loss: 0.00002482
Iteration 195/1000 | Loss: 0.00002482
Iteration 196/1000 | Loss: 0.00002482
Iteration 197/1000 | Loss: 0.00002482
Iteration 198/1000 | Loss: 0.00002482
Iteration 199/1000 | Loss: 0.00002482
Iteration 200/1000 | Loss: 0.00002482
Iteration 201/1000 | Loss: 0.00002482
Iteration 202/1000 | Loss: 0.00002481
Iteration 203/1000 | Loss: 0.00002481
Iteration 204/1000 | Loss: 0.00002481
Iteration 205/1000 | Loss: 0.00002481
Iteration 206/1000 | Loss: 0.00002481
Iteration 207/1000 | Loss: 0.00002481
Iteration 208/1000 | Loss: 0.00002481
Iteration 209/1000 | Loss: 0.00002481
Iteration 210/1000 | Loss: 0.00002481
Iteration 211/1000 | Loss: 0.00002481
Iteration 212/1000 | Loss: 0.00002481
Iteration 213/1000 | Loss: 0.00002481
Iteration 214/1000 | Loss: 0.00002481
Iteration 215/1000 | Loss: 0.00002481
Iteration 216/1000 | Loss: 0.00002481
Iteration 217/1000 | Loss: 0.00002481
Iteration 218/1000 | Loss: 0.00002481
Iteration 219/1000 | Loss: 0.00002481
Iteration 220/1000 | Loss: 0.00002480
Iteration 221/1000 | Loss: 0.00002480
Iteration 222/1000 | Loss: 0.00002480
Iteration 223/1000 | Loss: 0.00002480
Iteration 224/1000 | Loss: 0.00002480
Iteration 225/1000 | Loss: 0.00002480
Iteration 226/1000 | Loss: 0.00002480
Iteration 227/1000 | Loss: 0.00002480
Iteration 228/1000 | Loss: 0.00002480
Iteration 229/1000 | Loss: 0.00002480
Iteration 230/1000 | Loss: 0.00002479
Iteration 231/1000 | Loss: 0.00002479
Iteration 232/1000 | Loss: 0.00002479
Iteration 233/1000 | Loss: 0.00002479
Iteration 234/1000 | Loss: 0.00002479
Iteration 235/1000 | Loss: 0.00002479
Iteration 236/1000 | Loss: 0.00002479
Iteration 237/1000 | Loss: 0.00002479
Iteration 238/1000 | Loss: 0.00002479
Iteration 239/1000 | Loss: 0.00002479
Iteration 240/1000 | Loss: 0.00002479
Iteration 241/1000 | Loss: 0.00002479
Iteration 242/1000 | Loss: 0.00002478
Iteration 243/1000 | Loss: 0.00002478
Iteration 244/1000 | Loss: 0.00002478
Iteration 245/1000 | Loss: 0.00002478
Iteration 246/1000 | Loss: 0.00002478
Iteration 247/1000 | Loss: 0.00002478
Iteration 248/1000 | Loss: 0.00002477
Iteration 249/1000 | Loss: 0.00002477
Iteration 250/1000 | Loss: 0.00002477
Iteration 251/1000 | Loss: 0.00002477
Iteration 252/1000 | Loss: 0.00002477
Iteration 253/1000 | Loss: 0.00002477
Iteration 254/1000 | Loss: 0.00002477
Iteration 255/1000 | Loss: 0.00002477
Iteration 256/1000 | Loss: 0.00002477
Iteration 257/1000 | Loss: 0.00002477
Iteration 258/1000 | Loss: 0.00002477
Iteration 259/1000 | Loss: 0.00002477
Iteration 260/1000 | Loss: 0.00002477
Iteration 261/1000 | Loss: 0.00002477
Iteration 262/1000 | Loss: 0.00002476
Iteration 263/1000 | Loss: 0.00002476
Iteration 264/1000 | Loss: 0.00002476
Iteration 265/1000 | Loss: 0.00002476
Iteration 266/1000 | Loss: 0.00002476
Iteration 267/1000 | Loss: 0.00002476
Iteration 268/1000 | Loss: 0.00002476
Iteration 269/1000 | Loss: 0.00002476
Iteration 270/1000 | Loss: 0.00002476
Iteration 271/1000 | Loss: 0.00002476
Iteration 272/1000 | Loss: 0.00002476
Iteration 273/1000 | Loss: 0.00002476
Iteration 274/1000 | Loss: 0.00002476
Iteration 275/1000 | Loss: 0.00002476
Iteration 276/1000 | Loss: 0.00002476
Iteration 277/1000 | Loss: 0.00002476
Iteration 278/1000 | Loss: 0.00002476
Iteration 279/1000 | Loss: 0.00002476
Iteration 280/1000 | Loss: 0.00002476
Iteration 281/1000 | Loss: 0.00002476
Iteration 282/1000 | Loss: 0.00002476
Iteration 283/1000 | Loss: 0.00002476
Iteration 284/1000 | Loss: 0.00002476
Iteration 285/1000 | Loss: 0.00002476
Iteration 286/1000 | Loss: 0.00002476
Iteration 287/1000 | Loss: 0.00002476
Iteration 288/1000 | Loss: 0.00002476
Iteration 289/1000 | Loss: 0.00002476
Iteration 290/1000 | Loss: 0.00002476
Iteration 291/1000 | Loss: 0.00002476
Iteration 292/1000 | Loss: 0.00002476
Iteration 293/1000 | Loss: 0.00002476
Iteration 294/1000 | Loss: 0.00002476
Iteration 295/1000 | Loss: 0.00002476
Iteration 296/1000 | Loss: 0.00002476
Iteration 297/1000 | Loss: 0.00002476
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 297. Stopping optimization.
Last 5 losses: [2.476157715136651e-05, 2.476157715136651e-05, 2.476157715136651e-05, 2.476157715136651e-05, 2.476157715136651e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.476157715136651e-05

Optimization complete. Final v2v error: 4.130615234375 mm

Highest mean error: 5.4361042976379395 mm for frame 210

Lowest mean error: 3.64884352684021 mm for frame 18

Saving results

Total time: 54.753185749053955
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01018106
Iteration 2/25 | Loss: 0.01018106
Iteration 3/25 | Loss: 0.00334649
Iteration 4/25 | Loss: 0.00248659
Iteration 5/25 | Loss: 0.00189336
Iteration 6/25 | Loss: 0.00182511
Iteration 7/25 | Loss: 0.00178613
Iteration 8/25 | Loss: 0.00172456
Iteration 9/25 | Loss: 0.00168635
Iteration 10/25 | Loss: 0.00176799
Iteration 11/25 | Loss: 0.00201919
Iteration 12/25 | Loss: 0.00185069
Iteration 13/25 | Loss: 0.00153521
Iteration 14/25 | Loss: 0.00134174
Iteration 15/25 | Loss: 0.00130124
Iteration 16/25 | Loss: 0.00129159
Iteration 17/25 | Loss: 0.00128468
Iteration 18/25 | Loss: 0.00127695
Iteration 19/25 | Loss: 0.00126715
Iteration 20/25 | Loss: 0.00126649
Iteration 21/25 | Loss: 0.00126478
Iteration 22/25 | Loss: 0.00126437
Iteration 23/25 | Loss: 0.00126399
Iteration 24/25 | Loss: 0.00126951
Iteration 25/25 | Loss: 0.00126580

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67205417
Iteration 2/25 | Loss: 0.00072597
Iteration 3/25 | Loss: 0.00071690
Iteration 4/25 | Loss: 0.00071690
Iteration 5/25 | Loss: 0.00071690
Iteration 6/25 | Loss: 0.00071689
Iteration 7/25 | Loss: 0.00071689
Iteration 8/25 | Loss: 0.00071689
Iteration 9/25 | Loss: 0.00071689
Iteration 10/25 | Loss: 0.00071689
Iteration 11/25 | Loss: 0.00071689
Iteration 12/25 | Loss: 0.00071689
Iteration 13/25 | Loss: 0.00071689
Iteration 14/25 | Loss: 0.00071689
Iteration 15/25 | Loss: 0.00071689
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0007168931770138443, 0.0007168931770138443, 0.0007168931770138443, 0.0007168931770138443, 0.0007168931770138443]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007168931770138443

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071689
Iteration 2/1000 | Loss: 0.00013292
Iteration 3/1000 | Loss: 0.00005071
Iteration 4/1000 | Loss: 0.00014461
Iteration 5/1000 | Loss: 0.00010890
Iteration 6/1000 | Loss: 0.00005012
Iteration 7/1000 | Loss: 0.00015917
Iteration 8/1000 | Loss: 0.00014591
Iteration 9/1000 | Loss: 0.00024945
Iteration 10/1000 | Loss: 0.00013540
Iteration 11/1000 | Loss: 0.00008915
Iteration 12/1000 | Loss: 0.00007500
Iteration 13/1000 | Loss: 0.00005739
Iteration 14/1000 | Loss: 0.00004724
Iteration 15/1000 | Loss: 0.00004973
Iteration 16/1000 | Loss: 0.00015696
Iteration 17/1000 | Loss: 0.00061146
Iteration 18/1000 | Loss: 0.00041485
Iteration 19/1000 | Loss: 0.00033690
Iteration 20/1000 | Loss: 0.00015434
Iteration 21/1000 | Loss: 0.00004188
Iteration 22/1000 | Loss: 0.00004414
Iteration 23/1000 | Loss: 0.00024120
Iteration 24/1000 | Loss: 0.00022606
Iteration 25/1000 | Loss: 0.00026773
Iteration 26/1000 | Loss: 0.00020927
Iteration 27/1000 | Loss: 0.00047023
Iteration 28/1000 | Loss: 0.00011701
Iteration 29/1000 | Loss: 0.00013605
Iteration 30/1000 | Loss: 0.00003177
Iteration 31/1000 | Loss: 0.00018678
Iteration 32/1000 | Loss: 0.00004496
Iteration 33/1000 | Loss: 0.00016194
Iteration 34/1000 | Loss: 0.00013631
Iteration 35/1000 | Loss: 0.00014860
Iteration 36/1000 | Loss: 0.00014706
Iteration 37/1000 | Loss: 0.00009357
Iteration 38/1000 | Loss: 0.00013900
Iteration 39/1000 | Loss: 0.00021407
Iteration 40/1000 | Loss: 0.00010362
Iteration 41/1000 | Loss: 0.00004459
Iteration 42/1000 | Loss: 0.00015900
Iteration 43/1000 | Loss: 0.00009170
Iteration 44/1000 | Loss: 0.00003163
Iteration 45/1000 | Loss: 0.00002890
Iteration 46/1000 | Loss: 0.00002653
Iteration 47/1000 | Loss: 0.00002693
Iteration 48/1000 | Loss: 0.00002317
Iteration 49/1000 | Loss: 0.00003440
Iteration 50/1000 | Loss: 0.00003128
Iteration 51/1000 | Loss: 0.00002818
Iteration 52/1000 | Loss: 0.00018523
Iteration 53/1000 | Loss: 0.00017024
Iteration 54/1000 | Loss: 0.00018818
Iteration 55/1000 | Loss: 0.00005261
Iteration 56/1000 | Loss: 0.00017342
Iteration 57/1000 | Loss: 0.00016578
Iteration 58/1000 | Loss: 0.00021760
Iteration 59/1000 | Loss: 0.00011998
Iteration 60/1000 | Loss: 0.00002605
Iteration 61/1000 | Loss: 0.00003455
Iteration 62/1000 | Loss: 0.00002735
Iteration 63/1000 | Loss: 0.00002842
Iteration 64/1000 | Loss: 0.00033998
Iteration 65/1000 | Loss: 0.00005487
Iteration 66/1000 | Loss: 0.00004515
Iteration 67/1000 | Loss: 0.00035444
Iteration 68/1000 | Loss: 0.00003902
Iteration 69/1000 | Loss: 0.00004275
Iteration 70/1000 | Loss: 0.00048290
Iteration 71/1000 | Loss: 0.00072370
Iteration 72/1000 | Loss: 0.00023519
Iteration 73/1000 | Loss: 0.00004484
Iteration 74/1000 | Loss: 0.00004120
Iteration 75/1000 | Loss: 0.00003231
Iteration 76/1000 | Loss: 0.00023549
Iteration 77/1000 | Loss: 0.00017694
Iteration 78/1000 | Loss: 0.00005058
Iteration 79/1000 | Loss: 0.00003794
Iteration 80/1000 | Loss: 0.00003216
Iteration 81/1000 | Loss: 0.00003492
Iteration 82/1000 | Loss: 0.00021126
Iteration 83/1000 | Loss: 0.00041125
Iteration 84/1000 | Loss: 0.00004299
Iteration 85/1000 | Loss: 0.00019596
Iteration 86/1000 | Loss: 0.00015697
Iteration 87/1000 | Loss: 0.00016900
Iteration 88/1000 | Loss: 0.00003060
Iteration 89/1000 | Loss: 0.00003768
Iteration 90/1000 | Loss: 0.00036091
Iteration 91/1000 | Loss: 0.00002627
Iteration 92/1000 | Loss: 0.00002375
Iteration 93/1000 | Loss: 0.00002174
Iteration 94/1000 | Loss: 0.00002029
Iteration 95/1000 | Loss: 0.00001949
Iteration 96/1000 | Loss: 0.00001889
Iteration 97/1000 | Loss: 0.00001848
Iteration 98/1000 | Loss: 0.00001819
Iteration 99/1000 | Loss: 0.00017351
Iteration 100/1000 | Loss: 0.00014543
Iteration 101/1000 | Loss: 0.00007804
Iteration 102/1000 | Loss: 0.00017190
Iteration 103/1000 | Loss: 0.00006197
Iteration 104/1000 | Loss: 0.00007113
Iteration 105/1000 | Loss: 0.00017647
Iteration 106/1000 | Loss: 0.00002616
Iteration 107/1000 | Loss: 0.00002214
Iteration 108/1000 | Loss: 0.00001909
Iteration 109/1000 | Loss: 0.00001822
Iteration 110/1000 | Loss: 0.00001772
Iteration 111/1000 | Loss: 0.00001740
Iteration 112/1000 | Loss: 0.00001724
Iteration 113/1000 | Loss: 0.00001708
Iteration 114/1000 | Loss: 0.00001705
Iteration 115/1000 | Loss: 0.00001692
Iteration 116/1000 | Loss: 0.00001689
Iteration 117/1000 | Loss: 0.00001689
Iteration 118/1000 | Loss: 0.00014324
Iteration 119/1000 | Loss: 0.00008680
Iteration 120/1000 | Loss: 0.00013981
Iteration 121/1000 | Loss: 0.00009591
Iteration 122/1000 | Loss: 0.00016387
Iteration 123/1000 | Loss: 0.00014133
Iteration 124/1000 | Loss: 0.00009060
Iteration 125/1000 | Loss: 0.00016073
Iteration 126/1000 | Loss: 0.00011797
Iteration 127/1000 | Loss: 0.00011329
Iteration 128/1000 | Loss: 0.00002134
Iteration 129/1000 | Loss: 0.00001972
Iteration 130/1000 | Loss: 0.00001864
Iteration 131/1000 | Loss: 0.00001800
Iteration 132/1000 | Loss: 0.00001764
Iteration 133/1000 | Loss: 0.00001722
Iteration 134/1000 | Loss: 0.00001685
Iteration 135/1000 | Loss: 0.00001658
Iteration 136/1000 | Loss: 0.00001649
Iteration 137/1000 | Loss: 0.00001649
Iteration 138/1000 | Loss: 0.00001644
Iteration 139/1000 | Loss: 0.00001643
Iteration 140/1000 | Loss: 0.00001642
Iteration 141/1000 | Loss: 0.00001641
Iteration 142/1000 | Loss: 0.00001640
Iteration 143/1000 | Loss: 0.00001639
Iteration 144/1000 | Loss: 0.00001639
Iteration 145/1000 | Loss: 0.00001638
Iteration 146/1000 | Loss: 0.00001637
Iteration 147/1000 | Loss: 0.00001637
Iteration 148/1000 | Loss: 0.00001637
Iteration 149/1000 | Loss: 0.00001636
Iteration 150/1000 | Loss: 0.00001636
Iteration 151/1000 | Loss: 0.00001636
Iteration 152/1000 | Loss: 0.00001636
Iteration 153/1000 | Loss: 0.00001636
Iteration 154/1000 | Loss: 0.00001636
Iteration 155/1000 | Loss: 0.00001635
Iteration 156/1000 | Loss: 0.00001635
Iteration 157/1000 | Loss: 0.00001634
Iteration 158/1000 | Loss: 0.00001634
Iteration 159/1000 | Loss: 0.00001634
Iteration 160/1000 | Loss: 0.00001633
Iteration 161/1000 | Loss: 0.00001628
Iteration 162/1000 | Loss: 0.00001626
Iteration 163/1000 | Loss: 0.00001625
Iteration 164/1000 | Loss: 0.00001623
Iteration 165/1000 | Loss: 0.00001622
Iteration 166/1000 | Loss: 0.00001621
Iteration 167/1000 | Loss: 0.00001620
Iteration 168/1000 | Loss: 0.00001620
Iteration 169/1000 | Loss: 0.00001619
Iteration 170/1000 | Loss: 0.00001619
Iteration 171/1000 | Loss: 0.00001618
Iteration 172/1000 | Loss: 0.00001618
Iteration 173/1000 | Loss: 0.00001618
Iteration 174/1000 | Loss: 0.00001618
Iteration 175/1000 | Loss: 0.00001618
Iteration 176/1000 | Loss: 0.00001617
Iteration 177/1000 | Loss: 0.00001617
Iteration 178/1000 | Loss: 0.00001617
Iteration 179/1000 | Loss: 0.00001617
Iteration 180/1000 | Loss: 0.00001617
Iteration 181/1000 | Loss: 0.00001616
Iteration 182/1000 | Loss: 0.00001616
Iteration 183/1000 | Loss: 0.00001616
Iteration 184/1000 | Loss: 0.00001616
Iteration 185/1000 | Loss: 0.00001616
Iteration 186/1000 | Loss: 0.00001615
Iteration 187/1000 | Loss: 0.00001615
Iteration 188/1000 | Loss: 0.00001615
Iteration 189/1000 | Loss: 0.00001615
Iteration 190/1000 | Loss: 0.00001614
Iteration 191/1000 | Loss: 0.00001614
Iteration 192/1000 | Loss: 0.00001613
Iteration 193/1000 | Loss: 0.00001613
Iteration 194/1000 | Loss: 0.00001613
Iteration 195/1000 | Loss: 0.00001613
Iteration 196/1000 | Loss: 0.00001612
Iteration 197/1000 | Loss: 0.00001612
Iteration 198/1000 | Loss: 0.00001612
Iteration 199/1000 | Loss: 0.00001611
Iteration 200/1000 | Loss: 0.00001611
Iteration 201/1000 | Loss: 0.00001611
Iteration 202/1000 | Loss: 0.00001610
Iteration 203/1000 | Loss: 0.00001610
Iteration 204/1000 | Loss: 0.00001610
Iteration 205/1000 | Loss: 0.00001610
Iteration 206/1000 | Loss: 0.00001610
Iteration 207/1000 | Loss: 0.00001610
Iteration 208/1000 | Loss: 0.00001610
Iteration 209/1000 | Loss: 0.00001610
Iteration 210/1000 | Loss: 0.00001610
Iteration 211/1000 | Loss: 0.00001610
Iteration 212/1000 | Loss: 0.00001610
Iteration 213/1000 | Loss: 0.00001610
Iteration 214/1000 | Loss: 0.00001610
Iteration 215/1000 | Loss: 0.00001609
Iteration 216/1000 | Loss: 0.00001609
Iteration 217/1000 | Loss: 0.00001609
Iteration 218/1000 | Loss: 0.00001609
Iteration 219/1000 | Loss: 0.00001609
Iteration 220/1000 | Loss: 0.00001609
Iteration 221/1000 | Loss: 0.00001609
Iteration 222/1000 | Loss: 0.00001609
Iteration 223/1000 | Loss: 0.00001609
Iteration 224/1000 | Loss: 0.00001609
Iteration 225/1000 | Loss: 0.00001609
Iteration 226/1000 | Loss: 0.00001609
Iteration 227/1000 | Loss: 0.00001608
Iteration 228/1000 | Loss: 0.00001608
Iteration 229/1000 | Loss: 0.00001608
Iteration 230/1000 | Loss: 0.00001608
Iteration 231/1000 | Loss: 0.00001608
Iteration 232/1000 | Loss: 0.00001608
Iteration 233/1000 | Loss: 0.00001608
Iteration 234/1000 | Loss: 0.00001608
Iteration 235/1000 | Loss: 0.00001608
Iteration 236/1000 | Loss: 0.00001608
Iteration 237/1000 | Loss: 0.00001608
Iteration 238/1000 | Loss: 0.00001608
Iteration 239/1000 | Loss: 0.00001608
Iteration 240/1000 | Loss: 0.00001608
Iteration 241/1000 | Loss: 0.00001608
Iteration 242/1000 | Loss: 0.00001608
Iteration 243/1000 | Loss: 0.00001608
Iteration 244/1000 | Loss: 0.00001608
Iteration 245/1000 | Loss: 0.00001608
Iteration 246/1000 | Loss: 0.00001608
Iteration 247/1000 | Loss: 0.00001608
Iteration 248/1000 | Loss: 0.00001608
Iteration 249/1000 | Loss: 0.00001608
Iteration 250/1000 | Loss: 0.00001608
Iteration 251/1000 | Loss: 0.00001608
Iteration 252/1000 | Loss: 0.00001608
Iteration 253/1000 | Loss: 0.00001608
Iteration 254/1000 | Loss: 0.00001608
Iteration 255/1000 | Loss: 0.00001608
Iteration 256/1000 | Loss: 0.00001608
Iteration 257/1000 | Loss: 0.00001608
Iteration 258/1000 | Loss: 0.00001608
Iteration 259/1000 | Loss: 0.00001608
Iteration 260/1000 | Loss: 0.00001608
Iteration 261/1000 | Loss: 0.00001608
Iteration 262/1000 | Loss: 0.00001608
Iteration 263/1000 | Loss: 0.00001608
Iteration 264/1000 | Loss: 0.00001608
Iteration 265/1000 | Loss: 0.00001608
Iteration 266/1000 | Loss: 0.00001608
Iteration 267/1000 | Loss: 0.00001608
Iteration 268/1000 | Loss: 0.00001608
Iteration 269/1000 | Loss: 0.00001608
Iteration 270/1000 | Loss: 0.00001608
Iteration 271/1000 | Loss: 0.00001608
Iteration 272/1000 | Loss: 0.00001608
Iteration 273/1000 | Loss: 0.00001608
Iteration 274/1000 | Loss: 0.00001608
Iteration 275/1000 | Loss: 0.00001608
Iteration 276/1000 | Loss: 0.00001608
Iteration 277/1000 | Loss: 0.00001608
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 277. Stopping optimization.
Last 5 losses: [1.6079713532235473e-05, 1.6079713532235473e-05, 1.6079713532235473e-05, 1.6079713532235473e-05, 1.6079713532235473e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6079713532235473e-05

Optimization complete. Final v2v error: 3.4338691234588623 mm

Highest mean error: 4.427285671234131 mm for frame 80

Lowest mean error: 3.309583902359009 mm for frame 236

Saving results

Total time: 271.96309638023376
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01017134
Iteration 2/25 | Loss: 0.00229808
Iteration 3/25 | Loss: 0.00178062
Iteration 4/25 | Loss: 0.00159794
Iteration 5/25 | Loss: 0.00151639
Iteration 6/25 | Loss: 0.00151211
Iteration 7/25 | Loss: 0.00142906
Iteration 8/25 | Loss: 0.00143622
Iteration 9/25 | Loss: 0.00138150
Iteration 10/25 | Loss: 0.00137757
Iteration 11/25 | Loss: 0.00136680
Iteration 12/25 | Loss: 0.00136391
Iteration 13/25 | Loss: 0.00136314
Iteration 14/25 | Loss: 0.00136611
Iteration 15/25 | Loss: 0.00135513
Iteration 16/25 | Loss: 0.00135033
Iteration 17/25 | Loss: 0.00135388
Iteration 18/25 | Loss: 0.00135045
Iteration 19/25 | Loss: 0.00134697
Iteration 20/25 | Loss: 0.00134508
Iteration 21/25 | Loss: 0.00134483
Iteration 22/25 | Loss: 0.00134482
Iteration 23/25 | Loss: 0.00134481
Iteration 24/25 | Loss: 0.00134480
Iteration 25/25 | Loss: 0.00134480

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41598022
Iteration 2/25 | Loss: 0.00078828
Iteration 3/25 | Loss: 0.00078121
Iteration 4/25 | Loss: 0.00078121
Iteration 5/25 | Loss: 0.00078121
Iteration 6/25 | Loss: 0.00078121
Iteration 7/25 | Loss: 0.00078121
Iteration 8/25 | Loss: 0.00078121
Iteration 9/25 | Loss: 0.00078121
Iteration 10/25 | Loss: 0.00078121
Iteration 11/25 | Loss: 0.00078121
Iteration 12/25 | Loss: 0.00078121
Iteration 13/25 | Loss: 0.00078121
Iteration 14/25 | Loss: 0.00078121
Iteration 15/25 | Loss: 0.00078121
Iteration 16/25 | Loss: 0.00078121
Iteration 17/25 | Loss: 0.00078121
Iteration 18/25 | Loss: 0.00078121
Iteration 19/25 | Loss: 0.00078121
Iteration 20/25 | Loss: 0.00078121
Iteration 21/25 | Loss: 0.00078121
Iteration 22/25 | Loss: 0.00078121
Iteration 23/25 | Loss: 0.00078121
Iteration 24/25 | Loss: 0.00078121
Iteration 25/25 | Loss: 0.00078121

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078121
Iteration 2/1000 | Loss: 0.00006355
Iteration 3/1000 | Loss: 0.00004440
Iteration 4/1000 | Loss: 0.00003989
Iteration 5/1000 | Loss: 0.00004538
Iteration 6/1000 | Loss: 0.00003617
Iteration 7/1000 | Loss: 0.00003476
Iteration 8/1000 | Loss: 0.00003401
Iteration 9/1000 | Loss: 0.00003350
Iteration 10/1000 | Loss: 0.00003453
Iteration 11/1000 | Loss: 0.00017066
Iteration 12/1000 | Loss: 0.00013545
Iteration 13/1000 | Loss: 0.00018708
Iteration 14/1000 | Loss: 0.00003312
Iteration 15/1000 | Loss: 0.00005441
Iteration 16/1000 | Loss: 0.00002805
Iteration 17/1000 | Loss: 0.00002622
Iteration 18/1000 | Loss: 0.00002543
Iteration 19/1000 | Loss: 0.00003000
Iteration 20/1000 | Loss: 0.00002445
Iteration 21/1000 | Loss: 0.00002423
Iteration 22/1000 | Loss: 0.00002968
Iteration 23/1000 | Loss: 0.00002378
Iteration 24/1000 | Loss: 0.00002354
Iteration 25/1000 | Loss: 0.00002347
Iteration 26/1000 | Loss: 0.00002341
Iteration 27/1000 | Loss: 0.00002327
Iteration 28/1000 | Loss: 0.00002324
Iteration 29/1000 | Loss: 0.00002321
Iteration 30/1000 | Loss: 0.00002319
Iteration 31/1000 | Loss: 0.00002318
Iteration 32/1000 | Loss: 0.00002317
Iteration 33/1000 | Loss: 0.00002317
Iteration 34/1000 | Loss: 0.00002317
Iteration 35/1000 | Loss: 0.00002317
Iteration 36/1000 | Loss: 0.00002317
Iteration 37/1000 | Loss: 0.00002316
Iteration 38/1000 | Loss: 0.00002316
Iteration 39/1000 | Loss: 0.00002316
Iteration 40/1000 | Loss: 0.00002316
Iteration 41/1000 | Loss: 0.00002315
Iteration 42/1000 | Loss: 0.00002315
Iteration 43/1000 | Loss: 0.00002315
Iteration 44/1000 | Loss: 0.00002458
Iteration 45/1000 | Loss: 0.00002313
Iteration 46/1000 | Loss: 0.00002313
Iteration 47/1000 | Loss: 0.00002312
Iteration 48/1000 | Loss: 0.00002312
Iteration 49/1000 | Loss: 0.00002312
Iteration 50/1000 | Loss: 0.00002312
Iteration 51/1000 | Loss: 0.00002312
Iteration 52/1000 | Loss: 0.00002312
Iteration 53/1000 | Loss: 0.00002312
Iteration 54/1000 | Loss: 0.00002312
Iteration 55/1000 | Loss: 0.00002312
Iteration 56/1000 | Loss: 0.00002312
Iteration 57/1000 | Loss: 0.00002312
Iteration 58/1000 | Loss: 0.00002312
Iteration 59/1000 | Loss: 0.00002311
Iteration 60/1000 | Loss: 0.00002311
Iteration 61/1000 | Loss: 0.00002311
Iteration 62/1000 | Loss: 0.00002311
Iteration 63/1000 | Loss: 0.00002311
Iteration 64/1000 | Loss: 0.00002311
Iteration 65/1000 | Loss: 0.00002311
Iteration 66/1000 | Loss: 0.00002310
Iteration 67/1000 | Loss: 0.00002310
Iteration 68/1000 | Loss: 0.00002310
Iteration 69/1000 | Loss: 0.00002310
Iteration 70/1000 | Loss: 0.00002310
Iteration 71/1000 | Loss: 0.00002310
Iteration 72/1000 | Loss: 0.00002310
Iteration 73/1000 | Loss: 0.00002310
Iteration 74/1000 | Loss: 0.00002310
Iteration 75/1000 | Loss: 0.00002310
Iteration 76/1000 | Loss: 0.00002310
Iteration 77/1000 | Loss: 0.00002310
Iteration 78/1000 | Loss: 0.00002310
Iteration 79/1000 | Loss: 0.00002310
Iteration 80/1000 | Loss: 0.00002310
Iteration 81/1000 | Loss: 0.00002310
Iteration 82/1000 | Loss: 0.00002309
Iteration 83/1000 | Loss: 0.00002309
Iteration 84/1000 | Loss: 0.00002309
Iteration 85/1000 | Loss: 0.00002309
Iteration 86/1000 | Loss: 0.00002308
Iteration 87/1000 | Loss: 0.00002308
Iteration 88/1000 | Loss: 0.00002308
Iteration 89/1000 | Loss: 0.00002308
Iteration 90/1000 | Loss: 0.00002307
Iteration 91/1000 | Loss: 0.00002307
Iteration 92/1000 | Loss: 0.00002307
Iteration 93/1000 | Loss: 0.00002307
Iteration 94/1000 | Loss: 0.00002307
Iteration 95/1000 | Loss: 0.00002307
Iteration 96/1000 | Loss: 0.00002307
Iteration 97/1000 | Loss: 0.00002307
Iteration 98/1000 | Loss: 0.00002307
Iteration 99/1000 | Loss: 0.00002306
Iteration 100/1000 | Loss: 0.00002306
Iteration 101/1000 | Loss: 0.00002306
Iteration 102/1000 | Loss: 0.00002306
Iteration 103/1000 | Loss: 0.00002306
Iteration 104/1000 | Loss: 0.00002306
Iteration 105/1000 | Loss: 0.00002306
Iteration 106/1000 | Loss: 0.00002306
Iteration 107/1000 | Loss: 0.00002306
Iteration 108/1000 | Loss: 0.00002306
Iteration 109/1000 | Loss: 0.00002306
Iteration 110/1000 | Loss: 0.00002306
Iteration 111/1000 | Loss: 0.00002306
Iteration 112/1000 | Loss: 0.00002306
Iteration 113/1000 | Loss: 0.00002306
Iteration 114/1000 | Loss: 0.00002306
Iteration 115/1000 | Loss: 0.00002306
Iteration 116/1000 | Loss: 0.00002306
Iteration 117/1000 | Loss: 0.00002306
Iteration 118/1000 | Loss: 0.00002306
Iteration 119/1000 | Loss: 0.00002306
Iteration 120/1000 | Loss: 0.00002306
Iteration 121/1000 | Loss: 0.00002306
Iteration 122/1000 | Loss: 0.00002306
Iteration 123/1000 | Loss: 0.00002306
Iteration 124/1000 | Loss: 0.00002306
Iteration 125/1000 | Loss: 0.00002306
Iteration 126/1000 | Loss: 0.00002306
Iteration 127/1000 | Loss: 0.00002306
Iteration 128/1000 | Loss: 0.00002306
Iteration 129/1000 | Loss: 0.00002306
Iteration 130/1000 | Loss: 0.00002306
Iteration 131/1000 | Loss: 0.00002306
Iteration 132/1000 | Loss: 0.00002306
Iteration 133/1000 | Loss: 0.00002306
Iteration 134/1000 | Loss: 0.00002306
Iteration 135/1000 | Loss: 0.00002306
Iteration 136/1000 | Loss: 0.00002306
Iteration 137/1000 | Loss: 0.00002306
Iteration 138/1000 | Loss: 0.00002306
Iteration 139/1000 | Loss: 0.00002306
Iteration 140/1000 | Loss: 0.00002306
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [2.3061762476572767e-05, 2.3061762476572767e-05, 2.3061762476572767e-05, 2.3061762476572767e-05, 2.3061762476572767e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3061762476572767e-05

Optimization complete. Final v2v error: 3.984351634979248 mm

Highest mean error: 4.189276218414307 mm for frame 118

Lowest mean error: 3.6249806880950928 mm for frame 0

Saving results

Total time: 92.6634030342102
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00884011
Iteration 2/25 | Loss: 0.00146981
Iteration 3/25 | Loss: 0.00132241
Iteration 4/25 | Loss: 0.00130781
Iteration 5/25 | Loss: 0.00130433
Iteration 6/25 | Loss: 0.00130433
Iteration 7/25 | Loss: 0.00130433
Iteration 8/25 | Loss: 0.00130433
Iteration 9/25 | Loss: 0.00130433
Iteration 10/25 | Loss: 0.00130433
Iteration 11/25 | Loss: 0.00130433
Iteration 12/25 | Loss: 0.00130433
Iteration 13/25 | Loss: 0.00130433
Iteration 14/25 | Loss: 0.00130433
Iteration 15/25 | Loss: 0.00130433
Iteration 16/25 | Loss: 0.00130433
Iteration 17/25 | Loss: 0.00130433
Iteration 18/25 | Loss: 0.00130433
Iteration 19/25 | Loss: 0.00130433
Iteration 20/25 | Loss: 0.00130433
Iteration 21/25 | Loss: 0.00130433
Iteration 22/25 | Loss: 0.00130433
Iteration 23/25 | Loss: 0.00130433
Iteration 24/25 | Loss: 0.00130433
Iteration 25/25 | Loss: 0.00130433

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36709929
Iteration 2/25 | Loss: 0.00067164
Iteration 3/25 | Loss: 0.00067154
Iteration 4/25 | Loss: 0.00067154
Iteration 5/25 | Loss: 0.00067154
Iteration 6/25 | Loss: 0.00067154
Iteration 7/25 | Loss: 0.00067154
Iteration 8/25 | Loss: 0.00067154
Iteration 9/25 | Loss: 0.00067154
Iteration 10/25 | Loss: 0.00067154
Iteration 11/25 | Loss: 0.00067154
Iteration 12/25 | Loss: 0.00067154
Iteration 13/25 | Loss: 0.00067154
Iteration 14/25 | Loss: 0.00067154
Iteration 15/25 | Loss: 0.00067154
Iteration 16/25 | Loss: 0.00067154
Iteration 17/25 | Loss: 0.00067154
Iteration 18/25 | Loss: 0.00067154
Iteration 19/25 | Loss: 0.00067154
Iteration 20/25 | Loss: 0.00067154
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006715395138598979, 0.0006715395138598979, 0.0006715395138598979, 0.0006715395138598979, 0.0006715395138598979]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006715395138598979

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067154
Iteration 2/1000 | Loss: 0.00005832
Iteration 3/1000 | Loss: 0.00004393
Iteration 4/1000 | Loss: 0.00004044
Iteration 5/1000 | Loss: 0.00003848
Iteration 6/1000 | Loss: 0.00003737
Iteration 7/1000 | Loss: 0.00003664
Iteration 8/1000 | Loss: 0.00003624
Iteration 9/1000 | Loss: 0.00003570
Iteration 10/1000 | Loss: 0.00003514
Iteration 11/1000 | Loss: 0.00003481
Iteration 12/1000 | Loss: 0.00003446
Iteration 13/1000 | Loss: 0.00003411
Iteration 14/1000 | Loss: 0.00003395
Iteration 15/1000 | Loss: 0.00003381
Iteration 16/1000 | Loss: 0.00003380
Iteration 17/1000 | Loss: 0.00003375
Iteration 18/1000 | Loss: 0.00003370
Iteration 19/1000 | Loss: 0.00003369
Iteration 20/1000 | Loss: 0.00003369
Iteration 21/1000 | Loss: 0.00003369
Iteration 22/1000 | Loss: 0.00003368
Iteration 23/1000 | Loss: 0.00003367
Iteration 24/1000 | Loss: 0.00003366
Iteration 25/1000 | Loss: 0.00003366
Iteration 26/1000 | Loss: 0.00003366
Iteration 27/1000 | Loss: 0.00003365
Iteration 28/1000 | Loss: 0.00003365
Iteration 29/1000 | Loss: 0.00003363
Iteration 30/1000 | Loss: 0.00003363
Iteration 31/1000 | Loss: 0.00003363
Iteration 32/1000 | Loss: 0.00003361
Iteration 33/1000 | Loss: 0.00003361
Iteration 34/1000 | Loss: 0.00003360
Iteration 35/1000 | Loss: 0.00003360
Iteration 36/1000 | Loss: 0.00003360
Iteration 37/1000 | Loss: 0.00003359
Iteration 38/1000 | Loss: 0.00003358
Iteration 39/1000 | Loss: 0.00003358
Iteration 40/1000 | Loss: 0.00003357
Iteration 41/1000 | Loss: 0.00003357
Iteration 42/1000 | Loss: 0.00003356
Iteration 43/1000 | Loss: 0.00003354
Iteration 44/1000 | Loss: 0.00003354
Iteration 45/1000 | Loss: 0.00003353
Iteration 46/1000 | Loss: 0.00003353
Iteration 47/1000 | Loss: 0.00003352
Iteration 48/1000 | Loss: 0.00003352
Iteration 49/1000 | Loss: 0.00003352
Iteration 50/1000 | Loss: 0.00003351
Iteration 51/1000 | Loss: 0.00003351
Iteration 52/1000 | Loss: 0.00003350
Iteration 53/1000 | Loss: 0.00003350
Iteration 54/1000 | Loss: 0.00003349
Iteration 55/1000 | Loss: 0.00003349
Iteration 56/1000 | Loss: 0.00003348
Iteration 57/1000 | Loss: 0.00003347
Iteration 58/1000 | Loss: 0.00003347
Iteration 59/1000 | Loss: 0.00003346
Iteration 60/1000 | Loss: 0.00003346
Iteration 61/1000 | Loss: 0.00003344
Iteration 62/1000 | Loss: 0.00003344
Iteration 63/1000 | Loss: 0.00003343
Iteration 64/1000 | Loss: 0.00003343
Iteration 65/1000 | Loss: 0.00003343
Iteration 66/1000 | Loss: 0.00003343
Iteration 67/1000 | Loss: 0.00003343
Iteration 68/1000 | Loss: 0.00003343
Iteration 69/1000 | Loss: 0.00003342
Iteration 70/1000 | Loss: 0.00003342
Iteration 71/1000 | Loss: 0.00003342
Iteration 72/1000 | Loss: 0.00003342
Iteration 73/1000 | Loss: 0.00003342
Iteration 74/1000 | Loss: 0.00003342
Iteration 75/1000 | Loss: 0.00003341
Iteration 76/1000 | Loss: 0.00003341
Iteration 77/1000 | Loss: 0.00003340
Iteration 78/1000 | Loss: 0.00003339
Iteration 79/1000 | Loss: 0.00003339
Iteration 80/1000 | Loss: 0.00003338
Iteration 81/1000 | Loss: 0.00003338
Iteration 82/1000 | Loss: 0.00003338
Iteration 83/1000 | Loss: 0.00003337
Iteration 84/1000 | Loss: 0.00003337
Iteration 85/1000 | Loss: 0.00003336
Iteration 86/1000 | Loss: 0.00003335
Iteration 87/1000 | Loss: 0.00003335
Iteration 88/1000 | Loss: 0.00003335
Iteration 89/1000 | Loss: 0.00003334
Iteration 90/1000 | Loss: 0.00003334
Iteration 91/1000 | Loss: 0.00003334
Iteration 92/1000 | Loss: 0.00003333
Iteration 93/1000 | Loss: 0.00003333
Iteration 94/1000 | Loss: 0.00003333
Iteration 95/1000 | Loss: 0.00003333
Iteration 96/1000 | Loss: 0.00003332
Iteration 97/1000 | Loss: 0.00003332
Iteration 98/1000 | Loss: 0.00003332
Iteration 99/1000 | Loss: 0.00003332
Iteration 100/1000 | Loss: 0.00003332
Iteration 101/1000 | Loss: 0.00003332
Iteration 102/1000 | Loss: 0.00003331
Iteration 103/1000 | Loss: 0.00003331
Iteration 104/1000 | Loss: 0.00003331
Iteration 105/1000 | Loss: 0.00003331
Iteration 106/1000 | Loss: 0.00003331
Iteration 107/1000 | Loss: 0.00003331
Iteration 108/1000 | Loss: 0.00003330
Iteration 109/1000 | Loss: 0.00003330
Iteration 110/1000 | Loss: 0.00003330
Iteration 111/1000 | Loss: 0.00003330
Iteration 112/1000 | Loss: 0.00003330
Iteration 113/1000 | Loss: 0.00003330
Iteration 114/1000 | Loss: 0.00003330
Iteration 115/1000 | Loss: 0.00003330
Iteration 116/1000 | Loss: 0.00003330
Iteration 117/1000 | Loss: 0.00003330
Iteration 118/1000 | Loss: 0.00003330
Iteration 119/1000 | Loss: 0.00003330
Iteration 120/1000 | Loss: 0.00003330
Iteration 121/1000 | Loss: 0.00003329
Iteration 122/1000 | Loss: 0.00003329
Iteration 123/1000 | Loss: 0.00003329
Iteration 124/1000 | Loss: 0.00003329
Iteration 125/1000 | Loss: 0.00003329
Iteration 126/1000 | Loss: 0.00003329
Iteration 127/1000 | Loss: 0.00003329
Iteration 128/1000 | Loss: 0.00003329
Iteration 129/1000 | Loss: 0.00003329
Iteration 130/1000 | Loss: 0.00003329
Iteration 131/1000 | Loss: 0.00003329
Iteration 132/1000 | Loss: 0.00003329
Iteration 133/1000 | Loss: 0.00003329
Iteration 134/1000 | Loss: 0.00003329
Iteration 135/1000 | Loss: 0.00003329
Iteration 136/1000 | Loss: 0.00003329
Iteration 137/1000 | Loss: 0.00003329
Iteration 138/1000 | Loss: 0.00003329
Iteration 139/1000 | Loss: 0.00003329
Iteration 140/1000 | Loss: 0.00003329
Iteration 141/1000 | Loss: 0.00003329
Iteration 142/1000 | Loss: 0.00003329
Iteration 143/1000 | Loss: 0.00003329
Iteration 144/1000 | Loss: 0.00003329
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [3.32868512487039e-05, 3.32868512487039e-05, 3.32868512487039e-05, 3.32868512487039e-05, 3.32868512487039e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.32868512487039e-05

Optimization complete. Final v2v error: 4.701991558074951 mm

Highest mean error: 4.900406360626221 mm for frame 8

Lowest mean error: 4.5625104904174805 mm for frame 141

Saving results

Total time: 45.84949707984924
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01041536
Iteration 2/25 | Loss: 0.00181554
Iteration 3/25 | Loss: 0.00155385
Iteration 4/25 | Loss: 0.00127219
Iteration 5/25 | Loss: 0.00124310
Iteration 6/25 | Loss: 0.00123684
Iteration 7/25 | Loss: 0.00123220
Iteration 8/25 | Loss: 0.00122707
Iteration 9/25 | Loss: 0.00122601
Iteration 10/25 | Loss: 0.00122487
Iteration 11/25 | Loss: 0.00122850
Iteration 12/25 | Loss: 0.00122184
Iteration 13/25 | Loss: 0.00122035
Iteration 14/25 | Loss: 0.00121959
Iteration 15/25 | Loss: 0.00121953
Iteration 16/25 | Loss: 0.00121952
Iteration 17/25 | Loss: 0.00121952
Iteration 18/25 | Loss: 0.00121952
Iteration 19/25 | Loss: 0.00121952
Iteration 20/25 | Loss: 0.00121952
Iteration 21/25 | Loss: 0.00121952
Iteration 22/25 | Loss: 0.00121952
Iteration 23/25 | Loss: 0.00121952
Iteration 24/25 | Loss: 0.00121952
Iteration 25/25 | Loss: 0.00121952

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.64603233
Iteration 2/25 | Loss: 0.00106466
Iteration 3/25 | Loss: 0.00083240
Iteration 4/25 | Loss: 0.00083239
Iteration 5/25 | Loss: 0.00083239
Iteration 6/25 | Loss: 0.00083239
Iteration 7/25 | Loss: 0.00083238
Iteration 8/25 | Loss: 0.00083238
Iteration 9/25 | Loss: 0.00083238
Iteration 10/25 | Loss: 0.00083238
Iteration 11/25 | Loss: 0.00083238
Iteration 12/25 | Loss: 0.00083238
Iteration 13/25 | Loss: 0.00083238
Iteration 14/25 | Loss: 0.00083238
Iteration 15/25 | Loss: 0.00083238
Iteration 16/25 | Loss: 0.00083238
Iteration 17/25 | Loss: 0.00083238
Iteration 18/25 | Loss: 0.00083238
Iteration 19/25 | Loss: 0.00083238
Iteration 20/25 | Loss: 0.00083238
Iteration 21/25 | Loss: 0.00083238
Iteration 22/25 | Loss: 0.00083238
Iteration 23/25 | Loss: 0.00083238
Iteration 24/25 | Loss: 0.00083238
Iteration 25/25 | Loss: 0.00083238

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083238
Iteration 2/1000 | Loss: 0.00027423
Iteration 3/1000 | Loss: 0.00012724
Iteration 4/1000 | Loss: 0.00001868
Iteration 5/1000 | Loss: 0.00006292
Iteration 6/1000 | Loss: 0.00001648
Iteration 7/1000 | Loss: 0.00011072
Iteration 8/1000 | Loss: 0.00003887
Iteration 9/1000 | Loss: 0.00001564
Iteration 10/1000 | Loss: 0.00001542
Iteration 11/1000 | Loss: 0.00001521
Iteration 12/1000 | Loss: 0.00001496
Iteration 13/1000 | Loss: 0.00004383
Iteration 14/1000 | Loss: 0.00002309
Iteration 15/1000 | Loss: 0.00001477
Iteration 16/1000 | Loss: 0.00001475
Iteration 17/1000 | Loss: 0.00001475
Iteration 18/1000 | Loss: 0.00001475
Iteration 19/1000 | Loss: 0.00001471
Iteration 20/1000 | Loss: 0.00001464
Iteration 21/1000 | Loss: 0.00001463
Iteration 22/1000 | Loss: 0.00001462
Iteration 23/1000 | Loss: 0.00009378
Iteration 24/1000 | Loss: 0.00001477
Iteration 25/1000 | Loss: 0.00001449
Iteration 26/1000 | Loss: 0.00001447
Iteration 27/1000 | Loss: 0.00001444
Iteration 28/1000 | Loss: 0.00001441
Iteration 29/1000 | Loss: 0.00001438
Iteration 30/1000 | Loss: 0.00001437
Iteration 31/1000 | Loss: 0.00001433
Iteration 32/1000 | Loss: 0.00001431
Iteration 33/1000 | Loss: 0.00001430
Iteration 34/1000 | Loss: 0.00001427
Iteration 35/1000 | Loss: 0.00001427
Iteration 36/1000 | Loss: 0.00001427
Iteration 37/1000 | Loss: 0.00001426
Iteration 38/1000 | Loss: 0.00001426
Iteration 39/1000 | Loss: 0.00001425
Iteration 40/1000 | Loss: 0.00001425
Iteration 41/1000 | Loss: 0.00001424
Iteration 42/1000 | Loss: 0.00001420
Iteration 43/1000 | Loss: 0.00001419
Iteration 44/1000 | Loss: 0.00001419
Iteration 45/1000 | Loss: 0.00001419
Iteration 46/1000 | Loss: 0.00001419
Iteration 47/1000 | Loss: 0.00001419
Iteration 48/1000 | Loss: 0.00001419
Iteration 49/1000 | Loss: 0.00001419
Iteration 50/1000 | Loss: 0.00001419
Iteration 51/1000 | Loss: 0.00001419
Iteration 52/1000 | Loss: 0.00001418
Iteration 53/1000 | Loss: 0.00001418
Iteration 54/1000 | Loss: 0.00001418
Iteration 55/1000 | Loss: 0.00001416
Iteration 56/1000 | Loss: 0.00001415
Iteration 57/1000 | Loss: 0.00001415
Iteration 58/1000 | Loss: 0.00001414
Iteration 59/1000 | Loss: 0.00001413
Iteration 60/1000 | Loss: 0.00001412
Iteration 61/1000 | Loss: 0.00001412
Iteration 62/1000 | Loss: 0.00001409
Iteration 63/1000 | Loss: 0.00001408
Iteration 64/1000 | Loss: 0.00001408
Iteration 65/1000 | Loss: 0.00001407
Iteration 66/1000 | Loss: 0.00001407
Iteration 67/1000 | Loss: 0.00001406
Iteration 68/1000 | Loss: 0.00001406
Iteration 69/1000 | Loss: 0.00001405
Iteration 70/1000 | Loss: 0.00001405
Iteration 71/1000 | Loss: 0.00001405
Iteration 72/1000 | Loss: 0.00001404
Iteration 73/1000 | Loss: 0.00001404
Iteration 74/1000 | Loss: 0.00001404
Iteration 75/1000 | Loss: 0.00001404
Iteration 76/1000 | Loss: 0.00001403
Iteration 77/1000 | Loss: 0.00001403
Iteration 78/1000 | Loss: 0.00001403
Iteration 79/1000 | Loss: 0.00001402
Iteration 80/1000 | Loss: 0.00001402
Iteration 81/1000 | Loss: 0.00001402
Iteration 82/1000 | Loss: 0.00001401
Iteration 83/1000 | Loss: 0.00001401
Iteration 84/1000 | Loss: 0.00001401
Iteration 85/1000 | Loss: 0.00001401
Iteration 86/1000 | Loss: 0.00001400
Iteration 87/1000 | Loss: 0.00001400
Iteration 88/1000 | Loss: 0.00001400
Iteration 89/1000 | Loss: 0.00001400
Iteration 90/1000 | Loss: 0.00001400
Iteration 91/1000 | Loss: 0.00001400
Iteration 92/1000 | Loss: 0.00001399
Iteration 93/1000 | Loss: 0.00001399
Iteration 94/1000 | Loss: 0.00001398
Iteration 95/1000 | Loss: 0.00001398
Iteration 96/1000 | Loss: 0.00001398
Iteration 97/1000 | Loss: 0.00001398
Iteration 98/1000 | Loss: 0.00001397
Iteration 99/1000 | Loss: 0.00001397
Iteration 100/1000 | Loss: 0.00001397
Iteration 101/1000 | Loss: 0.00001397
Iteration 102/1000 | Loss: 0.00001397
Iteration 103/1000 | Loss: 0.00001397
Iteration 104/1000 | Loss: 0.00001397
Iteration 105/1000 | Loss: 0.00001397
Iteration 106/1000 | Loss: 0.00001397
Iteration 107/1000 | Loss: 0.00001396
Iteration 108/1000 | Loss: 0.00001395
Iteration 109/1000 | Loss: 0.00001395
Iteration 110/1000 | Loss: 0.00001395
Iteration 111/1000 | Loss: 0.00001395
Iteration 112/1000 | Loss: 0.00001394
Iteration 113/1000 | Loss: 0.00001394
Iteration 114/1000 | Loss: 0.00001394
Iteration 115/1000 | Loss: 0.00001394
Iteration 116/1000 | Loss: 0.00001394
Iteration 117/1000 | Loss: 0.00001394
Iteration 118/1000 | Loss: 0.00001394
Iteration 119/1000 | Loss: 0.00001393
Iteration 120/1000 | Loss: 0.00001393
Iteration 121/1000 | Loss: 0.00001393
Iteration 122/1000 | Loss: 0.00001393
Iteration 123/1000 | Loss: 0.00001392
Iteration 124/1000 | Loss: 0.00001392
Iteration 125/1000 | Loss: 0.00001392
Iteration 126/1000 | Loss: 0.00001391
Iteration 127/1000 | Loss: 0.00001391
Iteration 128/1000 | Loss: 0.00001391
Iteration 129/1000 | Loss: 0.00001391
Iteration 130/1000 | Loss: 0.00001391
Iteration 131/1000 | Loss: 0.00001391
Iteration 132/1000 | Loss: 0.00001391
Iteration 133/1000 | Loss: 0.00001391
Iteration 134/1000 | Loss: 0.00001391
Iteration 135/1000 | Loss: 0.00001391
Iteration 136/1000 | Loss: 0.00001391
Iteration 137/1000 | Loss: 0.00001390
Iteration 138/1000 | Loss: 0.00001390
Iteration 139/1000 | Loss: 0.00001390
Iteration 140/1000 | Loss: 0.00001390
Iteration 141/1000 | Loss: 0.00001390
Iteration 142/1000 | Loss: 0.00001390
Iteration 143/1000 | Loss: 0.00001390
Iteration 144/1000 | Loss: 0.00001390
Iteration 145/1000 | Loss: 0.00001390
Iteration 146/1000 | Loss: 0.00001390
Iteration 147/1000 | Loss: 0.00001390
Iteration 148/1000 | Loss: 0.00001390
Iteration 149/1000 | Loss: 0.00001390
Iteration 150/1000 | Loss: 0.00001390
Iteration 151/1000 | Loss: 0.00001390
Iteration 152/1000 | Loss: 0.00001389
Iteration 153/1000 | Loss: 0.00001389
Iteration 154/1000 | Loss: 0.00001389
Iteration 155/1000 | Loss: 0.00001389
Iteration 156/1000 | Loss: 0.00001389
Iteration 157/1000 | Loss: 0.00001389
Iteration 158/1000 | Loss: 0.00001389
Iteration 159/1000 | Loss: 0.00001389
Iteration 160/1000 | Loss: 0.00001389
Iteration 161/1000 | Loss: 0.00001389
Iteration 162/1000 | Loss: 0.00001389
Iteration 163/1000 | Loss: 0.00001389
Iteration 164/1000 | Loss: 0.00001389
Iteration 165/1000 | Loss: 0.00001389
Iteration 166/1000 | Loss: 0.00001389
Iteration 167/1000 | Loss: 0.00001389
Iteration 168/1000 | Loss: 0.00001389
Iteration 169/1000 | Loss: 0.00001389
Iteration 170/1000 | Loss: 0.00001389
Iteration 171/1000 | Loss: 0.00001388
Iteration 172/1000 | Loss: 0.00001388
Iteration 173/1000 | Loss: 0.00001388
Iteration 174/1000 | Loss: 0.00001388
Iteration 175/1000 | Loss: 0.00001388
Iteration 176/1000 | Loss: 0.00001388
Iteration 177/1000 | Loss: 0.00001388
Iteration 178/1000 | Loss: 0.00001388
Iteration 179/1000 | Loss: 0.00001388
Iteration 180/1000 | Loss: 0.00001388
Iteration 181/1000 | Loss: 0.00001388
Iteration 182/1000 | Loss: 0.00001388
Iteration 183/1000 | Loss: 0.00001388
Iteration 184/1000 | Loss: 0.00001388
Iteration 185/1000 | Loss: 0.00001388
Iteration 186/1000 | Loss: 0.00001388
Iteration 187/1000 | Loss: 0.00001388
Iteration 188/1000 | Loss: 0.00001388
Iteration 189/1000 | Loss: 0.00001388
Iteration 190/1000 | Loss: 0.00001388
Iteration 191/1000 | Loss: 0.00001388
Iteration 192/1000 | Loss: 0.00001388
Iteration 193/1000 | Loss: 0.00001388
Iteration 194/1000 | Loss: 0.00001388
Iteration 195/1000 | Loss: 0.00001388
Iteration 196/1000 | Loss: 0.00001388
Iteration 197/1000 | Loss: 0.00001388
Iteration 198/1000 | Loss: 0.00001388
Iteration 199/1000 | Loss: 0.00001388
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [1.3875297554477584e-05, 1.3875297554477584e-05, 1.3875297554477584e-05, 1.3875297554477584e-05, 1.3875297554477584e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3875297554477584e-05

Optimization complete. Final v2v error: 3.1697428226470947 mm

Highest mean error: 3.50433087348938 mm for frame 13

Lowest mean error: 2.8380630016326904 mm for frame 64

Saving results

Total time: 64.92970991134644
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00429826
Iteration 2/25 | Loss: 0.00136257
Iteration 3/25 | Loss: 0.00124336
Iteration 4/25 | Loss: 0.00123191
Iteration 5/25 | Loss: 0.00122839
Iteration 6/25 | Loss: 0.00122773
Iteration 7/25 | Loss: 0.00122773
Iteration 8/25 | Loss: 0.00122773
Iteration 9/25 | Loss: 0.00122773
Iteration 10/25 | Loss: 0.00122773
Iteration 11/25 | Loss: 0.00122773
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012277330970391631, 0.0012277330970391631, 0.0012277330970391631, 0.0012277330970391631, 0.0012277330970391631]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012277330970391631

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53137684
Iteration 2/25 | Loss: 0.00074952
Iteration 3/25 | Loss: 0.00074952
Iteration 4/25 | Loss: 0.00074952
Iteration 5/25 | Loss: 0.00074952
Iteration 6/25 | Loss: 0.00074952
Iteration 7/25 | Loss: 0.00074952
Iteration 8/25 | Loss: 0.00074952
Iteration 9/25 | Loss: 0.00074952
Iteration 10/25 | Loss: 0.00074952
Iteration 11/25 | Loss: 0.00074952
Iteration 12/25 | Loss: 0.00074952
Iteration 13/25 | Loss: 0.00074952
Iteration 14/25 | Loss: 0.00074952
Iteration 15/25 | Loss: 0.00074952
Iteration 16/25 | Loss: 0.00074952
Iteration 17/25 | Loss: 0.00074952
Iteration 18/25 | Loss: 0.00074952
Iteration 19/25 | Loss: 0.00074952
Iteration 20/25 | Loss: 0.00074952
Iteration 21/25 | Loss: 0.00074952
Iteration 22/25 | Loss: 0.00074952
Iteration 23/25 | Loss: 0.00074952
Iteration 24/25 | Loss: 0.00074952
Iteration 25/25 | Loss: 0.00074952

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074952
Iteration 2/1000 | Loss: 0.00003169
Iteration 3/1000 | Loss: 0.00002154
Iteration 4/1000 | Loss: 0.00001988
Iteration 5/1000 | Loss: 0.00001904
Iteration 6/1000 | Loss: 0.00001838
Iteration 7/1000 | Loss: 0.00001790
Iteration 8/1000 | Loss: 0.00001757
Iteration 9/1000 | Loss: 0.00001731
Iteration 10/1000 | Loss: 0.00001710
Iteration 11/1000 | Loss: 0.00001708
Iteration 12/1000 | Loss: 0.00001707
Iteration 13/1000 | Loss: 0.00001692
Iteration 14/1000 | Loss: 0.00001690
Iteration 15/1000 | Loss: 0.00001690
Iteration 16/1000 | Loss: 0.00001677
Iteration 17/1000 | Loss: 0.00001675
Iteration 18/1000 | Loss: 0.00001672
Iteration 19/1000 | Loss: 0.00001670
Iteration 20/1000 | Loss: 0.00001669
Iteration 21/1000 | Loss: 0.00001668
Iteration 22/1000 | Loss: 0.00001662
Iteration 23/1000 | Loss: 0.00001660
Iteration 24/1000 | Loss: 0.00001659
Iteration 25/1000 | Loss: 0.00001659
Iteration 26/1000 | Loss: 0.00001659
Iteration 27/1000 | Loss: 0.00001658
Iteration 28/1000 | Loss: 0.00001658
Iteration 29/1000 | Loss: 0.00001658
Iteration 30/1000 | Loss: 0.00001657
Iteration 31/1000 | Loss: 0.00001656
Iteration 32/1000 | Loss: 0.00001656
Iteration 33/1000 | Loss: 0.00001655
Iteration 34/1000 | Loss: 0.00001655
Iteration 35/1000 | Loss: 0.00001653
Iteration 36/1000 | Loss: 0.00001653
Iteration 37/1000 | Loss: 0.00001652
Iteration 38/1000 | Loss: 0.00001652
Iteration 39/1000 | Loss: 0.00001651
Iteration 40/1000 | Loss: 0.00001650
Iteration 41/1000 | Loss: 0.00001650
Iteration 42/1000 | Loss: 0.00001649
Iteration 43/1000 | Loss: 0.00001644
Iteration 44/1000 | Loss: 0.00001642
Iteration 45/1000 | Loss: 0.00001641
Iteration 46/1000 | Loss: 0.00001641
Iteration 47/1000 | Loss: 0.00001641
Iteration 48/1000 | Loss: 0.00001638
Iteration 49/1000 | Loss: 0.00001637
Iteration 50/1000 | Loss: 0.00001637
Iteration 51/1000 | Loss: 0.00001637
Iteration 52/1000 | Loss: 0.00001636
Iteration 53/1000 | Loss: 0.00001636
Iteration 54/1000 | Loss: 0.00001635
Iteration 55/1000 | Loss: 0.00001634
Iteration 56/1000 | Loss: 0.00001633
Iteration 57/1000 | Loss: 0.00001632
Iteration 58/1000 | Loss: 0.00001632
Iteration 59/1000 | Loss: 0.00001631
Iteration 60/1000 | Loss: 0.00001631
Iteration 61/1000 | Loss: 0.00001630
Iteration 62/1000 | Loss: 0.00001629
Iteration 63/1000 | Loss: 0.00001629
Iteration 64/1000 | Loss: 0.00001628
Iteration 65/1000 | Loss: 0.00001628
Iteration 66/1000 | Loss: 0.00001625
Iteration 67/1000 | Loss: 0.00001624
Iteration 68/1000 | Loss: 0.00001623
Iteration 69/1000 | Loss: 0.00001623
Iteration 70/1000 | Loss: 0.00001623
Iteration 71/1000 | Loss: 0.00001622
Iteration 72/1000 | Loss: 0.00001622
Iteration 73/1000 | Loss: 0.00001622
Iteration 74/1000 | Loss: 0.00001621
Iteration 75/1000 | Loss: 0.00001621
Iteration 76/1000 | Loss: 0.00001621
Iteration 77/1000 | Loss: 0.00001620
Iteration 78/1000 | Loss: 0.00001620
Iteration 79/1000 | Loss: 0.00001619
Iteration 80/1000 | Loss: 0.00001619
Iteration 81/1000 | Loss: 0.00001619
Iteration 82/1000 | Loss: 0.00001619
Iteration 83/1000 | Loss: 0.00001619
Iteration 84/1000 | Loss: 0.00001619
Iteration 85/1000 | Loss: 0.00001618
Iteration 86/1000 | Loss: 0.00001618
Iteration 87/1000 | Loss: 0.00001618
Iteration 88/1000 | Loss: 0.00001618
Iteration 89/1000 | Loss: 0.00001617
Iteration 90/1000 | Loss: 0.00001617
Iteration 91/1000 | Loss: 0.00001617
Iteration 92/1000 | Loss: 0.00001617
Iteration 93/1000 | Loss: 0.00001617
Iteration 94/1000 | Loss: 0.00001616
Iteration 95/1000 | Loss: 0.00001616
Iteration 96/1000 | Loss: 0.00001616
Iteration 97/1000 | Loss: 0.00001616
Iteration 98/1000 | Loss: 0.00001616
Iteration 99/1000 | Loss: 0.00001616
Iteration 100/1000 | Loss: 0.00001616
Iteration 101/1000 | Loss: 0.00001616
Iteration 102/1000 | Loss: 0.00001615
Iteration 103/1000 | Loss: 0.00001615
Iteration 104/1000 | Loss: 0.00001615
Iteration 105/1000 | Loss: 0.00001615
Iteration 106/1000 | Loss: 0.00001615
Iteration 107/1000 | Loss: 0.00001615
Iteration 108/1000 | Loss: 0.00001615
Iteration 109/1000 | Loss: 0.00001615
Iteration 110/1000 | Loss: 0.00001615
Iteration 111/1000 | Loss: 0.00001615
Iteration 112/1000 | Loss: 0.00001615
Iteration 113/1000 | Loss: 0.00001614
Iteration 114/1000 | Loss: 0.00001614
Iteration 115/1000 | Loss: 0.00001614
Iteration 116/1000 | Loss: 0.00001613
Iteration 117/1000 | Loss: 0.00001613
Iteration 118/1000 | Loss: 0.00001613
Iteration 119/1000 | Loss: 0.00001613
Iteration 120/1000 | Loss: 0.00001613
Iteration 121/1000 | Loss: 0.00001613
Iteration 122/1000 | Loss: 0.00001613
Iteration 123/1000 | Loss: 0.00001612
Iteration 124/1000 | Loss: 0.00001612
Iteration 125/1000 | Loss: 0.00001612
Iteration 126/1000 | Loss: 0.00001612
Iteration 127/1000 | Loss: 0.00001612
Iteration 128/1000 | Loss: 0.00001612
Iteration 129/1000 | Loss: 0.00001612
Iteration 130/1000 | Loss: 0.00001612
Iteration 131/1000 | Loss: 0.00001612
Iteration 132/1000 | Loss: 0.00001611
Iteration 133/1000 | Loss: 0.00001611
Iteration 134/1000 | Loss: 0.00001611
Iteration 135/1000 | Loss: 0.00001611
Iteration 136/1000 | Loss: 0.00001611
Iteration 137/1000 | Loss: 0.00001611
Iteration 138/1000 | Loss: 0.00001611
Iteration 139/1000 | Loss: 0.00001611
Iteration 140/1000 | Loss: 0.00001611
Iteration 141/1000 | Loss: 0.00001611
Iteration 142/1000 | Loss: 0.00001611
Iteration 143/1000 | Loss: 0.00001611
Iteration 144/1000 | Loss: 0.00001611
Iteration 145/1000 | Loss: 0.00001610
Iteration 146/1000 | Loss: 0.00001610
Iteration 147/1000 | Loss: 0.00001610
Iteration 148/1000 | Loss: 0.00001610
Iteration 149/1000 | Loss: 0.00001610
Iteration 150/1000 | Loss: 0.00001610
Iteration 151/1000 | Loss: 0.00001610
Iteration 152/1000 | Loss: 0.00001610
Iteration 153/1000 | Loss: 0.00001610
Iteration 154/1000 | Loss: 0.00001610
Iteration 155/1000 | Loss: 0.00001610
Iteration 156/1000 | Loss: 0.00001609
Iteration 157/1000 | Loss: 0.00001609
Iteration 158/1000 | Loss: 0.00001609
Iteration 159/1000 | Loss: 0.00001609
Iteration 160/1000 | Loss: 0.00001609
Iteration 161/1000 | Loss: 0.00001609
Iteration 162/1000 | Loss: 0.00001609
Iteration 163/1000 | Loss: 0.00001609
Iteration 164/1000 | Loss: 0.00001609
Iteration 165/1000 | Loss: 0.00001609
Iteration 166/1000 | Loss: 0.00001609
Iteration 167/1000 | Loss: 0.00001609
Iteration 168/1000 | Loss: 0.00001609
Iteration 169/1000 | Loss: 0.00001609
Iteration 170/1000 | Loss: 0.00001609
Iteration 171/1000 | Loss: 0.00001608
Iteration 172/1000 | Loss: 0.00001608
Iteration 173/1000 | Loss: 0.00001608
Iteration 174/1000 | Loss: 0.00001608
Iteration 175/1000 | Loss: 0.00001608
Iteration 176/1000 | Loss: 0.00001608
Iteration 177/1000 | Loss: 0.00001608
Iteration 178/1000 | Loss: 0.00001608
Iteration 179/1000 | Loss: 0.00001608
Iteration 180/1000 | Loss: 0.00001608
Iteration 181/1000 | Loss: 0.00001608
Iteration 182/1000 | Loss: 0.00001608
Iteration 183/1000 | Loss: 0.00001608
Iteration 184/1000 | Loss: 0.00001608
Iteration 185/1000 | Loss: 0.00001608
Iteration 186/1000 | Loss: 0.00001608
Iteration 187/1000 | Loss: 0.00001607
Iteration 188/1000 | Loss: 0.00001607
Iteration 189/1000 | Loss: 0.00001607
Iteration 190/1000 | Loss: 0.00001607
Iteration 191/1000 | Loss: 0.00001607
Iteration 192/1000 | Loss: 0.00001607
Iteration 193/1000 | Loss: 0.00001607
Iteration 194/1000 | Loss: 0.00001607
Iteration 195/1000 | Loss: 0.00001607
Iteration 196/1000 | Loss: 0.00001607
Iteration 197/1000 | Loss: 0.00001607
Iteration 198/1000 | Loss: 0.00001607
Iteration 199/1000 | Loss: 0.00001607
Iteration 200/1000 | Loss: 0.00001607
Iteration 201/1000 | Loss: 0.00001607
Iteration 202/1000 | Loss: 0.00001606
Iteration 203/1000 | Loss: 0.00001606
Iteration 204/1000 | Loss: 0.00001606
Iteration 205/1000 | Loss: 0.00001606
Iteration 206/1000 | Loss: 0.00001606
Iteration 207/1000 | Loss: 0.00001606
Iteration 208/1000 | Loss: 0.00001606
Iteration 209/1000 | Loss: 0.00001606
Iteration 210/1000 | Loss: 0.00001606
Iteration 211/1000 | Loss: 0.00001606
Iteration 212/1000 | Loss: 0.00001606
Iteration 213/1000 | Loss: 0.00001606
Iteration 214/1000 | Loss: 0.00001606
Iteration 215/1000 | Loss: 0.00001606
Iteration 216/1000 | Loss: 0.00001606
Iteration 217/1000 | Loss: 0.00001606
Iteration 218/1000 | Loss: 0.00001606
Iteration 219/1000 | Loss: 0.00001606
Iteration 220/1000 | Loss: 0.00001606
Iteration 221/1000 | Loss: 0.00001606
Iteration 222/1000 | Loss: 0.00001606
Iteration 223/1000 | Loss: 0.00001606
Iteration 224/1000 | Loss: 0.00001606
Iteration 225/1000 | Loss: 0.00001606
Iteration 226/1000 | Loss: 0.00001606
Iteration 227/1000 | Loss: 0.00001606
Iteration 228/1000 | Loss: 0.00001606
Iteration 229/1000 | Loss: 0.00001606
Iteration 230/1000 | Loss: 0.00001606
Iteration 231/1000 | Loss: 0.00001606
Iteration 232/1000 | Loss: 0.00001606
Iteration 233/1000 | Loss: 0.00001606
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 233. Stopping optimization.
Last 5 losses: [1.6058167602750473e-05, 1.6058167602750473e-05, 1.6058167602750473e-05, 1.6058167602750473e-05, 1.6058167602750473e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6058167602750473e-05

Optimization complete. Final v2v error: 3.334406614303589 mm

Highest mean error: 4.050703048706055 mm for frame 33

Lowest mean error: 2.6218960285186768 mm for frame 163

Saving results

Total time: 43.579102754592896
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00871612
Iteration 2/25 | Loss: 0.00154353
Iteration 3/25 | Loss: 0.00131104
Iteration 4/25 | Loss: 0.00128979
Iteration 5/25 | Loss: 0.00128587
Iteration 6/25 | Loss: 0.00128523
Iteration 7/25 | Loss: 0.00128523
Iteration 8/25 | Loss: 0.00128523
Iteration 9/25 | Loss: 0.00128523
Iteration 10/25 | Loss: 0.00128523
Iteration 11/25 | Loss: 0.00128523
Iteration 12/25 | Loss: 0.00128523
Iteration 13/25 | Loss: 0.00128523
Iteration 14/25 | Loss: 0.00128523
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001285230158828199, 0.001285230158828199, 0.001285230158828199, 0.001285230158828199, 0.001285230158828199]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001285230158828199

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.97298044
Iteration 2/25 | Loss: 0.00054877
Iteration 3/25 | Loss: 0.00054877
Iteration 4/25 | Loss: 0.00054877
Iteration 5/25 | Loss: 0.00054877
Iteration 6/25 | Loss: 0.00054877
Iteration 7/25 | Loss: 0.00054877
Iteration 8/25 | Loss: 0.00054877
Iteration 9/25 | Loss: 0.00054877
Iteration 10/25 | Loss: 0.00054877
Iteration 11/25 | Loss: 0.00054877
Iteration 12/25 | Loss: 0.00054877
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0005487677408382297, 0.0005487677408382297, 0.0005487677408382297, 0.0005487677408382297, 0.0005487677408382297]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005487677408382297

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054877
Iteration 2/1000 | Loss: 0.00004504
Iteration 3/1000 | Loss: 0.00003577
Iteration 4/1000 | Loss: 0.00003278
Iteration 5/1000 | Loss: 0.00003181
Iteration 6/1000 | Loss: 0.00003064
Iteration 7/1000 | Loss: 0.00002983
Iteration 8/1000 | Loss: 0.00002950
Iteration 9/1000 | Loss: 0.00002910
Iteration 10/1000 | Loss: 0.00002874
Iteration 11/1000 | Loss: 0.00002846
Iteration 12/1000 | Loss: 0.00002822
Iteration 13/1000 | Loss: 0.00002800
Iteration 14/1000 | Loss: 0.00002787
Iteration 15/1000 | Loss: 0.00002787
Iteration 16/1000 | Loss: 0.00002785
Iteration 17/1000 | Loss: 0.00002781
Iteration 18/1000 | Loss: 0.00002781
Iteration 19/1000 | Loss: 0.00002781
Iteration 20/1000 | Loss: 0.00002781
Iteration 21/1000 | Loss: 0.00002781
Iteration 22/1000 | Loss: 0.00002780
Iteration 23/1000 | Loss: 0.00002780
Iteration 24/1000 | Loss: 0.00002780
Iteration 25/1000 | Loss: 0.00002780
Iteration 26/1000 | Loss: 0.00002780
Iteration 27/1000 | Loss: 0.00002780
Iteration 28/1000 | Loss: 0.00002780
Iteration 29/1000 | Loss: 0.00002780
Iteration 30/1000 | Loss: 0.00002780
Iteration 31/1000 | Loss: 0.00002780
Iteration 32/1000 | Loss: 0.00002780
Iteration 33/1000 | Loss: 0.00002780
Iteration 34/1000 | Loss: 0.00002780
Iteration 35/1000 | Loss: 0.00002780
Iteration 36/1000 | Loss: 0.00002780
Iteration 37/1000 | Loss: 0.00002779
Iteration 38/1000 | Loss: 0.00002779
Iteration 39/1000 | Loss: 0.00002779
Iteration 40/1000 | Loss: 0.00002779
Iteration 41/1000 | Loss: 0.00002779
Iteration 42/1000 | Loss: 0.00002779
Iteration 43/1000 | Loss: 0.00002779
Iteration 44/1000 | Loss: 0.00002779
Iteration 45/1000 | Loss: 0.00002779
Iteration 46/1000 | Loss: 0.00002779
Iteration 47/1000 | Loss: 0.00002779
Iteration 48/1000 | Loss: 0.00002778
Iteration 49/1000 | Loss: 0.00002778
Iteration 50/1000 | Loss: 0.00002778
Iteration 51/1000 | Loss: 0.00002778
Iteration 52/1000 | Loss: 0.00002778
Iteration 53/1000 | Loss: 0.00002777
Iteration 54/1000 | Loss: 0.00002777
Iteration 55/1000 | Loss: 0.00002777
Iteration 56/1000 | Loss: 0.00002776
Iteration 57/1000 | Loss: 0.00002776
Iteration 58/1000 | Loss: 0.00002776
Iteration 59/1000 | Loss: 0.00002776
Iteration 60/1000 | Loss: 0.00002776
Iteration 61/1000 | Loss: 0.00002776
Iteration 62/1000 | Loss: 0.00002776
Iteration 63/1000 | Loss: 0.00002775
Iteration 64/1000 | Loss: 0.00002775
Iteration 65/1000 | Loss: 0.00002775
Iteration 66/1000 | Loss: 0.00002775
Iteration 67/1000 | Loss: 0.00002774
Iteration 68/1000 | Loss: 0.00002774
Iteration 69/1000 | Loss: 0.00002774
Iteration 70/1000 | Loss: 0.00002773
Iteration 71/1000 | Loss: 0.00002773
Iteration 72/1000 | Loss: 0.00002773
Iteration 73/1000 | Loss: 0.00002773
Iteration 74/1000 | Loss: 0.00002773
Iteration 75/1000 | Loss: 0.00002773
Iteration 76/1000 | Loss: 0.00002772
Iteration 77/1000 | Loss: 0.00002772
Iteration 78/1000 | Loss: 0.00002772
Iteration 79/1000 | Loss: 0.00002771
Iteration 80/1000 | Loss: 0.00002771
Iteration 81/1000 | Loss: 0.00002771
Iteration 82/1000 | Loss: 0.00002771
Iteration 83/1000 | Loss: 0.00002771
Iteration 84/1000 | Loss: 0.00002771
Iteration 85/1000 | Loss: 0.00002771
Iteration 86/1000 | Loss: 0.00002771
Iteration 87/1000 | Loss: 0.00002771
Iteration 88/1000 | Loss: 0.00002771
Iteration 89/1000 | Loss: 0.00002771
Iteration 90/1000 | Loss: 0.00002771
Iteration 91/1000 | Loss: 0.00002771
Iteration 92/1000 | Loss: 0.00002770
Iteration 93/1000 | Loss: 0.00002770
Iteration 94/1000 | Loss: 0.00002770
Iteration 95/1000 | Loss: 0.00002770
Iteration 96/1000 | Loss: 0.00002770
Iteration 97/1000 | Loss: 0.00002770
Iteration 98/1000 | Loss: 0.00002770
Iteration 99/1000 | Loss: 0.00002770
Iteration 100/1000 | Loss: 0.00002770
Iteration 101/1000 | Loss: 0.00002770
Iteration 102/1000 | Loss: 0.00002770
Iteration 103/1000 | Loss: 0.00002770
Iteration 104/1000 | Loss: 0.00002770
Iteration 105/1000 | Loss: 0.00002770
Iteration 106/1000 | Loss: 0.00002770
Iteration 107/1000 | Loss: 0.00002770
Iteration 108/1000 | Loss: 0.00002770
Iteration 109/1000 | Loss: 0.00002770
Iteration 110/1000 | Loss: 0.00002770
Iteration 111/1000 | Loss: 0.00002770
Iteration 112/1000 | Loss: 0.00002770
Iteration 113/1000 | Loss: 0.00002770
Iteration 114/1000 | Loss: 0.00002770
Iteration 115/1000 | Loss: 0.00002770
Iteration 116/1000 | Loss: 0.00002770
Iteration 117/1000 | Loss: 0.00002770
Iteration 118/1000 | Loss: 0.00002770
Iteration 119/1000 | Loss: 0.00002770
Iteration 120/1000 | Loss: 0.00002770
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [2.7696822144207545e-05, 2.7696822144207545e-05, 2.7696822144207545e-05, 2.7696822144207545e-05, 2.7696822144207545e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7696822144207545e-05

Optimization complete. Final v2v error: 4.442620277404785 mm

Highest mean error: 4.746359348297119 mm for frame 1

Lowest mean error: 4.134174346923828 mm for frame 81

Saving results

Total time: 33.16731572151184
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00531788
Iteration 2/25 | Loss: 0.00130127
Iteration 3/25 | Loss: 0.00123871
Iteration 4/25 | Loss: 0.00122957
Iteration 5/25 | Loss: 0.00122653
Iteration 6/25 | Loss: 0.00122616
Iteration 7/25 | Loss: 0.00122616
Iteration 8/25 | Loss: 0.00122616
Iteration 9/25 | Loss: 0.00122616
Iteration 10/25 | Loss: 0.00122616
Iteration 11/25 | Loss: 0.00122616
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012261559022590518, 0.0012261559022590518, 0.0012261559022590518, 0.0012261559022590518, 0.0012261559022590518]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012261559022590518

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.78992844
Iteration 2/25 | Loss: 0.00074893
Iteration 3/25 | Loss: 0.00074892
Iteration 4/25 | Loss: 0.00074892
Iteration 5/25 | Loss: 0.00074892
Iteration 6/25 | Loss: 0.00074892
Iteration 7/25 | Loss: 0.00074892
Iteration 8/25 | Loss: 0.00074892
Iteration 9/25 | Loss: 0.00074892
Iteration 10/25 | Loss: 0.00074892
Iteration 11/25 | Loss: 0.00074892
Iteration 12/25 | Loss: 0.00074892
Iteration 13/25 | Loss: 0.00074892
Iteration 14/25 | Loss: 0.00074892
Iteration 15/25 | Loss: 0.00074892
Iteration 16/25 | Loss: 0.00074892
Iteration 17/25 | Loss: 0.00074892
Iteration 18/25 | Loss: 0.00074892
Iteration 19/25 | Loss: 0.00074892
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007489183335565031, 0.0007489183335565031, 0.0007489183335565031, 0.0007489183335565031, 0.0007489183335565031]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007489183335565031

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074892
Iteration 2/1000 | Loss: 0.00002696
Iteration 3/1000 | Loss: 0.00002055
Iteration 4/1000 | Loss: 0.00001757
Iteration 5/1000 | Loss: 0.00001648
Iteration 6/1000 | Loss: 0.00001577
Iteration 7/1000 | Loss: 0.00001520
Iteration 8/1000 | Loss: 0.00001486
Iteration 9/1000 | Loss: 0.00001461
Iteration 10/1000 | Loss: 0.00001448
Iteration 11/1000 | Loss: 0.00001435
Iteration 12/1000 | Loss: 0.00001420
Iteration 13/1000 | Loss: 0.00001417
Iteration 14/1000 | Loss: 0.00001404
Iteration 15/1000 | Loss: 0.00001402
Iteration 16/1000 | Loss: 0.00001399
Iteration 17/1000 | Loss: 0.00001398
Iteration 18/1000 | Loss: 0.00001398
Iteration 19/1000 | Loss: 0.00001397
Iteration 20/1000 | Loss: 0.00001397
Iteration 21/1000 | Loss: 0.00001396
Iteration 22/1000 | Loss: 0.00001396
Iteration 23/1000 | Loss: 0.00001395
Iteration 24/1000 | Loss: 0.00001395
Iteration 25/1000 | Loss: 0.00001394
Iteration 26/1000 | Loss: 0.00001393
Iteration 27/1000 | Loss: 0.00001393
Iteration 28/1000 | Loss: 0.00001393
Iteration 29/1000 | Loss: 0.00001393
Iteration 30/1000 | Loss: 0.00001392
Iteration 31/1000 | Loss: 0.00001391
Iteration 32/1000 | Loss: 0.00001391
Iteration 33/1000 | Loss: 0.00001390
Iteration 34/1000 | Loss: 0.00001389
Iteration 35/1000 | Loss: 0.00001388
Iteration 36/1000 | Loss: 0.00001388
Iteration 37/1000 | Loss: 0.00001387
Iteration 38/1000 | Loss: 0.00001387
Iteration 39/1000 | Loss: 0.00001387
Iteration 40/1000 | Loss: 0.00001386
Iteration 41/1000 | Loss: 0.00001386
Iteration 42/1000 | Loss: 0.00001386
Iteration 43/1000 | Loss: 0.00001383
Iteration 44/1000 | Loss: 0.00001383
Iteration 45/1000 | Loss: 0.00001383
Iteration 46/1000 | Loss: 0.00001383
Iteration 47/1000 | Loss: 0.00001382
Iteration 48/1000 | Loss: 0.00001381
Iteration 49/1000 | Loss: 0.00001381
Iteration 50/1000 | Loss: 0.00001380
Iteration 51/1000 | Loss: 0.00001379
Iteration 52/1000 | Loss: 0.00001379
Iteration 53/1000 | Loss: 0.00001379
Iteration 54/1000 | Loss: 0.00001378
Iteration 55/1000 | Loss: 0.00001378
Iteration 56/1000 | Loss: 0.00001376
Iteration 57/1000 | Loss: 0.00001375
Iteration 58/1000 | Loss: 0.00001373
Iteration 59/1000 | Loss: 0.00001372
Iteration 60/1000 | Loss: 0.00001371
Iteration 61/1000 | Loss: 0.00001371
Iteration 62/1000 | Loss: 0.00001370
Iteration 63/1000 | Loss: 0.00001370
Iteration 64/1000 | Loss: 0.00001368
Iteration 65/1000 | Loss: 0.00001368
Iteration 66/1000 | Loss: 0.00001368
Iteration 67/1000 | Loss: 0.00001366
Iteration 68/1000 | Loss: 0.00001366
Iteration 69/1000 | Loss: 0.00001365
Iteration 70/1000 | Loss: 0.00001361
Iteration 71/1000 | Loss: 0.00001360
Iteration 72/1000 | Loss: 0.00001360
Iteration 73/1000 | Loss: 0.00001359
Iteration 74/1000 | Loss: 0.00001358
Iteration 75/1000 | Loss: 0.00001357
Iteration 76/1000 | Loss: 0.00001357
Iteration 77/1000 | Loss: 0.00001357
Iteration 78/1000 | Loss: 0.00001356
Iteration 79/1000 | Loss: 0.00001356
Iteration 80/1000 | Loss: 0.00001356
Iteration 81/1000 | Loss: 0.00001356
Iteration 82/1000 | Loss: 0.00001355
Iteration 83/1000 | Loss: 0.00001355
Iteration 84/1000 | Loss: 0.00001354
Iteration 85/1000 | Loss: 0.00001354
Iteration 86/1000 | Loss: 0.00001354
Iteration 87/1000 | Loss: 0.00001354
Iteration 88/1000 | Loss: 0.00001354
Iteration 89/1000 | Loss: 0.00001353
Iteration 90/1000 | Loss: 0.00001353
Iteration 91/1000 | Loss: 0.00001353
Iteration 92/1000 | Loss: 0.00001352
Iteration 93/1000 | Loss: 0.00001352
Iteration 94/1000 | Loss: 0.00001351
Iteration 95/1000 | Loss: 0.00001350
Iteration 96/1000 | Loss: 0.00001350
Iteration 97/1000 | Loss: 0.00001350
Iteration 98/1000 | Loss: 0.00001350
Iteration 99/1000 | Loss: 0.00001349
Iteration 100/1000 | Loss: 0.00001349
Iteration 101/1000 | Loss: 0.00001349
Iteration 102/1000 | Loss: 0.00001349
Iteration 103/1000 | Loss: 0.00001348
Iteration 104/1000 | Loss: 0.00001348
Iteration 105/1000 | Loss: 0.00001348
Iteration 106/1000 | Loss: 0.00001348
Iteration 107/1000 | Loss: 0.00001347
Iteration 108/1000 | Loss: 0.00001347
Iteration 109/1000 | Loss: 0.00001347
Iteration 110/1000 | Loss: 0.00001347
Iteration 111/1000 | Loss: 0.00001347
Iteration 112/1000 | Loss: 0.00001347
Iteration 113/1000 | Loss: 0.00001347
Iteration 114/1000 | Loss: 0.00001346
Iteration 115/1000 | Loss: 0.00001346
Iteration 116/1000 | Loss: 0.00001345
Iteration 117/1000 | Loss: 0.00001345
Iteration 118/1000 | Loss: 0.00001345
Iteration 119/1000 | Loss: 0.00001345
Iteration 120/1000 | Loss: 0.00001345
Iteration 121/1000 | Loss: 0.00001345
Iteration 122/1000 | Loss: 0.00001345
Iteration 123/1000 | Loss: 0.00001345
Iteration 124/1000 | Loss: 0.00001345
Iteration 125/1000 | Loss: 0.00001345
Iteration 126/1000 | Loss: 0.00001345
Iteration 127/1000 | Loss: 0.00001345
Iteration 128/1000 | Loss: 0.00001345
Iteration 129/1000 | Loss: 0.00001345
Iteration 130/1000 | Loss: 0.00001345
Iteration 131/1000 | Loss: 0.00001344
Iteration 132/1000 | Loss: 0.00001344
Iteration 133/1000 | Loss: 0.00001344
Iteration 134/1000 | Loss: 0.00001344
Iteration 135/1000 | Loss: 0.00001344
Iteration 136/1000 | Loss: 0.00001344
Iteration 137/1000 | Loss: 0.00001343
Iteration 138/1000 | Loss: 0.00001343
Iteration 139/1000 | Loss: 0.00001343
Iteration 140/1000 | Loss: 0.00001343
Iteration 141/1000 | Loss: 0.00001343
Iteration 142/1000 | Loss: 0.00001343
Iteration 143/1000 | Loss: 0.00001343
Iteration 144/1000 | Loss: 0.00001343
Iteration 145/1000 | Loss: 0.00001343
Iteration 146/1000 | Loss: 0.00001342
Iteration 147/1000 | Loss: 0.00001342
Iteration 148/1000 | Loss: 0.00001342
Iteration 149/1000 | Loss: 0.00001342
Iteration 150/1000 | Loss: 0.00001342
Iteration 151/1000 | Loss: 0.00001342
Iteration 152/1000 | Loss: 0.00001341
Iteration 153/1000 | Loss: 0.00001341
Iteration 154/1000 | Loss: 0.00001341
Iteration 155/1000 | Loss: 0.00001341
Iteration 156/1000 | Loss: 0.00001341
Iteration 157/1000 | Loss: 0.00001341
Iteration 158/1000 | Loss: 0.00001341
Iteration 159/1000 | Loss: 0.00001341
Iteration 160/1000 | Loss: 0.00001341
Iteration 161/1000 | Loss: 0.00001341
Iteration 162/1000 | Loss: 0.00001341
Iteration 163/1000 | Loss: 0.00001341
Iteration 164/1000 | Loss: 0.00001340
Iteration 165/1000 | Loss: 0.00001340
Iteration 166/1000 | Loss: 0.00001340
Iteration 167/1000 | Loss: 0.00001340
Iteration 168/1000 | Loss: 0.00001340
Iteration 169/1000 | Loss: 0.00001340
Iteration 170/1000 | Loss: 0.00001340
Iteration 171/1000 | Loss: 0.00001340
Iteration 172/1000 | Loss: 0.00001340
Iteration 173/1000 | Loss: 0.00001340
Iteration 174/1000 | Loss: 0.00001340
Iteration 175/1000 | Loss: 0.00001340
Iteration 176/1000 | Loss: 0.00001340
Iteration 177/1000 | Loss: 0.00001339
Iteration 178/1000 | Loss: 0.00001339
Iteration 179/1000 | Loss: 0.00001339
Iteration 180/1000 | Loss: 0.00001339
Iteration 181/1000 | Loss: 0.00001339
Iteration 182/1000 | Loss: 0.00001339
Iteration 183/1000 | Loss: 0.00001339
Iteration 184/1000 | Loss: 0.00001339
Iteration 185/1000 | Loss: 0.00001339
Iteration 186/1000 | Loss: 0.00001339
Iteration 187/1000 | Loss: 0.00001339
Iteration 188/1000 | Loss: 0.00001339
Iteration 189/1000 | Loss: 0.00001339
Iteration 190/1000 | Loss: 0.00001339
Iteration 191/1000 | Loss: 0.00001339
Iteration 192/1000 | Loss: 0.00001339
Iteration 193/1000 | Loss: 0.00001339
Iteration 194/1000 | Loss: 0.00001339
Iteration 195/1000 | Loss: 0.00001339
Iteration 196/1000 | Loss: 0.00001339
Iteration 197/1000 | Loss: 0.00001339
Iteration 198/1000 | Loss: 0.00001339
Iteration 199/1000 | Loss: 0.00001339
Iteration 200/1000 | Loss: 0.00001339
Iteration 201/1000 | Loss: 0.00001339
Iteration 202/1000 | Loss: 0.00001339
Iteration 203/1000 | Loss: 0.00001339
Iteration 204/1000 | Loss: 0.00001339
Iteration 205/1000 | Loss: 0.00001339
Iteration 206/1000 | Loss: 0.00001339
Iteration 207/1000 | Loss: 0.00001339
Iteration 208/1000 | Loss: 0.00001339
Iteration 209/1000 | Loss: 0.00001339
Iteration 210/1000 | Loss: 0.00001339
Iteration 211/1000 | Loss: 0.00001339
Iteration 212/1000 | Loss: 0.00001339
Iteration 213/1000 | Loss: 0.00001339
Iteration 214/1000 | Loss: 0.00001339
Iteration 215/1000 | Loss: 0.00001339
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.3385391866904683e-05, 1.3385391866904683e-05, 1.3385391866904683e-05, 1.3385391866904683e-05, 1.3385391866904683e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3385391866904683e-05

Optimization complete. Final v2v error: 3.1093192100524902 mm

Highest mean error: 3.8407833576202393 mm for frame 70

Lowest mean error: 2.785611629486084 mm for frame 28

Saving results

Total time: 41.45474433898926
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00567961
Iteration 2/25 | Loss: 0.00128196
Iteration 3/25 | Loss: 0.00122153
Iteration 4/25 | Loss: 0.00121117
Iteration 5/25 | Loss: 0.00120666
Iteration 6/25 | Loss: 0.00120598
Iteration 7/25 | Loss: 0.00120598
Iteration 8/25 | Loss: 0.00120598
Iteration 9/25 | Loss: 0.00120598
Iteration 10/25 | Loss: 0.00120598
Iteration 11/25 | Loss: 0.00120598
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012059812434017658, 0.0012059812434017658, 0.0012059812434017658, 0.0012059812434017658, 0.0012059812434017658]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012059812434017658

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.77658510
Iteration 2/25 | Loss: 0.00079900
Iteration 3/25 | Loss: 0.00079900
Iteration 4/25 | Loss: 0.00079899
Iteration 5/25 | Loss: 0.00079899
Iteration 6/25 | Loss: 0.00079899
Iteration 7/25 | Loss: 0.00079899
Iteration 8/25 | Loss: 0.00079899
Iteration 9/25 | Loss: 0.00079899
Iteration 10/25 | Loss: 0.00079899
Iteration 11/25 | Loss: 0.00079899
Iteration 12/25 | Loss: 0.00079899
Iteration 13/25 | Loss: 0.00079899
Iteration 14/25 | Loss: 0.00079899
Iteration 15/25 | Loss: 0.00079899
Iteration 16/25 | Loss: 0.00079899
Iteration 17/25 | Loss: 0.00079899
Iteration 18/25 | Loss: 0.00079899
Iteration 19/25 | Loss: 0.00079899
Iteration 20/25 | Loss: 0.00079899
Iteration 21/25 | Loss: 0.00079899
Iteration 22/25 | Loss: 0.00079899
Iteration 23/25 | Loss: 0.00079899
Iteration 24/25 | Loss: 0.00079899
Iteration 25/25 | Loss: 0.00079899

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079899
Iteration 2/1000 | Loss: 0.00002409
Iteration 3/1000 | Loss: 0.00001711
Iteration 4/1000 | Loss: 0.00001497
Iteration 5/1000 | Loss: 0.00001419
Iteration 6/1000 | Loss: 0.00001362
Iteration 7/1000 | Loss: 0.00001321
Iteration 8/1000 | Loss: 0.00001290
Iteration 9/1000 | Loss: 0.00001269
Iteration 10/1000 | Loss: 0.00001249
Iteration 11/1000 | Loss: 0.00001238
Iteration 12/1000 | Loss: 0.00001234
Iteration 13/1000 | Loss: 0.00001230
Iteration 14/1000 | Loss: 0.00001221
Iteration 15/1000 | Loss: 0.00001218
Iteration 16/1000 | Loss: 0.00001216
Iteration 17/1000 | Loss: 0.00001216
Iteration 18/1000 | Loss: 0.00001213
Iteration 19/1000 | Loss: 0.00001207
Iteration 20/1000 | Loss: 0.00001205
Iteration 21/1000 | Loss: 0.00001205
Iteration 22/1000 | Loss: 0.00001204
Iteration 23/1000 | Loss: 0.00001203
Iteration 24/1000 | Loss: 0.00001203
Iteration 25/1000 | Loss: 0.00001203
Iteration 26/1000 | Loss: 0.00001203
Iteration 27/1000 | Loss: 0.00001203
Iteration 28/1000 | Loss: 0.00001203
Iteration 29/1000 | Loss: 0.00001203
Iteration 30/1000 | Loss: 0.00001203
Iteration 31/1000 | Loss: 0.00001202
Iteration 32/1000 | Loss: 0.00001202
Iteration 33/1000 | Loss: 0.00001201
Iteration 34/1000 | Loss: 0.00001200
Iteration 35/1000 | Loss: 0.00001200
Iteration 36/1000 | Loss: 0.00001200
Iteration 37/1000 | Loss: 0.00001200
Iteration 38/1000 | Loss: 0.00001200
Iteration 39/1000 | Loss: 0.00001200
Iteration 40/1000 | Loss: 0.00001200
Iteration 41/1000 | Loss: 0.00001199
Iteration 42/1000 | Loss: 0.00001199
Iteration 43/1000 | Loss: 0.00001199
Iteration 44/1000 | Loss: 0.00001199
Iteration 45/1000 | Loss: 0.00001198
Iteration 46/1000 | Loss: 0.00001197
Iteration 47/1000 | Loss: 0.00001197
Iteration 48/1000 | Loss: 0.00001196
Iteration 49/1000 | Loss: 0.00001196
Iteration 50/1000 | Loss: 0.00001195
Iteration 51/1000 | Loss: 0.00001195
Iteration 52/1000 | Loss: 0.00001195
Iteration 53/1000 | Loss: 0.00001194
Iteration 54/1000 | Loss: 0.00001193
Iteration 55/1000 | Loss: 0.00001191
Iteration 56/1000 | Loss: 0.00001191
Iteration 57/1000 | Loss: 0.00001190
Iteration 58/1000 | Loss: 0.00001190
Iteration 59/1000 | Loss: 0.00001189
Iteration 60/1000 | Loss: 0.00001189
Iteration 61/1000 | Loss: 0.00001189
Iteration 62/1000 | Loss: 0.00001189
Iteration 63/1000 | Loss: 0.00001189
Iteration 64/1000 | Loss: 0.00001189
Iteration 65/1000 | Loss: 0.00001189
Iteration 66/1000 | Loss: 0.00001189
Iteration 67/1000 | Loss: 0.00001188
Iteration 68/1000 | Loss: 0.00001187
Iteration 69/1000 | Loss: 0.00001187
Iteration 70/1000 | Loss: 0.00001186
Iteration 71/1000 | Loss: 0.00001186
Iteration 72/1000 | Loss: 0.00001185
Iteration 73/1000 | Loss: 0.00001185
Iteration 74/1000 | Loss: 0.00001185
Iteration 75/1000 | Loss: 0.00001184
Iteration 76/1000 | Loss: 0.00001183
Iteration 77/1000 | Loss: 0.00001183
Iteration 78/1000 | Loss: 0.00001183
Iteration 79/1000 | Loss: 0.00001183
Iteration 80/1000 | Loss: 0.00001182
Iteration 81/1000 | Loss: 0.00001182
Iteration 82/1000 | Loss: 0.00001182
Iteration 83/1000 | Loss: 0.00001182
Iteration 84/1000 | Loss: 0.00001182
Iteration 85/1000 | Loss: 0.00001181
Iteration 86/1000 | Loss: 0.00001181
Iteration 87/1000 | Loss: 0.00001181
Iteration 88/1000 | Loss: 0.00001179
Iteration 89/1000 | Loss: 0.00001179
Iteration 90/1000 | Loss: 0.00001179
Iteration 91/1000 | Loss: 0.00001179
Iteration 92/1000 | Loss: 0.00001178
Iteration 93/1000 | Loss: 0.00001178
Iteration 94/1000 | Loss: 0.00001177
Iteration 95/1000 | Loss: 0.00001177
Iteration 96/1000 | Loss: 0.00001177
Iteration 97/1000 | Loss: 0.00001177
Iteration 98/1000 | Loss: 0.00001177
Iteration 99/1000 | Loss: 0.00001176
Iteration 100/1000 | Loss: 0.00001176
Iteration 101/1000 | Loss: 0.00001176
Iteration 102/1000 | Loss: 0.00001175
Iteration 103/1000 | Loss: 0.00001175
Iteration 104/1000 | Loss: 0.00001175
Iteration 105/1000 | Loss: 0.00001175
Iteration 106/1000 | Loss: 0.00001175
Iteration 107/1000 | Loss: 0.00001175
Iteration 108/1000 | Loss: 0.00001175
Iteration 109/1000 | Loss: 0.00001175
Iteration 110/1000 | Loss: 0.00001175
Iteration 111/1000 | Loss: 0.00001175
Iteration 112/1000 | Loss: 0.00001174
Iteration 113/1000 | Loss: 0.00001174
Iteration 114/1000 | Loss: 0.00001174
Iteration 115/1000 | Loss: 0.00001174
Iteration 116/1000 | Loss: 0.00001174
Iteration 117/1000 | Loss: 0.00001174
Iteration 118/1000 | Loss: 0.00001174
Iteration 119/1000 | Loss: 0.00001173
Iteration 120/1000 | Loss: 0.00001173
Iteration 121/1000 | Loss: 0.00001173
Iteration 122/1000 | Loss: 0.00001173
Iteration 123/1000 | Loss: 0.00001173
Iteration 124/1000 | Loss: 0.00001172
Iteration 125/1000 | Loss: 0.00001172
Iteration 126/1000 | Loss: 0.00001172
Iteration 127/1000 | Loss: 0.00001172
Iteration 128/1000 | Loss: 0.00001171
Iteration 129/1000 | Loss: 0.00001171
Iteration 130/1000 | Loss: 0.00001171
Iteration 131/1000 | Loss: 0.00001171
Iteration 132/1000 | Loss: 0.00001171
Iteration 133/1000 | Loss: 0.00001171
Iteration 134/1000 | Loss: 0.00001171
Iteration 135/1000 | Loss: 0.00001171
Iteration 136/1000 | Loss: 0.00001171
Iteration 137/1000 | Loss: 0.00001171
Iteration 138/1000 | Loss: 0.00001171
Iteration 139/1000 | Loss: 0.00001171
Iteration 140/1000 | Loss: 0.00001171
Iteration 141/1000 | Loss: 0.00001171
Iteration 142/1000 | Loss: 0.00001171
Iteration 143/1000 | Loss: 0.00001171
Iteration 144/1000 | Loss: 0.00001171
Iteration 145/1000 | Loss: 0.00001171
Iteration 146/1000 | Loss: 0.00001171
Iteration 147/1000 | Loss: 0.00001171
Iteration 148/1000 | Loss: 0.00001171
Iteration 149/1000 | Loss: 0.00001171
Iteration 150/1000 | Loss: 0.00001171
Iteration 151/1000 | Loss: 0.00001171
Iteration 152/1000 | Loss: 0.00001171
Iteration 153/1000 | Loss: 0.00001171
Iteration 154/1000 | Loss: 0.00001171
Iteration 155/1000 | Loss: 0.00001171
Iteration 156/1000 | Loss: 0.00001171
Iteration 157/1000 | Loss: 0.00001171
Iteration 158/1000 | Loss: 0.00001171
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [1.1706097211572342e-05, 1.1706097211572342e-05, 1.1706097211572342e-05, 1.1706097211572342e-05, 1.1706097211572342e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1706097211572342e-05

Optimization complete. Final v2v error: 2.9302053451538086 mm

Highest mean error: 3.202216148376465 mm for frame 59

Lowest mean error: 2.7753539085388184 mm for frame 123

Saving results

Total time: 36.694308042526245
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00351825
Iteration 2/25 | Loss: 0.00135702
Iteration 3/25 | Loss: 0.00122347
Iteration 4/25 | Loss: 0.00120337
Iteration 5/25 | Loss: 0.00119653
Iteration 6/25 | Loss: 0.00119493
Iteration 7/25 | Loss: 0.00119484
Iteration 8/25 | Loss: 0.00119484
Iteration 9/25 | Loss: 0.00119484
Iteration 10/25 | Loss: 0.00119484
Iteration 11/25 | Loss: 0.00119484
Iteration 12/25 | Loss: 0.00119484
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001194841112010181, 0.001194841112010181, 0.001194841112010181, 0.001194841112010181, 0.001194841112010181]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001194841112010181

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51416326
Iteration 2/25 | Loss: 0.00071417
Iteration 3/25 | Loss: 0.00071417
Iteration 4/25 | Loss: 0.00071417
Iteration 5/25 | Loss: 0.00071417
Iteration 6/25 | Loss: 0.00071417
Iteration 7/25 | Loss: 0.00071417
Iteration 8/25 | Loss: 0.00071417
Iteration 9/25 | Loss: 0.00071417
Iteration 10/25 | Loss: 0.00071416
Iteration 11/25 | Loss: 0.00071416
Iteration 12/25 | Loss: 0.00071416
Iteration 13/25 | Loss: 0.00071416
Iteration 14/25 | Loss: 0.00071416
Iteration 15/25 | Loss: 0.00071416
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0007141641690395772, 0.0007141641690395772, 0.0007141641690395772, 0.0007141641690395772, 0.0007141641690395772]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007141641690395772

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071416
Iteration 2/1000 | Loss: 0.00004700
Iteration 3/1000 | Loss: 0.00002690
Iteration 4/1000 | Loss: 0.00002043
Iteration 5/1000 | Loss: 0.00001846
Iteration 6/1000 | Loss: 0.00001742
Iteration 7/1000 | Loss: 0.00001648
Iteration 8/1000 | Loss: 0.00001592
Iteration 9/1000 | Loss: 0.00001537
Iteration 10/1000 | Loss: 0.00001500
Iteration 11/1000 | Loss: 0.00001470
Iteration 12/1000 | Loss: 0.00001461
Iteration 13/1000 | Loss: 0.00001455
Iteration 14/1000 | Loss: 0.00001447
Iteration 15/1000 | Loss: 0.00001441
Iteration 16/1000 | Loss: 0.00001440
Iteration 17/1000 | Loss: 0.00001438
Iteration 18/1000 | Loss: 0.00001432
Iteration 19/1000 | Loss: 0.00001424
Iteration 20/1000 | Loss: 0.00001419
Iteration 21/1000 | Loss: 0.00001416
Iteration 22/1000 | Loss: 0.00001412
Iteration 23/1000 | Loss: 0.00001412
Iteration 24/1000 | Loss: 0.00001411
Iteration 25/1000 | Loss: 0.00001411
Iteration 26/1000 | Loss: 0.00001410
Iteration 27/1000 | Loss: 0.00001407
Iteration 28/1000 | Loss: 0.00001404
Iteration 29/1000 | Loss: 0.00001404
Iteration 30/1000 | Loss: 0.00001403
Iteration 31/1000 | Loss: 0.00001403
Iteration 32/1000 | Loss: 0.00001398
Iteration 33/1000 | Loss: 0.00001397
Iteration 34/1000 | Loss: 0.00001397
Iteration 35/1000 | Loss: 0.00001396
Iteration 36/1000 | Loss: 0.00001396
Iteration 37/1000 | Loss: 0.00001395
Iteration 38/1000 | Loss: 0.00001394
Iteration 39/1000 | Loss: 0.00001394
Iteration 40/1000 | Loss: 0.00001394
Iteration 41/1000 | Loss: 0.00001394
Iteration 42/1000 | Loss: 0.00001394
Iteration 43/1000 | Loss: 0.00001394
Iteration 44/1000 | Loss: 0.00001393
Iteration 45/1000 | Loss: 0.00001393
Iteration 46/1000 | Loss: 0.00001393
Iteration 47/1000 | Loss: 0.00001392
Iteration 48/1000 | Loss: 0.00001392
Iteration 49/1000 | Loss: 0.00001391
Iteration 50/1000 | Loss: 0.00001391
Iteration 51/1000 | Loss: 0.00001391
Iteration 52/1000 | Loss: 0.00001390
Iteration 53/1000 | Loss: 0.00001390
Iteration 54/1000 | Loss: 0.00001390
Iteration 55/1000 | Loss: 0.00001390
Iteration 56/1000 | Loss: 0.00001390
Iteration 57/1000 | Loss: 0.00001389
Iteration 58/1000 | Loss: 0.00001389
Iteration 59/1000 | Loss: 0.00001389
Iteration 60/1000 | Loss: 0.00001388
Iteration 61/1000 | Loss: 0.00001388
Iteration 62/1000 | Loss: 0.00001388
Iteration 63/1000 | Loss: 0.00001388
Iteration 64/1000 | Loss: 0.00001388
Iteration 65/1000 | Loss: 0.00001388
Iteration 66/1000 | Loss: 0.00001387
Iteration 67/1000 | Loss: 0.00001387
Iteration 68/1000 | Loss: 0.00001387
Iteration 69/1000 | Loss: 0.00001387
Iteration 70/1000 | Loss: 0.00001387
Iteration 71/1000 | Loss: 0.00001387
Iteration 72/1000 | Loss: 0.00001386
Iteration 73/1000 | Loss: 0.00001386
Iteration 74/1000 | Loss: 0.00001386
Iteration 75/1000 | Loss: 0.00001386
Iteration 76/1000 | Loss: 0.00001385
Iteration 77/1000 | Loss: 0.00001385
Iteration 78/1000 | Loss: 0.00001385
Iteration 79/1000 | Loss: 0.00001384
Iteration 80/1000 | Loss: 0.00001384
Iteration 81/1000 | Loss: 0.00001384
Iteration 82/1000 | Loss: 0.00001384
Iteration 83/1000 | Loss: 0.00001384
Iteration 84/1000 | Loss: 0.00001384
Iteration 85/1000 | Loss: 0.00001383
Iteration 86/1000 | Loss: 0.00001383
Iteration 87/1000 | Loss: 0.00001383
Iteration 88/1000 | Loss: 0.00001383
Iteration 89/1000 | Loss: 0.00001383
Iteration 90/1000 | Loss: 0.00001382
Iteration 91/1000 | Loss: 0.00001382
Iteration 92/1000 | Loss: 0.00001382
Iteration 93/1000 | Loss: 0.00001382
Iteration 94/1000 | Loss: 0.00001381
Iteration 95/1000 | Loss: 0.00001381
Iteration 96/1000 | Loss: 0.00001381
Iteration 97/1000 | Loss: 0.00001381
Iteration 98/1000 | Loss: 0.00001380
Iteration 99/1000 | Loss: 0.00001380
Iteration 100/1000 | Loss: 0.00001380
Iteration 101/1000 | Loss: 0.00001380
Iteration 102/1000 | Loss: 0.00001380
Iteration 103/1000 | Loss: 0.00001380
Iteration 104/1000 | Loss: 0.00001379
Iteration 105/1000 | Loss: 0.00001379
Iteration 106/1000 | Loss: 0.00001379
Iteration 107/1000 | Loss: 0.00001379
Iteration 108/1000 | Loss: 0.00001379
Iteration 109/1000 | Loss: 0.00001378
Iteration 110/1000 | Loss: 0.00001378
Iteration 111/1000 | Loss: 0.00001378
Iteration 112/1000 | Loss: 0.00001378
Iteration 113/1000 | Loss: 0.00001378
Iteration 114/1000 | Loss: 0.00001378
Iteration 115/1000 | Loss: 0.00001378
Iteration 116/1000 | Loss: 0.00001378
Iteration 117/1000 | Loss: 0.00001377
Iteration 118/1000 | Loss: 0.00001377
Iteration 119/1000 | Loss: 0.00001377
Iteration 120/1000 | Loss: 0.00001377
Iteration 121/1000 | Loss: 0.00001377
Iteration 122/1000 | Loss: 0.00001377
Iteration 123/1000 | Loss: 0.00001376
Iteration 124/1000 | Loss: 0.00001376
Iteration 125/1000 | Loss: 0.00001376
Iteration 126/1000 | Loss: 0.00001376
Iteration 127/1000 | Loss: 0.00001376
Iteration 128/1000 | Loss: 0.00001376
Iteration 129/1000 | Loss: 0.00001376
Iteration 130/1000 | Loss: 0.00001376
Iteration 131/1000 | Loss: 0.00001376
Iteration 132/1000 | Loss: 0.00001376
Iteration 133/1000 | Loss: 0.00001376
Iteration 134/1000 | Loss: 0.00001376
Iteration 135/1000 | Loss: 0.00001375
Iteration 136/1000 | Loss: 0.00001375
Iteration 137/1000 | Loss: 0.00001375
Iteration 138/1000 | Loss: 0.00001375
Iteration 139/1000 | Loss: 0.00001375
Iteration 140/1000 | Loss: 0.00001375
Iteration 141/1000 | Loss: 0.00001375
Iteration 142/1000 | Loss: 0.00001375
Iteration 143/1000 | Loss: 0.00001375
Iteration 144/1000 | Loss: 0.00001375
Iteration 145/1000 | Loss: 0.00001375
Iteration 146/1000 | Loss: 0.00001375
Iteration 147/1000 | Loss: 0.00001375
Iteration 148/1000 | Loss: 0.00001375
Iteration 149/1000 | Loss: 0.00001375
Iteration 150/1000 | Loss: 0.00001375
Iteration 151/1000 | Loss: 0.00001375
Iteration 152/1000 | Loss: 0.00001375
Iteration 153/1000 | Loss: 0.00001375
Iteration 154/1000 | Loss: 0.00001374
Iteration 155/1000 | Loss: 0.00001374
Iteration 156/1000 | Loss: 0.00001374
Iteration 157/1000 | Loss: 0.00001374
Iteration 158/1000 | Loss: 0.00001374
Iteration 159/1000 | Loss: 0.00001374
Iteration 160/1000 | Loss: 0.00001374
Iteration 161/1000 | Loss: 0.00001374
Iteration 162/1000 | Loss: 0.00001374
Iteration 163/1000 | Loss: 0.00001374
Iteration 164/1000 | Loss: 0.00001374
Iteration 165/1000 | Loss: 0.00001374
Iteration 166/1000 | Loss: 0.00001374
Iteration 167/1000 | Loss: 0.00001374
Iteration 168/1000 | Loss: 0.00001374
Iteration 169/1000 | Loss: 0.00001374
Iteration 170/1000 | Loss: 0.00001374
Iteration 171/1000 | Loss: 0.00001374
Iteration 172/1000 | Loss: 0.00001374
Iteration 173/1000 | Loss: 0.00001374
Iteration 174/1000 | Loss: 0.00001374
Iteration 175/1000 | Loss: 0.00001374
Iteration 176/1000 | Loss: 0.00001374
Iteration 177/1000 | Loss: 0.00001374
Iteration 178/1000 | Loss: 0.00001374
Iteration 179/1000 | Loss: 0.00001374
Iteration 180/1000 | Loss: 0.00001374
Iteration 181/1000 | Loss: 0.00001374
Iteration 182/1000 | Loss: 0.00001374
Iteration 183/1000 | Loss: 0.00001374
Iteration 184/1000 | Loss: 0.00001374
Iteration 185/1000 | Loss: 0.00001374
Iteration 186/1000 | Loss: 0.00001374
Iteration 187/1000 | Loss: 0.00001374
Iteration 188/1000 | Loss: 0.00001374
Iteration 189/1000 | Loss: 0.00001374
Iteration 190/1000 | Loss: 0.00001374
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [1.3736613254877739e-05, 1.3736613254877739e-05, 1.3736613254877739e-05, 1.3736613254877739e-05, 1.3736613254877739e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3736613254877739e-05

Optimization complete. Final v2v error: 3.1693191528320312 mm

Highest mean error: 3.3830196857452393 mm for frame 104

Lowest mean error: 3.0032596588134766 mm for frame 20

Saving results

Total time: 40.69112229347229
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00783938
Iteration 2/25 | Loss: 0.00136900
Iteration 3/25 | Loss: 0.00126086
Iteration 4/25 | Loss: 0.00123914
Iteration 5/25 | Loss: 0.00123221
Iteration 6/25 | Loss: 0.00123138
Iteration 7/25 | Loss: 0.00123138
Iteration 8/25 | Loss: 0.00123138
Iteration 9/25 | Loss: 0.00123138
Iteration 10/25 | Loss: 0.00123138
Iteration 11/25 | Loss: 0.00123138
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012313842307776213, 0.0012313842307776213, 0.0012313842307776213, 0.0012313842307776213, 0.0012313842307776213]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012313842307776213

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41048765
Iteration 2/25 | Loss: 0.00076766
Iteration 3/25 | Loss: 0.00076766
Iteration 4/25 | Loss: 0.00076766
Iteration 5/25 | Loss: 0.00076766
Iteration 6/25 | Loss: 0.00076766
Iteration 7/25 | Loss: 0.00076766
Iteration 8/25 | Loss: 0.00076766
Iteration 9/25 | Loss: 0.00076766
Iteration 10/25 | Loss: 0.00076766
Iteration 11/25 | Loss: 0.00076766
Iteration 12/25 | Loss: 0.00076766
Iteration 13/25 | Loss: 0.00076766
Iteration 14/25 | Loss: 0.00076766
Iteration 15/25 | Loss: 0.00076766
Iteration 16/25 | Loss: 0.00076766
Iteration 17/25 | Loss: 0.00076766
Iteration 18/25 | Loss: 0.00076766
Iteration 19/25 | Loss: 0.00076766
Iteration 20/25 | Loss: 0.00076766
Iteration 21/25 | Loss: 0.00076766
Iteration 22/25 | Loss: 0.00076766
Iteration 23/25 | Loss: 0.00076766
Iteration 24/25 | Loss: 0.00076766
Iteration 25/25 | Loss: 0.00076766
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007676617824472487, 0.0007676617824472487, 0.0007676617824472487, 0.0007676617824472487, 0.0007676617824472487]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007676617824472487

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076766
Iteration 2/1000 | Loss: 0.00004045
Iteration 3/1000 | Loss: 0.00002886
Iteration 4/1000 | Loss: 0.00002535
Iteration 5/1000 | Loss: 0.00002410
Iteration 6/1000 | Loss: 0.00002318
Iteration 7/1000 | Loss: 0.00002234
Iteration 8/1000 | Loss: 0.00002185
Iteration 9/1000 | Loss: 0.00002147
Iteration 10/1000 | Loss: 0.00002107
Iteration 11/1000 | Loss: 0.00002070
Iteration 12/1000 | Loss: 0.00002040
Iteration 13/1000 | Loss: 0.00002018
Iteration 14/1000 | Loss: 0.00002015
Iteration 15/1000 | Loss: 0.00002004
Iteration 16/1000 | Loss: 0.00001993
Iteration 17/1000 | Loss: 0.00001989
Iteration 18/1000 | Loss: 0.00001986
Iteration 19/1000 | Loss: 0.00001986
Iteration 20/1000 | Loss: 0.00001985
Iteration 21/1000 | Loss: 0.00001985
Iteration 22/1000 | Loss: 0.00001984
Iteration 23/1000 | Loss: 0.00001982
Iteration 24/1000 | Loss: 0.00001982
Iteration 25/1000 | Loss: 0.00001981
Iteration 26/1000 | Loss: 0.00001981
Iteration 27/1000 | Loss: 0.00001980
Iteration 28/1000 | Loss: 0.00001979
Iteration 29/1000 | Loss: 0.00001979
Iteration 30/1000 | Loss: 0.00001979
Iteration 31/1000 | Loss: 0.00001978
Iteration 32/1000 | Loss: 0.00001978
Iteration 33/1000 | Loss: 0.00001977
Iteration 34/1000 | Loss: 0.00001977
Iteration 35/1000 | Loss: 0.00001976
Iteration 36/1000 | Loss: 0.00001976
Iteration 37/1000 | Loss: 0.00001975
Iteration 38/1000 | Loss: 0.00001974
Iteration 39/1000 | Loss: 0.00001974
Iteration 40/1000 | Loss: 0.00001973
Iteration 41/1000 | Loss: 0.00001973
Iteration 42/1000 | Loss: 0.00001972
Iteration 43/1000 | Loss: 0.00001972
Iteration 44/1000 | Loss: 0.00001972
Iteration 45/1000 | Loss: 0.00001971
Iteration 46/1000 | Loss: 0.00001971
Iteration 47/1000 | Loss: 0.00001971
Iteration 48/1000 | Loss: 0.00001971
Iteration 49/1000 | Loss: 0.00001970
Iteration 50/1000 | Loss: 0.00001970
Iteration 51/1000 | Loss: 0.00001969
Iteration 52/1000 | Loss: 0.00001969
Iteration 53/1000 | Loss: 0.00001968
Iteration 54/1000 | Loss: 0.00001968
Iteration 55/1000 | Loss: 0.00001967
Iteration 56/1000 | Loss: 0.00001967
Iteration 57/1000 | Loss: 0.00001966
Iteration 58/1000 | Loss: 0.00001966
Iteration 59/1000 | Loss: 0.00001966
Iteration 60/1000 | Loss: 0.00001966
Iteration 61/1000 | Loss: 0.00001965
Iteration 62/1000 | Loss: 0.00001965
Iteration 63/1000 | Loss: 0.00001964
Iteration 64/1000 | Loss: 0.00001964
Iteration 65/1000 | Loss: 0.00001964
Iteration 66/1000 | Loss: 0.00001964
Iteration 67/1000 | Loss: 0.00001963
Iteration 68/1000 | Loss: 0.00001963
Iteration 69/1000 | Loss: 0.00001962
Iteration 70/1000 | Loss: 0.00001962
Iteration 71/1000 | Loss: 0.00001962
Iteration 72/1000 | Loss: 0.00001961
Iteration 73/1000 | Loss: 0.00001961
Iteration 74/1000 | Loss: 0.00001961
Iteration 75/1000 | Loss: 0.00001960
Iteration 76/1000 | Loss: 0.00001960
Iteration 77/1000 | Loss: 0.00001960
Iteration 78/1000 | Loss: 0.00001959
Iteration 79/1000 | Loss: 0.00001959
Iteration 80/1000 | Loss: 0.00001959
Iteration 81/1000 | Loss: 0.00001958
Iteration 82/1000 | Loss: 0.00001958
Iteration 83/1000 | Loss: 0.00001958
Iteration 84/1000 | Loss: 0.00001958
Iteration 85/1000 | Loss: 0.00001957
Iteration 86/1000 | Loss: 0.00001957
Iteration 87/1000 | Loss: 0.00001957
Iteration 88/1000 | Loss: 0.00001957
Iteration 89/1000 | Loss: 0.00001957
Iteration 90/1000 | Loss: 0.00001957
Iteration 91/1000 | Loss: 0.00001956
Iteration 92/1000 | Loss: 0.00001956
Iteration 93/1000 | Loss: 0.00001956
Iteration 94/1000 | Loss: 0.00001956
Iteration 95/1000 | Loss: 0.00001956
Iteration 96/1000 | Loss: 0.00001956
Iteration 97/1000 | Loss: 0.00001956
Iteration 98/1000 | Loss: 0.00001955
Iteration 99/1000 | Loss: 0.00001955
Iteration 100/1000 | Loss: 0.00001955
Iteration 101/1000 | Loss: 0.00001955
Iteration 102/1000 | Loss: 0.00001955
Iteration 103/1000 | Loss: 0.00001955
Iteration 104/1000 | Loss: 0.00001955
Iteration 105/1000 | Loss: 0.00001954
Iteration 106/1000 | Loss: 0.00001954
Iteration 107/1000 | Loss: 0.00001954
Iteration 108/1000 | Loss: 0.00001954
Iteration 109/1000 | Loss: 0.00001954
Iteration 110/1000 | Loss: 0.00001953
Iteration 111/1000 | Loss: 0.00001953
Iteration 112/1000 | Loss: 0.00001953
Iteration 113/1000 | Loss: 0.00001953
Iteration 114/1000 | Loss: 0.00001953
Iteration 115/1000 | Loss: 0.00001953
Iteration 116/1000 | Loss: 0.00001953
Iteration 117/1000 | Loss: 0.00001953
Iteration 118/1000 | Loss: 0.00001952
Iteration 119/1000 | Loss: 0.00001952
Iteration 120/1000 | Loss: 0.00001952
Iteration 121/1000 | Loss: 0.00001952
Iteration 122/1000 | Loss: 0.00001952
Iteration 123/1000 | Loss: 0.00001952
Iteration 124/1000 | Loss: 0.00001952
Iteration 125/1000 | Loss: 0.00001951
Iteration 126/1000 | Loss: 0.00001951
Iteration 127/1000 | Loss: 0.00001951
Iteration 128/1000 | Loss: 0.00001951
Iteration 129/1000 | Loss: 0.00001951
Iteration 130/1000 | Loss: 0.00001951
Iteration 131/1000 | Loss: 0.00001951
Iteration 132/1000 | Loss: 0.00001951
Iteration 133/1000 | Loss: 0.00001951
Iteration 134/1000 | Loss: 0.00001951
Iteration 135/1000 | Loss: 0.00001951
Iteration 136/1000 | Loss: 0.00001950
Iteration 137/1000 | Loss: 0.00001950
Iteration 138/1000 | Loss: 0.00001950
Iteration 139/1000 | Loss: 0.00001950
Iteration 140/1000 | Loss: 0.00001950
Iteration 141/1000 | Loss: 0.00001950
Iteration 142/1000 | Loss: 0.00001950
Iteration 143/1000 | Loss: 0.00001950
Iteration 144/1000 | Loss: 0.00001950
Iteration 145/1000 | Loss: 0.00001950
Iteration 146/1000 | Loss: 0.00001950
Iteration 147/1000 | Loss: 0.00001950
Iteration 148/1000 | Loss: 0.00001949
Iteration 149/1000 | Loss: 0.00001949
Iteration 150/1000 | Loss: 0.00001949
Iteration 151/1000 | Loss: 0.00001949
Iteration 152/1000 | Loss: 0.00001949
Iteration 153/1000 | Loss: 0.00001949
Iteration 154/1000 | Loss: 0.00001949
Iteration 155/1000 | Loss: 0.00001949
Iteration 156/1000 | Loss: 0.00001949
Iteration 157/1000 | Loss: 0.00001948
Iteration 158/1000 | Loss: 0.00001948
Iteration 159/1000 | Loss: 0.00001948
Iteration 160/1000 | Loss: 0.00001948
Iteration 161/1000 | Loss: 0.00001948
Iteration 162/1000 | Loss: 0.00001948
Iteration 163/1000 | Loss: 0.00001948
Iteration 164/1000 | Loss: 0.00001948
Iteration 165/1000 | Loss: 0.00001948
Iteration 166/1000 | Loss: 0.00001948
Iteration 167/1000 | Loss: 0.00001948
Iteration 168/1000 | Loss: 0.00001948
Iteration 169/1000 | Loss: 0.00001948
Iteration 170/1000 | Loss: 0.00001948
Iteration 171/1000 | Loss: 0.00001948
Iteration 172/1000 | Loss: 0.00001948
Iteration 173/1000 | Loss: 0.00001948
Iteration 174/1000 | Loss: 0.00001948
Iteration 175/1000 | Loss: 0.00001948
Iteration 176/1000 | Loss: 0.00001948
Iteration 177/1000 | Loss: 0.00001948
Iteration 178/1000 | Loss: 0.00001948
Iteration 179/1000 | Loss: 0.00001947
Iteration 180/1000 | Loss: 0.00001947
Iteration 181/1000 | Loss: 0.00001947
Iteration 182/1000 | Loss: 0.00001947
Iteration 183/1000 | Loss: 0.00001947
Iteration 184/1000 | Loss: 0.00001947
Iteration 185/1000 | Loss: 0.00001947
Iteration 186/1000 | Loss: 0.00001947
Iteration 187/1000 | Loss: 0.00001947
Iteration 188/1000 | Loss: 0.00001947
Iteration 189/1000 | Loss: 0.00001947
Iteration 190/1000 | Loss: 0.00001947
Iteration 191/1000 | Loss: 0.00001947
Iteration 192/1000 | Loss: 0.00001947
Iteration 193/1000 | Loss: 0.00001947
Iteration 194/1000 | Loss: 0.00001947
Iteration 195/1000 | Loss: 0.00001947
Iteration 196/1000 | Loss: 0.00001947
Iteration 197/1000 | Loss: 0.00001947
Iteration 198/1000 | Loss: 0.00001947
Iteration 199/1000 | Loss: 0.00001947
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [1.9474913642625324e-05, 1.9474913642625324e-05, 1.9474913642625324e-05, 1.9474913642625324e-05, 1.9474913642625324e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9474913642625324e-05

Optimization complete. Final v2v error: 3.7370707988739014 mm

Highest mean error: 4.530911922454834 mm for frame 239

Lowest mean error: 3.455803155899048 mm for frame 38

Saving results

Total time: 48.20556282997131
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00503165
Iteration 2/25 | Loss: 0.00129953
Iteration 3/25 | Loss: 0.00120960
Iteration 4/25 | Loss: 0.00119367
Iteration 5/25 | Loss: 0.00118904
Iteration 6/25 | Loss: 0.00118841
Iteration 7/25 | Loss: 0.00118841
Iteration 8/25 | Loss: 0.00118841
Iteration 9/25 | Loss: 0.00118793
Iteration 10/25 | Loss: 0.00118793
Iteration 11/25 | Loss: 0.00118793
Iteration 12/25 | Loss: 0.00118793
Iteration 13/25 | Loss: 0.00118793
Iteration 14/25 | Loss: 0.00118793
Iteration 15/25 | Loss: 0.00118793
Iteration 16/25 | Loss: 0.00118793
Iteration 17/25 | Loss: 0.00118793
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001187934190966189, 0.001187934190966189, 0.001187934190966189, 0.001187934190966189, 0.001187934190966189]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001187934190966189

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57415569
Iteration 2/25 | Loss: 0.00073103
Iteration 3/25 | Loss: 0.00073102
Iteration 4/25 | Loss: 0.00073102
Iteration 5/25 | Loss: 0.00073102
Iteration 6/25 | Loss: 0.00073102
Iteration 7/25 | Loss: 0.00073102
Iteration 8/25 | Loss: 0.00073102
Iteration 9/25 | Loss: 0.00073102
Iteration 10/25 | Loss: 0.00073102
Iteration 11/25 | Loss: 0.00073102
Iteration 12/25 | Loss: 0.00073102
Iteration 13/25 | Loss: 0.00073102
Iteration 14/25 | Loss: 0.00073102
Iteration 15/25 | Loss: 0.00073102
Iteration 16/25 | Loss: 0.00073102
Iteration 17/25 | Loss: 0.00073102
Iteration 18/25 | Loss: 0.00073102
Iteration 19/25 | Loss: 0.00073102
Iteration 20/25 | Loss: 0.00073102
Iteration 21/25 | Loss: 0.00073102
Iteration 22/25 | Loss: 0.00073102
Iteration 23/25 | Loss: 0.00073102
Iteration 24/25 | Loss: 0.00073102
Iteration 25/25 | Loss: 0.00073102

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073102
Iteration 2/1000 | Loss: 0.00002609
Iteration 3/1000 | Loss: 0.00001812
Iteration 4/1000 | Loss: 0.00001528
Iteration 5/1000 | Loss: 0.00001450
Iteration 6/1000 | Loss: 0.00001380
Iteration 7/1000 | Loss: 0.00001326
Iteration 8/1000 | Loss: 0.00001304
Iteration 9/1000 | Loss: 0.00001292
Iteration 10/1000 | Loss: 0.00001279
Iteration 11/1000 | Loss: 0.00001256
Iteration 12/1000 | Loss: 0.00001251
Iteration 13/1000 | Loss: 0.00001248
Iteration 14/1000 | Loss: 0.00001242
Iteration 15/1000 | Loss: 0.00001241
Iteration 16/1000 | Loss: 0.00001230
Iteration 17/1000 | Loss: 0.00001223
Iteration 18/1000 | Loss: 0.00001222
Iteration 19/1000 | Loss: 0.00001221
Iteration 20/1000 | Loss: 0.00001221
Iteration 21/1000 | Loss: 0.00001218
Iteration 22/1000 | Loss: 0.00001217
Iteration 23/1000 | Loss: 0.00001214
Iteration 24/1000 | Loss: 0.00001213
Iteration 25/1000 | Loss: 0.00001212
Iteration 26/1000 | Loss: 0.00001211
Iteration 27/1000 | Loss: 0.00001210
Iteration 28/1000 | Loss: 0.00001210
Iteration 29/1000 | Loss: 0.00001209
Iteration 30/1000 | Loss: 0.00001209
Iteration 31/1000 | Loss: 0.00001208
Iteration 32/1000 | Loss: 0.00001208
Iteration 33/1000 | Loss: 0.00001207
Iteration 34/1000 | Loss: 0.00001207
Iteration 35/1000 | Loss: 0.00001206
Iteration 36/1000 | Loss: 0.00001205
Iteration 37/1000 | Loss: 0.00001202
Iteration 38/1000 | Loss: 0.00001201
Iteration 39/1000 | Loss: 0.00001201
Iteration 40/1000 | Loss: 0.00001201
Iteration 41/1000 | Loss: 0.00001201
Iteration 42/1000 | Loss: 0.00001201
Iteration 43/1000 | Loss: 0.00001201
Iteration 44/1000 | Loss: 0.00001200
Iteration 45/1000 | Loss: 0.00001200
Iteration 46/1000 | Loss: 0.00001198
Iteration 47/1000 | Loss: 0.00001198
Iteration 48/1000 | Loss: 0.00001198
Iteration 49/1000 | Loss: 0.00001198
Iteration 50/1000 | Loss: 0.00001198
Iteration 51/1000 | Loss: 0.00001197
Iteration 52/1000 | Loss: 0.00001197
Iteration 53/1000 | Loss: 0.00001197
Iteration 54/1000 | Loss: 0.00001196
Iteration 55/1000 | Loss: 0.00001196
Iteration 56/1000 | Loss: 0.00001194
Iteration 57/1000 | Loss: 0.00001194
Iteration 58/1000 | Loss: 0.00001193
Iteration 59/1000 | Loss: 0.00001193
Iteration 60/1000 | Loss: 0.00001192
Iteration 61/1000 | Loss: 0.00001192
Iteration 62/1000 | Loss: 0.00001192
Iteration 63/1000 | Loss: 0.00001192
Iteration 64/1000 | Loss: 0.00001191
Iteration 65/1000 | Loss: 0.00001191
Iteration 66/1000 | Loss: 0.00001191
Iteration 67/1000 | Loss: 0.00001190
Iteration 68/1000 | Loss: 0.00001189
Iteration 69/1000 | Loss: 0.00001189
Iteration 70/1000 | Loss: 0.00001189
Iteration 71/1000 | Loss: 0.00001188
Iteration 72/1000 | Loss: 0.00001188
Iteration 73/1000 | Loss: 0.00001188
Iteration 74/1000 | Loss: 0.00001187
Iteration 75/1000 | Loss: 0.00001187
Iteration 76/1000 | Loss: 0.00001187
Iteration 77/1000 | Loss: 0.00001186
Iteration 78/1000 | Loss: 0.00001186
Iteration 79/1000 | Loss: 0.00001186
Iteration 80/1000 | Loss: 0.00001186
Iteration 81/1000 | Loss: 0.00001186
Iteration 82/1000 | Loss: 0.00001186
Iteration 83/1000 | Loss: 0.00001186
Iteration 84/1000 | Loss: 0.00001186
Iteration 85/1000 | Loss: 0.00001186
Iteration 86/1000 | Loss: 0.00001186
Iteration 87/1000 | Loss: 0.00001186
Iteration 88/1000 | Loss: 0.00001186
Iteration 89/1000 | Loss: 0.00001186
Iteration 90/1000 | Loss: 0.00001186
Iteration 91/1000 | Loss: 0.00001186
Iteration 92/1000 | Loss: 0.00001186
Iteration 93/1000 | Loss: 0.00001186
Iteration 94/1000 | Loss: 0.00001186
Iteration 95/1000 | Loss: 0.00001186
Iteration 96/1000 | Loss: 0.00001186
Iteration 97/1000 | Loss: 0.00001186
Iteration 98/1000 | Loss: 0.00001186
Iteration 99/1000 | Loss: 0.00001186
Iteration 100/1000 | Loss: 0.00001186
Iteration 101/1000 | Loss: 0.00001186
Iteration 102/1000 | Loss: 0.00001184
Iteration 103/1000 | Loss: 0.00001184
Iteration 104/1000 | Loss: 0.00001183
Iteration 105/1000 | Loss: 0.00001183
Iteration 106/1000 | Loss: 0.00001183
Iteration 107/1000 | Loss: 0.00001183
Iteration 108/1000 | Loss: 0.00001183
Iteration 109/1000 | Loss: 0.00001183
Iteration 110/1000 | Loss: 0.00001183
Iteration 111/1000 | Loss: 0.00001183
Iteration 112/1000 | Loss: 0.00001183
Iteration 113/1000 | Loss: 0.00001183
Iteration 114/1000 | Loss: 0.00001182
Iteration 115/1000 | Loss: 0.00001182
Iteration 116/1000 | Loss: 0.00001182
Iteration 117/1000 | Loss: 0.00001182
Iteration 118/1000 | Loss: 0.00001182
Iteration 119/1000 | Loss: 0.00001181
Iteration 120/1000 | Loss: 0.00001181
Iteration 121/1000 | Loss: 0.00001180
Iteration 122/1000 | Loss: 0.00001180
Iteration 123/1000 | Loss: 0.00001180
Iteration 124/1000 | Loss: 0.00001180
Iteration 125/1000 | Loss: 0.00001180
Iteration 126/1000 | Loss: 0.00001180
Iteration 127/1000 | Loss: 0.00001180
Iteration 128/1000 | Loss: 0.00001180
Iteration 129/1000 | Loss: 0.00001179
Iteration 130/1000 | Loss: 0.00001179
Iteration 131/1000 | Loss: 0.00001179
Iteration 132/1000 | Loss: 0.00001179
Iteration 133/1000 | Loss: 0.00001179
Iteration 134/1000 | Loss: 0.00001179
Iteration 135/1000 | Loss: 0.00001179
Iteration 136/1000 | Loss: 0.00001179
Iteration 137/1000 | Loss: 0.00001179
Iteration 138/1000 | Loss: 0.00001179
Iteration 139/1000 | Loss: 0.00001179
Iteration 140/1000 | Loss: 0.00001179
Iteration 141/1000 | Loss: 0.00001178
Iteration 142/1000 | Loss: 0.00001178
Iteration 143/1000 | Loss: 0.00001178
Iteration 144/1000 | Loss: 0.00001178
Iteration 145/1000 | Loss: 0.00001177
Iteration 146/1000 | Loss: 0.00001177
Iteration 147/1000 | Loss: 0.00001177
Iteration 148/1000 | Loss: 0.00001177
Iteration 149/1000 | Loss: 0.00001177
Iteration 150/1000 | Loss: 0.00001176
Iteration 151/1000 | Loss: 0.00001176
Iteration 152/1000 | Loss: 0.00001176
Iteration 153/1000 | Loss: 0.00001176
Iteration 154/1000 | Loss: 0.00001176
Iteration 155/1000 | Loss: 0.00001176
Iteration 156/1000 | Loss: 0.00001176
Iteration 157/1000 | Loss: 0.00001176
Iteration 158/1000 | Loss: 0.00001175
Iteration 159/1000 | Loss: 0.00001175
Iteration 160/1000 | Loss: 0.00001175
Iteration 161/1000 | Loss: 0.00001175
Iteration 162/1000 | Loss: 0.00001175
Iteration 163/1000 | Loss: 0.00001175
Iteration 164/1000 | Loss: 0.00001175
Iteration 165/1000 | Loss: 0.00001175
Iteration 166/1000 | Loss: 0.00001175
Iteration 167/1000 | Loss: 0.00001175
Iteration 168/1000 | Loss: 0.00001175
Iteration 169/1000 | Loss: 0.00001174
Iteration 170/1000 | Loss: 0.00001174
Iteration 171/1000 | Loss: 0.00001174
Iteration 172/1000 | Loss: 0.00001174
Iteration 173/1000 | Loss: 0.00001174
Iteration 174/1000 | Loss: 0.00001174
Iteration 175/1000 | Loss: 0.00001173
Iteration 176/1000 | Loss: 0.00001173
Iteration 177/1000 | Loss: 0.00001173
Iteration 178/1000 | Loss: 0.00001173
Iteration 179/1000 | Loss: 0.00001173
Iteration 180/1000 | Loss: 0.00001172
Iteration 181/1000 | Loss: 0.00001172
Iteration 182/1000 | Loss: 0.00001172
Iteration 183/1000 | Loss: 0.00001172
Iteration 184/1000 | Loss: 0.00001172
Iteration 185/1000 | Loss: 0.00001172
Iteration 186/1000 | Loss: 0.00001172
Iteration 187/1000 | Loss: 0.00001172
Iteration 188/1000 | Loss: 0.00001172
Iteration 189/1000 | Loss: 0.00001172
Iteration 190/1000 | Loss: 0.00001172
Iteration 191/1000 | Loss: 0.00001172
Iteration 192/1000 | Loss: 0.00001172
Iteration 193/1000 | Loss: 0.00001172
Iteration 194/1000 | Loss: 0.00001172
Iteration 195/1000 | Loss: 0.00001172
Iteration 196/1000 | Loss: 0.00001172
Iteration 197/1000 | Loss: 0.00001172
Iteration 198/1000 | Loss: 0.00001172
Iteration 199/1000 | Loss: 0.00001172
Iteration 200/1000 | Loss: 0.00001172
Iteration 201/1000 | Loss: 0.00001172
Iteration 202/1000 | Loss: 0.00001172
Iteration 203/1000 | Loss: 0.00001172
Iteration 204/1000 | Loss: 0.00001172
Iteration 205/1000 | Loss: 0.00001172
Iteration 206/1000 | Loss: 0.00001172
Iteration 207/1000 | Loss: 0.00001172
Iteration 208/1000 | Loss: 0.00001172
Iteration 209/1000 | Loss: 0.00001172
Iteration 210/1000 | Loss: 0.00001172
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [1.1718465430021752e-05, 1.1718465430021752e-05, 1.1718465430021752e-05, 1.1718465430021752e-05, 1.1718465430021752e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1718465430021752e-05

Optimization complete. Final v2v error: 2.952240467071533 mm

Highest mean error: 3.2150633335113525 mm for frame 78

Lowest mean error: 2.7789180278778076 mm for frame 97

Saving results

Total time: 40.04997515678406
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00531862
Iteration 2/25 | Loss: 0.00144469
Iteration 3/25 | Loss: 0.00130892
Iteration 4/25 | Loss: 0.00126879
Iteration 5/25 | Loss: 0.00125007
Iteration 6/25 | Loss: 0.00126402
Iteration 7/25 | Loss: 0.00126564
Iteration 8/25 | Loss: 0.00126466
Iteration 9/25 | Loss: 0.00123008
Iteration 10/25 | Loss: 0.00122730
Iteration 11/25 | Loss: 0.00122748
Iteration 12/25 | Loss: 0.00122397
Iteration 13/25 | Loss: 0.00122210
Iteration 14/25 | Loss: 0.00121960
Iteration 15/25 | Loss: 0.00121884
Iteration 16/25 | Loss: 0.00121855
Iteration 17/25 | Loss: 0.00122220
Iteration 18/25 | Loss: 0.00122192
Iteration 19/25 | Loss: 0.00122100
Iteration 20/25 | Loss: 0.00121899
Iteration 21/25 | Loss: 0.00121625
Iteration 22/25 | Loss: 0.00121454
Iteration 23/25 | Loss: 0.00121353
Iteration 24/25 | Loss: 0.00121334
Iteration 25/25 | Loss: 0.00121329

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49335837
Iteration 2/25 | Loss: 0.00082749
Iteration 3/25 | Loss: 0.00082749
Iteration 4/25 | Loss: 0.00082749
Iteration 5/25 | Loss: 0.00082749
Iteration 6/25 | Loss: 0.00082749
Iteration 7/25 | Loss: 0.00082749
Iteration 8/25 | Loss: 0.00082749
Iteration 9/25 | Loss: 0.00082749
Iteration 10/25 | Loss: 0.00082749
Iteration 11/25 | Loss: 0.00082749
Iteration 12/25 | Loss: 0.00082748
Iteration 13/25 | Loss: 0.00082748
Iteration 14/25 | Loss: 0.00082748
Iteration 15/25 | Loss: 0.00082748
Iteration 16/25 | Loss: 0.00082748
Iteration 17/25 | Loss: 0.00082748
Iteration 18/25 | Loss: 0.00082748
Iteration 19/25 | Loss: 0.00082748
Iteration 20/25 | Loss: 0.00082748
Iteration 21/25 | Loss: 0.00082748
Iteration 22/25 | Loss: 0.00082748
Iteration 23/25 | Loss: 0.00082748
Iteration 24/25 | Loss: 0.00082748
Iteration 25/25 | Loss: 0.00082748

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082748
Iteration 2/1000 | Loss: 0.00002890
Iteration 3/1000 | Loss: 0.00001968
Iteration 4/1000 | Loss: 0.00001588
Iteration 5/1000 | Loss: 0.00001496
Iteration 6/1000 | Loss: 0.00001426
Iteration 7/1000 | Loss: 0.00001380
Iteration 8/1000 | Loss: 0.00001338
Iteration 9/1000 | Loss: 0.00001322
Iteration 10/1000 | Loss: 0.00001303
Iteration 11/1000 | Loss: 0.00001293
Iteration 12/1000 | Loss: 0.00001277
Iteration 13/1000 | Loss: 0.00001277
Iteration 14/1000 | Loss: 0.00001269
Iteration 15/1000 | Loss: 0.00001268
Iteration 16/1000 | Loss: 0.00001264
Iteration 17/1000 | Loss: 0.00001257
Iteration 18/1000 | Loss: 0.00001255
Iteration 19/1000 | Loss: 0.00001254
Iteration 20/1000 | Loss: 0.00001253
Iteration 21/1000 | Loss: 0.00001253
Iteration 22/1000 | Loss: 0.00001252
Iteration 23/1000 | Loss: 0.00001251
Iteration 24/1000 | Loss: 0.00001250
Iteration 25/1000 | Loss: 0.00001250
Iteration 26/1000 | Loss: 0.00001249
Iteration 27/1000 | Loss: 0.00001248
Iteration 28/1000 | Loss: 0.00001247
Iteration 29/1000 | Loss: 0.00001247
Iteration 30/1000 | Loss: 0.00001245
Iteration 31/1000 | Loss: 0.00001245
Iteration 32/1000 | Loss: 0.00001244
Iteration 33/1000 | Loss: 0.00001244
Iteration 34/1000 | Loss: 0.00001243
Iteration 35/1000 | Loss: 0.00001243
Iteration 36/1000 | Loss: 0.00001243
Iteration 37/1000 | Loss: 0.00001242
Iteration 38/1000 | Loss: 0.00001242
Iteration 39/1000 | Loss: 0.00001241
Iteration 40/1000 | Loss: 0.00001241
Iteration 41/1000 | Loss: 0.00001240
Iteration 42/1000 | Loss: 0.00001240
Iteration 43/1000 | Loss: 0.00001240
Iteration 44/1000 | Loss: 0.00001240
Iteration 45/1000 | Loss: 0.00001240
Iteration 46/1000 | Loss: 0.00001240
Iteration 47/1000 | Loss: 0.00001239
Iteration 48/1000 | Loss: 0.00001239
Iteration 49/1000 | Loss: 0.00001239
Iteration 50/1000 | Loss: 0.00001239
Iteration 51/1000 | Loss: 0.00001239
Iteration 52/1000 | Loss: 0.00001239
Iteration 53/1000 | Loss: 0.00001238
Iteration 54/1000 | Loss: 0.00001238
Iteration 55/1000 | Loss: 0.00001237
Iteration 56/1000 | Loss: 0.00001237
Iteration 57/1000 | Loss: 0.00001236
Iteration 58/1000 | Loss: 0.00001235
Iteration 59/1000 | Loss: 0.00001235
Iteration 60/1000 | Loss: 0.00001235
Iteration 61/1000 | Loss: 0.00001234
Iteration 62/1000 | Loss: 0.00001234
Iteration 63/1000 | Loss: 0.00001234
Iteration 64/1000 | Loss: 0.00001234
Iteration 65/1000 | Loss: 0.00001234
Iteration 66/1000 | Loss: 0.00001234
Iteration 67/1000 | Loss: 0.00001234
Iteration 68/1000 | Loss: 0.00001234
Iteration 69/1000 | Loss: 0.00001233
Iteration 70/1000 | Loss: 0.00001233
Iteration 71/1000 | Loss: 0.00001231
Iteration 72/1000 | Loss: 0.00001230
Iteration 73/1000 | Loss: 0.00001229
Iteration 74/1000 | Loss: 0.00001229
Iteration 75/1000 | Loss: 0.00001228
Iteration 76/1000 | Loss: 0.00001228
Iteration 77/1000 | Loss: 0.00001228
Iteration 78/1000 | Loss: 0.00001228
Iteration 79/1000 | Loss: 0.00001228
Iteration 80/1000 | Loss: 0.00001227
Iteration 81/1000 | Loss: 0.00001227
Iteration 82/1000 | Loss: 0.00001226
Iteration 83/1000 | Loss: 0.00001226
Iteration 84/1000 | Loss: 0.00001226
Iteration 85/1000 | Loss: 0.00001226
Iteration 86/1000 | Loss: 0.00001226
Iteration 87/1000 | Loss: 0.00001226
Iteration 88/1000 | Loss: 0.00001226
Iteration 89/1000 | Loss: 0.00001226
Iteration 90/1000 | Loss: 0.00001226
Iteration 91/1000 | Loss: 0.00001226
Iteration 92/1000 | Loss: 0.00001225
Iteration 93/1000 | Loss: 0.00001225
Iteration 94/1000 | Loss: 0.00001224
Iteration 95/1000 | Loss: 0.00001224
Iteration 96/1000 | Loss: 0.00001224
Iteration 97/1000 | Loss: 0.00001224
Iteration 98/1000 | Loss: 0.00001224
Iteration 99/1000 | Loss: 0.00001224
Iteration 100/1000 | Loss: 0.00001224
Iteration 101/1000 | Loss: 0.00001224
Iteration 102/1000 | Loss: 0.00001223
Iteration 103/1000 | Loss: 0.00001223
Iteration 104/1000 | Loss: 0.00001223
Iteration 105/1000 | Loss: 0.00001223
Iteration 106/1000 | Loss: 0.00001223
Iteration 107/1000 | Loss: 0.00001223
Iteration 108/1000 | Loss: 0.00001222
Iteration 109/1000 | Loss: 0.00001222
Iteration 110/1000 | Loss: 0.00001222
Iteration 111/1000 | Loss: 0.00001222
Iteration 112/1000 | Loss: 0.00001221
Iteration 113/1000 | Loss: 0.00001221
Iteration 114/1000 | Loss: 0.00001220
Iteration 115/1000 | Loss: 0.00001220
Iteration 116/1000 | Loss: 0.00001220
Iteration 117/1000 | Loss: 0.00001220
Iteration 118/1000 | Loss: 0.00001219
Iteration 119/1000 | Loss: 0.00001219
Iteration 120/1000 | Loss: 0.00001219
Iteration 121/1000 | Loss: 0.00001219
Iteration 122/1000 | Loss: 0.00001219
Iteration 123/1000 | Loss: 0.00001219
Iteration 124/1000 | Loss: 0.00001219
Iteration 125/1000 | Loss: 0.00001218
Iteration 126/1000 | Loss: 0.00001218
Iteration 127/1000 | Loss: 0.00001218
Iteration 128/1000 | Loss: 0.00001218
Iteration 129/1000 | Loss: 0.00001218
Iteration 130/1000 | Loss: 0.00001217
Iteration 131/1000 | Loss: 0.00001217
Iteration 132/1000 | Loss: 0.00001217
Iteration 133/1000 | Loss: 0.00001217
Iteration 134/1000 | Loss: 0.00001216
Iteration 135/1000 | Loss: 0.00001216
Iteration 136/1000 | Loss: 0.00001216
Iteration 137/1000 | Loss: 0.00001216
Iteration 138/1000 | Loss: 0.00001216
Iteration 139/1000 | Loss: 0.00001216
Iteration 140/1000 | Loss: 0.00001216
Iteration 141/1000 | Loss: 0.00001216
Iteration 142/1000 | Loss: 0.00001216
Iteration 143/1000 | Loss: 0.00001216
Iteration 144/1000 | Loss: 0.00001216
Iteration 145/1000 | Loss: 0.00001215
Iteration 146/1000 | Loss: 0.00001215
Iteration 147/1000 | Loss: 0.00001215
Iteration 148/1000 | Loss: 0.00001214
Iteration 149/1000 | Loss: 0.00001214
Iteration 150/1000 | Loss: 0.00001214
Iteration 151/1000 | Loss: 0.00001214
Iteration 152/1000 | Loss: 0.00001214
Iteration 153/1000 | Loss: 0.00001214
Iteration 154/1000 | Loss: 0.00001214
Iteration 155/1000 | Loss: 0.00001213
Iteration 156/1000 | Loss: 0.00001213
Iteration 157/1000 | Loss: 0.00001213
Iteration 158/1000 | Loss: 0.00001213
Iteration 159/1000 | Loss: 0.00001213
Iteration 160/1000 | Loss: 0.00001213
Iteration 161/1000 | Loss: 0.00001213
Iteration 162/1000 | Loss: 0.00001213
Iteration 163/1000 | Loss: 0.00001213
Iteration 164/1000 | Loss: 0.00001213
Iteration 165/1000 | Loss: 0.00001213
Iteration 166/1000 | Loss: 0.00001213
Iteration 167/1000 | Loss: 0.00001213
Iteration 168/1000 | Loss: 0.00001212
Iteration 169/1000 | Loss: 0.00001212
Iteration 170/1000 | Loss: 0.00001212
Iteration 171/1000 | Loss: 0.00001212
Iteration 172/1000 | Loss: 0.00001212
Iteration 173/1000 | Loss: 0.00001212
Iteration 174/1000 | Loss: 0.00001212
Iteration 175/1000 | Loss: 0.00001212
Iteration 176/1000 | Loss: 0.00001212
Iteration 177/1000 | Loss: 0.00001211
Iteration 178/1000 | Loss: 0.00001211
Iteration 179/1000 | Loss: 0.00001211
Iteration 180/1000 | Loss: 0.00001211
Iteration 181/1000 | Loss: 0.00001211
Iteration 182/1000 | Loss: 0.00001211
Iteration 183/1000 | Loss: 0.00001211
Iteration 184/1000 | Loss: 0.00001211
Iteration 185/1000 | Loss: 0.00001211
Iteration 186/1000 | Loss: 0.00001211
Iteration 187/1000 | Loss: 0.00001211
Iteration 188/1000 | Loss: 0.00001211
Iteration 189/1000 | Loss: 0.00001211
Iteration 190/1000 | Loss: 0.00001211
Iteration 191/1000 | Loss: 0.00001211
Iteration 192/1000 | Loss: 0.00001211
Iteration 193/1000 | Loss: 0.00001210
Iteration 194/1000 | Loss: 0.00001210
Iteration 195/1000 | Loss: 0.00001210
Iteration 196/1000 | Loss: 0.00001210
Iteration 197/1000 | Loss: 0.00001210
Iteration 198/1000 | Loss: 0.00001210
Iteration 199/1000 | Loss: 0.00001210
Iteration 200/1000 | Loss: 0.00001210
Iteration 201/1000 | Loss: 0.00001210
Iteration 202/1000 | Loss: 0.00001210
Iteration 203/1000 | Loss: 0.00001210
Iteration 204/1000 | Loss: 0.00001210
Iteration 205/1000 | Loss: 0.00001210
Iteration 206/1000 | Loss: 0.00001210
Iteration 207/1000 | Loss: 0.00001210
Iteration 208/1000 | Loss: 0.00001209
Iteration 209/1000 | Loss: 0.00001209
Iteration 210/1000 | Loss: 0.00001209
Iteration 211/1000 | Loss: 0.00001209
Iteration 212/1000 | Loss: 0.00001209
Iteration 213/1000 | Loss: 0.00001209
Iteration 214/1000 | Loss: 0.00001209
Iteration 215/1000 | Loss: 0.00001209
Iteration 216/1000 | Loss: 0.00001209
Iteration 217/1000 | Loss: 0.00001209
Iteration 218/1000 | Loss: 0.00001209
Iteration 219/1000 | Loss: 0.00001209
Iteration 220/1000 | Loss: 0.00001209
Iteration 221/1000 | Loss: 0.00001209
Iteration 222/1000 | Loss: 0.00001209
Iteration 223/1000 | Loss: 0.00001209
Iteration 224/1000 | Loss: 0.00001209
Iteration 225/1000 | Loss: 0.00001209
Iteration 226/1000 | Loss: 0.00001209
Iteration 227/1000 | Loss: 0.00001209
Iteration 228/1000 | Loss: 0.00001209
Iteration 229/1000 | Loss: 0.00001209
Iteration 230/1000 | Loss: 0.00001208
Iteration 231/1000 | Loss: 0.00001208
Iteration 232/1000 | Loss: 0.00001208
Iteration 233/1000 | Loss: 0.00001208
Iteration 234/1000 | Loss: 0.00001208
Iteration 235/1000 | Loss: 0.00001208
Iteration 236/1000 | Loss: 0.00001208
Iteration 237/1000 | Loss: 0.00001208
Iteration 238/1000 | Loss: 0.00001208
Iteration 239/1000 | Loss: 0.00001208
Iteration 240/1000 | Loss: 0.00001208
Iteration 241/1000 | Loss: 0.00001208
Iteration 242/1000 | Loss: 0.00001208
Iteration 243/1000 | Loss: 0.00001208
Iteration 244/1000 | Loss: 0.00001208
Iteration 245/1000 | Loss: 0.00001208
Iteration 246/1000 | Loss: 0.00001208
Iteration 247/1000 | Loss: 0.00001208
Iteration 248/1000 | Loss: 0.00001208
Iteration 249/1000 | Loss: 0.00001208
Iteration 250/1000 | Loss: 0.00001208
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 250. Stopping optimization.
Last 5 losses: [1.2080534361302853e-05, 1.2080534361302853e-05, 1.2080534361302853e-05, 1.2080534361302853e-05, 1.2080534361302853e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2080534361302853e-05

Optimization complete. Final v2v error: 2.9795682430267334 mm

Highest mean error: 3.5574653148651123 mm for frame 45

Lowest mean error: 2.6949825286865234 mm for frame 118

Saving results

Total time: 78.02117347717285
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00800027
Iteration 2/25 | Loss: 0.00149842
Iteration 3/25 | Loss: 0.00132207
Iteration 4/25 | Loss: 0.00129675
Iteration 5/25 | Loss: 0.00129359
Iteration 6/25 | Loss: 0.00128624
Iteration 7/25 | Loss: 0.00127748
Iteration 8/25 | Loss: 0.00127483
Iteration 9/25 | Loss: 0.00127697
Iteration 10/25 | Loss: 0.00127524
Iteration 11/25 | Loss: 0.00127193
Iteration 12/25 | Loss: 0.00127090
Iteration 13/25 | Loss: 0.00127058
Iteration 14/25 | Loss: 0.00127045
Iteration 15/25 | Loss: 0.00127032
Iteration 16/25 | Loss: 0.00127021
Iteration 17/25 | Loss: 0.00127012
Iteration 18/25 | Loss: 0.00127001
Iteration 19/25 | Loss: 0.00126985
Iteration 20/25 | Loss: 0.00126959
Iteration 21/25 | Loss: 0.00126930
Iteration 22/25 | Loss: 0.00127242
Iteration 23/25 | Loss: 0.00127325
Iteration 24/25 | Loss: 0.00126831
Iteration 25/25 | Loss: 0.00126717

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50484443
Iteration 2/25 | Loss: 0.00084367
Iteration 3/25 | Loss: 0.00084366
Iteration 4/25 | Loss: 0.00084366
Iteration 5/25 | Loss: 0.00084366
Iteration 6/25 | Loss: 0.00084366
Iteration 7/25 | Loss: 0.00084366
Iteration 8/25 | Loss: 0.00084366
Iteration 9/25 | Loss: 0.00084366
Iteration 10/25 | Loss: 0.00084366
Iteration 11/25 | Loss: 0.00084366
Iteration 12/25 | Loss: 0.00084366
Iteration 13/25 | Loss: 0.00084366
Iteration 14/25 | Loss: 0.00084366
Iteration 15/25 | Loss: 0.00084366
Iteration 16/25 | Loss: 0.00084366
Iteration 17/25 | Loss: 0.00084366
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000843659567181021, 0.000843659567181021, 0.000843659567181021, 0.000843659567181021, 0.000843659567181021]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000843659567181021

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084366
Iteration 2/1000 | Loss: 0.00007546
Iteration 3/1000 | Loss: 0.00005210
Iteration 4/1000 | Loss: 0.00004689
Iteration 5/1000 | Loss: 0.00004389
Iteration 6/1000 | Loss: 0.00004181
Iteration 7/1000 | Loss: 0.00004028
Iteration 8/1000 | Loss: 0.00003913
Iteration 9/1000 | Loss: 0.00003798
Iteration 10/1000 | Loss: 0.00003709
Iteration 11/1000 | Loss: 0.00003639
Iteration 12/1000 | Loss: 0.00003571
Iteration 13/1000 | Loss: 0.00003494
Iteration 14/1000 | Loss: 0.00003436
Iteration 15/1000 | Loss: 0.00003388
Iteration 16/1000 | Loss: 0.00003352
Iteration 17/1000 | Loss: 0.00003322
Iteration 18/1000 | Loss: 0.00045560
Iteration 19/1000 | Loss: 0.00106208
Iteration 20/1000 | Loss: 0.00007360
Iteration 21/1000 | Loss: 0.00034054
Iteration 22/1000 | Loss: 0.00021899
Iteration 23/1000 | Loss: 0.00003946
Iteration 24/1000 | Loss: 0.00025031
Iteration 25/1000 | Loss: 0.00004555
Iteration 26/1000 | Loss: 0.00003855
Iteration 27/1000 | Loss: 0.00003328
Iteration 28/1000 | Loss: 0.00003129
Iteration 29/1000 | Loss: 0.00002977
Iteration 30/1000 | Loss: 0.00002906
Iteration 31/1000 | Loss: 0.00002853
Iteration 32/1000 | Loss: 0.00002827
Iteration 33/1000 | Loss: 0.00002810
Iteration 34/1000 | Loss: 0.00002804
Iteration 35/1000 | Loss: 0.00002795
Iteration 36/1000 | Loss: 0.00002789
Iteration 37/1000 | Loss: 0.00002789
Iteration 38/1000 | Loss: 0.00002784
Iteration 39/1000 | Loss: 0.00002782
Iteration 40/1000 | Loss: 0.00002781
Iteration 41/1000 | Loss: 0.00002780
Iteration 42/1000 | Loss: 0.00002779
Iteration 43/1000 | Loss: 0.00002778
Iteration 44/1000 | Loss: 0.00002776
Iteration 45/1000 | Loss: 0.00002776
Iteration 46/1000 | Loss: 0.00002775
Iteration 47/1000 | Loss: 0.00002774
Iteration 48/1000 | Loss: 0.00002773
Iteration 49/1000 | Loss: 0.00002772
Iteration 50/1000 | Loss: 0.00002771
Iteration 51/1000 | Loss: 0.00002769
Iteration 52/1000 | Loss: 0.00002768
Iteration 53/1000 | Loss: 0.00002767
Iteration 54/1000 | Loss: 0.00002766
Iteration 55/1000 | Loss: 0.00002765
Iteration 56/1000 | Loss: 0.00002765
Iteration 57/1000 | Loss: 0.00002765
Iteration 58/1000 | Loss: 0.00002762
Iteration 59/1000 | Loss: 0.00002762
Iteration 60/1000 | Loss: 0.00002761
Iteration 61/1000 | Loss: 0.00002761
Iteration 62/1000 | Loss: 0.00002760
Iteration 63/1000 | Loss: 0.00002759
Iteration 64/1000 | Loss: 0.00002759
Iteration 65/1000 | Loss: 0.00002759
Iteration 66/1000 | Loss: 0.00002758
Iteration 67/1000 | Loss: 0.00002758
Iteration 68/1000 | Loss: 0.00002758
Iteration 69/1000 | Loss: 0.00002757
Iteration 70/1000 | Loss: 0.00002757
Iteration 71/1000 | Loss: 0.00002757
Iteration 72/1000 | Loss: 0.00002756
Iteration 73/1000 | Loss: 0.00002756
Iteration 74/1000 | Loss: 0.00002756
Iteration 75/1000 | Loss: 0.00002756
Iteration 76/1000 | Loss: 0.00002756
Iteration 77/1000 | Loss: 0.00002755
Iteration 78/1000 | Loss: 0.00002755
Iteration 79/1000 | Loss: 0.00002755
Iteration 80/1000 | Loss: 0.00002754
Iteration 81/1000 | Loss: 0.00002754
Iteration 82/1000 | Loss: 0.00002754
Iteration 83/1000 | Loss: 0.00002754
Iteration 84/1000 | Loss: 0.00002754
Iteration 85/1000 | Loss: 0.00002754
Iteration 86/1000 | Loss: 0.00002753
Iteration 87/1000 | Loss: 0.00002753
Iteration 88/1000 | Loss: 0.00002753
Iteration 89/1000 | Loss: 0.00002753
Iteration 90/1000 | Loss: 0.00002753
Iteration 91/1000 | Loss: 0.00002753
Iteration 92/1000 | Loss: 0.00002753
Iteration 93/1000 | Loss: 0.00002752
Iteration 94/1000 | Loss: 0.00002752
Iteration 95/1000 | Loss: 0.00002752
Iteration 96/1000 | Loss: 0.00002752
Iteration 97/1000 | Loss: 0.00002752
Iteration 98/1000 | Loss: 0.00002752
Iteration 99/1000 | Loss: 0.00002752
Iteration 100/1000 | Loss: 0.00002752
Iteration 101/1000 | Loss: 0.00002751
Iteration 102/1000 | Loss: 0.00002751
Iteration 103/1000 | Loss: 0.00002751
Iteration 104/1000 | Loss: 0.00002751
Iteration 105/1000 | Loss: 0.00002751
Iteration 106/1000 | Loss: 0.00002751
Iteration 107/1000 | Loss: 0.00002751
Iteration 108/1000 | Loss: 0.00002751
Iteration 109/1000 | Loss: 0.00002751
Iteration 110/1000 | Loss: 0.00002750
Iteration 111/1000 | Loss: 0.00002750
Iteration 112/1000 | Loss: 0.00002750
Iteration 113/1000 | Loss: 0.00002749
Iteration 114/1000 | Loss: 0.00002749
Iteration 115/1000 | Loss: 0.00002749
Iteration 116/1000 | Loss: 0.00002749
Iteration 117/1000 | Loss: 0.00002749
Iteration 118/1000 | Loss: 0.00002749
Iteration 119/1000 | Loss: 0.00002749
Iteration 120/1000 | Loss: 0.00002749
Iteration 121/1000 | Loss: 0.00002749
Iteration 122/1000 | Loss: 0.00002749
Iteration 123/1000 | Loss: 0.00002749
Iteration 124/1000 | Loss: 0.00002749
Iteration 125/1000 | Loss: 0.00002749
Iteration 126/1000 | Loss: 0.00002749
Iteration 127/1000 | Loss: 0.00002749
Iteration 128/1000 | Loss: 0.00002749
Iteration 129/1000 | Loss: 0.00002748
Iteration 130/1000 | Loss: 0.00002748
Iteration 131/1000 | Loss: 0.00002748
Iteration 132/1000 | Loss: 0.00002748
Iteration 133/1000 | Loss: 0.00002748
Iteration 134/1000 | Loss: 0.00002748
Iteration 135/1000 | Loss: 0.00002748
Iteration 136/1000 | Loss: 0.00002748
Iteration 137/1000 | Loss: 0.00002748
Iteration 138/1000 | Loss: 0.00002748
Iteration 139/1000 | Loss: 0.00002748
Iteration 140/1000 | Loss: 0.00002748
Iteration 141/1000 | Loss: 0.00002748
Iteration 142/1000 | Loss: 0.00002748
Iteration 143/1000 | Loss: 0.00002748
Iteration 144/1000 | Loss: 0.00002747
Iteration 145/1000 | Loss: 0.00002747
Iteration 146/1000 | Loss: 0.00002747
Iteration 147/1000 | Loss: 0.00002747
Iteration 148/1000 | Loss: 0.00002747
Iteration 149/1000 | Loss: 0.00002747
Iteration 150/1000 | Loss: 0.00002747
Iteration 151/1000 | Loss: 0.00002747
Iteration 152/1000 | Loss: 0.00002747
Iteration 153/1000 | Loss: 0.00002747
Iteration 154/1000 | Loss: 0.00002747
Iteration 155/1000 | Loss: 0.00002747
Iteration 156/1000 | Loss: 0.00002747
Iteration 157/1000 | Loss: 0.00002747
Iteration 158/1000 | Loss: 0.00002747
Iteration 159/1000 | Loss: 0.00002747
Iteration 160/1000 | Loss: 0.00002747
Iteration 161/1000 | Loss: 0.00002747
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [2.7467582185636275e-05, 2.7467582185636275e-05, 2.7467582185636275e-05, 2.7467582185636275e-05, 2.7467582185636275e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7467582185636275e-05

Optimization complete. Final v2v error: 3.8760507106781006 mm

Highest mean error: 11.349299430847168 mm for frame 169

Lowest mean error: 3.0835654735565186 mm for frame 148

Saving results

Total time: 117.09480094909668
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00415026
Iteration 2/25 | Loss: 0.00131392
Iteration 3/25 | Loss: 0.00120737
Iteration 4/25 | Loss: 0.00120015
Iteration 5/25 | Loss: 0.00119846
Iteration 6/25 | Loss: 0.00119834
Iteration 7/25 | Loss: 0.00119834
Iteration 8/25 | Loss: 0.00119834
Iteration 9/25 | Loss: 0.00119834
Iteration 10/25 | Loss: 0.00119834
Iteration 11/25 | Loss: 0.00119834
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011983448639512062, 0.0011983448639512062, 0.0011983448639512062, 0.0011983448639512062, 0.0011983448639512062]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011983448639512062

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44339037
Iteration 2/25 | Loss: 0.00079642
Iteration 3/25 | Loss: 0.00079642
Iteration 4/25 | Loss: 0.00079642
Iteration 5/25 | Loss: 0.00079642
Iteration 6/25 | Loss: 0.00079642
Iteration 7/25 | Loss: 0.00079642
Iteration 8/25 | Loss: 0.00079642
Iteration 9/25 | Loss: 0.00079642
Iteration 10/25 | Loss: 0.00079642
Iteration 11/25 | Loss: 0.00079642
Iteration 12/25 | Loss: 0.00079642
Iteration 13/25 | Loss: 0.00079642
Iteration 14/25 | Loss: 0.00079642
Iteration 15/25 | Loss: 0.00079642
Iteration 16/25 | Loss: 0.00079642
Iteration 17/25 | Loss: 0.00079642
Iteration 18/25 | Loss: 0.00079642
Iteration 19/25 | Loss: 0.00079642
Iteration 20/25 | Loss: 0.00079642
Iteration 21/25 | Loss: 0.00079642
Iteration 22/25 | Loss: 0.00079642
Iteration 23/25 | Loss: 0.00079642
Iteration 24/25 | Loss: 0.00079642
Iteration 25/25 | Loss: 0.00079642

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079642
Iteration 2/1000 | Loss: 0.00002973
Iteration 3/1000 | Loss: 0.00001870
Iteration 4/1000 | Loss: 0.00001569
Iteration 5/1000 | Loss: 0.00001378
Iteration 6/1000 | Loss: 0.00001289
Iteration 7/1000 | Loss: 0.00001218
Iteration 8/1000 | Loss: 0.00001183
Iteration 9/1000 | Loss: 0.00001166
Iteration 10/1000 | Loss: 0.00001165
Iteration 11/1000 | Loss: 0.00001157
Iteration 12/1000 | Loss: 0.00001154
Iteration 13/1000 | Loss: 0.00001153
Iteration 14/1000 | Loss: 0.00001152
Iteration 15/1000 | Loss: 0.00001139
Iteration 16/1000 | Loss: 0.00001135
Iteration 17/1000 | Loss: 0.00001135
Iteration 18/1000 | Loss: 0.00001134
Iteration 19/1000 | Loss: 0.00001134
Iteration 20/1000 | Loss: 0.00001133
Iteration 21/1000 | Loss: 0.00001133
Iteration 22/1000 | Loss: 0.00001132
Iteration 23/1000 | Loss: 0.00001130
Iteration 24/1000 | Loss: 0.00001129
Iteration 25/1000 | Loss: 0.00001129
Iteration 26/1000 | Loss: 0.00001128
Iteration 27/1000 | Loss: 0.00001127
Iteration 28/1000 | Loss: 0.00001125
Iteration 29/1000 | Loss: 0.00001124
Iteration 30/1000 | Loss: 0.00001123
Iteration 31/1000 | Loss: 0.00001122
Iteration 32/1000 | Loss: 0.00001118
Iteration 33/1000 | Loss: 0.00001118
Iteration 34/1000 | Loss: 0.00001116
Iteration 35/1000 | Loss: 0.00001115
Iteration 36/1000 | Loss: 0.00001114
Iteration 37/1000 | Loss: 0.00001114
Iteration 38/1000 | Loss: 0.00001113
Iteration 39/1000 | Loss: 0.00001113
Iteration 40/1000 | Loss: 0.00001113
Iteration 41/1000 | Loss: 0.00001112
Iteration 42/1000 | Loss: 0.00001112
Iteration 43/1000 | Loss: 0.00001112
Iteration 44/1000 | Loss: 0.00001112
Iteration 45/1000 | Loss: 0.00001112
Iteration 46/1000 | Loss: 0.00001111
Iteration 47/1000 | Loss: 0.00001111
Iteration 48/1000 | Loss: 0.00001111
Iteration 49/1000 | Loss: 0.00001111
Iteration 50/1000 | Loss: 0.00001111
Iteration 51/1000 | Loss: 0.00001111
Iteration 52/1000 | Loss: 0.00001111
Iteration 53/1000 | Loss: 0.00001110
Iteration 54/1000 | Loss: 0.00001110
Iteration 55/1000 | Loss: 0.00001109
Iteration 56/1000 | Loss: 0.00001108
Iteration 57/1000 | Loss: 0.00001108
Iteration 58/1000 | Loss: 0.00001108
Iteration 59/1000 | Loss: 0.00001108
Iteration 60/1000 | Loss: 0.00001107
Iteration 61/1000 | Loss: 0.00001107
Iteration 62/1000 | Loss: 0.00001107
Iteration 63/1000 | Loss: 0.00001106
Iteration 64/1000 | Loss: 0.00001106
Iteration 65/1000 | Loss: 0.00001106
Iteration 66/1000 | Loss: 0.00001106
Iteration 67/1000 | Loss: 0.00001106
Iteration 68/1000 | Loss: 0.00001105
Iteration 69/1000 | Loss: 0.00001105
Iteration 70/1000 | Loss: 0.00001105
Iteration 71/1000 | Loss: 0.00001104
Iteration 72/1000 | Loss: 0.00001104
Iteration 73/1000 | Loss: 0.00001103
Iteration 74/1000 | Loss: 0.00001103
Iteration 75/1000 | Loss: 0.00001102
Iteration 76/1000 | Loss: 0.00001102
Iteration 77/1000 | Loss: 0.00001102
Iteration 78/1000 | Loss: 0.00001101
Iteration 79/1000 | Loss: 0.00001101
Iteration 80/1000 | Loss: 0.00001101
Iteration 81/1000 | Loss: 0.00001101
Iteration 82/1000 | Loss: 0.00001101
Iteration 83/1000 | Loss: 0.00001101
Iteration 84/1000 | Loss: 0.00001101
Iteration 85/1000 | Loss: 0.00001101
Iteration 86/1000 | Loss: 0.00001100
Iteration 87/1000 | Loss: 0.00001100
Iteration 88/1000 | Loss: 0.00001099
Iteration 89/1000 | Loss: 0.00001098
Iteration 90/1000 | Loss: 0.00001098
Iteration 91/1000 | Loss: 0.00001098
Iteration 92/1000 | Loss: 0.00001097
Iteration 93/1000 | Loss: 0.00001097
Iteration 94/1000 | Loss: 0.00001097
Iteration 95/1000 | Loss: 0.00001096
Iteration 96/1000 | Loss: 0.00001096
Iteration 97/1000 | Loss: 0.00001096
Iteration 98/1000 | Loss: 0.00001096
Iteration 99/1000 | Loss: 0.00001095
Iteration 100/1000 | Loss: 0.00001095
Iteration 101/1000 | Loss: 0.00001095
Iteration 102/1000 | Loss: 0.00001095
Iteration 103/1000 | Loss: 0.00001094
Iteration 104/1000 | Loss: 0.00001094
Iteration 105/1000 | Loss: 0.00001094
Iteration 106/1000 | Loss: 0.00001093
Iteration 107/1000 | Loss: 0.00001093
Iteration 108/1000 | Loss: 0.00001093
Iteration 109/1000 | Loss: 0.00001093
Iteration 110/1000 | Loss: 0.00001093
Iteration 111/1000 | Loss: 0.00001092
Iteration 112/1000 | Loss: 0.00001092
Iteration 113/1000 | Loss: 0.00001092
Iteration 114/1000 | Loss: 0.00001092
Iteration 115/1000 | Loss: 0.00001092
Iteration 116/1000 | Loss: 0.00001092
Iteration 117/1000 | Loss: 0.00001091
Iteration 118/1000 | Loss: 0.00001091
Iteration 119/1000 | Loss: 0.00001091
Iteration 120/1000 | Loss: 0.00001091
Iteration 121/1000 | Loss: 0.00001091
Iteration 122/1000 | Loss: 0.00001090
Iteration 123/1000 | Loss: 0.00001090
Iteration 124/1000 | Loss: 0.00001089
Iteration 125/1000 | Loss: 0.00001089
Iteration 126/1000 | Loss: 0.00001088
Iteration 127/1000 | Loss: 0.00001088
Iteration 128/1000 | Loss: 0.00001088
Iteration 129/1000 | Loss: 0.00001087
Iteration 130/1000 | Loss: 0.00001087
Iteration 131/1000 | Loss: 0.00001087
Iteration 132/1000 | Loss: 0.00001086
Iteration 133/1000 | Loss: 0.00001085
Iteration 134/1000 | Loss: 0.00001085
Iteration 135/1000 | Loss: 0.00001085
Iteration 136/1000 | Loss: 0.00001085
Iteration 137/1000 | Loss: 0.00001085
Iteration 138/1000 | Loss: 0.00001084
Iteration 139/1000 | Loss: 0.00001084
Iteration 140/1000 | Loss: 0.00001084
Iteration 141/1000 | Loss: 0.00001084
Iteration 142/1000 | Loss: 0.00001084
Iteration 143/1000 | Loss: 0.00001084
Iteration 144/1000 | Loss: 0.00001084
Iteration 145/1000 | Loss: 0.00001083
Iteration 146/1000 | Loss: 0.00001083
Iteration 147/1000 | Loss: 0.00001083
Iteration 148/1000 | Loss: 0.00001083
Iteration 149/1000 | Loss: 0.00001083
Iteration 150/1000 | Loss: 0.00001083
Iteration 151/1000 | Loss: 0.00001083
Iteration 152/1000 | Loss: 0.00001082
Iteration 153/1000 | Loss: 0.00001082
Iteration 154/1000 | Loss: 0.00001082
Iteration 155/1000 | Loss: 0.00001082
Iteration 156/1000 | Loss: 0.00001082
Iteration 157/1000 | Loss: 0.00001082
Iteration 158/1000 | Loss: 0.00001082
Iteration 159/1000 | Loss: 0.00001081
Iteration 160/1000 | Loss: 0.00001081
Iteration 161/1000 | Loss: 0.00001081
Iteration 162/1000 | Loss: 0.00001081
Iteration 163/1000 | Loss: 0.00001081
Iteration 164/1000 | Loss: 0.00001081
Iteration 165/1000 | Loss: 0.00001081
Iteration 166/1000 | Loss: 0.00001081
Iteration 167/1000 | Loss: 0.00001080
Iteration 168/1000 | Loss: 0.00001080
Iteration 169/1000 | Loss: 0.00001080
Iteration 170/1000 | Loss: 0.00001080
Iteration 171/1000 | Loss: 0.00001080
Iteration 172/1000 | Loss: 0.00001080
Iteration 173/1000 | Loss: 0.00001080
Iteration 174/1000 | Loss: 0.00001080
Iteration 175/1000 | Loss: 0.00001080
Iteration 176/1000 | Loss: 0.00001080
Iteration 177/1000 | Loss: 0.00001079
Iteration 178/1000 | Loss: 0.00001079
Iteration 179/1000 | Loss: 0.00001079
Iteration 180/1000 | Loss: 0.00001079
Iteration 181/1000 | Loss: 0.00001079
Iteration 182/1000 | Loss: 0.00001079
Iteration 183/1000 | Loss: 0.00001079
Iteration 184/1000 | Loss: 0.00001079
Iteration 185/1000 | Loss: 0.00001078
Iteration 186/1000 | Loss: 0.00001078
Iteration 187/1000 | Loss: 0.00001078
Iteration 188/1000 | Loss: 0.00001078
Iteration 189/1000 | Loss: 0.00001078
Iteration 190/1000 | Loss: 0.00001078
Iteration 191/1000 | Loss: 0.00001078
Iteration 192/1000 | Loss: 0.00001078
Iteration 193/1000 | Loss: 0.00001078
Iteration 194/1000 | Loss: 0.00001078
Iteration 195/1000 | Loss: 0.00001078
Iteration 196/1000 | Loss: 0.00001077
Iteration 197/1000 | Loss: 0.00001077
Iteration 198/1000 | Loss: 0.00001077
Iteration 199/1000 | Loss: 0.00001077
Iteration 200/1000 | Loss: 0.00001077
Iteration 201/1000 | Loss: 0.00001077
Iteration 202/1000 | Loss: 0.00001077
Iteration 203/1000 | Loss: 0.00001077
Iteration 204/1000 | Loss: 0.00001077
Iteration 205/1000 | Loss: 0.00001077
Iteration 206/1000 | Loss: 0.00001076
Iteration 207/1000 | Loss: 0.00001076
Iteration 208/1000 | Loss: 0.00001076
Iteration 209/1000 | Loss: 0.00001076
Iteration 210/1000 | Loss: 0.00001076
Iteration 211/1000 | Loss: 0.00001076
Iteration 212/1000 | Loss: 0.00001076
Iteration 213/1000 | Loss: 0.00001076
Iteration 214/1000 | Loss: 0.00001076
Iteration 215/1000 | Loss: 0.00001076
Iteration 216/1000 | Loss: 0.00001076
Iteration 217/1000 | Loss: 0.00001076
Iteration 218/1000 | Loss: 0.00001076
Iteration 219/1000 | Loss: 0.00001076
Iteration 220/1000 | Loss: 0.00001076
Iteration 221/1000 | Loss: 0.00001076
Iteration 222/1000 | Loss: 0.00001076
Iteration 223/1000 | Loss: 0.00001075
Iteration 224/1000 | Loss: 0.00001075
Iteration 225/1000 | Loss: 0.00001075
Iteration 226/1000 | Loss: 0.00001075
Iteration 227/1000 | Loss: 0.00001075
Iteration 228/1000 | Loss: 0.00001075
Iteration 229/1000 | Loss: 0.00001075
Iteration 230/1000 | Loss: 0.00001075
Iteration 231/1000 | Loss: 0.00001075
Iteration 232/1000 | Loss: 0.00001075
Iteration 233/1000 | Loss: 0.00001075
Iteration 234/1000 | Loss: 0.00001075
Iteration 235/1000 | Loss: 0.00001074
Iteration 236/1000 | Loss: 0.00001074
Iteration 237/1000 | Loss: 0.00001074
Iteration 238/1000 | Loss: 0.00001074
Iteration 239/1000 | Loss: 0.00001074
Iteration 240/1000 | Loss: 0.00001074
Iteration 241/1000 | Loss: 0.00001074
Iteration 242/1000 | Loss: 0.00001074
Iteration 243/1000 | Loss: 0.00001074
Iteration 244/1000 | Loss: 0.00001074
Iteration 245/1000 | Loss: 0.00001074
Iteration 246/1000 | Loss: 0.00001073
Iteration 247/1000 | Loss: 0.00001073
Iteration 248/1000 | Loss: 0.00001073
Iteration 249/1000 | Loss: 0.00001073
Iteration 250/1000 | Loss: 0.00001073
Iteration 251/1000 | Loss: 0.00001073
Iteration 252/1000 | Loss: 0.00001073
Iteration 253/1000 | Loss: 0.00001073
Iteration 254/1000 | Loss: 0.00001072
Iteration 255/1000 | Loss: 0.00001072
Iteration 256/1000 | Loss: 0.00001072
Iteration 257/1000 | Loss: 0.00001072
Iteration 258/1000 | Loss: 0.00001072
Iteration 259/1000 | Loss: 0.00001072
Iteration 260/1000 | Loss: 0.00001072
Iteration 261/1000 | Loss: 0.00001072
Iteration 262/1000 | Loss: 0.00001072
Iteration 263/1000 | Loss: 0.00001072
Iteration 264/1000 | Loss: 0.00001072
Iteration 265/1000 | Loss: 0.00001072
Iteration 266/1000 | Loss: 0.00001072
Iteration 267/1000 | Loss: 0.00001072
Iteration 268/1000 | Loss: 0.00001071
Iteration 269/1000 | Loss: 0.00001071
Iteration 270/1000 | Loss: 0.00001071
Iteration 271/1000 | Loss: 0.00001071
Iteration 272/1000 | Loss: 0.00001071
Iteration 273/1000 | Loss: 0.00001071
Iteration 274/1000 | Loss: 0.00001071
Iteration 275/1000 | Loss: 0.00001071
Iteration 276/1000 | Loss: 0.00001071
Iteration 277/1000 | Loss: 0.00001071
Iteration 278/1000 | Loss: 0.00001071
Iteration 279/1000 | Loss: 0.00001071
Iteration 280/1000 | Loss: 0.00001071
Iteration 281/1000 | Loss: 0.00001071
Iteration 282/1000 | Loss: 0.00001071
Iteration 283/1000 | Loss: 0.00001071
Iteration 284/1000 | Loss: 0.00001071
Iteration 285/1000 | Loss: 0.00001071
Iteration 286/1000 | Loss: 0.00001071
Iteration 287/1000 | Loss: 0.00001071
Iteration 288/1000 | Loss: 0.00001071
Iteration 289/1000 | Loss: 0.00001071
Iteration 290/1000 | Loss: 0.00001071
Iteration 291/1000 | Loss: 0.00001071
Iteration 292/1000 | Loss: 0.00001071
Iteration 293/1000 | Loss: 0.00001071
Iteration 294/1000 | Loss: 0.00001071
Iteration 295/1000 | Loss: 0.00001071
Iteration 296/1000 | Loss: 0.00001071
Iteration 297/1000 | Loss: 0.00001071
Iteration 298/1000 | Loss: 0.00001071
Iteration 299/1000 | Loss: 0.00001071
Iteration 300/1000 | Loss: 0.00001071
Iteration 301/1000 | Loss: 0.00001071
Iteration 302/1000 | Loss: 0.00001071
Iteration 303/1000 | Loss: 0.00001071
Iteration 304/1000 | Loss: 0.00001071
Iteration 305/1000 | Loss: 0.00001071
Iteration 306/1000 | Loss: 0.00001071
Iteration 307/1000 | Loss: 0.00001071
Iteration 308/1000 | Loss: 0.00001071
Iteration 309/1000 | Loss: 0.00001071
Iteration 310/1000 | Loss: 0.00001071
Iteration 311/1000 | Loss: 0.00001071
Iteration 312/1000 | Loss: 0.00001071
Iteration 313/1000 | Loss: 0.00001071
Iteration 314/1000 | Loss: 0.00001071
Iteration 315/1000 | Loss: 0.00001071
Iteration 316/1000 | Loss: 0.00001071
Iteration 317/1000 | Loss: 0.00001071
Iteration 318/1000 | Loss: 0.00001071
Iteration 319/1000 | Loss: 0.00001071
Iteration 320/1000 | Loss: 0.00001071
Iteration 321/1000 | Loss: 0.00001071
Iteration 322/1000 | Loss: 0.00001071
Iteration 323/1000 | Loss: 0.00001071
Iteration 324/1000 | Loss: 0.00001071
Iteration 325/1000 | Loss: 0.00001071
Iteration 326/1000 | Loss: 0.00001071
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 326. Stopping optimization.
Last 5 losses: [1.0706882676458918e-05, 1.0706882676458918e-05, 1.0706882676458918e-05, 1.0706882676458918e-05, 1.0706882676458918e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0706882676458918e-05

Optimization complete. Final v2v error: 2.793760299682617 mm

Highest mean error: 3.5972378253936768 mm for frame 55

Lowest mean error: 2.6358113288879395 mm for frame 109

Saving results

Total time: 44.468968629837036
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00810093
Iteration 2/25 | Loss: 0.00145916
Iteration 3/25 | Loss: 0.00124293
Iteration 4/25 | Loss: 0.00122254
Iteration 5/25 | Loss: 0.00121633
Iteration 6/25 | Loss: 0.00121500
Iteration 7/25 | Loss: 0.00121500
Iteration 8/25 | Loss: 0.00121500
Iteration 9/25 | Loss: 0.00121500
Iteration 10/25 | Loss: 0.00121500
Iteration 11/25 | Loss: 0.00121500
Iteration 12/25 | Loss: 0.00121500
Iteration 13/25 | Loss: 0.00121500
Iteration 14/25 | Loss: 0.00121500
Iteration 15/25 | Loss: 0.00121500
Iteration 16/25 | Loss: 0.00121500
Iteration 17/25 | Loss: 0.00121500
Iteration 18/25 | Loss: 0.00121500
Iteration 19/25 | Loss: 0.00121500
Iteration 20/25 | Loss: 0.00121500
Iteration 21/25 | Loss: 0.00121500
Iteration 22/25 | Loss: 0.00121500
Iteration 23/25 | Loss: 0.00121500
Iteration 24/25 | Loss: 0.00121500
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.001215000287629664, 0.001215000287629664, 0.001215000287629664, 0.001215000287629664, 0.001215000287629664]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001215000287629664

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.79428554
Iteration 2/25 | Loss: 0.00081557
Iteration 3/25 | Loss: 0.00081557
Iteration 4/25 | Loss: 0.00081557
Iteration 5/25 | Loss: 0.00081557
Iteration 6/25 | Loss: 0.00081557
Iteration 7/25 | Loss: 0.00081557
Iteration 8/25 | Loss: 0.00081557
Iteration 9/25 | Loss: 0.00081557
Iteration 10/25 | Loss: 0.00081557
Iteration 11/25 | Loss: 0.00081557
Iteration 12/25 | Loss: 0.00081557
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008155658724717796, 0.0008155658724717796, 0.0008155658724717796, 0.0008155658724717796, 0.0008155658724717796]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008155658724717796

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081557
Iteration 2/1000 | Loss: 0.00002301
Iteration 3/1000 | Loss: 0.00001717
Iteration 4/1000 | Loss: 0.00001543
Iteration 5/1000 | Loss: 0.00001469
Iteration 6/1000 | Loss: 0.00001403
Iteration 7/1000 | Loss: 0.00001376
Iteration 8/1000 | Loss: 0.00001345
Iteration 9/1000 | Loss: 0.00001316
Iteration 10/1000 | Loss: 0.00001306
Iteration 11/1000 | Loss: 0.00001289
Iteration 12/1000 | Loss: 0.00001281
Iteration 13/1000 | Loss: 0.00001272
Iteration 14/1000 | Loss: 0.00001268
Iteration 15/1000 | Loss: 0.00001267
Iteration 16/1000 | Loss: 0.00001265
Iteration 17/1000 | Loss: 0.00001263
Iteration 18/1000 | Loss: 0.00001262
Iteration 19/1000 | Loss: 0.00001261
Iteration 20/1000 | Loss: 0.00001261
Iteration 21/1000 | Loss: 0.00001261
Iteration 22/1000 | Loss: 0.00001261
Iteration 23/1000 | Loss: 0.00001260
Iteration 24/1000 | Loss: 0.00001260
Iteration 25/1000 | Loss: 0.00001259
Iteration 26/1000 | Loss: 0.00001259
Iteration 27/1000 | Loss: 0.00001258
Iteration 28/1000 | Loss: 0.00001258
Iteration 29/1000 | Loss: 0.00001257
Iteration 30/1000 | Loss: 0.00001257
Iteration 31/1000 | Loss: 0.00001254
Iteration 32/1000 | Loss: 0.00001253
Iteration 33/1000 | Loss: 0.00001252
Iteration 34/1000 | Loss: 0.00001252
Iteration 35/1000 | Loss: 0.00001252
Iteration 36/1000 | Loss: 0.00001251
Iteration 37/1000 | Loss: 0.00001250
Iteration 38/1000 | Loss: 0.00001249
Iteration 39/1000 | Loss: 0.00001247
Iteration 40/1000 | Loss: 0.00001246
Iteration 41/1000 | Loss: 0.00001245
Iteration 42/1000 | Loss: 0.00001244
Iteration 43/1000 | Loss: 0.00001243
Iteration 44/1000 | Loss: 0.00001243
Iteration 45/1000 | Loss: 0.00001242
Iteration 46/1000 | Loss: 0.00001242
Iteration 47/1000 | Loss: 0.00001241
Iteration 48/1000 | Loss: 0.00001241
Iteration 49/1000 | Loss: 0.00001240
Iteration 50/1000 | Loss: 0.00001240
Iteration 51/1000 | Loss: 0.00001239
Iteration 52/1000 | Loss: 0.00001239
Iteration 53/1000 | Loss: 0.00001239
Iteration 54/1000 | Loss: 0.00001238
Iteration 55/1000 | Loss: 0.00001237
Iteration 56/1000 | Loss: 0.00001237
Iteration 57/1000 | Loss: 0.00001237
Iteration 58/1000 | Loss: 0.00001237
Iteration 59/1000 | Loss: 0.00001236
Iteration 60/1000 | Loss: 0.00001236
Iteration 61/1000 | Loss: 0.00001235
Iteration 62/1000 | Loss: 0.00001235
Iteration 63/1000 | Loss: 0.00001235
Iteration 64/1000 | Loss: 0.00001234
Iteration 65/1000 | Loss: 0.00001234
Iteration 66/1000 | Loss: 0.00001233
Iteration 67/1000 | Loss: 0.00001233
Iteration 68/1000 | Loss: 0.00001232
Iteration 69/1000 | Loss: 0.00001232
Iteration 70/1000 | Loss: 0.00001232
Iteration 71/1000 | Loss: 0.00001232
Iteration 72/1000 | Loss: 0.00001231
Iteration 73/1000 | Loss: 0.00001231
Iteration 74/1000 | Loss: 0.00001231
Iteration 75/1000 | Loss: 0.00001231
Iteration 76/1000 | Loss: 0.00001230
Iteration 77/1000 | Loss: 0.00001230
Iteration 78/1000 | Loss: 0.00001230
Iteration 79/1000 | Loss: 0.00001229
Iteration 80/1000 | Loss: 0.00001229
Iteration 81/1000 | Loss: 0.00001229
Iteration 82/1000 | Loss: 0.00001227
Iteration 83/1000 | Loss: 0.00001227
Iteration 84/1000 | Loss: 0.00001226
Iteration 85/1000 | Loss: 0.00001226
Iteration 86/1000 | Loss: 0.00001226
Iteration 87/1000 | Loss: 0.00001226
Iteration 88/1000 | Loss: 0.00001226
Iteration 89/1000 | Loss: 0.00001225
Iteration 90/1000 | Loss: 0.00001225
Iteration 91/1000 | Loss: 0.00001224
Iteration 92/1000 | Loss: 0.00001224
Iteration 93/1000 | Loss: 0.00001224
Iteration 94/1000 | Loss: 0.00001224
Iteration 95/1000 | Loss: 0.00001224
Iteration 96/1000 | Loss: 0.00001224
Iteration 97/1000 | Loss: 0.00001224
Iteration 98/1000 | Loss: 0.00001223
Iteration 99/1000 | Loss: 0.00001223
Iteration 100/1000 | Loss: 0.00001222
Iteration 101/1000 | Loss: 0.00001222
Iteration 102/1000 | Loss: 0.00001222
Iteration 103/1000 | Loss: 0.00001222
Iteration 104/1000 | Loss: 0.00001222
Iteration 105/1000 | Loss: 0.00001222
Iteration 106/1000 | Loss: 0.00001222
Iteration 107/1000 | Loss: 0.00001221
Iteration 108/1000 | Loss: 0.00001221
Iteration 109/1000 | Loss: 0.00001221
Iteration 110/1000 | Loss: 0.00001221
Iteration 111/1000 | Loss: 0.00001221
Iteration 112/1000 | Loss: 0.00001221
Iteration 113/1000 | Loss: 0.00001221
Iteration 114/1000 | Loss: 0.00001220
Iteration 115/1000 | Loss: 0.00001220
Iteration 116/1000 | Loss: 0.00001220
Iteration 117/1000 | Loss: 0.00001219
Iteration 118/1000 | Loss: 0.00001219
Iteration 119/1000 | Loss: 0.00001219
Iteration 120/1000 | Loss: 0.00001219
Iteration 121/1000 | Loss: 0.00001219
Iteration 122/1000 | Loss: 0.00001219
Iteration 123/1000 | Loss: 0.00001219
Iteration 124/1000 | Loss: 0.00001219
Iteration 125/1000 | Loss: 0.00001219
Iteration 126/1000 | Loss: 0.00001219
Iteration 127/1000 | Loss: 0.00001219
Iteration 128/1000 | Loss: 0.00001219
Iteration 129/1000 | Loss: 0.00001218
Iteration 130/1000 | Loss: 0.00001218
Iteration 131/1000 | Loss: 0.00001218
Iteration 132/1000 | Loss: 0.00001218
Iteration 133/1000 | Loss: 0.00001218
Iteration 134/1000 | Loss: 0.00001218
Iteration 135/1000 | Loss: 0.00001218
Iteration 136/1000 | Loss: 0.00001218
Iteration 137/1000 | Loss: 0.00001218
Iteration 138/1000 | Loss: 0.00001218
Iteration 139/1000 | Loss: 0.00001218
Iteration 140/1000 | Loss: 0.00001218
Iteration 141/1000 | Loss: 0.00001217
Iteration 142/1000 | Loss: 0.00001217
Iteration 143/1000 | Loss: 0.00001217
Iteration 144/1000 | Loss: 0.00001217
Iteration 145/1000 | Loss: 0.00001217
Iteration 146/1000 | Loss: 0.00001217
Iteration 147/1000 | Loss: 0.00001217
Iteration 148/1000 | Loss: 0.00001217
Iteration 149/1000 | Loss: 0.00001217
Iteration 150/1000 | Loss: 0.00001217
Iteration 151/1000 | Loss: 0.00001217
Iteration 152/1000 | Loss: 0.00001216
Iteration 153/1000 | Loss: 0.00001216
Iteration 154/1000 | Loss: 0.00001216
Iteration 155/1000 | Loss: 0.00001216
Iteration 156/1000 | Loss: 0.00001216
Iteration 157/1000 | Loss: 0.00001216
Iteration 158/1000 | Loss: 0.00001215
Iteration 159/1000 | Loss: 0.00001215
Iteration 160/1000 | Loss: 0.00001215
Iteration 161/1000 | Loss: 0.00001214
Iteration 162/1000 | Loss: 0.00001214
Iteration 163/1000 | Loss: 0.00001214
Iteration 164/1000 | Loss: 0.00001214
Iteration 165/1000 | Loss: 0.00001214
Iteration 166/1000 | Loss: 0.00001214
Iteration 167/1000 | Loss: 0.00001214
Iteration 168/1000 | Loss: 0.00001214
Iteration 169/1000 | Loss: 0.00001214
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.2142901141487528e-05, 1.2142901141487528e-05, 1.2142901141487528e-05, 1.2142901141487528e-05, 1.2142901141487528e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2142901141487528e-05

Optimization complete. Final v2v error: 2.9678239822387695 mm

Highest mean error: 3.237974166870117 mm for frame 177

Lowest mean error: 2.741633892059326 mm for frame 49

Saving results

Total time: 42.925251722335815
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00811402
Iteration 2/25 | Loss: 0.00228525
Iteration 3/25 | Loss: 0.00188724
Iteration 4/25 | Loss: 0.00175312
Iteration 5/25 | Loss: 0.00149316
Iteration 6/25 | Loss: 0.00141213
Iteration 7/25 | Loss: 0.00140907
Iteration 8/25 | Loss: 0.00140899
Iteration 9/25 | Loss: 0.00140899
Iteration 10/25 | Loss: 0.00140899
Iteration 11/25 | Loss: 0.00140899
Iteration 12/25 | Loss: 0.00140899
Iteration 13/25 | Loss: 0.00140899
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001408991520293057, 0.001408991520293057, 0.001408991520293057, 0.001408991520293057, 0.001408991520293057]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001408991520293057

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43320441
Iteration 2/25 | Loss: 0.00073280
Iteration 3/25 | Loss: 0.00073279
Iteration 4/25 | Loss: 0.00073278
Iteration 5/25 | Loss: 0.00073278
Iteration 6/25 | Loss: 0.00073278
Iteration 7/25 | Loss: 0.00073278
Iteration 8/25 | Loss: 0.00073278
Iteration 9/25 | Loss: 0.00073278
Iteration 10/25 | Loss: 0.00073278
Iteration 11/25 | Loss: 0.00073278
Iteration 12/25 | Loss: 0.00073278
Iteration 13/25 | Loss: 0.00073278
Iteration 14/25 | Loss: 0.00073278
Iteration 15/25 | Loss: 0.00073278
Iteration 16/25 | Loss: 0.00073278
Iteration 17/25 | Loss: 0.00073278
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007327813073061407, 0.0007327813073061407, 0.0007327813073061407, 0.0007327813073061407, 0.0007327813073061407]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007327813073061407

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073278
Iteration 2/1000 | Loss: 0.00003956
Iteration 3/1000 | Loss: 0.00002773
Iteration 4/1000 | Loss: 0.00002527
Iteration 5/1000 | Loss: 0.00002397
Iteration 6/1000 | Loss: 0.00002327
Iteration 7/1000 | Loss: 0.00002287
Iteration 8/1000 | Loss: 0.00002259
Iteration 9/1000 | Loss: 0.00002248
Iteration 10/1000 | Loss: 0.00002245
Iteration 11/1000 | Loss: 0.00002226
Iteration 12/1000 | Loss: 0.00002225
Iteration 13/1000 | Loss: 0.00002213
Iteration 14/1000 | Loss: 0.00002210
Iteration 15/1000 | Loss: 0.00002209
Iteration 16/1000 | Loss: 0.00002209
Iteration 17/1000 | Loss: 0.00002208
Iteration 18/1000 | Loss: 0.00002208
Iteration 19/1000 | Loss: 0.00002208
Iteration 20/1000 | Loss: 0.00002207
Iteration 21/1000 | Loss: 0.00002207
Iteration 22/1000 | Loss: 0.00002207
Iteration 23/1000 | Loss: 0.00002206
Iteration 24/1000 | Loss: 0.00002199
Iteration 25/1000 | Loss: 0.00002198
Iteration 26/1000 | Loss: 0.00002197
Iteration 27/1000 | Loss: 0.00002197
Iteration 28/1000 | Loss: 0.00002197
Iteration 29/1000 | Loss: 0.00002197
Iteration 30/1000 | Loss: 0.00002197
Iteration 31/1000 | Loss: 0.00002197
Iteration 32/1000 | Loss: 0.00002197
Iteration 33/1000 | Loss: 0.00002197
Iteration 34/1000 | Loss: 0.00002197
Iteration 35/1000 | Loss: 0.00002196
Iteration 36/1000 | Loss: 0.00002196
Iteration 37/1000 | Loss: 0.00002196
Iteration 38/1000 | Loss: 0.00002196
Iteration 39/1000 | Loss: 0.00002196
Iteration 40/1000 | Loss: 0.00002195
Iteration 41/1000 | Loss: 0.00002195
Iteration 42/1000 | Loss: 0.00002194
Iteration 43/1000 | Loss: 0.00002193
Iteration 44/1000 | Loss: 0.00002193
Iteration 45/1000 | Loss: 0.00002193
Iteration 46/1000 | Loss: 0.00002193
Iteration 47/1000 | Loss: 0.00002193
Iteration 48/1000 | Loss: 0.00002193
Iteration 49/1000 | Loss: 0.00002193
Iteration 50/1000 | Loss: 0.00002192
Iteration 51/1000 | Loss: 0.00002192
Iteration 52/1000 | Loss: 0.00002192
Iteration 53/1000 | Loss: 0.00002192
Iteration 54/1000 | Loss: 0.00002192
Iteration 55/1000 | Loss: 0.00002192
Iteration 56/1000 | Loss: 0.00002192
Iteration 57/1000 | Loss: 0.00002191
Iteration 58/1000 | Loss: 0.00002191
Iteration 59/1000 | Loss: 0.00002191
Iteration 60/1000 | Loss: 0.00002191
Iteration 61/1000 | Loss: 0.00002191
Iteration 62/1000 | Loss: 0.00002190
Iteration 63/1000 | Loss: 0.00002190
Iteration 64/1000 | Loss: 0.00002190
Iteration 65/1000 | Loss: 0.00002190
Iteration 66/1000 | Loss: 0.00002189
Iteration 67/1000 | Loss: 0.00002189
Iteration 68/1000 | Loss: 0.00002189
Iteration 69/1000 | Loss: 0.00002189
Iteration 70/1000 | Loss: 0.00002188
Iteration 71/1000 | Loss: 0.00002188
Iteration 72/1000 | Loss: 0.00002188
Iteration 73/1000 | Loss: 0.00002188
Iteration 74/1000 | Loss: 0.00002187
Iteration 75/1000 | Loss: 0.00002187
Iteration 76/1000 | Loss: 0.00002187
Iteration 77/1000 | Loss: 0.00002187
Iteration 78/1000 | Loss: 0.00002186
Iteration 79/1000 | Loss: 0.00002186
Iteration 80/1000 | Loss: 0.00002186
Iteration 81/1000 | Loss: 0.00002186
Iteration 82/1000 | Loss: 0.00002186
Iteration 83/1000 | Loss: 0.00002186
Iteration 84/1000 | Loss: 0.00002186
Iteration 85/1000 | Loss: 0.00002186
Iteration 86/1000 | Loss: 0.00002186
Iteration 87/1000 | Loss: 0.00002186
Iteration 88/1000 | Loss: 0.00002185
Iteration 89/1000 | Loss: 0.00002185
Iteration 90/1000 | Loss: 0.00002185
Iteration 91/1000 | Loss: 0.00002184
Iteration 92/1000 | Loss: 0.00002184
Iteration 93/1000 | Loss: 0.00002184
Iteration 94/1000 | Loss: 0.00002184
Iteration 95/1000 | Loss: 0.00002183
Iteration 96/1000 | Loss: 0.00002183
Iteration 97/1000 | Loss: 0.00002183
Iteration 98/1000 | Loss: 0.00002182
Iteration 99/1000 | Loss: 0.00002182
Iteration 100/1000 | Loss: 0.00002182
Iteration 101/1000 | Loss: 0.00002182
Iteration 102/1000 | Loss: 0.00002182
Iteration 103/1000 | Loss: 0.00002182
Iteration 104/1000 | Loss: 0.00002182
Iteration 105/1000 | Loss: 0.00002181
Iteration 106/1000 | Loss: 0.00002181
Iteration 107/1000 | Loss: 0.00002181
Iteration 108/1000 | Loss: 0.00002181
Iteration 109/1000 | Loss: 0.00002181
Iteration 110/1000 | Loss: 0.00002180
Iteration 111/1000 | Loss: 0.00002180
Iteration 112/1000 | Loss: 0.00002180
Iteration 113/1000 | Loss: 0.00002180
Iteration 114/1000 | Loss: 0.00002180
Iteration 115/1000 | Loss: 0.00002180
Iteration 116/1000 | Loss: 0.00002180
Iteration 117/1000 | Loss: 0.00002180
Iteration 118/1000 | Loss: 0.00002180
Iteration 119/1000 | Loss: 0.00002180
Iteration 120/1000 | Loss: 0.00002180
Iteration 121/1000 | Loss: 0.00002180
Iteration 122/1000 | Loss: 0.00002180
Iteration 123/1000 | Loss: 0.00002180
Iteration 124/1000 | Loss: 0.00002180
Iteration 125/1000 | Loss: 0.00002179
Iteration 126/1000 | Loss: 0.00002179
Iteration 127/1000 | Loss: 0.00002179
Iteration 128/1000 | Loss: 0.00002179
Iteration 129/1000 | Loss: 0.00002179
Iteration 130/1000 | Loss: 0.00002179
Iteration 131/1000 | Loss: 0.00002179
Iteration 132/1000 | Loss: 0.00002179
Iteration 133/1000 | Loss: 0.00002179
Iteration 134/1000 | Loss: 0.00002179
Iteration 135/1000 | Loss: 0.00002179
Iteration 136/1000 | Loss: 0.00002179
Iteration 137/1000 | Loss: 0.00002179
Iteration 138/1000 | Loss: 0.00002179
Iteration 139/1000 | Loss: 0.00002179
Iteration 140/1000 | Loss: 0.00002179
Iteration 141/1000 | Loss: 0.00002179
Iteration 142/1000 | Loss: 0.00002179
Iteration 143/1000 | Loss: 0.00002179
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [2.1787718651467003e-05, 2.1787718651467003e-05, 2.1787718651467003e-05, 2.1787718651467003e-05, 2.1787718651467003e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1787718651467003e-05

Optimization complete. Final v2v error: 3.9560720920562744 mm

Highest mean error: 4.148123741149902 mm for frame 202

Lowest mean error: 3.78997802734375 mm for frame 10

Saving results

Total time: 40.273239850997925
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00424234
Iteration 2/25 | Loss: 0.00124700
Iteration 3/25 | Loss: 0.00119427
Iteration 4/25 | Loss: 0.00118846
Iteration 5/25 | Loss: 0.00118695
Iteration 6/25 | Loss: 0.00118695
Iteration 7/25 | Loss: 0.00118695
Iteration 8/25 | Loss: 0.00118695
Iteration 9/25 | Loss: 0.00118695
Iteration 10/25 | Loss: 0.00118695
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001186952693387866, 0.001186952693387866, 0.001186952693387866, 0.001186952693387866, 0.001186952693387866]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001186952693387866

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.84510374
Iteration 2/25 | Loss: 0.00078826
Iteration 3/25 | Loss: 0.00078826
Iteration 4/25 | Loss: 0.00078826
Iteration 5/25 | Loss: 0.00078826
Iteration 6/25 | Loss: 0.00078826
Iteration 7/25 | Loss: 0.00078826
Iteration 8/25 | Loss: 0.00078826
Iteration 9/25 | Loss: 0.00078826
Iteration 10/25 | Loss: 0.00078826
Iteration 11/25 | Loss: 0.00078826
Iteration 12/25 | Loss: 0.00078826
Iteration 13/25 | Loss: 0.00078826
Iteration 14/25 | Loss: 0.00078826
Iteration 15/25 | Loss: 0.00078826
Iteration 16/25 | Loss: 0.00078826
Iteration 17/25 | Loss: 0.00078826
Iteration 18/25 | Loss: 0.00078826
Iteration 19/25 | Loss: 0.00078826
Iteration 20/25 | Loss: 0.00078826
Iteration 21/25 | Loss: 0.00078826
Iteration 22/25 | Loss: 0.00078826
Iteration 23/25 | Loss: 0.00078826
Iteration 24/25 | Loss: 0.00078826
Iteration 25/25 | Loss: 0.00078826

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078826
Iteration 2/1000 | Loss: 0.00002566
Iteration 3/1000 | Loss: 0.00001830
Iteration 4/1000 | Loss: 0.00001573
Iteration 5/1000 | Loss: 0.00001454
Iteration 6/1000 | Loss: 0.00001390
Iteration 7/1000 | Loss: 0.00001334
Iteration 8/1000 | Loss: 0.00001311
Iteration 9/1000 | Loss: 0.00001287
Iteration 10/1000 | Loss: 0.00001270
Iteration 11/1000 | Loss: 0.00001270
Iteration 12/1000 | Loss: 0.00001269
Iteration 13/1000 | Loss: 0.00001268
Iteration 14/1000 | Loss: 0.00001263
Iteration 15/1000 | Loss: 0.00001262
Iteration 16/1000 | Loss: 0.00001261
Iteration 17/1000 | Loss: 0.00001259
Iteration 18/1000 | Loss: 0.00001255
Iteration 19/1000 | Loss: 0.00001241
Iteration 20/1000 | Loss: 0.00001237
Iteration 21/1000 | Loss: 0.00001233
Iteration 22/1000 | Loss: 0.00001232
Iteration 23/1000 | Loss: 0.00001229
Iteration 24/1000 | Loss: 0.00001229
Iteration 25/1000 | Loss: 0.00001229
Iteration 26/1000 | Loss: 0.00001228
Iteration 27/1000 | Loss: 0.00001228
Iteration 28/1000 | Loss: 0.00001228
Iteration 29/1000 | Loss: 0.00001226
Iteration 30/1000 | Loss: 0.00001226
Iteration 31/1000 | Loss: 0.00001225
Iteration 32/1000 | Loss: 0.00001224
Iteration 33/1000 | Loss: 0.00001223
Iteration 34/1000 | Loss: 0.00001222
Iteration 35/1000 | Loss: 0.00001222
Iteration 36/1000 | Loss: 0.00001221
Iteration 37/1000 | Loss: 0.00001220
Iteration 38/1000 | Loss: 0.00001219
Iteration 39/1000 | Loss: 0.00001219
Iteration 40/1000 | Loss: 0.00001219
Iteration 41/1000 | Loss: 0.00001219
Iteration 42/1000 | Loss: 0.00001218
Iteration 43/1000 | Loss: 0.00001217
Iteration 44/1000 | Loss: 0.00001216
Iteration 45/1000 | Loss: 0.00001216
Iteration 46/1000 | Loss: 0.00001215
Iteration 47/1000 | Loss: 0.00001214
Iteration 48/1000 | Loss: 0.00001213
Iteration 49/1000 | Loss: 0.00001213
Iteration 50/1000 | Loss: 0.00001213
Iteration 51/1000 | Loss: 0.00001210
Iteration 52/1000 | Loss: 0.00001210
Iteration 53/1000 | Loss: 0.00001209
Iteration 54/1000 | Loss: 0.00001209
Iteration 55/1000 | Loss: 0.00001209
Iteration 56/1000 | Loss: 0.00001208
Iteration 57/1000 | Loss: 0.00001208
Iteration 58/1000 | Loss: 0.00001207
Iteration 59/1000 | Loss: 0.00001207
Iteration 60/1000 | Loss: 0.00001206
Iteration 61/1000 | Loss: 0.00001205
Iteration 62/1000 | Loss: 0.00001205
Iteration 63/1000 | Loss: 0.00001204
Iteration 64/1000 | Loss: 0.00001204
Iteration 65/1000 | Loss: 0.00001203
Iteration 66/1000 | Loss: 0.00001203
Iteration 67/1000 | Loss: 0.00001202
Iteration 68/1000 | Loss: 0.00001202
Iteration 69/1000 | Loss: 0.00001202
Iteration 70/1000 | Loss: 0.00001201
Iteration 71/1000 | Loss: 0.00001201
Iteration 72/1000 | Loss: 0.00001200
Iteration 73/1000 | Loss: 0.00001199
Iteration 74/1000 | Loss: 0.00001199
Iteration 75/1000 | Loss: 0.00001199
Iteration 76/1000 | Loss: 0.00001199
Iteration 77/1000 | Loss: 0.00001199
Iteration 78/1000 | Loss: 0.00001198
Iteration 79/1000 | Loss: 0.00001197
Iteration 80/1000 | Loss: 0.00001196
Iteration 81/1000 | Loss: 0.00001196
Iteration 82/1000 | Loss: 0.00001196
Iteration 83/1000 | Loss: 0.00001196
Iteration 84/1000 | Loss: 0.00001195
Iteration 85/1000 | Loss: 0.00001195
Iteration 86/1000 | Loss: 0.00001194
Iteration 87/1000 | Loss: 0.00001194
Iteration 88/1000 | Loss: 0.00001193
Iteration 89/1000 | Loss: 0.00001193
Iteration 90/1000 | Loss: 0.00001193
Iteration 91/1000 | Loss: 0.00001192
Iteration 92/1000 | Loss: 0.00001192
Iteration 93/1000 | Loss: 0.00001192
Iteration 94/1000 | Loss: 0.00001192
Iteration 95/1000 | Loss: 0.00001192
Iteration 96/1000 | Loss: 0.00001192
Iteration 97/1000 | Loss: 0.00001191
Iteration 98/1000 | Loss: 0.00001191
Iteration 99/1000 | Loss: 0.00001191
Iteration 100/1000 | Loss: 0.00001190
Iteration 101/1000 | Loss: 0.00001190
Iteration 102/1000 | Loss: 0.00001189
Iteration 103/1000 | Loss: 0.00001189
Iteration 104/1000 | Loss: 0.00001189
Iteration 105/1000 | Loss: 0.00001189
Iteration 106/1000 | Loss: 0.00001189
Iteration 107/1000 | Loss: 0.00001189
Iteration 108/1000 | Loss: 0.00001189
Iteration 109/1000 | Loss: 0.00001189
Iteration 110/1000 | Loss: 0.00001189
Iteration 111/1000 | Loss: 0.00001189
Iteration 112/1000 | Loss: 0.00001189
Iteration 113/1000 | Loss: 0.00001188
Iteration 114/1000 | Loss: 0.00001188
Iteration 115/1000 | Loss: 0.00001188
Iteration 116/1000 | Loss: 0.00001188
Iteration 117/1000 | Loss: 0.00001188
Iteration 118/1000 | Loss: 0.00001188
Iteration 119/1000 | Loss: 0.00001188
Iteration 120/1000 | Loss: 0.00001188
Iteration 121/1000 | Loss: 0.00001188
Iteration 122/1000 | Loss: 0.00001188
Iteration 123/1000 | Loss: 0.00001188
Iteration 124/1000 | Loss: 0.00001188
Iteration 125/1000 | Loss: 0.00001188
Iteration 126/1000 | Loss: 0.00001188
Iteration 127/1000 | Loss: 0.00001188
Iteration 128/1000 | Loss: 0.00001188
Iteration 129/1000 | Loss: 0.00001188
Iteration 130/1000 | Loss: 0.00001188
Iteration 131/1000 | Loss: 0.00001188
Iteration 132/1000 | Loss: 0.00001188
Iteration 133/1000 | Loss: 0.00001188
Iteration 134/1000 | Loss: 0.00001188
Iteration 135/1000 | Loss: 0.00001188
Iteration 136/1000 | Loss: 0.00001188
Iteration 137/1000 | Loss: 0.00001188
Iteration 138/1000 | Loss: 0.00001188
Iteration 139/1000 | Loss: 0.00001188
Iteration 140/1000 | Loss: 0.00001188
Iteration 141/1000 | Loss: 0.00001188
Iteration 142/1000 | Loss: 0.00001188
Iteration 143/1000 | Loss: 0.00001188
Iteration 144/1000 | Loss: 0.00001188
Iteration 145/1000 | Loss: 0.00001188
Iteration 146/1000 | Loss: 0.00001188
Iteration 147/1000 | Loss: 0.00001188
Iteration 148/1000 | Loss: 0.00001188
Iteration 149/1000 | Loss: 0.00001188
Iteration 150/1000 | Loss: 0.00001188
Iteration 151/1000 | Loss: 0.00001188
Iteration 152/1000 | Loss: 0.00001188
Iteration 153/1000 | Loss: 0.00001188
Iteration 154/1000 | Loss: 0.00001188
Iteration 155/1000 | Loss: 0.00001188
Iteration 156/1000 | Loss: 0.00001188
Iteration 157/1000 | Loss: 0.00001188
Iteration 158/1000 | Loss: 0.00001188
Iteration 159/1000 | Loss: 0.00001188
Iteration 160/1000 | Loss: 0.00001188
Iteration 161/1000 | Loss: 0.00001188
Iteration 162/1000 | Loss: 0.00001188
Iteration 163/1000 | Loss: 0.00001188
Iteration 164/1000 | Loss: 0.00001188
Iteration 165/1000 | Loss: 0.00001188
Iteration 166/1000 | Loss: 0.00001188
Iteration 167/1000 | Loss: 0.00001188
Iteration 168/1000 | Loss: 0.00001188
Iteration 169/1000 | Loss: 0.00001188
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.1881629689014517e-05, 1.1881629689014517e-05, 1.1881629689014517e-05, 1.1881629689014517e-05, 1.1881629689014517e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1881629689014517e-05

Optimization complete. Final v2v error: 2.9691591262817383 mm

Highest mean error: 3.2261757850646973 mm for frame 123

Lowest mean error: 2.8006439208984375 mm for frame 178

Saving results

Total time: 36.678014278411865
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00429154
Iteration 2/25 | Loss: 0.00129409
Iteration 3/25 | Loss: 0.00122865
Iteration 4/25 | Loss: 0.00121720
Iteration 5/25 | Loss: 0.00121296
Iteration 6/25 | Loss: 0.00121183
Iteration 7/25 | Loss: 0.00121183
Iteration 8/25 | Loss: 0.00121183
Iteration 9/25 | Loss: 0.00121183
Iteration 10/25 | Loss: 0.00121183
Iteration 11/25 | Loss: 0.00121183
Iteration 12/25 | Loss: 0.00121183
Iteration 13/25 | Loss: 0.00121183
Iteration 14/25 | Loss: 0.00121183
Iteration 15/25 | Loss: 0.00121183
Iteration 16/25 | Loss: 0.00121183
Iteration 17/25 | Loss: 0.00121183
Iteration 18/25 | Loss: 0.00121183
Iteration 19/25 | Loss: 0.00121183
Iteration 20/25 | Loss: 0.00121183
Iteration 21/25 | Loss: 0.00121183
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012118268059566617, 0.0012118268059566617, 0.0012118268059566617, 0.0012118268059566617, 0.0012118268059566617]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012118268059566617

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.87777257
Iteration 2/25 | Loss: 0.00078200
Iteration 3/25 | Loss: 0.00078197
Iteration 4/25 | Loss: 0.00078197
Iteration 5/25 | Loss: 0.00078197
Iteration 6/25 | Loss: 0.00078197
Iteration 7/25 | Loss: 0.00078197
Iteration 8/25 | Loss: 0.00078197
Iteration 9/25 | Loss: 0.00078197
Iteration 10/25 | Loss: 0.00078197
Iteration 11/25 | Loss: 0.00078197
Iteration 12/25 | Loss: 0.00078197
Iteration 13/25 | Loss: 0.00078197
Iteration 14/25 | Loss: 0.00078197
Iteration 15/25 | Loss: 0.00078197
Iteration 16/25 | Loss: 0.00078197
Iteration 17/25 | Loss: 0.00078197
Iteration 18/25 | Loss: 0.00078197
Iteration 19/25 | Loss: 0.00078197
Iteration 20/25 | Loss: 0.00078197
Iteration 21/25 | Loss: 0.00078197
Iteration 22/25 | Loss: 0.00078197
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007819675956852734, 0.0007819675956852734, 0.0007819675956852734, 0.0007819675956852734, 0.0007819675956852734]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007819675956852734

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078197
Iteration 2/1000 | Loss: 0.00003372
Iteration 3/1000 | Loss: 0.00002036
Iteration 4/1000 | Loss: 0.00001764
Iteration 5/1000 | Loss: 0.00001679
Iteration 6/1000 | Loss: 0.00001585
Iteration 7/1000 | Loss: 0.00001536
Iteration 8/1000 | Loss: 0.00001481
Iteration 9/1000 | Loss: 0.00001458
Iteration 10/1000 | Loss: 0.00001428
Iteration 11/1000 | Loss: 0.00001405
Iteration 12/1000 | Loss: 0.00001393
Iteration 13/1000 | Loss: 0.00001392
Iteration 14/1000 | Loss: 0.00001380
Iteration 15/1000 | Loss: 0.00001370
Iteration 16/1000 | Loss: 0.00001365
Iteration 17/1000 | Loss: 0.00001364
Iteration 18/1000 | Loss: 0.00001363
Iteration 19/1000 | Loss: 0.00001361
Iteration 20/1000 | Loss: 0.00001360
Iteration 21/1000 | Loss: 0.00001359
Iteration 22/1000 | Loss: 0.00001358
Iteration 23/1000 | Loss: 0.00001358
Iteration 24/1000 | Loss: 0.00001357
Iteration 25/1000 | Loss: 0.00001357
Iteration 26/1000 | Loss: 0.00001357
Iteration 27/1000 | Loss: 0.00001356
Iteration 28/1000 | Loss: 0.00001356
Iteration 29/1000 | Loss: 0.00001355
Iteration 30/1000 | Loss: 0.00001354
Iteration 31/1000 | Loss: 0.00001353
Iteration 32/1000 | Loss: 0.00001352
Iteration 33/1000 | Loss: 0.00001352
Iteration 34/1000 | Loss: 0.00001352
Iteration 35/1000 | Loss: 0.00001351
Iteration 36/1000 | Loss: 0.00001351
Iteration 37/1000 | Loss: 0.00001351
Iteration 38/1000 | Loss: 0.00001351
Iteration 39/1000 | Loss: 0.00001351
Iteration 40/1000 | Loss: 0.00001351
Iteration 41/1000 | Loss: 0.00001350
Iteration 42/1000 | Loss: 0.00001349
Iteration 43/1000 | Loss: 0.00001349
Iteration 44/1000 | Loss: 0.00001349
Iteration 45/1000 | Loss: 0.00001348
Iteration 46/1000 | Loss: 0.00001347
Iteration 47/1000 | Loss: 0.00001347
Iteration 48/1000 | Loss: 0.00001347
Iteration 49/1000 | Loss: 0.00001347
Iteration 50/1000 | Loss: 0.00001346
Iteration 51/1000 | Loss: 0.00001346
Iteration 52/1000 | Loss: 0.00001345
Iteration 53/1000 | Loss: 0.00001345
Iteration 54/1000 | Loss: 0.00001344
Iteration 55/1000 | Loss: 0.00001343
Iteration 56/1000 | Loss: 0.00001343
Iteration 57/1000 | Loss: 0.00001343
Iteration 58/1000 | Loss: 0.00001343
Iteration 59/1000 | Loss: 0.00001343
Iteration 60/1000 | Loss: 0.00001343
Iteration 61/1000 | Loss: 0.00001342
Iteration 62/1000 | Loss: 0.00001341
Iteration 63/1000 | Loss: 0.00001340
Iteration 64/1000 | Loss: 0.00001340
Iteration 65/1000 | Loss: 0.00001340
Iteration 66/1000 | Loss: 0.00001339
Iteration 67/1000 | Loss: 0.00001339
Iteration 68/1000 | Loss: 0.00001339
Iteration 69/1000 | Loss: 0.00001339
Iteration 70/1000 | Loss: 0.00001338
Iteration 71/1000 | Loss: 0.00001338
Iteration 72/1000 | Loss: 0.00001338
Iteration 73/1000 | Loss: 0.00001338
Iteration 74/1000 | Loss: 0.00001338
Iteration 75/1000 | Loss: 0.00001337
Iteration 76/1000 | Loss: 0.00001337
Iteration 77/1000 | Loss: 0.00001337
Iteration 78/1000 | Loss: 0.00001337
Iteration 79/1000 | Loss: 0.00001337
Iteration 80/1000 | Loss: 0.00001336
Iteration 81/1000 | Loss: 0.00001336
Iteration 82/1000 | Loss: 0.00001336
Iteration 83/1000 | Loss: 0.00001336
Iteration 84/1000 | Loss: 0.00001336
Iteration 85/1000 | Loss: 0.00001336
Iteration 86/1000 | Loss: 0.00001335
Iteration 87/1000 | Loss: 0.00001335
Iteration 88/1000 | Loss: 0.00001335
Iteration 89/1000 | Loss: 0.00001335
Iteration 90/1000 | Loss: 0.00001335
Iteration 91/1000 | Loss: 0.00001334
Iteration 92/1000 | Loss: 0.00001334
Iteration 93/1000 | Loss: 0.00001334
Iteration 94/1000 | Loss: 0.00001334
Iteration 95/1000 | Loss: 0.00001333
Iteration 96/1000 | Loss: 0.00001333
Iteration 97/1000 | Loss: 0.00001333
Iteration 98/1000 | Loss: 0.00001333
Iteration 99/1000 | Loss: 0.00001333
Iteration 100/1000 | Loss: 0.00001332
Iteration 101/1000 | Loss: 0.00001332
Iteration 102/1000 | Loss: 0.00001332
Iteration 103/1000 | Loss: 0.00001332
Iteration 104/1000 | Loss: 0.00001332
Iteration 105/1000 | Loss: 0.00001332
Iteration 106/1000 | Loss: 0.00001332
Iteration 107/1000 | Loss: 0.00001332
Iteration 108/1000 | Loss: 0.00001332
Iteration 109/1000 | Loss: 0.00001332
Iteration 110/1000 | Loss: 0.00001332
Iteration 111/1000 | Loss: 0.00001332
Iteration 112/1000 | Loss: 0.00001332
Iteration 113/1000 | Loss: 0.00001331
Iteration 114/1000 | Loss: 0.00001331
Iteration 115/1000 | Loss: 0.00001331
Iteration 116/1000 | Loss: 0.00001330
Iteration 117/1000 | Loss: 0.00001330
Iteration 118/1000 | Loss: 0.00001330
Iteration 119/1000 | Loss: 0.00001329
Iteration 120/1000 | Loss: 0.00001329
Iteration 121/1000 | Loss: 0.00001329
Iteration 122/1000 | Loss: 0.00001329
Iteration 123/1000 | Loss: 0.00001328
Iteration 124/1000 | Loss: 0.00001328
Iteration 125/1000 | Loss: 0.00001328
Iteration 126/1000 | Loss: 0.00001328
Iteration 127/1000 | Loss: 0.00001327
Iteration 128/1000 | Loss: 0.00001327
Iteration 129/1000 | Loss: 0.00001327
Iteration 130/1000 | Loss: 0.00001326
Iteration 131/1000 | Loss: 0.00001326
Iteration 132/1000 | Loss: 0.00001325
Iteration 133/1000 | Loss: 0.00001325
Iteration 134/1000 | Loss: 0.00001325
Iteration 135/1000 | Loss: 0.00001325
Iteration 136/1000 | Loss: 0.00001325
Iteration 137/1000 | Loss: 0.00001325
Iteration 138/1000 | Loss: 0.00001324
Iteration 139/1000 | Loss: 0.00001324
Iteration 140/1000 | Loss: 0.00001324
Iteration 141/1000 | Loss: 0.00001324
Iteration 142/1000 | Loss: 0.00001324
Iteration 143/1000 | Loss: 0.00001324
Iteration 144/1000 | Loss: 0.00001324
Iteration 145/1000 | Loss: 0.00001324
Iteration 146/1000 | Loss: 0.00001324
Iteration 147/1000 | Loss: 0.00001324
Iteration 148/1000 | Loss: 0.00001324
Iteration 149/1000 | Loss: 0.00001323
Iteration 150/1000 | Loss: 0.00001323
Iteration 151/1000 | Loss: 0.00001323
Iteration 152/1000 | Loss: 0.00001323
Iteration 153/1000 | Loss: 0.00001323
Iteration 154/1000 | Loss: 0.00001323
Iteration 155/1000 | Loss: 0.00001323
Iteration 156/1000 | Loss: 0.00001323
Iteration 157/1000 | Loss: 0.00001322
Iteration 158/1000 | Loss: 0.00001322
Iteration 159/1000 | Loss: 0.00001322
Iteration 160/1000 | Loss: 0.00001322
Iteration 161/1000 | Loss: 0.00001322
Iteration 162/1000 | Loss: 0.00001322
Iteration 163/1000 | Loss: 0.00001322
Iteration 164/1000 | Loss: 0.00001322
Iteration 165/1000 | Loss: 0.00001322
Iteration 166/1000 | Loss: 0.00001321
Iteration 167/1000 | Loss: 0.00001321
Iteration 168/1000 | Loss: 0.00001321
Iteration 169/1000 | Loss: 0.00001321
Iteration 170/1000 | Loss: 0.00001321
Iteration 171/1000 | Loss: 0.00001321
Iteration 172/1000 | Loss: 0.00001321
Iteration 173/1000 | Loss: 0.00001321
Iteration 174/1000 | Loss: 0.00001321
Iteration 175/1000 | Loss: 0.00001321
Iteration 176/1000 | Loss: 0.00001321
Iteration 177/1000 | Loss: 0.00001321
Iteration 178/1000 | Loss: 0.00001321
Iteration 179/1000 | Loss: 0.00001321
Iteration 180/1000 | Loss: 0.00001321
Iteration 181/1000 | Loss: 0.00001321
Iteration 182/1000 | Loss: 0.00001321
Iteration 183/1000 | Loss: 0.00001321
Iteration 184/1000 | Loss: 0.00001321
Iteration 185/1000 | Loss: 0.00001321
Iteration 186/1000 | Loss: 0.00001321
Iteration 187/1000 | Loss: 0.00001321
Iteration 188/1000 | Loss: 0.00001321
Iteration 189/1000 | Loss: 0.00001321
Iteration 190/1000 | Loss: 0.00001321
Iteration 191/1000 | Loss: 0.00001321
Iteration 192/1000 | Loss: 0.00001321
Iteration 193/1000 | Loss: 0.00001321
Iteration 194/1000 | Loss: 0.00001321
Iteration 195/1000 | Loss: 0.00001321
Iteration 196/1000 | Loss: 0.00001321
Iteration 197/1000 | Loss: 0.00001321
Iteration 198/1000 | Loss: 0.00001321
Iteration 199/1000 | Loss: 0.00001321
Iteration 200/1000 | Loss: 0.00001321
Iteration 201/1000 | Loss: 0.00001321
Iteration 202/1000 | Loss: 0.00001321
Iteration 203/1000 | Loss: 0.00001321
Iteration 204/1000 | Loss: 0.00001321
Iteration 205/1000 | Loss: 0.00001321
Iteration 206/1000 | Loss: 0.00001321
Iteration 207/1000 | Loss: 0.00001321
Iteration 208/1000 | Loss: 0.00001321
Iteration 209/1000 | Loss: 0.00001321
Iteration 210/1000 | Loss: 0.00001321
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [1.3211782061262056e-05, 1.3211782061262056e-05, 1.3211782061262056e-05, 1.3211782061262056e-05, 1.3211782061262056e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3211782061262056e-05

Optimization complete. Final v2v error: 3.0950350761413574 mm

Highest mean error: 3.541214942932129 mm for frame 81

Lowest mean error: 2.6820943355560303 mm for frame 4

Saving results

Total time: 40.87528610229492
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00695969
Iteration 2/25 | Loss: 0.00157329
Iteration 3/25 | Loss: 0.00143259
Iteration 4/25 | Loss: 0.00140839
Iteration 5/25 | Loss: 0.00140210
Iteration 6/25 | Loss: 0.00140147
Iteration 7/25 | Loss: 0.00140147
Iteration 8/25 | Loss: 0.00140147
Iteration 9/25 | Loss: 0.00140147
Iteration 10/25 | Loss: 0.00140147
Iteration 11/25 | Loss: 0.00140147
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001401465036906302, 0.001401465036906302, 0.001401465036906302, 0.001401465036906302, 0.001401465036906302]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001401465036906302

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.62481070
Iteration 2/25 | Loss: 0.00114195
Iteration 3/25 | Loss: 0.00114194
Iteration 4/25 | Loss: 0.00114194
Iteration 5/25 | Loss: 0.00114194
Iteration 6/25 | Loss: 0.00114194
Iteration 7/25 | Loss: 0.00114194
Iteration 8/25 | Loss: 0.00114194
Iteration 9/25 | Loss: 0.00114194
Iteration 10/25 | Loss: 0.00114194
Iteration 11/25 | Loss: 0.00114194
Iteration 12/25 | Loss: 0.00114194
Iteration 13/25 | Loss: 0.00114194
Iteration 14/25 | Loss: 0.00114194
Iteration 15/25 | Loss: 0.00114194
Iteration 16/25 | Loss: 0.00114194
Iteration 17/25 | Loss: 0.00114194
Iteration 18/25 | Loss: 0.00114194
Iteration 19/25 | Loss: 0.00114194
Iteration 20/25 | Loss: 0.00114194
Iteration 21/25 | Loss: 0.00114194
Iteration 22/25 | Loss: 0.00114194
Iteration 23/25 | Loss: 0.00114194
Iteration 24/25 | Loss: 0.00114194
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0011419396614655852, 0.0011419396614655852, 0.0011419396614655852, 0.0011419396614655852, 0.0011419396614655852]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011419396614655852

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114194
Iteration 2/1000 | Loss: 0.00009377
Iteration 3/1000 | Loss: 0.00004969
Iteration 4/1000 | Loss: 0.00004129
Iteration 5/1000 | Loss: 0.00003814
Iteration 6/1000 | Loss: 0.00003642
Iteration 7/1000 | Loss: 0.00003542
Iteration 8/1000 | Loss: 0.00003440
Iteration 9/1000 | Loss: 0.00003369
Iteration 10/1000 | Loss: 0.00003314
Iteration 11/1000 | Loss: 0.00003277
Iteration 12/1000 | Loss: 0.00003245
Iteration 13/1000 | Loss: 0.00003221
Iteration 14/1000 | Loss: 0.00003193
Iteration 15/1000 | Loss: 0.00003170
Iteration 16/1000 | Loss: 0.00003154
Iteration 17/1000 | Loss: 0.00003152
Iteration 18/1000 | Loss: 0.00003138
Iteration 19/1000 | Loss: 0.00003138
Iteration 20/1000 | Loss: 0.00003137
Iteration 21/1000 | Loss: 0.00003135
Iteration 22/1000 | Loss: 0.00003134
Iteration 23/1000 | Loss: 0.00003131
Iteration 24/1000 | Loss: 0.00003130
Iteration 25/1000 | Loss: 0.00003129
Iteration 26/1000 | Loss: 0.00003129
Iteration 27/1000 | Loss: 0.00003129
Iteration 28/1000 | Loss: 0.00003128
Iteration 29/1000 | Loss: 0.00003128
Iteration 30/1000 | Loss: 0.00003125
Iteration 31/1000 | Loss: 0.00003125
Iteration 32/1000 | Loss: 0.00003124
Iteration 33/1000 | Loss: 0.00003124
Iteration 34/1000 | Loss: 0.00003124
Iteration 35/1000 | Loss: 0.00003121
Iteration 36/1000 | Loss: 0.00003121
Iteration 37/1000 | Loss: 0.00003120
Iteration 38/1000 | Loss: 0.00003120
Iteration 39/1000 | Loss: 0.00003119
Iteration 40/1000 | Loss: 0.00003118
Iteration 41/1000 | Loss: 0.00003118
Iteration 42/1000 | Loss: 0.00003117
Iteration 43/1000 | Loss: 0.00003117
Iteration 44/1000 | Loss: 0.00003117
Iteration 45/1000 | Loss: 0.00003116
Iteration 46/1000 | Loss: 0.00003116
Iteration 47/1000 | Loss: 0.00003116
Iteration 48/1000 | Loss: 0.00003115
Iteration 49/1000 | Loss: 0.00003114
Iteration 50/1000 | Loss: 0.00003114
Iteration 51/1000 | Loss: 0.00003112
Iteration 52/1000 | Loss: 0.00003112
Iteration 53/1000 | Loss: 0.00003112
Iteration 54/1000 | Loss: 0.00003112
Iteration 55/1000 | Loss: 0.00003112
Iteration 56/1000 | Loss: 0.00003112
Iteration 57/1000 | Loss: 0.00003112
Iteration 58/1000 | Loss: 0.00003112
Iteration 59/1000 | Loss: 0.00003112
Iteration 60/1000 | Loss: 0.00003112
Iteration 61/1000 | Loss: 0.00003112
Iteration 62/1000 | Loss: 0.00003111
Iteration 63/1000 | Loss: 0.00003111
Iteration 64/1000 | Loss: 0.00003111
Iteration 65/1000 | Loss: 0.00003110
Iteration 66/1000 | Loss: 0.00003109
Iteration 67/1000 | Loss: 0.00003109
Iteration 68/1000 | Loss: 0.00003109
Iteration 69/1000 | Loss: 0.00003109
Iteration 70/1000 | Loss: 0.00003108
Iteration 71/1000 | Loss: 0.00003108
Iteration 72/1000 | Loss: 0.00003108
Iteration 73/1000 | Loss: 0.00003107
Iteration 74/1000 | Loss: 0.00003107
Iteration 75/1000 | Loss: 0.00003107
Iteration 76/1000 | Loss: 0.00003106
Iteration 77/1000 | Loss: 0.00003106
Iteration 78/1000 | Loss: 0.00003105
Iteration 79/1000 | Loss: 0.00003105
Iteration 80/1000 | Loss: 0.00003105
Iteration 81/1000 | Loss: 0.00003105
Iteration 82/1000 | Loss: 0.00003104
Iteration 83/1000 | Loss: 0.00003104
Iteration 84/1000 | Loss: 0.00003104
Iteration 85/1000 | Loss: 0.00003103
Iteration 86/1000 | Loss: 0.00003103
Iteration 87/1000 | Loss: 0.00003103
Iteration 88/1000 | Loss: 0.00003102
Iteration 89/1000 | Loss: 0.00003102
Iteration 90/1000 | Loss: 0.00003102
Iteration 91/1000 | Loss: 0.00003101
Iteration 92/1000 | Loss: 0.00003101
Iteration 93/1000 | Loss: 0.00003101
Iteration 94/1000 | Loss: 0.00003101
Iteration 95/1000 | Loss: 0.00003101
Iteration 96/1000 | Loss: 0.00003100
Iteration 97/1000 | Loss: 0.00003100
Iteration 98/1000 | Loss: 0.00003100
Iteration 99/1000 | Loss: 0.00003100
Iteration 100/1000 | Loss: 0.00003100
Iteration 101/1000 | Loss: 0.00003099
Iteration 102/1000 | Loss: 0.00003099
Iteration 103/1000 | Loss: 0.00003099
Iteration 104/1000 | Loss: 0.00003099
Iteration 105/1000 | Loss: 0.00003099
Iteration 106/1000 | Loss: 0.00003098
Iteration 107/1000 | Loss: 0.00003098
Iteration 108/1000 | Loss: 0.00003098
Iteration 109/1000 | Loss: 0.00003098
Iteration 110/1000 | Loss: 0.00003097
Iteration 111/1000 | Loss: 0.00003097
Iteration 112/1000 | Loss: 0.00003097
Iteration 113/1000 | Loss: 0.00003097
Iteration 114/1000 | Loss: 0.00003097
Iteration 115/1000 | Loss: 0.00003097
Iteration 116/1000 | Loss: 0.00003097
Iteration 117/1000 | Loss: 0.00003097
Iteration 118/1000 | Loss: 0.00003097
Iteration 119/1000 | Loss: 0.00003096
Iteration 120/1000 | Loss: 0.00003096
Iteration 121/1000 | Loss: 0.00003096
Iteration 122/1000 | Loss: 0.00003096
Iteration 123/1000 | Loss: 0.00003096
Iteration 124/1000 | Loss: 0.00003096
Iteration 125/1000 | Loss: 0.00003096
Iteration 126/1000 | Loss: 0.00003096
Iteration 127/1000 | Loss: 0.00003096
Iteration 128/1000 | Loss: 0.00003096
Iteration 129/1000 | Loss: 0.00003096
Iteration 130/1000 | Loss: 0.00003096
Iteration 131/1000 | Loss: 0.00003096
Iteration 132/1000 | Loss: 0.00003095
Iteration 133/1000 | Loss: 0.00003095
Iteration 134/1000 | Loss: 0.00003095
Iteration 135/1000 | Loss: 0.00003095
Iteration 136/1000 | Loss: 0.00003095
Iteration 137/1000 | Loss: 0.00003095
Iteration 138/1000 | Loss: 0.00003095
Iteration 139/1000 | Loss: 0.00003095
Iteration 140/1000 | Loss: 0.00003095
Iteration 141/1000 | Loss: 0.00003094
Iteration 142/1000 | Loss: 0.00003094
Iteration 143/1000 | Loss: 0.00003094
Iteration 144/1000 | Loss: 0.00003094
Iteration 145/1000 | Loss: 0.00003094
Iteration 146/1000 | Loss: 0.00003094
Iteration 147/1000 | Loss: 0.00003094
Iteration 148/1000 | Loss: 0.00003094
Iteration 149/1000 | Loss: 0.00003094
Iteration 150/1000 | Loss: 0.00003094
Iteration 151/1000 | Loss: 0.00003094
Iteration 152/1000 | Loss: 0.00003093
Iteration 153/1000 | Loss: 0.00003093
Iteration 154/1000 | Loss: 0.00003093
Iteration 155/1000 | Loss: 0.00003093
Iteration 156/1000 | Loss: 0.00003093
Iteration 157/1000 | Loss: 0.00003093
Iteration 158/1000 | Loss: 0.00003092
Iteration 159/1000 | Loss: 0.00003092
Iteration 160/1000 | Loss: 0.00003092
Iteration 161/1000 | Loss: 0.00003092
Iteration 162/1000 | Loss: 0.00003092
Iteration 163/1000 | Loss: 0.00003092
Iteration 164/1000 | Loss: 0.00003092
Iteration 165/1000 | Loss: 0.00003092
Iteration 166/1000 | Loss: 0.00003091
Iteration 167/1000 | Loss: 0.00003091
Iteration 168/1000 | Loss: 0.00003091
Iteration 169/1000 | Loss: 0.00003091
Iteration 170/1000 | Loss: 0.00003091
Iteration 171/1000 | Loss: 0.00003091
Iteration 172/1000 | Loss: 0.00003091
Iteration 173/1000 | Loss: 0.00003091
Iteration 174/1000 | Loss: 0.00003091
Iteration 175/1000 | Loss: 0.00003091
Iteration 176/1000 | Loss: 0.00003090
Iteration 177/1000 | Loss: 0.00003090
Iteration 178/1000 | Loss: 0.00003090
Iteration 179/1000 | Loss: 0.00003090
Iteration 180/1000 | Loss: 0.00003090
Iteration 181/1000 | Loss: 0.00003090
Iteration 182/1000 | Loss: 0.00003090
Iteration 183/1000 | Loss: 0.00003090
Iteration 184/1000 | Loss: 0.00003090
Iteration 185/1000 | Loss: 0.00003089
Iteration 186/1000 | Loss: 0.00003089
Iteration 187/1000 | Loss: 0.00003089
Iteration 188/1000 | Loss: 0.00003089
Iteration 189/1000 | Loss: 0.00003089
Iteration 190/1000 | Loss: 0.00003089
Iteration 191/1000 | Loss: 0.00003089
Iteration 192/1000 | Loss: 0.00003089
Iteration 193/1000 | Loss: 0.00003089
Iteration 194/1000 | Loss: 0.00003089
Iteration 195/1000 | Loss: 0.00003089
Iteration 196/1000 | Loss: 0.00003089
Iteration 197/1000 | Loss: 0.00003089
Iteration 198/1000 | Loss: 0.00003089
Iteration 199/1000 | Loss: 0.00003088
Iteration 200/1000 | Loss: 0.00003088
Iteration 201/1000 | Loss: 0.00003088
Iteration 202/1000 | Loss: 0.00003088
Iteration 203/1000 | Loss: 0.00003088
Iteration 204/1000 | Loss: 0.00003088
Iteration 205/1000 | Loss: 0.00003088
Iteration 206/1000 | Loss: 0.00003088
Iteration 207/1000 | Loss: 0.00003088
Iteration 208/1000 | Loss: 0.00003088
Iteration 209/1000 | Loss: 0.00003088
Iteration 210/1000 | Loss: 0.00003088
Iteration 211/1000 | Loss: 0.00003088
Iteration 212/1000 | Loss: 0.00003088
Iteration 213/1000 | Loss: 0.00003088
Iteration 214/1000 | Loss: 0.00003088
Iteration 215/1000 | Loss: 0.00003088
Iteration 216/1000 | Loss: 0.00003088
Iteration 217/1000 | Loss: 0.00003088
Iteration 218/1000 | Loss: 0.00003087
Iteration 219/1000 | Loss: 0.00003087
Iteration 220/1000 | Loss: 0.00003087
Iteration 221/1000 | Loss: 0.00003087
Iteration 222/1000 | Loss: 0.00003087
Iteration 223/1000 | Loss: 0.00003087
Iteration 224/1000 | Loss: 0.00003087
Iteration 225/1000 | Loss: 0.00003087
Iteration 226/1000 | Loss: 0.00003087
Iteration 227/1000 | Loss: 0.00003087
Iteration 228/1000 | Loss: 0.00003087
Iteration 229/1000 | Loss: 0.00003087
Iteration 230/1000 | Loss: 0.00003086
Iteration 231/1000 | Loss: 0.00003086
Iteration 232/1000 | Loss: 0.00003086
Iteration 233/1000 | Loss: 0.00003086
Iteration 234/1000 | Loss: 0.00003086
Iteration 235/1000 | Loss: 0.00003086
Iteration 236/1000 | Loss: 0.00003086
Iteration 237/1000 | Loss: 0.00003086
Iteration 238/1000 | Loss: 0.00003086
Iteration 239/1000 | Loss: 0.00003086
Iteration 240/1000 | Loss: 0.00003086
Iteration 241/1000 | Loss: 0.00003086
Iteration 242/1000 | Loss: 0.00003086
Iteration 243/1000 | Loss: 0.00003086
Iteration 244/1000 | Loss: 0.00003086
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 244. Stopping optimization.
Last 5 losses: [3.086065407842398e-05, 3.086065407842398e-05, 3.086065407842398e-05, 3.086065407842398e-05, 3.086065407842398e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.086065407842398e-05

Optimization complete. Final v2v error: 4.459003925323486 mm

Highest mean error: 6.172571659088135 mm for frame 197

Lowest mean error: 3.029395818710327 mm for frame 233

Saving results

Total time: 56.12315058708191
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00619659
Iteration 2/25 | Loss: 0.00146267
Iteration 3/25 | Loss: 0.00132376
Iteration 4/25 | Loss: 0.00129728
Iteration 5/25 | Loss: 0.00128234
Iteration 6/25 | Loss: 0.00126830
Iteration 7/25 | Loss: 0.00126413
Iteration 8/25 | Loss: 0.00127055
Iteration 9/25 | Loss: 0.00126328
Iteration 10/25 | Loss: 0.00125828
Iteration 11/25 | Loss: 0.00126246
Iteration 12/25 | Loss: 0.00125432
Iteration 13/25 | Loss: 0.00124866
Iteration 14/25 | Loss: 0.00124828
Iteration 15/25 | Loss: 0.00124819
Iteration 16/25 | Loss: 0.00124819
Iteration 17/25 | Loss: 0.00124819
Iteration 18/25 | Loss: 0.00124819
Iteration 19/25 | Loss: 0.00124818
Iteration 20/25 | Loss: 0.00124818
Iteration 21/25 | Loss: 0.00124818
Iteration 22/25 | Loss: 0.00124818
Iteration 23/25 | Loss: 0.00124818
Iteration 24/25 | Loss: 0.00124818
Iteration 25/25 | Loss: 0.00124818

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62741578
Iteration 2/25 | Loss: 0.00084907
Iteration 3/25 | Loss: 0.00084907
Iteration 4/25 | Loss: 0.00084906
Iteration 5/25 | Loss: 0.00084906
Iteration 6/25 | Loss: 0.00084906
Iteration 7/25 | Loss: 0.00084906
Iteration 8/25 | Loss: 0.00084906
Iteration 9/25 | Loss: 0.00084906
Iteration 10/25 | Loss: 0.00084906
Iteration 11/25 | Loss: 0.00084906
Iteration 12/25 | Loss: 0.00084906
Iteration 13/25 | Loss: 0.00084906
Iteration 14/25 | Loss: 0.00084906
Iteration 15/25 | Loss: 0.00084906
Iteration 16/25 | Loss: 0.00084906
Iteration 17/25 | Loss: 0.00084906
Iteration 18/25 | Loss: 0.00084906
Iteration 19/25 | Loss: 0.00084906
Iteration 20/25 | Loss: 0.00084906
Iteration 21/25 | Loss: 0.00084906
Iteration 22/25 | Loss: 0.00084906
Iteration 23/25 | Loss: 0.00084906
Iteration 24/25 | Loss: 0.00084906
Iteration 25/25 | Loss: 0.00084906

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084906
Iteration 2/1000 | Loss: 0.00003626
Iteration 3/1000 | Loss: 0.00002596
Iteration 4/1000 | Loss: 0.00002358
Iteration 5/1000 | Loss: 0.00002211
Iteration 6/1000 | Loss: 0.00002085
Iteration 7/1000 | Loss: 0.00002015
Iteration 8/1000 | Loss: 0.00001968
Iteration 9/1000 | Loss: 0.00001935
Iteration 10/1000 | Loss: 0.00001901
Iteration 11/1000 | Loss: 0.00001881
Iteration 12/1000 | Loss: 0.00001864
Iteration 13/1000 | Loss: 0.00001856
Iteration 14/1000 | Loss: 0.00001853
Iteration 15/1000 | Loss: 0.00001848
Iteration 16/1000 | Loss: 0.00001848
Iteration 17/1000 | Loss: 0.00001845
Iteration 18/1000 | Loss: 0.00001844
Iteration 19/1000 | Loss: 0.00001843
Iteration 20/1000 | Loss: 0.00001840
Iteration 21/1000 | Loss: 0.00001839
Iteration 22/1000 | Loss: 0.00001839
Iteration 23/1000 | Loss: 0.00001837
Iteration 24/1000 | Loss: 0.00001837
Iteration 25/1000 | Loss: 0.00001837
Iteration 26/1000 | Loss: 0.00001837
Iteration 27/1000 | Loss: 0.00001837
Iteration 28/1000 | Loss: 0.00001836
Iteration 29/1000 | Loss: 0.00001836
Iteration 30/1000 | Loss: 0.00001836
Iteration 31/1000 | Loss: 0.00001836
Iteration 32/1000 | Loss: 0.00001836
Iteration 33/1000 | Loss: 0.00001835
Iteration 34/1000 | Loss: 0.00001835
Iteration 35/1000 | Loss: 0.00001834
Iteration 36/1000 | Loss: 0.00001834
Iteration 37/1000 | Loss: 0.00001834
Iteration 38/1000 | Loss: 0.00001833
Iteration 39/1000 | Loss: 0.00001833
Iteration 40/1000 | Loss: 0.00001832
Iteration 41/1000 | Loss: 0.00001832
Iteration 42/1000 | Loss: 0.00001832
Iteration 43/1000 | Loss: 0.00001831
Iteration 44/1000 | Loss: 0.00001831
Iteration 45/1000 | Loss: 0.00001831
Iteration 46/1000 | Loss: 0.00001831
Iteration 47/1000 | Loss: 0.00001831
Iteration 48/1000 | Loss: 0.00001830
Iteration 49/1000 | Loss: 0.00001830
Iteration 50/1000 | Loss: 0.00001830
Iteration 51/1000 | Loss: 0.00001829
Iteration 52/1000 | Loss: 0.00001829
Iteration 53/1000 | Loss: 0.00001829
Iteration 54/1000 | Loss: 0.00001829
Iteration 55/1000 | Loss: 0.00001829
Iteration 56/1000 | Loss: 0.00001829
Iteration 57/1000 | Loss: 0.00001829
Iteration 58/1000 | Loss: 0.00001828
Iteration 59/1000 | Loss: 0.00001828
Iteration 60/1000 | Loss: 0.00001828
Iteration 61/1000 | Loss: 0.00001828
Iteration 62/1000 | Loss: 0.00001827
Iteration 63/1000 | Loss: 0.00001827
Iteration 64/1000 | Loss: 0.00001827
Iteration 65/1000 | Loss: 0.00001827
Iteration 66/1000 | Loss: 0.00001827
Iteration 67/1000 | Loss: 0.00001826
Iteration 68/1000 | Loss: 0.00001826
Iteration 69/1000 | Loss: 0.00001826
Iteration 70/1000 | Loss: 0.00001826
Iteration 71/1000 | Loss: 0.00001826
Iteration 72/1000 | Loss: 0.00001826
Iteration 73/1000 | Loss: 0.00001826
Iteration 74/1000 | Loss: 0.00001825
Iteration 75/1000 | Loss: 0.00001825
Iteration 76/1000 | Loss: 0.00001825
Iteration 77/1000 | Loss: 0.00001825
Iteration 78/1000 | Loss: 0.00001825
Iteration 79/1000 | Loss: 0.00001825
Iteration 80/1000 | Loss: 0.00001825
Iteration 81/1000 | Loss: 0.00001824
Iteration 82/1000 | Loss: 0.00001824
Iteration 83/1000 | Loss: 0.00001824
Iteration 84/1000 | Loss: 0.00001824
Iteration 85/1000 | Loss: 0.00001824
Iteration 86/1000 | Loss: 0.00001823
Iteration 87/1000 | Loss: 0.00001823
Iteration 88/1000 | Loss: 0.00001823
Iteration 89/1000 | Loss: 0.00001823
Iteration 90/1000 | Loss: 0.00001822
Iteration 91/1000 | Loss: 0.00001822
Iteration 92/1000 | Loss: 0.00001822
Iteration 93/1000 | Loss: 0.00001822
Iteration 94/1000 | Loss: 0.00001822
Iteration 95/1000 | Loss: 0.00001821
Iteration 96/1000 | Loss: 0.00001821
Iteration 97/1000 | Loss: 0.00001821
Iteration 98/1000 | Loss: 0.00001821
Iteration 99/1000 | Loss: 0.00001821
Iteration 100/1000 | Loss: 0.00001821
Iteration 101/1000 | Loss: 0.00001821
Iteration 102/1000 | Loss: 0.00001820
Iteration 103/1000 | Loss: 0.00001820
Iteration 104/1000 | Loss: 0.00001820
Iteration 105/1000 | Loss: 0.00001820
Iteration 106/1000 | Loss: 0.00001820
Iteration 107/1000 | Loss: 0.00001820
Iteration 108/1000 | Loss: 0.00001820
Iteration 109/1000 | Loss: 0.00001820
Iteration 110/1000 | Loss: 0.00001820
Iteration 111/1000 | Loss: 0.00001820
Iteration 112/1000 | Loss: 0.00001820
Iteration 113/1000 | Loss: 0.00001819
Iteration 114/1000 | Loss: 0.00001819
Iteration 115/1000 | Loss: 0.00001819
Iteration 116/1000 | Loss: 0.00001819
Iteration 117/1000 | Loss: 0.00001819
Iteration 118/1000 | Loss: 0.00001819
Iteration 119/1000 | Loss: 0.00001819
Iteration 120/1000 | Loss: 0.00001819
Iteration 121/1000 | Loss: 0.00001819
Iteration 122/1000 | Loss: 0.00001819
Iteration 123/1000 | Loss: 0.00001819
Iteration 124/1000 | Loss: 0.00001819
Iteration 125/1000 | Loss: 0.00001819
Iteration 126/1000 | Loss: 0.00001819
Iteration 127/1000 | Loss: 0.00001819
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.8192991774412803e-05, 1.8192991774412803e-05, 1.8192991774412803e-05, 1.8192991774412803e-05, 1.8192991774412803e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8192991774412803e-05

Optimization complete. Final v2v error: 3.525888204574585 mm

Highest mean error: 4.448760509490967 mm for frame 126

Lowest mean error: 2.9820797443389893 mm for frame 48

Saving results

Total time: 61.16871523857117
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01087413
Iteration 2/25 | Loss: 0.00276218
Iteration 3/25 | Loss: 0.00211958
Iteration 4/25 | Loss: 0.00202844
Iteration 5/25 | Loss: 0.00196948
Iteration 6/25 | Loss: 0.00188466
Iteration 7/25 | Loss: 0.00177076
Iteration 8/25 | Loss: 0.00174825
Iteration 9/25 | Loss: 0.00171619
Iteration 10/25 | Loss: 0.00172157
Iteration 11/25 | Loss: 0.00166371
Iteration 12/25 | Loss: 0.00163951
Iteration 13/25 | Loss: 0.00164163
Iteration 14/25 | Loss: 0.00162227
Iteration 15/25 | Loss: 0.00163764
Iteration 16/25 | Loss: 0.00163721
Iteration 17/25 | Loss: 0.00163142
Iteration 18/25 | Loss: 0.00161274
Iteration 19/25 | Loss: 0.00160825
Iteration 20/25 | Loss: 0.00160748
Iteration 21/25 | Loss: 0.00160738
Iteration 22/25 | Loss: 0.00160738
Iteration 23/25 | Loss: 0.00160738
Iteration 24/25 | Loss: 0.00160738
Iteration 25/25 | Loss: 0.00160738

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.49198580
Iteration 2/25 | Loss: 0.00150324
Iteration 3/25 | Loss: 0.00150324
Iteration 4/25 | Loss: 0.00150324
Iteration 5/25 | Loss: 0.00150324
Iteration 6/25 | Loss: 0.00150324
Iteration 7/25 | Loss: 0.00150324
Iteration 8/25 | Loss: 0.00150324
Iteration 9/25 | Loss: 0.00150324
Iteration 10/25 | Loss: 0.00150324
Iteration 11/25 | Loss: 0.00150324
Iteration 12/25 | Loss: 0.00150324
Iteration 13/25 | Loss: 0.00150324
Iteration 14/25 | Loss: 0.00150324
Iteration 15/25 | Loss: 0.00150324
Iteration 16/25 | Loss: 0.00150324
Iteration 17/25 | Loss: 0.00150324
Iteration 18/25 | Loss: 0.00150324
Iteration 19/25 | Loss: 0.00150324
Iteration 20/25 | Loss: 0.00150324
Iteration 21/25 | Loss: 0.00150324
Iteration 22/25 | Loss: 0.00150324
Iteration 23/25 | Loss: 0.00150324
Iteration 24/25 | Loss: 0.00150324
Iteration 25/25 | Loss: 0.00150324
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0015032385708764195, 0.0015032385708764195, 0.0015032385708764195, 0.0015032385708764195, 0.0015032385708764195]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015032385708764195

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00150324
Iteration 2/1000 | Loss: 0.01716635
Iteration 3/1000 | Loss: 0.00034738
Iteration 4/1000 | Loss: 0.00023137
Iteration 5/1000 | Loss: 0.00131781
Iteration 6/1000 | Loss: 0.00068589
Iteration 7/1000 | Loss: 0.00064807
Iteration 8/1000 | Loss: 0.00010534
Iteration 9/1000 | Loss: 0.00122786
Iteration 10/1000 | Loss: 0.00064032
Iteration 11/1000 | Loss: 0.00191751
Iteration 12/1000 | Loss: 0.00354237
Iteration 13/1000 | Loss: 0.00206031
Iteration 14/1000 | Loss: 0.00287236
Iteration 15/1000 | Loss: 0.00214053
Iteration 16/1000 | Loss: 0.00061215
Iteration 17/1000 | Loss: 0.00008269
Iteration 18/1000 | Loss: 0.00007059
Iteration 19/1000 | Loss: 0.00006039
Iteration 20/1000 | Loss: 0.00045519
Iteration 21/1000 | Loss: 0.00101575
Iteration 22/1000 | Loss: 0.00099940
Iteration 23/1000 | Loss: 0.00096289
Iteration 24/1000 | Loss: 0.00057715
Iteration 25/1000 | Loss: 0.00115848
Iteration 26/1000 | Loss: 0.00112003
Iteration 27/1000 | Loss: 0.00110852
Iteration 28/1000 | Loss: 0.00078601
Iteration 29/1000 | Loss: 0.00005483
Iteration 30/1000 | Loss: 0.00004857
Iteration 31/1000 | Loss: 0.00004672
Iteration 32/1000 | Loss: 0.00004542
Iteration 33/1000 | Loss: 0.00004459
Iteration 34/1000 | Loss: 0.00004372
Iteration 35/1000 | Loss: 0.00004303
Iteration 36/1000 | Loss: 0.00004263
Iteration 37/1000 | Loss: 0.00004235
Iteration 38/1000 | Loss: 0.00004208
Iteration 39/1000 | Loss: 0.00004183
Iteration 40/1000 | Loss: 0.00004166
Iteration 41/1000 | Loss: 0.00004163
Iteration 42/1000 | Loss: 0.00004163
Iteration 43/1000 | Loss: 0.00004150
Iteration 44/1000 | Loss: 0.00004147
Iteration 45/1000 | Loss: 0.00004139
Iteration 46/1000 | Loss: 0.00004124
Iteration 47/1000 | Loss: 0.00004122
Iteration 48/1000 | Loss: 0.00004113
Iteration 49/1000 | Loss: 0.00004101
Iteration 50/1000 | Loss: 0.00004101
Iteration 51/1000 | Loss: 0.00004101
Iteration 52/1000 | Loss: 0.00004100
Iteration 53/1000 | Loss: 0.00004100
Iteration 54/1000 | Loss: 0.00004100
Iteration 55/1000 | Loss: 0.00004100
Iteration 56/1000 | Loss: 0.00004095
Iteration 57/1000 | Loss: 0.00004093
Iteration 58/1000 | Loss: 0.00004091
Iteration 59/1000 | Loss: 0.00004091
Iteration 60/1000 | Loss: 0.00004091
Iteration 61/1000 | Loss: 0.00004091
Iteration 62/1000 | Loss: 0.00004091
Iteration 63/1000 | Loss: 0.00004091
Iteration 64/1000 | Loss: 0.00004091
Iteration 65/1000 | Loss: 0.00004091
Iteration 66/1000 | Loss: 0.00004091
Iteration 67/1000 | Loss: 0.00004091
Iteration 68/1000 | Loss: 0.00004091
Iteration 69/1000 | Loss: 0.00004091
Iteration 70/1000 | Loss: 0.00004090
Iteration 71/1000 | Loss: 0.00004090
Iteration 72/1000 | Loss: 0.00004090
Iteration 73/1000 | Loss: 0.00004090
Iteration 74/1000 | Loss: 0.00004090
Iteration 75/1000 | Loss: 0.00004089
Iteration 76/1000 | Loss: 0.00004089
Iteration 77/1000 | Loss: 0.00004086
Iteration 78/1000 | Loss: 0.00004086
Iteration 79/1000 | Loss: 0.00004083
Iteration 80/1000 | Loss: 0.00004083
Iteration 81/1000 | Loss: 0.00004083
Iteration 82/1000 | Loss: 0.00004082
Iteration 83/1000 | Loss: 0.00004082
Iteration 84/1000 | Loss: 0.00004082
Iteration 85/1000 | Loss: 0.00004081
Iteration 86/1000 | Loss: 0.00004081
Iteration 87/1000 | Loss: 0.00004081
Iteration 88/1000 | Loss: 0.00004081
Iteration 89/1000 | Loss: 0.00004081
Iteration 90/1000 | Loss: 0.00004081
Iteration 91/1000 | Loss: 0.00004081
Iteration 92/1000 | Loss: 0.00004081
Iteration 93/1000 | Loss: 0.00004081
Iteration 94/1000 | Loss: 0.00004081
Iteration 95/1000 | Loss: 0.00004080
Iteration 96/1000 | Loss: 0.00004080
Iteration 97/1000 | Loss: 0.00004080
Iteration 98/1000 | Loss: 0.00004079
Iteration 99/1000 | Loss: 0.00004079
Iteration 100/1000 | Loss: 0.00004079
Iteration 101/1000 | Loss: 0.00004079
Iteration 102/1000 | Loss: 0.00004078
Iteration 103/1000 | Loss: 0.00004078
Iteration 104/1000 | Loss: 0.00004078
Iteration 105/1000 | Loss: 0.00004078
Iteration 106/1000 | Loss: 0.00004078
Iteration 107/1000 | Loss: 0.00004077
Iteration 108/1000 | Loss: 0.00004077
Iteration 109/1000 | Loss: 0.00004077
Iteration 110/1000 | Loss: 0.00004076
Iteration 111/1000 | Loss: 0.00004076
Iteration 112/1000 | Loss: 0.00004076
Iteration 113/1000 | Loss: 0.00004076
Iteration 114/1000 | Loss: 0.00004075
Iteration 115/1000 | Loss: 0.00004075
Iteration 116/1000 | Loss: 0.00004075
Iteration 117/1000 | Loss: 0.00004075
Iteration 118/1000 | Loss: 0.00004075
Iteration 119/1000 | Loss: 0.00004075
Iteration 120/1000 | Loss: 0.00004075
Iteration 121/1000 | Loss: 0.00004075
Iteration 122/1000 | Loss: 0.00004075
Iteration 123/1000 | Loss: 0.00004073
Iteration 124/1000 | Loss: 0.00004073
Iteration 125/1000 | Loss: 0.00004073
Iteration 126/1000 | Loss: 0.00004072
Iteration 127/1000 | Loss: 0.00004072
Iteration 128/1000 | Loss: 0.00004072
Iteration 129/1000 | Loss: 0.00004072
Iteration 130/1000 | Loss: 0.00004072
Iteration 131/1000 | Loss: 0.00004072
Iteration 132/1000 | Loss: 0.00004071
Iteration 133/1000 | Loss: 0.00004071
Iteration 134/1000 | Loss: 0.00004071
Iteration 135/1000 | Loss: 0.00004071
Iteration 136/1000 | Loss: 0.00004071
Iteration 137/1000 | Loss: 0.00004070
Iteration 138/1000 | Loss: 0.00004070
Iteration 139/1000 | Loss: 0.00004070
Iteration 140/1000 | Loss: 0.00004070
Iteration 141/1000 | Loss: 0.00004069
Iteration 142/1000 | Loss: 0.00004069
Iteration 143/1000 | Loss: 0.00004069
Iteration 144/1000 | Loss: 0.00004069
Iteration 145/1000 | Loss: 0.00004068
Iteration 146/1000 | Loss: 0.00004068
Iteration 147/1000 | Loss: 0.00004068
Iteration 148/1000 | Loss: 0.00004068
Iteration 149/1000 | Loss: 0.00004068
Iteration 150/1000 | Loss: 0.00004068
Iteration 151/1000 | Loss: 0.00004068
Iteration 152/1000 | Loss: 0.00004067
Iteration 153/1000 | Loss: 0.00004067
Iteration 154/1000 | Loss: 0.00004067
Iteration 155/1000 | Loss: 0.00004067
Iteration 156/1000 | Loss: 0.00004067
Iteration 157/1000 | Loss: 0.00004067
Iteration 158/1000 | Loss: 0.00004067
Iteration 159/1000 | Loss: 0.00004067
Iteration 160/1000 | Loss: 0.00004067
Iteration 161/1000 | Loss: 0.00004067
Iteration 162/1000 | Loss: 0.00004067
Iteration 163/1000 | Loss: 0.00004066
Iteration 164/1000 | Loss: 0.00004066
Iteration 165/1000 | Loss: 0.00004066
Iteration 166/1000 | Loss: 0.00004066
Iteration 167/1000 | Loss: 0.00004066
Iteration 168/1000 | Loss: 0.00004066
Iteration 169/1000 | Loss: 0.00004065
Iteration 170/1000 | Loss: 0.00004065
Iteration 171/1000 | Loss: 0.00004064
Iteration 172/1000 | Loss: 0.00004064
Iteration 173/1000 | Loss: 0.00004064
Iteration 174/1000 | Loss: 0.00004064
Iteration 175/1000 | Loss: 0.00004063
Iteration 176/1000 | Loss: 0.00004063
Iteration 177/1000 | Loss: 0.00004063
Iteration 178/1000 | Loss: 0.00004063
Iteration 179/1000 | Loss: 0.00004063
Iteration 180/1000 | Loss: 0.00004063
Iteration 181/1000 | Loss: 0.00004063
Iteration 182/1000 | Loss: 0.00004063
Iteration 183/1000 | Loss: 0.00004063
Iteration 184/1000 | Loss: 0.00004063
Iteration 185/1000 | Loss: 0.00004063
Iteration 186/1000 | Loss: 0.00004062
Iteration 187/1000 | Loss: 0.00004062
Iteration 188/1000 | Loss: 0.00004062
Iteration 189/1000 | Loss: 0.00004062
Iteration 190/1000 | Loss: 0.00004062
Iteration 191/1000 | Loss: 0.00004062
Iteration 192/1000 | Loss: 0.00004062
Iteration 193/1000 | Loss: 0.00004062
Iteration 194/1000 | Loss: 0.00004062
Iteration 195/1000 | Loss: 0.00004062
Iteration 196/1000 | Loss: 0.00004062
Iteration 197/1000 | Loss: 0.00004062
Iteration 198/1000 | Loss: 0.00004062
Iteration 199/1000 | Loss: 0.00004062
Iteration 200/1000 | Loss: 0.00004061
Iteration 201/1000 | Loss: 0.00004061
Iteration 202/1000 | Loss: 0.00004061
Iteration 203/1000 | Loss: 0.00004061
Iteration 204/1000 | Loss: 0.00004061
Iteration 205/1000 | Loss: 0.00004061
Iteration 206/1000 | Loss: 0.00004061
Iteration 207/1000 | Loss: 0.00004060
Iteration 208/1000 | Loss: 0.00004060
Iteration 209/1000 | Loss: 0.00004060
Iteration 210/1000 | Loss: 0.00004060
Iteration 211/1000 | Loss: 0.00004060
Iteration 212/1000 | Loss: 0.00004060
Iteration 213/1000 | Loss: 0.00004060
Iteration 214/1000 | Loss: 0.00004060
Iteration 215/1000 | Loss: 0.00004060
Iteration 216/1000 | Loss: 0.00004060
Iteration 217/1000 | Loss: 0.00004060
Iteration 218/1000 | Loss: 0.00004060
Iteration 219/1000 | Loss: 0.00004059
Iteration 220/1000 | Loss: 0.00004059
Iteration 221/1000 | Loss: 0.00004059
Iteration 222/1000 | Loss: 0.00004059
Iteration 223/1000 | Loss: 0.00004059
Iteration 224/1000 | Loss: 0.00004059
Iteration 225/1000 | Loss: 0.00004059
Iteration 226/1000 | Loss: 0.00004059
Iteration 227/1000 | Loss: 0.00004059
Iteration 228/1000 | Loss: 0.00004059
Iteration 229/1000 | Loss: 0.00004059
Iteration 230/1000 | Loss: 0.00004059
Iteration 231/1000 | Loss: 0.00004059
Iteration 232/1000 | Loss: 0.00004059
Iteration 233/1000 | Loss: 0.00004059
Iteration 234/1000 | Loss: 0.00004059
Iteration 235/1000 | Loss: 0.00004059
Iteration 236/1000 | Loss: 0.00004059
Iteration 237/1000 | Loss: 0.00004059
Iteration 238/1000 | Loss: 0.00004059
Iteration 239/1000 | Loss: 0.00004059
Iteration 240/1000 | Loss: 0.00004059
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 240. Stopping optimization.
Last 5 losses: [4.0594954043626785e-05, 4.0594954043626785e-05, 4.0594954043626785e-05, 4.0594954043626785e-05, 4.0594954043626785e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.0594954043626785e-05

Optimization complete. Final v2v error: 5.196696758270264 mm

Highest mean error: 6.2918243408203125 mm for frame 123

Lowest mean error: 4.036262512207031 mm for frame 2

Saving results

Total time: 128.77839255332947
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01012890
Iteration 2/25 | Loss: 0.00292006
Iteration 3/25 | Loss: 0.00250615
Iteration 4/25 | Loss: 0.00236692
Iteration 5/25 | Loss: 0.00216861
Iteration 6/25 | Loss: 0.00204848
Iteration 7/25 | Loss: 0.00195163
Iteration 8/25 | Loss: 0.00184684
Iteration 9/25 | Loss: 0.00176132
Iteration 10/25 | Loss: 0.00172676
Iteration 11/25 | Loss: 0.00171971
Iteration 12/25 | Loss: 0.00165200
Iteration 13/25 | Loss: 0.00161484
Iteration 14/25 | Loss: 0.00160070
Iteration 15/25 | Loss: 0.00158149
Iteration 16/25 | Loss: 0.00156709
Iteration 17/25 | Loss: 0.00155854
Iteration 18/25 | Loss: 0.00154900
Iteration 19/25 | Loss: 0.00154551
Iteration 20/25 | Loss: 0.00154333
Iteration 21/25 | Loss: 0.00153423
Iteration 22/25 | Loss: 0.00153722
Iteration 23/25 | Loss: 0.00153732
Iteration 24/25 | Loss: 0.00152969
Iteration 25/25 | Loss: 0.00152405

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38132429
Iteration 2/25 | Loss: 0.00566323
Iteration 3/25 | Loss: 0.00566320
Iteration 4/25 | Loss: 0.00566320
Iteration 5/25 | Loss: 0.00566320
Iteration 6/25 | Loss: 0.00566320
Iteration 7/25 | Loss: 0.00566320
Iteration 8/25 | Loss: 0.00566320
Iteration 9/25 | Loss: 0.00566320
Iteration 10/25 | Loss: 0.00566320
Iteration 11/25 | Loss: 0.00566319
Iteration 12/25 | Loss: 0.00566319
Iteration 13/25 | Loss: 0.00566319
Iteration 14/25 | Loss: 0.00566319
Iteration 15/25 | Loss: 0.00566319
Iteration 16/25 | Loss: 0.00566319
Iteration 17/25 | Loss: 0.00566319
Iteration 18/25 | Loss: 0.00566319
Iteration 19/25 | Loss: 0.00566319
Iteration 20/25 | Loss: 0.00566319
Iteration 21/25 | Loss: 0.00566319
Iteration 22/25 | Loss: 0.00566319
Iteration 23/25 | Loss: 0.00566319
Iteration 24/25 | Loss: 0.00566319
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.005663193762302399, 0.005663193762302399, 0.005663193762302399, 0.005663193762302399, 0.005663193762302399]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005663193762302399

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00566319
Iteration 2/1000 | Loss: 0.00366009
Iteration 3/1000 | Loss: 0.00038728
Iteration 4/1000 | Loss: 0.00023430
Iteration 5/1000 | Loss: 0.00017886
Iteration 6/1000 | Loss: 0.00014108
Iteration 7/1000 | Loss: 0.00010311
Iteration 8/1000 | Loss: 0.00009380
Iteration 9/1000 | Loss: 0.00009261
Iteration 10/1000 | Loss: 0.00074656
Iteration 11/1000 | Loss: 0.00147533
Iteration 12/1000 | Loss: 0.00106105
Iteration 13/1000 | Loss: 0.00039089
Iteration 14/1000 | Loss: 0.00028980
Iteration 15/1000 | Loss: 0.00010191
Iteration 16/1000 | Loss: 0.00008956
Iteration 17/1000 | Loss: 0.00105121
Iteration 18/1000 | Loss: 0.00066327
Iteration 19/1000 | Loss: 0.00100420
Iteration 20/1000 | Loss: 0.00035680
Iteration 21/1000 | Loss: 0.00019201
Iteration 22/1000 | Loss: 0.00035314
Iteration 23/1000 | Loss: 0.00014678
Iteration 24/1000 | Loss: 0.00006850
Iteration 25/1000 | Loss: 0.00005022
Iteration 26/1000 | Loss: 0.00007021
Iteration 27/1000 | Loss: 0.00004596
Iteration 28/1000 | Loss: 0.00005713
Iteration 29/1000 | Loss: 0.00029817
Iteration 30/1000 | Loss: 0.00008154
Iteration 31/1000 | Loss: 0.00006777
Iteration 32/1000 | Loss: 0.00006650
Iteration 33/1000 | Loss: 0.00006212
Iteration 34/1000 | Loss: 0.00009425
Iteration 35/1000 | Loss: 0.00004926
Iteration 36/1000 | Loss: 0.00006003
Iteration 37/1000 | Loss: 0.00006027
Iteration 38/1000 | Loss: 0.00006717
Iteration 39/1000 | Loss: 0.00005166
Iteration 40/1000 | Loss: 0.00005150
Iteration 41/1000 | Loss: 0.00006245
Iteration 42/1000 | Loss: 0.00004605
Iteration 43/1000 | Loss: 0.00004987
Iteration 44/1000 | Loss: 0.00005996
Iteration 45/1000 | Loss: 0.00005245
Iteration 46/1000 | Loss: 0.00005329
Iteration 47/1000 | Loss: 0.00004756
Iteration 48/1000 | Loss: 0.00005520
Iteration 49/1000 | Loss: 0.00004623
Iteration 50/1000 | Loss: 0.00004252
Iteration 51/1000 | Loss: 0.00004674
Iteration 52/1000 | Loss: 0.00006431
Iteration 53/1000 | Loss: 0.00004817
Iteration 54/1000 | Loss: 0.00003500
Iteration 55/1000 | Loss: 0.00003585
Iteration 56/1000 | Loss: 0.00004714
Iteration 57/1000 | Loss: 0.00005314
Iteration 58/1000 | Loss: 0.00004335
Iteration 59/1000 | Loss: 0.00005640
Iteration 60/1000 | Loss: 0.00005340
Iteration 61/1000 | Loss: 0.00005211
Iteration 62/1000 | Loss: 0.00004388
Iteration 63/1000 | Loss: 0.00004400
Iteration 64/1000 | Loss: 0.00005385
Iteration 65/1000 | Loss: 0.00005548
Iteration 66/1000 | Loss: 0.00005368
Iteration 67/1000 | Loss: 0.00005943
Iteration 68/1000 | Loss: 0.00003651
Iteration 69/1000 | Loss: 0.00004919
Iteration 70/1000 | Loss: 0.00005173
Iteration 71/1000 | Loss: 0.00004973
Iteration 72/1000 | Loss: 0.00006727
Iteration 73/1000 | Loss: 0.00005129
Iteration 74/1000 | Loss: 0.00004781
Iteration 75/1000 | Loss: 0.00006895
Iteration 76/1000 | Loss: 0.00005198
Iteration 77/1000 | Loss: 0.00004985
Iteration 78/1000 | Loss: 0.00004400
Iteration 79/1000 | Loss: 0.00004906
Iteration 80/1000 | Loss: 0.00006675
Iteration 81/1000 | Loss: 0.00003749
Iteration 82/1000 | Loss: 0.00003467
Iteration 83/1000 | Loss: 0.00003279
Iteration 84/1000 | Loss: 0.00003229
Iteration 85/1000 | Loss: 0.00003171
Iteration 86/1000 | Loss: 0.00003137
Iteration 87/1000 | Loss: 0.00003119
Iteration 88/1000 | Loss: 0.00003117
Iteration 89/1000 | Loss: 0.00003110
Iteration 90/1000 | Loss: 0.00003107
Iteration 91/1000 | Loss: 0.00003107
Iteration 92/1000 | Loss: 0.00003105
Iteration 93/1000 | Loss: 0.00003105
Iteration 94/1000 | Loss: 0.00003104
Iteration 95/1000 | Loss: 0.00003104
Iteration 96/1000 | Loss: 0.00003103
Iteration 97/1000 | Loss: 0.00003103
Iteration 98/1000 | Loss: 0.00003103
Iteration 99/1000 | Loss: 0.00003103
Iteration 100/1000 | Loss: 0.00003103
Iteration 101/1000 | Loss: 0.00003102
Iteration 102/1000 | Loss: 0.00003102
Iteration 103/1000 | Loss: 0.00003102
Iteration 104/1000 | Loss: 0.00003101
Iteration 105/1000 | Loss: 0.00003096
Iteration 106/1000 | Loss: 0.00003092
Iteration 107/1000 | Loss: 0.00003091
Iteration 108/1000 | Loss: 0.00003089
Iteration 109/1000 | Loss: 0.00003088
Iteration 110/1000 | Loss: 0.00003086
Iteration 111/1000 | Loss: 0.00003084
Iteration 112/1000 | Loss: 0.00003084
Iteration 113/1000 | Loss: 0.00003082
Iteration 114/1000 | Loss: 0.00003080
Iteration 115/1000 | Loss: 0.00003079
Iteration 116/1000 | Loss: 0.00003077
Iteration 117/1000 | Loss: 0.00003074
Iteration 118/1000 | Loss: 0.00003074
Iteration 119/1000 | Loss: 0.00003074
Iteration 120/1000 | Loss: 0.00003074
Iteration 121/1000 | Loss: 0.00003072
Iteration 122/1000 | Loss: 0.00003071
Iteration 123/1000 | Loss: 0.00003071
Iteration 124/1000 | Loss: 0.00003071
Iteration 125/1000 | Loss: 0.00003070
Iteration 126/1000 | Loss: 0.00003070
Iteration 127/1000 | Loss: 0.00003069
Iteration 128/1000 | Loss: 0.00003069
Iteration 129/1000 | Loss: 0.00003068
Iteration 130/1000 | Loss: 0.00003068
Iteration 131/1000 | Loss: 0.00003068
Iteration 132/1000 | Loss: 0.00003068
Iteration 133/1000 | Loss: 0.00003068
Iteration 134/1000 | Loss: 0.00003068
Iteration 135/1000 | Loss: 0.00003067
Iteration 136/1000 | Loss: 0.00003067
Iteration 137/1000 | Loss: 0.00003067
Iteration 138/1000 | Loss: 0.00003067
Iteration 139/1000 | Loss: 0.00003067
Iteration 140/1000 | Loss: 0.00003067
Iteration 141/1000 | Loss: 0.00003067
Iteration 142/1000 | Loss: 0.00003067
Iteration 143/1000 | Loss: 0.00003067
Iteration 144/1000 | Loss: 0.00003066
Iteration 145/1000 | Loss: 0.00003066
Iteration 146/1000 | Loss: 0.00003066
Iteration 147/1000 | Loss: 0.00003066
Iteration 148/1000 | Loss: 0.00003066
Iteration 149/1000 | Loss: 0.00003066
Iteration 150/1000 | Loss: 0.00003066
Iteration 151/1000 | Loss: 0.00003065
Iteration 152/1000 | Loss: 0.00003065
Iteration 153/1000 | Loss: 0.00003065
Iteration 154/1000 | Loss: 0.00003065
Iteration 155/1000 | Loss: 0.00003064
Iteration 156/1000 | Loss: 0.00003064
Iteration 157/1000 | Loss: 0.00003064
Iteration 158/1000 | Loss: 0.00003064
Iteration 159/1000 | Loss: 0.00003063
Iteration 160/1000 | Loss: 0.00003063
Iteration 161/1000 | Loss: 0.00003063
Iteration 162/1000 | Loss: 0.00003062
Iteration 163/1000 | Loss: 0.00003062
Iteration 164/1000 | Loss: 0.00003062
Iteration 165/1000 | Loss: 0.00003062
Iteration 166/1000 | Loss: 0.00003062
Iteration 167/1000 | Loss: 0.00003062
Iteration 168/1000 | Loss: 0.00003061
Iteration 169/1000 | Loss: 0.00003061
Iteration 170/1000 | Loss: 0.00003061
Iteration 171/1000 | Loss: 0.00003061
Iteration 172/1000 | Loss: 0.00003061
Iteration 173/1000 | Loss: 0.00003061
Iteration 174/1000 | Loss: 0.00003061
Iteration 175/1000 | Loss: 0.00003061
Iteration 176/1000 | Loss: 0.00003061
Iteration 177/1000 | Loss: 0.00003061
Iteration 178/1000 | Loss: 0.00003061
Iteration 179/1000 | Loss: 0.00003061
Iteration 180/1000 | Loss: 0.00003061
Iteration 181/1000 | Loss: 0.00003061
Iteration 182/1000 | Loss: 0.00003061
Iteration 183/1000 | Loss: 0.00003061
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [3.060802919208072e-05, 3.060802919208072e-05, 3.060802919208072e-05, 3.060802919208072e-05, 3.060802919208072e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.060802919208072e-05

Optimization complete. Final v2v error: 3.7795276641845703 mm

Highest mean error: 10.477173805236816 mm for frame 126

Lowest mean error: 3.138443946838379 mm for frame 63

Saving results

Total time: 179.2053346633911
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00402196
Iteration 2/25 | Loss: 0.00128991
Iteration 3/25 | Loss: 0.00123043
Iteration 4/25 | Loss: 0.00122226
Iteration 5/25 | Loss: 0.00122160
Iteration 6/25 | Loss: 0.00122160
Iteration 7/25 | Loss: 0.00122160
Iteration 8/25 | Loss: 0.00122160
Iteration 9/25 | Loss: 0.00122160
Iteration 10/25 | Loss: 0.00122160
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012216047616675496, 0.0012216047616675496, 0.0012216047616675496, 0.0012216047616675496, 0.0012216047616675496]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012216047616675496

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44322717
Iteration 2/25 | Loss: 0.00077113
Iteration 3/25 | Loss: 0.00077113
Iteration 4/25 | Loss: 0.00077113
Iteration 5/25 | Loss: 0.00077113
Iteration 6/25 | Loss: 0.00077113
Iteration 7/25 | Loss: 0.00077113
Iteration 8/25 | Loss: 0.00077113
Iteration 9/25 | Loss: 0.00077113
Iteration 10/25 | Loss: 0.00077113
Iteration 11/25 | Loss: 0.00077113
Iteration 12/25 | Loss: 0.00077113
Iteration 13/25 | Loss: 0.00077113
Iteration 14/25 | Loss: 0.00077113
Iteration 15/25 | Loss: 0.00077113
Iteration 16/25 | Loss: 0.00077113
Iteration 17/25 | Loss: 0.00077113
Iteration 18/25 | Loss: 0.00077113
Iteration 19/25 | Loss: 0.00077113
Iteration 20/25 | Loss: 0.00077113
Iteration 21/25 | Loss: 0.00077113
Iteration 22/25 | Loss: 0.00077113
Iteration 23/25 | Loss: 0.00077113
Iteration 24/25 | Loss: 0.00077113
Iteration 25/25 | Loss: 0.00077113

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077113
Iteration 2/1000 | Loss: 0.00001961
Iteration 3/1000 | Loss: 0.00001552
Iteration 4/1000 | Loss: 0.00001457
Iteration 5/1000 | Loss: 0.00001393
Iteration 6/1000 | Loss: 0.00001348
Iteration 7/1000 | Loss: 0.00001322
Iteration 8/1000 | Loss: 0.00001295
Iteration 9/1000 | Loss: 0.00001275
Iteration 10/1000 | Loss: 0.00001254
Iteration 11/1000 | Loss: 0.00001240
Iteration 12/1000 | Loss: 0.00001238
Iteration 13/1000 | Loss: 0.00001228
Iteration 14/1000 | Loss: 0.00001228
Iteration 15/1000 | Loss: 0.00001220
Iteration 16/1000 | Loss: 0.00001219
Iteration 17/1000 | Loss: 0.00001215
Iteration 18/1000 | Loss: 0.00001211
Iteration 19/1000 | Loss: 0.00001210
Iteration 20/1000 | Loss: 0.00001210
Iteration 21/1000 | Loss: 0.00001207
Iteration 22/1000 | Loss: 0.00001207
Iteration 23/1000 | Loss: 0.00001203
Iteration 24/1000 | Loss: 0.00001203
Iteration 25/1000 | Loss: 0.00001197
Iteration 26/1000 | Loss: 0.00001196
Iteration 27/1000 | Loss: 0.00001194
Iteration 28/1000 | Loss: 0.00001194
Iteration 29/1000 | Loss: 0.00001193
Iteration 30/1000 | Loss: 0.00001193
Iteration 31/1000 | Loss: 0.00001192
Iteration 32/1000 | Loss: 0.00001191
Iteration 33/1000 | Loss: 0.00001191
Iteration 34/1000 | Loss: 0.00001190
Iteration 35/1000 | Loss: 0.00001190
Iteration 36/1000 | Loss: 0.00001189
Iteration 37/1000 | Loss: 0.00001188
Iteration 38/1000 | Loss: 0.00001188
Iteration 39/1000 | Loss: 0.00001184
Iteration 40/1000 | Loss: 0.00001183
Iteration 41/1000 | Loss: 0.00001182
Iteration 42/1000 | Loss: 0.00001182
Iteration 43/1000 | Loss: 0.00001182
Iteration 44/1000 | Loss: 0.00001182
Iteration 45/1000 | Loss: 0.00001182
Iteration 46/1000 | Loss: 0.00001181
Iteration 47/1000 | Loss: 0.00001177
Iteration 48/1000 | Loss: 0.00001175
Iteration 49/1000 | Loss: 0.00001174
Iteration 50/1000 | Loss: 0.00001174
Iteration 51/1000 | Loss: 0.00001173
Iteration 52/1000 | Loss: 0.00001173
Iteration 53/1000 | Loss: 0.00001173
Iteration 54/1000 | Loss: 0.00001173
Iteration 55/1000 | Loss: 0.00001173
Iteration 56/1000 | Loss: 0.00001173
Iteration 57/1000 | Loss: 0.00001173
Iteration 58/1000 | Loss: 0.00001173
Iteration 59/1000 | Loss: 0.00001173
Iteration 60/1000 | Loss: 0.00001173
Iteration 61/1000 | Loss: 0.00001172
Iteration 62/1000 | Loss: 0.00001171
Iteration 63/1000 | Loss: 0.00001170
Iteration 64/1000 | Loss: 0.00001170
Iteration 65/1000 | Loss: 0.00001170
Iteration 66/1000 | Loss: 0.00001170
Iteration 67/1000 | Loss: 0.00001169
Iteration 68/1000 | Loss: 0.00001169
Iteration 69/1000 | Loss: 0.00001168
Iteration 70/1000 | Loss: 0.00001168
Iteration 71/1000 | Loss: 0.00001168
Iteration 72/1000 | Loss: 0.00001166
Iteration 73/1000 | Loss: 0.00001165
Iteration 74/1000 | Loss: 0.00001165
Iteration 75/1000 | Loss: 0.00001164
Iteration 76/1000 | Loss: 0.00001164
Iteration 77/1000 | Loss: 0.00001163
Iteration 78/1000 | Loss: 0.00001162
Iteration 79/1000 | Loss: 0.00001162
Iteration 80/1000 | Loss: 0.00001162
Iteration 81/1000 | Loss: 0.00001162
Iteration 82/1000 | Loss: 0.00001161
Iteration 83/1000 | Loss: 0.00001161
Iteration 84/1000 | Loss: 0.00001161
Iteration 85/1000 | Loss: 0.00001161
Iteration 86/1000 | Loss: 0.00001161
Iteration 87/1000 | Loss: 0.00001161
Iteration 88/1000 | Loss: 0.00001161
Iteration 89/1000 | Loss: 0.00001161
Iteration 90/1000 | Loss: 0.00001161
Iteration 91/1000 | Loss: 0.00001161
Iteration 92/1000 | Loss: 0.00001161
Iteration 93/1000 | Loss: 0.00001160
Iteration 94/1000 | Loss: 0.00001160
Iteration 95/1000 | Loss: 0.00001159
Iteration 96/1000 | Loss: 0.00001159
Iteration 97/1000 | Loss: 0.00001159
Iteration 98/1000 | Loss: 0.00001158
Iteration 99/1000 | Loss: 0.00001158
Iteration 100/1000 | Loss: 0.00001158
Iteration 101/1000 | Loss: 0.00001158
Iteration 102/1000 | Loss: 0.00001158
Iteration 103/1000 | Loss: 0.00001157
Iteration 104/1000 | Loss: 0.00001157
Iteration 105/1000 | Loss: 0.00001157
Iteration 106/1000 | Loss: 0.00001157
Iteration 107/1000 | Loss: 0.00001157
Iteration 108/1000 | Loss: 0.00001157
Iteration 109/1000 | Loss: 0.00001157
Iteration 110/1000 | Loss: 0.00001157
Iteration 111/1000 | Loss: 0.00001157
Iteration 112/1000 | Loss: 0.00001157
Iteration 113/1000 | Loss: 0.00001157
Iteration 114/1000 | Loss: 0.00001157
Iteration 115/1000 | Loss: 0.00001157
Iteration 116/1000 | Loss: 0.00001157
Iteration 117/1000 | Loss: 0.00001157
Iteration 118/1000 | Loss: 0.00001156
Iteration 119/1000 | Loss: 0.00001156
Iteration 120/1000 | Loss: 0.00001156
Iteration 121/1000 | Loss: 0.00001156
Iteration 122/1000 | Loss: 0.00001156
Iteration 123/1000 | Loss: 0.00001156
Iteration 124/1000 | Loss: 0.00001156
Iteration 125/1000 | Loss: 0.00001156
Iteration 126/1000 | Loss: 0.00001156
Iteration 127/1000 | Loss: 0.00001156
Iteration 128/1000 | Loss: 0.00001156
Iteration 129/1000 | Loss: 0.00001156
Iteration 130/1000 | Loss: 0.00001155
Iteration 131/1000 | Loss: 0.00001155
Iteration 132/1000 | Loss: 0.00001155
Iteration 133/1000 | Loss: 0.00001155
Iteration 134/1000 | Loss: 0.00001155
Iteration 135/1000 | Loss: 0.00001155
Iteration 136/1000 | Loss: 0.00001155
Iteration 137/1000 | Loss: 0.00001155
Iteration 138/1000 | Loss: 0.00001155
Iteration 139/1000 | Loss: 0.00001155
Iteration 140/1000 | Loss: 0.00001155
Iteration 141/1000 | Loss: 0.00001155
Iteration 142/1000 | Loss: 0.00001155
Iteration 143/1000 | Loss: 0.00001155
Iteration 144/1000 | Loss: 0.00001155
Iteration 145/1000 | Loss: 0.00001155
Iteration 146/1000 | Loss: 0.00001155
Iteration 147/1000 | Loss: 0.00001155
Iteration 148/1000 | Loss: 0.00001155
Iteration 149/1000 | Loss: 0.00001155
Iteration 150/1000 | Loss: 0.00001155
Iteration 151/1000 | Loss: 0.00001155
Iteration 152/1000 | Loss: 0.00001155
Iteration 153/1000 | Loss: 0.00001155
Iteration 154/1000 | Loss: 0.00001155
Iteration 155/1000 | Loss: 0.00001155
Iteration 156/1000 | Loss: 0.00001155
Iteration 157/1000 | Loss: 0.00001155
Iteration 158/1000 | Loss: 0.00001155
Iteration 159/1000 | Loss: 0.00001155
Iteration 160/1000 | Loss: 0.00001155
Iteration 161/1000 | Loss: 0.00001155
Iteration 162/1000 | Loss: 0.00001155
Iteration 163/1000 | Loss: 0.00001155
Iteration 164/1000 | Loss: 0.00001155
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.1548760085133836e-05, 1.1548760085133836e-05, 1.1548760085133836e-05, 1.1548760085133836e-05, 1.1548760085133836e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1548760085133836e-05

Optimization complete. Final v2v error: 2.935662269592285 mm

Highest mean error: 3.119492292404175 mm for frame 127

Lowest mean error: 2.804028272628784 mm for frame 227

Saving results

Total time: 41.05811309814453
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00433030
Iteration 2/25 | Loss: 0.00140842
Iteration 3/25 | Loss: 0.00126972
Iteration 4/25 | Loss: 0.00125002
Iteration 5/25 | Loss: 0.00124420
Iteration 6/25 | Loss: 0.00124358
Iteration 7/25 | Loss: 0.00124358
Iteration 8/25 | Loss: 0.00124358
Iteration 9/25 | Loss: 0.00124358
Iteration 10/25 | Loss: 0.00124358
Iteration 11/25 | Loss: 0.00124358
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012435762910172343, 0.0012435762910172343, 0.0012435762910172343, 0.0012435762910172343, 0.0012435762910172343]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012435762910172343

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47819757
Iteration 2/25 | Loss: 0.00089795
Iteration 3/25 | Loss: 0.00089794
Iteration 4/25 | Loss: 0.00089794
Iteration 5/25 | Loss: 0.00089794
Iteration 6/25 | Loss: 0.00089794
Iteration 7/25 | Loss: 0.00089794
Iteration 8/25 | Loss: 0.00089794
Iteration 9/25 | Loss: 0.00089794
Iteration 10/25 | Loss: 0.00089794
Iteration 11/25 | Loss: 0.00089794
Iteration 12/25 | Loss: 0.00089794
Iteration 13/25 | Loss: 0.00089794
Iteration 14/25 | Loss: 0.00089794
Iteration 15/25 | Loss: 0.00089794
Iteration 16/25 | Loss: 0.00089794
Iteration 17/25 | Loss: 0.00089794
Iteration 18/25 | Loss: 0.00089794
Iteration 19/25 | Loss: 0.00089794
Iteration 20/25 | Loss: 0.00089794
Iteration 21/25 | Loss: 0.00089794
Iteration 22/25 | Loss: 0.00089794
Iteration 23/25 | Loss: 0.00089794
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008979400736279786, 0.0008979400736279786, 0.0008979400736279786, 0.0008979400736279786, 0.0008979400736279786]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008979400736279786

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089794
Iteration 2/1000 | Loss: 0.00003610
Iteration 3/1000 | Loss: 0.00002464
Iteration 4/1000 | Loss: 0.00002193
Iteration 5/1000 | Loss: 0.00002072
Iteration 6/1000 | Loss: 0.00001978
Iteration 7/1000 | Loss: 0.00001912
Iteration 8/1000 | Loss: 0.00001876
Iteration 9/1000 | Loss: 0.00001833
Iteration 10/1000 | Loss: 0.00001799
Iteration 11/1000 | Loss: 0.00001778
Iteration 12/1000 | Loss: 0.00001767
Iteration 13/1000 | Loss: 0.00001763
Iteration 14/1000 | Loss: 0.00001762
Iteration 15/1000 | Loss: 0.00001761
Iteration 16/1000 | Loss: 0.00001753
Iteration 17/1000 | Loss: 0.00001752
Iteration 18/1000 | Loss: 0.00001746
Iteration 19/1000 | Loss: 0.00001746
Iteration 20/1000 | Loss: 0.00001745
Iteration 21/1000 | Loss: 0.00001743
Iteration 22/1000 | Loss: 0.00001740
Iteration 23/1000 | Loss: 0.00001739
Iteration 24/1000 | Loss: 0.00001736
Iteration 25/1000 | Loss: 0.00001735
Iteration 26/1000 | Loss: 0.00001735
Iteration 27/1000 | Loss: 0.00001735
Iteration 28/1000 | Loss: 0.00001734
Iteration 29/1000 | Loss: 0.00001734
Iteration 30/1000 | Loss: 0.00001730
Iteration 31/1000 | Loss: 0.00001730
Iteration 32/1000 | Loss: 0.00001729
Iteration 33/1000 | Loss: 0.00001726
Iteration 34/1000 | Loss: 0.00001726
Iteration 35/1000 | Loss: 0.00001726
Iteration 36/1000 | Loss: 0.00001726
Iteration 37/1000 | Loss: 0.00001725
Iteration 38/1000 | Loss: 0.00001724
Iteration 39/1000 | Loss: 0.00001723
Iteration 40/1000 | Loss: 0.00001723
Iteration 41/1000 | Loss: 0.00001721
Iteration 42/1000 | Loss: 0.00001721
Iteration 43/1000 | Loss: 0.00001718
Iteration 44/1000 | Loss: 0.00001718
Iteration 45/1000 | Loss: 0.00001717
Iteration 46/1000 | Loss: 0.00001716
Iteration 47/1000 | Loss: 0.00001715
Iteration 48/1000 | Loss: 0.00001715
Iteration 49/1000 | Loss: 0.00001715
Iteration 50/1000 | Loss: 0.00001715
Iteration 51/1000 | Loss: 0.00001715
Iteration 52/1000 | Loss: 0.00001715
Iteration 53/1000 | Loss: 0.00001714
Iteration 54/1000 | Loss: 0.00001714
Iteration 55/1000 | Loss: 0.00001714
Iteration 56/1000 | Loss: 0.00001714
Iteration 57/1000 | Loss: 0.00001713
Iteration 58/1000 | Loss: 0.00001713
Iteration 59/1000 | Loss: 0.00001713
Iteration 60/1000 | Loss: 0.00001712
Iteration 61/1000 | Loss: 0.00001712
Iteration 62/1000 | Loss: 0.00001712
Iteration 63/1000 | Loss: 0.00001711
Iteration 64/1000 | Loss: 0.00001711
Iteration 65/1000 | Loss: 0.00001710
Iteration 66/1000 | Loss: 0.00001710
Iteration 67/1000 | Loss: 0.00001710
Iteration 68/1000 | Loss: 0.00001709
Iteration 69/1000 | Loss: 0.00001709
Iteration 70/1000 | Loss: 0.00001709
Iteration 71/1000 | Loss: 0.00001709
Iteration 72/1000 | Loss: 0.00001709
Iteration 73/1000 | Loss: 0.00001708
Iteration 74/1000 | Loss: 0.00001708
Iteration 75/1000 | Loss: 0.00001708
Iteration 76/1000 | Loss: 0.00001707
Iteration 77/1000 | Loss: 0.00001707
Iteration 78/1000 | Loss: 0.00001707
Iteration 79/1000 | Loss: 0.00001706
Iteration 80/1000 | Loss: 0.00001706
Iteration 81/1000 | Loss: 0.00001706
Iteration 82/1000 | Loss: 0.00001705
Iteration 83/1000 | Loss: 0.00001704
Iteration 84/1000 | Loss: 0.00001704
Iteration 85/1000 | Loss: 0.00001704
Iteration 86/1000 | Loss: 0.00001704
Iteration 87/1000 | Loss: 0.00001703
Iteration 88/1000 | Loss: 0.00001703
Iteration 89/1000 | Loss: 0.00001703
Iteration 90/1000 | Loss: 0.00001703
Iteration 91/1000 | Loss: 0.00001702
Iteration 92/1000 | Loss: 0.00001702
Iteration 93/1000 | Loss: 0.00001702
Iteration 94/1000 | Loss: 0.00001702
Iteration 95/1000 | Loss: 0.00001702
Iteration 96/1000 | Loss: 0.00001702
Iteration 97/1000 | Loss: 0.00001702
Iteration 98/1000 | Loss: 0.00001702
Iteration 99/1000 | Loss: 0.00001702
Iteration 100/1000 | Loss: 0.00001702
Iteration 101/1000 | Loss: 0.00001702
Iteration 102/1000 | Loss: 0.00001701
Iteration 103/1000 | Loss: 0.00001701
Iteration 104/1000 | Loss: 0.00001701
Iteration 105/1000 | Loss: 0.00001701
Iteration 106/1000 | Loss: 0.00001701
Iteration 107/1000 | Loss: 0.00001701
Iteration 108/1000 | Loss: 0.00001701
Iteration 109/1000 | Loss: 0.00001700
Iteration 110/1000 | Loss: 0.00001700
Iteration 111/1000 | Loss: 0.00001700
Iteration 112/1000 | Loss: 0.00001700
Iteration 113/1000 | Loss: 0.00001699
Iteration 114/1000 | Loss: 0.00001699
Iteration 115/1000 | Loss: 0.00001699
Iteration 116/1000 | Loss: 0.00001699
Iteration 117/1000 | Loss: 0.00001699
Iteration 118/1000 | Loss: 0.00001699
Iteration 119/1000 | Loss: 0.00001698
Iteration 120/1000 | Loss: 0.00001698
Iteration 121/1000 | Loss: 0.00001698
Iteration 122/1000 | Loss: 0.00001698
Iteration 123/1000 | Loss: 0.00001698
Iteration 124/1000 | Loss: 0.00001698
Iteration 125/1000 | Loss: 0.00001698
Iteration 126/1000 | Loss: 0.00001698
Iteration 127/1000 | Loss: 0.00001697
Iteration 128/1000 | Loss: 0.00001697
Iteration 129/1000 | Loss: 0.00001697
Iteration 130/1000 | Loss: 0.00001697
Iteration 131/1000 | Loss: 0.00001697
Iteration 132/1000 | Loss: 0.00001697
Iteration 133/1000 | Loss: 0.00001697
Iteration 134/1000 | Loss: 0.00001697
Iteration 135/1000 | Loss: 0.00001697
Iteration 136/1000 | Loss: 0.00001697
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [1.697160587355029e-05, 1.697160587355029e-05, 1.697160587355029e-05, 1.697160587355029e-05, 1.697160587355029e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.697160587355029e-05

Optimization complete. Final v2v error: 3.4762842655181885 mm

Highest mean error: 4.092580795288086 mm for frame 14

Lowest mean error: 3.0139901638031006 mm for frame 165

Saving results

Total time: 43.800121784210205
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01018336
Iteration 2/25 | Loss: 0.00189155
Iteration 3/25 | Loss: 0.00153764
Iteration 4/25 | Loss: 0.00144002
Iteration 5/25 | Loss: 0.00153794
Iteration 6/25 | Loss: 0.00153381
Iteration 7/25 | Loss: 0.00143344
Iteration 8/25 | Loss: 0.00138028
Iteration 9/25 | Loss: 0.00135347
Iteration 10/25 | Loss: 0.00134967
Iteration 11/25 | Loss: 0.00134237
Iteration 12/25 | Loss: 0.00132873
Iteration 13/25 | Loss: 0.00132581
Iteration 14/25 | Loss: 0.00131971
Iteration 15/25 | Loss: 0.00131492
Iteration 16/25 | Loss: 0.00131201
Iteration 17/25 | Loss: 0.00131114
Iteration 18/25 | Loss: 0.00130801
Iteration 19/25 | Loss: 0.00130478
Iteration 20/25 | Loss: 0.00130287
Iteration 21/25 | Loss: 0.00129716
Iteration 22/25 | Loss: 0.00129045
Iteration 23/25 | Loss: 0.00128690
Iteration 24/25 | Loss: 0.00128946
Iteration 25/25 | Loss: 0.00128651

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46291304
Iteration 2/25 | Loss: 0.00112758
Iteration 3/25 | Loss: 0.00112369
Iteration 4/25 | Loss: 0.00112369
Iteration 5/25 | Loss: 0.00112369
Iteration 6/25 | Loss: 0.00112369
Iteration 7/25 | Loss: 0.00112369
Iteration 8/25 | Loss: 0.00112369
Iteration 9/25 | Loss: 0.00112369
Iteration 10/25 | Loss: 0.00112369
Iteration 11/25 | Loss: 0.00112369
Iteration 12/25 | Loss: 0.00112369
Iteration 13/25 | Loss: 0.00112369
Iteration 14/25 | Loss: 0.00112368
Iteration 15/25 | Loss: 0.00112368
Iteration 16/25 | Loss: 0.00112368
Iteration 17/25 | Loss: 0.00112368
Iteration 18/25 | Loss: 0.00112368
Iteration 19/25 | Loss: 0.00112368
Iteration 20/25 | Loss: 0.00112368
Iteration 21/25 | Loss: 0.00112368
Iteration 22/25 | Loss: 0.00112368
Iteration 23/25 | Loss: 0.00112368
Iteration 24/25 | Loss: 0.00112368
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0011236846912652254, 0.0011236846912652254, 0.0011236846912652254, 0.0011236846912652254, 0.0011236846912652254]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011236846912652254

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112368
Iteration 2/1000 | Loss: 0.00031380
Iteration 3/1000 | Loss: 0.00014680
Iteration 4/1000 | Loss: 0.00024014
Iteration 5/1000 | Loss: 0.00035464
Iteration 6/1000 | Loss: 0.00034080
Iteration 7/1000 | Loss: 0.00029559
Iteration 8/1000 | Loss: 0.00031086
Iteration 9/1000 | Loss: 0.00017681
Iteration 10/1000 | Loss: 0.00021603
Iteration 11/1000 | Loss: 0.00034948
Iteration 12/1000 | Loss: 0.00036853
Iteration 13/1000 | Loss: 0.00043220
Iteration 14/1000 | Loss: 0.00043059
Iteration 15/1000 | Loss: 0.00046786
Iteration 16/1000 | Loss: 0.00035671
Iteration 17/1000 | Loss: 0.00047514
Iteration 18/1000 | Loss: 0.00035544
Iteration 19/1000 | Loss: 0.00021110
Iteration 20/1000 | Loss: 0.00040003
Iteration 21/1000 | Loss: 0.00033371
Iteration 22/1000 | Loss: 0.00021030
Iteration 23/1000 | Loss: 0.00015166
Iteration 24/1000 | Loss: 0.00034738
Iteration 25/1000 | Loss: 0.00022784
Iteration 26/1000 | Loss: 0.00028201
Iteration 27/1000 | Loss: 0.00038060
Iteration 28/1000 | Loss: 0.00034423
Iteration 29/1000 | Loss: 0.00036447
Iteration 30/1000 | Loss: 0.00049983
Iteration 31/1000 | Loss: 0.00035122
Iteration 32/1000 | Loss: 0.00022814
Iteration 33/1000 | Loss: 0.00036562
Iteration 34/1000 | Loss: 0.00038689
Iteration 35/1000 | Loss: 0.00023761
Iteration 36/1000 | Loss: 0.00011709
Iteration 37/1000 | Loss: 0.00005228
Iteration 38/1000 | Loss: 0.00026185
Iteration 39/1000 | Loss: 0.00047194
Iteration 40/1000 | Loss: 0.00024071
Iteration 41/1000 | Loss: 0.00024773
Iteration 42/1000 | Loss: 0.00017894
Iteration 43/1000 | Loss: 0.00018664
Iteration 44/1000 | Loss: 0.00015448
Iteration 45/1000 | Loss: 0.00009385
Iteration 46/1000 | Loss: 0.00016294
Iteration 47/1000 | Loss: 0.00090648
Iteration 48/1000 | Loss: 0.00052002
Iteration 49/1000 | Loss: 0.00018249
Iteration 50/1000 | Loss: 0.00015156
Iteration 51/1000 | Loss: 0.00009090
Iteration 52/1000 | Loss: 0.00014519
Iteration 53/1000 | Loss: 0.00015321
Iteration 54/1000 | Loss: 0.00014426
Iteration 55/1000 | Loss: 0.00016404
Iteration 56/1000 | Loss: 0.00029494
Iteration 57/1000 | Loss: 0.00028795
Iteration 58/1000 | Loss: 0.00037390
Iteration 59/1000 | Loss: 0.00019856
Iteration 60/1000 | Loss: 0.00026459
Iteration 61/1000 | Loss: 0.00026914
Iteration 62/1000 | Loss: 0.00044896
Iteration 63/1000 | Loss: 0.00029047
Iteration 64/1000 | Loss: 0.00010552
Iteration 65/1000 | Loss: 0.00016042
Iteration 66/1000 | Loss: 0.00011894
Iteration 67/1000 | Loss: 0.00024632
Iteration 68/1000 | Loss: 0.00011504
Iteration 69/1000 | Loss: 0.00069375
Iteration 70/1000 | Loss: 0.00057436
Iteration 71/1000 | Loss: 0.00034788
Iteration 72/1000 | Loss: 0.00015643
Iteration 73/1000 | Loss: 0.00022019
Iteration 74/1000 | Loss: 0.00018734
Iteration 75/1000 | Loss: 0.00014479
Iteration 76/1000 | Loss: 0.00013753
Iteration 77/1000 | Loss: 0.00013954
Iteration 78/1000 | Loss: 0.00014138
Iteration 79/1000 | Loss: 0.00014954
Iteration 80/1000 | Loss: 0.00012349
Iteration 81/1000 | Loss: 0.00010406
Iteration 82/1000 | Loss: 0.00011109
Iteration 83/1000 | Loss: 0.00019127
Iteration 84/1000 | Loss: 0.00017107
Iteration 85/1000 | Loss: 0.00013829
Iteration 86/1000 | Loss: 0.00012930
Iteration 87/1000 | Loss: 0.00013365
Iteration 88/1000 | Loss: 0.00015298
Iteration 89/1000 | Loss: 0.00014915
Iteration 90/1000 | Loss: 0.00024769
Iteration 91/1000 | Loss: 0.00020095
Iteration 92/1000 | Loss: 0.00028154
Iteration 93/1000 | Loss: 0.00087483
Iteration 94/1000 | Loss: 0.00085006
Iteration 95/1000 | Loss: 0.00085283
Iteration 96/1000 | Loss: 0.00095307
Iteration 97/1000 | Loss: 0.00035200
Iteration 98/1000 | Loss: 0.00027532
Iteration 99/1000 | Loss: 0.00016250
Iteration 100/1000 | Loss: 0.00014736
Iteration 101/1000 | Loss: 0.00004437
Iteration 102/1000 | Loss: 0.00020358
Iteration 103/1000 | Loss: 0.00013560
Iteration 104/1000 | Loss: 0.00009410
Iteration 105/1000 | Loss: 0.00003795
Iteration 106/1000 | Loss: 0.00011049
Iteration 107/1000 | Loss: 0.00008705
Iteration 108/1000 | Loss: 0.00010369
Iteration 109/1000 | Loss: 0.00050417
Iteration 110/1000 | Loss: 0.00010133
Iteration 111/1000 | Loss: 0.00017664
Iteration 112/1000 | Loss: 0.00007876
Iteration 113/1000 | Loss: 0.00003545
Iteration 114/1000 | Loss: 0.00016258
Iteration 115/1000 | Loss: 0.00012438
Iteration 116/1000 | Loss: 0.00002801
Iteration 117/1000 | Loss: 0.00002604
Iteration 118/1000 | Loss: 0.00011930
Iteration 119/1000 | Loss: 0.00027308
Iteration 120/1000 | Loss: 0.00017770
Iteration 121/1000 | Loss: 0.00016958
Iteration 122/1000 | Loss: 0.00009946
Iteration 123/1000 | Loss: 0.00015193
Iteration 124/1000 | Loss: 0.00016361
Iteration 125/1000 | Loss: 0.00021533
Iteration 126/1000 | Loss: 0.00049630
Iteration 127/1000 | Loss: 0.00032099
Iteration 128/1000 | Loss: 0.00012639
Iteration 129/1000 | Loss: 0.00052344
Iteration 130/1000 | Loss: 0.00014174
Iteration 131/1000 | Loss: 0.00002997
Iteration 132/1000 | Loss: 0.00002844
Iteration 133/1000 | Loss: 0.00002715
Iteration 134/1000 | Loss: 0.00002603
Iteration 135/1000 | Loss: 0.00040355
Iteration 136/1000 | Loss: 0.00010125
Iteration 137/1000 | Loss: 0.00022914
Iteration 138/1000 | Loss: 0.00008355
Iteration 139/1000 | Loss: 0.00008943
Iteration 140/1000 | Loss: 0.00022216
Iteration 141/1000 | Loss: 0.00003138
Iteration 142/1000 | Loss: 0.00002840
Iteration 143/1000 | Loss: 0.00002569
Iteration 144/1000 | Loss: 0.00002340
Iteration 145/1000 | Loss: 0.00002271
Iteration 146/1000 | Loss: 0.00002222
Iteration 147/1000 | Loss: 0.00002302
Iteration 148/1000 | Loss: 0.00018200
Iteration 149/1000 | Loss: 0.00014929
Iteration 150/1000 | Loss: 0.00015922
Iteration 151/1000 | Loss: 0.00018303
Iteration 152/1000 | Loss: 0.00003508
Iteration 153/1000 | Loss: 0.00017624
Iteration 154/1000 | Loss: 0.00065026
Iteration 155/1000 | Loss: 0.00048803
Iteration 156/1000 | Loss: 0.00003839
Iteration 157/1000 | Loss: 0.00003502
Iteration 158/1000 | Loss: 0.00002836
Iteration 159/1000 | Loss: 0.00002535
Iteration 160/1000 | Loss: 0.00002423
Iteration 161/1000 | Loss: 0.00003370
Iteration 162/1000 | Loss: 0.00017431
Iteration 163/1000 | Loss: 0.00010435
Iteration 164/1000 | Loss: 0.00029660
Iteration 165/1000 | Loss: 0.00020041
Iteration 166/1000 | Loss: 0.00073753
Iteration 167/1000 | Loss: 0.00056981
Iteration 168/1000 | Loss: 0.00059044
Iteration 169/1000 | Loss: 0.00043345
Iteration 170/1000 | Loss: 0.00047569
Iteration 171/1000 | Loss: 0.00012548
Iteration 172/1000 | Loss: 0.00002715
Iteration 173/1000 | Loss: 0.00002482
Iteration 174/1000 | Loss: 0.00002313
Iteration 175/1000 | Loss: 0.00012022
Iteration 176/1000 | Loss: 0.00008612
Iteration 177/1000 | Loss: 0.00009654
Iteration 178/1000 | Loss: 0.00002764
Iteration 179/1000 | Loss: 0.00002339
Iteration 180/1000 | Loss: 0.00002617
Iteration 181/1000 | Loss: 0.00002273
Iteration 182/1000 | Loss: 0.00002147
Iteration 183/1000 | Loss: 0.00002086
Iteration 184/1000 | Loss: 0.00002026
Iteration 185/1000 | Loss: 0.00001982
Iteration 186/1000 | Loss: 0.00012986
Iteration 187/1000 | Loss: 0.00002542
Iteration 188/1000 | Loss: 0.00021845
Iteration 189/1000 | Loss: 0.00017955
Iteration 190/1000 | Loss: 0.00016103
Iteration 191/1000 | Loss: 0.00015403
Iteration 192/1000 | Loss: 0.00033904
Iteration 193/1000 | Loss: 0.00064094
Iteration 194/1000 | Loss: 0.00014598
Iteration 195/1000 | Loss: 0.00019883
Iteration 196/1000 | Loss: 0.00018157
Iteration 197/1000 | Loss: 0.00023649
Iteration 198/1000 | Loss: 0.00025625
Iteration 199/1000 | Loss: 0.00016598
Iteration 200/1000 | Loss: 0.00025563
Iteration 201/1000 | Loss: 0.00014653
Iteration 202/1000 | Loss: 0.00023973
Iteration 203/1000 | Loss: 0.00027659
Iteration 204/1000 | Loss: 0.00010570
Iteration 205/1000 | Loss: 0.00015909
Iteration 206/1000 | Loss: 0.00014330
Iteration 207/1000 | Loss: 0.00019153
Iteration 208/1000 | Loss: 0.00027529
Iteration 209/1000 | Loss: 0.00015063
Iteration 210/1000 | Loss: 0.00012497
Iteration 211/1000 | Loss: 0.00018073
Iteration 212/1000 | Loss: 0.00023409
Iteration 213/1000 | Loss: 0.00012477
Iteration 214/1000 | Loss: 0.00004485
Iteration 215/1000 | Loss: 0.00012888
Iteration 216/1000 | Loss: 0.00011591
Iteration 217/1000 | Loss: 0.00013655
Iteration 218/1000 | Loss: 0.00046648
Iteration 219/1000 | Loss: 0.00053105
Iteration 220/1000 | Loss: 0.00041365
Iteration 221/1000 | Loss: 0.00034143
Iteration 222/1000 | Loss: 0.00104612
Iteration 223/1000 | Loss: 0.00053079
Iteration 224/1000 | Loss: 0.00003494
Iteration 225/1000 | Loss: 0.00045411
Iteration 226/1000 | Loss: 0.00008661
Iteration 227/1000 | Loss: 0.00006492
Iteration 228/1000 | Loss: 0.00002683
Iteration 229/1000 | Loss: 0.00002577
Iteration 230/1000 | Loss: 0.00002431
Iteration 231/1000 | Loss: 0.00018007
Iteration 232/1000 | Loss: 0.00005685
Iteration 233/1000 | Loss: 0.00019140
Iteration 234/1000 | Loss: 0.00005908
Iteration 235/1000 | Loss: 0.00011046
Iteration 236/1000 | Loss: 0.00010001
Iteration 237/1000 | Loss: 0.00008153
Iteration 238/1000 | Loss: 0.00009768
Iteration 239/1000 | Loss: 0.00002796
Iteration 240/1000 | Loss: 0.00002394
Iteration 241/1000 | Loss: 0.00026523
Iteration 242/1000 | Loss: 0.00012908
Iteration 243/1000 | Loss: 0.00002875
Iteration 244/1000 | Loss: 0.00052072
Iteration 245/1000 | Loss: 0.00020213
Iteration 246/1000 | Loss: 0.00024830
Iteration 247/1000 | Loss: 0.00018608
Iteration 248/1000 | Loss: 0.00002958
Iteration 249/1000 | Loss: 0.00018432
Iteration 250/1000 | Loss: 0.00011279
Iteration 251/1000 | Loss: 0.00002333
Iteration 252/1000 | Loss: 0.00002146
Iteration 253/1000 | Loss: 0.00014844
Iteration 254/1000 | Loss: 0.00013070
Iteration 255/1000 | Loss: 0.00005063
Iteration 256/1000 | Loss: 0.00008319
Iteration 257/1000 | Loss: 0.00008437
Iteration 258/1000 | Loss: 0.00007978
Iteration 259/1000 | Loss: 0.00007546
Iteration 260/1000 | Loss: 0.00007529
Iteration 261/1000 | Loss: 0.00007132
Iteration 262/1000 | Loss: 0.00012297
Iteration 263/1000 | Loss: 0.00003207
Iteration 264/1000 | Loss: 0.00010705
Iteration 265/1000 | Loss: 0.00006159
Iteration 266/1000 | Loss: 0.00002409
Iteration 267/1000 | Loss: 0.00010550
Iteration 268/1000 | Loss: 0.00002334
Iteration 269/1000 | Loss: 0.00002426
Iteration 270/1000 | Loss: 0.00010568
Iteration 271/1000 | Loss: 0.00002196
Iteration 272/1000 | Loss: 0.00012555
Iteration 273/1000 | Loss: 0.00009962
Iteration 274/1000 | Loss: 0.00012405
Iteration 275/1000 | Loss: 0.00006233
Iteration 276/1000 | Loss: 0.00002345
Iteration 277/1000 | Loss: 0.00002211
Iteration 278/1000 | Loss: 0.00005335
Iteration 279/1000 | Loss: 0.00014966
Iteration 280/1000 | Loss: 0.00003916
Iteration 281/1000 | Loss: 0.00010558
Iteration 282/1000 | Loss: 0.00007376
Iteration 283/1000 | Loss: 0.00002765
Iteration 284/1000 | Loss: 0.00004612
Iteration 285/1000 | Loss: 0.00004663
Iteration 286/1000 | Loss: 0.00005779
Iteration 287/1000 | Loss: 0.00008077
Iteration 288/1000 | Loss: 0.00005966
Iteration 289/1000 | Loss: 0.00005999
Iteration 290/1000 | Loss: 0.00002596
Iteration 291/1000 | Loss: 0.00007144
Iteration 292/1000 | Loss: 0.00004996
Iteration 293/1000 | Loss: 0.00010010
Iteration 294/1000 | Loss: 0.00002400
Iteration 295/1000 | Loss: 0.00001976
Iteration 296/1000 | Loss: 0.00001901
Iteration 297/1000 | Loss: 0.00001867
Iteration 298/1000 | Loss: 0.00013562
Iteration 299/1000 | Loss: 0.00015476
Iteration 300/1000 | Loss: 0.00002236
Iteration 301/1000 | Loss: 0.00002077
Iteration 302/1000 | Loss: 0.00018016
Iteration 303/1000 | Loss: 0.00014040
Iteration 304/1000 | Loss: 0.00010996
Iteration 305/1000 | Loss: 0.00012766
Iteration 306/1000 | Loss: 0.00012202
Iteration 307/1000 | Loss: 0.00012605
Iteration 308/1000 | Loss: 0.00006855
Iteration 309/1000 | Loss: 0.00007933
Iteration 310/1000 | Loss: 0.00015201
Iteration 311/1000 | Loss: 0.00016627
Iteration 312/1000 | Loss: 0.00015004
Iteration 313/1000 | Loss: 0.00002208
Iteration 314/1000 | Loss: 0.00001939
Iteration 315/1000 | Loss: 0.00001826
Iteration 316/1000 | Loss: 0.00001760
Iteration 317/1000 | Loss: 0.00001736
Iteration 318/1000 | Loss: 0.00001713
Iteration 319/1000 | Loss: 0.00001712
Iteration 320/1000 | Loss: 0.00001711
Iteration 321/1000 | Loss: 0.00001707
Iteration 322/1000 | Loss: 0.00001706
Iteration 323/1000 | Loss: 0.00001705
Iteration 324/1000 | Loss: 0.00001705
Iteration 325/1000 | Loss: 0.00001704
Iteration 326/1000 | Loss: 0.00001703
Iteration 327/1000 | Loss: 0.00001702
Iteration 328/1000 | Loss: 0.00001701
Iteration 329/1000 | Loss: 0.00001701
Iteration 330/1000 | Loss: 0.00001700
Iteration 331/1000 | Loss: 0.00001700
Iteration 332/1000 | Loss: 0.00001700
Iteration 333/1000 | Loss: 0.00001696
Iteration 334/1000 | Loss: 0.00001696
Iteration 335/1000 | Loss: 0.00001694
Iteration 336/1000 | Loss: 0.00001694
Iteration 337/1000 | Loss: 0.00001694
Iteration 338/1000 | Loss: 0.00001693
Iteration 339/1000 | Loss: 0.00001693
Iteration 340/1000 | Loss: 0.00001693
Iteration 341/1000 | Loss: 0.00001692
Iteration 342/1000 | Loss: 0.00001692
Iteration 343/1000 | Loss: 0.00001692
Iteration 344/1000 | Loss: 0.00001692
Iteration 345/1000 | Loss: 0.00001691
Iteration 346/1000 | Loss: 0.00001691
Iteration 347/1000 | Loss: 0.00001691
Iteration 348/1000 | Loss: 0.00001691
Iteration 349/1000 | Loss: 0.00001690
Iteration 350/1000 | Loss: 0.00001690
Iteration 351/1000 | Loss: 0.00001690
Iteration 352/1000 | Loss: 0.00001689
Iteration 353/1000 | Loss: 0.00001689
Iteration 354/1000 | Loss: 0.00001689
Iteration 355/1000 | Loss: 0.00001688
Iteration 356/1000 | Loss: 0.00001688
Iteration 357/1000 | Loss: 0.00001688
Iteration 358/1000 | Loss: 0.00001687
Iteration 359/1000 | Loss: 0.00001687
Iteration 360/1000 | Loss: 0.00001687
Iteration 361/1000 | Loss: 0.00001686
Iteration 362/1000 | Loss: 0.00001686
Iteration 363/1000 | Loss: 0.00001686
Iteration 364/1000 | Loss: 0.00001686
Iteration 365/1000 | Loss: 0.00001686
Iteration 366/1000 | Loss: 0.00001686
Iteration 367/1000 | Loss: 0.00001686
Iteration 368/1000 | Loss: 0.00001685
Iteration 369/1000 | Loss: 0.00001685
Iteration 370/1000 | Loss: 0.00001685
Iteration 371/1000 | Loss: 0.00001685
Iteration 372/1000 | Loss: 0.00001685
Iteration 373/1000 | Loss: 0.00001685
Iteration 374/1000 | Loss: 0.00001685
Iteration 375/1000 | Loss: 0.00001685
Iteration 376/1000 | Loss: 0.00001685
Iteration 377/1000 | Loss: 0.00001685
Iteration 378/1000 | Loss: 0.00001684
Iteration 379/1000 | Loss: 0.00001684
Iteration 380/1000 | Loss: 0.00001684
Iteration 381/1000 | Loss: 0.00001684
Iteration 382/1000 | Loss: 0.00001684
Iteration 383/1000 | Loss: 0.00001684
Iteration 384/1000 | Loss: 0.00001684
Iteration 385/1000 | Loss: 0.00001684
Iteration 386/1000 | Loss: 0.00001684
Iteration 387/1000 | Loss: 0.00001684
Iteration 388/1000 | Loss: 0.00001684
Iteration 389/1000 | Loss: 0.00001684
Iteration 390/1000 | Loss: 0.00001684
Iteration 391/1000 | Loss: 0.00001684
Iteration 392/1000 | Loss: 0.00001684
Iteration 393/1000 | Loss: 0.00001684
Iteration 394/1000 | Loss: 0.00001684
Iteration 395/1000 | Loss: 0.00001684
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 395. Stopping optimization.
Last 5 losses: [1.683872142166365e-05, 1.683872142166365e-05, 1.683872142166365e-05, 1.683872142166365e-05, 1.683872142166365e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.683872142166365e-05

Optimization complete. Final v2v error: 3.2591254711151123 mm

Highest mean error: 12.077061653137207 mm for frame 225

Lowest mean error: 2.905024766921997 mm for frame 233

Saving results

Total time: 561.7328658103943
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_032/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_032/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00493080
Iteration 2/25 | Loss: 0.00140294
Iteration 3/25 | Loss: 0.00130387
Iteration 4/25 | Loss: 0.00128578
Iteration 5/25 | Loss: 0.00127987
Iteration 6/25 | Loss: 0.00127969
Iteration 7/25 | Loss: 0.00127969
Iteration 8/25 | Loss: 0.00127969
Iteration 9/25 | Loss: 0.00127969
Iteration 10/25 | Loss: 0.00127969
Iteration 11/25 | Loss: 0.00127969
Iteration 12/25 | Loss: 0.00127969
Iteration 13/25 | Loss: 0.00127969
Iteration 14/25 | Loss: 0.00127969
Iteration 15/25 | Loss: 0.00127969
Iteration 16/25 | Loss: 0.00127969
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012796921655535698, 0.0012796921655535698, 0.0012796921655535698, 0.0012796921655535698, 0.0012796921655535698]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012796921655535698

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41506517
Iteration 2/25 | Loss: 0.00084206
Iteration 3/25 | Loss: 0.00084206
Iteration 4/25 | Loss: 0.00084206
Iteration 5/25 | Loss: 0.00084205
Iteration 6/25 | Loss: 0.00084205
Iteration 7/25 | Loss: 0.00084205
Iteration 8/25 | Loss: 0.00084205
Iteration 9/25 | Loss: 0.00084205
Iteration 10/25 | Loss: 0.00084205
Iteration 11/25 | Loss: 0.00084205
Iteration 12/25 | Loss: 0.00084205
Iteration 13/25 | Loss: 0.00084205
Iteration 14/25 | Loss: 0.00084205
Iteration 15/25 | Loss: 0.00084205
Iteration 16/25 | Loss: 0.00084205
Iteration 17/25 | Loss: 0.00084205
Iteration 18/25 | Loss: 0.00084205
Iteration 19/25 | Loss: 0.00084205
Iteration 20/25 | Loss: 0.00084205
Iteration 21/25 | Loss: 0.00084205
Iteration 22/25 | Loss: 0.00084205
Iteration 23/25 | Loss: 0.00084205
Iteration 24/25 | Loss: 0.00084205
Iteration 25/25 | Loss: 0.00084205

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084205
Iteration 2/1000 | Loss: 0.00003937
Iteration 3/1000 | Loss: 0.00002954
Iteration 4/1000 | Loss: 0.00002699
Iteration 5/1000 | Loss: 0.00002584
Iteration 6/1000 | Loss: 0.00002515
Iteration 7/1000 | Loss: 0.00002448
Iteration 8/1000 | Loss: 0.00002415
Iteration 9/1000 | Loss: 0.00002386
Iteration 10/1000 | Loss: 0.00002357
Iteration 11/1000 | Loss: 0.00002338
Iteration 12/1000 | Loss: 0.00002317
Iteration 13/1000 | Loss: 0.00002314
Iteration 14/1000 | Loss: 0.00002313
Iteration 15/1000 | Loss: 0.00002300
Iteration 16/1000 | Loss: 0.00002300
Iteration 17/1000 | Loss: 0.00002299
Iteration 18/1000 | Loss: 0.00002294
Iteration 19/1000 | Loss: 0.00002292
Iteration 20/1000 | Loss: 0.00002289
Iteration 21/1000 | Loss: 0.00002289
Iteration 22/1000 | Loss: 0.00002282
Iteration 23/1000 | Loss: 0.00002273
Iteration 24/1000 | Loss: 0.00002272
Iteration 25/1000 | Loss: 0.00002271
Iteration 26/1000 | Loss: 0.00002267
Iteration 27/1000 | Loss: 0.00002267
Iteration 28/1000 | Loss: 0.00002264
Iteration 29/1000 | Loss: 0.00002264
Iteration 30/1000 | Loss: 0.00002264
Iteration 31/1000 | Loss: 0.00002264
Iteration 32/1000 | Loss: 0.00002264
Iteration 33/1000 | Loss: 0.00002264
Iteration 34/1000 | Loss: 0.00002264
Iteration 35/1000 | Loss: 0.00002262
Iteration 36/1000 | Loss: 0.00002258
Iteration 37/1000 | Loss: 0.00002257
Iteration 38/1000 | Loss: 0.00002257
Iteration 39/1000 | Loss: 0.00002256
Iteration 40/1000 | Loss: 0.00002256
Iteration 41/1000 | Loss: 0.00002255
Iteration 42/1000 | Loss: 0.00002255
Iteration 43/1000 | Loss: 0.00002255
Iteration 44/1000 | Loss: 0.00002255
Iteration 45/1000 | Loss: 0.00002254
Iteration 46/1000 | Loss: 0.00002254
Iteration 47/1000 | Loss: 0.00002254
Iteration 48/1000 | Loss: 0.00002253
Iteration 49/1000 | Loss: 0.00002253
Iteration 50/1000 | Loss: 0.00002253
Iteration 51/1000 | Loss: 0.00002253
Iteration 52/1000 | Loss: 0.00002252
Iteration 53/1000 | Loss: 0.00002252
Iteration 54/1000 | Loss: 0.00002251
Iteration 55/1000 | Loss: 0.00002251
Iteration 56/1000 | Loss: 0.00002251
Iteration 57/1000 | Loss: 0.00002251
Iteration 58/1000 | Loss: 0.00002251
Iteration 59/1000 | Loss: 0.00002250
Iteration 60/1000 | Loss: 0.00002250
Iteration 61/1000 | Loss: 0.00002250
Iteration 62/1000 | Loss: 0.00002250
Iteration 63/1000 | Loss: 0.00002250
Iteration 64/1000 | Loss: 0.00002250
Iteration 65/1000 | Loss: 0.00002250
Iteration 66/1000 | Loss: 0.00002250
Iteration 67/1000 | Loss: 0.00002250
Iteration 68/1000 | Loss: 0.00002250
Iteration 69/1000 | Loss: 0.00002249
Iteration 70/1000 | Loss: 0.00002249
Iteration 71/1000 | Loss: 0.00002249
Iteration 72/1000 | Loss: 0.00002248
Iteration 73/1000 | Loss: 0.00002248
Iteration 74/1000 | Loss: 0.00002248
Iteration 75/1000 | Loss: 0.00002248
Iteration 76/1000 | Loss: 0.00002247
Iteration 77/1000 | Loss: 0.00002247
Iteration 78/1000 | Loss: 0.00002247
Iteration 79/1000 | Loss: 0.00002246
Iteration 80/1000 | Loss: 0.00002246
Iteration 81/1000 | Loss: 0.00002246
Iteration 82/1000 | Loss: 0.00002246
Iteration 83/1000 | Loss: 0.00002245
Iteration 84/1000 | Loss: 0.00002245
Iteration 85/1000 | Loss: 0.00002245
Iteration 86/1000 | Loss: 0.00002245
Iteration 87/1000 | Loss: 0.00002244
Iteration 88/1000 | Loss: 0.00002244
Iteration 89/1000 | Loss: 0.00002244
Iteration 90/1000 | Loss: 0.00002244
Iteration 91/1000 | Loss: 0.00002244
Iteration 92/1000 | Loss: 0.00002244
Iteration 93/1000 | Loss: 0.00002244
Iteration 94/1000 | Loss: 0.00002244
Iteration 95/1000 | Loss: 0.00002244
Iteration 96/1000 | Loss: 0.00002244
Iteration 97/1000 | Loss: 0.00002244
Iteration 98/1000 | Loss: 0.00002244
Iteration 99/1000 | Loss: 0.00002244
Iteration 100/1000 | Loss: 0.00002244
Iteration 101/1000 | Loss: 0.00002244
Iteration 102/1000 | Loss: 0.00002243
Iteration 103/1000 | Loss: 0.00002243
Iteration 104/1000 | Loss: 0.00002243
Iteration 105/1000 | Loss: 0.00002243
Iteration 106/1000 | Loss: 0.00002243
Iteration 107/1000 | Loss: 0.00002242
Iteration 108/1000 | Loss: 0.00002242
Iteration 109/1000 | Loss: 0.00002242
Iteration 110/1000 | Loss: 0.00002242
Iteration 111/1000 | Loss: 0.00002242
Iteration 112/1000 | Loss: 0.00002242
Iteration 113/1000 | Loss: 0.00002242
Iteration 114/1000 | Loss: 0.00002242
Iteration 115/1000 | Loss: 0.00002242
Iteration 116/1000 | Loss: 0.00002241
Iteration 117/1000 | Loss: 0.00002241
Iteration 118/1000 | Loss: 0.00002241
Iteration 119/1000 | Loss: 0.00002241
Iteration 120/1000 | Loss: 0.00002241
Iteration 121/1000 | Loss: 0.00002241
Iteration 122/1000 | Loss: 0.00002241
Iteration 123/1000 | Loss: 0.00002241
Iteration 124/1000 | Loss: 0.00002240
Iteration 125/1000 | Loss: 0.00002240
Iteration 126/1000 | Loss: 0.00002240
Iteration 127/1000 | Loss: 0.00002240
Iteration 128/1000 | Loss: 0.00002240
Iteration 129/1000 | Loss: 0.00002239
Iteration 130/1000 | Loss: 0.00002239
Iteration 131/1000 | Loss: 0.00002239
Iteration 132/1000 | Loss: 0.00002239
Iteration 133/1000 | Loss: 0.00002239
Iteration 134/1000 | Loss: 0.00002239
Iteration 135/1000 | Loss: 0.00002239
Iteration 136/1000 | Loss: 0.00002239
Iteration 137/1000 | Loss: 0.00002239
Iteration 138/1000 | Loss: 0.00002238
Iteration 139/1000 | Loss: 0.00002238
Iteration 140/1000 | Loss: 0.00002238
Iteration 141/1000 | Loss: 0.00002238
Iteration 142/1000 | Loss: 0.00002238
Iteration 143/1000 | Loss: 0.00002238
Iteration 144/1000 | Loss: 0.00002238
Iteration 145/1000 | Loss: 0.00002237
Iteration 146/1000 | Loss: 0.00002237
Iteration 147/1000 | Loss: 0.00002237
Iteration 148/1000 | Loss: 0.00002237
Iteration 149/1000 | Loss: 0.00002237
Iteration 150/1000 | Loss: 0.00002237
Iteration 151/1000 | Loss: 0.00002237
Iteration 152/1000 | Loss: 0.00002237
Iteration 153/1000 | Loss: 0.00002237
Iteration 154/1000 | Loss: 0.00002237
Iteration 155/1000 | Loss: 0.00002237
Iteration 156/1000 | Loss: 0.00002237
Iteration 157/1000 | Loss: 0.00002237
Iteration 158/1000 | Loss: 0.00002237
Iteration 159/1000 | Loss: 0.00002237
Iteration 160/1000 | Loss: 0.00002237
Iteration 161/1000 | Loss: 0.00002237
Iteration 162/1000 | Loss: 0.00002237
Iteration 163/1000 | Loss: 0.00002237
Iteration 164/1000 | Loss: 0.00002237
Iteration 165/1000 | Loss: 0.00002237
Iteration 166/1000 | Loss: 0.00002237
Iteration 167/1000 | Loss: 0.00002237
Iteration 168/1000 | Loss: 0.00002237
Iteration 169/1000 | Loss: 0.00002237
Iteration 170/1000 | Loss: 0.00002237
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [2.2370120859704912e-05, 2.2370120859704912e-05, 2.2370120859704912e-05, 2.2370120859704912e-05, 2.2370120859704912e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2370120859704912e-05

Optimization complete. Final v2v error: 3.7240562438964844 mm

Highest mean error: 4.72245454788208 mm for frame 76

Lowest mean error: 3.273667097091675 mm for frame 114

Saving results

Total time: 41.57786226272583
