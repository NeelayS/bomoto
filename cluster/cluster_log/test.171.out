Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=171, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 9576-9631
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00936602
Iteration 2/25 | Loss: 0.00147973
Iteration 3/25 | Loss: 0.00096259
Iteration 4/25 | Loss: 0.00090333
Iteration 5/25 | Loss: 0.00088076
Iteration 6/25 | Loss: 0.00087307
Iteration 7/25 | Loss: 0.00087204
Iteration 8/25 | Loss: 0.00087199
Iteration 9/25 | Loss: 0.00087199
Iteration 10/25 | Loss: 0.00087199
Iteration 11/25 | Loss: 0.00087199
Iteration 12/25 | Loss: 0.00087199
Iteration 13/25 | Loss: 0.00087199
Iteration 14/25 | Loss: 0.00087199
Iteration 15/25 | Loss: 0.00087199
Iteration 16/25 | Loss: 0.00087199
Iteration 17/25 | Loss: 0.00087199
Iteration 18/25 | Loss: 0.00087199
Iteration 19/25 | Loss: 0.00087199
Iteration 20/25 | Loss: 0.00087199
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008719926117919385, 0.0008719926117919385, 0.0008719926117919385, 0.0008719926117919385, 0.0008719926117919385]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008719926117919385

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.12450576
Iteration 2/25 | Loss: 0.00040568
Iteration 3/25 | Loss: 0.00040567
Iteration 4/25 | Loss: 0.00040566
Iteration 5/25 | Loss: 0.00040566
Iteration 6/25 | Loss: 0.00040566
Iteration 7/25 | Loss: 0.00040566
Iteration 8/25 | Loss: 0.00040566
Iteration 9/25 | Loss: 0.00040566
Iteration 10/25 | Loss: 0.00040566
Iteration 11/25 | Loss: 0.00040566
Iteration 12/25 | Loss: 0.00040566
Iteration 13/25 | Loss: 0.00040566
Iteration 14/25 | Loss: 0.00040566
Iteration 15/25 | Loss: 0.00040566
Iteration 16/25 | Loss: 0.00040566
Iteration 17/25 | Loss: 0.00040566
Iteration 18/25 | Loss: 0.00040566
Iteration 19/25 | Loss: 0.00040566
Iteration 20/25 | Loss: 0.00040566
Iteration 21/25 | Loss: 0.00040566
Iteration 22/25 | Loss: 0.00040566
Iteration 23/25 | Loss: 0.00040566
Iteration 24/25 | Loss: 0.00040566
Iteration 25/25 | Loss: 0.00040566

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00040566
Iteration 2/1000 | Loss: 0.00004731
Iteration 3/1000 | Loss: 0.00003781
Iteration 4/1000 | Loss: 0.00003464
Iteration 5/1000 | Loss: 0.00003247
Iteration 6/1000 | Loss: 0.00003081
Iteration 7/1000 | Loss: 0.00002970
Iteration 8/1000 | Loss: 0.00002884
Iteration 9/1000 | Loss: 0.00002842
Iteration 10/1000 | Loss: 0.00002811
Iteration 11/1000 | Loss: 0.00002788
Iteration 12/1000 | Loss: 0.00002770
Iteration 13/1000 | Loss: 0.00002756
Iteration 14/1000 | Loss: 0.00002748
Iteration 15/1000 | Loss: 0.00002739
Iteration 16/1000 | Loss: 0.00002736
Iteration 17/1000 | Loss: 0.00002735
Iteration 18/1000 | Loss: 0.00002733
Iteration 19/1000 | Loss: 0.00002733
Iteration 20/1000 | Loss: 0.00002732
Iteration 21/1000 | Loss: 0.00002732
Iteration 22/1000 | Loss: 0.00002732
Iteration 23/1000 | Loss: 0.00002731
Iteration 24/1000 | Loss: 0.00002725
Iteration 25/1000 | Loss: 0.00002720
Iteration 26/1000 | Loss: 0.00002714
Iteration 27/1000 | Loss: 0.00002714
Iteration 28/1000 | Loss: 0.00002713
Iteration 29/1000 | Loss: 0.00002712
Iteration 30/1000 | Loss: 0.00002712
Iteration 31/1000 | Loss: 0.00002710
Iteration 32/1000 | Loss: 0.00002709
Iteration 33/1000 | Loss: 0.00002707
Iteration 34/1000 | Loss: 0.00002707
Iteration 35/1000 | Loss: 0.00002704
Iteration 36/1000 | Loss: 0.00002702
Iteration 37/1000 | Loss: 0.00002701
Iteration 38/1000 | Loss: 0.00002700
Iteration 39/1000 | Loss: 0.00002700
Iteration 40/1000 | Loss: 0.00002699
Iteration 41/1000 | Loss: 0.00002698
Iteration 42/1000 | Loss: 0.00002698
Iteration 43/1000 | Loss: 0.00002698
Iteration 44/1000 | Loss: 0.00002697
Iteration 45/1000 | Loss: 0.00002697
Iteration 46/1000 | Loss: 0.00002696
Iteration 47/1000 | Loss: 0.00002694
Iteration 48/1000 | Loss: 0.00002694
Iteration 49/1000 | Loss: 0.00002693
Iteration 50/1000 | Loss: 0.00002693
Iteration 51/1000 | Loss: 0.00002692
Iteration 52/1000 | Loss: 0.00002690
Iteration 53/1000 | Loss: 0.00002689
Iteration 54/1000 | Loss: 0.00002689
Iteration 55/1000 | Loss: 0.00002688
Iteration 56/1000 | Loss: 0.00002688
Iteration 57/1000 | Loss: 0.00002686
Iteration 58/1000 | Loss: 0.00002686
Iteration 59/1000 | Loss: 0.00002686
Iteration 60/1000 | Loss: 0.00002685
Iteration 61/1000 | Loss: 0.00002685
Iteration 62/1000 | Loss: 0.00002685
Iteration 63/1000 | Loss: 0.00002685
Iteration 64/1000 | Loss: 0.00002685
Iteration 65/1000 | Loss: 0.00002684
Iteration 66/1000 | Loss: 0.00002684
Iteration 67/1000 | Loss: 0.00002684
Iteration 68/1000 | Loss: 0.00002683
Iteration 69/1000 | Loss: 0.00002683
Iteration 70/1000 | Loss: 0.00002683
Iteration 71/1000 | Loss: 0.00002683
Iteration 72/1000 | Loss: 0.00002683
Iteration 73/1000 | Loss: 0.00002683
Iteration 74/1000 | Loss: 0.00002683
Iteration 75/1000 | Loss: 0.00002682
Iteration 76/1000 | Loss: 0.00002682
Iteration 77/1000 | Loss: 0.00002682
Iteration 78/1000 | Loss: 0.00002681
Iteration 79/1000 | Loss: 0.00002681
Iteration 80/1000 | Loss: 0.00002681
Iteration 81/1000 | Loss: 0.00002681
Iteration 82/1000 | Loss: 0.00002681
Iteration 83/1000 | Loss: 0.00002681
Iteration 84/1000 | Loss: 0.00002681
Iteration 85/1000 | Loss: 0.00002681
Iteration 86/1000 | Loss: 0.00002681
Iteration 87/1000 | Loss: 0.00002680
Iteration 88/1000 | Loss: 0.00002680
Iteration 89/1000 | Loss: 0.00002680
Iteration 90/1000 | Loss: 0.00002680
Iteration 91/1000 | Loss: 0.00002680
Iteration 92/1000 | Loss: 0.00002680
Iteration 93/1000 | Loss: 0.00002680
Iteration 94/1000 | Loss: 0.00002680
Iteration 95/1000 | Loss: 0.00002679
Iteration 96/1000 | Loss: 0.00002679
Iteration 97/1000 | Loss: 0.00002679
Iteration 98/1000 | Loss: 0.00002679
Iteration 99/1000 | Loss: 0.00002679
Iteration 100/1000 | Loss: 0.00002679
Iteration 101/1000 | Loss: 0.00002679
Iteration 102/1000 | Loss: 0.00002679
Iteration 103/1000 | Loss: 0.00002678
Iteration 104/1000 | Loss: 0.00002678
Iteration 105/1000 | Loss: 0.00002678
Iteration 106/1000 | Loss: 0.00002678
Iteration 107/1000 | Loss: 0.00002678
Iteration 108/1000 | Loss: 0.00002678
Iteration 109/1000 | Loss: 0.00002678
Iteration 110/1000 | Loss: 0.00002678
Iteration 111/1000 | Loss: 0.00002678
Iteration 112/1000 | Loss: 0.00002678
Iteration 113/1000 | Loss: 0.00002678
Iteration 114/1000 | Loss: 0.00002677
Iteration 115/1000 | Loss: 0.00002677
Iteration 116/1000 | Loss: 0.00002677
Iteration 117/1000 | Loss: 0.00002677
Iteration 118/1000 | Loss: 0.00002677
Iteration 119/1000 | Loss: 0.00002677
Iteration 120/1000 | Loss: 0.00002677
Iteration 121/1000 | Loss: 0.00002677
Iteration 122/1000 | Loss: 0.00002677
Iteration 123/1000 | Loss: 0.00002677
Iteration 124/1000 | Loss: 0.00002677
Iteration 125/1000 | Loss: 0.00002677
Iteration 126/1000 | Loss: 0.00002676
Iteration 127/1000 | Loss: 0.00002676
Iteration 128/1000 | Loss: 0.00002676
Iteration 129/1000 | Loss: 0.00002676
Iteration 130/1000 | Loss: 0.00002676
Iteration 131/1000 | Loss: 0.00002676
Iteration 132/1000 | Loss: 0.00002676
Iteration 133/1000 | Loss: 0.00002676
Iteration 134/1000 | Loss: 0.00002676
Iteration 135/1000 | Loss: 0.00002676
Iteration 136/1000 | Loss: 0.00002676
Iteration 137/1000 | Loss: 0.00002676
Iteration 138/1000 | Loss: 0.00002675
Iteration 139/1000 | Loss: 0.00002675
Iteration 140/1000 | Loss: 0.00002675
Iteration 141/1000 | Loss: 0.00002674
Iteration 142/1000 | Loss: 0.00002674
Iteration 143/1000 | Loss: 0.00002674
Iteration 144/1000 | Loss: 0.00002674
Iteration 145/1000 | Loss: 0.00002674
Iteration 146/1000 | Loss: 0.00002674
Iteration 147/1000 | Loss: 0.00002674
Iteration 148/1000 | Loss: 0.00002674
Iteration 149/1000 | Loss: 0.00002674
Iteration 150/1000 | Loss: 0.00002674
Iteration 151/1000 | Loss: 0.00002674
Iteration 152/1000 | Loss: 0.00002673
Iteration 153/1000 | Loss: 0.00002673
Iteration 154/1000 | Loss: 0.00002673
Iteration 155/1000 | Loss: 0.00002673
Iteration 156/1000 | Loss: 0.00002673
Iteration 157/1000 | Loss: 0.00002673
Iteration 158/1000 | Loss: 0.00002673
Iteration 159/1000 | Loss: 0.00002673
Iteration 160/1000 | Loss: 0.00002673
Iteration 161/1000 | Loss: 0.00002672
Iteration 162/1000 | Loss: 0.00002672
Iteration 163/1000 | Loss: 0.00002672
Iteration 164/1000 | Loss: 0.00002672
Iteration 165/1000 | Loss: 0.00002672
Iteration 166/1000 | Loss: 0.00002672
Iteration 167/1000 | Loss: 0.00002672
Iteration 168/1000 | Loss: 0.00002672
Iteration 169/1000 | Loss: 0.00002672
Iteration 170/1000 | Loss: 0.00002672
Iteration 171/1000 | Loss: 0.00002672
Iteration 172/1000 | Loss: 0.00002672
Iteration 173/1000 | Loss: 0.00002672
Iteration 174/1000 | Loss: 0.00002672
Iteration 175/1000 | Loss: 0.00002672
Iteration 176/1000 | Loss: 0.00002671
Iteration 177/1000 | Loss: 0.00002671
Iteration 178/1000 | Loss: 0.00002671
Iteration 179/1000 | Loss: 0.00002671
Iteration 180/1000 | Loss: 0.00002671
Iteration 181/1000 | Loss: 0.00002671
Iteration 182/1000 | Loss: 0.00002671
Iteration 183/1000 | Loss: 0.00002671
Iteration 184/1000 | Loss: 0.00002671
Iteration 185/1000 | Loss: 0.00002671
Iteration 186/1000 | Loss: 0.00002671
Iteration 187/1000 | Loss: 0.00002671
Iteration 188/1000 | Loss: 0.00002671
Iteration 189/1000 | Loss: 0.00002671
Iteration 190/1000 | Loss: 0.00002671
Iteration 191/1000 | Loss: 0.00002671
Iteration 192/1000 | Loss: 0.00002671
Iteration 193/1000 | Loss: 0.00002671
Iteration 194/1000 | Loss: 0.00002671
Iteration 195/1000 | Loss: 0.00002671
Iteration 196/1000 | Loss: 0.00002671
Iteration 197/1000 | Loss: 0.00002671
Iteration 198/1000 | Loss: 0.00002671
Iteration 199/1000 | Loss: 0.00002671
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [2.6712086764746346e-05, 2.6712086764746346e-05, 2.6712086764746346e-05, 2.6712086764746346e-05, 2.6712086764746346e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6712086764746346e-05

Optimization complete. Final v2v error: 4.2166924476623535 mm

Highest mean error: 5.414915561676025 mm for frame 103

Lowest mean error: 3.445338726043701 mm for frame 39

Saving results

Total time: 50.23249316215515
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01118644
Iteration 2/25 | Loss: 0.00505499
Iteration 3/25 | Loss: 0.00298657
Iteration 4/25 | Loss: 0.00265050
Iteration 5/25 | Loss: 0.00242926
Iteration 6/25 | Loss: 0.00220588
Iteration 7/25 | Loss: 0.00207818
Iteration 8/25 | Loss: 0.00200303
Iteration 9/25 | Loss: 0.00199665
Iteration 10/25 | Loss: 0.00194469
Iteration 11/25 | Loss: 0.00190609
Iteration 12/25 | Loss: 0.00188875
Iteration 13/25 | Loss: 0.00187465
Iteration 14/25 | Loss: 0.00186943
Iteration 15/25 | Loss: 0.00185290
Iteration 16/25 | Loss: 0.00185512
Iteration 17/25 | Loss: 0.00185522
Iteration 18/25 | Loss: 0.00184578
Iteration 19/25 | Loss: 0.00183947
Iteration 20/25 | Loss: 0.00182172
Iteration 21/25 | Loss: 0.00178107
Iteration 22/25 | Loss: 0.00174719
Iteration 23/25 | Loss: 0.00171953
Iteration 24/25 | Loss: 0.00169596
Iteration 25/25 | Loss: 0.00166370

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.03365040
Iteration 2/25 | Loss: 0.00904185
Iteration 3/25 | Loss: 0.00763897
Iteration 4/25 | Loss: 0.00763889
Iteration 5/25 | Loss: 0.00763889
Iteration 6/25 | Loss: 0.00763888
Iteration 7/25 | Loss: 0.00763888
Iteration 8/25 | Loss: 0.00763888
Iteration 9/25 | Loss: 0.00763888
Iteration 10/25 | Loss: 0.00763888
Iteration 11/25 | Loss: 0.00763888
Iteration 12/25 | Loss: 0.00763888
Iteration 13/25 | Loss: 0.00763888
Iteration 14/25 | Loss: 0.00763888
Iteration 15/25 | Loss: 0.00763888
Iteration 16/25 | Loss: 0.00763888
Iteration 17/25 | Loss: 0.00763888
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0076388828456401825, 0.0076388828456401825, 0.0076388828456401825, 0.0076388828456401825, 0.0076388828456401825]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0076388828456401825

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00763888
Iteration 2/1000 | Loss: 0.00551926
Iteration 3/1000 | Loss: 0.00223184
Iteration 4/1000 | Loss: 0.00106750
Iteration 5/1000 | Loss: 0.00091554
Iteration 6/1000 | Loss: 0.00063891
Iteration 7/1000 | Loss: 0.00052045
Iteration 8/1000 | Loss: 0.00093219
Iteration 9/1000 | Loss: 0.00093433
Iteration 10/1000 | Loss: 0.00055651
Iteration 11/1000 | Loss: 0.00133232
Iteration 12/1000 | Loss: 0.00040448
Iteration 13/1000 | Loss: 0.00149198
Iteration 14/1000 | Loss: 0.00191217
Iteration 15/1000 | Loss: 0.00043171
Iteration 16/1000 | Loss: 0.00032404
Iteration 17/1000 | Loss: 0.00093592
Iteration 18/1000 | Loss: 0.00040094
Iteration 19/1000 | Loss: 0.00035249
Iteration 20/1000 | Loss: 0.00207744
Iteration 21/1000 | Loss: 0.00237617
Iteration 22/1000 | Loss: 0.00250402
Iteration 23/1000 | Loss: 0.00132162
Iteration 24/1000 | Loss: 0.00434851
Iteration 25/1000 | Loss: 0.00856923
Iteration 26/1000 | Loss: 0.00378713
Iteration 27/1000 | Loss: 0.00107308
Iteration 28/1000 | Loss: 0.00091680
Iteration 29/1000 | Loss: 0.00085031
Iteration 30/1000 | Loss: 0.00053192
Iteration 31/1000 | Loss: 0.00134742
Iteration 32/1000 | Loss: 0.00098154
Iteration 33/1000 | Loss: 0.00063838
Iteration 34/1000 | Loss: 0.00098078
Iteration 35/1000 | Loss: 0.00050556
Iteration 36/1000 | Loss: 0.00082070
Iteration 37/1000 | Loss: 0.00040712
Iteration 38/1000 | Loss: 0.00039715
Iteration 39/1000 | Loss: 0.00072273
Iteration 40/1000 | Loss: 0.00024405
Iteration 41/1000 | Loss: 0.00128351
Iteration 42/1000 | Loss: 0.00281022
Iteration 43/1000 | Loss: 0.00060324
Iteration 44/1000 | Loss: 0.00140165
Iteration 45/1000 | Loss: 0.00230360
Iteration 46/1000 | Loss: 0.00383068
Iteration 47/1000 | Loss: 0.00100785
Iteration 48/1000 | Loss: 0.00068049
Iteration 49/1000 | Loss: 0.00066983
Iteration 50/1000 | Loss: 0.00206071
Iteration 51/1000 | Loss: 0.00193355
Iteration 52/1000 | Loss: 0.00103506
Iteration 53/1000 | Loss: 0.00247304
Iteration 54/1000 | Loss: 0.00097735
Iteration 55/1000 | Loss: 0.00030166
Iteration 56/1000 | Loss: 0.00092499
Iteration 57/1000 | Loss: 0.00023102
Iteration 58/1000 | Loss: 0.00141010
Iteration 59/1000 | Loss: 0.00142496
Iteration 60/1000 | Loss: 0.00025120
Iteration 61/1000 | Loss: 0.00020187
Iteration 62/1000 | Loss: 0.00018677
Iteration 63/1000 | Loss: 0.00048244
Iteration 64/1000 | Loss: 0.00049660
Iteration 65/1000 | Loss: 0.00016740
Iteration 66/1000 | Loss: 0.00200123
Iteration 67/1000 | Loss: 0.00021730
Iteration 68/1000 | Loss: 0.00042551
Iteration 69/1000 | Loss: 0.00075574
Iteration 70/1000 | Loss: 0.00042096
Iteration 71/1000 | Loss: 0.00058015
Iteration 72/1000 | Loss: 0.00015772
Iteration 73/1000 | Loss: 0.00153342
Iteration 74/1000 | Loss: 0.00062284
Iteration 75/1000 | Loss: 0.00188445
Iteration 76/1000 | Loss: 0.00100149
Iteration 77/1000 | Loss: 0.00104553
Iteration 78/1000 | Loss: 0.00119210
Iteration 79/1000 | Loss: 0.00072339
Iteration 80/1000 | Loss: 0.00047348
Iteration 81/1000 | Loss: 0.00015278
Iteration 82/1000 | Loss: 0.00063422
Iteration 83/1000 | Loss: 0.00013631
Iteration 84/1000 | Loss: 0.00013094
Iteration 85/1000 | Loss: 0.00012514
Iteration 86/1000 | Loss: 0.00012003
Iteration 87/1000 | Loss: 0.00088037
Iteration 88/1000 | Loss: 0.00067062
Iteration 89/1000 | Loss: 0.00085468
Iteration 90/1000 | Loss: 0.00036162
Iteration 91/1000 | Loss: 0.00016819
Iteration 92/1000 | Loss: 0.00011855
Iteration 93/1000 | Loss: 0.00077524
Iteration 94/1000 | Loss: 0.00011246
Iteration 95/1000 | Loss: 0.00010514
Iteration 96/1000 | Loss: 0.00010057
Iteration 97/1000 | Loss: 0.00023017
Iteration 98/1000 | Loss: 0.00009935
Iteration 99/1000 | Loss: 0.00009472
Iteration 100/1000 | Loss: 0.00181939
Iteration 101/1000 | Loss: 0.00232880
Iteration 102/1000 | Loss: 0.00019329
Iteration 103/1000 | Loss: 0.00009555
Iteration 104/1000 | Loss: 0.00177289
Iteration 105/1000 | Loss: 0.00009426
Iteration 106/1000 | Loss: 0.00009030
Iteration 107/1000 | Loss: 0.00048659
Iteration 108/1000 | Loss: 0.00121447
Iteration 109/1000 | Loss: 0.00125018
Iteration 110/1000 | Loss: 0.00017441
Iteration 111/1000 | Loss: 0.00009404
Iteration 112/1000 | Loss: 0.00008900
Iteration 113/1000 | Loss: 0.00008669
Iteration 114/1000 | Loss: 0.00116124
Iteration 115/1000 | Loss: 0.00150510
Iteration 116/1000 | Loss: 0.00011572
Iteration 117/1000 | Loss: 0.00008634
Iteration 118/1000 | Loss: 0.00008451
Iteration 119/1000 | Loss: 0.00020743
Iteration 120/1000 | Loss: 0.00022766
Iteration 121/1000 | Loss: 0.00008402
Iteration 122/1000 | Loss: 0.00008022
Iteration 123/1000 | Loss: 0.00046396
Iteration 124/1000 | Loss: 0.00007958
Iteration 125/1000 | Loss: 0.00007700
Iteration 126/1000 | Loss: 0.00119532
Iteration 127/1000 | Loss: 0.00197547
Iteration 128/1000 | Loss: 0.00050941
Iteration 129/1000 | Loss: 0.00008382
Iteration 130/1000 | Loss: 0.00063494
Iteration 131/1000 | Loss: 0.00018109
Iteration 132/1000 | Loss: 0.00046047
Iteration 133/1000 | Loss: 0.00059161
Iteration 134/1000 | Loss: 0.00026688
Iteration 135/1000 | Loss: 0.00009787
Iteration 136/1000 | Loss: 0.00008512
Iteration 137/1000 | Loss: 0.00030966
Iteration 138/1000 | Loss: 0.00007505
Iteration 139/1000 | Loss: 0.00070033
Iteration 140/1000 | Loss: 0.00058788
Iteration 141/1000 | Loss: 0.00017032
Iteration 142/1000 | Loss: 0.00010498
Iteration 143/1000 | Loss: 0.00006923
Iteration 144/1000 | Loss: 0.00047870
Iteration 145/1000 | Loss: 0.00019781
Iteration 146/1000 | Loss: 0.00060238
Iteration 147/1000 | Loss: 0.00015738
Iteration 148/1000 | Loss: 0.00006662
Iteration 149/1000 | Loss: 0.00023631
Iteration 150/1000 | Loss: 0.00039373
Iteration 151/1000 | Loss: 0.00049987
Iteration 152/1000 | Loss: 0.00039814
Iteration 153/1000 | Loss: 0.00076386
Iteration 154/1000 | Loss: 0.00050162
Iteration 155/1000 | Loss: 0.00041496
Iteration 156/1000 | Loss: 0.00046082
Iteration 157/1000 | Loss: 0.00024203
Iteration 158/1000 | Loss: 0.00012091
Iteration 159/1000 | Loss: 0.00012818
Iteration 160/1000 | Loss: 0.00011037
Iteration 161/1000 | Loss: 0.00014200
Iteration 162/1000 | Loss: 0.00036362
Iteration 163/1000 | Loss: 0.00030215
Iteration 164/1000 | Loss: 0.00040941
Iteration 165/1000 | Loss: 0.00025422
Iteration 166/1000 | Loss: 0.00017869
Iteration 167/1000 | Loss: 0.00011876
Iteration 168/1000 | Loss: 0.00012127
Iteration 169/1000 | Loss: 0.00019854
Iteration 170/1000 | Loss: 0.00012127
Iteration 171/1000 | Loss: 0.00020106
Iteration 172/1000 | Loss: 0.00053017
Iteration 173/1000 | Loss: 0.00020547
Iteration 174/1000 | Loss: 0.00007683
Iteration 175/1000 | Loss: 0.00019364
Iteration 176/1000 | Loss: 0.00006956
Iteration 177/1000 | Loss: 0.00006643
Iteration 178/1000 | Loss: 0.00006450
Iteration 179/1000 | Loss: 0.00006338
Iteration 180/1000 | Loss: 0.00075028
Iteration 181/1000 | Loss: 0.00085502
Iteration 182/1000 | Loss: 0.00009064
Iteration 183/1000 | Loss: 0.00006497
Iteration 184/1000 | Loss: 0.00006275
Iteration 185/1000 | Loss: 0.00006163
Iteration 186/1000 | Loss: 0.00134770
Iteration 187/1000 | Loss: 0.00068991
Iteration 188/1000 | Loss: 0.00021783
Iteration 189/1000 | Loss: 0.00010004
Iteration 190/1000 | Loss: 0.00008915
Iteration 191/1000 | Loss: 0.00006764
Iteration 192/1000 | Loss: 0.00006888
Iteration 193/1000 | Loss: 0.00152557
Iteration 194/1000 | Loss: 0.00039614
Iteration 195/1000 | Loss: 0.00008625
Iteration 196/1000 | Loss: 0.00052105
Iteration 197/1000 | Loss: 0.00024741
Iteration 198/1000 | Loss: 0.00134935
Iteration 199/1000 | Loss: 0.00035639
Iteration 200/1000 | Loss: 0.00086830
Iteration 201/1000 | Loss: 0.00034706
Iteration 202/1000 | Loss: 0.00032473
Iteration 203/1000 | Loss: 0.00075725
Iteration 204/1000 | Loss: 0.00048721
Iteration 205/1000 | Loss: 0.00016133
Iteration 206/1000 | Loss: 0.00024413
Iteration 207/1000 | Loss: 0.00013838
Iteration 208/1000 | Loss: 0.00023285
Iteration 209/1000 | Loss: 0.00012289
Iteration 210/1000 | Loss: 0.00014894
Iteration 211/1000 | Loss: 0.00010839
Iteration 212/1000 | Loss: 0.00043133
Iteration 213/1000 | Loss: 0.00072365
Iteration 214/1000 | Loss: 0.00068557
Iteration 215/1000 | Loss: 0.00095598
Iteration 216/1000 | Loss: 0.00035045
Iteration 217/1000 | Loss: 0.00021566
Iteration 218/1000 | Loss: 0.00030156
Iteration 219/1000 | Loss: 0.00057081
Iteration 220/1000 | Loss: 0.00041359
Iteration 221/1000 | Loss: 0.00037020
Iteration 222/1000 | Loss: 0.00032917
Iteration 223/1000 | Loss: 0.00016771
Iteration 224/1000 | Loss: 0.00020882
Iteration 225/1000 | Loss: 0.00023435
Iteration 226/1000 | Loss: 0.00132485
Iteration 227/1000 | Loss: 0.00021832
Iteration 228/1000 | Loss: 0.00030192
Iteration 229/1000 | Loss: 0.00077466
Iteration 230/1000 | Loss: 0.00012774
Iteration 231/1000 | Loss: 0.00076101
Iteration 232/1000 | Loss: 0.00017948
Iteration 233/1000 | Loss: 0.00033045
Iteration 234/1000 | Loss: 0.00007411
Iteration 235/1000 | Loss: 0.00046288
Iteration 236/1000 | Loss: 0.00048043
Iteration 237/1000 | Loss: 0.00123565
Iteration 238/1000 | Loss: 0.00012308
Iteration 239/1000 | Loss: 0.00007515
Iteration 240/1000 | Loss: 0.00006771
Iteration 241/1000 | Loss: 0.00006469
Iteration 242/1000 | Loss: 0.00012946
Iteration 243/1000 | Loss: 0.00006159
Iteration 244/1000 | Loss: 0.00006093
Iteration 245/1000 | Loss: 0.00006058
Iteration 246/1000 | Loss: 0.00006025
Iteration 247/1000 | Loss: 0.00005996
Iteration 248/1000 | Loss: 0.00118999
Iteration 249/1000 | Loss: 0.00009404
Iteration 250/1000 | Loss: 0.00006230
Iteration 251/1000 | Loss: 0.00006002
Iteration 252/1000 | Loss: 0.00119346
Iteration 253/1000 | Loss: 0.00244223
Iteration 254/1000 | Loss: 0.00031274
Iteration 255/1000 | Loss: 0.00054633
Iteration 256/1000 | Loss: 0.00117433
Iteration 257/1000 | Loss: 0.00060564
Iteration 258/1000 | Loss: 0.00058152
Iteration 259/1000 | Loss: 0.00006351
Iteration 260/1000 | Loss: 0.00005852
Iteration 261/1000 | Loss: 0.00005583
Iteration 262/1000 | Loss: 0.00065631
Iteration 263/1000 | Loss: 0.00013655
Iteration 264/1000 | Loss: 0.00024962
Iteration 265/1000 | Loss: 0.00005490
Iteration 266/1000 | Loss: 0.00005169
Iteration 267/1000 | Loss: 0.00005031
Iteration 268/1000 | Loss: 0.00004969
Iteration 269/1000 | Loss: 0.00004918
Iteration 270/1000 | Loss: 0.00004882
Iteration 271/1000 | Loss: 0.00004863
Iteration 272/1000 | Loss: 0.00004840
Iteration 273/1000 | Loss: 0.00004835
Iteration 274/1000 | Loss: 0.00004826
Iteration 275/1000 | Loss: 0.00027765
Iteration 276/1000 | Loss: 0.00004732
Iteration 277/1000 | Loss: 0.00004678
Iteration 278/1000 | Loss: 0.00004641
Iteration 279/1000 | Loss: 0.00004607
Iteration 280/1000 | Loss: 0.00004588
Iteration 281/1000 | Loss: 0.00004584
Iteration 282/1000 | Loss: 0.00004584
Iteration 283/1000 | Loss: 0.00004584
Iteration 284/1000 | Loss: 0.00004584
Iteration 285/1000 | Loss: 0.00004584
Iteration 286/1000 | Loss: 0.00004584
Iteration 287/1000 | Loss: 0.00004580
Iteration 288/1000 | Loss: 0.00004580
Iteration 289/1000 | Loss: 0.00004579
Iteration 290/1000 | Loss: 0.00004579
Iteration 291/1000 | Loss: 0.00004577
Iteration 292/1000 | Loss: 0.00004577
Iteration 293/1000 | Loss: 0.00004576
Iteration 294/1000 | Loss: 0.00004576
Iteration 295/1000 | Loss: 0.00004576
Iteration 296/1000 | Loss: 0.00004574
Iteration 297/1000 | Loss: 0.00004573
Iteration 298/1000 | Loss: 0.00004572
Iteration 299/1000 | Loss: 0.00004572
Iteration 300/1000 | Loss: 0.00004572
Iteration 301/1000 | Loss: 0.00004571
Iteration 302/1000 | Loss: 0.00004571
Iteration 303/1000 | Loss: 0.00004571
Iteration 304/1000 | Loss: 0.00004570
Iteration 305/1000 | Loss: 0.00004569
Iteration 306/1000 | Loss: 0.00004568
Iteration 307/1000 | Loss: 0.00004567
Iteration 308/1000 | Loss: 0.00004566
Iteration 309/1000 | Loss: 0.00004566
Iteration 310/1000 | Loss: 0.00004566
Iteration 311/1000 | Loss: 0.00004566
Iteration 312/1000 | Loss: 0.00004565
Iteration 313/1000 | Loss: 0.00004565
Iteration 314/1000 | Loss: 0.00004565
Iteration 315/1000 | Loss: 0.00004565
Iteration 316/1000 | Loss: 0.00004565
Iteration 317/1000 | Loss: 0.00004565
Iteration 318/1000 | Loss: 0.00004564
Iteration 319/1000 | Loss: 0.00004564
Iteration 320/1000 | Loss: 0.00004564
Iteration 321/1000 | Loss: 0.00004564
Iteration 322/1000 | Loss: 0.00004563
Iteration 323/1000 | Loss: 0.00004562
Iteration 324/1000 | Loss: 0.00004562
Iteration 325/1000 | Loss: 0.00004562
Iteration 326/1000 | Loss: 0.00004561
Iteration 327/1000 | Loss: 0.00004561
Iteration 328/1000 | Loss: 0.00004561
Iteration 329/1000 | Loss: 0.00004560
Iteration 330/1000 | Loss: 0.00004560
Iteration 331/1000 | Loss: 0.00004560
Iteration 332/1000 | Loss: 0.00004560
Iteration 333/1000 | Loss: 0.00004559
Iteration 334/1000 | Loss: 0.00004559
Iteration 335/1000 | Loss: 0.00004559
Iteration 336/1000 | Loss: 0.00004559
Iteration 337/1000 | Loss: 0.00004558
Iteration 338/1000 | Loss: 0.00004558
Iteration 339/1000 | Loss: 0.00004558
Iteration 340/1000 | Loss: 0.00004558
Iteration 341/1000 | Loss: 0.00004558
Iteration 342/1000 | Loss: 0.00004558
Iteration 343/1000 | Loss: 0.00004558
Iteration 344/1000 | Loss: 0.00004558
Iteration 345/1000 | Loss: 0.00004558
Iteration 346/1000 | Loss: 0.00004557
Iteration 347/1000 | Loss: 0.00004557
Iteration 348/1000 | Loss: 0.00004557
Iteration 349/1000 | Loss: 0.00004557
Iteration 350/1000 | Loss: 0.00004557
Iteration 351/1000 | Loss: 0.00004557
Iteration 352/1000 | Loss: 0.00004557
Iteration 353/1000 | Loss: 0.00004557
Iteration 354/1000 | Loss: 0.00004557
Iteration 355/1000 | Loss: 0.00004556
Iteration 356/1000 | Loss: 0.00004556
Iteration 357/1000 | Loss: 0.00004556
Iteration 358/1000 | Loss: 0.00004556
Iteration 359/1000 | Loss: 0.00004556
Iteration 360/1000 | Loss: 0.00004555
Iteration 361/1000 | Loss: 0.00004555
Iteration 362/1000 | Loss: 0.00004555
Iteration 363/1000 | Loss: 0.00004555
Iteration 364/1000 | Loss: 0.00004554
Iteration 365/1000 | Loss: 0.00004554
Iteration 366/1000 | Loss: 0.00004554
Iteration 367/1000 | Loss: 0.00004554
Iteration 368/1000 | Loss: 0.00004554
Iteration 369/1000 | Loss: 0.00004554
Iteration 370/1000 | Loss: 0.00004554
Iteration 371/1000 | Loss: 0.00004554
Iteration 372/1000 | Loss: 0.00004554
Iteration 373/1000 | Loss: 0.00004554
Iteration 374/1000 | Loss: 0.00004554
Iteration 375/1000 | Loss: 0.00004554
Iteration 376/1000 | Loss: 0.00004554
Iteration 377/1000 | Loss: 0.00004554
Iteration 378/1000 | Loss: 0.00004554
Iteration 379/1000 | Loss: 0.00004553
Iteration 380/1000 | Loss: 0.00004553
Iteration 381/1000 | Loss: 0.00004553
Iteration 382/1000 | Loss: 0.00004553
Iteration 383/1000 | Loss: 0.00004553
Iteration 384/1000 | Loss: 0.00004553
Iteration 385/1000 | Loss: 0.00004553
Iteration 386/1000 | Loss: 0.00004553
Iteration 387/1000 | Loss: 0.00004553
Iteration 388/1000 | Loss: 0.00004553
Iteration 389/1000 | Loss: 0.00004553
Iteration 390/1000 | Loss: 0.00004553
Iteration 391/1000 | Loss: 0.00004553
Iteration 392/1000 | Loss: 0.00004553
Iteration 393/1000 | Loss: 0.00004552
Iteration 394/1000 | Loss: 0.00004552
Iteration 395/1000 | Loss: 0.00004552
Iteration 396/1000 | Loss: 0.00004552
Iteration 397/1000 | Loss: 0.00004552
Iteration 398/1000 | Loss: 0.00004552
Iteration 399/1000 | Loss: 0.00004552
Iteration 400/1000 | Loss: 0.00004552
Iteration 401/1000 | Loss: 0.00004552
Iteration 402/1000 | Loss: 0.00004552
Iteration 403/1000 | Loss: 0.00004552
Iteration 404/1000 | Loss: 0.00004552
Iteration 405/1000 | Loss: 0.00004552
Iteration 406/1000 | Loss: 0.00004552
Iteration 407/1000 | Loss: 0.00004552
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 407. Stopping optimization.
Last 5 losses: [4.551986057776958e-05, 4.551986057776958e-05, 4.551986057776958e-05, 4.551986057776958e-05, 4.551986057776958e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.551986057776958e-05

Optimization complete. Final v2v error: 4.601801872253418 mm

Highest mean error: 12.415019989013672 mm for frame 3

Lowest mean error: 3.3194708824157715 mm for frame 188

Saving results

Total time: 515.0981187820435
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00982914
Iteration 2/25 | Loss: 0.00262170
Iteration 3/25 | Loss: 0.00193570
Iteration 4/25 | Loss: 0.00172502
Iteration 5/25 | Loss: 0.00170781
Iteration 6/25 | Loss: 0.00152924
Iteration 7/25 | Loss: 0.00131958
Iteration 8/25 | Loss: 0.00116409
Iteration 9/25 | Loss: 0.00109553
Iteration 10/25 | Loss: 0.00107370
Iteration 11/25 | Loss: 0.00106360
Iteration 12/25 | Loss: 0.00105366
Iteration 13/25 | Loss: 0.00105369
Iteration 14/25 | Loss: 0.00104833
Iteration 15/25 | Loss: 0.00104440
Iteration 16/25 | Loss: 0.00103652
Iteration 17/25 | Loss: 0.00103933
Iteration 18/25 | Loss: 0.00103251
Iteration 19/25 | Loss: 0.00103007
Iteration 20/25 | Loss: 0.00102907
Iteration 21/25 | Loss: 0.00102807
Iteration 22/25 | Loss: 0.00103271
Iteration 23/25 | Loss: 0.00102657
Iteration 24/25 | Loss: 0.00102516
Iteration 25/25 | Loss: 0.00102497

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44827056
Iteration 2/25 | Loss: 0.00336639
Iteration 3/25 | Loss: 0.00336639
Iteration 4/25 | Loss: 0.00336639
Iteration 5/25 | Loss: 0.00336638
Iteration 6/25 | Loss: 0.00336638
Iteration 7/25 | Loss: 0.00336638
Iteration 8/25 | Loss: 0.00336638
Iteration 9/25 | Loss: 0.00336638
Iteration 10/25 | Loss: 0.00336638
Iteration 11/25 | Loss: 0.00336638
Iteration 12/25 | Loss: 0.00336638
Iteration 13/25 | Loss: 0.00336638
Iteration 14/25 | Loss: 0.00336638
Iteration 15/25 | Loss: 0.00336638
Iteration 16/25 | Loss: 0.00336638
Iteration 17/25 | Loss: 0.00336638
Iteration 18/25 | Loss: 0.00336638
Iteration 19/25 | Loss: 0.00336638
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.003366383258253336, 0.003366383258253336, 0.003366383258253336, 0.003366383258253336, 0.003366383258253336]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003366383258253336

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00336638
Iteration 2/1000 | Loss: 0.00033325
Iteration 3/1000 | Loss: 0.00027201
Iteration 4/1000 | Loss: 0.00020638
Iteration 5/1000 | Loss: 0.00021650
Iteration 6/1000 | Loss: 0.00016341
Iteration 7/1000 | Loss: 0.00015302
Iteration 8/1000 | Loss: 0.00014307
Iteration 9/1000 | Loss: 0.00013774
Iteration 10/1000 | Loss: 0.00013289
Iteration 11/1000 | Loss: 0.00012903
Iteration 12/1000 | Loss: 0.00012608
Iteration 13/1000 | Loss: 0.00012423
Iteration 14/1000 | Loss: 0.00012234
Iteration 15/1000 | Loss: 0.00012012
Iteration 16/1000 | Loss: 0.00011869
Iteration 17/1000 | Loss: 0.00011738
Iteration 18/1000 | Loss: 0.00011653
Iteration 19/1000 | Loss: 0.00011590
Iteration 20/1000 | Loss: 0.00011530
Iteration 21/1000 | Loss: 0.00011496
Iteration 22/1000 | Loss: 0.00011471
Iteration 23/1000 | Loss: 0.00011458
Iteration 24/1000 | Loss: 0.00011449
Iteration 25/1000 | Loss: 0.00011444
Iteration 26/1000 | Loss: 0.00011443
Iteration 27/1000 | Loss: 0.00011443
Iteration 28/1000 | Loss: 0.00011443
Iteration 29/1000 | Loss: 0.00011443
Iteration 30/1000 | Loss: 0.00011439
Iteration 31/1000 | Loss: 0.00011434
Iteration 32/1000 | Loss: 0.00011434
Iteration 33/1000 | Loss: 0.00011434
Iteration 34/1000 | Loss: 0.00011434
Iteration 35/1000 | Loss: 0.00011433
Iteration 36/1000 | Loss: 0.00011433
Iteration 37/1000 | Loss: 0.00011432
Iteration 38/1000 | Loss: 0.00011432
Iteration 39/1000 | Loss: 0.00011431
Iteration 40/1000 | Loss: 0.00011431
Iteration 41/1000 | Loss: 0.00011431
Iteration 42/1000 | Loss: 0.00011431
Iteration 43/1000 | Loss: 0.00011430
Iteration 44/1000 | Loss: 0.00011430
Iteration 45/1000 | Loss: 0.00011430
Iteration 46/1000 | Loss: 0.00011430
Iteration 47/1000 | Loss: 0.00011430
Iteration 48/1000 | Loss: 0.00011429
Iteration 49/1000 | Loss: 0.00011429
Iteration 50/1000 | Loss: 0.00011429
Iteration 51/1000 | Loss: 0.00011429
Iteration 52/1000 | Loss: 0.00011429
Iteration 53/1000 | Loss: 0.00011429
Iteration 54/1000 | Loss: 0.00011429
Iteration 55/1000 | Loss: 0.00011428
Iteration 56/1000 | Loss: 0.00011428
Iteration 57/1000 | Loss: 0.00011427
Iteration 58/1000 | Loss: 0.00011427
Iteration 59/1000 | Loss: 0.00011427
Iteration 60/1000 | Loss: 0.00011427
Iteration 61/1000 | Loss: 0.00011427
Iteration 62/1000 | Loss: 0.00011427
Iteration 63/1000 | Loss: 0.00011426
Iteration 64/1000 | Loss: 0.00011426
Iteration 65/1000 | Loss: 0.00011426
Iteration 66/1000 | Loss: 0.00011425
Iteration 67/1000 | Loss: 0.00011425
Iteration 68/1000 | Loss: 0.00011425
Iteration 69/1000 | Loss: 0.00011425
Iteration 70/1000 | Loss: 0.00011425
Iteration 71/1000 | Loss: 0.00011425
Iteration 72/1000 | Loss: 0.00011425
Iteration 73/1000 | Loss: 0.00011425
Iteration 74/1000 | Loss: 0.00011424
Iteration 75/1000 | Loss: 0.00011424
Iteration 76/1000 | Loss: 0.00011424
Iteration 77/1000 | Loss: 0.00011424
Iteration 78/1000 | Loss: 0.00011423
Iteration 79/1000 | Loss: 0.00011423
Iteration 80/1000 | Loss: 0.00011423
Iteration 81/1000 | Loss: 0.00011423
Iteration 82/1000 | Loss: 0.00011423
Iteration 83/1000 | Loss: 0.00011423
Iteration 84/1000 | Loss: 0.00011423
Iteration 85/1000 | Loss: 0.00011422
Iteration 86/1000 | Loss: 0.00011422
Iteration 87/1000 | Loss: 0.00011422
Iteration 88/1000 | Loss: 0.00011422
Iteration 89/1000 | Loss: 0.00011422
Iteration 90/1000 | Loss: 0.00011421
Iteration 91/1000 | Loss: 0.00011421
Iteration 92/1000 | Loss: 0.00011421
Iteration 93/1000 | Loss: 0.00011421
Iteration 94/1000 | Loss: 0.00011421
Iteration 95/1000 | Loss: 0.00011421
Iteration 96/1000 | Loss: 0.00011421
Iteration 97/1000 | Loss: 0.00011421
Iteration 98/1000 | Loss: 0.00011421
Iteration 99/1000 | Loss: 0.00011421
Iteration 100/1000 | Loss: 0.00011421
Iteration 101/1000 | Loss: 0.00011420
Iteration 102/1000 | Loss: 0.00011420
Iteration 103/1000 | Loss: 0.00011420
Iteration 104/1000 | Loss: 0.00011420
Iteration 105/1000 | Loss: 0.00011420
Iteration 106/1000 | Loss: 0.00011420
Iteration 107/1000 | Loss: 0.00011420
Iteration 108/1000 | Loss: 0.00011420
Iteration 109/1000 | Loss: 0.00011420
Iteration 110/1000 | Loss: 0.00011420
Iteration 111/1000 | Loss: 0.00011420
Iteration 112/1000 | Loss: 0.00011419
Iteration 113/1000 | Loss: 0.00011419
Iteration 114/1000 | Loss: 0.00011419
Iteration 115/1000 | Loss: 0.00011419
Iteration 116/1000 | Loss: 0.00011419
Iteration 117/1000 | Loss: 0.00011418
Iteration 118/1000 | Loss: 0.00011418
Iteration 119/1000 | Loss: 0.00011418
Iteration 120/1000 | Loss: 0.00011418
Iteration 121/1000 | Loss: 0.00011418
Iteration 122/1000 | Loss: 0.00011418
Iteration 123/1000 | Loss: 0.00011418
Iteration 124/1000 | Loss: 0.00011418
Iteration 125/1000 | Loss: 0.00011418
Iteration 126/1000 | Loss: 0.00011417
Iteration 127/1000 | Loss: 0.00011417
Iteration 128/1000 | Loss: 0.00011417
Iteration 129/1000 | Loss: 0.00011417
Iteration 130/1000 | Loss: 0.00011417
Iteration 131/1000 | Loss: 0.00011417
Iteration 132/1000 | Loss: 0.00011417
Iteration 133/1000 | Loss: 0.00011417
Iteration 134/1000 | Loss: 0.00011417
Iteration 135/1000 | Loss: 0.00011417
Iteration 136/1000 | Loss: 0.00011417
Iteration 137/1000 | Loss: 0.00011417
Iteration 138/1000 | Loss: 0.00011417
Iteration 139/1000 | Loss: 0.00011417
Iteration 140/1000 | Loss: 0.00011417
Iteration 141/1000 | Loss: 0.00011417
Iteration 142/1000 | Loss: 0.00011417
Iteration 143/1000 | Loss: 0.00011417
Iteration 144/1000 | Loss: 0.00011416
Iteration 145/1000 | Loss: 0.00011416
Iteration 146/1000 | Loss: 0.00011416
Iteration 147/1000 | Loss: 0.00011416
Iteration 148/1000 | Loss: 0.00011416
Iteration 149/1000 | Loss: 0.00011416
Iteration 150/1000 | Loss: 0.00011416
Iteration 151/1000 | Loss: 0.00011416
Iteration 152/1000 | Loss: 0.00011416
Iteration 153/1000 | Loss: 0.00011416
Iteration 154/1000 | Loss: 0.00011416
Iteration 155/1000 | Loss: 0.00011416
Iteration 156/1000 | Loss: 0.00011416
Iteration 157/1000 | Loss: 0.00011416
Iteration 158/1000 | Loss: 0.00011416
Iteration 159/1000 | Loss: 0.00011416
Iteration 160/1000 | Loss: 0.00011416
Iteration 161/1000 | Loss: 0.00011416
Iteration 162/1000 | Loss: 0.00011416
Iteration 163/1000 | Loss: 0.00011416
Iteration 164/1000 | Loss: 0.00011416
Iteration 165/1000 | Loss: 0.00011416
Iteration 166/1000 | Loss: 0.00011416
Iteration 167/1000 | Loss: 0.00011416
Iteration 168/1000 | Loss: 0.00011416
Iteration 169/1000 | Loss: 0.00011416
Iteration 170/1000 | Loss: 0.00011416
Iteration 171/1000 | Loss: 0.00011416
Iteration 172/1000 | Loss: 0.00011416
Iteration 173/1000 | Loss: 0.00011416
Iteration 174/1000 | Loss: 0.00011416
Iteration 175/1000 | Loss: 0.00011416
Iteration 176/1000 | Loss: 0.00011416
Iteration 177/1000 | Loss: 0.00011416
Iteration 178/1000 | Loss: 0.00011416
Iteration 179/1000 | Loss: 0.00011416
Iteration 180/1000 | Loss: 0.00011416
Iteration 181/1000 | Loss: 0.00011416
Iteration 182/1000 | Loss: 0.00011416
Iteration 183/1000 | Loss: 0.00011416
Iteration 184/1000 | Loss: 0.00011416
Iteration 185/1000 | Loss: 0.00011416
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [0.00011416451161494479, 0.00011416451161494479, 0.00011416451161494479, 0.00011416451161494479, 0.00011416451161494479]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00011416451161494479

Optimization complete. Final v2v error: 5.599161624908447 mm

Highest mean error: 11.377689361572266 mm for frame 164

Lowest mean error: 3.5814645290374756 mm for frame 114

Saving results

Total time: 93.37574052810669
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00440071
Iteration 2/25 | Loss: 0.00099970
Iteration 3/25 | Loss: 0.00084366
Iteration 4/25 | Loss: 0.00080874
Iteration 5/25 | Loss: 0.00079771
Iteration 6/25 | Loss: 0.00079566
Iteration 7/25 | Loss: 0.00079543
Iteration 8/25 | Loss: 0.00079543
Iteration 9/25 | Loss: 0.00079543
Iteration 10/25 | Loss: 0.00079543
Iteration 11/25 | Loss: 0.00079543
Iteration 12/25 | Loss: 0.00079543
Iteration 13/25 | Loss: 0.00079543
Iteration 14/25 | Loss: 0.00079543
Iteration 15/25 | Loss: 0.00079543
Iteration 16/25 | Loss: 0.00079543
Iteration 17/25 | Loss: 0.00079543
Iteration 18/25 | Loss: 0.00079543
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007954305619932711, 0.0007954305619932711, 0.0007954305619932711, 0.0007954305619932711, 0.0007954305619932711]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007954305619932711

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45717788
Iteration 2/25 | Loss: 0.00046425
Iteration 3/25 | Loss: 0.00046424
Iteration 4/25 | Loss: 0.00046424
Iteration 5/25 | Loss: 0.00046424
Iteration 6/25 | Loss: 0.00046424
Iteration 7/25 | Loss: 0.00046424
Iteration 8/25 | Loss: 0.00046424
Iteration 9/25 | Loss: 0.00046424
Iteration 10/25 | Loss: 0.00046424
Iteration 11/25 | Loss: 0.00046424
Iteration 12/25 | Loss: 0.00046424
Iteration 13/25 | Loss: 0.00046424
Iteration 14/25 | Loss: 0.00046424
Iteration 15/25 | Loss: 0.00046424
Iteration 16/25 | Loss: 0.00046424
Iteration 17/25 | Loss: 0.00046424
Iteration 18/25 | Loss: 0.00046424
Iteration 19/25 | Loss: 0.00046424
Iteration 20/25 | Loss: 0.00046424
Iteration 21/25 | Loss: 0.00046424
Iteration 22/25 | Loss: 0.00046424
Iteration 23/25 | Loss: 0.00046424
Iteration 24/25 | Loss: 0.00046424
Iteration 25/25 | Loss: 0.00046424

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046424
Iteration 2/1000 | Loss: 0.00003796
Iteration 3/1000 | Loss: 0.00002955
Iteration 4/1000 | Loss: 0.00002781
Iteration 5/1000 | Loss: 0.00002659
Iteration 6/1000 | Loss: 0.00002567
Iteration 7/1000 | Loss: 0.00002500
Iteration 8/1000 | Loss: 0.00002445
Iteration 9/1000 | Loss: 0.00002409
Iteration 10/1000 | Loss: 0.00002385
Iteration 11/1000 | Loss: 0.00002375
Iteration 12/1000 | Loss: 0.00002367
Iteration 13/1000 | Loss: 0.00002367
Iteration 14/1000 | Loss: 0.00002366
Iteration 15/1000 | Loss: 0.00002366
Iteration 16/1000 | Loss: 0.00002361
Iteration 17/1000 | Loss: 0.00002357
Iteration 18/1000 | Loss: 0.00002356
Iteration 19/1000 | Loss: 0.00002356
Iteration 20/1000 | Loss: 0.00002353
Iteration 21/1000 | Loss: 0.00002352
Iteration 22/1000 | Loss: 0.00002352
Iteration 23/1000 | Loss: 0.00002345
Iteration 24/1000 | Loss: 0.00002345
Iteration 25/1000 | Loss: 0.00002345
Iteration 26/1000 | Loss: 0.00002344
Iteration 27/1000 | Loss: 0.00002343
Iteration 28/1000 | Loss: 0.00002343
Iteration 29/1000 | Loss: 0.00002342
Iteration 30/1000 | Loss: 0.00002342
Iteration 31/1000 | Loss: 0.00002341
Iteration 32/1000 | Loss: 0.00002341
Iteration 33/1000 | Loss: 0.00002341
Iteration 34/1000 | Loss: 0.00002340
Iteration 35/1000 | Loss: 0.00002340
Iteration 36/1000 | Loss: 0.00002340
Iteration 37/1000 | Loss: 0.00002340
Iteration 38/1000 | Loss: 0.00002340
Iteration 39/1000 | Loss: 0.00002339
Iteration 40/1000 | Loss: 0.00002339
Iteration 41/1000 | Loss: 0.00002339
Iteration 42/1000 | Loss: 0.00002339
Iteration 43/1000 | Loss: 0.00002339
Iteration 44/1000 | Loss: 0.00002339
Iteration 45/1000 | Loss: 0.00002339
Iteration 46/1000 | Loss: 0.00002339
Iteration 47/1000 | Loss: 0.00002338
Iteration 48/1000 | Loss: 0.00002338
Iteration 49/1000 | Loss: 0.00002338
Iteration 50/1000 | Loss: 0.00002338
Iteration 51/1000 | Loss: 0.00002338
Iteration 52/1000 | Loss: 0.00002338
Iteration 53/1000 | Loss: 0.00002338
Iteration 54/1000 | Loss: 0.00002337
Iteration 55/1000 | Loss: 0.00002337
Iteration 56/1000 | Loss: 0.00002337
Iteration 57/1000 | Loss: 0.00002337
Iteration 58/1000 | Loss: 0.00002337
Iteration 59/1000 | Loss: 0.00002337
Iteration 60/1000 | Loss: 0.00002337
Iteration 61/1000 | Loss: 0.00002337
Iteration 62/1000 | Loss: 0.00002336
Iteration 63/1000 | Loss: 0.00002336
Iteration 64/1000 | Loss: 0.00002336
Iteration 65/1000 | Loss: 0.00002336
Iteration 66/1000 | Loss: 0.00002336
Iteration 67/1000 | Loss: 0.00002336
Iteration 68/1000 | Loss: 0.00002336
Iteration 69/1000 | Loss: 0.00002336
Iteration 70/1000 | Loss: 0.00002336
Iteration 71/1000 | Loss: 0.00002335
Iteration 72/1000 | Loss: 0.00002335
Iteration 73/1000 | Loss: 0.00002335
Iteration 74/1000 | Loss: 0.00002335
Iteration 75/1000 | Loss: 0.00002335
Iteration 76/1000 | Loss: 0.00002335
Iteration 77/1000 | Loss: 0.00002335
Iteration 78/1000 | Loss: 0.00002335
Iteration 79/1000 | Loss: 0.00002335
Iteration 80/1000 | Loss: 0.00002334
Iteration 81/1000 | Loss: 0.00002334
Iteration 82/1000 | Loss: 0.00002334
Iteration 83/1000 | Loss: 0.00002334
Iteration 84/1000 | Loss: 0.00002334
Iteration 85/1000 | Loss: 0.00002333
Iteration 86/1000 | Loss: 0.00002333
Iteration 87/1000 | Loss: 0.00002333
Iteration 88/1000 | Loss: 0.00002333
Iteration 89/1000 | Loss: 0.00002332
Iteration 90/1000 | Loss: 0.00002332
Iteration 91/1000 | Loss: 0.00002332
Iteration 92/1000 | Loss: 0.00002331
Iteration 93/1000 | Loss: 0.00002331
Iteration 94/1000 | Loss: 0.00002331
Iteration 95/1000 | Loss: 0.00002330
Iteration 96/1000 | Loss: 0.00002330
Iteration 97/1000 | Loss: 0.00002330
Iteration 98/1000 | Loss: 0.00002329
Iteration 99/1000 | Loss: 0.00002329
Iteration 100/1000 | Loss: 0.00002329
Iteration 101/1000 | Loss: 0.00002328
Iteration 102/1000 | Loss: 0.00002328
Iteration 103/1000 | Loss: 0.00002328
Iteration 104/1000 | Loss: 0.00002328
Iteration 105/1000 | Loss: 0.00002328
Iteration 106/1000 | Loss: 0.00002327
Iteration 107/1000 | Loss: 0.00002327
Iteration 108/1000 | Loss: 0.00002327
Iteration 109/1000 | Loss: 0.00002327
Iteration 110/1000 | Loss: 0.00002326
Iteration 111/1000 | Loss: 0.00002326
Iteration 112/1000 | Loss: 0.00002325
Iteration 113/1000 | Loss: 0.00002325
Iteration 114/1000 | Loss: 0.00002324
Iteration 115/1000 | Loss: 0.00002324
Iteration 116/1000 | Loss: 0.00002324
Iteration 117/1000 | Loss: 0.00002324
Iteration 118/1000 | Loss: 0.00002323
Iteration 119/1000 | Loss: 0.00002323
Iteration 120/1000 | Loss: 0.00002323
Iteration 121/1000 | Loss: 0.00002323
Iteration 122/1000 | Loss: 0.00002323
Iteration 123/1000 | Loss: 0.00002323
Iteration 124/1000 | Loss: 0.00002322
Iteration 125/1000 | Loss: 0.00002322
Iteration 126/1000 | Loss: 0.00002322
Iteration 127/1000 | Loss: 0.00002322
Iteration 128/1000 | Loss: 0.00002322
Iteration 129/1000 | Loss: 0.00002322
Iteration 130/1000 | Loss: 0.00002321
Iteration 131/1000 | Loss: 0.00002321
Iteration 132/1000 | Loss: 0.00002321
Iteration 133/1000 | Loss: 0.00002321
Iteration 134/1000 | Loss: 0.00002321
Iteration 135/1000 | Loss: 0.00002321
Iteration 136/1000 | Loss: 0.00002321
Iteration 137/1000 | Loss: 0.00002320
Iteration 138/1000 | Loss: 0.00002320
Iteration 139/1000 | Loss: 0.00002320
Iteration 140/1000 | Loss: 0.00002320
Iteration 141/1000 | Loss: 0.00002320
Iteration 142/1000 | Loss: 0.00002320
Iteration 143/1000 | Loss: 0.00002320
Iteration 144/1000 | Loss: 0.00002320
Iteration 145/1000 | Loss: 0.00002320
Iteration 146/1000 | Loss: 0.00002320
Iteration 147/1000 | Loss: 0.00002319
Iteration 148/1000 | Loss: 0.00002319
Iteration 149/1000 | Loss: 0.00002319
Iteration 150/1000 | Loss: 0.00002319
Iteration 151/1000 | Loss: 0.00002319
Iteration 152/1000 | Loss: 0.00002319
Iteration 153/1000 | Loss: 0.00002319
Iteration 154/1000 | Loss: 0.00002319
Iteration 155/1000 | Loss: 0.00002318
Iteration 156/1000 | Loss: 0.00002318
Iteration 157/1000 | Loss: 0.00002318
Iteration 158/1000 | Loss: 0.00002318
Iteration 159/1000 | Loss: 0.00002318
Iteration 160/1000 | Loss: 0.00002318
Iteration 161/1000 | Loss: 0.00002318
Iteration 162/1000 | Loss: 0.00002317
Iteration 163/1000 | Loss: 0.00002317
Iteration 164/1000 | Loss: 0.00002317
Iteration 165/1000 | Loss: 0.00002317
Iteration 166/1000 | Loss: 0.00002317
Iteration 167/1000 | Loss: 0.00002317
Iteration 168/1000 | Loss: 0.00002317
Iteration 169/1000 | Loss: 0.00002317
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [2.3173097360995598e-05, 2.3173097360995598e-05, 2.3173097360995598e-05, 2.3173097360995598e-05, 2.3173097360995598e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3173097360995598e-05

Optimization complete. Final v2v error: 4.021215915679932 mm

Highest mean error: 4.448019027709961 mm for frame 177

Lowest mean error: 3.605067491531372 mm for frame 213

Saving results

Total time: 44.57127094268799
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00537906
Iteration 2/25 | Loss: 0.00123053
Iteration 3/25 | Loss: 0.00081105
Iteration 4/25 | Loss: 0.00078084
Iteration 5/25 | Loss: 0.00077548
Iteration 6/25 | Loss: 0.00077303
Iteration 7/25 | Loss: 0.00077229
Iteration 8/25 | Loss: 0.00077225
Iteration 9/25 | Loss: 0.00077225
Iteration 10/25 | Loss: 0.00077225
Iteration 11/25 | Loss: 0.00077225
Iteration 12/25 | Loss: 0.00077225
Iteration 13/25 | Loss: 0.00077225
Iteration 14/25 | Loss: 0.00077225
Iteration 15/25 | Loss: 0.00077225
Iteration 16/25 | Loss: 0.00077225
Iteration 17/25 | Loss: 0.00077225
Iteration 18/25 | Loss: 0.00077225
Iteration 19/25 | Loss: 0.00077225
Iteration 20/25 | Loss: 0.00077225
Iteration 21/25 | Loss: 0.00077225
Iteration 22/25 | Loss: 0.00077225
Iteration 23/25 | Loss: 0.00077225
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007722480804659426, 0.0007722480804659426, 0.0007722480804659426, 0.0007722480804659426, 0.0007722480804659426]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007722480804659426

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.83228862
Iteration 2/25 | Loss: 0.00036643
Iteration 3/25 | Loss: 0.00036643
Iteration 4/25 | Loss: 0.00036642
Iteration 5/25 | Loss: 0.00036642
Iteration 6/25 | Loss: 0.00036642
Iteration 7/25 | Loss: 0.00036642
Iteration 8/25 | Loss: 0.00036642
Iteration 9/25 | Loss: 0.00036642
Iteration 10/25 | Loss: 0.00036642
Iteration 11/25 | Loss: 0.00036642
Iteration 12/25 | Loss: 0.00036642
Iteration 13/25 | Loss: 0.00036642
Iteration 14/25 | Loss: 0.00036642
Iteration 15/25 | Loss: 0.00036642
Iteration 16/25 | Loss: 0.00036642
Iteration 17/25 | Loss: 0.00036642
Iteration 18/25 | Loss: 0.00036642
Iteration 19/25 | Loss: 0.00036642
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0003664215619210154, 0.0003664215619210154, 0.0003664215619210154, 0.0003664215619210154, 0.0003664215619210154]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003664215619210154

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036642
Iteration 2/1000 | Loss: 0.00003052
Iteration 3/1000 | Loss: 0.00001650
Iteration 4/1000 | Loss: 0.00001421
Iteration 5/1000 | Loss: 0.00001359
Iteration 6/1000 | Loss: 0.00001300
Iteration 7/1000 | Loss: 0.00001267
Iteration 8/1000 | Loss: 0.00001235
Iteration 9/1000 | Loss: 0.00001230
Iteration 10/1000 | Loss: 0.00001227
Iteration 11/1000 | Loss: 0.00001226
Iteration 12/1000 | Loss: 0.00001210
Iteration 13/1000 | Loss: 0.00001206
Iteration 14/1000 | Loss: 0.00001190
Iteration 15/1000 | Loss: 0.00001180
Iteration 16/1000 | Loss: 0.00001175
Iteration 17/1000 | Loss: 0.00001171
Iteration 18/1000 | Loss: 0.00001165
Iteration 19/1000 | Loss: 0.00001165
Iteration 20/1000 | Loss: 0.00001164
Iteration 21/1000 | Loss: 0.00001162
Iteration 22/1000 | Loss: 0.00001162
Iteration 23/1000 | Loss: 0.00001162
Iteration 24/1000 | Loss: 0.00001162
Iteration 25/1000 | Loss: 0.00001162
Iteration 26/1000 | Loss: 0.00001162
Iteration 27/1000 | Loss: 0.00001162
Iteration 28/1000 | Loss: 0.00001162
Iteration 29/1000 | Loss: 0.00001162
Iteration 30/1000 | Loss: 0.00001161
Iteration 31/1000 | Loss: 0.00001161
Iteration 32/1000 | Loss: 0.00001161
Iteration 33/1000 | Loss: 0.00001161
Iteration 34/1000 | Loss: 0.00001160
Iteration 35/1000 | Loss: 0.00001159
Iteration 36/1000 | Loss: 0.00001158
Iteration 37/1000 | Loss: 0.00001158
Iteration 38/1000 | Loss: 0.00001157
Iteration 39/1000 | Loss: 0.00001156
Iteration 40/1000 | Loss: 0.00001156
Iteration 41/1000 | Loss: 0.00001154
Iteration 42/1000 | Loss: 0.00001154
Iteration 43/1000 | Loss: 0.00001154
Iteration 44/1000 | Loss: 0.00001154
Iteration 45/1000 | Loss: 0.00001154
Iteration 46/1000 | Loss: 0.00001154
Iteration 47/1000 | Loss: 0.00001154
Iteration 48/1000 | Loss: 0.00001154
Iteration 49/1000 | Loss: 0.00001153
Iteration 50/1000 | Loss: 0.00001152
Iteration 51/1000 | Loss: 0.00001152
Iteration 52/1000 | Loss: 0.00001152
Iteration 53/1000 | Loss: 0.00001151
Iteration 54/1000 | Loss: 0.00001151
Iteration 55/1000 | Loss: 0.00001151
Iteration 56/1000 | Loss: 0.00001150
Iteration 57/1000 | Loss: 0.00001149
Iteration 58/1000 | Loss: 0.00001149
Iteration 59/1000 | Loss: 0.00001148
Iteration 60/1000 | Loss: 0.00001148
Iteration 61/1000 | Loss: 0.00001145
Iteration 62/1000 | Loss: 0.00001141
Iteration 63/1000 | Loss: 0.00001140
Iteration 64/1000 | Loss: 0.00001140
Iteration 65/1000 | Loss: 0.00001140
Iteration 66/1000 | Loss: 0.00001140
Iteration 67/1000 | Loss: 0.00001140
Iteration 68/1000 | Loss: 0.00001140
Iteration 69/1000 | Loss: 0.00001140
Iteration 70/1000 | Loss: 0.00001139
Iteration 71/1000 | Loss: 0.00001139
Iteration 72/1000 | Loss: 0.00001139
Iteration 73/1000 | Loss: 0.00001139
Iteration 74/1000 | Loss: 0.00001139
Iteration 75/1000 | Loss: 0.00001139
Iteration 76/1000 | Loss: 0.00001138
Iteration 77/1000 | Loss: 0.00001137
Iteration 78/1000 | Loss: 0.00001136
Iteration 79/1000 | Loss: 0.00001136
Iteration 80/1000 | Loss: 0.00001136
Iteration 81/1000 | Loss: 0.00001136
Iteration 82/1000 | Loss: 0.00001135
Iteration 83/1000 | Loss: 0.00001135
Iteration 84/1000 | Loss: 0.00001134
Iteration 85/1000 | Loss: 0.00001134
Iteration 86/1000 | Loss: 0.00001134
Iteration 87/1000 | Loss: 0.00001134
Iteration 88/1000 | Loss: 0.00001134
Iteration 89/1000 | Loss: 0.00001134
Iteration 90/1000 | Loss: 0.00001134
Iteration 91/1000 | Loss: 0.00001134
Iteration 92/1000 | Loss: 0.00001134
Iteration 93/1000 | Loss: 0.00001134
Iteration 94/1000 | Loss: 0.00001134
Iteration 95/1000 | Loss: 0.00001134
Iteration 96/1000 | Loss: 0.00001134
Iteration 97/1000 | Loss: 0.00001133
Iteration 98/1000 | Loss: 0.00001133
Iteration 99/1000 | Loss: 0.00001133
Iteration 100/1000 | Loss: 0.00001133
Iteration 101/1000 | Loss: 0.00001133
Iteration 102/1000 | Loss: 0.00001133
Iteration 103/1000 | Loss: 0.00001133
Iteration 104/1000 | Loss: 0.00001132
Iteration 105/1000 | Loss: 0.00001132
Iteration 106/1000 | Loss: 0.00001132
Iteration 107/1000 | Loss: 0.00001132
Iteration 108/1000 | Loss: 0.00001132
Iteration 109/1000 | Loss: 0.00001132
Iteration 110/1000 | Loss: 0.00001132
Iteration 111/1000 | Loss: 0.00001131
Iteration 112/1000 | Loss: 0.00001131
Iteration 113/1000 | Loss: 0.00001131
Iteration 114/1000 | Loss: 0.00001131
Iteration 115/1000 | Loss: 0.00001131
Iteration 116/1000 | Loss: 0.00001131
Iteration 117/1000 | Loss: 0.00001131
Iteration 118/1000 | Loss: 0.00001131
Iteration 119/1000 | Loss: 0.00001131
Iteration 120/1000 | Loss: 0.00001130
Iteration 121/1000 | Loss: 0.00001130
Iteration 122/1000 | Loss: 0.00001130
Iteration 123/1000 | Loss: 0.00001130
Iteration 124/1000 | Loss: 0.00001130
Iteration 125/1000 | Loss: 0.00001129
Iteration 126/1000 | Loss: 0.00001129
Iteration 127/1000 | Loss: 0.00001129
Iteration 128/1000 | Loss: 0.00001129
Iteration 129/1000 | Loss: 0.00001129
Iteration 130/1000 | Loss: 0.00001128
Iteration 131/1000 | Loss: 0.00001128
Iteration 132/1000 | Loss: 0.00001128
Iteration 133/1000 | Loss: 0.00001128
Iteration 134/1000 | Loss: 0.00001128
Iteration 135/1000 | Loss: 0.00001127
Iteration 136/1000 | Loss: 0.00001127
Iteration 137/1000 | Loss: 0.00001126
Iteration 138/1000 | Loss: 0.00001126
Iteration 139/1000 | Loss: 0.00001126
Iteration 140/1000 | Loss: 0.00001125
Iteration 141/1000 | Loss: 0.00001125
Iteration 142/1000 | Loss: 0.00001125
Iteration 143/1000 | Loss: 0.00001125
Iteration 144/1000 | Loss: 0.00001125
Iteration 145/1000 | Loss: 0.00001125
Iteration 146/1000 | Loss: 0.00001124
Iteration 147/1000 | Loss: 0.00001124
Iteration 148/1000 | Loss: 0.00001124
Iteration 149/1000 | Loss: 0.00001124
Iteration 150/1000 | Loss: 0.00001124
Iteration 151/1000 | Loss: 0.00001124
Iteration 152/1000 | Loss: 0.00001124
Iteration 153/1000 | Loss: 0.00001124
Iteration 154/1000 | Loss: 0.00001124
Iteration 155/1000 | Loss: 0.00001124
Iteration 156/1000 | Loss: 0.00001124
Iteration 157/1000 | Loss: 0.00001124
Iteration 158/1000 | Loss: 0.00001123
Iteration 159/1000 | Loss: 0.00001123
Iteration 160/1000 | Loss: 0.00001123
Iteration 161/1000 | Loss: 0.00001123
Iteration 162/1000 | Loss: 0.00001123
Iteration 163/1000 | Loss: 0.00001123
Iteration 164/1000 | Loss: 0.00001123
Iteration 165/1000 | Loss: 0.00001123
Iteration 166/1000 | Loss: 0.00001123
Iteration 167/1000 | Loss: 0.00001123
Iteration 168/1000 | Loss: 0.00001122
Iteration 169/1000 | Loss: 0.00001122
Iteration 170/1000 | Loss: 0.00001122
Iteration 171/1000 | Loss: 0.00001122
Iteration 172/1000 | Loss: 0.00001122
Iteration 173/1000 | Loss: 0.00001122
Iteration 174/1000 | Loss: 0.00001121
Iteration 175/1000 | Loss: 0.00001121
Iteration 176/1000 | Loss: 0.00001121
Iteration 177/1000 | Loss: 0.00001121
Iteration 178/1000 | Loss: 0.00001121
Iteration 179/1000 | Loss: 0.00001121
Iteration 180/1000 | Loss: 0.00001121
Iteration 181/1000 | Loss: 0.00001120
Iteration 182/1000 | Loss: 0.00001120
Iteration 183/1000 | Loss: 0.00001120
Iteration 184/1000 | Loss: 0.00001120
Iteration 185/1000 | Loss: 0.00001120
Iteration 186/1000 | Loss: 0.00001120
Iteration 187/1000 | Loss: 0.00001119
Iteration 188/1000 | Loss: 0.00001119
Iteration 189/1000 | Loss: 0.00001119
Iteration 190/1000 | Loss: 0.00001118
Iteration 191/1000 | Loss: 0.00001118
Iteration 192/1000 | Loss: 0.00001118
Iteration 193/1000 | Loss: 0.00001118
Iteration 194/1000 | Loss: 0.00001118
Iteration 195/1000 | Loss: 0.00001118
Iteration 196/1000 | Loss: 0.00001117
Iteration 197/1000 | Loss: 0.00001117
Iteration 198/1000 | Loss: 0.00001117
Iteration 199/1000 | Loss: 0.00001117
Iteration 200/1000 | Loss: 0.00001117
Iteration 201/1000 | Loss: 0.00001116
Iteration 202/1000 | Loss: 0.00001116
Iteration 203/1000 | Loss: 0.00001116
Iteration 204/1000 | Loss: 0.00001116
Iteration 205/1000 | Loss: 0.00001115
Iteration 206/1000 | Loss: 0.00001115
Iteration 207/1000 | Loss: 0.00001115
Iteration 208/1000 | Loss: 0.00001115
Iteration 209/1000 | Loss: 0.00001115
Iteration 210/1000 | Loss: 0.00001115
Iteration 211/1000 | Loss: 0.00001115
Iteration 212/1000 | Loss: 0.00001115
Iteration 213/1000 | Loss: 0.00001115
Iteration 214/1000 | Loss: 0.00001114
Iteration 215/1000 | Loss: 0.00001114
Iteration 216/1000 | Loss: 0.00001114
Iteration 217/1000 | Loss: 0.00001114
Iteration 218/1000 | Loss: 0.00001114
Iteration 219/1000 | Loss: 0.00001113
Iteration 220/1000 | Loss: 0.00001113
Iteration 221/1000 | Loss: 0.00001113
Iteration 222/1000 | Loss: 0.00001113
Iteration 223/1000 | Loss: 0.00001112
Iteration 224/1000 | Loss: 0.00001112
Iteration 225/1000 | Loss: 0.00001112
Iteration 226/1000 | Loss: 0.00001112
Iteration 227/1000 | Loss: 0.00001112
Iteration 228/1000 | Loss: 0.00001112
Iteration 229/1000 | Loss: 0.00001112
Iteration 230/1000 | Loss: 0.00001112
Iteration 231/1000 | Loss: 0.00001112
Iteration 232/1000 | Loss: 0.00001111
Iteration 233/1000 | Loss: 0.00001111
Iteration 234/1000 | Loss: 0.00001111
Iteration 235/1000 | Loss: 0.00001111
Iteration 236/1000 | Loss: 0.00001111
Iteration 237/1000 | Loss: 0.00001111
Iteration 238/1000 | Loss: 0.00001111
Iteration 239/1000 | Loss: 0.00001111
Iteration 240/1000 | Loss: 0.00001111
Iteration 241/1000 | Loss: 0.00001111
Iteration 242/1000 | Loss: 0.00001111
Iteration 243/1000 | Loss: 0.00001110
Iteration 244/1000 | Loss: 0.00001110
Iteration 245/1000 | Loss: 0.00001110
Iteration 246/1000 | Loss: 0.00001110
Iteration 247/1000 | Loss: 0.00001110
Iteration 248/1000 | Loss: 0.00001110
Iteration 249/1000 | Loss: 0.00001110
Iteration 250/1000 | Loss: 0.00001110
Iteration 251/1000 | Loss: 0.00001110
Iteration 252/1000 | Loss: 0.00001110
Iteration 253/1000 | Loss: 0.00001110
Iteration 254/1000 | Loss: 0.00001110
Iteration 255/1000 | Loss: 0.00001110
Iteration 256/1000 | Loss: 0.00001110
Iteration 257/1000 | Loss: 0.00001110
Iteration 258/1000 | Loss: 0.00001110
Iteration 259/1000 | Loss: 0.00001110
Iteration 260/1000 | Loss: 0.00001109
Iteration 261/1000 | Loss: 0.00001109
Iteration 262/1000 | Loss: 0.00001109
Iteration 263/1000 | Loss: 0.00001109
Iteration 264/1000 | Loss: 0.00001109
Iteration 265/1000 | Loss: 0.00001109
Iteration 266/1000 | Loss: 0.00001109
Iteration 267/1000 | Loss: 0.00001109
Iteration 268/1000 | Loss: 0.00001109
Iteration 269/1000 | Loss: 0.00001109
Iteration 270/1000 | Loss: 0.00001109
Iteration 271/1000 | Loss: 0.00001109
Iteration 272/1000 | Loss: 0.00001109
Iteration 273/1000 | Loss: 0.00001109
Iteration 274/1000 | Loss: 0.00001109
Iteration 275/1000 | Loss: 0.00001109
Iteration 276/1000 | Loss: 0.00001109
Iteration 277/1000 | Loss: 0.00001109
Iteration 278/1000 | Loss: 0.00001109
Iteration 279/1000 | Loss: 0.00001109
Iteration 280/1000 | Loss: 0.00001109
Iteration 281/1000 | Loss: 0.00001109
Iteration 282/1000 | Loss: 0.00001109
Iteration 283/1000 | Loss: 0.00001109
Iteration 284/1000 | Loss: 0.00001109
Iteration 285/1000 | Loss: 0.00001109
Iteration 286/1000 | Loss: 0.00001109
Iteration 287/1000 | Loss: 0.00001109
Iteration 288/1000 | Loss: 0.00001109
Iteration 289/1000 | Loss: 0.00001109
Iteration 290/1000 | Loss: 0.00001109
Iteration 291/1000 | Loss: 0.00001109
Iteration 292/1000 | Loss: 0.00001109
Iteration 293/1000 | Loss: 0.00001109
Iteration 294/1000 | Loss: 0.00001109
Iteration 295/1000 | Loss: 0.00001109
Iteration 296/1000 | Loss: 0.00001109
Iteration 297/1000 | Loss: 0.00001109
Iteration 298/1000 | Loss: 0.00001109
Iteration 299/1000 | Loss: 0.00001109
Iteration 300/1000 | Loss: 0.00001109
Iteration 301/1000 | Loss: 0.00001109
Iteration 302/1000 | Loss: 0.00001109
Iteration 303/1000 | Loss: 0.00001109
Iteration 304/1000 | Loss: 0.00001109
Iteration 305/1000 | Loss: 0.00001109
Iteration 306/1000 | Loss: 0.00001109
Iteration 307/1000 | Loss: 0.00001109
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 307. Stopping optimization.
Last 5 losses: [1.10859737105784e-05, 1.10859737105784e-05, 1.10859737105784e-05, 1.10859737105784e-05, 1.10859737105784e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.10859737105784e-05

Optimization complete. Final v2v error: 2.7726223468780518 mm

Highest mean error: 3.3919763565063477 mm for frame 15

Lowest mean error: 2.5524895191192627 mm for frame 59

Saving results

Total time: 50.10807418823242
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00366353
Iteration 2/25 | Loss: 0.00082509
Iteration 3/25 | Loss: 0.00071998
Iteration 4/25 | Loss: 0.00070857
Iteration 5/25 | Loss: 0.00070530
Iteration 6/25 | Loss: 0.00070407
Iteration 7/25 | Loss: 0.00070392
Iteration 8/25 | Loss: 0.00070392
Iteration 9/25 | Loss: 0.00070392
Iteration 10/25 | Loss: 0.00070392
Iteration 11/25 | Loss: 0.00070392
Iteration 12/25 | Loss: 0.00070392
Iteration 13/25 | Loss: 0.00070392
Iteration 14/25 | Loss: 0.00070392
Iteration 15/25 | Loss: 0.00070392
Iteration 16/25 | Loss: 0.00070392
Iteration 17/25 | Loss: 0.00070392
Iteration 18/25 | Loss: 0.00070392
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007039161282591522, 0.0007039161282591522, 0.0007039161282591522, 0.0007039161282591522, 0.0007039161282591522]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007039161282591522

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.94799256
Iteration 2/25 | Loss: 0.00045617
Iteration 3/25 | Loss: 0.00045616
Iteration 4/25 | Loss: 0.00045616
Iteration 5/25 | Loss: 0.00045616
Iteration 6/25 | Loss: 0.00045616
Iteration 7/25 | Loss: 0.00045616
Iteration 8/25 | Loss: 0.00045616
Iteration 9/25 | Loss: 0.00045616
Iteration 10/25 | Loss: 0.00045616
Iteration 11/25 | Loss: 0.00045616
Iteration 12/25 | Loss: 0.00045616
Iteration 13/25 | Loss: 0.00045616
Iteration 14/25 | Loss: 0.00045616
Iteration 15/25 | Loss: 0.00045616
Iteration 16/25 | Loss: 0.00045616
Iteration 17/25 | Loss: 0.00045616
Iteration 18/25 | Loss: 0.00045616
Iteration 19/25 | Loss: 0.00045616
Iteration 20/25 | Loss: 0.00045616
Iteration 21/25 | Loss: 0.00045616
Iteration 22/25 | Loss: 0.00045616
Iteration 23/25 | Loss: 0.00045616
Iteration 24/25 | Loss: 0.00045616
Iteration 25/25 | Loss: 0.00045616

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045616
Iteration 2/1000 | Loss: 0.00002086
Iteration 3/1000 | Loss: 0.00001134
Iteration 4/1000 | Loss: 0.00001052
Iteration 5/1000 | Loss: 0.00000986
Iteration 6/1000 | Loss: 0.00000957
Iteration 7/1000 | Loss: 0.00000938
Iteration 8/1000 | Loss: 0.00000935
Iteration 9/1000 | Loss: 0.00000932
Iteration 10/1000 | Loss: 0.00000932
Iteration 11/1000 | Loss: 0.00000932
Iteration 12/1000 | Loss: 0.00000932
Iteration 13/1000 | Loss: 0.00000932
Iteration 14/1000 | Loss: 0.00000931
Iteration 15/1000 | Loss: 0.00000931
Iteration 16/1000 | Loss: 0.00000931
Iteration 17/1000 | Loss: 0.00000931
Iteration 18/1000 | Loss: 0.00000931
Iteration 19/1000 | Loss: 0.00000931
Iteration 20/1000 | Loss: 0.00000931
Iteration 21/1000 | Loss: 0.00000931
Iteration 22/1000 | Loss: 0.00000930
Iteration 23/1000 | Loss: 0.00000927
Iteration 24/1000 | Loss: 0.00000926
Iteration 25/1000 | Loss: 0.00000926
Iteration 26/1000 | Loss: 0.00000925
Iteration 27/1000 | Loss: 0.00000924
Iteration 28/1000 | Loss: 0.00000921
Iteration 29/1000 | Loss: 0.00000920
Iteration 30/1000 | Loss: 0.00000920
Iteration 31/1000 | Loss: 0.00000920
Iteration 32/1000 | Loss: 0.00000919
Iteration 33/1000 | Loss: 0.00000919
Iteration 34/1000 | Loss: 0.00000919
Iteration 35/1000 | Loss: 0.00000916
Iteration 36/1000 | Loss: 0.00000916
Iteration 37/1000 | Loss: 0.00000915
Iteration 38/1000 | Loss: 0.00000915
Iteration 39/1000 | Loss: 0.00000914
Iteration 40/1000 | Loss: 0.00000911
Iteration 41/1000 | Loss: 0.00000911
Iteration 42/1000 | Loss: 0.00000910
Iteration 43/1000 | Loss: 0.00000910
Iteration 44/1000 | Loss: 0.00000910
Iteration 45/1000 | Loss: 0.00000909
Iteration 46/1000 | Loss: 0.00000909
Iteration 47/1000 | Loss: 0.00000909
Iteration 48/1000 | Loss: 0.00000908
Iteration 49/1000 | Loss: 0.00000906
Iteration 50/1000 | Loss: 0.00000905
Iteration 51/1000 | Loss: 0.00000905
Iteration 52/1000 | Loss: 0.00000905
Iteration 53/1000 | Loss: 0.00000905
Iteration 54/1000 | Loss: 0.00000904
Iteration 55/1000 | Loss: 0.00000904
Iteration 56/1000 | Loss: 0.00000903
Iteration 57/1000 | Loss: 0.00000903
Iteration 58/1000 | Loss: 0.00000903
Iteration 59/1000 | Loss: 0.00000902
Iteration 60/1000 | Loss: 0.00000902
Iteration 61/1000 | Loss: 0.00000902
Iteration 62/1000 | Loss: 0.00000902
Iteration 63/1000 | Loss: 0.00000901
Iteration 64/1000 | Loss: 0.00000901
Iteration 65/1000 | Loss: 0.00000900
Iteration 66/1000 | Loss: 0.00000900
Iteration 67/1000 | Loss: 0.00000900
Iteration 68/1000 | Loss: 0.00000899
Iteration 69/1000 | Loss: 0.00000899
Iteration 70/1000 | Loss: 0.00000899
Iteration 71/1000 | Loss: 0.00000898
Iteration 72/1000 | Loss: 0.00000898
Iteration 73/1000 | Loss: 0.00000898
Iteration 74/1000 | Loss: 0.00000898
Iteration 75/1000 | Loss: 0.00000898
Iteration 76/1000 | Loss: 0.00000897
Iteration 77/1000 | Loss: 0.00000897
Iteration 78/1000 | Loss: 0.00000897
Iteration 79/1000 | Loss: 0.00000896
Iteration 80/1000 | Loss: 0.00000896
Iteration 81/1000 | Loss: 0.00000895
Iteration 82/1000 | Loss: 0.00000895
Iteration 83/1000 | Loss: 0.00000895
Iteration 84/1000 | Loss: 0.00000895
Iteration 85/1000 | Loss: 0.00000895
Iteration 86/1000 | Loss: 0.00000895
Iteration 87/1000 | Loss: 0.00000895
Iteration 88/1000 | Loss: 0.00000894
Iteration 89/1000 | Loss: 0.00000894
Iteration 90/1000 | Loss: 0.00000894
Iteration 91/1000 | Loss: 0.00000894
Iteration 92/1000 | Loss: 0.00000894
Iteration 93/1000 | Loss: 0.00000894
Iteration 94/1000 | Loss: 0.00000894
Iteration 95/1000 | Loss: 0.00000894
Iteration 96/1000 | Loss: 0.00000894
Iteration 97/1000 | Loss: 0.00000894
Iteration 98/1000 | Loss: 0.00000894
Iteration 99/1000 | Loss: 0.00000894
Iteration 100/1000 | Loss: 0.00000894
Iteration 101/1000 | Loss: 0.00000894
Iteration 102/1000 | Loss: 0.00000894
Iteration 103/1000 | Loss: 0.00000894
Iteration 104/1000 | Loss: 0.00000894
Iteration 105/1000 | Loss: 0.00000893
Iteration 106/1000 | Loss: 0.00000893
Iteration 107/1000 | Loss: 0.00000893
Iteration 108/1000 | Loss: 0.00000893
Iteration 109/1000 | Loss: 0.00000893
Iteration 110/1000 | Loss: 0.00000893
Iteration 111/1000 | Loss: 0.00000892
Iteration 112/1000 | Loss: 0.00000892
Iteration 113/1000 | Loss: 0.00000892
Iteration 114/1000 | Loss: 0.00000892
Iteration 115/1000 | Loss: 0.00000892
Iteration 116/1000 | Loss: 0.00000892
Iteration 117/1000 | Loss: 0.00000892
Iteration 118/1000 | Loss: 0.00000891
Iteration 119/1000 | Loss: 0.00000891
Iteration 120/1000 | Loss: 0.00000891
Iteration 121/1000 | Loss: 0.00000891
Iteration 122/1000 | Loss: 0.00000891
Iteration 123/1000 | Loss: 0.00000891
Iteration 124/1000 | Loss: 0.00000890
Iteration 125/1000 | Loss: 0.00000890
Iteration 126/1000 | Loss: 0.00000890
Iteration 127/1000 | Loss: 0.00000890
Iteration 128/1000 | Loss: 0.00000890
Iteration 129/1000 | Loss: 0.00000889
Iteration 130/1000 | Loss: 0.00000889
Iteration 131/1000 | Loss: 0.00000889
Iteration 132/1000 | Loss: 0.00000889
Iteration 133/1000 | Loss: 0.00000888
Iteration 134/1000 | Loss: 0.00000888
Iteration 135/1000 | Loss: 0.00000888
Iteration 136/1000 | Loss: 0.00000888
Iteration 137/1000 | Loss: 0.00000888
Iteration 138/1000 | Loss: 0.00000888
Iteration 139/1000 | Loss: 0.00000888
Iteration 140/1000 | Loss: 0.00000888
Iteration 141/1000 | Loss: 0.00000888
Iteration 142/1000 | Loss: 0.00000888
Iteration 143/1000 | Loss: 0.00000887
Iteration 144/1000 | Loss: 0.00000887
Iteration 145/1000 | Loss: 0.00000886
Iteration 146/1000 | Loss: 0.00000886
Iteration 147/1000 | Loss: 0.00000886
Iteration 148/1000 | Loss: 0.00000886
Iteration 149/1000 | Loss: 0.00000886
Iteration 150/1000 | Loss: 0.00000885
Iteration 151/1000 | Loss: 0.00000885
Iteration 152/1000 | Loss: 0.00000885
Iteration 153/1000 | Loss: 0.00000885
Iteration 154/1000 | Loss: 0.00000884
Iteration 155/1000 | Loss: 0.00000884
Iteration 156/1000 | Loss: 0.00000884
Iteration 157/1000 | Loss: 0.00000884
Iteration 158/1000 | Loss: 0.00000884
Iteration 159/1000 | Loss: 0.00000884
Iteration 160/1000 | Loss: 0.00000884
Iteration 161/1000 | Loss: 0.00000884
Iteration 162/1000 | Loss: 0.00000884
Iteration 163/1000 | Loss: 0.00000883
Iteration 164/1000 | Loss: 0.00000883
Iteration 165/1000 | Loss: 0.00000883
Iteration 166/1000 | Loss: 0.00000882
Iteration 167/1000 | Loss: 0.00000882
Iteration 168/1000 | Loss: 0.00000881
Iteration 169/1000 | Loss: 0.00000881
Iteration 170/1000 | Loss: 0.00000881
Iteration 171/1000 | Loss: 0.00000881
Iteration 172/1000 | Loss: 0.00000881
Iteration 173/1000 | Loss: 0.00000880
Iteration 174/1000 | Loss: 0.00000880
Iteration 175/1000 | Loss: 0.00000880
Iteration 176/1000 | Loss: 0.00000880
Iteration 177/1000 | Loss: 0.00000880
Iteration 178/1000 | Loss: 0.00000880
Iteration 179/1000 | Loss: 0.00000880
Iteration 180/1000 | Loss: 0.00000880
Iteration 181/1000 | Loss: 0.00000880
Iteration 182/1000 | Loss: 0.00000880
Iteration 183/1000 | Loss: 0.00000880
Iteration 184/1000 | Loss: 0.00000879
Iteration 185/1000 | Loss: 0.00000879
Iteration 186/1000 | Loss: 0.00000879
Iteration 187/1000 | Loss: 0.00000879
Iteration 188/1000 | Loss: 0.00000879
Iteration 189/1000 | Loss: 0.00000879
Iteration 190/1000 | Loss: 0.00000879
Iteration 191/1000 | Loss: 0.00000879
Iteration 192/1000 | Loss: 0.00000879
Iteration 193/1000 | Loss: 0.00000879
Iteration 194/1000 | Loss: 0.00000879
Iteration 195/1000 | Loss: 0.00000879
Iteration 196/1000 | Loss: 0.00000879
Iteration 197/1000 | Loss: 0.00000879
Iteration 198/1000 | Loss: 0.00000879
Iteration 199/1000 | Loss: 0.00000879
Iteration 200/1000 | Loss: 0.00000879
Iteration 201/1000 | Loss: 0.00000879
Iteration 202/1000 | Loss: 0.00000879
Iteration 203/1000 | Loss: 0.00000879
Iteration 204/1000 | Loss: 0.00000879
Iteration 205/1000 | Loss: 0.00000879
Iteration 206/1000 | Loss: 0.00000879
Iteration 207/1000 | Loss: 0.00000879
Iteration 208/1000 | Loss: 0.00000879
Iteration 209/1000 | Loss: 0.00000879
Iteration 210/1000 | Loss: 0.00000879
Iteration 211/1000 | Loss: 0.00000879
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [8.786668331595138e-06, 8.786668331595138e-06, 8.786668331595138e-06, 8.786668331595138e-06, 8.786668331595138e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.786668331595138e-06

Optimization complete. Final v2v error: 2.5371146202087402 mm

Highest mean error: 3.1033213138580322 mm for frame 77

Lowest mean error: 2.4055721759796143 mm for frame 3

Saving results

Total time: 36.95957136154175
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00973623
Iteration 2/25 | Loss: 0.00164399
Iteration 3/25 | Loss: 0.00102403
Iteration 4/25 | Loss: 0.00097345
Iteration 5/25 | Loss: 0.00095654
Iteration 6/25 | Loss: 0.00095167
Iteration 7/25 | Loss: 0.00095080
Iteration 8/25 | Loss: 0.00095080
Iteration 9/25 | Loss: 0.00095080
Iteration 10/25 | Loss: 0.00095080
Iteration 11/25 | Loss: 0.00095080
Iteration 12/25 | Loss: 0.00095080
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009508039220236242, 0.0009508039220236242, 0.0009508039220236242, 0.0009508039220236242, 0.0009508039220236242]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009508039220236242

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.97018427
Iteration 2/25 | Loss: 0.00041940
Iteration 3/25 | Loss: 0.00041940
Iteration 4/25 | Loss: 0.00041940
Iteration 5/25 | Loss: 0.00041940
Iteration 6/25 | Loss: 0.00041940
Iteration 7/25 | Loss: 0.00041940
Iteration 8/25 | Loss: 0.00041940
Iteration 9/25 | Loss: 0.00041940
Iteration 10/25 | Loss: 0.00041940
Iteration 11/25 | Loss: 0.00041940
Iteration 12/25 | Loss: 0.00041940
Iteration 13/25 | Loss: 0.00041940
Iteration 14/25 | Loss: 0.00041940
Iteration 15/25 | Loss: 0.00041940
Iteration 16/25 | Loss: 0.00041940
Iteration 17/25 | Loss: 0.00041940
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00041939737275242805, 0.00041939737275242805, 0.00041939737275242805, 0.00041939737275242805, 0.00041939737275242805]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00041939737275242805

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041940
Iteration 2/1000 | Loss: 0.00006902
Iteration 3/1000 | Loss: 0.00004346
Iteration 4/1000 | Loss: 0.00004061
Iteration 5/1000 | Loss: 0.00003863
Iteration 6/1000 | Loss: 0.00003771
Iteration 7/1000 | Loss: 0.00003684
Iteration 8/1000 | Loss: 0.00003617
Iteration 9/1000 | Loss: 0.00003574
Iteration 10/1000 | Loss: 0.00003543
Iteration 11/1000 | Loss: 0.00003510
Iteration 12/1000 | Loss: 0.00003485
Iteration 13/1000 | Loss: 0.00003455
Iteration 14/1000 | Loss: 0.00003434
Iteration 15/1000 | Loss: 0.00003414
Iteration 16/1000 | Loss: 0.00003397
Iteration 17/1000 | Loss: 0.00003387
Iteration 18/1000 | Loss: 0.00003381
Iteration 19/1000 | Loss: 0.00003381
Iteration 20/1000 | Loss: 0.00003377
Iteration 21/1000 | Loss: 0.00003374
Iteration 22/1000 | Loss: 0.00003374
Iteration 23/1000 | Loss: 0.00003371
Iteration 24/1000 | Loss: 0.00003371
Iteration 25/1000 | Loss: 0.00003369
Iteration 26/1000 | Loss: 0.00003369
Iteration 27/1000 | Loss: 0.00003369
Iteration 28/1000 | Loss: 0.00003369
Iteration 29/1000 | Loss: 0.00003368
Iteration 30/1000 | Loss: 0.00003367
Iteration 31/1000 | Loss: 0.00003367
Iteration 32/1000 | Loss: 0.00003365
Iteration 33/1000 | Loss: 0.00003364
Iteration 34/1000 | Loss: 0.00003364
Iteration 35/1000 | Loss: 0.00003363
Iteration 36/1000 | Loss: 0.00003363
Iteration 37/1000 | Loss: 0.00003363
Iteration 38/1000 | Loss: 0.00003363
Iteration 39/1000 | Loss: 0.00003363
Iteration 40/1000 | Loss: 0.00003363
Iteration 41/1000 | Loss: 0.00003362
Iteration 42/1000 | Loss: 0.00003362
Iteration 43/1000 | Loss: 0.00003362
Iteration 44/1000 | Loss: 0.00003362
Iteration 45/1000 | Loss: 0.00003362
Iteration 46/1000 | Loss: 0.00003362
Iteration 47/1000 | Loss: 0.00003362
Iteration 48/1000 | Loss: 0.00003361
Iteration 49/1000 | Loss: 0.00003358
Iteration 50/1000 | Loss: 0.00003356
Iteration 51/1000 | Loss: 0.00003356
Iteration 52/1000 | Loss: 0.00003355
Iteration 53/1000 | Loss: 0.00003355
Iteration 54/1000 | Loss: 0.00003354
Iteration 55/1000 | Loss: 0.00003354
Iteration 56/1000 | Loss: 0.00003353
Iteration 57/1000 | Loss: 0.00003353
Iteration 58/1000 | Loss: 0.00003353
Iteration 59/1000 | Loss: 0.00003353
Iteration 60/1000 | Loss: 0.00003353
Iteration 61/1000 | Loss: 0.00003353
Iteration 62/1000 | Loss: 0.00003353
Iteration 63/1000 | Loss: 0.00003353
Iteration 64/1000 | Loss: 0.00003353
Iteration 65/1000 | Loss: 0.00003353
Iteration 66/1000 | Loss: 0.00003353
Iteration 67/1000 | Loss: 0.00003353
Iteration 68/1000 | Loss: 0.00003352
Iteration 69/1000 | Loss: 0.00003352
Iteration 70/1000 | Loss: 0.00003352
Iteration 71/1000 | Loss: 0.00003352
Iteration 72/1000 | Loss: 0.00003352
Iteration 73/1000 | Loss: 0.00003352
Iteration 74/1000 | Loss: 0.00003352
Iteration 75/1000 | Loss: 0.00003352
Iteration 76/1000 | Loss: 0.00003352
Iteration 77/1000 | Loss: 0.00003352
Iteration 78/1000 | Loss: 0.00003352
Iteration 79/1000 | Loss: 0.00003352
Iteration 80/1000 | Loss: 0.00003352
Iteration 81/1000 | Loss: 0.00003351
Iteration 82/1000 | Loss: 0.00003351
Iteration 83/1000 | Loss: 0.00003351
Iteration 84/1000 | Loss: 0.00003350
Iteration 85/1000 | Loss: 0.00003349
Iteration 86/1000 | Loss: 0.00003349
Iteration 87/1000 | Loss: 0.00003349
Iteration 88/1000 | Loss: 0.00003348
Iteration 89/1000 | Loss: 0.00003348
Iteration 90/1000 | Loss: 0.00003348
Iteration 91/1000 | Loss: 0.00003348
Iteration 92/1000 | Loss: 0.00003348
Iteration 93/1000 | Loss: 0.00003347
Iteration 94/1000 | Loss: 0.00003347
Iteration 95/1000 | Loss: 0.00003347
Iteration 96/1000 | Loss: 0.00003347
Iteration 97/1000 | Loss: 0.00003347
Iteration 98/1000 | Loss: 0.00003347
Iteration 99/1000 | Loss: 0.00003346
Iteration 100/1000 | Loss: 0.00003346
Iteration 101/1000 | Loss: 0.00003346
Iteration 102/1000 | Loss: 0.00003346
Iteration 103/1000 | Loss: 0.00003346
Iteration 104/1000 | Loss: 0.00003346
Iteration 105/1000 | Loss: 0.00003345
Iteration 106/1000 | Loss: 0.00003345
Iteration 107/1000 | Loss: 0.00003345
Iteration 108/1000 | Loss: 0.00003345
Iteration 109/1000 | Loss: 0.00003345
Iteration 110/1000 | Loss: 0.00003345
Iteration 111/1000 | Loss: 0.00003345
Iteration 112/1000 | Loss: 0.00003345
Iteration 113/1000 | Loss: 0.00003345
Iteration 114/1000 | Loss: 0.00003345
Iteration 115/1000 | Loss: 0.00003345
Iteration 116/1000 | Loss: 0.00003344
Iteration 117/1000 | Loss: 0.00003344
Iteration 118/1000 | Loss: 0.00003344
Iteration 119/1000 | Loss: 0.00003344
Iteration 120/1000 | Loss: 0.00003344
Iteration 121/1000 | Loss: 0.00003343
Iteration 122/1000 | Loss: 0.00003343
Iteration 123/1000 | Loss: 0.00003343
Iteration 124/1000 | Loss: 0.00003343
Iteration 125/1000 | Loss: 0.00003342
Iteration 126/1000 | Loss: 0.00003342
Iteration 127/1000 | Loss: 0.00003342
Iteration 128/1000 | Loss: 0.00003342
Iteration 129/1000 | Loss: 0.00003342
Iteration 130/1000 | Loss: 0.00003342
Iteration 131/1000 | Loss: 0.00003342
Iteration 132/1000 | Loss: 0.00003342
Iteration 133/1000 | Loss: 0.00003341
Iteration 134/1000 | Loss: 0.00003341
Iteration 135/1000 | Loss: 0.00003341
Iteration 136/1000 | Loss: 0.00003341
Iteration 137/1000 | Loss: 0.00003340
Iteration 138/1000 | Loss: 0.00003340
Iteration 139/1000 | Loss: 0.00003340
Iteration 140/1000 | Loss: 0.00003340
Iteration 141/1000 | Loss: 0.00003339
Iteration 142/1000 | Loss: 0.00003339
Iteration 143/1000 | Loss: 0.00003339
Iteration 144/1000 | Loss: 0.00003339
Iteration 145/1000 | Loss: 0.00003339
Iteration 146/1000 | Loss: 0.00003339
Iteration 147/1000 | Loss: 0.00003338
Iteration 148/1000 | Loss: 0.00003338
Iteration 149/1000 | Loss: 0.00003338
Iteration 150/1000 | Loss: 0.00003338
Iteration 151/1000 | Loss: 0.00003338
Iteration 152/1000 | Loss: 0.00003338
Iteration 153/1000 | Loss: 0.00003338
Iteration 154/1000 | Loss: 0.00003338
Iteration 155/1000 | Loss: 0.00003338
Iteration 156/1000 | Loss: 0.00003337
Iteration 157/1000 | Loss: 0.00003337
Iteration 158/1000 | Loss: 0.00003337
Iteration 159/1000 | Loss: 0.00003337
Iteration 160/1000 | Loss: 0.00003337
Iteration 161/1000 | Loss: 0.00003337
Iteration 162/1000 | Loss: 0.00003337
Iteration 163/1000 | Loss: 0.00003337
Iteration 164/1000 | Loss: 0.00003336
Iteration 165/1000 | Loss: 0.00003336
Iteration 166/1000 | Loss: 0.00003336
Iteration 167/1000 | Loss: 0.00003336
Iteration 168/1000 | Loss: 0.00003336
Iteration 169/1000 | Loss: 0.00003336
Iteration 170/1000 | Loss: 0.00003336
Iteration 171/1000 | Loss: 0.00003336
Iteration 172/1000 | Loss: 0.00003336
Iteration 173/1000 | Loss: 0.00003336
Iteration 174/1000 | Loss: 0.00003336
Iteration 175/1000 | Loss: 0.00003336
Iteration 176/1000 | Loss: 0.00003336
Iteration 177/1000 | Loss: 0.00003336
Iteration 178/1000 | Loss: 0.00003335
Iteration 179/1000 | Loss: 0.00003335
Iteration 180/1000 | Loss: 0.00003335
Iteration 181/1000 | Loss: 0.00003335
Iteration 182/1000 | Loss: 0.00003335
Iteration 183/1000 | Loss: 0.00003335
Iteration 184/1000 | Loss: 0.00003335
Iteration 185/1000 | Loss: 0.00003334
Iteration 186/1000 | Loss: 0.00003334
Iteration 187/1000 | Loss: 0.00003334
Iteration 188/1000 | Loss: 0.00003334
Iteration 189/1000 | Loss: 0.00003334
Iteration 190/1000 | Loss: 0.00003334
Iteration 191/1000 | Loss: 0.00003334
Iteration 192/1000 | Loss: 0.00003334
Iteration 193/1000 | Loss: 0.00003333
Iteration 194/1000 | Loss: 0.00003333
Iteration 195/1000 | Loss: 0.00003333
Iteration 196/1000 | Loss: 0.00003333
Iteration 197/1000 | Loss: 0.00003333
Iteration 198/1000 | Loss: 0.00003333
Iteration 199/1000 | Loss: 0.00003333
Iteration 200/1000 | Loss: 0.00003333
Iteration 201/1000 | Loss: 0.00003333
Iteration 202/1000 | Loss: 0.00003333
Iteration 203/1000 | Loss: 0.00003333
Iteration 204/1000 | Loss: 0.00003333
Iteration 205/1000 | Loss: 0.00003333
Iteration 206/1000 | Loss: 0.00003333
Iteration 207/1000 | Loss: 0.00003333
Iteration 208/1000 | Loss: 0.00003333
Iteration 209/1000 | Loss: 0.00003333
Iteration 210/1000 | Loss: 0.00003332
Iteration 211/1000 | Loss: 0.00003332
Iteration 212/1000 | Loss: 0.00003332
Iteration 213/1000 | Loss: 0.00003332
Iteration 214/1000 | Loss: 0.00003332
Iteration 215/1000 | Loss: 0.00003332
Iteration 216/1000 | Loss: 0.00003332
Iteration 217/1000 | Loss: 0.00003332
Iteration 218/1000 | Loss: 0.00003332
Iteration 219/1000 | Loss: 0.00003332
Iteration 220/1000 | Loss: 0.00003332
Iteration 221/1000 | Loss: 0.00003332
Iteration 222/1000 | Loss: 0.00003332
Iteration 223/1000 | Loss: 0.00003332
Iteration 224/1000 | Loss: 0.00003332
Iteration 225/1000 | Loss: 0.00003332
Iteration 226/1000 | Loss: 0.00003331
Iteration 227/1000 | Loss: 0.00003331
Iteration 228/1000 | Loss: 0.00003331
Iteration 229/1000 | Loss: 0.00003331
Iteration 230/1000 | Loss: 0.00003331
Iteration 231/1000 | Loss: 0.00003331
Iteration 232/1000 | Loss: 0.00003331
Iteration 233/1000 | Loss: 0.00003331
Iteration 234/1000 | Loss: 0.00003331
Iteration 235/1000 | Loss: 0.00003331
Iteration 236/1000 | Loss: 0.00003331
Iteration 237/1000 | Loss: 0.00003331
Iteration 238/1000 | Loss: 0.00003331
Iteration 239/1000 | Loss: 0.00003331
Iteration 240/1000 | Loss: 0.00003331
Iteration 241/1000 | Loss: 0.00003330
Iteration 242/1000 | Loss: 0.00003330
Iteration 243/1000 | Loss: 0.00003330
Iteration 244/1000 | Loss: 0.00003330
Iteration 245/1000 | Loss: 0.00003330
Iteration 246/1000 | Loss: 0.00003330
Iteration 247/1000 | Loss: 0.00003330
Iteration 248/1000 | Loss: 0.00003330
Iteration 249/1000 | Loss: 0.00003330
Iteration 250/1000 | Loss: 0.00003330
Iteration 251/1000 | Loss: 0.00003330
Iteration 252/1000 | Loss: 0.00003330
Iteration 253/1000 | Loss: 0.00003330
Iteration 254/1000 | Loss: 0.00003330
Iteration 255/1000 | Loss: 0.00003330
Iteration 256/1000 | Loss: 0.00003330
Iteration 257/1000 | Loss: 0.00003330
Iteration 258/1000 | Loss: 0.00003329
Iteration 259/1000 | Loss: 0.00003329
Iteration 260/1000 | Loss: 0.00003329
Iteration 261/1000 | Loss: 0.00003329
Iteration 262/1000 | Loss: 0.00003329
Iteration 263/1000 | Loss: 0.00003329
Iteration 264/1000 | Loss: 0.00003329
Iteration 265/1000 | Loss: 0.00003329
Iteration 266/1000 | Loss: 0.00003329
Iteration 267/1000 | Loss: 0.00003329
Iteration 268/1000 | Loss: 0.00003329
Iteration 269/1000 | Loss: 0.00003329
Iteration 270/1000 | Loss: 0.00003329
Iteration 271/1000 | Loss: 0.00003329
Iteration 272/1000 | Loss: 0.00003329
Iteration 273/1000 | Loss: 0.00003329
Iteration 274/1000 | Loss: 0.00003329
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 274. Stopping optimization.
Last 5 losses: [3.32923409587238e-05, 3.32923409587238e-05, 3.32923409587238e-05, 3.32923409587238e-05, 3.32923409587238e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.32923409587238e-05

Optimization complete. Final v2v error: 4.764121055603027 mm

Highest mean error: 6.096587181091309 mm for frame 119

Lowest mean error: 3.9503540992736816 mm for frame 1

Saving results

Total time: 61.43197321891785
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01048859
Iteration 2/25 | Loss: 0.00246270
Iteration 3/25 | Loss: 0.00188906
Iteration 4/25 | Loss: 0.00163647
Iteration 5/25 | Loss: 0.00132272
Iteration 6/25 | Loss: 0.00113440
Iteration 7/25 | Loss: 0.00088419
Iteration 8/25 | Loss: 0.00086606
Iteration 9/25 | Loss: 0.00088375
Iteration 10/25 | Loss: 0.00083401
Iteration 11/25 | Loss: 0.00084070
Iteration 12/25 | Loss: 0.00081435
Iteration 13/25 | Loss: 0.00081366
Iteration 14/25 | Loss: 0.00081356
Iteration 15/25 | Loss: 0.00081356
Iteration 16/25 | Loss: 0.00081356
Iteration 17/25 | Loss: 0.00081355
Iteration 18/25 | Loss: 0.00081355
Iteration 19/25 | Loss: 0.00081355
Iteration 20/25 | Loss: 0.00081355
Iteration 21/25 | Loss: 0.00081355
Iteration 22/25 | Loss: 0.00081355
Iteration 23/25 | Loss: 0.00081355
Iteration 24/25 | Loss: 0.00081355
Iteration 25/25 | Loss: 0.00081355

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46860480
Iteration 2/25 | Loss: 0.00051090
Iteration 3/25 | Loss: 0.00051090
Iteration 4/25 | Loss: 0.00051089
Iteration 5/25 | Loss: 0.00051089
Iteration 6/25 | Loss: 0.00051089
Iteration 7/25 | Loss: 0.00051089
Iteration 8/25 | Loss: 0.00051089
Iteration 9/25 | Loss: 0.00051089
Iteration 10/25 | Loss: 0.00051089
Iteration 11/25 | Loss: 0.00051089
Iteration 12/25 | Loss: 0.00051089
Iteration 13/25 | Loss: 0.00051089
Iteration 14/25 | Loss: 0.00051089
Iteration 15/25 | Loss: 0.00051089
Iteration 16/25 | Loss: 0.00051089
Iteration 17/25 | Loss: 0.00051089
Iteration 18/25 | Loss: 0.00051089
Iteration 19/25 | Loss: 0.00051089
Iteration 20/25 | Loss: 0.00051089
Iteration 21/25 | Loss: 0.00051089
Iteration 22/25 | Loss: 0.00051089
Iteration 23/25 | Loss: 0.00051089
Iteration 24/25 | Loss: 0.00051089
Iteration 25/25 | Loss: 0.00051089

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051089
Iteration 2/1000 | Loss: 0.00002857
Iteration 3/1000 | Loss: 0.00002083
Iteration 4/1000 | Loss: 0.00001924
Iteration 5/1000 | Loss: 0.00001857
Iteration 6/1000 | Loss: 0.00001788
Iteration 7/1000 | Loss: 0.00001741
Iteration 8/1000 | Loss: 0.00001710
Iteration 9/1000 | Loss: 0.00001692
Iteration 10/1000 | Loss: 0.00001691
Iteration 11/1000 | Loss: 0.00001688
Iteration 12/1000 | Loss: 0.00001676
Iteration 13/1000 | Loss: 0.00001669
Iteration 14/1000 | Loss: 0.00001666
Iteration 15/1000 | Loss: 0.00001665
Iteration 16/1000 | Loss: 0.00001665
Iteration 17/1000 | Loss: 0.00001665
Iteration 18/1000 | Loss: 0.00001664
Iteration 19/1000 | Loss: 0.00001664
Iteration 20/1000 | Loss: 0.00001663
Iteration 21/1000 | Loss: 0.00001663
Iteration 22/1000 | Loss: 0.00001663
Iteration 23/1000 | Loss: 0.00001660
Iteration 24/1000 | Loss: 0.00001655
Iteration 25/1000 | Loss: 0.00001655
Iteration 26/1000 | Loss: 0.00001654
Iteration 27/1000 | Loss: 0.00001654
Iteration 28/1000 | Loss: 0.00001654
Iteration 29/1000 | Loss: 0.00001653
Iteration 30/1000 | Loss: 0.00001653
Iteration 31/1000 | Loss: 0.00001653
Iteration 32/1000 | Loss: 0.00001652
Iteration 33/1000 | Loss: 0.00001652
Iteration 34/1000 | Loss: 0.00001652
Iteration 35/1000 | Loss: 0.00001652
Iteration 36/1000 | Loss: 0.00001652
Iteration 37/1000 | Loss: 0.00001651
Iteration 38/1000 | Loss: 0.00001651
Iteration 39/1000 | Loss: 0.00001651
Iteration 40/1000 | Loss: 0.00001651
Iteration 41/1000 | Loss: 0.00001650
Iteration 42/1000 | Loss: 0.00001650
Iteration 43/1000 | Loss: 0.00001649
Iteration 44/1000 | Loss: 0.00001649
Iteration 45/1000 | Loss: 0.00001649
Iteration 46/1000 | Loss: 0.00001648
Iteration 47/1000 | Loss: 0.00001648
Iteration 48/1000 | Loss: 0.00001647
Iteration 49/1000 | Loss: 0.00001647
Iteration 50/1000 | Loss: 0.00001646
Iteration 51/1000 | Loss: 0.00001646
Iteration 52/1000 | Loss: 0.00001646
Iteration 53/1000 | Loss: 0.00001646
Iteration 54/1000 | Loss: 0.00001646
Iteration 55/1000 | Loss: 0.00001646
Iteration 56/1000 | Loss: 0.00001646
Iteration 57/1000 | Loss: 0.00001645
Iteration 58/1000 | Loss: 0.00001645
Iteration 59/1000 | Loss: 0.00001645
Iteration 60/1000 | Loss: 0.00001645
Iteration 61/1000 | Loss: 0.00001645
Iteration 62/1000 | Loss: 0.00001645
Iteration 63/1000 | Loss: 0.00001645
Iteration 64/1000 | Loss: 0.00001645
Iteration 65/1000 | Loss: 0.00001645
Iteration 66/1000 | Loss: 0.00001645
Iteration 67/1000 | Loss: 0.00001645
Iteration 68/1000 | Loss: 0.00001644
Iteration 69/1000 | Loss: 0.00001644
Iteration 70/1000 | Loss: 0.00001644
Iteration 71/1000 | Loss: 0.00001643
Iteration 72/1000 | Loss: 0.00001643
Iteration 73/1000 | Loss: 0.00001643
Iteration 74/1000 | Loss: 0.00001643
Iteration 75/1000 | Loss: 0.00001643
Iteration 76/1000 | Loss: 0.00001643
Iteration 77/1000 | Loss: 0.00001643
Iteration 78/1000 | Loss: 0.00001643
Iteration 79/1000 | Loss: 0.00001643
Iteration 80/1000 | Loss: 0.00001643
Iteration 81/1000 | Loss: 0.00001643
Iteration 82/1000 | Loss: 0.00001643
Iteration 83/1000 | Loss: 0.00001643
Iteration 84/1000 | Loss: 0.00001643
Iteration 85/1000 | Loss: 0.00001642
Iteration 86/1000 | Loss: 0.00001642
Iteration 87/1000 | Loss: 0.00001642
Iteration 88/1000 | Loss: 0.00001642
Iteration 89/1000 | Loss: 0.00001642
Iteration 90/1000 | Loss: 0.00001642
Iteration 91/1000 | Loss: 0.00001641
Iteration 92/1000 | Loss: 0.00001641
Iteration 93/1000 | Loss: 0.00001641
Iteration 94/1000 | Loss: 0.00001641
Iteration 95/1000 | Loss: 0.00001641
Iteration 96/1000 | Loss: 0.00001641
Iteration 97/1000 | Loss: 0.00001641
Iteration 98/1000 | Loss: 0.00001641
Iteration 99/1000 | Loss: 0.00001641
Iteration 100/1000 | Loss: 0.00001640
Iteration 101/1000 | Loss: 0.00001640
Iteration 102/1000 | Loss: 0.00001640
Iteration 103/1000 | Loss: 0.00001640
Iteration 104/1000 | Loss: 0.00001640
Iteration 105/1000 | Loss: 0.00001640
Iteration 106/1000 | Loss: 0.00001640
Iteration 107/1000 | Loss: 0.00001640
Iteration 108/1000 | Loss: 0.00001640
Iteration 109/1000 | Loss: 0.00001640
Iteration 110/1000 | Loss: 0.00001640
Iteration 111/1000 | Loss: 0.00001640
Iteration 112/1000 | Loss: 0.00001640
Iteration 113/1000 | Loss: 0.00001640
Iteration 114/1000 | Loss: 0.00001640
Iteration 115/1000 | Loss: 0.00001639
Iteration 116/1000 | Loss: 0.00001639
Iteration 117/1000 | Loss: 0.00001639
Iteration 118/1000 | Loss: 0.00001639
Iteration 119/1000 | Loss: 0.00001639
Iteration 120/1000 | Loss: 0.00001639
Iteration 121/1000 | Loss: 0.00001639
Iteration 122/1000 | Loss: 0.00001639
Iteration 123/1000 | Loss: 0.00001639
Iteration 124/1000 | Loss: 0.00001639
Iteration 125/1000 | Loss: 0.00001639
Iteration 126/1000 | Loss: 0.00001639
Iteration 127/1000 | Loss: 0.00001639
Iteration 128/1000 | Loss: 0.00001639
Iteration 129/1000 | Loss: 0.00001639
Iteration 130/1000 | Loss: 0.00001639
Iteration 131/1000 | Loss: 0.00001639
Iteration 132/1000 | Loss: 0.00001639
Iteration 133/1000 | Loss: 0.00001639
Iteration 134/1000 | Loss: 0.00001639
Iteration 135/1000 | Loss: 0.00001639
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [1.6387708456022665e-05, 1.6387708456022665e-05, 1.6387708456022665e-05, 1.6387708456022665e-05, 1.6387708456022665e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6387708456022665e-05

Optimization complete. Final v2v error: 3.3232693672180176 mm

Highest mean error: 3.501626968383789 mm for frame 12

Lowest mean error: 3.265265703201294 mm for frame 26

Saving results

Total time: 47.606614112854004
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01001205
Iteration 2/25 | Loss: 0.00464798
Iteration 3/25 | Loss: 0.00297246
Iteration 4/25 | Loss: 0.00239635
Iteration 5/25 | Loss: 0.00211609
Iteration 6/25 | Loss: 0.00192171
Iteration 7/25 | Loss: 0.00192140
Iteration 8/25 | Loss: 0.00184796
Iteration 9/25 | Loss: 0.00178376
Iteration 10/25 | Loss: 0.00166688
Iteration 11/25 | Loss: 0.00158837
Iteration 12/25 | Loss: 0.00158497
Iteration 13/25 | Loss: 0.00152968
Iteration 14/25 | Loss: 0.00145800
Iteration 15/25 | Loss: 0.00144210
Iteration 16/25 | Loss: 0.00140316
Iteration 17/25 | Loss: 0.00139448
Iteration 18/25 | Loss: 0.00138824
Iteration 19/25 | Loss: 0.00138937
Iteration 20/25 | Loss: 0.00135962
Iteration 21/25 | Loss: 0.00134014
Iteration 22/25 | Loss: 0.00134103
Iteration 23/25 | Loss: 0.00133475
Iteration 24/25 | Loss: 0.00133648
Iteration 25/25 | Loss: 0.00131214

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59259343
Iteration 2/25 | Loss: 0.00868069
Iteration 3/25 | Loss: 0.00568131
Iteration 4/25 | Loss: 0.00568130
Iteration 5/25 | Loss: 0.00568130
Iteration 6/25 | Loss: 0.00568130
Iteration 7/25 | Loss: 0.00568130
Iteration 8/25 | Loss: 0.00568130
Iteration 9/25 | Loss: 0.00568130
Iteration 10/25 | Loss: 0.00568130
Iteration 11/25 | Loss: 0.00568130
Iteration 12/25 | Loss: 0.00568130
Iteration 13/25 | Loss: 0.00568130
Iteration 14/25 | Loss: 0.00568130
Iteration 15/25 | Loss: 0.00568130
Iteration 16/25 | Loss: 0.00568130
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.005681298673152924, 0.005681298673152924, 0.005681298673152924, 0.005681298673152924, 0.005681298673152924]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005681298673152924

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00568130
Iteration 2/1000 | Loss: 0.00420183
Iteration 3/1000 | Loss: 0.00727396
Iteration 4/1000 | Loss: 0.00263156
Iteration 5/1000 | Loss: 0.00179503
Iteration 6/1000 | Loss: 0.00150679
Iteration 7/1000 | Loss: 0.00341166
Iteration 8/1000 | Loss: 0.00112328
Iteration 9/1000 | Loss: 0.00295159
Iteration 10/1000 | Loss: 0.00137319
Iteration 11/1000 | Loss: 0.00201179
Iteration 12/1000 | Loss: 0.00108265
Iteration 13/1000 | Loss: 0.00212876
Iteration 14/1000 | Loss: 0.00116582
Iteration 15/1000 | Loss: 0.00048590
Iteration 16/1000 | Loss: 0.00101727
Iteration 17/1000 | Loss: 0.00096585
Iteration 18/1000 | Loss: 0.00036470
Iteration 19/1000 | Loss: 0.00124639
Iteration 20/1000 | Loss: 0.00046045
Iteration 21/1000 | Loss: 0.00047967
Iteration 22/1000 | Loss: 0.00060393
Iteration 23/1000 | Loss: 0.00040888
Iteration 24/1000 | Loss: 0.00050649
Iteration 25/1000 | Loss: 0.00177160
Iteration 26/1000 | Loss: 0.00207554
Iteration 27/1000 | Loss: 0.00048677
Iteration 28/1000 | Loss: 0.00013783
Iteration 29/1000 | Loss: 0.00039889
Iteration 30/1000 | Loss: 0.00014015
Iteration 31/1000 | Loss: 0.00094077
Iteration 32/1000 | Loss: 0.00051166
Iteration 33/1000 | Loss: 0.00024886
Iteration 34/1000 | Loss: 0.00012764
Iteration 35/1000 | Loss: 0.00014752
Iteration 36/1000 | Loss: 0.00102975
Iteration 37/1000 | Loss: 0.00046067
Iteration 38/1000 | Loss: 0.00018576
Iteration 39/1000 | Loss: 0.00060127
Iteration 40/1000 | Loss: 0.00012378
Iteration 41/1000 | Loss: 0.00055781
Iteration 42/1000 | Loss: 0.00131157
Iteration 43/1000 | Loss: 0.00107216
Iteration 44/1000 | Loss: 0.00074690
Iteration 45/1000 | Loss: 0.00022335
Iteration 46/1000 | Loss: 0.00106591
Iteration 47/1000 | Loss: 0.00198605
Iteration 48/1000 | Loss: 0.00012507
Iteration 49/1000 | Loss: 0.00010081
Iteration 50/1000 | Loss: 0.00010950
Iteration 51/1000 | Loss: 0.00104132
Iteration 52/1000 | Loss: 0.00054753
Iteration 53/1000 | Loss: 0.00066817
Iteration 54/1000 | Loss: 0.00012955
Iteration 55/1000 | Loss: 0.00010017
Iteration 56/1000 | Loss: 0.00009987
Iteration 57/1000 | Loss: 0.00017227
Iteration 58/1000 | Loss: 0.00052431
Iteration 59/1000 | Loss: 0.00088476
Iteration 60/1000 | Loss: 0.00044033
Iteration 61/1000 | Loss: 0.00031882
Iteration 62/1000 | Loss: 0.00008835
Iteration 63/1000 | Loss: 0.00046652
Iteration 64/1000 | Loss: 0.00045252
Iteration 65/1000 | Loss: 0.00025200
Iteration 66/1000 | Loss: 0.00021357
Iteration 67/1000 | Loss: 0.00008254
Iteration 68/1000 | Loss: 0.00007560
Iteration 69/1000 | Loss: 0.00006981
Iteration 70/1000 | Loss: 0.00015377
Iteration 71/1000 | Loss: 0.00007491
Iteration 72/1000 | Loss: 0.00008391
Iteration 73/1000 | Loss: 0.00006217
Iteration 74/1000 | Loss: 0.00006686
Iteration 75/1000 | Loss: 0.00006044
Iteration 76/1000 | Loss: 0.00036466
Iteration 77/1000 | Loss: 0.00032417
Iteration 78/1000 | Loss: 0.00023711
Iteration 79/1000 | Loss: 0.00011818
Iteration 80/1000 | Loss: 0.00011006
Iteration 81/1000 | Loss: 0.00009506
Iteration 82/1000 | Loss: 0.00007115
Iteration 83/1000 | Loss: 0.00009397
Iteration 84/1000 | Loss: 0.00032941
Iteration 85/1000 | Loss: 0.00027797
Iteration 86/1000 | Loss: 0.00036499
Iteration 87/1000 | Loss: 0.00027760
Iteration 88/1000 | Loss: 0.00060986
Iteration 89/1000 | Loss: 0.00009482
Iteration 90/1000 | Loss: 0.00015437
Iteration 91/1000 | Loss: 0.00006055
Iteration 92/1000 | Loss: 0.00005624
Iteration 93/1000 | Loss: 0.00005351
Iteration 94/1000 | Loss: 0.00025271
Iteration 95/1000 | Loss: 0.00030777
Iteration 96/1000 | Loss: 0.00024170
Iteration 97/1000 | Loss: 0.00028737
Iteration 98/1000 | Loss: 0.00059548
Iteration 99/1000 | Loss: 0.00013054
Iteration 100/1000 | Loss: 0.00010058
Iteration 101/1000 | Loss: 0.00005397
Iteration 102/1000 | Loss: 0.00005146
Iteration 103/1000 | Loss: 0.00004935
Iteration 104/1000 | Loss: 0.00004810
Iteration 105/1000 | Loss: 0.00004744
Iteration 106/1000 | Loss: 0.00004663
Iteration 107/1000 | Loss: 0.00004602
Iteration 108/1000 | Loss: 0.00004552
Iteration 109/1000 | Loss: 0.00004536
Iteration 110/1000 | Loss: 0.00022622
Iteration 111/1000 | Loss: 0.00068780
Iteration 112/1000 | Loss: 0.00079484
Iteration 113/1000 | Loss: 0.00005725
Iteration 114/1000 | Loss: 0.00005021
Iteration 115/1000 | Loss: 0.00004763
Iteration 116/1000 | Loss: 0.00021946
Iteration 117/1000 | Loss: 0.00036475
Iteration 118/1000 | Loss: 0.00016798
Iteration 119/1000 | Loss: 0.00004904
Iteration 120/1000 | Loss: 0.00004543
Iteration 121/1000 | Loss: 0.00004345
Iteration 122/1000 | Loss: 0.00005533
Iteration 123/1000 | Loss: 0.00004725
Iteration 124/1000 | Loss: 0.00004129
Iteration 125/1000 | Loss: 0.00032729
Iteration 126/1000 | Loss: 0.00035023
Iteration 127/1000 | Loss: 0.00033232
Iteration 128/1000 | Loss: 0.00047411
Iteration 129/1000 | Loss: 0.00005699
Iteration 130/1000 | Loss: 0.00004427
Iteration 131/1000 | Loss: 0.00007127
Iteration 132/1000 | Loss: 0.00004156
Iteration 133/1000 | Loss: 0.00004057
Iteration 134/1000 | Loss: 0.00003967
Iteration 135/1000 | Loss: 0.00003932
Iteration 136/1000 | Loss: 0.00003918
Iteration 137/1000 | Loss: 0.00022478
Iteration 138/1000 | Loss: 0.00005772
Iteration 139/1000 | Loss: 0.00005305
Iteration 140/1000 | Loss: 0.00003895
Iteration 141/1000 | Loss: 0.00003882
Iteration 142/1000 | Loss: 0.00003879
Iteration 143/1000 | Loss: 0.00003879
Iteration 144/1000 | Loss: 0.00003879
Iteration 145/1000 | Loss: 0.00003879
Iteration 146/1000 | Loss: 0.00003879
Iteration 147/1000 | Loss: 0.00003879
Iteration 148/1000 | Loss: 0.00003879
Iteration 149/1000 | Loss: 0.00003878
Iteration 150/1000 | Loss: 0.00003876
Iteration 151/1000 | Loss: 0.00003863
Iteration 152/1000 | Loss: 0.00003862
Iteration 153/1000 | Loss: 0.00003861
Iteration 154/1000 | Loss: 0.00003861
Iteration 155/1000 | Loss: 0.00003860
Iteration 156/1000 | Loss: 0.00003860
Iteration 157/1000 | Loss: 0.00003859
Iteration 158/1000 | Loss: 0.00003859
Iteration 159/1000 | Loss: 0.00003859
Iteration 160/1000 | Loss: 0.00003859
Iteration 161/1000 | Loss: 0.00003858
Iteration 162/1000 | Loss: 0.00003858
Iteration 163/1000 | Loss: 0.00003858
Iteration 164/1000 | Loss: 0.00003858
Iteration 165/1000 | Loss: 0.00003858
Iteration 166/1000 | Loss: 0.00003858
Iteration 167/1000 | Loss: 0.00003857
Iteration 168/1000 | Loss: 0.00003857
Iteration 169/1000 | Loss: 0.00003856
Iteration 170/1000 | Loss: 0.00003856
Iteration 171/1000 | Loss: 0.00003855
Iteration 172/1000 | Loss: 0.00003855
Iteration 173/1000 | Loss: 0.00003855
Iteration 174/1000 | Loss: 0.00003855
Iteration 175/1000 | Loss: 0.00003855
Iteration 176/1000 | Loss: 0.00003855
Iteration 177/1000 | Loss: 0.00003855
Iteration 178/1000 | Loss: 0.00003855
Iteration 179/1000 | Loss: 0.00003854
Iteration 180/1000 | Loss: 0.00003854
Iteration 181/1000 | Loss: 0.00003854
Iteration 182/1000 | Loss: 0.00003853
Iteration 183/1000 | Loss: 0.00003853
Iteration 184/1000 | Loss: 0.00003853
Iteration 185/1000 | Loss: 0.00003853
Iteration 186/1000 | Loss: 0.00003853
Iteration 187/1000 | Loss: 0.00003853
Iteration 188/1000 | Loss: 0.00003853
Iteration 189/1000 | Loss: 0.00003853
Iteration 190/1000 | Loss: 0.00003853
Iteration 191/1000 | Loss: 0.00003853
Iteration 192/1000 | Loss: 0.00003853
Iteration 193/1000 | Loss: 0.00003853
Iteration 194/1000 | Loss: 0.00003853
Iteration 195/1000 | Loss: 0.00003853
Iteration 196/1000 | Loss: 0.00003853
Iteration 197/1000 | Loss: 0.00003853
Iteration 198/1000 | Loss: 0.00003853
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [3.852979716612026e-05, 3.852979716612026e-05, 3.852979716612026e-05, 3.852979716612026e-05, 3.852979716612026e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.852979716612026e-05

Optimization complete. Final v2v error: 3.844106912612915 mm

Highest mean error: 11.932579040527344 mm for frame 109

Lowest mean error: 3.065427303314209 mm for frame 172

Saving results

Total time: 286.1950945854187
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00400141
Iteration 2/25 | Loss: 0.00088488
Iteration 3/25 | Loss: 0.00077813
Iteration 4/25 | Loss: 0.00076504
Iteration 5/25 | Loss: 0.00075991
Iteration 6/25 | Loss: 0.00075942
Iteration 7/25 | Loss: 0.00075942
Iteration 8/25 | Loss: 0.00075942
Iteration 9/25 | Loss: 0.00075942
Iteration 10/25 | Loss: 0.00075942
Iteration 11/25 | Loss: 0.00075942
Iteration 12/25 | Loss: 0.00075942
Iteration 13/25 | Loss: 0.00075942
Iteration 14/25 | Loss: 0.00075942
Iteration 15/25 | Loss: 0.00075942
Iteration 16/25 | Loss: 0.00075942
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007594234775751829, 0.0007594234775751829, 0.0007594234775751829, 0.0007594234775751829, 0.0007594234775751829]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007594234775751829

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.76609850
Iteration 2/25 | Loss: 0.00055770
Iteration 3/25 | Loss: 0.00055770
Iteration 4/25 | Loss: 0.00055770
Iteration 5/25 | Loss: 0.00055770
Iteration 6/25 | Loss: 0.00055770
Iteration 7/25 | Loss: 0.00055770
Iteration 8/25 | Loss: 0.00055770
Iteration 9/25 | Loss: 0.00055770
Iteration 10/25 | Loss: 0.00055770
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0005576959229074419, 0.0005576959229074419, 0.0005576959229074419, 0.0005576959229074419, 0.0005576959229074419]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005576959229074419

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055770
Iteration 2/1000 | Loss: 0.00002625
Iteration 3/1000 | Loss: 0.00001832
Iteration 4/1000 | Loss: 0.00001538
Iteration 5/1000 | Loss: 0.00001448
Iteration 6/1000 | Loss: 0.00001370
Iteration 7/1000 | Loss: 0.00001334
Iteration 8/1000 | Loss: 0.00001310
Iteration 9/1000 | Loss: 0.00001286
Iteration 10/1000 | Loss: 0.00001285
Iteration 11/1000 | Loss: 0.00001281
Iteration 12/1000 | Loss: 0.00001273
Iteration 13/1000 | Loss: 0.00001268
Iteration 14/1000 | Loss: 0.00001267
Iteration 15/1000 | Loss: 0.00001266
Iteration 16/1000 | Loss: 0.00001265
Iteration 17/1000 | Loss: 0.00001264
Iteration 18/1000 | Loss: 0.00001264
Iteration 19/1000 | Loss: 0.00001259
Iteration 20/1000 | Loss: 0.00001255
Iteration 21/1000 | Loss: 0.00001255
Iteration 22/1000 | Loss: 0.00001255
Iteration 23/1000 | Loss: 0.00001254
Iteration 24/1000 | Loss: 0.00001254
Iteration 25/1000 | Loss: 0.00001254
Iteration 26/1000 | Loss: 0.00001253
Iteration 27/1000 | Loss: 0.00001250
Iteration 28/1000 | Loss: 0.00001250
Iteration 29/1000 | Loss: 0.00001250
Iteration 30/1000 | Loss: 0.00001249
Iteration 31/1000 | Loss: 0.00001249
Iteration 32/1000 | Loss: 0.00001249
Iteration 33/1000 | Loss: 0.00001248
Iteration 34/1000 | Loss: 0.00001247
Iteration 35/1000 | Loss: 0.00001247
Iteration 36/1000 | Loss: 0.00001243
Iteration 37/1000 | Loss: 0.00001243
Iteration 38/1000 | Loss: 0.00001242
Iteration 39/1000 | Loss: 0.00001240
Iteration 40/1000 | Loss: 0.00001240
Iteration 41/1000 | Loss: 0.00001240
Iteration 42/1000 | Loss: 0.00001239
Iteration 43/1000 | Loss: 0.00001239
Iteration 44/1000 | Loss: 0.00001239
Iteration 45/1000 | Loss: 0.00001239
Iteration 46/1000 | Loss: 0.00001239
Iteration 47/1000 | Loss: 0.00001239
Iteration 48/1000 | Loss: 0.00001239
Iteration 49/1000 | Loss: 0.00001239
Iteration 50/1000 | Loss: 0.00001239
Iteration 51/1000 | Loss: 0.00001239
Iteration 52/1000 | Loss: 0.00001238
Iteration 53/1000 | Loss: 0.00001238
Iteration 54/1000 | Loss: 0.00001238
Iteration 55/1000 | Loss: 0.00001236
Iteration 56/1000 | Loss: 0.00001236
Iteration 57/1000 | Loss: 0.00001235
Iteration 58/1000 | Loss: 0.00001235
Iteration 59/1000 | Loss: 0.00001235
Iteration 60/1000 | Loss: 0.00001235
Iteration 61/1000 | Loss: 0.00001235
Iteration 62/1000 | Loss: 0.00001235
Iteration 63/1000 | Loss: 0.00001235
Iteration 64/1000 | Loss: 0.00001235
Iteration 65/1000 | Loss: 0.00001235
Iteration 66/1000 | Loss: 0.00001235
Iteration 67/1000 | Loss: 0.00001235
Iteration 68/1000 | Loss: 0.00001235
Iteration 69/1000 | Loss: 0.00001234
Iteration 70/1000 | Loss: 0.00001234
Iteration 71/1000 | Loss: 0.00001234
Iteration 72/1000 | Loss: 0.00001234
Iteration 73/1000 | Loss: 0.00001234
Iteration 74/1000 | Loss: 0.00001233
Iteration 75/1000 | Loss: 0.00001233
Iteration 76/1000 | Loss: 0.00001232
Iteration 77/1000 | Loss: 0.00001232
Iteration 78/1000 | Loss: 0.00001232
Iteration 79/1000 | Loss: 0.00001232
Iteration 80/1000 | Loss: 0.00001231
Iteration 81/1000 | Loss: 0.00001231
Iteration 82/1000 | Loss: 0.00001231
Iteration 83/1000 | Loss: 0.00001231
Iteration 84/1000 | Loss: 0.00001231
Iteration 85/1000 | Loss: 0.00001231
Iteration 86/1000 | Loss: 0.00001230
Iteration 87/1000 | Loss: 0.00001230
Iteration 88/1000 | Loss: 0.00001230
Iteration 89/1000 | Loss: 0.00001230
Iteration 90/1000 | Loss: 0.00001230
Iteration 91/1000 | Loss: 0.00001230
Iteration 92/1000 | Loss: 0.00001230
Iteration 93/1000 | Loss: 0.00001230
Iteration 94/1000 | Loss: 0.00001229
Iteration 95/1000 | Loss: 0.00001229
Iteration 96/1000 | Loss: 0.00001229
Iteration 97/1000 | Loss: 0.00001229
Iteration 98/1000 | Loss: 0.00001229
Iteration 99/1000 | Loss: 0.00001229
Iteration 100/1000 | Loss: 0.00001229
Iteration 101/1000 | Loss: 0.00001229
Iteration 102/1000 | Loss: 0.00001229
Iteration 103/1000 | Loss: 0.00001229
Iteration 104/1000 | Loss: 0.00001229
Iteration 105/1000 | Loss: 0.00001229
Iteration 106/1000 | Loss: 0.00001229
Iteration 107/1000 | Loss: 0.00001229
Iteration 108/1000 | Loss: 0.00001229
Iteration 109/1000 | Loss: 0.00001229
Iteration 110/1000 | Loss: 0.00001229
Iteration 111/1000 | Loss: 0.00001229
Iteration 112/1000 | Loss: 0.00001229
Iteration 113/1000 | Loss: 0.00001229
Iteration 114/1000 | Loss: 0.00001228
Iteration 115/1000 | Loss: 0.00001228
Iteration 116/1000 | Loss: 0.00001228
Iteration 117/1000 | Loss: 0.00001228
Iteration 118/1000 | Loss: 0.00001228
Iteration 119/1000 | Loss: 0.00001228
Iteration 120/1000 | Loss: 0.00001228
Iteration 121/1000 | Loss: 0.00001228
Iteration 122/1000 | Loss: 0.00001228
Iteration 123/1000 | Loss: 0.00001228
Iteration 124/1000 | Loss: 0.00001228
Iteration 125/1000 | Loss: 0.00001228
Iteration 126/1000 | Loss: 0.00001228
Iteration 127/1000 | Loss: 0.00001228
Iteration 128/1000 | Loss: 0.00001228
Iteration 129/1000 | Loss: 0.00001228
Iteration 130/1000 | Loss: 0.00001227
Iteration 131/1000 | Loss: 0.00001227
Iteration 132/1000 | Loss: 0.00001227
Iteration 133/1000 | Loss: 0.00001227
Iteration 134/1000 | Loss: 0.00001227
Iteration 135/1000 | Loss: 0.00001227
Iteration 136/1000 | Loss: 0.00001227
Iteration 137/1000 | Loss: 0.00001227
Iteration 138/1000 | Loss: 0.00001227
Iteration 139/1000 | Loss: 0.00001227
Iteration 140/1000 | Loss: 0.00001227
Iteration 141/1000 | Loss: 0.00001227
Iteration 142/1000 | Loss: 0.00001227
Iteration 143/1000 | Loss: 0.00001227
Iteration 144/1000 | Loss: 0.00001227
Iteration 145/1000 | Loss: 0.00001227
Iteration 146/1000 | Loss: 0.00001227
Iteration 147/1000 | Loss: 0.00001227
Iteration 148/1000 | Loss: 0.00001227
Iteration 149/1000 | Loss: 0.00001227
Iteration 150/1000 | Loss: 0.00001227
Iteration 151/1000 | Loss: 0.00001227
Iteration 152/1000 | Loss: 0.00001227
Iteration 153/1000 | Loss: 0.00001227
Iteration 154/1000 | Loss: 0.00001227
Iteration 155/1000 | Loss: 0.00001227
Iteration 156/1000 | Loss: 0.00001227
Iteration 157/1000 | Loss: 0.00001227
Iteration 158/1000 | Loss: 0.00001227
Iteration 159/1000 | Loss: 0.00001227
Iteration 160/1000 | Loss: 0.00001227
Iteration 161/1000 | Loss: 0.00001227
Iteration 162/1000 | Loss: 0.00001227
Iteration 163/1000 | Loss: 0.00001227
Iteration 164/1000 | Loss: 0.00001227
Iteration 165/1000 | Loss: 0.00001227
Iteration 166/1000 | Loss: 0.00001227
Iteration 167/1000 | Loss: 0.00001227
Iteration 168/1000 | Loss: 0.00001227
Iteration 169/1000 | Loss: 0.00001227
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.2270967090444174e-05, 1.2270967090444174e-05, 1.2270967090444174e-05, 1.2270967090444174e-05, 1.2270967090444174e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2270967090444174e-05

Optimization complete. Final v2v error: 2.9938271045684814 mm

Highest mean error: 3.3221566677093506 mm for frame 204

Lowest mean error: 2.791698932647705 mm for frame 112

Saving results

Total time: 37.04045224189758
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00699469
Iteration 2/25 | Loss: 0.00124597
Iteration 3/25 | Loss: 0.00099390
Iteration 4/25 | Loss: 0.00092975
Iteration 5/25 | Loss: 0.00089467
Iteration 6/25 | Loss: 0.00089546
Iteration 7/25 | Loss: 0.00087951
Iteration 8/25 | Loss: 0.00087040
Iteration 9/25 | Loss: 0.00087270
Iteration 10/25 | Loss: 0.00086764
Iteration 11/25 | Loss: 0.00089452
Iteration 12/25 | Loss: 0.00086310
Iteration 13/25 | Loss: 0.00086923
Iteration 14/25 | Loss: 0.00085738
Iteration 15/25 | Loss: 0.00085679
Iteration 16/25 | Loss: 0.00085672
Iteration 17/25 | Loss: 0.00085672
Iteration 18/25 | Loss: 0.00085672
Iteration 19/25 | Loss: 0.00085672
Iteration 20/25 | Loss: 0.00085672
Iteration 21/25 | Loss: 0.00085672
Iteration 22/25 | Loss: 0.00085672
Iteration 23/25 | Loss: 0.00085672
Iteration 24/25 | Loss: 0.00085672
Iteration 25/25 | Loss: 0.00085672

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54858673
Iteration 2/25 | Loss: 0.00066970
Iteration 3/25 | Loss: 0.00066969
Iteration 4/25 | Loss: 0.00066969
Iteration 5/25 | Loss: 0.00066969
Iteration 6/25 | Loss: 0.00066969
Iteration 7/25 | Loss: 0.00066969
Iteration 8/25 | Loss: 0.00066969
Iteration 9/25 | Loss: 0.00066969
Iteration 10/25 | Loss: 0.00066969
Iteration 11/25 | Loss: 0.00066969
Iteration 12/25 | Loss: 0.00066969
Iteration 13/25 | Loss: 0.00066969
Iteration 14/25 | Loss: 0.00066969
Iteration 15/25 | Loss: 0.00066969
Iteration 16/25 | Loss: 0.00066969
Iteration 17/25 | Loss: 0.00066969
Iteration 18/25 | Loss: 0.00066969
Iteration 19/25 | Loss: 0.00066969
Iteration 20/25 | Loss: 0.00066969
Iteration 21/25 | Loss: 0.00066969
Iteration 22/25 | Loss: 0.00066969
Iteration 23/25 | Loss: 0.00066969
Iteration 24/25 | Loss: 0.00066969
Iteration 25/25 | Loss: 0.00066969

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066969
Iteration 2/1000 | Loss: 0.00006657
Iteration 3/1000 | Loss: 0.00004170
Iteration 4/1000 | Loss: 0.00003175
Iteration 5/1000 | Loss: 0.00002860
Iteration 6/1000 | Loss: 0.00002761
Iteration 7/1000 | Loss: 0.00002687
Iteration 8/1000 | Loss: 0.00002637
Iteration 9/1000 | Loss: 0.00002606
Iteration 10/1000 | Loss: 0.00002568
Iteration 11/1000 | Loss: 0.00002536
Iteration 12/1000 | Loss: 0.00002514
Iteration 13/1000 | Loss: 0.00002491
Iteration 14/1000 | Loss: 0.00002474
Iteration 15/1000 | Loss: 0.00002463
Iteration 16/1000 | Loss: 0.00002462
Iteration 17/1000 | Loss: 0.00002462
Iteration 18/1000 | Loss: 0.00002460
Iteration 19/1000 | Loss: 0.00002456
Iteration 20/1000 | Loss: 0.00002455
Iteration 21/1000 | Loss: 0.00002454
Iteration 22/1000 | Loss: 0.00002454
Iteration 23/1000 | Loss: 0.00002453
Iteration 24/1000 | Loss: 0.00002452
Iteration 25/1000 | Loss: 0.00002452
Iteration 26/1000 | Loss: 0.00002451
Iteration 27/1000 | Loss: 0.00002450
Iteration 28/1000 | Loss: 0.00002450
Iteration 29/1000 | Loss: 0.00002448
Iteration 30/1000 | Loss: 0.00002447
Iteration 31/1000 | Loss: 0.00002447
Iteration 32/1000 | Loss: 0.00002447
Iteration 33/1000 | Loss: 0.00002446
Iteration 34/1000 | Loss: 0.00002445
Iteration 35/1000 | Loss: 0.00002445
Iteration 36/1000 | Loss: 0.00002445
Iteration 37/1000 | Loss: 0.00002445
Iteration 38/1000 | Loss: 0.00002444
Iteration 39/1000 | Loss: 0.00002444
Iteration 40/1000 | Loss: 0.00002444
Iteration 41/1000 | Loss: 0.00002444
Iteration 42/1000 | Loss: 0.00002443
Iteration 43/1000 | Loss: 0.00002443
Iteration 44/1000 | Loss: 0.00002443
Iteration 45/1000 | Loss: 0.00002443
Iteration 46/1000 | Loss: 0.00002442
Iteration 47/1000 | Loss: 0.00002442
Iteration 48/1000 | Loss: 0.00002442
Iteration 49/1000 | Loss: 0.00002441
Iteration 50/1000 | Loss: 0.00002441
Iteration 51/1000 | Loss: 0.00002441
Iteration 52/1000 | Loss: 0.00002440
Iteration 53/1000 | Loss: 0.00002440
Iteration 54/1000 | Loss: 0.00002440
Iteration 55/1000 | Loss: 0.00002440
Iteration 56/1000 | Loss: 0.00002440
Iteration 57/1000 | Loss: 0.00002440
Iteration 58/1000 | Loss: 0.00002440
Iteration 59/1000 | Loss: 0.00002440
Iteration 60/1000 | Loss: 0.00002439
Iteration 61/1000 | Loss: 0.00002439
Iteration 62/1000 | Loss: 0.00002439
Iteration 63/1000 | Loss: 0.00002439
Iteration 64/1000 | Loss: 0.00002439
Iteration 65/1000 | Loss: 0.00002438
Iteration 66/1000 | Loss: 0.00002438
Iteration 67/1000 | Loss: 0.00002438
Iteration 68/1000 | Loss: 0.00002438
Iteration 69/1000 | Loss: 0.00002438
Iteration 70/1000 | Loss: 0.00002438
Iteration 71/1000 | Loss: 0.00002438
Iteration 72/1000 | Loss: 0.00002438
Iteration 73/1000 | Loss: 0.00002437
Iteration 74/1000 | Loss: 0.00002437
Iteration 75/1000 | Loss: 0.00002437
Iteration 76/1000 | Loss: 0.00002437
Iteration 77/1000 | Loss: 0.00002437
Iteration 78/1000 | Loss: 0.00002437
Iteration 79/1000 | Loss: 0.00002437
Iteration 80/1000 | Loss: 0.00002437
Iteration 81/1000 | Loss: 0.00002437
Iteration 82/1000 | Loss: 0.00002437
Iteration 83/1000 | Loss: 0.00002437
Iteration 84/1000 | Loss: 0.00002437
Iteration 85/1000 | Loss: 0.00002437
Iteration 86/1000 | Loss: 0.00002436
Iteration 87/1000 | Loss: 0.00002436
Iteration 88/1000 | Loss: 0.00002436
Iteration 89/1000 | Loss: 0.00002436
Iteration 90/1000 | Loss: 0.00002436
Iteration 91/1000 | Loss: 0.00002436
Iteration 92/1000 | Loss: 0.00002436
Iteration 93/1000 | Loss: 0.00002436
Iteration 94/1000 | Loss: 0.00002436
Iteration 95/1000 | Loss: 0.00002436
Iteration 96/1000 | Loss: 0.00002436
Iteration 97/1000 | Loss: 0.00002436
Iteration 98/1000 | Loss: 0.00002436
Iteration 99/1000 | Loss: 0.00002435
Iteration 100/1000 | Loss: 0.00002435
Iteration 101/1000 | Loss: 0.00002435
Iteration 102/1000 | Loss: 0.00002435
Iteration 103/1000 | Loss: 0.00002435
Iteration 104/1000 | Loss: 0.00002435
Iteration 105/1000 | Loss: 0.00002435
Iteration 106/1000 | Loss: 0.00002435
Iteration 107/1000 | Loss: 0.00002435
Iteration 108/1000 | Loss: 0.00002435
Iteration 109/1000 | Loss: 0.00002435
Iteration 110/1000 | Loss: 0.00002435
Iteration 111/1000 | Loss: 0.00002435
Iteration 112/1000 | Loss: 0.00002435
Iteration 113/1000 | Loss: 0.00002435
Iteration 114/1000 | Loss: 0.00002434
Iteration 115/1000 | Loss: 0.00002434
Iteration 116/1000 | Loss: 0.00002434
Iteration 117/1000 | Loss: 0.00002434
Iteration 118/1000 | Loss: 0.00002434
Iteration 119/1000 | Loss: 0.00002434
Iteration 120/1000 | Loss: 0.00002434
Iteration 121/1000 | Loss: 0.00002434
Iteration 122/1000 | Loss: 0.00002434
Iteration 123/1000 | Loss: 0.00002434
Iteration 124/1000 | Loss: 0.00002434
Iteration 125/1000 | Loss: 0.00002434
Iteration 126/1000 | Loss: 0.00002434
Iteration 127/1000 | Loss: 0.00002434
Iteration 128/1000 | Loss: 0.00002434
Iteration 129/1000 | Loss: 0.00002434
Iteration 130/1000 | Loss: 0.00002433
Iteration 131/1000 | Loss: 0.00002433
Iteration 132/1000 | Loss: 0.00002433
Iteration 133/1000 | Loss: 0.00002433
Iteration 134/1000 | Loss: 0.00002433
Iteration 135/1000 | Loss: 0.00002433
Iteration 136/1000 | Loss: 0.00002433
Iteration 137/1000 | Loss: 0.00002433
Iteration 138/1000 | Loss: 0.00002433
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [2.4334998670383357e-05, 2.4334998670383357e-05, 2.4334998670383357e-05, 2.4334998670383357e-05, 2.4334998670383357e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4334998670383357e-05

Optimization complete. Final v2v error: 4.156495571136475 mm

Highest mean error: 4.68800163269043 mm for frame 122

Lowest mean error: 3.1216392517089844 mm for frame 180

Saving results

Total time: 59.390491247177124
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00938501
Iteration 2/25 | Loss: 0.00149093
Iteration 3/25 | Loss: 0.00108865
Iteration 4/25 | Loss: 0.00093528
Iteration 5/25 | Loss: 0.00090204
Iteration 6/25 | Loss: 0.00089442
Iteration 7/25 | Loss: 0.00088985
Iteration 8/25 | Loss: 0.00088673
Iteration 9/25 | Loss: 0.00088847
Iteration 10/25 | Loss: 0.00088780
Iteration 11/25 | Loss: 0.00089110
Iteration 12/25 | Loss: 0.00087684
Iteration 13/25 | Loss: 0.00087502
Iteration 14/25 | Loss: 0.00087320
Iteration 15/25 | Loss: 0.00087301
Iteration 16/25 | Loss: 0.00087295
Iteration 17/25 | Loss: 0.00087295
Iteration 18/25 | Loss: 0.00087295
Iteration 19/25 | Loss: 0.00087295
Iteration 20/25 | Loss: 0.00087295
Iteration 21/25 | Loss: 0.00087295
Iteration 22/25 | Loss: 0.00087294
Iteration 23/25 | Loss: 0.00087294
Iteration 24/25 | Loss: 0.00087294
Iteration 25/25 | Loss: 0.00087294

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.47142339
Iteration 2/25 | Loss: 0.00059254
Iteration 3/25 | Loss: 0.00059253
Iteration 4/25 | Loss: 0.00059253
Iteration 5/25 | Loss: 0.00059253
Iteration 6/25 | Loss: 0.00059253
Iteration 7/25 | Loss: 0.00059253
Iteration 8/25 | Loss: 0.00059253
Iteration 9/25 | Loss: 0.00059253
Iteration 10/25 | Loss: 0.00059253
Iteration 11/25 | Loss: 0.00059253
Iteration 12/25 | Loss: 0.00059253
Iteration 13/25 | Loss: 0.00059253
Iteration 14/25 | Loss: 0.00059253
Iteration 15/25 | Loss: 0.00059253
Iteration 16/25 | Loss: 0.00059253
Iteration 17/25 | Loss: 0.00059253
Iteration 18/25 | Loss: 0.00059253
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005925304139964283, 0.0005925304139964283, 0.0005925304139964283, 0.0005925304139964283, 0.0005925304139964283]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005925304139964283

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059253
Iteration 2/1000 | Loss: 0.00003404
Iteration 3/1000 | Loss: 0.00002469
Iteration 4/1000 | Loss: 0.00002291
Iteration 5/1000 | Loss: 0.00002207
Iteration 6/1000 | Loss: 0.00002163
Iteration 7/1000 | Loss: 0.00002129
Iteration 8/1000 | Loss: 0.00002092
Iteration 9/1000 | Loss: 0.00002069
Iteration 10/1000 | Loss: 0.00002068
Iteration 11/1000 | Loss: 0.00002059
Iteration 12/1000 | Loss: 0.00002050
Iteration 13/1000 | Loss: 0.00002045
Iteration 14/1000 | Loss: 0.00002043
Iteration 15/1000 | Loss: 0.00002043
Iteration 16/1000 | Loss: 0.00002043
Iteration 17/1000 | Loss: 0.00004117
Iteration 18/1000 | Loss: 0.00002784
Iteration 19/1000 | Loss: 0.00002026
Iteration 20/1000 | Loss: 0.00002715
Iteration 21/1000 | Loss: 0.00002021
Iteration 22/1000 | Loss: 0.00002020
Iteration 23/1000 | Loss: 0.00002019
Iteration 24/1000 | Loss: 0.00002019
Iteration 25/1000 | Loss: 0.00002018
Iteration 26/1000 | Loss: 0.00002017
Iteration 27/1000 | Loss: 0.00002017
Iteration 28/1000 | Loss: 0.00002017
Iteration 29/1000 | Loss: 0.00002017
Iteration 30/1000 | Loss: 0.00002017
Iteration 31/1000 | Loss: 0.00002017
Iteration 32/1000 | Loss: 0.00002017
Iteration 33/1000 | Loss: 0.00002017
Iteration 34/1000 | Loss: 0.00002016
Iteration 35/1000 | Loss: 0.00002016
Iteration 36/1000 | Loss: 0.00002016
Iteration 37/1000 | Loss: 0.00002015
Iteration 38/1000 | Loss: 0.00002014
Iteration 39/1000 | Loss: 0.00002014
Iteration 40/1000 | Loss: 0.00002014
Iteration 41/1000 | Loss: 0.00002013
Iteration 42/1000 | Loss: 0.00002013
Iteration 43/1000 | Loss: 0.00002013
Iteration 44/1000 | Loss: 0.00002013
Iteration 45/1000 | Loss: 0.00002013
Iteration 46/1000 | Loss: 0.00002013
Iteration 47/1000 | Loss: 0.00002013
Iteration 48/1000 | Loss: 0.00002012
Iteration 49/1000 | Loss: 0.00002012
Iteration 50/1000 | Loss: 0.00002011
Iteration 51/1000 | Loss: 0.00002011
Iteration 52/1000 | Loss: 0.00002011
Iteration 53/1000 | Loss: 0.00002011
Iteration 54/1000 | Loss: 0.00002011
Iteration 55/1000 | Loss: 0.00002011
Iteration 56/1000 | Loss: 0.00002011
Iteration 57/1000 | Loss: 0.00002010
Iteration 58/1000 | Loss: 0.00002010
Iteration 59/1000 | Loss: 0.00002010
Iteration 60/1000 | Loss: 0.00002009
Iteration 61/1000 | Loss: 0.00002009
Iteration 62/1000 | Loss: 0.00002009
Iteration 63/1000 | Loss: 0.00002008
Iteration 64/1000 | Loss: 0.00002008
Iteration 65/1000 | Loss: 0.00002008
Iteration 66/1000 | Loss: 0.00002007
Iteration 67/1000 | Loss: 0.00002007
Iteration 68/1000 | Loss: 0.00002007
Iteration 69/1000 | Loss: 0.00002007
Iteration 70/1000 | Loss: 0.00002006
Iteration 71/1000 | Loss: 0.00002006
Iteration 72/1000 | Loss: 0.00002006
Iteration 73/1000 | Loss: 0.00002006
Iteration 74/1000 | Loss: 0.00002006
Iteration 75/1000 | Loss: 0.00002006
Iteration 76/1000 | Loss: 0.00002006
Iteration 77/1000 | Loss: 0.00002006
Iteration 78/1000 | Loss: 0.00002005
Iteration 79/1000 | Loss: 0.00002005
Iteration 80/1000 | Loss: 0.00002005
Iteration 81/1000 | Loss: 0.00002005
Iteration 82/1000 | Loss: 0.00002005
Iteration 83/1000 | Loss: 0.00002005
Iteration 84/1000 | Loss: 0.00002005
Iteration 85/1000 | Loss: 0.00002005
Iteration 86/1000 | Loss: 0.00002005
Iteration 87/1000 | Loss: 0.00002004
Iteration 88/1000 | Loss: 0.00002004
Iteration 89/1000 | Loss: 0.00002004
Iteration 90/1000 | Loss: 0.00002004
Iteration 91/1000 | Loss: 0.00002004
Iteration 92/1000 | Loss: 0.00002004
Iteration 93/1000 | Loss: 0.00002004
Iteration 94/1000 | Loss: 0.00002004
Iteration 95/1000 | Loss: 0.00002003
Iteration 96/1000 | Loss: 0.00002003
Iteration 97/1000 | Loss: 0.00002003
Iteration 98/1000 | Loss: 0.00002003
Iteration 99/1000 | Loss: 0.00002003
Iteration 100/1000 | Loss: 0.00002003
Iteration 101/1000 | Loss: 0.00002003
Iteration 102/1000 | Loss: 0.00002003
Iteration 103/1000 | Loss: 0.00002002
Iteration 104/1000 | Loss: 0.00002002
Iteration 105/1000 | Loss: 0.00002002
Iteration 106/1000 | Loss: 0.00002002
Iteration 107/1000 | Loss: 0.00002002
Iteration 108/1000 | Loss: 0.00002002
Iteration 109/1000 | Loss: 0.00002002
Iteration 110/1000 | Loss: 0.00002002
Iteration 111/1000 | Loss: 0.00002002
Iteration 112/1000 | Loss: 0.00002002
Iteration 113/1000 | Loss: 0.00002002
Iteration 114/1000 | Loss: 0.00002002
Iteration 115/1000 | Loss: 0.00002002
Iteration 116/1000 | Loss: 0.00002002
Iteration 117/1000 | Loss: 0.00002002
Iteration 118/1000 | Loss: 0.00002001
Iteration 119/1000 | Loss: 0.00002001
Iteration 120/1000 | Loss: 0.00002001
Iteration 121/1000 | Loss: 0.00002001
Iteration 122/1000 | Loss: 0.00002001
Iteration 123/1000 | Loss: 0.00002001
Iteration 124/1000 | Loss: 0.00002001
Iteration 125/1000 | Loss: 0.00002001
Iteration 126/1000 | Loss: 0.00002001
Iteration 127/1000 | Loss: 0.00002001
Iteration 128/1000 | Loss: 0.00002001
Iteration 129/1000 | Loss: 0.00002001
Iteration 130/1000 | Loss: 0.00002000
Iteration 131/1000 | Loss: 0.00002000
Iteration 132/1000 | Loss: 0.00002000
Iteration 133/1000 | Loss: 0.00002000
Iteration 134/1000 | Loss: 0.00002000
Iteration 135/1000 | Loss: 0.00002000
Iteration 136/1000 | Loss: 0.00002000
Iteration 137/1000 | Loss: 0.00002000
Iteration 138/1000 | Loss: 0.00002000
Iteration 139/1000 | Loss: 0.00002000
Iteration 140/1000 | Loss: 0.00002000
Iteration 141/1000 | Loss: 0.00002000
Iteration 142/1000 | Loss: 0.00002000
Iteration 143/1000 | Loss: 0.00002000
Iteration 144/1000 | Loss: 0.00002000
Iteration 145/1000 | Loss: 0.00002000
Iteration 146/1000 | Loss: 0.00002000
Iteration 147/1000 | Loss: 0.00002000
Iteration 148/1000 | Loss: 0.00002000
Iteration 149/1000 | Loss: 0.00002000
Iteration 150/1000 | Loss: 0.00002000
Iteration 151/1000 | Loss: 0.00002000
Iteration 152/1000 | Loss: 0.00002000
Iteration 153/1000 | Loss: 0.00002000
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [2.000320455408655e-05, 2.000320455408655e-05, 2.000320455408655e-05, 2.000320455408655e-05, 2.000320455408655e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.000320455408655e-05

Optimization complete. Final v2v error: 3.668342351913452 mm

Highest mean error: 4.011050224304199 mm for frame 138

Lowest mean error: 3.341857433319092 mm for frame 187

Saving results

Total time: 65.27525758743286
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00200016
Iteration 2/25 | Loss: 0.00089390
Iteration 3/25 | Loss: 0.00079779
Iteration 4/25 | Loss: 0.00075557
Iteration 5/25 | Loss: 0.00073929
Iteration 6/25 | Loss: 0.00073627
Iteration 7/25 | Loss: 0.00073470
Iteration 8/25 | Loss: 0.00073439
Iteration 9/25 | Loss: 0.00073439
Iteration 10/25 | Loss: 0.00073439
Iteration 11/25 | Loss: 0.00073439
Iteration 12/25 | Loss: 0.00073439
Iteration 13/25 | Loss: 0.00073439
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007343864999711514, 0.0007343864999711514, 0.0007343864999711514, 0.0007343864999711514, 0.0007343864999711514]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007343864999711514

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51776791
Iteration 2/25 | Loss: 0.00053486
Iteration 3/25 | Loss: 0.00053486
Iteration 4/25 | Loss: 0.00053485
Iteration 5/25 | Loss: 0.00053485
Iteration 6/25 | Loss: 0.00053485
Iteration 7/25 | Loss: 0.00053485
Iteration 8/25 | Loss: 0.00053485
Iteration 9/25 | Loss: 0.00053485
Iteration 10/25 | Loss: 0.00053485
Iteration 11/25 | Loss: 0.00053485
Iteration 12/25 | Loss: 0.00053485
Iteration 13/25 | Loss: 0.00053485
Iteration 14/25 | Loss: 0.00053485
Iteration 15/25 | Loss: 0.00053485
Iteration 16/25 | Loss: 0.00053485
Iteration 17/25 | Loss: 0.00053485
Iteration 18/25 | Loss: 0.00053485
Iteration 19/25 | Loss: 0.00053485
Iteration 20/25 | Loss: 0.00053485
Iteration 21/25 | Loss: 0.00053485
Iteration 22/25 | Loss: 0.00053485
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0005348522099666297, 0.0005348522099666297, 0.0005348522099666297, 0.0005348522099666297, 0.0005348522099666297]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005348522099666297

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053485
Iteration 2/1000 | Loss: 0.00003899
Iteration 3/1000 | Loss: 0.00002309
Iteration 4/1000 | Loss: 0.00001924
Iteration 5/1000 | Loss: 0.00001796
Iteration 6/1000 | Loss: 0.00001705
Iteration 7/1000 | Loss: 0.00001657
Iteration 8/1000 | Loss: 0.00001617
Iteration 9/1000 | Loss: 0.00001609
Iteration 10/1000 | Loss: 0.00001602
Iteration 11/1000 | Loss: 0.00001582
Iteration 12/1000 | Loss: 0.00001565
Iteration 13/1000 | Loss: 0.00001556
Iteration 14/1000 | Loss: 0.00001553
Iteration 15/1000 | Loss: 0.00001553
Iteration 16/1000 | Loss: 0.00001548
Iteration 17/1000 | Loss: 0.00001539
Iteration 18/1000 | Loss: 0.00001531
Iteration 19/1000 | Loss: 0.00001531
Iteration 20/1000 | Loss: 0.00001527
Iteration 21/1000 | Loss: 0.00001527
Iteration 22/1000 | Loss: 0.00001527
Iteration 23/1000 | Loss: 0.00001526
Iteration 24/1000 | Loss: 0.00001525
Iteration 25/1000 | Loss: 0.00001524
Iteration 26/1000 | Loss: 0.00001524
Iteration 27/1000 | Loss: 0.00001524
Iteration 28/1000 | Loss: 0.00001524
Iteration 29/1000 | Loss: 0.00001524
Iteration 30/1000 | Loss: 0.00001523
Iteration 31/1000 | Loss: 0.00001523
Iteration 32/1000 | Loss: 0.00001523
Iteration 33/1000 | Loss: 0.00001523
Iteration 34/1000 | Loss: 0.00001522
Iteration 35/1000 | Loss: 0.00001522
Iteration 36/1000 | Loss: 0.00001522
Iteration 37/1000 | Loss: 0.00001522
Iteration 38/1000 | Loss: 0.00001522
Iteration 39/1000 | Loss: 0.00001522
Iteration 40/1000 | Loss: 0.00001522
Iteration 41/1000 | Loss: 0.00001522
Iteration 42/1000 | Loss: 0.00001521
Iteration 43/1000 | Loss: 0.00001521
Iteration 44/1000 | Loss: 0.00001521
Iteration 45/1000 | Loss: 0.00001520
Iteration 46/1000 | Loss: 0.00001520
Iteration 47/1000 | Loss: 0.00001520
Iteration 48/1000 | Loss: 0.00001520
Iteration 49/1000 | Loss: 0.00001519
Iteration 50/1000 | Loss: 0.00001518
Iteration 51/1000 | Loss: 0.00001518
Iteration 52/1000 | Loss: 0.00001517
Iteration 53/1000 | Loss: 0.00001517
Iteration 54/1000 | Loss: 0.00001517
Iteration 55/1000 | Loss: 0.00001517
Iteration 56/1000 | Loss: 0.00001517
Iteration 57/1000 | Loss: 0.00001516
Iteration 58/1000 | Loss: 0.00001516
Iteration 59/1000 | Loss: 0.00001516
Iteration 60/1000 | Loss: 0.00001516
Iteration 61/1000 | Loss: 0.00001516
Iteration 62/1000 | Loss: 0.00001516
Iteration 63/1000 | Loss: 0.00001516
Iteration 64/1000 | Loss: 0.00001516
Iteration 65/1000 | Loss: 0.00001516
Iteration 66/1000 | Loss: 0.00001516
Iteration 67/1000 | Loss: 0.00001516
Iteration 68/1000 | Loss: 0.00001516
Iteration 69/1000 | Loss: 0.00001515
Iteration 70/1000 | Loss: 0.00001515
Iteration 71/1000 | Loss: 0.00001515
Iteration 72/1000 | Loss: 0.00001515
Iteration 73/1000 | Loss: 0.00001514
Iteration 74/1000 | Loss: 0.00001514
Iteration 75/1000 | Loss: 0.00001514
Iteration 76/1000 | Loss: 0.00001514
Iteration 77/1000 | Loss: 0.00001514
Iteration 78/1000 | Loss: 0.00001514
Iteration 79/1000 | Loss: 0.00001514
Iteration 80/1000 | Loss: 0.00001514
Iteration 81/1000 | Loss: 0.00001514
Iteration 82/1000 | Loss: 0.00001514
Iteration 83/1000 | Loss: 0.00001514
Iteration 84/1000 | Loss: 0.00001514
Iteration 85/1000 | Loss: 0.00001514
Iteration 86/1000 | Loss: 0.00001513
Iteration 87/1000 | Loss: 0.00001513
Iteration 88/1000 | Loss: 0.00001513
Iteration 89/1000 | Loss: 0.00001513
Iteration 90/1000 | Loss: 0.00001513
Iteration 91/1000 | Loss: 0.00001513
Iteration 92/1000 | Loss: 0.00001513
Iteration 93/1000 | Loss: 0.00001512
Iteration 94/1000 | Loss: 0.00001512
Iteration 95/1000 | Loss: 0.00001512
Iteration 96/1000 | Loss: 0.00001512
Iteration 97/1000 | Loss: 0.00001512
Iteration 98/1000 | Loss: 0.00001512
Iteration 99/1000 | Loss: 0.00001512
Iteration 100/1000 | Loss: 0.00001512
Iteration 101/1000 | Loss: 0.00001512
Iteration 102/1000 | Loss: 0.00001512
Iteration 103/1000 | Loss: 0.00001512
Iteration 104/1000 | Loss: 0.00001512
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [1.512154904048657e-05, 1.512154904048657e-05, 1.512154904048657e-05, 1.512154904048657e-05, 1.512154904048657e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.512154904048657e-05

Optimization complete. Final v2v error: 3.300858497619629 mm

Highest mean error: 3.5564310550689697 mm for frame 23

Lowest mean error: 3.051882266998291 mm for frame 85

Saving results

Total time: 39.119057178497314
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00799171
Iteration 2/25 | Loss: 0.00129934
Iteration 3/25 | Loss: 0.00088774
Iteration 4/25 | Loss: 0.00081296
Iteration 5/25 | Loss: 0.00079963
Iteration 6/25 | Loss: 0.00080644
Iteration 7/25 | Loss: 0.00079348
Iteration 8/25 | Loss: 0.00079230
Iteration 9/25 | Loss: 0.00078313
Iteration 10/25 | Loss: 0.00078207
Iteration 11/25 | Loss: 0.00078026
Iteration 12/25 | Loss: 0.00077754
Iteration 13/25 | Loss: 0.00077690
Iteration 14/25 | Loss: 0.00077934
Iteration 15/25 | Loss: 0.00077967
Iteration 16/25 | Loss: 0.00077868
Iteration 17/25 | Loss: 0.00077897
Iteration 18/25 | Loss: 0.00077886
Iteration 19/25 | Loss: 0.00077879
Iteration 20/25 | Loss: 0.00077905
Iteration 21/25 | Loss: 0.00077985
Iteration 22/25 | Loss: 0.00077809
Iteration 23/25 | Loss: 0.00077940
Iteration 24/25 | Loss: 0.00077869
Iteration 25/25 | Loss: 0.00077995

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.03998446
Iteration 2/25 | Loss: 0.00065983
Iteration 3/25 | Loss: 0.00065366
Iteration 4/25 | Loss: 0.00065366
Iteration 5/25 | Loss: 0.00065365
Iteration 6/25 | Loss: 0.00065365
Iteration 7/25 | Loss: 0.00065365
Iteration 8/25 | Loss: 0.00065365
Iteration 9/25 | Loss: 0.00065365
Iteration 10/25 | Loss: 0.00065365
Iteration 11/25 | Loss: 0.00065365
Iteration 12/25 | Loss: 0.00065365
Iteration 13/25 | Loss: 0.00065365
Iteration 14/25 | Loss: 0.00065365
Iteration 15/25 | Loss: 0.00065365
Iteration 16/25 | Loss: 0.00065365
Iteration 17/25 | Loss: 0.00065365
Iteration 18/25 | Loss: 0.00065365
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006536534056067467, 0.0006536534056067467, 0.0006536534056067467, 0.0006536534056067467, 0.0006536534056067467]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006536534056067467

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065365
Iteration 2/1000 | Loss: 0.00020963
Iteration 3/1000 | Loss: 0.00006786
Iteration 4/1000 | Loss: 0.00007153
Iteration 5/1000 | Loss: 0.00004752
Iteration 6/1000 | Loss: 0.00024023
Iteration 7/1000 | Loss: 0.00003467
Iteration 8/1000 | Loss: 0.00026194
Iteration 9/1000 | Loss: 0.00020940
Iteration 10/1000 | Loss: 0.00005655
Iteration 11/1000 | Loss: 0.00013092
Iteration 12/1000 | Loss: 0.00029632
Iteration 13/1000 | Loss: 0.00008967
Iteration 14/1000 | Loss: 0.00023751
Iteration 15/1000 | Loss: 0.00013386
Iteration 16/1000 | Loss: 0.00014630
Iteration 17/1000 | Loss: 0.00021877
Iteration 18/1000 | Loss: 0.00010316
Iteration 19/1000 | Loss: 0.00026888
Iteration 20/1000 | Loss: 0.00016470
Iteration 21/1000 | Loss: 0.00015024
Iteration 22/1000 | Loss: 0.00026937
Iteration 23/1000 | Loss: 0.00025967
Iteration 24/1000 | Loss: 0.00007680
Iteration 25/1000 | Loss: 0.00002944
Iteration 26/1000 | Loss: 0.00002560
Iteration 27/1000 | Loss: 0.00005373
Iteration 28/1000 | Loss: 0.00015451
Iteration 29/1000 | Loss: 0.00008589
Iteration 30/1000 | Loss: 0.00011233
Iteration 31/1000 | Loss: 0.00005676
Iteration 32/1000 | Loss: 0.00024453
Iteration 33/1000 | Loss: 0.00026791
Iteration 34/1000 | Loss: 0.00024756
Iteration 35/1000 | Loss: 0.00101976
Iteration 36/1000 | Loss: 0.00032092
Iteration 37/1000 | Loss: 0.00032406
Iteration 38/1000 | Loss: 0.00016332
Iteration 39/1000 | Loss: 0.00011695
Iteration 40/1000 | Loss: 0.00004630
Iteration 41/1000 | Loss: 0.00005144
Iteration 42/1000 | Loss: 0.00014945
Iteration 43/1000 | Loss: 0.00014252
Iteration 44/1000 | Loss: 0.00052101
Iteration 45/1000 | Loss: 0.00024302
Iteration 46/1000 | Loss: 0.00026315
Iteration 47/1000 | Loss: 0.00029687
Iteration 48/1000 | Loss: 0.00037325
Iteration 49/1000 | Loss: 0.00022443
Iteration 50/1000 | Loss: 0.00099613
Iteration 51/1000 | Loss: 0.00032212
Iteration 52/1000 | Loss: 0.00010395
Iteration 53/1000 | Loss: 0.00007773
Iteration 54/1000 | Loss: 0.00015012
Iteration 55/1000 | Loss: 0.00038188
Iteration 56/1000 | Loss: 0.00017063
Iteration 57/1000 | Loss: 0.00021541
Iteration 58/1000 | Loss: 0.00019335
Iteration 59/1000 | Loss: 0.00007664
Iteration 60/1000 | Loss: 0.00008076
Iteration 61/1000 | Loss: 0.00004936
Iteration 62/1000 | Loss: 0.00010800
Iteration 63/1000 | Loss: 0.00005082
Iteration 64/1000 | Loss: 0.00010627
Iteration 65/1000 | Loss: 0.00066915
Iteration 66/1000 | Loss: 0.00036748
Iteration 67/1000 | Loss: 0.00030491
Iteration 68/1000 | Loss: 0.00011272
Iteration 69/1000 | Loss: 0.00023367
Iteration 70/1000 | Loss: 0.00074868
Iteration 71/1000 | Loss: 0.00030275
Iteration 72/1000 | Loss: 0.00016544
Iteration 73/1000 | Loss: 0.00013053
Iteration 74/1000 | Loss: 0.00013015
Iteration 75/1000 | Loss: 0.00040901
Iteration 76/1000 | Loss: 0.00040530
Iteration 77/1000 | Loss: 0.00028748
Iteration 78/1000 | Loss: 0.00015422
Iteration 79/1000 | Loss: 0.00007189
Iteration 80/1000 | Loss: 0.00013562
Iteration 81/1000 | Loss: 0.00034037
Iteration 82/1000 | Loss: 0.00030546
Iteration 83/1000 | Loss: 0.00020862
Iteration 84/1000 | Loss: 0.00057406
Iteration 85/1000 | Loss: 0.00035243
Iteration 86/1000 | Loss: 0.00037954
Iteration 87/1000 | Loss: 0.00032075
Iteration 88/1000 | Loss: 0.00017115
Iteration 89/1000 | Loss: 0.00022213
Iteration 90/1000 | Loss: 0.00023940
Iteration 91/1000 | Loss: 0.00038838
Iteration 92/1000 | Loss: 0.00020164
Iteration 93/1000 | Loss: 0.00074848
Iteration 94/1000 | Loss: 0.00021299
Iteration 95/1000 | Loss: 0.00020364
Iteration 96/1000 | Loss: 0.00007868
Iteration 97/1000 | Loss: 0.00031258
Iteration 98/1000 | Loss: 0.00017734
Iteration 99/1000 | Loss: 0.00014525
Iteration 100/1000 | Loss: 0.00019897
Iteration 101/1000 | Loss: 0.00002829
Iteration 102/1000 | Loss: 0.00018622
Iteration 103/1000 | Loss: 0.00020280
Iteration 104/1000 | Loss: 0.00002410
Iteration 105/1000 | Loss: 0.00020658
Iteration 106/1000 | Loss: 0.00021648
Iteration 107/1000 | Loss: 0.00009174
Iteration 108/1000 | Loss: 0.00018789
Iteration 109/1000 | Loss: 0.00002634
Iteration 110/1000 | Loss: 0.00004293
Iteration 111/1000 | Loss: 0.00006952
Iteration 112/1000 | Loss: 0.00002425
Iteration 113/1000 | Loss: 0.00002218
Iteration 114/1000 | Loss: 0.00002008
Iteration 115/1000 | Loss: 0.00002377
Iteration 116/1000 | Loss: 0.00002001
Iteration 117/1000 | Loss: 0.00001897
Iteration 118/1000 | Loss: 0.00001892
Iteration 119/1000 | Loss: 0.00001853
Iteration 120/1000 | Loss: 0.00001809
Iteration 121/1000 | Loss: 0.00001753
Iteration 122/1000 | Loss: 0.00001704
Iteration 123/1000 | Loss: 0.00001675
Iteration 124/1000 | Loss: 0.00001675
Iteration 125/1000 | Loss: 0.00001674
Iteration 126/1000 | Loss: 0.00001665
Iteration 127/1000 | Loss: 0.00001664
Iteration 128/1000 | Loss: 0.00001663
Iteration 129/1000 | Loss: 0.00001657
Iteration 130/1000 | Loss: 0.00001649
Iteration 131/1000 | Loss: 0.00001647
Iteration 132/1000 | Loss: 0.00001647
Iteration 133/1000 | Loss: 0.00001646
Iteration 134/1000 | Loss: 0.00001645
Iteration 135/1000 | Loss: 0.00001640
Iteration 136/1000 | Loss: 0.00001638
Iteration 137/1000 | Loss: 0.00001638
Iteration 138/1000 | Loss: 0.00001637
Iteration 139/1000 | Loss: 0.00001637
Iteration 140/1000 | Loss: 0.00001637
Iteration 141/1000 | Loss: 0.00001636
Iteration 142/1000 | Loss: 0.00001636
Iteration 143/1000 | Loss: 0.00001636
Iteration 144/1000 | Loss: 0.00001636
Iteration 145/1000 | Loss: 0.00001635
Iteration 146/1000 | Loss: 0.00001635
Iteration 147/1000 | Loss: 0.00001634
Iteration 148/1000 | Loss: 0.00001634
Iteration 149/1000 | Loss: 0.00001633
Iteration 150/1000 | Loss: 0.00001633
Iteration 151/1000 | Loss: 0.00001633
Iteration 152/1000 | Loss: 0.00001632
Iteration 153/1000 | Loss: 0.00001632
Iteration 154/1000 | Loss: 0.00001632
Iteration 155/1000 | Loss: 0.00001632
Iteration 156/1000 | Loss: 0.00001632
Iteration 157/1000 | Loss: 0.00001632
Iteration 158/1000 | Loss: 0.00001632
Iteration 159/1000 | Loss: 0.00001632
Iteration 160/1000 | Loss: 0.00001631
Iteration 161/1000 | Loss: 0.00001631
Iteration 162/1000 | Loss: 0.00001631
Iteration 163/1000 | Loss: 0.00001631
Iteration 164/1000 | Loss: 0.00001631
Iteration 165/1000 | Loss: 0.00001631
Iteration 166/1000 | Loss: 0.00001631
Iteration 167/1000 | Loss: 0.00001631
Iteration 168/1000 | Loss: 0.00001631
Iteration 169/1000 | Loss: 0.00001631
Iteration 170/1000 | Loss: 0.00001631
Iteration 171/1000 | Loss: 0.00001631
Iteration 172/1000 | Loss: 0.00001630
Iteration 173/1000 | Loss: 0.00001630
Iteration 174/1000 | Loss: 0.00001630
Iteration 175/1000 | Loss: 0.00001630
Iteration 176/1000 | Loss: 0.00001630
Iteration 177/1000 | Loss: 0.00001630
Iteration 178/1000 | Loss: 0.00001630
Iteration 179/1000 | Loss: 0.00001630
Iteration 180/1000 | Loss: 0.00001630
Iteration 181/1000 | Loss: 0.00001630
Iteration 182/1000 | Loss: 0.00001630
Iteration 183/1000 | Loss: 0.00001630
Iteration 184/1000 | Loss: 0.00001629
Iteration 185/1000 | Loss: 0.00001629
Iteration 186/1000 | Loss: 0.00001629
Iteration 187/1000 | Loss: 0.00001629
Iteration 188/1000 | Loss: 0.00001629
Iteration 189/1000 | Loss: 0.00001629
Iteration 190/1000 | Loss: 0.00001629
Iteration 191/1000 | Loss: 0.00001629
Iteration 192/1000 | Loss: 0.00001628
Iteration 193/1000 | Loss: 0.00001628
Iteration 194/1000 | Loss: 0.00001628
Iteration 195/1000 | Loss: 0.00001628
Iteration 196/1000 | Loss: 0.00001628
Iteration 197/1000 | Loss: 0.00001628
Iteration 198/1000 | Loss: 0.00001628
Iteration 199/1000 | Loss: 0.00001627
Iteration 200/1000 | Loss: 0.00001627
Iteration 201/1000 | Loss: 0.00001627
Iteration 202/1000 | Loss: 0.00001627
Iteration 203/1000 | Loss: 0.00001627
Iteration 204/1000 | Loss: 0.00001627
Iteration 205/1000 | Loss: 0.00001627
Iteration 206/1000 | Loss: 0.00001626
Iteration 207/1000 | Loss: 0.00001626
Iteration 208/1000 | Loss: 0.00001626
Iteration 209/1000 | Loss: 0.00001626
Iteration 210/1000 | Loss: 0.00001626
Iteration 211/1000 | Loss: 0.00001626
Iteration 212/1000 | Loss: 0.00001626
Iteration 213/1000 | Loss: 0.00001625
Iteration 214/1000 | Loss: 0.00001625
Iteration 215/1000 | Loss: 0.00001625
Iteration 216/1000 | Loss: 0.00001625
Iteration 217/1000 | Loss: 0.00001625
Iteration 218/1000 | Loss: 0.00001625
Iteration 219/1000 | Loss: 0.00001625
Iteration 220/1000 | Loss: 0.00001625
Iteration 221/1000 | Loss: 0.00001625
Iteration 222/1000 | Loss: 0.00001625
Iteration 223/1000 | Loss: 0.00001625
Iteration 224/1000 | Loss: 0.00001624
Iteration 225/1000 | Loss: 0.00001624
Iteration 226/1000 | Loss: 0.00001624
Iteration 227/1000 | Loss: 0.00001624
Iteration 228/1000 | Loss: 0.00001624
Iteration 229/1000 | Loss: 0.00001624
Iteration 230/1000 | Loss: 0.00001624
Iteration 231/1000 | Loss: 0.00001624
Iteration 232/1000 | Loss: 0.00001624
Iteration 233/1000 | Loss: 0.00001624
Iteration 234/1000 | Loss: 0.00001624
Iteration 235/1000 | Loss: 0.00001624
Iteration 236/1000 | Loss: 0.00001624
Iteration 237/1000 | Loss: 0.00001624
Iteration 238/1000 | Loss: 0.00001624
Iteration 239/1000 | Loss: 0.00001624
Iteration 240/1000 | Loss: 0.00001624
Iteration 241/1000 | Loss: 0.00001624
Iteration 242/1000 | Loss: 0.00001624
Iteration 243/1000 | Loss: 0.00001624
Iteration 244/1000 | Loss: 0.00001624
Iteration 245/1000 | Loss: 0.00001624
Iteration 246/1000 | Loss: 0.00001624
Iteration 247/1000 | Loss: 0.00001624
Iteration 248/1000 | Loss: 0.00001624
Iteration 249/1000 | Loss: 0.00001624
Iteration 250/1000 | Loss: 0.00001624
Iteration 251/1000 | Loss: 0.00001624
Iteration 252/1000 | Loss: 0.00001624
Iteration 253/1000 | Loss: 0.00001624
Iteration 254/1000 | Loss: 0.00001624
Iteration 255/1000 | Loss: 0.00001624
Iteration 256/1000 | Loss: 0.00001624
Iteration 257/1000 | Loss: 0.00001624
Iteration 258/1000 | Loss: 0.00001624
Iteration 259/1000 | Loss: 0.00001624
Iteration 260/1000 | Loss: 0.00001624
Iteration 261/1000 | Loss: 0.00001624
Iteration 262/1000 | Loss: 0.00001624
Iteration 263/1000 | Loss: 0.00001624
Iteration 264/1000 | Loss: 0.00001624
Iteration 265/1000 | Loss: 0.00001624
Iteration 266/1000 | Loss: 0.00001624
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 266. Stopping optimization.
Last 5 losses: [1.6238653188338503e-05, 1.6238653188338503e-05, 1.6238653188338503e-05, 1.6238653188338503e-05, 1.6238653188338503e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6238653188338503e-05

Optimization complete. Final v2v error: 3.3954975605010986 mm

Highest mean error: 4.380881309509277 mm for frame 38

Lowest mean error: 3.045663595199585 mm for frame 236

Saving results

Total time: 264.5895779132843
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00993443
Iteration 2/25 | Loss: 0.00402628
Iteration 3/25 | Loss: 0.00196237
Iteration 4/25 | Loss: 0.00175482
Iteration 5/25 | Loss: 0.00164029
Iteration 6/25 | Loss: 0.00153882
Iteration 7/25 | Loss: 0.00149279
Iteration 8/25 | Loss: 0.00146571
Iteration 9/25 | Loss: 0.00140042
Iteration 10/25 | Loss: 0.00136597
Iteration 11/25 | Loss: 0.00125576
Iteration 12/25 | Loss: 0.00116648
Iteration 13/25 | Loss: 0.00112246
Iteration 14/25 | Loss: 0.00110717
Iteration 15/25 | Loss: 0.00109788
Iteration 16/25 | Loss: 0.00108331
Iteration 17/25 | Loss: 0.00107142
Iteration 18/25 | Loss: 0.00105904
Iteration 19/25 | Loss: 0.00105009
Iteration 20/25 | Loss: 0.00104113
Iteration 21/25 | Loss: 0.00103670
Iteration 22/25 | Loss: 0.00102267
Iteration 23/25 | Loss: 0.00102130
Iteration 24/25 | Loss: 0.00101298
Iteration 25/25 | Loss: 0.00101141

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52496910
Iteration 2/25 | Loss: 0.00491834
Iteration 3/25 | Loss: 0.00411987
Iteration 4/25 | Loss: 0.00411987
Iteration 5/25 | Loss: 0.00411987
Iteration 6/25 | Loss: 0.00411987
Iteration 7/25 | Loss: 0.00411987
Iteration 8/25 | Loss: 0.00411987
Iteration 9/25 | Loss: 0.00411987
Iteration 10/25 | Loss: 0.00411987
Iteration 11/25 | Loss: 0.00411987
Iteration 12/25 | Loss: 0.00411987
Iteration 13/25 | Loss: 0.00411987
Iteration 14/25 | Loss: 0.00411987
Iteration 15/25 | Loss: 0.00411987
Iteration 16/25 | Loss: 0.00411987
Iteration 17/25 | Loss: 0.00411987
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.004119866993278265, 0.004119866993278265, 0.004119866993278265, 0.004119866993278265, 0.004119866993278265]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004119866993278265

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00411987
Iteration 2/1000 | Loss: 0.00202659
Iteration 3/1000 | Loss: 0.00138802
Iteration 4/1000 | Loss: 0.00052210
Iteration 5/1000 | Loss: 0.00052996
Iteration 6/1000 | Loss: 0.00045029
Iteration 7/1000 | Loss: 0.00022278
Iteration 8/1000 | Loss: 0.00008757
Iteration 9/1000 | Loss: 0.00005023
Iteration 10/1000 | Loss: 0.00017359
Iteration 11/1000 | Loss: 0.00005486
Iteration 12/1000 | Loss: 0.00004538
Iteration 13/1000 | Loss: 0.00004991
Iteration 14/1000 | Loss: 0.00003072
Iteration 15/1000 | Loss: 0.00004754
Iteration 16/1000 | Loss: 0.00003233
Iteration 17/1000 | Loss: 0.00011066
Iteration 18/1000 | Loss: 0.00020697
Iteration 19/1000 | Loss: 0.00058807
Iteration 20/1000 | Loss: 0.00022577
Iteration 21/1000 | Loss: 0.00045154
Iteration 22/1000 | Loss: 0.00018241
Iteration 23/1000 | Loss: 0.00024398
Iteration 24/1000 | Loss: 0.00005527
Iteration 25/1000 | Loss: 0.00020207
Iteration 26/1000 | Loss: 0.00048305
Iteration 27/1000 | Loss: 0.00014754
Iteration 28/1000 | Loss: 0.00003250
Iteration 29/1000 | Loss: 0.00006814
Iteration 30/1000 | Loss: 0.00008766
Iteration 31/1000 | Loss: 0.00003898
Iteration 32/1000 | Loss: 0.00027262
Iteration 33/1000 | Loss: 0.00016513
Iteration 34/1000 | Loss: 0.00003749
Iteration 35/1000 | Loss: 0.00002870
Iteration 36/1000 | Loss: 0.00004225
Iteration 37/1000 | Loss: 0.00001744
Iteration 38/1000 | Loss: 0.00002991
Iteration 39/1000 | Loss: 0.00010007
Iteration 40/1000 | Loss: 0.00014808
Iteration 41/1000 | Loss: 0.00008174
Iteration 42/1000 | Loss: 0.00001795
Iteration 43/1000 | Loss: 0.00003892
Iteration 44/1000 | Loss: 0.00001999
Iteration 45/1000 | Loss: 0.00002969
Iteration 46/1000 | Loss: 0.00002138
Iteration 47/1000 | Loss: 0.00001459
Iteration 48/1000 | Loss: 0.00001927
Iteration 49/1000 | Loss: 0.00002442
Iteration 50/1000 | Loss: 0.00001516
Iteration 51/1000 | Loss: 0.00001647
Iteration 52/1000 | Loss: 0.00001417
Iteration 53/1000 | Loss: 0.00001425
Iteration 54/1000 | Loss: 0.00001425
Iteration 55/1000 | Loss: 0.00001415
Iteration 56/1000 | Loss: 0.00001415
Iteration 57/1000 | Loss: 0.00001415
Iteration 58/1000 | Loss: 0.00001415
Iteration 59/1000 | Loss: 0.00001415
Iteration 60/1000 | Loss: 0.00001415
Iteration 61/1000 | Loss: 0.00001415
Iteration 62/1000 | Loss: 0.00001415
Iteration 63/1000 | Loss: 0.00001415
Iteration 64/1000 | Loss: 0.00001415
Iteration 65/1000 | Loss: 0.00001414
Iteration 66/1000 | Loss: 0.00001414
Iteration 67/1000 | Loss: 0.00001414
Iteration 68/1000 | Loss: 0.00001414
Iteration 69/1000 | Loss: 0.00001414
Iteration 70/1000 | Loss: 0.00001414
Iteration 71/1000 | Loss: 0.00001414
Iteration 72/1000 | Loss: 0.00001414
Iteration 73/1000 | Loss: 0.00001414
Iteration 74/1000 | Loss: 0.00001414
Iteration 75/1000 | Loss: 0.00001414
Iteration 76/1000 | Loss: 0.00001413
Iteration 77/1000 | Loss: 0.00001542
Iteration 78/1000 | Loss: 0.00001428
Iteration 79/1000 | Loss: 0.00001502
Iteration 80/1000 | Loss: 0.00002645
Iteration 81/1000 | Loss: 0.00001415
Iteration 82/1000 | Loss: 0.00002517
Iteration 83/1000 | Loss: 0.00005998
Iteration 84/1000 | Loss: 0.00008320
Iteration 85/1000 | Loss: 0.00001411
Iteration 86/1000 | Loss: 0.00001485
Iteration 87/1000 | Loss: 0.00002826
Iteration 88/1000 | Loss: 0.00001446
Iteration 89/1000 | Loss: 0.00001410
Iteration 90/1000 | Loss: 0.00001635
Iteration 91/1000 | Loss: 0.00002554
Iteration 92/1000 | Loss: 0.00001395
Iteration 93/1000 | Loss: 0.00001395
Iteration 94/1000 | Loss: 0.00001395
Iteration 95/1000 | Loss: 0.00001395
Iteration 96/1000 | Loss: 0.00001395
Iteration 97/1000 | Loss: 0.00001395
Iteration 98/1000 | Loss: 0.00001395
Iteration 99/1000 | Loss: 0.00001394
Iteration 100/1000 | Loss: 0.00001394
Iteration 101/1000 | Loss: 0.00001394
Iteration 102/1000 | Loss: 0.00001394
Iteration 103/1000 | Loss: 0.00001394
Iteration 104/1000 | Loss: 0.00001393
Iteration 105/1000 | Loss: 0.00001393
Iteration 106/1000 | Loss: 0.00001393
Iteration 107/1000 | Loss: 0.00001393
Iteration 108/1000 | Loss: 0.00001393
Iteration 109/1000 | Loss: 0.00001393
Iteration 110/1000 | Loss: 0.00001393
Iteration 111/1000 | Loss: 0.00001393
Iteration 112/1000 | Loss: 0.00001392
Iteration 113/1000 | Loss: 0.00014037
Iteration 114/1000 | Loss: 0.00010510
Iteration 115/1000 | Loss: 0.00033878
Iteration 116/1000 | Loss: 0.00031908
Iteration 117/1000 | Loss: 0.00023122
Iteration 118/1000 | Loss: 0.00002515
Iteration 119/1000 | Loss: 0.00003662
Iteration 120/1000 | Loss: 0.00002687
Iteration 121/1000 | Loss: 0.00001897
Iteration 122/1000 | Loss: 0.00002380
Iteration 123/1000 | Loss: 0.00001287
Iteration 124/1000 | Loss: 0.00002957
Iteration 125/1000 | Loss: 0.00001248
Iteration 126/1000 | Loss: 0.00001258
Iteration 127/1000 | Loss: 0.00002813
Iteration 128/1000 | Loss: 0.00001697
Iteration 129/1000 | Loss: 0.00011555
Iteration 130/1000 | Loss: 0.00004570
Iteration 131/1000 | Loss: 0.00003336
Iteration 132/1000 | Loss: 0.00003604
Iteration 133/1000 | Loss: 0.00001228
Iteration 134/1000 | Loss: 0.00001198
Iteration 135/1000 | Loss: 0.00001286
Iteration 136/1000 | Loss: 0.00002654
Iteration 137/1000 | Loss: 0.00003431
Iteration 138/1000 | Loss: 0.00001495
Iteration 139/1000 | Loss: 0.00001231
Iteration 140/1000 | Loss: 0.00001210
Iteration 141/1000 | Loss: 0.00001184
Iteration 142/1000 | Loss: 0.00001184
Iteration 143/1000 | Loss: 0.00001184
Iteration 144/1000 | Loss: 0.00001184
Iteration 145/1000 | Loss: 0.00001184
Iteration 146/1000 | Loss: 0.00001184
Iteration 147/1000 | Loss: 0.00001184
Iteration 148/1000 | Loss: 0.00001184
Iteration 149/1000 | Loss: 0.00001184
Iteration 150/1000 | Loss: 0.00001184
Iteration 151/1000 | Loss: 0.00001184
Iteration 152/1000 | Loss: 0.00001184
Iteration 153/1000 | Loss: 0.00001184
Iteration 154/1000 | Loss: 0.00001183
Iteration 155/1000 | Loss: 0.00001183
Iteration 156/1000 | Loss: 0.00001183
Iteration 157/1000 | Loss: 0.00001191
Iteration 158/1000 | Loss: 0.00001191
Iteration 159/1000 | Loss: 0.00001374
Iteration 160/1000 | Loss: 0.00001709
Iteration 161/1000 | Loss: 0.00001497
Iteration 162/1000 | Loss: 0.00001230
Iteration 163/1000 | Loss: 0.00001189
Iteration 164/1000 | Loss: 0.00001178
Iteration 165/1000 | Loss: 0.00001178
Iteration 166/1000 | Loss: 0.00001243
Iteration 167/1000 | Loss: 0.00001312
Iteration 168/1000 | Loss: 0.00002105
Iteration 169/1000 | Loss: 0.00001318
Iteration 170/1000 | Loss: 0.00001181
Iteration 171/1000 | Loss: 0.00001177
Iteration 172/1000 | Loss: 0.00001176
Iteration 173/1000 | Loss: 0.00001179
Iteration 174/1000 | Loss: 0.00001179
Iteration 175/1000 | Loss: 0.00001176
Iteration 176/1000 | Loss: 0.00001176
Iteration 177/1000 | Loss: 0.00001176
Iteration 178/1000 | Loss: 0.00001175
Iteration 179/1000 | Loss: 0.00001175
Iteration 180/1000 | Loss: 0.00001175
Iteration 181/1000 | Loss: 0.00001175
Iteration 182/1000 | Loss: 0.00001174
Iteration 183/1000 | Loss: 0.00001174
Iteration 184/1000 | Loss: 0.00001174
Iteration 185/1000 | Loss: 0.00001174
Iteration 186/1000 | Loss: 0.00001174
Iteration 187/1000 | Loss: 0.00001174
Iteration 188/1000 | Loss: 0.00001174
Iteration 189/1000 | Loss: 0.00001174
Iteration 190/1000 | Loss: 0.00001174
Iteration 191/1000 | Loss: 0.00001173
Iteration 192/1000 | Loss: 0.00001173
Iteration 193/1000 | Loss: 0.00001173
Iteration 194/1000 | Loss: 0.00001173
Iteration 195/1000 | Loss: 0.00001173
Iteration 196/1000 | Loss: 0.00001173
Iteration 197/1000 | Loss: 0.00001173
Iteration 198/1000 | Loss: 0.00001173
Iteration 199/1000 | Loss: 0.00001173
Iteration 200/1000 | Loss: 0.00001268
Iteration 201/1000 | Loss: 0.00001268
Iteration 202/1000 | Loss: 0.00002557
Iteration 203/1000 | Loss: 0.00001348
Iteration 204/1000 | Loss: 0.00001806
Iteration 205/1000 | Loss: 0.00001241
Iteration 206/1000 | Loss: 0.00001651
Iteration 207/1000 | Loss: 0.00001171
Iteration 208/1000 | Loss: 0.00001171
Iteration 209/1000 | Loss: 0.00001171
Iteration 210/1000 | Loss: 0.00001171
Iteration 211/1000 | Loss: 0.00001171
Iteration 212/1000 | Loss: 0.00001171
Iteration 213/1000 | Loss: 0.00001171
Iteration 214/1000 | Loss: 0.00001171
Iteration 215/1000 | Loss: 0.00001171
Iteration 216/1000 | Loss: 0.00001170
Iteration 217/1000 | Loss: 0.00001170
Iteration 218/1000 | Loss: 0.00001170
Iteration 219/1000 | Loss: 0.00001170
Iteration 220/1000 | Loss: 0.00001170
Iteration 221/1000 | Loss: 0.00001228
Iteration 222/1000 | Loss: 0.00001228
Iteration 223/1000 | Loss: 0.00001228
Iteration 224/1000 | Loss: 0.00002237
Iteration 225/1000 | Loss: 0.00001384
Iteration 226/1000 | Loss: 0.00002370
Iteration 227/1000 | Loss: 0.00001228
Iteration 228/1000 | Loss: 0.00001172
Iteration 229/1000 | Loss: 0.00001171
Iteration 230/1000 | Loss: 0.00001170
Iteration 231/1000 | Loss: 0.00001170
Iteration 232/1000 | Loss: 0.00001170
Iteration 233/1000 | Loss: 0.00001169
Iteration 234/1000 | Loss: 0.00001169
Iteration 235/1000 | Loss: 0.00001169
Iteration 236/1000 | Loss: 0.00001169
Iteration 237/1000 | Loss: 0.00001169
Iteration 238/1000 | Loss: 0.00001169
Iteration 239/1000 | Loss: 0.00001169
Iteration 240/1000 | Loss: 0.00001169
Iteration 241/1000 | Loss: 0.00001169
Iteration 242/1000 | Loss: 0.00001169
Iteration 243/1000 | Loss: 0.00001169
Iteration 244/1000 | Loss: 0.00001169
Iteration 245/1000 | Loss: 0.00001178
Iteration 246/1000 | Loss: 0.00001178
Iteration 247/1000 | Loss: 0.00001169
Iteration 248/1000 | Loss: 0.00001169
Iteration 249/1000 | Loss: 0.00001168
Iteration 250/1000 | Loss: 0.00001168
Iteration 251/1000 | Loss: 0.00001168
Iteration 252/1000 | Loss: 0.00001168
Iteration 253/1000 | Loss: 0.00001168
Iteration 254/1000 | Loss: 0.00001168
Iteration 255/1000 | Loss: 0.00001168
Iteration 256/1000 | Loss: 0.00001168
Iteration 257/1000 | Loss: 0.00001168
Iteration 258/1000 | Loss: 0.00001168
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 258. Stopping optimization.
Last 5 losses: [1.1679760063998401e-05, 1.1679760063998401e-05, 1.1679760063998401e-05, 1.1679760063998401e-05, 1.1679760063998401e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1679760063998401e-05

Optimization complete. Final v2v error: 2.8853747844696045 mm

Highest mean error: 3.613499641418457 mm for frame 10

Lowest mean error: 2.6469507217407227 mm for frame 93

Saving results

Total time: 226.2613549232483
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01074242
Iteration 2/25 | Loss: 0.00345098
Iteration 3/25 | Loss: 0.00218298
Iteration 4/25 | Loss: 0.00195780
Iteration 5/25 | Loss: 0.00180289
Iteration 6/25 | Loss: 0.00179920
Iteration 7/25 | Loss: 0.00178854
Iteration 8/25 | Loss: 0.00172911
Iteration 9/25 | Loss: 0.00170339
Iteration 10/25 | Loss: 0.00168832
Iteration 11/25 | Loss: 0.00165513
Iteration 12/25 | Loss: 0.00164409
Iteration 13/25 | Loss: 0.00162930
Iteration 14/25 | Loss: 0.00163342
Iteration 15/25 | Loss: 0.00163914
Iteration 16/25 | Loss: 0.00162266
Iteration 17/25 | Loss: 0.00162490
Iteration 18/25 | Loss: 0.00161035
Iteration 19/25 | Loss: 0.00160940
Iteration 20/25 | Loss: 0.00160781
Iteration 21/25 | Loss: 0.00161025
Iteration 22/25 | Loss: 0.00160384
Iteration 23/25 | Loss: 0.00160143
Iteration 24/25 | Loss: 0.00160128
Iteration 25/25 | Loss: 0.00160832

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.56517792
Iteration 2/25 | Loss: 0.01604043
Iteration 3/25 | Loss: 0.00795188
Iteration 4/25 | Loss: 0.00795188
Iteration 5/25 | Loss: 0.00795187
Iteration 6/25 | Loss: 0.00795187
Iteration 7/25 | Loss: 0.00795187
Iteration 8/25 | Loss: 0.00795187
Iteration 9/25 | Loss: 0.00795187
Iteration 10/25 | Loss: 0.00795187
Iteration 11/25 | Loss: 0.00795187
Iteration 12/25 | Loss: 0.00795187
Iteration 13/25 | Loss: 0.00795187
Iteration 14/25 | Loss: 0.00795187
Iteration 15/25 | Loss: 0.00795187
Iteration 16/25 | Loss: 0.00795187
Iteration 17/25 | Loss: 0.00795187
Iteration 18/25 | Loss: 0.00795187
Iteration 19/25 | Loss: 0.00795187
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.007951870560646057, 0.007951870560646057, 0.007951870560646057, 0.007951870560646057, 0.007951870560646057]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.007951870560646057

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00795187
Iteration 2/1000 | Loss: 0.00665017
Iteration 3/1000 | Loss: 0.00165142
Iteration 4/1000 | Loss: 0.00046290
Iteration 5/1000 | Loss: 0.00168095
Iteration 6/1000 | Loss: 0.00284648
Iteration 7/1000 | Loss: 0.00216555
Iteration 8/1000 | Loss: 0.00979621
Iteration 9/1000 | Loss: 0.00221441
Iteration 10/1000 | Loss: 0.00548762
Iteration 11/1000 | Loss: 0.00165547
Iteration 12/1000 | Loss: 0.00057479
Iteration 13/1000 | Loss: 0.00111961
Iteration 14/1000 | Loss: 0.00019260
Iteration 15/1000 | Loss: 0.00108513
Iteration 16/1000 | Loss: 0.00058388
Iteration 17/1000 | Loss: 0.00051339
Iteration 18/1000 | Loss: 0.00007144
Iteration 19/1000 | Loss: 0.00155629
Iteration 20/1000 | Loss: 0.00201101
Iteration 21/1000 | Loss: 0.00046315
Iteration 22/1000 | Loss: 0.00119143
Iteration 23/1000 | Loss: 0.00005846
Iteration 24/1000 | Loss: 0.00004264
Iteration 25/1000 | Loss: 0.00003586
Iteration 26/1000 | Loss: 0.00003094
Iteration 27/1000 | Loss: 0.00072307
Iteration 28/1000 | Loss: 0.00003070
Iteration 29/1000 | Loss: 0.00059665
Iteration 30/1000 | Loss: 0.00010784
Iteration 31/1000 | Loss: 0.00016698
Iteration 32/1000 | Loss: 0.00003180
Iteration 33/1000 | Loss: 0.00004602
Iteration 34/1000 | Loss: 0.00002341
Iteration 35/1000 | Loss: 0.00002748
Iteration 36/1000 | Loss: 0.00003094
Iteration 37/1000 | Loss: 0.00002629
Iteration 38/1000 | Loss: 0.00002247
Iteration 39/1000 | Loss: 0.00002011
Iteration 40/1000 | Loss: 0.00002410
Iteration 41/1000 | Loss: 0.00038289
Iteration 42/1000 | Loss: 0.00003028
Iteration 43/1000 | Loss: 0.00002829
Iteration 44/1000 | Loss: 0.00003436
Iteration 45/1000 | Loss: 0.00003883
Iteration 46/1000 | Loss: 0.00044354
Iteration 47/1000 | Loss: 0.00002876
Iteration 48/1000 | Loss: 0.00002195
Iteration 49/1000 | Loss: 0.00002472
Iteration 50/1000 | Loss: 0.00003575
Iteration 51/1000 | Loss: 0.00004351
Iteration 52/1000 | Loss: 0.00003663
Iteration 53/1000 | Loss: 0.00003290
Iteration 54/1000 | Loss: 0.00003784
Iteration 55/1000 | Loss: 0.00003270
Iteration 56/1000 | Loss: 0.00003891
Iteration 57/1000 | Loss: 0.00004199
Iteration 58/1000 | Loss: 0.00002545
Iteration 59/1000 | Loss: 0.00003747
Iteration 60/1000 | Loss: 0.00003309
Iteration 61/1000 | Loss: 0.00003684
Iteration 62/1000 | Loss: 0.00003380
Iteration 63/1000 | Loss: 0.00003639
Iteration 64/1000 | Loss: 0.00003389
Iteration 65/1000 | Loss: 0.00003562
Iteration 66/1000 | Loss: 0.00003397
Iteration 67/1000 | Loss: 0.00003721
Iteration 68/1000 | Loss: 0.00004620
Iteration 69/1000 | Loss: 0.00003871
Iteration 70/1000 | Loss: 0.00004378
Iteration 71/1000 | Loss: 0.00002474
Iteration 72/1000 | Loss: 0.00004040
Iteration 73/1000 | Loss: 0.00002985
Iteration 74/1000 | Loss: 0.00003993
Iteration 75/1000 | Loss: 0.00003017
Iteration 76/1000 | Loss: 0.00002810
Iteration 77/1000 | Loss: 0.00003499
Iteration 78/1000 | Loss: 0.00003423
Iteration 79/1000 | Loss: 0.00003425
Iteration 80/1000 | Loss: 0.00003519
Iteration 81/1000 | Loss: 0.00003640
Iteration 82/1000 | Loss: 0.00003163
Iteration 83/1000 | Loss: 0.00003237
Iteration 84/1000 | Loss: 0.00003364
Iteration 85/1000 | Loss: 0.00003872
Iteration 86/1000 | Loss: 0.00003361
Iteration 87/1000 | Loss: 0.00003318
Iteration 88/1000 | Loss: 0.00003299
Iteration 89/1000 | Loss: 0.00003909
Iteration 90/1000 | Loss: 0.00004475
Iteration 91/1000 | Loss: 0.00044217
Iteration 92/1000 | Loss: 0.00023071
Iteration 93/1000 | Loss: 0.00007172
Iteration 94/1000 | Loss: 0.00003446
Iteration 95/1000 | Loss: 0.00003513
Iteration 96/1000 | Loss: 0.00027014
Iteration 97/1000 | Loss: 0.00040132
Iteration 98/1000 | Loss: 0.00003677
Iteration 99/1000 | Loss: 0.00003645
Iteration 100/1000 | Loss: 0.00003089
Iteration 101/1000 | Loss: 0.00003314
Iteration 102/1000 | Loss: 0.00003685
Iteration 103/1000 | Loss: 0.00003783
Iteration 104/1000 | Loss: 0.00003625
Iteration 105/1000 | Loss: 0.00003463
Iteration 106/1000 | Loss: 0.00003432
Iteration 107/1000 | Loss: 0.00003305
Iteration 108/1000 | Loss: 0.00034709
Iteration 109/1000 | Loss: 0.00045724
Iteration 110/1000 | Loss: 0.00003313
Iteration 111/1000 | Loss: 0.00003436
Iteration 112/1000 | Loss: 0.00003470
Iteration 113/1000 | Loss: 0.00003881
Iteration 114/1000 | Loss: 0.00003412
Iteration 115/1000 | Loss: 0.00003842
Iteration 116/1000 | Loss: 0.00003403
Iteration 117/1000 | Loss: 0.00003862
Iteration 118/1000 | Loss: 0.00003524
Iteration 119/1000 | Loss: 0.00003710
Iteration 120/1000 | Loss: 0.00002722
Iteration 121/1000 | Loss: 0.00034634
Iteration 122/1000 | Loss: 0.00031347
Iteration 123/1000 | Loss: 0.00005389
Iteration 124/1000 | Loss: 0.00003285
Iteration 125/1000 | Loss: 0.00032270
Iteration 126/1000 | Loss: 0.00003453
Iteration 127/1000 | Loss: 0.00004275
Iteration 128/1000 | Loss: 0.00003491
Iteration 129/1000 | Loss: 0.00003326
Iteration 130/1000 | Loss: 0.00003486
Iteration 131/1000 | Loss: 0.00003230
Iteration 132/1000 | Loss: 0.00003420
Iteration 133/1000 | Loss: 0.00003189
Iteration 134/1000 | Loss: 0.00003404
Iteration 135/1000 | Loss: 0.00003194
Iteration 136/1000 | Loss: 0.00003783
Iteration 137/1000 | Loss: 0.00003737
Iteration 138/1000 | Loss: 0.00004183
Iteration 139/1000 | Loss: 0.00003419
Iteration 140/1000 | Loss: 0.00003870
Iteration 141/1000 | Loss: 0.00003846
Iteration 142/1000 | Loss: 0.00004098
Iteration 143/1000 | Loss: 0.00003177
Iteration 144/1000 | Loss: 0.00003204
Iteration 145/1000 | Loss: 0.00003879
Iteration 146/1000 | Loss: 0.00003755
Iteration 147/1000 | Loss: 0.00003446
Iteration 148/1000 | Loss: 0.00003501
Iteration 149/1000 | Loss: 0.00003318
Iteration 150/1000 | Loss: 0.00003462
Iteration 151/1000 | Loss: 0.00003322
Iteration 152/1000 | Loss: 0.00003865
Iteration 153/1000 | Loss: 0.00003317
Iteration 154/1000 | Loss: 0.00036223
Iteration 155/1000 | Loss: 0.00004334
Iteration 156/1000 | Loss: 0.00003643
Iteration 157/1000 | Loss: 0.00003781
Iteration 158/1000 | Loss: 0.00003184
Iteration 159/1000 | Loss: 0.00003741
Iteration 160/1000 | Loss: 0.00002896
Iteration 161/1000 | Loss: 0.00005057
Iteration 162/1000 | Loss: 0.00003491
Iteration 163/1000 | Loss: 0.00003388
Iteration 164/1000 | Loss: 0.00035353
Iteration 165/1000 | Loss: 0.00005885
Iteration 166/1000 | Loss: 0.00003391
Iteration 167/1000 | Loss: 0.00003819
Iteration 168/1000 | Loss: 0.00003376
Iteration 169/1000 | Loss: 0.00030134
Iteration 170/1000 | Loss: 0.00003347
Iteration 171/1000 | Loss: 0.00003560
Iteration 172/1000 | Loss: 0.00003477
Iteration 173/1000 | Loss: 0.00003817
Iteration 174/1000 | Loss: 0.00003458
Iteration 175/1000 | Loss: 0.00029252
Iteration 176/1000 | Loss: 0.00006600
Iteration 177/1000 | Loss: 0.00003764
Iteration 178/1000 | Loss: 0.00003619
Iteration 179/1000 | Loss: 0.00010727
Iteration 180/1000 | Loss: 0.00005353
Iteration 181/1000 | Loss: 0.00002907
Iteration 182/1000 | Loss: 0.00003629
Iteration 183/1000 | Loss: 0.00003479
Iteration 184/1000 | Loss: 0.00003528
Iteration 185/1000 | Loss: 0.00003463
Iteration 186/1000 | Loss: 0.00002349
Iteration 187/1000 | Loss: 0.00003611
Iteration 188/1000 | Loss: 0.00003572
Iteration 189/1000 | Loss: 0.00003682
Iteration 190/1000 | Loss: 0.00016660
Iteration 191/1000 | Loss: 0.00042368
Iteration 192/1000 | Loss: 0.00003738
Iteration 193/1000 | Loss: 0.00003285
Iteration 194/1000 | Loss: 0.00003522
Iteration 195/1000 | Loss: 0.00003351
Iteration 196/1000 | Loss: 0.00002411
Iteration 197/1000 | Loss: 0.00027239
Iteration 198/1000 | Loss: 0.00001982
Iteration 199/1000 | Loss: 0.00010255
Iteration 200/1000 | Loss: 0.00001846
Iteration 201/1000 | Loss: 0.00001775
Iteration 202/1000 | Loss: 0.00001715
Iteration 203/1000 | Loss: 0.00001698
Iteration 204/1000 | Loss: 0.00001684
Iteration 205/1000 | Loss: 0.00001679
Iteration 206/1000 | Loss: 0.00001678
Iteration 207/1000 | Loss: 0.00001676
Iteration 208/1000 | Loss: 0.00001676
Iteration 209/1000 | Loss: 0.00001676
Iteration 210/1000 | Loss: 0.00001675
Iteration 211/1000 | Loss: 0.00001675
Iteration 212/1000 | Loss: 0.00001675
Iteration 213/1000 | Loss: 0.00001674
Iteration 214/1000 | Loss: 0.00001674
Iteration 215/1000 | Loss: 0.00001673
Iteration 216/1000 | Loss: 0.00001673
Iteration 217/1000 | Loss: 0.00001673
Iteration 218/1000 | Loss: 0.00001673
Iteration 219/1000 | Loss: 0.00001672
Iteration 220/1000 | Loss: 0.00001672
Iteration 221/1000 | Loss: 0.00001672
Iteration 222/1000 | Loss: 0.00001672
Iteration 223/1000 | Loss: 0.00001671
Iteration 224/1000 | Loss: 0.00001671
Iteration 225/1000 | Loss: 0.00001671
Iteration 226/1000 | Loss: 0.00001671
Iteration 227/1000 | Loss: 0.00001670
Iteration 228/1000 | Loss: 0.00001670
Iteration 229/1000 | Loss: 0.00001670
Iteration 230/1000 | Loss: 0.00001670
Iteration 231/1000 | Loss: 0.00001670
Iteration 232/1000 | Loss: 0.00001670
Iteration 233/1000 | Loss: 0.00001670
Iteration 234/1000 | Loss: 0.00001669
Iteration 235/1000 | Loss: 0.00001669
Iteration 236/1000 | Loss: 0.00001669
Iteration 237/1000 | Loss: 0.00001669
Iteration 238/1000 | Loss: 0.00001669
Iteration 239/1000 | Loss: 0.00001669
Iteration 240/1000 | Loss: 0.00001668
Iteration 241/1000 | Loss: 0.00001668
Iteration 242/1000 | Loss: 0.00001668
Iteration 243/1000 | Loss: 0.00001668
Iteration 244/1000 | Loss: 0.00001668
Iteration 245/1000 | Loss: 0.00001668
Iteration 246/1000 | Loss: 0.00001668
Iteration 247/1000 | Loss: 0.00001667
Iteration 248/1000 | Loss: 0.00001667
Iteration 249/1000 | Loss: 0.00001667
Iteration 250/1000 | Loss: 0.00001667
Iteration 251/1000 | Loss: 0.00001667
Iteration 252/1000 | Loss: 0.00001667
Iteration 253/1000 | Loss: 0.00001667
Iteration 254/1000 | Loss: 0.00001667
Iteration 255/1000 | Loss: 0.00001667
Iteration 256/1000 | Loss: 0.00001667
Iteration 257/1000 | Loss: 0.00001667
Iteration 258/1000 | Loss: 0.00001667
Iteration 259/1000 | Loss: 0.00001667
Iteration 260/1000 | Loss: 0.00001667
Iteration 261/1000 | Loss: 0.00001667
Iteration 262/1000 | Loss: 0.00001667
Iteration 263/1000 | Loss: 0.00001667
Iteration 264/1000 | Loss: 0.00001667
Iteration 265/1000 | Loss: 0.00001667
Iteration 266/1000 | Loss: 0.00001667
Iteration 267/1000 | Loss: 0.00001667
Iteration 268/1000 | Loss: 0.00001667
Iteration 269/1000 | Loss: 0.00001666
Iteration 270/1000 | Loss: 0.00001666
Iteration 271/1000 | Loss: 0.00001666
Iteration 272/1000 | Loss: 0.00001666
Iteration 273/1000 | Loss: 0.00001666
Iteration 274/1000 | Loss: 0.00001666
Iteration 275/1000 | Loss: 0.00001666
Iteration 276/1000 | Loss: 0.00001666
Iteration 277/1000 | Loss: 0.00001666
Iteration 278/1000 | Loss: 0.00001666
Iteration 279/1000 | Loss: 0.00001666
Iteration 280/1000 | Loss: 0.00001666
Iteration 281/1000 | Loss: 0.00001666
Iteration 282/1000 | Loss: 0.00001666
Iteration 283/1000 | Loss: 0.00001665
Iteration 284/1000 | Loss: 0.00001665
Iteration 285/1000 | Loss: 0.00001665
Iteration 286/1000 | Loss: 0.00001665
Iteration 287/1000 | Loss: 0.00001665
Iteration 288/1000 | Loss: 0.00001664
Iteration 289/1000 | Loss: 0.00001664
Iteration 290/1000 | Loss: 0.00001664
Iteration 291/1000 | Loss: 0.00001664
Iteration 292/1000 | Loss: 0.00001664
Iteration 293/1000 | Loss: 0.00001664
Iteration 294/1000 | Loss: 0.00001664
Iteration 295/1000 | Loss: 0.00001664
Iteration 296/1000 | Loss: 0.00001664
Iteration 297/1000 | Loss: 0.00001664
Iteration 298/1000 | Loss: 0.00001663
Iteration 299/1000 | Loss: 0.00001663
Iteration 300/1000 | Loss: 0.00001663
Iteration 301/1000 | Loss: 0.00001663
Iteration 302/1000 | Loss: 0.00001663
Iteration 303/1000 | Loss: 0.00001663
Iteration 304/1000 | Loss: 0.00001663
Iteration 305/1000 | Loss: 0.00001663
Iteration 306/1000 | Loss: 0.00001663
Iteration 307/1000 | Loss: 0.00001663
Iteration 308/1000 | Loss: 0.00001662
Iteration 309/1000 | Loss: 0.00001662
Iteration 310/1000 | Loss: 0.00001662
Iteration 311/1000 | Loss: 0.00001662
Iteration 312/1000 | Loss: 0.00001662
Iteration 313/1000 | Loss: 0.00001662
Iteration 314/1000 | Loss: 0.00001662
Iteration 315/1000 | Loss: 0.00001662
Iteration 316/1000 | Loss: 0.00001662
Iteration 317/1000 | Loss: 0.00001662
Iteration 318/1000 | Loss: 0.00001662
Iteration 319/1000 | Loss: 0.00001662
Iteration 320/1000 | Loss: 0.00001662
Iteration 321/1000 | Loss: 0.00001662
Iteration 322/1000 | Loss: 0.00001661
Iteration 323/1000 | Loss: 0.00001661
Iteration 324/1000 | Loss: 0.00001661
Iteration 325/1000 | Loss: 0.00001661
Iteration 326/1000 | Loss: 0.00001661
Iteration 327/1000 | Loss: 0.00001661
Iteration 328/1000 | Loss: 0.00001661
Iteration 329/1000 | Loss: 0.00001660
Iteration 330/1000 | Loss: 0.00001660
Iteration 331/1000 | Loss: 0.00001660
Iteration 332/1000 | Loss: 0.00001660
Iteration 333/1000 | Loss: 0.00001660
Iteration 334/1000 | Loss: 0.00001660
Iteration 335/1000 | Loss: 0.00001660
Iteration 336/1000 | Loss: 0.00001660
Iteration 337/1000 | Loss: 0.00001660
Iteration 338/1000 | Loss: 0.00001660
Iteration 339/1000 | Loss: 0.00001660
Iteration 340/1000 | Loss: 0.00001660
Iteration 341/1000 | Loss: 0.00001660
Iteration 342/1000 | Loss: 0.00001660
Iteration 343/1000 | Loss: 0.00001660
Iteration 344/1000 | Loss: 0.00001659
Iteration 345/1000 | Loss: 0.00001659
Iteration 346/1000 | Loss: 0.00001659
Iteration 347/1000 | Loss: 0.00001659
Iteration 348/1000 | Loss: 0.00001659
Iteration 349/1000 | Loss: 0.00001659
Iteration 350/1000 | Loss: 0.00001659
Iteration 351/1000 | Loss: 0.00001659
Iteration 352/1000 | Loss: 0.00001659
Iteration 353/1000 | Loss: 0.00001659
Iteration 354/1000 | Loss: 0.00001659
Iteration 355/1000 | Loss: 0.00001659
Iteration 356/1000 | Loss: 0.00001659
Iteration 357/1000 | Loss: 0.00001659
Iteration 358/1000 | Loss: 0.00001659
Iteration 359/1000 | Loss: 0.00001659
Iteration 360/1000 | Loss: 0.00001659
Iteration 361/1000 | Loss: 0.00001659
Iteration 362/1000 | Loss: 0.00001659
Iteration 363/1000 | Loss: 0.00001659
Iteration 364/1000 | Loss: 0.00001659
Iteration 365/1000 | Loss: 0.00001659
Iteration 366/1000 | Loss: 0.00001659
Iteration 367/1000 | Loss: 0.00001659
Iteration 368/1000 | Loss: 0.00001659
Iteration 369/1000 | Loss: 0.00001658
Iteration 370/1000 | Loss: 0.00001658
Iteration 371/1000 | Loss: 0.00001658
Iteration 372/1000 | Loss: 0.00001658
Iteration 373/1000 | Loss: 0.00001658
Iteration 374/1000 | Loss: 0.00001658
Iteration 375/1000 | Loss: 0.00001658
Iteration 376/1000 | Loss: 0.00001658
Iteration 377/1000 | Loss: 0.00001658
Iteration 378/1000 | Loss: 0.00001658
Iteration 379/1000 | Loss: 0.00001658
Iteration 380/1000 | Loss: 0.00001658
Iteration 381/1000 | Loss: 0.00001658
Iteration 382/1000 | Loss: 0.00001658
Iteration 383/1000 | Loss: 0.00001658
Iteration 384/1000 | Loss: 0.00001658
Iteration 385/1000 | Loss: 0.00001658
Iteration 386/1000 | Loss: 0.00001658
Iteration 387/1000 | Loss: 0.00001658
Iteration 388/1000 | Loss: 0.00001658
Iteration 389/1000 | Loss: 0.00001658
Iteration 390/1000 | Loss: 0.00001658
Iteration 391/1000 | Loss: 0.00001658
Iteration 392/1000 | Loss: 0.00001658
Iteration 393/1000 | Loss: 0.00001658
Iteration 394/1000 | Loss: 0.00001658
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 394. Stopping optimization.
Last 5 losses: [1.658488872635644e-05, 1.658488872635644e-05, 1.658488872635644e-05, 1.658488872635644e-05, 1.658488872635644e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.658488872635644e-05

Optimization complete. Final v2v error: 3.4125895500183105 mm

Highest mean error: 3.9968085289001465 mm for frame 21

Lowest mean error: 3.065737009048462 mm for frame 9

Saving results

Total time: 357.0385501384735
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01059296
Iteration 2/25 | Loss: 0.00278522
Iteration 3/25 | Loss: 0.00200524
Iteration 4/25 | Loss: 0.00178665
Iteration 5/25 | Loss: 0.00164078
Iteration 6/25 | Loss: 0.00140737
Iteration 7/25 | Loss: 0.00104077
Iteration 8/25 | Loss: 0.00099800
Iteration 9/25 | Loss: 0.00098943
Iteration 10/25 | Loss: 0.00094230
Iteration 11/25 | Loss: 0.00089485
Iteration 12/25 | Loss: 0.00086904
Iteration 13/25 | Loss: 0.00086249
Iteration 14/25 | Loss: 0.00086125
Iteration 15/25 | Loss: 0.00086070
Iteration 16/25 | Loss: 0.00086053
Iteration 17/25 | Loss: 0.00086053
Iteration 18/25 | Loss: 0.00086053
Iteration 19/25 | Loss: 0.00086053
Iteration 20/25 | Loss: 0.00086053
Iteration 21/25 | Loss: 0.00086053
Iteration 22/25 | Loss: 0.00086053
Iteration 23/25 | Loss: 0.00086053
Iteration 24/25 | Loss: 0.00086053
Iteration 25/25 | Loss: 0.00086053

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55360031
Iteration 2/25 | Loss: 0.00041845
Iteration 3/25 | Loss: 0.00041844
Iteration 4/25 | Loss: 0.00041844
Iteration 5/25 | Loss: 0.00041844
Iteration 6/25 | Loss: 0.00041844
Iteration 7/25 | Loss: 0.00041844
Iteration 8/25 | Loss: 0.00041844
Iteration 9/25 | Loss: 0.00041844
Iteration 10/25 | Loss: 0.00041844
Iteration 11/25 | Loss: 0.00041844
Iteration 12/25 | Loss: 0.00041844
Iteration 13/25 | Loss: 0.00041844
Iteration 14/25 | Loss: 0.00041844
Iteration 15/25 | Loss: 0.00041844
Iteration 16/25 | Loss: 0.00041844
Iteration 17/25 | Loss: 0.00041844
Iteration 18/25 | Loss: 0.00041844
Iteration 19/25 | Loss: 0.00041844
Iteration 20/25 | Loss: 0.00041844
Iteration 21/25 | Loss: 0.00041844
Iteration 22/25 | Loss: 0.00041844
Iteration 23/25 | Loss: 0.00041844
Iteration 24/25 | Loss: 0.00041844
Iteration 25/25 | Loss: 0.00041844

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041844
Iteration 2/1000 | Loss: 0.00003105
Iteration 3/1000 | Loss: 0.00002399
Iteration 4/1000 | Loss: 0.00002191
Iteration 5/1000 | Loss: 0.00002091
Iteration 6/1000 | Loss: 0.00002014
Iteration 7/1000 | Loss: 0.00001973
Iteration 8/1000 | Loss: 0.00001941
Iteration 9/1000 | Loss: 0.00001928
Iteration 10/1000 | Loss: 0.00001925
Iteration 11/1000 | Loss: 0.00001924
Iteration 12/1000 | Loss: 0.00001918
Iteration 13/1000 | Loss: 0.00001915
Iteration 14/1000 | Loss: 0.00001914
Iteration 15/1000 | Loss: 0.00001912
Iteration 16/1000 | Loss: 0.00001912
Iteration 17/1000 | Loss: 0.00001912
Iteration 18/1000 | Loss: 0.00001911
Iteration 19/1000 | Loss: 0.00001911
Iteration 20/1000 | Loss: 0.00001910
Iteration 21/1000 | Loss: 0.00001910
Iteration 22/1000 | Loss: 0.00001910
Iteration 23/1000 | Loss: 0.00001910
Iteration 24/1000 | Loss: 0.00001909
Iteration 25/1000 | Loss: 0.00001909
Iteration 26/1000 | Loss: 0.00001908
Iteration 27/1000 | Loss: 0.00001908
Iteration 28/1000 | Loss: 0.00001905
Iteration 29/1000 | Loss: 0.00001904
Iteration 30/1000 | Loss: 0.00001904
Iteration 31/1000 | Loss: 0.00001904
Iteration 32/1000 | Loss: 0.00001904
Iteration 33/1000 | Loss: 0.00001904
Iteration 34/1000 | Loss: 0.00001904
Iteration 35/1000 | Loss: 0.00001904
Iteration 36/1000 | Loss: 0.00001903
Iteration 37/1000 | Loss: 0.00001903
Iteration 38/1000 | Loss: 0.00001903
Iteration 39/1000 | Loss: 0.00001902
Iteration 40/1000 | Loss: 0.00001902
Iteration 41/1000 | Loss: 0.00001902
Iteration 42/1000 | Loss: 0.00001901
Iteration 43/1000 | Loss: 0.00001901
Iteration 44/1000 | Loss: 0.00001901
Iteration 45/1000 | Loss: 0.00001901
Iteration 46/1000 | Loss: 0.00001901
Iteration 47/1000 | Loss: 0.00001899
Iteration 48/1000 | Loss: 0.00001898
Iteration 49/1000 | Loss: 0.00001898
Iteration 50/1000 | Loss: 0.00001898
Iteration 51/1000 | Loss: 0.00001898
Iteration 52/1000 | Loss: 0.00001898
Iteration 53/1000 | Loss: 0.00001897
Iteration 54/1000 | Loss: 0.00001897
Iteration 55/1000 | Loss: 0.00001897
Iteration 56/1000 | Loss: 0.00001897
Iteration 57/1000 | Loss: 0.00001896
Iteration 58/1000 | Loss: 0.00001896
Iteration 59/1000 | Loss: 0.00001896
Iteration 60/1000 | Loss: 0.00001896
Iteration 61/1000 | Loss: 0.00001896
Iteration 62/1000 | Loss: 0.00001896
Iteration 63/1000 | Loss: 0.00001896
Iteration 64/1000 | Loss: 0.00001896
Iteration 65/1000 | Loss: 0.00001896
Iteration 66/1000 | Loss: 0.00001896
Iteration 67/1000 | Loss: 0.00001896
Iteration 68/1000 | Loss: 0.00001896
Iteration 69/1000 | Loss: 0.00001896
Iteration 70/1000 | Loss: 0.00001895
Iteration 71/1000 | Loss: 0.00001895
Iteration 72/1000 | Loss: 0.00001895
Iteration 73/1000 | Loss: 0.00001895
Iteration 74/1000 | Loss: 0.00001895
Iteration 75/1000 | Loss: 0.00001894
Iteration 76/1000 | Loss: 0.00001894
Iteration 77/1000 | Loss: 0.00001894
Iteration 78/1000 | Loss: 0.00001894
Iteration 79/1000 | Loss: 0.00001894
Iteration 80/1000 | Loss: 0.00001894
Iteration 81/1000 | Loss: 0.00001894
Iteration 82/1000 | Loss: 0.00001894
Iteration 83/1000 | Loss: 0.00001894
Iteration 84/1000 | Loss: 0.00001894
Iteration 85/1000 | Loss: 0.00001893
Iteration 86/1000 | Loss: 0.00001893
Iteration 87/1000 | Loss: 0.00001893
Iteration 88/1000 | Loss: 0.00001893
Iteration 89/1000 | Loss: 0.00001893
Iteration 90/1000 | Loss: 0.00001893
Iteration 91/1000 | Loss: 0.00001893
Iteration 92/1000 | Loss: 0.00001893
Iteration 93/1000 | Loss: 0.00001893
Iteration 94/1000 | Loss: 0.00001893
Iteration 95/1000 | Loss: 0.00001893
Iteration 96/1000 | Loss: 0.00001893
Iteration 97/1000 | Loss: 0.00001893
Iteration 98/1000 | Loss: 0.00001893
Iteration 99/1000 | Loss: 0.00001893
Iteration 100/1000 | Loss: 0.00001893
Iteration 101/1000 | Loss: 0.00001893
Iteration 102/1000 | Loss: 0.00001893
Iteration 103/1000 | Loss: 0.00001893
Iteration 104/1000 | Loss: 0.00001893
Iteration 105/1000 | Loss: 0.00001893
Iteration 106/1000 | Loss: 0.00001893
Iteration 107/1000 | Loss: 0.00001893
Iteration 108/1000 | Loss: 0.00001893
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [1.8933877072413452e-05, 1.8933877072413452e-05, 1.8933877072413452e-05, 1.8933877072413452e-05, 1.8933877072413452e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8933877072413452e-05

Optimization complete. Final v2v error: 3.671581506729126 mm

Highest mean error: 3.858596086502075 mm for frame 146

Lowest mean error: 3.5052781105041504 mm for frame 167

Saving results

Total time: 48.50055480003357
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00382233
Iteration 2/25 | Loss: 0.00101249
Iteration 3/25 | Loss: 0.00084034
Iteration 4/25 | Loss: 0.00079339
Iteration 5/25 | Loss: 0.00078020
Iteration 6/25 | Loss: 0.00077682
Iteration 7/25 | Loss: 0.00077594
Iteration 8/25 | Loss: 0.00077574
Iteration 9/25 | Loss: 0.00077574
Iteration 10/25 | Loss: 0.00077574
Iteration 11/25 | Loss: 0.00077573
Iteration 12/25 | Loss: 0.00077573
Iteration 13/25 | Loss: 0.00077573
Iteration 14/25 | Loss: 0.00077573
Iteration 15/25 | Loss: 0.00077573
Iteration 16/25 | Loss: 0.00077573
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007757282583042979, 0.0007757282583042979, 0.0007757282583042979, 0.0007757282583042979, 0.0007757282583042979]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007757282583042979

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47235394
Iteration 2/25 | Loss: 0.00050482
Iteration 3/25 | Loss: 0.00050482
Iteration 4/25 | Loss: 0.00050482
Iteration 5/25 | Loss: 0.00050482
Iteration 6/25 | Loss: 0.00050481
Iteration 7/25 | Loss: 0.00050481
Iteration 8/25 | Loss: 0.00050481
Iteration 9/25 | Loss: 0.00050481
Iteration 10/25 | Loss: 0.00050481
Iteration 11/25 | Loss: 0.00050481
Iteration 12/25 | Loss: 0.00050481
Iteration 13/25 | Loss: 0.00050481
Iteration 14/25 | Loss: 0.00050481
Iteration 15/25 | Loss: 0.00050481
Iteration 16/25 | Loss: 0.00050481
Iteration 17/25 | Loss: 0.00050481
Iteration 18/25 | Loss: 0.00050481
Iteration 19/25 | Loss: 0.00050481
Iteration 20/25 | Loss: 0.00050481
Iteration 21/25 | Loss: 0.00050481
Iteration 22/25 | Loss: 0.00050481
Iteration 23/25 | Loss: 0.00050481
Iteration 24/25 | Loss: 0.00050481
Iteration 25/25 | Loss: 0.00050481

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050481
Iteration 2/1000 | Loss: 0.00005133
Iteration 3/1000 | Loss: 0.00003736
Iteration 4/1000 | Loss: 0.00003042
Iteration 5/1000 | Loss: 0.00002821
Iteration 6/1000 | Loss: 0.00002665
Iteration 7/1000 | Loss: 0.00002552
Iteration 8/1000 | Loss: 0.00002478
Iteration 9/1000 | Loss: 0.00002421
Iteration 10/1000 | Loss: 0.00002375
Iteration 11/1000 | Loss: 0.00002343
Iteration 12/1000 | Loss: 0.00002317
Iteration 13/1000 | Loss: 0.00002300
Iteration 14/1000 | Loss: 0.00002292
Iteration 15/1000 | Loss: 0.00002284
Iteration 16/1000 | Loss: 0.00002279
Iteration 17/1000 | Loss: 0.00002263
Iteration 18/1000 | Loss: 0.00002253
Iteration 19/1000 | Loss: 0.00002247
Iteration 20/1000 | Loss: 0.00002247
Iteration 21/1000 | Loss: 0.00002244
Iteration 22/1000 | Loss: 0.00002244
Iteration 23/1000 | Loss: 0.00002243
Iteration 24/1000 | Loss: 0.00002242
Iteration 25/1000 | Loss: 0.00002238
Iteration 26/1000 | Loss: 0.00002238
Iteration 27/1000 | Loss: 0.00002237
Iteration 28/1000 | Loss: 0.00002236
Iteration 29/1000 | Loss: 0.00002236
Iteration 30/1000 | Loss: 0.00002235
Iteration 31/1000 | Loss: 0.00002235
Iteration 32/1000 | Loss: 0.00002235
Iteration 33/1000 | Loss: 0.00002234
Iteration 34/1000 | Loss: 0.00002234
Iteration 35/1000 | Loss: 0.00002234
Iteration 36/1000 | Loss: 0.00002233
Iteration 37/1000 | Loss: 0.00002233
Iteration 38/1000 | Loss: 0.00002232
Iteration 39/1000 | Loss: 0.00002232
Iteration 40/1000 | Loss: 0.00002232
Iteration 41/1000 | Loss: 0.00002232
Iteration 42/1000 | Loss: 0.00002231
Iteration 43/1000 | Loss: 0.00002231
Iteration 44/1000 | Loss: 0.00002230
Iteration 45/1000 | Loss: 0.00002230
Iteration 46/1000 | Loss: 0.00002230
Iteration 47/1000 | Loss: 0.00002229
Iteration 48/1000 | Loss: 0.00002229
Iteration 49/1000 | Loss: 0.00002229
Iteration 50/1000 | Loss: 0.00002229
Iteration 51/1000 | Loss: 0.00002229
Iteration 52/1000 | Loss: 0.00002229
Iteration 53/1000 | Loss: 0.00002228
Iteration 54/1000 | Loss: 0.00002228
Iteration 55/1000 | Loss: 0.00002228
Iteration 56/1000 | Loss: 0.00002227
Iteration 57/1000 | Loss: 0.00002227
Iteration 58/1000 | Loss: 0.00002227
Iteration 59/1000 | Loss: 0.00002227
Iteration 60/1000 | Loss: 0.00002227
Iteration 61/1000 | Loss: 0.00002226
Iteration 62/1000 | Loss: 0.00002226
Iteration 63/1000 | Loss: 0.00002226
Iteration 64/1000 | Loss: 0.00002225
Iteration 65/1000 | Loss: 0.00002225
Iteration 66/1000 | Loss: 0.00002225
Iteration 67/1000 | Loss: 0.00002224
Iteration 68/1000 | Loss: 0.00002224
Iteration 69/1000 | Loss: 0.00002224
Iteration 70/1000 | Loss: 0.00002223
Iteration 71/1000 | Loss: 0.00002223
Iteration 72/1000 | Loss: 0.00002223
Iteration 73/1000 | Loss: 0.00002223
Iteration 74/1000 | Loss: 0.00002222
Iteration 75/1000 | Loss: 0.00002222
Iteration 76/1000 | Loss: 0.00002222
Iteration 77/1000 | Loss: 0.00002222
Iteration 78/1000 | Loss: 0.00002221
Iteration 79/1000 | Loss: 0.00002221
Iteration 80/1000 | Loss: 0.00002221
Iteration 81/1000 | Loss: 0.00002221
Iteration 82/1000 | Loss: 0.00002221
Iteration 83/1000 | Loss: 0.00002220
Iteration 84/1000 | Loss: 0.00002220
Iteration 85/1000 | Loss: 0.00002220
Iteration 86/1000 | Loss: 0.00002220
Iteration 87/1000 | Loss: 0.00002220
Iteration 88/1000 | Loss: 0.00002219
Iteration 89/1000 | Loss: 0.00002219
Iteration 90/1000 | Loss: 0.00002219
Iteration 91/1000 | Loss: 0.00002219
Iteration 92/1000 | Loss: 0.00002219
Iteration 93/1000 | Loss: 0.00002219
Iteration 94/1000 | Loss: 0.00002219
Iteration 95/1000 | Loss: 0.00002219
Iteration 96/1000 | Loss: 0.00002219
Iteration 97/1000 | Loss: 0.00002218
Iteration 98/1000 | Loss: 0.00002218
Iteration 99/1000 | Loss: 0.00002218
Iteration 100/1000 | Loss: 0.00002218
Iteration 101/1000 | Loss: 0.00002218
Iteration 102/1000 | Loss: 0.00002218
Iteration 103/1000 | Loss: 0.00002218
Iteration 104/1000 | Loss: 0.00002217
Iteration 105/1000 | Loss: 0.00002217
Iteration 106/1000 | Loss: 0.00002217
Iteration 107/1000 | Loss: 0.00002217
Iteration 108/1000 | Loss: 0.00002217
Iteration 109/1000 | Loss: 0.00002217
Iteration 110/1000 | Loss: 0.00002217
Iteration 111/1000 | Loss: 0.00002217
Iteration 112/1000 | Loss: 0.00002217
Iteration 113/1000 | Loss: 0.00002217
Iteration 114/1000 | Loss: 0.00002217
Iteration 115/1000 | Loss: 0.00002217
Iteration 116/1000 | Loss: 0.00002217
Iteration 117/1000 | Loss: 0.00002216
Iteration 118/1000 | Loss: 0.00002216
Iteration 119/1000 | Loss: 0.00002216
Iteration 120/1000 | Loss: 0.00002216
Iteration 121/1000 | Loss: 0.00002216
Iteration 122/1000 | Loss: 0.00002216
Iteration 123/1000 | Loss: 0.00002216
Iteration 124/1000 | Loss: 0.00002216
Iteration 125/1000 | Loss: 0.00002216
Iteration 126/1000 | Loss: 0.00002216
Iteration 127/1000 | Loss: 0.00002216
Iteration 128/1000 | Loss: 0.00002216
Iteration 129/1000 | Loss: 0.00002216
Iteration 130/1000 | Loss: 0.00002216
Iteration 131/1000 | Loss: 0.00002216
Iteration 132/1000 | Loss: 0.00002216
Iteration 133/1000 | Loss: 0.00002216
Iteration 134/1000 | Loss: 0.00002216
Iteration 135/1000 | Loss: 0.00002215
Iteration 136/1000 | Loss: 0.00002215
Iteration 137/1000 | Loss: 0.00002215
Iteration 138/1000 | Loss: 0.00002215
Iteration 139/1000 | Loss: 0.00002215
Iteration 140/1000 | Loss: 0.00002215
Iteration 141/1000 | Loss: 0.00002215
Iteration 142/1000 | Loss: 0.00002215
Iteration 143/1000 | Loss: 0.00002215
Iteration 144/1000 | Loss: 0.00002215
Iteration 145/1000 | Loss: 0.00002215
Iteration 146/1000 | Loss: 0.00002215
Iteration 147/1000 | Loss: 0.00002215
Iteration 148/1000 | Loss: 0.00002215
Iteration 149/1000 | Loss: 0.00002215
Iteration 150/1000 | Loss: 0.00002215
Iteration 151/1000 | Loss: 0.00002215
Iteration 152/1000 | Loss: 0.00002214
Iteration 153/1000 | Loss: 0.00002214
Iteration 154/1000 | Loss: 0.00002214
Iteration 155/1000 | Loss: 0.00002214
Iteration 156/1000 | Loss: 0.00002214
Iteration 157/1000 | Loss: 0.00002214
Iteration 158/1000 | Loss: 0.00002214
Iteration 159/1000 | Loss: 0.00002214
Iteration 160/1000 | Loss: 0.00002214
Iteration 161/1000 | Loss: 0.00002214
Iteration 162/1000 | Loss: 0.00002214
Iteration 163/1000 | Loss: 0.00002214
Iteration 164/1000 | Loss: 0.00002214
Iteration 165/1000 | Loss: 0.00002214
Iteration 166/1000 | Loss: 0.00002214
Iteration 167/1000 | Loss: 0.00002214
Iteration 168/1000 | Loss: 0.00002214
Iteration 169/1000 | Loss: 0.00002214
Iteration 170/1000 | Loss: 0.00002214
Iteration 171/1000 | Loss: 0.00002214
Iteration 172/1000 | Loss: 0.00002214
Iteration 173/1000 | Loss: 0.00002214
Iteration 174/1000 | Loss: 0.00002214
Iteration 175/1000 | Loss: 0.00002214
Iteration 176/1000 | Loss: 0.00002214
Iteration 177/1000 | Loss: 0.00002214
Iteration 178/1000 | Loss: 0.00002214
Iteration 179/1000 | Loss: 0.00002214
Iteration 180/1000 | Loss: 0.00002214
Iteration 181/1000 | Loss: 0.00002214
Iteration 182/1000 | Loss: 0.00002214
Iteration 183/1000 | Loss: 0.00002214
Iteration 184/1000 | Loss: 0.00002214
Iteration 185/1000 | Loss: 0.00002214
Iteration 186/1000 | Loss: 0.00002214
Iteration 187/1000 | Loss: 0.00002214
Iteration 188/1000 | Loss: 0.00002214
Iteration 189/1000 | Loss: 0.00002214
Iteration 190/1000 | Loss: 0.00002214
Iteration 191/1000 | Loss: 0.00002214
Iteration 192/1000 | Loss: 0.00002214
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [2.2144831746118143e-05, 2.2144831746118143e-05, 2.2144831746118143e-05, 2.2144831746118143e-05, 2.2144831746118143e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2144831746118143e-05

Optimization complete. Final v2v error: 3.904695510864258 mm

Highest mean error: 4.30486536026001 mm for frame 91

Lowest mean error: 3.2248833179473877 mm for frame 81

Saving results

Total time: 45.64978909492493
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01028983
Iteration 2/25 | Loss: 0.00186599
Iteration 3/25 | Loss: 0.00127080
Iteration 4/25 | Loss: 0.00124073
Iteration 5/25 | Loss: 0.00130922
Iteration 6/25 | Loss: 0.00136170
Iteration 7/25 | Loss: 0.00125325
Iteration 8/25 | Loss: 0.00116048
Iteration 9/25 | Loss: 0.00110824
Iteration 10/25 | Loss: 0.00113247
Iteration 11/25 | Loss: 0.00101375
Iteration 12/25 | Loss: 0.00099340
Iteration 13/25 | Loss: 0.00098401
Iteration 14/25 | Loss: 0.00097872
Iteration 15/25 | Loss: 0.00098668
Iteration 16/25 | Loss: 0.00099114
Iteration 17/25 | Loss: 0.00097145
Iteration 18/25 | Loss: 0.00096615
Iteration 19/25 | Loss: 0.00095989
Iteration 20/25 | Loss: 0.00095210
Iteration 21/25 | Loss: 0.00095225
Iteration 22/25 | Loss: 0.00094935
Iteration 23/25 | Loss: 0.00095440
Iteration 24/25 | Loss: 0.00095404
Iteration 25/25 | Loss: 0.00095270

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.01623988
Iteration 2/25 | Loss: 0.00092687
Iteration 3/25 | Loss: 0.00092684
Iteration 4/25 | Loss: 0.00092684
Iteration 5/25 | Loss: 0.00092684
Iteration 6/25 | Loss: 0.00092684
Iteration 7/25 | Loss: 0.00092684
Iteration 8/25 | Loss: 0.00092684
Iteration 9/25 | Loss: 0.00092684
Iteration 10/25 | Loss: 0.00092684
Iteration 11/25 | Loss: 0.00092684
Iteration 12/25 | Loss: 0.00092684
Iteration 13/25 | Loss: 0.00092684
Iteration 14/25 | Loss: 0.00092684
Iteration 15/25 | Loss: 0.00092684
Iteration 16/25 | Loss: 0.00092684
Iteration 17/25 | Loss: 0.00092684
Iteration 18/25 | Loss: 0.00092684
Iteration 19/25 | Loss: 0.00092684
Iteration 20/25 | Loss: 0.00092684
Iteration 21/25 | Loss: 0.00092684
Iteration 22/25 | Loss: 0.00092684
Iteration 23/25 | Loss: 0.00092684
Iteration 24/25 | Loss: 0.00092684
Iteration 25/25 | Loss: 0.00092684

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092684
Iteration 2/1000 | Loss: 0.00017031
Iteration 3/1000 | Loss: 0.00088770
Iteration 4/1000 | Loss: 0.00027177
Iteration 5/1000 | Loss: 0.00069690
Iteration 6/1000 | Loss: 0.00097212
Iteration 7/1000 | Loss: 0.00125792
Iteration 8/1000 | Loss: 0.00040980
Iteration 9/1000 | Loss: 0.00188196
Iteration 10/1000 | Loss: 0.00189924
Iteration 11/1000 | Loss: 0.00054101
Iteration 12/1000 | Loss: 0.00022861
Iteration 13/1000 | Loss: 0.00012740
Iteration 14/1000 | Loss: 0.00042662
Iteration 15/1000 | Loss: 0.00027025
Iteration 16/1000 | Loss: 0.00020218
Iteration 17/1000 | Loss: 0.00008449
Iteration 18/1000 | Loss: 0.00040574
Iteration 19/1000 | Loss: 0.00042895
Iteration 20/1000 | Loss: 0.00038795
Iteration 21/1000 | Loss: 0.00015454
Iteration 22/1000 | Loss: 0.00016309
Iteration 23/1000 | Loss: 0.00018097
Iteration 24/1000 | Loss: 0.00035219
Iteration 25/1000 | Loss: 0.00085626
Iteration 26/1000 | Loss: 0.00019679
Iteration 27/1000 | Loss: 0.00015481
Iteration 28/1000 | Loss: 0.00011140
Iteration 29/1000 | Loss: 0.00055674
Iteration 30/1000 | Loss: 0.00114319
Iteration 31/1000 | Loss: 0.00083283
Iteration 32/1000 | Loss: 0.00019557
Iteration 33/1000 | Loss: 0.00115888
Iteration 34/1000 | Loss: 0.00057869
Iteration 35/1000 | Loss: 0.00013204
Iteration 36/1000 | Loss: 0.00007863
Iteration 37/1000 | Loss: 0.00011812
Iteration 38/1000 | Loss: 0.00016499
Iteration 39/1000 | Loss: 0.00142964
Iteration 40/1000 | Loss: 0.00080942
Iteration 41/1000 | Loss: 0.00094303
Iteration 42/1000 | Loss: 0.00059015
Iteration 43/1000 | Loss: 0.00083858
Iteration 44/1000 | Loss: 0.00161548
Iteration 45/1000 | Loss: 0.00051301
Iteration 46/1000 | Loss: 0.00014648
Iteration 47/1000 | Loss: 0.00012843
Iteration 48/1000 | Loss: 0.00176356
Iteration 49/1000 | Loss: 0.00012941
Iteration 50/1000 | Loss: 0.00012763
Iteration 51/1000 | Loss: 0.00012871
Iteration 52/1000 | Loss: 0.00012551
Iteration 53/1000 | Loss: 0.00014417
Iteration 54/1000 | Loss: 0.00074329
Iteration 55/1000 | Loss: 0.00011248
Iteration 56/1000 | Loss: 0.00013406
Iteration 57/1000 | Loss: 0.00013674
Iteration 58/1000 | Loss: 0.00014908
Iteration 59/1000 | Loss: 0.00064360
Iteration 60/1000 | Loss: 0.00053882
Iteration 61/1000 | Loss: 0.00163071
Iteration 62/1000 | Loss: 0.00073000
Iteration 63/1000 | Loss: 0.00052100
Iteration 64/1000 | Loss: 0.00033934
Iteration 65/1000 | Loss: 0.00022686
Iteration 66/1000 | Loss: 0.00012100
Iteration 67/1000 | Loss: 0.00098587
Iteration 68/1000 | Loss: 0.00053850
Iteration 69/1000 | Loss: 0.00068449
Iteration 70/1000 | Loss: 0.00101227
Iteration 71/1000 | Loss: 0.00097882
Iteration 72/1000 | Loss: 0.00093146
Iteration 73/1000 | Loss: 0.00104433
Iteration 74/1000 | Loss: 0.00098137
Iteration 75/1000 | Loss: 0.00052935
Iteration 76/1000 | Loss: 0.00121860
Iteration 77/1000 | Loss: 0.00118654
Iteration 78/1000 | Loss: 0.00222468
Iteration 79/1000 | Loss: 0.00112747
Iteration 80/1000 | Loss: 0.00081377
Iteration 81/1000 | Loss: 0.00121249
Iteration 82/1000 | Loss: 0.00063703
Iteration 83/1000 | Loss: 0.00089023
Iteration 84/1000 | Loss: 0.00015883
Iteration 85/1000 | Loss: 0.00105350
Iteration 86/1000 | Loss: 0.00168398
Iteration 87/1000 | Loss: 0.00112465
Iteration 88/1000 | Loss: 0.00017998
Iteration 89/1000 | Loss: 0.00040155
Iteration 90/1000 | Loss: 0.00020308
Iteration 91/1000 | Loss: 0.00012260
Iteration 92/1000 | Loss: 0.00011146
Iteration 93/1000 | Loss: 0.00011949
Iteration 94/1000 | Loss: 0.00010328
Iteration 95/1000 | Loss: 0.00013749
Iteration 96/1000 | Loss: 0.00111681
Iteration 97/1000 | Loss: 0.00103942
Iteration 98/1000 | Loss: 0.00108228
Iteration 99/1000 | Loss: 0.00028719
Iteration 100/1000 | Loss: 0.00010353
Iteration 101/1000 | Loss: 0.00011475
Iteration 102/1000 | Loss: 0.00071086
Iteration 103/1000 | Loss: 0.00073147
Iteration 104/1000 | Loss: 0.00096194
Iteration 105/1000 | Loss: 0.00018465
Iteration 106/1000 | Loss: 0.00035315
Iteration 107/1000 | Loss: 0.00008965
Iteration 108/1000 | Loss: 0.00040605
Iteration 109/1000 | Loss: 0.00052411
Iteration 110/1000 | Loss: 0.00024636
Iteration 111/1000 | Loss: 0.00011927
Iteration 112/1000 | Loss: 0.00008709
Iteration 113/1000 | Loss: 0.00009229
Iteration 114/1000 | Loss: 0.00009448
Iteration 115/1000 | Loss: 0.00011350
Iteration 116/1000 | Loss: 0.00011117
Iteration 117/1000 | Loss: 0.00010899
Iteration 118/1000 | Loss: 0.00011766
Iteration 119/1000 | Loss: 0.00013033
Iteration 120/1000 | Loss: 0.00011959
Iteration 121/1000 | Loss: 0.00021922
Iteration 122/1000 | Loss: 0.00009592
Iteration 123/1000 | Loss: 0.00009926
Iteration 124/1000 | Loss: 0.00022757
Iteration 125/1000 | Loss: 0.00012129
Iteration 126/1000 | Loss: 0.00011670
Iteration 127/1000 | Loss: 0.00011350
Iteration 128/1000 | Loss: 0.00007035
Iteration 129/1000 | Loss: 0.00008562
Iteration 130/1000 | Loss: 0.00008191
Iteration 131/1000 | Loss: 0.00004753
Iteration 132/1000 | Loss: 0.00008128
Iteration 133/1000 | Loss: 0.00007768
Iteration 134/1000 | Loss: 0.00007919
Iteration 135/1000 | Loss: 0.00011134
Iteration 136/1000 | Loss: 0.00009405
Iteration 137/1000 | Loss: 0.00009294
Iteration 138/1000 | Loss: 0.00010698
Iteration 139/1000 | Loss: 0.00011334
Iteration 140/1000 | Loss: 0.00010358
Iteration 141/1000 | Loss: 0.00009680
Iteration 142/1000 | Loss: 0.00011277
Iteration 143/1000 | Loss: 0.00008372
Iteration 144/1000 | Loss: 0.00010112
Iteration 145/1000 | Loss: 0.00009728
Iteration 146/1000 | Loss: 0.00010539
Iteration 147/1000 | Loss: 0.00009491
Iteration 148/1000 | Loss: 0.00010155
Iteration 149/1000 | Loss: 0.00009195
Iteration 150/1000 | Loss: 0.00010549
Iteration 151/1000 | Loss: 0.00007589
Iteration 152/1000 | Loss: 0.00010803
Iteration 153/1000 | Loss: 0.00008455
Iteration 154/1000 | Loss: 0.00007164
Iteration 155/1000 | Loss: 0.00005981
Iteration 156/1000 | Loss: 0.00007941
Iteration 157/1000 | Loss: 0.00009585
Iteration 158/1000 | Loss: 0.00008065
Iteration 159/1000 | Loss: 0.00006402
Iteration 160/1000 | Loss: 0.00007146
Iteration 161/1000 | Loss: 0.00010700
Iteration 162/1000 | Loss: 0.00008913
Iteration 163/1000 | Loss: 0.00010998
Iteration 164/1000 | Loss: 0.00008701
Iteration 165/1000 | Loss: 0.00009017
Iteration 166/1000 | Loss: 0.00008624
Iteration 167/1000 | Loss: 0.00009018
Iteration 168/1000 | Loss: 0.00009074
Iteration 169/1000 | Loss: 0.00007052
Iteration 170/1000 | Loss: 0.00006299
Iteration 171/1000 | Loss: 0.00006509
Iteration 172/1000 | Loss: 0.00006215
Iteration 173/1000 | Loss: 0.00007815
Iteration 174/1000 | Loss: 0.00008685
Iteration 175/1000 | Loss: 0.00008068
Iteration 176/1000 | Loss: 0.00008994
Iteration 177/1000 | Loss: 0.00008693
Iteration 178/1000 | Loss: 0.00011169
Iteration 179/1000 | Loss: 0.00009073
Iteration 180/1000 | Loss: 0.00009796
Iteration 181/1000 | Loss: 0.00009374
Iteration 182/1000 | Loss: 0.00008536
Iteration 183/1000 | Loss: 0.00009431
Iteration 184/1000 | Loss: 0.00008747
Iteration 185/1000 | Loss: 0.00008877
Iteration 186/1000 | Loss: 0.00008682
Iteration 187/1000 | Loss: 0.00008750
Iteration 188/1000 | Loss: 0.00009022
Iteration 189/1000 | Loss: 0.00009799
Iteration 190/1000 | Loss: 0.00008078
Iteration 191/1000 | Loss: 0.00008918
Iteration 192/1000 | Loss: 0.00009104
Iteration 193/1000 | Loss: 0.00009007
Iteration 194/1000 | Loss: 0.00008902
Iteration 195/1000 | Loss: 0.00009158
Iteration 196/1000 | Loss: 0.00009072
Iteration 197/1000 | Loss: 0.00008376
Iteration 198/1000 | Loss: 0.00008145
Iteration 199/1000 | Loss: 0.00007367
Iteration 200/1000 | Loss: 0.00008156
Iteration 201/1000 | Loss: 0.00008916
Iteration 202/1000 | Loss: 0.00008562
Iteration 203/1000 | Loss: 0.00009479
Iteration 204/1000 | Loss: 0.00009274
Iteration 205/1000 | Loss: 0.00008457
Iteration 206/1000 | Loss: 0.00009291
Iteration 207/1000 | Loss: 0.00009593
Iteration 208/1000 | Loss: 0.00008828
Iteration 209/1000 | Loss: 0.00007345
Iteration 210/1000 | Loss: 0.00008957
Iteration 211/1000 | Loss: 0.00008958
Iteration 212/1000 | Loss: 0.00009514
Iteration 213/1000 | Loss: 0.00006807
Iteration 214/1000 | Loss: 0.00008292
Iteration 215/1000 | Loss: 0.00007337
Iteration 216/1000 | Loss: 0.00006900
Iteration 217/1000 | Loss: 0.00007842
Iteration 218/1000 | Loss: 0.00005829
Iteration 219/1000 | Loss: 0.00009608
Iteration 220/1000 | Loss: 0.00008715
Iteration 221/1000 | Loss: 0.00008865
Iteration 222/1000 | Loss: 0.00009150
Iteration 223/1000 | Loss: 0.00009222
Iteration 224/1000 | Loss: 0.00004910
Iteration 225/1000 | Loss: 0.00006175
Iteration 226/1000 | Loss: 0.00005908
Iteration 227/1000 | Loss: 0.00007634
Iteration 228/1000 | Loss: 0.00006639
Iteration 229/1000 | Loss: 0.00007175
Iteration 230/1000 | Loss: 0.00006241
Iteration 231/1000 | Loss: 0.00007941
Iteration 232/1000 | Loss: 0.00006738
Iteration 233/1000 | Loss: 0.00008792
Iteration 234/1000 | Loss: 0.00008561
Iteration 235/1000 | Loss: 0.00008896
Iteration 236/1000 | Loss: 0.00008153
Iteration 237/1000 | Loss: 0.00005697
Iteration 238/1000 | Loss: 0.00006124
Iteration 239/1000 | Loss: 0.00005400
Iteration 240/1000 | Loss: 0.00005109
Iteration 241/1000 | Loss: 0.00004729
Iteration 242/1000 | Loss: 0.00005001
Iteration 243/1000 | Loss: 0.00006183
Iteration 244/1000 | Loss: 0.00006231
Iteration 245/1000 | Loss: 0.00005734
Iteration 246/1000 | Loss: 0.00005348
Iteration 247/1000 | Loss: 0.00005283
Iteration 248/1000 | Loss: 0.00004801
Iteration 249/1000 | Loss: 0.00006639
Iteration 250/1000 | Loss: 0.00005802
Iteration 251/1000 | Loss: 0.00005924
Iteration 252/1000 | Loss: 0.00005825
Iteration 253/1000 | Loss: 0.00005238
Iteration 254/1000 | Loss: 0.00007126
Iteration 255/1000 | Loss: 0.00005128
Iteration 256/1000 | Loss: 0.00005890
Iteration 257/1000 | Loss: 0.00005362
Iteration 258/1000 | Loss: 0.00006172
Iteration 259/1000 | Loss: 0.00005329
Iteration 260/1000 | Loss: 0.00006254
Iteration 261/1000 | Loss: 0.00005335
Iteration 262/1000 | Loss: 0.00006003
Iteration 263/1000 | Loss: 0.00005313
Iteration 264/1000 | Loss: 0.00005958
Iteration 265/1000 | Loss: 0.00005289
Iteration 266/1000 | Loss: 0.00005802
Iteration 267/1000 | Loss: 0.00006242
Iteration 268/1000 | Loss: 0.00005431
Iteration 269/1000 | Loss: 0.00003228
Iteration 270/1000 | Loss: 0.00005980
Iteration 271/1000 | Loss: 0.00004816
Iteration 272/1000 | Loss: 0.00005588
Iteration 273/1000 | Loss: 0.00006083
Iteration 274/1000 | Loss: 0.00006396
Iteration 275/1000 | Loss: 0.00006245
Iteration 276/1000 | Loss: 0.00006158
Iteration 277/1000 | Loss: 0.00005472
Iteration 278/1000 | Loss: 0.00005684
Iteration 279/1000 | Loss: 0.00006436
Iteration 280/1000 | Loss: 0.00005956
Iteration 281/1000 | Loss: 0.00004835
Iteration 282/1000 | Loss: 0.00004636
Iteration 283/1000 | Loss: 0.00004778
Iteration 284/1000 | Loss: 0.00005586
Iteration 285/1000 | Loss: 0.00005236
Iteration 286/1000 | Loss: 0.00006736
Iteration 287/1000 | Loss: 0.00005202
Iteration 288/1000 | Loss: 0.00005900
Iteration 289/1000 | Loss: 0.00005930
Iteration 290/1000 | Loss: 0.00005446
Iteration 291/1000 | Loss: 0.00005487
Iteration 292/1000 | Loss: 0.00005659
Iteration 293/1000 | Loss: 0.00005128
Iteration 294/1000 | Loss: 0.00005457
Iteration 295/1000 | Loss: 0.00005613
Iteration 296/1000 | Loss: 0.00005677
Iteration 297/1000 | Loss: 0.00005721
Iteration 298/1000 | Loss: 0.00005763
Iteration 299/1000 | Loss: 0.00005728
Iteration 300/1000 | Loss: 0.00005496
Iteration 301/1000 | Loss: 0.00005568
Iteration 302/1000 | Loss: 0.00005573
Iteration 303/1000 | Loss: 0.00005224
Iteration 304/1000 | Loss: 0.00005524
Iteration 305/1000 | Loss: 0.00005225
Iteration 306/1000 | Loss: 0.00005493
Iteration 307/1000 | Loss: 0.00005416
Iteration 308/1000 | Loss: 0.00005618
Iteration 309/1000 | Loss: 0.00005099
Iteration 310/1000 | Loss: 0.00004483
Iteration 311/1000 | Loss: 0.00003808
Iteration 312/1000 | Loss: 0.00005812
Iteration 313/1000 | Loss: 0.00005028
Iteration 314/1000 | Loss: 0.00004777
Iteration 315/1000 | Loss: 0.00004990
Iteration 316/1000 | Loss: 0.00004660
Iteration 317/1000 | Loss: 0.00006538
Iteration 318/1000 | Loss: 0.00005424
Iteration 319/1000 | Loss: 0.00006063
Iteration 320/1000 | Loss: 0.00003929
Iteration 321/1000 | Loss: 0.00003035
Iteration 322/1000 | Loss: 0.00002742
Iteration 323/1000 | Loss: 0.00002562
Iteration 324/1000 | Loss: 0.00002436
Iteration 325/1000 | Loss: 0.00002380
Iteration 326/1000 | Loss: 0.00002327
Iteration 327/1000 | Loss: 0.00002293
Iteration 328/1000 | Loss: 0.00002275
Iteration 329/1000 | Loss: 0.00002259
Iteration 330/1000 | Loss: 0.00002254
Iteration 331/1000 | Loss: 0.00002253
Iteration 332/1000 | Loss: 0.00002252
Iteration 333/1000 | Loss: 0.00002243
Iteration 334/1000 | Loss: 0.00002243
Iteration 335/1000 | Loss: 0.00002238
Iteration 336/1000 | Loss: 0.00002234
Iteration 337/1000 | Loss: 0.00002231
Iteration 338/1000 | Loss: 0.00002231
Iteration 339/1000 | Loss: 0.00002231
Iteration 340/1000 | Loss: 0.00002230
Iteration 341/1000 | Loss: 0.00002229
Iteration 342/1000 | Loss: 0.00002229
Iteration 343/1000 | Loss: 0.00002226
Iteration 344/1000 | Loss: 0.00002225
Iteration 345/1000 | Loss: 0.00002221
Iteration 346/1000 | Loss: 0.00002221
Iteration 347/1000 | Loss: 0.00002221
Iteration 348/1000 | Loss: 0.00002220
Iteration 349/1000 | Loss: 0.00002220
Iteration 350/1000 | Loss: 0.00002220
Iteration 351/1000 | Loss: 0.00002219
Iteration 352/1000 | Loss: 0.00002219
Iteration 353/1000 | Loss: 0.00002219
Iteration 354/1000 | Loss: 0.00002219
Iteration 355/1000 | Loss: 0.00002219
Iteration 356/1000 | Loss: 0.00002219
Iteration 357/1000 | Loss: 0.00002219
Iteration 358/1000 | Loss: 0.00002219
Iteration 359/1000 | Loss: 0.00002218
Iteration 360/1000 | Loss: 0.00002218
Iteration 361/1000 | Loss: 0.00020385
Iteration 362/1000 | Loss: 0.00002763
Iteration 363/1000 | Loss: 0.00002585
Iteration 364/1000 | Loss: 0.00002483
Iteration 365/1000 | Loss: 0.00002410
Iteration 366/1000 | Loss: 0.00002335
Iteration 367/1000 | Loss: 0.00002274
Iteration 368/1000 | Loss: 0.00002247
Iteration 369/1000 | Loss: 0.00002243
Iteration 370/1000 | Loss: 0.00002236
Iteration 371/1000 | Loss: 0.00002235
Iteration 372/1000 | Loss: 0.00002234
Iteration 373/1000 | Loss: 0.00002233
Iteration 374/1000 | Loss: 0.00002233
Iteration 375/1000 | Loss: 0.00002232
Iteration 376/1000 | Loss: 0.00002232
Iteration 377/1000 | Loss: 0.00002231
Iteration 378/1000 | Loss: 0.00002231
Iteration 379/1000 | Loss: 0.00002230
Iteration 380/1000 | Loss: 0.00002230
Iteration 381/1000 | Loss: 0.00002230
Iteration 382/1000 | Loss: 0.00002229
Iteration 383/1000 | Loss: 0.00002229
Iteration 384/1000 | Loss: 0.00002228
Iteration 385/1000 | Loss: 0.00002225
Iteration 386/1000 | Loss: 0.00002225
Iteration 387/1000 | Loss: 0.00002224
Iteration 388/1000 | Loss: 0.00002224
Iteration 389/1000 | Loss: 0.00002224
Iteration 390/1000 | Loss: 0.00002224
Iteration 391/1000 | Loss: 0.00002223
Iteration 392/1000 | Loss: 0.00002223
Iteration 393/1000 | Loss: 0.00002223
Iteration 394/1000 | Loss: 0.00002223
Iteration 395/1000 | Loss: 0.00002223
Iteration 396/1000 | Loss: 0.00002223
Iteration 397/1000 | Loss: 0.00002222
Iteration 398/1000 | Loss: 0.00002222
Iteration 399/1000 | Loss: 0.00002222
Iteration 400/1000 | Loss: 0.00002222
Iteration 401/1000 | Loss: 0.00002222
Iteration 402/1000 | Loss: 0.00002221
Iteration 403/1000 | Loss: 0.00002221
Iteration 404/1000 | Loss: 0.00002221
Iteration 405/1000 | Loss: 0.00002217
Iteration 406/1000 | Loss: 0.00002217
Iteration 407/1000 | Loss: 0.00002217
Iteration 408/1000 | Loss: 0.00002217
Iteration 409/1000 | Loss: 0.00002217
Iteration 410/1000 | Loss: 0.00002216
Iteration 411/1000 | Loss: 0.00002216
Iteration 412/1000 | Loss: 0.00002216
Iteration 413/1000 | Loss: 0.00002216
Iteration 414/1000 | Loss: 0.00002216
Iteration 415/1000 | Loss: 0.00002216
Iteration 416/1000 | Loss: 0.00002216
Iteration 417/1000 | Loss: 0.00002216
Iteration 418/1000 | Loss: 0.00002216
Iteration 419/1000 | Loss: 0.00002215
Iteration 420/1000 | Loss: 0.00002215
Iteration 421/1000 | Loss: 0.00002215
Iteration 422/1000 | Loss: 0.00003245
Iteration 423/1000 | Loss: 0.00002252
Iteration 424/1000 | Loss: 0.00002210
Iteration 425/1000 | Loss: 0.00002203
Iteration 426/1000 | Loss: 0.00002187
Iteration 427/1000 | Loss: 0.00002185
Iteration 428/1000 | Loss: 0.00002185
Iteration 429/1000 | Loss: 0.00002185
Iteration 430/1000 | Loss: 0.00002185
Iteration 431/1000 | Loss: 0.00002185
Iteration 432/1000 | Loss: 0.00002185
Iteration 433/1000 | Loss: 0.00002185
Iteration 434/1000 | Loss: 0.00002185
Iteration 435/1000 | Loss: 0.00002185
Iteration 436/1000 | Loss: 0.00002185
Iteration 437/1000 | Loss: 0.00002185
Iteration 438/1000 | Loss: 0.00002184
Iteration 439/1000 | Loss: 0.00002183
Iteration 440/1000 | Loss: 0.00002183
Iteration 441/1000 | Loss: 0.00002183
Iteration 442/1000 | Loss: 0.00002182
Iteration 443/1000 | Loss: 0.00002182
Iteration 444/1000 | Loss: 0.00002182
Iteration 445/1000 | Loss: 0.00002181
Iteration 446/1000 | Loss: 0.00002181
Iteration 447/1000 | Loss: 0.00002180
Iteration 448/1000 | Loss: 0.00002180
Iteration 449/1000 | Loss: 0.00002180
Iteration 450/1000 | Loss: 0.00002180
Iteration 451/1000 | Loss: 0.00002180
Iteration 452/1000 | Loss: 0.00002180
Iteration 453/1000 | Loss: 0.00002179
Iteration 454/1000 | Loss: 0.00002179
Iteration 455/1000 | Loss: 0.00002179
Iteration 456/1000 | Loss: 0.00002179
Iteration 457/1000 | Loss: 0.00002179
Iteration 458/1000 | Loss: 0.00002179
Iteration 459/1000 | Loss: 0.00002179
Iteration 460/1000 | Loss: 0.00002179
Iteration 461/1000 | Loss: 0.00002178
Iteration 462/1000 | Loss: 0.00002178
Iteration 463/1000 | Loss: 0.00002178
Iteration 464/1000 | Loss: 0.00002178
Iteration 465/1000 | Loss: 0.00002178
Iteration 466/1000 | Loss: 0.00002178
Iteration 467/1000 | Loss: 0.00002177
Iteration 468/1000 | Loss: 0.00002177
Iteration 469/1000 | Loss: 0.00002177
Iteration 470/1000 | Loss: 0.00002177
Iteration 471/1000 | Loss: 0.00002177
Iteration 472/1000 | Loss: 0.00002176
Iteration 473/1000 | Loss: 0.00002176
Iteration 474/1000 | Loss: 0.00002176
Iteration 475/1000 | Loss: 0.00002176
Iteration 476/1000 | Loss: 0.00002176
Iteration 477/1000 | Loss: 0.00002176
Iteration 478/1000 | Loss: 0.00002176
Iteration 479/1000 | Loss: 0.00002175
Iteration 480/1000 | Loss: 0.00002175
Iteration 481/1000 | Loss: 0.00002175
Iteration 482/1000 | Loss: 0.00002175
Iteration 483/1000 | Loss: 0.00002175
Iteration 484/1000 | Loss: 0.00002175
Iteration 485/1000 | Loss: 0.00002174
Iteration 486/1000 | Loss: 0.00002174
Iteration 487/1000 | Loss: 0.00002174
Iteration 488/1000 | Loss: 0.00002174
Iteration 489/1000 | Loss: 0.00002174
Iteration 490/1000 | Loss: 0.00002173
Iteration 491/1000 | Loss: 0.00002173
Iteration 492/1000 | Loss: 0.00002173
Iteration 493/1000 | Loss: 0.00002173
Iteration 494/1000 | Loss: 0.00002173
Iteration 495/1000 | Loss: 0.00002173
Iteration 496/1000 | Loss: 0.00002173
Iteration 497/1000 | Loss: 0.00002173
Iteration 498/1000 | Loss: 0.00002173
Iteration 499/1000 | Loss: 0.00002173
Iteration 500/1000 | Loss: 0.00002172
Iteration 501/1000 | Loss: 0.00002172
Iteration 502/1000 | Loss: 0.00002172
Iteration 503/1000 | Loss: 0.00002172
Iteration 504/1000 | Loss: 0.00002172
Iteration 505/1000 | Loss: 0.00002172
Iteration 506/1000 | Loss: 0.00002172
Iteration 507/1000 | Loss: 0.00002172
Iteration 508/1000 | Loss: 0.00002172
Iteration 509/1000 | Loss: 0.00002172
Iteration 510/1000 | Loss: 0.00002172
Iteration 511/1000 | Loss: 0.00002172
Iteration 512/1000 | Loss: 0.00002172
Iteration 513/1000 | Loss: 0.00002172
Iteration 514/1000 | Loss: 0.00002172
Iteration 515/1000 | Loss: 0.00002172
Iteration 516/1000 | Loss: 0.00002172
Iteration 517/1000 | Loss: 0.00002172
Iteration 518/1000 | Loss: 0.00002172
Iteration 519/1000 | Loss: 0.00002172
Iteration 520/1000 | Loss: 0.00002172
Iteration 521/1000 | Loss: 0.00002172
Iteration 522/1000 | Loss: 0.00002172
Iteration 523/1000 | Loss: 0.00002172
Iteration 524/1000 | Loss: 0.00002172
Iteration 525/1000 | Loss: 0.00002172
Iteration 526/1000 | Loss: 0.00002172
Iteration 527/1000 | Loss: 0.00002172
Iteration 528/1000 | Loss: 0.00002172
Iteration 529/1000 | Loss: 0.00002172
Iteration 530/1000 | Loss: 0.00002172
Iteration 531/1000 | Loss: 0.00002172
Iteration 532/1000 | Loss: 0.00002172
Iteration 533/1000 | Loss: 0.00002172
Iteration 534/1000 | Loss: 0.00002172
Iteration 535/1000 | Loss: 0.00002172
Iteration 536/1000 | Loss: 0.00002172
Iteration 537/1000 | Loss: 0.00002172
Iteration 538/1000 | Loss: 0.00002172
Iteration 539/1000 | Loss: 0.00002172
Iteration 540/1000 | Loss: 0.00002172
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 540. Stopping optimization.
Last 5 losses: [2.1716239643865265e-05, 2.1716239643865265e-05, 2.1716239643865265e-05, 2.1716239643865265e-05, 2.1716239643865265e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1716239643865265e-05

Optimization complete. Final v2v error: 3.761032819747925 mm

Highest mean error: 5.3161702156066895 mm for frame 5

Lowest mean error: 3.437945604324341 mm for frame 1

Saving results

Total time: 616.129072189331
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00862184
Iteration 2/25 | Loss: 0.00136038
Iteration 3/25 | Loss: 0.00095694
Iteration 4/25 | Loss: 0.00086814
Iteration 5/25 | Loss: 0.00084447
Iteration 6/25 | Loss: 0.00084217
Iteration 7/25 | Loss: 0.00084191
Iteration 8/25 | Loss: 0.00084191
Iteration 9/25 | Loss: 0.00084191
Iteration 10/25 | Loss: 0.00084191
Iteration 11/25 | Loss: 0.00084191
Iteration 12/25 | Loss: 0.00084191
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008419090881943703, 0.0008419090881943703, 0.0008419090881943703, 0.0008419090881943703, 0.0008419090881943703]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008419090881943703

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.81790972
Iteration 2/25 | Loss: 0.00050403
Iteration 3/25 | Loss: 0.00050402
Iteration 4/25 | Loss: 0.00050402
Iteration 5/25 | Loss: 0.00050402
Iteration 6/25 | Loss: 0.00050402
Iteration 7/25 | Loss: 0.00050402
Iteration 8/25 | Loss: 0.00050402
Iteration 9/25 | Loss: 0.00050402
Iteration 10/25 | Loss: 0.00050402
Iteration 11/25 | Loss: 0.00050402
Iteration 12/25 | Loss: 0.00050402
Iteration 13/25 | Loss: 0.00050402
Iteration 14/25 | Loss: 0.00050402
Iteration 15/25 | Loss: 0.00050402
Iteration 16/25 | Loss: 0.00050402
Iteration 17/25 | Loss: 0.00050402
Iteration 18/25 | Loss: 0.00050402
Iteration 19/25 | Loss: 0.00050402
Iteration 20/25 | Loss: 0.00050402
Iteration 21/25 | Loss: 0.00050402
Iteration 22/25 | Loss: 0.00050402
Iteration 23/25 | Loss: 0.00050402
Iteration 24/25 | Loss: 0.00050402
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0005040157120674849, 0.0005040157120674849, 0.0005040157120674849, 0.0005040157120674849, 0.0005040157120674849]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005040157120674849

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050402
Iteration 2/1000 | Loss: 0.00003607
Iteration 3/1000 | Loss: 0.00002572
Iteration 4/1000 | Loss: 0.00002322
Iteration 5/1000 | Loss: 0.00002234
Iteration 6/1000 | Loss: 0.00002172
Iteration 7/1000 | Loss: 0.00002129
Iteration 8/1000 | Loss: 0.00002086
Iteration 9/1000 | Loss: 0.00002047
Iteration 10/1000 | Loss: 0.00002028
Iteration 11/1000 | Loss: 0.00002010
Iteration 12/1000 | Loss: 0.00002008
Iteration 13/1000 | Loss: 0.00002004
Iteration 14/1000 | Loss: 0.00002002
Iteration 15/1000 | Loss: 0.00002000
Iteration 16/1000 | Loss: 0.00001995
Iteration 17/1000 | Loss: 0.00001991
Iteration 18/1000 | Loss: 0.00001991
Iteration 19/1000 | Loss: 0.00001987
Iteration 20/1000 | Loss: 0.00001986
Iteration 21/1000 | Loss: 0.00001985
Iteration 22/1000 | Loss: 0.00001985
Iteration 23/1000 | Loss: 0.00001985
Iteration 24/1000 | Loss: 0.00001982
Iteration 25/1000 | Loss: 0.00001982
Iteration 26/1000 | Loss: 0.00001982
Iteration 27/1000 | Loss: 0.00001982
Iteration 28/1000 | Loss: 0.00001981
Iteration 29/1000 | Loss: 0.00001981
Iteration 30/1000 | Loss: 0.00001981
Iteration 31/1000 | Loss: 0.00001981
Iteration 32/1000 | Loss: 0.00001981
Iteration 33/1000 | Loss: 0.00001981
Iteration 34/1000 | Loss: 0.00001979
Iteration 35/1000 | Loss: 0.00001979
Iteration 36/1000 | Loss: 0.00001979
Iteration 37/1000 | Loss: 0.00001978
Iteration 38/1000 | Loss: 0.00001977
Iteration 39/1000 | Loss: 0.00001977
Iteration 40/1000 | Loss: 0.00001977
Iteration 41/1000 | Loss: 0.00001977
Iteration 42/1000 | Loss: 0.00001977
Iteration 43/1000 | Loss: 0.00001976
Iteration 44/1000 | Loss: 0.00001976
Iteration 45/1000 | Loss: 0.00001976
Iteration 46/1000 | Loss: 0.00001976
Iteration 47/1000 | Loss: 0.00001976
Iteration 48/1000 | Loss: 0.00001976
Iteration 49/1000 | Loss: 0.00001976
Iteration 50/1000 | Loss: 0.00001976
Iteration 51/1000 | Loss: 0.00001975
Iteration 52/1000 | Loss: 0.00001975
Iteration 53/1000 | Loss: 0.00001975
Iteration 54/1000 | Loss: 0.00001975
Iteration 55/1000 | Loss: 0.00001975
Iteration 56/1000 | Loss: 0.00001975
Iteration 57/1000 | Loss: 0.00001975
Iteration 58/1000 | Loss: 0.00001975
Iteration 59/1000 | Loss: 0.00001975
Iteration 60/1000 | Loss: 0.00001974
Iteration 61/1000 | Loss: 0.00001974
Iteration 62/1000 | Loss: 0.00001974
Iteration 63/1000 | Loss: 0.00001974
Iteration 64/1000 | Loss: 0.00001974
Iteration 65/1000 | Loss: 0.00001974
Iteration 66/1000 | Loss: 0.00001974
Iteration 67/1000 | Loss: 0.00001974
Iteration 68/1000 | Loss: 0.00001974
Iteration 69/1000 | Loss: 0.00001974
Iteration 70/1000 | Loss: 0.00001974
Iteration 71/1000 | Loss: 0.00001973
Iteration 72/1000 | Loss: 0.00001973
Iteration 73/1000 | Loss: 0.00001973
Iteration 74/1000 | Loss: 0.00001972
Iteration 75/1000 | Loss: 0.00001972
Iteration 76/1000 | Loss: 0.00001972
Iteration 77/1000 | Loss: 0.00001971
Iteration 78/1000 | Loss: 0.00001971
Iteration 79/1000 | Loss: 0.00001971
Iteration 80/1000 | Loss: 0.00001970
Iteration 81/1000 | Loss: 0.00001970
Iteration 82/1000 | Loss: 0.00001970
Iteration 83/1000 | Loss: 0.00001970
Iteration 84/1000 | Loss: 0.00001970
Iteration 85/1000 | Loss: 0.00001970
Iteration 86/1000 | Loss: 0.00001969
Iteration 87/1000 | Loss: 0.00001969
Iteration 88/1000 | Loss: 0.00001969
Iteration 89/1000 | Loss: 0.00001969
Iteration 90/1000 | Loss: 0.00001969
Iteration 91/1000 | Loss: 0.00001969
Iteration 92/1000 | Loss: 0.00001969
Iteration 93/1000 | Loss: 0.00001969
Iteration 94/1000 | Loss: 0.00001968
Iteration 95/1000 | Loss: 0.00001968
Iteration 96/1000 | Loss: 0.00001968
Iteration 97/1000 | Loss: 0.00001968
Iteration 98/1000 | Loss: 0.00001968
Iteration 99/1000 | Loss: 0.00001967
Iteration 100/1000 | Loss: 0.00001967
Iteration 101/1000 | Loss: 0.00001967
Iteration 102/1000 | Loss: 0.00001967
Iteration 103/1000 | Loss: 0.00001967
Iteration 104/1000 | Loss: 0.00001966
Iteration 105/1000 | Loss: 0.00001966
Iteration 106/1000 | Loss: 0.00001966
Iteration 107/1000 | Loss: 0.00001966
Iteration 108/1000 | Loss: 0.00001966
Iteration 109/1000 | Loss: 0.00001966
Iteration 110/1000 | Loss: 0.00001966
Iteration 111/1000 | Loss: 0.00001966
Iteration 112/1000 | Loss: 0.00001965
Iteration 113/1000 | Loss: 0.00001965
Iteration 114/1000 | Loss: 0.00001965
Iteration 115/1000 | Loss: 0.00001965
Iteration 116/1000 | Loss: 0.00001965
Iteration 117/1000 | Loss: 0.00001965
Iteration 118/1000 | Loss: 0.00001965
Iteration 119/1000 | Loss: 0.00001965
Iteration 120/1000 | Loss: 0.00001965
Iteration 121/1000 | Loss: 0.00001965
Iteration 122/1000 | Loss: 0.00001964
Iteration 123/1000 | Loss: 0.00001964
Iteration 124/1000 | Loss: 0.00001964
Iteration 125/1000 | Loss: 0.00001964
Iteration 126/1000 | Loss: 0.00001964
Iteration 127/1000 | Loss: 0.00001964
Iteration 128/1000 | Loss: 0.00001964
Iteration 129/1000 | Loss: 0.00001964
Iteration 130/1000 | Loss: 0.00001964
Iteration 131/1000 | Loss: 0.00001964
Iteration 132/1000 | Loss: 0.00001963
Iteration 133/1000 | Loss: 0.00001963
Iteration 134/1000 | Loss: 0.00001963
Iteration 135/1000 | Loss: 0.00001963
Iteration 136/1000 | Loss: 0.00001963
Iteration 137/1000 | Loss: 0.00001963
Iteration 138/1000 | Loss: 0.00001962
Iteration 139/1000 | Loss: 0.00001962
Iteration 140/1000 | Loss: 0.00001962
Iteration 141/1000 | Loss: 0.00001962
Iteration 142/1000 | Loss: 0.00001962
Iteration 143/1000 | Loss: 0.00001962
Iteration 144/1000 | Loss: 0.00001962
Iteration 145/1000 | Loss: 0.00001962
Iteration 146/1000 | Loss: 0.00001962
Iteration 147/1000 | Loss: 0.00001962
Iteration 148/1000 | Loss: 0.00001962
Iteration 149/1000 | Loss: 0.00001962
Iteration 150/1000 | Loss: 0.00001962
Iteration 151/1000 | Loss: 0.00001962
Iteration 152/1000 | Loss: 0.00001962
Iteration 153/1000 | Loss: 0.00001962
Iteration 154/1000 | Loss: 0.00001962
Iteration 155/1000 | Loss: 0.00001961
Iteration 156/1000 | Loss: 0.00001961
Iteration 157/1000 | Loss: 0.00001961
Iteration 158/1000 | Loss: 0.00001961
Iteration 159/1000 | Loss: 0.00001961
Iteration 160/1000 | Loss: 0.00001961
Iteration 161/1000 | Loss: 0.00001961
Iteration 162/1000 | Loss: 0.00001961
Iteration 163/1000 | Loss: 0.00001961
Iteration 164/1000 | Loss: 0.00001961
Iteration 165/1000 | Loss: 0.00001961
Iteration 166/1000 | Loss: 0.00001960
Iteration 167/1000 | Loss: 0.00001960
Iteration 168/1000 | Loss: 0.00001960
Iteration 169/1000 | Loss: 0.00001960
Iteration 170/1000 | Loss: 0.00001960
Iteration 171/1000 | Loss: 0.00001960
Iteration 172/1000 | Loss: 0.00001960
Iteration 173/1000 | Loss: 0.00001960
Iteration 174/1000 | Loss: 0.00001960
Iteration 175/1000 | Loss: 0.00001960
Iteration 176/1000 | Loss: 0.00001959
Iteration 177/1000 | Loss: 0.00001959
Iteration 178/1000 | Loss: 0.00001959
Iteration 179/1000 | Loss: 0.00001959
Iteration 180/1000 | Loss: 0.00001959
Iteration 181/1000 | Loss: 0.00001959
Iteration 182/1000 | Loss: 0.00001959
Iteration 183/1000 | Loss: 0.00001958
Iteration 184/1000 | Loss: 0.00001958
Iteration 185/1000 | Loss: 0.00001958
Iteration 186/1000 | Loss: 0.00001958
Iteration 187/1000 | Loss: 0.00001958
Iteration 188/1000 | Loss: 0.00001958
Iteration 189/1000 | Loss: 0.00001958
Iteration 190/1000 | Loss: 0.00001958
Iteration 191/1000 | Loss: 0.00001958
Iteration 192/1000 | Loss: 0.00001958
Iteration 193/1000 | Loss: 0.00001958
Iteration 194/1000 | Loss: 0.00001958
Iteration 195/1000 | Loss: 0.00001957
Iteration 196/1000 | Loss: 0.00001957
Iteration 197/1000 | Loss: 0.00001957
Iteration 198/1000 | Loss: 0.00001957
Iteration 199/1000 | Loss: 0.00001957
Iteration 200/1000 | Loss: 0.00001957
Iteration 201/1000 | Loss: 0.00001957
Iteration 202/1000 | Loss: 0.00001957
Iteration 203/1000 | Loss: 0.00001957
Iteration 204/1000 | Loss: 0.00001957
Iteration 205/1000 | Loss: 0.00001957
Iteration 206/1000 | Loss: 0.00001957
Iteration 207/1000 | Loss: 0.00001957
Iteration 208/1000 | Loss: 0.00001957
Iteration 209/1000 | Loss: 0.00001957
Iteration 210/1000 | Loss: 0.00001957
Iteration 211/1000 | Loss: 0.00001957
Iteration 212/1000 | Loss: 0.00001957
Iteration 213/1000 | Loss: 0.00001957
Iteration 214/1000 | Loss: 0.00001957
Iteration 215/1000 | Loss: 0.00001957
Iteration 216/1000 | Loss: 0.00001957
Iteration 217/1000 | Loss: 0.00001957
Iteration 218/1000 | Loss: 0.00001957
Iteration 219/1000 | Loss: 0.00001957
Iteration 220/1000 | Loss: 0.00001957
Iteration 221/1000 | Loss: 0.00001957
Iteration 222/1000 | Loss: 0.00001957
Iteration 223/1000 | Loss: 0.00001957
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [1.9573562894947827e-05, 1.9573562894947827e-05, 1.9573562894947827e-05, 1.9573562894947827e-05, 1.9573562894947827e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9573562894947827e-05

Optimization complete. Final v2v error: 3.69071888923645 mm

Highest mean error: 3.947190523147583 mm for frame 62

Lowest mean error: 3.4744935035705566 mm for frame 110

Saving results

Total time: 40.17665195465088
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01030724
Iteration 2/25 | Loss: 0.01030724
Iteration 3/25 | Loss: 0.00291732
Iteration 4/25 | Loss: 0.00219241
Iteration 5/25 | Loss: 0.00168238
Iteration 6/25 | Loss: 0.00166837
Iteration 7/25 | Loss: 0.00139133
Iteration 8/25 | Loss: 0.00139931
Iteration 9/25 | Loss: 0.00137227
Iteration 10/25 | Loss: 0.00130728
Iteration 11/25 | Loss: 0.00128873
Iteration 12/25 | Loss: 0.00123518
Iteration 13/25 | Loss: 0.00114965
Iteration 14/25 | Loss: 0.00114559
Iteration 15/25 | Loss: 0.00113143
Iteration 16/25 | Loss: 0.00112458
Iteration 17/25 | Loss: 0.00109892
Iteration 18/25 | Loss: 0.00109106
Iteration 19/25 | Loss: 0.00106702
Iteration 20/25 | Loss: 0.00103569
Iteration 21/25 | Loss: 0.00102608
Iteration 22/25 | Loss: 0.00104128
Iteration 23/25 | Loss: 0.00101058
Iteration 24/25 | Loss: 0.00100806
Iteration 25/25 | Loss: 0.00099957

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.78902841
Iteration 2/25 | Loss: 0.00366942
Iteration 3/25 | Loss: 0.00220556
Iteration 4/25 | Loss: 0.00220556
Iteration 5/25 | Loss: 0.00220556
Iteration 6/25 | Loss: 0.00220556
Iteration 7/25 | Loss: 0.00220556
Iteration 8/25 | Loss: 0.00220556
Iteration 9/25 | Loss: 0.00220556
Iteration 10/25 | Loss: 0.00220556
Iteration 11/25 | Loss: 0.00220556
Iteration 12/25 | Loss: 0.00220556
Iteration 13/25 | Loss: 0.00220555
Iteration 14/25 | Loss: 0.00220555
Iteration 15/25 | Loss: 0.00220555
Iteration 16/25 | Loss: 0.00220555
Iteration 17/25 | Loss: 0.00220555
Iteration 18/25 | Loss: 0.00220555
Iteration 19/25 | Loss: 0.00220555
Iteration 20/25 | Loss: 0.00220555
Iteration 21/25 | Loss: 0.00220555
Iteration 22/25 | Loss: 0.00220555
Iteration 23/25 | Loss: 0.00220555
Iteration 24/25 | Loss: 0.00220555
Iteration 25/25 | Loss: 0.00220555
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0022055543959140778, 0.0022055543959140778, 0.0022055543959140778, 0.0022055543959140778, 0.0022055543959140778]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022055543959140778

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00220555
Iteration 2/1000 | Loss: 0.00089149
Iteration 3/1000 | Loss: 0.00071404
Iteration 4/1000 | Loss: 0.00028342
Iteration 5/1000 | Loss: 0.00031994
Iteration 6/1000 | Loss: 0.00040110
Iteration 7/1000 | Loss: 0.00024943
Iteration 8/1000 | Loss: 0.00080471
Iteration 9/1000 | Loss: 0.00027416
Iteration 10/1000 | Loss: 0.00050075
Iteration 11/1000 | Loss: 0.00071063
Iteration 12/1000 | Loss: 0.00078419
Iteration 13/1000 | Loss: 0.00055238
Iteration 14/1000 | Loss: 0.00054537
Iteration 15/1000 | Loss: 0.00060565
Iteration 16/1000 | Loss: 0.00038750
Iteration 17/1000 | Loss: 0.00044960
Iteration 18/1000 | Loss: 0.00029854
Iteration 19/1000 | Loss: 0.00035486
Iteration 20/1000 | Loss: 0.00045215
Iteration 21/1000 | Loss: 0.00035020
Iteration 22/1000 | Loss: 0.00029212
Iteration 23/1000 | Loss: 0.00023390
Iteration 24/1000 | Loss: 0.00047266
Iteration 25/1000 | Loss: 0.00020126
Iteration 26/1000 | Loss: 0.00012391
Iteration 27/1000 | Loss: 0.00049958
Iteration 28/1000 | Loss: 0.00016430
Iteration 29/1000 | Loss: 0.00041387
Iteration 30/1000 | Loss: 0.00027779
Iteration 31/1000 | Loss: 0.00038209
Iteration 32/1000 | Loss: 0.00121401
Iteration 33/1000 | Loss: 0.00068907
Iteration 34/1000 | Loss: 0.00044899
Iteration 35/1000 | Loss: 0.00024447
Iteration 36/1000 | Loss: 0.00022448
Iteration 37/1000 | Loss: 0.00018110
Iteration 38/1000 | Loss: 0.00013748
Iteration 39/1000 | Loss: 0.00037936
Iteration 40/1000 | Loss: 0.00023196
Iteration 41/1000 | Loss: 0.00020764
Iteration 42/1000 | Loss: 0.00021502
Iteration 43/1000 | Loss: 0.00026170
Iteration 44/1000 | Loss: 0.00019851
Iteration 45/1000 | Loss: 0.00014479
Iteration 46/1000 | Loss: 0.00021317
Iteration 47/1000 | Loss: 0.00016653
Iteration 48/1000 | Loss: 0.00014019
Iteration 49/1000 | Loss: 0.00015850
Iteration 50/1000 | Loss: 0.00023147
Iteration 51/1000 | Loss: 0.00021540
Iteration 52/1000 | Loss: 0.00021385
Iteration 53/1000 | Loss: 0.00020340
Iteration 54/1000 | Loss: 0.00019209
Iteration 55/1000 | Loss: 0.00020852
Iteration 56/1000 | Loss: 0.00021266
Iteration 57/1000 | Loss: 0.00021026
Iteration 58/1000 | Loss: 0.00032379
Iteration 59/1000 | Loss: 0.00043060
Iteration 60/1000 | Loss: 0.00031894
Iteration 61/1000 | Loss: 0.00033283
Iteration 62/1000 | Loss: 0.00039406
Iteration 63/1000 | Loss: 0.00031088
Iteration 64/1000 | Loss: 0.00016646
Iteration 65/1000 | Loss: 0.00025629
Iteration 66/1000 | Loss: 0.00032222
Iteration 67/1000 | Loss: 0.00034331
Iteration 68/1000 | Loss: 0.00024190
Iteration 69/1000 | Loss: 0.00037155
Iteration 70/1000 | Loss: 0.00013271
Iteration 71/1000 | Loss: 0.00028771
Iteration 72/1000 | Loss: 0.00030715
Iteration 73/1000 | Loss: 0.00060152
Iteration 74/1000 | Loss: 0.00032628
Iteration 75/1000 | Loss: 0.00028227
Iteration 76/1000 | Loss: 0.00037756
Iteration 77/1000 | Loss: 0.00031692
Iteration 78/1000 | Loss: 0.00037187
Iteration 79/1000 | Loss: 0.00035086
Iteration 80/1000 | Loss: 0.00030402
Iteration 81/1000 | Loss: 0.00048697
Iteration 82/1000 | Loss: 0.00030819
Iteration 83/1000 | Loss: 0.00087359
Iteration 84/1000 | Loss: 0.00034752
Iteration 85/1000 | Loss: 0.00042520
Iteration 86/1000 | Loss: 0.00038436
Iteration 87/1000 | Loss: 0.00029849
Iteration 88/1000 | Loss: 0.00036391
Iteration 89/1000 | Loss: 0.00036940
Iteration 90/1000 | Loss: 0.00027361
Iteration 91/1000 | Loss: 0.00034068
Iteration 92/1000 | Loss: 0.00040274
Iteration 93/1000 | Loss: 0.00021903
Iteration 94/1000 | Loss: 0.00027580
Iteration 95/1000 | Loss: 0.00009413
Iteration 96/1000 | Loss: 0.00011386
Iteration 97/1000 | Loss: 0.00009757
Iteration 98/1000 | Loss: 0.00012071
Iteration 99/1000 | Loss: 0.00025449
Iteration 100/1000 | Loss: 0.00010173
Iteration 101/1000 | Loss: 0.00063238
Iteration 102/1000 | Loss: 0.00036555
Iteration 103/1000 | Loss: 0.00077265
Iteration 104/1000 | Loss: 0.00064346
Iteration 105/1000 | Loss: 0.00050967
Iteration 106/1000 | Loss: 0.00053770
Iteration 107/1000 | Loss: 0.00027737
Iteration 108/1000 | Loss: 0.00022991
Iteration 109/1000 | Loss: 0.00025281
Iteration 110/1000 | Loss: 0.00051690
Iteration 111/1000 | Loss: 0.00015177
Iteration 112/1000 | Loss: 0.00011267
Iteration 113/1000 | Loss: 0.00010658
Iteration 114/1000 | Loss: 0.00043014
Iteration 115/1000 | Loss: 0.00016096
Iteration 116/1000 | Loss: 0.00017683
Iteration 117/1000 | Loss: 0.00014727
Iteration 118/1000 | Loss: 0.00010683
Iteration 119/1000 | Loss: 0.00008562
Iteration 120/1000 | Loss: 0.00010843
Iteration 121/1000 | Loss: 0.00011038
Iteration 122/1000 | Loss: 0.00011476
Iteration 123/1000 | Loss: 0.00011015
Iteration 124/1000 | Loss: 0.00009617
Iteration 125/1000 | Loss: 0.00044291
Iteration 126/1000 | Loss: 0.00013822
Iteration 127/1000 | Loss: 0.00018255
Iteration 128/1000 | Loss: 0.00014500
Iteration 129/1000 | Loss: 0.00011026
Iteration 130/1000 | Loss: 0.00009848
Iteration 131/1000 | Loss: 0.00008216
Iteration 132/1000 | Loss: 0.00017121
Iteration 133/1000 | Loss: 0.00027430
Iteration 134/1000 | Loss: 0.00017536
Iteration 135/1000 | Loss: 0.00007939
Iteration 136/1000 | Loss: 0.00041392
Iteration 137/1000 | Loss: 0.00053944
Iteration 138/1000 | Loss: 0.00051095
Iteration 139/1000 | Loss: 0.00202848
Iteration 140/1000 | Loss: 0.00219772
Iteration 141/1000 | Loss: 0.00365709
Iteration 142/1000 | Loss: 0.00070960
Iteration 143/1000 | Loss: 0.00030821
Iteration 144/1000 | Loss: 0.00087129
Iteration 145/1000 | Loss: 0.00048748
Iteration 146/1000 | Loss: 0.00041027
Iteration 147/1000 | Loss: 0.00023762
Iteration 148/1000 | Loss: 0.00032040
Iteration 149/1000 | Loss: 0.00025708
Iteration 150/1000 | Loss: 0.00009895
Iteration 151/1000 | Loss: 0.00009066
Iteration 152/1000 | Loss: 0.00030150
Iteration 153/1000 | Loss: 0.00021266
Iteration 154/1000 | Loss: 0.00024324
Iteration 155/1000 | Loss: 0.00022024
Iteration 156/1000 | Loss: 0.00024513
Iteration 157/1000 | Loss: 0.00016047
Iteration 158/1000 | Loss: 0.00019260
Iteration 159/1000 | Loss: 0.00021025
Iteration 160/1000 | Loss: 0.00048860
Iteration 161/1000 | Loss: 0.00038503
Iteration 162/1000 | Loss: 0.00028818
Iteration 163/1000 | Loss: 0.00040940
Iteration 164/1000 | Loss: 0.00090566
Iteration 165/1000 | Loss: 0.00080236
Iteration 166/1000 | Loss: 0.00036977
Iteration 167/1000 | Loss: 0.00036727
Iteration 168/1000 | Loss: 0.00145870
Iteration 169/1000 | Loss: 0.00074543
Iteration 170/1000 | Loss: 0.00021009
Iteration 171/1000 | Loss: 0.00041945
Iteration 172/1000 | Loss: 0.00022357
Iteration 173/1000 | Loss: 0.00061664
Iteration 174/1000 | Loss: 0.00022533
Iteration 175/1000 | Loss: 0.00047198
Iteration 176/1000 | Loss: 0.00036621
Iteration 177/1000 | Loss: 0.00017430
Iteration 178/1000 | Loss: 0.00013132
Iteration 179/1000 | Loss: 0.00009550
Iteration 180/1000 | Loss: 0.00010546
Iteration 181/1000 | Loss: 0.00066326
Iteration 182/1000 | Loss: 0.00053466
Iteration 183/1000 | Loss: 0.00104155
Iteration 184/1000 | Loss: 0.00070185
Iteration 185/1000 | Loss: 0.00061550
Iteration 186/1000 | Loss: 0.00024890
Iteration 187/1000 | Loss: 0.00028568
Iteration 188/1000 | Loss: 0.00028953
Iteration 189/1000 | Loss: 0.00045489
Iteration 190/1000 | Loss: 0.00026327
Iteration 191/1000 | Loss: 0.00025101
Iteration 192/1000 | Loss: 0.00017227
Iteration 193/1000 | Loss: 0.00026425
Iteration 194/1000 | Loss: 0.00037208
Iteration 195/1000 | Loss: 0.00034444
Iteration 196/1000 | Loss: 0.00109718
Iteration 197/1000 | Loss: 0.00048891
Iteration 198/1000 | Loss: 0.00057619
Iteration 199/1000 | Loss: 0.00024077
Iteration 200/1000 | Loss: 0.00008717
Iteration 201/1000 | Loss: 0.00032373
Iteration 202/1000 | Loss: 0.00058698
Iteration 203/1000 | Loss: 0.00037657
Iteration 204/1000 | Loss: 0.00069292
Iteration 205/1000 | Loss: 0.00073434
Iteration 206/1000 | Loss: 0.00065563
Iteration 207/1000 | Loss: 0.00014127
Iteration 208/1000 | Loss: 0.00041094
Iteration 209/1000 | Loss: 0.00062133
Iteration 210/1000 | Loss: 0.00018498
Iteration 211/1000 | Loss: 0.00073384
Iteration 212/1000 | Loss: 0.00099001
Iteration 213/1000 | Loss: 0.00038953
Iteration 214/1000 | Loss: 0.00025203
Iteration 215/1000 | Loss: 0.00021725
Iteration 216/1000 | Loss: 0.00019738
Iteration 217/1000 | Loss: 0.00034316
Iteration 218/1000 | Loss: 0.00024994
Iteration 219/1000 | Loss: 0.00035582
Iteration 220/1000 | Loss: 0.00039703
Iteration 221/1000 | Loss: 0.00029660
Iteration 222/1000 | Loss: 0.00038002
Iteration 223/1000 | Loss: 0.00058404
Iteration 224/1000 | Loss: 0.00030752
Iteration 225/1000 | Loss: 0.00039151
Iteration 226/1000 | Loss: 0.00020825
Iteration 227/1000 | Loss: 0.00015098
Iteration 228/1000 | Loss: 0.00011792
Iteration 229/1000 | Loss: 0.00053440
Iteration 230/1000 | Loss: 0.00023171
Iteration 231/1000 | Loss: 0.00046877
Iteration 232/1000 | Loss: 0.00008699
Iteration 233/1000 | Loss: 0.00007621
Iteration 234/1000 | Loss: 0.00007344
Iteration 235/1000 | Loss: 0.00017819
Iteration 236/1000 | Loss: 0.00058209
Iteration 237/1000 | Loss: 0.00087140
Iteration 238/1000 | Loss: 0.00068001
Iteration 239/1000 | Loss: 0.00035858
Iteration 240/1000 | Loss: 0.00026609
Iteration 241/1000 | Loss: 0.00007229
Iteration 242/1000 | Loss: 0.00007067
Iteration 243/1000 | Loss: 0.00043779
Iteration 244/1000 | Loss: 0.00017765
Iteration 245/1000 | Loss: 0.00035510
Iteration 246/1000 | Loss: 0.00066843
Iteration 247/1000 | Loss: 0.00044850
Iteration 248/1000 | Loss: 0.00039226
Iteration 249/1000 | Loss: 0.00077876
Iteration 250/1000 | Loss: 0.00016137
Iteration 251/1000 | Loss: 0.00041784
Iteration 252/1000 | Loss: 0.00021985
Iteration 253/1000 | Loss: 0.00032905
Iteration 254/1000 | Loss: 0.00027771
Iteration 255/1000 | Loss: 0.00033739
Iteration 256/1000 | Loss: 0.00032422
Iteration 257/1000 | Loss: 0.00034249
Iteration 258/1000 | Loss: 0.00023308
Iteration 259/1000 | Loss: 0.00062376
Iteration 260/1000 | Loss: 0.00008762
Iteration 261/1000 | Loss: 0.00008218
Iteration 262/1000 | Loss: 0.00007006
Iteration 263/1000 | Loss: 0.00006811
Iteration 264/1000 | Loss: 0.00018295
Iteration 265/1000 | Loss: 0.00032605
Iteration 266/1000 | Loss: 0.00010682
Iteration 267/1000 | Loss: 0.00008954
Iteration 268/1000 | Loss: 0.00007311
Iteration 269/1000 | Loss: 0.00009027
Iteration 270/1000 | Loss: 0.00007012
Iteration 271/1000 | Loss: 0.00015001
Iteration 272/1000 | Loss: 0.00017409
Iteration 273/1000 | Loss: 0.00015862
Iteration 274/1000 | Loss: 0.00013415
Iteration 275/1000 | Loss: 0.00007702
Iteration 276/1000 | Loss: 0.00007350
Iteration 277/1000 | Loss: 0.00055185
Iteration 278/1000 | Loss: 0.00075861
Iteration 279/1000 | Loss: 0.00060720
Iteration 280/1000 | Loss: 0.00011501
Iteration 281/1000 | Loss: 0.00012864
Iteration 282/1000 | Loss: 0.00013485
Iteration 283/1000 | Loss: 0.00010458
Iteration 284/1000 | Loss: 0.00012143
Iteration 285/1000 | Loss: 0.00010028
Iteration 286/1000 | Loss: 0.00011885
Iteration 287/1000 | Loss: 0.00010564
Iteration 288/1000 | Loss: 0.00011480
Iteration 289/1000 | Loss: 0.00012437
Iteration 290/1000 | Loss: 0.00061092
Iteration 291/1000 | Loss: 0.00031606
Iteration 292/1000 | Loss: 0.00033288
Iteration 293/1000 | Loss: 0.00022241
Iteration 294/1000 | Loss: 0.00038381
Iteration 295/1000 | Loss: 0.00007496
Iteration 296/1000 | Loss: 0.00016579
Iteration 297/1000 | Loss: 0.00006798
Iteration 298/1000 | Loss: 0.00006536
Iteration 299/1000 | Loss: 0.00016925
Iteration 300/1000 | Loss: 0.00100449
Iteration 301/1000 | Loss: 0.00009306
Iteration 302/1000 | Loss: 0.00006887
Iteration 303/1000 | Loss: 0.00006326
Iteration 304/1000 | Loss: 0.00006202
Iteration 305/1000 | Loss: 0.00006135
Iteration 306/1000 | Loss: 0.00006092
Iteration 307/1000 | Loss: 0.00006065
Iteration 308/1000 | Loss: 0.00087271
Iteration 309/1000 | Loss: 0.00006977
Iteration 310/1000 | Loss: 0.00006592
Iteration 311/1000 | Loss: 0.00023959
Iteration 312/1000 | Loss: 0.00006077
Iteration 313/1000 | Loss: 0.00058362
Iteration 314/1000 | Loss: 0.00007042
Iteration 315/1000 | Loss: 0.00006249
Iteration 316/1000 | Loss: 0.00006020
Iteration 317/1000 | Loss: 0.00005936
Iteration 318/1000 | Loss: 0.00005890
Iteration 319/1000 | Loss: 0.00046111
Iteration 320/1000 | Loss: 0.00016706
Iteration 321/1000 | Loss: 0.00005897
Iteration 322/1000 | Loss: 0.00015687
Iteration 323/1000 | Loss: 0.00005826
Iteration 324/1000 | Loss: 0.00005792
Iteration 325/1000 | Loss: 0.00005759
Iteration 326/1000 | Loss: 0.00005734
Iteration 327/1000 | Loss: 0.00005712
Iteration 328/1000 | Loss: 0.00005693
Iteration 329/1000 | Loss: 0.00115898
Iteration 330/1000 | Loss: 0.00070203
Iteration 331/1000 | Loss: 0.00099401
Iteration 332/1000 | Loss: 0.00041925
Iteration 333/1000 | Loss: 0.00044889
Iteration 334/1000 | Loss: 0.00057008
Iteration 335/1000 | Loss: 0.00059963
Iteration 336/1000 | Loss: 0.00082079
Iteration 337/1000 | Loss: 0.00019829
Iteration 338/1000 | Loss: 0.00006155
Iteration 339/1000 | Loss: 0.00024900
Iteration 340/1000 | Loss: 0.00014086
Iteration 341/1000 | Loss: 0.00005746
Iteration 342/1000 | Loss: 0.00018265
Iteration 343/1000 | Loss: 0.00047685
Iteration 344/1000 | Loss: 0.00022552
Iteration 345/1000 | Loss: 0.00006308
Iteration 346/1000 | Loss: 0.00022433
Iteration 347/1000 | Loss: 0.00007091
Iteration 348/1000 | Loss: 0.00006000
Iteration 349/1000 | Loss: 0.00005794
Iteration 350/1000 | Loss: 0.00005707
Iteration 351/1000 | Loss: 0.00025890
Iteration 352/1000 | Loss: 0.00011734
Iteration 353/1000 | Loss: 0.00020208
Iteration 354/1000 | Loss: 0.00019576
Iteration 355/1000 | Loss: 0.00056838
Iteration 356/1000 | Loss: 0.00044995
Iteration 357/1000 | Loss: 0.00035085
Iteration 358/1000 | Loss: 0.00009675
Iteration 359/1000 | Loss: 0.00006905
Iteration 360/1000 | Loss: 0.00007114
Iteration 361/1000 | Loss: 0.00006019
Iteration 362/1000 | Loss: 0.00005941
Iteration 363/1000 | Loss: 0.00005877
Iteration 364/1000 | Loss: 0.00005826
Iteration 365/1000 | Loss: 0.00005794
Iteration 366/1000 | Loss: 0.00005747
Iteration 367/1000 | Loss: 0.00036940
Iteration 368/1000 | Loss: 0.00053034
Iteration 369/1000 | Loss: 0.00008965
Iteration 370/1000 | Loss: 0.00005868
Iteration 371/1000 | Loss: 0.00005738
Iteration 372/1000 | Loss: 0.00005595
Iteration 373/1000 | Loss: 0.00005485
Iteration 374/1000 | Loss: 0.00005429
Iteration 375/1000 | Loss: 0.00005404
Iteration 376/1000 | Loss: 0.00005387
Iteration 377/1000 | Loss: 0.00005382
Iteration 378/1000 | Loss: 0.00005376
Iteration 379/1000 | Loss: 0.00005367
Iteration 380/1000 | Loss: 0.00005359
Iteration 381/1000 | Loss: 0.00005343
Iteration 382/1000 | Loss: 0.00005334
Iteration 383/1000 | Loss: 0.00005330
Iteration 384/1000 | Loss: 0.00005325
Iteration 385/1000 | Loss: 0.00005324
Iteration 386/1000 | Loss: 0.00005320
Iteration 387/1000 | Loss: 0.00005320
Iteration 388/1000 | Loss: 0.00005318
Iteration 389/1000 | Loss: 0.00005318
Iteration 390/1000 | Loss: 0.00005318
Iteration 391/1000 | Loss: 0.00005317
Iteration 392/1000 | Loss: 0.00005317
Iteration 393/1000 | Loss: 0.00005317
Iteration 394/1000 | Loss: 0.00005317
Iteration 395/1000 | Loss: 0.00005317
Iteration 396/1000 | Loss: 0.00005315
Iteration 397/1000 | Loss: 0.00005315
Iteration 398/1000 | Loss: 0.00005315
Iteration 399/1000 | Loss: 0.00005315
Iteration 400/1000 | Loss: 0.00005315
Iteration 401/1000 | Loss: 0.00005315
Iteration 402/1000 | Loss: 0.00005315
Iteration 403/1000 | Loss: 0.00005314
Iteration 404/1000 | Loss: 0.00005314
Iteration 405/1000 | Loss: 0.00005314
Iteration 406/1000 | Loss: 0.00005314
Iteration 407/1000 | Loss: 0.00005313
Iteration 408/1000 | Loss: 0.00005313
Iteration 409/1000 | Loss: 0.00005313
Iteration 410/1000 | Loss: 0.00005313
Iteration 411/1000 | Loss: 0.00005309
Iteration 412/1000 | Loss: 0.00005309
Iteration 413/1000 | Loss: 0.00005309
Iteration 414/1000 | Loss: 0.00005309
Iteration 415/1000 | Loss: 0.00005309
Iteration 416/1000 | Loss: 0.00005308
Iteration 417/1000 | Loss: 0.00005307
Iteration 418/1000 | Loss: 0.00005307
Iteration 419/1000 | Loss: 0.00005307
Iteration 420/1000 | Loss: 0.00005306
Iteration 421/1000 | Loss: 0.00005306
Iteration 422/1000 | Loss: 0.00005306
Iteration 423/1000 | Loss: 0.00005306
Iteration 424/1000 | Loss: 0.00005306
Iteration 425/1000 | Loss: 0.00005305
Iteration 426/1000 | Loss: 0.00005305
Iteration 427/1000 | Loss: 0.00005305
Iteration 428/1000 | Loss: 0.00005305
Iteration 429/1000 | Loss: 0.00005305
Iteration 430/1000 | Loss: 0.00005305
Iteration 431/1000 | Loss: 0.00005304
Iteration 432/1000 | Loss: 0.00005304
Iteration 433/1000 | Loss: 0.00005304
Iteration 434/1000 | Loss: 0.00005304
Iteration 435/1000 | Loss: 0.00005304
Iteration 436/1000 | Loss: 0.00005304
Iteration 437/1000 | Loss: 0.00005304
Iteration 438/1000 | Loss: 0.00005303
Iteration 439/1000 | Loss: 0.00005303
Iteration 440/1000 | Loss: 0.00005303
Iteration 441/1000 | Loss: 0.00005303
Iteration 442/1000 | Loss: 0.00005303
Iteration 443/1000 | Loss: 0.00005303
Iteration 444/1000 | Loss: 0.00005303
Iteration 445/1000 | Loss: 0.00005303
Iteration 446/1000 | Loss: 0.00005302
Iteration 447/1000 | Loss: 0.00005302
Iteration 448/1000 | Loss: 0.00005302
Iteration 449/1000 | Loss: 0.00005302
Iteration 450/1000 | Loss: 0.00005302
Iteration 451/1000 | Loss: 0.00005302
Iteration 452/1000 | Loss: 0.00005302
Iteration 453/1000 | Loss: 0.00005302
Iteration 454/1000 | Loss: 0.00005302
Iteration 455/1000 | Loss: 0.00005302
Iteration 456/1000 | Loss: 0.00005302
Iteration 457/1000 | Loss: 0.00005301
Iteration 458/1000 | Loss: 0.00005301
Iteration 459/1000 | Loss: 0.00005301
Iteration 460/1000 | Loss: 0.00005301
Iteration 461/1000 | Loss: 0.00005301
Iteration 462/1000 | Loss: 0.00005301
Iteration 463/1000 | Loss: 0.00005301
Iteration 464/1000 | Loss: 0.00005301
Iteration 465/1000 | Loss: 0.00005301
Iteration 466/1000 | Loss: 0.00005301
Iteration 467/1000 | Loss: 0.00005301
Iteration 468/1000 | Loss: 0.00005301
Iteration 469/1000 | Loss: 0.00005300
Iteration 470/1000 | Loss: 0.00005300
Iteration 471/1000 | Loss: 0.00005300
Iteration 472/1000 | Loss: 0.00005300
Iteration 473/1000 | Loss: 0.00005300
Iteration 474/1000 | Loss: 0.00005300
Iteration 475/1000 | Loss: 0.00005300
Iteration 476/1000 | Loss: 0.00005300
Iteration 477/1000 | Loss: 0.00005300
Iteration 478/1000 | Loss: 0.00005300
Iteration 479/1000 | Loss: 0.00005300
Iteration 480/1000 | Loss: 0.00005300
Iteration 481/1000 | Loss: 0.00005300
Iteration 482/1000 | Loss: 0.00005300
Iteration 483/1000 | Loss: 0.00005300
Iteration 484/1000 | Loss: 0.00005300
Iteration 485/1000 | Loss: 0.00005300
Iteration 486/1000 | Loss: 0.00005300
Iteration 487/1000 | Loss: 0.00005300
Iteration 488/1000 | Loss: 0.00005300
Iteration 489/1000 | Loss: 0.00005299
Iteration 490/1000 | Loss: 0.00005299
Iteration 491/1000 | Loss: 0.00005299
Iteration 492/1000 | Loss: 0.00005299
Iteration 493/1000 | Loss: 0.00005299
Iteration 494/1000 | Loss: 0.00005299
Iteration 495/1000 | Loss: 0.00005299
Iteration 496/1000 | Loss: 0.00005299
Iteration 497/1000 | Loss: 0.00005299
Iteration 498/1000 | Loss: 0.00005299
Iteration 499/1000 | Loss: 0.00005299
Iteration 500/1000 | Loss: 0.00005299
Iteration 501/1000 | Loss: 0.00005299
Iteration 502/1000 | Loss: 0.00005298
Iteration 503/1000 | Loss: 0.00005298
Iteration 504/1000 | Loss: 0.00005298
Iteration 505/1000 | Loss: 0.00005298
Iteration 506/1000 | Loss: 0.00005298
Iteration 507/1000 | Loss: 0.00005298
Iteration 508/1000 | Loss: 0.00005298
Iteration 509/1000 | Loss: 0.00005298
Iteration 510/1000 | Loss: 0.00005298
Iteration 511/1000 | Loss: 0.00005298
Iteration 512/1000 | Loss: 0.00005298
Iteration 513/1000 | Loss: 0.00005298
Iteration 514/1000 | Loss: 0.00005298
Iteration 515/1000 | Loss: 0.00005298
Iteration 516/1000 | Loss: 0.00005297
Iteration 517/1000 | Loss: 0.00005297
Iteration 518/1000 | Loss: 0.00005297
Iteration 519/1000 | Loss: 0.00005297
Iteration 520/1000 | Loss: 0.00005297
Iteration 521/1000 | Loss: 0.00005297
Iteration 522/1000 | Loss: 0.00005297
Iteration 523/1000 | Loss: 0.00005297
Iteration 524/1000 | Loss: 0.00005297
Iteration 525/1000 | Loss: 0.00005297
Iteration 526/1000 | Loss: 0.00005297
Iteration 527/1000 | Loss: 0.00005297
Iteration 528/1000 | Loss: 0.00005297
Iteration 529/1000 | Loss: 0.00005297
Iteration 530/1000 | Loss: 0.00005297
Iteration 531/1000 | Loss: 0.00005297
Iteration 532/1000 | Loss: 0.00005297
Iteration 533/1000 | Loss: 0.00005296
Iteration 534/1000 | Loss: 0.00005296
Iteration 535/1000 | Loss: 0.00005296
Iteration 536/1000 | Loss: 0.00005296
Iteration 537/1000 | Loss: 0.00005296
Iteration 538/1000 | Loss: 0.00005296
Iteration 539/1000 | Loss: 0.00005296
Iteration 540/1000 | Loss: 0.00005296
Iteration 541/1000 | Loss: 0.00005296
Iteration 542/1000 | Loss: 0.00005296
Iteration 543/1000 | Loss: 0.00005296
Iteration 544/1000 | Loss: 0.00005296
Iteration 545/1000 | Loss: 0.00005296
Iteration 546/1000 | Loss: 0.00005296
Iteration 547/1000 | Loss: 0.00005296
Iteration 548/1000 | Loss: 0.00005296
Iteration 549/1000 | Loss: 0.00005296
Iteration 550/1000 | Loss: 0.00005296
Iteration 551/1000 | Loss: 0.00005296
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 551. Stopping optimization.
Last 5 losses: [5.2958821470383555e-05, 5.2958821470383555e-05, 5.2958821470383555e-05, 5.2958821470383555e-05, 5.2958821470383555e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.2958821470383555e-05

Optimization complete. Final v2v error: 4.600949287414551 mm

Highest mean error: 12.852459907531738 mm for frame 145

Lowest mean error: 3.384492874145508 mm for frame 159

Saving results

Total time: 669.9764714241028
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00846607
Iteration 2/25 | Loss: 0.00111014
Iteration 3/25 | Loss: 0.00093402
Iteration 4/25 | Loss: 0.00089223
Iteration 5/25 | Loss: 0.00088207
Iteration 6/25 | Loss: 0.00088086
Iteration 7/25 | Loss: 0.00088081
Iteration 8/25 | Loss: 0.00088081
Iteration 9/25 | Loss: 0.00088081
Iteration 10/25 | Loss: 0.00088081
Iteration 11/25 | Loss: 0.00088081
Iteration 12/25 | Loss: 0.00088081
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008808139245957136, 0.0008808139245957136, 0.0008808139245957136, 0.0008808139245957136, 0.0008808139245957136]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008808139245957136

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.00481415
Iteration 2/25 | Loss: 0.00049147
Iteration 3/25 | Loss: 0.00049145
Iteration 4/25 | Loss: 0.00049145
Iteration 5/25 | Loss: 0.00049145
Iteration 6/25 | Loss: 0.00049145
Iteration 7/25 | Loss: 0.00049145
Iteration 8/25 | Loss: 0.00049145
Iteration 9/25 | Loss: 0.00049145
Iteration 10/25 | Loss: 0.00049145
Iteration 11/25 | Loss: 0.00049145
Iteration 12/25 | Loss: 0.00049145
Iteration 13/25 | Loss: 0.00049145
Iteration 14/25 | Loss: 0.00049145
Iteration 15/25 | Loss: 0.00049145
Iteration 16/25 | Loss: 0.00049145
Iteration 17/25 | Loss: 0.00049145
Iteration 18/25 | Loss: 0.00049145
Iteration 19/25 | Loss: 0.00049145
Iteration 20/25 | Loss: 0.00049145
Iteration 21/25 | Loss: 0.00049145
Iteration 22/25 | Loss: 0.00049145
Iteration 23/25 | Loss: 0.00049145
Iteration 24/25 | Loss: 0.00049145
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0004914524615742266, 0.0004914524615742266, 0.0004914524615742266, 0.0004914524615742266, 0.0004914524615742266]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004914524615742266

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049145
Iteration 2/1000 | Loss: 0.00005577
Iteration 3/1000 | Loss: 0.00004380
Iteration 4/1000 | Loss: 0.00004039
Iteration 5/1000 | Loss: 0.00003785
Iteration 6/1000 | Loss: 0.00003653
Iteration 7/1000 | Loss: 0.00003539
Iteration 8/1000 | Loss: 0.00003482
Iteration 9/1000 | Loss: 0.00003452
Iteration 10/1000 | Loss: 0.00003423
Iteration 11/1000 | Loss: 0.00003404
Iteration 12/1000 | Loss: 0.00003399
Iteration 13/1000 | Loss: 0.00003386
Iteration 14/1000 | Loss: 0.00003385
Iteration 15/1000 | Loss: 0.00003385
Iteration 16/1000 | Loss: 0.00003385
Iteration 17/1000 | Loss: 0.00003384
Iteration 18/1000 | Loss: 0.00003384
Iteration 19/1000 | Loss: 0.00003384
Iteration 20/1000 | Loss: 0.00003384
Iteration 21/1000 | Loss: 0.00003384
Iteration 22/1000 | Loss: 0.00003382
Iteration 23/1000 | Loss: 0.00003381
Iteration 24/1000 | Loss: 0.00003378
Iteration 25/1000 | Loss: 0.00003377
Iteration 26/1000 | Loss: 0.00003377
Iteration 27/1000 | Loss: 0.00003373
Iteration 28/1000 | Loss: 0.00003373
Iteration 29/1000 | Loss: 0.00003373
Iteration 30/1000 | Loss: 0.00003373
Iteration 31/1000 | Loss: 0.00003373
Iteration 32/1000 | Loss: 0.00003373
Iteration 33/1000 | Loss: 0.00003373
Iteration 34/1000 | Loss: 0.00003373
Iteration 35/1000 | Loss: 0.00003372
Iteration 36/1000 | Loss: 0.00003372
Iteration 37/1000 | Loss: 0.00003372
Iteration 38/1000 | Loss: 0.00003372
Iteration 39/1000 | Loss: 0.00003372
Iteration 40/1000 | Loss: 0.00003372
Iteration 41/1000 | Loss: 0.00003372
Iteration 42/1000 | Loss: 0.00003372
Iteration 43/1000 | Loss: 0.00003371
Iteration 44/1000 | Loss: 0.00003371
Iteration 45/1000 | Loss: 0.00003371
Iteration 46/1000 | Loss: 0.00003371
Iteration 47/1000 | Loss: 0.00003371
Iteration 48/1000 | Loss: 0.00003370
Iteration 49/1000 | Loss: 0.00003370
Iteration 50/1000 | Loss: 0.00003370
Iteration 51/1000 | Loss: 0.00003370
Iteration 52/1000 | Loss: 0.00003370
Iteration 53/1000 | Loss: 0.00003369
Iteration 54/1000 | Loss: 0.00003369
Iteration 55/1000 | Loss: 0.00003369
Iteration 56/1000 | Loss: 0.00003369
Iteration 57/1000 | Loss: 0.00003369
Iteration 58/1000 | Loss: 0.00003368
Iteration 59/1000 | Loss: 0.00003368
Iteration 60/1000 | Loss: 0.00003368
Iteration 61/1000 | Loss: 0.00003367
Iteration 62/1000 | Loss: 0.00003365
Iteration 63/1000 | Loss: 0.00003365
Iteration 64/1000 | Loss: 0.00003365
Iteration 65/1000 | Loss: 0.00003365
Iteration 66/1000 | Loss: 0.00003365
Iteration 67/1000 | Loss: 0.00003365
Iteration 68/1000 | Loss: 0.00003365
Iteration 69/1000 | Loss: 0.00003365
Iteration 70/1000 | Loss: 0.00003365
Iteration 71/1000 | Loss: 0.00003365
Iteration 72/1000 | Loss: 0.00003364
Iteration 73/1000 | Loss: 0.00003363
Iteration 74/1000 | Loss: 0.00003363
Iteration 75/1000 | Loss: 0.00003363
Iteration 76/1000 | Loss: 0.00003363
Iteration 77/1000 | Loss: 0.00003363
Iteration 78/1000 | Loss: 0.00003363
Iteration 79/1000 | Loss: 0.00003362
Iteration 80/1000 | Loss: 0.00003362
Iteration 81/1000 | Loss: 0.00003362
Iteration 82/1000 | Loss: 0.00003362
Iteration 83/1000 | Loss: 0.00003361
Iteration 84/1000 | Loss: 0.00003361
Iteration 85/1000 | Loss: 0.00003361
Iteration 86/1000 | Loss: 0.00003361
Iteration 87/1000 | Loss: 0.00003361
Iteration 88/1000 | Loss: 0.00003361
Iteration 89/1000 | Loss: 0.00003361
Iteration 90/1000 | Loss: 0.00003361
Iteration 91/1000 | Loss: 0.00003361
Iteration 92/1000 | Loss: 0.00003360
Iteration 93/1000 | Loss: 0.00003360
Iteration 94/1000 | Loss: 0.00003360
Iteration 95/1000 | Loss: 0.00003360
Iteration 96/1000 | Loss: 0.00003359
Iteration 97/1000 | Loss: 0.00003359
Iteration 98/1000 | Loss: 0.00003359
Iteration 99/1000 | Loss: 0.00003359
Iteration 100/1000 | Loss: 0.00003359
Iteration 101/1000 | Loss: 0.00003359
Iteration 102/1000 | Loss: 0.00003359
Iteration 103/1000 | Loss: 0.00003359
Iteration 104/1000 | Loss: 0.00003359
Iteration 105/1000 | Loss: 0.00003359
Iteration 106/1000 | Loss: 0.00003358
Iteration 107/1000 | Loss: 0.00003358
Iteration 108/1000 | Loss: 0.00003358
Iteration 109/1000 | Loss: 0.00003358
Iteration 110/1000 | Loss: 0.00003358
Iteration 111/1000 | Loss: 0.00003357
Iteration 112/1000 | Loss: 0.00003357
Iteration 113/1000 | Loss: 0.00003357
Iteration 114/1000 | Loss: 0.00003356
Iteration 115/1000 | Loss: 0.00003356
Iteration 116/1000 | Loss: 0.00003356
Iteration 117/1000 | Loss: 0.00003356
Iteration 118/1000 | Loss: 0.00003356
Iteration 119/1000 | Loss: 0.00003356
Iteration 120/1000 | Loss: 0.00003356
Iteration 121/1000 | Loss: 0.00003355
Iteration 122/1000 | Loss: 0.00003355
Iteration 123/1000 | Loss: 0.00003355
Iteration 124/1000 | Loss: 0.00003355
Iteration 125/1000 | Loss: 0.00003355
Iteration 126/1000 | Loss: 0.00003355
Iteration 127/1000 | Loss: 0.00003355
Iteration 128/1000 | Loss: 0.00003355
Iteration 129/1000 | Loss: 0.00003354
Iteration 130/1000 | Loss: 0.00003354
Iteration 131/1000 | Loss: 0.00003354
Iteration 132/1000 | Loss: 0.00003354
Iteration 133/1000 | Loss: 0.00003354
Iteration 134/1000 | Loss: 0.00003354
Iteration 135/1000 | Loss: 0.00003354
Iteration 136/1000 | Loss: 0.00003353
Iteration 137/1000 | Loss: 0.00003353
Iteration 138/1000 | Loss: 0.00003353
Iteration 139/1000 | Loss: 0.00003353
Iteration 140/1000 | Loss: 0.00003353
Iteration 141/1000 | Loss: 0.00003352
Iteration 142/1000 | Loss: 0.00003352
Iteration 143/1000 | Loss: 0.00003352
Iteration 144/1000 | Loss: 0.00003352
Iteration 145/1000 | Loss: 0.00003352
Iteration 146/1000 | Loss: 0.00003352
Iteration 147/1000 | Loss: 0.00003352
Iteration 148/1000 | Loss: 0.00003352
Iteration 149/1000 | Loss: 0.00003352
Iteration 150/1000 | Loss: 0.00003351
Iteration 151/1000 | Loss: 0.00003351
Iteration 152/1000 | Loss: 0.00003351
Iteration 153/1000 | Loss: 0.00003351
Iteration 154/1000 | Loss: 0.00003351
Iteration 155/1000 | Loss: 0.00003351
Iteration 156/1000 | Loss: 0.00003351
Iteration 157/1000 | Loss: 0.00003350
Iteration 158/1000 | Loss: 0.00003350
Iteration 159/1000 | Loss: 0.00003350
Iteration 160/1000 | Loss: 0.00003350
Iteration 161/1000 | Loss: 0.00003350
Iteration 162/1000 | Loss: 0.00003350
Iteration 163/1000 | Loss: 0.00003350
Iteration 164/1000 | Loss: 0.00003350
Iteration 165/1000 | Loss: 0.00003350
Iteration 166/1000 | Loss: 0.00003350
Iteration 167/1000 | Loss: 0.00003350
Iteration 168/1000 | Loss: 0.00003350
Iteration 169/1000 | Loss: 0.00003350
Iteration 170/1000 | Loss: 0.00003350
Iteration 171/1000 | Loss: 0.00003350
Iteration 172/1000 | Loss: 0.00003350
Iteration 173/1000 | Loss: 0.00003350
Iteration 174/1000 | Loss: 0.00003350
Iteration 175/1000 | Loss: 0.00003350
Iteration 176/1000 | Loss: 0.00003349
Iteration 177/1000 | Loss: 0.00003349
Iteration 178/1000 | Loss: 0.00003349
Iteration 179/1000 | Loss: 0.00003349
Iteration 180/1000 | Loss: 0.00003349
Iteration 181/1000 | Loss: 0.00003349
Iteration 182/1000 | Loss: 0.00003349
Iteration 183/1000 | Loss: 0.00003349
Iteration 184/1000 | Loss: 0.00003349
Iteration 185/1000 | Loss: 0.00003349
Iteration 186/1000 | Loss: 0.00003349
Iteration 187/1000 | Loss: 0.00003349
Iteration 188/1000 | Loss: 0.00003349
Iteration 189/1000 | Loss: 0.00003348
Iteration 190/1000 | Loss: 0.00003348
Iteration 191/1000 | Loss: 0.00003348
Iteration 192/1000 | Loss: 0.00003348
Iteration 193/1000 | Loss: 0.00003348
Iteration 194/1000 | Loss: 0.00003348
Iteration 195/1000 | Loss: 0.00003348
Iteration 196/1000 | Loss: 0.00003348
Iteration 197/1000 | Loss: 0.00003348
Iteration 198/1000 | Loss: 0.00003348
Iteration 199/1000 | Loss: 0.00003348
Iteration 200/1000 | Loss: 0.00003348
Iteration 201/1000 | Loss: 0.00003348
Iteration 202/1000 | Loss: 0.00003348
Iteration 203/1000 | Loss: 0.00003348
Iteration 204/1000 | Loss: 0.00003348
Iteration 205/1000 | Loss: 0.00003348
Iteration 206/1000 | Loss: 0.00003348
Iteration 207/1000 | Loss: 0.00003348
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [3.348411701153964e-05, 3.348411701153964e-05, 3.348411701153964e-05, 3.348411701153964e-05, 3.348411701153964e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.348411701153964e-05

Optimization complete. Final v2v error: 4.901021480560303 mm

Highest mean error: 5.249776363372803 mm for frame 29

Lowest mean error: 4.646111488342285 mm for frame 15

Saving results

Total time: 39.17176842689514
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01227107
Iteration 2/25 | Loss: 0.00270077
Iteration 3/25 | Loss: 0.00174904
Iteration 4/25 | Loss: 0.00162421
Iteration 5/25 | Loss: 0.00185693
Iteration 6/25 | Loss: 0.00171661
Iteration 7/25 | Loss: 0.00165180
Iteration 8/25 | Loss: 0.00159228
Iteration 9/25 | Loss: 0.00139523
Iteration 10/25 | Loss: 0.00139466
Iteration 11/25 | Loss: 0.00134101
Iteration 12/25 | Loss: 0.00133451
Iteration 13/25 | Loss: 0.00131738
Iteration 14/25 | Loss: 0.00130922
Iteration 15/25 | Loss: 0.00130735
Iteration 16/25 | Loss: 0.00130632
Iteration 17/25 | Loss: 0.00131413
Iteration 18/25 | Loss: 0.00130984
Iteration 19/25 | Loss: 0.00130517
Iteration 20/25 | Loss: 0.00130439
Iteration 21/25 | Loss: 0.00130368
Iteration 22/25 | Loss: 0.00130276
Iteration 23/25 | Loss: 0.00130182
Iteration 24/25 | Loss: 0.00130130
Iteration 25/25 | Loss: 0.00130115

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.78313780
Iteration 2/25 | Loss: 0.00266331
Iteration 3/25 | Loss: 0.00266330
Iteration 4/25 | Loss: 0.00266330
Iteration 5/25 | Loss: 0.00266330
Iteration 6/25 | Loss: 0.00266329
Iteration 7/25 | Loss: 0.00266329
Iteration 8/25 | Loss: 0.00266329
Iteration 9/25 | Loss: 0.00266329
Iteration 10/25 | Loss: 0.00266329
Iteration 11/25 | Loss: 0.00266329
Iteration 12/25 | Loss: 0.00266329
Iteration 13/25 | Loss: 0.00266329
Iteration 14/25 | Loss: 0.00266329
Iteration 15/25 | Loss: 0.00266329
Iteration 16/25 | Loss: 0.00266329
Iteration 17/25 | Loss: 0.00266329
Iteration 18/25 | Loss: 0.00266329
Iteration 19/25 | Loss: 0.00266329
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0026632926892489195, 0.0026632926892489195, 0.0026632926892489195, 0.0026632926892489195, 0.0026632926892489195]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026632926892489195

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00266329
Iteration 2/1000 | Loss: 0.00103268
Iteration 3/1000 | Loss: 0.00031345
Iteration 4/1000 | Loss: 0.00025664
Iteration 5/1000 | Loss: 0.00023220
Iteration 6/1000 | Loss: 0.00021370
Iteration 7/1000 | Loss: 0.00180206
Iteration 8/1000 | Loss: 0.00108994
Iteration 9/1000 | Loss: 0.00202160
Iteration 10/1000 | Loss: 0.00101746
Iteration 11/1000 | Loss: 0.00677367
Iteration 12/1000 | Loss: 0.00235810
Iteration 13/1000 | Loss: 0.01115216
Iteration 14/1000 | Loss: 0.00294058
Iteration 15/1000 | Loss: 0.00962197
Iteration 16/1000 | Loss: 0.00301465
Iteration 17/1000 | Loss: 0.00222179
Iteration 18/1000 | Loss: 0.00065022
Iteration 19/1000 | Loss: 0.00040678
Iteration 20/1000 | Loss: 0.00130276
Iteration 21/1000 | Loss: 0.00150147
Iteration 22/1000 | Loss: 0.00147424
Iteration 23/1000 | Loss: 0.00041938
Iteration 24/1000 | Loss: 0.00027577
Iteration 25/1000 | Loss: 0.00073864
Iteration 26/1000 | Loss: 0.00127969
Iteration 27/1000 | Loss: 0.00056705
Iteration 28/1000 | Loss: 0.00063917
Iteration 29/1000 | Loss: 0.00045909
Iteration 30/1000 | Loss: 0.00128334
Iteration 31/1000 | Loss: 0.00061814
Iteration 32/1000 | Loss: 0.00080253
Iteration 33/1000 | Loss: 0.00020792
Iteration 34/1000 | Loss: 0.00018204
Iteration 35/1000 | Loss: 0.00017545
Iteration 36/1000 | Loss: 0.00162773
Iteration 37/1000 | Loss: 0.00149061
Iteration 38/1000 | Loss: 0.00019193
Iteration 39/1000 | Loss: 0.00017100
Iteration 40/1000 | Loss: 0.00016125
Iteration 41/1000 | Loss: 0.00015526
Iteration 42/1000 | Loss: 0.00092410
Iteration 43/1000 | Loss: 0.00039233
Iteration 44/1000 | Loss: 0.00015647
Iteration 45/1000 | Loss: 0.00327680
Iteration 46/1000 | Loss: 0.00155834
Iteration 47/1000 | Loss: 0.00336591
Iteration 48/1000 | Loss: 0.00383202
Iteration 49/1000 | Loss: 0.00498565
Iteration 50/1000 | Loss: 0.00124532
Iteration 51/1000 | Loss: 0.00045409
Iteration 52/1000 | Loss: 0.00253042
Iteration 53/1000 | Loss: 0.00087318
Iteration 54/1000 | Loss: 0.00118365
Iteration 55/1000 | Loss: 0.00016097
Iteration 56/1000 | Loss: 0.00131004
Iteration 57/1000 | Loss: 0.00122555
Iteration 58/1000 | Loss: 0.00099662
Iteration 59/1000 | Loss: 0.00024887
Iteration 60/1000 | Loss: 0.00040108
Iteration 61/1000 | Loss: 0.00015568
Iteration 62/1000 | Loss: 0.00014912
Iteration 63/1000 | Loss: 0.00297842
Iteration 64/1000 | Loss: 0.00230560
Iteration 65/1000 | Loss: 0.00071245
Iteration 66/1000 | Loss: 0.00435185
Iteration 67/1000 | Loss: 0.00420235
Iteration 68/1000 | Loss: 0.00230697
Iteration 69/1000 | Loss: 0.00134851
Iteration 70/1000 | Loss: 0.00164130
Iteration 71/1000 | Loss: 0.00182815
Iteration 72/1000 | Loss: 0.00153537
Iteration 73/1000 | Loss: 0.00117430
Iteration 74/1000 | Loss: 0.00141684
Iteration 75/1000 | Loss: 0.00128770
Iteration 76/1000 | Loss: 0.00033621
Iteration 77/1000 | Loss: 0.00195515
Iteration 78/1000 | Loss: 0.00442195
Iteration 79/1000 | Loss: 0.00081605
Iteration 80/1000 | Loss: 0.00160592
Iteration 81/1000 | Loss: 0.00045815
Iteration 82/1000 | Loss: 0.00313985
Iteration 83/1000 | Loss: 0.00015654
Iteration 84/1000 | Loss: 0.00013845
Iteration 85/1000 | Loss: 0.00201542
Iteration 86/1000 | Loss: 0.00095939
Iteration 87/1000 | Loss: 0.00746763
Iteration 88/1000 | Loss: 0.00894421
Iteration 89/1000 | Loss: 0.00531637
Iteration 90/1000 | Loss: 0.00297957
Iteration 91/1000 | Loss: 0.00101457
Iteration 92/1000 | Loss: 0.00155091
Iteration 93/1000 | Loss: 0.00566302
Iteration 94/1000 | Loss: 0.00203548
Iteration 95/1000 | Loss: 0.00163663
Iteration 96/1000 | Loss: 0.00171141
Iteration 97/1000 | Loss: 0.00091593
Iteration 98/1000 | Loss: 0.00576106
Iteration 99/1000 | Loss: 0.00123625
Iteration 100/1000 | Loss: 0.00729974
Iteration 101/1000 | Loss: 0.00261752
Iteration 102/1000 | Loss: 0.00048996
Iteration 103/1000 | Loss: 0.00156119
Iteration 104/1000 | Loss: 0.00125412
Iteration 105/1000 | Loss: 0.00197467
Iteration 106/1000 | Loss: 0.00245379
Iteration 107/1000 | Loss: 0.00290305
Iteration 108/1000 | Loss: 0.00226505
Iteration 109/1000 | Loss: 0.00184260
Iteration 110/1000 | Loss: 0.00227724
Iteration 111/1000 | Loss: 0.00017714
Iteration 112/1000 | Loss: 0.00021510
Iteration 113/1000 | Loss: 0.00118979
Iteration 114/1000 | Loss: 0.00046652
Iteration 115/1000 | Loss: 0.00113153
Iteration 116/1000 | Loss: 0.00025568
Iteration 117/1000 | Loss: 0.00046835
Iteration 118/1000 | Loss: 0.00076164
Iteration 119/1000 | Loss: 0.00199967
Iteration 120/1000 | Loss: 0.00191219
Iteration 121/1000 | Loss: 0.00209574
Iteration 122/1000 | Loss: 0.00169907
Iteration 123/1000 | Loss: 0.00174403
Iteration 124/1000 | Loss: 0.00108942
Iteration 125/1000 | Loss: 0.00094949
Iteration 126/1000 | Loss: 0.00067493
Iteration 127/1000 | Loss: 0.00245346
Iteration 128/1000 | Loss: 0.00180029
Iteration 129/1000 | Loss: 0.00127517
Iteration 130/1000 | Loss: 0.00244293
Iteration 131/1000 | Loss: 0.00161805
Iteration 132/1000 | Loss: 0.00170160
Iteration 133/1000 | Loss: 0.00085275
Iteration 134/1000 | Loss: 0.00089084
Iteration 135/1000 | Loss: 0.00147413
Iteration 136/1000 | Loss: 0.00063668
Iteration 137/1000 | Loss: 0.00052400
Iteration 138/1000 | Loss: 0.00020619
Iteration 139/1000 | Loss: 0.00099024
Iteration 140/1000 | Loss: 0.00064524
Iteration 141/1000 | Loss: 0.00090109
Iteration 142/1000 | Loss: 0.00093524
Iteration 143/1000 | Loss: 0.00081099
Iteration 144/1000 | Loss: 0.00036513
Iteration 145/1000 | Loss: 0.00013542
Iteration 146/1000 | Loss: 0.00098745
Iteration 147/1000 | Loss: 0.00087491
Iteration 148/1000 | Loss: 0.00088837
Iteration 149/1000 | Loss: 0.00067325
Iteration 150/1000 | Loss: 0.00010551
Iteration 151/1000 | Loss: 0.00031662
Iteration 152/1000 | Loss: 0.00028914
Iteration 153/1000 | Loss: 0.00010339
Iteration 154/1000 | Loss: 0.00009562
Iteration 155/1000 | Loss: 0.00031970
Iteration 156/1000 | Loss: 0.00240453
Iteration 157/1000 | Loss: 0.00234198
Iteration 158/1000 | Loss: 0.00071213
Iteration 159/1000 | Loss: 0.00073808
Iteration 160/1000 | Loss: 0.00025540
Iteration 161/1000 | Loss: 0.00125756
Iteration 162/1000 | Loss: 0.00285166
Iteration 163/1000 | Loss: 0.00031610
Iteration 164/1000 | Loss: 0.00053853
Iteration 165/1000 | Loss: 0.00012208
Iteration 166/1000 | Loss: 0.00040903
Iteration 167/1000 | Loss: 0.00027068
Iteration 168/1000 | Loss: 0.00010045
Iteration 169/1000 | Loss: 0.00009313
Iteration 170/1000 | Loss: 0.00073701
Iteration 171/1000 | Loss: 0.00109488
Iteration 172/1000 | Loss: 0.00070934
Iteration 173/1000 | Loss: 0.00008757
Iteration 174/1000 | Loss: 0.00008076
Iteration 175/1000 | Loss: 0.00007723
Iteration 176/1000 | Loss: 0.00007425
Iteration 177/1000 | Loss: 0.00007192
Iteration 178/1000 | Loss: 0.00007014
Iteration 179/1000 | Loss: 0.00006750
Iteration 180/1000 | Loss: 0.00006548
Iteration 181/1000 | Loss: 0.00006360
Iteration 182/1000 | Loss: 0.00039568
Iteration 183/1000 | Loss: 0.00007106
Iteration 184/1000 | Loss: 0.00006409
Iteration 185/1000 | Loss: 0.00006147
Iteration 186/1000 | Loss: 0.00006012
Iteration 187/1000 | Loss: 0.00005929
Iteration 188/1000 | Loss: 0.00005888
Iteration 189/1000 | Loss: 0.00005837
Iteration 190/1000 | Loss: 0.00005786
Iteration 191/1000 | Loss: 0.00005741
Iteration 192/1000 | Loss: 0.00005711
Iteration 193/1000 | Loss: 0.00005689
Iteration 194/1000 | Loss: 0.00005671
Iteration 195/1000 | Loss: 0.00005653
Iteration 196/1000 | Loss: 0.00005651
Iteration 197/1000 | Loss: 0.00005646
Iteration 198/1000 | Loss: 0.00005646
Iteration 199/1000 | Loss: 0.00005644
Iteration 200/1000 | Loss: 0.00005643
Iteration 201/1000 | Loss: 0.00005642
Iteration 202/1000 | Loss: 0.00005642
Iteration 203/1000 | Loss: 0.00005640
Iteration 204/1000 | Loss: 0.00005639
Iteration 205/1000 | Loss: 0.00005639
Iteration 206/1000 | Loss: 0.00005639
Iteration 207/1000 | Loss: 0.00005639
Iteration 208/1000 | Loss: 0.00005639
Iteration 209/1000 | Loss: 0.00005639
Iteration 210/1000 | Loss: 0.00005638
Iteration 211/1000 | Loss: 0.00005638
Iteration 212/1000 | Loss: 0.00005638
Iteration 213/1000 | Loss: 0.00005638
Iteration 214/1000 | Loss: 0.00005638
Iteration 215/1000 | Loss: 0.00005638
Iteration 216/1000 | Loss: 0.00005638
Iteration 217/1000 | Loss: 0.00005638
Iteration 218/1000 | Loss: 0.00005638
Iteration 219/1000 | Loss: 0.00005637
Iteration 220/1000 | Loss: 0.00005636
Iteration 221/1000 | Loss: 0.00005636
Iteration 222/1000 | Loss: 0.00005636
Iteration 223/1000 | Loss: 0.00005635
Iteration 224/1000 | Loss: 0.00005635
Iteration 225/1000 | Loss: 0.00005635
Iteration 226/1000 | Loss: 0.00005635
Iteration 227/1000 | Loss: 0.00005635
Iteration 228/1000 | Loss: 0.00005635
Iteration 229/1000 | Loss: 0.00005635
Iteration 230/1000 | Loss: 0.00005635
Iteration 231/1000 | Loss: 0.00005634
Iteration 232/1000 | Loss: 0.00005634
Iteration 233/1000 | Loss: 0.00005634
Iteration 234/1000 | Loss: 0.00005634
Iteration 235/1000 | Loss: 0.00005634
Iteration 236/1000 | Loss: 0.00005634
Iteration 237/1000 | Loss: 0.00005634
Iteration 238/1000 | Loss: 0.00005634
Iteration 239/1000 | Loss: 0.00005634
Iteration 240/1000 | Loss: 0.00005633
Iteration 241/1000 | Loss: 0.00005633
Iteration 242/1000 | Loss: 0.00005633
Iteration 243/1000 | Loss: 0.00005633
Iteration 244/1000 | Loss: 0.00005633
Iteration 245/1000 | Loss: 0.00005633
Iteration 246/1000 | Loss: 0.00005632
Iteration 247/1000 | Loss: 0.00005632
Iteration 248/1000 | Loss: 0.00005632
Iteration 249/1000 | Loss: 0.00005632
Iteration 250/1000 | Loss: 0.00005631
Iteration 251/1000 | Loss: 0.00005631
Iteration 252/1000 | Loss: 0.00005631
Iteration 253/1000 | Loss: 0.00005631
Iteration 254/1000 | Loss: 0.00005631
Iteration 255/1000 | Loss: 0.00005631
Iteration 256/1000 | Loss: 0.00005631
Iteration 257/1000 | Loss: 0.00005630
Iteration 258/1000 | Loss: 0.00005630
Iteration 259/1000 | Loss: 0.00005629
Iteration 260/1000 | Loss: 0.00005629
Iteration 261/1000 | Loss: 0.00005629
Iteration 262/1000 | Loss: 0.00005628
Iteration 263/1000 | Loss: 0.00005628
Iteration 264/1000 | Loss: 0.00005628
Iteration 265/1000 | Loss: 0.00005628
Iteration 266/1000 | Loss: 0.00005627
Iteration 267/1000 | Loss: 0.00005627
Iteration 268/1000 | Loss: 0.00005627
Iteration 269/1000 | Loss: 0.00005627
Iteration 270/1000 | Loss: 0.00005627
Iteration 271/1000 | Loss: 0.00005627
Iteration 272/1000 | Loss: 0.00005626
Iteration 273/1000 | Loss: 0.00005626
Iteration 274/1000 | Loss: 0.00005626
Iteration 275/1000 | Loss: 0.00005626
Iteration 276/1000 | Loss: 0.00005626
Iteration 277/1000 | Loss: 0.00005626
Iteration 278/1000 | Loss: 0.00005626
Iteration 279/1000 | Loss: 0.00005625
Iteration 280/1000 | Loss: 0.00005625
Iteration 281/1000 | Loss: 0.00005625
Iteration 282/1000 | Loss: 0.00005625
Iteration 283/1000 | Loss: 0.00005625
Iteration 284/1000 | Loss: 0.00005625
Iteration 285/1000 | Loss: 0.00005625
Iteration 286/1000 | Loss: 0.00005625
Iteration 287/1000 | Loss: 0.00005625
Iteration 288/1000 | Loss: 0.00005625
Iteration 289/1000 | Loss: 0.00005625
Iteration 290/1000 | Loss: 0.00005625
Iteration 291/1000 | Loss: 0.00005624
Iteration 292/1000 | Loss: 0.00005624
Iteration 293/1000 | Loss: 0.00005624
Iteration 294/1000 | Loss: 0.00005624
Iteration 295/1000 | Loss: 0.00005624
Iteration 296/1000 | Loss: 0.00005623
Iteration 297/1000 | Loss: 0.00005623
Iteration 298/1000 | Loss: 0.00005623
Iteration 299/1000 | Loss: 0.00005623
Iteration 300/1000 | Loss: 0.00005623
Iteration 301/1000 | Loss: 0.00005623
Iteration 302/1000 | Loss: 0.00005623
Iteration 303/1000 | Loss: 0.00005623
Iteration 304/1000 | Loss: 0.00005623
Iteration 305/1000 | Loss: 0.00005623
Iteration 306/1000 | Loss: 0.00005622
Iteration 307/1000 | Loss: 0.00005622
Iteration 308/1000 | Loss: 0.00005622
Iteration 309/1000 | Loss: 0.00005622
Iteration 310/1000 | Loss: 0.00005622
Iteration 311/1000 | Loss: 0.00005622
Iteration 312/1000 | Loss: 0.00005622
Iteration 313/1000 | Loss: 0.00005621
Iteration 314/1000 | Loss: 0.00005621
Iteration 315/1000 | Loss: 0.00005621
Iteration 316/1000 | Loss: 0.00005621
Iteration 317/1000 | Loss: 0.00005621
Iteration 318/1000 | Loss: 0.00005621
Iteration 319/1000 | Loss: 0.00005621
Iteration 320/1000 | Loss: 0.00005621
Iteration 321/1000 | Loss: 0.00005621
Iteration 322/1000 | Loss: 0.00005621
Iteration 323/1000 | Loss: 0.00005621
Iteration 324/1000 | Loss: 0.00005621
Iteration 325/1000 | Loss: 0.00005621
Iteration 326/1000 | Loss: 0.00005620
Iteration 327/1000 | Loss: 0.00005620
Iteration 328/1000 | Loss: 0.00005620
Iteration 329/1000 | Loss: 0.00005620
Iteration 330/1000 | Loss: 0.00005620
Iteration 331/1000 | Loss: 0.00005620
Iteration 332/1000 | Loss: 0.00005620
Iteration 333/1000 | Loss: 0.00005620
Iteration 334/1000 | Loss: 0.00005620
Iteration 335/1000 | Loss: 0.00005619
Iteration 336/1000 | Loss: 0.00005619
Iteration 337/1000 | Loss: 0.00005619
Iteration 338/1000 | Loss: 0.00005619
Iteration 339/1000 | Loss: 0.00005619
Iteration 340/1000 | Loss: 0.00005619
Iteration 341/1000 | Loss: 0.00005619
Iteration 342/1000 | Loss: 0.00005618
Iteration 343/1000 | Loss: 0.00005618
Iteration 344/1000 | Loss: 0.00005618
Iteration 345/1000 | Loss: 0.00005618
Iteration 346/1000 | Loss: 0.00005618
Iteration 347/1000 | Loss: 0.00005618
Iteration 348/1000 | Loss: 0.00005618
Iteration 349/1000 | Loss: 0.00005618
Iteration 350/1000 | Loss: 0.00005618
Iteration 351/1000 | Loss: 0.00005618
Iteration 352/1000 | Loss: 0.00005618
Iteration 353/1000 | Loss: 0.00005618
Iteration 354/1000 | Loss: 0.00005618
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 354. Stopping optimization.
Last 5 losses: [5.6181477702921256e-05, 5.6181477702921256e-05, 5.6181477702921256e-05, 5.6181477702921256e-05, 5.6181477702921256e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.6181477702921256e-05

Optimization complete. Final v2v error: 5.421354293823242 mm

Highest mean error: 12.455992698669434 mm for frame 34

Lowest mean error: 4.61928653717041 mm for frame 134

Saving results

Total time: 325.6267726421356
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00512990
Iteration 2/25 | Loss: 0.00134170
Iteration 3/25 | Loss: 0.00090369
Iteration 4/25 | Loss: 0.00084426
Iteration 5/25 | Loss: 0.00082919
Iteration 6/25 | Loss: 0.00082363
Iteration 7/25 | Loss: 0.00082218
Iteration 8/25 | Loss: 0.00082190
Iteration 9/25 | Loss: 0.00082190
Iteration 10/25 | Loss: 0.00082190
Iteration 11/25 | Loss: 0.00082190
Iteration 12/25 | Loss: 0.00082190
Iteration 13/25 | Loss: 0.00082190
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008218979346565902, 0.0008218979346565902, 0.0008218979346565902, 0.0008218979346565902, 0.0008218979346565902]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008218979346565902

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18655491
Iteration 2/25 | Loss: 0.00046897
Iteration 3/25 | Loss: 0.00046895
Iteration 4/25 | Loss: 0.00046895
Iteration 5/25 | Loss: 0.00046895
Iteration 6/25 | Loss: 0.00046895
Iteration 7/25 | Loss: 0.00046895
Iteration 8/25 | Loss: 0.00046895
Iteration 9/25 | Loss: 0.00046895
Iteration 10/25 | Loss: 0.00046895
Iteration 11/25 | Loss: 0.00046895
Iteration 12/25 | Loss: 0.00046895
Iteration 13/25 | Loss: 0.00046895
Iteration 14/25 | Loss: 0.00046895
Iteration 15/25 | Loss: 0.00046895
Iteration 16/25 | Loss: 0.00046895
Iteration 17/25 | Loss: 0.00046895
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00046894856495782733, 0.00046894856495782733, 0.00046894856495782733, 0.00046894856495782733, 0.00046894856495782733]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00046894856495782733

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046895
Iteration 2/1000 | Loss: 0.00004933
Iteration 3/1000 | Loss: 0.00003294
Iteration 4/1000 | Loss: 0.00002799
Iteration 5/1000 | Loss: 0.00002641
Iteration 6/1000 | Loss: 0.00002510
Iteration 7/1000 | Loss: 0.00002452
Iteration 8/1000 | Loss: 0.00002398
Iteration 9/1000 | Loss: 0.00002363
Iteration 10/1000 | Loss: 0.00002336
Iteration 11/1000 | Loss: 0.00002310
Iteration 12/1000 | Loss: 0.00002291
Iteration 13/1000 | Loss: 0.00002269
Iteration 14/1000 | Loss: 0.00002249
Iteration 15/1000 | Loss: 0.00002233
Iteration 16/1000 | Loss: 0.00002229
Iteration 17/1000 | Loss: 0.00002223
Iteration 18/1000 | Loss: 0.00002216
Iteration 19/1000 | Loss: 0.00002216
Iteration 20/1000 | Loss: 0.00002215
Iteration 21/1000 | Loss: 0.00002213
Iteration 22/1000 | Loss: 0.00002212
Iteration 23/1000 | Loss: 0.00002209
Iteration 24/1000 | Loss: 0.00002209
Iteration 25/1000 | Loss: 0.00002208
Iteration 26/1000 | Loss: 0.00002207
Iteration 27/1000 | Loss: 0.00002205
Iteration 28/1000 | Loss: 0.00002203
Iteration 29/1000 | Loss: 0.00002203
Iteration 30/1000 | Loss: 0.00002201
Iteration 31/1000 | Loss: 0.00002200
Iteration 32/1000 | Loss: 0.00002199
Iteration 33/1000 | Loss: 0.00002199
Iteration 34/1000 | Loss: 0.00002198
Iteration 35/1000 | Loss: 0.00002197
Iteration 36/1000 | Loss: 0.00002197
Iteration 37/1000 | Loss: 0.00002196
Iteration 38/1000 | Loss: 0.00002196
Iteration 39/1000 | Loss: 0.00002196
Iteration 40/1000 | Loss: 0.00002196
Iteration 41/1000 | Loss: 0.00002195
Iteration 42/1000 | Loss: 0.00002195
Iteration 43/1000 | Loss: 0.00002195
Iteration 44/1000 | Loss: 0.00002194
Iteration 45/1000 | Loss: 0.00002194
Iteration 46/1000 | Loss: 0.00002194
Iteration 47/1000 | Loss: 0.00002194
Iteration 48/1000 | Loss: 0.00002194
Iteration 49/1000 | Loss: 0.00002194
Iteration 50/1000 | Loss: 0.00002194
Iteration 51/1000 | Loss: 0.00002194
Iteration 52/1000 | Loss: 0.00002194
Iteration 53/1000 | Loss: 0.00002194
Iteration 54/1000 | Loss: 0.00002194
Iteration 55/1000 | Loss: 0.00002194
Iteration 56/1000 | Loss: 0.00002194
Iteration 57/1000 | Loss: 0.00002193
Iteration 58/1000 | Loss: 0.00002193
Iteration 59/1000 | Loss: 0.00002193
Iteration 60/1000 | Loss: 0.00002193
Iteration 61/1000 | Loss: 0.00002193
Iteration 62/1000 | Loss: 0.00002193
Iteration 63/1000 | Loss: 0.00002193
Iteration 64/1000 | Loss: 0.00002193
Iteration 65/1000 | Loss: 0.00002193
Iteration 66/1000 | Loss: 0.00002193
Iteration 67/1000 | Loss: 0.00002193
Iteration 68/1000 | Loss: 0.00002193
Iteration 69/1000 | Loss: 0.00002192
Iteration 70/1000 | Loss: 0.00002192
Iteration 71/1000 | Loss: 0.00002192
Iteration 72/1000 | Loss: 0.00002191
Iteration 73/1000 | Loss: 0.00002191
Iteration 74/1000 | Loss: 0.00002191
Iteration 75/1000 | Loss: 0.00002191
Iteration 76/1000 | Loss: 0.00002191
Iteration 77/1000 | Loss: 0.00002191
Iteration 78/1000 | Loss: 0.00002191
Iteration 79/1000 | Loss: 0.00002191
Iteration 80/1000 | Loss: 0.00002190
Iteration 81/1000 | Loss: 0.00002190
Iteration 82/1000 | Loss: 0.00002190
Iteration 83/1000 | Loss: 0.00002190
Iteration 84/1000 | Loss: 0.00002190
Iteration 85/1000 | Loss: 0.00002190
Iteration 86/1000 | Loss: 0.00002190
Iteration 87/1000 | Loss: 0.00002190
Iteration 88/1000 | Loss: 0.00002190
Iteration 89/1000 | Loss: 0.00002190
Iteration 90/1000 | Loss: 0.00002189
Iteration 91/1000 | Loss: 0.00002189
Iteration 92/1000 | Loss: 0.00002189
Iteration 93/1000 | Loss: 0.00002188
Iteration 94/1000 | Loss: 0.00002188
Iteration 95/1000 | Loss: 0.00002188
Iteration 96/1000 | Loss: 0.00002188
Iteration 97/1000 | Loss: 0.00002188
Iteration 98/1000 | Loss: 0.00002187
Iteration 99/1000 | Loss: 0.00002187
Iteration 100/1000 | Loss: 0.00002187
Iteration 101/1000 | Loss: 0.00002187
Iteration 102/1000 | Loss: 0.00002187
Iteration 103/1000 | Loss: 0.00002187
Iteration 104/1000 | Loss: 0.00002187
Iteration 105/1000 | Loss: 0.00002187
Iteration 106/1000 | Loss: 0.00002186
Iteration 107/1000 | Loss: 0.00002186
Iteration 108/1000 | Loss: 0.00002186
Iteration 109/1000 | Loss: 0.00002186
Iteration 110/1000 | Loss: 0.00002186
Iteration 111/1000 | Loss: 0.00002186
Iteration 112/1000 | Loss: 0.00002186
Iteration 113/1000 | Loss: 0.00002185
Iteration 114/1000 | Loss: 0.00002185
Iteration 115/1000 | Loss: 0.00002185
Iteration 116/1000 | Loss: 0.00002185
Iteration 117/1000 | Loss: 0.00002185
Iteration 118/1000 | Loss: 0.00002185
Iteration 119/1000 | Loss: 0.00002185
Iteration 120/1000 | Loss: 0.00002185
Iteration 121/1000 | Loss: 0.00002185
Iteration 122/1000 | Loss: 0.00002185
Iteration 123/1000 | Loss: 0.00002185
Iteration 124/1000 | Loss: 0.00002185
Iteration 125/1000 | Loss: 0.00002185
Iteration 126/1000 | Loss: 0.00002185
Iteration 127/1000 | Loss: 0.00002185
Iteration 128/1000 | Loss: 0.00002185
Iteration 129/1000 | Loss: 0.00002185
Iteration 130/1000 | Loss: 0.00002185
Iteration 131/1000 | Loss: 0.00002185
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [2.1849264157935977e-05, 2.1849264157935977e-05, 2.1849264157935977e-05, 2.1849264157935977e-05, 2.1849264157935977e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1849264157935977e-05

Optimization complete. Final v2v error: 3.7622079849243164 mm

Highest mean error: 5.423121929168701 mm for frame 103

Lowest mean error: 2.712909460067749 mm for frame 74

Saving results

Total time: 50.18303680419922
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01004394
Iteration 2/25 | Loss: 0.00273758
Iteration 3/25 | Loss: 0.00165969
Iteration 4/25 | Loss: 0.00133833
Iteration 5/25 | Loss: 0.00130507
Iteration 6/25 | Loss: 0.00129638
Iteration 7/25 | Loss: 0.00106680
Iteration 8/25 | Loss: 0.00090698
Iteration 9/25 | Loss: 0.00086130
Iteration 10/25 | Loss: 0.00086496
Iteration 11/25 | Loss: 0.00087010
Iteration 12/25 | Loss: 0.00084349
Iteration 13/25 | Loss: 0.00082777
Iteration 14/25 | Loss: 0.00082104
Iteration 15/25 | Loss: 0.00082431
Iteration 16/25 | Loss: 0.00082146
Iteration 17/25 | Loss: 0.00082025
Iteration 18/25 | Loss: 0.00081976
Iteration 19/25 | Loss: 0.00081976
Iteration 20/25 | Loss: 0.00081976
Iteration 21/25 | Loss: 0.00081976
Iteration 22/25 | Loss: 0.00081976
Iteration 23/25 | Loss: 0.00081976
Iteration 24/25 | Loss: 0.00081976
Iteration 25/25 | Loss: 0.00081975

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53621519
Iteration 2/25 | Loss: 0.00145976
Iteration 3/25 | Loss: 0.00067687
Iteration 4/25 | Loss: 0.00067686
Iteration 5/25 | Loss: 0.00067686
Iteration 6/25 | Loss: 0.00067686
Iteration 7/25 | Loss: 0.00067686
Iteration 8/25 | Loss: 0.00067686
Iteration 9/25 | Loss: 0.00067686
Iteration 10/25 | Loss: 0.00067686
Iteration 11/25 | Loss: 0.00067686
Iteration 12/25 | Loss: 0.00067686
Iteration 13/25 | Loss: 0.00067686
Iteration 14/25 | Loss: 0.00067686
Iteration 15/25 | Loss: 0.00067686
Iteration 16/25 | Loss: 0.00067686
Iteration 17/25 | Loss: 0.00067686
Iteration 18/25 | Loss: 0.00067686
Iteration 19/25 | Loss: 0.00067686
Iteration 20/25 | Loss: 0.00067686
Iteration 21/25 | Loss: 0.00067686
Iteration 22/25 | Loss: 0.00067686
Iteration 23/25 | Loss: 0.00067686
Iteration 24/25 | Loss: 0.00067686
Iteration 25/25 | Loss: 0.00067686

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067686
Iteration 2/1000 | Loss: 0.00035752
Iteration 3/1000 | Loss: 0.00021929
Iteration 4/1000 | Loss: 0.00026150
Iteration 5/1000 | Loss: 0.00004768
Iteration 6/1000 | Loss: 0.00005755
Iteration 7/1000 | Loss: 0.00003346
Iteration 8/1000 | Loss: 0.00002617
Iteration 9/1000 | Loss: 0.00003540
Iteration 10/1000 | Loss: 0.00001951
Iteration 11/1000 | Loss: 0.00001875
Iteration 12/1000 | Loss: 0.00001826
Iteration 13/1000 | Loss: 0.00006772
Iteration 14/1000 | Loss: 0.00001809
Iteration 15/1000 | Loss: 0.00001778
Iteration 16/1000 | Loss: 0.00010131
Iteration 17/1000 | Loss: 0.00001770
Iteration 18/1000 | Loss: 0.00010112
Iteration 19/1000 | Loss: 0.00006408
Iteration 20/1000 | Loss: 0.00013135
Iteration 21/1000 | Loss: 0.00003153
Iteration 22/1000 | Loss: 0.00003710
Iteration 23/1000 | Loss: 0.00001755
Iteration 24/1000 | Loss: 0.00001736
Iteration 25/1000 | Loss: 0.00001736
Iteration 26/1000 | Loss: 0.00001733
Iteration 27/1000 | Loss: 0.00001732
Iteration 28/1000 | Loss: 0.00001732
Iteration 29/1000 | Loss: 0.00001732
Iteration 30/1000 | Loss: 0.00001731
Iteration 31/1000 | Loss: 0.00001731
Iteration 32/1000 | Loss: 0.00001730
Iteration 33/1000 | Loss: 0.00001730
Iteration 34/1000 | Loss: 0.00001727
Iteration 35/1000 | Loss: 0.00001726
Iteration 36/1000 | Loss: 0.00001724
Iteration 37/1000 | Loss: 0.00001723
Iteration 38/1000 | Loss: 0.00001722
Iteration 39/1000 | Loss: 0.00001722
Iteration 40/1000 | Loss: 0.00001721
Iteration 41/1000 | Loss: 0.00001719
Iteration 42/1000 | Loss: 0.00001718
Iteration 43/1000 | Loss: 0.00001718
Iteration 44/1000 | Loss: 0.00001718
Iteration 45/1000 | Loss: 0.00001718
Iteration 46/1000 | Loss: 0.00001718
Iteration 47/1000 | Loss: 0.00001718
Iteration 48/1000 | Loss: 0.00001718
Iteration 49/1000 | Loss: 0.00001718
Iteration 50/1000 | Loss: 0.00001718
Iteration 51/1000 | Loss: 0.00001716
Iteration 52/1000 | Loss: 0.00001716
Iteration 53/1000 | Loss: 0.00001715
Iteration 54/1000 | Loss: 0.00001715
Iteration 55/1000 | Loss: 0.00001715
Iteration 56/1000 | Loss: 0.00001715
Iteration 57/1000 | Loss: 0.00001715
Iteration 58/1000 | Loss: 0.00001715
Iteration 59/1000 | Loss: 0.00001715
Iteration 60/1000 | Loss: 0.00001715
Iteration 61/1000 | Loss: 0.00001714
Iteration 62/1000 | Loss: 0.00001714
Iteration 63/1000 | Loss: 0.00001714
Iteration 64/1000 | Loss: 0.00001714
Iteration 65/1000 | Loss: 0.00001713
Iteration 66/1000 | Loss: 0.00001713
Iteration 67/1000 | Loss: 0.00001713
Iteration 68/1000 | Loss: 0.00001712
Iteration 69/1000 | Loss: 0.00001712
Iteration 70/1000 | Loss: 0.00001712
Iteration 71/1000 | Loss: 0.00001712
Iteration 72/1000 | Loss: 0.00001711
Iteration 73/1000 | Loss: 0.00001711
Iteration 74/1000 | Loss: 0.00001711
Iteration 75/1000 | Loss: 0.00001711
Iteration 76/1000 | Loss: 0.00001711
Iteration 77/1000 | Loss: 0.00001711
Iteration 78/1000 | Loss: 0.00001710
Iteration 79/1000 | Loss: 0.00001710
Iteration 80/1000 | Loss: 0.00001710
Iteration 81/1000 | Loss: 0.00001710
Iteration 82/1000 | Loss: 0.00001709
Iteration 83/1000 | Loss: 0.00001709
Iteration 84/1000 | Loss: 0.00001709
Iteration 85/1000 | Loss: 0.00001709
Iteration 86/1000 | Loss: 0.00001709
Iteration 87/1000 | Loss: 0.00001709
Iteration 88/1000 | Loss: 0.00001709
Iteration 89/1000 | Loss: 0.00001709
Iteration 90/1000 | Loss: 0.00001709
Iteration 91/1000 | Loss: 0.00001709
Iteration 92/1000 | Loss: 0.00001708
Iteration 93/1000 | Loss: 0.00001708
Iteration 94/1000 | Loss: 0.00001708
Iteration 95/1000 | Loss: 0.00001708
Iteration 96/1000 | Loss: 0.00001707
Iteration 97/1000 | Loss: 0.00001707
Iteration 98/1000 | Loss: 0.00001707
Iteration 99/1000 | Loss: 0.00001707
Iteration 100/1000 | Loss: 0.00001706
Iteration 101/1000 | Loss: 0.00001706
Iteration 102/1000 | Loss: 0.00001706
Iteration 103/1000 | Loss: 0.00001706
Iteration 104/1000 | Loss: 0.00001705
Iteration 105/1000 | Loss: 0.00001705
Iteration 106/1000 | Loss: 0.00001705
Iteration 107/1000 | Loss: 0.00001705
Iteration 108/1000 | Loss: 0.00001704
Iteration 109/1000 | Loss: 0.00001704
Iteration 110/1000 | Loss: 0.00001704
Iteration 111/1000 | Loss: 0.00001703
Iteration 112/1000 | Loss: 0.00001703
Iteration 113/1000 | Loss: 0.00001703
Iteration 114/1000 | Loss: 0.00001703
Iteration 115/1000 | Loss: 0.00001703
Iteration 116/1000 | Loss: 0.00001703
Iteration 117/1000 | Loss: 0.00001702
Iteration 118/1000 | Loss: 0.00001702
Iteration 119/1000 | Loss: 0.00001702
Iteration 120/1000 | Loss: 0.00001702
Iteration 121/1000 | Loss: 0.00001702
Iteration 122/1000 | Loss: 0.00001702
Iteration 123/1000 | Loss: 0.00001702
Iteration 124/1000 | Loss: 0.00001702
Iteration 125/1000 | Loss: 0.00001702
Iteration 126/1000 | Loss: 0.00001702
Iteration 127/1000 | Loss: 0.00001702
Iteration 128/1000 | Loss: 0.00001701
Iteration 129/1000 | Loss: 0.00001701
Iteration 130/1000 | Loss: 0.00001701
Iteration 131/1000 | Loss: 0.00001701
Iteration 132/1000 | Loss: 0.00001701
Iteration 133/1000 | Loss: 0.00001701
Iteration 134/1000 | Loss: 0.00001701
Iteration 135/1000 | Loss: 0.00001701
Iteration 136/1000 | Loss: 0.00001701
Iteration 137/1000 | Loss: 0.00001701
Iteration 138/1000 | Loss: 0.00001701
Iteration 139/1000 | Loss: 0.00001701
Iteration 140/1000 | Loss: 0.00001701
Iteration 141/1000 | Loss: 0.00001701
Iteration 142/1000 | Loss: 0.00001701
Iteration 143/1000 | Loss: 0.00001701
Iteration 144/1000 | Loss: 0.00001701
Iteration 145/1000 | Loss: 0.00001701
Iteration 146/1000 | Loss: 0.00001701
Iteration 147/1000 | Loss: 0.00001701
Iteration 148/1000 | Loss: 0.00001701
Iteration 149/1000 | Loss: 0.00001701
Iteration 150/1000 | Loss: 0.00001701
Iteration 151/1000 | Loss: 0.00001701
Iteration 152/1000 | Loss: 0.00001701
Iteration 153/1000 | Loss: 0.00001701
Iteration 154/1000 | Loss: 0.00001701
Iteration 155/1000 | Loss: 0.00001701
Iteration 156/1000 | Loss: 0.00001701
Iteration 157/1000 | Loss: 0.00001701
Iteration 158/1000 | Loss: 0.00001701
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [1.7012574971886352e-05, 1.7012574971886352e-05, 1.7012574971886352e-05, 1.7012574971886352e-05, 1.7012574971886352e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7012574971886352e-05

Optimization complete. Final v2v error: 3.3842244148254395 mm

Highest mean error: 4.262399196624756 mm for frame 0

Lowest mean error: 2.9205870628356934 mm for frame 170

Saving results

Total time: 73.19347357749939
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01038652
Iteration 2/25 | Loss: 0.00206484
Iteration 3/25 | Loss: 0.00139420
Iteration 4/25 | Loss: 0.00121482
Iteration 5/25 | Loss: 0.00125487
Iteration 6/25 | Loss: 0.00121889
Iteration 7/25 | Loss: 0.00118389
Iteration 8/25 | Loss: 0.00115055
Iteration 9/25 | Loss: 0.00110861
Iteration 10/25 | Loss: 0.00110195
Iteration 11/25 | Loss: 0.00107744
Iteration 12/25 | Loss: 0.00103841
Iteration 13/25 | Loss: 0.00101480
Iteration 14/25 | Loss: 0.00099556
Iteration 15/25 | Loss: 0.00096009
Iteration 16/25 | Loss: 0.00095232
Iteration 17/25 | Loss: 0.00094154
Iteration 18/25 | Loss: 0.00093937
Iteration 19/25 | Loss: 0.00094305
Iteration 20/25 | Loss: 0.00093977
Iteration 21/25 | Loss: 0.00093532
Iteration 22/25 | Loss: 0.00092671
Iteration 23/25 | Loss: 0.00092253
Iteration 24/25 | Loss: 0.00091594
Iteration 25/25 | Loss: 0.00092217

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38986182
Iteration 2/25 | Loss: 0.00210565
Iteration 3/25 | Loss: 0.00193009
Iteration 4/25 | Loss: 0.00193009
Iteration 5/25 | Loss: 0.00193009
Iteration 6/25 | Loss: 0.00193009
Iteration 7/25 | Loss: 0.00193009
Iteration 8/25 | Loss: 0.00193009
Iteration 9/25 | Loss: 0.00193009
Iteration 10/25 | Loss: 0.00193009
Iteration 11/25 | Loss: 0.00193009
Iteration 12/25 | Loss: 0.00193009
Iteration 13/25 | Loss: 0.00193009
Iteration 14/25 | Loss: 0.00193009
Iteration 15/25 | Loss: 0.00193009
Iteration 16/25 | Loss: 0.00193009
Iteration 17/25 | Loss: 0.00193009
Iteration 18/25 | Loss: 0.00193009
Iteration 19/25 | Loss: 0.00193009
Iteration 20/25 | Loss: 0.00193009
Iteration 21/25 | Loss: 0.00193009
Iteration 22/25 | Loss: 0.00193009
Iteration 23/25 | Loss: 0.00193009
Iteration 24/25 | Loss: 0.00193009
Iteration 25/25 | Loss: 0.00193009

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00193009
Iteration 2/1000 | Loss: 0.00103442
Iteration 3/1000 | Loss: 0.00036471
Iteration 4/1000 | Loss: 0.00065716
Iteration 5/1000 | Loss: 0.00057387
Iteration 6/1000 | Loss: 0.00052061
Iteration 7/1000 | Loss: 0.00050374
Iteration 8/1000 | Loss: 0.00036446
Iteration 9/1000 | Loss: 0.00047984
Iteration 10/1000 | Loss: 0.00045111
Iteration 11/1000 | Loss: 0.00034528
Iteration 12/1000 | Loss: 0.00052849
Iteration 13/1000 | Loss: 0.00040385
Iteration 14/1000 | Loss: 0.00048238
Iteration 15/1000 | Loss: 0.00038114
Iteration 16/1000 | Loss: 0.00034928
Iteration 17/1000 | Loss: 0.00032920
Iteration 18/1000 | Loss: 0.00037225
Iteration 19/1000 | Loss: 0.00019240
Iteration 20/1000 | Loss: 0.00046082
Iteration 21/1000 | Loss: 0.00037619
Iteration 22/1000 | Loss: 0.00033027
Iteration 23/1000 | Loss: 0.00021135
Iteration 24/1000 | Loss: 0.00027265
Iteration 25/1000 | Loss: 0.00029299
Iteration 26/1000 | Loss: 0.00035799
Iteration 27/1000 | Loss: 0.00020809
Iteration 28/1000 | Loss: 0.00018211
Iteration 29/1000 | Loss: 0.00011938
Iteration 30/1000 | Loss: 0.00023264
Iteration 31/1000 | Loss: 0.00018088
Iteration 32/1000 | Loss: 0.00012340
Iteration 33/1000 | Loss: 0.00023338
Iteration 34/1000 | Loss: 0.00027791
Iteration 35/1000 | Loss: 0.00021643
Iteration 36/1000 | Loss: 0.00013505
Iteration 37/1000 | Loss: 0.00011577
Iteration 38/1000 | Loss: 0.00013180
Iteration 39/1000 | Loss: 0.00021604
Iteration 40/1000 | Loss: 0.00018409
Iteration 41/1000 | Loss: 0.00013879
Iteration 42/1000 | Loss: 0.00014160
Iteration 43/1000 | Loss: 0.00021973
Iteration 44/1000 | Loss: 0.00018748
Iteration 45/1000 | Loss: 0.00026320
Iteration 46/1000 | Loss: 0.00018173
Iteration 47/1000 | Loss: 0.00029369
Iteration 48/1000 | Loss: 0.00023283
Iteration 49/1000 | Loss: 0.00015887
Iteration 50/1000 | Loss: 0.00013438
Iteration 51/1000 | Loss: 0.00049649
Iteration 52/1000 | Loss: 0.00040466
Iteration 53/1000 | Loss: 0.00075730
Iteration 54/1000 | Loss: 0.00036037
Iteration 55/1000 | Loss: 0.00023521
Iteration 56/1000 | Loss: 0.00023042
Iteration 57/1000 | Loss: 0.00025574
Iteration 58/1000 | Loss: 0.00035183
Iteration 59/1000 | Loss: 0.00089762
Iteration 60/1000 | Loss: 0.00128402
Iteration 61/1000 | Loss: 0.00057733
Iteration 62/1000 | Loss: 0.00074221
Iteration 63/1000 | Loss: 0.00041154
Iteration 64/1000 | Loss: 0.00068070
Iteration 65/1000 | Loss: 0.00033231
Iteration 66/1000 | Loss: 0.00054663
Iteration 67/1000 | Loss: 0.00011776
Iteration 68/1000 | Loss: 0.00031016
Iteration 69/1000 | Loss: 0.00020735
Iteration 70/1000 | Loss: 0.00029488
Iteration 71/1000 | Loss: 0.00016427
Iteration 72/1000 | Loss: 0.00021810
Iteration 73/1000 | Loss: 0.00017590
Iteration 74/1000 | Loss: 0.00013447
Iteration 75/1000 | Loss: 0.00013151
Iteration 76/1000 | Loss: 0.00024104
Iteration 77/1000 | Loss: 0.00020692
Iteration 78/1000 | Loss: 0.00013582
Iteration 79/1000 | Loss: 0.00013350
Iteration 80/1000 | Loss: 0.00063857
Iteration 81/1000 | Loss: 0.00027929
Iteration 82/1000 | Loss: 0.00036829
Iteration 83/1000 | Loss: 0.00034445
Iteration 84/1000 | Loss: 0.00024456
Iteration 85/1000 | Loss: 0.00024269
Iteration 86/1000 | Loss: 0.00081939
Iteration 87/1000 | Loss: 0.00250522
Iteration 88/1000 | Loss: 0.00077727
Iteration 89/1000 | Loss: 0.00154104
Iteration 90/1000 | Loss: 0.00092919
Iteration 91/1000 | Loss: 0.00131737
Iteration 92/1000 | Loss: 0.00183394
Iteration 93/1000 | Loss: 0.00180907
Iteration 94/1000 | Loss: 0.00103795
Iteration 95/1000 | Loss: 0.00072215
Iteration 96/1000 | Loss: 0.00047482
Iteration 97/1000 | Loss: 0.00021339
Iteration 98/1000 | Loss: 0.00043961
Iteration 99/1000 | Loss: 0.00150870
Iteration 100/1000 | Loss: 0.00052415
Iteration 101/1000 | Loss: 0.00057024
Iteration 102/1000 | Loss: 0.00074075
Iteration 103/1000 | Loss: 0.00007047
Iteration 104/1000 | Loss: 0.00005723
Iteration 105/1000 | Loss: 0.00005577
Iteration 106/1000 | Loss: 0.00006101
Iteration 107/1000 | Loss: 0.00003890
Iteration 108/1000 | Loss: 0.00003243
Iteration 109/1000 | Loss: 0.00002738
Iteration 110/1000 | Loss: 0.00002503
Iteration 111/1000 | Loss: 0.00002315
Iteration 112/1000 | Loss: 0.00002202
Iteration 113/1000 | Loss: 0.00002126
Iteration 114/1000 | Loss: 0.00002055
Iteration 115/1000 | Loss: 0.00001980
Iteration 116/1000 | Loss: 0.00001908
Iteration 117/1000 | Loss: 0.00001818
Iteration 118/1000 | Loss: 0.00001760
Iteration 119/1000 | Loss: 0.00001731
Iteration 120/1000 | Loss: 0.00001707
Iteration 121/1000 | Loss: 0.00001702
Iteration 122/1000 | Loss: 0.00001697
Iteration 123/1000 | Loss: 0.00001694
Iteration 124/1000 | Loss: 0.00001693
Iteration 125/1000 | Loss: 0.00001693
Iteration 126/1000 | Loss: 0.00001692
Iteration 127/1000 | Loss: 0.00001691
Iteration 128/1000 | Loss: 0.00001690
Iteration 129/1000 | Loss: 0.00001690
Iteration 130/1000 | Loss: 0.00001689
Iteration 131/1000 | Loss: 0.00001688
Iteration 132/1000 | Loss: 0.00001688
Iteration 133/1000 | Loss: 0.00001687
Iteration 134/1000 | Loss: 0.00001687
Iteration 135/1000 | Loss: 0.00001687
Iteration 136/1000 | Loss: 0.00001687
Iteration 137/1000 | Loss: 0.00001687
Iteration 138/1000 | Loss: 0.00001687
Iteration 139/1000 | Loss: 0.00001686
Iteration 140/1000 | Loss: 0.00001686
Iteration 141/1000 | Loss: 0.00001686
Iteration 142/1000 | Loss: 0.00001686
Iteration 143/1000 | Loss: 0.00001686
Iteration 144/1000 | Loss: 0.00001685
Iteration 145/1000 | Loss: 0.00001685
Iteration 146/1000 | Loss: 0.00001685
Iteration 147/1000 | Loss: 0.00001685
Iteration 148/1000 | Loss: 0.00001685
Iteration 149/1000 | Loss: 0.00001685
Iteration 150/1000 | Loss: 0.00001684
Iteration 151/1000 | Loss: 0.00001684
Iteration 152/1000 | Loss: 0.00001684
Iteration 153/1000 | Loss: 0.00001684
Iteration 154/1000 | Loss: 0.00001684
Iteration 155/1000 | Loss: 0.00001684
Iteration 156/1000 | Loss: 0.00001683
Iteration 157/1000 | Loss: 0.00001683
Iteration 158/1000 | Loss: 0.00001683
Iteration 159/1000 | Loss: 0.00001683
Iteration 160/1000 | Loss: 0.00001683
Iteration 161/1000 | Loss: 0.00001683
Iteration 162/1000 | Loss: 0.00001683
Iteration 163/1000 | Loss: 0.00001683
Iteration 164/1000 | Loss: 0.00001682
Iteration 165/1000 | Loss: 0.00001682
Iteration 166/1000 | Loss: 0.00001682
Iteration 167/1000 | Loss: 0.00001682
Iteration 168/1000 | Loss: 0.00001682
Iteration 169/1000 | Loss: 0.00001682
Iteration 170/1000 | Loss: 0.00001682
Iteration 171/1000 | Loss: 0.00001682
Iteration 172/1000 | Loss: 0.00001682
Iteration 173/1000 | Loss: 0.00001681
Iteration 174/1000 | Loss: 0.00001681
Iteration 175/1000 | Loss: 0.00001681
Iteration 176/1000 | Loss: 0.00001680
Iteration 177/1000 | Loss: 0.00001680
Iteration 178/1000 | Loss: 0.00001680
Iteration 179/1000 | Loss: 0.00001679
Iteration 180/1000 | Loss: 0.00001679
Iteration 181/1000 | Loss: 0.00001679
Iteration 182/1000 | Loss: 0.00001679
Iteration 183/1000 | Loss: 0.00001679
Iteration 184/1000 | Loss: 0.00001679
Iteration 185/1000 | Loss: 0.00001678
Iteration 186/1000 | Loss: 0.00001678
Iteration 187/1000 | Loss: 0.00001678
Iteration 188/1000 | Loss: 0.00001678
Iteration 189/1000 | Loss: 0.00001678
Iteration 190/1000 | Loss: 0.00001678
Iteration 191/1000 | Loss: 0.00001678
Iteration 192/1000 | Loss: 0.00001678
Iteration 193/1000 | Loss: 0.00001678
Iteration 194/1000 | Loss: 0.00001677
Iteration 195/1000 | Loss: 0.00001677
Iteration 196/1000 | Loss: 0.00001677
Iteration 197/1000 | Loss: 0.00001677
Iteration 198/1000 | Loss: 0.00001676
Iteration 199/1000 | Loss: 0.00001676
Iteration 200/1000 | Loss: 0.00001676
Iteration 201/1000 | Loss: 0.00001675
Iteration 202/1000 | Loss: 0.00001675
Iteration 203/1000 | Loss: 0.00001675
Iteration 204/1000 | Loss: 0.00001675
Iteration 205/1000 | Loss: 0.00001675
Iteration 206/1000 | Loss: 0.00001675
Iteration 207/1000 | Loss: 0.00001675
Iteration 208/1000 | Loss: 0.00001675
Iteration 209/1000 | Loss: 0.00001675
Iteration 210/1000 | Loss: 0.00001674
Iteration 211/1000 | Loss: 0.00001674
Iteration 212/1000 | Loss: 0.00001674
Iteration 213/1000 | Loss: 0.00001674
Iteration 214/1000 | Loss: 0.00001674
Iteration 215/1000 | Loss: 0.00001674
Iteration 216/1000 | Loss: 0.00001674
Iteration 217/1000 | Loss: 0.00001674
Iteration 218/1000 | Loss: 0.00001673
Iteration 219/1000 | Loss: 0.00001673
Iteration 220/1000 | Loss: 0.00001673
Iteration 221/1000 | Loss: 0.00001673
Iteration 222/1000 | Loss: 0.00001673
Iteration 223/1000 | Loss: 0.00001673
Iteration 224/1000 | Loss: 0.00001672
Iteration 225/1000 | Loss: 0.00001672
Iteration 226/1000 | Loss: 0.00001672
Iteration 227/1000 | Loss: 0.00001672
Iteration 228/1000 | Loss: 0.00001672
Iteration 229/1000 | Loss: 0.00001672
Iteration 230/1000 | Loss: 0.00001672
Iteration 231/1000 | Loss: 0.00001672
Iteration 232/1000 | Loss: 0.00001672
Iteration 233/1000 | Loss: 0.00001672
Iteration 234/1000 | Loss: 0.00001671
Iteration 235/1000 | Loss: 0.00001671
Iteration 236/1000 | Loss: 0.00001670
Iteration 237/1000 | Loss: 0.00001670
Iteration 238/1000 | Loss: 0.00001670
Iteration 239/1000 | Loss: 0.00001670
Iteration 240/1000 | Loss: 0.00001670
Iteration 241/1000 | Loss: 0.00001670
Iteration 242/1000 | Loss: 0.00001670
Iteration 243/1000 | Loss: 0.00001670
Iteration 244/1000 | Loss: 0.00001669
Iteration 245/1000 | Loss: 0.00001669
Iteration 246/1000 | Loss: 0.00001669
Iteration 247/1000 | Loss: 0.00001669
Iteration 248/1000 | Loss: 0.00001669
Iteration 249/1000 | Loss: 0.00001669
Iteration 250/1000 | Loss: 0.00001669
Iteration 251/1000 | Loss: 0.00001669
Iteration 252/1000 | Loss: 0.00001669
Iteration 253/1000 | Loss: 0.00001669
Iteration 254/1000 | Loss: 0.00001669
Iteration 255/1000 | Loss: 0.00001669
Iteration 256/1000 | Loss: 0.00001668
Iteration 257/1000 | Loss: 0.00001668
Iteration 258/1000 | Loss: 0.00001668
Iteration 259/1000 | Loss: 0.00001668
Iteration 260/1000 | Loss: 0.00001668
Iteration 261/1000 | Loss: 0.00001668
Iteration 262/1000 | Loss: 0.00001668
Iteration 263/1000 | Loss: 0.00001668
Iteration 264/1000 | Loss: 0.00001668
Iteration 265/1000 | Loss: 0.00001668
Iteration 266/1000 | Loss: 0.00001668
Iteration 267/1000 | Loss: 0.00001668
Iteration 268/1000 | Loss: 0.00001668
Iteration 269/1000 | Loss: 0.00001667
Iteration 270/1000 | Loss: 0.00001667
Iteration 271/1000 | Loss: 0.00001667
Iteration 272/1000 | Loss: 0.00001667
Iteration 273/1000 | Loss: 0.00001667
Iteration 274/1000 | Loss: 0.00001667
Iteration 275/1000 | Loss: 0.00001667
Iteration 276/1000 | Loss: 0.00001667
Iteration 277/1000 | Loss: 0.00001667
Iteration 278/1000 | Loss: 0.00001667
Iteration 279/1000 | Loss: 0.00001666
Iteration 280/1000 | Loss: 0.00001666
Iteration 281/1000 | Loss: 0.00001666
Iteration 282/1000 | Loss: 0.00001666
Iteration 283/1000 | Loss: 0.00001666
Iteration 284/1000 | Loss: 0.00001666
Iteration 285/1000 | Loss: 0.00001666
Iteration 286/1000 | Loss: 0.00001666
Iteration 287/1000 | Loss: 0.00001666
Iteration 288/1000 | Loss: 0.00001666
Iteration 289/1000 | Loss: 0.00001666
Iteration 290/1000 | Loss: 0.00001666
Iteration 291/1000 | Loss: 0.00001666
Iteration 292/1000 | Loss: 0.00001666
Iteration 293/1000 | Loss: 0.00001665
Iteration 294/1000 | Loss: 0.00001665
Iteration 295/1000 | Loss: 0.00001665
Iteration 296/1000 | Loss: 0.00001665
Iteration 297/1000 | Loss: 0.00001665
Iteration 298/1000 | Loss: 0.00001665
Iteration 299/1000 | Loss: 0.00001665
Iteration 300/1000 | Loss: 0.00001665
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 300. Stopping optimization.
Last 5 losses: [1.6654335922794417e-05, 1.6654335922794417e-05, 1.6654335922794417e-05, 1.6654335922794417e-05, 1.6654335922794417e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6654335922794417e-05

Optimization complete. Final v2v error: 3.382152557373047 mm

Highest mean error: 3.940249443054199 mm for frame 67

Lowest mean error: 3.129788637161255 mm for frame 200

Saving results

Total time: 260.6989109516144
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00479952
Iteration 2/25 | Loss: 0.00089414
Iteration 3/25 | Loss: 0.00078431
Iteration 4/25 | Loss: 0.00075754
Iteration 5/25 | Loss: 0.00074677
Iteration 6/25 | Loss: 0.00074455
Iteration 7/25 | Loss: 0.00074390
Iteration 8/25 | Loss: 0.00074381
Iteration 9/25 | Loss: 0.00074381
Iteration 10/25 | Loss: 0.00074381
Iteration 11/25 | Loss: 0.00074381
Iteration 12/25 | Loss: 0.00074381
Iteration 13/25 | Loss: 0.00074381
Iteration 14/25 | Loss: 0.00074381
Iteration 15/25 | Loss: 0.00074381
Iteration 16/25 | Loss: 0.00074381
Iteration 17/25 | Loss: 0.00074381
Iteration 18/25 | Loss: 0.00074381
Iteration 19/25 | Loss: 0.00074381
Iteration 20/25 | Loss: 0.00074381
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007438061875291169, 0.0007438061875291169, 0.0007438061875291169, 0.0007438061875291169, 0.0007438061875291169]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007438061875291169

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.95088768
Iteration 2/25 | Loss: 0.00049739
Iteration 3/25 | Loss: 0.00049738
Iteration 4/25 | Loss: 0.00049738
Iteration 5/25 | Loss: 0.00049738
Iteration 6/25 | Loss: 0.00049738
Iteration 7/25 | Loss: 0.00049738
Iteration 8/25 | Loss: 0.00049738
Iteration 9/25 | Loss: 0.00049738
Iteration 10/25 | Loss: 0.00049738
Iteration 11/25 | Loss: 0.00049738
Iteration 12/25 | Loss: 0.00049738
Iteration 13/25 | Loss: 0.00049738
Iteration 14/25 | Loss: 0.00049738
Iteration 15/25 | Loss: 0.00049738
Iteration 16/25 | Loss: 0.00049738
Iteration 17/25 | Loss: 0.00049738
Iteration 18/25 | Loss: 0.00049738
Iteration 19/25 | Loss: 0.00049738
Iteration 20/25 | Loss: 0.00049738
Iteration 21/25 | Loss: 0.00049738
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0004973803297616541, 0.0004973803297616541, 0.0004973803297616541, 0.0004973803297616541, 0.0004973803297616541]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004973803297616541

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049738
Iteration 2/1000 | Loss: 0.00003395
Iteration 3/1000 | Loss: 0.00001938
Iteration 4/1000 | Loss: 0.00001723
Iteration 5/1000 | Loss: 0.00001597
Iteration 6/1000 | Loss: 0.00001548
Iteration 7/1000 | Loss: 0.00001506
Iteration 8/1000 | Loss: 0.00001474
Iteration 9/1000 | Loss: 0.00001456
Iteration 10/1000 | Loss: 0.00001453
Iteration 11/1000 | Loss: 0.00001452
Iteration 12/1000 | Loss: 0.00001439
Iteration 13/1000 | Loss: 0.00001432
Iteration 14/1000 | Loss: 0.00001422
Iteration 15/1000 | Loss: 0.00001422
Iteration 16/1000 | Loss: 0.00001421
Iteration 17/1000 | Loss: 0.00001420
Iteration 18/1000 | Loss: 0.00001420
Iteration 19/1000 | Loss: 0.00001418
Iteration 20/1000 | Loss: 0.00001417
Iteration 21/1000 | Loss: 0.00001417
Iteration 22/1000 | Loss: 0.00001417
Iteration 23/1000 | Loss: 0.00001417
Iteration 24/1000 | Loss: 0.00001417
Iteration 25/1000 | Loss: 0.00001417
Iteration 26/1000 | Loss: 0.00001416
Iteration 27/1000 | Loss: 0.00001416
Iteration 28/1000 | Loss: 0.00001416
Iteration 29/1000 | Loss: 0.00001416
Iteration 30/1000 | Loss: 0.00001415
Iteration 31/1000 | Loss: 0.00001415
Iteration 32/1000 | Loss: 0.00001415
Iteration 33/1000 | Loss: 0.00001414
Iteration 34/1000 | Loss: 0.00001413
Iteration 35/1000 | Loss: 0.00001413
Iteration 36/1000 | Loss: 0.00001413
Iteration 37/1000 | Loss: 0.00001413
Iteration 38/1000 | Loss: 0.00001412
Iteration 39/1000 | Loss: 0.00001412
Iteration 40/1000 | Loss: 0.00001411
Iteration 41/1000 | Loss: 0.00001411
Iteration 42/1000 | Loss: 0.00001411
Iteration 43/1000 | Loss: 0.00001410
Iteration 44/1000 | Loss: 0.00001410
Iteration 45/1000 | Loss: 0.00001410
Iteration 46/1000 | Loss: 0.00001410
Iteration 47/1000 | Loss: 0.00001409
Iteration 48/1000 | Loss: 0.00001409
Iteration 49/1000 | Loss: 0.00001409
Iteration 50/1000 | Loss: 0.00001408
Iteration 51/1000 | Loss: 0.00001408
Iteration 52/1000 | Loss: 0.00001408
Iteration 53/1000 | Loss: 0.00001407
Iteration 54/1000 | Loss: 0.00001407
Iteration 55/1000 | Loss: 0.00001407
Iteration 56/1000 | Loss: 0.00001406
Iteration 57/1000 | Loss: 0.00001406
Iteration 58/1000 | Loss: 0.00001406
Iteration 59/1000 | Loss: 0.00001406
Iteration 60/1000 | Loss: 0.00001406
Iteration 61/1000 | Loss: 0.00001405
Iteration 62/1000 | Loss: 0.00001403
Iteration 63/1000 | Loss: 0.00001403
Iteration 64/1000 | Loss: 0.00001402
Iteration 65/1000 | Loss: 0.00001402
Iteration 66/1000 | Loss: 0.00001402
Iteration 67/1000 | Loss: 0.00001401
Iteration 68/1000 | Loss: 0.00001401
Iteration 69/1000 | Loss: 0.00001400
Iteration 70/1000 | Loss: 0.00001399
Iteration 71/1000 | Loss: 0.00001399
Iteration 72/1000 | Loss: 0.00001399
Iteration 73/1000 | Loss: 0.00001398
Iteration 74/1000 | Loss: 0.00001398
Iteration 75/1000 | Loss: 0.00001398
Iteration 76/1000 | Loss: 0.00001398
Iteration 77/1000 | Loss: 0.00001397
Iteration 78/1000 | Loss: 0.00001397
Iteration 79/1000 | Loss: 0.00001397
Iteration 80/1000 | Loss: 0.00001397
Iteration 81/1000 | Loss: 0.00001397
Iteration 82/1000 | Loss: 0.00001397
Iteration 83/1000 | Loss: 0.00001396
Iteration 84/1000 | Loss: 0.00001394
Iteration 85/1000 | Loss: 0.00001394
Iteration 86/1000 | Loss: 0.00001394
Iteration 87/1000 | Loss: 0.00001394
Iteration 88/1000 | Loss: 0.00001394
Iteration 89/1000 | Loss: 0.00001394
Iteration 90/1000 | Loss: 0.00001394
Iteration 91/1000 | Loss: 0.00001394
Iteration 92/1000 | Loss: 0.00001394
Iteration 93/1000 | Loss: 0.00001393
Iteration 94/1000 | Loss: 0.00001393
Iteration 95/1000 | Loss: 0.00001393
Iteration 96/1000 | Loss: 0.00001392
Iteration 97/1000 | Loss: 0.00001391
Iteration 98/1000 | Loss: 0.00001391
Iteration 99/1000 | Loss: 0.00001391
Iteration 100/1000 | Loss: 0.00001391
Iteration 101/1000 | Loss: 0.00001391
Iteration 102/1000 | Loss: 0.00001391
Iteration 103/1000 | Loss: 0.00001391
Iteration 104/1000 | Loss: 0.00001391
Iteration 105/1000 | Loss: 0.00001391
Iteration 106/1000 | Loss: 0.00001391
Iteration 107/1000 | Loss: 0.00001391
Iteration 108/1000 | Loss: 0.00001391
Iteration 109/1000 | Loss: 0.00001391
Iteration 110/1000 | Loss: 0.00001391
Iteration 111/1000 | Loss: 0.00001391
Iteration 112/1000 | Loss: 0.00001391
Iteration 113/1000 | Loss: 0.00001391
Iteration 114/1000 | Loss: 0.00001391
Iteration 115/1000 | Loss: 0.00001391
Iteration 116/1000 | Loss: 0.00001391
Iteration 117/1000 | Loss: 0.00001391
Iteration 118/1000 | Loss: 0.00001391
Iteration 119/1000 | Loss: 0.00001391
Iteration 120/1000 | Loss: 0.00001391
Iteration 121/1000 | Loss: 0.00001391
Iteration 122/1000 | Loss: 0.00001391
Iteration 123/1000 | Loss: 0.00001391
Iteration 124/1000 | Loss: 0.00001391
Iteration 125/1000 | Loss: 0.00001391
Iteration 126/1000 | Loss: 0.00001391
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.3905603736930061e-05, 1.3905603736930061e-05, 1.3905603736930061e-05, 1.3905603736930061e-05, 1.3905603736930061e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3905603736930061e-05

Optimization complete. Final v2v error: 3.17100191116333 mm

Highest mean error: 3.5872459411621094 mm for frame 116

Lowest mean error: 2.9483630657196045 mm for frame 62

Saving results

Total time: 36.050891637802124
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00843754
Iteration 2/25 | Loss: 0.00118615
Iteration 3/25 | Loss: 0.00092285
Iteration 4/25 | Loss: 0.00082814
Iteration 5/25 | Loss: 0.00081242
Iteration 6/25 | Loss: 0.00080569
Iteration 7/25 | Loss: 0.00081300
Iteration 8/25 | Loss: 0.00080385
Iteration 9/25 | Loss: 0.00080373
Iteration 10/25 | Loss: 0.00080372
Iteration 11/25 | Loss: 0.00080368
Iteration 12/25 | Loss: 0.00080368
Iteration 13/25 | Loss: 0.00080368
Iteration 14/25 | Loss: 0.00080368
Iteration 15/25 | Loss: 0.00080368
Iteration 16/25 | Loss: 0.00080368
Iteration 17/25 | Loss: 0.00080368
Iteration 18/25 | Loss: 0.00080368
Iteration 19/25 | Loss: 0.00080367
Iteration 20/25 | Loss: 0.00080367
Iteration 21/25 | Loss: 0.00080367
Iteration 22/25 | Loss: 0.00080367
Iteration 23/25 | Loss: 0.00080367
Iteration 24/25 | Loss: 0.00080367
Iteration 25/25 | Loss: 0.00080367

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.07803917
Iteration 2/25 | Loss: 0.00053016
Iteration 3/25 | Loss: 0.00053011
Iteration 4/25 | Loss: 0.00053011
Iteration 5/25 | Loss: 0.00053011
Iteration 6/25 | Loss: 0.00053011
Iteration 7/25 | Loss: 0.00053011
Iteration 8/25 | Loss: 0.00053011
Iteration 9/25 | Loss: 0.00053011
Iteration 10/25 | Loss: 0.00053011
Iteration 11/25 | Loss: 0.00053011
Iteration 12/25 | Loss: 0.00053011
Iteration 13/25 | Loss: 0.00053011
Iteration 14/25 | Loss: 0.00053011
Iteration 15/25 | Loss: 0.00053011
Iteration 16/25 | Loss: 0.00053011
Iteration 17/25 | Loss: 0.00053011
Iteration 18/25 | Loss: 0.00053011
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005301081109791994, 0.0005301081109791994, 0.0005301081109791994, 0.0005301081109791994, 0.0005301081109791994]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005301081109791994

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053011
Iteration 2/1000 | Loss: 0.00003649
Iteration 3/1000 | Loss: 0.00002569
Iteration 4/1000 | Loss: 0.00002160
Iteration 5/1000 | Loss: 0.00002051
Iteration 6/1000 | Loss: 0.00001982
Iteration 7/1000 | Loss: 0.00001939
Iteration 8/1000 | Loss: 0.00001908
Iteration 9/1000 | Loss: 0.00001882
Iteration 10/1000 | Loss: 0.00001866
Iteration 11/1000 | Loss: 0.00001850
Iteration 12/1000 | Loss: 0.00001833
Iteration 13/1000 | Loss: 0.00001828
Iteration 14/1000 | Loss: 0.00001825
Iteration 15/1000 | Loss: 0.00001825
Iteration 16/1000 | Loss: 0.00001824
Iteration 17/1000 | Loss: 0.00001822
Iteration 18/1000 | Loss: 0.00001821
Iteration 19/1000 | Loss: 0.00001819
Iteration 20/1000 | Loss: 0.00001818
Iteration 21/1000 | Loss: 0.00001817
Iteration 22/1000 | Loss: 0.00001812
Iteration 23/1000 | Loss: 0.00001811
Iteration 24/1000 | Loss: 0.00001808
Iteration 25/1000 | Loss: 0.00001807
Iteration 26/1000 | Loss: 0.00001807
Iteration 27/1000 | Loss: 0.00001806
Iteration 28/1000 | Loss: 0.00001806
Iteration 29/1000 | Loss: 0.00001805
Iteration 30/1000 | Loss: 0.00001804
Iteration 31/1000 | Loss: 0.00001804
Iteration 32/1000 | Loss: 0.00001803
Iteration 33/1000 | Loss: 0.00001802
Iteration 34/1000 | Loss: 0.00001802
Iteration 35/1000 | Loss: 0.00001802
Iteration 36/1000 | Loss: 0.00001801
Iteration 37/1000 | Loss: 0.00001801
Iteration 38/1000 | Loss: 0.00001801
Iteration 39/1000 | Loss: 0.00001801
Iteration 40/1000 | Loss: 0.00001800
Iteration 41/1000 | Loss: 0.00001800
Iteration 42/1000 | Loss: 0.00001798
Iteration 43/1000 | Loss: 0.00001798
Iteration 44/1000 | Loss: 0.00001797
Iteration 45/1000 | Loss: 0.00001797
Iteration 46/1000 | Loss: 0.00001795
Iteration 47/1000 | Loss: 0.00001795
Iteration 48/1000 | Loss: 0.00001794
Iteration 49/1000 | Loss: 0.00001794
Iteration 50/1000 | Loss: 0.00001793
Iteration 51/1000 | Loss: 0.00001793
Iteration 52/1000 | Loss: 0.00001793
Iteration 53/1000 | Loss: 0.00001793
Iteration 54/1000 | Loss: 0.00001793
Iteration 55/1000 | Loss: 0.00001793
Iteration 56/1000 | Loss: 0.00001793
Iteration 57/1000 | Loss: 0.00001793
Iteration 58/1000 | Loss: 0.00001793
Iteration 59/1000 | Loss: 0.00001793
Iteration 60/1000 | Loss: 0.00001793
Iteration 61/1000 | Loss: 0.00001792
Iteration 62/1000 | Loss: 0.00001792
Iteration 63/1000 | Loss: 0.00001792
Iteration 64/1000 | Loss: 0.00001792
Iteration 65/1000 | Loss: 0.00001791
Iteration 66/1000 | Loss: 0.00001791
Iteration 67/1000 | Loss: 0.00001790
Iteration 68/1000 | Loss: 0.00001790
Iteration 69/1000 | Loss: 0.00001790
Iteration 70/1000 | Loss: 0.00001790
Iteration 71/1000 | Loss: 0.00001790
Iteration 72/1000 | Loss: 0.00001789
Iteration 73/1000 | Loss: 0.00001789
Iteration 74/1000 | Loss: 0.00001789
Iteration 75/1000 | Loss: 0.00001789
Iteration 76/1000 | Loss: 0.00001788
Iteration 77/1000 | Loss: 0.00001788
Iteration 78/1000 | Loss: 0.00001788
Iteration 79/1000 | Loss: 0.00001788
Iteration 80/1000 | Loss: 0.00001787
Iteration 81/1000 | Loss: 0.00001787
Iteration 82/1000 | Loss: 0.00001787
Iteration 83/1000 | Loss: 0.00001786
Iteration 84/1000 | Loss: 0.00001786
Iteration 85/1000 | Loss: 0.00001786
Iteration 86/1000 | Loss: 0.00001785
Iteration 87/1000 | Loss: 0.00001785
Iteration 88/1000 | Loss: 0.00001785
Iteration 89/1000 | Loss: 0.00001785
Iteration 90/1000 | Loss: 0.00001784
Iteration 91/1000 | Loss: 0.00001784
Iteration 92/1000 | Loss: 0.00001784
Iteration 93/1000 | Loss: 0.00001783
Iteration 94/1000 | Loss: 0.00001783
Iteration 95/1000 | Loss: 0.00001783
Iteration 96/1000 | Loss: 0.00001783
Iteration 97/1000 | Loss: 0.00001783
Iteration 98/1000 | Loss: 0.00001783
Iteration 99/1000 | Loss: 0.00001783
Iteration 100/1000 | Loss: 0.00001782
Iteration 101/1000 | Loss: 0.00001782
Iteration 102/1000 | Loss: 0.00001782
Iteration 103/1000 | Loss: 0.00001782
Iteration 104/1000 | Loss: 0.00001782
Iteration 105/1000 | Loss: 0.00001782
Iteration 106/1000 | Loss: 0.00001782
Iteration 107/1000 | Loss: 0.00001782
Iteration 108/1000 | Loss: 0.00001782
Iteration 109/1000 | Loss: 0.00001782
Iteration 110/1000 | Loss: 0.00001782
Iteration 111/1000 | Loss: 0.00001782
Iteration 112/1000 | Loss: 0.00001782
Iteration 113/1000 | Loss: 0.00001782
Iteration 114/1000 | Loss: 0.00001781
Iteration 115/1000 | Loss: 0.00001781
Iteration 116/1000 | Loss: 0.00001781
Iteration 117/1000 | Loss: 0.00001781
Iteration 118/1000 | Loss: 0.00001781
Iteration 119/1000 | Loss: 0.00001781
Iteration 120/1000 | Loss: 0.00001781
Iteration 121/1000 | Loss: 0.00001781
Iteration 122/1000 | Loss: 0.00001781
Iteration 123/1000 | Loss: 0.00001781
Iteration 124/1000 | Loss: 0.00001781
Iteration 125/1000 | Loss: 0.00001781
Iteration 126/1000 | Loss: 0.00001780
Iteration 127/1000 | Loss: 0.00001780
Iteration 128/1000 | Loss: 0.00001780
Iteration 129/1000 | Loss: 0.00001780
Iteration 130/1000 | Loss: 0.00001780
Iteration 131/1000 | Loss: 0.00001780
Iteration 132/1000 | Loss: 0.00001780
Iteration 133/1000 | Loss: 0.00001780
Iteration 134/1000 | Loss: 0.00001780
Iteration 135/1000 | Loss: 0.00001780
Iteration 136/1000 | Loss: 0.00001780
Iteration 137/1000 | Loss: 0.00001780
Iteration 138/1000 | Loss: 0.00001780
Iteration 139/1000 | Loss: 0.00001780
Iteration 140/1000 | Loss: 0.00001779
Iteration 141/1000 | Loss: 0.00001779
Iteration 142/1000 | Loss: 0.00001779
Iteration 143/1000 | Loss: 0.00001779
Iteration 144/1000 | Loss: 0.00001779
Iteration 145/1000 | Loss: 0.00001779
Iteration 146/1000 | Loss: 0.00001779
Iteration 147/1000 | Loss: 0.00001779
Iteration 148/1000 | Loss: 0.00001779
Iteration 149/1000 | Loss: 0.00001779
Iteration 150/1000 | Loss: 0.00001779
Iteration 151/1000 | Loss: 0.00001779
Iteration 152/1000 | Loss: 0.00001779
Iteration 153/1000 | Loss: 0.00001779
Iteration 154/1000 | Loss: 0.00001779
Iteration 155/1000 | Loss: 0.00001779
Iteration 156/1000 | Loss: 0.00001779
Iteration 157/1000 | Loss: 0.00001779
Iteration 158/1000 | Loss: 0.00001779
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [1.778799924068153e-05, 1.778799924068153e-05, 1.778799924068153e-05, 1.778799924068153e-05, 1.778799924068153e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.778799924068153e-05

Optimization complete. Final v2v error: 3.5172784328460693 mm

Highest mean error: 4.25595235824585 mm for frame 57

Lowest mean error: 2.9518790245056152 mm for frame 112

Saving results

Total time: 45.378395795822144
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00464522
Iteration 2/25 | Loss: 0.00114523
Iteration 3/25 | Loss: 0.00081413
Iteration 4/25 | Loss: 0.00075378
Iteration 5/25 | Loss: 0.00074042
Iteration 6/25 | Loss: 0.00073645
Iteration 7/25 | Loss: 0.00073522
Iteration 8/25 | Loss: 0.00073505
Iteration 9/25 | Loss: 0.00073505
Iteration 10/25 | Loss: 0.00073505
Iteration 11/25 | Loss: 0.00073505
Iteration 12/25 | Loss: 0.00073505
Iteration 13/25 | Loss: 0.00073505
Iteration 14/25 | Loss: 0.00073505
Iteration 15/25 | Loss: 0.00073505
Iteration 16/25 | Loss: 0.00073505
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007350458181463182, 0.0007350458181463182, 0.0007350458181463182, 0.0007350458181463182, 0.0007350458181463182]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007350458181463182

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50083876
Iteration 2/25 | Loss: 0.00044077
Iteration 3/25 | Loss: 0.00044076
Iteration 4/25 | Loss: 0.00044076
Iteration 5/25 | Loss: 0.00044076
Iteration 6/25 | Loss: 0.00044076
Iteration 7/25 | Loss: 0.00044076
Iteration 8/25 | Loss: 0.00044076
Iteration 9/25 | Loss: 0.00044076
Iteration 10/25 | Loss: 0.00044076
Iteration 11/25 | Loss: 0.00044076
Iteration 12/25 | Loss: 0.00044076
Iteration 13/25 | Loss: 0.00044076
Iteration 14/25 | Loss: 0.00044076
Iteration 15/25 | Loss: 0.00044076
Iteration 16/25 | Loss: 0.00044076
Iteration 17/25 | Loss: 0.00044076
Iteration 18/25 | Loss: 0.00044076
Iteration 19/25 | Loss: 0.00044076
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0004407597880344838, 0.0004407597880344838, 0.0004407597880344838, 0.0004407597880344838, 0.0004407597880344838]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004407597880344838

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044076
Iteration 2/1000 | Loss: 0.00002085
Iteration 3/1000 | Loss: 0.00001433
Iteration 4/1000 | Loss: 0.00001308
Iteration 5/1000 | Loss: 0.00001237
Iteration 6/1000 | Loss: 0.00001186
Iteration 7/1000 | Loss: 0.00001152
Iteration 8/1000 | Loss: 0.00001130
Iteration 9/1000 | Loss: 0.00001125
Iteration 10/1000 | Loss: 0.00001124
Iteration 11/1000 | Loss: 0.00001124
Iteration 12/1000 | Loss: 0.00001120
Iteration 13/1000 | Loss: 0.00001120
Iteration 14/1000 | Loss: 0.00001117
Iteration 15/1000 | Loss: 0.00001113
Iteration 16/1000 | Loss: 0.00001113
Iteration 17/1000 | Loss: 0.00001112
Iteration 18/1000 | Loss: 0.00001107
Iteration 19/1000 | Loss: 0.00001103
Iteration 20/1000 | Loss: 0.00001102
Iteration 21/1000 | Loss: 0.00001102
Iteration 22/1000 | Loss: 0.00001100
Iteration 23/1000 | Loss: 0.00001099
Iteration 24/1000 | Loss: 0.00001099
Iteration 25/1000 | Loss: 0.00001098
Iteration 26/1000 | Loss: 0.00001098
Iteration 27/1000 | Loss: 0.00001097
Iteration 28/1000 | Loss: 0.00001096
Iteration 29/1000 | Loss: 0.00001095
Iteration 30/1000 | Loss: 0.00001095
Iteration 31/1000 | Loss: 0.00001094
Iteration 32/1000 | Loss: 0.00001094
Iteration 33/1000 | Loss: 0.00001094
Iteration 34/1000 | Loss: 0.00001093
Iteration 35/1000 | Loss: 0.00001090
Iteration 36/1000 | Loss: 0.00001090
Iteration 37/1000 | Loss: 0.00001090
Iteration 38/1000 | Loss: 0.00001090
Iteration 39/1000 | Loss: 0.00001090
Iteration 40/1000 | Loss: 0.00001090
Iteration 41/1000 | Loss: 0.00001090
Iteration 42/1000 | Loss: 0.00001090
Iteration 43/1000 | Loss: 0.00001089
Iteration 44/1000 | Loss: 0.00001089
Iteration 45/1000 | Loss: 0.00001088
Iteration 46/1000 | Loss: 0.00001088
Iteration 47/1000 | Loss: 0.00001088
Iteration 48/1000 | Loss: 0.00001087
Iteration 49/1000 | Loss: 0.00001087
Iteration 50/1000 | Loss: 0.00001087
Iteration 51/1000 | Loss: 0.00001086
Iteration 52/1000 | Loss: 0.00001086
Iteration 53/1000 | Loss: 0.00001086
Iteration 54/1000 | Loss: 0.00001086
Iteration 55/1000 | Loss: 0.00001085
Iteration 56/1000 | Loss: 0.00001085
Iteration 57/1000 | Loss: 0.00001085
Iteration 58/1000 | Loss: 0.00001085
Iteration 59/1000 | Loss: 0.00001085
Iteration 60/1000 | Loss: 0.00001085
Iteration 61/1000 | Loss: 0.00001085
Iteration 62/1000 | Loss: 0.00001085
Iteration 63/1000 | Loss: 0.00001085
Iteration 64/1000 | Loss: 0.00001085
Iteration 65/1000 | Loss: 0.00001084
Iteration 66/1000 | Loss: 0.00001084
Iteration 67/1000 | Loss: 0.00001084
Iteration 68/1000 | Loss: 0.00001084
Iteration 69/1000 | Loss: 0.00001084
Iteration 70/1000 | Loss: 0.00001084
Iteration 71/1000 | Loss: 0.00001084
Iteration 72/1000 | Loss: 0.00001084
Iteration 73/1000 | Loss: 0.00001084
Iteration 74/1000 | Loss: 0.00001083
Iteration 75/1000 | Loss: 0.00001083
Iteration 76/1000 | Loss: 0.00001083
Iteration 77/1000 | Loss: 0.00001083
Iteration 78/1000 | Loss: 0.00001083
Iteration 79/1000 | Loss: 0.00001083
Iteration 80/1000 | Loss: 0.00001083
Iteration 81/1000 | Loss: 0.00001083
Iteration 82/1000 | Loss: 0.00001083
Iteration 83/1000 | Loss: 0.00001083
Iteration 84/1000 | Loss: 0.00001083
Iteration 85/1000 | Loss: 0.00001082
Iteration 86/1000 | Loss: 0.00001082
Iteration 87/1000 | Loss: 0.00001082
Iteration 88/1000 | Loss: 0.00001082
Iteration 89/1000 | Loss: 0.00001082
Iteration 90/1000 | Loss: 0.00001082
Iteration 91/1000 | Loss: 0.00001082
Iteration 92/1000 | Loss: 0.00001082
Iteration 93/1000 | Loss: 0.00001082
Iteration 94/1000 | Loss: 0.00001082
Iteration 95/1000 | Loss: 0.00001082
Iteration 96/1000 | Loss: 0.00001082
Iteration 97/1000 | Loss: 0.00001082
Iteration 98/1000 | Loss: 0.00001082
Iteration 99/1000 | Loss: 0.00001082
Iteration 100/1000 | Loss: 0.00001082
Iteration 101/1000 | Loss: 0.00001081
Iteration 102/1000 | Loss: 0.00001081
Iteration 103/1000 | Loss: 0.00001081
Iteration 104/1000 | Loss: 0.00001081
Iteration 105/1000 | Loss: 0.00001081
Iteration 106/1000 | Loss: 0.00001081
Iteration 107/1000 | Loss: 0.00001081
Iteration 108/1000 | Loss: 0.00001080
Iteration 109/1000 | Loss: 0.00001080
Iteration 110/1000 | Loss: 0.00001080
Iteration 111/1000 | Loss: 0.00001080
Iteration 112/1000 | Loss: 0.00001080
Iteration 113/1000 | Loss: 0.00001080
Iteration 114/1000 | Loss: 0.00001080
Iteration 115/1000 | Loss: 0.00001080
Iteration 116/1000 | Loss: 0.00001080
Iteration 117/1000 | Loss: 0.00001080
Iteration 118/1000 | Loss: 0.00001080
Iteration 119/1000 | Loss: 0.00001080
Iteration 120/1000 | Loss: 0.00001079
Iteration 121/1000 | Loss: 0.00001079
Iteration 122/1000 | Loss: 0.00001079
Iteration 123/1000 | Loss: 0.00001079
Iteration 124/1000 | Loss: 0.00001079
Iteration 125/1000 | Loss: 0.00001079
Iteration 126/1000 | Loss: 0.00001079
Iteration 127/1000 | Loss: 0.00001079
Iteration 128/1000 | Loss: 0.00001078
Iteration 129/1000 | Loss: 0.00001078
Iteration 130/1000 | Loss: 0.00001078
Iteration 131/1000 | Loss: 0.00001078
Iteration 132/1000 | Loss: 0.00001078
Iteration 133/1000 | Loss: 0.00001078
Iteration 134/1000 | Loss: 0.00001078
Iteration 135/1000 | Loss: 0.00001078
Iteration 136/1000 | Loss: 0.00001078
Iteration 137/1000 | Loss: 0.00001078
Iteration 138/1000 | Loss: 0.00001078
Iteration 139/1000 | Loss: 0.00001077
Iteration 140/1000 | Loss: 0.00001077
Iteration 141/1000 | Loss: 0.00001077
Iteration 142/1000 | Loss: 0.00001077
Iteration 143/1000 | Loss: 0.00001077
Iteration 144/1000 | Loss: 0.00001077
Iteration 145/1000 | Loss: 0.00001076
Iteration 146/1000 | Loss: 0.00001076
Iteration 147/1000 | Loss: 0.00001076
Iteration 148/1000 | Loss: 0.00001076
Iteration 149/1000 | Loss: 0.00001076
Iteration 150/1000 | Loss: 0.00001076
Iteration 151/1000 | Loss: 0.00001076
Iteration 152/1000 | Loss: 0.00001075
Iteration 153/1000 | Loss: 0.00001075
Iteration 154/1000 | Loss: 0.00001075
Iteration 155/1000 | Loss: 0.00001075
Iteration 156/1000 | Loss: 0.00001075
Iteration 157/1000 | Loss: 0.00001074
Iteration 158/1000 | Loss: 0.00001074
Iteration 159/1000 | Loss: 0.00001074
Iteration 160/1000 | Loss: 0.00001074
Iteration 161/1000 | Loss: 0.00001074
Iteration 162/1000 | Loss: 0.00001074
Iteration 163/1000 | Loss: 0.00001074
Iteration 164/1000 | Loss: 0.00001074
Iteration 165/1000 | Loss: 0.00001074
Iteration 166/1000 | Loss: 0.00001074
Iteration 167/1000 | Loss: 0.00001073
Iteration 168/1000 | Loss: 0.00001073
Iteration 169/1000 | Loss: 0.00001073
Iteration 170/1000 | Loss: 0.00001073
Iteration 171/1000 | Loss: 0.00001073
Iteration 172/1000 | Loss: 0.00001073
Iteration 173/1000 | Loss: 0.00001073
Iteration 174/1000 | Loss: 0.00001072
Iteration 175/1000 | Loss: 0.00001072
Iteration 176/1000 | Loss: 0.00001072
Iteration 177/1000 | Loss: 0.00001072
Iteration 178/1000 | Loss: 0.00001072
Iteration 179/1000 | Loss: 0.00001072
Iteration 180/1000 | Loss: 0.00001072
Iteration 181/1000 | Loss: 0.00001071
Iteration 182/1000 | Loss: 0.00001071
Iteration 183/1000 | Loss: 0.00001071
Iteration 184/1000 | Loss: 0.00001071
Iteration 185/1000 | Loss: 0.00001071
Iteration 186/1000 | Loss: 0.00001071
Iteration 187/1000 | Loss: 0.00001071
Iteration 188/1000 | Loss: 0.00001071
Iteration 189/1000 | Loss: 0.00001071
Iteration 190/1000 | Loss: 0.00001071
Iteration 191/1000 | Loss: 0.00001071
Iteration 192/1000 | Loss: 0.00001071
Iteration 193/1000 | Loss: 0.00001071
Iteration 194/1000 | Loss: 0.00001071
Iteration 195/1000 | Loss: 0.00001071
Iteration 196/1000 | Loss: 0.00001071
Iteration 197/1000 | Loss: 0.00001071
Iteration 198/1000 | Loss: 0.00001071
Iteration 199/1000 | Loss: 0.00001071
Iteration 200/1000 | Loss: 0.00001071
Iteration 201/1000 | Loss: 0.00001071
Iteration 202/1000 | Loss: 0.00001071
Iteration 203/1000 | Loss: 0.00001071
Iteration 204/1000 | Loss: 0.00001071
Iteration 205/1000 | Loss: 0.00001071
Iteration 206/1000 | Loss: 0.00001071
Iteration 207/1000 | Loss: 0.00001071
Iteration 208/1000 | Loss: 0.00001071
Iteration 209/1000 | Loss: 0.00001071
Iteration 210/1000 | Loss: 0.00001071
Iteration 211/1000 | Loss: 0.00001071
Iteration 212/1000 | Loss: 0.00001071
Iteration 213/1000 | Loss: 0.00001071
Iteration 214/1000 | Loss: 0.00001071
Iteration 215/1000 | Loss: 0.00001071
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.070807957148645e-05, 1.070807957148645e-05, 1.070807957148645e-05, 1.070807957148645e-05, 1.070807957148645e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.070807957148645e-05

Optimization complete. Final v2v error: 2.7423152923583984 mm

Highest mean error: 3.4826629161834717 mm for frame 73

Lowest mean error: 2.4799582958221436 mm for frame 130

Saving results

Total time: 38.54260206222534
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00818205
Iteration 2/25 | Loss: 0.00109057
Iteration 3/25 | Loss: 0.00077784
Iteration 4/25 | Loss: 0.00075252
Iteration 5/25 | Loss: 0.00074342
Iteration 6/25 | Loss: 0.00074134
Iteration 7/25 | Loss: 0.00074099
Iteration 8/25 | Loss: 0.00074099
Iteration 9/25 | Loss: 0.00074099
Iteration 10/25 | Loss: 0.00074099
Iteration 11/25 | Loss: 0.00074099
Iteration 12/25 | Loss: 0.00074099
Iteration 13/25 | Loss: 0.00074099
Iteration 14/25 | Loss: 0.00074099
Iteration 15/25 | Loss: 0.00074099
Iteration 16/25 | Loss: 0.00074099
Iteration 17/25 | Loss: 0.00074099
Iteration 18/25 | Loss: 0.00074099
Iteration 19/25 | Loss: 0.00074099
Iteration 20/25 | Loss: 0.00074099
Iteration 21/25 | Loss: 0.00074099
Iteration 22/25 | Loss: 0.00074099
Iteration 23/25 | Loss: 0.00074099
Iteration 24/25 | Loss: 0.00074099
Iteration 25/25 | Loss: 0.00074099

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31299675
Iteration 2/25 | Loss: 0.00042656
Iteration 3/25 | Loss: 0.00042656
Iteration 4/25 | Loss: 0.00042655
Iteration 5/25 | Loss: 0.00042655
Iteration 6/25 | Loss: 0.00042655
Iteration 7/25 | Loss: 0.00042655
Iteration 8/25 | Loss: 0.00042655
Iteration 9/25 | Loss: 0.00042655
Iteration 10/25 | Loss: 0.00042655
Iteration 11/25 | Loss: 0.00042655
Iteration 12/25 | Loss: 0.00042655
Iteration 13/25 | Loss: 0.00042655
Iteration 14/25 | Loss: 0.00042655
Iteration 15/25 | Loss: 0.00042655
Iteration 16/25 | Loss: 0.00042655
Iteration 17/25 | Loss: 0.00042655
Iteration 18/25 | Loss: 0.00042655
Iteration 19/25 | Loss: 0.00042655
Iteration 20/25 | Loss: 0.00042655
Iteration 21/25 | Loss: 0.00042655
Iteration 22/25 | Loss: 0.00042655
Iteration 23/25 | Loss: 0.00042655
Iteration 24/25 | Loss: 0.00042655
Iteration 25/25 | Loss: 0.00042655

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042655
Iteration 2/1000 | Loss: 0.00003870
Iteration 3/1000 | Loss: 0.00002555
Iteration 4/1000 | Loss: 0.00002209
Iteration 5/1000 | Loss: 0.00002056
Iteration 6/1000 | Loss: 0.00001952
Iteration 7/1000 | Loss: 0.00001881
Iteration 8/1000 | Loss: 0.00001833
Iteration 9/1000 | Loss: 0.00001785
Iteration 10/1000 | Loss: 0.00001770
Iteration 11/1000 | Loss: 0.00001768
Iteration 12/1000 | Loss: 0.00001755
Iteration 13/1000 | Loss: 0.00001744
Iteration 14/1000 | Loss: 0.00001740
Iteration 15/1000 | Loss: 0.00001736
Iteration 16/1000 | Loss: 0.00001735
Iteration 17/1000 | Loss: 0.00001734
Iteration 18/1000 | Loss: 0.00001731
Iteration 19/1000 | Loss: 0.00001726
Iteration 20/1000 | Loss: 0.00001725
Iteration 21/1000 | Loss: 0.00001719
Iteration 22/1000 | Loss: 0.00001714
Iteration 23/1000 | Loss: 0.00001714
Iteration 24/1000 | Loss: 0.00001709
Iteration 25/1000 | Loss: 0.00001709
Iteration 26/1000 | Loss: 0.00001708
Iteration 27/1000 | Loss: 0.00001703
Iteration 28/1000 | Loss: 0.00001702
Iteration 29/1000 | Loss: 0.00001702
Iteration 30/1000 | Loss: 0.00001701
Iteration 31/1000 | Loss: 0.00001699
Iteration 32/1000 | Loss: 0.00001698
Iteration 33/1000 | Loss: 0.00001695
Iteration 34/1000 | Loss: 0.00001695
Iteration 35/1000 | Loss: 0.00001695
Iteration 36/1000 | Loss: 0.00001694
Iteration 37/1000 | Loss: 0.00001694
Iteration 38/1000 | Loss: 0.00001694
Iteration 39/1000 | Loss: 0.00001694
Iteration 40/1000 | Loss: 0.00001694
Iteration 41/1000 | Loss: 0.00001694
Iteration 42/1000 | Loss: 0.00001693
Iteration 43/1000 | Loss: 0.00001693
Iteration 44/1000 | Loss: 0.00001692
Iteration 45/1000 | Loss: 0.00001692
Iteration 46/1000 | Loss: 0.00001692
Iteration 47/1000 | Loss: 0.00001692
Iteration 48/1000 | Loss: 0.00001691
Iteration 49/1000 | Loss: 0.00001691
Iteration 50/1000 | Loss: 0.00001691
Iteration 51/1000 | Loss: 0.00001690
Iteration 52/1000 | Loss: 0.00001690
Iteration 53/1000 | Loss: 0.00001690
Iteration 54/1000 | Loss: 0.00001689
Iteration 55/1000 | Loss: 0.00001689
Iteration 56/1000 | Loss: 0.00001689
Iteration 57/1000 | Loss: 0.00001689
Iteration 58/1000 | Loss: 0.00001688
Iteration 59/1000 | Loss: 0.00001688
Iteration 60/1000 | Loss: 0.00001688
Iteration 61/1000 | Loss: 0.00001688
Iteration 62/1000 | Loss: 0.00001687
Iteration 63/1000 | Loss: 0.00001687
Iteration 64/1000 | Loss: 0.00001687
Iteration 65/1000 | Loss: 0.00001686
Iteration 66/1000 | Loss: 0.00001686
Iteration 67/1000 | Loss: 0.00001686
Iteration 68/1000 | Loss: 0.00001686
Iteration 69/1000 | Loss: 0.00001685
Iteration 70/1000 | Loss: 0.00001685
Iteration 71/1000 | Loss: 0.00001685
Iteration 72/1000 | Loss: 0.00001685
Iteration 73/1000 | Loss: 0.00001685
Iteration 74/1000 | Loss: 0.00001685
Iteration 75/1000 | Loss: 0.00001684
Iteration 76/1000 | Loss: 0.00001684
Iteration 77/1000 | Loss: 0.00001684
Iteration 78/1000 | Loss: 0.00001683
Iteration 79/1000 | Loss: 0.00001682
Iteration 80/1000 | Loss: 0.00001682
Iteration 81/1000 | Loss: 0.00001681
Iteration 82/1000 | Loss: 0.00001680
Iteration 83/1000 | Loss: 0.00001680
Iteration 84/1000 | Loss: 0.00001679
Iteration 85/1000 | Loss: 0.00001679
Iteration 86/1000 | Loss: 0.00001678
Iteration 87/1000 | Loss: 0.00001678
Iteration 88/1000 | Loss: 0.00001677
Iteration 89/1000 | Loss: 0.00001677
Iteration 90/1000 | Loss: 0.00001675
Iteration 91/1000 | Loss: 0.00001674
Iteration 92/1000 | Loss: 0.00001674
Iteration 93/1000 | Loss: 0.00001674
Iteration 94/1000 | Loss: 0.00001673
Iteration 95/1000 | Loss: 0.00001673
Iteration 96/1000 | Loss: 0.00001673
Iteration 97/1000 | Loss: 0.00001672
Iteration 98/1000 | Loss: 0.00001672
Iteration 99/1000 | Loss: 0.00001672
Iteration 100/1000 | Loss: 0.00001672
Iteration 101/1000 | Loss: 0.00001672
Iteration 102/1000 | Loss: 0.00001671
Iteration 103/1000 | Loss: 0.00001671
Iteration 104/1000 | Loss: 0.00001671
Iteration 105/1000 | Loss: 0.00001671
Iteration 106/1000 | Loss: 0.00001671
Iteration 107/1000 | Loss: 0.00001671
Iteration 108/1000 | Loss: 0.00001671
Iteration 109/1000 | Loss: 0.00001671
Iteration 110/1000 | Loss: 0.00001670
Iteration 111/1000 | Loss: 0.00001670
Iteration 112/1000 | Loss: 0.00001670
Iteration 113/1000 | Loss: 0.00001670
Iteration 114/1000 | Loss: 0.00001670
Iteration 115/1000 | Loss: 0.00001669
Iteration 116/1000 | Loss: 0.00001669
Iteration 117/1000 | Loss: 0.00001669
Iteration 118/1000 | Loss: 0.00001669
Iteration 119/1000 | Loss: 0.00001669
Iteration 120/1000 | Loss: 0.00001669
Iteration 121/1000 | Loss: 0.00001669
Iteration 122/1000 | Loss: 0.00001669
Iteration 123/1000 | Loss: 0.00001669
Iteration 124/1000 | Loss: 0.00001669
Iteration 125/1000 | Loss: 0.00001669
Iteration 126/1000 | Loss: 0.00001668
Iteration 127/1000 | Loss: 0.00001668
Iteration 128/1000 | Loss: 0.00001668
Iteration 129/1000 | Loss: 0.00001668
Iteration 130/1000 | Loss: 0.00001668
Iteration 131/1000 | Loss: 0.00001668
Iteration 132/1000 | Loss: 0.00001668
Iteration 133/1000 | Loss: 0.00001668
Iteration 134/1000 | Loss: 0.00001668
Iteration 135/1000 | Loss: 0.00001667
Iteration 136/1000 | Loss: 0.00001667
Iteration 137/1000 | Loss: 0.00001667
Iteration 138/1000 | Loss: 0.00001667
Iteration 139/1000 | Loss: 0.00001666
Iteration 140/1000 | Loss: 0.00001666
Iteration 141/1000 | Loss: 0.00001666
Iteration 142/1000 | Loss: 0.00001666
Iteration 143/1000 | Loss: 0.00001666
Iteration 144/1000 | Loss: 0.00001666
Iteration 145/1000 | Loss: 0.00001666
Iteration 146/1000 | Loss: 0.00001666
Iteration 147/1000 | Loss: 0.00001666
Iteration 148/1000 | Loss: 0.00001666
Iteration 149/1000 | Loss: 0.00001665
Iteration 150/1000 | Loss: 0.00001665
Iteration 151/1000 | Loss: 0.00001665
Iteration 152/1000 | Loss: 0.00001665
Iteration 153/1000 | Loss: 0.00001665
Iteration 154/1000 | Loss: 0.00001665
Iteration 155/1000 | Loss: 0.00001665
Iteration 156/1000 | Loss: 0.00001665
Iteration 157/1000 | Loss: 0.00001665
Iteration 158/1000 | Loss: 0.00001665
Iteration 159/1000 | Loss: 0.00001665
Iteration 160/1000 | Loss: 0.00001665
Iteration 161/1000 | Loss: 0.00001665
Iteration 162/1000 | Loss: 0.00001665
Iteration 163/1000 | Loss: 0.00001665
Iteration 164/1000 | Loss: 0.00001664
Iteration 165/1000 | Loss: 0.00001664
Iteration 166/1000 | Loss: 0.00001664
Iteration 167/1000 | Loss: 0.00001664
Iteration 168/1000 | Loss: 0.00001664
Iteration 169/1000 | Loss: 0.00001664
Iteration 170/1000 | Loss: 0.00001664
Iteration 171/1000 | Loss: 0.00001664
Iteration 172/1000 | Loss: 0.00001664
Iteration 173/1000 | Loss: 0.00001664
Iteration 174/1000 | Loss: 0.00001664
Iteration 175/1000 | Loss: 0.00001664
Iteration 176/1000 | Loss: 0.00001664
Iteration 177/1000 | Loss: 0.00001664
Iteration 178/1000 | Loss: 0.00001663
Iteration 179/1000 | Loss: 0.00001663
Iteration 180/1000 | Loss: 0.00001663
Iteration 181/1000 | Loss: 0.00001663
Iteration 182/1000 | Loss: 0.00001663
Iteration 183/1000 | Loss: 0.00001663
Iteration 184/1000 | Loss: 0.00001663
Iteration 185/1000 | Loss: 0.00001663
Iteration 186/1000 | Loss: 0.00001663
Iteration 187/1000 | Loss: 0.00001663
Iteration 188/1000 | Loss: 0.00001663
Iteration 189/1000 | Loss: 0.00001663
Iteration 190/1000 | Loss: 0.00001663
Iteration 191/1000 | Loss: 0.00001663
Iteration 192/1000 | Loss: 0.00001662
Iteration 193/1000 | Loss: 0.00001662
Iteration 194/1000 | Loss: 0.00001662
Iteration 195/1000 | Loss: 0.00001662
Iteration 196/1000 | Loss: 0.00001662
Iteration 197/1000 | Loss: 0.00001662
Iteration 198/1000 | Loss: 0.00001662
Iteration 199/1000 | Loss: 0.00001662
Iteration 200/1000 | Loss: 0.00001662
Iteration 201/1000 | Loss: 0.00001662
Iteration 202/1000 | Loss: 0.00001662
Iteration 203/1000 | Loss: 0.00001662
Iteration 204/1000 | Loss: 0.00001662
Iteration 205/1000 | Loss: 0.00001662
Iteration 206/1000 | Loss: 0.00001661
Iteration 207/1000 | Loss: 0.00001661
Iteration 208/1000 | Loss: 0.00001661
Iteration 209/1000 | Loss: 0.00001661
Iteration 210/1000 | Loss: 0.00001661
Iteration 211/1000 | Loss: 0.00001661
Iteration 212/1000 | Loss: 0.00001661
Iteration 213/1000 | Loss: 0.00001661
Iteration 214/1000 | Loss: 0.00001661
Iteration 215/1000 | Loss: 0.00001661
Iteration 216/1000 | Loss: 0.00001661
Iteration 217/1000 | Loss: 0.00001661
Iteration 218/1000 | Loss: 0.00001661
Iteration 219/1000 | Loss: 0.00001661
Iteration 220/1000 | Loss: 0.00001661
Iteration 221/1000 | Loss: 0.00001661
Iteration 222/1000 | Loss: 0.00001661
Iteration 223/1000 | Loss: 0.00001661
Iteration 224/1000 | Loss: 0.00001661
Iteration 225/1000 | Loss: 0.00001661
Iteration 226/1000 | Loss: 0.00001661
Iteration 227/1000 | Loss: 0.00001661
Iteration 228/1000 | Loss: 0.00001661
Iteration 229/1000 | Loss: 0.00001661
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 229. Stopping optimization.
Last 5 losses: [1.660517773416359e-05, 1.660517773416359e-05, 1.660517773416359e-05, 1.660517773416359e-05, 1.660517773416359e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.660517773416359e-05

Optimization complete. Final v2v error: 3.3830440044403076 mm

Highest mean error: 5.072197437286377 mm for frame 75

Lowest mean error: 2.7760732173919678 mm for frame 107

Saving results

Total time: 45.89543175697327
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00466403
Iteration 2/25 | Loss: 0.00133923
Iteration 3/25 | Loss: 0.00101423
Iteration 4/25 | Loss: 0.00091093
Iteration 5/25 | Loss: 0.00087282
Iteration 6/25 | Loss: 0.00086627
Iteration 7/25 | Loss: 0.00084661
Iteration 8/25 | Loss: 0.00082776
Iteration 9/25 | Loss: 0.00082869
Iteration 10/25 | Loss: 0.00083241
Iteration 11/25 | Loss: 0.00082312
Iteration 12/25 | Loss: 0.00082047
Iteration 13/25 | Loss: 0.00081849
Iteration 14/25 | Loss: 0.00082396
Iteration 15/25 | Loss: 0.00082196
Iteration 16/25 | Loss: 0.00083259
Iteration 17/25 | Loss: 0.00082630
Iteration 18/25 | Loss: 0.00083221
Iteration 19/25 | Loss: 0.00082821
Iteration 20/25 | Loss: 0.00082745
Iteration 21/25 | Loss: 0.00082130
Iteration 22/25 | Loss: 0.00082255
Iteration 23/25 | Loss: 0.00082232
Iteration 24/25 | Loss: 0.00082007
Iteration 25/25 | Loss: 0.00081919

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50964880
Iteration 2/25 | Loss: 0.00077493
Iteration 3/25 | Loss: 0.00077491
Iteration 4/25 | Loss: 0.00077491
Iteration 5/25 | Loss: 0.00077491
Iteration 6/25 | Loss: 0.00077491
Iteration 7/25 | Loss: 0.00077491
Iteration 8/25 | Loss: 0.00077491
Iteration 9/25 | Loss: 0.00077491
Iteration 10/25 | Loss: 0.00077491
Iteration 11/25 | Loss: 0.00077491
Iteration 12/25 | Loss: 0.00077491
Iteration 13/25 | Loss: 0.00077491
Iteration 14/25 | Loss: 0.00077491
Iteration 15/25 | Loss: 0.00077491
Iteration 16/25 | Loss: 0.00077491
Iteration 17/25 | Loss: 0.00077491
Iteration 18/25 | Loss: 0.00077491
Iteration 19/25 | Loss: 0.00077491
Iteration 20/25 | Loss: 0.00077491
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007749104406684637, 0.0007749104406684637, 0.0007749104406684637, 0.0007749104406684637, 0.0007749104406684637]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007749104406684637

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077491
Iteration 2/1000 | Loss: 0.00009200
Iteration 3/1000 | Loss: 0.00005643
Iteration 4/1000 | Loss: 0.00006871
Iteration 5/1000 | Loss: 0.00004102
Iteration 6/1000 | Loss: 0.00005778
Iteration 7/1000 | Loss: 0.00004247
Iteration 8/1000 | Loss: 0.00003539
Iteration 9/1000 | Loss: 0.00005630
Iteration 10/1000 | Loss: 0.00004758
Iteration 11/1000 | Loss: 0.00005619
Iteration 12/1000 | Loss: 0.00004334
Iteration 13/1000 | Loss: 0.00005027
Iteration 14/1000 | Loss: 0.00004987
Iteration 15/1000 | Loss: 0.00003905
Iteration 16/1000 | Loss: 0.00005524
Iteration 17/1000 | Loss: 0.00005312
Iteration 18/1000 | Loss: 0.00008465
Iteration 19/1000 | Loss: 0.00007612
Iteration 20/1000 | Loss: 0.00005481
Iteration 21/1000 | Loss: 0.00003461
Iteration 22/1000 | Loss: 0.00002910
Iteration 23/1000 | Loss: 0.00003402
Iteration 24/1000 | Loss: 0.00003528
Iteration 25/1000 | Loss: 0.00003327
Iteration 26/1000 | Loss: 0.00002895
Iteration 27/1000 | Loss: 0.00002719
Iteration 28/1000 | Loss: 0.00003084
Iteration 29/1000 | Loss: 0.00003345
Iteration 30/1000 | Loss: 0.00002837
Iteration 31/1000 | Loss: 0.00002496
Iteration 32/1000 | Loss: 0.00002542
Iteration 33/1000 | Loss: 0.00002474
Iteration 34/1000 | Loss: 0.00003137
Iteration 35/1000 | Loss: 0.00002963
Iteration 36/1000 | Loss: 0.00003426
Iteration 37/1000 | Loss: 0.00003059
Iteration 38/1000 | Loss: 0.00003992
Iteration 39/1000 | Loss: 0.00004094
Iteration 40/1000 | Loss: 0.00003184
Iteration 41/1000 | Loss: 0.00002894
Iteration 42/1000 | Loss: 0.00002756
Iteration 43/1000 | Loss: 0.00002758
Iteration 44/1000 | Loss: 0.00004304
Iteration 45/1000 | Loss: 0.00004453
Iteration 46/1000 | Loss: 0.00003103
Iteration 47/1000 | Loss: 0.00003243
Iteration 48/1000 | Loss: 0.00003515
Iteration 49/1000 | Loss: 0.00003454
Iteration 50/1000 | Loss: 0.00004007
Iteration 51/1000 | Loss: 0.00004064
Iteration 52/1000 | Loss: 0.00004276
Iteration 53/1000 | Loss: 0.00003952
Iteration 54/1000 | Loss: 0.00004277
Iteration 55/1000 | Loss: 0.00003729
Iteration 56/1000 | Loss: 0.00004401
Iteration 57/1000 | Loss: 0.00004421
Iteration 58/1000 | Loss: 0.00003040
Iteration 59/1000 | Loss: 0.00002694
Iteration 60/1000 | Loss: 0.00004336
Iteration 61/1000 | Loss: 0.00003422
Iteration 62/1000 | Loss: 0.00002949
Iteration 63/1000 | Loss: 0.00003248
Iteration 64/1000 | Loss: 0.00003014
Iteration 65/1000 | Loss: 0.00004489
Iteration 66/1000 | Loss: 0.00003877
Iteration 67/1000 | Loss: 0.00003978
Iteration 68/1000 | Loss: 0.00003145
Iteration 69/1000 | Loss: 0.00002583
Iteration 70/1000 | Loss: 0.00002784
Iteration 71/1000 | Loss: 0.00005126
Iteration 72/1000 | Loss: 0.00003567
Iteration 73/1000 | Loss: 0.00004786
Iteration 74/1000 | Loss: 0.00003408
Iteration 75/1000 | Loss: 0.00004563
Iteration 76/1000 | Loss: 0.00002981
Iteration 77/1000 | Loss: 0.00002778
Iteration 78/1000 | Loss: 0.00002770
Iteration 79/1000 | Loss: 0.00002800
Iteration 80/1000 | Loss: 0.00004890
Iteration 81/1000 | Loss: 0.00003297
Iteration 82/1000 | Loss: 0.00005394
Iteration 83/1000 | Loss: 0.00003355
Iteration 84/1000 | Loss: 0.00002957
Iteration 85/1000 | Loss: 0.00002697
Iteration 86/1000 | Loss: 0.00002557
Iteration 87/1000 | Loss: 0.00002583
Iteration 88/1000 | Loss: 0.00002453
Iteration 89/1000 | Loss: 0.00002683
Iteration 90/1000 | Loss: 0.00003058
Iteration 91/1000 | Loss: 0.00004010
Iteration 92/1000 | Loss: 0.00003596
Iteration 93/1000 | Loss: 0.00004697
Iteration 94/1000 | Loss: 0.00003703
Iteration 95/1000 | Loss: 0.00005438
Iteration 96/1000 | Loss: 0.00004419
Iteration 97/1000 | Loss: 0.00006462
Iteration 98/1000 | Loss: 0.00004703
Iteration 99/1000 | Loss: 0.00003563
Iteration 100/1000 | Loss: 0.00003320
Iteration 101/1000 | Loss: 0.00002610
Iteration 102/1000 | Loss: 0.00003478
Iteration 103/1000 | Loss: 0.00002795
Iteration 104/1000 | Loss: 0.00003278
Iteration 105/1000 | Loss: 0.00002904
Iteration 106/1000 | Loss: 0.00005258
Iteration 107/1000 | Loss: 0.00004040
Iteration 108/1000 | Loss: 0.00003937
Iteration 109/1000 | Loss: 0.00003352
Iteration 110/1000 | Loss: 0.00003240
Iteration 111/1000 | Loss: 0.00002682
Iteration 112/1000 | Loss: 0.00002580
Iteration 113/1000 | Loss: 0.00002416
Iteration 114/1000 | Loss: 0.00003274
Iteration 115/1000 | Loss: 0.00003019
Iteration 116/1000 | Loss: 0.00004955
Iteration 117/1000 | Loss: 0.00003964
Iteration 118/1000 | Loss: 0.00007176
Iteration 119/1000 | Loss: 0.00004467
Iteration 120/1000 | Loss: 0.00007287
Iteration 121/1000 | Loss: 0.00004713
Iteration 122/1000 | Loss: 0.00006546
Iteration 123/1000 | Loss: 0.00003251
Iteration 124/1000 | Loss: 0.00003531
Iteration 125/1000 | Loss: 0.00003003
Iteration 126/1000 | Loss: 0.00005250
Iteration 127/1000 | Loss: 0.00003900
Iteration 128/1000 | Loss: 0.00007037
Iteration 129/1000 | Loss: 0.00004825
Iteration 130/1000 | Loss: 0.00008718
Iteration 131/1000 | Loss: 0.00005244
Iteration 132/1000 | Loss: 0.00008825
Iteration 133/1000 | Loss: 0.00003867
Iteration 134/1000 | Loss: 0.00002703
Iteration 135/1000 | Loss: 0.00003390
Iteration 136/1000 | Loss: 0.00002731
Iteration 137/1000 | Loss: 0.00004204
Iteration 138/1000 | Loss: 0.00004230
Iteration 139/1000 | Loss: 0.00005683
Iteration 140/1000 | Loss: 0.00004129
Iteration 141/1000 | Loss: 0.00005510
Iteration 142/1000 | Loss: 0.00004087
Iteration 143/1000 | Loss: 0.00003389
Iteration 144/1000 | Loss: 0.00002863
Iteration 145/1000 | Loss: 0.00004612
Iteration 146/1000 | Loss: 0.00003853
Iteration 147/1000 | Loss: 0.00004925
Iteration 148/1000 | Loss: 0.00004381
Iteration 149/1000 | Loss: 0.00004292
Iteration 150/1000 | Loss: 0.00003357
Iteration 151/1000 | Loss: 0.00004984
Iteration 152/1000 | Loss: 0.00004182
Iteration 153/1000 | Loss: 0.00005103
Iteration 154/1000 | Loss: 0.00004760
Iteration 155/1000 | Loss: 0.00005353
Iteration 156/1000 | Loss: 0.00004261
Iteration 157/1000 | Loss: 0.00003550
Iteration 158/1000 | Loss: 0.00003382
Iteration 159/1000 | Loss: 0.00005101
Iteration 160/1000 | Loss: 0.00004442
Iteration 161/1000 | Loss: 0.00005046
Iteration 162/1000 | Loss: 0.00003916
Iteration 163/1000 | Loss: 0.00002873
Iteration 164/1000 | Loss: 0.00002869
Iteration 165/1000 | Loss: 0.00002398
Iteration 166/1000 | Loss: 0.00002569
Iteration 167/1000 | Loss: 0.00002781
Iteration 168/1000 | Loss: 0.00003252
Iteration 169/1000 | Loss: 0.00003179
Iteration 170/1000 | Loss: 0.00002490
Iteration 171/1000 | Loss: 0.00002762
Iteration 172/1000 | Loss: 0.00002726
Iteration 173/1000 | Loss: 0.00002771
Iteration 174/1000 | Loss: 0.00003378
Iteration 175/1000 | Loss: 0.00002947
Iteration 176/1000 | Loss: 0.00002677
Iteration 177/1000 | Loss: 0.00002541
Iteration 178/1000 | Loss: 0.00002865
Iteration 179/1000 | Loss: 0.00002744
Iteration 180/1000 | Loss: 0.00003594
Iteration 181/1000 | Loss: 0.00002765
Iteration 182/1000 | Loss: 0.00002634
Iteration 183/1000 | Loss: 0.00003116
Iteration 184/1000 | Loss: 0.00002882
Iteration 185/1000 | Loss: 0.00002664
Iteration 186/1000 | Loss: 0.00003270
Iteration 187/1000 | Loss: 0.00003506
Iteration 188/1000 | Loss: 0.00003208
Iteration 189/1000 | Loss: 0.00003274
Iteration 190/1000 | Loss: 0.00003785
Iteration 191/1000 | Loss: 0.00003971
Iteration 192/1000 | Loss: 0.00003927
Iteration 193/1000 | Loss: 0.00003114
Iteration 194/1000 | Loss: 0.00002435
Iteration 195/1000 | Loss: 0.00002520
Iteration 196/1000 | Loss: 0.00002655
Iteration 197/1000 | Loss: 0.00003566
Iteration 198/1000 | Loss: 0.00003651
Iteration 199/1000 | Loss: 0.00004527
Iteration 200/1000 | Loss: 0.00004440
Iteration 201/1000 | Loss: 0.00004998
Iteration 202/1000 | Loss: 0.00004682
Iteration 203/1000 | Loss: 0.00003500
Iteration 204/1000 | Loss: 0.00002686
Iteration 205/1000 | Loss: 0.00003251
Iteration 206/1000 | Loss: 0.00004164
Iteration 207/1000 | Loss: 0.00005584
Iteration 208/1000 | Loss: 0.00005463
Iteration 209/1000 | Loss: 0.00002830
Iteration 210/1000 | Loss: 0.00002305
Iteration 211/1000 | Loss: 0.00002672
Iteration 212/1000 | Loss: 0.00003212
Iteration 213/1000 | Loss: 0.00005157
Iteration 214/1000 | Loss: 0.00004451
Iteration 215/1000 | Loss: 0.00005312
Iteration 216/1000 | Loss: 0.00005218
Iteration 217/1000 | Loss: 0.00003176
Iteration 218/1000 | Loss: 0.00003392
Iteration 219/1000 | Loss: 0.00005924
Iteration 220/1000 | Loss: 0.00006350
Iteration 221/1000 | Loss: 0.00005878
Iteration 222/1000 | Loss: 0.00003478
Iteration 223/1000 | Loss: 0.00002785
Iteration 224/1000 | Loss: 0.00002818
Iteration 225/1000 | Loss: 0.00003135
Iteration 226/1000 | Loss: 0.00002739
Iteration 227/1000 | Loss: 0.00002591
Iteration 228/1000 | Loss: 0.00002866
Iteration 229/1000 | Loss: 0.00003450
Iteration 230/1000 | Loss: 0.00004209
Iteration 231/1000 | Loss: 0.00003698
Iteration 232/1000 | Loss: 0.00004348
Iteration 233/1000 | Loss: 0.00004102
Iteration 234/1000 | Loss: 0.00004735
Iteration 235/1000 | Loss: 0.00004989
Iteration 236/1000 | Loss: 0.00002786
Iteration 237/1000 | Loss: 0.00002426
Iteration 238/1000 | Loss: 0.00002492
Iteration 239/1000 | Loss: 0.00002558
Iteration 240/1000 | Loss: 0.00002510
Iteration 241/1000 | Loss: 0.00002862
Iteration 242/1000 | Loss: 0.00002798
Iteration 243/1000 | Loss: 0.00002713
Iteration 244/1000 | Loss: 0.00003255
Iteration 245/1000 | Loss: 0.00002653
Iteration 246/1000 | Loss: 0.00003138
Iteration 247/1000 | Loss: 0.00003039
Iteration 248/1000 | Loss: 0.00003417
Iteration 249/1000 | Loss: 0.00002826
Iteration 250/1000 | Loss: 0.00002781
Iteration 251/1000 | Loss: 0.00002734
Iteration 252/1000 | Loss: 0.00003097
Iteration 253/1000 | Loss: 0.00002900
Iteration 254/1000 | Loss: 0.00004443
Iteration 255/1000 | Loss: 0.00003548
Iteration 256/1000 | Loss: 0.00005629
Iteration 257/1000 | Loss: 0.00003968
Iteration 258/1000 | Loss: 0.00004095
Iteration 259/1000 | Loss: 0.00004094
Iteration 260/1000 | Loss: 0.00005558
Iteration 261/1000 | Loss: 0.00004811
Iteration 262/1000 | Loss: 0.00003894
Iteration 263/1000 | Loss: 0.00003169
Iteration 264/1000 | Loss: 0.00003921
Iteration 265/1000 | Loss: 0.00003326
Iteration 266/1000 | Loss: 0.00003369
Iteration 267/1000 | Loss: 0.00003232
Iteration 268/1000 | Loss: 0.00004134
Iteration 269/1000 | Loss: 0.00003523
Iteration 270/1000 | Loss: 0.00004766
Iteration 271/1000 | Loss: 0.00003769
Iteration 272/1000 | Loss: 0.00004043
Iteration 273/1000 | Loss: 0.00003887
Iteration 274/1000 | Loss: 0.00004321
Iteration 275/1000 | Loss: 0.00002743
Iteration 276/1000 | Loss: 0.00003221
Iteration 277/1000 | Loss: 0.00002844
Iteration 278/1000 | Loss: 0.00003711
Iteration 279/1000 | Loss: 0.00002918
Iteration 280/1000 | Loss: 0.00004240
Iteration 281/1000 | Loss: 0.00004015
Iteration 282/1000 | Loss: 0.00005409
Iteration 283/1000 | Loss: 0.00004378
Iteration 284/1000 | Loss: 0.00006037
Iteration 285/1000 | Loss: 0.00005353
Iteration 286/1000 | Loss: 0.00006928
Iteration 287/1000 | Loss: 0.00004310
Iteration 288/1000 | Loss: 0.00004491
Iteration 289/1000 | Loss: 0.00004651
Iteration 290/1000 | Loss: 0.00004179
Iteration 291/1000 | Loss: 0.00004838
Iteration 292/1000 | Loss: 0.00005886
Iteration 293/1000 | Loss: 0.00003977
Iteration 294/1000 | Loss: 0.00003506
Iteration 295/1000 | Loss: 0.00003134
Iteration 296/1000 | Loss: 0.00002919
Iteration 297/1000 | Loss: 0.00003005
Iteration 298/1000 | Loss: 0.00003919
Iteration 299/1000 | Loss: 0.00004159
Iteration 300/1000 | Loss: 0.00004640
Iteration 301/1000 | Loss: 0.00004613
Iteration 302/1000 | Loss: 0.00005983
Iteration 303/1000 | Loss: 0.00005049
Iteration 304/1000 | Loss: 0.00006831
Iteration 305/1000 | Loss: 0.00006628
Iteration 306/1000 | Loss: 0.00007232
Iteration 307/1000 | Loss: 0.00003913
Iteration 308/1000 | Loss: 0.00002754
Iteration 309/1000 | Loss: 0.00005482
Iteration 310/1000 | Loss: 0.00006418
Iteration 311/1000 | Loss: 0.00005809
Iteration 312/1000 | Loss: 0.00006414
Iteration 313/1000 | Loss: 0.00005011
Iteration 314/1000 | Loss: 0.00004818
Iteration 315/1000 | Loss: 0.00006789
Iteration 316/1000 | Loss: 0.00006577
Iteration 317/1000 | Loss: 0.00005800
Iteration 318/1000 | Loss: 0.00003348
Iteration 319/1000 | Loss: 0.00004514
Iteration 320/1000 | Loss: 0.00003650
Iteration 321/1000 | Loss: 0.00003340
Iteration 322/1000 | Loss: 0.00002928
Iteration 323/1000 | Loss: 0.00005722
Iteration 324/1000 | Loss: 0.00005177
Iteration 325/1000 | Loss: 0.00006965
Iteration 326/1000 | Loss: 0.00005533
Iteration 327/1000 | Loss: 0.00004547
Iteration 328/1000 | Loss: 0.00004767
Iteration 329/1000 | Loss: 0.00004720
Iteration 330/1000 | Loss: 0.00004571
Iteration 331/1000 | Loss: 0.00006205
Iteration 332/1000 | Loss: 0.00005302
Iteration 333/1000 | Loss: 0.00005290
Iteration 334/1000 | Loss: 0.00004579
Iteration 335/1000 | Loss: 0.00004455
Iteration 336/1000 | Loss: 0.00004829
Iteration 337/1000 | Loss: 0.00005233
Iteration 338/1000 | Loss: 0.00005233
Iteration 339/1000 | Loss: 0.00004437
Iteration 340/1000 | Loss: 0.00005007
Iteration 341/1000 | Loss: 0.00005337
Iteration 342/1000 | Loss: 0.00006031
Iteration 343/1000 | Loss: 0.00006069
Iteration 344/1000 | Loss: 0.00006637
Iteration 345/1000 | Loss: 0.00005649
Iteration 346/1000 | Loss: 0.00006515
Iteration 347/1000 | Loss: 0.00006651
Iteration 348/1000 | Loss: 0.00005747
Iteration 349/1000 | Loss: 0.00006484
Iteration 350/1000 | Loss: 0.00006536
Iteration 351/1000 | Loss: 0.00007089
Iteration 352/1000 | Loss: 0.00007076
Iteration 353/1000 | Loss: 0.00006635
Iteration 354/1000 | Loss: 0.00007006
Iteration 355/1000 | Loss: 0.00007630
Iteration 356/1000 | Loss: 0.00006756
Iteration 357/1000 | Loss: 0.00006630
Iteration 358/1000 | Loss: 0.00007072
Iteration 359/1000 | Loss: 0.00007638
Iteration 360/1000 | Loss: 0.00006879
Iteration 361/1000 | Loss: 0.00007386
Iteration 362/1000 | Loss: 0.00007216
Iteration 363/1000 | Loss: 0.00006214
Iteration 364/1000 | Loss: 0.00004723
Iteration 365/1000 | Loss: 0.00006123
Iteration 366/1000 | Loss: 0.00004873
Iteration 367/1000 | Loss: 0.00006817
Iteration 368/1000 | Loss: 0.00006374
Iteration 369/1000 | Loss: 0.00006496
Iteration 370/1000 | Loss: 0.00006029
Iteration 371/1000 | Loss: 0.00023688
Iteration 372/1000 | Loss: 0.00012565
Iteration 373/1000 | Loss: 0.00005656
Iteration 374/1000 | Loss: 0.00003819
Iteration 375/1000 | Loss: 0.00003068
Iteration 376/1000 | Loss: 0.00003511
Iteration 377/1000 | Loss: 0.00002947
Iteration 378/1000 | Loss: 0.00003361
Iteration 379/1000 | Loss: 0.00003249
Iteration 380/1000 | Loss: 0.00002642
Iteration 381/1000 | Loss: 0.00003014
Iteration 382/1000 | Loss: 0.00003126
Iteration 383/1000 | Loss: 0.00004053
Iteration 384/1000 | Loss: 0.00003347
Iteration 385/1000 | Loss: 0.00003987
Iteration 386/1000 | Loss: 0.00003440
Iteration 387/1000 | Loss: 0.00003636
Iteration 388/1000 | Loss: 0.00003207
Iteration 389/1000 | Loss: 0.00003842
Iteration 390/1000 | Loss: 0.00003125
Iteration 391/1000 | Loss: 0.00004233
Iteration 392/1000 | Loss: 0.00004480
Iteration 393/1000 | Loss: 0.00005298
Iteration 394/1000 | Loss: 0.00005258
Iteration 395/1000 | Loss: 0.00005706
Iteration 396/1000 | Loss: 0.00004921
Iteration 397/1000 | Loss: 0.00005770
Iteration 398/1000 | Loss: 0.00005980
Iteration 399/1000 | Loss: 0.00007158
Iteration 400/1000 | Loss: 0.00006101
Iteration 401/1000 | Loss: 0.00005981
Iteration 402/1000 | Loss: 0.00006294
Iteration 403/1000 | Loss: 0.00007058
Iteration 404/1000 | Loss: 0.00006027
Iteration 405/1000 | Loss: 0.00008069
Iteration 406/1000 | Loss: 0.00005369
Iteration 407/1000 | Loss: 0.00008324
Iteration 408/1000 | Loss: 0.00005670
Iteration 409/1000 | Loss: 0.00007823
Iteration 410/1000 | Loss: 0.00007076
Iteration 411/1000 | Loss: 0.00008697
Iteration 412/1000 | Loss: 0.00007803
Iteration 413/1000 | Loss: 0.00007709
Iteration 414/1000 | Loss: 0.00003419
Iteration 415/1000 | Loss: 0.00002952
Iteration 416/1000 | Loss: 0.00002302
Iteration 417/1000 | Loss: 0.00002735
Iteration 418/1000 | Loss: 0.00002838
Iteration 419/1000 | Loss: 0.00003604
Iteration 420/1000 | Loss: 0.00002995
Iteration 421/1000 | Loss: 0.00002912
Iteration 422/1000 | Loss: 0.00004070
Iteration 423/1000 | Loss: 0.00004525
Iteration 424/1000 | Loss: 0.00005818
Iteration 425/1000 | Loss: 0.00005778
Iteration 426/1000 | Loss: 0.00007127
Iteration 427/1000 | Loss: 0.00004979
Iteration 428/1000 | Loss: 0.00007120
Iteration 429/1000 | Loss: 0.00005012
Iteration 430/1000 | Loss: 0.00006977
Iteration 431/1000 | Loss: 0.00006009
Iteration 432/1000 | Loss: 0.00008507
Iteration 433/1000 | Loss: 0.00006342
Iteration 434/1000 | Loss: 0.00011246
Iteration 435/1000 | Loss: 0.00006187
Iteration 436/1000 | Loss: 0.00008649
Iteration 437/1000 | Loss: 0.00003495
Iteration 438/1000 | Loss: 0.00004438
Iteration 439/1000 | Loss: 0.00002709
Iteration 440/1000 | Loss: 0.00002659
Iteration 441/1000 | Loss: 0.00002298
Iteration 442/1000 | Loss: 0.00003974
Iteration 443/1000 | Loss: 0.00003784
Iteration 444/1000 | Loss: 0.00005835
Iteration 445/1000 | Loss: 0.00004732
Iteration 446/1000 | Loss: 0.00007024
Iteration 447/1000 | Loss: 0.00004905
Iteration 448/1000 | Loss: 0.00006472
Iteration 449/1000 | Loss: 0.00005299
Iteration 450/1000 | Loss: 0.00005810
Iteration 451/1000 | Loss: 0.00004027
Iteration 452/1000 | Loss: 0.00007002
Iteration 453/1000 | Loss: 0.00005614
Iteration 454/1000 | Loss: 0.00006975
Iteration 455/1000 | Loss: 0.00005779
Iteration 456/1000 | Loss: 0.00003057
Iteration 457/1000 | Loss: 0.00002843
Iteration 458/1000 | Loss: 0.00004106
Iteration 459/1000 | Loss: 0.00003429
Iteration 460/1000 | Loss: 0.00003071
Iteration 461/1000 | Loss: 0.00003803
Iteration 462/1000 | Loss: 0.00004257
Iteration 463/1000 | Loss: 0.00002716
Iteration 464/1000 | Loss: 0.00002288
Iteration 465/1000 | Loss: 0.00002730
Iteration 466/1000 | Loss: 0.00002547
Iteration 467/1000 | Loss: 0.00003479
Iteration 468/1000 | Loss: 0.00002903
Iteration 469/1000 | Loss: 0.00002824
Iteration 470/1000 | Loss: 0.00002625
Iteration 471/1000 | Loss: 0.00005112
Iteration 472/1000 | Loss: 0.00004044
Iteration 473/1000 | Loss: 0.00005817
Iteration 474/1000 | Loss: 0.00005066
Iteration 475/1000 | Loss: 0.00007137
Iteration 476/1000 | Loss: 0.00004732
Iteration 477/1000 | Loss: 0.00006561
Iteration 478/1000 | Loss: 0.00005103
Iteration 479/1000 | Loss: 0.00007579
Iteration 480/1000 | Loss: 0.00005470
Iteration 481/1000 | Loss: 0.00007994
Iteration 482/1000 | Loss: 0.00006511
Iteration 483/1000 | Loss: 0.00008526
Iteration 484/1000 | Loss: 0.00005886
Iteration 485/1000 | Loss: 0.00006457
Iteration 486/1000 | Loss: 0.00005817
Iteration 487/1000 | Loss: 0.00007787
Iteration 488/1000 | Loss: 0.00006869
Iteration 489/1000 | Loss: 0.00008860
Iteration 490/1000 | Loss: 0.00006963
Iteration 491/1000 | Loss: 0.00008240
Iteration 492/1000 | Loss: 0.00005850
Iteration 493/1000 | Loss: 0.00009264
Iteration 494/1000 | Loss: 0.00006925
Iteration 495/1000 | Loss: 0.00006351
Iteration 496/1000 | Loss: 0.00005307
Iteration 497/1000 | Loss: 0.00006975
Iteration 498/1000 | Loss: 0.00005145
Iteration 499/1000 | Loss: 0.00007549
Iteration 500/1000 | Loss: 0.00006535
Iteration 501/1000 | Loss: 0.00007386
Iteration 502/1000 | Loss: 0.00006037
Iteration 503/1000 | Loss: 0.00006323
Iteration 504/1000 | Loss: 0.00004350
Iteration 505/1000 | Loss: 0.00003044
Iteration 506/1000 | Loss: 0.00002649
Iteration 507/1000 | Loss: 0.00002362
Iteration 508/1000 | Loss: 0.00002370
Iteration 509/1000 | Loss: 0.00002788
Iteration 510/1000 | Loss: 0.00002796
Iteration 511/1000 | Loss: 0.00002660
Iteration 512/1000 | Loss: 0.00002746
Iteration 513/1000 | Loss: 0.00002628
Iteration 514/1000 | Loss: 0.00002603
Iteration 515/1000 | Loss: 0.00002574
Iteration 516/1000 | Loss: 0.00002530
Iteration 517/1000 | Loss: 0.00002633
Iteration 518/1000 | Loss: 0.00002654
Iteration 519/1000 | Loss: 0.00002663
Iteration 520/1000 | Loss: 0.00003034
Iteration 521/1000 | Loss: 0.00002758
Iteration 522/1000 | Loss: 0.00002804
Iteration 523/1000 | Loss: 0.00002707
Iteration 524/1000 | Loss: 0.00003094
Iteration 525/1000 | Loss: 0.00003201
Iteration 526/1000 | Loss: 0.00003812
Iteration 527/1000 | Loss: 0.00003707
Iteration 528/1000 | Loss: 0.00004073
Iteration 529/1000 | Loss: 0.00003880
Iteration 530/1000 | Loss: 0.00004168
Iteration 531/1000 | Loss: 0.00004021
Iteration 532/1000 | Loss: 0.00004566
Iteration 533/1000 | Loss: 0.00004590
Iteration 534/1000 | Loss: 0.00003919
Iteration 535/1000 | Loss: 0.00003503
Iteration 536/1000 | Loss: 0.00003513
Iteration 537/1000 | Loss: 0.00003892
Iteration 538/1000 | Loss: 0.00004618
Iteration 539/1000 | Loss: 0.00003992
Iteration 540/1000 | Loss: 0.00003303
Iteration 541/1000 | Loss: 0.00003520
Iteration 542/1000 | Loss: 0.00004495
Iteration 543/1000 | Loss: 0.00004646
Iteration 544/1000 | Loss: 0.00004250
Iteration 545/1000 | Loss: 0.00003617
Iteration 546/1000 | Loss: 0.00004439
Iteration 547/1000 | Loss: 0.00004308
Iteration 548/1000 | Loss: 0.00004948
Iteration 549/1000 | Loss: 0.00003881
Iteration 550/1000 | Loss: 0.00004926
Iteration 551/1000 | Loss: 0.00005072
Iteration 552/1000 | Loss: 0.00005882
Iteration 553/1000 | Loss: 0.00004230
Iteration 554/1000 | Loss: 0.00002807
Iteration 555/1000 | Loss: 0.00003428
Iteration 556/1000 | Loss: 0.00003202
Iteration 557/1000 | Loss: 0.00005202
Iteration 558/1000 | Loss: 0.00004339
Iteration 559/1000 | Loss: 0.00005760
Iteration 560/1000 | Loss: 0.00004232
Iteration 561/1000 | Loss: 0.00006016
Iteration 562/1000 | Loss: 0.00004496
Iteration 563/1000 | Loss: 0.00004831
Iteration 564/1000 | Loss: 0.00004462
Iteration 565/1000 | Loss: 0.00005890
Iteration 566/1000 | Loss: 0.00004590
Iteration 567/1000 | Loss: 0.00004438
Iteration 568/1000 | Loss: 0.00003890
Iteration 569/1000 | Loss: 0.00004640
Iteration 570/1000 | Loss: 0.00004915
Iteration 571/1000 | Loss: 0.00005070
Iteration 572/1000 | Loss: 0.00005453
Iteration 573/1000 | Loss: 0.00006086
Iteration 574/1000 | Loss: 0.00006120
Iteration 575/1000 | Loss: 0.00005731
Iteration 576/1000 | Loss: 0.00006040
Iteration 577/1000 | Loss: 0.00006726
Iteration 578/1000 | Loss: 0.00006316
Iteration 579/1000 | Loss: 0.00005931
Iteration 580/1000 | Loss: 0.00006181
Iteration 581/1000 | Loss: 0.00007189
Iteration 582/1000 | Loss: 0.00006641
Iteration 583/1000 | Loss: 0.00007551
Iteration 584/1000 | Loss: 0.00006691
Iteration 585/1000 | Loss: 0.00007189
Iteration 586/1000 | Loss: 0.00005280
Iteration 587/1000 | Loss: 0.00004300
Iteration 588/1000 | Loss: 0.00004211
Iteration 589/1000 | Loss: 0.00004033
Iteration 590/1000 | Loss: 0.00005841
Iteration 591/1000 | Loss: 0.00006147
Iteration 592/1000 | Loss: 0.00006599
Iteration 593/1000 | Loss: 0.00007713
Iteration 594/1000 | Loss: 0.00006369
Iteration 595/1000 | Loss: 0.00004761
Iteration 596/1000 | Loss: 0.00006048
Iteration 597/1000 | Loss: 0.00008369
Iteration 598/1000 | Loss: 0.00007691
Iteration 599/1000 | Loss: 0.00008017
Iteration 600/1000 | Loss: 0.00007250
Iteration 601/1000 | Loss: 0.00008894
Iteration 602/1000 | Loss: 0.00006876
Iteration 603/1000 | Loss: 0.00003895
Iteration 604/1000 | Loss: 0.00002830
Iteration 605/1000 | Loss: 0.00002456
Iteration 606/1000 | Loss: 0.00002376
Iteration 607/1000 | Loss: 0.00002692
Iteration 608/1000 | Loss: 0.00002352
Iteration 609/1000 | Loss: 0.00002246
Iteration 610/1000 | Loss: 0.00002353
Iteration 611/1000 | Loss: 0.00003353
Iteration 612/1000 | Loss: 0.00002688
Iteration 613/1000 | Loss: 0.00003469
Iteration 614/1000 | Loss: 0.00003080
Iteration 615/1000 | Loss: 0.00002784
Iteration 616/1000 | Loss: 0.00002872
Iteration 617/1000 | Loss: 0.00003428
Iteration 618/1000 | Loss: 0.00003160
Iteration 619/1000 | Loss: 0.00002750
Iteration 620/1000 | Loss: 0.00002634
Iteration 621/1000 | Loss: 0.00002655
Iteration 622/1000 | Loss: 0.00002326
Iteration 623/1000 | Loss: 0.00002375
Iteration 624/1000 | Loss: 0.00002632
Iteration 625/1000 | Loss: 0.00002705
Iteration 626/1000 | Loss: 0.00003634
Iteration 627/1000 | Loss: 0.00003691
Iteration 628/1000 | Loss: 0.00004205
Iteration 629/1000 | Loss: 0.00003856
Iteration 630/1000 | Loss: 0.00004460
Iteration 631/1000 | Loss: 0.00004315
Iteration 632/1000 | Loss: 0.00003681
Iteration 633/1000 | Loss: 0.00004230
Iteration 634/1000 | Loss: 0.00005855
Iteration 635/1000 | Loss: 0.00004719
Iteration 636/1000 | Loss: 0.00003505
Iteration 637/1000 | Loss: 0.00002635
Iteration 638/1000 | Loss: 0.00002259
Iteration 639/1000 | Loss: 0.00002304
Iteration 640/1000 | Loss: 0.00002419
Iteration 641/1000 | Loss: 0.00002938
Iteration 642/1000 | Loss: 0.00003374
Iteration 643/1000 | Loss: 0.00004800
Iteration 644/1000 | Loss: 0.00004679
Iteration 645/1000 | Loss: 0.00004790
Iteration 646/1000 | Loss: 0.00004990
Iteration 647/1000 | Loss: 0.00004963
Iteration 648/1000 | Loss: 0.00002972
Iteration 649/1000 | Loss: 0.00004277
Iteration 650/1000 | Loss: 0.00004785
Iteration 651/1000 | Loss: 0.00003337
Iteration 652/1000 | Loss: 0.00002612
Iteration 653/1000 | Loss: 0.00002490
Iteration 654/1000 | Loss: 0.00003522
Iteration 655/1000 | Loss: 0.00003166
Iteration 656/1000 | Loss: 0.00005374
Iteration 657/1000 | Loss: 0.00005293
Iteration 658/1000 | Loss: 0.00006430
Iteration 659/1000 | Loss: 0.00005558
Iteration 660/1000 | Loss: 0.00005074
Iteration 661/1000 | Loss: 0.00004415
Iteration 662/1000 | Loss: 0.00004528
Iteration 663/1000 | Loss: 0.00004112
Iteration 664/1000 | Loss: 0.00006406
Iteration 665/1000 | Loss: 0.00005167
Iteration 666/1000 | Loss: 0.00006723
Iteration 667/1000 | Loss: 0.00005465
Iteration 668/1000 | Loss: 0.00006177
Iteration 669/1000 | Loss: 0.00004331
Iteration 670/1000 | Loss: 0.00005621
Iteration 671/1000 | Loss: 0.00005824
Iteration 672/1000 | Loss: 0.00007507
Iteration 673/1000 | Loss: 0.00003226
Iteration 674/1000 | Loss: 0.00003127
Iteration 675/1000 | Loss: 0.00004711
Iteration 676/1000 | Loss: 0.00004653
Iteration 677/1000 | Loss: 0.00004574
Iteration 678/1000 | Loss: 0.00004559
Iteration 679/1000 | Loss: 0.00003431
Iteration 680/1000 | Loss: 0.00004094
Iteration 681/1000 | Loss: 0.00005073
Iteration 682/1000 | Loss: 0.00004744
Iteration 683/1000 | Loss: 0.00005210
Iteration 684/1000 | Loss: 0.00005648
Iteration 685/1000 | Loss: 0.00004796
Iteration 686/1000 | Loss: 0.00006638
Iteration 687/1000 | Loss: 0.00005791
Iteration 688/1000 | Loss: 0.00006741
Iteration 689/1000 | Loss: 0.00002676
Iteration 690/1000 | Loss: 0.00002345
Iteration 691/1000 | Loss: 0.00002288
Iteration 692/1000 | Loss: 0.00002420
Iteration 693/1000 | Loss: 0.00003265
Iteration 694/1000 | Loss: 0.00003151
Iteration 695/1000 | Loss: 0.00002644
Iteration 696/1000 | Loss: 0.00002905
Iteration 697/1000 | Loss: 0.00003060
Iteration 698/1000 | Loss: 0.00004152
Iteration 699/1000 | Loss: 0.00003980
Iteration 700/1000 | Loss: 0.00005261
Iteration 701/1000 | Loss: 0.00004477
Iteration 702/1000 | Loss: 0.00005362
Iteration 703/1000 | Loss: 0.00003918
Iteration 704/1000 | Loss: 0.00004568
Iteration 705/1000 | Loss: 0.00004713
Iteration 706/1000 | Loss: 0.00003253
Iteration 707/1000 | Loss: 0.00003293
Iteration 708/1000 | Loss: 0.00003846
Iteration 709/1000 | Loss: 0.00004421
Iteration 710/1000 | Loss: 0.00004851
Iteration 711/1000 | Loss: 0.00005734
Iteration 712/1000 | Loss: 0.00002835
Iteration 713/1000 | Loss: 0.00003001
Iteration 714/1000 | Loss: 0.00003187
Iteration 715/1000 | Loss: 0.00003381
Iteration 716/1000 | Loss: 0.00002532
Iteration 717/1000 | Loss: 0.00002532
Iteration 718/1000 | Loss: 0.00002221
Iteration 719/1000 | Loss: 0.00002971
Iteration 720/1000 | Loss: 0.00002942
Iteration 721/1000 | Loss: 0.00002518
Iteration 722/1000 | Loss: 0.00003084
Iteration 723/1000 | Loss: 0.00002733
Iteration 724/1000 | Loss: 0.00002523
Iteration 725/1000 | Loss: 0.00004265
Iteration 726/1000 | Loss: 0.00003164
Iteration 727/1000 | Loss: 0.00003862
Iteration 728/1000 | Loss: 0.00003053
Iteration 729/1000 | Loss: 0.00004206
Iteration 730/1000 | Loss: 0.00003372
Iteration 731/1000 | Loss: 0.00005467
Iteration 732/1000 | Loss: 0.00004116
Iteration 733/1000 | Loss: 0.00006120
Iteration 734/1000 | Loss: 0.00004341
Iteration 735/1000 | Loss: 0.00006290
Iteration 736/1000 | Loss: 0.00003528
Iteration 737/1000 | Loss: 0.00004649
Iteration 738/1000 | Loss: 0.00004125
Iteration 739/1000 | Loss: 0.00004015
Iteration 740/1000 | Loss: 0.00003404
Iteration 741/1000 | Loss: 0.00005005
Iteration 742/1000 | Loss: 0.00003548
Iteration 743/1000 | Loss: 0.00002819
Iteration 744/1000 | Loss: 0.00002359
Iteration 745/1000 | Loss: 0.00002221
Iteration 746/1000 | Loss: 0.00002359
Iteration 747/1000 | Loss: 0.00002220
Iteration 748/1000 | Loss: 0.00003557
Iteration 749/1000 | Loss: 0.00002986
Iteration 750/1000 | Loss: 0.00003736
Iteration 751/1000 | Loss: 0.00003860
Iteration 752/1000 | Loss: 0.00005511
Iteration 753/1000 | Loss: 0.00004403
Iteration 754/1000 | Loss: 0.00002974
Iteration 755/1000 | Loss: 0.00002722
Iteration 756/1000 | Loss: 0.00002300
Iteration 757/1000 | Loss: 0.00002256
Iteration 758/1000 | Loss: 0.00002532
Iteration 759/1000 | Loss: 0.00002405
Iteration 760/1000 | Loss: 0.00003685
Iteration 761/1000 | Loss: 0.00002737
Iteration 762/1000 | Loss: 0.00002412
Iteration 763/1000 | Loss: 0.00002489
Iteration 764/1000 | Loss: 0.00002814
Iteration 765/1000 | Loss: 0.00002497
Iteration 766/1000 | Loss: 0.00002424
Iteration 767/1000 | Loss: 0.00004545
Iteration 768/1000 | Loss: 0.00003138
Iteration 769/1000 | Loss: 0.00003187
Iteration 770/1000 | Loss: 0.00003312
Iteration 771/1000 | Loss: 0.00002718
Iteration 772/1000 | Loss: 0.00004944
Iteration 773/1000 | Loss: 0.00003880
Iteration 774/1000 | Loss: 0.00004680
Iteration 775/1000 | Loss: 0.00003489
Iteration 776/1000 | Loss: 0.00002627
Iteration 777/1000 | Loss: 0.00002680
Iteration 778/1000 | Loss: 0.00002660
Iteration 779/1000 | Loss: 0.00002672
Iteration 780/1000 | Loss: 0.00002383
Iteration 781/1000 | Loss: 0.00002558
Iteration 782/1000 | Loss: 0.00002613
Iteration 783/1000 | Loss: 0.00003212
Iteration 784/1000 | Loss: 0.00002927
Iteration 785/1000 | Loss: 0.00002859
Iteration 786/1000 | Loss: 0.00002759
Iteration 787/1000 | Loss: 0.00003717
Iteration 788/1000 | Loss: 0.00003512
Iteration 789/1000 | Loss: 0.00003411
Iteration 790/1000 | Loss: 0.00003311
Iteration 791/1000 | Loss: 0.00003132
Iteration 792/1000 | Loss: 0.00002966
Iteration 793/1000 | Loss: 0.00002627
Iteration 794/1000 | Loss: 0.00003014
Iteration 795/1000 | Loss: 0.00003478
Iteration 796/1000 | Loss: 0.00003151
Iteration 797/1000 | Loss: 0.00003845
Iteration 798/1000 | Loss: 0.00003556
Iteration 799/1000 | Loss: 0.00004097
Iteration 800/1000 | Loss: 0.00003978
Iteration 801/1000 | Loss: 0.00003425
Iteration 802/1000 | Loss: 0.00002582
Iteration 803/1000 | Loss: 0.00002693
Iteration 804/1000 | Loss: 0.00003330
Iteration 805/1000 | Loss: 0.00002629
Iteration 806/1000 | Loss: 0.00002383
Iteration 807/1000 | Loss: 0.00002690
Iteration 808/1000 | Loss: 0.00002682
Iteration 809/1000 | Loss: 0.00002936
Iteration 810/1000 | Loss: 0.00002961
Iteration 811/1000 | Loss: 0.00003212
Iteration 812/1000 | Loss: 0.00003147
Iteration 813/1000 | Loss: 0.00003000
Iteration 814/1000 | Loss: 0.00002790
Iteration 815/1000 | Loss: 0.00003412
Iteration 816/1000 | Loss: 0.00003137
Iteration 817/1000 | Loss: 0.00002681
Iteration 818/1000 | Loss: 0.00003444
Iteration 819/1000 | Loss: 0.00004570
Iteration 820/1000 | Loss: 0.00004269
Iteration 821/1000 | Loss: 0.00003089
Iteration 822/1000 | Loss: 0.00002781
Iteration 823/1000 | Loss: 0.00003377
Iteration 824/1000 | Loss: 0.00004045
Iteration 825/1000 | Loss: 0.00003306
Iteration 826/1000 | Loss: 0.00004250
Iteration 827/1000 | Loss: 0.00003416
Iteration 828/1000 | Loss: 0.00003804
Iteration 829/1000 | Loss: 0.00003877
Iteration 830/1000 | Loss: 0.00003102
Iteration 831/1000 | Loss: 0.00003048
Iteration 832/1000 | Loss: 0.00004425
Iteration 833/1000 | Loss: 0.00004072
Iteration 834/1000 | Loss: 0.00005017
Iteration 835/1000 | Loss: 0.00003451
Iteration 836/1000 | Loss: 0.00004974
Iteration 837/1000 | Loss: 0.00004613
Iteration 838/1000 | Loss: 0.00005702
Iteration 839/1000 | Loss: 0.00002672
Iteration 840/1000 | Loss: 0.00002963
Iteration 841/1000 | Loss: 0.00002503
Iteration 842/1000 | Loss: 0.00002466
Iteration 843/1000 | Loss: 0.00003638
Iteration 844/1000 | Loss: 0.00003576
Iteration 845/1000 | Loss: 0.00004239
Iteration 846/1000 | Loss: 0.00004525
Iteration 847/1000 | Loss: 0.00003444
Iteration 848/1000 | Loss: 0.00003435
Iteration 849/1000 | Loss: 0.00002594
Iteration 850/1000 | Loss: 0.00002234
Iteration 851/1000 | Loss: 0.00002399
Iteration 852/1000 | Loss: 0.00002967
Iteration 853/1000 | Loss: 0.00003808
Iteration 854/1000 | Loss: 0.00004426
Iteration 855/1000 | Loss: 0.00004981
Iteration 856/1000 | Loss: 0.00005485
Iteration 857/1000 | Loss: 0.00005753
Iteration 858/1000 | Loss: 0.00004910
Iteration 859/1000 | Loss: 0.00004982
Iteration 860/1000 | Loss: 0.00004780
Iteration 861/1000 | Loss: 0.00005428
Iteration 862/1000 | Loss: 0.00004753
Iteration 863/1000 | Loss: 0.00005580
Iteration 864/1000 | Loss: 0.00005354
Iteration 865/1000 | Loss: 0.00004969
Iteration 866/1000 | Loss: 0.00004405
Iteration 867/1000 | Loss: 0.00004989
Iteration 868/1000 | Loss: 0.00003719
Iteration 869/1000 | Loss: 0.00003395
Iteration 870/1000 | Loss: 0.00004806
Iteration 871/1000 | Loss: 0.00004843
Iteration 872/1000 | Loss: 0.00006218
Iteration 873/1000 | Loss: 0.00006185
Iteration 874/1000 | Loss: 0.00004836
Iteration 875/1000 | Loss: 0.00005246
Iteration 876/1000 | Loss: 0.00006248
Iteration 877/1000 | Loss: 0.00004039
Iteration 878/1000 | Loss: 0.00003674
Iteration 879/1000 | Loss: 0.00003069
Iteration 880/1000 | Loss: 0.00005070
Iteration 881/1000 | Loss: 0.00004988
Iteration 882/1000 | Loss: 0.00005155
Iteration 883/1000 | Loss: 0.00005710
Iteration 884/1000 | Loss: 0.00006258
Iteration 885/1000 | Loss: 0.00004536
Iteration 886/1000 | Loss: 0.00004318
Iteration 887/1000 | Loss: 0.00004032
Iteration 888/1000 | Loss: 0.00002666
Iteration 889/1000 | Loss: 0.00002578
Iteration 890/1000 | Loss: 0.00003355
Iteration 891/1000 | Loss: 0.00003475
Iteration 892/1000 | Loss: 0.00007193
Iteration 893/1000 | Loss: 0.00004356
Iteration 894/1000 | Loss: 0.00006234
Iteration 895/1000 | Loss: 0.00005045
Iteration 896/1000 | Loss: 0.00006874
Iteration 897/1000 | Loss: 0.00003598
Iteration 898/1000 | Loss: 0.00002582
Iteration 899/1000 | Loss: 0.00002352
Iteration 900/1000 | Loss: 0.00004166
Iteration 901/1000 | Loss: 0.00003593
Iteration 902/1000 | Loss: 0.00005066
Iteration 903/1000 | Loss: 0.00005488
Iteration 904/1000 | Loss: 0.00007160
Iteration 905/1000 | Loss: 0.00005595
Iteration 906/1000 | Loss: 0.00005084
Iteration 907/1000 | Loss: 0.00005572
Iteration 908/1000 | Loss: 0.00003788
Iteration 909/1000 | Loss: 0.00002862
Iteration 910/1000 | Loss: 0.00005547
Iteration 911/1000 | Loss: 0.00004924
Iteration 912/1000 | Loss: 0.00006073
Iteration 913/1000 | Loss: 0.00005699
Iteration 914/1000 | Loss: 0.00006383
Iteration 915/1000 | Loss: 0.00005336
Iteration 916/1000 | Loss: 0.00006214
Iteration 917/1000 | Loss: 0.00006142
Iteration 918/1000 | Loss: 0.00003383
Iteration 919/1000 | Loss: 0.00002502
Iteration 920/1000 | Loss: 0.00002805
Iteration 921/1000 | Loss: 0.00002531
Iteration 922/1000 | Loss: 0.00002470
Iteration 923/1000 | Loss: 0.00004822
Iteration 924/1000 | Loss: 0.00002890
Iteration 925/1000 | Loss: 0.00003146
Iteration 926/1000 | Loss: 0.00003205
Iteration 927/1000 | Loss: 0.00002687
Iteration 928/1000 | Loss: 0.00002923
Iteration 929/1000 | Loss: 0.00002807
Iteration 930/1000 | Loss: 0.00003410
Iteration 931/1000 | Loss: 0.00003420
Iteration 932/1000 | Loss: 0.00004061
Iteration 933/1000 | Loss: 0.00004049
Iteration 934/1000 | Loss: 0.00002928
Iteration 935/1000 | Loss: 0.00002717
Iteration 936/1000 | Loss: 0.00003656
Iteration 937/1000 | Loss: 0.00004229
Iteration 938/1000 | Loss: 0.00004205
Iteration 939/1000 | Loss: 0.00004654
Iteration 940/1000 | Loss: 0.00004973
Iteration 941/1000 | Loss: 0.00005402
Iteration 942/1000 | Loss: 0.00004156
Iteration 943/1000 | Loss: 0.00003282
Iteration 944/1000 | Loss: 0.00004037
Iteration 945/1000 | Loss: 0.00004514
Iteration 946/1000 | Loss: 0.00005395
Iteration 947/1000 | Loss: 0.00005435
Iteration 948/1000 | Loss: 0.00004181
Iteration 949/1000 | Loss: 0.00004613
Iteration 950/1000 | Loss: 0.00005714
Iteration 951/1000 | Loss: 0.00005264
Iteration 952/1000 | Loss: 0.00002905
Iteration 953/1000 | Loss: 0.00002574
Iteration 954/1000 | Loss: 0.00005177
Iteration 955/1000 | Loss: 0.00005874
Iteration 956/1000 | Loss: 0.00006062
Iteration 957/1000 | Loss: 0.00006150
Iteration 958/1000 | Loss: 0.00005530
Iteration 959/1000 | Loss: 0.00005132
Iteration 960/1000 | Loss: 0.00006461
Iteration 961/1000 | Loss: 0.00005331
Iteration 962/1000 | Loss: 0.00005990
Iteration 963/1000 | Loss: 0.00006005
Iteration 964/1000 | Loss: 0.00006010
Iteration 965/1000 | Loss: 0.00005628
Iteration 966/1000 | Loss: 0.00007123
Iteration 967/1000 | Loss: 0.00006772
Iteration 968/1000 | Loss: 0.00007056
Iteration 969/1000 | Loss: 0.00006965
Iteration 970/1000 | Loss: 0.00005893
Iteration 971/1000 | Loss: 0.00006677
Iteration 972/1000 | Loss: 0.00007277
Iteration 973/1000 | Loss: 0.00006612
Iteration 974/1000 | Loss: 0.00007897
Iteration 975/1000 | Loss: 0.00009688
Iteration 976/1000 | Loss: 0.00006067
Iteration 977/1000 | Loss: 0.00007088
Iteration 978/1000 | Loss: 0.00004794
Iteration 979/1000 | Loss: 0.00002688
Iteration 980/1000 | Loss: 0.00002518
Iteration 981/1000 | Loss: 0.00002252
Iteration 982/1000 | Loss: 0.00002236
Iteration 983/1000 | Loss: 0.00002470
Iteration 984/1000 | Loss: 0.00003860
Iteration 985/1000 | Loss: 0.00002739
Iteration 986/1000 | Loss: 0.00002494
Iteration 987/1000 | Loss: 0.00002269
Iteration 988/1000 | Loss: 0.00002222
Iteration 989/1000 | Loss: 0.00003168
Iteration 990/1000 | Loss: 0.00002703
Iteration 991/1000 | Loss: 0.00004180
Iteration 992/1000 | Loss: 0.00003320
Iteration 993/1000 | Loss: 0.00004137
Iteration 994/1000 | Loss: 0.00003185
Iteration 995/1000 | Loss: 0.00006583
Iteration 996/1000 | Loss: 0.00004273
Iteration 997/1000 | Loss: 0.00002893
Iteration 998/1000 | Loss: 0.00002821
Iteration 999/1000 | Loss: 0.00002785
Iteration 1000/1000 | Loss: 0.00002580

Optimization complete. Final v2v error: 3.839812755584717 mm

Highest mean error: 8.311746597290039 mm for frame 134

Lowest mean error: 2.7244760990142822 mm for frame 45

Saving results

Total time: 1450.6521899700165
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00814878
Iteration 2/25 | Loss: 0.00156925
Iteration 3/25 | Loss: 0.00104406
Iteration 4/25 | Loss: 0.00098999
Iteration 5/25 | Loss: 0.00097589
Iteration 6/25 | Loss: 0.00097213
Iteration 7/25 | Loss: 0.00097136
Iteration 8/25 | Loss: 0.00097136
Iteration 9/25 | Loss: 0.00097136
Iteration 10/25 | Loss: 0.00097136
Iteration 11/25 | Loss: 0.00097136
Iteration 12/25 | Loss: 0.00097136
Iteration 13/25 | Loss: 0.00097136
Iteration 14/25 | Loss: 0.00097136
Iteration 15/25 | Loss: 0.00097136
Iteration 16/25 | Loss: 0.00097136
Iteration 17/25 | Loss: 0.00097136
Iteration 18/25 | Loss: 0.00097136
Iteration 19/25 | Loss: 0.00097136
Iteration 20/25 | Loss: 0.00097136
Iteration 21/25 | Loss: 0.00097136
Iteration 22/25 | Loss: 0.00097136
Iteration 23/25 | Loss: 0.00097136
Iteration 24/25 | Loss: 0.00097136
Iteration 25/25 | Loss: 0.00097136

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.28552219
Iteration 2/25 | Loss: 0.00047619
Iteration 3/25 | Loss: 0.00047619
Iteration 4/25 | Loss: 0.00047619
Iteration 5/25 | Loss: 0.00047619
Iteration 6/25 | Loss: 0.00047619
Iteration 7/25 | Loss: 0.00047619
Iteration 8/25 | Loss: 0.00047619
Iteration 9/25 | Loss: 0.00047619
Iteration 10/25 | Loss: 0.00047619
Iteration 11/25 | Loss: 0.00047619
Iteration 12/25 | Loss: 0.00047619
Iteration 13/25 | Loss: 0.00047619
Iteration 14/25 | Loss: 0.00047619
Iteration 15/25 | Loss: 0.00047619
Iteration 16/25 | Loss: 0.00047619
Iteration 17/25 | Loss: 0.00047619
Iteration 18/25 | Loss: 0.00047619
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0004761888412758708, 0.0004761888412758708, 0.0004761888412758708, 0.0004761888412758708, 0.0004761888412758708]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004761888412758708

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047619
Iteration 2/1000 | Loss: 0.00007180
Iteration 3/1000 | Loss: 0.00005055
Iteration 4/1000 | Loss: 0.00004373
Iteration 5/1000 | Loss: 0.00004084
Iteration 6/1000 | Loss: 0.00003885
Iteration 7/1000 | Loss: 0.00003744
Iteration 8/1000 | Loss: 0.00003625
Iteration 9/1000 | Loss: 0.00003550
Iteration 10/1000 | Loss: 0.00003497
Iteration 11/1000 | Loss: 0.00003454
Iteration 12/1000 | Loss: 0.00003417
Iteration 13/1000 | Loss: 0.00003385
Iteration 14/1000 | Loss: 0.00003365
Iteration 15/1000 | Loss: 0.00003346
Iteration 16/1000 | Loss: 0.00003333
Iteration 17/1000 | Loss: 0.00003326
Iteration 18/1000 | Loss: 0.00003309
Iteration 19/1000 | Loss: 0.00003291
Iteration 20/1000 | Loss: 0.00003279
Iteration 21/1000 | Loss: 0.00003278
Iteration 22/1000 | Loss: 0.00003277
Iteration 23/1000 | Loss: 0.00003273
Iteration 24/1000 | Loss: 0.00003270
Iteration 25/1000 | Loss: 0.00003267
Iteration 26/1000 | Loss: 0.00003261
Iteration 27/1000 | Loss: 0.00003261
Iteration 28/1000 | Loss: 0.00003260
Iteration 29/1000 | Loss: 0.00003259
Iteration 30/1000 | Loss: 0.00003259
Iteration 31/1000 | Loss: 0.00003256
Iteration 32/1000 | Loss: 0.00003256
Iteration 33/1000 | Loss: 0.00003254
Iteration 34/1000 | Loss: 0.00003253
Iteration 35/1000 | Loss: 0.00003253
Iteration 36/1000 | Loss: 0.00003252
Iteration 37/1000 | Loss: 0.00003251
Iteration 38/1000 | Loss: 0.00003250
Iteration 39/1000 | Loss: 0.00003249
Iteration 40/1000 | Loss: 0.00003249
Iteration 41/1000 | Loss: 0.00003249
Iteration 42/1000 | Loss: 0.00003248
Iteration 43/1000 | Loss: 0.00003247
Iteration 44/1000 | Loss: 0.00003247
Iteration 45/1000 | Loss: 0.00003247
Iteration 46/1000 | Loss: 0.00003247
Iteration 47/1000 | Loss: 0.00003247
Iteration 48/1000 | Loss: 0.00003247
Iteration 49/1000 | Loss: 0.00003247
Iteration 50/1000 | Loss: 0.00003247
Iteration 51/1000 | Loss: 0.00003247
Iteration 52/1000 | Loss: 0.00003246
Iteration 53/1000 | Loss: 0.00003246
Iteration 54/1000 | Loss: 0.00003246
Iteration 55/1000 | Loss: 0.00003245
Iteration 56/1000 | Loss: 0.00003245
Iteration 57/1000 | Loss: 0.00003245
Iteration 58/1000 | Loss: 0.00003244
Iteration 59/1000 | Loss: 0.00003244
Iteration 60/1000 | Loss: 0.00003244
Iteration 61/1000 | Loss: 0.00003244
Iteration 62/1000 | Loss: 0.00003244
Iteration 63/1000 | Loss: 0.00003243
Iteration 64/1000 | Loss: 0.00003243
Iteration 65/1000 | Loss: 0.00003243
Iteration 66/1000 | Loss: 0.00003243
Iteration 67/1000 | Loss: 0.00003242
Iteration 68/1000 | Loss: 0.00003242
Iteration 69/1000 | Loss: 0.00003242
Iteration 70/1000 | Loss: 0.00003241
Iteration 71/1000 | Loss: 0.00003241
Iteration 72/1000 | Loss: 0.00003241
Iteration 73/1000 | Loss: 0.00003240
Iteration 74/1000 | Loss: 0.00003240
Iteration 75/1000 | Loss: 0.00003240
Iteration 76/1000 | Loss: 0.00003240
Iteration 77/1000 | Loss: 0.00003239
Iteration 78/1000 | Loss: 0.00003239
Iteration 79/1000 | Loss: 0.00003239
Iteration 80/1000 | Loss: 0.00003239
Iteration 81/1000 | Loss: 0.00003239
Iteration 82/1000 | Loss: 0.00003238
Iteration 83/1000 | Loss: 0.00003238
Iteration 84/1000 | Loss: 0.00003238
Iteration 85/1000 | Loss: 0.00003238
Iteration 86/1000 | Loss: 0.00003238
Iteration 87/1000 | Loss: 0.00003238
Iteration 88/1000 | Loss: 0.00003238
Iteration 89/1000 | Loss: 0.00003238
Iteration 90/1000 | Loss: 0.00003237
Iteration 91/1000 | Loss: 0.00003237
Iteration 92/1000 | Loss: 0.00003237
Iteration 93/1000 | Loss: 0.00003237
Iteration 94/1000 | Loss: 0.00003237
Iteration 95/1000 | Loss: 0.00003236
Iteration 96/1000 | Loss: 0.00003236
Iteration 97/1000 | Loss: 0.00003236
Iteration 98/1000 | Loss: 0.00003235
Iteration 99/1000 | Loss: 0.00003235
Iteration 100/1000 | Loss: 0.00003235
Iteration 101/1000 | Loss: 0.00003235
Iteration 102/1000 | Loss: 0.00003235
Iteration 103/1000 | Loss: 0.00003234
Iteration 104/1000 | Loss: 0.00003234
Iteration 105/1000 | Loss: 0.00003234
Iteration 106/1000 | Loss: 0.00003234
Iteration 107/1000 | Loss: 0.00003234
Iteration 108/1000 | Loss: 0.00003234
Iteration 109/1000 | Loss: 0.00003234
Iteration 110/1000 | Loss: 0.00003234
Iteration 111/1000 | Loss: 0.00003234
Iteration 112/1000 | Loss: 0.00003233
Iteration 113/1000 | Loss: 0.00003233
Iteration 114/1000 | Loss: 0.00003233
Iteration 115/1000 | Loss: 0.00003233
Iteration 116/1000 | Loss: 0.00003233
Iteration 117/1000 | Loss: 0.00003232
Iteration 118/1000 | Loss: 0.00003232
Iteration 119/1000 | Loss: 0.00003232
Iteration 120/1000 | Loss: 0.00003232
Iteration 121/1000 | Loss: 0.00003232
Iteration 122/1000 | Loss: 0.00003232
Iteration 123/1000 | Loss: 0.00003232
Iteration 124/1000 | Loss: 0.00003232
Iteration 125/1000 | Loss: 0.00003231
Iteration 126/1000 | Loss: 0.00003231
Iteration 127/1000 | Loss: 0.00003231
Iteration 128/1000 | Loss: 0.00003231
Iteration 129/1000 | Loss: 0.00003231
Iteration 130/1000 | Loss: 0.00003230
Iteration 131/1000 | Loss: 0.00003230
Iteration 132/1000 | Loss: 0.00003230
Iteration 133/1000 | Loss: 0.00003230
Iteration 134/1000 | Loss: 0.00003230
Iteration 135/1000 | Loss: 0.00003229
Iteration 136/1000 | Loss: 0.00003229
Iteration 137/1000 | Loss: 0.00003229
Iteration 138/1000 | Loss: 0.00003229
Iteration 139/1000 | Loss: 0.00003229
Iteration 140/1000 | Loss: 0.00003229
Iteration 141/1000 | Loss: 0.00003229
Iteration 142/1000 | Loss: 0.00003228
Iteration 143/1000 | Loss: 0.00003228
Iteration 144/1000 | Loss: 0.00003228
Iteration 145/1000 | Loss: 0.00003228
Iteration 146/1000 | Loss: 0.00003228
Iteration 147/1000 | Loss: 0.00003228
Iteration 148/1000 | Loss: 0.00003228
Iteration 149/1000 | Loss: 0.00003227
Iteration 150/1000 | Loss: 0.00003227
Iteration 151/1000 | Loss: 0.00003227
Iteration 152/1000 | Loss: 0.00003227
Iteration 153/1000 | Loss: 0.00003226
Iteration 154/1000 | Loss: 0.00003226
Iteration 155/1000 | Loss: 0.00003226
Iteration 156/1000 | Loss: 0.00003226
Iteration 157/1000 | Loss: 0.00003226
Iteration 158/1000 | Loss: 0.00003226
Iteration 159/1000 | Loss: 0.00003226
Iteration 160/1000 | Loss: 0.00003226
Iteration 161/1000 | Loss: 0.00003225
Iteration 162/1000 | Loss: 0.00003225
Iteration 163/1000 | Loss: 0.00003225
Iteration 164/1000 | Loss: 0.00003225
Iteration 165/1000 | Loss: 0.00003225
Iteration 166/1000 | Loss: 0.00003225
Iteration 167/1000 | Loss: 0.00003225
Iteration 168/1000 | Loss: 0.00003225
Iteration 169/1000 | Loss: 0.00003225
Iteration 170/1000 | Loss: 0.00003225
Iteration 171/1000 | Loss: 0.00003225
Iteration 172/1000 | Loss: 0.00003225
Iteration 173/1000 | Loss: 0.00003225
Iteration 174/1000 | Loss: 0.00003225
Iteration 175/1000 | Loss: 0.00003225
Iteration 176/1000 | Loss: 0.00003225
Iteration 177/1000 | Loss: 0.00003225
Iteration 178/1000 | Loss: 0.00003225
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [3.224888860131614e-05, 3.224888860131614e-05, 3.224888860131614e-05, 3.224888860131614e-05, 3.224888860131614e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.224888860131614e-05

Optimization complete. Final v2v error: 4.4610419273376465 mm

Highest mean error: 6.040165901184082 mm for frame 152

Lowest mean error: 3.3219351768493652 mm for frame 16

Saving results

Total time: 51.813724756240845
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01041894
Iteration 2/25 | Loss: 0.00411279
Iteration 3/25 | Loss: 0.00220635
Iteration 4/25 | Loss: 0.00171431
Iteration 5/25 | Loss: 0.00143118
Iteration 6/25 | Loss: 0.00133745
Iteration 7/25 | Loss: 0.00122393
Iteration 8/25 | Loss: 0.00117461
Iteration 9/25 | Loss: 0.00105152
Iteration 10/25 | Loss: 0.00102048
Iteration 11/25 | Loss: 0.00098116
Iteration 12/25 | Loss: 0.00096741
Iteration 13/25 | Loss: 0.00095558
Iteration 14/25 | Loss: 0.00095127
Iteration 15/25 | Loss: 0.00095465
Iteration 16/25 | Loss: 0.00094169
Iteration 17/25 | Loss: 0.00094151
Iteration 18/25 | Loss: 0.00094007
Iteration 19/25 | Loss: 0.00093931
Iteration 20/25 | Loss: 0.00092960
Iteration 21/25 | Loss: 0.00092309
Iteration 22/25 | Loss: 0.00092289
Iteration 23/25 | Loss: 0.00092342
Iteration 24/25 | Loss: 0.00091887
Iteration 25/25 | Loss: 0.00091895

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49605370
Iteration 2/25 | Loss: 0.00218783
Iteration 3/25 | Loss: 0.00172480
Iteration 4/25 | Loss: 0.00172478
Iteration 5/25 | Loss: 0.00169230
Iteration 6/25 | Loss: 0.00169230
Iteration 7/25 | Loss: 0.00169230
Iteration 8/25 | Loss: 0.00169230
Iteration 9/25 | Loss: 0.00169230
Iteration 10/25 | Loss: 0.00169230
Iteration 11/25 | Loss: 0.00169230
Iteration 12/25 | Loss: 0.00169230
Iteration 13/25 | Loss: 0.00169230
Iteration 14/25 | Loss: 0.00169230
Iteration 15/25 | Loss: 0.00169230
Iteration 16/25 | Loss: 0.00169230
Iteration 17/25 | Loss: 0.00169230
Iteration 18/25 | Loss: 0.00169230
Iteration 19/25 | Loss: 0.00169230
Iteration 20/25 | Loss: 0.00169230
Iteration 21/25 | Loss: 0.00169230
Iteration 22/25 | Loss: 0.00169230
Iteration 23/25 | Loss: 0.00169230
Iteration 24/25 | Loss: 0.00169230
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0016922972863540053, 0.0016922972863540053, 0.0016922972863540053, 0.0016922972863540053, 0.0016922972863540053]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016922972863540053

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00169230
Iteration 2/1000 | Loss: 0.00098153
Iteration 3/1000 | Loss: 0.00065731
Iteration 4/1000 | Loss: 0.00020468
Iteration 5/1000 | Loss: 0.00020990
Iteration 6/1000 | Loss: 0.00017575
Iteration 7/1000 | Loss: 0.00023140
Iteration 8/1000 | Loss: 0.00012034
Iteration 9/1000 | Loss: 0.00011516
Iteration 10/1000 | Loss: 0.00021152
Iteration 11/1000 | Loss: 0.00012884
Iteration 12/1000 | Loss: 0.00026037
Iteration 13/1000 | Loss: 0.00010547
Iteration 14/1000 | Loss: 0.00025717
Iteration 15/1000 | Loss: 0.00031701
Iteration 16/1000 | Loss: 0.00081688
Iteration 17/1000 | Loss: 0.00537976
Iteration 18/1000 | Loss: 0.00210603
Iteration 19/1000 | Loss: 0.00022846
Iteration 20/1000 | Loss: 0.00033973
Iteration 21/1000 | Loss: 0.00014387
Iteration 22/1000 | Loss: 0.00045192
Iteration 23/1000 | Loss: 0.00007869
Iteration 24/1000 | Loss: 0.00020092
Iteration 25/1000 | Loss: 0.00005295
Iteration 26/1000 | Loss: 0.00024987
Iteration 27/1000 | Loss: 0.00005677
Iteration 28/1000 | Loss: 0.00020161
Iteration 29/1000 | Loss: 0.00005179
Iteration 30/1000 | Loss: 0.00003609
Iteration 31/1000 | Loss: 0.00002216
Iteration 32/1000 | Loss: 0.00005180
Iteration 33/1000 | Loss: 0.00001979
Iteration 34/1000 | Loss: 0.00003109
Iteration 35/1000 | Loss: 0.00003569
Iteration 36/1000 | Loss: 0.00007102
Iteration 37/1000 | Loss: 0.00001906
Iteration 38/1000 | Loss: 0.00002077
Iteration 39/1000 | Loss: 0.00005886
Iteration 40/1000 | Loss: 0.00001765
Iteration 41/1000 | Loss: 0.00001593
Iteration 42/1000 | Loss: 0.00001561
Iteration 43/1000 | Loss: 0.00002740
Iteration 44/1000 | Loss: 0.00003534
Iteration 45/1000 | Loss: 0.00002764
Iteration 46/1000 | Loss: 0.00001943
Iteration 47/1000 | Loss: 0.00003043
Iteration 48/1000 | Loss: 0.00002226
Iteration 49/1000 | Loss: 0.00004083
Iteration 50/1000 | Loss: 0.00003295
Iteration 51/1000 | Loss: 0.00003422
Iteration 52/1000 | Loss: 0.00004870
Iteration 53/1000 | Loss: 0.00004327
Iteration 54/1000 | Loss: 0.00003535
Iteration 55/1000 | Loss: 0.00003474
Iteration 56/1000 | Loss: 0.00003738
Iteration 57/1000 | Loss: 0.00003209
Iteration 58/1000 | Loss: 0.00003205
Iteration 59/1000 | Loss: 0.00003416
Iteration 60/1000 | Loss: 0.00001945
Iteration 61/1000 | Loss: 0.00004260
Iteration 62/1000 | Loss: 0.00001926
Iteration 63/1000 | Loss: 0.00001479
Iteration 64/1000 | Loss: 0.00001618
Iteration 65/1000 | Loss: 0.00001373
Iteration 66/1000 | Loss: 0.00001435
Iteration 67/1000 | Loss: 0.00001366
Iteration 68/1000 | Loss: 0.00001366
Iteration 69/1000 | Loss: 0.00001366
Iteration 70/1000 | Loss: 0.00001366
Iteration 71/1000 | Loss: 0.00001366
Iteration 72/1000 | Loss: 0.00001365
Iteration 73/1000 | Loss: 0.00001365
Iteration 74/1000 | Loss: 0.00001365
Iteration 75/1000 | Loss: 0.00001365
Iteration 76/1000 | Loss: 0.00001364
Iteration 77/1000 | Loss: 0.00001772
Iteration 78/1000 | Loss: 0.00001349
Iteration 79/1000 | Loss: 0.00002170
Iteration 80/1000 | Loss: 0.00001345
Iteration 81/1000 | Loss: 0.00001460
Iteration 82/1000 | Loss: 0.00001593
Iteration 83/1000 | Loss: 0.00001946
Iteration 84/1000 | Loss: 0.00001478
Iteration 85/1000 | Loss: 0.00001338
Iteration 86/1000 | Loss: 0.00001338
Iteration 87/1000 | Loss: 0.00001338
Iteration 88/1000 | Loss: 0.00001338
Iteration 89/1000 | Loss: 0.00001338
Iteration 90/1000 | Loss: 0.00001338
Iteration 91/1000 | Loss: 0.00001338
Iteration 92/1000 | Loss: 0.00001338
Iteration 93/1000 | Loss: 0.00001338
Iteration 94/1000 | Loss: 0.00001337
Iteration 95/1000 | Loss: 0.00001337
Iteration 96/1000 | Loss: 0.00001337
Iteration 97/1000 | Loss: 0.00001337
Iteration 98/1000 | Loss: 0.00001337
Iteration 99/1000 | Loss: 0.00001337
Iteration 100/1000 | Loss: 0.00001337
Iteration 101/1000 | Loss: 0.00001337
Iteration 102/1000 | Loss: 0.00001336
Iteration 103/1000 | Loss: 0.00001336
Iteration 104/1000 | Loss: 0.00001359
Iteration 105/1000 | Loss: 0.00001335
Iteration 106/1000 | Loss: 0.00001335
Iteration 107/1000 | Loss: 0.00001335
Iteration 108/1000 | Loss: 0.00001335
Iteration 109/1000 | Loss: 0.00001335
Iteration 110/1000 | Loss: 0.00001335
Iteration 111/1000 | Loss: 0.00001335
Iteration 112/1000 | Loss: 0.00001335
Iteration 113/1000 | Loss: 0.00001335
Iteration 114/1000 | Loss: 0.00001335
Iteration 115/1000 | Loss: 0.00001335
Iteration 116/1000 | Loss: 0.00001335
Iteration 117/1000 | Loss: 0.00001335
Iteration 118/1000 | Loss: 0.00001335
Iteration 119/1000 | Loss: 0.00001335
Iteration 120/1000 | Loss: 0.00001335
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [1.3350399967748672e-05, 1.3350399967748672e-05, 1.3350399967748672e-05, 1.3350399967748672e-05, 1.3350399967748672e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3350399967748672e-05

Optimization complete. Final v2v error: 3.078887462615967 mm

Highest mean error: 4.180413246154785 mm for frame 9

Lowest mean error: 2.7701432704925537 mm for frame 198

Saving results

Total time: 173.22057032585144
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00815949
Iteration 2/25 | Loss: 0.00135218
Iteration 3/25 | Loss: 0.00089856
Iteration 4/25 | Loss: 0.00082248
Iteration 5/25 | Loss: 0.00081095
Iteration 6/25 | Loss: 0.00080968
Iteration 7/25 | Loss: 0.00080968
Iteration 8/25 | Loss: 0.00080968
Iteration 9/25 | Loss: 0.00080968
Iteration 10/25 | Loss: 0.00080968
Iteration 11/25 | Loss: 0.00080968
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008096759556792676, 0.0008096759556792676, 0.0008096759556792676, 0.0008096759556792676, 0.0008096759556792676]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008096759556792676

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55307293
Iteration 2/25 | Loss: 0.00065194
Iteration 3/25 | Loss: 0.00065193
Iteration 4/25 | Loss: 0.00065193
Iteration 5/25 | Loss: 0.00065193
Iteration 6/25 | Loss: 0.00065193
Iteration 7/25 | Loss: 0.00065193
Iteration 8/25 | Loss: 0.00065193
Iteration 9/25 | Loss: 0.00065193
Iteration 10/25 | Loss: 0.00065193
Iteration 11/25 | Loss: 0.00065193
Iteration 12/25 | Loss: 0.00065193
Iteration 13/25 | Loss: 0.00065193
Iteration 14/25 | Loss: 0.00065193
Iteration 15/25 | Loss: 0.00065193
Iteration 16/25 | Loss: 0.00065193
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006519260350614786, 0.0006519260350614786, 0.0006519260350614786, 0.0006519260350614786, 0.0006519260350614786]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006519260350614786

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065193
Iteration 2/1000 | Loss: 0.00003478
Iteration 3/1000 | Loss: 0.00002356
Iteration 4/1000 | Loss: 0.00002178
Iteration 5/1000 | Loss: 0.00002081
Iteration 6/1000 | Loss: 0.00002003
Iteration 7/1000 | Loss: 0.00001944
Iteration 8/1000 | Loss: 0.00001887
Iteration 9/1000 | Loss: 0.00001855
Iteration 10/1000 | Loss: 0.00001854
Iteration 11/1000 | Loss: 0.00001837
Iteration 12/1000 | Loss: 0.00001832
Iteration 13/1000 | Loss: 0.00001828
Iteration 14/1000 | Loss: 0.00001827
Iteration 15/1000 | Loss: 0.00001827
Iteration 16/1000 | Loss: 0.00001820
Iteration 17/1000 | Loss: 0.00001820
Iteration 18/1000 | Loss: 0.00001819
Iteration 19/1000 | Loss: 0.00001819
Iteration 20/1000 | Loss: 0.00001818
Iteration 21/1000 | Loss: 0.00001817
Iteration 22/1000 | Loss: 0.00001817
Iteration 23/1000 | Loss: 0.00001816
Iteration 24/1000 | Loss: 0.00001816
Iteration 25/1000 | Loss: 0.00001816
Iteration 26/1000 | Loss: 0.00001816
Iteration 27/1000 | Loss: 0.00001816
Iteration 28/1000 | Loss: 0.00001816
Iteration 29/1000 | Loss: 0.00001816
Iteration 30/1000 | Loss: 0.00001816
Iteration 31/1000 | Loss: 0.00001816
Iteration 32/1000 | Loss: 0.00001816
Iteration 33/1000 | Loss: 0.00001815
Iteration 34/1000 | Loss: 0.00001815
Iteration 35/1000 | Loss: 0.00001815
Iteration 36/1000 | Loss: 0.00001814
Iteration 37/1000 | Loss: 0.00001813
Iteration 38/1000 | Loss: 0.00001813
Iteration 39/1000 | Loss: 0.00001813
Iteration 40/1000 | Loss: 0.00001813
Iteration 41/1000 | Loss: 0.00001813
Iteration 42/1000 | Loss: 0.00001813
Iteration 43/1000 | Loss: 0.00001813
Iteration 44/1000 | Loss: 0.00001813
Iteration 45/1000 | Loss: 0.00001812
Iteration 46/1000 | Loss: 0.00001812
Iteration 47/1000 | Loss: 0.00001812
Iteration 48/1000 | Loss: 0.00001812
Iteration 49/1000 | Loss: 0.00001812
Iteration 50/1000 | Loss: 0.00001812
Iteration 51/1000 | Loss: 0.00001812
Iteration 52/1000 | Loss: 0.00001812
Iteration 53/1000 | Loss: 0.00001812
Iteration 54/1000 | Loss: 0.00001812
Iteration 55/1000 | Loss: 0.00001812
Iteration 56/1000 | Loss: 0.00001812
Iteration 57/1000 | Loss: 0.00001812
Iteration 58/1000 | Loss: 0.00001812
Iteration 59/1000 | Loss: 0.00001812
Iteration 60/1000 | Loss: 0.00001812
Iteration 61/1000 | Loss: 0.00001812
Iteration 62/1000 | Loss: 0.00001812
Iteration 63/1000 | Loss: 0.00001811
Iteration 64/1000 | Loss: 0.00001811
Iteration 65/1000 | Loss: 0.00001810
Iteration 66/1000 | Loss: 0.00001810
Iteration 67/1000 | Loss: 0.00001810
Iteration 68/1000 | Loss: 0.00001810
Iteration 69/1000 | Loss: 0.00001810
Iteration 70/1000 | Loss: 0.00001810
Iteration 71/1000 | Loss: 0.00001810
Iteration 72/1000 | Loss: 0.00001809
Iteration 73/1000 | Loss: 0.00001809
Iteration 74/1000 | Loss: 0.00001809
Iteration 75/1000 | Loss: 0.00001809
Iteration 76/1000 | Loss: 0.00001809
Iteration 77/1000 | Loss: 0.00001809
Iteration 78/1000 | Loss: 0.00001809
Iteration 79/1000 | Loss: 0.00001808
Iteration 80/1000 | Loss: 0.00001808
Iteration 81/1000 | Loss: 0.00001808
Iteration 82/1000 | Loss: 0.00001808
Iteration 83/1000 | Loss: 0.00001808
Iteration 84/1000 | Loss: 0.00001808
Iteration 85/1000 | Loss: 0.00001807
Iteration 86/1000 | Loss: 0.00001807
Iteration 87/1000 | Loss: 0.00001807
Iteration 88/1000 | Loss: 0.00001807
Iteration 89/1000 | Loss: 0.00001807
Iteration 90/1000 | Loss: 0.00001807
Iteration 91/1000 | Loss: 0.00001807
Iteration 92/1000 | Loss: 0.00001807
Iteration 93/1000 | Loss: 0.00001806
Iteration 94/1000 | Loss: 0.00001806
Iteration 95/1000 | Loss: 0.00001806
Iteration 96/1000 | Loss: 0.00001806
Iteration 97/1000 | Loss: 0.00001806
Iteration 98/1000 | Loss: 0.00001806
Iteration 99/1000 | Loss: 0.00001806
Iteration 100/1000 | Loss: 0.00001806
Iteration 101/1000 | Loss: 0.00001806
Iteration 102/1000 | Loss: 0.00001806
Iteration 103/1000 | Loss: 0.00001806
Iteration 104/1000 | Loss: 0.00001806
Iteration 105/1000 | Loss: 0.00001806
Iteration 106/1000 | Loss: 0.00001806
Iteration 107/1000 | Loss: 0.00001806
Iteration 108/1000 | Loss: 0.00001806
Iteration 109/1000 | Loss: 0.00001805
Iteration 110/1000 | Loss: 0.00001805
Iteration 111/1000 | Loss: 0.00001805
Iteration 112/1000 | Loss: 0.00001805
Iteration 113/1000 | Loss: 0.00001805
Iteration 114/1000 | Loss: 0.00001805
Iteration 115/1000 | Loss: 0.00001805
Iteration 116/1000 | Loss: 0.00001805
Iteration 117/1000 | Loss: 0.00001805
Iteration 118/1000 | Loss: 0.00001805
Iteration 119/1000 | Loss: 0.00001805
Iteration 120/1000 | Loss: 0.00001805
Iteration 121/1000 | Loss: 0.00001805
Iteration 122/1000 | Loss: 0.00001805
Iteration 123/1000 | Loss: 0.00001805
Iteration 124/1000 | Loss: 0.00001805
Iteration 125/1000 | Loss: 0.00001805
Iteration 126/1000 | Loss: 0.00001805
Iteration 127/1000 | Loss: 0.00001804
Iteration 128/1000 | Loss: 0.00001804
Iteration 129/1000 | Loss: 0.00001804
Iteration 130/1000 | Loss: 0.00001804
Iteration 131/1000 | Loss: 0.00001804
Iteration 132/1000 | Loss: 0.00001804
Iteration 133/1000 | Loss: 0.00001804
Iteration 134/1000 | Loss: 0.00001804
Iteration 135/1000 | Loss: 0.00001804
Iteration 136/1000 | Loss: 0.00001804
Iteration 137/1000 | Loss: 0.00001804
Iteration 138/1000 | Loss: 0.00001803
Iteration 139/1000 | Loss: 0.00001803
Iteration 140/1000 | Loss: 0.00001803
Iteration 141/1000 | Loss: 0.00001803
Iteration 142/1000 | Loss: 0.00001803
Iteration 143/1000 | Loss: 0.00001803
Iteration 144/1000 | Loss: 0.00001803
Iteration 145/1000 | Loss: 0.00001803
Iteration 146/1000 | Loss: 0.00001803
Iteration 147/1000 | Loss: 0.00001803
Iteration 148/1000 | Loss: 0.00001803
Iteration 149/1000 | Loss: 0.00001803
Iteration 150/1000 | Loss: 0.00001803
Iteration 151/1000 | Loss: 0.00001803
Iteration 152/1000 | Loss: 0.00001802
Iteration 153/1000 | Loss: 0.00001802
Iteration 154/1000 | Loss: 0.00001802
Iteration 155/1000 | Loss: 0.00001802
Iteration 156/1000 | Loss: 0.00001802
Iteration 157/1000 | Loss: 0.00001802
Iteration 158/1000 | Loss: 0.00001802
Iteration 159/1000 | Loss: 0.00001802
Iteration 160/1000 | Loss: 0.00001802
Iteration 161/1000 | Loss: 0.00001802
Iteration 162/1000 | Loss: 0.00001802
Iteration 163/1000 | Loss: 0.00001802
Iteration 164/1000 | Loss: 0.00001802
Iteration 165/1000 | Loss: 0.00001802
Iteration 166/1000 | Loss: 0.00001802
Iteration 167/1000 | Loss: 0.00001802
Iteration 168/1000 | Loss: 0.00001802
Iteration 169/1000 | Loss: 0.00001802
Iteration 170/1000 | Loss: 0.00001802
Iteration 171/1000 | Loss: 0.00001802
Iteration 172/1000 | Loss: 0.00001802
Iteration 173/1000 | Loss: 0.00001802
Iteration 174/1000 | Loss: 0.00001802
Iteration 175/1000 | Loss: 0.00001802
Iteration 176/1000 | Loss: 0.00001802
Iteration 177/1000 | Loss: 0.00001802
Iteration 178/1000 | Loss: 0.00001802
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [1.8021593859884888e-05, 1.8021593859884888e-05, 1.8021593859884888e-05, 1.8021593859884888e-05, 1.8021593859884888e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8021593859884888e-05

Optimization complete. Final v2v error: 3.6104116439819336 mm

Highest mean error: 4.269402027130127 mm for frame 12

Lowest mean error: 3.2221827507019043 mm for frame 237

Saving results

Total time: 39.85424017906189
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00688390
Iteration 2/25 | Loss: 0.00096714
Iteration 3/25 | Loss: 0.00081248
Iteration 4/25 | Loss: 0.00077749
Iteration 5/25 | Loss: 0.00076277
Iteration 6/25 | Loss: 0.00076012
Iteration 7/25 | Loss: 0.00075940
Iteration 8/25 | Loss: 0.00075940
Iteration 9/25 | Loss: 0.00075940
Iteration 10/25 | Loss: 0.00075940
Iteration 11/25 | Loss: 0.00075940
Iteration 12/25 | Loss: 0.00075940
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007593969930894673, 0.0007593969930894673, 0.0007593969930894673, 0.0007593969930894673, 0.0007593969930894673]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007593969930894673

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.07099128
Iteration 2/25 | Loss: 0.00047464
Iteration 3/25 | Loss: 0.00047463
Iteration 4/25 | Loss: 0.00047463
Iteration 5/25 | Loss: 0.00047463
Iteration 6/25 | Loss: 0.00047463
Iteration 7/25 | Loss: 0.00047463
Iteration 8/25 | Loss: 0.00047463
Iteration 9/25 | Loss: 0.00047463
Iteration 10/25 | Loss: 0.00047463
Iteration 11/25 | Loss: 0.00047463
Iteration 12/25 | Loss: 0.00047463
Iteration 13/25 | Loss: 0.00047463
Iteration 14/25 | Loss: 0.00047463
Iteration 15/25 | Loss: 0.00047463
Iteration 16/25 | Loss: 0.00047463
Iteration 17/25 | Loss: 0.00047463
Iteration 18/25 | Loss: 0.00047463
Iteration 19/25 | Loss: 0.00047463
Iteration 20/25 | Loss: 0.00047463
Iteration 21/25 | Loss: 0.00047463
Iteration 22/25 | Loss: 0.00047463
Iteration 23/25 | Loss: 0.00047463
Iteration 24/25 | Loss: 0.00047463
Iteration 25/25 | Loss: 0.00047463

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047463
Iteration 2/1000 | Loss: 0.00002971
Iteration 3/1000 | Loss: 0.00001881
Iteration 4/1000 | Loss: 0.00001715
Iteration 5/1000 | Loss: 0.00001601
Iteration 6/1000 | Loss: 0.00001557
Iteration 7/1000 | Loss: 0.00001532
Iteration 8/1000 | Loss: 0.00001512
Iteration 9/1000 | Loss: 0.00001508
Iteration 10/1000 | Loss: 0.00001504
Iteration 11/1000 | Loss: 0.00001501
Iteration 12/1000 | Loss: 0.00001500
Iteration 13/1000 | Loss: 0.00001496
Iteration 14/1000 | Loss: 0.00001493
Iteration 15/1000 | Loss: 0.00001493
Iteration 16/1000 | Loss: 0.00001486
Iteration 17/1000 | Loss: 0.00001484
Iteration 18/1000 | Loss: 0.00001483
Iteration 19/1000 | Loss: 0.00001483
Iteration 20/1000 | Loss: 0.00001483
Iteration 21/1000 | Loss: 0.00001481
Iteration 22/1000 | Loss: 0.00001481
Iteration 23/1000 | Loss: 0.00001480
Iteration 24/1000 | Loss: 0.00001479
Iteration 25/1000 | Loss: 0.00001479
Iteration 26/1000 | Loss: 0.00001478
Iteration 27/1000 | Loss: 0.00001478
Iteration 28/1000 | Loss: 0.00001477
Iteration 29/1000 | Loss: 0.00001477
Iteration 30/1000 | Loss: 0.00001477
Iteration 31/1000 | Loss: 0.00001476
Iteration 32/1000 | Loss: 0.00001476
Iteration 33/1000 | Loss: 0.00001475
Iteration 34/1000 | Loss: 0.00001473
Iteration 35/1000 | Loss: 0.00001471
Iteration 36/1000 | Loss: 0.00001470
Iteration 37/1000 | Loss: 0.00001470
Iteration 38/1000 | Loss: 0.00001469
Iteration 39/1000 | Loss: 0.00001468
Iteration 40/1000 | Loss: 0.00001468
Iteration 41/1000 | Loss: 0.00001468
Iteration 42/1000 | Loss: 0.00001468
Iteration 43/1000 | Loss: 0.00001468
Iteration 44/1000 | Loss: 0.00001468
Iteration 45/1000 | Loss: 0.00001468
Iteration 46/1000 | Loss: 0.00001467
Iteration 47/1000 | Loss: 0.00001467
Iteration 48/1000 | Loss: 0.00001466
Iteration 49/1000 | Loss: 0.00001465
Iteration 50/1000 | Loss: 0.00001465
Iteration 51/1000 | Loss: 0.00001465
Iteration 52/1000 | Loss: 0.00001464
Iteration 53/1000 | Loss: 0.00001464
Iteration 54/1000 | Loss: 0.00001464
Iteration 55/1000 | Loss: 0.00001464
Iteration 56/1000 | Loss: 0.00001464
Iteration 57/1000 | Loss: 0.00001464
Iteration 58/1000 | Loss: 0.00001463
Iteration 59/1000 | Loss: 0.00001462
Iteration 60/1000 | Loss: 0.00001462
Iteration 61/1000 | Loss: 0.00001461
Iteration 62/1000 | Loss: 0.00001460
Iteration 63/1000 | Loss: 0.00001459
Iteration 64/1000 | Loss: 0.00001458
Iteration 65/1000 | Loss: 0.00001458
Iteration 66/1000 | Loss: 0.00001458
Iteration 67/1000 | Loss: 0.00001458
Iteration 68/1000 | Loss: 0.00001457
Iteration 69/1000 | Loss: 0.00001457
Iteration 70/1000 | Loss: 0.00001456
Iteration 71/1000 | Loss: 0.00001456
Iteration 72/1000 | Loss: 0.00001455
Iteration 73/1000 | Loss: 0.00001455
Iteration 74/1000 | Loss: 0.00001455
Iteration 75/1000 | Loss: 0.00001454
Iteration 76/1000 | Loss: 0.00001454
Iteration 77/1000 | Loss: 0.00001454
Iteration 78/1000 | Loss: 0.00001454
Iteration 79/1000 | Loss: 0.00001452
Iteration 80/1000 | Loss: 0.00001451
Iteration 81/1000 | Loss: 0.00001450
Iteration 82/1000 | Loss: 0.00001450
Iteration 83/1000 | Loss: 0.00001449
Iteration 84/1000 | Loss: 0.00001449
Iteration 85/1000 | Loss: 0.00001449
Iteration 86/1000 | Loss: 0.00001449
Iteration 87/1000 | Loss: 0.00001448
Iteration 88/1000 | Loss: 0.00001446
Iteration 89/1000 | Loss: 0.00001446
Iteration 90/1000 | Loss: 0.00001445
Iteration 91/1000 | Loss: 0.00001445
Iteration 92/1000 | Loss: 0.00001445
Iteration 93/1000 | Loss: 0.00001445
Iteration 94/1000 | Loss: 0.00001445
Iteration 95/1000 | Loss: 0.00001444
Iteration 96/1000 | Loss: 0.00001444
Iteration 97/1000 | Loss: 0.00001443
Iteration 98/1000 | Loss: 0.00001443
Iteration 99/1000 | Loss: 0.00001443
Iteration 100/1000 | Loss: 0.00001443
Iteration 101/1000 | Loss: 0.00001443
Iteration 102/1000 | Loss: 0.00001442
Iteration 103/1000 | Loss: 0.00001442
Iteration 104/1000 | Loss: 0.00001442
Iteration 105/1000 | Loss: 0.00001442
Iteration 106/1000 | Loss: 0.00001442
Iteration 107/1000 | Loss: 0.00001442
Iteration 108/1000 | Loss: 0.00001442
Iteration 109/1000 | Loss: 0.00001442
Iteration 110/1000 | Loss: 0.00001442
Iteration 111/1000 | Loss: 0.00001442
Iteration 112/1000 | Loss: 0.00001442
Iteration 113/1000 | Loss: 0.00001442
Iteration 114/1000 | Loss: 0.00001442
Iteration 115/1000 | Loss: 0.00001442
Iteration 116/1000 | Loss: 0.00001442
Iteration 117/1000 | Loss: 0.00001442
Iteration 118/1000 | Loss: 0.00001442
Iteration 119/1000 | Loss: 0.00001442
Iteration 120/1000 | Loss: 0.00001442
Iteration 121/1000 | Loss: 0.00001442
Iteration 122/1000 | Loss: 0.00001442
Iteration 123/1000 | Loss: 0.00001442
Iteration 124/1000 | Loss: 0.00001442
Iteration 125/1000 | Loss: 0.00001442
Iteration 126/1000 | Loss: 0.00001442
Iteration 127/1000 | Loss: 0.00001442
Iteration 128/1000 | Loss: 0.00001442
Iteration 129/1000 | Loss: 0.00001442
Iteration 130/1000 | Loss: 0.00001442
Iteration 131/1000 | Loss: 0.00001442
Iteration 132/1000 | Loss: 0.00001442
Iteration 133/1000 | Loss: 0.00001442
Iteration 134/1000 | Loss: 0.00001442
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.4424825167225208e-05, 1.4424825167225208e-05, 1.4424825167225208e-05, 1.4424825167225208e-05, 1.4424825167225208e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4424825167225208e-05

Optimization complete. Final v2v error: 3.2290735244750977 mm

Highest mean error: 3.4528722763061523 mm for frame 45

Lowest mean error: 2.995769739151001 mm for frame 18

Saving results

Total time: 35.49177026748657
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00834633
Iteration 2/25 | Loss: 0.00119315
Iteration 3/25 | Loss: 0.00082127
Iteration 4/25 | Loss: 0.00077208
Iteration 5/25 | Loss: 0.00076227
Iteration 6/25 | Loss: 0.00075901
Iteration 7/25 | Loss: 0.00075743
Iteration 8/25 | Loss: 0.00075688
Iteration 9/25 | Loss: 0.00075664
Iteration 10/25 | Loss: 0.00075645
Iteration 11/25 | Loss: 0.00075789
Iteration 12/25 | Loss: 0.00075944
Iteration 13/25 | Loss: 0.00075750
Iteration 14/25 | Loss: 0.00075786
Iteration 15/25 | Loss: 0.00075230
Iteration 16/25 | Loss: 0.00075005
Iteration 17/25 | Loss: 0.00074901
Iteration 18/25 | Loss: 0.00074859
Iteration 19/25 | Loss: 0.00074834
Iteration 20/25 | Loss: 0.00074825
Iteration 21/25 | Loss: 0.00074825
Iteration 22/25 | Loss: 0.00074824
Iteration 23/25 | Loss: 0.00074824
Iteration 24/25 | Loss: 0.00074824
Iteration 25/25 | Loss: 0.00074824

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50678742
Iteration 2/25 | Loss: 0.00047910
Iteration 3/25 | Loss: 0.00047910
Iteration 4/25 | Loss: 0.00047910
Iteration 5/25 | Loss: 0.00047910
Iteration 6/25 | Loss: 0.00047910
Iteration 7/25 | Loss: 0.00047909
Iteration 8/25 | Loss: 0.00047909
Iteration 9/25 | Loss: 0.00047909
Iteration 10/25 | Loss: 0.00047909
Iteration 11/25 | Loss: 0.00047909
Iteration 12/25 | Loss: 0.00047909
Iteration 13/25 | Loss: 0.00047909
Iteration 14/25 | Loss: 0.00047909
Iteration 15/25 | Loss: 0.00047909
Iteration 16/25 | Loss: 0.00047909
Iteration 17/25 | Loss: 0.00047909
Iteration 18/25 | Loss: 0.00047909
Iteration 19/25 | Loss: 0.00047909
Iteration 20/25 | Loss: 0.00047909
Iteration 21/25 | Loss: 0.00047909
Iteration 22/25 | Loss: 0.00047909
Iteration 23/25 | Loss: 0.00047909
Iteration 24/25 | Loss: 0.00047909
Iteration 25/25 | Loss: 0.00047909

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047909
Iteration 2/1000 | Loss: 0.00002167
Iteration 3/1000 | Loss: 0.00001484
Iteration 4/1000 | Loss: 0.00001375
Iteration 5/1000 | Loss: 0.00001304
Iteration 6/1000 | Loss: 0.00001267
Iteration 7/1000 | Loss: 0.00001234
Iteration 8/1000 | Loss: 0.00001214
Iteration 9/1000 | Loss: 0.00001213
Iteration 10/1000 | Loss: 0.00001209
Iteration 11/1000 | Loss: 0.00001207
Iteration 12/1000 | Loss: 0.00001205
Iteration 13/1000 | Loss: 0.00001196
Iteration 14/1000 | Loss: 0.00001194
Iteration 15/1000 | Loss: 0.00001188
Iteration 16/1000 | Loss: 0.00001179
Iteration 17/1000 | Loss: 0.00001173
Iteration 18/1000 | Loss: 0.00001171
Iteration 19/1000 | Loss: 0.00001170
Iteration 20/1000 | Loss: 0.00001170
Iteration 21/1000 | Loss: 0.00001170
Iteration 22/1000 | Loss: 0.00001170
Iteration 23/1000 | Loss: 0.00001170
Iteration 24/1000 | Loss: 0.00001169
Iteration 25/1000 | Loss: 0.00001169
Iteration 26/1000 | Loss: 0.00001169
Iteration 27/1000 | Loss: 0.00001169
Iteration 28/1000 | Loss: 0.00001168
Iteration 29/1000 | Loss: 0.00001168
Iteration 30/1000 | Loss: 0.00001168
Iteration 31/1000 | Loss: 0.00001167
Iteration 32/1000 | Loss: 0.00001167
Iteration 33/1000 | Loss: 0.00001166
Iteration 34/1000 | Loss: 0.00001166
Iteration 35/1000 | Loss: 0.00001163
Iteration 36/1000 | Loss: 0.00001162
Iteration 37/1000 | Loss: 0.00001162
Iteration 38/1000 | Loss: 0.00001162
Iteration 39/1000 | Loss: 0.00001162
Iteration 40/1000 | Loss: 0.00001162
Iteration 41/1000 | Loss: 0.00001162
Iteration 42/1000 | Loss: 0.00001162
Iteration 43/1000 | Loss: 0.00001162
Iteration 44/1000 | Loss: 0.00001162
Iteration 45/1000 | Loss: 0.00001162
Iteration 46/1000 | Loss: 0.00001162
Iteration 47/1000 | Loss: 0.00001162
Iteration 48/1000 | Loss: 0.00001162
Iteration 49/1000 | Loss: 0.00001162
Iteration 50/1000 | Loss: 0.00001162
Iteration 51/1000 | Loss: 0.00001162
Iteration 52/1000 | Loss: 0.00001162
Iteration 53/1000 | Loss: 0.00001162
Iteration 54/1000 | Loss: 0.00001162
Iteration 55/1000 | Loss: 0.00001162
Iteration 56/1000 | Loss: 0.00001162
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 56. Stopping optimization.
Last 5 losses: [1.1616586562013254e-05, 1.1616586562013254e-05, 1.1616586562013254e-05, 1.1616586562013254e-05, 1.1616586562013254e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1616586562013254e-05

Optimization complete. Final v2v error: 2.8864991664886475 mm

Highest mean error: 3.1553754806518555 mm for frame 123

Lowest mean error: 2.6653759479522705 mm for frame 7

Saving results

Total time: 60.47507095336914
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01001036
Iteration 2/25 | Loss: 0.00389010
Iteration 3/25 | Loss: 0.00278788
Iteration 4/25 | Loss: 0.00226851
Iteration 5/25 | Loss: 0.00215887
Iteration 6/25 | Loss: 0.00207800
Iteration 7/25 | Loss: 0.00203090
Iteration 8/25 | Loss: 0.00192513
Iteration 9/25 | Loss: 0.00189518
Iteration 10/25 | Loss: 0.00168309
Iteration 11/25 | Loss: 0.00160101
Iteration 12/25 | Loss: 0.00151494
Iteration 13/25 | Loss: 0.00146699
Iteration 14/25 | Loss: 0.00141299
Iteration 15/25 | Loss: 0.00139380
Iteration 16/25 | Loss: 0.00137195
Iteration 17/25 | Loss: 0.00136660
Iteration 18/25 | Loss: 0.00135644
Iteration 19/25 | Loss: 0.00133605
Iteration 20/25 | Loss: 0.00133208
Iteration 21/25 | Loss: 0.00134626
Iteration 22/25 | Loss: 0.00134279
Iteration 23/25 | Loss: 0.00132188
Iteration 24/25 | Loss: 0.00131751
Iteration 25/25 | Loss: 0.00130524

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.53390837
Iteration 2/25 | Loss: 0.00647428
Iteration 3/25 | Loss: 0.00553004
Iteration 4/25 | Loss: 0.00553004
Iteration 5/25 | Loss: 0.00553003
Iteration 6/25 | Loss: 0.00553003
Iteration 7/25 | Loss: 0.00553003
Iteration 8/25 | Loss: 0.00553003
Iteration 9/25 | Loss: 0.00553003
Iteration 10/25 | Loss: 0.00553003
Iteration 11/25 | Loss: 0.00553003
Iteration 12/25 | Loss: 0.00553003
Iteration 13/25 | Loss: 0.00553003
Iteration 14/25 | Loss: 0.00553003
Iteration 15/25 | Loss: 0.00553003
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.00553003279492259, 0.00553003279492259, 0.00553003279492259, 0.00553003279492259, 0.00553003279492259]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00553003279492259

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00553003
Iteration 2/1000 | Loss: 0.00284667
Iteration 3/1000 | Loss: 0.00350590
Iteration 4/1000 | Loss: 0.00934340
Iteration 5/1000 | Loss: 0.00907033
Iteration 6/1000 | Loss: 0.00117040
Iteration 7/1000 | Loss: 0.00176114
Iteration 8/1000 | Loss: 0.00352709
Iteration 9/1000 | Loss: 0.00121797
Iteration 10/1000 | Loss: 0.00433869
Iteration 11/1000 | Loss: 0.00286336
Iteration 12/1000 | Loss: 0.00075784
Iteration 13/1000 | Loss: 0.00267969
Iteration 14/1000 | Loss: 0.00113248
Iteration 15/1000 | Loss: 0.00275100
Iteration 16/1000 | Loss: 0.00201655
Iteration 17/1000 | Loss: 0.00033616
Iteration 18/1000 | Loss: 0.00235310
Iteration 19/1000 | Loss: 0.00066249
Iteration 20/1000 | Loss: 0.00046130
Iteration 21/1000 | Loss: 0.00051829
Iteration 22/1000 | Loss: 0.00219356
Iteration 23/1000 | Loss: 0.00123785
Iteration 24/1000 | Loss: 0.00206790
Iteration 25/1000 | Loss: 0.00466699
Iteration 26/1000 | Loss: 0.00989674
Iteration 27/1000 | Loss: 0.00702569
Iteration 28/1000 | Loss: 0.00695538
Iteration 29/1000 | Loss: 0.00492286
Iteration 30/1000 | Loss: 0.00266729
Iteration 31/1000 | Loss: 0.00123680
Iteration 32/1000 | Loss: 0.00088984
Iteration 33/1000 | Loss: 0.00043147
Iteration 34/1000 | Loss: 0.00045257
Iteration 35/1000 | Loss: 0.00056965
Iteration 36/1000 | Loss: 0.00105929
Iteration 37/1000 | Loss: 0.00053158
Iteration 38/1000 | Loss: 0.00073500
Iteration 39/1000 | Loss: 0.00162204
Iteration 40/1000 | Loss: 0.00115118
Iteration 41/1000 | Loss: 0.00215883
Iteration 42/1000 | Loss: 0.00149813
Iteration 43/1000 | Loss: 0.00046588
Iteration 44/1000 | Loss: 0.00022800
Iteration 45/1000 | Loss: 0.00058538
Iteration 46/1000 | Loss: 0.00163905
Iteration 47/1000 | Loss: 0.00008975
Iteration 48/1000 | Loss: 0.00008239
Iteration 49/1000 | Loss: 0.00096535
Iteration 50/1000 | Loss: 0.00008429
Iteration 51/1000 | Loss: 0.00043435
Iteration 52/1000 | Loss: 0.00005277
Iteration 53/1000 | Loss: 0.00031195
Iteration 54/1000 | Loss: 0.00014344
Iteration 55/1000 | Loss: 0.00050322
Iteration 56/1000 | Loss: 0.00041898
Iteration 57/1000 | Loss: 0.00007187
Iteration 58/1000 | Loss: 0.00023891
Iteration 59/1000 | Loss: 0.00004378
Iteration 60/1000 | Loss: 0.00005196
Iteration 61/1000 | Loss: 0.00003583
Iteration 62/1000 | Loss: 0.00016821
Iteration 63/1000 | Loss: 0.00004482
Iteration 64/1000 | Loss: 0.00003887
Iteration 65/1000 | Loss: 0.00005592
Iteration 66/1000 | Loss: 0.00062497
Iteration 67/1000 | Loss: 0.00034098
Iteration 68/1000 | Loss: 0.00003787
Iteration 69/1000 | Loss: 0.00047414
Iteration 70/1000 | Loss: 0.00043188
Iteration 71/1000 | Loss: 0.00004045
Iteration 72/1000 | Loss: 0.00007509
Iteration 73/1000 | Loss: 0.00053003
Iteration 74/1000 | Loss: 0.00034257
Iteration 75/1000 | Loss: 0.00003070
Iteration 76/1000 | Loss: 0.00002759
Iteration 77/1000 | Loss: 0.00002702
Iteration 78/1000 | Loss: 0.00003594
Iteration 79/1000 | Loss: 0.00002640
Iteration 80/1000 | Loss: 0.00045243
Iteration 81/1000 | Loss: 0.00023998
Iteration 82/1000 | Loss: 0.00004343
Iteration 83/1000 | Loss: 0.00002618
Iteration 84/1000 | Loss: 0.00060075
Iteration 85/1000 | Loss: 0.00005784
Iteration 86/1000 | Loss: 0.00004634
Iteration 87/1000 | Loss: 0.00048528
Iteration 88/1000 | Loss: 0.00008629
Iteration 89/1000 | Loss: 0.00002965
Iteration 90/1000 | Loss: 0.00002606
Iteration 91/1000 | Loss: 0.00052834
Iteration 92/1000 | Loss: 0.00006581
Iteration 93/1000 | Loss: 0.00004716
Iteration 94/1000 | Loss: 0.00002610
Iteration 95/1000 | Loss: 0.00002574
Iteration 96/1000 | Loss: 0.00002556
Iteration 97/1000 | Loss: 0.00002553
Iteration 98/1000 | Loss: 0.00002547
Iteration 99/1000 | Loss: 0.00002546
Iteration 100/1000 | Loss: 0.00002546
Iteration 101/1000 | Loss: 0.00002545
Iteration 102/1000 | Loss: 0.00002545
Iteration 103/1000 | Loss: 0.00002542
Iteration 104/1000 | Loss: 0.00053232
Iteration 105/1000 | Loss: 0.00007172
Iteration 106/1000 | Loss: 0.00002719
Iteration 107/1000 | Loss: 0.00003419
Iteration 108/1000 | Loss: 0.00067152
Iteration 109/1000 | Loss: 0.00006062
Iteration 110/1000 | Loss: 0.00004958
Iteration 111/1000 | Loss: 0.00002570
Iteration 112/1000 | Loss: 0.00052059
Iteration 113/1000 | Loss: 0.00040185
Iteration 114/1000 | Loss: 0.00014255
Iteration 115/1000 | Loss: 0.00019358
Iteration 116/1000 | Loss: 0.00072448
Iteration 117/1000 | Loss: 0.00047383
Iteration 118/1000 | Loss: 0.00039516
Iteration 119/1000 | Loss: 0.00054032
Iteration 120/1000 | Loss: 0.00042826
Iteration 121/1000 | Loss: 0.00035543
Iteration 122/1000 | Loss: 0.00003667
Iteration 123/1000 | Loss: 0.00004480
Iteration 124/1000 | Loss: 0.00022267
Iteration 125/1000 | Loss: 0.00055424
Iteration 126/1000 | Loss: 0.00016078
Iteration 127/1000 | Loss: 0.00015026
Iteration 128/1000 | Loss: 0.00019896
Iteration 129/1000 | Loss: 0.00040356
Iteration 130/1000 | Loss: 0.00042392
Iteration 131/1000 | Loss: 0.00015809
Iteration 132/1000 | Loss: 0.00014618
Iteration 133/1000 | Loss: 0.00003196
Iteration 134/1000 | Loss: 0.00004013
Iteration 135/1000 | Loss: 0.00009865
Iteration 136/1000 | Loss: 0.00003883
Iteration 137/1000 | Loss: 0.00002769
Iteration 138/1000 | Loss: 0.00037547
Iteration 139/1000 | Loss: 0.00031131
Iteration 140/1000 | Loss: 0.00038719
Iteration 141/1000 | Loss: 0.00028831
Iteration 142/1000 | Loss: 0.00152884
Iteration 143/1000 | Loss: 0.00045091
Iteration 144/1000 | Loss: 0.00029773
Iteration 145/1000 | Loss: 0.00020417
Iteration 146/1000 | Loss: 0.00055277
Iteration 147/1000 | Loss: 0.00009192
Iteration 148/1000 | Loss: 0.00005173
Iteration 149/1000 | Loss: 0.00023918
Iteration 150/1000 | Loss: 0.00015630
Iteration 151/1000 | Loss: 0.00021150
Iteration 152/1000 | Loss: 0.00033308
Iteration 153/1000 | Loss: 0.00019244
Iteration 154/1000 | Loss: 0.00040530
Iteration 155/1000 | Loss: 0.00055556
Iteration 156/1000 | Loss: 0.00048781
Iteration 157/1000 | Loss: 0.00021279
Iteration 158/1000 | Loss: 0.00024783
Iteration 159/1000 | Loss: 0.00071465
Iteration 160/1000 | Loss: 0.00007240
Iteration 161/1000 | Loss: 0.00069976
Iteration 162/1000 | Loss: 0.00118791
Iteration 163/1000 | Loss: 0.00074134
Iteration 164/1000 | Loss: 0.00010790
Iteration 165/1000 | Loss: 0.00006106
Iteration 166/1000 | Loss: 0.00009903
Iteration 167/1000 | Loss: 0.00010972
Iteration 168/1000 | Loss: 0.00003597
Iteration 169/1000 | Loss: 0.00002784
Iteration 170/1000 | Loss: 0.00042986
Iteration 171/1000 | Loss: 0.00035132
Iteration 172/1000 | Loss: 0.00029226
Iteration 173/1000 | Loss: 0.00013146
Iteration 174/1000 | Loss: 0.00003190
Iteration 175/1000 | Loss: 0.00002695
Iteration 176/1000 | Loss: 0.00002455
Iteration 177/1000 | Loss: 0.00003519
Iteration 178/1000 | Loss: 0.00006523
Iteration 179/1000 | Loss: 0.00002284
Iteration 180/1000 | Loss: 0.00003527
Iteration 181/1000 | Loss: 0.00010378
Iteration 182/1000 | Loss: 0.00002250
Iteration 183/1000 | Loss: 0.00002231
Iteration 184/1000 | Loss: 0.00003305
Iteration 185/1000 | Loss: 0.00003270
Iteration 186/1000 | Loss: 0.00002221
Iteration 187/1000 | Loss: 0.00002595
Iteration 188/1000 | Loss: 0.00002330
Iteration 189/1000 | Loss: 0.00002212
Iteration 190/1000 | Loss: 0.00002212
Iteration 191/1000 | Loss: 0.00002212
Iteration 192/1000 | Loss: 0.00002211
Iteration 193/1000 | Loss: 0.00002211
Iteration 194/1000 | Loss: 0.00002211
Iteration 195/1000 | Loss: 0.00002211
Iteration 196/1000 | Loss: 0.00002211
Iteration 197/1000 | Loss: 0.00002211
Iteration 198/1000 | Loss: 0.00002211
Iteration 199/1000 | Loss: 0.00002211
Iteration 200/1000 | Loss: 0.00002211
Iteration 201/1000 | Loss: 0.00002211
Iteration 202/1000 | Loss: 0.00002211
Iteration 203/1000 | Loss: 0.00002211
Iteration 204/1000 | Loss: 0.00002211
Iteration 205/1000 | Loss: 0.00002210
Iteration 206/1000 | Loss: 0.00002210
Iteration 207/1000 | Loss: 0.00002210
Iteration 208/1000 | Loss: 0.00002210
Iteration 209/1000 | Loss: 0.00002210
Iteration 210/1000 | Loss: 0.00002210
Iteration 211/1000 | Loss: 0.00002209
Iteration 212/1000 | Loss: 0.00002209
Iteration 213/1000 | Loss: 0.00002209
Iteration 214/1000 | Loss: 0.00002209
Iteration 215/1000 | Loss: 0.00002208
Iteration 216/1000 | Loss: 0.00002208
Iteration 217/1000 | Loss: 0.00002207
Iteration 218/1000 | Loss: 0.00002207
Iteration 219/1000 | Loss: 0.00002207
Iteration 220/1000 | Loss: 0.00002207
Iteration 221/1000 | Loss: 0.00002206
Iteration 222/1000 | Loss: 0.00002206
Iteration 223/1000 | Loss: 0.00002206
Iteration 224/1000 | Loss: 0.00002206
Iteration 225/1000 | Loss: 0.00002205
Iteration 226/1000 | Loss: 0.00002205
Iteration 227/1000 | Loss: 0.00002205
Iteration 228/1000 | Loss: 0.00002205
Iteration 229/1000 | Loss: 0.00002205
Iteration 230/1000 | Loss: 0.00002205
Iteration 231/1000 | Loss: 0.00002205
Iteration 232/1000 | Loss: 0.00002205
Iteration 233/1000 | Loss: 0.00002205
Iteration 234/1000 | Loss: 0.00002205
Iteration 235/1000 | Loss: 0.00002205
Iteration 236/1000 | Loss: 0.00002205
Iteration 237/1000 | Loss: 0.00002205
Iteration 238/1000 | Loss: 0.00002205
Iteration 239/1000 | Loss: 0.00002205
Iteration 240/1000 | Loss: 0.00002205
Iteration 241/1000 | Loss: 0.00002205
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 241. Stopping optimization.
Last 5 losses: [2.204840347985737e-05, 2.204840347985737e-05, 2.204840347985737e-05, 2.204840347985737e-05, 2.204840347985737e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.204840347985737e-05

Optimization complete. Final v2v error: 3.7847812175750732 mm

Highest mean error: 5.384613990783691 mm for frame 43

Lowest mean error: 2.982985734939575 mm for frame 152

Saving results

Total time: 318.2639629840851
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01087075
Iteration 2/25 | Loss: 0.00167598
Iteration 3/25 | Loss: 0.00097822
Iteration 4/25 | Loss: 0.00091887
Iteration 5/25 | Loss: 0.00089238
Iteration 6/25 | Loss: 0.00089206
Iteration 7/25 | Loss: 0.00085835
Iteration 8/25 | Loss: 0.00083960
Iteration 9/25 | Loss: 0.00083590
Iteration 10/25 | Loss: 0.00083553
Iteration 11/25 | Loss: 0.00083548
Iteration 12/25 | Loss: 0.00083548
Iteration 13/25 | Loss: 0.00083548
Iteration 14/25 | Loss: 0.00083548
Iteration 15/25 | Loss: 0.00083547
Iteration 16/25 | Loss: 0.00083547
Iteration 17/25 | Loss: 0.00083547
Iteration 18/25 | Loss: 0.00083547
Iteration 19/25 | Loss: 0.00083547
Iteration 20/25 | Loss: 0.00083547
Iteration 21/25 | Loss: 0.00083547
Iteration 22/25 | Loss: 0.00083547
Iteration 23/25 | Loss: 0.00083546
Iteration 24/25 | Loss: 0.00083546
Iteration 25/25 | Loss: 0.00083546

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48426437
Iteration 2/25 | Loss: 0.00053375
Iteration 3/25 | Loss: 0.00049789
Iteration 4/25 | Loss: 0.00049789
Iteration 5/25 | Loss: 0.00049789
Iteration 6/25 | Loss: 0.00049789
Iteration 7/25 | Loss: 0.00049789
Iteration 8/25 | Loss: 0.00049789
Iteration 9/25 | Loss: 0.00049789
Iteration 10/25 | Loss: 0.00049789
Iteration 11/25 | Loss: 0.00049789
Iteration 12/25 | Loss: 0.00049789
Iteration 13/25 | Loss: 0.00049789
Iteration 14/25 | Loss: 0.00049789
Iteration 15/25 | Loss: 0.00049789
Iteration 16/25 | Loss: 0.00049789
Iteration 17/25 | Loss: 0.00049789
Iteration 18/25 | Loss: 0.00049789
Iteration 19/25 | Loss: 0.00049789
Iteration 20/25 | Loss: 0.00049789
Iteration 21/25 | Loss: 0.00049789
Iteration 22/25 | Loss: 0.00049789
Iteration 23/25 | Loss: 0.00049789
Iteration 24/25 | Loss: 0.00049789
Iteration 25/25 | Loss: 0.00049789

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049789
Iteration 2/1000 | Loss: 0.00003318
Iteration 3/1000 | Loss: 0.00002361
Iteration 4/1000 | Loss: 0.00002143
Iteration 5/1000 | Loss: 0.00002026
Iteration 6/1000 | Loss: 0.00001971
Iteration 7/1000 | Loss: 0.00001920
Iteration 8/1000 | Loss: 0.00001882
Iteration 9/1000 | Loss: 0.00001857
Iteration 10/1000 | Loss: 0.00001833
Iteration 11/1000 | Loss: 0.00001828
Iteration 12/1000 | Loss: 0.00001828
Iteration 13/1000 | Loss: 0.00001819
Iteration 14/1000 | Loss: 0.00001807
Iteration 15/1000 | Loss: 0.00001801
Iteration 16/1000 | Loss: 0.00001800
Iteration 17/1000 | Loss: 0.00001799
Iteration 18/1000 | Loss: 0.00001798
Iteration 19/1000 | Loss: 0.00001797
Iteration 20/1000 | Loss: 0.00001794
Iteration 21/1000 | Loss: 0.00001794
Iteration 22/1000 | Loss: 0.00001794
Iteration 23/1000 | Loss: 0.00001794
Iteration 24/1000 | Loss: 0.00001793
Iteration 25/1000 | Loss: 0.00001793
Iteration 26/1000 | Loss: 0.00001793
Iteration 27/1000 | Loss: 0.00001792
Iteration 28/1000 | Loss: 0.00001788
Iteration 29/1000 | Loss: 0.00001787
Iteration 30/1000 | Loss: 0.00001787
Iteration 31/1000 | Loss: 0.00001783
Iteration 32/1000 | Loss: 0.00001782
Iteration 33/1000 | Loss: 0.00001781
Iteration 34/1000 | Loss: 0.00001781
Iteration 35/1000 | Loss: 0.00001780
Iteration 36/1000 | Loss: 0.00001780
Iteration 37/1000 | Loss: 0.00001779
Iteration 38/1000 | Loss: 0.00001779
Iteration 39/1000 | Loss: 0.00001778
Iteration 40/1000 | Loss: 0.00001777
Iteration 41/1000 | Loss: 0.00001776
Iteration 42/1000 | Loss: 0.00001776
Iteration 43/1000 | Loss: 0.00001772
Iteration 44/1000 | Loss: 0.00001772
Iteration 45/1000 | Loss: 0.00001771
Iteration 46/1000 | Loss: 0.00001771
Iteration 47/1000 | Loss: 0.00001769
Iteration 48/1000 | Loss: 0.00001769
Iteration 49/1000 | Loss: 0.00001768
Iteration 50/1000 | Loss: 0.00001768
Iteration 51/1000 | Loss: 0.00001768
Iteration 52/1000 | Loss: 0.00001766
Iteration 53/1000 | Loss: 0.00001766
Iteration 54/1000 | Loss: 0.00001766
Iteration 55/1000 | Loss: 0.00001765
Iteration 56/1000 | Loss: 0.00001765
Iteration 57/1000 | Loss: 0.00001765
Iteration 58/1000 | Loss: 0.00001765
Iteration 59/1000 | Loss: 0.00001765
Iteration 60/1000 | Loss: 0.00001764
Iteration 61/1000 | Loss: 0.00001764
Iteration 62/1000 | Loss: 0.00001764
Iteration 63/1000 | Loss: 0.00001763
Iteration 64/1000 | Loss: 0.00001762
Iteration 65/1000 | Loss: 0.00001762
Iteration 66/1000 | Loss: 0.00001762
Iteration 67/1000 | Loss: 0.00001762
Iteration 68/1000 | Loss: 0.00001762
Iteration 69/1000 | Loss: 0.00001762
Iteration 70/1000 | Loss: 0.00001761
Iteration 71/1000 | Loss: 0.00001761
Iteration 72/1000 | Loss: 0.00001761
Iteration 73/1000 | Loss: 0.00001761
Iteration 74/1000 | Loss: 0.00001761
Iteration 75/1000 | Loss: 0.00001761
Iteration 76/1000 | Loss: 0.00001761
Iteration 77/1000 | Loss: 0.00001761
Iteration 78/1000 | Loss: 0.00001760
Iteration 79/1000 | Loss: 0.00001760
Iteration 80/1000 | Loss: 0.00001760
Iteration 81/1000 | Loss: 0.00001759
Iteration 82/1000 | Loss: 0.00001759
Iteration 83/1000 | Loss: 0.00001759
Iteration 84/1000 | Loss: 0.00001758
Iteration 85/1000 | Loss: 0.00001758
Iteration 86/1000 | Loss: 0.00001758
Iteration 87/1000 | Loss: 0.00001758
Iteration 88/1000 | Loss: 0.00001758
Iteration 89/1000 | Loss: 0.00001758
Iteration 90/1000 | Loss: 0.00001758
Iteration 91/1000 | Loss: 0.00001758
Iteration 92/1000 | Loss: 0.00001758
Iteration 93/1000 | Loss: 0.00001757
Iteration 94/1000 | Loss: 0.00001757
Iteration 95/1000 | Loss: 0.00001757
Iteration 96/1000 | Loss: 0.00001757
Iteration 97/1000 | Loss: 0.00001757
Iteration 98/1000 | Loss: 0.00001757
Iteration 99/1000 | Loss: 0.00001757
Iteration 100/1000 | Loss: 0.00001756
Iteration 101/1000 | Loss: 0.00001756
Iteration 102/1000 | Loss: 0.00001756
Iteration 103/1000 | Loss: 0.00001756
Iteration 104/1000 | Loss: 0.00001756
Iteration 105/1000 | Loss: 0.00001756
Iteration 106/1000 | Loss: 0.00001756
Iteration 107/1000 | Loss: 0.00001756
Iteration 108/1000 | Loss: 0.00001756
Iteration 109/1000 | Loss: 0.00001756
Iteration 110/1000 | Loss: 0.00001756
Iteration 111/1000 | Loss: 0.00001756
Iteration 112/1000 | Loss: 0.00001756
Iteration 113/1000 | Loss: 0.00001756
Iteration 114/1000 | Loss: 0.00001755
Iteration 115/1000 | Loss: 0.00001755
Iteration 116/1000 | Loss: 0.00001755
Iteration 117/1000 | Loss: 0.00001755
Iteration 118/1000 | Loss: 0.00001755
Iteration 119/1000 | Loss: 0.00001755
Iteration 120/1000 | Loss: 0.00001755
Iteration 121/1000 | Loss: 0.00001755
Iteration 122/1000 | Loss: 0.00001755
Iteration 123/1000 | Loss: 0.00001755
Iteration 124/1000 | Loss: 0.00001755
Iteration 125/1000 | Loss: 0.00001755
Iteration 126/1000 | Loss: 0.00001754
Iteration 127/1000 | Loss: 0.00001754
Iteration 128/1000 | Loss: 0.00001754
Iteration 129/1000 | Loss: 0.00001754
Iteration 130/1000 | Loss: 0.00001754
Iteration 131/1000 | Loss: 0.00001754
Iteration 132/1000 | Loss: 0.00001754
Iteration 133/1000 | Loss: 0.00001754
Iteration 134/1000 | Loss: 0.00001754
Iteration 135/1000 | Loss: 0.00001754
Iteration 136/1000 | Loss: 0.00001754
Iteration 137/1000 | Loss: 0.00001754
Iteration 138/1000 | Loss: 0.00001754
Iteration 139/1000 | Loss: 0.00001754
Iteration 140/1000 | Loss: 0.00001754
Iteration 141/1000 | Loss: 0.00001754
Iteration 142/1000 | Loss: 0.00001754
Iteration 143/1000 | Loss: 0.00001754
Iteration 144/1000 | Loss: 0.00001754
Iteration 145/1000 | Loss: 0.00001754
Iteration 146/1000 | Loss: 0.00001754
Iteration 147/1000 | Loss: 0.00001754
Iteration 148/1000 | Loss: 0.00001754
Iteration 149/1000 | Loss: 0.00001754
Iteration 150/1000 | Loss: 0.00001754
Iteration 151/1000 | Loss: 0.00001754
Iteration 152/1000 | Loss: 0.00001754
Iteration 153/1000 | Loss: 0.00001754
Iteration 154/1000 | Loss: 0.00001754
Iteration 155/1000 | Loss: 0.00001754
Iteration 156/1000 | Loss: 0.00001754
Iteration 157/1000 | Loss: 0.00001754
Iteration 158/1000 | Loss: 0.00001754
Iteration 159/1000 | Loss: 0.00001754
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [1.7544818547321483e-05, 1.7544818547321483e-05, 1.7544818547321483e-05, 1.7544818547321483e-05, 1.7544818547321483e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7544818547321483e-05

Optimization complete. Final v2v error: 3.511866807937622 mm

Highest mean error: 9.1083402633667 mm for frame 53

Lowest mean error: 3.282418727874756 mm for frame 100

Saving results

Total time: 47.46382117271423
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00433031
Iteration 2/25 | Loss: 0.00089325
Iteration 3/25 | Loss: 0.00077162
Iteration 4/25 | Loss: 0.00074833
Iteration 5/25 | Loss: 0.00074259
Iteration 6/25 | Loss: 0.00074125
Iteration 7/25 | Loss: 0.00074093
Iteration 8/25 | Loss: 0.00074093
Iteration 9/25 | Loss: 0.00074093
Iteration 10/25 | Loss: 0.00074093
Iteration 11/25 | Loss: 0.00074093
Iteration 12/25 | Loss: 0.00074093
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007409325917251408, 0.0007409325917251408, 0.0007409325917251408, 0.0007409325917251408, 0.0007409325917251408]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007409325917251408

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.71231842
Iteration 2/25 | Loss: 0.00047708
Iteration 3/25 | Loss: 0.00047706
Iteration 4/25 | Loss: 0.00047706
Iteration 5/25 | Loss: 0.00047706
Iteration 6/25 | Loss: 0.00047706
Iteration 7/25 | Loss: 0.00047706
Iteration 8/25 | Loss: 0.00047706
Iteration 9/25 | Loss: 0.00047706
Iteration 10/25 | Loss: 0.00047706
Iteration 11/25 | Loss: 0.00047706
Iteration 12/25 | Loss: 0.00047706
Iteration 13/25 | Loss: 0.00047706
Iteration 14/25 | Loss: 0.00047706
Iteration 15/25 | Loss: 0.00047706
Iteration 16/25 | Loss: 0.00047706
Iteration 17/25 | Loss: 0.00047706
Iteration 18/25 | Loss: 0.00047706
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0004770563973579556, 0.0004770563973579556, 0.0004770563973579556, 0.0004770563973579556, 0.0004770563973579556]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004770563973579556

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047706
Iteration 2/1000 | Loss: 0.00003069
Iteration 3/1000 | Loss: 0.00001950
Iteration 4/1000 | Loss: 0.00001800
Iteration 5/1000 | Loss: 0.00001730
Iteration 6/1000 | Loss: 0.00001649
Iteration 7/1000 | Loss: 0.00001608
Iteration 8/1000 | Loss: 0.00001574
Iteration 9/1000 | Loss: 0.00001553
Iteration 10/1000 | Loss: 0.00001537
Iteration 11/1000 | Loss: 0.00001528
Iteration 12/1000 | Loss: 0.00001527
Iteration 13/1000 | Loss: 0.00001522
Iteration 14/1000 | Loss: 0.00001518
Iteration 15/1000 | Loss: 0.00001517
Iteration 16/1000 | Loss: 0.00001517
Iteration 17/1000 | Loss: 0.00001516
Iteration 18/1000 | Loss: 0.00001515
Iteration 19/1000 | Loss: 0.00001509
Iteration 20/1000 | Loss: 0.00001508
Iteration 21/1000 | Loss: 0.00001508
Iteration 22/1000 | Loss: 0.00001507
Iteration 23/1000 | Loss: 0.00001506
Iteration 24/1000 | Loss: 0.00001497
Iteration 25/1000 | Loss: 0.00001490
Iteration 26/1000 | Loss: 0.00001488
Iteration 27/1000 | Loss: 0.00001487
Iteration 28/1000 | Loss: 0.00001487
Iteration 29/1000 | Loss: 0.00001486
Iteration 30/1000 | Loss: 0.00001486
Iteration 31/1000 | Loss: 0.00001486
Iteration 32/1000 | Loss: 0.00001485
Iteration 33/1000 | Loss: 0.00001485
Iteration 34/1000 | Loss: 0.00001484
Iteration 35/1000 | Loss: 0.00001484
Iteration 36/1000 | Loss: 0.00001484
Iteration 37/1000 | Loss: 0.00001483
Iteration 38/1000 | Loss: 0.00001482
Iteration 39/1000 | Loss: 0.00001482
Iteration 40/1000 | Loss: 0.00001479
Iteration 41/1000 | Loss: 0.00001479
Iteration 42/1000 | Loss: 0.00001478
Iteration 43/1000 | Loss: 0.00001477
Iteration 44/1000 | Loss: 0.00001476
Iteration 45/1000 | Loss: 0.00001476
Iteration 46/1000 | Loss: 0.00001475
Iteration 47/1000 | Loss: 0.00001475
Iteration 48/1000 | Loss: 0.00001475
Iteration 49/1000 | Loss: 0.00001474
Iteration 50/1000 | Loss: 0.00001474
Iteration 51/1000 | Loss: 0.00001474
Iteration 52/1000 | Loss: 0.00001474
Iteration 53/1000 | Loss: 0.00001473
Iteration 54/1000 | Loss: 0.00001473
Iteration 55/1000 | Loss: 0.00001473
Iteration 56/1000 | Loss: 0.00001472
Iteration 57/1000 | Loss: 0.00001471
Iteration 58/1000 | Loss: 0.00001470
Iteration 59/1000 | Loss: 0.00001470
Iteration 60/1000 | Loss: 0.00001469
Iteration 61/1000 | Loss: 0.00001468
Iteration 62/1000 | Loss: 0.00001468
Iteration 63/1000 | Loss: 0.00001466
Iteration 64/1000 | Loss: 0.00001465
Iteration 65/1000 | Loss: 0.00001465
Iteration 66/1000 | Loss: 0.00001464
Iteration 67/1000 | Loss: 0.00001464
Iteration 68/1000 | Loss: 0.00001463
Iteration 69/1000 | Loss: 0.00001462
Iteration 70/1000 | Loss: 0.00001461
Iteration 71/1000 | Loss: 0.00001460
Iteration 72/1000 | Loss: 0.00001459
Iteration 73/1000 | Loss: 0.00001459
Iteration 74/1000 | Loss: 0.00001458
Iteration 75/1000 | Loss: 0.00001458
Iteration 76/1000 | Loss: 0.00001458
Iteration 77/1000 | Loss: 0.00001457
Iteration 78/1000 | Loss: 0.00001457
Iteration 79/1000 | Loss: 0.00001457
Iteration 80/1000 | Loss: 0.00001457
Iteration 81/1000 | Loss: 0.00001456
Iteration 82/1000 | Loss: 0.00001456
Iteration 83/1000 | Loss: 0.00001456
Iteration 84/1000 | Loss: 0.00001456
Iteration 85/1000 | Loss: 0.00001456
Iteration 86/1000 | Loss: 0.00001456
Iteration 87/1000 | Loss: 0.00001456
Iteration 88/1000 | Loss: 0.00001455
Iteration 89/1000 | Loss: 0.00001455
Iteration 90/1000 | Loss: 0.00001455
Iteration 91/1000 | Loss: 0.00001455
Iteration 92/1000 | Loss: 0.00001454
Iteration 93/1000 | Loss: 0.00001454
Iteration 94/1000 | Loss: 0.00001454
Iteration 95/1000 | Loss: 0.00001453
Iteration 96/1000 | Loss: 0.00001453
Iteration 97/1000 | Loss: 0.00001453
Iteration 98/1000 | Loss: 0.00001453
Iteration 99/1000 | Loss: 0.00001453
Iteration 100/1000 | Loss: 0.00001453
Iteration 101/1000 | Loss: 0.00001453
Iteration 102/1000 | Loss: 0.00001452
Iteration 103/1000 | Loss: 0.00001452
Iteration 104/1000 | Loss: 0.00001452
Iteration 105/1000 | Loss: 0.00001452
Iteration 106/1000 | Loss: 0.00001452
Iteration 107/1000 | Loss: 0.00001452
Iteration 108/1000 | Loss: 0.00001451
Iteration 109/1000 | Loss: 0.00001451
Iteration 110/1000 | Loss: 0.00001451
Iteration 111/1000 | Loss: 0.00001451
Iteration 112/1000 | Loss: 0.00001451
Iteration 113/1000 | Loss: 0.00001451
Iteration 114/1000 | Loss: 0.00001451
Iteration 115/1000 | Loss: 0.00001450
Iteration 116/1000 | Loss: 0.00001450
Iteration 117/1000 | Loss: 0.00001450
Iteration 118/1000 | Loss: 0.00001450
Iteration 119/1000 | Loss: 0.00001450
Iteration 120/1000 | Loss: 0.00001450
Iteration 121/1000 | Loss: 0.00001450
Iteration 122/1000 | Loss: 0.00001450
Iteration 123/1000 | Loss: 0.00001450
Iteration 124/1000 | Loss: 0.00001450
Iteration 125/1000 | Loss: 0.00001449
Iteration 126/1000 | Loss: 0.00001449
Iteration 127/1000 | Loss: 0.00001449
Iteration 128/1000 | Loss: 0.00001449
Iteration 129/1000 | Loss: 0.00001448
Iteration 130/1000 | Loss: 0.00001448
Iteration 131/1000 | Loss: 0.00001448
Iteration 132/1000 | Loss: 0.00001448
Iteration 133/1000 | Loss: 0.00001448
Iteration 134/1000 | Loss: 0.00001447
Iteration 135/1000 | Loss: 0.00001447
Iteration 136/1000 | Loss: 0.00001447
Iteration 137/1000 | Loss: 0.00001447
Iteration 138/1000 | Loss: 0.00001447
Iteration 139/1000 | Loss: 0.00001447
Iteration 140/1000 | Loss: 0.00001447
Iteration 141/1000 | Loss: 0.00001447
Iteration 142/1000 | Loss: 0.00001447
Iteration 143/1000 | Loss: 0.00001447
Iteration 144/1000 | Loss: 0.00001447
Iteration 145/1000 | Loss: 0.00001447
Iteration 146/1000 | Loss: 0.00001447
Iteration 147/1000 | Loss: 0.00001447
Iteration 148/1000 | Loss: 0.00001446
Iteration 149/1000 | Loss: 0.00001446
Iteration 150/1000 | Loss: 0.00001446
Iteration 151/1000 | Loss: 0.00001446
Iteration 152/1000 | Loss: 0.00001446
Iteration 153/1000 | Loss: 0.00001446
Iteration 154/1000 | Loss: 0.00001446
Iteration 155/1000 | Loss: 0.00001446
Iteration 156/1000 | Loss: 0.00001446
Iteration 157/1000 | Loss: 0.00001446
Iteration 158/1000 | Loss: 0.00001446
Iteration 159/1000 | Loss: 0.00001446
Iteration 160/1000 | Loss: 0.00001446
Iteration 161/1000 | Loss: 0.00001446
Iteration 162/1000 | Loss: 0.00001446
Iteration 163/1000 | Loss: 0.00001446
Iteration 164/1000 | Loss: 0.00001446
Iteration 165/1000 | Loss: 0.00001445
Iteration 166/1000 | Loss: 0.00001445
Iteration 167/1000 | Loss: 0.00001445
Iteration 168/1000 | Loss: 0.00001445
Iteration 169/1000 | Loss: 0.00001445
Iteration 170/1000 | Loss: 0.00001445
Iteration 171/1000 | Loss: 0.00001445
Iteration 172/1000 | Loss: 0.00001445
Iteration 173/1000 | Loss: 0.00001445
Iteration 174/1000 | Loss: 0.00001445
Iteration 175/1000 | Loss: 0.00001445
Iteration 176/1000 | Loss: 0.00001445
Iteration 177/1000 | Loss: 0.00001445
Iteration 178/1000 | Loss: 0.00001445
Iteration 179/1000 | Loss: 0.00001445
Iteration 180/1000 | Loss: 0.00001445
Iteration 181/1000 | Loss: 0.00001445
Iteration 182/1000 | Loss: 0.00001445
Iteration 183/1000 | Loss: 0.00001445
Iteration 184/1000 | Loss: 0.00001445
Iteration 185/1000 | Loss: 0.00001445
Iteration 186/1000 | Loss: 0.00001445
Iteration 187/1000 | Loss: 0.00001445
Iteration 188/1000 | Loss: 0.00001445
Iteration 189/1000 | Loss: 0.00001445
Iteration 190/1000 | Loss: 0.00001445
Iteration 191/1000 | Loss: 0.00001445
Iteration 192/1000 | Loss: 0.00001445
Iteration 193/1000 | Loss: 0.00001445
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [1.4449673471972346e-05, 1.4449673471972346e-05, 1.4449673471972346e-05, 1.4449673471972346e-05, 1.4449673471972346e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4449673471972346e-05

Optimization complete. Final v2v error: 3.2248964309692383 mm

Highest mean error: 3.7346389293670654 mm for frame 101

Lowest mean error: 2.9269731044769287 mm for frame 196

Saving results

Total time: 46.4944486618042
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00759832
Iteration 2/25 | Loss: 0.00188123
Iteration 3/25 | Loss: 0.00138170
Iteration 4/25 | Loss: 0.00126975
Iteration 5/25 | Loss: 0.00123896
Iteration 6/25 | Loss: 0.00120387
Iteration 7/25 | Loss: 0.00117878
Iteration 8/25 | Loss: 0.00116212
Iteration 9/25 | Loss: 0.00117636
Iteration 10/25 | Loss: 0.00115046
Iteration 11/25 | Loss: 0.00111535
Iteration 12/25 | Loss: 0.00109740
Iteration 13/25 | Loss: 0.00108232
Iteration 14/25 | Loss: 0.00109668
Iteration 15/25 | Loss: 0.00113718
Iteration 16/25 | Loss: 0.00116409
Iteration 17/25 | Loss: 0.00103883
Iteration 18/25 | Loss: 0.00110234
Iteration 19/25 | Loss: 0.00101308
Iteration 20/25 | Loss: 0.00100802
Iteration 21/25 | Loss: 0.00097924
Iteration 22/25 | Loss: 0.00098247
Iteration 23/25 | Loss: 0.00096973
Iteration 24/25 | Loss: 0.00096427
Iteration 25/25 | Loss: 0.00096428

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.83115244
Iteration 2/25 | Loss: 0.00067382
Iteration 3/25 | Loss: 0.00067347
Iteration 4/25 | Loss: 0.00067347
Iteration 5/25 | Loss: 0.00067347
Iteration 6/25 | Loss: 0.00067347
Iteration 7/25 | Loss: 0.00067347
Iteration 8/25 | Loss: 0.00067347
Iteration 9/25 | Loss: 0.00067347
Iteration 10/25 | Loss: 0.00067347
Iteration 11/25 | Loss: 0.00067347
Iteration 12/25 | Loss: 0.00067347
Iteration 13/25 | Loss: 0.00067347
Iteration 14/25 | Loss: 0.00067347
Iteration 15/25 | Loss: 0.00067347
Iteration 16/25 | Loss: 0.00067347
Iteration 17/25 | Loss: 0.00067347
Iteration 18/25 | Loss: 0.00067347
Iteration 19/25 | Loss: 0.00067347
Iteration 20/25 | Loss: 0.00067347
Iteration 21/25 | Loss: 0.00067347
Iteration 22/25 | Loss: 0.00067347
Iteration 23/25 | Loss: 0.00067347
Iteration 24/25 | Loss: 0.00067347
Iteration 25/25 | Loss: 0.00067347
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006734709604643285, 0.0006734709604643285, 0.0006734709604643285, 0.0006734709604643285, 0.0006734709604643285]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006734709604643285

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067347
Iteration 2/1000 | Loss: 0.00007830
Iteration 3/1000 | Loss: 0.00005318
Iteration 4/1000 | Loss: 0.00004436
Iteration 5/1000 | Loss: 0.00004117
Iteration 6/1000 | Loss: 0.00005157
Iteration 7/1000 | Loss: 0.00004517
Iteration 8/1000 | Loss: 0.00004585
Iteration 9/1000 | Loss: 0.00005014
Iteration 10/1000 | Loss: 0.00004904
Iteration 11/1000 | Loss: 0.00004904
Iteration 12/1000 | Loss: 0.00004724
Iteration 13/1000 | Loss: 0.00004729
Iteration 14/1000 | Loss: 0.00004539
Iteration 15/1000 | Loss: 0.00003889
Iteration 16/1000 | Loss: 0.00004411
Iteration 17/1000 | Loss: 0.00006612
Iteration 18/1000 | Loss: 0.00004431
Iteration 19/1000 | Loss: 0.00004105
Iteration 20/1000 | Loss: 0.00004954
Iteration 21/1000 | Loss: 0.00004821
Iteration 22/1000 | Loss: 0.00004974
Iteration 23/1000 | Loss: 0.00004729
Iteration 24/1000 | Loss: 0.00004828
Iteration 25/1000 | Loss: 0.00004624
Iteration 26/1000 | Loss: 0.00004739
Iteration 27/1000 | Loss: 0.00004639
Iteration 28/1000 | Loss: 0.00004708
Iteration 29/1000 | Loss: 0.00004653
Iteration 30/1000 | Loss: 0.00004774
Iteration 31/1000 | Loss: 0.00004515
Iteration 32/1000 | Loss: 0.00005078
Iteration 33/1000 | Loss: 0.00004609
Iteration 34/1000 | Loss: 0.00004952
Iteration 35/1000 | Loss: 0.00004271
Iteration 36/1000 | Loss: 0.00004646
Iteration 37/1000 | Loss: 0.00004257
Iteration 38/1000 | Loss: 0.00005215
Iteration 39/1000 | Loss: 0.00004236
Iteration 40/1000 | Loss: 0.00004992
Iteration 41/1000 | Loss: 0.00004222
Iteration 42/1000 | Loss: 0.00004569
Iteration 43/1000 | Loss: 0.00004188
Iteration 44/1000 | Loss: 0.00004893
Iteration 45/1000 | Loss: 0.00004308
Iteration 46/1000 | Loss: 0.00004747
Iteration 47/1000 | Loss: 0.00004522
Iteration 48/1000 | Loss: 0.00004660
Iteration 49/1000 | Loss: 0.00004283
Iteration 50/1000 | Loss: 0.00004646
Iteration 51/1000 | Loss: 0.00004269
Iteration 52/1000 | Loss: 0.00004948
Iteration 53/1000 | Loss: 0.00004254
Iteration 54/1000 | Loss: 0.00004670
Iteration 55/1000 | Loss: 0.00004248
Iteration 56/1000 | Loss: 0.00004577
Iteration 57/1000 | Loss: 0.00004293
Iteration 58/1000 | Loss: 0.00004675
Iteration 59/1000 | Loss: 0.00004278
Iteration 60/1000 | Loss: 0.00004883
Iteration 61/1000 | Loss: 0.00004238
Iteration 62/1000 | Loss: 0.00005002
Iteration 63/1000 | Loss: 0.00004251
Iteration 64/1000 | Loss: 0.00005040
Iteration 65/1000 | Loss: 0.00004244
Iteration 66/1000 | Loss: 0.00004323
Iteration 67/1000 | Loss: 0.00005018
Iteration 68/1000 | Loss: 0.00004118
Iteration 69/1000 | Loss: 0.00003825
Iteration 70/1000 | Loss: 0.00003487
Iteration 71/1000 | Loss: 0.00003663
Iteration 72/1000 | Loss: 0.00003625
Iteration 73/1000 | Loss: 0.00003883
Iteration 74/1000 | Loss: 0.00003980
Iteration 75/1000 | Loss: 0.00003586
Iteration 76/1000 | Loss: 0.00003774
Iteration 77/1000 | Loss: 0.00004621
Iteration 78/1000 | Loss: 0.00005500
Iteration 79/1000 | Loss: 0.00004035
Iteration 80/1000 | Loss: 0.00003437
Iteration 81/1000 | Loss: 0.00003570
Iteration 82/1000 | Loss: 0.00004896
Iteration 83/1000 | Loss: 0.00005275
Iteration 84/1000 | Loss: 0.00004922
Iteration 85/1000 | Loss: 0.00005329
Iteration 86/1000 | Loss: 0.00003885
Iteration 87/1000 | Loss: 0.00003633
Iteration 88/1000 | Loss: 0.00003473
Iteration 89/1000 | Loss: 0.00003535
Iteration 90/1000 | Loss: 0.00003426
Iteration 91/1000 | Loss: 0.00003420
Iteration 92/1000 | Loss: 0.00003419
Iteration 93/1000 | Loss: 0.00003419
Iteration 94/1000 | Loss: 0.00003418
Iteration 95/1000 | Loss: 0.00003418
Iteration 96/1000 | Loss: 0.00003428
Iteration 97/1000 | Loss: 0.00004701
Iteration 98/1000 | Loss: 0.00005429
Iteration 99/1000 | Loss: 0.00006756
Iteration 100/1000 | Loss: 0.00004955
Iteration 101/1000 | Loss: 0.00003448
Iteration 102/1000 | Loss: 0.00003463
Iteration 103/1000 | Loss: 0.00003419
Iteration 104/1000 | Loss: 0.00003516
Iteration 105/1000 | Loss: 0.00003424
Iteration 106/1000 | Loss: 0.00003583
Iteration 107/1000 | Loss: 0.00003442
Iteration 108/1000 | Loss: 0.00003404
Iteration 109/1000 | Loss: 0.00003413
Iteration 110/1000 | Loss: 0.00003598
Iteration 111/1000 | Loss: 0.00003448
Iteration 112/1000 | Loss: 0.00003402
Iteration 113/1000 | Loss: 0.00003402
Iteration 114/1000 | Loss: 0.00003402
Iteration 115/1000 | Loss: 0.00003402
Iteration 116/1000 | Loss: 0.00003402
Iteration 117/1000 | Loss: 0.00003402
Iteration 118/1000 | Loss: 0.00003402
Iteration 119/1000 | Loss: 0.00003402
Iteration 120/1000 | Loss: 0.00003401
Iteration 121/1000 | Loss: 0.00003401
Iteration 122/1000 | Loss: 0.00003401
Iteration 123/1000 | Loss: 0.00003401
Iteration 124/1000 | Loss: 0.00003401
Iteration 125/1000 | Loss: 0.00003401
Iteration 126/1000 | Loss: 0.00003401
Iteration 127/1000 | Loss: 0.00003401
Iteration 128/1000 | Loss: 0.00003401
Iteration 129/1000 | Loss: 0.00003409
Iteration 130/1000 | Loss: 0.00003408
Iteration 131/1000 | Loss: 0.00003408
Iteration 132/1000 | Loss: 0.00003408
Iteration 133/1000 | Loss: 0.00003407
Iteration 134/1000 | Loss: 0.00003406
Iteration 135/1000 | Loss: 0.00003404
Iteration 136/1000 | Loss: 0.00003403
Iteration 137/1000 | Loss: 0.00003402
Iteration 138/1000 | Loss: 0.00003402
Iteration 139/1000 | Loss: 0.00003402
Iteration 140/1000 | Loss: 0.00003402
Iteration 141/1000 | Loss: 0.00003402
Iteration 142/1000 | Loss: 0.00003402
Iteration 143/1000 | Loss: 0.00003402
Iteration 144/1000 | Loss: 0.00003402
Iteration 145/1000 | Loss: 0.00003402
Iteration 146/1000 | Loss: 0.00003402
Iteration 147/1000 | Loss: 0.00003401
Iteration 148/1000 | Loss: 0.00003401
Iteration 149/1000 | Loss: 0.00003401
Iteration 150/1000 | Loss: 0.00003401
Iteration 151/1000 | Loss: 0.00003401
Iteration 152/1000 | Loss: 0.00003401
Iteration 153/1000 | Loss: 0.00003401
Iteration 154/1000 | Loss: 0.00003401
Iteration 155/1000 | Loss: 0.00003401
Iteration 156/1000 | Loss: 0.00003400
Iteration 157/1000 | Loss: 0.00003400
Iteration 158/1000 | Loss: 0.00003400
Iteration 159/1000 | Loss: 0.00003400
Iteration 160/1000 | Loss: 0.00003400
Iteration 161/1000 | Loss: 0.00003400
Iteration 162/1000 | Loss: 0.00003400
Iteration 163/1000 | Loss: 0.00003400
Iteration 164/1000 | Loss: 0.00003400
Iteration 165/1000 | Loss: 0.00003399
Iteration 166/1000 | Loss: 0.00003399
Iteration 167/1000 | Loss: 0.00003420
Iteration 168/1000 | Loss: 0.00003401
Iteration 169/1000 | Loss: 0.00003399
Iteration 170/1000 | Loss: 0.00003399
Iteration 171/1000 | Loss: 0.00003399
Iteration 172/1000 | Loss: 0.00003403
Iteration 173/1000 | Loss: 0.00003403
Iteration 174/1000 | Loss: 0.00003399
Iteration 175/1000 | Loss: 0.00003399
Iteration 176/1000 | Loss: 0.00003399
Iteration 177/1000 | Loss: 0.00003398
Iteration 178/1000 | Loss: 0.00003404
Iteration 179/1000 | Loss: 0.00003404
Iteration 180/1000 | Loss: 0.00003402
Iteration 181/1000 | Loss: 0.00003397
Iteration 182/1000 | Loss: 0.00003397
Iteration 183/1000 | Loss: 0.00003397
Iteration 184/1000 | Loss: 0.00003396
Iteration 185/1000 | Loss: 0.00003396
Iteration 186/1000 | Loss: 0.00003396
Iteration 187/1000 | Loss: 0.00003396
Iteration 188/1000 | Loss: 0.00003396
Iteration 189/1000 | Loss: 0.00003396
Iteration 190/1000 | Loss: 0.00003395
Iteration 191/1000 | Loss: 0.00003395
Iteration 192/1000 | Loss: 0.00003395
Iteration 193/1000 | Loss: 0.00003395
Iteration 194/1000 | Loss: 0.00003394
Iteration 195/1000 | Loss: 0.00003427
Iteration 196/1000 | Loss: 0.00003392
Iteration 197/1000 | Loss: 0.00003392
Iteration 198/1000 | Loss: 0.00003392
Iteration 199/1000 | Loss: 0.00003392
Iteration 200/1000 | Loss: 0.00003392
Iteration 201/1000 | Loss: 0.00003392
Iteration 202/1000 | Loss: 0.00003392
Iteration 203/1000 | Loss: 0.00003392
Iteration 204/1000 | Loss: 0.00003392
Iteration 205/1000 | Loss: 0.00003392
Iteration 206/1000 | Loss: 0.00003392
Iteration 207/1000 | Loss: 0.00003391
Iteration 208/1000 | Loss: 0.00003391
Iteration 209/1000 | Loss: 0.00003391
Iteration 210/1000 | Loss: 0.00003390
Iteration 211/1000 | Loss: 0.00003390
Iteration 212/1000 | Loss: 0.00003396
Iteration 213/1000 | Loss: 0.00003395
Iteration 214/1000 | Loss: 0.00004702
Iteration 215/1000 | Loss: 0.00003561
Iteration 216/1000 | Loss: 0.00003947
Iteration 217/1000 | Loss: 0.00004618
Iteration 218/1000 | Loss: 0.00003783
Iteration 219/1000 | Loss: 0.00003402
Iteration 220/1000 | Loss: 0.00003397
Iteration 221/1000 | Loss: 0.00003371
Iteration 222/1000 | Loss: 0.00003368
Iteration 223/1000 | Loss: 0.00003537
Iteration 224/1000 | Loss: 0.00003348
Iteration 225/1000 | Loss: 0.00003376
Iteration 226/1000 | Loss: 0.00003376
Iteration 227/1000 | Loss: 0.00003345
Iteration 228/1000 | Loss: 0.00003329
Iteration 229/1000 | Loss: 0.00003329
Iteration 230/1000 | Loss: 0.00003329
Iteration 231/1000 | Loss: 0.00003329
Iteration 232/1000 | Loss: 0.00003329
Iteration 233/1000 | Loss: 0.00003329
Iteration 234/1000 | Loss: 0.00003329
Iteration 235/1000 | Loss: 0.00003329
Iteration 236/1000 | Loss: 0.00003329
Iteration 237/1000 | Loss: 0.00003329
Iteration 238/1000 | Loss: 0.00003329
Iteration 239/1000 | Loss: 0.00003329
Iteration 240/1000 | Loss: 0.00003329
Iteration 241/1000 | Loss: 0.00003329
Iteration 242/1000 | Loss: 0.00003329
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 242. Stopping optimization.
Last 5 losses: [3.329096216475591e-05, 3.329096216475591e-05, 3.329096216475591e-05, 3.329096216475591e-05, 3.329096216475591e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.329096216475591e-05

Optimization complete. Final v2v error: 4.535289287567139 mm

Highest mean error: 8.69214153289795 mm for frame 179

Lowest mean error: 3.001234292984009 mm for frame 239

Saving results

Total time: 251.57025456428528
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00973647
Iteration 2/25 | Loss: 0.00164520
Iteration 3/25 | Loss: 0.00102438
Iteration 4/25 | Loss: 0.00097338
Iteration 5/25 | Loss: 0.00095652
Iteration 6/25 | Loss: 0.00095163
Iteration 7/25 | Loss: 0.00095103
Iteration 8/25 | Loss: 0.00095103
Iteration 9/25 | Loss: 0.00095103
Iteration 10/25 | Loss: 0.00095103
Iteration 11/25 | Loss: 0.00095103
Iteration 12/25 | Loss: 0.00095103
Iteration 13/25 | Loss: 0.00095103
Iteration 14/25 | Loss: 0.00095103
Iteration 15/25 | Loss: 0.00095103
Iteration 16/25 | Loss: 0.00095103
Iteration 17/25 | Loss: 0.00095103
Iteration 18/25 | Loss: 0.00095103
Iteration 19/25 | Loss: 0.00095103
Iteration 20/25 | Loss: 0.00095103
Iteration 21/25 | Loss: 0.00095103
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009510283125564456, 0.0009510283125564456, 0.0009510283125564456, 0.0009510283125564456, 0.0009510283125564456]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009510283125564456

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.97013420
Iteration 2/25 | Loss: 0.00041814
Iteration 3/25 | Loss: 0.00041813
Iteration 4/25 | Loss: 0.00041813
Iteration 5/25 | Loss: 0.00041813
Iteration 6/25 | Loss: 0.00041813
Iteration 7/25 | Loss: 0.00041813
Iteration 8/25 | Loss: 0.00041813
Iteration 9/25 | Loss: 0.00041813
Iteration 10/25 | Loss: 0.00041813
Iteration 11/25 | Loss: 0.00041813
Iteration 12/25 | Loss: 0.00041813
Iteration 13/25 | Loss: 0.00041813
Iteration 14/25 | Loss: 0.00041813
Iteration 15/25 | Loss: 0.00041813
Iteration 16/25 | Loss: 0.00041813
Iteration 17/25 | Loss: 0.00041813
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0004181306285317987, 0.0004181306285317987, 0.0004181306285317987, 0.0004181306285317987, 0.0004181306285317987]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004181306285317987

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041813
Iteration 2/1000 | Loss: 0.00006879
Iteration 3/1000 | Loss: 0.00004373
Iteration 4/1000 | Loss: 0.00004051
Iteration 5/1000 | Loss: 0.00003841
Iteration 6/1000 | Loss: 0.00003752
Iteration 7/1000 | Loss: 0.00003661
Iteration 8/1000 | Loss: 0.00003596
Iteration 9/1000 | Loss: 0.00003562
Iteration 10/1000 | Loss: 0.00003526
Iteration 11/1000 | Loss: 0.00003496
Iteration 12/1000 | Loss: 0.00003467
Iteration 13/1000 | Loss: 0.00003443
Iteration 14/1000 | Loss: 0.00003424
Iteration 15/1000 | Loss: 0.00003406
Iteration 16/1000 | Loss: 0.00003390
Iteration 17/1000 | Loss: 0.00003388
Iteration 18/1000 | Loss: 0.00003383
Iteration 19/1000 | Loss: 0.00003379
Iteration 20/1000 | Loss: 0.00003376
Iteration 21/1000 | Loss: 0.00003375
Iteration 22/1000 | Loss: 0.00003375
Iteration 23/1000 | Loss: 0.00003374
Iteration 24/1000 | Loss: 0.00003372
Iteration 25/1000 | Loss: 0.00003370
Iteration 26/1000 | Loss: 0.00003370
Iteration 27/1000 | Loss: 0.00003368
Iteration 28/1000 | Loss: 0.00003368
Iteration 29/1000 | Loss: 0.00003367
Iteration 30/1000 | Loss: 0.00003367
Iteration 31/1000 | Loss: 0.00003366
Iteration 32/1000 | Loss: 0.00003365
Iteration 33/1000 | Loss: 0.00003365
Iteration 34/1000 | Loss: 0.00003365
Iteration 35/1000 | Loss: 0.00003364
Iteration 36/1000 | Loss: 0.00003362
Iteration 37/1000 | Loss: 0.00003362
Iteration 38/1000 | Loss: 0.00003361
Iteration 39/1000 | Loss: 0.00003361
Iteration 40/1000 | Loss: 0.00003361
Iteration 41/1000 | Loss: 0.00003359
Iteration 42/1000 | Loss: 0.00003358
Iteration 43/1000 | Loss: 0.00003357
Iteration 44/1000 | Loss: 0.00003357
Iteration 45/1000 | Loss: 0.00003357
Iteration 46/1000 | Loss: 0.00003357
Iteration 47/1000 | Loss: 0.00003357
Iteration 48/1000 | Loss: 0.00003357
Iteration 49/1000 | Loss: 0.00003357
Iteration 50/1000 | Loss: 0.00003357
Iteration 51/1000 | Loss: 0.00003356
Iteration 52/1000 | Loss: 0.00003356
Iteration 53/1000 | Loss: 0.00003356
Iteration 54/1000 | Loss: 0.00003354
Iteration 55/1000 | Loss: 0.00003354
Iteration 56/1000 | Loss: 0.00003353
Iteration 57/1000 | Loss: 0.00003353
Iteration 58/1000 | Loss: 0.00003353
Iteration 59/1000 | Loss: 0.00003352
Iteration 60/1000 | Loss: 0.00003352
Iteration 61/1000 | Loss: 0.00003352
Iteration 62/1000 | Loss: 0.00003351
Iteration 63/1000 | Loss: 0.00003351
Iteration 64/1000 | Loss: 0.00003350
Iteration 65/1000 | Loss: 0.00003350
Iteration 66/1000 | Loss: 0.00003350
Iteration 67/1000 | Loss: 0.00003350
Iteration 68/1000 | Loss: 0.00003350
Iteration 69/1000 | Loss: 0.00003349
Iteration 70/1000 | Loss: 0.00003349
Iteration 71/1000 | Loss: 0.00003349
Iteration 72/1000 | Loss: 0.00003349
Iteration 73/1000 | Loss: 0.00003349
Iteration 74/1000 | Loss: 0.00003349
Iteration 75/1000 | Loss: 0.00003349
Iteration 76/1000 | Loss: 0.00003349
Iteration 77/1000 | Loss: 0.00003349
Iteration 78/1000 | Loss: 0.00003349
Iteration 79/1000 | Loss: 0.00003349
Iteration 80/1000 | Loss: 0.00003349
Iteration 81/1000 | Loss: 0.00003349
Iteration 82/1000 | Loss: 0.00003349
Iteration 83/1000 | Loss: 0.00003349
Iteration 84/1000 | Loss: 0.00003349
Iteration 85/1000 | Loss: 0.00003348
Iteration 86/1000 | Loss: 0.00003348
Iteration 87/1000 | Loss: 0.00003347
Iteration 88/1000 | Loss: 0.00003347
Iteration 89/1000 | Loss: 0.00003347
Iteration 90/1000 | Loss: 0.00003346
Iteration 91/1000 | Loss: 0.00003346
Iteration 92/1000 | Loss: 0.00003346
Iteration 93/1000 | Loss: 0.00003346
Iteration 94/1000 | Loss: 0.00003346
Iteration 95/1000 | Loss: 0.00003346
Iteration 96/1000 | Loss: 0.00003346
Iteration 97/1000 | Loss: 0.00003346
Iteration 98/1000 | Loss: 0.00003346
Iteration 99/1000 | Loss: 0.00003346
Iteration 100/1000 | Loss: 0.00003346
Iteration 101/1000 | Loss: 0.00003345
Iteration 102/1000 | Loss: 0.00003345
Iteration 103/1000 | Loss: 0.00003345
Iteration 104/1000 | Loss: 0.00003344
Iteration 105/1000 | Loss: 0.00003344
Iteration 106/1000 | Loss: 0.00003344
Iteration 107/1000 | Loss: 0.00003344
Iteration 108/1000 | Loss: 0.00003344
Iteration 109/1000 | Loss: 0.00003344
Iteration 110/1000 | Loss: 0.00003344
Iteration 111/1000 | Loss: 0.00003344
Iteration 112/1000 | Loss: 0.00003344
Iteration 113/1000 | Loss: 0.00003344
Iteration 114/1000 | Loss: 0.00003344
Iteration 115/1000 | Loss: 0.00003343
Iteration 116/1000 | Loss: 0.00003343
Iteration 117/1000 | Loss: 0.00003343
Iteration 118/1000 | Loss: 0.00003343
Iteration 119/1000 | Loss: 0.00003343
Iteration 120/1000 | Loss: 0.00003343
Iteration 121/1000 | Loss: 0.00003343
Iteration 122/1000 | Loss: 0.00003343
Iteration 123/1000 | Loss: 0.00003343
Iteration 124/1000 | Loss: 0.00003343
Iteration 125/1000 | Loss: 0.00003343
Iteration 126/1000 | Loss: 0.00003343
Iteration 127/1000 | Loss: 0.00003343
Iteration 128/1000 | Loss: 0.00003343
Iteration 129/1000 | Loss: 0.00003343
Iteration 130/1000 | Loss: 0.00003343
Iteration 131/1000 | Loss: 0.00003343
Iteration 132/1000 | Loss: 0.00003343
Iteration 133/1000 | Loss: 0.00003343
Iteration 134/1000 | Loss: 0.00003343
Iteration 135/1000 | Loss: 0.00003342
Iteration 136/1000 | Loss: 0.00003342
Iteration 137/1000 | Loss: 0.00003342
Iteration 138/1000 | Loss: 0.00003342
Iteration 139/1000 | Loss: 0.00003342
Iteration 140/1000 | Loss: 0.00003342
Iteration 141/1000 | Loss: 0.00003341
Iteration 142/1000 | Loss: 0.00003341
Iteration 143/1000 | Loss: 0.00003341
Iteration 144/1000 | Loss: 0.00003341
Iteration 145/1000 | Loss: 0.00003341
Iteration 146/1000 | Loss: 0.00003341
Iteration 147/1000 | Loss: 0.00003341
Iteration 148/1000 | Loss: 0.00003340
Iteration 149/1000 | Loss: 0.00003340
Iteration 150/1000 | Loss: 0.00003340
Iteration 151/1000 | Loss: 0.00003340
Iteration 152/1000 | Loss: 0.00003340
Iteration 153/1000 | Loss: 0.00003340
Iteration 154/1000 | Loss: 0.00003339
Iteration 155/1000 | Loss: 0.00003339
Iteration 156/1000 | Loss: 0.00003339
Iteration 157/1000 | Loss: 0.00003339
Iteration 158/1000 | Loss: 0.00003339
Iteration 159/1000 | Loss: 0.00003339
Iteration 160/1000 | Loss: 0.00003338
Iteration 161/1000 | Loss: 0.00003338
Iteration 162/1000 | Loss: 0.00003338
Iteration 163/1000 | Loss: 0.00003338
Iteration 164/1000 | Loss: 0.00003338
Iteration 165/1000 | Loss: 0.00003338
Iteration 166/1000 | Loss: 0.00003337
Iteration 167/1000 | Loss: 0.00003337
Iteration 168/1000 | Loss: 0.00003337
Iteration 169/1000 | Loss: 0.00003337
Iteration 170/1000 | Loss: 0.00003337
Iteration 171/1000 | Loss: 0.00003337
Iteration 172/1000 | Loss: 0.00003337
Iteration 173/1000 | Loss: 0.00003337
Iteration 174/1000 | Loss: 0.00003337
Iteration 175/1000 | Loss: 0.00003337
Iteration 176/1000 | Loss: 0.00003337
Iteration 177/1000 | Loss: 0.00003336
Iteration 178/1000 | Loss: 0.00003336
Iteration 179/1000 | Loss: 0.00003336
Iteration 180/1000 | Loss: 0.00003336
Iteration 181/1000 | Loss: 0.00003336
Iteration 182/1000 | Loss: 0.00003336
Iteration 183/1000 | Loss: 0.00003336
Iteration 184/1000 | Loss: 0.00003336
Iteration 185/1000 | Loss: 0.00003336
Iteration 186/1000 | Loss: 0.00003335
Iteration 187/1000 | Loss: 0.00003335
Iteration 188/1000 | Loss: 0.00003335
Iteration 189/1000 | Loss: 0.00003335
Iteration 190/1000 | Loss: 0.00003335
Iteration 191/1000 | Loss: 0.00003335
Iteration 192/1000 | Loss: 0.00003335
Iteration 193/1000 | Loss: 0.00003335
Iteration 194/1000 | Loss: 0.00003334
Iteration 195/1000 | Loss: 0.00003334
Iteration 196/1000 | Loss: 0.00003334
Iteration 197/1000 | Loss: 0.00003334
Iteration 198/1000 | Loss: 0.00003334
Iteration 199/1000 | Loss: 0.00003334
Iteration 200/1000 | Loss: 0.00003334
Iteration 201/1000 | Loss: 0.00003334
Iteration 202/1000 | Loss: 0.00003334
Iteration 203/1000 | Loss: 0.00003334
Iteration 204/1000 | Loss: 0.00003334
Iteration 205/1000 | Loss: 0.00003334
Iteration 206/1000 | Loss: 0.00003334
Iteration 207/1000 | Loss: 0.00003334
Iteration 208/1000 | Loss: 0.00003334
Iteration 209/1000 | Loss: 0.00003334
Iteration 210/1000 | Loss: 0.00003334
Iteration 211/1000 | Loss: 0.00003334
Iteration 212/1000 | Loss: 0.00003334
Iteration 213/1000 | Loss: 0.00003334
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [3.334369830554351e-05, 3.334369830554351e-05, 3.334369830554351e-05, 3.334369830554351e-05, 3.334369830554351e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.334369830554351e-05

Optimization complete. Final v2v error: 4.767899036407471 mm

Highest mean error: 6.117770195007324 mm for frame 119

Lowest mean error: 3.9671883583068848 mm for frame 0

Saving results

Total time: 54.23706841468811
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00502295
Iteration 2/25 | Loss: 0.00107009
Iteration 3/25 | Loss: 0.00082182
Iteration 4/25 | Loss: 0.00078430
Iteration 5/25 | Loss: 0.00077743
Iteration 6/25 | Loss: 0.00077650
Iteration 7/25 | Loss: 0.00077622
Iteration 8/25 | Loss: 0.00077622
Iteration 9/25 | Loss: 0.00077622
Iteration 10/25 | Loss: 0.00077622
Iteration 11/25 | Loss: 0.00077622
Iteration 12/25 | Loss: 0.00077622
Iteration 13/25 | Loss: 0.00077622
Iteration 14/25 | Loss: 0.00077622
Iteration 15/25 | Loss: 0.00077622
Iteration 16/25 | Loss: 0.00077622
Iteration 17/25 | Loss: 0.00077622
Iteration 18/25 | Loss: 0.00077622
Iteration 19/25 | Loss: 0.00077622
Iteration 20/25 | Loss: 0.00077622
Iteration 21/25 | Loss: 0.00077622
Iteration 22/25 | Loss: 0.00077622
Iteration 23/25 | Loss: 0.00077622
Iteration 24/25 | Loss: 0.00077622
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.000776223954744637, 0.000776223954744637, 0.000776223954744637, 0.000776223954744637, 0.000776223954744637]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000776223954744637

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09634447
Iteration 2/25 | Loss: 0.00048383
Iteration 3/25 | Loss: 0.00048377
Iteration 4/25 | Loss: 0.00048377
Iteration 5/25 | Loss: 0.00048377
Iteration 6/25 | Loss: 0.00048377
Iteration 7/25 | Loss: 0.00048376
Iteration 8/25 | Loss: 0.00048376
Iteration 9/25 | Loss: 0.00048376
Iteration 10/25 | Loss: 0.00048376
Iteration 11/25 | Loss: 0.00048376
Iteration 12/25 | Loss: 0.00048376
Iteration 13/25 | Loss: 0.00048376
Iteration 14/25 | Loss: 0.00048376
Iteration 15/25 | Loss: 0.00048376
Iteration 16/25 | Loss: 0.00048376
Iteration 17/25 | Loss: 0.00048376
Iteration 18/25 | Loss: 0.00048376
Iteration 19/25 | Loss: 0.00048376
Iteration 20/25 | Loss: 0.00048376
Iteration 21/25 | Loss: 0.00048376
Iteration 22/25 | Loss: 0.00048376
Iteration 23/25 | Loss: 0.00048376
Iteration 24/25 | Loss: 0.00048376
Iteration 25/25 | Loss: 0.00048376

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048376
Iteration 2/1000 | Loss: 0.00004351
Iteration 3/1000 | Loss: 0.00003017
Iteration 4/1000 | Loss: 0.00002622
Iteration 5/1000 | Loss: 0.00002498
Iteration 6/1000 | Loss: 0.00002402
Iteration 7/1000 | Loss: 0.00002311
Iteration 8/1000 | Loss: 0.00002262
Iteration 9/1000 | Loss: 0.00002238
Iteration 10/1000 | Loss: 0.00002207
Iteration 11/1000 | Loss: 0.00002179
Iteration 12/1000 | Loss: 0.00002144
Iteration 13/1000 | Loss: 0.00002122
Iteration 14/1000 | Loss: 0.00002116
Iteration 15/1000 | Loss: 0.00002116
Iteration 16/1000 | Loss: 0.00002115
Iteration 17/1000 | Loss: 0.00002112
Iteration 18/1000 | Loss: 0.00002109
Iteration 19/1000 | Loss: 0.00002098
Iteration 20/1000 | Loss: 0.00002087
Iteration 21/1000 | Loss: 0.00002082
Iteration 22/1000 | Loss: 0.00002078
Iteration 23/1000 | Loss: 0.00002077
Iteration 24/1000 | Loss: 0.00002077
Iteration 25/1000 | Loss: 0.00002077
Iteration 26/1000 | Loss: 0.00002077
Iteration 27/1000 | Loss: 0.00002076
Iteration 28/1000 | Loss: 0.00002074
Iteration 29/1000 | Loss: 0.00002074
Iteration 30/1000 | Loss: 0.00002074
Iteration 31/1000 | Loss: 0.00002074
Iteration 32/1000 | Loss: 0.00002074
Iteration 33/1000 | Loss: 0.00002074
Iteration 34/1000 | Loss: 0.00002074
Iteration 35/1000 | Loss: 0.00002073
Iteration 36/1000 | Loss: 0.00002073
Iteration 37/1000 | Loss: 0.00002073
Iteration 38/1000 | Loss: 0.00002072
Iteration 39/1000 | Loss: 0.00002072
Iteration 40/1000 | Loss: 0.00002071
Iteration 41/1000 | Loss: 0.00002071
Iteration 42/1000 | Loss: 0.00002070
Iteration 43/1000 | Loss: 0.00002070
Iteration 44/1000 | Loss: 0.00002069
Iteration 45/1000 | Loss: 0.00002069
Iteration 46/1000 | Loss: 0.00002069
Iteration 47/1000 | Loss: 0.00002069
Iteration 48/1000 | Loss: 0.00002069
Iteration 49/1000 | Loss: 0.00002069
Iteration 50/1000 | Loss: 0.00002069
Iteration 51/1000 | Loss: 0.00002069
Iteration 52/1000 | Loss: 0.00002069
Iteration 53/1000 | Loss: 0.00002069
Iteration 54/1000 | Loss: 0.00002069
Iteration 55/1000 | Loss: 0.00002068
Iteration 56/1000 | Loss: 0.00002068
Iteration 57/1000 | Loss: 0.00002066
Iteration 58/1000 | Loss: 0.00002065
Iteration 59/1000 | Loss: 0.00002065
Iteration 60/1000 | Loss: 0.00002064
Iteration 61/1000 | Loss: 0.00002064
Iteration 62/1000 | Loss: 0.00002063
Iteration 63/1000 | Loss: 0.00002062
Iteration 64/1000 | Loss: 0.00002062
Iteration 65/1000 | Loss: 0.00002061
Iteration 66/1000 | Loss: 0.00002061
Iteration 67/1000 | Loss: 0.00002061
Iteration 68/1000 | Loss: 0.00002061
Iteration 69/1000 | Loss: 0.00002061
Iteration 70/1000 | Loss: 0.00002061
Iteration 71/1000 | Loss: 0.00002061
Iteration 72/1000 | Loss: 0.00002061
Iteration 73/1000 | Loss: 0.00002061
Iteration 74/1000 | Loss: 0.00002061
Iteration 75/1000 | Loss: 0.00002061
Iteration 76/1000 | Loss: 0.00002060
Iteration 77/1000 | Loss: 0.00002060
Iteration 78/1000 | Loss: 0.00002060
Iteration 79/1000 | Loss: 0.00002060
Iteration 80/1000 | Loss: 0.00002060
Iteration 81/1000 | Loss: 0.00002060
Iteration 82/1000 | Loss: 0.00002060
Iteration 83/1000 | Loss: 0.00002060
Iteration 84/1000 | Loss: 0.00002060
Iteration 85/1000 | Loss: 0.00002060
Iteration 86/1000 | Loss: 0.00002060
Iteration 87/1000 | Loss: 0.00002060
Iteration 88/1000 | Loss: 0.00002060
Iteration 89/1000 | Loss: 0.00002059
Iteration 90/1000 | Loss: 0.00002059
Iteration 91/1000 | Loss: 0.00002058
Iteration 92/1000 | Loss: 0.00002058
Iteration 93/1000 | Loss: 0.00002056
Iteration 94/1000 | Loss: 0.00002056
Iteration 95/1000 | Loss: 0.00002056
Iteration 96/1000 | Loss: 0.00002056
Iteration 97/1000 | Loss: 0.00002056
Iteration 98/1000 | Loss: 0.00002056
Iteration 99/1000 | Loss: 0.00002056
Iteration 100/1000 | Loss: 0.00002056
Iteration 101/1000 | Loss: 0.00002056
Iteration 102/1000 | Loss: 0.00002056
Iteration 103/1000 | Loss: 0.00002055
Iteration 104/1000 | Loss: 0.00002055
Iteration 105/1000 | Loss: 0.00002055
Iteration 106/1000 | Loss: 0.00002055
Iteration 107/1000 | Loss: 0.00002055
Iteration 108/1000 | Loss: 0.00002055
Iteration 109/1000 | Loss: 0.00002054
Iteration 110/1000 | Loss: 0.00002054
Iteration 111/1000 | Loss: 0.00002054
Iteration 112/1000 | Loss: 0.00002054
Iteration 113/1000 | Loss: 0.00002054
Iteration 114/1000 | Loss: 0.00002054
Iteration 115/1000 | Loss: 0.00002054
Iteration 116/1000 | Loss: 0.00002054
Iteration 117/1000 | Loss: 0.00002054
Iteration 118/1000 | Loss: 0.00002053
Iteration 119/1000 | Loss: 0.00002053
Iteration 120/1000 | Loss: 0.00002053
Iteration 121/1000 | Loss: 0.00002053
Iteration 122/1000 | Loss: 0.00002053
Iteration 123/1000 | Loss: 0.00002053
Iteration 124/1000 | Loss: 0.00002053
Iteration 125/1000 | Loss: 0.00002053
Iteration 126/1000 | Loss: 0.00002053
Iteration 127/1000 | Loss: 0.00002053
Iteration 128/1000 | Loss: 0.00002053
Iteration 129/1000 | Loss: 0.00002053
Iteration 130/1000 | Loss: 0.00002053
Iteration 131/1000 | Loss: 0.00002053
Iteration 132/1000 | Loss: 0.00002053
Iteration 133/1000 | Loss: 0.00002053
Iteration 134/1000 | Loss: 0.00002053
Iteration 135/1000 | Loss: 0.00002053
Iteration 136/1000 | Loss: 0.00002053
Iteration 137/1000 | Loss: 0.00002053
Iteration 138/1000 | Loss: 0.00002053
Iteration 139/1000 | Loss: 0.00002053
Iteration 140/1000 | Loss: 0.00002052
Iteration 141/1000 | Loss: 0.00002052
Iteration 142/1000 | Loss: 0.00002052
Iteration 143/1000 | Loss: 0.00002052
Iteration 144/1000 | Loss: 0.00002052
Iteration 145/1000 | Loss: 0.00002052
Iteration 146/1000 | Loss: 0.00002051
Iteration 147/1000 | Loss: 0.00002051
Iteration 148/1000 | Loss: 0.00002051
Iteration 149/1000 | Loss: 0.00002051
Iteration 150/1000 | Loss: 0.00002051
Iteration 151/1000 | Loss: 0.00002051
Iteration 152/1000 | Loss: 0.00002051
Iteration 153/1000 | Loss: 0.00002051
Iteration 154/1000 | Loss: 0.00002051
Iteration 155/1000 | Loss: 0.00002051
Iteration 156/1000 | Loss: 0.00002051
Iteration 157/1000 | Loss: 0.00002051
Iteration 158/1000 | Loss: 0.00002051
Iteration 159/1000 | Loss: 0.00002051
Iteration 160/1000 | Loss: 0.00002051
Iteration 161/1000 | Loss: 0.00002051
Iteration 162/1000 | Loss: 0.00002051
Iteration 163/1000 | Loss: 0.00002051
Iteration 164/1000 | Loss: 0.00002051
Iteration 165/1000 | Loss: 0.00002051
Iteration 166/1000 | Loss: 0.00002051
Iteration 167/1000 | Loss: 0.00002051
Iteration 168/1000 | Loss: 0.00002051
Iteration 169/1000 | Loss: 0.00002051
Iteration 170/1000 | Loss: 0.00002051
Iteration 171/1000 | Loss: 0.00002051
Iteration 172/1000 | Loss: 0.00002051
Iteration 173/1000 | Loss: 0.00002051
Iteration 174/1000 | Loss: 0.00002051
Iteration 175/1000 | Loss: 0.00002051
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [2.0509922251221724e-05, 2.0509922251221724e-05, 2.0509922251221724e-05, 2.0509922251221724e-05, 2.0509922251221724e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0509922251221724e-05

Optimization complete. Final v2v error: 3.789053440093994 mm

Highest mean error: 3.8272862434387207 mm for frame 60

Lowest mean error: 3.7511045932769775 mm for frame 0

Saving results

Total time: 42.19851636886597
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00908691
Iteration 2/25 | Loss: 0.00096881
Iteration 3/25 | Loss: 0.00084544
Iteration 4/25 | Loss: 0.00080814
Iteration 5/25 | Loss: 0.00080341
Iteration 6/25 | Loss: 0.00080235
Iteration 7/25 | Loss: 0.00080205
Iteration 8/25 | Loss: 0.00080205
Iteration 9/25 | Loss: 0.00080205
Iteration 10/25 | Loss: 0.00080205
Iteration 11/25 | Loss: 0.00080205
Iteration 12/25 | Loss: 0.00080205
Iteration 13/25 | Loss: 0.00080205
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008020461536943913, 0.0008020461536943913, 0.0008020461536943913, 0.0008020461536943913, 0.0008020461536943913]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008020461536943913

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51031280
Iteration 2/25 | Loss: 0.00050584
Iteration 3/25 | Loss: 0.00050583
Iteration 4/25 | Loss: 0.00050583
Iteration 5/25 | Loss: 0.00050583
Iteration 6/25 | Loss: 0.00050583
Iteration 7/25 | Loss: 0.00050583
Iteration 8/25 | Loss: 0.00050583
Iteration 9/25 | Loss: 0.00050583
Iteration 10/25 | Loss: 0.00050583
Iteration 11/25 | Loss: 0.00050583
Iteration 12/25 | Loss: 0.00050583
Iteration 13/25 | Loss: 0.00050583
Iteration 14/25 | Loss: 0.00050583
Iteration 15/25 | Loss: 0.00050583
Iteration 16/25 | Loss: 0.00050583
Iteration 17/25 | Loss: 0.00050583
Iteration 18/25 | Loss: 0.00050583
Iteration 19/25 | Loss: 0.00050583
Iteration 20/25 | Loss: 0.00050583
Iteration 21/25 | Loss: 0.00050583
Iteration 22/25 | Loss: 0.00050583
Iteration 23/25 | Loss: 0.00050583
Iteration 24/25 | Loss: 0.00050583
Iteration 25/25 | Loss: 0.00050583
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0005058315000496805, 0.0005058315000496805, 0.0005058315000496805, 0.0005058315000496805, 0.0005058315000496805]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005058315000496805

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050583
Iteration 2/1000 | Loss: 0.00003519
Iteration 3/1000 | Loss: 0.00002620
Iteration 4/1000 | Loss: 0.00002369
Iteration 5/1000 | Loss: 0.00002252
Iteration 6/1000 | Loss: 0.00002162
Iteration 7/1000 | Loss: 0.00002118
Iteration 8/1000 | Loss: 0.00002089
Iteration 9/1000 | Loss: 0.00002074
Iteration 10/1000 | Loss: 0.00002069
Iteration 11/1000 | Loss: 0.00002061
Iteration 12/1000 | Loss: 0.00002061
Iteration 13/1000 | Loss: 0.00002059
Iteration 14/1000 | Loss: 0.00002056
Iteration 15/1000 | Loss: 0.00002055
Iteration 16/1000 | Loss: 0.00002055
Iteration 17/1000 | Loss: 0.00002050
Iteration 18/1000 | Loss: 0.00002049
Iteration 19/1000 | Loss: 0.00002047
Iteration 20/1000 | Loss: 0.00002047
Iteration 21/1000 | Loss: 0.00002047
Iteration 22/1000 | Loss: 0.00002047
Iteration 23/1000 | Loss: 0.00002046
Iteration 24/1000 | Loss: 0.00002046
Iteration 25/1000 | Loss: 0.00002046
Iteration 26/1000 | Loss: 0.00002046
Iteration 27/1000 | Loss: 0.00002045
Iteration 28/1000 | Loss: 0.00002045
Iteration 29/1000 | Loss: 0.00002044
Iteration 30/1000 | Loss: 0.00002044
Iteration 31/1000 | Loss: 0.00002043
Iteration 32/1000 | Loss: 0.00002043
Iteration 33/1000 | Loss: 0.00002043
Iteration 34/1000 | Loss: 0.00002042
Iteration 35/1000 | Loss: 0.00002042
Iteration 36/1000 | Loss: 0.00002042
Iteration 37/1000 | Loss: 0.00002041
Iteration 38/1000 | Loss: 0.00002040
Iteration 39/1000 | Loss: 0.00002040
Iteration 40/1000 | Loss: 0.00002040
Iteration 41/1000 | Loss: 0.00002040
Iteration 42/1000 | Loss: 0.00002040
Iteration 43/1000 | Loss: 0.00002040
Iteration 44/1000 | Loss: 0.00002040
Iteration 45/1000 | Loss: 0.00002039
Iteration 46/1000 | Loss: 0.00002039
Iteration 47/1000 | Loss: 0.00002039
Iteration 48/1000 | Loss: 0.00002039
Iteration 49/1000 | Loss: 0.00002039
Iteration 50/1000 | Loss: 0.00002039
Iteration 51/1000 | Loss: 0.00002038
Iteration 52/1000 | Loss: 0.00002038
Iteration 53/1000 | Loss: 0.00002038
Iteration 54/1000 | Loss: 0.00002037
Iteration 55/1000 | Loss: 0.00002037
Iteration 56/1000 | Loss: 0.00002037
Iteration 57/1000 | Loss: 0.00002037
Iteration 58/1000 | Loss: 0.00002037
Iteration 59/1000 | Loss: 0.00002037
Iteration 60/1000 | Loss: 0.00002037
Iteration 61/1000 | Loss: 0.00002037
Iteration 62/1000 | Loss: 0.00002037
Iteration 63/1000 | Loss: 0.00002036
Iteration 64/1000 | Loss: 0.00002036
Iteration 65/1000 | Loss: 0.00002035
Iteration 66/1000 | Loss: 0.00002035
Iteration 67/1000 | Loss: 0.00002035
Iteration 68/1000 | Loss: 0.00002035
Iteration 69/1000 | Loss: 0.00002034
Iteration 70/1000 | Loss: 0.00002033
Iteration 71/1000 | Loss: 0.00002033
Iteration 72/1000 | Loss: 0.00002033
Iteration 73/1000 | Loss: 0.00002033
Iteration 74/1000 | Loss: 0.00002032
Iteration 75/1000 | Loss: 0.00002032
Iteration 76/1000 | Loss: 0.00002032
Iteration 77/1000 | Loss: 0.00002032
Iteration 78/1000 | Loss: 0.00002032
Iteration 79/1000 | Loss: 0.00002032
Iteration 80/1000 | Loss: 0.00002030
Iteration 81/1000 | Loss: 0.00002026
Iteration 82/1000 | Loss: 0.00002025
Iteration 83/1000 | Loss: 0.00002025
Iteration 84/1000 | Loss: 0.00002025
Iteration 85/1000 | Loss: 0.00002025
Iteration 86/1000 | Loss: 0.00002025
Iteration 87/1000 | Loss: 0.00002025
Iteration 88/1000 | Loss: 0.00002025
Iteration 89/1000 | Loss: 0.00002025
Iteration 90/1000 | Loss: 0.00002024
Iteration 91/1000 | Loss: 0.00002023
Iteration 92/1000 | Loss: 0.00002023
Iteration 93/1000 | Loss: 0.00002022
Iteration 94/1000 | Loss: 0.00002022
Iteration 95/1000 | Loss: 0.00002022
Iteration 96/1000 | Loss: 0.00002022
Iteration 97/1000 | Loss: 0.00002022
Iteration 98/1000 | Loss: 0.00002022
Iteration 99/1000 | Loss: 0.00002022
Iteration 100/1000 | Loss: 0.00002022
Iteration 101/1000 | Loss: 0.00002021
Iteration 102/1000 | Loss: 0.00002021
Iteration 103/1000 | Loss: 0.00002021
Iteration 104/1000 | Loss: 0.00002021
Iteration 105/1000 | Loss: 0.00002021
Iteration 106/1000 | Loss: 0.00002021
Iteration 107/1000 | Loss: 0.00002020
Iteration 108/1000 | Loss: 0.00002020
Iteration 109/1000 | Loss: 0.00002020
Iteration 110/1000 | Loss: 0.00002020
Iteration 111/1000 | Loss: 0.00002020
Iteration 112/1000 | Loss: 0.00002020
Iteration 113/1000 | Loss: 0.00002019
Iteration 114/1000 | Loss: 0.00002019
Iteration 115/1000 | Loss: 0.00002018
Iteration 116/1000 | Loss: 0.00002018
Iteration 117/1000 | Loss: 0.00002018
Iteration 118/1000 | Loss: 0.00002018
Iteration 119/1000 | Loss: 0.00002018
Iteration 120/1000 | Loss: 0.00002017
Iteration 121/1000 | Loss: 0.00002017
Iteration 122/1000 | Loss: 0.00002017
Iteration 123/1000 | Loss: 0.00002016
Iteration 124/1000 | Loss: 0.00002016
Iteration 125/1000 | Loss: 0.00002016
Iteration 126/1000 | Loss: 0.00002016
Iteration 127/1000 | Loss: 0.00002015
Iteration 128/1000 | Loss: 0.00002015
Iteration 129/1000 | Loss: 0.00002015
Iteration 130/1000 | Loss: 0.00002014
Iteration 131/1000 | Loss: 0.00002014
Iteration 132/1000 | Loss: 0.00002014
Iteration 133/1000 | Loss: 0.00002013
Iteration 134/1000 | Loss: 0.00002013
Iteration 135/1000 | Loss: 0.00002013
Iteration 136/1000 | Loss: 0.00002013
Iteration 137/1000 | Loss: 0.00002013
Iteration 138/1000 | Loss: 0.00002013
Iteration 139/1000 | Loss: 0.00002012
Iteration 140/1000 | Loss: 0.00002012
Iteration 141/1000 | Loss: 0.00002012
Iteration 142/1000 | Loss: 0.00002012
Iteration 143/1000 | Loss: 0.00002012
Iteration 144/1000 | Loss: 0.00002012
Iteration 145/1000 | Loss: 0.00002012
Iteration 146/1000 | Loss: 0.00002012
Iteration 147/1000 | Loss: 0.00002012
Iteration 148/1000 | Loss: 0.00002012
Iteration 149/1000 | Loss: 0.00002012
Iteration 150/1000 | Loss: 0.00002011
Iteration 151/1000 | Loss: 0.00002011
Iteration 152/1000 | Loss: 0.00002011
Iteration 153/1000 | Loss: 0.00002011
Iteration 154/1000 | Loss: 0.00002011
Iteration 155/1000 | Loss: 0.00002011
Iteration 156/1000 | Loss: 0.00002011
Iteration 157/1000 | Loss: 0.00002011
Iteration 158/1000 | Loss: 0.00002011
Iteration 159/1000 | Loss: 0.00002011
Iteration 160/1000 | Loss: 0.00002011
Iteration 161/1000 | Loss: 0.00002011
Iteration 162/1000 | Loss: 0.00002011
Iteration 163/1000 | Loss: 0.00002011
Iteration 164/1000 | Loss: 0.00002011
Iteration 165/1000 | Loss: 0.00002010
Iteration 166/1000 | Loss: 0.00002010
Iteration 167/1000 | Loss: 0.00002010
Iteration 168/1000 | Loss: 0.00002010
Iteration 169/1000 | Loss: 0.00002010
Iteration 170/1000 | Loss: 0.00002010
Iteration 171/1000 | Loss: 0.00002010
Iteration 172/1000 | Loss: 0.00002010
Iteration 173/1000 | Loss: 0.00002010
Iteration 174/1000 | Loss: 0.00002010
Iteration 175/1000 | Loss: 0.00002010
Iteration 176/1000 | Loss: 0.00002010
Iteration 177/1000 | Loss: 0.00002010
Iteration 178/1000 | Loss: 0.00002010
Iteration 179/1000 | Loss: 0.00002010
Iteration 180/1000 | Loss: 0.00002010
Iteration 181/1000 | Loss: 0.00002010
Iteration 182/1000 | Loss: 0.00002010
Iteration 183/1000 | Loss: 0.00002010
Iteration 184/1000 | Loss: 0.00002010
Iteration 185/1000 | Loss: 0.00002010
Iteration 186/1000 | Loss: 0.00002010
Iteration 187/1000 | Loss: 0.00002010
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [2.009586023632437e-05, 2.009586023632437e-05, 2.009586023632437e-05, 2.009586023632437e-05, 2.009586023632437e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.009586023632437e-05

Optimization complete. Final v2v error: 3.7609026432037354 mm

Highest mean error: 4.092648029327393 mm for frame 6

Lowest mean error: 3.5276083946228027 mm for frame 34

Saving results

Total time: 37.082619190216064
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00899460
Iteration 2/25 | Loss: 0.00189637
Iteration 3/25 | Loss: 0.00103920
Iteration 4/25 | Loss: 0.00092597
Iteration 5/25 | Loss: 0.00086772
Iteration 6/25 | Loss: 0.00089663
Iteration 7/25 | Loss: 0.00085071
Iteration 8/25 | Loss: 0.00082803
Iteration 9/25 | Loss: 0.00080890
Iteration 10/25 | Loss: 0.00080425
Iteration 11/25 | Loss: 0.00079829
Iteration 12/25 | Loss: 0.00079684
Iteration 13/25 | Loss: 0.00079650
Iteration 14/25 | Loss: 0.00079641
Iteration 15/25 | Loss: 0.00079641
Iteration 16/25 | Loss: 0.00079641
Iteration 17/25 | Loss: 0.00079641
Iteration 18/25 | Loss: 0.00079641
Iteration 19/25 | Loss: 0.00079641
Iteration 20/25 | Loss: 0.00079641
Iteration 21/25 | Loss: 0.00079641
Iteration 22/25 | Loss: 0.00079641
Iteration 23/25 | Loss: 0.00079641
Iteration 24/25 | Loss: 0.00079641
Iteration 25/25 | Loss: 0.00079641

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.36968350
Iteration 2/25 | Loss: 0.00051046
Iteration 3/25 | Loss: 0.00051045
Iteration 4/25 | Loss: 0.00051045
Iteration 5/25 | Loss: 0.00051045
Iteration 6/25 | Loss: 0.00051045
Iteration 7/25 | Loss: 0.00051045
Iteration 8/25 | Loss: 0.00051045
Iteration 9/25 | Loss: 0.00051045
Iteration 10/25 | Loss: 0.00051045
Iteration 11/25 | Loss: 0.00051045
Iteration 12/25 | Loss: 0.00051045
Iteration 13/25 | Loss: 0.00051045
Iteration 14/25 | Loss: 0.00051045
Iteration 15/25 | Loss: 0.00051045
Iteration 16/25 | Loss: 0.00051045
Iteration 17/25 | Loss: 0.00051045
Iteration 18/25 | Loss: 0.00051045
Iteration 19/25 | Loss: 0.00051045
Iteration 20/25 | Loss: 0.00051045
Iteration 21/25 | Loss: 0.00051045
Iteration 22/25 | Loss: 0.00051045
Iteration 23/25 | Loss: 0.00051045
Iteration 24/25 | Loss: 0.00051045
Iteration 25/25 | Loss: 0.00051045

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051045
Iteration 2/1000 | Loss: 0.00003320
Iteration 3/1000 | Loss: 0.00002828
Iteration 4/1000 | Loss: 0.00002679
Iteration 5/1000 | Loss: 0.00002576
Iteration 6/1000 | Loss: 0.00002524
Iteration 7/1000 | Loss: 0.00042447
Iteration 8/1000 | Loss: 0.00002551
Iteration 9/1000 | Loss: 0.00002369
Iteration 10/1000 | Loss: 0.00002303
Iteration 11/1000 | Loss: 0.00002266
Iteration 12/1000 | Loss: 0.00002241
Iteration 13/1000 | Loss: 0.00002223
Iteration 14/1000 | Loss: 0.00002223
Iteration 15/1000 | Loss: 0.00002220
Iteration 16/1000 | Loss: 0.00002217
Iteration 17/1000 | Loss: 0.00002217
Iteration 18/1000 | Loss: 0.00002212
Iteration 19/1000 | Loss: 0.00002206
Iteration 20/1000 | Loss: 0.00002200
Iteration 21/1000 | Loss: 0.00002184
Iteration 22/1000 | Loss: 0.00002184
Iteration 23/1000 | Loss: 0.00002184
Iteration 24/1000 | Loss: 0.00002183
Iteration 25/1000 | Loss: 0.00002183
Iteration 26/1000 | Loss: 0.00002182
Iteration 27/1000 | Loss: 0.00002180
Iteration 28/1000 | Loss: 0.00002179
Iteration 29/1000 | Loss: 0.00002178
Iteration 30/1000 | Loss: 0.00002178
Iteration 31/1000 | Loss: 0.00002178
Iteration 32/1000 | Loss: 0.00002177
Iteration 33/1000 | Loss: 0.00002177
Iteration 34/1000 | Loss: 0.00002177
Iteration 35/1000 | Loss: 0.00002176
Iteration 36/1000 | Loss: 0.00002176
Iteration 37/1000 | Loss: 0.00002176
Iteration 38/1000 | Loss: 0.00002174
Iteration 39/1000 | Loss: 0.00002173
Iteration 40/1000 | Loss: 0.00002173
Iteration 41/1000 | Loss: 0.00002173
Iteration 42/1000 | Loss: 0.00002173
Iteration 43/1000 | Loss: 0.00002173
Iteration 44/1000 | Loss: 0.00002172
Iteration 45/1000 | Loss: 0.00002172
Iteration 46/1000 | Loss: 0.00002172
Iteration 47/1000 | Loss: 0.00002171
Iteration 48/1000 | Loss: 0.00002169
Iteration 49/1000 | Loss: 0.00002168
Iteration 50/1000 | Loss: 0.00002168
Iteration 51/1000 | Loss: 0.00002168
Iteration 52/1000 | Loss: 0.00002168
Iteration 53/1000 | Loss: 0.00002167
Iteration 54/1000 | Loss: 0.00002167
Iteration 55/1000 | Loss: 0.00002167
Iteration 56/1000 | Loss: 0.00002166
Iteration 57/1000 | Loss: 0.00002166
Iteration 58/1000 | Loss: 0.00002166
Iteration 59/1000 | Loss: 0.00002165
Iteration 60/1000 | Loss: 0.00002165
Iteration 61/1000 | Loss: 0.00002165
Iteration 62/1000 | Loss: 0.00002165
Iteration 63/1000 | Loss: 0.00002164
Iteration 64/1000 | Loss: 0.00002164
Iteration 65/1000 | Loss: 0.00002164
Iteration 66/1000 | Loss: 0.00002164
Iteration 67/1000 | Loss: 0.00002164
Iteration 68/1000 | Loss: 0.00002164
Iteration 69/1000 | Loss: 0.00002164
Iteration 70/1000 | Loss: 0.00002164
Iteration 71/1000 | Loss: 0.00002164
Iteration 72/1000 | Loss: 0.00002164
Iteration 73/1000 | Loss: 0.00002164
Iteration 74/1000 | Loss: 0.00002164
Iteration 75/1000 | Loss: 0.00002164
Iteration 76/1000 | Loss: 0.00002163
Iteration 77/1000 | Loss: 0.00002163
Iteration 78/1000 | Loss: 0.00002163
Iteration 79/1000 | Loss: 0.00002163
Iteration 80/1000 | Loss: 0.00002163
Iteration 81/1000 | Loss: 0.00002163
Iteration 82/1000 | Loss: 0.00002163
Iteration 83/1000 | Loss: 0.00002163
Iteration 84/1000 | Loss: 0.00002163
Iteration 85/1000 | Loss: 0.00002163
Iteration 86/1000 | Loss: 0.00002162
Iteration 87/1000 | Loss: 0.00002162
Iteration 88/1000 | Loss: 0.00002162
Iteration 89/1000 | Loss: 0.00002162
Iteration 90/1000 | Loss: 0.00002162
Iteration 91/1000 | Loss: 0.00002162
Iteration 92/1000 | Loss: 0.00002161
Iteration 93/1000 | Loss: 0.00002161
Iteration 94/1000 | Loss: 0.00002161
Iteration 95/1000 | Loss: 0.00002161
Iteration 96/1000 | Loss: 0.00002160
Iteration 97/1000 | Loss: 0.00002160
Iteration 98/1000 | Loss: 0.00002160
Iteration 99/1000 | Loss: 0.00002160
Iteration 100/1000 | Loss: 0.00002160
Iteration 101/1000 | Loss: 0.00002160
Iteration 102/1000 | Loss: 0.00002160
Iteration 103/1000 | Loss: 0.00002160
Iteration 104/1000 | Loss: 0.00002160
Iteration 105/1000 | Loss: 0.00002160
Iteration 106/1000 | Loss: 0.00002160
Iteration 107/1000 | Loss: 0.00002159
Iteration 108/1000 | Loss: 0.00002159
Iteration 109/1000 | Loss: 0.00002159
Iteration 110/1000 | Loss: 0.00002159
Iteration 111/1000 | Loss: 0.00002159
Iteration 112/1000 | Loss: 0.00002158
Iteration 113/1000 | Loss: 0.00002158
Iteration 114/1000 | Loss: 0.00002158
Iteration 115/1000 | Loss: 0.00002158
Iteration 116/1000 | Loss: 0.00002158
Iteration 117/1000 | Loss: 0.00002158
Iteration 118/1000 | Loss: 0.00002158
Iteration 119/1000 | Loss: 0.00002158
Iteration 120/1000 | Loss: 0.00002158
Iteration 121/1000 | Loss: 0.00002158
Iteration 122/1000 | Loss: 0.00002158
Iteration 123/1000 | Loss: 0.00002158
Iteration 124/1000 | Loss: 0.00002158
Iteration 125/1000 | Loss: 0.00002158
Iteration 126/1000 | Loss: 0.00002158
Iteration 127/1000 | Loss: 0.00002158
Iteration 128/1000 | Loss: 0.00002158
Iteration 129/1000 | Loss: 0.00002158
Iteration 130/1000 | Loss: 0.00002158
Iteration 131/1000 | Loss: 0.00002158
Iteration 132/1000 | Loss: 0.00002158
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [2.1581845430773683e-05, 2.1581845430773683e-05, 2.1581845430773683e-05, 2.1581845430773683e-05, 2.1581845430773683e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1581845430773683e-05

Optimization complete. Final v2v error: 3.8838295936584473 mm

Highest mean error: 4.6044697761535645 mm for frame 203

Lowest mean error: 3.459519147872925 mm for frame 4

Saving results

Total time: 62.115872859954834
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01136891
Iteration 2/25 | Loss: 0.00157363
Iteration 3/25 | Loss: 0.00101021
Iteration 4/25 | Loss: 0.00094758
Iteration 5/25 | Loss: 0.00093253
Iteration 6/25 | Loss: 0.00092887
Iteration 7/25 | Loss: 0.00092801
Iteration 8/25 | Loss: 0.00092801
Iteration 9/25 | Loss: 0.00092801
Iteration 10/25 | Loss: 0.00092801
Iteration 11/25 | Loss: 0.00092801
Iteration 12/25 | Loss: 0.00092801
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009280128870159388, 0.0009280128870159388, 0.0009280128870159388, 0.0009280128870159388, 0.0009280128870159388]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009280128870159388

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24483061
Iteration 2/25 | Loss: 0.00039074
Iteration 3/25 | Loss: 0.00039071
Iteration 4/25 | Loss: 0.00039071
Iteration 5/25 | Loss: 0.00039071
Iteration 6/25 | Loss: 0.00039071
Iteration 7/25 | Loss: 0.00039071
Iteration 8/25 | Loss: 0.00039071
Iteration 9/25 | Loss: 0.00039071
Iteration 10/25 | Loss: 0.00039071
Iteration 11/25 | Loss: 0.00039071
Iteration 12/25 | Loss: 0.00039071
Iteration 13/25 | Loss: 0.00039071
Iteration 14/25 | Loss: 0.00039071
Iteration 15/25 | Loss: 0.00039071
Iteration 16/25 | Loss: 0.00039071
Iteration 17/25 | Loss: 0.00039071
Iteration 18/25 | Loss: 0.00039071
Iteration 19/25 | Loss: 0.00039071
Iteration 20/25 | Loss: 0.00039071
Iteration 21/25 | Loss: 0.00039071
Iteration 22/25 | Loss: 0.00039071
Iteration 23/25 | Loss: 0.00039071
Iteration 24/25 | Loss: 0.00039071
Iteration 25/25 | Loss: 0.00039071

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039071
Iteration 2/1000 | Loss: 0.00005915
Iteration 3/1000 | Loss: 0.00003978
Iteration 4/1000 | Loss: 0.00003532
Iteration 5/1000 | Loss: 0.00003362
Iteration 6/1000 | Loss: 0.00003277
Iteration 7/1000 | Loss: 0.00003226
Iteration 8/1000 | Loss: 0.00003184
Iteration 9/1000 | Loss: 0.00003158
Iteration 10/1000 | Loss: 0.00003132
Iteration 11/1000 | Loss: 0.00003113
Iteration 12/1000 | Loss: 0.00003097
Iteration 13/1000 | Loss: 0.00003081
Iteration 14/1000 | Loss: 0.00003064
Iteration 15/1000 | Loss: 0.00003055
Iteration 16/1000 | Loss: 0.00003054
Iteration 17/1000 | Loss: 0.00003049
Iteration 18/1000 | Loss: 0.00003043
Iteration 19/1000 | Loss: 0.00003039
Iteration 20/1000 | Loss: 0.00003039
Iteration 21/1000 | Loss: 0.00003038
Iteration 22/1000 | Loss: 0.00003037
Iteration 23/1000 | Loss: 0.00003037
Iteration 24/1000 | Loss: 0.00003036
Iteration 25/1000 | Loss: 0.00003036
Iteration 26/1000 | Loss: 0.00003035
Iteration 27/1000 | Loss: 0.00003034
Iteration 28/1000 | Loss: 0.00003034
Iteration 29/1000 | Loss: 0.00003033
Iteration 30/1000 | Loss: 0.00003033
Iteration 31/1000 | Loss: 0.00003033
Iteration 32/1000 | Loss: 0.00003033
Iteration 33/1000 | Loss: 0.00003032
Iteration 34/1000 | Loss: 0.00003031
Iteration 35/1000 | Loss: 0.00003031
Iteration 36/1000 | Loss: 0.00003031
Iteration 37/1000 | Loss: 0.00003030
Iteration 38/1000 | Loss: 0.00003030
Iteration 39/1000 | Loss: 0.00003030
Iteration 40/1000 | Loss: 0.00003030
Iteration 41/1000 | Loss: 0.00003030
Iteration 42/1000 | Loss: 0.00003029
Iteration 43/1000 | Loss: 0.00003029
Iteration 44/1000 | Loss: 0.00003029
Iteration 45/1000 | Loss: 0.00003029
Iteration 46/1000 | Loss: 0.00003029
Iteration 47/1000 | Loss: 0.00003029
Iteration 48/1000 | Loss: 0.00003029
Iteration 49/1000 | Loss: 0.00003028
Iteration 50/1000 | Loss: 0.00003028
Iteration 51/1000 | Loss: 0.00003028
Iteration 52/1000 | Loss: 0.00003028
Iteration 53/1000 | Loss: 0.00003028
Iteration 54/1000 | Loss: 0.00003028
Iteration 55/1000 | Loss: 0.00003027
Iteration 56/1000 | Loss: 0.00003027
Iteration 57/1000 | Loss: 0.00003027
Iteration 58/1000 | Loss: 0.00003026
Iteration 59/1000 | Loss: 0.00003026
Iteration 60/1000 | Loss: 0.00003026
Iteration 61/1000 | Loss: 0.00003026
Iteration 62/1000 | Loss: 0.00003025
Iteration 63/1000 | Loss: 0.00003025
Iteration 64/1000 | Loss: 0.00003025
Iteration 65/1000 | Loss: 0.00003024
Iteration 66/1000 | Loss: 0.00003024
Iteration 67/1000 | Loss: 0.00003024
Iteration 68/1000 | Loss: 0.00003024
Iteration 69/1000 | Loss: 0.00003024
Iteration 70/1000 | Loss: 0.00003023
Iteration 71/1000 | Loss: 0.00003023
Iteration 72/1000 | Loss: 0.00003023
Iteration 73/1000 | Loss: 0.00003023
Iteration 74/1000 | Loss: 0.00003022
Iteration 75/1000 | Loss: 0.00003022
Iteration 76/1000 | Loss: 0.00003022
Iteration 77/1000 | Loss: 0.00003022
Iteration 78/1000 | Loss: 0.00003021
Iteration 79/1000 | Loss: 0.00003021
Iteration 80/1000 | Loss: 0.00003021
Iteration 81/1000 | Loss: 0.00003020
Iteration 82/1000 | Loss: 0.00003020
Iteration 83/1000 | Loss: 0.00003020
Iteration 84/1000 | Loss: 0.00003019
Iteration 85/1000 | Loss: 0.00003019
Iteration 86/1000 | Loss: 0.00003019
Iteration 87/1000 | Loss: 0.00003019
Iteration 88/1000 | Loss: 0.00003019
Iteration 89/1000 | Loss: 0.00003019
Iteration 90/1000 | Loss: 0.00003019
Iteration 91/1000 | Loss: 0.00003019
Iteration 92/1000 | Loss: 0.00003018
Iteration 93/1000 | Loss: 0.00003018
Iteration 94/1000 | Loss: 0.00003018
Iteration 95/1000 | Loss: 0.00003018
Iteration 96/1000 | Loss: 0.00003018
Iteration 97/1000 | Loss: 0.00003018
Iteration 98/1000 | Loss: 0.00003017
Iteration 99/1000 | Loss: 0.00003017
Iteration 100/1000 | Loss: 0.00003017
Iteration 101/1000 | Loss: 0.00003016
Iteration 102/1000 | Loss: 0.00003016
Iteration 103/1000 | Loss: 0.00003016
Iteration 104/1000 | Loss: 0.00003016
Iteration 105/1000 | Loss: 0.00003016
Iteration 106/1000 | Loss: 0.00003015
Iteration 107/1000 | Loss: 0.00003015
Iteration 108/1000 | Loss: 0.00003015
Iteration 109/1000 | Loss: 0.00003015
Iteration 110/1000 | Loss: 0.00003015
Iteration 111/1000 | Loss: 0.00003015
Iteration 112/1000 | Loss: 0.00003014
Iteration 113/1000 | Loss: 0.00003014
Iteration 114/1000 | Loss: 0.00003014
Iteration 115/1000 | Loss: 0.00003014
Iteration 116/1000 | Loss: 0.00003014
Iteration 117/1000 | Loss: 0.00003014
Iteration 118/1000 | Loss: 0.00003014
Iteration 119/1000 | Loss: 0.00003013
Iteration 120/1000 | Loss: 0.00003013
Iteration 121/1000 | Loss: 0.00003013
Iteration 122/1000 | Loss: 0.00003013
Iteration 123/1000 | Loss: 0.00003012
Iteration 124/1000 | Loss: 0.00003012
Iteration 125/1000 | Loss: 0.00003012
Iteration 126/1000 | Loss: 0.00003012
Iteration 127/1000 | Loss: 0.00003011
Iteration 128/1000 | Loss: 0.00003011
Iteration 129/1000 | Loss: 0.00003011
Iteration 130/1000 | Loss: 0.00003011
Iteration 131/1000 | Loss: 0.00003011
Iteration 132/1000 | Loss: 0.00003011
Iteration 133/1000 | Loss: 0.00003010
Iteration 134/1000 | Loss: 0.00003010
Iteration 135/1000 | Loss: 0.00003010
Iteration 136/1000 | Loss: 0.00003010
Iteration 137/1000 | Loss: 0.00003009
Iteration 138/1000 | Loss: 0.00003009
Iteration 139/1000 | Loss: 0.00003009
Iteration 140/1000 | Loss: 0.00003009
Iteration 141/1000 | Loss: 0.00003009
Iteration 142/1000 | Loss: 0.00003008
Iteration 143/1000 | Loss: 0.00003008
Iteration 144/1000 | Loss: 0.00003008
Iteration 145/1000 | Loss: 0.00003008
Iteration 146/1000 | Loss: 0.00003008
Iteration 147/1000 | Loss: 0.00003008
Iteration 148/1000 | Loss: 0.00003008
Iteration 149/1000 | Loss: 0.00003008
Iteration 150/1000 | Loss: 0.00003008
Iteration 151/1000 | Loss: 0.00003008
Iteration 152/1000 | Loss: 0.00003008
Iteration 153/1000 | Loss: 0.00003008
Iteration 154/1000 | Loss: 0.00003008
Iteration 155/1000 | Loss: 0.00003008
Iteration 156/1000 | Loss: 0.00003007
Iteration 157/1000 | Loss: 0.00003007
Iteration 158/1000 | Loss: 0.00003007
Iteration 159/1000 | Loss: 0.00003007
Iteration 160/1000 | Loss: 0.00003007
Iteration 161/1000 | Loss: 0.00003007
Iteration 162/1000 | Loss: 0.00003007
Iteration 163/1000 | Loss: 0.00003007
Iteration 164/1000 | Loss: 0.00003007
Iteration 165/1000 | Loss: 0.00003006
Iteration 166/1000 | Loss: 0.00003006
Iteration 167/1000 | Loss: 0.00003006
Iteration 168/1000 | Loss: 0.00003006
Iteration 169/1000 | Loss: 0.00003006
Iteration 170/1000 | Loss: 0.00003006
Iteration 171/1000 | Loss: 0.00003006
Iteration 172/1000 | Loss: 0.00003006
Iteration 173/1000 | Loss: 0.00003006
Iteration 174/1000 | Loss: 0.00003006
Iteration 175/1000 | Loss: 0.00003005
Iteration 176/1000 | Loss: 0.00003005
Iteration 177/1000 | Loss: 0.00003005
Iteration 178/1000 | Loss: 0.00003005
Iteration 179/1000 | Loss: 0.00003005
Iteration 180/1000 | Loss: 0.00003005
Iteration 181/1000 | Loss: 0.00003005
Iteration 182/1000 | Loss: 0.00003005
Iteration 183/1000 | Loss: 0.00003005
Iteration 184/1000 | Loss: 0.00003005
Iteration 185/1000 | Loss: 0.00003005
Iteration 186/1000 | Loss: 0.00003005
Iteration 187/1000 | Loss: 0.00003005
Iteration 188/1000 | Loss: 0.00003005
Iteration 189/1000 | Loss: 0.00003005
Iteration 190/1000 | Loss: 0.00003005
Iteration 191/1000 | Loss: 0.00003005
Iteration 192/1000 | Loss: 0.00003005
Iteration 193/1000 | Loss: 0.00003005
Iteration 194/1000 | Loss: 0.00003005
Iteration 195/1000 | Loss: 0.00003004
Iteration 196/1000 | Loss: 0.00003004
Iteration 197/1000 | Loss: 0.00003004
Iteration 198/1000 | Loss: 0.00003004
Iteration 199/1000 | Loss: 0.00003004
Iteration 200/1000 | Loss: 0.00003004
Iteration 201/1000 | Loss: 0.00003004
Iteration 202/1000 | Loss: 0.00003004
Iteration 203/1000 | Loss: 0.00003004
Iteration 204/1000 | Loss: 0.00003004
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [3.0044397135498002e-05, 3.0044397135498002e-05, 3.0044397135498002e-05, 3.0044397135498002e-05, 3.0044397135498002e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0044397135498002e-05

Optimization complete. Final v2v error: 4.362820148468018 mm

Highest mean error: 5.626857757568359 mm for frame 75

Lowest mean error: 3.6386542320251465 mm for frame 55

Saving results

Total time: 51.88938593864441
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00398706
Iteration 2/25 | Loss: 0.00090197
Iteration 3/25 | Loss: 0.00078641
Iteration 4/25 | Loss: 0.00076366
Iteration 5/25 | Loss: 0.00075530
Iteration 6/25 | Loss: 0.00075298
Iteration 7/25 | Loss: 0.00075259
Iteration 8/25 | Loss: 0.00075259
Iteration 9/25 | Loss: 0.00075259
Iteration 10/25 | Loss: 0.00075259
Iteration 11/25 | Loss: 0.00075259
Iteration 12/25 | Loss: 0.00075259
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000752590480260551, 0.000752590480260551, 0.000752590480260551, 0.000752590480260551, 0.000752590480260551]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000752590480260551

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55025983
Iteration 2/25 | Loss: 0.00051669
Iteration 3/25 | Loss: 0.00051669
Iteration 4/25 | Loss: 0.00051669
Iteration 5/25 | Loss: 0.00051669
Iteration 6/25 | Loss: 0.00051669
Iteration 7/25 | Loss: 0.00051669
Iteration 8/25 | Loss: 0.00051669
Iteration 9/25 | Loss: 0.00051669
Iteration 10/25 | Loss: 0.00051669
Iteration 11/25 | Loss: 0.00051669
Iteration 12/25 | Loss: 0.00051669
Iteration 13/25 | Loss: 0.00051669
Iteration 14/25 | Loss: 0.00051669
Iteration 15/25 | Loss: 0.00051669
Iteration 16/25 | Loss: 0.00051669
Iteration 17/25 | Loss: 0.00051669
Iteration 18/25 | Loss: 0.00051669
Iteration 19/25 | Loss: 0.00051669
Iteration 20/25 | Loss: 0.00051669
Iteration 21/25 | Loss: 0.00051669
Iteration 22/25 | Loss: 0.00051669
Iteration 23/25 | Loss: 0.00051669
Iteration 24/25 | Loss: 0.00051669
Iteration 25/25 | Loss: 0.00051669

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051669
Iteration 2/1000 | Loss: 0.00003571
Iteration 3/1000 | Loss: 0.00002171
Iteration 4/1000 | Loss: 0.00001854
Iteration 5/1000 | Loss: 0.00001698
Iteration 6/1000 | Loss: 0.00001616
Iteration 7/1000 | Loss: 0.00001563
Iteration 8/1000 | Loss: 0.00001536
Iteration 9/1000 | Loss: 0.00001517
Iteration 10/1000 | Loss: 0.00001508
Iteration 11/1000 | Loss: 0.00001506
Iteration 12/1000 | Loss: 0.00001505
Iteration 13/1000 | Loss: 0.00001504
Iteration 14/1000 | Loss: 0.00001504
Iteration 15/1000 | Loss: 0.00001492
Iteration 16/1000 | Loss: 0.00001485
Iteration 17/1000 | Loss: 0.00001483
Iteration 18/1000 | Loss: 0.00001483
Iteration 19/1000 | Loss: 0.00001482
Iteration 20/1000 | Loss: 0.00001482
Iteration 21/1000 | Loss: 0.00001481
Iteration 22/1000 | Loss: 0.00001478
Iteration 23/1000 | Loss: 0.00001477
Iteration 24/1000 | Loss: 0.00001477
Iteration 25/1000 | Loss: 0.00001476
Iteration 26/1000 | Loss: 0.00001476
Iteration 27/1000 | Loss: 0.00001476
Iteration 28/1000 | Loss: 0.00001475
Iteration 29/1000 | Loss: 0.00001470
Iteration 30/1000 | Loss: 0.00001467
Iteration 31/1000 | Loss: 0.00001467
Iteration 32/1000 | Loss: 0.00001467
Iteration 33/1000 | Loss: 0.00001466
Iteration 34/1000 | Loss: 0.00001466
Iteration 35/1000 | Loss: 0.00001465
Iteration 36/1000 | Loss: 0.00001465
Iteration 37/1000 | Loss: 0.00001464
Iteration 38/1000 | Loss: 0.00001464
Iteration 39/1000 | Loss: 0.00001463
Iteration 40/1000 | Loss: 0.00001462
Iteration 41/1000 | Loss: 0.00001461
Iteration 42/1000 | Loss: 0.00001460
Iteration 43/1000 | Loss: 0.00001460
Iteration 44/1000 | Loss: 0.00001459
Iteration 45/1000 | Loss: 0.00001459
Iteration 46/1000 | Loss: 0.00001459
Iteration 47/1000 | Loss: 0.00001458
Iteration 48/1000 | Loss: 0.00001458
Iteration 49/1000 | Loss: 0.00001457
Iteration 50/1000 | Loss: 0.00001456
Iteration 51/1000 | Loss: 0.00001455
Iteration 52/1000 | Loss: 0.00001454
Iteration 53/1000 | Loss: 0.00001454
Iteration 54/1000 | Loss: 0.00001453
Iteration 55/1000 | Loss: 0.00001453
Iteration 56/1000 | Loss: 0.00001453
Iteration 57/1000 | Loss: 0.00001453
Iteration 58/1000 | Loss: 0.00001453
Iteration 59/1000 | Loss: 0.00001453
Iteration 60/1000 | Loss: 0.00001453
Iteration 61/1000 | Loss: 0.00001452
Iteration 62/1000 | Loss: 0.00001452
Iteration 63/1000 | Loss: 0.00001451
Iteration 64/1000 | Loss: 0.00001450
Iteration 65/1000 | Loss: 0.00001450
Iteration 66/1000 | Loss: 0.00001450
Iteration 67/1000 | Loss: 0.00001449
Iteration 68/1000 | Loss: 0.00001449
Iteration 69/1000 | Loss: 0.00001449
Iteration 70/1000 | Loss: 0.00001448
Iteration 71/1000 | Loss: 0.00001448
Iteration 72/1000 | Loss: 0.00001448
Iteration 73/1000 | Loss: 0.00001448
Iteration 74/1000 | Loss: 0.00001447
Iteration 75/1000 | Loss: 0.00001447
Iteration 76/1000 | Loss: 0.00001447
Iteration 77/1000 | Loss: 0.00001447
Iteration 78/1000 | Loss: 0.00001447
Iteration 79/1000 | Loss: 0.00001446
Iteration 80/1000 | Loss: 0.00001445
Iteration 81/1000 | Loss: 0.00001445
Iteration 82/1000 | Loss: 0.00001445
Iteration 83/1000 | Loss: 0.00001445
Iteration 84/1000 | Loss: 0.00001445
Iteration 85/1000 | Loss: 0.00001445
Iteration 86/1000 | Loss: 0.00001444
Iteration 87/1000 | Loss: 0.00001444
Iteration 88/1000 | Loss: 0.00001444
Iteration 89/1000 | Loss: 0.00001444
Iteration 90/1000 | Loss: 0.00001444
Iteration 91/1000 | Loss: 0.00001444
Iteration 92/1000 | Loss: 0.00001444
Iteration 93/1000 | Loss: 0.00001444
Iteration 94/1000 | Loss: 0.00001444
Iteration 95/1000 | Loss: 0.00001444
Iteration 96/1000 | Loss: 0.00001443
Iteration 97/1000 | Loss: 0.00001443
Iteration 98/1000 | Loss: 0.00001443
Iteration 99/1000 | Loss: 0.00001443
Iteration 100/1000 | Loss: 0.00001442
Iteration 101/1000 | Loss: 0.00001442
Iteration 102/1000 | Loss: 0.00001442
Iteration 103/1000 | Loss: 0.00001442
Iteration 104/1000 | Loss: 0.00001442
Iteration 105/1000 | Loss: 0.00001442
Iteration 106/1000 | Loss: 0.00001441
Iteration 107/1000 | Loss: 0.00001441
Iteration 108/1000 | Loss: 0.00001441
Iteration 109/1000 | Loss: 0.00001441
Iteration 110/1000 | Loss: 0.00001441
Iteration 111/1000 | Loss: 0.00001441
Iteration 112/1000 | Loss: 0.00001441
Iteration 113/1000 | Loss: 0.00001441
Iteration 114/1000 | Loss: 0.00001440
Iteration 115/1000 | Loss: 0.00001440
Iteration 116/1000 | Loss: 0.00001440
Iteration 117/1000 | Loss: 0.00001440
Iteration 118/1000 | Loss: 0.00001440
Iteration 119/1000 | Loss: 0.00001440
Iteration 120/1000 | Loss: 0.00001440
Iteration 121/1000 | Loss: 0.00001440
Iteration 122/1000 | Loss: 0.00001440
Iteration 123/1000 | Loss: 0.00001439
Iteration 124/1000 | Loss: 0.00001439
Iteration 125/1000 | Loss: 0.00001439
Iteration 126/1000 | Loss: 0.00001439
Iteration 127/1000 | Loss: 0.00001439
Iteration 128/1000 | Loss: 0.00001439
Iteration 129/1000 | Loss: 0.00001439
Iteration 130/1000 | Loss: 0.00001439
Iteration 131/1000 | Loss: 0.00001439
Iteration 132/1000 | Loss: 0.00001438
Iteration 133/1000 | Loss: 0.00001438
Iteration 134/1000 | Loss: 0.00001438
Iteration 135/1000 | Loss: 0.00001438
Iteration 136/1000 | Loss: 0.00001438
Iteration 137/1000 | Loss: 0.00001438
Iteration 138/1000 | Loss: 0.00001438
Iteration 139/1000 | Loss: 0.00001438
Iteration 140/1000 | Loss: 0.00001438
Iteration 141/1000 | Loss: 0.00001437
Iteration 142/1000 | Loss: 0.00001437
Iteration 143/1000 | Loss: 0.00001437
Iteration 144/1000 | Loss: 0.00001437
Iteration 145/1000 | Loss: 0.00001437
Iteration 146/1000 | Loss: 0.00001437
Iteration 147/1000 | Loss: 0.00001437
Iteration 148/1000 | Loss: 0.00001437
Iteration 149/1000 | Loss: 0.00001437
Iteration 150/1000 | Loss: 0.00001437
Iteration 151/1000 | Loss: 0.00001436
Iteration 152/1000 | Loss: 0.00001436
Iteration 153/1000 | Loss: 0.00001436
Iteration 154/1000 | Loss: 0.00001436
Iteration 155/1000 | Loss: 0.00001435
Iteration 156/1000 | Loss: 0.00001435
Iteration 157/1000 | Loss: 0.00001435
Iteration 158/1000 | Loss: 0.00001434
Iteration 159/1000 | Loss: 0.00001434
Iteration 160/1000 | Loss: 0.00001434
Iteration 161/1000 | Loss: 0.00001434
Iteration 162/1000 | Loss: 0.00001434
Iteration 163/1000 | Loss: 0.00001434
Iteration 164/1000 | Loss: 0.00001434
Iteration 165/1000 | Loss: 0.00001433
Iteration 166/1000 | Loss: 0.00001433
Iteration 167/1000 | Loss: 0.00001433
Iteration 168/1000 | Loss: 0.00001433
Iteration 169/1000 | Loss: 0.00001433
Iteration 170/1000 | Loss: 0.00001433
Iteration 171/1000 | Loss: 0.00001433
Iteration 172/1000 | Loss: 0.00001433
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.4334614206745755e-05, 1.4334614206745755e-05, 1.4334614206745755e-05, 1.4334614206745755e-05, 1.4334614206745755e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4334614206745755e-05

Optimization complete. Final v2v error: 3.1544198989868164 mm

Highest mean error: 3.7216241359710693 mm for frame 118

Lowest mean error: 2.718940496444702 mm for frame 191

Saving results

Total time: 43.231034994125366
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00647968
Iteration 2/25 | Loss: 0.00124072
Iteration 3/25 | Loss: 0.00095399
Iteration 4/25 | Loss: 0.00087967
Iteration 5/25 | Loss: 0.00085434
Iteration 6/25 | Loss: 0.00085425
Iteration 7/25 | Loss: 0.00084121
Iteration 8/25 | Loss: 0.00084034
Iteration 9/25 | Loss: 0.00084007
Iteration 10/25 | Loss: 0.00083995
Iteration 11/25 | Loss: 0.00083990
Iteration 12/25 | Loss: 0.00083990
Iteration 13/25 | Loss: 0.00083990
Iteration 14/25 | Loss: 0.00083989
Iteration 15/25 | Loss: 0.00083989
Iteration 16/25 | Loss: 0.00083989
Iteration 17/25 | Loss: 0.00083989
Iteration 18/25 | Loss: 0.00083988
Iteration 19/25 | Loss: 0.00083988
Iteration 20/25 | Loss: 0.00083988
Iteration 21/25 | Loss: 0.00083988
Iteration 22/25 | Loss: 0.00083988
Iteration 23/25 | Loss: 0.00083988
Iteration 24/25 | Loss: 0.00083988
Iteration 25/25 | Loss: 0.00083988

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.78919864
Iteration 2/25 | Loss: 0.00079809
Iteration 3/25 | Loss: 0.00079809
Iteration 4/25 | Loss: 0.00079808
Iteration 5/25 | Loss: 0.00079808
Iteration 6/25 | Loss: 0.00079808
Iteration 7/25 | Loss: 0.00079808
Iteration 8/25 | Loss: 0.00079808
Iteration 9/25 | Loss: 0.00079808
Iteration 10/25 | Loss: 0.00079808
Iteration 11/25 | Loss: 0.00079808
Iteration 12/25 | Loss: 0.00079808
Iteration 13/25 | Loss: 0.00079808
Iteration 14/25 | Loss: 0.00079808
Iteration 15/25 | Loss: 0.00079808
Iteration 16/25 | Loss: 0.00079808
Iteration 17/25 | Loss: 0.00079808
Iteration 18/25 | Loss: 0.00079808
Iteration 19/25 | Loss: 0.00079808
Iteration 20/25 | Loss: 0.00079808
Iteration 21/25 | Loss: 0.00079808
Iteration 22/25 | Loss: 0.00079808
Iteration 23/25 | Loss: 0.00079808
Iteration 24/25 | Loss: 0.00079808
Iteration 25/25 | Loss: 0.00079808

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079808
Iteration 2/1000 | Loss: 0.00012573
Iteration 3/1000 | Loss: 0.00006032
Iteration 4/1000 | Loss: 0.00004609
Iteration 5/1000 | Loss: 0.00003988
Iteration 6/1000 | Loss: 0.00003692
Iteration 7/1000 | Loss: 0.00003469
Iteration 8/1000 | Loss: 0.00107871
Iteration 9/1000 | Loss: 0.00004531
Iteration 10/1000 | Loss: 0.00003622
Iteration 11/1000 | Loss: 0.00003293
Iteration 12/1000 | Loss: 0.00003165
Iteration 13/1000 | Loss: 0.00003041
Iteration 14/1000 | Loss: 0.00002952
Iteration 15/1000 | Loss: 0.00002887
Iteration 16/1000 | Loss: 0.00002845
Iteration 17/1000 | Loss: 0.00002801
Iteration 18/1000 | Loss: 0.00002781
Iteration 19/1000 | Loss: 0.00002762
Iteration 20/1000 | Loss: 0.00002754
Iteration 21/1000 | Loss: 0.00002752
Iteration 22/1000 | Loss: 0.00002746
Iteration 23/1000 | Loss: 0.00002744
Iteration 24/1000 | Loss: 0.00002728
Iteration 25/1000 | Loss: 0.00002727
Iteration 26/1000 | Loss: 0.00002709
Iteration 27/1000 | Loss: 0.00002690
Iteration 28/1000 | Loss: 0.00002680
Iteration 29/1000 | Loss: 0.00002663
Iteration 30/1000 | Loss: 0.00002637
Iteration 31/1000 | Loss: 0.00002607
Iteration 32/1000 | Loss: 0.00002580
Iteration 33/1000 | Loss: 0.00002548
Iteration 34/1000 | Loss: 0.00002512
Iteration 35/1000 | Loss: 0.00002488
Iteration 36/1000 | Loss: 0.00002464
Iteration 37/1000 | Loss: 0.00002457
Iteration 38/1000 | Loss: 0.00002456
Iteration 39/1000 | Loss: 0.00002452
Iteration 40/1000 | Loss: 0.00002444
Iteration 41/1000 | Loss: 0.00002444
Iteration 42/1000 | Loss: 0.00002442
Iteration 43/1000 | Loss: 0.00002441
Iteration 44/1000 | Loss: 0.00002441
Iteration 45/1000 | Loss: 0.00002441
Iteration 46/1000 | Loss: 0.00002440
Iteration 47/1000 | Loss: 0.00002438
Iteration 48/1000 | Loss: 0.00002438
Iteration 49/1000 | Loss: 0.00002436
Iteration 50/1000 | Loss: 0.00002435
Iteration 51/1000 | Loss: 0.00002435
Iteration 52/1000 | Loss: 0.00002435
Iteration 53/1000 | Loss: 0.00002435
Iteration 54/1000 | Loss: 0.00002434
Iteration 55/1000 | Loss: 0.00002434
Iteration 56/1000 | Loss: 0.00002433
Iteration 57/1000 | Loss: 0.00002433
Iteration 58/1000 | Loss: 0.00002432
Iteration 59/1000 | Loss: 0.00002432
Iteration 60/1000 | Loss: 0.00002432
Iteration 61/1000 | Loss: 0.00002432
Iteration 62/1000 | Loss: 0.00002432
Iteration 63/1000 | Loss: 0.00002432
Iteration 64/1000 | Loss: 0.00002432
Iteration 65/1000 | Loss: 0.00002432
Iteration 66/1000 | Loss: 0.00002432
Iteration 67/1000 | Loss: 0.00002432
Iteration 68/1000 | Loss: 0.00002432
Iteration 69/1000 | Loss: 0.00002431
Iteration 70/1000 | Loss: 0.00002431
Iteration 71/1000 | Loss: 0.00002431
Iteration 72/1000 | Loss: 0.00002431
Iteration 73/1000 | Loss: 0.00002431
Iteration 74/1000 | Loss: 0.00002431
Iteration 75/1000 | Loss: 0.00002430
Iteration 76/1000 | Loss: 0.00002430
Iteration 77/1000 | Loss: 0.00002429
Iteration 78/1000 | Loss: 0.00002429
Iteration 79/1000 | Loss: 0.00002429
Iteration 80/1000 | Loss: 0.00002429
Iteration 81/1000 | Loss: 0.00002429
Iteration 82/1000 | Loss: 0.00002429
Iteration 83/1000 | Loss: 0.00002429
Iteration 84/1000 | Loss: 0.00002428
Iteration 85/1000 | Loss: 0.00002428
Iteration 86/1000 | Loss: 0.00002428
Iteration 87/1000 | Loss: 0.00002428
Iteration 88/1000 | Loss: 0.00002428
Iteration 89/1000 | Loss: 0.00002427
Iteration 90/1000 | Loss: 0.00002427
Iteration 91/1000 | Loss: 0.00002427
Iteration 92/1000 | Loss: 0.00002427
Iteration 93/1000 | Loss: 0.00002427
Iteration 94/1000 | Loss: 0.00002427
Iteration 95/1000 | Loss: 0.00002426
Iteration 96/1000 | Loss: 0.00002426
Iteration 97/1000 | Loss: 0.00002426
Iteration 98/1000 | Loss: 0.00002426
Iteration 99/1000 | Loss: 0.00002426
Iteration 100/1000 | Loss: 0.00002426
Iteration 101/1000 | Loss: 0.00002426
Iteration 102/1000 | Loss: 0.00002426
Iteration 103/1000 | Loss: 0.00002426
Iteration 104/1000 | Loss: 0.00002426
Iteration 105/1000 | Loss: 0.00002426
Iteration 106/1000 | Loss: 0.00002426
Iteration 107/1000 | Loss: 0.00002426
Iteration 108/1000 | Loss: 0.00002426
Iteration 109/1000 | Loss: 0.00002426
Iteration 110/1000 | Loss: 0.00002426
Iteration 111/1000 | Loss: 0.00002426
Iteration 112/1000 | Loss: 0.00002426
Iteration 113/1000 | Loss: 0.00002426
Iteration 114/1000 | Loss: 0.00002426
Iteration 115/1000 | Loss: 0.00002426
Iteration 116/1000 | Loss: 0.00002426
Iteration 117/1000 | Loss: 0.00002426
Iteration 118/1000 | Loss: 0.00002426
Iteration 119/1000 | Loss: 0.00002426
Iteration 120/1000 | Loss: 0.00002426
Iteration 121/1000 | Loss: 0.00002426
Iteration 122/1000 | Loss: 0.00002426
Iteration 123/1000 | Loss: 0.00002426
Iteration 124/1000 | Loss: 0.00002426
Iteration 125/1000 | Loss: 0.00002426
Iteration 126/1000 | Loss: 0.00002426
Iteration 127/1000 | Loss: 0.00002426
Iteration 128/1000 | Loss: 0.00002426
Iteration 129/1000 | Loss: 0.00002426
Iteration 130/1000 | Loss: 0.00002426
Iteration 131/1000 | Loss: 0.00002426
Iteration 132/1000 | Loss: 0.00002426
Iteration 133/1000 | Loss: 0.00002426
Iteration 134/1000 | Loss: 0.00002426
Iteration 135/1000 | Loss: 0.00002426
Iteration 136/1000 | Loss: 0.00002426
Iteration 137/1000 | Loss: 0.00002426
Iteration 138/1000 | Loss: 0.00002426
Iteration 139/1000 | Loss: 0.00002426
Iteration 140/1000 | Loss: 0.00002426
Iteration 141/1000 | Loss: 0.00002426
Iteration 142/1000 | Loss: 0.00002426
Iteration 143/1000 | Loss: 0.00002426
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [2.4261513317469507e-05, 2.4261513317469507e-05, 2.4261513317469507e-05, 2.4261513317469507e-05, 2.4261513317469507e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4261513317469507e-05

Optimization complete. Final v2v error: 4.155425071716309 mm

Highest mean error: 5.506767272949219 mm for frame 63

Lowest mean error: 3.467759609222412 mm for frame 77

Saving results

Total time: 70.88537168502808
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00530903
Iteration 2/25 | Loss: 0.00116537
Iteration 3/25 | Loss: 0.00085257
Iteration 4/25 | Loss: 0.00081394
Iteration 5/25 | Loss: 0.00080351
Iteration 6/25 | Loss: 0.00080130
Iteration 7/25 | Loss: 0.00080091
Iteration 8/25 | Loss: 0.00080091
Iteration 9/25 | Loss: 0.00080091
Iteration 10/25 | Loss: 0.00080091
Iteration 11/25 | Loss: 0.00080091
Iteration 12/25 | Loss: 0.00080091
Iteration 13/25 | Loss: 0.00080091
Iteration 14/25 | Loss: 0.00080091
Iteration 15/25 | Loss: 0.00080091
Iteration 16/25 | Loss: 0.00080091
Iteration 17/25 | Loss: 0.00080091
Iteration 18/25 | Loss: 0.00080091
Iteration 19/25 | Loss: 0.00080091
Iteration 20/25 | Loss: 0.00080091
Iteration 21/25 | Loss: 0.00080091
Iteration 22/25 | Loss: 0.00080091
Iteration 23/25 | Loss: 0.00080091
Iteration 24/25 | Loss: 0.00080091
Iteration 25/25 | Loss: 0.00080091

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47269166
Iteration 2/25 | Loss: 0.00048417
Iteration 3/25 | Loss: 0.00048416
Iteration 4/25 | Loss: 0.00048416
Iteration 5/25 | Loss: 0.00048415
Iteration 6/25 | Loss: 0.00048415
Iteration 7/25 | Loss: 0.00048415
Iteration 8/25 | Loss: 0.00048415
Iteration 9/25 | Loss: 0.00048415
Iteration 10/25 | Loss: 0.00048415
Iteration 11/25 | Loss: 0.00048415
Iteration 12/25 | Loss: 0.00048415
Iteration 13/25 | Loss: 0.00048415
Iteration 14/25 | Loss: 0.00048415
Iteration 15/25 | Loss: 0.00048415
Iteration 16/25 | Loss: 0.00048415
Iteration 17/25 | Loss: 0.00048415
Iteration 18/25 | Loss: 0.00048415
Iteration 19/25 | Loss: 0.00048415
Iteration 20/25 | Loss: 0.00048415
Iteration 21/25 | Loss: 0.00048415
Iteration 22/25 | Loss: 0.00048415
Iteration 23/25 | Loss: 0.00048415
Iteration 24/25 | Loss: 0.00048415
Iteration 25/25 | Loss: 0.00048415

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00048415
Iteration 2/1000 | Loss: 0.00003433
Iteration 3/1000 | Loss: 0.00002597
Iteration 4/1000 | Loss: 0.00002347
Iteration 5/1000 | Loss: 0.00002218
Iteration 6/1000 | Loss: 0.00002141
Iteration 7/1000 | Loss: 0.00002079
Iteration 8/1000 | Loss: 0.00002034
Iteration 9/1000 | Loss: 0.00002003
Iteration 10/1000 | Loss: 0.00001981
Iteration 11/1000 | Loss: 0.00001959
Iteration 12/1000 | Loss: 0.00001957
Iteration 13/1000 | Loss: 0.00001943
Iteration 14/1000 | Loss: 0.00001940
Iteration 15/1000 | Loss: 0.00001940
Iteration 16/1000 | Loss: 0.00001939
Iteration 17/1000 | Loss: 0.00001939
Iteration 18/1000 | Loss: 0.00001939
Iteration 19/1000 | Loss: 0.00001938
Iteration 20/1000 | Loss: 0.00001938
Iteration 21/1000 | Loss: 0.00001935
Iteration 22/1000 | Loss: 0.00001934
Iteration 23/1000 | Loss: 0.00001933
Iteration 24/1000 | Loss: 0.00001932
Iteration 25/1000 | Loss: 0.00001931
Iteration 26/1000 | Loss: 0.00001931
Iteration 27/1000 | Loss: 0.00001931
Iteration 28/1000 | Loss: 0.00001931
Iteration 29/1000 | Loss: 0.00001931
Iteration 30/1000 | Loss: 0.00001931
Iteration 31/1000 | Loss: 0.00001931
Iteration 32/1000 | Loss: 0.00001931
Iteration 33/1000 | Loss: 0.00001931
Iteration 34/1000 | Loss: 0.00001931
Iteration 35/1000 | Loss: 0.00001931
Iteration 36/1000 | Loss: 0.00001930
Iteration 37/1000 | Loss: 0.00001930
Iteration 38/1000 | Loss: 0.00001930
Iteration 39/1000 | Loss: 0.00001929
Iteration 40/1000 | Loss: 0.00001929
Iteration 41/1000 | Loss: 0.00001929
Iteration 42/1000 | Loss: 0.00001929
Iteration 43/1000 | Loss: 0.00001929
Iteration 44/1000 | Loss: 0.00001929
Iteration 45/1000 | Loss: 0.00001929
Iteration 46/1000 | Loss: 0.00001929
Iteration 47/1000 | Loss: 0.00001928
Iteration 48/1000 | Loss: 0.00001928
Iteration 49/1000 | Loss: 0.00001928
Iteration 50/1000 | Loss: 0.00001928
Iteration 51/1000 | Loss: 0.00001928
Iteration 52/1000 | Loss: 0.00001928
Iteration 53/1000 | Loss: 0.00001928
Iteration 54/1000 | Loss: 0.00001927
Iteration 55/1000 | Loss: 0.00001927
Iteration 56/1000 | Loss: 0.00001927
Iteration 57/1000 | Loss: 0.00001926
Iteration 58/1000 | Loss: 0.00001926
Iteration 59/1000 | Loss: 0.00001926
Iteration 60/1000 | Loss: 0.00001926
Iteration 61/1000 | Loss: 0.00001925
Iteration 62/1000 | Loss: 0.00001925
Iteration 63/1000 | Loss: 0.00001925
Iteration 64/1000 | Loss: 0.00001925
Iteration 65/1000 | Loss: 0.00001924
Iteration 66/1000 | Loss: 0.00001924
Iteration 67/1000 | Loss: 0.00001924
Iteration 68/1000 | Loss: 0.00001924
Iteration 69/1000 | Loss: 0.00001923
Iteration 70/1000 | Loss: 0.00001923
Iteration 71/1000 | Loss: 0.00001923
Iteration 72/1000 | Loss: 0.00001922
Iteration 73/1000 | Loss: 0.00001922
Iteration 74/1000 | Loss: 0.00001922
Iteration 75/1000 | Loss: 0.00001921
Iteration 76/1000 | Loss: 0.00001921
Iteration 77/1000 | Loss: 0.00001921
Iteration 78/1000 | Loss: 0.00001921
Iteration 79/1000 | Loss: 0.00001921
Iteration 80/1000 | Loss: 0.00001921
Iteration 81/1000 | Loss: 0.00001921
Iteration 82/1000 | Loss: 0.00001921
Iteration 83/1000 | Loss: 0.00001921
Iteration 84/1000 | Loss: 0.00001921
Iteration 85/1000 | Loss: 0.00001920
Iteration 86/1000 | Loss: 0.00001920
Iteration 87/1000 | Loss: 0.00001920
Iteration 88/1000 | Loss: 0.00001920
Iteration 89/1000 | Loss: 0.00001920
Iteration 90/1000 | Loss: 0.00001920
Iteration 91/1000 | Loss: 0.00001920
Iteration 92/1000 | Loss: 0.00001920
Iteration 93/1000 | Loss: 0.00001920
Iteration 94/1000 | Loss: 0.00001920
Iteration 95/1000 | Loss: 0.00001919
Iteration 96/1000 | Loss: 0.00001919
Iteration 97/1000 | Loss: 0.00001919
Iteration 98/1000 | Loss: 0.00001919
Iteration 99/1000 | Loss: 0.00001919
Iteration 100/1000 | Loss: 0.00001919
Iteration 101/1000 | Loss: 0.00001919
Iteration 102/1000 | Loss: 0.00001918
Iteration 103/1000 | Loss: 0.00001918
Iteration 104/1000 | Loss: 0.00001918
Iteration 105/1000 | Loss: 0.00001918
Iteration 106/1000 | Loss: 0.00001918
Iteration 107/1000 | Loss: 0.00001918
Iteration 108/1000 | Loss: 0.00001918
Iteration 109/1000 | Loss: 0.00001918
Iteration 110/1000 | Loss: 0.00001918
Iteration 111/1000 | Loss: 0.00001917
Iteration 112/1000 | Loss: 0.00001917
Iteration 113/1000 | Loss: 0.00001917
Iteration 114/1000 | Loss: 0.00001917
Iteration 115/1000 | Loss: 0.00001917
Iteration 116/1000 | Loss: 0.00001917
Iteration 117/1000 | Loss: 0.00001917
Iteration 118/1000 | Loss: 0.00001917
Iteration 119/1000 | Loss: 0.00001917
Iteration 120/1000 | Loss: 0.00001917
Iteration 121/1000 | Loss: 0.00001917
Iteration 122/1000 | Loss: 0.00001917
Iteration 123/1000 | Loss: 0.00001917
Iteration 124/1000 | Loss: 0.00001917
Iteration 125/1000 | Loss: 0.00001917
Iteration 126/1000 | Loss: 0.00001917
Iteration 127/1000 | Loss: 0.00001917
Iteration 128/1000 | Loss: 0.00001916
Iteration 129/1000 | Loss: 0.00001916
Iteration 130/1000 | Loss: 0.00001916
Iteration 131/1000 | Loss: 0.00001916
Iteration 132/1000 | Loss: 0.00001916
Iteration 133/1000 | Loss: 0.00001916
Iteration 134/1000 | Loss: 0.00001916
Iteration 135/1000 | Loss: 0.00001916
Iteration 136/1000 | Loss: 0.00001916
Iteration 137/1000 | Loss: 0.00001916
Iteration 138/1000 | Loss: 0.00001915
Iteration 139/1000 | Loss: 0.00001915
Iteration 140/1000 | Loss: 0.00001915
Iteration 141/1000 | Loss: 0.00001915
Iteration 142/1000 | Loss: 0.00001915
Iteration 143/1000 | Loss: 0.00001915
Iteration 144/1000 | Loss: 0.00001915
Iteration 145/1000 | Loss: 0.00001915
Iteration 146/1000 | Loss: 0.00001915
Iteration 147/1000 | Loss: 0.00001915
Iteration 148/1000 | Loss: 0.00001915
Iteration 149/1000 | Loss: 0.00001915
Iteration 150/1000 | Loss: 0.00001915
Iteration 151/1000 | Loss: 0.00001915
Iteration 152/1000 | Loss: 0.00001915
Iteration 153/1000 | Loss: 0.00001915
Iteration 154/1000 | Loss: 0.00001915
Iteration 155/1000 | Loss: 0.00001915
Iteration 156/1000 | Loss: 0.00001915
Iteration 157/1000 | Loss: 0.00001915
Iteration 158/1000 | Loss: 0.00001915
Iteration 159/1000 | Loss: 0.00001915
Iteration 160/1000 | Loss: 0.00001915
Iteration 161/1000 | Loss: 0.00001915
Iteration 162/1000 | Loss: 0.00001915
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.915105531224981e-05, 1.915105531224981e-05, 1.915105531224981e-05, 1.915105531224981e-05, 1.915105531224981e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.915105531224981e-05

Optimization complete. Final v2v error: 3.6451334953308105 mm

Highest mean error: 3.9836721420288086 mm for frame 81

Lowest mean error: 3.21720552444458 mm for frame 103

Saving results

Total time: 37.28134727478027
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00859148
Iteration 2/25 | Loss: 0.00120758
Iteration 3/25 | Loss: 0.00086027
Iteration 4/25 | Loss: 0.00078748
Iteration 5/25 | Loss: 0.00076752
Iteration 6/25 | Loss: 0.00076238
Iteration 7/25 | Loss: 0.00076102
Iteration 8/25 | Loss: 0.00076062
Iteration 9/25 | Loss: 0.00076062
Iteration 10/25 | Loss: 0.00076062
Iteration 11/25 | Loss: 0.00076062
Iteration 12/25 | Loss: 0.00076062
Iteration 13/25 | Loss: 0.00076062
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007606218568980694, 0.0007606218568980694, 0.0007606218568980694, 0.0007606218568980694, 0.0007606218568980694]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007606218568980694

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52108812
Iteration 2/25 | Loss: 0.00047784
Iteration 3/25 | Loss: 0.00047784
Iteration 4/25 | Loss: 0.00047784
Iteration 5/25 | Loss: 0.00047784
Iteration 6/25 | Loss: 0.00047784
Iteration 7/25 | Loss: 0.00047784
Iteration 8/25 | Loss: 0.00047784
Iteration 9/25 | Loss: 0.00047784
Iteration 10/25 | Loss: 0.00047784
Iteration 11/25 | Loss: 0.00047784
Iteration 12/25 | Loss: 0.00047784
Iteration 13/25 | Loss: 0.00047784
Iteration 14/25 | Loss: 0.00047784
Iteration 15/25 | Loss: 0.00047784
Iteration 16/25 | Loss: 0.00047784
Iteration 17/25 | Loss: 0.00047784
Iteration 18/25 | Loss: 0.00047784
Iteration 19/25 | Loss: 0.00047784
Iteration 20/25 | Loss: 0.00047784
Iteration 21/25 | Loss: 0.00047784
Iteration 22/25 | Loss: 0.00047784
Iteration 23/25 | Loss: 0.00047784
Iteration 24/25 | Loss: 0.00047784
Iteration 25/25 | Loss: 0.00047784

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047784
Iteration 2/1000 | Loss: 0.00002336
Iteration 3/1000 | Loss: 0.00001599
Iteration 4/1000 | Loss: 0.00001448
Iteration 5/1000 | Loss: 0.00001373
Iteration 6/1000 | Loss: 0.00001294
Iteration 7/1000 | Loss: 0.00001264
Iteration 8/1000 | Loss: 0.00001242
Iteration 9/1000 | Loss: 0.00001235
Iteration 10/1000 | Loss: 0.00001235
Iteration 11/1000 | Loss: 0.00001232
Iteration 12/1000 | Loss: 0.00001221
Iteration 13/1000 | Loss: 0.00001213
Iteration 14/1000 | Loss: 0.00001209
Iteration 15/1000 | Loss: 0.00001208
Iteration 16/1000 | Loss: 0.00001206
Iteration 17/1000 | Loss: 0.00001205
Iteration 18/1000 | Loss: 0.00001205
Iteration 19/1000 | Loss: 0.00001204
Iteration 20/1000 | Loss: 0.00001203
Iteration 21/1000 | Loss: 0.00001203
Iteration 22/1000 | Loss: 0.00001202
Iteration 23/1000 | Loss: 0.00001201
Iteration 24/1000 | Loss: 0.00001200
Iteration 25/1000 | Loss: 0.00001200
Iteration 26/1000 | Loss: 0.00001200
Iteration 27/1000 | Loss: 0.00001199
Iteration 28/1000 | Loss: 0.00001199
Iteration 29/1000 | Loss: 0.00001199
Iteration 30/1000 | Loss: 0.00001198
Iteration 31/1000 | Loss: 0.00001198
Iteration 32/1000 | Loss: 0.00001196
Iteration 33/1000 | Loss: 0.00001196
Iteration 34/1000 | Loss: 0.00001195
Iteration 35/1000 | Loss: 0.00001195
Iteration 36/1000 | Loss: 0.00001194
Iteration 37/1000 | Loss: 0.00001194
Iteration 38/1000 | Loss: 0.00001193
Iteration 39/1000 | Loss: 0.00001192
Iteration 40/1000 | Loss: 0.00001192
Iteration 41/1000 | Loss: 0.00001192
Iteration 42/1000 | Loss: 0.00001192
Iteration 43/1000 | Loss: 0.00001192
Iteration 44/1000 | Loss: 0.00001192
Iteration 45/1000 | Loss: 0.00001191
Iteration 46/1000 | Loss: 0.00001191
Iteration 47/1000 | Loss: 0.00001191
Iteration 48/1000 | Loss: 0.00001191
Iteration 49/1000 | Loss: 0.00001191
Iteration 50/1000 | Loss: 0.00001190
Iteration 51/1000 | Loss: 0.00001190
Iteration 52/1000 | Loss: 0.00001190
Iteration 53/1000 | Loss: 0.00001190
Iteration 54/1000 | Loss: 0.00001190
Iteration 55/1000 | Loss: 0.00001189
Iteration 56/1000 | Loss: 0.00001189
Iteration 57/1000 | Loss: 0.00001189
Iteration 58/1000 | Loss: 0.00001189
Iteration 59/1000 | Loss: 0.00001189
Iteration 60/1000 | Loss: 0.00001189
Iteration 61/1000 | Loss: 0.00001189
Iteration 62/1000 | Loss: 0.00001189
Iteration 63/1000 | Loss: 0.00001189
Iteration 64/1000 | Loss: 0.00001189
Iteration 65/1000 | Loss: 0.00001189
Iteration 66/1000 | Loss: 0.00001189
Iteration 67/1000 | Loss: 0.00001189
Iteration 68/1000 | Loss: 0.00001189
Iteration 69/1000 | Loss: 0.00001189
Iteration 70/1000 | Loss: 0.00001189
Iteration 71/1000 | Loss: 0.00001189
Iteration 72/1000 | Loss: 0.00001189
Iteration 73/1000 | Loss: 0.00001189
Iteration 74/1000 | Loss: 0.00001189
Iteration 75/1000 | Loss: 0.00001189
Iteration 76/1000 | Loss: 0.00001189
Iteration 77/1000 | Loss: 0.00001189
Iteration 78/1000 | Loss: 0.00001189
Iteration 79/1000 | Loss: 0.00001189
Iteration 80/1000 | Loss: 0.00001189
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 80. Stopping optimization.
Last 5 losses: [1.1891298527189065e-05, 1.1891298527189065e-05, 1.1891298527189065e-05, 1.1891298527189065e-05, 1.1891298527189065e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1891298527189065e-05

Optimization complete. Final v2v error: 2.934263229370117 mm

Highest mean error: 3.4073734283447266 mm for frame 96

Lowest mean error: 2.7154626846313477 mm for frame 221

Saving results

Total time: 35.96577596664429
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00791270
Iteration 2/25 | Loss: 0.00110108
Iteration 3/25 | Loss: 0.00084329
Iteration 4/25 | Loss: 0.00079161
Iteration 5/25 | Loss: 0.00078174
Iteration 6/25 | Loss: 0.00078055
Iteration 7/25 | Loss: 0.00078054
Iteration 8/25 | Loss: 0.00078054
Iteration 9/25 | Loss: 0.00078054
Iteration 10/25 | Loss: 0.00078054
Iteration 11/25 | Loss: 0.00078055
Iteration 12/25 | Loss: 0.00078054
Iteration 13/25 | Loss: 0.00078054
Iteration 14/25 | Loss: 0.00078054
Iteration 15/25 | Loss: 0.00078054
Iteration 16/25 | Loss: 0.00078054
Iteration 17/25 | Loss: 0.00078054
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007805449422448874, 0.0007805449422448874, 0.0007805449422448874, 0.0007805449422448874, 0.0007805449422448874]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007805449422448874

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49278939
Iteration 2/25 | Loss: 0.00053594
Iteration 3/25 | Loss: 0.00053591
Iteration 4/25 | Loss: 0.00053590
Iteration 5/25 | Loss: 0.00053590
Iteration 6/25 | Loss: 0.00053590
Iteration 7/25 | Loss: 0.00053590
Iteration 8/25 | Loss: 0.00053590
Iteration 9/25 | Loss: 0.00053590
Iteration 10/25 | Loss: 0.00053590
Iteration 11/25 | Loss: 0.00053590
Iteration 12/25 | Loss: 0.00053590
Iteration 13/25 | Loss: 0.00053590
Iteration 14/25 | Loss: 0.00053590
Iteration 15/25 | Loss: 0.00053590
Iteration 16/25 | Loss: 0.00053590
Iteration 17/25 | Loss: 0.00053590
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005359025672078133, 0.0005359025672078133, 0.0005359025672078133, 0.0005359025672078133, 0.0005359025672078133]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005359025672078133

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053590
Iteration 2/1000 | Loss: 0.00004632
Iteration 3/1000 | Loss: 0.00003382
Iteration 4/1000 | Loss: 0.00002780
Iteration 5/1000 | Loss: 0.00002571
Iteration 6/1000 | Loss: 0.00002406
Iteration 7/1000 | Loss: 0.00002314
Iteration 8/1000 | Loss: 0.00002267
Iteration 9/1000 | Loss: 0.00002207
Iteration 10/1000 | Loss: 0.00002187
Iteration 11/1000 | Loss: 0.00002165
Iteration 12/1000 | Loss: 0.00002138
Iteration 13/1000 | Loss: 0.00002117
Iteration 14/1000 | Loss: 0.00002103
Iteration 15/1000 | Loss: 0.00002096
Iteration 16/1000 | Loss: 0.00002095
Iteration 17/1000 | Loss: 0.00002091
Iteration 18/1000 | Loss: 0.00002090
Iteration 19/1000 | Loss: 0.00002090
Iteration 20/1000 | Loss: 0.00002090
Iteration 21/1000 | Loss: 0.00002090
Iteration 22/1000 | Loss: 0.00002089
Iteration 23/1000 | Loss: 0.00002087
Iteration 24/1000 | Loss: 0.00002087
Iteration 25/1000 | Loss: 0.00002086
Iteration 26/1000 | Loss: 0.00002086
Iteration 27/1000 | Loss: 0.00002086
Iteration 28/1000 | Loss: 0.00002086
Iteration 29/1000 | Loss: 0.00002085
Iteration 30/1000 | Loss: 0.00002085
Iteration 31/1000 | Loss: 0.00002085
Iteration 32/1000 | Loss: 0.00002084
Iteration 33/1000 | Loss: 0.00002084
Iteration 34/1000 | Loss: 0.00002084
Iteration 35/1000 | Loss: 0.00002083
Iteration 36/1000 | Loss: 0.00002083
Iteration 37/1000 | Loss: 0.00002083
Iteration 38/1000 | Loss: 0.00002082
Iteration 39/1000 | Loss: 0.00002082
Iteration 40/1000 | Loss: 0.00002082
Iteration 41/1000 | Loss: 0.00002082
Iteration 42/1000 | Loss: 0.00002080
Iteration 43/1000 | Loss: 0.00002080
Iteration 44/1000 | Loss: 0.00002080
Iteration 45/1000 | Loss: 0.00002080
Iteration 46/1000 | Loss: 0.00002080
Iteration 47/1000 | Loss: 0.00002080
Iteration 48/1000 | Loss: 0.00002080
Iteration 49/1000 | Loss: 0.00002080
Iteration 50/1000 | Loss: 0.00002080
Iteration 51/1000 | Loss: 0.00002079
Iteration 52/1000 | Loss: 0.00002079
Iteration 53/1000 | Loss: 0.00002077
Iteration 54/1000 | Loss: 0.00002076
Iteration 55/1000 | Loss: 0.00002075
Iteration 56/1000 | Loss: 0.00002075
Iteration 57/1000 | Loss: 0.00002075
Iteration 58/1000 | Loss: 0.00002074
Iteration 59/1000 | Loss: 0.00002074
Iteration 60/1000 | Loss: 0.00002074
Iteration 61/1000 | Loss: 0.00002073
Iteration 62/1000 | Loss: 0.00002073
Iteration 63/1000 | Loss: 0.00002072
Iteration 64/1000 | Loss: 0.00002072
Iteration 65/1000 | Loss: 0.00002072
Iteration 66/1000 | Loss: 0.00002072
Iteration 67/1000 | Loss: 0.00002072
Iteration 68/1000 | Loss: 0.00002072
Iteration 69/1000 | Loss: 0.00002072
Iteration 70/1000 | Loss: 0.00002072
Iteration 71/1000 | Loss: 0.00002072
Iteration 72/1000 | Loss: 0.00002071
Iteration 73/1000 | Loss: 0.00002071
Iteration 74/1000 | Loss: 0.00002071
Iteration 75/1000 | Loss: 0.00002071
Iteration 76/1000 | Loss: 0.00002071
Iteration 77/1000 | Loss: 0.00002071
Iteration 78/1000 | Loss: 0.00002070
Iteration 79/1000 | Loss: 0.00002070
Iteration 80/1000 | Loss: 0.00002070
Iteration 81/1000 | Loss: 0.00002070
Iteration 82/1000 | Loss: 0.00002070
Iteration 83/1000 | Loss: 0.00002070
Iteration 84/1000 | Loss: 0.00002070
Iteration 85/1000 | Loss: 0.00002070
Iteration 86/1000 | Loss: 0.00002070
Iteration 87/1000 | Loss: 0.00002070
Iteration 88/1000 | Loss: 0.00002070
Iteration 89/1000 | Loss: 0.00002070
Iteration 90/1000 | Loss: 0.00002069
Iteration 91/1000 | Loss: 0.00002069
Iteration 92/1000 | Loss: 0.00002069
Iteration 93/1000 | Loss: 0.00002069
Iteration 94/1000 | Loss: 0.00002069
Iteration 95/1000 | Loss: 0.00002069
Iteration 96/1000 | Loss: 0.00002068
Iteration 97/1000 | Loss: 0.00002068
Iteration 98/1000 | Loss: 0.00002068
Iteration 99/1000 | Loss: 0.00002068
Iteration 100/1000 | Loss: 0.00002068
Iteration 101/1000 | Loss: 0.00002068
Iteration 102/1000 | Loss: 0.00002068
Iteration 103/1000 | Loss: 0.00002067
Iteration 104/1000 | Loss: 0.00002067
Iteration 105/1000 | Loss: 0.00002067
Iteration 106/1000 | Loss: 0.00002067
Iteration 107/1000 | Loss: 0.00002067
Iteration 108/1000 | Loss: 0.00002067
Iteration 109/1000 | Loss: 0.00002067
Iteration 110/1000 | Loss: 0.00002067
Iteration 111/1000 | Loss: 0.00002067
Iteration 112/1000 | Loss: 0.00002067
Iteration 113/1000 | Loss: 0.00002067
Iteration 114/1000 | Loss: 0.00002067
Iteration 115/1000 | Loss: 0.00002067
Iteration 116/1000 | Loss: 0.00002067
Iteration 117/1000 | Loss: 0.00002067
Iteration 118/1000 | Loss: 0.00002067
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [2.0670169760705903e-05, 2.0670169760705903e-05, 2.0670169760705903e-05, 2.0670169760705903e-05, 2.0670169760705903e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0670169760705903e-05

Optimization complete. Final v2v error: 3.906076192855835 mm

Highest mean error: 4.330262184143066 mm for frame 204

Lowest mean error: 3.559499740600586 mm for frame 42

Saving results

Total time: 41.072242736816406
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01045513
Iteration 2/25 | Loss: 0.00205274
Iteration 3/25 | Loss: 0.00187407
Iteration 4/25 | Loss: 0.00151143
Iteration 5/25 | Loss: 0.00116371
Iteration 6/25 | Loss: 0.00134713
Iteration 7/25 | Loss: 0.00121057
Iteration 8/25 | Loss: 0.00099754
Iteration 9/25 | Loss: 0.00093577
Iteration 10/25 | Loss: 0.00092115
Iteration 11/25 | Loss: 0.00090143
Iteration 12/25 | Loss: 0.00088678
Iteration 13/25 | Loss: 0.00088765
Iteration 14/25 | Loss: 0.00088174
Iteration 15/25 | Loss: 0.00087999
Iteration 16/25 | Loss: 0.00088063
Iteration 17/25 | Loss: 0.00088119
Iteration 18/25 | Loss: 0.00087369
Iteration 19/25 | Loss: 0.00086702
Iteration 20/25 | Loss: 0.00085695
Iteration 21/25 | Loss: 0.00086018
Iteration 22/25 | Loss: 0.00086365
Iteration 23/25 | Loss: 0.00085298
Iteration 24/25 | Loss: 0.00085453
Iteration 25/25 | Loss: 0.00086651

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53013444
Iteration 2/25 | Loss: 0.00089136
Iteration 3/25 | Loss: 0.00089136
Iteration 4/25 | Loss: 0.00089136
Iteration 5/25 | Loss: 0.00089136
Iteration 6/25 | Loss: 0.00089136
Iteration 7/25 | Loss: 0.00089136
Iteration 8/25 | Loss: 0.00089136
Iteration 9/25 | Loss: 0.00089136
Iteration 10/25 | Loss: 0.00089136
Iteration 11/25 | Loss: 0.00089136
Iteration 12/25 | Loss: 0.00089136
Iteration 13/25 | Loss: 0.00089136
Iteration 14/25 | Loss: 0.00089136
Iteration 15/25 | Loss: 0.00089136
Iteration 16/25 | Loss: 0.00089136
Iteration 17/25 | Loss: 0.00089136
Iteration 18/25 | Loss: 0.00089136
Iteration 19/25 | Loss: 0.00089136
Iteration 20/25 | Loss: 0.00089136
Iteration 21/25 | Loss: 0.00089136
Iteration 22/25 | Loss: 0.00089136
Iteration 23/25 | Loss: 0.00089136
Iteration 24/25 | Loss: 0.00089136
Iteration 25/25 | Loss: 0.00089136
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.000891359057277441, 0.000891359057277441, 0.000891359057277441, 0.000891359057277441, 0.000891359057277441]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000891359057277441

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089136
Iteration 2/1000 | Loss: 0.00019623
Iteration 3/1000 | Loss: 0.00048081
Iteration 4/1000 | Loss: 0.00055064
Iteration 5/1000 | Loss: 0.00045172
Iteration 6/1000 | Loss: 0.00032024
Iteration 7/1000 | Loss: 0.00043635
Iteration 8/1000 | Loss: 0.00035572
Iteration 9/1000 | Loss: 0.00048920
Iteration 10/1000 | Loss: 0.00011506
Iteration 11/1000 | Loss: 0.00034934
Iteration 12/1000 | Loss: 0.00037621
Iteration 13/1000 | Loss: 0.00020191
Iteration 14/1000 | Loss: 0.00033101
Iteration 15/1000 | Loss: 0.00025746
Iteration 16/1000 | Loss: 0.00013768
Iteration 17/1000 | Loss: 0.00026261
Iteration 18/1000 | Loss: 0.00013210
Iteration 19/1000 | Loss: 0.00008492
Iteration 20/1000 | Loss: 0.00033070
Iteration 21/1000 | Loss: 0.00006988
Iteration 22/1000 | Loss: 0.00004210
Iteration 23/1000 | Loss: 0.00003727
Iteration 24/1000 | Loss: 0.00009899
Iteration 25/1000 | Loss: 0.00004923
Iteration 26/1000 | Loss: 0.00003138
Iteration 27/1000 | Loss: 0.00069276
Iteration 28/1000 | Loss: 0.00059119
Iteration 29/1000 | Loss: 0.00073196
Iteration 30/1000 | Loss: 0.00028067
Iteration 31/1000 | Loss: 0.00030404
Iteration 32/1000 | Loss: 0.00006112
Iteration 33/1000 | Loss: 0.00002768
Iteration 34/1000 | Loss: 0.00002503
Iteration 35/1000 | Loss: 0.00002305
Iteration 36/1000 | Loss: 0.00002202
Iteration 37/1000 | Loss: 0.00002135
Iteration 38/1000 | Loss: 0.00022485
Iteration 39/1000 | Loss: 0.00014199
Iteration 40/1000 | Loss: 0.00023254
Iteration 41/1000 | Loss: 0.00023247
Iteration 42/1000 | Loss: 0.00006996
Iteration 43/1000 | Loss: 0.00021668
Iteration 44/1000 | Loss: 0.00008207
Iteration 45/1000 | Loss: 0.00029718
Iteration 46/1000 | Loss: 0.00023093
Iteration 47/1000 | Loss: 0.00025505
Iteration 48/1000 | Loss: 0.00020854
Iteration 49/1000 | Loss: 0.00025969
Iteration 50/1000 | Loss: 0.00011296
Iteration 51/1000 | Loss: 0.00025679
Iteration 52/1000 | Loss: 0.00004343
Iteration 53/1000 | Loss: 0.00022262
Iteration 54/1000 | Loss: 0.00003199
Iteration 55/1000 | Loss: 0.00063187
Iteration 56/1000 | Loss: 0.00017756
Iteration 57/1000 | Loss: 0.00015597
Iteration 58/1000 | Loss: 0.00016481
Iteration 59/1000 | Loss: 0.00020349
Iteration 60/1000 | Loss: 0.00004171
Iteration 61/1000 | Loss: 0.00023353
Iteration 62/1000 | Loss: 0.00005689
Iteration 63/1000 | Loss: 0.00003378
Iteration 64/1000 | Loss: 0.00002957
Iteration 65/1000 | Loss: 0.00002780
Iteration 66/1000 | Loss: 0.00002691
Iteration 67/1000 | Loss: 0.00025920
Iteration 68/1000 | Loss: 0.00036958
Iteration 69/1000 | Loss: 0.00029380
Iteration 70/1000 | Loss: 0.00044910
Iteration 71/1000 | Loss: 0.00002868
Iteration 72/1000 | Loss: 0.00002692
Iteration 73/1000 | Loss: 0.00036488
Iteration 74/1000 | Loss: 0.00004335
Iteration 75/1000 | Loss: 0.00003004
Iteration 76/1000 | Loss: 0.00106065
Iteration 77/1000 | Loss: 0.00029691
Iteration 78/1000 | Loss: 0.00076773
Iteration 79/1000 | Loss: 0.00006134
Iteration 80/1000 | Loss: 0.00005309
Iteration 81/1000 | Loss: 0.00005969
Iteration 82/1000 | Loss: 0.00003741
Iteration 83/1000 | Loss: 0.00005411
Iteration 84/1000 | Loss: 0.00002510
Iteration 85/1000 | Loss: 0.00002279
Iteration 86/1000 | Loss: 0.00002159
Iteration 87/1000 | Loss: 0.00002070
Iteration 88/1000 | Loss: 0.00002009
Iteration 89/1000 | Loss: 0.00001979
Iteration 90/1000 | Loss: 0.00001951
Iteration 91/1000 | Loss: 0.00002266
Iteration 92/1000 | Loss: 0.00001945
Iteration 93/1000 | Loss: 0.00001898
Iteration 94/1000 | Loss: 0.00001873
Iteration 95/1000 | Loss: 0.00001862
Iteration 96/1000 | Loss: 0.00001847
Iteration 97/1000 | Loss: 0.00001824
Iteration 98/1000 | Loss: 0.00001814
Iteration 99/1000 | Loss: 0.00001810
Iteration 100/1000 | Loss: 0.00001807
Iteration 101/1000 | Loss: 0.00001807
Iteration 102/1000 | Loss: 0.00001807
Iteration 103/1000 | Loss: 0.00001806
Iteration 104/1000 | Loss: 0.00001806
Iteration 105/1000 | Loss: 0.00001806
Iteration 106/1000 | Loss: 0.00001805
Iteration 107/1000 | Loss: 0.00001803
Iteration 108/1000 | Loss: 0.00001797
Iteration 109/1000 | Loss: 0.00001797
Iteration 110/1000 | Loss: 0.00001797
Iteration 111/1000 | Loss: 0.00001796
Iteration 112/1000 | Loss: 0.00001796
Iteration 113/1000 | Loss: 0.00001795
Iteration 114/1000 | Loss: 0.00001795
Iteration 115/1000 | Loss: 0.00001795
Iteration 116/1000 | Loss: 0.00001794
Iteration 117/1000 | Loss: 0.00001794
Iteration 118/1000 | Loss: 0.00001794
Iteration 119/1000 | Loss: 0.00001793
Iteration 120/1000 | Loss: 0.00001793
Iteration 121/1000 | Loss: 0.00001793
Iteration 122/1000 | Loss: 0.00001793
Iteration 123/1000 | Loss: 0.00001792
Iteration 124/1000 | Loss: 0.00001792
Iteration 125/1000 | Loss: 0.00001792
Iteration 126/1000 | Loss: 0.00001792
Iteration 127/1000 | Loss: 0.00001792
Iteration 128/1000 | Loss: 0.00001791
Iteration 129/1000 | Loss: 0.00001791
Iteration 130/1000 | Loss: 0.00001791
Iteration 131/1000 | Loss: 0.00001791
Iteration 132/1000 | Loss: 0.00001791
Iteration 133/1000 | Loss: 0.00001791
Iteration 134/1000 | Loss: 0.00001791
Iteration 135/1000 | Loss: 0.00001791
Iteration 136/1000 | Loss: 0.00001791
Iteration 137/1000 | Loss: 0.00001791
Iteration 138/1000 | Loss: 0.00001791
Iteration 139/1000 | Loss: 0.00001791
Iteration 140/1000 | Loss: 0.00001791
Iteration 141/1000 | Loss: 0.00001791
Iteration 142/1000 | Loss: 0.00001791
Iteration 143/1000 | Loss: 0.00001791
Iteration 144/1000 | Loss: 0.00001790
Iteration 145/1000 | Loss: 0.00001790
Iteration 146/1000 | Loss: 0.00001790
Iteration 147/1000 | Loss: 0.00001790
Iteration 148/1000 | Loss: 0.00001790
Iteration 149/1000 | Loss: 0.00001790
Iteration 150/1000 | Loss: 0.00001790
Iteration 151/1000 | Loss: 0.00001790
Iteration 152/1000 | Loss: 0.00001790
Iteration 153/1000 | Loss: 0.00001790
Iteration 154/1000 | Loss: 0.00001790
Iteration 155/1000 | Loss: 0.00001790
Iteration 156/1000 | Loss: 0.00001790
Iteration 157/1000 | Loss: 0.00001790
Iteration 158/1000 | Loss: 0.00001790
Iteration 159/1000 | Loss: 0.00001790
Iteration 160/1000 | Loss: 0.00001790
Iteration 161/1000 | Loss: 0.00001790
Iteration 162/1000 | Loss: 0.00001790
Iteration 163/1000 | Loss: 0.00001790
Iteration 164/1000 | Loss: 0.00001790
Iteration 165/1000 | Loss: 0.00001790
Iteration 166/1000 | Loss: 0.00001790
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [1.790288843039889e-05, 1.790288843039889e-05, 1.790288843039889e-05, 1.790288843039889e-05, 1.790288843039889e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.790288843039889e-05

Optimization complete. Final v2v error: 3.5505945682525635 mm

Highest mean error: 4.267642974853516 mm for frame 61

Lowest mean error: 2.901029586791992 mm for frame 43

Saving results

Total time: 183.1677782535553
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00661173
Iteration 2/25 | Loss: 0.00114589
Iteration 3/25 | Loss: 0.00088927
Iteration 4/25 | Loss: 0.00084901
Iteration 5/25 | Loss: 0.00084009
Iteration 6/25 | Loss: 0.00083741
Iteration 7/25 | Loss: 0.00083678
Iteration 8/25 | Loss: 0.00083678
Iteration 9/25 | Loss: 0.00083678
Iteration 10/25 | Loss: 0.00083678
Iteration 11/25 | Loss: 0.00083678
Iteration 12/25 | Loss: 0.00083678
Iteration 13/25 | Loss: 0.00083678
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008367798291146755, 0.0008367798291146755, 0.0008367798291146755, 0.0008367798291146755, 0.0008367798291146755]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008367798291146755

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67378473
Iteration 2/25 | Loss: 0.00062729
Iteration 3/25 | Loss: 0.00062729
Iteration 4/25 | Loss: 0.00062729
Iteration 5/25 | Loss: 0.00062729
Iteration 6/25 | Loss: 0.00062729
Iteration 7/25 | Loss: 0.00062729
Iteration 8/25 | Loss: 0.00062729
Iteration 9/25 | Loss: 0.00062729
Iteration 10/25 | Loss: 0.00062729
Iteration 11/25 | Loss: 0.00062729
Iteration 12/25 | Loss: 0.00062729
Iteration 13/25 | Loss: 0.00062729
Iteration 14/25 | Loss: 0.00062729
Iteration 15/25 | Loss: 0.00062729
Iteration 16/25 | Loss: 0.00062729
Iteration 17/25 | Loss: 0.00062729
Iteration 18/25 | Loss: 0.00062729
Iteration 19/25 | Loss: 0.00062729
Iteration 20/25 | Loss: 0.00062729
Iteration 21/25 | Loss: 0.00062729
Iteration 22/25 | Loss: 0.00062729
Iteration 23/25 | Loss: 0.00062729
Iteration 24/25 | Loss: 0.00062729
Iteration 25/25 | Loss: 0.00062729

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062729
Iteration 2/1000 | Loss: 0.00005348
Iteration 3/1000 | Loss: 0.00003160
Iteration 4/1000 | Loss: 0.00002570
Iteration 5/1000 | Loss: 0.00002337
Iteration 6/1000 | Loss: 0.00002209
Iteration 7/1000 | Loss: 0.00002118
Iteration 8/1000 | Loss: 0.00002051
Iteration 9/1000 | Loss: 0.00002012
Iteration 10/1000 | Loss: 0.00001988
Iteration 11/1000 | Loss: 0.00001970
Iteration 12/1000 | Loss: 0.00001967
Iteration 13/1000 | Loss: 0.00001951
Iteration 14/1000 | Loss: 0.00001949
Iteration 15/1000 | Loss: 0.00001939
Iteration 16/1000 | Loss: 0.00001933
Iteration 17/1000 | Loss: 0.00001932
Iteration 18/1000 | Loss: 0.00001931
Iteration 19/1000 | Loss: 0.00001931
Iteration 20/1000 | Loss: 0.00001930
Iteration 21/1000 | Loss: 0.00001929
Iteration 22/1000 | Loss: 0.00001928
Iteration 23/1000 | Loss: 0.00001924
Iteration 24/1000 | Loss: 0.00001919
Iteration 25/1000 | Loss: 0.00001917
Iteration 26/1000 | Loss: 0.00001916
Iteration 27/1000 | Loss: 0.00001916
Iteration 28/1000 | Loss: 0.00001915
Iteration 29/1000 | Loss: 0.00001915
Iteration 30/1000 | Loss: 0.00001914
Iteration 31/1000 | Loss: 0.00001914
Iteration 32/1000 | Loss: 0.00001913
Iteration 33/1000 | Loss: 0.00001913
Iteration 34/1000 | Loss: 0.00001910
Iteration 35/1000 | Loss: 0.00001908
Iteration 36/1000 | Loss: 0.00001907
Iteration 37/1000 | Loss: 0.00001906
Iteration 38/1000 | Loss: 0.00001905
Iteration 39/1000 | Loss: 0.00001905
Iteration 40/1000 | Loss: 0.00001904
Iteration 41/1000 | Loss: 0.00001904
Iteration 42/1000 | Loss: 0.00001903
Iteration 43/1000 | Loss: 0.00001902
Iteration 44/1000 | Loss: 0.00001902
Iteration 45/1000 | Loss: 0.00001902
Iteration 46/1000 | Loss: 0.00001901
Iteration 47/1000 | Loss: 0.00001901
Iteration 48/1000 | Loss: 0.00001900
Iteration 49/1000 | Loss: 0.00001900
Iteration 50/1000 | Loss: 0.00001900
Iteration 51/1000 | Loss: 0.00001899
Iteration 52/1000 | Loss: 0.00001898
Iteration 53/1000 | Loss: 0.00001898
Iteration 54/1000 | Loss: 0.00001898
Iteration 55/1000 | Loss: 0.00001897
Iteration 56/1000 | Loss: 0.00001897
Iteration 57/1000 | Loss: 0.00001897
Iteration 58/1000 | Loss: 0.00001896
Iteration 59/1000 | Loss: 0.00001896
Iteration 60/1000 | Loss: 0.00001896
Iteration 61/1000 | Loss: 0.00001895
Iteration 62/1000 | Loss: 0.00001895
Iteration 63/1000 | Loss: 0.00001895
Iteration 64/1000 | Loss: 0.00001894
Iteration 65/1000 | Loss: 0.00001894
Iteration 66/1000 | Loss: 0.00001894
Iteration 67/1000 | Loss: 0.00001894
Iteration 68/1000 | Loss: 0.00001893
Iteration 69/1000 | Loss: 0.00001893
Iteration 70/1000 | Loss: 0.00001893
Iteration 71/1000 | Loss: 0.00001892
Iteration 72/1000 | Loss: 0.00001892
Iteration 73/1000 | Loss: 0.00001892
Iteration 74/1000 | Loss: 0.00001891
Iteration 75/1000 | Loss: 0.00001891
Iteration 76/1000 | Loss: 0.00001891
Iteration 77/1000 | Loss: 0.00001891
Iteration 78/1000 | Loss: 0.00001890
Iteration 79/1000 | Loss: 0.00001890
Iteration 80/1000 | Loss: 0.00001890
Iteration 81/1000 | Loss: 0.00001890
Iteration 82/1000 | Loss: 0.00001890
Iteration 83/1000 | Loss: 0.00001890
Iteration 84/1000 | Loss: 0.00001890
Iteration 85/1000 | Loss: 0.00001890
Iteration 86/1000 | Loss: 0.00001890
Iteration 87/1000 | Loss: 0.00001889
Iteration 88/1000 | Loss: 0.00001889
Iteration 89/1000 | Loss: 0.00001889
Iteration 90/1000 | Loss: 0.00001889
Iteration 91/1000 | Loss: 0.00001888
Iteration 92/1000 | Loss: 0.00001888
Iteration 93/1000 | Loss: 0.00001888
Iteration 94/1000 | Loss: 0.00001888
Iteration 95/1000 | Loss: 0.00001887
Iteration 96/1000 | Loss: 0.00001887
Iteration 97/1000 | Loss: 0.00001887
Iteration 98/1000 | Loss: 0.00001887
Iteration 99/1000 | Loss: 0.00001887
Iteration 100/1000 | Loss: 0.00001886
Iteration 101/1000 | Loss: 0.00001886
Iteration 102/1000 | Loss: 0.00001886
Iteration 103/1000 | Loss: 0.00001885
Iteration 104/1000 | Loss: 0.00001885
Iteration 105/1000 | Loss: 0.00001885
Iteration 106/1000 | Loss: 0.00001885
Iteration 107/1000 | Loss: 0.00001885
Iteration 108/1000 | Loss: 0.00001885
Iteration 109/1000 | Loss: 0.00001885
Iteration 110/1000 | Loss: 0.00001885
Iteration 111/1000 | Loss: 0.00001884
Iteration 112/1000 | Loss: 0.00001884
Iteration 113/1000 | Loss: 0.00001884
Iteration 114/1000 | Loss: 0.00001884
Iteration 115/1000 | Loss: 0.00001884
Iteration 116/1000 | Loss: 0.00001884
Iteration 117/1000 | Loss: 0.00001884
Iteration 118/1000 | Loss: 0.00001884
Iteration 119/1000 | Loss: 0.00001884
Iteration 120/1000 | Loss: 0.00001883
Iteration 121/1000 | Loss: 0.00001883
Iteration 122/1000 | Loss: 0.00001883
Iteration 123/1000 | Loss: 0.00001883
Iteration 124/1000 | Loss: 0.00001883
Iteration 125/1000 | Loss: 0.00001883
Iteration 126/1000 | Loss: 0.00001883
Iteration 127/1000 | Loss: 0.00001883
Iteration 128/1000 | Loss: 0.00001883
Iteration 129/1000 | Loss: 0.00001883
Iteration 130/1000 | Loss: 0.00001883
Iteration 131/1000 | Loss: 0.00001882
Iteration 132/1000 | Loss: 0.00001882
Iteration 133/1000 | Loss: 0.00001882
Iteration 134/1000 | Loss: 0.00001882
Iteration 135/1000 | Loss: 0.00001881
Iteration 136/1000 | Loss: 0.00001881
Iteration 137/1000 | Loss: 0.00001881
Iteration 138/1000 | Loss: 0.00001881
Iteration 139/1000 | Loss: 0.00001881
Iteration 140/1000 | Loss: 0.00001881
Iteration 141/1000 | Loss: 0.00001881
Iteration 142/1000 | Loss: 0.00001880
Iteration 143/1000 | Loss: 0.00001880
Iteration 144/1000 | Loss: 0.00001880
Iteration 145/1000 | Loss: 0.00001880
Iteration 146/1000 | Loss: 0.00001880
Iteration 147/1000 | Loss: 0.00001880
Iteration 148/1000 | Loss: 0.00001880
Iteration 149/1000 | Loss: 0.00001880
Iteration 150/1000 | Loss: 0.00001880
Iteration 151/1000 | Loss: 0.00001880
Iteration 152/1000 | Loss: 0.00001879
Iteration 153/1000 | Loss: 0.00001879
Iteration 154/1000 | Loss: 0.00001879
Iteration 155/1000 | Loss: 0.00001879
Iteration 156/1000 | Loss: 0.00001879
Iteration 157/1000 | Loss: 0.00001879
Iteration 158/1000 | Loss: 0.00001879
Iteration 159/1000 | Loss: 0.00001879
Iteration 160/1000 | Loss: 0.00001879
Iteration 161/1000 | Loss: 0.00001879
Iteration 162/1000 | Loss: 0.00001879
Iteration 163/1000 | Loss: 0.00001879
Iteration 164/1000 | Loss: 0.00001879
Iteration 165/1000 | Loss: 0.00001879
Iteration 166/1000 | Loss: 0.00001879
Iteration 167/1000 | Loss: 0.00001879
Iteration 168/1000 | Loss: 0.00001879
Iteration 169/1000 | Loss: 0.00001879
Iteration 170/1000 | Loss: 0.00001879
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [1.878984949144069e-05, 1.878984949144069e-05, 1.878984949144069e-05, 1.878984949144069e-05, 1.878984949144069e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.878984949144069e-05

Optimization complete. Final v2v error: 3.5092897415161133 mm

Highest mean error: 4.779136657714844 mm for frame 81

Lowest mean error: 2.7115116119384766 mm for frame 38

Saving results

Total time: 47.74583101272583
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00894481
Iteration 2/25 | Loss: 0.00325011
Iteration 3/25 | Loss: 0.00230624
Iteration 4/25 | Loss: 0.00166036
Iteration 5/25 | Loss: 0.00140063
Iteration 6/25 | Loss: 0.00135583
Iteration 7/25 | Loss: 0.00127981
Iteration 8/25 | Loss: 0.00130437
Iteration 9/25 | Loss: 0.00140022
Iteration 10/25 | Loss: 0.00120832
Iteration 11/25 | Loss: 0.00118293
Iteration 12/25 | Loss: 0.00118007
Iteration 13/25 | Loss: 0.00113901
Iteration 14/25 | Loss: 0.00114386
Iteration 15/25 | Loss: 0.00112694
Iteration 16/25 | Loss: 0.00110278
Iteration 17/25 | Loss: 0.00108605
Iteration 18/25 | Loss: 0.00107137
Iteration 19/25 | Loss: 0.00106873
Iteration 20/25 | Loss: 0.00107724
Iteration 21/25 | Loss: 0.00106553
Iteration 22/25 | Loss: 0.00110093
Iteration 23/25 | Loss: 0.00107832
Iteration 24/25 | Loss: 0.00105587
Iteration 25/25 | Loss: 0.00105698

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.55237532
Iteration 2/25 | Loss: 0.00928174
Iteration 3/25 | Loss: 0.00804629
Iteration 4/25 | Loss: 0.00874812
Iteration 5/25 | Loss: 0.00617205
Iteration 6/25 | Loss: 0.00568674
Iteration 7/25 | Loss: 0.00468505
Iteration 8/25 | Loss: 0.00467043
Iteration 9/25 | Loss: 0.00467043
Iteration 10/25 | Loss: 0.00467043
Iteration 11/25 | Loss: 0.00467043
Iteration 12/25 | Loss: 0.00467043
Iteration 13/25 | Loss: 0.00467043
Iteration 14/25 | Loss: 0.00467043
Iteration 15/25 | Loss: 0.00467043
Iteration 16/25 | Loss: 0.00467043
Iteration 17/25 | Loss: 0.00467043
Iteration 18/25 | Loss: 0.00467043
Iteration 19/25 | Loss: 0.00467043
Iteration 20/25 | Loss: 0.00467043
Iteration 21/25 | Loss: 0.00467043
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.004670430440455675, 0.004670430440455675, 0.004670430440455675, 0.004670430440455675, 0.004670430440455675]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004670430440455675

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00467043
Iteration 2/1000 | Loss: 0.00218443
Iteration 3/1000 | Loss: 0.00340092
Iteration 4/1000 | Loss: 0.00639448
Iteration 5/1000 | Loss: 0.00337954
Iteration 6/1000 | Loss: 0.00423617
Iteration 7/1000 | Loss: 0.00070041
Iteration 8/1000 | Loss: 0.00618770
Iteration 9/1000 | Loss: 0.00300881
Iteration 10/1000 | Loss: 0.00645637
Iteration 11/1000 | Loss: 0.00512179
Iteration 12/1000 | Loss: 0.00340458
Iteration 13/1000 | Loss: 0.00393696
Iteration 14/1000 | Loss: 0.00377015
Iteration 15/1000 | Loss: 0.00346868
Iteration 16/1000 | Loss: 0.00398390
Iteration 17/1000 | Loss: 0.00123138
Iteration 18/1000 | Loss: 0.00016911
Iteration 19/1000 | Loss: 0.00263879
Iteration 20/1000 | Loss: 0.00075935
Iteration 21/1000 | Loss: 0.00515864
Iteration 22/1000 | Loss: 0.00145765
Iteration 23/1000 | Loss: 0.00042872
Iteration 24/1000 | Loss: 0.00345111
Iteration 25/1000 | Loss: 0.00399200
Iteration 26/1000 | Loss: 0.00200828
Iteration 27/1000 | Loss: 0.00091926
Iteration 28/1000 | Loss: 0.00067535
Iteration 29/1000 | Loss: 0.00101493
Iteration 30/1000 | Loss: 0.00032757
Iteration 31/1000 | Loss: 0.00021758
Iteration 32/1000 | Loss: 0.00111291
Iteration 33/1000 | Loss: 0.00062089
Iteration 34/1000 | Loss: 0.00262173
Iteration 35/1000 | Loss: 0.00152904
Iteration 36/1000 | Loss: 0.00851716
Iteration 37/1000 | Loss: 0.00261640
Iteration 38/1000 | Loss: 0.00222152
Iteration 39/1000 | Loss: 0.00209493
Iteration 40/1000 | Loss: 0.00094225
Iteration 41/1000 | Loss: 0.00220282
Iteration 42/1000 | Loss: 0.00125978
Iteration 43/1000 | Loss: 0.00127876
Iteration 44/1000 | Loss: 0.00158145
Iteration 45/1000 | Loss: 0.00134801
Iteration 46/1000 | Loss: 0.00253353
Iteration 47/1000 | Loss: 0.00198938
Iteration 48/1000 | Loss: 0.00180284
Iteration 49/1000 | Loss: 0.00246191
Iteration 50/1000 | Loss: 0.00239202
Iteration 51/1000 | Loss: 0.00368911
Iteration 52/1000 | Loss: 0.00110858
Iteration 53/1000 | Loss: 0.00092489
Iteration 54/1000 | Loss: 0.00092390
Iteration 55/1000 | Loss: 0.00311185
Iteration 56/1000 | Loss: 0.00230311
Iteration 57/1000 | Loss: 0.00285388
Iteration 58/1000 | Loss: 0.00229426
Iteration 59/1000 | Loss: 0.00322960
Iteration 60/1000 | Loss: 0.00189485
Iteration 61/1000 | Loss: 0.00306476
Iteration 62/1000 | Loss: 0.00542781
Iteration 63/1000 | Loss: 0.00510267
Iteration 64/1000 | Loss: 0.00217104
Iteration 65/1000 | Loss: 0.00309535
Iteration 66/1000 | Loss: 0.00261539
Iteration 67/1000 | Loss: 0.00335232
Iteration 68/1000 | Loss: 0.00226615
Iteration 69/1000 | Loss: 0.00177099
Iteration 70/1000 | Loss: 0.00145606
Iteration 71/1000 | Loss: 0.00260699
Iteration 72/1000 | Loss: 0.00141624
Iteration 73/1000 | Loss: 0.00206981
Iteration 74/1000 | Loss: 0.00163380
Iteration 75/1000 | Loss: 0.00031439
Iteration 76/1000 | Loss: 0.00077945
Iteration 77/1000 | Loss: 0.00060239
Iteration 78/1000 | Loss: 0.00059555
Iteration 79/1000 | Loss: 0.00122111
Iteration 80/1000 | Loss: 0.00138012
Iteration 81/1000 | Loss: 0.00068450
Iteration 82/1000 | Loss: 0.00270605
Iteration 83/1000 | Loss: 0.00104931
Iteration 84/1000 | Loss: 0.00088111
Iteration 85/1000 | Loss: 0.00036441
Iteration 86/1000 | Loss: 0.00008692
Iteration 87/1000 | Loss: 0.00142136
Iteration 88/1000 | Loss: 0.00072984
Iteration 89/1000 | Loss: 0.00117243
Iteration 90/1000 | Loss: 0.00099460
Iteration 91/1000 | Loss: 0.00090402
Iteration 92/1000 | Loss: 0.00093584
Iteration 93/1000 | Loss: 0.00082022
Iteration 94/1000 | Loss: 0.00062064
Iteration 95/1000 | Loss: 0.00047030
Iteration 96/1000 | Loss: 0.00035748
Iteration 97/1000 | Loss: 0.00070196
Iteration 98/1000 | Loss: 0.00085958
Iteration 99/1000 | Loss: 0.00070105
Iteration 100/1000 | Loss: 0.00060992
Iteration 101/1000 | Loss: 0.00067838
Iteration 102/1000 | Loss: 0.00070661
Iteration 103/1000 | Loss: 0.00175641
Iteration 104/1000 | Loss: 0.00085584
Iteration 105/1000 | Loss: 0.00066045
Iteration 106/1000 | Loss: 0.00057041
Iteration 107/1000 | Loss: 0.00050997
Iteration 108/1000 | Loss: 0.00251764
Iteration 109/1000 | Loss: 0.00047406
Iteration 110/1000 | Loss: 0.00026056
Iteration 111/1000 | Loss: 0.00027672
Iteration 112/1000 | Loss: 0.00038021
Iteration 113/1000 | Loss: 0.00155605
Iteration 114/1000 | Loss: 0.00040940
Iteration 115/1000 | Loss: 0.00064447
Iteration 116/1000 | Loss: 0.00038262
Iteration 117/1000 | Loss: 0.00029063
Iteration 118/1000 | Loss: 0.00035159
Iteration 119/1000 | Loss: 0.00107928
Iteration 120/1000 | Loss: 0.00057289
Iteration 121/1000 | Loss: 0.00037250
Iteration 122/1000 | Loss: 0.00055200
Iteration 123/1000 | Loss: 0.00141604
Iteration 124/1000 | Loss: 0.00046880
Iteration 125/1000 | Loss: 0.00018764
Iteration 126/1000 | Loss: 0.00098211
Iteration 127/1000 | Loss: 0.00104613
Iteration 128/1000 | Loss: 0.00237801
Iteration 129/1000 | Loss: 0.00081497
Iteration 130/1000 | Loss: 0.00075262
Iteration 131/1000 | Loss: 0.00091928
Iteration 132/1000 | Loss: 0.00069175
Iteration 133/1000 | Loss: 0.00090387
Iteration 134/1000 | Loss: 0.00048654
Iteration 135/1000 | Loss: 0.00053636
Iteration 136/1000 | Loss: 0.00043325
Iteration 137/1000 | Loss: 0.00068909
Iteration 138/1000 | Loss: 0.00100703
Iteration 139/1000 | Loss: 0.00044110
Iteration 140/1000 | Loss: 0.00048011
Iteration 141/1000 | Loss: 0.00054379
Iteration 142/1000 | Loss: 0.00027676
Iteration 143/1000 | Loss: 0.00036776
Iteration 144/1000 | Loss: 0.00050744
Iteration 145/1000 | Loss: 0.00017950
Iteration 146/1000 | Loss: 0.00046852
Iteration 147/1000 | Loss: 0.00018633
Iteration 148/1000 | Loss: 0.00004855
Iteration 149/1000 | Loss: 0.00003943
Iteration 150/1000 | Loss: 0.00045993
Iteration 151/1000 | Loss: 0.00027039
Iteration 152/1000 | Loss: 0.00063592
Iteration 153/1000 | Loss: 0.00040480
Iteration 154/1000 | Loss: 0.00050321
Iteration 155/1000 | Loss: 0.00066634
Iteration 156/1000 | Loss: 0.00105513
Iteration 157/1000 | Loss: 0.00056769
Iteration 158/1000 | Loss: 0.00089339
Iteration 159/1000 | Loss: 0.00071934
Iteration 160/1000 | Loss: 0.00005107
Iteration 161/1000 | Loss: 0.00085729
Iteration 162/1000 | Loss: 0.00029636
Iteration 163/1000 | Loss: 0.00036722
Iteration 164/1000 | Loss: 0.00024810
Iteration 165/1000 | Loss: 0.00072392
Iteration 166/1000 | Loss: 0.00127360
Iteration 167/1000 | Loss: 0.00067930
Iteration 168/1000 | Loss: 0.00176682
Iteration 169/1000 | Loss: 0.00020567
Iteration 170/1000 | Loss: 0.00015878
Iteration 171/1000 | Loss: 0.00019441
Iteration 172/1000 | Loss: 0.00003736
Iteration 173/1000 | Loss: 0.00010301
Iteration 174/1000 | Loss: 0.00002798
Iteration 175/1000 | Loss: 0.00002599
Iteration 176/1000 | Loss: 0.00041328
Iteration 177/1000 | Loss: 0.00003111
Iteration 178/1000 | Loss: 0.00045865
Iteration 179/1000 | Loss: 0.00046310
Iteration 180/1000 | Loss: 0.00100494
Iteration 181/1000 | Loss: 0.00038615
Iteration 182/1000 | Loss: 0.00092986
Iteration 183/1000 | Loss: 0.00044696
Iteration 184/1000 | Loss: 0.00073865
Iteration 185/1000 | Loss: 0.00003445
Iteration 186/1000 | Loss: 0.00003448
Iteration 187/1000 | Loss: 0.00002471
Iteration 188/1000 | Loss: 0.00002350
Iteration 189/1000 | Loss: 0.00029660
Iteration 190/1000 | Loss: 0.00012443
Iteration 191/1000 | Loss: 0.00040184
Iteration 192/1000 | Loss: 0.00005885
Iteration 193/1000 | Loss: 0.00004157
Iteration 194/1000 | Loss: 0.00002643
Iteration 195/1000 | Loss: 0.00038290
Iteration 196/1000 | Loss: 0.00004268
Iteration 197/1000 | Loss: 0.00033559
Iteration 198/1000 | Loss: 0.00002426
Iteration 199/1000 | Loss: 0.00002297
Iteration 200/1000 | Loss: 0.00002239
Iteration 201/1000 | Loss: 0.00029193
Iteration 202/1000 | Loss: 0.00150239
Iteration 203/1000 | Loss: 0.00027822
Iteration 204/1000 | Loss: 0.00019700
Iteration 205/1000 | Loss: 0.00068041
Iteration 206/1000 | Loss: 0.00041227
Iteration 207/1000 | Loss: 0.00035300
Iteration 208/1000 | Loss: 0.00022495
Iteration 209/1000 | Loss: 0.00146311
Iteration 210/1000 | Loss: 0.00041886
Iteration 211/1000 | Loss: 0.00040570
Iteration 212/1000 | Loss: 0.00025456
Iteration 213/1000 | Loss: 0.00053533
Iteration 214/1000 | Loss: 0.00047041
Iteration 215/1000 | Loss: 0.00057444
Iteration 216/1000 | Loss: 0.00073850
Iteration 217/1000 | Loss: 0.00067920
Iteration 218/1000 | Loss: 0.00004667
Iteration 219/1000 | Loss: 0.00003157
Iteration 220/1000 | Loss: 0.00002622
Iteration 221/1000 | Loss: 0.00002351
Iteration 222/1000 | Loss: 0.00002209
Iteration 223/1000 | Loss: 0.00002091
Iteration 224/1000 | Loss: 0.00002022
Iteration 225/1000 | Loss: 0.00006654
Iteration 226/1000 | Loss: 0.00002639
Iteration 227/1000 | Loss: 0.00003094
Iteration 228/1000 | Loss: 0.00001998
Iteration 229/1000 | Loss: 0.00001885
Iteration 230/1000 | Loss: 0.00001805
Iteration 231/1000 | Loss: 0.00001750
Iteration 232/1000 | Loss: 0.00001715
Iteration 233/1000 | Loss: 0.00001688
Iteration 234/1000 | Loss: 0.00001684
Iteration 235/1000 | Loss: 0.00001679
Iteration 236/1000 | Loss: 0.00001677
Iteration 237/1000 | Loss: 0.00001674
Iteration 238/1000 | Loss: 0.00001668
Iteration 239/1000 | Loss: 0.00001661
Iteration 240/1000 | Loss: 0.00001661
Iteration 241/1000 | Loss: 0.00001659
Iteration 242/1000 | Loss: 0.00001659
Iteration 243/1000 | Loss: 0.00001658
Iteration 244/1000 | Loss: 0.00001658
Iteration 245/1000 | Loss: 0.00001657
Iteration 246/1000 | Loss: 0.00001657
Iteration 247/1000 | Loss: 0.00001657
Iteration 248/1000 | Loss: 0.00001657
Iteration 249/1000 | Loss: 0.00001657
Iteration 250/1000 | Loss: 0.00001656
Iteration 251/1000 | Loss: 0.00001656
Iteration 252/1000 | Loss: 0.00001656
Iteration 253/1000 | Loss: 0.00001656
Iteration 254/1000 | Loss: 0.00001656
Iteration 255/1000 | Loss: 0.00001655
Iteration 256/1000 | Loss: 0.00001655
Iteration 257/1000 | Loss: 0.00001655
Iteration 258/1000 | Loss: 0.00001655
Iteration 259/1000 | Loss: 0.00001655
Iteration 260/1000 | Loss: 0.00001654
Iteration 261/1000 | Loss: 0.00001654
Iteration 262/1000 | Loss: 0.00001654
Iteration 263/1000 | Loss: 0.00001654
Iteration 264/1000 | Loss: 0.00001654
Iteration 265/1000 | Loss: 0.00001654
Iteration 266/1000 | Loss: 0.00001654
Iteration 267/1000 | Loss: 0.00001654
Iteration 268/1000 | Loss: 0.00001654
Iteration 269/1000 | Loss: 0.00001654
Iteration 270/1000 | Loss: 0.00001654
Iteration 271/1000 | Loss: 0.00001653
Iteration 272/1000 | Loss: 0.00001653
Iteration 273/1000 | Loss: 0.00001653
Iteration 274/1000 | Loss: 0.00001653
Iteration 275/1000 | Loss: 0.00001653
Iteration 276/1000 | Loss: 0.00001653
Iteration 277/1000 | Loss: 0.00001653
Iteration 278/1000 | Loss: 0.00001653
Iteration 279/1000 | Loss: 0.00001653
Iteration 280/1000 | Loss: 0.00001653
Iteration 281/1000 | Loss: 0.00001653
Iteration 282/1000 | Loss: 0.00001653
Iteration 283/1000 | Loss: 0.00001653
Iteration 284/1000 | Loss: 0.00001653
Iteration 285/1000 | Loss: 0.00001653
Iteration 286/1000 | Loss: 0.00001653
Iteration 287/1000 | Loss: 0.00001653
Iteration 288/1000 | Loss: 0.00001653
Iteration 289/1000 | Loss: 0.00001653
Iteration 290/1000 | Loss: 0.00001653
Iteration 291/1000 | Loss: 0.00001653
Iteration 292/1000 | Loss: 0.00001653
Iteration 293/1000 | Loss: 0.00001653
Iteration 294/1000 | Loss: 0.00001653
Iteration 295/1000 | Loss: 0.00001653
Iteration 296/1000 | Loss: 0.00001653
Iteration 297/1000 | Loss: 0.00001653
Iteration 298/1000 | Loss: 0.00001653
Iteration 299/1000 | Loss: 0.00001653
Iteration 300/1000 | Loss: 0.00001653
Iteration 301/1000 | Loss: 0.00001653
Iteration 302/1000 | Loss: 0.00001653
Iteration 303/1000 | Loss: 0.00001653
Iteration 304/1000 | Loss: 0.00001653
Iteration 305/1000 | Loss: 0.00001653
Iteration 306/1000 | Loss: 0.00001653
Iteration 307/1000 | Loss: 0.00001653
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 307. Stopping optimization.
Last 5 losses: [1.6525411410839297e-05, 1.6525411410839297e-05, 1.6525411410839297e-05, 1.6525411410839297e-05, 1.6525411410839297e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6525411410839297e-05

Optimization complete. Final v2v error: 3.327486753463745 mm

Highest mean error: 5.352881908416748 mm for frame 90

Lowest mean error: 2.646374464035034 mm for frame 216

Saving results

Total time: 433.24863839149475
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00823488
Iteration 2/25 | Loss: 0.00098046
Iteration 3/25 | Loss: 0.00078819
Iteration 4/25 | Loss: 0.00075716
Iteration 5/25 | Loss: 0.00074818
Iteration 6/25 | Loss: 0.00074573
Iteration 7/25 | Loss: 0.00074530
Iteration 8/25 | Loss: 0.00074530
Iteration 9/25 | Loss: 0.00074530
Iteration 10/25 | Loss: 0.00074530
Iteration 11/25 | Loss: 0.00074530
Iteration 12/25 | Loss: 0.00074530
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007452953141182661, 0.0007452953141182661, 0.0007452953141182661, 0.0007452953141182661, 0.0007452953141182661]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007452953141182661

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54422605
Iteration 2/25 | Loss: 0.00054770
Iteration 3/25 | Loss: 0.00054770
Iteration 4/25 | Loss: 0.00054770
Iteration 5/25 | Loss: 0.00054770
Iteration 6/25 | Loss: 0.00054770
Iteration 7/25 | Loss: 0.00054770
Iteration 8/25 | Loss: 0.00054770
Iteration 9/25 | Loss: 0.00054770
Iteration 10/25 | Loss: 0.00054770
Iteration 11/25 | Loss: 0.00054770
Iteration 12/25 | Loss: 0.00054770
Iteration 13/25 | Loss: 0.00054770
Iteration 14/25 | Loss: 0.00054770
Iteration 15/25 | Loss: 0.00054770
Iteration 16/25 | Loss: 0.00054770
Iteration 17/25 | Loss: 0.00054770
Iteration 18/25 | Loss: 0.00054770
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005476983496919274, 0.0005476983496919274, 0.0005476983496919274, 0.0005476983496919274, 0.0005476983496919274]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005476983496919274

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054770
Iteration 2/1000 | Loss: 0.00003266
Iteration 3/1000 | Loss: 0.00002186
Iteration 4/1000 | Loss: 0.00001862
Iteration 5/1000 | Loss: 0.00001748
Iteration 6/1000 | Loss: 0.00001671
Iteration 7/1000 | Loss: 0.00001621
Iteration 8/1000 | Loss: 0.00001583
Iteration 9/1000 | Loss: 0.00001559
Iteration 10/1000 | Loss: 0.00001534
Iteration 11/1000 | Loss: 0.00001515
Iteration 12/1000 | Loss: 0.00001512
Iteration 13/1000 | Loss: 0.00001509
Iteration 14/1000 | Loss: 0.00001507
Iteration 15/1000 | Loss: 0.00001504
Iteration 16/1000 | Loss: 0.00001503
Iteration 17/1000 | Loss: 0.00001496
Iteration 18/1000 | Loss: 0.00001495
Iteration 19/1000 | Loss: 0.00001492
Iteration 20/1000 | Loss: 0.00001492
Iteration 21/1000 | Loss: 0.00001491
Iteration 22/1000 | Loss: 0.00001490
Iteration 23/1000 | Loss: 0.00001489
Iteration 24/1000 | Loss: 0.00001489
Iteration 25/1000 | Loss: 0.00001488
Iteration 26/1000 | Loss: 0.00001487
Iteration 27/1000 | Loss: 0.00001487
Iteration 28/1000 | Loss: 0.00001486
Iteration 29/1000 | Loss: 0.00001486
Iteration 30/1000 | Loss: 0.00001484
Iteration 31/1000 | Loss: 0.00001484
Iteration 32/1000 | Loss: 0.00001483
Iteration 33/1000 | Loss: 0.00001483
Iteration 34/1000 | Loss: 0.00001483
Iteration 35/1000 | Loss: 0.00001483
Iteration 36/1000 | Loss: 0.00001482
Iteration 37/1000 | Loss: 0.00001482
Iteration 38/1000 | Loss: 0.00001481
Iteration 39/1000 | Loss: 0.00001481
Iteration 40/1000 | Loss: 0.00001480
Iteration 41/1000 | Loss: 0.00001480
Iteration 42/1000 | Loss: 0.00001480
Iteration 43/1000 | Loss: 0.00001480
Iteration 44/1000 | Loss: 0.00001480
Iteration 45/1000 | Loss: 0.00001480
Iteration 46/1000 | Loss: 0.00001480
Iteration 47/1000 | Loss: 0.00001480
Iteration 48/1000 | Loss: 0.00001479
Iteration 49/1000 | Loss: 0.00001479
Iteration 50/1000 | Loss: 0.00001479
Iteration 51/1000 | Loss: 0.00001478
Iteration 52/1000 | Loss: 0.00001478
Iteration 53/1000 | Loss: 0.00001477
Iteration 54/1000 | Loss: 0.00001477
Iteration 55/1000 | Loss: 0.00001477
Iteration 56/1000 | Loss: 0.00001476
Iteration 57/1000 | Loss: 0.00001476
Iteration 58/1000 | Loss: 0.00001476
Iteration 59/1000 | Loss: 0.00001476
Iteration 60/1000 | Loss: 0.00001475
Iteration 61/1000 | Loss: 0.00001475
Iteration 62/1000 | Loss: 0.00001475
Iteration 63/1000 | Loss: 0.00001474
Iteration 64/1000 | Loss: 0.00001474
Iteration 65/1000 | Loss: 0.00001474
Iteration 66/1000 | Loss: 0.00001473
Iteration 67/1000 | Loss: 0.00001473
Iteration 68/1000 | Loss: 0.00001473
Iteration 69/1000 | Loss: 0.00001473
Iteration 70/1000 | Loss: 0.00001473
Iteration 71/1000 | Loss: 0.00001472
Iteration 72/1000 | Loss: 0.00001472
Iteration 73/1000 | Loss: 0.00001472
Iteration 74/1000 | Loss: 0.00001472
Iteration 75/1000 | Loss: 0.00001472
Iteration 76/1000 | Loss: 0.00001472
Iteration 77/1000 | Loss: 0.00001472
Iteration 78/1000 | Loss: 0.00001472
Iteration 79/1000 | Loss: 0.00001470
Iteration 80/1000 | Loss: 0.00001470
Iteration 81/1000 | Loss: 0.00001470
Iteration 82/1000 | Loss: 0.00001470
Iteration 83/1000 | Loss: 0.00001470
Iteration 84/1000 | Loss: 0.00001470
Iteration 85/1000 | Loss: 0.00001470
Iteration 86/1000 | Loss: 0.00001470
Iteration 87/1000 | Loss: 0.00001469
Iteration 88/1000 | Loss: 0.00001469
Iteration 89/1000 | Loss: 0.00001469
Iteration 90/1000 | Loss: 0.00001468
Iteration 91/1000 | Loss: 0.00001467
Iteration 92/1000 | Loss: 0.00001467
Iteration 93/1000 | Loss: 0.00001467
Iteration 94/1000 | Loss: 0.00001466
Iteration 95/1000 | Loss: 0.00001466
Iteration 96/1000 | Loss: 0.00001466
Iteration 97/1000 | Loss: 0.00001465
Iteration 98/1000 | Loss: 0.00001465
Iteration 99/1000 | Loss: 0.00001464
Iteration 100/1000 | Loss: 0.00001464
Iteration 101/1000 | Loss: 0.00001464
Iteration 102/1000 | Loss: 0.00001464
Iteration 103/1000 | Loss: 0.00001464
Iteration 104/1000 | Loss: 0.00001463
Iteration 105/1000 | Loss: 0.00001463
Iteration 106/1000 | Loss: 0.00001463
Iteration 107/1000 | Loss: 0.00001463
Iteration 108/1000 | Loss: 0.00001463
Iteration 109/1000 | Loss: 0.00001462
Iteration 110/1000 | Loss: 0.00001462
Iteration 111/1000 | Loss: 0.00001462
Iteration 112/1000 | Loss: 0.00001462
Iteration 113/1000 | Loss: 0.00001462
Iteration 114/1000 | Loss: 0.00001462
Iteration 115/1000 | Loss: 0.00001462
Iteration 116/1000 | Loss: 0.00001461
Iteration 117/1000 | Loss: 0.00001461
Iteration 118/1000 | Loss: 0.00001461
Iteration 119/1000 | Loss: 0.00001461
Iteration 120/1000 | Loss: 0.00001461
Iteration 121/1000 | Loss: 0.00001461
Iteration 122/1000 | Loss: 0.00001461
Iteration 123/1000 | Loss: 0.00001461
Iteration 124/1000 | Loss: 0.00001461
Iteration 125/1000 | Loss: 0.00001461
Iteration 126/1000 | Loss: 0.00001460
Iteration 127/1000 | Loss: 0.00001460
Iteration 128/1000 | Loss: 0.00001460
Iteration 129/1000 | Loss: 0.00001460
Iteration 130/1000 | Loss: 0.00001460
Iteration 131/1000 | Loss: 0.00001460
Iteration 132/1000 | Loss: 0.00001460
Iteration 133/1000 | Loss: 0.00001460
Iteration 134/1000 | Loss: 0.00001460
Iteration 135/1000 | Loss: 0.00001460
Iteration 136/1000 | Loss: 0.00001460
Iteration 137/1000 | Loss: 0.00001459
Iteration 138/1000 | Loss: 0.00001459
Iteration 139/1000 | Loss: 0.00001459
Iteration 140/1000 | Loss: 0.00001459
Iteration 141/1000 | Loss: 0.00001459
Iteration 142/1000 | Loss: 0.00001459
Iteration 143/1000 | Loss: 0.00001459
Iteration 144/1000 | Loss: 0.00001459
Iteration 145/1000 | Loss: 0.00001459
Iteration 146/1000 | Loss: 0.00001459
Iteration 147/1000 | Loss: 0.00001459
Iteration 148/1000 | Loss: 0.00001458
Iteration 149/1000 | Loss: 0.00001458
Iteration 150/1000 | Loss: 0.00001458
Iteration 151/1000 | Loss: 0.00001458
Iteration 152/1000 | Loss: 0.00001458
Iteration 153/1000 | Loss: 0.00001458
Iteration 154/1000 | Loss: 0.00001458
Iteration 155/1000 | Loss: 0.00001458
Iteration 156/1000 | Loss: 0.00001458
Iteration 157/1000 | Loss: 0.00001458
Iteration 158/1000 | Loss: 0.00001458
Iteration 159/1000 | Loss: 0.00001458
Iteration 160/1000 | Loss: 0.00001458
Iteration 161/1000 | Loss: 0.00001458
Iteration 162/1000 | Loss: 0.00001458
Iteration 163/1000 | Loss: 0.00001458
Iteration 164/1000 | Loss: 0.00001458
Iteration 165/1000 | Loss: 0.00001458
Iteration 166/1000 | Loss: 0.00001458
Iteration 167/1000 | Loss: 0.00001458
Iteration 168/1000 | Loss: 0.00001458
Iteration 169/1000 | Loss: 0.00001458
Iteration 170/1000 | Loss: 0.00001458
Iteration 171/1000 | Loss: 0.00001458
Iteration 172/1000 | Loss: 0.00001458
Iteration 173/1000 | Loss: 0.00001458
Iteration 174/1000 | Loss: 0.00001458
Iteration 175/1000 | Loss: 0.00001458
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.4576085959561169e-05, 1.4576085959561169e-05, 1.4576085959561169e-05, 1.4576085959561169e-05, 1.4576085959561169e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4576085959561169e-05

Optimization complete. Final v2v error: 3.221459150314331 mm

Highest mean error: 4.146424770355225 mm for frame 154

Lowest mean error: 2.803819417953491 mm for frame 183

Saving results

Total time: 44.37220072746277
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00791941
Iteration 2/25 | Loss: 0.00109205
Iteration 3/25 | Loss: 0.00076692
Iteration 4/25 | Loss: 0.00073090
Iteration 5/25 | Loss: 0.00072218
Iteration 6/25 | Loss: 0.00071990
Iteration 7/25 | Loss: 0.00071914
Iteration 8/25 | Loss: 0.00071912
Iteration 9/25 | Loss: 0.00071912
Iteration 10/25 | Loss: 0.00071912
Iteration 11/25 | Loss: 0.00071912
Iteration 12/25 | Loss: 0.00071912
Iteration 13/25 | Loss: 0.00071912
Iteration 14/25 | Loss: 0.00071912
Iteration 15/25 | Loss: 0.00071912
Iteration 16/25 | Loss: 0.00071912
Iteration 17/25 | Loss: 0.00071912
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007191176991909742, 0.0007191176991909742, 0.0007191176991909742, 0.0007191176991909742, 0.0007191176991909742]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007191176991909742

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.04832411
Iteration 2/25 | Loss: 0.00045690
Iteration 3/25 | Loss: 0.00045690
Iteration 4/25 | Loss: 0.00045690
Iteration 5/25 | Loss: 0.00045690
Iteration 6/25 | Loss: 0.00045690
Iteration 7/25 | Loss: 0.00045690
Iteration 8/25 | Loss: 0.00045690
Iteration 9/25 | Loss: 0.00045690
Iteration 10/25 | Loss: 0.00045690
Iteration 11/25 | Loss: 0.00045690
Iteration 12/25 | Loss: 0.00045690
Iteration 13/25 | Loss: 0.00045690
Iteration 14/25 | Loss: 0.00045690
Iteration 15/25 | Loss: 0.00045690
Iteration 16/25 | Loss: 0.00045690
Iteration 17/25 | Loss: 0.00045690
Iteration 18/25 | Loss: 0.00045690
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000456899288110435, 0.000456899288110435, 0.000456899288110435, 0.000456899288110435, 0.000456899288110435]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000456899288110435

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045690
Iteration 2/1000 | Loss: 0.00002640
Iteration 3/1000 | Loss: 0.00001621
Iteration 4/1000 | Loss: 0.00001515
Iteration 5/1000 | Loss: 0.00001426
Iteration 6/1000 | Loss: 0.00001384
Iteration 7/1000 | Loss: 0.00001348
Iteration 8/1000 | Loss: 0.00001327
Iteration 9/1000 | Loss: 0.00001325
Iteration 10/1000 | Loss: 0.00001306
Iteration 11/1000 | Loss: 0.00001306
Iteration 12/1000 | Loss: 0.00001306
Iteration 13/1000 | Loss: 0.00001303
Iteration 14/1000 | Loss: 0.00001302
Iteration 15/1000 | Loss: 0.00001298
Iteration 16/1000 | Loss: 0.00001297
Iteration 17/1000 | Loss: 0.00001296
Iteration 18/1000 | Loss: 0.00001296
Iteration 19/1000 | Loss: 0.00001296
Iteration 20/1000 | Loss: 0.00001296
Iteration 21/1000 | Loss: 0.00001295
Iteration 22/1000 | Loss: 0.00001292
Iteration 23/1000 | Loss: 0.00001291
Iteration 24/1000 | Loss: 0.00001291
Iteration 25/1000 | Loss: 0.00001289
Iteration 26/1000 | Loss: 0.00001283
Iteration 27/1000 | Loss: 0.00001277
Iteration 28/1000 | Loss: 0.00001268
Iteration 29/1000 | Loss: 0.00001265
Iteration 30/1000 | Loss: 0.00001265
Iteration 31/1000 | Loss: 0.00001264
Iteration 32/1000 | Loss: 0.00001264
Iteration 33/1000 | Loss: 0.00001263
Iteration 34/1000 | Loss: 0.00001263
Iteration 35/1000 | Loss: 0.00001263
Iteration 36/1000 | Loss: 0.00001261
Iteration 37/1000 | Loss: 0.00001261
Iteration 38/1000 | Loss: 0.00001260
Iteration 39/1000 | Loss: 0.00001260
Iteration 40/1000 | Loss: 0.00001259
Iteration 41/1000 | Loss: 0.00001259
Iteration 42/1000 | Loss: 0.00001258
Iteration 43/1000 | Loss: 0.00001258
Iteration 44/1000 | Loss: 0.00001258
Iteration 45/1000 | Loss: 0.00001258
Iteration 46/1000 | Loss: 0.00001258
Iteration 47/1000 | Loss: 0.00001258
Iteration 48/1000 | Loss: 0.00001258
Iteration 49/1000 | Loss: 0.00001258
Iteration 50/1000 | Loss: 0.00001258
Iteration 51/1000 | Loss: 0.00001258
Iteration 52/1000 | Loss: 0.00001258
Iteration 53/1000 | Loss: 0.00001257
Iteration 54/1000 | Loss: 0.00001257
Iteration 55/1000 | Loss: 0.00001255
Iteration 56/1000 | Loss: 0.00001255
Iteration 57/1000 | Loss: 0.00001254
Iteration 58/1000 | Loss: 0.00001254
Iteration 59/1000 | Loss: 0.00001253
Iteration 60/1000 | Loss: 0.00001253
Iteration 61/1000 | Loss: 0.00001253
Iteration 62/1000 | Loss: 0.00001252
Iteration 63/1000 | Loss: 0.00001247
Iteration 64/1000 | Loss: 0.00001247
Iteration 65/1000 | Loss: 0.00001246
Iteration 66/1000 | Loss: 0.00001246
Iteration 67/1000 | Loss: 0.00001245
Iteration 68/1000 | Loss: 0.00001244
Iteration 69/1000 | Loss: 0.00001243
Iteration 70/1000 | Loss: 0.00001243
Iteration 71/1000 | Loss: 0.00001243
Iteration 72/1000 | Loss: 0.00001243
Iteration 73/1000 | Loss: 0.00001243
Iteration 74/1000 | Loss: 0.00001243
Iteration 75/1000 | Loss: 0.00001242
Iteration 76/1000 | Loss: 0.00001242
Iteration 77/1000 | Loss: 0.00001242
Iteration 78/1000 | Loss: 0.00001242
Iteration 79/1000 | Loss: 0.00001241
Iteration 80/1000 | Loss: 0.00001241
Iteration 81/1000 | Loss: 0.00001241
Iteration 82/1000 | Loss: 0.00001240
Iteration 83/1000 | Loss: 0.00001240
Iteration 84/1000 | Loss: 0.00001240
Iteration 85/1000 | Loss: 0.00001240
Iteration 86/1000 | Loss: 0.00001240
Iteration 87/1000 | Loss: 0.00001239
Iteration 88/1000 | Loss: 0.00001239
Iteration 89/1000 | Loss: 0.00001239
Iteration 90/1000 | Loss: 0.00001239
Iteration 91/1000 | Loss: 0.00001239
Iteration 92/1000 | Loss: 0.00001239
Iteration 93/1000 | Loss: 0.00001239
Iteration 94/1000 | Loss: 0.00001239
Iteration 95/1000 | Loss: 0.00001239
Iteration 96/1000 | Loss: 0.00001239
Iteration 97/1000 | Loss: 0.00001239
Iteration 98/1000 | Loss: 0.00001239
Iteration 99/1000 | Loss: 0.00001239
Iteration 100/1000 | Loss: 0.00001239
Iteration 101/1000 | Loss: 0.00001239
Iteration 102/1000 | Loss: 0.00001239
Iteration 103/1000 | Loss: 0.00001239
Iteration 104/1000 | Loss: 0.00001238
Iteration 105/1000 | Loss: 0.00001238
Iteration 106/1000 | Loss: 0.00001238
Iteration 107/1000 | Loss: 0.00001238
Iteration 108/1000 | Loss: 0.00001238
Iteration 109/1000 | Loss: 0.00001238
Iteration 110/1000 | Loss: 0.00001238
Iteration 111/1000 | Loss: 0.00001238
Iteration 112/1000 | Loss: 0.00001237
Iteration 113/1000 | Loss: 0.00001237
Iteration 114/1000 | Loss: 0.00001237
Iteration 115/1000 | Loss: 0.00001237
Iteration 116/1000 | Loss: 0.00001237
Iteration 117/1000 | Loss: 0.00001236
Iteration 118/1000 | Loss: 0.00001236
Iteration 119/1000 | Loss: 0.00001236
Iteration 120/1000 | Loss: 0.00001236
Iteration 121/1000 | Loss: 0.00001236
Iteration 122/1000 | Loss: 0.00001236
Iteration 123/1000 | Loss: 0.00001236
Iteration 124/1000 | Loss: 0.00001235
Iteration 125/1000 | Loss: 0.00001235
Iteration 126/1000 | Loss: 0.00001235
Iteration 127/1000 | Loss: 0.00001235
Iteration 128/1000 | Loss: 0.00001235
Iteration 129/1000 | Loss: 0.00001235
Iteration 130/1000 | Loss: 0.00001235
Iteration 131/1000 | Loss: 0.00001235
Iteration 132/1000 | Loss: 0.00001235
Iteration 133/1000 | Loss: 0.00001235
Iteration 134/1000 | Loss: 0.00001235
Iteration 135/1000 | Loss: 0.00001234
Iteration 136/1000 | Loss: 0.00001234
Iteration 137/1000 | Loss: 0.00001234
Iteration 138/1000 | Loss: 0.00001234
Iteration 139/1000 | Loss: 0.00001234
Iteration 140/1000 | Loss: 0.00001234
Iteration 141/1000 | Loss: 0.00001234
Iteration 142/1000 | Loss: 0.00001234
Iteration 143/1000 | Loss: 0.00001234
Iteration 144/1000 | Loss: 0.00001234
Iteration 145/1000 | Loss: 0.00001234
Iteration 146/1000 | Loss: 0.00001234
Iteration 147/1000 | Loss: 0.00001233
Iteration 148/1000 | Loss: 0.00001233
Iteration 149/1000 | Loss: 0.00001233
Iteration 150/1000 | Loss: 0.00001233
Iteration 151/1000 | Loss: 0.00001233
Iteration 152/1000 | Loss: 0.00001233
Iteration 153/1000 | Loss: 0.00001233
Iteration 154/1000 | Loss: 0.00001233
Iteration 155/1000 | Loss: 0.00001233
Iteration 156/1000 | Loss: 0.00001232
Iteration 157/1000 | Loss: 0.00001232
Iteration 158/1000 | Loss: 0.00001232
Iteration 159/1000 | Loss: 0.00001232
Iteration 160/1000 | Loss: 0.00001232
Iteration 161/1000 | Loss: 0.00001232
Iteration 162/1000 | Loss: 0.00001232
Iteration 163/1000 | Loss: 0.00001232
Iteration 164/1000 | Loss: 0.00001232
Iteration 165/1000 | Loss: 0.00001232
Iteration 166/1000 | Loss: 0.00001232
Iteration 167/1000 | Loss: 0.00001232
Iteration 168/1000 | Loss: 0.00001232
Iteration 169/1000 | Loss: 0.00001232
Iteration 170/1000 | Loss: 0.00001232
Iteration 171/1000 | Loss: 0.00001232
Iteration 172/1000 | Loss: 0.00001232
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.2324385352258105e-05, 1.2324385352258105e-05, 1.2324385352258105e-05, 1.2324385352258105e-05, 1.2324385352258105e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2324385352258105e-05

Optimization complete. Final v2v error: 3.020388126373291 mm

Highest mean error: 3.231809377670288 mm for frame 64

Lowest mean error: 2.8932037353515625 mm for frame 44

Saving results

Total time: 37.93317103385925
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_046/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_046/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00404488
Iteration 2/25 | Loss: 0.00087115
Iteration 3/25 | Loss: 0.00076298
Iteration 4/25 | Loss: 0.00075196
Iteration 5/25 | Loss: 0.00074777
Iteration 6/25 | Loss: 0.00074664
Iteration 7/25 | Loss: 0.00074664
Iteration 8/25 | Loss: 0.00074664
Iteration 9/25 | Loss: 0.00074664
Iteration 10/25 | Loss: 0.00074664
Iteration 11/25 | Loss: 0.00074664
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007466381066478789, 0.0007466381066478789, 0.0007466381066478789, 0.0007466381066478789, 0.0007466381066478789]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007466381066478789

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49304652
Iteration 2/25 | Loss: 0.00052251
Iteration 3/25 | Loss: 0.00052251
Iteration 4/25 | Loss: 0.00052251
Iteration 5/25 | Loss: 0.00052251
Iteration 6/25 | Loss: 0.00052251
Iteration 7/25 | Loss: 0.00052251
Iteration 8/25 | Loss: 0.00052251
Iteration 9/25 | Loss: 0.00052251
Iteration 10/25 | Loss: 0.00052251
Iteration 11/25 | Loss: 0.00052251
Iteration 12/25 | Loss: 0.00052251
Iteration 13/25 | Loss: 0.00052251
Iteration 14/25 | Loss: 0.00052251
Iteration 15/25 | Loss: 0.00052251
Iteration 16/25 | Loss: 0.00052251
Iteration 17/25 | Loss: 0.00052251
Iteration 18/25 | Loss: 0.00052251
Iteration 19/25 | Loss: 0.00052251
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0005225083441473544, 0.0005225083441473544, 0.0005225083441473544, 0.0005225083441473544, 0.0005225083441473544]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005225083441473544

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052251
Iteration 2/1000 | Loss: 0.00002014
Iteration 3/1000 | Loss: 0.00001453
Iteration 4/1000 | Loss: 0.00001347
Iteration 5/1000 | Loss: 0.00001283
Iteration 6/1000 | Loss: 0.00001264
Iteration 7/1000 | Loss: 0.00001230
Iteration 8/1000 | Loss: 0.00001219
Iteration 9/1000 | Loss: 0.00001211
Iteration 10/1000 | Loss: 0.00001209
Iteration 11/1000 | Loss: 0.00001208
Iteration 12/1000 | Loss: 0.00001201
Iteration 13/1000 | Loss: 0.00001201
Iteration 14/1000 | Loss: 0.00001200
Iteration 15/1000 | Loss: 0.00001200
Iteration 16/1000 | Loss: 0.00001194
Iteration 17/1000 | Loss: 0.00001194
Iteration 18/1000 | Loss: 0.00001193
Iteration 19/1000 | Loss: 0.00001193
Iteration 20/1000 | Loss: 0.00001193
Iteration 21/1000 | Loss: 0.00001193
Iteration 22/1000 | Loss: 0.00001193
Iteration 23/1000 | Loss: 0.00001190
Iteration 24/1000 | Loss: 0.00001188
Iteration 25/1000 | Loss: 0.00001188
Iteration 26/1000 | Loss: 0.00001187
Iteration 27/1000 | Loss: 0.00001185
Iteration 28/1000 | Loss: 0.00001185
Iteration 29/1000 | Loss: 0.00001182
Iteration 30/1000 | Loss: 0.00001182
Iteration 31/1000 | Loss: 0.00001181
Iteration 32/1000 | Loss: 0.00001181
Iteration 33/1000 | Loss: 0.00001180
Iteration 34/1000 | Loss: 0.00001179
Iteration 35/1000 | Loss: 0.00001179
Iteration 36/1000 | Loss: 0.00001179
Iteration 37/1000 | Loss: 0.00001179
Iteration 38/1000 | Loss: 0.00001179
Iteration 39/1000 | Loss: 0.00001179
Iteration 40/1000 | Loss: 0.00001179
Iteration 41/1000 | Loss: 0.00001179
Iteration 42/1000 | Loss: 0.00001179
Iteration 43/1000 | Loss: 0.00001179
Iteration 44/1000 | Loss: 0.00001179
Iteration 45/1000 | Loss: 0.00001178
Iteration 46/1000 | Loss: 0.00001177
Iteration 47/1000 | Loss: 0.00001177
Iteration 48/1000 | Loss: 0.00001177
Iteration 49/1000 | Loss: 0.00001177
Iteration 50/1000 | Loss: 0.00001177
Iteration 51/1000 | Loss: 0.00001177
Iteration 52/1000 | Loss: 0.00001177
Iteration 53/1000 | Loss: 0.00001177
Iteration 54/1000 | Loss: 0.00001177
Iteration 55/1000 | Loss: 0.00001177
Iteration 56/1000 | Loss: 0.00001176
Iteration 57/1000 | Loss: 0.00001176
Iteration 58/1000 | Loss: 0.00001176
Iteration 59/1000 | Loss: 0.00001176
Iteration 60/1000 | Loss: 0.00001175
Iteration 61/1000 | Loss: 0.00001173
Iteration 62/1000 | Loss: 0.00001173
Iteration 63/1000 | Loss: 0.00001173
Iteration 64/1000 | Loss: 0.00001173
Iteration 65/1000 | Loss: 0.00001173
Iteration 66/1000 | Loss: 0.00001173
Iteration 67/1000 | Loss: 0.00001173
Iteration 68/1000 | Loss: 0.00001172
Iteration 69/1000 | Loss: 0.00001172
Iteration 70/1000 | Loss: 0.00001172
Iteration 71/1000 | Loss: 0.00001172
Iteration 72/1000 | Loss: 0.00001172
Iteration 73/1000 | Loss: 0.00001172
Iteration 74/1000 | Loss: 0.00001172
Iteration 75/1000 | Loss: 0.00001172
Iteration 76/1000 | Loss: 0.00001172
Iteration 77/1000 | Loss: 0.00001172
Iteration 78/1000 | Loss: 0.00001172
Iteration 79/1000 | Loss: 0.00001172
Iteration 80/1000 | Loss: 0.00001172
Iteration 81/1000 | Loss: 0.00001171
Iteration 82/1000 | Loss: 0.00001171
Iteration 83/1000 | Loss: 0.00001171
Iteration 84/1000 | Loss: 0.00001171
Iteration 85/1000 | Loss: 0.00001171
Iteration 86/1000 | Loss: 0.00001171
Iteration 87/1000 | Loss: 0.00001171
Iteration 88/1000 | Loss: 0.00001170
Iteration 89/1000 | Loss: 0.00001170
Iteration 90/1000 | Loss: 0.00001170
Iteration 91/1000 | Loss: 0.00001170
Iteration 92/1000 | Loss: 0.00001169
Iteration 93/1000 | Loss: 0.00001169
Iteration 94/1000 | Loss: 0.00001169
Iteration 95/1000 | Loss: 0.00001169
Iteration 96/1000 | Loss: 0.00001169
Iteration 97/1000 | Loss: 0.00001168
Iteration 98/1000 | Loss: 0.00001168
Iteration 99/1000 | Loss: 0.00001168
Iteration 100/1000 | Loss: 0.00001168
Iteration 101/1000 | Loss: 0.00001168
Iteration 102/1000 | Loss: 0.00001168
Iteration 103/1000 | Loss: 0.00001168
Iteration 104/1000 | Loss: 0.00001168
Iteration 105/1000 | Loss: 0.00001167
Iteration 106/1000 | Loss: 0.00001167
Iteration 107/1000 | Loss: 0.00001167
Iteration 108/1000 | Loss: 0.00001166
Iteration 109/1000 | Loss: 0.00001166
Iteration 110/1000 | Loss: 0.00001166
Iteration 111/1000 | Loss: 0.00001166
Iteration 112/1000 | Loss: 0.00001166
Iteration 113/1000 | Loss: 0.00001166
Iteration 114/1000 | Loss: 0.00001166
Iteration 115/1000 | Loss: 0.00001166
Iteration 116/1000 | Loss: 0.00001166
Iteration 117/1000 | Loss: 0.00001165
Iteration 118/1000 | Loss: 0.00001165
Iteration 119/1000 | Loss: 0.00001165
Iteration 120/1000 | Loss: 0.00001165
Iteration 121/1000 | Loss: 0.00001164
Iteration 122/1000 | Loss: 0.00001164
Iteration 123/1000 | Loss: 0.00001164
Iteration 124/1000 | Loss: 0.00001164
Iteration 125/1000 | Loss: 0.00001164
Iteration 126/1000 | Loss: 0.00001164
Iteration 127/1000 | Loss: 0.00001163
Iteration 128/1000 | Loss: 0.00001163
Iteration 129/1000 | Loss: 0.00001163
Iteration 130/1000 | Loss: 0.00001163
Iteration 131/1000 | Loss: 0.00001163
Iteration 132/1000 | Loss: 0.00001163
Iteration 133/1000 | Loss: 0.00001163
Iteration 134/1000 | Loss: 0.00001163
Iteration 135/1000 | Loss: 0.00001163
Iteration 136/1000 | Loss: 0.00001163
Iteration 137/1000 | Loss: 0.00001163
Iteration 138/1000 | Loss: 0.00001163
Iteration 139/1000 | Loss: 0.00001163
Iteration 140/1000 | Loss: 0.00001163
Iteration 141/1000 | Loss: 0.00001162
Iteration 142/1000 | Loss: 0.00001162
Iteration 143/1000 | Loss: 0.00001162
Iteration 144/1000 | Loss: 0.00001162
Iteration 145/1000 | Loss: 0.00001162
Iteration 146/1000 | Loss: 0.00001162
Iteration 147/1000 | Loss: 0.00001162
Iteration 148/1000 | Loss: 0.00001162
Iteration 149/1000 | Loss: 0.00001162
Iteration 150/1000 | Loss: 0.00001162
Iteration 151/1000 | Loss: 0.00001162
Iteration 152/1000 | Loss: 0.00001162
Iteration 153/1000 | Loss: 0.00001162
Iteration 154/1000 | Loss: 0.00001162
Iteration 155/1000 | Loss: 0.00001161
Iteration 156/1000 | Loss: 0.00001161
Iteration 157/1000 | Loss: 0.00001161
Iteration 158/1000 | Loss: 0.00001161
Iteration 159/1000 | Loss: 0.00001161
Iteration 160/1000 | Loss: 0.00001161
Iteration 161/1000 | Loss: 0.00001161
Iteration 162/1000 | Loss: 0.00001161
Iteration 163/1000 | Loss: 0.00001161
Iteration 164/1000 | Loss: 0.00001161
Iteration 165/1000 | Loss: 0.00001161
Iteration 166/1000 | Loss: 0.00001161
Iteration 167/1000 | Loss: 0.00001160
Iteration 168/1000 | Loss: 0.00001160
Iteration 169/1000 | Loss: 0.00001160
Iteration 170/1000 | Loss: 0.00001160
Iteration 171/1000 | Loss: 0.00001160
Iteration 172/1000 | Loss: 0.00001160
Iteration 173/1000 | Loss: 0.00001160
Iteration 174/1000 | Loss: 0.00001160
Iteration 175/1000 | Loss: 0.00001160
Iteration 176/1000 | Loss: 0.00001159
Iteration 177/1000 | Loss: 0.00001159
Iteration 178/1000 | Loss: 0.00001159
Iteration 179/1000 | Loss: 0.00001159
Iteration 180/1000 | Loss: 0.00001158
Iteration 181/1000 | Loss: 0.00001158
Iteration 182/1000 | Loss: 0.00001158
Iteration 183/1000 | Loss: 0.00001158
Iteration 184/1000 | Loss: 0.00001158
Iteration 185/1000 | Loss: 0.00001158
Iteration 186/1000 | Loss: 0.00001158
Iteration 187/1000 | Loss: 0.00001158
Iteration 188/1000 | Loss: 0.00001158
Iteration 189/1000 | Loss: 0.00001158
Iteration 190/1000 | Loss: 0.00001158
Iteration 191/1000 | Loss: 0.00001158
Iteration 192/1000 | Loss: 0.00001158
Iteration 193/1000 | Loss: 0.00001158
Iteration 194/1000 | Loss: 0.00001158
Iteration 195/1000 | Loss: 0.00001158
Iteration 196/1000 | Loss: 0.00001158
Iteration 197/1000 | Loss: 0.00001158
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [1.1579832971619908e-05, 1.1579832971619908e-05, 1.1579832971619908e-05, 1.1579832971619908e-05, 1.1579832971619908e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1579832971619908e-05

Optimization complete. Final v2v error: 2.862086296081543 mm

Highest mean error: 3.252546548843384 mm for frame 29

Lowest mean error: 2.6824655532836914 mm for frame 114

Saving results

Total time: 34.174214601516724
