Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=99, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 5544-5599
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00724625
Iteration 2/25 | Loss: 0.00160595
Iteration 3/25 | Loss: 0.00141646
Iteration 4/25 | Loss: 0.00121630
Iteration 5/25 | Loss: 0.00120309
Iteration 6/25 | Loss: 0.00120254
Iteration 7/25 | Loss: 0.00119797
Iteration 8/25 | Loss: 0.00119987
Iteration 9/25 | Loss: 0.00119652
Iteration 10/25 | Loss: 0.00119562
Iteration 11/25 | Loss: 0.00119398
Iteration 12/25 | Loss: 0.00119291
Iteration 13/25 | Loss: 0.00119250
Iteration 14/25 | Loss: 0.00119239
Iteration 15/25 | Loss: 0.00119236
Iteration 16/25 | Loss: 0.00119235
Iteration 17/25 | Loss: 0.00119235
Iteration 18/25 | Loss: 0.00119235
Iteration 19/25 | Loss: 0.00119234
Iteration 20/25 | Loss: 0.00119234
Iteration 21/25 | Loss: 0.00119234
Iteration 22/25 | Loss: 0.00119234
Iteration 23/25 | Loss: 0.00119234
Iteration 24/25 | Loss: 0.00119234
Iteration 25/25 | Loss: 0.00119234

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.77509260
Iteration 2/25 | Loss: 0.00132440
Iteration 3/25 | Loss: 0.00132439
Iteration 4/25 | Loss: 0.00132439
Iteration 5/25 | Loss: 0.00132439
Iteration 6/25 | Loss: 0.00132439
Iteration 7/25 | Loss: 0.00132439
Iteration 8/25 | Loss: 0.00132439
Iteration 9/25 | Loss: 0.00132439
Iteration 10/25 | Loss: 0.00132439
Iteration 11/25 | Loss: 0.00132439
Iteration 12/25 | Loss: 0.00132439
Iteration 13/25 | Loss: 0.00132439
Iteration 14/25 | Loss: 0.00132439
Iteration 15/25 | Loss: 0.00132439
Iteration 16/25 | Loss: 0.00132439
Iteration 17/25 | Loss: 0.00132439
Iteration 18/25 | Loss: 0.00132439
Iteration 19/25 | Loss: 0.00132439
Iteration 20/25 | Loss: 0.00132439
Iteration 21/25 | Loss: 0.00132439
Iteration 22/25 | Loss: 0.00132439
Iteration 23/25 | Loss: 0.00132439
Iteration 24/25 | Loss: 0.00132439
Iteration 25/25 | Loss: 0.00132439

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132439
Iteration 2/1000 | Loss: 0.00002695
Iteration 3/1000 | Loss: 0.00016617
Iteration 4/1000 | Loss: 0.00002138
Iteration 5/1000 | Loss: 0.00001924
Iteration 6/1000 | Loss: 0.00001880
Iteration 7/1000 | Loss: 0.00001818
Iteration 8/1000 | Loss: 0.00001768
Iteration 9/1000 | Loss: 0.00001742
Iteration 10/1000 | Loss: 0.00001715
Iteration 11/1000 | Loss: 0.00001692
Iteration 12/1000 | Loss: 0.00014584
Iteration 13/1000 | Loss: 0.00003487
Iteration 14/1000 | Loss: 0.00001670
Iteration 15/1000 | Loss: 0.00001655
Iteration 16/1000 | Loss: 0.00001652
Iteration 17/1000 | Loss: 0.00001651
Iteration 18/1000 | Loss: 0.00001647
Iteration 19/1000 | Loss: 0.00001647
Iteration 20/1000 | Loss: 0.00001647
Iteration 21/1000 | Loss: 0.00001643
Iteration 22/1000 | Loss: 0.00001642
Iteration 23/1000 | Loss: 0.00001642
Iteration 24/1000 | Loss: 0.00001641
Iteration 25/1000 | Loss: 0.00001641
Iteration 26/1000 | Loss: 0.00001633
Iteration 27/1000 | Loss: 0.00001627
Iteration 28/1000 | Loss: 0.00001627
Iteration 29/1000 | Loss: 0.00001626
Iteration 30/1000 | Loss: 0.00001625
Iteration 31/1000 | Loss: 0.00001625
Iteration 32/1000 | Loss: 0.00001624
Iteration 33/1000 | Loss: 0.00001624
Iteration 34/1000 | Loss: 0.00001624
Iteration 35/1000 | Loss: 0.00001623
Iteration 36/1000 | Loss: 0.00001623
Iteration 37/1000 | Loss: 0.00001623
Iteration 38/1000 | Loss: 0.00001622
Iteration 39/1000 | Loss: 0.00001622
Iteration 40/1000 | Loss: 0.00001621
Iteration 41/1000 | Loss: 0.00001621
Iteration 42/1000 | Loss: 0.00001621
Iteration 43/1000 | Loss: 0.00001620
Iteration 44/1000 | Loss: 0.00001620
Iteration 45/1000 | Loss: 0.00001620
Iteration 46/1000 | Loss: 0.00001620
Iteration 47/1000 | Loss: 0.00001620
Iteration 48/1000 | Loss: 0.00001619
Iteration 49/1000 | Loss: 0.00001619
Iteration 50/1000 | Loss: 0.00001619
Iteration 51/1000 | Loss: 0.00001619
Iteration 52/1000 | Loss: 0.00001618
Iteration 53/1000 | Loss: 0.00001618
Iteration 54/1000 | Loss: 0.00001618
Iteration 55/1000 | Loss: 0.00001618
Iteration 56/1000 | Loss: 0.00001618
Iteration 57/1000 | Loss: 0.00001618
Iteration 58/1000 | Loss: 0.00001617
Iteration 59/1000 | Loss: 0.00001617
Iteration 60/1000 | Loss: 0.00001616
Iteration 61/1000 | Loss: 0.00001615
Iteration 62/1000 | Loss: 0.00001615
Iteration 63/1000 | Loss: 0.00001615
Iteration 64/1000 | Loss: 0.00001615
Iteration 65/1000 | Loss: 0.00001615
Iteration 66/1000 | Loss: 0.00001615
Iteration 67/1000 | Loss: 0.00001615
Iteration 68/1000 | Loss: 0.00001614
Iteration 69/1000 | Loss: 0.00001613
Iteration 70/1000 | Loss: 0.00001612
Iteration 71/1000 | Loss: 0.00001611
Iteration 72/1000 | Loss: 0.00001611
Iteration 73/1000 | Loss: 0.00001610
Iteration 74/1000 | Loss: 0.00001610
Iteration 75/1000 | Loss: 0.00001609
Iteration 76/1000 | Loss: 0.00001609
Iteration 77/1000 | Loss: 0.00001609
Iteration 78/1000 | Loss: 0.00001609
Iteration 79/1000 | Loss: 0.00001608
Iteration 80/1000 | Loss: 0.00001608
Iteration 81/1000 | Loss: 0.00001608
Iteration 82/1000 | Loss: 0.00001608
Iteration 83/1000 | Loss: 0.00001608
Iteration 84/1000 | Loss: 0.00001608
Iteration 85/1000 | Loss: 0.00001607
Iteration 86/1000 | Loss: 0.00001607
Iteration 87/1000 | Loss: 0.00001607
Iteration 88/1000 | Loss: 0.00001607
Iteration 89/1000 | Loss: 0.00001607
Iteration 90/1000 | Loss: 0.00001607
Iteration 91/1000 | Loss: 0.00001606
Iteration 92/1000 | Loss: 0.00001606
Iteration 93/1000 | Loss: 0.00001606
Iteration 94/1000 | Loss: 0.00001606
Iteration 95/1000 | Loss: 0.00001606
Iteration 96/1000 | Loss: 0.00001606
Iteration 97/1000 | Loss: 0.00001606
Iteration 98/1000 | Loss: 0.00001605
Iteration 99/1000 | Loss: 0.00001605
Iteration 100/1000 | Loss: 0.00001605
Iteration 101/1000 | Loss: 0.00001605
Iteration 102/1000 | Loss: 0.00001605
Iteration 103/1000 | Loss: 0.00001605
Iteration 104/1000 | Loss: 0.00001604
Iteration 105/1000 | Loss: 0.00001604
Iteration 106/1000 | Loss: 0.00001604
Iteration 107/1000 | Loss: 0.00001604
Iteration 108/1000 | Loss: 0.00001604
Iteration 109/1000 | Loss: 0.00001604
Iteration 110/1000 | Loss: 0.00001604
Iteration 111/1000 | Loss: 0.00001604
Iteration 112/1000 | Loss: 0.00001604
Iteration 113/1000 | Loss: 0.00001604
Iteration 114/1000 | Loss: 0.00001604
Iteration 115/1000 | Loss: 0.00001604
Iteration 116/1000 | Loss: 0.00001604
Iteration 117/1000 | Loss: 0.00001604
Iteration 118/1000 | Loss: 0.00001604
Iteration 119/1000 | Loss: 0.00001604
Iteration 120/1000 | Loss: 0.00001604
Iteration 121/1000 | Loss: 0.00001603
Iteration 122/1000 | Loss: 0.00001603
Iteration 123/1000 | Loss: 0.00001603
Iteration 124/1000 | Loss: 0.00001603
Iteration 125/1000 | Loss: 0.00001603
Iteration 126/1000 | Loss: 0.00001603
Iteration 127/1000 | Loss: 0.00001603
Iteration 128/1000 | Loss: 0.00001603
Iteration 129/1000 | Loss: 0.00001603
Iteration 130/1000 | Loss: 0.00001603
Iteration 131/1000 | Loss: 0.00001603
Iteration 132/1000 | Loss: 0.00001602
Iteration 133/1000 | Loss: 0.00001602
Iteration 134/1000 | Loss: 0.00001602
Iteration 135/1000 | Loss: 0.00001602
Iteration 136/1000 | Loss: 0.00001602
Iteration 137/1000 | Loss: 0.00001602
Iteration 138/1000 | Loss: 0.00001602
Iteration 139/1000 | Loss: 0.00001602
Iteration 140/1000 | Loss: 0.00001602
Iteration 141/1000 | Loss: 0.00001601
Iteration 142/1000 | Loss: 0.00001601
Iteration 143/1000 | Loss: 0.00001601
Iteration 144/1000 | Loss: 0.00001601
Iteration 145/1000 | Loss: 0.00001601
Iteration 146/1000 | Loss: 0.00001601
Iteration 147/1000 | Loss: 0.00001601
Iteration 148/1000 | Loss: 0.00001601
Iteration 149/1000 | Loss: 0.00001601
Iteration 150/1000 | Loss: 0.00001601
Iteration 151/1000 | Loss: 0.00001601
Iteration 152/1000 | Loss: 0.00001601
Iteration 153/1000 | Loss: 0.00001601
Iteration 154/1000 | Loss: 0.00001601
Iteration 155/1000 | Loss: 0.00001601
Iteration 156/1000 | Loss: 0.00001601
Iteration 157/1000 | Loss: 0.00001601
Iteration 158/1000 | Loss: 0.00001600
Iteration 159/1000 | Loss: 0.00001600
Iteration 160/1000 | Loss: 0.00001600
Iteration 161/1000 | Loss: 0.00001600
Iteration 162/1000 | Loss: 0.00001600
Iteration 163/1000 | Loss: 0.00001600
Iteration 164/1000 | Loss: 0.00001600
Iteration 165/1000 | Loss: 0.00001600
Iteration 166/1000 | Loss: 0.00001600
Iteration 167/1000 | Loss: 0.00001600
Iteration 168/1000 | Loss: 0.00001600
Iteration 169/1000 | Loss: 0.00001600
Iteration 170/1000 | Loss: 0.00001600
Iteration 171/1000 | Loss: 0.00001600
Iteration 172/1000 | Loss: 0.00001600
Iteration 173/1000 | Loss: 0.00001600
Iteration 174/1000 | Loss: 0.00001600
Iteration 175/1000 | Loss: 0.00001600
Iteration 176/1000 | Loss: 0.00001600
Iteration 177/1000 | Loss: 0.00001600
Iteration 178/1000 | Loss: 0.00001600
Iteration 179/1000 | Loss: 0.00001600
Iteration 180/1000 | Loss: 0.00001600
Iteration 181/1000 | Loss: 0.00001600
Iteration 182/1000 | Loss: 0.00001600
Iteration 183/1000 | Loss: 0.00001600
Iteration 184/1000 | Loss: 0.00001600
Iteration 185/1000 | Loss: 0.00001600
Iteration 186/1000 | Loss: 0.00001600
Iteration 187/1000 | Loss: 0.00001600
Iteration 188/1000 | Loss: 0.00001600
Iteration 189/1000 | Loss: 0.00001600
Iteration 190/1000 | Loss: 0.00001600
Iteration 191/1000 | Loss: 0.00001600
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [1.599912138772197e-05, 1.599912138772197e-05, 1.599912138772197e-05, 1.599912138772197e-05, 1.599912138772197e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.599912138772197e-05

Optimization complete. Final v2v error: 3.4172911643981934 mm

Highest mean error: 3.806166887283325 mm for frame 106

Lowest mean error: 3.093884229660034 mm for frame 127

Saving results

Total time: 58.61760354042053
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00693203
Iteration 2/25 | Loss: 0.00145922
Iteration 3/25 | Loss: 0.00128132
Iteration 4/25 | Loss: 0.00126145
Iteration 5/25 | Loss: 0.00126013
Iteration 6/25 | Loss: 0.00126006
Iteration 7/25 | Loss: 0.00126006
Iteration 8/25 | Loss: 0.00126006
Iteration 9/25 | Loss: 0.00126006
Iteration 10/25 | Loss: 0.00126006
Iteration 11/25 | Loss: 0.00126006
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012600596528500319, 0.0012600596528500319, 0.0012600596528500319, 0.0012600596528500319, 0.0012600596528500319]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012600596528500319

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26611185
Iteration 2/25 | Loss: 0.00116250
Iteration 3/25 | Loss: 0.00116247
Iteration 4/25 | Loss: 0.00116247
Iteration 5/25 | Loss: 0.00116247
Iteration 6/25 | Loss: 0.00116247
Iteration 7/25 | Loss: 0.00116247
Iteration 8/25 | Loss: 0.00116247
Iteration 9/25 | Loss: 0.00116247
Iteration 10/25 | Loss: 0.00116247
Iteration 11/25 | Loss: 0.00116247
Iteration 12/25 | Loss: 0.00116247
Iteration 13/25 | Loss: 0.00116247
Iteration 14/25 | Loss: 0.00116247
Iteration 15/25 | Loss: 0.00116247
Iteration 16/25 | Loss: 0.00116247
Iteration 17/25 | Loss: 0.00116247
Iteration 18/25 | Loss: 0.00116247
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011624721810221672, 0.0011624721810221672, 0.0011624721810221672, 0.0011624721810221672, 0.0011624721810221672]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011624721810221672

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116247
Iteration 2/1000 | Loss: 0.00002815
Iteration 3/1000 | Loss: 0.00002078
Iteration 4/1000 | Loss: 0.00001881
Iteration 5/1000 | Loss: 0.00001789
Iteration 6/1000 | Loss: 0.00001751
Iteration 7/1000 | Loss: 0.00001716
Iteration 8/1000 | Loss: 0.00001687
Iteration 9/1000 | Loss: 0.00001658
Iteration 10/1000 | Loss: 0.00001637
Iteration 11/1000 | Loss: 0.00001615
Iteration 12/1000 | Loss: 0.00001589
Iteration 13/1000 | Loss: 0.00001564
Iteration 14/1000 | Loss: 0.00001541
Iteration 15/1000 | Loss: 0.00001529
Iteration 16/1000 | Loss: 0.00001528
Iteration 17/1000 | Loss: 0.00001523
Iteration 18/1000 | Loss: 0.00001523
Iteration 19/1000 | Loss: 0.00001522
Iteration 20/1000 | Loss: 0.00001522
Iteration 21/1000 | Loss: 0.00001520
Iteration 22/1000 | Loss: 0.00001518
Iteration 23/1000 | Loss: 0.00001517
Iteration 24/1000 | Loss: 0.00001517
Iteration 25/1000 | Loss: 0.00001516
Iteration 26/1000 | Loss: 0.00001510
Iteration 27/1000 | Loss: 0.00001510
Iteration 28/1000 | Loss: 0.00001510
Iteration 29/1000 | Loss: 0.00001507
Iteration 30/1000 | Loss: 0.00001505
Iteration 31/1000 | Loss: 0.00001505
Iteration 32/1000 | Loss: 0.00001504
Iteration 33/1000 | Loss: 0.00001504
Iteration 34/1000 | Loss: 0.00001504
Iteration 35/1000 | Loss: 0.00001503
Iteration 36/1000 | Loss: 0.00001503
Iteration 37/1000 | Loss: 0.00001503
Iteration 38/1000 | Loss: 0.00001502
Iteration 39/1000 | Loss: 0.00001502
Iteration 40/1000 | Loss: 0.00001501
Iteration 41/1000 | Loss: 0.00001500
Iteration 42/1000 | Loss: 0.00001500
Iteration 43/1000 | Loss: 0.00001499
Iteration 44/1000 | Loss: 0.00001499
Iteration 45/1000 | Loss: 0.00001499
Iteration 46/1000 | Loss: 0.00001499
Iteration 47/1000 | Loss: 0.00001498
Iteration 48/1000 | Loss: 0.00001498
Iteration 49/1000 | Loss: 0.00001498
Iteration 50/1000 | Loss: 0.00001497
Iteration 51/1000 | Loss: 0.00001497
Iteration 52/1000 | Loss: 0.00001496
Iteration 53/1000 | Loss: 0.00001496
Iteration 54/1000 | Loss: 0.00001495
Iteration 55/1000 | Loss: 0.00001495
Iteration 56/1000 | Loss: 0.00001495
Iteration 57/1000 | Loss: 0.00001495
Iteration 58/1000 | Loss: 0.00001495
Iteration 59/1000 | Loss: 0.00001494
Iteration 60/1000 | Loss: 0.00001494
Iteration 61/1000 | Loss: 0.00001494
Iteration 62/1000 | Loss: 0.00001494
Iteration 63/1000 | Loss: 0.00001493
Iteration 64/1000 | Loss: 0.00001493
Iteration 65/1000 | Loss: 0.00001493
Iteration 66/1000 | Loss: 0.00001493
Iteration 67/1000 | Loss: 0.00001492
Iteration 68/1000 | Loss: 0.00001492
Iteration 69/1000 | Loss: 0.00001492
Iteration 70/1000 | Loss: 0.00001492
Iteration 71/1000 | Loss: 0.00001492
Iteration 72/1000 | Loss: 0.00001491
Iteration 73/1000 | Loss: 0.00001491
Iteration 74/1000 | Loss: 0.00001491
Iteration 75/1000 | Loss: 0.00001491
Iteration 76/1000 | Loss: 0.00001491
Iteration 77/1000 | Loss: 0.00001490
Iteration 78/1000 | Loss: 0.00001490
Iteration 79/1000 | Loss: 0.00001490
Iteration 80/1000 | Loss: 0.00001489
Iteration 81/1000 | Loss: 0.00001489
Iteration 82/1000 | Loss: 0.00001488
Iteration 83/1000 | Loss: 0.00001488
Iteration 84/1000 | Loss: 0.00001488
Iteration 85/1000 | Loss: 0.00001487
Iteration 86/1000 | Loss: 0.00001487
Iteration 87/1000 | Loss: 0.00001487
Iteration 88/1000 | Loss: 0.00001487
Iteration 89/1000 | Loss: 0.00001487
Iteration 90/1000 | Loss: 0.00001487
Iteration 91/1000 | Loss: 0.00001487
Iteration 92/1000 | Loss: 0.00001487
Iteration 93/1000 | Loss: 0.00001487
Iteration 94/1000 | Loss: 0.00001487
Iteration 95/1000 | Loss: 0.00001487
Iteration 96/1000 | Loss: 0.00001487
Iteration 97/1000 | Loss: 0.00001487
Iteration 98/1000 | Loss: 0.00001487
Iteration 99/1000 | Loss: 0.00001487
Iteration 100/1000 | Loss: 0.00001487
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [1.4866502169752494e-05, 1.4866502169752494e-05, 1.4866502169752494e-05, 1.4866502169752494e-05, 1.4866502169752494e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4866502169752494e-05

Optimization complete. Final v2v error: 3.2848384380340576 mm

Highest mean error: 3.4889440536499023 mm for frame 91

Lowest mean error: 3.0910143852233887 mm for frame 194

Saving results

Total time: 40.5849826335907
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00581271
Iteration 2/25 | Loss: 0.00141573
Iteration 3/25 | Loss: 0.00127881
Iteration 4/25 | Loss: 0.00124589
Iteration 5/25 | Loss: 0.00123649
Iteration 6/25 | Loss: 0.00123424
Iteration 7/25 | Loss: 0.00123366
Iteration 8/25 | Loss: 0.00123363
Iteration 9/25 | Loss: 0.00123363
Iteration 10/25 | Loss: 0.00123363
Iteration 11/25 | Loss: 0.00123363
Iteration 12/25 | Loss: 0.00123363
Iteration 13/25 | Loss: 0.00123363
Iteration 14/25 | Loss: 0.00123363
Iteration 15/25 | Loss: 0.00123363
Iteration 16/25 | Loss: 0.00123363
Iteration 17/25 | Loss: 0.00123363
Iteration 18/25 | Loss: 0.00123363
Iteration 19/25 | Loss: 0.00123363
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012336274376139045, 0.0012336274376139045, 0.0012336274376139045, 0.0012336274376139045, 0.0012336274376139045]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012336274376139045

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25688839
Iteration 2/25 | Loss: 0.00174036
Iteration 3/25 | Loss: 0.00174032
Iteration 4/25 | Loss: 0.00174032
Iteration 5/25 | Loss: 0.00174032
Iteration 6/25 | Loss: 0.00174032
Iteration 7/25 | Loss: 0.00174032
Iteration 8/25 | Loss: 0.00174032
Iteration 9/25 | Loss: 0.00174032
Iteration 10/25 | Loss: 0.00174032
Iteration 11/25 | Loss: 0.00174032
Iteration 12/25 | Loss: 0.00174032
Iteration 13/25 | Loss: 0.00174032
Iteration 14/25 | Loss: 0.00174032
Iteration 15/25 | Loss: 0.00174032
Iteration 16/25 | Loss: 0.00174032
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001740321284160018, 0.001740321284160018, 0.001740321284160018, 0.001740321284160018, 0.001740321284160018]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001740321284160018

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00174032
Iteration 2/1000 | Loss: 0.00006743
Iteration 3/1000 | Loss: 0.00004678
Iteration 4/1000 | Loss: 0.00003842
Iteration 5/1000 | Loss: 0.00003626
Iteration 6/1000 | Loss: 0.00003489
Iteration 7/1000 | Loss: 0.00003390
Iteration 8/1000 | Loss: 0.00003342
Iteration 9/1000 | Loss: 0.00003297
Iteration 10/1000 | Loss: 0.00003260
Iteration 11/1000 | Loss: 0.00003232
Iteration 12/1000 | Loss: 0.00003208
Iteration 13/1000 | Loss: 0.00003207
Iteration 14/1000 | Loss: 0.00003189
Iteration 15/1000 | Loss: 0.00003182
Iteration 16/1000 | Loss: 0.00003180
Iteration 17/1000 | Loss: 0.00003165
Iteration 18/1000 | Loss: 0.00003160
Iteration 19/1000 | Loss: 0.00003159
Iteration 20/1000 | Loss: 0.00003158
Iteration 21/1000 | Loss: 0.00003157
Iteration 22/1000 | Loss: 0.00003155
Iteration 23/1000 | Loss: 0.00003154
Iteration 24/1000 | Loss: 0.00003148
Iteration 25/1000 | Loss: 0.00003144
Iteration 26/1000 | Loss: 0.00003143
Iteration 27/1000 | Loss: 0.00003143
Iteration 28/1000 | Loss: 0.00003142
Iteration 29/1000 | Loss: 0.00003141
Iteration 30/1000 | Loss: 0.00003141
Iteration 31/1000 | Loss: 0.00003141
Iteration 32/1000 | Loss: 0.00003140
Iteration 33/1000 | Loss: 0.00003140
Iteration 34/1000 | Loss: 0.00003139
Iteration 35/1000 | Loss: 0.00003139
Iteration 36/1000 | Loss: 0.00003138
Iteration 37/1000 | Loss: 0.00003138
Iteration 38/1000 | Loss: 0.00003138
Iteration 39/1000 | Loss: 0.00003138
Iteration 40/1000 | Loss: 0.00003138
Iteration 41/1000 | Loss: 0.00003138
Iteration 42/1000 | Loss: 0.00003137
Iteration 43/1000 | Loss: 0.00003137
Iteration 44/1000 | Loss: 0.00003137
Iteration 45/1000 | Loss: 0.00003137
Iteration 46/1000 | Loss: 0.00003137
Iteration 47/1000 | Loss: 0.00003136
Iteration 48/1000 | Loss: 0.00003136
Iteration 49/1000 | Loss: 0.00003136
Iteration 50/1000 | Loss: 0.00003136
Iteration 51/1000 | Loss: 0.00003136
Iteration 52/1000 | Loss: 0.00003136
Iteration 53/1000 | Loss: 0.00003135
Iteration 54/1000 | Loss: 0.00003135
Iteration 55/1000 | Loss: 0.00003134
Iteration 56/1000 | Loss: 0.00003134
Iteration 57/1000 | Loss: 0.00003134
Iteration 58/1000 | Loss: 0.00003134
Iteration 59/1000 | Loss: 0.00003134
Iteration 60/1000 | Loss: 0.00003134
Iteration 61/1000 | Loss: 0.00003134
Iteration 62/1000 | Loss: 0.00003134
Iteration 63/1000 | Loss: 0.00003133
Iteration 64/1000 | Loss: 0.00003133
Iteration 65/1000 | Loss: 0.00003133
Iteration 66/1000 | Loss: 0.00003133
Iteration 67/1000 | Loss: 0.00003133
Iteration 68/1000 | Loss: 0.00003133
Iteration 69/1000 | Loss: 0.00003132
Iteration 70/1000 | Loss: 0.00003132
Iteration 71/1000 | Loss: 0.00003132
Iteration 72/1000 | Loss: 0.00003132
Iteration 73/1000 | Loss: 0.00003132
Iteration 74/1000 | Loss: 0.00003132
Iteration 75/1000 | Loss: 0.00003131
Iteration 76/1000 | Loss: 0.00003131
Iteration 77/1000 | Loss: 0.00003131
Iteration 78/1000 | Loss: 0.00003131
Iteration 79/1000 | Loss: 0.00003131
Iteration 80/1000 | Loss: 0.00003131
Iteration 81/1000 | Loss: 0.00003131
Iteration 82/1000 | Loss: 0.00003131
Iteration 83/1000 | Loss: 0.00003130
Iteration 84/1000 | Loss: 0.00003130
Iteration 85/1000 | Loss: 0.00003130
Iteration 86/1000 | Loss: 0.00003130
Iteration 87/1000 | Loss: 0.00003130
Iteration 88/1000 | Loss: 0.00003130
Iteration 89/1000 | Loss: 0.00003130
Iteration 90/1000 | Loss: 0.00003130
Iteration 91/1000 | Loss: 0.00003129
Iteration 92/1000 | Loss: 0.00003129
Iteration 93/1000 | Loss: 0.00003129
Iteration 94/1000 | Loss: 0.00003129
Iteration 95/1000 | Loss: 0.00003129
Iteration 96/1000 | Loss: 0.00003129
Iteration 97/1000 | Loss: 0.00003129
Iteration 98/1000 | Loss: 0.00003129
Iteration 99/1000 | Loss: 0.00003129
Iteration 100/1000 | Loss: 0.00003129
Iteration 101/1000 | Loss: 0.00003129
Iteration 102/1000 | Loss: 0.00003128
Iteration 103/1000 | Loss: 0.00003128
Iteration 104/1000 | Loss: 0.00003128
Iteration 105/1000 | Loss: 0.00003128
Iteration 106/1000 | Loss: 0.00003128
Iteration 107/1000 | Loss: 0.00003127
Iteration 108/1000 | Loss: 0.00003127
Iteration 109/1000 | Loss: 0.00003127
Iteration 110/1000 | Loss: 0.00003127
Iteration 111/1000 | Loss: 0.00003127
Iteration 112/1000 | Loss: 0.00003127
Iteration 113/1000 | Loss: 0.00003127
Iteration 114/1000 | Loss: 0.00003127
Iteration 115/1000 | Loss: 0.00003127
Iteration 116/1000 | Loss: 0.00003127
Iteration 117/1000 | Loss: 0.00003126
Iteration 118/1000 | Loss: 0.00003126
Iteration 119/1000 | Loss: 0.00003126
Iteration 120/1000 | Loss: 0.00003126
Iteration 121/1000 | Loss: 0.00003126
Iteration 122/1000 | Loss: 0.00003126
Iteration 123/1000 | Loss: 0.00003125
Iteration 124/1000 | Loss: 0.00003125
Iteration 125/1000 | Loss: 0.00003125
Iteration 126/1000 | Loss: 0.00003124
Iteration 127/1000 | Loss: 0.00003124
Iteration 128/1000 | Loss: 0.00003124
Iteration 129/1000 | Loss: 0.00003124
Iteration 130/1000 | Loss: 0.00003124
Iteration 131/1000 | Loss: 0.00003123
Iteration 132/1000 | Loss: 0.00003123
Iteration 133/1000 | Loss: 0.00003123
Iteration 134/1000 | Loss: 0.00003123
Iteration 135/1000 | Loss: 0.00003122
Iteration 136/1000 | Loss: 0.00003122
Iteration 137/1000 | Loss: 0.00003122
Iteration 138/1000 | Loss: 0.00003122
Iteration 139/1000 | Loss: 0.00003121
Iteration 140/1000 | Loss: 0.00003121
Iteration 141/1000 | Loss: 0.00003121
Iteration 142/1000 | Loss: 0.00003121
Iteration 143/1000 | Loss: 0.00003121
Iteration 144/1000 | Loss: 0.00003121
Iteration 145/1000 | Loss: 0.00003121
Iteration 146/1000 | Loss: 0.00003121
Iteration 147/1000 | Loss: 0.00003121
Iteration 148/1000 | Loss: 0.00003120
Iteration 149/1000 | Loss: 0.00003120
Iteration 150/1000 | Loss: 0.00003120
Iteration 151/1000 | Loss: 0.00003120
Iteration 152/1000 | Loss: 0.00003120
Iteration 153/1000 | Loss: 0.00003120
Iteration 154/1000 | Loss: 0.00003120
Iteration 155/1000 | Loss: 0.00003120
Iteration 156/1000 | Loss: 0.00003120
Iteration 157/1000 | Loss: 0.00003120
Iteration 158/1000 | Loss: 0.00003120
Iteration 159/1000 | Loss: 0.00003119
Iteration 160/1000 | Loss: 0.00003119
Iteration 161/1000 | Loss: 0.00003119
Iteration 162/1000 | Loss: 0.00003119
Iteration 163/1000 | Loss: 0.00003119
Iteration 164/1000 | Loss: 0.00003119
Iteration 165/1000 | Loss: 0.00003119
Iteration 166/1000 | Loss: 0.00003119
Iteration 167/1000 | Loss: 0.00003119
Iteration 168/1000 | Loss: 0.00003119
Iteration 169/1000 | Loss: 0.00003119
Iteration 170/1000 | Loss: 0.00003119
Iteration 171/1000 | Loss: 0.00003119
Iteration 172/1000 | Loss: 0.00003119
Iteration 173/1000 | Loss: 0.00003119
Iteration 174/1000 | Loss: 0.00003119
Iteration 175/1000 | Loss: 0.00003119
Iteration 176/1000 | Loss: 0.00003119
Iteration 177/1000 | Loss: 0.00003119
Iteration 178/1000 | Loss: 0.00003118
Iteration 179/1000 | Loss: 0.00003118
Iteration 180/1000 | Loss: 0.00003118
Iteration 181/1000 | Loss: 0.00003118
Iteration 182/1000 | Loss: 0.00003118
Iteration 183/1000 | Loss: 0.00003118
Iteration 184/1000 | Loss: 0.00003118
Iteration 185/1000 | Loss: 0.00003118
Iteration 186/1000 | Loss: 0.00003118
Iteration 187/1000 | Loss: 0.00003118
Iteration 188/1000 | Loss: 0.00003118
Iteration 189/1000 | Loss: 0.00003118
Iteration 190/1000 | Loss: 0.00003118
Iteration 191/1000 | Loss: 0.00003118
Iteration 192/1000 | Loss: 0.00003118
Iteration 193/1000 | Loss: 0.00003118
Iteration 194/1000 | Loss: 0.00003118
Iteration 195/1000 | Loss: 0.00003118
Iteration 196/1000 | Loss: 0.00003118
Iteration 197/1000 | Loss: 0.00003117
Iteration 198/1000 | Loss: 0.00003117
Iteration 199/1000 | Loss: 0.00003117
Iteration 200/1000 | Loss: 0.00003117
Iteration 201/1000 | Loss: 0.00003117
Iteration 202/1000 | Loss: 0.00003117
Iteration 203/1000 | Loss: 0.00003117
Iteration 204/1000 | Loss: 0.00003117
Iteration 205/1000 | Loss: 0.00003117
Iteration 206/1000 | Loss: 0.00003117
Iteration 207/1000 | Loss: 0.00003117
Iteration 208/1000 | Loss: 0.00003117
Iteration 209/1000 | Loss: 0.00003116
Iteration 210/1000 | Loss: 0.00003116
Iteration 211/1000 | Loss: 0.00003116
Iteration 212/1000 | Loss: 0.00003116
Iteration 213/1000 | Loss: 0.00003116
Iteration 214/1000 | Loss: 0.00003116
Iteration 215/1000 | Loss: 0.00003116
Iteration 216/1000 | Loss: 0.00003116
Iteration 217/1000 | Loss: 0.00003116
Iteration 218/1000 | Loss: 0.00003116
Iteration 219/1000 | Loss: 0.00003116
Iteration 220/1000 | Loss: 0.00003116
Iteration 221/1000 | Loss: 0.00003116
Iteration 222/1000 | Loss: 0.00003116
Iteration 223/1000 | Loss: 0.00003116
Iteration 224/1000 | Loss: 0.00003116
Iteration 225/1000 | Loss: 0.00003116
Iteration 226/1000 | Loss: 0.00003116
Iteration 227/1000 | Loss: 0.00003116
Iteration 228/1000 | Loss: 0.00003116
Iteration 229/1000 | Loss: 0.00003116
Iteration 230/1000 | Loss: 0.00003116
Iteration 231/1000 | Loss: 0.00003116
Iteration 232/1000 | Loss: 0.00003116
Iteration 233/1000 | Loss: 0.00003116
Iteration 234/1000 | Loss: 0.00003116
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 234. Stopping optimization.
Last 5 losses: [3.115815343335271e-05, 3.115815343335271e-05, 3.115815343335271e-05, 3.115815343335271e-05, 3.115815343335271e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.115815343335271e-05

Optimization complete. Final v2v error: 4.643660068511963 mm

Highest mean error: 5.353021144866943 mm for frame 121

Lowest mean error: 3.507596492767334 mm for frame 17

Saving results

Total time: 46.58915185928345
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00482065
Iteration 2/25 | Loss: 0.00128785
Iteration 3/25 | Loss: 0.00121208
Iteration 4/25 | Loss: 0.00120149
Iteration 5/25 | Loss: 0.00119875
Iteration 6/25 | Loss: 0.00119808
Iteration 7/25 | Loss: 0.00119808
Iteration 8/25 | Loss: 0.00119808
Iteration 9/25 | Loss: 0.00119808
Iteration 10/25 | Loss: 0.00119808
Iteration 11/25 | Loss: 0.00119808
Iteration 12/25 | Loss: 0.00119808
Iteration 13/25 | Loss: 0.00119808
Iteration 14/25 | Loss: 0.00119808
Iteration 15/25 | Loss: 0.00119808
Iteration 16/25 | Loss: 0.00119808
Iteration 17/25 | Loss: 0.00119808
Iteration 18/25 | Loss: 0.00119808
Iteration 19/25 | Loss: 0.00119808
Iteration 20/25 | Loss: 0.00119808
Iteration 21/25 | Loss: 0.00119808
Iteration 22/25 | Loss: 0.00119808
Iteration 23/25 | Loss: 0.00119808
Iteration 24/25 | Loss: 0.00119808
Iteration 25/25 | Loss: 0.00119808

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32895172
Iteration 2/25 | Loss: 0.00121967
Iteration 3/25 | Loss: 0.00121964
Iteration 4/25 | Loss: 0.00121964
Iteration 5/25 | Loss: 0.00121964
Iteration 6/25 | Loss: 0.00121964
Iteration 7/25 | Loss: 0.00121964
Iteration 8/25 | Loss: 0.00121964
Iteration 9/25 | Loss: 0.00121964
Iteration 10/25 | Loss: 0.00121964
Iteration 11/25 | Loss: 0.00121964
Iteration 12/25 | Loss: 0.00121964
Iteration 13/25 | Loss: 0.00121964
Iteration 14/25 | Loss: 0.00121964
Iteration 15/25 | Loss: 0.00121964
Iteration 16/25 | Loss: 0.00121964
Iteration 17/25 | Loss: 0.00121964
Iteration 18/25 | Loss: 0.00121964
Iteration 19/25 | Loss: 0.00121964
Iteration 20/25 | Loss: 0.00121964
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0012196410680189729, 0.0012196410680189729, 0.0012196410680189729, 0.0012196410680189729, 0.0012196410680189729]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012196410680189729

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121964
Iteration 2/1000 | Loss: 0.00002496
Iteration 3/1000 | Loss: 0.00001729
Iteration 4/1000 | Loss: 0.00001562
Iteration 5/1000 | Loss: 0.00001475
Iteration 6/1000 | Loss: 0.00001433
Iteration 7/1000 | Loss: 0.00001389
Iteration 8/1000 | Loss: 0.00001360
Iteration 9/1000 | Loss: 0.00001338
Iteration 10/1000 | Loss: 0.00001314
Iteration 11/1000 | Loss: 0.00001293
Iteration 12/1000 | Loss: 0.00001288
Iteration 13/1000 | Loss: 0.00001287
Iteration 14/1000 | Loss: 0.00001282
Iteration 15/1000 | Loss: 0.00001276
Iteration 16/1000 | Loss: 0.00001268
Iteration 17/1000 | Loss: 0.00001259
Iteration 18/1000 | Loss: 0.00001256
Iteration 19/1000 | Loss: 0.00001256
Iteration 20/1000 | Loss: 0.00001255
Iteration 21/1000 | Loss: 0.00001254
Iteration 22/1000 | Loss: 0.00001254
Iteration 23/1000 | Loss: 0.00001252
Iteration 24/1000 | Loss: 0.00001252
Iteration 25/1000 | Loss: 0.00001250
Iteration 26/1000 | Loss: 0.00001250
Iteration 27/1000 | Loss: 0.00001249
Iteration 28/1000 | Loss: 0.00001248
Iteration 29/1000 | Loss: 0.00001247
Iteration 30/1000 | Loss: 0.00001246
Iteration 31/1000 | Loss: 0.00001245
Iteration 32/1000 | Loss: 0.00001245
Iteration 33/1000 | Loss: 0.00001244
Iteration 34/1000 | Loss: 0.00001244
Iteration 35/1000 | Loss: 0.00001244
Iteration 36/1000 | Loss: 0.00001243
Iteration 37/1000 | Loss: 0.00001240
Iteration 38/1000 | Loss: 0.00001239
Iteration 39/1000 | Loss: 0.00001239
Iteration 40/1000 | Loss: 0.00001239
Iteration 41/1000 | Loss: 0.00001239
Iteration 42/1000 | Loss: 0.00001239
Iteration 43/1000 | Loss: 0.00001239
Iteration 44/1000 | Loss: 0.00001239
Iteration 45/1000 | Loss: 0.00001238
Iteration 46/1000 | Loss: 0.00001238
Iteration 47/1000 | Loss: 0.00001238
Iteration 48/1000 | Loss: 0.00001238
Iteration 49/1000 | Loss: 0.00001238
Iteration 50/1000 | Loss: 0.00001238
Iteration 51/1000 | Loss: 0.00001238
Iteration 52/1000 | Loss: 0.00001238
Iteration 53/1000 | Loss: 0.00001238
Iteration 54/1000 | Loss: 0.00001238
Iteration 55/1000 | Loss: 0.00001238
Iteration 56/1000 | Loss: 0.00001237
Iteration 57/1000 | Loss: 0.00001237
Iteration 58/1000 | Loss: 0.00001235
Iteration 59/1000 | Loss: 0.00001235
Iteration 60/1000 | Loss: 0.00001235
Iteration 61/1000 | Loss: 0.00001234
Iteration 62/1000 | Loss: 0.00001234
Iteration 63/1000 | Loss: 0.00001233
Iteration 64/1000 | Loss: 0.00001233
Iteration 65/1000 | Loss: 0.00001233
Iteration 66/1000 | Loss: 0.00001232
Iteration 67/1000 | Loss: 0.00001231
Iteration 68/1000 | Loss: 0.00001230
Iteration 69/1000 | Loss: 0.00001229
Iteration 70/1000 | Loss: 0.00001229
Iteration 71/1000 | Loss: 0.00001229
Iteration 72/1000 | Loss: 0.00001229
Iteration 73/1000 | Loss: 0.00001228
Iteration 74/1000 | Loss: 0.00001227
Iteration 75/1000 | Loss: 0.00001226
Iteration 76/1000 | Loss: 0.00001226
Iteration 77/1000 | Loss: 0.00001226
Iteration 78/1000 | Loss: 0.00001226
Iteration 79/1000 | Loss: 0.00001225
Iteration 80/1000 | Loss: 0.00001225
Iteration 81/1000 | Loss: 0.00001224
Iteration 82/1000 | Loss: 0.00001224
Iteration 83/1000 | Loss: 0.00001224
Iteration 84/1000 | Loss: 0.00001223
Iteration 85/1000 | Loss: 0.00001223
Iteration 86/1000 | Loss: 0.00001222
Iteration 87/1000 | Loss: 0.00001222
Iteration 88/1000 | Loss: 0.00001222
Iteration 89/1000 | Loss: 0.00001222
Iteration 90/1000 | Loss: 0.00001221
Iteration 91/1000 | Loss: 0.00001221
Iteration 92/1000 | Loss: 0.00001221
Iteration 93/1000 | Loss: 0.00001221
Iteration 94/1000 | Loss: 0.00001221
Iteration 95/1000 | Loss: 0.00001220
Iteration 96/1000 | Loss: 0.00001220
Iteration 97/1000 | Loss: 0.00001220
Iteration 98/1000 | Loss: 0.00001219
Iteration 99/1000 | Loss: 0.00001218
Iteration 100/1000 | Loss: 0.00001218
Iteration 101/1000 | Loss: 0.00001218
Iteration 102/1000 | Loss: 0.00001218
Iteration 103/1000 | Loss: 0.00001218
Iteration 104/1000 | Loss: 0.00001218
Iteration 105/1000 | Loss: 0.00001218
Iteration 106/1000 | Loss: 0.00001218
Iteration 107/1000 | Loss: 0.00001218
Iteration 108/1000 | Loss: 0.00001218
Iteration 109/1000 | Loss: 0.00001216
Iteration 110/1000 | Loss: 0.00001216
Iteration 111/1000 | Loss: 0.00001215
Iteration 112/1000 | Loss: 0.00001215
Iteration 113/1000 | Loss: 0.00001215
Iteration 114/1000 | Loss: 0.00001215
Iteration 115/1000 | Loss: 0.00001215
Iteration 116/1000 | Loss: 0.00001215
Iteration 117/1000 | Loss: 0.00001214
Iteration 118/1000 | Loss: 0.00001214
Iteration 119/1000 | Loss: 0.00001214
Iteration 120/1000 | Loss: 0.00001214
Iteration 121/1000 | Loss: 0.00001214
Iteration 122/1000 | Loss: 0.00001213
Iteration 123/1000 | Loss: 0.00001213
Iteration 124/1000 | Loss: 0.00001213
Iteration 125/1000 | Loss: 0.00001212
Iteration 126/1000 | Loss: 0.00001212
Iteration 127/1000 | Loss: 0.00001212
Iteration 128/1000 | Loss: 0.00001212
Iteration 129/1000 | Loss: 0.00001212
Iteration 130/1000 | Loss: 0.00001212
Iteration 131/1000 | Loss: 0.00001212
Iteration 132/1000 | Loss: 0.00001212
Iteration 133/1000 | Loss: 0.00001212
Iteration 134/1000 | Loss: 0.00001212
Iteration 135/1000 | Loss: 0.00001212
Iteration 136/1000 | Loss: 0.00001212
Iteration 137/1000 | Loss: 0.00001212
Iteration 138/1000 | Loss: 0.00001212
Iteration 139/1000 | Loss: 0.00001211
Iteration 140/1000 | Loss: 0.00001211
Iteration 141/1000 | Loss: 0.00001211
Iteration 142/1000 | Loss: 0.00001211
Iteration 143/1000 | Loss: 0.00001211
Iteration 144/1000 | Loss: 0.00001211
Iteration 145/1000 | Loss: 0.00001211
Iteration 146/1000 | Loss: 0.00001211
Iteration 147/1000 | Loss: 0.00001211
Iteration 148/1000 | Loss: 0.00001211
Iteration 149/1000 | Loss: 0.00001211
Iteration 150/1000 | Loss: 0.00001211
Iteration 151/1000 | Loss: 0.00001211
Iteration 152/1000 | Loss: 0.00001210
Iteration 153/1000 | Loss: 0.00001210
Iteration 154/1000 | Loss: 0.00001210
Iteration 155/1000 | Loss: 0.00001210
Iteration 156/1000 | Loss: 0.00001210
Iteration 157/1000 | Loss: 0.00001210
Iteration 158/1000 | Loss: 0.00001210
Iteration 159/1000 | Loss: 0.00001210
Iteration 160/1000 | Loss: 0.00001210
Iteration 161/1000 | Loss: 0.00001210
Iteration 162/1000 | Loss: 0.00001210
Iteration 163/1000 | Loss: 0.00001210
Iteration 164/1000 | Loss: 0.00001210
Iteration 165/1000 | Loss: 0.00001209
Iteration 166/1000 | Loss: 0.00001209
Iteration 167/1000 | Loss: 0.00001209
Iteration 168/1000 | Loss: 0.00001209
Iteration 169/1000 | Loss: 0.00001209
Iteration 170/1000 | Loss: 0.00001209
Iteration 171/1000 | Loss: 0.00001209
Iteration 172/1000 | Loss: 0.00001209
Iteration 173/1000 | Loss: 0.00001209
Iteration 174/1000 | Loss: 0.00001209
Iteration 175/1000 | Loss: 0.00001209
Iteration 176/1000 | Loss: 0.00001209
Iteration 177/1000 | Loss: 0.00001209
Iteration 178/1000 | Loss: 0.00001209
Iteration 179/1000 | Loss: 0.00001209
Iteration 180/1000 | Loss: 0.00001209
Iteration 181/1000 | Loss: 0.00001209
Iteration 182/1000 | Loss: 0.00001209
Iteration 183/1000 | Loss: 0.00001209
Iteration 184/1000 | Loss: 0.00001208
Iteration 185/1000 | Loss: 0.00001208
Iteration 186/1000 | Loss: 0.00001208
Iteration 187/1000 | Loss: 0.00001208
Iteration 188/1000 | Loss: 0.00001208
Iteration 189/1000 | Loss: 0.00001208
Iteration 190/1000 | Loss: 0.00001208
Iteration 191/1000 | Loss: 0.00001208
Iteration 192/1000 | Loss: 0.00001208
Iteration 193/1000 | Loss: 0.00001208
Iteration 194/1000 | Loss: 0.00001208
Iteration 195/1000 | Loss: 0.00001208
Iteration 196/1000 | Loss: 0.00001208
Iteration 197/1000 | Loss: 0.00001208
Iteration 198/1000 | Loss: 0.00001208
Iteration 199/1000 | Loss: 0.00001208
Iteration 200/1000 | Loss: 0.00001208
Iteration 201/1000 | Loss: 0.00001208
Iteration 202/1000 | Loss: 0.00001208
Iteration 203/1000 | Loss: 0.00001208
Iteration 204/1000 | Loss: 0.00001208
Iteration 205/1000 | Loss: 0.00001208
Iteration 206/1000 | Loss: 0.00001208
Iteration 207/1000 | Loss: 0.00001208
Iteration 208/1000 | Loss: 0.00001208
Iteration 209/1000 | Loss: 0.00001208
Iteration 210/1000 | Loss: 0.00001208
Iteration 211/1000 | Loss: 0.00001208
Iteration 212/1000 | Loss: 0.00001208
Iteration 213/1000 | Loss: 0.00001208
Iteration 214/1000 | Loss: 0.00001208
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [1.2078443432983477e-05, 1.2078443432983477e-05, 1.2078443432983477e-05, 1.2078443432983477e-05, 1.2078443432983477e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2078443432983477e-05

Optimization complete. Final v2v error: 2.9447500705718994 mm

Highest mean error: 3.369036912918091 mm for frame 48

Lowest mean error: 2.6636533737182617 mm for frame 148

Saving results

Total time: 42.006264209747314
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00401652
Iteration 2/25 | Loss: 0.00133586
Iteration 3/25 | Loss: 0.00119895
Iteration 4/25 | Loss: 0.00118095
Iteration 5/25 | Loss: 0.00117689
Iteration 6/25 | Loss: 0.00117546
Iteration 7/25 | Loss: 0.00117546
Iteration 8/25 | Loss: 0.00117546
Iteration 9/25 | Loss: 0.00117546
Iteration 10/25 | Loss: 0.00117546
Iteration 11/25 | Loss: 0.00117546
Iteration 12/25 | Loss: 0.00117546
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011754640145227313, 0.0011754640145227313, 0.0011754640145227313, 0.0011754640145227313, 0.0011754640145227313]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011754640145227313

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34450376
Iteration 2/25 | Loss: 0.00159400
Iteration 3/25 | Loss: 0.00159399
Iteration 4/25 | Loss: 0.00159399
Iteration 5/25 | Loss: 0.00159399
Iteration 6/25 | Loss: 0.00159399
Iteration 7/25 | Loss: 0.00159399
Iteration 8/25 | Loss: 0.00159399
Iteration 9/25 | Loss: 0.00159399
Iteration 10/25 | Loss: 0.00159399
Iteration 11/25 | Loss: 0.00159399
Iteration 12/25 | Loss: 0.00159399
Iteration 13/25 | Loss: 0.00159399
Iteration 14/25 | Loss: 0.00159399
Iteration 15/25 | Loss: 0.00159399
Iteration 16/25 | Loss: 0.00159399
Iteration 17/25 | Loss: 0.00159399
Iteration 18/25 | Loss: 0.00159399
Iteration 19/25 | Loss: 0.00159399
Iteration 20/25 | Loss: 0.00159399
Iteration 21/25 | Loss: 0.00159399
Iteration 22/25 | Loss: 0.00159399
Iteration 23/25 | Loss: 0.00159399
Iteration 24/25 | Loss: 0.00159399
Iteration 25/25 | Loss: 0.00159399

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00159399
Iteration 2/1000 | Loss: 0.00003634
Iteration 3/1000 | Loss: 0.00002264
Iteration 4/1000 | Loss: 0.00001693
Iteration 5/1000 | Loss: 0.00001513
Iteration 6/1000 | Loss: 0.00001426
Iteration 7/1000 | Loss: 0.00001363
Iteration 8/1000 | Loss: 0.00001328
Iteration 9/1000 | Loss: 0.00001294
Iteration 10/1000 | Loss: 0.00001273
Iteration 11/1000 | Loss: 0.00001259
Iteration 12/1000 | Loss: 0.00001245
Iteration 13/1000 | Loss: 0.00001236
Iteration 14/1000 | Loss: 0.00001227
Iteration 15/1000 | Loss: 0.00001220
Iteration 16/1000 | Loss: 0.00001218
Iteration 17/1000 | Loss: 0.00001217
Iteration 18/1000 | Loss: 0.00001216
Iteration 19/1000 | Loss: 0.00001215
Iteration 20/1000 | Loss: 0.00001215
Iteration 21/1000 | Loss: 0.00001215
Iteration 22/1000 | Loss: 0.00001214
Iteration 23/1000 | Loss: 0.00001214
Iteration 24/1000 | Loss: 0.00001212
Iteration 25/1000 | Loss: 0.00001208
Iteration 26/1000 | Loss: 0.00001207
Iteration 27/1000 | Loss: 0.00001204
Iteration 28/1000 | Loss: 0.00001204
Iteration 29/1000 | Loss: 0.00001202
Iteration 30/1000 | Loss: 0.00001201
Iteration 31/1000 | Loss: 0.00001199
Iteration 32/1000 | Loss: 0.00001198
Iteration 33/1000 | Loss: 0.00001197
Iteration 34/1000 | Loss: 0.00001196
Iteration 35/1000 | Loss: 0.00001194
Iteration 36/1000 | Loss: 0.00001193
Iteration 37/1000 | Loss: 0.00001192
Iteration 38/1000 | Loss: 0.00001191
Iteration 39/1000 | Loss: 0.00001191
Iteration 40/1000 | Loss: 0.00001190
Iteration 41/1000 | Loss: 0.00001190
Iteration 42/1000 | Loss: 0.00001189
Iteration 43/1000 | Loss: 0.00001189
Iteration 44/1000 | Loss: 0.00001189
Iteration 45/1000 | Loss: 0.00001186
Iteration 46/1000 | Loss: 0.00001186
Iteration 47/1000 | Loss: 0.00001185
Iteration 48/1000 | Loss: 0.00001182
Iteration 49/1000 | Loss: 0.00001182
Iteration 50/1000 | Loss: 0.00001182
Iteration 51/1000 | Loss: 0.00001182
Iteration 52/1000 | Loss: 0.00001181
Iteration 53/1000 | Loss: 0.00001181
Iteration 54/1000 | Loss: 0.00001180
Iteration 55/1000 | Loss: 0.00001179
Iteration 56/1000 | Loss: 0.00001179
Iteration 57/1000 | Loss: 0.00001178
Iteration 58/1000 | Loss: 0.00001178
Iteration 59/1000 | Loss: 0.00001178
Iteration 60/1000 | Loss: 0.00001178
Iteration 61/1000 | Loss: 0.00001177
Iteration 62/1000 | Loss: 0.00001177
Iteration 63/1000 | Loss: 0.00001177
Iteration 64/1000 | Loss: 0.00001177
Iteration 65/1000 | Loss: 0.00001176
Iteration 66/1000 | Loss: 0.00001176
Iteration 67/1000 | Loss: 0.00001176
Iteration 68/1000 | Loss: 0.00001175
Iteration 69/1000 | Loss: 0.00001175
Iteration 70/1000 | Loss: 0.00001175
Iteration 71/1000 | Loss: 0.00001175
Iteration 72/1000 | Loss: 0.00001175
Iteration 73/1000 | Loss: 0.00001175
Iteration 74/1000 | Loss: 0.00001175
Iteration 75/1000 | Loss: 0.00001175
Iteration 76/1000 | Loss: 0.00001175
Iteration 77/1000 | Loss: 0.00001175
Iteration 78/1000 | Loss: 0.00001174
Iteration 79/1000 | Loss: 0.00001174
Iteration 80/1000 | Loss: 0.00001174
Iteration 81/1000 | Loss: 0.00001174
Iteration 82/1000 | Loss: 0.00001174
Iteration 83/1000 | Loss: 0.00001174
Iteration 84/1000 | Loss: 0.00001174
Iteration 85/1000 | Loss: 0.00001174
Iteration 86/1000 | Loss: 0.00001174
Iteration 87/1000 | Loss: 0.00001173
Iteration 88/1000 | Loss: 0.00001173
Iteration 89/1000 | Loss: 0.00001173
Iteration 90/1000 | Loss: 0.00001173
Iteration 91/1000 | Loss: 0.00001173
Iteration 92/1000 | Loss: 0.00001173
Iteration 93/1000 | Loss: 0.00001173
Iteration 94/1000 | Loss: 0.00001173
Iteration 95/1000 | Loss: 0.00001173
Iteration 96/1000 | Loss: 0.00001173
Iteration 97/1000 | Loss: 0.00001172
Iteration 98/1000 | Loss: 0.00001172
Iteration 99/1000 | Loss: 0.00001172
Iteration 100/1000 | Loss: 0.00001172
Iteration 101/1000 | Loss: 0.00001172
Iteration 102/1000 | Loss: 0.00001172
Iteration 103/1000 | Loss: 0.00001171
Iteration 104/1000 | Loss: 0.00001171
Iteration 105/1000 | Loss: 0.00001171
Iteration 106/1000 | Loss: 0.00001171
Iteration 107/1000 | Loss: 0.00001171
Iteration 108/1000 | Loss: 0.00001170
Iteration 109/1000 | Loss: 0.00001170
Iteration 110/1000 | Loss: 0.00001170
Iteration 111/1000 | Loss: 0.00001169
Iteration 112/1000 | Loss: 0.00001169
Iteration 113/1000 | Loss: 0.00001169
Iteration 114/1000 | Loss: 0.00001169
Iteration 115/1000 | Loss: 0.00001169
Iteration 116/1000 | Loss: 0.00001169
Iteration 117/1000 | Loss: 0.00001168
Iteration 118/1000 | Loss: 0.00001168
Iteration 119/1000 | Loss: 0.00001168
Iteration 120/1000 | Loss: 0.00001167
Iteration 121/1000 | Loss: 0.00001167
Iteration 122/1000 | Loss: 0.00001167
Iteration 123/1000 | Loss: 0.00001167
Iteration 124/1000 | Loss: 0.00001166
Iteration 125/1000 | Loss: 0.00001166
Iteration 126/1000 | Loss: 0.00001166
Iteration 127/1000 | Loss: 0.00001166
Iteration 128/1000 | Loss: 0.00001165
Iteration 129/1000 | Loss: 0.00001165
Iteration 130/1000 | Loss: 0.00001165
Iteration 131/1000 | Loss: 0.00001165
Iteration 132/1000 | Loss: 0.00001165
Iteration 133/1000 | Loss: 0.00001164
Iteration 134/1000 | Loss: 0.00001164
Iteration 135/1000 | Loss: 0.00001164
Iteration 136/1000 | Loss: 0.00001164
Iteration 137/1000 | Loss: 0.00001164
Iteration 138/1000 | Loss: 0.00001163
Iteration 139/1000 | Loss: 0.00001163
Iteration 140/1000 | Loss: 0.00001163
Iteration 141/1000 | Loss: 0.00001163
Iteration 142/1000 | Loss: 0.00001163
Iteration 143/1000 | Loss: 0.00001163
Iteration 144/1000 | Loss: 0.00001163
Iteration 145/1000 | Loss: 0.00001163
Iteration 146/1000 | Loss: 0.00001162
Iteration 147/1000 | Loss: 0.00001162
Iteration 148/1000 | Loss: 0.00001162
Iteration 149/1000 | Loss: 0.00001162
Iteration 150/1000 | Loss: 0.00001162
Iteration 151/1000 | Loss: 0.00001161
Iteration 152/1000 | Loss: 0.00001161
Iteration 153/1000 | Loss: 0.00001161
Iteration 154/1000 | Loss: 0.00001161
Iteration 155/1000 | Loss: 0.00001161
Iteration 156/1000 | Loss: 0.00001161
Iteration 157/1000 | Loss: 0.00001160
Iteration 158/1000 | Loss: 0.00001160
Iteration 159/1000 | Loss: 0.00001160
Iteration 160/1000 | Loss: 0.00001160
Iteration 161/1000 | Loss: 0.00001160
Iteration 162/1000 | Loss: 0.00001160
Iteration 163/1000 | Loss: 0.00001160
Iteration 164/1000 | Loss: 0.00001160
Iteration 165/1000 | Loss: 0.00001160
Iteration 166/1000 | Loss: 0.00001160
Iteration 167/1000 | Loss: 0.00001159
Iteration 168/1000 | Loss: 0.00001159
Iteration 169/1000 | Loss: 0.00001159
Iteration 170/1000 | Loss: 0.00001159
Iteration 171/1000 | Loss: 0.00001159
Iteration 172/1000 | Loss: 0.00001159
Iteration 173/1000 | Loss: 0.00001158
Iteration 174/1000 | Loss: 0.00001158
Iteration 175/1000 | Loss: 0.00001158
Iteration 176/1000 | Loss: 0.00001157
Iteration 177/1000 | Loss: 0.00001157
Iteration 178/1000 | Loss: 0.00001157
Iteration 179/1000 | Loss: 0.00001156
Iteration 180/1000 | Loss: 0.00001156
Iteration 181/1000 | Loss: 0.00001156
Iteration 182/1000 | Loss: 0.00001156
Iteration 183/1000 | Loss: 0.00001155
Iteration 184/1000 | Loss: 0.00001155
Iteration 185/1000 | Loss: 0.00001155
Iteration 186/1000 | Loss: 0.00001155
Iteration 187/1000 | Loss: 0.00001155
Iteration 188/1000 | Loss: 0.00001154
Iteration 189/1000 | Loss: 0.00001154
Iteration 190/1000 | Loss: 0.00001154
Iteration 191/1000 | Loss: 0.00001154
Iteration 192/1000 | Loss: 0.00001154
Iteration 193/1000 | Loss: 0.00001154
Iteration 194/1000 | Loss: 0.00001154
Iteration 195/1000 | Loss: 0.00001153
Iteration 196/1000 | Loss: 0.00001153
Iteration 197/1000 | Loss: 0.00001153
Iteration 198/1000 | Loss: 0.00001152
Iteration 199/1000 | Loss: 0.00001152
Iteration 200/1000 | Loss: 0.00001152
Iteration 201/1000 | Loss: 0.00001152
Iteration 202/1000 | Loss: 0.00001152
Iteration 203/1000 | Loss: 0.00001152
Iteration 204/1000 | Loss: 0.00001152
Iteration 205/1000 | Loss: 0.00001152
Iteration 206/1000 | Loss: 0.00001151
Iteration 207/1000 | Loss: 0.00001151
Iteration 208/1000 | Loss: 0.00001151
Iteration 209/1000 | Loss: 0.00001151
Iteration 210/1000 | Loss: 0.00001151
Iteration 211/1000 | Loss: 0.00001151
Iteration 212/1000 | Loss: 0.00001151
Iteration 213/1000 | Loss: 0.00001151
Iteration 214/1000 | Loss: 0.00001151
Iteration 215/1000 | Loss: 0.00001151
Iteration 216/1000 | Loss: 0.00001150
Iteration 217/1000 | Loss: 0.00001150
Iteration 218/1000 | Loss: 0.00001150
Iteration 219/1000 | Loss: 0.00001150
Iteration 220/1000 | Loss: 0.00001150
Iteration 221/1000 | Loss: 0.00001150
Iteration 222/1000 | Loss: 0.00001150
Iteration 223/1000 | Loss: 0.00001150
Iteration 224/1000 | Loss: 0.00001150
Iteration 225/1000 | Loss: 0.00001150
Iteration 226/1000 | Loss: 0.00001149
Iteration 227/1000 | Loss: 0.00001149
Iteration 228/1000 | Loss: 0.00001149
Iteration 229/1000 | Loss: 0.00001149
Iteration 230/1000 | Loss: 0.00001149
Iteration 231/1000 | Loss: 0.00001149
Iteration 232/1000 | Loss: 0.00001149
Iteration 233/1000 | Loss: 0.00001148
Iteration 234/1000 | Loss: 0.00001148
Iteration 235/1000 | Loss: 0.00001148
Iteration 236/1000 | Loss: 0.00001148
Iteration 237/1000 | Loss: 0.00001148
Iteration 238/1000 | Loss: 0.00001147
Iteration 239/1000 | Loss: 0.00001147
Iteration 240/1000 | Loss: 0.00001147
Iteration 241/1000 | Loss: 0.00001147
Iteration 242/1000 | Loss: 0.00001147
Iteration 243/1000 | Loss: 0.00001147
Iteration 244/1000 | Loss: 0.00001147
Iteration 245/1000 | Loss: 0.00001147
Iteration 246/1000 | Loss: 0.00001146
Iteration 247/1000 | Loss: 0.00001146
Iteration 248/1000 | Loss: 0.00001146
Iteration 249/1000 | Loss: 0.00001146
Iteration 250/1000 | Loss: 0.00001146
Iteration 251/1000 | Loss: 0.00001146
Iteration 252/1000 | Loss: 0.00001146
Iteration 253/1000 | Loss: 0.00001146
Iteration 254/1000 | Loss: 0.00001146
Iteration 255/1000 | Loss: 0.00001146
Iteration 256/1000 | Loss: 0.00001146
Iteration 257/1000 | Loss: 0.00001146
Iteration 258/1000 | Loss: 0.00001146
Iteration 259/1000 | Loss: 0.00001146
Iteration 260/1000 | Loss: 0.00001146
Iteration 261/1000 | Loss: 0.00001146
Iteration 262/1000 | Loss: 0.00001145
Iteration 263/1000 | Loss: 0.00001145
Iteration 264/1000 | Loss: 0.00001145
Iteration 265/1000 | Loss: 0.00001145
Iteration 266/1000 | Loss: 0.00001145
Iteration 267/1000 | Loss: 0.00001145
Iteration 268/1000 | Loss: 0.00001144
Iteration 269/1000 | Loss: 0.00001144
Iteration 270/1000 | Loss: 0.00001144
Iteration 271/1000 | Loss: 0.00001144
Iteration 272/1000 | Loss: 0.00001144
Iteration 273/1000 | Loss: 0.00001144
Iteration 274/1000 | Loss: 0.00001144
Iteration 275/1000 | Loss: 0.00001144
Iteration 276/1000 | Loss: 0.00001144
Iteration 277/1000 | Loss: 0.00001144
Iteration 278/1000 | Loss: 0.00001144
Iteration 279/1000 | Loss: 0.00001144
Iteration 280/1000 | Loss: 0.00001143
Iteration 281/1000 | Loss: 0.00001143
Iteration 282/1000 | Loss: 0.00001143
Iteration 283/1000 | Loss: 0.00001143
Iteration 284/1000 | Loss: 0.00001143
Iteration 285/1000 | Loss: 0.00001143
Iteration 286/1000 | Loss: 0.00001143
Iteration 287/1000 | Loss: 0.00001143
Iteration 288/1000 | Loss: 0.00001143
Iteration 289/1000 | Loss: 0.00001143
Iteration 290/1000 | Loss: 0.00001143
Iteration 291/1000 | Loss: 0.00001143
Iteration 292/1000 | Loss: 0.00001143
Iteration 293/1000 | Loss: 0.00001143
Iteration 294/1000 | Loss: 0.00001143
Iteration 295/1000 | Loss: 0.00001143
Iteration 296/1000 | Loss: 0.00001143
Iteration 297/1000 | Loss: 0.00001143
Iteration 298/1000 | Loss: 0.00001143
Iteration 299/1000 | Loss: 0.00001143
Iteration 300/1000 | Loss: 0.00001143
Iteration 301/1000 | Loss: 0.00001143
Iteration 302/1000 | Loss: 0.00001143
Iteration 303/1000 | Loss: 0.00001143
Iteration 304/1000 | Loss: 0.00001143
Iteration 305/1000 | Loss: 0.00001143
Iteration 306/1000 | Loss: 0.00001143
Iteration 307/1000 | Loss: 0.00001143
Iteration 308/1000 | Loss: 0.00001143
Iteration 309/1000 | Loss: 0.00001143
Iteration 310/1000 | Loss: 0.00001143
Iteration 311/1000 | Loss: 0.00001143
Iteration 312/1000 | Loss: 0.00001143
Iteration 313/1000 | Loss: 0.00001143
Iteration 314/1000 | Loss: 0.00001143
Iteration 315/1000 | Loss: 0.00001143
Iteration 316/1000 | Loss: 0.00001143
Iteration 317/1000 | Loss: 0.00001143
Iteration 318/1000 | Loss: 0.00001143
Iteration 319/1000 | Loss: 0.00001143
Iteration 320/1000 | Loss: 0.00001143
Iteration 321/1000 | Loss: 0.00001143
Iteration 322/1000 | Loss: 0.00001143
Iteration 323/1000 | Loss: 0.00001143
Iteration 324/1000 | Loss: 0.00001143
Iteration 325/1000 | Loss: 0.00001143
Iteration 326/1000 | Loss: 0.00001143
Iteration 327/1000 | Loss: 0.00001143
Iteration 328/1000 | Loss: 0.00001143
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 328. Stopping optimization.
Last 5 losses: [1.1427985555201303e-05, 1.1427985555201303e-05, 1.1427985555201303e-05, 1.1427985555201303e-05, 1.1427985555201303e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1427985555201303e-05

Optimization complete. Final v2v error: 2.9010424613952637 mm

Highest mean error: 3.659074544906616 mm for frame 106

Lowest mean error: 2.396664619445801 mm for frame 160

Saving results

Total time: 49.58184051513672
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00817513
Iteration 2/25 | Loss: 0.00126095
Iteration 3/25 | Loss: 0.00116139
Iteration 4/25 | Loss: 0.00115094
Iteration 5/25 | Loss: 0.00114916
Iteration 6/25 | Loss: 0.00114891
Iteration 7/25 | Loss: 0.00114891
Iteration 8/25 | Loss: 0.00114891
Iteration 9/25 | Loss: 0.00114891
Iteration 10/25 | Loss: 0.00114891
Iteration 11/25 | Loss: 0.00114891
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011489143362268806, 0.0011489143362268806, 0.0011489143362268806, 0.0011489143362268806, 0.0011489143362268806]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011489143362268806

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29219317
Iteration 2/25 | Loss: 0.00115512
Iteration 3/25 | Loss: 0.00115510
Iteration 4/25 | Loss: 0.00115509
Iteration 5/25 | Loss: 0.00115509
Iteration 6/25 | Loss: 0.00115509
Iteration 7/25 | Loss: 0.00115509
Iteration 8/25 | Loss: 0.00115509
Iteration 9/25 | Loss: 0.00115509
Iteration 10/25 | Loss: 0.00115509
Iteration 11/25 | Loss: 0.00115509
Iteration 12/25 | Loss: 0.00115509
Iteration 13/25 | Loss: 0.00115509
Iteration 14/25 | Loss: 0.00115509
Iteration 15/25 | Loss: 0.00115509
Iteration 16/25 | Loss: 0.00115509
Iteration 17/25 | Loss: 0.00115509
Iteration 18/25 | Loss: 0.00115509
Iteration 19/25 | Loss: 0.00115509
Iteration 20/25 | Loss: 0.00115509
Iteration 21/25 | Loss: 0.00115509
Iteration 22/25 | Loss: 0.00115509
Iteration 23/25 | Loss: 0.00115509
Iteration 24/25 | Loss: 0.00115509
Iteration 25/25 | Loss: 0.00115509

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115509
Iteration 2/1000 | Loss: 0.00002255
Iteration 3/1000 | Loss: 0.00001511
Iteration 4/1000 | Loss: 0.00001153
Iteration 5/1000 | Loss: 0.00001045
Iteration 6/1000 | Loss: 0.00000980
Iteration 7/1000 | Loss: 0.00000928
Iteration 8/1000 | Loss: 0.00000884
Iteration 9/1000 | Loss: 0.00000879
Iteration 10/1000 | Loss: 0.00000865
Iteration 11/1000 | Loss: 0.00000864
Iteration 12/1000 | Loss: 0.00000843
Iteration 13/1000 | Loss: 0.00000832
Iteration 14/1000 | Loss: 0.00000827
Iteration 15/1000 | Loss: 0.00000819
Iteration 16/1000 | Loss: 0.00000817
Iteration 17/1000 | Loss: 0.00000813
Iteration 18/1000 | Loss: 0.00000807
Iteration 19/1000 | Loss: 0.00000804
Iteration 20/1000 | Loss: 0.00000803
Iteration 21/1000 | Loss: 0.00000803
Iteration 22/1000 | Loss: 0.00000803
Iteration 23/1000 | Loss: 0.00000803
Iteration 24/1000 | Loss: 0.00000802
Iteration 25/1000 | Loss: 0.00000802
Iteration 26/1000 | Loss: 0.00000802
Iteration 27/1000 | Loss: 0.00000802
Iteration 28/1000 | Loss: 0.00000802
Iteration 29/1000 | Loss: 0.00000802
Iteration 30/1000 | Loss: 0.00000802
Iteration 31/1000 | Loss: 0.00000801
Iteration 32/1000 | Loss: 0.00000801
Iteration 33/1000 | Loss: 0.00000800
Iteration 34/1000 | Loss: 0.00000800
Iteration 35/1000 | Loss: 0.00000799
Iteration 36/1000 | Loss: 0.00000799
Iteration 37/1000 | Loss: 0.00000798
Iteration 38/1000 | Loss: 0.00000797
Iteration 39/1000 | Loss: 0.00000797
Iteration 40/1000 | Loss: 0.00000797
Iteration 41/1000 | Loss: 0.00000797
Iteration 42/1000 | Loss: 0.00000796
Iteration 43/1000 | Loss: 0.00000796
Iteration 44/1000 | Loss: 0.00000796
Iteration 45/1000 | Loss: 0.00000796
Iteration 46/1000 | Loss: 0.00000796
Iteration 47/1000 | Loss: 0.00000796
Iteration 48/1000 | Loss: 0.00000795
Iteration 49/1000 | Loss: 0.00000795
Iteration 50/1000 | Loss: 0.00000795
Iteration 51/1000 | Loss: 0.00000794
Iteration 52/1000 | Loss: 0.00000794
Iteration 53/1000 | Loss: 0.00000794
Iteration 54/1000 | Loss: 0.00000794
Iteration 55/1000 | Loss: 0.00000794
Iteration 56/1000 | Loss: 0.00000794
Iteration 57/1000 | Loss: 0.00000794
Iteration 58/1000 | Loss: 0.00000794
Iteration 59/1000 | Loss: 0.00000794
Iteration 60/1000 | Loss: 0.00000794
Iteration 61/1000 | Loss: 0.00000793
Iteration 62/1000 | Loss: 0.00000793
Iteration 63/1000 | Loss: 0.00000793
Iteration 64/1000 | Loss: 0.00000793
Iteration 65/1000 | Loss: 0.00000793
Iteration 66/1000 | Loss: 0.00000793
Iteration 67/1000 | Loss: 0.00000793
Iteration 68/1000 | Loss: 0.00000792
Iteration 69/1000 | Loss: 0.00000792
Iteration 70/1000 | Loss: 0.00000792
Iteration 71/1000 | Loss: 0.00000792
Iteration 72/1000 | Loss: 0.00000792
Iteration 73/1000 | Loss: 0.00000791
Iteration 74/1000 | Loss: 0.00000791
Iteration 75/1000 | Loss: 0.00000791
Iteration 76/1000 | Loss: 0.00000791
Iteration 77/1000 | Loss: 0.00000791
Iteration 78/1000 | Loss: 0.00000791
Iteration 79/1000 | Loss: 0.00000791
Iteration 80/1000 | Loss: 0.00000791
Iteration 81/1000 | Loss: 0.00000791
Iteration 82/1000 | Loss: 0.00000791
Iteration 83/1000 | Loss: 0.00000791
Iteration 84/1000 | Loss: 0.00000791
Iteration 85/1000 | Loss: 0.00000791
Iteration 86/1000 | Loss: 0.00000791
Iteration 87/1000 | Loss: 0.00000791
Iteration 88/1000 | Loss: 0.00000791
Iteration 89/1000 | Loss: 0.00000791
Iteration 90/1000 | Loss: 0.00000790
Iteration 91/1000 | Loss: 0.00000790
Iteration 92/1000 | Loss: 0.00000790
Iteration 93/1000 | Loss: 0.00000789
Iteration 94/1000 | Loss: 0.00000789
Iteration 95/1000 | Loss: 0.00000789
Iteration 96/1000 | Loss: 0.00000788
Iteration 97/1000 | Loss: 0.00000788
Iteration 98/1000 | Loss: 0.00000788
Iteration 99/1000 | Loss: 0.00000787
Iteration 100/1000 | Loss: 0.00000787
Iteration 101/1000 | Loss: 0.00000787
Iteration 102/1000 | Loss: 0.00000787
Iteration 103/1000 | Loss: 0.00000787
Iteration 104/1000 | Loss: 0.00000787
Iteration 105/1000 | Loss: 0.00000787
Iteration 106/1000 | Loss: 0.00000787
Iteration 107/1000 | Loss: 0.00000787
Iteration 108/1000 | Loss: 0.00000787
Iteration 109/1000 | Loss: 0.00000787
Iteration 110/1000 | Loss: 0.00000786
Iteration 111/1000 | Loss: 0.00000786
Iteration 112/1000 | Loss: 0.00000786
Iteration 113/1000 | Loss: 0.00000786
Iteration 114/1000 | Loss: 0.00000786
Iteration 115/1000 | Loss: 0.00000785
Iteration 116/1000 | Loss: 0.00000785
Iteration 117/1000 | Loss: 0.00000784
Iteration 118/1000 | Loss: 0.00000784
Iteration 119/1000 | Loss: 0.00000784
Iteration 120/1000 | Loss: 0.00000783
Iteration 121/1000 | Loss: 0.00000783
Iteration 122/1000 | Loss: 0.00000783
Iteration 123/1000 | Loss: 0.00000782
Iteration 124/1000 | Loss: 0.00000782
Iteration 125/1000 | Loss: 0.00000782
Iteration 126/1000 | Loss: 0.00000782
Iteration 127/1000 | Loss: 0.00000781
Iteration 128/1000 | Loss: 0.00000781
Iteration 129/1000 | Loss: 0.00000781
Iteration 130/1000 | Loss: 0.00000780
Iteration 131/1000 | Loss: 0.00000780
Iteration 132/1000 | Loss: 0.00000780
Iteration 133/1000 | Loss: 0.00000780
Iteration 134/1000 | Loss: 0.00000780
Iteration 135/1000 | Loss: 0.00000780
Iteration 136/1000 | Loss: 0.00000779
Iteration 137/1000 | Loss: 0.00000779
Iteration 138/1000 | Loss: 0.00000778
Iteration 139/1000 | Loss: 0.00000778
Iteration 140/1000 | Loss: 0.00000778
Iteration 141/1000 | Loss: 0.00000778
Iteration 142/1000 | Loss: 0.00000777
Iteration 143/1000 | Loss: 0.00000777
Iteration 144/1000 | Loss: 0.00000777
Iteration 145/1000 | Loss: 0.00000777
Iteration 146/1000 | Loss: 0.00000777
Iteration 147/1000 | Loss: 0.00000777
Iteration 148/1000 | Loss: 0.00000777
Iteration 149/1000 | Loss: 0.00000776
Iteration 150/1000 | Loss: 0.00000776
Iteration 151/1000 | Loss: 0.00000776
Iteration 152/1000 | Loss: 0.00000776
Iteration 153/1000 | Loss: 0.00000776
Iteration 154/1000 | Loss: 0.00000776
Iteration 155/1000 | Loss: 0.00000775
Iteration 156/1000 | Loss: 0.00000775
Iteration 157/1000 | Loss: 0.00000775
Iteration 158/1000 | Loss: 0.00000775
Iteration 159/1000 | Loss: 0.00000775
Iteration 160/1000 | Loss: 0.00000775
Iteration 161/1000 | Loss: 0.00000774
Iteration 162/1000 | Loss: 0.00000774
Iteration 163/1000 | Loss: 0.00000774
Iteration 164/1000 | Loss: 0.00000774
Iteration 165/1000 | Loss: 0.00000774
Iteration 166/1000 | Loss: 0.00000774
Iteration 167/1000 | Loss: 0.00000774
Iteration 168/1000 | Loss: 0.00000774
Iteration 169/1000 | Loss: 0.00000774
Iteration 170/1000 | Loss: 0.00000774
Iteration 171/1000 | Loss: 0.00000774
Iteration 172/1000 | Loss: 0.00000774
Iteration 173/1000 | Loss: 0.00000774
Iteration 174/1000 | Loss: 0.00000774
Iteration 175/1000 | Loss: 0.00000774
Iteration 176/1000 | Loss: 0.00000774
Iteration 177/1000 | Loss: 0.00000774
Iteration 178/1000 | Loss: 0.00000774
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [7.73770170781063e-06, 7.73770170781063e-06, 7.73770170781063e-06, 7.73770170781063e-06, 7.73770170781063e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.73770170781063e-06

Optimization complete. Final v2v error: 2.409811019897461 mm

Highest mean error: 2.5508906841278076 mm for frame 23

Lowest mean error: 2.3180062770843506 mm for frame 64

Saving results

Total time: 38.753700256347656
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01024758
Iteration 2/25 | Loss: 0.01024758
Iteration 3/25 | Loss: 0.00268265
Iteration 4/25 | Loss: 0.00192380
Iteration 5/25 | Loss: 0.00151165
Iteration 6/25 | Loss: 0.00145532
Iteration 7/25 | Loss: 0.00141912
Iteration 8/25 | Loss: 0.00136880
Iteration 9/25 | Loss: 0.00135064
Iteration 10/25 | Loss: 0.00133971
Iteration 11/25 | Loss: 0.00133371
Iteration 12/25 | Loss: 0.00134445
Iteration 13/25 | Loss: 0.00133696
Iteration 14/25 | Loss: 0.00132316
Iteration 15/25 | Loss: 0.00131071
Iteration 16/25 | Loss: 0.00131138
Iteration 17/25 | Loss: 0.00130413
Iteration 18/25 | Loss: 0.00130875
Iteration 19/25 | Loss: 0.00130493
Iteration 20/25 | Loss: 0.00130616
Iteration 21/25 | Loss: 0.00130327
Iteration 22/25 | Loss: 0.00130308
Iteration 23/25 | Loss: 0.00130306
Iteration 24/25 | Loss: 0.00130306
Iteration 25/25 | Loss: 0.00130306

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22926593
Iteration 2/25 | Loss: 0.00353915
Iteration 3/25 | Loss: 0.00265926
Iteration 4/25 | Loss: 0.00265926
Iteration 5/25 | Loss: 0.00265926
Iteration 6/25 | Loss: 0.00265926
Iteration 7/25 | Loss: 0.00265926
Iteration 8/25 | Loss: 0.00265926
Iteration 9/25 | Loss: 0.00265926
Iteration 10/25 | Loss: 0.00265926
Iteration 11/25 | Loss: 0.00265926
Iteration 12/25 | Loss: 0.00265926
Iteration 13/25 | Loss: 0.00265926
Iteration 14/25 | Loss: 0.00265926
Iteration 15/25 | Loss: 0.00265926
Iteration 16/25 | Loss: 0.00265926
Iteration 17/25 | Loss: 0.00265926
Iteration 18/25 | Loss: 0.00265926
Iteration 19/25 | Loss: 0.00265926
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.002659259596839547, 0.002659259596839547, 0.002659259596839547, 0.002659259596839547, 0.002659259596839547]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002659259596839547

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00265926
Iteration 2/1000 | Loss: 0.00026167
Iteration 3/1000 | Loss: 0.00033487
Iteration 4/1000 | Loss: 0.00033240
Iteration 5/1000 | Loss: 0.00028381
Iteration 6/1000 | Loss: 0.00039511
Iteration 7/1000 | Loss: 0.00021765
Iteration 8/1000 | Loss: 0.00105859
Iteration 9/1000 | Loss: 0.00020455
Iteration 10/1000 | Loss: 0.00010922
Iteration 11/1000 | Loss: 0.00009220
Iteration 12/1000 | Loss: 0.00036129
Iteration 13/1000 | Loss: 0.00052390
Iteration 14/1000 | Loss: 0.00008520
Iteration 15/1000 | Loss: 0.00007786
Iteration 16/1000 | Loss: 0.00158581
Iteration 17/1000 | Loss: 0.00015364
Iteration 18/1000 | Loss: 0.00009734
Iteration 19/1000 | Loss: 0.00007273
Iteration 20/1000 | Loss: 0.00006826
Iteration 21/1000 | Loss: 0.00034447
Iteration 22/1000 | Loss: 0.00089666
Iteration 23/1000 | Loss: 0.00030369
Iteration 24/1000 | Loss: 0.00071262
Iteration 25/1000 | Loss: 0.00274538
Iteration 26/1000 | Loss: 0.00219060
Iteration 27/1000 | Loss: 0.00218577
Iteration 28/1000 | Loss: 0.00118806
Iteration 29/1000 | Loss: 0.00035252
Iteration 30/1000 | Loss: 0.00019697
Iteration 31/1000 | Loss: 0.00016006
Iteration 32/1000 | Loss: 0.00031431
Iteration 33/1000 | Loss: 0.00006635
Iteration 34/1000 | Loss: 0.00025492
Iteration 35/1000 | Loss: 0.00006573
Iteration 36/1000 | Loss: 0.00013352
Iteration 37/1000 | Loss: 0.00009266
Iteration 38/1000 | Loss: 0.00007602
Iteration 39/1000 | Loss: 0.00022646
Iteration 40/1000 | Loss: 0.00013321
Iteration 41/1000 | Loss: 0.00017752
Iteration 42/1000 | Loss: 0.00005248
Iteration 43/1000 | Loss: 0.00005090
Iteration 44/1000 | Loss: 0.00263962
Iteration 45/1000 | Loss: 0.00442888
Iteration 46/1000 | Loss: 0.00256956
Iteration 47/1000 | Loss: 0.00107235
Iteration 48/1000 | Loss: 0.00044964
Iteration 49/1000 | Loss: 0.00027131
Iteration 50/1000 | Loss: 0.00019851
Iteration 51/1000 | Loss: 0.00188426
Iteration 52/1000 | Loss: 0.00022818
Iteration 53/1000 | Loss: 0.00012100
Iteration 54/1000 | Loss: 0.00010311
Iteration 55/1000 | Loss: 0.00030336
Iteration 56/1000 | Loss: 0.00066740
Iteration 57/1000 | Loss: 0.00028138
Iteration 58/1000 | Loss: 0.00021222
Iteration 59/1000 | Loss: 0.00004513
Iteration 60/1000 | Loss: 0.00003344
Iteration 61/1000 | Loss: 0.00002901
Iteration 62/1000 | Loss: 0.00038106
Iteration 63/1000 | Loss: 0.00005772
Iteration 64/1000 | Loss: 0.00002803
Iteration 65/1000 | Loss: 0.00002522
Iteration 66/1000 | Loss: 0.00002241
Iteration 67/1000 | Loss: 0.00004526
Iteration 68/1000 | Loss: 0.00017125
Iteration 69/1000 | Loss: 0.00013078
Iteration 70/1000 | Loss: 0.00002058
Iteration 71/1000 | Loss: 0.00001825
Iteration 72/1000 | Loss: 0.00006302
Iteration 73/1000 | Loss: 0.00078451
Iteration 74/1000 | Loss: 0.00017526
Iteration 75/1000 | Loss: 0.00013710
Iteration 76/1000 | Loss: 0.00001852
Iteration 77/1000 | Loss: 0.00001753
Iteration 78/1000 | Loss: 0.00016053
Iteration 79/1000 | Loss: 0.00013595
Iteration 80/1000 | Loss: 0.00007940
Iteration 81/1000 | Loss: 0.00007962
Iteration 82/1000 | Loss: 0.00017280
Iteration 83/1000 | Loss: 0.00017274
Iteration 84/1000 | Loss: 0.00002257
Iteration 85/1000 | Loss: 0.00003707
Iteration 86/1000 | Loss: 0.00001893
Iteration 87/1000 | Loss: 0.00001841
Iteration 88/1000 | Loss: 0.00009652
Iteration 89/1000 | Loss: 0.00016832
Iteration 90/1000 | Loss: 0.00002230
Iteration 91/1000 | Loss: 0.00003609
Iteration 92/1000 | Loss: 0.00001979
Iteration 93/1000 | Loss: 0.00003061
Iteration 94/1000 | Loss: 0.00004100
Iteration 95/1000 | Loss: 0.00001746
Iteration 96/1000 | Loss: 0.00005235
Iteration 97/1000 | Loss: 0.00001801
Iteration 98/1000 | Loss: 0.00001657
Iteration 99/1000 | Loss: 0.00001722
Iteration 100/1000 | Loss: 0.00001617
Iteration 101/1000 | Loss: 0.00001617
Iteration 102/1000 | Loss: 0.00001617
Iteration 103/1000 | Loss: 0.00001616
Iteration 104/1000 | Loss: 0.00001609
Iteration 105/1000 | Loss: 0.00001609
Iteration 106/1000 | Loss: 0.00001807
Iteration 107/1000 | Loss: 0.00003807
Iteration 108/1000 | Loss: 0.00003807
Iteration 109/1000 | Loss: 0.00001928
Iteration 110/1000 | Loss: 0.00001661
Iteration 111/1000 | Loss: 0.00001596
Iteration 112/1000 | Loss: 0.00001590
Iteration 113/1000 | Loss: 0.00001589
Iteration 114/1000 | Loss: 0.00001589
Iteration 115/1000 | Loss: 0.00001587
Iteration 116/1000 | Loss: 0.00001587
Iteration 117/1000 | Loss: 0.00001587
Iteration 118/1000 | Loss: 0.00001586
Iteration 119/1000 | Loss: 0.00001585
Iteration 120/1000 | Loss: 0.00001585
Iteration 121/1000 | Loss: 0.00001585
Iteration 122/1000 | Loss: 0.00001585
Iteration 123/1000 | Loss: 0.00001584
Iteration 124/1000 | Loss: 0.00001584
Iteration 125/1000 | Loss: 0.00001584
Iteration 126/1000 | Loss: 0.00001583
Iteration 127/1000 | Loss: 0.00001583
Iteration 128/1000 | Loss: 0.00001583
Iteration 129/1000 | Loss: 0.00001582
Iteration 130/1000 | Loss: 0.00001582
Iteration 131/1000 | Loss: 0.00001582
Iteration 132/1000 | Loss: 0.00001582
Iteration 133/1000 | Loss: 0.00001582
Iteration 134/1000 | Loss: 0.00001582
Iteration 135/1000 | Loss: 0.00001581
Iteration 136/1000 | Loss: 0.00001581
Iteration 137/1000 | Loss: 0.00001581
Iteration 138/1000 | Loss: 0.00001581
Iteration 139/1000 | Loss: 0.00001581
Iteration 140/1000 | Loss: 0.00001581
Iteration 141/1000 | Loss: 0.00001581
Iteration 142/1000 | Loss: 0.00001580
Iteration 143/1000 | Loss: 0.00001580
Iteration 144/1000 | Loss: 0.00001580
Iteration 145/1000 | Loss: 0.00001580
Iteration 146/1000 | Loss: 0.00001580
Iteration 147/1000 | Loss: 0.00001580
Iteration 148/1000 | Loss: 0.00001580
Iteration 149/1000 | Loss: 0.00001580
Iteration 150/1000 | Loss: 0.00001580
Iteration 151/1000 | Loss: 0.00001580
Iteration 152/1000 | Loss: 0.00001580
Iteration 153/1000 | Loss: 0.00001580
Iteration 154/1000 | Loss: 0.00001580
Iteration 155/1000 | Loss: 0.00001580
Iteration 156/1000 | Loss: 0.00001580
Iteration 157/1000 | Loss: 0.00001580
Iteration 158/1000 | Loss: 0.00001580
Iteration 159/1000 | Loss: 0.00001580
Iteration 160/1000 | Loss: 0.00001580
Iteration 161/1000 | Loss: 0.00001580
Iteration 162/1000 | Loss: 0.00001580
Iteration 163/1000 | Loss: 0.00001580
Iteration 164/1000 | Loss: 0.00001580
Iteration 165/1000 | Loss: 0.00001580
Iteration 166/1000 | Loss: 0.00001580
Iteration 167/1000 | Loss: 0.00001580
Iteration 168/1000 | Loss: 0.00001580
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.5800242181285284e-05, 1.5800242181285284e-05, 1.5800242181285284e-05, 1.5800242181285284e-05, 1.5800242181285284e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5800242181285284e-05

Optimization complete. Final v2v error: 3.372230052947998 mm

Highest mean error: 5.686829566955566 mm for frame 209

Lowest mean error: 2.8838186264038086 mm for frame 23

Saving results

Total time: 214.2436535358429
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00680562
Iteration 2/25 | Loss: 0.00171443
Iteration 3/25 | Loss: 0.00143689
Iteration 4/25 | Loss: 0.00135154
Iteration 5/25 | Loss: 0.00134128
Iteration 6/25 | Loss: 0.00130411
Iteration 7/25 | Loss: 0.00130610
Iteration 8/25 | Loss: 0.00130927
Iteration 9/25 | Loss: 0.00130646
Iteration 10/25 | Loss: 0.00130556
Iteration 11/25 | Loss: 0.00129308
Iteration 12/25 | Loss: 0.00129466
Iteration 13/25 | Loss: 0.00129216
Iteration 14/25 | Loss: 0.00128937
Iteration 15/25 | Loss: 0.00128590
Iteration 16/25 | Loss: 0.00128565
Iteration 17/25 | Loss: 0.00128554
Iteration 18/25 | Loss: 0.00128553
Iteration 19/25 | Loss: 0.00128550
Iteration 20/25 | Loss: 0.00128550
Iteration 21/25 | Loss: 0.00128549
Iteration 22/25 | Loss: 0.00128549
Iteration 23/25 | Loss: 0.00128549
Iteration 24/25 | Loss: 0.00128549
Iteration 25/25 | Loss: 0.00128549

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.19141054
Iteration 2/25 | Loss: 0.00125186
Iteration 3/25 | Loss: 0.00125181
Iteration 4/25 | Loss: 0.00125181
Iteration 5/25 | Loss: 0.00125181
Iteration 6/25 | Loss: 0.00125181
Iteration 7/25 | Loss: 0.00125181
Iteration 8/25 | Loss: 0.00125181
Iteration 9/25 | Loss: 0.00125181
Iteration 10/25 | Loss: 0.00125181
Iteration 11/25 | Loss: 0.00125181
Iteration 12/25 | Loss: 0.00125181
Iteration 13/25 | Loss: 0.00125181
Iteration 14/25 | Loss: 0.00125181
Iteration 15/25 | Loss: 0.00125181
Iteration 16/25 | Loss: 0.00125181
Iteration 17/25 | Loss: 0.00125181
Iteration 18/25 | Loss: 0.00125181
Iteration 19/25 | Loss: 0.00125181
Iteration 20/25 | Loss: 0.00125181
Iteration 21/25 | Loss: 0.00125181
Iteration 22/25 | Loss: 0.00125181
Iteration 23/25 | Loss: 0.00125181
Iteration 24/25 | Loss: 0.00125181
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.001251810579560697, 0.001251810579560697, 0.001251810579560697, 0.001251810579560697, 0.001251810579560697]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001251810579560697

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125181
Iteration 2/1000 | Loss: 0.00026908
Iteration 3/1000 | Loss: 0.00003080
Iteration 4/1000 | Loss: 0.00002549
Iteration 5/1000 | Loss: 0.00024297
Iteration 6/1000 | Loss: 0.00003108
Iteration 7/1000 | Loss: 0.00002599
Iteration 8/1000 | Loss: 0.00002473
Iteration 9/1000 | Loss: 0.00002403
Iteration 10/1000 | Loss: 0.00002343
Iteration 11/1000 | Loss: 0.00002256
Iteration 12/1000 | Loss: 0.00011607
Iteration 13/1000 | Loss: 0.00002448
Iteration 14/1000 | Loss: 0.00002208
Iteration 15/1000 | Loss: 0.00002118
Iteration 16/1000 | Loss: 0.00002006
Iteration 17/1000 | Loss: 0.00001922
Iteration 18/1000 | Loss: 0.00001860
Iteration 19/1000 | Loss: 0.00001816
Iteration 20/1000 | Loss: 0.00001768
Iteration 21/1000 | Loss: 0.00029766
Iteration 22/1000 | Loss: 0.00002190
Iteration 23/1000 | Loss: 0.00002044
Iteration 24/1000 | Loss: 0.00001881
Iteration 25/1000 | Loss: 0.00001736
Iteration 26/1000 | Loss: 0.00001674
Iteration 27/1000 | Loss: 0.00001647
Iteration 28/1000 | Loss: 0.00001631
Iteration 29/1000 | Loss: 0.00001629
Iteration 30/1000 | Loss: 0.00001628
Iteration 31/1000 | Loss: 0.00001628
Iteration 32/1000 | Loss: 0.00001624
Iteration 33/1000 | Loss: 0.00001624
Iteration 34/1000 | Loss: 0.00001623
Iteration 35/1000 | Loss: 0.00001623
Iteration 36/1000 | Loss: 0.00001622
Iteration 37/1000 | Loss: 0.00001622
Iteration 38/1000 | Loss: 0.00001621
Iteration 39/1000 | Loss: 0.00001620
Iteration 40/1000 | Loss: 0.00001620
Iteration 41/1000 | Loss: 0.00001620
Iteration 42/1000 | Loss: 0.00001619
Iteration 43/1000 | Loss: 0.00001619
Iteration 44/1000 | Loss: 0.00001618
Iteration 45/1000 | Loss: 0.00001618
Iteration 46/1000 | Loss: 0.00001617
Iteration 47/1000 | Loss: 0.00001617
Iteration 48/1000 | Loss: 0.00001616
Iteration 49/1000 | Loss: 0.00001616
Iteration 50/1000 | Loss: 0.00001616
Iteration 51/1000 | Loss: 0.00001615
Iteration 52/1000 | Loss: 0.00001615
Iteration 53/1000 | Loss: 0.00001614
Iteration 54/1000 | Loss: 0.00001614
Iteration 55/1000 | Loss: 0.00001614
Iteration 56/1000 | Loss: 0.00001613
Iteration 57/1000 | Loss: 0.00001612
Iteration 58/1000 | Loss: 0.00001612
Iteration 59/1000 | Loss: 0.00001612
Iteration 60/1000 | Loss: 0.00001612
Iteration 61/1000 | Loss: 0.00001611
Iteration 62/1000 | Loss: 0.00001611
Iteration 63/1000 | Loss: 0.00001611
Iteration 64/1000 | Loss: 0.00001610
Iteration 65/1000 | Loss: 0.00001610
Iteration 66/1000 | Loss: 0.00001610
Iteration 67/1000 | Loss: 0.00001610
Iteration 68/1000 | Loss: 0.00001609
Iteration 69/1000 | Loss: 0.00001609
Iteration 70/1000 | Loss: 0.00001609
Iteration 71/1000 | Loss: 0.00001608
Iteration 72/1000 | Loss: 0.00001608
Iteration 73/1000 | Loss: 0.00001607
Iteration 74/1000 | Loss: 0.00001607
Iteration 75/1000 | Loss: 0.00001607
Iteration 76/1000 | Loss: 0.00001607
Iteration 77/1000 | Loss: 0.00001606
Iteration 78/1000 | Loss: 0.00001606
Iteration 79/1000 | Loss: 0.00001606
Iteration 80/1000 | Loss: 0.00001606
Iteration 81/1000 | Loss: 0.00001605
Iteration 82/1000 | Loss: 0.00001605
Iteration 83/1000 | Loss: 0.00001605
Iteration 84/1000 | Loss: 0.00001605
Iteration 85/1000 | Loss: 0.00001604
Iteration 86/1000 | Loss: 0.00001600
Iteration 87/1000 | Loss: 0.00001600
Iteration 88/1000 | Loss: 0.00001597
Iteration 89/1000 | Loss: 0.00001597
Iteration 90/1000 | Loss: 0.00001593
Iteration 91/1000 | Loss: 0.00001592
Iteration 92/1000 | Loss: 0.00001592
Iteration 93/1000 | Loss: 0.00001591
Iteration 94/1000 | Loss: 0.00001591
Iteration 95/1000 | Loss: 0.00001591
Iteration 96/1000 | Loss: 0.00001590
Iteration 97/1000 | Loss: 0.00001590
Iteration 98/1000 | Loss: 0.00001589
Iteration 99/1000 | Loss: 0.00001589
Iteration 100/1000 | Loss: 0.00001588
Iteration 101/1000 | Loss: 0.00001588
Iteration 102/1000 | Loss: 0.00001588
Iteration 103/1000 | Loss: 0.00001587
Iteration 104/1000 | Loss: 0.00001587
Iteration 105/1000 | Loss: 0.00001587
Iteration 106/1000 | Loss: 0.00001587
Iteration 107/1000 | Loss: 0.00001587
Iteration 108/1000 | Loss: 0.00001586
Iteration 109/1000 | Loss: 0.00001586
Iteration 110/1000 | Loss: 0.00001586
Iteration 111/1000 | Loss: 0.00001586
Iteration 112/1000 | Loss: 0.00001585
Iteration 113/1000 | Loss: 0.00001585
Iteration 114/1000 | Loss: 0.00001585
Iteration 115/1000 | Loss: 0.00001584
Iteration 116/1000 | Loss: 0.00001584
Iteration 117/1000 | Loss: 0.00001584
Iteration 118/1000 | Loss: 0.00001584
Iteration 119/1000 | Loss: 0.00001584
Iteration 120/1000 | Loss: 0.00001584
Iteration 121/1000 | Loss: 0.00001584
Iteration 122/1000 | Loss: 0.00001584
Iteration 123/1000 | Loss: 0.00001584
Iteration 124/1000 | Loss: 0.00001584
Iteration 125/1000 | Loss: 0.00001584
Iteration 126/1000 | Loss: 0.00001584
Iteration 127/1000 | Loss: 0.00001584
Iteration 128/1000 | Loss: 0.00001584
Iteration 129/1000 | Loss: 0.00001584
Iteration 130/1000 | Loss: 0.00001584
Iteration 131/1000 | Loss: 0.00001583
Iteration 132/1000 | Loss: 0.00001583
Iteration 133/1000 | Loss: 0.00001583
Iteration 134/1000 | Loss: 0.00001583
Iteration 135/1000 | Loss: 0.00001583
Iteration 136/1000 | Loss: 0.00001583
Iteration 137/1000 | Loss: 0.00001583
Iteration 138/1000 | Loss: 0.00001583
Iteration 139/1000 | Loss: 0.00001583
Iteration 140/1000 | Loss: 0.00001583
Iteration 141/1000 | Loss: 0.00001583
Iteration 142/1000 | Loss: 0.00001583
Iteration 143/1000 | Loss: 0.00001583
Iteration 144/1000 | Loss: 0.00001583
Iteration 145/1000 | Loss: 0.00001582
Iteration 146/1000 | Loss: 0.00001582
Iteration 147/1000 | Loss: 0.00001582
Iteration 148/1000 | Loss: 0.00001582
Iteration 149/1000 | Loss: 0.00001582
Iteration 150/1000 | Loss: 0.00001582
Iteration 151/1000 | Loss: 0.00001582
Iteration 152/1000 | Loss: 0.00001582
Iteration 153/1000 | Loss: 0.00001582
Iteration 154/1000 | Loss: 0.00001581
Iteration 155/1000 | Loss: 0.00001581
Iteration 156/1000 | Loss: 0.00001581
Iteration 157/1000 | Loss: 0.00001581
Iteration 158/1000 | Loss: 0.00001580
Iteration 159/1000 | Loss: 0.00001580
Iteration 160/1000 | Loss: 0.00001580
Iteration 161/1000 | Loss: 0.00001580
Iteration 162/1000 | Loss: 0.00001580
Iteration 163/1000 | Loss: 0.00001579
Iteration 164/1000 | Loss: 0.00001579
Iteration 165/1000 | Loss: 0.00001579
Iteration 166/1000 | Loss: 0.00001579
Iteration 167/1000 | Loss: 0.00001579
Iteration 168/1000 | Loss: 0.00001578
Iteration 169/1000 | Loss: 0.00001578
Iteration 170/1000 | Loss: 0.00001578
Iteration 171/1000 | Loss: 0.00001577
Iteration 172/1000 | Loss: 0.00001577
Iteration 173/1000 | Loss: 0.00001577
Iteration 174/1000 | Loss: 0.00001577
Iteration 175/1000 | Loss: 0.00001577
Iteration 176/1000 | Loss: 0.00001576
Iteration 177/1000 | Loss: 0.00001576
Iteration 178/1000 | Loss: 0.00001576
Iteration 179/1000 | Loss: 0.00001576
Iteration 180/1000 | Loss: 0.00001576
Iteration 181/1000 | Loss: 0.00001576
Iteration 182/1000 | Loss: 0.00001576
Iteration 183/1000 | Loss: 0.00001576
Iteration 184/1000 | Loss: 0.00001576
Iteration 185/1000 | Loss: 0.00001576
Iteration 186/1000 | Loss: 0.00001576
Iteration 187/1000 | Loss: 0.00001576
Iteration 188/1000 | Loss: 0.00001576
Iteration 189/1000 | Loss: 0.00001576
Iteration 190/1000 | Loss: 0.00001576
Iteration 191/1000 | Loss: 0.00001576
Iteration 192/1000 | Loss: 0.00001576
Iteration 193/1000 | Loss: 0.00001576
Iteration 194/1000 | Loss: 0.00001576
Iteration 195/1000 | Loss: 0.00001576
Iteration 196/1000 | Loss: 0.00001576
Iteration 197/1000 | Loss: 0.00001576
Iteration 198/1000 | Loss: 0.00001576
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [1.5763896954013035e-05, 1.5763896954013035e-05, 1.5763896954013035e-05, 1.5763896954013035e-05, 1.5763896954013035e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5763896954013035e-05

Optimization complete. Final v2v error: 3.2517850399017334 mm

Highest mean error: 6.905277252197266 mm for frame 17

Lowest mean error: 2.781097650527954 mm for frame 133

Saving results

Total time: 86.625070810318
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00919507
Iteration 2/25 | Loss: 0.00170188
Iteration 3/25 | Loss: 0.00131974
Iteration 4/25 | Loss: 0.00129610
Iteration 5/25 | Loss: 0.00128991
Iteration 6/25 | Loss: 0.00128817
Iteration 7/25 | Loss: 0.00128817
Iteration 8/25 | Loss: 0.00128817
Iteration 9/25 | Loss: 0.00128817
Iteration 10/25 | Loss: 0.00128817
Iteration 11/25 | Loss: 0.00128817
Iteration 12/25 | Loss: 0.00128817
Iteration 13/25 | Loss: 0.00128817
Iteration 14/25 | Loss: 0.00128817
Iteration 15/25 | Loss: 0.00128817
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012881739530712366, 0.0012881739530712366, 0.0012881739530712366, 0.0012881739530712366, 0.0012881739530712366]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012881739530712366

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.86527407
Iteration 2/25 | Loss: 0.00122193
Iteration 3/25 | Loss: 0.00122192
Iteration 4/25 | Loss: 0.00122192
Iteration 5/25 | Loss: 0.00122192
Iteration 6/25 | Loss: 0.00122192
Iteration 7/25 | Loss: 0.00122192
Iteration 8/25 | Loss: 0.00122192
Iteration 9/25 | Loss: 0.00122192
Iteration 10/25 | Loss: 0.00122192
Iteration 11/25 | Loss: 0.00122192
Iteration 12/25 | Loss: 0.00122192
Iteration 13/25 | Loss: 0.00122192
Iteration 14/25 | Loss: 0.00122192
Iteration 15/25 | Loss: 0.00122191
Iteration 16/25 | Loss: 0.00122192
Iteration 17/25 | Loss: 0.00122192
Iteration 18/25 | Loss: 0.00122192
Iteration 19/25 | Loss: 0.00122192
Iteration 20/25 | Loss: 0.00122191
Iteration 21/25 | Loss: 0.00122191
Iteration 22/25 | Loss: 0.00122191
Iteration 23/25 | Loss: 0.00122191
Iteration 24/25 | Loss: 0.00122191
Iteration 25/25 | Loss: 0.00122191
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0012219148920848966, 0.0012219148920848966, 0.0012219148920848966, 0.0012219148920848966, 0.0012219148920848966]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012219148920848966

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122191
Iteration 2/1000 | Loss: 0.00005385
Iteration 3/1000 | Loss: 0.00003856
Iteration 4/1000 | Loss: 0.00003413
Iteration 5/1000 | Loss: 0.00003252
Iteration 6/1000 | Loss: 0.00003083
Iteration 7/1000 | Loss: 0.00002980
Iteration 8/1000 | Loss: 0.00002891
Iteration 9/1000 | Loss: 0.00002839
Iteration 10/1000 | Loss: 0.00002803
Iteration 11/1000 | Loss: 0.00002777
Iteration 12/1000 | Loss: 0.00002745
Iteration 13/1000 | Loss: 0.00002721
Iteration 14/1000 | Loss: 0.00002695
Iteration 15/1000 | Loss: 0.00002670
Iteration 16/1000 | Loss: 0.00002650
Iteration 17/1000 | Loss: 0.00002644
Iteration 18/1000 | Loss: 0.00002628
Iteration 19/1000 | Loss: 0.00002615
Iteration 20/1000 | Loss: 0.00002609
Iteration 21/1000 | Loss: 0.00002603
Iteration 22/1000 | Loss: 0.00002602
Iteration 23/1000 | Loss: 0.00002601
Iteration 24/1000 | Loss: 0.00002596
Iteration 25/1000 | Loss: 0.00002596
Iteration 26/1000 | Loss: 0.00002590
Iteration 27/1000 | Loss: 0.00002582
Iteration 28/1000 | Loss: 0.00002576
Iteration 29/1000 | Loss: 0.00002576
Iteration 30/1000 | Loss: 0.00002575
Iteration 31/1000 | Loss: 0.00002574
Iteration 32/1000 | Loss: 0.00002574
Iteration 33/1000 | Loss: 0.00002574
Iteration 34/1000 | Loss: 0.00002574
Iteration 35/1000 | Loss: 0.00002574
Iteration 36/1000 | Loss: 0.00002574
Iteration 37/1000 | Loss: 0.00002574
Iteration 38/1000 | Loss: 0.00002574
Iteration 39/1000 | Loss: 0.00002574
Iteration 40/1000 | Loss: 0.00002574
Iteration 41/1000 | Loss: 0.00002573
Iteration 42/1000 | Loss: 0.00002573
Iteration 43/1000 | Loss: 0.00002573
Iteration 44/1000 | Loss: 0.00002573
Iteration 45/1000 | Loss: 0.00002573
Iteration 46/1000 | Loss: 0.00002573
Iteration 47/1000 | Loss: 0.00002573
Iteration 48/1000 | Loss: 0.00002573
Iteration 49/1000 | Loss: 0.00002573
Iteration 50/1000 | Loss: 0.00002573
Iteration 51/1000 | Loss: 0.00002572
Iteration 52/1000 | Loss: 0.00002572
Iteration 53/1000 | Loss: 0.00002572
Iteration 54/1000 | Loss: 0.00002572
Iteration 55/1000 | Loss: 0.00002572
Iteration 56/1000 | Loss: 0.00002571
Iteration 57/1000 | Loss: 0.00002571
Iteration 58/1000 | Loss: 0.00002571
Iteration 59/1000 | Loss: 0.00002571
Iteration 60/1000 | Loss: 0.00002571
Iteration 61/1000 | Loss: 0.00002571
Iteration 62/1000 | Loss: 0.00002571
Iteration 63/1000 | Loss: 0.00002571
Iteration 64/1000 | Loss: 0.00002570
Iteration 65/1000 | Loss: 0.00002570
Iteration 66/1000 | Loss: 0.00002570
Iteration 67/1000 | Loss: 0.00002570
Iteration 68/1000 | Loss: 0.00002570
Iteration 69/1000 | Loss: 0.00002570
Iteration 70/1000 | Loss: 0.00002570
Iteration 71/1000 | Loss: 0.00002570
Iteration 72/1000 | Loss: 0.00002570
Iteration 73/1000 | Loss: 0.00002570
Iteration 74/1000 | Loss: 0.00002569
Iteration 75/1000 | Loss: 0.00002569
Iteration 76/1000 | Loss: 0.00002569
Iteration 77/1000 | Loss: 0.00002569
Iteration 78/1000 | Loss: 0.00002569
Iteration 79/1000 | Loss: 0.00002569
Iteration 80/1000 | Loss: 0.00002569
Iteration 81/1000 | Loss: 0.00002569
Iteration 82/1000 | Loss: 0.00002568
Iteration 83/1000 | Loss: 0.00002568
Iteration 84/1000 | Loss: 0.00002568
Iteration 85/1000 | Loss: 0.00002568
Iteration 86/1000 | Loss: 0.00002568
Iteration 87/1000 | Loss: 0.00002568
Iteration 88/1000 | Loss: 0.00002567
Iteration 89/1000 | Loss: 0.00002567
Iteration 90/1000 | Loss: 0.00002567
Iteration 91/1000 | Loss: 0.00002567
Iteration 92/1000 | Loss: 0.00002567
Iteration 93/1000 | Loss: 0.00002567
Iteration 94/1000 | Loss: 0.00002567
Iteration 95/1000 | Loss: 0.00002567
Iteration 96/1000 | Loss: 0.00002567
Iteration 97/1000 | Loss: 0.00002567
Iteration 98/1000 | Loss: 0.00002567
Iteration 99/1000 | Loss: 0.00002567
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [2.5672215997474268e-05, 2.5672215997474268e-05, 2.5672215997474268e-05, 2.5672215997474268e-05, 2.5672215997474268e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5672215997474268e-05

Optimization complete. Final v2v error: 4.245303630828857 mm

Highest mean error: 5.088567733764648 mm for frame 132

Lowest mean error: 3.368165969848633 mm for frame 26

Saving results

Total time: 44.62293863296509
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00803070
Iteration 2/25 | Loss: 0.00156708
Iteration 3/25 | Loss: 0.00122644
Iteration 4/25 | Loss: 0.00119458
Iteration 5/25 | Loss: 0.00119093
Iteration 6/25 | Loss: 0.00118989
Iteration 7/25 | Loss: 0.00118943
Iteration 8/25 | Loss: 0.00118920
Iteration 9/25 | Loss: 0.00118904
Iteration 10/25 | Loss: 0.00118883
Iteration 11/25 | Loss: 0.00119116
Iteration 12/25 | Loss: 0.00119294
Iteration 13/25 | Loss: 0.00119302
Iteration 14/25 | Loss: 0.00119177
Iteration 15/25 | Loss: 0.00119139
Iteration 16/25 | Loss: 0.00119246
Iteration 17/25 | Loss: 0.00118913
Iteration 18/25 | Loss: 0.00119113
Iteration 19/25 | Loss: 0.00119199
Iteration 20/25 | Loss: 0.00118879
Iteration 21/25 | Loss: 0.00119127
Iteration 22/25 | Loss: 0.00119078
Iteration 23/25 | Loss: 0.00119214
Iteration 24/25 | Loss: 0.00119073
Iteration 25/25 | Loss: 0.00119109

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29587364
Iteration 2/25 | Loss: 0.00135802
Iteration 3/25 | Loss: 0.00135802
Iteration 4/25 | Loss: 0.00135802
Iteration 5/25 | Loss: 0.00135802
Iteration 6/25 | Loss: 0.00135802
Iteration 7/25 | Loss: 0.00135802
Iteration 8/25 | Loss: 0.00135802
Iteration 9/25 | Loss: 0.00135802
Iteration 10/25 | Loss: 0.00135802
Iteration 11/25 | Loss: 0.00135802
Iteration 12/25 | Loss: 0.00135802
Iteration 13/25 | Loss: 0.00135802
Iteration 14/25 | Loss: 0.00135802
Iteration 15/25 | Loss: 0.00135802
Iteration 16/25 | Loss: 0.00135802
Iteration 17/25 | Loss: 0.00135802
Iteration 18/25 | Loss: 0.00135802
Iteration 19/25 | Loss: 0.00135802
Iteration 20/25 | Loss: 0.00135802
Iteration 21/25 | Loss: 0.00135802
Iteration 22/25 | Loss: 0.00135802
Iteration 23/25 | Loss: 0.00135802
Iteration 24/25 | Loss: 0.00135802
Iteration 25/25 | Loss: 0.00135802

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00135802
Iteration 2/1000 | Loss: 0.00004880
Iteration 3/1000 | Loss: 0.00006134
Iteration 4/1000 | Loss: 0.00003501
Iteration 5/1000 | Loss: 0.00005123
Iteration 6/1000 | Loss: 0.00004287
Iteration 7/1000 | Loss: 0.00004789
Iteration 8/1000 | Loss: 0.00005561
Iteration 9/1000 | Loss: 0.00006386
Iteration 10/1000 | Loss: 0.00004470
Iteration 11/1000 | Loss: 0.00004363
Iteration 12/1000 | Loss: 0.00004815
Iteration 13/1000 | Loss: 0.00007067
Iteration 14/1000 | Loss: 0.00005157
Iteration 15/1000 | Loss: 0.00004654
Iteration 16/1000 | Loss: 0.00002665
Iteration 17/1000 | Loss: 0.00004908
Iteration 18/1000 | Loss: 0.00002489
Iteration 19/1000 | Loss: 0.00003883
Iteration 20/1000 | Loss: 0.00005338
Iteration 21/1000 | Loss: 0.00006204
Iteration 22/1000 | Loss: 0.00002602
Iteration 23/1000 | Loss: 0.00006911
Iteration 24/1000 | Loss: 0.00004299
Iteration 25/1000 | Loss: 0.00006772
Iteration 26/1000 | Loss: 0.00004712
Iteration 27/1000 | Loss: 0.00003695
Iteration 28/1000 | Loss: 0.00001746
Iteration 29/1000 | Loss: 0.00003818
Iteration 30/1000 | Loss: 0.00002855
Iteration 31/1000 | Loss: 0.00002529
Iteration 32/1000 | Loss: 0.00003282
Iteration 33/1000 | Loss: 0.00002878
Iteration 34/1000 | Loss: 0.00002754
Iteration 35/1000 | Loss: 0.00003109
Iteration 36/1000 | Loss: 0.00003194
Iteration 37/1000 | Loss: 0.00004814
Iteration 38/1000 | Loss: 0.00005344
Iteration 39/1000 | Loss: 0.00004366
Iteration 40/1000 | Loss: 0.00001711
Iteration 41/1000 | Loss: 0.00002720
Iteration 42/1000 | Loss: 0.00002875
Iteration 43/1000 | Loss: 0.00001507
Iteration 44/1000 | Loss: 0.00002008
Iteration 45/1000 | Loss: 0.00003270
Iteration 46/1000 | Loss: 0.00003430
Iteration 47/1000 | Loss: 0.00003037
Iteration 48/1000 | Loss: 0.00002331
Iteration 49/1000 | Loss: 0.00001962
Iteration 50/1000 | Loss: 0.00003211
Iteration 51/1000 | Loss: 0.00001367
Iteration 52/1000 | Loss: 0.00001182
Iteration 53/1000 | Loss: 0.00001133
Iteration 54/1000 | Loss: 0.00001119
Iteration 55/1000 | Loss: 0.00001115
Iteration 56/1000 | Loss: 0.00001083
Iteration 57/1000 | Loss: 0.00001064
Iteration 58/1000 | Loss: 0.00001050
Iteration 59/1000 | Loss: 0.00001028
Iteration 60/1000 | Loss: 0.00001024
Iteration 61/1000 | Loss: 0.00001021
Iteration 62/1000 | Loss: 0.00001014
Iteration 63/1000 | Loss: 0.00001011
Iteration 64/1000 | Loss: 0.00001010
Iteration 65/1000 | Loss: 0.00001010
Iteration 66/1000 | Loss: 0.00001009
Iteration 67/1000 | Loss: 0.00001006
Iteration 68/1000 | Loss: 0.00001005
Iteration 69/1000 | Loss: 0.00001004
Iteration 70/1000 | Loss: 0.00001004
Iteration 71/1000 | Loss: 0.00001004
Iteration 72/1000 | Loss: 0.00001003
Iteration 73/1000 | Loss: 0.00001002
Iteration 74/1000 | Loss: 0.00001000
Iteration 75/1000 | Loss: 0.00000999
Iteration 76/1000 | Loss: 0.00000999
Iteration 77/1000 | Loss: 0.00000998
Iteration 78/1000 | Loss: 0.00000998
Iteration 79/1000 | Loss: 0.00000997
Iteration 80/1000 | Loss: 0.00000996
Iteration 81/1000 | Loss: 0.00000996
Iteration 82/1000 | Loss: 0.00000996
Iteration 83/1000 | Loss: 0.00000996
Iteration 84/1000 | Loss: 0.00000995
Iteration 85/1000 | Loss: 0.00000995
Iteration 86/1000 | Loss: 0.00000994
Iteration 87/1000 | Loss: 0.00000992
Iteration 88/1000 | Loss: 0.00000992
Iteration 89/1000 | Loss: 0.00000991
Iteration 90/1000 | Loss: 0.00000991
Iteration 91/1000 | Loss: 0.00000991
Iteration 92/1000 | Loss: 0.00000991
Iteration 93/1000 | Loss: 0.00000991
Iteration 94/1000 | Loss: 0.00000990
Iteration 95/1000 | Loss: 0.00000990
Iteration 96/1000 | Loss: 0.00000990
Iteration 97/1000 | Loss: 0.00000990
Iteration 98/1000 | Loss: 0.00000990
Iteration 99/1000 | Loss: 0.00000989
Iteration 100/1000 | Loss: 0.00000989
Iteration 101/1000 | Loss: 0.00000989
Iteration 102/1000 | Loss: 0.00000989
Iteration 103/1000 | Loss: 0.00000989
Iteration 104/1000 | Loss: 0.00000989
Iteration 105/1000 | Loss: 0.00000989
Iteration 106/1000 | Loss: 0.00000989
Iteration 107/1000 | Loss: 0.00000988
Iteration 108/1000 | Loss: 0.00000988
Iteration 109/1000 | Loss: 0.00000988
Iteration 110/1000 | Loss: 0.00000988
Iteration 111/1000 | Loss: 0.00000988
Iteration 112/1000 | Loss: 0.00000987
Iteration 113/1000 | Loss: 0.00000987
Iteration 114/1000 | Loss: 0.00000987
Iteration 115/1000 | Loss: 0.00000986
Iteration 116/1000 | Loss: 0.00000986
Iteration 117/1000 | Loss: 0.00000986
Iteration 118/1000 | Loss: 0.00000985
Iteration 119/1000 | Loss: 0.00000985
Iteration 120/1000 | Loss: 0.00000985
Iteration 121/1000 | Loss: 0.00000985
Iteration 122/1000 | Loss: 0.00000985
Iteration 123/1000 | Loss: 0.00000984
Iteration 124/1000 | Loss: 0.00000984
Iteration 125/1000 | Loss: 0.00000984
Iteration 126/1000 | Loss: 0.00000983
Iteration 127/1000 | Loss: 0.00000981
Iteration 128/1000 | Loss: 0.00000981
Iteration 129/1000 | Loss: 0.00000981
Iteration 130/1000 | Loss: 0.00000981
Iteration 131/1000 | Loss: 0.00000981
Iteration 132/1000 | Loss: 0.00000981
Iteration 133/1000 | Loss: 0.00000981
Iteration 134/1000 | Loss: 0.00000979
Iteration 135/1000 | Loss: 0.00000979
Iteration 136/1000 | Loss: 0.00000979
Iteration 137/1000 | Loss: 0.00000978
Iteration 138/1000 | Loss: 0.00000978
Iteration 139/1000 | Loss: 0.00000977
Iteration 140/1000 | Loss: 0.00000977
Iteration 141/1000 | Loss: 0.00000977
Iteration 142/1000 | Loss: 0.00000977
Iteration 143/1000 | Loss: 0.00000977
Iteration 144/1000 | Loss: 0.00000976
Iteration 145/1000 | Loss: 0.00000976
Iteration 146/1000 | Loss: 0.00000976
Iteration 147/1000 | Loss: 0.00000976
Iteration 148/1000 | Loss: 0.00000976
Iteration 149/1000 | Loss: 0.00000976
Iteration 150/1000 | Loss: 0.00000976
Iteration 151/1000 | Loss: 0.00000976
Iteration 152/1000 | Loss: 0.00000976
Iteration 153/1000 | Loss: 0.00000976
Iteration 154/1000 | Loss: 0.00000976
Iteration 155/1000 | Loss: 0.00000976
Iteration 156/1000 | Loss: 0.00000975
Iteration 157/1000 | Loss: 0.00000974
Iteration 158/1000 | Loss: 0.00000974
Iteration 159/1000 | Loss: 0.00000974
Iteration 160/1000 | Loss: 0.00000973
Iteration 161/1000 | Loss: 0.00000973
Iteration 162/1000 | Loss: 0.00000973
Iteration 163/1000 | Loss: 0.00000972
Iteration 164/1000 | Loss: 0.00000971
Iteration 165/1000 | Loss: 0.00000971
Iteration 166/1000 | Loss: 0.00000971
Iteration 167/1000 | Loss: 0.00000970
Iteration 168/1000 | Loss: 0.00000970
Iteration 169/1000 | Loss: 0.00000970
Iteration 170/1000 | Loss: 0.00000969
Iteration 171/1000 | Loss: 0.00000969
Iteration 172/1000 | Loss: 0.00000969
Iteration 173/1000 | Loss: 0.00000969
Iteration 174/1000 | Loss: 0.00000969
Iteration 175/1000 | Loss: 0.00000969
Iteration 176/1000 | Loss: 0.00000969
Iteration 177/1000 | Loss: 0.00000969
Iteration 178/1000 | Loss: 0.00000969
Iteration 179/1000 | Loss: 0.00000969
Iteration 180/1000 | Loss: 0.00000968
Iteration 181/1000 | Loss: 0.00000968
Iteration 182/1000 | Loss: 0.00000968
Iteration 183/1000 | Loss: 0.00000968
Iteration 184/1000 | Loss: 0.00000968
Iteration 185/1000 | Loss: 0.00000967
Iteration 186/1000 | Loss: 0.00000967
Iteration 187/1000 | Loss: 0.00000967
Iteration 188/1000 | Loss: 0.00000967
Iteration 189/1000 | Loss: 0.00000967
Iteration 190/1000 | Loss: 0.00000967
Iteration 191/1000 | Loss: 0.00000967
Iteration 192/1000 | Loss: 0.00000967
Iteration 193/1000 | Loss: 0.00000966
Iteration 194/1000 | Loss: 0.00000966
Iteration 195/1000 | Loss: 0.00000966
Iteration 196/1000 | Loss: 0.00000966
Iteration 197/1000 | Loss: 0.00000966
Iteration 198/1000 | Loss: 0.00000966
Iteration 199/1000 | Loss: 0.00000966
Iteration 200/1000 | Loss: 0.00000966
Iteration 201/1000 | Loss: 0.00000966
Iteration 202/1000 | Loss: 0.00000966
Iteration 203/1000 | Loss: 0.00000966
Iteration 204/1000 | Loss: 0.00000965
Iteration 205/1000 | Loss: 0.00000965
Iteration 206/1000 | Loss: 0.00000965
Iteration 207/1000 | Loss: 0.00000965
Iteration 208/1000 | Loss: 0.00000965
Iteration 209/1000 | Loss: 0.00000965
Iteration 210/1000 | Loss: 0.00000965
Iteration 211/1000 | Loss: 0.00000964
Iteration 212/1000 | Loss: 0.00000964
Iteration 213/1000 | Loss: 0.00000964
Iteration 214/1000 | Loss: 0.00000964
Iteration 215/1000 | Loss: 0.00000964
Iteration 216/1000 | Loss: 0.00000964
Iteration 217/1000 | Loss: 0.00000964
Iteration 218/1000 | Loss: 0.00000963
Iteration 219/1000 | Loss: 0.00000963
Iteration 220/1000 | Loss: 0.00000963
Iteration 221/1000 | Loss: 0.00000963
Iteration 222/1000 | Loss: 0.00000963
Iteration 223/1000 | Loss: 0.00000963
Iteration 224/1000 | Loss: 0.00000962
Iteration 225/1000 | Loss: 0.00000962
Iteration 226/1000 | Loss: 0.00000962
Iteration 227/1000 | Loss: 0.00000962
Iteration 228/1000 | Loss: 0.00000962
Iteration 229/1000 | Loss: 0.00000962
Iteration 230/1000 | Loss: 0.00000962
Iteration 231/1000 | Loss: 0.00000962
Iteration 232/1000 | Loss: 0.00000962
Iteration 233/1000 | Loss: 0.00000961
Iteration 234/1000 | Loss: 0.00000961
Iteration 235/1000 | Loss: 0.00000961
Iteration 236/1000 | Loss: 0.00000961
Iteration 237/1000 | Loss: 0.00000961
Iteration 238/1000 | Loss: 0.00000961
Iteration 239/1000 | Loss: 0.00000961
Iteration 240/1000 | Loss: 0.00000961
Iteration 241/1000 | Loss: 0.00000961
Iteration 242/1000 | Loss: 0.00000960
Iteration 243/1000 | Loss: 0.00000960
Iteration 244/1000 | Loss: 0.00000960
Iteration 245/1000 | Loss: 0.00000960
Iteration 246/1000 | Loss: 0.00000960
Iteration 247/1000 | Loss: 0.00000960
Iteration 248/1000 | Loss: 0.00000960
Iteration 249/1000 | Loss: 0.00000960
Iteration 250/1000 | Loss: 0.00000960
Iteration 251/1000 | Loss: 0.00000960
Iteration 252/1000 | Loss: 0.00000960
Iteration 253/1000 | Loss: 0.00000960
Iteration 254/1000 | Loss: 0.00000960
Iteration 255/1000 | Loss: 0.00000959
Iteration 256/1000 | Loss: 0.00000959
Iteration 257/1000 | Loss: 0.00000959
Iteration 258/1000 | Loss: 0.00000958
Iteration 259/1000 | Loss: 0.00000958
Iteration 260/1000 | Loss: 0.00000958
Iteration 261/1000 | Loss: 0.00000958
Iteration 262/1000 | Loss: 0.00000958
Iteration 263/1000 | Loss: 0.00000958
Iteration 264/1000 | Loss: 0.00000958
Iteration 265/1000 | Loss: 0.00000958
Iteration 266/1000 | Loss: 0.00000958
Iteration 267/1000 | Loss: 0.00000958
Iteration 268/1000 | Loss: 0.00000957
Iteration 269/1000 | Loss: 0.00000957
Iteration 270/1000 | Loss: 0.00000957
Iteration 271/1000 | Loss: 0.00000957
Iteration 272/1000 | Loss: 0.00000957
Iteration 273/1000 | Loss: 0.00000957
Iteration 274/1000 | Loss: 0.00000957
Iteration 275/1000 | Loss: 0.00000957
Iteration 276/1000 | Loss: 0.00000957
Iteration 277/1000 | Loss: 0.00000957
Iteration 278/1000 | Loss: 0.00000957
Iteration 279/1000 | Loss: 0.00000957
Iteration 280/1000 | Loss: 0.00000957
Iteration 281/1000 | Loss: 0.00000957
Iteration 282/1000 | Loss: 0.00000957
Iteration 283/1000 | Loss: 0.00000957
Iteration 284/1000 | Loss: 0.00000957
Iteration 285/1000 | Loss: 0.00000957
Iteration 286/1000 | Loss: 0.00000957
Iteration 287/1000 | Loss: 0.00000957
Iteration 288/1000 | Loss: 0.00000957
Iteration 289/1000 | Loss: 0.00000957
Iteration 290/1000 | Loss: 0.00000957
Iteration 291/1000 | Loss: 0.00000957
Iteration 292/1000 | Loss: 0.00000957
Iteration 293/1000 | Loss: 0.00000957
Iteration 294/1000 | Loss: 0.00000957
Iteration 295/1000 | Loss: 0.00000957
Iteration 296/1000 | Loss: 0.00000957
Iteration 297/1000 | Loss: 0.00000957
Iteration 298/1000 | Loss: 0.00000957
Iteration 299/1000 | Loss: 0.00000957
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 299. Stopping optimization.
Last 5 losses: [9.574308023729827e-06, 9.574308023729827e-06, 9.574308023729827e-06, 9.574308023729827e-06, 9.574308023729827e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.574308023729827e-06

Optimization complete. Final v2v error: 2.67199969291687 mm

Highest mean error: 3.495713710784912 mm for frame 54

Lowest mean error: 2.5521740913391113 mm for frame 29

Saving results

Total time: 147.28025841712952
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00382708
Iteration 2/25 | Loss: 0.00124981
Iteration 3/25 | Loss: 0.00116935
Iteration 4/25 | Loss: 0.00116520
Iteration 5/25 | Loss: 0.00116520
Iteration 6/25 | Loss: 0.00116520
Iteration 7/25 | Loss: 0.00116520
Iteration 8/25 | Loss: 0.00116520
Iteration 9/25 | Loss: 0.00116520
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.0011651951353996992, 0.0011651951353996992, 0.0011651951353996992, 0.0011651951353996992, 0.0011651951353996992]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011651951353996992

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45483959
Iteration 2/25 | Loss: 0.00117555
Iteration 3/25 | Loss: 0.00117555
Iteration 4/25 | Loss: 0.00117555
Iteration 5/25 | Loss: 0.00117555
Iteration 6/25 | Loss: 0.00117555
Iteration 7/25 | Loss: 0.00117555
Iteration 8/25 | Loss: 0.00117555
Iteration 9/25 | Loss: 0.00117555
Iteration 10/25 | Loss: 0.00117555
Iteration 11/25 | Loss: 0.00117555
Iteration 12/25 | Loss: 0.00117555
Iteration 13/25 | Loss: 0.00117555
Iteration 14/25 | Loss: 0.00117555
Iteration 15/25 | Loss: 0.00117555
Iteration 16/25 | Loss: 0.00117555
Iteration 17/25 | Loss: 0.00117555
Iteration 18/25 | Loss: 0.00117555
Iteration 19/25 | Loss: 0.00117555
Iteration 20/25 | Loss: 0.00117555
Iteration 21/25 | Loss: 0.00117555
Iteration 22/25 | Loss: 0.00117555
Iteration 23/25 | Loss: 0.00117555
Iteration 24/25 | Loss: 0.00117555
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.001175550278276205, 0.001175550278276205, 0.001175550278276205, 0.001175550278276205, 0.001175550278276205]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001175550278276205

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117555
Iteration 2/1000 | Loss: 0.00002512
Iteration 3/1000 | Loss: 0.00001625
Iteration 4/1000 | Loss: 0.00001323
Iteration 5/1000 | Loss: 0.00001209
Iteration 6/1000 | Loss: 0.00001146
Iteration 7/1000 | Loss: 0.00001101
Iteration 8/1000 | Loss: 0.00001080
Iteration 9/1000 | Loss: 0.00001048
Iteration 10/1000 | Loss: 0.00001013
Iteration 11/1000 | Loss: 0.00000994
Iteration 12/1000 | Loss: 0.00000991
Iteration 13/1000 | Loss: 0.00000988
Iteration 14/1000 | Loss: 0.00000977
Iteration 15/1000 | Loss: 0.00000975
Iteration 16/1000 | Loss: 0.00000969
Iteration 17/1000 | Loss: 0.00000962
Iteration 18/1000 | Loss: 0.00000961
Iteration 19/1000 | Loss: 0.00000961
Iteration 20/1000 | Loss: 0.00000961
Iteration 21/1000 | Loss: 0.00000961
Iteration 22/1000 | Loss: 0.00000961
Iteration 23/1000 | Loss: 0.00000960
Iteration 24/1000 | Loss: 0.00000960
Iteration 25/1000 | Loss: 0.00000959
Iteration 26/1000 | Loss: 0.00000959
Iteration 27/1000 | Loss: 0.00000958
Iteration 28/1000 | Loss: 0.00000958
Iteration 29/1000 | Loss: 0.00000957
Iteration 30/1000 | Loss: 0.00000957
Iteration 31/1000 | Loss: 0.00000957
Iteration 32/1000 | Loss: 0.00000957
Iteration 33/1000 | Loss: 0.00000957
Iteration 34/1000 | Loss: 0.00000957
Iteration 35/1000 | Loss: 0.00000956
Iteration 36/1000 | Loss: 0.00000956
Iteration 37/1000 | Loss: 0.00000955
Iteration 38/1000 | Loss: 0.00000953
Iteration 39/1000 | Loss: 0.00000951
Iteration 40/1000 | Loss: 0.00000951
Iteration 41/1000 | Loss: 0.00000950
Iteration 42/1000 | Loss: 0.00000949
Iteration 43/1000 | Loss: 0.00000946
Iteration 44/1000 | Loss: 0.00000946
Iteration 45/1000 | Loss: 0.00000945
Iteration 46/1000 | Loss: 0.00000944
Iteration 47/1000 | Loss: 0.00000944
Iteration 48/1000 | Loss: 0.00000936
Iteration 49/1000 | Loss: 0.00000935
Iteration 50/1000 | Loss: 0.00000934
Iteration 51/1000 | Loss: 0.00000933
Iteration 52/1000 | Loss: 0.00000933
Iteration 53/1000 | Loss: 0.00000932
Iteration 54/1000 | Loss: 0.00000932
Iteration 55/1000 | Loss: 0.00000929
Iteration 56/1000 | Loss: 0.00000928
Iteration 57/1000 | Loss: 0.00000928
Iteration 58/1000 | Loss: 0.00000927
Iteration 59/1000 | Loss: 0.00000927
Iteration 60/1000 | Loss: 0.00000927
Iteration 61/1000 | Loss: 0.00000927
Iteration 62/1000 | Loss: 0.00000926
Iteration 63/1000 | Loss: 0.00000926
Iteration 64/1000 | Loss: 0.00000923
Iteration 65/1000 | Loss: 0.00000923
Iteration 66/1000 | Loss: 0.00000923
Iteration 67/1000 | Loss: 0.00000922
Iteration 68/1000 | Loss: 0.00000922
Iteration 69/1000 | Loss: 0.00000922
Iteration 70/1000 | Loss: 0.00000922
Iteration 71/1000 | Loss: 0.00000921
Iteration 72/1000 | Loss: 0.00000921
Iteration 73/1000 | Loss: 0.00000920
Iteration 74/1000 | Loss: 0.00000920
Iteration 75/1000 | Loss: 0.00000919
Iteration 76/1000 | Loss: 0.00000919
Iteration 77/1000 | Loss: 0.00000919
Iteration 78/1000 | Loss: 0.00000919
Iteration 79/1000 | Loss: 0.00000919
Iteration 80/1000 | Loss: 0.00000919
Iteration 81/1000 | Loss: 0.00000919
Iteration 82/1000 | Loss: 0.00000919
Iteration 83/1000 | Loss: 0.00000919
Iteration 84/1000 | Loss: 0.00000919
Iteration 85/1000 | Loss: 0.00000919
Iteration 86/1000 | Loss: 0.00000919
Iteration 87/1000 | Loss: 0.00000918
Iteration 88/1000 | Loss: 0.00000918
Iteration 89/1000 | Loss: 0.00000918
Iteration 90/1000 | Loss: 0.00000918
Iteration 91/1000 | Loss: 0.00000917
Iteration 92/1000 | Loss: 0.00000917
Iteration 93/1000 | Loss: 0.00000916
Iteration 94/1000 | Loss: 0.00000915
Iteration 95/1000 | Loss: 0.00000915
Iteration 96/1000 | Loss: 0.00000915
Iteration 97/1000 | Loss: 0.00000915
Iteration 98/1000 | Loss: 0.00000915
Iteration 99/1000 | Loss: 0.00000914
Iteration 100/1000 | Loss: 0.00000914
Iteration 101/1000 | Loss: 0.00000914
Iteration 102/1000 | Loss: 0.00000914
Iteration 103/1000 | Loss: 0.00000914
Iteration 104/1000 | Loss: 0.00000914
Iteration 105/1000 | Loss: 0.00000913
Iteration 106/1000 | Loss: 0.00000913
Iteration 107/1000 | Loss: 0.00000913
Iteration 108/1000 | Loss: 0.00000912
Iteration 109/1000 | Loss: 0.00000912
Iteration 110/1000 | Loss: 0.00000912
Iteration 111/1000 | Loss: 0.00000912
Iteration 112/1000 | Loss: 0.00000912
Iteration 113/1000 | Loss: 0.00000912
Iteration 114/1000 | Loss: 0.00000912
Iteration 115/1000 | Loss: 0.00000912
Iteration 116/1000 | Loss: 0.00000912
Iteration 117/1000 | Loss: 0.00000912
Iteration 118/1000 | Loss: 0.00000912
Iteration 119/1000 | Loss: 0.00000912
Iteration 120/1000 | Loss: 0.00000911
Iteration 121/1000 | Loss: 0.00000911
Iteration 122/1000 | Loss: 0.00000911
Iteration 123/1000 | Loss: 0.00000911
Iteration 124/1000 | Loss: 0.00000911
Iteration 125/1000 | Loss: 0.00000910
Iteration 126/1000 | Loss: 0.00000909
Iteration 127/1000 | Loss: 0.00000909
Iteration 128/1000 | Loss: 0.00000909
Iteration 129/1000 | Loss: 0.00000909
Iteration 130/1000 | Loss: 0.00000908
Iteration 131/1000 | Loss: 0.00000908
Iteration 132/1000 | Loss: 0.00000908
Iteration 133/1000 | Loss: 0.00000908
Iteration 134/1000 | Loss: 0.00000908
Iteration 135/1000 | Loss: 0.00000907
Iteration 136/1000 | Loss: 0.00000907
Iteration 137/1000 | Loss: 0.00000907
Iteration 138/1000 | Loss: 0.00000906
Iteration 139/1000 | Loss: 0.00000906
Iteration 140/1000 | Loss: 0.00000905
Iteration 141/1000 | Loss: 0.00000905
Iteration 142/1000 | Loss: 0.00000905
Iteration 143/1000 | Loss: 0.00000905
Iteration 144/1000 | Loss: 0.00000905
Iteration 145/1000 | Loss: 0.00000905
Iteration 146/1000 | Loss: 0.00000904
Iteration 147/1000 | Loss: 0.00000904
Iteration 148/1000 | Loss: 0.00000904
Iteration 149/1000 | Loss: 0.00000904
Iteration 150/1000 | Loss: 0.00000903
Iteration 151/1000 | Loss: 0.00000903
Iteration 152/1000 | Loss: 0.00000902
Iteration 153/1000 | Loss: 0.00000902
Iteration 154/1000 | Loss: 0.00000902
Iteration 155/1000 | Loss: 0.00000902
Iteration 156/1000 | Loss: 0.00000902
Iteration 157/1000 | Loss: 0.00000901
Iteration 158/1000 | Loss: 0.00000901
Iteration 159/1000 | Loss: 0.00000901
Iteration 160/1000 | Loss: 0.00000901
Iteration 161/1000 | Loss: 0.00000901
Iteration 162/1000 | Loss: 0.00000901
Iteration 163/1000 | Loss: 0.00000901
Iteration 164/1000 | Loss: 0.00000900
Iteration 165/1000 | Loss: 0.00000900
Iteration 166/1000 | Loss: 0.00000900
Iteration 167/1000 | Loss: 0.00000899
Iteration 168/1000 | Loss: 0.00000899
Iteration 169/1000 | Loss: 0.00000899
Iteration 170/1000 | Loss: 0.00000899
Iteration 171/1000 | Loss: 0.00000899
Iteration 172/1000 | Loss: 0.00000898
Iteration 173/1000 | Loss: 0.00000898
Iteration 174/1000 | Loss: 0.00000898
Iteration 175/1000 | Loss: 0.00000898
Iteration 176/1000 | Loss: 0.00000898
Iteration 177/1000 | Loss: 0.00000898
Iteration 178/1000 | Loss: 0.00000898
Iteration 179/1000 | Loss: 0.00000898
Iteration 180/1000 | Loss: 0.00000898
Iteration 181/1000 | Loss: 0.00000897
Iteration 182/1000 | Loss: 0.00000897
Iteration 183/1000 | Loss: 0.00000897
Iteration 184/1000 | Loss: 0.00000897
Iteration 185/1000 | Loss: 0.00000897
Iteration 186/1000 | Loss: 0.00000897
Iteration 187/1000 | Loss: 0.00000897
Iteration 188/1000 | Loss: 0.00000897
Iteration 189/1000 | Loss: 0.00000897
Iteration 190/1000 | Loss: 0.00000897
Iteration 191/1000 | Loss: 0.00000897
Iteration 192/1000 | Loss: 0.00000897
Iteration 193/1000 | Loss: 0.00000897
Iteration 194/1000 | Loss: 0.00000897
Iteration 195/1000 | Loss: 0.00000897
Iteration 196/1000 | Loss: 0.00000897
Iteration 197/1000 | Loss: 0.00000897
Iteration 198/1000 | Loss: 0.00000897
Iteration 199/1000 | Loss: 0.00000897
Iteration 200/1000 | Loss: 0.00000897
Iteration 201/1000 | Loss: 0.00000897
Iteration 202/1000 | Loss: 0.00000897
Iteration 203/1000 | Loss: 0.00000897
Iteration 204/1000 | Loss: 0.00000897
Iteration 205/1000 | Loss: 0.00000897
Iteration 206/1000 | Loss: 0.00000897
Iteration 207/1000 | Loss: 0.00000897
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [8.96674464456737e-06, 8.96674464456737e-06, 8.96674464456737e-06, 8.96674464456737e-06, 8.96674464456737e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.96674464456737e-06

Optimization complete. Final v2v error: 2.56575083732605 mm

Highest mean error: 2.7098333835601807 mm for frame 142

Lowest mean error: 2.4347305297851562 mm for frame 68

Saving results

Total time: 46.49180030822754
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01026163
Iteration 2/25 | Loss: 0.01026163
Iteration 3/25 | Loss: 0.01026163
Iteration 4/25 | Loss: 0.00244285
Iteration 5/25 | Loss: 0.00230799
Iteration 6/25 | Loss: 0.00168675
Iteration 7/25 | Loss: 0.00161565
Iteration 8/25 | Loss: 0.00149064
Iteration 9/25 | Loss: 0.00141583
Iteration 10/25 | Loss: 0.00136392
Iteration 11/25 | Loss: 0.00134596
Iteration 12/25 | Loss: 0.00132983
Iteration 13/25 | Loss: 0.00134030
Iteration 14/25 | Loss: 0.00131432
Iteration 15/25 | Loss: 0.00130386
Iteration 16/25 | Loss: 0.00130021
Iteration 17/25 | Loss: 0.00129828
Iteration 18/25 | Loss: 0.00130102
Iteration 19/25 | Loss: 0.00129769
Iteration 20/25 | Loss: 0.00129545
Iteration 21/25 | Loss: 0.00129931
Iteration 22/25 | Loss: 0.00129827
Iteration 23/25 | Loss: 0.00129384
Iteration 24/25 | Loss: 0.00129128
Iteration 25/25 | Loss: 0.00129114

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28944576
Iteration 2/25 | Loss: 0.00178638
Iteration 3/25 | Loss: 0.00148205
Iteration 4/25 | Loss: 0.00148205
Iteration 5/25 | Loss: 0.00148205
Iteration 6/25 | Loss: 0.00148205
Iteration 7/25 | Loss: 0.00148205
Iteration 8/25 | Loss: 0.00148205
Iteration 9/25 | Loss: 0.00148205
Iteration 10/25 | Loss: 0.00148205
Iteration 11/25 | Loss: 0.00148205
Iteration 12/25 | Loss: 0.00148205
Iteration 13/25 | Loss: 0.00148205
Iteration 14/25 | Loss: 0.00148205
Iteration 15/25 | Loss: 0.00148205
Iteration 16/25 | Loss: 0.00148205
Iteration 17/25 | Loss: 0.00148205
Iteration 18/25 | Loss: 0.00148205
Iteration 19/25 | Loss: 0.00148205
Iteration 20/25 | Loss: 0.00148205
Iteration 21/25 | Loss: 0.00148205
Iteration 22/25 | Loss: 0.00148205
Iteration 23/25 | Loss: 0.00148205
Iteration 24/25 | Loss: 0.00148205
Iteration 25/25 | Loss: 0.00148205

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00148205
Iteration 2/1000 | Loss: 0.00084765
Iteration 3/1000 | Loss: 0.00013449
Iteration 4/1000 | Loss: 0.00041055
Iteration 5/1000 | Loss: 0.00012043
Iteration 6/1000 | Loss: 0.00009371
Iteration 7/1000 | Loss: 0.00018055
Iteration 8/1000 | Loss: 0.00005305
Iteration 9/1000 | Loss: 0.00005269
Iteration 10/1000 | Loss: 0.00013286
Iteration 11/1000 | Loss: 0.00005078
Iteration 12/1000 | Loss: 0.00005125
Iteration 13/1000 | Loss: 0.00012680
Iteration 14/1000 | Loss: 0.00008849
Iteration 15/1000 | Loss: 0.00005088
Iteration 16/1000 | Loss: 0.00005150
Iteration 17/1000 | Loss: 0.00009842
Iteration 18/1000 | Loss: 0.00004082
Iteration 19/1000 | Loss: 0.00004982
Iteration 20/1000 | Loss: 0.00004355
Iteration 21/1000 | Loss: 0.00003817
Iteration 22/1000 | Loss: 0.00004517
Iteration 23/1000 | Loss: 0.00010196
Iteration 24/1000 | Loss: 0.00003716
Iteration 25/1000 | Loss: 0.00004942
Iteration 26/1000 | Loss: 0.00005231
Iteration 27/1000 | Loss: 0.00005071
Iteration 28/1000 | Loss: 0.00005093
Iteration 29/1000 | Loss: 0.00005024
Iteration 30/1000 | Loss: 0.00008834
Iteration 31/1000 | Loss: 0.00004024
Iteration 32/1000 | Loss: 0.00008022
Iteration 33/1000 | Loss: 0.00003795
Iteration 34/1000 | Loss: 0.00003664
Iteration 35/1000 | Loss: 0.00003614
Iteration 36/1000 | Loss: 0.00011888
Iteration 37/1000 | Loss: 0.00003544
Iteration 38/1000 | Loss: 0.00003479
Iteration 39/1000 | Loss: 0.00003437
Iteration 40/1000 | Loss: 0.00003394
Iteration 41/1000 | Loss: 0.00003363
Iteration 42/1000 | Loss: 0.00003339
Iteration 43/1000 | Loss: 0.00003323
Iteration 44/1000 | Loss: 0.00003309
Iteration 45/1000 | Loss: 0.00003308
Iteration 46/1000 | Loss: 0.00003298
Iteration 47/1000 | Loss: 0.00003298
Iteration 48/1000 | Loss: 0.00003296
Iteration 49/1000 | Loss: 0.00003295
Iteration 50/1000 | Loss: 0.00003294
Iteration 51/1000 | Loss: 0.00003294
Iteration 52/1000 | Loss: 0.00003293
Iteration 53/1000 | Loss: 0.00003293
Iteration 54/1000 | Loss: 0.00003293
Iteration 55/1000 | Loss: 0.00003292
Iteration 56/1000 | Loss: 0.00003292
Iteration 57/1000 | Loss: 0.00003291
Iteration 58/1000 | Loss: 0.00003291
Iteration 59/1000 | Loss: 0.00003290
Iteration 60/1000 | Loss: 0.00003290
Iteration 61/1000 | Loss: 0.00003290
Iteration 62/1000 | Loss: 0.00003289
Iteration 63/1000 | Loss: 0.00003289
Iteration 64/1000 | Loss: 0.00003289
Iteration 65/1000 | Loss: 0.00003288
Iteration 66/1000 | Loss: 0.00003288
Iteration 67/1000 | Loss: 0.00003288
Iteration 68/1000 | Loss: 0.00003288
Iteration 69/1000 | Loss: 0.00003287
Iteration 70/1000 | Loss: 0.00003287
Iteration 71/1000 | Loss: 0.00003287
Iteration 72/1000 | Loss: 0.00003287
Iteration 73/1000 | Loss: 0.00003287
Iteration 74/1000 | Loss: 0.00003287
Iteration 75/1000 | Loss: 0.00003286
Iteration 76/1000 | Loss: 0.00003286
Iteration 77/1000 | Loss: 0.00003286
Iteration 78/1000 | Loss: 0.00003286
Iteration 79/1000 | Loss: 0.00003286
Iteration 80/1000 | Loss: 0.00003286
Iteration 81/1000 | Loss: 0.00003286
Iteration 82/1000 | Loss: 0.00003285
Iteration 83/1000 | Loss: 0.00003285
Iteration 84/1000 | Loss: 0.00003285
Iteration 85/1000 | Loss: 0.00003284
Iteration 86/1000 | Loss: 0.00003284
Iteration 87/1000 | Loss: 0.00003284
Iteration 88/1000 | Loss: 0.00003283
Iteration 89/1000 | Loss: 0.00003283
Iteration 90/1000 | Loss: 0.00003283
Iteration 91/1000 | Loss: 0.00003283
Iteration 92/1000 | Loss: 0.00003283
Iteration 93/1000 | Loss: 0.00003282
Iteration 94/1000 | Loss: 0.00003282
Iteration 95/1000 | Loss: 0.00003282
Iteration 96/1000 | Loss: 0.00003282
Iteration 97/1000 | Loss: 0.00003281
Iteration 98/1000 | Loss: 0.00003281
Iteration 99/1000 | Loss: 0.00003281
Iteration 100/1000 | Loss: 0.00003281
Iteration 101/1000 | Loss: 0.00003281
Iteration 102/1000 | Loss: 0.00003281
Iteration 103/1000 | Loss: 0.00003281
Iteration 104/1000 | Loss: 0.00003280
Iteration 105/1000 | Loss: 0.00003280
Iteration 106/1000 | Loss: 0.00003280
Iteration 107/1000 | Loss: 0.00003280
Iteration 108/1000 | Loss: 0.00003280
Iteration 109/1000 | Loss: 0.00003280
Iteration 110/1000 | Loss: 0.00003279
Iteration 111/1000 | Loss: 0.00003279
Iteration 112/1000 | Loss: 0.00003279
Iteration 113/1000 | Loss: 0.00003279
Iteration 114/1000 | Loss: 0.00003278
Iteration 115/1000 | Loss: 0.00003278
Iteration 116/1000 | Loss: 0.00003278
Iteration 117/1000 | Loss: 0.00003278
Iteration 118/1000 | Loss: 0.00003278
Iteration 119/1000 | Loss: 0.00003278
Iteration 120/1000 | Loss: 0.00003278
Iteration 121/1000 | Loss: 0.00003278
Iteration 122/1000 | Loss: 0.00003278
Iteration 123/1000 | Loss: 0.00003277
Iteration 124/1000 | Loss: 0.00003277
Iteration 125/1000 | Loss: 0.00003277
Iteration 126/1000 | Loss: 0.00003277
Iteration 127/1000 | Loss: 0.00003276
Iteration 128/1000 | Loss: 0.00003276
Iteration 129/1000 | Loss: 0.00003276
Iteration 130/1000 | Loss: 0.00003276
Iteration 131/1000 | Loss: 0.00003276
Iteration 132/1000 | Loss: 0.00003276
Iteration 133/1000 | Loss: 0.00003276
Iteration 134/1000 | Loss: 0.00003275
Iteration 135/1000 | Loss: 0.00003275
Iteration 136/1000 | Loss: 0.00003275
Iteration 137/1000 | Loss: 0.00003275
Iteration 138/1000 | Loss: 0.00003275
Iteration 139/1000 | Loss: 0.00003275
Iteration 140/1000 | Loss: 0.00003275
Iteration 141/1000 | Loss: 0.00003275
Iteration 142/1000 | Loss: 0.00003275
Iteration 143/1000 | Loss: 0.00003275
Iteration 144/1000 | Loss: 0.00003275
Iteration 145/1000 | Loss: 0.00003275
Iteration 146/1000 | Loss: 0.00003275
Iteration 147/1000 | Loss: 0.00003274
Iteration 148/1000 | Loss: 0.00003274
Iteration 149/1000 | Loss: 0.00003274
Iteration 150/1000 | Loss: 0.00003274
Iteration 151/1000 | Loss: 0.00003274
Iteration 152/1000 | Loss: 0.00003274
Iteration 153/1000 | Loss: 0.00003274
Iteration 154/1000 | Loss: 0.00003274
Iteration 155/1000 | Loss: 0.00003273
Iteration 156/1000 | Loss: 0.00003273
Iteration 157/1000 | Loss: 0.00003273
Iteration 158/1000 | Loss: 0.00003273
Iteration 159/1000 | Loss: 0.00003273
Iteration 160/1000 | Loss: 0.00003273
Iteration 161/1000 | Loss: 0.00003273
Iteration 162/1000 | Loss: 0.00003273
Iteration 163/1000 | Loss: 0.00003272
Iteration 164/1000 | Loss: 0.00003272
Iteration 165/1000 | Loss: 0.00003272
Iteration 166/1000 | Loss: 0.00003272
Iteration 167/1000 | Loss: 0.00003272
Iteration 168/1000 | Loss: 0.00003272
Iteration 169/1000 | Loss: 0.00003272
Iteration 170/1000 | Loss: 0.00003272
Iteration 171/1000 | Loss: 0.00003272
Iteration 172/1000 | Loss: 0.00003272
Iteration 173/1000 | Loss: 0.00003272
Iteration 174/1000 | Loss: 0.00003272
Iteration 175/1000 | Loss: 0.00003272
Iteration 176/1000 | Loss: 0.00003272
Iteration 177/1000 | Loss: 0.00003272
Iteration 178/1000 | Loss: 0.00003272
Iteration 179/1000 | Loss: 0.00003272
Iteration 180/1000 | Loss: 0.00003272
Iteration 181/1000 | Loss: 0.00003272
Iteration 182/1000 | Loss: 0.00003271
Iteration 183/1000 | Loss: 0.00003271
Iteration 184/1000 | Loss: 0.00003271
Iteration 185/1000 | Loss: 0.00003271
Iteration 186/1000 | Loss: 0.00003271
Iteration 187/1000 | Loss: 0.00003271
Iteration 188/1000 | Loss: 0.00003271
Iteration 189/1000 | Loss: 0.00003271
Iteration 190/1000 | Loss: 0.00003271
Iteration 191/1000 | Loss: 0.00003271
Iteration 192/1000 | Loss: 0.00003271
Iteration 193/1000 | Loss: 0.00003271
Iteration 194/1000 | Loss: 0.00003271
Iteration 195/1000 | Loss: 0.00003271
Iteration 196/1000 | Loss: 0.00003271
Iteration 197/1000 | Loss: 0.00003271
Iteration 198/1000 | Loss: 0.00003270
Iteration 199/1000 | Loss: 0.00003270
Iteration 200/1000 | Loss: 0.00003270
Iteration 201/1000 | Loss: 0.00003270
Iteration 202/1000 | Loss: 0.00003270
Iteration 203/1000 | Loss: 0.00003270
Iteration 204/1000 | Loss: 0.00003270
Iteration 205/1000 | Loss: 0.00003270
Iteration 206/1000 | Loss: 0.00003270
Iteration 207/1000 | Loss: 0.00003270
Iteration 208/1000 | Loss: 0.00003270
Iteration 209/1000 | Loss: 0.00003270
Iteration 210/1000 | Loss: 0.00003270
Iteration 211/1000 | Loss: 0.00003270
Iteration 212/1000 | Loss: 0.00003270
Iteration 213/1000 | Loss: 0.00003270
Iteration 214/1000 | Loss: 0.00003270
Iteration 215/1000 | Loss: 0.00003270
Iteration 216/1000 | Loss: 0.00003270
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [3.27046072925441e-05, 3.27046072925441e-05, 3.27046072925441e-05, 3.27046072925441e-05, 3.27046072925441e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.27046072925441e-05

Optimization complete. Final v2v error: 3.8634660243988037 mm

Highest mean error: 11.80547046661377 mm for frame 214

Lowest mean error: 2.8852484226226807 mm for frame 153

Saving results

Total time: 130.61320281028748
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00805046
Iteration 2/25 | Loss: 0.00124356
Iteration 3/25 | Loss: 0.00117947
Iteration 4/25 | Loss: 0.00117145
Iteration 5/25 | Loss: 0.00116945
Iteration 6/25 | Loss: 0.00116945
Iteration 7/25 | Loss: 0.00116945
Iteration 8/25 | Loss: 0.00116945
Iteration 9/25 | Loss: 0.00116945
Iteration 10/25 | Loss: 0.00116945
Iteration 11/25 | Loss: 0.00116945
Iteration 12/25 | Loss: 0.00116945
Iteration 13/25 | Loss: 0.00116945
Iteration 14/25 | Loss: 0.00116945
Iteration 15/25 | Loss: 0.00116945
Iteration 16/25 | Loss: 0.00116945
Iteration 17/25 | Loss: 0.00116945
Iteration 18/25 | Loss: 0.00116945
Iteration 19/25 | Loss: 0.00116945
Iteration 20/25 | Loss: 0.00116945
Iteration 21/25 | Loss: 0.00116945
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0011694483691826463, 0.0011694483691826463, 0.0011694483691826463, 0.0011694483691826463, 0.0011694483691826463]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011694483691826463

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.06652117
Iteration 2/25 | Loss: 0.00116629
Iteration 3/25 | Loss: 0.00116627
Iteration 4/25 | Loss: 0.00116627
Iteration 5/25 | Loss: 0.00116627
Iteration 6/25 | Loss: 0.00116627
Iteration 7/25 | Loss: 0.00116627
Iteration 8/25 | Loss: 0.00116627
Iteration 9/25 | Loss: 0.00116627
Iteration 10/25 | Loss: 0.00116627
Iteration 11/25 | Loss: 0.00116627
Iteration 12/25 | Loss: 0.00116627
Iteration 13/25 | Loss: 0.00116627
Iteration 14/25 | Loss: 0.00116627
Iteration 15/25 | Loss: 0.00116627
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001166266854852438, 0.001166266854852438, 0.001166266854852438, 0.001166266854852438, 0.001166266854852438]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001166266854852438

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116627
Iteration 2/1000 | Loss: 0.00002353
Iteration 3/1000 | Loss: 0.00001600
Iteration 4/1000 | Loss: 0.00001455
Iteration 5/1000 | Loss: 0.00001379
Iteration 6/1000 | Loss: 0.00001341
Iteration 7/1000 | Loss: 0.00001292
Iteration 8/1000 | Loss: 0.00001263
Iteration 9/1000 | Loss: 0.00001234
Iteration 10/1000 | Loss: 0.00001206
Iteration 11/1000 | Loss: 0.00001188
Iteration 12/1000 | Loss: 0.00001179
Iteration 13/1000 | Loss: 0.00001172
Iteration 14/1000 | Loss: 0.00001169
Iteration 15/1000 | Loss: 0.00001169
Iteration 16/1000 | Loss: 0.00001164
Iteration 17/1000 | Loss: 0.00001162
Iteration 18/1000 | Loss: 0.00001161
Iteration 19/1000 | Loss: 0.00001160
Iteration 20/1000 | Loss: 0.00001159
Iteration 21/1000 | Loss: 0.00001159
Iteration 22/1000 | Loss: 0.00001159
Iteration 23/1000 | Loss: 0.00001159
Iteration 24/1000 | Loss: 0.00001159
Iteration 25/1000 | Loss: 0.00001158
Iteration 26/1000 | Loss: 0.00001158
Iteration 27/1000 | Loss: 0.00001158
Iteration 28/1000 | Loss: 0.00001158
Iteration 29/1000 | Loss: 0.00001158
Iteration 30/1000 | Loss: 0.00001158
Iteration 31/1000 | Loss: 0.00001158
Iteration 32/1000 | Loss: 0.00001158
Iteration 33/1000 | Loss: 0.00001158
Iteration 34/1000 | Loss: 0.00001158
Iteration 35/1000 | Loss: 0.00001158
Iteration 36/1000 | Loss: 0.00001158
Iteration 37/1000 | Loss: 0.00001158
Iteration 38/1000 | Loss: 0.00001158
Iteration 39/1000 | Loss: 0.00001158
Iteration 40/1000 | Loss: 0.00001158
Iteration 41/1000 | Loss: 0.00001158
Iteration 42/1000 | Loss: 0.00001158
Iteration 43/1000 | Loss: 0.00001158
Iteration 44/1000 | Loss: 0.00001158
Iteration 45/1000 | Loss: 0.00001158
Iteration 46/1000 | Loss: 0.00001158
Iteration 47/1000 | Loss: 0.00001158
Iteration 48/1000 | Loss: 0.00001158
Iteration 49/1000 | Loss: 0.00001154
Iteration 50/1000 | Loss: 0.00001153
Iteration 51/1000 | Loss: 0.00001152
Iteration 52/1000 | Loss: 0.00001152
Iteration 53/1000 | Loss: 0.00001152
Iteration 54/1000 | Loss: 0.00001152
Iteration 55/1000 | Loss: 0.00001151
Iteration 56/1000 | Loss: 0.00001151
Iteration 57/1000 | Loss: 0.00001151
Iteration 58/1000 | Loss: 0.00001150
Iteration 59/1000 | Loss: 0.00001150
Iteration 60/1000 | Loss: 0.00001149
Iteration 61/1000 | Loss: 0.00001147
Iteration 62/1000 | Loss: 0.00001147
Iteration 63/1000 | Loss: 0.00001147
Iteration 64/1000 | Loss: 0.00001146
Iteration 65/1000 | Loss: 0.00001142
Iteration 66/1000 | Loss: 0.00001142
Iteration 67/1000 | Loss: 0.00001142
Iteration 68/1000 | Loss: 0.00001141
Iteration 69/1000 | Loss: 0.00001141
Iteration 70/1000 | Loss: 0.00001141
Iteration 71/1000 | Loss: 0.00001141
Iteration 72/1000 | Loss: 0.00001141
Iteration 73/1000 | Loss: 0.00001141
Iteration 74/1000 | Loss: 0.00001141
Iteration 75/1000 | Loss: 0.00001141
Iteration 76/1000 | Loss: 0.00001140
Iteration 77/1000 | Loss: 0.00001140
Iteration 78/1000 | Loss: 0.00001139
Iteration 79/1000 | Loss: 0.00001138
Iteration 80/1000 | Loss: 0.00001138
Iteration 81/1000 | Loss: 0.00001137
Iteration 82/1000 | Loss: 0.00001137
Iteration 83/1000 | Loss: 0.00001137
Iteration 84/1000 | Loss: 0.00001136
Iteration 85/1000 | Loss: 0.00001136
Iteration 86/1000 | Loss: 0.00001135
Iteration 87/1000 | Loss: 0.00001135
Iteration 88/1000 | Loss: 0.00001135
Iteration 89/1000 | Loss: 0.00001134
Iteration 90/1000 | Loss: 0.00001133
Iteration 91/1000 | Loss: 0.00001133
Iteration 92/1000 | Loss: 0.00001133
Iteration 93/1000 | Loss: 0.00001132
Iteration 94/1000 | Loss: 0.00001132
Iteration 95/1000 | Loss: 0.00001131
Iteration 96/1000 | Loss: 0.00001130
Iteration 97/1000 | Loss: 0.00001130
Iteration 98/1000 | Loss: 0.00001129
Iteration 99/1000 | Loss: 0.00001129
Iteration 100/1000 | Loss: 0.00001128
Iteration 101/1000 | Loss: 0.00001128
Iteration 102/1000 | Loss: 0.00001128
Iteration 103/1000 | Loss: 0.00001127
Iteration 104/1000 | Loss: 0.00001126
Iteration 105/1000 | Loss: 0.00001125
Iteration 106/1000 | Loss: 0.00001125
Iteration 107/1000 | Loss: 0.00001125
Iteration 108/1000 | Loss: 0.00001125
Iteration 109/1000 | Loss: 0.00001125
Iteration 110/1000 | Loss: 0.00001125
Iteration 111/1000 | Loss: 0.00001125
Iteration 112/1000 | Loss: 0.00001124
Iteration 113/1000 | Loss: 0.00001124
Iteration 114/1000 | Loss: 0.00001124
Iteration 115/1000 | Loss: 0.00001124
Iteration 116/1000 | Loss: 0.00001124
Iteration 117/1000 | Loss: 0.00001124
Iteration 118/1000 | Loss: 0.00001124
Iteration 119/1000 | Loss: 0.00001124
Iteration 120/1000 | Loss: 0.00001123
Iteration 121/1000 | Loss: 0.00001123
Iteration 122/1000 | Loss: 0.00001123
Iteration 123/1000 | Loss: 0.00001123
Iteration 124/1000 | Loss: 0.00001122
Iteration 125/1000 | Loss: 0.00001122
Iteration 126/1000 | Loss: 0.00001121
Iteration 127/1000 | Loss: 0.00001121
Iteration 128/1000 | Loss: 0.00001121
Iteration 129/1000 | Loss: 0.00001121
Iteration 130/1000 | Loss: 0.00001121
Iteration 131/1000 | Loss: 0.00001121
Iteration 132/1000 | Loss: 0.00001120
Iteration 133/1000 | Loss: 0.00001120
Iteration 134/1000 | Loss: 0.00001120
Iteration 135/1000 | Loss: 0.00001120
Iteration 136/1000 | Loss: 0.00001120
Iteration 137/1000 | Loss: 0.00001118
Iteration 138/1000 | Loss: 0.00001118
Iteration 139/1000 | Loss: 0.00001118
Iteration 140/1000 | Loss: 0.00001118
Iteration 141/1000 | Loss: 0.00001118
Iteration 142/1000 | Loss: 0.00001118
Iteration 143/1000 | Loss: 0.00001118
Iteration 144/1000 | Loss: 0.00001118
Iteration 145/1000 | Loss: 0.00001118
Iteration 146/1000 | Loss: 0.00001118
Iteration 147/1000 | Loss: 0.00001118
Iteration 148/1000 | Loss: 0.00001118
Iteration 149/1000 | Loss: 0.00001117
Iteration 150/1000 | Loss: 0.00001117
Iteration 151/1000 | Loss: 0.00001117
Iteration 152/1000 | Loss: 0.00001117
Iteration 153/1000 | Loss: 0.00001117
Iteration 154/1000 | Loss: 0.00001117
Iteration 155/1000 | Loss: 0.00001117
Iteration 156/1000 | Loss: 0.00001117
Iteration 157/1000 | Loss: 0.00001117
Iteration 158/1000 | Loss: 0.00001117
Iteration 159/1000 | Loss: 0.00001117
Iteration 160/1000 | Loss: 0.00001117
Iteration 161/1000 | Loss: 0.00001117
Iteration 162/1000 | Loss: 0.00001117
Iteration 163/1000 | Loss: 0.00001117
Iteration 164/1000 | Loss: 0.00001117
Iteration 165/1000 | Loss: 0.00001116
Iteration 166/1000 | Loss: 0.00001116
Iteration 167/1000 | Loss: 0.00001116
Iteration 168/1000 | Loss: 0.00001116
Iteration 169/1000 | Loss: 0.00001116
Iteration 170/1000 | Loss: 0.00001116
Iteration 171/1000 | Loss: 0.00001116
Iteration 172/1000 | Loss: 0.00001116
Iteration 173/1000 | Loss: 0.00001116
Iteration 174/1000 | Loss: 0.00001116
Iteration 175/1000 | Loss: 0.00001116
Iteration 176/1000 | Loss: 0.00001116
Iteration 177/1000 | Loss: 0.00001116
Iteration 178/1000 | Loss: 0.00001116
Iteration 179/1000 | Loss: 0.00001116
Iteration 180/1000 | Loss: 0.00001115
Iteration 181/1000 | Loss: 0.00001115
Iteration 182/1000 | Loss: 0.00001115
Iteration 183/1000 | Loss: 0.00001115
Iteration 184/1000 | Loss: 0.00001115
Iteration 185/1000 | Loss: 0.00001115
Iteration 186/1000 | Loss: 0.00001115
Iteration 187/1000 | Loss: 0.00001115
Iteration 188/1000 | Loss: 0.00001115
Iteration 189/1000 | Loss: 0.00001115
Iteration 190/1000 | Loss: 0.00001115
Iteration 191/1000 | Loss: 0.00001115
Iteration 192/1000 | Loss: 0.00001115
Iteration 193/1000 | Loss: 0.00001115
Iteration 194/1000 | Loss: 0.00001115
Iteration 195/1000 | Loss: 0.00001115
Iteration 196/1000 | Loss: 0.00001115
Iteration 197/1000 | Loss: 0.00001115
Iteration 198/1000 | Loss: 0.00001115
Iteration 199/1000 | Loss: 0.00001115
Iteration 200/1000 | Loss: 0.00001115
Iteration 201/1000 | Loss: 0.00001115
Iteration 202/1000 | Loss: 0.00001115
Iteration 203/1000 | Loss: 0.00001114
Iteration 204/1000 | Loss: 0.00001114
Iteration 205/1000 | Loss: 0.00001114
Iteration 206/1000 | Loss: 0.00001114
Iteration 207/1000 | Loss: 0.00001114
Iteration 208/1000 | Loss: 0.00001114
Iteration 209/1000 | Loss: 0.00001114
Iteration 210/1000 | Loss: 0.00001114
Iteration 211/1000 | Loss: 0.00001114
Iteration 212/1000 | Loss: 0.00001114
Iteration 213/1000 | Loss: 0.00001114
Iteration 214/1000 | Loss: 0.00001114
Iteration 215/1000 | Loss: 0.00001114
Iteration 216/1000 | Loss: 0.00001114
Iteration 217/1000 | Loss: 0.00001114
Iteration 218/1000 | Loss: 0.00001114
Iteration 219/1000 | Loss: 0.00001114
Iteration 220/1000 | Loss: 0.00001114
Iteration 221/1000 | Loss: 0.00001114
Iteration 222/1000 | Loss: 0.00001114
Iteration 223/1000 | Loss: 0.00001114
Iteration 224/1000 | Loss: 0.00001114
Iteration 225/1000 | Loss: 0.00001114
Iteration 226/1000 | Loss: 0.00001114
Iteration 227/1000 | Loss: 0.00001114
Iteration 228/1000 | Loss: 0.00001114
Iteration 229/1000 | Loss: 0.00001114
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 229. Stopping optimization.
Last 5 losses: [1.1137813089590054e-05, 1.1137813089590054e-05, 1.1137813089590054e-05, 1.1137813089590054e-05, 1.1137813089590054e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1137813089590054e-05

Optimization complete. Final v2v error: 2.8582868576049805 mm

Highest mean error: 2.9665021896362305 mm for frame 23

Lowest mean error: 2.6727781295776367 mm for frame 95

Saving results

Total time: 39.75129747390747
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01066930
Iteration 2/25 | Loss: 0.00594645
Iteration 3/25 | Loss: 0.00361481
Iteration 4/25 | Loss: 0.00329071
Iteration 5/25 | Loss: 0.00270402
Iteration 6/25 | Loss: 0.00212496
Iteration 7/25 | Loss: 0.00173762
Iteration 8/25 | Loss: 0.00151991
Iteration 9/25 | Loss: 0.00147644
Iteration 10/25 | Loss: 0.00140831
Iteration 11/25 | Loss: 0.00140693
Iteration 12/25 | Loss: 0.00136233
Iteration 13/25 | Loss: 0.00135196
Iteration 14/25 | Loss: 0.00134777
Iteration 15/25 | Loss: 0.00135203
Iteration 16/25 | Loss: 0.00134779
Iteration 17/25 | Loss: 0.00134517
Iteration 18/25 | Loss: 0.00134341
Iteration 19/25 | Loss: 0.00134206
Iteration 20/25 | Loss: 0.00134125
Iteration 21/25 | Loss: 0.00134124
Iteration 22/25 | Loss: 0.00134123
Iteration 23/25 | Loss: 0.00134123
Iteration 24/25 | Loss: 0.00134122
Iteration 25/25 | Loss: 0.00134122

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.54040676
Iteration 2/25 | Loss: 0.00116262
Iteration 3/25 | Loss: 0.00116262
Iteration 4/25 | Loss: 0.00116262
Iteration 5/25 | Loss: 0.00116262
Iteration 6/25 | Loss: 0.00115473
Iteration 7/25 | Loss: 0.00115473
Iteration 8/25 | Loss: 0.00115473
Iteration 9/25 | Loss: 0.00115473
Iteration 10/25 | Loss: 0.00115473
Iteration 11/25 | Loss: 0.00115473
Iteration 12/25 | Loss: 0.00115473
Iteration 13/25 | Loss: 0.00115473
Iteration 14/25 | Loss: 0.00115473
Iteration 15/25 | Loss: 0.00115473
Iteration 16/25 | Loss: 0.00115473
Iteration 17/25 | Loss: 0.00115473
Iteration 18/25 | Loss: 0.00115473
Iteration 19/25 | Loss: 0.00115473
Iteration 20/25 | Loss: 0.00115473
Iteration 21/25 | Loss: 0.00115473
Iteration 22/25 | Loss: 0.00115473
Iteration 23/25 | Loss: 0.00115473
Iteration 24/25 | Loss: 0.00115473
Iteration 25/25 | Loss: 0.00115473

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115473
Iteration 2/1000 | Loss: 0.00010162
Iteration 3/1000 | Loss: 0.00004105
Iteration 4/1000 | Loss: 0.00003161
Iteration 5/1000 | Loss: 0.00003029
Iteration 6/1000 | Loss: 0.00002968
Iteration 7/1000 | Loss: 0.00002903
Iteration 8/1000 | Loss: 0.00002850
Iteration 9/1000 | Loss: 0.00002801
Iteration 10/1000 | Loss: 0.00002776
Iteration 11/1000 | Loss: 0.00002741
Iteration 12/1000 | Loss: 0.00227235
Iteration 13/1000 | Loss: 0.00012458
Iteration 14/1000 | Loss: 0.00003284
Iteration 15/1000 | Loss: 0.00002747
Iteration 16/1000 | Loss: 0.00002415
Iteration 17/1000 | Loss: 0.00002396
Iteration 18/1000 | Loss: 0.00002163
Iteration 19/1000 | Loss: 0.00002066
Iteration 20/1000 | Loss: 0.00002835
Iteration 21/1000 | Loss: 0.00008956
Iteration 22/1000 | Loss: 0.00001961
Iteration 23/1000 | Loss: 0.00001859
Iteration 24/1000 | Loss: 0.00001809
Iteration 25/1000 | Loss: 0.00001786
Iteration 26/1000 | Loss: 0.00001754
Iteration 27/1000 | Loss: 0.00001738
Iteration 28/1000 | Loss: 0.00001732
Iteration 29/1000 | Loss: 0.00001732
Iteration 30/1000 | Loss: 0.00001723
Iteration 31/1000 | Loss: 0.00001721
Iteration 32/1000 | Loss: 0.00001721
Iteration 33/1000 | Loss: 0.00001720
Iteration 34/1000 | Loss: 0.00001720
Iteration 35/1000 | Loss: 0.00001720
Iteration 36/1000 | Loss: 0.00001719
Iteration 37/1000 | Loss: 0.00001719
Iteration 38/1000 | Loss: 0.00001719
Iteration 39/1000 | Loss: 0.00001719
Iteration 40/1000 | Loss: 0.00001719
Iteration 41/1000 | Loss: 0.00001718
Iteration 42/1000 | Loss: 0.00001718
Iteration 43/1000 | Loss: 0.00001718
Iteration 44/1000 | Loss: 0.00001718
Iteration 45/1000 | Loss: 0.00001718
Iteration 46/1000 | Loss: 0.00001718
Iteration 47/1000 | Loss: 0.00001717
Iteration 48/1000 | Loss: 0.00001717
Iteration 49/1000 | Loss: 0.00001717
Iteration 50/1000 | Loss: 0.00001717
Iteration 51/1000 | Loss: 0.00001717
Iteration 52/1000 | Loss: 0.00001716
Iteration 53/1000 | Loss: 0.00001716
Iteration 54/1000 | Loss: 0.00001715
Iteration 55/1000 | Loss: 0.00001714
Iteration 56/1000 | Loss: 0.00001714
Iteration 57/1000 | Loss: 0.00001714
Iteration 58/1000 | Loss: 0.00001714
Iteration 59/1000 | Loss: 0.00001714
Iteration 60/1000 | Loss: 0.00001714
Iteration 61/1000 | Loss: 0.00001714
Iteration 62/1000 | Loss: 0.00001714
Iteration 63/1000 | Loss: 0.00001712
Iteration 64/1000 | Loss: 0.00001712
Iteration 65/1000 | Loss: 0.00001711
Iteration 66/1000 | Loss: 0.00001711
Iteration 67/1000 | Loss: 0.00001711
Iteration 68/1000 | Loss: 0.00001711
Iteration 69/1000 | Loss: 0.00001711
Iteration 70/1000 | Loss: 0.00001711
Iteration 71/1000 | Loss: 0.00001710
Iteration 72/1000 | Loss: 0.00001710
Iteration 73/1000 | Loss: 0.00001710
Iteration 74/1000 | Loss: 0.00001709
Iteration 75/1000 | Loss: 0.00001709
Iteration 76/1000 | Loss: 0.00001709
Iteration 77/1000 | Loss: 0.00001709
Iteration 78/1000 | Loss: 0.00001709
Iteration 79/1000 | Loss: 0.00001709
Iteration 80/1000 | Loss: 0.00001709
Iteration 81/1000 | Loss: 0.00001709
Iteration 82/1000 | Loss: 0.00001709
Iteration 83/1000 | Loss: 0.00001709
Iteration 84/1000 | Loss: 0.00001709
Iteration 85/1000 | Loss: 0.00001709
Iteration 86/1000 | Loss: 0.00001708
Iteration 87/1000 | Loss: 0.00001708
Iteration 88/1000 | Loss: 0.00001708
Iteration 89/1000 | Loss: 0.00001708
Iteration 90/1000 | Loss: 0.00001708
Iteration 91/1000 | Loss: 0.00001708
Iteration 92/1000 | Loss: 0.00001708
Iteration 93/1000 | Loss: 0.00001708
Iteration 94/1000 | Loss: 0.00001708
Iteration 95/1000 | Loss: 0.00001708
Iteration 96/1000 | Loss: 0.00001708
Iteration 97/1000 | Loss: 0.00001707
Iteration 98/1000 | Loss: 0.00001707
Iteration 99/1000 | Loss: 0.00001707
Iteration 100/1000 | Loss: 0.00001707
Iteration 101/1000 | Loss: 0.00001707
Iteration 102/1000 | Loss: 0.00001707
Iteration 103/1000 | Loss: 0.00001707
Iteration 104/1000 | Loss: 0.00001706
Iteration 105/1000 | Loss: 0.00001706
Iteration 106/1000 | Loss: 0.00001706
Iteration 107/1000 | Loss: 0.00001706
Iteration 108/1000 | Loss: 0.00001706
Iteration 109/1000 | Loss: 0.00001706
Iteration 110/1000 | Loss: 0.00001706
Iteration 111/1000 | Loss: 0.00001706
Iteration 112/1000 | Loss: 0.00001706
Iteration 113/1000 | Loss: 0.00001705
Iteration 114/1000 | Loss: 0.00001705
Iteration 115/1000 | Loss: 0.00001705
Iteration 116/1000 | Loss: 0.00001705
Iteration 117/1000 | Loss: 0.00001704
Iteration 118/1000 | Loss: 0.00001704
Iteration 119/1000 | Loss: 0.00001703
Iteration 120/1000 | Loss: 0.00001703
Iteration 121/1000 | Loss: 0.00001703
Iteration 122/1000 | Loss: 0.00001703
Iteration 123/1000 | Loss: 0.00001703
Iteration 124/1000 | Loss: 0.00001702
Iteration 125/1000 | Loss: 0.00001702
Iteration 126/1000 | Loss: 0.00001702
Iteration 127/1000 | Loss: 0.00001702
Iteration 128/1000 | Loss: 0.00001702
Iteration 129/1000 | Loss: 0.00001702
Iteration 130/1000 | Loss: 0.00001702
Iteration 131/1000 | Loss: 0.00001702
Iteration 132/1000 | Loss: 0.00001701
Iteration 133/1000 | Loss: 0.00001701
Iteration 134/1000 | Loss: 0.00001701
Iteration 135/1000 | Loss: 0.00001701
Iteration 136/1000 | Loss: 0.00001701
Iteration 137/1000 | Loss: 0.00001701
Iteration 138/1000 | Loss: 0.00001701
Iteration 139/1000 | Loss: 0.00001701
Iteration 140/1000 | Loss: 0.00001701
Iteration 141/1000 | Loss: 0.00001701
Iteration 142/1000 | Loss: 0.00001701
Iteration 143/1000 | Loss: 0.00001701
Iteration 144/1000 | Loss: 0.00001700
Iteration 145/1000 | Loss: 0.00001700
Iteration 146/1000 | Loss: 0.00001700
Iteration 147/1000 | Loss: 0.00001700
Iteration 148/1000 | Loss: 0.00001700
Iteration 149/1000 | Loss: 0.00001700
Iteration 150/1000 | Loss: 0.00001700
Iteration 151/1000 | Loss: 0.00001700
Iteration 152/1000 | Loss: 0.00001700
Iteration 153/1000 | Loss: 0.00001700
Iteration 154/1000 | Loss: 0.00001700
Iteration 155/1000 | Loss: 0.00001700
Iteration 156/1000 | Loss: 0.00001700
Iteration 157/1000 | Loss: 0.00001700
Iteration 158/1000 | Loss: 0.00001700
Iteration 159/1000 | Loss: 0.00001700
Iteration 160/1000 | Loss: 0.00001700
Iteration 161/1000 | Loss: 0.00001700
Iteration 162/1000 | Loss: 0.00001700
Iteration 163/1000 | Loss: 0.00001700
Iteration 164/1000 | Loss: 0.00001700
Iteration 165/1000 | Loss: 0.00001700
Iteration 166/1000 | Loss: 0.00001700
Iteration 167/1000 | Loss: 0.00001700
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.6999732906697318e-05, 1.6999732906697318e-05, 1.6999732906697318e-05, 1.6999732906697318e-05, 1.6999732906697318e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6999732906697318e-05

Optimization complete. Final v2v error: 3.572148323059082 mm

Highest mean error: 3.837679386138916 mm for frame 227

Lowest mean error: 3.4656765460968018 mm for frame 31

Saving results

Total time: 94.06708002090454
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00379518
Iteration 2/25 | Loss: 0.00138573
Iteration 3/25 | Loss: 0.00121475
Iteration 4/25 | Loss: 0.00119851
Iteration 5/25 | Loss: 0.00119340
Iteration 6/25 | Loss: 0.00119201
Iteration 7/25 | Loss: 0.00119201
Iteration 8/25 | Loss: 0.00119201
Iteration 9/25 | Loss: 0.00119201
Iteration 10/25 | Loss: 0.00119201
Iteration 11/25 | Loss: 0.00119201
Iteration 12/25 | Loss: 0.00119201
Iteration 13/25 | Loss: 0.00119201
Iteration 14/25 | Loss: 0.00119201
Iteration 15/25 | Loss: 0.00119201
Iteration 16/25 | Loss: 0.00119201
Iteration 17/25 | Loss: 0.00119201
Iteration 18/25 | Loss: 0.00119201
Iteration 19/25 | Loss: 0.00119201
Iteration 20/25 | Loss: 0.00119201
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011920141987502575, 0.0011920141987502575, 0.0011920141987502575, 0.0011920141987502575, 0.0011920141987502575]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011920141987502575

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40019977
Iteration 2/25 | Loss: 0.00174231
Iteration 3/25 | Loss: 0.00174231
Iteration 4/25 | Loss: 0.00174231
Iteration 5/25 | Loss: 0.00174231
Iteration 6/25 | Loss: 0.00174231
Iteration 7/25 | Loss: 0.00174231
Iteration 8/25 | Loss: 0.00174231
Iteration 9/25 | Loss: 0.00174231
Iteration 10/25 | Loss: 0.00174231
Iteration 11/25 | Loss: 0.00174231
Iteration 12/25 | Loss: 0.00174231
Iteration 13/25 | Loss: 0.00174231
Iteration 14/25 | Loss: 0.00174231
Iteration 15/25 | Loss: 0.00174231
Iteration 16/25 | Loss: 0.00174231
Iteration 17/25 | Loss: 0.00174231
Iteration 18/25 | Loss: 0.00174231
Iteration 19/25 | Loss: 0.00174231
Iteration 20/25 | Loss: 0.00174231
Iteration 21/25 | Loss: 0.00174231
Iteration 22/25 | Loss: 0.00174231
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0017423058161512017, 0.0017423058161512017, 0.0017423058161512017, 0.0017423058161512017, 0.0017423058161512017]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017423058161512017

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00174231
Iteration 2/1000 | Loss: 0.00003911
Iteration 3/1000 | Loss: 0.00002524
Iteration 4/1000 | Loss: 0.00002049
Iteration 5/1000 | Loss: 0.00001854
Iteration 6/1000 | Loss: 0.00001746
Iteration 7/1000 | Loss: 0.00001684
Iteration 8/1000 | Loss: 0.00001630
Iteration 9/1000 | Loss: 0.00001588
Iteration 10/1000 | Loss: 0.00001553
Iteration 11/1000 | Loss: 0.00001519
Iteration 12/1000 | Loss: 0.00001518
Iteration 13/1000 | Loss: 0.00001505
Iteration 14/1000 | Loss: 0.00001505
Iteration 15/1000 | Loss: 0.00001504
Iteration 16/1000 | Loss: 0.00001504
Iteration 17/1000 | Loss: 0.00001497
Iteration 18/1000 | Loss: 0.00001497
Iteration 19/1000 | Loss: 0.00001491
Iteration 20/1000 | Loss: 0.00001490
Iteration 21/1000 | Loss: 0.00001490
Iteration 22/1000 | Loss: 0.00001489
Iteration 23/1000 | Loss: 0.00001486
Iteration 24/1000 | Loss: 0.00001485
Iteration 25/1000 | Loss: 0.00001485
Iteration 26/1000 | Loss: 0.00001484
Iteration 27/1000 | Loss: 0.00001482
Iteration 28/1000 | Loss: 0.00001479
Iteration 29/1000 | Loss: 0.00001479
Iteration 30/1000 | Loss: 0.00001478
Iteration 31/1000 | Loss: 0.00001478
Iteration 32/1000 | Loss: 0.00001477
Iteration 33/1000 | Loss: 0.00001476
Iteration 34/1000 | Loss: 0.00001476
Iteration 35/1000 | Loss: 0.00001476
Iteration 36/1000 | Loss: 0.00001475
Iteration 37/1000 | Loss: 0.00001475
Iteration 38/1000 | Loss: 0.00001474
Iteration 39/1000 | Loss: 0.00001474
Iteration 40/1000 | Loss: 0.00001473
Iteration 41/1000 | Loss: 0.00001473
Iteration 42/1000 | Loss: 0.00001473
Iteration 43/1000 | Loss: 0.00001472
Iteration 44/1000 | Loss: 0.00001471
Iteration 45/1000 | Loss: 0.00001471
Iteration 46/1000 | Loss: 0.00001471
Iteration 47/1000 | Loss: 0.00001470
Iteration 48/1000 | Loss: 0.00001470
Iteration 49/1000 | Loss: 0.00001470
Iteration 50/1000 | Loss: 0.00001469
Iteration 51/1000 | Loss: 0.00001469
Iteration 52/1000 | Loss: 0.00001469
Iteration 53/1000 | Loss: 0.00001469
Iteration 54/1000 | Loss: 0.00001469
Iteration 55/1000 | Loss: 0.00001469
Iteration 56/1000 | Loss: 0.00001468
Iteration 57/1000 | Loss: 0.00001467
Iteration 58/1000 | Loss: 0.00001467
Iteration 59/1000 | Loss: 0.00001466
Iteration 60/1000 | Loss: 0.00001466
Iteration 61/1000 | Loss: 0.00001466
Iteration 62/1000 | Loss: 0.00001466
Iteration 63/1000 | Loss: 0.00001466
Iteration 64/1000 | Loss: 0.00001466
Iteration 65/1000 | Loss: 0.00001466
Iteration 66/1000 | Loss: 0.00001465
Iteration 67/1000 | Loss: 0.00001465
Iteration 68/1000 | Loss: 0.00001465
Iteration 69/1000 | Loss: 0.00001464
Iteration 70/1000 | Loss: 0.00001464
Iteration 71/1000 | Loss: 0.00001463
Iteration 72/1000 | Loss: 0.00001463
Iteration 73/1000 | Loss: 0.00001463
Iteration 74/1000 | Loss: 0.00001463
Iteration 75/1000 | Loss: 0.00001462
Iteration 76/1000 | Loss: 0.00001462
Iteration 77/1000 | Loss: 0.00001462
Iteration 78/1000 | Loss: 0.00001462
Iteration 79/1000 | Loss: 0.00001461
Iteration 80/1000 | Loss: 0.00001461
Iteration 81/1000 | Loss: 0.00001461
Iteration 82/1000 | Loss: 0.00001461
Iteration 83/1000 | Loss: 0.00001458
Iteration 84/1000 | Loss: 0.00001458
Iteration 85/1000 | Loss: 0.00001457
Iteration 86/1000 | Loss: 0.00001457
Iteration 87/1000 | Loss: 0.00001456
Iteration 88/1000 | Loss: 0.00001453
Iteration 89/1000 | Loss: 0.00001453
Iteration 90/1000 | Loss: 0.00001451
Iteration 91/1000 | Loss: 0.00001450
Iteration 92/1000 | Loss: 0.00001450
Iteration 93/1000 | Loss: 0.00001449
Iteration 94/1000 | Loss: 0.00001448
Iteration 95/1000 | Loss: 0.00001448
Iteration 96/1000 | Loss: 0.00001448
Iteration 97/1000 | Loss: 0.00001448
Iteration 98/1000 | Loss: 0.00001448
Iteration 99/1000 | Loss: 0.00001447
Iteration 100/1000 | Loss: 0.00001447
Iteration 101/1000 | Loss: 0.00001447
Iteration 102/1000 | Loss: 0.00001447
Iteration 103/1000 | Loss: 0.00001447
Iteration 104/1000 | Loss: 0.00001447
Iteration 105/1000 | Loss: 0.00001447
Iteration 106/1000 | Loss: 0.00001447
Iteration 107/1000 | Loss: 0.00001447
Iteration 108/1000 | Loss: 0.00001446
Iteration 109/1000 | Loss: 0.00001446
Iteration 110/1000 | Loss: 0.00001446
Iteration 111/1000 | Loss: 0.00001445
Iteration 112/1000 | Loss: 0.00001445
Iteration 113/1000 | Loss: 0.00001445
Iteration 114/1000 | Loss: 0.00001444
Iteration 115/1000 | Loss: 0.00001444
Iteration 116/1000 | Loss: 0.00001444
Iteration 117/1000 | Loss: 0.00001443
Iteration 118/1000 | Loss: 0.00001443
Iteration 119/1000 | Loss: 0.00001443
Iteration 120/1000 | Loss: 0.00001443
Iteration 121/1000 | Loss: 0.00001443
Iteration 122/1000 | Loss: 0.00001443
Iteration 123/1000 | Loss: 0.00001443
Iteration 124/1000 | Loss: 0.00001443
Iteration 125/1000 | Loss: 0.00001442
Iteration 126/1000 | Loss: 0.00001442
Iteration 127/1000 | Loss: 0.00001442
Iteration 128/1000 | Loss: 0.00001441
Iteration 129/1000 | Loss: 0.00001441
Iteration 130/1000 | Loss: 0.00001441
Iteration 131/1000 | Loss: 0.00001441
Iteration 132/1000 | Loss: 0.00001440
Iteration 133/1000 | Loss: 0.00001440
Iteration 134/1000 | Loss: 0.00001440
Iteration 135/1000 | Loss: 0.00001440
Iteration 136/1000 | Loss: 0.00001440
Iteration 137/1000 | Loss: 0.00001439
Iteration 138/1000 | Loss: 0.00001439
Iteration 139/1000 | Loss: 0.00001439
Iteration 140/1000 | Loss: 0.00001439
Iteration 141/1000 | Loss: 0.00001439
Iteration 142/1000 | Loss: 0.00001439
Iteration 143/1000 | Loss: 0.00001439
Iteration 144/1000 | Loss: 0.00001439
Iteration 145/1000 | Loss: 0.00001439
Iteration 146/1000 | Loss: 0.00001439
Iteration 147/1000 | Loss: 0.00001438
Iteration 148/1000 | Loss: 0.00001438
Iteration 149/1000 | Loss: 0.00001438
Iteration 150/1000 | Loss: 0.00001438
Iteration 151/1000 | Loss: 0.00001438
Iteration 152/1000 | Loss: 0.00001438
Iteration 153/1000 | Loss: 0.00001438
Iteration 154/1000 | Loss: 0.00001438
Iteration 155/1000 | Loss: 0.00001438
Iteration 156/1000 | Loss: 0.00001438
Iteration 157/1000 | Loss: 0.00001438
Iteration 158/1000 | Loss: 0.00001438
Iteration 159/1000 | Loss: 0.00001437
Iteration 160/1000 | Loss: 0.00001437
Iteration 161/1000 | Loss: 0.00001437
Iteration 162/1000 | Loss: 0.00001437
Iteration 163/1000 | Loss: 0.00001437
Iteration 164/1000 | Loss: 0.00001437
Iteration 165/1000 | Loss: 0.00001437
Iteration 166/1000 | Loss: 0.00001437
Iteration 167/1000 | Loss: 0.00001436
Iteration 168/1000 | Loss: 0.00001436
Iteration 169/1000 | Loss: 0.00001436
Iteration 170/1000 | Loss: 0.00001436
Iteration 171/1000 | Loss: 0.00001436
Iteration 172/1000 | Loss: 0.00001436
Iteration 173/1000 | Loss: 0.00001436
Iteration 174/1000 | Loss: 0.00001436
Iteration 175/1000 | Loss: 0.00001436
Iteration 176/1000 | Loss: 0.00001436
Iteration 177/1000 | Loss: 0.00001436
Iteration 178/1000 | Loss: 0.00001436
Iteration 179/1000 | Loss: 0.00001436
Iteration 180/1000 | Loss: 0.00001436
Iteration 181/1000 | Loss: 0.00001435
Iteration 182/1000 | Loss: 0.00001435
Iteration 183/1000 | Loss: 0.00001435
Iteration 184/1000 | Loss: 0.00001435
Iteration 185/1000 | Loss: 0.00001435
Iteration 186/1000 | Loss: 0.00001435
Iteration 187/1000 | Loss: 0.00001435
Iteration 188/1000 | Loss: 0.00001435
Iteration 189/1000 | Loss: 0.00001435
Iteration 190/1000 | Loss: 0.00001435
Iteration 191/1000 | Loss: 0.00001435
Iteration 192/1000 | Loss: 0.00001435
Iteration 193/1000 | Loss: 0.00001435
Iteration 194/1000 | Loss: 0.00001435
Iteration 195/1000 | Loss: 0.00001435
Iteration 196/1000 | Loss: 0.00001435
Iteration 197/1000 | Loss: 0.00001435
Iteration 198/1000 | Loss: 0.00001435
Iteration 199/1000 | Loss: 0.00001435
Iteration 200/1000 | Loss: 0.00001435
Iteration 201/1000 | Loss: 0.00001435
Iteration 202/1000 | Loss: 0.00001435
Iteration 203/1000 | Loss: 0.00001435
Iteration 204/1000 | Loss: 0.00001435
Iteration 205/1000 | Loss: 0.00001435
Iteration 206/1000 | Loss: 0.00001435
Iteration 207/1000 | Loss: 0.00001435
Iteration 208/1000 | Loss: 0.00001435
Iteration 209/1000 | Loss: 0.00001435
Iteration 210/1000 | Loss: 0.00001435
Iteration 211/1000 | Loss: 0.00001435
Iteration 212/1000 | Loss: 0.00001435
Iteration 213/1000 | Loss: 0.00001435
Iteration 214/1000 | Loss: 0.00001435
Iteration 215/1000 | Loss: 0.00001435
Iteration 216/1000 | Loss: 0.00001435
Iteration 217/1000 | Loss: 0.00001435
Iteration 218/1000 | Loss: 0.00001435
Iteration 219/1000 | Loss: 0.00001435
Iteration 220/1000 | Loss: 0.00001435
Iteration 221/1000 | Loss: 0.00001435
Iteration 222/1000 | Loss: 0.00001435
Iteration 223/1000 | Loss: 0.00001435
Iteration 224/1000 | Loss: 0.00001435
Iteration 225/1000 | Loss: 0.00001435
Iteration 226/1000 | Loss: 0.00001435
Iteration 227/1000 | Loss: 0.00001435
Iteration 228/1000 | Loss: 0.00001435
Iteration 229/1000 | Loss: 0.00001435
Iteration 230/1000 | Loss: 0.00001435
Iteration 231/1000 | Loss: 0.00001435
Iteration 232/1000 | Loss: 0.00001435
Iteration 233/1000 | Loss: 0.00001435
Iteration 234/1000 | Loss: 0.00001435
Iteration 235/1000 | Loss: 0.00001435
Iteration 236/1000 | Loss: 0.00001435
Iteration 237/1000 | Loss: 0.00001435
Iteration 238/1000 | Loss: 0.00001435
Iteration 239/1000 | Loss: 0.00001435
Iteration 240/1000 | Loss: 0.00001435
Iteration 241/1000 | Loss: 0.00001435
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 241. Stopping optimization.
Last 5 losses: [1.4350641322380397e-05, 1.4350641322380397e-05, 1.4350641322380397e-05, 1.4350641322380397e-05, 1.4350641322380397e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4350641322380397e-05

Optimization complete. Final v2v error: 3.20371675491333 mm

Highest mean error: 3.5073928833007812 mm for frame 39

Lowest mean error: 2.593975782394409 mm for frame 5

Saving results

Total time: 42.86671829223633
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00473116
Iteration 2/25 | Loss: 0.00135755
Iteration 3/25 | Loss: 0.00123569
Iteration 4/25 | Loss: 0.00122023
Iteration 5/25 | Loss: 0.00121509
Iteration 6/25 | Loss: 0.00121467
Iteration 7/25 | Loss: 0.00121467
Iteration 8/25 | Loss: 0.00121467
Iteration 9/25 | Loss: 0.00121467
Iteration 10/25 | Loss: 0.00121467
Iteration 11/25 | Loss: 0.00121467
Iteration 12/25 | Loss: 0.00121467
Iteration 13/25 | Loss: 0.00121467
Iteration 14/25 | Loss: 0.00121467
Iteration 15/25 | Loss: 0.00121467
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012146681547164917, 0.0012146681547164917, 0.0012146681547164917, 0.0012146681547164917, 0.0012146681547164917]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012146681547164917

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.85275745
Iteration 2/25 | Loss: 0.00157854
Iteration 3/25 | Loss: 0.00157853
Iteration 4/25 | Loss: 0.00157853
Iteration 5/25 | Loss: 0.00157853
Iteration 6/25 | Loss: 0.00157853
Iteration 7/25 | Loss: 0.00157853
Iteration 8/25 | Loss: 0.00157853
Iteration 9/25 | Loss: 0.00157853
Iteration 10/25 | Loss: 0.00157853
Iteration 11/25 | Loss: 0.00157853
Iteration 12/25 | Loss: 0.00157853
Iteration 13/25 | Loss: 0.00157853
Iteration 14/25 | Loss: 0.00157853
Iteration 15/25 | Loss: 0.00157853
Iteration 16/25 | Loss: 0.00157853
Iteration 17/25 | Loss: 0.00157853
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001578527851961553, 0.001578527851961553, 0.001578527851961553, 0.001578527851961553, 0.001578527851961553]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001578527851961553

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00157853
Iteration 2/1000 | Loss: 0.00004278
Iteration 3/1000 | Loss: 0.00002706
Iteration 4/1000 | Loss: 0.00002239
Iteration 5/1000 | Loss: 0.00002084
Iteration 6/1000 | Loss: 0.00001973
Iteration 7/1000 | Loss: 0.00001876
Iteration 8/1000 | Loss: 0.00001826
Iteration 9/1000 | Loss: 0.00001789
Iteration 10/1000 | Loss: 0.00001759
Iteration 11/1000 | Loss: 0.00001732
Iteration 12/1000 | Loss: 0.00001709
Iteration 13/1000 | Loss: 0.00001699
Iteration 14/1000 | Loss: 0.00001697
Iteration 15/1000 | Loss: 0.00001696
Iteration 16/1000 | Loss: 0.00001689
Iteration 17/1000 | Loss: 0.00001685
Iteration 18/1000 | Loss: 0.00001681
Iteration 19/1000 | Loss: 0.00001676
Iteration 20/1000 | Loss: 0.00001670
Iteration 21/1000 | Loss: 0.00001668
Iteration 22/1000 | Loss: 0.00001660
Iteration 23/1000 | Loss: 0.00001653
Iteration 24/1000 | Loss: 0.00001651
Iteration 25/1000 | Loss: 0.00001650
Iteration 26/1000 | Loss: 0.00001649
Iteration 27/1000 | Loss: 0.00001648
Iteration 28/1000 | Loss: 0.00001648
Iteration 29/1000 | Loss: 0.00001647
Iteration 30/1000 | Loss: 0.00001647
Iteration 31/1000 | Loss: 0.00001647
Iteration 32/1000 | Loss: 0.00001647
Iteration 33/1000 | Loss: 0.00001646
Iteration 34/1000 | Loss: 0.00001646
Iteration 35/1000 | Loss: 0.00001645
Iteration 36/1000 | Loss: 0.00001645
Iteration 37/1000 | Loss: 0.00001644
Iteration 38/1000 | Loss: 0.00001644
Iteration 39/1000 | Loss: 0.00001643
Iteration 40/1000 | Loss: 0.00001643
Iteration 41/1000 | Loss: 0.00001643
Iteration 42/1000 | Loss: 0.00001642
Iteration 43/1000 | Loss: 0.00001642
Iteration 44/1000 | Loss: 0.00001641
Iteration 45/1000 | Loss: 0.00001641
Iteration 46/1000 | Loss: 0.00001641
Iteration 47/1000 | Loss: 0.00001640
Iteration 48/1000 | Loss: 0.00001640
Iteration 49/1000 | Loss: 0.00001639
Iteration 50/1000 | Loss: 0.00001639
Iteration 51/1000 | Loss: 0.00001639
Iteration 52/1000 | Loss: 0.00001638
Iteration 53/1000 | Loss: 0.00001638
Iteration 54/1000 | Loss: 0.00001637
Iteration 55/1000 | Loss: 0.00001637
Iteration 56/1000 | Loss: 0.00001637
Iteration 57/1000 | Loss: 0.00001637
Iteration 58/1000 | Loss: 0.00001637
Iteration 59/1000 | Loss: 0.00001637
Iteration 60/1000 | Loss: 0.00001637
Iteration 61/1000 | Loss: 0.00001637
Iteration 62/1000 | Loss: 0.00001636
Iteration 63/1000 | Loss: 0.00001636
Iteration 64/1000 | Loss: 0.00001636
Iteration 65/1000 | Loss: 0.00001636
Iteration 66/1000 | Loss: 0.00001636
Iteration 67/1000 | Loss: 0.00001636
Iteration 68/1000 | Loss: 0.00001636
Iteration 69/1000 | Loss: 0.00001636
Iteration 70/1000 | Loss: 0.00001636
Iteration 71/1000 | Loss: 0.00001636
Iteration 72/1000 | Loss: 0.00001636
Iteration 73/1000 | Loss: 0.00001636
Iteration 74/1000 | Loss: 0.00001634
Iteration 75/1000 | Loss: 0.00001634
Iteration 76/1000 | Loss: 0.00001634
Iteration 77/1000 | Loss: 0.00001633
Iteration 78/1000 | Loss: 0.00001633
Iteration 79/1000 | Loss: 0.00001633
Iteration 80/1000 | Loss: 0.00001633
Iteration 81/1000 | Loss: 0.00001633
Iteration 82/1000 | Loss: 0.00001632
Iteration 83/1000 | Loss: 0.00001632
Iteration 84/1000 | Loss: 0.00001632
Iteration 85/1000 | Loss: 0.00001632
Iteration 86/1000 | Loss: 0.00001631
Iteration 87/1000 | Loss: 0.00001631
Iteration 88/1000 | Loss: 0.00001631
Iteration 89/1000 | Loss: 0.00001630
Iteration 90/1000 | Loss: 0.00001630
Iteration 91/1000 | Loss: 0.00001630
Iteration 92/1000 | Loss: 0.00001629
Iteration 93/1000 | Loss: 0.00001629
Iteration 94/1000 | Loss: 0.00001629
Iteration 95/1000 | Loss: 0.00001628
Iteration 96/1000 | Loss: 0.00001628
Iteration 97/1000 | Loss: 0.00001628
Iteration 98/1000 | Loss: 0.00001627
Iteration 99/1000 | Loss: 0.00001627
Iteration 100/1000 | Loss: 0.00001627
Iteration 101/1000 | Loss: 0.00001627
Iteration 102/1000 | Loss: 0.00001626
Iteration 103/1000 | Loss: 0.00001626
Iteration 104/1000 | Loss: 0.00001626
Iteration 105/1000 | Loss: 0.00001625
Iteration 106/1000 | Loss: 0.00001625
Iteration 107/1000 | Loss: 0.00001625
Iteration 108/1000 | Loss: 0.00001625
Iteration 109/1000 | Loss: 0.00001625
Iteration 110/1000 | Loss: 0.00001625
Iteration 111/1000 | Loss: 0.00001624
Iteration 112/1000 | Loss: 0.00001624
Iteration 113/1000 | Loss: 0.00001624
Iteration 114/1000 | Loss: 0.00001624
Iteration 115/1000 | Loss: 0.00001624
Iteration 116/1000 | Loss: 0.00001624
Iteration 117/1000 | Loss: 0.00001623
Iteration 118/1000 | Loss: 0.00001623
Iteration 119/1000 | Loss: 0.00001623
Iteration 120/1000 | Loss: 0.00001622
Iteration 121/1000 | Loss: 0.00001622
Iteration 122/1000 | Loss: 0.00001622
Iteration 123/1000 | Loss: 0.00001622
Iteration 124/1000 | Loss: 0.00001622
Iteration 125/1000 | Loss: 0.00001621
Iteration 126/1000 | Loss: 0.00001621
Iteration 127/1000 | Loss: 0.00001621
Iteration 128/1000 | Loss: 0.00001621
Iteration 129/1000 | Loss: 0.00001621
Iteration 130/1000 | Loss: 0.00001621
Iteration 131/1000 | Loss: 0.00001621
Iteration 132/1000 | Loss: 0.00001621
Iteration 133/1000 | Loss: 0.00001621
Iteration 134/1000 | Loss: 0.00001621
Iteration 135/1000 | Loss: 0.00001621
Iteration 136/1000 | Loss: 0.00001621
Iteration 137/1000 | Loss: 0.00001621
Iteration 138/1000 | Loss: 0.00001621
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.6206191503442824e-05, 1.6206191503442824e-05, 1.6206191503442824e-05, 1.6206191503442824e-05, 1.6206191503442824e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6206191503442824e-05

Optimization complete. Final v2v error: 3.3632915019989014 mm

Highest mean error: 4.728847980499268 mm for frame 55

Lowest mean error: 2.6829278469085693 mm for frame 67

Saving results

Total time: 45.56405425071716
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00755682
Iteration 2/25 | Loss: 0.00139694
Iteration 3/25 | Loss: 0.00117553
Iteration 4/25 | Loss: 0.00116429
Iteration 5/25 | Loss: 0.00116180
Iteration 6/25 | Loss: 0.00116174
Iteration 7/25 | Loss: 0.00116174
Iteration 8/25 | Loss: 0.00116174
Iteration 9/25 | Loss: 0.00116174
Iteration 10/25 | Loss: 0.00116174
Iteration 11/25 | Loss: 0.00116174
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001161741092801094, 0.001161741092801094, 0.001161741092801094, 0.001161741092801094, 0.001161741092801094]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001161741092801094

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28009307
Iteration 2/25 | Loss: 0.00110254
Iteration 3/25 | Loss: 0.00110254
Iteration 4/25 | Loss: 0.00110253
Iteration 5/25 | Loss: 0.00110253
Iteration 6/25 | Loss: 0.00110253
Iteration 7/25 | Loss: 0.00110253
Iteration 8/25 | Loss: 0.00110253
Iteration 9/25 | Loss: 0.00110253
Iteration 10/25 | Loss: 0.00110253
Iteration 11/25 | Loss: 0.00110253
Iteration 12/25 | Loss: 0.00110253
Iteration 13/25 | Loss: 0.00110253
Iteration 14/25 | Loss: 0.00110253
Iteration 15/25 | Loss: 0.00110253
Iteration 16/25 | Loss: 0.00110253
Iteration 17/25 | Loss: 0.00110253
Iteration 18/25 | Loss: 0.00110253
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011025310959666967, 0.0011025310959666967, 0.0011025310959666967, 0.0011025310959666967, 0.0011025310959666967]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011025310959666967

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110253
Iteration 2/1000 | Loss: 0.00002507
Iteration 3/1000 | Loss: 0.00001905
Iteration 4/1000 | Loss: 0.00001692
Iteration 5/1000 | Loss: 0.00001567
Iteration 6/1000 | Loss: 0.00001489
Iteration 7/1000 | Loss: 0.00001438
Iteration 8/1000 | Loss: 0.00001394
Iteration 9/1000 | Loss: 0.00001362
Iteration 10/1000 | Loss: 0.00001326
Iteration 11/1000 | Loss: 0.00001300
Iteration 12/1000 | Loss: 0.00001285
Iteration 13/1000 | Loss: 0.00001281
Iteration 14/1000 | Loss: 0.00001266
Iteration 15/1000 | Loss: 0.00001266
Iteration 16/1000 | Loss: 0.00001257
Iteration 17/1000 | Loss: 0.00001252
Iteration 18/1000 | Loss: 0.00001249
Iteration 19/1000 | Loss: 0.00001244
Iteration 20/1000 | Loss: 0.00001239
Iteration 21/1000 | Loss: 0.00001235
Iteration 22/1000 | Loss: 0.00001230
Iteration 23/1000 | Loss: 0.00001229
Iteration 24/1000 | Loss: 0.00001225
Iteration 25/1000 | Loss: 0.00001222
Iteration 26/1000 | Loss: 0.00001220
Iteration 27/1000 | Loss: 0.00001220
Iteration 28/1000 | Loss: 0.00001219
Iteration 29/1000 | Loss: 0.00001218
Iteration 30/1000 | Loss: 0.00001218
Iteration 31/1000 | Loss: 0.00001218
Iteration 32/1000 | Loss: 0.00001217
Iteration 33/1000 | Loss: 0.00001216
Iteration 34/1000 | Loss: 0.00001216
Iteration 35/1000 | Loss: 0.00001215
Iteration 36/1000 | Loss: 0.00001215
Iteration 37/1000 | Loss: 0.00001213
Iteration 38/1000 | Loss: 0.00001213
Iteration 39/1000 | Loss: 0.00001212
Iteration 40/1000 | Loss: 0.00001212
Iteration 41/1000 | Loss: 0.00001212
Iteration 42/1000 | Loss: 0.00001212
Iteration 43/1000 | Loss: 0.00001211
Iteration 44/1000 | Loss: 0.00001210
Iteration 45/1000 | Loss: 0.00001210
Iteration 46/1000 | Loss: 0.00001210
Iteration 47/1000 | Loss: 0.00001209
Iteration 48/1000 | Loss: 0.00001209
Iteration 49/1000 | Loss: 0.00001209
Iteration 50/1000 | Loss: 0.00001209
Iteration 51/1000 | Loss: 0.00001209
Iteration 52/1000 | Loss: 0.00001209
Iteration 53/1000 | Loss: 0.00001207
Iteration 54/1000 | Loss: 0.00001207
Iteration 55/1000 | Loss: 0.00001207
Iteration 56/1000 | Loss: 0.00001207
Iteration 57/1000 | Loss: 0.00001207
Iteration 58/1000 | Loss: 0.00001207
Iteration 59/1000 | Loss: 0.00001207
Iteration 60/1000 | Loss: 0.00001206
Iteration 61/1000 | Loss: 0.00001206
Iteration 62/1000 | Loss: 0.00001206
Iteration 63/1000 | Loss: 0.00001205
Iteration 64/1000 | Loss: 0.00001203
Iteration 65/1000 | Loss: 0.00001203
Iteration 66/1000 | Loss: 0.00001203
Iteration 67/1000 | Loss: 0.00001203
Iteration 68/1000 | Loss: 0.00001203
Iteration 69/1000 | Loss: 0.00001203
Iteration 70/1000 | Loss: 0.00001203
Iteration 71/1000 | Loss: 0.00001203
Iteration 72/1000 | Loss: 0.00001203
Iteration 73/1000 | Loss: 0.00001203
Iteration 74/1000 | Loss: 0.00001203
Iteration 75/1000 | Loss: 0.00001202
Iteration 76/1000 | Loss: 0.00001202
Iteration 77/1000 | Loss: 0.00001202
Iteration 78/1000 | Loss: 0.00001201
Iteration 79/1000 | Loss: 0.00001201
Iteration 80/1000 | Loss: 0.00001200
Iteration 81/1000 | Loss: 0.00001199
Iteration 82/1000 | Loss: 0.00001199
Iteration 83/1000 | Loss: 0.00001199
Iteration 84/1000 | Loss: 0.00001198
Iteration 85/1000 | Loss: 0.00001198
Iteration 86/1000 | Loss: 0.00001197
Iteration 87/1000 | Loss: 0.00001197
Iteration 88/1000 | Loss: 0.00001197
Iteration 89/1000 | Loss: 0.00001197
Iteration 90/1000 | Loss: 0.00001197
Iteration 91/1000 | Loss: 0.00001196
Iteration 92/1000 | Loss: 0.00001196
Iteration 93/1000 | Loss: 0.00001196
Iteration 94/1000 | Loss: 0.00001195
Iteration 95/1000 | Loss: 0.00001195
Iteration 96/1000 | Loss: 0.00001195
Iteration 97/1000 | Loss: 0.00001194
Iteration 98/1000 | Loss: 0.00001194
Iteration 99/1000 | Loss: 0.00001193
Iteration 100/1000 | Loss: 0.00001193
Iteration 101/1000 | Loss: 0.00001193
Iteration 102/1000 | Loss: 0.00001192
Iteration 103/1000 | Loss: 0.00001192
Iteration 104/1000 | Loss: 0.00001192
Iteration 105/1000 | Loss: 0.00001191
Iteration 106/1000 | Loss: 0.00001191
Iteration 107/1000 | Loss: 0.00001191
Iteration 108/1000 | Loss: 0.00001191
Iteration 109/1000 | Loss: 0.00001189
Iteration 110/1000 | Loss: 0.00001188
Iteration 111/1000 | Loss: 0.00001188
Iteration 112/1000 | Loss: 0.00001188
Iteration 113/1000 | Loss: 0.00001188
Iteration 114/1000 | Loss: 0.00001188
Iteration 115/1000 | Loss: 0.00001188
Iteration 116/1000 | Loss: 0.00001188
Iteration 117/1000 | Loss: 0.00001188
Iteration 118/1000 | Loss: 0.00001188
Iteration 119/1000 | Loss: 0.00001188
Iteration 120/1000 | Loss: 0.00001187
Iteration 121/1000 | Loss: 0.00001187
Iteration 122/1000 | Loss: 0.00001187
Iteration 123/1000 | Loss: 0.00001186
Iteration 124/1000 | Loss: 0.00001185
Iteration 125/1000 | Loss: 0.00001185
Iteration 126/1000 | Loss: 0.00001185
Iteration 127/1000 | Loss: 0.00001185
Iteration 128/1000 | Loss: 0.00001185
Iteration 129/1000 | Loss: 0.00001184
Iteration 130/1000 | Loss: 0.00001184
Iteration 131/1000 | Loss: 0.00001183
Iteration 132/1000 | Loss: 0.00001183
Iteration 133/1000 | Loss: 0.00001183
Iteration 134/1000 | Loss: 0.00001183
Iteration 135/1000 | Loss: 0.00001183
Iteration 136/1000 | Loss: 0.00001182
Iteration 137/1000 | Loss: 0.00001182
Iteration 138/1000 | Loss: 0.00001182
Iteration 139/1000 | Loss: 0.00001182
Iteration 140/1000 | Loss: 0.00001181
Iteration 141/1000 | Loss: 0.00001181
Iteration 142/1000 | Loss: 0.00001180
Iteration 143/1000 | Loss: 0.00001180
Iteration 144/1000 | Loss: 0.00001180
Iteration 145/1000 | Loss: 0.00001180
Iteration 146/1000 | Loss: 0.00001180
Iteration 147/1000 | Loss: 0.00001179
Iteration 148/1000 | Loss: 0.00001179
Iteration 149/1000 | Loss: 0.00001179
Iteration 150/1000 | Loss: 0.00001178
Iteration 151/1000 | Loss: 0.00001178
Iteration 152/1000 | Loss: 0.00001178
Iteration 153/1000 | Loss: 0.00001178
Iteration 154/1000 | Loss: 0.00001178
Iteration 155/1000 | Loss: 0.00001178
Iteration 156/1000 | Loss: 0.00001178
Iteration 157/1000 | Loss: 0.00001178
Iteration 158/1000 | Loss: 0.00001178
Iteration 159/1000 | Loss: 0.00001178
Iteration 160/1000 | Loss: 0.00001178
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.178063939732965e-05, 1.178063939732965e-05, 1.178063939732965e-05, 1.178063939732965e-05, 1.178063939732965e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.178063939732965e-05

Optimization complete. Final v2v error: 2.868236541748047 mm

Highest mean error: 4.95680046081543 mm for frame 0

Lowest mean error: 2.4772229194641113 mm for frame 164

Saving results

Total time: 47.398083209991455
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00391355
Iteration 2/25 | Loss: 0.00126828
Iteration 3/25 | Loss: 0.00117485
Iteration 4/25 | Loss: 0.00116360
Iteration 5/25 | Loss: 0.00116144
Iteration 6/25 | Loss: 0.00116144
Iteration 7/25 | Loss: 0.00116144
Iteration 8/25 | Loss: 0.00116144
Iteration 9/25 | Loss: 0.00116144
Iteration 10/25 | Loss: 0.00116144
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011614355025812984, 0.0011614355025812984, 0.0011614355025812984, 0.0011614355025812984, 0.0011614355025812984]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011614355025812984

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28139460
Iteration 2/25 | Loss: 0.00124022
Iteration 3/25 | Loss: 0.00124022
Iteration 4/25 | Loss: 0.00124022
Iteration 5/25 | Loss: 0.00124022
Iteration 6/25 | Loss: 0.00124022
Iteration 7/25 | Loss: 0.00124022
Iteration 8/25 | Loss: 0.00124022
Iteration 9/25 | Loss: 0.00124022
Iteration 10/25 | Loss: 0.00124022
Iteration 11/25 | Loss: 0.00124022
Iteration 12/25 | Loss: 0.00124022
Iteration 13/25 | Loss: 0.00124022
Iteration 14/25 | Loss: 0.00124022
Iteration 15/25 | Loss: 0.00124022
Iteration 16/25 | Loss: 0.00124022
Iteration 17/25 | Loss: 0.00124022
Iteration 18/25 | Loss: 0.00124022
Iteration 19/25 | Loss: 0.00124022
Iteration 20/25 | Loss: 0.00124022
Iteration 21/25 | Loss: 0.00124022
Iteration 22/25 | Loss: 0.00124022
Iteration 23/25 | Loss: 0.00124022
Iteration 24/25 | Loss: 0.00124022
Iteration 25/25 | Loss: 0.00124022

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124022
Iteration 2/1000 | Loss: 0.00002873
Iteration 3/1000 | Loss: 0.00002200
Iteration 4/1000 | Loss: 0.00001898
Iteration 5/1000 | Loss: 0.00001738
Iteration 6/1000 | Loss: 0.00001638
Iteration 7/1000 | Loss: 0.00001569
Iteration 8/1000 | Loss: 0.00001519
Iteration 9/1000 | Loss: 0.00001480
Iteration 10/1000 | Loss: 0.00001443
Iteration 11/1000 | Loss: 0.00001409
Iteration 12/1000 | Loss: 0.00001392
Iteration 13/1000 | Loss: 0.00001369
Iteration 14/1000 | Loss: 0.00001369
Iteration 15/1000 | Loss: 0.00001358
Iteration 16/1000 | Loss: 0.00001357
Iteration 17/1000 | Loss: 0.00001350
Iteration 18/1000 | Loss: 0.00001348
Iteration 19/1000 | Loss: 0.00001347
Iteration 20/1000 | Loss: 0.00001345
Iteration 21/1000 | Loss: 0.00001344
Iteration 22/1000 | Loss: 0.00001343
Iteration 23/1000 | Loss: 0.00001342
Iteration 24/1000 | Loss: 0.00001341
Iteration 25/1000 | Loss: 0.00001332
Iteration 26/1000 | Loss: 0.00001331
Iteration 27/1000 | Loss: 0.00001330
Iteration 28/1000 | Loss: 0.00001330
Iteration 29/1000 | Loss: 0.00001326
Iteration 30/1000 | Loss: 0.00001320
Iteration 31/1000 | Loss: 0.00001319
Iteration 32/1000 | Loss: 0.00001317
Iteration 33/1000 | Loss: 0.00001317
Iteration 34/1000 | Loss: 0.00001316
Iteration 35/1000 | Loss: 0.00001315
Iteration 36/1000 | Loss: 0.00001314
Iteration 37/1000 | Loss: 0.00001311
Iteration 38/1000 | Loss: 0.00001307
Iteration 39/1000 | Loss: 0.00001306
Iteration 40/1000 | Loss: 0.00001306
Iteration 41/1000 | Loss: 0.00001305
Iteration 42/1000 | Loss: 0.00001303
Iteration 43/1000 | Loss: 0.00001302
Iteration 44/1000 | Loss: 0.00001302
Iteration 45/1000 | Loss: 0.00001301
Iteration 46/1000 | Loss: 0.00001301
Iteration 47/1000 | Loss: 0.00001300
Iteration 48/1000 | Loss: 0.00001300
Iteration 49/1000 | Loss: 0.00001300
Iteration 50/1000 | Loss: 0.00001295
Iteration 51/1000 | Loss: 0.00001294
Iteration 52/1000 | Loss: 0.00001293
Iteration 53/1000 | Loss: 0.00001293
Iteration 54/1000 | Loss: 0.00001293
Iteration 55/1000 | Loss: 0.00001293
Iteration 56/1000 | Loss: 0.00001293
Iteration 57/1000 | Loss: 0.00001293
Iteration 58/1000 | Loss: 0.00001292
Iteration 59/1000 | Loss: 0.00001292
Iteration 60/1000 | Loss: 0.00001292
Iteration 61/1000 | Loss: 0.00001292
Iteration 62/1000 | Loss: 0.00001292
Iteration 63/1000 | Loss: 0.00001292
Iteration 64/1000 | Loss: 0.00001292
Iteration 65/1000 | Loss: 0.00001291
Iteration 66/1000 | Loss: 0.00001291
Iteration 67/1000 | Loss: 0.00001291
Iteration 68/1000 | Loss: 0.00001291
Iteration 69/1000 | Loss: 0.00001290
Iteration 70/1000 | Loss: 0.00001290
Iteration 71/1000 | Loss: 0.00001290
Iteration 72/1000 | Loss: 0.00001290
Iteration 73/1000 | Loss: 0.00001290
Iteration 74/1000 | Loss: 0.00001290
Iteration 75/1000 | Loss: 0.00001290
Iteration 76/1000 | Loss: 0.00001289
Iteration 77/1000 | Loss: 0.00001289
Iteration 78/1000 | Loss: 0.00001289
Iteration 79/1000 | Loss: 0.00001289
Iteration 80/1000 | Loss: 0.00001288
Iteration 81/1000 | Loss: 0.00001288
Iteration 82/1000 | Loss: 0.00001288
Iteration 83/1000 | Loss: 0.00001288
Iteration 84/1000 | Loss: 0.00001287
Iteration 85/1000 | Loss: 0.00001287
Iteration 86/1000 | Loss: 0.00001286
Iteration 87/1000 | Loss: 0.00001286
Iteration 88/1000 | Loss: 0.00001286
Iteration 89/1000 | Loss: 0.00001286
Iteration 90/1000 | Loss: 0.00001286
Iteration 91/1000 | Loss: 0.00001286
Iteration 92/1000 | Loss: 0.00001285
Iteration 93/1000 | Loss: 0.00001285
Iteration 94/1000 | Loss: 0.00001285
Iteration 95/1000 | Loss: 0.00001284
Iteration 96/1000 | Loss: 0.00001284
Iteration 97/1000 | Loss: 0.00001284
Iteration 98/1000 | Loss: 0.00001284
Iteration 99/1000 | Loss: 0.00001284
Iteration 100/1000 | Loss: 0.00001283
Iteration 101/1000 | Loss: 0.00001283
Iteration 102/1000 | Loss: 0.00001283
Iteration 103/1000 | Loss: 0.00001282
Iteration 104/1000 | Loss: 0.00001282
Iteration 105/1000 | Loss: 0.00001282
Iteration 106/1000 | Loss: 0.00001282
Iteration 107/1000 | Loss: 0.00001281
Iteration 108/1000 | Loss: 0.00001281
Iteration 109/1000 | Loss: 0.00001281
Iteration 110/1000 | Loss: 0.00001281
Iteration 111/1000 | Loss: 0.00001281
Iteration 112/1000 | Loss: 0.00001280
Iteration 113/1000 | Loss: 0.00001280
Iteration 114/1000 | Loss: 0.00001280
Iteration 115/1000 | Loss: 0.00001280
Iteration 116/1000 | Loss: 0.00001280
Iteration 117/1000 | Loss: 0.00001280
Iteration 118/1000 | Loss: 0.00001279
Iteration 119/1000 | Loss: 0.00001279
Iteration 120/1000 | Loss: 0.00001279
Iteration 121/1000 | Loss: 0.00001279
Iteration 122/1000 | Loss: 0.00001278
Iteration 123/1000 | Loss: 0.00001278
Iteration 124/1000 | Loss: 0.00001278
Iteration 125/1000 | Loss: 0.00001277
Iteration 126/1000 | Loss: 0.00001277
Iteration 127/1000 | Loss: 0.00001277
Iteration 128/1000 | Loss: 0.00001277
Iteration 129/1000 | Loss: 0.00001277
Iteration 130/1000 | Loss: 0.00001277
Iteration 131/1000 | Loss: 0.00001276
Iteration 132/1000 | Loss: 0.00001275
Iteration 133/1000 | Loss: 0.00001274
Iteration 134/1000 | Loss: 0.00001273
Iteration 135/1000 | Loss: 0.00001273
Iteration 136/1000 | Loss: 0.00001272
Iteration 137/1000 | Loss: 0.00001272
Iteration 138/1000 | Loss: 0.00001272
Iteration 139/1000 | Loss: 0.00001272
Iteration 140/1000 | Loss: 0.00001272
Iteration 141/1000 | Loss: 0.00001272
Iteration 142/1000 | Loss: 0.00001272
Iteration 143/1000 | Loss: 0.00001271
Iteration 144/1000 | Loss: 0.00001271
Iteration 145/1000 | Loss: 0.00001271
Iteration 146/1000 | Loss: 0.00001271
Iteration 147/1000 | Loss: 0.00001271
Iteration 148/1000 | Loss: 0.00001271
Iteration 149/1000 | Loss: 0.00001271
Iteration 150/1000 | Loss: 0.00001271
Iteration 151/1000 | Loss: 0.00001271
Iteration 152/1000 | Loss: 0.00001271
Iteration 153/1000 | Loss: 0.00001271
Iteration 154/1000 | Loss: 0.00001271
Iteration 155/1000 | Loss: 0.00001271
Iteration 156/1000 | Loss: 0.00001271
Iteration 157/1000 | Loss: 0.00001270
Iteration 158/1000 | Loss: 0.00001270
Iteration 159/1000 | Loss: 0.00001270
Iteration 160/1000 | Loss: 0.00001270
Iteration 161/1000 | Loss: 0.00001270
Iteration 162/1000 | Loss: 0.00001270
Iteration 163/1000 | Loss: 0.00001270
Iteration 164/1000 | Loss: 0.00001270
Iteration 165/1000 | Loss: 0.00001270
Iteration 166/1000 | Loss: 0.00001270
Iteration 167/1000 | Loss: 0.00001270
Iteration 168/1000 | Loss: 0.00001270
Iteration 169/1000 | Loss: 0.00001270
Iteration 170/1000 | Loss: 0.00001269
Iteration 171/1000 | Loss: 0.00001269
Iteration 172/1000 | Loss: 0.00001269
Iteration 173/1000 | Loss: 0.00001269
Iteration 174/1000 | Loss: 0.00001269
Iteration 175/1000 | Loss: 0.00001269
Iteration 176/1000 | Loss: 0.00001269
Iteration 177/1000 | Loss: 0.00001269
Iteration 178/1000 | Loss: 0.00001269
Iteration 179/1000 | Loss: 0.00001269
Iteration 180/1000 | Loss: 0.00001269
Iteration 181/1000 | Loss: 0.00001269
Iteration 182/1000 | Loss: 0.00001269
Iteration 183/1000 | Loss: 0.00001269
Iteration 184/1000 | Loss: 0.00001269
Iteration 185/1000 | Loss: 0.00001269
Iteration 186/1000 | Loss: 0.00001269
Iteration 187/1000 | Loss: 0.00001269
Iteration 188/1000 | Loss: 0.00001269
Iteration 189/1000 | Loss: 0.00001269
Iteration 190/1000 | Loss: 0.00001269
Iteration 191/1000 | Loss: 0.00001269
Iteration 192/1000 | Loss: 0.00001269
Iteration 193/1000 | Loss: 0.00001269
Iteration 194/1000 | Loss: 0.00001269
Iteration 195/1000 | Loss: 0.00001269
Iteration 196/1000 | Loss: 0.00001269
Iteration 197/1000 | Loss: 0.00001269
Iteration 198/1000 | Loss: 0.00001269
Iteration 199/1000 | Loss: 0.00001269
Iteration 200/1000 | Loss: 0.00001269
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [1.2687649359577335e-05, 1.2687649359577335e-05, 1.2687649359577335e-05, 1.2687649359577335e-05, 1.2687649359577335e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2687649359577335e-05

Optimization complete. Final v2v error: 2.98183012008667 mm

Highest mean error: 3.4726645946502686 mm for frame 103

Lowest mean error: 2.56195330619812 mm for frame 12

Saving results

Total time: 44.366926431655884
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00417074
Iteration 2/25 | Loss: 0.00123514
Iteration 3/25 | Loss: 0.00117355
Iteration 4/25 | Loss: 0.00116316
Iteration 5/25 | Loss: 0.00115962
Iteration 6/25 | Loss: 0.00115869
Iteration 7/25 | Loss: 0.00115839
Iteration 8/25 | Loss: 0.00115832
Iteration 9/25 | Loss: 0.00115832
Iteration 10/25 | Loss: 0.00115832
Iteration 11/25 | Loss: 0.00115832
Iteration 12/25 | Loss: 0.00115832
Iteration 13/25 | Loss: 0.00115832
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011583188315853477, 0.0011583188315853477, 0.0011583188315853477, 0.0011583188315853477, 0.0011583188315853477]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011583188315853477

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64763522
Iteration 2/25 | Loss: 0.00139523
Iteration 3/25 | Loss: 0.00139522
Iteration 4/25 | Loss: 0.00139522
Iteration 5/25 | Loss: 0.00139522
Iteration 6/25 | Loss: 0.00139522
Iteration 7/25 | Loss: 0.00139522
Iteration 8/25 | Loss: 0.00139522
Iteration 9/25 | Loss: 0.00139522
Iteration 10/25 | Loss: 0.00139522
Iteration 11/25 | Loss: 0.00139522
Iteration 12/25 | Loss: 0.00139522
Iteration 13/25 | Loss: 0.00139522
Iteration 14/25 | Loss: 0.00139522
Iteration 15/25 | Loss: 0.00139522
Iteration 16/25 | Loss: 0.00139522
Iteration 17/25 | Loss: 0.00139522
Iteration 18/25 | Loss: 0.00139522
Iteration 19/25 | Loss: 0.00139522
Iteration 20/25 | Loss: 0.00139522
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0013952205190435052, 0.0013952205190435052, 0.0013952205190435052, 0.0013952205190435052, 0.0013952205190435052]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013952205190435052

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00139522
Iteration 2/1000 | Loss: 0.00002749
Iteration 3/1000 | Loss: 0.00001888
Iteration 4/1000 | Loss: 0.00001483
Iteration 5/1000 | Loss: 0.00001365
Iteration 6/1000 | Loss: 0.00001301
Iteration 7/1000 | Loss: 0.00001242
Iteration 8/1000 | Loss: 0.00001198
Iteration 9/1000 | Loss: 0.00001161
Iteration 10/1000 | Loss: 0.00001139
Iteration 11/1000 | Loss: 0.00001129
Iteration 12/1000 | Loss: 0.00001108
Iteration 13/1000 | Loss: 0.00001106
Iteration 14/1000 | Loss: 0.00001104
Iteration 15/1000 | Loss: 0.00001103
Iteration 16/1000 | Loss: 0.00001101
Iteration 17/1000 | Loss: 0.00001100
Iteration 18/1000 | Loss: 0.00001100
Iteration 19/1000 | Loss: 0.00001097
Iteration 20/1000 | Loss: 0.00001097
Iteration 21/1000 | Loss: 0.00001091
Iteration 22/1000 | Loss: 0.00001090
Iteration 23/1000 | Loss: 0.00001089
Iteration 24/1000 | Loss: 0.00001087
Iteration 25/1000 | Loss: 0.00001082
Iteration 26/1000 | Loss: 0.00001078
Iteration 27/1000 | Loss: 0.00001076
Iteration 28/1000 | Loss: 0.00001076
Iteration 29/1000 | Loss: 0.00001074
Iteration 30/1000 | Loss: 0.00001072
Iteration 31/1000 | Loss: 0.00001071
Iteration 32/1000 | Loss: 0.00001071
Iteration 33/1000 | Loss: 0.00001071
Iteration 34/1000 | Loss: 0.00001071
Iteration 35/1000 | Loss: 0.00001070
Iteration 36/1000 | Loss: 0.00001070
Iteration 37/1000 | Loss: 0.00001070
Iteration 38/1000 | Loss: 0.00001069
Iteration 39/1000 | Loss: 0.00001069
Iteration 40/1000 | Loss: 0.00001068
Iteration 41/1000 | Loss: 0.00001067
Iteration 42/1000 | Loss: 0.00001067
Iteration 43/1000 | Loss: 0.00001066
Iteration 44/1000 | Loss: 0.00001066
Iteration 45/1000 | Loss: 0.00001066
Iteration 46/1000 | Loss: 0.00001066
Iteration 47/1000 | Loss: 0.00001066
Iteration 48/1000 | Loss: 0.00001065
Iteration 49/1000 | Loss: 0.00001065
Iteration 50/1000 | Loss: 0.00001065
Iteration 51/1000 | Loss: 0.00001065
Iteration 52/1000 | Loss: 0.00001065
Iteration 53/1000 | Loss: 0.00001065
Iteration 54/1000 | Loss: 0.00001064
Iteration 55/1000 | Loss: 0.00001064
Iteration 56/1000 | Loss: 0.00001064
Iteration 57/1000 | Loss: 0.00001063
Iteration 58/1000 | Loss: 0.00001062
Iteration 59/1000 | Loss: 0.00001062
Iteration 60/1000 | Loss: 0.00001061
Iteration 61/1000 | Loss: 0.00001061
Iteration 62/1000 | Loss: 0.00001061
Iteration 63/1000 | Loss: 0.00001061
Iteration 64/1000 | Loss: 0.00001060
Iteration 65/1000 | Loss: 0.00001060
Iteration 66/1000 | Loss: 0.00001060
Iteration 67/1000 | Loss: 0.00001060
Iteration 68/1000 | Loss: 0.00001060
Iteration 69/1000 | Loss: 0.00001060
Iteration 70/1000 | Loss: 0.00001059
Iteration 71/1000 | Loss: 0.00001059
Iteration 72/1000 | Loss: 0.00001059
Iteration 73/1000 | Loss: 0.00001059
Iteration 74/1000 | Loss: 0.00001059
Iteration 75/1000 | Loss: 0.00001058
Iteration 76/1000 | Loss: 0.00001058
Iteration 77/1000 | Loss: 0.00001058
Iteration 78/1000 | Loss: 0.00001058
Iteration 79/1000 | Loss: 0.00001058
Iteration 80/1000 | Loss: 0.00001058
Iteration 81/1000 | Loss: 0.00001058
Iteration 82/1000 | Loss: 0.00001058
Iteration 83/1000 | Loss: 0.00001057
Iteration 84/1000 | Loss: 0.00001057
Iteration 85/1000 | Loss: 0.00001057
Iteration 86/1000 | Loss: 0.00001056
Iteration 87/1000 | Loss: 0.00001056
Iteration 88/1000 | Loss: 0.00001056
Iteration 89/1000 | Loss: 0.00001056
Iteration 90/1000 | Loss: 0.00001056
Iteration 91/1000 | Loss: 0.00001056
Iteration 92/1000 | Loss: 0.00001056
Iteration 93/1000 | Loss: 0.00001056
Iteration 94/1000 | Loss: 0.00001055
Iteration 95/1000 | Loss: 0.00001054
Iteration 96/1000 | Loss: 0.00001054
Iteration 97/1000 | Loss: 0.00001054
Iteration 98/1000 | Loss: 0.00001054
Iteration 99/1000 | Loss: 0.00001054
Iteration 100/1000 | Loss: 0.00001054
Iteration 101/1000 | Loss: 0.00001054
Iteration 102/1000 | Loss: 0.00001054
Iteration 103/1000 | Loss: 0.00001054
Iteration 104/1000 | Loss: 0.00001054
Iteration 105/1000 | Loss: 0.00001053
Iteration 106/1000 | Loss: 0.00001053
Iteration 107/1000 | Loss: 0.00001053
Iteration 108/1000 | Loss: 0.00001053
Iteration 109/1000 | Loss: 0.00001053
Iteration 110/1000 | Loss: 0.00001053
Iteration 111/1000 | Loss: 0.00001053
Iteration 112/1000 | Loss: 0.00001052
Iteration 113/1000 | Loss: 0.00001052
Iteration 114/1000 | Loss: 0.00001052
Iteration 115/1000 | Loss: 0.00001052
Iteration 116/1000 | Loss: 0.00001052
Iteration 117/1000 | Loss: 0.00001052
Iteration 118/1000 | Loss: 0.00001051
Iteration 119/1000 | Loss: 0.00001051
Iteration 120/1000 | Loss: 0.00001051
Iteration 121/1000 | Loss: 0.00001050
Iteration 122/1000 | Loss: 0.00001050
Iteration 123/1000 | Loss: 0.00001049
Iteration 124/1000 | Loss: 0.00001049
Iteration 125/1000 | Loss: 0.00001048
Iteration 126/1000 | Loss: 0.00001048
Iteration 127/1000 | Loss: 0.00001048
Iteration 128/1000 | Loss: 0.00001047
Iteration 129/1000 | Loss: 0.00001047
Iteration 130/1000 | Loss: 0.00001047
Iteration 131/1000 | Loss: 0.00001046
Iteration 132/1000 | Loss: 0.00001046
Iteration 133/1000 | Loss: 0.00001046
Iteration 134/1000 | Loss: 0.00001045
Iteration 135/1000 | Loss: 0.00001045
Iteration 136/1000 | Loss: 0.00001044
Iteration 137/1000 | Loss: 0.00001044
Iteration 138/1000 | Loss: 0.00001044
Iteration 139/1000 | Loss: 0.00001044
Iteration 140/1000 | Loss: 0.00001043
Iteration 141/1000 | Loss: 0.00001043
Iteration 142/1000 | Loss: 0.00001043
Iteration 143/1000 | Loss: 0.00001043
Iteration 144/1000 | Loss: 0.00001043
Iteration 145/1000 | Loss: 0.00001042
Iteration 146/1000 | Loss: 0.00001042
Iteration 147/1000 | Loss: 0.00001042
Iteration 148/1000 | Loss: 0.00001041
Iteration 149/1000 | Loss: 0.00001041
Iteration 150/1000 | Loss: 0.00001041
Iteration 151/1000 | Loss: 0.00001041
Iteration 152/1000 | Loss: 0.00001041
Iteration 153/1000 | Loss: 0.00001041
Iteration 154/1000 | Loss: 0.00001041
Iteration 155/1000 | Loss: 0.00001041
Iteration 156/1000 | Loss: 0.00001041
Iteration 157/1000 | Loss: 0.00001040
Iteration 158/1000 | Loss: 0.00001040
Iteration 159/1000 | Loss: 0.00001040
Iteration 160/1000 | Loss: 0.00001040
Iteration 161/1000 | Loss: 0.00001040
Iteration 162/1000 | Loss: 0.00001040
Iteration 163/1000 | Loss: 0.00001040
Iteration 164/1000 | Loss: 0.00001040
Iteration 165/1000 | Loss: 0.00001040
Iteration 166/1000 | Loss: 0.00001040
Iteration 167/1000 | Loss: 0.00001040
Iteration 168/1000 | Loss: 0.00001039
Iteration 169/1000 | Loss: 0.00001039
Iteration 170/1000 | Loss: 0.00001039
Iteration 171/1000 | Loss: 0.00001039
Iteration 172/1000 | Loss: 0.00001039
Iteration 173/1000 | Loss: 0.00001039
Iteration 174/1000 | Loss: 0.00001039
Iteration 175/1000 | Loss: 0.00001039
Iteration 176/1000 | Loss: 0.00001039
Iteration 177/1000 | Loss: 0.00001039
Iteration 178/1000 | Loss: 0.00001039
Iteration 179/1000 | Loss: 0.00001039
Iteration 180/1000 | Loss: 0.00001039
Iteration 181/1000 | Loss: 0.00001039
Iteration 182/1000 | Loss: 0.00001039
Iteration 183/1000 | Loss: 0.00001039
Iteration 184/1000 | Loss: 0.00001038
Iteration 185/1000 | Loss: 0.00001038
Iteration 186/1000 | Loss: 0.00001038
Iteration 187/1000 | Loss: 0.00001038
Iteration 188/1000 | Loss: 0.00001038
Iteration 189/1000 | Loss: 0.00001038
Iteration 190/1000 | Loss: 0.00001038
Iteration 191/1000 | Loss: 0.00001037
Iteration 192/1000 | Loss: 0.00001037
Iteration 193/1000 | Loss: 0.00001037
Iteration 194/1000 | Loss: 0.00001037
Iteration 195/1000 | Loss: 0.00001037
Iteration 196/1000 | Loss: 0.00001037
Iteration 197/1000 | Loss: 0.00001037
Iteration 198/1000 | Loss: 0.00001037
Iteration 199/1000 | Loss: 0.00001037
Iteration 200/1000 | Loss: 0.00001036
Iteration 201/1000 | Loss: 0.00001036
Iteration 202/1000 | Loss: 0.00001036
Iteration 203/1000 | Loss: 0.00001036
Iteration 204/1000 | Loss: 0.00001036
Iteration 205/1000 | Loss: 0.00001036
Iteration 206/1000 | Loss: 0.00001036
Iteration 207/1000 | Loss: 0.00001036
Iteration 208/1000 | Loss: 0.00001036
Iteration 209/1000 | Loss: 0.00001036
Iteration 210/1000 | Loss: 0.00001036
Iteration 211/1000 | Loss: 0.00001036
Iteration 212/1000 | Loss: 0.00001036
Iteration 213/1000 | Loss: 0.00001036
Iteration 214/1000 | Loss: 0.00001035
Iteration 215/1000 | Loss: 0.00001035
Iteration 216/1000 | Loss: 0.00001035
Iteration 217/1000 | Loss: 0.00001035
Iteration 218/1000 | Loss: 0.00001035
Iteration 219/1000 | Loss: 0.00001035
Iteration 220/1000 | Loss: 0.00001035
Iteration 221/1000 | Loss: 0.00001034
Iteration 222/1000 | Loss: 0.00001034
Iteration 223/1000 | Loss: 0.00001034
Iteration 224/1000 | Loss: 0.00001034
Iteration 225/1000 | Loss: 0.00001034
Iteration 226/1000 | Loss: 0.00001034
Iteration 227/1000 | Loss: 0.00001034
Iteration 228/1000 | Loss: 0.00001034
Iteration 229/1000 | Loss: 0.00001034
Iteration 230/1000 | Loss: 0.00001034
Iteration 231/1000 | Loss: 0.00001034
Iteration 232/1000 | Loss: 0.00001034
Iteration 233/1000 | Loss: 0.00001034
Iteration 234/1000 | Loss: 0.00001034
Iteration 235/1000 | Loss: 0.00001034
Iteration 236/1000 | Loss: 0.00001034
Iteration 237/1000 | Loss: 0.00001034
Iteration 238/1000 | Loss: 0.00001034
Iteration 239/1000 | Loss: 0.00001034
Iteration 240/1000 | Loss: 0.00001034
Iteration 241/1000 | Loss: 0.00001034
Iteration 242/1000 | Loss: 0.00001034
Iteration 243/1000 | Loss: 0.00001033
Iteration 244/1000 | Loss: 0.00001033
Iteration 245/1000 | Loss: 0.00001033
Iteration 246/1000 | Loss: 0.00001033
Iteration 247/1000 | Loss: 0.00001033
Iteration 248/1000 | Loss: 0.00001033
Iteration 249/1000 | Loss: 0.00001033
Iteration 250/1000 | Loss: 0.00001032
Iteration 251/1000 | Loss: 0.00001032
Iteration 252/1000 | Loss: 0.00001032
Iteration 253/1000 | Loss: 0.00001032
Iteration 254/1000 | Loss: 0.00001032
Iteration 255/1000 | Loss: 0.00001032
Iteration 256/1000 | Loss: 0.00001032
Iteration 257/1000 | Loss: 0.00001032
Iteration 258/1000 | Loss: 0.00001032
Iteration 259/1000 | Loss: 0.00001032
Iteration 260/1000 | Loss: 0.00001032
Iteration 261/1000 | Loss: 0.00001032
Iteration 262/1000 | Loss: 0.00001032
Iteration 263/1000 | Loss: 0.00001032
Iteration 264/1000 | Loss: 0.00001032
Iteration 265/1000 | Loss: 0.00001032
Iteration 266/1000 | Loss: 0.00001032
Iteration 267/1000 | Loss: 0.00001032
Iteration 268/1000 | Loss: 0.00001032
Iteration 269/1000 | Loss: 0.00001032
Iteration 270/1000 | Loss: 0.00001032
Iteration 271/1000 | Loss: 0.00001031
Iteration 272/1000 | Loss: 0.00001031
Iteration 273/1000 | Loss: 0.00001031
Iteration 274/1000 | Loss: 0.00001031
Iteration 275/1000 | Loss: 0.00001031
Iteration 276/1000 | Loss: 0.00001031
Iteration 277/1000 | Loss: 0.00001031
Iteration 278/1000 | Loss: 0.00001031
Iteration 279/1000 | Loss: 0.00001031
Iteration 280/1000 | Loss: 0.00001031
Iteration 281/1000 | Loss: 0.00001031
Iteration 282/1000 | Loss: 0.00001031
Iteration 283/1000 | Loss: 0.00001031
Iteration 284/1000 | Loss: 0.00001031
Iteration 285/1000 | Loss: 0.00001031
Iteration 286/1000 | Loss: 0.00001031
Iteration 287/1000 | Loss: 0.00001031
Iteration 288/1000 | Loss: 0.00001031
Iteration 289/1000 | Loss: 0.00001031
Iteration 290/1000 | Loss: 0.00001031
Iteration 291/1000 | Loss: 0.00001031
Iteration 292/1000 | Loss: 0.00001031
Iteration 293/1000 | Loss: 0.00001031
Iteration 294/1000 | Loss: 0.00001031
Iteration 295/1000 | Loss: 0.00001031
Iteration 296/1000 | Loss: 0.00001031
Iteration 297/1000 | Loss: 0.00001031
Iteration 298/1000 | Loss: 0.00001031
Iteration 299/1000 | Loss: 0.00001031
Iteration 300/1000 | Loss: 0.00001031
Iteration 301/1000 | Loss: 0.00001031
Iteration 302/1000 | Loss: 0.00001031
Iteration 303/1000 | Loss: 0.00001031
Iteration 304/1000 | Loss: 0.00001031
Iteration 305/1000 | Loss: 0.00001031
Iteration 306/1000 | Loss: 0.00001031
Iteration 307/1000 | Loss: 0.00001031
Iteration 308/1000 | Loss: 0.00001031
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 308. Stopping optimization.
Last 5 losses: [1.0308407581760548e-05, 1.0308407581760548e-05, 1.0308407581760548e-05, 1.0308407581760548e-05, 1.0308407581760548e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0308407581760548e-05

Optimization complete. Final v2v error: 2.7549479007720947 mm

Highest mean error: 3.6798181533813477 mm for frame 74

Lowest mean error: 2.4696388244628906 mm for frame 96

Saving results

Total time: 47.689335346221924
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00965861
Iteration 2/25 | Loss: 0.00163483
Iteration 3/25 | Loss: 0.00129780
Iteration 4/25 | Loss: 0.00126716
Iteration 5/25 | Loss: 0.00125843
Iteration 6/25 | Loss: 0.00125709
Iteration 7/25 | Loss: 0.00125709
Iteration 8/25 | Loss: 0.00125709
Iteration 9/25 | Loss: 0.00125709
Iteration 10/25 | Loss: 0.00125709
Iteration 11/25 | Loss: 0.00125709
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012570894323289394, 0.0012570894323289394, 0.0012570894323289394, 0.0012570894323289394, 0.0012570894323289394]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012570894323289394

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.89490247
Iteration 2/25 | Loss: 0.00101919
Iteration 3/25 | Loss: 0.00101918
Iteration 4/25 | Loss: 0.00101918
Iteration 5/25 | Loss: 0.00101918
Iteration 6/25 | Loss: 0.00101918
Iteration 7/25 | Loss: 0.00101918
Iteration 8/25 | Loss: 0.00101918
Iteration 9/25 | Loss: 0.00101918
Iteration 10/25 | Loss: 0.00101918
Iteration 11/25 | Loss: 0.00101918
Iteration 12/25 | Loss: 0.00101918
Iteration 13/25 | Loss: 0.00101918
Iteration 14/25 | Loss: 0.00101918
Iteration 15/25 | Loss: 0.00101918
Iteration 16/25 | Loss: 0.00101918
Iteration 17/25 | Loss: 0.00101918
Iteration 18/25 | Loss: 0.00101918
Iteration 19/25 | Loss: 0.00101918
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010191785404458642, 0.0010191785404458642, 0.0010191785404458642, 0.0010191785404458642, 0.0010191785404458642]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010191785404458642

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101918
Iteration 2/1000 | Loss: 0.00004719
Iteration 3/1000 | Loss: 0.00002786
Iteration 4/1000 | Loss: 0.00002520
Iteration 5/1000 | Loss: 0.00002385
Iteration 6/1000 | Loss: 0.00002241
Iteration 7/1000 | Loss: 0.00002161
Iteration 8/1000 | Loss: 0.00002121
Iteration 9/1000 | Loss: 0.00002088
Iteration 10/1000 | Loss: 0.00002066
Iteration 11/1000 | Loss: 0.00002061
Iteration 12/1000 | Loss: 0.00002056
Iteration 13/1000 | Loss: 0.00002054
Iteration 14/1000 | Loss: 0.00002053
Iteration 15/1000 | Loss: 0.00002040
Iteration 16/1000 | Loss: 0.00002040
Iteration 17/1000 | Loss: 0.00002039
Iteration 18/1000 | Loss: 0.00002038
Iteration 19/1000 | Loss: 0.00002038
Iteration 20/1000 | Loss: 0.00002038
Iteration 21/1000 | Loss: 0.00002037
Iteration 22/1000 | Loss: 0.00002036
Iteration 23/1000 | Loss: 0.00002035
Iteration 24/1000 | Loss: 0.00002034
Iteration 25/1000 | Loss: 0.00002033
Iteration 26/1000 | Loss: 0.00002032
Iteration 27/1000 | Loss: 0.00002032
Iteration 28/1000 | Loss: 0.00002032
Iteration 29/1000 | Loss: 0.00002031
Iteration 30/1000 | Loss: 0.00002031
Iteration 31/1000 | Loss: 0.00002031
Iteration 32/1000 | Loss: 0.00002031
Iteration 33/1000 | Loss: 0.00002030
Iteration 34/1000 | Loss: 0.00002028
Iteration 35/1000 | Loss: 0.00002028
Iteration 36/1000 | Loss: 0.00002026
Iteration 37/1000 | Loss: 0.00002024
Iteration 38/1000 | Loss: 0.00002024
Iteration 39/1000 | Loss: 0.00002023
Iteration 40/1000 | Loss: 0.00002023
Iteration 41/1000 | Loss: 0.00002023
Iteration 42/1000 | Loss: 0.00002023
Iteration 43/1000 | Loss: 0.00002023
Iteration 44/1000 | Loss: 0.00002023
Iteration 45/1000 | Loss: 0.00002023
Iteration 46/1000 | Loss: 0.00002023
Iteration 47/1000 | Loss: 0.00002023
Iteration 48/1000 | Loss: 0.00002023
Iteration 49/1000 | Loss: 0.00002023
Iteration 50/1000 | Loss: 0.00002022
Iteration 51/1000 | Loss: 0.00002022
Iteration 52/1000 | Loss: 0.00002021
Iteration 53/1000 | Loss: 0.00002021
Iteration 54/1000 | Loss: 0.00002021
Iteration 55/1000 | Loss: 0.00002020
Iteration 56/1000 | Loss: 0.00002020
Iteration 57/1000 | Loss: 0.00002020
Iteration 58/1000 | Loss: 0.00002020
Iteration 59/1000 | Loss: 0.00002020
Iteration 60/1000 | Loss: 0.00002020
Iteration 61/1000 | Loss: 0.00002020
Iteration 62/1000 | Loss: 0.00002020
Iteration 63/1000 | Loss: 0.00002020
Iteration 64/1000 | Loss: 0.00002019
Iteration 65/1000 | Loss: 0.00002019
Iteration 66/1000 | Loss: 0.00002019
Iteration 67/1000 | Loss: 0.00002019
Iteration 68/1000 | Loss: 0.00002019
Iteration 69/1000 | Loss: 0.00002019
Iteration 70/1000 | Loss: 0.00002019
Iteration 71/1000 | Loss: 0.00002019
Iteration 72/1000 | Loss: 0.00002019
Iteration 73/1000 | Loss: 0.00002019
Iteration 74/1000 | Loss: 0.00002019
Iteration 75/1000 | Loss: 0.00002019
Iteration 76/1000 | Loss: 0.00002019
Iteration 77/1000 | Loss: 0.00002019
Iteration 78/1000 | Loss: 0.00002019
Iteration 79/1000 | Loss: 0.00002018
Iteration 80/1000 | Loss: 0.00002018
Iteration 81/1000 | Loss: 0.00002018
Iteration 82/1000 | Loss: 0.00002018
Iteration 83/1000 | Loss: 0.00002018
Iteration 84/1000 | Loss: 0.00002018
Iteration 85/1000 | Loss: 0.00002018
Iteration 86/1000 | Loss: 0.00002018
Iteration 87/1000 | Loss: 0.00002018
Iteration 88/1000 | Loss: 0.00002018
Iteration 89/1000 | Loss: 0.00002018
Iteration 90/1000 | Loss: 0.00002018
Iteration 91/1000 | Loss: 0.00002018
Iteration 92/1000 | Loss: 0.00002018
Iteration 93/1000 | Loss: 0.00002018
Iteration 94/1000 | Loss: 0.00002018
Iteration 95/1000 | Loss: 0.00002018
Iteration 96/1000 | Loss: 0.00002018
Iteration 97/1000 | Loss: 0.00002018
Iteration 98/1000 | Loss: 0.00002018
Iteration 99/1000 | Loss: 0.00002018
Iteration 100/1000 | Loss: 0.00002018
Iteration 101/1000 | Loss: 0.00002018
Iteration 102/1000 | Loss: 0.00002018
Iteration 103/1000 | Loss: 0.00002018
Iteration 104/1000 | Loss: 0.00002018
Iteration 105/1000 | Loss: 0.00002018
Iteration 106/1000 | Loss: 0.00002018
Iteration 107/1000 | Loss: 0.00002018
Iteration 108/1000 | Loss: 0.00002018
Iteration 109/1000 | Loss: 0.00002018
Iteration 110/1000 | Loss: 0.00002018
Iteration 111/1000 | Loss: 0.00002018
Iteration 112/1000 | Loss: 0.00002018
Iteration 113/1000 | Loss: 0.00002018
Iteration 114/1000 | Loss: 0.00002018
Iteration 115/1000 | Loss: 0.00002018
Iteration 116/1000 | Loss: 0.00002018
Iteration 117/1000 | Loss: 0.00002018
Iteration 118/1000 | Loss: 0.00002018
Iteration 119/1000 | Loss: 0.00002018
Iteration 120/1000 | Loss: 0.00002018
Iteration 121/1000 | Loss: 0.00002018
Iteration 122/1000 | Loss: 0.00002018
Iteration 123/1000 | Loss: 0.00002018
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [2.0175744793959893e-05, 2.0175744793959893e-05, 2.0175744793959893e-05, 2.0175744793959893e-05, 2.0175744793959893e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0175744793959893e-05

Optimization complete. Final v2v error: 3.830787181854248 mm

Highest mean error: 4.19036865234375 mm for frame 117

Lowest mean error: 3.6341516971588135 mm for frame 10

Saving results

Total time: 31.73134160041809
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01023451
Iteration 2/25 | Loss: 0.00208629
Iteration 3/25 | Loss: 0.00154975
Iteration 4/25 | Loss: 0.00157140
Iteration 5/25 | Loss: 0.00145984
Iteration 6/25 | Loss: 0.00143914
Iteration 7/25 | Loss: 0.00147292
Iteration 8/25 | Loss: 0.00139055
Iteration 9/25 | Loss: 0.00133456
Iteration 10/25 | Loss: 0.00129909
Iteration 11/25 | Loss: 0.00127203
Iteration 12/25 | Loss: 0.00126453
Iteration 13/25 | Loss: 0.00125391
Iteration 14/25 | Loss: 0.00125003
Iteration 15/25 | Loss: 0.00124822
Iteration 16/25 | Loss: 0.00124260
Iteration 17/25 | Loss: 0.00124113
Iteration 18/25 | Loss: 0.00124380
Iteration 19/25 | Loss: 0.00124276
Iteration 20/25 | Loss: 0.00124189
Iteration 21/25 | Loss: 0.00123790
Iteration 22/25 | Loss: 0.00127018
Iteration 23/25 | Loss: 0.00123144
Iteration 24/25 | Loss: 0.00123321
Iteration 25/25 | Loss: 0.00122790

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58362365
Iteration 2/25 | Loss: 0.00185378
Iteration 3/25 | Loss: 0.00185378
Iteration 4/25 | Loss: 0.00185378
Iteration 5/25 | Loss: 0.00185378
Iteration 6/25 | Loss: 0.00185378
Iteration 7/25 | Loss: 0.00185378
Iteration 8/25 | Loss: 0.00185378
Iteration 9/25 | Loss: 0.00185378
Iteration 10/25 | Loss: 0.00185378
Iteration 11/25 | Loss: 0.00185378
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0018537804717198014, 0.0018537804717198014, 0.0018537804717198014, 0.0018537804717198014, 0.0018537804717198014]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018537804717198014

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00185378
Iteration 2/1000 | Loss: 0.00031887
Iteration 3/1000 | Loss: 0.00029065
Iteration 4/1000 | Loss: 0.00032884
Iteration 5/1000 | Loss: 0.00038287
Iteration 6/1000 | Loss: 0.00034395
Iteration 7/1000 | Loss: 0.00027001
Iteration 8/1000 | Loss: 0.00018076
Iteration 9/1000 | Loss: 0.00024061
Iteration 10/1000 | Loss: 0.00025343
Iteration 11/1000 | Loss: 0.00041197
Iteration 12/1000 | Loss: 0.00041898
Iteration 13/1000 | Loss: 0.00039274
Iteration 14/1000 | Loss: 0.00024683
Iteration 15/1000 | Loss: 0.00008615
Iteration 16/1000 | Loss: 0.00017796
Iteration 17/1000 | Loss: 0.00014951
Iteration 18/1000 | Loss: 0.00016473
Iteration 19/1000 | Loss: 0.00016287
Iteration 20/1000 | Loss: 0.00005287
Iteration 21/1000 | Loss: 0.00044820
Iteration 22/1000 | Loss: 0.00040792
Iteration 23/1000 | Loss: 0.00013908
Iteration 24/1000 | Loss: 0.00039766
Iteration 25/1000 | Loss: 0.00014692
Iteration 26/1000 | Loss: 0.00029469
Iteration 27/1000 | Loss: 0.00038603
Iteration 28/1000 | Loss: 0.00027267
Iteration 29/1000 | Loss: 0.00006512
Iteration 30/1000 | Loss: 0.00014493
Iteration 31/1000 | Loss: 0.00024055
Iteration 32/1000 | Loss: 0.00017835
Iteration 33/1000 | Loss: 0.00009297
Iteration 34/1000 | Loss: 0.00003691
Iteration 35/1000 | Loss: 0.00003394
Iteration 36/1000 | Loss: 0.00003257
Iteration 37/1000 | Loss: 0.00017399
Iteration 38/1000 | Loss: 0.00019800
Iteration 39/1000 | Loss: 0.00017855
Iteration 40/1000 | Loss: 0.00016319
Iteration 41/1000 | Loss: 0.00016262
Iteration 42/1000 | Loss: 0.00003596
Iteration 43/1000 | Loss: 0.00003251
Iteration 44/1000 | Loss: 0.00003118
Iteration 45/1000 | Loss: 0.00029334
Iteration 46/1000 | Loss: 0.00005260
Iteration 47/1000 | Loss: 0.00021680
Iteration 48/1000 | Loss: 0.00024411
Iteration 49/1000 | Loss: 0.00016517
Iteration 50/1000 | Loss: 0.00018054
Iteration 51/1000 | Loss: 0.00013103
Iteration 52/1000 | Loss: 0.00058791
Iteration 53/1000 | Loss: 0.00004208
Iteration 54/1000 | Loss: 0.00006748
Iteration 55/1000 | Loss: 0.00004732
Iteration 56/1000 | Loss: 0.00003176
Iteration 57/1000 | Loss: 0.00003018
Iteration 58/1000 | Loss: 0.00002873
Iteration 59/1000 | Loss: 0.00033730
Iteration 60/1000 | Loss: 0.00049448
Iteration 61/1000 | Loss: 0.00034274
Iteration 62/1000 | Loss: 0.00021841
Iteration 63/1000 | Loss: 0.00034968
Iteration 64/1000 | Loss: 0.00002568
Iteration 65/1000 | Loss: 0.00002335
Iteration 66/1000 | Loss: 0.00002142
Iteration 67/1000 | Loss: 0.00002001
Iteration 68/1000 | Loss: 0.00001925
Iteration 69/1000 | Loss: 0.00038358
Iteration 70/1000 | Loss: 0.00001975
Iteration 71/1000 | Loss: 0.00001807
Iteration 72/1000 | Loss: 0.00001688
Iteration 73/1000 | Loss: 0.00001600
Iteration 74/1000 | Loss: 0.00001551
Iteration 75/1000 | Loss: 0.00001535
Iteration 76/1000 | Loss: 0.00001511
Iteration 77/1000 | Loss: 0.00001507
Iteration 78/1000 | Loss: 0.00001502
Iteration 79/1000 | Loss: 0.00036919
Iteration 80/1000 | Loss: 0.00001551
Iteration 81/1000 | Loss: 0.00001427
Iteration 82/1000 | Loss: 0.00001353
Iteration 83/1000 | Loss: 0.00001287
Iteration 84/1000 | Loss: 0.00001249
Iteration 85/1000 | Loss: 0.00001234
Iteration 86/1000 | Loss: 0.00001220
Iteration 87/1000 | Loss: 0.00001214
Iteration 88/1000 | Loss: 0.00001212
Iteration 89/1000 | Loss: 0.00001211
Iteration 90/1000 | Loss: 0.00001211
Iteration 91/1000 | Loss: 0.00001210
Iteration 92/1000 | Loss: 0.00001210
Iteration 93/1000 | Loss: 0.00001209
Iteration 94/1000 | Loss: 0.00001208
Iteration 95/1000 | Loss: 0.00001207
Iteration 96/1000 | Loss: 0.00001207
Iteration 97/1000 | Loss: 0.00001206
Iteration 98/1000 | Loss: 0.00001205
Iteration 99/1000 | Loss: 0.00001205
Iteration 100/1000 | Loss: 0.00001205
Iteration 101/1000 | Loss: 0.00001204
Iteration 102/1000 | Loss: 0.00001204
Iteration 103/1000 | Loss: 0.00001204
Iteration 104/1000 | Loss: 0.00001204
Iteration 105/1000 | Loss: 0.00001204
Iteration 106/1000 | Loss: 0.00001204
Iteration 107/1000 | Loss: 0.00001204
Iteration 108/1000 | Loss: 0.00001204
Iteration 109/1000 | Loss: 0.00001203
Iteration 110/1000 | Loss: 0.00001203
Iteration 111/1000 | Loss: 0.00001203
Iteration 112/1000 | Loss: 0.00001203
Iteration 113/1000 | Loss: 0.00001203
Iteration 114/1000 | Loss: 0.00001203
Iteration 115/1000 | Loss: 0.00001203
Iteration 116/1000 | Loss: 0.00001202
Iteration 117/1000 | Loss: 0.00001202
Iteration 118/1000 | Loss: 0.00001202
Iteration 119/1000 | Loss: 0.00001202
Iteration 120/1000 | Loss: 0.00001202
Iteration 121/1000 | Loss: 0.00001202
Iteration 122/1000 | Loss: 0.00001202
Iteration 123/1000 | Loss: 0.00001201
Iteration 124/1000 | Loss: 0.00001201
Iteration 125/1000 | Loss: 0.00001201
Iteration 126/1000 | Loss: 0.00001201
Iteration 127/1000 | Loss: 0.00001201
Iteration 128/1000 | Loss: 0.00001201
Iteration 129/1000 | Loss: 0.00001200
Iteration 130/1000 | Loss: 0.00001200
Iteration 131/1000 | Loss: 0.00001200
Iteration 132/1000 | Loss: 0.00001200
Iteration 133/1000 | Loss: 0.00001200
Iteration 134/1000 | Loss: 0.00001200
Iteration 135/1000 | Loss: 0.00001199
Iteration 136/1000 | Loss: 0.00001199
Iteration 137/1000 | Loss: 0.00001199
Iteration 138/1000 | Loss: 0.00001199
Iteration 139/1000 | Loss: 0.00001199
Iteration 140/1000 | Loss: 0.00001198
Iteration 141/1000 | Loss: 0.00001198
Iteration 142/1000 | Loss: 0.00001198
Iteration 143/1000 | Loss: 0.00001198
Iteration 144/1000 | Loss: 0.00001198
Iteration 145/1000 | Loss: 0.00001198
Iteration 146/1000 | Loss: 0.00001197
Iteration 147/1000 | Loss: 0.00001197
Iteration 148/1000 | Loss: 0.00001197
Iteration 149/1000 | Loss: 0.00001197
Iteration 150/1000 | Loss: 0.00001197
Iteration 151/1000 | Loss: 0.00001196
Iteration 152/1000 | Loss: 0.00001196
Iteration 153/1000 | Loss: 0.00001196
Iteration 154/1000 | Loss: 0.00001196
Iteration 155/1000 | Loss: 0.00001196
Iteration 156/1000 | Loss: 0.00001196
Iteration 157/1000 | Loss: 0.00001196
Iteration 158/1000 | Loss: 0.00001195
Iteration 159/1000 | Loss: 0.00001195
Iteration 160/1000 | Loss: 0.00001195
Iteration 161/1000 | Loss: 0.00001195
Iteration 162/1000 | Loss: 0.00001195
Iteration 163/1000 | Loss: 0.00001195
Iteration 164/1000 | Loss: 0.00001195
Iteration 165/1000 | Loss: 0.00001195
Iteration 166/1000 | Loss: 0.00001195
Iteration 167/1000 | Loss: 0.00001195
Iteration 168/1000 | Loss: 0.00001194
Iteration 169/1000 | Loss: 0.00001194
Iteration 170/1000 | Loss: 0.00001194
Iteration 171/1000 | Loss: 0.00001194
Iteration 172/1000 | Loss: 0.00001194
Iteration 173/1000 | Loss: 0.00001194
Iteration 174/1000 | Loss: 0.00001194
Iteration 175/1000 | Loss: 0.00001194
Iteration 176/1000 | Loss: 0.00001194
Iteration 177/1000 | Loss: 0.00001194
Iteration 178/1000 | Loss: 0.00001194
Iteration 179/1000 | Loss: 0.00001193
Iteration 180/1000 | Loss: 0.00001193
Iteration 181/1000 | Loss: 0.00001193
Iteration 182/1000 | Loss: 0.00001193
Iteration 183/1000 | Loss: 0.00001193
Iteration 184/1000 | Loss: 0.00001193
Iteration 185/1000 | Loss: 0.00001193
Iteration 186/1000 | Loss: 0.00001193
Iteration 187/1000 | Loss: 0.00001193
Iteration 188/1000 | Loss: 0.00001193
Iteration 189/1000 | Loss: 0.00001193
Iteration 190/1000 | Loss: 0.00001192
Iteration 191/1000 | Loss: 0.00001192
Iteration 192/1000 | Loss: 0.00001192
Iteration 193/1000 | Loss: 0.00001192
Iteration 194/1000 | Loss: 0.00001191
Iteration 195/1000 | Loss: 0.00001191
Iteration 196/1000 | Loss: 0.00001191
Iteration 197/1000 | Loss: 0.00001191
Iteration 198/1000 | Loss: 0.00001191
Iteration 199/1000 | Loss: 0.00001191
Iteration 200/1000 | Loss: 0.00001191
Iteration 201/1000 | Loss: 0.00001190
Iteration 202/1000 | Loss: 0.00001190
Iteration 203/1000 | Loss: 0.00001190
Iteration 204/1000 | Loss: 0.00001190
Iteration 205/1000 | Loss: 0.00001190
Iteration 206/1000 | Loss: 0.00001190
Iteration 207/1000 | Loss: 0.00001189
Iteration 208/1000 | Loss: 0.00001189
Iteration 209/1000 | Loss: 0.00001189
Iteration 210/1000 | Loss: 0.00001189
Iteration 211/1000 | Loss: 0.00001189
Iteration 212/1000 | Loss: 0.00001189
Iteration 213/1000 | Loss: 0.00001189
Iteration 214/1000 | Loss: 0.00001189
Iteration 215/1000 | Loss: 0.00001189
Iteration 216/1000 | Loss: 0.00001189
Iteration 217/1000 | Loss: 0.00001189
Iteration 218/1000 | Loss: 0.00001189
Iteration 219/1000 | Loss: 0.00001189
Iteration 220/1000 | Loss: 0.00001188
Iteration 221/1000 | Loss: 0.00001188
Iteration 222/1000 | Loss: 0.00001188
Iteration 223/1000 | Loss: 0.00001188
Iteration 224/1000 | Loss: 0.00001188
Iteration 225/1000 | Loss: 0.00001188
Iteration 226/1000 | Loss: 0.00001188
Iteration 227/1000 | Loss: 0.00001188
Iteration 228/1000 | Loss: 0.00001188
Iteration 229/1000 | Loss: 0.00001188
Iteration 230/1000 | Loss: 0.00001188
Iteration 231/1000 | Loss: 0.00001188
Iteration 232/1000 | Loss: 0.00001188
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 232. Stopping optimization.
Last 5 losses: [1.1883094884979073e-05, 1.1883094884979073e-05, 1.1883094884979073e-05, 1.1883094884979073e-05, 1.1883094884979073e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1883094884979073e-05

Optimization complete. Final v2v error: 2.9117250442504883 mm

Highest mean error: 5.950138092041016 mm for frame 93

Lowest mean error: 2.5487325191497803 mm for frame 20

Saving results

Total time: 172.89502477645874
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00628054
Iteration 2/25 | Loss: 0.00161918
Iteration 3/25 | Loss: 0.00133945
Iteration 4/25 | Loss: 0.00131784
Iteration 5/25 | Loss: 0.00131392
Iteration 6/25 | Loss: 0.00131322
Iteration 7/25 | Loss: 0.00131322
Iteration 8/25 | Loss: 0.00131322
Iteration 9/25 | Loss: 0.00131322
Iteration 10/25 | Loss: 0.00131322
Iteration 11/25 | Loss: 0.00131322
Iteration 12/25 | Loss: 0.00131322
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013132162857800722, 0.0013132162857800722, 0.0013132162857800722, 0.0013132162857800722, 0.0013132162857800722]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013132162857800722

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27126181
Iteration 2/25 | Loss: 0.00157338
Iteration 3/25 | Loss: 0.00157335
Iteration 4/25 | Loss: 0.00157335
Iteration 5/25 | Loss: 0.00157335
Iteration 6/25 | Loss: 0.00157335
Iteration 7/25 | Loss: 0.00157335
Iteration 8/25 | Loss: 0.00157335
Iteration 9/25 | Loss: 0.00157335
Iteration 10/25 | Loss: 0.00157335
Iteration 11/25 | Loss: 0.00157335
Iteration 12/25 | Loss: 0.00157335
Iteration 13/25 | Loss: 0.00157335
Iteration 14/25 | Loss: 0.00157335
Iteration 15/25 | Loss: 0.00157335
Iteration 16/25 | Loss: 0.00157335
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0015733472537249327, 0.0015733472537249327, 0.0015733472537249327, 0.0015733472537249327, 0.0015733472537249327]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015733472537249327

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00157335
Iteration 2/1000 | Loss: 0.00003683
Iteration 3/1000 | Loss: 0.00002724
Iteration 4/1000 | Loss: 0.00002502
Iteration 5/1000 | Loss: 0.00002391
Iteration 6/1000 | Loss: 0.00002326
Iteration 7/1000 | Loss: 0.00002280
Iteration 8/1000 | Loss: 0.00002242
Iteration 9/1000 | Loss: 0.00002201
Iteration 10/1000 | Loss: 0.00002177
Iteration 11/1000 | Loss: 0.00002156
Iteration 12/1000 | Loss: 0.00002134
Iteration 13/1000 | Loss: 0.00002112
Iteration 14/1000 | Loss: 0.00002095
Iteration 15/1000 | Loss: 0.00002080
Iteration 16/1000 | Loss: 0.00002079
Iteration 17/1000 | Loss: 0.00002078
Iteration 18/1000 | Loss: 0.00002076
Iteration 19/1000 | Loss: 0.00002075
Iteration 20/1000 | Loss: 0.00002075
Iteration 21/1000 | Loss: 0.00002075
Iteration 22/1000 | Loss: 0.00002067
Iteration 23/1000 | Loss: 0.00002062
Iteration 24/1000 | Loss: 0.00002058
Iteration 25/1000 | Loss: 0.00002058
Iteration 26/1000 | Loss: 0.00002057
Iteration 27/1000 | Loss: 0.00002057
Iteration 28/1000 | Loss: 0.00002056
Iteration 29/1000 | Loss: 0.00002056
Iteration 30/1000 | Loss: 0.00002056
Iteration 31/1000 | Loss: 0.00002055
Iteration 32/1000 | Loss: 0.00002052
Iteration 33/1000 | Loss: 0.00002052
Iteration 34/1000 | Loss: 0.00002052
Iteration 35/1000 | Loss: 0.00002051
Iteration 36/1000 | Loss: 0.00002050
Iteration 37/1000 | Loss: 0.00002049
Iteration 38/1000 | Loss: 0.00002049
Iteration 39/1000 | Loss: 0.00002048
Iteration 40/1000 | Loss: 0.00002048
Iteration 41/1000 | Loss: 0.00002047
Iteration 42/1000 | Loss: 0.00002045
Iteration 43/1000 | Loss: 0.00002045
Iteration 44/1000 | Loss: 0.00002045
Iteration 45/1000 | Loss: 0.00002044
Iteration 46/1000 | Loss: 0.00002044
Iteration 47/1000 | Loss: 0.00002044
Iteration 48/1000 | Loss: 0.00002044
Iteration 49/1000 | Loss: 0.00002044
Iteration 50/1000 | Loss: 0.00002044
Iteration 51/1000 | Loss: 0.00002044
Iteration 52/1000 | Loss: 0.00002043
Iteration 53/1000 | Loss: 0.00002043
Iteration 54/1000 | Loss: 0.00002041
Iteration 55/1000 | Loss: 0.00002041
Iteration 56/1000 | Loss: 0.00002041
Iteration 57/1000 | Loss: 0.00002041
Iteration 58/1000 | Loss: 0.00002041
Iteration 59/1000 | Loss: 0.00002041
Iteration 60/1000 | Loss: 0.00002041
Iteration 61/1000 | Loss: 0.00002041
Iteration 62/1000 | Loss: 0.00002041
Iteration 63/1000 | Loss: 0.00002041
Iteration 64/1000 | Loss: 0.00002041
Iteration 65/1000 | Loss: 0.00002041
Iteration 66/1000 | Loss: 0.00002041
Iteration 67/1000 | Loss: 0.00002040
Iteration 68/1000 | Loss: 0.00002040
Iteration 69/1000 | Loss: 0.00002040
Iteration 70/1000 | Loss: 0.00002039
Iteration 71/1000 | Loss: 0.00002039
Iteration 72/1000 | Loss: 0.00002039
Iteration 73/1000 | Loss: 0.00002039
Iteration 74/1000 | Loss: 0.00002039
Iteration 75/1000 | Loss: 0.00002039
Iteration 76/1000 | Loss: 0.00002039
Iteration 77/1000 | Loss: 0.00002039
Iteration 78/1000 | Loss: 0.00002039
Iteration 79/1000 | Loss: 0.00002038
Iteration 80/1000 | Loss: 0.00002038
Iteration 81/1000 | Loss: 0.00002038
Iteration 82/1000 | Loss: 0.00002038
Iteration 83/1000 | Loss: 0.00002038
Iteration 84/1000 | Loss: 0.00002038
Iteration 85/1000 | Loss: 0.00002038
Iteration 86/1000 | Loss: 0.00002038
Iteration 87/1000 | Loss: 0.00002038
Iteration 88/1000 | Loss: 0.00002038
Iteration 89/1000 | Loss: 0.00002038
Iteration 90/1000 | Loss: 0.00002038
Iteration 91/1000 | Loss: 0.00002038
Iteration 92/1000 | Loss: 0.00002038
Iteration 93/1000 | Loss: 0.00002038
Iteration 94/1000 | Loss: 0.00002038
Iteration 95/1000 | Loss: 0.00002038
Iteration 96/1000 | Loss: 0.00002038
Iteration 97/1000 | Loss: 0.00002038
Iteration 98/1000 | Loss: 0.00002038
Iteration 99/1000 | Loss: 0.00002038
Iteration 100/1000 | Loss: 0.00002038
Iteration 101/1000 | Loss: 0.00002038
Iteration 102/1000 | Loss: 0.00002038
Iteration 103/1000 | Loss: 0.00002038
Iteration 104/1000 | Loss: 0.00002038
Iteration 105/1000 | Loss: 0.00002038
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [2.0383251467137598e-05, 2.0383251467137598e-05, 2.0383251467137598e-05, 2.0383251467137598e-05, 2.0383251467137598e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0383251467137598e-05

Optimization complete. Final v2v error: 3.7627463340759277 mm

Highest mean error: 4.061994552612305 mm for frame 205

Lowest mean error: 3.3971121311187744 mm for frame 239

Saving results

Total time: 42.558905839920044
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00994436
Iteration 2/25 | Loss: 0.00994436
Iteration 3/25 | Loss: 0.00387556
Iteration 4/25 | Loss: 0.00249611
Iteration 5/25 | Loss: 0.00226649
Iteration 6/25 | Loss: 0.00209766
Iteration 7/25 | Loss: 0.00207654
Iteration 8/25 | Loss: 0.00204525
Iteration 9/25 | Loss: 0.00198490
Iteration 10/25 | Loss: 0.00197062
Iteration 11/25 | Loss: 0.00187158
Iteration 12/25 | Loss: 0.00186529
Iteration 13/25 | Loss: 0.00183693
Iteration 14/25 | Loss: 0.00183067
Iteration 15/25 | Loss: 0.00182600
Iteration 16/25 | Loss: 0.00182232
Iteration 17/25 | Loss: 0.00181770
Iteration 18/25 | Loss: 0.00181240
Iteration 19/25 | Loss: 0.00180966
Iteration 20/25 | Loss: 0.00180596
Iteration 21/25 | Loss: 0.00180451
Iteration 22/25 | Loss: 0.00180058
Iteration 23/25 | Loss: 0.00180439
Iteration 24/25 | Loss: 0.00179948
Iteration 25/25 | Loss: 0.00179719

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24732971
Iteration 2/25 | Loss: 0.00751478
Iteration 3/25 | Loss: 0.00599552
Iteration 4/25 | Loss: 0.00599551
Iteration 5/25 | Loss: 0.00599551
Iteration 6/25 | Loss: 0.00599551
Iteration 7/25 | Loss: 0.00599551
Iteration 8/25 | Loss: 0.00599551
Iteration 9/25 | Loss: 0.00599551
Iteration 10/25 | Loss: 0.00599551
Iteration 11/25 | Loss: 0.00599551
Iteration 12/25 | Loss: 0.00599551
Iteration 13/25 | Loss: 0.00599551
Iteration 14/25 | Loss: 0.00599551
Iteration 15/25 | Loss: 0.00599551
Iteration 16/25 | Loss: 0.00599551
Iteration 17/25 | Loss: 0.00599551
Iteration 18/25 | Loss: 0.00599551
Iteration 19/25 | Loss: 0.00599551
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00599551061168313, 0.00599551061168313, 0.00599551061168313, 0.00599551061168313, 0.00599551061168313]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00599551061168313

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00599551
Iteration 2/1000 | Loss: 0.00800833
Iteration 3/1000 | Loss: 0.00088541
Iteration 4/1000 | Loss: 0.00151274
Iteration 5/1000 | Loss: 0.00092227
Iteration 6/1000 | Loss: 0.00044279
Iteration 7/1000 | Loss: 0.00059380
Iteration 8/1000 | Loss: 0.00079207
Iteration 9/1000 | Loss: 0.00049680
Iteration 10/1000 | Loss: 0.00054224
Iteration 11/1000 | Loss: 0.00034866
Iteration 12/1000 | Loss: 0.00033492
Iteration 13/1000 | Loss: 0.00049787
Iteration 14/1000 | Loss: 0.00030920
Iteration 15/1000 | Loss: 0.00052484
Iteration 16/1000 | Loss: 0.00054472
Iteration 17/1000 | Loss: 0.00073015
Iteration 18/1000 | Loss: 0.00053113
Iteration 19/1000 | Loss: 0.00079370
Iteration 20/1000 | Loss: 0.00266209
Iteration 21/1000 | Loss: 0.01399804
Iteration 22/1000 | Loss: 0.01866106
Iteration 23/1000 | Loss: 0.00373108
Iteration 24/1000 | Loss: 0.00882606
Iteration 25/1000 | Loss: 0.00933282
Iteration 26/1000 | Loss: 0.00095754
Iteration 27/1000 | Loss: 0.00147238
Iteration 28/1000 | Loss: 0.00056364
Iteration 29/1000 | Loss: 0.00040432
Iteration 30/1000 | Loss: 0.00054477
Iteration 31/1000 | Loss: 0.00044881
Iteration 32/1000 | Loss: 0.00054475
Iteration 33/1000 | Loss: 0.00013609
Iteration 34/1000 | Loss: 0.00039224
Iteration 35/1000 | Loss: 0.00025945
Iteration 36/1000 | Loss: 0.00019364
Iteration 37/1000 | Loss: 0.00018767
Iteration 38/1000 | Loss: 0.00012801
Iteration 39/1000 | Loss: 0.00010649
Iteration 40/1000 | Loss: 0.00069208
Iteration 41/1000 | Loss: 0.00096941
Iteration 42/1000 | Loss: 0.00017390
Iteration 43/1000 | Loss: 0.00016348
Iteration 44/1000 | Loss: 0.00007662
Iteration 45/1000 | Loss: 0.00029433
Iteration 46/1000 | Loss: 0.00009513
Iteration 47/1000 | Loss: 0.00004469
Iteration 48/1000 | Loss: 0.00047289
Iteration 49/1000 | Loss: 0.00027543
Iteration 50/1000 | Loss: 0.00038579
Iteration 51/1000 | Loss: 0.00033819
Iteration 52/1000 | Loss: 0.00016903
Iteration 53/1000 | Loss: 0.00054095
Iteration 54/1000 | Loss: 0.00006201
Iteration 55/1000 | Loss: 0.00021402
Iteration 56/1000 | Loss: 0.00035324
Iteration 57/1000 | Loss: 0.00019421
Iteration 58/1000 | Loss: 0.00026130
Iteration 59/1000 | Loss: 0.00004040
Iteration 60/1000 | Loss: 0.00002337
Iteration 61/1000 | Loss: 0.00003181
Iteration 62/1000 | Loss: 0.00005008
Iteration 63/1000 | Loss: 0.00002254
Iteration 64/1000 | Loss: 0.00007748
Iteration 65/1000 | Loss: 0.00002357
Iteration 66/1000 | Loss: 0.00004034
Iteration 67/1000 | Loss: 0.00010486
Iteration 68/1000 | Loss: 0.00014850
Iteration 69/1000 | Loss: 0.00002948
Iteration 70/1000 | Loss: 0.00011268
Iteration 71/1000 | Loss: 0.00007625
Iteration 72/1000 | Loss: 0.00004025
Iteration 73/1000 | Loss: 0.00003882
Iteration 74/1000 | Loss: 0.00003245
Iteration 75/1000 | Loss: 0.00001993
Iteration 76/1000 | Loss: 0.00003310
Iteration 77/1000 | Loss: 0.00003419
Iteration 78/1000 | Loss: 0.00002002
Iteration 79/1000 | Loss: 0.00002994
Iteration 80/1000 | Loss: 0.00003743
Iteration 81/1000 | Loss: 0.00001986
Iteration 82/1000 | Loss: 0.00001877
Iteration 83/1000 | Loss: 0.00002829
Iteration 84/1000 | Loss: 0.00003687
Iteration 85/1000 | Loss: 0.00004732
Iteration 86/1000 | Loss: 0.00006499
Iteration 87/1000 | Loss: 0.00002587
Iteration 88/1000 | Loss: 0.00002761
Iteration 89/1000 | Loss: 0.00001886
Iteration 90/1000 | Loss: 0.00001891
Iteration 91/1000 | Loss: 0.00001893
Iteration 92/1000 | Loss: 0.00004222
Iteration 93/1000 | Loss: 0.00002239
Iteration 94/1000 | Loss: 0.00001989
Iteration 95/1000 | Loss: 0.00001989
Iteration 96/1000 | Loss: 0.00003233
Iteration 97/1000 | Loss: 0.00002106
Iteration 98/1000 | Loss: 0.00002095
Iteration 99/1000 | Loss: 0.00001805
Iteration 100/1000 | Loss: 0.00001805
Iteration 101/1000 | Loss: 0.00001804
Iteration 102/1000 | Loss: 0.00001835
Iteration 103/1000 | Loss: 0.00001835
Iteration 104/1000 | Loss: 0.00001835
Iteration 105/1000 | Loss: 0.00001835
Iteration 106/1000 | Loss: 0.00001835
Iteration 107/1000 | Loss: 0.00001835
Iteration 108/1000 | Loss: 0.00001834
Iteration 109/1000 | Loss: 0.00001993
Iteration 110/1000 | Loss: 0.00016053
Iteration 111/1000 | Loss: 0.00009695
Iteration 112/1000 | Loss: 0.00001945
Iteration 113/1000 | Loss: 0.00002469
Iteration 114/1000 | Loss: 0.00002065
Iteration 115/1000 | Loss: 0.00002106
Iteration 116/1000 | Loss: 0.00001911
Iteration 117/1000 | Loss: 0.00001823
Iteration 118/1000 | Loss: 0.00002099
Iteration 119/1000 | Loss: 0.00001808
Iteration 120/1000 | Loss: 0.00001795
Iteration 121/1000 | Loss: 0.00001795
Iteration 122/1000 | Loss: 0.00001795
Iteration 123/1000 | Loss: 0.00001795
Iteration 124/1000 | Loss: 0.00001795
Iteration 125/1000 | Loss: 0.00001795
Iteration 126/1000 | Loss: 0.00001795
Iteration 127/1000 | Loss: 0.00001794
Iteration 128/1000 | Loss: 0.00001794
Iteration 129/1000 | Loss: 0.00001794
Iteration 130/1000 | Loss: 0.00001794
Iteration 131/1000 | Loss: 0.00001794
Iteration 132/1000 | Loss: 0.00001793
Iteration 133/1000 | Loss: 0.00001793
Iteration 134/1000 | Loss: 0.00001793
Iteration 135/1000 | Loss: 0.00001792
Iteration 136/1000 | Loss: 0.00001792
Iteration 137/1000 | Loss: 0.00001792
Iteration 138/1000 | Loss: 0.00001792
Iteration 139/1000 | Loss: 0.00001792
Iteration 140/1000 | Loss: 0.00001792
Iteration 141/1000 | Loss: 0.00001792
Iteration 142/1000 | Loss: 0.00001792
Iteration 143/1000 | Loss: 0.00001792
Iteration 144/1000 | Loss: 0.00001792
Iteration 145/1000 | Loss: 0.00001792
Iteration 146/1000 | Loss: 0.00001791
Iteration 147/1000 | Loss: 0.00001791
Iteration 148/1000 | Loss: 0.00001819
Iteration 149/1000 | Loss: 0.00002465
Iteration 150/1000 | Loss: 0.00012719
Iteration 151/1000 | Loss: 0.00010473
Iteration 152/1000 | Loss: 0.00002384
Iteration 153/1000 | Loss: 0.00001901
Iteration 154/1000 | Loss: 0.00001874
Iteration 155/1000 | Loss: 0.00013056
Iteration 156/1000 | Loss: 0.00010267
Iteration 157/1000 | Loss: 0.00003925
Iteration 158/1000 | Loss: 0.00002116
Iteration 159/1000 | Loss: 0.00013909
Iteration 160/1000 | Loss: 0.00018508
Iteration 161/1000 | Loss: 0.00002070
Iteration 162/1000 | Loss: 0.00012764
Iteration 163/1000 | Loss: 0.00004375
Iteration 164/1000 | Loss: 0.00021348
Iteration 165/1000 | Loss: 0.00010569
Iteration 166/1000 | Loss: 0.00002328
Iteration 167/1000 | Loss: 0.00003437
Iteration 168/1000 | Loss: 0.00002145
Iteration 169/1000 | Loss: 0.00003480
Iteration 170/1000 | Loss: 0.00002568
Iteration 171/1000 | Loss: 0.00017294
Iteration 172/1000 | Loss: 0.00011847
Iteration 173/1000 | Loss: 0.00003352
Iteration 174/1000 | Loss: 0.00005762
Iteration 175/1000 | Loss: 0.00012893
Iteration 176/1000 | Loss: 0.00004205
Iteration 177/1000 | Loss: 0.00044402
Iteration 178/1000 | Loss: 0.00026882
Iteration 179/1000 | Loss: 0.00018566
Iteration 180/1000 | Loss: 0.00014296
Iteration 181/1000 | Loss: 0.00010289
Iteration 182/1000 | Loss: 0.00002728
Iteration 183/1000 | Loss: 0.00004210
Iteration 184/1000 | Loss: 0.00002373
Iteration 185/1000 | Loss: 0.00002849
Iteration 186/1000 | Loss: 0.00003633
Iteration 187/1000 | Loss: 0.00002448
Iteration 188/1000 | Loss: 0.00003634
Iteration 189/1000 | Loss: 0.00002395
Iteration 190/1000 | Loss: 0.00003108
Iteration 191/1000 | Loss: 0.00006267
Iteration 192/1000 | Loss: 0.00002397
Iteration 193/1000 | Loss: 0.00003357
Iteration 194/1000 | Loss: 0.00003586
Iteration 195/1000 | Loss: 0.00002446
Iteration 196/1000 | Loss: 0.00002116
Iteration 197/1000 | Loss: 0.00002025
Iteration 198/1000 | Loss: 0.00006502
Iteration 199/1000 | Loss: 0.00002221
Iteration 200/1000 | Loss: 0.00005541
Iteration 201/1000 | Loss: 0.00002335
Iteration 202/1000 | Loss: 0.00003113
Iteration 203/1000 | Loss: 0.00004615
Iteration 204/1000 | Loss: 0.00003307
Iteration 205/1000 | Loss: 0.00019625
Iteration 206/1000 | Loss: 0.00004956
Iteration 207/1000 | Loss: 0.00003082
Iteration 208/1000 | Loss: 0.00009027
Iteration 209/1000 | Loss: 0.00002564
Iteration 210/1000 | Loss: 0.00002345
Iteration 211/1000 | Loss: 0.00003725
Iteration 212/1000 | Loss: 0.00021238
Iteration 213/1000 | Loss: 0.00006686
Iteration 214/1000 | Loss: 0.00002970
Iteration 215/1000 | Loss: 0.00002993
Iteration 216/1000 | Loss: 0.00030566
Iteration 217/1000 | Loss: 0.00006747
Iteration 218/1000 | Loss: 0.00004752
Iteration 219/1000 | Loss: 0.00002480
Iteration 220/1000 | Loss: 0.00001910
Iteration 221/1000 | Loss: 0.00002404
Iteration 222/1000 | Loss: 0.00002161
Iteration 223/1000 | Loss: 0.00024379
Iteration 224/1000 | Loss: 0.00005450
Iteration 225/1000 | Loss: 0.00002171
Iteration 226/1000 | Loss: 0.00003603
Iteration 227/1000 | Loss: 0.00003174
Iteration 228/1000 | Loss: 0.00021184
Iteration 229/1000 | Loss: 0.00008295
Iteration 230/1000 | Loss: 0.00002035
Iteration 231/1000 | Loss: 0.00003408
Iteration 232/1000 | Loss: 0.00036427
Iteration 233/1000 | Loss: 0.00017696
Iteration 234/1000 | Loss: 0.00005548
Iteration 235/1000 | Loss: 0.00005794
Iteration 236/1000 | Loss: 0.00002294
Iteration 237/1000 | Loss: 0.00003814
Iteration 238/1000 | Loss: 0.00007694
Iteration 239/1000 | Loss: 0.00003399
Iteration 240/1000 | Loss: 0.00004329
Iteration 241/1000 | Loss: 0.00003795
Iteration 242/1000 | Loss: 0.00003066
Iteration 243/1000 | Loss: 0.00003118
Iteration 244/1000 | Loss: 0.00003079
Iteration 245/1000 | Loss: 0.00003195
Iteration 246/1000 | Loss: 0.00003341
Iteration 247/1000 | Loss: 0.00003607
Iteration 248/1000 | Loss: 0.00017319
Iteration 249/1000 | Loss: 0.00008421
Iteration 250/1000 | Loss: 0.00006535
Iteration 251/1000 | Loss: 0.00015938
Iteration 252/1000 | Loss: 0.00010978
Iteration 253/1000 | Loss: 0.00017962
Iteration 254/1000 | Loss: 0.00010678
Iteration 255/1000 | Loss: 0.00007422
Iteration 256/1000 | Loss: 0.00015681
Iteration 257/1000 | Loss: 0.00005880
Iteration 258/1000 | Loss: 0.00004459
Iteration 259/1000 | Loss: 0.00003332
Iteration 260/1000 | Loss: 0.00002064
Iteration 261/1000 | Loss: 0.00002931
Iteration 262/1000 | Loss: 0.00002611
Iteration 263/1000 | Loss: 0.00001996
Iteration 264/1000 | Loss: 0.00002511
Iteration 265/1000 | Loss: 0.00013043
Iteration 266/1000 | Loss: 0.00008879
Iteration 267/1000 | Loss: 0.00002967
Iteration 268/1000 | Loss: 0.00002438
Iteration 269/1000 | Loss: 0.00012995
Iteration 270/1000 | Loss: 0.00012176
Iteration 271/1000 | Loss: 0.00016726
Iteration 272/1000 | Loss: 0.00083724
Iteration 273/1000 | Loss: 0.00003293
Iteration 274/1000 | Loss: 0.00002270
Iteration 275/1000 | Loss: 0.00002898
Iteration 276/1000 | Loss: 0.00002280
Iteration 277/1000 | Loss: 0.00001895
Iteration 278/1000 | Loss: 0.00002715
Iteration 279/1000 | Loss: 0.00002715
Iteration 280/1000 | Loss: 0.00019401
Iteration 281/1000 | Loss: 0.00027048
Iteration 282/1000 | Loss: 0.00009528
Iteration 283/1000 | Loss: 0.00019355
Iteration 284/1000 | Loss: 0.00015727
Iteration 285/1000 | Loss: 0.00004416
Iteration 286/1000 | Loss: 0.00002478
Iteration 287/1000 | Loss: 0.00003174
Iteration 288/1000 | Loss: 0.00004027
Iteration 289/1000 | Loss: 0.00013417
Iteration 290/1000 | Loss: 0.00005014
Iteration 291/1000 | Loss: 0.00002377
Iteration 292/1000 | Loss: 0.00015110
Iteration 293/1000 | Loss: 0.00005535
Iteration 294/1000 | Loss: 0.00011980
Iteration 295/1000 | Loss: 0.00006978
Iteration 296/1000 | Loss: 0.00021884
Iteration 297/1000 | Loss: 0.00009921
Iteration 298/1000 | Loss: 0.00007662
Iteration 299/1000 | Loss: 0.00002253
Iteration 300/1000 | Loss: 0.00014846
Iteration 301/1000 | Loss: 0.00006755
Iteration 302/1000 | Loss: 0.00004254
Iteration 303/1000 | Loss: 0.00013505
Iteration 304/1000 | Loss: 0.00011870
Iteration 305/1000 | Loss: 0.00006072
Iteration 306/1000 | Loss: 0.00008490
Iteration 307/1000 | Loss: 0.00006829
Iteration 308/1000 | Loss: 0.00010077
Iteration 309/1000 | Loss: 0.00006948
Iteration 310/1000 | Loss: 0.00006672
Iteration 311/1000 | Loss: 0.00008705
Iteration 312/1000 | Loss: 0.00007585
Iteration 313/1000 | Loss: 0.00006463
Iteration 314/1000 | Loss: 0.00008996
Iteration 315/1000 | Loss: 0.00009863
Iteration 316/1000 | Loss: 0.00004306
Iteration 317/1000 | Loss: 0.00002189
Iteration 318/1000 | Loss: 0.00002022
Iteration 319/1000 | Loss: 0.00002509
Iteration 320/1000 | Loss: 0.00002004
Iteration 321/1000 | Loss: 0.00001851
Iteration 322/1000 | Loss: 0.00001850
Iteration 323/1000 | Loss: 0.00001850
Iteration 324/1000 | Loss: 0.00001858
Iteration 325/1000 | Loss: 0.00002089
Iteration 326/1000 | Loss: 0.00002089
Iteration 327/1000 | Loss: 0.00005524
Iteration 328/1000 | Loss: 0.00001903
Iteration 329/1000 | Loss: 0.00002092
Iteration 330/1000 | Loss: 0.00002428
Iteration 331/1000 | Loss: 0.00001943
Iteration 332/1000 | Loss: 0.00005293
Iteration 333/1000 | Loss: 0.00002048
Iteration 334/1000 | Loss: 0.00003427
Iteration 335/1000 | Loss: 0.00002937
Iteration 336/1000 | Loss: 0.00002156
Iteration 337/1000 | Loss: 0.00001976
Iteration 338/1000 | Loss: 0.00003061
Iteration 339/1000 | Loss: 0.00002265
Iteration 340/1000 | Loss: 0.00001904
Iteration 341/1000 | Loss: 0.00001786
Iteration 342/1000 | Loss: 0.00001786
Iteration 343/1000 | Loss: 0.00001786
Iteration 344/1000 | Loss: 0.00001785
Iteration 345/1000 | Loss: 0.00001785
Iteration 346/1000 | Loss: 0.00001785
Iteration 347/1000 | Loss: 0.00001837
Iteration 348/1000 | Loss: 0.00002636
Iteration 349/1000 | Loss: 0.00002002
Iteration 350/1000 | Loss: 0.00001785
Iteration 351/1000 | Loss: 0.00001785
Iteration 352/1000 | Loss: 0.00001785
Iteration 353/1000 | Loss: 0.00001785
Iteration 354/1000 | Loss: 0.00001785
Iteration 355/1000 | Loss: 0.00001809
Iteration 356/1000 | Loss: 0.00014364
Iteration 357/1000 | Loss: 0.00010788
Iteration 358/1000 | Loss: 0.00008407
Iteration 359/1000 | Loss: 0.00002533
Iteration 360/1000 | Loss: 0.00002819
Iteration 361/1000 | Loss: 0.00004844
Iteration 362/1000 | Loss: 0.00006588
Iteration 363/1000 | Loss: 0.00003474
Iteration 364/1000 | Loss: 0.00002170
Iteration 365/1000 | Loss: 0.00004597
Iteration 366/1000 | Loss: 0.00002465
Iteration 367/1000 | Loss: 0.00002364
Iteration 368/1000 | Loss: 0.00002567
Iteration 369/1000 | Loss: 0.00012265
Iteration 370/1000 | Loss: 0.00005541
Iteration 371/1000 | Loss: 0.00002147
Iteration 372/1000 | Loss: 0.00001824
Iteration 373/1000 | Loss: 0.00002017
Iteration 374/1000 | Loss: 0.00003034
Iteration 375/1000 | Loss: 0.00002050
Iteration 376/1000 | Loss: 0.00001934
Iteration 377/1000 | Loss: 0.00002619
Iteration 378/1000 | Loss: 0.00001867
Iteration 379/1000 | Loss: 0.00002410
Iteration 380/1000 | Loss: 0.00002595
Iteration 381/1000 | Loss: 0.00002316
Iteration 382/1000 | Loss: 0.00001782
Iteration 383/1000 | Loss: 0.00001782
Iteration 384/1000 | Loss: 0.00001978
Iteration 385/1000 | Loss: 0.00001781
Iteration 386/1000 | Loss: 0.00001781
Iteration 387/1000 | Loss: 0.00001781
Iteration 388/1000 | Loss: 0.00001780
Iteration 389/1000 | Loss: 0.00001780
Iteration 390/1000 | Loss: 0.00001780
Iteration 391/1000 | Loss: 0.00001780
Iteration 392/1000 | Loss: 0.00001780
Iteration 393/1000 | Loss: 0.00001780
Iteration 394/1000 | Loss: 0.00001780
Iteration 395/1000 | Loss: 0.00001780
Iteration 396/1000 | Loss: 0.00001779
Iteration 397/1000 | Loss: 0.00001819
Iteration 398/1000 | Loss: 0.00001996
Iteration 399/1000 | Loss: 0.00002126
Iteration 400/1000 | Loss: 0.00001797
Iteration 401/1000 | Loss: 0.00001778
Iteration 402/1000 | Loss: 0.00002013
Iteration 403/1000 | Loss: 0.00001789
Iteration 404/1000 | Loss: 0.00001775
Iteration 405/1000 | Loss: 0.00001775
Iteration 406/1000 | Loss: 0.00001774
Iteration 407/1000 | Loss: 0.00001774
Iteration 408/1000 | Loss: 0.00001774
Iteration 409/1000 | Loss: 0.00001774
Iteration 410/1000 | Loss: 0.00001774
Iteration 411/1000 | Loss: 0.00001774
Iteration 412/1000 | Loss: 0.00001774
Iteration 413/1000 | Loss: 0.00001774
Iteration 414/1000 | Loss: 0.00001774
Iteration 415/1000 | Loss: 0.00001774
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 415. Stopping optimization.
Last 5 losses: [1.774379052221775e-05, 1.774379052221775e-05, 1.774379052221775e-05, 1.774379052221775e-05, 1.774379052221775e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.774379052221775e-05

Optimization complete. Final v2v error: 3.4570512771606445 mm

Highest mean error: 7.261162757873535 mm for frame 85

Lowest mean error: 2.9358203411102295 mm for frame 46

Saving results

Total time: 557.5763971805573
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00795981
Iteration 2/25 | Loss: 0.00129375
Iteration 3/25 | Loss: 0.00117772
Iteration 4/25 | Loss: 0.00115810
Iteration 5/25 | Loss: 0.00115407
Iteration 6/25 | Loss: 0.00115375
Iteration 7/25 | Loss: 0.00115375
Iteration 8/25 | Loss: 0.00115375
Iteration 9/25 | Loss: 0.00115375
Iteration 10/25 | Loss: 0.00115375
Iteration 11/25 | Loss: 0.00115375
Iteration 12/25 | Loss: 0.00115375
Iteration 13/25 | Loss: 0.00115375
Iteration 14/25 | Loss: 0.00115375
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0011537517420947552, 0.0011537517420947552, 0.0011537517420947552, 0.0011537517420947552, 0.0011537517420947552]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011537517420947552

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29140484
Iteration 2/25 | Loss: 0.00138387
Iteration 3/25 | Loss: 0.00138387
Iteration 4/25 | Loss: 0.00138387
Iteration 5/25 | Loss: 0.00138387
Iteration 6/25 | Loss: 0.00138387
Iteration 7/25 | Loss: 0.00138387
Iteration 8/25 | Loss: 0.00138387
Iteration 9/25 | Loss: 0.00138387
Iteration 10/25 | Loss: 0.00138387
Iteration 11/25 | Loss: 0.00138387
Iteration 12/25 | Loss: 0.00138387
Iteration 13/25 | Loss: 0.00138387
Iteration 14/25 | Loss: 0.00138387
Iteration 15/25 | Loss: 0.00138387
Iteration 16/25 | Loss: 0.00138387
Iteration 17/25 | Loss: 0.00138387
Iteration 18/25 | Loss: 0.00138387
Iteration 19/25 | Loss: 0.00138387
Iteration 20/25 | Loss: 0.00138387
Iteration 21/25 | Loss: 0.00138387
Iteration 22/25 | Loss: 0.00138387
Iteration 23/25 | Loss: 0.00138387
Iteration 24/25 | Loss: 0.00138387
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0013838701415807009, 0.0013838701415807009, 0.0013838701415807009, 0.0013838701415807009, 0.0013838701415807009]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013838701415807009

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00138387
Iteration 2/1000 | Loss: 0.00002055
Iteration 3/1000 | Loss: 0.00001521
Iteration 4/1000 | Loss: 0.00001376
Iteration 5/1000 | Loss: 0.00001267
Iteration 6/1000 | Loss: 0.00001188
Iteration 7/1000 | Loss: 0.00001149
Iteration 8/1000 | Loss: 0.00001116
Iteration 9/1000 | Loss: 0.00001079
Iteration 10/1000 | Loss: 0.00001064
Iteration 11/1000 | Loss: 0.00001046
Iteration 12/1000 | Loss: 0.00001042
Iteration 13/1000 | Loss: 0.00001037
Iteration 14/1000 | Loss: 0.00001037
Iteration 15/1000 | Loss: 0.00001035
Iteration 16/1000 | Loss: 0.00001033
Iteration 17/1000 | Loss: 0.00001032
Iteration 18/1000 | Loss: 0.00001031
Iteration 19/1000 | Loss: 0.00001030
Iteration 20/1000 | Loss: 0.00001029
Iteration 21/1000 | Loss: 0.00001028
Iteration 22/1000 | Loss: 0.00001027
Iteration 23/1000 | Loss: 0.00001026
Iteration 24/1000 | Loss: 0.00001025
Iteration 25/1000 | Loss: 0.00001024
Iteration 26/1000 | Loss: 0.00001023
Iteration 27/1000 | Loss: 0.00001023
Iteration 28/1000 | Loss: 0.00001022
Iteration 29/1000 | Loss: 0.00001021
Iteration 30/1000 | Loss: 0.00001019
Iteration 31/1000 | Loss: 0.00001018
Iteration 32/1000 | Loss: 0.00001017
Iteration 33/1000 | Loss: 0.00001017
Iteration 34/1000 | Loss: 0.00001016
Iteration 35/1000 | Loss: 0.00001016
Iteration 36/1000 | Loss: 0.00001015
Iteration 37/1000 | Loss: 0.00001015
Iteration 38/1000 | Loss: 0.00001014
Iteration 39/1000 | Loss: 0.00001014
Iteration 40/1000 | Loss: 0.00001013
Iteration 41/1000 | Loss: 0.00001013
Iteration 42/1000 | Loss: 0.00001012
Iteration 43/1000 | Loss: 0.00001011
Iteration 44/1000 | Loss: 0.00001011
Iteration 45/1000 | Loss: 0.00001010
Iteration 46/1000 | Loss: 0.00001010
Iteration 47/1000 | Loss: 0.00001005
Iteration 48/1000 | Loss: 0.00000999
Iteration 49/1000 | Loss: 0.00000999
Iteration 50/1000 | Loss: 0.00000998
Iteration 51/1000 | Loss: 0.00000997
Iteration 52/1000 | Loss: 0.00000997
Iteration 53/1000 | Loss: 0.00000996
Iteration 54/1000 | Loss: 0.00000996
Iteration 55/1000 | Loss: 0.00000996
Iteration 56/1000 | Loss: 0.00000995
Iteration 57/1000 | Loss: 0.00000995
Iteration 58/1000 | Loss: 0.00000995
Iteration 59/1000 | Loss: 0.00000993
Iteration 60/1000 | Loss: 0.00000993
Iteration 61/1000 | Loss: 0.00000991
Iteration 62/1000 | Loss: 0.00000989
Iteration 63/1000 | Loss: 0.00000989
Iteration 64/1000 | Loss: 0.00000986
Iteration 65/1000 | Loss: 0.00000986
Iteration 66/1000 | Loss: 0.00000986
Iteration 67/1000 | Loss: 0.00000986
Iteration 68/1000 | Loss: 0.00000986
Iteration 69/1000 | Loss: 0.00000986
Iteration 70/1000 | Loss: 0.00000986
Iteration 71/1000 | Loss: 0.00000986
Iteration 72/1000 | Loss: 0.00000985
Iteration 73/1000 | Loss: 0.00000985
Iteration 74/1000 | Loss: 0.00000985
Iteration 75/1000 | Loss: 0.00000985
Iteration 76/1000 | Loss: 0.00000985
Iteration 77/1000 | Loss: 0.00000985
Iteration 78/1000 | Loss: 0.00000985
Iteration 79/1000 | Loss: 0.00000984
Iteration 80/1000 | Loss: 0.00000984
Iteration 81/1000 | Loss: 0.00000984
Iteration 82/1000 | Loss: 0.00000983
Iteration 83/1000 | Loss: 0.00000983
Iteration 84/1000 | Loss: 0.00000982
Iteration 85/1000 | Loss: 0.00000982
Iteration 86/1000 | Loss: 0.00000981
Iteration 87/1000 | Loss: 0.00000981
Iteration 88/1000 | Loss: 0.00000981
Iteration 89/1000 | Loss: 0.00000980
Iteration 90/1000 | Loss: 0.00000980
Iteration 91/1000 | Loss: 0.00000980
Iteration 92/1000 | Loss: 0.00000979
Iteration 93/1000 | Loss: 0.00000979
Iteration 94/1000 | Loss: 0.00000979
Iteration 95/1000 | Loss: 0.00000979
Iteration 96/1000 | Loss: 0.00000978
Iteration 97/1000 | Loss: 0.00000978
Iteration 98/1000 | Loss: 0.00000978
Iteration 99/1000 | Loss: 0.00000978
Iteration 100/1000 | Loss: 0.00000978
Iteration 101/1000 | Loss: 0.00000978
Iteration 102/1000 | Loss: 0.00000978
Iteration 103/1000 | Loss: 0.00000978
Iteration 104/1000 | Loss: 0.00000977
Iteration 105/1000 | Loss: 0.00000977
Iteration 106/1000 | Loss: 0.00000977
Iteration 107/1000 | Loss: 0.00000976
Iteration 108/1000 | Loss: 0.00000976
Iteration 109/1000 | Loss: 0.00000976
Iteration 110/1000 | Loss: 0.00000976
Iteration 111/1000 | Loss: 0.00000976
Iteration 112/1000 | Loss: 0.00000975
Iteration 113/1000 | Loss: 0.00000975
Iteration 114/1000 | Loss: 0.00000975
Iteration 115/1000 | Loss: 0.00000974
Iteration 116/1000 | Loss: 0.00000974
Iteration 117/1000 | Loss: 0.00000973
Iteration 118/1000 | Loss: 0.00000973
Iteration 119/1000 | Loss: 0.00000973
Iteration 120/1000 | Loss: 0.00000973
Iteration 121/1000 | Loss: 0.00000973
Iteration 122/1000 | Loss: 0.00000973
Iteration 123/1000 | Loss: 0.00000973
Iteration 124/1000 | Loss: 0.00000973
Iteration 125/1000 | Loss: 0.00000972
Iteration 126/1000 | Loss: 0.00000972
Iteration 127/1000 | Loss: 0.00000972
Iteration 128/1000 | Loss: 0.00000972
Iteration 129/1000 | Loss: 0.00000972
Iteration 130/1000 | Loss: 0.00000971
Iteration 131/1000 | Loss: 0.00000971
Iteration 132/1000 | Loss: 0.00000971
Iteration 133/1000 | Loss: 0.00000970
Iteration 134/1000 | Loss: 0.00000970
Iteration 135/1000 | Loss: 0.00000970
Iteration 136/1000 | Loss: 0.00000970
Iteration 137/1000 | Loss: 0.00000970
Iteration 138/1000 | Loss: 0.00000969
Iteration 139/1000 | Loss: 0.00000969
Iteration 140/1000 | Loss: 0.00000969
Iteration 141/1000 | Loss: 0.00000969
Iteration 142/1000 | Loss: 0.00000969
Iteration 143/1000 | Loss: 0.00000969
Iteration 144/1000 | Loss: 0.00000969
Iteration 145/1000 | Loss: 0.00000968
Iteration 146/1000 | Loss: 0.00000968
Iteration 147/1000 | Loss: 0.00000968
Iteration 148/1000 | Loss: 0.00000968
Iteration 149/1000 | Loss: 0.00000968
Iteration 150/1000 | Loss: 0.00000967
Iteration 151/1000 | Loss: 0.00000967
Iteration 152/1000 | Loss: 0.00000966
Iteration 153/1000 | Loss: 0.00000966
Iteration 154/1000 | Loss: 0.00000966
Iteration 155/1000 | Loss: 0.00000966
Iteration 156/1000 | Loss: 0.00000966
Iteration 157/1000 | Loss: 0.00000966
Iteration 158/1000 | Loss: 0.00000965
Iteration 159/1000 | Loss: 0.00000965
Iteration 160/1000 | Loss: 0.00000965
Iteration 161/1000 | Loss: 0.00000965
Iteration 162/1000 | Loss: 0.00000965
Iteration 163/1000 | Loss: 0.00000965
Iteration 164/1000 | Loss: 0.00000964
Iteration 165/1000 | Loss: 0.00000964
Iteration 166/1000 | Loss: 0.00000964
Iteration 167/1000 | Loss: 0.00000964
Iteration 168/1000 | Loss: 0.00000964
Iteration 169/1000 | Loss: 0.00000964
Iteration 170/1000 | Loss: 0.00000964
Iteration 171/1000 | Loss: 0.00000964
Iteration 172/1000 | Loss: 0.00000964
Iteration 173/1000 | Loss: 0.00000963
Iteration 174/1000 | Loss: 0.00000963
Iteration 175/1000 | Loss: 0.00000963
Iteration 176/1000 | Loss: 0.00000963
Iteration 177/1000 | Loss: 0.00000963
Iteration 178/1000 | Loss: 0.00000963
Iteration 179/1000 | Loss: 0.00000963
Iteration 180/1000 | Loss: 0.00000962
Iteration 181/1000 | Loss: 0.00000962
Iteration 182/1000 | Loss: 0.00000962
Iteration 183/1000 | Loss: 0.00000962
Iteration 184/1000 | Loss: 0.00000962
Iteration 185/1000 | Loss: 0.00000962
Iteration 186/1000 | Loss: 0.00000962
Iteration 187/1000 | Loss: 0.00000961
Iteration 188/1000 | Loss: 0.00000961
Iteration 189/1000 | Loss: 0.00000961
Iteration 190/1000 | Loss: 0.00000961
Iteration 191/1000 | Loss: 0.00000961
Iteration 192/1000 | Loss: 0.00000961
Iteration 193/1000 | Loss: 0.00000961
Iteration 194/1000 | Loss: 0.00000961
Iteration 195/1000 | Loss: 0.00000960
Iteration 196/1000 | Loss: 0.00000960
Iteration 197/1000 | Loss: 0.00000960
Iteration 198/1000 | Loss: 0.00000960
Iteration 199/1000 | Loss: 0.00000960
Iteration 200/1000 | Loss: 0.00000960
Iteration 201/1000 | Loss: 0.00000960
Iteration 202/1000 | Loss: 0.00000960
Iteration 203/1000 | Loss: 0.00000960
Iteration 204/1000 | Loss: 0.00000960
Iteration 205/1000 | Loss: 0.00000960
Iteration 206/1000 | Loss: 0.00000960
Iteration 207/1000 | Loss: 0.00000960
Iteration 208/1000 | Loss: 0.00000960
Iteration 209/1000 | Loss: 0.00000960
Iteration 210/1000 | Loss: 0.00000960
Iteration 211/1000 | Loss: 0.00000959
Iteration 212/1000 | Loss: 0.00000959
Iteration 213/1000 | Loss: 0.00000959
Iteration 214/1000 | Loss: 0.00000959
Iteration 215/1000 | Loss: 0.00000959
Iteration 216/1000 | Loss: 0.00000959
Iteration 217/1000 | Loss: 0.00000959
Iteration 218/1000 | Loss: 0.00000959
Iteration 219/1000 | Loss: 0.00000959
Iteration 220/1000 | Loss: 0.00000958
Iteration 221/1000 | Loss: 0.00000958
Iteration 222/1000 | Loss: 0.00000958
Iteration 223/1000 | Loss: 0.00000958
Iteration 224/1000 | Loss: 0.00000958
Iteration 225/1000 | Loss: 0.00000958
Iteration 226/1000 | Loss: 0.00000958
Iteration 227/1000 | Loss: 0.00000958
Iteration 228/1000 | Loss: 0.00000958
Iteration 229/1000 | Loss: 0.00000958
Iteration 230/1000 | Loss: 0.00000958
Iteration 231/1000 | Loss: 0.00000958
Iteration 232/1000 | Loss: 0.00000958
Iteration 233/1000 | Loss: 0.00000957
Iteration 234/1000 | Loss: 0.00000957
Iteration 235/1000 | Loss: 0.00000957
Iteration 236/1000 | Loss: 0.00000957
Iteration 237/1000 | Loss: 0.00000957
Iteration 238/1000 | Loss: 0.00000957
Iteration 239/1000 | Loss: 0.00000957
Iteration 240/1000 | Loss: 0.00000957
Iteration 241/1000 | Loss: 0.00000957
Iteration 242/1000 | Loss: 0.00000957
Iteration 243/1000 | Loss: 0.00000957
Iteration 244/1000 | Loss: 0.00000956
Iteration 245/1000 | Loss: 0.00000956
Iteration 246/1000 | Loss: 0.00000956
Iteration 247/1000 | Loss: 0.00000956
Iteration 248/1000 | Loss: 0.00000956
Iteration 249/1000 | Loss: 0.00000956
Iteration 250/1000 | Loss: 0.00000956
Iteration 251/1000 | Loss: 0.00000956
Iteration 252/1000 | Loss: 0.00000956
Iteration 253/1000 | Loss: 0.00000956
Iteration 254/1000 | Loss: 0.00000956
Iteration 255/1000 | Loss: 0.00000956
Iteration 256/1000 | Loss: 0.00000956
Iteration 257/1000 | Loss: 0.00000956
Iteration 258/1000 | Loss: 0.00000956
Iteration 259/1000 | Loss: 0.00000956
Iteration 260/1000 | Loss: 0.00000956
Iteration 261/1000 | Loss: 0.00000956
Iteration 262/1000 | Loss: 0.00000956
Iteration 263/1000 | Loss: 0.00000956
Iteration 264/1000 | Loss: 0.00000955
Iteration 265/1000 | Loss: 0.00000955
Iteration 266/1000 | Loss: 0.00000955
Iteration 267/1000 | Loss: 0.00000955
Iteration 268/1000 | Loss: 0.00000955
Iteration 269/1000 | Loss: 0.00000955
Iteration 270/1000 | Loss: 0.00000955
Iteration 271/1000 | Loss: 0.00000955
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 271. Stopping optimization.
Last 5 losses: [9.554884854878765e-06, 9.554884854878765e-06, 9.554884854878765e-06, 9.554884854878765e-06, 9.554884854878765e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.554884854878765e-06

Optimization complete. Final v2v error: 2.627997875213623 mm

Highest mean error: 3.013261318206787 mm for frame 100

Lowest mean error: 2.375777006149292 mm for frame 250

Saving results

Total time: 52.12643575668335
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00277714
Iteration 2/25 | Loss: 0.00136281
Iteration 3/25 | Loss: 0.00122290
Iteration 4/25 | Loss: 0.00119171
Iteration 5/25 | Loss: 0.00118571
Iteration 6/25 | Loss: 0.00118437
Iteration 7/25 | Loss: 0.00118412
Iteration 8/25 | Loss: 0.00118412
Iteration 9/25 | Loss: 0.00118412
Iteration 10/25 | Loss: 0.00118412
Iteration 11/25 | Loss: 0.00118412
Iteration 12/25 | Loss: 0.00118412
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011841224040836096, 0.0011841224040836096, 0.0011841224040836096, 0.0011841224040836096, 0.0011841224040836096]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011841224040836096

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27649331
Iteration 2/25 | Loss: 0.00181206
Iteration 3/25 | Loss: 0.00181206
Iteration 4/25 | Loss: 0.00181205
Iteration 5/25 | Loss: 0.00181205
Iteration 6/25 | Loss: 0.00181205
Iteration 7/25 | Loss: 0.00181205
Iteration 8/25 | Loss: 0.00181205
Iteration 9/25 | Loss: 0.00181205
Iteration 10/25 | Loss: 0.00181205
Iteration 11/25 | Loss: 0.00181205
Iteration 12/25 | Loss: 0.00181205
Iteration 13/25 | Loss: 0.00181205
Iteration 14/25 | Loss: 0.00181205
Iteration 15/25 | Loss: 0.00181205
Iteration 16/25 | Loss: 0.00181205
Iteration 17/25 | Loss: 0.00181205
Iteration 18/25 | Loss: 0.00181205
Iteration 19/25 | Loss: 0.00181205
Iteration 20/25 | Loss: 0.00181205
Iteration 21/25 | Loss: 0.00181205
Iteration 22/25 | Loss: 0.00181205
Iteration 23/25 | Loss: 0.00181205
Iteration 24/25 | Loss: 0.00181205
Iteration 25/25 | Loss: 0.00181205

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00181205
Iteration 2/1000 | Loss: 0.00004596
Iteration 3/1000 | Loss: 0.00003099
Iteration 4/1000 | Loss: 0.00002011
Iteration 5/1000 | Loss: 0.00001851
Iteration 6/1000 | Loss: 0.00001761
Iteration 7/1000 | Loss: 0.00001697
Iteration 8/1000 | Loss: 0.00001653
Iteration 9/1000 | Loss: 0.00001613
Iteration 10/1000 | Loss: 0.00001574
Iteration 11/1000 | Loss: 0.00001539
Iteration 12/1000 | Loss: 0.00001516
Iteration 13/1000 | Loss: 0.00001496
Iteration 14/1000 | Loss: 0.00001481
Iteration 15/1000 | Loss: 0.00001478
Iteration 16/1000 | Loss: 0.00001473
Iteration 17/1000 | Loss: 0.00001473
Iteration 18/1000 | Loss: 0.00001472
Iteration 19/1000 | Loss: 0.00001470
Iteration 20/1000 | Loss: 0.00001470
Iteration 21/1000 | Loss: 0.00001469
Iteration 22/1000 | Loss: 0.00001466
Iteration 23/1000 | Loss: 0.00001464
Iteration 24/1000 | Loss: 0.00001463
Iteration 25/1000 | Loss: 0.00001462
Iteration 26/1000 | Loss: 0.00001460
Iteration 27/1000 | Loss: 0.00001460
Iteration 28/1000 | Loss: 0.00001460
Iteration 29/1000 | Loss: 0.00001459
Iteration 30/1000 | Loss: 0.00001459
Iteration 31/1000 | Loss: 0.00001458
Iteration 32/1000 | Loss: 0.00001458
Iteration 33/1000 | Loss: 0.00001457
Iteration 34/1000 | Loss: 0.00001457
Iteration 35/1000 | Loss: 0.00001456
Iteration 36/1000 | Loss: 0.00001456
Iteration 37/1000 | Loss: 0.00001456
Iteration 38/1000 | Loss: 0.00001455
Iteration 39/1000 | Loss: 0.00001455
Iteration 40/1000 | Loss: 0.00001455
Iteration 41/1000 | Loss: 0.00001454
Iteration 42/1000 | Loss: 0.00001453
Iteration 43/1000 | Loss: 0.00001453
Iteration 44/1000 | Loss: 0.00001452
Iteration 45/1000 | Loss: 0.00001451
Iteration 46/1000 | Loss: 0.00001451
Iteration 47/1000 | Loss: 0.00001451
Iteration 48/1000 | Loss: 0.00001451
Iteration 49/1000 | Loss: 0.00001451
Iteration 50/1000 | Loss: 0.00001450
Iteration 51/1000 | Loss: 0.00001450
Iteration 52/1000 | Loss: 0.00001450
Iteration 53/1000 | Loss: 0.00001449
Iteration 54/1000 | Loss: 0.00001449
Iteration 55/1000 | Loss: 0.00001449
Iteration 56/1000 | Loss: 0.00001449
Iteration 57/1000 | Loss: 0.00001449
Iteration 58/1000 | Loss: 0.00001449
Iteration 59/1000 | Loss: 0.00001449
Iteration 60/1000 | Loss: 0.00001449
Iteration 61/1000 | Loss: 0.00001449
Iteration 62/1000 | Loss: 0.00001448
Iteration 63/1000 | Loss: 0.00001448
Iteration 64/1000 | Loss: 0.00001448
Iteration 65/1000 | Loss: 0.00001447
Iteration 66/1000 | Loss: 0.00001447
Iteration 67/1000 | Loss: 0.00001446
Iteration 68/1000 | Loss: 0.00001446
Iteration 69/1000 | Loss: 0.00001446
Iteration 70/1000 | Loss: 0.00001445
Iteration 71/1000 | Loss: 0.00001445
Iteration 72/1000 | Loss: 0.00001445
Iteration 73/1000 | Loss: 0.00001444
Iteration 74/1000 | Loss: 0.00001444
Iteration 75/1000 | Loss: 0.00001444
Iteration 76/1000 | Loss: 0.00001444
Iteration 77/1000 | Loss: 0.00001444
Iteration 78/1000 | Loss: 0.00001444
Iteration 79/1000 | Loss: 0.00001443
Iteration 80/1000 | Loss: 0.00001443
Iteration 81/1000 | Loss: 0.00001443
Iteration 82/1000 | Loss: 0.00001442
Iteration 83/1000 | Loss: 0.00001442
Iteration 84/1000 | Loss: 0.00001442
Iteration 85/1000 | Loss: 0.00001442
Iteration 86/1000 | Loss: 0.00001442
Iteration 87/1000 | Loss: 0.00001442
Iteration 88/1000 | Loss: 0.00001442
Iteration 89/1000 | Loss: 0.00001442
Iteration 90/1000 | Loss: 0.00001442
Iteration 91/1000 | Loss: 0.00001442
Iteration 92/1000 | Loss: 0.00001442
Iteration 93/1000 | Loss: 0.00001441
Iteration 94/1000 | Loss: 0.00001441
Iteration 95/1000 | Loss: 0.00001441
Iteration 96/1000 | Loss: 0.00001441
Iteration 97/1000 | Loss: 0.00001441
Iteration 98/1000 | Loss: 0.00001441
Iteration 99/1000 | Loss: 0.00001441
Iteration 100/1000 | Loss: 0.00001441
Iteration 101/1000 | Loss: 0.00001441
Iteration 102/1000 | Loss: 0.00001440
Iteration 103/1000 | Loss: 0.00001440
Iteration 104/1000 | Loss: 0.00001439
Iteration 105/1000 | Loss: 0.00001439
Iteration 106/1000 | Loss: 0.00001439
Iteration 107/1000 | Loss: 0.00001439
Iteration 108/1000 | Loss: 0.00001439
Iteration 109/1000 | Loss: 0.00001439
Iteration 110/1000 | Loss: 0.00001439
Iteration 111/1000 | Loss: 0.00001439
Iteration 112/1000 | Loss: 0.00001439
Iteration 113/1000 | Loss: 0.00001439
Iteration 114/1000 | Loss: 0.00001438
Iteration 115/1000 | Loss: 0.00001438
Iteration 116/1000 | Loss: 0.00001438
Iteration 117/1000 | Loss: 0.00001438
Iteration 118/1000 | Loss: 0.00001438
Iteration 119/1000 | Loss: 0.00001438
Iteration 120/1000 | Loss: 0.00001438
Iteration 121/1000 | Loss: 0.00001438
Iteration 122/1000 | Loss: 0.00001437
Iteration 123/1000 | Loss: 0.00001437
Iteration 124/1000 | Loss: 0.00001437
Iteration 125/1000 | Loss: 0.00001437
Iteration 126/1000 | Loss: 0.00001437
Iteration 127/1000 | Loss: 0.00001437
Iteration 128/1000 | Loss: 0.00001437
Iteration 129/1000 | Loss: 0.00001437
Iteration 130/1000 | Loss: 0.00001436
Iteration 131/1000 | Loss: 0.00001436
Iteration 132/1000 | Loss: 0.00001436
Iteration 133/1000 | Loss: 0.00001436
Iteration 134/1000 | Loss: 0.00001436
Iteration 135/1000 | Loss: 0.00001436
Iteration 136/1000 | Loss: 0.00001436
Iteration 137/1000 | Loss: 0.00001436
Iteration 138/1000 | Loss: 0.00001436
Iteration 139/1000 | Loss: 0.00001435
Iteration 140/1000 | Loss: 0.00001435
Iteration 141/1000 | Loss: 0.00001435
Iteration 142/1000 | Loss: 0.00001435
Iteration 143/1000 | Loss: 0.00001435
Iteration 144/1000 | Loss: 0.00001435
Iteration 145/1000 | Loss: 0.00001434
Iteration 146/1000 | Loss: 0.00001434
Iteration 147/1000 | Loss: 0.00001434
Iteration 148/1000 | Loss: 0.00001434
Iteration 149/1000 | Loss: 0.00001434
Iteration 150/1000 | Loss: 0.00001434
Iteration 151/1000 | Loss: 0.00001434
Iteration 152/1000 | Loss: 0.00001434
Iteration 153/1000 | Loss: 0.00001434
Iteration 154/1000 | Loss: 0.00001434
Iteration 155/1000 | Loss: 0.00001434
Iteration 156/1000 | Loss: 0.00001434
Iteration 157/1000 | Loss: 0.00001434
Iteration 158/1000 | Loss: 0.00001434
Iteration 159/1000 | Loss: 0.00001434
Iteration 160/1000 | Loss: 0.00001434
Iteration 161/1000 | Loss: 0.00001434
Iteration 162/1000 | Loss: 0.00001434
Iteration 163/1000 | Loss: 0.00001434
Iteration 164/1000 | Loss: 0.00001434
Iteration 165/1000 | Loss: 0.00001434
Iteration 166/1000 | Loss: 0.00001434
Iteration 167/1000 | Loss: 0.00001434
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.4335625564854126e-05, 1.4335625564854126e-05, 1.4335625564854126e-05, 1.4335625564854126e-05, 1.4335625564854126e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4335625564854126e-05

Optimization complete. Final v2v error: 3.179990530014038 mm

Highest mean error: 3.6310007572174072 mm for frame 88

Lowest mean error: 2.923454761505127 mm for frame 0

Saving results

Total time: 39.70538663864136
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00805658
Iteration 2/25 | Loss: 0.00144851
Iteration 3/25 | Loss: 0.00120917
Iteration 4/25 | Loss: 0.00118425
Iteration 5/25 | Loss: 0.00118108
Iteration 6/25 | Loss: 0.00118017
Iteration 7/25 | Loss: 0.00117981
Iteration 8/25 | Loss: 0.00117966
Iteration 9/25 | Loss: 0.00117957
Iteration 10/25 | Loss: 0.00117949
Iteration 11/25 | Loss: 0.00118067
Iteration 12/25 | Loss: 0.00118028
Iteration 13/25 | Loss: 0.00118125
Iteration 14/25 | Loss: 0.00118155
Iteration 15/25 | Loss: 0.00117852
Iteration 16/25 | Loss: 0.00117736
Iteration 17/25 | Loss: 0.00117697
Iteration 18/25 | Loss: 0.00117692
Iteration 19/25 | Loss: 0.00117692
Iteration 20/25 | Loss: 0.00117691
Iteration 21/25 | Loss: 0.00117691
Iteration 22/25 | Loss: 0.00117691
Iteration 23/25 | Loss: 0.00117691
Iteration 24/25 | Loss: 0.00117691
Iteration 25/25 | Loss: 0.00117691

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29807830
Iteration 2/25 | Loss: 0.00133414
Iteration 3/25 | Loss: 0.00133414
Iteration 4/25 | Loss: 0.00133414
Iteration 5/25 | Loss: 0.00133414
Iteration 6/25 | Loss: 0.00133414
Iteration 7/25 | Loss: 0.00133414
Iteration 8/25 | Loss: 0.00133413
Iteration 9/25 | Loss: 0.00133413
Iteration 10/25 | Loss: 0.00133413
Iteration 11/25 | Loss: 0.00133413
Iteration 12/25 | Loss: 0.00133413
Iteration 13/25 | Loss: 0.00133413
Iteration 14/25 | Loss: 0.00133413
Iteration 15/25 | Loss: 0.00133413
Iteration 16/25 | Loss: 0.00133413
Iteration 17/25 | Loss: 0.00133413
Iteration 18/25 | Loss: 0.00133413
Iteration 19/25 | Loss: 0.00133413
Iteration 20/25 | Loss: 0.00133413
Iteration 21/25 | Loss: 0.00133413
Iteration 22/25 | Loss: 0.00133413
Iteration 23/25 | Loss: 0.00133413
Iteration 24/25 | Loss: 0.00133413
Iteration 25/25 | Loss: 0.00133413
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.001334133092314005, 0.001334133092314005, 0.001334133092314005, 0.001334133092314005, 0.001334133092314005]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001334133092314005

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133413
Iteration 2/1000 | Loss: 0.00002059
Iteration 3/1000 | Loss: 0.00001477
Iteration 4/1000 | Loss: 0.00001293
Iteration 5/1000 | Loss: 0.00001194
Iteration 6/1000 | Loss: 0.00001139
Iteration 7/1000 | Loss: 0.00001089
Iteration 8/1000 | Loss: 0.00001054
Iteration 9/1000 | Loss: 0.00001026
Iteration 10/1000 | Loss: 0.00001001
Iteration 11/1000 | Loss: 0.00000995
Iteration 12/1000 | Loss: 0.00000991
Iteration 13/1000 | Loss: 0.00000989
Iteration 14/1000 | Loss: 0.00000988
Iteration 15/1000 | Loss: 0.00000985
Iteration 16/1000 | Loss: 0.00000980
Iteration 17/1000 | Loss: 0.00000976
Iteration 18/1000 | Loss: 0.00000974
Iteration 19/1000 | Loss: 0.00000974
Iteration 20/1000 | Loss: 0.00000974
Iteration 21/1000 | Loss: 0.00000972
Iteration 22/1000 | Loss: 0.00000970
Iteration 23/1000 | Loss: 0.00000970
Iteration 24/1000 | Loss: 0.00000970
Iteration 25/1000 | Loss: 0.00000970
Iteration 26/1000 | Loss: 0.00000970
Iteration 27/1000 | Loss: 0.00000970
Iteration 28/1000 | Loss: 0.00000970
Iteration 29/1000 | Loss: 0.00000970
Iteration 30/1000 | Loss: 0.00000970
Iteration 31/1000 | Loss: 0.00000970
Iteration 32/1000 | Loss: 0.00000970
Iteration 33/1000 | Loss: 0.00000970
Iteration 34/1000 | Loss: 0.00000970
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 34. Stopping optimization.
Last 5 losses: [9.701637281978037e-06, 9.701637281978037e-06, 9.701637281978037e-06, 9.701637281978037e-06, 9.701637281978037e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.701637281978037e-06

Optimization complete. Final v2v error: 2.71065092086792 mm

Highest mean error: 3.0573513507843018 mm for frame 87

Lowest mean error: 2.5953691005706787 mm for frame 1

Saving results

Total time: 47.61015009880066
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01036839
Iteration 2/25 | Loss: 0.01036839
Iteration 3/25 | Loss: 0.01036839
Iteration 4/25 | Loss: 0.01036838
Iteration 5/25 | Loss: 0.01036838
Iteration 6/25 | Loss: 0.01036838
Iteration 7/25 | Loss: 0.01036838
Iteration 8/25 | Loss: 0.01036838
Iteration 9/25 | Loss: 0.01036838
Iteration 10/25 | Loss: 0.01036838
Iteration 11/25 | Loss: 0.01036838
Iteration 12/25 | Loss: 0.01036838
Iteration 13/25 | Loss: 0.01036838
Iteration 14/25 | Loss: 0.01036837
Iteration 15/25 | Loss: 0.01036837
Iteration 16/25 | Loss: 0.01036837
Iteration 17/25 | Loss: 0.01036837
Iteration 18/25 | Loss: 0.01036837
Iteration 19/25 | Loss: 0.01036837
Iteration 20/25 | Loss: 0.01036837
Iteration 21/25 | Loss: 0.01036837
Iteration 22/25 | Loss: 0.01036837
Iteration 23/25 | Loss: 0.01036836
Iteration 24/25 | Loss: 0.01036836
Iteration 25/25 | Loss: 0.01036836

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64783967
Iteration 2/25 | Loss: 0.08654851
Iteration 3/25 | Loss: 0.08652186
Iteration 4/25 | Loss: 0.08652186
Iteration 5/25 | Loss: 0.08652185
Iteration 6/25 | Loss: 0.08652184
Iteration 7/25 | Loss: 0.08652184
Iteration 8/25 | Loss: 0.08652184
Iteration 9/25 | Loss: 0.08652183
Iteration 10/25 | Loss: 0.08652183
Iteration 11/25 | Loss: 0.08652183
Iteration 12/25 | Loss: 0.08652183
Iteration 13/25 | Loss: 0.08652183
Iteration 14/25 | Loss: 0.08652183
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.08652182668447495, 0.08652182668447495, 0.08652182668447495, 0.08652182668447495, 0.08652182668447495]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08652182668447495

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08652183
Iteration 2/1000 | Loss: 0.00651368
Iteration 3/1000 | Loss: 0.00337289
Iteration 4/1000 | Loss: 0.00708408
Iteration 5/1000 | Loss: 0.00090421
Iteration 6/1000 | Loss: 0.00130157
Iteration 7/1000 | Loss: 0.01110425
Iteration 8/1000 | Loss: 0.00067562
Iteration 9/1000 | Loss: 0.00277871
Iteration 10/1000 | Loss: 0.00313803
Iteration 11/1000 | Loss: 0.00093213
Iteration 12/1000 | Loss: 0.00217600
Iteration 13/1000 | Loss: 0.00120987
Iteration 14/1000 | Loss: 0.00205325
Iteration 15/1000 | Loss: 0.00061602
Iteration 16/1000 | Loss: 0.00030854
Iteration 17/1000 | Loss: 0.00043670
Iteration 18/1000 | Loss: 0.00016853
Iteration 19/1000 | Loss: 0.00035315
Iteration 20/1000 | Loss: 0.00296007
Iteration 21/1000 | Loss: 0.00217937
Iteration 22/1000 | Loss: 0.00164905
Iteration 23/1000 | Loss: 0.00024156
Iteration 24/1000 | Loss: 0.00181035
Iteration 25/1000 | Loss: 0.00128307
Iteration 26/1000 | Loss: 0.00031952
Iteration 27/1000 | Loss: 0.00008553
Iteration 28/1000 | Loss: 0.00006737
Iteration 29/1000 | Loss: 0.00014395
Iteration 30/1000 | Loss: 0.00007859
Iteration 31/1000 | Loss: 0.00020178
Iteration 32/1000 | Loss: 0.00024073
Iteration 33/1000 | Loss: 0.00007544
Iteration 34/1000 | Loss: 0.00006242
Iteration 35/1000 | Loss: 0.00004435
Iteration 36/1000 | Loss: 0.00004352
Iteration 37/1000 | Loss: 0.00028476
Iteration 38/1000 | Loss: 0.00088692
Iteration 39/1000 | Loss: 0.00004436
Iteration 40/1000 | Loss: 0.00007380
Iteration 41/1000 | Loss: 0.00003643
Iteration 42/1000 | Loss: 0.00003541
Iteration 43/1000 | Loss: 0.00003399
Iteration 44/1000 | Loss: 0.00023225
Iteration 45/1000 | Loss: 0.00003329
Iteration 46/1000 | Loss: 0.00003253
Iteration 47/1000 | Loss: 0.00004670
Iteration 48/1000 | Loss: 0.00029789
Iteration 49/1000 | Loss: 0.00004005
Iteration 50/1000 | Loss: 0.00003756
Iteration 51/1000 | Loss: 0.00004326
Iteration 52/1000 | Loss: 0.00012480
Iteration 53/1000 | Loss: 0.00003689
Iteration 54/1000 | Loss: 0.00026866
Iteration 55/1000 | Loss: 0.00022220
Iteration 56/1000 | Loss: 0.00004263
Iteration 57/1000 | Loss: 0.00003495
Iteration 58/1000 | Loss: 0.00003265
Iteration 59/1000 | Loss: 0.00003507
Iteration 60/1000 | Loss: 0.00003470
Iteration 61/1000 | Loss: 0.00035564
Iteration 62/1000 | Loss: 0.00056915
Iteration 63/1000 | Loss: 0.00018924
Iteration 64/1000 | Loss: 0.00032651
Iteration 65/1000 | Loss: 0.00003608
Iteration 66/1000 | Loss: 0.00012282
Iteration 67/1000 | Loss: 0.00003357
Iteration 68/1000 | Loss: 0.00018010
Iteration 69/1000 | Loss: 0.00125461
Iteration 70/1000 | Loss: 0.00003632
Iteration 71/1000 | Loss: 0.00005277
Iteration 72/1000 | Loss: 0.00004765
Iteration 73/1000 | Loss: 0.00005448
Iteration 74/1000 | Loss: 0.00004507
Iteration 75/1000 | Loss: 0.00043842
Iteration 76/1000 | Loss: 0.00016674
Iteration 77/1000 | Loss: 0.00003401
Iteration 78/1000 | Loss: 0.00003450
Iteration 79/1000 | Loss: 0.00003881
Iteration 80/1000 | Loss: 0.00028134
Iteration 81/1000 | Loss: 0.00006440
Iteration 82/1000 | Loss: 0.00003737
Iteration 83/1000 | Loss: 0.00005080
Iteration 84/1000 | Loss: 0.00019892
Iteration 85/1000 | Loss: 0.00028176
Iteration 86/1000 | Loss: 0.00009687
Iteration 87/1000 | Loss: 0.00018074
Iteration 88/1000 | Loss: 0.00007398
Iteration 89/1000 | Loss: 0.00027307
Iteration 90/1000 | Loss: 0.00005617
Iteration 91/1000 | Loss: 0.00003436
Iteration 92/1000 | Loss: 0.00005322
Iteration 93/1000 | Loss: 0.00003294
Iteration 94/1000 | Loss: 0.00003130
Iteration 95/1000 | Loss: 0.00003038
Iteration 96/1000 | Loss: 0.00006702
Iteration 97/1000 | Loss: 0.00002987
Iteration 98/1000 | Loss: 0.00002962
Iteration 99/1000 | Loss: 0.00002937
Iteration 100/1000 | Loss: 0.00010800
Iteration 101/1000 | Loss: 0.00002907
Iteration 102/1000 | Loss: 0.00002898
Iteration 103/1000 | Loss: 0.00002889
Iteration 104/1000 | Loss: 0.00002874
Iteration 105/1000 | Loss: 0.00002873
Iteration 106/1000 | Loss: 0.00002873
Iteration 107/1000 | Loss: 0.00002872
Iteration 108/1000 | Loss: 0.00002871
Iteration 109/1000 | Loss: 0.00002871
Iteration 110/1000 | Loss: 0.00002867
Iteration 111/1000 | Loss: 0.00002864
Iteration 112/1000 | Loss: 0.00002863
Iteration 113/1000 | Loss: 0.00002862
Iteration 114/1000 | Loss: 0.00014031
Iteration 115/1000 | Loss: 0.00017230
Iteration 116/1000 | Loss: 0.00002903
Iteration 117/1000 | Loss: 0.00002861
Iteration 118/1000 | Loss: 0.00002854
Iteration 119/1000 | Loss: 0.00002854
Iteration 120/1000 | Loss: 0.00002854
Iteration 121/1000 | Loss: 0.00002854
Iteration 122/1000 | Loss: 0.00002853
Iteration 123/1000 | Loss: 0.00002853
Iteration 124/1000 | Loss: 0.00002853
Iteration 125/1000 | Loss: 0.00002852
Iteration 126/1000 | Loss: 0.00002851
Iteration 127/1000 | Loss: 0.00002851
Iteration 128/1000 | Loss: 0.00002850
Iteration 129/1000 | Loss: 0.00002850
Iteration 130/1000 | Loss: 0.00002850
Iteration 131/1000 | Loss: 0.00002849
Iteration 132/1000 | Loss: 0.00002849
Iteration 133/1000 | Loss: 0.00002849
Iteration 134/1000 | Loss: 0.00002849
Iteration 135/1000 | Loss: 0.00002849
Iteration 136/1000 | Loss: 0.00002849
Iteration 137/1000 | Loss: 0.00002849
Iteration 138/1000 | Loss: 0.00002848
Iteration 139/1000 | Loss: 0.00002848
Iteration 140/1000 | Loss: 0.00002848
Iteration 141/1000 | Loss: 0.00002848
Iteration 142/1000 | Loss: 0.00002847
Iteration 143/1000 | Loss: 0.00002847
Iteration 144/1000 | Loss: 0.00002847
Iteration 145/1000 | Loss: 0.00002847
Iteration 146/1000 | Loss: 0.00002847
Iteration 147/1000 | Loss: 0.00002846
Iteration 148/1000 | Loss: 0.00002846
Iteration 149/1000 | Loss: 0.00002846
Iteration 150/1000 | Loss: 0.00002845
Iteration 151/1000 | Loss: 0.00002845
Iteration 152/1000 | Loss: 0.00002845
Iteration 153/1000 | Loss: 0.00002845
Iteration 154/1000 | Loss: 0.00002845
Iteration 155/1000 | Loss: 0.00002845
Iteration 156/1000 | Loss: 0.00002844
Iteration 157/1000 | Loss: 0.00002844
Iteration 158/1000 | Loss: 0.00002844
Iteration 159/1000 | Loss: 0.00002843
Iteration 160/1000 | Loss: 0.00002843
Iteration 161/1000 | Loss: 0.00002843
Iteration 162/1000 | Loss: 0.00002843
Iteration 163/1000 | Loss: 0.00002843
Iteration 164/1000 | Loss: 0.00002843
Iteration 165/1000 | Loss: 0.00002842
Iteration 166/1000 | Loss: 0.00002842
Iteration 167/1000 | Loss: 0.00002842
Iteration 168/1000 | Loss: 0.00002842
Iteration 169/1000 | Loss: 0.00002842
Iteration 170/1000 | Loss: 0.00002842
Iteration 171/1000 | Loss: 0.00002841
Iteration 172/1000 | Loss: 0.00002841
Iteration 173/1000 | Loss: 0.00002841
Iteration 174/1000 | Loss: 0.00002841
Iteration 175/1000 | Loss: 0.00002841
Iteration 176/1000 | Loss: 0.00002841
Iteration 177/1000 | Loss: 0.00002841
Iteration 178/1000 | Loss: 0.00002841
Iteration 179/1000 | Loss: 0.00002840
Iteration 180/1000 | Loss: 0.00002840
Iteration 181/1000 | Loss: 0.00002840
Iteration 182/1000 | Loss: 0.00002840
Iteration 183/1000 | Loss: 0.00002840
Iteration 184/1000 | Loss: 0.00002840
Iteration 185/1000 | Loss: 0.00002840
Iteration 186/1000 | Loss: 0.00002840
Iteration 187/1000 | Loss: 0.00002840
Iteration 188/1000 | Loss: 0.00002839
Iteration 189/1000 | Loss: 0.00002839
Iteration 190/1000 | Loss: 0.00002839
Iteration 191/1000 | Loss: 0.00002839
Iteration 192/1000 | Loss: 0.00002839
Iteration 193/1000 | Loss: 0.00002838
Iteration 194/1000 | Loss: 0.00002838
Iteration 195/1000 | Loss: 0.00002838
Iteration 196/1000 | Loss: 0.00002838
Iteration 197/1000 | Loss: 0.00002838
Iteration 198/1000 | Loss: 0.00002838
Iteration 199/1000 | Loss: 0.00002837
Iteration 200/1000 | Loss: 0.00002837
Iteration 201/1000 | Loss: 0.00002837
Iteration 202/1000 | Loss: 0.00002837
Iteration 203/1000 | Loss: 0.00002837
Iteration 204/1000 | Loss: 0.00002837
Iteration 205/1000 | Loss: 0.00002837
Iteration 206/1000 | Loss: 0.00002837
Iteration 207/1000 | Loss: 0.00002837
Iteration 208/1000 | Loss: 0.00002837
Iteration 209/1000 | Loss: 0.00002837
Iteration 210/1000 | Loss: 0.00002837
Iteration 211/1000 | Loss: 0.00002837
Iteration 212/1000 | Loss: 0.00002837
Iteration 213/1000 | Loss: 0.00002837
Iteration 214/1000 | Loss: 0.00002837
Iteration 215/1000 | Loss: 0.00002837
Iteration 216/1000 | Loss: 0.00002836
Iteration 217/1000 | Loss: 0.00002836
Iteration 218/1000 | Loss: 0.00002836
Iteration 219/1000 | Loss: 0.00002836
Iteration 220/1000 | Loss: 0.00002836
Iteration 221/1000 | Loss: 0.00002836
Iteration 222/1000 | Loss: 0.00002836
Iteration 223/1000 | Loss: 0.00002836
Iteration 224/1000 | Loss: 0.00002836
Iteration 225/1000 | Loss: 0.00002836
Iteration 226/1000 | Loss: 0.00002836
Iteration 227/1000 | Loss: 0.00002836
Iteration 228/1000 | Loss: 0.00002836
Iteration 229/1000 | Loss: 0.00002836
Iteration 230/1000 | Loss: 0.00002836
Iteration 231/1000 | Loss: 0.00002836
Iteration 232/1000 | Loss: 0.00002836
Iteration 233/1000 | Loss: 0.00002836
Iteration 234/1000 | Loss: 0.00002836
Iteration 235/1000 | Loss: 0.00002836
Iteration 236/1000 | Loss: 0.00002836
Iteration 237/1000 | Loss: 0.00002836
Iteration 238/1000 | Loss: 0.00002836
Iteration 239/1000 | Loss: 0.00002836
Iteration 240/1000 | Loss: 0.00002836
Iteration 241/1000 | Loss: 0.00002835
Iteration 242/1000 | Loss: 0.00002835
Iteration 243/1000 | Loss: 0.00002835
Iteration 244/1000 | Loss: 0.00002835
Iteration 245/1000 | Loss: 0.00002835
Iteration 246/1000 | Loss: 0.00002835
Iteration 247/1000 | Loss: 0.00002835
Iteration 248/1000 | Loss: 0.00002835
Iteration 249/1000 | Loss: 0.00002835
Iteration 250/1000 | Loss: 0.00002835
Iteration 251/1000 | Loss: 0.00002835
Iteration 252/1000 | Loss: 0.00002835
Iteration 253/1000 | Loss: 0.00002835
Iteration 254/1000 | Loss: 0.00002835
Iteration 255/1000 | Loss: 0.00002835
Iteration 256/1000 | Loss: 0.00002834
Iteration 257/1000 | Loss: 0.00002834
Iteration 258/1000 | Loss: 0.00002834
Iteration 259/1000 | Loss: 0.00002834
Iteration 260/1000 | Loss: 0.00002834
Iteration 261/1000 | Loss: 0.00002834
Iteration 262/1000 | Loss: 0.00002834
Iteration 263/1000 | Loss: 0.00002834
Iteration 264/1000 | Loss: 0.00002834
Iteration 265/1000 | Loss: 0.00002834
Iteration 266/1000 | Loss: 0.00002833
Iteration 267/1000 | Loss: 0.00002833
Iteration 268/1000 | Loss: 0.00002833
Iteration 269/1000 | Loss: 0.00002833
Iteration 270/1000 | Loss: 0.00002832
Iteration 271/1000 | Loss: 0.00002832
Iteration 272/1000 | Loss: 0.00011584
Iteration 273/1000 | Loss: 0.00014694
Iteration 274/1000 | Loss: 0.00005648
Iteration 275/1000 | Loss: 0.00002855
Iteration 276/1000 | Loss: 0.00010338
Iteration 277/1000 | Loss: 0.00002834
Iteration 278/1000 | Loss: 0.00002829
Iteration 279/1000 | Loss: 0.00002829
Iteration 280/1000 | Loss: 0.00002827
Iteration 281/1000 | Loss: 0.00002827
Iteration 282/1000 | Loss: 0.00002826
Iteration 283/1000 | Loss: 0.00002826
Iteration 284/1000 | Loss: 0.00002826
Iteration 285/1000 | Loss: 0.00002826
Iteration 286/1000 | Loss: 0.00002825
Iteration 287/1000 | Loss: 0.00002825
Iteration 288/1000 | Loss: 0.00002825
Iteration 289/1000 | Loss: 0.00002825
Iteration 290/1000 | Loss: 0.00002825
Iteration 291/1000 | Loss: 0.00002825
Iteration 292/1000 | Loss: 0.00002825
Iteration 293/1000 | Loss: 0.00002825
Iteration 294/1000 | Loss: 0.00002825
Iteration 295/1000 | Loss: 0.00002825
Iteration 296/1000 | Loss: 0.00002825
Iteration 297/1000 | Loss: 0.00002825
Iteration 298/1000 | Loss: 0.00002825
Iteration 299/1000 | Loss: 0.00002825
Iteration 300/1000 | Loss: 0.00002825
Iteration 301/1000 | Loss: 0.00002825
Iteration 302/1000 | Loss: 0.00002825
Iteration 303/1000 | Loss: 0.00002824
Iteration 304/1000 | Loss: 0.00002824
Iteration 305/1000 | Loss: 0.00002824
Iteration 306/1000 | Loss: 0.00002824
Iteration 307/1000 | Loss: 0.00002824
Iteration 308/1000 | Loss: 0.00002824
Iteration 309/1000 | Loss: 0.00002824
Iteration 310/1000 | Loss: 0.00002824
Iteration 311/1000 | Loss: 0.00002824
Iteration 312/1000 | Loss: 0.00002824
Iteration 313/1000 | Loss: 0.00002824
Iteration 314/1000 | Loss: 0.00002824
Iteration 315/1000 | Loss: 0.00002823
Iteration 316/1000 | Loss: 0.00002823
Iteration 317/1000 | Loss: 0.00002823
Iteration 318/1000 | Loss: 0.00002823
Iteration 319/1000 | Loss: 0.00002823
Iteration 320/1000 | Loss: 0.00002823
Iteration 321/1000 | Loss: 0.00002823
Iteration 322/1000 | Loss: 0.00002823
Iteration 323/1000 | Loss: 0.00002823
Iteration 324/1000 | Loss: 0.00002823
Iteration 325/1000 | Loss: 0.00002823
Iteration 326/1000 | Loss: 0.00002823
Iteration 327/1000 | Loss: 0.00002823
Iteration 328/1000 | Loss: 0.00002823
Iteration 329/1000 | Loss: 0.00002823
Iteration 330/1000 | Loss: 0.00002823
Iteration 331/1000 | Loss: 0.00002823
Iteration 332/1000 | Loss: 0.00002823
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 332. Stopping optimization.
Last 5 losses: [2.8227366783539765e-05, 2.8227366783539765e-05, 2.8227366783539765e-05, 2.8227366783539765e-05, 2.8227366783539765e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8227366783539765e-05

Optimization complete. Final v2v error: 3.9861857891082764 mm

Highest mean error: 6.451955318450928 mm for frame 39

Lowest mean error: 3.0442328453063965 mm for frame 90

Saving results

Total time: 202.03051853179932
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00455666
Iteration 2/25 | Loss: 0.00128100
Iteration 3/25 | Loss: 0.00118508
Iteration 4/25 | Loss: 0.00117607
Iteration 5/25 | Loss: 0.00117387
Iteration 6/25 | Loss: 0.00117356
Iteration 7/25 | Loss: 0.00117356
Iteration 8/25 | Loss: 0.00117356
Iteration 9/25 | Loss: 0.00117356
Iteration 10/25 | Loss: 0.00117356
Iteration 11/25 | Loss: 0.00117356
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011735563166439533, 0.0011735563166439533, 0.0011735563166439533, 0.0011735563166439533, 0.0011735563166439533]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011735563166439533

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30809975
Iteration 2/25 | Loss: 0.00146599
Iteration 3/25 | Loss: 0.00146598
Iteration 4/25 | Loss: 0.00146598
Iteration 5/25 | Loss: 0.00146598
Iteration 6/25 | Loss: 0.00146598
Iteration 7/25 | Loss: 0.00146598
Iteration 8/25 | Loss: 0.00146598
Iteration 9/25 | Loss: 0.00146597
Iteration 10/25 | Loss: 0.00146597
Iteration 11/25 | Loss: 0.00146597
Iteration 12/25 | Loss: 0.00146597
Iteration 13/25 | Loss: 0.00146597
Iteration 14/25 | Loss: 0.00146597
Iteration 15/25 | Loss: 0.00146597
Iteration 16/25 | Loss: 0.00146597
Iteration 17/25 | Loss: 0.00146597
Iteration 18/25 | Loss: 0.00146597
Iteration 19/25 | Loss: 0.00146597
Iteration 20/25 | Loss: 0.00146597
Iteration 21/25 | Loss: 0.00146597
Iteration 22/25 | Loss: 0.00146597
Iteration 23/25 | Loss: 0.00146597
Iteration 24/25 | Loss: 0.00146597
Iteration 25/25 | Loss: 0.00146597

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00146597
Iteration 2/1000 | Loss: 0.00002978
Iteration 3/1000 | Loss: 0.00001647
Iteration 4/1000 | Loss: 0.00001328
Iteration 5/1000 | Loss: 0.00001204
Iteration 6/1000 | Loss: 0.00001126
Iteration 7/1000 | Loss: 0.00001085
Iteration 8/1000 | Loss: 0.00001043
Iteration 9/1000 | Loss: 0.00001018
Iteration 10/1000 | Loss: 0.00001016
Iteration 11/1000 | Loss: 0.00001006
Iteration 12/1000 | Loss: 0.00001004
Iteration 13/1000 | Loss: 0.00000996
Iteration 14/1000 | Loss: 0.00000981
Iteration 15/1000 | Loss: 0.00000979
Iteration 16/1000 | Loss: 0.00000978
Iteration 17/1000 | Loss: 0.00000976
Iteration 18/1000 | Loss: 0.00000974
Iteration 19/1000 | Loss: 0.00000974
Iteration 20/1000 | Loss: 0.00000973
Iteration 21/1000 | Loss: 0.00000970
Iteration 22/1000 | Loss: 0.00000968
Iteration 23/1000 | Loss: 0.00000968
Iteration 24/1000 | Loss: 0.00000968
Iteration 25/1000 | Loss: 0.00000967
Iteration 26/1000 | Loss: 0.00000963
Iteration 27/1000 | Loss: 0.00000963
Iteration 28/1000 | Loss: 0.00000963
Iteration 29/1000 | Loss: 0.00000962
Iteration 30/1000 | Loss: 0.00000961
Iteration 31/1000 | Loss: 0.00000961
Iteration 32/1000 | Loss: 0.00000960
Iteration 33/1000 | Loss: 0.00000959
Iteration 34/1000 | Loss: 0.00000958
Iteration 35/1000 | Loss: 0.00000957
Iteration 36/1000 | Loss: 0.00000957
Iteration 37/1000 | Loss: 0.00000957
Iteration 38/1000 | Loss: 0.00000955
Iteration 39/1000 | Loss: 0.00000954
Iteration 40/1000 | Loss: 0.00000953
Iteration 41/1000 | Loss: 0.00000952
Iteration 42/1000 | Loss: 0.00000952
Iteration 43/1000 | Loss: 0.00000951
Iteration 44/1000 | Loss: 0.00000951
Iteration 45/1000 | Loss: 0.00000948
Iteration 46/1000 | Loss: 0.00000948
Iteration 47/1000 | Loss: 0.00000948
Iteration 48/1000 | Loss: 0.00000948
Iteration 49/1000 | Loss: 0.00000948
Iteration 50/1000 | Loss: 0.00000948
Iteration 51/1000 | Loss: 0.00000948
Iteration 52/1000 | Loss: 0.00000948
Iteration 53/1000 | Loss: 0.00000948
Iteration 54/1000 | Loss: 0.00000948
Iteration 55/1000 | Loss: 0.00000947
Iteration 56/1000 | Loss: 0.00000947
Iteration 57/1000 | Loss: 0.00000947
Iteration 58/1000 | Loss: 0.00000947
Iteration 59/1000 | Loss: 0.00000947
Iteration 60/1000 | Loss: 0.00000946
Iteration 61/1000 | Loss: 0.00000945
Iteration 62/1000 | Loss: 0.00000945
Iteration 63/1000 | Loss: 0.00000944
Iteration 64/1000 | Loss: 0.00000944
Iteration 65/1000 | Loss: 0.00000944
Iteration 66/1000 | Loss: 0.00000943
Iteration 67/1000 | Loss: 0.00000943
Iteration 68/1000 | Loss: 0.00000942
Iteration 69/1000 | Loss: 0.00000942
Iteration 70/1000 | Loss: 0.00000942
Iteration 71/1000 | Loss: 0.00000941
Iteration 72/1000 | Loss: 0.00000941
Iteration 73/1000 | Loss: 0.00000940
Iteration 74/1000 | Loss: 0.00000940
Iteration 75/1000 | Loss: 0.00000940
Iteration 76/1000 | Loss: 0.00000939
Iteration 77/1000 | Loss: 0.00000938
Iteration 78/1000 | Loss: 0.00000937
Iteration 79/1000 | Loss: 0.00000937
Iteration 80/1000 | Loss: 0.00000937
Iteration 81/1000 | Loss: 0.00000937
Iteration 82/1000 | Loss: 0.00000937
Iteration 83/1000 | Loss: 0.00000936
Iteration 84/1000 | Loss: 0.00000936
Iteration 85/1000 | Loss: 0.00000936
Iteration 86/1000 | Loss: 0.00000936
Iteration 87/1000 | Loss: 0.00000936
Iteration 88/1000 | Loss: 0.00000936
Iteration 89/1000 | Loss: 0.00000935
Iteration 90/1000 | Loss: 0.00000935
Iteration 91/1000 | Loss: 0.00000934
Iteration 92/1000 | Loss: 0.00000934
Iteration 93/1000 | Loss: 0.00000934
Iteration 94/1000 | Loss: 0.00000934
Iteration 95/1000 | Loss: 0.00000934
Iteration 96/1000 | Loss: 0.00000934
Iteration 97/1000 | Loss: 0.00000933
Iteration 98/1000 | Loss: 0.00000933
Iteration 99/1000 | Loss: 0.00000933
Iteration 100/1000 | Loss: 0.00000933
Iteration 101/1000 | Loss: 0.00000933
Iteration 102/1000 | Loss: 0.00000933
Iteration 103/1000 | Loss: 0.00000933
Iteration 104/1000 | Loss: 0.00000933
Iteration 105/1000 | Loss: 0.00000932
Iteration 106/1000 | Loss: 0.00000932
Iteration 107/1000 | Loss: 0.00000932
Iteration 108/1000 | Loss: 0.00000932
Iteration 109/1000 | Loss: 0.00000932
Iteration 110/1000 | Loss: 0.00000931
Iteration 111/1000 | Loss: 0.00000931
Iteration 112/1000 | Loss: 0.00000930
Iteration 113/1000 | Loss: 0.00000930
Iteration 114/1000 | Loss: 0.00000930
Iteration 115/1000 | Loss: 0.00000930
Iteration 116/1000 | Loss: 0.00000930
Iteration 117/1000 | Loss: 0.00000929
Iteration 118/1000 | Loss: 0.00000929
Iteration 119/1000 | Loss: 0.00000929
Iteration 120/1000 | Loss: 0.00000929
Iteration 121/1000 | Loss: 0.00000929
Iteration 122/1000 | Loss: 0.00000929
Iteration 123/1000 | Loss: 0.00000928
Iteration 124/1000 | Loss: 0.00000928
Iteration 125/1000 | Loss: 0.00000928
Iteration 126/1000 | Loss: 0.00000927
Iteration 127/1000 | Loss: 0.00000927
Iteration 128/1000 | Loss: 0.00000927
Iteration 129/1000 | Loss: 0.00000926
Iteration 130/1000 | Loss: 0.00000926
Iteration 131/1000 | Loss: 0.00000926
Iteration 132/1000 | Loss: 0.00000926
Iteration 133/1000 | Loss: 0.00000926
Iteration 134/1000 | Loss: 0.00000926
Iteration 135/1000 | Loss: 0.00000926
Iteration 136/1000 | Loss: 0.00000926
Iteration 137/1000 | Loss: 0.00000926
Iteration 138/1000 | Loss: 0.00000925
Iteration 139/1000 | Loss: 0.00000925
Iteration 140/1000 | Loss: 0.00000924
Iteration 141/1000 | Loss: 0.00000924
Iteration 142/1000 | Loss: 0.00000924
Iteration 143/1000 | Loss: 0.00000924
Iteration 144/1000 | Loss: 0.00000924
Iteration 145/1000 | Loss: 0.00000923
Iteration 146/1000 | Loss: 0.00000923
Iteration 147/1000 | Loss: 0.00000923
Iteration 148/1000 | Loss: 0.00000923
Iteration 149/1000 | Loss: 0.00000922
Iteration 150/1000 | Loss: 0.00000922
Iteration 151/1000 | Loss: 0.00000922
Iteration 152/1000 | Loss: 0.00000922
Iteration 153/1000 | Loss: 0.00000922
Iteration 154/1000 | Loss: 0.00000921
Iteration 155/1000 | Loss: 0.00000921
Iteration 156/1000 | Loss: 0.00000921
Iteration 157/1000 | Loss: 0.00000920
Iteration 158/1000 | Loss: 0.00000920
Iteration 159/1000 | Loss: 0.00000919
Iteration 160/1000 | Loss: 0.00000919
Iteration 161/1000 | Loss: 0.00000919
Iteration 162/1000 | Loss: 0.00000919
Iteration 163/1000 | Loss: 0.00000919
Iteration 164/1000 | Loss: 0.00000918
Iteration 165/1000 | Loss: 0.00000918
Iteration 166/1000 | Loss: 0.00000918
Iteration 167/1000 | Loss: 0.00000918
Iteration 168/1000 | Loss: 0.00000918
Iteration 169/1000 | Loss: 0.00000917
Iteration 170/1000 | Loss: 0.00000917
Iteration 171/1000 | Loss: 0.00000917
Iteration 172/1000 | Loss: 0.00000917
Iteration 173/1000 | Loss: 0.00000917
Iteration 174/1000 | Loss: 0.00000917
Iteration 175/1000 | Loss: 0.00000917
Iteration 176/1000 | Loss: 0.00000917
Iteration 177/1000 | Loss: 0.00000917
Iteration 178/1000 | Loss: 0.00000917
Iteration 179/1000 | Loss: 0.00000917
Iteration 180/1000 | Loss: 0.00000916
Iteration 181/1000 | Loss: 0.00000916
Iteration 182/1000 | Loss: 0.00000916
Iteration 183/1000 | Loss: 0.00000916
Iteration 184/1000 | Loss: 0.00000916
Iteration 185/1000 | Loss: 0.00000916
Iteration 186/1000 | Loss: 0.00000915
Iteration 187/1000 | Loss: 0.00000915
Iteration 188/1000 | Loss: 0.00000915
Iteration 189/1000 | Loss: 0.00000915
Iteration 190/1000 | Loss: 0.00000915
Iteration 191/1000 | Loss: 0.00000915
Iteration 192/1000 | Loss: 0.00000915
Iteration 193/1000 | Loss: 0.00000915
Iteration 194/1000 | Loss: 0.00000915
Iteration 195/1000 | Loss: 0.00000914
Iteration 196/1000 | Loss: 0.00000914
Iteration 197/1000 | Loss: 0.00000914
Iteration 198/1000 | Loss: 0.00000914
Iteration 199/1000 | Loss: 0.00000914
Iteration 200/1000 | Loss: 0.00000914
Iteration 201/1000 | Loss: 0.00000914
Iteration 202/1000 | Loss: 0.00000914
Iteration 203/1000 | Loss: 0.00000914
Iteration 204/1000 | Loss: 0.00000914
Iteration 205/1000 | Loss: 0.00000914
Iteration 206/1000 | Loss: 0.00000914
Iteration 207/1000 | Loss: 0.00000914
Iteration 208/1000 | Loss: 0.00000914
Iteration 209/1000 | Loss: 0.00000914
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [9.140599104284775e-06, 9.140599104284775e-06, 9.140599104284775e-06, 9.140599104284775e-06, 9.140599104284775e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.140599104284775e-06

Optimization complete. Final v2v error: 2.521304130554199 mm

Highest mean error: 2.929074287414551 mm for frame 61

Lowest mean error: 2.3545796871185303 mm for frame 97

Saving results

Total time: 40.320080518722534
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00655156
Iteration 2/25 | Loss: 0.00133590
Iteration 3/25 | Loss: 0.00122894
Iteration 4/25 | Loss: 0.00121025
Iteration 5/25 | Loss: 0.00120434
Iteration 6/25 | Loss: 0.00120427
Iteration 7/25 | Loss: 0.00120427
Iteration 8/25 | Loss: 0.00120427
Iteration 9/25 | Loss: 0.00120427
Iteration 10/25 | Loss: 0.00120427
Iteration 11/25 | Loss: 0.00120427
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012042727321386337, 0.0012042727321386337, 0.0012042727321386337, 0.0012042727321386337, 0.0012042727321386337]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012042727321386337

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29256845
Iteration 2/25 | Loss: 0.00139007
Iteration 3/25 | Loss: 0.00139006
Iteration 4/25 | Loss: 0.00139006
Iteration 5/25 | Loss: 0.00139006
Iteration 6/25 | Loss: 0.00139006
Iteration 7/25 | Loss: 0.00139006
Iteration 8/25 | Loss: 0.00139006
Iteration 9/25 | Loss: 0.00139006
Iteration 10/25 | Loss: 0.00139006
Iteration 11/25 | Loss: 0.00139006
Iteration 12/25 | Loss: 0.00139006
Iteration 13/25 | Loss: 0.00139006
Iteration 14/25 | Loss: 0.00139006
Iteration 15/25 | Loss: 0.00139006
Iteration 16/25 | Loss: 0.00139006
Iteration 17/25 | Loss: 0.00139006
Iteration 18/25 | Loss: 0.00139006
Iteration 19/25 | Loss: 0.00139006
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0013900580815970898, 0.0013900580815970898, 0.0013900580815970898, 0.0013900580815970898, 0.0013900580815970898]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013900580815970898

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00139006
Iteration 2/1000 | Loss: 0.00003781
Iteration 3/1000 | Loss: 0.00002351
Iteration 4/1000 | Loss: 0.00002139
Iteration 5/1000 | Loss: 0.00002066
Iteration 6/1000 | Loss: 0.00002008
Iteration 7/1000 | Loss: 0.00001963
Iteration 8/1000 | Loss: 0.00001928
Iteration 9/1000 | Loss: 0.00001890
Iteration 10/1000 | Loss: 0.00001858
Iteration 11/1000 | Loss: 0.00001830
Iteration 12/1000 | Loss: 0.00001804
Iteration 13/1000 | Loss: 0.00001793
Iteration 14/1000 | Loss: 0.00001777
Iteration 15/1000 | Loss: 0.00001776
Iteration 16/1000 | Loss: 0.00001775
Iteration 17/1000 | Loss: 0.00001765
Iteration 18/1000 | Loss: 0.00001756
Iteration 19/1000 | Loss: 0.00001755
Iteration 20/1000 | Loss: 0.00001755
Iteration 21/1000 | Loss: 0.00001754
Iteration 22/1000 | Loss: 0.00001754
Iteration 23/1000 | Loss: 0.00001754
Iteration 24/1000 | Loss: 0.00001753
Iteration 25/1000 | Loss: 0.00001751
Iteration 26/1000 | Loss: 0.00001751
Iteration 27/1000 | Loss: 0.00001751
Iteration 28/1000 | Loss: 0.00001751
Iteration 29/1000 | Loss: 0.00001751
Iteration 30/1000 | Loss: 0.00001751
Iteration 31/1000 | Loss: 0.00001751
Iteration 32/1000 | Loss: 0.00001750
Iteration 33/1000 | Loss: 0.00001750
Iteration 34/1000 | Loss: 0.00001750
Iteration 35/1000 | Loss: 0.00001750
Iteration 36/1000 | Loss: 0.00001750
Iteration 37/1000 | Loss: 0.00001749
Iteration 38/1000 | Loss: 0.00001749
Iteration 39/1000 | Loss: 0.00001749
Iteration 40/1000 | Loss: 0.00001748
Iteration 41/1000 | Loss: 0.00001748
Iteration 42/1000 | Loss: 0.00001748
Iteration 43/1000 | Loss: 0.00001748
Iteration 44/1000 | Loss: 0.00001747
Iteration 45/1000 | Loss: 0.00001747
Iteration 46/1000 | Loss: 0.00001747
Iteration 47/1000 | Loss: 0.00001746
Iteration 48/1000 | Loss: 0.00001746
Iteration 49/1000 | Loss: 0.00001746
Iteration 50/1000 | Loss: 0.00001746
Iteration 51/1000 | Loss: 0.00001746
Iteration 52/1000 | Loss: 0.00001746
Iteration 53/1000 | Loss: 0.00001746
Iteration 54/1000 | Loss: 0.00001746
Iteration 55/1000 | Loss: 0.00001745
Iteration 56/1000 | Loss: 0.00001745
Iteration 57/1000 | Loss: 0.00001745
Iteration 58/1000 | Loss: 0.00001745
Iteration 59/1000 | Loss: 0.00001745
Iteration 60/1000 | Loss: 0.00001745
Iteration 61/1000 | Loss: 0.00001745
Iteration 62/1000 | Loss: 0.00001744
Iteration 63/1000 | Loss: 0.00001744
Iteration 64/1000 | Loss: 0.00001744
Iteration 65/1000 | Loss: 0.00001744
Iteration 66/1000 | Loss: 0.00001744
Iteration 67/1000 | Loss: 0.00001744
Iteration 68/1000 | Loss: 0.00001744
Iteration 69/1000 | Loss: 0.00001744
Iteration 70/1000 | Loss: 0.00001744
Iteration 71/1000 | Loss: 0.00001744
Iteration 72/1000 | Loss: 0.00001744
Iteration 73/1000 | Loss: 0.00001743
Iteration 74/1000 | Loss: 0.00001743
Iteration 75/1000 | Loss: 0.00001743
Iteration 76/1000 | Loss: 0.00001743
Iteration 77/1000 | Loss: 0.00001743
Iteration 78/1000 | Loss: 0.00001743
Iteration 79/1000 | Loss: 0.00001743
Iteration 80/1000 | Loss: 0.00001743
Iteration 81/1000 | Loss: 0.00001743
Iteration 82/1000 | Loss: 0.00001743
Iteration 83/1000 | Loss: 0.00001743
Iteration 84/1000 | Loss: 0.00001742
Iteration 85/1000 | Loss: 0.00001742
Iteration 86/1000 | Loss: 0.00001742
Iteration 87/1000 | Loss: 0.00001742
Iteration 88/1000 | Loss: 0.00001742
Iteration 89/1000 | Loss: 0.00001742
Iteration 90/1000 | Loss: 0.00001742
Iteration 91/1000 | Loss: 0.00001742
Iteration 92/1000 | Loss: 0.00001742
Iteration 93/1000 | Loss: 0.00001742
Iteration 94/1000 | Loss: 0.00001742
Iteration 95/1000 | Loss: 0.00001742
Iteration 96/1000 | Loss: 0.00001741
Iteration 97/1000 | Loss: 0.00001741
Iteration 98/1000 | Loss: 0.00001741
Iteration 99/1000 | Loss: 0.00001741
Iteration 100/1000 | Loss: 0.00001741
Iteration 101/1000 | Loss: 0.00001741
Iteration 102/1000 | Loss: 0.00001741
Iteration 103/1000 | Loss: 0.00001741
Iteration 104/1000 | Loss: 0.00001740
Iteration 105/1000 | Loss: 0.00001740
Iteration 106/1000 | Loss: 0.00001740
Iteration 107/1000 | Loss: 0.00001740
Iteration 108/1000 | Loss: 0.00001740
Iteration 109/1000 | Loss: 0.00001739
Iteration 110/1000 | Loss: 0.00001739
Iteration 111/1000 | Loss: 0.00001739
Iteration 112/1000 | Loss: 0.00001739
Iteration 113/1000 | Loss: 0.00001739
Iteration 114/1000 | Loss: 0.00001738
Iteration 115/1000 | Loss: 0.00001738
Iteration 116/1000 | Loss: 0.00001738
Iteration 117/1000 | Loss: 0.00001738
Iteration 118/1000 | Loss: 0.00001738
Iteration 119/1000 | Loss: 0.00001737
Iteration 120/1000 | Loss: 0.00001737
Iteration 121/1000 | Loss: 0.00001737
Iteration 122/1000 | Loss: 0.00001737
Iteration 123/1000 | Loss: 0.00001737
Iteration 124/1000 | Loss: 0.00001737
Iteration 125/1000 | Loss: 0.00001737
Iteration 126/1000 | Loss: 0.00001737
Iteration 127/1000 | Loss: 0.00001737
Iteration 128/1000 | Loss: 0.00001736
Iteration 129/1000 | Loss: 0.00001736
Iteration 130/1000 | Loss: 0.00001736
Iteration 131/1000 | Loss: 0.00001736
Iteration 132/1000 | Loss: 0.00001736
Iteration 133/1000 | Loss: 0.00001736
Iteration 134/1000 | Loss: 0.00001736
Iteration 135/1000 | Loss: 0.00001736
Iteration 136/1000 | Loss: 0.00001735
Iteration 137/1000 | Loss: 0.00001735
Iteration 138/1000 | Loss: 0.00001735
Iteration 139/1000 | Loss: 0.00001735
Iteration 140/1000 | Loss: 0.00001735
Iteration 141/1000 | Loss: 0.00001735
Iteration 142/1000 | Loss: 0.00001735
Iteration 143/1000 | Loss: 0.00001735
Iteration 144/1000 | Loss: 0.00001735
Iteration 145/1000 | Loss: 0.00001735
Iteration 146/1000 | Loss: 0.00001734
Iteration 147/1000 | Loss: 0.00001734
Iteration 148/1000 | Loss: 0.00001734
Iteration 149/1000 | Loss: 0.00001734
Iteration 150/1000 | Loss: 0.00001734
Iteration 151/1000 | Loss: 0.00001734
Iteration 152/1000 | Loss: 0.00001733
Iteration 153/1000 | Loss: 0.00001733
Iteration 154/1000 | Loss: 0.00001733
Iteration 155/1000 | Loss: 0.00001733
Iteration 156/1000 | Loss: 0.00001733
Iteration 157/1000 | Loss: 0.00001733
Iteration 158/1000 | Loss: 0.00001733
Iteration 159/1000 | Loss: 0.00001733
Iteration 160/1000 | Loss: 0.00001733
Iteration 161/1000 | Loss: 0.00001733
Iteration 162/1000 | Loss: 0.00001733
Iteration 163/1000 | Loss: 0.00001733
Iteration 164/1000 | Loss: 0.00001733
Iteration 165/1000 | Loss: 0.00001733
Iteration 166/1000 | Loss: 0.00001733
Iteration 167/1000 | Loss: 0.00001733
Iteration 168/1000 | Loss: 0.00001733
Iteration 169/1000 | Loss: 0.00001733
Iteration 170/1000 | Loss: 0.00001733
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [1.732899181661196e-05, 1.732899181661196e-05, 1.732899181661196e-05, 1.732899181661196e-05, 1.732899181661196e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.732899181661196e-05

Optimization complete. Final v2v error: 3.5378072261810303 mm

Highest mean error: 3.831575632095337 mm for frame 217

Lowest mean error: 3.2469534873962402 mm for frame 230

Saving results

Total time: 44.05647921562195
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00704431
Iteration 2/25 | Loss: 0.00138625
Iteration 3/25 | Loss: 0.00129385
Iteration 4/25 | Loss: 0.00129002
Iteration 5/25 | Loss: 0.00128950
Iteration 6/25 | Loss: 0.00128950
Iteration 7/25 | Loss: 0.00128950
Iteration 8/25 | Loss: 0.00128950
Iteration 9/25 | Loss: 0.00128950
Iteration 10/25 | Loss: 0.00128950
Iteration 11/25 | Loss: 0.00128950
Iteration 12/25 | Loss: 0.00128950
Iteration 13/25 | Loss: 0.00128950
Iteration 14/25 | Loss: 0.00128950
Iteration 15/25 | Loss: 0.00128950
Iteration 16/25 | Loss: 0.00128950
Iteration 17/25 | Loss: 0.00128950
Iteration 18/25 | Loss: 0.00128950
Iteration 19/25 | Loss: 0.00128950
Iteration 20/25 | Loss: 0.00128950
Iteration 21/25 | Loss: 0.00128950
Iteration 22/25 | Loss: 0.00128950
Iteration 23/25 | Loss: 0.00128950
Iteration 24/25 | Loss: 0.00128950
Iteration 25/25 | Loss: 0.00128950

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.75141144
Iteration 2/25 | Loss: 0.00111164
Iteration 3/25 | Loss: 0.00111164
Iteration 4/25 | Loss: 0.00111164
Iteration 5/25 | Loss: 0.00111164
Iteration 6/25 | Loss: 0.00111164
Iteration 7/25 | Loss: 0.00111164
Iteration 8/25 | Loss: 0.00111164
Iteration 9/25 | Loss: 0.00111164
Iteration 10/25 | Loss: 0.00111164
Iteration 11/25 | Loss: 0.00111164
Iteration 12/25 | Loss: 0.00111164
Iteration 13/25 | Loss: 0.00111164
Iteration 14/25 | Loss: 0.00111164
Iteration 15/25 | Loss: 0.00111164
Iteration 16/25 | Loss: 0.00111164
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011116350069642067, 0.0011116350069642067, 0.0011116350069642067, 0.0011116350069642067, 0.0011116350069642067]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011116350069642067

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111164
Iteration 2/1000 | Loss: 0.00003756
Iteration 3/1000 | Loss: 0.00002487
Iteration 4/1000 | Loss: 0.00002241
Iteration 5/1000 | Loss: 0.00002119
Iteration 6/1000 | Loss: 0.00002049
Iteration 7/1000 | Loss: 0.00001995
Iteration 8/1000 | Loss: 0.00001958
Iteration 9/1000 | Loss: 0.00001920
Iteration 10/1000 | Loss: 0.00001895
Iteration 11/1000 | Loss: 0.00001889
Iteration 12/1000 | Loss: 0.00001877
Iteration 13/1000 | Loss: 0.00001860
Iteration 14/1000 | Loss: 0.00001858
Iteration 15/1000 | Loss: 0.00001851
Iteration 16/1000 | Loss: 0.00001845
Iteration 17/1000 | Loss: 0.00001841
Iteration 18/1000 | Loss: 0.00001829
Iteration 19/1000 | Loss: 0.00001822
Iteration 20/1000 | Loss: 0.00001822
Iteration 21/1000 | Loss: 0.00001822
Iteration 22/1000 | Loss: 0.00001822
Iteration 23/1000 | Loss: 0.00001822
Iteration 24/1000 | Loss: 0.00001822
Iteration 25/1000 | Loss: 0.00001821
Iteration 26/1000 | Loss: 0.00001821
Iteration 27/1000 | Loss: 0.00001818
Iteration 28/1000 | Loss: 0.00001818
Iteration 29/1000 | Loss: 0.00001818
Iteration 30/1000 | Loss: 0.00001815
Iteration 31/1000 | Loss: 0.00001814
Iteration 32/1000 | Loss: 0.00001814
Iteration 33/1000 | Loss: 0.00001813
Iteration 34/1000 | Loss: 0.00001812
Iteration 35/1000 | Loss: 0.00001808
Iteration 36/1000 | Loss: 0.00001808
Iteration 37/1000 | Loss: 0.00001807
Iteration 38/1000 | Loss: 0.00001807
Iteration 39/1000 | Loss: 0.00001806
Iteration 40/1000 | Loss: 0.00001805
Iteration 41/1000 | Loss: 0.00001802
Iteration 42/1000 | Loss: 0.00001802
Iteration 43/1000 | Loss: 0.00001802
Iteration 44/1000 | Loss: 0.00001802
Iteration 45/1000 | Loss: 0.00001801
Iteration 46/1000 | Loss: 0.00001801
Iteration 47/1000 | Loss: 0.00001801
Iteration 48/1000 | Loss: 0.00001798
Iteration 49/1000 | Loss: 0.00001797
Iteration 50/1000 | Loss: 0.00001797
Iteration 51/1000 | Loss: 0.00001797
Iteration 52/1000 | Loss: 0.00001797
Iteration 53/1000 | Loss: 0.00001797
Iteration 54/1000 | Loss: 0.00001797
Iteration 55/1000 | Loss: 0.00001796
Iteration 56/1000 | Loss: 0.00001795
Iteration 57/1000 | Loss: 0.00001795
Iteration 58/1000 | Loss: 0.00001794
Iteration 59/1000 | Loss: 0.00001794
Iteration 60/1000 | Loss: 0.00001794
Iteration 61/1000 | Loss: 0.00001794
Iteration 62/1000 | Loss: 0.00001794
Iteration 63/1000 | Loss: 0.00001792
Iteration 64/1000 | Loss: 0.00001792
Iteration 65/1000 | Loss: 0.00001792
Iteration 66/1000 | Loss: 0.00001791
Iteration 67/1000 | Loss: 0.00001791
Iteration 68/1000 | Loss: 0.00001791
Iteration 69/1000 | Loss: 0.00001791
Iteration 70/1000 | Loss: 0.00001791
Iteration 71/1000 | Loss: 0.00001791
Iteration 72/1000 | Loss: 0.00001791
Iteration 73/1000 | Loss: 0.00001791
Iteration 74/1000 | Loss: 0.00001790
Iteration 75/1000 | Loss: 0.00001790
Iteration 76/1000 | Loss: 0.00001790
Iteration 77/1000 | Loss: 0.00001789
Iteration 78/1000 | Loss: 0.00001789
Iteration 79/1000 | Loss: 0.00001788
Iteration 80/1000 | Loss: 0.00001788
Iteration 81/1000 | Loss: 0.00001787
Iteration 82/1000 | Loss: 0.00001787
Iteration 83/1000 | Loss: 0.00001787
Iteration 84/1000 | Loss: 0.00001787
Iteration 85/1000 | Loss: 0.00001787
Iteration 86/1000 | Loss: 0.00001786
Iteration 87/1000 | Loss: 0.00001786
Iteration 88/1000 | Loss: 0.00001786
Iteration 89/1000 | Loss: 0.00001786
Iteration 90/1000 | Loss: 0.00001786
Iteration 91/1000 | Loss: 0.00001785
Iteration 92/1000 | Loss: 0.00001785
Iteration 93/1000 | Loss: 0.00001785
Iteration 94/1000 | Loss: 0.00001784
Iteration 95/1000 | Loss: 0.00001784
Iteration 96/1000 | Loss: 0.00001784
Iteration 97/1000 | Loss: 0.00001784
Iteration 98/1000 | Loss: 0.00001784
Iteration 99/1000 | Loss: 0.00001784
Iteration 100/1000 | Loss: 0.00001784
Iteration 101/1000 | Loss: 0.00001784
Iteration 102/1000 | Loss: 0.00001784
Iteration 103/1000 | Loss: 0.00001784
Iteration 104/1000 | Loss: 0.00001783
Iteration 105/1000 | Loss: 0.00001783
Iteration 106/1000 | Loss: 0.00001783
Iteration 107/1000 | Loss: 0.00001783
Iteration 108/1000 | Loss: 0.00001783
Iteration 109/1000 | Loss: 0.00001783
Iteration 110/1000 | Loss: 0.00001782
Iteration 111/1000 | Loss: 0.00001782
Iteration 112/1000 | Loss: 0.00001782
Iteration 113/1000 | Loss: 0.00001782
Iteration 114/1000 | Loss: 0.00001782
Iteration 115/1000 | Loss: 0.00001782
Iteration 116/1000 | Loss: 0.00001782
Iteration 117/1000 | Loss: 0.00001782
Iteration 118/1000 | Loss: 0.00001782
Iteration 119/1000 | Loss: 0.00001782
Iteration 120/1000 | Loss: 0.00001782
Iteration 121/1000 | Loss: 0.00001782
Iteration 122/1000 | Loss: 0.00001782
Iteration 123/1000 | Loss: 0.00001781
Iteration 124/1000 | Loss: 0.00001781
Iteration 125/1000 | Loss: 0.00001781
Iteration 126/1000 | Loss: 0.00001781
Iteration 127/1000 | Loss: 0.00001781
Iteration 128/1000 | Loss: 0.00001781
Iteration 129/1000 | Loss: 0.00001781
Iteration 130/1000 | Loss: 0.00001781
Iteration 131/1000 | Loss: 0.00001781
Iteration 132/1000 | Loss: 0.00001781
Iteration 133/1000 | Loss: 0.00001781
Iteration 134/1000 | Loss: 0.00001781
Iteration 135/1000 | Loss: 0.00001781
Iteration 136/1000 | Loss: 0.00001780
Iteration 137/1000 | Loss: 0.00001780
Iteration 138/1000 | Loss: 0.00001780
Iteration 139/1000 | Loss: 0.00001780
Iteration 140/1000 | Loss: 0.00001780
Iteration 141/1000 | Loss: 0.00001780
Iteration 142/1000 | Loss: 0.00001780
Iteration 143/1000 | Loss: 0.00001780
Iteration 144/1000 | Loss: 0.00001780
Iteration 145/1000 | Loss: 0.00001780
Iteration 146/1000 | Loss: 0.00001779
Iteration 147/1000 | Loss: 0.00001779
Iteration 148/1000 | Loss: 0.00001779
Iteration 149/1000 | Loss: 0.00001779
Iteration 150/1000 | Loss: 0.00001779
Iteration 151/1000 | Loss: 0.00001779
Iteration 152/1000 | Loss: 0.00001779
Iteration 153/1000 | Loss: 0.00001779
Iteration 154/1000 | Loss: 0.00001779
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.7794312952901237e-05, 1.7794312952901237e-05, 1.7794312952901237e-05, 1.7794312952901237e-05, 1.7794312952901237e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7794312952901237e-05

Optimization complete. Final v2v error: 3.5292656421661377 mm

Highest mean error: 4.039130210876465 mm for frame 198

Lowest mean error: 3.159841537475586 mm for frame 176

Saving results

Total time: 43.48418164253235
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00918999
Iteration 2/25 | Loss: 0.00170122
Iteration 3/25 | Loss: 0.00144949
Iteration 4/25 | Loss: 0.00141707
Iteration 5/25 | Loss: 0.00141383
Iteration 6/25 | Loss: 0.00141383
Iteration 7/25 | Loss: 0.00141383
Iteration 8/25 | Loss: 0.00141383
Iteration 9/25 | Loss: 0.00141383
Iteration 10/25 | Loss: 0.00141383
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0014138290425762534, 0.0014138290425762534, 0.0014138290425762534, 0.0014138290425762534, 0.0014138290425762534]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014138290425762534

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.71278083
Iteration 2/25 | Loss: 0.00078389
Iteration 3/25 | Loss: 0.00078389
Iteration 4/25 | Loss: 0.00078389
Iteration 5/25 | Loss: 0.00078389
Iteration 6/25 | Loss: 0.00078389
Iteration 7/25 | Loss: 0.00078389
Iteration 8/25 | Loss: 0.00078389
Iteration 9/25 | Loss: 0.00078389
Iteration 10/25 | Loss: 0.00078389
Iteration 11/25 | Loss: 0.00078389
Iteration 12/25 | Loss: 0.00078388
Iteration 13/25 | Loss: 0.00078388
Iteration 14/25 | Loss: 0.00078388
Iteration 15/25 | Loss: 0.00078388
Iteration 16/25 | Loss: 0.00078388
Iteration 17/25 | Loss: 0.00078388
Iteration 18/25 | Loss: 0.00078388
Iteration 19/25 | Loss: 0.00078388
Iteration 20/25 | Loss: 0.00078388
Iteration 21/25 | Loss: 0.00078388
Iteration 22/25 | Loss: 0.00078388
Iteration 23/25 | Loss: 0.00078388
Iteration 24/25 | Loss: 0.00078388
Iteration 25/25 | Loss: 0.00078388

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078388
Iteration 2/1000 | Loss: 0.00005924
Iteration 3/1000 | Loss: 0.00003945
Iteration 4/1000 | Loss: 0.00003559
Iteration 5/1000 | Loss: 0.00003449
Iteration 6/1000 | Loss: 0.00003361
Iteration 7/1000 | Loss: 0.00003319
Iteration 8/1000 | Loss: 0.00003273
Iteration 9/1000 | Loss: 0.00003254
Iteration 10/1000 | Loss: 0.00003234
Iteration 11/1000 | Loss: 0.00003218
Iteration 12/1000 | Loss: 0.00003202
Iteration 13/1000 | Loss: 0.00003196
Iteration 14/1000 | Loss: 0.00003179
Iteration 15/1000 | Loss: 0.00003175
Iteration 16/1000 | Loss: 0.00003165
Iteration 17/1000 | Loss: 0.00003164
Iteration 18/1000 | Loss: 0.00003159
Iteration 19/1000 | Loss: 0.00003158
Iteration 20/1000 | Loss: 0.00003158
Iteration 21/1000 | Loss: 0.00003157
Iteration 22/1000 | Loss: 0.00003156
Iteration 23/1000 | Loss: 0.00003156
Iteration 24/1000 | Loss: 0.00003155
Iteration 25/1000 | Loss: 0.00003155
Iteration 26/1000 | Loss: 0.00003154
Iteration 27/1000 | Loss: 0.00003154
Iteration 28/1000 | Loss: 0.00003154
Iteration 29/1000 | Loss: 0.00003154
Iteration 30/1000 | Loss: 0.00003154
Iteration 31/1000 | Loss: 0.00003154
Iteration 32/1000 | Loss: 0.00003154
Iteration 33/1000 | Loss: 0.00003153
Iteration 34/1000 | Loss: 0.00003153
Iteration 35/1000 | Loss: 0.00003153
Iteration 36/1000 | Loss: 0.00003153
Iteration 37/1000 | Loss: 0.00003152
Iteration 38/1000 | Loss: 0.00003152
Iteration 39/1000 | Loss: 0.00003152
Iteration 40/1000 | Loss: 0.00003152
Iteration 41/1000 | Loss: 0.00003152
Iteration 42/1000 | Loss: 0.00003152
Iteration 43/1000 | Loss: 0.00003152
Iteration 44/1000 | Loss: 0.00003152
Iteration 45/1000 | Loss: 0.00003152
Iteration 46/1000 | Loss: 0.00003151
Iteration 47/1000 | Loss: 0.00003151
Iteration 48/1000 | Loss: 0.00003151
Iteration 49/1000 | Loss: 0.00003141
Iteration 50/1000 | Loss: 0.00003139
Iteration 51/1000 | Loss: 0.00003139
Iteration 52/1000 | Loss: 0.00003136
Iteration 53/1000 | Loss: 0.00003135
Iteration 54/1000 | Loss: 0.00003135
Iteration 55/1000 | Loss: 0.00003133
Iteration 56/1000 | Loss: 0.00003133
Iteration 57/1000 | Loss: 0.00003131
Iteration 58/1000 | Loss: 0.00003131
Iteration 59/1000 | Loss: 0.00003131
Iteration 60/1000 | Loss: 0.00003130
Iteration 61/1000 | Loss: 0.00003130
Iteration 62/1000 | Loss: 0.00003130
Iteration 63/1000 | Loss: 0.00003129
Iteration 64/1000 | Loss: 0.00003129
Iteration 65/1000 | Loss: 0.00003128
Iteration 66/1000 | Loss: 0.00003128
Iteration 67/1000 | Loss: 0.00003128
Iteration 68/1000 | Loss: 0.00003128
Iteration 69/1000 | Loss: 0.00003128
Iteration 70/1000 | Loss: 0.00003128
Iteration 71/1000 | Loss: 0.00003127
Iteration 72/1000 | Loss: 0.00003127
Iteration 73/1000 | Loss: 0.00003127
Iteration 74/1000 | Loss: 0.00003127
Iteration 75/1000 | Loss: 0.00003127
Iteration 76/1000 | Loss: 0.00003127
Iteration 77/1000 | Loss: 0.00003127
Iteration 78/1000 | Loss: 0.00003127
Iteration 79/1000 | Loss: 0.00003126
Iteration 80/1000 | Loss: 0.00003126
Iteration 81/1000 | Loss: 0.00003126
Iteration 82/1000 | Loss: 0.00003126
Iteration 83/1000 | Loss: 0.00003126
Iteration 84/1000 | Loss: 0.00003126
Iteration 85/1000 | Loss: 0.00003126
Iteration 86/1000 | Loss: 0.00003125
Iteration 87/1000 | Loss: 0.00003125
Iteration 88/1000 | Loss: 0.00003125
Iteration 89/1000 | Loss: 0.00003125
Iteration 90/1000 | Loss: 0.00003125
Iteration 91/1000 | Loss: 0.00003125
Iteration 92/1000 | Loss: 0.00003125
Iteration 93/1000 | Loss: 0.00003124
Iteration 94/1000 | Loss: 0.00003124
Iteration 95/1000 | Loss: 0.00003124
Iteration 96/1000 | Loss: 0.00003124
Iteration 97/1000 | Loss: 0.00003124
Iteration 98/1000 | Loss: 0.00003124
Iteration 99/1000 | Loss: 0.00003124
Iteration 100/1000 | Loss: 0.00003124
Iteration 101/1000 | Loss: 0.00003124
Iteration 102/1000 | Loss: 0.00003124
Iteration 103/1000 | Loss: 0.00003124
Iteration 104/1000 | Loss: 0.00003124
Iteration 105/1000 | Loss: 0.00003124
Iteration 106/1000 | Loss: 0.00003124
Iteration 107/1000 | Loss: 0.00003124
Iteration 108/1000 | Loss: 0.00003124
Iteration 109/1000 | Loss: 0.00003124
Iteration 110/1000 | Loss: 0.00003124
Iteration 111/1000 | Loss: 0.00003124
Iteration 112/1000 | Loss: 0.00003124
Iteration 113/1000 | Loss: 0.00003124
Iteration 114/1000 | Loss: 0.00003124
Iteration 115/1000 | Loss: 0.00003124
Iteration 116/1000 | Loss: 0.00003124
Iteration 117/1000 | Loss: 0.00003124
Iteration 118/1000 | Loss: 0.00003124
Iteration 119/1000 | Loss: 0.00003124
Iteration 120/1000 | Loss: 0.00003124
Iteration 121/1000 | Loss: 0.00003124
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [3.123818169115111e-05, 3.123818169115111e-05, 3.123818169115111e-05, 3.123818169115111e-05, 3.123818169115111e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.123818169115111e-05

Optimization complete. Final v2v error: 4.597159385681152 mm

Highest mean error: 4.838507175445557 mm for frame 35

Lowest mean error: 4.228494644165039 mm for frame 0

Saving results

Total time: 36.11231303215027
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00961942
Iteration 2/25 | Loss: 0.00212737
Iteration 3/25 | Loss: 0.00157930
Iteration 4/25 | Loss: 0.00153474
Iteration 5/25 | Loss: 0.00147303
Iteration 6/25 | Loss: 0.00133928
Iteration 7/25 | Loss: 0.00134802
Iteration 8/25 | Loss: 0.00128787
Iteration 9/25 | Loss: 0.00127020
Iteration 10/25 | Loss: 0.00125747
Iteration 11/25 | Loss: 0.00125476
Iteration 12/25 | Loss: 0.00125389
Iteration 13/25 | Loss: 0.00125371
Iteration 14/25 | Loss: 0.00125358
Iteration 15/25 | Loss: 0.00125355
Iteration 16/25 | Loss: 0.00125355
Iteration 17/25 | Loss: 0.00125354
Iteration 18/25 | Loss: 0.00125354
Iteration 19/25 | Loss: 0.00125354
Iteration 20/25 | Loss: 0.00125354
Iteration 21/25 | Loss: 0.00125353
Iteration 22/25 | Loss: 0.00125353
Iteration 23/25 | Loss: 0.00125353
Iteration 24/25 | Loss: 0.00125353
Iteration 25/25 | Loss: 0.00125353

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42910457
Iteration 2/25 | Loss: 0.00136913
Iteration 3/25 | Loss: 0.00136913
Iteration 4/25 | Loss: 0.00136913
Iteration 5/25 | Loss: 0.00136913
Iteration 6/25 | Loss: 0.00136913
Iteration 7/25 | Loss: 0.00136913
Iteration 8/25 | Loss: 0.00136913
Iteration 9/25 | Loss: 0.00136913
Iteration 10/25 | Loss: 0.00136913
Iteration 11/25 | Loss: 0.00136913
Iteration 12/25 | Loss: 0.00136913
Iteration 13/25 | Loss: 0.00136913
Iteration 14/25 | Loss: 0.00136913
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0013691262574866414, 0.0013691262574866414, 0.0013691262574866414, 0.0013691262574866414, 0.0013691262574866414]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013691262574866414

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00136913
Iteration 2/1000 | Loss: 0.00030288
Iteration 3/1000 | Loss: 0.00024536
Iteration 4/1000 | Loss: 0.00003274
Iteration 5/1000 | Loss: 0.00026045
Iteration 6/1000 | Loss: 0.00021526
Iteration 7/1000 | Loss: 0.00081546
Iteration 8/1000 | Loss: 0.00307483
Iteration 9/1000 | Loss: 0.00104692
Iteration 10/1000 | Loss: 0.00093007
Iteration 11/1000 | Loss: 0.00137079
Iteration 12/1000 | Loss: 0.00219909
Iteration 13/1000 | Loss: 0.00017690
Iteration 14/1000 | Loss: 0.00008154
Iteration 15/1000 | Loss: 0.00179639
Iteration 16/1000 | Loss: 0.00016809
Iteration 17/1000 | Loss: 0.00228880
Iteration 18/1000 | Loss: 0.00011516
Iteration 19/1000 | Loss: 0.00004870
Iteration 20/1000 | Loss: 0.00057131
Iteration 21/1000 | Loss: 0.00003617
Iteration 22/1000 | Loss: 0.00003033
Iteration 23/1000 | Loss: 0.00002729
Iteration 24/1000 | Loss: 0.00002458
Iteration 25/1000 | Loss: 0.00002261
Iteration 26/1000 | Loss: 0.00004714
Iteration 27/1000 | Loss: 0.00002021
Iteration 28/1000 | Loss: 0.00003709
Iteration 29/1000 | Loss: 0.00003414
Iteration 30/1000 | Loss: 0.00006827
Iteration 31/1000 | Loss: 0.00001899
Iteration 32/1000 | Loss: 0.00001780
Iteration 33/1000 | Loss: 0.00001670
Iteration 34/1000 | Loss: 0.00001637
Iteration 35/1000 | Loss: 0.00001602
Iteration 36/1000 | Loss: 0.00007202
Iteration 37/1000 | Loss: 0.00001560
Iteration 38/1000 | Loss: 0.00002588
Iteration 39/1000 | Loss: 0.00002588
Iteration 40/1000 | Loss: 0.00001531
Iteration 41/1000 | Loss: 0.00001530
Iteration 42/1000 | Loss: 0.00001530
Iteration 43/1000 | Loss: 0.00001522
Iteration 44/1000 | Loss: 0.00001513
Iteration 45/1000 | Loss: 0.00001511
Iteration 46/1000 | Loss: 0.00001510
Iteration 47/1000 | Loss: 0.00001510
Iteration 48/1000 | Loss: 0.00001509
Iteration 49/1000 | Loss: 0.00001509
Iteration 50/1000 | Loss: 0.00001508
Iteration 51/1000 | Loss: 0.00001507
Iteration 52/1000 | Loss: 0.00001506
Iteration 53/1000 | Loss: 0.00001506
Iteration 54/1000 | Loss: 0.00001505
Iteration 55/1000 | Loss: 0.00001505
Iteration 56/1000 | Loss: 0.00001504
Iteration 57/1000 | Loss: 0.00001503
Iteration 58/1000 | Loss: 0.00001503
Iteration 59/1000 | Loss: 0.00001503
Iteration 60/1000 | Loss: 0.00001502
Iteration 61/1000 | Loss: 0.00001495
Iteration 62/1000 | Loss: 0.00001493
Iteration 63/1000 | Loss: 0.00001491
Iteration 64/1000 | Loss: 0.00001490
Iteration 65/1000 | Loss: 0.00001490
Iteration 66/1000 | Loss: 0.00010125
Iteration 67/1000 | Loss: 0.00032313
Iteration 68/1000 | Loss: 0.00012503
Iteration 69/1000 | Loss: 0.00001833
Iteration 70/1000 | Loss: 0.00001593
Iteration 71/1000 | Loss: 0.00001529
Iteration 72/1000 | Loss: 0.00001495
Iteration 73/1000 | Loss: 0.00001480
Iteration 74/1000 | Loss: 0.00001480
Iteration 75/1000 | Loss: 0.00001480
Iteration 76/1000 | Loss: 0.00001480
Iteration 77/1000 | Loss: 0.00001480
Iteration 78/1000 | Loss: 0.00001480
Iteration 79/1000 | Loss: 0.00001480
Iteration 80/1000 | Loss: 0.00001479
Iteration 81/1000 | Loss: 0.00001479
Iteration 82/1000 | Loss: 0.00001478
Iteration 83/1000 | Loss: 0.00001478
Iteration 84/1000 | Loss: 0.00001478
Iteration 85/1000 | Loss: 0.00001477
Iteration 86/1000 | Loss: 0.00001477
Iteration 87/1000 | Loss: 0.00001477
Iteration 88/1000 | Loss: 0.00001477
Iteration 89/1000 | Loss: 0.00001477
Iteration 90/1000 | Loss: 0.00001477
Iteration 91/1000 | Loss: 0.00002836
Iteration 92/1000 | Loss: 0.00001478
Iteration 93/1000 | Loss: 0.00001476
Iteration 94/1000 | Loss: 0.00001475
Iteration 95/1000 | Loss: 0.00001475
Iteration 96/1000 | Loss: 0.00001475
Iteration 97/1000 | Loss: 0.00001475
Iteration 98/1000 | Loss: 0.00001475
Iteration 99/1000 | Loss: 0.00001475
Iteration 100/1000 | Loss: 0.00001475
Iteration 101/1000 | Loss: 0.00001475
Iteration 102/1000 | Loss: 0.00001475
Iteration 103/1000 | Loss: 0.00001474
Iteration 104/1000 | Loss: 0.00001474
Iteration 105/1000 | Loss: 0.00001474
Iteration 106/1000 | Loss: 0.00001474
Iteration 107/1000 | Loss: 0.00001474
Iteration 108/1000 | Loss: 0.00001474
Iteration 109/1000 | Loss: 0.00001474
Iteration 110/1000 | Loss: 0.00001474
Iteration 111/1000 | Loss: 0.00001474
Iteration 112/1000 | Loss: 0.00001474
Iteration 113/1000 | Loss: 0.00001474
Iteration 114/1000 | Loss: 0.00001474
Iteration 115/1000 | Loss: 0.00001474
Iteration 116/1000 | Loss: 0.00001473
Iteration 117/1000 | Loss: 0.00001473
Iteration 118/1000 | Loss: 0.00001473
Iteration 119/1000 | Loss: 0.00001473
Iteration 120/1000 | Loss: 0.00001473
Iteration 121/1000 | Loss: 0.00001473
Iteration 122/1000 | Loss: 0.00001473
Iteration 123/1000 | Loss: 0.00001473
Iteration 124/1000 | Loss: 0.00001473
Iteration 125/1000 | Loss: 0.00001473
Iteration 126/1000 | Loss: 0.00001473
Iteration 127/1000 | Loss: 0.00001473
Iteration 128/1000 | Loss: 0.00001473
Iteration 129/1000 | Loss: 0.00001473
Iteration 130/1000 | Loss: 0.00001473
Iteration 131/1000 | Loss: 0.00001473
Iteration 132/1000 | Loss: 0.00001473
Iteration 133/1000 | Loss: 0.00001473
Iteration 134/1000 | Loss: 0.00001473
Iteration 135/1000 | Loss: 0.00001473
Iteration 136/1000 | Loss: 0.00001473
Iteration 137/1000 | Loss: 0.00001473
Iteration 138/1000 | Loss: 0.00001472
Iteration 139/1000 | Loss: 0.00001472
Iteration 140/1000 | Loss: 0.00001472
Iteration 141/1000 | Loss: 0.00001472
Iteration 142/1000 | Loss: 0.00001472
Iteration 143/1000 | Loss: 0.00001472
Iteration 144/1000 | Loss: 0.00001472
Iteration 145/1000 | Loss: 0.00001472
Iteration 146/1000 | Loss: 0.00001472
Iteration 147/1000 | Loss: 0.00001472
Iteration 148/1000 | Loss: 0.00001472
Iteration 149/1000 | Loss: 0.00001472
Iteration 150/1000 | Loss: 0.00001472
Iteration 151/1000 | Loss: 0.00001472
Iteration 152/1000 | Loss: 0.00001472
Iteration 153/1000 | Loss: 0.00001472
Iteration 154/1000 | Loss: 0.00001472
Iteration 155/1000 | Loss: 0.00001472
Iteration 156/1000 | Loss: 0.00001472
Iteration 157/1000 | Loss: 0.00001472
Iteration 158/1000 | Loss: 0.00001472
Iteration 159/1000 | Loss: 0.00001472
Iteration 160/1000 | Loss: 0.00001472
Iteration 161/1000 | Loss: 0.00001472
Iteration 162/1000 | Loss: 0.00001472
Iteration 163/1000 | Loss: 0.00001472
Iteration 164/1000 | Loss: 0.00001472
Iteration 165/1000 | Loss: 0.00001472
Iteration 166/1000 | Loss: 0.00001472
Iteration 167/1000 | Loss: 0.00001472
Iteration 168/1000 | Loss: 0.00001472
Iteration 169/1000 | Loss: 0.00001472
Iteration 170/1000 | Loss: 0.00001472
Iteration 171/1000 | Loss: 0.00001472
Iteration 172/1000 | Loss: 0.00001472
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.4718515558342915e-05, 1.4718515558342915e-05, 1.4718515558342915e-05, 1.4718515558342915e-05, 1.4718515558342915e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4718515558342915e-05

Optimization complete. Final v2v error: 3.199456214904785 mm

Highest mean error: 3.790402889251709 mm for frame 114

Lowest mean error: 2.6436500549316406 mm for frame 5

Saving results

Total time: 97.70175409317017
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00348426
Iteration 2/25 | Loss: 0.00121687
Iteration 3/25 | Loss: 0.00114126
Iteration 4/25 | Loss: 0.00113193
Iteration 5/25 | Loss: 0.00112908
Iteration 6/25 | Loss: 0.00112816
Iteration 7/25 | Loss: 0.00112816
Iteration 8/25 | Loss: 0.00112816
Iteration 9/25 | Loss: 0.00112816
Iteration 10/25 | Loss: 0.00112816
Iteration 11/25 | Loss: 0.00112816
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011281591141596437, 0.0011281591141596437, 0.0011281591141596437, 0.0011281591141596437, 0.0011281591141596437]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011281591141596437

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30539382
Iteration 2/25 | Loss: 0.00156000
Iteration 3/25 | Loss: 0.00156000
Iteration 4/25 | Loss: 0.00156000
Iteration 5/25 | Loss: 0.00156000
Iteration 6/25 | Loss: 0.00156000
Iteration 7/25 | Loss: 0.00156000
Iteration 8/25 | Loss: 0.00156000
Iteration 9/25 | Loss: 0.00156000
Iteration 10/25 | Loss: 0.00156000
Iteration 11/25 | Loss: 0.00156000
Iteration 12/25 | Loss: 0.00156000
Iteration 13/25 | Loss: 0.00156000
Iteration 14/25 | Loss: 0.00156000
Iteration 15/25 | Loss: 0.00156000
Iteration 16/25 | Loss: 0.00156000
Iteration 17/25 | Loss: 0.00156000
Iteration 18/25 | Loss: 0.00156000
Iteration 19/25 | Loss: 0.00156000
Iteration 20/25 | Loss: 0.00156000
Iteration 21/25 | Loss: 0.00156000
Iteration 22/25 | Loss: 0.00156000
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.001559997326694429, 0.001559997326694429, 0.001559997326694429, 0.001559997326694429, 0.001559997326694429]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001559997326694429

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00156000
Iteration 2/1000 | Loss: 0.00002592
Iteration 3/1000 | Loss: 0.00001758
Iteration 4/1000 | Loss: 0.00001487
Iteration 5/1000 | Loss: 0.00001360
Iteration 6/1000 | Loss: 0.00001299
Iteration 7/1000 | Loss: 0.00001244
Iteration 8/1000 | Loss: 0.00001196
Iteration 9/1000 | Loss: 0.00001173
Iteration 10/1000 | Loss: 0.00001167
Iteration 11/1000 | Loss: 0.00001165
Iteration 12/1000 | Loss: 0.00001164
Iteration 13/1000 | Loss: 0.00001151
Iteration 14/1000 | Loss: 0.00001136
Iteration 15/1000 | Loss: 0.00001126
Iteration 16/1000 | Loss: 0.00001123
Iteration 17/1000 | Loss: 0.00001117
Iteration 18/1000 | Loss: 0.00001116
Iteration 19/1000 | Loss: 0.00001116
Iteration 20/1000 | Loss: 0.00001116
Iteration 21/1000 | Loss: 0.00001116
Iteration 22/1000 | Loss: 0.00001116
Iteration 23/1000 | Loss: 0.00001106
Iteration 24/1000 | Loss: 0.00001101
Iteration 25/1000 | Loss: 0.00001095
Iteration 26/1000 | Loss: 0.00001094
Iteration 27/1000 | Loss: 0.00001092
Iteration 28/1000 | Loss: 0.00001090
Iteration 29/1000 | Loss: 0.00001090
Iteration 30/1000 | Loss: 0.00001089
Iteration 31/1000 | Loss: 0.00001089
Iteration 32/1000 | Loss: 0.00001088
Iteration 33/1000 | Loss: 0.00001084
Iteration 34/1000 | Loss: 0.00001084
Iteration 35/1000 | Loss: 0.00001083
Iteration 36/1000 | Loss: 0.00001083
Iteration 37/1000 | Loss: 0.00001083
Iteration 38/1000 | Loss: 0.00001083
Iteration 39/1000 | Loss: 0.00001083
Iteration 40/1000 | Loss: 0.00001082
Iteration 41/1000 | Loss: 0.00001081
Iteration 42/1000 | Loss: 0.00001079
Iteration 43/1000 | Loss: 0.00001079
Iteration 44/1000 | Loss: 0.00001079
Iteration 45/1000 | Loss: 0.00001079
Iteration 46/1000 | Loss: 0.00001079
Iteration 47/1000 | Loss: 0.00001079
Iteration 48/1000 | Loss: 0.00001079
Iteration 49/1000 | Loss: 0.00001079
Iteration 50/1000 | Loss: 0.00001079
Iteration 51/1000 | Loss: 0.00001079
Iteration 52/1000 | Loss: 0.00001079
Iteration 53/1000 | Loss: 0.00001079
Iteration 54/1000 | Loss: 0.00001078
Iteration 55/1000 | Loss: 0.00001078
Iteration 56/1000 | Loss: 0.00001077
Iteration 57/1000 | Loss: 0.00001076
Iteration 58/1000 | Loss: 0.00001076
Iteration 59/1000 | Loss: 0.00001076
Iteration 60/1000 | Loss: 0.00001075
Iteration 61/1000 | Loss: 0.00001075
Iteration 62/1000 | Loss: 0.00001075
Iteration 63/1000 | Loss: 0.00001074
Iteration 64/1000 | Loss: 0.00001074
Iteration 65/1000 | Loss: 0.00001073
Iteration 66/1000 | Loss: 0.00001072
Iteration 67/1000 | Loss: 0.00001072
Iteration 68/1000 | Loss: 0.00001072
Iteration 69/1000 | Loss: 0.00001072
Iteration 70/1000 | Loss: 0.00001072
Iteration 71/1000 | Loss: 0.00001071
Iteration 72/1000 | Loss: 0.00001071
Iteration 73/1000 | Loss: 0.00001071
Iteration 74/1000 | Loss: 0.00001070
Iteration 75/1000 | Loss: 0.00001070
Iteration 76/1000 | Loss: 0.00001069
Iteration 77/1000 | Loss: 0.00001068
Iteration 78/1000 | Loss: 0.00001068
Iteration 79/1000 | Loss: 0.00001067
Iteration 80/1000 | Loss: 0.00001067
Iteration 81/1000 | Loss: 0.00001067
Iteration 82/1000 | Loss: 0.00001067
Iteration 83/1000 | Loss: 0.00001067
Iteration 84/1000 | Loss: 0.00001067
Iteration 85/1000 | Loss: 0.00001067
Iteration 86/1000 | Loss: 0.00001066
Iteration 87/1000 | Loss: 0.00001066
Iteration 88/1000 | Loss: 0.00001066
Iteration 89/1000 | Loss: 0.00001066
Iteration 90/1000 | Loss: 0.00001066
Iteration 91/1000 | Loss: 0.00001065
Iteration 92/1000 | Loss: 0.00001065
Iteration 93/1000 | Loss: 0.00001065
Iteration 94/1000 | Loss: 0.00001065
Iteration 95/1000 | Loss: 0.00001065
Iteration 96/1000 | Loss: 0.00001065
Iteration 97/1000 | Loss: 0.00001064
Iteration 98/1000 | Loss: 0.00001064
Iteration 99/1000 | Loss: 0.00001064
Iteration 100/1000 | Loss: 0.00001064
Iteration 101/1000 | Loss: 0.00001063
Iteration 102/1000 | Loss: 0.00001063
Iteration 103/1000 | Loss: 0.00001063
Iteration 104/1000 | Loss: 0.00001062
Iteration 105/1000 | Loss: 0.00001060
Iteration 106/1000 | Loss: 0.00001060
Iteration 107/1000 | Loss: 0.00001060
Iteration 108/1000 | Loss: 0.00001060
Iteration 109/1000 | Loss: 0.00001059
Iteration 110/1000 | Loss: 0.00001058
Iteration 111/1000 | Loss: 0.00001057
Iteration 112/1000 | Loss: 0.00001057
Iteration 113/1000 | Loss: 0.00001057
Iteration 114/1000 | Loss: 0.00001057
Iteration 115/1000 | Loss: 0.00001057
Iteration 116/1000 | Loss: 0.00001057
Iteration 117/1000 | Loss: 0.00001056
Iteration 118/1000 | Loss: 0.00001056
Iteration 119/1000 | Loss: 0.00001056
Iteration 120/1000 | Loss: 0.00001056
Iteration 121/1000 | Loss: 0.00001055
Iteration 122/1000 | Loss: 0.00001055
Iteration 123/1000 | Loss: 0.00001054
Iteration 124/1000 | Loss: 0.00001053
Iteration 125/1000 | Loss: 0.00001053
Iteration 126/1000 | Loss: 0.00001053
Iteration 127/1000 | Loss: 0.00001053
Iteration 128/1000 | Loss: 0.00001052
Iteration 129/1000 | Loss: 0.00001052
Iteration 130/1000 | Loss: 0.00001052
Iteration 131/1000 | Loss: 0.00001052
Iteration 132/1000 | Loss: 0.00001051
Iteration 133/1000 | Loss: 0.00001051
Iteration 134/1000 | Loss: 0.00001051
Iteration 135/1000 | Loss: 0.00001051
Iteration 136/1000 | Loss: 0.00001050
Iteration 137/1000 | Loss: 0.00001050
Iteration 138/1000 | Loss: 0.00001050
Iteration 139/1000 | Loss: 0.00001049
Iteration 140/1000 | Loss: 0.00001049
Iteration 141/1000 | Loss: 0.00001049
Iteration 142/1000 | Loss: 0.00001049
Iteration 143/1000 | Loss: 0.00001049
Iteration 144/1000 | Loss: 0.00001049
Iteration 145/1000 | Loss: 0.00001049
Iteration 146/1000 | Loss: 0.00001049
Iteration 147/1000 | Loss: 0.00001049
Iteration 148/1000 | Loss: 0.00001049
Iteration 149/1000 | Loss: 0.00001048
Iteration 150/1000 | Loss: 0.00001048
Iteration 151/1000 | Loss: 0.00001048
Iteration 152/1000 | Loss: 0.00001048
Iteration 153/1000 | Loss: 0.00001048
Iteration 154/1000 | Loss: 0.00001048
Iteration 155/1000 | Loss: 0.00001047
Iteration 156/1000 | Loss: 0.00001047
Iteration 157/1000 | Loss: 0.00001047
Iteration 158/1000 | Loss: 0.00001047
Iteration 159/1000 | Loss: 0.00001047
Iteration 160/1000 | Loss: 0.00001047
Iteration 161/1000 | Loss: 0.00001047
Iteration 162/1000 | Loss: 0.00001047
Iteration 163/1000 | Loss: 0.00001047
Iteration 164/1000 | Loss: 0.00001046
Iteration 165/1000 | Loss: 0.00001046
Iteration 166/1000 | Loss: 0.00001046
Iteration 167/1000 | Loss: 0.00001046
Iteration 168/1000 | Loss: 0.00001046
Iteration 169/1000 | Loss: 0.00001046
Iteration 170/1000 | Loss: 0.00001046
Iteration 171/1000 | Loss: 0.00001046
Iteration 172/1000 | Loss: 0.00001046
Iteration 173/1000 | Loss: 0.00001046
Iteration 174/1000 | Loss: 0.00001046
Iteration 175/1000 | Loss: 0.00001046
Iteration 176/1000 | Loss: 0.00001046
Iteration 177/1000 | Loss: 0.00001046
Iteration 178/1000 | Loss: 0.00001046
Iteration 179/1000 | Loss: 0.00001045
Iteration 180/1000 | Loss: 0.00001045
Iteration 181/1000 | Loss: 0.00001045
Iteration 182/1000 | Loss: 0.00001045
Iteration 183/1000 | Loss: 0.00001044
Iteration 184/1000 | Loss: 0.00001044
Iteration 185/1000 | Loss: 0.00001044
Iteration 186/1000 | Loss: 0.00001044
Iteration 187/1000 | Loss: 0.00001044
Iteration 188/1000 | Loss: 0.00001044
Iteration 189/1000 | Loss: 0.00001044
Iteration 190/1000 | Loss: 0.00001044
Iteration 191/1000 | Loss: 0.00001044
Iteration 192/1000 | Loss: 0.00001044
Iteration 193/1000 | Loss: 0.00001044
Iteration 194/1000 | Loss: 0.00001044
Iteration 195/1000 | Loss: 0.00001044
Iteration 196/1000 | Loss: 0.00001044
Iteration 197/1000 | Loss: 0.00001044
Iteration 198/1000 | Loss: 0.00001044
Iteration 199/1000 | Loss: 0.00001044
Iteration 200/1000 | Loss: 0.00001044
Iteration 201/1000 | Loss: 0.00001044
Iteration 202/1000 | Loss: 0.00001044
Iteration 203/1000 | Loss: 0.00001043
Iteration 204/1000 | Loss: 0.00001043
Iteration 205/1000 | Loss: 0.00001043
Iteration 206/1000 | Loss: 0.00001043
Iteration 207/1000 | Loss: 0.00001043
Iteration 208/1000 | Loss: 0.00001043
Iteration 209/1000 | Loss: 0.00001042
Iteration 210/1000 | Loss: 0.00001042
Iteration 211/1000 | Loss: 0.00001042
Iteration 212/1000 | Loss: 0.00001042
Iteration 213/1000 | Loss: 0.00001042
Iteration 214/1000 | Loss: 0.00001042
Iteration 215/1000 | Loss: 0.00001042
Iteration 216/1000 | Loss: 0.00001042
Iteration 217/1000 | Loss: 0.00001042
Iteration 218/1000 | Loss: 0.00001042
Iteration 219/1000 | Loss: 0.00001042
Iteration 220/1000 | Loss: 0.00001042
Iteration 221/1000 | Loss: 0.00001042
Iteration 222/1000 | Loss: 0.00001042
Iteration 223/1000 | Loss: 0.00001042
Iteration 224/1000 | Loss: 0.00001042
Iteration 225/1000 | Loss: 0.00001042
Iteration 226/1000 | Loss: 0.00001042
Iteration 227/1000 | Loss: 0.00001042
Iteration 228/1000 | Loss: 0.00001042
Iteration 229/1000 | Loss: 0.00001042
Iteration 230/1000 | Loss: 0.00001042
Iteration 231/1000 | Loss: 0.00001042
Iteration 232/1000 | Loss: 0.00001042
Iteration 233/1000 | Loss: 0.00001042
Iteration 234/1000 | Loss: 0.00001042
Iteration 235/1000 | Loss: 0.00001042
Iteration 236/1000 | Loss: 0.00001042
Iteration 237/1000 | Loss: 0.00001042
Iteration 238/1000 | Loss: 0.00001042
Iteration 239/1000 | Loss: 0.00001042
Iteration 240/1000 | Loss: 0.00001042
Iteration 241/1000 | Loss: 0.00001042
Iteration 242/1000 | Loss: 0.00001042
Iteration 243/1000 | Loss: 0.00001042
Iteration 244/1000 | Loss: 0.00001042
Iteration 245/1000 | Loss: 0.00001042
Iteration 246/1000 | Loss: 0.00001042
Iteration 247/1000 | Loss: 0.00001042
Iteration 248/1000 | Loss: 0.00001042
Iteration 249/1000 | Loss: 0.00001042
Iteration 250/1000 | Loss: 0.00001042
Iteration 251/1000 | Loss: 0.00001042
Iteration 252/1000 | Loss: 0.00001042
Iteration 253/1000 | Loss: 0.00001042
Iteration 254/1000 | Loss: 0.00001042
Iteration 255/1000 | Loss: 0.00001042
Iteration 256/1000 | Loss: 0.00001042
Iteration 257/1000 | Loss: 0.00001042
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 257. Stopping optimization.
Last 5 losses: [1.0420735634397715e-05, 1.0420735634397715e-05, 1.0420735634397715e-05, 1.0420735634397715e-05, 1.0420735634397715e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0420735634397715e-05

Optimization complete. Final v2v error: 2.743091344833374 mm

Highest mean error: 3.5618298053741455 mm for frame 10

Lowest mean error: 2.2762796878814697 mm for frame 132

Saving results

Total time: 43.41314911842346
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00410170
Iteration 2/25 | Loss: 0.00128161
Iteration 3/25 | Loss: 0.00119869
Iteration 4/25 | Loss: 0.00118783
Iteration 5/25 | Loss: 0.00118485
Iteration 6/25 | Loss: 0.00118485
Iteration 7/25 | Loss: 0.00118485
Iteration 8/25 | Loss: 0.00118485
Iteration 9/25 | Loss: 0.00118485
Iteration 10/25 | Loss: 0.00118485
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011848481371998787, 0.0011848481371998787, 0.0011848481371998787, 0.0011848481371998787, 0.0011848481371998787]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011848481371998787

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30096459
Iteration 2/25 | Loss: 0.00144578
Iteration 3/25 | Loss: 0.00144577
Iteration 4/25 | Loss: 0.00144577
Iteration 5/25 | Loss: 0.00144577
Iteration 6/25 | Loss: 0.00144577
Iteration 7/25 | Loss: 0.00144577
Iteration 8/25 | Loss: 0.00144577
Iteration 9/25 | Loss: 0.00144577
Iteration 10/25 | Loss: 0.00144577
Iteration 11/25 | Loss: 0.00144577
Iteration 12/25 | Loss: 0.00144577
Iteration 13/25 | Loss: 0.00144577
Iteration 14/25 | Loss: 0.00144577
Iteration 15/25 | Loss: 0.00144577
Iteration 16/25 | Loss: 0.00144577
Iteration 17/25 | Loss: 0.00144577
Iteration 18/25 | Loss: 0.00144577
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0014457680517807603, 0.0014457680517807603, 0.0014457680517807603, 0.0014457680517807603, 0.0014457680517807603]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014457680517807603

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00144577
Iteration 2/1000 | Loss: 0.00002146
Iteration 3/1000 | Loss: 0.00001533
Iteration 4/1000 | Loss: 0.00001390
Iteration 5/1000 | Loss: 0.00001315
Iteration 6/1000 | Loss: 0.00001264
Iteration 7/1000 | Loss: 0.00001241
Iteration 8/1000 | Loss: 0.00001202
Iteration 9/1000 | Loss: 0.00001169
Iteration 10/1000 | Loss: 0.00001148
Iteration 11/1000 | Loss: 0.00001139
Iteration 12/1000 | Loss: 0.00001139
Iteration 13/1000 | Loss: 0.00001136
Iteration 14/1000 | Loss: 0.00001133
Iteration 15/1000 | Loss: 0.00001129
Iteration 16/1000 | Loss: 0.00001129
Iteration 17/1000 | Loss: 0.00001128
Iteration 18/1000 | Loss: 0.00001121
Iteration 19/1000 | Loss: 0.00001121
Iteration 20/1000 | Loss: 0.00001118
Iteration 21/1000 | Loss: 0.00001113
Iteration 22/1000 | Loss: 0.00001112
Iteration 23/1000 | Loss: 0.00001112
Iteration 24/1000 | Loss: 0.00001111
Iteration 25/1000 | Loss: 0.00001110
Iteration 26/1000 | Loss: 0.00001104
Iteration 27/1000 | Loss: 0.00001103
Iteration 28/1000 | Loss: 0.00001103
Iteration 29/1000 | Loss: 0.00001102
Iteration 30/1000 | Loss: 0.00001102
Iteration 31/1000 | Loss: 0.00001099
Iteration 32/1000 | Loss: 0.00001097
Iteration 33/1000 | Loss: 0.00001096
Iteration 34/1000 | Loss: 0.00001090
Iteration 35/1000 | Loss: 0.00001087
Iteration 36/1000 | Loss: 0.00001086
Iteration 37/1000 | Loss: 0.00001084
Iteration 38/1000 | Loss: 0.00001082
Iteration 39/1000 | Loss: 0.00001081
Iteration 40/1000 | Loss: 0.00001080
Iteration 41/1000 | Loss: 0.00001077
Iteration 42/1000 | Loss: 0.00001075
Iteration 43/1000 | Loss: 0.00001075
Iteration 44/1000 | Loss: 0.00001074
Iteration 45/1000 | Loss: 0.00001074
Iteration 46/1000 | Loss: 0.00001073
Iteration 47/1000 | Loss: 0.00001073
Iteration 48/1000 | Loss: 0.00001072
Iteration 49/1000 | Loss: 0.00001072
Iteration 50/1000 | Loss: 0.00001071
Iteration 51/1000 | Loss: 0.00001070
Iteration 52/1000 | Loss: 0.00001070
Iteration 53/1000 | Loss: 0.00001069
Iteration 54/1000 | Loss: 0.00001069
Iteration 55/1000 | Loss: 0.00001069
Iteration 56/1000 | Loss: 0.00001068
Iteration 57/1000 | Loss: 0.00001068
Iteration 58/1000 | Loss: 0.00001068
Iteration 59/1000 | Loss: 0.00001067
Iteration 60/1000 | Loss: 0.00001067
Iteration 61/1000 | Loss: 0.00001066
Iteration 62/1000 | Loss: 0.00001066
Iteration 63/1000 | Loss: 0.00001066
Iteration 64/1000 | Loss: 0.00001066
Iteration 65/1000 | Loss: 0.00001065
Iteration 66/1000 | Loss: 0.00001065
Iteration 67/1000 | Loss: 0.00001065
Iteration 68/1000 | Loss: 0.00001065
Iteration 69/1000 | Loss: 0.00001065
Iteration 70/1000 | Loss: 0.00001064
Iteration 71/1000 | Loss: 0.00001064
Iteration 72/1000 | Loss: 0.00001064
Iteration 73/1000 | Loss: 0.00001063
Iteration 74/1000 | Loss: 0.00001063
Iteration 75/1000 | Loss: 0.00001063
Iteration 76/1000 | Loss: 0.00001063
Iteration 77/1000 | Loss: 0.00001062
Iteration 78/1000 | Loss: 0.00001062
Iteration 79/1000 | Loss: 0.00001062
Iteration 80/1000 | Loss: 0.00001062
Iteration 81/1000 | Loss: 0.00001061
Iteration 82/1000 | Loss: 0.00001061
Iteration 83/1000 | Loss: 0.00001060
Iteration 84/1000 | Loss: 0.00001060
Iteration 85/1000 | Loss: 0.00001060
Iteration 86/1000 | Loss: 0.00001059
Iteration 87/1000 | Loss: 0.00001058
Iteration 88/1000 | Loss: 0.00001058
Iteration 89/1000 | Loss: 0.00001058
Iteration 90/1000 | Loss: 0.00001057
Iteration 91/1000 | Loss: 0.00001057
Iteration 92/1000 | Loss: 0.00001057
Iteration 93/1000 | Loss: 0.00001057
Iteration 94/1000 | Loss: 0.00001056
Iteration 95/1000 | Loss: 0.00001056
Iteration 96/1000 | Loss: 0.00001055
Iteration 97/1000 | Loss: 0.00001055
Iteration 98/1000 | Loss: 0.00001055
Iteration 99/1000 | Loss: 0.00001055
Iteration 100/1000 | Loss: 0.00001055
Iteration 101/1000 | Loss: 0.00001054
Iteration 102/1000 | Loss: 0.00001054
Iteration 103/1000 | Loss: 0.00001053
Iteration 104/1000 | Loss: 0.00001053
Iteration 105/1000 | Loss: 0.00001053
Iteration 106/1000 | Loss: 0.00001053
Iteration 107/1000 | Loss: 0.00001053
Iteration 108/1000 | Loss: 0.00001053
Iteration 109/1000 | Loss: 0.00001052
Iteration 110/1000 | Loss: 0.00001052
Iteration 111/1000 | Loss: 0.00001052
Iteration 112/1000 | Loss: 0.00001052
Iteration 113/1000 | Loss: 0.00001052
Iteration 114/1000 | Loss: 0.00001052
Iteration 115/1000 | Loss: 0.00001052
Iteration 116/1000 | Loss: 0.00001052
Iteration 117/1000 | Loss: 0.00001051
Iteration 118/1000 | Loss: 0.00001051
Iteration 119/1000 | Loss: 0.00001051
Iteration 120/1000 | Loss: 0.00001051
Iteration 121/1000 | Loss: 0.00001051
Iteration 122/1000 | Loss: 0.00001051
Iteration 123/1000 | Loss: 0.00001051
Iteration 124/1000 | Loss: 0.00001051
Iteration 125/1000 | Loss: 0.00001051
Iteration 126/1000 | Loss: 0.00001051
Iteration 127/1000 | Loss: 0.00001051
Iteration 128/1000 | Loss: 0.00001051
Iteration 129/1000 | Loss: 0.00001051
Iteration 130/1000 | Loss: 0.00001051
Iteration 131/1000 | Loss: 0.00001051
Iteration 132/1000 | Loss: 0.00001051
Iteration 133/1000 | Loss: 0.00001051
Iteration 134/1000 | Loss: 0.00001051
Iteration 135/1000 | Loss: 0.00001051
Iteration 136/1000 | Loss: 0.00001051
Iteration 137/1000 | Loss: 0.00001051
Iteration 138/1000 | Loss: 0.00001051
Iteration 139/1000 | Loss: 0.00001051
Iteration 140/1000 | Loss: 0.00001051
Iteration 141/1000 | Loss: 0.00001051
Iteration 142/1000 | Loss: 0.00001051
Iteration 143/1000 | Loss: 0.00001051
Iteration 144/1000 | Loss: 0.00001051
Iteration 145/1000 | Loss: 0.00001051
Iteration 146/1000 | Loss: 0.00001051
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.0513816960155964e-05, 1.0513816960155964e-05, 1.0513816960155964e-05, 1.0513816960155964e-05, 1.0513816960155964e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0513816960155964e-05

Optimization complete. Final v2v error: 2.7888712882995605 mm

Highest mean error: 3.2276291847229004 mm for frame 229

Lowest mean error: 2.5439302921295166 mm for frame 165

Saving results

Total time: 41.92814064025879
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00946153
Iteration 2/25 | Loss: 0.00946153
Iteration 3/25 | Loss: 0.00395063
Iteration 4/25 | Loss: 0.00209606
Iteration 5/25 | Loss: 0.00183427
Iteration 6/25 | Loss: 0.00170938
Iteration 7/25 | Loss: 0.00172479
Iteration 8/25 | Loss: 0.00171032
Iteration 9/25 | Loss: 0.00152952
Iteration 10/25 | Loss: 0.00144903
Iteration 11/25 | Loss: 0.00143524
Iteration 12/25 | Loss: 0.00144240
Iteration 13/25 | Loss: 0.00143487
Iteration 14/25 | Loss: 0.00143150
Iteration 15/25 | Loss: 0.00142917
Iteration 16/25 | Loss: 0.00143238
Iteration 17/25 | Loss: 0.00142974
Iteration 18/25 | Loss: 0.00143206
Iteration 19/25 | Loss: 0.00140729
Iteration 20/25 | Loss: 0.00138852
Iteration 21/25 | Loss: 0.00139591
Iteration 22/25 | Loss: 0.00139654
Iteration 23/25 | Loss: 0.00138399
Iteration 24/25 | Loss: 0.00137855
Iteration 25/25 | Loss: 0.00136830

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27259147
Iteration 2/25 | Loss: 0.00412173
Iteration 3/25 | Loss: 0.00259698
Iteration 4/25 | Loss: 0.00259698
Iteration 5/25 | Loss: 0.00259698
Iteration 6/25 | Loss: 0.00259698
Iteration 7/25 | Loss: 0.00259698
Iteration 8/25 | Loss: 0.00259698
Iteration 9/25 | Loss: 0.00259698
Iteration 10/25 | Loss: 0.00259698
Iteration 11/25 | Loss: 0.00259698
Iteration 12/25 | Loss: 0.00259698
Iteration 13/25 | Loss: 0.00259698
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0025969811249524355, 0.0025969811249524355, 0.0025969811249524355, 0.0025969811249524355, 0.0025969811249524355]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025969811249524355

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00259698
Iteration 2/1000 | Loss: 0.00157245
Iteration 3/1000 | Loss: 0.00174222
Iteration 4/1000 | Loss: 0.00037587
Iteration 5/1000 | Loss: 0.00081990
Iteration 6/1000 | Loss: 0.00025322
Iteration 7/1000 | Loss: 0.00048902
Iteration 8/1000 | Loss: 0.00046678
Iteration 9/1000 | Loss: 0.00059254
Iteration 10/1000 | Loss: 0.00038726
Iteration 11/1000 | Loss: 0.00054911
Iteration 12/1000 | Loss: 0.00042235
Iteration 13/1000 | Loss: 0.00098829
Iteration 14/1000 | Loss: 0.00349570
Iteration 15/1000 | Loss: 0.00270133
Iteration 16/1000 | Loss: 0.00267085
Iteration 17/1000 | Loss: 0.00140681
Iteration 18/1000 | Loss: 0.00115830
Iteration 19/1000 | Loss: 0.00219595
Iteration 20/1000 | Loss: 0.00062317
Iteration 21/1000 | Loss: 0.00058786
Iteration 22/1000 | Loss: 0.00067977
Iteration 23/1000 | Loss: 0.00037674
Iteration 24/1000 | Loss: 0.00048094
Iteration 25/1000 | Loss: 0.00164508
Iteration 26/1000 | Loss: 0.00073746
Iteration 27/1000 | Loss: 0.00036480
Iteration 28/1000 | Loss: 0.00196494
Iteration 29/1000 | Loss: 0.00175874
Iteration 30/1000 | Loss: 0.00202308
Iteration 31/1000 | Loss: 0.00285704
Iteration 32/1000 | Loss: 0.00237726
Iteration 33/1000 | Loss: 0.00272203
Iteration 34/1000 | Loss: 0.00148661
Iteration 35/1000 | Loss: 0.00032416
Iteration 36/1000 | Loss: 0.00023428
Iteration 37/1000 | Loss: 0.00017909
Iteration 38/1000 | Loss: 0.00036074
Iteration 39/1000 | Loss: 0.00200206
Iteration 40/1000 | Loss: 0.00297805
Iteration 41/1000 | Loss: 0.00099041
Iteration 42/1000 | Loss: 0.00020339
Iteration 43/1000 | Loss: 0.00009604
Iteration 44/1000 | Loss: 0.00008039
Iteration 45/1000 | Loss: 0.00017353
Iteration 46/1000 | Loss: 0.00194796
Iteration 47/1000 | Loss: 0.00268327
Iteration 48/1000 | Loss: 0.00038130
Iteration 49/1000 | Loss: 0.00040047
Iteration 50/1000 | Loss: 0.00033354
Iteration 51/1000 | Loss: 0.00026735
Iteration 52/1000 | Loss: 0.00027159
Iteration 53/1000 | Loss: 0.00044360
Iteration 54/1000 | Loss: 0.00082690
Iteration 55/1000 | Loss: 0.00033587
Iteration 56/1000 | Loss: 0.00029707
Iteration 57/1000 | Loss: 0.00030212
Iteration 58/1000 | Loss: 0.00026509
Iteration 59/1000 | Loss: 0.00028523
Iteration 60/1000 | Loss: 0.00025708
Iteration 61/1000 | Loss: 0.00028534
Iteration 62/1000 | Loss: 0.00028106
Iteration 63/1000 | Loss: 0.00009730
Iteration 64/1000 | Loss: 0.00017652
Iteration 65/1000 | Loss: 0.00013482
Iteration 66/1000 | Loss: 0.00019656
Iteration 67/1000 | Loss: 0.00019630
Iteration 68/1000 | Loss: 0.00016755
Iteration 69/1000 | Loss: 0.00017336
Iteration 70/1000 | Loss: 0.00016557
Iteration 71/1000 | Loss: 0.00017448
Iteration 72/1000 | Loss: 0.00083061
Iteration 73/1000 | Loss: 0.00027407
Iteration 74/1000 | Loss: 0.00019321
Iteration 75/1000 | Loss: 0.00015836
Iteration 76/1000 | Loss: 0.00011997
Iteration 77/1000 | Loss: 0.00025569
Iteration 78/1000 | Loss: 0.00020669
Iteration 79/1000 | Loss: 0.00019697
Iteration 80/1000 | Loss: 0.00006557
Iteration 81/1000 | Loss: 0.00006862
Iteration 82/1000 | Loss: 0.00011497
Iteration 83/1000 | Loss: 0.00014689
Iteration 84/1000 | Loss: 0.00016538
Iteration 85/1000 | Loss: 0.00021340
Iteration 86/1000 | Loss: 0.00022486
Iteration 87/1000 | Loss: 0.00041154
Iteration 88/1000 | Loss: 0.00010150
Iteration 89/1000 | Loss: 0.00008314
Iteration 90/1000 | Loss: 0.00035117
Iteration 91/1000 | Loss: 0.00056331
Iteration 92/1000 | Loss: 0.00023631
Iteration 93/1000 | Loss: 0.00014473
Iteration 94/1000 | Loss: 0.00017970
Iteration 95/1000 | Loss: 0.00016752
Iteration 96/1000 | Loss: 0.00067356
Iteration 97/1000 | Loss: 0.00013450
Iteration 98/1000 | Loss: 0.00015460
Iteration 99/1000 | Loss: 0.00005997
Iteration 100/1000 | Loss: 0.00086786
Iteration 101/1000 | Loss: 0.00093881
Iteration 102/1000 | Loss: 0.00050909
Iteration 103/1000 | Loss: 0.00067677
Iteration 104/1000 | Loss: 0.00049323
Iteration 105/1000 | Loss: 0.00034246
Iteration 106/1000 | Loss: 0.00013093
Iteration 107/1000 | Loss: 0.00006500
Iteration 108/1000 | Loss: 0.00005454
Iteration 109/1000 | Loss: 0.00011934
Iteration 110/1000 | Loss: 0.00038309
Iteration 111/1000 | Loss: 0.00057910
Iteration 112/1000 | Loss: 0.00019460
Iteration 113/1000 | Loss: 0.00033688
Iteration 114/1000 | Loss: 0.00005869
Iteration 115/1000 | Loss: 0.00005400
Iteration 116/1000 | Loss: 0.00025064
Iteration 117/1000 | Loss: 0.00017392
Iteration 118/1000 | Loss: 0.00025742
Iteration 119/1000 | Loss: 0.00022126
Iteration 120/1000 | Loss: 0.00007959
Iteration 121/1000 | Loss: 0.00005298
Iteration 122/1000 | Loss: 0.00005215
Iteration 123/1000 | Loss: 0.00017473
Iteration 124/1000 | Loss: 0.00079860
Iteration 125/1000 | Loss: 0.00137084
Iteration 126/1000 | Loss: 0.00030005
Iteration 127/1000 | Loss: 0.00034083
Iteration 128/1000 | Loss: 0.00005243
Iteration 129/1000 | Loss: 0.00005299
Iteration 130/1000 | Loss: 0.00053960
Iteration 131/1000 | Loss: 0.00035998
Iteration 132/1000 | Loss: 0.00008401
Iteration 133/1000 | Loss: 0.00034691
Iteration 134/1000 | Loss: 0.00022765
Iteration 135/1000 | Loss: 0.00006226
Iteration 136/1000 | Loss: 0.00067267
Iteration 137/1000 | Loss: 0.00020379
Iteration 138/1000 | Loss: 0.00011739
Iteration 139/1000 | Loss: 0.00016708
Iteration 140/1000 | Loss: 0.00010070
Iteration 141/1000 | Loss: 0.00011812
Iteration 142/1000 | Loss: 0.00014880
Iteration 143/1000 | Loss: 0.00015793
Iteration 144/1000 | Loss: 0.00007533
Iteration 145/1000 | Loss: 0.00005480
Iteration 146/1000 | Loss: 0.00005467
Iteration 147/1000 | Loss: 0.00006530
Iteration 148/1000 | Loss: 0.00024882
Iteration 149/1000 | Loss: 0.00028232
Iteration 150/1000 | Loss: 0.00011316
Iteration 151/1000 | Loss: 0.00026396
Iteration 152/1000 | Loss: 0.00048827
Iteration 153/1000 | Loss: 0.00006623
Iteration 154/1000 | Loss: 0.00005614
Iteration 155/1000 | Loss: 0.00024624
Iteration 156/1000 | Loss: 0.00026595
Iteration 157/1000 | Loss: 0.00049314
Iteration 158/1000 | Loss: 0.00026926
Iteration 159/1000 | Loss: 0.00032625
Iteration 160/1000 | Loss: 0.00025234
Iteration 161/1000 | Loss: 0.00008098
Iteration 162/1000 | Loss: 0.00035732
Iteration 163/1000 | Loss: 0.00034914
Iteration 164/1000 | Loss: 0.00021746
Iteration 165/1000 | Loss: 0.00042123
Iteration 166/1000 | Loss: 0.00014774
Iteration 167/1000 | Loss: 0.00010605
Iteration 168/1000 | Loss: 0.00032838
Iteration 169/1000 | Loss: 0.00029546
Iteration 170/1000 | Loss: 0.00025376
Iteration 171/1000 | Loss: 0.00007421
Iteration 172/1000 | Loss: 0.00026174
Iteration 173/1000 | Loss: 0.00013318
Iteration 174/1000 | Loss: 0.00080810
Iteration 175/1000 | Loss: 0.00022874
Iteration 176/1000 | Loss: 0.00032986
Iteration 177/1000 | Loss: 0.00032859
Iteration 178/1000 | Loss: 0.00013995
Iteration 179/1000 | Loss: 0.00012367
Iteration 180/1000 | Loss: 0.00030646
Iteration 181/1000 | Loss: 0.00013575
Iteration 182/1000 | Loss: 0.00007697
Iteration 183/1000 | Loss: 0.00032531
Iteration 184/1000 | Loss: 0.00016212
Iteration 185/1000 | Loss: 0.00037167
Iteration 186/1000 | Loss: 0.00042709
Iteration 187/1000 | Loss: 0.00012680
Iteration 188/1000 | Loss: 0.00023780
Iteration 189/1000 | Loss: 0.00033972
Iteration 190/1000 | Loss: 0.00009987
Iteration 191/1000 | Loss: 0.00028008
Iteration 192/1000 | Loss: 0.00013363
Iteration 193/1000 | Loss: 0.00034411
Iteration 194/1000 | Loss: 0.00042704
Iteration 195/1000 | Loss: 0.00005261
Iteration 196/1000 | Loss: 0.00013744
Iteration 197/1000 | Loss: 0.00054160
Iteration 198/1000 | Loss: 0.00062589
Iteration 199/1000 | Loss: 0.00015845
Iteration 200/1000 | Loss: 0.00113576
Iteration 201/1000 | Loss: 0.00004924
Iteration 202/1000 | Loss: 0.00004640
Iteration 203/1000 | Loss: 0.00003733
Iteration 204/1000 | Loss: 0.00003855
Iteration 205/1000 | Loss: 0.00008011
Iteration 206/1000 | Loss: 0.00003223
Iteration 207/1000 | Loss: 0.00007849
Iteration 208/1000 | Loss: 0.00092554
Iteration 209/1000 | Loss: 0.00029667
Iteration 210/1000 | Loss: 0.00028003
Iteration 211/1000 | Loss: 0.00010177
Iteration 212/1000 | Loss: 0.00019805
Iteration 213/1000 | Loss: 0.00007594
Iteration 214/1000 | Loss: 0.00003261
Iteration 215/1000 | Loss: 0.00029232
Iteration 216/1000 | Loss: 0.00014223
Iteration 217/1000 | Loss: 0.00024926
Iteration 218/1000 | Loss: 0.00006436
Iteration 219/1000 | Loss: 0.00006386
Iteration 220/1000 | Loss: 0.00004703
Iteration 221/1000 | Loss: 0.00005335
Iteration 222/1000 | Loss: 0.00021473
Iteration 223/1000 | Loss: 0.00008544
Iteration 224/1000 | Loss: 0.00011381
Iteration 225/1000 | Loss: 0.00027520
Iteration 226/1000 | Loss: 0.00011547
Iteration 227/1000 | Loss: 0.00025562
Iteration 228/1000 | Loss: 0.00024506
Iteration 229/1000 | Loss: 0.00011572
Iteration 230/1000 | Loss: 0.00015476
Iteration 231/1000 | Loss: 0.00024249
Iteration 232/1000 | Loss: 0.00031951
Iteration 233/1000 | Loss: 0.00013004
Iteration 234/1000 | Loss: 0.00008069
Iteration 235/1000 | Loss: 0.00021960
Iteration 236/1000 | Loss: 0.00013166
Iteration 237/1000 | Loss: 0.00010999
Iteration 238/1000 | Loss: 0.00013965
Iteration 239/1000 | Loss: 0.00008822
Iteration 240/1000 | Loss: 0.00005035
Iteration 241/1000 | Loss: 0.00012286
Iteration 242/1000 | Loss: 0.00008838
Iteration 243/1000 | Loss: 0.00009236
Iteration 244/1000 | Loss: 0.00003524
Iteration 245/1000 | Loss: 0.00013645
Iteration 246/1000 | Loss: 0.00014863
Iteration 247/1000 | Loss: 0.00003852
Iteration 248/1000 | Loss: 0.00018455
Iteration 249/1000 | Loss: 0.00016338
Iteration 250/1000 | Loss: 0.00008985
Iteration 251/1000 | Loss: 0.00021565
Iteration 252/1000 | Loss: 0.00013138
Iteration 253/1000 | Loss: 0.00010197
Iteration 254/1000 | Loss: 0.00021659
Iteration 255/1000 | Loss: 0.00014536
Iteration 256/1000 | Loss: 0.00003523
Iteration 257/1000 | Loss: 0.00004759
Iteration 258/1000 | Loss: 0.00007175
Iteration 259/1000 | Loss: 0.00004610
Iteration 260/1000 | Loss: 0.00006594
Iteration 261/1000 | Loss: 0.00006695
Iteration 262/1000 | Loss: 0.00006151
Iteration 263/1000 | Loss: 0.00015520
Iteration 264/1000 | Loss: 0.00024902
Iteration 265/1000 | Loss: 0.00016674
Iteration 266/1000 | Loss: 0.00023376
Iteration 267/1000 | Loss: 0.00015440
Iteration 268/1000 | Loss: 0.00007085
Iteration 269/1000 | Loss: 0.00034115
Iteration 270/1000 | Loss: 0.00005257
Iteration 271/1000 | Loss: 0.00012128
Iteration 272/1000 | Loss: 0.00019831
Iteration 273/1000 | Loss: 0.00027551
Iteration 274/1000 | Loss: 0.00020700
Iteration 275/1000 | Loss: 0.00019949
Iteration 276/1000 | Loss: 0.00016476
Iteration 277/1000 | Loss: 0.00020776
Iteration 278/1000 | Loss: 0.00014931
Iteration 279/1000 | Loss: 0.00024241
Iteration 280/1000 | Loss: 0.00019101
Iteration 281/1000 | Loss: 0.00040894
Iteration 282/1000 | Loss: 0.00046801
Iteration 283/1000 | Loss: 0.00037525
Iteration 284/1000 | Loss: 0.00030189
Iteration 285/1000 | Loss: 0.00003805
Iteration 286/1000 | Loss: 0.00003137
Iteration 287/1000 | Loss: 0.00005884
Iteration 288/1000 | Loss: 0.00004330
Iteration 289/1000 | Loss: 0.00003327
Iteration 290/1000 | Loss: 0.00005009
Iteration 291/1000 | Loss: 0.00002949
Iteration 292/1000 | Loss: 0.00005686
Iteration 293/1000 | Loss: 0.00036704
Iteration 294/1000 | Loss: 0.00009972
Iteration 295/1000 | Loss: 0.00017568
Iteration 296/1000 | Loss: 0.00003259
Iteration 297/1000 | Loss: 0.00003069
Iteration 298/1000 | Loss: 0.00021855
Iteration 299/1000 | Loss: 0.00005097
Iteration 300/1000 | Loss: 0.00004248
Iteration 301/1000 | Loss: 0.00006928
Iteration 302/1000 | Loss: 0.00004964
Iteration 303/1000 | Loss: 0.00007496
Iteration 304/1000 | Loss: 0.00006003
Iteration 305/1000 | Loss: 0.00004019
Iteration 306/1000 | Loss: 0.00004180
Iteration 307/1000 | Loss: 0.00003570
Iteration 308/1000 | Loss: 0.00006279
Iteration 309/1000 | Loss: 0.00003806
Iteration 310/1000 | Loss: 0.00003275
Iteration 311/1000 | Loss: 0.00002714
Iteration 312/1000 | Loss: 0.00002620
Iteration 313/1000 | Loss: 0.00002616
Iteration 314/1000 | Loss: 0.00014168
Iteration 315/1000 | Loss: 0.00008549
Iteration 316/1000 | Loss: 0.00010764
Iteration 317/1000 | Loss: 0.00003624
Iteration 318/1000 | Loss: 0.00002528
Iteration 319/1000 | Loss: 0.00002482
Iteration 320/1000 | Loss: 0.00002450
Iteration 321/1000 | Loss: 0.00002424
Iteration 322/1000 | Loss: 0.00004293
Iteration 323/1000 | Loss: 0.00002448
Iteration 324/1000 | Loss: 0.00002390
Iteration 325/1000 | Loss: 0.00002387
Iteration 326/1000 | Loss: 0.00002384
Iteration 327/1000 | Loss: 0.00002384
Iteration 328/1000 | Loss: 0.00002384
Iteration 329/1000 | Loss: 0.00002383
Iteration 330/1000 | Loss: 0.00002380
Iteration 331/1000 | Loss: 0.00002378
Iteration 332/1000 | Loss: 0.00002378
Iteration 333/1000 | Loss: 0.00002377
Iteration 334/1000 | Loss: 0.00002376
Iteration 335/1000 | Loss: 0.00002376
Iteration 336/1000 | Loss: 0.00002375
Iteration 337/1000 | Loss: 0.00002375
Iteration 338/1000 | Loss: 0.00002374
Iteration 339/1000 | Loss: 0.00002374
Iteration 340/1000 | Loss: 0.00002374
Iteration 341/1000 | Loss: 0.00002373
Iteration 342/1000 | Loss: 0.00002372
Iteration 343/1000 | Loss: 0.00002372
Iteration 344/1000 | Loss: 0.00002371
Iteration 345/1000 | Loss: 0.00002371
Iteration 346/1000 | Loss: 0.00002370
Iteration 347/1000 | Loss: 0.00002370
Iteration 348/1000 | Loss: 0.00002370
Iteration 349/1000 | Loss: 0.00002369
Iteration 350/1000 | Loss: 0.00002369
Iteration 351/1000 | Loss: 0.00002369
Iteration 352/1000 | Loss: 0.00002369
Iteration 353/1000 | Loss: 0.00002369
Iteration 354/1000 | Loss: 0.00042691
Iteration 355/1000 | Loss: 0.00002731
Iteration 356/1000 | Loss: 0.00002459
Iteration 357/1000 | Loss: 0.00004420
Iteration 358/1000 | Loss: 0.00005580
Iteration 359/1000 | Loss: 0.00002331
Iteration 360/1000 | Loss: 0.00003511
Iteration 361/1000 | Loss: 0.00002725
Iteration 362/1000 | Loss: 0.00041773
Iteration 363/1000 | Loss: 0.00002499
Iteration 364/1000 | Loss: 0.00002266
Iteration 365/1000 | Loss: 0.00002202
Iteration 366/1000 | Loss: 0.00002128
Iteration 367/1000 | Loss: 0.00003383
Iteration 368/1000 | Loss: 0.00002377
Iteration 369/1000 | Loss: 0.00002063
Iteration 370/1000 | Loss: 0.00002040
Iteration 371/1000 | Loss: 0.00002026
Iteration 372/1000 | Loss: 0.00002025
Iteration 373/1000 | Loss: 0.00002025
Iteration 374/1000 | Loss: 0.00002025
Iteration 375/1000 | Loss: 0.00002024
Iteration 376/1000 | Loss: 0.00002023
Iteration 377/1000 | Loss: 0.00002021
Iteration 378/1000 | Loss: 0.00002021
Iteration 379/1000 | Loss: 0.00002020
Iteration 380/1000 | Loss: 0.00002019
Iteration 381/1000 | Loss: 0.00002019
Iteration 382/1000 | Loss: 0.00002014
Iteration 383/1000 | Loss: 0.00002014
Iteration 384/1000 | Loss: 0.00002014
Iteration 385/1000 | Loss: 0.00002013
Iteration 386/1000 | Loss: 0.00002013
Iteration 387/1000 | Loss: 0.00003671
Iteration 388/1000 | Loss: 0.00002148
Iteration 389/1000 | Loss: 0.00002011
Iteration 390/1000 | Loss: 0.00002010
Iteration 391/1000 | Loss: 0.00002009
Iteration 392/1000 | Loss: 0.00002009
Iteration 393/1000 | Loss: 0.00002008
Iteration 394/1000 | Loss: 0.00002008
Iteration 395/1000 | Loss: 0.00002007
Iteration 396/1000 | Loss: 0.00002006
Iteration 397/1000 | Loss: 0.00002004
Iteration 398/1000 | Loss: 0.00002004
Iteration 399/1000 | Loss: 0.00002004
Iteration 400/1000 | Loss: 0.00002003
Iteration 401/1000 | Loss: 0.00002003
Iteration 402/1000 | Loss: 0.00002002
Iteration 403/1000 | Loss: 0.00002002
Iteration 404/1000 | Loss: 0.00002002
Iteration 405/1000 | Loss: 0.00002001
Iteration 406/1000 | Loss: 0.00002001
Iteration 407/1000 | Loss: 0.00002001
Iteration 408/1000 | Loss: 0.00002001
Iteration 409/1000 | Loss: 0.00002001
Iteration 410/1000 | Loss: 0.00002001
Iteration 411/1000 | Loss: 0.00002000
Iteration 412/1000 | Loss: 0.00002000
Iteration 413/1000 | Loss: 0.00001999
Iteration 414/1000 | Loss: 0.00001999
Iteration 415/1000 | Loss: 0.00001999
Iteration 416/1000 | Loss: 0.00001999
Iteration 417/1000 | Loss: 0.00001998
Iteration 418/1000 | Loss: 0.00001998
Iteration 419/1000 | Loss: 0.00001998
Iteration 420/1000 | Loss: 0.00001997
Iteration 421/1000 | Loss: 0.00001997
Iteration 422/1000 | Loss: 0.00001997
Iteration 423/1000 | Loss: 0.00001997
Iteration 424/1000 | Loss: 0.00001997
Iteration 425/1000 | Loss: 0.00001996
Iteration 426/1000 | Loss: 0.00001996
Iteration 427/1000 | Loss: 0.00001996
Iteration 428/1000 | Loss: 0.00001996
Iteration 429/1000 | Loss: 0.00001996
Iteration 430/1000 | Loss: 0.00001996
Iteration 431/1000 | Loss: 0.00001996
Iteration 432/1000 | Loss: 0.00001996
Iteration 433/1000 | Loss: 0.00001996
Iteration 434/1000 | Loss: 0.00001996
Iteration 435/1000 | Loss: 0.00001996
Iteration 436/1000 | Loss: 0.00001996
Iteration 437/1000 | Loss: 0.00001996
Iteration 438/1000 | Loss: 0.00001996
Iteration 439/1000 | Loss: 0.00001996
Iteration 440/1000 | Loss: 0.00001996
Iteration 441/1000 | Loss: 0.00001996
Iteration 442/1000 | Loss: 0.00001996
Iteration 443/1000 | Loss: 0.00001996
Iteration 444/1000 | Loss: 0.00001996
Iteration 445/1000 | Loss: 0.00001996
Iteration 446/1000 | Loss: 0.00001996
Iteration 447/1000 | Loss: 0.00001996
Iteration 448/1000 | Loss: 0.00001996
Iteration 449/1000 | Loss: 0.00001996
Iteration 450/1000 | Loss: 0.00001996
Iteration 451/1000 | Loss: 0.00001996
Iteration 452/1000 | Loss: 0.00001996
Iteration 453/1000 | Loss: 0.00001996
Iteration 454/1000 | Loss: 0.00001996
Iteration 455/1000 | Loss: 0.00001996
Iteration 456/1000 | Loss: 0.00001996
Iteration 457/1000 | Loss: 0.00001996
Iteration 458/1000 | Loss: 0.00001996
Iteration 459/1000 | Loss: 0.00001996
Iteration 460/1000 | Loss: 0.00001996
Iteration 461/1000 | Loss: 0.00001996
Iteration 462/1000 | Loss: 0.00001996
Iteration 463/1000 | Loss: 0.00001996
Iteration 464/1000 | Loss: 0.00001996
Iteration 465/1000 | Loss: 0.00001996
Iteration 466/1000 | Loss: 0.00001996
Iteration 467/1000 | Loss: 0.00001996
Iteration 468/1000 | Loss: 0.00001996
Iteration 469/1000 | Loss: 0.00001996
Iteration 470/1000 | Loss: 0.00001996
Iteration 471/1000 | Loss: 0.00001996
Iteration 472/1000 | Loss: 0.00001996
Iteration 473/1000 | Loss: 0.00001996
Iteration 474/1000 | Loss: 0.00001996
Iteration 475/1000 | Loss: 0.00001996
Iteration 476/1000 | Loss: 0.00001996
Iteration 477/1000 | Loss: 0.00001996
Iteration 478/1000 | Loss: 0.00001996
Iteration 479/1000 | Loss: 0.00001996
Iteration 480/1000 | Loss: 0.00001996
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 480. Stopping optimization.
Last 5 losses: [1.9959426936111413e-05, 1.9959426936111413e-05, 1.9959426936111413e-05, 1.9959426936111413e-05, 1.9959426936111413e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9959426936111413e-05

Optimization complete. Final v2v error: 3.753955125808716 mm

Highest mean error: 5.628120422363281 mm for frame 239

Lowest mean error: 2.801438331604004 mm for frame 43

Saving results

Total time: 602.6511964797974
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00996163
Iteration 2/25 | Loss: 0.00237241
Iteration 3/25 | Loss: 0.00155900
Iteration 4/25 | Loss: 0.00141079
Iteration 5/25 | Loss: 0.00133435
Iteration 6/25 | Loss: 0.00129993
Iteration 7/25 | Loss: 0.00126780
Iteration 8/25 | Loss: 0.00125650
Iteration 9/25 | Loss: 0.00125347
Iteration 10/25 | Loss: 0.00125213
Iteration 11/25 | Loss: 0.00125236
Iteration 12/25 | Loss: 0.00124760
Iteration 13/25 | Loss: 0.00124358
Iteration 14/25 | Loss: 0.00123776
Iteration 15/25 | Loss: 0.00123671
Iteration 16/25 | Loss: 0.00123413
Iteration 17/25 | Loss: 0.00123382
Iteration 18/25 | Loss: 0.00123375
Iteration 19/25 | Loss: 0.00123375
Iteration 20/25 | Loss: 0.00123373
Iteration 21/25 | Loss: 0.00123373
Iteration 22/25 | Loss: 0.00123372
Iteration 23/25 | Loss: 0.00123372
Iteration 24/25 | Loss: 0.00123374
Iteration 25/25 | Loss: 0.00123371

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43607318
Iteration 2/25 | Loss: 0.00133807
Iteration 3/25 | Loss: 0.00129888
Iteration 4/25 | Loss: 0.00129888
Iteration 5/25 | Loss: 0.00129888
Iteration 6/25 | Loss: 0.00129888
Iteration 7/25 | Loss: 0.00129887
Iteration 8/25 | Loss: 0.00129887
Iteration 9/25 | Loss: 0.00129887
Iteration 10/25 | Loss: 0.00129887
Iteration 11/25 | Loss: 0.00129887
Iteration 12/25 | Loss: 0.00129887
Iteration 13/25 | Loss: 0.00129887
Iteration 14/25 | Loss: 0.00129887
Iteration 15/25 | Loss: 0.00129887
Iteration 16/25 | Loss: 0.00129887
Iteration 17/25 | Loss: 0.00129887
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012988735688850284, 0.0012988735688850284, 0.0012988735688850284, 0.0012988735688850284, 0.0012988735688850284]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012988735688850284

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129887
Iteration 2/1000 | Loss: 0.00008103
Iteration 3/1000 | Loss: 0.00005501
Iteration 4/1000 | Loss: 0.00003070
Iteration 5/1000 | Loss: 0.00014970
Iteration 6/1000 | Loss: 0.00021080
Iteration 7/1000 | Loss: 0.00004475
Iteration 8/1000 | Loss: 0.00003154
Iteration 9/1000 | Loss: 0.00002780
Iteration 10/1000 | Loss: 0.00005239
Iteration 11/1000 | Loss: 0.00004023
Iteration 12/1000 | Loss: 0.00002364
Iteration 13/1000 | Loss: 0.00013934
Iteration 14/1000 | Loss: 0.00005152
Iteration 15/1000 | Loss: 0.00003848
Iteration 16/1000 | Loss: 0.00002129
Iteration 17/1000 | Loss: 0.00001986
Iteration 18/1000 | Loss: 0.00001792
Iteration 19/1000 | Loss: 0.00002126
Iteration 20/1000 | Loss: 0.00001731
Iteration 21/1000 | Loss: 0.00002313
Iteration 22/1000 | Loss: 0.00002270
Iteration 23/1000 | Loss: 0.00001896
Iteration 24/1000 | Loss: 0.00004578
Iteration 25/1000 | Loss: 0.00005004
Iteration 26/1000 | Loss: 0.00001776
Iteration 27/1000 | Loss: 0.00001672
Iteration 28/1000 | Loss: 0.00001661
Iteration 29/1000 | Loss: 0.00001661
Iteration 30/1000 | Loss: 0.00001661
Iteration 31/1000 | Loss: 0.00001661
Iteration 32/1000 | Loss: 0.00001661
Iteration 33/1000 | Loss: 0.00001661
Iteration 34/1000 | Loss: 0.00001661
Iteration 35/1000 | Loss: 0.00001661
Iteration 36/1000 | Loss: 0.00001661
Iteration 37/1000 | Loss: 0.00001661
Iteration 38/1000 | Loss: 0.00001712
Iteration 39/1000 | Loss: 0.00001660
Iteration 40/1000 | Loss: 0.00001658
Iteration 41/1000 | Loss: 0.00001657
Iteration 42/1000 | Loss: 0.00001657
Iteration 43/1000 | Loss: 0.00001657
Iteration 44/1000 | Loss: 0.00002116
Iteration 45/1000 | Loss: 0.00003847
Iteration 46/1000 | Loss: 0.00005155
Iteration 47/1000 | Loss: 0.00001662
Iteration 48/1000 | Loss: 0.00002130
Iteration 49/1000 | Loss: 0.00001632
Iteration 50/1000 | Loss: 0.00001631
Iteration 51/1000 | Loss: 0.00001629
Iteration 52/1000 | Loss: 0.00001654
Iteration 53/1000 | Loss: 0.00034746
Iteration 54/1000 | Loss: 0.00002495
Iteration 55/1000 | Loss: 0.00003888
Iteration 56/1000 | Loss: 0.00002868
Iteration 57/1000 | Loss: 0.00002117
Iteration 58/1000 | Loss: 0.00001889
Iteration 59/1000 | Loss: 0.00002994
Iteration 60/1000 | Loss: 0.00008993
Iteration 61/1000 | Loss: 0.00001631
Iteration 62/1000 | Loss: 0.00001717
Iteration 63/1000 | Loss: 0.00001480
Iteration 64/1000 | Loss: 0.00001502
Iteration 65/1000 | Loss: 0.00001690
Iteration 66/1000 | Loss: 0.00003207
Iteration 67/1000 | Loss: 0.00001461
Iteration 68/1000 | Loss: 0.00002212
Iteration 69/1000 | Loss: 0.00001746
Iteration 70/1000 | Loss: 0.00001482
Iteration 71/1000 | Loss: 0.00001439
Iteration 72/1000 | Loss: 0.00001439
Iteration 73/1000 | Loss: 0.00001439
Iteration 74/1000 | Loss: 0.00001439
Iteration 75/1000 | Loss: 0.00001439
Iteration 76/1000 | Loss: 0.00001439
Iteration 77/1000 | Loss: 0.00001439
Iteration 78/1000 | Loss: 0.00001439
Iteration 79/1000 | Loss: 0.00001439
Iteration 80/1000 | Loss: 0.00001439
Iteration 81/1000 | Loss: 0.00001439
Iteration 82/1000 | Loss: 0.00001439
Iteration 83/1000 | Loss: 0.00001438
Iteration 84/1000 | Loss: 0.00001438
Iteration 85/1000 | Loss: 0.00001438
Iteration 86/1000 | Loss: 0.00001438
Iteration 87/1000 | Loss: 0.00001436
Iteration 88/1000 | Loss: 0.00001436
Iteration 89/1000 | Loss: 0.00001436
Iteration 90/1000 | Loss: 0.00001436
Iteration 91/1000 | Loss: 0.00001436
Iteration 92/1000 | Loss: 0.00001436
Iteration 93/1000 | Loss: 0.00001435
Iteration 94/1000 | Loss: 0.00001435
Iteration 95/1000 | Loss: 0.00001572
Iteration 96/1000 | Loss: 0.00001473
Iteration 97/1000 | Loss: 0.00001890
Iteration 98/1000 | Loss: 0.00001889
Iteration 99/1000 | Loss: 0.00011992
Iteration 100/1000 | Loss: 0.00001443
Iteration 101/1000 | Loss: 0.00001727
Iteration 102/1000 | Loss: 0.00001429
Iteration 103/1000 | Loss: 0.00001429
Iteration 104/1000 | Loss: 0.00001429
Iteration 105/1000 | Loss: 0.00001429
Iteration 106/1000 | Loss: 0.00001428
Iteration 107/1000 | Loss: 0.00001428
Iteration 108/1000 | Loss: 0.00001428
Iteration 109/1000 | Loss: 0.00001428
Iteration 110/1000 | Loss: 0.00001428
Iteration 111/1000 | Loss: 0.00001428
Iteration 112/1000 | Loss: 0.00001428
Iteration 113/1000 | Loss: 0.00001428
Iteration 114/1000 | Loss: 0.00001428
Iteration 115/1000 | Loss: 0.00001428
Iteration 116/1000 | Loss: 0.00001473
Iteration 117/1000 | Loss: 0.00001447
Iteration 118/1000 | Loss: 0.00002026
Iteration 119/1000 | Loss: 0.00001442
Iteration 120/1000 | Loss: 0.00001428
Iteration 121/1000 | Loss: 0.00001428
Iteration 122/1000 | Loss: 0.00001428
Iteration 123/1000 | Loss: 0.00001428
Iteration 124/1000 | Loss: 0.00001428
Iteration 125/1000 | Loss: 0.00001428
Iteration 126/1000 | Loss: 0.00001427
Iteration 127/1000 | Loss: 0.00001427
Iteration 128/1000 | Loss: 0.00001426
Iteration 129/1000 | Loss: 0.00001426
Iteration 130/1000 | Loss: 0.00001426
Iteration 131/1000 | Loss: 0.00001426
Iteration 132/1000 | Loss: 0.00001430
Iteration 133/1000 | Loss: 0.00001430
Iteration 134/1000 | Loss: 0.00001430
Iteration 135/1000 | Loss: 0.00001429
Iteration 136/1000 | Loss: 0.00001429
Iteration 137/1000 | Loss: 0.00001429
Iteration 138/1000 | Loss: 0.00001429
Iteration 139/1000 | Loss: 0.00001428
Iteration 140/1000 | Loss: 0.00001428
Iteration 141/1000 | Loss: 0.00001428
Iteration 142/1000 | Loss: 0.00001428
Iteration 143/1000 | Loss: 0.00002873
Iteration 144/1000 | Loss: 0.00002280
Iteration 145/1000 | Loss: 0.00002374
Iteration 146/1000 | Loss: 0.00001569
Iteration 147/1000 | Loss: 0.00001663
Iteration 148/1000 | Loss: 0.00001619
Iteration 149/1000 | Loss: 0.00001444
Iteration 150/1000 | Loss: 0.00001422
Iteration 151/1000 | Loss: 0.00001422
Iteration 152/1000 | Loss: 0.00001421
Iteration 153/1000 | Loss: 0.00001421
Iteration 154/1000 | Loss: 0.00001421
Iteration 155/1000 | Loss: 0.00001421
Iteration 156/1000 | Loss: 0.00001421
Iteration 157/1000 | Loss: 0.00001421
Iteration 158/1000 | Loss: 0.00001421
Iteration 159/1000 | Loss: 0.00001421
Iteration 160/1000 | Loss: 0.00001421
Iteration 161/1000 | Loss: 0.00001421
Iteration 162/1000 | Loss: 0.00001421
Iteration 163/1000 | Loss: 0.00001421
Iteration 164/1000 | Loss: 0.00001421
Iteration 165/1000 | Loss: 0.00001421
Iteration 166/1000 | Loss: 0.00001420
Iteration 167/1000 | Loss: 0.00001420
Iteration 168/1000 | Loss: 0.00001420
Iteration 169/1000 | Loss: 0.00001420
Iteration 170/1000 | Loss: 0.00001420
Iteration 171/1000 | Loss: 0.00001420
Iteration 172/1000 | Loss: 0.00001420
Iteration 173/1000 | Loss: 0.00001420
Iteration 174/1000 | Loss: 0.00001420
Iteration 175/1000 | Loss: 0.00001420
Iteration 176/1000 | Loss: 0.00001420
Iteration 177/1000 | Loss: 0.00001420
Iteration 178/1000 | Loss: 0.00001419
Iteration 179/1000 | Loss: 0.00001419
Iteration 180/1000 | Loss: 0.00001419
Iteration 181/1000 | Loss: 0.00001419
Iteration 182/1000 | Loss: 0.00001419
Iteration 183/1000 | Loss: 0.00001419
Iteration 184/1000 | Loss: 0.00001419
Iteration 185/1000 | Loss: 0.00001419
Iteration 186/1000 | Loss: 0.00001419
Iteration 187/1000 | Loss: 0.00001419
Iteration 188/1000 | Loss: 0.00001419
Iteration 189/1000 | Loss: 0.00001419
Iteration 190/1000 | Loss: 0.00001419
Iteration 191/1000 | Loss: 0.00001419
Iteration 192/1000 | Loss: 0.00001419
Iteration 193/1000 | Loss: 0.00001419
Iteration 194/1000 | Loss: 0.00001419
Iteration 195/1000 | Loss: 0.00001419
Iteration 196/1000 | Loss: 0.00001419
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [1.4190100046107545e-05, 1.4190100046107545e-05, 1.4190100046107545e-05, 1.4190100046107545e-05, 1.4190100046107545e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4190100046107545e-05

Optimization complete. Final v2v error: 3.053975820541382 mm

Highest mean error: 10.73338794708252 mm for frame 181

Lowest mean error: 2.564157009124756 mm for frame 8

Saving results

Total time: 144.24736404418945
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00900187
Iteration 2/25 | Loss: 0.00136859
Iteration 3/25 | Loss: 0.00125445
Iteration 4/25 | Loss: 0.00123385
Iteration 5/25 | Loss: 0.00122706
Iteration 6/25 | Loss: 0.00122527
Iteration 7/25 | Loss: 0.00122526
Iteration 8/25 | Loss: 0.00122526
Iteration 9/25 | Loss: 0.00122526
Iteration 10/25 | Loss: 0.00122526
Iteration 11/25 | Loss: 0.00122526
Iteration 12/25 | Loss: 0.00122526
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012252615997567773, 0.0012252615997567773, 0.0012252615997567773, 0.0012252615997567773, 0.0012252615997567773]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012252615997567773

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31620717
Iteration 2/25 | Loss: 0.00121381
Iteration 3/25 | Loss: 0.00121379
Iteration 4/25 | Loss: 0.00121379
Iteration 5/25 | Loss: 0.00121379
Iteration 6/25 | Loss: 0.00121379
Iteration 7/25 | Loss: 0.00121379
Iteration 8/25 | Loss: 0.00121379
Iteration 9/25 | Loss: 0.00121379
Iteration 10/25 | Loss: 0.00121379
Iteration 11/25 | Loss: 0.00121379
Iteration 12/25 | Loss: 0.00121379
Iteration 13/25 | Loss: 0.00121379
Iteration 14/25 | Loss: 0.00121379
Iteration 15/25 | Loss: 0.00121379
Iteration 16/25 | Loss: 0.00121379
Iteration 17/25 | Loss: 0.00121379
Iteration 18/25 | Loss: 0.00121379
Iteration 19/25 | Loss: 0.00121379
Iteration 20/25 | Loss: 0.00121379
Iteration 21/25 | Loss: 0.00121379
Iteration 22/25 | Loss: 0.00121379
Iteration 23/25 | Loss: 0.00121379
Iteration 24/25 | Loss: 0.00121379
Iteration 25/25 | Loss: 0.00121379

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121379
Iteration 2/1000 | Loss: 0.00004721
Iteration 3/1000 | Loss: 0.00003219
Iteration 4/1000 | Loss: 0.00002648
Iteration 5/1000 | Loss: 0.00002429
Iteration 6/1000 | Loss: 0.00002312
Iteration 7/1000 | Loss: 0.00002233
Iteration 8/1000 | Loss: 0.00002178
Iteration 9/1000 | Loss: 0.00002113
Iteration 10/1000 | Loss: 0.00002076
Iteration 11/1000 | Loss: 0.00002047
Iteration 12/1000 | Loss: 0.00002042
Iteration 13/1000 | Loss: 0.00002026
Iteration 14/1000 | Loss: 0.00002010
Iteration 15/1000 | Loss: 0.00002008
Iteration 16/1000 | Loss: 0.00002008
Iteration 17/1000 | Loss: 0.00002007
Iteration 18/1000 | Loss: 0.00001996
Iteration 19/1000 | Loss: 0.00001996
Iteration 20/1000 | Loss: 0.00001991
Iteration 21/1000 | Loss: 0.00001989
Iteration 22/1000 | Loss: 0.00001984
Iteration 23/1000 | Loss: 0.00001982
Iteration 24/1000 | Loss: 0.00001980
Iteration 25/1000 | Loss: 0.00001980
Iteration 26/1000 | Loss: 0.00001978
Iteration 27/1000 | Loss: 0.00001978
Iteration 28/1000 | Loss: 0.00001973
Iteration 29/1000 | Loss: 0.00001970
Iteration 30/1000 | Loss: 0.00001969
Iteration 31/1000 | Loss: 0.00001968
Iteration 32/1000 | Loss: 0.00001968
Iteration 33/1000 | Loss: 0.00001967
Iteration 34/1000 | Loss: 0.00001967
Iteration 35/1000 | Loss: 0.00001967
Iteration 36/1000 | Loss: 0.00001967
Iteration 37/1000 | Loss: 0.00001966
Iteration 38/1000 | Loss: 0.00001965
Iteration 39/1000 | Loss: 0.00001964
Iteration 40/1000 | Loss: 0.00001964
Iteration 41/1000 | Loss: 0.00001964
Iteration 42/1000 | Loss: 0.00001963
Iteration 43/1000 | Loss: 0.00001963
Iteration 44/1000 | Loss: 0.00001963
Iteration 45/1000 | Loss: 0.00001963
Iteration 46/1000 | Loss: 0.00001962
Iteration 47/1000 | Loss: 0.00001962
Iteration 48/1000 | Loss: 0.00001960
Iteration 49/1000 | Loss: 0.00001960
Iteration 50/1000 | Loss: 0.00001960
Iteration 51/1000 | Loss: 0.00001960
Iteration 52/1000 | Loss: 0.00001960
Iteration 53/1000 | Loss: 0.00001959
Iteration 54/1000 | Loss: 0.00001959
Iteration 55/1000 | Loss: 0.00001959
Iteration 56/1000 | Loss: 0.00001958
Iteration 57/1000 | Loss: 0.00001958
Iteration 58/1000 | Loss: 0.00001957
Iteration 59/1000 | Loss: 0.00001957
Iteration 60/1000 | Loss: 0.00001956
Iteration 61/1000 | Loss: 0.00001956
Iteration 62/1000 | Loss: 0.00001955
Iteration 63/1000 | Loss: 0.00001955
Iteration 64/1000 | Loss: 0.00001955
Iteration 65/1000 | Loss: 0.00001954
Iteration 66/1000 | Loss: 0.00001954
Iteration 67/1000 | Loss: 0.00001954
Iteration 68/1000 | Loss: 0.00001953
Iteration 69/1000 | Loss: 0.00001953
Iteration 70/1000 | Loss: 0.00001952
Iteration 71/1000 | Loss: 0.00001952
Iteration 72/1000 | Loss: 0.00001951
Iteration 73/1000 | Loss: 0.00001950
Iteration 74/1000 | Loss: 0.00001950
Iteration 75/1000 | Loss: 0.00001949
Iteration 76/1000 | Loss: 0.00001949
Iteration 77/1000 | Loss: 0.00001949
Iteration 78/1000 | Loss: 0.00001949
Iteration 79/1000 | Loss: 0.00001948
Iteration 80/1000 | Loss: 0.00001948
Iteration 81/1000 | Loss: 0.00001948
Iteration 82/1000 | Loss: 0.00001948
Iteration 83/1000 | Loss: 0.00001947
Iteration 84/1000 | Loss: 0.00001947
Iteration 85/1000 | Loss: 0.00001947
Iteration 86/1000 | Loss: 0.00001946
Iteration 87/1000 | Loss: 0.00001946
Iteration 88/1000 | Loss: 0.00001945
Iteration 89/1000 | Loss: 0.00001945
Iteration 90/1000 | Loss: 0.00001944
Iteration 91/1000 | Loss: 0.00001944
Iteration 92/1000 | Loss: 0.00001944
Iteration 93/1000 | Loss: 0.00001944
Iteration 94/1000 | Loss: 0.00001943
Iteration 95/1000 | Loss: 0.00001943
Iteration 96/1000 | Loss: 0.00001943
Iteration 97/1000 | Loss: 0.00001943
Iteration 98/1000 | Loss: 0.00001942
Iteration 99/1000 | Loss: 0.00001942
Iteration 100/1000 | Loss: 0.00001942
Iteration 101/1000 | Loss: 0.00001941
Iteration 102/1000 | Loss: 0.00001941
Iteration 103/1000 | Loss: 0.00001941
Iteration 104/1000 | Loss: 0.00001941
Iteration 105/1000 | Loss: 0.00001940
Iteration 106/1000 | Loss: 0.00001940
Iteration 107/1000 | Loss: 0.00001940
Iteration 108/1000 | Loss: 0.00001940
Iteration 109/1000 | Loss: 0.00001939
Iteration 110/1000 | Loss: 0.00001939
Iteration 111/1000 | Loss: 0.00001939
Iteration 112/1000 | Loss: 0.00001939
Iteration 113/1000 | Loss: 0.00001938
Iteration 114/1000 | Loss: 0.00001938
Iteration 115/1000 | Loss: 0.00001938
Iteration 116/1000 | Loss: 0.00001938
Iteration 117/1000 | Loss: 0.00001938
Iteration 118/1000 | Loss: 0.00001937
Iteration 119/1000 | Loss: 0.00001937
Iteration 120/1000 | Loss: 0.00001937
Iteration 121/1000 | Loss: 0.00001937
Iteration 122/1000 | Loss: 0.00001937
Iteration 123/1000 | Loss: 0.00001937
Iteration 124/1000 | Loss: 0.00001937
Iteration 125/1000 | Loss: 0.00001937
Iteration 126/1000 | Loss: 0.00001936
Iteration 127/1000 | Loss: 0.00001936
Iteration 128/1000 | Loss: 0.00001936
Iteration 129/1000 | Loss: 0.00001936
Iteration 130/1000 | Loss: 0.00001936
Iteration 131/1000 | Loss: 0.00001936
Iteration 132/1000 | Loss: 0.00001936
Iteration 133/1000 | Loss: 0.00001936
Iteration 134/1000 | Loss: 0.00001935
Iteration 135/1000 | Loss: 0.00001935
Iteration 136/1000 | Loss: 0.00001935
Iteration 137/1000 | Loss: 0.00001935
Iteration 138/1000 | Loss: 0.00001934
Iteration 139/1000 | Loss: 0.00001934
Iteration 140/1000 | Loss: 0.00001934
Iteration 141/1000 | Loss: 0.00001934
Iteration 142/1000 | Loss: 0.00001934
Iteration 143/1000 | Loss: 0.00001934
Iteration 144/1000 | Loss: 0.00001934
Iteration 145/1000 | Loss: 0.00001934
Iteration 146/1000 | Loss: 0.00001934
Iteration 147/1000 | Loss: 0.00001934
Iteration 148/1000 | Loss: 0.00001934
Iteration 149/1000 | Loss: 0.00001933
Iteration 150/1000 | Loss: 0.00001933
Iteration 151/1000 | Loss: 0.00001933
Iteration 152/1000 | Loss: 0.00001932
Iteration 153/1000 | Loss: 0.00001932
Iteration 154/1000 | Loss: 0.00001932
Iteration 155/1000 | Loss: 0.00001931
Iteration 156/1000 | Loss: 0.00001931
Iteration 157/1000 | Loss: 0.00001931
Iteration 158/1000 | Loss: 0.00001931
Iteration 159/1000 | Loss: 0.00001931
Iteration 160/1000 | Loss: 0.00001930
Iteration 161/1000 | Loss: 0.00001930
Iteration 162/1000 | Loss: 0.00001930
Iteration 163/1000 | Loss: 0.00001930
Iteration 164/1000 | Loss: 0.00001930
Iteration 165/1000 | Loss: 0.00001930
Iteration 166/1000 | Loss: 0.00001930
Iteration 167/1000 | Loss: 0.00001930
Iteration 168/1000 | Loss: 0.00001930
Iteration 169/1000 | Loss: 0.00001930
Iteration 170/1000 | Loss: 0.00001930
Iteration 171/1000 | Loss: 0.00001929
Iteration 172/1000 | Loss: 0.00001929
Iteration 173/1000 | Loss: 0.00001929
Iteration 174/1000 | Loss: 0.00001929
Iteration 175/1000 | Loss: 0.00001929
Iteration 176/1000 | Loss: 0.00001928
Iteration 177/1000 | Loss: 0.00001928
Iteration 178/1000 | Loss: 0.00001928
Iteration 179/1000 | Loss: 0.00001928
Iteration 180/1000 | Loss: 0.00001928
Iteration 181/1000 | Loss: 0.00001928
Iteration 182/1000 | Loss: 0.00001928
Iteration 183/1000 | Loss: 0.00001928
Iteration 184/1000 | Loss: 0.00001928
Iteration 185/1000 | Loss: 0.00001927
Iteration 186/1000 | Loss: 0.00001927
Iteration 187/1000 | Loss: 0.00001927
Iteration 188/1000 | Loss: 0.00001927
Iteration 189/1000 | Loss: 0.00001927
Iteration 190/1000 | Loss: 0.00001927
Iteration 191/1000 | Loss: 0.00001927
Iteration 192/1000 | Loss: 0.00001927
Iteration 193/1000 | Loss: 0.00001927
Iteration 194/1000 | Loss: 0.00001927
Iteration 195/1000 | Loss: 0.00001927
Iteration 196/1000 | Loss: 0.00001927
Iteration 197/1000 | Loss: 0.00001927
Iteration 198/1000 | Loss: 0.00001927
Iteration 199/1000 | Loss: 0.00001927
Iteration 200/1000 | Loss: 0.00001927
Iteration 201/1000 | Loss: 0.00001927
Iteration 202/1000 | Loss: 0.00001927
Iteration 203/1000 | Loss: 0.00001927
Iteration 204/1000 | Loss: 0.00001926
Iteration 205/1000 | Loss: 0.00001926
Iteration 206/1000 | Loss: 0.00001926
Iteration 207/1000 | Loss: 0.00001926
Iteration 208/1000 | Loss: 0.00001926
Iteration 209/1000 | Loss: 0.00001926
Iteration 210/1000 | Loss: 0.00001926
Iteration 211/1000 | Loss: 0.00001926
Iteration 212/1000 | Loss: 0.00001926
Iteration 213/1000 | Loss: 0.00001926
Iteration 214/1000 | Loss: 0.00001926
Iteration 215/1000 | Loss: 0.00001926
Iteration 216/1000 | Loss: 0.00001926
Iteration 217/1000 | Loss: 0.00001926
Iteration 218/1000 | Loss: 0.00001926
Iteration 219/1000 | Loss: 0.00001926
Iteration 220/1000 | Loss: 0.00001926
Iteration 221/1000 | Loss: 0.00001926
Iteration 222/1000 | Loss: 0.00001925
Iteration 223/1000 | Loss: 0.00001925
Iteration 224/1000 | Loss: 0.00001925
Iteration 225/1000 | Loss: 0.00001925
Iteration 226/1000 | Loss: 0.00001925
Iteration 227/1000 | Loss: 0.00001925
Iteration 228/1000 | Loss: 0.00001925
Iteration 229/1000 | Loss: 0.00001925
Iteration 230/1000 | Loss: 0.00001925
Iteration 231/1000 | Loss: 0.00001925
Iteration 232/1000 | Loss: 0.00001925
Iteration 233/1000 | Loss: 0.00001925
Iteration 234/1000 | Loss: 0.00001925
Iteration 235/1000 | Loss: 0.00001925
Iteration 236/1000 | Loss: 0.00001925
Iteration 237/1000 | Loss: 0.00001925
Iteration 238/1000 | Loss: 0.00001925
Iteration 239/1000 | Loss: 0.00001925
Iteration 240/1000 | Loss: 0.00001925
Iteration 241/1000 | Loss: 0.00001925
Iteration 242/1000 | Loss: 0.00001925
Iteration 243/1000 | Loss: 0.00001925
Iteration 244/1000 | Loss: 0.00001925
Iteration 245/1000 | Loss: 0.00001925
Iteration 246/1000 | Loss: 0.00001925
Iteration 247/1000 | Loss: 0.00001925
Iteration 248/1000 | Loss: 0.00001925
Iteration 249/1000 | Loss: 0.00001925
Iteration 250/1000 | Loss: 0.00001925
Iteration 251/1000 | Loss: 0.00001925
Iteration 252/1000 | Loss: 0.00001925
Iteration 253/1000 | Loss: 0.00001925
Iteration 254/1000 | Loss: 0.00001925
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 254. Stopping optimization.
Last 5 losses: [1.9245284420321696e-05, 1.9245284420321696e-05, 1.9245284420321696e-05, 1.9245284420321696e-05, 1.9245284420321696e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9245284420321696e-05

Optimization complete. Final v2v error: 3.688957929611206 mm

Highest mean error: 5.240966796875 mm for frame 68

Lowest mean error: 3.108949899673462 mm for frame 44

Saving results

Total time: 46.49407911300659
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00900439
Iteration 2/25 | Loss: 0.00143569
Iteration 3/25 | Loss: 0.00126349
Iteration 4/25 | Loss: 0.00124150
Iteration 5/25 | Loss: 0.00123496
Iteration 6/25 | Loss: 0.00123327
Iteration 7/25 | Loss: 0.00123327
Iteration 8/25 | Loss: 0.00123327
Iteration 9/25 | Loss: 0.00123327
Iteration 10/25 | Loss: 0.00123327
Iteration 11/25 | Loss: 0.00123327
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012332744663581252, 0.0012332744663581252, 0.0012332744663581252, 0.0012332744663581252, 0.0012332744663581252]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012332744663581252

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26898658
Iteration 2/25 | Loss: 0.00109951
Iteration 3/25 | Loss: 0.00109951
Iteration 4/25 | Loss: 0.00109951
Iteration 5/25 | Loss: 0.00109951
Iteration 6/25 | Loss: 0.00109951
Iteration 7/25 | Loss: 0.00109951
Iteration 8/25 | Loss: 0.00109951
Iteration 9/25 | Loss: 0.00109951
Iteration 10/25 | Loss: 0.00109951
Iteration 11/25 | Loss: 0.00109951
Iteration 12/25 | Loss: 0.00109951
Iteration 13/25 | Loss: 0.00109951
Iteration 14/25 | Loss: 0.00109951
Iteration 15/25 | Loss: 0.00109951
Iteration 16/25 | Loss: 0.00109951
Iteration 17/25 | Loss: 0.00109951
Iteration 18/25 | Loss: 0.00109951
Iteration 19/25 | Loss: 0.00109951
Iteration 20/25 | Loss: 0.00109951
Iteration 21/25 | Loss: 0.00109951
Iteration 22/25 | Loss: 0.00109951
Iteration 23/25 | Loss: 0.00109951
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0010995113989338279, 0.0010995113989338279, 0.0010995113989338279, 0.0010995113989338279, 0.0010995113989338279]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010995113989338279

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109951
Iteration 2/1000 | Loss: 0.00003976
Iteration 3/1000 | Loss: 0.00002742
Iteration 4/1000 | Loss: 0.00002333
Iteration 5/1000 | Loss: 0.00002171
Iteration 6/1000 | Loss: 0.00002079
Iteration 7/1000 | Loss: 0.00002016
Iteration 8/1000 | Loss: 0.00001955
Iteration 9/1000 | Loss: 0.00001912
Iteration 10/1000 | Loss: 0.00001886
Iteration 11/1000 | Loss: 0.00001873
Iteration 12/1000 | Loss: 0.00001869
Iteration 13/1000 | Loss: 0.00001863
Iteration 14/1000 | Loss: 0.00001861
Iteration 15/1000 | Loss: 0.00001860
Iteration 16/1000 | Loss: 0.00001846
Iteration 17/1000 | Loss: 0.00001839
Iteration 18/1000 | Loss: 0.00001839
Iteration 19/1000 | Loss: 0.00001834
Iteration 20/1000 | Loss: 0.00001831
Iteration 21/1000 | Loss: 0.00001830
Iteration 22/1000 | Loss: 0.00001829
Iteration 23/1000 | Loss: 0.00001821
Iteration 24/1000 | Loss: 0.00001818
Iteration 25/1000 | Loss: 0.00001817
Iteration 26/1000 | Loss: 0.00001815
Iteration 27/1000 | Loss: 0.00001813
Iteration 28/1000 | Loss: 0.00001813
Iteration 29/1000 | Loss: 0.00001812
Iteration 30/1000 | Loss: 0.00001811
Iteration 31/1000 | Loss: 0.00001811
Iteration 32/1000 | Loss: 0.00001811
Iteration 33/1000 | Loss: 0.00001811
Iteration 34/1000 | Loss: 0.00001811
Iteration 35/1000 | Loss: 0.00001811
Iteration 36/1000 | Loss: 0.00001811
Iteration 37/1000 | Loss: 0.00001811
Iteration 38/1000 | Loss: 0.00001811
Iteration 39/1000 | Loss: 0.00001811
Iteration 40/1000 | Loss: 0.00001810
Iteration 41/1000 | Loss: 0.00001810
Iteration 42/1000 | Loss: 0.00001809
Iteration 43/1000 | Loss: 0.00001808
Iteration 44/1000 | Loss: 0.00001808
Iteration 45/1000 | Loss: 0.00001807
Iteration 46/1000 | Loss: 0.00001807
Iteration 47/1000 | Loss: 0.00001806
Iteration 48/1000 | Loss: 0.00001806
Iteration 49/1000 | Loss: 0.00001806
Iteration 50/1000 | Loss: 0.00001806
Iteration 51/1000 | Loss: 0.00001806
Iteration 52/1000 | Loss: 0.00001806
Iteration 53/1000 | Loss: 0.00001805
Iteration 54/1000 | Loss: 0.00001805
Iteration 55/1000 | Loss: 0.00001805
Iteration 56/1000 | Loss: 0.00001805
Iteration 57/1000 | Loss: 0.00001805
Iteration 58/1000 | Loss: 0.00001805
Iteration 59/1000 | Loss: 0.00001804
Iteration 60/1000 | Loss: 0.00001803
Iteration 61/1000 | Loss: 0.00001803
Iteration 62/1000 | Loss: 0.00001803
Iteration 63/1000 | Loss: 0.00001803
Iteration 64/1000 | Loss: 0.00001802
Iteration 65/1000 | Loss: 0.00001802
Iteration 66/1000 | Loss: 0.00001802
Iteration 67/1000 | Loss: 0.00001802
Iteration 68/1000 | Loss: 0.00001802
Iteration 69/1000 | Loss: 0.00001802
Iteration 70/1000 | Loss: 0.00001802
Iteration 71/1000 | Loss: 0.00001801
Iteration 72/1000 | Loss: 0.00001801
Iteration 73/1000 | Loss: 0.00001800
Iteration 74/1000 | Loss: 0.00001800
Iteration 75/1000 | Loss: 0.00001800
Iteration 76/1000 | Loss: 0.00001800
Iteration 77/1000 | Loss: 0.00001799
Iteration 78/1000 | Loss: 0.00001799
Iteration 79/1000 | Loss: 0.00001798
Iteration 80/1000 | Loss: 0.00001798
Iteration 81/1000 | Loss: 0.00001798
Iteration 82/1000 | Loss: 0.00001797
Iteration 83/1000 | Loss: 0.00001797
Iteration 84/1000 | Loss: 0.00001797
Iteration 85/1000 | Loss: 0.00001797
Iteration 86/1000 | Loss: 0.00001797
Iteration 87/1000 | Loss: 0.00001797
Iteration 88/1000 | Loss: 0.00001796
Iteration 89/1000 | Loss: 0.00001796
Iteration 90/1000 | Loss: 0.00001796
Iteration 91/1000 | Loss: 0.00001796
Iteration 92/1000 | Loss: 0.00001795
Iteration 93/1000 | Loss: 0.00001795
Iteration 94/1000 | Loss: 0.00001795
Iteration 95/1000 | Loss: 0.00001794
Iteration 96/1000 | Loss: 0.00001794
Iteration 97/1000 | Loss: 0.00001794
Iteration 98/1000 | Loss: 0.00001793
Iteration 99/1000 | Loss: 0.00001792
Iteration 100/1000 | Loss: 0.00001792
Iteration 101/1000 | Loss: 0.00001792
Iteration 102/1000 | Loss: 0.00001791
Iteration 103/1000 | Loss: 0.00001791
Iteration 104/1000 | Loss: 0.00001791
Iteration 105/1000 | Loss: 0.00001790
Iteration 106/1000 | Loss: 0.00001789
Iteration 107/1000 | Loss: 0.00001789
Iteration 108/1000 | Loss: 0.00001789
Iteration 109/1000 | Loss: 0.00001788
Iteration 110/1000 | Loss: 0.00001788
Iteration 111/1000 | Loss: 0.00001788
Iteration 112/1000 | Loss: 0.00001788
Iteration 113/1000 | Loss: 0.00001787
Iteration 114/1000 | Loss: 0.00001787
Iteration 115/1000 | Loss: 0.00001787
Iteration 116/1000 | Loss: 0.00001787
Iteration 117/1000 | Loss: 0.00001786
Iteration 118/1000 | Loss: 0.00001786
Iteration 119/1000 | Loss: 0.00001786
Iteration 120/1000 | Loss: 0.00001786
Iteration 121/1000 | Loss: 0.00001786
Iteration 122/1000 | Loss: 0.00001785
Iteration 123/1000 | Loss: 0.00001785
Iteration 124/1000 | Loss: 0.00001785
Iteration 125/1000 | Loss: 0.00001785
Iteration 126/1000 | Loss: 0.00001785
Iteration 127/1000 | Loss: 0.00001784
Iteration 128/1000 | Loss: 0.00001784
Iteration 129/1000 | Loss: 0.00001784
Iteration 130/1000 | Loss: 0.00001784
Iteration 131/1000 | Loss: 0.00001784
Iteration 132/1000 | Loss: 0.00001783
Iteration 133/1000 | Loss: 0.00001783
Iteration 134/1000 | Loss: 0.00001783
Iteration 135/1000 | Loss: 0.00001783
Iteration 136/1000 | Loss: 0.00001783
Iteration 137/1000 | Loss: 0.00001783
Iteration 138/1000 | Loss: 0.00001782
Iteration 139/1000 | Loss: 0.00001782
Iteration 140/1000 | Loss: 0.00001782
Iteration 141/1000 | Loss: 0.00001782
Iteration 142/1000 | Loss: 0.00001782
Iteration 143/1000 | Loss: 0.00001782
Iteration 144/1000 | Loss: 0.00001782
Iteration 145/1000 | Loss: 0.00001782
Iteration 146/1000 | Loss: 0.00001782
Iteration 147/1000 | Loss: 0.00001781
Iteration 148/1000 | Loss: 0.00001781
Iteration 149/1000 | Loss: 0.00001781
Iteration 150/1000 | Loss: 0.00001781
Iteration 151/1000 | Loss: 0.00001781
Iteration 152/1000 | Loss: 0.00001781
Iteration 153/1000 | Loss: 0.00001781
Iteration 154/1000 | Loss: 0.00001781
Iteration 155/1000 | Loss: 0.00001780
Iteration 156/1000 | Loss: 0.00001780
Iteration 157/1000 | Loss: 0.00001780
Iteration 158/1000 | Loss: 0.00001780
Iteration 159/1000 | Loss: 0.00001780
Iteration 160/1000 | Loss: 0.00001780
Iteration 161/1000 | Loss: 0.00001780
Iteration 162/1000 | Loss: 0.00001780
Iteration 163/1000 | Loss: 0.00001780
Iteration 164/1000 | Loss: 0.00001780
Iteration 165/1000 | Loss: 0.00001780
Iteration 166/1000 | Loss: 0.00001779
Iteration 167/1000 | Loss: 0.00001779
Iteration 168/1000 | Loss: 0.00001779
Iteration 169/1000 | Loss: 0.00001779
Iteration 170/1000 | Loss: 0.00001779
Iteration 171/1000 | Loss: 0.00001778
Iteration 172/1000 | Loss: 0.00001778
Iteration 173/1000 | Loss: 0.00001778
Iteration 174/1000 | Loss: 0.00001777
Iteration 175/1000 | Loss: 0.00001777
Iteration 176/1000 | Loss: 0.00001777
Iteration 177/1000 | Loss: 0.00001777
Iteration 178/1000 | Loss: 0.00001777
Iteration 179/1000 | Loss: 0.00001776
Iteration 180/1000 | Loss: 0.00001776
Iteration 181/1000 | Loss: 0.00001776
Iteration 182/1000 | Loss: 0.00001776
Iteration 183/1000 | Loss: 0.00001776
Iteration 184/1000 | Loss: 0.00001776
Iteration 185/1000 | Loss: 0.00001776
Iteration 186/1000 | Loss: 0.00001776
Iteration 187/1000 | Loss: 0.00001775
Iteration 188/1000 | Loss: 0.00001775
Iteration 189/1000 | Loss: 0.00001775
Iteration 190/1000 | Loss: 0.00001775
Iteration 191/1000 | Loss: 0.00001775
Iteration 192/1000 | Loss: 0.00001775
Iteration 193/1000 | Loss: 0.00001775
Iteration 194/1000 | Loss: 0.00001775
Iteration 195/1000 | Loss: 0.00001775
Iteration 196/1000 | Loss: 0.00001775
Iteration 197/1000 | Loss: 0.00001775
Iteration 198/1000 | Loss: 0.00001774
Iteration 199/1000 | Loss: 0.00001774
Iteration 200/1000 | Loss: 0.00001774
Iteration 201/1000 | Loss: 0.00001774
Iteration 202/1000 | Loss: 0.00001773
Iteration 203/1000 | Loss: 0.00001773
Iteration 204/1000 | Loss: 0.00001773
Iteration 205/1000 | Loss: 0.00001773
Iteration 206/1000 | Loss: 0.00001773
Iteration 207/1000 | Loss: 0.00001773
Iteration 208/1000 | Loss: 0.00001773
Iteration 209/1000 | Loss: 0.00001773
Iteration 210/1000 | Loss: 0.00001773
Iteration 211/1000 | Loss: 0.00001773
Iteration 212/1000 | Loss: 0.00001773
Iteration 213/1000 | Loss: 0.00001773
Iteration 214/1000 | Loss: 0.00001772
Iteration 215/1000 | Loss: 0.00001772
Iteration 216/1000 | Loss: 0.00001772
Iteration 217/1000 | Loss: 0.00001772
Iteration 218/1000 | Loss: 0.00001772
Iteration 219/1000 | Loss: 0.00001772
Iteration 220/1000 | Loss: 0.00001772
Iteration 221/1000 | Loss: 0.00001772
Iteration 222/1000 | Loss: 0.00001772
Iteration 223/1000 | Loss: 0.00001772
Iteration 224/1000 | Loss: 0.00001772
Iteration 225/1000 | Loss: 0.00001772
Iteration 226/1000 | Loss: 0.00001772
Iteration 227/1000 | Loss: 0.00001772
Iteration 228/1000 | Loss: 0.00001772
Iteration 229/1000 | Loss: 0.00001772
Iteration 230/1000 | Loss: 0.00001772
Iteration 231/1000 | Loss: 0.00001772
Iteration 232/1000 | Loss: 0.00001772
Iteration 233/1000 | Loss: 0.00001772
Iteration 234/1000 | Loss: 0.00001772
Iteration 235/1000 | Loss: 0.00001772
Iteration 236/1000 | Loss: 0.00001772
Iteration 237/1000 | Loss: 0.00001772
Iteration 238/1000 | Loss: 0.00001772
Iteration 239/1000 | Loss: 0.00001772
Iteration 240/1000 | Loss: 0.00001772
Iteration 241/1000 | Loss: 0.00001772
Iteration 242/1000 | Loss: 0.00001772
Iteration 243/1000 | Loss: 0.00001772
Iteration 244/1000 | Loss: 0.00001772
Iteration 245/1000 | Loss: 0.00001772
Iteration 246/1000 | Loss: 0.00001772
Iteration 247/1000 | Loss: 0.00001772
Iteration 248/1000 | Loss: 0.00001772
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 248. Stopping optimization.
Last 5 losses: [1.7720210962579586e-05, 1.7720210962579586e-05, 1.7720210962579586e-05, 1.7720210962579586e-05, 1.7720210962579586e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7720210962579586e-05

Optimization complete. Final v2v error: 3.5582194328308105 mm

Highest mean error: 4.666169166564941 mm for frame 65

Lowest mean error: 2.957247734069824 mm for frame 46

Saving results

Total time: 43.28934931755066
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00454073
Iteration 2/25 | Loss: 0.00140081
Iteration 3/25 | Loss: 0.00122298
Iteration 4/25 | Loss: 0.00120240
Iteration 5/25 | Loss: 0.00119663
Iteration 6/25 | Loss: 0.00119546
Iteration 7/25 | Loss: 0.00119546
Iteration 8/25 | Loss: 0.00119546
Iteration 9/25 | Loss: 0.00119546
Iteration 10/25 | Loss: 0.00119546
Iteration 11/25 | Loss: 0.00119546
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001195460557937622, 0.001195460557937622, 0.001195460557937622, 0.001195460557937622, 0.001195460557937622]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001195460557937622

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30162644
Iteration 2/25 | Loss: 0.00159012
Iteration 3/25 | Loss: 0.00159012
Iteration 4/25 | Loss: 0.00159012
Iteration 5/25 | Loss: 0.00159012
Iteration 6/25 | Loss: 0.00159012
Iteration 7/25 | Loss: 0.00159012
Iteration 8/25 | Loss: 0.00159012
Iteration 9/25 | Loss: 0.00159012
Iteration 10/25 | Loss: 0.00159012
Iteration 11/25 | Loss: 0.00159012
Iteration 12/25 | Loss: 0.00159011
Iteration 13/25 | Loss: 0.00159011
Iteration 14/25 | Loss: 0.00159011
Iteration 15/25 | Loss: 0.00159011
Iteration 16/25 | Loss: 0.00159011
Iteration 17/25 | Loss: 0.00159011
Iteration 18/25 | Loss: 0.00159011
Iteration 19/25 | Loss: 0.00159011
Iteration 20/25 | Loss: 0.00159011
Iteration 21/25 | Loss: 0.00159011
Iteration 22/25 | Loss: 0.00159011
Iteration 23/25 | Loss: 0.00159011
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0015901143196970224, 0.0015901143196970224, 0.0015901143196970224, 0.0015901143196970224, 0.0015901143196970224]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015901143196970224

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00159011
Iteration 2/1000 | Loss: 0.00003034
Iteration 3/1000 | Loss: 0.00002102
Iteration 4/1000 | Loss: 0.00001910
Iteration 5/1000 | Loss: 0.00001811
Iteration 6/1000 | Loss: 0.00001734
Iteration 7/1000 | Loss: 0.00001693
Iteration 8/1000 | Loss: 0.00001664
Iteration 9/1000 | Loss: 0.00001642
Iteration 10/1000 | Loss: 0.00001626
Iteration 11/1000 | Loss: 0.00001623
Iteration 12/1000 | Loss: 0.00001614
Iteration 13/1000 | Loss: 0.00001603
Iteration 14/1000 | Loss: 0.00001602
Iteration 15/1000 | Loss: 0.00001599
Iteration 16/1000 | Loss: 0.00001598
Iteration 17/1000 | Loss: 0.00001596
Iteration 18/1000 | Loss: 0.00001596
Iteration 19/1000 | Loss: 0.00001596
Iteration 20/1000 | Loss: 0.00001595
Iteration 21/1000 | Loss: 0.00001591
Iteration 22/1000 | Loss: 0.00001586
Iteration 23/1000 | Loss: 0.00001581
Iteration 24/1000 | Loss: 0.00001577
Iteration 25/1000 | Loss: 0.00001576
Iteration 26/1000 | Loss: 0.00001575
Iteration 27/1000 | Loss: 0.00001574
Iteration 28/1000 | Loss: 0.00001574
Iteration 29/1000 | Loss: 0.00001573
Iteration 30/1000 | Loss: 0.00001570
Iteration 31/1000 | Loss: 0.00001570
Iteration 32/1000 | Loss: 0.00001569
Iteration 33/1000 | Loss: 0.00001569
Iteration 34/1000 | Loss: 0.00001568
Iteration 35/1000 | Loss: 0.00001566
Iteration 36/1000 | Loss: 0.00001560
Iteration 37/1000 | Loss: 0.00001560
Iteration 38/1000 | Loss: 0.00001557
Iteration 39/1000 | Loss: 0.00001557
Iteration 40/1000 | Loss: 0.00001555
Iteration 41/1000 | Loss: 0.00001553
Iteration 42/1000 | Loss: 0.00001551
Iteration 43/1000 | Loss: 0.00001551
Iteration 44/1000 | Loss: 0.00001548
Iteration 45/1000 | Loss: 0.00001548
Iteration 46/1000 | Loss: 0.00001548
Iteration 47/1000 | Loss: 0.00001548
Iteration 48/1000 | Loss: 0.00001547
Iteration 49/1000 | Loss: 0.00001546
Iteration 50/1000 | Loss: 0.00001545
Iteration 51/1000 | Loss: 0.00001545
Iteration 52/1000 | Loss: 0.00001545
Iteration 53/1000 | Loss: 0.00001545
Iteration 54/1000 | Loss: 0.00001545
Iteration 55/1000 | Loss: 0.00001544
Iteration 56/1000 | Loss: 0.00001544
Iteration 57/1000 | Loss: 0.00001541
Iteration 58/1000 | Loss: 0.00001541
Iteration 59/1000 | Loss: 0.00001541
Iteration 60/1000 | Loss: 0.00001541
Iteration 61/1000 | Loss: 0.00001541
Iteration 62/1000 | Loss: 0.00001541
Iteration 63/1000 | Loss: 0.00001540
Iteration 64/1000 | Loss: 0.00001540
Iteration 65/1000 | Loss: 0.00001540
Iteration 66/1000 | Loss: 0.00001540
Iteration 67/1000 | Loss: 0.00001539
Iteration 68/1000 | Loss: 0.00001539
Iteration 69/1000 | Loss: 0.00001538
Iteration 70/1000 | Loss: 0.00001538
Iteration 71/1000 | Loss: 0.00001538
Iteration 72/1000 | Loss: 0.00001537
Iteration 73/1000 | Loss: 0.00001537
Iteration 74/1000 | Loss: 0.00001537
Iteration 75/1000 | Loss: 0.00001536
Iteration 76/1000 | Loss: 0.00001536
Iteration 77/1000 | Loss: 0.00001536
Iteration 78/1000 | Loss: 0.00001536
Iteration 79/1000 | Loss: 0.00001536
Iteration 80/1000 | Loss: 0.00001536
Iteration 81/1000 | Loss: 0.00001536
Iteration 82/1000 | Loss: 0.00001536
Iteration 83/1000 | Loss: 0.00001535
Iteration 84/1000 | Loss: 0.00001535
Iteration 85/1000 | Loss: 0.00001535
Iteration 86/1000 | Loss: 0.00001535
Iteration 87/1000 | Loss: 0.00001535
Iteration 88/1000 | Loss: 0.00001535
Iteration 89/1000 | Loss: 0.00001535
Iteration 90/1000 | Loss: 0.00001534
Iteration 91/1000 | Loss: 0.00001534
Iteration 92/1000 | Loss: 0.00001534
Iteration 93/1000 | Loss: 0.00001534
Iteration 94/1000 | Loss: 0.00001534
Iteration 95/1000 | Loss: 0.00001533
Iteration 96/1000 | Loss: 0.00001533
Iteration 97/1000 | Loss: 0.00001533
Iteration 98/1000 | Loss: 0.00001533
Iteration 99/1000 | Loss: 0.00001533
Iteration 100/1000 | Loss: 0.00001533
Iteration 101/1000 | Loss: 0.00001532
Iteration 102/1000 | Loss: 0.00001532
Iteration 103/1000 | Loss: 0.00001532
Iteration 104/1000 | Loss: 0.00001532
Iteration 105/1000 | Loss: 0.00001531
Iteration 106/1000 | Loss: 0.00001531
Iteration 107/1000 | Loss: 0.00001531
Iteration 108/1000 | Loss: 0.00001531
Iteration 109/1000 | Loss: 0.00001531
Iteration 110/1000 | Loss: 0.00001531
Iteration 111/1000 | Loss: 0.00001530
Iteration 112/1000 | Loss: 0.00001530
Iteration 113/1000 | Loss: 0.00001530
Iteration 114/1000 | Loss: 0.00001530
Iteration 115/1000 | Loss: 0.00001530
Iteration 116/1000 | Loss: 0.00001530
Iteration 117/1000 | Loss: 0.00001530
Iteration 118/1000 | Loss: 0.00001530
Iteration 119/1000 | Loss: 0.00001530
Iteration 120/1000 | Loss: 0.00001529
Iteration 121/1000 | Loss: 0.00001529
Iteration 122/1000 | Loss: 0.00001529
Iteration 123/1000 | Loss: 0.00001529
Iteration 124/1000 | Loss: 0.00001529
Iteration 125/1000 | Loss: 0.00001529
Iteration 126/1000 | Loss: 0.00001529
Iteration 127/1000 | Loss: 0.00001529
Iteration 128/1000 | Loss: 0.00001529
Iteration 129/1000 | Loss: 0.00001529
Iteration 130/1000 | Loss: 0.00001529
Iteration 131/1000 | Loss: 0.00001529
Iteration 132/1000 | Loss: 0.00001529
Iteration 133/1000 | Loss: 0.00001529
Iteration 134/1000 | Loss: 0.00001529
Iteration 135/1000 | Loss: 0.00001528
Iteration 136/1000 | Loss: 0.00001528
Iteration 137/1000 | Loss: 0.00001528
Iteration 138/1000 | Loss: 0.00001528
Iteration 139/1000 | Loss: 0.00001528
Iteration 140/1000 | Loss: 0.00001528
Iteration 141/1000 | Loss: 0.00001528
Iteration 142/1000 | Loss: 0.00001528
Iteration 143/1000 | Loss: 0.00001528
Iteration 144/1000 | Loss: 0.00001528
Iteration 145/1000 | Loss: 0.00001528
Iteration 146/1000 | Loss: 0.00001528
Iteration 147/1000 | Loss: 0.00001528
Iteration 148/1000 | Loss: 0.00001528
Iteration 149/1000 | Loss: 0.00001528
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.5280224033631384e-05, 1.5280224033631384e-05, 1.5280224033631384e-05, 1.5280224033631384e-05, 1.5280224033631384e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5280224033631384e-05

Optimization complete. Final v2v error: 3.251948833465576 mm

Highest mean error: 4.095739841461182 mm for frame 116

Lowest mean error: 2.520005464553833 mm for frame 164

Saving results

Total time: 45.73911499977112
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00898185
Iteration 2/25 | Loss: 0.00154395
Iteration 3/25 | Loss: 0.00129646
Iteration 4/25 | Loss: 0.00127592
Iteration 5/25 | Loss: 0.00127083
Iteration 6/25 | Loss: 0.00126974
Iteration 7/25 | Loss: 0.00126974
Iteration 8/25 | Loss: 0.00126974
Iteration 9/25 | Loss: 0.00126974
Iteration 10/25 | Loss: 0.00126974
Iteration 11/25 | Loss: 0.00126974
Iteration 12/25 | Loss: 0.00126974
Iteration 13/25 | Loss: 0.00126974
Iteration 14/25 | Loss: 0.00126974
Iteration 15/25 | Loss: 0.00126974
Iteration 16/25 | Loss: 0.00126974
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012697447091341019, 0.0012697447091341019, 0.0012697447091341019, 0.0012697447091341019, 0.0012697447091341019]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012697447091341019

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.05088639
Iteration 2/25 | Loss: 0.00110873
Iteration 3/25 | Loss: 0.00110871
Iteration 4/25 | Loss: 0.00110871
Iteration 5/25 | Loss: 0.00110871
Iteration 6/25 | Loss: 0.00110871
Iteration 7/25 | Loss: 0.00110871
Iteration 8/25 | Loss: 0.00110871
Iteration 9/25 | Loss: 0.00110871
Iteration 10/25 | Loss: 0.00110871
Iteration 11/25 | Loss: 0.00110871
Iteration 12/25 | Loss: 0.00110871
Iteration 13/25 | Loss: 0.00110871
Iteration 14/25 | Loss: 0.00110871
Iteration 15/25 | Loss: 0.00110871
Iteration 16/25 | Loss: 0.00110871
Iteration 17/25 | Loss: 0.00110871
Iteration 18/25 | Loss: 0.00110871
Iteration 19/25 | Loss: 0.00110871
Iteration 20/25 | Loss: 0.00110871
Iteration 21/25 | Loss: 0.00110871
Iteration 22/25 | Loss: 0.00110871
Iteration 23/25 | Loss: 0.00110871
Iteration 24/25 | Loss: 0.00110871
Iteration 25/25 | Loss: 0.00110871

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110871
Iteration 2/1000 | Loss: 0.00005171
Iteration 3/1000 | Loss: 0.00003518
Iteration 4/1000 | Loss: 0.00002789
Iteration 5/1000 | Loss: 0.00002635
Iteration 6/1000 | Loss: 0.00002513
Iteration 7/1000 | Loss: 0.00002449
Iteration 8/1000 | Loss: 0.00002382
Iteration 9/1000 | Loss: 0.00002323
Iteration 10/1000 | Loss: 0.00002292
Iteration 11/1000 | Loss: 0.00002266
Iteration 12/1000 | Loss: 0.00002236
Iteration 13/1000 | Loss: 0.00002217
Iteration 14/1000 | Loss: 0.00002196
Iteration 15/1000 | Loss: 0.00002184
Iteration 16/1000 | Loss: 0.00002165
Iteration 17/1000 | Loss: 0.00002149
Iteration 18/1000 | Loss: 0.00002133
Iteration 19/1000 | Loss: 0.00002119
Iteration 20/1000 | Loss: 0.00002111
Iteration 21/1000 | Loss: 0.00002105
Iteration 22/1000 | Loss: 0.00002101
Iteration 23/1000 | Loss: 0.00002101
Iteration 24/1000 | Loss: 0.00002100
Iteration 25/1000 | Loss: 0.00002099
Iteration 26/1000 | Loss: 0.00002099
Iteration 27/1000 | Loss: 0.00002097
Iteration 28/1000 | Loss: 0.00002091
Iteration 29/1000 | Loss: 0.00002091
Iteration 30/1000 | Loss: 0.00002087
Iteration 31/1000 | Loss: 0.00002084
Iteration 32/1000 | Loss: 0.00002084
Iteration 33/1000 | Loss: 0.00002082
Iteration 34/1000 | Loss: 0.00002081
Iteration 35/1000 | Loss: 0.00002081
Iteration 36/1000 | Loss: 0.00002080
Iteration 37/1000 | Loss: 0.00002080
Iteration 38/1000 | Loss: 0.00002080
Iteration 39/1000 | Loss: 0.00002079
Iteration 40/1000 | Loss: 0.00002079
Iteration 41/1000 | Loss: 0.00002078
Iteration 42/1000 | Loss: 0.00002078
Iteration 43/1000 | Loss: 0.00002078
Iteration 44/1000 | Loss: 0.00002078
Iteration 45/1000 | Loss: 0.00002077
Iteration 46/1000 | Loss: 0.00002077
Iteration 47/1000 | Loss: 0.00002077
Iteration 48/1000 | Loss: 0.00002076
Iteration 49/1000 | Loss: 0.00002076
Iteration 50/1000 | Loss: 0.00002076
Iteration 51/1000 | Loss: 0.00002076
Iteration 52/1000 | Loss: 0.00002075
Iteration 53/1000 | Loss: 0.00002075
Iteration 54/1000 | Loss: 0.00002075
Iteration 55/1000 | Loss: 0.00002075
Iteration 56/1000 | Loss: 0.00002075
Iteration 57/1000 | Loss: 0.00002075
Iteration 58/1000 | Loss: 0.00002075
Iteration 59/1000 | Loss: 0.00002074
Iteration 60/1000 | Loss: 0.00002074
Iteration 61/1000 | Loss: 0.00002074
Iteration 62/1000 | Loss: 0.00002074
Iteration 63/1000 | Loss: 0.00002074
Iteration 64/1000 | Loss: 0.00002074
Iteration 65/1000 | Loss: 0.00002074
Iteration 66/1000 | Loss: 0.00002074
Iteration 67/1000 | Loss: 0.00002074
Iteration 68/1000 | Loss: 0.00002074
Iteration 69/1000 | Loss: 0.00002074
Iteration 70/1000 | Loss: 0.00002074
Iteration 71/1000 | Loss: 0.00002073
Iteration 72/1000 | Loss: 0.00002073
Iteration 73/1000 | Loss: 0.00002073
Iteration 74/1000 | Loss: 0.00002073
Iteration 75/1000 | Loss: 0.00002073
Iteration 76/1000 | Loss: 0.00002073
Iteration 77/1000 | Loss: 0.00002073
Iteration 78/1000 | Loss: 0.00002073
Iteration 79/1000 | Loss: 0.00002073
Iteration 80/1000 | Loss: 0.00002073
Iteration 81/1000 | Loss: 0.00002073
Iteration 82/1000 | Loss: 0.00002072
Iteration 83/1000 | Loss: 0.00002072
Iteration 84/1000 | Loss: 0.00002072
Iteration 85/1000 | Loss: 0.00002072
Iteration 86/1000 | Loss: 0.00002072
Iteration 87/1000 | Loss: 0.00002072
Iteration 88/1000 | Loss: 0.00002072
Iteration 89/1000 | Loss: 0.00002072
Iteration 90/1000 | Loss: 0.00002072
Iteration 91/1000 | Loss: 0.00002072
Iteration 92/1000 | Loss: 0.00002072
Iteration 93/1000 | Loss: 0.00002072
Iteration 94/1000 | Loss: 0.00002072
Iteration 95/1000 | Loss: 0.00002072
Iteration 96/1000 | Loss: 0.00002072
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [2.0722620320157148e-05, 2.0722620320157148e-05, 2.0722620320157148e-05, 2.0722620320157148e-05, 2.0722620320157148e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0722620320157148e-05

Optimization complete. Final v2v error: 3.763549327850342 mm

Highest mean error: 5.104247570037842 mm for frame 103

Lowest mean error: 2.962390661239624 mm for frame 122

Saving results

Total time: 43.58482575416565
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00381289
Iteration 2/25 | Loss: 0.00133134
Iteration 3/25 | Loss: 0.00119569
Iteration 4/25 | Loss: 0.00117150
Iteration 5/25 | Loss: 0.00116523
Iteration 6/25 | Loss: 0.00116352
Iteration 7/25 | Loss: 0.00116315
Iteration 8/25 | Loss: 0.00116315
Iteration 9/25 | Loss: 0.00116315
Iteration 10/25 | Loss: 0.00116315
Iteration 11/25 | Loss: 0.00116315
Iteration 12/25 | Loss: 0.00116315
Iteration 13/25 | Loss: 0.00116315
Iteration 14/25 | Loss: 0.00116315
Iteration 15/25 | Loss: 0.00116315
Iteration 16/25 | Loss: 0.00116315
Iteration 17/25 | Loss: 0.00116315
Iteration 18/25 | Loss: 0.00116315
Iteration 19/25 | Loss: 0.00116315
Iteration 20/25 | Loss: 0.00116315
Iteration 21/25 | Loss: 0.00116315
Iteration 22/25 | Loss: 0.00116315
Iteration 23/25 | Loss: 0.00116315
Iteration 24/25 | Loss: 0.00116315
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0011631533270701766, 0.0011631533270701766, 0.0011631533270701766, 0.0011631533270701766, 0.0011631533270701766]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011631533270701766

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25623548
Iteration 2/25 | Loss: 0.00146881
Iteration 3/25 | Loss: 0.00146881
Iteration 4/25 | Loss: 0.00146881
Iteration 5/25 | Loss: 0.00146881
Iteration 6/25 | Loss: 0.00146881
Iteration 7/25 | Loss: 0.00146881
Iteration 8/25 | Loss: 0.00146881
Iteration 9/25 | Loss: 0.00146881
Iteration 10/25 | Loss: 0.00146881
Iteration 11/25 | Loss: 0.00146881
Iteration 12/25 | Loss: 0.00146881
Iteration 13/25 | Loss: 0.00146881
Iteration 14/25 | Loss: 0.00146881
Iteration 15/25 | Loss: 0.00146881
Iteration 16/25 | Loss: 0.00146881
Iteration 17/25 | Loss: 0.00146881
Iteration 18/25 | Loss: 0.00146881
Iteration 19/25 | Loss: 0.00146881
Iteration 20/25 | Loss: 0.00146881
Iteration 21/25 | Loss: 0.00146881
Iteration 22/25 | Loss: 0.00146881
Iteration 23/25 | Loss: 0.00146881
Iteration 24/25 | Loss: 0.00146881
Iteration 25/25 | Loss: 0.00146881

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00146881
Iteration 2/1000 | Loss: 0.00004547
Iteration 3/1000 | Loss: 0.00003173
Iteration 4/1000 | Loss: 0.00002434
Iteration 5/1000 | Loss: 0.00002194
Iteration 6/1000 | Loss: 0.00002011
Iteration 7/1000 | Loss: 0.00001897
Iteration 8/1000 | Loss: 0.00001776
Iteration 9/1000 | Loss: 0.00001724
Iteration 10/1000 | Loss: 0.00001685
Iteration 11/1000 | Loss: 0.00001655
Iteration 12/1000 | Loss: 0.00001632
Iteration 13/1000 | Loss: 0.00001613
Iteration 14/1000 | Loss: 0.00001612
Iteration 15/1000 | Loss: 0.00001596
Iteration 16/1000 | Loss: 0.00001587
Iteration 17/1000 | Loss: 0.00001583
Iteration 18/1000 | Loss: 0.00001582
Iteration 19/1000 | Loss: 0.00001578
Iteration 20/1000 | Loss: 0.00001576
Iteration 21/1000 | Loss: 0.00001575
Iteration 22/1000 | Loss: 0.00001574
Iteration 23/1000 | Loss: 0.00001573
Iteration 24/1000 | Loss: 0.00001572
Iteration 25/1000 | Loss: 0.00001572
Iteration 26/1000 | Loss: 0.00001571
Iteration 27/1000 | Loss: 0.00001570
Iteration 28/1000 | Loss: 0.00001570
Iteration 29/1000 | Loss: 0.00001569
Iteration 30/1000 | Loss: 0.00001565
Iteration 31/1000 | Loss: 0.00001563
Iteration 32/1000 | Loss: 0.00001562
Iteration 33/1000 | Loss: 0.00001561
Iteration 34/1000 | Loss: 0.00001560
Iteration 35/1000 | Loss: 0.00001557
Iteration 36/1000 | Loss: 0.00001557
Iteration 37/1000 | Loss: 0.00001555
Iteration 38/1000 | Loss: 0.00001555
Iteration 39/1000 | Loss: 0.00001554
Iteration 40/1000 | Loss: 0.00001554
Iteration 41/1000 | Loss: 0.00001554
Iteration 42/1000 | Loss: 0.00001553
Iteration 43/1000 | Loss: 0.00001553
Iteration 44/1000 | Loss: 0.00001552
Iteration 45/1000 | Loss: 0.00001552
Iteration 46/1000 | Loss: 0.00001552
Iteration 47/1000 | Loss: 0.00001550
Iteration 48/1000 | Loss: 0.00001550
Iteration 49/1000 | Loss: 0.00001550
Iteration 50/1000 | Loss: 0.00001550
Iteration 51/1000 | Loss: 0.00001550
Iteration 52/1000 | Loss: 0.00001550
Iteration 53/1000 | Loss: 0.00001549
Iteration 54/1000 | Loss: 0.00001549
Iteration 55/1000 | Loss: 0.00001548
Iteration 56/1000 | Loss: 0.00001548
Iteration 57/1000 | Loss: 0.00001548
Iteration 58/1000 | Loss: 0.00001547
Iteration 59/1000 | Loss: 0.00001547
Iteration 60/1000 | Loss: 0.00001546
Iteration 61/1000 | Loss: 0.00001546
Iteration 62/1000 | Loss: 0.00001546
Iteration 63/1000 | Loss: 0.00001546
Iteration 64/1000 | Loss: 0.00001546
Iteration 65/1000 | Loss: 0.00001545
Iteration 66/1000 | Loss: 0.00001545
Iteration 67/1000 | Loss: 0.00001545
Iteration 68/1000 | Loss: 0.00001545
Iteration 69/1000 | Loss: 0.00001545
Iteration 70/1000 | Loss: 0.00001545
Iteration 71/1000 | Loss: 0.00001545
Iteration 72/1000 | Loss: 0.00001545
Iteration 73/1000 | Loss: 0.00001545
Iteration 74/1000 | Loss: 0.00001545
Iteration 75/1000 | Loss: 0.00001544
Iteration 76/1000 | Loss: 0.00001544
Iteration 77/1000 | Loss: 0.00001544
Iteration 78/1000 | Loss: 0.00001544
Iteration 79/1000 | Loss: 0.00001543
Iteration 80/1000 | Loss: 0.00001543
Iteration 81/1000 | Loss: 0.00001543
Iteration 82/1000 | Loss: 0.00001543
Iteration 83/1000 | Loss: 0.00001543
Iteration 84/1000 | Loss: 0.00001542
Iteration 85/1000 | Loss: 0.00001542
Iteration 86/1000 | Loss: 0.00001542
Iteration 87/1000 | Loss: 0.00001542
Iteration 88/1000 | Loss: 0.00001541
Iteration 89/1000 | Loss: 0.00001541
Iteration 90/1000 | Loss: 0.00001541
Iteration 91/1000 | Loss: 0.00001540
Iteration 92/1000 | Loss: 0.00001540
Iteration 93/1000 | Loss: 0.00001540
Iteration 94/1000 | Loss: 0.00001539
Iteration 95/1000 | Loss: 0.00001539
Iteration 96/1000 | Loss: 0.00001539
Iteration 97/1000 | Loss: 0.00001539
Iteration 98/1000 | Loss: 0.00001539
Iteration 99/1000 | Loss: 0.00001539
Iteration 100/1000 | Loss: 0.00001539
Iteration 101/1000 | Loss: 0.00001539
Iteration 102/1000 | Loss: 0.00001538
Iteration 103/1000 | Loss: 0.00001538
Iteration 104/1000 | Loss: 0.00001538
Iteration 105/1000 | Loss: 0.00001538
Iteration 106/1000 | Loss: 0.00001538
Iteration 107/1000 | Loss: 0.00001538
Iteration 108/1000 | Loss: 0.00001538
Iteration 109/1000 | Loss: 0.00001538
Iteration 110/1000 | Loss: 0.00001538
Iteration 111/1000 | Loss: 0.00001537
Iteration 112/1000 | Loss: 0.00001537
Iteration 113/1000 | Loss: 0.00001537
Iteration 114/1000 | Loss: 0.00001537
Iteration 115/1000 | Loss: 0.00001537
Iteration 116/1000 | Loss: 0.00001537
Iteration 117/1000 | Loss: 0.00001537
Iteration 118/1000 | Loss: 0.00001537
Iteration 119/1000 | Loss: 0.00001536
Iteration 120/1000 | Loss: 0.00001536
Iteration 121/1000 | Loss: 0.00001536
Iteration 122/1000 | Loss: 0.00001536
Iteration 123/1000 | Loss: 0.00001536
Iteration 124/1000 | Loss: 0.00001536
Iteration 125/1000 | Loss: 0.00001536
Iteration 126/1000 | Loss: 0.00001536
Iteration 127/1000 | Loss: 0.00001536
Iteration 128/1000 | Loss: 0.00001535
Iteration 129/1000 | Loss: 0.00001535
Iteration 130/1000 | Loss: 0.00001535
Iteration 131/1000 | Loss: 0.00001535
Iteration 132/1000 | Loss: 0.00001535
Iteration 133/1000 | Loss: 0.00001535
Iteration 134/1000 | Loss: 0.00001535
Iteration 135/1000 | Loss: 0.00001535
Iteration 136/1000 | Loss: 0.00001535
Iteration 137/1000 | Loss: 0.00001535
Iteration 138/1000 | Loss: 0.00001535
Iteration 139/1000 | Loss: 0.00001535
Iteration 140/1000 | Loss: 0.00001534
Iteration 141/1000 | Loss: 0.00001534
Iteration 142/1000 | Loss: 0.00001534
Iteration 143/1000 | Loss: 0.00001534
Iteration 144/1000 | Loss: 0.00001533
Iteration 145/1000 | Loss: 0.00001533
Iteration 146/1000 | Loss: 0.00001533
Iteration 147/1000 | Loss: 0.00001533
Iteration 148/1000 | Loss: 0.00001533
Iteration 149/1000 | Loss: 0.00001533
Iteration 150/1000 | Loss: 0.00001533
Iteration 151/1000 | Loss: 0.00001533
Iteration 152/1000 | Loss: 0.00001532
Iteration 153/1000 | Loss: 0.00001532
Iteration 154/1000 | Loss: 0.00001532
Iteration 155/1000 | Loss: 0.00001532
Iteration 156/1000 | Loss: 0.00001532
Iteration 157/1000 | Loss: 0.00001532
Iteration 158/1000 | Loss: 0.00001532
Iteration 159/1000 | Loss: 0.00001532
Iteration 160/1000 | Loss: 0.00001532
Iteration 161/1000 | Loss: 0.00001531
Iteration 162/1000 | Loss: 0.00001531
Iteration 163/1000 | Loss: 0.00001531
Iteration 164/1000 | Loss: 0.00001531
Iteration 165/1000 | Loss: 0.00001531
Iteration 166/1000 | Loss: 0.00001531
Iteration 167/1000 | Loss: 0.00001531
Iteration 168/1000 | Loss: 0.00001531
Iteration 169/1000 | Loss: 0.00001531
Iteration 170/1000 | Loss: 0.00001531
Iteration 171/1000 | Loss: 0.00001531
Iteration 172/1000 | Loss: 0.00001530
Iteration 173/1000 | Loss: 0.00001530
Iteration 174/1000 | Loss: 0.00001530
Iteration 175/1000 | Loss: 0.00001530
Iteration 176/1000 | Loss: 0.00001530
Iteration 177/1000 | Loss: 0.00001530
Iteration 178/1000 | Loss: 0.00001530
Iteration 179/1000 | Loss: 0.00001530
Iteration 180/1000 | Loss: 0.00001530
Iteration 181/1000 | Loss: 0.00001529
Iteration 182/1000 | Loss: 0.00001529
Iteration 183/1000 | Loss: 0.00001529
Iteration 184/1000 | Loss: 0.00001529
Iteration 185/1000 | Loss: 0.00001529
Iteration 186/1000 | Loss: 0.00001529
Iteration 187/1000 | Loss: 0.00001529
Iteration 188/1000 | Loss: 0.00001529
Iteration 189/1000 | Loss: 0.00001529
Iteration 190/1000 | Loss: 0.00001528
Iteration 191/1000 | Loss: 0.00001528
Iteration 192/1000 | Loss: 0.00001528
Iteration 193/1000 | Loss: 0.00001528
Iteration 194/1000 | Loss: 0.00001528
Iteration 195/1000 | Loss: 0.00001528
Iteration 196/1000 | Loss: 0.00001528
Iteration 197/1000 | Loss: 0.00001527
Iteration 198/1000 | Loss: 0.00001527
Iteration 199/1000 | Loss: 0.00001527
Iteration 200/1000 | Loss: 0.00001527
Iteration 201/1000 | Loss: 0.00001527
Iteration 202/1000 | Loss: 0.00001527
Iteration 203/1000 | Loss: 0.00001527
Iteration 204/1000 | Loss: 0.00001527
Iteration 205/1000 | Loss: 0.00001527
Iteration 206/1000 | Loss: 0.00001527
Iteration 207/1000 | Loss: 0.00001527
Iteration 208/1000 | Loss: 0.00001527
Iteration 209/1000 | Loss: 0.00001526
Iteration 210/1000 | Loss: 0.00001526
Iteration 211/1000 | Loss: 0.00001526
Iteration 212/1000 | Loss: 0.00001526
Iteration 213/1000 | Loss: 0.00001526
Iteration 214/1000 | Loss: 0.00001526
Iteration 215/1000 | Loss: 0.00001526
Iteration 216/1000 | Loss: 0.00001526
Iteration 217/1000 | Loss: 0.00001525
Iteration 218/1000 | Loss: 0.00001525
Iteration 219/1000 | Loss: 0.00001525
Iteration 220/1000 | Loss: 0.00001525
Iteration 221/1000 | Loss: 0.00001525
Iteration 222/1000 | Loss: 0.00001525
Iteration 223/1000 | Loss: 0.00001525
Iteration 224/1000 | Loss: 0.00001525
Iteration 225/1000 | Loss: 0.00001525
Iteration 226/1000 | Loss: 0.00001525
Iteration 227/1000 | Loss: 0.00001524
Iteration 228/1000 | Loss: 0.00001524
Iteration 229/1000 | Loss: 0.00001524
Iteration 230/1000 | Loss: 0.00001524
Iteration 231/1000 | Loss: 0.00001524
Iteration 232/1000 | Loss: 0.00001524
Iteration 233/1000 | Loss: 0.00001524
Iteration 234/1000 | Loss: 0.00001523
Iteration 235/1000 | Loss: 0.00001523
Iteration 236/1000 | Loss: 0.00001523
Iteration 237/1000 | Loss: 0.00001523
Iteration 238/1000 | Loss: 0.00001523
Iteration 239/1000 | Loss: 0.00001523
Iteration 240/1000 | Loss: 0.00001523
Iteration 241/1000 | Loss: 0.00001523
Iteration 242/1000 | Loss: 0.00001523
Iteration 243/1000 | Loss: 0.00001522
Iteration 244/1000 | Loss: 0.00001522
Iteration 245/1000 | Loss: 0.00001522
Iteration 246/1000 | Loss: 0.00001522
Iteration 247/1000 | Loss: 0.00001522
Iteration 248/1000 | Loss: 0.00001522
Iteration 249/1000 | Loss: 0.00001522
Iteration 250/1000 | Loss: 0.00001522
Iteration 251/1000 | Loss: 0.00001522
Iteration 252/1000 | Loss: 0.00001522
Iteration 253/1000 | Loss: 0.00001522
Iteration 254/1000 | Loss: 0.00001522
Iteration 255/1000 | Loss: 0.00001522
Iteration 256/1000 | Loss: 0.00001522
Iteration 257/1000 | Loss: 0.00001522
Iteration 258/1000 | Loss: 0.00001522
Iteration 259/1000 | Loss: 0.00001522
Iteration 260/1000 | Loss: 0.00001522
Iteration 261/1000 | Loss: 0.00001521
Iteration 262/1000 | Loss: 0.00001521
Iteration 263/1000 | Loss: 0.00001521
Iteration 264/1000 | Loss: 0.00001521
Iteration 265/1000 | Loss: 0.00001521
Iteration 266/1000 | Loss: 0.00001521
Iteration 267/1000 | Loss: 0.00001521
Iteration 268/1000 | Loss: 0.00001521
Iteration 269/1000 | Loss: 0.00001521
Iteration 270/1000 | Loss: 0.00001521
Iteration 271/1000 | Loss: 0.00001521
Iteration 272/1000 | Loss: 0.00001521
Iteration 273/1000 | Loss: 0.00001520
Iteration 274/1000 | Loss: 0.00001520
Iteration 275/1000 | Loss: 0.00001520
Iteration 276/1000 | Loss: 0.00001520
Iteration 277/1000 | Loss: 0.00001520
Iteration 278/1000 | Loss: 0.00001520
Iteration 279/1000 | Loss: 0.00001520
Iteration 280/1000 | Loss: 0.00001520
Iteration 281/1000 | Loss: 0.00001519
Iteration 282/1000 | Loss: 0.00001519
Iteration 283/1000 | Loss: 0.00001519
Iteration 284/1000 | Loss: 0.00001519
Iteration 285/1000 | Loss: 0.00001519
Iteration 286/1000 | Loss: 0.00001519
Iteration 287/1000 | Loss: 0.00001519
Iteration 288/1000 | Loss: 0.00001518
Iteration 289/1000 | Loss: 0.00001518
Iteration 290/1000 | Loss: 0.00001518
Iteration 291/1000 | Loss: 0.00001518
Iteration 292/1000 | Loss: 0.00001518
Iteration 293/1000 | Loss: 0.00001518
Iteration 294/1000 | Loss: 0.00001518
Iteration 295/1000 | Loss: 0.00001518
Iteration 296/1000 | Loss: 0.00001518
Iteration 297/1000 | Loss: 0.00001518
Iteration 298/1000 | Loss: 0.00001518
Iteration 299/1000 | Loss: 0.00001518
Iteration 300/1000 | Loss: 0.00001518
Iteration 301/1000 | Loss: 0.00001518
Iteration 302/1000 | Loss: 0.00001518
Iteration 303/1000 | Loss: 0.00001518
Iteration 304/1000 | Loss: 0.00001518
Iteration 305/1000 | Loss: 0.00001518
Iteration 306/1000 | Loss: 0.00001518
Iteration 307/1000 | Loss: 0.00001518
Iteration 308/1000 | Loss: 0.00001517
Iteration 309/1000 | Loss: 0.00001517
Iteration 310/1000 | Loss: 0.00001517
Iteration 311/1000 | Loss: 0.00001517
Iteration 312/1000 | Loss: 0.00001517
Iteration 313/1000 | Loss: 0.00001517
Iteration 314/1000 | Loss: 0.00001517
Iteration 315/1000 | Loss: 0.00001517
Iteration 316/1000 | Loss: 0.00001517
Iteration 317/1000 | Loss: 0.00001517
Iteration 318/1000 | Loss: 0.00001517
Iteration 319/1000 | Loss: 0.00001517
Iteration 320/1000 | Loss: 0.00001517
Iteration 321/1000 | Loss: 0.00001517
Iteration 322/1000 | Loss: 0.00001517
Iteration 323/1000 | Loss: 0.00001517
Iteration 324/1000 | Loss: 0.00001517
Iteration 325/1000 | Loss: 0.00001517
Iteration 326/1000 | Loss: 0.00001517
Iteration 327/1000 | Loss: 0.00001517
Iteration 328/1000 | Loss: 0.00001517
Iteration 329/1000 | Loss: 0.00001517
Iteration 330/1000 | Loss: 0.00001517
Iteration 331/1000 | Loss: 0.00001517
Iteration 332/1000 | Loss: 0.00001517
Iteration 333/1000 | Loss: 0.00001517
Iteration 334/1000 | Loss: 0.00001517
Iteration 335/1000 | Loss: 0.00001517
Iteration 336/1000 | Loss: 0.00001517
Iteration 337/1000 | Loss: 0.00001517
Iteration 338/1000 | Loss: 0.00001517
Iteration 339/1000 | Loss: 0.00001517
Iteration 340/1000 | Loss: 0.00001517
Iteration 341/1000 | Loss: 0.00001517
Iteration 342/1000 | Loss: 0.00001517
Iteration 343/1000 | Loss: 0.00001517
Iteration 344/1000 | Loss: 0.00001517
Iteration 345/1000 | Loss: 0.00001517
Iteration 346/1000 | Loss: 0.00001517
Iteration 347/1000 | Loss: 0.00001517
Iteration 348/1000 | Loss: 0.00001517
Iteration 349/1000 | Loss: 0.00001517
Iteration 350/1000 | Loss: 0.00001517
Iteration 351/1000 | Loss: 0.00001517
Iteration 352/1000 | Loss: 0.00001517
Iteration 353/1000 | Loss: 0.00001517
Iteration 354/1000 | Loss: 0.00001517
Iteration 355/1000 | Loss: 0.00001517
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 355. Stopping optimization.
Last 5 losses: [1.517259170213947e-05, 1.517259170213947e-05, 1.517259170213947e-05, 1.517259170213947e-05, 1.517259170213947e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.517259170213947e-05

Optimization complete. Final v2v error: 3.299769639968872 mm

Highest mean error: 3.9450061321258545 mm for frame 73

Lowest mean error: 2.7311062812805176 mm for frame 83

Saving results

Total time: 52.7408242225647
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00382618
Iteration 2/25 | Loss: 0.00126125
Iteration 3/25 | Loss: 0.00117397
Iteration 4/25 | Loss: 0.00116509
Iteration 5/25 | Loss: 0.00116278
Iteration 6/25 | Loss: 0.00116278
Iteration 7/25 | Loss: 0.00116278
Iteration 8/25 | Loss: 0.00116278
Iteration 9/25 | Loss: 0.00116278
Iteration 10/25 | Loss: 0.00116278
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011627767235040665, 0.0011627767235040665, 0.0011627767235040665, 0.0011627767235040665, 0.0011627767235040665]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011627767235040665

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43202353
Iteration 2/25 | Loss: 0.00126802
Iteration 3/25 | Loss: 0.00126802
Iteration 4/25 | Loss: 0.00126802
Iteration 5/25 | Loss: 0.00126802
Iteration 6/25 | Loss: 0.00126802
Iteration 7/25 | Loss: 0.00126802
Iteration 8/25 | Loss: 0.00126802
Iteration 9/25 | Loss: 0.00126802
Iteration 10/25 | Loss: 0.00126802
Iteration 11/25 | Loss: 0.00126802
Iteration 12/25 | Loss: 0.00126802
Iteration 13/25 | Loss: 0.00126802
Iteration 14/25 | Loss: 0.00126802
Iteration 15/25 | Loss: 0.00126802
Iteration 16/25 | Loss: 0.00126802
Iteration 17/25 | Loss: 0.00126802
Iteration 18/25 | Loss: 0.00126802
Iteration 19/25 | Loss: 0.00126802
Iteration 20/25 | Loss: 0.00126802
Iteration 21/25 | Loss: 0.00126802
Iteration 22/25 | Loss: 0.00126802
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012680167565122247, 0.0012680167565122247, 0.0012680167565122247, 0.0012680167565122247, 0.0012680167565122247]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012680167565122247

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126802
Iteration 2/1000 | Loss: 0.00002626
Iteration 3/1000 | Loss: 0.00001501
Iteration 4/1000 | Loss: 0.00001299
Iteration 5/1000 | Loss: 0.00001174
Iteration 6/1000 | Loss: 0.00001086
Iteration 7/1000 | Loss: 0.00001043
Iteration 8/1000 | Loss: 0.00001009
Iteration 9/1000 | Loss: 0.00000974
Iteration 10/1000 | Loss: 0.00000945
Iteration 11/1000 | Loss: 0.00000938
Iteration 12/1000 | Loss: 0.00000935
Iteration 13/1000 | Loss: 0.00000926
Iteration 14/1000 | Loss: 0.00000921
Iteration 15/1000 | Loss: 0.00000918
Iteration 16/1000 | Loss: 0.00000914
Iteration 17/1000 | Loss: 0.00000909
Iteration 18/1000 | Loss: 0.00000908
Iteration 19/1000 | Loss: 0.00000908
Iteration 20/1000 | Loss: 0.00000908
Iteration 21/1000 | Loss: 0.00000907
Iteration 22/1000 | Loss: 0.00000907
Iteration 23/1000 | Loss: 0.00000906
Iteration 24/1000 | Loss: 0.00000905
Iteration 25/1000 | Loss: 0.00000905
Iteration 26/1000 | Loss: 0.00000904
Iteration 27/1000 | Loss: 0.00000903
Iteration 28/1000 | Loss: 0.00000902
Iteration 29/1000 | Loss: 0.00000902
Iteration 30/1000 | Loss: 0.00000901
Iteration 31/1000 | Loss: 0.00000900
Iteration 32/1000 | Loss: 0.00000900
Iteration 33/1000 | Loss: 0.00000899
Iteration 34/1000 | Loss: 0.00000896
Iteration 35/1000 | Loss: 0.00000896
Iteration 36/1000 | Loss: 0.00000896
Iteration 37/1000 | Loss: 0.00000896
Iteration 38/1000 | Loss: 0.00000896
Iteration 39/1000 | Loss: 0.00000896
Iteration 40/1000 | Loss: 0.00000896
Iteration 41/1000 | Loss: 0.00000896
Iteration 42/1000 | Loss: 0.00000896
Iteration 43/1000 | Loss: 0.00000895
Iteration 44/1000 | Loss: 0.00000895
Iteration 45/1000 | Loss: 0.00000893
Iteration 46/1000 | Loss: 0.00000893
Iteration 47/1000 | Loss: 0.00000892
Iteration 48/1000 | Loss: 0.00000892
Iteration 49/1000 | Loss: 0.00000891
Iteration 50/1000 | Loss: 0.00000890
Iteration 51/1000 | Loss: 0.00000889
Iteration 52/1000 | Loss: 0.00000889
Iteration 53/1000 | Loss: 0.00000889
Iteration 54/1000 | Loss: 0.00000888
Iteration 55/1000 | Loss: 0.00000888
Iteration 56/1000 | Loss: 0.00000888
Iteration 57/1000 | Loss: 0.00000887
Iteration 58/1000 | Loss: 0.00000887
Iteration 59/1000 | Loss: 0.00000887
Iteration 60/1000 | Loss: 0.00000887
Iteration 61/1000 | Loss: 0.00000886
Iteration 62/1000 | Loss: 0.00000885
Iteration 63/1000 | Loss: 0.00000885
Iteration 64/1000 | Loss: 0.00000883
Iteration 65/1000 | Loss: 0.00000881
Iteration 66/1000 | Loss: 0.00000881
Iteration 67/1000 | Loss: 0.00000881
Iteration 68/1000 | Loss: 0.00000880
Iteration 69/1000 | Loss: 0.00000880
Iteration 70/1000 | Loss: 0.00000880
Iteration 71/1000 | Loss: 0.00000879
Iteration 72/1000 | Loss: 0.00000878
Iteration 73/1000 | Loss: 0.00000878
Iteration 74/1000 | Loss: 0.00000877
Iteration 75/1000 | Loss: 0.00000877
Iteration 76/1000 | Loss: 0.00000876
Iteration 77/1000 | Loss: 0.00000876
Iteration 78/1000 | Loss: 0.00000875
Iteration 79/1000 | Loss: 0.00000875
Iteration 80/1000 | Loss: 0.00000875
Iteration 81/1000 | Loss: 0.00000875
Iteration 82/1000 | Loss: 0.00000874
Iteration 83/1000 | Loss: 0.00000873
Iteration 84/1000 | Loss: 0.00000873
Iteration 85/1000 | Loss: 0.00000872
Iteration 86/1000 | Loss: 0.00000872
Iteration 87/1000 | Loss: 0.00000871
Iteration 88/1000 | Loss: 0.00000871
Iteration 89/1000 | Loss: 0.00000871
Iteration 90/1000 | Loss: 0.00000871
Iteration 91/1000 | Loss: 0.00000870
Iteration 92/1000 | Loss: 0.00000870
Iteration 93/1000 | Loss: 0.00000870
Iteration 94/1000 | Loss: 0.00000869
Iteration 95/1000 | Loss: 0.00000869
Iteration 96/1000 | Loss: 0.00000869
Iteration 97/1000 | Loss: 0.00000868
Iteration 98/1000 | Loss: 0.00000868
Iteration 99/1000 | Loss: 0.00000867
Iteration 100/1000 | Loss: 0.00000867
Iteration 101/1000 | Loss: 0.00000866
Iteration 102/1000 | Loss: 0.00000866
Iteration 103/1000 | Loss: 0.00000866
Iteration 104/1000 | Loss: 0.00000866
Iteration 105/1000 | Loss: 0.00000866
Iteration 106/1000 | Loss: 0.00000866
Iteration 107/1000 | Loss: 0.00000865
Iteration 108/1000 | Loss: 0.00000865
Iteration 109/1000 | Loss: 0.00000865
Iteration 110/1000 | Loss: 0.00000865
Iteration 111/1000 | Loss: 0.00000864
Iteration 112/1000 | Loss: 0.00000864
Iteration 113/1000 | Loss: 0.00000864
Iteration 114/1000 | Loss: 0.00000864
Iteration 115/1000 | Loss: 0.00000864
Iteration 116/1000 | Loss: 0.00000863
Iteration 117/1000 | Loss: 0.00000863
Iteration 118/1000 | Loss: 0.00000863
Iteration 119/1000 | Loss: 0.00000862
Iteration 120/1000 | Loss: 0.00000862
Iteration 121/1000 | Loss: 0.00000862
Iteration 122/1000 | Loss: 0.00000861
Iteration 123/1000 | Loss: 0.00000861
Iteration 124/1000 | Loss: 0.00000861
Iteration 125/1000 | Loss: 0.00000860
Iteration 126/1000 | Loss: 0.00000860
Iteration 127/1000 | Loss: 0.00000859
Iteration 128/1000 | Loss: 0.00000859
Iteration 129/1000 | Loss: 0.00000858
Iteration 130/1000 | Loss: 0.00000858
Iteration 131/1000 | Loss: 0.00000858
Iteration 132/1000 | Loss: 0.00000858
Iteration 133/1000 | Loss: 0.00000858
Iteration 134/1000 | Loss: 0.00000858
Iteration 135/1000 | Loss: 0.00000858
Iteration 136/1000 | Loss: 0.00000858
Iteration 137/1000 | Loss: 0.00000858
Iteration 138/1000 | Loss: 0.00000857
Iteration 139/1000 | Loss: 0.00000857
Iteration 140/1000 | Loss: 0.00000857
Iteration 141/1000 | Loss: 0.00000857
Iteration 142/1000 | Loss: 0.00000856
Iteration 143/1000 | Loss: 0.00000856
Iteration 144/1000 | Loss: 0.00000855
Iteration 145/1000 | Loss: 0.00000855
Iteration 146/1000 | Loss: 0.00000855
Iteration 147/1000 | Loss: 0.00000854
Iteration 148/1000 | Loss: 0.00000854
Iteration 149/1000 | Loss: 0.00000854
Iteration 150/1000 | Loss: 0.00000854
Iteration 151/1000 | Loss: 0.00000854
Iteration 152/1000 | Loss: 0.00000853
Iteration 153/1000 | Loss: 0.00000853
Iteration 154/1000 | Loss: 0.00000853
Iteration 155/1000 | Loss: 0.00000853
Iteration 156/1000 | Loss: 0.00000853
Iteration 157/1000 | Loss: 0.00000852
Iteration 158/1000 | Loss: 0.00000852
Iteration 159/1000 | Loss: 0.00000851
Iteration 160/1000 | Loss: 0.00000851
Iteration 161/1000 | Loss: 0.00000851
Iteration 162/1000 | Loss: 0.00000850
Iteration 163/1000 | Loss: 0.00000850
Iteration 164/1000 | Loss: 0.00000850
Iteration 165/1000 | Loss: 0.00000849
Iteration 166/1000 | Loss: 0.00000849
Iteration 167/1000 | Loss: 0.00000849
Iteration 168/1000 | Loss: 0.00000849
Iteration 169/1000 | Loss: 0.00000848
Iteration 170/1000 | Loss: 0.00000848
Iteration 171/1000 | Loss: 0.00000848
Iteration 172/1000 | Loss: 0.00000848
Iteration 173/1000 | Loss: 0.00000848
Iteration 174/1000 | Loss: 0.00000848
Iteration 175/1000 | Loss: 0.00000848
Iteration 176/1000 | Loss: 0.00000848
Iteration 177/1000 | Loss: 0.00000848
Iteration 178/1000 | Loss: 0.00000848
Iteration 179/1000 | Loss: 0.00000848
Iteration 180/1000 | Loss: 0.00000848
Iteration 181/1000 | Loss: 0.00000848
Iteration 182/1000 | Loss: 0.00000848
Iteration 183/1000 | Loss: 0.00000847
Iteration 184/1000 | Loss: 0.00000847
Iteration 185/1000 | Loss: 0.00000847
Iteration 186/1000 | Loss: 0.00000847
Iteration 187/1000 | Loss: 0.00000847
Iteration 188/1000 | Loss: 0.00000847
Iteration 189/1000 | Loss: 0.00000847
Iteration 190/1000 | Loss: 0.00000847
Iteration 191/1000 | Loss: 0.00000847
Iteration 192/1000 | Loss: 0.00000847
Iteration 193/1000 | Loss: 0.00000847
Iteration 194/1000 | Loss: 0.00000847
Iteration 195/1000 | Loss: 0.00000847
Iteration 196/1000 | Loss: 0.00000847
Iteration 197/1000 | Loss: 0.00000847
Iteration 198/1000 | Loss: 0.00000846
Iteration 199/1000 | Loss: 0.00000846
Iteration 200/1000 | Loss: 0.00000846
Iteration 201/1000 | Loss: 0.00000846
Iteration 202/1000 | Loss: 0.00000846
Iteration 203/1000 | Loss: 0.00000846
Iteration 204/1000 | Loss: 0.00000846
Iteration 205/1000 | Loss: 0.00000846
Iteration 206/1000 | Loss: 0.00000846
Iteration 207/1000 | Loss: 0.00000846
Iteration 208/1000 | Loss: 0.00000846
Iteration 209/1000 | Loss: 0.00000846
Iteration 210/1000 | Loss: 0.00000846
Iteration 211/1000 | Loss: 0.00000846
Iteration 212/1000 | Loss: 0.00000846
Iteration 213/1000 | Loss: 0.00000846
Iteration 214/1000 | Loss: 0.00000846
Iteration 215/1000 | Loss: 0.00000846
Iteration 216/1000 | Loss: 0.00000846
Iteration 217/1000 | Loss: 0.00000845
Iteration 218/1000 | Loss: 0.00000845
Iteration 219/1000 | Loss: 0.00000845
Iteration 220/1000 | Loss: 0.00000845
Iteration 221/1000 | Loss: 0.00000845
Iteration 222/1000 | Loss: 0.00000845
Iteration 223/1000 | Loss: 0.00000845
Iteration 224/1000 | Loss: 0.00000845
Iteration 225/1000 | Loss: 0.00000845
Iteration 226/1000 | Loss: 0.00000845
Iteration 227/1000 | Loss: 0.00000845
Iteration 228/1000 | Loss: 0.00000845
Iteration 229/1000 | Loss: 0.00000845
Iteration 230/1000 | Loss: 0.00000845
Iteration 231/1000 | Loss: 0.00000845
Iteration 232/1000 | Loss: 0.00000845
Iteration 233/1000 | Loss: 0.00000845
Iteration 234/1000 | Loss: 0.00000845
Iteration 235/1000 | Loss: 0.00000845
Iteration 236/1000 | Loss: 0.00000845
Iteration 237/1000 | Loss: 0.00000845
Iteration 238/1000 | Loss: 0.00000845
Iteration 239/1000 | Loss: 0.00000845
Iteration 240/1000 | Loss: 0.00000845
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 240. Stopping optimization.
Last 5 losses: [8.44871647132095e-06, 8.44871647132095e-06, 8.44871647132095e-06, 8.44871647132095e-06, 8.44871647132095e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.44871647132095e-06

Optimization complete. Final v2v error: 2.517291307449341 mm

Highest mean error: 2.900664806365967 mm for frame 145

Lowest mean error: 2.314413547515869 mm for frame 68

Saving results

Total time: 47.36459684371948
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00420927
Iteration 2/25 | Loss: 0.00129934
Iteration 3/25 | Loss: 0.00119365
Iteration 4/25 | Loss: 0.00118069
Iteration 5/25 | Loss: 0.00117741
Iteration 6/25 | Loss: 0.00117677
Iteration 7/25 | Loss: 0.00117677
Iteration 8/25 | Loss: 0.00117677
Iteration 9/25 | Loss: 0.00117677
Iteration 10/25 | Loss: 0.00117677
Iteration 11/25 | Loss: 0.00117677
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011767683317884803, 0.0011767683317884803, 0.0011767683317884803, 0.0011767683317884803, 0.0011767683317884803]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011767683317884803

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29220593
Iteration 2/25 | Loss: 0.00138502
Iteration 3/25 | Loss: 0.00138501
Iteration 4/25 | Loss: 0.00138501
Iteration 5/25 | Loss: 0.00138501
Iteration 6/25 | Loss: 0.00138501
Iteration 7/25 | Loss: 0.00138501
Iteration 8/25 | Loss: 0.00138501
Iteration 9/25 | Loss: 0.00138501
Iteration 10/25 | Loss: 0.00138501
Iteration 11/25 | Loss: 0.00138501
Iteration 12/25 | Loss: 0.00138501
Iteration 13/25 | Loss: 0.00138501
Iteration 14/25 | Loss: 0.00138501
Iteration 15/25 | Loss: 0.00138501
Iteration 16/25 | Loss: 0.00138501
Iteration 17/25 | Loss: 0.00138501
Iteration 18/25 | Loss: 0.00138501
Iteration 19/25 | Loss: 0.00138501
Iteration 20/25 | Loss: 0.00138501
Iteration 21/25 | Loss: 0.00138501
Iteration 22/25 | Loss: 0.00138501
Iteration 23/25 | Loss: 0.00138501
Iteration 24/25 | Loss: 0.00138501
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0013850078685209155, 0.0013850078685209155, 0.0013850078685209155, 0.0013850078685209155, 0.0013850078685209155]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013850078685209155

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00138501
Iteration 2/1000 | Loss: 0.00002671
Iteration 3/1000 | Loss: 0.00001794
Iteration 4/1000 | Loss: 0.00001484
Iteration 5/1000 | Loss: 0.00001363
Iteration 6/1000 | Loss: 0.00001286
Iteration 7/1000 | Loss: 0.00001243
Iteration 8/1000 | Loss: 0.00001194
Iteration 9/1000 | Loss: 0.00001170
Iteration 10/1000 | Loss: 0.00001147
Iteration 11/1000 | Loss: 0.00001120
Iteration 12/1000 | Loss: 0.00001116
Iteration 13/1000 | Loss: 0.00001098
Iteration 14/1000 | Loss: 0.00001096
Iteration 15/1000 | Loss: 0.00001094
Iteration 16/1000 | Loss: 0.00001093
Iteration 17/1000 | Loss: 0.00001091
Iteration 18/1000 | Loss: 0.00001088
Iteration 19/1000 | Loss: 0.00001088
Iteration 20/1000 | Loss: 0.00001083
Iteration 21/1000 | Loss: 0.00001080
Iteration 22/1000 | Loss: 0.00001079
Iteration 23/1000 | Loss: 0.00001079
Iteration 24/1000 | Loss: 0.00001079
Iteration 25/1000 | Loss: 0.00001078
Iteration 26/1000 | Loss: 0.00001078
Iteration 27/1000 | Loss: 0.00001078
Iteration 28/1000 | Loss: 0.00001078
Iteration 29/1000 | Loss: 0.00001077
Iteration 30/1000 | Loss: 0.00001073
Iteration 31/1000 | Loss: 0.00001067
Iteration 32/1000 | Loss: 0.00001066
Iteration 33/1000 | Loss: 0.00001066
Iteration 34/1000 | Loss: 0.00001065
Iteration 35/1000 | Loss: 0.00001064
Iteration 36/1000 | Loss: 0.00001064
Iteration 37/1000 | Loss: 0.00001063
Iteration 38/1000 | Loss: 0.00001063
Iteration 39/1000 | Loss: 0.00001062
Iteration 40/1000 | Loss: 0.00001062
Iteration 41/1000 | Loss: 0.00001062
Iteration 42/1000 | Loss: 0.00001061
Iteration 43/1000 | Loss: 0.00001060
Iteration 44/1000 | Loss: 0.00001058
Iteration 45/1000 | Loss: 0.00001057
Iteration 46/1000 | Loss: 0.00001057
Iteration 47/1000 | Loss: 0.00001056
Iteration 48/1000 | Loss: 0.00001056
Iteration 49/1000 | Loss: 0.00001056
Iteration 50/1000 | Loss: 0.00001055
Iteration 51/1000 | Loss: 0.00001054
Iteration 52/1000 | Loss: 0.00001053
Iteration 53/1000 | Loss: 0.00001053
Iteration 54/1000 | Loss: 0.00001052
Iteration 55/1000 | Loss: 0.00001052
Iteration 56/1000 | Loss: 0.00001051
Iteration 57/1000 | Loss: 0.00001051
Iteration 58/1000 | Loss: 0.00001050
Iteration 59/1000 | Loss: 0.00001050
Iteration 60/1000 | Loss: 0.00001050
Iteration 61/1000 | Loss: 0.00001049
Iteration 62/1000 | Loss: 0.00001049
Iteration 63/1000 | Loss: 0.00001049
Iteration 64/1000 | Loss: 0.00001049
Iteration 65/1000 | Loss: 0.00001049
Iteration 66/1000 | Loss: 0.00001049
Iteration 67/1000 | Loss: 0.00001048
Iteration 68/1000 | Loss: 0.00001048
Iteration 69/1000 | Loss: 0.00001048
Iteration 70/1000 | Loss: 0.00001048
Iteration 71/1000 | Loss: 0.00001048
Iteration 72/1000 | Loss: 0.00001048
Iteration 73/1000 | Loss: 0.00001048
Iteration 74/1000 | Loss: 0.00001048
Iteration 75/1000 | Loss: 0.00001048
Iteration 76/1000 | Loss: 0.00001048
Iteration 77/1000 | Loss: 0.00001048
Iteration 78/1000 | Loss: 0.00001048
Iteration 79/1000 | Loss: 0.00001047
Iteration 80/1000 | Loss: 0.00001047
Iteration 81/1000 | Loss: 0.00001047
Iteration 82/1000 | Loss: 0.00001047
Iteration 83/1000 | Loss: 0.00001047
Iteration 84/1000 | Loss: 0.00001046
Iteration 85/1000 | Loss: 0.00001046
Iteration 86/1000 | Loss: 0.00001046
Iteration 87/1000 | Loss: 0.00001046
Iteration 88/1000 | Loss: 0.00001046
Iteration 89/1000 | Loss: 0.00001046
Iteration 90/1000 | Loss: 0.00001045
Iteration 91/1000 | Loss: 0.00001045
Iteration 92/1000 | Loss: 0.00001045
Iteration 93/1000 | Loss: 0.00001044
Iteration 94/1000 | Loss: 0.00001044
Iteration 95/1000 | Loss: 0.00001044
Iteration 96/1000 | Loss: 0.00001044
Iteration 97/1000 | Loss: 0.00001044
Iteration 98/1000 | Loss: 0.00001043
Iteration 99/1000 | Loss: 0.00001043
Iteration 100/1000 | Loss: 0.00001043
Iteration 101/1000 | Loss: 0.00001043
Iteration 102/1000 | Loss: 0.00001043
Iteration 103/1000 | Loss: 0.00001042
Iteration 104/1000 | Loss: 0.00001042
Iteration 105/1000 | Loss: 0.00001042
Iteration 106/1000 | Loss: 0.00001042
Iteration 107/1000 | Loss: 0.00001042
Iteration 108/1000 | Loss: 0.00001042
Iteration 109/1000 | Loss: 0.00001041
Iteration 110/1000 | Loss: 0.00001041
Iteration 111/1000 | Loss: 0.00001041
Iteration 112/1000 | Loss: 0.00001041
Iteration 113/1000 | Loss: 0.00001041
Iteration 114/1000 | Loss: 0.00001041
Iteration 115/1000 | Loss: 0.00001041
Iteration 116/1000 | Loss: 0.00001041
Iteration 117/1000 | Loss: 0.00001041
Iteration 118/1000 | Loss: 0.00001041
Iteration 119/1000 | Loss: 0.00001041
Iteration 120/1000 | Loss: 0.00001041
Iteration 121/1000 | Loss: 0.00001040
Iteration 122/1000 | Loss: 0.00001040
Iteration 123/1000 | Loss: 0.00001039
Iteration 124/1000 | Loss: 0.00001039
Iteration 125/1000 | Loss: 0.00001039
Iteration 126/1000 | Loss: 0.00001038
Iteration 127/1000 | Loss: 0.00001038
Iteration 128/1000 | Loss: 0.00001038
Iteration 129/1000 | Loss: 0.00001038
Iteration 130/1000 | Loss: 0.00001038
Iteration 131/1000 | Loss: 0.00001038
Iteration 132/1000 | Loss: 0.00001038
Iteration 133/1000 | Loss: 0.00001037
Iteration 134/1000 | Loss: 0.00001037
Iteration 135/1000 | Loss: 0.00001037
Iteration 136/1000 | Loss: 0.00001037
Iteration 137/1000 | Loss: 0.00001037
Iteration 138/1000 | Loss: 0.00001037
Iteration 139/1000 | Loss: 0.00001037
Iteration 140/1000 | Loss: 0.00001037
Iteration 141/1000 | Loss: 0.00001037
Iteration 142/1000 | Loss: 0.00001036
Iteration 143/1000 | Loss: 0.00001036
Iteration 144/1000 | Loss: 0.00001036
Iteration 145/1000 | Loss: 0.00001036
Iteration 146/1000 | Loss: 0.00001036
Iteration 147/1000 | Loss: 0.00001036
Iteration 148/1000 | Loss: 0.00001036
Iteration 149/1000 | Loss: 0.00001036
Iteration 150/1000 | Loss: 0.00001036
Iteration 151/1000 | Loss: 0.00001035
Iteration 152/1000 | Loss: 0.00001035
Iteration 153/1000 | Loss: 0.00001035
Iteration 154/1000 | Loss: 0.00001035
Iteration 155/1000 | Loss: 0.00001035
Iteration 156/1000 | Loss: 0.00001035
Iteration 157/1000 | Loss: 0.00001035
Iteration 158/1000 | Loss: 0.00001035
Iteration 159/1000 | Loss: 0.00001035
Iteration 160/1000 | Loss: 0.00001034
Iteration 161/1000 | Loss: 0.00001034
Iteration 162/1000 | Loss: 0.00001034
Iteration 163/1000 | Loss: 0.00001034
Iteration 164/1000 | Loss: 0.00001034
Iteration 165/1000 | Loss: 0.00001034
Iteration 166/1000 | Loss: 0.00001034
Iteration 167/1000 | Loss: 0.00001034
Iteration 168/1000 | Loss: 0.00001033
Iteration 169/1000 | Loss: 0.00001033
Iteration 170/1000 | Loss: 0.00001033
Iteration 171/1000 | Loss: 0.00001033
Iteration 172/1000 | Loss: 0.00001033
Iteration 173/1000 | Loss: 0.00001032
Iteration 174/1000 | Loss: 0.00001032
Iteration 175/1000 | Loss: 0.00001032
Iteration 176/1000 | Loss: 0.00001032
Iteration 177/1000 | Loss: 0.00001032
Iteration 178/1000 | Loss: 0.00001032
Iteration 179/1000 | Loss: 0.00001032
Iteration 180/1000 | Loss: 0.00001032
Iteration 181/1000 | Loss: 0.00001032
Iteration 182/1000 | Loss: 0.00001032
Iteration 183/1000 | Loss: 0.00001032
Iteration 184/1000 | Loss: 0.00001032
Iteration 185/1000 | Loss: 0.00001032
Iteration 186/1000 | Loss: 0.00001032
Iteration 187/1000 | Loss: 0.00001031
Iteration 188/1000 | Loss: 0.00001031
Iteration 189/1000 | Loss: 0.00001031
Iteration 190/1000 | Loss: 0.00001030
Iteration 191/1000 | Loss: 0.00001030
Iteration 192/1000 | Loss: 0.00001030
Iteration 193/1000 | Loss: 0.00001030
Iteration 194/1000 | Loss: 0.00001030
Iteration 195/1000 | Loss: 0.00001030
Iteration 196/1000 | Loss: 0.00001030
Iteration 197/1000 | Loss: 0.00001030
Iteration 198/1000 | Loss: 0.00001030
Iteration 199/1000 | Loss: 0.00001030
Iteration 200/1000 | Loss: 0.00001030
Iteration 201/1000 | Loss: 0.00001030
Iteration 202/1000 | Loss: 0.00001030
Iteration 203/1000 | Loss: 0.00001029
Iteration 204/1000 | Loss: 0.00001029
Iteration 205/1000 | Loss: 0.00001029
Iteration 206/1000 | Loss: 0.00001029
Iteration 207/1000 | Loss: 0.00001029
Iteration 208/1000 | Loss: 0.00001029
Iteration 209/1000 | Loss: 0.00001029
Iteration 210/1000 | Loss: 0.00001029
Iteration 211/1000 | Loss: 0.00001029
Iteration 212/1000 | Loss: 0.00001029
Iteration 213/1000 | Loss: 0.00001029
Iteration 214/1000 | Loss: 0.00001029
Iteration 215/1000 | Loss: 0.00001028
Iteration 216/1000 | Loss: 0.00001028
Iteration 217/1000 | Loss: 0.00001028
Iteration 218/1000 | Loss: 0.00001028
Iteration 219/1000 | Loss: 0.00001028
Iteration 220/1000 | Loss: 0.00001028
Iteration 221/1000 | Loss: 0.00001028
Iteration 222/1000 | Loss: 0.00001028
Iteration 223/1000 | Loss: 0.00001028
Iteration 224/1000 | Loss: 0.00001028
Iteration 225/1000 | Loss: 0.00001028
Iteration 226/1000 | Loss: 0.00001027
Iteration 227/1000 | Loss: 0.00001027
Iteration 228/1000 | Loss: 0.00001027
Iteration 229/1000 | Loss: 0.00001027
Iteration 230/1000 | Loss: 0.00001027
Iteration 231/1000 | Loss: 0.00001027
Iteration 232/1000 | Loss: 0.00001027
Iteration 233/1000 | Loss: 0.00001027
Iteration 234/1000 | Loss: 0.00001027
Iteration 235/1000 | Loss: 0.00001027
Iteration 236/1000 | Loss: 0.00001027
Iteration 237/1000 | Loss: 0.00001026
Iteration 238/1000 | Loss: 0.00001026
Iteration 239/1000 | Loss: 0.00001026
Iteration 240/1000 | Loss: 0.00001026
Iteration 241/1000 | Loss: 0.00001026
Iteration 242/1000 | Loss: 0.00001026
Iteration 243/1000 | Loss: 0.00001026
Iteration 244/1000 | Loss: 0.00001026
Iteration 245/1000 | Loss: 0.00001026
Iteration 246/1000 | Loss: 0.00001025
Iteration 247/1000 | Loss: 0.00001025
Iteration 248/1000 | Loss: 0.00001025
Iteration 249/1000 | Loss: 0.00001025
Iteration 250/1000 | Loss: 0.00001025
Iteration 251/1000 | Loss: 0.00001025
Iteration 252/1000 | Loss: 0.00001025
Iteration 253/1000 | Loss: 0.00001025
Iteration 254/1000 | Loss: 0.00001025
Iteration 255/1000 | Loss: 0.00001025
Iteration 256/1000 | Loss: 0.00001025
Iteration 257/1000 | Loss: 0.00001025
Iteration 258/1000 | Loss: 0.00001025
Iteration 259/1000 | Loss: 0.00001025
Iteration 260/1000 | Loss: 0.00001025
Iteration 261/1000 | Loss: 0.00001025
Iteration 262/1000 | Loss: 0.00001025
Iteration 263/1000 | Loss: 0.00001025
Iteration 264/1000 | Loss: 0.00001025
Iteration 265/1000 | Loss: 0.00001025
Iteration 266/1000 | Loss: 0.00001025
Iteration 267/1000 | Loss: 0.00001025
Iteration 268/1000 | Loss: 0.00001024
Iteration 269/1000 | Loss: 0.00001024
Iteration 270/1000 | Loss: 0.00001024
Iteration 271/1000 | Loss: 0.00001024
Iteration 272/1000 | Loss: 0.00001024
Iteration 273/1000 | Loss: 0.00001024
Iteration 274/1000 | Loss: 0.00001024
Iteration 275/1000 | Loss: 0.00001024
Iteration 276/1000 | Loss: 0.00001024
Iteration 277/1000 | Loss: 0.00001024
Iteration 278/1000 | Loss: 0.00001024
Iteration 279/1000 | Loss: 0.00001024
Iteration 280/1000 | Loss: 0.00001024
Iteration 281/1000 | Loss: 0.00001024
Iteration 282/1000 | Loss: 0.00001024
Iteration 283/1000 | Loss: 0.00001024
Iteration 284/1000 | Loss: 0.00001024
Iteration 285/1000 | Loss: 0.00001024
Iteration 286/1000 | Loss: 0.00001024
Iteration 287/1000 | Loss: 0.00001024
Iteration 288/1000 | Loss: 0.00001024
Iteration 289/1000 | Loss: 0.00001024
Iteration 290/1000 | Loss: 0.00001024
Iteration 291/1000 | Loss: 0.00001024
Iteration 292/1000 | Loss: 0.00001024
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 292. Stopping optimization.
Last 5 losses: [1.0235659829049837e-05, 1.0235659829049837e-05, 1.0235659829049837e-05, 1.0235659829049837e-05, 1.0235659829049837e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0235659829049837e-05

Optimization complete. Final v2v error: 2.741671323776245 mm

Highest mean error: 3.815946102142334 mm for frame 64

Lowest mean error: 2.4688854217529297 mm for frame 44

Saving results

Total time: 53.051472187042236
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00864402
Iteration 2/25 | Loss: 0.00133494
Iteration 3/25 | Loss: 0.00120310
Iteration 4/25 | Loss: 0.00118995
Iteration 5/25 | Loss: 0.00118666
Iteration 6/25 | Loss: 0.00118599
Iteration 7/25 | Loss: 0.00118599
Iteration 8/25 | Loss: 0.00118599
Iteration 9/25 | Loss: 0.00118599
Iteration 10/25 | Loss: 0.00118599
Iteration 11/25 | Loss: 0.00118599
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011859944788739085, 0.0011859944788739085, 0.0011859944788739085, 0.0011859944788739085, 0.0011859944788739085]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011859944788739085

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38272500
Iteration 2/25 | Loss: 0.00124083
Iteration 3/25 | Loss: 0.00124083
Iteration 4/25 | Loss: 0.00124083
Iteration 5/25 | Loss: 0.00124083
Iteration 6/25 | Loss: 0.00124083
Iteration 7/25 | Loss: 0.00124083
Iteration 8/25 | Loss: 0.00124083
Iteration 9/25 | Loss: 0.00124083
Iteration 10/25 | Loss: 0.00124083
Iteration 11/25 | Loss: 0.00124083
Iteration 12/25 | Loss: 0.00124083
Iteration 13/25 | Loss: 0.00124083
Iteration 14/25 | Loss: 0.00124083
Iteration 15/25 | Loss: 0.00124083
Iteration 16/25 | Loss: 0.00124083
Iteration 17/25 | Loss: 0.00124083
Iteration 18/25 | Loss: 0.00124083
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012408301699906588, 0.0012408301699906588, 0.0012408301699906588, 0.0012408301699906588, 0.0012408301699906588]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012408301699906588

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124083
Iteration 2/1000 | Loss: 0.00002495
Iteration 3/1000 | Loss: 0.00001869
Iteration 4/1000 | Loss: 0.00001740
Iteration 5/1000 | Loss: 0.00001669
Iteration 6/1000 | Loss: 0.00001613
Iteration 7/1000 | Loss: 0.00001561
Iteration 8/1000 | Loss: 0.00001529
Iteration 9/1000 | Loss: 0.00001493
Iteration 10/1000 | Loss: 0.00001468
Iteration 11/1000 | Loss: 0.00001460
Iteration 12/1000 | Loss: 0.00001448
Iteration 13/1000 | Loss: 0.00001442
Iteration 14/1000 | Loss: 0.00001437
Iteration 15/1000 | Loss: 0.00001435
Iteration 16/1000 | Loss: 0.00001433
Iteration 17/1000 | Loss: 0.00001420
Iteration 18/1000 | Loss: 0.00001416
Iteration 19/1000 | Loss: 0.00001416
Iteration 20/1000 | Loss: 0.00001407
Iteration 21/1000 | Loss: 0.00001407
Iteration 22/1000 | Loss: 0.00001406
Iteration 23/1000 | Loss: 0.00001401
Iteration 24/1000 | Loss: 0.00001401
Iteration 25/1000 | Loss: 0.00001401
Iteration 26/1000 | Loss: 0.00001401
Iteration 27/1000 | Loss: 0.00001401
Iteration 28/1000 | Loss: 0.00001401
Iteration 29/1000 | Loss: 0.00001401
Iteration 30/1000 | Loss: 0.00001400
Iteration 31/1000 | Loss: 0.00001400
Iteration 32/1000 | Loss: 0.00001400
Iteration 33/1000 | Loss: 0.00001399
Iteration 34/1000 | Loss: 0.00001398
Iteration 35/1000 | Loss: 0.00001397
Iteration 36/1000 | Loss: 0.00001397
Iteration 37/1000 | Loss: 0.00001396
Iteration 38/1000 | Loss: 0.00001396
Iteration 39/1000 | Loss: 0.00001396
Iteration 40/1000 | Loss: 0.00001396
Iteration 41/1000 | Loss: 0.00001395
Iteration 42/1000 | Loss: 0.00001395
Iteration 43/1000 | Loss: 0.00001394
Iteration 44/1000 | Loss: 0.00001394
Iteration 45/1000 | Loss: 0.00001393
Iteration 46/1000 | Loss: 0.00001392
Iteration 47/1000 | Loss: 0.00001392
Iteration 48/1000 | Loss: 0.00001392
Iteration 49/1000 | Loss: 0.00001392
Iteration 50/1000 | Loss: 0.00001392
Iteration 51/1000 | Loss: 0.00001392
Iteration 52/1000 | Loss: 0.00001392
Iteration 53/1000 | Loss: 0.00001391
Iteration 54/1000 | Loss: 0.00001391
Iteration 55/1000 | Loss: 0.00001391
Iteration 56/1000 | Loss: 0.00001390
Iteration 57/1000 | Loss: 0.00001390
Iteration 58/1000 | Loss: 0.00001390
Iteration 59/1000 | Loss: 0.00001390
Iteration 60/1000 | Loss: 0.00001389
Iteration 61/1000 | Loss: 0.00001389
Iteration 62/1000 | Loss: 0.00001389
Iteration 63/1000 | Loss: 0.00001388
Iteration 64/1000 | Loss: 0.00001388
Iteration 65/1000 | Loss: 0.00001388
Iteration 66/1000 | Loss: 0.00001388
Iteration 67/1000 | Loss: 0.00001388
Iteration 68/1000 | Loss: 0.00001388
Iteration 69/1000 | Loss: 0.00001388
Iteration 70/1000 | Loss: 0.00001387
Iteration 71/1000 | Loss: 0.00001387
Iteration 72/1000 | Loss: 0.00001387
Iteration 73/1000 | Loss: 0.00001387
Iteration 74/1000 | Loss: 0.00001387
Iteration 75/1000 | Loss: 0.00001387
Iteration 76/1000 | Loss: 0.00001387
Iteration 77/1000 | Loss: 0.00001386
Iteration 78/1000 | Loss: 0.00001386
Iteration 79/1000 | Loss: 0.00001386
Iteration 80/1000 | Loss: 0.00001386
Iteration 81/1000 | Loss: 0.00001386
Iteration 82/1000 | Loss: 0.00001386
Iteration 83/1000 | Loss: 0.00001385
Iteration 84/1000 | Loss: 0.00001385
Iteration 85/1000 | Loss: 0.00001385
Iteration 86/1000 | Loss: 0.00001385
Iteration 87/1000 | Loss: 0.00001385
Iteration 88/1000 | Loss: 0.00001385
Iteration 89/1000 | Loss: 0.00001385
Iteration 90/1000 | Loss: 0.00001385
Iteration 91/1000 | Loss: 0.00001385
Iteration 92/1000 | Loss: 0.00001385
Iteration 93/1000 | Loss: 0.00001384
Iteration 94/1000 | Loss: 0.00001384
Iteration 95/1000 | Loss: 0.00001384
Iteration 96/1000 | Loss: 0.00001384
Iteration 97/1000 | Loss: 0.00001384
Iteration 98/1000 | Loss: 0.00001384
Iteration 99/1000 | Loss: 0.00001384
Iteration 100/1000 | Loss: 0.00001384
Iteration 101/1000 | Loss: 0.00001384
Iteration 102/1000 | Loss: 0.00001384
Iteration 103/1000 | Loss: 0.00001384
Iteration 104/1000 | Loss: 0.00001383
Iteration 105/1000 | Loss: 0.00001383
Iteration 106/1000 | Loss: 0.00001383
Iteration 107/1000 | Loss: 0.00001383
Iteration 108/1000 | Loss: 0.00001383
Iteration 109/1000 | Loss: 0.00001383
Iteration 110/1000 | Loss: 0.00001383
Iteration 111/1000 | Loss: 0.00001383
Iteration 112/1000 | Loss: 0.00001383
Iteration 113/1000 | Loss: 0.00001383
Iteration 114/1000 | Loss: 0.00001383
Iteration 115/1000 | Loss: 0.00001383
Iteration 116/1000 | Loss: 0.00001383
Iteration 117/1000 | Loss: 0.00001382
Iteration 118/1000 | Loss: 0.00001382
Iteration 119/1000 | Loss: 0.00001382
Iteration 120/1000 | Loss: 0.00001382
Iteration 121/1000 | Loss: 0.00001382
Iteration 122/1000 | Loss: 0.00001382
Iteration 123/1000 | Loss: 0.00001382
Iteration 124/1000 | Loss: 0.00001382
Iteration 125/1000 | Loss: 0.00001382
Iteration 126/1000 | Loss: 0.00001381
Iteration 127/1000 | Loss: 0.00001381
Iteration 128/1000 | Loss: 0.00001381
Iteration 129/1000 | Loss: 0.00001381
Iteration 130/1000 | Loss: 0.00001381
Iteration 131/1000 | Loss: 0.00001381
Iteration 132/1000 | Loss: 0.00001381
Iteration 133/1000 | Loss: 0.00001381
Iteration 134/1000 | Loss: 0.00001381
Iteration 135/1000 | Loss: 0.00001381
Iteration 136/1000 | Loss: 0.00001381
Iteration 137/1000 | Loss: 0.00001380
Iteration 138/1000 | Loss: 0.00001380
Iteration 139/1000 | Loss: 0.00001380
Iteration 140/1000 | Loss: 0.00001380
Iteration 141/1000 | Loss: 0.00001380
Iteration 142/1000 | Loss: 0.00001380
Iteration 143/1000 | Loss: 0.00001380
Iteration 144/1000 | Loss: 0.00001380
Iteration 145/1000 | Loss: 0.00001380
Iteration 146/1000 | Loss: 0.00001380
Iteration 147/1000 | Loss: 0.00001380
Iteration 148/1000 | Loss: 0.00001380
Iteration 149/1000 | Loss: 0.00001380
Iteration 150/1000 | Loss: 0.00001380
Iteration 151/1000 | Loss: 0.00001380
Iteration 152/1000 | Loss: 0.00001380
Iteration 153/1000 | Loss: 0.00001379
Iteration 154/1000 | Loss: 0.00001379
Iteration 155/1000 | Loss: 0.00001379
Iteration 156/1000 | Loss: 0.00001379
Iteration 157/1000 | Loss: 0.00001379
Iteration 158/1000 | Loss: 0.00001379
Iteration 159/1000 | Loss: 0.00001379
Iteration 160/1000 | Loss: 0.00001379
Iteration 161/1000 | Loss: 0.00001379
Iteration 162/1000 | Loss: 0.00001379
Iteration 163/1000 | Loss: 0.00001379
Iteration 164/1000 | Loss: 0.00001379
Iteration 165/1000 | Loss: 0.00001379
Iteration 166/1000 | Loss: 0.00001379
Iteration 167/1000 | Loss: 0.00001379
Iteration 168/1000 | Loss: 0.00001378
Iteration 169/1000 | Loss: 0.00001378
Iteration 170/1000 | Loss: 0.00001378
Iteration 171/1000 | Loss: 0.00001378
Iteration 172/1000 | Loss: 0.00001378
Iteration 173/1000 | Loss: 0.00001378
Iteration 174/1000 | Loss: 0.00001378
Iteration 175/1000 | Loss: 0.00001378
Iteration 176/1000 | Loss: 0.00001378
Iteration 177/1000 | Loss: 0.00001378
Iteration 178/1000 | Loss: 0.00001378
Iteration 179/1000 | Loss: 0.00001378
Iteration 180/1000 | Loss: 0.00001378
Iteration 181/1000 | Loss: 0.00001378
Iteration 182/1000 | Loss: 0.00001378
Iteration 183/1000 | Loss: 0.00001377
Iteration 184/1000 | Loss: 0.00001377
Iteration 185/1000 | Loss: 0.00001377
Iteration 186/1000 | Loss: 0.00001377
Iteration 187/1000 | Loss: 0.00001377
Iteration 188/1000 | Loss: 0.00001377
Iteration 189/1000 | Loss: 0.00001377
Iteration 190/1000 | Loss: 0.00001377
Iteration 191/1000 | Loss: 0.00001377
Iteration 192/1000 | Loss: 0.00001377
Iteration 193/1000 | Loss: 0.00001377
Iteration 194/1000 | Loss: 0.00001377
Iteration 195/1000 | Loss: 0.00001377
Iteration 196/1000 | Loss: 0.00001377
Iteration 197/1000 | Loss: 0.00001377
Iteration 198/1000 | Loss: 0.00001376
Iteration 199/1000 | Loss: 0.00001376
Iteration 200/1000 | Loss: 0.00001376
Iteration 201/1000 | Loss: 0.00001376
Iteration 202/1000 | Loss: 0.00001376
Iteration 203/1000 | Loss: 0.00001376
Iteration 204/1000 | Loss: 0.00001376
Iteration 205/1000 | Loss: 0.00001376
Iteration 206/1000 | Loss: 0.00001375
Iteration 207/1000 | Loss: 0.00001375
Iteration 208/1000 | Loss: 0.00001375
Iteration 209/1000 | Loss: 0.00001375
Iteration 210/1000 | Loss: 0.00001375
Iteration 211/1000 | Loss: 0.00001375
Iteration 212/1000 | Loss: 0.00001375
Iteration 213/1000 | Loss: 0.00001375
Iteration 214/1000 | Loss: 0.00001375
Iteration 215/1000 | Loss: 0.00001375
Iteration 216/1000 | Loss: 0.00001375
Iteration 217/1000 | Loss: 0.00001375
Iteration 218/1000 | Loss: 0.00001375
Iteration 219/1000 | Loss: 0.00001375
Iteration 220/1000 | Loss: 0.00001375
Iteration 221/1000 | Loss: 0.00001375
Iteration 222/1000 | Loss: 0.00001375
Iteration 223/1000 | Loss: 0.00001375
Iteration 224/1000 | Loss: 0.00001375
Iteration 225/1000 | Loss: 0.00001375
Iteration 226/1000 | Loss: 0.00001375
Iteration 227/1000 | Loss: 0.00001375
Iteration 228/1000 | Loss: 0.00001375
Iteration 229/1000 | Loss: 0.00001375
Iteration 230/1000 | Loss: 0.00001375
Iteration 231/1000 | Loss: 0.00001375
Iteration 232/1000 | Loss: 0.00001375
Iteration 233/1000 | Loss: 0.00001375
Iteration 234/1000 | Loss: 0.00001375
Iteration 235/1000 | Loss: 0.00001375
Iteration 236/1000 | Loss: 0.00001375
Iteration 237/1000 | Loss: 0.00001375
Iteration 238/1000 | Loss: 0.00001375
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 238. Stopping optimization.
Last 5 losses: [1.3745096111961175e-05, 1.3745096111961175e-05, 1.3745096111961175e-05, 1.3745096111961175e-05, 1.3745096111961175e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3745096111961175e-05

Optimization complete. Final v2v error: 3.050130605697632 mm

Highest mean error: 4.494270324707031 mm for frame 56

Lowest mean error: 2.6792497634887695 mm for frame 22

Saving results

Total time: 41.26873230934143
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00444547
Iteration 2/25 | Loss: 0.00131819
Iteration 3/25 | Loss: 0.00121985
Iteration 4/25 | Loss: 0.00120360
Iteration 5/25 | Loss: 0.00119882
Iteration 6/25 | Loss: 0.00119790
Iteration 7/25 | Loss: 0.00119773
Iteration 8/25 | Loss: 0.00119773
Iteration 9/25 | Loss: 0.00119773
Iteration 10/25 | Loss: 0.00119773
Iteration 11/25 | Loss: 0.00119773
Iteration 12/25 | Loss: 0.00119773
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011977340327575803, 0.0011977340327575803, 0.0011977340327575803, 0.0011977340327575803, 0.0011977340327575803]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011977340327575803

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29269171
Iteration 2/25 | Loss: 0.00130198
Iteration 3/25 | Loss: 0.00130198
Iteration 4/25 | Loss: 0.00130198
Iteration 5/25 | Loss: 0.00130198
Iteration 6/25 | Loss: 0.00130198
Iteration 7/25 | Loss: 0.00130198
Iteration 8/25 | Loss: 0.00130198
Iteration 9/25 | Loss: 0.00130198
Iteration 10/25 | Loss: 0.00130198
Iteration 11/25 | Loss: 0.00130198
Iteration 12/25 | Loss: 0.00130198
Iteration 13/25 | Loss: 0.00130198
Iteration 14/25 | Loss: 0.00130198
Iteration 15/25 | Loss: 0.00130198
Iteration 16/25 | Loss: 0.00130198
Iteration 17/25 | Loss: 0.00130198
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001301976153627038, 0.001301976153627038, 0.001301976153627038, 0.001301976153627038, 0.001301976153627038]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001301976153627038

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00130198
Iteration 2/1000 | Loss: 0.00003181
Iteration 3/1000 | Loss: 0.00002433
Iteration 4/1000 | Loss: 0.00002178
Iteration 5/1000 | Loss: 0.00002058
Iteration 6/1000 | Loss: 0.00001977
Iteration 7/1000 | Loss: 0.00001916
Iteration 8/1000 | Loss: 0.00001868
Iteration 9/1000 | Loss: 0.00001837
Iteration 10/1000 | Loss: 0.00001808
Iteration 11/1000 | Loss: 0.00001774
Iteration 12/1000 | Loss: 0.00001755
Iteration 13/1000 | Loss: 0.00001747
Iteration 14/1000 | Loss: 0.00001743
Iteration 15/1000 | Loss: 0.00001735
Iteration 16/1000 | Loss: 0.00001732
Iteration 17/1000 | Loss: 0.00001731
Iteration 18/1000 | Loss: 0.00001730
Iteration 19/1000 | Loss: 0.00001730
Iteration 20/1000 | Loss: 0.00001729
Iteration 21/1000 | Loss: 0.00001722
Iteration 22/1000 | Loss: 0.00001718
Iteration 23/1000 | Loss: 0.00001707
Iteration 24/1000 | Loss: 0.00001706
Iteration 25/1000 | Loss: 0.00001704
Iteration 26/1000 | Loss: 0.00001702
Iteration 27/1000 | Loss: 0.00001697
Iteration 28/1000 | Loss: 0.00001697
Iteration 29/1000 | Loss: 0.00001695
Iteration 30/1000 | Loss: 0.00001694
Iteration 31/1000 | Loss: 0.00001694
Iteration 32/1000 | Loss: 0.00001693
Iteration 33/1000 | Loss: 0.00001693
Iteration 34/1000 | Loss: 0.00001693
Iteration 35/1000 | Loss: 0.00001692
Iteration 36/1000 | Loss: 0.00001690
Iteration 37/1000 | Loss: 0.00001689
Iteration 38/1000 | Loss: 0.00001689
Iteration 39/1000 | Loss: 0.00001688
Iteration 40/1000 | Loss: 0.00001688
Iteration 41/1000 | Loss: 0.00001688
Iteration 42/1000 | Loss: 0.00001687
Iteration 43/1000 | Loss: 0.00001687
Iteration 44/1000 | Loss: 0.00001687
Iteration 45/1000 | Loss: 0.00001686
Iteration 46/1000 | Loss: 0.00001686
Iteration 47/1000 | Loss: 0.00001686
Iteration 48/1000 | Loss: 0.00001686
Iteration 49/1000 | Loss: 0.00001685
Iteration 50/1000 | Loss: 0.00001685
Iteration 51/1000 | Loss: 0.00001685
Iteration 52/1000 | Loss: 0.00001685
Iteration 53/1000 | Loss: 0.00001685
Iteration 54/1000 | Loss: 0.00001685
Iteration 55/1000 | Loss: 0.00001685
Iteration 56/1000 | Loss: 0.00001685
Iteration 57/1000 | Loss: 0.00001684
Iteration 58/1000 | Loss: 0.00001684
Iteration 59/1000 | Loss: 0.00001684
Iteration 60/1000 | Loss: 0.00001684
Iteration 61/1000 | Loss: 0.00001684
Iteration 62/1000 | Loss: 0.00001684
Iteration 63/1000 | Loss: 0.00001683
Iteration 64/1000 | Loss: 0.00001683
Iteration 65/1000 | Loss: 0.00001683
Iteration 66/1000 | Loss: 0.00001683
Iteration 67/1000 | Loss: 0.00001682
Iteration 68/1000 | Loss: 0.00001682
Iteration 69/1000 | Loss: 0.00001682
Iteration 70/1000 | Loss: 0.00001681
Iteration 71/1000 | Loss: 0.00001681
Iteration 72/1000 | Loss: 0.00001681
Iteration 73/1000 | Loss: 0.00001681
Iteration 74/1000 | Loss: 0.00001681
Iteration 75/1000 | Loss: 0.00001680
Iteration 76/1000 | Loss: 0.00001680
Iteration 77/1000 | Loss: 0.00001680
Iteration 78/1000 | Loss: 0.00001680
Iteration 79/1000 | Loss: 0.00001680
Iteration 80/1000 | Loss: 0.00001680
Iteration 81/1000 | Loss: 0.00001679
Iteration 82/1000 | Loss: 0.00001679
Iteration 83/1000 | Loss: 0.00001679
Iteration 84/1000 | Loss: 0.00001679
Iteration 85/1000 | Loss: 0.00001679
Iteration 86/1000 | Loss: 0.00001679
Iteration 87/1000 | Loss: 0.00001679
Iteration 88/1000 | Loss: 0.00001678
Iteration 89/1000 | Loss: 0.00001678
Iteration 90/1000 | Loss: 0.00001678
Iteration 91/1000 | Loss: 0.00001678
Iteration 92/1000 | Loss: 0.00001678
Iteration 93/1000 | Loss: 0.00001678
Iteration 94/1000 | Loss: 0.00001678
Iteration 95/1000 | Loss: 0.00001678
Iteration 96/1000 | Loss: 0.00001677
Iteration 97/1000 | Loss: 0.00001677
Iteration 98/1000 | Loss: 0.00001677
Iteration 99/1000 | Loss: 0.00001677
Iteration 100/1000 | Loss: 0.00001677
Iteration 101/1000 | Loss: 0.00001677
Iteration 102/1000 | Loss: 0.00001677
Iteration 103/1000 | Loss: 0.00001677
Iteration 104/1000 | Loss: 0.00001677
Iteration 105/1000 | Loss: 0.00001677
Iteration 106/1000 | Loss: 0.00001677
Iteration 107/1000 | Loss: 0.00001677
Iteration 108/1000 | Loss: 0.00001676
Iteration 109/1000 | Loss: 0.00001676
Iteration 110/1000 | Loss: 0.00001676
Iteration 111/1000 | Loss: 0.00001676
Iteration 112/1000 | Loss: 0.00001676
Iteration 113/1000 | Loss: 0.00001676
Iteration 114/1000 | Loss: 0.00001676
Iteration 115/1000 | Loss: 0.00001676
Iteration 116/1000 | Loss: 0.00001676
Iteration 117/1000 | Loss: 0.00001676
Iteration 118/1000 | Loss: 0.00001676
Iteration 119/1000 | Loss: 0.00001676
Iteration 120/1000 | Loss: 0.00001675
Iteration 121/1000 | Loss: 0.00001675
Iteration 122/1000 | Loss: 0.00001675
Iteration 123/1000 | Loss: 0.00001675
Iteration 124/1000 | Loss: 0.00001675
Iteration 125/1000 | Loss: 0.00001675
Iteration 126/1000 | Loss: 0.00001675
Iteration 127/1000 | Loss: 0.00001675
Iteration 128/1000 | Loss: 0.00001675
Iteration 129/1000 | Loss: 0.00001675
Iteration 130/1000 | Loss: 0.00001675
Iteration 131/1000 | Loss: 0.00001675
Iteration 132/1000 | Loss: 0.00001675
Iteration 133/1000 | Loss: 0.00001675
Iteration 134/1000 | Loss: 0.00001675
Iteration 135/1000 | Loss: 0.00001675
Iteration 136/1000 | Loss: 0.00001675
Iteration 137/1000 | Loss: 0.00001674
Iteration 138/1000 | Loss: 0.00001674
Iteration 139/1000 | Loss: 0.00001674
Iteration 140/1000 | Loss: 0.00001674
Iteration 141/1000 | Loss: 0.00001674
Iteration 142/1000 | Loss: 0.00001674
Iteration 143/1000 | Loss: 0.00001674
Iteration 144/1000 | Loss: 0.00001674
Iteration 145/1000 | Loss: 0.00001674
Iteration 146/1000 | Loss: 0.00001674
Iteration 147/1000 | Loss: 0.00001674
Iteration 148/1000 | Loss: 0.00001674
Iteration 149/1000 | Loss: 0.00001674
Iteration 150/1000 | Loss: 0.00001674
Iteration 151/1000 | Loss: 0.00001674
Iteration 152/1000 | Loss: 0.00001674
Iteration 153/1000 | Loss: 0.00001674
Iteration 154/1000 | Loss: 0.00001674
Iteration 155/1000 | Loss: 0.00001674
Iteration 156/1000 | Loss: 0.00001673
Iteration 157/1000 | Loss: 0.00001673
Iteration 158/1000 | Loss: 0.00001673
Iteration 159/1000 | Loss: 0.00001673
Iteration 160/1000 | Loss: 0.00001673
Iteration 161/1000 | Loss: 0.00001673
Iteration 162/1000 | Loss: 0.00001673
Iteration 163/1000 | Loss: 0.00001673
Iteration 164/1000 | Loss: 0.00001673
Iteration 165/1000 | Loss: 0.00001673
Iteration 166/1000 | Loss: 0.00001673
Iteration 167/1000 | Loss: 0.00001673
Iteration 168/1000 | Loss: 0.00001673
Iteration 169/1000 | Loss: 0.00001673
Iteration 170/1000 | Loss: 0.00001673
Iteration 171/1000 | Loss: 0.00001673
Iteration 172/1000 | Loss: 0.00001673
Iteration 173/1000 | Loss: 0.00001673
Iteration 174/1000 | Loss: 0.00001673
Iteration 175/1000 | Loss: 0.00001673
Iteration 176/1000 | Loss: 0.00001673
Iteration 177/1000 | Loss: 0.00001673
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [1.673334372753743e-05, 1.673334372753743e-05, 1.673334372753743e-05, 1.673334372753743e-05, 1.673334372753743e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.673334372753743e-05

Optimization complete. Final v2v error: 3.496680974960327 mm

Highest mean error: 4.218414783477783 mm for frame 29

Lowest mean error: 3.337825059890747 mm for frame 3

Saving results

Total time: 42.01685571670532
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00551783
Iteration 2/25 | Loss: 0.00136528
Iteration 3/25 | Loss: 0.00123247
Iteration 4/25 | Loss: 0.00122180
Iteration 5/25 | Loss: 0.00122019
Iteration 6/25 | Loss: 0.00122019
Iteration 7/25 | Loss: 0.00122019
Iteration 8/25 | Loss: 0.00122019
Iteration 9/25 | Loss: 0.00122019
Iteration 10/25 | Loss: 0.00122019
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012201850768178701, 0.0012201850768178701, 0.0012201850768178701, 0.0012201850768178701, 0.0012201850768178701]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012201850768178701

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25342989
Iteration 2/25 | Loss: 0.00118726
Iteration 3/25 | Loss: 0.00118721
Iteration 4/25 | Loss: 0.00118721
Iteration 5/25 | Loss: 0.00118721
Iteration 6/25 | Loss: 0.00118721
Iteration 7/25 | Loss: 0.00118721
Iteration 8/25 | Loss: 0.00118721
Iteration 9/25 | Loss: 0.00118721
Iteration 10/25 | Loss: 0.00118721
Iteration 11/25 | Loss: 0.00118721
Iteration 12/25 | Loss: 0.00118721
Iteration 13/25 | Loss: 0.00118721
Iteration 14/25 | Loss: 0.00118721
Iteration 15/25 | Loss: 0.00118721
Iteration 16/25 | Loss: 0.00118721
Iteration 17/25 | Loss: 0.00118721
Iteration 18/25 | Loss: 0.00118721
Iteration 19/25 | Loss: 0.00118721
Iteration 20/25 | Loss: 0.00118721
Iteration 21/25 | Loss: 0.00118721
Iteration 22/25 | Loss: 0.00118721
Iteration 23/25 | Loss: 0.00118721
Iteration 24/25 | Loss: 0.00118721
Iteration 25/25 | Loss: 0.00118721
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0011872093891724944, 0.0011872093891724944, 0.0011872093891724944, 0.0011872093891724944, 0.0011872093891724944]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011872093891724944

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118721
Iteration 2/1000 | Loss: 0.00003406
Iteration 3/1000 | Loss: 0.00001924
Iteration 4/1000 | Loss: 0.00001759
Iteration 5/1000 | Loss: 0.00001617
Iteration 6/1000 | Loss: 0.00001547
Iteration 7/1000 | Loss: 0.00001510
Iteration 8/1000 | Loss: 0.00001479
Iteration 9/1000 | Loss: 0.00001447
Iteration 10/1000 | Loss: 0.00001410
Iteration 11/1000 | Loss: 0.00001387
Iteration 12/1000 | Loss: 0.00001379
Iteration 13/1000 | Loss: 0.00001364
Iteration 14/1000 | Loss: 0.00001361
Iteration 15/1000 | Loss: 0.00001348
Iteration 16/1000 | Loss: 0.00001338
Iteration 17/1000 | Loss: 0.00001321
Iteration 18/1000 | Loss: 0.00001309
Iteration 19/1000 | Loss: 0.00001307
Iteration 20/1000 | Loss: 0.00001305
Iteration 21/1000 | Loss: 0.00001301
Iteration 22/1000 | Loss: 0.00001294
Iteration 23/1000 | Loss: 0.00001294
Iteration 24/1000 | Loss: 0.00001293
Iteration 25/1000 | Loss: 0.00001291
Iteration 26/1000 | Loss: 0.00001291
Iteration 27/1000 | Loss: 0.00001290
Iteration 28/1000 | Loss: 0.00001290
Iteration 29/1000 | Loss: 0.00001290
Iteration 30/1000 | Loss: 0.00001289
Iteration 31/1000 | Loss: 0.00001289
Iteration 32/1000 | Loss: 0.00001288
Iteration 33/1000 | Loss: 0.00001288
Iteration 34/1000 | Loss: 0.00001288
Iteration 35/1000 | Loss: 0.00001288
Iteration 36/1000 | Loss: 0.00001287
Iteration 37/1000 | Loss: 0.00001287
Iteration 38/1000 | Loss: 0.00001287
Iteration 39/1000 | Loss: 0.00001287
Iteration 40/1000 | Loss: 0.00001286
Iteration 41/1000 | Loss: 0.00001286
Iteration 42/1000 | Loss: 0.00001286
Iteration 43/1000 | Loss: 0.00001286
Iteration 44/1000 | Loss: 0.00001286
Iteration 45/1000 | Loss: 0.00001286
Iteration 46/1000 | Loss: 0.00001285
Iteration 47/1000 | Loss: 0.00001285
Iteration 48/1000 | Loss: 0.00001285
Iteration 49/1000 | Loss: 0.00001284
Iteration 50/1000 | Loss: 0.00001283
Iteration 51/1000 | Loss: 0.00001283
Iteration 52/1000 | Loss: 0.00001283
Iteration 53/1000 | Loss: 0.00001283
Iteration 54/1000 | Loss: 0.00001282
Iteration 55/1000 | Loss: 0.00001282
Iteration 56/1000 | Loss: 0.00001282
Iteration 57/1000 | Loss: 0.00001282
Iteration 58/1000 | Loss: 0.00001282
Iteration 59/1000 | Loss: 0.00001282
Iteration 60/1000 | Loss: 0.00001282
Iteration 61/1000 | Loss: 0.00001281
Iteration 62/1000 | Loss: 0.00001281
Iteration 63/1000 | Loss: 0.00001281
Iteration 64/1000 | Loss: 0.00001281
Iteration 65/1000 | Loss: 0.00001281
Iteration 66/1000 | Loss: 0.00001281
Iteration 67/1000 | Loss: 0.00001281
Iteration 68/1000 | Loss: 0.00001281
Iteration 69/1000 | Loss: 0.00001281
Iteration 70/1000 | Loss: 0.00001281
Iteration 71/1000 | Loss: 0.00001281
Iteration 72/1000 | Loss: 0.00001280
Iteration 73/1000 | Loss: 0.00001280
Iteration 74/1000 | Loss: 0.00001280
Iteration 75/1000 | Loss: 0.00001280
Iteration 76/1000 | Loss: 0.00001280
Iteration 77/1000 | Loss: 0.00001280
Iteration 78/1000 | Loss: 0.00001280
Iteration 79/1000 | Loss: 0.00001280
Iteration 80/1000 | Loss: 0.00001280
Iteration 81/1000 | Loss: 0.00001280
Iteration 82/1000 | Loss: 0.00001279
Iteration 83/1000 | Loss: 0.00001279
Iteration 84/1000 | Loss: 0.00001279
Iteration 85/1000 | Loss: 0.00001279
Iteration 86/1000 | Loss: 0.00001278
Iteration 87/1000 | Loss: 0.00001278
Iteration 88/1000 | Loss: 0.00001278
Iteration 89/1000 | Loss: 0.00001277
Iteration 90/1000 | Loss: 0.00001277
Iteration 91/1000 | Loss: 0.00001277
Iteration 92/1000 | Loss: 0.00001277
Iteration 93/1000 | Loss: 0.00001277
Iteration 94/1000 | Loss: 0.00001277
Iteration 95/1000 | Loss: 0.00001277
Iteration 96/1000 | Loss: 0.00001277
Iteration 97/1000 | Loss: 0.00001276
Iteration 98/1000 | Loss: 0.00001276
Iteration 99/1000 | Loss: 0.00001276
Iteration 100/1000 | Loss: 0.00001276
Iteration 101/1000 | Loss: 0.00001276
Iteration 102/1000 | Loss: 0.00001275
Iteration 103/1000 | Loss: 0.00001275
Iteration 104/1000 | Loss: 0.00001275
Iteration 105/1000 | Loss: 0.00001275
Iteration 106/1000 | Loss: 0.00001275
Iteration 107/1000 | Loss: 0.00001275
Iteration 108/1000 | Loss: 0.00001275
Iteration 109/1000 | Loss: 0.00001274
Iteration 110/1000 | Loss: 0.00001274
Iteration 111/1000 | Loss: 0.00001274
Iteration 112/1000 | Loss: 0.00001274
Iteration 113/1000 | Loss: 0.00001274
Iteration 114/1000 | Loss: 0.00001274
Iteration 115/1000 | Loss: 0.00001274
Iteration 116/1000 | Loss: 0.00001274
Iteration 117/1000 | Loss: 0.00001274
Iteration 118/1000 | Loss: 0.00001274
Iteration 119/1000 | Loss: 0.00001274
Iteration 120/1000 | Loss: 0.00001273
Iteration 121/1000 | Loss: 0.00001273
Iteration 122/1000 | Loss: 0.00001273
Iteration 123/1000 | Loss: 0.00001273
Iteration 124/1000 | Loss: 0.00001273
Iteration 125/1000 | Loss: 0.00001272
Iteration 126/1000 | Loss: 0.00001272
Iteration 127/1000 | Loss: 0.00001272
Iteration 128/1000 | Loss: 0.00001272
Iteration 129/1000 | Loss: 0.00001272
Iteration 130/1000 | Loss: 0.00001272
Iteration 131/1000 | Loss: 0.00001272
Iteration 132/1000 | Loss: 0.00001272
Iteration 133/1000 | Loss: 0.00001272
Iteration 134/1000 | Loss: 0.00001271
Iteration 135/1000 | Loss: 0.00001271
Iteration 136/1000 | Loss: 0.00001271
Iteration 137/1000 | Loss: 0.00001271
Iteration 138/1000 | Loss: 0.00001271
Iteration 139/1000 | Loss: 0.00001271
Iteration 140/1000 | Loss: 0.00001271
Iteration 141/1000 | Loss: 0.00001271
Iteration 142/1000 | Loss: 0.00001271
Iteration 143/1000 | Loss: 0.00001271
Iteration 144/1000 | Loss: 0.00001271
Iteration 145/1000 | Loss: 0.00001271
Iteration 146/1000 | Loss: 0.00001270
Iteration 147/1000 | Loss: 0.00001270
Iteration 148/1000 | Loss: 0.00001270
Iteration 149/1000 | Loss: 0.00001270
Iteration 150/1000 | Loss: 0.00001270
Iteration 151/1000 | Loss: 0.00001270
Iteration 152/1000 | Loss: 0.00001270
Iteration 153/1000 | Loss: 0.00001270
Iteration 154/1000 | Loss: 0.00001270
Iteration 155/1000 | Loss: 0.00001270
Iteration 156/1000 | Loss: 0.00001270
Iteration 157/1000 | Loss: 0.00001270
Iteration 158/1000 | Loss: 0.00001270
Iteration 159/1000 | Loss: 0.00001270
Iteration 160/1000 | Loss: 0.00001269
Iteration 161/1000 | Loss: 0.00001269
Iteration 162/1000 | Loss: 0.00001269
Iteration 163/1000 | Loss: 0.00001269
Iteration 164/1000 | Loss: 0.00001269
Iteration 165/1000 | Loss: 0.00001269
Iteration 166/1000 | Loss: 0.00001269
Iteration 167/1000 | Loss: 0.00001269
Iteration 168/1000 | Loss: 0.00001269
Iteration 169/1000 | Loss: 0.00001269
Iteration 170/1000 | Loss: 0.00001269
Iteration 171/1000 | Loss: 0.00001269
Iteration 172/1000 | Loss: 0.00001269
Iteration 173/1000 | Loss: 0.00001269
Iteration 174/1000 | Loss: 0.00001269
Iteration 175/1000 | Loss: 0.00001269
Iteration 176/1000 | Loss: 0.00001269
Iteration 177/1000 | Loss: 0.00001269
Iteration 178/1000 | Loss: 0.00001269
Iteration 179/1000 | Loss: 0.00001269
Iteration 180/1000 | Loss: 0.00001269
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.269069889531238e-05, 1.269069889531238e-05, 1.269069889531238e-05, 1.269069889531238e-05, 1.269069889531238e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.269069889531238e-05

Optimization complete. Final v2v error: 3.075920343399048 mm

Highest mean error: 3.251904249191284 mm for frame 162

Lowest mean error: 2.901128053665161 mm for frame 182

Saving results

Total time: 46.68295645713806
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00695032
Iteration 2/25 | Loss: 0.00153667
Iteration 3/25 | Loss: 0.00127158
Iteration 4/25 | Loss: 0.00124613
Iteration 5/25 | Loss: 0.00123109
Iteration 6/25 | Loss: 0.00124015
Iteration 7/25 | Loss: 0.00124282
Iteration 8/25 | Loss: 0.00122645
Iteration 9/25 | Loss: 0.00121808
Iteration 10/25 | Loss: 0.00121112
Iteration 11/25 | Loss: 0.00120670
Iteration 12/25 | Loss: 0.00120455
Iteration 13/25 | Loss: 0.00120249
Iteration 14/25 | Loss: 0.00120174
Iteration 15/25 | Loss: 0.00120142
Iteration 16/25 | Loss: 0.00120118
Iteration 17/25 | Loss: 0.00120086
Iteration 18/25 | Loss: 0.00120065
Iteration 19/25 | Loss: 0.00120056
Iteration 20/25 | Loss: 0.00120084
Iteration 21/25 | Loss: 0.00119994
Iteration 22/25 | Loss: 0.00119958
Iteration 23/25 | Loss: 0.00119940
Iteration 24/25 | Loss: 0.00119939
Iteration 25/25 | Loss: 0.00119939

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.96448088
Iteration 2/25 | Loss: 0.00129560
Iteration 3/25 | Loss: 0.00129558
Iteration 4/25 | Loss: 0.00129558
Iteration 5/25 | Loss: 0.00129558
Iteration 6/25 | Loss: 0.00129558
Iteration 7/25 | Loss: 0.00129558
Iteration 8/25 | Loss: 0.00129558
Iteration 9/25 | Loss: 0.00129558
Iteration 10/25 | Loss: 0.00129558
Iteration 11/25 | Loss: 0.00129558
Iteration 12/25 | Loss: 0.00129558
Iteration 13/25 | Loss: 0.00129558
Iteration 14/25 | Loss: 0.00129558
Iteration 15/25 | Loss: 0.00129558
Iteration 16/25 | Loss: 0.00129558
Iteration 17/25 | Loss: 0.00129558
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012955766869708896, 0.0012955766869708896, 0.0012955766869708896, 0.0012955766869708896, 0.0012955766869708896]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012955766869708896

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129558
Iteration 2/1000 | Loss: 0.00003867
Iteration 3/1000 | Loss: 0.00002474
Iteration 4/1000 | Loss: 0.00002134
Iteration 5/1000 | Loss: 0.00001457
Iteration 6/1000 | Loss: 0.00001402
Iteration 7/1000 | Loss: 0.00001367
Iteration 8/1000 | Loss: 0.00001349
Iteration 9/1000 | Loss: 0.00001322
Iteration 10/1000 | Loss: 0.00001301
Iteration 11/1000 | Loss: 0.00001291
Iteration 12/1000 | Loss: 0.00001283
Iteration 13/1000 | Loss: 0.00001274
Iteration 14/1000 | Loss: 0.00001273
Iteration 15/1000 | Loss: 0.00001268
Iteration 16/1000 | Loss: 0.00001268
Iteration 17/1000 | Loss: 0.00001267
Iteration 18/1000 | Loss: 0.00001266
Iteration 19/1000 | Loss: 0.00001265
Iteration 20/1000 | Loss: 0.00001264
Iteration 21/1000 | Loss: 0.00001264
Iteration 22/1000 | Loss: 0.00001264
Iteration 23/1000 | Loss: 0.00001263
Iteration 24/1000 | Loss: 0.00001263
Iteration 25/1000 | Loss: 0.00001262
Iteration 26/1000 | Loss: 0.00001262
Iteration 27/1000 | Loss: 0.00001262
Iteration 28/1000 | Loss: 0.00001261
Iteration 29/1000 | Loss: 0.00001260
Iteration 30/1000 | Loss: 0.00001259
Iteration 31/1000 | Loss: 0.00001258
Iteration 32/1000 | Loss: 0.00001258
Iteration 33/1000 | Loss: 0.00001257
Iteration 34/1000 | Loss: 0.00001257
Iteration 35/1000 | Loss: 0.00001256
Iteration 36/1000 | Loss: 0.00001256
Iteration 37/1000 | Loss: 0.00001256
Iteration 38/1000 | Loss: 0.00001254
Iteration 39/1000 | Loss: 0.00001254
Iteration 40/1000 | Loss: 0.00001253
Iteration 41/1000 | Loss: 0.00001253
Iteration 42/1000 | Loss: 0.00001252
Iteration 43/1000 | Loss: 0.00001250
Iteration 44/1000 | Loss: 0.00001250
Iteration 45/1000 | Loss: 0.00001249
Iteration 46/1000 | Loss: 0.00001249
Iteration 47/1000 | Loss: 0.00001248
Iteration 48/1000 | Loss: 0.00001248
Iteration 49/1000 | Loss: 0.00001248
Iteration 50/1000 | Loss: 0.00001248
Iteration 51/1000 | Loss: 0.00001247
Iteration 52/1000 | Loss: 0.00001247
Iteration 53/1000 | Loss: 0.00001247
Iteration 54/1000 | Loss: 0.00001247
Iteration 55/1000 | Loss: 0.00001246
Iteration 56/1000 | Loss: 0.00001245
Iteration 57/1000 | Loss: 0.00001244
Iteration 58/1000 | Loss: 0.00001244
Iteration 59/1000 | Loss: 0.00001244
Iteration 60/1000 | Loss: 0.00001244
Iteration 61/1000 | Loss: 0.00001243
Iteration 62/1000 | Loss: 0.00001243
Iteration 63/1000 | Loss: 0.00001243
Iteration 64/1000 | Loss: 0.00001242
Iteration 65/1000 | Loss: 0.00001241
Iteration 66/1000 | Loss: 0.00001241
Iteration 67/1000 | Loss: 0.00001241
Iteration 68/1000 | Loss: 0.00001240
Iteration 69/1000 | Loss: 0.00001239
Iteration 70/1000 | Loss: 0.00001239
Iteration 71/1000 | Loss: 0.00001238
Iteration 72/1000 | Loss: 0.00001238
Iteration 73/1000 | Loss: 0.00001238
Iteration 74/1000 | Loss: 0.00001237
Iteration 75/1000 | Loss: 0.00001237
Iteration 76/1000 | Loss: 0.00001236
Iteration 77/1000 | Loss: 0.00001236
Iteration 78/1000 | Loss: 0.00001236
Iteration 79/1000 | Loss: 0.00001236
Iteration 80/1000 | Loss: 0.00001236
Iteration 81/1000 | Loss: 0.00001235
Iteration 82/1000 | Loss: 0.00001235
Iteration 83/1000 | Loss: 0.00001235
Iteration 84/1000 | Loss: 0.00001235
Iteration 85/1000 | Loss: 0.00001235
Iteration 86/1000 | Loss: 0.00001235
Iteration 87/1000 | Loss: 0.00001234
Iteration 88/1000 | Loss: 0.00001234
Iteration 89/1000 | Loss: 0.00001234
Iteration 90/1000 | Loss: 0.00001234
Iteration 91/1000 | Loss: 0.00001234
Iteration 92/1000 | Loss: 0.00001233
Iteration 93/1000 | Loss: 0.00001233
Iteration 94/1000 | Loss: 0.00001233
Iteration 95/1000 | Loss: 0.00001233
Iteration 96/1000 | Loss: 0.00001233
Iteration 97/1000 | Loss: 0.00001233
Iteration 98/1000 | Loss: 0.00001233
Iteration 99/1000 | Loss: 0.00001232
Iteration 100/1000 | Loss: 0.00001232
Iteration 101/1000 | Loss: 0.00001232
Iteration 102/1000 | Loss: 0.00001232
Iteration 103/1000 | Loss: 0.00001231
Iteration 104/1000 | Loss: 0.00001231
Iteration 105/1000 | Loss: 0.00001231
Iteration 106/1000 | Loss: 0.00001230
Iteration 107/1000 | Loss: 0.00001230
Iteration 108/1000 | Loss: 0.00001230
Iteration 109/1000 | Loss: 0.00001229
Iteration 110/1000 | Loss: 0.00001229
Iteration 111/1000 | Loss: 0.00001229
Iteration 112/1000 | Loss: 0.00001229
Iteration 113/1000 | Loss: 0.00001229
Iteration 114/1000 | Loss: 0.00001228
Iteration 115/1000 | Loss: 0.00001228
Iteration 116/1000 | Loss: 0.00001228
Iteration 117/1000 | Loss: 0.00001228
Iteration 118/1000 | Loss: 0.00001227
Iteration 119/1000 | Loss: 0.00001227
Iteration 120/1000 | Loss: 0.00001227
Iteration 121/1000 | Loss: 0.00001226
Iteration 122/1000 | Loss: 0.00001226
Iteration 123/1000 | Loss: 0.00001226
Iteration 124/1000 | Loss: 0.00001226
Iteration 125/1000 | Loss: 0.00001226
Iteration 126/1000 | Loss: 0.00001226
Iteration 127/1000 | Loss: 0.00001226
Iteration 128/1000 | Loss: 0.00001226
Iteration 129/1000 | Loss: 0.00001225
Iteration 130/1000 | Loss: 0.00001225
Iteration 131/1000 | Loss: 0.00001225
Iteration 132/1000 | Loss: 0.00001225
Iteration 133/1000 | Loss: 0.00001224
Iteration 134/1000 | Loss: 0.00001224
Iteration 135/1000 | Loss: 0.00001224
Iteration 136/1000 | Loss: 0.00001224
Iteration 137/1000 | Loss: 0.00001224
Iteration 138/1000 | Loss: 0.00001224
Iteration 139/1000 | Loss: 0.00001223
Iteration 140/1000 | Loss: 0.00001223
Iteration 141/1000 | Loss: 0.00001223
Iteration 142/1000 | Loss: 0.00001223
Iteration 143/1000 | Loss: 0.00001223
Iteration 144/1000 | Loss: 0.00001223
Iteration 145/1000 | Loss: 0.00001223
Iteration 146/1000 | Loss: 0.00001223
Iteration 147/1000 | Loss: 0.00001222
Iteration 148/1000 | Loss: 0.00001221
Iteration 149/1000 | Loss: 0.00001221
Iteration 150/1000 | Loss: 0.00001221
Iteration 151/1000 | Loss: 0.00001220
Iteration 152/1000 | Loss: 0.00001220
Iteration 153/1000 | Loss: 0.00001220
Iteration 154/1000 | Loss: 0.00001220
Iteration 155/1000 | Loss: 0.00001220
Iteration 156/1000 | Loss: 0.00001220
Iteration 157/1000 | Loss: 0.00001219
Iteration 158/1000 | Loss: 0.00001219
Iteration 159/1000 | Loss: 0.00001219
Iteration 160/1000 | Loss: 0.00001218
Iteration 161/1000 | Loss: 0.00001218
Iteration 162/1000 | Loss: 0.00001218
Iteration 163/1000 | Loss: 0.00001218
Iteration 164/1000 | Loss: 0.00001217
Iteration 165/1000 | Loss: 0.00001217
Iteration 166/1000 | Loss: 0.00001217
Iteration 167/1000 | Loss: 0.00001217
Iteration 168/1000 | Loss: 0.00001217
Iteration 169/1000 | Loss: 0.00001217
Iteration 170/1000 | Loss: 0.00001217
Iteration 171/1000 | Loss: 0.00001217
Iteration 172/1000 | Loss: 0.00001217
Iteration 173/1000 | Loss: 0.00001217
Iteration 174/1000 | Loss: 0.00001217
Iteration 175/1000 | Loss: 0.00001217
Iteration 176/1000 | Loss: 0.00001217
Iteration 177/1000 | Loss: 0.00001217
Iteration 178/1000 | Loss: 0.00001217
Iteration 179/1000 | Loss: 0.00001217
Iteration 180/1000 | Loss: 0.00001217
Iteration 181/1000 | Loss: 0.00001217
Iteration 182/1000 | Loss: 0.00001217
Iteration 183/1000 | Loss: 0.00001217
Iteration 184/1000 | Loss: 0.00001217
Iteration 185/1000 | Loss: 0.00001217
Iteration 186/1000 | Loss: 0.00001217
Iteration 187/1000 | Loss: 0.00001217
Iteration 188/1000 | Loss: 0.00001217
Iteration 189/1000 | Loss: 0.00001217
Iteration 190/1000 | Loss: 0.00001217
Iteration 191/1000 | Loss: 0.00001217
Iteration 192/1000 | Loss: 0.00001217
Iteration 193/1000 | Loss: 0.00001217
Iteration 194/1000 | Loss: 0.00001217
Iteration 195/1000 | Loss: 0.00001217
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [1.216985492646927e-05, 1.216985492646927e-05, 1.216985492646927e-05, 1.216985492646927e-05, 1.216985492646927e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.216985492646927e-05

Optimization complete. Final v2v error: 2.9495866298675537 mm

Highest mean error: 3.550082206726074 mm for frame 122

Lowest mean error: 2.4544713497161865 mm for frame 219

Saving results

Total time: 80.74398636817932
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00930669
Iteration 2/25 | Loss: 0.00151802
Iteration 3/25 | Loss: 0.00134299
Iteration 4/25 | Loss: 0.00131873
Iteration 5/25 | Loss: 0.00131381
Iteration 6/25 | Loss: 0.00129396
Iteration 7/25 | Loss: 0.00129164
Iteration 8/25 | Loss: 0.00128891
Iteration 9/25 | Loss: 0.00128745
Iteration 10/25 | Loss: 0.00128717
Iteration 11/25 | Loss: 0.00128709
Iteration 12/25 | Loss: 0.00128707
Iteration 13/25 | Loss: 0.00128706
Iteration 14/25 | Loss: 0.00128706
Iteration 15/25 | Loss: 0.00128706
Iteration 16/25 | Loss: 0.00128706
Iteration 17/25 | Loss: 0.00128706
Iteration 18/25 | Loss: 0.00128706
Iteration 19/25 | Loss: 0.00128706
Iteration 20/25 | Loss: 0.00128706
Iteration 21/25 | Loss: 0.00128706
Iteration 22/25 | Loss: 0.00128706
Iteration 23/25 | Loss: 0.00128706
Iteration 24/25 | Loss: 0.00128705
Iteration 25/25 | Loss: 0.00128705

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.03629565
Iteration 2/25 | Loss: 0.00131761
Iteration 3/25 | Loss: 0.00131761
Iteration 4/25 | Loss: 0.00131761
Iteration 5/25 | Loss: 0.00131761
Iteration 6/25 | Loss: 0.00131761
Iteration 7/25 | Loss: 0.00131761
Iteration 8/25 | Loss: 0.00131761
Iteration 9/25 | Loss: 0.00131761
Iteration 10/25 | Loss: 0.00131761
Iteration 11/25 | Loss: 0.00131761
Iteration 12/25 | Loss: 0.00131761
Iteration 13/25 | Loss: 0.00131761
Iteration 14/25 | Loss: 0.00131761
Iteration 15/25 | Loss: 0.00131761
Iteration 16/25 | Loss: 0.00131761
Iteration 17/25 | Loss: 0.00131761
Iteration 18/25 | Loss: 0.00131761
Iteration 19/25 | Loss: 0.00131761
Iteration 20/25 | Loss: 0.00131761
Iteration 21/25 | Loss: 0.00131761
Iteration 22/25 | Loss: 0.00131761
Iteration 23/25 | Loss: 0.00131761
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001317607588134706, 0.001317607588134706, 0.001317607588134706, 0.001317607588134706, 0.001317607588134706]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001317607588134706

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131761
Iteration 2/1000 | Loss: 0.00004612
Iteration 3/1000 | Loss: 0.00003090
Iteration 4/1000 | Loss: 0.00002759
Iteration 5/1000 | Loss: 0.00002579
Iteration 6/1000 | Loss: 0.00002504
Iteration 7/1000 | Loss: 0.00002455
Iteration 8/1000 | Loss: 0.00002424
Iteration 9/1000 | Loss: 0.00002396
Iteration 10/1000 | Loss: 0.00002369
Iteration 11/1000 | Loss: 0.00002352
Iteration 12/1000 | Loss: 0.00002337
Iteration 13/1000 | Loss: 0.00002331
Iteration 14/1000 | Loss: 0.00002319
Iteration 15/1000 | Loss: 0.00002311
Iteration 16/1000 | Loss: 0.00002307
Iteration 17/1000 | Loss: 0.00002307
Iteration 18/1000 | Loss: 0.00002306
Iteration 19/1000 | Loss: 0.00002306
Iteration 20/1000 | Loss: 0.00002306
Iteration 21/1000 | Loss: 0.00002305
Iteration 22/1000 | Loss: 0.00002304
Iteration 23/1000 | Loss: 0.00002304
Iteration 24/1000 | Loss: 0.00002304
Iteration 25/1000 | Loss: 0.00002304
Iteration 26/1000 | Loss: 0.00002304
Iteration 27/1000 | Loss: 0.00002304
Iteration 28/1000 | Loss: 0.00002304
Iteration 29/1000 | Loss: 0.00002304
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 29. Stopping optimization.
Last 5 losses: [2.3036305719870143e-05, 2.3036305719870143e-05, 2.3036305719870143e-05, 2.3036305719870143e-05, 2.3036305719870143e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3036305719870143e-05

Optimization complete. Final v2v error: 3.774045467376709 mm

Highest mean error: 4.4268999099731445 mm for frame 105

Lowest mean error: 3.2901241779327393 mm for frame 145

Saving results

Total time: 44.253422021865845
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00792931
Iteration 2/25 | Loss: 0.00136484
Iteration 3/25 | Loss: 0.00120128
Iteration 4/25 | Loss: 0.00118129
Iteration 5/25 | Loss: 0.00117747
Iteration 6/25 | Loss: 0.00117683
Iteration 7/25 | Loss: 0.00117683
Iteration 8/25 | Loss: 0.00117683
Iteration 9/25 | Loss: 0.00117683
Iteration 10/25 | Loss: 0.00117683
Iteration 11/25 | Loss: 0.00117683
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011768339900299907, 0.0011768339900299907, 0.0011768339900299907, 0.0011768339900299907, 0.0011768339900299907]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011768339900299907

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.21753192
Iteration 2/25 | Loss: 0.00128992
Iteration 3/25 | Loss: 0.00128989
Iteration 4/25 | Loss: 0.00128988
Iteration 5/25 | Loss: 0.00128988
Iteration 6/25 | Loss: 0.00128988
Iteration 7/25 | Loss: 0.00128988
Iteration 8/25 | Loss: 0.00128988
Iteration 9/25 | Loss: 0.00128988
Iteration 10/25 | Loss: 0.00128988
Iteration 11/25 | Loss: 0.00128988
Iteration 12/25 | Loss: 0.00128988
Iteration 13/25 | Loss: 0.00128988
Iteration 14/25 | Loss: 0.00128988
Iteration 15/25 | Loss: 0.00128988
Iteration 16/25 | Loss: 0.00128988
Iteration 17/25 | Loss: 0.00128988
Iteration 18/25 | Loss: 0.00128988
Iteration 19/25 | Loss: 0.00128988
Iteration 20/25 | Loss: 0.00128988
Iteration 21/25 | Loss: 0.00128988
Iteration 22/25 | Loss: 0.00128988
Iteration 23/25 | Loss: 0.00128988
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0012898797867819667, 0.0012898797867819667, 0.0012898797867819667, 0.0012898797867819667, 0.0012898797867819667]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012898797867819667

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128988
Iteration 2/1000 | Loss: 0.00004034
Iteration 3/1000 | Loss: 0.00002623
Iteration 4/1000 | Loss: 0.00002184
Iteration 5/1000 | Loss: 0.00002019
Iteration 6/1000 | Loss: 0.00001918
Iteration 7/1000 | Loss: 0.00001854
Iteration 8/1000 | Loss: 0.00001807
Iteration 9/1000 | Loss: 0.00001779
Iteration 10/1000 | Loss: 0.00001768
Iteration 11/1000 | Loss: 0.00001737
Iteration 12/1000 | Loss: 0.00001702
Iteration 13/1000 | Loss: 0.00001699
Iteration 14/1000 | Loss: 0.00001689
Iteration 15/1000 | Loss: 0.00001676
Iteration 16/1000 | Loss: 0.00001664
Iteration 17/1000 | Loss: 0.00001652
Iteration 18/1000 | Loss: 0.00001650
Iteration 19/1000 | Loss: 0.00001639
Iteration 20/1000 | Loss: 0.00001639
Iteration 21/1000 | Loss: 0.00001636
Iteration 22/1000 | Loss: 0.00001630
Iteration 23/1000 | Loss: 0.00001625
Iteration 24/1000 | Loss: 0.00001620
Iteration 25/1000 | Loss: 0.00001615
Iteration 26/1000 | Loss: 0.00001615
Iteration 27/1000 | Loss: 0.00001615
Iteration 28/1000 | Loss: 0.00001615
Iteration 29/1000 | Loss: 0.00001615
Iteration 30/1000 | Loss: 0.00001615
Iteration 31/1000 | Loss: 0.00001615
Iteration 32/1000 | Loss: 0.00001615
Iteration 33/1000 | Loss: 0.00001614
Iteration 34/1000 | Loss: 0.00001614
Iteration 35/1000 | Loss: 0.00001614
Iteration 36/1000 | Loss: 0.00001613
Iteration 37/1000 | Loss: 0.00001612
Iteration 38/1000 | Loss: 0.00001612
Iteration 39/1000 | Loss: 0.00001612
Iteration 40/1000 | Loss: 0.00001611
Iteration 41/1000 | Loss: 0.00001611
Iteration 42/1000 | Loss: 0.00001611
Iteration 43/1000 | Loss: 0.00001610
Iteration 44/1000 | Loss: 0.00001610
Iteration 45/1000 | Loss: 0.00001610
Iteration 46/1000 | Loss: 0.00001609
Iteration 47/1000 | Loss: 0.00001609
Iteration 48/1000 | Loss: 0.00001608
Iteration 49/1000 | Loss: 0.00001608
Iteration 50/1000 | Loss: 0.00001608
Iteration 51/1000 | Loss: 0.00001608
Iteration 52/1000 | Loss: 0.00001607
Iteration 53/1000 | Loss: 0.00001607
Iteration 54/1000 | Loss: 0.00001607
Iteration 55/1000 | Loss: 0.00001606
Iteration 56/1000 | Loss: 0.00001606
Iteration 57/1000 | Loss: 0.00001606
Iteration 58/1000 | Loss: 0.00001606
Iteration 59/1000 | Loss: 0.00001605
Iteration 60/1000 | Loss: 0.00001605
Iteration 61/1000 | Loss: 0.00001605
Iteration 62/1000 | Loss: 0.00001605
Iteration 63/1000 | Loss: 0.00001604
Iteration 64/1000 | Loss: 0.00001604
Iteration 65/1000 | Loss: 0.00001604
Iteration 66/1000 | Loss: 0.00001604
Iteration 67/1000 | Loss: 0.00001604
Iteration 68/1000 | Loss: 0.00001604
Iteration 69/1000 | Loss: 0.00001603
Iteration 70/1000 | Loss: 0.00001603
Iteration 71/1000 | Loss: 0.00001603
Iteration 72/1000 | Loss: 0.00001603
Iteration 73/1000 | Loss: 0.00001602
Iteration 74/1000 | Loss: 0.00001602
Iteration 75/1000 | Loss: 0.00001602
Iteration 76/1000 | Loss: 0.00001602
Iteration 77/1000 | Loss: 0.00001602
Iteration 78/1000 | Loss: 0.00001602
Iteration 79/1000 | Loss: 0.00001601
Iteration 80/1000 | Loss: 0.00001601
Iteration 81/1000 | Loss: 0.00001601
Iteration 82/1000 | Loss: 0.00001601
Iteration 83/1000 | Loss: 0.00001601
Iteration 84/1000 | Loss: 0.00001601
Iteration 85/1000 | Loss: 0.00001601
Iteration 86/1000 | Loss: 0.00001601
Iteration 87/1000 | Loss: 0.00001600
Iteration 88/1000 | Loss: 0.00001600
Iteration 89/1000 | Loss: 0.00001600
Iteration 90/1000 | Loss: 0.00001600
Iteration 91/1000 | Loss: 0.00001600
Iteration 92/1000 | Loss: 0.00001600
Iteration 93/1000 | Loss: 0.00001600
Iteration 94/1000 | Loss: 0.00001599
Iteration 95/1000 | Loss: 0.00001599
Iteration 96/1000 | Loss: 0.00001599
Iteration 97/1000 | Loss: 0.00001598
Iteration 98/1000 | Loss: 0.00001598
Iteration 99/1000 | Loss: 0.00001598
Iteration 100/1000 | Loss: 0.00001598
Iteration 101/1000 | Loss: 0.00001598
Iteration 102/1000 | Loss: 0.00001597
Iteration 103/1000 | Loss: 0.00001597
Iteration 104/1000 | Loss: 0.00001597
Iteration 105/1000 | Loss: 0.00001597
Iteration 106/1000 | Loss: 0.00001597
Iteration 107/1000 | Loss: 0.00001597
Iteration 108/1000 | Loss: 0.00001597
Iteration 109/1000 | Loss: 0.00001597
Iteration 110/1000 | Loss: 0.00001597
Iteration 111/1000 | Loss: 0.00001597
Iteration 112/1000 | Loss: 0.00001597
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [1.5972349501680583e-05, 1.5972349501680583e-05, 1.5972349501680583e-05, 1.5972349501680583e-05, 1.5972349501680583e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5972349501680583e-05

Optimization complete. Final v2v error: 3.307526111602783 mm

Highest mean error: 4.640171527862549 mm for frame 66

Lowest mean error: 2.4760704040527344 mm for frame 181

Saving results

Total time: 46.154815673828125
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00711412
Iteration 2/25 | Loss: 0.00160052
Iteration 3/25 | Loss: 0.00136490
Iteration 4/25 | Loss: 0.00132452
Iteration 5/25 | Loss: 0.00131296
Iteration 6/25 | Loss: 0.00130903
Iteration 7/25 | Loss: 0.00130703
Iteration 8/25 | Loss: 0.00129733
Iteration 9/25 | Loss: 0.00129942
Iteration 10/25 | Loss: 0.00129508
Iteration 11/25 | Loss: 0.00130383
Iteration 12/25 | Loss: 0.00129983
Iteration 13/25 | Loss: 0.00130824
Iteration 14/25 | Loss: 0.00131671
Iteration 15/25 | Loss: 0.00129073
Iteration 16/25 | Loss: 0.00128599
Iteration 17/25 | Loss: 0.00128090
Iteration 18/25 | Loss: 0.00127052
Iteration 19/25 | Loss: 0.00126345
Iteration 20/25 | Loss: 0.00126494
Iteration 21/25 | Loss: 0.00126413
Iteration 22/25 | Loss: 0.00126229
Iteration 23/25 | Loss: 0.00126411
Iteration 24/25 | Loss: 0.00126525
Iteration 25/25 | Loss: 0.00126418

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.03717995
Iteration 2/25 | Loss: 0.00942364
Iteration 3/25 | Loss: 0.01143738
Iteration 4/25 | Loss: 0.01218040
Iteration 5/25 | Loss: 0.01008354
Iteration 6/25 | Loss: 0.01115486
Iteration 7/25 | Loss: 0.00178095
Iteration 8/25 | Loss: 0.00178088
Iteration 9/25 | Loss: 0.00178088
Iteration 10/25 | Loss: 0.00178088
Iteration 11/25 | Loss: 0.00178088
Iteration 12/25 | Loss: 0.00178088
Iteration 13/25 | Loss: 0.00178088
Iteration 14/25 | Loss: 0.00178088
Iteration 15/25 | Loss: 0.00178088
Iteration 16/25 | Loss: 0.00178088
Iteration 17/25 | Loss: 0.00178088
Iteration 18/25 | Loss: 0.00178088
Iteration 19/25 | Loss: 0.00178088
Iteration 20/25 | Loss: 0.00178088
Iteration 21/25 | Loss: 0.00178088
Iteration 22/25 | Loss: 0.00178088
Iteration 23/25 | Loss: 0.00178088
Iteration 24/25 | Loss: 0.00178088
Iteration 25/25 | Loss: 0.00178088

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00178088
Iteration 2/1000 | Loss: 0.00017228
Iteration 3/1000 | Loss: 0.00008700
Iteration 4/1000 | Loss: 0.00462478
Iteration 5/1000 | Loss: 0.00875233
Iteration 6/1000 | Loss: 0.00563899
Iteration 7/1000 | Loss: 0.00314170
Iteration 8/1000 | Loss: 0.00276102
Iteration 9/1000 | Loss: 0.00018029
Iteration 10/1000 | Loss: 0.00073224
Iteration 11/1000 | Loss: 0.00196765
Iteration 12/1000 | Loss: 0.00023247
Iteration 13/1000 | Loss: 0.00020838
Iteration 14/1000 | Loss: 0.00022421
Iteration 15/1000 | Loss: 0.00023971
Iteration 16/1000 | Loss: 0.00023259
Iteration 17/1000 | Loss: 0.00020871
Iteration 18/1000 | Loss: 0.00019110
Iteration 19/1000 | Loss: 0.00022133
Iteration 20/1000 | Loss: 0.00020478
Iteration 21/1000 | Loss: 0.00014536
Iteration 22/1000 | Loss: 0.00017806
Iteration 23/1000 | Loss: 0.00019820
Iteration 24/1000 | Loss: 0.00021444
Iteration 25/1000 | Loss: 0.00020806
Iteration 26/1000 | Loss: 0.00023628
Iteration 27/1000 | Loss: 0.00004670
Iteration 28/1000 | Loss: 0.00027958
Iteration 29/1000 | Loss: 0.00013335
Iteration 30/1000 | Loss: 0.00004240
Iteration 31/1000 | Loss: 0.00010057
Iteration 32/1000 | Loss: 0.00021050
Iteration 33/1000 | Loss: 0.00013459
Iteration 34/1000 | Loss: 0.00018512
Iteration 35/1000 | Loss: 0.00020090
Iteration 36/1000 | Loss: 0.00018461
Iteration 37/1000 | Loss: 0.00021176
Iteration 38/1000 | Loss: 0.00023257
Iteration 39/1000 | Loss: 0.00004887
Iteration 40/1000 | Loss: 0.00016407
Iteration 41/1000 | Loss: 0.00022492
Iteration 42/1000 | Loss: 0.00017231
Iteration 43/1000 | Loss: 0.00020068
Iteration 44/1000 | Loss: 0.00012832
Iteration 45/1000 | Loss: 0.00016442
Iteration 46/1000 | Loss: 0.00018516
Iteration 47/1000 | Loss: 0.00018757
Iteration 48/1000 | Loss: 0.00042820
Iteration 49/1000 | Loss: 0.00005636
Iteration 50/1000 | Loss: 0.00003897
Iteration 51/1000 | Loss: 0.00003232
Iteration 52/1000 | Loss: 0.00002829
Iteration 53/1000 | Loss: 0.00002656
Iteration 54/1000 | Loss: 0.00002573
Iteration 55/1000 | Loss: 0.00002490
Iteration 56/1000 | Loss: 0.00002436
Iteration 57/1000 | Loss: 0.00002398
Iteration 58/1000 | Loss: 0.00002351
Iteration 59/1000 | Loss: 0.00002311
Iteration 60/1000 | Loss: 0.00002286
Iteration 61/1000 | Loss: 0.00002274
Iteration 62/1000 | Loss: 0.00002252
Iteration 63/1000 | Loss: 0.00002230
Iteration 64/1000 | Loss: 0.00026239
Iteration 65/1000 | Loss: 0.00020043
Iteration 66/1000 | Loss: 0.00024356
Iteration 67/1000 | Loss: 0.00034106
Iteration 68/1000 | Loss: 0.00013588
Iteration 69/1000 | Loss: 0.00022619
Iteration 70/1000 | Loss: 0.00011957
Iteration 71/1000 | Loss: 0.00002728
Iteration 72/1000 | Loss: 0.00002361
Iteration 73/1000 | Loss: 0.00002273
Iteration 74/1000 | Loss: 0.00002236
Iteration 75/1000 | Loss: 0.00002220
Iteration 76/1000 | Loss: 0.00002218
Iteration 77/1000 | Loss: 0.00002217
Iteration 78/1000 | Loss: 0.00002205
Iteration 79/1000 | Loss: 0.00002205
Iteration 80/1000 | Loss: 0.00002205
Iteration 81/1000 | Loss: 0.00002204
Iteration 82/1000 | Loss: 0.00002204
Iteration 83/1000 | Loss: 0.00002204
Iteration 84/1000 | Loss: 0.00002203
Iteration 85/1000 | Loss: 0.00002203
Iteration 86/1000 | Loss: 0.00002203
Iteration 87/1000 | Loss: 0.00002203
Iteration 88/1000 | Loss: 0.00002202
Iteration 89/1000 | Loss: 0.00002202
Iteration 90/1000 | Loss: 0.00002202
Iteration 91/1000 | Loss: 0.00002202
Iteration 92/1000 | Loss: 0.00002202
Iteration 93/1000 | Loss: 0.00002202
Iteration 94/1000 | Loss: 0.00002202
Iteration 95/1000 | Loss: 0.00002201
Iteration 96/1000 | Loss: 0.00002201
Iteration 97/1000 | Loss: 0.00002201
Iteration 98/1000 | Loss: 0.00002200
Iteration 99/1000 | Loss: 0.00002200
Iteration 100/1000 | Loss: 0.00002200
Iteration 101/1000 | Loss: 0.00002199
Iteration 102/1000 | Loss: 0.00002199
Iteration 103/1000 | Loss: 0.00002199
Iteration 104/1000 | Loss: 0.00002199
Iteration 105/1000 | Loss: 0.00002198
Iteration 106/1000 | Loss: 0.00002198
Iteration 107/1000 | Loss: 0.00002198
Iteration 108/1000 | Loss: 0.00002198
Iteration 109/1000 | Loss: 0.00002198
Iteration 110/1000 | Loss: 0.00002197
Iteration 111/1000 | Loss: 0.00002197
Iteration 112/1000 | Loss: 0.00002197
Iteration 113/1000 | Loss: 0.00002197
Iteration 114/1000 | Loss: 0.00002197
Iteration 115/1000 | Loss: 0.00002197
Iteration 116/1000 | Loss: 0.00002197
Iteration 117/1000 | Loss: 0.00002197
Iteration 118/1000 | Loss: 0.00002197
Iteration 119/1000 | Loss: 0.00002197
Iteration 120/1000 | Loss: 0.00002196
Iteration 121/1000 | Loss: 0.00002196
Iteration 122/1000 | Loss: 0.00002196
Iteration 123/1000 | Loss: 0.00002196
Iteration 124/1000 | Loss: 0.00002195
Iteration 125/1000 | Loss: 0.00002195
Iteration 126/1000 | Loss: 0.00002195
Iteration 127/1000 | Loss: 0.00002194
Iteration 128/1000 | Loss: 0.00002194
Iteration 129/1000 | Loss: 0.00002193
Iteration 130/1000 | Loss: 0.00002193
Iteration 131/1000 | Loss: 0.00002193
Iteration 132/1000 | Loss: 0.00002193
Iteration 133/1000 | Loss: 0.00002192
Iteration 134/1000 | Loss: 0.00002190
Iteration 135/1000 | Loss: 0.00002190
Iteration 136/1000 | Loss: 0.00002189
Iteration 137/1000 | Loss: 0.00002189
Iteration 138/1000 | Loss: 0.00002189
Iteration 139/1000 | Loss: 0.00002189
Iteration 140/1000 | Loss: 0.00002188
Iteration 141/1000 | Loss: 0.00002188
Iteration 142/1000 | Loss: 0.00002188
Iteration 143/1000 | Loss: 0.00002188
Iteration 144/1000 | Loss: 0.00002188
Iteration 145/1000 | Loss: 0.00002188
Iteration 146/1000 | Loss: 0.00002188
Iteration 147/1000 | Loss: 0.00002188
Iteration 148/1000 | Loss: 0.00002188
Iteration 149/1000 | Loss: 0.00002187
Iteration 150/1000 | Loss: 0.00002187
Iteration 151/1000 | Loss: 0.00002187
Iteration 152/1000 | Loss: 0.00002187
Iteration 153/1000 | Loss: 0.00002186
Iteration 154/1000 | Loss: 0.00002186
Iteration 155/1000 | Loss: 0.00002186
Iteration 156/1000 | Loss: 0.00002186
Iteration 157/1000 | Loss: 0.00002186
Iteration 158/1000 | Loss: 0.00002186
Iteration 159/1000 | Loss: 0.00002186
Iteration 160/1000 | Loss: 0.00002186
Iteration 161/1000 | Loss: 0.00002186
Iteration 162/1000 | Loss: 0.00002185
Iteration 163/1000 | Loss: 0.00002185
Iteration 164/1000 | Loss: 0.00002185
Iteration 165/1000 | Loss: 0.00002184
Iteration 166/1000 | Loss: 0.00002184
Iteration 167/1000 | Loss: 0.00002183
Iteration 168/1000 | Loss: 0.00002183
Iteration 169/1000 | Loss: 0.00002183
Iteration 170/1000 | Loss: 0.00002181
Iteration 171/1000 | Loss: 0.00002179
Iteration 172/1000 | Loss: 0.00002178
Iteration 173/1000 | Loss: 0.00002178
Iteration 174/1000 | Loss: 0.00002178
Iteration 175/1000 | Loss: 0.00002177
Iteration 176/1000 | Loss: 0.00002177
Iteration 177/1000 | Loss: 0.00002177
Iteration 178/1000 | Loss: 0.00002177
Iteration 179/1000 | Loss: 0.00002177
Iteration 180/1000 | Loss: 0.00002176
Iteration 181/1000 | Loss: 0.00002176
Iteration 182/1000 | Loss: 0.00002176
Iteration 183/1000 | Loss: 0.00002176
Iteration 184/1000 | Loss: 0.00002176
Iteration 185/1000 | Loss: 0.00002175
Iteration 186/1000 | Loss: 0.00002175
Iteration 187/1000 | Loss: 0.00002175
Iteration 188/1000 | Loss: 0.00002175
Iteration 189/1000 | Loss: 0.00002175
Iteration 190/1000 | Loss: 0.00002175
Iteration 191/1000 | Loss: 0.00002174
Iteration 192/1000 | Loss: 0.00002174
Iteration 193/1000 | Loss: 0.00002174
Iteration 194/1000 | Loss: 0.00002174
Iteration 195/1000 | Loss: 0.00002173
Iteration 196/1000 | Loss: 0.00002173
Iteration 197/1000 | Loss: 0.00002173
Iteration 198/1000 | Loss: 0.00002173
Iteration 199/1000 | Loss: 0.00002173
Iteration 200/1000 | Loss: 0.00002173
Iteration 201/1000 | Loss: 0.00002173
Iteration 202/1000 | Loss: 0.00002173
Iteration 203/1000 | Loss: 0.00002173
Iteration 204/1000 | Loss: 0.00002173
Iteration 205/1000 | Loss: 0.00002173
Iteration 206/1000 | Loss: 0.00002173
Iteration 207/1000 | Loss: 0.00002172
Iteration 208/1000 | Loss: 0.00002172
Iteration 209/1000 | Loss: 0.00002172
Iteration 210/1000 | Loss: 0.00002172
Iteration 211/1000 | Loss: 0.00002172
Iteration 212/1000 | Loss: 0.00002172
Iteration 213/1000 | Loss: 0.00002172
Iteration 214/1000 | Loss: 0.00002172
Iteration 215/1000 | Loss: 0.00002172
Iteration 216/1000 | Loss: 0.00002172
Iteration 217/1000 | Loss: 0.00002172
Iteration 218/1000 | Loss: 0.00002172
Iteration 219/1000 | Loss: 0.00002172
Iteration 220/1000 | Loss: 0.00002172
Iteration 221/1000 | Loss: 0.00002172
Iteration 222/1000 | Loss: 0.00002172
Iteration 223/1000 | Loss: 0.00002172
Iteration 224/1000 | Loss: 0.00002172
Iteration 225/1000 | Loss: 0.00002172
Iteration 226/1000 | Loss: 0.00002172
Iteration 227/1000 | Loss: 0.00002172
Iteration 228/1000 | Loss: 0.00002172
Iteration 229/1000 | Loss: 0.00002172
Iteration 230/1000 | Loss: 0.00002172
Iteration 231/1000 | Loss: 0.00002172
Iteration 232/1000 | Loss: 0.00002172
Iteration 233/1000 | Loss: 0.00002172
Iteration 234/1000 | Loss: 0.00002172
Iteration 235/1000 | Loss: 0.00002172
Iteration 236/1000 | Loss: 0.00002172
Iteration 237/1000 | Loss: 0.00002172
Iteration 238/1000 | Loss: 0.00002172
Iteration 239/1000 | Loss: 0.00002172
Iteration 240/1000 | Loss: 0.00002172
Iteration 241/1000 | Loss: 0.00002172
Iteration 242/1000 | Loss: 0.00002172
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 242. Stopping optimization.
Last 5 losses: [2.1721436496591195e-05, 2.1721436496591195e-05, 2.1721436496591195e-05, 2.1721436496591195e-05, 2.1721436496591195e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1721436496591195e-05

Optimization complete. Final v2v error: 3.699954032897949 mm

Highest mean error: 8.617913246154785 mm for frame 77

Lowest mean error: 2.658430814743042 mm for frame 12

Saving results

Total time: 166.85153341293335
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00814843
Iteration 2/25 | Loss: 0.00142489
Iteration 3/25 | Loss: 0.00122647
Iteration 4/25 | Loss: 0.00122101
Iteration 5/25 | Loss: 0.00122101
Iteration 6/25 | Loss: 0.00122101
Iteration 7/25 | Loss: 0.00122101
Iteration 8/25 | Loss: 0.00122101
Iteration 9/25 | Loss: 0.00122101
Iteration 10/25 | Loss: 0.00122101
Iteration 11/25 | Loss: 0.00122101
Iteration 12/25 | Loss: 0.00122101
Iteration 13/25 | Loss: 0.00122101
Iteration 14/25 | Loss: 0.00122101
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012210089480504394, 0.0012210089480504394, 0.0012210089480504394, 0.0012210089480504394, 0.0012210089480504394]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012210089480504394

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.92681247
Iteration 2/25 | Loss: 0.00069246
Iteration 3/25 | Loss: 0.00069245
Iteration 4/25 | Loss: 0.00069245
Iteration 5/25 | Loss: 0.00069245
Iteration 6/25 | Loss: 0.00069245
Iteration 7/25 | Loss: 0.00069245
Iteration 8/25 | Loss: 0.00069245
Iteration 9/25 | Loss: 0.00069245
Iteration 10/25 | Loss: 0.00069245
Iteration 11/25 | Loss: 0.00069245
Iteration 12/25 | Loss: 0.00069245
Iteration 13/25 | Loss: 0.00069245
Iteration 14/25 | Loss: 0.00069245
Iteration 15/25 | Loss: 0.00069245
Iteration 16/25 | Loss: 0.00069245
Iteration 17/25 | Loss: 0.00069245
Iteration 18/25 | Loss: 0.00069245
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006924502085894346, 0.0006924502085894346, 0.0006924502085894346, 0.0006924502085894346, 0.0006924502085894346]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006924502085894346

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069245
Iteration 2/1000 | Loss: 0.00002962
Iteration 3/1000 | Loss: 0.00002400
Iteration 4/1000 | Loss: 0.00002261
Iteration 5/1000 | Loss: 0.00002138
Iteration 6/1000 | Loss: 0.00002072
Iteration 7/1000 | Loss: 0.00002041
Iteration 8/1000 | Loss: 0.00001991
Iteration 9/1000 | Loss: 0.00001973
Iteration 10/1000 | Loss: 0.00001950
Iteration 11/1000 | Loss: 0.00001939
Iteration 12/1000 | Loss: 0.00001934
Iteration 13/1000 | Loss: 0.00001933
Iteration 14/1000 | Loss: 0.00001931
Iteration 15/1000 | Loss: 0.00001920
Iteration 16/1000 | Loss: 0.00001905
Iteration 17/1000 | Loss: 0.00001901
Iteration 18/1000 | Loss: 0.00001901
Iteration 19/1000 | Loss: 0.00001900
Iteration 20/1000 | Loss: 0.00001898
Iteration 21/1000 | Loss: 0.00001898
Iteration 22/1000 | Loss: 0.00001889
Iteration 23/1000 | Loss: 0.00001889
Iteration 24/1000 | Loss: 0.00001888
Iteration 25/1000 | Loss: 0.00001888
Iteration 26/1000 | Loss: 0.00001887
Iteration 27/1000 | Loss: 0.00001887
Iteration 28/1000 | Loss: 0.00001887
Iteration 29/1000 | Loss: 0.00001886
Iteration 30/1000 | Loss: 0.00001886
Iteration 31/1000 | Loss: 0.00001885
Iteration 32/1000 | Loss: 0.00001885
Iteration 33/1000 | Loss: 0.00001885
Iteration 34/1000 | Loss: 0.00001884
Iteration 35/1000 | Loss: 0.00001884
Iteration 36/1000 | Loss: 0.00001884
Iteration 37/1000 | Loss: 0.00001881
Iteration 38/1000 | Loss: 0.00001880
Iteration 39/1000 | Loss: 0.00001880
Iteration 40/1000 | Loss: 0.00001880
Iteration 41/1000 | Loss: 0.00001880
Iteration 42/1000 | Loss: 0.00001880
Iteration 43/1000 | Loss: 0.00001880
Iteration 44/1000 | Loss: 0.00001879
Iteration 45/1000 | Loss: 0.00001872
Iteration 46/1000 | Loss: 0.00001867
Iteration 47/1000 | Loss: 0.00001867
Iteration 48/1000 | Loss: 0.00001867
Iteration 49/1000 | Loss: 0.00001867
Iteration 50/1000 | Loss: 0.00001867
Iteration 51/1000 | Loss: 0.00001866
Iteration 52/1000 | Loss: 0.00001866
Iteration 53/1000 | Loss: 0.00001866
Iteration 54/1000 | Loss: 0.00001866
Iteration 55/1000 | Loss: 0.00001866
Iteration 56/1000 | Loss: 0.00001866
Iteration 57/1000 | Loss: 0.00001866
Iteration 58/1000 | Loss: 0.00001866
Iteration 59/1000 | Loss: 0.00001866
Iteration 60/1000 | Loss: 0.00001866
Iteration 61/1000 | Loss: 0.00001865
Iteration 62/1000 | Loss: 0.00001865
Iteration 63/1000 | Loss: 0.00001864
Iteration 64/1000 | Loss: 0.00001864
Iteration 65/1000 | Loss: 0.00001863
Iteration 66/1000 | Loss: 0.00001863
Iteration 67/1000 | Loss: 0.00001863
Iteration 68/1000 | Loss: 0.00001863
Iteration 69/1000 | Loss: 0.00001863
Iteration 70/1000 | Loss: 0.00001863
Iteration 71/1000 | Loss: 0.00001863
Iteration 72/1000 | Loss: 0.00001863
Iteration 73/1000 | Loss: 0.00001862
Iteration 74/1000 | Loss: 0.00001861
Iteration 75/1000 | Loss: 0.00001861
Iteration 76/1000 | Loss: 0.00001861
Iteration 77/1000 | Loss: 0.00001861
Iteration 78/1000 | Loss: 0.00001861
Iteration 79/1000 | Loss: 0.00001861
Iteration 80/1000 | Loss: 0.00001861
Iteration 81/1000 | Loss: 0.00001861
Iteration 82/1000 | Loss: 0.00001861
Iteration 83/1000 | Loss: 0.00001861
Iteration 84/1000 | Loss: 0.00001861
Iteration 85/1000 | Loss: 0.00001861
Iteration 86/1000 | Loss: 0.00001861
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 86. Stopping optimization.
Last 5 losses: [1.8611475752550177e-05, 1.8611475752550177e-05, 1.8611475752550177e-05, 1.8611475752550177e-05, 1.8611475752550177e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8611475752550177e-05

Optimization complete. Final v2v error: 3.6095387935638428 mm

Highest mean error: 3.7398300170898438 mm for frame 101

Lowest mean error: 3.4952874183654785 mm for frame 239

Saving results

Total time: 36.78068733215332
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00678564
Iteration 2/25 | Loss: 0.00138060
Iteration 3/25 | Loss: 0.00128274
Iteration 4/25 | Loss: 0.00125747
Iteration 5/25 | Loss: 0.00124767
Iteration 6/25 | Loss: 0.00124632
Iteration 7/25 | Loss: 0.00124632
Iteration 8/25 | Loss: 0.00124632
Iteration 9/25 | Loss: 0.00124632
Iteration 10/25 | Loss: 0.00124632
Iteration 11/25 | Loss: 0.00124632
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012463151942938566, 0.0012463151942938566, 0.0012463151942938566, 0.0012463151942938566, 0.0012463151942938566]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012463151942938566

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56109893
Iteration 2/25 | Loss: 0.00164276
Iteration 3/25 | Loss: 0.00164276
Iteration 4/25 | Loss: 0.00164275
Iteration 5/25 | Loss: 0.00164275
Iteration 6/25 | Loss: 0.00164275
Iteration 7/25 | Loss: 0.00164275
Iteration 8/25 | Loss: 0.00164275
Iteration 9/25 | Loss: 0.00164275
Iteration 10/25 | Loss: 0.00164275
Iteration 11/25 | Loss: 0.00164275
Iteration 12/25 | Loss: 0.00164275
Iteration 13/25 | Loss: 0.00164275
Iteration 14/25 | Loss: 0.00164275
Iteration 15/25 | Loss: 0.00164275
Iteration 16/25 | Loss: 0.00164275
Iteration 17/25 | Loss: 0.00164275
Iteration 18/25 | Loss: 0.00164275
Iteration 19/25 | Loss: 0.00164275
Iteration 20/25 | Loss: 0.00164275
Iteration 21/25 | Loss: 0.00164275
Iteration 22/25 | Loss: 0.00164275
Iteration 23/25 | Loss: 0.00164275
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0016427510417997837, 0.0016427510417997837, 0.0016427510417997837, 0.0016427510417997837, 0.0016427510417997837]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016427510417997837

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00164275
Iteration 2/1000 | Loss: 0.00007236
Iteration 3/1000 | Loss: 0.00004238
Iteration 4/1000 | Loss: 0.00003281
Iteration 5/1000 | Loss: 0.00003005
Iteration 6/1000 | Loss: 0.00002858
Iteration 7/1000 | Loss: 0.00002744
Iteration 8/1000 | Loss: 0.00002667
Iteration 9/1000 | Loss: 0.00002611
Iteration 10/1000 | Loss: 0.00002564
Iteration 11/1000 | Loss: 0.00002533
Iteration 12/1000 | Loss: 0.00002530
Iteration 13/1000 | Loss: 0.00002508
Iteration 14/1000 | Loss: 0.00002504
Iteration 15/1000 | Loss: 0.00002501
Iteration 16/1000 | Loss: 0.00002484
Iteration 17/1000 | Loss: 0.00002470
Iteration 18/1000 | Loss: 0.00002468
Iteration 19/1000 | Loss: 0.00002465
Iteration 20/1000 | Loss: 0.00002464
Iteration 21/1000 | Loss: 0.00002464
Iteration 22/1000 | Loss: 0.00002461
Iteration 23/1000 | Loss: 0.00002461
Iteration 24/1000 | Loss: 0.00002460
Iteration 25/1000 | Loss: 0.00002459
Iteration 26/1000 | Loss: 0.00002459
Iteration 27/1000 | Loss: 0.00002458
Iteration 28/1000 | Loss: 0.00002455
Iteration 29/1000 | Loss: 0.00002455
Iteration 30/1000 | Loss: 0.00002455
Iteration 31/1000 | Loss: 0.00002454
Iteration 32/1000 | Loss: 0.00002454
Iteration 33/1000 | Loss: 0.00002454
Iteration 34/1000 | Loss: 0.00002453
Iteration 35/1000 | Loss: 0.00002452
Iteration 36/1000 | Loss: 0.00002452
Iteration 37/1000 | Loss: 0.00002452
Iteration 38/1000 | Loss: 0.00002452
Iteration 39/1000 | Loss: 0.00002451
Iteration 40/1000 | Loss: 0.00002450
Iteration 41/1000 | Loss: 0.00002450
Iteration 42/1000 | Loss: 0.00002449
Iteration 43/1000 | Loss: 0.00002449
Iteration 44/1000 | Loss: 0.00002449
Iteration 45/1000 | Loss: 0.00002448
Iteration 46/1000 | Loss: 0.00002448
Iteration 47/1000 | Loss: 0.00002447
Iteration 48/1000 | Loss: 0.00002447
Iteration 49/1000 | Loss: 0.00002446
Iteration 50/1000 | Loss: 0.00002446
Iteration 51/1000 | Loss: 0.00002445
Iteration 52/1000 | Loss: 0.00002445
Iteration 53/1000 | Loss: 0.00002444
Iteration 54/1000 | Loss: 0.00002444
Iteration 55/1000 | Loss: 0.00002444
Iteration 56/1000 | Loss: 0.00002443
Iteration 57/1000 | Loss: 0.00002443
Iteration 58/1000 | Loss: 0.00002442
Iteration 59/1000 | Loss: 0.00002442
Iteration 60/1000 | Loss: 0.00002442
Iteration 61/1000 | Loss: 0.00002441
Iteration 62/1000 | Loss: 0.00002441
Iteration 63/1000 | Loss: 0.00002441
Iteration 64/1000 | Loss: 0.00002441
Iteration 65/1000 | Loss: 0.00002440
Iteration 66/1000 | Loss: 0.00002440
Iteration 67/1000 | Loss: 0.00002439
Iteration 68/1000 | Loss: 0.00002439
Iteration 69/1000 | Loss: 0.00002438
Iteration 70/1000 | Loss: 0.00002438
Iteration 71/1000 | Loss: 0.00002438
Iteration 72/1000 | Loss: 0.00002437
Iteration 73/1000 | Loss: 0.00002437
Iteration 74/1000 | Loss: 0.00002437
Iteration 75/1000 | Loss: 0.00002437
Iteration 76/1000 | Loss: 0.00002436
Iteration 77/1000 | Loss: 0.00002436
Iteration 78/1000 | Loss: 0.00002436
Iteration 79/1000 | Loss: 0.00002435
Iteration 80/1000 | Loss: 0.00002435
Iteration 81/1000 | Loss: 0.00002435
Iteration 82/1000 | Loss: 0.00002435
Iteration 83/1000 | Loss: 0.00002434
Iteration 84/1000 | Loss: 0.00002434
Iteration 85/1000 | Loss: 0.00002434
Iteration 86/1000 | Loss: 0.00002434
Iteration 87/1000 | Loss: 0.00002434
Iteration 88/1000 | Loss: 0.00002434
Iteration 89/1000 | Loss: 0.00002433
Iteration 90/1000 | Loss: 0.00002433
Iteration 91/1000 | Loss: 0.00002433
Iteration 92/1000 | Loss: 0.00002433
Iteration 93/1000 | Loss: 0.00002432
Iteration 94/1000 | Loss: 0.00002432
Iteration 95/1000 | Loss: 0.00002432
Iteration 96/1000 | Loss: 0.00002431
Iteration 97/1000 | Loss: 0.00002431
Iteration 98/1000 | Loss: 0.00002431
Iteration 99/1000 | Loss: 0.00002430
Iteration 100/1000 | Loss: 0.00002430
Iteration 101/1000 | Loss: 0.00002430
Iteration 102/1000 | Loss: 0.00002430
Iteration 103/1000 | Loss: 0.00002430
Iteration 104/1000 | Loss: 0.00002429
Iteration 105/1000 | Loss: 0.00002429
Iteration 106/1000 | Loss: 0.00002429
Iteration 107/1000 | Loss: 0.00002429
Iteration 108/1000 | Loss: 0.00002429
Iteration 109/1000 | Loss: 0.00002428
Iteration 110/1000 | Loss: 0.00002428
Iteration 111/1000 | Loss: 0.00002428
Iteration 112/1000 | Loss: 0.00002427
Iteration 113/1000 | Loss: 0.00002427
Iteration 114/1000 | Loss: 0.00002427
Iteration 115/1000 | Loss: 0.00002427
Iteration 116/1000 | Loss: 0.00002427
Iteration 117/1000 | Loss: 0.00002426
Iteration 118/1000 | Loss: 0.00002426
Iteration 119/1000 | Loss: 0.00002426
Iteration 120/1000 | Loss: 0.00002426
Iteration 121/1000 | Loss: 0.00002426
Iteration 122/1000 | Loss: 0.00002425
Iteration 123/1000 | Loss: 0.00002425
Iteration 124/1000 | Loss: 0.00002425
Iteration 125/1000 | Loss: 0.00002425
Iteration 126/1000 | Loss: 0.00002425
Iteration 127/1000 | Loss: 0.00002425
Iteration 128/1000 | Loss: 0.00002425
Iteration 129/1000 | Loss: 0.00002425
Iteration 130/1000 | Loss: 0.00002425
Iteration 131/1000 | Loss: 0.00002425
Iteration 132/1000 | Loss: 0.00002425
Iteration 133/1000 | Loss: 0.00002425
Iteration 134/1000 | Loss: 0.00002425
Iteration 135/1000 | Loss: 0.00002425
Iteration 136/1000 | Loss: 0.00002425
Iteration 137/1000 | Loss: 0.00002425
Iteration 138/1000 | Loss: 0.00002425
Iteration 139/1000 | Loss: 0.00002425
Iteration 140/1000 | Loss: 0.00002425
Iteration 141/1000 | Loss: 0.00002425
Iteration 142/1000 | Loss: 0.00002425
Iteration 143/1000 | Loss: 0.00002425
Iteration 144/1000 | Loss: 0.00002425
Iteration 145/1000 | Loss: 0.00002425
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [2.42509413510561e-05, 2.42509413510561e-05, 2.42509413510561e-05, 2.42509413510561e-05, 2.42509413510561e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.42509413510561e-05

Optimization complete. Final v2v error: 4.03244161605835 mm

Highest mean error: 5.142590522766113 mm for frame 164

Lowest mean error: 3.3196659088134766 mm for frame 65

Saving results

Total time: 44.77646017074585
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00772270
Iteration 2/25 | Loss: 0.00163629
Iteration 3/25 | Loss: 0.00137721
Iteration 4/25 | Loss: 0.00125651
Iteration 5/25 | Loss: 0.00125063
Iteration 6/25 | Loss: 0.00123295
Iteration 7/25 | Loss: 0.00121364
Iteration 8/25 | Loss: 0.00120719
Iteration 9/25 | Loss: 0.00120259
Iteration 10/25 | Loss: 0.00120150
Iteration 11/25 | Loss: 0.00119377
Iteration 12/25 | Loss: 0.00119259
Iteration 13/25 | Loss: 0.00119231
Iteration 14/25 | Loss: 0.00119217
Iteration 15/25 | Loss: 0.00119213
Iteration 16/25 | Loss: 0.00119212
Iteration 17/25 | Loss: 0.00119212
Iteration 18/25 | Loss: 0.00119212
Iteration 19/25 | Loss: 0.00119212
Iteration 20/25 | Loss: 0.00119212
Iteration 21/25 | Loss: 0.00119212
Iteration 22/25 | Loss: 0.00119211
Iteration 23/25 | Loss: 0.00119211
Iteration 24/25 | Loss: 0.00119211
Iteration 25/25 | Loss: 0.00119211

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.73139000
Iteration 2/25 | Loss: 0.00155429
Iteration 3/25 | Loss: 0.00155427
Iteration 4/25 | Loss: 0.00142123
Iteration 5/25 | Loss: 0.00142105
Iteration 6/25 | Loss: 0.00142105
Iteration 7/25 | Loss: 0.00142105
Iteration 8/25 | Loss: 0.00142105
Iteration 9/25 | Loss: 0.00142105
Iteration 10/25 | Loss: 0.00142105
Iteration 11/25 | Loss: 0.00142105
Iteration 12/25 | Loss: 0.00142105
Iteration 13/25 | Loss: 0.00142105
Iteration 14/25 | Loss: 0.00142105
Iteration 15/25 | Loss: 0.00142105
Iteration 16/25 | Loss: 0.00142105
Iteration 17/25 | Loss: 0.00142105
Iteration 18/25 | Loss: 0.00142105
Iteration 19/25 | Loss: 0.00142105
Iteration 20/25 | Loss: 0.00142105
Iteration 21/25 | Loss: 0.00142105
Iteration 22/25 | Loss: 0.00142105
Iteration 23/25 | Loss: 0.00142105
Iteration 24/25 | Loss: 0.00142105
Iteration 25/25 | Loss: 0.00142105

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00142105
Iteration 2/1000 | Loss: 0.00014285
Iteration 3/1000 | Loss: 0.00008868
Iteration 4/1000 | Loss: 0.00002012
Iteration 5/1000 | Loss: 0.00008290
Iteration 6/1000 | Loss: 0.00004838
Iteration 7/1000 | Loss: 0.00001806
Iteration 8/1000 | Loss: 0.00001760
Iteration 9/1000 | Loss: 0.00001715
Iteration 10/1000 | Loss: 0.00001674
Iteration 11/1000 | Loss: 0.00007100
Iteration 12/1000 | Loss: 0.00002245
Iteration 13/1000 | Loss: 0.00034450
Iteration 14/1000 | Loss: 0.00080183
Iteration 15/1000 | Loss: 0.00004754
Iteration 16/1000 | Loss: 0.00002390
Iteration 17/1000 | Loss: 0.00001700
Iteration 18/1000 | Loss: 0.00002033
Iteration 19/1000 | Loss: 0.00001325
Iteration 20/1000 | Loss: 0.00012008
Iteration 21/1000 | Loss: 0.00002225
Iteration 22/1000 | Loss: 0.00001189
Iteration 23/1000 | Loss: 0.00001147
Iteration 24/1000 | Loss: 0.00001124
Iteration 25/1000 | Loss: 0.00001105
Iteration 26/1000 | Loss: 0.00001093
Iteration 27/1000 | Loss: 0.00001069
Iteration 28/1000 | Loss: 0.00001068
Iteration 29/1000 | Loss: 0.00001056
Iteration 30/1000 | Loss: 0.00003894
Iteration 31/1000 | Loss: 0.00003663
Iteration 32/1000 | Loss: 0.00001038
Iteration 33/1000 | Loss: 0.00001029
Iteration 34/1000 | Loss: 0.00001028
Iteration 35/1000 | Loss: 0.00001028
Iteration 36/1000 | Loss: 0.00001712
Iteration 37/1000 | Loss: 0.00001603
Iteration 38/1000 | Loss: 0.00001025
Iteration 39/1000 | Loss: 0.00001025
Iteration 40/1000 | Loss: 0.00001025
Iteration 41/1000 | Loss: 0.00001024
Iteration 42/1000 | Loss: 0.00001024
Iteration 43/1000 | Loss: 0.00001024
Iteration 44/1000 | Loss: 0.00001024
Iteration 45/1000 | Loss: 0.00001024
Iteration 46/1000 | Loss: 0.00001024
Iteration 47/1000 | Loss: 0.00001024
Iteration 48/1000 | Loss: 0.00001024
Iteration 49/1000 | Loss: 0.00001024
Iteration 50/1000 | Loss: 0.00001024
Iteration 51/1000 | Loss: 0.00001024
Iteration 52/1000 | Loss: 0.00001024
Iteration 53/1000 | Loss: 0.00001023
Iteration 54/1000 | Loss: 0.00001023
Iteration 55/1000 | Loss: 0.00001023
Iteration 56/1000 | Loss: 0.00001023
Iteration 57/1000 | Loss: 0.00001023
Iteration 58/1000 | Loss: 0.00001023
Iteration 59/1000 | Loss: 0.00001023
Iteration 60/1000 | Loss: 0.00001023
Iteration 61/1000 | Loss: 0.00001022
Iteration 62/1000 | Loss: 0.00001022
Iteration 63/1000 | Loss: 0.00001021
Iteration 64/1000 | Loss: 0.00001021
Iteration 65/1000 | Loss: 0.00001021
Iteration 66/1000 | Loss: 0.00001021
Iteration 67/1000 | Loss: 0.00001020
Iteration 68/1000 | Loss: 0.00001020
Iteration 69/1000 | Loss: 0.00001017
Iteration 70/1000 | Loss: 0.00001016
Iteration 71/1000 | Loss: 0.00001015
Iteration 72/1000 | Loss: 0.00001015
Iteration 73/1000 | Loss: 0.00001015
Iteration 74/1000 | Loss: 0.00001015
Iteration 75/1000 | Loss: 0.00001014
Iteration 76/1000 | Loss: 0.00001014
Iteration 77/1000 | Loss: 0.00001014
Iteration 78/1000 | Loss: 0.00001014
Iteration 79/1000 | Loss: 0.00001014
Iteration 80/1000 | Loss: 0.00001014
Iteration 81/1000 | Loss: 0.00001013
Iteration 82/1000 | Loss: 0.00001013
Iteration 83/1000 | Loss: 0.00001013
Iteration 84/1000 | Loss: 0.00001012
Iteration 85/1000 | Loss: 0.00001012
Iteration 86/1000 | Loss: 0.00001012
Iteration 87/1000 | Loss: 0.00001012
Iteration 88/1000 | Loss: 0.00001011
Iteration 89/1000 | Loss: 0.00001011
Iteration 90/1000 | Loss: 0.00001011
Iteration 91/1000 | Loss: 0.00001010
Iteration 92/1000 | Loss: 0.00001010
Iteration 93/1000 | Loss: 0.00001010
Iteration 94/1000 | Loss: 0.00001010
Iteration 95/1000 | Loss: 0.00001010
Iteration 96/1000 | Loss: 0.00001010
Iteration 97/1000 | Loss: 0.00001009
Iteration 98/1000 | Loss: 0.00001009
Iteration 99/1000 | Loss: 0.00001009
Iteration 100/1000 | Loss: 0.00001009
Iteration 101/1000 | Loss: 0.00001008
Iteration 102/1000 | Loss: 0.00001008
Iteration 103/1000 | Loss: 0.00001008
Iteration 104/1000 | Loss: 0.00001008
Iteration 105/1000 | Loss: 0.00001008
Iteration 106/1000 | Loss: 0.00001007
Iteration 107/1000 | Loss: 0.00001007
Iteration 108/1000 | Loss: 0.00001007
Iteration 109/1000 | Loss: 0.00001007
Iteration 110/1000 | Loss: 0.00001007
Iteration 111/1000 | Loss: 0.00001007
Iteration 112/1000 | Loss: 0.00001007
Iteration 113/1000 | Loss: 0.00001006
Iteration 114/1000 | Loss: 0.00001006
Iteration 115/1000 | Loss: 0.00001006
Iteration 116/1000 | Loss: 0.00001006
Iteration 117/1000 | Loss: 0.00001006
Iteration 118/1000 | Loss: 0.00001005
Iteration 119/1000 | Loss: 0.00001005
Iteration 120/1000 | Loss: 0.00001005
Iteration 121/1000 | Loss: 0.00001005
Iteration 122/1000 | Loss: 0.00001005
Iteration 123/1000 | Loss: 0.00001005
Iteration 124/1000 | Loss: 0.00001005
Iteration 125/1000 | Loss: 0.00001005
Iteration 126/1000 | Loss: 0.00001005
Iteration 127/1000 | Loss: 0.00001005
Iteration 128/1000 | Loss: 0.00001005
Iteration 129/1000 | Loss: 0.00001005
Iteration 130/1000 | Loss: 0.00001005
Iteration 131/1000 | Loss: 0.00001005
Iteration 132/1000 | Loss: 0.00001005
Iteration 133/1000 | Loss: 0.00001005
Iteration 134/1000 | Loss: 0.00001004
Iteration 135/1000 | Loss: 0.00001004
Iteration 136/1000 | Loss: 0.00001004
Iteration 137/1000 | Loss: 0.00001004
Iteration 138/1000 | Loss: 0.00001004
Iteration 139/1000 | Loss: 0.00001004
Iteration 140/1000 | Loss: 0.00001004
Iteration 141/1000 | Loss: 0.00001004
Iteration 142/1000 | Loss: 0.00001004
Iteration 143/1000 | Loss: 0.00001004
Iteration 144/1000 | Loss: 0.00001004
Iteration 145/1000 | Loss: 0.00001004
Iteration 146/1000 | Loss: 0.00001004
Iteration 147/1000 | Loss: 0.00001004
Iteration 148/1000 | Loss: 0.00001004
Iteration 149/1000 | Loss: 0.00001004
Iteration 150/1000 | Loss: 0.00001004
Iteration 151/1000 | Loss: 0.00001003
Iteration 152/1000 | Loss: 0.00001003
Iteration 153/1000 | Loss: 0.00001003
Iteration 154/1000 | Loss: 0.00001003
Iteration 155/1000 | Loss: 0.00001003
Iteration 156/1000 | Loss: 0.00001003
Iteration 157/1000 | Loss: 0.00001003
Iteration 158/1000 | Loss: 0.00001003
Iteration 159/1000 | Loss: 0.00001003
Iteration 160/1000 | Loss: 0.00001003
Iteration 161/1000 | Loss: 0.00001003
Iteration 162/1000 | Loss: 0.00001003
Iteration 163/1000 | Loss: 0.00001003
Iteration 164/1000 | Loss: 0.00001003
Iteration 165/1000 | Loss: 0.00001003
Iteration 166/1000 | Loss: 0.00001003
Iteration 167/1000 | Loss: 0.00001003
Iteration 168/1000 | Loss: 0.00001003
Iteration 169/1000 | Loss: 0.00001003
Iteration 170/1000 | Loss: 0.00001003
Iteration 171/1000 | Loss: 0.00001003
Iteration 172/1000 | Loss: 0.00001003
Iteration 173/1000 | Loss: 0.00001003
Iteration 174/1000 | Loss: 0.00001003
Iteration 175/1000 | Loss: 0.00001003
Iteration 176/1000 | Loss: 0.00001003
Iteration 177/1000 | Loss: 0.00001003
Iteration 178/1000 | Loss: 0.00001003
Iteration 179/1000 | Loss: 0.00001003
Iteration 180/1000 | Loss: 0.00001003
Iteration 181/1000 | Loss: 0.00001003
Iteration 182/1000 | Loss: 0.00001003
Iteration 183/1000 | Loss: 0.00001003
Iteration 184/1000 | Loss: 0.00001003
Iteration 185/1000 | Loss: 0.00001003
Iteration 186/1000 | Loss: 0.00001003
Iteration 187/1000 | Loss: 0.00001003
Iteration 188/1000 | Loss: 0.00001003
Iteration 189/1000 | Loss: 0.00001003
Iteration 190/1000 | Loss: 0.00001003
Iteration 191/1000 | Loss: 0.00001003
Iteration 192/1000 | Loss: 0.00001003
Iteration 193/1000 | Loss: 0.00001003
Iteration 194/1000 | Loss: 0.00001003
Iteration 195/1000 | Loss: 0.00001003
Iteration 196/1000 | Loss: 0.00001003
Iteration 197/1000 | Loss: 0.00001003
Iteration 198/1000 | Loss: 0.00001003
Iteration 199/1000 | Loss: 0.00001003
Iteration 200/1000 | Loss: 0.00001003
Iteration 201/1000 | Loss: 0.00001003
Iteration 202/1000 | Loss: 0.00001003
Iteration 203/1000 | Loss: 0.00001003
Iteration 204/1000 | Loss: 0.00001003
Iteration 205/1000 | Loss: 0.00001003
Iteration 206/1000 | Loss: 0.00001003
Iteration 207/1000 | Loss: 0.00001003
Iteration 208/1000 | Loss: 0.00001003
Iteration 209/1000 | Loss: 0.00001003
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [1.0030052180809435e-05, 1.0030052180809435e-05, 1.0030052180809435e-05, 1.0030052180809435e-05, 1.0030052180809435e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0030052180809435e-05

Optimization complete. Final v2v error: 2.7185628414154053 mm

Highest mean error: 3.2078120708465576 mm for frame 48

Lowest mean error: 2.4718916416168213 mm for frame 37

Saving results

Total time: 92.75950789451599
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00954911
Iteration 2/25 | Loss: 0.00298511
Iteration 3/25 | Loss: 0.00220307
Iteration 4/25 | Loss: 0.00190545
Iteration 5/25 | Loss: 0.00173130
Iteration 6/25 | Loss: 0.00163084
Iteration 7/25 | Loss: 0.00149238
Iteration 8/25 | Loss: 0.00147544
Iteration 9/25 | Loss: 0.00146614
Iteration 10/25 | Loss: 0.00145543
Iteration 11/25 | Loss: 0.00145589
Iteration 12/25 | Loss: 0.00145356
Iteration 13/25 | Loss: 0.00145103
Iteration 14/25 | Loss: 0.00145059
Iteration 15/25 | Loss: 0.00145047
Iteration 16/25 | Loss: 0.00145039
Iteration 17/25 | Loss: 0.00145038
Iteration 18/25 | Loss: 0.00145038
Iteration 19/25 | Loss: 0.00145038
Iteration 20/25 | Loss: 0.00145038
Iteration 21/25 | Loss: 0.00145038
Iteration 22/25 | Loss: 0.00145038
Iteration 23/25 | Loss: 0.00145038
Iteration 24/25 | Loss: 0.00145038
Iteration 25/25 | Loss: 0.00145038

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26432645
Iteration 2/25 | Loss: 0.00157767
Iteration 3/25 | Loss: 0.00165080
Iteration 4/25 | Loss: 0.00157765
Iteration 5/25 | Loss: 0.00157764
Iteration 6/25 | Loss: 0.00157764
Iteration 7/25 | Loss: 0.00157764
Iteration 8/25 | Loss: 0.00157764
Iteration 9/25 | Loss: 0.00157764
Iteration 10/25 | Loss: 0.00157764
Iteration 11/25 | Loss: 0.00157764
Iteration 12/25 | Loss: 0.00157764
Iteration 13/25 | Loss: 0.00157764
Iteration 14/25 | Loss: 0.00157764
Iteration 15/25 | Loss: 0.00157764
Iteration 16/25 | Loss: 0.00157764
Iteration 17/25 | Loss: 0.00157764
Iteration 18/25 | Loss: 0.00157764
Iteration 19/25 | Loss: 0.00157764
Iteration 20/25 | Loss: 0.00157764
Iteration 21/25 | Loss: 0.00157764
Iteration 22/25 | Loss: 0.00157764
Iteration 23/25 | Loss: 0.00157764
Iteration 24/25 | Loss: 0.00157764
Iteration 25/25 | Loss: 0.00157764

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00157764
Iteration 2/1000 | Loss: 0.00014519
Iteration 3/1000 | Loss: 0.00010174
Iteration 4/1000 | Loss: 0.00015066
Iteration 5/1000 | Loss: 0.00004568
Iteration 6/1000 | Loss: 0.00004245
Iteration 7/1000 | Loss: 0.00009543
Iteration 8/1000 | Loss: 0.00004045
Iteration 9/1000 | Loss: 0.00003912
Iteration 10/1000 | Loss: 0.00003877
Iteration 11/1000 | Loss: 0.00008609
Iteration 12/1000 | Loss: 0.00003766
Iteration 13/1000 | Loss: 0.00003727
Iteration 14/1000 | Loss: 0.00003689
Iteration 15/1000 | Loss: 0.00003664
Iteration 16/1000 | Loss: 0.00009023
Iteration 17/1000 | Loss: 0.00003645
Iteration 18/1000 | Loss: 0.00007614
Iteration 19/1000 | Loss: 0.00003618
Iteration 20/1000 | Loss: 0.00003615
Iteration 21/1000 | Loss: 0.00003950
Iteration 22/1000 | Loss: 0.00003597
Iteration 23/1000 | Loss: 0.00003595
Iteration 24/1000 | Loss: 0.00003586
Iteration 25/1000 | Loss: 0.00003586
Iteration 26/1000 | Loss: 0.00003585
Iteration 27/1000 | Loss: 0.00003585
Iteration 28/1000 | Loss: 0.00003585
Iteration 29/1000 | Loss: 0.00003585
Iteration 30/1000 | Loss: 0.00003585
Iteration 31/1000 | Loss: 0.00003585
Iteration 32/1000 | Loss: 0.00003585
Iteration 33/1000 | Loss: 0.00003585
Iteration 34/1000 | Loss: 0.00003585
Iteration 35/1000 | Loss: 0.00003585
Iteration 36/1000 | Loss: 0.00003585
Iteration 37/1000 | Loss: 0.00003584
Iteration 38/1000 | Loss: 0.00003926
Iteration 39/1000 | Loss: 0.00003575
Iteration 40/1000 | Loss: 0.00003575
Iteration 41/1000 | Loss: 0.00003572
Iteration 42/1000 | Loss: 0.00003572
Iteration 43/1000 | Loss: 0.00003572
Iteration 44/1000 | Loss: 0.00003572
Iteration 45/1000 | Loss: 0.00003572
Iteration 46/1000 | Loss: 0.00003572
Iteration 47/1000 | Loss: 0.00003572
Iteration 48/1000 | Loss: 0.00003572
Iteration 49/1000 | Loss: 0.00003572
Iteration 50/1000 | Loss: 0.00003572
Iteration 51/1000 | Loss: 0.00003572
Iteration 52/1000 | Loss: 0.00003571
Iteration 53/1000 | Loss: 0.00003571
Iteration 54/1000 | Loss: 0.00003571
Iteration 55/1000 | Loss: 0.00003569
Iteration 56/1000 | Loss: 0.00003569
Iteration 57/1000 | Loss: 0.00003569
Iteration 58/1000 | Loss: 0.00003569
Iteration 59/1000 | Loss: 0.00003569
Iteration 60/1000 | Loss: 0.00003569
Iteration 61/1000 | Loss: 0.00003569
Iteration 62/1000 | Loss: 0.00003569
Iteration 63/1000 | Loss: 0.00003568
Iteration 64/1000 | Loss: 0.00003568
Iteration 65/1000 | Loss: 0.00003566
Iteration 66/1000 | Loss: 0.00003560
Iteration 67/1000 | Loss: 0.00003932
Iteration 68/1000 | Loss: 0.00003557
Iteration 69/1000 | Loss: 0.00003557
Iteration 70/1000 | Loss: 0.00003557
Iteration 71/1000 | Loss: 0.00003557
Iteration 72/1000 | Loss: 0.00003557
Iteration 73/1000 | Loss: 0.00003556
Iteration 74/1000 | Loss: 0.00003556
Iteration 75/1000 | Loss: 0.00003556
Iteration 76/1000 | Loss: 0.00003556
Iteration 77/1000 | Loss: 0.00003555
Iteration 78/1000 | Loss: 0.00003555
Iteration 79/1000 | Loss: 0.00003554
Iteration 80/1000 | Loss: 0.00003554
Iteration 81/1000 | Loss: 0.00003554
Iteration 82/1000 | Loss: 0.00003554
Iteration 83/1000 | Loss: 0.00003554
Iteration 84/1000 | Loss: 0.00003554
Iteration 85/1000 | Loss: 0.00003554
Iteration 86/1000 | Loss: 0.00003554
Iteration 87/1000 | Loss: 0.00003553
Iteration 88/1000 | Loss: 0.00003553
Iteration 89/1000 | Loss: 0.00003552
Iteration 90/1000 | Loss: 0.00003552
Iteration 91/1000 | Loss: 0.00003754
Iteration 92/1000 | Loss: 0.00003548
Iteration 93/1000 | Loss: 0.00003548
Iteration 94/1000 | Loss: 0.00003548
Iteration 95/1000 | Loss: 0.00003547
Iteration 96/1000 | Loss: 0.00003547
Iteration 97/1000 | Loss: 0.00003547
Iteration 98/1000 | Loss: 0.00003547
Iteration 99/1000 | Loss: 0.00003547
Iteration 100/1000 | Loss: 0.00003547
Iteration 101/1000 | Loss: 0.00003547
Iteration 102/1000 | Loss: 0.00003547
Iteration 103/1000 | Loss: 0.00003547
Iteration 104/1000 | Loss: 0.00003547
Iteration 105/1000 | Loss: 0.00003547
Iteration 106/1000 | Loss: 0.00003547
Iteration 107/1000 | Loss: 0.00003547
Iteration 108/1000 | Loss: 0.00003547
Iteration 109/1000 | Loss: 0.00003547
Iteration 110/1000 | Loss: 0.00003547
Iteration 111/1000 | Loss: 0.00003547
Iteration 112/1000 | Loss: 0.00003547
Iteration 113/1000 | Loss: 0.00003547
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [3.546965672285296e-05, 3.546965672285296e-05, 3.546965672285296e-05, 3.546965672285296e-05, 3.546965672285296e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.546965672285296e-05

Optimization complete. Final v2v error: 4.7965874671936035 mm

Highest mean error: 12.132147789001465 mm for frame 80

Lowest mean error: 3.603139877319336 mm for frame 155

Saving results

Total time: 77.89890789985657
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00503719
Iteration 2/25 | Loss: 0.00131784
Iteration 3/25 | Loss: 0.00124446
Iteration 4/25 | Loss: 0.00123121
Iteration 5/25 | Loss: 0.00122652
Iteration 6/25 | Loss: 0.00122549
Iteration 7/25 | Loss: 0.00122549
Iteration 8/25 | Loss: 0.00122549
Iteration 9/25 | Loss: 0.00122549
Iteration 10/25 | Loss: 0.00122549
Iteration 11/25 | Loss: 0.00122549
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012254876783117652, 0.0012254876783117652, 0.0012254876783117652, 0.0012254876783117652, 0.0012254876783117652]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012254876783117652

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30767488
Iteration 2/25 | Loss: 0.00137652
Iteration 3/25 | Loss: 0.00137649
Iteration 4/25 | Loss: 0.00137649
Iteration 5/25 | Loss: 0.00137649
Iteration 6/25 | Loss: 0.00137649
Iteration 7/25 | Loss: 0.00137649
Iteration 8/25 | Loss: 0.00137649
Iteration 9/25 | Loss: 0.00137649
Iteration 10/25 | Loss: 0.00137649
Iteration 11/25 | Loss: 0.00137649
Iteration 12/25 | Loss: 0.00137649
Iteration 13/25 | Loss: 0.00137649
Iteration 14/25 | Loss: 0.00137649
Iteration 15/25 | Loss: 0.00137649
Iteration 16/25 | Loss: 0.00137649
Iteration 17/25 | Loss: 0.00137649
Iteration 18/25 | Loss: 0.00137649
Iteration 19/25 | Loss: 0.00137649
Iteration 20/25 | Loss: 0.00137649
Iteration 21/25 | Loss: 0.00137649
Iteration 22/25 | Loss: 0.00137649
Iteration 23/25 | Loss: 0.00137649
Iteration 24/25 | Loss: 0.00137649
Iteration 25/25 | Loss: 0.00137649

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137649
Iteration 2/1000 | Loss: 0.00003210
Iteration 3/1000 | Loss: 0.00002195
Iteration 4/1000 | Loss: 0.00001873
Iteration 5/1000 | Loss: 0.00001782
Iteration 6/1000 | Loss: 0.00001706
Iteration 7/1000 | Loss: 0.00001655
Iteration 8/1000 | Loss: 0.00001618
Iteration 9/1000 | Loss: 0.00001589
Iteration 10/1000 | Loss: 0.00001568
Iteration 11/1000 | Loss: 0.00001545
Iteration 12/1000 | Loss: 0.00001527
Iteration 13/1000 | Loss: 0.00001519
Iteration 14/1000 | Loss: 0.00001519
Iteration 15/1000 | Loss: 0.00001517
Iteration 16/1000 | Loss: 0.00001512
Iteration 17/1000 | Loss: 0.00001511
Iteration 18/1000 | Loss: 0.00001510
Iteration 19/1000 | Loss: 0.00001509
Iteration 20/1000 | Loss: 0.00001508
Iteration 21/1000 | Loss: 0.00001508
Iteration 22/1000 | Loss: 0.00001503
Iteration 23/1000 | Loss: 0.00001495
Iteration 24/1000 | Loss: 0.00001489
Iteration 25/1000 | Loss: 0.00001488
Iteration 26/1000 | Loss: 0.00001486
Iteration 27/1000 | Loss: 0.00001486
Iteration 28/1000 | Loss: 0.00001484
Iteration 29/1000 | Loss: 0.00001484
Iteration 30/1000 | Loss: 0.00001481
Iteration 31/1000 | Loss: 0.00001481
Iteration 32/1000 | Loss: 0.00001480
Iteration 33/1000 | Loss: 0.00001479
Iteration 34/1000 | Loss: 0.00001479
Iteration 35/1000 | Loss: 0.00001479
Iteration 36/1000 | Loss: 0.00001478
Iteration 37/1000 | Loss: 0.00001473
Iteration 38/1000 | Loss: 0.00001473
Iteration 39/1000 | Loss: 0.00001471
Iteration 40/1000 | Loss: 0.00001471
Iteration 41/1000 | Loss: 0.00001470
Iteration 42/1000 | Loss: 0.00001470
Iteration 43/1000 | Loss: 0.00001469
Iteration 44/1000 | Loss: 0.00001469
Iteration 45/1000 | Loss: 0.00001468
Iteration 46/1000 | Loss: 0.00001468
Iteration 47/1000 | Loss: 0.00001467
Iteration 48/1000 | Loss: 0.00001467
Iteration 49/1000 | Loss: 0.00001467
Iteration 50/1000 | Loss: 0.00001467
Iteration 51/1000 | Loss: 0.00001466
Iteration 52/1000 | Loss: 0.00001466
Iteration 53/1000 | Loss: 0.00001465
Iteration 54/1000 | Loss: 0.00001465
Iteration 55/1000 | Loss: 0.00001465
Iteration 56/1000 | Loss: 0.00001464
Iteration 57/1000 | Loss: 0.00001464
Iteration 58/1000 | Loss: 0.00001463
Iteration 59/1000 | Loss: 0.00001463
Iteration 60/1000 | Loss: 0.00001463
Iteration 61/1000 | Loss: 0.00001462
Iteration 62/1000 | Loss: 0.00001462
Iteration 63/1000 | Loss: 0.00001461
Iteration 64/1000 | Loss: 0.00001461
Iteration 65/1000 | Loss: 0.00001461
Iteration 66/1000 | Loss: 0.00001460
Iteration 67/1000 | Loss: 0.00001460
Iteration 68/1000 | Loss: 0.00001460
Iteration 69/1000 | Loss: 0.00001460
Iteration 70/1000 | Loss: 0.00001459
Iteration 71/1000 | Loss: 0.00001459
Iteration 72/1000 | Loss: 0.00001459
Iteration 73/1000 | Loss: 0.00001459
Iteration 74/1000 | Loss: 0.00001459
Iteration 75/1000 | Loss: 0.00001458
Iteration 76/1000 | Loss: 0.00001458
Iteration 77/1000 | Loss: 0.00001458
Iteration 78/1000 | Loss: 0.00001458
Iteration 79/1000 | Loss: 0.00001458
Iteration 80/1000 | Loss: 0.00001458
Iteration 81/1000 | Loss: 0.00001457
Iteration 82/1000 | Loss: 0.00001457
Iteration 83/1000 | Loss: 0.00001457
Iteration 84/1000 | Loss: 0.00001457
Iteration 85/1000 | Loss: 0.00001457
Iteration 86/1000 | Loss: 0.00001457
Iteration 87/1000 | Loss: 0.00001456
Iteration 88/1000 | Loss: 0.00001456
Iteration 89/1000 | Loss: 0.00001456
Iteration 90/1000 | Loss: 0.00001456
Iteration 91/1000 | Loss: 0.00001456
Iteration 92/1000 | Loss: 0.00001455
Iteration 93/1000 | Loss: 0.00001455
Iteration 94/1000 | Loss: 0.00001455
Iteration 95/1000 | Loss: 0.00001455
Iteration 96/1000 | Loss: 0.00001455
Iteration 97/1000 | Loss: 0.00001455
Iteration 98/1000 | Loss: 0.00001455
Iteration 99/1000 | Loss: 0.00001454
Iteration 100/1000 | Loss: 0.00001454
Iteration 101/1000 | Loss: 0.00001454
Iteration 102/1000 | Loss: 0.00001454
Iteration 103/1000 | Loss: 0.00001453
Iteration 104/1000 | Loss: 0.00001453
Iteration 105/1000 | Loss: 0.00001453
Iteration 106/1000 | Loss: 0.00001453
Iteration 107/1000 | Loss: 0.00001453
Iteration 108/1000 | Loss: 0.00001453
Iteration 109/1000 | Loss: 0.00001453
Iteration 110/1000 | Loss: 0.00001453
Iteration 111/1000 | Loss: 0.00001453
Iteration 112/1000 | Loss: 0.00001452
Iteration 113/1000 | Loss: 0.00001452
Iteration 114/1000 | Loss: 0.00001452
Iteration 115/1000 | Loss: 0.00001452
Iteration 116/1000 | Loss: 0.00001451
Iteration 117/1000 | Loss: 0.00001451
Iteration 118/1000 | Loss: 0.00001451
Iteration 119/1000 | Loss: 0.00001451
Iteration 120/1000 | Loss: 0.00001451
Iteration 121/1000 | Loss: 0.00001451
Iteration 122/1000 | Loss: 0.00001451
Iteration 123/1000 | Loss: 0.00001450
Iteration 124/1000 | Loss: 0.00001450
Iteration 125/1000 | Loss: 0.00001450
Iteration 126/1000 | Loss: 0.00001450
Iteration 127/1000 | Loss: 0.00001450
Iteration 128/1000 | Loss: 0.00001450
Iteration 129/1000 | Loss: 0.00001450
Iteration 130/1000 | Loss: 0.00001450
Iteration 131/1000 | Loss: 0.00001449
Iteration 132/1000 | Loss: 0.00001449
Iteration 133/1000 | Loss: 0.00001449
Iteration 134/1000 | Loss: 0.00001449
Iteration 135/1000 | Loss: 0.00001449
Iteration 136/1000 | Loss: 0.00001449
Iteration 137/1000 | Loss: 0.00001448
Iteration 138/1000 | Loss: 0.00001448
Iteration 139/1000 | Loss: 0.00001448
Iteration 140/1000 | Loss: 0.00001448
Iteration 141/1000 | Loss: 0.00001448
Iteration 142/1000 | Loss: 0.00001448
Iteration 143/1000 | Loss: 0.00001448
Iteration 144/1000 | Loss: 0.00001448
Iteration 145/1000 | Loss: 0.00001447
Iteration 146/1000 | Loss: 0.00001447
Iteration 147/1000 | Loss: 0.00001447
Iteration 148/1000 | Loss: 0.00001446
Iteration 149/1000 | Loss: 0.00001446
Iteration 150/1000 | Loss: 0.00001446
Iteration 151/1000 | Loss: 0.00001446
Iteration 152/1000 | Loss: 0.00001446
Iteration 153/1000 | Loss: 0.00001446
Iteration 154/1000 | Loss: 0.00001446
Iteration 155/1000 | Loss: 0.00001446
Iteration 156/1000 | Loss: 0.00001446
Iteration 157/1000 | Loss: 0.00001446
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.4461928913078737e-05, 1.4461928913078737e-05, 1.4461928913078737e-05, 1.4461928913078737e-05, 1.4461928913078737e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4461928913078737e-05

Optimization complete. Final v2v error: 3.1754167079925537 mm

Highest mean error: 3.9643194675445557 mm for frame 155

Lowest mean error: 2.742304563522339 mm for frame 53

Saving results

Total time: 47.22826933860779
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aneko_posed_011/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aneko_posed_011/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01035419
Iteration 2/25 | Loss: 0.01035418
Iteration 3/25 | Loss: 0.01035418
Iteration 4/25 | Loss: 0.01035418
Iteration 5/25 | Loss: 0.01035418
Iteration 6/25 | Loss: 0.01035418
Iteration 7/25 | Loss: 0.01035418
Iteration 8/25 | Loss: 0.01035418
Iteration 9/25 | Loss: 0.01035418
Iteration 10/25 | Loss: 0.01035418
Iteration 11/25 | Loss: 0.01035417
Iteration 12/25 | Loss: 0.01035417
Iteration 13/25 | Loss: 0.01035417
Iteration 14/25 | Loss: 0.01035417
Iteration 15/25 | Loss: 0.01035417
Iteration 16/25 | Loss: 0.01035417
Iteration 17/25 | Loss: 0.01035417
Iteration 18/25 | Loss: 0.01035416
Iteration 19/25 | Loss: 0.01035416
Iteration 20/25 | Loss: 0.01035416
Iteration 21/25 | Loss: 0.01035416
Iteration 22/25 | Loss: 0.01035416
Iteration 23/25 | Loss: 0.01035416
Iteration 24/25 | Loss: 0.01035416
Iteration 25/25 | Loss: 0.01035416

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25714111
Iteration 2/25 | Loss: 0.18392067
Iteration 3/25 | Loss: 0.17843957
Iteration 4/25 | Loss: 0.17794845
Iteration 5/25 | Loss: 0.17794842
Iteration 6/25 | Loss: 0.17794842
Iteration 7/25 | Loss: 0.17794837
Iteration 8/25 | Loss: 0.17794840
Iteration 9/25 | Loss: 0.17794837
Iteration 10/25 | Loss: 0.17794837
Iteration 11/25 | Loss: 0.17794840
Iteration 12/25 | Loss: 0.17794840
Iteration 13/25 | Loss: 0.17794840
Iteration 14/25 | Loss: 0.17794840
Iteration 15/25 | Loss: 0.17794837
Iteration 16/25 | Loss: 0.17794837
Iteration 17/25 | Loss: 0.17794837
Iteration 18/25 | Loss: 0.17794837
Iteration 19/25 | Loss: 0.17794837
Iteration 20/25 | Loss: 0.17794837
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.17794837057590485, 0.17794837057590485, 0.17794837057590485, 0.17794837057590485, 0.17794837057590485]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.17794837057590485

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17794837
Iteration 2/1000 | Loss: 0.00284665
Iteration 3/1000 | Loss: 0.00074846
Iteration 4/1000 | Loss: 0.00167504
Iteration 5/1000 | Loss: 0.00010158
Iteration 6/1000 | Loss: 0.00005900
Iteration 7/1000 | Loss: 0.00003560
Iteration 8/1000 | Loss: 0.00002986
Iteration 9/1000 | Loss: 0.00002631
Iteration 10/1000 | Loss: 0.00002423
Iteration 11/1000 | Loss: 0.00002240
Iteration 12/1000 | Loss: 0.00026407
Iteration 13/1000 | Loss: 0.00003630
Iteration 14/1000 | Loss: 0.00002608
Iteration 15/1000 | Loss: 0.00002069
Iteration 16/1000 | Loss: 0.00001958
Iteration 17/1000 | Loss: 0.00049147
Iteration 18/1000 | Loss: 0.00047119
Iteration 19/1000 | Loss: 0.00004818
Iteration 20/1000 | Loss: 0.00003985
Iteration 21/1000 | Loss: 0.00047829
Iteration 22/1000 | Loss: 0.00024660
Iteration 23/1000 | Loss: 0.00047731
Iteration 24/1000 | Loss: 0.00011825
Iteration 25/1000 | Loss: 0.00003908
Iteration 26/1000 | Loss: 0.00019776
Iteration 27/1000 | Loss: 0.00025385
Iteration 28/1000 | Loss: 0.00003833
Iteration 29/1000 | Loss: 0.00001825
Iteration 30/1000 | Loss: 0.00001758
Iteration 31/1000 | Loss: 0.00014088
Iteration 32/1000 | Loss: 0.00002024
Iteration 33/1000 | Loss: 0.00002333
Iteration 34/1000 | Loss: 0.00001696
Iteration 35/1000 | Loss: 0.00001650
Iteration 36/1000 | Loss: 0.00001604
Iteration 37/1000 | Loss: 0.00001571
Iteration 38/1000 | Loss: 0.00001539
Iteration 39/1000 | Loss: 0.00024688
Iteration 40/1000 | Loss: 0.00006217
Iteration 41/1000 | Loss: 0.00006845
Iteration 42/1000 | Loss: 0.00001808
Iteration 43/1000 | Loss: 0.00001522
Iteration 44/1000 | Loss: 0.00001487
Iteration 45/1000 | Loss: 0.00001471
Iteration 46/1000 | Loss: 0.00001471
Iteration 47/1000 | Loss: 0.00001470
Iteration 48/1000 | Loss: 0.00001470
Iteration 49/1000 | Loss: 0.00001466
Iteration 50/1000 | Loss: 0.00001466
Iteration 51/1000 | Loss: 0.00001466
Iteration 52/1000 | Loss: 0.00001465
Iteration 53/1000 | Loss: 0.00001465
Iteration 54/1000 | Loss: 0.00001465
Iteration 55/1000 | Loss: 0.00001464
Iteration 56/1000 | Loss: 0.00001464
Iteration 57/1000 | Loss: 0.00001464
Iteration 58/1000 | Loss: 0.00001464
Iteration 59/1000 | Loss: 0.00001463
Iteration 60/1000 | Loss: 0.00001463
Iteration 61/1000 | Loss: 0.00001463
Iteration 62/1000 | Loss: 0.00001462
Iteration 63/1000 | Loss: 0.00001462
Iteration 64/1000 | Loss: 0.00001462
Iteration 65/1000 | Loss: 0.00001462
Iteration 66/1000 | Loss: 0.00001462
Iteration 67/1000 | Loss: 0.00001462
Iteration 68/1000 | Loss: 0.00001461
Iteration 69/1000 | Loss: 0.00001461
Iteration 70/1000 | Loss: 0.00001461
Iteration 71/1000 | Loss: 0.00001461
Iteration 72/1000 | Loss: 0.00001461
Iteration 73/1000 | Loss: 0.00001461
Iteration 74/1000 | Loss: 0.00001460
Iteration 75/1000 | Loss: 0.00001460
Iteration 76/1000 | Loss: 0.00001460
Iteration 77/1000 | Loss: 0.00001460
Iteration 78/1000 | Loss: 0.00001460
Iteration 79/1000 | Loss: 0.00001460
Iteration 80/1000 | Loss: 0.00001460
Iteration 81/1000 | Loss: 0.00001460
Iteration 82/1000 | Loss: 0.00001460
Iteration 83/1000 | Loss: 0.00001460
Iteration 84/1000 | Loss: 0.00001460
Iteration 85/1000 | Loss: 0.00001460
Iteration 86/1000 | Loss: 0.00001460
Iteration 87/1000 | Loss: 0.00001460
Iteration 88/1000 | Loss: 0.00001459
Iteration 89/1000 | Loss: 0.00001459
Iteration 90/1000 | Loss: 0.00001459
Iteration 91/1000 | Loss: 0.00001459
Iteration 92/1000 | Loss: 0.00001459
Iteration 93/1000 | Loss: 0.00001458
Iteration 94/1000 | Loss: 0.00001458
Iteration 95/1000 | Loss: 0.00001458
Iteration 96/1000 | Loss: 0.00001458
Iteration 97/1000 | Loss: 0.00001458
Iteration 98/1000 | Loss: 0.00001458
Iteration 99/1000 | Loss: 0.00001458
Iteration 100/1000 | Loss: 0.00001458
Iteration 101/1000 | Loss: 0.00001457
Iteration 102/1000 | Loss: 0.00001457
Iteration 103/1000 | Loss: 0.00001457
Iteration 104/1000 | Loss: 0.00001457
Iteration 105/1000 | Loss: 0.00001457
Iteration 106/1000 | Loss: 0.00001457
Iteration 107/1000 | Loss: 0.00001457
Iteration 108/1000 | Loss: 0.00001457
Iteration 109/1000 | Loss: 0.00001457
Iteration 110/1000 | Loss: 0.00001457
Iteration 111/1000 | Loss: 0.00001457
Iteration 112/1000 | Loss: 0.00001456
Iteration 113/1000 | Loss: 0.00001456
Iteration 114/1000 | Loss: 0.00001455
Iteration 115/1000 | Loss: 0.00001455
Iteration 116/1000 | Loss: 0.00001454
Iteration 117/1000 | Loss: 0.00001454
Iteration 118/1000 | Loss: 0.00001454
Iteration 119/1000 | Loss: 0.00001454
Iteration 120/1000 | Loss: 0.00001454
Iteration 121/1000 | Loss: 0.00001454
Iteration 122/1000 | Loss: 0.00001454
Iteration 123/1000 | Loss: 0.00001454
Iteration 124/1000 | Loss: 0.00001454
Iteration 125/1000 | Loss: 0.00001453
Iteration 126/1000 | Loss: 0.00001453
Iteration 127/1000 | Loss: 0.00001453
Iteration 128/1000 | Loss: 0.00001452
Iteration 129/1000 | Loss: 0.00001452
Iteration 130/1000 | Loss: 0.00001451
Iteration 131/1000 | Loss: 0.00001450
Iteration 132/1000 | Loss: 0.00001449
Iteration 133/1000 | Loss: 0.00001448
Iteration 134/1000 | Loss: 0.00001448
Iteration 135/1000 | Loss: 0.00001448
Iteration 136/1000 | Loss: 0.00001448
Iteration 137/1000 | Loss: 0.00001448
Iteration 138/1000 | Loss: 0.00001448
Iteration 139/1000 | Loss: 0.00001448
Iteration 140/1000 | Loss: 0.00001448
Iteration 141/1000 | Loss: 0.00001447
Iteration 142/1000 | Loss: 0.00001447
Iteration 143/1000 | Loss: 0.00001447
Iteration 144/1000 | Loss: 0.00001446
Iteration 145/1000 | Loss: 0.00001446
Iteration 146/1000 | Loss: 0.00001445
Iteration 147/1000 | Loss: 0.00001445
Iteration 148/1000 | Loss: 0.00001444
Iteration 149/1000 | Loss: 0.00001444
Iteration 150/1000 | Loss: 0.00001444
Iteration 151/1000 | Loss: 0.00001444
Iteration 152/1000 | Loss: 0.00001444
Iteration 153/1000 | Loss: 0.00001444
Iteration 154/1000 | Loss: 0.00001444
Iteration 155/1000 | Loss: 0.00001444
Iteration 156/1000 | Loss: 0.00001443
Iteration 157/1000 | Loss: 0.00001443
Iteration 158/1000 | Loss: 0.00001443
Iteration 159/1000 | Loss: 0.00001442
Iteration 160/1000 | Loss: 0.00001442
Iteration 161/1000 | Loss: 0.00001442
Iteration 162/1000 | Loss: 0.00001442
Iteration 163/1000 | Loss: 0.00001441
Iteration 164/1000 | Loss: 0.00001441
Iteration 165/1000 | Loss: 0.00001441
Iteration 166/1000 | Loss: 0.00001441
Iteration 167/1000 | Loss: 0.00001441
Iteration 168/1000 | Loss: 0.00001441
Iteration 169/1000 | Loss: 0.00001441
Iteration 170/1000 | Loss: 0.00001441
Iteration 171/1000 | Loss: 0.00001441
Iteration 172/1000 | Loss: 0.00001441
Iteration 173/1000 | Loss: 0.00001441
Iteration 174/1000 | Loss: 0.00001441
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [1.4409227333089802e-05, 1.4409227333089802e-05, 1.4409227333089802e-05, 1.4409227333089802e-05, 1.4409227333089802e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4409227333089802e-05

Optimization complete. Final v2v error: 3.28426194190979 mm

Highest mean error: 3.4713633060455322 mm for frame 7

Lowest mean error: 3.1183700561523438 mm for frame 212

Saving results

Total time: 90.15087604522705
