Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=282, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 15792-15847
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00415784
Iteration 2/25 | Loss: 0.00092737
Iteration 3/25 | Loss: 0.00068581
Iteration 4/25 | Loss: 0.00066838
Iteration 5/25 | Loss: 0.00066387
Iteration 6/25 | Loss: 0.00066257
Iteration 7/25 | Loss: 0.00066242
Iteration 8/25 | Loss: 0.00066242
Iteration 9/25 | Loss: 0.00066242
Iteration 10/25 | Loss: 0.00066242
Iteration 11/25 | Loss: 0.00066242
Iteration 12/25 | Loss: 0.00066242
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006624222151003778, 0.0006624222151003778, 0.0006624222151003778, 0.0006624222151003778, 0.0006624222151003778]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006624222151003778

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.25629759
Iteration 2/25 | Loss: 0.00032680
Iteration 3/25 | Loss: 0.00032678
Iteration 4/25 | Loss: 0.00032678
Iteration 5/25 | Loss: 0.00032678
Iteration 6/25 | Loss: 0.00032678
Iteration 7/25 | Loss: 0.00032678
Iteration 8/25 | Loss: 0.00032678
Iteration 9/25 | Loss: 0.00032678
Iteration 10/25 | Loss: 0.00032678
Iteration 11/25 | Loss: 0.00032678
Iteration 12/25 | Loss: 0.00032678
Iteration 13/25 | Loss: 0.00032678
Iteration 14/25 | Loss: 0.00032678
Iteration 15/25 | Loss: 0.00032678
Iteration 16/25 | Loss: 0.00032678
Iteration 17/25 | Loss: 0.00032678
Iteration 18/25 | Loss: 0.00032678
Iteration 19/25 | Loss: 0.00032678
Iteration 20/25 | Loss: 0.00032678
Iteration 21/25 | Loss: 0.00032678
Iteration 22/25 | Loss: 0.00032678
Iteration 23/25 | Loss: 0.00032678
Iteration 24/25 | Loss: 0.00032678
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0003267773427069187, 0.0003267773427069187, 0.0003267773427069187, 0.0003267773427069187, 0.0003267773427069187]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003267773427069187

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032678
Iteration 2/1000 | Loss: 0.00003121
Iteration 3/1000 | Loss: 0.00002186
Iteration 4/1000 | Loss: 0.00002045
Iteration 5/1000 | Loss: 0.00001931
Iteration 6/1000 | Loss: 0.00001864
Iteration 7/1000 | Loss: 0.00001813
Iteration 8/1000 | Loss: 0.00001778
Iteration 9/1000 | Loss: 0.00001757
Iteration 10/1000 | Loss: 0.00001754
Iteration 11/1000 | Loss: 0.00001751
Iteration 12/1000 | Loss: 0.00001750
Iteration 13/1000 | Loss: 0.00001749
Iteration 14/1000 | Loss: 0.00001735
Iteration 15/1000 | Loss: 0.00001734
Iteration 16/1000 | Loss: 0.00001730
Iteration 17/1000 | Loss: 0.00001728
Iteration 18/1000 | Loss: 0.00001728
Iteration 19/1000 | Loss: 0.00001728
Iteration 20/1000 | Loss: 0.00001726
Iteration 21/1000 | Loss: 0.00001726
Iteration 22/1000 | Loss: 0.00001725
Iteration 23/1000 | Loss: 0.00001725
Iteration 24/1000 | Loss: 0.00001722
Iteration 25/1000 | Loss: 0.00001722
Iteration 26/1000 | Loss: 0.00001721
Iteration 27/1000 | Loss: 0.00001720
Iteration 28/1000 | Loss: 0.00001719
Iteration 29/1000 | Loss: 0.00001719
Iteration 30/1000 | Loss: 0.00001719
Iteration 31/1000 | Loss: 0.00001718
Iteration 32/1000 | Loss: 0.00001718
Iteration 33/1000 | Loss: 0.00001718
Iteration 34/1000 | Loss: 0.00001718
Iteration 35/1000 | Loss: 0.00001718
Iteration 36/1000 | Loss: 0.00001718
Iteration 37/1000 | Loss: 0.00001718
Iteration 38/1000 | Loss: 0.00001718
Iteration 39/1000 | Loss: 0.00001717
Iteration 40/1000 | Loss: 0.00001717
Iteration 41/1000 | Loss: 0.00001717
Iteration 42/1000 | Loss: 0.00001717
Iteration 43/1000 | Loss: 0.00001717
Iteration 44/1000 | Loss: 0.00001716
Iteration 45/1000 | Loss: 0.00001716
Iteration 46/1000 | Loss: 0.00001716
Iteration 47/1000 | Loss: 0.00001715
Iteration 48/1000 | Loss: 0.00001715
Iteration 49/1000 | Loss: 0.00001715
Iteration 50/1000 | Loss: 0.00001714
Iteration 51/1000 | Loss: 0.00001714
Iteration 52/1000 | Loss: 0.00001714
Iteration 53/1000 | Loss: 0.00001714
Iteration 54/1000 | Loss: 0.00001713
Iteration 55/1000 | Loss: 0.00001713
Iteration 56/1000 | Loss: 0.00001713
Iteration 57/1000 | Loss: 0.00001712
Iteration 58/1000 | Loss: 0.00001712
Iteration 59/1000 | Loss: 0.00001712
Iteration 60/1000 | Loss: 0.00001712
Iteration 61/1000 | Loss: 0.00001712
Iteration 62/1000 | Loss: 0.00001712
Iteration 63/1000 | Loss: 0.00001711
Iteration 64/1000 | Loss: 0.00001711
Iteration 65/1000 | Loss: 0.00001711
Iteration 66/1000 | Loss: 0.00001711
Iteration 67/1000 | Loss: 0.00001711
Iteration 68/1000 | Loss: 0.00001710
Iteration 69/1000 | Loss: 0.00001710
Iteration 70/1000 | Loss: 0.00001710
Iteration 71/1000 | Loss: 0.00001710
Iteration 72/1000 | Loss: 0.00001710
Iteration 73/1000 | Loss: 0.00001710
Iteration 74/1000 | Loss: 0.00001710
Iteration 75/1000 | Loss: 0.00001710
Iteration 76/1000 | Loss: 0.00001710
Iteration 77/1000 | Loss: 0.00001710
Iteration 78/1000 | Loss: 0.00001710
Iteration 79/1000 | Loss: 0.00001710
Iteration 80/1000 | Loss: 0.00001710
Iteration 81/1000 | Loss: 0.00001710
Iteration 82/1000 | Loss: 0.00001710
Iteration 83/1000 | Loss: 0.00001710
Iteration 84/1000 | Loss: 0.00001710
Iteration 85/1000 | Loss: 0.00001710
Iteration 86/1000 | Loss: 0.00001710
Iteration 87/1000 | Loss: 0.00001710
Iteration 88/1000 | Loss: 0.00001710
Iteration 89/1000 | Loss: 0.00001710
Iteration 90/1000 | Loss: 0.00001710
Iteration 91/1000 | Loss: 0.00001710
Iteration 92/1000 | Loss: 0.00001710
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [1.7095311704906635e-05, 1.7095311704906635e-05, 1.7095311704906635e-05, 1.7095311704906635e-05, 1.7095311704906635e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7095311704906635e-05

Optimization complete. Final v2v error: 3.5164577960968018 mm

Highest mean error: 3.9403209686279297 mm for frame 72

Lowest mean error: 3.2605342864990234 mm for frame 104

Saving results

Total time: 31.78419017791748
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00585957
Iteration 2/25 | Loss: 0.00115238
Iteration 3/25 | Loss: 0.00075463
Iteration 4/25 | Loss: 0.00066754
Iteration 5/25 | Loss: 0.00062861
Iteration 6/25 | Loss: 0.00062336
Iteration 7/25 | Loss: 0.00062467
Iteration 8/25 | Loss: 0.00062321
Iteration 9/25 | Loss: 0.00062079
Iteration 10/25 | Loss: 0.00061990
Iteration 11/25 | Loss: 0.00061889
Iteration 12/25 | Loss: 0.00061922
Iteration 13/25 | Loss: 0.00061861
Iteration 14/25 | Loss: 0.00061780
Iteration 15/25 | Loss: 0.00061804
Iteration 16/25 | Loss: 0.00061761
Iteration 17/25 | Loss: 0.00061791
Iteration 18/25 | Loss: 0.00061774
Iteration 19/25 | Loss: 0.00061804
Iteration 20/25 | Loss: 0.00061787
Iteration 21/25 | Loss: 0.00061807
Iteration 22/25 | Loss: 0.00061790
Iteration 23/25 | Loss: 0.00061810
Iteration 24/25 | Loss: 0.00061785
Iteration 25/25 | Loss: 0.00061803

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.61647892
Iteration 2/25 | Loss: 0.00031063
Iteration 3/25 | Loss: 0.00031059
Iteration 4/25 | Loss: 0.00031058
Iteration 5/25 | Loss: 0.00031058
Iteration 6/25 | Loss: 0.00031058
Iteration 7/25 | Loss: 0.00031058
Iteration 8/25 | Loss: 0.00031058
Iteration 9/25 | Loss: 0.00031058
Iteration 10/25 | Loss: 0.00031058
Iteration 11/25 | Loss: 0.00031058
Iteration 12/25 | Loss: 0.00031058
Iteration 13/25 | Loss: 0.00031058
Iteration 14/25 | Loss: 0.00031058
Iteration 15/25 | Loss: 0.00031058
Iteration 16/25 | Loss: 0.00031058
Iteration 17/25 | Loss: 0.00031058
Iteration 18/25 | Loss: 0.00031058
Iteration 19/25 | Loss: 0.00031058
Iteration 20/25 | Loss: 0.00031058
Iteration 21/25 | Loss: 0.00031058
Iteration 22/25 | Loss: 0.00031058
Iteration 23/25 | Loss: 0.00031058
Iteration 24/25 | Loss: 0.00031058
Iteration 25/25 | Loss: 0.00031058

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031058
Iteration 2/1000 | Loss: 0.00002011
Iteration 3/1000 | Loss: 0.00005423
Iteration 4/1000 | Loss: 0.00001409
Iteration 5/1000 | Loss: 0.00001332
Iteration 6/1000 | Loss: 0.00006089
Iteration 7/1000 | Loss: 0.00001262
Iteration 8/1000 | Loss: 0.00008206
Iteration 9/1000 | Loss: 0.00013420
Iteration 10/1000 | Loss: 0.00001774
Iteration 11/1000 | Loss: 0.00001528
Iteration 12/1000 | Loss: 0.00001254
Iteration 13/1000 | Loss: 0.00003330
Iteration 14/1000 | Loss: 0.00001390
Iteration 15/1000 | Loss: 0.00001341
Iteration 16/1000 | Loss: 0.00001204
Iteration 17/1000 | Loss: 0.00001204
Iteration 18/1000 | Loss: 0.00001204
Iteration 19/1000 | Loss: 0.00001204
Iteration 20/1000 | Loss: 0.00001203
Iteration 21/1000 | Loss: 0.00001203
Iteration 22/1000 | Loss: 0.00001203
Iteration 23/1000 | Loss: 0.00001203
Iteration 24/1000 | Loss: 0.00001203
Iteration 25/1000 | Loss: 0.00001203
Iteration 26/1000 | Loss: 0.00001203
Iteration 27/1000 | Loss: 0.00001202
Iteration 28/1000 | Loss: 0.00001202
Iteration 29/1000 | Loss: 0.00001201
Iteration 30/1000 | Loss: 0.00001201
Iteration 31/1000 | Loss: 0.00001201
Iteration 32/1000 | Loss: 0.00001201
Iteration 33/1000 | Loss: 0.00001200
Iteration 34/1000 | Loss: 0.00001200
Iteration 35/1000 | Loss: 0.00001189
Iteration 36/1000 | Loss: 0.00001188
Iteration 37/1000 | Loss: 0.00001188
Iteration 38/1000 | Loss: 0.00001339
Iteration 39/1000 | Loss: 0.00001214
Iteration 40/1000 | Loss: 0.00001187
Iteration 41/1000 | Loss: 0.00001182
Iteration 42/1000 | Loss: 0.00001182
Iteration 43/1000 | Loss: 0.00001181
Iteration 44/1000 | Loss: 0.00001181
Iteration 45/1000 | Loss: 0.00001180
Iteration 46/1000 | Loss: 0.00001180
Iteration 47/1000 | Loss: 0.00001179
Iteration 48/1000 | Loss: 0.00001179
Iteration 49/1000 | Loss: 0.00001179
Iteration 50/1000 | Loss: 0.00001178
Iteration 51/1000 | Loss: 0.00001178
Iteration 52/1000 | Loss: 0.00001177
Iteration 53/1000 | Loss: 0.00001177
Iteration 54/1000 | Loss: 0.00001177
Iteration 55/1000 | Loss: 0.00001177
Iteration 56/1000 | Loss: 0.00001177
Iteration 57/1000 | Loss: 0.00001177
Iteration 58/1000 | Loss: 0.00001177
Iteration 59/1000 | Loss: 0.00001176
Iteration 60/1000 | Loss: 0.00001176
Iteration 61/1000 | Loss: 0.00001176
Iteration 62/1000 | Loss: 0.00001176
Iteration 63/1000 | Loss: 0.00001175
Iteration 64/1000 | Loss: 0.00001175
Iteration 65/1000 | Loss: 0.00001285
Iteration 66/1000 | Loss: 0.00001221
Iteration 67/1000 | Loss: 0.00001266
Iteration 68/1000 | Loss: 0.00001238
Iteration 69/1000 | Loss: 0.00001263
Iteration 70/1000 | Loss: 0.00001252
Iteration 71/1000 | Loss: 0.00001262
Iteration 72/1000 | Loss: 0.00001243
Iteration 73/1000 | Loss: 0.00001261
Iteration 74/1000 | Loss: 0.00001238
Iteration 75/1000 | Loss: 0.00001176
Iteration 76/1000 | Loss: 0.00001271
Iteration 77/1000 | Loss: 0.00001271
Iteration 78/1000 | Loss: 0.00001228
Iteration 79/1000 | Loss: 0.00001170
Iteration 80/1000 | Loss: 0.00001169
Iteration 81/1000 | Loss: 0.00001169
Iteration 82/1000 | Loss: 0.00001168
Iteration 83/1000 | Loss: 0.00001168
Iteration 84/1000 | Loss: 0.00001167
Iteration 85/1000 | Loss: 0.00001167
Iteration 86/1000 | Loss: 0.00001167
Iteration 87/1000 | Loss: 0.00001167
Iteration 88/1000 | Loss: 0.00001167
Iteration 89/1000 | Loss: 0.00001167
Iteration 90/1000 | Loss: 0.00001167
Iteration 91/1000 | Loss: 0.00001167
Iteration 92/1000 | Loss: 0.00001167
Iteration 93/1000 | Loss: 0.00001167
Iteration 94/1000 | Loss: 0.00001167
Iteration 95/1000 | Loss: 0.00001167
Iteration 96/1000 | Loss: 0.00001167
Iteration 97/1000 | Loss: 0.00001167
Iteration 98/1000 | Loss: 0.00001167
Iteration 99/1000 | Loss: 0.00001167
Iteration 100/1000 | Loss: 0.00001167
Iteration 101/1000 | Loss: 0.00001167
Iteration 102/1000 | Loss: 0.00001167
Iteration 103/1000 | Loss: 0.00001167
Iteration 104/1000 | Loss: 0.00001167
Iteration 105/1000 | Loss: 0.00001167
Iteration 106/1000 | Loss: 0.00001167
Iteration 107/1000 | Loss: 0.00001167
Iteration 108/1000 | Loss: 0.00001167
Iteration 109/1000 | Loss: 0.00001167
Iteration 110/1000 | Loss: 0.00001167
Iteration 111/1000 | Loss: 0.00001167
Iteration 112/1000 | Loss: 0.00001167
Iteration 113/1000 | Loss: 0.00001167
Iteration 114/1000 | Loss: 0.00001167
Iteration 115/1000 | Loss: 0.00001167
Iteration 116/1000 | Loss: 0.00001167
Iteration 117/1000 | Loss: 0.00001167
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [1.1666519640129991e-05, 1.1666519640129991e-05, 1.1666519640129991e-05, 1.1666519640129991e-05, 1.1666519640129991e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1666519640129991e-05

Optimization complete. Final v2v error: 2.8722825050354004 mm

Highest mean error: 8.645381927490234 mm for frame 218

Lowest mean error: 2.464508056640625 mm for frame 113

Saving results

Total time: 100.3081386089325
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01071058
Iteration 2/25 | Loss: 0.00396036
Iteration 3/25 | Loss: 0.00242390
Iteration 4/25 | Loss: 0.00146172
Iteration 5/25 | Loss: 0.00113510
Iteration 6/25 | Loss: 0.00102960
Iteration 7/25 | Loss: 0.00096231
Iteration 8/25 | Loss: 0.00089077
Iteration 9/25 | Loss: 0.00084666
Iteration 10/25 | Loss: 0.00083623
Iteration 11/25 | Loss: 0.00083040
Iteration 12/25 | Loss: 0.00082341
Iteration 13/25 | Loss: 0.00081909
Iteration 14/25 | Loss: 0.00082015
Iteration 15/25 | Loss: 0.00081645
Iteration 16/25 | Loss: 0.00082138
Iteration 17/25 | Loss: 0.00081936
Iteration 18/25 | Loss: 0.00081584
Iteration 19/25 | Loss: 0.00080912
Iteration 20/25 | Loss: 0.00080925
Iteration 21/25 | Loss: 0.00080876
Iteration 22/25 | Loss: 0.00081103
Iteration 23/25 | Loss: 0.00080272
Iteration 24/25 | Loss: 0.00080389
Iteration 25/25 | Loss: 0.00080714

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47243392
Iteration 2/25 | Loss: 0.00126579
Iteration 3/25 | Loss: 0.00126579
Iteration 4/25 | Loss: 0.00126579
Iteration 5/25 | Loss: 0.00126579
Iteration 6/25 | Loss: 0.00126579
Iteration 7/25 | Loss: 0.00126579
Iteration 8/25 | Loss: 0.00126579
Iteration 9/25 | Loss: 0.00126579
Iteration 10/25 | Loss: 0.00126579
Iteration 11/25 | Loss: 0.00126579
Iteration 12/25 | Loss: 0.00126579
Iteration 13/25 | Loss: 0.00126579
Iteration 14/25 | Loss: 0.00126579
Iteration 15/25 | Loss: 0.00126579
Iteration 16/25 | Loss: 0.00126578
Iteration 17/25 | Loss: 0.00126579
Iteration 18/25 | Loss: 0.00126578
Iteration 19/25 | Loss: 0.00126578
Iteration 20/25 | Loss: 0.00126578
Iteration 21/25 | Loss: 0.00126578
Iteration 22/25 | Loss: 0.00126578
Iteration 23/25 | Loss: 0.00126578
Iteration 24/25 | Loss: 0.00126578
Iteration 25/25 | Loss: 0.00126578

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126578
Iteration 2/1000 | Loss: 0.00027425
Iteration 3/1000 | Loss: 0.00036156
Iteration 4/1000 | Loss: 0.00058515
Iteration 5/1000 | Loss: 0.00050640
Iteration 6/1000 | Loss: 0.00055772
Iteration 7/1000 | Loss: 0.00027460
Iteration 8/1000 | Loss: 0.00044326
Iteration 9/1000 | Loss: 0.00030117
Iteration 10/1000 | Loss: 0.00034533
Iteration 11/1000 | Loss: 0.00018760
Iteration 12/1000 | Loss: 0.00069670
Iteration 13/1000 | Loss: 0.00018937
Iteration 14/1000 | Loss: 0.00016442
Iteration 15/1000 | Loss: 0.00025984
Iteration 16/1000 | Loss: 0.00036398
Iteration 17/1000 | Loss: 0.00026489
Iteration 18/1000 | Loss: 0.00019928
Iteration 19/1000 | Loss: 0.00028976
Iteration 20/1000 | Loss: 0.00032678
Iteration 21/1000 | Loss: 0.00011472
Iteration 22/1000 | Loss: 0.00034764
Iteration 23/1000 | Loss: 0.00037250
Iteration 24/1000 | Loss: 0.00020184
Iteration 25/1000 | Loss: 0.00017365
Iteration 26/1000 | Loss: 0.00023463
Iteration 27/1000 | Loss: 0.00023364
Iteration 28/1000 | Loss: 0.00032011
Iteration 29/1000 | Loss: 0.00030797
Iteration 30/1000 | Loss: 0.00031948
Iteration 31/1000 | Loss: 0.00036103
Iteration 32/1000 | Loss: 0.00080667
Iteration 33/1000 | Loss: 0.00030988
Iteration 34/1000 | Loss: 0.00030657
Iteration 35/1000 | Loss: 0.00038130
Iteration 36/1000 | Loss: 0.00033012
Iteration 37/1000 | Loss: 0.00045021
Iteration 38/1000 | Loss: 0.00041617
Iteration 39/1000 | Loss: 0.00042710
Iteration 40/1000 | Loss: 0.00035927
Iteration 41/1000 | Loss: 0.00026921
Iteration 42/1000 | Loss: 0.00024775
Iteration 43/1000 | Loss: 0.00026228
Iteration 44/1000 | Loss: 0.00023780
Iteration 45/1000 | Loss: 0.00017699
Iteration 46/1000 | Loss: 0.00018886
Iteration 47/1000 | Loss: 0.00026562
Iteration 48/1000 | Loss: 0.00029631
Iteration 49/1000 | Loss: 0.00026902
Iteration 50/1000 | Loss: 0.00034396
Iteration 51/1000 | Loss: 0.00018313
Iteration 52/1000 | Loss: 0.00030129
Iteration 53/1000 | Loss: 0.00079730
Iteration 54/1000 | Loss: 0.00044955
Iteration 55/1000 | Loss: 0.00029762
Iteration 56/1000 | Loss: 0.00031420
Iteration 57/1000 | Loss: 0.00027583
Iteration 58/1000 | Loss: 0.00021670
Iteration 59/1000 | Loss: 0.00056703
Iteration 60/1000 | Loss: 0.00026515
Iteration 61/1000 | Loss: 0.00041932
Iteration 62/1000 | Loss: 0.00012287
Iteration 63/1000 | Loss: 0.00041759
Iteration 64/1000 | Loss: 0.00025620
Iteration 65/1000 | Loss: 0.00008701
Iteration 66/1000 | Loss: 0.00036661
Iteration 67/1000 | Loss: 0.00040855
Iteration 68/1000 | Loss: 0.00031639
Iteration 69/1000 | Loss: 0.00023978
Iteration 70/1000 | Loss: 0.00006797
Iteration 71/1000 | Loss: 0.00033858
Iteration 72/1000 | Loss: 0.00046512
Iteration 73/1000 | Loss: 0.00033951
Iteration 74/1000 | Loss: 0.00038234
Iteration 75/1000 | Loss: 0.00101932
Iteration 76/1000 | Loss: 0.00032810
Iteration 77/1000 | Loss: 0.00033774
Iteration 78/1000 | Loss: 0.00051505
Iteration 79/1000 | Loss: 0.00025003
Iteration 80/1000 | Loss: 0.00014388
Iteration 81/1000 | Loss: 0.00021013
Iteration 82/1000 | Loss: 0.00065588
Iteration 83/1000 | Loss: 0.00110597
Iteration 84/1000 | Loss: 0.00071535
Iteration 85/1000 | Loss: 0.00025028
Iteration 86/1000 | Loss: 0.00039757
Iteration 87/1000 | Loss: 0.00010312
Iteration 88/1000 | Loss: 0.00006187
Iteration 89/1000 | Loss: 0.00005644
Iteration 90/1000 | Loss: 0.00007149
Iteration 91/1000 | Loss: 0.00005267
Iteration 92/1000 | Loss: 0.00005073
Iteration 93/1000 | Loss: 0.00004949
Iteration 94/1000 | Loss: 0.00004861
Iteration 95/1000 | Loss: 0.00004755
Iteration 96/1000 | Loss: 0.00004662
Iteration 97/1000 | Loss: 0.00004585
Iteration 98/1000 | Loss: 0.00004532
Iteration 99/1000 | Loss: 0.00004490
Iteration 100/1000 | Loss: 0.00004461
Iteration 101/1000 | Loss: 0.00004437
Iteration 102/1000 | Loss: 0.00004421
Iteration 103/1000 | Loss: 0.00004400
Iteration 104/1000 | Loss: 0.00004378
Iteration 105/1000 | Loss: 0.00004371
Iteration 106/1000 | Loss: 0.00004366
Iteration 107/1000 | Loss: 0.00062707
Iteration 108/1000 | Loss: 0.00005770
Iteration 109/1000 | Loss: 0.00004720
Iteration 110/1000 | Loss: 0.00004319
Iteration 111/1000 | Loss: 0.00004216
Iteration 112/1000 | Loss: 0.00004129
Iteration 113/1000 | Loss: 0.00004074
Iteration 114/1000 | Loss: 0.00004066
Iteration 115/1000 | Loss: 0.00004051
Iteration 116/1000 | Loss: 0.00004047
Iteration 117/1000 | Loss: 0.00004046
Iteration 118/1000 | Loss: 0.00004044
Iteration 119/1000 | Loss: 0.00004038
Iteration 120/1000 | Loss: 0.00004037
Iteration 121/1000 | Loss: 0.00004035
Iteration 122/1000 | Loss: 0.00004032
Iteration 123/1000 | Loss: 0.00004018
Iteration 124/1000 | Loss: 0.00004017
Iteration 125/1000 | Loss: 0.00004016
Iteration 126/1000 | Loss: 0.00004014
Iteration 127/1000 | Loss: 0.00004014
Iteration 128/1000 | Loss: 0.00004014
Iteration 129/1000 | Loss: 0.00004014
Iteration 130/1000 | Loss: 0.00004013
Iteration 131/1000 | Loss: 0.00004013
Iteration 132/1000 | Loss: 0.00004010
Iteration 133/1000 | Loss: 0.00004008
Iteration 134/1000 | Loss: 0.00004003
Iteration 135/1000 | Loss: 0.00004002
Iteration 136/1000 | Loss: 0.00004001
Iteration 137/1000 | Loss: 0.00004000
Iteration 138/1000 | Loss: 0.00004000
Iteration 139/1000 | Loss: 0.00003999
Iteration 140/1000 | Loss: 0.00003999
Iteration 141/1000 | Loss: 0.00003998
Iteration 142/1000 | Loss: 0.00003998
Iteration 143/1000 | Loss: 0.00003997
Iteration 144/1000 | Loss: 0.00003997
Iteration 145/1000 | Loss: 0.00003997
Iteration 146/1000 | Loss: 0.00003996
Iteration 147/1000 | Loss: 0.00003996
Iteration 148/1000 | Loss: 0.00003995
Iteration 149/1000 | Loss: 0.00003983
Iteration 150/1000 | Loss: 0.00003977
Iteration 151/1000 | Loss: 0.00003977
Iteration 152/1000 | Loss: 0.00003977
Iteration 153/1000 | Loss: 0.00003976
Iteration 154/1000 | Loss: 0.00003976
Iteration 155/1000 | Loss: 0.00003965
Iteration 156/1000 | Loss: 0.00003961
Iteration 157/1000 | Loss: 0.00003961
Iteration 158/1000 | Loss: 0.00003961
Iteration 159/1000 | Loss: 0.00003961
Iteration 160/1000 | Loss: 0.00003961
Iteration 161/1000 | Loss: 0.00003961
Iteration 162/1000 | Loss: 0.00003961
Iteration 163/1000 | Loss: 0.00003961
Iteration 164/1000 | Loss: 0.00003961
Iteration 165/1000 | Loss: 0.00003961
Iteration 166/1000 | Loss: 0.00003960
Iteration 167/1000 | Loss: 0.00003960
Iteration 168/1000 | Loss: 0.00003960
Iteration 169/1000 | Loss: 0.00003960
Iteration 170/1000 | Loss: 0.00003960
Iteration 171/1000 | Loss: 0.00003959
Iteration 172/1000 | Loss: 0.00003959
Iteration 173/1000 | Loss: 0.00003959
Iteration 174/1000 | Loss: 0.00003959
Iteration 175/1000 | Loss: 0.00003959
Iteration 176/1000 | Loss: 0.00003959
Iteration 177/1000 | Loss: 0.00003959
Iteration 178/1000 | Loss: 0.00003958
Iteration 179/1000 | Loss: 0.00003958
Iteration 180/1000 | Loss: 0.00003958
Iteration 181/1000 | Loss: 0.00003958
Iteration 182/1000 | Loss: 0.00003958
Iteration 183/1000 | Loss: 0.00003958
Iteration 184/1000 | Loss: 0.00003958
Iteration 185/1000 | Loss: 0.00003958
Iteration 186/1000 | Loss: 0.00003957
Iteration 187/1000 | Loss: 0.00003957
Iteration 188/1000 | Loss: 0.00003957
Iteration 189/1000 | Loss: 0.00003957
Iteration 190/1000 | Loss: 0.00003957
Iteration 191/1000 | Loss: 0.00003957
Iteration 192/1000 | Loss: 0.00003957
Iteration 193/1000 | Loss: 0.00003957
Iteration 194/1000 | Loss: 0.00003957
Iteration 195/1000 | Loss: 0.00003957
Iteration 196/1000 | Loss: 0.00003957
Iteration 197/1000 | Loss: 0.00003957
Iteration 198/1000 | Loss: 0.00003957
Iteration 199/1000 | Loss: 0.00003957
Iteration 200/1000 | Loss: 0.00003957
Iteration 201/1000 | Loss: 0.00003957
Iteration 202/1000 | Loss: 0.00003956
Iteration 203/1000 | Loss: 0.00003956
Iteration 204/1000 | Loss: 0.00003956
Iteration 205/1000 | Loss: 0.00003956
Iteration 206/1000 | Loss: 0.00003956
Iteration 207/1000 | Loss: 0.00003956
Iteration 208/1000 | Loss: 0.00003956
Iteration 209/1000 | Loss: 0.00003956
Iteration 210/1000 | Loss: 0.00003956
Iteration 211/1000 | Loss: 0.00003956
Iteration 212/1000 | Loss: 0.00003956
Iteration 213/1000 | Loss: 0.00003956
Iteration 214/1000 | Loss: 0.00003956
Iteration 215/1000 | Loss: 0.00003956
Iteration 216/1000 | Loss: 0.00003956
Iteration 217/1000 | Loss: 0.00003956
Iteration 218/1000 | Loss: 0.00003956
Iteration 219/1000 | Loss: 0.00003956
Iteration 220/1000 | Loss: 0.00003956
Iteration 221/1000 | Loss: 0.00003955
Iteration 222/1000 | Loss: 0.00003955
Iteration 223/1000 | Loss: 0.00003955
Iteration 224/1000 | Loss: 0.00003955
Iteration 225/1000 | Loss: 0.00003955
Iteration 226/1000 | Loss: 0.00003955
Iteration 227/1000 | Loss: 0.00003955
Iteration 228/1000 | Loss: 0.00003955
Iteration 229/1000 | Loss: 0.00003955
Iteration 230/1000 | Loss: 0.00003955
Iteration 231/1000 | Loss: 0.00003955
Iteration 232/1000 | Loss: 0.00003955
Iteration 233/1000 | Loss: 0.00003955
Iteration 234/1000 | Loss: 0.00003955
Iteration 235/1000 | Loss: 0.00003955
Iteration 236/1000 | Loss: 0.00003955
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 236. Stopping optimization.
Last 5 losses: [3.9549715438624844e-05, 3.9549715438624844e-05, 3.9549715438624844e-05, 3.9549715438624844e-05, 3.9549715438624844e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.9549715438624844e-05

Optimization complete. Final v2v error: 4.359030246734619 mm

Highest mean error: 12.084619522094727 mm for frame 150

Lowest mean error: 3.1013731956481934 mm for frame 10

Saving results

Total time: 224.28541231155396
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00786405
Iteration 2/25 | Loss: 0.00150112
Iteration 3/25 | Loss: 0.00099012
Iteration 4/25 | Loss: 0.00088434
Iteration 5/25 | Loss: 0.00084646
Iteration 6/25 | Loss: 0.00083706
Iteration 7/25 | Loss: 0.00083435
Iteration 8/25 | Loss: 0.00083383
Iteration 9/25 | Loss: 0.00083383
Iteration 10/25 | Loss: 0.00083383
Iteration 11/25 | Loss: 0.00083383
Iteration 12/25 | Loss: 0.00083383
Iteration 13/25 | Loss: 0.00083383
Iteration 14/25 | Loss: 0.00083383
Iteration 15/25 | Loss: 0.00083383
Iteration 16/25 | Loss: 0.00083383
Iteration 17/25 | Loss: 0.00083383
Iteration 18/25 | Loss: 0.00083383
Iteration 19/25 | Loss: 0.00083383
Iteration 20/25 | Loss: 0.00083383
Iteration 21/25 | Loss: 0.00083383
Iteration 22/25 | Loss: 0.00083383
Iteration 23/25 | Loss: 0.00083383
Iteration 24/25 | Loss: 0.00083383
Iteration 25/25 | Loss: 0.00083383

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24911308
Iteration 2/25 | Loss: 0.00038866
Iteration 3/25 | Loss: 0.00038864
Iteration 4/25 | Loss: 0.00038864
Iteration 5/25 | Loss: 0.00038864
Iteration 6/25 | Loss: 0.00038864
Iteration 7/25 | Loss: 0.00038864
Iteration 8/25 | Loss: 0.00038864
Iteration 9/25 | Loss: 0.00038864
Iteration 10/25 | Loss: 0.00038864
Iteration 11/25 | Loss: 0.00038864
Iteration 12/25 | Loss: 0.00038864
Iteration 13/25 | Loss: 0.00038864
Iteration 14/25 | Loss: 0.00038864
Iteration 15/25 | Loss: 0.00038864
Iteration 16/25 | Loss: 0.00038864
Iteration 17/25 | Loss: 0.00038864
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00038863817462697625, 0.00038863817462697625, 0.00038863817462697625, 0.00038863817462697625, 0.00038863817462697625]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00038863817462697625

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038864
Iteration 2/1000 | Loss: 0.00006973
Iteration 3/1000 | Loss: 0.00004621
Iteration 4/1000 | Loss: 0.00004062
Iteration 5/1000 | Loss: 0.00003798
Iteration 6/1000 | Loss: 0.00003628
Iteration 7/1000 | Loss: 0.00003524
Iteration 8/1000 | Loss: 0.00003449
Iteration 9/1000 | Loss: 0.00003371
Iteration 10/1000 | Loss: 0.00003327
Iteration 11/1000 | Loss: 0.00003289
Iteration 12/1000 | Loss: 0.00003271
Iteration 13/1000 | Loss: 0.00003254
Iteration 14/1000 | Loss: 0.00003238
Iteration 15/1000 | Loss: 0.00003230
Iteration 16/1000 | Loss: 0.00003216
Iteration 17/1000 | Loss: 0.00003207
Iteration 18/1000 | Loss: 0.00003206
Iteration 19/1000 | Loss: 0.00003205
Iteration 20/1000 | Loss: 0.00003205
Iteration 21/1000 | Loss: 0.00003204
Iteration 22/1000 | Loss: 0.00003201
Iteration 23/1000 | Loss: 0.00003200
Iteration 24/1000 | Loss: 0.00003198
Iteration 25/1000 | Loss: 0.00003197
Iteration 26/1000 | Loss: 0.00003197
Iteration 27/1000 | Loss: 0.00003196
Iteration 28/1000 | Loss: 0.00003196
Iteration 29/1000 | Loss: 0.00003195
Iteration 30/1000 | Loss: 0.00003194
Iteration 31/1000 | Loss: 0.00003194
Iteration 32/1000 | Loss: 0.00003194
Iteration 33/1000 | Loss: 0.00003194
Iteration 34/1000 | Loss: 0.00003193
Iteration 35/1000 | Loss: 0.00003193
Iteration 36/1000 | Loss: 0.00003193
Iteration 37/1000 | Loss: 0.00003192
Iteration 38/1000 | Loss: 0.00003192
Iteration 39/1000 | Loss: 0.00003191
Iteration 40/1000 | Loss: 0.00003191
Iteration 41/1000 | Loss: 0.00003190
Iteration 42/1000 | Loss: 0.00003190
Iteration 43/1000 | Loss: 0.00003189
Iteration 44/1000 | Loss: 0.00003189
Iteration 45/1000 | Loss: 0.00003189
Iteration 46/1000 | Loss: 0.00003188
Iteration 47/1000 | Loss: 0.00003188
Iteration 48/1000 | Loss: 0.00003188
Iteration 49/1000 | Loss: 0.00003187
Iteration 50/1000 | Loss: 0.00003187
Iteration 51/1000 | Loss: 0.00003187
Iteration 52/1000 | Loss: 0.00003186
Iteration 53/1000 | Loss: 0.00003186
Iteration 54/1000 | Loss: 0.00003185
Iteration 55/1000 | Loss: 0.00003185
Iteration 56/1000 | Loss: 0.00003185
Iteration 57/1000 | Loss: 0.00003185
Iteration 58/1000 | Loss: 0.00003184
Iteration 59/1000 | Loss: 0.00003184
Iteration 60/1000 | Loss: 0.00003184
Iteration 61/1000 | Loss: 0.00003183
Iteration 62/1000 | Loss: 0.00003183
Iteration 63/1000 | Loss: 0.00003183
Iteration 64/1000 | Loss: 0.00003182
Iteration 65/1000 | Loss: 0.00003182
Iteration 66/1000 | Loss: 0.00003182
Iteration 67/1000 | Loss: 0.00003182
Iteration 68/1000 | Loss: 0.00003181
Iteration 69/1000 | Loss: 0.00003181
Iteration 70/1000 | Loss: 0.00003181
Iteration 71/1000 | Loss: 0.00003181
Iteration 72/1000 | Loss: 0.00003181
Iteration 73/1000 | Loss: 0.00003180
Iteration 74/1000 | Loss: 0.00003180
Iteration 75/1000 | Loss: 0.00003180
Iteration 76/1000 | Loss: 0.00003180
Iteration 77/1000 | Loss: 0.00003180
Iteration 78/1000 | Loss: 0.00003179
Iteration 79/1000 | Loss: 0.00003179
Iteration 80/1000 | Loss: 0.00003179
Iteration 81/1000 | Loss: 0.00003178
Iteration 82/1000 | Loss: 0.00003178
Iteration 83/1000 | Loss: 0.00003178
Iteration 84/1000 | Loss: 0.00003178
Iteration 85/1000 | Loss: 0.00003178
Iteration 86/1000 | Loss: 0.00003178
Iteration 87/1000 | Loss: 0.00003178
Iteration 88/1000 | Loss: 0.00003178
Iteration 89/1000 | Loss: 0.00003178
Iteration 90/1000 | Loss: 0.00003178
Iteration 91/1000 | Loss: 0.00003178
Iteration 92/1000 | Loss: 0.00003177
Iteration 93/1000 | Loss: 0.00003177
Iteration 94/1000 | Loss: 0.00003177
Iteration 95/1000 | Loss: 0.00003177
Iteration 96/1000 | Loss: 0.00003177
Iteration 97/1000 | Loss: 0.00003177
Iteration 98/1000 | Loss: 0.00003177
Iteration 99/1000 | Loss: 0.00003177
Iteration 100/1000 | Loss: 0.00003177
Iteration 101/1000 | Loss: 0.00003176
Iteration 102/1000 | Loss: 0.00003176
Iteration 103/1000 | Loss: 0.00003176
Iteration 104/1000 | Loss: 0.00003176
Iteration 105/1000 | Loss: 0.00003176
Iteration 106/1000 | Loss: 0.00003176
Iteration 107/1000 | Loss: 0.00003176
Iteration 108/1000 | Loss: 0.00003176
Iteration 109/1000 | Loss: 0.00003176
Iteration 110/1000 | Loss: 0.00003175
Iteration 111/1000 | Loss: 0.00003175
Iteration 112/1000 | Loss: 0.00003175
Iteration 113/1000 | Loss: 0.00003175
Iteration 114/1000 | Loss: 0.00003175
Iteration 115/1000 | Loss: 0.00003175
Iteration 116/1000 | Loss: 0.00003175
Iteration 117/1000 | Loss: 0.00003175
Iteration 118/1000 | Loss: 0.00003175
Iteration 119/1000 | Loss: 0.00003175
Iteration 120/1000 | Loss: 0.00003175
Iteration 121/1000 | Loss: 0.00003175
Iteration 122/1000 | Loss: 0.00003175
Iteration 123/1000 | Loss: 0.00003174
Iteration 124/1000 | Loss: 0.00003174
Iteration 125/1000 | Loss: 0.00003174
Iteration 126/1000 | Loss: 0.00003174
Iteration 127/1000 | Loss: 0.00003174
Iteration 128/1000 | Loss: 0.00003173
Iteration 129/1000 | Loss: 0.00003173
Iteration 130/1000 | Loss: 0.00003173
Iteration 131/1000 | Loss: 0.00003173
Iteration 132/1000 | Loss: 0.00003173
Iteration 133/1000 | Loss: 0.00003173
Iteration 134/1000 | Loss: 0.00003172
Iteration 135/1000 | Loss: 0.00003172
Iteration 136/1000 | Loss: 0.00003172
Iteration 137/1000 | Loss: 0.00003172
Iteration 138/1000 | Loss: 0.00003172
Iteration 139/1000 | Loss: 0.00003172
Iteration 140/1000 | Loss: 0.00003172
Iteration 141/1000 | Loss: 0.00003171
Iteration 142/1000 | Loss: 0.00003171
Iteration 143/1000 | Loss: 0.00003171
Iteration 144/1000 | Loss: 0.00003171
Iteration 145/1000 | Loss: 0.00003171
Iteration 146/1000 | Loss: 0.00003171
Iteration 147/1000 | Loss: 0.00003171
Iteration 148/1000 | Loss: 0.00003170
Iteration 149/1000 | Loss: 0.00003170
Iteration 150/1000 | Loss: 0.00003170
Iteration 151/1000 | Loss: 0.00003170
Iteration 152/1000 | Loss: 0.00003170
Iteration 153/1000 | Loss: 0.00003170
Iteration 154/1000 | Loss: 0.00003170
Iteration 155/1000 | Loss: 0.00003170
Iteration 156/1000 | Loss: 0.00003170
Iteration 157/1000 | Loss: 0.00003169
Iteration 158/1000 | Loss: 0.00003169
Iteration 159/1000 | Loss: 0.00003169
Iteration 160/1000 | Loss: 0.00003169
Iteration 161/1000 | Loss: 0.00003169
Iteration 162/1000 | Loss: 0.00003169
Iteration 163/1000 | Loss: 0.00003169
Iteration 164/1000 | Loss: 0.00003168
Iteration 165/1000 | Loss: 0.00003168
Iteration 166/1000 | Loss: 0.00003168
Iteration 167/1000 | Loss: 0.00003168
Iteration 168/1000 | Loss: 0.00003168
Iteration 169/1000 | Loss: 0.00003168
Iteration 170/1000 | Loss: 0.00003168
Iteration 171/1000 | Loss: 0.00003168
Iteration 172/1000 | Loss: 0.00003168
Iteration 173/1000 | Loss: 0.00003168
Iteration 174/1000 | Loss: 0.00003168
Iteration 175/1000 | Loss: 0.00003168
Iteration 176/1000 | Loss: 0.00003168
Iteration 177/1000 | Loss: 0.00003168
Iteration 178/1000 | Loss: 0.00003168
Iteration 179/1000 | Loss: 0.00003168
Iteration 180/1000 | Loss: 0.00003168
Iteration 181/1000 | Loss: 0.00003168
Iteration 182/1000 | Loss: 0.00003168
Iteration 183/1000 | Loss: 0.00003168
Iteration 184/1000 | Loss: 0.00003168
Iteration 185/1000 | Loss: 0.00003168
Iteration 186/1000 | Loss: 0.00003168
Iteration 187/1000 | Loss: 0.00003168
Iteration 188/1000 | Loss: 0.00003168
Iteration 189/1000 | Loss: 0.00003168
Iteration 190/1000 | Loss: 0.00003168
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [3.167660906910896e-05, 3.167660906910896e-05, 3.167660906910896e-05, 3.167660906910896e-05, 3.167660906910896e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.167660906910896e-05

Optimization complete. Final v2v error: 4.675119876861572 mm

Highest mean error: 5.9589738845825195 mm for frame 74

Lowest mean error: 3.679009199142456 mm for frame 165

Saving results

Total time: 53.80227065086365
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00826958
Iteration 2/25 | Loss: 0.00081223
Iteration 3/25 | Loss: 0.00064170
Iteration 4/25 | Loss: 0.00061524
Iteration 5/25 | Loss: 0.00060835
Iteration 6/25 | Loss: 0.00060583
Iteration 7/25 | Loss: 0.00060499
Iteration 8/25 | Loss: 0.00060498
Iteration 9/25 | Loss: 0.00060498
Iteration 10/25 | Loss: 0.00060498
Iteration 11/25 | Loss: 0.00060498
Iteration 12/25 | Loss: 0.00060498
Iteration 13/25 | Loss: 0.00060498
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006049794610589743, 0.0006049794610589743, 0.0006049794610589743, 0.0006049794610589743, 0.0006049794610589743]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006049794610589743

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46366286
Iteration 2/25 | Loss: 0.00025910
Iteration 3/25 | Loss: 0.00025909
Iteration 4/25 | Loss: 0.00025909
Iteration 5/25 | Loss: 0.00025909
Iteration 6/25 | Loss: 0.00025909
Iteration 7/25 | Loss: 0.00025909
Iteration 8/25 | Loss: 0.00025909
Iteration 9/25 | Loss: 0.00025909
Iteration 10/25 | Loss: 0.00025909
Iteration 11/25 | Loss: 0.00025909
Iteration 12/25 | Loss: 0.00025909
Iteration 13/25 | Loss: 0.00025909
Iteration 14/25 | Loss: 0.00025909
Iteration 15/25 | Loss: 0.00025909
Iteration 16/25 | Loss: 0.00025909
Iteration 17/25 | Loss: 0.00025909
Iteration 18/25 | Loss: 0.00025909
Iteration 19/25 | Loss: 0.00025909
Iteration 20/25 | Loss: 0.00025909
Iteration 21/25 | Loss: 0.00025909
Iteration 22/25 | Loss: 0.00025909
Iteration 23/25 | Loss: 0.00025909
Iteration 24/25 | Loss: 0.00025909
Iteration 25/25 | Loss: 0.00025909

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025909
Iteration 2/1000 | Loss: 0.00002407
Iteration 3/1000 | Loss: 0.00001483
Iteration 4/1000 | Loss: 0.00001360
Iteration 5/1000 | Loss: 0.00001292
Iteration 6/1000 | Loss: 0.00001238
Iteration 7/1000 | Loss: 0.00001216
Iteration 8/1000 | Loss: 0.00001201
Iteration 9/1000 | Loss: 0.00001192
Iteration 10/1000 | Loss: 0.00001191
Iteration 11/1000 | Loss: 0.00001188
Iteration 12/1000 | Loss: 0.00001185
Iteration 13/1000 | Loss: 0.00001184
Iteration 14/1000 | Loss: 0.00001182
Iteration 15/1000 | Loss: 0.00001182
Iteration 16/1000 | Loss: 0.00001174
Iteration 17/1000 | Loss: 0.00001173
Iteration 18/1000 | Loss: 0.00001171
Iteration 19/1000 | Loss: 0.00001170
Iteration 20/1000 | Loss: 0.00001170
Iteration 21/1000 | Loss: 0.00001170
Iteration 22/1000 | Loss: 0.00001169
Iteration 23/1000 | Loss: 0.00001169
Iteration 24/1000 | Loss: 0.00001169
Iteration 25/1000 | Loss: 0.00001169
Iteration 26/1000 | Loss: 0.00001169
Iteration 27/1000 | Loss: 0.00001168
Iteration 28/1000 | Loss: 0.00001167
Iteration 29/1000 | Loss: 0.00001167
Iteration 30/1000 | Loss: 0.00001165
Iteration 31/1000 | Loss: 0.00001165
Iteration 32/1000 | Loss: 0.00001164
Iteration 33/1000 | Loss: 0.00001164
Iteration 34/1000 | Loss: 0.00001164
Iteration 35/1000 | Loss: 0.00001163
Iteration 36/1000 | Loss: 0.00001159
Iteration 37/1000 | Loss: 0.00001159
Iteration 38/1000 | Loss: 0.00001158
Iteration 39/1000 | Loss: 0.00001157
Iteration 40/1000 | Loss: 0.00001157
Iteration 41/1000 | Loss: 0.00001157
Iteration 42/1000 | Loss: 0.00001156
Iteration 43/1000 | Loss: 0.00001156
Iteration 44/1000 | Loss: 0.00001156
Iteration 45/1000 | Loss: 0.00001156
Iteration 46/1000 | Loss: 0.00001155
Iteration 47/1000 | Loss: 0.00001155
Iteration 48/1000 | Loss: 0.00001154
Iteration 49/1000 | Loss: 0.00001154
Iteration 50/1000 | Loss: 0.00001154
Iteration 51/1000 | Loss: 0.00001154
Iteration 52/1000 | Loss: 0.00001153
Iteration 53/1000 | Loss: 0.00001153
Iteration 54/1000 | Loss: 0.00001153
Iteration 55/1000 | Loss: 0.00001153
Iteration 56/1000 | Loss: 0.00001153
Iteration 57/1000 | Loss: 0.00001152
Iteration 58/1000 | Loss: 0.00001152
Iteration 59/1000 | Loss: 0.00001152
Iteration 60/1000 | Loss: 0.00001152
Iteration 61/1000 | Loss: 0.00001151
Iteration 62/1000 | Loss: 0.00001151
Iteration 63/1000 | Loss: 0.00001151
Iteration 64/1000 | Loss: 0.00001150
Iteration 65/1000 | Loss: 0.00001150
Iteration 66/1000 | Loss: 0.00001150
Iteration 67/1000 | Loss: 0.00001150
Iteration 68/1000 | Loss: 0.00001149
Iteration 69/1000 | Loss: 0.00001149
Iteration 70/1000 | Loss: 0.00001149
Iteration 71/1000 | Loss: 0.00001148
Iteration 72/1000 | Loss: 0.00001148
Iteration 73/1000 | Loss: 0.00001147
Iteration 74/1000 | Loss: 0.00001147
Iteration 75/1000 | Loss: 0.00001147
Iteration 76/1000 | Loss: 0.00001146
Iteration 77/1000 | Loss: 0.00001146
Iteration 78/1000 | Loss: 0.00001146
Iteration 79/1000 | Loss: 0.00001146
Iteration 80/1000 | Loss: 0.00001146
Iteration 81/1000 | Loss: 0.00001146
Iteration 82/1000 | Loss: 0.00001146
Iteration 83/1000 | Loss: 0.00001146
Iteration 84/1000 | Loss: 0.00001146
Iteration 85/1000 | Loss: 0.00001146
Iteration 86/1000 | Loss: 0.00001145
Iteration 87/1000 | Loss: 0.00001145
Iteration 88/1000 | Loss: 0.00001145
Iteration 89/1000 | Loss: 0.00001145
Iteration 90/1000 | Loss: 0.00001145
Iteration 91/1000 | Loss: 0.00001145
Iteration 92/1000 | Loss: 0.00001145
Iteration 93/1000 | Loss: 0.00001145
Iteration 94/1000 | Loss: 0.00001144
Iteration 95/1000 | Loss: 0.00001144
Iteration 96/1000 | Loss: 0.00001144
Iteration 97/1000 | Loss: 0.00001144
Iteration 98/1000 | Loss: 0.00001144
Iteration 99/1000 | Loss: 0.00001144
Iteration 100/1000 | Loss: 0.00001144
Iteration 101/1000 | Loss: 0.00001144
Iteration 102/1000 | Loss: 0.00001144
Iteration 103/1000 | Loss: 0.00001144
Iteration 104/1000 | Loss: 0.00001144
Iteration 105/1000 | Loss: 0.00001144
Iteration 106/1000 | Loss: 0.00001144
Iteration 107/1000 | Loss: 0.00001144
Iteration 108/1000 | Loss: 0.00001144
Iteration 109/1000 | Loss: 0.00001144
Iteration 110/1000 | Loss: 0.00001144
Iteration 111/1000 | Loss: 0.00001144
Iteration 112/1000 | Loss: 0.00001144
Iteration 113/1000 | Loss: 0.00001144
Iteration 114/1000 | Loss: 0.00001144
Iteration 115/1000 | Loss: 0.00001144
Iteration 116/1000 | Loss: 0.00001144
Iteration 117/1000 | Loss: 0.00001144
Iteration 118/1000 | Loss: 0.00001144
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [1.1442627510405146e-05, 1.1442627510405146e-05, 1.1442627510405146e-05, 1.1442627510405146e-05, 1.1442627510405146e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1442627510405146e-05

Optimization complete. Final v2v error: 2.788188934326172 mm

Highest mean error: 4.115806579589844 mm for frame 79

Lowest mean error: 2.392439603805542 mm for frame 216

Saving results

Total time: 37.83069109916687
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00366814
Iteration 2/25 | Loss: 0.00073189
Iteration 3/25 | Loss: 0.00061212
Iteration 4/25 | Loss: 0.00060363
Iteration 5/25 | Loss: 0.00060040
Iteration 6/25 | Loss: 0.00059969
Iteration 7/25 | Loss: 0.00059969
Iteration 8/25 | Loss: 0.00059969
Iteration 9/25 | Loss: 0.00059969
Iteration 10/25 | Loss: 0.00059969
Iteration 11/25 | Loss: 0.00059969
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000599689141381532, 0.000599689141381532, 0.000599689141381532, 0.000599689141381532, 0.000599689141381532]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000599689141381532

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49821067
Iteration 2/25 | Loss: 0.00033203
Iteration 3/25 | Loss: 0.00033203
Iteration 4/25 | Loss: 0.00033203
Iteration 5/25 | Loss: 0.00033203
Iteration 6/25 | Loss: 0.00033203
Iteration 7/25 | Loss: 0.00033203
Iteration 8/25 | Loss: 0.00033203
Iteration 9/25 | Loss: 0.00033203
Iteration 10/25 | Loss: 0.00033203
Iteration 11/25 | Loss: 0.00033203
Iteration 12/25 | Loss: 0.00033203
Iteration 13/25 | Loss: 0.00033203
Iteration 14/25 | Loss: 0.00033203
Iteration 15/25 | Loss: 0.00033203
Iteration 16/25 | Loss: 0.00033203
Iteration 17/25 | Loss: 0.00033203
Iteration 18/25 | Loss: 0.00033203
Iteration 19/25 | Loss: 0.00033203
Iteration 20/25 | Loss: 0.00033203
Iteration 21/25 | Loss: 0.00033203
Iteration 22/25 | Loss: 0.00033203
Iteration 23/25 | Loss: 0.00033203
Iteration 24/25 | Loss: 0.00033203
Iteration 25/25 | Loss: 0.00033203

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033203
Iteration 2/1000 | Loss: 0.00001795
Iteration 3/1000 | Loss: 0.00001288
Iteration 4/1000 | Loss: 0.00001222
Iteration 5/1000 | Loss: 0.00001172
Iteration 6/1000 | Loss: 0.00001150
Iteration 7/1000 | Loss: 0.00001142
Iteration 8/1000 | Loss: 0.00001129
Iteration 9/1000 | Loss: 0.00001128
Iteration 10/1000 | Loss: 0.00001119
Iteration 11/1000 | Loss: 0.00001119
Iteration 12/1000 | Loss: 0.00001119
Iteration 13/1000 | Loss: 0.00001112
Iteration 14/1000 | Loss: 0.00001112
Iteration 15/1000 | Loss: 0.00001111
Iteration 16/1000 | Loss: 0.00001110
Iteration 17/1000 | Loss: 0.00001110
Iteration 18/1000 | Loss: 0.00001106
Iteration 19/1000 | Loss: 0.00001105
Iteration 20/1000 | Loss: 0.00001105
Iteration 21/1000 | Loss: 0.00001099
Iteration 22/1000 | Loss: 0.00001099
Iteration 23/1000 | Loss: 0.00001096
Iteration 24/1000 | Loss: 0.00001096
Iteration 25/1000 | Loss: 0.00001094
Iteration 26/1000 | Loss: 0.00001093
Iteration 27/1000 | Loss: 0.00001092
Iteration 28/1000 | Loss: 0.00001092
Iteration 29/1000 | Loss: 0.00001091
Iteration 30/1000 | Loss: 0.00001090
Iteration 31/1000 | Loss: 0.00001090
Iteration 32/1000 | Loss: 0.00001089
Iteration 33/1000 | Loss: 0.00001089
Iteration 34/1000 | Loss: 0.00001088
Iteration 35/1000 | Loss: 0.00001087
Iteration 36/1000 | Loss: 0.00001087
Iteration 37/1000 | Loss: 0.00001087
Iteration 38/1000 | Loss: 0.00001086
Iteration 39/1000 | Loss: 0.00001086
Iteration 40/1000 | Loss: 0.00001085
Iteration 41/1000 | Loss: 0.00001084
Iteration 42/1000 | Loss: 0.00001084
Iteration 43/1000 | Loss: 0.00001084
Iteration 44/1000 | Loss: 0.00001084
Iteration 45/1000 | Loss: 0.00001084
Iteration 46/1000 | Loss: 0.00001084
Iteration 47/1000 | Loss: 0.00001084
Iteration 48/1000 | Loss: 0.00001084
Iteration 49/1000 | Loss: 0.00001084
Iteration 50/1000 | Loss: 0.00001083
Iteration 51/1000 | Loss: 0.00001082
Iteration 52/1000 | Loss: 0.00001082
Iteration 53/1000 | Loss: 0.00001081
Iteration 54/1000 | Loss: 0.00001081
Iteration 55/1000 | Loss: 0.00001081
Iteration 56/1000 | Loss: 0.00001078
Iteration 57/1000 | Loss: 0.00001078
Iteration 58/1000 | Loss: 0.00001077
Iteration 59/1000 | Loss: 0.00001077
Iteration 60/1000 | Loss: 0.00001077
Iteration 61/1000 | Loss: 0.00001076
Iteration 62/1000 | Loss: 0.00001076
Iteration 63/1000 | Loss: 0.00001076
Iteration 64/1000 | Loss: 0.00001076
Iteration 65/1000 | Loss: 0.00001076
Iteration 66/1000 | Loss: 0.00001076
Iteration 67/1000 | Loss: 0.00001076
Iteration 68/1000 | Loss: 0.00001076
Iteration 69/1000 | Loss: 0.00001076
Iteration 70/1000 | Loss: 0.00001076
Iteration 71/1000 | Loss: 0.00001075
Iteration 72/1000 | Loss: 0.00001075
Iteration 73/1000 | Loss: 0.00001075
Iteration 74/1000 | Loss: 0.00001075
Iteration 75/1000 | Loss: 0.00001075
Iteration 76/1000 | Loss: 0.00001075
Iteration 77/1000 | Loss: 0.00001075
Iteration 78/1000 | Loss: 0.00001075
Iteration 79/1000 | Loss: 0.00001075
Iteration 80/1000 | Loss: 0.00001075
Iteration 81/1000 | Loss: 0.00001075
Iteration 82/1000 | Loss: 0.00001075
Iteration 83/1000 | Loss: 0.00001075
Iteration 84/1000 | Loss: 0.00001075
Iteration 85/1000 | Loss: 0.00001074
Iteration 86/1000 | Loss: 0.00001074
Iteration 87/1000 | Loss: 0.00001074
Iteration 88/1000 | Loss: 0.00001074
Iteration 89/1000 | Loss: 0.00001074
Iteration 90/1000 | Loss: 0.00001074
Iteration 91/1000 | Loss: 0.00001074
Iteration 92/1000 | Loss: 0.00001074
Iteration 93/1000 | Loss: 0.00001074
Iteration 94/1000 | Loss: 0.00001074
Iteration 95/1000 | Loss: 0.00001074
Iteration 96/1000 | Loss: 0.00001074
Iteration 97/1000 | Loss: 0.00001074
Iteration 98/1000 | Loss: 0.00001074
Iteration 99/1000 | Loss: 0.00001073
Iteration 100/1000 | Loss: 0.00001073
Iteration 101/1000 | Loss: 0.00001073
Iteration 102/1000 | Loss: 0.00001073
Iteration 103/1000 | Loss: 0.00001073
Iteration 104/1000 | Loss: 0.00001073
Iteration 105/1000 | Loss: 0.00001073
Iteration 106/1000 | Loss: 0.00001073
Iteration 107/1000 | Loss: 0.00001073
Iteration 108/1000 | Loss: 0.00001073
Iteration 109/1000 | Loss: 0.00001073
Iteration 110/1000 | Loss: 0.00001073
Iteration 111/1000 | Loss: 0.00001072
Iteration 112/1000 | Loss: 0.00001072
Iteration 113/1000 | Loss: 0.00001072
Iteration 114/1000 | Loss: 0.00001072
Iteration 115/1000 | Loss: 0.00001072
Iteration 116/1000 | Loss: 0.00001072
Iteration 117/1000 | Loss: 0.00001072
Iteration 118/1000 | Loss: 0.00001072
Iteration 119/1000 | Loss: 0.00001072
Iteration 120/1000 | Loss: 0.00001071
Iteration 121/1000 | Loss: 0.00001071
Iteration 122/1000 | Loss: 0.00001071
Iteration 123/1000 | Loss: 0.00001071
Iteration 124/1000 | Loss: 0.00001071
Iteration 125/1000 | Loss: 0.00001071
Iteration 126/1000 | Loss: 0.00001071
Iteration 127/1000 | Loss: 0.00001071
Iteration 128/1000 | Loss: 0.00001071
Iteration 129/1000 | Loss: 0.00001071
Iteration 130/1000 | Loss: 0.00001071
Iteration 131/1000 | Loss: 0.00001071
Iteration 132/1000 | Loss: 0.00001071
Iteration 133/1000 | Loss: 0.00001071
Iteration 134/1000 | Loss: 0.00001071
Iteration 135/1000 | Loss: 0.00001071
Iteration 136/1000 | Loss: 0.00001071
Iteration 137/1000 | Loss: 0.00001071
Iteration 138/1000 | Loss: 0.00001071
Iteration 139/1000 | Loss: 0.00001071
Iteration 140/1000 | Loss: 0.00001070
Iteration 141/1000 | Loss: 0.00001070
Iteration 142/1000 | Loss: 0.00001070
Iteration 143/1000 | Loss: 0.00001070
Iteration 144/1000 | Loss: 0.00001070
Iteration 145/1000 | Loss: 0.00001070
Iteration 146/1000 | Loss: 0.00001070
Iteration 147/1000 | Loss: 0.00001070
Iteration 148/1000 | Loss: 0.00001070
Iteration 149/1000 | Loss: 0.00001070
Iteration 150/1000 | Loss: 0.00001070
Iteration 151/1000 | Loss: 0.00001070
Iteration 152/1000 | Loss: 0.00001070
Iteration 153/1000 | Loss: 0.00001069
Iteration 154/1000 | Loss: 0.00001069
Iteration 155/1000 | Loss: 0.00001069
Iteration 156/1000 | Loss: 0.00001069
Iteration 157/1000 | Loss: 0.00001069
Iteration 158/1000 | Loss: 0.00001069
Iteration 159/1000 | Loss: 0.00001069
Iteration 160/1000 | Loss: 0.00001068
Iteration 161/1000 | Loss: 0.00001068
Iteration 162/1000 | Loss: 0.00001068
Iteration 163/1000 | Loss: 0.00001068
Iteration 164/1000 | Loss: 0.00001068
Iteration 165/1000 | Loss: 0.00001068
Iteration 166/1000 | Loss: 0.00001068
Iteration 167/1000 | Loss: 0.00001068
Iteration 168/1000 | Loss: 0.00001068
Iteration 169/1000 | Loss: 0.00001068
Iteration 170/1000 | Loss: 0.00001068
Iteration 171/1000 | Loss: 0.00001068
Iteration 172/1000 | Loss: 0.00001067
Iteration 173/1000 | Loss: 0.00001067
Iteration 174/1000 | Loss: 0.00001067
Iteration 175/1000 | Loss: 0.00001067
Iteration 176/1000 | Loss: 0.00001067
Iteration 177/1000 | Loss: 0.00001067
Iteration 178/1000 | Loss: 0.00001067
Iteration 179/1000 | Loss: 0.00001067
Iteration 180/1000 | Loss: 0.00001067
Iteration 181/1000 | Loss: 0.00001067
Iteration 182/1000 | Loss: 0.00001067
Iteration 183/1000 | Loss: 0.00001067
Iteration 184/1000 | Loss: 0.00001067
Iteration 185/1000 | Loss: 0.00001067
Iteration 186/1000 | Loss: 0.00001067
Iteration 187/1000 | Loss: 0.00001067
Iteration 188/1000 | Loss: 0.00001067
Iteration 189/1000 | Loss: 0.00001067
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.0669780749594793e-05, 1.0669780749594793e-05, 1.0669780749594793e-05, 1.0669780749594793e-05, 1.0669780749594793e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0669780749594793e-05

Optimization complete. Final v2v error: 2.745048761367798 mm

Highest mean error: 2.9508166313171387 mm for frame 12

Lowest mean error: 2.5593628883361816 mm for frame 133

Saving results

Total time: 33.541831970214844
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00919351
Iteration 2/25 | Loss: 0.00083911
Iteration 3/25 | Loss: 0.00067299
Iteration 4/25 | Loss: 0.00064078
Iteration 5/25 | Loss: 0.00062676
Iteration 6/25 | Loss: 0.00062431
Iteration 7/25 | Loss: 0.00062388
Iteration 8/25 | Loss: 0.00062388
Iteration 9/25 | Loss: 0.00062388
Iteration 10/25 | Loss: 0.00062388
Iteration 11/25 | Loss: 0.00062388
Iteration 12/25 | Loss: 0.00062388
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006238778005354106, 0.0006238778005354106, 0.0006238778005354106, 0.0006238778005354106, 0.0006238778005354106]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006238778005354106

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45748580
Iteration 2/25 | Loss: 0.00031327
Iteration 3/25 | Loss: 0.00031326
Iteration 4/25 | Loss: 0.00031326
Iteration 5/25 | Loss: 0.00031326
Iteration 6/25 | Loss: 0.00031326
Iteration 7/25 | Loss: 0.00031326
Iteration 8/25 | Loss: 0.00031326
Iteration 9/25 | Loss: 0.00031326
Iteration 10/25 | Loss: 0.00031326
Iteration 11/25 | Loss: 0.00031326
Iteration 12/25 | Loss: 0.00031326
Iteration 13/25 | Loss: 0.00031326
Iteration 14/25 | Loss: 0.00031326
Iteration 15/25 | Loss: 0.00031326
Iteration 16/25 | Loss: 0.00031326
Iteration 17/25 | Loss: 0.00031326
Iteration 18/25 | Loss: 0.00031326
Iteration 19/25 | Loss: 0.00031326
Iteration 20/25 | Loss: 0.00031326
Iteration 21/25 | Loss: 0.00031326
Iteration 22/25 | Loss: 0.00031326
Iteration 23/25 | Loss: 0.00031326
Iteration 24/25 | Loss: 0.00031326
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0003132574202027172, 0.0003132574202027172, 0.0003132574202027172, 0.0003132574202027172, 0.0003132574202027172]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003132574202027172

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00031326
Iteration 2/1000 | Loss: 0.00002325
Iteration 3/1000 | Loss: 0.00001708
Iteration 4/1000 | Loss: 0.00001579
Iteration 5/1000 | Loss: 0.00001509
Iteration 6/1000 | Loss: 0.00001468
Iteration 7/1000 | Loss: 0.00001432
Iteration 8/1000 | Loss: 0.00001411
Iteration 9/1000 | Loss: 0.00001394
Iteration 10/1000 | Loss: 0.00001392
Iteration 11/1000 | Loss: 0.00001389
Iteration 12/1000 | Loss: 0.00001388
Iteration 13/1000 | Loss: 0.00001383
Iteration 14/1000 | Loss: 0.00001376
Iteration 15/1000 | Loss: 0.00001375
Iteration 16/1000 | Loss: 0.00001375
Iteration 17/1000 | Loss: 0.00001375
Iteration 18/1000 | Loss: 0.00001374
Iteration 19/1000 | Loss: 0.00001373
Iteration 20/1000 | Loss: 0.00001373
Iteration 21/1000 | Loss: 0.00001371
Iteration 22/1000 | Loss: 0.00001368
Iteration 23/1000 | Loss: 0.00001368
Iteration 24/1000 | Loss: 0.00001368
Iteration 25/1000 | Loss: 0.00001368
Iteration 26/1000 | Loss: 0.00001367
Iteration 27/1000 | Loss: 0.00001367
Iteration 28/1000 | Loss: 0.00001367
Iteration 29/1000 | Loss: 0.00001365
Iteration 30/1000 | Loss: 0.00001364
Iteration 31/1000 | Loss: 0.00001364
Iteration 32/1000 | Loss: 0.00001364
Iteration 33/1000 | Loss: 0.00001364
Iteration 34/1000 | Loss: 0.00001364
Iteration 35/1000 | Loss: 0.00001363
Iteration 36/1000 | Loss: 0.00001363
Iteration 37/1000 | Loss: 0.00001363
Iteration 38/1000 | Loss: 0.00001363
Iteration 39/1000 | Loss: 0.00001363
Iteration 40/1000 | Loss: 0.00001363
Iteration 41/1000 | Loss: 0.00001362
Iteration 42/1000 | Loss: 0.00001362
Iteration 43/1000 | Loss: 0.00001362
Iteration 44/1000 | Loss: 0.00001360
Iteration 45/1000 | Loss: 0.00001360
Iteration 46/1000 | Loss: 0.00001360
Iteration 47/1000 | Loss: 0.00001359
Iteration 48/1000 | Loss: 0.00001359
Iteration 49/1000 | Loss: 0.00001359
Iteration 50/1000 | Loss: 0.00001359
Iteration 51/1000 | Loss: 0.00001359
Iteration 52/1000 | Loss: 0.00001359
Iteration 53/1000 | Loss: 0.00001359
Iteration 54/1000 | Loss: 0.00001359
Iteration 55/1000 | Loss: 0.00001359
Iteration 56/1000 | Loss: 0.00001359
Iteration 57/1000 | Loss: 0.00001359
Iteration 58/1000 | Loss: 0.00001359
Iteration 59/1000 | Loss: 0.00001358
Iteration 60/1000 | Loss: 0.00001358
Iteration 61/1000 | Loss: 0.00001358
Iteration 62/1000 | Loss: 0.00001358
Iteration 63/1000 | Loss: 0.00001358
Iteration 64/1000 | Loss: 0.00001358
Iteration 65/1000 | Loss: 0.00001356
Iteration 66/1000 | Loss: 0.00001356
Iteration 67/1000 | Loss: 0.00001356
Iteration 68/1000 | Loss: 0.00001356
Iteration 69/1000 | Loss: 0.00001356
Iteration 70/1000 | Loss: 0.00001355
Iteration 71/1000 | Loss: 0.00001355
Iteration 72/1000 | Loss: 0.00001355
Iteration 73/1000 | Loss: 0.00001355
Iteration 74/1000 | Loss: 0.00001355
Iteration 75/1000 | Loss: 0.00001355
Iteration 76/1000 | Loss: 0.00001355
Iteration 77/1000 | Loss: 0.00001355
Iteration 78/1000 | Loss: 0.00001355
Iteration 79/1000 | Loss: 0.00001354
Iteration 80/1000 | Loss: 0.00001354
Iteration 81/1000 | Loss: 0.00001354
Iteration 82/1000 | Loss: 0.00001354
Iteration 83/1000 | Loss: 0.00001354
Iteration 84/1000 | Loss: 0.00001354
Iteration 85/1000 | Loss: 0.00001354
Iteration 86/1000 | Loss: 0.00001354
Iteration 87/1000 | Loss: 0.00001354
Iteration 88/1000 | Loss: 0.00001353
Iteration 89/1000 | Loss: 0.00001353
Iteration 90/1000 | Loss: 0.00001353
Iteration 91/1000 | Loss: 0.00001353
Iteration 92/1000 | Loss: 0.00001353
Iteration 93/1000 | Loss: 0.00001353
Iteration 94/1000 | Loss: 0.00001352
Iteration 95/1000 | Loss: 0.00001352
Iteration 96/1000 | Loss: 0.00001352
Iteration 97/1000 | Loss: 0.00001352
Iteration 98/1000 | Loss: 0.00001352
Iteration 99/1000 | Loss: 0.00001352
Iteration 100/1000 | Loss: 0.00001352
Iteration 101/1000 | Loss: 0.00001352
Iteration 102/1000 | Loss: 0.00001352
Iteration 103/1000 | Loss: 0.00001351
Iteration 104/1000 | Loss: 0.00001351
Iteration 105/1000 | Loss: 0.00001351
Iteration 106/1000 | Loss: 0.00001350
Iteration 107/1000 | Loss: 0.00001350
Iteration 108/1000 | Loss: 0.00001350
Iteration 109/1000 | Loss: 0.00001350
Iteration 110/1000 | Loss: 0.00001350
Iteration 111/1000 | Loss: 0.00001350
Iteration 112/1000 | Loss: 0.00001350
Iteration 113/1000 | Loss: 0.00001350
Iteration 114/1000 | Loss: 0.00001350
Iteration 115/1000 | Loss: 0.00001350
Iteration 116/1000 | Loss: 0.00001350
Iteration 117/1000 | Loss: 0.00001350
Iteration 118/1000 | Loss: 0.00001350
Iteration 119/1000 | Loss: 0.00001350
Iteration 120/1000 | Loss: 0.00001350
Iteration 121/1000 | Loss: 0.00001350
Iteration 122/1000 | Loss: 0.00001349
Iteration 123/1000 | Loss: 0.00001349
Iteration 124/1000 | Loss: 0.00001349
Iteration 125/1000 | Loss: 0.00001349
Iteration 126/1000 | Loss: 0.00001349
Iteration 127/1000 | Loss: 0.00001349
Iteration 128/1000 | Loss: 0.00001349
Iteration 129/1000 | Loss: 0.00001349
Iteration 130/1000 | Loss: 0.00001349
Iteration 131/1000 | Loss: 0.00001349
Iteration 132/1000 | Loss: 0.00001349
Iteration 133/1000 | Loss: 0.00001349
Iteration 134/1000 | Loss: 0.00001349
Iteration 135/1000 | Loss: 0.00001349
Iteration 136/1000 | Loss: 0.00001349
Iteration 137/1000 | Loss: 0.00001349
Iteration 138/1000 | Loss: 0.00001349
Iteration 139/1000 | Loss: 0.00001348
Iteration 140/1000 | Loss: 0.00001348
Iteration 141/1000 | Loss: 0.00001348
Iteration 142/1000 | Loss: 0.00001348
Iteration 143/1000 | Loss: 0.00001348
Iteration 144/1000 | Loss: 0.00001348
Iteration 145/1000 | Loss: 0.00001348
Iteration 146/1000 | Loss: 0.00001348
Iteration 147/1000 | Loss: 0.00001348
Iteration 148/1000 | Loss: 0.00001348
Iteration 149/1000 | Loss: 0.00001348
Iteration 150/1000 | Loss: 0.00001348
Iteration 151/1000 | Loss: 0.00001348
Iteration 152/1000 | Loss: 0.00001348
Iteration 153/1000 | Loss: 0.00001348
Iteration 154/1000 | Loss: 0.00001348
Iteration 155/1000 | Loss: 0.00001348
Iteration 156/1000 | Loss: 0.00001348
Iteration 157/1000 | Loss: 0.00001348
Iteration 158/1000 | Loss: 0.00001347
Iteration 159/1000 | Loss: 0.00001347
Iteration 160/1000 | Loss: 0.00001347
Iteration 161/1000 | Loss: 0.00001347
Iteration 162/1000 | Loss: 0.00001347
Iteration 163/1000 | Loss: 0.00001347
Iteration 164/1000 | Loss: 0.00001347
Iteration 165/1000 | Loss: 0.00001347
Iteration 166/1000 | Loss: 0.00001347
Iteration 167/1000 | Loss: 0.00001347
Iteration 168/1000 | Loss: 0.00001347
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.347170564258704e-05, 1.347170564258704e-05, 1.347170564258704e-05, 1.347170564258704e-05, 1.347170564258704e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.347170564258704e-05

Optimization complete. Final v2v error: 3.103884220123291 mm

Highest mean error: 4.050561428070068 mm for frame 175

Lowest mean error: 2.7984251976013184 mm for frame 77

Saving results

Total time: 39.77852964401245
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00854543
Iteration 2/25 | Loss: 0.00116104
Iteration 3/25 | Loss: 0.00076644
Iteration 4/25 | Loss: 0.00070439
Iteration 5/25 | Loss: 0.00069692
Iteration 6/25 | Loss: 0.00069587
Iteration 7/25 | Loss: 0.00069583
Iteration 8/25 | Loss: 0.00069583
Iteration 9/25 | Loss: 0.00069583
Iteration 10/25 | Loss: 0.00069583
Iteration 11/25 | Loss: 0.00069583
Iteration 12/25 | Loss: 0.00069583
Iteration 13/25 | Loss: 0.00069583
Iteration 14/25 | Loss: 0.00069583
Iteration 15/25 | Loss: 0.00069583
Iteration 16/25 | Loss: 0.00069583
Iteration 17/25 | Loss: 0.00069583
Iteration 18/25 | Loss: 0.00069583
Iteration 19/25 | Loss: 0.00069583
Iteration 20/25 | Loss: 0.00069583
Iteration 21/25 | Loss: 0.00069583
Iteration 22/25 | Loss: 0.00069583
Iteration 23/25 | Loss: 0.00069583
Iteration 24/25 | Loss: 0.00069583
Iteration 25/25 | Loss: 0.00069583

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.03727448
Iteration 2/25 | Loss: 0.00024987
Iteration 3/25 | Loss: 0.00024986
Iteration 4/25 | Loss: 0.00024985
Iteration 5/25 | Loss: 0.00024985
Iteration 6/25 | Loss: 0.00024985
Iteration 7/25 | Loss: 0.00024985
Iteration 8/25 | Loss: 0.00024985
Iteration 9/25 | Loss: 0.00024985
Iteration 10/25 | Loss: 0.00024985
Iteration 11/25 | Loss: 0.00024985
Iteration 12/25 | Loss: 0.00024985
Iteration 13/25 | Loss: 0.00024985
Iteration 14/25 | Loss: 0.00024985
Iteration 15/25 | Loss: 0.00024985
Iteration 16/25 | Loss: 0.00024985
Iteration 17/25 | Loss: 0.00024985
Iteration 18/25 | Loss: 0.00024985
Iteration 19/25 | Loss: 0.00024985
Iteration 20/25 | Loss: 0.00024985
Iteration 21/25 | Loss: 0.00024985
Iteration 22/25 | Loss: 0.00024985
Iteration 23/25 | Loss: 0.00024985
Iteration 24/25 | Loss: 0.00024985
Iteration 25/25 | Loss: 0.00024985

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024985
Iteration 2/1000 | Loss: 0.00002788
Iteration 3/1000 | Loss: 0.00002274
Iteration 4/1000 | Loss: 0.00002151
Iteration 5/1000 | Loss: 0.00002030
Iteration 6/1000 | Loss: 0.00001960
Iteration 7/1000 | Loss: 0.00001915
Iteration 8/1000 | Loss: 0.00001888
Iteration 9/1000 | Loss: 0.00001878
Iteration 10/1000 | Loss: 0.00001878
Iteration 11/1000 | Loss: 0.00001878
Iteration 12/1000 | Loss: 0.00001875
Iteration 13/1000 | Loss: 0.00001866
Iteration 14/1000 | Loss: 0.00001856
Iteration 15/1000 | Loss: 0.00001856
Iteration 16/1000 | Loss: 0.00001854
Iteration 17/1000 | Loss: 0.00001853
Iteration 18/1000 | Loss: 0.00001852
Iteration 19/1000 | Loss: 0.00001851
Iteration 20/1000 | Loss: 0.00001851
Iteration 21/1000 | Loss: 0.00001851
Iteration 22/1000 | Loss: 0.00001848
Iteration 23/1000 | Loss: 0.00001848
Iteration 24/1000 | Loss: 0.00001848
Iteration 25/1000 | Loss: 0.00001847
Iteration 26/1000 | Loss: 0.00001847
Iteration 27/1000 | Loss: 0.00001847
Iteration 28/1000 | Loss: 0.00001847
Iteration 29/1000 | Loss: 0.00001847
Iteration 30/1000 | Loss: 0.00001847
Iteration 31/1000 | Loss: 0.00001847
Iteration 32/1000 | Loss: 0.00001847
Iteration 33/1000 | Loss: 0.00001847
Iteration 34/1000 | Loss: 0.00001846
Iteration 35/1000 | Loss: 0.00001846
Iteration 36/1000 | Loss: 0.00001846
Iteration 37/1000 | Loss: 0.00001846
Iteration 38/1000 | Loss: 0.00001846
Iteration 39/1000 | Loss: 0.00001846
Iteration 40/1000 | Loss: 0.00001846
Iteration 41/1000 | Loss: 0.00001846
Iteration 42/1000 | Loss: 0.00001845
Iteration 43/1000 | Loss: 0.00001845
Iteration 44/1000 | Loss: 0.00001845
Iteration 45/1000 | Loss: 0.00001845
Iteration 46/1000 | Loss: 0.00001845
Iteration 47/1000 | Loss: 0.00001845
Iteration 48/1000 | Loss: 0.00001845
Iteration 49/1000 | Loss: 0.00001844
Iteration 50/1000 | Loss: 0.00001844
Iteration 51/1000 | Loss: 0.00001844
Iteration 52/1000 | Loss: 0.00001843
Iteration 53/1000 | Loss: 0.00001843
Iteration 54/1000 | Loss: 0.00001843
Iteration 55/1000 | Loss: 0.00001843
Iteration 56/1000 | Loss: 0.00001842
Iteration 57/1000 | Loss: 0.00001842
Iteration 58/1000 | Loss: 0.00001842
Iteration 59/1000 | Loss: 0.00001842
Iteration 60/1000 | Loss: 0.00001842
Iteration 61/1000 | Loss: 0.00001842
Iteration 62/1000 | Loss: 0.00001842
Iteration 63/1000 | Loss: 0.00001842
Iteration 64/1000 | Loss: 0.00001842
Iteration 65/1000 | Loss: 0.00001842
Iteration 66/1000 | Loss: 0.00001842
Iteration 67/1000 | Loss: 0.00001842
Iteration 68/1000 | Loss: 0.00001841
Iteration 69/1000 | Loss: 0.00001841
Iteration 70/1000 | Loss: 0.00001841
Iteration 71/1000 | Loss: 0.00001841
Iteration 72/1000 | Loss: 0.00001841
Iteration 73/1000 | Loss: 0.00001841
Iteration 74/1000 | Loss: 0.00001841
Iteration 75/1000 | Loss: 0.00001841
Iteration 76/1000 | Loss: 0.00001840
Iteration 77/1000 | Loss: 0.00001840
Iteration 78/1000 | Loss: 0.00001840
Iteration 79/1000 | Loss: 0.00001840
Iteration 80/1000 | Loss: 0.00001840
Iteration 81/1000 | Loss: 0.00001839
Iteration 82/1000 | Loss: 0.00001839
Iteration 83/1000 | Loss: 0.00001839
Iteration 84/1000 | Loss: 0.00001839
Iteration 85/1000 | Loss: 0.00001839
Iteration 86/1000 | Loss: 0.00001839
Iteration 87/1000 | Loss: 0.00001838
Iteration 88/1000 | Loss: 0.00001838
Iteration 89/1000 | Loss: 0.00001838
Iteration 90/1000 | Loss: 0.00001838
Iteration 91/1000 | Loss: 0.00001838
Iteration 92/1000 | Loss: 0.00001838
Iteration 93/1000 | Loss: 0.00001838
Iteration 94/1000 | Loss: 0.00001838
Iteration 95/1000 | Loss: 0.00001838
Iteration 96/1000 | Loss: 0.00001838
Iteration 97/1000 | Loss: 0.00001837
Iteration 98/1000 | Loss: 0.00001837
Iteration 99/1000 | Loss: 0.00001837
Iteration 100/1000 | Loss: 0.00001837
Iteration 101/1000 | Loss: 0.00001837
Iteration 102/1000 | Loss: 0.00001837
Iteration 103/1000 | Loss: 0.00001837
Iteration 104/1000 | Loss: 0.00001837
Iteration 105/1000 | Loss: 0.00001837
Iteration 106/1000 | Loss: 0.00001837
Iteration 107/1000 | Loss: 0.00001837
Iteration 108/1000 | Loss: 0.00001837
Iteration 109/1000 | Loss: 0.00001837
Iteration 110/1000 | Loss: 0.00001837
Iteration 111/1000 | Loss: 0.00001837
Iteration 112/1000 | Loss: 0.00001837
Iteration 113/1000 | Loss: 0.00001837
Iteration 114/1000 | Loss: 0.00001837
Iteration 115/1000 | Loss: 0.00001837
Iteration 116/1000 | Loss: 0.00001837
Iteration 117/1000 | Loss: 0.00001837
Iteration 118/1000 | Loss: 0.00001837
Iteration 119/1000 | Loss: 0.00001837
Iteration 120/1000 | Loss: 0.00001837
Iteration 121/1000 | Loss: 0.00001837
Iteration 122/1000 | Loss: 0.00001837
Iteration 123/1000 | Loss: 0.00001837
Iteration 124/1000 | Loss: 0.00001837
Iteration 125/1000 | Loss: 0.00001837
Iteration 126/1000 | Loss: 0.00001837
Iteration 127/1000 | Loss: 0.00001837
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.8366061340202577e-05, 1.8366061340202577e-05, 1.8366061340202577e-05, 1.8366061340202577e-05, 1.8366061340202577e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8366061340202577e-05

Optimization complete. Final v2v error: 3.672981023788452 mm

Highest mean error: 4.261947154998779 mm for frame 1

Lowest mean error: 3.330235719680786 mm for frame 51

Saving results

Total time: 31.421398162841797
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00854099
Iteration 2/25 | Loss: 0.00122480
Iteration 3/25 | Loss: 0.00081517
Iteration 4/25 | Loss: 0.00074791
Iteration 5/25 | Loss: 0.00071740
Iteration 6/25 | Loss: 0.00070598
Iteration 7/25 | Loss: 0.00070994
Iteration 8/25 | Loss: 0.00068010
Iteration 9/25 | Loss: 0.00067422
Iteration 10/25 | Loss: 0.00067105
Iteration 11/25 | Loss: 0.00067148
Iteration 12/25 | Loss: 0.00066791
Iteration 13/25 | Loss: 0.00066584
Iteration 14/25 | Loss: 0.00066501
Iteration 15/25 | Loss: 0.00066455
Iteration 16/25 | Loss: 0.00066443
Iteration 17/25 | Loss: 0.00066438
Iteration 18/25 | Loss: 0.00066438
Iteration 19/25 | Loss: 0.00066438
Iteration 20/25 | Loss: 0.00066438
Iteration 21/25 | Loss: 0.00066437
Iteration 22/25 | Loss: 0.00066437
Iteration 23/25 | Loss: 0.00066437
Iteration 24/25 | Loss: 0.00066437
Iteration 25/25 | Loss: 0.00066437

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.98750234
Iteration 2/25 | Loss: 0.00036215
Iteration 3/25 | Loss: 0.00036214
Iteration 4/25 | Loss: 0.00036214
Iteration 5/25 | Loss: 0.00036214
Iteration 6/25 | Loss: 0.00036214
Iteration 7/25 | Loss: 0.00036214
Iteration 8/25 | Loss: 0.00036214
Iteration 9/25 | Loss: 0.00036214
Iteration 10/25 | Loss: 0.00036214
Iteration 11/25 | Loss: 0.00036214
Iteration 12/25 | Loss: 0.00036213
Iteration 13/25 | Loss: 0.00036213
Iteration 14/25 | Loss: 0.00036213
Iteration 15/25 | Loss: 0.00036213
Iteration 16/25 | Loss: 0.00036213
Iteration 17/25 | Loss: 0.00036213
Iteration 18/25 | Loss: 0.00036213
Iteration 19/25 | Loss: 0.00036213
Iteration 20/25 | Loss: 0.00036213
Iteration 21/25 | Loss: 0.00036213
Iteration 22/25 | Loss: 0.00036213
Iteration 23/25 | Loss: 0.00036213
Iteration 24/25 | Loss: 0.00036213
Iteration 25/25 | Loss: 0.00036213

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036213
Iteration 2/1000 | Loss: 0.00003133
Iteration 3/1000 | Loss: 0.00001862
Iteration 4/1000 | Loss: 0.00001726
Iteration 5/1000 | Loss: 0.00001645
Iteration 6/1000 | Loss: 0.00001596
Iteration 7/1000 | Loss: 0.00001562
Iteration 8/1000 | Loss: 0.00001560
Iteration 9/1000 | Loss: 0.00001537
Iteration 10/1000 | Loss: 0.00001520
Iteration 11/1000 | Loss: 0.00001508
Iteration 12/1000 | Loss: 0.00001506
Iteration 13/1000 | Loss: 0.00001506
Iteration 14/1000 | Loss: 0.00001505
Iteration 15/1000 | Loss: 0.00001505
Iteration 16/1000 | Loss: 0.00001504
Iteration 17/1000 | Loss: 0.00001504
Iteration 18/1000 | Loss: 0.00001503
Iteration 19/1000 | Loss: 0.00001501
Iteration 20/1000 | Loss: 0.00001501
Iteration 21/1000 | Loss: 0.00001501
Iteration 22/1000 | Loss: 0.00001500
Iteration 23/1000 | Loss: 0.00001499
Iteration 24/1000 | Loss: 0.00001498
Iteration 25/1000 | Loss: 0.00001498
Iteration 26/1000 | Loss: 0.00001497
Iteration 27/1000 | Loss: 0.00001496
Iteration 28/1000 | Loss: 0.00001496
Iteration 29/1000 | Loss: 0.00001495
Iteration 30/1000 | Loss: 0.00001494
Iteration 31/1000 | Loss: 0.00001493
Iteration 32/1000 | Loss: 0.00001490
Iteration 33/1000 | Loss: 0.00001489
Iteration 34/1000 | Loss: 0.00001489
Iteration 35/1000 | Loss: 0.00001489
Iteration 36/1000 | Loss: 0.00001489
Iteration 37/1000 | Loss: 0.00001489
Iteration 38/1000 | Loss: 0.00001486
Iteration 39/1000 | Loss: 0.00001485
Iteration 40/1000 | Loss: 0.00001484
Iteration 41/1000 | Loss: 0.00001484
Iteration 42/1000 | Loss: 0.00001483
Iteration 43/1000 | Loss: 0.00001483
Iteration 44/1000 | Loss: 0.00001482
Iteration 45/1000 | Loss: 0.00001481
Iteration 46/1000 | Loss: 0.00001480
Iteration 47/1000 | Loss: 0.00001480
Iteration 48/1000 | Loss: 0.00001479
Iteration 49/1000 | Loss: 0.00001479
Iteration 50/1000 | Loss: 0.00001479
Iteration 51/1000 | Loss: 0.00001479
Iteration 52/1000 | Loss: 0.00001479
Iteration 53/1000 | Loss: 0.00001479
Iteration 54/1000 | Loss: 0.00001478
Iteration 55/1000 | Loss: 0.00001478
Iteration 56/1000 | Loss: 0.00001478
Iteration 57/1000 | Loss: 0.00001477
Iteration 58/1000 | Loss: 0.00001476
Iteration 59/1000 | Loss: 0.00001476
Iteration 60/1000 | Loss: 0.00001476
Iteration 61/1000 | Loss: 0.00001476
Iteration 62/1000 | Loss: 0.00001476
Iteration 63/1000 | Loss: 0.00001475
Iteration 64/1000 | Loss: 0.00001475
Iteration 65/1000 | Loss: 0.00001475
Iteration 66/1000 | Loss: 0.00001475
Iteration 67/1000 | Loss: 0.00001475
Iteration 68/1000 | Loss: 0.00001475
Iteration 69/1000 | Loss: 0.00001475
Iteration 70/1000 | Loss: 0.00001475
Iteration 71/1000 | Loss: 0.00001475
Iteration 72/1000 | Loss: 0.00001475
Iteration 73/1000 | Loss: 0.00001474
Iteration 74/1000 | Loss: 0.00001474
Iteration 75/1000 | Loss: 0.00001474
Iteration 76/1000 | Loss: 0.00001474
Iteration 77/1000 | Loss: 0.00001474
Iteration 78/1000 | Loss: 0.00001474
Iteration 79/1000 | Loss: 0.00001473
Iteration 80/1000 | Loss: 0.00001473
Iteration 81/1000 | Loss: 0.00001473
Iteration 82/1000 | Loss: 0.00001473
Iteration 83/1000 | Loss: 0.00001473
Iteration 84/1000 | Loss: 0.00001473
Iteration 85/1000 | Loss: 0.00001473
Iteration 86/1000 | Loss: 0.00001473
Iteration 87/1000 | Loss: 0.00001472
Iteration 88/1000 | Loss: 0.00001472
Iteration 89/1000 | Loss: 0.00001472
Iteration 90/1000 | Loss: 0.00001472
Iteration 91/1000 | Loss: 0.00001471
Iteration 92/1000 | Loss: 0.00001471
Iteration 93/1000 | Loss: 0.00001471
Iteration 94/1000 | Loss: 0.00001470
Iteration 95/1000 | Loss: 0.00001470
Iteration 96/1000 | Loss: 0.00001470
Iteration 97/1000 | Loss: 0.00001470
Iteration 98/1000 | Loss: 0.00001470
Iteration 99/1000 | Loss: 0.00001469
Iteration 100/1000 | Loss: 0.00001469
Iteration 101/1000 | Loss: 0.00001469
Iteration 102/1000 | Loss: 0.00001469
Iteration 103/1000 | Loss: 0.00001468
Iteration 104/1000 | Loss: 0.00001468
Iteration 105/1000 | Loss: 0.00001467
Iteration 106/1000 | Loss: 0.00001467
Iteration 107/1000 | Loss: 0.00001467
Iteration 108/1000 | Loss: 0.00001467
Iteration 109/1000 | Loss: 0.00001467
Iteration 110/1000 | Loss: 0.00001467
Iteration 111/1000 | Loss: 0.00001467
Iteration 112/1000 | Loss: 0.00001466
Iteration 113/1000 | Loss: 0.00001466
Iteration 114/1000 | Loss: 0.00001466
Iteration 115/1000 | Loss: 0.00001466
Iteration 116/1000 | Loss: 0.00001465
Iteration 117/1000 | Loss: 0.00001465
Iteration 118/1000 | Loss: 0.00001464
Iteration 119/1000 | Loss: 0.00001464
Iteration 120/1000 | Loss: 0.00001464
Iteration 121/1000 | Loss: 0.00001464
Iteration 122/1000 | Loss: 0.00001464
Iteration 123/1000 | Loss: 0.00001464
Iteration 124/1000 | Loss: 0.00001463
Iteration 125/1000 | Loss: 0.00001463
Iteration 126/1000 | Loss: 0.00001463
Iteration 127/1000 | Loss: 0.00001463
Iteration 128/1000 | Loss: 0.00001463
Iteration 129/1000 | Loss: 0.00001463
Iteration 130/1000 | Loss: 0.00001463
Iteration 131/1000 | Loss: 0.00001463
Iteration 132/1000 | Loss: 0.00001463
Iteration 133/1000 | Loss: 0.00001463
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [1.4633509636041708e-05, 1.4633509636041708e-05, 1.4633509636041708e-05, 1.4633509636041708e-05, 1.4633509636041708e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4633509636041708e-05

Optimization complete. Final v2v error: 3.217186212539673 mm

Highest mean error: 9.285213470458984 mm for frame 41

Lowest mean error: 2.8386037349700928 mm for frame 37

Saving results

Total time: 61.4743275642395
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01047633
Iteration 2/25 | Loss: 0.00186380
Iteration 3/25 | Loss: 0.00117796
Iteration 4/25 | Loss: 0.00102206
Iteration 5/25 | Loss: 0.00100134
Iteration 6/25 | Loss: 0.00103358
Iteration 7/25 | Loss: 0.00094959
Iteration 8/25 | Loss: 0.00090089
Iteration 9/25 | Loss: 0.00086943
Iteration 10/25 | Loss: 0.00085110
Iteration 11/25 | Loss: 0.00084186
Iteration 12/25 | Loss: 0.00083808
Iteration 13/25 | Loss: 0.00083698
Iteration 14/25 | Loss: 0.00083652
Iteration 15/25 | Loss: 0.00083631
Iteration 16/25 | Loss: 0.00083628
Iteration 17/25 | Loss: 0.00083628
Iteration 18/25 | Loss: 0.00083628
Iteration 19/25 | Loss: 0.00083627
Iteration 20/25 | Loss: 0.00083627
Iteration 21/25 | Loss: 0.00083627
Iteration 22/25 | Loss: 0.00083627
Iteration 23/25 | Loss: 0.00083627
Iteration 24/25 | Loss: 0.00083627
Iteration 25/25 | Loss: 0.00083627

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.86451554
Iteration 2/25 | Loss: 0.00062553
Iteration 3/25 | Loss: 0.00062551
Iteration 4/25 | Loss: 0.00062551
Iteration 5/25 | Loss: 0.00062551
Iteration 6/25 | Loss: 0.00062551
Iteration 7/25 | Loss: 0.00062551
Iteration 8/25 | Loss: 0.00062551
Iteration 9/25 | Loss: 0.00062551
Iteration 10/25 | Loss: 0.00062551
Iteration 11/25 | Loss: 0.00062551
Iteration 12/25 | Loss: 0.00062551
Iteration 13/25 | Loss: 0.00062551
Iteration 14/25 | Loss: 0.00062551
Iteration 15/25 | Loss: 0.00062551
Iteration 16/25 | Loss: 0.00062551
Iteration 17/25 | Loss: 0.00062551
Iteration 18/25 | Loss: 0.00062551
Iteration 19/25 | Loss: 0.00062551
Iteration 20/25 | Loss: 0.00062551
Iteration 21/25 | Loss: 0.00062551
Iteration 22/25 | Loss: 0.00062551
Iteration 23/25 | Loss: 0.00062551
Iteration 24/25 | Loss: 0.00062551
Iteration 25/25 | Loss: 0.00062551

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062551
Iteration 2/1000 | Loss: 0.00007422
Iteration 3/1000 | Loss: 0.00005646
Iteration 4/1000 | Loss: 0.00004588
Iteration 5/1000 | Loss: 0.00059850
Iteration 6/1000 | Loss: 0.00031188
Iteration 7/1000 | Loss: 0.00054844
Iteration 8/1000 | Loss: 0.00047615
Iteration 9/1000 | Loss: 0.00050567
Iteration 10/1000 | Loss: 0.00201952
Iteration 11/1000 | Loss: 0.00166083
Iteration 12/1000 | Loss: 0.00144957
Iteration 13/1000 | Loss: 0.00075813
Iteration 14/1000 | Loss: 0.00183327
Iteration 15/1000 | Loss: 0.00037793
Iteration 16/1000 | Loss: 0.00010725
Iteration 17/1000 | Loss: 0.00006438
Iteration 18/1000 | Loss: 0.00166936
Iteration 19/1000 | Loss: 0.00140089
Iteration 20/1000 | Loss: 0.00113636
Iteration 21/1000 | Loss: 0.00124138
Iteration 22/1000 | Loss: 0.00195414
Iteration 23/1000 | Loss: 0.00171898
Iteration 24/1000 | Loss: 0.00063082
Iteration 25/1000 | Loss: 0.00118955
Iteration 26/1000 | Loss: 0.00041851
Iteration 27/1000 | Loss: 0.00057532
Iteration 28/1000 | Loss: 0.00100430
Iteration 29/1000 | Loss: 0.00168936
Iteration 30/1000 | Loss: 0.00049829
Iteration 31/1000 | Loss: 0.00009068
Iteration 32/1000 | Loss: 0.00004767
Iteration 33/1000 | Loss: 0.00004228
Iteration 34/1000 | Loss: 0.00006684
Iteration 35/1000 | Loss: 0.00003643
Iteration 36/1000 | Loss: 0.00051584
Iteration 37/1000 | Loss: 0.00043440
Iteration 38/1000 | Loss: 0.00041798
Iteration 39/1000 | Loss: 0.00004004
Iteration 40/1000 | Loss: 0.00030657
Iteration 41/1000 | Loss: 0.00019675
Iteration 42/1000 | Loss: 0.00036371
Iteration 43/1000 | Loss: 0.00032111
Iteration 44/1000 | Loss: 0.00034750
Iteration 45/1000 | Loss: 0.00007914
Iteration 46/1000 | Loss: 0.00003332
Iteration 47/1000 | Loss: 0.00008455
Iteration 48/1000 | Loss: 0.00002742
Iteration 49/1000 | Loss: 0.00002608
Iteration 50/1000 | Loss: 0.00009620
Iteration 51/1000 | Loss: 0.00004347
Iteration 52/1000 | Loss: 0.00003640
Iteration 53/1000 | Loss: 0.00002997
Iteration 54/1000 | Loss: 0.00002473
Iteration 55/1000 | Loss: 0.00002430
Iteration 56/1000 | Loss: 0.00002428
Iteration 57/1000 | Loss: 0.00002422
Iteration 58/1000 | Loss: 0.00002420
Iteration 59/1000 | Loss: 0.00002419
Iteration 60/1000 | Loss: 0.00002418
Iteration 61/1000 | Loss: 0.00002417
Iteration 62/1000 | Loss: 0.00002411
Iteration 63/1000 | Loss: 0.00002409
Iteration 64/1000 | Loss: 0.00002409
Iteration 65/1000 | Loss: 0.00002408
Iteration 66/1000 | Loss: 0.00002408
Iteration 67/1000 | Loss: 0.00002408
Iteration 68/1000 | Loss: 0.00002407
Iteration 69/1000 | Loss: 0.00002405
Iteration 70/1000 | Loss: 0.00002405
Iteration 71/1000 | Loss: 0.00002405
Iteration 72/1000 | Loss: 0.00002404
Iteration 73/1000 | Loss: 0.00002404
Iteration 74/1000 | Loss: 0.00002404
Iteration 75/1000 | Loss: 0.00002403
Iteration 76/1000 | Loss: 0.00002403
Iteration 77/1000 | Loss: 0.00002402
Iteration 78/1000 | Loss: 0.00002402
Iteration 79/1000 | Loss: 0.00002402
Iteration 80/1000 | Loss: 0.00002402
Iteration 81/1000 | Loss: 0.00002402
Iteration 82/1000 | Loss: 0.00002401
Iteration 83/1000 | Loss: 0.00002401
Iteration 84/1000 | Loss: 0.00002401
Iteration 85/1000 | Loss: 0.00002401
Iteration 86/1000 | Loss: 0.00002401
Iteration 87/1000 | Loss: 0.00002400
Iteration 88/1000 | Loss: 0.00002400
Iteration 89/1000 | Loss: 0.00002399
Iteration 90/1000 | Loss: 0.00002399
Iteration 91/1000 | Loss: 0.00002399
Iteration 92/1000 | Loss: 0.00002399
Iteration 93/1000 | Loss: 0.00002399
Iteration 94/1000 | Loss: 0.00002399
Iteration 95/1000 | Loss: 0.00002399
Iteration 96/1000 | Loss: 0.00002398
Iteration 97/1000 | Loss: 0.00002398
Iteration 98/1000 | Loss: 0.00002398
Iteration 99/1000 | Loss: 0.00002398
Iteration 100/1000 | Loss: 0.00002398
Iteration 101/1000 | Loss: 0.00002398
Iteration 102/1000 | Loss: 0.00002398
Iteration 103/1000 | Loss: 0.00002398
Iteration 104/1000 | Loss: 0.00002397
Iteration 105/1000 | Loss: 0.00002397
Iteration 106/1000 | Loss: 0.00002397
Iteration 107/1000 | Loss: 0.00002397
Iteration 108/1000 | Loss: 0.00002397
Iteration 109/1000 | Loss: 0.00002397
Iteration 110/1000 | Loss: 0.00002397
Iteration 111/1000 | Loss: 0.00002397
Iteration 112/1000 | Loss: 0.00002396
Iteration 113/1000 | Loss: 0.00002396
Iteration 114/1000 | Loss: 0.00002396
Iteration 115/1000 | Loss: 0.00002396
Iteration 116/1000 | Loss: 0.00002396
Iteration 117/1000 | Loss: 0.00002396
Iteration 118/1000 | Loss: 0.00002396
Iteration 119/1000 | Loss: 0.00002396
Iteration 120/1000 | Loss: 0.00002396
Iteration 121/1000 | Loss: 0.00002396
Iteration 122/1000 | Loss: 0.00002396
Iteration 123/1000 | Loss: 0.00002396
Iteration 124/1000 | Loss: 0.00002396
Iteration 125/1000 | Loss: 0.00002396
Iteration 126/1000 | Loss: 0.00002396
Iteration 127/1000 | Loss: 0.00002396
Iteration 128/1000 | Loss: 0.00002395
Iteration 129/1000 | Loss: 0.00002395
Iteration 130/1000 | Loss: 0.00002395
Iteration 131/1000 | Loss: 0.00002395
Iteration 132/1000 | Loss: 0.00002395
Iteration 133/1000 | Loss: 0.00002395
Iteration 134/1000 | Loss: 0.00002395
Iteration 135/1000 | Loss: 0.00002395
Iteration 136/1000 | Loss: 0.00002395
Iteration 137/1000 | Loss: 0.00002395
Iteration 138/1000 | Loss: 0.00002395
Iteration 139/1000 | Loss: 0.00002395
Iteration 140/1000 | Loss: 0.00002395
Iteration 141/1000 | Loss: 0.00002395
Iteration 142/1000 | Loss: 0.00002395
Iteration 143/1000 | Loss: 0.00002395
Iteration 144/1000 | Loss: 0.00002395
Iteration 145/1000 | Loss: 0.00002394
Iteration 146/1000 | Loss: 0.00002394
Iteration 147/1000 | Loss: 0.00002394
Iteration 148/1000 | Loss: 0.00002394
Iteration 149/1000 | Loss: 0.00002394
Iteration 150/1000 | Loss: 0.00002394
Iteration 151/1000 | Loss: 0.00002394
Iteration 152/1000 | Loss: 0.00002394
Iteration 153/1000 | Loss: 0.00002394
Iteration 154/1000 | Loss: 0.00002394
Iteration 155/1000 | Loss: 0.00002394
Iteration 156/1000 | Loss: 0.00002394
Iteration 157/1000 | Loss: 0.00002393
Iteration 158/1000 | Loss: 0.00002393
Iteration 159/1000 | Loss: 0.00002393
Iteration 160/1000 | Loss: 0.00002393
Iteration 161/1000 | Loss: 0.00002393
Iteration 162/1000 | Loss: 0.00002393
Iteration 163/1000 | Loss: 0.00002392
Iteration 164/1000 | Loss: 0.00002392
Iteration 165/1000 | Loss: 0.00002392
Iteration 166/1000 | Loss: 0.00002392
Iteration 167/1000 | Loss: 0.00002392
Iteration 168/1000 | Loss: 0.00002392
Iteration 169/1000 | Loss: 0.00002392
Iteration 170/1000 | Loss: 0.00002392
Iteration 171/1000 | Loss: 0.00002392
Iteration 172/1000 | Loss: 0.00002392
Iteration 173/1000 | Loss: 0.00002392
Iteration 174/1000 | Loss: 0.00002392
Iteration 175/1000 | Loss: 0.00002392
Iteration 176/1000 | Loss: 0.00002392
Iteration 177/1000 | Loss: 0.00002392
Iteration 178/1000 | Loss: 0.00002391
Iteration 179/1000 | Loss: 0.00002391
Iteration 180/1000 | Loss: 0.00002391
Iteration 181/1000 | Loss: 0.00002391
Iteration 182/1000 | Loss: 0.00002391
Iteration 183/1000 | Loss: 0.00002391
Iteration 184/1000 | Loss: 0.00002391
Iteration 185/1000 | Loss: 0.00002391
Iteration 186/1000 | Loss: 0.00002391
Iteration 187/1000 | Loss: 0.00002391
Iteration 188/1000 | Loss: 0.00002391
Iteration 189/1000 | Loss: 0.00002391
Iteration 190/1000 | Loss: 0.00002391
Iteration 191/1000 | Loss: 0.00002390
Iteration 192/1000 | Loss: 0.00002390
Iteration 193/1000 | Loss: 0.00002390
Iteration 194/1000 | Loss: 0.00002390
Iteration 195/1000 | Loss: 0.00002390
Iteration 196/1000 | Loss: 0.00002390
Iteration 197/1000 | Loss: 0.00002390
Iteration 198/1000 | Loss: 0.00002390
Iteration 199/1000 | Loss: 0.00002390
Iteration 200/1000 | Loss: 0.00002390
Iteration 201/1000 | Loss: 0.00002389
Iteration 202/1000 | Loss: 0.00002389
Iteration 203/1000 | Loss: 0.00002389
Iteration 204/1000 | Loss: 0.00002389
Iteration 205/1000 | Loss: 0.00002389
Iteration 206/1000 | Loss: 0.00002389
Iteration 207/1000 | Loss: 0.00002389
Iteration 208/1000 | Loss: 0.00002389
Iteration 209/1000 | Loss: 0.00002388
Iteration 210/1000 | Loss: 0.00002388
Iteration 211/1000 | Loss: 0.00002388
Iteration 212/1000 | Loss: 0.00002388
Iteration 213/1000 | Loss: 0.00002388
Iteration 214/1000 | Loss: 0.00002388
Iteration 215/1000 | Loss: 0.00002388
Iteration 216/1000 | Loss: 0.00002388
Iteration 217/1000 | Loss: 0.00002388
Iteration 218/1000 | Loss: 0.00002388
Iteration 219/1000 | Loss: 0.00002388
Iteration 220/1000 | Loss: 0.00002388
Iteration 221/1000 | Loss: 0.00002388
Iteration 222/1000 | Loss: 0.00002388
Iteration 223/1000 | Loss: 0.00002388
Iteration 224/1000 | Loss: 0.00002388
Iteration 225/1000 | Loss: 0.00002388
Iteration 226/1000 | Loss: 0.00002388
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [2.3881488232291304e-05, 2.3881488232291304e-05, 2.3881488232291304e-05, 2.3881488232291304e-05, 2.3881488232291304e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3881488232291304e-05

Optimization complete. Final v2v error: 3.953491687774658 mm

Highest mean error: 4.463373184204102 mm for frame 71

Lowest mean error: 3.272372007369995 mm for frame 8

Saving results

Total time: 112.27454090118408
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00681898
Iteration 2/25 | Loss: 0.00096233
Iteration 3/25 | Loss: 0.00077593
Iteration 4/25 | Loss: 0.00075330
Iteration 5/25 | Loss: 0.00074504
Iteration 6/25 | Loss: 0.00074276
Iteration 7/25 | Loss: 0.00074220
Iteration 8/25 | Loss: 0.00074220
Iteration 9/25 | Loss: 0.00074220
Iteration 10/25 | Loss: 0.00074220
Iteration 11/25 | Loss: 0.00074220
Iteration 12/25 | Loss: 0.00074220
Iteration 13/25 | Loss: 0.00074220
Iteration 14/25 | Loss: 0.00074220
Iteration 15/25 | Loss: 0.00074220
Iteration 16/25 | Loss: 0.00074220
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007421999471262097, 0.0007421999471262097, 0.0007421999471262097, 0.0007421999471262097, 0.0007421999471262097]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007421999471262097

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.13139606
Iteration 2/25 | Loss: 0.00027650
Iteration 3/25 | Loss: 0.00027650
Iteration 4/25 | Loss: 0.00027650
Iteration 5/25 | Loss: 0.00027650
Iteration 6/25 | Loss: 0.00027650
Iteration 7/25 | Loss: 0.00027650
Iteration 8/25 | Loss: 0.00027650
Iteration 9/25 | Loss: 0.00027650
Iteration 10/25 | Loss: 0.00027650
Iteration 11/25 | Loss: 0.00027650
Iteration 12/25 | Loss: 0.00027650
Iteration 13/25 | Loss: 0.00027650
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0002764972159639001, 0.0002764972159639001, 0.0002764972159639001, 0.0002764972159639001, 0.0002764972159639001]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002764972159639001

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027650
Iteration 2/1000 | Loss: 0.00003600
Iteration 3/1000 | Loss: 0.00002164
Iteration 4/1000 | Loss: 0.00001973
Iteration 5/1000 | Loss: 0.00001890
Iteration 6/1000 | Loss: 0.00001855
Iteration 7/1000 | Loss: 0.00001822
Iteration 8/1000 | Loss: 0.00001799
Iteration 9/1000 | Loss: 0.00001785
Iteration 10/1000 | Loss: 0.00001783
Iteration 11/1000 | Loss: 0.00001782
Iteration 12/1000 | Loss: 0.00001778
Iteration 13/1000 | Loss: 0.00001777
Iteration 14/1000 | Loss: 0.00001776
Iteration 15/1000 | Loss: 0.00001772
Iteration 16/1000 | Loss: 0.00001764
Iteration 17/1000 | Loss: 0.00001764
Iteration 18/1000 | Loss: 0.00001759
Iteration 19/1000 | Loss: 0.00001758
Iteration 20/1000 | Loss: 0.00001758
Iteration 21/1000 | Loss: 0.00001758
Iteration 22/1000 | Loss: 0.00001757
Iteration 23/1000 | Loss: 0.00001754
Iteration 24/1000 | Loss: 0.00001754
Iteration 25/1000 | Loss: 0.00001754
Iteration 26/1000 | Loss: 0.00001754
Iteration 27/1000 | Loss: 0.00001753
Iteration 28/1000 | Loss: 0.00001753
Iteration 29/1000 | Loss: 0.00001753
Iteration 30/1000 | Loss: 0.00001753
Iteration 31/1000 | Loss: 0.00001752
Iteration 32/1000 | Loss: 0.00001752
Iteration 33/1000 | Loss: 0.00001752
Iteration 34/1000 | Loss: 0.00001752
Iteration 35/1000 | Loss: 0.00001751
Iteration 36/1000 | Loss: 0.00001750
Iteration 37/1000 | Loss: 0.00001750
Iteration 38/1000 | Loss: 0.00001750
Iteration 39/1000 | Loss: 0.00001750
Iteration 40/1000 | Loss: 0.00001749
Iteration 41/1000 | Loss: 0.00001749
Iteration 42/1000 | Loss: 0.00001749
Iteration 43/1000 | Loss: 0.00001748
Iteration 44/1000 | Loss: 0.00001748
Iteration 45/1000 | Loss: 0.00001748
Iteration 46/1000 | Loss: 0.00001748
Iteration 47/1000 | Loss: 0.00001747
Iteration 48/1000 | Loss: 0.00001747
Iteration 49/1000 | Loss: 0.00001747
Iteration 50/1000 | Loss: 0.00001746
Iteration 51/1000 | Loss: 0.00001746
Iteration 52/1000 | Loss: 0.00001746
Iteration 53/1000 | Loss: 0.00001745
Iteration 54/1000 | Loss: 0.00001745
Iteration 55/1000 | Loss: 0.00001745
Iteration 56/1000 | Loss: 0.00001744
Iteration 57/1000 | Loss: 0.00001744
Iteration 58/1000 | Loss: 0.00001744
Iteration 59/1000 | Loss: 0.00001744
Iteration 60/1000 | Loss: 0.00001743
Iteration 61/1000 | Loss: 0.00001743
Iteration 62/1000 | Loss: 0.00001743
Iteration 63/1000 | Loss: 0.00001743
Iteration 64/1000 | Loss: 0.00001743
Iteration 65/1000 | Loss: 0.00001743
Iteration 66/1000 | Loss: 0.00001743
Iteration 67/1000 | Loss: 0.00001743
Iteration 68/1000 | Loss: 0.00001743
Iteration 69/1000 | Loss: 0.00001743
Iteration 70/1000 | Loss: 0.00001743
Iteration 71/1000 | Loss: 0.00001742
Iteration 72/1000 | Loss: 0.00001742
Iteration 73/1000 | Loss: 0.00001742
Iteration 74/1000 | Loss: 0.00001742
Iteration 75/1000 | Loss: 0.00001742
Iteration 76/1000 | Loss: 0.00001742
Iteration 77/1000 | Loss: 0.00001742
Iteration 78/1000 | Loss: 0.00001742
Iteration 79/1000 | Loss: 0.00001742
Iteration 80/1000 | Loss: 0.00001741
Iteration 81/1000 | Loss: 0.00001741
Iteration 82/1000 | Loss: 0.00001741
Iteration 83/1000 | Loss: 0.00001741
Iteration 84/1000 | Loss: 0.00001741
Iteration 85/1000 | Loss: 0.00001741
Iteration 86/1000 | Loss: 0.00001741
Iteration 87/1000 | Loss: 0.00001741
Iteration 88/1000 | Loss: 0.00001741
Iteration 89/1000 | Loss: 0.00001741
Iteration 90/1000 | Loss: 0.00001741
Iteration 91/1000 | Loss: 0.00001740
Iteration 92/1000 | Loss: 0.00001740
Iteration 93/1000 | Loss: 0.00001740
Iteration 94/1000 | Loss: 0.00001740
Iteration 95/1000 | Loss: 0.00001740
Iteration 96/1000 | Loss: 0.00001740
Iteration 97/1000 | Loss: 0.00001740
Iteration 98/1000 | Loss: 0.00001740
Iteration 99/1000 | Loss: 0.00001740
Iteration 100/1000 | Loss: 0.00001740
Iteration 101/1000 | Loss: 0.00001740
Iteration 102/1000 | Loss: 0.00001740
Iteration 103/1000 | Loss: 0.00001739
Iteration 104/1000 | Loss: 0.00001739
Iteration 105/1000 | Loss: 0.00001739
Iteration 106/1000 | Loss: 0.00001739
Iteration 107/1000 | Loss: 0.00001739
Iteration 108/1000 | Loss: 0.00001739
Iteration 109/1000 | Loss: 0.00001739
Iteration 110/1000 | Loss: 0.00001739
Iteration 111/1000 | Loss: 0.00001739
Iteration 112/1000 | Loss: 0.00001739
Iteration 113/1000 | Loss: 0.00001739
Iteration 114/1000 | Loss: 0.00001739
Iteration 115/1000 | Loss: 0.00001739
Iteration 116/1000 | Loss: 0.00001739
Iteration 117/1000 | Loss: 0.00001739
Iteration 118/1000 | Loss: 0.00001739
Iteration 119/1000 | Loss: 0.00001739
Iteration 120/1000 | Loss: 0.00001739
Iteration 121/1000 | Loss: 0.00001739
Iteration 122/1000 | Loss: 0.00001739
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.7392898371326737e-05, 1.7392898371326737e-05, 1.7392898371326737e-05, 1.7392898371326737e-05, 1.7392898371326737e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7392898371326737e-05

Optimization complete. Final v2v error: 3.54931902885437 mm

Highest mean error: 4.078716278076172 mm for frame 61

Lowest mean error: 3.2904531955718994 mm for frame 3

Saving results

Total time: 33.30711102485657
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01105256
Iteration 2/25 | Loss: 0.00157739
Iteration 3/25 | Loss: 0.00098106
Iteration 4/25 | Loss: 0.00086923
Iteration 5/25 | Loss: 0.00084271
Iteration 6/25 | Loss: 0.00083563
Iteration 7/25 | Loss: 0.00083330
Iteration 8/25 | Loss: 0.00083283
Iteration 9/25 | Loss: 0.00083283
Iteration 10/25 | Loss: 0.00083283
Iteration 11/25 | Loss: 0.00083283
Iteration 12/25 | Loss: 0.00083283
Iteration 13/25 | Loss: 0.00083283
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.000832830963190645, 0.000832830963190645, 0.000832830963190645, 0.000832830963190645, 0.000832830963190645]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000832830963190645

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.71615922
Iteration 2/25 | Loss: 0.00053624
Iteration 3/25 | Loss: 0.00053612
Iteration 4/25 | Loss: 0.00053612
Iteration 5/25 | Loss: 0.00053612
Iteration 6/25 | Loss: 0.00053612
Iteration 7/25 | Loss: 0.00053612
Iteration 8/25 | Loss: 0.00053612
Iteration 9/25 | Loss: 0.00053612
Iteration 10/25 | Loss: 0.00053612
Iteration 11/25 | Loss: 0.00053612
Iteration 12/25 | Loss: 0.00053612
Iteration 13/25 | Loss: 0.00053612
Iteration 14/25 | Loss: 0.00053612
Iteration 15/25 | Loss: 0.00053612
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0005361202638596296, 0.0005361202638596296, 0.0005361202638596296, 0.0005361202638596296, 0.0005361202638596296]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005361202638596296

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053612
Iteration 2/1000 | Loss: 0.00005453
Iteration 3/1000 | Loss: 0.00003624
Iteration 4/1000 | Loss: 0.00003207
Iteration 5/1000 | Loss: 0.00003029
Iteration 6/1000 | Loss: 0.00002912
Iteration 7/1000 | Loss: 0.00002843
Iteration 8/1000 | Loss: 0.00002809
Iteration 9/1000 | Loss: 0.00002773
Iteration 10/1000 | Loss: 0.00002747
Iteration 11/1000 | Loss: 0.00002733
Iteration 12/1000 | Loss: 0.00002720
Iteration 13/1000 | Loss: 0.00002719
Iteration 14/1000 | Loss: 0.00002710
Iteration 15/1000 | Loss: 0.00002703
Iteration 16/1000 | Loss: 0.00002703
Iteration 17/1000 | Loss: 0.00002698
Iteration 18/1000 | Loss: 0.00002698
Iteration 19/1000 | Loss: 0.00002696
Iteration 20/1000 | Loss: 0.00002692
Iteration 21/1000 | Loss: 0.00002691
Iteration 22/1000 | Loss: 0.00002690
Iteration 23/1000 | Loss: 0.00002690
Iteration 24/1000 | Loss: 0.00002689
Iteration 25/1000 | Loss: 0.00002689
Iteration 26/1000 | Loss: 0.00002688
Iteration 27/1000 | Loss: 0.00002687
Iteration 28/1000 | Loss: 0.00002686
Iteration 29/1000 | Loss: 0.00002686
Iteration 30/1000 | Loss: 0.00002685
Iteration 31/1000 | Loss: 0.00002685
Iteration 32/1000 | Loss: 0.00002684
Iteration 33/1000 | Loss: 0.00002684
Iteration 34/1000 | Loss: 0.00002684
Iteration 35/1000 | Loss: 0.00002682
Iteration 36/1000 | Loss: 0.00002682
Iteration 37/1000 | Loss: 0.00002681
Iteration 38/1000 | Loss: 0.00002681
Iteration 39/1000 | Loss: 0.00002681
Iteration 40/1000 | Loss: 0.00002681
Iteration 41/1000 | Loss: 0.00002680
Iteration 42/1000 | Loss: 0.00002680
Iteration 43/1000 | Loss: 0.00002679
Iteration 44/1000 | Loss: 0.00002679
Iteration 45/1000 | Loss: 0.00002679
Iteration 46/1000 | Loss: 0.00002678
Iteration 47/1000 | Loss: 0.00002678
Iteration 48/1000 | Loss: 0.00002678
Iteration 49/1000 | Loss: 0.00002678
Iteration 50/1000 | Loss: 0.00002678
Iteration 51/1000 | Loss: 0.00002678
Iteration 52/1000 | Loss: 0.00002678
Iteration 53/1000 | Loss: 0.00002678
Iteration 54/1000 | Loss: 0.00002678
Iteration 55/1000 | Loss: 0.00002677
Iteration 56/1000 | Loss: 0.00002677
Iteration 57/1000 | Loss: 0.00002677
Iteration 58/1000 | Loss: 0.00002677
Iteration 59/1000 | Loss: 0.00002676
Iteration 60/1000 | Loss: 0.00002676
Iteration 61/1000 | Loss: 0.00002676
Iteration 62/1000 | Loss: 0.00002675
Iteration 63/1000 | Loss: 0.00002675
Iteration 64/1000 | Loss: 0.00002675
Iteration 65/1000 | Loss: 0.00002675
Iteration 66/1000 | Loss: 0.00002675
Iteration 67/1000 | Loss: 0.00002675
Iteration 68/1000 | Loss: 0.00002675
Iteration 69/1000 | Loss: 0.00002675
Iteration 70/1000 | Loss: 0.00002675
Iteration 71/1000 | Loss: 0.00002675
Iteration 72/1000 | Loss: 0.00002674
Iteration 73/1000 | Loss: 0.00002674
Iteration 74/1000 | Loss: 0.00002674
Iteration 75/1000 | Loss: 0.00002673
Iteration 76/1000 | Loss: 0.00002673
Iteration 77/1000 | Loss: 0.00002673
Iteration 78/1000 | Loss: 0.00002673
Iteration 79/1000 | Loss: 0.00002673
Iteration 80/1000 | Loss: 0.00002672
Iteration 81/1000 | Loss: 0.00002672
Iteration 82/1000 | Loss: 0.00002672
Iteration 83/1000 | Loss: 0.00002671
Iteration 84/1000 | Loss: 0.00002671
Iteration 85/1000 | Loss: 0.00002671
Iteration 86/1000 | Loss: 0.00002671
Iteration 87/1000 | Loss: 0.00002671
Iteration 88/1000 | Loss: 0.00002670
Iteration 89/1000 | Loss: 0.00002670
Iteration 90/1000 | Loss: 0.00002669
Iteration 91/1000 | Loss: 0.00002669
Iteration 92/1000 | Loss: 0.00002669
Iteration 93/1000 | Loss: 0.00002669
Iteration 94/1000 | Loss: 0.00002669
Iteration 95/1000 | Loss: 0.00002669
Iteration 96/1000 | Loss: 0.00002669
Iteration 97/1000 | Loss: 0.00002668
Iteration 98/1000 | Loss: 0.00002668
Iteration 99/1000 | Loss: 0.00002668
Iteration 100/1000 | Loss: 0.00002667
Iteration 101/1000 | Loss: 0.00002667
Iteration 102/1000 | Loss: 0.00002667
Iteration 103/1000 | Loss: 0.00002667
Iteration 104/1000 | Loss: 0.00002666
Iteration 105/1000 | Loss: 0.00002666
Iteration 106/1000 | Loss: 0.00002666
Iteration 107/1000 | Loss: 0.00002666
Iteration 108/1000 | Loss: 0.00002666
Iteration 109/1000 | Loss: 0.00002666
Iteration 110/1000 | Loss: 0.00002666
Iteration 111/1000 | Loss: 0.00002666
Iteration 112/1000 | Loss: 0.00002666
Iteration 113/1000 | Loss: 0.00002666
Iteration 114/1000 | Loss: 0.00002666
Iteration 115/1000 | Loss: 0.00002665
Iteration 116/1000 | Loss: 0.00002665
Iteration 117/1000 | Loss: 0.00002665
Iteration 118/1000 | Loss: 0.00002665
Iteration 119/1000 | Loss: 0.00002665
Iteration 120/1000 | Loss: 0.00002664
Iteration 121/1000 | Loss: 0.00002664
Iteration 122/1000 | Loss: 0.00002664
Iteration 123/1000 | Loss: 0.00002663
Iteration 124/1000 | Loss: 0.00002663
Iteration 125/1000 | Loss: 0.00002663
Iteration 126/1000 | Loss: 0.00002663
Iteration 127/1000 | Loss: 0.00002662
Iteration 128/1000 | Loss: 0.00002662
Iteration 129/1000 | Loss: 0.00002662
Iteration 130/1000 | Loss: 0.00002662
Iteration 131/1000 | Loss: 0.00002661
Iteration 132/1000 | Loss: 0.00002661
Iteration 133/1000 | Loss: 0.00002661
Iteration 134/1000 | Loss: 0.00002661
Iteration 135/1000 | Loss: 0.00002661
Iteration 136/1000 | Loss: 0.00002661
Iteration 137/1000 | Loss: 0.00002660
Iteration 138/1000 | Loss: 0.00002660
Iteration 139/1000 | Loss: 0.00002660
Iteration 140/1000 | Loss: 0.00002659
Iteration 141/1000 | Loss: 0.00002659
Iteration 142/1000 | Loss: 0.00002659
Iteration 143/1000 | Loss: 0.00002659
Iteration 144/1000 | Loss: 0.00002659
Iteration 145/1000 | Loss: 0.00002658
Iteration 146/1000 | Loss: 0.00002658
Iteration 147/1000 | Loss: 0.00002658
Iteration 148/1000 | Loss: 0.00002657
Iteration 149/1000 | Loss: 0.00002657
Iteration 150/1000 | Loss: 0.00002657
Iteration 151/1000 | Loss: 0.00002657
Iteration 152/1000 | Loss: 0.00002657
Iteration 153/1000 | Loss: 0.00002657
Iteration 154/1000 | Loss: 0.00002657
Iteration 155/1000 | Loss: 0.00002657
Iteration 156/1000 | Loss: 0.00002657
Iteration 157/1000 | Loss: 0.00002657
Iteration 158/1000 | Loss: 0.00002657
Iteration 159/1000 | Loss: 0.00002657
Iteration 160/1000 | Loss: 0.00002656
Iteration 161/1000 | Loss: 0.00002656
Iteration 162/1000 | Loss: 0.00002656
Iteration 163/1000 | Loss: 0.00002656
Iteration 164/1000 | Loss: 0.00002656
Iteration 165/1000 | Loss: 0.00002656
Iteration 166/1000 | Loss: 0.00002656
Iteration 167/1000 | Loss: 0.00002656
Iteration 168/1000 | Loss: 0.00002656
Iteration 169/1000 | Loss: 0.00002656
Iteration 170/1000 | Loss: 0.00002656
Iteration 171/1000 | Loss: 0.00002656
Iteration 172/1000 | Loss: 0.00002655
Iteration 173/1000 | Loss: 0.00002655
Iteration 174/1000 | Loss: 0.00002655
Iteration 175/1000 | Loss: 0.00002655
Iteration 176/1000 | Loss: 0.00002654
Iteration 177/1000 | Loss: 0.00002654
Iteration 178/1000 | Loss: 0.00002654
Iteration 179/1000 | Loss: 0.00002654
Iteration 180/1000 | Loss: 0.00002654
Iteration 181/1000 | Loss: 0.00002654
Iteration 182/1000 | Loss: 0.00002654
Iteration 183/1000 | Loss: 0.00002654
Iteration 184/1000 | Loss: 0.00002654
Iteration 185/1000 | Loss: 0.00002654
Iteration 186/1000 | Loss: 0.00002654
Iteration 187/1000 | Loss: 0.00002654
Iteration 188/1000 | Loss: 0.00002654
Iteration 189/1000 | Loss: 0.00002654
Iteration 190/1000 | Loss: 0.00002654
Iteration 191/1000 | Loss: 0.00002654
Iteration 192/1000 | Loss: 0.00002654
Iteration 193/1000 | Loss: 0.00002654
Iteration 194/1000 | Loss: 0.00002654
Iteration 195/1000 | Loss: 0.00002654
Iteration 196/1000 | Loss: 0.00002654
Iteration 197/1000 | Loss: 0.00002654
Iteration 198/1000 | Loss: 0.00002654
Iteration 199/1000 | Loss: 0.00002654
Iteration 200/1000 | Loss: 0.00002654
Iteration 201/1000 | Loss: 0.00002654
Iteration 202/1000 | Loss: 0.00002654
Iteration 203/1000 | Loss: 0.00002654
Iteration 204/1000 | Loss: 0.00002654
Iteration 205/1000 | Loss: 0.00002654
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [2.6542587875155732e-05, 2.6542587875155732e-05, 2.6542587875155732e-05, 2.6542587875155732e-05, 2.6542587875155732e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6542587875155732e-05

Optimization complete. Final v2v error: 4.178531169891357 mm

Highest mean error: 5.343278408050537 mm for frame 211

Lowest mean error: 3.283111095428467 mm for frame 68

Saving results

Total time: 49.89454984664917
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01070723
Iteration 2/25 | Loss: 0.00834219
Iteration 3/25 | Loss: 0.00306358
Iteration 4/25 | Loss: 0.00310533
Iteration 5/25 | Loss: 0.00166919
Iteration 6/25 | Loss: 0.00145544
Iteration 7/25 | Loss: 0.00119388
Iteration 8/25 | Loss: 0.00115359
Iteration 9/25 | Loss: 0.00105683
Iteration 10/25 | Loss: 0.00095913
Iteration 11/25 | Loss: 0.00095330
Iteration 12/25 | Loss: 0.00087749
Iteration 13/25 | Loss: 0.00083438
Iteration 14/25 | Loss: 0.00078698
Iteration 15/25 | Loss: 0.00078111
Iteration 16/25 | Loss: 0.00076016
Iteration 17/25 | Loss: 0.00075660
Iteration 18/25 | Loss: 0.00075118
Iteration 19/25 | Loss: 0.00074991
Iteration 20/25 | Loss: 0.00075205
Iteration 21/25 | Loss: 0.00074854
Iteration 22/25 | Loss: 0.00074790
Iteration 23/25 | Loss: 0.00074772
Iteration 24/25 | Loss: 0.00074764
Iteration 25/25 | Loss: 0.00074764

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45178366
Iteration 2/25 | Loss: 0.00037707
Iteration 3/25 | Loss: 0.00037707
Iteration 4/25 | Loss: 0.00037707
Iteration 5/25 | Loss: 0.00037707
Iteration 6/25 | Loss: 0.00037707
Iteration 7/25 | Loss: 0.00037707
Iteration 8/25 | Loss: 0.00037707
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 8. Stopping optimization.
Last 5 losses: [0.00037707138108089566, 0.00037707138108089566, 0.00037707138108089566, 0.00037707138108089566, 0.00037707138108089566]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00037707138108089566

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037707
Iteration 2/1000 | Loss: 0.00024767
Iteration 3/1000 | Loss: 0.00004057
Iteration 4/1000 | Loss: 0.00008200
Iteration 5/1000 | Loss: 0.00002952
Iteration 6/1000 | Loss: 0.00002375
Iteration 7/1000 | Loss: 0.00002249
Iteration 8/1000 | Loss: 0.00002166
Iteration 9/1000 | Loss: 0.00002114
Iteration 10/1000 | Loss: 0.00008371
Iteration 11/1000 | Loss: 0.00002727
Iteration 12/1000 | Loss: 0.00002362
Iteration 13/1000 | Loss: 0.00002068
Iteration 14/1000 | Loss: 0.00002043
Iteration 15/1000 | Loss: 0.00002039
Iteration 16/1000 | Loss: 0.00002038
Iteration 17/1000 | Loss: 0.00002030
Iteration 18/1000 | Loss: 0.00002025
Iteration 19/1000 | Loss: 0.00002025
Iteration 20/1000 | Loss: 0.00002023
Iteration 21/1000 | Loss: 0.00002023
Iteration 22/1000 | Loss: 0.00002023
Iteration 23/1000 | Loss: 0.00002022
Iteration 24/1000 | Loss: 0.00002022
Iteration 25/1000 | Loss: 0.00002022
Iteration 26/1000 | Loss: 0.00002021
Iteration 27/1000 | Loss: 0.00002021
Iteration 28/1000 | Loss: 0.00002019
Iteration 29/1000 | Loss: 0.00002018
Iteration 30/1000 | Loss: 0.00002018
Iteration 31/1000 | Loss: 0.00011155
Iteration 32/1000 | Loss: 0.00019466
Iteration 33/1000 | Loss: 0.00002011
Iteration 34/1000 | Loss: 0.00002007
Iteration 35/1000 | Loss: 0.00002005
Iteration 36/1000 | Loss: 0.00010396
Iteration 37/1000 | Loss: 0.00002510
Iteration 38/1000 | Loss: 0.00002256
Iteration 39/1000 | Loss: 0.00002016
Iteration 40/1000 | Loss: 0.00002003
Iteration 41/1000 | Loss: 0.00002003
Iteration 42/1000 | Loss: 0.00002001
Iteration 43/1000 | Loss: 0.00002001
Iteration 44/1000 | Loss: 0.00002000
Iteration 45/1000 | Loss: 0.00001999
Iteration 46/1000 | Loss: 0.00001999
Iteration 47/1000 | Loss: 0.00001999
Iteration 48/1000 | Loss: 0.00001999
Iteration 49/1000 | Loss: 0.00001999
Iteration 50/1000 | Loss: 0.00001998
Iteration 51/1000 | Loss: 0.00001998
Iteration 52/1000 | Loss: 0.00001998
Iteration 53/1000 | Loss: 0.00001998
Iteration 54/1000 | Loss: 0.00001998
Iteration 55/1000 | Loss: 0.00001998
Iteration 56/1000 | Loss: 0.00001998
Iteration 57/1000 | Loss: 0.00001998
Iteration 58/1000 | Loss: 0.00001998
Iteration 59/1000 | Loss: 0.00001998
Iteration 60/1000 | Loss: 0.00001998
Iteration 61/1000 | Loss: 0.00001998
Iteration 62/1000 | Loss: 0.00001998
Iteration 63/1000 | Loss: 0.00001998
Iteration 64/1000 | Loss: 0.00001998
Iteration 65/1000 | Loss: 0.00001998
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 65. Stopping optimization.
Last 5 losses: [1.9979817807325162e-05, 1.9979817807325162e-05, 1.9979817807325162e-05, 1.9979817807325162e-05, 1.9979817807325162e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9979817807325162e-05

Optimization complete. Final v2v error: 3.81457781791687 mm

Highest mean error: 4.3192524909973145 mm for frame 20

Lowest mean error: 3.5516107082366943 mm for frame 103

Saving results

Total time: 83.97116088867188
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00944015
Iteration 2/25 | Loss: 0.00088049
Iteration 3/25 | Loss: 0.00068704
Iteration 4/25 | Loss: 0.00066412
Iteration 5/25 | Loss: 0.00065953
Iteration 6/25 | Loss: 0.00065788
Iteration 7/25 | Loss: 0.00065752
Iteration 8/25 | Loss: 0.00065752
Iteration 9/25 | Loss: 0.00065752
Iteration 10/25 | Loss: 0.00065752
Iteration 11/25 | Loss: 0.00065752
Iteration 12/25 | Loss: 0.00065752
Iteration 13/25 | Loss: 0.00065752
Iteration 14/25 | Loss: 0.00065752
Iteration 15/25 | Loss: 0.00065752
Iteration 16/25 | Loss: 0.00065752
Iteration 17/25 | Loss: 0.00065752
Iteration 18/25 | Loss: 0.00065752
Iteration 19/25 | Loss: 0.00065752
Iteration 20/25 | Loss: 0.00065752
Iteration 21/25 | Loss: 0.00065752
Iteration 22/25 | Loss: 0.00065752
Iteration 23/25 | Loss: 0.00065752
Iteration 24/25 | Loss: 0.00065752
Iteration 25/25 | Loss: 0.00065752

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27382863
Iteration 2/25 | Loss: 0.00036121
Iteration 3/25 | Loss: 0.00036119
Iteration 4/25 | Loss: 0.00036119
Iteration 5/25 | Loss: 0.00036119
Iteration 6/25 | Loss: 0.00036119
Iteration 7/25 | Loss: 0.00036119
Iteration 8/25 | Loss: 0.00036119
Iteration 9/25 | Loss: 0.00036119
Iteration 10/25 | Loss: 0.00036119
Iteration 11/25 | Loss: 0.00036119
Iteration 12/25 | Loss: 0.00036119
Iteration 13/25 | Loss: 0.00036119
Iteration 14/25 | Loss: 0.00036119
Iteration 15/25 | Loss: 0.00036119
Iteration 16/25 | Loss: 0.00036119
Iteration 17/25 | Loss: 0.00036119
Iteration 18/25 | Loss: 0.00036119
Iteration 19/25 | Loss: 0.00036119
Iteration 20/25 | Loss: 0.00036119
Iteration 21/25 | Loss: 0.00036119
Iteration 22/25 | Loss: 0.00036119
Iteration 23/25 | Loss: 0.00036119
Iteration 24/25 | Loss: 0.00036119
Iteration 25/25 | Loss: 0.00036119

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036119
Iteration 2/1000 | Loss: 0.00003049
Iteration 3/1000 | Loss: 0.00001783
Iteration 4/1000 | Loss: 0.00001501
Iteration 5/1000 | Loss: 0.00001427
Iteration 6/1000 | Loss: 0.00001384
Iteration 7/1000 | Loss: 0.00001358
Iteration 8/1000 | Loss: 0.00001353
Iteration 9/1000 | Loss: 0.00001352
Iteration 10/1000 | Loss: 0.00001348
Iteration 11/1000 | Loss: 0.00001348
Iteration 12/1000 | Loss: 0.00001347
Iteration 13/1000 | Loss: 0.00001326
Iteration 14/1000 | Loss: 0.00001322
Iteration 15/1000 | Loss: 0.00001314
Iteration 16/1000 | Loss: 0.00001307
Iteration 17/1000 | Loss: 0.00001303
Iteration 18/1000 | Loss: 0.00001303
Iteration 19/1000 | Loss: 0.00001298
Iteration 20/1000 | Loss: 0.00001297
Iteration 21/1000 | Loss: 0.00001297
Iteration 22/1000 | Loss: 0.00001296
Iteration 23/1000 | Loss: 0.00001296
Iteration 24/1000 | Loss: 0.00001286
Iteration 25/1000 | Loss: 0.00001286
Iteration 26/1000 | Loss: 0.00001284
Iteration 27/1000 | Loss: 0.00001284
Iteration 28/1000 | Loss: 0.00001282
Iteration 29/1000 | Loss: 0.00001282
Iteration 30/1000 | Loss: 0.00001282
Iteration 31/1000 | Loss: 0.00001282
Iteration 32/1000 | Loss: 0.00001281
Iteration 33/1000 | Loss: 0.00001281
Iteration 34/1000 | Loss: 0.00001279
Iteration 35/1000 | Loss: 0.00001279
Iteration 36/1000 | Loss: 0.00001279
Iteration 37/1000 | Loss: 0.00001278
Iteration 38/1000 | Loss: 0.00001278
Iteration 39/1000 | Loss: 0.00001277
Iteration 40/1000 | Loss: 0.00001272
Iteration 41/1000 | Loss: 0.00001269
Iteration 42/1000 | Loss: 0.00001269
Iteration 43/1000 | Loss: 0.00001269
Iteration 44/1000 | Loss: 0.00001268
Iteration 45/1000 | Loss: 0.00001268
Iteration 46/1000 | Loss: 0.00001267
Iteration 47/1000 | Loss: 0.00001267
Iteration 48/1000 | Loss: 0.00001266
Iteration 49/1000 | Loss: 0.00001266
Iteration 50/1000 | Loss: 0.00001266
Iteration 51/1000 | Loss: 0.00001265
Iteration 52/1000 | Loss: 0.00001265
Iteration 53/1000 | Loss: 0.00001265
Iteration 54/1000 | Loss: 0.00001265
Iteration 55/1000 | Loss: 0.00001265
Iteration 56/1000 | Loss: 0.00001265
Iteration 57/1000 | Loss: 0.00001265
Iteration 58/1000 | Loss: 0.00001264
Iteration 59/1000 | Loss: 0.00001264
Iteration 60/1000 | Loss: 0.00001264
Iteration 61/1000 | Loss: 0.00001264
Iteration 62/1000 | Loss: 0.00001264
Iteration 63/1000 | Loss: 0.00001263
Iteration 64/1000 | Loss: 0.00001263
Iteration 65/1000 | Loss: 0.00001263
Iteration 66/1000 | Loss: 0.00001263
Iteration 67/1000 | Loss: 0.00001263
Iteration 68/1000 | Loss: 0.00001262
Iteration 69/1000 | Loss: 0.00001262
Iteration 70/1000 | Loss: 0.00001262
Iteration 71/1000 | Loss: 0.00001262
Iteration 72/1000 | Loss: 0.00001262
Iteration 73/1000 | Loss: 0.00001262
Iteration 74/1000 | Loss: 0.00001261
Iteration 75/1000 | Loss: 0.00001261
Iteration 76/1000 | Loss: 0.00001261
Iteration 77/1000 | Loss: 0.00001261
Iteration 78/1000 | Loss: 0.00001261
Iteration 79/1000 | Loss: 0.00001261
Iteration 80/1000 | Loss: 0.00001261
Iteration 81/1000 | Loss: 0.00001261
Iteration 82/1000 | Loss: 0.00001261
Iteration 83/1000 | Loss: 0.00001261
Iteration 84/1000 | Loss: 0.00001261
Iteration 85/1000 | Loss: 0.00001261
Iteration 86/1000 | Loss: 0.00001261
Iteration 87/1000 | Loss: 0.00001261
Iteration 88/1000 | Loss: 0.00001261
Iteration 89/1000 | Loss: 0.00001261
Iteration 90/1000 | Loss: 0.00001261
Iteration 91/1000 | Loss: 0.00001261
Iteration 92/1000 | Loss: 0.00001261
Iteration 93/1000 | Loss: 0.00001261
Iteration 94/1000 | Loss: 0.00001261
Iteration 95/1000 | Loss: 0.00001261
Iteration 96/1000 | Loss: 0.00001261
Iteration 97/1000 | Loss: 0.00001261
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [1.2608463293872774e-05, 1.2608463293872774e-05, 1.2608463293872774e-05, 1.2608463293872774e-05, 1.2608463293872774e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2608463293872774e-05

Optimization complete. Final v2v error: 3.023519992828369 mm

Highest mean error: 3.1561636924743652 mm for frame 8

Lowest mean error: 2.8957438468933105 mm for frame 102

Saving results

Total time: 32.256648778915405
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00986842
Iteration 2/25 | Loss: 0.00282981
Iteration 3/25 | Loss: 0.00206119
Iteration 4/25 | Loss: 0.00168035
Iteration 5/25 | Loss: 0.00149461
Iteration 6/25 | Loss: 0.00143874
Iteration 7/25 | Loss: 0.00135145
Iteration 8/25 | Loss: 0.00132424
Iteration 9/25 | Loss: 0.00128235
Iteration 10/25 | Loss: 0.00122401
Iteration 11/25 | Loss: 0.00118659
Iteration 12/25 | Loss: 0.00114457
Iteration 13/25 | Loss: 0.00110857
Iteration 14/25 | Loss: 0.00108792
Iteration 15/25 | Loss: 0.00108135
Iteration 16/25 | Loss: 0.00108947
Iteration 17/25 | Loss: 0.00104292
Iteration 18/25 | Loss: 0.00102070
Iteration 19/25 | Loss: 0.00100417
Iteration 20/25 | Loss: 0.00101601
Iteration 21/25 | Loss: 0.00099616
Iteration 22/25 | Loss: 0.00099233
Iteration 23/25 | Loss: 0.00099198
Iteration 24/25 | Loss: 0.00099171
Iteration 25/25 | Loss: 0.00097529

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45997679
Iteration 2/25 | Loss: 0.00305743
Iteration 3/25 | Loss: 0.00305742
Iteration 4/25 | Loss: 0.00305742
Iteration 5/25 | Loss: 0.00305742
Iteration 6/25 | Loss: 0.00305742
Iteration 7/25 | Loss: 0.00305742
Iteration 8/25 | Loss: 0.00305742
Iteration 9/25 | Loss: 0.00305742
Iteration 10/25 | Loss: 0.00305742
Iteration 11/25 | Loss: 0.00305742
Iteration 12/25 | Loss: 0.00305742
Iteration 13/25 | Loss: 0.00305742
Iteration 14/25 | Loss: 0.00305742
Iteration 15/25 | Loss: 0.00305742
Iteration 16/25 | Loss: 0.00305742
Iteration 17/25 | Loss: 0.00305742
Iteration 18/25 | Loss: 0.00305742
Iteration 19/25 | Loss: 0.00305742
Iteration 20/25 | Loss: 0.00305742
Iteration 21/25 | Loss: 0.00305742
Iteration 22/25 | Loss: 0.00305742
Iteration 23/25 | Loss: 0.00305742
Iteration 24/25 | Loss: 0.00305742
Iteration 25/25 | Loss: 0.00305742

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00305742
Iteration 2/1000 | Loss: 0.00225437
Iteration 3/1000 | Loss: 0.00098150
Iteration 4/1000 | Loss: 0.00201600
Iteration 5/1000 | Loss: 0.00040081
Iteration 6/1000 | Loss: 0.00127171
Iteration 7/1000 | Loss: 0.00099853
Iteration 8/1000 | Loss: 0.00130420
Iteration 9/1000 | Loss: 0.00100358
Iteration 10/1000 | Loss: 0.00075356
Iteration 11/1000 | Loss: 0.00019322
Iteration 12/1000 | Loss: 0.00046937
Iteration 13/1000 | Loss: 0.00042123
Iteration 14/1000 | Loss: 0.00107491
Iteration 15/1000 | Loss: 0.00057319
Iteration 16/1000 | Loss: 0.00017122
Iteration 17/1000 | Loss: 0.00026965
Iteration 18/1000 | Loss: 0.00022491
Iteration 19/1000 | Loss: 0.00035521
Iteration 20/1000 | Loss: 0.00011460
Iteration 21/1000 | Loss: 0.00066942
Iteration 22/1000 | Loss: 0.00020357
Iteration 23/1000 | Loss: 0.00015461
Iteration 24/1000 | Loss: 0.00125584
Iteration 25/1000 | Loss: 0.00464398
Iteration 26/1000 | Loss: 0.00491955
Iteration 27/1000 | Loss: 0.00432696
Iteration 28/1000 | Loss: 0.00658205
Iteration 29/1000 | Loss: 0.00495430
Iteration 30/1000 | Loss: 0.00271640
Iteration 31/1000 | Loss: 0.00208161
Iteration 32/1000 | Loss: 0.00175127
Iteration 33/1000 | Loss: 0.00155817
Iteration 34/1000 | Loss: 0.00021973
Iteration 35/1000 | Loss: 0.00018513
Iteration 36/1000 | Loss: 0.00179993
Iteration 37/1000 | Loss: 0.00128310
Iteration 38/1000 | Loss: 0.00153123
Iteration 39/1000 | Loss: 0.00263674
Iteration 40/1000 | Loss: 0.00211281
Iteration 41/1000 | Loss: 0.00013088
Iteration 42/1000 | Loss: 0.00008018
Iteration 43/1000 | Loss: 0.00006463
Iteration 44/1000 | Loss: 0.00029074
Iteration 45/1000 | Loss: 0.00005607
Iteration 46/1000 | Loss: 0.00011699
Iteration 47/1000 | Loss: 0.00016658
Iteration 48/1000 | Loss: 0.00010986
Iteration 49/1000 | Loss: 0.00004569
Iteration 50/1000 | Loss: 0.00011911
Iteration 51/1000 | Loss: 0.00004259
Iteration 52/1000 | Loss: 0.00004140
Iteration 53/1000 | Loss: 0.00012453
Iteration 54/1000 | Loss: 0.00015763
Iteration 55/1000 | Loss: 0.00003983
Iteration 56/1000 | Loss: 0.00012065
Iteration 57/1000 | Loss: 0.00003899
Iteration 58/1000 | Loss: 0.00003865
Iteration 59/1000 | Loss: 0.00003837
Iteration 60/1000 | Loss: 0.00008904
Iteration 61/1000 | Loss: 0.00003820
Iteration 62/1000 | Loss: 0.00003799
Iteration 63/1000 | Loss: 0.00012834
Iteration 64/1000 | Loss: 0.00003799
Iteration 65/1000 | Loss: 0.00003782
Iteration 66/1000 | Loss: 0.00003777
Iteration 67/1000 | Loss: 0.00003776
Iteration 68/1000 | Loss: 0.00003776
Iteration 69/1000 | Loss: 0.00003776
Iteration 70/1000 | Loss: 0.00003776
Iteration 71/1000 | Loss: 0.00003776
Iteration 72/1000 | Loss: 0.00003776
Iteration 73/1000 | Loss: 0.00003776
Iteration 74/1000 | Loss: 0.00003775
Iteration 75/1000 | Loss: 0.00003775
Iteration 76/1000 | Loss: 0.00003775
Iteration 77/1000 | Loss: 0.00003774
Iteration 78/1000 | Loss: 0.00003772
Iteration 79/1000 | Loss: 0.00003771
Iteration 80/1000 | Loss: 0.00003768
Iteration 81/1000 | Loss: 0.00003751
Iteration 82/1000 | Loss: 0.00003745
Iteration 83/1000 | Loss: 0.00011839
Iteration 84/1000 | Loss: 0.00003737
Iteration 85/1000 | Loss: 0.00012139
Iteration 86/1000 | Loss: 0.00003719
Iteration 87/1000 | Loss: 0.00003713
Iteration 88/1000 | Loss: 0.00003712
Iteration 89/1000 | Loss: 0.00003712
Iteration 90/1000 | Loss: 0.00003711
Iteration 91/1000 | Loss: 0.00003710
Iteration 92/1000 | Loss: 0.00003707
Iteration 93/1000 | Loss: 0.00003706
Iteration 94/1000 | Loss: 0.00003706
Iteration 95/1000 | Loss: 0.00003705
Iteration 96/1000 | Loss: 0.00003705
Iteration 97/1000 | Loss: 0.00003705
Iteration 98/1000 | Loss: 0.00003705
Iteration 99/1000 | Loss: 0.00003704
Iteration 100/1000 | Loss: 0.00003704
Iteration 101/1000 | Loss: 0.00003704
Iteration 102/1000 | Loss: 0.00003704
Iteration 103/1000 | Loss: 0.00003703
Iteration 104/1000 | Loss: 0.00003703
Iteration 105/1000 | Loss: 0.00003700
Iteration 106/1000 | Loss: 0.00003700
Iteration 107/1000 | Loss: 0.00003699
Iteration 108/1000 | Loss: 0.00003698
Iteration 109/1000 | Loss: 0.00003698
Iteration 110/1000 | Loss: 0.00003698
Iteration 111/1000 | Loss: 0.00003698
Iteration 112/1000 | Loss: 0.00003698
Iteration 113/1000 | Loss: 0.00003698
Iteration 114/1000 | Loss: 0.00003697
Iteration 115/1000 | Loss: 0.00003697
Iteration 116/1000 | Loss: 0.00003697
Iteration 117/1000 | Loss: 0.00003697
Iteration 118/1000 | Loss: 0.00003697
Iteration 119/1000 | Loss: 0.00003697
Iteration 120/1000 | Loss: 0.00003697
Iteration 121/1000 | Loss: 0.00003697
Iteration 122/1000 | Loss: 0.00003697
Iteration 123/1000 | Loss: 0.00003697
Iteration 124/1000 | Loss: 0.00003697
Iteration 125/1000 | Loss: 0.00003697
Iteration 126/1000 | Loss: 0.00007448
Iteration 127/1000 | Loss: 0.00003715
Iteration 128/1000 | Loss: 0.00003696
Iteration 129/1000 | Loss: 0.00003693
Iteration 130/1000 | Loss: 0.00003693
Iteration 131/1000 | Loss: 0.00003693
Iteration 132/1000 | Loss: 0.00003693
Iteration 133/1000 | Loss: 0.00003693
Iteration 134/1000 | Loss: 0.00003693
Iteration 135/1000 | Loss: 0.00003693
Iteration 136/1000 | Loss: 0.00003693
Iteration 137/1000 | Loss: 0.00003692
Iteration 138/1000 | Loss: 0.00003692
Iteration 139/1000 | Loss: 0.00003692
Iteration 140/1000 | Loss: 0.00003692
Iteration 141/1000 | Loss: 0.00003692
Iteration 142/1000 | Loss: 0.00003692
Iteration 143/1000 | Loss: 0.00003692
Iteration 144/1000 | Loss: 0.00003692
Iteration 145/1000 | Loss: 0.00003692
Iteration 146/1000 | Loss: 0.00003691
Iteration 147/1000 | Loss: 0.00003691
Iteration 148/1000 | Loss: 0.00003691
Iteration 149/1000 | Loss: 0.00003691
Iteration 150/1000 | Loss: 0.00003691
Iteration 151/1000 | Loss: 0.00003691
Iteration 152/1000 | Loss: 0.00003691
Iteration 153/1000 | Loss: 0.00003691
Iteration 154/1000 | Loss: 0.00003691
Iteration 155/1000 | Loss: 0.00003691
Iteration 156/1000 | Loss: 0.00003691
Iteration 157/1000 | Loss: 0.00003691
Iteration 158/1000 | Loss: 0.00003691
Iteration 159/1000 | Loss: 0.00003691
Iteration 160/1000 | Loss: 0.00003691
Iteration 161/1000 | Loss: 0.00003691
Iteration 162/1000 | Loss: 0.00003691
Iteration 163/1000 | Loss: 0.00003691
Iteration 164/1000 | Loss: 0.00003691
Iteration 165/1000 | Loss: 0.00003691
Iteration 166/1000 | Loss: 0.00003691
Iteration 167/1000 | Loss: 0.00003691
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [3.691263191285543e-05, 3.691263191285543e-05, 3.691263191285543e-05, 3.691263191285543e-05, 3.691263191285543e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.691263191285543e-05

Optimization complete. Final v2v error: 3.8541648387908936 mm

Highest mean error: 11.45669174194336 mm for frame 57

Lowest mean error: 2.903975009918213 mm for frame 9

Saving results

Total time: 150.87597680091858
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00445644
Iteration 2/25 | Loss: 0.00078363
Iteration 3/25 | Loss: 0.00066524
Iteration 4/25 | Loss: 0.00064560
Iteration 5/25 | Loss: 0.00063834
Iteration 6/25 | Loss: 0.00063685
Iteration 7/25 | Loss: 0.00063653
Iteration 8/25 | Loss: 0.00063653
Iteration 9/25 | Loss: 0.00063653
Iteration 10/25 | Loss: 0.00063653
Iteration 11/25 | Loss: 0.00063653
Iteration 12/25 | Loss: 0.00063653
Iteration 13/25 | Loss: 0.00063653
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.000636533135548234, 0.000636533135548234, 0.000636533135548234, 0.000636533135548234, 0.000636533135548234]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000636533135548234

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.50855684
Iteration 2/25 | Loss: 0.00025469
Iteration 3/25 | Loss: 0.00025469
Iteration 4/25 | Loss: 0.00025468
Iteration 5/25 | Loss: 0.00025468
Iteration 6/25 | Loss: 0.00025468
Iteration 7/25 | Loss: 0.00025468
Iteration 8/25 | Loss: 0.00025468
Iteration 9/25 | Loss: 0.00025468
Iteration 10/25 | Loss: 0.00025468
Iteration 11/25 | Loss: 0.00025468
Iteration 12/25 | Loss: 0.00025468
Iteration 13/25 | Loss: 0.00025468
Iteration 14/25 | Loss: 0.00025468
Iteration 15/25 | Loss: 0.00025468
Iteration 16/25 | Loss: 0.00025468
Iteration 17/25 | Loss: 0.00025468
Iteration 18/25 | Loss: 0.00025468
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0002546817122492939, 0.0002546817122492939, 0.0002546817122492939, 0.0002546817122492939, 0.0002546817122492939]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002546817122492939

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00025468
Iteration 2/1000 | Loss: 0.00003052
Iteration 3/1000 | Loss: 0.00002180
Iteration 4/1000 | Loss: 0.00002033
Iteration 5/1000 | Loss: 0.00001933
Iteration 6/1000 | Loss: 0.00001882
Iteration 7/1000 | Loss: 0.00001835
Iteration 8/1000 | Loss: 0.00001819
Iteration 9/1000 | Loss: 0.00001818
Iteration 10/1000 | Loss: 0.00001815
Iteration 11/1000 | Loss: 0.00001813
Iteration 12/1000 | Loss: 0.00001792
Iteration 13/1000 | Loss: 0.00001780
Iteration 14/1000 | Loss: 0.00001779
Iteration 15/1000 | Loss: 0.00001768
Iteration 16/1000 | Loss: 0.00001768
Iteration 17/1000 | Loss: 0.00001767
Iteration 18/1000 | Loss: 0.00001761
Iteration 19/1000 | Loss: 0.00001760
Iteration 20/1000 | Loss: 0.00001759
Iteration 21/1000 | Loss: 0.00001756
Iteration 22/1000 | Loss: 0.00001756
Iteration 23/1000 | Loss: 0.00001755
Iteration 24/1000 | Loss: 0.00001755
Iteration 25/1000 | Loss: 0.00001755
Iteration 26/1000 | Loss: 0.00001754
Iteration 27/1000 | Loss: 0.00001753
Iteration 28/1000 | Loss: 0.00001753
Iteration 29/1000 | Loss: 0.00001753
Iteration 30/1000 | Loss: 0.00001753
Iteration 31/1000 | Loss: 0.00001753
Iteration 32/1000 | Loss: 0.00001753
Iteration 33/1000 | Loss: 0.00001753
Iteration 34/1000 | Loss: 0.00001753
Iteration 35/1000 | Loss: 0.00001752
Iteration 36/1000 | Loss: 0.00001749
Iteration 37/1000 | Loss: 0.00001749
Iteration 38/1000 | Loss: 0.00001749
Iteration 39/1000 | Loss: 0.00001748
Iteration 40/1000 | Loss: 0.00001747
Iteration 41/1000 | Loss: 0.00001746
Iteration 42/1000 | Loss: 0.00001745
Iteration 43/1000 | Loss: 0.00001745
Iteration 44/1000 | Loss: 0.00001744
Iteration 45/1000 | Loss: 0.00001744
Iteration 46/1000 | Loss: 0.00001744
Iteration 47/1000 | Loss: 0.00001741
Iteration 48/1000 | Loss: 0.00001740
Iteration 49/1000 | Loss: 0.00001739
Iteration 50/1000 | Loss: 0.00001739
Iteration 51/1000 | Loss: 0.00001738
Iteration 52/1000 | Loss: 0.00001738
Iteration 53/1000 | Loss: 0.00001735
Iteration 54/1000 | Loss: 0.00001735
Iteration 55/1000 | Loss: 0.00001735
Iteration 56/1000 | Loss: 0.00001734
Iteration 57/1000 | Loss: 0.00001734
Iteration 58/1000 | Loss: 0.00001734
Iteration 59/1000 | Loss: 0.00001734
Iteration 60/1000 | Loss: 0.00001734
Iteration 61/1000 | Loss: 0.00001734
Iteration 62/1000 | Loss: 0.00001734
Iteration 63/1000 | Loss: 0.00001733
Iteration 64/1000 | Loss: 0.00001732
Iteration 65/1000 | Loss: 0.00001732
Iteration 66/1000 | Loss: 0.00001732
Iteration 67/1000 | Loss: 0.00001732
Iteration 68/1000 | Loss: 0.00001731
Iteration 69/1000 | Loss: 0.00001731
Iteration 70/1000 | Loss: 0.00001731
Iteration 71/1000 | Loss: 0.00001730
Iteration 72/1000 | Loss: 0.00001730
Iteration 73/1000 | Loss: 0.00001730
Iteration 74/1000 | Loss: 0.00001729
Iteration 75/1000 | Loss: 0.00001729
Iteration 76/1000 | Loss: 0.00001729
Iteration 77/1000 | Loss: 0.00001729
Iteration 78/1000 | Loss: 0.00001729
Iteration 79/1000 | Loss: 0.00001729
Iteration 80/1000 | Loss: 0.00001729
Iteration 81/1000 | Loss: 0.00001729
Iteration 82/1000 | Loss: 0.00001729
Iteration 83/1000 | Loss: 0.00001728
Iteration 84/1000 | Loss: 0.00001728
Iteration 85/1000 | Loss: 0.00001728
Iteration 86/1000 | Loss: 0.00001728
Iteration 87/1000 | Loss: 0.00001727
Iteration 88/1000 | Loss: 0.00001727
Iteration 89/1000 | Loss: 0.00001727
Iteration 90/1000 | Loss: 0.00001726
Iteration 91/1000 | Loss: 0.00001726
Iteration 92/1000 | Loss: 0.00001726
Iteration 93/1000 | Loss: 0.00001726
Iteration 94/1000 | Loss: 0.00001725
Iteration 95/1000 | Loss: 0.00001725
Iteration 96/1000 | Loss: 0.00001725
Iteration 97/1000 | Loss: 0.00001725
Iteration 98/1000 | Loss: 0.00001724
Iteration 99/1000 | Loss: 0.00001724
Iteration 100/1000 | Loss: 0.00001724
Iteration 101/1000 | Loss: 0.00001724
Iteration 102/1000 | Loss: 0.00001724
Iteration 103/1000 | Loss: 0.00001724
Iteration 104/1000 | Loss: 0.00001723
Iteration 105/1000 | Loss: 0.00001723
Iteration 106/1000 | Loss: 0.00001723
Iteration 107/1000 | Loss: 0.00001723
Iteration 108/1000 | Loss: 0.00001723
Iteration 109/1000 | Loss: 0.00001723
Iteration 110/1000 | Loss: 0.00001723
Iteration 111/1000 | Loss: 0.00001723
Iteration 112/1000 | Loss: 0.00001723
Iteration 113/1000 | Loss: 0.00001723
Iteration 114/1000 | Loss: 0.00001723
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [1.7225343981408514e-05, 1.7225343981408514e-05, 1.7225343981408514e-05, 1.7225343981408514e-05, 1.7225343981408514e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7225343981408514e-05

Optimization complete. Final v2v error: 3.5168724060058594 mm

Highest mean error: 4.132278919219971 mm for frame 79

Lowest mean error: 3.173325538635254 mm for frame 35

Saving results

Total time: 33.927939891815186
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00874565
Iteration 2/25 | Loss: 0.00112219
Iteration 3/25 | Loss: 0.00083009
Iteration 4/25 | Loss: 0.00078416
Iteration 5/25 | Loss: 0.00077527
Iteration 6/25 | Loss: 0.00077463
Iteration 7/25 | Loss: 0.00077463
Iteration 8/25 | Loss: 0.00077463
Iteration 9/25 | Loss: 0.00077463
Iteration 10/25 | Loss: 0.00077463
Iteration 11/25 | Loss: 0.00077463
Iteration 12/25 | Loss: 0.00077463
Iteration 13/25 | Loss: 0.00077463
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007746345945633948, 0.0007746345945633948, 0.0007746345945633948, 0.0007746345945633948, 0.0007746345945633948]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007746345945633948

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.03214419
Iteration 2/25 | Loss: 0.00023094
Iteration 3/25 | Loss: 0.00023093
Iteration 4/25 | Loss: 0.00023093
Iteration 5/25 | Loss: 0.00023093
Iteration 6/25 | Loss: 0.00023093
Iteration 7/25 | Loss: 0.00023093
Iteration 8/25 | Loss: 0.00023093
Iteration 9/25 | Loss: 0.00023093
Iteration 10/25 | Loss: 0.00023093
Iteration 11/25 | Loss: 0.00023093
Iteration 12/25 | Loss: 0.00023093
Iteration 13/25 | Loss: 0.00023093
Iteration 14/25 | Loss: 0.00023093
Iteration 15/25 | Loss: 0.00023093
Iteration 16/25 | Loss: 0.00023093
Iteration 17/25 | Loss: 0.00023093
Iteration 18/25 | Loss: 0.00023093
Iteration 19/25 | Loss: 0.00023093
Iteration 20/25 | Loss: 0.00023093
Iteration 21/25 | Loss: 0.00023093
Iteration 22/25 | Loss: 0.00023093
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0002309302508365363, 0.0002309302508365363, 0.0002309302508365363, 0.0002309302508365363, 0.0002309302508365363]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002309302508365363

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00023093
Iteration 2/1000 | Loss: 0.00003707
Iteration 3/1000 | Loss: 0.00003139
Iteration 4/1000 | Loss: 0.00002945
Iteration 5/1000 | Loss: 0.00002766
Iteration 6/1000 | Loss: 0.00002695
Iteration 7/1000 | Loss: 0.00002625
Iteration 8/1000 | Loss: 0.00002581
Iteration 9/1000 | Loss: 0.00002558
Iteration 10/1000 | Loss: 0.00002538
Iteration 11/1000 | Loss: 0.00002517
Iteration 12/1000 | Loss: 0.00002513
Iteration 13/1000 | Loss: 0.00002509
Iteration 14/1000 | Loss: 0.00002509
Iteration 15/1000 | Loss: 0.00002509
Iteration 16/1000 | Loss: 0.00002509
Iteration 17/1000 | Loss: 0.00002509
Iteration 18/1000 | Loss: 0.00002509
Iteration 19/1000 | Loss: 0.00002508
Iteration 20/1000 | Loss: 0.00002504
Iteration 21/1000 | Loss: 0.00002504
Iteration 22/1000 | Loss: 0.00002504
Iteration 23/1000 | Loss: 0.00002504
Iteration 24/1000 | Loss: 0.00002504
Iteration 25/1000 | Loss: 0.00002504
Iteration 26/1000 | Loss: 0.00002503
Iteration 27/1000 | Loss: 0.00002503
Iteration 28/1000 | Loss: 0.00002503
Iteration 29/1000 | Loss: 0.00002503
Iteration 30/1000 | Loss: 0.00002503
Iteration 31/1000 | Loss: 0.00002502
Iteration 32/1000 | Loss: 0.00002501
Iteration 33/1000 | Loss: 0.00002500
Iteration 34/1000 | Loss: 0.00002498
Iteration 35/1000 | Loss: 0.00002497
Iteration 36/1000 | Loss: 0.00002497
Iteration 37/1000 | Loss: 0.00002497
Iteration 38/1000 | Loss: 0.00002497
Iteration 39/1000 | Loss: 0.00002497
Iteration 40/1000 | Loss: 0.00002497
Iteration 41/1000 | Loss: 0.00002497
Iteration 42/1000 | Loss: 0.00002497
Iteration 43/1000 | Loss: 0.00002496
Iteration 44/1000 | Loss: 0.00002496
Iteration 45/1000 | Loss: 0.00002496
Iteration 46/1000 | Loss: 0.00002496
Iteration 47/1000 | Loss: 0.00002496
Iteration 48/1000 | Loss: 0.00002496
Iteration 49/1000 | Loss: 0.00002496
Iteration 50/1000 | Loss: 0.00002496
Iteration 51/1000 | Loss: 0.00002496
Iteration 52/1000 | Loss: 0.00002495
Iteration 53/1000 | Loss: 0.00002495
Iteration 54/1000 | Loss: 0.00002495
Iteration 55/1000 | Loss: 0.00002495
Iteration 56/1000 | Loss: 0.00002495
Iteration 57/1000 | Loss: 0.00002494
Iteration 58/1000 | Loss: 0.00002494
Iteration 59/1000 | Loss: 0.00002494
Iteration 60/1000 | Loss: 0.00002493
Iteration 61/1000 | Loss: 0.00002493
Iteration 62/1000 | Loss: 0.00002493
Iteration 63/1000 | Loss: 0.00002493
Iteration 64/1000 | Loss: 0.00002493
Iteration 65/1000 | Loss: 0.00002493
Iteration 66/1000 | Loss: 0.00002493
Iteration 67/1000 | Loss: 0.00002492
Iteration 68/1000 | Loss: 0.00002492
Iteration 69/1000 | Loss: 0.00002492
Iteration 70/1000 | Loss: 0.00002492
Iteration 71/1000 | Loss: 0.00002492
Iteration 72/1000 | Loss: 0.00002492
Iteration 73/1000 | Loss: 0.00002492
Iteration 74/1000 | Loss: 0.00002492
Iteration 75/1000 | Loss: 0.00002492
Iteration 76/1000 | Loss: 0.00002491
Iteration 77/1000 | Loss: 0.00002491
Iteration 78/1000 | Loss: 0.00002491
Iteration 79/1000 | Loss: 0.00002491
Iteration 80/1000 | Loss: 0.00002490
Iteration 81/1000 | Loss: 0.00002490
Iteration 82/1000 | Loss: 0.00002490
Iteration 83/1000 | Loss: 0.00002490
Iteration 84/1000 | Loss: 0.00002490
Iteration 85/1000 | Loss: 0.00002490
Iteration 86/1000 | Loss: 0.00002490
Iteration 87/1000 | Loss: 0.00002490
Iteration 88/1000 | Loss: 0.00002490
Iteration 89/1000 | Loss: 0.00002490
Iteration 90/1000 | Loss: 0.00002490
Iteration 91/1000 | Loss: 0.00002490
Iteration 92/1000 | Loss: 0.00002490
Iteration 93/1000 | Loss: 0.00002490
Iteration 94/1000 | Loss: 0.00002490
Iteration 95/1000 | Loss: 0.00002490
Iteration 96/1000 | Loss: 0.00002490
Iteration 97/1000 | Loss: 0.00002490
Iteration 98/1000 | Loss: 0.00002490
Iteration 99/1000 | Loss: 0.00002490
Iteration 100/1000 | Loss: 0.00002490
Iteration 101/1000 | Loss: 0.00002490
Iteration 102/1000 | Loss: 0.00002490
Iteration 103/1000 | Loss: 0.00002490
Iteration 104/1000 | Loss: 0.00002490
Iteration 105/1000 | Loss: 0.00002490
Iteration 106/1000 | Loss: 0.00002490
Iteration 107/1000 | Loss: 0.00002490
Iteration 108/1000 | Loss: 0.00002490
Iteration 109/1000 | Loss: 0.00002490
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [2.4898819901864044e-05, 2.4898819901864044e-05, 2.4898819901864044e-05, 2.4898819901864044e-05, 2.4898819901864044e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4898819901864044e-05

Optimization complete. Final v2v error: 4.185539722442627 mm

Highest mean error: 4.2367634773254395 mm for frame 77

Lowest mean error: 4.128312110900879 mm for frame 131

Saving results

Total time: 30.02191162109375
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01093298
Iteration 2/25 | Loss: 0.01093298
Iteration 3/25 | Loss: 0.01093298
Iteration 4/25 | Loss: 0.01093298
Iteration 5/25 | Loss: 0.01093298
Iteration 6/25 | Loss: 0.01093297
Iteration 7/25 | Loss: 0.01093297
Iteration 8/25 | Loss: 0.01093297
Iteration 9/25 | Loss: 0.01093297
Iteration 10/25 | Loss: 0.01093297
Iteration 11/25 | Loss: 0.01093297
Iteration 12/25 | Loss: 0.01093297
Iteration 13/25 | Loss: 0.01093297
Iteration 14/25 | Loss: 0.01093297
Iteration 15/25 | Loss: 0.01093296
Iteration 16/25 | Loss: 0.01093296
Iteration 17/25 | Loss: 0.01093296
Iteration 18/25 | Loss: 0.01093296
Iteration 19/25 | Loss: 0.01093296
Iteration 20/25 | Loss: 0.01093296
Iteration 21/25 | Loss: 0.01093296
Iteration 22/25 | Loss: 0.01093296
Iteration 23/25 | Loss: 0.01093296
Iteration 24/25 | Loss: 0.01093296
Iteration 25/25 | Loss: 0.01093296

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.89957011
Iteration 2/25 | Loss: 0.05363082
Iteration 3/25 | Loss: 0.05362993
Iteration 4/25 | Loss: 0.05362991
Iteration 5/25 | Loss: 0.05362161
Iteration 6/25 | Loss: 0.05362161
Iteration 7/25 | Loss: 0.05362161
Iteration 8/25 | Loss: 0.05362161
Iteration 9/25 | Loss: 0.05362161
Iteration 10/25 | Loss: 0.05362161
Iteration 11/25 | Loss: 0.05362160
Iteration 12/25 | Loss: 0.05362160
Iteration 13/25 | Loss: 0.05362160
Iteration 14/25 | Loss: 0.05362160
Iteration 15/25 | Loss: 0.05362160
Iteration 16/25 | Loss: 0.05362160
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.053621597588062286, 0.053621597588062286, 0.053621597588062286, 0.053621597588062286, 0.053621597588062286]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.053621597588062286

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.05362160
Iteration 2/1000 | Loss: 0.01019598
Iteration 3/1000 | Loss: 0.00057777
Iteration 4/1000 | Loss: 0.00177559
Iteration 5/1000 | Loss: 0.00010308
Iteration 6/1000 | Loss: 0.00016029
Iteration 7/1000 | Loss: 0.00013228
Iteration 8/1000 | Loss: 0.00013454
Iteration 9/1000 | Loss: 0.00005242
Iteration 10/1000 | Loss: 0.00025372
Iteration 11/1000 | Loss: 0.00010622
Iteration 12/1000 | Loss: 0.00003661
Iteration 13/1000 | Loss: 0.00005014
Iteration 14/1000 | Loss: 0.00003097
Iteration 15/1000 | Loss: 0.00002833
Iteration 16/1000 | Loss: 0.00005137
Iteration 17/1000 | Loss: 0.00002541
Iteration 18/1000 | Loss: 0.00007230
Iteration 19/1000 | Loss: 0.00002302
Iteration 20/1000 | Loss: 0.00005781
Iteration 21/1000 | Loss: 0.00010333
Iteration 22/1000 | Loss: 0.00013542
Iteration 23/1000 | Loss: 0.00002623
Iteration 24/1000 | Loss: 0.00009652
Iteration 25/1000 | Loss: 0.00002598
Iteration 26/1000 | Loss: 0.00001946
Iteration 27/1000 | Loss: 0.00001885
Iteration 28/1000 | Loss: 0.00011814
Iteration 29/1000 | Loss: 0.00009949
Iteration 30/1000 | Loss: 0.00005041
Iteration 31/1000 | Loss: 0.00007125
Iteration 32/1000 | Loss: 0.00001787
Iteration 33/1000 | Loss: 0.00003942
Iteration 34/1000 | Loss: 0.00001726
Iteration 35/1000 | Loss: 0.00003264
Iteration 36/1000 | Loss: 0.00001703
Iteration 37/1000 | Loss: 0.00001679
Iteration 38/1000 | Loss: 0.00001652
Iteration 39/1000 | Loss: 0.00001644
Iteration 40/1000 | Loss: 0.00015138
Iteration 41/1000 | Loss: 0.00002155
Iteration 42/1000 | Loss: 0.00003568
Iteration 43/1000 | Loss: 0.00002288
Iteration 44/1000 | Loss: 0.00001627
Iteration 45/1000 | Loss: 0.00001611
Iteration 46/1000 | Loss: 0.00001611
Iteration 47/1000 | Loss: 0.00001606
Iteration 48/1000 | Loss: 0.00001605
Iteration 49/1000 | Loss: 0.00001604
Iteration 50/1000 | Loss: 0.00001604
Iteration 51/1000 | Loss: 0.00001604
Iteration 52/1000 | Loss: 0.00001603
Iteration 53/1000 | Loss: 0.00001603
Iteration 54/1000 | Loss: 0.00001602
Iteration 55/1000 | Loss: 0.00001665
Iteration 56/1000 | Loss: 0.00001665
Iteration 57/1000 | Loss: 0.00001620
Iteration 58/1000 | Loss: 0.00001617
Iteration 59/1000 | Loss: 0.00001611
Iteration 60/1000 | Loss: 0.00001611
Iteration 61/1000 | Loss: 0.00001610
Iteration 62/1000 | Loss: 0.00001609
Iteration 63/1000 | Loss: 0.00001608
Iteration 64/1000 | Loss: 0.00001607
Iteration 65/1000 | Loss: 0.00001606
Iteration 66/1000 | Loss: 0.00001606
Iteration 67/1000 | Loss: 0.00001605
Iteration 68/1000 | Loss: 0.00001605
Iteration 69/1000 | Loss: 0.00001605
Iteration 70/1000 | Loss: 0.00001604
Iteration 71/1000 | Loss: 0.00001604
Iteration 72/1000 | Loss: 0.00001603
Iteration 73/1000 | Loss: 0.00001601
Iteration 74/1000 | Loss: 0.00001600
Iteration 75/1000 | Loss: 0.00001600
Iteration 76/1000 | Loss: 0.00001600
Iteration 77/1000 | Loss: 0.00001600
Iteration 78/1000 | Loss: 0.00001599
Iteration 79/1000 | Loss: 0.00001599
Iteration 80/1000 | Loss: 0.00001599
Iteration 81/1000 | Loss: 0.00001598
Iteration 82/1000 | Loss: 0.00001598
Iteration 83/1000 | Loss: 0.00001598
Iteration 84/1000 | Loss: 0.00001597
Iteration 85/1000 | Loss: 0.00001597
Iteration 86/1000 | Loss: 0.00001596
Iteration 87/1000 | Loss: 0.00001596
Iteration 88/1000 | Loss: 0.00001596
Iteration 89/1000 | Loss: 0.00001596
Iteration 90/1000 | Loss: 0.00001595
Iteration 91/1000 | Loss: 0.00001595
Iteration 92/1000 | Loss: 0.00001595
Iteration 93/1000 | Loss: 0.00001595
Iteration 94/1000 | Loss: 0.00001594
Iteration 95/1000 | Loss: 0.00001594
Iteration 96/1000 | Loss: 0.00001594
Iteration 97/1000 | Loss: 0.00001594
Iteration 98/1000 | Loss: 0.00001594
Iteration 99/1000 | Loss: 0.00001594
Iteration 100/1000 | Loss: 0.00001594
Iteration 101/1000 | Loss: 0.00001593
Iteration 102/1000 | Loss: 0.00001593
Iteration 103/1000 | Loss: 0.00001593
Iteration 104/1000 | Loss: 0.00001593
Iteration 105/1000 | Loss: 0.00001593
Iteration 106/1000 | Loss: 0.00001593
Iteration 107/1000 | Loss: 0.00001593
Iteration 108/1000 | Loss: 0.00001593
Iteration 109/1000 | Loss: 0.00001593
Iteration 110/1000 | Loss: 0.00001592
Iteration 111/1000 | Loss: 0.00001592
Iteration 112/1000 | Loss: 0.00001592
Iteration 113/1000 | Loss: 0.00001592
Iteration 114/1000 | Loss: 0.00001592
Iteration 115/1000 | Loss: 0.00001592
Iteration 116/1000 | Loss: 0.00001591
Iteration 117/1000 | Loss: 0.00001591
Iteration 118/1000 | Loss: 0.00001591
Iteration 119/1000 | Loss: 0.00001591
Iteration 120/1000 | Loss: 0.00001590
Iteration 121/1000 | Loss: 0.00001590
Iteration 122/1000 | Loss: 0.00001590
Iteration 123/1000 | Loss: 0.00001590
Iteration 124/1000 | Loss: 0.00001590
Iteration 125/1000 | Loss: 0.00001590
Iteration 126/1000 | Loss: 0.00001590
Iteration 127/1000 | Loss: 0.00001589
Iteration 128/1000 | Loss: 0.00001589
Iteration 129/1000 | Loss: 0.00001589
Iteration 130/1000 | Loss: 0.00001589
Iteration 131/1000 | Loss: 0.00001589
Iteration 132/1000 | Loss: 0.00001589
Iteration 133/1000 | Loss: 0.00001589
Iteration 134/1000 | Loss: 0.00001589
Iteration 135/1000 | Loss: 0.00001589
Iteration 136/1000 | Loss: 0.00001589
Iteration 137/1000 | Loss: 0.00001589
Iteration 138/1000 | Loss: 0.00001589
Iteration 139/1000 | Loss: 0.00001589
Iteration 140/1000 | Loss: 0.00001589
Iteration 141/1000 | Loss: 0.00001589
Iteration 142/1000 | Loss: 0.00001589
Iteration 143/1000 | Loss: 0.00001589
Iteration 144/1000 | Loss: 0.00001589
Iteration 145/1000 | Loss: 0.00001589
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [1.58884849952301e-05, 1.58884849952301e-05, 1.58884849952301e-05, 1.58884849952301e-05, 1.58884849952301e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.58884849952301e-05

Optimization complete. Final v2v error: 3.3540728092193604 mm

Highest mean error: 10.425840377807617 mm for frame 143

Lowest mean error: 3.033933162689209 mm for frame 224

Saving results

Total time: 88.53163528442383
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00474120
Iteration 2/25 | Loss: 0.00097442
Iteration 3/25 | Loss: 0.00067047
Iteration 4/25 | Loss: 0.00063558
Iteration 5/25 | Loss: 0.00062589
Iteration 6/25 | Loss: 0.00062360
Iteration 7/25 | Loss: 0.00062278
Iteration 8/25 | Loss: 0.00062278
Iteration 9/25 | Loss: 0.00062278
Iteration 10/25 | Loss: 0.00062278
Iteration 11/25 | Loss: 0.00062278
Iteration 12/25 | Loss: 0.00062278
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006227847770787776, 0.0006227847770787776, 0.0006227847770787776, 0.0006227847770787776, 0.0006227847770787776]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006227847770787776

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.90331388
Iteration 2/25 | Loss: 0.00023281
Iteration 3/25 | Loss: 0.00023280
Iteration 4/25 | Loss: 0.00023280
Iteration 5/25 | Loss: 0.00023280
Iteration 6/25 | Loss: 0.00023280
Iteration 7/25 | Loss: 0.00023280
Iteration 8/25 | Loss: 0.00023280
Iteration 9/25 | Loss: 0.00023280
Iteration 10/25 | Loss: 0.00023280
Iteration 11/25 | Loss: 0.00023280
Iteration 12/25 | Loss: 0.00023280
Iteration 13/25 | Loss: 0.00023280
Iteration 14/25 | Loss: 0.00023280
Iteration 15/25 | Loss: 0.00023280
Iteration 16/25 | Loss: 0.00023280
Iteration 17/25 | Loss: 0.00023280
Iteration 18/25 | Loss: 0.00023280
Iteration 19/25 | Loss: 0.00023280
Iteration 20/25 | Loss: 0.00023280
Iteration 21/25 | Loss: 0.00023280
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0002327966649318114, 0.0002327966649318114, 0.0002327966649318114, 0.0002327966649318114, 0.0002327966649318114]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002327966649318114

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00023280
Iteration 2/1000 | Loss: 0.00002325
Iteration 3/1000 | Loss: 0.00001706
Iteration 4/1000 | Loss: 0.00001590
Iteration 5/1000 | Loss: 0.00001518
Iteration 6/1000 | Loss: 0.00001483
Iteration 7/1000 | Loss: 0.00001442
Iteration 8/1000 | Loss: 0.00001419
Iteration 9/1000 | Loss: 0.00001399
Iteration 10/1000 | Loss: 0.00001392
Iteration 11/1000 | Loss: 0.00001379
Iteration 12/1000 | Loss: 0.00001378
Iteration 13/1000 | Loss: 0.00001377
Iteration 14/1000 | Loss: 0.00001376
Iteration 15/1000 | Loss: 0.00001375
Iteration 16/1000 | Loss: 0.00001375
Iteration 17/1000 | Loss: 0.00001374
Iteration 18/1000 | Loss: 0.00001373
Iteration 19/1000 | Loss: 0.00001373
Iteration 20/1000 | Loss: 0.00001370
Iteration 21/1000 | Loss: 0.00001369
Iteration 22/1000 | Loss: 0.00001364
Iteration 23/1000 | Loss: 0.00001364
Iteration 24/1000 | Loss: 0.00001361
Iteration 25/1000 | Loss: 0.00001359
Iteration 26/1000 | Loss: 0.00001358
Iteration 27/1000 | Loss: 0.00001358
Iteration 28/1000 | Loss: 0.00001357
Iteration 29/1000 | Loss: 0.00001357
Iteration 30/1000 | Loss: 0.00001355
Iteration 31/1000 | Loss: 0.00001353
Iteration 32/1000 | Loss: 0.00001353
Iteration 33/1000 | Loss: 0.00001352
Iteration 34/1000 | Loss: 0.00001352
Iteration 35/1000 | Loss: 0.00001352
Iteration 36/1000 | Loss: 0.00001351
Iteration 37/1000 | Loss: 0.00001351
Iteration 38/1000 | Loss: 0.00001351
Iteration 39/1000 | Loss: 0.00001351
Iteration 40/1000 | Loss: 0.00001350
Iteration 41/1000 | Loss: 0.00001350
Iteration 42/1000 | Loss: 0.00001350
Iteration 43/1000 | Loss: 0.00001350
Iteration 44/1000 | Loss: 0.00001350
Iteration 45/1000 | Loss: 0.00001349
Iteration 46/1000 | Loss: 0.00001349
Iteration 47/1000 | Loss: 0.00001349
Iteration 48/1000 | Loss: 0.00001349
Iteration 49/1000 | Loss: 0.00001349
Iteration 50/1000 | Loss: 0.00001349
Iteration 51/1000 | Loss: 0.00001349
Iteration 52/1000 | Loss: 0.00001349
Iteration 53/1000 | Loss: 0.00001349
Iteration 54/1000 | Loss: 0.00001349
Iteration 55/1000 | Loss: 0.00001349
Iteration 56/1000 | Loss: 0.00001349
Iteration 57/1000 | Loss: 0.00001349
Iteration 58/1000 | Loss: 0.00001349
Iteration 59/1000 | Loss: 0.00001349
Iteration 60/1000 | Loss: 0.00001349
Iteration 61/1000 | Loss: 0.00001349
Iteration 62/1000 | Loss: 0.00001349
Iteration 63/1000 | Loss: 0.00001349
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 63. Stopping optimization.
Last 5 losses: [1.3487480828189291e-05, 1.3487480828189291e-05, 1.3487480828189291e-05, 1.3487480828189291e-05, 1.3487480828189291e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3487480828189291e-05

Optimization complete. Final v2v error: 3.168325662612915 mm

Highest mean error: 3.4195165634155273 mm for frame 0

Lowest mean error: 3.0255329608917236 mm for frame 31

Saving results

Total time: 35.62676477432251
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00325897
Iteration 2/25 | Loss: 0.00080094
Iteration 3/25 | Loss: 0.00068982
Iteration 4/25 | Loss: 0.00066225
Iteration 5/25 | Loss: 0.00065327
Iteration 6/25 | Loss: 0.00065140
Iteration 7/25 | Loss: 0.00065079
Iteration 8/25 | Loss: 0.00065079
Iteration 9/25 | Loss: 0.00065079
Iteration 10/25 | Loss: 0.00065079
Iteration 11/25 | Loss: 0.00065079
Iteration 12/25 | Loss: 0.00065079
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006507858051918447, 0.0006507858051918447, 0.0006507858051918447, 0.0006507858051918447, 0.0006507858051918447]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006507858051918447

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.87942457
Iteration 2/25 | Loss: 0.00037754
Iteration 3/25 | Loss: 0.00037749
Iteration 4/25 | Loss: 0.00037749
Iteration 5/25 | Loss: 0.00037749
Iteration 6/25 | Loss: 0.00037749
Iteration 7/25 | Loss: 0.00037749
Iteration 8/25 | Loss: 0.00037749
Iteration 9/25 | Loss: 0.00037749
Iteration 10/25 | Loss: 0.00037749
Iteration 11/25 | Loss: 0.00037749
Iteration 12/25 | Loss: 0.00037749
Iteration 13/25 | Loss: 0.00037749
Iteration 14/25 | Loss: 0.00037749
Iteration 15/25 | Loss: 0.00037749
Iteration 16/25 | Loss: 0.00037749
Iteration 17/25 | Loss: 0.00037749
Iteration 18/25 | Loss: 0.00037749
Iteration 19/25 | Loss: 0.00037749
Iteration 20/25 | Loss: 0.00037749
Iteration 21/25 | Loss: 0.00037749
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00037748526665382087, 0.00037748526665382087, 0.00037748526665382087, 0.00037748526665382087, 0.00037748526665382087]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00037748526665382087

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037749
Iteration 2/1000 | Loss: 0.00003699
Iteration 3/1000 | Loss: 0.00002164
Iteration 4/1000 | Loss: 0.00001888
Iteration 5/1000 | Loss: 0.00001789
Iteration 6/1000 | Loss: 0.00001720
Iteration 7/1000 | Loss: 0.00001679
Iteration 8/1000 | Loss: 0.00001638
Iteration 9/1000 | Loss: 0.00001614
Iteration 10/1000 | Loss: 0.00001589
Iteration 11/1000 | Loss: 0.00001587
Iteration 12/1000 | Loss: 0.00001578
Iteration 13/1000 | Loss: 0.00001561
Iteration 14/1000 | Loss: 0.00001560
Iteration 15/1000 | Loss: 0.00001558
Iteration 16/1000 | Loss: 0.00001558
Iteration 17/1000 | Loss: 0.00001557
Iteration 18/1000 | Loss: 0.00001554
Iteration 19/1000 | Loss: 0.00001550
Iteration 20/1000 | Loss: 0.00001548
Iteration 21/1000 | Loss: 0.00001547
Iteration 22/1000 | Loss: 0.00001543
Iteration 23/1000 | Loss: 0.00001540
Iteration 24/1000 | Loss: 0.00001540
Iteration 25/1000 | Loss: 0.00001536
Iteration 26/1000 | Loss: 0.00001536
Iteration 27/1000 | Loss: 0.00001536
Iteration 28/1000 | Loss: 0.00001535
Iteration 29/1000 | Loss: 0.00001535
Iteration 30/1000 | Loss: 0.00001535
Iteration 31/1000 | Loss: 0.00001535
Iteration 32/1000 | Loss: 0.00001535
Iteration 33/1000 | Loss: 0.00001535
Iteration 34/1000 | Loss: 0.00001535
Iteration 35/1000 | Loss: 0.00001534
Iteration 36/1000 | Loss: 0.00001534
Iteration 37/1000 | Loss: 0.00001533
Iteration 38/1000 | Loss: 0.00001533
Iteration 39/1000 | Loss: 0.00001533
Iteration 40/1000 | Loss: 0.00001533
Iteration 41/1000 | Loss: 0.00001532
Iteration 42/1000 | Loss: 0.00001532
Iteration 43/1000 | Loss: 0.00001532
Iteration 44/1000 | Loss: 0.00001532
Iteration 45/1000 | Loss: 0.00001531
Iteration 46/1000 | Loss: 0.00001531
Iteration 47/1000 | Loss: 0.00001531
Iteration 48/1000 | Loss: 0.00001531
Iteration 49/1000 | Loss: 0.00001531
Iteration 50/1000 | Loss: 0.00001531
Iteration 51/1000 | Loss: 0.00001531
Iteration 52/1000 | Loss: 0.00001531
Iteration 53/1000 | Loss: 0.00001530
Iteration 54/1000 | Loss: 0.00001530
Iteration 55/1000 | Loss: 0.00001530
Iteration 56/1000 | Loss: 0.00001529
Iteration 57/1000 | Loss: 0.00001529
Iteration 58/1000 | Loss: 0.00001529
Iteration 59/1000 | Loss: 0.00001529
Iteration 60/1000 | Loss: 0.00001529
Iteration 61/1000 | Loss: 0.00001528
Iteration 62/1000 | Loss: 0.00001528
Iteration 63/1000 | Loss: 0.00001528
Iteration 64/1000 | Loss: 0.00001528
Iteration 65/1000 | Loss: 0.00001528
Iteration 66/1000 | Loss: 0.00001528
Iteration 67/1000 | Loss: 0.00001528
Iteration 68/1000 | Loss: 0.00001528
Iteration 69/1000 | Loss: 0.00001528
Iteration 70/1000 | Loss: 0.00001528
Iteration 71/1000 | Loss: 0.00001528
Iteration 72/1000 | Loss: 0.00001527
Iteration 73/1000 | Loss: 0.00001527
Iteration 74/1000 | Loss: 0.00001526
Iteration 75/1000 | Loss: 0.00001526
Iteration 76/1000 | Loss: 0.00001526
Iteration 77/1000 | Loss: 0.00001525
Iteration 78/1000 | Loss: 0.00001525
Iteration 79/1000 | Loss: 0.00001525
Iteration 80/1000 | Loss: 0.00001525
Iteration 81/1000 | Loss: 0.00001525
Iteration 82/1000 | Loss: 0.00001525
Iteration 83/1000 | Loss: 0.00001524
Iteration 84/1000 | Loss: 0.00001524
Iteration 85/1000 | Loss: 0.00001524
Iteration 86/1000 | Loss: 0.00001524
Iteration 87/1000 | Loss: 0.00001524
Iteration 88/1000 | Loss: 0.00001524
Iteration 89/1000 | Loss: 0.00001524
Iteration 90/1000 | Loss: 0.00001524
Iteration 91/1000 | Loss: 0.00001523
Iteration 92/1000 | Loss: 0.00001523
Iteration 93/1000 | Loss: 0.00001523
Iteration 94/1000 | Loss: 0.00001523
Iteration 95/1000 | Loss: 0.00001523
Iteration 96/1000 | Loss: 0.00001523
Iteration 97/1000 | Loss: 0.00001522
Iteration 98/1000 | Loss: 0.00001522
Iteration 99/1000 | Loss: 0.00001522
Iteration 100/1000 | Loss: 0.00001521
Iteration 101/1000 | Loss: 0.00001521
Iteration 102/1000 | Loss: 0.00001521
Iteration 103/1000 | Loss: 0.00001520
Iteration 104/1000 | Loss: 0.00001520
Iteration 105/1000 | Loss: 0.00001520
Iteration 106/1000 | Loss: 0.00001520
Iteration 107/1000 | Loss: 0.00001520
Iteration 108/1000 | Loss: 0.00001520
Iteration 109/1000 | Loss: 0.00001520
Iteration 110/1000 | Loss: 0.00001519
Iteration 111/1000 | Loss: 0.00001519
Iteration 112/1000 | Loss: 0.00001519
Iteration 113/1000 | Loss: 0.00001519
Iteration 114/1000 | Loss: 0.00001519
Iteration 115/1000 | Loss: 0.00001518
Iteration 116/1000 | Loss: 0.00001518
Iteration 117/1000 | Loss: 0.00001518
Iteration 118/1000 | Loss: 0.00001518
Iteration 119/1000 | Loss: 0.00001518
Iteration 120/1000 | Loss: 0.00001517
Iteration 121/1000 | Loss: 0.00001517
Iteration 122/1000 | Loss: 0.00001517
Iteration 123/1000 | Loss: 0.00001517
Iteration 124/1000 | Loss: 0.00001517
Iteration 125/1000 | Loss: 0.00001517
Iteration 126/1000 | Loss: 0.00001517
Iteration 127/1000 | Loss: 0.00001517
Iteration 128/1000 | Loss: 0.00001517
Iteration 129/1000 | Loss: 0.00001517
Iteration 130/1000 | Loss: 0.00001517
Iteration 131/1000 | Loss: 0.00001517
Iteration 132/1000 | Loss: 0.00001516
Iteration 133/1000 | Loss: 0.00001516
Iteration 134/1000 | Loss: 0.00001516
Iteration 135/1000 | Loss: 0.00001516
Iteration 136/1000 | Loss: 0.00001516
Iteration 137/1000 | Loss: 0.00001516
Iteration 138/1000 | Loss: 0.00001516
Iteration 139/1000 | Loss: 0.00001516
Iteration 140/1000 | Loss: 0.00001516
Iteration 141/1000 | Loss: 0.00001516
Iteration 142/1000 | Loss: 0.00001516
Iteration 143/1000 | Loss: 0.00001516
Iteration 144/1000 | Loss: 0.00001516
Iteration 145/1000 | Loss: 0.00001516
Iteration 146/1000 | Loss: 0.00001516
Iteration 147/1000 | Loss: 0.00001516
Iteration 148/1000 | Loss: 0.00001516
Iteration 149/1000 | Loss: 0.00001516
Iteration 150/1000 | Loss: 0.00001516
Iteration 151/1000 | Loss: 0.00001516
Iteration 152/1000 | Loss: 0.00001516
Iteration 153/1000 | Loss: 0.00001516
Iteration 154/1000 | Loss: 0.00001516
Iteration 155/1000 | Loss: 0.00001516
Iteration 156/1000 | Loss: 0.00001516
Iteration 157/1000 | Loss: 0.00001516
Iteration 158/1000 | Loss: 0.00001516
Iteration 159/1000 | Loss: 0.00001516
Iteration 160/1000 | Loss: 0.00001516
Iteration 161/1000 | Loss: 0.00001516
Iteration 162/1000 | Loss: 0.00001516
Iteration 163/1000 | Loss: 0.00001516
Iteration 164/1000 | Loss: 0.00001516
Iteration 165/1000 | Loss: 0.00001516
Iteration 166/1000 | Loss: 0.00001516
Iteration 167/1000 | Loss: 0.00001516
Iteration 168/1000 | Loss: 0.00001516
Iteration 169/1000 | Loss: 0.00001516
Iteration 170/1000 | Loss: 0.00001516
Iteration 171/1000 | Loss: 0.00001516
Iteration 172/1000 | Loss: 0.00001516
Iteration 173/1000 | Loss: 0.00001516
Iteration 174/1000 | Loss: 0.00001516
Iteration 175/1000 | Loss: 0.00001516
Iteration 176/1000 | Loss: 0.00001516
Iteration 177/1000 | Loss: 0.00001516
Iteration 178/1000 | Loss: 0.00001516
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [1.5161489500314929e-05, 1.5161489500314929e-05, 1.5161489500314929e-05, 1.5161489500314929e-05, 1.5161489500314929e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5161489500314929e-05

Optimization complete. Final v2v error: 3.2890167236328125 mm

Highest mean error: 3.6329240798950195 mm for frame 152

Lowest mean error: 2.9632067680358887 mm for frame 199

Saving results

Total time: 44.20811867713928
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00549650
Iteration 2/25 | Loss: 0.00098661
Iteration 3/25 | Loss: 0.00078192
Iteration 4/25 | Loss: 0.00075693
Iteration 5/25 | Loss: 0.00074815
Iteration 6/25 | Loss: 0.00074609
Iteration 7/25 | Loss: 0.00074581
Iteration 8/25 | Loss: 0.00074581
Iteration 9/25 | Loss: 0.00074581
Iteration 10/25 | Loss: 0.00074581
Iteration 11/25 | Loss: 0.00074581
Iteration 12/25 | Loss: 0.00074581
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000745805271435529, 0.000745805271435529, 0.000745805271435529, 0.000745805271435529, 0.000745805271435529]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000745805271435529

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.94146395
Iteration 2/25 | Loss: 0.00038563
Iteration 3/25 | Loss: 0.00038561
Iteration 4/25 | Loss: 0.00038561
Iteration 5/25 | Loss: 0.00038561
Iteration 6/25 | Loss: 0.00038560
Iteration 7/25 | Loss: 0.00038560
Iteration 8/25 | Loss: 0.00038560
Iteration 9/25 | Loss: 0.00038560
Iteration 10/25 | Loss: 0.00038560
Iteration 11/25 | Loss: 0.00038560
Iteration 12/25 | Loss: 0.00038560
Iteration 13/25 | Loss: 0.00038560
Iteration 14/25 | Loss: 0.00038560
Iteration 15/25 | Loss: 0.00038560
Iteration 16/25 | Loss: 0.00038560
Iteration 17/25 | Loss: 0.00038560
Iteration 18/25 | Loss: 0.00038560
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0003856035473290831, 0.0003856035473290831, 0.0003856035473290831, 0.0003856035473290831, 0.0003856035473290831]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003856035473290831

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00038560
Iteration 2/1000 | Loss: 0.00003329
Iteration 3/1000 | Loss: 0.00002393
Iteration 4/1000 | Loss: 0.00002175
Iteration 5/1000 | Loss: 0.00002071
Iteration 6/1000 | Loss: 0.00002005
Iteration 7/1000 | Loss: 0.00001962
Iteration 8/1000 | Loss: 0.00001933
Iteration 9/1000 | Loss: 0.00001908
Iteration 10/1000 | Loss: 0.00001900
Iteration 11/1000 | Loss: 0.00001897
Iteration 12/1000 | Loss: 0.00001892
Iteration 13/1000 | Loss: 0.00001892
Iteration 14/1000 | Loss: 0.00001891
Iteration 15/1000 | Loss: 0.00001891
Iteration 16/1000 | Loss: 0.00001890
Iteration 17/1000 | Loss: 0.00001889
Iteration 18/1000 | Loss: 0.00001888
Iteration 19/1000 | Loss: 0.00001888
Iteration 20/1000 | Loss: 0.00001888
Iteration 21/1000 | Loss: 0.00001888
Iteration 22/1000 | Loss: 0.00001887
Iteration 23/1000 | Loss: 0.00001887
Iteration 24/1000 | Loss: 0.00001886
Iteration 25/1000 | Loss: 0.00001885
Iteration 26/1000 | Loss: 0.00001885
Iteration 27/1000 | Loss: 0.00001884
Iteration 28/1000 | Loss: 0.00001883
Iteration 29/1000 | Loss: 0.00001882
Iteration 30/1000 | Loss: 0.00001881
Iteration 31/1000 | Loss: 0.00001881
Iteration 32/1000 | Loss: 0.00001880
Iteration 33/1000 | Loss: 0.00001880
Iteration 34/1000 | Loss: 0.00001879
Iteration 35/1000 | Loss: 0.00001877
Iteration 36/1000 | Loss: 0.00001877
Iteration 37/1000 | Loss: 0.00001876
Iteration 38/1000 | Loss: 0.00001876
Iteration 39/1000 | Loss: 0.00001876
Iteration 40/1000 | Loss: 0.00001876
Iteration 41/1000 | Loss: 0.00001876
Iteration 42/1000 | Loss: 0.00001875
Iteration 43/1000 | Loss: 0.00001875
Iteration 44/1000 | Loss: 0.00001873
Iteration 45/1000 | Loss: 0.00001873
Iteration 46/1000 | Loss: 0.00001872
Iteration 47/1000 | Loss: 0.00001872
Iteration 48/1000 | Loss: 0.00001872
Iteration 49/1000 | Loss: 0.00001872
Iteration 50/1000 | Loss: 0.00001871
Iteration 51/1000 | Loss: 0.00001871
Iteration 52/1000 | Loss: 0.00001871
Iteration 53/1000 | Loss: 0.00001871
Iteration 54/1000 | Loss: 0.00001871
Iteration 55/1000 | Loss: 0.00001870
Iteration 56/1000 | Loss: 0.00001870
Iteration 57/1000 | Loss: 0.00001869
Iteration 58/1000 | Loss: 0.00001869
Iteration 59/1000 | Loss: 0.00001869
Iteration 60/1000 | Loss: 0.00001868
Iteration 61/1000 | Loss: 0.00001868
Iteration 62/1000 | Loss: 0.00001868
Iteration 63/1000 | Loss: 0.00001868
Iteration 64/1000 | Loss: 0.00001868
Iteration 65/1000 | Loss: 0.00001868
Iteration 66/1000 | Loss: 0.00001868
Iteration 67/1000 | Loss: 0.00001868
Iteration 68/1000 | Loss: 0.00001868
Iteration 69/1000 | Loss: 0.00001868
Iteration 70/1000 | Loss: 0.00001868
Iteration 71/1000 | Loss: 0.00001868
Iteration 72/1000 | Loss: 0.00001868
Iteration 73/1000 | Loss: 0.00001868
Iteration 74/1000 | Loss: 0.00001868
Iteration 75/1000 | Loss: 0.00001867
Iteration 76/1000 | Loss: 0.00001867
Iteration 77/1000 | Loss: 0.00001866
Iteration 78/1000 | Loss: 0.00001866
Iteration 79/1000 | Loss: 0.00001866
Iteration 80/1000 | Loss: 0.00001866
Iteration 81/1000 | Loss: 0.00001866
Iteration 82/1000 | Loss: 0.00001866
Iteration 83/1000 | Loss: 0.00001866
Iteration 84/1000 | Loss: 0.00001865
Iteration 85/1000 | Loss: 0.00001865
Iteration 86/1000 | Loss: 0.00001865
Iteration 87/1000 | Loss: 0.00001864
Iteration 88/1000 | Loss: 0.00001864
Iteration 89/1000 | Loss: 0.00001863
Iteration 90/1000 | Loss: 0.00001863
Iteration 91/1000 | Loss: 0.00001863
Iteration 92/1000 | Loss: 0.00001863
Iteration 93/1000 | Loss: 0.00001863
Iteration 94/1000 | Loss: 0.00001863
Iteration 95/1000 | Loss: 0.00001863
Iteration 96/1000 | Loss: 0.00001862
Iteration 97/1000 | Loss: 0.00001862
Iteration 98/1000 | Loss: 0.00001862
Iteration 99/1000 | Loss: 0.00001862
Iteration 100/1000 | Loss: 0.00001862
Iteration 101/1000 | Loss: 0.00001862
Iteration 102/1000 | Loss: 0.00001861
Iteration 103/1000 | Loss: 0.00001861
Iteration 104/1000 | Loss: 0.00001861
Iteration 105/1000 | Loss: 0.00001861
Iteration 106/1000 | Loss: 0.00001861
Iteration 107/1000 | Loss: 0.00001861
Iteration 108/1000 | Loss: 0.00001861
Iteration 109/1000 | Loss: 0.00001861
Iteration 110/1000 | Loss: 0.00001861
Iteration 111/1000 | Loss: 0.00001861
Iteration 112/1000 | Loss: 0.00001861
Iteration 113/1000 | Loss: 0.00001861
Iteration 114/1000 | Loss: 0.00001861
Iteration 115/1000 | Loss: 0.00001861
Iteration 116/1000 | Loss: 0.00001861
Iteration 117/1000 | Loss: 0.00001861
Iteration 118/1000 | Loss: 0.00001861
Iteration 119/1000 | Loss: 0.00001861
Iteration 120/1000 | Loss: 0.00001861
Iteration 121/1000 | Loss: 0.00001861
Iteration 122/1000 | Loss: 0.00001861
Iteration 123/1000 | Loss: 0.00001861
Iteration 124/1000 | Loss: 0.00001861
Iteration 125/1000 | Loss: 0.00001861
Iteration 126/1000 | Loss: 0.00001861
Iteration 127/1000 | Loss: 0.00001861
Iteration 128/1000 | Loss: 0.00001861
Iteration 129/1000 | Loss: 0.00001861
Iteration 130/1000 | Loss: 0.00001861
Iteration 131/1000 | Loss: 0.00001861
Iteration 132/1000 | Loss: 0.00001861
Iteration 133/1000 | Loss: 0.00001861
Iteration 134/1000 | Loss: 0.00001861
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.8608816390042193e-05, 1.8608816390042193e-05, 1.8608816390042193e-05, 1.8608816390042193e-05, 1.8608816390042193e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8608816390042193e-05

Optimization complete. Final v2v error: 3.6389851570129395 mm

Highest mean error: 4.010209083557129 mm for frame 164

Lowest mean error: 3.2969589233398438 mm for frame 44

Saving results

Total time: 37.125694274902344
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00485510
Iteration 2/25 | Loss: 0.00094629
Iteration 3/25 | Loss: 0.00077018
Iteration 4/25 | Loss: 0.00073874
Iteration 5/25 | Loss: 0.00073151
Iteration 6/25 | Loss: 0.00073016
Iteration 7/25 | Loss: 0.00072970
Iteration 8/25 | Loss: 0.00072966
Iteration 9/25 | Loss: 0.00072966
Iteration 10/25 | Loss: 0.00072966
Iteration 11/25 | Loss: 0.00072966
Iteration 12/25 | Loss: 0.00072966
Iteration 13/25 | Loss: 0.00072966
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007296559051610529, 0.0007296559051610529, 0.0007296559051610529, 0.0007296559051610529, 0.0007296559051610529]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007296559051610529

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45241702
Iteration 2/25 | Loss: 0.00030242
Iteration 3/25 | Loss: 0.00030242
Iteration 4/25 | Loss: 0.00030242
Iteration 5/25 | Loss: 0.00030242
Iteration 6/25 | Loss: 0.00030242
Iteration 7/25 | Loss: 0.00030242
Iteration 8/25 | Loss: 0.00030242
Iteration 9/25 | Loss: 0.00030242
Iteration 10/25 | Loss: 0.00030242
Iteration 11/25 | Loss: 0.00030242
Iteration 12/25 | Loss: 0.00030242
Iteration 13/25 | Loss: 0.00030242
Iteration 14/25 | Loss: 0.00030242
Iteration 15/25 | Loss: 0.00030242
Iteration 16/25 | Loss: 0.00030242
Iteration 17/25 | Loss: 0.00030242
Iteration 18/25 | Loss: 0.00030242
Iteration 19/25 | Loss: 0.00030242
Iteration 20/25 | Loss: 0.00030242
Iteration 21/25 | Loss: 0.00030242
Iteration 22/25 | Loss: 0.00030242
Iteration 23/25 | Loss: 0.00030242
Iteration 24/25 | Loss: 0.00030242
Iteration 25/25 | Loss: 0.00030242

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030242
Iteration 2/1000 | Loss: 0.00005791
Iteration 3/1000 | Loss: 0.00004097
Iteration 4/1000 | Loss: 0.00003617
Iteration 5/1000 | Loss: 0.00003441
Iteration 6/1000 | Loss: 0.00003318
Iteration 7/1000 | Loss: 0.00003223
Iteration 8/1000 | Loss: 0.00003144
Iteration 9/1000 | Loss: 0.00003099
Iteration 10/1000 | Loss: 0.00003069
Iteration 11/1000 | Loss: 0.00003049
Iteration 12/1000 | Loss: 0.00003028
Iteration 13/1000 | Loss: 0.00003022
Iteration 14/1000 | Loss: 0.00003021
Iteration 15/1000 | Loss: 0.00003019
Iteration 16/1000 | Loss: 0.00003019
Iteration 17/1000 | Loss: 0.00003016
Iteration 18/1000 | Loss: 0.00003015
Iteration 19/1000 | Loss: 0.00003015
Iteration 20/1000 | Loss: 0.00003011
Iteration 21/1000 | Loss: 0.00003007
Iteration 22/1000 | Loss: 0.00003002
Iteration 23/1000 | Loss: 0.00002995
Iteration 24/1000 | Loss: 0.00002993
Iteration 25/1000 | Loss: 0.00002993
Iteration 26/1000 | Loss: 0.00002992
Iteration 27/1000 | Loss: 0.00002992
Iteration 28/1000 | Loss: 0.00002992
Iteration 29/1000 | Loss: 0.00002991
Iteration 30/1000 | Loss: 0.00002991
Iteration 31/1000 | Loss: 0.00002991
Iteration 32/1000 | Loss: 0.00002991
Iteration 33/1000 | Loss: 0.00002990
Iteration 34/1000 | Loss: 0.00002990
Iteration 35/1000 | Loss: 0.00002990
Iteration 36/1000 | Loss: 0.00002990
Iteration 37/1000 | Loss: 0.00002989
Iteration 38/1000 | Loss: 0.00002988
Iteration 39/1000 | Loss: 0.00002988
Iteration 40/1000 | Loss: 0.00002988
Iteration 41/1000 | Loss: 0.00002987
Iteration 42/1000 | Loss: 0.00002987
Iteration 43/1000 | Loss: 0.00002987
Iteration 44/1000 | Loss: 0.00002987
Iteration 45/1000 | Loss: 0.00002986
Iteration 46/1000 | Loss: 0.00002986
Iteration 47/1000 | Loss: 0.00002986
Iteration 48/1000 | Loss: 0.00002985
Iteration 49/1000 | Loss: 0.00002985
Iteration 50/1000 | Loss: 0.00002985
Iteration 51/1000 | Loss: 0.00002984
Iteration 52/1000 | Loss: 0.00002984
Iteration 53/1000 | Loss: 0.00002984
Iteration 54/1000 | Loss: 0.00002984
Iteration 55/1000 | Loss: 0.00002983
Iteration 56/1000 | Loss: 0.00002983
Iteration 57/1000 | Loss: 0.00002983
Iteration 58/1000 | Loss: 0.00002983
Iteration 59/1000 | Loss: 0.00002983
Iteration 60/1000 | Loss: 0.00002983
Iteration 61/1000 | Loss: 0.00002983
Iteration 62/1000 | Loss: 0.00002983
Iteration 63/1000 | Loss: 0.00002982
Iteration 64/1000 | Loss: 0.00002982
Iteration 65/1000 | Loss: 0.00002982
Iteration 66/1000 | Loss: 0.00002982
Iteration 67/1000 | Loss: 0.00002981
Iteration 68/1000 | Loss: 0.00002981
Iteration 69/1000 | Loss: 0.00002981
Iteration 70/1000 | Loss: 0.00002981
Iteration 71/1000 | Loss: 0.00002980
Iteration 72/1000 | Loss: 0.00002980
Iteration 73/1000 | Loss: 0.00002980
Iteration 74/1000 | Loss: 0.00002979
Iteration 75/1000 | Loss: 0.00002979
Iteration 76/1000 | Loss: 0.00002979
Iteration 77/1000 | Loss: 0.00002979
Iteration 78/1000 | Loss: 0.00002979
Iteration 79/1000 | Loss: 0.00002979
Iteration 80/1000 | Loss: 0.00002978
Iteration 81/1000 | Loss: 0.00002978
Iteration 82/1000 | Loss: 0.00002978
Iteration 83/1000 | Loss: 0.00002978
Iteration 84/1000 | Loss: 0.00002978
Iteration 85/1000 | Loss: 0.00002978
Iteration 86/1000 | Loss: 0.00002977
Iteration 87/1000 | Loss: 0.00002977
Iteration 88/1000 | Loss: 0.00002977
Iteration 89/1000 | Loss: 0.00002976
Iteration 90/1000 | Loss: 0.00002976
Iteration 91/1000 | Loss: 0.00002976
Iteration 92/1000 | Loss: 0.00002976
Iteration 93/1000 | Loss: 0.00002976
Iteration 94/1000 | Loss: 0.00002975
Iteration 95/1000 | Loss: 0.00002975
Iteration 96/1000 | Loss: 0.00002975
Iteration 97/1000 | Loss: 0.00002975
Iteration 98/1000 | Loss: 0.00002975
Iteration 99/1000 | Loss: 0.00002975
Iteration 100/1000 | Loss: 0.00002975
Iteration 101/1000 | Loss: 0.00002975
Iteration 102/1000 | Loss: 0.00002975
Iteration 103/1000 | Loss: 0.00002975
Iteration 104/1000 | Loss: 0.00002975
Iteration 105/1000 | Loss: 0.00002975
Iteration 106/1000 | Loss: 0.00002974
Iteration 107/1000 | Loss: 0.00002974
Iteration 108/1000 | Loss: 0.00002974
Iteration 109/1000 | Loss: 0.00002974
Iteration 110/1000 | Loss: 0.00002974
Iteration 111/1000 | Loss: 0.00002974
Iteration 112/1000 | Loss: 0.00002973
Iteration 113/1000 | Loss: 0.00002973
Iteration 114/1000 | Loss: 0.00002973
Iteration 115/1000 | Loss: 0.00002973
Iteration 116/1000 | Loss: 0.00002973
Iteration 117/1000 | Loss: 0.00002973
Iteration 118/1000 | Loss: 0.00002973
Iteration 119/1000 | Loss: 0.00002973
Iteration 120/1000 | Loss: 0.00002973
Iteration 121/1000 | Loss: 0.00002973
Iteration 122/1000 | Loss: 0.00002973
Iteration 123/1000 | Loss: 0.00002973
Iteration 124/1000 | Loss: 0.00002973
Iteration 125/1000 | Loss: 0.00002973
Iteration 126/1000 | Loss: 0.00002973
Iteration 127/1000 | Loss: 0.00002973
Iteration 128/1000 | Loss: 0.00002973
Iteration 129/1000 | Loss: 0.00002973
Iteration 130/1000 | Loss: 0.00002973
Iteration 131/1000 | Loss: 0.00002973
Iteration 132/1000 | Loss: 0.00002973
Iteration 133/1000 | Loss: 0.00002973
Iteration 134/1000 | Loss: 0.00002973
Iteration 135/1000 | Loss: 0.00002973
Iteration 136/1000 | Loss: 0.00002973
Iteration 137/1000 | Loss: 0.00002973
Iteration 138/1000 | Loss: 0.00002973
Iteration 139/1000 | Loss: 0.00002973
Iteration 140/1000 | Loss: 0.00002973
Iteration 141/1000 | Loss: 0.00002973
Iteration 142/1000 | Loss: 0.00002973
Iteration 143/1000 | Loss: 0.00002973
Iteration 144/1000 | Loss: 0.00002973
Iteration 145/1000 | Loss: 0.00002973
Iteration 146/1000 | Loss: 0.00002973
Iteration 147/1000 | Loss: 0.00002973
Iteration 148/1000 | Loss: 0.00002973
Iteration 149/1000 | Loss: 0.00002973
Iteration 150/1000 | Loss: 0.00002973
Iteration 151/1000 | Loss: 0.00002973
Iteration 152/1000 | Loss: 0.00002973
Iteration 153/1000 | Loss: 0.00002973
Iteration 154/1000 | Loss: 0.00002973
Iteration 155/1000 | Loss: 0.00002973
Iteration 156/1000 | Loss: 0.00002973
Iteration 157/1000 | Loss: 0.00002973
Iteration 158/1000 | Loss: 0.00002973
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [2.9729817470069975e-05, 2.9729817470069975e-05, 2.9729817470069975e-05, 2.9729817470069975e-05, 2.9729817470069975e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9729817470069975e-05

Optimization complete. Final v2v error: 4.491764068603516 mm

Highest mean error: 5.614936351776123 mm for frame 39

Lowest mean error: 4.013852119445801 mm for frame 22

Saving results

Total time: 39.575626373291016
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00399517
Iteration 2/25 | Loss: 0.00077701
Iteration 3/25 | Loss: 0.00067953
Iteration 4/25 | Loss: 0.00065088
Iteration 5/25 | Loss: 0.00064705
Iteration 6/25 | Loss: 0.00064623
Iteration 7/25 | Loss: 0.00064595
Iteration 8/25 | Loss: 0.00064595
Iteration 9/25 | Loss: 0.00064595
Iteration 10/25 | Loss: 0.00064595
Iteration 11/25 | Loss: 0.00064595
Iteration 12/25 | Loss: 0.00064595
Iteration 13/25 | Loss: 0.00064595
Iteration 14/25 | Loss: 0.00064595
Iteration 15/25 | Loss: 0.00064595
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0006459529395215213, 0.0006459529395215213, 0.0006459529395215213, 0.0006459529395215213, 0.0006459529395215213]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006459529395215213

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45545614
Iteration 2/25 | Loss: 0.00027450
Iteration 3/25 | Loss: 0.00027450
Iteration 4/25 | Loss: 0.00027450
Iteration 5/25 | Loss: 0.00027450
Iteration 6/25 | Loss: 0.00027450
Iteration 7/25 | Loss: 0.00027450
Iteration 8/25 | Loss: 0.00027450
Iteration 9/25 | Loss: 0.00027450
Iteration 10/25 | Loss: 0.00027450
Iteration 11/25 | Loss: 0.00027450
Iteration 12/25 | Loss: 0.00027450
Iteration 13/25 | Loss: 0.00027450
Iteration 14/25 | Loss: 0.00027450
Iteration 15/25 | Loss: 0.00027450
Iteration 16/25 | Loss: 0.00027450
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00027449868503026664, 0.00027449868503026664, 0.00027449868503026664, 0.00027449868503026664, 0.00027449868503026664]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00027449868503026664

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00027450
Iteration 2/1000 | Loss: 0.00002900
Iteration 3/1000 | Loss: 0.00002141
Iteration 4/1000 | Loss: 0.00002004
Iteration 5/1000 | Loss: 0.00001878
Iteration 6/1000 | Loss: 0.00001825
Iteration 7/1000 | Loss: 0.00001771
Iteration 8/1000 | Loss: 0.00001771
Iteration 9/1000 | Loss: 0.00001745
Iteration 10/1000 | Loss: 0.00001726
Iteration 11/1000 | Loss: 0.00001719
Iteration 12/1000 | Loss: 0.00001719
Iteration 13/1000 | Loss: 0.00001718
Iteration 14/1000 | Loss: 0.00001718
Iteration 15/1000 | Loss: 0.00001714
Iteration 16/1000 | Loss: 0.00001714
Iteration 17/1000 | Loss: 0.00001712
Iteration 18/1000 | Loss: 0.00001712
Iteration 19/1000 | Loss: 0.00001712
Iteration 20/1000 | Loss: 0.00001711
Iteration 21/1000 | Loss: 0.00001709
Iteration 22/1000 | Loss: 0.00001709
Iteration 23/1000 | Loss: 0.00001709
Iteration 24/1000 | Loss: 0.00001709
Iteration 25/1000 | Loss: 0.00001709
Iteration 26/1000 | Loss: 0.00001709
Iteration 27/1000 | Loss: 0.00001709
Iteration 28/1000 | Loss: 0.00001709
Iteration 29/1000 | Loss: 0.00001709
Iteration 30/1000 | Loss: 0.00001708
Iteration 31/1000 | Loss: 0.00001708
Iteration 32/1000 | Loss: 0.00001707
Iteration 33/1000 | Loss: 0.00001707
Iteration 34/1000 | Loss: 0.00001705
Iteration 35/1000 | Loss: 0.00001704
Iteration 36/1000 | Loss: 0.00001704
Iteration 37/1000 | Loss: 0.00001703
Iteration 38/1000 | Loss: 0.00001703
Iteration 39/1000 | Loss: 0.00001703
Iteration 40/1000 | Loss: 0.00001703
Iteration 41/1000 | Loss: 0.00001703
Iteration 42/1000 | Loss: 0.00001703
Iteration 43/1000 | Loss: 0.00001702
Iteration 44/1000 | Loss: 0.00001702
Iteration 45/1000 | Loss: 0.00001702
Iteration 46/1000 | Loss: 0.00001701
Iteration 47/1000 | Loss: 0.00001701
Iteration 48/1000 | Loss: 0.00001701
Iteration 49/1000 | Loss: 0.00001700
Iteration 50/1000 | Loss: 0.00001700
Iteration 51/1000 | Loss: 0.00001700
Iteration 52/1000 | Loss: 0.00001700
Iteration 53/1000 | Loss: 0.00001700
Iteration 54/1000 | Loss: 0.00001700
Iteration 55/1000 | Loss: 0.00001699
Iteration 56/1000 | Loss: 0.00001699
Iteration 57/1000 | Loss: 0.00001699
Iteration 58/1000 | Loss: 0.00001699
Iteration 59/1000 | Loss: 0.00001699
Iteration 60/1000 | Loss: 0.00001699
Iteration 61/1000 | Loss: 0.00001699
Iteration 62/1000 | Loss: 0.00001699
Iteration 63/1000 | Loss: 0.00001699
Iteration 64/1000 | Loss: 0.00001699
Iteration 65/1000 | Loss: 0.00001698
Iteration 66/1000 | Loss: 0.00001698
Iteration 67/1000 | Loss: 0.00001698
Iteration 68/1000 | Loss: 0.00001698
Iteration 69/1000 | Loss: 0.00001698
Iteration 70/1000 | Loss: 0.00001698
Iteration 71/1000 | Loss: 0.00001697
Iteration 72/1000 | Loss: 0.00001697
Iteration 73/1000 | Loss: 0.00001697
Iteration 74/1000 | Loss: 0.00001696
Iteration 75/1000 | Loss: 0.00001696
Iteration 76/1000 | Loss: 0.00001696
Iteration 77/1000 | Loss: 0.00001696
Iteration 78/1000 | Loss: 0.00001696
Iteration 79/1000 | Loss: 0.00001696
Iteration 80/1000 | Loss: 0.00001696
Iteration 81/1000 | Loss: 0.00001696
Iteration 82/1000 | Loss: 0.00001696
Iteration 83/1000 | Loss: 0.00001696
Iteration 84/1000 | Loss: 0.00001696
Iteration 85/1000 | Loss: 0.00001696
Iteration 86/1000 | Loss: 0.00001695
Iteration 87/1000 | Loss: 0.00001695
Iteration 88/1000 | Loss: 0.00001695
Iteration 89/1000 | Loss: 0.00001695
Iteration 90/1000 | Loss: 0.00001695
Iteration 91/1000 | Loss: 0.00001695
Iteration 92/1000 | Loss: 0.00001694
Iteration 93/1000 | Loss: 0.00001694
Iteration 94/1000 | Loss: 0.00001694
Iteration 95/1000 | Loss: 0.00001694
Iteration 96/1000 | Loss: 0.00001694
Iteration 97/1000 | Loss: 0.00001694
Iteration 98/1000 | Loss: 0.00001694
Iteration 99/1000 | Loss: 0.00001693
Iteration 100/1000 | Loss: 0.00001693
Iteration 101/1000 | Loss: 0.00001693
Iteration 102/1000 | Loss: 0.00001693
Iteration 103/1000 | Loss: 0.00001693
Iteration 104/1000 | Loss: 0.00001693
Iteration 105/1000 | Loss: 0.00001692
Iteration 106/1000 | Loss: 0.00001692
Iteration 107/1000 | Loss: 0.00001691
Iteration 108/1000 | Loss: 0.00001690
Iteration 109/1000 | Loss: 0.00001689
Iteration 110/1000 | Loss: 0.00001689
Iteration 111/1000 | Loss: 0.00001689
Iteration 112/1000 | Loss: 0.00001689
Iteration 113/1000 | Loss: 0.00001689
Iteration 114/1000 | Loss: 0.00001689
Iteration 115/1000 | Loss: 0.00001689
Iteration 116/1000 | Loss: 0.00001689
Iteration 117/1000 | Loss: 0.00001689
Iteration 118/1000 | Loss: 0.00001688
Iteration 119/1000 | Loss: 0.00001688
Iteration 120/1000 | Loss: 0.00001686
Iteration 121/1000 | Loss: 0.00001685
Iteration 122/1000 | Loss: 0.00001685
Iteration 123/1000 | Loss: 0.00001684
Iteration 124/1000 | Loss: 0.00001683
Iteration 125/1000 | Loss: 0.00001683
Iteration 126/1000 | Loss: 0.00001682
Iteration 127/1000 | Loss: 0.00001682
Iteration 128/1000 | Loss: 0.00001682
Iteration 129/1000 | Loss: 0.00001682
Iteration 130/1000 | Loss: 0.00001682
Iteration 131/1000 | Loss: 0.00001682
Iteration 132/1000 | Loss: 0.00001682
Iteration 133/1000 | Loss: 0.00001682
Iteration 134/1000 | Loss: 0.00001682
Iteration 135/1000 | Loss: 0.00001682
Iteration 136/1000 | Loss: 0.00001681
Iteration 137/1000 | Loss: 0.00001681
Iteration 138/1000 | Loss: 0.00001681
Iteration 139/1000 | Loss: 0.00001681
Iteration 140/1000 | Loss: 0.00001681
Iteration 141/1000 | Loss: 0.00001681
Iteration 142/1000 | Loss: 0.00001681
Iteration 143/1000 | Loss: 0.00001681
Iteration 144/1000 | Loss: 0.00001681
Iteration 145/1000 | Loss: 0.00001681
Iteration 146/1000 | Loss: 0.00001681
Iteration 147/1000 | Loss: 0.00001681
Iteration 148/1000 | Loss: 0.00001681
Iteration 149/1000 | Loss: 0.00001681
Iteration 150/1000 | Loss: 0.00001681
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.680794775893446e-05, 1.680794775893446e-05, 1.680794775893446e-05, 1.680794775893446e-05, 1.680794775893446e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.680794775893446e-05

Optimization complete. Final v2v error: 3.4470298290252686 mm

Highest mean error: 3.6368954181671143 mm for frame 119

Lowest mean error: 3.358963966369629 mm for frame 110

Saving results

Total time: 34.65219283103943
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00960424
Iteration 2/25 | Loss: 0.00225472
Iteration 3/25 | Loss: 0.00124840
Iteration 4/25 | Loss: 0.00108357
Iteration 5/25 | Loss: 0.00096713
Iteration 6/25 | Loss: 0.00095454
Iteration 7/25 | Loss: 0.00095961
Iteration 8/25 | Loss: 0.00092489
Iteration 9/25 | Loss: 0.00092054
Iteration 10/25 | Loss: 0.00089753
Iteration 11/25 | Loss: 0.00088663
Iteration 12/25 | Loss: 0.00087630
Iteration 13/25 | Loss: 0.00088190
Iteration 14/25 | Loss: 0.00087812
Iteration 15/25 | Loss: 0.00087596
Iteration 16/25 | Loss: 0.00087274
Iteration 17/25 | Loss: 0.00086670
Iteration 18/25 | Loss: 0.00086668
Iteration 19/25 | Loss: 0.00086741
Iteration 20/25 | Loss: 0.00086749
Iteration 21/25 | Loss: 0.00086699
Iteration 22/25 | Loss: 0.00086842
Iteration 23/25 | Loss: 0.00086209
Iteration 24/25 | Loss: 0.00086052
Iteration 25/25 | Loss: 0.00086453

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.46148586
Iteration 2/25 | Loss: 0.00223294
Iteration 3/25 | Loss: 0.00223293
Iteration 4/25 | Loss: 0.00223293
Iteration 5/25 | Loss: 0.00223293
Iteration 6/25 | Loss: 0.00223293
Iteration 7/25 | Loss: 0.00223293
Iteration 8/25 | Loss: 0.00223293
Iteration 9/25 | Loss: 0.00223293
Iteration 10/25 | Loss: 0.00223293
Iteration 11/25 | Loss: 0.00223293
Iteration 12/25 | Loss: 0.00223293
Iteration 13/25 | Loss: 0.00223293
Iteration 14/25 | Loss: 0.00223293
Iteration 15/25 | Loss: 0.00223293
Iteration 16/25 | Loss: 0.00223293
Iteration 17/25 | Loss: 0.00223293
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0022329320199787617, 0.0022329320199787617, 0.0022329320199787617, 0.0022329320199787617, 0.0022329320199787617]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022329320199787617

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00223293
Iteration 2/1000 | Loss: 0.00044557
Iteration 3/1000 | Loss: 0.00153927
Iteration 4/1000 | Loss: 0.00388812
Iteration 5/1000 | Loss: 0.00133261
Iteration 6/1000 | Loss: 0.00239937
Iteration 7/1000 | Loss: 0.00258703
Iteration 8/1000 | Loss: 0.00110288
Iteration 9/1000 | Loss: 0.00110466
Iteration 10/1000 | Loss: 0.00012367
Iteration 11/1000 | Loss: 0.00009948
Iteration 12/1000 | Loss: 0.00005731
Iteration 13/1000 | Loss: 0.00004527
Iteration 14/1000 | Loss: 0.00003770
Iteration 15/1000 | Loss: 0.00003365
Iteration 16/1000 | Loss: 0.00003077
Iteration 17/1000 | Loss: 0.00002825
Iteration 18/1000 | Loss: 0.00002675
Iteration 19/1000 | Loss: 0.00018998
Iteration 20/1000 | Loss: 0.00003164
Iteration 21/1000 | Loss: 0.00002870
Iteration 22/1000 | Loss: 0.00023758
Iteration 23/1000 | Loss: 0.00003768
Iteration 24/1000 | Loss: 0.00107290
Iteration 25/1000 | Loss: 0.00005336
Iteration 26/1000 | Loss: 0.00004221
Iteration 27/1000 | Loss: 0.00003327
Iteration 28/1000 | Loss: 0.00002649
Iteration 29/1000 | Loss: 0.00002365
Iteration 30/1000 | Loss: 0.00002140
Iteration 31/1000 | Loss: 0.00002033
Iteration 32/1000 | Loss: 0.00002988
Iteration 33/1000 | Loss: 0.00002040
Iteration 34/1000 | Loss: 0.00001963
Iteration 35/1000 | Loss: 0.00001910
Iteration 36/1000 | Loss: 0.00001878
Iteration 37/1000 | Loss: 0.00001851
Iteration 38/1000 | Loss: 0.00001844
Iteration 39/1000 | Loss: 0.00001843
Iteration 40/1000 | Loss: 0.00001842
Iteration 41/1000 | Loss: 0.00001841
Iteration 42/1000 | Loss: 0.00001822
Iteration 43/1000 | Loss: 0.00001807
Iteration 44/1000 | Loss: 0.00001800
Iteration 45/1000 | Loss: 0.00001794
Iteration 46/1000 | Loss: 0.00001790
Iteration 47/1000 | Loss: 0.00001789
Iteration 48/1000 | Loss: 0.00001789
Iteration 49/1000 | Loss: 0.00001788
Iteration 50/1000 | Loss: 0.00001788
Iteration 51/1000 | Loss: 0.00001787
Iteration 52/1000 | Loss: 0.00001787
Iteration 53/1000 | Loss: 0.00001787
Iteration 54/1000 | Loss: 0.00001786
Iteration 55/1000 | Loss: 0.00001786
Iteration 56/1000 | Loss: 0.00001786
Iteration 57/1000 | Loss: 0.00001786
Iteration 58/1000 | Loss: 0.00001785
Iteration 59/1000 | Loss: 0.00001785
Iteration 60/1000 | Loss: 0.00001784
Iteration 61/1000 | Loss: 0.00001784
Iteration 62/1000 | Loss: 0.00001783
Iteration 63/1000 | Loss: 0.00001783
Iteration 64/1000 | Loss: 0.00001782
Iteration 65/1000 | Loss: 0.00001782
Iteration 66/1000 | Loss: 0.00001782
Iteration 67/1000 | Loss: 0.00001781
Iteration 68/1000 | Loss: 0.00001781
Iteration 69/1000 | Loss: 0.00001781
Iteration 70/1000 | Loss: 0.00001780
Iteration 71/1000 | Loss: 0.00001780
Iteration 72/1000 | Loss: 0.00001780
Iteration 73/1000 | Loss: 0.00001779
Iteration 74/1000 | Loss: 0.00001779
Iteration 75/1000 | Loss: 0.00001779
Iteration 76/1000 | Loss: 0.00001778
Iteration 77/1000 | Loss: 0.00001778
Iteration 78/1000 | Loss: 0.00001778
Iteration 79/1000 | Loss: 0.00001777
Iteration 80/1000 | Loss: 0.00001777
Iteration 81/1000 | Loss: 0.00001776
Iteration 82/1000 | Loss: 0.00001776
Iteration 83/1000 | Loss: 0.00001776
Iteration 84/1000 | Loss: 0.00001775
Iteration 85/1000 | Loss: 0.00001775
Iteration 86/1000 | Loss: 0.00001775
Iteration 87/1000 | Loss: 0.00001774
Iteration 88/1000 | Loss: 0.00001774
Iteration 89/1000 | Loss: 0.00001774
Iteration 90/1000 | Loss: 0.00001774
Iteration 91/1000 | Loss: 0.00001774
Iteration 92/1000 | Loss: 0.00001773
Iteration 93/1000 | Loss: 0.00001773
Iteration 94/1000 | Loss: 0.00001773
Iteration 95/1000 | Loss: 0.00001772
Iteration 96/1000 | Loss: 0.00001772
Iteration 97/1000 | Loss: 0.00001772
Iteration 98/1000 | Loss: 0.00001772
Iteration 99/1000 | Loss: 0.00001772
Iteration 100/1000 | Loss: 0.00001772
Iteration 101/1000 | Loss: 0.00001772
Iteration 102/1000 | Loss: 0.00001771
Iteration 103/1000 | Loss: 0.00001771
Iteration 104/1000 | Loss: 0.00001771
Iteration 105/1000 | Loss: 0.00001771
Iteration 106/1000 | Loss: 0.00001771
Iteration 107/1000 | Loss: 0.00001771
Iteration 108/1000 | Loss: 0.00001771
Iteration 109/1000 | Loss: 0.00001770
Iteration 110/1000 | Loss: 0.00001770
Iteration 111/1000 | Loss: 0.00001770
Iteration 112/1000 | Loss: 0.00001770
Iteration 113/1000 | Loss: 0.00001770
Iteration 114/1000 | Loss: 0.00001770
Iteration 115/1000 | Loss: 0.00001770
Iteration 116/1000 | Loss: 0.00001770
Iteration 117/1000 | Loss: 0.00001770
Iteration 118/1000 | Loss: 0.00001770
Iteration 119/1000 | Loss: 0.00001770
Iteration 120/1000 | Loss: 0.00001770
Iteration 121/1000 | Loss: 0.00001770
Iteration 122/1000 | Loss: 0.00001770
Iteration 123/1000 | Loss: 0.00001770
Iteration 124/1000 | Loss: 0.00001770
Iteration 125/1000 | Loss: 0.00001770
Iteration 126/1000 | Loss: 0.00001770
Iteration 127/1000 | Loss: 0.00001770
Iteration 128/1000 | Loss: 0.00001770
Iteration 129/1000 | Loss: 0.00001770
Iteration 130/1000 | Loss: 0.00001770
Iteration 131/1000 | Loss: 0.00001770
Iteration 132/1000 | Loss: 0.00001770
Iteration 133/1000 | Loss: 0.00001770
Iteration 134/1000 | Loss: 0.00001770
Iteration 135/1000 | Loss: 0.00001770
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [1.7696218492346816e-05, 1.7696218492346816e-05, 1.7696218492346816e-05, 1.7696218492346816e-05, 1.7696218492346816e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7696218492346816e-05

Optimization complete. Final v2v error: 3.447437286376953 mm

Highest mean error: 5.677114009857178 mm for frame 62

Lowest mean error: 2.7877137660980225 mm for frame 119

Saving results

Total time: 108.38905048370361
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01058702
Iteration 2/25 | Loss: 0.00187510
Iteration 3/25 | Loss: 0.00120618
Iteration 4/25 | Loss: 0.00093172
Iteration 5/25 | Loss: 0.00087791
Iteration 6/25 | Loss: 0.00078629
Iteration 7/25 | Loss: 0.00075837
Iteration 8/25 | Loss: 0.00075367
Iteration 9/25 | Loss: 0.00075279
Iteration 10/25 | Loss: 0.00075747
Iteration 11/25 | Loss: 0.00075105
Iteration 12/25 | Loss: 0.00074927
Iteration 13/25 | Loss: 0.00074876
Iteration 14/25 | Loss: 0.00074868
Iteration 15/25 | Loss: 0.00074868
Iteration 16/25 | Loss: 0.00074868
Iteration 17/25 | Loss: 0.00074868
Iteration 18/25 | Loss: 0.00074868
Iteration 19/25 | Loss: 0.00074868
Iteration 20/25 | Loss: 0.00074868
Iteration 21/25 | Loss: 0.00074868
Iteration 22/25 | Loss: 0.00074868
Iteration 23/25 | Loss: 0.00074867
Iteration 24/25 | Loss: 0.00074867
Iteration 25/25 | Loss: 0.00074867

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42856181
Iteration 2/25 | Loss: 0.00042359
Iteration 3/25 | Loss: 0.00042359
Iteration 4/25 | Loss: 0.00042359
Iteration 5/25 | Loss: 0.00042359
Iteration 6/25 | Loss: 0.00042359
Iteration 7/25 | Loss: 0.00042359
Iteration 8/25 | Loss: 0.00042359
Iteration 9/25 | Loss: 0.00042359
Iteration 10/25 | Loss: 0.00042359
Iteration 11/25 | Loss: 0.00042359
Iteration 12/25 | Loss: 0.00042359
Iteration 13/25 | Loss: 0.00042359
Iteration 14/25 | Loss: 0.00042359
Iteration 15/25 | Loss: 0.00042359
Iteration 16/25 | Loss: 0.00042359
Iteration 17/25 | Loss: 0.00042359
Iteration 18/25 | Loss: 0.00042359
Iteration 19/25 | Loss: 0.00042359
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00042358573409728706, 0.00042358573409728706, 0.00042358573409728706, 0.00042358573409728706, 0.00042358573409728706]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00042358573409728706

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00042359
Iteration 2/1000 | Loss: 0.00003631
Iteration 3/1000 | Loss: 0.00002493
Iteration 4/1000 | Loss: 0.00002281
Iteration 5/1000 | Loss: 0.00002140
Iteration 6/1000 | Loss: 0.00002074
Iteration 7/1000 | Loss: 0.00002016
Iteration 8/1000 | Loss: 0.00001978
Iteration 9/1000 | Loss: 0.00001938
Iteration 10/1000 | Loss: 0.00001919
Iteration 11/1000 | Loss: 0.00001916
Iteration 12/1000 | Loss: 0.00001915
Iteration 13/1000 | Loss: 0.00001909
Iteration 14/1000 | Loss: 0.00001903
Iteration 15/1000 | Loss: 0.00001902
Iteration 16/1000 | Loss: 0.00001901
Iteration 17/1000 | Loss: 0.00001901
Iteration 18/1000 | Loss: 0.00001901
Iteration 19/1000 | Loss: 0.00001900
Iteration 20/1000 | Loss: 0.00001900
Iteration 21/1000 | Loss: 0.00001900
Iteration 22/1000 | Loss: 0.00001898
Iteration 23/1000 | Loss: 0.00001898
Iteration 24/1000 | Loss: 0.00001897
Iteration 25/1000 | Loss: 0.00001893
Iteration 26/1000 | Loss: 0.00001889
Iteration 27/1000 | Loss: 0.00001889
Iteration 28/1000 | Loss: 0.00001887
Iteration 29/1000 | Loss: 0.00001887
Iteration 30/1000 | Loss: 0.00001886
Iteration 31/1000 | Loss: 0.00001886
Iteration 32/1000 | Loss: 0.00001886
Iteration 33/1000 | Loss: 0.00001886
Iteration 34/1000 | Loss: 0.00001886
Iteration 35/1000 | Loss: 0.00001886
Iteration 36/1000 | Loss: 0.00001886
Iteration 37/1000 | Loss: 0.00001885
Iteration 38/1000 | Loss: 0.00001885
Iteration 39/1000 | Loss: 0.00001885
Iteration 40/1000 | Loss: 0.00001885
Iteration 41/1000 | Loss: 0.00001884
Iteration 42/1000 | Loss: 0.00001884
Iteration 43/1000 | Loss: 0.00001884
Iteration 44/1000 | Loss: 0.00001884
Iteration 45/1000 | Loss: 0.00001883
Iteration 46/1000 | Loss: 0.00001883
Iteration 47/1000 | Loss: 0.00001883
Iteration 48/1000 | Loss: 0.00001883
Iteration 49/1000 | Loss: 0.00001883
Iteration 50/1000 | Loss: 0.00001882
Iteration 51/1000 | Loss: 0.00001882
Iteration 52/1000 | Loss: 0.00001882
Iteration 53/1000 | Loss: 0.00001882
Iteration 54/1000 | Loss: 0.00001882
Iteration 55/1000 | Loss: 0.00001882
Iteration 56/1000 | Loss: 0.00001882
Iteration 57/1000 | Loss: 0.00001882
Iteration 58/1000 | Loss: 0.00001882
Iteration 59/1000 | Loss: 0.00001882
Iteration 60/1000 | Loss: 0.00001882
Iteration 61/1000 | Loss: 0.00001881
Iteration 62/1000 | Loss: 0.00001881
Iteration 63/1000 | Loss: 0.00001881
Iteration 64/1000 | Loss: 0.00001881
Iteration 65/1000 | Loss: 0.00001881
Iteration 66/1000 | Loss: 0.00001881
Iteration 67/1000 | Loss: 0.00001881
Iteration 68/1000 | Loss: 0.00001881
Iteration 69/1000 | Loss: 0.00001881
Iteration 70/1000 | Loss: 0.00001880
Iteration 71/1000 | Loss: 0.00001880
Iteration 72/1000 | Loss: 0.00001880
Iteration 73/1000 | Loss: 0.00001880
Iteration 74/1000 | Loss: 0.00001880
Iteration 75/1000 | Loss: 0.00001880
Iteration 76/1000 | Loss: 0.00001880
Iteration 77/1000 | Loss: 0.00001880
Iteration 78/1000 | Loss: 0.00001880
Iteration 79/1000 | Loss: 0.00001880
Iteration 80/1000 | Loss: 0.00001880
Iteration 81/1000 | Loss: 0.00001880
Iteration 82/1000 | Loss: 0.00001880
Iteration 83/1000 | Loss: 0.00001879
Iteration 84/1000 | Loss: 0.00001879
Iteration 85/1000 | Loss: 0.00001878
Iteration 86/1000 | Loss: 0.00001878
Iteration 87/1000 | Loss: 0.00001878
Iteration 88/1000 | Loss: 0.00001878
Iteration 89/1000 | Loss: 0.00001877
Iteration 90/1000 | Loss: 0.00001877
Iteration 91/1000 | Loss: 0.00001876
Iteration 92/1000 | Loss: 0.00001876
Iteration 93/1000 | Loss: 0.00001876
Iteration 94/1000 | Loss: 0.00001875
Iteration 95/1000 | Loss: 0.00001875
Iteration 96/1000 | Loss: 0.00001875
Iteration 97/1000 | Loss: 0.00001875
Iteration 98/1000 | Loss: 0.00001875
Iteration 99/1000 | Loss: 0.00001875
Iteration 100/1000 | Loss: 0.00001875
Iteration 101/1000 | Loss: 0.00001875
Iteration 102/1000 | Loss: 0.00001875
Iteration 103/1000 | Loss: 0.00001875
Iteration 104/1000 | Loss: 0.00001875
Iteration 105/1000 | Loss: 0.00001875
Iteration 106/1000 | Loss: 0.00001875
Iteration 107/1000 | Loss: 0.00001874
Iteration 108/1000 | Loss: 0.00001874
Iteration 109/1000 | Loss: 0.00001874
Iteration 110/1000 | Loss: 0.00001874
Iteration 111/1000 | Loss: 0.00001874
Iteration 112/1000 | Loss: 0.00001874
Iteration 113/1000 | Loss: 0.00001874
Iteration 114/1000 | Loss: 0.00001874
Iteration 115/1000 | Loss: 0.00001874
Iteration 116/1000 | Loss: 0.00001874
Iteration 117/1000 | Loss: 0.00001874
Iteration 118/1000 | Loss: 0.00001874
Iteration 119/1000 | Loss: 0.00001874
Iteration 120/1000 | Loss: 0.00001874
Iteration 121/1000 | Loss: 0.00001874
Iteration 122/1000 | Loss: 0.00001874
Iteration 123/1000 | Loss: 0.00001873
Iteration 124/1000 | Loss: 0.00001873
Iteration 125/1000 | Loss: 0.00001873
Iteration 126/1000 | Loss: 0.00001873
Iteration 127/1000 | Loss: 0.00001873
Iteration 128/1000 | Loss: 0.00001873
Iteration 129/1000 | Loss: 0.00001873
Iteration 130/1000 | Loss: 0.00001873
Iteration 131/1000 | Loss: 0.00001873
Iteration 132/1000 | Loss: 0.00001873
Iteration 133/1000 | Loss: 0.00001873
Iteration 134/1000 | Loss: 0.00001873
Iteration 135/1000 | Loss: 0.00001873
Iteration 136/1000 | Loss: 0.00001873
Iteration 137/1000 | Loss: 0.00001873
Iteration 138/1000 | Loss: 0.00001873
Iteration 139/1000 | Loss: 0.00001873
Iteration 140/1000 | Loss: 0.00001873
Iteration 141/1000 | Loss: 0.00001872
Iteration 142/1000 | Loss: 0.00001872
Iteration 143/1000 | Loss: 0.00001872
Iteration 144/1000 | Loss: 0.00001872
Iteration 145/1000 | Loss: 0.00001872
Iteration 146/1000 | Loss: 0.00001872
Iteration 147/1000 | Loss: 0.00001872
Iteration 148/1000 | Loss: 0.00001872
Iteration 149/1000 | Loss: 0.00001872
Iteration 150/1000 | Loss: 0.00001872
Iteration 151/1000 | Loss: 0.00001872
Iteration 152/1000 | Loss: 0.00001871
Iteration 153/1000 | Loss: 0.00001871
Iteration 154/1000 | Loss: 0.00001871
Iteration 155/1000 | Loss: 0.00001871
Iteration 156/1000 | Loss: 0.00001871
Iteration 157/1000 | Loss: 0.00001871
Iteration 158/1000 | Loss: 0.00001871
Iteration 159/1000 | Loss: 0.00001871
Iteration 160/1000 | Loss: 0.00001871
Iteration 161/1000 | Loss: 0.00001870
Iteration 162/1000 | Loss: 0.00001870
Iteration 163/1000 | Loss: 0.00001870
Iteration 164/1000 | Loss: 0.00001870
Iteration 165/1000 | Loss: 0.00001870
Iteration 166/1000 | Loss: 0.00001870
Iteration 167/1000 | Loss: 0.00001870
Iteration 168/1000 | Loss: 0.00001870
Iteration 169/1000 | Loss: 0.00001870
Iteration 170/1000 | Loss: 0.00001870
Iteration 171/1000 | Loss: 0.00001870
Iteration 172/1000 | Loss: 0.00001870
Iteration 173/1000 | Loss: 0.00001870
Iteration 174/1000 | Loss: 0.00001870
Iteration 175/1000 | Loss: 0.00001870
Iteration 176/1000 | Loss: 0.00001869
Iteration 177/1000 | Loss: 0.00001869
Iteration 178/1000 | Loss: 0.00001869
Iteration 179/1000 | Loss: 0.00001869
Iteration 180/1000 | Loss: 0.00001869
Iteration 181/1000 | Loss: 0.00001869
Iteration 182/1000 | Loss: 0.00001869
Iteration 183/1000 | Loss: 0.00001869
Iteration 184/1000 | Loss: 0.00001869
Iteration 185/1000 | Loss: 0.00001869
Iteration 186/1000 | Loss: 0.00001869
Iteration 187/1000 | Loss: 0.00001869
Iteration 188/1000 | Loss: 0.00001869
Iteration 189/1000 | Loss: 0.00001869
Iteration 190/1000 | Loss: 0.00001869
Iteration 191/1000 | Loss: 0.00001869
Iteration 192/1000 | Loss: 0.00001869
Iteration 193/1000 | Loss: 0.00001869
Iteration 194/1000 | Loss: 0.00001869
Iteration 195/1000 | Loss: 0.00001869
Iteration 196/1000 | Loss: 0.00001869
Iteration 197/1000 | Loss: 0.00001869
Iteration 198/1000 | Loss: 0.00001869
Iteration 199/1000 | Loss: 0.00001868
Iteration 200/1000 | Loss: 0.00001868
Iteration 201/1000 | Loss: 0.00001868
Iteration 202/1000 | Loss: 0.00001868
Iteration 203/1000 | Loss: 0.00001868
Iteration 204/1000 | Loss: 0.00001868
Iteration 205/1000 | Loss: 0.00001868
Iteration 206/1000 | Loss: 0.00001868
Iteration 207/1000 | Loss: 0.00001868
Iteration 208/1000 | Loss: 0.00001868
Iteration 209/1000 | Loss: 0.00001868
Iteration 210/1000 | Loss: 0.00001868
Iteration 211/1000 | Loss: 0.00001868
Iteration 212/1000 | Loss: 0.00001868
Iteration 213/1000 | Loss: 0.00001868
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [1.8681757865124382e-05, 1.8681757865124382e-05, 1.8681757865124382e-05, 1.8681757865124382e-05, 1.8681757865124382e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8681757865124382e-05

Optimization complete. Final v2v error: 3.5530409812927246 mm

Highest mean error: 3.833785057067871 mm for frame 0

Lowest mean error: 3.424351453781128 mm for frame 45

Saving results

Total time: 49.37874960899353
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00442739
Iteration 2/25 | Loss: 0.00075477
Iteration 3/25 | Loss: 0.00063771
Iteration 4/25 | Loss: 0.00061031
Iteration 5/25 | Loss: 0.00060233
Iteration 6/25 | Loss: 0.00060043
Iteration 7/25 | Loss: 0.00059985
Iteration 8/25 | Loss: 0.00059985
Iteration 9/25 | Loss: 0.00059985
Iteration 10/25 | Loss: 0.00059985
Iteration 11/25 | Loss: 0.00059985
Iteration 12/25 | Loss: 0.00059985
Iteration 13/25 | Loss: 0.00059985
Iteration 14/25 | Loss: 0.00059985
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0005998458946123719, 0.0005998458946123719, 0.0005998458946123719, 0.0005998458946123719, 0.0005998458946123719]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005998458946123719

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59489393
Iteration 2/25 | Loss: 0.00024363
Iteration 3/25 | Loss: 0.00024363
Iteration 4/25 | Loss: 0.00024363
Iteration 5/25 | Loss: 0.00024363
Iteration 6/25 | Loss: 0.00024362
Iteration 7/25 | Loss: 0.00024362
Iteration 8/25 | Loss: 0.00024362
Iteration 9/25 | Loss: 0.00024362
Iteration 10/25 | Loss: 0.00024362
Iteration 11/25 | Loss: 0.00024362
Iteration 12/25 | Loss: 0.00024362
Iteration 13/25 | Loss: 0.00024362
Iteration 14/25 | Loss: 0.00024362
Iteration 15/25 | Loss: 0.00024362
Iteration 16/25 | Loss: 0.00024362
Iteration 17/25 | Loss: 0.00024362
Iteration 18/25 | Loss: 0.00024362
Iteration 19/25 | Loss: 0.00024362
Iteration 20/25 | Loss: 0.00024362
Iteration 21/25 | Loss: 0.00024362
Iteration 22/25 | Loss: 0.00024362
Iteration 23/25 | Loss: 0.00024362
Iteration 24/25 | Loss: 0.00024362
Iteration 25/25 | Loss: 0.00024362

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024362
Iteration 2/1000 | Loss: 0.00002778
Iteration 3/1000 | Loss: 0.00001749
Iteration 4/1000 | Loss: 0.00001611
Iteration 5/1000 | Loss: 0.00001517
Iteration 6/1000 | Loss: 0.00001475
Iteration 7/1000 | Loss: 0.00001453
Iteration 8/1000 | Loss: 0.00001424
Iteration 9/1000 | Loss: 0.00001415
Iteration 10/1000 | Loss: 0.00001409
Iteration 11/1000 | Loss: 0.00001406
Iteration 12/1000 | Loss: 0.00001398
Iteration 13/1000 | Loss: 0.00001395
Iteration 14/1000 | Loss: 0.00001395
Iteration 15/1000 | Loss: 0.00001392
Iteration 16/1000 | Loss: 0.00001391
Iteration 17/1000 | Loss: 0.00001388
Iteration 18/1000 | Loss: 0.00001387
Iteration 19/1000 | Loss: 0.00001387
Iteration 20/1000 | Loss: 0.00001386
Iteration 21/1000 | Loss: 0.00001381
Iteration 22/1000 | Loss: 0.00001381
Iteration 23/1000 | Loss: 0.00001381
Iteration 24/1000 | Loss: 0.00001381
Iteration 25/1000 | Loss: 0.00001381
Iteration 26/1000 | Loss: 0.00001381
Iteration 27/1000 | Loss: 0.00001381
Iteration 28/1000 | Loss: 0.00001381
Iteration 29/1000 | Loss: 0.00001380
Iteration 30/1000 | Loss: 0.00001380
Iteration 31/1000 | Loss: 0.00001375
Iteration 32/1000 | Loss: 0.00001371
Iteration 33/1000 | Loss: 0.00001370
Iteration 34/1000 | Loss: 0.00001370
Iteration 35/1000 | Loss: 0.00001369
Iteration 36/1000 | Loss: 0.00001369
Iteration 37/1000 | Loss: 0.00001368
Iteration 38/1000 | Loss: 0.00001368
Iteration 39/1000 | Loss: 0.00001368
Iteration 40/1000 | Loss: 0.00001367
Iteration 41/1000 | Loss: 0.00001367
Iteration 42/1000 | Loss: 0.00001366
Iteration 43/1000 | Loss: 0.00001366
Iteration 44/1000 | Loss: 0.00001363
Iteration 45/1000 | Loss: 0.00001362
Iteration 46/1000 | Loss: 0.00001362
Iteration 47/1000 | Loss: 0.00001362
Iteration 48/1000 | Loss: 0.00001361
Iteration 49/1000 | Loss: 0.00001360
Iteration 50/1000 | Loss: 0.00001360
Iteration 51/1000 | Loss: 0.00001359
Iteration 52/1000 | Loss: 0.00001359
Iteration 53/1000 | Loss: 0.00001359
Iteration 54/1000 | Loss: 0.00001359
Iteration 55/1000 | Loss: 0.00001359
Iteration 56/1000 | Loss: 0.00001359
Iteration 57/1000 | Loss: 0.00001359
Iteration 58/1000 | Loss: 0.00001359
Iteration 59/1000 | Loss: 0.00001359
Iteration 60/1000 | Loss: 0.00001359
Iteration 61/1000 | Loss: 0.00001358
Iteration 62/1000 | Loss: 0.00001358
Iteration 63/1000 | Loss: 0.00001358
Iteration 64/1000 | Loss: 0.00001358
Iteration 65/1000 | Loss: 0.00001358
Iteration 66/1000 | Loss: 0.00001357
Iteration 67/1000 | Loss: 0.00001357
Iteration 68/1000 | Loss: 0.00001357
Iteration 69/1000 | Loss: 0.00001357
Iteration 70/1000 | Loss: 0.00001356
Iteration 71/1000 | Loss: 0.00001356
Iteration 72/1000 | Loss: 0.00001356
Iteration 73/1000 | Loss: 0.00001355
Iteration 74/1000 | Loss: 0.00001355
Iteration 75/1000 | Loss: 0.00001355
Iteration 76/1000 | Loss: 0.00001355
Iteration 77/1000 | Loss: 0.00001354
Iteration 78/1000 | Loss: 0.00001354
Iteration 79/1000 | Loss: 0.00001354
Iteration 80/1000 | Loss: 0.00001353
Iteration 81/1000 | Loss: 0.00001352
Iteration 82/1000 | Loss: 0.00001349
Iteration 83/1000 | Loss: 0.00001349
Iteration 84/1000 | Loss: 0.00001349
Iteration 85/1000 | Loss: 0.00001349
Iteration 86/1000 | Loss: 0.00001349
Iteration 87/1000 | Loss: 0.00001349
Iteration 88/1000 | Loss: 0.00001349
Iteration 89/1000 | Loss: 0.00001348
Iteration 90/1000 | Loss: 0.00001348
Iteration 91/1000 | Loss: 0.00001348
Iteration 92/1000 | Loss: 0.00001348
Iteration 93/1000 | Loss: 0.00001346
Iteration 94/1000 | Loss: 0.00001346
Iteration 95/1000 | Loss: 0.00001346
Iteration 96/1000 | Loss: 0.00001346
Iteration 97/1000 | Loss: 0.00001346
Iteration 98/1000 | Loss: 0.00001346
Iteration 99/1000 | Loss: 0.00001346
Iteration 100/1000 | Loss: 0.00001346
Iteration 101/1000 | Loss: 0.00001346
Iteration 102/1000 | Loss: 0.00001346
Iteration 103/1000 | Loss: 0.00001346
Iteration 104/1000 | Loss: 0.00001346
Iteration 105/1000 | Loss: 0.00001346
Iteration 106/1000 | Loss: 0.00001345
Iteration 107/1000 | Loss: 0.00001345
Iteration 108/1000 | Loss: 0.00001345
Iteration 109/1000 | Loss: 0.00001344
Iteration 110/1000 | Loss: 0.00001344
Iteration 111/1000 | Loss: 0.00001344
Iteration 112/1000 | Loss: 0.00001344
Iteration 113/1000 | Loss: 0.00001343
Iteration 114/1000 | Loss: 0.00001343
Iteration 115/1000 | Loss: 0.00001343
Iteration 116/1000 | Loss: 0.00001343
Iteration 117/1000 | Loss: 0.00001343
Iteration 118/1000 | Loss: 0.00001343
Iteration 119/1000 | Loss: 0.00001343
Iteration 120/1000 | Loss: 0.00001343
Iteration 121/1000 | Loss: 0.00001343
Iteration 122/1000 | Loss: 0.00001343
Iteration 123/1000 | Loss: 0.00001343
Iteration 124/1000 | Loss: 0.00001343
Iteration 125/1000 | Loss: 0.00001343
Iteration 126/1000 | Loss: 0.00001343
Iteration 127/1000 | Loss: 0.00001343
Iteration 128/1000 | Loss: 0.00001343
Iteration 129/1000 | Loss: 0.00001343
Iteration 130/1000 | Loss: 0.00001343
Iteration 131/1000 | Loss: 0.00001343
Iteration 132/1000 | Loss: 0.00001343
Iteration 133/1000 | Loss: 0.00001343
Iteration 134/1000 | Loss: 0.00001343
Iteration 135/1000 | Loss: 0.00001343
Iteration 136/1000 | Loss: 0.00001343
Iteration 137/1000 | Loss: 0.00001343
Iteration 138/1000 | Loss: 0.00001343
Iteration 139/1000 | Loss: 0.00001343
Iteration 140/1000 | Loss: 0.00001343
Iteration 141/1000 | Loss: 0.00001343
Iteration 142/1000 | Loss: 0.00001343
Iteration 143/1000 | Loss: 0.00001343
Iteration 144/1000 | Loss: 0.00001343
Iteration 145/1000 | Loss: 0.00001343
Iteration 146/1000 | Loss: 0.00001343
Iteration 147/1000 | Loss: 0.00001343
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.3429962564259768e-05, 1.3429962564259768e-05, 1.3429962564259768e-05, 1.3429962564259768e-05, 1.3429962564259768e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3429962564259768e-05

Optimization complete. Final v2v error: 3.1261684894561768 mm

Highest mean error: 3.31341552734375 mm for frame 29

Lowest mean error: 2.9057729244232178 mm for frame 16

Saving results

Total time: 32.91042971611023
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00860972
Iteration 2/25 | Loss: 0.00077733
Iteration 3/25 | Loss: 0.00062263
Iteration 4/25 | Loss: 0.00060495
Iteration 5/25 | Loss: 0.00059936
Iteration 6/25 | Loss: 0.00059790
Iteration 7/25 | Loss: 0.00059777
Iteration 8/25 | Loss: 0.00059777
Iteration 9/25 | Loss: 0.00059777
Iteration 10/25 | Loss: 0.00059777
Iteration 11/25 | Loss: 0.00059777
Iteration 12/25 | Loss: 0.00059777
Iteration 13/25 | Loss: 0.00059777
Iteration 14/25 | Loss: 0.00059777
Iteration 15/25 | Loss: 0.00059777
Iteration 16/25 | Loss: 0.00059777
Iteration 17/25 | Loss: 0.00059777
Iteration 18/25 | Loss: 0.00059777
Iteration 19/25 | Loss: 0.00059777
Iteration 20/25 | Loss: 0.00059777
Iteration 21/25 | Loss: 0.00059777
Iteration 22/25 | Loss: 0.00059777
Iteration 23/25 | Loss: 0.00059777
Iteration 24/25 | Loss: 0.00059777
Iteration 25/25 | Loss: 0.00059777

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46058571
Iteration 2/25 | Loss: 0.00024657
Iteration 3/25 | Loss: 0.00024655
Iteration 4/25 | Loss: 0.00024655
Iteration 5/25 | Loss: 0.00024655
Iteration 6/25 | Loss: 0.00024655
Iteration 7/25 | Loss: 0.00024655
Iteration 8/25 | Loss: 0.00024655
Iteration 9/25 | Loss: 0.00024655
Iteration 10/25 | Loss: 0.00024655
Iteration 11/25 | Loss: 0.00024655
Iteration 12/25 | Loss: 0.00024655
Iteration 13/25 | Loss: 0.00024655
Iteration 14/25 | Loss: 0.00024655
Iteration 15/25 | Loss: 0.00024655
Iteration 16/25 | Loss: 0.00024655
Iteration 17/25 | Loss: 0.00024655
Iteration 18/25 | Loss: 0.00024655
Iteration 19/25 | Loss: 0.00024655
Iteration 20/25 | Loss: 0.00024655
Iteration 21/25 | Loss: 0.00024655
Iteration 22/25 | Loss: 0.00024655
Iteration 23/25 | Loss: 0.00024655
Iteration 24/25 | Loss: 0.00024655
Iteration 25/25 | Loss: 0.00024655

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024655
Iteration 2/1000 | Loss: 0.00002032
Iteration 3/1000 | Loss: 0.00001509
Iteration 4/1000 | Loss: 0.00001367
Iteration 5/1000 | Loss: 0.00001279
Iteration 6/1000 | Loss: 0.00001237
Iteration 7/1000 | Loss: 0.00001197
Iteration 8/1000 | Loss: 0.00001186
Iteration 9/1000 | Loss: 0.00001185
Iteration 10/1000 | Loss: 0.00001174
Iteration 11/1000 | Loss: 0.00001173
Iteration 12/1000 | Loss: 0.00001166
Iteration 13/1000 | Loss: 0.00001166
Iteration 14/1000 | Loss: 0.00001165
Iteration 15/1000 | Loss: 0.00001164
Iteration 16/1000 | Loss: 0.00001162
Iteration 17/1000 | Loss: 0.00001162
Iteration 18/1000 | Loss: 0.00001160
Iteration 19/1000 | Loss: 0.00001160
Iteration 20/1000 | Loss: 0.00001159
Iteration 21/1000 | Loss: 0.00001159
Iteration 22/1000 | Loss: 0.00001156
Iteration 23/1000 | Loss: 0.00001151
Iteration 24/1000 | Loss: 0.00001151
Iteration 25/1000 | Loss: 0.00001151
Iteration 26/1000 | Loss: 0.00001151
Iteration 27/1000 | Loss: 0.00001151
Iteration 28/1000 | Loss: 0.00001151
Iteration 29/1000 | Loss: 0.00001151
Iteration 30/1000 | Loss: 0.00001150
Iteration 31/1000 | Loss: 0.00001150
Iteration 32/1000 | Loss: 0.00001150
Iteration 33/1000 | Loss: 0.00001150
Iteration 34/1000 | Loss: 0.00001150
Iteration 35/1000 | Loss: 0.00001150
Iteration 36/1000 | Loss: 0.00001150
Iteration 37/1000 | Loss: 0.00001150
Iteration 38/1000 | Loss: 0.00001150
Iteration 39/1000 | Loss: 0.00001150
Iteration 40/1000 | Loss: 0.00001149
Iteration 41/1000 | Loss: 0.00001149
Iteration 42/1000 | Loss: 0.00001149
Iteration 43/1000 | Loss: 0.00001148
Iteration 44/1000 | Loss: 0.00001148
Iteration 45/1000 | Loss: 0.00001147
Iteration 46/1000 | Loss: 0.00001147
Iteration 47/1000 | Loss: 0.00001147
Iteration 48/1000 | Loss: 0.00001147
Iteration 49/1000 | Loss: 0.00001147
Iteration 50/1000 | Loss: 0.00001147
Iteration 51/1000 | Loss: 0.00001147
Iteration 52/1000 | Loss: 0.00001146
Iteration 53/1000 | Loss: 0.00001146
Iteration 54/1000 | Loss: 0.00001146
Iteration 55/1000 | Loss: 0.00001146
Iteration 56/1000 | Loss: 0.00001146
Iteration 57/1000 | Loss: 0.00001146
Iteration 58/1000 | Loss: 0.00001146
Iteration 59/1000 | Loss: 0.00001146
Iteration 60/1000 | Loss: 0.00001146
Iteration 61/1000 | Loss: 0.00001146
Iteration 62/1000 | Loss: 0.00001146
Iteration 63/1000 | Loss: 0.00001146
Iteration 64/1000 | Loss: 0.00001146
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 64. Stopping optimization.
Last 5 losses: [1.1455124877102207e-05, 1.1455124877102207e-05, 1.1455124877102207e-05, 1.1455124877102207e-05, 1.1455124877102207e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1455124877102207e-05

Optimization complete. Final v2v error: 2.889338493347168 mm

Highest mean error: 3.0977487564086914 mm for frame 3

Lowest mean error: 2.7640581130981445 mm for frame 140

Saving results

Total time: 27.275346994400024
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00858989
Iteration 2/25 | Loss: 0.00073309
Iteration 3/25 | Loss: 0.00061231
Iteration 4/25 | Loss: 0.00059266
Iteration 5/25 | Loss: 0.00058735
Iteration 6/25 | Loss: 0.00058619
Iteration 7/25 | Loss: 0.00058619
Iteration 8/25 | Loss: 0.00058619
Iteration 9/25 | Loss: 0.00058619
Iteration 10/25 | Loss: 0.00058619
Iteration 11/25 | Loss: 0.00058619
Iteration 12/25 | Loss: 0.00058619
Iteration 13/25 | Loss: 0.00058619
Iteration 14/25 | Loss: 0.00058619
Iteration 15/25 | Loss: 0.00058619
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0005861861282028258, 0.0005861861282028258, 0.0005861861282028258, 0.0005861861282028258, 0.0005861861282028258]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005861861282028258

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.59100389
Iteration 2/25 | Loss: 0.00026804
Iteration 3/25 | Loss: 0.00026803
Iteration 4/25 | Loss: 0.00026803
Iteration 5/25 | Loss: 0.00026803
Iteration 6/25 | Loss: 0.00026803
Iteration 7/25 | Loss: 0.00026803
Iteration 8/25 | Loss: 0.00026803
Iteration 9/25 | Loss: 0.00026803
Iteration 10/25 | Loss: 0.00026803
Iteration 11/25 | Loss: 0.00026803
Iteration 12/25 | Loss: 0.00026803
Iteration 13/25 | Loss: 0.00026803
Iteration 14/25 | Loss: 0.00026803
Iteration 15/25 | Loss: 0.00026803
Iteration 16/25 | Loss: 0.00026803
Iteration 17/25 | Loss: 0.00026803
Iteration 18/25 | Loss: 0.00026803
Iteration 19/25 | Loss: 0.00026803
Iteration 20/25 | Loss: 0.00026803
Iteration 21/25 | Loss: 0.00026803
Iteration 22/25 | Loss: 0.00026803
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0002680308243725449, 0.0002680308243725449, 0.0002680308243725449, 0.0002680308243725449, 0.0002680308243725449]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002680308243725449

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00026803
Iteration 2/1000 | Loss: 0.00001752
Iteration 3/1000 | Loss: 0.00001391
Iteration 4/1000 | Loss: 0.00001316
Iteration 5/1000 | Loss: 0.00001237
Iteration 6/1000 | Loss: 0.00001197
Iteration 7/1000 | Loss: 0.00001176
Iteration 8/1000 | Loss: 0.00001164
Iteration 9/1000 | Loss: 0.00001159
Iteration 10/1000 | Loss: 0.00001156
Iteration 11/1000 | Loss: 0.00001155
Iteration 12/1000 | Loss: 0.00001154
Iteration 13/1000 | Loss: 0.00001154
Iteration 14/1000 | Loss: 0.00001154
Iteration 15/1000 | Loss: 0.00001154
Iteration 16/1000 | Loss: 0.00001153
Iteration 17/1000 | Loss: 0.00001153
Iteration 18/1000 | Loss: 0.00001152
Iteration 19/1000 | Loss: 0.00001151
Iteration 20/1000 | Loss: 0.00001151
Iteration 21/1000 | Loss: 0.00001151
Iteration 22/1000 | Loss: 0.00001150
Iteration 23/1000 | Loss: 0.00001150
Iteration 24/1000 | Loss: 0.00001150
Iteration 25/1000 | Loss: 0.00001150
Iteration 26/1000 | Loss: 0.00001149
Iteration 27/1000 | Loss: 0.00001149
Iteration 28/1000 | Loss: 0.00001149
Iteration 29/1000 | Loss: 0.00001149
Iteration 30/1000 | Loss: 0.00001148
Iteration 31/1000 | Loss: 0.00001146
Iteration 32/1000 | Loss: 0.00001146
Iteration 33/1000 | Loss: 0.00001145
Iteration 34/1000 | Loss: 0.00001145
Iteration 35/1000 | Loss: 0.00001145
Iteration 36/1000 | Loss: 0.00001145
Iteration 37/1000 | Loss: 0.00001145
Iteration 38/1000 | Loss: 0.00001145
Iteration 39/1000 | Loss: 0.00001143
Iteration 40/1000 | Loss: 0.00001142
Iteration 41/1000 | Loss: 0.00001142
Iteration 42/1000 | Loss: 0.00001142
Iteration 43/1000 | Loss: 0.00001142
Iteration 44/1000 | Loss: 0.00001142
Iteration 45/1000 | Loss: 0.00001142
Iteration 46/1000 | Loss: 0.00001142
Iteration 47/1000 | Loss: 0.00001142
Iteration 48/1000 | Loss: 0.00001142
Iteration 49/1000 | Loss: 0.00001141
Iteration 50/1000 | Loss: 0.00001141
Iteration 51/1000 | Loss: 0.00001141
Iteration 52/1000 | Loss: 0.00001141
Iteration 53/1000 | Loss: 0.00001141
Iteration 54/1000 | Loss: 0.00001141
Iteration 55/1000 | Loss: 0.00001140
Iteration 56/1000 | Loss: 0.00001140
Iteration 57/1000 | Loss: 0.00001139
Iteration 58/1000 | Loss: 0.00001139
Iteration 59/1000 | Loss: 0.00001139
Iteration 60/1000 | Loss: 0.00001139
Iteration 61/1000 | Loss: 0.00001139
Iteration 62/1000 | Loss: 0.00001139
Iteration 63/1000 | Loss: 0.00001139
Iteration 64/1000 | Loss: 0.00001139
Iteration 65/1000 | Loss: 0.00001139
Iteration 66/1000 | Loss: 0.00001139
Iteration 67/1000 | Loss: 0.00001139
Iteration 68/1000 | Loss: 0.00001139
Iteration 69/1000 | Loss: 0.00001139
Iteration 70/1000 | Loss: 0.00001139
Iteration 71/1000 | Loss: 0.00001139
Iteration 72/1000 | Loss: 0.00001139
Iteration 73/1000 | Loss: 0.00001139
Iteration 74/1000 | Loss: 0.00001139
Iteration 75/1000 | Loss: 0.00001139
Iteration 76/1000 | Loss: 0.00001139
Iteration 77/1000 | Loss: 0.00001139
Iteration 78/1000 | Loss: 0.00001139
Iteration 79/1000 | Loss: 0.00001139
Iteration 80/1000 | Loss: 0.00001139
Iteration 81/1000 | Loss: 0.00001139
Iteration 82/1000 | Loss: 0.00001139
Iteration 83/1000 | Loss: 0.00001139
Iteration 84/1000 | Loss: 0.00001139
Iteration 85/1000 | Loss: 0.00001139
Iteration 86/1000 | Loss: 0.00001139
Iteration 87/1000 | Loss: 0.00001139
Iteration 88/1000 | Loss: 0.00001139
Iteration 89/1000 | Loss: 0.00001139
Iteration 90/1000 | Loss: 0.00001139
Iteration 91/1000 | Loss: 0.00001139
Iteration 92/1000 | Loss: 0.00001139
Iteration 93/1000 | Loss: 0.00001139
Iteration 94/1000 | Loss: 0.00001139
Iteration 95/1000 | Loss: 0.00001139
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 95. Stopping optimization.
Last 5 losses: [1.1385394827811979e-05, 1.1385394827811979e-05, 1.1385394827811979e-05, 1.1385394827811979e-05, 1.1385394827811979e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1385394827811979e-05

Optimization complete. Final v2v error: 2.8710246086120605 mm

Highest mean error: 3.325488567352295 mm for frame 195

Lowest mean error: 2.7258682250976562 mm for frame 19

Saving results

Total time: 29.66481876373291
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00408889
Iteration 2/25 | Loss: 0.00088317
Iteration 3/25 | Loss: 0.00074562
Iteration 4/25 | Loss: 0.00071751
Iteration 5/25 | Loss: 0.00070813
Iteration 6/25 | Loss: 0.00070682
Iteration 7/25 | Loss: 0.00070634
Iteration 8/25 | Loss: 0.00070633
Iteration 9/25 | Loss: 0.00070633
Iteration 10/25 | Loss: 0.00070633
Iteration 11/25 | Loss: 0.00070633
Iteration 12/25 | Loss: 0.00070633
Iteration 13/25 | Loss: 0.00070633
Iteration 14/25 | Loss: 0.00070633
Iteration 15/25 | Loss: 0.00070633
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0007063253433443606, 0.0007063253433443606, 0.0007063253433443606, 0.0007063253433443606, 0.0007063253433443606]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007063253433443606

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45407343
Iteration 2/25 | Loss: 0.00030825
Iteration 3/25 | Loss: 0.00030824
Iteration 4/25 | Loss: 0.00030824
Iteration 5/25 | Loss: 0.00030824
Iteration 6/25 | Loss: 0.00030824
Iteration 7/25 | Loss: 0.00030824
Iteration 8/25 | Loss: 0.00030824
Iteration 9/25 | Loss: 0.00030824
Iteration 10/25 | Loss: 0.00030824
Iteration 11/25 | Loss: 0.00030824
Iteration 12/25 | Loss: 0.00030824
Iteration 13/25 | Loss: 0.00030824
Iteration 14/25 | Loss: 0.00030824
Iteration 15/25 | Loss: 0.00030824
Iteration 16/25 | Loss: 0.00030824
Iteration 17/25 | Loss: 0.00030824
Iteration 18/25 | Loss: 0.00030824
Iteration 19/25 | Loss: 0.00030824
Iteration 20/25 | Loss: 0.00030824
Iteration 21/25 | Loss: 0.00030824
Iteration 22/25 | Loss: 0.00030824
Iteration 23/25 | Loss: 0.00030824
Iteration 24/25 | Loss: 0.00030824
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.00030824245186522603, 0.00030824245186522603, 0.00030824245186522603, 0.00030824245186522603, 0.00030824245186522603]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00030824245186522603

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00030824
Iteration 2/1000 | Loss: 0.00005143
Iteration 3/1000 | Loss: 0.00003714
Iteration 4/1000 | Loss: 0.00003194
Iteration 5/1000 | Loss: 0.00003029
Iteration 6/1000 | Loss: 0.00002881
Iteration 7/1000 | Loss: 0.00002791
Iteration 8/1000 | Loss: 0.00002728
Iteration 9/1000 | Loss: 0.00002683
Iteration 10/1000 | Loss: 0.00002665
Iteration 11/1000 | Loss: 0.00002654
Iteration 12/1000 | Loss: 0.00002635
Iteration 13/1000 | Loss: 0.00002634
Iteration 14/1000 | Loss: 0.00002619
Iteration 15/1000 | Loss: 0.00002607
Iteration 16/1000 | Loss: 0.00002603
Iteration 17/1000 | Loss: 0.00002603
Iteration 18/1000 | Loss: 0.00002602
Iteration 19/1000 | Loss: 0.00002601
Iteration 20/1000 | Loss: 0.00002601
Iteration 21/1000 | Loss: 0.00002600
Iteration 22/1000 | Loss: 0.00002600
Iteration 23/1000 | Loss: 0.00002597
Iteration 24/1000 | Loss: 0.00002597
Iteration 25/1000 | Loss: 0.00002597
Iteration 26/1000 | Loss: 0.00002594
Iteration 27/1000 | Loss: 0.00002591
Iteration 28/1000 | Loss: 0.00002590
Iteration 29/1000 | Loss: 0.00002588
Iteration 30/1000 | Loss: 0.00002588
Iteration 31/1000 | Loss: 0.00002587
Iteration 32/1000 | Loss: 0.00002582
Iteration 33/1000 | Loss: 0.00002582
Iteration 34/1000 | Loss: 0.00002581
Iteration 35/1000 | Loss: 0.00002581
Iteration 36/1000 | Loss: 0.00002580
Iteration 37/1000 | Loss: 0.00002580
Iteration 38/1000 | Loss: 0.00002580
Iteration 39/1000 | Loss: 0.00002579
Iteration 40/1000 | Loss: 0.00002579
Iteration 41/1000 | Loss: 0.00002579
Iteration 42/1000 | Loss: 0.00002579
Iteration 43/1000 | Loss: 0.00002578
Iteration 44/1000 | Loss: 0.00002578
Iteration 45/1000 | Loss: 0.00002578
Iteration 46/1000 | Loss: 0.00002578
Iteration 47/1000 | Loss: 0.00002578
Iteration 48/1000 | Loss: 0.00002578
Iteration 49/1000 | Loss: 0.00002578
Iteration 50/1000 | Loss: 0.00002578
Iteration 51/1000 | Loss: 0.00002578
Iteration 52/1000 | Loss: 0.00002578
Iteration 53/1000 | Loss: 0.00002577
Iteration 54/1000 | Loss: 0.00002577
Iteration 55/1000 | Loss: 0.00002577
Iteration 56/1000 | Loss: 0.00002577
Iteration 57/1000 | Loss: 0.00002576
Iteration 58/1000 | Loss: 0.00002576
Iteration 59/1000 | Loss: 0.00002576
Iteration 60/1000 | Loss: 0.00002576
Iteration 61/1000 | Loss: 0.00002576
Iteration 62/1000 | Loss: 0.00002576
Iteration 63/1000 | Loss: 0.00002576
Iteration 64/1000 | Loss: 0.00002576
Iteration 65/1000 | Loss: 0.00002576
Iteration 66/1000 | Loss: 0.00002576
Iteration 67/1000 | Loss: 0.00002576
Iteration 68/1000 | Loss: 0.00002576
Iteration 69/1000 | Loss: 0.00002575
Iteration 70/1000 | Loss: 0.00002575
Iteration 71/1000 | Loss: 0.00002575
Iteration 72/1000 | Loss: 0.00002575
Iteration 73/1000 | Loss: 0.00002575
Iteration 74/1000 | Loss: 0.00002575
Iteration 75/1000 | Loss: 0.00002575
Iteration 76/1000 | Loss: 0.00002575
Iteration 77/1000 | Loss: 0.00002575
Iteration 78/1000 | Loss: 0.00002575
Iteration 79/1000 | Loss: 0.00002574
Iteration 80/1000 | Loss: 0.00002574
Iteration 81/1000 | Loss: 0.00002574
Iteration 82/1000 | Loss: 0.00002574
Iteration 83/1000 | Loss: 0.00002574
Iteration 84/1000 | Loss: 0.00002574
Iteration 85/1000 | Loss: 0.00002574
Iteration 86/1000 | Loss: 0.00002574
Iteration 87/1000 | Loss: 0.00002574
Iteration 88/1000 | Loss: 0.00002574
Iteration 89/1000 | Loss: 0.00002574
Iteration 90/1000 | Loss: 0.00002574
Iteration 91/1000 | Loss: 0.00002574
Iteration 92/1000 | Loss: 0.00002574
Iteration 93/1000 | Loss: 0.00002574
Iteration 94/1000 | Loss: 0.00002574
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [2.573920392023865e-05, 2.573920392023865e-05, 2.573920392023865e-05, 2.573920392023865e-05, 2.573920392023865e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.573920392023865e-05

Optimization complete. Final v2v error: 4.20355224609375 mm

Highest mean error: 4.601958751678467 mm for frame 15

Lowest mean error: 3.8059701919555664 mm for frame 33

Saving results

Total time: 35.75041913986206
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00904296
Iteration 2/25 | Loss: 0.00151937
Iteration 3/25 | Loss: 0.00077543
Iteration 4/25 | Loss: 0.00071839
Iteration 5/25 | Loss: 0.00066582
Iteration 6/25 | Loss: 0.00064977
Iteration 7/25 | Loss: 0.00064902
Iteration 8/25 | Loss: 0.00064720
Iteration 9/25 | Loss: 0.00064086
Iteration 10/25 | Loss: 0.00063909
Iteration 11/25 | Loss: 0.00063826
Iteration 12/25 | Loss: 0.00063754
Iteration 13/25 | Loss: 0.00063651
Iteration 14/25 | Loss: 0.00063618
Iteration 15/25 | Loss: 0.00063607
Iteration 16/25 | Loss: 0.00063607
Iteration 17/25 | Loss: 0.00063606
Iteration 18/25 | Loss: 0.00063606
Iteration 19/25 | Loss: 0.00063606
Iteration 20/25 | Loss: 0.00063606
Iteration 21/25 | Loss: 0.00063606
Iteration 22/25 | Loss: 0.00063606
Iteration 23/25 | Loss: 0.00063606
Iteration 24/25 | Loss: 0.00063605
Iteration 25/25 | Loss: 0.00063605

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.27669096
Iteration 2/25 | Loss: 0.00032271
Iteration 3/25 | Loss: 0.00032271
Iteration 4/25 | Loss: 0.00032271
Iteration 5/25 | Loss: 0.00032271
Iteration 6/25 | Loss: 0.00032270
Iteration 7/25 | Loss: 0.00032270
Iteration 8/25 | Loss: 0.00032270
Iteration 9/25 | Loss: 0.00032270
Iteration 10/25 | Loss: 0.00032270
Iteration 11/25 | Loss: 0.00032270
Iteration 12/25 | Loss: 0.00032270
Iteration 13/25 | Loss: 0.00032270
Iteration 14/25 | Loss: 0.00032270
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.00032270376686938107, 0.00032270376686938107, 0.00032270376686938107, 0.00032270376686938107, 0.00032270376686938107]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00032270376686938107

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032270
Iteration 2/1000 | Loss: 0.00005773
Iteration 3/1000 | Loss: 0.00002002
Iteration 4/1000 | Loss: 0.00001756
Iteration 5/1000 | Loss: 0.00001660
Iteration 6/1000 | Loss: 0.00001609
Iteration 7/1000 | Loss: 0.00001569
Iteration 8/1000 | Loss: 0.00005452
Iteration 9/1000 | Loss: 0.00001559
Iteration 10/1000 | Loss: 0.00001522
Iteration 11/1000 | Loss: 0.00001512
Iteration 12/1000 | Loss: 0.00001511
Iteration 13/1000 | Loss: 0.00001510
Iteration 14/1000 | Loss: 0.00001510
Iteration 15/1000 | Loss: 0.00001509
Iteration 16/1000 | Loss: 0.00001509
Iteration 17/1000 | Loss: 0.00001509
Iteration 18/1000 | Loss: 0.00001509
Iteration 19/1000 | Loss: 0.00001507
Iteration 20/1000 | Loss: 0.00001507
Iteration 21/1000 | Loss: 0.00001507
Iteration 22/1000 | Loss: 0.00001506
Iteration 23/1000 | Loss: 0.00001505
Iteration 24/1000 | Loss: 0.00001504
Iteration 25/1000 | Loss: 0.00006696
Iteration 26/1000 | Loss: 0.00002343
Iteration 27/1000 | Loss: 0.00002410
Iteration 28/1000 | Loss: 0.00001501
Iteration 29/1000 | Loss: 0.00001497
Iteration 30/1000 | Loss: 0.00001495
Iteration 31/1000 | Loss: 0.00001494
Iteration 32/1000 | Loss: 0.00001493
Iteration 33/1000 | Loss: 0.00001493
Iteration 34/1000 | Loss: 0.00001493
Iteration 35/1000 | Loss: 0.00001492
Iteration 36/1000 | Loss: 0.00001491
Iteration 37/1000 | Loss: 0.00001491
Iteration 38/1000 | Loss: 0.00001490
Iteration 39/1000 | Loss: 0.00001487
Iteration 40/1000 | Loss: 0.00001487
Iteration 41/1000 | Loss: 0.00001486
Iteration 42/1000 | Loss: 0.00001485
Iteration 43/1000 | Loss: 0.00001485
Iteration 44/1000 | Loss: 0.00001484
Iteration 45/1000 | Loss: 0.00001483
Iteration 46/1000 | Loss: 0.00001483
Iteration 47/1000 | Loss: 0.00001482
Iteration 48/1000 | Loss: 0.00001482
Iteration 49/1000 | Loss: 0.00001481
Iteration 50/1000 | Loss: 0.00001481
Iteration 51/1000 | Loss: 0.00001480
Iteration 52/1000 | Loss: 0.00001480
Iteration 53/1000 | Loss: 0.00001480
Iteration 54/1000 | Loss: 0.00001480
Iteration 55/1000 | Loss: 0.00001480
Iteration 56/1000 | Loss: 0.00001480
Iteration 57/1000 | Loss: 0.00001479
Iteration 58/1000 | Loss: 0.00001479
Iteration 59/1000 | Loss: 0.00001479
Iteration 60/1000 | Loss: 0.00001479
Iteration 61/1000 | Loss: 0.00001479
Iteration 62/1000 | Loss: 0.00001479
Iteration 63/1000 | Loss: 0.00001479
Iteration 64/1000 | Loss: 0.00001478
Iteration 65/1000 | Loss: 0.00001477
Iteration 66/1000 | Loss: 0.00001477
Iteration 67/1000 | Loss: 0.00001477
Iteration 68/1000 | Loss: 0.00001477
Iteration 69/1000 | Loss: 0.00001477
Iteration 70/1000 | Loss: 0.00001477
Iteration 71/1000 | Loss: 0.00001477
Iteration 72/1000 | Loss: 0.00001477
Iteration 73/1000 | Loss: 0.00001477
Iteration 74/1000 | Loss: 0.00001477
Iteration 75/1000 | Loss: 0.00001476
Iteration 76/1000 | Loss: 0.00001476
Iteration 77/1000 | Loss: 0.00001476
Iteration 78/1000 | Loss: 0.00001476
Iteration 79/1000 | Loss: 0.00001476
Iteration 80/1000 | Loss: 0.00001475
Iteration 81/1000 | Loss: 0.00001475
Iteration 82/1000 | Loss: 0.00001475
Iteration 83/1000 | Loss: 0.00001474
Iteration 84/1000 | Loss: 0.00001474
Iteration 85/1000 | Loss: 0.00001474
Iteration 86/1000 | Loss: 0.00001474
Iteration 87/1000 | Loss: 0.00001474
Iteration 88/1000 | Loss: 0.00001474
Iteration 89/1000 | Loss: 0.00001474
Iteration 90/1000 | Loss: 0.00001474
Iteration 91/1000 | Loss: 0.00001474
Iteration 92/1000 | Loss: 0.00001474
Iteration 93/1000 | Loss: 0.00001474
Iteration 94/1000 | Loss: 0.00001474
Iteration 95/1000 | Loss: 0.00001473
Iteration 96/1000 | Loss: 0.00001473
Iteration 97/1000 | Loss: 0.00001473
Iteration 98/1000 | Loss: 0.00001473
Iteration 99/1000 | Loss: 0.00001473
Iteration 100/1000 | Loss: 0.00001473
Iteration 101/1000 | Loss: 0.00001473
Iteration 102/1000 | Loss: 0.00001473
Iteration 103/1000 | Loss: 0.00001473
Iteration 104/1000 | Loss: 0.00001473
Iteration 105/1000 | Loss: 0.00001473
Iteration 106/1000 | Loss: 0.00001473
Iteration 107/1000 | Loss: 0.00001473
Iteration 108/1000 | Loss: 0.00001473
Iteration 109/1000 | Loss: 0.00001472
Iteration 110/1000 | Loss: 0.00001472
Iteration 111/1000 | Loss: 0.00001472
Iteration 112/1000 | Loss: 0.00001472
Iteration 113/1000 | Loss: 0.00001472
Iteration 114/1000 | Loss: 0.00001472
Iteration 115/1000 | Loss: 0.00001471
Iteration 116/1000 | Loss: 0.00001471
Iteration 117/1000 | Loss: 0.00001471
Iteration 118/1000 | Loss: 0.00001471
Iteration 119/1000 | Loss: 0.00001471
Iteration 120/1000 | Loss: 0.00001471
Iteration 121/1000 | Loss: 0.00001471
Iteration 122/1000 | Loss: 0.00001471
Iteration 123/1000 | Loss: 0.00001471
Iteration 124/1000 | Loss: 0.00001470
Iteration 125/1000 | Loss: 0.00001470
Iteration 126/1000 | Loss: 0.00001470
Iteration 127/1000 | Loss: 0.00001470
Iteration 128/1000 | Loss: 0.00001470
Iteration 129/1000 | Loss: 0.00001470
Iteration 130/1000 | Loss: 0.00001470
Iteration 131/1000 | Loss: 0.00001470
Iteration 132/1000 | Loss: 0.00001470
Iteration 133/1000 | Loss: 0.00001470
Iteration 134/1000 | Loss: 0.00001470
Iteration 135/1000 | Loss: 0.00001470
Iteration 136/1000 | Loss: 0.00001470
Iteration 137/1000 | Loss: 0.00001470
Iteration 138/1000 | Loss: 0.00001470
Iteration 139/1000 | Loss: 0.00001470
Iteration 140/1000 | Loss: 0.00001470
Iteration 141/1000 | Loss: 0.00001470
Iteration 142/1000 | Loss: 0.00001470
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [1.4700037354486994e-05, 1.4700037354486994e-05, 1.4700037354486994e-05, 1.4700037354486994e-05, 1.4700037354486994e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4700037354486994e-05

Optimization complete. Final v2v error: 3.241523265838623 mm

Highest mean error: 3.861374855041504 mm for frame 48

Lowest mean error: 2.990579843521118 mm for frame 226

Saving results

Total time: 62.13917279243469
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00401300
Iteration 2/25 | Loss: 0.00077800
Iteration 3/25 | Loss: 0.00064317
Iteration 4/25 | Loss: 0.00061853
Iteration 5/25 | Loss: 0.00060689
Iteration 6/25 | Loss: 0.00060273
Iteration 7/25 | Loss: 0.00060147
Iteration 8/25 | Loss: 0.00060100
Iteration 9/25 | Loss: 0.00060090
Iteration 10/25 | Loss: 0.00060090
Iteration 11/25 | Loss: 0.00060090
Iteration 12/25 | Loss: 0.00060090
Iteration 13/25 | Loss: 0.00060090
Iteration 14/25 | Loss: 0.00060090
Iteration 15/25 | Loss: 0.00060090
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0006008998025208712, 0.0006008998025208712, 0.0006008998025208712, 0.0006008998025208712, 0.0006008998025208712]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006008998025208712

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46389091
Iteration 2/25 | Loss: 0.00028111
Iteration 3/25 | Loss: 0.00028111
Iteration 4/25 | Loss: 0.00028111
Iteration 5/25 | Loss: 0.00028111
Iteration 6/25 | Loss: 0.00028111
Iteration 7/25 | Loss: 0.00028111
Iteration 8/25 | Loss: 0.00028111
Iteration 9/25 | Loss: 0.00028111
Iteration 10/25 | Loss: 0.00028111
Iteration 11/25 | Loss: 0.00028111
Iteration 12/25 | Loss: 0.00028111
Iteration 13/25 | Loss: 0.00028111
Iteration 14/25 | Loss: 0.00028111
Iteration 15/25 | Loss: 0.00028111
Iteration 16/25 | Loss: 0.00028111
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.00028110650600865483, 0.00028110650600865483, 0.00028110650600865483, 0.00028110650600865483, 0.00028110650600865483]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00028110650600865483

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00028111
Iteration 2/1000 | Loss: 0.00002622
Iteration 3/1000 | Loss: 0.00001777
Iteration 4/1000 | Loss: 0.00001391
Iteration 5/1000 | Loss: 0.00001297
Iteration 6/1000 | Loss: 0.00001240
Iteration 7/1000 | Loss: 0.00001204
Iteration 8/1000 | Loss: 0.00001173
Iteration 9/1000 | Loss: 0.00001163
Iteration 10/1000 | Loss: 0.00001162
Iteration 11/1000 | Loss: 0.00001161
Iteration 12/1000 | Loss: 0.00001155
Iteration 13/1000 | Loss: 0.00001149
Iteration 14/1000 | Loss: 0.00001146
Iteration 15/1000 | Loss: 0.00001145
Iteration 16/1000 | Loss: 0.00001144
Iteration 17/1000 | Loss: 0.00001144
Iteration 18/1000 | Loss: 0.00001141
Iteration 19/1000 | Loss: 0.00001140
Iteration 20/1000 | Loss: 0.00001140
Iteration 21/1000 | Loss: 0.00001139
Iteration 22/1000 | Loss: 0.00001139
Iteration 23/1000 | Loss: 0.00001138
Iteration 24/1000 | Loss: 0.00001133
Iteration 25/1000 | Loss: 0.00001132
Iteration 26/1000 | Loss: 0.00001131
Iteration 27/1000 | Loss: 0.00001131
Iteration 28/1000 | Loss: 0.00001130
Iteration 29/1000 | Loss: 0.00001130
Iteration 30/1000 | Loss: 0.00001130
Iteration 31/1000 | Loss: 0.00001130
Iteration 32/1000 | Loss: 0.00001130
Iteration 33/1000 | Loss: 0.00001129
Iteration 34/1000 | Loss: 0.00001129
Iteration 35/1000 | Loss: 0.00001128
Iteration 36/1000 | Loss: 0.00001127
Iteration 37/1000 | Loss: 0.00001127
Iteration 38/1000 | Loss: 0.00001127
Iteration 39/1000 | Loss: 0.00001127
Iteration 40/1000 | Loss: 0.00001126
Iteration 41/1000 | Loss: 0.00001126
Iteration 42/1000 | Loss: 0.00001126
Iteration 43/1000 | Loss: 0.00001126
Iteration 44/1000 | Loss: 0.00001125
Iteration 45/1000 | Loss: 0.00001125
Iteration 46/1000 | Loss: 0.00001125
Iteration 47/1000 | Loss: 0.00001124
Iteration 48/1000 | Loss: 0.00001124
Iteration 49/1000 | Loss: 0.00001123
Iteration 50/1000 | Loss: 0.00001123
Iteration 51/1000 | Loss: 0.00001122
Iteration 52/1000 | Loss: 0.00001122
Iteration 53/1000 | Loss: 0.00001122
Iteration 54/1000 | Loss: 0.00001121
Iteration 55/1000 | Loss: 0.00001121
Iteration 56/1000 | Loss: 0.00001121
Iteration 57/1000 | Loss: 0.00001120
Iteration 58/1000 | Loss: 0.00001120
Iteration 59/1000 | Loss: 0.00001120
Iteration 60/1000 | Loss: 0.00001119
Iteration 61/1000 | Loss: 0.00001119
Iteration 62/1000 | Loss: 0.00001119
Iteration 63/1000 | Loss: 0.00001119
Iteration 64/1000 | Loss: 0.00001119
Iteration 65/1000 | Loss: 0.00001119
Iteration 66/1000 | Loss: 0.00001119
Iteration 67/1000 | Loss: 0.00001119
Iteration 68/1000 | Loss: 0.00001119
Iteration 69/1000 | Loss: 0.00001118
Iteration 70/1000 | Loss: 0.00001118
Iteration 71/1000 | Loss: 0.00001118
Iteration 72/1000 | Loss: 0.00001118
Iteration 73/1000 | Loss: 0.00001118
Iteration 74/1000 | Loss: 0.00001118
Iteration 75/1000 | Loss: 0.00001118
Iteration 76/1000 | Loss: 0.00001118
Iteration 77/1000 | Loss: 0.00001118
Iteration 78/1000 | Loss: 0.00001118
Iteration 79/1000 | Loss: 0.00001117
Iteration 80/1000 | Loss: 0.00001116
Iteration 81/1000 | Loss: 0.00001116
Iteration 82/1000 | Loss: 0.00001114
Iteration 83/1000 | Loss: 0.00001113
Iteration 84/1000 | Loss: 0.00001113
Iteration 85/1000 | Loss: 0.00001113
Iteration 86/1000 | Loss: 0.00001112
Iteration 87/1000 | Loss: 0.00001111
Iteration 88/1000 | Loss: 0.00001111
Iteration 89/1000 | Loss: 0.00001111
Iteration 90/1000 | Loss: 0.00001111
Iteration 91/1000 | Loss: 0.00001111
Iteration 92/1000 | Loss: 0.00001111
Iteration 93/1000 | Loss: 0.00001111
Iteration 94/1000 | Loss: 0.00001111
Iteration 95/1000 | Loss: 0.00001111
Iteration 96/1000 | Loss: 0.00001111
Iteration 97/1000 | Loss: 0.00001110
Iteration 98/1000 | Loss: 0.00001110
Iteration 99/1000 | Loss: 0.00001110
Iteration 100/1000 | Loss: 0.00001110
Iteration 101/1000 | Loss: 0.00001110
Iteration 102/1000 | Loss: 0.00001109
Iteration 103/1000 | Loss: 0.00001109
Iteration 104/1000 | Loss: 0.00001108
Iteration 105/1000 | Loss: 0.00001108
Iteration 106/1000 | Loss: 0.00001108
Iteration 107/1000 | Loss: 0.00001107
Iteration 108/1000 | Loss: 0.00001107
Iteration 109/1000 | Loss: 0.00001107
Iteration 110/1000 | Loss: 0.00001107
Iteration 111/1000 | Loss: 0.00001106
Iteration 112/1000 | Loss: 0.00001106
Iteration 113/1000 | Loss: 0.00001106
Iteration 114/1000 | Loss: 0.00001106
Iteration 115/1000 | Loss: 0.00001106
Iteration 116/1000 | Loss: 0.00001105
Iteration 117/1000 | Loss: 0.00001105
Iteration 118/1000 | Loss: 0.00001105
Iteration 119/1000 | Loss: 0.00001105
Iteration 120/1000 | Loss: 0.00001105
Iteration 121/1000 | Loss: 0.00001105
Iteration 122/1000 | Loss: 0.00001105
Iteration 123/1000 | Loss: 0.00001105
Iteration 124/1000 | Loss: 0.00001105
Iteration 125/1000 | Loss: 0.00001105
Iteration 126/1000 | Loss: 0.00001105
Iteration 127/1000 | Loss: 0.00001105
Iteration 128/1000 | Loss: 0.00001105
Iteration 129/1000 | Loss: 0.00001105
Iteration 130/1000 | Loss: 0.00001105
Iteration 131/1000 | Loss: 0.00001105
Iteration 132/1000 | Loss: 0.00001105
Iteration 133/1000 | Loss: 0.00001105
Iteration 134/1000 | Loss: 0.00001105
Iteration 135/1000 | Loss: 0.00001105
Iteration 136/1000 | Loss: 0.00001105
Iteration 137/1000 | Loss: 0.00001105
Iteration 138/1000 | Loss: 0.00001105
Iteration 139/1000 | Loss: 0.00001105
Iteration 140/1000 | Loss: 0.00001105
Iteration 141/1000 | Loss: 0.00001105
Iteration 142/1000 | Loss: 0.00001105
Iteration 143/1000 | Loss: 0.00001105
Iteration 144/1000 | Loss: 0.00001105
Iteration 145/1000 | Loss: 0.00001105
Iteration 146/1000 | Loss: 0.00001105
Iteration 147/1000 | Loss: 0.00001105
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.1047920452256221e-05, 1.1047920452256221e-05, 1.1047920452256221e-05, 1.1047920452256221e-05, 1.1047920452256221e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1047920452256221e-05

Optimization complete. Final v2v error: 2.800215244293213 mm

Highest mean error: 3.7215869426727295 mm for frame 27

Lowest mean error: 2.522423028945923 mm for frame 87

Saving results

Total time: 36.76193332672119
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00491010
Iteration 2/25 | Loss: 0.00092165
Iteration 3/25 | Loss: 0.00067642
Iteration 4/25 | Loss: 0.00063980
Iteration 5/25 | Loss: 0.00063462
Iteration 6/25 | Loss: 0.00063375
Iteration 7/25 | Loss: 0.00063375
Iteration 8/25 | Loss: 0.00063375
Iteration 9/25 | Loss: 0.00063375
Iteration 10/25 | Loss: 0.00063375
Iteration 11/25 | Loss: 0.00063375
Iteration 12/25 | Loss: 0.00063375
Iteration 13/25 | Loss: 0.00063375
Iteration 14/25 | Loss: 0.00063375
Iteration 15/25 | Loss: 0.00063375
Iteration 16/25 | Loss: 0.00063375
Iteration 17/25 | Loss: 0.00063375
Iteration 18/25 | Loss: 0.00063375
Iteration 19/25 | Loss: 0.00063375
Iteration 20/25 | Loss: 0.00063375
Iteration 21/25 | Loss: 0.00063375
Iteration 22/25 | Loss: 0.00063375
Iteration 23/25 | Loss: 0.00063375
Iteration 24/25 | Loss: 0.00063375
Iteration 25/25 | Loss: 0.00063375

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.87819177
Iteration 2/25 | Loss: 0.00023184
Iteration 3/25 | Loss: 0.00023183
Iteration 4/25 | Loss: 0.00023183
Iteration 5/25 | Loss: 0.00023183
Iteration 6/25 | Loss: 0.00023183
Iteration 7/25 | Loss: 0.00023183
Iteration 8/25 | Loss: 0.00023183
Iteration 9/25 | Loss: 0.00023183
Iteration 10/25 | Loss: 0.00023183
Iteration 11/25 | Loss: 0.00023183
Iteration 12/25 | Loss: 0.00023183
Iteration 13/25 | Loss: 0.00023183
Iteration 14/25 | Loss: 0.00023183
Iteration 15/25 | Loss: 0.00023183
Iteration 16/25 | Loss: 0.00023183
Iteration 17/25 | Loss: 0.00023183
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00023182954464573413, 0.00023182954464573413, 0.00023182954464573413, 0.00023182954464573413, 0.00023182954464573413]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00023182954464573413

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00023183
Iteration 2/1000 | Loss: 0.00002036
Iteration 3/1000 | Loss: 0.00001520
Iteration 4/1000 | Loss: 0.00001434
Iteration 5/1000 | Loss: 0.00001379
Iteration 6/1000 | Loss: 0.00001342
Iteration 7/1000 | Loss: 0.00001317
Iteration 8/1000 | Loss: 0.00001295
Iteration 9/1000 | Loss: 0.00001290
Iteration 10/1000 | Loss: 0.00001288
Iteration 11/1000 | Loss: 0.00001287
Iteration 12/1000 | Loss: 0.00001273
Iteration 13/1000 | Loss: 0.00001272
Iteration 14/1000 | Loss: 0.00001272
Iteration 15/1000 | Loss: 0.00001272
Iteration 16/1000 | Loss: 0.00001272
Iteration 17/1000 | Loss: 0.00001271
Iteration 18/1000 | Loss: 0.00001271
Iteration 19/1000 | Loss: 0.00001269
Iteration 20/1000 | Loss: 0.00001268
Iteration 21/1000 | Loss: 0.00001268
Iteration 22/1000 | Loss: 0.00001265
Iteration 23/1000 | Loss: 0.00001265
Iteration 24/1000 | Loss: 0.00001263
Iteration 25/1000 | Loss: 0.00001263
Iteration 26/1000 | Loss: 0.00001263
Iteration 27/1000 | Loss: 0.00001262
Iteration 28/1000 | Loss: 0.00001262
Iteration 29/1000 | Loss: 0.00001261
Iteration 30/1000 | Loss: 0.00001261
Iteration 31/1000 | Loss: 0.00001261
Iteration 32/1000 | Loss: 0.00001260
Iteration 33/1000 | Loss: 0.00001260
Iteration 34/1000 | Loss: 0.00001259
Iteration 35/1000 | Loss: 0.00001258
Iteration 36/1000 | Loss: 0.00001258
Iteration 37/1000 | Loss: 0.00001257
Iteration 38/1000 | Loss: 0.00001256
Iteration 39/1000 | Loss: 0.00001256
Iteration 40/1000 | Loss: 0.00001256
Iteration 41/1000 | Loss: 0.00001256
Iteration 42/1000 | Loss: 0.00001255
Iteration 43/1000 | Loss: 0.00001255
Iteration 44/1000 | Loss: 0.00001255
Iteration 45/1000 | Loss: 0.00001253
Iteration 46/1000 | Loss: 0.00001253
Iteration 47/1000 | Loss: 0.00001251
Iteration 48/1000 | Loss: 0.00001250
Iteration 49/1000 | Loss: 0.00001250
Iteration 50/1000 | Loss: 0.00001249
Iteration 51/1000 | Loss: 0.00001248
Iteration 52/1000 | Loss: 0.00001248
Iteration 53/1000 | Loss: 0.00001247
Iteration 54/1000 | Loss: 0.00001247
Iteration 55/1000 | Loss: 0.00001247
Iteration 56/1000 | Loss: 0.00001247
Iteration 57/1000 | Loss: 0.00001247
Iteration 58/1000 | Loss: 0.00001247
Iteration 59/1000 | Loss: 0.00001247
Iteration 60/1000 | Loss: 0.00001247
Iteration 61/1000 | Loss: 0.00001247
Iteration 62/1000 | Loss: 0.00001247
Iteration 63/1000 | Loss: 0.00001246
Iteration 64/1000 | Loss: 0.00001246
Iteration 65/1000 | Loss: 0.00001246
Iteration 66/1000 | Loss: 0.00001246
Iteration 67/1000 | Loss: 0.00001246
Iteration 68/1000 | Loss: 0.00001246
Iteration 69/1000 | Loss: 0.00001245
Iteration 70/1000 | Loss: 0.00001245
Iteration 71/1000 | Loss: 0.00001245
Iteration 72/1000 | Loss: 0.00001244
Iteration 73/1000 | Loss: 0.00001244
Iteration 74/1000 | Loss: 0.00001244
Iteration 75/1000 | Loss: 0.00001244
Iteration 76/1000 | Loss: 0.00001243
Iteration 77/1000 | Loss: 0.00001243
Iteration 78/1000 | Loss: 0.00001243
Iteration 79/1000 | Loss: 0.00001243
Iteration 80/1000 | Loss: 0.00001243
Iteration 81/1000 | Loss: 0.00001243
Iteration 82/1000 | Loss: 0.00001243
Iteration 83/1000 | Loss: 0.00001243
Iteration 84/1000 | Loss: 0.00001243
Iteration 85/1000 | Loss: 0.00001243
Iteration 86/1000 | Loss: 0.00001243
Iteration 87/1000 | Loss: 0.00001243
Iteration 88/1000 | Loss: 0.00001243
Iteration 89/1000 | Loss: 0.00001242
Iteration 90/1000 | Loss: 0.00001242
Iteration 91/1000 | Loss: 0.00001242
Iteration 92/1000 | Loss: 0.00001242
Iteration 93/1000 | Loss: 0.00001241
Iteration 94/1000 | Loss: 0.00001241
Iteration 95/1000 | Loss: 0.00001241
Iteration 96/1000 | Loss: 0.00001241
Iteration 97/1000 | Loss: 0.00001241
Iteration 98/1000 | Loss: 0.00001241
Iteration 99/1000 | Loss: 0.00001241
Iteration 100/1000 | Loss: 0.00001241
Iteration 101/1000 | Loss: 0.00001241
Iteration 102/1000 | Loss: 0.00001241
Iteration 103/1000 | Loss: 0.00001241
Iteration 104/1000 | Loss: 0.00001241
Iteration 105/1000 | Loss: 0.00001240
Iteration 106/1000 | Loss: 0.00001240
Iteration 107/1000 | Loss: 0.00001240
Iteration 108/1000 | Loss: 0.00001240
Iteration 109/1000 | Loss: 0.00001240
Iteration 110/1000 | Loss: 0.00001240
Iteration 111/1000 | Loss: 0.00001240
Iteration 112/1000 | Loss: 0.00001240
Iteration 113/1000 | Loss: 0.00001239
Iteration 114/1000 | Loss: 0.00001239
Iteration 115/1000 | Loss: 0.00001239
Iteration 116/1000 | Loss: 0.00001239
Iteration 117/1000 | Loss: 0.00001239
Iteration 118/1000 | Loss: 0.00001239
Iteration 119/1000 | Loss: 0.00001238
Iteration 120/1000 | Loss: 0.00001238
Iteration 121/1000 | Loss: 0.00001238
Iteration 122/1000 | Loss: 0.00001238
Iteration 123/1000 | Loss: 0.00001238
Iteration 124/1000 | Loss: 0.00001238
Iteration 125/1000 | Loss: 0.00001237
Iteration 126/1000 | Loss: 0.00001237
Iteration 127/1000 | Loss: 0.00001237
Iteration 128/1000 | Loss: 0.00001237
Iteration 129/1000 | Loss: 0.00001237
Iteration 130/1000 | Loss: 0.00001237
Iteration 131/1000 | Loss: 0.00001237
Iteration 132/1000 | Loss: 0.00001237
Iteration 133/1000 | Loss: 0.00001237
Iteration 134/1000 | Loss: 0.00001237
Iteration 135/1000 | Loss: 0.00001237
Iteration 136/1000 | Loss: 0.00001237
Iteration 137/1000 | Loss: 0.00001237
Iteration 138/1000 | Loss: 0.00001237
Iteration 139/1000 | Loss: 0.00001237
Iteration 140/1000 | Loss: 0.00001237
Iteration 141/1000 | Loss: 0.00001237
Iteration 142/1000 | Loss: 0.00001236
Iteration 143/1000 | Loss: 0.00001236
Iteration 144/1000 | Loss: 0.00001236
Iteration 145/1000 | Loss: 0.00001236
Iteration 146/1000 | Loss: 0.00001236
Iteration 147/1000 | Loss: 0.00001236
Iteration 148/1000 | Loss: 0.00001236
Iteration 149/1000 | Loss: 0.00001236
Iteration 150/1000 | Loss: 0.00001236
Iteration 151/1000 | Loss: 0.00001236
Iteration 152/1000 | Loss: 0.00001235
Iteration 153/1000 | Loss: 0.00001235
Iteration 154/1000 | Loss: 0.00001235
Iteration 155/1000 | Loss: 0.00001235
Iteration 156/1000 | Loss: 0.00001235
Iteration 157/1000 | Loss: 0.00001235
Iteration 158/1000 | Loss: 0.00001235
Iteration 159/1000 | Loss: 0.00001235
Iteration 160/1000 | Loss: 0.00001235
Iteration 161/1000 | Loss: 0.00001235
Iteration 162/1000 | Loss: 0.00001235
Iteration 163/1000 | Loss: 0.00001235
Iteration 164/1000 | Loss: 0.00001235
Iteration 165/1000 | Loss: 0.00001234
Iteration 166/1000 | Loss: 0.00001234
Iteration 167/1000 | Loss: 0.00001234
Iteration 168/1000 | Loss: 0.00001234
Iteration 169/1000 | Loss: 0.00001234
Iteration 170/1000 | Loss: 0.00001234
Iteration 171/1000 | Loss: 0.00001234
Iteration 172/1000 | Loss: 0.00001234
Iteration 173/1000 | Loss: 0.00001234
Iteration 174/1000 | Loss: 0.00001234
Iteration 175/1000 | Loss: 0.00001234
Iteration 176/1000 | Loss: 0.00001234
Iteration 177/1000 | Loss: 0.00001233
Iteration 178/1000 | Loss: 0.00001233
Iteration 179/1000 | Loss: 0.00001233
Iteration 180/1000 | Loss: 0.00001233
Iteration 181/1000 | Loss: 0.00001233
Iteration 182/1000 | Loss: 0.00001233
Iteration 183/1000 | Loss: 0.00001233
Iteration 184/1000 | Loss: 0.00001233
Iteration 185/1000 | Loss: 0.00001233
Iteration 186/1000 | Loss: 0.00001233
Iteration 187/1000 | Loss: 0.00001233
Iteration 188/1000 | Loss: 0.00001233
Iteration 189/1000 | Loss: 0.00001233
Iteration 190/1000 | Loss: 0.00001233
Iteration 191/1000 | Loss: 0.00001233
Iteration 192/1000 | Loss: 0.00001233
Iteration 193/1000 | Loss: 0.00001232
Iteration 194/1000 | Loss: 0.00001232
Iteration 195/1000 | Loss: 0.00001232
Iteration 196/1000 | Loss: 0.00001232
Iteration 197/1000 | Loss: 0.00001232
Iteration 198/1000 | Loss: 0.00001232
Iteration 199/1000 | Loss: 0.00001232
Iteration 200/1000 | Loss: 0.00001231
Iteration 201/1000 | Loss: 0.00001231
Iteration 202/1000 | Loss: 0.00001231
Iteration 203/1000 | Loss: 0.00001231
Iteration 204/1000 | Loss: 0.00001231
Iteration 205/1000 | Loss: 0.00001230
Iteration 206/1000 | Loss: 0.00001230
Iteration 207/1000 | Loss: 0.00001230
Iteration 208/1000 | Loss: 0.00001230
Iteration 209/1000 | Loss: 0.00001230
Iteration 210/1000 | Loss: 0.00001230
Iteration 211/1000 | Loss: 0.00001230
Iteration 212/1000 | Loss: 0.00001230
Iteration 213/1000 | Loss: 0.00001230
Iteration 214/1000 | Loss: 0.00001230
Iteration 215/1000 | Loss: 0.00001230
Iteration 216/1000 | Loss: 0.00001230
Iteration 217/1000 | Loss: 0.00001230
Iteration 218/1000 | Loss: 0.00001230
Iteration 219/1000 | Loss: 0.00001230
Iteration 220/1000 | Loss: 0.00001230
Iteration 221/1000 | Loss: 0.00001230
Iteration 222/1000 | Loss: 0.00001230
Iteration 223/1000 | Loss: 0.00001230
Iteration 224/1000 | Loss: 0.00001230
Iteration 225/1000 | Loss: 0.00001230
Iteration 226/1000 | Loss: 0.00001230
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [1.2298479305172805e-05, 1.2298479305172805e-05, 1.2298479305172805e-05, 1.2298479305172805e-05, 1.2298479305172805e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2298479305172805e-05

Optimization complete. Final v2v error: 3.0147242546081543 mm

Highest mean error: 3.0518274307250977 mm for frame 113

Lowest mean error: 2.969498634338379 mm for frame 5

Saving results

Total time: 36.79812240600586
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_corey_posed_014/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_corey_posed_014/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01072993
Iteration 2/25 | Loss: 0.00189352
Iteration 3/25 | Loss: 0.00130667
Iteration 4/25 | Loss: 0.00118529
Iteration 5/25 | Loss: 0.00111543
Iteration 6/25 | Loss: 0.00108623
Iteration 7/25 | Loss: 0.00104317
Iteration 8/25 | Loss: 0.00098968
Iteration 9/25 | Loss: 0.00090898
Iteration 10/25 | Loss: 0.00084159
Iteration 11/25 | Loss: 0.00083761
Iteration 12/25 | Loss: 0.00084193
Iteration 13/25 | Loss: 0.00082476
Iteration 14/25 | Loss: 0.00081335
Iteration 15/25 | Loss: 0.00079710
Iteration 16/25 | Loss: 0.00079390
Iteration 17/25 | Loss: 0.00079236
Iteration 18/25 | Loss: 0.00079166
Iteration 19/25 | Loss: 0.00079123
Iteration 20/25 | Loss: 0.00079091
Iteration 21/25 | Loss: 0.00079085
Iteration 22/25 | Loss: 0.00079085
Iteration 23/25 | Loss: 0.00079084
Iteration 24/25 | Loss: 0.00079084
Iteration 25/25 | Loss: 0.00079084

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45389462
Iteration 2/25 | Loss: 0.00097276
Iteration 3/25 | Loss: 0.00097276
Iteration 4/25 | Loss: 0.00097276
Iteration 5/25 | Loss: 0.00097276
Iteration 6/25 | Loss: 0.00097275
Iteration 7/25 | Loss: 0.00097275
Iteration 8/25 | Loss: 0.00097275
Iteration 9/25 | Loss: 0.00097275
Iteration 10/25 | Loss: 0.00097275
Iteration 11/25 | Loss: 0.00097275
Iteration 12/25 | Loss: 0.00097275
Iteration 13/25 | Loss: 0.00097275
Iteration 14/25 | Loss: 0.00097275
Iteration 15/25 | Loss: 0.00097275
Iteration 16/25 | Loss: 0.00097275
Iteration 17/25 | Loss: 0.00097275
Iteration 18/25 | Loss: 0.00097275
Iteration 19/25 | Loss: 0.00097275
Iteration 20/25 | Loss: 0.00097275
Iteration 21/25 | Loss: 0.00097275
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009727539145387709, 0.0009727539145387709, 0.0009727539145387709, 0.0009727539145387709, 0.0009727539145387709]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009727539145387709

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097275
Iteration 2/1000 | Loss: 0.00192594
Iteration 3/1000 | Loss: 0.00037680
Iteration 4/1000 | Loss: 0.00015241
Iteration 5/1000 | Loss: 0.00010911
Iteration 6/1000 | Loss: 0.00007921
Iteration 7/1000 | Loss: 0.00006448
Iteration 8/1000 | Loss: 0.00101686
Iteration 9/1000 | Loss: 0.00054562
Iteration 10/1000 | Loss: 0.00127953
Iteration 11/1000 | Loss: 0.00123210
Iteration 12/1000 | Loss: 0.00040529
Iteration 13/1000 | Loss: 0.00079755
Iteration 14/1000 | Loss: 0.00045974
Iteration 15/1000 | Loss: 0.00006297
Iteration 16/1000 | Loss: 0.00005018
Iteration 17/1000 | Loss: 0.00029653
Iteration 18/1000 | Loss: 0.00025744
Iteration 19/1000 | Loss: 0.00027973
Iteration 20/1000 | Loss: 0.00025931
Iteration 21/1000 | Loss: 0.00045461
Iteration 22/1000 | Loss: 0.00004060
Iteration 23/1000 | Loss: 0.00011538
Iteration 24/1000 | Loss: 0.00003706
Iteration 25/1000 | Loss: 0.00101750
Iteration 26/1000 | Loss: 0.00097115
Iteration 27/1000 | Loss: 0.00091906
Iteration 28/1000 | Loss: 0.00014746
Iteration 29/1000 | Loss: 0.00048043
Iteration 30/1000 | Loss: 0.00120567
Iteration 31/1000 | Loss: 0.00073835
Iteration 32/1000 | Loss: 0.00033835
Iteration 33/1000 | Loss: 0.00050937
Iteration 34/1000 | Loss: 0.00063088
Iteration 35/1000 | Loss: 0.00047788
Iteration 36/1000 | Loss: 0.00030437
Iteration 37/1000 | Loss: 0.00011845
Iteration 38/1000 | Loss: 0.00041013
Iteration 39/1000 | Loss: 0.00035825
Iteration 40/1000 | Loss: 0.00004135
Iteration 41/1000 | Loss: 0.00038228
Iteration 42/1000 | Loss: 0.00019928
Iteration 43/1000 | Loss: 0.00059730
Iteration 44/1000 | Loss: 0.00016925
Iteration 45/1000 | Loss: 0.00017599
Iteration 46/1000 | Loss: 0.00006961
Iteration 47/1000 | Loss: 0.00019159
Iteration 48/1000 | Loss: 0.00011736
Iteration 49/1000 | Loss: 0.00017118
Iteration 50/1000 | Loss: 0.00024684
Iteration 51/1000 | Loss: 0.00005884
Iteration 52/1000 | Loss: 0.00019433
Iteration 53/1000 | Loss: 0.00002735
Iteration 54/1000 | Loss: 0.00002604
Iteration 55/1000 | Loss: 0.00002504
Iteration 56/1000 | Loss: 0.00002406
Iteration 57/1000 | Loss: 0.00031127
Iteration 58/1000 | Loss: 0.00024098
Iteration 59/1000 | Loss: 0.00029504
Iteration 60/1000 | Loss: 0.00022570
Iteration 61/1000 | Loss: 0.00003231
Iteration 62/1000 | Loss: 0.00002677
Iteration 63/1000 | Loss: 0.00045739
Iteration 64/1000 | Loss: 0.00005139
Iteration 65/1000 | Loss: 0.00049803
Iteration 66/1000 | Loss: 0.00024417
Iteration 67/1000 | Loss: 0.00003884
Iteration 68/1000 | Loss: 0.00033219
Iteration 69/1000 | Loss: 0.00017820
Iteration 70/1000 | Loss: 0.00016361
Iteration 71/1000 | Loss: 0.00032256
Iteration 72/1000 | Loss: 0.00002680
Iteration 73/1000 | Loss: 0.00002380
Iteration 74/1000 | Loss: 0.00002155
Iteration 75/1000 | Loss: 0.00002040
Iteration 76/1000 | Loss: 0.00001933
Iteration 77/1000 | Loss: 0.00001891
Iteration 78/1000 | Loss: 0.00001859
Iteration 79/1000 | Loss: 0.00001841
Iteration 80/1000 | Loss: 0.00001827
Iteration 81/1000 | Loss: 0.00001823
Iteration 82/1000 | Loss: 0.00001818
Iteration 83/1000 | Loss: 0.00001818
Iteration 84/1000 | Loss: 0.00001817
Iteration 85/1000 | Loss: 0.00001813
Iteration 86/1000 | Loss: 0.00001812
Iteration 87/1000 | Loss: 0.00001812
Iteration 88/1000 | Loss: 0.00001812
Iteration 89/1000 | Loss: 0.00001811
Iteration 90/1000 | Loss: 0.00001810
Iteration 91/1000 | Loss: 0.00001810
Iteration 92/1000 | Loss: 0.00001810
Iteration 93/1000 | Loss: 0.00001810
Iteration 94/1000 | Loss: 0.00001809
Iteration 95/1000 | Loss: 0.00001809
Iteration 96/1000 | Loss: 0.00001809
Iteration 97/1000 | Loss: 0.00001809
Iteration 98/1000 | Loss: 0.00001809
Iteration 99/1000 | Loss: 0.00001809
Iteration 100/1000 | Loss: 0.00001809
Iteration 101/1000 | Loss: 0.00001808
Iteration 102/1000 | Loss: 0.00001808
Iteration 103/1000 | Loss: 0.00001807
Iteration 104/1000 | Loss: 0.00001807
Iteration 105/1000 | Loss: 0.00001807
Iteration 106/1000 | Loss: 0.00001807
Iteration 107/1000 | Loss: 0.00001806
Iteration 108/1000 | Loss: 0.00001806
Iteration 109/1000 | Loss: 0.00001806
Iteration 110/1000 | Loss: 0.00001805
Iteration 111/1000 | Loss: 0.00001805
Iteration 112/1000 | Loss: 0.00001805
Iteration 113/1000 | Loss: 0.00001804
Iteration 114/1000 | Loss: 0.00001804
Iteration 115/1000 | Loss: 0.00001804
Iteration 116/1000 | Loss: 0.00001803
Iteration 117/1000 | Loss: 0.00001803
Iteration 118/1000 | Loss: 0.00001802
Iteration 119/1000 | Loss: 0.00001802
Iteration 120/1000 | Loss: 0.00001802
Iteration 121/1000 | Loss: 0.00001802
Iteration 122/1000 | Loss: 0.00001801
Iteration 123/1000 | Loss: 0.00001801
Iteration 124/1000 | Loss: 0.00001801
Iteration 125/1000 | Loss: 0.00001800
Iteration 126/1000 | Loss: 0.00001800
Iteration 127/1000 | Loss: 0.00001800
Iteration 128/1000 | Loss: 0.00001800
Iteration 129/1000 | Loss: 0.00001799
Iteration 130/1000 | Loss: 0.00001799
Iteration 131/1000 | Loss: 0.00001799
Iteration 132/1000 | Loss: 0.00001799
Iteration 133/1000 | Loss: 0.00001799
Iteration 134/1000 | Loss: 0.00001798
Iteration 135/1000 | Loss: 0.00001798
Iteration 136/1000 | Loss: 0.00001798
Iteration 137/1000 | Loss: 0.00001798
Iteration 138/1000 | Loss: 0.00001798
Iteration 139/1000 | Loss: 0.00001798
Iteration 140/1000 | Loss: 0.00001798
Iteration 141/1000 | Loss: 0.00001798
Iteration 142/1000 | Loss: 0.00001798
Iteration 143/1000 | Loss: 0.00001798
Iteration 144/1000 | Loss: 0.00001798
Iteration 145/1000 | Loss: 0.00001797
Iteration 146/1000 | Loss: 0.00001797
Iteration 147/1000 | Loss: 0.00001797
Iteration 148/1000 | Loss: 0.00001797
Iteration 149/1000 | Loss: 0.00001797
Iteration 150/1000 | Loss: 0.00001797
Iteration 151/1000 | Loss: 0.00001796
Iteration 152/1000 | Loss: 0.00001796
Iteration 153/1000 | Loss: 0.00001796
Iteration 154/1000 | Loss: 0.00001796
Iteration 155/1000 | Loss: 0.00001796
Iteration 156/1000 | Loss: 0.00001796
Iteration 157/1000 | Loss: 0.00001796
Iteration 158/1000 | Loss: 0.00001796
Iteration 159/1000 | Loss: 0.00001796
Iteration 160/1000 | Loss: 0.00001796
Iteration 161/1000 | Loss: 0.00001796
Iteration 162/1000 | Loss: 0.00001796
Iteration 163/1000 | Loss: 0.00001795
Iteration 164/1000 | Loss: 0.00001795
Iteration 165/1000 | Loss: 0.00001795
Iteration 166/1000 | Loss: 0.00001794
Iteration 167/1000 | Loss: 0.00001794
Iteration 168/1000 | Loss: 0.00001794
Iteration 169/1000 | Loss: 0.00001794
Iteration 170/1000 | Loss: 0.00001794
Iteration 171/1000 | Loss: 0.00001794
Iteration 172/1000 | Loss: 0.00001794
Iteration 173/1000 | Loss: 0.00001794
Iteration 174/1000 | Loss: 0.00001794
Iteration 175/1000 | Loss: 0.00001794
Iteration 176/1000 | Loss: 0.00001794
Iteration 177/1000 | Loss: 0.00001794
Iteration 178/1000 | Loss: 0.00001794
Iteration 179/1000 | Loss: 0.00001794
Iteration 180/1000 | Loss: 0.00001794
Iteration 181/1000 | Loss: 0.00001794
Iteration 182/1000 | Loss: 0.00001794
Iteration 183/1000 | Loss: 0.00001794
Iteration 184/1000 | Loss: 0.00001794
Iteration 185/1000 | Loss: 0.00001794
Iteration 186/1000 | Loss: 0.00001794
Iteration 187/1000 | Loss: 0.00001794
Iteration 188/1000 | Loss: 0.00001794
Iteration 189/1000 | Loss: 0.00001794
Iteration 190/1000 | Loss: 0.00001794
Iteration 191/1000 | Loss: 0.00001794
Iteration 192/1000 | Loss: 0.00001794
Iteration 193/1000 | Loss: 0.00001794
Iteration 194/1000 | Loss: 0.00001794
Iteration 195/1000 | Loss: 0.00001794
Iteration 196/1000 | Loss: 0.00001794
Iteration 197/1000 | Loss: 0.00001794
Iteration 198/1000 | Loss: 0.00001794
Iteration 199/1000 | Loss: 0.00001794
Iteration 200/1000 | Loss: 0.00001794
Iteration 201/1000 | Loss: 0.00001794
Iteration 202/1000 | Loss: 0.00001794
Iteration 203/1000 | Loss: 0.00001794
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [1.7938027667696588e-05, 1.7938027667696588e-05, 1.7938027667696588e-05, 1.7938027667696588e-05, 1.7938027667696588e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7938027667696588e-05

Optimization complete. Final v2v error: 3.548250675201416 mm

Highest mean error: 4.068861484527588 mm for frame 164

Lowest mean error: 3.1770126819610596 mm for frame 228

Saving results

Total time: 176.73477840423584
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_0196/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0196/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0196/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00982450
Iteration 2/25 | Loss: 0.00359789
Iteration 3/25 | Loss: 0.00234062
Iteration 4/25 | Loss: 0.00196181
Iteration 5/25 | Loss: 0.00189994
Iteration 6/25 | Loss: 0.00189139
Iteration 7/25 | Loss: 0.00185847
Iteration 8/25 | Loss: 0.00182723
Iteration 9/25 | Loss: 0.00178209
Iteration 10/25 | Loss: 0.00177408
Iteration 11/25 | Loss: 0.00177410
Iteration 12/25 | Loss: 0.00176970
Iteration 13/25 | Loss: 0.00176869
Iteration 14/25 | Loss: 0.00176492
Iteration 15/25 | Loss: 0.00176263
Iteration 16/25 | Loss: 0.00176180
Iteration 17/25 | Loss: 0.00176161
Iteration 18/25 | Loss: 0.00176152
Iteration 19/25 | Loss: 0.00176144
Iteration 20/25 | Loss: 0.00176140
Iteration 21/25 | Loss: 0.00176140
Iteration 22/25 | Loss: 0.00176137
Iteration 23/25 | Loss: 0.00176137
Iteration 24/25 | Loss: 0.00176137
Iteration 25/25 | Loss: 0.00176137

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.26311970
Iteration 2/25 | Loss: 0.00569192
Iteration 3/25 | Loss: 0.00568586
Iteration 4/25 | Loss: 0.00568586
Iteration 5/25 | Loss: 0.00568586
Iteration 6/25 | Loss: 0.00568586
Iteration 7/25 | Loss: 0.00568586
Iteration 8/25 | Loss: 0.00568586
Iteration 9/25 | Loss: 0.00568586
Iteration 10/25 | Loss: 0.00568586
Iteration 11/25 | Loss: 0.00568586
Iteration 12/25 | Loss: 0.00568586
Iteration 13/25 | Loss: 0.00568586
Iteration 14/25 | Loss: 0.00568586
Iteration 15/25 | Loss: 0.00568586
Iteration 16/25 | Loss: 0.00568586
Iteration 17/25 | Loss: 0.00568586
Iteration 18/25 | Loss: 0.00568586
Iteration 19/25 | Loss: 0.00568586
Iteration 20/25 | Loss: 0.00568586
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.005685862153768539, 0.005685862153768539, 0.005685862153768539, 0.005685862153768539, 0.005685862153768539]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005685862153768539

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00568586
Iteration 2/1000 | Loss: 0.00088459
Iteration 3/1000 | Loss: 0.00161852
Iteration 4/1000 | Loss: 0.00220475
Iteration 5/1000 | Loss: 0.00401302
Iteration 6/1000 | Loss: 0.01913341
Iteration 7/1000 | Loss: 0.00645907
Iteration 8/1000 | Loss: 0.01422349
Iteration 9/1000 | Loss: 0.01181573
Iteration 10/1000 | Loss: 0.00516058
Iteration 11/1000 | Loss: 0.00473033
Iteration 12/1000 | Loss: 0.00057037
Iteration 13/1000 | Loss: 0.00149471
Iteration 14/1000 | Loss: 0.00359446
Iteration 15/1000 | Loss: 0.00586135
Iteration 16/1000 | Loss: 0.00313840
Iteration 17/1000 | Loss: 0.00304288
Iteration 18/1000 | Loss: 0.00137598
Iteration 19/1000 | Loss: 0.00252625
Iteration 20/1000 | Loss: 0.00501771
Iteration 21/1000 | Loss: 0.00157865
Iteration 22/1000 | Loss: 0.00793803
Iteration 23/1000 | Loss: 0.00077678
Iteration 24/1000 | Loss: 0.00223446
Iteration 25/1000 | Loss: 0.00338205
Iteration 26/1000 | Loss: 0.00094663
Iteration 27/1000 | Loss: 0.00410772
Iteration 28/1000 | Loss: 0.00112266
Iteration 29/1000 | Loss: 0.00187450
Iteration 30/1000 | Loss: 0.00389183
Iteration 31/1000 | Loss: 0.00404022
Iteration 32/1000 | Loss: 0.00171885
Iteration 33/1000 | Loss: 0.00180771
Iteration 34/1000 | Loss: 0.00458165
Iteration 35/1000 | Loss: 0.00212217
Iteration 36/1000 | Loss: 0.00210971
Iteration 37/1000 | Loss: 0.00242470
Iteration 38/1000 | Loss: 0.00160129
Iteration 39/1000 | Loss: 0.00116746
Iteration 40/1000 | Loss: 0.00167618
Iteration 41/1000 | Loss: 0.00134352
Iteration 42/1000 | Loss: 0.00304680
Iteration 43/1000 | Loss: 0.00186011
Iteration 44/1000 | Loss: 0.00125338
Iteration 45/1000 | Loss: 0.00538199
Iteration 46/1000 | Loss: 0.00162309
Iteration 47/1000 | Loss: 0.00364338
Iteration 48/1000 | Loss: 0.00329526
Iteration 49/1000 | Loss: 0.00845908
Iteration 50/1000 | Loss: 0.00288733
Iteration 51/1000 | Loss: 0.00313523
Iteration 52/1000 | Loss: 0.00209151
Iteration 53/1000 | Loss: 0.00310630
Iteration 54/1000 | Loss: 0.00239926
Iteration 55/1000 | Loss: 0.00152493
Iteration 56/1000 | Loss: 0.00142505
Iteration 57/1000 | Loss: 0.00180291
Iteration 58/1000 | Loss: 0.00297883
Iteration 59/1000 | Loss: 0.00265389
Iteration 60/1000 | Loss: 0.00411416
Iteration 61/1000 | Loss: 0.00298857
Iteration 62/1000 | Loss: 0.00150987
Iteration 63/1000 | Loss: 0.00269595
Iteration 64/1000 | Loss: 0.00267876
Iteration 65/1000 | Loss: 0.00218321
Iteration 66/1000 | Loss: 0.00178825
Iteration 67/1000 | Loss: 0.00287429
Iteration 68/1000 | Loss: 0.00107603
Iteration 69/1000 | Loss: 0.00090722
Iteration 70/1000 | Loss: 0.00171405
Iteration 71/1000 | Loss: 0.00072446
Iteration 72/1000 | Loss: 0.00132174
Iteration 73/1000 | Loss: 0.00129782
Iteration 74/1000 | Loss: 0.00128229
Iteration 75/1000 | Loss: 0.00108524
Iteration 76/1000 | Loss: 0.00065840
Iteration 77/1000 | Loss: 0.00122310
Iteration 78/1000 | Loss: 0.00119660
Iteration 79/1000 | Loss: 0.00102463
Iteration 80/1000 | Loss: 0.00141972
Iteration 81/1000 | Loss: 0.00088469
Iteration 82/1000 | Loss: 0.00140788
Iteration 83/1000 | Loss: 0.00231182
Iteration 84/1000 | Loss: 0.00092479
Iteration 85/1000 | Loss: 0.00096981
Iteration 86/1000 | Loss: 0.00213793
Iteration 87/1000 | Loss: 0.00342942
Iteration 88/1000 | Loss: 0.00387866
Iteration 89/1000 | Loss: 0.00273689
Iteration 90/1000 | Loss: 0.00185214
Iteration 91/1000 | Loss: 0.00066285
Iteration 92/1000 | Loss: 0.00315094
Iteration 93/1000 | Loss: 0.00245573
Iteration 94/1000 | Loss: 0.00310822
Iteration 95/1000 | Loss: 0.00103952
Iteration 96/1000 | Loss: 0.00098178
Iteration 97/1000 | Loss: 0.00097369
Iteration 98/1000 | Loss: 0.00092585
Iteration 99/1000 | Loss: 0.00037678
Iteration 100/1000 | Loss: 0.00303608
Iteration 101/1000 | Loss: 0.00218297
Iteration 102/1000 | Loss: 0.00141691
Iteration 103/1000 | Loss: 0.00196658
Iteration 104/1000 | Loss: 0.00182155
Iteration 105/1000 | Loss: 0.00189101
Iteration 106/1000 | Loss: 0.00191155
Iteration 107/1000 | Loss: 0.00241864
Iteration 108/1000 | Loss: 0.00241141
Iteration 109/1000 | Loss: 0.00169094
Iteration 110/1000 | Loss: 0.00293088
Iteration 111/1000 | Loss: 0.00229730
Iteration 112/1000 | Loss: 0.00040513
Iteration 113/1000 | Loss: 0.00117141
Iteration 114/1000 | Loss: 0.00041874
Iteration 115/1000 | Loss: 0.00013898
Iteration 116/1000 | Loss: 0.00063558
Iteration 117/1000 | Loss: 0.00012659
Iteration 118/1000 | Loss: 0.00037770
Iteration 119/1000 | Loss: 0.00011304
Iteration 120/1000 | Loss: 0.00007565
Iteration 121/1000 | Loss: 0.00033408
Iteration 122/1000 | Loss: 0.00090343
Iteration 123/1000 | Loss: 0.00114711
Iteration 124/1000 | Loss: 0.00088357
Iteration 125/1000 | Loss: 0.00022912
Iteration 126/1000 | Loss: 0.00028240
Iteration 127/1000 | Loss: 0.00006835
Iteration 128/1000 | Loss: 0.00008956
Iteration 129/1000 | Loss: 0.00010647
Iteration 130/1000 | Loss: 0.00023668
Iteration 131/1000 | Loss: 0.00005750
Iteration 132/1000 | Loss: 0.00008608
Iteration 133/1000 | Loss: 0.00005440
Iteration 134/1000 | Loss: 0.00005351
Iteration 135/1000 | Loss: 0.00057791
Iteration 136/1000 | Loss: 0.00039490
Iteration 137/1000 | Loss: 0.00063860
Iteration 138/1000 | Loss: 0.00023997
Iteration 139/1000 | Loss: 0.00007962
Iteration 140/1000 | Loss: 0.00048995
Iteration 141/1000 | Loss: 0.00020074
Iteration 142/1000 | Loss: 0.00043957
Iteration 143/1000 | Loss: 0.00026151
Iteration 144/1000 | Loss: 0.00072043
Iteration 145/1000 | Loss: 0.00022143
Iteration 146/1000 | Loss: 0.00013227
Iteration 147/1000 | Loss: 0.00223496
Iteration 148/1000 | Loss: 0.00088241
Iteration 149/1000 | Loss: 0.00088384
Iteration 150/1000 | Loss: 0.00020817
Iteration 151/1000 | Loss: 0.00174144
Iteration 152/1000 | Loss: 0.00105650
Iteration 153/1000 | Loss: 0.00063400
Iteration 154/1000 | Loss: 0.00129580
Iteration 155/1000 | Loss: 0.00047585
Iteration 156/1000 | Loss: 0.00052115
Iteration 157/1000 | Loss: 0.00058097
Iteration 158/1000 | Loss: 0.00028734
Iteration 159/1000 | Loss: 0.00039614
Iteration 160/1000 | Loss: 0.00021162
Iteration 161/1000 | Loss: 0.00029578
Iteration 162/1000 | Loss: 0.00010198
Iteration 163/1000 | Loss: 0.00015610
Iteration 164/1000 | Loss: 0.00011835
Iteration 165/1000 | Loss: 0.00014790
Iteration 166/1000 | Loss: 0.00009603
Iteration 167/1000 | Loss: 0.00010047
Iteration 168/1000 | Loss: 0.00010521
Iteration 169/1000 | Loss: 0.00010156
Iteration 170/1000 | Loss: 0.00007787
Iteration 171/1000 | Loss: 0.00019978
Iteration 172/1000 | Loss: 0.00010653
Iteration 173/1000 | Loss: 0.00025736
Iteration 174/1000 | Loss: 0.00033846
Iteration 175/1000 | Loss: 0.00011997
Iteration 176/1000 | Loss: 0.00010060
Iteration 177/1000 | Loss: 0.00012535
Iteration 178/1000 | Loss: 0.00010727
Iteration 179/1000 | Loss: 0.00008413
Iteration 180/1000 | Loss: 0.00027195
Iteration 181/1000 | Loss: 0.00009279
Iteration 182/1000 | Loss: 0.00008983
Iteration 183/1000 | Loss: 0.00007854
Iteration 184/1000 | Loss: 0.00009473
Iteration 185/1000 | Loss: 0.00010885
Iteration 186/1000 | Loss: 0.00015628
Iteration 187/1000 | Loss: 0.00009521
Iteration 188/1000 | Loss: 0.00009528
Iteration 189/1000 | Loss: 0.00009287
Iteration 190/1000 | Loss: 0.00009226
Iteration 191/1000 | Loss: 0.00008117
Iteration 192/1000 | Loss: 0.00010824
Iteration 193/1000 | Loss: 0.00008323
Iteration 194/1000 | Loss: 0.00010202
Iteration 195/1000 | Loss: 0.00029585
Iteration 196/1000 | Loss: 0.00011921
Iteration 197/1000 | Loss: 0.00027659
Iteration 198/1000 | Loss: 0.00019740
Iteration 199/1000 | Loss: 0.00011301
Iteration 200/1000 | Loss: 0.00023882
Iteration 201/1000 | Loss: 0.00017917
Iteration 202/1000 | Loss: 0.00009728
Iteration 203/1000 | Loss: 0.00028287
Iteration 204/1000 | Loss: 0.00009331
Iteration 205/1000 | Loss: 0.00008138
Iteration 206/1000 | Loss: 0.00008824
Iteration 207/1000 | Loss: 0.00010635
Iteration 208/1000 | Loss: 0.00008667
Iteration 209/1000 | Loss: 0.00010164
Iteration 210/1000 | Loss: 0.00008670
Iteration 211/1000 | Loss: 0.00009320
Iteration 212/1000 | Loss: 0.00015886
Iteration 213/1000 | Loss: 0.00009571
Iteration 214/1000 | Loss: 0.00009374
Iteration 215/1000 | Loss: 0.00014062
Iteration 216/1000 | Loss: 0.00009760
Iteration 217/1000 | Loss: 0.00010095
Iteration 218/1000 | Loss: 0.00008296
Iteration 219/1000 | Loss: 0.00008126
Iteration 220/1000 | Loss: 0.00009445
Iteration 221/1000 | Loss: 0.00019617
Iteration 222/1000 | Loss: 0.00014023
Iteration 223/1000 | Loss: 0.00020560
Iteration 224/1000 | Loss: 0.00004977
Iteration 225/1000 | Loss: 0.00006763
Iteration 226/1000 | Loss: 0.00031731
Iteration 227/1000 | Loss: 0.00004746
Iteration 228/1000 | Loss: 0.00006658
Iteration 229/1000 | Loss: 0.00006316
Iteration 230/1000 | Loss: 0.00004215
Iteration 231/1000 | Loss: 0.00052589
Iteration 232/1000 | Loss: 0.00020426
Iteration 233/1000 | Loss: 0.00074324
Iteration 234/1000 | Loss: 0.00053005
Iteration 235/1000 | Loss: 0.00115546
Iteration 236/1000 | Loss: 0.00060998
Iteration 237/1000 | Loss: 0.00022448
Iteration 238/1000 | Loss: 0.00133102
Iteration 239/1000 | Loss: 0.00076739
Iteration 240/1000 | Loss: 0.00049162
Iteration 241/1000 | Loss: 0.00028069
Iteration 242/1000 | Loss: 0.00081293
Iteration 243/1000 | Loss: 0.00032045
Iteration 244/1000 | Loss: 0.00032063
Iteration 245/1000 | Loss: 0.00006532
Iteration 246/1000 | Loss: 0.00010259
Iteration 247/1000 | Loss: 0.00004930
Iteration 248/1000 | Loss: 0.00004501
Iteration 249/1000 | Loss: 0.00005636
Iteration 250/1000 | Loss: 0.00004131
Iteration 251/1000 | Loss: 0.00006289
Iteration 252/1000 | Loss: 0.00005402
Iteration 253/1000 | Loss: 0.00004007
Iteration 254/1000 | Loss: 0.00003971
Iteration 255/1000 | Loss: 0.00006268
Iteration 256/1000 | Loss: 0.00004650
Iteration 257/1000 | Loss: 0.00005676
Iteration 258/1000 | Loss: 0.00004056
Iteration 259/1000 | Loss: 0.00004647
Iteration 260/1000 | Loss: 0.00003898
Iteration 261/1000 | Loss: 0.00003872
Iteration 262/1000 | Loss: 0.00005860
Iteration 263/1000 | Loss: 0.00004799
Iteration 264/1000 | Loss: 0.00004575
Iteration 265/1000 | Loss: 0.00004801
Iteration 266/1000 | Loss: 0.00008268
Iteration 267/1000 | Loss: 0.00004297
Iteration 268/1000 | Loss: 0.00050849
Iteration 269/1000 | Loss: 0.00005333
Iteration 270/1000 | Loss: 0.00004028
Iteration 271/1000 | Loss: 0.00005975
Iteration 272/1000 | Loss: 0.00004198
Iteration 273/1000 | Loss: 0.00004731
Iteration 274/1000 | Loss: 0.00003623
Iteration 275/1000 | Loss: 0.00003542
Iteration 276/1000 | Loss: 0.00003705
Iteration 277/1000 | Loss: 0.00003501
Iteration 278/1000 | Loss: 0.00004610
Iteration 279/1000 | Loss: 0.00003496
Iteration 280/1000 | Loss: 0.00003490
Iteration 281/1000 | Loss: 0.00003487
Iteration 282/1000 | Loss: 0.00003487
Iteration 283/1000 | Loss: 0.00003487
Iteration 284/1000 | Loss: 0.00003486
Iteration 285/1000 | Loss: 0.00003486
Iteration 286/1000 | Loss: 0.00003486
Iteration 287/1000 | Loss: 0.00003485
Iteration 288/1000 | Loss: 0.00003484
Iteration 289/1000 | Loss: 0.00003484
Iteration 290/1000 | Loss: 0.00003483
Iteration 291/1000 | Loss: 0.00003483
Iteration 292/1000 | Loss: 0.00003483
Iteration 293/1000 | Loss: 0.00003483
Iteration 294/1000 | Loss: 0.00003482
Iteration 295/1000 | Loss: 0.00003482
Iteration 296/1000 | Loss: 0.00003482
Iteration 297/1000 | Loss: 0.00003481
Iteration 298/1000 | Loss: 0.00003481
Iteration 299/1000 | Loss: 0.00003480
Iteration 300/1000 | Loss: 0.00003480
Iteration 301/1000 | Loss: 0.00003480
Iteration 302/1000 | Loss: 0.00003479
Iteration 303/1000 | Loss: 0.00003479
Iteration 304/1000 | Loss: 0.00004585
Iteration 305/1000 | Loss: 0.00003480
Iteration 306/1000 | Loss: 0.00003476
Iteration 307/1000 | Loss: 0.00003471
Iteration 308/1000 | Loss: 0.00003471
Iteration 309/1000 | Loss: 0.00003471
Iteration 310/1000 | Loss: 0.00003471
Iteration 311/1000 | Loss: 0.00003471
Iteration 312/1000 | Loss: 0.00005323
Iteration 313/1000 | Loss: 0.00004012
Iteration 314/1000 | Loss: 0.00004591
Iteration 315/1000 | Loss: 0.00003468
Iteration 316/1000 | Loss: 0.00003467
Iteration 317/1000 | Loss: 0.00003467
Iteration 318/1000 | Loss: 0.00003467
Iteration 319/1000 | Loss: 0.00003467
Iteration 320/1000 | Loss: 0.00003467
Iteration 321/1000 | Loss: 0.00003467
Iteration 322/1000 | Loss: 0.00003467
Iteration 323/1000 | Loss: 0.00003466
Iteration 324/1000 | Loss: 0.00003466
Iteration 325/1000 | Loss: 0.00003465
Iteration 326/1000 | Loss: 0.00003465
Iteration 327/1000 | Loss: 0.00003465
Iteration 328/1000 | Loss: 0.00003465
Iteration 329/1000 | Loss: 0.00003465
Iteration 330/1000 | Loss: 0.00003464
Iteration 331/1000 | Loss: 0.00003464
Iteration 332/1000 | Loss: 0.00003464
Iteration 333/1000 | Loss: 0.00003464
Iteration 334/1000 | Loss: 0.00003464
Iteration 335/1000 | Loss: 0.00003464
Iteration 336/1000 | Loss: 0.00003464
Iteration 337/1000 | Loss: 0.00003464
Iteration 338/1000 | Loss: 0.00003464
Iteration 339/1000 | Loss: 0.00003464
Iteration 340/1000 | Loss: 0.00003464
Iteration 341/1000 | Loss: 0.00003463
Iteration 342/1000 | Loss: 0.00003463
Iteration 343/1000 | Loss: 0.00003463
Iteration 344/1000 | Loss: 0.00003463
Iteration 345/1000 | Loss: 0.00003463
Iteration 346/1000 | Loss: 0.00003462
Iteration 347/1000 | Loss: 0.00003461
Iteration 348/1000 | Loss: 0.00003461
Iteration 349/1000 | Loss: 0.00003461
Iteration 350/1000 | Loss: 0.00003461
Iteration 351/1000 | Loss: 0.00003461
Iteration 352/1000 | Loss: 0.00003460
Iteration 353/1000 | Loss: 0.00003460
Iteration 354/1000 | Loss: 0.00003460
Iteration 355/1000 | Loss: 0.00003460
Iteration 356/1000 | Loss: 0.00010196
Iteration 357/1000 | Loss: 0.00017438
Iteration 358/1000 | Loss: 0.00004889
Iteration 359/1000 | Loss: 0.00003516
Iteration 360/1000 | Loss: 0.00007405
Iteration 361/1000 | Loss: 0.00004818
Iteration 362/1000 | Loss: 0.00013277
Iteration 363/1000 | Loss: 0.00006343
Iteration 364/1000 | Loss: 0.00010542
Iteration 365/1000 | Loss: 0.00021963
Iteration 366/1000 | Loss: 0.00013253
Iteration 367/1000 | Loss: 0.00011035
Iteration 368/1000 | Loss: 0.00005521
Iteration 369/1000 | Loss: 0.00003564
Iteration 370/1000 | Loss: 0.00004438
Iteration 371/1000 | Loss: 0.00009072
Iteration 372/1000 | Loss: 0.00009594
Iteration 373/1000 | Loss: 0.00006898
Iteration 374/1000 | Loss: 0.00011358
Iteration 375/1000 | Loss: 0.00011424
Iteration 376/1000 | Loss: 0.00008269
Iteration 377/1000 | Loss: 0.00008708
Iteration 378/1000 | Loss: 0.00006578
Iteration 379/1000 | Loss: 0.00017430
Iteration 380/1000 | Loss: 0.00011337
Iteration 381/1000 | Loss: 0.00013987
Iteration 382/1000 | Loss: 0.00007014
Iteration 383/1000 | Loss: 0.00013110
Iteration 384/1000 | Loss: 0.00013332
Iteration 385/1000 | Loss: 0.00009152
Iteration 386/1000 | Loss: 0.00009344
Iteration 387/1000 | Loss: 0.00003650
Iteration 388/1000 | Loss: 0.00003502
Iteration 389/1000 | Loss: 0.00006155
Iteration 390/1000 | Loss: 0.00003445
Iteration 391/1000 | Loss: 0.00006088
Iteration 392/1000 | Loss: 0.00005609
Iteration 393/1000 | Loss: 0.00011043
Iteration 394/1000 | Loss: 0.00004242
Iteration 395/1000 | Loss: 0.00004752
Iteration 396/1000 | Loss: 0.00003411
Iteration 397/1000 | Loss: 0.00006837
Iteration 398/1000 | Loss: 0.00003403
Iteration 399/1000 | Loss: 0.00005755
Iteration 400/1000 | Loss: 0.00006181
Iteration 401/1000 | Loss: 0.00003391
Iteration 402/1000 | Loss: 0.00003391
Iteration 403/1000 | Loss: 0.00003391
Iteration 404/1000 | Loss: 0.00003391
Iteration 405/1000 | Loss: 0.00003390
Iteration 406/1000 | Loss: 0.00003390
Iteration 407/1000 | Loss: 0.00003390
Iteration 408/1000 | Loss: 0.00003390
Iteration 409/1000 | Loss: 0.00003390
Iteration 410/1000 | Loss: 0.00003390
Iteration 411/1000 | Loss: 0.00003390
Iteration 412/1000 | Loss: 0.00003389
Iteration 413/1000 | Loss: 0.00003389
Iteration 414/1000 | Loss: 0.00003389
Iteration 415/1000 | Loss: 0.00003389
Iteration 416/1000 | Loss: 0.00006442
Iteration 417/1000 | Loss: 0.00003391
Iteration 418/1000 | Loss: 0.00003389
Iteration 419/1000 | Loss: 0.00006516
Iteration 420/1000 | Loss: 0.00003654
Iteration 421/1000 | Loss: 0.00003937
Iteration 422/1000 | Loss: 0.00003384
Iteration 423/1000 | Loss: 0.00003382
Iteration 424/1000 | Loss: 0.00003381
Iteration 425/1000 | Loss: 0.00003381
Iteration 426/1000 | Loss: 0.00003380
Iteration 427/1000 | Loss: 0.00003380
Iteration 428/1000 | Loss: 0.00003380
Iteration 429/1000 | Loss: 0.00003578
Iteration 430/1000 | Loss: 0.00003377
Iteration 431/1000 | Loss: 0.00003377
Iteration 432/1000 | Loss: 0.00003377
Iteration 433/1000 | Loss: 0.00003377
Iteration 434/1000 | Loss: 0.00003377
Iteration 435/1000 | Loss: 0.00003377
Iteration 436/1000 | Loss: 0.00003377
Iteration 437/1000 | Loss: 0.00003377
Iteration 438/1000 | Loss: 0.00003376
Iteration 439/1000 | Loss: 0.00003376
Iteration 440/1000 | Loss: 0.00003376
Iteration 441/1000 | Loss: 0.00003376
Iteration 442/1000 | Loss: 0.00003376
Iteration 443/1000 | Loss: 0.00003376
Iteration 444/1000 | Loss: 0.00003376
Iteration 445/1000 | Loss: 0.00003376
Iteration 446/1000 | Loss: 0.00003376
Iteration 447/1000 | Loss: 0.00003376
Iteration 448/1000 | Loss: 0.00003376
Iteration 449/1000 | Loss: 0.00003376
Iteration 450/1000 | Loss: 0.00003376
Iteration 451/1000 | Loss: 0.00003376
Iteration 452/1000 | Loss: 0.00003376
Iteration 453/1000 | Loss: 0.00003376
Iteration 454/1000 | Loss: 0.00003376
Iteration 455/1000 | Loss: 0.00003376
Iteration 456/1000 | Loss: 0.00003376
Iteration 457/1000 | Loss: 0.00003376
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 457. Stopping optimization.
Last 5 losses: [3.375651067472063e-05, 3.375651067472063e-05, 3.375651067472063e-05, 3.375651067472063e-05, 3.375651067472063e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.375651067472063e-05

Optimization complete. Final v2v error: 4.605072975158691 mm

Highest mean error: 22.0975341796875 mm for frame 15

Lowest mean error: 3.8051555156707764 mm for frame 27

Saving results

Total time: 565.3544821739197
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_0196/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0196/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0196/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00836446
Iteration 2/25 | Loss: 0.00225701
Iteration 3/25 | Loss: 0.00195486
Iteration 4/25 | Loss: 0.00192257
Iteration 5/25 | Loss: 0.00185527
Iteration 6/25 | Loss: 0.00186939
Iteration 7/25 | Loss: 0.00182779
Iteration 8/25 | Loss: 0.00182193
Iteration 9/25 | Loss: 0.00187154
Iteration 10/25 | Loss: 0.00185909
Iteration 11/25 | Loss: 0.00179305
Iteration 12/25 | Loss: 0.00168969
Iteration 13/25 | Loss: 0.00165791
Iteration 14/25 | Loss: 0.00164741
Iteration 15/25 | Loss: 0.00164349
Iteration 16/25 | Loss: 0.00164297
Iteration 17/25 | Loss: 0.00164292
Iteration 18/25 | Loss: 0.00164292
Iteration 19/25 | Loss: 0.00164292
Iteration 20/25 | Loss: 0.00164292
Iteration 21/25 | Loss: 0.00164292
Iteration 22/25 | Loss: 0.00164292
Iteration 23/25 | Loss: 0.00164292
Iteration 24/25 | Loss: 0.00164292
Iteration 25/25 | Loss: 0.00164292

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.97407126
Iteration 2/25 | Loss: 0.00298793
Iteration 3/25 | Loss: 0.00298768
Iteration 4/25 | Loss: 0.00298768
Iteration 5/25 | Loss: 0.00298768
Iteration 6/25 | Loss: 0.00298768
Iteration 7/25 | Loss: 0.00298768
Iteration 8/25 | Loss: 0.00298768
Iteration 9/25 | Loss: 0.00298768
Iteration 10/25 | Loss: 0.00298768
Iteration 11/25 | Loss: 0.00298768
Iteration 12/25 | Loss: 0.00298768
Iteration 13/25 | Loss: 0.00298768
Iteration 14/25 | Loss: 0.00298768
Iteration 15/25 | Loss: 0.00298768
Iteration 16/25 | Loss: 0.00298768
Iteration 17/25 | Loss: 0.00298768
Iteration 18/25 | Loss: 0.00298768
Iteration 19/25 | Loss: 0.00298768
Iteration 20/25 | Loss: 0.00298768
Iteration 21/25 | Loss: 0.00298768
Iteration 22/25 | Loss: 0.00298768
Iteration 23/25 | Loss: 0.00298768
Iteration 24/25 | Loss: 0.00298768
Iteration 25/25 | Loss: 0.00298768

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00298768
Iteration 2/1000 | Loss: 0.00007525
Iteration 3/1000 | Loss: 0.00005849
Iteration 4/1000 | Loss: 0.00005273
Iteration 5/1000 | Loss: 0.00005033
Iteration 6/1000 | Loss: 0.00004812
Iteration 7/1000 | Loss: 0.00004672
Iteration 8/1000 | Loss: 0.00004560
Iteration 9/1000 | Loss: 0.00004474
Iteration 10/1000 | Loss: 0.00004419
Iteration 11/1000 | Loss: 0.00004379
Iteration 12/1000 | Loss: 0.00004348
Iteration 13/1000 | Loss: 0.00004313
Iteration 14/1000 | Loss: 0.00004292
Iteration 15/1000 | Loss: 0.00004274
Iteration 16/1000 | Loss: 0.00004266
Iteration 17/1000 | Loss: 0.00004259
Iteration 18/1000 | Loss: 0.00004258
Iteration 19/1000 | Loss: 0.00004257
Iteration 20/1000 | Loss: 0.00004251
Iteration 21/1000 | Loss: 0.00004250
Iteration 22/1000 | Loss: 0.00004249
Iteration 23/1000 | Loss: 0.00004248
Iteration 24/1000 | Loss: 0.00004248
Iteration 25/1000 | Loss: 0.00004247
Iteration 26/1000 | Loss: 0.00004247
Iteration 27/1000 | Loss: 0.00004246
Iteration 28/1000 | Loss: 0.00004246
Iteration 29/1000 | Loss: 0.00004245
Iteration 30/1000 | Loss: 0.00004245
Iteration 31/1000 | Loss: 0.00004244
Iteration 32/1000 | Loss: 0.00004244
Iteration 33/1000 | Loss: 0.00004243
Iteration 34/1000 | Loss: 0.00004243
Iteration 35/1000 | Loss: 0.00004243
Iteration 36/1000 | Loss: 0.00004242
Iteration 37/1000 | Loss: 0.00004242
Iteration 38/1000 | Loss: 0.00004242
Iteration 39/1000 | Loss: 0.00004241
Iteration 40/1000 | Loss: 0.00004241
Iteration 41/1000 | Loss: 0.00004240
Iteration 42/1000 | Loss: 0.00004240
Iteration 43/1000 | Loss: 0.00004240
Iteration 44/1000 | Loss: 0.00004240
Iteration 45/1000 | Loss: 0.00004240
Iteration 46/1000 | Loss: 0.00004240
Iteration 47/1000 | Loss: 0.00004240
Iteration 48/1000 | Loss: 0.00004238
Iteration 49/1000 | Loss: 0.00004238
Iteration 50/1000 | Loss: 0.00004237
Iteration 51/1000 | Loss: 0.00004237
Iteration 52/1000 | Loss: 0.00004237
Iteration 53/1000 | Loss: 0.00004236
Iteration 54/1000 | Loss: 0.00004236
Iteration 55/1000 | Loss: 0.00004236
Iteration 56/1000 | Loss: 0.00004235
Iteration 57/1000 | Loss: 0.00004235
Iteration 58/1000 | Loss: 0.00004235
Iteration 59/1000 | Loss: 0.00004235
Iteration 60/1000 | Loss: 0.00004235
Iteration 61/1000 | Loss: 0.00004235
Iteration 62/1000 | Loss: 0.00004234
Iteration 63/1000 | Loss: 0.00004234
Iteration 64/1000 | Loss: 0.00004234
Iteration 65/1000 | Loss: 0.00004234
Iteration 66/1000 | Loss: 0.00004234
Iteration 67/1000 | Loss: 0.00004233
Iteration 68/1000 | Loss: 0.00004233
Iteration 69/1000 | Loss: 0.00004233
Iteration 70/1000 | Loss: 0.00004233
Iteration 71/1000 | Loss: 0.00004233
Iteration 72/1000 | Loss: 0.00004233
Iteration 73/1000 | Loss: 0.00004233
Iteration 74/1000 | Loss: 0.00004232
Iteration 75/1000 | Loss: 0.00004232
Iteration 76/1000 | Loss: 0.00004232
Iteration 77/1000 | Loss: 0.00004232
Iteration 78/1000 | Loss: 0.00004231
Iteration 79/1000 | Loss: 0.00004231
Iteration 80/1000 | Loss: 0.00004231
Iteration 81/1000 | Loss: 0.00004230
Iteration 82/1000 | Loss: 0.00004230
Iteration 83/1000 | Loss: 0.00004230
Iteration 84/1000 | Loss: 0.00004230
Iteration 85/1000 | Loss: 0.00004230
Iteration 86/1000 | Loss: 0.00004229
Iteration 87/1000 | Loss: 0.00004229
Iteration 88/1000 | Loss: 0.00004229
Iteration 89/1000 | Loss: 0.00004229
Iteration 90/1000 | Loss: 0.00004229
Iteration 91/1000 | Loss: 0.00004229
Iteration 92/1000 | Loss: 0.00004228
Iteration 93/1000 | Loss: 0.00004228
Iteration 94/1000 | Loss: 0.00004228
Iteration 95/1000 | Loss: 0.00004228
Iteration 96/1000 | Loss: 0.00004227
Iteration 97/1000 | Loss: 0.00004227
Iteration 98/1000 | Loss: 0.00004227
Iteration 99/1000 | Loss: 0.00004227
Iteration 100/1000 | Loss: 0.00004226
Iteration 101/1000 | Loss: 0.00004226
Iteration 102/1000 | Loss: 0.00004226
Iteration 103/1000 | Loss: 0.00004226
Iteration 104/1000 | Loss: 0.00004226
Iteration 105/1000 | Loss: 0.00004225
Iteration 106/1000 | Loss: 0.00004225
Iteration 107/1000 | Loss: 0.00004225
Iteration 108/1000 | Loss: 0.00004225
Iteration 109/1000 | Loss: 0.00004224
Iteration 110/1000 | Loss: 0.00004224
Iteration 111/1000 | Loss: 0.00004224
Iteration 112/1000 | Loss: 0.00004224
Iteration 113/1000 | Loss: 0.00004224
Iteration 114/1000 | Loss: 0.00004223
Iteration 115/1000 | Loss: 0.00004223
Iteration 116/1000 | Loss: 0.00004223
Iteration 117/1000 | Loss: 0.00004223
Iteration 118/1000 | Loss: 0.00004223
Iteration 119/1000 | Loss: 0.00004223
Iteration 120/1000 | Loss: 0.00004223
Iteration 121/1000 | Loss: 0.00004223
Iteration 122/1000 | Loss: 0.00004223
Iteration 123/1000 | Loss: 0.00004223
Iteration 124/1000 | Loss: 0.00004223
Iteration 125/1000 | Loss: 0.00004222
Iteration 126/1000 | Loss: 0.00004222
Iteration 127/1000 | Loss: 0.00004222
Iteration 128/1000 | Loss: 0.00004222
Iteration 129/1000 | Loss: 0.00004221
Iteration 130/1000 | Loss: 0.00004221
Iteration 131/1000 | Loss: 0.00004221
Iteration 132/1000 | Loss: 0.00004221
Iteration 133/1000 | Loss: 0.00004221
Iteration 134/1000 | Loss: 0.00004220
Iteration 135/1000 | Loss: 0.00004220
Iteration 136/1000 | Loss: 0.00004220
Iteration 137/1000 | Loss: 0.00004219
Iteration 138/1000 | Loss: 0.00004219
Iteration 139/1000 | Loss: 0.00004219
Iteration 140/1000 | Loss: 0.00004219
Iteration 141/1000 | Loss: 0.00004219
Iteration 142/1000 | Loss: 0.00004218
Iteration 143/1000 | Loss: 0.00004218
Iteration 144/1000 | Loss: 0.00004218
Iteration 145/1000 | Loss: 0.00004218
Iteration 146/1000 | Loss: 0.00004218
Iteration 147/1000 | Loss: 0.00004217
Iteration 148/1000 | Loss: 0.00004217
Iteration 149/1000 | Loss: 0.00004217
Iteration 150/1000 | Loss: 0.00004217
Iteration 151/1000 | Loss: 0.00004217
Iteration 152/1000 | Loss: 0.00004216
Iteration 153/1000 | Loss: 0.00004216
Iteration 154/1000 | Loss: 0.00004216
Iteration 155/1000 | Loss: 0.00004216
Iteration 156/1000 | Loss: 0.00004216
Iteration 157/1000 | Loss: 0.00004216
Iteration 158/1000 | Loss: 0.00004216
Iteration 159/1000 | Loss: 0.00004216
Iteration 160/1000 | Loss: 0.00004215
Iteration 161/1000 | Loss: 0.00004215
Iteration 162/1000 | Loss: 0.00004215
Iteration 163/1000 | Loss: 0.00004215
Iteration 164/1000 | Loss: 0.00004215
Iteration 165/1000 | Loss: 0.00004215
Iteration 166/1000 | Loss: 0.00004214
Iteration 167/1000 | Loss: 0.00004214
Iteration 168/1000 | Loss: 0.00004214
Iteration 169/1000 | Loss: 0.00004214
Iteration 170/1000 | Loss: 0.00004214
Iteration 171/1000 | Loss: 0.00004213
Iteration 172/1000 | Loss: 0.00004213
Iteration 173/1000 | Loss: 0.00004213
Iteration 174/1000 | Loss: 0.00004213
Iteration 175/1000 | Loss: 0.00004213
Iteration 176/1000 | Loss: 0.00004213
Iteration 177/1000 | Loss: 0.00004213
Iteration 178/1000 | Loss: 0.00004213
Iteration 179/1000 | Loss: 0.00004213
Iteration 180/1000 | Loss: 0.00004213
Iteration 181/1000 | Loss: 0.00004213
Iteration 182/1000 | Loss: 0.00004212
Iteration 183/1000 | Loss: 0.00004212
Iteration 184/1000 | Loss: 0.00004212
Iteration 185/1000 | Loss: 0.00004212
Iteration 186/1000 | Loss: 0.00004212
Iteration 187/1000 | Loss: 0.00004212
Iteration 188/1000 | Loss: 0.00004212
Iteration 189/1000 | Loss: 0.00004212
Iteration 190/1000 | Loss: 0.00004212
Iteration 191/1000 | Loss: 0.00004212
Iteration 192/1000 | Loss: 0.00004212
Iteration 193/1000 | Loss: 0.00004212
Iteration 194/1000 | Loss: 0.00004212
Iteration 195/1000 | Loss: 0.00004212
Iteration 196/1000 | Loss: 0.00004212
Iteration 197/1000 | Loss: 0.00004212
Iteration 198/1000 | Loss: 0.00004212
Iteration 199/1000 | Loss: 0.00004212
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [4.2124720494030043e-05, 4.2124720494030043e-05, 4.2124720494030043e-05, 4.2124720494030043e-05, 4.2124720494030043e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.2124720494030043e-05

Optimization complete. Final v2v error: 5.376796245574951 mm

Highest mean error: 7.718844413757324 mm for frame 177

Lowest mean error: 4.0411200523376465 mm for frame 207

Saving results

Total time: 75.40117883682251
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_0196/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0196/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0196/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00693577
Iteration 2/25 | Loss: 0.00169030
Iteration 3/25 | Loss: 0.00157552
Iteration 4/25 | Loss: 0.00155485
Iteration 5/25 | Loss: 0.00155043
Iteration 6/25 | Loss: 0.00154950
Iteration 7/25 | Loss: 0.00154950
Iteration 8/25 | Loss: 0.00154950
Iteration 9/25 | Loss: 0.00154950
Iteration 10/25 | Loss: 0.00154950
Iteration 11/25 | Loss: 0.00154950
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001549502369016409, 0.001549502369016409, 0.001549502369016409, 0.001549502369016409, 0.001549502369016409]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001549502369016409

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.03088760
Iteration 2/25 | Loss: 0.00273300
Iteration 3/25 | Loss: 0.00273300
Iteration 4/25 | Loss: 0.00273300
Iteration 5/25 | Loss: 0.00273300
Iteration 6/25 | Loss: 0.00273300
Iteration 7/25 | Loss: 0.00273300
Iteration 8/25 | Loss: 0.00273300
Iteration 9/25 | Loss: 0.00273300
Iteration 10/25 | Loss: 0.00273300
Iteration 11/25 | Loss: 0.00273300
Iteration 12/25 | Loss: 0.00273300
Iteration 13/25 | Loss: 0.00273300
Iteration 14/25 | Loss: 0.00273300
Iteration 15/25 | Loss: 0.00273300
Iteration 16/25 | Loss: 0.00273300
Iteration 17/25 | Loss: 0.00273300
Iteration 18/25 | Loss: 0.00273300
Iteration 19/25 | Loss: 0.00273300
Iteration 20/25 | Loss: 0.00273300
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.002732999622821808, 0.002732999622821808, 0.002732999622821808, 0.002732999622821808, 0.002732999622821808]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002732999622821808

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00273300
Iteration 2/1000 | Loss: 0.00004736
Iteration 3/1000 | Loss: 0.00003746
Iteration 4/1000 | Loss: 0.00003363
Iteration 5/1000 | Loss: 0.00003153
Iteration 6/1000 | Loss: 0.00002987
Iteration 7/1000 | Loss: 0.00002871
Iteration 8/1000 | Loss: 0.00002780
Iteration 9/1000 | Loss: 0.00002723
Iteration 10/1000 | Loss: 0.00002687
Iteration 11/1000 | Loss: 0.00002655
Iteration 12/1000 | Loss: 0.00002639
Iteration 13/1000 | Loss: 0.00002637
Iteration 14/1000 | Loss: 0.00002635
Iteration 15/1000 | Loss: 0.00002634
Iteration 16/1000 | Loss: 0.00002633
Iteration 17/1000 | Loss: 0.00002633
Iteration 18/1000 | Loss: 0.00002632
Iteration 19/1000 | Loss: 0.00002632
Iteration 20/1000 | Loss: 0.00002631
Iteration 21/1000 | Loss: 0.00002630
Iteration 22/1000 | Loss: 0.00002626
Iteration 23/1000 | Loss: 0.00002626
Iteration 24/1000 | Loss: 0.00002626
Iteration 25/1000 | Loss: 0.00002625
Iteration 26/1000 | Loss: 0.00002625
Iteration 27/1000 | Loss: 0.00002624
Iteration 28/1000 | Loss: 0.00002624
Iteration 29/1000 | Loss: 0.00002622
Iteration 30/1000 | Loss: 0.00002620
Iteration 31/1000 | Loss: 0.00002619
Iteration 32/1000 | Loss: 0.00002619
Iteration 33/1000 | Loss: 0.00002619
Iteration 34/1000 | Loss: 0.00002619
Iteration 35/1000 | Loss: 0.00002618
Iteration 36/1000 | Loss: 0.00002618
Iteration 37/1000 | Loss: 0.00002618
Iteration 38/1000 | Loss: 0.00002617
Iteration 39/1000 | Loss: 0.00002616
Iteration 40/1000 | Loss: 0.00002615
Iteration 41/1000 | Loss: 0.00002614
Iteration 42/1000 | Loss: 0.00002614
Iteration 43/1000 | Loss: 0.00002614
Iteration 44/1000 | Loss: 0.00002614
Iteration 45/1000 | Loss: 0.00002614
Iteration 46/1000 | Loss: 0.00002613
Iteration 47/1000 | Loss: 0.00002612
Iteration 48/1000 | Loss: 0.00002612
Iteration 49/1000 | Loss: 0.00002611
Iteration 50/1000 | Loss: 0.00002611
Iteration 51/1000 | Loss: 0.00002610
Iteration 52/1000 | Loss: 0.00002610
Iteration 53/1000 | Loss: 0.00002610
Iteration 54/1000 | Loss: 0.00002610
Iteration 55/1000 | Loss: 0.00002609
Iteration 56/1000 | Loss: 0.00002609
Iteration 57/1000 | Loss: 0.00002608
Iteration 58/1000 | Loss: 0.00002607
Iteration 59/1000 | Loss: 0.00002607
Iteration 60/1000 | Loss: 0.00002607
Iteration 61/1000 | Loss: 0.00002606
Iteration 62/1000 | Loss: 0.00002606
Iteration 63/1000 | Loss: 0.00002606
Iteration 64/1000 | Loss: 0.00002606
Iteration 65/1000 | Loss: 0.00002605
Iteration 66/1000 | Loss: 0.00002605
Iteration 67/1000 | Loss: 0.00002604
Iteration 68/1000 | Loss: 0.00002603
Iteration 69/1000 | Loss: 0.00002603
Iteration 70/1000 | Loss: 0.00002603
Iteration 71/1000 | Loss: 0.00002602
Iteration 72/1000 | Loss: 0.00002602
Iteration 73/1000 | Loss: 0.00002602
Iteration 74/1000 | Loss: 0.00002602
Iteration 75/1000 | Loss: 0.00002601
Iteration 76/1000 | Loss: 0.00002601
Iteration 77/1000 | Loss: 0.00002601
Iteration 78/1000 | Loss: 0.00002601
Iteration 79/1000 | Loss: 0.00002600
Iteration 80/1000 | Loss: 0.00002600
Iteration 81/1000 | Loss: 0.00002600
Iteration 82/1000 | Loss: 0.00002599
Iteration 83/1000 | Loss: 0.00002599
Iteration 84/1000 | Loss: 0.00002599
Iteration 85/1000 | Loss: 0.00002599
Iteration 86/1000 | Loss: 0.00002599
Iteration 87/1000 | Loss: 0.00002599
Iteration 88/1000 | Loss: 0.00002599
Iteration 89/1000 | Loss: 0.00002599
Iteration 90/1000 | Loss: 0.00002599
Iteration 91/1000 | Loss: 0.00002599
Iteration 92/1000 | Loss: 0.00002599
Iteration 93/1000 | Loss: 0.00002598
Iteration 94/1000 | Loss: 0.00002598
Iteration 95/1000 | Loss: 0.00002598
Iteration 96/1000 | Loss: 0.00002598
Iteration 97/1000 | Loss: 0.00002598
Iteration 98/1000 | Loss: 0.00002598
Iteration 99/1000 | Loss: 0.00002597
Iteration 100/1000 | Loss: 0.00002597
Iteration 101/1000 | Loss: 0.00002597
Iteration 102/1000 | Loss: 0.00002596
Iteration 103/1000 | Loss: 0.00002596
Iteration 104/1000 | Loss: 0.00002596
Iteration 105/1000 | Loss: 0.00002596
Iteration 106/1000 | Loss: 0.00002596
Iteration 107/1000 | Loss: 0.00002596
Iteration 108/1000 | Loss: 0.00002596
Iteration 109/1000 | Loss: 0.00002596
Iteration 110/1000 | Loss: 0.00002596
Iteration 111/1000 | Loss: 0.00002595
Iteration 112/1000 | Loss: 0.00002595
Iteration 113/1000 | Loss: 0.00002595
Iteration 114/1000 | Loss: 0.00002595
Iteration 115/1000 | Loss: 0.00002595
Iteration 116/1000 | Loss: 0.00002595
Iteration 117/1000 | Loss: 0.00002595
Iteration 118/1000 | Loss: 0.00002594
Iteration 119/1000 | Loss: 0.00002594
Iteration 120/1000 | Loss: 0.00002594
Iteration 121/1000 | Loss: 0.00002593
Iteration 122/1000 | Loss: 0.00002593
Iteration 123/1000 | Loss: 0.00002593
Iteration 124/1000 | Loss: 0.00002593
Iteration 125/1000 | Loss: 0.00002593
Iteration 126/1000 | Loss: 0.00002592
Iteration 127/1000 | Loss: 0.00002592
Iteration 128/1000 | Loss: 0.00002592
Iteration 129/1000 | Loss: 0.00002592
Iteration 130/1000 | Loss: 0.00002591
Iteration 131/1000 | Loss: 0.00002591
Iteration 132/1000 | Loss: 0.00002591
Iteration 133/1000 | Loss: 0.00002591
Iteration 134/1000 | Loss: 0.00002591
Iteration 135/1000 | Loss: 0.00002591
Iteration 136/1000 | Loss: 0.00002590
Iteration 137/1000 | Loss: 0.00002590
Iteration 138/1000 | Loss: 0.00002590
Iteration 139/1000 | Loss: 0.00002590
Iteration 140/1000 | Loss: 0.00002590
Iteration 141/1000 | Loss: 0.00002590
Iteration 142/1000 | Loss: 0.00002590
Iteration 143/1000 | Loss: 0.00002589
Iteration 144/1000 | Loss: 0.00002589
Iteration 145/1000 | Loss: 0.00002589
Iteration 146/1000 | Loss: 0.00002589
Iteration 147/1000 | Loss: 0.00002588
Iteration 148/1000 | Loss: 0.00002588
Iteration 149/1000 | Loss: 0.00002588
Iteration 150/1000 | Loss: 0.00002588
Iteration 151/1000 | Loss: 0.00002588
Iteration 152/1000 | Loss: 0.00002588
Iteration 153/1000 | Loss: 0.00002588
Iteration 154/1000 | Loss: 0.00002588
Iteration 155/1000 | Loss: 0.00002587
Iteration 156/1000 | Loss: 0.00002587
Iteration 157/1000 | Loss: 0.00002587
Iteration 158/1000 | Loss: 0.00002587
Iteration 159/1000 | Loss: 0.00002587
Iteration 160/1000 | Loss: 0.00002587
Iteration 161/1000 | Loss: 0.00002587
Iteration 162/1000 | Loss: 0.00002587
Iteration 163/1000 | Loss: 0.00002587
Iteration 164/1000 | Loss: 0.00002587
Iteration 165/1000 | Loss: 0.00002587
Iteration 166/1000 | Loss: 0.00002587
Iteration 167/1000 | Loss: 0.00002587
Iteration 168/1000 | Loss: 0.00002586
Iteration 169/1000 | Loss: 0.00002586
Iteration 170/1000 | Loss: 0.00002586
Iteration 171/1000 | Loss: 0.00002586
Iteration 172/1000 | Loss: 0.00002586
Iteration 173/1000 | Loss: 0.00002586
Iteration 174/1000 | Loss: 0.00002586
Iteration 175/1000 | Loss: 0.00002586
Iteration 176/1000 | Loss: 0.00002586
Iteration 177/1000 | Loss: 0.00002586
Iteration 178/1000 | Loss: 0.00002586
Iteration 179/1000 | Loss: 0.00002586
Iteration 180/1000 | Loss: 0.00002586
Iteration 181/1000 | Loss: 0.00002586
Iteration 182/1000 | Loss: 0.00002586
Iteration 183/1000 | Loss: 0.00002586
Iteration 184/1000 | Loss: 0.00002586
Iteration 185/1000 | Loss: 0.00002586
Iteration 186/1000 | Loss: 0.00002585
Iteration 187/1000 | Loss: 0.00002585
Iteration 188/1000 | Loss: 0.00002585
Iteration 189/1000 | Loss: 0.00002585
Iteration 190/1000 | Loss: 0.00002585
Iteration 191/1000 | Loss: 0.00002585
Iteration 192/1000 | Loss: 0.00002585
Iteration 193/1000 | Loss: 0.00002585
Iteration 194/1000 | Loss: 0.00002585
Iteration 195/1000 | Loss: 0.00002585
Iteration 196/1000 | Loss: 0.00002585
Iteration 197/1000 | Loss: 0.00002585
Iteration 198/1000 | Loss: 0.00002585
Iteration 199/1000 | Loss: 0.00002585
Iteration 200/1000 | Loss: 0.00002585
Iteration 201/1000 | Loss: 0.00002585
Iteration 202/1000 | Loss: 0.00002585
Iteration 203/1000 | Loss: 0.00002584
Iteration 204/1000 | Loss: 0.00002584
Iteration 205/1000 | Loss: 0.00002584
Iteration 206/1000 | Loss: 0.00002584
Iteration 207/1000 | Loss: 0.00002584
Iteration 208/1000 | Loss: 0.00002584
Iteration 209/1000 | Loss: 0.00002584
Iteration 210/1000 | Loss: 0.00002584
Iteration 211/1000 | Loss: 0.00002584
Iteration 212/1000 | Loss: 0.00002584
Iteration 213/1000 | Loss: 0.00002584
Iteration 214/1000 | Loss: 0.00002584
Iteration 215/1000 | Loss: 0.00002584
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [2.5843470211839303e-05, 2.5843470211839303e-05, 2.5843470211839303e-05, 2.5843470211839303e-05, 2.5843470211839303e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5843470211839303e-05

Optimization complete. Final v2v error: 4.402400970458984 mm

Highest mean error: 4.761615753173828 mm for frame 187

Lowest mean error: 4.03013801574707 mm for frame 0

Saving results

Total time: 46.45332360267639
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_0196/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0196/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0196/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01029437
Iteration 2/25 | Loss: 0.00187638
Iteration 3/25 | Loss: 0.00163553
Iteration 4/25 | Loss: 0.00159764
Iteration 5/25 | Loss: 0.00159038
Iteration 6/25 | Loss: 0.00158974
Iteration 7/25 | Loss: 0.00158974
Iteration 8/25 | Loss: 0.00158974
Iteration 9/25 | Loss: 0.00158974
Iteration 10/25 | Loss: 0.00158974
Iteration 11/25 | Loss: 0.00158974
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0015897356206551194, 0.0015897356206551194, 0.0015897356206551194, 0.0015897356206551194, 0.0015897356206551194]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015897356206551194

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49539852
Iteration 2/25 | Loss: 0.00222733
Iteration 3/25 | Loss: 0.00222723
Iteration 4/25 | Loss: 0.00222723
Iteration 5/25 | Loss: 0.00222723
Iteration 6/25 | Loss: 0.00222723
Iteration 7/25 | Loss: 0.00222722
Iteration 8/25 | Loss: 0.00222722
Iteration 9/25 | Loss: 0.00222722
Iteration 10/25 | Loss: 0.00222722
Iteration 11/25 | Loss: 0.00222722
Iteration 12/25 | Loss: 0.00222722
Iteration 13/25 | Loss: 0.00222722
Iteration 14/25 | Loss: 0.00222722
Iteration 15/25 | Loss: 0.00222722
Iteration 16/25 | Loss: 0.00222722
Iteration 17/25 | Loss: 0.00222722
Iteration 18/25 | Loss: 0.00222722
Iteration 19/25 | Loss: 0.00222722
Iteration 20/25 | Loss: 0.00222722
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0022272239439189434, 0.0022272239439189434, 0.0022272239439189434, 0.0022272239439189434, 0.0022272239439189434]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022272239439189434

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00222722
Iteration 2/1000 | Loss: 0.00004968
Iteration 3/1000 | Loss: 0.00004446
Iteration 4/1000 | Loss: 0.00004085
Iteration 5/1000 | Loss: 0.00003902
Iteration 6/1000 | Loss: 0.00003717
Iteration 7/1000 | Loss: 0.00003594
Iteration 8/1000 | Loss: 0.00003515
Iteration 9/1000 | Loss: 0.00003482
Iteration 10/1000 | Loss: 0.00003451
Iteration 11/1000 | Loss: 0.00003426
Iteration 12/1000 | Loss: 0.00003421
Iteration 13/1000 | Loss: 0.00003418
Iteration 14/1000 | Loss: 0.00003417
Iteration 15/1000 | Loss: 0.00003416
Iteration 16/1000 | Loss: 0.00003416
Iteration 17/1000 | Loss: 0.00003416
Iteration 18/1000 | Loss: 0.00003416
Iteration 19/1000 | Loss: 0.00003415
Iteration 20/1000 | Loss: 0.00003415
Iteration 21/1000 | Loss: 0.00003413
Iteration 22/1000 | Loss: 0.00003413
Iteration 23/1000 | Loss: 0.00003413
Iteration 24/1000 | Loss: 0.00003412
Iteration 25/1000 | Loss: 0.00003412
Iteration 26/1000 | Loss: 0.00003411
Iteration 27/1000 | Loss: 0.00003411
Iteration 28/1000 | Loss: 0.00003411
Iteration 29/1000 | Loss: 0.00003410
Iteration 30/1000 | Loss: 0.00003410
Iteration 31/1000 | Loss: 0.00003409
Iteration 32/1000 | Loss: 0.00003409
Iteration 33/1000 | Loss: 0.00003409
Iteration 34/1000 | Loss: 0.00003409
Iteration 35/1000 | Loss: 0.00003409
Iteration 36/1000 | Loss: 0.00003409
Iteration 37/1000 | Loss: 0.00003408
Iteration 38/1000 | Loss: 0.00003408
Iteration 39/1000 | Loss: 0.00003408
Iteration 40/1000 | Loss: 0.00003407
Iteration 41/1000 | Loss: 0.00003407
Iteration 42/1000 | Loss: 0.00003407
Iteration 43/1000 | Loss: 0.00003406
Iteration 44/1000 | Loss: 0.00003406
Iteration 45/1000 | Loss: 0.00003404
Iteration 46/1000 | Loss: 0.00003404
Iteration 47/1000 | Loss: 0.00003404
Iteration 48/1000 | Loss: 0.00003404
Iteration 49/1000 | Loss: 0.00003404
Iteration 50/1000 | Loss: 0.00003404
Iteration 51/1000 | Loss: 0.00003404
Iteration 52/1000 | Loss: 0.00003404
Iteration 53/1000 | Loss: 0.00003403
Iteration 54/1000 | Loss: 0.00003403
Iteration 55/1000 | Loss: 0.00003403
Iteration 56/1000 | Loss: 0.00003403
Iteration 57/1000 | Loss: 0.00003403
Iteration 58/1000 | Loss: 0.00003403
Iteration 59/1000 | Loss: 0.00003403
Iteration 60/1000 | Loss: 0.00003403
Iteration 61/1000 | Loss: 0.00003403
Iteration 62/1000 | Loss: 0.00003403
Iteration 63/1000 | Loss: 0.00003403
Iteration 64/1000 | Loss: 0.00003403
Iteration 65/1000 | Loss: 0.00003402
Iteration 66/1000 | Loss: 0.00003402
Iteration 67/1000 | Loss: 0.00003402
Iteration 68/1000 | Loss: 0.00003402
Iteration 69/1000 | Loss: 0.00003402
Iteration 70/1000 | Loss: 0.00003402
Iteration 71/1000 | Loss: 0.00003402
Iteration 72/1000 | Loss: 0.00003402
Iteration 73/1000 | Loss: 0.00003401
Iteration 74/1000 | Loss: 0.00003401
Iteration 75/1000 | Loss: 0.00003401
Iteration 76/1000 | Loss: 0.00003401
Iteration 77/1000 | Loss: 0.00003401
Iteration 78/1000 | Loss: 0.00003401
Iteration 79/1000 | Loss: 0.00003401
Iteration 80/1000 | Loss: 0.00003401
Iteration 81/1000 | Loss: 0.00003401
Iteration 82/1000 | Loss: 0.00003400
Iteration 83/1000 | Loss: 0.00003400
Iteration 84/1000 | Loss: 0.00003400
Iteration 85/1000 | Loss: 0.00003400
Iteration 86/1000 | Loss: 0.00003400
Iteration 87/1000 | Loss: 0.00003400
Iteration 88/1000 | Loss: 0.00003400
Iteration 89/1000 | Loss: 0.00003400
Iteration 90/1000 | Loss: 0.00003400
Iteration 91/1000 | Loss: 0.00003400
Iteration 92/1000 | Loss: 0.00003399
Iteration 93/1000 | Loss: 0.00003399
Iteration 94/1000 | Loss: 0.00003399
Iteration 95/1000 | Loss: 0.00003399
Iteration 96/1000 | Loss: 0.00003399
Iteration 97/1000 | Loss: 0.00003398
Iteration 98/1000 | Loss: 0.00003398
Iteration 99/1000 | Loss: 0.00003398
Iteration 100/1000 | Loss: 0.00003398
Iteration 101/1000 | Loss: 0.00003398
Iteration 102/1000 | Loss: 0.00003398
Iteration 103/1000 | Loss: 0.00003397
Iteration 104/1000 | Loss: 0.00003397
Iteration 105/1000 | Loss: 0.00003397
Iteration 106/1000 | Loss: 0.00003397
Iteration 107/1000 | Loss: 0.00003397
Iteration 108/1000 | Loss: 0.00003397
Iteration 109/1000 | Loss: 0.00003397
Iteration 110/1000 | Loss: 0.00003397
Iteration 111/1000 | Loss: 0.00003397
Iteration 112/1000 | Loss: 0.00003397
Iteration 113/1000 | Loss: 0.00003397
Iteration 114/1000 | Loss: 0.00003397
Iteration 115/1000 | Loss: 0.00003397
Iteration 116/1000 | Loss: 0.00003397
Iteration 117/1000 | Loss: 0.00003397
Iteration 118/1000 | Loss: 0.00003396
Iteration 119/1000 | Loss: 0.00003396
Iteration 120/1000 | Loss: 0.00003396
Iteration 121/1000 | Loss: 0.00003396
Iteration 122/1000 | Loss: 0.00003396
Iteration 123/1000 | Loss: 0.00003396
Iteration 124/1000 | Loss: 0.00003396
Iteration 125/1000 | Loss: 0.00003396
Iteration 126/1000 | Loss: 0.00003396
Iteration 127/1000 | Loss: 0.00003396
Iteration 128/1000 | Loss: 0.00003396
Iteration 129/1000 | Loss: 0.00003396
Iteration 130/1000 | Loss: 0.00003396
Iteration 131/1000 | Loss: 0.00003396
Iteration 132/1000 | Loss: 0.00003396
Iteration 133/1000 | Loss: 0.00003396
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [3.395599196664989e-05, 3.395599196664989e-05, 3.395599196664989e-05, 3.395599196664989e-05, 3.395599196664989e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.395599196664989e-05

Optimization complete. Final v2v error: 5.015338897705078 mm

Highest mean error: 5.1580328941345215 mm for frame 47

Lowest mean error: 4.890357971191406 mm for frame 218

Saving results

Total time: 37.460846185684204
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_0196/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0196/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0196/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00935301
Iteration 2/25 | Loss: 0.00219018
Iteration 3/25 | Loss: 0.00172022
Iteration 4/25 | Loss: 0.00163723
Iteration 5/25 | Loss: 0.00156931
Iteration 6/25 | Loss: 0.00154826
Iteration 7/25 | Loss: 0.00154212
Iteration 8/25 | Loss: 0.00154061
Iteration 9/25 | Loss: 0.00154017
Iteration 10/25 | Loss: 0.00153999
Iteration 11/25 | Loss: 0.00153997
Iteration 12/25 | Loss: 0.00153997
Iteration 13/25 | Loss: 0.00153997
Iteration 14/25 | Loss: 0.00153996
Iteration 15/25 | Loss: 0.00153996
Iteration 16/25 | Loss: 0.00153996
Iteration 17/25 | Loss: 0.00153996
Iteration 18/25 | Loss: 0.00153996
Iteration 19/25 | Loss: 0.00153996
Iteration 20/25 | Loss: 0.00153996
Iteration 21/25 | Loss: 0.00153996
Iteration 22/25 | Loss: 0.00153996
Iteration 23/25 | Loss: 0.00153996
Iteration 24/25 | Loss: 0.00153996
Iteration 25/25 | Loss: 0.00153996

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56502509
Iteration 2/25 | Loss: 0.00247800
Iteration 3/25 | Loss: 0.00247798
Iteration 4/25 | Loss: 0.00247798
Iteration 5/25 | Loss: 0.00247798
Iteration 6/25 | Loss: 0.00247798
Iteration 7/25 | Loss: 0.00247798
Iteration 8/25 | Loss: 0.00247798
Iteration 9/25 | Loss: 0.00247798
Iteration 10/25 | Loss: 0.00247797
Iteration 11/25 | Loss: 0.00247797
Iteration 12/25 | Loss: 0.00247797
Iteration 13/25 | Loss: 0.00247797
Iteration 14/25 | Loss: 0.00247797
Iteration 15/25 | Loss: 0.00247797
Iteration 16/25 | Loss: 0.00247797
Iteration 17/25 | Loss: 0.00247797
Iteration 18/25 | Loss: 0.00247797
Iteration 19/25 | Loss: 0.00247797
Iteration 20/25 | Loss: 0.00247797
Iteration 21/25 | Loss: 0.00247797
Iteration 22/25 | Loss: 0.00247797
Iteration 23/25 | Loss: 0.00247797
Iteration 24/25 | Loss: 0.00247797
Iteration 25/25 | Loss: 0.00247797

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00247797
Iteration 2/1000 | Loss: 0.00004018
Iteration 3/1000 | Loss: 0.00003126
Iteration 4/1000 | Loss: 0.00002910
Iteration 5/1000 | Loss: 0.00002766
Iteration 6/1000 | Loss: 0.00002694
Iteration 7/1000 | Loss: 0.00002650
Iteration 8/1000 | Loss: 0.00002623
Iteration 9/1000 | Loss: 0.00002606
Iteration 10/1000 | Loss: 0.00002596
Iteration 11/1000 | Loss: 0.00002587
Iteration 12/1000 | Loss: 0.00002580
Iteration 13/1000 | Loss: 0.00002575
Iteration 14/1000 | Loss: 0.00002573
Iteration 15/1000 | Loss: 0.00002572
Iteration 16/1000 | Loss: 0.00002572
Iteration 17/1000 | Loss: 0.00002566
Iteration 18/1000 | Loss: 0.00002565
Iteration 19/1000 | Loss: 0.00002564
Iteration 20/1000 | Loss: 0.00002563
Iteration 21/1000 | Loss: 0.00002562
Iteration 22/1000 | Loss: 0.00002561
Iteration 23/1000 | Loss: 0.00002561
Iteration 24/1000 | Loss: 0.00002561
Iteration 25/1000 | Loss: 0.00002560
Iteration 26/1000 | Loss: 0.00002560
Iteration 27/1000 | Loss: 0.00002559
Iteration 28/1000 | Loss: 0.00002559
Iteration 29/1000 | Loss: 0.00002558
Iteration 30/1000 | Loss: 0.00002557
Iteration 31/1000 | Loss: 0.00002557
Iteration 32/1000 | Loss: 0.00002557
Iteration 33/1000 | Loss: 0.00002556
Iteration 34/1000 | Loss: 0.00002556
Iteration 35/1000 | Loss: 0.00002555
Iteration 36/1000 | Loss: 0.00002555
Iteration 37/1000 | Loss: 0.00002555
Iteration 38/1000 | Loss: 0.00002555
Iteration 39/1000 | Loss: 0.00002555
Iteration 40/1000 | Loss: 0.00002555
Iteration 41/1000 | Loss: 0.00002555
Iteration 42/1000 | Loss: 0.00002555
Iteration 43/1000 | Loss: 0.00002555
Iteration 44/1000 | Loss: 0.00002555
Iteration 45/1000 | Loss: 0.00002555
Iteration 46/1000 | Loss: 0.00002553
Iteration 47/1000 | Loss: 0.00002553
Iteration 48/1000 | Loss: 0.00002553
Iteration 49/1000 | Loss: 0.00002553
Iteration 50/1000 | Loss: 0.00002553
Iteration 51/1000 | Loss: 0.00002553
Iteration 52/1000 | Loss: 0.00002553
Iteration 53/1000 | Loss: 0.00002553
Iteration 54/1000 | Loss: 0.00002553
Iteration 55/1000 | Loss: 0.00002552
Iteration 56/1000 | Loss: 0.00002552
Iteration 57/1000 | Loss: 0.00002552
Iteration 58/1000 | Loss: 0.00002552
Iteration 59/1000 | Loss: 0.00002552
Iteration 60/1000 | Loss: 0.00002552
Iteration 61/1000 | Loss: 0.00002552
Iteration 62/1000 | Loss: 0.00002552
Iteration 63/1000 | Loss: 0.00002552
Iteration 64/1000 | Loss: 0.00002552
Iteration 65/1000 | Loss: 0.00002551
Iteration 66/1000 | Loss: 0.00002551
Iteration 67/1000 | Loss: 0.00002551
Iteration 68/1000 | Loss: 0.00002551
Iteration 69/1000 | Loss: 0.00002551
Iteration 70/1000 | Loss: 0.00002551
Iteration 71/1000 | Loss: 0.00002550
Iteration 72/1000 | Loss: 0.00002550
Iteration 73/1000 | Loss: 0.00002550
Iteration 74/1000 | Loss: 0.00002550
Iteration 75/1000 | Loss: 0.00002550
Iteration 76/1000 | Loss: 0.00002550
Iteration 77/1000 | Loss: 0.00002550
Iteration 78/1000 | Loss: 0.00002550
Iteration 79/1000 | Loss: 0.00002550
Iteration 80/1000 | Loss: 0.00002550
Iteration 81/1000 | Loss: 0.00002550
Iteration 82/1000 | Loss: 0.00002549
Iteration 83/1000 | Loss: 0.00002549
Iteration 84/1000 | Loss: 0.00002549
Iteration 85/1000 | Loss: 0.00002549
Iteration 86/1000 | Loss: 0.00002549
Iteration 87/1000 | Loss: 0.00002549
Iteration 88/1000 | Loss: 0.00002549
Iteration 89/1000 | Loss: 0.00002549
Iteration 90/1000 | Loss: 0.00002548
Iteration 91/1000 | Loss: 0.00002548
Iteration 92/1000 | Loss: 0.00002548
Iteration 93/1000 | Loss: 0.00002548
Iteration 94/1000 | Loss: 0.00002548
Iteration 95/1000 | Loss: 0.00002548
Iteration 96/1000 | Loss: 0.00002548
Iteration 97/1000 | Loss: 0.00002548
Iteration 98/1000 | Loss: 0.00002548
Iteration 99/1000 | Loss: 0.00002548
Iteration 100/1000 | Loss: 0.00002548
Iteration 101/1000 | Loss: 0.00002548
Iteration 102/1000 | Loss: 0.00002548
Iteration 103/1000 | Loss: 0.00002548
Iteration 104/1000 | Loss: 0.00002548
Iteration 105/1000 | Loss: 0.00002548
Iteration 106/1000 | Loss: 0.00002548
Iteration 107/1000 | Loss: 0.00002548
Iteration 108/1000 | Loss: 0.00002548
Iteration 109/1000 | Loss: 0.00002548
Iteration 110/1000 | Loss: 0.00002548
Iteration 111/1000 | Loss: 0.00002548
Iteration 112/1000 | Loss: 0.00002548
Iteration 113/1000 | Loss: 0.00002548
Iteration 114/1000 | Loss: 0.00002548
Iteration 115/1000 | Loss: 0.00002548
Iteration 116/1000 | Loss: 0.00002548
Iteration 117/1000 | Loss: 0.00002548
Iteration 118/1000 | Loss: 0.00002548
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [2.548096927057486e-05, 2.548096927057486e-05, 2.548096927057486e-05, 2.548096927057486e-05, 2.548096927057486e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.548096927057486e-05

Optimization complete. Final v2v error: 4.431213855743408 mm

Highest mean error: 4.611878871917725 mm for frame 12

Lowest mean error: 4.317713260650635 mm for frame 65

Saving results

Total time: 39.80341982841492
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_0196/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0196/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0196/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01195324
Iteration 2/25 | Loss: 0.00330705
Iteration 3/25 | Loss: 0.00289467
Iteration 4/25 | Loss: 0.00305381
Iteration 5/25 | Loss: 0.00307187
Iteration 6/25 | Loss: 0.00284622
Iteration 7/25 | Loss: 0.00265025
Iteration 8/25 | Loss: 0.00271292
Iteration 9/25 | Loss: 0.00277841
Iteration 10/25 | Loss: 0.00257777
Iteration 11/25 | Loss: 0.00235628
Iteration 12/25 | Loss: 0.00214663
Iteration 13/25 | Loss: 0.00203694
Iteration 14/25 | Loss: 0.00195459
Iteration 15/25 | Loss: 0.00195943
Iteration 16/25 | Loss: 0.00195056
Iteration 17/25 | Loss: 0.00185087
Iteration 18/25 | Loss: 0.00185382
Iteration 19/25 | Loss: 0.00184377
Iteration 20/25 | Loss: 0.00185546
Iteration 21/25 | Loss: 0.00189554
Iteration 22/25 | Loss: 0.00187804
Iteration 23/25 | Loss: 0.00186410
Iteration 24/25 | Loss: 0.00182647
Iteration 25/25 | Loss: 0.00185327

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.83690250
Iteration 2/25 | Loss: 0.00769108
Iteration 3/25 | Loss: 0.00704055
Iteration 4/25 | Loss: 0.00782363
Iteration 5/25 | Loss: 0.00827557
Iteration 6/25 | Loss: 0.00652960
Iteration 7/25 | Loss: 0.00652948
Iteration 8/25 | Loss: 0.00652845
Iteration 9/25 | Loss: 0.00652845
Iteration 10/25 | Loss: 0.00652845
Iteration 11/25 | Loss: 0.00652845
Iteration 12/25 | Loss: 0.00652845
Iteration 13/25 | Loss: 0.00652844
Iteration 14/25 | Loss: 0.00652844
Iteration 15/25 | Loss: 0.00652844
Iteration 16/25 | Loss: 0.00652844
Iteration 17/25 | Loss: 0.00652844
Iteration 18/25 | Loss: 0.00652844
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.006528444122523069, 0.006528444122523069, 0.006528444122523069, 0.006528444122523069, 0.006528444122523069]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.006528444122523069

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00652844
Iteration 2/1000 | Loss: 0.00280328
Iteration 3/1000 | Loss: 0.00273008
Iteration 4/1000 | Loss: 0.00453487
Iteration 5/1000 | Loss: 0.00290746
Iteration 6/1000 | Loss: 0.00280699
Iteration 7/1000 | Loss: 0.00304710
Iteration 8/1000 | Loss: 0.00368570
Iteration 9/1000 | Loss: 0.00285751
Iteration 10/1000 | Loss: 0.00361181
Iteration 11/1000 | Loss: 0.00627666
Iteration 12/1000 | Loss: 0.00446947
Iteration 13/1000 | Loss: 0.00530116
Iteration 14/1000 | Loss: 0.00142176
Iteration 15/1000 | Loss: 0.00123226
Iteration 16/1000 | Loss: 0.00325849
Iteration 17/1000 | Loss: 0.00251337
Iteration 18/1000 | Loss: 0.00338291
Iteration 19/1000 | Loss: 0.00360300
Iteration 20/1000 | Loss: 0.00188763
Iteration 21/1000 | Loss: 0.00310636
Iteration 22/1000 | Loss: 0.00350494
Iteration 23/1000 | Loss: 0.00308726
Iteration 24/1000 | Loss: 0.00328988
Iteration 25/1000 | Loss: 0.00454689
Iteration 26/1000 | Loss: 0.00193944
Iteration 27/1000 | Loss: 0.00318875
Iteration 28/1000 | Loss: 0.00399070
Iteration 29/1000 | Loss: 0.00276302
Iteration 30/1000 | Loss: 0.00350427
Iteration 31/1000 | Loss: 0.00342097
Iteration 32/1000 | Loss: 0.00401358
Iteration 33/1000 | Loss: 0.00341112
Iteration 34/1000 | Loss: 0.00429409
Iteration 35/1000 | Loss: 0.00311998
Iteration 36/1000 | Loss: 0.00232518
Iteration 37/1000 | Loss: 0.00316486
Iteration 38/1000 | Loss: 0.00344143
Iteration 39/1000 | Loss: 0.00380237
Iteration 40/1000 | Loss: 0.00382082
Iteration 41/1000 | Loss: 0.00317088
Iteration 42/1000 | Loss: 0.00294180
Iteration 43/1000 | Loss: 0.00338899
Iteration 44/1000 | Loss: 0.00533414
Iteration 45/1000 | Loss: 0.00220328
Iteration 46/1000 | Loss: 0.00314975
Iteration 47/1000 | Loss: 0.00186882
Iteration 48/1000 | Loss: 0.00171975
Iteration 49/1000 | Loss: 0.00155789
Iteration 50/1000 | Loss: 0.00283791
Iteration 51/1000 | Loss: 0.00139576
Iteration 52/1000 | Loss: 0.00338017
Iteration 53/1000 | Loss: 0.00301348
Iteration 54/1000 | Loss: 0.00264348
Iteration 55/1000 | Loss: 0.00269761
Iteration 56/1000 | Loss: 0.00177825
Iteration 57/1000 | Loss: 0.00282926
Iteration 58/1000 | Loss: 0.00298027
Iteration 59/1000 | Loss: 0.00286519
Iteration 60/1000 | Loss: 0.00292559
Iteration 61/1000 | Loss: 0.00425647
Iteration 62/1000 | Loss: 0.00313857
Iteration 63/1000 | Loss: 0.00208373
Iteration 64/1000 | Loss: 0.00228423
Iteration 65/1000 | Loss: 0.00295888
Iteration 66/1000 | Loss: 0.00214572
Iteration 67/1000 | Loss: 0.00197036
Iteration 68/1000 | Loss: 0.00138135
Iteration 69/1000 | Loss: 0.00112529
Iteration 70/1000 | Loss: 0.00086452
Iteration 71/1000 | Loss: 0.00219632
Iteration 72/1000 | Loss: 0.00156379
Iteration 73/1000 | Loss: 0.00240881
Iteration 74/1000 | Loss: 0.00217563
Iteration 75/1000 | Loss: 0.00148144
Iteration 76/1000 | Loss: 0.00293717
Iteration 77/1000 | Loss: 0.00214182
Iteration 78/1000 | Loss: 0.00072620
Iteration 79/1000 | Loss: 0.00279011
Iteration 80/1000 | Loss: 0.00152300
Iteration 81/1000 | Loss: 0.00118989
Iteration 82/1000 | Loss: 0.00167329
Iteration 83/1000 | Loss: 0.00212461
Iteration 84/1000 | Loss: 0.00171718
Iteration 85/1000 | Loss: 0.00236219
Iteration 86/1000 | Loss: 0.00116391
Iteration 87/1000 | Loss: 0.00134660
Iteration 88/1000 | Loss: 0.00105602
Iteration 89/1000 | Loss: 0.00142094
Iteration 90/1000 | Loss: 0.00132469
Iteration 91/1000 | Loss: 0.00146354
Iteration 92/1000 | Loss: 0.00141513
Iteration 93/1000 | Loss: 0.00119774
Iteration 94/1000 | Loss: 0.00098449
Iteration 95/1000 | Loss: 0.00112037
Iteration 96/1000 | Loss: 0.00128979
Iteration 97/1000 | Loss: 0.00132500
Iteration 98/1000 | Loss: 0.00075404
Iteration 99/1000 | Loss: 0.00051146
Iteration 100/1000 | Loss: 0.00082468
Iteration 101/1000 | Loss: 0.00066839
Iteration 102/1000 | Loss: 0.00078596
Iteration 103/1000 | Loss: 0.00056115
Iteration 104/1000 | Loss: 0.00076804
Iteration 105/1000 | Loss: 0.00062177
Iteration 106/1000 | Loss: 0.00098389
Iteration 107/1000 | Loss: 0.00065297
Iteration 108/1000 | Loss: 0.00068713
Iteration 109/1000 | Loss: 0.00066454
Iteration 110/1000 | Loss: 0.00072509
Iteration 111/1000 | Loss: 0.00099680
Iteration 112/1000 | Loss: 0.00068272
Iteration 113/1000 | Loss: 0.00068074
Iteration 114/1000 | Loss: 0.00042055
Iteration 115/1000 | Loss: 0.00040742
Iteration 116/1000 | Loss: 0.00052015
Iteration 117/1000 | Loss: 0.00058128
Iteration 118/1000 | Loss: 0.00051854
Iteration 119/1000 | Loss: 0.00041214
Iteration 120/1000 | Loss: 0.00056349
Iteration 121/1000 | Loss: 0.00059793
Iteration 122/1000 | Loss: 0.00052464
Iteration 123/1000 | Loss: 0.00055115
Iteration 124/1000 | Loss: 0.00061831
Iteration 125/1000 | Loss: 0.00056836
Iteration 126/1000 | Loss: 0.00059479
Iteration 127/1000 | Loss: 0.00058685
Iteration 128/1000 | Loss: 0.00088525
Iteration 129/1000 | Loss: 0.00060022
Iteration 130/1000 | Loss: 0.00067780
Iteration 131/1000 | Loss: 0.00051049
Iteration 132/1000 | Loss: 0.00040515
Iteration 133/1000 | Loss: 0.00055773
Iteration 134/1000 | Loss: 0.00052888
Iteration 135/1000 | Loss: 0.00058993
Iteration 136/1000 | Loss: 0.00053581
Iteration 137/1000 | Loss: 0.00055463
Iteration 138/1000 | Loss: 0.00053640
Iteration 139/1000 | Loss: 0.00048409
Iteration 140/1000 | Loss: 0.00052637
Iteration 141/1000 | Loss: 0.00052072
Iteration 142/1000 | Loss: 0.00059676
Iteration 143/1000 | Loss: 0.00054252
Iteration 144/1000 | Loss: 0.00061296
Iteration 145/1000 | Loss: 0.00058975
Iteration 146/1000 | Loss: 0.00050374
Iteration 147/1000 | Loss: 0.00040221
Iteration 148/1000 | Loss: 0.00056384
Iteration 149/1000 | Loss: 0.00047208
Iteration 150/1000 | Loss: 0.00048810
Iteration 151/1000 | Loss: 0.00059030
Iteration 152/1000 | Loss: 0.00054257
Iteration 153/1000 | Loss: 0.00058169
Iteration 154/1000 | Loss: 0.00056612
Iteration 155/1000 | Loss: 0.00058278
Iteration 156/1000 | Loss: 0.00057905
Iteration 157/1000 | Loss: 0.00080845
Iteration 158/1000 | Loss: 0.00057189
Iteration 159/1000 | Loss: 0.00064377
Iteration 160/1000 | Loss: 0.00059850
Iteration 161/1000 | Loss: 0.00066278
Iteration 162/1000 | Loss: 0.00057047
Iteration 163/1000 | Loss: 0.00068879
Iteration 164/1000 | Loss: 0.00064121
Iteration 165/1000 | Loss: 0.00056159
Iteration 166/1000 | Loss: 0.00061490
Iteration 167/1000 | Loss: 0.00045463
Iteration 168/1000 | Loss: 0.00037548
Iteration 169/1000 | Loss: 0.00049627
Iteration 170/1000 | Loss: 0.00053455
Iteration 171/1000 | Loss: 0.00055000
Iteration 172/1000 | Loss: 0.00062457
Iteration 173/1000 | Loss: 0.00060154
Iteration 174/1000 | Loss: 0.00061167
Iteration 175/1000 | Loss: 0.00061172
Iteration 176/1000 | Loss: 0.00067309
Iteration 177/1000 | Loss: 0.00061532
Iteration 178/1000 | Loss: 0.00062334
Iteration 179/1000 | Loss: 0.00057924
Iteration 180/1000 | Loss: 0.00053861
Iteration 181/1000 | Loss: 0.00050785
Iteration 182/1000 | Loss: 0.00066804
Iteration 183/1000 | Loss: 0.00072625
Iteration 184/1000 | Loss: 0.00051722
Iteration 185/1000 | Loss: 0.00058237
Iteration 186/1000 | Loss: 0.00051989
Iteration 187/1000 | Loss: 0.00047838
Iteration 188/1000 | Loss: 0.00041815
Iteration 189/1000 | Loss: 0.00061865
Iteration 190/1000 | Loss: 0.00058033
Iteration 191/1000 | Loss: 0.00057268
Iteration 192/1000 | Loss: 0.00114182
Iteration 193/1000 | Loss: 0.00076421
Iteration 194/1000 | Loss: 0.00108183
Iteration 195/1000 | Loss: 0.00057459
Iteration 196/1000 | Loss: 0.00046414
Iteration 197/1000 | Loss: 0.00046247
Iteration 198/1000 | Loss: 0.00048153
Iteration 199/1000 | Loss: 0.00074134
Iteration 200/1000 | Loss: 0.00048689
Iteration 201/1000 | Loss: 0.00089962
Iteration 202/1000 | Loss: 0.00086587
Iteration 203/1000 | Loss: 0.00057572
Iteration 204/1000 | Loss: 0.00061098
Iteration 205/1000 | Loss: 0.00064065
Iteration 206/1000 | Loss: 0.00064472
Iteration 207/1000 | Loss: 0.00072382
Iteration 208/1000 | Loss: 0.00057324
Iteration 209/1000 | Loss: 0.00067350
Iteration 210/1000 | Loss: 0.00051721
Iteration 211/1000 | Loss: 0.00079096
Iteration 212/1000 | Loss: 0.00045756
Iteration 213/1000 | Loss: 0.00057197
Iteration 214/1000 | Loss: 0.00068670
Iteration 215/1000 | Loss: 0.00060296
Iteration 216/1000 | Loss: 0.00070274
Iteration 217/1000 | Loss: 0.00109471
Iteration 218/1000 | Loss: 0.00062471
Iteration 219/1000 | Loss: 0.00103029
Iteration 220/1000 | Loss: 0.00078173
Iteration 221/1000 | Loss: 0.00060906
Iteration 222/1000 | Loss: 0.00084831
Iteration 223/1000 | Loss: 0.00060165
Iteration 224/1000 | Loss: 0.00050961
Iteration 225/1000 | Loss: 0.00094928
Iteration 226/1000 | Loss: 0.00103110
Iteration 227/1000 | Loss: 0.00050972
Iteration 228/1000 | Loss: 0.00065303
Iteration 229/1000 | Loss: 0.00075998
Iteration 230/1000 | Loss: 0.00055604
Iteration 231/1000 | Loss: 0.00055611
Iteration 232/1000 | Loss: 0.00036738
Iteration 233/1000 | Loss: 0.00033453
Iteration 234/1000 | Loss: 0.00033931
Iteration 235/1000 | Loss: 0.00034949
Iteration 236/1000 | Loss: 0.00027951
Iteration 237/1000 | Loss: 0.00028782
Iteration 238/1000 | Loss: 0.00030191
Iteration 239/1000 | Loss: 0.00026374
Iteration 240/1000 | Loss: 0.00031139
Iteration 241/1000 | Loss: 0.00034149
Iteration 242/1000 | Loss: 0.00039483
Iteration 243/1000 | Loss: 0.00037912
Iteration 244/1000 | Loss: 0.00037367
Iteration 245/1000 | Loss: 0.00036502
Iteration 246/1000 | Loss: 0.00038235
Iteration 247/1000 | Loss: 0.00028837
Iteration 248/1000 | Loss: 0.00032238
Iteration 249/1000 | Loss: 0.00040423
Iteration 250/1000 | Loss: 0.00041300
Iteration 251/1000 | Loss: 0.00031884
Iteration 252/1000 | Loss: 0.00034193
Iteration 253/1000 | Loss: 0.00034084
Iteration 254/1000 | Loss: 0.00038952
Iteration 255/1000 | Loss: 0.00042514
Iteration 256/1000 | Loss: 0.00039045
Iteration 257/1000 | Loss: 0.00029627
Iteration 258/1000 | Loss: 0.00033687
Iteration 259/1000 | Loss: 0.00042204
Iteration 260/1000 | Loss: 0.00037344
Iteration 261/1000 | Loss: 0.00029002
Iteration 262/1000 | Loss: 0.00020889
Iteration 263/1000 | Loss: 0.00028182
Iteration 264/1000 | Loss: 0.00029923
Iteration 265/1000 | Loss: 0.00027758
Iteration 266/1000 | Loss: 0.00025041
Iteration 267/1000 | Loss: 0.00023544
Iteration 268/1000 | Loss: 0.00025047
Iteration 269/1000 | Loss: 0.00023990
Iteration 270/1000 | Loss: 0.00025358
Iteration 271/1000 | Loss: 0.00024921
Iteration 272/1000 | Loss: 0.00028396
Iteration 273/1000 | Loss: 0.00029821
Iteration 274/1000 | Loss: 0.00027530
Iteration 275/1000 | Loss: 0.00028270
Iteration 276/1000 | Loss: 0.00034299
Iteration 277/1000 | Loss: 0.00026672
Iteration 278/1000 | Loss: 0.00027811
Iteration 279/1000 | Loss: 0.00025151
Iteration 280/1000 | Loss: 0.00054621
Iteration 281/1000 | Loss: 0.00044880
Iteration 282/1000 | Loss: 0.00028138
Iteration 283/1000 | Loss: 0.00029389
Iteration 284/1000 | Loss: 0.00030113
Iteration 285/1000 | Loss: 0.00026076
Iteration 286/1000 | Loss: 0.00030326
Iteration 287/1000 | Loss: 0.00024038
Iteration 288/1000 | Loss: 0.00047566
Iteration 289/1000 | Loss: 0.00025218
Iteration 290/1000 | Loss: 0.00032045
Iteration 291/1000 | Loss: 0.00066189
Iteration 292/1000 | Loss: 0.00020873
Iteration 293/1000 | Loss: 0.00032570
Iteration 294/1000 | Loss: 0.00028177
Iteration 295/1000 | Loss: 0.00028409
Iteration 296/1000 | Loss: 0.00026228
Iteration 297/1000 | Loss: 0.00026494
Iteration 298/1000 | Loss: 0.00024016
Iteration 299/1000 | Loss: 0.00023921
Iteration 300/1000 | Loss: 0.00024012
Iteration 301/1000 | Loss: 0.00023527
Iteration 302/1000 | Loss: 0.00023271
Iteration 303/1000 | Loss: 0.00023359
Iteration 304/1000 | Loss: 0.00026101
Iteration 305/1000 | Loss: 0.00023272
Iteration 306/1000 | Loss: 0.00023579
Iteration 307/1000 | Loss: 0.00025492
Iteration 308/1000 | Loss: 0.00047205
Iteration 309/1000 | Loss: 0.00025913
Iteration 310/1000 | Loss: 0.00028182
Iteration 311/1000 | Loss: 0.00026625
Iteration 312/1000 | Loss: 0.00027733
Iteration 313/1000 | Loss: 0.00024025
Iteration 314/1000 | Loss: 0.00021438
Iteration 315/1000 | Loss: 0.00022407
Iteration 316/1000 | Loss: 0.00026189
Iteration 317/1000 | Loss: 0.00022449
Iteration 318/1000 | Loss: 0.00025994
Iteration 319/1000 | Loss: 0.00049214
Iteration 320/1000 | Loss: 0.00041754
Iteration 321/1000 | Loss: 0.00025488
Iteration 322/1000 | Loss: 0.00020707
Iteration 323/1000 | Loss: 0.00020464
Iteration 324/1000 | Loss: 0.00024677
Iteration 325/1000 | Loss: 0.00024748
Iteration 326/1000 | Loss: 0.00045370
Iteration 327/1000 | Loss: 0.00028544
Iteration 328/1000 | Loss: 0.00024532
Iteration 329/1000 | Loss: 0.00026399
Iteration 330/1000 | Loss: 0.00022274
Iteration 331/1000 | Loss: 0.00049622
Iteration 332/1000 | Loss: 0.00020593
Iteration 333/1000 | Loss: 0.00022050
Iteration 334/1000 | Loss: 0.00024628
Iteration 335/1000 | Loss: 0.00024738
Iteration 336/1000 | Loss: 0.00027843
Iteration 337/1000 | Loss: 0.00026768
Iteration 338/1000 | Loss: 0.00027765
Iteration 339/1000 | Loss: 0.00025233
Iteration 340/1000 | Loss: 0.00023118
Iteration 341/1000 | Loss: 0.00024493
Iteration 342/1000 | Loss: 0.00023066
Iteration 343/1000 | Loss: 0.00024281
Iteration 344/1000 | Loss: 0.00022982
Iteration 345/1000 | Loss: 0.00023960
Iteration 346/1000 | Loss: 0.00024206
Iteration 347/1000 | Loss: 0.00026426
Iteration 348/1000 | Loss: 0.00027632
Iteration 349/1000 | Loss: 0.00024227
Iteration 350/1000 | Loss: 0.00024311
Iteration 351/1000 | Loss: 0.00021613
Iteration 352/1000 | Loss: 0.00023532
Iteration 353/1000 | Loss: 0.00023048
Iteration 354/1000 | Loss: 0.00025564
Iteration 355/1000 | Loss: 0.00024466
Iteration 356/1000 | Loss: 0.00027162
Iteration 357/1000 | Loss: 0.00020264
Iteration 358/1000 | Loss: 0.00020803
Iteration 359/1000 | Loss: 0.00023820
Iteration 360/1000 | Loss: 0.00025481
Iteration 361/1000 | Loss: 0.00021998
Iteration 362/1000 | Loss: 0.00021147
Iteration 363/1000 | Loss: 0.00018391
Iteration 364/1000 | Loss: 0.00024587
Iteration 365/1000 | Loss: 0.00020540
Iteration 366/1000 | Loss: 0.00018778
Iteration 367/1000 | Loss: 0.00014505
Iteration 368/1000 | Loss: 0.00024277
Iteration 369/1000 | Loss: 0.00023410
Iteration 370/1000 | Loss: 0.00023184
Iteration 371/1000 | Loss: 0.00023293
Iteration 372/1000 | Loss: 0.00022807
Iteration 373/1000 | Loss: 0.00020451
Iteration 374/1000 | Loss: 0.00020113
Iteration 375/1000 | Loss: 0.00024386
Iteration 376/1000 | Loss: 0.00025623
Iteration 377/1000 | Loss: 0.00027105
Iteration 378/1000 | Loss: 0.00024954
Iteration 379/1000 | Loss: 0.00022032
Iteration 380/1000 | Loss: 0.00020106
Iteration 381/1000 | Loss: 0.00024275
Iteration 382/1000 | Loss: 0.00025212
Iteration 383/1000 | Loss: 0.00025744
Iteration 384/1000 | Loss: 0.00023855
Iteration 385/1000 | Loss: 0.00023539
Iteration 386/1000 | Loss: 0.00024409
Iteration 387/1000 | Loss: 0.00024961
Iteration 388/1000 | Loss: 0.00024113
Iteration 389/1000 | Loss: 0.00022880
Iteration 390/1000 | Loss: 0.00024250
Iteration 391/1000 | Loss: 0.00023239
Iteration 392/1000 | Loss: 0.00022803
Iteration 393/1000 | Loss: 0.00023570
Iteration 394/1000 | Loss: 0.00043845
Iteration 395/1000 | Loss: 0.00025464
Iteration 396/1000 | Loss: 0.00018257
Iteration 397/1000 | Loss: 0.00011792
Iteration 398/1000 | Loss: 0.00008430
Iteration 399/1000 | Loss: 0.00009403
Iteration 400/1000 | Loss: 0.00006784
Iteration 401/1000 | Loss: 0.00005414
Iteration 402/1000 | Loss: 0.00005015
Iteration 403/1000 | Loss: 0.00004707
Iteration 404/1000 | Loss: 0.00004482
Iteration 405/1000 | Loss: 0.00004333
Iteration 406/1000 | Loss: 0.00004225
Iteration 407/1000 | Loss: 0.00004137
Iteration 408/1000 | Loss: 0.00004053
Iteration 409/1000 | Loss: 0.00003964
Iteration 410/1000 | Loss: 0.00003900
Iteration 411/1000 | Loss: 0.00003854
Iteration 412/1000 | Loss: 0.00003817
Iteration 413/1000 | Loss: 0.00003771
Iteration 414/1000 | Loss: 0.00003743
Iteration 415/1000 | Loss: 0.00003716
Iteration 416/1000 | Loss: 0.00078213
Iteration 417/1000 | Loss: 0.00027694
Iteration 418/1000 | Loss: 0.00006037
Iteration 419/1000 | Loss: 0.00004603
Iteration 420/1000 | Loss: 0.00003885
Iteration 421/1000 | Loss: 0.00003632
Iteration 422/1000 | Loss: 0.00003559
Iteration 423/1000 | Loss: 0.00003521
Iteration 424/1000 | Loss: 0.00003477
Iteration 425/1000 | Loss: 0.00003442
Iteration 426/1000 | Loss: 0.00003422
Iteration 427/1000 | Loss: 0.00003419
Iteration 428/1000 | Loss: 0.00003411
Iteration 429/1000 | Loss: 0.00003410
Iteration 430/1000 | Loss: 0.00003409
Iteration 431/1000 | Loss: 0.00003408
Iteration 432/1000 | Loss: 0.00003406
Iteration 433/1000 | Loss: 0.00003405
Iteration 434/1000 | Loss: 0.00003403
Iteration 435/1000 | Loss: 0.00003403
Iteration 436/1000 | Loss: 0.00003401
Iteration 437/1000 | Loss: 0.00003401
Iteration 438/1000 | Loss: 0.00003401
Iteration 439/1000 | Loss: 0.00003401
Iteration 440/1000 | Loss: 0.00003400
Iteration 441/1000 | Loss: 0.00003400
Iteration 442/1000 | Loss: 0.00003398
Iteration 443/1000 | Loss: 0.00003398
Iteration 444/1000 | Loss: 0.00003398
Iteration 445/1000 | Loss: 0.00003397
Iteration 446/1000 | Loss: 0.00003397
Iteration 447/1000 | Loss: 0.00003394
Iteration 448/1000 | Loss: 0.00003393
Iteration 449/1000 | Loss: 0.00003393
Iteration 450/1000 | Loss: 0.00003392
Iteration 451/1000 | Loss: 0.00003392
Iteration 452/1000 | Loss: 0.00003392
Iteration 453/1000 | Loss: 0.00003391
Iteration 454/1000 | Loss: 0.00003391
Iteration 455/1000 | Loss: 0.00003390
Iteration 456/1000 | Loss: 0.00003390
Iteration 457/1000 | Loss: 0.00003389
Iteration 458/1000 | Loss: 0.00003389
Iteration 459/1000 | Loss: 0.00003389
Iteration 460/1000 | Loss: 0.00003389
Iteration 461/1000 | Loss: 0.00003389
Iteration 462/1000 | Loss: 0.00003389
Iteration 463/1000 | Loss: 0.00003389
Iteration 464/1000 | Loss: 0.00003389
Iteration 465/1000 | Loss: 0.00003389
Iteration 466/1000 | Loss: 0.00003389
Iteration 467/1000 | Loss: 0.00003388
Iteration 468/1000 | Loss: 0.00003388
Iteration 469/1000 | Loss: 0.00003388
Iteration 470/1000 | Loss: 0.00003387
Iteration 471/1000 | Loss: 0.00003387
Iteration 472/1000 | Loss: 0.00003387
Iteration 473/1000 | Loss: 0.00003387
Iteration 474/1000 | Loss: 0.00003387
Iteration 475/1000 | Loss: 0.00003387
Iteration 476/1000 | Loss: 0.00003386
Iteration 477/1000 | Loss: 0.00003386
Iteration 478/1000 | Loss: 0.00003386
Iteration 479/1000 | Loss: 0.00003386
Iteration 480/1000 | Loss: 0.00003386
Iteration 481/1000 | Loss: 0.00003386
Iteration 482/1000 | Loss: 0.00003386
Iteration 483/1000 | Loss: 0.00003386
Iteration 484/1000 | Loss: 0.00003386
Iteration 485/1000 | Loss: 0.00003386
Iteration 486/1000 | Loss: 0.00003386
Iteration 487/1000 | Loss: 0.00003385
Iteration 488/1000 | Loss: 0.00003385
Iteration 489/1000 | Loss: 0.00003385
Iteration 490/1000 | Loss: 0.00003385
Iteration 491/1000 | Loss: 0.00003385
Iteration 492/1000 | Loss: 0.00003385
Iteration 493/1000 | Loss: 0.00003385
Iteration 494/1000 | Loss: 0.00003385
Iteration 495/1000 | Loss: 0.00003385
Iteration 496/1000 | Loss: 0.00003385
Iteration 497/1000 | Loss: 0.00003385
Iteration 498/1000 | Loss: 0.00003385
Iteration 499/1000 | Loss: 0.00003385
Iteration 500/1000 | Loss: 0.00003385
Iteration 501/1000 | Loss: 0.00003385
Iteration 502/1000 | Loss: 0.00003384
Iteration 503/1000 | Loss: 0.00003384
Iteration 504/1000 | Loss: 0.00003384
Iteration 505/1000 | Loss: 0.00003384
Iteration 506/1000 | Loss: 0.00003384
Iteration 507/1000 | Loss: 0.00003384
Iteration 508/1000 | Loss: 0.00003384
Iteration 509/1000 | Loss: 0.00003384
Iteration 510/1000 | Loss: 0.00003384
Iteration 511/1000 | Loss: 0.00003384
Iteration 512/1000 | Loss: 0.00003384
Iteration 513/1000 | Loss: 0.00003384
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 513. Stopping optimization.
Last 5 losses: [3.384378942428157e-05, 3.384378942428157e-05, 3.384378942428157e-05, 3.384378942428157e-05, 3.384378942428157e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.384378942428157e-05

Optimization complete. Final v2v error: 4.814267158508301 mm

Highest mean error: 14.870916366577148 mm for frame 26

Lowest mean error: 4.501969814300537 mm for frame 204

Saving results

Total time: 754.8616585731506
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_0196/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0196/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0196/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00795449
Iteration 2/25 | Loss: 0.00190203
Iteration 3/25 | Loss: 0.00157801
Iteration 4/25 | Loss: 0.00155657
Iteration 5/25 | Loss: 0.00155238
Iteration 6/25 | Loss: 0.00155206
Iteration 7/25 | Loss: 0.00155206
Iteration 8/25 | Loss: 0.00155206
Iteration 9/25 | Loss: 0.00155206
Iteration 10/25 | Loss: 0.00155206
Iteration 11/25 | Loss: 0.00155206
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0015520588494837284, 0.0015520588494837284, 0.0015520588494837284, 0.0015520588494837284, 0.0015520588494837284]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015520588494837284

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.18448806
Iteration 2/25 | Loss: 0.00288674
Iteration 3/25 | Loss: 0.00288671
Iteration 4/25 | Loss: 0.00288671
Iteration 5/25 | Loss: 0.00288671
Iteration 6/25 | Loss: 0.00288671
Iteration 7/25 | Loss: 0.00288671
Iteration 8/25 | Loss: 0.00288671
Iteration 9/25 | Loss: 0.00288671
Iteration 10/25 | Loss: 0.00288671
Iteration 11/25 | Loss: 0.00288671
Iteration 12/25 | Loss: 0.00288671
Iteration 13/25 | Loss: 0.00288671
Iteration 14/25 | Loss: 0.00288671
Iteration 15/25 | Loss: 0.00288671
Iteration 16/25 | Loss: 0.00288671
Iteration 17/25 | Loss: 0.00288671
Iteration 18/25 | Loss: 0.00288671
Iteration 19/25 | Loss: 0.00288671
Iteration 20/25 | Loss: 0.00288671
Iteration 21/25 | Loss: 0.00288671
Iteration 22/25 | Loss: 0.00288671
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0028867064975202084, 0.0028867064975202084, 0.0028867064975202084, 0.0028867064975202084, 0.0028867064975202084]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0028867064975202084

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00288671
Iteration 2/1000 | Loss: 0.00004999
Iteration 3/1000 | Loss: 0.00003909
Iteration 4/1000 | Loss: 0.00003461
Iteration 5/1000 | Loss: 0.00003210
Iteration 6/1000 | Loss: 0.00003063
Iteration 7/1000 | Loss: 0.00002982
Iteration 8/1000 | Loss: 0.00002927
Iteration 9/1000 | Loss: 0.00002892
Iteration 10/1000 | Loss: 0.00002862
Iteration 11/1000 | Loss: 0.00002840
Iteration 12/1000 | Loss: 0.00002833
Iteration 13/1000 | Loss: 0.00002826
Iteration 14/1000 | Loss: 0.00002811
Iteration 15/1000 | Loss: 0.00002809
Iteration 16/1000 | Loss: 0.00002803
Iteration 17/1000 | Loss: 0.00002800
Iteration 18/1000 | Loss: 0.00002798
Iteration 19/1000 | Loss: 0.00002796
Iteration 20/1000 | Loss: 0.00002795
Iteration 21/1000 | Loss: 0.00002795
Iteration 22/1000 | Loss: 0.00002795
Iteration 23/1000 | Loss: 0.00002794
Iteration 24/1000 | Loss: 0.00002794
Iteration 25/1000 | Loss: 0.00002794
Iteration 26/1000 | Loss: 0.00002793
Iteration 27/1000 | Loss: 0.00002793
Iteration 28/1000 | Loss: 0.00002791
Iteration 29/1000 | Loss: 0.00002788
Iteration 30/1000 | Loss: 0.00002787
Iteration 31/1000 | Loss: 0.00002787
Iteration 32/1000 | Loss: 0.00002786
Iteration 33/1000 | Loss: 0.00002786
Iteration 34/1000 | Loss: 0.00002786
Iteration 35/1000 | Loss: 0.00002786
Iteration 36/1000 | Loss: 0.00002785
Iteration 37/1000 | Loss: 0.00002785
Iteration 38/1000 | Loss: 0.00002785
Iteration 39/1000 | Loss: 0.00002784
Iteration 40/1000 | Loss: 0.00002784
Iteration 41/1000 | Loss: 0.00002783
Iteration 42/1000 | Loss: 0.00002783
Iteration 43/1000 | Loss: 0.00002783
Iteration 44/1000 | Loss: 0.00002783
Iteration 45/1000 | Loss: 0.00002783
Iteration 46/1000 | Loss: 0.00002783
Iteration 47/1000 | Loss: 0.00002782
Iteration 48/1000 | Loss: 0.00002782
Iteration 49/1000 | Loss: 0.00002782
Iteration 50/1000 | Loss: 0.00002782
Iteration 51/1000 | Loss: 0.00002781
Iteration 52/1000 | Loss: 0.00002781
Iteration 53/1000 | Loss: 0.00002781
Iteration 54/1000 | Loss: 0.00002781
Iteration 55/1000 | Loss: 0.00002780
Iteration 56/1000 | Loss: 0.00002780
Iteration 57/1000 | Loss: 0.00002779
Iteration 58/1000 | Loss: 0.00002779
Iteration 59/1000 | Loss: 0.00002779
Iteration 60/1000 | Loss: 0.00002779
Iteration 61/1000 | Loss: 0.00002779
Iteration 62/1000 | Loss: 0.00002779
Iteration 63/1000 | Loss: 0.00002779
Iteration 64/1000 | Loss: 0.00002779
Iteration 65/1000 | Loss: 0.00002779
Iteration 66/1000 | Loss: 0.00002779
Iteration 67/1000 | Loss: 0.00002778
Iteration 68/1000 | Loss: 0.00002778
Iteration 69/1000 | Loss: 0.00002778
Iteration 70/1000 | Loss: 0.00002778
Iteration 71/1000 | Loss: 0.00002778
Iteration 72/1000 | Loss: 0.00002778
Iteration 73/1000 | Loss: 0.00002778
Iteration 74/1000 | Loss: 0.00002778
Iteration 75/1000 | Loss: 0.00002778
Iteration 76/1000 | Loss: 0.00002778
Iteration 77/1000 | Loss: 0.00002778
Iteration 78/1000 | Loss: 0.00002778
Iteration 79/1000 | Loss: 0.00002778
Iteration 80/1000 | Loss: 0.00002778
Iteration 81/1000 | Loss: 0.00002778
Iteration 82/1000 | Loss: 0.00002778
Iteration 83/1000 | Loss: 0.00002778
Iteration 84/1000 | Loss: 0.00002778
Iteration 85/1000 | Loss: 0.00002778
Iteration 86/1000 | Loss: 0.00002778
Iteration 87/1000 | Loss: 0.00002778
Iteration 88/1000 | Loss: 0.00002778
Iteration 89/1000 | Loss: 0.00002778
Iteration 90/1000 | Loss: 0.00002778
Iteration 91/1000 | Loss: 0.00002778
Iteration 92/1000 | Loss: 0.00002778
Iteration 93/1000 | Loss: 0.00002778
Iteration 94/1000 | Loss: 0.00002778
Iteration 95/1000 | Loss: 0.00002778
Iteration 96/1000 | Loss: 0.00002778
Iteration 97/1000 | Loss: 0.00002778
Iteration 98/1000 | Loss: 0.00002778
Iteration 99/1000 | Loss: 0.00002778
Iteration 100/1000 | Loss: 0.00002778
Iteration 101/1000 | Loss: 0.00002778
Iteration 102/1000 | Loss: 0.00002778
Iteration 103/1000 | Loss: 0.00002778
Iteration 104/1000 | Loss: 0.00002778
Iteration 105/1000 | Loss: 0.00002778
Iteration 106/1000 | Loss: 0.00002778
Iteration 107/1000 | Loss: 0.00002778
Iteration 108/1000 | Loss: 0.00002778
Iteration 109/1000 | Loss: 0.00002778
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [2.7776366550824605e-05, 2.7776366550824605e-05, 2.7776366550824605e-05, 2.7776366550824605e-05, 2.7776366550824605e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7776366550824605e-05

Optimization complete. Final v2v error: 4.65302038192749 mm

Highest mean error: 5.122550964355469 mm for frame 32

Lowest mean error: 4.328498840332031 mm for frame 137

Saving results

Total time: 37.25396704673767
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_0196/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0196/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0196/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00569745
Iteration 2/25 | Loss: 0.00164916
Iteration 3/25 | Loss: 0.00154418
Iteration 4/25 | Loss: 0.00153246
Iteration 5/25 | Loss: 0.00152889
Iteration 6/25 | Loss: 0.00152831
Iteration 7/25 | Loss: 0.00152831
Iteration 8/25 | Loss: 0.00152831
Iteration 9/25 | Loss: 0.00152831
Iteration 10/25 | Loss: 0.00152831
Iteration 11/25 | Loss: 0.00152831
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0015283080283552408, 0.0015283080283552408, 0.0015283080283552408, 0.0015283080283552408, 0.0015283080283552408]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015283080283552408

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.94803417
Iteration 2/25 | Loss: 0.00229222
Iteration 3/25 | Loss: 0.00229221
Iteration 4/25 | Loss: 0.00229221
Iteration 5/25 | Loss: 0.00229221
Iteration 6/25 | Loss: 0.00229221
Iteration 7/25 | Loss: 0.00229221
Iteration 8/25 | Loss: 0.00229221
Iteration 9/25 | Loss: 0.00229221
Iteration 10/25 | Loss: 0.00229221
Iteration 11/25 | Loss: 0.00229221
Iteration 12/25 | Loss: 0.00229221
Iteration 13/25 | Loss: 0.00229221
Iteration 14/25 | Loss: 0.00229221
Iteration 15/25 | Loss: 0.00229221
Iteration 16/25 | Loss: 0.00229221
Iteration 17/25 | Loss: 0.00229221
Iteration 18/25 | Loss: 0.00229221
Iteration 19/25 | Loss: 0.00229221
Iteration 20/25 | Loss: 0.00229221
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0022922069765627384, 0.0022922069765627384, 0.0022922069765627384, 0.0022922069765627384, 0.0022922069765627384]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022922069765627384

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00229221
Iteration 2/1000 | Loss: 0.00004406
Iteration 3/1000 | Loss: 0.00003721
Iteration 4/1000 | Loss: 0.00003469
Iteration 5/1000 | Loss: 0.00003384
Iteration 6/1000 | Loss: 0.00003294
Iteration 7/1000 | Loss: 0.00003238
Iteration 8/1000 | Loss: 0.00003164
Iteration 9/1000 | Loss: 0.00003119
Iteration 10/1000 | Loss: 0.00003090
Iteration 11/1000 | Loss: 0.00003079
Iteration 12/1000 | Loss: 0.00003062
Iteration 13/1000 | Loss: 0.00003048
Iteration 14/1000 | Loss: 0.00003042
Iteration 15/1000 | Loss: 0.00003034
Iteration 16/1000 | Loss: 0.00003031
Iteration 17/1000 | Loss: 0.00003029
Iteration 18/1000 | Loss: 0.00003023
Iteration 19/1000 | Loss: 0.00003022
Iteration 20/1000 | Loss: 0.00003022
Iteration 21/1000 | Loss: 0.00003022
Iteration 22/1000 | Loss: 0.00003022
Iteration 23/1000 | Loss: 0.00003020
Iteration 24/1000 | Loss: 0.00003019
Iteration 25/1000 | Loss: 0.00003019
Iteration 26/1000 | Loss: 0.00003019
Iteration 27/1000 | Loss: 0.00003019
Iteration 28/1000 | Loss: 0.00003018
Iteration 29/1000 | Loss: 0.00003018
Iteration 30/1000 | Loss: 0.00003018
Iteration 31/1000 | Loss: 0.00003018
Iteration 32/1000 | Loss: 0.00003018
Iteration 33/1000 | Loss: 0.00003018
Iteration 34/1000 | Loss: 0.00003018
Iteration 35/1000 | Loss: 0.00003017
Iteration 36/1000 | Loss: 0.00003017
Iteration 37/1000 | Loss: 0.00003017
Iteration 38/1000 | Loss: 0.00003016
Iteration 39/1000 | Loss: 0.00003016
Iteration 40/1000 | Loss: 0.00003016
Iteration 41/1000 | Loss: 0.00003015
Iteration 42/1000 | Loss: 0.00003015
Iteration 43/1000 | Loss: 0.00003014
Iteration 44/1000 | Loss: 0.00003013
Iteration 45/1000 | Loss: 0.00003013
Iteration 46/1000 | Loss: 0.00003013
Iteration 47/1000 | Loss: 0.00003013
Iteration 48/1000 | Loss: 0.00003013
Iteration 49/1000 | Loss: 0.00003013
Iteration 50/1000 | Loss: 0.00003013
Iteration 51/1000 | Loss: 0.00003013
Iteration 52/1000 | Loss: 0.00003013
Iteration 53/1000 | Loss: 0.00003012
Iteration 54/1000 | Loss: 0.00003012
Iteration 55/1000 | Loss: 0.00003012
Iteration 56/1000 | Loss: 0.00003012
Iteration 57/1000 | Loss: 0.00003012
Iteration 58/1000 | Loss: 0.00003012
Iteration 59/1000 | Loss: 0.00003011
Iteration 60/1000 | Loss: 0.00003011
Iteration 61/1000 | Loss: 0.00003011
Iteration 62/1000 | Loss: 0.00003011
Iteration 63/1000 | Loss: 0.00003011
Iteration 64/1000 | Loss: 0.00003011
Iteration 65/1000 | Loss: 0.00003011
Iteration 66/1000 | Loss: 0.00003011
Iteration 67/1000 | Loss: 0.00003011
Iteration 68/1000 | Loss: 0.00003011
Iteration 69/1000 | Loss: 0.00003010
Iteration 70/1000 | Loss: 0.00003010
Iteration 71/1000 | Loss: 0.00003010
Iteration 72/1000 | Loss: 0.00003010
Iteration 73/1000 | Loss: 0.00003010
Iteration 74/1000 | Loss: 0.00003010
Iteration 75/1000 | Loss: 0.00003010
Iteration 76/1000 | Loss: 0.00003009
Iteration 77/1000 | Loss: 0.00003009
Iteration 78/1000 | Loss: 0.00003009
Iteration 79/1000 | Loss: 0.00003009
Iteration 80/1000 | Loss: 0.00003009
Iteration 81/1000 | Loss: 0.00003009
Iteration 82/1000 | Loss: 0.00003009
Iteration 83/1000 | Loss: 0.00003009
Iteration 84/1000 | Loss: 0.00003009
Iteration 85/1000 | Loss: 0.00003008
Iteration 86/1000 | Loss: 0.00003008
Iteration 87/1000 | Loss: 0.00003008
Iteration 88/1000 | Loss: 0.00003008
Iteration 89/1000 | Loss: 0.00003008
Iteration 90/1000 | Loss: 0.00003008
Iteration 91/1000 | Loss: 0.00003008
Iteration 92/1000 | Loss: 0.00003008
Iteration 93/1000 | Loss: 0.00003008
Iteration 94/1000 | Loss: 0.00003007
Iteration 95/1000 | Loss: 0.00003007
Iteration 96/1000 | Loss: 0.00003007
Iteration 97/1000 | Loss: 0.00003007
Iteration 98/1000 | Loss: 0.00003007
Iteration 99/1000 | Loss: 0.00003007
Iteration 100/1000 | Loss: 0.00003007
Iteration 101/1000 | Loss: 0.00003007
Iteration 102/1000 | Loss: 0.00003007
Iteration 103/1000 | Loss: 0.00003007
Iteration 104/1000 | Loss: 0.00003007
Iteration 105/1000 | Loss: 0.00003007
Iteration 106/1000 | Loss: 0.00003007
Iteration 107/1000 | Loss: 0.00003007
Iteration 108/1000 | Loss: 0.00003006
Iteration 109/1000 | Loss: 0.00003006
Iteration 110/1000 | Loss: 0.00003006
Iteration 111/1000 | Loss: 0.00003006
Iteration 112/1000 | Loss: 0.00003006
Iteration 113/1000 | Loss: 0.00003006
Iteration 114/1000 | Loss: 0.00003006
Iteration 115/1000 | Loss: 0.00003006
Iteration 116/1000 | Loss: 0.00003006
Iteration 117/1000 | Loss: 0.00003006
Iteration 118/1000 | Loss: 0.00003006
Iteration 119/1000 | Loss: 0.00003006
Iteration 120/1000 | Loss: 0.00003006
Iteration 121/1000 | Loss: 0.00003006
Iteration 122/1000 | Loss: 0.00003006
Iteration 123/1000 | Loss: 0.00003006
Iteration 124/1000 | Loss: 0.00003006
Iteration 125/1000 | Loss: 0.00003006
Iteration 126/1000 | Loss: 0.00003006
Iteration 127/1000 | Loss: 0.00003006
Iteration 128/1000 | Loss: 0.00003006
Iteration 129/1000 | Loss: 0.00003006
Iteration 130/1000 | Loss: 0.00003006
Iteration 131/1000 | Loss: 0.00003006
Iteration 132/1000 | Loss: 0.00003006
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [3.0061943107284606e-05, 3.0061943107284606e-05, 3.0061943107284606e-05, 3.0061943107284606e-05, 3.0061943107284606e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0061943107284606e-05

Optimization complete. Final v2v error: 4.792196750640869 mm

Highest mean error: 4.978178024291992 mm for frame 238

Lowest mean error: 4.593047142028809 mm for frame 163

Saving results

Total time: 41.56920289993286
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_0196/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0196/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0196/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01117953
Iteration 2/25 | Loss: 0.00384643
Iteration 3/25 | Loss: 0.00337488
Iteration 4/25 | Loss: 0.00306198
Iteration 5/25 | Loss: 0.00270449
Iteration 6/25 | Loss: 0.00284059
Iteration 7/25 | Loss: 0.00249094
Iteration 8/25 | Loss: 0.00234479
Iteration 9/25 | Loss: 0.00227743
Iteration 10/25 | Loss: 0.00225392
Iteration 11/25 | Loss: 0.00220775
Iteration 12/25 | Loss: 0.00220360
Iteration 13/25 | Loss: 0.00218568
Iteration 14/25 | Loss: 0.00216156
Iteration 15/25 | Loss: 0.00217085
Iteration 16/25 | Loss: 0.00214441
Iteration 17/25 | Loss: 0.00214062
Iteration 18/25 | Loss: 0.00214227
Iteration 19/25 | Loss: 0.00213498
Iteration 20/25 | Loss: 0.00213322
Iteration 21/25 | Loss: 0.00213588
Iteration 22/25 | Loss: 0.00212896
Iteration 23/25 | Loss: 0.00212838
Iteration 24/25 | Loss: 0.00212816
Iteration 25/25 | Loss: 0.00212795

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63465333
Iteration 2/25 | Loss: 0.01077216
Iteration 3/25 | Loss: 0.01077216
Iteration 4/25 | Loss: 0.01077216
Iteration 5/25 | Loss: 0.01077216
Iteration 6/25 | Loss: 0.01077216
Iteration 7/25 | Loss: 0.01077216
Iteration 8/25 | Loss: 0.01077216
Iteration 9/25 | Loss: 0.01077216
Iteration 10/25 | Loss: 0.01077216
Iteration 11/25 | Loss: 0.01077216
Iteration 12/25 | Loss: 0.01077216
Iteration 13/25 | Loss: 0.01077216
Iteration 14/25 | Loss: 0.01077216
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.010772157460451126, 0.010772157460451126, 0.010772157460451126, 0.010772157460451126, 0.010772157460451126]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.010772157460451126

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.01077216
Iteration 2/1000 | Loss: 0.00098714
Iteration 3/1000 | Loss: 0.00072167
Iteration 4/1000 | Loss: 0.00058396
Iteration 5/1000 | Loss: 0.00182423
Iteration 6/1000 | Loss: 0.00704383
Iteration 7/1000 | Loss: 0.00977057
Iteration 8/1000 | Loss: 0.00714018
Iteration 9/1000 | Loss: 0.00184995
Iteration 10/1000 | Loss: 0.00121730
Iteration 11/1000 | Loss: 0.00076853
Iteration 12/1000 | Loss: 0.00189772
Iteration 13/1000 | Loss: 0.00362284
Iteration 14/1000 | Loss: 0.00475457
Iteration 15/1000 | Loss: 0.00088553
Iteration 16/1000 | Loss: 0.00162024
Iteration 17/1000 | Loss: 0.00047993
Iteration 18/1000 | Loss: 0.00033335
Iteration 19/1000 | Loss: 0.00028443
Iteration 20/1000 | Loss: 0.00020306
Iteration 21/1000 | Loss: 0.00056646
Iteration 22/1000 | Loss: 0.00017395
Iteration 23/1000 | Loss: 0.00035135
Iteration 24/1000 | Loss: 0.00056737
Iteration 25/1000 | Loss: 0.00066933
Iteration 26/1000 | Loss: 0.00033687
Iteration 27/1000 | Loss: 0.00044891
Iteration 28/1000 | Loss: 0.00027658
Iteration 29/1000 | Loss: 0.00041375
Iteration 30/1000 | Loss: 0.00116009
Iteration 31/1000 | Loss: 0.00041573
Iteration 32/1000 | Loss: 0.00067572
Iteration 33/1000 | Loss: 0.00032100
Iteration 34/1000 | Loss: 0.00022217
Iteration 35/1000 | Loss: 0.00061304
Iteration 36/1000 | Loss: 0.00066992
Iteration 37/1000 | Loss: 0.00072906
Iteration 38/1000 | Loss: 0.00040720
Iteration 39/1000 | Loss: 0.00075389
Iteration 40/1000 | Loss: 0.00054055
Iteration 41/1000 | Loss: 0.00014843
Iteration 42/1000 | Loss: 0.00032330
Iteration 43/1000 | Loss: 0.00008511
Iteration 44/1000 | Loss: 0.00047436
Iteration 45/1000 | Loss: 0.00076919
Iteration 46/1000 | Loss: 0.00103472
Iteration 47/1000 | Loss: 0.00056383
Iteration 48/1000 | Loss: 0.00061418
Iteration 49/1000 | Loss: 0.00018114
Iteration 50/1000 | Loss: 0.00036610
Iteration 51/1000 | Loss: 0.00027813
Iteration 52/1000 | Loss: 0.00027773
Iteration 53/1000 | Loss: 0.00038061
Iteration 54/1000 | Loss: 0.00005719
Iteration 55/1000 | Loss: 0.00004938
Iteration 56/1000 | Loss: 0.00004197
Iteration 57/1000 | Loss: 0.00003729
Iteration 58/1000 | Loss: 0.00038100
Iteration 59/1000 | Loss: 0.00004644
Iteration 60/1000 | Loss: 0.00003668
Iteration 61/1000 | Loss: 0.00003241
Iteration 62/1000 | Loss: 0.00026285
Iteration 63/1000 | Loss: 0.00004494
Iteration 64/1000 | Loss: 0.00003712
Iteration 65/1000 | Loss: 0.00003216
Iteration 66/1000 | Loss: 0.00003099
Iteration 67/1000 | Loss: 0.00003017
Iteration 68/1000 | Loss: 0.00002905
Iteration 69/1000 | Loss: 0.00006508
Iteration 70/1000 | Loss: 0.00003780
Iteration 71/1000 | Loss: 0.00003240
Iteration 72/1000 | Loss: 0.00002884
Iteration 73/1000 | Loss: 0.00002809
Iteration 74/1000 | Loss: 0.00002738
Iteration 75/1000 | Loss: 0.00002687
Iteration 76/1000 | Loss: 0.00002651
Iteration 77/1000 | Loss: 0.00002620
Iteration 78/1000 | Loss: 0.00002609
Iteration 79/1000 | Loss: 0.00002605
Iteration 80/1000 | Loss: 0.00002604
Iteration 81/1000 | Loss: 0.00002604
Iteration 82/1000 | Loss: 0.00002604
Iteration 83/1000 | Loss: 0.00002604
Iteration 84/1000 | Loss: 0.00002603
Iteration 85/1000 | Loss: 0.00002603
Iteration 86/1000 | Loss: 0.00002603
Iteration 87/1000 | Loss: 0.00002603
Iteration 88/1000 | Loss: 0.00002603
Iteration 89/1000 | Loss: 0.00002601
Iteration 90/1000 | Loss: 0.00002598
Iteration 91/1000 | Loss: 0.00002598
Iteration 92/1000 | Loss: 0.00002595
Iteration 93/1000 | Loss: 0.00002594
Iteration 94/1000 | Loss: 0.00002593
Iteration 95/1000 | Loss: 0.00002593
Iteration 96/1000 | Loss: 0.00002590
Iteration 97/1000 | Loss: 0.00002590
Iteration 98/1000 | Loss: 0.00002589
Iteration 99/1000 | Loss: 0.00002588
Iteration 100/1000 | Loss: 0.00002588
Iteration 101/1000 | Loss: 0.00002588
Iteration 102/1000 | Loss: 0.00002588
Iteration 103/1000 | Loss: 0.00002588
Iteration 104/1000 | Loss: 0.00002587
Iteration 105/1000 | Loss: 0.00002587
Iteration 106/1000 | Loss: 0.00002587
Iteration 107/1000 | Loss: 0.00002587
Iteration 108/1000 | Loss: 0.00002586
Iteration 109/1000 | Loss: 0.00002583
Iteration 110/1000 | Loss: 0.00002583
Iteration 111/1000 | Loss: 0.00002583
Iteration 112/1000 | Loss: 0.00002583
Iteration 113/1000 | Loss: 0.00002583
Iteration 114/1000 | Loss: 0.00002583
Iteration 115/1000 | Loss: 0.00002583
Iteration 116/1000 | Loss: 0.00002583
Iteration 117/1000 | Loss: 0.00002583
Iteration 118/1000 | Loss: 0.00002583
Iteration 119/1000 | Loss: 0.00002582
Iteration 120/1000 | Loss: 0.00002582
Iteration 121/1000 | Loss: 0.00002581
Iteration 122/1000 | Loss: 0.00002581
Iteration 123/1000 | Loss: 0.00002580
Iteration 124/1000 | Loss: 0.00002580
Iteration 125/1000 | Loss: 0.00002580
Iteration 126/1000 | Loss: 0.00002579
Iteration 127/1000 | Loss: 0.00002579
Iteration 128/1000 | Loss: 0.00002579
Iteration 129/1000 | Loss: 0.00002578
Iteration 130/1000 | Loss: 0.00002578
Iteration 131/1000 | Loss: 0.00002577
Iteration 132/1000 | Loss: 0.00002577
Iteration 133/1000 | Loss: 0.00002577
Iteration 134/1000 | Loss: 0.00002577
Iteration 135/1000 | Loss: 0.00002577
Iteration 136/1000 | Loss: 0.00002577
Iteration 137/1000 | Loss: 0.00002576
Iteration 138/1000 | Loss: 0.00002576
Iteration 139/1000 | Loss: 0.00002576
Iteration 140/1000 | Loss: 0.00002576
Iteration 141/1000 | Loss: 0.00002575
Iteration 142/1000 | Loss: 0.00002575
Iteration 143/1000 | Loss: 0.00002575
Iteration 144/1000 | Loss: 0.00002575
Iteration 145/1000 | Loss: 0.00002575
Iteration 146/1000 | Loss: 0.00002575
Iteration 147/1000 | Loss: 0.00002575
Iteration 148/1000 | Loss: 0.00002575
Iteration 149/1000 | Loss: 0.00002574
Iteration 150/1000 | Loss: 0.00002574
Iteration 151/1000 | Loss: 0.00002574
Iteration 152/1000 | Loss: 0.00002574
Iteration 153/1000 | Loss: 0.00002574
Iteration 154/1000 | Loss: 0.00002574
Iteration 155/1000 | Loss: 0.00002573
Iteration 156/1000 | Loss: 0.00002573
Iteration 157/1000 | Loss: 0.00002573
Iteration 158/1000 | Loss: 0.00002573
Iteration 159/1000 | Loss: 0.00002573
Iteration 160/1000 | Loss: 0.00002573
Iteration 161/1000 | Loss: 0.00002573
Iteration 162/1000 | Loss: 0.00002573
Iteration 163/1000 | Loss: 0.00002573
Iteration 164/1000 | Loss: 0.00002573
Iteration 165/1000 | Loss: 0.00002573
Iteration 166/1000 | Loss: 0.00002573
Iteration 167/1000 | Loss: 0.00002573
Iteration 168/1000 | Loss: 0.00002572
Iteration 169/1000 | Loss: 0.00002572
Iteration 170/1000 | Loss: 0.00002572
Iteration 171/1000 | Loss: 0.00002572
Iteration 172/1000 | Loss: 0.00002572
Iteration 173/1000 | Loss: 0.00002572
Iteration 174/1000 | Loss: 0.00002571
Iteration 175/1000 | Loss: 0.00002571
Iteration 176/1000 | Loss: 0.00002571
Iteration 177/1000 | Loss: 0.00002571
Iteration 178/1000 | Loss: 0.00002571
Iteration 179/1000 | Loss: 0.00002571
Iteration 180/1000 | Loss: 0.00002571
Iteration 181/1000 | Loss: 0.00002571
Iteration 182/1000 | Loss: 0.00002571
Iteration 183/1000 | Loss: 0.00002571
Iteration 184/1000 | Loss: 0.00002571
Iteration 185/1000 | Loss: 0.00002571
Iteration 186/1000 | Loss: 0.00002571
Iteration 187/1000 | Loss: 0.00002571
Iteration 188/1000 | Loss: 0.00002570
Iteration 189/1000 | Loss: 0.00002570
Iteration 190/1000 | Loss: 0.00002570
Iteration 191/1000 | Loss: 0.00002570
Iteration 192/1000 | Loss: 0.00002570
Iteration 193/1000 | Loss: 0.00002570
Iteration 194/1000 | Loss: 0.00002570
Iteration 195/1000 | Loss: 0.00002570
Iteration 196/1000 | Loss: 0.00002570
Iteration 197/1000 | Loss: 0.00002570
Iteration 198/1000 | Loss: 0.00002570
Iteration 199/1000 | Loss: 0.00002569
Iteration 200/1000 | Loss: 0.00002569
Iteration 201/1000 | Loss: 0.00002569
Iteration 202/1000 | Loss: 0.00002569
Iteration 203/1000 | Loss: 0.00002569
Iteration 204/1000 | Loss: 0.00002569
Iteration 205/1000 | Loss: 0.00002569
Iteration 206/1000 | Loss: 0.00002569
Iteration 207/1000 | Loss: 0.00002569
Iteration 208/1000 | Loss: 0.00002569
Iteration 209/1000 | Loss: 0.00002569
Iteration 210/1000 | Loss: 0.00002569
Iteration 211/1000 | Loss: 0.00002569
Iteration 212/1000 | Loss: 0.00002569
Iteration 213/1000 | Loss: 0.00002569
Iteration 214/1000 | Loss: 0.00002569
Iteration 215/1000 | Loss: 0.00002569
Iteration 216/1000 | Loss: 0.00002569
Iteration 217/1000 | Loss: 0.00002569
Iteration 218/1000 | Loss: 0.00002569
Iteration 219/1000 | Loss: 0.00002569
Iteration 220/1000 | Loss: 0.00002569
Iteration 221/1000 | Loss: 0.00002569
Iteration 222/1000 | Loss: 0.00002569
Iteration 223/1000 | Loss: 0.00002569
Iteration 224/1000 | Loss: 0.00002569
Iteration 225/1000 | Loss: 0.00002569
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [2.569095158833079e-05, 2.569095158833079e-05, 2.569095158833079e-05, 2.569095158833079e-05, 2.569095158833079e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.569095158833079e-05

Optimization complete. Final v2v error: 4.34515905380249 mm

Highest mean error: 10.798005104064941 mm for frame 50

Lowest mean error: 4.008954048156738 mm for frame 65

Saving results

Total time: 160.3526895046234
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_0196/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0196/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0196/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00453032
Iteration 2/25 | Loss: 0.00204008
Iteration 3/25 | Loss: 0.00161239
Iteration 4/25 | Loss: 0.00153881
Iteration 5/25 | Loss: 0.00152955
Iteration 6/25 | Loss: 0.00152792
Iteration 7/25 | Loss: 0.00152768
Iteration 8/25 | Loss: 0.00152768
Iteration 9/25 | Loss: 0.00152768
Iteration 10/25 | Loss: 0.00152768
Iteration 11/25 | Loss: 0.00152768
Iteration 12/25 | Loss: 0.00152768
Iteration 13/25 | Loss: 0.00152768
Iteration 14/25 | Loss: 0.00152768
Iteration 15/25 | Loss: 0.00152768
Iteration 16/25 | Loss: 0.00152768
Iteration 17/25 | Loss: 0.00152768
Iteration 18/25 | Loss: 0.00152768
Iteration 19/25 | Loss: 0.00152768
Iteration 20/25 | Loss: 0.00152768
Iteration 21/25 | Loss: 0.00152768
Iteration 22/25 | Loss: 0.00152768
Iteration 23/25 | Loss: 0.00152768
Iteration 24/25 | Loss: 0.00152768
Iteration 25/25 | Loss: 0.00152768

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66508913
Iteration 2/25 | Loss: 0.00302025
Iteration 3/25 | Loss: 0.00302025
Iteration 4/25 | Loss: 0.00302025
Iteration 5/25 | Loss: 0.00302025
Iteration 6/25 | Loss: 0.00302025
Iteration 7/25 | Loss: 0.00302025
Iteration 8/25 | Loss: 0.00302025
Iteration 9/25 | Loss: 0.00302025
Iteration 10/25 | Loss: 0.00302025
Iteration 11/25 | Loss: 0.00302025
Iteration 12/25 | Loss: 0.00302025
Iteration 13/25 | Loss: 0.00302025
Iteration 14/25 | Loss: 0.00302025
Iteration 15/25 | Loss: 0.00302025
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0030202476773411036, 0.0030202476773411036, 0.0030202476773411036, 0.0030202476773411036, 0.0030202476773411036]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0030202476773411036

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00302025
Iteration 2/1000 | Loss: 0.00005065
Iteration 3/1000 | Loss: 0.00003557
Iteration 4/1000 | Loss: 0.00002917
Iteration 5/1000 | Loss: 0.00002686
Iteration 6/1000 | Loss: 0.00002530
Iteration 7/1000 | Loss: 0.00002436
Iteration 8/1000 | Loss: 0.00002388
Iteration 9/1000 | Loss: 0.00002353
Iteration 10/1000 | Loss: 0.00002320
Iteration 11/1000 | Loss: 0.00002300
Iteration 12/1000 | Loss: 0.00002281
Iteration 13/1000 | Loss: 0.00002281
Iteration 14/1000 | Loss: 0.00002277
Iteration 15/1000 | Loss: 0.00002274
Iteration 16/1000 | Loss: 0.00002274
Iteration 17/1000 | Loss: 0.00002274
Iteration 18/1000 | Loss: 0.00002274
Iteration 19/1000 | Loss: 0.00002274
Iteration 20/1000 | Loss: 0.00002274
Iteration 21/1000 | Loss: 0.00002274
Iteration 22/1000 | Loss: 0.00002274
Iteration 23/1000 | Loss: 0.00002273
Iteration 24/1000 | Loss: 0.00002273
Iteration 25/1000 | Loss: 0.00002273
Iteration 26/1000 | Loss: 0.00002273
Iteration 27/1000 | Loss: 0.00002272
Iteration 28/1000 | Loss: 0.00002272
Iteration 29/1000 | Loss: 0.00002272
Iteration 30/1000 | Loss: 0.00002271
Iteration 31/1000 | Loss: 0.00002271
Iteration 32/1000 | Loss: 0.00002270
Iteration 33/1000 | Loss: 0.00002270
Iteration 34/1000 | Loss: 0.00002269
Iteration 35/1000 | Loss: 0.00002269
Iteration 36/1000 | Loss: 0.00002269
Iteration 37/1000 | Loss: 0.00002268
Iteration 38/1000 | Loss: 0.00002268
Iteration 39/1000 | Loss: 0.00002267
Iteration 40/1000 | Loss: 0.00002267
Iteration 41/1000 | Loss: 0.00002266
Iteration 42/1000 | Loss: 0.00002265
Iteration 43/1000 | Loss: 0.00002264
Iteration 44/1000 | Loss: 0.00002264
Iteration 45/1000 | Loss: 0.00002264
Iteration 46/1000 | Loss: 0.00002263
Iteration 47/1000 | Loss: 0.00002263
Iteration 48/1000 | Loss: 0.00002263
Iteration 49/1000 | Loss: 0.00002262
Iteration 50/1000 | Loss: 0.00002262
Iteration 51/1000 | Loss: 0.00002262
Iteration 52/1000 | Loss: 0.00002261
Iteration 53/1000 | Loss: 0.00002261
Iteration 54/1000 | Loss: 0.00002261
Iteration 55/1000 | Loss: 0.00002260
Iteration 56/1000 | Loss: 0.00002260
Iteration 57/1000 | Loss: 0.00002260
Iteration 58/1000 | Loss: 0.00002260
Iteration 59/1000 | Loss: 0.00002259
Iteration 60/1000 | Loss: 0.00002259
Iteration 61/1000 | Loss: 0.00002259
Iteration 62/1000 | Loss: 0.00002259
Iteration 63/1000 | Loss: 0.00002258
Iteration 64/1000 | Loss: 0.00002258
Iteration 65/1000 | Loss: 0.00002258
Iteration 66/1000 | Loss: 0.00002258
Iteration 67/1000 | Loss: 0.00002258
Iteration 68/1000 | Loss: 0.00002258
Iteration 69/1000 | Loss: 0.00002258
Iteration 70/1000 | Loss: 0.00002258
Iteration 71/1000 | Loss: 0.00002258
Iteration 72/1000 | Loss: 0.00002258
Iteration 73/1000 | Loss: 0.00002258
Iteration 74/1000 | Loss: 0.00002257
Iteration 75/1000 | Loss: 0.00002257
Iteration 76/1000 | Loss: 0.00002257
Iteration 77/1000 | Loss: 0.00002257
Iteration 78/1000 | Loss: 0.00002257
Iteration 79/1000 | Loss: 0.00002257
Iteration 80/1000 | Loss: 0.00002256
Iteration 81/1000 | Loss: 0.00002256
Iteration 82/1000 | Loss: 0.00002256
Iteration 83/1000 | Loss: 0.00002256
Iteration 84/1000 | Loss: 0.00002256
Iteration 85/1000 | Loss: 0.00002256
Iteration 86/1000 | Loss: 0.00002256
Iteration 87/1000 | Loss: 0.00002256
Iteration 88/1000 | Loss: 0.00002256
Iteration 89/1000 | Loss: 0.00002256
Iteration 90/1000 | Loss: 0.00002256
Iteration 91/1000 | Loss: 0.00002256
Iteration 92/1000 | Loss: 0.00002255
Iteration 93/1000 | Loss: 0.00002255
Iteration 94/1000 | Loss: 0.00002255
Iteration 95/1000 | Loss: 0.00002255
Iteration 96/1000 | Loss: 0.00002255
Iteration 97/1000 | Loss: 0.00002254
Iteration 98/1000 | Loss: 0.00002254
Iteration 99/1000 | Loss: 0.00002254
Iteration 100/1000 | Loss: 0.00002254
Iteration 101/1000 | Loss: 0.00002253
Iteration 102/1000 | Loss: 0.00002253
Iteration 103/1000 | Loss: 0.00002253
Iteration 104/1000 | Loss: 0.00002252
Iteration 105/1000 | Loss: 0.00002252
Iteration 106/1000 | Loss: 0.00002252
Iteration 107/1000 | Loss: 0.00002252
Iteration 108/1000 | Loss: 0.00002252
Iteration 109/1000 | Loss: 0.00002252
Iteration 110/1000 | Loss: 0.00002252
Iteration 111/1000 | Loss: 0.00002252
Iteration 112/1000 | Loss: 0.00002251
Iteration 113/1000 | Loss: 0.00002251
Iteration 114/1000 | Loss: 0.00002251
Iteration 115/1000 | Loss: 0.00002251
Iteration 116/1000 | Loss: 0.00002251
Iteration 117/1000 | Loss: 0.00002251
Iteration 118/1000 | Loss: 0.00002251
Iteration 119/1000 | Loss: 0.00002251
Iteration 120/1000 | Loss: 0.00002251
Iteration 121/1000 | Loss: 0.00002251
Iteration 122/1000 | Loss: 0.00002251
Iteration 123/1000 | Loss: 0.00002251
Iteration 124/1000 | Loss: 0.00002251
Iteration 125/1000 | Loss: 0.00002250
Iteration 126/1000 | Loss: 0.00002250
Iteration 127/1000 | Loss: 0.00002250
Iteration 128/1000 | Loss: 0.00002250
Iteration 129/1000 | Loss: 0.00002250
Iteration 130/1000 | Loss: 0.00002250
Iteration 131/1000 | Loss: 0.00002250
Iteration 132/1000 | Loss: 0.00002250
Iteration 133/1000 | Loss: 0.00002250
Iteration 134/1000 | Loss: 0.00002250
Iteration 135/1000 | Loss: 0.00002250
Iteration 136/1000 | Loss: 0.00002250
Iteration 137/1000 | Loss: 0.00002250
Iteration 138/1000 | Loss: 0.00002250
Iteration 139/1000 | Loss: 0.00002250
Iteration 140/1000 | Loss: 0.00002250
Iteration 141/1000 | Loss: 0.00002250
Iteration 142/1000 | Loss: 0.00002249
Iteration 143/1000 | Loss: 0.00002249
Iteration 144/1000 | Loss: 0.00002249
Iteration 145/1000 | Loss: 0.00002249
Iteration 146/1000 | Loss: 0.00002249
Iteration 147/1000 | Loss: 0.00002249
Iteration 148/1000 | Loss: 0.00002249
Iteration 149/1000 | Loss: 0.00002249
Iteration 150/1000 | Loss: 0.00002248
Iteration 151/1000 | Loss: 0.00002248
Iteration 152/1000 | Loss: 0.00002248
Iteration 153/1000 | Loss: 0.00002248
Iteration 154/1000 | Loss: 0.00002248
Iteration 155/1000 | Loss: 0.00002248
Iteration 156/1000 | Loss: 0.00002248
Iteration 157/1000 | Loss: 0.00002248
Iteration 158/1000 | Loss: 0.00002248
Iteration 159/1000 | Loss: 0.00002247
Iteration 160/1000 | Loss: 0.00002247
Iteration 161/1000 | Loss: 0.00002247
Iteration 162/1000 | Loss: 0.00002247
Iteration 163/1000 | Loss: 0.00002247
Iteration 164/1000 | Loss: 0.00002247
Iteration 165/1000 | Loss: 0.00002247
Iteration 166/1000 | Loss: 0.00002247
Iteration 167/1000 | Loss: 0.00002246
Iteration 168/1000 | Loss: 0.00002246
Iteration 169/1000 | Loss: 0.00002246
Iteration 170/1000 | Loss: 0.00002246
Iteration 171/1000 | Loss: 0.00002246
Iteration 172/1000 | Loss: 0.00002246
Iteration 173/1000 | Loss: 0.00002246
Iteration 174/1000 | Loss: 0.00002246
Iteration 175/1000 | Loss: 0.00002246
Iteration 176/1000 | Loss: 0.00002246
Iteration 177/1000 | Loss: 0.00002246
Iteration 178/1000 | Loss: 0.00002245
Iteration 179/1000 | Loss: 0.00002245
Iteration 180/1000 | Loss: 0.00002244
Iteration 181/1000 | Loss: 0.00002244
Iteration 182/1000 | Loss: 0.00002244
Iteration 183/1000 | Loss: 0.00002243
Iteration 184/1000 | Loss: 0.00002243
Iteration 185/1000 | Loss: 0.00002243
Iteration 186/1000 | Loss: 0.00002243
Iteration 187/1000 | Loss: 0.00002242
Iteration 188/1000 | Loss: 0.00002242
Iteration 189/1000 | Loss: 0.00002242
Iteration 190/1000 | Loss: 0.00002242
Iteration 191/1000 | Loss: 0.00002242
Iteration 192/1000 | Loss: 0.00002242
Iteration 193/1000 | Loss: 0.00002242
Iteration 194/1000 | Loss: 0.00002241
Iteration 195/1000 | Loss: 0.00002241
Iteration 196/1000 | Loss: 0.00002241
Iteration 197/1000 | Loss: 0.00002241
Iteration 198/1000 | Loss: 0.00002240
Iteration 199/1000 | Loss: 0.00002240
Iteration 200/1000 | Loss: 0.00002240
Iteration 201/1000 | Loss: 0.00002240
Iteration 202/1000 | Loss: 0.00002239
Iteration 203/1000 | Loss: 0.00002239
Iteration 204/1000 | Loss: 0.00002239
Iteration 205/1000 | Loss: 0.00002239
Iteration 206/1000 | Loss: 0.00002239
Iteration 207/1000 | Loss: 0.00002238
Iteration 208/1000 | Loss: 0.00002238
Iteration 209/1000 | Loss: 0.00002238
Iteration 210/1000 | Loss: 0.00002238
Iteration 211/1000 | Loss: 0.00002238
Iteration 212/1000 | Loss: 0.00002238
Iteration 213/1000 | Loss: 0.00002238
Iteration 214/1000 | Loss: 0.00002237
Iteration 215/1000 | Loss: 0.00002237
Iteration 216/1000 | Loss: 0.00002237
Iteration 217/1000 | Loss: 0.00002237
Iteration 218/1000 | Loss: 0.00002237
Iteration 219/1000 | Loss: 0.00002237
Iteration 220/1000 | Loss: 0.00002236
Iteration 221/1000 | Loss: 0.00002236
Iteration 222/1000 | Loss: 0.00002236
Iteration 223/1000 | Loss: 0.00002236
Iteration 224/1000 | Loss: 0.00002236
Iteration 225/1000 | Loss: 0.00002236
Iteration 226/1000 | Loss: 0.00002236
Iteration 227/1000 | Loss: 0.00002235
Iteration 228/1000 | Loss: 0.00002235
Iteration 229/1000 | Loss: 0.00002235
Iteration 230/1000 | Loss: 0.00002235
Iteration 231/1000 | Loss: 0.00002235
Iteration 232/1000 | Loss: 0.00002234
Iteration 233/1000 | Loss: 0.00002234
Iteration 234/1000 | Loss: 0.00002234
Iteration 235/1000 | Loss: 0.00002234
Iteration 236/1000 | Loss: 0.00002234
Iteration 237/1000 | Loss: 0.00002234
Iteration 238/1000 | Loss: 0.00002234
Iteration 239/1000 | Loss: 0.00002234
Iteration 240/1000 | Loss: 0.00002234
Iteration 241/1000 | Loss: 0.00002233
Iteration 242/1000 | Loss: 0.00002233
Iteration 243/1000 | Loss: 0.00002233
Iteration 244/1000 | Loss: 0.00002233
Iteration 245/1000 | Loss: 0.00002233
Iteration 246/1000 | Loss: 0.00002233
Iteration 247/1000 | Loss: 0.00002233
Iteration 248/1000 | Loss: 0.00002233
Iteration 249/1000 | Loss: 0.00002233
Iteration 250/1000 | Loss: 0.00002233
Iteration 251/1000 | Loss: 0.00002233
Iteration 252/1000 | Loss: 0.00002233
Iteration 253/1000 | Loss: 0.00002233
Iteration 254/1000 | Loss: 0.00002233
Iteration 255/1000 | Loss: 0.00002233
Iteration 256/1000 | Loss: 0.00002233
Iteration 257/1000 | Loss: 0.00002233
Iteration 258/1000 | Loss: 0.00002233
Iteration 259/1000 | Loss: 0.00002233
Iteration 260/1000 | Loss: 0.00002233
Iteration 261/1000 | Loss: 0.00002232
Iteration 262/1000 | Loss: 0.00002232
Iteration 263/1000 | Loss: 0.00002232
Iteration 264/1000 | Loss: 0.00002232
Iteration 265/1000 | Loss: 0.00002232
Iteration 266/1000 | Loss: 0.00002232
Iteration 267/1000 | Loss: 0.00002232
Iteration 268/1000 | Loss: 0.00002232
Iteration 269/1000 | Loss: 0.00002232
Iteration 270/1000 | Loss: 0.00002232
Iteration 271/1000 | Loss: 0.00002232
Iteration 272/1000 | Loss: 0.00002232
Iteration 273/1000 | Loss: 0.00002232
Iteration 274/1000 | Loss: 0.00002232
Iteration 275/1000 | Loss: 0.00002232
Iteration 276/1000 | Loss: 0.00002232
Iteration 277/1000 | Loss: 0.00002232
Iteration 278/1000 | Loss: 0.00002232
Iteration 279/1000 | Loss: 0.00002232
Iteration 280/1000 | Loss: 0.00002232
Iteration 281/1000 | Loss: 0.00002232
Iteration 282/1000 | Loss: 0.00002232
Iteration 283/1000 | Loss: 0.00002232
Iteration 284/1000 | Loss: 0.00002232
Iteration 285/1000 | Loss: 0.00002232
Iteration 286/1000 | Loss: 0.00002232
Iteration 287/1000 | Loss: 0.00002232
Iteration 288/1000 | Loss: 0.00002232
Iteration 289/1000 | Loss: 0.00002232
Iteration 290/1000 | Loss: 0.00002232
Iteration 291/1000 | Loss: 0.00002232
Iteration 292/1000 | Loss: 0.00002232
Iteration 293/1000 | Loss: 0.00002232
Iteration 294/1000 | Loss: 0.00002232
Iteration 295/1000 | Loss: 0.00002232
Iteration 296/1000 | Loss: 0.00002232
Iteration 297/1000 | Loss: 0.00002232
Iteration 298/1000 | Loss: 0.00002232
Iteration 299/1000 | Loss: 0.00002232
Iteration 300/1000 | Loss: 0.00002232
Iteration 301/1000 | Loss: 0.00002232
Iteration 302/1000 | Loss: 0.00002232
Iteration 303/1000 | Loss: 0.00002232
Iteration 304/1000 | Loss: 0.00002232
Iteration 305/1000 | Loss: 0.00002232
Iteration 306/1000 | Loss: 0.00002232
Iteration 307/1000 | Loss: 0.00002232
Iteration 308/1000 | Loss: 0.00002232
Iteration 309/1000 | Loss: 0.00002232
Iteration 310/1000 | Loss: 0.00002232
Iteration 311/1000 | Loss: 0.00002232
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 311. Stopping optimization.
Last 5 losses: [2.231726102763787e-05, 2.231726102763787e-05, 2.231726102763787e-05, 2.231726102763787e-05, 2.231726102763787e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.231726102763787e-05

Optimization complete. Final v2v error: 4.100277423858643 mm

Highest mean error: 4.302903175354004 mm for frame 7

Lowest mean error: 3.9373114109039307 mm for frame 125

Saving results

Total time: 43.74997925758362
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_0196/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0196/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0196/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01224253
Iteration 2/25 | Loss: 0.00204249
Iteration 3/25 | Loss: 0.00170657
Iteration 4/25 | Loss: 0.00166386
Iteration 5/25 | Loss: 0.00163559
Iteration 6/25 | Loss: 0.00163092
Iteration 7/25 | Loss: 0.00162656
Iteration 8/25 | Loss: 0.00162623
Iteration 9/25 | Loss: 0.00162612
Iteration 10/25 | Loss: 0.00162611
Iteration 11/25 | Loss: 0.00162611
Iteration 12/25 | Loss: 0.00162611
Iteration 13/25 | Loss: 0.00162611
Iteration 14/25 | Loss: 0.00162611
Iteration 15/25 | Loss: 0.00162611
Iteration 16/25 | Loss: 0.00162611
Iteration 17/25 | Loss: 0.00162611
Iteration 18/25 | Loss: 0.00162611
Iteration 19/25 | Loss: 0.00162611
Iteration 20/25 | Loss: 0.00162611
Iteration 21/25 | Loss: 0.00162611
Iteration 22/25 | Loss: 0.00162611
Iteration 23/25 | Loss: 0.00162611
Iteration 24/25 | Loss: 0.00162611
Iteration 25/25 | Loss: 0.00162610

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66769421
Iteration 2/25 | Loss: 0.00288684
Iteration 3/25 | Loss: 0.00288684
Iteration 4/25 | Loss: 0.00288684
Iteration 5/25 | Loss: 0.00288684
Iteration 6/25 | Loss: 0.00288684
Iteration 7/25 | Loss: 0.00288684
Iteration 8/25 | Loss: 0.00288684
Iteration 9/25 | Loss: 0.00288684
Iteration 10/25 | Loss: 0.00288684
Iteration 11/25 | Loss: 0.00288684
Iteration 12/25 | Loss: 0.00288684
Iteration 13/25 | Loss: 0.00288684
Iteration 14/25 | Loss: 0.00288684
Iteration 15/25 | Loss: 0.00288684
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.002886835252866149, 0.002886835252866149, 0.002886835252866149, 0.002886835252866149, 0.002886835252866149]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002886835252866149

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00288684
Iteration 2/1000 | Loss: 0.00006232
Iteration 3/1000 | Loss: 0.00004860
Iteration 4/1000 | Loss: 0.00004281
Iteration 5/1000 | Loss: 0.00004092
Iteration 6/1000 | Loss: 0.00003970
Iteration 7/1000 | Loss: 0.00003902
Iteration 8/1000 | Loss: 0.00003838
Iteration 9/1000 | Loss: 0.00003799
Iteration 10/1000 | Loss: 0.00003774
Iteration 11/1000 | Loss: 0.00003752
Iteration 12/1000 | Loss: 0.00003751
Iteration 13/1000 | Loss: 0.00003746
Iteration 14/1000 | Loss: 0.00003745
Iteration 15/1000 | Loss: 0.00003743
Iteration 16/1000 | Loss: 0.00003743
Iteration 17/1000 | Loss: 0.00003741
Iteration 18/1000 | Loss: 0.00003741
Iteration 19/1000 | Loss: 0.00003740
Iteration 20/1000 | Loss: 0.00003740
Iteration 21/1000 | Loss: 0.00003739
Iteration 22/1000 | Loss: 0.00003738
Iteration 23/1000 | Loss: 0.00003738
Iteration 24/1000 | Loss: 0.00003738
Iteration 25/1000 | Loss: 0.00003738
Iteration 26/1000 | Loss: 0.00003738
Iteration 27/1000 | Loss: 0.00003738
Iteration 28/1000 | Loss: 0.00003737
Iteration 29/1000 | Loss: 0.00003737
Iteration 30/1000 | Loss: 0.00003737
Iteration 31/1000 | Loss: 0.00003737
Iteration 32/1000 | Loss: 0.00003737
Iteration 33/1000 | Loss: 0.00003736
Iteration 34/1000 | Loss: 0.00003736
Iteration 35/1000 | Loss: 0.00003736
Iteration 36/1000 | Loss: 0.00003735
Iteration 37/1000 | Loss: 0.00003735
Iteration 38/1000 | Loss: 0.00003734
Iteration 39/1000 | Loss: 0.00003733
Iteration 40/1000 | Loss: 0.00003733
Iteration 41/1000 | Loss: 0.00003733
Iteration 42/1000 | Loss: 0.00003733
Iteration 43/1000 | Loss: 0.00003732
Iteration 44/1000 | Loss: 0.00003732
Iteration 45/1000 | Loss: 0.00003732
Iteration 46/1000 | Loss: 0.00003732
Iteration 47/1000 | Loss: 0.00003731
Iteration 48/1000 | Loss: 0.00003731
Iteration 49/1000 | Loss: 0.00003731
Iteration 50/1000 | Loss: 0.00003731
Iteration 51/1000 | Loss: 0.00003731
Iteration 52/1000 | Loss: 0.00003731
Iteration 53/1000 | Loss: 0.00003730
Iteration 54/1000 | Loss: 0.00003730
Iteration 55/1000 | Loss: 0.00003730
Iteration 56/1000 | Loss: 0.00003730
Iteration 57/1000 | Loss: 0.00003730
Iteration 58/1000 | Loss: 0.00003730
Iteration 59/1000 | Loss: 0.00003730
Iteration 60/1000 | Loss: 0.00003729
Iteration 61/1000 | Loss: 0.00003729
Iteration 62/1000 | Loss: 0.00003729
Iteration 63/1000 | Loss: 0.00003729
Iteration 64/1000 | Loss: 0.00003729
Iteration 65/1000 | Loss: 0.00003728
Iteration 66/1000 | Loss: 0.00003728
Iteration 67/1000 | Loss: 0.00003728
Iteration 68/1000 | Loss: 0.00003728
Iteration 69/1000 | Loss: 0.00003728
Iteration 70/1000 | Loss: 0.00003728
Iteration 71/1000 | Loss: 0.00003728
Iteration 72/1000 | Loss: 0.00003727
Iteration 73/1000 | Loss: 0.00003727
Iteration 74/1000 | Loss: 0.00003727
Iteration 75/1000 | Loss: 0.00003727
Iteration 76/1000 | Loss: 0.00003727
Iteration 77/1000 | Loss: 0.00003727
Iteration 78/1000 | Loss: 0.00003727
Iteration 79/1000 | Loss: 0.00003727
Iteration 80/1000 | Loss: 0.00003727
Iteration 81/1000 | Loss: 0.00003726
Iteration 82/1000 | Loss: 0.00003726
Iteration 83/1000 | Loss: 0.00003726
Iteration 84/1000 | Loss: 0.00003726
Iteration 85/1000 | Loss: 0.00003726
Iteration 86/1000 | Loss: 0.00003726
Iteration 87/1000 | Loss: 0.00003726
Iteration 88/1000 | Loss: 0.00003726
Iteration 89/1000 | Loss: 0.00003726
Iteration 90/1000 | Loss: 0.00003725
Iteration 91/1000 | Loss: 0.00003725
Iteration 92/1000 | Loss: 0.00003725
Iteration 93/1000 | Loss: 0.00003725
Iteration 94/1000 | Loss: 0.00003725
Iteration 95/1000 | Loss: 0.00003724
Iteration 96/1000 | Loss: 0.00003724
Iteration 97/1000 | Loss: 0.00003724
Iteration 98/1000 | Loss: 0.00003724
Iteration 99/1000 | Loss: 0.00003724
Iteration 100/1000 | Loss: 0.00003724
Iteration 101/1000 | Loss: 0.00003723
Iteration 102/1000 | Loss: 0.00003723
Iteration 103/1000 | Loss: 0.00003723
Iteration 104/1000 | Loss: 0.00003723
Iteration 105/1000 | Loss: 0.00003723
Iteration 106/1000 | Loss: 0.00003722
Iteration 107/1000 | Loss: 0.00003722
Iteration 108/1000 | Loss: 0.00003722
Iteration 109/1000 | Loss: 0.00003722
Iteration 110/1000 | Loss: 0.00003722
Iteration 111/1000 | Loss: 0.00003722
Iteration 112/1000 | Loss: 0.00003722
Iteration 113/1000 | Loss: 0.00003722
Iteration 114/1000 | Loss: 0.00003721
Iteration 115/1000 | Loss: 0.00003721
Iteration 116/1000 | Loss: 0.00003721
Iteration 117/1000 | Loss: 0.00003721
Iteration 118/1000 | Loss: 0.00003721
Iteration 119/1000 | Loss: 0.00003721
Iteration 120/1000 | Loss: 0.00003721
Iteration 121/1000 | Loss: 0.00003721
Iteration 122/1000 | Loss: 0.00003721
Iteration 123/1000 | Loss: 0.00003721
Iteration 124/1000 | Loss: 0.00003721
Iteration 125/1000 | Loss: 0.00003720
Iteration 126/1000 | Loss: 0.00003720
Iteration 127/1000 | Loss: 0.00003720
Iteration 128/1000 | Loss: 0.00003720
Iteration 129/1000 | Loss: 0.00003720
Iteration 130/1000 | Loss: 0.00003720
Iteration 131/1000 | Loss: 0.00003720
Iteration 132/1000 | Loss: 0.00003720
Iteration 133/1000 | Loss: 0.00003720
Iteration 134/1000 | Loss: 0.00003719
Iteration 135/1000 | Loss: 0.00003719
Iteration 136/1000 | Loss: 0.00003719
Iteration 137/1000 | Loss: 0.00003719
Iteration 138/1000 | Loss: 0.00003719
Iteration 139/1000 | Loss: 0.00003719
Iteration 140/1000 | Loss: 0.00003719
Iteration 141/1000 | Loss: 0.00003719
Iteration 142/1000 | Loss: 0.00003719
Iteration 143/1000 | Loss: 0.00003719
Iteration 144/1000 | Loss: 0.00003719
Iteration 145/1000 | Loss: 0.00003719
Iteration 146/1000 | Loss: 0.00003719
Iteration 147/1000 | Loss: 0.00003719
Iteration 148/1000 | Loss: 0.00003719
Iteration 149/1000 | Loss: 0.00003719
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [3.719372762134299e-05, 3.719372762134299e-05, 3.719372762134299e-05, 3.719372762134299e-05, 3.719372762134299e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.719372762134299e-05

Optimization complete. Final v2v error: 5.2209954261779785 mm

Highest mean error: 5.61606502532959 mm for frame 13

Lowest mean error: 4.7852091789245605 mm for frame 103

Saving results

Total time: 40.48953318595886
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_0196/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0196/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0196/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00908391
Iteration 2/25 | Loss: 0.00170831
Iteration 3/25 | Loss: 0.00156919
Iteration 4/25 | Loss: 0.00155118
Iteration 5/25 | Loss: 0.00154101
Iteration 6/25 | Loss: 0.00153897
Iteration 7/25 | Loss: 0.00153787
Iteration 8/25 | Loss: 0.00154049
Iteration 9/25 | Loss: 0.00153662
Iteration 10/25 | Loss: 0.00153447
Iteration 11/25 | Loss: 0.00153378
Iteration 12/25 | Loss: 0.00153368
Iteration 13/25 | Loss: 0.00153368
Iteration 14/25 | Loss: 0.00153368
Iteration 15/25 | Loss: 0.00153367
Iteration 16/25 | Loss: 0.00153367
Iteration 17/25 | Loss: 0.00153367
Iteration 18/25 | Loss: 0.00153367
Iteration 19/25 | Loss: 0.00153367
Iteration 20/25 | Loss: 0.00153367
Iteration 21/25 | Loss: 0.00153367
Iteration 22/25 | Loss: 0.00153367
Iteration 23/25 | Loss: 0.00153367
Iteration 24/25 | Loss: 0.00153367
Iteration 25/25 | Loss: 0.00153367

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.82206511
Iteration 2/25 | Loss: 0.00271148
Iteration 3/25 | Loss: 0.00271147
Iteration 4/25 | Loss: 0.00271147
Iteration 5/25 | Loss: 0.00271147
Iteration 6/25 | Loss: 0.00271147
Iteration 7/25 | Loss: 0.00271147
Iteration 8/25 | Loss: 0.00271147
Iteration 9/25 | Loss: 0.00271147
Iteration 10/25 | Loss: 0.00271147
Iteration 11/25 | Loss: 0.00271147
Iteration 12/25 | Loss: 0.00271147
Iteration 13/25 | Loss: 0.00271147
Iteration 14/25 | Loss: 0.00271147
Iteration 15/25 | Loss: 0.00271147
Iteration 16/25 | Loss: 0.00271147
Iteration 17/25 | Loss: 0.00271147
Iteration 18/25 | Loss: 0.00271147
Iteration 19/25 | Loss: 0.00271147
Iteration 20/25 | Loss: 0.00271147
Iteration 21/25 | Loss: 0.00271147
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0027114697732031345, 0.0027114697732031345, 0.0027114697732031345, 0.0027114697732031345, 0.0027114697732031345]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0027114697732031345

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00271147
Iteration 2/1000 | Loss: 0.00003682
Iteration 3/1000 | Loss: 0.00002893
Iteration 4/1000 | Loss: 0.00002679
Iteration 5/1000 | Loss: 0.00002557
Iteration 6/1000 | Loss: 0.00002481
Iteration 7/1000 | Loss: 0.00002439
Iteration 8/1000 | Loss: 0.00002403
Iteration 9/1000 | Loss: 0.00002371
Iteration 10/1000 | Loss: 0.00002351
Iteration 11/1000 | Loss: 0.00002349
Iteration 12/1000 | Loss: 0.00002339
Iteration 13/1000 | Loss: 0.00002336
Iteration 14/1000 | Loss: 0.00002331
Iteration 15/1000 | Loss: 0.00002327
Iteration 16/1000 | Loss: 0.00002325
Iteration 17/1000 | Loss: 0.00002324
Iteration 18/1000 | Loss: 0.00002324
Iteration 19/1000 | Loss: 0.00002323
Iteration 20/1000 | Loss: 0.00002321
Iteration 21/1000 | Loss: 0.00002321
Iteration 22/1000 | Loss: 0.00002320
Iteration 23/1000 | Loss: 0.00002320
Iteration 24/1000 | Loss: 0.00002320
Iteration 25/1000 | Loss: 0.00002319
Iteration 26/1000 | Loss: 0.00002318
Iteration 27/1000 | Loss: 0.00002317
Iteration 28/1000 | Loss: 0.00002317
Iteration 29/1000 | Loss: 0.00002316
Iteration 30/1000 | Loss: 0.00002316
Iteration 31/1000 | Loss: 0.00002316
Iteration 32/1000 | Loss: 0.00002316
Iteration 33/1000 | Loss: 0.00002316
Iteration 34/1000 | Loss: 0.00002316
Iteration 35/1000 | Loss: 0.00002316
Iteration 36/1000 | Loss: 0.00002316
Iteration 37/1000 | Loss: 0.00002315
Iteration 38/1000 | Loss: 0.00002314
Iteration 39/1000 | Loss: 0.00002314
Iteration 40/1000 | Loss: 0.00002314
Iteration 41/1000 | Loss: 0.00002313
Iteration 42/1000 | Loss: 0.00002313
Iteration 43/1000 | Loss: 0.00002313
Iteration 44/1000 | Loss: 0.00002312
Iteration 45/1000 | Loss: 0.00002312
Iteration 46/1000 | Loss: 0.00002311
Iteration 47/1000 | Loss: 0.00002310
Iteration 48/1000 | Loss: 0.00002310
Iteration 49/1000 | Loss: 0.00002310
Iteration 50/1000 | Loss: 0.00002309
Iteration 51/1000 | Loss: 0.00002309
Iteration 52/1000 | Loss: 0.00002308
Iteration 53/1000 | Loss: 0.00002308
Iteration 54/1000 | Loss: 0.00002308
Iteration 55/1000 | Loss: 0.00002308
Iteration 56/1000 | Loss: 0.00002308
Iteration 57/1000 | Loss: 0.00002308
Iteration 58/1000 | Loss: 0.00002308
Iteration 59/1000 | Loss: 0.00002307
Iteration 60/1000 | Loss: 0.00002307
Iteration 61/1000 | Loss: 0.00002307
Iteration 62/1000 | Loss: 0.00002307
Iteration 63/1000 | Loss: 0.00002307
Iteration 64/1000 | Loss: 0.00002307
Iteration 65/1000 | Loss: 0.00002307
Iteration 66/1000 | Loss: 0.00002307
Iteration 67/1000 | Loss: 0.00002307
Iteration 68/1000 | Loss: 0.00002307
Iteration 69/1000 | Loss: 0.00002307
Iteration 70/1000 | Loss: 0.00002307
Iteration 71/1000 | Loss: 0.00002307
Iteration 72/1000 | Loss: 0.00002307
Iteration 73/1000 | Loss: 0.00002307
Iteration 74/1000 | Loss: 0.00002306
Iteration 75/1000 | Loss: 0.00002306
Iteration 76/1000 | Loss: 0.00002306
Iteration 77/1000 | Loss: 0.00002306
Iteration 78/1000 | Loss: 0.00002306
Iteration 79/1000 | Loss: 0.00002306
Iteration 80/1000 | Loss: 0.00002306
Iteration 81/1000 | Loss: 0.00002306
Iteration 82/1000 | Loss: 0.00002306
Iteration 83/1000 | Loss: 0.00002306
Iteration 84/1000 | Loss: 0.00002306
Iteration 85/1000 | Loss: 0.00002305
Iteration 86/1000 | Loss: 0.00002305
Iteration 87/1000 | Loss: 0.00002305
Iteration 88/1000 | Loss: 0.00002305
Iteration 89/1000 | Loss: 0.00002305
Iteration 90/1000 | Loss: 0.00002305
Iteration 91/1000 | Loss: 0.00002305
Iteration 92/1000 | Loss: 0.00002305
Iteration 93/1000 | Loss: 0.00002305
Iteration 94/1000 | Loss: 0.00002305
Iteration 95/1000 | Loss: 0.00002305
Iteration 96/1000 | Loss: 0.00002305
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [2.3051516109262593e-05, 2.3051516109262593e-05, 2.3051516109262593e-05, 2.3051516109262593e-05, 2.3051516109262593e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3051516109262593e-05

Optimization complete. Final v2v error: 4.148176193237305 mm

Highest mean error: 4.771869659423828 mm for frame 173

Lowest mean error: 3.814055919647217 mm for frame 142

Saving results

Total time: 48.18543553352356
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_0196/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0196/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0196/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00962745
Iteration 2/25 | Loss: 0.00232329
Iteration 3/25 | Loss: 0.00167826
Iteration 4/25 | Loss: 0.00161021
Iteration 5/25 | Loss: 0.00160408
Iteration 6/25 | Loss: 0.00160316
Iteration 7/25 | Loss: 0.00160316
Iteration 8/25 | Loss: 0.00160316
Iteration 9/25 | Loss: 0.00160316
Iteration 10/25 | Loss: 0.00160316
Iteration 11/25 | Loss: 0.00160316
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0016031613340601325, 0.0016031613340601325, 0.0016031613340601325, 0.0016031613340601325, 0.0016031613340601325]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016031613340601325

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45799470
Iteration 2/25 | Loss: 0.00259598
Iteration 3/25 | Loss: 0.00259597
Iteration 4/25 | Loss: 0.00259597
Iteration 5/25 | Loss: 0.00259597
Iteration 6/25 | Loss: 0.00259597
Iteration 7/25 | Loss: 0.00259597
Iteration 8/25 | Loss: 0.00259597
Iteration 9/25 | Loss: 0.00259597
Iteration 10/25 | Loss: 0.00259597
Iteration 11/25 | Loss: 0.00259597
Iteration 12/25 | Loss: 0.00259597
Iteration 13/25 | Loss: 0.00259597
Iteration 14/25 | Loss: 0.00259597
Iteration 15/25 | Loss: 0.00259597
Iteration 16/25 | Loss: 0.00259597
Iteration 17/25 | Loss: 0.00259597
Iteration 18/25 | Loss: 0.00259597
Iteration 19/25 | Loss: 0.00259597
Iteration 20/25 | Loss: 0.00259597
Iteration 21/25 | Loss: 0.00259597
Iteration 22/25 | Loss: 0.00259597
Iteration 23/25 | Loss: 0.00259597
Iteration 24/25 | Loss: 0.00259597
Iteration 25/25 | Loss: 0.00259597

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00259597
Iteration 2/1000 | Loss: 0.00005263
Iteration 3/1000 | Loss: 0.00004362
Iteration 4/1000 | Loss: 0.00004018
Iteration 5/1000 | Loss: 0.00003797
Iteration 6/1000 | Loss: 0.00003696
Iteration 7/1000 | Loss: 0.00003647
Iteration 8/1000 | Loss: 0.00003612
Iteration 9/1000 | Loss: 0.00003580
Iteration 10/1000 | Loss: 0.00003552
Iteration 11/1000 | Loss: 0.00003550
Iteration 12/1000 | Loss: 0.00003533
Iteration 13/1000 | Loss: 0.00003532
Iteration 14/1000 | Loss: 0.00003531
Iteration 15/1000 | Loss: 0.00003530
Iteration 16/1000 | Loss: 0.00003530
Iteration 17/1000 | Loss: 0.00003530
Iteration 18/1000 | Loss: 0.00003529
Iteration 19/1000 | Loss: 0.00003529
Iteration 20/1000 | Loss: 0.00003529
Iteration 21/1000 | Loss: 0.00003528
Iteration 22/1000 | Loss: 0.00003525
Iteration 23/1000 | Loss: 0.00003524
Iteration 24/1000 | Loss: 0.00003523
Iteration 25/1000 | Loss: 0.00003520
Iteration 26/1000 | Loss: 0.00003520
Iteration 27/1000 | Loss: 0.00003518
Iteration 28/1000 | Loss: 0.00003516
Iteration 29/1000 | Loss: 0.00003514
Iteration 30/1000 | Loss: 0.00003513
Iteration 31/1000 | Loss: 0.00003513
Iteration 32/1000 | Loss: 0.00003513
Iteration 33/1000 | Loss: 0.00003513
Iteration 34/1000 | Loss: 0.00003513
Iteration 35/1000 | Loss: 0.00003513
Iteration 36/1000 | Loss: 0.00003513
Iteration 37/1000 | Loss: 0.00003513
Iteration 38/1000 | Loss: 0.00003513
Iteration 39/1000 | Loss: 0.00003513
Iteration 40/1000 | Loss: 0.00003508
Iteration 41/1000 | Loss: 0.00003508
Iteration 42/1000 | Loss: 0.00003508
Iteration 43/1000 | Loss: 0.00003508
Iteration 44/1000 | Loss: 0.00003508
Iteration 45/1000 | Loss: 0.00003507
Iteration 46/1000 | Loss: 0.00003507
Iteration 47/1000 | Loss: 0.00003507
Iteration 48/1000 | Loss: 0.00003507
Iteration 49/1000 | Loss: 0.00003507
Iteration 50/1000 | Loss: 0.00003506
Iteration 51/1000 | Loss: 0.00003506
Iteration 52/1000 | Loss: 0.00003506
Iteration 53/1000 | Loss: 0.00003506
Iteration 54/1000 | Loss: 0.00003505
Iteration 55/1000 | Loss: 0.00003505
Iteration 56/1000 | Loss: 0.00003504
Iteration 57/1000 | Loss: 0.00003504
Iteration 58/1000 | Loss: 0.00003503
Iteration 59/1000 | Loss: 0.00003503
Iteration 60/1000 | Loss: 0.00003503
Iteration 61/1000 | Loss: 0.00003503
Iteration 62/1000 | Loss: 0.00003502
Iteration 63/1000 | Loss: 0.00003502
Iteration 64/1000 | Loss: 0.00003502
Iteration 65/1000 | Loss: 0.00003502
Iteration 66/1000 | Loss: 0.00003502
Iteration 67/1000 | Loss: 0.00003502
Iteration 68/1000 | Loss: 0.00003501
Iteration 69/1000 | Loss: 0.00003501
Iteration 70/1000 | Loss: 0.00003501
Iteration 71/1000 | Loss: 0.00003501
Iteration 72/1000 | Loss: 0.00003501
Iteration 73/1000 | Loss: 0.00003500
Iteration 74/1000 | Loss: 0.00003500
Iteration 75/1000 | Loss: 0.00003500
Iteration 76/1000 | Loss: 0.00003500
Iteration 77/1000 | Loss: 0.00003500
Iteration 78/1000 | Loss: 0.00003500
Iteration 79/1000 | Loss: 0.00003500
Iteration 80/1000 | Loss: 0.00003500
Iteration 81/1000 | Loss: 0.00003500
Iteration 82/1000 | Loss: 0.00003500
Iteration 83/1000 | Loss: 0.00003500
Iteration 84/1000 | Loss: 0.00003500
Iteration 85/1000 | Loss: 0.00003499
Iteration 86/1000 | Loss: 0.00003499
Iteration 87/1000 | Loss: 0.00003499
Iteration 88/1000 | Loss: 0.00003499
Iteration 89/1000 | Loss: 0.00003499
Iteration 90/1000 | Loss: 0.00003499
Iteration 91/1000 | Loss: 0.00003499
Iteration 92/1000 | Loss: 0.00003499
Iteration 93/1000 | Loss: 0.00003499
Iteration 94/1000 | Loss: 0.00003499
Iteration 95/1000 | Loss: 0.00003499
Iteration 96/1000 | Loss: 0.00003499
Iteration 97/1000 | Loss: 0.00003499
Iteration 98/1000 | Loss: 0.00003499
Iteration 99/1000 | Loss: 0.00003499
Iteration 100/1000 | Loss: 0.00003499
Iteration 101/1000 | Loss: 0.00003498
Iteration 102/1000 | Loss: 0.00003498
Iteration 103/1000 | Loss: 0.00003498
Iteration 104/1000 | Loss: 0.00003498
Iteration 105/1000 | Loss: 0.00003498
Iteration 106/1000 | Loss: 0.00003498
Iteration 107/1000 | Loss: 0.00003498
Iteration 108/1000 | Loss: 0.00003498
Iteration 109/1000 | Loss: 0.00003498
Iteration 110/1000 | Loss: 0.00003498
Iteration 111/1000 | Loss: 0.00003498
Iteration 112/1000 | Loss: 0.00003498
Iteration 113/1000 | Loss: 0.00003497
Iteration 114/1000 | Loss: 0.00003497
Iteration 115/1000 | Loss: 0.00003497
Iteration 116/1000 | Loss: 0.00003497
Iteration 117/1000 | Loss: 0.00003497
Iteration 118/1000 | Loss: 0.00003497
Iteration 119/1000 | Loss: 0.00003497
Iteration 120/1000 | Loss: 0.00003497
Iteration 121/1000 | Loss: 0.00003497
Iteration 122/1000 | Loss: 0.00003497
Iteration 123/1000 | Loss: 0.00003497
Iteration 124/1000 | Loss: 0.00003497
Iteration 125/1000 | Loss: 0.00003497
Iteration 126/1000 | Loss: 0.00003497
Iteration 127/1000 | Loss: 0.00003496
Iteration 128/1000 | Loss: 0.00003496
Iteration 129/1000 | Loss: 0.00003496
Iteration 130/1000 | Loss: 0.00003496
Iteration 131/1000 | Loss: 0.00003496
Iteration 132/1000 | Loss: 0.00003496
Iteration 133/1000 | Loss: 0.00003496
Iteration 134/1000 | Loss: 0.00003496
Iteration 135/1000 | Loss: 0.00003496
Iteration 136/1000 | Loss: 0.00003496
Iteration 137/1000 | Loss: 0.00003496
Iteration 138/1000 | Loss: 0.00003496
Iteration 139/1000 | Loss: 0.00003496
Iteration 140/1000 | Loss: 0.00003496
Iteration 141/1000 | Loss: 0.00003496
Iteration 142/1000 | Loss: 0.00003496
Iteration 143/1000 | Loss: 0.00003496
Iteration 144/1000 | Loss: 0.00003496
Iteration 145/1000 | Loss: 0.00003496
Iteration 146/1000 | Loss: 0.00003495
Iteration 147/1000 | Loss: 0.00003495
Iteration 148/1000 | Loss: 0.00003495
Iteration 149/1000 | Loss: 0.00003495
Iteration 150/1000 | Loss: 0.00003495
Iteration 151/1000 | Loss: 0.00003495
Iteration 152/1000 | Loss: 0.00003495
Iteration 153/1000 | Loss: 0.00003495
Iteration 154/1000 | Loss: 0.00003495
Iteration 155/1000 | Loss: 0.00003495
Iteration 156/1000 | Loss: 0.00003495
Iteration 157/1000 | Loss: 0.00003495
Iteration 158/1000 | Loss: 0.00003495
Iteration 159/1000 | Loss: 0.00003495
Iteration 160/1000 | Loss: 0.00003495
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [3.495041164569557e-05, 3.495041164569557e-05, 3.495041164569557e-05, 3.495041164569557e-05, 3.495041164569557e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.495041164569557e-05

Optimization complete. Final v2v error: 5.169469833374023 mm

Highest mean error: 5.379345417022705 mm for frame 155

Lowest mean error: 4.975430011749268 mm for frame 238

Saving results

Total time: 40.026832818984985
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_0196/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0196/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0196/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00469542
Iteration 2/25 | Loss: 0.00158091
Iteration 3/25 | Loss: 0.00150005
Iteration 4/25 | Loss: 0.00148343
Iteration 5/25 | Loss: 0.00147812
Iteration 6/25 | Loss: 0.00147663
Iteration 7/25 | Loss: 0.00147645
Iteration 8/25 | Loss: 0.00147645
Iteration 9/25 | Loss: 0.00147645
Iteration 10/25 | Loss: 0.00147645
Iteration 11/25 | Loss: 0.00147645
Iteration 12/25 | Loss: 0.00147645
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001476447214372456, 0.001476447214372456, 0.001476447214372456, 0.001476447214372456, 0.001476447214372456]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001476447214372456

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.11704397
Iteration 2/25 | Loss: 0.00252917
Iteration 3/25 | Loss: 0.00252916
Iteration 4/25 | Loss: 0.00252916
Iteration 5/25 | Loss: 0.00252916
Iteration 6/25 | Loss: 0.00252916
Iteration 7/25 | Loss: 0.00252915
Iteration 8/25 | Loss: 0.00252915
Iteration 9/25 | Loss: 0.00252915
Iteration 10/25 | Loss: 0.00252915
Iteration 11/25 | Loss: 0.00252915
Iteration 12/25 | Loss: 0.00252915
Iteration 13/25 | Loss: 0.00252915
Iteration 14/25 | Loss: 0.00252915
Iteration 15/25 | Loss: 0.00252915
Iteration 16/25 | Loss: 0.00252915
Iteration 17/25 | Loss: 0.00252915
Iteration 18/25 | Loss: 0.00252915
Iteration 19/25 | Loss: 0.00252915
Iteration 20/25 | Loss: 0.00252915
Iteration 21/25 | Loss: 0.00252915
Iteration 22/25 | Loss: 0.00252915
Iteration 23/25 | Loss: 0.00252915
Iteration 24/25 | Loss: 0.00252915
Iteration 25/25 | Loss: 0.00252915

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00252915
Iteration 2/1000 | Loss: 0.00003920
Iteration 3/1000 | Loss: 0.00003229
Iteration 4/1000 | Loss: 0.00002992
Iteration 5/1000 | Loss: 0.00002850
Iteration 6/1000 | Loss: 0.00002767
Iteration 7/1000 | Loss: 0.00002719
Iteration 8/1000 | Loss: 0.00002676
Iteration 9/1000 | Loss: 0.00002641
Iteration 10/1000 | Loss: 0.00002617
Iteration 11/1000 | Loss: 0.00002613
Iteration 12/1000 | Loss: 0.00002608
Iteration 13/1000 | Loss: 0.00002608
Iteration 14/1000 | Loss: 0.00002608
Iteration 15/1000 | Loss: 0.00002607
Iteration 16/1000 | Loss: 0.00002603
Iteration 17/1000 | Loss: 0.00002602
Iteration 18/1000 | Loss: 0.00002602
Iteration 19/1000 | Loss: 0.00002602
Iteration 20/1000 | Loss: 0.00002602
Iteration 21/1000 | Loss: 0.00002601
Iteration 22/1000 | Loss: 0.00002601
Iteration 23/1000 | Loss: 0.00002600
Iteration 24/1000 | Loss: 0.00002600
Iteration 25/1000 | Loss: 0.00002599
Iteration 26/1000 | Loss: 0.00002599
Iteration 27/1000 | Loss: 0.00002598
Iteration 28/1000 | Loss: 0.00002598
Iteration 29/1000 | Loss: 0.00002597
Iteration 30/1000 | Loss: 0.00002595
Iteration 31/1000 | Loss: 0.00002594
Iteration 32/1000 | Loss: 0.00002594
Iteration 33/1000 | Loss: 0.00002593
Iteration 34/1000 | Loss: 0.00002591
Iteration 35/1000 | Loss: 0.00002591
Iteration 36/1000 | Loss: 0.00002591
Iteration 37/1000 | Loss: 0.00002591
Iteration 38/1000 | Loss: 0.00002590
Iteration 39/1000 | Loss: 0.00002590
Iteration 40/1000 | Loss: 0.00002589
Iteration 41/1000 | Loss: 0.00002589
Iteration 42/1000 | Loss: 0.00002589
Iteration 43/1000 | Loss: 0.00002588
Iteration 44/1000 | Loss: 0.00002588
Iteration 45/1000 | Loss: 0.00002588
Iteration 46/1000 | Loss: 0.00002587
Iteration 47/1000 | Loss: 0.00002587
Iteration 48/1000 | Loss: 0.00002585
Iteration 49/1000 | Loss: 0.00002584
Iteration 50/1000 | Loss: 0.00002584
Iteration 51/1000 | Loss: 0.00002584
Iteration 52/1000 | Loss: 0.00002584
Iteration 53/1000 | Loss: 0.00002584
Iteration 54/1000 | Loss: 0.00002584
Iteration 55/1000 | Loss: 0.00002583
Iteration 56/1000 | Loss: 0.00002583
Iteration 57/1000 | Loss: 0.00002583
Iteration 58/1000 | Loss: 0.00002583
Iteration 59/1000 | Loss: 0.00002583
Iteration 60/1000 | Loss: 0.00002583
Iteration 61/1000 | Loss: 0.00002583
Iteration 62/1000 | Loss: 0.00002583
Iteration 63/1000 | Loss: 0.00002583
Iteration 64/1000 | Loss: 0.00002583
Iteration 65/1000 | Loss: 0.00002582
Iteration 66/1000 | Loss: 0.00002582
Iteration 67/1000 | Loss: 0.00002581
Iteration 68/1000 | Loss: 0.00002581
Iteration 69/1000 | Loss: 0.00002581
Iteration 70/1000 | Loss: 0.00002581
Iteration 71/1000 | Loss: 0.00002581
Iteration 72/1000 | Loss: 0.00002581
Iteration 73/1000 | Loss: 0.00002581
Iteration 74/1000 | Loss: 0.00002581
Iteration 75/1000 | Loss: 0.00002581
Iteration 76/1000 | Loss: 0.00002581
Iteration 77/1000 | Loss: 0.00002581
Iteration 78/1000 | Loss: 0.00002581
Iteration 79/1000 | Loss: 0.00002581
Iteration 80/1000 | Loss: 0.00002580
Iteration 81/1000 | Loss: 0.00002580
Iteration 82/1000 | Loss: 0.00002580
Iteration 83/1000 | Loss: 0.00002579
Iteration 84/1000 | Loss: 0.00002579
Iteration 85/1000 | Loss: 0.00002579
Iteration 86/1000 | Loss: 0.00002579
Iteration 87/1000 | Loss: 0.00002578
Iteration 88/1000 | Loss: 0.00002578
Iteration 89/1000 | Loss: 0.00002578
Iteration 90/1000 | Loss: 0.00002578
Iteration 91/1000 | Loss: 0.00002578
Iteration 92/1000 | Loss: 0.00002578
Iteration 93/1000 | Loss: 0.00002578
Iteration 94/1000 | Loss: 0.00002577
Iteration 95/1000 | Loss: 0.00002577
Iteration 96/1000 | Loss: 0.00002577
Iteration 97/1000 | Loss: 0.00002577
Iteration 98/1000 | Loss: 0.00002577
Iteration 99/1000 | Loss: 0.00002577
Iteration 100/1000 | Loss: 0.00002577
Iteration 101/1000 | Loss: 0.00002577
Iteration 102/1000 | Loss: 0.00002577
Iteration 103/1000 | Loss: 0.00002577
Iteration 104/1000 | Loss: 0.00002577
Iteration 105/1000 | Loss: 0.00002577
Iteration 106/1000 | Loss: 0.00002577
Iteration 107/1000 | Loss: 0.00002577
Iteration 108/1000 | Loss: 0.00002576
Iteration 109/1000 | Loss: 0.00002576
Iteration 110/1000 | Loss: 0.00002576
Iteration 111/1000 | Loss: 0.00002576
Iteration 112/1000 | Loss: 0.00002576
Iteration 113/1000 | Loss: 0.00002576
Iteration 114/1000 | Loss: 0.00002576
Iteration 115/1000 | Loss: 0.00002575
Iteration 116/1000 | Loss: 0.00002575
Iteration 117/1000 | Loss: 0.00002575
Iteration 118/1000 | Loss: 0.00002575
Iteration 119/1000 | Loss: 0.00002575
Iteration 120/1000 | Loss: 0.00002575
Iteration 121/1000 | Loss: 0.00002575
Iteration 122/1000 | Loss: 0.00002575
Iteration 123/1000 | Loss: 0.00002575
Iteration 124/1000 | Loss: 0.00002575
Iteration 125/1000 | Loss: 0.00002575
Iteration 126/1000 | Loss: 0.00002575
Iteration 127/1000 | Loss: 0.00002575
Iteration 128/1000 | Loss: 0.00002575
Iteration 129/1000 | Loss: 0.00002574
Iteration 130/1000 | Loss: 0.00002574
Iteration 131/1000 | Loss: 0.00002574
Iteration 132/1000 | Loss: 0.00002574
Iteration 133/1000 | Loss: 0.00002574
Iteration 134/1000 | Loss: 0.00002574
Iteration 135/1000 | Loss: 0.00002574
Iteration 136/1000 | Loss: 0.00002574
Iteration 137/1000 | Loss: 0.00002574
Iteration 138/1000 | Loss: 0.00002574
Iteration 139/1000 | Loss: 0.00002574
Iteration 140/1000 | Loss: 0.00002574
Iteration 141/1000 | Loss: 0.00002574
Iteration 142/1000 | Loss: 0.00002574
Iteration 143/1000 | Loss: 0.00002574
Iteration 144/1000 | Loss: 0.00002574
Iteration 145/1000 | Loss: 0.00002574
Iteration 146/1000 | Loss: 0.00002574
Iteration 147/1000 | Loss: 0.00002574
Iteration 148/1000 | Loss: 0.00002574
Iteration 149/1000 | Loss: 0.00002574
Iteration 150/1000 | Loss: 0.00002574
Iteration 151/1000 | Loss: 0.00002573
Iteration 152/1000 | Loss: 0.00002573
Iteration 153/1000 | Loss: 0.00002573
Iteration 154/1000 | Loss: 0.00002573
Iteration 155/1000 | Loss: 0.00002573
Iteration 156/1000 | Loss: 0.00002573
Iteration 157/1000 | Loss: 0.00002573
Iteration 158/1000 | Loss: 0.00002573
Iteration 159/1000 | Loss: 0.00002573
Iteration 160/1000 | Loss: 0.00002573
Iteration 161/1000 | Loss: 0.00002573
Iteration 162/1000 | Loss: 0.00002573
Iteration 163/1000 | Loss: 0.00002573
Iteration 164/1000 | Loss: 0.00002573
Iteration 165/1000 | Loss: 0.00002573
Iteration 166/1000 | Loss: 0.00002573
Iteration 167/1000 | Loss: 0.00002573
Iteration 168/1000 | Loss: 0.00002573
Iteration 169/1000 | Loss: 0.00002573
Iteration 170/1000 | Loss: 0.00002573
Iteration 171/1000 | Loss: 0.00002573
Iteration 172/1000 | Loss: 0.00002573
Iteration 173/1000 | Loss: 0.00002573
Iteration 174/1000 | Loss: 0.00002573
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [2.5733706934261136e-05, 2.5733706934261136e-05, 2.5733706934261136e-05, 2.5733706934261136e-05, 2.5733706934261136e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5733706934261136e-05

Optimization complete. Final v2v error: 4.405569076538086 mm

Highest mean error: 4.83005952835083 mm for frame 170

Lowest mean error: 4.119837284088135 mm for frame 50

Saving results

Total time: 35.568950176239014
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_0196/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0196/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0196/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01152530
Iteration 2/25 | Loss: 0.00239602
Iteration 3/25 | Loss: 0.00183052
Iteration 4/25 | Loss: 0.00159169
Iteration 5/25 | Loss: 0.00159859
Iteration 6/25 | Loss: 0.00160035
Iteration 7/25 | Loss: 0.00155178
Iteration 8/25 | Loss: 0.00150288
Iteration 9/25 | Loss: 0.00149086
Iteration 10/25 | Loss: 0.00149423
Iteration 11/25 | Loss: 0.00148558
Iteration 12/25 | Loss: 0.00146863
Iteration 13/25 | Loss: 0.00147135
Iteration 14/25 | Loss: 0.00145230
Iteration 15/25 | Loss: 0.00145587
Iteration 16/25 | Loss: 0.00145536
Iteration 17/25 | Loss: 0.00145565
Iteration 18/25 | Loss: 0.00145790
Iteration 19/25 | Loss: 0.00145638
Iteration 20/25 | Loss: 0.00146045
Iteration 21/25 | Loss: 0.00145745
Iteration 22/25 | Loss: 0.00146367
Iteration 23/25 | Loss: 0.00145799
Iteration 24/25 | Loss: 0.00146365
Iteration 25/25 | Loss: 0.00145388

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63358736
Iteration 2/25 | Loss: 0.00345735
Iteration 3/25 | Loss: 0.00311997
Iteration 4/25 | Loss: 0.00311997
Iteration 5/25 | Loss: 0.00311997
Iteration 6/25 | Loss: 0.00311997
Iteration 7/25 | Loss: 0.00311997
Iteration 8/25 | Loss: 0.00311997
Iteration 9/25 | Loss: 0.00311997
Iteration 10/25 | Loss: 0.00311997
Iteration 11/25 | Loss: 0.00311997
Iteration 12/25 | Loss: 0.00311997
Iteration 13/25 | Loss: 0.00311997
Iteration 14/25 | Loss: 0.00311997
Iteration 15/25 | Loss: 0.00311997
Iteration 16/25 | Loss: 0.00311997
Iteration 17/25 | Loss: 0.00311997
Iteration 18/25 | Loss: 0.00311997
Iteration 19/25 | Loss: 0.00311997
Iteration 20/25 | Loss: 0.00311997
Iteration 21/25 | Loss: 0.00311997
Iteration 22/25 | Loss: 0.00311997
Iteration 23/25 | Loss: 0.00311997
Iteration 24/25 | Loss: 0.00311997
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.003119968343526125, 0.003119968343526125, 0.003119968343526125, 0.003119968343526125, 0.003119968343526125]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003119968343526125

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00311997
Iteration 2/1000 | Loss: 0.00099969
Iteration 3/1000 | Loss: 0.00229025
Iteration 4/1000 | Loss: 0.00025755
Iteration 5/1000 | Loss: 0.00095227
Iteration 6/1000 | Loss: 0.00025872
Iteration 7/1000 | Loss: 0.00007152
Iteration 8/1000 | Loss: 0.00046034
Iteration 9/1000 | Loss: 0.00037281
Iteration 10/1000 | Loss: 0.00014662
Iteration 11/1000 | Loss: 0.00154640
Iteration 12/1000 | Loss: 0.00171242
Iteration 13/1000 | Loss: 0.00142996
Iteration 14/1000 | Loss: 0.00120381
Iteration 15/1000 | Loss: 0.00034516
Iteration 16/1000 | Loss: 0.00069420
Iteration 17/1000 | Loss: 0.00028387
Iteration 18/1000 | Loss: 0.00029408
Iteration 19/1000 | Loss: 0.00018819
Iteration 20/1000 | Loss: 0.00023932
Iteration 21/1000 | Loss: 0.00043908
Iteration 22/1000 | Loss: 0.00005597
Iteration 23/1000 | Loss: 0.00004861
Iteration 24/1000 | Loss: 0.00004455
Iteration 25/1000 | Loss: 0.00085314
Iteration 26/1000 | Loss: 0.00033647
Iteration 27/1000 | Loss: 0.00019632
Iteration 28/1000 | Loss: 0.00033906
Iteration 29/1000 | Loss: 0.00023231
Iteration 30/1000 | Loss: 0.00021320
Iteration 31/1000 | Loss: 0.00019872
Iteration 32/1000 | Loss: 0.00013118
Iteration 33/1000 | Loss: 0.00004467
Iteration 34/1000 | Loss: 0.00004196
Iteration 35/1000 | Loss: 0.00004209
Iteration 36/1000 | Loss: 0.00048138
Iteration 37/1000 | Loss: 0.00021908
Iteration 38/1000 | Loss: 0.00027673
Iteration 39/1000 | Loss: 0.00060850
Iteration 40/1000 | Loss: 0.00073390
Iteration 41/1000 | Loss: 0.00069419
Iteration 42/1000 | Loss: 0.00028026
Iteration 43/1000 | Loss: 0.00056188
Iteration 44/1000 | Loss: 0.00032537
Iteration 45/1000 | Loss: 0.00032173
Iteration 46/1000 | Loss: 0.00032296
Iteration 47/1000 | Loss: 0.00038770
Iteration 48/1000 | Loss: 0.00036493
Iteration 49/1000 | Loss: 0.00040698
Iteration 50/1000 | Loss: 0.00032556
Iteration 51/1000 | Loss: 0.00014444
Iteration 52/1000 | Loss: 0.00004975
Iteration 53/1000 | Loss: 0.00002701
Iteration 54/1000 | Loss: 0.00002450
Iteration 55/1000 | Loss: 0.00002356
Iteration 56/1000 | Loss: 0.00002322
Iteration 57/1000 | Loss: 0.00002582
Iteration 58/1000 | Loss: 0.00002378
Iteration 59/1000 | Loss: 0.00002284
Iteration 60/1000 | Loss: 0.00002270
Iteration 61/1000 | Loss: 0.00002549
Iteration 62/1000 | Loss: 0.00002325
Iteration 63/1000 | Loss: 0.00002250
Iteration 64/1000 | Loss: 0.00002250
Iteration 65/1000 | Loss: 0.00002249
Iteration 66/1000 | Loss: 0.00002249
Iteration 67/1000 | Loss: 0.00002249
Iteration 68/1000 | Loss: 0.00002249
Iteration 69/1000 | Loss: 0.00002249
Iteration 70/1000 | Loss: 0.00002249
Iteration 71/1000 | Loss: 0.00002519
Iteration 72/1000 | Loss: 0.00002350
Iteration 73/1000 | Loss: 0.00002245
Iteration 74/1000 | Loss: 0.00002245
Iteration 75/1000 | Loss: 0.00002243
Iteration 76/1000 | Loss: 0.00002243
Iteration 77/1000 | Loss: 0.00002243
Iteration 78/1000 | Loss: 0.00002242
Iteration 79/1000 | Loss: 0.00002498
Iteration 80/1000 | Loss: 0.00002498
Iteration 81/1000 | Loss: 0.00002337
Iteration 82/1000 | Loss: 0.00002239
Iteration 83/1000 | Loss: 0.00002238
Iteration 84/1000 | Loss: 0.00002238
Iteration 85/1000 | Loss: 0.00002237
Iteration 86/1000 | Loss: 0.00002237
Iteration 87/1000 | Loss: 0.00002237
Iteration 88/1000 | Loss: 0.00002236
Iteration 89/1000 | Loss: 0.00002236
Iteration 90/1000 | Loss: 0.00002236
Iteration 91/1000 | Loss: 0.00002236
Iteration 92/1000 | Loss: 0.00002235
Iteration 93/1000 | Loss: 0.00002235
Iteration 94/1000 | Loss: 0.00002235
Iteration 95/1000 | Loss: 0.00002235
Iteration 96/1000 | Loss: 0.00002233
Iteration 97/1000 | Loss: 0.00002476
Iteration 98/1000 | Loss: 0.00002318
Iteration 99/1000 | Loss: 0.00002229
Iteration 100/1000 | Loss: 0.00002229
Iteration 101/1000 | Loss: 0.00002229
Iteration 102/1000 | Loss: 0.00002229
Iteration 103/1000 | Loss: 0.00002228
Iteration 104/1000 | Loss: 0.00002228
Iteration 105/1000 | Loss: 0.00002228
Iteration 106/1000 | Loss: 0.00002228
Iteration 107/1000 | Loss: 0.00002228
Iteration 108/1000 | Loss: 0.00002228
Iteration 109/1000 | Loss: 0.00002228
Iteration 110/1000 | Loss: 0.00002228
Iteration 111/1000 | Loss: 0.00002228
Iteration 112/1000 | Loss: 0.00002228
Iteration 113/1000 | Loss: 0.00002228
Iteration 114/1000 | Loss: 0.00002227
Iteration 115/1000 | Loss: 0.00002227
Iteration 116/1000 | Loss: 0.00002227
Iteration 117/1000 | Loss: 0.00002227
Iteration 118/1000 | Loss: 0.00002227
Iteration 119/1000 | Loss: 0.00002227
Iteration 120/1000 | Loss: 0.00002226
Iteration 121/1000 | Loss: 0.00002226
Iteration 122/1000 | Loss: 0.00002226
Iteration 123/1000 | Loss: 0.00002226
Iteration 124/1000 | Loss: 0.00002460
Iteration 125/1000 | Loss: 0.00002359
Iteration 126/1000 | Loss: 0.00002441
Iteration 127/1000 | Loss: 0.00002390
Iteration 128/1000 | Loss: 0.00002443
Iteration 129/1000 | Loss: 0.00002372
Iteration 130/1000 | Loss: 0.00002446
Iteration 131/1000 | Loss: 0.00002374
Iteration 132/1000 | Loss: 0.00002449
Iteration 133/1000 | Loss: 0.00002430
Iteration 134/1000 | Loss: 0.00002302
Iteration 135/1000 | Loss: 0.00002387
Iteration 136/1000 | Loss: 0.00002374
Iteration 137/1000 | Loss: 0.00002386
Iteration 138/1000 | Loss: 0.00002404
Iteration 139/1000 | Loss: 0.00002395
Iteration 140/1000 | Loss: 0.00002431
Iteration 141/1000 | Loss: 0.00002409
Iteration 142/1000 | Loss: 0.00002410
Iteration 143/1000 | Loss: 0.00002305
Iteration 144/1000 | Loss: 0.00002336
Iteration 145/1000 | Loss: 0.00002349
Iteration 146/1000 | Loss: 0.00002454
Iteration 147/1000 | Loss: 0.00002396
Iteration 148/1000 | Loss: 0.00002493
Iteration 149/1000 | Loss: 0.00002371
Iteration 150/1000 | Loss: 0.00002270
Iteration 151/1000 | Loss: 0.00002347
Iteration 152/1000 | Loss: 0.00002477
Iteration 153/1000 | Loss: 0.00002393
Iteration 154/1000 | Loss: 0.00002511
Iteration 155/1000 | Loss: 0.00002392
Iteration 156/1000 | Loss: 0.00002471
Iteration 157/1000 | Loss: 0.00002421
Iteration 158/1000 | Loss: 0.00002507
Iteration 159/1000 | Loss: 0.00002409
Iteration 160/1000 | Loss: 0.00002509
Iteration 161/1000 | Loss: 0.00002450
Iteration 162/1000 | Loss: 0.00002519
Iteration 163/1000 | Loss: 0.00002418
Iteration 164/1000 | Loss: 0.00002512
Iteration 165/1000 | Loss: 0.00002437
Iteration 166/1000 | Loss: 0.00002482
Iteration 167/1000 | Loss: 0.00002446
Iteration 168/1000 | Loss: 0.00002501
Iteration 169/1000 | Loss: 0.00002392
Iteration 170/1000 | Loss: 0.00002451
Iteration 171/1000 | Loss: 0.00002435
Iteration 172/1000 | Loss: 0.00002498
Iteration 173/1000 | Loss: 0.00002435
Iteration 174/1000 | Loss: 0.00002494
Iteration 175/1000 | Loss: 0.00002464
Iteration 176/1000 | Loss: 0.00002458
Iteration 177/1000 | Loss: 0.00002443
Iteration 178/1000 | Loss: 0.00002435
Iteration 179/1000 | Loss: 0.00002434
Iteration 180/1000 | Loss: 0.00002415
Iteration 181/1000 | Loss: 0.00002432
Iteration 182/1000 | Loss: 0.00002523
Iteration 183/1000 | Loss: 0.00002433
Iteration 184/1000 | Loss: 0.00002495
Iteration 185/1000 | Loss: 0.00002452
Iteration 186/1000 | Loss: 0.00002496
Iteration 187/1000 | Loss: 0.00002246
Iteration 188/1000 | Loss: 0.00002438
Iteration 189/1000 | Loss: 0.00002497
Iteration 190/1000 | Loss: 0.00002271
Iteration 191/1000 | Loss: 0.00002434
Iteration 192/1000 | Loss: 0.00002476
Iteration 193/1000 | Loss: 0.00002431
Iteration 194/1000 | Loss: 0.00002496
Iteration 195/1000 | Loss: 0.00002430
Iteration 196/1000 | Loss: 0.00002497
Iteration 197/1000 | Loss: 0.00002413
Iteration 198/1000 | Loss: 0.00002487
Iteration 199/1000 | Loss: 0.00002405
Iteration 200/1000 | Loss: 0.00002413
Iteration 201/1000 | Loss: 0.00002411
Iteration 202/1000 | Loss: 0.00002494
Iteration 203/1000 | Loss: 0.00002409
Iteration 204/1000 | Loss: 0.00002482
Iteration 205/1000 | Loss: 0.00002421
Iteration 206/1000 | Loss: 0.00002472
Iteration 207/1000 | Loss: 0.00002439
Iteration 208/1000 | Loss: 0.00002339
Iteration 209/1000 | Loss: 0.00002383
Iteration 210/1000 | Loss: 0.00002449
Iteration 211/1000 | Loss: 0.00002451
Iteration 212/1000 | Loss: 0.00002451
Iteration 213/1000 | Loss: 0.00002306
Iteration 214/1000 | Loss: 0.00002216
Iteration 215/1000 | Loss: 0.00002215
Iteration 216/1000 | Loss: 0.00002214
Iteration 217/1000 | Loss: 0.00002214
Iteration 218/1000 | Loss: 0.00002214
Iteration 219/1000 | Loss: 0.00002214
Iteration 220/1000 | Loss: 0.00002214
Iteration 221/1000 | Loss: 0.00002214
Iteration 222/1000 | Loss: 0.00002214
Iteration 223/1000 | Loss: 0.00002214
Iteration 224/1000 | Loss: 0.00002214
Iteration 225/1000 | Loss: 0.00002214
Iteration 226/1000 | Loss: 0.00002214
Iteration 227/1000 | Loss: 0.00002214
Iteration 228/1000 | Loss: 0.00002214
Iteration 229/1000 | Loss: 0.00002213
Iteration 230/1000 | Loss: 0.00002213
Iteration 231/1000 | Loss: 0.00002213
Iteration 232/1000 | Loss: 0.00002213
Iteration 233/1000 | Loss: 0.00002213
Iteration 234/1000 | Loss: 0.00002213
Iteration 235/1000 | Loss: 0.00002213
Iteration 236/1000 | Loss: 0.00002213
Iteration 237/1000 | Loss: 0.00002213
Iteration 238/1000 | Loss: 0.00002213
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 238. Stopping optimization.
Last 5 losses: [2.213474908785429e-05, 2.213474908785429e-05, 2.213474908785429e-05, 2.213474908785429e-05, 2.213474908785429e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.213474908785429e-05

Optimization complete. Final v2v error: 4.026875019073486 mm

Highest mean error: 10.409647941589355 mm for frame 66

Lowest mean error: 3.5157904624938965 mm for frame 128

Saving results

Total time: 254.35995483398438
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_0196/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0196/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0196/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01181608
Iteration 2/25 | Loss: 0.01181608
Iteration 3/25 | Loss: 0.01181608
Iteration 4/25 | Loss: 0.01181608
Iteration 5/25 | Loss: 0.01181608
Iteration 6/25 | Loss: 0.01181607
Iteration 7/25 | Loss: 0.01181607
Iteration 8/25 | Loss: 0.01181607
Iteration 9/25 | Loss: 0.01181607
Iteration 10/25 | Loss: 0.01181607
Iteration 11/25 | Loss: 0.01181607
Iteration 12/25 | Loss: 0.01181606
Iteration 13/25 | Loss: 0.01181606
Iteration 14/25 | Loss: 0.01181606
Iteration 15/25 | Loss: 0.01181606
Iteration 16/25 | Loss: 0.01181606
Iteration 17/25 | Loss: 0.01181606
Iteration 18/25 | Loss: 0.01181606
Iteration 19/25 | Loss: 0.01181605
Iteration 20/25 | Loss: 0.01181605
Iteration 21/25 | Loss: 0.01181605
Iteration 22/25 | Loss: 0.01181605
Iteration 23/25 | Loss: 0.01181605
Iteration 24/25 | Loss: 0.01181604
Iteration 25/25 | Loss: 0.01181604

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.04340219
Iteration 2/25 | Loss: 0.10172141
Iteration 3/25 | Loss: 0.10167953
Iteration 4/25 | Loss: 0.10167953
Iteration 5/25 | Loss: 0.10167953
Iteration 6/25 | Loss: 0.10167953
Iteration 7/25 | Loss: 0.10167952
Iteration 8/25 | Loss: 0.10167952
Iteration 9/25 | Loss: 0.10167953
Iteration 10/25 | Loss: 0.10167952
Iteration 11/25 | Loss: 0.10167952
Iteration 12/25 | Loss: 0.10167952
Iteration 13/25 | Loss: 0.10167952
Iteration 14/25 | Loss: 0.10167952
Iteration 15/25 | Loss: 0.10167952
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.10167951881885529, 0.10167951881885529, 0.10167951881885529, 0.10167951881885529, 0.10167951881885529]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.10167951881885529

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.10167952
Iteration 2/1000 | Loss: 0.01269271
Iteration 3/1000 | Loss: 0.00183444
Iteration 4/1000 | Loss: 0.00085548
Iteration 5/1000 | Loss: 0.00075888
Iteration 6/1000 | Loss: 0.00048653
Iteration 7/1000 | Loss: 0.00033363
Iteration 8/1000 | Loss: 0.00067012
Iteration 9/1000 | Loss: 0.00020499
Iteration 10/1000 | Loss: 0.00037813
Iteration 11/1000 | Loss: 0.00260142
Iteration 12/1000 | Loss: 0.00010503
Iteration 13/1000 | Loss: 0.00016290
Iteration 14/1000 | Loss: 0.00007975
Iteration 15/1000 | Loss: 0.00072939
Iteration 16/1000 | Loss: 0.00027674
Iteration 17/1000 | Loss: 0.00006597
Iteration 18/1000 | Loss: 0.00005855
Iteration 19/1000 | Loss: 0.00005264
Iteration 20/1000 | Loss: 0.00004690
Iteration 21/1000 | Loss: 0.00025071
Iteration 22/1000 | Loss: 0.00004245
Iteration 23/1000 | Loss: 0.00004066
Iteration 24/1000 | Loss: 0.00019560
Iteration 25/1000 | Loss: 0.00003820
Iteration 26/1000 | Loss: 0.00003757
Iteration 27/1000 | Loss: 0.00003688
Iteration 28/1000 | Loss: 0.00035506
Iteration 29/1000 | Loss: 0.00003625
Iteration 30/1000 | Loss: 0.00003568
Iteration 31/1000 | Loss: 0.00003525
Iteration 32/1000 | Loss: 0.00003489
Iteration 33/1000 | Loss: 0.00023297
Iteration 34/1000 | Loss: 0.00023297
Iteration 35/1000 | Loss: 0.00013035
Iteration 36/1000 | Loss: 0.00004662
Iteration 37/1000 | Loss: 0.00048281
Iteration 38/1000 | Loss: 0.00003674
Iteration 39/1000 | Loss: 0.00003468
Iteration 40/1000 | Loss: 0.00003452
Iteration 41/1000 | Loss: 0.00003447
Iteration 42/1000 | Loss: 0.00004531
Iteration 43/1000 | Loss: 0.00003425
Iteration 44/1000 | Loss: 0.00003424
Iteration 45/1000 | Loss: 0.00004063
Iteration 46/1000 | Loss: 0.00003418
Iteration 47/1000 | Loss: 0.00003418
Iteration 48/1000 | Loss: 0.00003418
Iteration 49/1000 | Loss: 0.00003418
Iteration 50/1000 | Loss: 0.00003417
Iteration 51/1000 | Loss: 0.00003417
Iteration 52/1000 | Loss: 0.00003417
Iteration 53/1000 | Loss: 0.00003417
Iteration 54/1000 | Loss: 0.00003417
Iteration 55/1000 | Loss: 0.00003416
Iteration 56/1000 | Loss: 0.00003416
Iteration 57/1000 | Loss: 0.00004093
Iteration 58/1000 | Loss: 0.00004185
Iteration 59/1000 | Loss: 0.00003407
Iteration 60/1000 | Loss: 0.00003406
Iteration 61/1000 | Loss: 0.00003406
Iteration 62/1000 | Loss: 0.00003406
Iteration 63/1000 | Loss: 0.00003406
Iteration 64/1000 | Loss: 0.00003406
Iteration 65/1000 | Loss: 0.00003406
Iteration 66/1000 | Loss: 0.00003406
Iteration 67/1000 | Loss: 0.00003406
Iteration 68/1000 | Loss: 0.00003406
Iteration 69/1000 | Loss: 0.00003406
Iteration 70/1000 | Loss: 0.00003405
Iteration 71/1000 | Loss: 0.00003403
Iteration 72/1000 | Loss: 0.00003403
Iteration 73/1000 | Loss: 0.00003402
Iteration 74/1000 | Loss: 0.00003401
Iteration 75/1000 | Loss: 0.00003400
Iteration 76/1000 | Loss: 0.00003400
Iteration 77/1000 | Loss: 0.00003400
Iteration 78/1000 | Loss: 0.00003400
Iteration 79/1000 | Loss: 0.00003400
Iteration 80/1000 | Loss: 0.00003399
Iteration 81/1000 | Loss: 0.00003399
Iteration 82/1000 | Loss: 0.00003399
Iteration 83/1000 | Loss: 0.00003399
Iteration 84/1000 | Loss: 0.00003399
Iteration 85/1000 | Loss: 0.00003398
Iteration 86/1000 | Loss: 0.00003398
Iteration 87/1000 | Loss: 0.00003398
Iteration 88/1000 | Loss: 0.00003398
Iteration 89/1000 | Loss: 0.00003398
Iteration 90/1000 | Loss: 0.00003398
Iteration 91/1000 | Loss: 0.00003398
Iteration 92/1000 | Loss: 0.00003397
Iteration 93/1000 | Loss: 0.00003397
Iteration 94/1000 | Loss: 0.00003397
Iteration 95/1000 | Loss: 0.00003397
Iteration 96/1000 | Loss: 0.00003397
Iteration 97/1000 | Loss: 0.00003397
Iteration 98/1000 | Loss: 0.00003397
Iteration 99/1000 | Loss: 0.00003397
Iteration 100/1000 | Loss: 0.00003397
Iteration 101/1000 | Loss: 0.00003397
Iteration 102/1000 | Loss: 0.00003397
Iteration 103/1000 | Loss: 0.00003429
Iteration 104/1000 | Loss: 0.00003396
Iteration 105/1000 | Loss: 0.00003396
Iteration 106/1000 | Loss: 0.00003396
Iteration 107/1000 | Loss: 0.00003396
Iteration 108/1000 | Loss: 0.00003396
Iteration 109/1000 | Loss: 0.00003395
Iteration 110/1000 | Loss: 0.00003395
Iteration 111/1000 | Loss: 0.00003395
Iteration 112/1000 | Loss: 0.00003395
Iteration 113/1000 | Loss: 0.00003395
Iteration 114/1000 | Loss: 0.00003395
Iteration 115/1000 | Loss: 0.00003395
Iteration 116/1000 | Loss: 0.00003394
Iteration 117/1000 | Loss: 0.00003394
Iteration 118/1000 | Loss: 0.00003394
Iteration 119/1000 | Loss: 0.00003394
Iteration 120/1000 | Loss: 0.00003394
Iteration 121/1000 | Loss: 0.00003394
Iteration 122/1000 | Loss: 0.00003394
Iteration 123/1000 | Loss: 0.00003394
Iteration 124/1000 | Loss: 0.00003394
Iteration 125/1000 | Loss: 0.00003394
Iteration 126/1000 | Loss: 0.00003394
Iteration 127/1000 | Loss: 0.00003393
Iteration 128/1000 | Loss: 0.00003393
Iteration 129/1000 | Loss: 0.00003393
Iteration 130/1000 | Loss: 0.00003393
Iteration 131/1000 | Loss: 0.00003393
Iteration 132/1000 | Loss: 0.00003392
Iteration 133/1000 | Loss: 0.00003392
Iteration 134/1000 | Loss: 0.00003392
Iteration 135/1000 | Loss: 0.00003392
Iteration 136/1000 | Loss: 0.00003392
Iteration 137/1000 | Loss: 0.00003392
Iteration 138/1000 | Loss: 0.00003392
Iteration 139/1000 | Loss: 0.00003391
Iteration 140/1000 | Loss: 0.00003391
Iteration 141/1000 | Loss: 0.00003391
Iteration 142/1000 | Loss: 0.00003391
Iteration 143/1000 | Loss: 0.00003391
Iteration 144/1000 | Loss: 0.00003391
Iteration 145/1000 | Loss: 0.00003391
Iteration 146/1000 | Loss: 0.00003391
Iteration 147/1000 | Loss: 0.00003391
Iteration 148/1000 | Loss: 0.00003391
Iteration 149/1000 | Loss: 0.00003391
Iteration 150/1000 | Loss: 0.00003391
Iteration 151/1000 | Loss: 0.00003391
Iteration 152/1000 | Loss: 0.00003391
Iteration 153/1000 | Loss: 0.00003391
Iteration 154/1000 | Loss: 0.00003391
Iteration 155/1000 | Loss: 0.00003390
Iteration 156/1000 | Loss: 0.00003390
Iteration 157/1000 | Loss: 0.00003390
Iteration 158/1000 | Loss: 0.00003390
Iteration 159/1000 | Loss: 0.00003390
Iteration 160/1000 | Loss: 0.00003390
Iteration 161/1000 | Loss: 0.00003390
Iteration 162/1000 | Loss: 0.00003390
Iteration 163/1000 | Loss: 0.00003389
Iteration 164/1000 | Loss: 0.00003389
Iteration 165/1000 | Loss: 0.00003389
Iteration 166/1000 | Loss: 0.00003389
Iteration 167/1000 | Loss: 0.00003389
Iteration 168/1000 | Loss: 0.00003389
Iteration 169/1000 | Loss: 0.00003389
Iteration 170/1000 | Loss: 0.00003388
Iteration 171/1000 | Loss: 0.00003388
Iteration 172/1000 | Loss: 0.00003577
Iteration 173/1000 | Loss: 0.00003394
Iteration 174/1000 | Loss: 0.00003401
Iteration 175/1000 | Loss: 0.00003389
Iteration 176/1000 | Loss: 0.00003389
Iteration 177/1000 | Loss: 0.00003389
Iteration 178/1000 | Loss: 0.00003389
Iteration 179/1000 | Loss: 0.00003389
Iteration 180/1000 | Loss: 0.00003389
Iteration 181/1000 | Loss: 0.00003389
Iteration 182/1000 | Loss: 0.00003389
Iteration 183/1000 | Loss: 0.00003389
Iteration 184/1000 | Loss: 0.00003389
Iteration 185/1000 | Loss: 0.00003389
Iteration 186/1000 | Loss: 0.00003389
Iteration 187/1000 | Loss: 0.00003389
Iteration 188/1000 | Loss: 0.00003389
Iteration 189/1000 | Loss: 0.00003389
Iteration 190/1000 | Loss: 0.00003388
Iteration 191/1000 | Loss: 0.00003388
Iteration 192/1000 | Loss: 0.00003388
Iteration 193/1000 | Loss: 0.00003388
Iteration 194/1000 | Loss: 0.00003388
Iteration 195/1000 | Loss: 0.00003388
Iteration 196/1000 | Loss: 0.00003388
Iteration 197/1000 | Loss: 0.00003388
Iteration 198/1000 | Loss: 0.00003388
Iteration 199/1000 | Loss: 0.00003388
Iteration 200/1000 | Loss: 0.00003387
Iteration 201/1000 | Loss: 0.00003387
Iteration 202/1000 | Loss: 0.00003387
Iteration 203/1000 | Loss: 0.00003387
Iteration 204/1000 | Loss: 0.00003387
Iteration 205/1000 | Loss: 0.00003387
Iteration 206/1000 | Loss: 0.00003387
Iteration 207/1000 | Loss: 0.00003387
Iteration 208/1000 | Loss: 0.00003387
Iteration 209/1000 | Loss: 0.00003387
Iteration 210/1000 | Loss: 0.00003387
Iteration 211/1000 | Loss: 0.00003387
Iteration 212/1000 | Loss: 0.00003387
Iteration 213/1000 | Loss: 0.00003387
Iteration 214/1000 | Loss: 0.00003387
Iteration 215/1000 | Loss: 0.00003386
Iteration 216/1000 | Loss: 0.00003399
Iteration 217/1000 | Loss: 0.00003387
Iteration 218/1000 | Loss: 0.00003387
Iteration 219/1000 | Loss: 0.00003387
Iteration 220/1000 | Loss: 0.00003387
Iteration 221/1000 | Loss: 0.00003387
Iteration 222/1000 | Loss: 0.00003387
Iteration 223/1000 | Loss: 0.00003387
Iteration 224/1000 | Loss: 0.00003387
Iteration 225/1000 | Loss: 0.00003387
Iteration 226/1000 | Loss: 0.00003387
Iteration 227/1000 | Loss: 0.00003387
Iteration 228/1000 | Loss: 0.00003387
Iteration 229/1000 | Loss: 0.00003387
Iteration 230/1000 | Loss: 0.00003387
Iteration 231/1000 | Loss: 0.00003387
Iteration 232/1000 | Loss: 0.00003387
Iteration 233/1000 | Loss: 0.00003387
Iteration 234/1000 | Loss: 0.00003387
Iteration 235/1000 | Loss: 0.00003387
Iteration 236/1000 | Loss: 0.00003387
Iteration 237/1000 | Loss: 0.00003387
Iteration 238/1000 | Loss: 0.00003387
Iteration 239/1000 | Loss: 0.00003387
Iteration 240/1000 | Loss: 0.00003387
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 240. Stopping optimization.
Last 5 losses: [3.386689422768541e-05, 3.386689422768541e-05, 3.386689422768541e-05, 3.386689422768541e-05, 3.386689422768541e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.386689422768541e-05

Optimization complete. Final v2v error: 5.018170356750488 mm

Highest mean error: 15.144601821899414 mm for frame 68

Lowest mean error: 4.52940559387207 mm for frame 2

Saving results

Total time: 91.8611228466034
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_0196/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0196/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0196/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01039165
Iteration 2/25 | Loss: 0.00237718
Iteration 3/25 | Loss: 0.00190168
Iteration 4/25 | Loss: 0.00180754
Iteration 5/25 | Loss: 0.00177345
Iteration 6/25 | Loss: 0.00170317
Iteration 7/25 | Loss: 0.00167384
Iteration 8/25 | Loss: 0.00164220
Iteration 9/25 | Loss: 0.00163495
Iteration 10/25 | Loss: 0.00162532
Iteration 11/25 | Loss: 0.00161689
Iteration 12/25 | Loss: 0.00161857
Iteration 13/25 | Loss: 0.00162014
Iteration 14/25 | Loss: 0.00160988
Iteration 15/25 | Loss: 0.00160636
Iteration 16/25 | Loss: 0.00160573
Iteration 17/25 | Loss: 0.00160554
Iteration 18/25 | Loss: 0.00160545
Iteration 19/25 | Loss: 0.00160545
Iteration 20/25 | Loss: 0.00160543
Iteration 21/25 | Loss: 0.00160542
Iteration 22/25 | Loss: 0.00160542
Iteration 23/25 | Loss: 0.00160542
Iteration 24/25 | Loss: 0.00160541
Iteration 25/25 | Loss: 0.00160541

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.44884396
Iteration 2/25 | Loss: 0.00266995
Iteration 3/25 | Loss: 0.00266860
Iteration 4/25 | Loss: 0.00266859
Iteration 5/25 | Loss: 0.00266859
Iteration 6/25 | Loss: 0.00266859
Iteration 7/25 | Loss: 0.00266859
Iteration 8/25 | Loss: 0.00266859
Iteration 9/25 | Loss: 0.00266859
Iteration 10/25 | Loss: 0.00266859
Iteration 11/25 | Loss: 0.00266859
Iteration 12/25 | Loss: 0.00266859
Iteration 13/25 | Loss: 0.00266859
Iteration 14/25 | Loss: 0.00266859
Iteration 15/25 | Loss: 0.00266859
Iteration 16/25 | Loss: 0.00266859
Iteration 17/25 | Loss: 0.00266859
Iteration 18/25 | Loss: 0.00266859
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.002668592380359769, 0.002668592380359769, 0.002668592380359769, 0.002668592380359769, 0.002668592380359769]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002668592380359769

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00266859
Iteration 2/1000 | Loss: 0.00042020
Iteration 3/1000 | Loss: 0.00035553
Iteration 4/1000 | Loss: 0.00017714
Iteration 5/1000 | Loss: 0.00014346
Iteration 6/1000 | Loss: 0.00010967
Iteration 7/1000 | Loss: 0.00009649
Iteration 8/1000 | Loss: 0.00008830
Iteration 9/1000 | Loss: 0.00008276
Iteration 10/1000 | Loss: 0.00029530
Iteration 11/1000 | Loss: 0.00011480
Iteration 12/1000 | Loss: 0.00009680
Iteration 13/1000 | Loss: 0.00008431
Iteration 14/1000 | Loss: 0.00007490
Iteration 15/1000 | Loss: 0.00006836
Iteration 16/1000 | Loss: 0.00006352
Iteration 17/1000 | Loss: 0.00006151
Iteration 18/1000 | Loss: 0.00005974
Iteration 19/1000 | Loss: 0.00005839
Iteration 20/1000 | Loss: 0.00005732
Iteration 21/1000 | Loss: 0.00005662
Iteration 22/1000 | Loss: 0.00005620
Iteration 23/1000 | Loss: 0.00010291
Iteration 24/1000 | Loss: 0.00006411
Iteration 25/1000 | Loss: 0.00006021
Iteration 26/1000 | Loss: 0.00008371
Iteration 27/1000 | Loss: 0.00018588
Iteration 28/1000 | Loss: 0.00016902
Iteration 29/1000 | Loss: 0.00010325
Iteration 30/1000 | Loss: 0.00009455
Iteration 31/1000 | Loss: 0.00007705
Iteration 32/1000 | Loss: 0.00011468
Iteration 33/1000 | Loss: 0.00017151
Iteration 34/1000 | Loss: 0.00010228
Iteration 35/1000 | Loss: 0.00016587
Iteration 36/1000 | Loss: 0.00011637
Iteration 37/1000 | Loss: 0.00011758
Iteration 38/1000 | Loss: 0.00012543
Iteration 39/1000 | Loss: 0.00011240
Iteration 40/1000 | Loss: 0.00008800
Iteration 41/1000 | Loss: 0.00008657
Iteration 42/1000 | Loss: 0.00007406
Iteration 43/1000 | Loss: 0.00008356
Iteration 44/1000 | Loss: 0.00007282
Iteration 45/1000 | Loss: 0.00006765
Iteration 46/1000 | Loss: 0.00006258
Iteration 47/1000 | Loss: 0.00006293
Iteration 48/1000 | Loss: 0.00009702
Iteration 49/1000 | Loss: 0.00008921
Iteration 50/1000 | Loss: 0.00007187
Iteration 51/1000 | Loss: 0.00008592
Iteration 52/1000 | Loss: 0.00008224
Iteration 53/1000 | Loss: 0.00008283
Iteration 54/1000 | Loss: 0.00011295
Iteration 55/1000 | Loss: 0.00009991
Iteration 56/1000 | Loss: 0.00009474
Iteration 57/1000 | Loss: 0.00008362
Iteration 58/1000 | Loss: 0.00009888
Iteration 59/1000 | Loss: 0.00009950
Iteration 60/1000 | Loss: 0.00007301
Iteration 61/1000 | Loss: 0.00007735
Iteration 62/1000 | Loss: 0.00008533
Iteration 63/1000 | Loss: 0.00008192
Iteration 64/1000 | Loss: 0.00009043
Iteration 65/1000 | Loss: 0.00008776
Iteration 66/1000 | Loss: 0.00006560
Iteration 67/1000 | Loss: 0.00006633
Iteration 68/1000 | Loss: 0.00010668
Iteration 69/1000 | Loss: 0.00009857
Iteration 70/1000 | Loss: 0.00009576
Iteration 71/1000 | Loss: 0.00009054
Iteration 72/1000 | Loss: 0.00009477
Iteration 73/1000 | Loss: 0.00008796
Iteration 74/1000 | Loss: 0.00008561
Iteration 75/1000 | Loss: 0.00008389
Iteration 76/1000 | Loss: 0.00008234
Iteration 77/1000 | Loss: 0.00008313
Iteration 78/1000 | Loss: 0.00009245
Iteration 79/1000 | Loss: 0.00007826
Iteration 80/1000 | Loss: 0.00006760
Iteration 81/1000 | Loss: 0.00009484
Iteration 82/1000 | Loss: 0.00012464
Iteration 83/1000 | Loss: 0.00009241
Iteration 84/1000 | Loss: 0.00005588
Iteration 85/1000 | Loss: 0.00007550
Iteration 86/1000 | Loss: 0.00007236
Iteration 87/1000 | Loss: 0.00009525
Iteration 88/1000 | Loss: 0.00008916
Iteration 89/1000 | Loss: 0.00006604
Iteration 90/1000 | Loss: 0.00007620
Iteration 91/1000 | Loss: 0.00010560
Iteration 92/1000 | Loss: 0.00008650
Iteration 93/1000 | Loss: 0.00008866
Iteration 94/1000 | Loss: 0.00008724
Iteration 95/1000 | Loss: 0.00010932
Iteration 96/1000 | Loss: 0.00009126
Iteration 97/1000 | Loss: 0.00007727
Iteration 98/1000 | Loss: 0.00008674
Iteration 99/1000 | Loss: 0.00008881
Iteration 100/1000 | Loss: 0.00006771
Iteration 101/1000 | Loss: 0.00009636
Iteration 102/1000 | Loss: 0.00010023
Iteration 103/1000 | Loss: 0.00008590
Iteration 104/1000 | Loss: 0.00007989
Iteration 105/1000 | Loss: 0.00010234
Iteration 106/1000 | Loss: 0.00009224
Iteration 107/1000 | Loss: 0.00009516
Iteration 108/1000 | Loss: 0.00006658
Iteration 109/1000 | Loss: 0.00007258
Iteration 110/1000 | Loss: 0.00006963
Iteration 111/1000 | Loss: 0.00008140
Iteration 112/1000 | Loss: 0.00007639
Iteration 113/1000 | Loss: 0.00006231
Iteration 114/1000 | Loss: 0.00005813
Iteration 115/1000 | Loss: 0.00006467
Iteration 116/1000 | Loss: 0.00009049
Iteration 117/1000 | Loss: 0.00007336
Iteration 118/1000 | Loss: 0.00006918
Iteration 119/1000 | Loss: 0.00007667
Iteration 120/1000 | Loss: 0.00007159
Iteration 121/1000 | Loss: 0.00007955
Iteration 122/1000 | Loss: 0.00008856
Iteration 123/1000 | Loss: 0.00008515
Iteration 124/1000 | Loss: 0.00008752
Iteration 125/1000 | Loss: 0.00007454
Iteration 126/1000 | Loss: 0.00008110
Iteration 127/1000 | Loss: 0.00008280
Iteration 128/1000 | Loss: 0.00010456
Iteration 129/1000 | Loss: 0.00008576
Iteration 130/1000 | Loss: 0.00007438
Iteration 131/1000 | Loss: 0.00007041
Iteration 132/1000 | Loss: 0.00006782
Iteration 133/1000 | Loss: 0.00006213
Iteration 134/1000 | Loss: 0.00007139
Iteration 135/1000 | Loss: 0.00007527
Iteration 136/1000 | Loss: 0.00005684
Iteration 137/1000 | Loss: 0.00005689
Iteration 138/1000 | Loss: 0.00005450
Iteration 139/1000 | Loss: 0.00007408
Iteration 140/1000 | Loss: 0.00008253
Iteration 141/1000 | Loss: 0.00007478
Iteration 142/1000 | Loss: 0.00005678
Iteration 143/1000 | Loss: 0.00006480
Iteration 144/1000 | Loss: 0.00006358
Iteration 145/1000 | Loss: 0.00007412
Iteration 146/1000 | Loss: 0.00006429
Iteration 147/1000 | Loss: 0.00007094
Iteration 148/1000 | Loss: 0.00007633
Iteration 149/1000 | Loss: 0.00006198
Iteration 150/1000 | Loss: 0.00008124
Iteration 151/1000 | Loss: 0.00008034
Iteration 152/1000 | Loss: 0.00007505
Iteration 153/1000 | Loss: 0.00006300
Iteration 154/1000 | Loss: 0.00008242
Iteration 155/1000 | Loss: 0.00008860
Iteration 156/1000 | Loss: 0.00008065
Iteration 157/1000 | Loss: 0.00007063
Iteration 158/1000 | Loss: 0.00007067
Iteration 159/1000 | Loss: 0.00008735
Iteration 160/1000 | Loss: 0.00008497
Iteration 161/1000 | Loss: 0.00007944
Iteration 162/1000 | Loss: 0.00007343
Iteration 163/1000 | Loss: 0.00006370
Iteration 164/1000 | Loss: 0.00007318
Iteration 165/1000 | Loss: 0.00006982
Iteration 166/1000 | Loss: 0.00008065
Iteration 167/1000 | Loss: 0.00007653
Iteration 168/1000 | Loss: 0.00006968
Iteration 169/1000 | Loss: 0.00006160
Iteration 170/1000 | Loss: 0.00006369
Iteration 171/1000 | Loss: 0.00005940
Iteration 172/1000 | Loss: 0.00006253
Iteration 173/1000 | Loss: 0.00005662
Iteration 174/1000 | Loss: 0.00005930
Iteration 175/1000 | Loss: 0.00005127
Iteration 176/1000 | Loss: 0.00005743
Iteration 177/1000 | Loss: 0.00005787
Iteration 178/1000 | Loss: 0.00005030
Iteration 179/1000 | Loss: 0.00005273
Iteration 180/1000 | Loss: 0.00005594
Iteration 181/1000 | Loss: 0.00005577
Iteration 182/1000 | Loss: 0.00005626
Iteration 183/1000 | Loss: 0.00005584
Iteration 184/1000 | Loss: 0.00005638
Iteration 185/1000 | Loss: 0.00005760
Iteration 186/1000 | Loss: 0.00005616
Iteration 187/1000 | Loss: 0.00005792
Iteration 188/1000 | Loss: 0.00005746
Iteration 189/1000 | Loss: 0.00005570
Iteration 190/1000 | Loss: 0.00005510
Iteration 191/1000 | Loss: 0.00005558
Iteration 192/1000 | Loss: 0.00004999
Iteration 193/1000 | Loss: 0.00005603
Iteration 194/1000 | Loss: 0.00005817
Iteration 195/1000 | Loss: 0.00005060
Iteration 196/1000 | Loss: 0.00005517
Iteration 197/1000 | Loss: 0.00005481
Iteration 198/1000 | Loss: 0.00005532
Iteration 199/1000 | Loss: 0.00005509
Iteration 200/1000 | Loss: 0.00005539
Iteration 201/1000 | Loss: 0.00005194
Iteration 202/1000 | Loss: 0.00004983
Iteration 203/1000 | Loss: 0.00005601
Iteration 204/1000 | Loss: 0.00005577
Iteration 205/1000 | Loss: 0.00005789
Iteration 206/1000 | Loss: 0.00005707
Iteration 207/1000 | Loss: 0.00006472
Iteration 208/1000 | Loss: 0.00005627
Iteration 209/1000 | Loss: 0.00005328
Iteration 210/1000 | Loss: 0.00005115
Iteration 211/1000 | Loss: 0.00005209
Iteration 212/1000 | Loss: 0.00005144
Iteration 213/1000 | Loss: 0.00005184
Iteration 214/1000 | Loss: 0.00005834
Iteration 215/1000 | Loss: 0.00005703
Iteration 216/1000 | Loss: 0.00005588
Iteration 217/1000 | Loss: 0.00005578
Iteration 218/1000 | Loss: 0.00005317
Iteration 219/1000 | Loss: 0.00005497
Iteration 220/1000 | Loss: 0.00005752
Iteration 221/1000 | Loss: 0.00005838
Iteration 222/1000 | Loss: 0.00006694
Iteration 223/1000 | Loss: 0.00005563
Iteration 224/1000 | Loss: 0.00008448
Iteration 225/1000 | Loss: 0.00010034
Iteration 226/1000 | Loss: 0.00005594
Iteration 227/1000 | Loss: 0.00005733
Iteration 228/1000 | Loss: 0.00006436
Iteration 229/1000 | Loss: 0.00005894
Iteration 230/1000 | Loss: 0.00005199
Iteration 231/1000 | Loss: 0.00005372
Iteration 232/1000 | Loss: 0.00006080
Iteration 233/1000 | Loss: 0.00005304
Iteration 234/1000 | Loss: 0.00005430
Iteration 235/1000 | Loss: 0.00005201
Iteration 236/1000 | Loss: 0.00005916
Iteration 237/1000 | Loss: 0.00005717
Iteration 238/1000 | Loss: 0.00005432
Iteration 239/1000 | Loss: 0.00005542
Iteration 240/1000 | Loss: 0.00005212
Iteration 241/1000 | Loss: 0.00005120
Iteration 242/1000 | Loss: 0.00004800
Iteration 243/1000 | Loss: 0.00004754
Iteration 244/1000 | Loss: 0.00004740
Iteration 245/1000 | Loss: 0.00004730
Iteration 246/1000 | Loss: 0.00004730
Iteration 247/1000 | Loss: 0.00004729
Iteration 248/1000 | Loss: 0.00004729
Iteration 249/1000 | Loss: 0.00004728
Iteration 250/1000 | Loss: 0.00004728
Iteration 251/1000 | Loss: 0.00004728
Iteration 252/1000 | Loss: 0.00004728
Iteration 253/1000 | Loss: 0.00004728
Iteration 254/1000 | Loss: 0.00004728
Iteration 255/1000 | Loss: 0.00004727
Iteration 256/1000 | Loss: 0.00004727
Iteration 257/1000 | Loss: 0.00004727
Iteration 258/1000 | Loss: 0.00004727
Iteration 259/1000 | Loss: 0.00004726
Iteration 260/1000 | Loss: 0.00004721
Iteration 261/1000 | Loss: 0.00004720
Iteration 262/1000 | Loss: 0.00004720
Iteration 263/1000 | Loss: 0.00004720
Iteration 264/1000 | Loss: 0.00004719
Iteration 265/1000 | Loss: 0.00004719
Iteration 266/1000 | Loss: 0.00004719
Iteration 267/1000 | Loss: 0.00004718
Iteration 268/1000 | Loss: 0.00004718
Iteration 269/1000 | Loss: 0.00004718
Iteration 270/1000 | Loss: 0.00004717
Iteration 271/1000 | Loss: 0.00004717
Iteration 272/1000 | Loss: 0.00004717
Iteration 273/1000 | Loss: 0.00004717
Iteration 274/1000 | Loss: 0.00004717
Iteration 275/1000 | Loss: 0.00004717
Iteration 276/1000 | Loss: 0.00004716
Iteration 277/1000 | Loss: 0.00004716
Iteration 278/1000 | Loss: 0.00004716
Iteration 279/1000 | Loss: 0.00004716
Iteration 280/1000 | Loss: 0.00004715
Iteration 281/1000 | Loss: 0.00004715
Iteration 282/1000 | Loss: 0.00004715
Iteration 283/1000 | Loss: 0.00004715
Iteration 284/1000 | Loss: 0.00004715
Iteration 285/1000 | Loss: 0.00004715
Iteration 286/1000 | Loss: 0.00004715
Iteration 287/1000 | Loss: 0.00004714
Iteration 288/1000 | Loss: 0.00004714
Iteration 289/1000 | Loss: 0.00004714
Iteration 290/1000 | Loss: 0.00004714
Iteration 291/1000 | Loss: 0.00004714
Iteration 292/1000 | Loss: 0.00004714
Iteration 293/1000 | Loss: 0.00004714
Iteration 294/1000 | Loss: 0.00004714
Iteration 295/1000 | Loss: 0.00004714
Iteration 296/1000 | Loss: 0.00004714
Iteration 297/1000 | Loss: 0.00004714
Iteration 298/1000 | Loss: 0.00004714
Iteration 299/1000 | Loss: 0.00004714
Iteration 300/1000 | Loss: 0.00004714
Iteration 301/1000 | Loss: 0.00004714
Iteration 302/1000 | Loss: 0.00004714
Iteration 303/1000 | Loss: 0.00004714
Iteration 304/1000 | Loss: 0.00004714
Iteration 305/1000 | Loss: 0.00004714
Iteration 306/1000 | Loss: 0.00004714
Iteration 307/1000 | Loss: 0.00004714
Iteration 308/1000 | Loss: 0.00004714
Iteration 309/1000 | Loss: 0.00004714
Iteration 310/1000 | Loss: 0.00004714
Iteration 311/1000 | Loss: 0.00004714
Iteration 312/1000 | Loss: 0.00004714
Iteration 313/1000 | Loss: 0.00004714
Iteration 314/1000 | Loss: 0.00004714
Iteration 315/1000 | Loss: 0.00004714
Iteration 316/1000 | Loss: 0.00004714
Iteration 317/1000 | Loss: 0.00004714
Iteration 318/1000 | Loss: 0.00004714
Iteration 319/1000 | Loss: 0.00004714
Iteration 320/1000 | Loss: 0.00004714
Iteration 321/1000 | Loss: 0.00004714
Iteration 322/1000 | Loss: 0.00004714
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 322. Stopping optimization.
Last 5 losses: [4.714156239060685e-05, 4.714156239060685e-05, 4.714156239060685e-05, 4.714156239060685e-05, 4.714156239060685e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.714156239060685e-05

Optimization complete. Final v2v error: 5.4497151374816895 mm

Highest mean error: 11.390151977539062 mm for frame 117

Lowest mean error: 4.219120502471924 mm for frame 73

Saving results

Total time: 392.03420996665955
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_0196/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0196/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0196/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00520307
Iteration 2/25 | Loss: 0.00163913
Iteration 3/25 | Loss: 0.00148772
Iteration 4/25 | Loss: 0.00147446
Iteration 5/25 | Loss: 0.00146931
Iteration 6/25 | Loss: 0.00146820
Iteration 7/25 | Loss: 0.00146820
Iteration 8/25 | Loss: 0.00146820
Iteration 9/25 | Loss: 0.00146820
Iteration 10/25 | Loss: 0.00146820
Iteration 11/25 | Loss: 0.00146820
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001468195696361363, 0.001468195696361363, 0.001468195696361363, 0.001468195696361363, 0.001468195696361363]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001468195696361363

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63400078
Iteration 2/25 | Loss: 0.00281273
Iteration 3/25 | Loss: 0.00281273
Iteration 4/25 | Loss: 0.00281273
Iteration 5/25 | Loss: 0.00281273
Iteration 6/25 | Loss: 0.00281273
Iteration 7/25 | Loss: 0.00281273
Iteration 8/25 | Loss: 0.00281273
Iteration 9/25 | Loss: 0.00281273
Iteration 10/25 | Loss: 0.00281273
Iteration 11/25 | Loss: 0.00281273
Iteration 12/25 | Loss: 0.00281273
Iteration 13/25 | Loss: 0.00281273
Iteration 14/25 | Loss: 0.00281273
Iteration 15/25 | Loss: 0.00281273
Iteration 16/25 | Loss: 0.00281273
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0028127289842814207, 0.0028127289842814207, 0.0028127289842814207, 0.0028127289842814207, 0.0028127289842814207]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0028127289842814207

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00281273
Iteration 2/1000 | Loss: 0.00005078
Iteration 3/1000 | Loss: 0.00003853
Iteration 4/1000 | Loss: 0.00003559
Iteration 5/1000 | Loss: 0.00003372
Iteration 6/1000 | Loss: 0.00003281
Iteration 7/1000 | Loss: 0.00003210
Iteration 8/1000 | Loss: 0.00003161
Iteration 9/1000 | Loss: 0.00003127
Iteration 10/1000 | Loss: 0.00003103
Iteration 11/1000 | Loss: 0.00003089
Iteration 12/1000 | Loss: 0.00003073
Iteration 13/1000 | Loss: 0.00003073
Iteration 14/1000 | Loss: 0.00003069
Iteration 15/1000 | Loss: 0.00003055
Iteration 16/1000 | Loss: 0.00003052
Iteration 17/1000 | Loss: 0.00003048
Iteration 18/1000 | Loss: 0.00003048
Iteration 19/1000 | Loss: 0.00003047
Iteration 20/1000 | Loss: 0.00003047
Iteration 21/1000 | Loss: 0.00003044
Iteration 22/1000 | Loss: 0.00003044
Iteration 23/1000 | Loss: 0.00003043
Iteration 24/1000 | Loss: 0.00003043
Iteration 25/1000 | Loss: 0.00003041
Iteration 26/1000 | Loss: 0.00003041
Iteration 27/1000 | Loss: 0.00003041
Iteration 28/1000 | Loss: 0.00003041
Iteration 29/1000 | Loss: 0.00003041
Iteration 30/1000 | Loss: 0.00003041
Iteration 31/1000 | Loss: 0.00003041
Iteration 32/1000 | Loss: 0.00003041
Iteration 33/1000 | Loss: 0.00003041
Iteration 34/1000 | Loss: 0.00003040
Iteration 35/1000 | Loss: 0.00003040
Iteration 36/1000 | Loss: 0.00003040
Iteration 37/1000 | Loss: 0.00003039
Iteration 38/1000 | Loss: 0.00003038
Iteration 39/1000 | Loss: 0.00003038
Iteration 40/1000 | Loss: 0.00003038
Iteration 41/1000 | Loss: 0.00003037
Iteration 42/1000 | Loss: 0.00003037
Iteration 43/1000 | Loss: 0.00003037
Iteration 44/1000 | Loss: 0.00003036
Iteration 45/1000 | Loss: 0.00003036
Iteration 46/1000 | Loss: 0.00003036
Iteration 47/1000 | Loss: 0.00003035
Iteration 48/1000 | Loss: 0.00003035
Iteration 49/1000 | Loss: 0.00003035
Iteration 50/1000 | Loss: 0.00003034
Iteration 51/1000 | Loss: 0.00003034
Iteration 52/1000 | Loss: 0.00003033
Iteration 53/1000 | Loss: 0.00003033
Iteration 54/1000 | Loss: 0.00003033
Iteration 55/1000 | Loss: 0.00003033
Iteration 56/1000 | Loss: 0.00003033
Iteration 57/1000 | Loss: 0.00003033
Iteration 58/1000 | Loss: 0.00003033
Iteration 59/1000 | Loss: 0.00003032
Iteration 60/1000 | Loss: 0.00003032
Iteration 61/1000 | Loss: 0.00003032
Iteration 62/1000 | Loss: 0.00003031
Iteration 63/1000 | Loss: 0.00003031
Iteration 64/1000 | Loss: 0.00003030
Iteration 65/1000 | Loss: 0.00003030
Iteration 66/1000 | Loss: 0.00003030
Iteration 67/1000 | Loss: 0.00003030
Iteration 68/1000 | Loss: 0.00003030
Iteration 69/1000 | Loss: 0.00003030
Iteration 70/1000 | Loss: 0.00003030
Iteration 71/1000 | Loss: 0.00003030
Iteration 72/1000 | Loss: 0.00003030
Iteration 73/1000 | Loss: 0.00003029
Iteration 74/1000 | Loss: 0.00003029
Iteration 75/1000 | Loss: 0.00003028
Iteration 76/1000 | Loss: 0.00003028
Iteration 77/1000 | Loss: 0.00003028
Iteration 78/1000 | Loss: 0.00003028
Iteration 79/1000 | Loss: 0.00003028
Iteration 80/1000 | Loss: 0.00003028
Iteration 81/1000 | Loss: 0.00003028
Iteration 82/1000 | Loss: 0.00003028
Iteration 83/1000 | Loss: 0.00003028
Iteration 84/1000 | Loss: 0.00003027
Iteration 85/1000 | Loss: 0.00003027
Iteration 86/1000 | Loss: 0.00003027
Iteration 87/1000 | Loss: 0.00003027
Iteration 88/1000 | Loss: 0.00003027
Iteration 89/1000 | Loss: 0.00003027
Iteration 90/1000 | Loss: 0.00003027
Iteration 91/1000 | Loss: 0.00003027
Iteration 92/1000 | Loss: 0.00003027
Iteration 93/1000 | Loss: 0.00003027
Iteration 94/1000 | Loss: 0.00003026
Iteration 95/1000 | Loss: 0.00003026
Iteration 96/1000 | Loss: 0.00003026
Iteration 97/1000 | Loss: 0.00003026
Iteration 98/1000 | Loss: 0.00003026
Iteration 99/1000 | Loss: 0.00003025
Iteration 100/1000 | Loss: 0.00003025
Iteration 101/1000 | Loss: 0.00003025
Iteration 102/1000 | Loss: 0.00003025
Iteration 103/1000 | Loss: 0.00003025
Iteration 104/1000 | Loss: 0.00003025
Iteration 105/1000 | Loss: 0.00003025
Iteration 106/1000 | Loss: 0.00003025
Iteration 107/1000 | Loss: 0.00003025
Iteration 108/1000 | Loss: 0.00003024
Iteration 109/1000 | Loss: 0.00003024
Iteration 110/1000 | Loss: 0.00003024
Iteration 111/1000 | Loss: 0.00003023
Iteration 112/1000 | Loss: 0.00003023
Iteration 113/1000 | Loss: 0.00003023
Iteration 114/1000 | Loss: 0.00003023
Iteration 115/1000 | Loss: 0.00003023
Iteration 116/1000 | Loss: 0.00003023
Iteration 117/1000 | Loss: 0.00003023
Iteration 118/1000 | Loss: 0.00003023
Iteration 119/1000 | Loss: 0.00003023
Iteration 120/1000 | Loss: 0.00003022
Iteration 121/1000 | Loss: 0.00003022
Iteration 122/1000 | Loss: 0.00003022
Iteration 123/1000 | Loss: 0.00003022
Iteration 124/1000 | Loss: 0.00003022
Iteration 125/1000 | Loss: 0.00003022
Iteration 126/1000 | Loss: 0.00003022
Iteration 127/1000 | Loss: 0.00003022
Iteration 128/1000 | Loss: 0.00003022
Iteration 129/1000 | Loss: 0.00003021
Iteration 130/1000 | Loss: 0.00003021
Iteration 131/1000 | Loss: 0.00003021
Iteration 132/1000 | Loss: 0.00003021
Iteration 133/1000 | Loss: 0.00003021
Iteration 134/1000 | Loss: 0.00003021
Iteration 135/1000 | Loss: 0.00003021
Iteration 136/1000 | Loss: 0.00003021
Iteration 137/1000 | Loss: 0.00003021
Iteration 138/1000 | Loss: 0.00003021
Iteration 139/1000 | Loss: 0.00003021
Iteration 140/1000 | Loss: 0.00003021
Iteration 141/1000 | Loss: 0.00003021
Iteration 142/1000 | Loss: 0.00003021
Iteration 143/1000 | Loss: 0.00003021
Iteration 144/1000 | Loss: 0.00003021
Iteration 145/1000 | Loss: 0.00003021
Iteration 146/1000 | Loss: 0.00003021
Iteration 147/1000 | Loss: 0.00003021
Iteration 148/1000 | Loss: 0.00003021
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [3.02146581816487e-05, 3.02146581816487e-05, 3.02146581816487e-05, 3.02146581816487e-05, 3.02146581816487e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.02146581816487e-05

Optimization complete. Final v2v error: 4.729792594909668 mm

Highest mean error: 5.186088562011719 mm for frame 15

Lowest mean error: 4.102715015411377 mm for frame 189

Saving results

Total time: 41.7586829662323
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_0196/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0196/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0196/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00965296
Iteration 2/25 | Loss: 0.00193888
Iteration 3/25 | Loss: 0.00159610
Iteration 4/25 | Loss: 0.00154630
Iteration 5/25 | Loss: 0.00153906
Iteration 6/25 | Loss: 0.00153882
Iteration 7/25 | Loss: 0.00153882
Iteration 8/25 | Loss: 0.00153882
Iteration 9/25 | Loss: 0.00153882
Iteration 10/25 | Loss: 0.00153882
Iteration 11/25 | Loss: 0.00153882
Iteration 12/25 | Loss: 0.00153882
Iteration 13/25 | Loss: 0.00153882
Iteration 14/25 | Loss: 0.00153882
Iteration 15/25 | Loss: 0.00153882
Iteration 16/25 | Loss: 0.00153882
Iteration 17/25 | Loss: 0.00153882
Iteration 18/25 | Loss: 0.00153882
Iteration 19/25 | Loss: 0.00153882
Iteration 20/25 | Loss: 0.00153882
Iteration 21/25 | Loss: 0.00153882
Iteration 22/25 | Loss: 0.00153882
Iteration 23/25 | Loss: 0.00153882
Iteration 24/25 | Loss: 0.00153882
Iteration 25/25 | Loss: 0.00153882

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63823473
Iteration 2/25 | Loss: 0.00268723
Iteration 3/25 | Loss: 0.00268723
Iteration 4/25 | Loss: 0.00268723
Iteration 5/25 | Loss: 0.00268723
Iteration 6/25 | Loss: 0.00268723
Iteration 7/25 | Loss: 0.00268723
Iteration 8/25 | Loss: 0.00268723
Iteration 9/25 | Loss: 0.00268723
Iteration 10/25 | Loss: 0.00268723
Iteration 11/25 | Loss: 0.00268723
Iteration 12/25 | Loss: 0.00268723
Iteration 13/25 | Loss: 0.00268723
Iteration 14/25 | Loss: 0.00268723
Iteration 15/25 | Loss: 0.00268723
Iteration 16/25 | Loss: 0.00268723
Iteration 17/25 | Loss: 0.00268723
Iteration 18/25 | Loss: 0.00268723
Iteration 19/25 | Loss: 0.00268723
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0026872267480939627, 0.0026872267480939627, 0.0026872267480939627, 0.0026872267480939627, 0.0026872267480939627]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026872267480939627

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00268723
Iteration 2/1000 | Loss: 0.00004922
Iteration 3/1000 | Loss: 0.00004160
Iteration 4/1000 | Loss: 0.00003882
Iteration 5/1000 | Loss: 0.00003727
Iteration 6/1000 | Loss: 0.00003620
Iteration 7/1000 | Loss: 0.00003551
Iteration 8/1000 | Loss: 0.00003512
Iteration 9/1000 | Loss: 0.00003464
Iteration 10/1000 | Loss: 0.00003442
Iteration 11/1000 | Loss: 0.00003440
Iteration 12/1000 | Loss: 0.00003424
Iteration 13/1000 | Loss: 0.00003405
Iteration 14/1000 | Loss: 0.00003397
Iteration 15/1000 | Loss: 0.00003397
Iteration 16/1000 | Loss: 0.00003390
Iteration 17/1000 | Loss: 0.00003389
Iteration 18/1000 | Loss: 0.00003388
Iteration 19/1000 | Loss: 0.00003388
Iteration 20/1000 | Loss: 0.00003388
Iteration 21/1000 | Loss: 0.00003388
Iteration 22/1000 | Loss: 0.00003388
Iteration 23/1000 | Loss: 0.00003388
Iteration 24/1000 | Loss: 0.00003388
Iteration 25/1000 | Loss: 0.00003388
Iteration 26/1000 | Loss: 0.00003388
Iteration 27/1000 | Loss: 0.00003388
Iteration 28/1000 | Loss: 0.00003388
Iteration 29/1000 | Loss: 0.00003387
Iteration 30/1000 | Loss: 0.00003387
Iteration 31/1000 | Loss: 0.00003387
Iteration 32/1000 | Loss: 0.00003387
Iteration 33/1000 | Loss: 0.00003387
Iteration 34/1000 | Loss: 0.00003387
Iteration 35/1000 | Loss: 0.00003387
Iteration 36/1000 | Loss: 0.00003387
Iteration 37/1000 | Loss: 0.00003386
Iteration 38/1000 | Loss: 0.00003386
Iteration 39/1000 | Loss: 0.00003386
Iteration 40/1000 | Loss: 0.00003386
Iteration 41/1000 | Loss: 0.00003386
Iteration 42/1000 | Loss: 0.00003386
Iteration 43/1000 | Loss: 0.00003386
Iteration 44/1000 | Loss: 0.00003386
Iteration 45/1000 | Loss: 0.00003386
Iteration 46/1000 | Loss: 0.00003386
Iteration 47/1000 | Loss: 0.00003386
Iteration 48/1000 | Loss: 0.00003386
Iteration 49/1000 | Loss: 0.00003386
Iteration 50/1000 | Loss: 0.00003386
Iteration 51/1000 | Loss: 0.00003386
Iteration 52/1000 | Loss: 0.00003386
Iteration 53/1000 | Loss: 0.00003386
Iteration 54/1000 | Loss: 0.00003386
Iteration 55/1000 | Loss: 0.00003386
Iteration 56/1000 | Loss: 0.00003386
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 56. Stopping optimization.
Last 5 losses: [3.3863456337712705e-05, 3.3863456337712705e-05, 3.3863456337712705e-05, 3.3863456337712705e-05, 3.3863456337712705e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.3863456337712705e-05

Optimization complete. Final v2v error: 4.9460015296936035 mm

Highest mean error: 5.197946548461914 mm for frame 121

Lowest mean error: 4.806097507476807 mm for frame 164

Saving results

Total time: 29.15000009536743
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_0196/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0196/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0196/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00957622
Iteration 2/25 | Loss: 0.00329767
Iteration 3/25 | Loss: 0.00276011
Iteration 4/25 | Loss: 0.00262757
Iteration 5/25 | Loss: 0.00258343
Iteration 6/25 | Loss: 0.00245865
Iteration 7/25 | Loss: 0.00227953
Iteration 8/25 | Loss: 0.00221462
Iteration 9/25 | Loss: 0.00218383
Iteration 10/25 | Loss: 0.00218623
Iteration 11/25 | Loss: 0.00218179
Iteration 12/25 | Loss: 0.00217108
Iteration 13/25 | Loss: 0.00216845
Iteration 14/25 | Loss: 0.00218946
Iteration 15/25 | Loss: 0.00221860
Iteration 16/25 | Loss: 0.00226099
Iteration 17/25 | Loss: 0.00216444
Iteration 18/25 | Loss: 0.00213202
Iteration 19/25 | Loss: 0.00212393
Iteration 20/25 | Loss: 0.00212073
Iteration 21/25 | Loss: 0.00211901
Iteration 22/25 | Loss: 0.00212422
Iteration 23/25 | Loss: 0.00212274
Iteration 24/25 | Loss: 0.00211551
Iteration 25/25 | Loss: 0.00211356

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59937096
Iteration 2/25 | Loss: 0.00818215
Iteration 3/25 | Loss: 0.00314140
Iteration 4/25 | Loss: 0.00314139
Iteration 5/25 | Loss: 0.00314139
Iteration 6/25 | Loss: 0.00314139
Iteration 7/25 | Loss: 0.00314139
Iteration 8/25 | Loss: 0.00314139
Iteration 9/25 | Loss: 0.00314139
Iteration 10/25 | Loss: 0.00314139
Iteration 11/25 | Loss: 0.00314139
Iteration 12/25 | Loss: 0.00314139
Iteration 13/25 | Loss: 0.00314139
Iteration 14/25 | Loss: 0.00314139
Iteration 15/25 | Loss: 0.00314139
Iteration 16/25 | Loss: 0.00314139
Iteration 17/25 | Loss: 0.00314139
Iteration 18/25 | Loss: 0.00314139
Iteration 19/25 | Loss: 0.00314139
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0031413903925567865, 0.0031413903925567865, 0.0031413903925567865, 0.0031413903925567865, 0.0031413903925567865]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0031413903925567865

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00314139
Iteration 2/1000 | Loss: 0.00681708
Iteration 3/1000 | Loss: 0.00041933
Iteration 4/1000 | Loss: 0.00029899
Iteration 5/1000 | Loss: 0.00027400
Iteration 6/1000 | Loss: 0.00026396
Iteration 7/1000 | Loss: 0.00025057
Iteration 8/1000 | Loss: 0.00024175
Iteration 9/1000 | Loss: 0.00042793
Iteration 10/1000 | Loss: 0.00024805
Iteration 11/1000 | Loss: 0.00032290
Iteration 12/1000 | Loss: 0.00023727
Iteration 13/1000 | Loss: 0.00023208
Iteration 14/1000 | Loss: 0.00022940
Iteration 15/1000 | Loss: 0.00022805
Iteration 16/1000 | Loss: 0.00022784
Iteration 17/1000 | Loss: 0.00022574
Iteration 18/1000 | Loss: 0.00022439
Iteration 19/1000 | Loss: 0.00024124
Iteration 20/1000 | Loss: 0.00022647
Iteration 21/1000 | Loss: 0.00022427
Iteration 22/1000 | Loss: 0.00023043
Iteration 23/1000 | Loss: 0.00022254
Iteration 24/1000 | Loss: 0.00022203
Iteration 25/1000 | Loss: 0.00039851
Iteration 26/1000 | Loss: 0.00023618
Iteration 27/1000 | Loss: 0.00022344
Iteration 28/1000 | Loss: 0.00022117
Iteration 29/1000 | Loss: 0.00021986
Iteration 30/1000 | Loss: 0.00021884
Iteration 31/1000 | Loss: 0.00021839
Iteration 32/1000 | Loss: 0.00021811
Iteration 33/1000 | Loss: 0.00022252
Iteration 34/1000 | Loss: 0.00022470
Iteration 35/1000 | Loss: 0.00022030
Iteration 36/1000 | Loss: 0.00021850
Iteration 37/1000 | Loss: 0.00021798
Iteration 38/1000 | Loss: 0.00022282
Iteration 39/1000 | Loss: 0.00023649
Iteration 40/1000 | Loss: 0.00023476
Iteration 41/1000 | Loss: 0.00022366
Iteration 42/1000 | Loss: 0.00022002
Iteration 43/1000 | Loss: 0.00021876
Iteration 44/1000 | Loss: 0.00021841
Iteration 45/1000 | Loss: 0.00021800
Iteration 46/1000 | Loss: 0.00021765
Iteration 47/1000 | Loss: 0.00021738
Iteration 48/1000 | Loss: 0.00021734
Iteration 49/1000 | Loss: 0.00021733
Iteration 50/1000 | Loss: 0.00021730
Iteration 51/1000 | Loss: 0.00021729
Iteration 52/1000 | Loss: 0.00021728
Iteration 53/1000 | Loss: 0.00021728
Iteration 54/1000 | Loss: 0.00021728
Iteration 55/1000 | Loss: 0.00021728
Iteration 56/1000 | Loss: 0.00021727
Iteration 57/1000 | Loss: 0.00021727
Iteration 58/1000 | Loss: 0.00021727
Iteration 59/1000 | Loss: 0.00021727
Iteration 60/1000 | Loss: 0.00021727
Iteration 61/1000 | Loss: 0.00021727
Iteration 62/1000 | Loss: 0.00021727
Iteration 63/1000 | Loss: 0.00021727
Iteration 64/1000 | Loss: 0.00021726
Iteration 65/1000 | Loss: 0.00021726
Iteration 66/1000 | Loss: 0.00021725
Iteration 67/1000 | Loss: 0.00021725
Iteration 68/1000 | Loss: 0.00021725
Iteration 69/1000 | Loss: 0.00021725
Iteration 70/1000 | Loss: 0.00021724
Iteration 71/1000 | Loss: 0.00021724
Iteration 72/1000 | Loss: 0.00021724
Iteration 73/1000 | Loss: 0.00021724
Iteration 74/1000 | Loss: 0.00021724
Iteration 75/1000 | Loss: 0.00021723
Iteration 76/1000 | Loss: 0.00021723
Iteration 77/1000 | Loss: 0.00021723
Iteration 78/1000 | Loss: 0.00021723
Iteration 79/1000 | Loss: 0.00021723
Iteration 80/1000 | Loss: 0.00021723
Iteration 81/1000 | Loss: 0.00021722
Iteration 82/1000 | Loss: 0.00021722
Iteration 83/1000 | Loss: 0.00021722
Iteration 84/1000 | Loss: 0.00021722
Iteration 85/1000 | Loss: 0.00021722
Iteration 86/1000 | Loss: 0.00021722
Iteration 87/1000 | Loss: 0.00021722
Iteration 88/1000 | Loss: 0.00021722
Iteration 89/1000 | Loss: 0.00021722
Iteration 90/1000 | Loss: 0.00021722
Iteration 91/1000 | Loss: 0.00021722
Iteration 92/1000 | Loss: 0.00021722
Iteration 93/1000 | Loss: 0.00021722
Iteration 94/1000 | Loss: 0.00021721
Iteration 95/1000 | Loss: 0.00021721
Iteration 96/1000 | Loss: 0.00021721
Iteration 97/1000 | Loss: 0.00021721
Iteration 98/1000 | Loss: 0.00021721
Iteration 99/1000 | Loss: 0.00021721
Iteration 100/1000 | Loss: 0.00021721
Iteration 101/1000 | Loss: 0.00021721
Iteration 102/1000 | Loss: 0.00021721
Iteration 103/1000 | Loss: 0.00021721
Iteration 104/1000 | Loss: 0.00021721
Iteration 105/1000 | Loss: 0.00021721
Iteration 106/1000 | Loss: 0.00021721
Iteration 107/1000 | Loss: 0.00021721
Iteration 108/1000 | Loss: 0.00021721
Iteration 109/1000 | Loss: 0.00021721
Iteration 110/1000 | Loss: 0.00021721
Iteration 111/1000 | Loss: 0.00021721
Iteration 112/1000 | Loss: 0.00021721
Iteration 113/1000 | Loss: 0.00021721
Iteration 114/1000 | Loss: 0.00021721
Iteration 115/1000 | Loss: 0.00021721
Iteration 116/1000 | Loss: 0.00021721
Iteration 117/1000 | Loss: 0.00021721
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [0.00021720505901612341, 0.00021720505901612341, 0.00021720505901612341, 0.00021720505901612341, 0.00021720505901612341]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00021720505901612341

Optimization complete. Final v2v error: 10.05721378326416 mm

Highest mean error: 22.395954132080078 mm for frame 12

Lowest mean error: 7.4116435050964355 mm for frame 117

Saving results

Total time: 112.64354014396667
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_0196/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0196/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0196/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01182058
Iteration 2/25 | Loss: 0.00286628
Iteration 3/25 | Loss: 0.00189317
Iteration 4/25 | Loss: 0.00176953
Iteration 5/25 | Loss: 0.00174657
Iteration 6/25 | Loss: 0.00171765
Iteration 7/25 | Loss: 0.00164464
Iteration 8/25 | Loss: 0.00166778
Iteration 9/25 | Loss: 0.00161297
Iteration 10/25 | Loss: 0.00158692
Iteration 11/25 | Loss: 0.00156899
Iteration 12/25 | Loss: 0.00156502
Iteration 13/25 | Loss: 0.00156229
Iteration 14/25 | Loss: 0.00156542
Iteration 15/25 | Loss: 0.00153900
Iteration 16/25 | Loss: 0.00155526
Iteration 17/25 | Loss: 0.00155134
Iteration 18/25 | Loss: 0.00155035
Iteration 19/25 | Loss: 0.00153149
Iteration 20/25 | Loss: 0.00154829
Iteration 21/25 | Loss: 0.00151869
Iteration 22/25 | Loss: 0.00151208
Iteration 23/25 | Loss: 0.00150724
Iteration 24/25 | Loss: 0.00150686
Iteration 25/25 | Loss: 0.00151129

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54612172
Iteration 2/25 | Loss: 0.00289678
Iteration 3/25 | Loss: 0.00279537
Iteration 4/25 | Loss: 0.00279537
Iteration 5/25 | Loss: 0.00279537
Iteration 6/25 | Loss: 0.00279537
Iteration 7/25 | Loss: 0.00279536
Iteration 8/25 | Loss: 0.00279536
Iteration 9/25 | Loss: 0.00279537
Iteration 10/25 | Loss: 0.00279536
Iteration 11/25 | Loss: 0.00279536
Iteration 12/25 | Loss: 0.00279536
Iteration 13/25 | Loss: 0.00279536
Iteration 14/25 | Loss: 0.00279536
Iteration 15/25 | Loss: 0.00279537
Iteration 16/25 | Loss: 0.00279537
Iteration 17/25 | Loss: 0.00279537
Iteration 18/25 | Loss: 0.00279537
Iteration 19/25 | Loss: 0.00279537
Iteration 20/25 | Loss: 0.00279537
Iteration 21/25 | Loss: 0.00279537
Iteration 22/25 | Loss: 0.00279537
Iteration 23/25 | Loss: 0.00279537
Iteration 24/25 | Loss: 0.00279537
Iteration 25/25 | Loss: 0.00279537

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00279537
Iteration 2/1000 | Loss: 0.00005557
Iteration 3/1000 | Loss: 0.00004744
Iteration 4/1000 | Loss: 0.00004278
Iteration 5/1000 | Loss: 0.00004039
Iteration 6/1000 | Loss: 0.00003940
Iteration 7/1000 | Loss: 0.00003872
Iteration 8/1000 | Loss: 0.00003821
Iteration 9/1000 | Loss: 0.00003775
Iteration 10/1000 | Loss: 0.00003782
Iteration 11/1000 | Loss: 0.00003766
Iteration 12/1000 | Loss: 0.00003769
Iteration 13/1000 | Loss: 0.00003750
Iteration 14/1000 | Loss: 0.00003764
Iteration 15/1000 | Loss: 0.00003751
Iteration 16/1000 | Loss: 0.00003734
Iteration 17/1000 | Loss: 0.00003730
Iteration 18/1000 | Loss: 0.00003729
Iteration 19/1000 | Loss: 0.00003728
Iteration 20/1000 | Loss: 0.00003727
Iteration 21/1000 | Loss: 0.00003727
Iteration 22/1000 | Loss: 0.00003726
Iteration 23/1000 | Loss: 0.00003726
Iteration 24/1000 | Loss: 0.00003726
Iteration 25/1000 | Loss: 0.00003726
Iteration 26/1000 | Loss: 0.00003726
Iteration 27/1000 | Loss: 0.00003726
Iteration 28/1000 | Loss: 0.00003726
Iteration 29/1000 | Loss: 0.00003726
Iteration 30/1000 | Loss: 0.00003726
Iteration 31/1000 | Loss: 0.00003726
Iteration 32/1000 | Loss: 0.00003726
Iteration 33/1000 | Loss: 0.00003726
Iteration 34/1000 | Loss: 0.00003726
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 34. Stopping optimization.
Last 5 losses: [3.7262143450789154e-05, 3.7262143450789154e-05, 3.7262143450789154e-05, 3.7262143450789154e-05, 3.7262143450789154e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.7262143450789154e-05

Optimization complete. Final v2v error: 5.13896369934082 mm

Highest mean error: 12.645642280578613 mm for frame 221

Lowest mean error: 4.662192344665527 mm for frame 119

Saving results

Total time: 79.02074670791626
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_0196/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0196/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0196/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01193371
Iteration 2/25 | Loss: 0.00254732
Iteration 3/25 | Loss: 0.00251397
Iteration 4/25 | Loss: 0.00224629
Iteration 5/25 | Loss: 0.00200410
Iteration 6/25 | Loss: 0.00183958
Iteration 7/25 | Loss: 0.00169112
Iteration 8/25 | Loss: 0.00161497
Iteration 9/25 | Loss: 0.00151498
Iteration 10/25 | Loss: 0.00154675
Iteration 11/25 | Loss: 0.00149686
Iteration 12/25 | Loss: 0.00147640
Iteration 13/25 | Loss: 0.00151882
Iteration 14/25 | Loss: 0.00148562
Iteration 15/25 | Loss: 0.00152892
Iteration 16/25 | Loss: 0.00146491
Iteration 17/25 | Loss: 0.00152901
Iteration 18/25 | Loss: 0.00150010
Iteration 19/25 | Loss: 0.00146998
Iteration 20/25 | Loss: 0.00146250
Iteration 21/25 | Loss: 0.00151166
Iteration 22/25 | Loss: 0.00150800
Iteration 23/25 | Loss: 0.00146566
Iteration 24/25 | Loss: 0.00146673
Iteration 25/25 | Loss: 0.00145416

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61654568
Iteration 2/25 | Loss: 0.00250729
Iteration 3/25 | Loss: 0.00250729
Iteration 4/25 | Loss: 0.00250729
Iteration 5/25 | Loss: 0.00250729
Iteration 6/25 | Loss: 0.00250729
Iteration 7/25 | Loss: 0.00250729
Iteration 8/25 | Loss: 0.00250729
Iteration 9/25 | Loss: 0.00250729
Iteration 10/25 | Loss: 0.00250729
Iteration 11/25 | Loss: 0.00250729
Iteration 12/25 | Loss: 0.00250729
Iteration 13/25 | Loss: 0.00250729
Iteration 14/25 | Loss: 0.00250729
Iteration 15/25 | Loss: 0.00250729
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0025072889402508736, 0.0025072889402508736, 0.0025072889402508736, 0.0025072889402508736, 0.0025072889402508736]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025072889402508736

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00250729
Iteration 2/1000 | Loss: 0.00004113
Iteration 3/1000 | Loss: 0.00034435
Iteration 4/1000 | Loss: 0.00003552
Iteration 5/1000 | Loss: 0.00003057
Iteration 6/1000 | Loss: 0.00002739
Iteration 7/1000 | Loss: 0.00002535
Iteration 8/1000 | Loss: 0.00002422
Iteration 9/1000 | Loss: 0.00002364
Iteration 10/1000 | Loss: 0.00002321
Iteration 11/1000 | Loss: 0.00002288
Iteration 12/1000 | Loss: 0.00002261
Iteration 13/1000 | Loss: 0.00002233
Iteration 14/1000 | Loss: 0.00002224
Iteration 15/1000 | Loss: 0.00002215
Iteration 16/1000 | Loss: 0.00002212
Iteration 17/1000 | Loss: 0.00002211
Iteration 18/1000 | Loss: 0.00002211
Iteration 19/1000 | Loss: 0.00002211
Iteration 20/1000 | Loss: 0.00002211
Iteration 21/1000 | Loss: 0.00002210
Iteration 22/1000 | Loss: 0.00002210
Iteration 23/1000 | Loss: 0.00002210
Iteration 24/1000 | Loss: 0.00002208
Iteration 25/1000 | Loss: 0.00002208
Iteration 26/1000 | Loss: 0.00002208
Iteration 27/1000 | Loss: 0.00002207
Iteration 28/1000 | Loss: 0.00002204
Iteration 29/1000 | Loss: 0.00002201
Iteration 30/1000 | Loss: 0.00002201
Iteration 31/1000 | Loss: 0.00002200
Iteration 32/1000 | Loss: 0.00002200
Iteration 33/1000 | Loss: 0.00002199
Iteration 34/1000 | Loss: 0.00002199
Iteration 35/1000 | Loss: 0.00002199
Iteration 36/1000 | Loss: 0.00002198
Iteration 37/1000 | Loss: 0.00002198
Iteration 38/1000 | Loss: 0.00002198
Iteration 39/1000 | Loss: 0.00002198
Iteration 40/1000 | Loss: 0.00002197
Iteration 41/1000 | Loss: 0.00002197
Iteration 42/1000 | Loss: 0.00002197
Iteration 43/1000 | Loss: 0.00002197
Iteration 44/1000 | Loss: 0.00002197
Iteration 45/1000 | Loss: 0.00002197
Iteration 46/1000 | Loss: 0.00002197
Iteration 47/1000 | Loss: 0.00002197
Iteration 48/1000 | Loss: 0.00002197
Iteration 49/1000 | Loss: 0.00002197
Iteration 50/1000 | Loss: 0.00002197
Iteration 51/1000 | Loss: 0.00002197
Iteration 52/1000 | Loss: 0.00002196
Iteration 53/1000 | Loss: 0.00002196
Iteration 54/1000 | Loss: 0.00002196
Iteration 55/1000 | Loss: 0.00002196
Iteration 56/1000 | Loss: 0.00002195
Iteration 57/1000 | Loss: 0.00002195
Iteration 58/1000 | Loss: 0.00002195
Iteration 59/1000 | Loss: 0.00002195
Iteration 60/1000 | Loss: 0.00002194
Iteration 61/1000 | Loss: 0.00002194
Iteration 62/1000 | Loss: 0.00002194
Iteration 63/1000 | Loss: 0.00002194
Iteration 64/1000 | Loss: 0.00002194
Iteration 65/1000 | Loss: 0.00002193
Iteration 66/1000 | Loss: 0.00002193
Iteration 67/1000 | Loss: 0.00002193
Iteration 68/1000 | Loss: 0.00002193
Iteration 69/1000 | Loss: 0.00002193
Iteration 70/1000 | Loss: 0.00002193
Iteration 71/1000 | Loss: 0.00002193
Iteration 72/1000 | Loss: 0.00002193
Iteration 73/1000 | Loss: 0.00002193
Iteration 74/1000 | Loss: 0.00002192
Iteration 75/1000 | Loss: 0.00002192
Iteration 76/1000 | Loss: 0.00002192
Iteration 77/1000 | Loss: 0.00002192
Iteration 78/1000 | Loss: 0.00002192
Iteration 79/1000 | Loss: 0.00002192
Iteration 80/1000 | Loss: 0.00002192
Iteration 81/1000 | Loss: 0.00002192
Iteration 82/1000 | Loss: 0.00002192
Iteration 83/1000 | Loss: 0.00002192
Iteration 84/1000 | Loss: 0.00002192
Iteration 85/1000 | Loss: 0.00002192
Iteration 86/1000 | Loss: 0.00002192
Iteration 87/1000 | Loss: 0.00002192
Iteration 88/1000 | Loss: 0.00002191
Iteration 89/1000 | Loss: 0.00002191
Iteration 90/1000 | Loss: 0.00002191
Iteration 91/1000 | Loss: 0.00002191
Iteration 92/1000 | Loss: 0.00002191
Iteration 93/1000 | Loss: 0.00002191
Iteration 94/1000 | Loss: 0.00002191
Iteration 95/1000 | Loss: 0.00002191
Iteration 96/1000 | Loss: 0.00002191
Iteration 97/1000 | Loss: 0.00002191
Iteration 98/1000 | Loss: 0.00002191
Iteration 99/1000 | Loss: 0.00002190
Iteration 100/1000 | Loss: 0.00002190
Iteration 101/1000 | Loss: 0.00002190
Iteration 102/1000 | Loss: 0.00002190
Iteration 103/1000 | Loss: 0.00002190
Iteration 104/1000 | Loss: 0.00002190
Iteration 105/1000 | Loss: 0.00002190
Iteration 106/1000 | Loss: 0.00002190
Iteration 107/1000 | Loss: 0.00002190
Iteration 108/1000 | Loss: 0.00002190
Iteration 109/1000 | Loss: 0.00002190
Iteration 110/1000 | Loss: 0.00002190
Iteration 111/1000 | Loss: 0.00002190
Iteration 112/1000 | Loss: 0.00002190
Iteration 113/1000 | Loss: 0.00002190
Iteration 114/1000 | Loss: 0.00002190
Iteration 115/1000 | Loss: 0.00002190
Iteration 116/1000 | Loss: 0.00002190
Iteration 117/1000 | Loss: 0.00002190
Iteration 118/1000 | Loss: 0.00002190
Iteration 119/1000 | Loss: 0.00002190
Iteration 120/1000 | Loss: 0.00002190
Iteration 121/1000 | Loss: 0.00002189
Iteration 122/1000 | Loss: 0.00002189
Iteration 123/1000 | Loss: 0.00002189
Iteration 124/1000 | Loss: 0.00002189
Iteration 125/1000 | Loss: 0.00002189
Iteration 126/1000 | Loss: 0.00002189
Iteration 127/1000 | Loss: 0.00002189
Iteration 128/1000 | Loss: 0.00002189
Iteration 129/1000 | Loss: 0.00002189
Iteration 130/1000 | Loss: 0.00002189
Iteration 131/1000 | Loss: 0.00002189
Iteration 132/1000 | Loss: 0.00002189
Iteration 133/1000 | Loss: 0.00002189
Iteration 134/1000 | Loss: 0.00002189
Iteration 135/1000 | Loss: 0.00002188
Iteration 136/1000 | Loss: 0.00002188
Iteration 137/1000 | Loss: 0.00002188
Iteration 138/1000 | Loss: 0.00002188
Iteration 139/1000 | Loss: 0.00002188
Iteration 140/1000 | Loss: 0.00002188
Iteration 141/1000 | Loss: 0.00002188
Iteration 142/1000 | Loss: 0.00002188
Iteration 143/1000 | Loss: 0.00002188
Iteration 144/1000 | Loss: 0.00002188
Iteration 145/1000 | Loss: 0.00002188
Iteration 146/1000 | Loss: 0.00002188
Iteration 147/1000 | Loss: 0.00002188
Iteration 148/1000 | Loss: 0.00002188
Iteration 149/1000 | Loss: 0.00002188
Iteration 150/1000 | Loss: 0.00002188
Iteration 151/1000 | Loss: 0.00002188
Iteration 152/1000 | Loss: 0.00002188
Iteration 153/1000 | Loss: 0.00002188
Iteration 154/1000 | Loss: 0.00002188
Iteration 155/1000 | Loss: 0.00002188
Iteration 156/1000 | Loss: 0.00002187
Iteration 157/1000 | Loss: 0.00002187
Iteration 158/1000 | Loss: 0.00002187
Iteration 159/1000 | Loss: 0.00002187
Iteration 160/1000 | Loss: 0.00002187
Iteration 161/1000 | Loss: 0.00002187
Iteration 162/1000 | Loss: 0.00002187
Iteration 163/1000 | Loss: 0.00002187
Iteration 164/1000 | Loss: 0.00002187
Iteration 165/1000 | Loss: 0.00002187
Iteration 166/1000 | Loss: 0.00002187
Iteration 167/1000 | Loss: 0.00002187
Iteration 168/1000 | Loss: 0.00002187
Iteration 169/1000 | Loss: 0.00002187
Iteration 170/1000 | Loss: 0.00002187
Iteration 171/1000 | Loss: 0.00002187
Iteration 172/1000 | Loss: 0.00002187
Iteration 173/1000 | Loss: 0.00002187
Iteration 174/1000 | Loss: 0.00002187
Iteration 175/1000 | Loss: 0.00002187
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [2.187176505685784e-05, 2.187176505685784e-05, 2.187176505685784e-05, 2.187176505685784e-05, 2.187176505685784e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.187176505685784e-05

Optimization complete. Final v2v error: 4.0685858726501465 mm

Highest mean error: 5.0628228187561035 mm for frame 71

Lowest mean error: 3.639573335647583 mm for frame 0

Saving results

Total time: 73.39744472503662
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_0196/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0196/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0196/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01013634
Iteration 2/25 | Loss: 0.00177859
Iteration 3/25 | Loss: 0.00164442
Iteration 4/25 | Loss: 0.00161676
Iteration 5/25 | Loss: 0.00160922
Iteration 6/25 | Loss: 0.00160702
Iteration 7/25 | Loss: 0.00160702
Iteration 8/25 | Loss: 0.00160702
Iteration 9/25 | Loss: 0.00160702
Iteration 10/25 | Loss: 0.00160702
Iteration 11/25 | Loss: 0.00160702
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0016070186393335462, 0.0016070186393335462, 0.0016070186393335462, 0.0016070186393335462, 0.0016070186393335462]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016070186393335462

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54834187
Iteration 2/25 | Loss: 0.00239359
Iteration 3/25 | Loss: 0.00239352
Iteration 4/25 | Loss: 0.00239352
Iteration 5/25 | Loss: 0.00239352
Iteration 6/25 | Loss: 0.00239352
Iteration 7/25 | Loss: 0.00239352
Iteration 8/25 | Loss: 0.00239352
Iteration 9/25 | Loss: 0.00239352
Iteration 10/25 | Loss: 0.00239352
Iteration 11/25 | Loss: 0.00239352
Iteration 12/25 | Loss: 0.00239352
Iteration 13/25 | Loss: 0.00239352
Iteration 14/25 | Loss: 0.00239352
Iteration 15/25 | Loss: 0.00239352
Iteration 16/25 | Loss: 0.00239352
Iteration 17/25 | Loss: 0.00239352
Iteration 18/25 | Loss: 0.00239352
Iteration 19/25 | Loss: 0.00239352
Iteration 20/25 | Loss: 0.00239352
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.002393518341705203, 0.002393518341705203, 0.002393518341705203, 0.002393518341705203, 0.002393518341705203]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002393518341705203

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00239352
Iteration 2/1000 | Loss: 0.00007077
Iteration 3/1000 | Loss: 0.00005105
Iteration 4/1000 | Loss: 0.00004515
Iteration 5/1000 | Loss: 0.00004257
Iteration 6/1000 | Loss: 0.00004065
Iteration 7/1000 | Loss: 0.00003972
Iteration 8/1000 | Loss: 0.00003885
Iteration 9/1000 | Loss: 0.00003805
Iteration 10/1000 | Loss: 0.00003759
Iteration 11/1000 | Loss: 0.00003718
Iteration 12/1000 | Loss: 0.00003691
Iteration 13/1000 | Loss: 0.00003664
Iteration 14/1000 | Loss: 0.00003658
Iteration 15/1000 | Loss: 0.00003647
Iteration 16/1000 | Loss: 0.00003645
Iteration 17/1000 | Loss: 0.00003641
Iteration 18/1000 | Loss: 0.00003632
Iteration 19/1000 | Loss: 0.00003630
Iteration 20/1000 | Loss: 0.00003627
Iteration 21/1000 | Loss: 0.00003626
Iteration 22/1000 | Loss: 0.00003625
Iteration 23/1000 | Loss: 0.00003624
Iteration 24/1000 | Loss: 0.00003624
Iteration 25/1000 | Loss: 0.00003624
Iteration 26/1000 | Loss: 0.00003624
Iteration 27/1000 | Loss: 0.00003623
Iteration 28/1000 | Loss: 0.00003623
Iteration 29/1000 | Loss: 0.00003623
Iteration 30/1000 | Loss: 0.00003622
Iteration 31/1000 | Loss: 0.00003622
Iteration 32/1000 | Loss: 0.00003622
Iteration 33/1000 | Loss: 0.00003621
Iteration 34/1000 | Loss: 0.00003621
Iteration 35/1000 | Loss: 0.00003620
Iteration 36/1000 | Loss: 0.00003620
Iteration 37/1000 | Loss: 0.00003620
Iteration 38/1000 | Loss: 0.00003620
Iteration 39/1000 | Loss: 0.00003619
Iteration 40/1000 | Loss: 0.00003619
Iteration 41/1000 | Loss: 0.00003619
Iteration 42/1000 | Loss: 0.00003619
Iteration 43/1000 | Loss: 0.00003618
Iteration 44/1000 | Loss: 0.00003618
Iteration 45/1000 | Loss: 0.00003618
Iteration 46/1000 | Loss: 0.00003617
Iteration 47/1000 | Loss: 0.00003617
Iteration 48/1000 | Loss: 0.00003617
Iteration 49/1000 | Loss: 0.00003617
Iteration 50/1000 | Loss: 0.00003617
Iteration 51/1000 | Loss: 0.00003617
Iteration 52/1000 | Loss: 0.00003617
Iteration 53/1000 | Loss: 0.00003616
Iteration 54/1000 | Loss: 0.00003616
Iteration 55/1000 | Loss: 0.00003616
Iteration 56/1000 | Loss: 0.00003615
Iteration 57/1000 | Loss: 0.00003615
Iteration 58/1000 | Loss: 0.00003615
Iteration 59/1000 | Loss: 0.00003615
Iteration 60/1000 | Loss: 0.00003614
Iteration 61/1000 | Loss: 0.00003614
Iteration 62/1000 | Loss: 0.00003614
Iteration 63/1000 | Loss: 0.00003613
Iteration 64/1000 | Loss: 0.00003613
Iteration 65/1000 | Loss: 0.00003613
Iteration 66/1000 | Loss: 0.00003613
Iteration 67/1000 | Loss: 0.00003613
Iteration 68/1000 | Loss: 0.00003613
Iteration 69/1000 | Loss: 0.00003613
Iteration 70/1000 | Loss: 0.00003613
Iteration 71/1000 | Loss: 0.00003613
Iteration 72/1000 | Loss: 0.00003613
Iteration 73/1000 | Loss: 0.00003613
Iteration 74/1000 | Loss: 0.00003612
Iteration 75/1000 | Loss: 0.00003612
Iteration 76/1000 | Loss: 0.00003612
Iteration 77/1000 | Loss: 0.00003612
Iteration 78/1000 | Loss: 0.00003612
Iteration 79/1000 | Loss: 0.00003612
Iteration 80/1000 | Loss: 0.00003611
Iteration 81/1000 | Loss: 0.00003611
Iteration 82/1000 | Loss: 0.00003611
Iteration 83/1000 | Loss: 0.00003611
Iteration 84/1000 | Loss: 0.00003611
Iteration 85/1000 | Loss: 0.00003611
Iteration 86/1000 | Loss: 0.00003611
Iteration 87/1000 | Loss: 0.00003611
Iteration 88/1000 | Loss: 0.00003611
Iteration 89/1000 | Loss: 0.00003611
Iteration 90/1000 | Loss: 0.00003610
Iteration 91/1000 | Loss: 0.00003610
Iteration 92/1000 | Loss: 0.00003610
Iteration 93/1000 | Loss: 0.00003610
Iteration 94/1000 | Loss: 0.00003610
Iteration 95/1000 | Loss: 0.00003610
Iteration 96/1000 | Loss: 0.00003610
Iteration 97/1000 | Loss: 0.00003610
Iteration 98/1000 | Loss: 0.00003610
Iteration 99/1000 | Loss: 0.00003610
Iteration 100/1000 | Loss: 0.00003610
Iteration 101/1000 | Loss: 0.00003610
Iteration 102/1000 | Loss: 0.00003610
Iteration 103/1000 | Loss: 0.00003610
Iteration 104/1000 | Loss: 0.00003610
Iteration 105/1000 | Loss: 0.00003610
Iteration 106/1000 | Loss: 0.00003610
Iteration 107/1000 | Loss: 0.00003610
Iteration 108/1000 | Loss: 0.00003610
Iteration 109/1000 | Loss: 0.00003610
Iteration 110/1000 | Loss: 0.00003610
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [3.61015772796236e-05, 3.61015772796236e-05, 3.61015772796236e-05, 3.61015772796236e-05, 3.61015772796236e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.61015772796236e-05

Optimization complete. Final v2v error: 5.237921237945557 mm

Highest mean error: 5.508779048919678 mm for frame 156

Lowest mean error: 4.951416015625 mm for frame 2

Saving results

Total time: 41.96334934234619
