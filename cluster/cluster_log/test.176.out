Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=176, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 9856-9911
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00373641
Iteration 2/25 | Loss: 0.00105248
Iteration 3/25 | Loss: 0.00097326
Iteration 4/25 | Loss: 0.00096423
Iteration 5/25 | Loss: 0.00096164
Iteration 6/25 | Loss: 0.00096117
Iteration 7/25 | Loss: 0.00096117
Iteration 8/25 | Loss: 0.00096117
Iteration 9/25 | Loss: 0.00096117
Iteration 10/25 | Loss: 0.00096117
Iteration 11/25 | Loss: 0.00096117
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009611739660613239, 0.0009611739660613239, 0.0009611739660613239, 0.0009611739660613239, 0.0009611739660613239]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009611739660613239

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43125665
Iteration 2/25 | Loss: 0.00068959
Iteration 3/25 | Loss: 0.00068959
Iteration 4/25 | Loss: 0.00068959
Iteration 5/25 | Loss: 0.00068959
Iteration 6/25 | Loss: 0.00068959
Iteration 7/25 | Loss: 0.00068959
Iteration 8/25 | Loss: 0.00068959
Iteration 9/25 | Loss: 0.00068959
Iteration 10/25 | Loss: 0.00068959
Iteration 11/25 | Loss: 0.00068959
Iteration 12/25 | Loss: 0.00068959
Iteration 13/25 | Loss: 0.00068959
Iteration 14/25 | Loss: 0.00068958
Iteration 15/25 | Loss: 0.00068958
Iteration 16/25 | Loss: 0.00068958
Iteration 17/25 | Loss: 0.00068958
Iteration 18/25 | Loss: 0.00068958
Iteration 19/25 | Loss: 0.00068958
Iteration 20/25 | Loss: 0.00068958
Iteration 21/25 | Loss: 0.00068958
Iteration 22/25 | Loss: 0.00068958
Iteration 23/25 | Loss: 0.00068958
Iteration 24/25 | Loss: 0.00068958
Iteration 25/25 | Loss: 0.00068958

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068958
Iteration 2/1000 | Loss: 0.00001524
Iteration 3/1000 | Loss: 0.00001000
Iteration 4/1000 | Loss: 0.00000894
Iteration 5/1000 | Loss: 0.00000827
Iteration 6/1000 | Loss: 0.00000801
Iteration 7/1000 | Loss: 0.00000777
Iteration 8/1000 | Loss: 0.00000776
Iteration 9/1000 | Loss: 0.00000768
Iteration 10/1000 | Loss: 0.00000768
Iteration 11/1000 | Loss: 0.00000764
Iteration 12/1000 | Loss: 0.00000762
Iteration 13/1000 | Loss: 0.00000761
Iteration 14/1000 | Loss: 0.00000760
Iteration 15/1000 | Loss: 0.00000759
Iteration 16/1000 | Loss: 0.00000757
Iteration 17/1000 | Loss: 0.00000753
Iteration 18/1000 | Loss: 0.00000752
Iteration 19/1000 | Loss: 0.00000749
Iteration 20/1000 | Loss: 0.00000749
Iteration 21/1000 | Loss: 0.00000748
Iteration 22/1000 | Loss: 0.00000748
Iteration 23/1000 | Loss: 0.00000747
Iteration 24/1000 | Loss: 0.00000747
Iteration 25/1000 | Loss: 0.00000746
Iteration 26/1000 | Loss: 0.00000744
Iteration 27/1000 | Loss: 0.00000743
Iteration 28/1000 | Loss: 0.00000743
Iteration 29/1000 | Loss: 0.00000742
Iteration 30/1000 | Loss: 0.00000742
Iteration 31/1000 | Loss: 0.00000742
Iteration 32/1000 | Loss: 0.00000741
Iteration 33/1000 | Loss: 0.00000741
Iteration 34/1000 | Loss: 0.00000740
Iteration 35/1000 | Loss: 0.00000739
Iteration 36/1000 | Loss: 0.00000739
Iteration 37/1000 | Loss: 0.00000738
Iteration 38/1000 | Loss: 0.00000738
Iteration 39/1000 | Loss: 0.00000738
Iteration 40/1000 | Loss: 0.00000738
Iteration 41/1000 | Loss: 0.00000737
Iteration 42/1000 | Loss: 0.00000736
Iteration 43/1000 | Loss: 0.00000736
Iteration 44/1000 | Loss: 0.00000735
Iteration 45/1000 | Loss: 0.00000735
Iteration 46/1000 | Loss: 0.00000735
Iteration 47/1000 | Loss: 0.00000734
Iteration 48/1000 | Loss: 0.00000734
Iteration 49/1000 | Loss: 0.00000734
Iteration 50/1000 | Loss: 0.00000734
Iteration 51/1000 | Loss: 0.00000733
Iteration 52/1000 | Loss: 0.00000733
Iteration 53/1000 | Loss: 0.00000733
Iteration 54/1000 | Loss: 0.00000733
Iteration 55/1000 | Loss: 0.00000732
Iteration 56/1000 | Loss: 0.00000732
Iteration 57/1000 | Loss: 0.00000732
Iteration 58/1000 | Loss: 0.00000731
Iteration 59/1000 | Loss: 0.00000731
Iteration 60/1000 | Loss: 0.00000731
Iteration 61/1000 | Loss: 0.00000731
Iteration 62/1000 | Loss: 0.00000731
Iteration 63/1000 | Loss: 0.00000731
Iteration 64/1000 | Loss: 0.00000730
Iteration 65/1000 | Loss: 0.00000730
Iteration 66/1000 | Loss: 0.00000730
Iteration 67/1000 | Loss: 0.00000730
Iteration 68/1000 | Loss: 0.00000730
Iteration 69/1000 | Loss: 0.00000730
Iteration 70/1000 | Loss: 0.00000730
Iteration 71/1000 | Loss: 0.00000730
Iteration 72/1000 | Loss: 0.00000729
Iteration 73/1000 | Loss: 0.00000729
Iteration 74/1000 | Loss: 0.00000729
Iteration 75/1000 | Loss: 0.00000729
Iteration 76/1000 | Loss: 0.00000729
Iteration 77/1000 | Loss: 0.00000729
Iteration 78/1000 | Loss: 0.00000729
Iteration 79/1000 | Loss: 0.00000729
Iteration 80/1000 | Loss: 0.00000729
Iteration 81/1000 | Loss: 0.00000728
Iteration 82/1000 | Loss: 0.00000728
Iteration 83/1000 | Loss: 0.00000728
Iteration 84/1000 | Loss: 0.00000728
Iteration 85/1000 | Loss: 0.00000728
Iteration 86/1000 | Loss: 0.00000728
Iteration 87/1000 | Loss: 0.00000728
Iteration 88/1000 | Loss: 0.00000728
Iteration 89/1000 | Loss: 0.00000728
Iteration 90/1000 | Loss: 0.00000728
Iteration 91/1000 | Loss: 0.00000728
Iteration 92/1000 | Loss: 0.00000728
Iteration 93/1000 | Loss: 0.00000727
Iteration 94/1000 | Loss: 0.00000727
Iteration 95/1000 | Loss: 0.00000727
Iteration 96/1000 | Loss: 0.00000727
Iteration 97/1000 | Loss: 0.00000726
Iteration 98/1000 | Loss: 0.00000726
Iteration 99/1000 | Loss: 0.00000725
Iteration 100/1000 | Loss: 0.00000725
Iteration 101/1000 | Loss: 0.00000725
Iteration 102/1000 | Loss: 0.00000725
Iteration 103/1000 | Loss: 0.00000725
Iteration 104/1000 | Loss: 0.00000725
Iteration 105/1000 | Loss: 0.00000725
Iteration 106/1000 | Loss: 0.00000725
Iteration 107/1000 | Loss: 0.00000725
Iteration 108/1000 | Loss: 0.00000725
Iteration 109/1000 | Loss: 0.00000725
Iteration 110/1000 | Loss: 0.00000725
Iteration 111/1000 | Loss: 0.00000724
Iteration 112/1000 | Loss: 0.00000724
Iteration 113/1000 | Loss: 0.00000724
Iteration 114/1000 | Loss: 0.00000724
Iteration 115/1000 | Loss: 0.00000724
Iteration 116/1000 | Loss: 0.00000724
Iteration 117/1000 | Loss: 0.00000724
Iteration 118/1000 | Loss: 0.00000723
Iteration 119/1000 | Loss: 0.00000723
Iteration 120/1000 | Loss: 0.00000723
Iteration 121/1000 | Loss: 0.00000723
Iteration 122/1000 | Loss: 0.00000722
Iteration 123/1000 | Loss: 0.00000722
Iteration 124/1000 | Loss: 0.00000722
Iteration 125/1000 | Loss: 0.00000722
Iteration 126/1000 | Loss: 0.00000722
Iteration 127/1000 | Loss: 0.00000722
Iteration 128/1000 | Loss: 0.00000722
Iteration 129/1000 | Loss: 0.00000722
Iteration 130/1000 | Loss: 0.00000722
Iteration 131/1000 | Loss: 0.00000722
Iteration 132/1000 | Loss: 0.00000722
Iteration 133/1000 | Loss: 0.00000722
Iteration 134/1000 | Loss: 0.00000722
Iteration 135/1000 | Loss: 0.00000721
Iteration 136/1000 | Loss: 0.00000721
Iteration 137/1000 | Loss: 0.00000721
Iteration 138/1000 | Loss: 0.00000721
Iteration 139/1000 | Loss: 0.00000721
Iteration 140/1000 | Loss: 0.00000721
Iteration 141/1000 | Loss: 0.00000721
Iteration 142/1000 | Loss: 0.00000721
Iteration 143/1000 | Loss: 0.00000720
Iteration 144/1000 | Loss: 0.00000720
Iteration 145/1000 | Loss: 0.00000720
Iteration 146/1000 | Loss: 0.00000720
Iteration 147/1000 | Loss: 0.00000720
Iteration 148/1000 | Loss: 0.00000720
Iteration 149/1000 | Loss: 0.00000720
Iteration 150/1000 | Loss: 0.00000719
Iteration 151/1000 | Loss: 0.00000719
Iteration 152/1000 | Loss: 0.00000719
Iteration 153/1000 | Loss: 0.00000718
Iteration 154/1000 | Loss: 0.00000718
Iteration 155/1000 | Loss: 0.00000718
Iteration 156/1000 | Loss: 0.00000718
Iteration 157/1000 | Loss: 0.00000718
Iteration 158/1000 | Loss: 0.00000718
Iteration 159/1000 | Loss: 0.00000718
Iteration 160/1000 | Loss: 0.00000717
Iteration 161/1000 | Loss: 0.00000717
Iteration 162/1000 | Loss: 0.00000717
Iteration 163/1000 | Loss: 0.00000716
Iteration 164/1000 | Loss: 0.00000716
Iteration 165/1000 | Loss: 0.00000716
Iteration 166/1000 | Loss: 0.00000716
Iteration 167/1000 | Loss: 0.00000715
Iteration 168/1000 | Loss: 0.00000715
Iteration 169/1000 | Loss: 0.00000715
Iteration 170/1000 | Loss: 0.00000715
Iteration 171/1000 | Loss: 0.00000715
Iteration 172/1000 | Loss: 0.00000715
Iteration 173/1000 | Loss: 0.00000715
Iteration 174/1000 | Loss: 0.00000715
Iteration 175/1000 | Loss: 0.00000715
Iteration 176/1000 | Loss: 0.00000715
Iteration 177/1000 | Loss: 0.00000715
Iteration 178/1000 | Loss: 0.00000715
Iteration 179/1000 | Loss: 0.00000715
Iteration 180/1000 | Loss: 0.00000715
Iteration 181/1000 | Loss: 0.00000715
Iteration 182/1000 | Loss: 0.00000715
Iteration 183/1000 | Loss: 0.00000715
Iteration 184/1000 | Loss: 0.00000715
Iteration 185/1000 | Loss: 0.00000715
Iteration 186/1000 | Loss: 0.00000715
Iteration 187/1000 | Loss: 0.00000715
Iteration 188/1000 | Loss: 0.00000715
Iteration 189/1000 | Loss: 0.00000715
Iteration 190/1000 | Loss: 0.00000715
Iteration 191/1000 | Loss: 0.00000715
Iteration 192/1000 | Loss: 0.00000715
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [7.150875262595946e-06, 7.150875262595946e-06, 7.150875262595946e-06, 7.150875262595946e-06, 7.150875262595946e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.150875262595946e-06

Optimization complete. Final v2v error: 2.2786998748779297 mm

Highest mean error: 2.8433849811553955 mm for frame 79

Lowest mean error: 2.1464955806732178 mm for frame 186

Saving results

Total time: 36.262476682662964
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00386597
Iteration 2/25 | Loss: 0.00108778
Iteration 3/25 | Loss: 0.00097772
Iteration 4/25 | Loss: 0.00096422
Iteration 5/25 | Loss: 0.00096157
Iteration 6/25 | Loss: 0.00096157
Iteration 7/25 | Loss: 0.00096157
Iteration 8/25 | Loss: 0.00096157
Iteration 9/25 | Loss: 0.00096157
Iteration 10/25 | Loss: 0.00096157
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0009615686140023172, 0.0009615686140023172, 0.0009615686140023172, 0.0009615686140023172, 0.0009615686140023172]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009615686140023172

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36920869
Iteration 2/25 | Loss: 0.00051011
Iteration 3/25 | Loss: 0.00051011
Iteration 4/25 | Loss: 0.00051011
Iteration 5/25 | Loss: 0.00051011
Iteration 6/25 | Loss: 0.00051011
Iteration 7/25 | Loss: 0.00051011
Iteration 8/25 | Loss: 0.00051011
Iteration 9/25 | Loss: 0.00051011
Iteration 10/25 | Loss: 0.00051011
Iteration 11/25 | Loss: 0.00051011
Iteration 12/25 | Loss: 0.00051011
Iteration 13/25 | Loss: 0.00051011
Iteration 14/25 | Loss: 0.00051011
Iteration 15/25 | Loss: 0.00051011
Iteration 16/25 | Loss: 0.00051011
Iteration 17/25 | Loss: 0.00051011
Iteration 18/25 | Loss: 0.00051011
Iteration 19/25 | Loss: 0.00051011
Iteration 20/25 | Loss: 0.00051011
Iteration 21/25 | Loss: 0.00051011
Iteration 22/25 | Loss: 0.00051011
Iteration 23/25 | Loss: 0.00051011
Iteration 24/25 | Loss: 0.00051011
Iteration 25/25 | Loss: 0.00051011

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051011
Iteration 2/1000 | Loss: 0.00002472
Iteration 3/1000 | Loss: 0.00001827
Iteration 4/1000 | Loss: 0.00001525
Iteration 5/1000 | Loss: 0.00001405
Iteration 6/1000 | Loss: 0.00001306
Iteration 7/1000 | Loss: 0.00001252
Iteration 8/1000 | Loss: 0.00001247
Iteration 9/1000 | Loss: 0.00001209
Iteration 10/1000 | Loss: 0.00001180
Iteration 11/1000 | Loss: 0.00001152
Iteration 12/1000 | Loss: 0.00001136
Iteration 13/1000 | Loss: 0.00001130
Iteration 14/1000 | Loss: 0.00001127
Iteration 15/1000 | Loss: 0.00001122
Iteration 16/1000 | Loss: 0.00001121
Iteration 17/1000 | Loss: 0.00001107
Iteration 18/1000 | Loss: 0.00001103
Iteration 19/1000 | Loss: 0.00001100
Iteration 20/1000 | Loss: 0.00001099
Iteration 21/1000 | Loss: 0.00001099
Iteration 22/1000 | Loss: 0.00001099
Iteration 23/1000 | Loss: 0.00001098
Iteration 24/1000 | Loss: 0.00001097
Iteration 25/1000 | Loss: 0.00001096
Iteration 26/1000 | Loss: 0.00001096
Iteration 27/1000 | Loss: 0.00001096
Iteration 28/1000 | Loss: 0.00001095
Iteration 29/1000 | Loss: 0.00001095
Iteration 30/1000 | Loss: 0.00001094
Iteration 31/1000 | Loss: 0.00001090
Iteration 32/1000 | Loss: 0.00001085
Iteration 33/1000 | Loss: 0.00001084
Iteration 34/1000 | Loss: 0.00001083
Iteration 35/1000 | Loss: 0.00001079
Iteration 36/1000 | Loss: 0.00001079
Iteration 37/1000 | Loss: 0.00001078
Iteration 38/1000 | Loss: 0.00001078
Iteration 39/1000 | Loss: 0.00001077
Iteration 40/1000 | Loss: 0.00001077
Iteration 41/1000 | Loss: 0.00001075
Iteration 42/1000 | Loss: 0.00001075
Iteration 43/1000 | Loss: 0.00001075
Iteration 44/1000 | Loss: 0.00001075
Iteration 45/1000 | Loss: 0.00001075
Iteration 46/1000 | Loss: 0.00001075
Iteration 47/1000 | Loss: 0.00001074
Iteration 48/1000 | Loss: 0.00001074
Iteration 49/1000 | Loss: 0.00001073
Iteration 50/1000 | Loss: 0.00001072
Iteration 51/1000 | Loss: 0.00001072
Iteration 52/1000 | Loss: 0.00001071
Iteration 53/1000 | Loss: 0.00001071
Iteration 54/1000 | Loss: 0.00001070
Iteration 55/1000 | Loss: 0.00001070
Iteration 56/1000 | Loss: 0.00001070
Iteration 57/1000 | Loss: 0.00001070
Iteration 58/1000 | Loss: 0.00001070
Iteration 59/1000 | Loss: 0.00001070
Iteration 60/1000 | Loss: 0.00001069
Iteration 61/1000 | Loss: 0.00001069
Iteration 62/1000 | Loss: 0.00001069
Iteration 63/1000 | Loss: 0.00001069
Iteration 64/1000 | Loss: 0.00001069
Iteration 65/1000 | Loss: 0.00001069
Iteration 66/1000 | Loss: 0.00001069
Iteration 67/1000 | Loss: 0.00001069
Iteration 68/1000 | Loss: 0.00001069
Iteration 69/1000 | Loss: 0.00001068
Iteration 70/1000 | Loss: 0.00001068
Iteration 71/1000 | Loss: 0.00001068
Iteration 72/1000 | Loss: 0.00001067
Iteration 73/1000 | Loss: 0.00001067
Iteration 74/1000 | Loss: 0.00001067
Iteration 75/1000 | Loss: 0.00001067
Iteration 76/1000 | Loss: 0.00001067
Iteration 77/1000 | Loss: 0.00001065
Iteration 78/1000 | Loss: 0.00001065
Iteration 79/1000 | Loss: 0.00001065
Iteration 80/1000 | Loss: 0.00001064
Iteration 81/1000 | Loss: 0.00001064
Iteration 82/1000 | Loss: 0.00001064
Iteration 83/1000 | Loss: 0.00001064
Iteration 84/1000 | Loss: 0.00001063
Iteration 85/1000 | Loss: 0.00001063
Iteration 86/1000 | Loss: 0.00001062
Iteration 87/1000 | Loss: 0.00001062
Iteration 88/1000 | Loss: 0.00001062
Iteration 89/1000 | Loss: 0.00001062
Iteration 90/1000 | Loss: 0.00001062
Iteration 91/1000 | Loss: 0.00001062
Iteration 92/1000 | Loss: 0.00001062
Iteration 93/1000 | Loss: 0.00001062
Iteration 94/1000 | Loss: 0.00001061
Iteration 95/1000 | Loss: 0.00001061
Iteration 96/1000 | Loss: 0.00001061
Iteration 97/1000 | Loss: 0.00001061
Iteration 98/1000 | Loss: 0.00001060
Iteration 99/1000 | Loss: 0.00001060
Iteration 100/1000 | Loss: 0.00001060
Iteration 101/1000 | Loss: 0.00001060
Iteration 102/1000 | Loss: 0.00001060
Iteration 103/1000 | Loss: 0.00001060
Iteration 104/1000 | Loss: 0.00001060
Iteration 105/1000 | Loss: 0.00001059
Iteration 106/1000 | Loss: 0.00001059
Iteration 107/1000 | Loss: 0.00001059
Iteration 108/1000 | Loss: 0.00001059
Iteration 109/1000 | Loss: 0.00001059
Iteration 110/1000 | Loss: 0.00001059
Iteration 111/1000 | Loss: 0.00001059
Iteration 112/1000 | Loss: 0.00001059
Iteration 113/1000 | Loss: 0.00001059
Iteration 114/1000 | Loss: 0.00001059
Iteration 115/1000 | Loss: 0.00001059
Iteration 116/1000 | Loss: 0.00001058
Iteration 117/1000 | Loss: 0.00001058
Iteration 118/1000 | Loss: 0.00001058
Iteration 119/1000 | Loss: 0.00001058
Iteration 120/1000 | Loss: 0.00001058
Iteration 121/1000 | Loss: 0.00001058
Iteration 122/1000 | Loss: 0.00001058
Iteration 123/1000 | Loss: 0.00001058
Iteration 124/1000 | Loss: 0.00001058
Iteration 125/1000 | Loss: 0.00001058
Iteration 126/1000 | Loss: 0.00001057
Iteration 127/1000 | Loss: 0.00001057
Iteration 128/1000 | Loss: 0.00001057
Iteration 129/1000 | Loss: 0.00001057
Iteration 130/1000 | Loss: 0.00001057
Iteration 131/1000 | Loss: 0.00001057
Iteration 132/1000 | Loss: 0.00001057
Iteration 133/1000 | Loss: 0.00001056
Iteration 134/1000 | Loss: 0.00001056
Iteration 135/1000 | Loss: 0.00001056
Iteration 136/1000 | Loss: 0.00001056
Iteration 137/1000 | Loss: 0.00001056
Iteration 138/1000 | Loss: 0.00001056
Iteration 139/1000 | Loss: 0.00001055
Iteration 140/1000 | Loss: 0.00001055
Iteration 141/1000 | Loss: 0.00001055
Iteration 142/1000 | Loss: 0.00001054
Iteration 143/1000 | Loss: 0.00001054
Iteration 144/1000 | Loss: 0.00001054
Iteration 145/1000 | Loss: 0.00001054
Iteration 146/1000 | Loss: 0.00001054
Iteration 147/1000 | Loss: 0.00001053
Iteration 148/1000 | Loss: 0.00001053
Iteration 149/1000 | Loss: 0.00001053
Iteration 150/1000 | Loss: 0.00001053
Iteration 151/1000 | Loss: 0.00001053
Iteration 152/1000 | Loss: 0.00001053
Iteration 153/1000 | Loss: 0.00001053
Iteration 154/1000 | Loss: 0.00001053
Iteration 155/1000 | Loss: 0.00001053
Iteration 156/1000 | Loss: 0.00001053
Iteration 157/1000 | Loss: 0.00001052
Iteration 158/1000 | Loss: 0.00001052
Iteration 159/1000 | Loss: 0.00001052
Iteration 160/1000 | Loss: 0.00001052
Iteration 161/1000 | Loss: 0.00001052
Iteration 162/1000 | Loss: 0.00001052
Iteration 163/1000 | Loss: 0.00001052
Iteration 164/1000 | Loss: 0.00001052
Iteration 165/1000 | Loss: 0.00001051
Iteration 166/1000 | Loss: 0.00001051
Iteration 167/1000 | Loss: 0.00001051
Iteration 168/1000 | Loss: 0.00001051
Iteration 169/1000 | Loss: 0.00001051
Iteration 170/1000 | Loss: 0.00001050
Iteration 171/1000 | Loss: 0.00001050
Iteration 172/1000 | Loss: 0.00001050
Iteration 173/1000 | Loss: 0.00001050
Iteration 174/1000 | Loss: 0.00001050
Iteration 175/1000 | Loss: 0.00001050
Iteration 176/1000 | Loss: 0.00001050
Iteration 177/1000 | Loss: 0.00001050
Iteration 178/1000 | Loss: 0.00001050
Iteration 179/1000 | Loss: 0.00001050
Iteration 180/1000 | Loss: 0.00001050
Iteration 181/1000 | Loss: 0.00001050
Iteration 182/1000 | Loss: 0.00001050
Iteration 183/1000 | Loss: 0.00001050
Iteration 184/1000 | Loss: 0.00001050
Iteration 185/1000 | Loss: 0.00001050
Iteration 186/1000 | Loss: 0.00001050
Iteration 187/1000 | Loss: 0.00001050
Iteration 188/1000 | Loss: 0.00001050
Iteration 189/1000 | Loss: 0.00001050
Iteration 190/1000 | Loss: 0.00001050
Iteration 191/1000 | Loss: 0.00001050
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [1.0495108654140495e-05, 1.0495108654140495e-05, 1.0495108654140495e-05, 1.0495108654140495e-05, 1.0495108654140495e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0495108654140495e-05

Optimization complete. Final v2v error: 2.6887366771698 mm

Highest mean error: 3.246814489364624 mm for frame 120

Lowest mean error: 2.3236465454101562 mm for frame 20

Saving results

Total time: 43.545342445373535
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00822384
Iteration 2/25 | Loss: 0.00112231
Iteration 3/25 | Loss: 0.00099601
Iteration 4/25 | Loss: 0.00098819
Iteration 5/25 | Loss: 0.00098586
Iteration 6/25 | Loss: 0.00098538
Iteration 7/25 | Loss: 0.00098538
Iteration 8/25 | Loss: 0.00098538
Iteration 9/25 | Loss: 0.00098538
Iteration 10/25 | Loss: 0.00098538
Iteration 11/25 | Loss: 0.00098538
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009853772353380919, 0.0009853772353380919, 0.0009853772353380919, 0.0009853772353380919, 0.0009853772353380919]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009853772353380919

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38763678
Iteration 2/25 | Loss: 0.00068434
Iteration 3/25 | Loss: 0.00068434
Iteration 4/25 | Loss: 0.00068434
Iteration 5/25 | Loss: 0.00068434
Iteration 6/25 | Loss: 0.00068433
Iteration 7/25 | Loss: 0.00068433
Iteration 8/25 | Loss: 0.00068433
Iteration 9/25 | Loss: 0.00068433
Iteration 10/25 | Loss: 0.00068433
Iteration 11/25 | Loss: 0.00068433
Iteration 12/25 | Loss: 0.00068433
Iteration 13/25 | Loss: 0.00068433
Iteration 14/25 | Loss: 0.00068433
Iteration 15/25 | Loss: 0.00068433
Iteration 16/25 | Loss: 0.00068433
Iteration 17/25 | Loss: 0.00068433
Iteration 18/25 | Loss: 0.00068433
Iteration 19/25 | Loss: 0.00068433
Iteration 20/25 | Loss: 0.00068433
Iteration 21/25 | Loss: 0.00068433
Iteration 22/25 | Loss: 0.00068433
Iteration 23/25 | Loss: 0.00068433
Iteration 24/25 | Loss: 0.00068433
Iteration 25/25 | Loss: 0.00068433

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068433
Iteration 2/1000 | Loss: 0.00002923
Iteration 3/1000 | Loss: 0.00001859
Iteration 4/1000 | Loss: 0.00001417
Iteration 5/1000 | Loss: 0.00001222
Iteration 6/1000 | Loss: 0.00001135
Iteration 7/1000 | Loss: 0.00001063
Iteration 8/1000 | Loss: 0.00001021
Iteration 9/1000 | Loss: 0.00000991
Iteration 10/1000 | Loss: 0.00000974
Iteration 11/1000 | Loss: 0.00000960
Iteration 12/1000 | Loss: 0.00000954
Iteration 13/1000 | Loss: 0.00000954
Iteration 14/1000 | Loss: 0.00000952
Iteration 15/1000 | Loss: 0.00000951
Iteration 16/1000 | Loss: 0.00000950
Iteration 17/1000 | Loss: 0.00000949
Iteration 18/1000 | Loss: 0.00000949
Iteration 19/1000 | Loss: 0.00000948
Iteration 20/1000 | Loss: 0.00000948
Iteration 21/1000 | Loss: 0.00000942
Iteration 22/1000 | Loss: 0.00000940
Iteration 23/1000 | Loss: 0.00000940
Iteration 24/1000 | Loss: 0.00000939
Iteration 25/1000 | Loss: 0.00000938
Iteration 26/1000 | Loss: 0.00000938
Iteration 27/1000 | Loss: 0.00000937
Iteration 28/1000 | Loss: 0.00000937
Iteration 29/1000 | Loss: 0.00000936
Iteration 30/1000 | Loss: 0.00000936
Iteration 31/1000 | Loss: 0.00000935
Iteration 32/1000 | Loss: 0.00000935
Iteration 33/1000 | Loss: 0.00000934
Iteration 34/1000 | Loss: 0.00000934
Iteration 35/1000 | Loss: 0.00000934
Iteration 36/1000 | Loss: 0.00000933
Iteration 37/1000 | Loss: 0.00000933
Iteration 38/1000 | Loss: 0.00000932
Iteration 39/1000 | Loss: 0.00000932
Iteration 40/1000 | Loss: 0.00000932
Iteration 41/1000 | Loss: 0.00000931
Iteration 42/1000 | Loss: 0.00000931
Iteration 43/1000 | Loss: 0.00000931
Iteration 44/1000 | Loss: 0.00000930
Iteration 45/1000 | Loss: 0.00000929
Iteration 46/1000 | Loss: 0.00000929
Iteration 47/1000 | Loss: 0.00000929
Iteration 48/1000 | Loss: 0.00000929
Iteration 49/1000 | Loss: 0.00000929
Iteration 50/1000 | Loss: 0.00000928
Iteration 51/1000 | Loss: 0.00000928
Iteration 52/1000 | Loss: 0.00000927
Iteration 53/1000 | Loss: 0.00000927
Iteration 54/1000 | Loss: 0.00000927
Iteration 55/1000 | Loss: 0.00000926
Iteration 56/1000 | Loss: 0.00000926
Iteration 57/1000 | Loss: 0.00000926
Iteration 58/1000 | Loss: 0.00000926
Iteration 59/1000 | Loss: 0.00000926
Iteration 60/1000 | Loss: 0.00000925
Iteration 61/1000 | Loss: 0.00000925
Iteration 62/1000 | Loss: 0.00000925
Iteration 63/1000 | Loss: 0.00000924
Iteration 64/1000 | Loss: 0.00000924
Iteration 65/1000 | Loss: 0.00000924
Iteration 66/1000 | Loss: 0.00000924
Iteration 67/1000 | Loss: 0.00000924
Iteration 68/1000 | Loss: 0.00000923
Iteration 69/1000 | Loss: 0.00000923
Iteration 70/1000 | Loss: 0.00000923
Iteration 71/1000 | Loss: 0.00000922
Iteration 72/1000 | Loss: 0.00000922
Iteration 73/1000 | Loss: 0.00000922
Iteration 74/1000 | Loss: 0.00000922
Iteration 75/1000 | Loss: 0.00000922
Iteration 76/1000 | Loss: 0.00000922
Iteration 77/1000 | Loss: 0.00000922
Iteration 78/1000 | Loss: 0.00000921
Iteration 79/1000 | Loss: 0.00000921
Iteration 80/1000 | Loss: 0.00000921
Iteration 81/1000 | Loss: 0.00000921
Iteration 82/1000 | Loss: 0.00000921
Iteration 83/1000 | Loss: 0.00000921
Iteration 84/1000 | Loss: 0.00000921
Iteration 85/1000 | Loss: 0.00000921
Iteration 86/1000 | Loss: 0.00000921
Iteration 87/1000 | Loss: 0.00000921
Iteration 88/1000 | Loss: 0.00000921
Iteration 89/1000 | Loss: 0.00000920
Iteration 90/1000 | Loss: 0.00000920
Iteration 91/1000 | Loss: 0.00000920
Iteration 92/1000 | Loss: 0.00000920
Iteration 93/1000 | Loss: 0.00000920
Iteration 94/1000 | Loss: 0.00000920
Iteration 95/1000 | Loss: 0.00000920
Iteration 96/1000 | Loss: 0.00000919
Iteration 97/1000 | Loss: 0.00000919
Iteration 98/1000 | Loss: 0.00000919
Iteration 99/1000 | Loss: 0.00000919
Iteration 100/1000 | Loss: 0.00000919
Iteration 101/1000 | Loss: 0.00000919
Iteration 102/1000 | Loss: 0.00000919
Iteration 103/1000 | Loss: 0.00000919
Iteration 104/1000 | Loss: 0.00000919
Iteration 105/1000 | Loss: 0.00000919
Iteration 106/1000 | Loss: 0.00000918
Iteration 107/1000 | Loss: 0.00000918
Iteration 108/1000 | Loss: 0.00000918
Iteration 109/1000 | Loss: 0.00000918
Iteration 110/1000 | Loss: 0.00000917
Iteration 111/1000 | Loss: 0.00000917
Iteration 112/1000 | Loss: 0.00000917
Iteration 113/1000 | Loss: 0.00000917
Iteration 114/1000 | Loss: 0.00000917
Iteration 115/1000 | Loss: 0.00000917
Iteration 116/1000 | Loss: 0.00000916
Iteration 117/1000 | Loss: 0.00000916
Iteration 118/1000 | Loss: 0.00000916
Iteration 119/1000 | Loss: 0.00000916
Iteration 120/1000 | Loss: 0.00000916
Iteration 121/1000 | Loss: 0.00000916
Iteration 122/1000 | Loss: 0.00000916
Iteration 123/1000 | Loss: 0.00000916
Iteration 124/1000 | Loss: 0.00000916
Iteration 125/1000 | Loss: 0.00000916
Iteration 126/1000 | Loss: 0.00000916
Iteration 127/1000 | Loss: 0.00000916
Iteration 128/1000 | Loss: 0.00000916
Iteration 129/1000 | Loss: 0.00000916
Iteration 130/1000 | Loss: 0.00000916
Iteration 131/1000 | Loss: 0.00000916
Iteration 132/1000 | Loss: 0.00000915
Iteration 133/1000 | Loss: 0.00000915
Iteration 134/1000 | Loss: 0.00000915
Iteration 135/1000 | Loss: 0.00000915
Iteration 136/1000 | Loss: 0.00000915
Iteration 137/1000 | Loss: 0.00000915
Iteration 138/1000 | Loss: 0.00000915
Iteration 139/1000 | Loss: 0.00000915
Iteration 140/1000 | Loss: 0.00000915
Iteration 141/1000 | Loss: 0.00000915
Iteration 142/1000 | Loss: 0.00000915
Iteration 143/1000 | Loss: 0.00000915
Iteration 144/1000 | Loss: 0.00000914
Iteration 145/1000 | Loss: 0.00000914
Iteration 146/1000 | Loss: 0.00000914
Iteration 147/1000 | Loss: 0.00000914
Iteration 148/1000 | Loss: 0.00000914
Iteration 149/1000 | Loss: 0.00000914
Iteration 150/1000 | Loss: 0.00000914
Iteration 151/1000 | Loss: 0.00000914
Iteration 152/1000 | Loss: 0.00000914
Iteration 153/1000 | Loss: 0.00000914
Iteration 154/1000 | Loss: 0.00000914
Iteration 155/1000 | Loss: 0.00000914
Iteration 156/1000 | Loss: 0.00000914
Iteration 157/1000 | Loss: 0.00000914
Iteration 158/1000 | Loss: 0.00000914
Iteration 159/1000 | Loss: 0.00000914
Iteration 160/1000 | Loss: 0.00000914
Iteration 161/1000 | Loss: 0.00000914
Iteration 162/1000 | Loss: 0.00000913
Iteration 163/1000 | Loss: 0.00000913
Iteration 164/1000 | Loss: 0.00000913
Iteration 165/1000 | Loss: 0.00000913
Iteration 166/1000 | Loss: 0.00000913
Iteration 167/1000 | Loss: 0.00000913
Iteration 168/1000 | Loss: 0.00000913
Iteration 169/1000 | Loss: 0.00000913
Iteration 170/1000 | Loss: 0.00000913
Iteration 171/1000 | Loss: 0.00000913
Iteration 172/1000 | Loss: 0.00000913
Iteration 173/1000 | Loss: 0.00000913
Iteration 174/1000 | Loss: 0.00000913
Iteration 175/1000 | Loss: 0.00000913
Iteration 176/1000 | Loss: 0.00000913
Iteration 177/1000 | Loss: 0.00000913
Iteration 178/1000 | Loss: 0.00000913
Iteration 179/1000 | Loss: 0.00000913
Iteration 180/1000 | Loss: 0.00000912
Iteration 181/1000 | Loss: 0.00000912
Iteration 182/1000 | Loss: 0.00000912
Iteration 183/1000 | Loss: 0.00000912
Iteration 184/1000 | Loss: 0.00000912
Iteration 185/1000 | Loss: 0.00000912
Iteration 186/1000 | Loss: 0.00000912
Iteration 187/1000 | Loss: 0.00000911
Iteration 188/1000 | Loss: 0.00000911
Iteration 189/1000 | Loss: 0.00000911
Iteration 190/1000 | Loss: 0.00000911
Iteration 191/1000 | Loss: 0.00000911
Iteration 192/1000 | Loss: 0.00000911
Iteration 193/1000 | Loss: 0.00000910
Iteration 194/1000 | Loss: 0.00000910
Iteration 195/1000 | Loss: 0.00000910
Iteration 196/1000 | Loss: 0.00000910
Iteration 197/1000 | Loss: 0.00000910
Iteration 198/1000 | Loss: 0.00000910
Iteration 199/1000 | Loss: 0.00000910
Iteration 200/1000 | Loss: 0.00000909
Iteration 201/1000 | Loss: 0.00000909
Iteration 202/1000 | Loss: 0.00000909
Iteration 203/1000 | Loss: 0.00000909
Iteration 204/1000 | Loss: 0.00000909
Iteration 205/1000 | Loss: 0.00000909
Iteration 206/1000 | Loss: 0.00000909
Iteration 207/1000 | Loss: 0.00000909
Iteration 208/1000 | Loss: 0.00000909
Iteration 209/1000 | Loss: 0.00000909
Iteration 210/1000 | Loss: 0.00000909
Iteration 211/1000 | Loss: 0.00000909
Iteration 212/1000 | Loss: 0.00000909
Iteration 213/1000 | Loss: 0.00000909
Iteration 214/1000 | Loss: 0.00000909
Iteration 215/1000 | Loss: 0.00000909
Iteration 216/1000 | Loss: 0.00000908
Iteration 217/1000 | Loss: 0.00000908
Iteration 218/1000 | Loss: 0.00000908
Iteration 219/1000 | Loss: 0.00000908
Iteration 220/1000 | Loss: 0.00000908
Iteration 221/1000 | Loss: 0.00000908
Iteration 222/1000 | Loss: 0.00000908
Iteration 223/1000 | Loss: 0.00000908
Iteration 224/1000 | Loss: 0.00000907
Iteration 225/1000 | Loss: 0.00000907
Iteration 226/1000 | Loss: 0.00000907
Iteration 227/1000 | Loss: 0.00000907
Iteration 228/1000 | Loss: 0.00000907
Iteration 229/1000 | Loss: 0.00000907
Iteration 230/1000 | Loss: 0.00000907
Iteration 231/1000 | Loss: 0.00000907
Iteration 232/1000 | Loss: 0.00000907
Iteration 233/1000 | Loss: 0.00000907
Iteration 234/1000 | Loss: 0.00000907
Iteration 235/1000 | Loss: 0.00000907
Iteration 236/1000 | Loss: 0.00000907
Iteration 237/1000 | Loss: 0.00000906
Iteration 238/1000 | Loss: 0.00000906
Iteration 239/1000 | Loss: 0.00000906
Iteration 240/1000 | Loss: 0.00000906
Iteration 241/1000 | Loss: 0.00000906
Iteration 242/1000 | Loss: 0.00000906
Iteration 243/1000 | Loss: 0.00000906
Iteration 244/1000 | Loss: 0.00000906
Iteration 245/1000 | Loss: 0.00000906
Iteration 246/1000 | Loss: 0.00000906
Iteration 247/1000 | Loss: 0.00000906
Iteration 248/1000 | Loss: 0.00000906
Iteration 249/1000 | Loss: 0.00000906
Iteration 250/1000 | Loss: 0.00000906
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 250. Stopping optimization.
Last 5 losses: [9.061530363396741e-06, 9.061530363396741e-06, 9.061530363396741e-06, 9.061530363396741e-06, 9.061530363396741e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.061530363396741e-06

Optimization complete. Final v2v error: 2.4979465007781982 mm

Highest mean error: 3.5100505352020264 mm for frame 61

Lowest mean error: 2.1885428428649902 mm for frame 162

Saving results

Total time: 41.52113175392151
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00416378
Iteration 2/25 | Loss: 0.00114986
Iteration 3/25 | Loss: 0.00101622
Iteration 4/25 | Loss: 0.00100653
Iteration 5/25 | Loss: 0.00100493
Iteration 6/25 | Loss: 0.00100468
Iteration 7/25 | Loss: 0.00100468
Iteration 8/25 | Loss: 0.00100468
Iteration 9/25 | Loss: 0.00100468
Iteration 10/25 | Loss: 0.00100468
Iteration 11/25 | Loss: 0.00100468
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010046822717413306, 0.0010046822717413306, 0.0010046822717413306, 0.0010046822717413306, 0.0010046822717413306]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010046822717413306

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.07971048
Iteration 2/25 | Loss: 0.00051979
Iteration 3/25 | Loss: 0.00051977
Iteration 4/25 | Loss: 0.00051977
Iteration 5/25 | Loss: 0.00051977
Iteration 6/25 | Loss: 0.00051977
Iteration 7/25 | Loss: 0.00051977
Iteration 8/25 | Loss: 0.00051977
Iteration 9/25 | Loss: 0.00051977
Iteration 10/25 | Loss: 0.00051977
Iteration 11/25 | Loss: 0.00051977
Iteration 12/25 | Loss: 0.00051977
Iteration 13/25 | Loss: 0.00051977
Iteration 14/25 | Loss: 0.00051977
Iteration 15/25 | Loss: 0.00051977
Iteration 16/25 | Loss: 0.00051977
Iteration 17/25 | Loss: 0.00051977
Iteration 18/25 | Loss: 0.00051977
Iteration 19/25 | Loss: 0.00051977
Iteration 20/25 | Loss: 0.00051977
Iteration 21/25 | Loss: 0.00051977
Iteration 22/25 | Loss: 0.00051977
Iteration 23/25 | Loss: 0.00051977
Iteration 24/25 | Loss: 0.00051977
Iteration 25/25 | Loss: 0.00051977

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051977
Iteration 2/1000 | Loss: 0.00002468
Iteration 3/1000 | Loss: 0.00001774
Iteration 4/1000 | Loss: 0.00001505
Iteration 5/1000 | Loss: 0.00001418
Iteration 6/1000 | Loss: 0.00001344
Iteration 7/1000 | Loss: 0.00001299
Iteration 8/1000 | Loss: 0.00001253
Iteration 9/1000 | Loss: 0.00001239
Iteration 10/1000 | Loss: 0.00001237
Iteration 11/1000 | Loss: 0.00001218
Iteration 12/1000 | Loss: 0.00001216
Iteration 13/1000 | Loss: 0.00001201
Iteration 14/1000 | Loss: 0.00001195
Iteration 15/1000 | Loss: 0.00001187
Iteration 16/1000 | Loss: 0.00001184
Iteration 17/1000 | Loss: 0.00001184
Iteration 18/1000 | Loss: 0.00001179
Iteration 19/1000 | Loss: 0.00001173
Iteration 20/1000 | Loss: 0.00001173
Iteration 21/1000 | Loss: 0.00001173
Iteration 22/1000 | Loss: 0.00001173
Iteration 23/1000 | Loss: 0.00001173
Iteration 24/1000 | Loss: 0.00001173
Iteration 25/1000 | Loss: 0.00001173
Iteration 26/1000 | Loss: 0.00001173
Iteration 27/1000 | Loss: 0.00001173
Iteration 28/1000 | Loss: 0.00001173
Iteration 29/1000 | Loss: 0.00001173
Iteration 30/1000 | Loss: 0.00001173
Iteration 31/1000 | Loss: 0.00001173
Iteration 32/1000 | Loss: 0.00001173
Iteration 33/1000 | Loss: 0.00001173
Iteration 34/1000 | Loss: 0.00001173
Iteration 35/1000 | Loss: 0.00001173
Iteration 36/1000 | Loss: 0.00001172
Iteration 37/1000 | Loss: 0.00001172
Iteration 38/1000 | Loss: 0.00001172
Iteration 39/1000 | Loss: 0.00001172
Iteration 40/1000 | Loss: 0.00001172
Iteration 41/1000 | Loss: 0.00001172
Iteration 42/1000 | Loss: 0.00001172
Iteration 43/1000 | Loss: 0.00001172
Iteration 44/1000 | Loss: 0.00001172
Iteration 45/1000 | Loss: 0.00001172
Iteration 46/1000 | Loss: 0.00001172
Iteration 47/1000 | Loss: 0.00001172
Iteration 48/1000 | Loss: 0.00001172
Iteration 49/1000 | Loss: 0.00001172
Iteration 50/1000 | Loss: 0.00001172
Iteration 51/1000 | Loss: 0.00001172
Iteration 52/1000 | Loss: 0.00001172
Iteration 53/1000 | Loss: 0.00001172
Iteration 54/1000 | Loss: 0.00001172
Iteration 55/1000 | Loss: 0.00001172
Iteration 56/1000 | Loss: 0.00001172
Iteration 57/1000 | Loss: 0.00001172
Iteration 58/1000 | Loss: 0.00001172
Iteration 59/1000 | Loss: 0.00001172
Iteration 60/1000 | Loss: 0.00001172
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 60. Stopping optimization.
Last 5 losses: [1.1724489922926296e-05, 1.1724489922926296e-05, 1.1724489922926296e-05, 1.1724489922926296e-05, 1.1724489922926296e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1724489922926296e-05

Optimization complete. Final v2v error: 2.913055896759033 mm

Highest mean error: 3.4962973594665527 mm for frame 104

Lowest mean error: 2.6060714721679688 mm for frame 0

Saving results

Total time: 27.036359310150146
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00835458
Iteration 2/25 | Loss: 0.00155663
Iteration 3/25 | Loss: 0.00118224
Iteration 4/25 | Loss: 0.00115534
Iteration 5/25 | Loss: 0.00115304
Iteration 6/25 | Loss: 0.00115304
Iteration 7/25 | Loss: 0.00115304
Iteration 8/25 | Loss: 0.00115304
Iteration 9/25 | Loss: 0.00115304
Iteration 10/25 | Loss: 0.00115304
Iteration 11/25 | Loss: 0.00115304
Iteration 12/25 | Loss: 0.00115304
Iteration 13/25 | Loss: 0.00115304
Iteration 14/25 | Loss: 0.00115304
Iteration 15/25 | Loss: 0.00115304
Iteration 16/25 | Loss: 0.00115304
Iteration 17/25 | Loss: 0.00115304
Iteration 18/25 | Loss: 0.00115304
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011530423071235418, 0.0011530423071235418, 0.0011530423071235418, 0.0011530423071235418, 0.0011530423071235418]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011530423071235418

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.86413312
Iteration 2/25 | Loss: 0.00075961
Iteration 3/25 | Loss: 0.00075958
Iteration 4/25 | Loss: 0.00075958
Iteration 5/25 | Loss: 0.00075958
Iteration 6/25 | Loss: 0.00075958
Iteration 7/25 | Loss: 0.00075958
Iteration 8/25 | Loss: 0.00075958
Iteration 9/25 | Loss: 0.00075958
Iteration 10/25 | Loss: 0.00075958
Iteration 11/25 | Loss: 0.00075958
Iteration 12/25 | Loss: 0.00075958
Iteration 13/25 | Loss: 0.00075958
Iteration 14/25 | Loss: 0.00075958
Iteration 15/25 | Loss: 0.00075958
Iteration 16/25 | Loss: 0.00075958
Iteration 17/25 | Loss: 0.00075958
Iteration 18/25 | Loss: 0.00075958
Iteration 19/25 | Loss: 0.00075958
Iteration 20/25 | Loss: 0.00075958
Iteration 21/25 | Loss: 0.00075958
Iteration 22/25 | Loss: 0.00075958
Iteration 23/25 | Loss: 0.00075958
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007595751085318625, 0.0007595751085318625, 0.0007595751085318625, 0.0007595751085318625, 0.0007595751085318625]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007595751085318625

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075958
Iteration 2/1000 | Loss: 0.00004399
Iteration 3/1000 | Loss: 0.00002139
Iteration 4/1000 | Loss: 0.00001803
Iteration 5/1000 | Loss: 0.00001689
Iteration 6/1000 | Loss: 0.00001626
Iteration 7/1000 | Loss: 0.00001592
Iteration 8/1000 | Loss: 0.00001573
Iteration 9/1000 | Loss: 0.00001549
Iteration 10/1000 | Loss: 0.00001537
Iteration 11/1000 | Loss: 0.00001529
Iteration 12/1000 | Loss: 0.00001524
Iteration 13/1000 | Loss: 0.00001516
Iteration 14/1000 | Loss: 0.00001510
Iteration 15/1000 | Loss: 0.00001508
Iteration 16/1000 | Loss: 0.00001507
Iteration 17/1000 | Loss: 0.00001507
Iteration 18/1000 | Loss: 0.00001506
Iteration 19/1000 | Loss: 0.00001506
Iteration 20/1000 | Loss: 0.00001506
Iteration 21/1000 | Loss: 0.00001505
Iteration 22/1000 | Loss: 0.00001504
Iteration 23/1000 | Loss: 0.00001503
Iteration 24/1000 | Loss: 0.00001502
Iteration 25/1000 | Loss: 0.00001501
Iteration 26/1000 | Loss: 0.00001501
Iteration 27/1000 | Loss: 0.00001500
Iteration 28/1000 | Loss: 0.00001499
Iteration 29/1000 | Loss: 0.00001499
Iteration 30/1000 | Loss: 0.00001498
Iteration 31/1000 | Loss: 0.00001498
Iteration 32/1000 | Loss: 0.00001497
Iteration 33/1000 | Loss: 0.00001497
Iteration 34/1000 | Loss: 0.00001496
Iteration 35/1000 | Loss: 0.00001496
Iteration 36/1000 | Loss: 0.00001495
Iteration 37/1000 | Loss: 0.00001495
Iteration 38/1000 | Loss: 0.00001495
Iteration 39/1000 | Loss: 0.00001494
Iteration 40/1000 | Loss: 0.00001494
Iteration 41/1000 | Loss: 0.00001494
Iteration 42/1000 | Loss: 0.00001494
Iteration 43/1000 | Loss: 0.00001493
Iteration 44/1000 | Loss: 0.00001493
Iteration 45/1000 | Loss: 0.00001493
Iteration 46/1000 | Loss: 0.00001493
Iteration 47/1000 | Loss: 0.00001492
Iteration 48/1000 | Loss: 0.00001492
Iteration 49/1000 | Loss: 0.00001492
Iteration 50/1000 | Loss: 0.00001492
Iteration 51/1000 | Loss: 0.00001492
Iteration 52/1000 | Loss: 0.00001491
Iteration 53/1000 | Loss: 0.00001491
Iteration 54/1000 | Loss: 0.00001491
Iteration 55/1000 | Loss: 0.00001491
Iteration 56/1000 | Loss: 0.00001491
Iteration 57/1000 | Loss: 0.00001491
Iteration 58/1000 | Loss: 0.00001490
Iteration 59/1000 | Loss: 0.00001490
Iteration 60/1000 | Loss: 0.00001490
Iteration 61/1000 | Loss: 0.00001490
Iteration 62/1000 | Loss: 0.00001490
Iteration 63/1000 | Loss: 0.00001490
Iteration 64/1000 | Loss: 0.00001490
Iteration 65/1000 | Loss: 0.00001489
Iteration 66/1000 | Loss: 0.00001489
Iteration 67/1000 | Loss: 0.00001489
Iteration 68/1000 | Loss: 0.00001489
Iteration 69/1000 | Loss: 0.00001489
Iteration 70/1000 | Loss: 0.00001489
Iteration 71/1000 | Loss: 0.00001489
Iteration 72/1000 | Loss: 0.00001489
Iteration 73/1000 | Loss: 0.00001489
Iteration 74/1000 | Loss: 0.00001489
Iteration 75/1000 | Loss: 0.00001489
Iteration 76/1000 | Loss: 0.00001489
Iteration 77/1000 | Loss: 0.00001489
Iteration 78/1000 | Loss: 0.00001489
Iteration 79/1000 | Loss: 0.00001489
Iteration 80/1000 | Loss: 0.00001489
Iteration 81/1000 | Loss: 0.00001489
Iteration 82/1000 | Loss: 0.00001489
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 82. Stopping optimization.
Last 5 losses: [1.489446822233731e-05, 1.489446822233731e-05, 1.489446822233731e-05, 1.489446822233731e-05, 1.489446822233731e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.489446822233731e-05

Optimization complete. Final v2v error: 3.2389156818389893 mm

Highest mean error: 3.5399014949798584 mm for frame 116

Lowest mean error: 2.8801887035369873 mm for frame 5

Saving results

Total time: 33.054102659225464
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00496245
Iteration 2/25 | Loss: 0.00146967
Iteration 3/25 | Loss: 0.00125062
Iteration 4/25 | Loss: 0.00120465
Iteration 5/25 | Loss: 0.00120057
Iteration 6/25 | Loss: 0.00117584
Iteration 7/25 | Loss: 0.00117135
Iteration 8/25 | Loss: 0.00115333
Iteration 9/25 | Loss: 0.00114853
Iteration 10/25 | Loss: 0.00113865
Iteration 11/25 | Loss: 0.00112760
Iteration 12/25 | Loss: 0.00112416
Iteration 13/25 | Loss: 0.00112652
Iteration 14/25 | Loss: 0.00112540
Iteration 15/25 | Loss: 0.00112007
Iteration 16/25 | Loss: 0.00111636
Iteration 17/25 | Loss: 0.00111524
Iteration 18/25 | Loss: 0.00111469
Iteration 19/25 | Loss: 0.00111929
Iteration 20/25 | Loss: 0.00111761
Iteration 21/25 | Loss: 0.00111395
Iteration 22/25 | Loss: 0.00111319
Iteration 23/25 | Loss: 0.00111303
Iteration 24/25 | Loss: 0.00111302
Iteration 25/25 | Loss: 0.00111302

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38297963
Iteration 2/25 | Loss: 0.00288650
Iteration 3/25 | Loss: 0.00288650
Iteration 4/25 | Loss: 0.00288650
Iteration 5/25 | Loss: 0.00288650
Iteration 6/25 | Loss: 0.00288650
Iteration 7/25 | Loss: 0.00288650
Iteration 8/25 | Loss: 0.00288650
Iteration 9/25 | Loss: 0.00288650
Iteration 10/25 | Loss: 0.00288650
Iteration 11/25 | Loss: 0.00288650
Iteration 12/25 | Loss: 0.00288650
Iteration 13/25 | Loss: 0.00288650
Iteration 14/25 | Loss: 0.00288650
Iteration 15/25 | Loss: 0.00288650
Iteration 16/25 | Loss: 0.00288650
Iteration 17/25 | Loss: 0.00288650
Iteration 18/25 | Loss: 0.00288650
Iteration 19/25 | Loss: 0.00288650
Iteration 20/25 | Loss: 0.00288650
Iteration 21/25 | Loss: 0.00288650
Iteration 22/25 | Loss: 0.00288650
Iteration 23/25 | Loss: 0.00288650
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0028864957857877016, 0.0028864957857877016, 0.0028864957857877016, 0.0028864957857877016, 0.0028864957857877016]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0028864957857877016

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00288650
Iteration 2/1000 | Loss: 0.00021097
Iteration 3/1000 | Loss: 0.00012503
Iteration 4/1000 | Loss: 0.00009868
Iteration 5/1000 | Loss: 0.00008742
Iteration 6/1000 | Loss: 0.00008004
Iteration 7/1000 | Loss: 0.00007597
Iteration 8/1000 | Loss: 0.00007335
Iteration 9/1000 | Loss: 0.00007086
Iteration 10/1000 | Loss: 0.00006913
Iteration 11/1000 | Loss: 0.00006790
Iteration 12/1000 | Loss: 0.00006647
Iteration 13/1000 | Loss: 0.00006538
Iteration 14/1000 | Loss: 0.00006421
Iteration 15/1000 | Loss: 0.00052875
Iteration 16/1000 | Loss: 0.00008370
Iteration 17/1000 | Loss: 0.00007431
Iteration 18/1000 | Loss: 0.00006782
Iteration 19/1000 | Loss: 0.00006471
Iteration 20/1000 | Loss: 0.00006291
Iteration 21/1000 | Loss: 0.00006186
Iteration 22/1000 | Loss: 0.00006079
Iteration 23/1000 | Loss: 0.00005971
Iteration 24/1000 | Loss: 0.00032806
Iteration 25/1000 | Loss: 0.00031976
Iteration 26/1000 | Loss: 0.00018579
Iteration 27/1000 | Loss: 0.00007269
Iteration 28/1000 | Loss: 0.00006451
Iteration 29/1000 | Loss: 0.00006166
Iteration 30/1000 | Loss: 0.00033461
Iteration 31/1000 | Loss: 0.00006978
Iteration 32/1000 | Loss: 0.00006344
Iteration 33/1000 | Loss: 0.00006169
Iteration 34/1000 | Loss: 0.00005939
Iteration 35/1000 | Loss: 0.00005790
Iteration 36/1000 | Loss: 0.00005651
Iteration 37/1000 | Loss: 0.00005555
Iteration 38/1000 | Loss: 0.00032537
Iteration 39/1000 | Loss: 0.00006610
Iteration 40/1000 | Loss: 0.00005883
Iteration 41/1000 | Loss: 0.00005587
Iteration 42/1000 | Loss: 0.00005358
Iteration 43/1000 | Loss: 0.00005179
Iteration 44/1000 | Loss: 0.00005115
Iteration 45/1000 | Loss: 0.00005076
Iteration 46/1000 | Loss: 0.00032382
Iteration 47/1000 | Loss: 0.00005766
Iteration 48/1000 | Loss: 0.00005324
Iteration 49/1000 | Loss: 0.00005147
Iteration 50/1000 | Loss: 0.00004978
Iteration 51/1000 | Loss: 0.00004891
Iteration 52/1000 | Loss: 0.00004865
Iteration 53/1000 | Loss: 0.00004861
Iteration 54/1000 | Loss: 0.00004847
Iteration 55/1000 | Loss: 0.00004826
Iteration 56/1000 | Loss: 0.00004800
Iteration 57/1000 | Loss: 0.00004778
Iteration 58/1000 | Loss: 0.00004766
Iteration 59/1000 | Loss: 0.00004763
Iteration 60/1000 | Loss: 0.00004763
Iteration 61/1000 | Loss: 0.00004762
Iteration 62/1000 | Loss: 0.00004762
Iteration 63/1000 | Loss: 0.00004761
Iteration 64/1000 | Loss: 0.00004761
Iteration 65/1000 | Loss: 0.00004760
Iteration 66/1000 | Loss: 0.00004760
Iteration 67/1000 | Loss: 0.00004760
Iteration 68/1000 | Loss: 0.00004759
Iteration 69/1000 | Loss: 0.00004759
Iteration 70/1000 | Loss: 0.00004757
Iteration 71/1000 | Loss: 0.00004757
Iteration 72/1000 | Loss: 0.00004756
Iteration 73/1000 | Loss: 0.00004756
Iteration 74/1000 | Loss: 0.00004755
Iteration 75/1000 | Loss: 0.00004752
Iteration 76/1000 | Loss: 0.00004752
Iteration 77/1000 | Loss: 0.00004752
Iteration 78/1000 | Loss: 0.00004751
Iteration 79/1000 | Loss: 0.00004750
Iteration 80/1000 | Loss: 0.00004749
Iteration 81/1000 | Loss: 0.00004749
Iteration 82/1000 | Loss: 0.00004748
Iteration 83/1000 | Loss: 0.00004748
Iteration 84/1000 | Loss: 0.00004746
Iteration 85/1000 | Loss: 0.00004746
Iteration 86/1000 | Loss: 0.00004746
Iteration 87/1000 | Loss: 0.00004746
Iteration 88/1000 | Loss: 0.00004746
Iteration 89/1000 | Loss: 0.00004745
Iteration 90/1000 | Loss: 0.00004745
Iteration 91/1000 | Loss: 0.00004744
Iteration 92/1000 | Loss: 0.00004742
Iteration 93/1000 | Loss: 0.00004742
Iteration 94/1000 | Loss: 0.00004741
Iteration 95/1000 | Loss: 0.00004741
Iteration 96/1000 | Loss: 0.00004741
Iteration 97/1000 | Loss: 0.00004741
Iteration 98/1000 | Loss: 0.00004741
Iteration 99/1000 | Loss: 0.00004741
Iteration 100/1000 | Loss: 0.00004741
Iteration 101/1000 | Loss: 0.00004741
Iteration 102/1000 | Loss: 0.00004740
Iteration 103/1000 | Loss: 0.00004740
Iteration 104/1000 | Loss: 0.00004740
Iteration 105/1000 | Loss: 0.00004740
Iteration 106/1000 | Loss: 0.00004740
Iteration 107/1000 | Loss: 0.00004739
Iteration 108/1000 | Loss: 0.00004739
Iteration 109/1000 | Loss: 0.00004739
Iteration 110/1000 | Loss: 0.00004739
Iteration 111/1000 | Loss: 0.00004739
Iteration 112/1000 | Loss: 0.00004739
Iteration 113/1000 | Loss: 0.00004739
Iteration 114/1000 | Loss: 0.00004739
Iteration 115/1000 | Loss: 0.00004739
Iteration 116/1000 | Loss: 0.00004738
Iteration 117/1000 | Loss: 0.00004738
Iteration 118/1000 | Loss: 0.00004738
Iteration 119/1000 | Loss: 0.00004738
Iteration 120/1000 | Loss: 0.00004737
Iteration 121/1000 | Loss: 0.00004737
Iteration 122/1000 | Loss: 0.00004737
Iteration 123/1000 | Loss: 0.00004737
Iteration 124/1000 | Loss: 0.00004736
Iteration 125/1000 | Loss: 0.00004736
Iteration 126/1000 | Loss: 0.00004735
Iteration 127/1000 | Loss: 0.00004735
Iteration 128/1000 | Loss: 0.00004734
Iteration 129/1000 | Loss: 0.00004734
Iteration 130/1000 | Loss: 0.00004733
Iteration 131/1000 | Loss: 0.00004733
Iteration 132/1000 | Loss: 0.00004733
Iteration 133/1000 | Loss: 0.00004732
Iteration 134/1000 | Loss: 0.00004732
Iteration 135/1000 | Loss: 0.00004731
Iteration 136/1000 | Loss: 0.00004731
Iteration 137/1000 | Loss: 0.00004731
Iteration 138/1000 | Loss: 0.00004730
Iteration 139/1000 | Loss: 0.00004730
Iteration 140/1000 | Loss: 0.00004729
Iteration 141/1000 | Loss: 0.00004729
Iteration 142/1000 | Loss: 0.00004727
Iteration 143/1000 | Loss: 0.00004727
Iteration 144/1000 | Loss: 0.00004727
Iteration 145/1000 | Loss: 0.00004727
Iteration 146/1000 | Loss: 0.00004727
Iteration 147/1000 | Loss: 0.00004727
Iteration 148/1000 | Loss: 0.00004727
Iteration 149/1000 | Loss: 0.00004726
Iteration 150/1000 | Loss: 0.00004726
Iteration 151/1000 | Loss: 0.00004726
Iteration 152/1000 | Loss: 0.00004726
Iteration 153/1000 | Loss: 0.00004726
Iteration 154/1000 | Loss: 0.00004726
Iteration 155/1000 | Loss: 0.00004725
Iteration 156/1000 | Loss: 0.00004725
Iteration 157/1000 | Loss: 0.00004725
Iteration 158/1000 | Loss: 0.00004724
Iteration 159/1000 | Loss: 0.00004724
Iteration 160/1000 | Loss: 0.00004724
Iteration 161/1000 | Loss: 0.00004724
Iteration 162/1000 | Loss: 0.00004724
Iteration 163/1000 | Loss: 0.00004724
Iteration 164/1000 | Loss: 0.00004723
Iteration 165/1000 | Loss: 0.00004723
Iteration 166/1000 | Loss: 0.00004723
Iteration 167/1000 | Loss: 0.00004723
Iteration 168/1000 | Loss: 0.00004723
Iteration 169/1000 | Loss: 0.00004722
Iteration 170/1000 | Loss: 0.00004722
Iteration 171/1000 | Loss: 0.00004722
Iteration 172/1000 | Loss: 0.00004722
Iteration 173/1000 | Loss: 0.00004722
Iteration 174/1000 | Loss: 0.00004721
Iteration 175/1000 | Loss: 0.00004721
Iteration 176/1000 | Loss: 0.00004721
Iteration 177/1000 | Loss: 0.00004721
Iteration 178/1000 | Loss: 0.00004721
Iteration 179/1000 | Loss: 0.00004720
Iteration 180/1000 | Loss: 0.00004720
Iteration 181/1000 | Loss: 0.00004720
Iteration 182/1000 | Loss: 0.00004719
Iteration 183/1000 | Loss: 0.00004719
Iteration 184/1000 | Loss: 0.00004719
Iteration 185/1000 | Loss: 0.00004719
Iteration 186/1000 | Loss: 0.00004719
Iteration 187/1000 | Loss: 0.00004719
Iteration 188/1000 | Loss: 0.00004718
Iteration 189/1000 | Loss: 0.00004718
Iteration 190/1000 | Loss: 0.00004718
Iteration 191/1000 | Loss: 0.00004718
Iteration 192/1000 | Loss: 0.00004717
Iteration 193/1000 | Loss: 0.00004717
Iteration 194/1000 | Loss: 0.00004717
Iteration 195/1000 | Loss: 0.00004716
Iteration 196/1000 | Loss: 0.00004715
Iteration 197/1000 | Loss: 0.00004714
Iteration 198/1000 | Loss: 0.00004714
Iteration 199/1000 | Loss: 0.00004713
Iteration 200/1000 | Loss: 0.00004713
Iteration 201/1000 | Loss: 0.00004712
Iteration 202/1000 | Loss: 0.00004709
Iteration 203/1000 | Loss: 0.00004703
Iteration 204/1000 | Loss: 0.00004700
Iteration 205/1000 | Loss: 0.00004700
Iteration 206/1000 | Loss: 0.00004699
Iteration 207/1000 | Loss: 0.00004697
Iteration 208/1000 | Loss: 0.00004696
Iteration 209/1000 | Loss: 0.00004691
Iteration 210/1000 | Loss: 0.00004691
Iteration 211/1000 | Loss: 0.00004690
Iteration 212/1000 | Loss: 0.00004690
Iteration 213/1000 | Loss: 0.00004689
Iteration 214/1000 | Loss: 0.00004689
Iteration 215/1000 | Loss: 0.00004689
Iteration 216/1000 | Loss: 0.00004689
Iteration 217/1000 | Loss: 0.00004689
Iteration 218/1000 | Loss: 0.00004689
Iteration 219/1000 | Loss: 0.00004689
Iteration 220/1000 | Loss: 0.00004688
Iteration 221/1000 | Loss: 0.00004688
Iteration 222/1000 | Loss: 0.00004688
Iteration 223/1000 | Loss: 0.00004688
Iteration 224/1000 | Loss: 0.00004688
Iteration 225/1000 | Loss: 0.00004688
Iteration 226/1000 | Loss: 0.00004688
Iteration 227/1000 | Loss: 0.00004688
Iteration 228/1000 | Loss: 0.00004688
Iteration 229/1000 | Loss: 0.00004688
Iteration 230/1000 | Loss: 0.00004687
Iteration 231/1000 | Loss: 0.00004687
Iteration 232/1000 | Loss: 0.00004687
Iteration 233/1000 | Loss: 0.00004687
Iteration 234/1000 | Loss: 0.00004687
Iteration 235/1000 | Loss: 0.00004687
Iteration 236/1000 | Loss: 0.00004687
Iteration 237/1000 | Loss: 0.00004687
Iteration 238/1000 | Loss: 0.00004687
Iteration 239/1000 | Loss: 0.00004687
Iteration 240/1000 | Loss: 0.00004687
Iteration 241/1000 | Loss: 0.00004687
Iteration 242/1000 | Loss: 0.00004687
Iteration 243/1000 | Loss: 0.00004687
Iteration 244/1000 | Loss: 0.00004687
Iteration 245/1000 | Loss: 0.00004687
Iteration 246/1000 | Loss: 0.00004687
Iteration 247/1000 | Loss: 0.00004687
Iteration 248/1000 | Loss: 0.00004687
Iteration 249/1000 | Loss: 0.00004687
Iteration 250/1000 | Loss: 0.00004687
Iteration 251/1000 | Loss: 0.00004687
Iteration 252/1000 | Loss: 0.00004687
Iteration 253/1000 | Loss: 0.00004687
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 253. Stopping optimization.
Last 5 losses: [4.686538886744529e-05, 4.686538886744529e-05, 4.686538886744529e-05, 4.686538886744529e-05, 4.686538886744529e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.686538886744529e-05

Optimization complete. Final v2v error: 3.9499387741088867 mm

Highest mean error: 11.49133014678955 mm for frame 108

Lowest mean error: 2.311647415161133 mm for frame 173

Saving results

Total time: 144.68578624725342
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00399448
Iteration 2/25 | Loss: 0.00114899
Iteration 3/25 | Loss: 0.00107112
Iteration 4/25 | Loss: 0.00105077
Iteration 5/25 | Loss: 0.00104422
Iteration 6/25 | Loss: 0.00104284
Iteration 7/25 | Loss: 0.00104284
Iteration 8/25 | Loss: 0.00104284
Iteration 9/25 | Loss: 0.00104284
Iteration 10/25 | Loss: 0.00104284
Iteration 11/25 | Loss: 0.00104284
Iteration 12/25 | Loss: 0.00104284
Iteration 13/25 | Loss: 0.00104284
Iteration 14/25 | Loss: 0.00104284
Iteration 15/25 | Loss: 0.00104284
Iteration 16/25 | Loss: 0.00104284
Iteration 17/25 | Loss: 0.00104284
Iteration 18/25 | Loss: 0.00104284
Iteration 19/25 | Loss: 0.00104284
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010428389068692923, 0.0010428389068692923, 0.0010428389068692923, 0.0010428389068692923, 0.0010428389068692923]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010428389068692923

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.68035650
Iteration 2/25 | Loss: 0.00070449
Iteration 3/25 | Loss: 0.00070448
Iteration 4/25 | Loss: 0.00070448
Iteration 5/25 | Loss: 0.00070448
Iteration 6/25 | Loss: 0.00070448
Iteration 7/25 | Loss: 0.00070448
Iteration 8/25 | Loss: 0.00070448
Iteration 9/25 | Loss: 0.00070448
Iteration 10/25 | Loss: 0.00070448
Iteration 11/25 | Loss: 0.00070448
Iteration 12/25 | Loss: 0.00070448
Iteration 13/25 | Loss: 0.00070448
Iteration 14/25 | Loss: 0.00070448
Iteration 15/25 | Loss: 0.00070448
Iteration 16/25 | Loss: 0.00070448
Iteration 17/25 | Loss: 0.00070448
Iteration 18/25 | Loss: 0.00070448
Iteration 19/25 | Loss: 0.00070448
Iteration 20/25 | Loss: 0.00070448
Iteration 21/25 | Loss: 0.00070448
Iteration 22/25 | Loss: 0.00070448
Iteration 23/25 | Loss: 0.00070448
Iteration 24/25 | Loss: 0.00070448
Iteration 25/25 | Loss: 0.00070448

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070448
Iteration 2/1000 | Loss: 0.00002729
Iteration 3/1000 | Loss: 0.00002217
Iteration 4/1000 | Loss: 0.00002134
Iteration 5/1000 | Loss: 0.00002068
Iteration 6/1000 | Loss: 0.00002030
Iteration 7/1000 | Loss: 0.00001992
Iteration 8/1000 | Loss: 0.00001966
Iteration 9/1000 | Loss: 0.00001939
Iteration 10/1000 | Loss: 0.00001938
Iteration 11/1000 | Loss: 0.00001930
Iteration 12/1000 | Loss: 0.00001920
Iteration 13/1000 | Loss: 0.00001916
Iteration 14/1000 | Loss: 0.00001907
Iteration 15/1000 | Loss: 0.00001899
Iteration 16/1000 | Loss: 0.00001896
Iteration 17/1000 | Loss: 0.00001895
Iteration 18/1000 | Loss: 0.00001894
Iteration 19/1000 | Loss: 0.00001894
Iteration 20/1000 | Loss: 0.00001893
Iteration 21/1000 | Loss: 0.00001892
Iteration 22/1000 | Loss: 0.00001892
Iteration 23/1000 | Loss: 0.00001892
Iteration 24/1000 | Loss: 0.00001891
Iteration 25/1000 | Loss: 0.00001891
Iteration 26/1000 | Loss: 0.00001890
Iteration 27/1000 | Loss: 0.00001890
Iteration 28/1000 | Loss: 0.00001890
Iteration 29/1000 | Loss: 0.00001889
Iteration 30/1000 | Loss: 0.00001889
Iteration 31/1000 | Loss: 0.00001889
Iteration 32/1000 | Loss: 0.00001888
Iteration 33/1000 | Loss: 0.00001888
Iteration 34/1000 | Loss: 0.00001888
Iteration 35/1000 | Loss: 0.00001887
Iteration 36/1000 | Loss: 0.00001887
Iteration 37/1000 | Loss: 0.00001886
Iteration 38/1000 | Loss: 0.00001886
Iteration 39/1000 | Loss: 0.00001886
Iteration 40/1000 | Loss: 0.00001886
Iteration 41/1000 | Loss: 0.00001886
Iteration 42/1000 | Loss: 0.00001885
Iteration 43/1000 | Loss: 0.00001885
Iteration 44/1000 | Loss: 0.00001885
Iteration 45/1000 | Loss: 0.00001884
Iteration 46/1000 | Loss: 0.00001883
Iteration 47/1000 | Loss: 0.00001882
Iteration 48/1000 | Loss: 0.00001882
Iteration 49/1000 | Loss: 0.00001882
Iteration 50/1000 | Loss: 0.00001881
Iteration 51/1000 | Loss: 0.00001880
Iteration 52/1000 | Loss: 0.00001880
Iteration 53/1000 | Loss: 0.00001880
Iteration 54/1000 | Loss: 0.00001880
Iteration 55/1000 | Loss: 0.00001880
Iteration 56/1000 | Loss: 0.00001880
Iteration 57/1000 | Loss: 0.00001880
Iteration 58/1000 | Loss: 0.00001880
Iteration 59/1000 | Loss: 0.00001879
Iteration 60/1000 | Loss: 0.00001879
Iteration 61/1000 | Loss: 0.00001879
Iteration 62/1000 | Loss: 0.00001879
Iteration 63/1000 | Loss: 0.00001879
Iteration 64/1000 | Loss: 0.00001879
Iteration 65/1000 | Loss: 0.00001879
Iteration 66/1000 | Loss: 0.00001879
Iteration 67/1000 | Loss: 0.00001879
Iteration 68/1000 | Loss: 0.00001879
Iteration 69/1000 | Loss: 0.00001878
Iteration 70/1000 | Loss: 0.00001878
Iteration 71/1000 | Loss: 0.00001878
Iteration 72/1000 | Loss: 0.00001877
Iteration 73/1000 | Loss: 0.00001877
Iteration 74/1000 | Loss: 0.00001877
Iteration 75/1000 | Loss: 0.00001877
Iteration 76/1000 | Loss: 0.00001877
Iteration 77/1000 | Loss: 0.00001877
Iteration 78/1000 | Loss: 0.00001877
Iteration 79/1000 | Loss: 0.00001876
Iteration 80/1000 | Loss: 0.00001876
Iteration 81/1000 | Loss: 0.00001876
Iteration 82/1000 | Loss: 0.00001875
Iteration 83/1000 | Loss: 0.00001875
Iteration 84/1000 | Loss: 0.00001875
Iteration 85/1000 | Loss: 0.00001875
Iteration 86/1000 | Loss: 0.00001874
Iteration 87/1000 | Loss: 0.00001874
Iteration 88/1000 | Loss: 0.00001874
Iteration 89/1000 | Loss: 0.00001874
Iteration 90/1000 | Loss: 0.00001874
Iteration 91/1000 | Loss: 0.00001874
Iteration 92/1000 | Loss: 0.00001874
Iteration 93/1000 | Loss: 0.00001874
Iteration 94/1000 | Loss: 0.00001873
Iteration 95/1000 | Loss: 0.00001873
Iteration 96/1000 | Loss: 0.00001873
Iteration 97/1000 | Loss: 0.00001872
Iteration 98/1000 | Loss: 0.00001872
Iteration 99/1000 | Loss: 0.00001872
Iteration 100/1000 | Loss: 0.00001872
Iteration 101/1000 | Loss: 0.00001871
Iteration 102/1000 | Loss: 0.00001871
Iteration 103/1000 | Loss: 0.00001871
Iteration 104/1000 | Loss: 0.00001871
Iteration 105/1000 | Loss: 0.00001871
Iteration 106/1000 | Loss: 0.00001871
Iteration 107/1000 | Loss: 0.00001871
Iteration 108/1000 | Loss: 0.00001871
Iteration 109/1000 | Loss: 0.00001871
Iteration 110/1000 | Loss: 0.00001871
Iteration 111/1000 | Loss: 0.00001871
Iteration 112/1000 | Loss: 0.00001870
Iteration 113/1000 | Loss: 0.00001870
Iteration 114/1000 | Loss: 0.00001870
Iteration 115/1000 | Loss: 0.00001870
Iteration 116/1000 | Loss: 0.00001870
Iteration 117/1000 | Loss: 0.00001869
Iteration 118/1000 | Loss: 0.00001869
Iteration 119/1000 | Loss: 0.00001869
Iteration 120/1000 | Loss: 0.00001869
Iteration 121/1000 | Loss: 0.00001869
Iteration 122/1000 | Loss: 0.00001869
Iteration 123/1000 | Loss: 0.00001869
Iteration 124/1000 | Loss: 0.00001869
Iteration 125/1000 | Loss: 0.00001869
Iteration 126/1000 | Loss: 0.00001869
Iteration 127/1000 | Loss: 0.00001869
Iteration 128/1000 | Loss: 0.00001868
Iteration 129/1000 | Loss: 0.00001868
Iteration 130/1000 | Loss: 0.00001868
Iteration 131/1000 | Loss: 0.00001868
Iteration 132/1000 | Loss: 0.00001868
Iteration 133/1000 | Loss: 0.00001868
Iteration 134/1000 | Loss: 0.00001868
Iteration 135/1000 | Loss: 0.00001868
Iteration 136/1000 | Loss: 0.00001868
Iteration 137/1000 | Loss: 0.00001868
Iteration 138/1000 | Loss: 0.00001868
Iteration 139/1000 | Loss: 0.00001868
Iteration 140/1000 | Loss: 0.00001867
Iteration 141/1000 | Loss: 0.00001867
Iteration 142/1000 | Loss: 0.00001867
Iteration 143/1000 | Loss: 0.00001867
Iteration 144/1000 | Loss: 0.00001867
Iteration 145/1000 | Loss: 0.00001867
Iteration 146/1000 | Loss: 0.00001867
Iteration 147/1000 | Loss: 0.00001867
Iteration 148/1000 | Loss: 0.00001867
Iteration 149/1000 | Loss: 0.00001867
Iteration 150/1000 | Loss: 0.00001867
Iteration 151/1000 | Loss: 0.00001866
Iteration 152/1000 | Loss: 0.00001866
Iteration 153/1000 | Loss: 0.00001866
Iteration 154/1000 | Loss: 0.00001866
Iteration 155/1000 | Loss: 0.00001866
Iteration 156/1000 | Loss: 0.00001866
Iteration 157/1000 | Loss: 0.00001866
Iteration 158/1000 | Loss: 0.00001866
Iteration 159/1000 | Loss: 0.00001866
Iteration 160/1000 | Loss: 0.00001866
Iteration 161/1000 | Loss: 0.00001866
Iteration 162/1000 | Loss: 0.00001865
Iteration 163/1000 | Loss: 0.00001865
Iteration 164/1000 | Loss: 0.00001865
Iteration 165/1000 | Loss: 0.00001864
Iteration 166/1000 | Loss: 0.00001864
Iteration 167/1000 | Loss: 0.00001864
Iteration 168/1000 | Loss: 0.00001864
Iteration 169/1000 | Loss: 0.00001864
Iteration 170/1000 | Loss: 0.00001864
Iteration 171/1000 | Loss: 0.00001864
Iteration 172/1000 | Loss: 0.00001864
Iteration 173/1000 | Loss: 0.00001863
Iteration 174/1000 | Loss: 0.00001863
Iteration 175/1000 | Loss: 0.00001863
Iteration 176/1000 | Loss: 0.00001863
Iteration 177/1000 | Loss: 0.00001863
Iteration 178/1000 | Loss: 0.00001863
Iteration 179/1000 | Loss: 0.00001863
Iteration 180/1000 | Loss: 0.00001863
Iteration 181/1000 | Loss: 0.00001863
Iteration 182/1000 | Loss: 0.00001863
Iteration 183/1000 | Loss: 0.00001863
Iteration 184/1000 | Loss: 0.00001863
Iteration 185/1000 | Loss: 0.00001863
Iteration 186/1000 | Loss: 0.00001863
Iteration 187/1000 | Loss: 0.00001863
Iteration 188/1000 | Loss: 0.00001863
Iteration 189/1000 | Loss: 0.00001863
Iteration 190/1000 | Loss: 0.00001863
Iteration 191/1000 | Loss: 0.00001863
Iteration 192/1000 | Loss: 0.00001863
Iteration 193/1000 | Loss: 0.00001863
Iteration 194/1000 | Loss: 0.00001863
Iteration 195/1000 | Loss: 0.00001863
Iteration 196/1000 | Loss: 0.00001863
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [1.8629552869242616e-05, 1.8629552869242616e-05, 1.8629552869242616e-05, 1.8629552869242616e-05, 1.8629552869242616e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8629552869242616e-05

Optimization complete. Final v2v error: 3.630685806274414 mm

Highest mean error: 3.9524552822113037 mm for frame 21

Lowest mean error: 3.3235199451446533 mm for frame 12

Saving results

Total time: 40.094343185424805
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00836856
Iteration 2/25 | Loss: 0.00188654
Iteration 3/25 | Loss: 0.00152809
Iteration 4/25 | Loss: 0.00168378
Iteration 5/25 | Loss: 0.00185530
Iteration 6/25 | Loss: 0.00134334
Iteration 7/25 | Loss: 0.00128296
Iteration 8/25 | Loss: 0.00128005
Iteration 9/25 | Loss: 0.00127947
Iteration 10/25 | Loss: 0.00128868
Iteration 11/25 | Loss: 0.00127844
Iteration 12/25 | Loss: 0.00127661
Iteration 13/25 | Loss: 0.00127643
Iteration 14/25 | Loss: 0.00127641
Iteration 15/25 | Loss: 0.00127640
Iteration 16/25 | Loss: 0.00127639
Iteration 17/25 | Loss: 0.00127639
Iteration 18/25 | Loss: 0.00127639
Iteration 19/25 | Loss: 0.00127639
Iteration 20/25 | Loss: 0.00127639
Iteration 21/25 | Loss: 0.00127639
Iteration 22/25 | Loss: 0.00127639
Iteration 23/25 | Loss: 0.00127638
Iteration 24/25 | Loss: 0.00127638
Iteration 25/25 | Loss: 0.00127638

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.10044432
Iteration 2/25 | Loss: 0.00075579
Iteration 3/25 | Loss: 0.00075579
Iteration 4/25 | Loss: 0.00075579
Iteration 5/25 | Loss: 0.00075579
Iteration 6/25 | Loss: 0.00075579
Iteration 7/25 | Loss: 0.00075579
Iteration 8/25 | Loss: 0.00075578
Iteration 9/25 | Loss: 0.00075578
Iteration 10/25 | Loss: 0.00075578
Iteration 11/25 | Loss: 0.00075578
Iteration 12/25 | Loss: 0.00075578
Iteration 13/25 | Loss: 0.00075578
Iteration 14/25 | Loss: 0.00075578
Iteration 15/25 | Loss: 0.00075578
Iteration 16/25 | Loss: 0.00075578
Iteration 17/25 | Loss: 0.00075578
Iteration 18/25 | Loss: 0.00075578
Iteration 19/25 | Loss: 0.00075578
Iteration 20/25 | Loss: 0.00075578
Iteration 21/25 | Loss: 0.00075578
Iteration 22/25 | Loss: 0.00075578
Iteration 23/25 | Loss: 0.00075578
Iteration 24/25 | Loss: 0.00075578
Iteration 25/25 | Loss: 0.00075578

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075578
Iteration 2/1000 | Loss: 0.00005928
Iteration 3/1000 | Loss: 0.00004826
Iteration 4/1000 | Loss: 0.00004544
Iteration 5/1000 | Loss: 0.00004415
Iteration 6/1000 | Loss: 0.00004328
Iteration 7/1000 | Loss: 0.00004267
Iteration 8/1000 | Loss: 0.00004208
Iteration 9/1000 | Loss: 0.00004185
Iteration 10/1000 | Loss: 0.00004164
Iteration 11/1000 | Loss: 0.00004147
Iteration 12/1000 | Loss: 0.00004128
Iteration 13/1000 | Loss: 0.00004120
Iteration 14/1000 | Loss: 0.00004120
Iteration 15/1000 | Loss: 0.00004119
Iteration 16/1000 | Loss: 0.00004119
Iteration 17/1000 | Loss: 0.00004099
Iteration 18/1000 | Loss: 0.00004096
Iteration 19/1000 | Loss: 0.00004094
Iteration 20/1000 | Loss: 0.00004092
Iteration 21/1000 | Loss: 0.00004092
Iteration 22/1000 | Loss: 0.00004092
Iteration 23/1000 | Loss: 0.00004091
Iteration 24/1000 | Loss: 0.00004091
Iteration 25/1000 | Loss: 0.00004091
Iteration 26/1000 | Loss: 0.00004091
Iteration 27/1000 | Loss: 0.00004090
Iteration 28/1000 | Loss: 0.00004090
Iteration 29/1000 | Loss: 0.00004090
Iteration 30/1000 | Loss: 0.00004089
Iteration 31/1000 | Loss: 0.00004089
Iteration 32/1000 | Loss: 0.00004088
Iteration 33/1000 | Loss: 0.00004088
Iteration 34/1000 | Loss: 0.00004088
Iteration 35/1000 | Loss: 0.00004088
Iteration 36/1000 | Loss: 0.00004088
Iteration 37/1000 | Loss: 0.00004088
Iteration 38/1000 | Loss: 0.00004088
Iteration 39/1000 | Loss: 0.00004088
Iteration 40/1000 | Loss: 0.00004087
Iteration 41/1000 | Loss: 0.00004087
Iteration 42/1000 | Loss: 0.00004087
Iteration 43/1000 | Loss: 0.00004086
Iteration 44/1000 | Loss: 0.00004085
Iteration 45/1000 | Loss: 0.00004085
Iteration 46/1000 | Loss: 0.00004085
Iteration 47/1000 | Loss: 0.00004085
Iteration 48/1000 | Loss: 0.00004085
Iteration 49/1000 | Loss: 0.00004085
Iteration 50/1000 | Loss: 0.00004085
Iteration 51/1000 | Loss: 0.00004085
Iteration 52/1000 | Loss: 0.00004085
Iteration 53/1000 | Loss: 0.00004085
Iteration 54/1000 | Loss: 0.00004084
Iteration 55/1000 | Loss: 0.00004084
Iteration 56/1000 | Loss: 0.00004082
Iteration 57/1000 | Loss: 0.00004082
Iteration 58/1000 | Loss: 0.00004082
Iteration 59/1000 | Loss: 0.00004082
Iteration 60/1000 | Loss: 0.00004082
Iteration 61/1000 | Loss: 0.00004082
Iteration 62/1000 | Loss: 0.00004082
Iteration 63/1000 | Loss: 0.00004081
Iteration 64/1000 | Loss: 0.00004081
Iteration 65/1000 | Loss: 0.00004080
Iteration 66/1000 | Loss: 0.00004080
Iteration 67/1000 | Loss: 0.00004080
Iteration 68/1000 | Loss: 0.00004079
Iteration 69/1000 | Loss: 0.00004079
Iteration 70/1000 | Loss: 0.00004072
Iteration 71/1000 | Loss: 0.00004071
Iteration 72/1000 | Loss: 0.00004071
Iteration 73/1000 | Loss: 0.00004070
Iteration 74/1000 | Loss: 0.00004070
Iteration 75/1000 | Loss: 0.00004069
Iteration 76/1000 | Loss: 0.00004069
Iteration 77/1000 | Loss: 0.00004069
Iteration 78/1000 | Loss: 0.00004068
Iteration 79/1000 | Loss: 0.00004068
Iteration 80/1000 | Loss: 0.00004068
Iteration 81/1000 | Loss: 0.00004068
Iteration 82/1000 | Loss: 0.00004068
Iteration 83/1000 | Loss: 0.00004067
Iteration 84/1000 | Loss: 0.00004067
Iteration 85/1000 | Loss: 0.00004067
Iteration 86/1000 | Loss: 0.00004067
Iteration 87/1000 | Loss: 0.00004067
Iteration 88/1000 | Loss: 0.00004067
Iteration 89/1000 | Loss: 0.00004067
Iteration 90/1000 | Loss: 0.00004067
Iteration 91/1000 | Loss: 0.00004067
Iteration 92/1000 | Loss: 0.00004067
Iteration 93/1000 | Loss: 0.00004067
Iteration 94/1000 | Loss: 0.00004066
Iteration 95/1000 | Loss: 0.00004066
Iteration 96/1000 | Loss: 0.00004066
Iteration 97/1000 | Loss: 0.00004065
Iteration 98/1000 | Loss: 0.00004065
Iteration 99/1000 | Loss: 0.00004065
Iteration 100/1000 | Loss: 0.00004065
Iteration 101/1000 | Loss: 0.00004063
Iteration 102/1000 | Loss: 0.00004063
Iteration 103/1000 | Loss: 0.00004061
Iteration 104/1000 | Loss: 0.00004060
Iteration 105/1000 | Loss: 0.00004060
Iteration 106/1000 | Loss: 0.00004060
Iteration 107/1000 | Loss: 0.00004059
Iteration 108/1000 | Loss: 0.00004059
Iteration 109/1000 | Loss: 0.00004059
Iteration 110/1000 | Loss: 0.00004059
Iteration 111/1000 | Loss: 0.00004059
Iteration 112/1000 | Loss: 0.00004059
Iteration 113/1000 | Loss: 0.00004059
Iteration 114/1000 | Loss: 0.00004059
Iteration 115/1000 | Loss: 0.00004058
Iteration 116/1000 | Loss: 0.00004058
Iteration 117/1000 | Loss: 0.00004058
Iteration 118/1000 | Loss: 0.00004058
Iteration 119/1000 | Loss: 0.00004058
Iteration 120/1000 | Loss: 0.00004058
Iteration 121/1000 | Loss: 0.00004058
Iteration 122/1000 | Loss: 0.00004058
Iteration 123/1000 | Loss: 0.00004057
Iteration 124/1000 | Loss: 0.00004057
Iteration 125/1000 | Loss: 0.00004057
Iteration 126/1000 | Loss: 0.00004057
Iteration 127/1000 | Loss: 0.00004057
Iteration 128/1000 | Loss: 0.00004056
Iteration 129/1000 | Loss: 0.00004056
Iteration 130/1000 | Loss: 0.00004056
Iteration 131/1000 | Loss: 0.00004056
Iteration 132/1000 | Loss: 0.00004056
Iteration 133/1000 | Loss: 0.00004056
Iteration 134/1000 | Loss: 0.00004056
Iteration 135/1000 | Loss: 0.00004055
Iteration 136/1000 | Loss: 0.00004055
Iteration 137/1000 | Loss: 0.00004055
Iteration 138/1000 | Loss: 0.00004055
Iteration 139/1000 | Loss: 0.00004055
Iteration 140/1000 | Loss: 0.00004055
Iteration 141/1000 | Loss: 0.00004055
Iteration 142/1000 | Loss: 0.00004055
Iteration 143/1000 | Loss: 0.00004055
Iteration 144/1000 | Loss: 0.00004055
Iteration 145/1000 | Loss: 0.00004055
Iteration 146/1000 | Loss: 0.00004055
Iteration 147/1000 | Loss: 0.00004055
Iteration 148/1000 | Loss: 0.00004055
Iteration 149/1000 | Loss: 0.00004055
Iteration 150/1000 | Loss: 0.00004055
Iteration 151/1000 | Loss: 0.00004055
Iteration 152/1000 | Loss: 0.00004055
Iteration 153/1000 | Loss: 0.00004055
Iteration 154/1000 | Loss: 0.00004055
Iteration 155/1000 | Loss: 0.00004055
Iteration 156/1000 | Loss: 0.00004055
Iteration 157/1000 | Loss: 0.00004055
Iteration 158/1000 | Loss: 0.00004055
Iteration 159/1000 | Loss: 0.00004055
Iteration 160/1000 | Loss: 0.00004055
Iteration 161/1000 | Loss: 0.00004055
Iteration 162/1000 | Loss: 0.00004055
Iteration 163/1000 | Loss: 0.00004055
Iteration 164/1000 | Loss: 0.00004055
Iteration 165/1000 | Loss: 0.00004055
Iteration 166/1000 | Loss: 0.00004055
Iteration 167/1000 | Loss: 0.00004055
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [4.0551160054747015e-05, 4.0551160054747015e-05, 4.0551160054747015e-05, 4.0551160054747015e-05, 4.0551160054747015e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.0551160054747015e-05

Optimization complete. Final v2v error: 5.085712909698486 mm

Highest mean error: 5.207411289215088 mm for frame 26

Lowest mean error: 5.01987361907959 mm for frame 100

Saving results

Total time: 53.47352957725525
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00501553
Iteration 2/25 | Loss: 0.00113074
Iteration 3/25 | Loss: 0.00104135
Iteration 4/25 | Loss: 0.00101865
Iteration 5/25 | Loss: 0.00101118
Iteration 6/25 | Loss: 0.00100940
Iteration 7/25 | Loss: 0.00100893
Iteration 8/25 | Loss: 0.00100893
Iteration 9/25 | Loss: 0.00100893
Iteration 10/25 | Loss: 0.00100893
Iteration 11/25 | Loss: 0.00100893
Iteration 12/25 | Loss: 0.00100893
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001008927938528359, 0.001008927938528359, 0.001008927938528359, 0.001008927938528359, 0.001008927938528359]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001008927938528359

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.71928835
Iteration 2/25 | Loss: 0.00068536
Iteration 3/25 | Loss: 0.00068536
Iteration 4/25 | Loss: 0.00068536
Iteration 5/25 | Loss: 0.00068536
Iteration 6/25 | Loss: 0.00068535
Iteration 7/25 | Loss: 0.00068535
Iteration 8/25 | Loss: 0.00068535
Iteration 9/25 | Loss: 0.00068535
Iteration 10/25 | Loss: 0.00068535
Iteration 11/25 | Loss: 0.00068535
Iteration 12/25 | Loss: 0.00068535
Iteration 13/25 | Loss: 0.00068535
Iteration 14/25 | Loss: 0.00068535
Iteration 15/25 | Loss: 0.00068535
Iteration 16/25 | Loss: 0.00068535
Iteration 17/25 | Loss: 0.00068535
Iteration 18/25 | Loss: 0.00068535
Iteration 19/25 | Loss: 0.00068535
Iteration 20/25 | Loss: 0.00068535
Iteration 21/25 | Loss: 0.00068535
Iteration 22/25 | Loss: 0.00068535
Iteration 23/25 | Loss: 0.00068535
Iteration 24/25 | Loss: 0.00068535
Iteration 25/25 | Loss: 0.00068535

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068535
Iteration 2/1000 | Loss: 0.00003118
Iteration 3/1000 | Loss: 0.00002220
Iteration 4/1000 | Loss: 0.00002007
Iteration 5/1000 | Loss: 0.00001894
Iteration 6/1000 | Loss: 0.00001831
Iteration 7/1000 | Loss: 0.00001779
Iteration 8/1000 | Loss: 0.00001739
Iteration 9/1000 | Loss: 0.00001712
Iteration 10/1000 | Loss: 0.00001691
Iteration 11/1000 | Loss: 0.00001690
Iteration 12/1000 | Loss: 0.00001673
Iteration 13/1000 | Loss: 0.00001664
Iteration 14/1000 | Loss: 0.00001659
Iteration 15/1000 | Loss: 0.00001658
Iteration 16/1000 | Loss: 0.00001658
Iteration 17/1000 | Loss: 0.00001656
Iteration 18/1000 | Loss: 0.00001654
Iteration 19/1000 | Loss: 0.00001648
Iteration 20/1000 | Loss: 0.00001639
Iteration 21/1000 | Loss: 0.00001639
Iteration 22/1000 | Loss: 0.00001635
Iteration 23/1000 | Loss: 0.00001635
Iteration 24/1000 | Loss: 0.00001635
Iteration 25/1000 | Loss: 0.00001635
Iteration 26/1000 | Loss: 0.00001635
Iteration 27/1000 | Loss: 0.00001635
Iteration 28/1000 | Loss: 0.00001634
Iteration 29/1000 | Loss: 0.00001634
Iteration 30/1000 | Loss: 0.00001633
Iteration 31/1000 | Loss: 0.00001633
Iteration 32/1000 | Loss: 0.00001632
Iteration 33/1000 | Loss: 0.00001632
Iteration 34/1000 | Loss: 0.00001631
Iteration 35/1000 | Loss: 0.00001631
Iteration 36/1000 | Loss: 0.00001631
Iteration 37/1000 | Loss: 0.00001631
Iteration 38/1000 | Loss: 0.00001630
Iteration 39/1000 | Loss: 0.00001630
Iteration 40/1000 | Loss: 0.00001630
Iteration 41/1000 | Loss: 0.00001630
Iteration 42/1000 | Loss: 0.00001629
Iteration 43/1000 | Loss: 0.00001629
Iteration 44/1000 | Loss: 0.00001629
Iteration 45/1000 | Loss: 0.00001629
Iteration 46/1000 | Loss: 0.00001628
Iteration 47/1000 | Loss: 0.00001628
Iteration 48/1000 | Loss: 0.00001628
Iteration 49/1000 | Loss: 0.00001627
Iteration 50/1000 | Loss: 0.00001627
Iteration 51/1000 | Loss: 0.00001627
Iteration 52/1000 | Loss: 0.00001627
Iteration 53/1000 | Loss: 0.00001627
Iteration 54/1000 | Loss: 0.00001627
Iteration 55/1000 | Loss: 0.00001627
Iteration 56/1000 | Loss: 0.00001627
Iteration 57/1000 | Loss: 0.00001626
Iteration 58/1000 | Loss: 0.00001626
Iteration 59/1000 | Loss: 0.00001626
Iteration 60/1000 | Loss: 0.00001626
Iteration 61/1000 | Loss: 0.00001626
Iteration 62/1000 | Loss: 0.00001626
Iteration 63/1000 | Loss: 0.00001626
Iteration 64/1000 | Loss: 0.00001626
Iteration 65/1000 | Loss: 0.00001626
Iteration 66/1000 | Loss: 0.00001625
Iteration 67/1000 | Loss: 0.00001625
Iteration 68/1000 | Loss: 0.00001625
Iteration 69/1000 | Loss: 0.00001625
Iteration 70/1000 | Loss: 0.00001625
Iteration 71/1000 | Loss: 0.00001624
Iteration 72/1000 | Loss: 0.00001624
Iteration 73/1000 | Loss: 0.00001624
Iteration 74/1000 | Loss: 0.00001623
Iteration 75/1000 | Loss: 0.00001623
Iteration 76/1000 | Loss: 0.00001623
Iteration 77/1000 | Loss: 0.00001623
Iteration 78/1000 | Loss: 0.00001623
Iteration 79/1000 | Loss: 0.00001623
Iteration 80/1000 | Loss: 0.00001623
Iteration 81/1000 | Loss: 0.00001623
Iteration 82/1000 | Loss: 0.00001623
Iteration 83/1000 | Loss: 0.00001623
Iteration 84/1000 | Loss: 0.00001622
Iteration 85/1000 | Loss: 0.00001622
Iteration 86/1000 | Loss: 0.00001622
Iteration 87/1000 | Loss: 0.00001622
Iteration 88/1000 | Loss: 0.00001622
Iteration 89/1000 | Loss: 0.00001622
Iteration 90/1000 | Loss: 0.00001622
Iteration 91/1000 | Loss: 0.00001622
Iteration 92/1000 | Loss: 0.00001622
Iteration 93/1000 | Loss: 0.00001621
Iteration 94/1000 | Loss: 0.00001621
Iteration 95/1000 | Loss: 0.00001621
Iteration 96/1000 | Loss: 0.00001620
Iteration 97/1000 | Loss: 0.00001620
Iteration 98/1000 | Loss: 0.00001620
Iteration 99/1000 | Loss: 0.00001620
Iteration 100/1000 | Loss: 0.00001619
Iteration 101/1000 | Loss: 0.00001619
Iteration 102/1000 | Loss: 0.00001619
Iteration 103/1000 | Loss: 0.00001619
Iteration 104/1000 | Loss: 0.00001619
Iteration 105/1000 | Loss: 0.00001619
Iteration 106/1000 | Loss: 0.00001619
Iteration 107/1000 | Loss: 0.00001619
Iteration 108/1000 | Loss: 0.00001618
Iteration 109/1000 | Loss: 0.00001618
Iteration 110/1000 | Loss: 0.00001618
Iteration 111/1000 | Loss: 0.00001618
Iteration 112/1000 | Loss: 0.00001618
Iteration 113/1000 | Loss: 0.00001618
Iteration 114/1000 | Loss: 0.00001618
Iteration 115/1000 | Loss: 0.00001618
Iteration 116/1000 | Loss: 0.00001617
Iteration 117/1000 | Loss: 0.00001617
Iteration 118/1000 | Loss: 0.00001617
Iteration 119/1000 | Loss: 0.00001617
Iteration 120/1000 | Loss: 0.00001617
Iteration 121/1000 | Loss: 0.00001616
Iteration 122/1000 | Loss: 0.00001616
Iteration 123/1000 | Loss: 0.00001616
Iteration 124/1000 | Loss: 0.00001616
Iteration 125/1000 | Loss: 0.00001616
Iteration 126/1000 | Loss: 0.00001616
Iteration 127/1000 | Loss: 0.00001616
Iteration 128/1000 | Loss: 0.00001616
Iteration 129/1000 | Loss: 0.00001616
Iteration 130/1000 | Loss: 0.00001616
Iteration 131/1000 | Loss: 0.00001616
Iteration 132/1000 | Loss: 0.00001616
Iteration 133/1000 | Loss: 0.00001615
Iteration 134/1000 | Loss: 0.00001615
Iteration 135/1000 | Loss: 0.00001615
Iteration 136/1000 | Loss: 0.00001615
Iteration 137/1000 | Loss: 0.00001615
Iteration 138/1000 | Loss: 0.00001615
Iteration 139/1000 | Loss: 0.00001614
Iteration 140/1000 | Loss: 0.00001614
Iteration 141/1000 | Loss: 0.00001614
Iteration 142/1000 | Loss: 0.00001613
Iteration 143/1000 | Loss: 0.00001613
Iteration 144/1000 | Loss: 0.00001613
Iteration 145/1000 | Loss: 0.00001613
Iteration 146/1000 | Loss: 0.00001613
Iteration 147/1000 | Loss: 0.00001613
Iteration 148/1000 | Loss: 0.00001613
Iteration 149/1000 | Loss: 0.00001613
Iteration 150/1000 | Loss: 0.00001613
Iteration 151/1000 | Loss: 0.00001613
Iteration 152/1000 | Loss: 0.00001613
Iteration 153/1000 | Loss: 0.00001613
Iteration 154/1000 | Loss: 0.00001613
Iteration 155/1000 | Loss: 0.00001613
Iteration 156/1000 | Loss: 0.00001613
Iteration 157/1000 | Loss: 0.00001613
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.6130905351019464e-05, 1.6130905351019464e-05, 1.6130905351019464e-05, 1.6130905351019464e-05, 1.6130905351019464e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6130905351019464e-05

Optimization complete. Final v2v error: 3.4106740951538086 mm

Highest mean error: 3.682471513748169 mm for frame 94

Lowest mean error: 3.0075840950012207 mm for frame 27

Saving results

Total time: 38.36866497993469
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00384800
Iteration 2/25 | Loss: 0.00101829
Iteration 3/25 | Loss: 0.00095002
Iteration 4/25 | Loss: 0.00094282
Iteration 5/25 | Loss: 0.00094131
Iteration 6/25 | Loss: 0.00094131
Iteration 7/25 | Loss: 0.00094131
Iteration 8/25 | Loss: 0.00094131
Iteration 9/25 | Loss: 0.00094131
Iteration 10/25 | Loss: 0.00094131
Iteration 11/25 | Loss: 0.00094131
Iteration 12/25 | Loss: 0.00094131
Iteration 13/25 | Loss: 0.00094131
Iteration 14/25 | Loss: 0.00094131
Iteration 15/25 | Loss: 0.00094131
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0009413109510205686, 0.0009413109510205686, 0.0009413109510205686, 0.0009413109510205686, 0.0009413109510205686]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009413109510205686

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.43533826
Iteration 2/25 | Loss: 0.00063851
Iteration 3/25 | Loss: 0.00063851
Iteration 4/25 | Loss: 0.00063851
Iteration 5/25 | Loss: 0.00063851
Iteration 6/25 | Loss: 0.00063850
Iteration 7/25 | Loss: 0.00063850
Iteration 8/25 | Loss: 0.00063850
Iteration 9/25 | Loss: 0.00063850
Iteration 10/25 | Loss: 0.00063850
Iteration 11/25 | Loss: 0.00063850
Iteration 12/25 | Loss: 0.00063850
Iteration 13/25 | Loss: 0.00063850
Iteration 14/25 | Loss: 0.00063850
Iteration 15/25 | Loss: 0.00063850
Iteration 16/25 | Loss: 0.00063850
Iteration 17/25 | Loss: 0.00063850
Iteration 18/25 | Loss: 0.00063850
Iteration 19/25 | Loss: 0.00063850
Iteration 20/25 | Loss: 0.00063850
Iteration 21/25 | Loss: 0.00063850
Iteration 22/25 | Loss: 0.00063850
Iteration 23/25 | Loss: 0.00063850
Iteration 24/25 | Loss: 0.00063850
Iteration 25/25 | Loss: 0.00063850

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063850
Iteration 2/1000 | Loss: 0.00001347
Iteration 3/1000 | Loss: 0.00001048
Iteration 4/1000 | Loss: 0.00000936
Iteration 5/1000 | Loss: 0.00000888
Iteration 6/1000 | Loss: 0.00000861
Iteration 7/1000 | Loss: 0.00000845
Iteration 8/1000 | Loss: 0.00000844
Iteration 9/1000 | Loss: 0.00000842
Iteration 10/1000 | Loss: 0.00000817
Iteration 11/1000 | Loss: 0.00000816
Iteration 12/1000 | Loss: 0.00000816
Iteration 13/1000 | Loss: 0.00000807
Iteration 14/1000 | Loss: 0.00000806
Iteration 15/1000 | Loss: 0.00000804
Iteration 16/1000 | Loss: 0.00000804
Iteration 17/1000 | Loss: 0.00000803
Iteration 18/1000 | Loss: 0.00000803
Iteration 19/1000 | Loss: 0.00000802
Iteration 20/1000 | Loss: 0.00000797
Iteration 21/1000 | Loss: 0.00000792
Iteration 22/1000 | Loss: 0.00000791
Iteration 23/1000 | Loss: 0.00000790
Iteration 24/1000 | Loss: 0.00000789
Iteration 25/1000 | Loss: 0.00000788
Iteration 26/1000 | Loss: 0.00000788
Iteration 27/1000 | Loss: 0.00000786
Iteration 28/1000 | Loss: 0.00000786
Iteration 29/1000 | Loss: 0.00000786
Iteration 30/1000 | Loss: 0.00000786
Iteration 31/1000 | Loss: 0.00000786
Iteration 32/1000 | Loss: 0.00000786
Iteration 33/1000 | Loss: 0.00000786
Iteration 34/1000 | Loss: 0.00000786
Iteration 35/1000 | Loss: 0.00000786
Iteration 36/1000 | Loss: 0.00000785
Iteration 37/1000 | Loss: 0.00000785
Iteration 38/1000 | Loss: 0.00000784
Iteration 39/1000 | Loss: 0.00000784
Iteration 40/1000 | Loss: 0.00000782
Iteration 41/1000 | Loss: 0.00000782
Iteration 42/1000 | Loss: 0.00000782
Iteration 43/1000 | Loss: 0.00000782
Iteration 44/1000 | Loss: 0.00000782
Iteration 45/1000 | Loss: 0.00000782
Iteration 46/1000 | Loss: 0.00000782
Iteration 47/1000 | Loss: 0.00000782
Iteration 48/1000 | Loss: 0.00000782
Iteration 49/1000 | Loss: 0.00000782
Iteration 50/1000 | Loss: 0.00000781
Iteration 51/1000 | Loss: 0.00000781
Iteration 52/1000 | Loss: 0.00000781
Iteration 53/1000 | Loss: 0.00000781
Iteration 54/1000 | Loss: 0.00000781
Iteration 55/1000 | Loss: 0.00000781
Iteration 56/1000 | Loss: 0.00000781
Iteration 57/1000 | Loss: 0.00000780
Iteration 58/1000 | Loss: 0.00000780
Iteration 59/1000 | Loss: 0.00000779
Iteration 60/1000 | Loss: 0.00000779
Iteration 61/1000 | Loss: 0.00000779
Iteration 62/1000 | Loss: 0.00000778
Iteration 63/1000 | Loss: 0.00000778
Iteration 64/1000 | Loss: 0.00000778
Iteration 65/1000 | Loss: 0.00000777
Iteration 66/1000 | Loss: 0.00000777
Iteration 67/1000 | Loss: 0.00000776
Iteration 68/1000 | Loss: 0.00000776
Iteration 69/1000 | Loss: 0.00000772
Iteration 70/1000 | Loss: 0.00000772
Iteration 71/1000 | Loss: 0.00000772
Iteration 72/1000 | Loss: 0.00000771
Iteration 73/1000 | Loss: 0.00000769
Iteration 74/1000 | Loss: 0.00000768
Iteration 75/1000 | Loss: 0.00000768
Iteration 76/1000 | Loss: 0.00000768
Iteration 77/1000 | Loss: 0.00000768
Iteration 78/1000 | Loss: 0.00000767
Iteration 79/1000 | Loss: 0.00000767
Iteration 80/1000 | Loss: 0.00000766
Iteration 81/1000 | Loss: 0.00000765
Iteration 82/1000 | Loss: 0.00000765
Iteration 83/1000 | Loss: 0.00000765
Iteration 84/1000 | Loss: 0.00000765
Iteration 85/1000 | Loss: 0.00000764
Iteration 86/1000 | Loss: 0.00000764
Iteration 87/1000 | Loss: 0.00000764
Iteration 88/1000 | Loss: 0.00000761
Iteration 89/1000 | Loss: 0.00000761
Iteration 90/1000 | Loss: 0.00000760
Iteration 91/1000 | Loss: 0.00000760
Iteration 92/1000 | Loss: 0.00000760
Iteration 93/1000 | Loss: 0.00000758
Iteration 94/1000 | Loss: 0.00000757
Iteration 95/1000 | Loss: 0.00000757
Iteration 96/1000 | Loss: 0.00000757
Iteration 97/1000 | Loss: 0.00000757
Iteration 98/1000 | Loss: 0.00000757
Iteration 99/1000 | Loss: 0.00000756
Iteration 100/1000 | Loss: 0.00000756
Iteration 101/1000 | Loss: 0.00000756
Iteration 102/1000 | Loss: 0.00000756
Iteration 103/1000 | Loss: 0.00000756
Iteration 104/1000 | Loss: 0.00000756
Iteration 105/1000 | Loss: 0.00000756
Iteration 106/1000 | Loss: 0.00000756
Iteration 107/1000 | Loss: 0.00000756
Iteration 108/1000 | Loss: 0.00000756
Iteration 109/1000 | Loss: 0.00000756
Iteration 110/1000 | Loss: 0.00000756
Iteration 111/1000 | Loss: 0.00000755
Iteration 112/1000 | Loss: 0.00000755
Iteration 113/1000 | Loss: 0.00000755
Iteration 114/1000 | Loss: 0.00000755
Iteration 115/1000 | Loss: 0.00000755
Iteration 116/1000 | Loss: 0.00000755
Iteration 117/1000 | Loss: 0.00000754
Iteration 118/1000 | Loss: 0.00000754
Iteration 119/1000 | Loss: 0.00000754
Iteration 120/1000 | Loss: 0.00000754
Iteration 121/1000 | Loss: 0.00000754
Iteration 122/1000 | Loss: 0.00000754
Iteration 123/1000 | Loss: 0.00000754
Iteration 124/1000 | Loss: 0.00000753
Iteration 125/1000 | Loss: 0.00000753
Iteration 126/1000 | Loss: 0.00000753
Iteration 127/1000 | Loss: 0.00000753
Iteration 128/1000 | Loss: 0.00000753
Iteration 129/1000 | Loss: 0.00000753
Iteration 130/1000 | Loss: 0.00000752
Iteration 131/1000 | Loss: 0.00000752
Iteration 132/1000 | Loss: 0.00000752
Iteration 133/1000 | Loss: 0.00000752
Iteration 134/1000 | Loss: 0.00000751
Iteration 135/1000 | Loss: 0.00000751
Iteration 136/1000 | Loss: 0.00000751
Iteration 137/1000 | Loss: 0.00000751
Iteration 138/1000 | Loss: 0.00000751
Iteration 139/1000 | Loss: 0.00000751
Iteration 140/1000 | Loss: 0.00000751
Iteration 141/1000 | Loss: 0.00000750
Iteration 142/1000 | Loss: 0.00000750
Iteration 143/1000 | Loss: 0.00000750
Iteration 144/1000 | Loss: 0.00000750
Iteration 145/1000 | Loss: 0.00000749
Iteration 146/1000 | Loss: 0.00000749
Iteration 147/1000 | Loss: 0.00000749
Iteration 148/1000 | Loss: 0.00000749
Iteration 149/1000 | Loss: 0.00000749
Iteration 150/1000 | Loss: 0.00000749
Iteration 151/1000 | Loss: 0.00000749
Iteration 152/1000 | Loss: 0.00000749
Iteration 153/1000 | Loss: 0.00000749
Iteration 154/1000 | Loss: 0.00000749
Iteration 155/1000 | Loss: 0.00000749
Iteration 156/1000 | Loss: 0.00000749
Iteration 157/1000 | Loss: 0.00000749
Iteration 158/1000 | Loss: 0.00000749
Iteration 159/1000 | Loss: 0.00000749
Iteration 160/1000 | Loss: 0.00000749
Iteration 161/1000 | Loss: 0.00000749
Iteration 162/1000 | Loss: 0.00000749
Iteration 163/1000 | Loss: 0.00000749
Iteration 164/1000 | Loss: 0.00000749
Iteration 165/1000 | Loss: 0.00000749
Iteration 166/1000 | Loss: 0.00000749
Iteration 167/1000 | Loss: 0.00000749
Iteration 168/1000 | Loss: 0.00000749
Iteration 169/1000 | Loss: 0.00000749
Iteration 170/1000 | Loss: 0.00000749
Iteration 171/1000 | Loss: 0.00000749
Iteration 172/1000 | Loss: 0.00000749
Iteration 173/1000 | Loss: 0.00000749
Iteration 174/1000 | Loss: 0.00000749
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [7.493750672438182e-06, 7.493750672438182e-06, 7.493750672438182e-06, 7.493750672438182e-06, 7.493750672438182e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.493750672438182e-06

Optimization complete. Final v2v error: 2.378039598464966 mm

Highest mean error: 2.560520887374878 mm for frame 126

Lowest mean error: 2.265937328338623 mm for frame 2

Saving results

Total time: 37.16872692108154
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00797228
Iteration 2/25 | Loss: 0.00109669
Iteration 3/25 | Loss: 0.00097067
Iteration 4/25 | Loss: 0.00096244
Iteration 5/25 | Loss: 0.00095983
Iteration 6/25 | Loss: 0.00095979
Iteration 7/25 | Loss: 0.00095979
Iteration 8/25 | Loss: 0.00095979
Iteration 9/25 | Loss: 0.00095979
Iteration 10/25 | Loss: 0.00095979
Iteration 11/25 | Loss: 0.00095979
Iteration 12/25 | Loss: 0.00095979
Iteration 13/25 | Loss: 0.00095979
Iteration 14/25 | Loss: 0.00095979
Iteration 15/25 | Loss: 0.00095979
Iteration 16/25 | Loss: 0.00095979
Iteration 17/25 | Loss: 0.00095979
Iteration 18/25 | Loss: 0.00095979
Iteration 19/25 | Loss: 0.00095979
Iteration 20/25 | Loss: 0.00095979
Iteration 21/25 | Loss: 0.00095979
Iteration 22/25 | Loss: 0.00095979
Iteration 23/25 | Loss: 0.00095979
Iteration 24/25 | Loss: 0.00095979
Iteration 25/25 | Loss: 0.00095979

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39234900
Iteration 2/25 | Loss: 0.00062390
Iteration 3/25 | Loss: 0.00062390
Iteration 4/25 | Loss: 0.00062390
Iteration 5/25 | Loss: 0.00062390
Iteration 6/25 | Loss: 0.00062390
Iteration 7/25 | Loss: 0.00062390
Iteration 8/25 | Loss: 0.00062390
Iteration 9/25 | Loss: 0.00062390
Iteration 10/25 | Loss: 0.00062390
Iteration 11/25 | Loss: 0.00062390
Iteration 12/25 | Loss: 0.00062390
Iteration 13/25 | Loss: 0.00062390
Iteration 14/25 | Loss: 0.00062390
Iteration 15/25 | Loss: 0.00062390
Iteration 16/25 | Loss: 0.00062390
Iteration 17/25 | Loss: 0.00062390
Iteration 18/25 | Loss: 0.00062390
Iteration 19/25 | Loss: 0.00062390
Iteration 20/25 | Loss: 0.00062390
Iteration 21/25 | Loss: 0.00062390
Iteration 22/25 | Loss: 0.00062390
Iteration 23/25 | Loss: 0.00062390
Iteration 24/25 | Loss: 0.00062390
Iteration 25/25 | Loss: 0.00062390

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062390
Iteration 2/1000 | Loss: 0.00002042
Iteration 3/1000 | Loss: 0.00001254
Iteration 4/1000 | Loss: 0.00001025
Iteration 5/1000 | Loss: 0.00000943
Iteration 6/1000 | Loss: 0.00000895
Iteration 7/1000 | Loss: 0.00000861
Iteration 8/1000 | Loss: 0.00000835
Iteration 9/1000 | Loss: 0.00000833
Iteration 10/1000 | Loss: 0.00000824
Iteration 11/1000 | Loss: 0.00000810
Iteration 12/1000 | Loss: 0.00000806
Iteration 13/1000 | Loss: 0.00000803
Iteration 14/1000 | Loss: 0.00000802
Iteration 15/1000 | Loss: 0.00000802
Iteration 16/1000 | Loss: 0.00000802
Iteration 17/1000 | Loss: 0.00000801
Iteration 18/1000 | Loss: 0.00000800
Iteration 19/1000 | Loss: 0.00000800
Iteration 20/1000 | Loss: 0.00000799
Iteration 21/1000 | Loss: 0.00000792
Iteration 22/1000 | Loss: 0.00000792
Iteration 23/1000 | Loss: 0.00000791
Iteration 24/1000 | Loss: 0.00000791
Iteration 25/1000 | Loss: 0.00000790
Iteration 26/1000 | Loss: 0.00000788
Iteration 27/1000 | Loss: 0.00000788
Iteration 28/1000 | Loss: 0.00000788
Iteration 29/1000 | Loss: 0.00000788
Iteration 30/1000 | Loss: 0.00000788
Iteration 31/1000 | Loss: 0.00000788
Iteration 32/1000 | Loss: 0.00000788
Iteration 33/1000 | Loss: 0.00000787
Iteration 34/1000 | Loss: 0.00000787
Iteration 35/1000 | Loss: 0.00000787
Iteration 36/1000 | Loss: 0.00000787
Iteration 37/1000 | Loss: 0.00000786
Iteration 38/1000 | Loss: 0.00000785
Iteration 39/1000 | Loss: 0.00000784
Iteration 40/1000 | Loss: 0.00000784
Iteration 41/1000 | Loss: 0.00000784
Iteration 42/1000 | Loss: 0.00000783
Iteration 43/1000 | Loss: 0.00000783
Iteration 44/1000 | Loss: 0.00000782
Iteration 45/1000 | Loss: 0.00000782
Iteration 46/1000 | Loss: 0.00000781
Iteration 47/1000 | Loss: 0.00000781
Iteration 48/1000 | Loss: 0.00000781
Iteration 49/1000 | Loss: 0.00000780
Iteration 50/1000 | Loss: 0.00000780
Iteration 51/1000 | Loss: 0.00000780
Iteration 52/1000 | Loss: 0.00000780
Iteration 53/1000 | Loss: 0.00000779
Iteration 54/1000 | Loss: 0.00000779
Iteration 55/1000 | Loss: 0.00000779
Iteration 56/1000 | Loss: 0.00000779
Iteration 57/1000 | Loss: 0.00000779
Iteration 58/1000 | Loss: 0.00000778
Iteration 59/1000 | Loss: 0.00000778
Iteration 60/1000 | Loss: 0.00000778
Iteration 61/1000 | Loss: 0.00000778
Iteration 62/1000 | Loss: 0.00000778
Iteration 63/1000 | Loss: 0.00000778
Iteration 64/1000 | Loss: 0.00000778
Iteration 65/1000 | Loss: 0.00000777
Iteration 66/1000 | Loss: 0.00000777
Iteration 67/1000 | Loss: 0.00000777
Iteration 68/1000 | Loss: 0.00000776
Iteration 69/1000 | Loss: 0.00000776
Iteration 70/1000 | Loss: 0.00000776
Iteration 71/1000 | Loss: 0.00000776
Iteration 72/1000 | Loss: 0.00000775
Iteration 73/1000 | Loss: 0.00000775
Iteration 74/1000 | Loss: 0.00000775
Iteration 75/1000 | Loss: 0.00000774
Iteration 76/1000 | Loss: 0.00000774
Iteration 77/1000 | Loss: 0.00000774
Iteration 78/1000 | Loss: 0.00000774
Iteration 79/1000 | Loss: 0.00000773
Iteration 80/1000 | Loss: 0.00000773
Iteration 81/1000 | Loss: 0.00000773
Iteration 82/1000 | Loss: 0.00000773
Iteration 83/1000 | Loss: 0.00000773
Iteration 84/1000 | Loss: 0.00000773
Iteration 85/1000 | Loss: 0.00000773
Iteration 86/1000 | Loss: 0.00000773
Iteration 87/1000 | Loss: 0.00000772
Iteration 88/1000 | Loss: 0.00000772
Iteration 89/1000 | Loss: 0.00000772
Iteration 90/1000 | Loss: 0.00000772
Iteration 91/1000 | Loss: 0.00000772
Iteration 92/1000 | Loss: 0.00000772
Iteration 93/1000 | Loss: 0.00000772
Iteration 94/1000 | Loss: 0.00000772
Iteration 95/1000 | Loss: 0.00000772
Iteration 96/1000 | Loss: 0.00000772
Iteration 97/1000 | Loss: 0.00000771
Iteration 98/1000 | Loss: 0.00000771
Iteration 99/1000 | Loss: 0.00000771
Iteration 100/1000 | Loss: 0.00000770
Iteration 101/1000 | Loss: 0.00000770
Iteration 102/1000 | Loss: 0.00000770
Iteration 103/1000 | Loss: 0.00000770
Iteration 104/1000 | Loss: 0.00000768
Iteration 105/1000 | Loss: 0.00000768
Iteration 106/1000 | Loss: 0.00000768
Iteration 107/1000 | Loss: 0.00000768
Iteration 108/1000 | Loss: 0.00000767
Iteration 109/1000 | Loss: 0.00000767
Iteration 110/1000 | Loss: 0.00000767
Iteration 111/1000 | Loss: 0.00000767
Iteration 112/1000 | Loss: 0.00000767
Iteration 113/1000 | Loss: 0.00000767
Iteration 114/1000 | Loss: 0.00000767
Iteration 115/1000 | Loss: 0.00000767
Iteration 116/1000 | Loss: 0.00000767
Iteration 117/1000 | Loss: 0.00000766
Iteration 118/1000 | Loss: 0.00000766
Iteration 119/1000 | Loss: 0.00000766
Iteration 120/1000 | Loss: 0.00000766
Iteration 121/1000 | Loss: 0.00000766
Iteration 122/1000 | Loss: 0.00000766
Iteration 123/1000 | Loss: 0.00000765
Iteration 124/1000 | Loss: 0.00000765
Iteration 125/1000 | Loss: 0.00000765
Iteration 126/1000 | Loss: 0.00000765
Iteration 127/1000 | Loss: 0.00000765
Iteration 128/1000 | Loss: 0.00000765
Iteration 129/1000 | Loss: 0.00000765
Iteration 130/1000 | Loss: 0.00000765
Iteration 131/1000 | Loss: 0.00000765
Iteration 132/1000 | Loss: 0.00000765
Iteration 133/1000 | Loss: 0.00000765
Iteration 134/1000 | Loss: 0.00000765
Iteration 135/1000 | Loss: 0.00000765
Iteration 136/1000 | Loss: 0.00000764
Iteration 137/1000 | Loss: 0.00000764
Iteration 138/1000 | Loss: 0.00000764
Iteration 139/1000 | Loss: 0.00000764
Iteration 140/1000 | Loss: 0.00000764
Iteration 141/1000 | Loss: 0.00000764
Iteration 142/1000 | Loss: 0.00000764
Iteration 143/1000 | Loss: 0.00000764
Iteration 144/1000 | Loss: 0.00000763
Iteration 145/1000 | Loss: 0.00000763
Iteration 146/1000 | Loss: 0.00000763
Iteration 147/1000 | Loss: 0.00000763
Iteration 148/1000 | Loss: 0.00000763
Iteration 149/1000 | Loss: 0.00000763
Iteration 150/1000 | Loss: 0.00000762
Iteration 151/1000 | Loss: 0.00000762
Iteration 152/1000 | Loss: 0.00000762
Iteration 153/1000 | Loss: 0.00000762
Iteration 154/1000 | Loss: 0.00000762
Iteration 155/1000 | Loss: 0.00000761
Iteration 156/1000 | Loss: 0.00000761
Iteration 157/1000 | Loss: 0.00000761
Iteration 158/1000 | Loss: 0.00000760
Iteration 159/1000 | Loss: 0.00000760
Iteration 160/1000 | Loss: 0.00000760
Iteration 161/1000 | Loss: 0.00000760
Iteration 162/1000 | Loss: 0.00000759
Iteration 163/1000 | Loss: 0.00000759
Iteration 164/1000 | Loss: 0.00000759
Iteration 165/1000 | Loss: 0.00000759
Iteration 166/1000 | Loss: 0.00000759
Iteration 167/1000 | Loss: 0.00000759
Iteration 168/1000 | Loss: 0.00000759
Iteration 169/1000 | Loss: 0.00000759
Iteration 170/1000 | Loss: 0.00000758
Iteration 171/1000 | Loss: 0.00000758
Iteration 172/1000 | Loss: 0.00000758
Iteration 173/1000 | Loss: 0.00000758
Iteration 174/1000 | Loss: 0.00000757
Iteration 175/1000 | Loss: 0.00000757
Iteration 176/1000 | Loss: 0.00000757
Iteration 177/1000 | Loss: 0.00000757
Iteration 178/1000 | Loss: 0.00000757
Iteration 179/1000 | Loss: 0.00000757
Iteration 180/1000 | Loss: 0.00000757
Iteration 181/1000 | Loss: 0.00000757
Iteration 182/1000 | Loss: 0.00000757
Iteration 183/1000 | Loss: 0.00000757
Iteration 184/1000 | Loss: 0.00000757
Iteration 185/1000 | Loss: 0.00000756
Iteration 186/1000 | Loss: 0.00000756
Iteration 187/1000 | Loss: 0.00000756
Iteration 188/1000 | Loss: 0.00000756
Iteration 189/1000 | Loss: 0.00000756
Iteration 190/1000 | Loss: 0.00000755
Iteration 191/1000 | Loss: 0.00000755
Iteration 192/1000 | Loss: 0.00000755
Iteration 193/1000 | Loss: 0.00000755
Iteration 194/1000 | Loss: 0.00000755
Iteration 195/1000 | Loss: 0.00000755
Iteration 196/1000 | Loss: 0.00000755
Iteration 197/1000 | Loss: 0.00000755
Iteration 198/1000 | Loss: 0.00000755
Iteration 199/1000 | Loss: 0.00000755
Iteration 200/1000 | Loss: 0.00000755
Iteration 201/1000 | Loss: 0.00000754
Iteration 202/1000 | Loss: 0.00000754
Iteration 203/1000 | Loss: 0.00000754
Iteration 204/1000 | Loss: 0.00000754
Iteration 205/1000 | Loss: 0.00000754
Iteration 206/1000 | Loss: 0.00000754
Iteration 207/1000 | Loss: 0.00000754
Iteration 208/1000 | Loss: 0.00000754
Iteration 209/1000 | Loss: 0.00000754
Iteration 210/1000 | Loss: 0.00000754
Iteration 211/1000 | Loss: 0.00000754
Iteration 212/1000 | Loss: 0.00000754
Iteration 213/1000 | Loss: 0.00000754
Iteration 214/1000 | Loss: 0.00000754
Iteration 215/1000 | Loss: 0.00000754
Iteration 216/1000 | Loss: 0.00000754
Iteration 217/1000 | Loss: 0.00000754
Iteration 218/1000 | Loss: 0.00000753
Iteration 219/1000 | Loss: 0.00000753
Iteration 220/1000 | Loss: 0.00000753
Iteration 221/1000 | Loss: 0.00000753
Iteration 222/1000 | Loss: 0.00000753
Iteration 223/1000 | Loss: 0.00000753
Iteration 224/1000 | Loss: 0.00000753
Iteration 225/1000 | Loss: 0.00000753
Iteration 226/1000 | Loss: 0.00000753
Iteration 227/1000 | Loss: 0.00000753
Iteration 228/1000 | Loss: 0.00000753
Iteration 229/1000 | Loss: 0.00000753
Iteration 230/1000 | Loss: 0.00000753
Iteration 231/1000 | Loss: 0.00000753
Iteration 232/1000 | Loss: 0.00000753
Iteration 233/1000 | Loss: 0.00000753
Iteration 234/1000 | Loss: 0.00000753
Iteration 235/1000 | Loss: 0.00000753
Iteration 236/1000 | Loss: 0.00000753
Iteration 237/1000 | Loss: 0.00000753
Iteration 238/1000 | Loss: 0.00000753
Iteration 239/1000 | Loss: 0.00000753
Iteration 240/1000 | Loss: 0.00000753
Iteration 241/1000 | Loss: 0.00000753
Iteration 242/1000 | Loss: 0.00000753
Iteration 243/1000 | Loss: 0.00000753
Iteration 244/1000 | Loss: 0.00000753
Iteration 245/1000 | Loss: 0.00000753
Iteration 246/1000 | Loss: 0.00000753
Iteration 247/1000 | Loss: 0.00000753
Iteration 248/1000 | Loss: 0.00000753
Iteration 249/1000 | Loss: 0.00000753
Iteration 250/1000 | Loss: 0.00000753
Iteration 251/1000 | Loss: 0.00000753
Iteration 252/1000 | Loss: 0.00000753
Iteration 253/1000 | Loss: 0.00000753
Iteration 254/1000 | Loss: 0.00000753
Iteration 255/1000 | Loss: 0.00000753
Iteration 256/1000 | Loss: 0.00000753
Iteration 257/1000 | Loss: 0.00000753
Iteration 258/1000 | Loss: 0.00000753
Iteration 259/1000 | Loss: 0.00000753
Iteration 260/1000 | Loss: 0.00000753
Iteration 261/1000 | Loss: 0.00000753
Iteration 262/1000 | Loss: 0.00000753
Iteration 263/1000 | Loss: 0.00000753
Iteration 264/1000 | Loss: 0.00000753
Iteration 265/1000 | Loss: 0.00000753
Iteration 266/1000 | Loss: 0.00000753
Iteration 267/1000 | Loss: 0.00000753
Iteration 268/1000 | Loss: 0.00000753
Iteration 269/1000 | Loss: 0.00000753
Iteration 270/1000 | Loss: 0.00000753
Iteration 271/1000 | Loss: 0.00000753
Iteration 272/1000 | Loss: 0.00000753
Iteration 273/1000 | Loss: 0.00000753
Iteration 274/1000 | Loss: 0.00000753
Iteration 275/1000 | Loss: 0.00000753
Iteration 276/1000 | Loss: 0.00000753
Iteration 277/1000 | Loss: 0.00000753
Iteration 278/1000 | Loss: 0.00000753
Iteration 279/1000 | Loss: 0.00000753
Iteration 280/1000 | Loss: 0.00000753
Iteration 281/1000 | Loss: 0.00000753
Iteration 282/1000 | Loss: 0.00000753
Iteration 283/1000 | Loss: 0.00000753
Iteration 284/1000 | Loss: 0.00000753
Iteration 285/1000 | Loss: 0.00000753
Iteration 286/1000 | Loss: 0.00000753
Iteration 287/1000 | Loss: 0.00000753
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 287. Stopping optimization.
Last 5 losses: [7.528452897531679e-06, 7.528452897531679e-06, 7.528452897531679e-06, 7.528452897531679e-06, 7.528452897531679e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.528452897531679e-06

Optimization complete. Final v2v error: 2.3334872722625732 mm

Highest mean error: 2.566920757293701 mm for frame 44

Lowest mean error: 2.17732834815979 mm for frame 162

Saving results

Total time: 40.44769048690796
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01035722
Iteration 2/25 | Loss: 0.00181602
Iteration 3/25 | Loss: 0.00161298
Iteration 4/25 | Loss: 0.00104636
Iteration 5/25 | Loss: 0.00102433
Iteration 6/25 | Loss: 0.00102156
Iteration 7/25 | Loss: 0.00101098
Iteration 8/25 | Loss: 0.00101770
Iteration 9/25 | Loss: 0.00100648
Iteration 10/25 | Loss: 0.00100392
Iteration 11/25 | Loss: 0.00100298
Iteration 12/25 | Loss: 0.00100223
Iteration 13/25 | Loss: 0.00100204
Iteration 14/25 | Loss: 0.00100201
Iteration 15/25 | Loss: 0.00100201
Iteration 16/25 | Loss: 0.00100201
Iteration 17/25 | Loss: 0.00100200
Iteration 18/25 | Loss: 0.00100200
Iteration 19/25 | Loss: 0.00100200
Iteration 20/25 | Loss: 0.00100200
Iteration 21/25 | Loss: 0.00100200
Iteration 22/25 | Loss: 0.00100200
Iteration 23/25 | Loss: 0.00100200
Iteration 24/25 | Loss: 0.00100200
Iteration 25/25 | Loss: 0.00100200

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34163952
Iteration 2/25 | Loss: 0.00054448
Iteration 3/25 | Loss: 0.00054448
Iteration 4/25 | Loss: 0.00054448
Iteration 5/25 | Loss: 0.00054448
Iteration 6/25 | Loss: 0.00054448
Iteration 7/25 | Loss: 0.00054448
Iteration 8/25 | Loss: 0.00054448
Iteration 9/25 | Loss: 0.00054448
Iteration 10/25 | Loss: 0.00054448
Iteration 11/25 | Loss: 0.00054448
Iteration 12/25 | Loss: 0.00054448
Iteration 13/25 | Loss: 0.00054448
Iteration 14/25 | Loss: 0.00054448
Iteration 15/25 | Loss: 0.00054448
Iteration 16/25 | Loss: 0.00054448
Iteration 17/25 | Loss: 0.00054448
Iteration 18/25 | Loss: 0.00054448
Iteration 19/25 | Loss: 0.00054448
Iteration 20/25 | Loss: 0.00054448
Iteration 21/25 | Loss: 0.00054448
Iteration 22/25 | Loss: 0.00054448
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0005444752750918269, 0.0005444752750918269, 0.0005444752750918269, 0.0005444752750918269, 0.0005444752750918269]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005444752750918269

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00054448
Iteration 2/1000 | Loss: 0.00003258
Iteration 3/1000 | Loss: 0.00001738
Iteration 4/1000 | Loss: 0.00001458
Iteration 5/1000 | Loss: 0.00001339
Iteration 6/1000 | Loss: 0.00001255
Iteration 7/1000 | Loss: 0.00001197
Iteration 8/1000 | Loss: 0.00001153
Iteration 9/1000 | Loss: 0.00001112
Iteration 10/1000 | Loss: 0.00001080
Iteration 11/1000 | Loss: 0.00001059
Iteration 12/1000 | Loss: 0.00001058
Iteration 13/1000 | Loss: 0.00001040
Iteration 14/1000 | Loss: 0.00001039
Iteration 15/1000 | Loss: 0.00001036
Iteration 16/1000 | Loss: 0.00001036
Iteration 17/1000 | Loss: 0.00001034
Iteration 18/1000 | Loss: 0.00001033
Iteration 19/1000 | Loss: 0.00001027
Iteration 20/1000 | Loss: 0.00001027
Iteration 21/1000 | Loss: 0.00001027
Iteration 22/1000 | Loss: 0.00001027
Iteration 23/1000 | Loss: 0.00001027
Iteration 24/1000 | Loss: 0.00001027
Iteration 25/1000 | Loss: 0.00001027
Iteration 26/1000 | Loss: 0.00001026
Iteration 27/1000 | Loss: 0.00001026
Iteration 28/1000 | Loss: 0.00001026
Iteration 29/1000 | Loss: 0.00001026
Iteration 30/1000 | Loss: 0.00001025
Iteration 31/1000 | Loss: 0.00001024
Iteration 32/1000 | Loss: 0.00001024
Iteration 33/1000 | Loss: 0.00001022
Iteration 34/1000 | Loss: 0.00001021
Iteration 35/1000 | Loss: 0.00001020
Iteration 36/1000 | Loss: 0.00001020
Iteration 37/1000 | Loss: 0.00001019
Iteration 38/1000 | Loss: 0.00001019
Iteration 39/1000 | Loss: 0.00001019
Iteration 40/1000 | Loss: 0.00001019
Iteration 41/1000 | Loss: 0.00001018
Iteration 42/1000 | Loss: 0.00001018
Iteration 43/1000 | Loss: 0.00001018
Iteration 44/1000 | Loss: 0.00001018
Iteration 45/1000 | Loss: 0.00001017
Iteration 46/1000 | Loss: 0.00001016
Iteration 47/1000 | Loss: 0.00001015
Iteration 48/1000 | Loss: 0.00001015
Iteration 49/1000 | Loss: 0.00001014
Iteration 50/1000 | Loss: 0.00001013
Iteration 51/1000 | Loss: 0.00001012
Iteration 52/1000 | Loss: 0.00001012
Iteration 53/1000 | Loss: 0.00001011
Iteration 54/1000 | Loss: 0.00001011
Iteration 55/1000 | Loss: 0.00001011
Iteration 56/1000 | Loss: 0.00001011
Iteration 57/1000 | Loss: 0.00001011
Iteration 58/1000 | Loss: 0.00001010
Iteration 59/1000 | Loss: 0.00001010
Iteration 60/1000 | Loss: 0.00001010
Iteration 61/1000 | Loss: 0.00001010
Iteration 62/1000 | Loss: 0.00001010
Iteration 63/1000 | Loss: 0.00001010
Iteration 64/1000 | Loss: 0.00001010
Iteration 65/1000 | Loss: 0.00001010
Iteration 66/1000 | Loss: 0.00001010
Iteration 67/1000 | Loss: 0.00001010
Iteration 68/1000 | Loss: 0.00001010
Iteration 69/1000 | Loss: 0.00001010
Iteration 70/1000 | Loss: 0.00001009
Iteration 71/1000 | Loss: 0.00001009
Iteration 72/1000 | Loss: 0.00001009
Iteration 73/1000 | Loss: 0.00001009
Iteration 74/1000 | Loss: 0.00001009
Iteration 75/1000 | Loss: 0.00001009
Iteration 76/1000 | Loss: 0.00001009
Iteration 77/1000 | Loss: 0.00001008
Iteration 78/1000 | Loss: 0.00001008
Iteration 79/1000 | Loss: 0.00001007
Iteration 80/1000 | Loss: 0.00001007
Iteration 81/1000 | Loss: 0.00001007
Iteration 82/1000 | Loss: 0.00001006
Iteration 83/1000 | Loss: 0.00001006
Iteration 84/1000 | Loss: 0.00001006
Iteration 85/1000 | Loss: 0.00001006
Iteration 86/1000 | Loss: 0.00001006
Iteration 87/1000 | Loss: 0.00001006
Iteration 88/1000 | Loss: 0.00001006
Iteration 89/1000 | Loss: 0.00001006
Iteration 90/1000 | Loss: 0.00001006
Iteration 91/1000 | Loss: 0.00001006
Iteration 92/1000 | Loss: 0.00001006
Iteration 93/1000 | Loss: 0.00001006
Iteration 94/1000 | Loss: 0.00001006
Iteration 95/1000 | Loss: 0.00001006
Iteration 96/1000 | Loss: 0.00001005
Iteration 97/1000 | Loss: 0.00001005
Iteration 98/1000 | Loss: 0.00001004
Iteration 99/1000 | Loss: 0.00001004
Iteration 100/1000 | Loss: 0.00001004
Iteration 101/1000 | Loss: 0.00001004
Iteration 102/1000 | Loss: 0.00001003
Iteration 103/1000 | Loss: 0.00001003
Iteration 104/1000 | Loss: 0.00001003
Iteration 105/1000 | Loss: 0.00001003
Iteration 106/1000 | Loss: 0.00001003
Iteration 107/1000 | Loss: 0.00001003
Iteration 108/1000 | Loss: 0.00001003
Iteration 109/1000 | Loss: 0.00001003
Iteration 110/1000 | Loss: 0.00001003
Iteration 111/1000 | Loss: 0.00001003
Iteration 112/1000 | Loss: 0.00001003
Iteration 113/1000 | Loss: 0.00001003
Iteration 114/1000 | Loss: 0.00001002
Iteration 115/1000 | Loss: 0.00001002
Iteration 116/1000 | Loss: 0.00001002
Iteration 117/1000 | Loss: 0.00001002
Iteration 118/1000 | Loss: 0.00001002
Iteration 119/1000 | Loss: 0.00001002
Iteration 120/1000 | Loss: 0.00001002
Iteration 121/1000 | Loss: 0.00001002
Iteration 122/1000 | Loss: 0.00001002
Iteration 123/1000 | Loss: 0.00001002
Iteration 124/1000 | Loss: 0.00001002
Iteration 125/1000 | Loss: 0.00001002
Iteration 126/1000 | Loss: 0.00001002
Iteration 127/1000 | Loss: 0.00001002
Iteration 128/1000 | Loss: 0.00001002
Iteration 129/1000 | Loss: 0.00001002
Iteration 130/1000 | Loss: 0.00001002
Iteration 131/1000 | Loss: 0.00001002
Iteration 132/1000 | Loss: 0.00001002
Iteration 133/1000 | Loss: 0.00001002
Iteration 134/1000 | Loss: 0.00001002
Iteration 135/1000 | Loss: 0.00001002
Iteration 136/1000 | Loss: 0.00001002
Iteration 137/1000 | Loss: 0.00001002
Iteration 138/1000 | Loss: 0.00001002
Iteration 139/1000 | Loss: 0.00001002
Iteration 140/1000 | Loss: 0.00001002
Iteration 141/1000 | Loss: 0.00001002
Iteration 142/1000 | Loss: 0.00001002
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [1.0020702575275209e-05, 1.0020702575275209e-05, 1.0020702575275209e-05, 1.0020702575275209e-05, 1.0020702575275209e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0020702575275209e-05

Optimization complete. Final v2v error: 2.6961796283721924 mm

Highest mean error: 2.9638161659240723 mm for frame 103

Lowest mean error: 2.458259105682373 mm for frame 0

Saving results

Total time: 48.10837697982788
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00851854
Iteration 2/25 | Loss: 0.00143008
Iteration 3/25 | Loss: 0.00116604
Iteration 4/25 | Loss: 0.00112817
Iteration 5/25 | Loss: 0.00111702
Iteration 6/25 | Loss: 0.00110955
Iteration 7/25 | Loss: 0.00112563
Iteration 8/25 | Loss: 0.00107665
Iteration 9/25 | Loss: 0.00108867
Iteration 10/25 | Loss: 0.00104132
Iteration 11/25 | Loss: 0.00103638
Iteration 12/25 | Loss: 0.00103587
Iteration 13/25 | Loss: 0.00103574
Iteration 14/25 | Loss: 0.00103572
Iteration 15/25 | Loss: 0.00103572
Iteration 16/25 | Loss: 0.00103572
Iteration 17/25 | Loss: 0.00103572
Iteration 18/25 | Loss: 0.00103572
Iteration 19/25 | Loss: 0.00103572
Iteration 20/25 | Loss: 0.00103572
Iteration 21/25 | Loss: 0.00103572
Iteration 22/25 | Loss: 0.00103572
Iteration 23/25 | Loss: 0.00103572
Iteration 24/25 | Loss: 0.00103572
Iteration 25/25 | Loss: 0.00103572

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.93911850
Iteration 2/25 | Loss: 0.00032118
Iteration 3/25 | Loss: 0.00032117
Iteration 4/25 | Loss: 0.00032117
Iteration 5/25 | Loss: 0.00032117
Iteration 6/25 | Loss: 0.00032117
Iteration 7/25 | Loss: 0.00032117
Iteration 8/25 | Loss: 0.00032117
Iteration 9/25 | Loss: 0.00032117
Iteration 10/25 | Loss: 0.00032117
Iteration 11/25 | Loss: 0.00032117
Iteration 12/25 | Loss: 0.00032117
Iteration 13/25 | Loss: 0.00032117
Iteration 14/25 | Loss: 0.00032117
Iteration 15/25 | Loss: 0.00032117
Iteration 16/25 | Loss: 0.00032117
Iteration 17/25 | Loss: 0.00032117
Iteration 18/25 | Loss: 0.00032117
Iteration 19/25 | Loss: 0.00032117
Iteration 20/25 | Loss: 0.00032117
Iteration 21/25 | Loss: 0.00032117
Iteration 22/25 | Loss: 0.00032117
Iteration 23/25 | Loss: 0.00032117
Iteration 24/25 | Loss: 0.00032117
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0003211709554307163, 0.0003211709554307163, 0.0003211709554307163, 0.0003211709554307163, 0.0003211709554307163]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003211709554307163

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00032117
Iteration 2/1000 | Loss: 0.00002953
Iteration 3/1000 | Loss: 0.00002233
Iteration 4/1000 | Loss: 0.00001996
Iteration 5/1000 | Loss: 0.00001914
Iteration 6/1000 | Loss: 0.00001854
Iteration 7/1000 | Loss: 0.00001818
Iteration 8/1000 | Loss: 0.00001782
Iteration 9/1000 | Loss: 0.00001759
Iteration 10/1000 | Loss: 0.00001741
Iteration 11/1000 | Loss: 0.00001727
Iteration 12/1000 | Loss: 0.00001725
Iteration 13/1000 | Loss: 0.00001724
Iteration 14/1000 | Loss: 0.00001722
Iteration 15/1000 | Loss: 0.00001722
Iteration 16/1000 | Loss: 0.00001722
Iteration 17/1000 | Loss: 0.00001722
Iteration 18/1000 | Loss: 0.00001722
Iteration 19/1000 | Loss: 0.00001720
Iteration 20/1000 | Loss: 0.00001717
Iteration 21/1000 | Loss: 0.00001716
Iteration 22/1000 | Loss: 0.00001713
Iteration 23/1000 | Loss: 0.00001713
Iteration 24/1000 | Loss: 0.00001713
Iteration 25/1000 | Loss: 0.00001711
Iteration 26/1000 | Loss: 0.00001708
Iteration 27/1000 | Loss: 0.00001708
Iteration 28/1000 | Loss: 0.00001708
Iteration 29/1000 | Loss: 0.00001708
Iteration 30/1000 | Loss: 0.00001707
Iteration 31/1000 | Loss: 0.00001707
Iteration 32/1000 | Loss: 0.00001706
Iteration 33/1000 | Loss: 0.00001703
Iteration 34/1000 | Loss: 0.00001703
Iteration 35/1000 | Loss: 0.00001702
Iteration 36/1000 | Loss: 0.00001701
Iteration 37/1000 | Loss: 0.00001701
Iteration 38/1000 | Loss: 0.00001701
Iteration 39/1000 | Loss: 0.00001701
Iteration 40/1000 | Loss: 0.00001701
Iteration 41/1000 | Loss: 0.00001701
Iteration 42/1000 | Loss: 0.00001700
Iteration 43/1000 | Loss: 0.00001700
Iteration 44/1000 | Loss: 0.00001700
Iteration 45/1000 | Loss: 0.00001700
Iteration 46/1000 | Loss: 0.00001700
Iteration 47/1000 | Loss: 0.00001700
Iteration 48/1000 | Loss: 0.00001699
Iteration 49/1000 | Loss: 0.00001699
Iteration 50/1000 | Loss: 0.00001698
Iteration 51/1000 | Loss: 0.00001698
Iteration 52/1000 | Loss: 0.00001698
Iteration 53/1000 | Loss: 0.00001698
Iteration 54/1000 | Loss: 0.00001698
Iteration 55/1000 | Loss: 0.00001698
Iteration 56/1000 | Loss: 0.00001698
Iteration 57/1000 | Loss: 0.00001698
Iteration 58/1000 | Loss: 0.00001697
Iteration 59/1000 | Loss: 0.00001697
Iteration 60/1000 | Loss: 0.00001697
Iteration 61/1000 | Loss: 0.00001697
Iteration 62/1000 | Loss: 0.00001697
Iteration 63/1000 | Loss: 0.00001697
Iteration 64/1000 | Loss: 0.00001696
Iteration 65/1000 | Loss: 0.00001696
Iteration 66/1000 | Loss: 0.00001696
Iteration 67/1000 | Loss: 0.00001696
Iteration 68/1000 | Loss: 0.00001696
Iteration 69/1000 | Loss: 0.00001696
Iteration 70/1000 | Loss: 0.00001696
Iteration 71/1000 | Loss: 0.00001695
Iteration 72/1000 | Loss: 0.00001695
Iteration 73/1000 | Loss: 0.00001695
Iteration 74/1000 | Loss: 0.00001695
Iteration 75/1000 | Loss: 0.00001695
Iteration 76/1000 | Loss: 0.00001695
Iteration 77/1000 | Loss: 0.00001695
Iteration 78/1000 | Loss: 0.00001695
Iteration 79/1000 | Loss: 0.00001695
Iteration 80/1000 | Loss: 0.00001695
Iteration 81/1000 | Loss: 0.00001694
Iteration 82/1000 | Loss: 0.00001694
Iteration 83/1000 | Loss: 0.00001694
Iteration 84/1000 | Loss: 0.00001694
Iteration 85/1000 | Loss: 0.00001694
Iteration 86/1000 | Loss: 0.00001694
Iteration 87/1000 | Loss: 0.00001694
Iteration 88/1000 | Loss: 0.00001694
Iteration 89/1000 | Loss: 0.00001694
Iteration 90/1000 | Loss: 0.00001694
Iteration 91/1000 | Loss: 0.00001694
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [1.694248931016773e-05, 1.694248931016773e-05, 1.694248931016773e-05, 1.694248931016773e-05, 1.694248931016773e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.694248931016773e-05

Optimization complete. Final v2v error: 3.454683303833008 mm

Highest mean error: 3.592233896255493 mm for frame 27

Lowest mean error: 3.30940842628479 mm for frame 6

Saving results

Total time: 45.87239742279053
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00511504
Iteration 2/25 | Loss: 0.00107383
Iteration 3/25 | Loss: 0.00099430
Iteration 4/25 | Loss: 0.00098209
Iteration 5/25 | Loss: 0.00097820
Iteration 6/25 | Loss: 0.00097820
Iteration 7/25 | Loss: 0.00097820
Iteration 8/25 | Loss: 0.00097820
Iteration 9/25 | Loss: 0.00097820
Iteration 10/25 | Loss: 0.00097820
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0009782048873603344, 0.0009782048873603344, 0.0009782048873603344, 0.0009782048873603344, 0.0009782048873603344]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009782048873603344

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.41377020
Iteration 2/25 | Loss: 0.00066821
Iteration 3/25 | Loss: 0.00066821
Iteration 4/25 | Loss: 0.00066821
Iteration 5/25 | Loss: 0.00066821
Iteration 6/25 | Loss: 0.00066820
Iteration 7/25 | Loss: 0.00066820
Iteration 8/25 | Loss: 0.00066820
Iteration 9/25 | Loss: 0.00066820
Iteration 10/25 | Loss: 0.00066820
Iteration 11/25 | Loss: 0.00066820
Iteration 12/25 | Loss: 0.00066820
Iteration 13/25 | Loss: 0.00066820
Iteration 14/25 | Loss: 0.00066820
Iteration 15/25 | Loss: 0.00066820
Iteration 16/25 | Loss: 0.00066820
Iteration 17/25 | Loss: 0.00066820
Iteration 18/25 | Loss: 0.00066820
Iteration 19/25 | Loss: 0.00066820
Iteration 20/25 | Loss: 0.00066820
Iteration 21/25 | Loss: 0.00066820
Iteration 22/25 | Loss: 0.00066820
Iteration 23/25 | Loss: 0.00066820
Iteration 24/25 | Loss: 0.00066820
Iteration 25/25 | Loss: 0.00066820

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066820
Iteration 2/1000 | Loss: 0.00001721
Iteration 3/1000 | Loss: 0.00001387
Iteration 4/1000 | Loss: 0.00001240
Iteration 5/1000 | Loss: 0.00001165
Iteration 6/1000 | Loss: 0.00001130
Iteration 7/1000 | Loss: 0.00001100
Iteration 8/1000 | Loss: 0.00001071
Iteration 9/1000 | Loss: 0.00001049
Iteration 10/1000 | Loss: 0.00001045
Iteration 11/1000 | Loss: 0.00001039
Iteration 12/1000 | Loss: 0.00001039
Iteration 13/1000 | Loss: 0.00001033
Iteration 14/1000 | Loss: 0.00001032
Iteration 15/1000 | Loss: 0.00001031
Iteration 16/1000 | Loss: 0.00001022
Iteration 17/1000 | Loss: 0.00001019
Iteration 18/1000 | Loss: 0.00001019
Iteration 19/1000 | Loss: 0.00001018
Iteration 20/1000 | Loss: 0.00001018
Iteration 21/1000 | Loss: 0.00001014
Iteration 22/1000 | Loss: 0.00001013
Iteration 23/1000 | Loss: 0.00001013
Iteration 24/1000 | Loss: 0.00001013
Iteration 25/1000 | Loss: 0.00001012
Iteration 26/1000 | Loss: 0.00001012
Iteration 27/1000 | Loss: 0.00001010
Iteration 28/1000 | Loss: 0.00001009
Iteration 29/1000 | Loss: 0.00001009
Iteration 30/1000 | Loss: 0.00001008
Iteration 31/1000 | Loss: 0.00001008
Iteration 32/1000 | Loss: 0.00001007
Iteration 33/1000 | Loss: 0.00001006
Iteration 34/1000 | Loss: 0.00001004
Iteration 35/1000 | Loss: 0.00001004
Iteration 36/1000 | Loss: 0.00001002
Iteration 37/1000 | Loss: 0.00001002
Iteration 38/1000 | Loss: 0.00001000
Iteration 39/1000 | Loss: 0.00001000
Iteration 40/1000 | Loss: 0.00000999
Iteration 41/1000 | Loss: 0.00000998
Iteration 42/1000 | Loss: 0.00000997
Iteration 43/1000 | Loss: 0.00000996
Iteration 44/1000 | Loss: 0.00000996
Iteration 45/1000 | Loss: 0.00000996
Iteration 46/1000 | Loss: 0.00000996
Iteration 47/1000 | Loss: 0.00000995
Iteration 48/1000 | Loss: 0.00000995
Iteration 49/1000 | Loss: 0.00000995
Iteration 50/1000 | Loss: 0.00000995
Iteration 51/1000 | Loss: 0.00000994
Iteration 52/1000 | Loss: 0.00000994
Iteration 53/1000 | Loss: 0.00000993
Iteration 54/1000 | Loss: 0.00000993
Iteration 55/1000 | Loss: 0.00000992
Iteration 56/1000 | Loss: 0.00000992
Iteration 57/1000 | Loss: 0.00000991
Iteration 58/1000 | Loss: 0.00000991
Iteration 59/1000 | Loss: 0.00000991
Iteration 60/1000 | Loss: 0.00000991
Iteration 61/1000 | Loss: 0.00000990
Iteration 62/1000 | Loss: 0.00000990
Iteration 63/1000 | Loss: 0.00000989
Iteration 64/1000 | Loss: 0.00000988
Iteration 65/1000 | Loss: 0.00000988
Iteration 66/1000 | Loss: 0.00000986
Iteration 67/1000 | Loss: 0.00000986
Iteration 68/1000 | Loss: 0.00000986
Iteration 69/1000 | Loss: 0.00000986
Iteration 70/1000 | Loss: 0.00000986
Iteration 71/1000 | Loss: 0.00000986
Iteration 72/1000 | Loss: 0.00000986
Iteration 73/1000 | Loss: 0.00000986
Iteration 74/1000 | Loss: 0.00000986
Iteration 75/1000 | Loss: 0.00000986
Iteration 76/1000 | Loss: 0.00000986
Iteration 77/1000 | Loss: 0.00000986
Iteration 78/1000 | Loss: 0.00000986
Iteration 79/1000 | Loss: 0.00000986
Iteration 80/1000 | Loss: 0.00000986
Iteration 81/1000 | Loss: 0.00000986
Iteration 82/1000 | Loss: 0.00000986
Iteration 83/1000 | Loss: 0.00000986
Iteration 84/1000 | Loss: 0.00000986
Iteration 85/1000 | Loss: 0.00000986
Iteration 86/1000 | Loss: 0.00000986
Iteration 87/1000 | Loss: 0.00000986
Iteration 88/1000 | Loss: 0.00000986
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [9.860257705440745e-06, 9.860257705440745e-06, 9.860257705440745e-06, 9.860257705440745e-06, 9.860257705440745e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.860257705440745e-06

Optimization complete. Final v2v error: 2.7305045127868652 mm

Highest mean error: 2.95917010307312 mm for frame 182

Lowest mean error: 2.471571445465088 mm for frame 13

Saving results

Total time: 34.646817684173584
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01020703
Iteration 2/25 | Loss: 0.00237463
Iteration 3/25 | Loss: 0.00163974
Iteration 4/25 | Loss: 0.00168784
Iteration 5/25 | Loss: 0.00137339
Iteration 6/25 | Loss: 0.00123375
Iteration 7/25 | Loss: 0.00113132
Iteration 8/25 | Loss: 0.00112076
Iteration 9/25 | Loss: 0.00112471
Iteration 10/25 | Loss: 0.00111026
Iteration 11/25 | Loss: 0.00109710
Iteration 12/25 | Loss: 0.00109347
Iteration 13/25 | Loss: 0.00109280
Iteration 14/25 | Loss: 0.00109260
Iteration 15/25 | Loss: 0.00109254
Iteration 16/25 | Loss: 0.00109254
Iteration 17/25 | Loss: 0.00109254
Iteration 18/25 | Loss: 0.00109254
Iteration 19/25 | Loss: 0.00109254
Iteration 20/25 | Loss: 0.00109253
Iteration 21/25 | Loss: 0.00109253
Iteration 22/25 | Loss: 0.00109253
Iteration 23/25 | Loss: 0.00109253
Iteration 24/25 | Loss: 0.00109253
Iteration 25/25 | Loss: 0.00109253

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36237860
Iteration 2/25 | Loss: 0.00063771
Iteration 3/25 | Loss: 0.00063771
Iteration 4/25 | Loss: 0.00063771
Iteration 5/25 | Loss: 0.00063771
Iteration 6/25 | Loss: 0.00063771
Iteration 7/25 | Loss: 0.00063771
Iteration 8/25 | Loss: 0.00063771
Iteration 9/25 | Loss: 0.00063771
Iteration 10/25 | Loss: 0.00063771
Iteration 11/25 | Loss: 0.00063771
Iteration 12/25 | Loss: 0.00063771
Iteration 13/25 | Loss: 0.00063771
Iteration 14/25 | Loss: 0.00063771
Iteration 15/25 | Loss: 0.00063771
Iteration 16/25 | Loss: 0.00063771
Iteration 17/25 | Loss: 0.00063771
Iteration 18/25 | Loss: 0.00063770
Iteration 19/25 | Loss: 0.00063771
Iteration 20/25 | Loss: 0.00063770
Iteration 21/25 | Loss: 0.00063770
Iteration 22/25 | Loss: 0.00063770
Iteration 23/25 | Loss: 0.00063770
Iteration 24/25 | Loss: 0.00063770
Iteration 25/25 | Loss: 0.00063770

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063770
Iteration 2/1000 | Loss: 0.00002834
Iteration 3/1000 | Loss: 0.00002008
Iteration 4/1000 | Loss: 0.00001765
Iteration 5/1000 | Loss: 0.00001702
Iteration 6/1000 | Loss: 0.00001673
Iteration 7/1000 | Loss: 0.00001644
Iteration 8/1000 | Loss: 0.00001630
Iteration 9/1000 | Loss: 0.00001611
Iteration 10/1000 | Loss: 0.00001603
Iteration 11/1000 | Loss: 0.00001603
Iteration 12/1000 | Loss: 0.00001590
Iteration 13/1000 | Loss: 0.00001586
Iteration 14/1000 | Loss: 0.00001584
Iteration 15/1000 | Loss: 0.00001581
Iteration 16/1000 | Loss: 0.00001580
Iteration 17/1000 | Loss: 0.00001574
Iteration 18/1000 | Loss: 0.00001574
Iteration 19/1000 | Loss: 0.00001572
Iteration 20/1000 | Loss: 0.00001569
Iteration 21/1000 | Loss: 0.00001568
Iteration 22/1000 | Loss: 0.00001568
Iteration 23/1000 | Loss: 0.00001568
Iteration 24/1000 | Loss: 0.00001568
Iteration 25/1000 | Loss: 0.00001568
Iteration 26/1000 | Loss: 0.00001568
Iteration 27/1000 | Loss: 0.00001568
Iteration 28/1000 | Loss: 0.00001568
Iteration 29/1000 | Loss: 0.00001567
Iteration 30/1000 | Loss: 0.00001567
Iteration 31/1000 | Loss: 0.00001567
Iteration 32/1000 | Loss: 0.00001567
Iteration 33/1000 | Loss: 0.00001567
Iteration 34/1000 | Loss: 0.00001566
Iteration 35/1000 | Loss: 0.00001566
Iteration 36/1000 | Loss: 0.00001565
Iteration 37/1000 | Loss: 0.00001565
Iteration 38/1000 | Loss: 0.00001564
Iteration 39/1000 | Loss: 0.00001564
Iteration 40/1000 | Loss: 0.00001563
Iteration 41/1000 | Loss: 0.00001563
Iteration 42/1000 | Loss: 0.00001563
Iteration 43/1000 | Loss: 0.00001562
Iteration 44/1000 | Loss: 0.00001560
Iteration 45/1000 | Loss: 0.00001558
Iteration 46/1000 | Loss: 0.00001558
Iteration 47/1000 | Loss: 0.00001558
Iteration 48/1000 | Loss: 0.00001557
Iteration 49/1000 | Loss: 0.00001557
Iteration 50/1000 | Loss: 0.00001557
Iteration 51/1000 | Loss: 0.00001557
Iteration 52/1000 | Loss: 0.00001557
Iteration 53/1000 | Loss: 0.00001557
Iteration 54/1000 | Loss: 0.00001557
Iteration 55/1000 | Loss: 0.00001557
Iteration 56/1000 | Loss: 0.00001557
Iteration 57/1000 | Loss: 0.00001557
Iteration 58/1000 | Loss: 0.00001557
Iteration 59/1000 | Loss: 0.00001556
Iteration 60/1000 | Loss: 0.00001556
Iteration 61/1000 | Loss: 0.00001556
Iteration 62/1000 | Loss: 0.00001556
Iteration 63/1000 | Loss: 0.00001556
Iteration 64/1000 | Loss: 0.00001556
Iteration 65/1000 | Loss: 0.00001556
Iteration 66/1000 | Loss: 0.00001556
Iteration 67/1000 | Loss: 0.00001555
Iteration 68/1000 | Loss: 0.00001555
Iteration 69/1000 | Loss: 0.00001555
Iteration 70/1000 | Loss: 0.00001555
Iteration 71/1000 | Loss: 0.00001555
Iteration 72/1000 | Loss: 0.00001555
Iteration 73/1000 | Loss: 0.00001554
Iteration 74/1000 | Loss: 0.00001554
Iteration 75/1000 | Loss: 0.00001554
Iteration 76/1000 | Loss: 0.00001554
Iteration 77/1000 | Loss: 0.00001554
Iteration 78/1000 | Loss: 0.00001554
Iteration 79/1000 | Loss: 0.00001554
Iteration 80/1000 | Loss: 0.00001553
Iteration 81/1000 | Loss: 0.00001553
Iteration 82/1000 | Loss: 0.00001553
Iteration 83/1000 | Loss: 0.00001553
Iteration 84/1000 | Loss: 0.00001553
Iteration 85/1000 | Loss: 0.00001553
Iteration 86/1000 | Loss: 0.00001553
Iteration 87/1000 | Loss: 0.00001553
Iteration 88/1000 | Loss: 0.00001553
Iteration 89/1000 | Loss: 0.00001553
Iteration 90/1000 | Loss: 0.00001553
Iteration 91/1000 | Loss: 0.00001553
Iteration 92/1000 | Loss: 0.00001553
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [1.5529756637988612e-05, 1.5529756637988612e-05, 1.5529756637988612e-05, 1.5529756637988612e-05, 1.5529756637988612e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5529756637988612e-05

Optimization complete. Final v2v error: 3.2624261379241943 mm

Highest mean error: 3.327108144760132 mm for frame 1

Lowest mean error: 3.189915180206299 mm for frame 43

Saving results

Total time: 45.254576683044434
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01027695
Iteration 2/25 | Loss: 0.00181614
Iteration 3/25 | Loss: 0.00160321
Iteration 4/25 | Loss: 0.00125282
Iteration 5/25 | Loss: 0.00120061
Iteration 6/25 | Loss: 0.00117098
Iteration 7/25 | Loss: 0.00111546
Iteration 8/25 | Loss: 0.00107836
Iteration 9/25 | Loss: 0.00105842
Iteration 10/25 | Loss: 0.00105915
Iteration 11/25 | Loss: 0.00104897
Iteration 12/25 | Loss: 0.00105077
Iteration 13/25 | Loss: 0.00104324
Iteration 14/25 | Loss: 0.00103920
Iteration 15/25 | Loss: 0.00103294
Iteration 16/25 | Loss: 0.00103194
Iteration 17/25 | Loss: 0.00103169
Iteration 18/25 | Loss: 0.00103151
Iteration 19/25 | Loss: 0.00103116
Iteration 20/25 | Loss: 0.00103077
Iteration 21/25 | Loss: 0.00103053
Iteration 22/25 | Loss: 0.00103042
Iteration 23/25 | Loss: 0.00103039
Iteration 24/25 | Loss: 0.00103039
Iteration 25/25 | Loss: 0.00103039

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49025738
Iteration 2/25 | Loss: 0.00137078
Iteration 3/25 | Loss: 0.00092999
Iteration 4/25 | Loss: 0.00092999
Iteration 5/25 | Loss: 0.00092999
Iteration 6/25 | Loss: 0.00092999
Iteration 7/25 | Loss: 0.00092999
Iteration 8/25 | Loss: 0.00092999
Iteration 9/25 | Loss: 0.00092999
Iteration 10/25 | Loss: 0.00092999
Iteration 11/25 | Loss: 0.00092999
Iteration 12/25 | Loss: 0.00092999
Iteration 13/25 | Loss: 0.00092999
Iteration 14/25 | Loss: 0.00092999
Iteration 15/25 | Loss: 0.00092999
Iteration 16/25 | Loss: 0.00092999
Iteration 17/25 | Loss: 0.00092999
Iteration 18/25 | Loss: 0.00092999
Iteration 19/25 | Loss: 0.00092999
Iteration 20/25 | Loss: 0.00092999
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009299893281422555, 0.0009299893281422555, 0.0009299893281422555, 0.0009299893281422555, 0.0009299893281422555]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009299893281422555

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092999
Iteration 2/1000 | Loss: 0.00017237
Iteration 3/1000 | Loss: 0.00002857
Iteration 4/1000 | Loss: 0.00002427
Iteration 5/1000 | Loss: 0.00002241
Iteration 6/1000 | Loss: 0.00010109
Iteration 7/1000 | Loss: 0.00002127
Iteration 8/1000 | Loss: 0.00020531
Iteration 9/1000 | Loss: 0.00023143
Iteration 10/1000 | Loss: 0.00020053
Iteration 11/1000 | Loss: 0.00020128
Iteration 12/1000 | Loss: 0.00011431
Iteration 13/1000 | Loss: 0.00003596
Iteration 14/1000 | Loss: 0.00002212
Iteration 15/1000 | Loss: 0.00002022
Iteration 16/1000 | Loss: 0.00007476
Iteration 17/1000 | Loss: 0.00014138
Iteration 18/1000 | Loss: 0.00005314
Iteration 19/1000 | Loss: 0.00001866
Iteration 20/1000 | Loss: 0.00008491
Iteration 21/1000 | Loss: 0.00010337
Iteration 22/1000 | Loss: 0.00009562
Iteration 23/1000 | Loss: 0.00016746
Iteration 24/1000 | Loss: 0.00001863
Iteration 25/1000 | Loss: 0.00001810
Iteration 26/1000 | Loss: 0.00008497
Iteration 27/1000 | Loss: 0.00006630
Iteration 28/1000 | Loss: 0.00007631
Iteration 29/1000 | Loss: 0.00003518
Iteration 30/1000 | Loss: 0.00007239
Iteration 31/1000 | Loss: 0.00002042
Iteration 32/1000 | Loss: 0.00001887
Iteration 33/1000 | Loss: 0.00001785
Iteration 34/1000 | Loss: 0.00001767
Iteration 35/1000 | Loss: 0.00001765
Iteration 36/1000 | Loss: 0.00001764
Iteration 37/1000 | Loss: 0.00001764
Iteration 38/1000 | Loss: 0.00001762
Iteration 39/1000 | Loss: 0.00001761
Iteration 40/1000 | Loss: 0.00001760
Iteration 41/1000 | Loss: 0.00001746
Iteration 42/1000 | Loss: 0.00001742
Iteration 43/1000 | Loss: 0.00001741
Iteration 44/1000 | Loss: 0.00001741
Iteration 45/1000 | Loss: 0.00001740
Iteration 46/1000 | Loss: 0.00001740
Iteration 47/1000 | Loss: 0.00001739
Iteration 48/1000 | Loss: 0.00001739
Iteration 49/1000 | Loss: 0.00001739
Iteration 50/1000 | Loss: 0.00001738
Iteration 51/1000 | Loss: 0.00001738
Iteration 52/1000 | Loss: 0.00001737
Iteration 53/1000 | Loss: 0.00001736
Iteration 54/1000 | Loss: 0.00001736
Iteration 55/1000 | Loss: 0.00001736
Iteration 56/1000 | Loss: 0.00001736
Iteration 57/1000 | Loss: 0.00001736
Iteration 58/1000 | Loss: 0.00001735
Iteration 59/1000 | Loss: 0.00001735
Iteration 60/1000 | Loss: 0.00001735
Iteration 61/1000 | Loss: 0.00001735
Iteration 62/1000 | Loss: 0.00001735
Iteration 63/1000 | Loss: 0.00001735
Iteration 64/1000 | Loss: 0.00001735
Iteration 65/1000 | Loss: 0.00001735
Iteration 66/1000 | Loss: 0.00001735
Iteration 67/1000 | Loss: 0.00001735
Iteration 68/1000 | Loss: 0.00001735
Iteration 69/1000 | Loss: 0.00001735
Iteration 70/1000 | Loss: 0.00001734
Iteration 71/1000 | Loss: 0.00001734
Iteration 72/1000 | Loss: 0.00001734
Iteration 73/1000 | Loss: 0.00001734
Iteration 74/1000 | Loss: 0.00001733
Iteration 75/1000 | Loss: 0.00001733
Iteration 76/1000 | Loss: 0.00001733
Iteration 77/1000 | Loss: 0.00001733
Iteration 78/1000 | Loss: 0.00001732
Iteration 79/1000 | Loss: 0.00001732
Iteration 80/1000 | Loss: 0.00001731
Iteration 81/1000 | Loss: 0.00001731
Iteration 82/1000 | Loss: 0.00001731
Iteration 83/1000 | Loss: 0.00001731
Iteration 84/1000 | Loss: 0.00001731
Iteration 85/1000 | Loss: 0.00001731
Iteration 86/1000 | Loss: 0.00001731
Iteration 87/1000 | Loss: 0.00001731
Iteration 88/1000 | Loss: 0.00001731
Iteration 89/1000 | Loss: 0.00001731
Iteration 90/1000 | Loss: 0.00001731
Iteration 91/1000 | Loss: 0.00001731
Iteration 92/1000 | Loss: 0.00001731
Iteration 93/1000 | Loss: 0.00001731
Iteration 94/1000 | Loss: 0.00001731
Iteration 95/1000 | Loss: 0.00001731
Iteration 96/1000 | Loss: 0.00001731
Iteration 97/1000 | Loss: 0.00001731
Iteration 98/1000 | Loss: 0.00001731
Iteration 99/1000 | Loss: 0.00001731
Iteration 100/1000 | Loss: 0.00001731
Iteration 101/1000 | Loss: 0.00001731
Iteration 102/1000 | Loss: 0.00001731
Iteration 103/1000 | Loss: 0.00001731
Iteration 104/1000 | Loss: 0.00001731
Iteration 105/1000 | Loss: 0.00001731
Iteration 106/1000 | Loss: 0.00001731
Iteration 107/1000 | Loss: 0.00001731
Iteration 108/1000 | Loss: 0.00001731
Iteration 109/1000 | Loss: 0.00001731
Iteration 110/1000 | Loss: 0.00001731
Iteration 111/1000 | Loss: 0.00001731
Iteration 112/1000 | Loss: 0.00001731
Iteration 113/1000 | Loss: 0.00001731
Iteration 114/1000 | Loss: 0.00001731
Iteration 115/1000 | Loss: 0.00001731
Iteration 116/1000 | Loss: 0.00001731
Iteration 117/1000 | Loss: 0.00001731
Iteration 118/1000 | Loss: 0.00001731
Iteration 119/1000 | Loss: 0.00001731
Iteration 120/1000 | Loss: 0.00001731
Iteration 121/1000 | Loss: 0.00001731
Iteration 122/1000 | Loss: 0.00001731
Iteration 123/1000 | Loss: 0.00001731
Iteration 124/1000 | Loss: 0.00001731
Iteration 125/1000 | Loss: 0.00001731
Iteration 126/1000 | Loss: 0.00001731
Iteration 127/1000 | Loss: 0.00001731
Iteration 128/1000 | Loss: 0.00001731
Iteration 129/1000 | Loss: 0.00001731
Iteration 130/1000 | Loss: 0.00001731
Iteration 131/1000 | Loss: 0.00001731
Iteration 132/1000 | Loss: 0.00001731
Iteration 133/1000 | Loss: 0.00001731
Iteration 134/1000 | Loss: 0.00001731
Iteration 135/1000 | Loss: 0.00001731
Iteration 136/1000 | Loss: 0.00001731
Iteration 137/1000 | Loss: 0.00001731
Iteration 138/1000 | Loss: 0.00001731
Iteration 139/1000 | Loss: 0.00001731
Iteration 140/1000 | Loss: 0.00001731
Iteration 141/1000 | Loss: 0.00001731
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.730857002257835e-05, 1.730857002257835e-05, 1.730857002257835e-05, 1.730857002257835e-05, 1.730857002257835e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.730857002257835e-05

Optimization complete. Final v2v error: 2.7905988693237305 mm

Highest mean error: 11.128763198852539 mm for frame 126

Lowest mean error: 2.311648368835449 mm for frame 22

Saving results

Total time: 100.53927540779114
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01019777
Iteration 2/25 | Loss: 0.00175434
Iteration 3/25 | Loss: 0.00124534
Iteration 4/25 | Loss: 0.00119331
Iteration 5/25 | Loss: 0.00117983
Iteration 6/25 | Loss: 0.00114918
Iteration 7/25 | Loss: 0.00112512
Iteration 8/25 | Loss: 0.00112475
Iteration 9/25 | Loss: 0.00112241
Iteration 10/25 | Loss: 0.00110826
Iteration 11/25 | Loss: 0.00111019
Iteration 12/25 | Loss: 0.00110621
Iteration 13/25 | Loss: 0.00110689
Iteration 14/25 | Loss: 0.00110086
Iteration 15/25 | Loss: 0.00109256
Iteration 16/25 | Loss: 0.00108771
Iteration 17/25 | Loss: 0.00108788
Iteration 18/25 | Loss: 0.00108581
Iteration 19/25 | Loss: 0.00108753
Iteration 20/25 | Loss: 0.00108632
Iteration 21/25 | Loss: 0.00108504
Iteration 22/25 | Loss: 0.00108765
Iteration 23/25 | Loss: 0.00108870
Iteration 24/25 | Loss: 0.00108656
Iteration 25/25 | Loss: 0.00108566

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37827659
Iteration 2/25 | Loss: 0.00186833
Iteration 3/25 | Loss: 0.00147135
Iteration 4/25 | Loss: 0.00147135
Iteration 5/25 | Loss: 0.00147135
Iteration 6/25 | Loss: 0.00147135
Iteration 7/25 | Loss: 0.00147135
Iteration 8/25 | Loss: 0.00147135
Iteration 9/25 | Loss: 0.00147135
Iteration 10/25 | Loss: 0.00147134
Iteration 11/25 | Loss: 0.00147134
Iteration 12/25 | Loss: 0.00147134
Iteration 13/25 | Loss: 0.00147134
Iteration 14/25 | Loss: 0.00147134
Iteration 15/25 | Loss: 0.00147134
Iteration 16/25 | Loss: 0.00147134
Iteration 17/25 | Loss: 0.00147134
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0014713448472321033, 0.0014713448472321033, 0.0014713448472321033, 0.0014713448472321033, 0.0014713448472321033]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014713448472321033

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00147134
Iteration 2/1000 | Loss: 0.00117012
Iteration 3/1000 | Loss: 0.00255369
Iteration 4/1000 | Loss: 0.00015073
Iteration 5/1000 | Loss: 0.00009482
Iteration 6/1000 | Loss: 0.00017313
Iteration 7/1000 | Loss: 0.00013723
Iteration 8/1000 | Loss: 0.00020276
Iteration 9/1000 | Loss: 0.00006868
Iteration 10/1000 | Loss: 0.00104861
Iteration 11/1000 | Loss: 0.00081924
Iteration 12/1000 | Loss: 0.00048010
Iteration 13/1000 | Loss: 0.00051318
Iteration 14/1000 | Loss: 0.00005920
Iteration 15/1000 | Loss: 0.00169500
Iteration 16/1000 | Loss: 0.00089129
Iteration 17/1000 | Loss: 0.00060076
Iteration 18/1000 | Loss: 0.00047592
Iteration 19/1000 | Loss: 0.00137576
Iteration 20/1000 | Loss: 0.00024128
Iteration 21/1000 | Loss: 0.00107163
Iteration 22/1000 | Loss: 0.00123757
Iteration 23/1000 | Loss: 0.00012864
Iteration 24/1000 | Loss: 0.00016100
Iteration 25/1000 | Loss: 0.00011908
Iteration 26/1000 | Loss: 0.00051429
Iteration 27/1000 | Loss: 0.00004303
Iteration 28/1000 | Loss: 0.00009180
Iteration 29/1000 | Loss: 0.00010339
Iteration 30/1000 | Loss: 0.00045482
Iteration 31/1000 | Loss: 0.00090267
Iteration 32/1000 | Loss: 0.00126632
Iteration 33/1000 | Loss: 0.00036836
Iteration 34/1000 | Loss: 0.00036797
Iteration 35/1000 | Loss: 0.00025311
Iteration 36/1000 | Loss: 0.00022377
Iteration 37/1000 | Loss: 0.00003218
Iteration 38/1000 | Loss: 0.00011842
Iteration 39/1000 | Loss: 0.00003292
Iteration 40/1000 | Loss: 0.00002706
Iteration 41/1000 | Loss: 0.00042249
Iteration 42/1000 | Loss: 0.00031383
Iteration 43/1000 | Loss: 0.00008396
Iteration 44/1000 | Loss: 0.00003006
Iteration 45/1000 | Loss: 0.00002879
Iteration 46/1000 | Loss: 0.00002384
Iteration 47/1000 | Loss: 0.00003244
Iteration 48/1000 | Loss: 0.00005607
Iteration 49/1000 | Loss: 0.00002282
Iteration 50/1000 | Loss: 0.00002751
Iteration 51/1000 | Loss: 0.00002742
Iteration 52/1000 | Loss: 0.00008808
Iteration 53/1000 | Loss: 0.00021049
Iteration 54/1000 | Loss: 0.00005597
Iteration 55/1000 | Loss: 0.00002194
Iteration 56/1000 | Loss: 0.00002085
Iteration 57/1000 | Loss: 0.00004315
Iteration 58/1000 | Loss: 0.00002073
Iteration 59/1000 | Loss: 0.00002017
Iteration 60/1000 | Loss: 0.00002013
Iteration 61/1000 | Loss: 0.00001992
Iteration 62/1000 | Loss: 0.00001977
Iteration 63/1000 | Loss: 0.00001975
Iteration 64/1000 | Loss: 0.00005006
Iteration 65/1000 | Loss: 0.00008618
Iteration 66/1000 | Loss: 0.00027614
Iteration 67/1000 | Loss: 0.00010459
Iteration 68/1000 | Loss: 0.00002006
Iteration 69/1000 | Loss: 0.00001944
Iteration 70/1000 | Loss: 0.00020000
Iteration 71/1000 | Loss: 0.00016373
Iteration 72/1000 | Loss: 0.00013918
Iteration 73/1000 | Loss: 0.00004768
Iteration 74/1000 | Loss: 0.00002476
Iteration 75/1000 | Loss: 0.00002128
Iteration 76/1000 | Loss: 0.00012796
Iteration 77/1000 | Loss: 0.00005435
Iteration 78/1000 | Loss: 0.00019126
Iteration 79/1000 | Loss: 0.00035066
Iteration 80/1000 | Loss: 0.00018582
Iteration 81/1000 | Loss: 0.00006162
Iteration 82/1000 | Loss: 0.00023852
Iteration 83/1000 | Loss: 0.00006680
Iteration 84/1000 | Loss: 0.00024983
Iteration 85/1000 | Loss: 0.00021520
Iteration 86/1000 | Loss: 0.00015453
Iteration 87/1000 | Loss: 0.00002840
Iteration 88/1000 | Loss: 0.00005922
Iteration 89/1000 | Loss: 0.00001958
Iteration 90/1000 | Loss: 0.00001871
Iteration 91/1000 | Loss: 0.00001809
Iteration 92/1000 | Loss: 0.00003275
Iteration 93/1000 | Loss: 0.00002993
Iteration 94/1000 | Loss: 0.00007470
Iteration 95/1000 | Loss: 0.00015244
Iteration 96/1000 | Loss: 0.00006144
Iteration 97/1000 | Loss: 0.00004397
Iteration 98/1000 | Loss: 0.00001868
Iteration 99/1000 | Loss: 0.00001752
Iteration 100/1000 | Loss: 0.00001745
Iteration 101/1000 | Loss: 0.00001742
Iteration 102/1000 | Loss: 0.00001740
Iteration 103/1000 | Loss: 0.00004902
Iteration 104/1000 | Loss: 0.00006193
Iteration 105/1000 | Loss: 0.00001718
Iteration 106/1000 | Loss: 0.00003135
Iteration 107/1000 | Loss: 0.00001960
Iteration 108/1000 | Loss: 0.00001705
Iteration 109/1000 | Loss: 0.00001704
Iteration 110/1000 | Loss: 0.00001704
Iteration 111/1000 | Loss: 0.00001704
Iteration 112/1000 | Loss: 0.00001704
Iteration 113/1000 | Loss: 0.00001704
Iteration 114/1000 | Loss: 0.00001703
Iteration 115/1000 | Loss: 0.00002203
Iteration 116/1000 | Loss: 0.00001701
Iteration 117/1000 | Loss: 0.00001701
Iteration 118/1000 | Loss: 0.00001701
Iteration 119/1000 | Loss: 0.00001701
Iteration 120/1000 | Loss: 0.00001701
Iteration 121/1000 | Loss: 0.00001701
Iteration 122/1000 | Loss: 0.00001701
Iteration 123/1000 | Loss: 0.00001701
Iteration 124/1000 | Loss: 0.00001701
Iteration 125/1000 | Loss: 0.00001700
Iteration 126/1000 | Loss: 0.00001700
Iteration 127/1000 | Loss: 0.00001697
Iteration 128/1000 | Loss: 0.00001697
Iteration 129/1000 | Loss: 0.00006122
Iteration 130/1000 | Loss: 0.00020718
Iteration 131/1000 | Loss: 0.00001813
Iteration 132/1000 | Loss: 0.00005460
Iteration 133/1000 | Loss: 0.00001806
Iteration 134/1000 | Loss: 0.00001692
Iteration 135/1000 | Loss: 0.00001690
Iteration 136/1000 | Loss: 0.00001689
Iteration 137/1000 | Loss: 0.00001689
Iteration 138/1000 | Loss: 0.00001689
Iteration 139/1000 | Loss: 0.00001689
Iteration 140/1000 | Loss: 0.00001689
Iteration 141/1000 | Loss: 0.00001688
Iteration 142/1000 | Loss: 0.00001688
Iteration 143/1000 | Loss: 0.00001688
Iteration 144/1000 | Loss: 0.00001688
Iteration 145/1000 | Loss: 0.00001688
Iteration 146/1000 | Loss: 0.00001688
Iteration 147/1000 | Loss: 0.00001688
Iteration 148/1000 | Loss: 0.00001688
Iteration 149/1000 | Loss: 0.00001688
Iteration 150/1000 | Loss: 0.00001688
Iteration 151/1000 | Loss: 0.00001688
Iteration 152/1000 | Loss: 0.00001688
Iteration 153/1000 | Loss: 0.00001688
Iteration 154/1000 | Loss: 0.00001688
Iteration 155/1000 | Loss: 0.00001688
Iteration 156/1000 | Loss: 0.00001688
Iteration 157/1000 | Loss: 0.00001688
Iteration 158/1000 | Loss: 0.00001688
Iteration 159/1000 | Loss: 0.00001688
Iteration 160/1000 | Loss: 0.00001688
Iteration 161/1000 | Loss: 0.00001688
Iteration 162/1000 | Loss: 0.00001688
Iteration 163/1000 | Loss: 0.00001688
Iteration 164/1000 | Loss: 0.00001688
Iteration 165/1000 | Loss: 0.00001688
Iteration 166/1000 | Loss: 0.00001688
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [1.6879910617717542e-05, 1.6879910617717542e-05, 1.6879910617717542e-05, 1.6879910617717542e-05, 1.6879910617717542e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6879910617717542e-05

Optimization complete. Final v2v error: 2.794332265853882 mm

Highest mean error: 12.275440216064453 mm for frame 5

Lowest mean error: 2.390523910522461 mm for frame 226

Saving results

Total time: 225.90848422050476
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00513598
Iteration 2/25 | Loss: 0.00130911
Iteration 3/25 | Loss: 0.00108411
Iteration 4/25 | Loss: 0.00106778
Iteration 5/25 | Loss: 0.00106410
Iteration 6/25 | Loss: 0.00106239
Iteration 7/25 | Loss: 0.00106239
Iteration 8/25 | Loss: 0.00106239
Iteration 9/25 | Loss: 0.00106239
Iteration 10/25 | Loss: 0.00106239
Iteration 11/25 | Loss: 0.00106239
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010623895796015859, 0.0010623895796015859, 0.0010623895796015859, 0.0010623895796015859, 0.0010623895796015859]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010623895796015859

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38705444
Iteration 2/25 | Loss: 0.00080067
Iteration 3/25 | Loss: 0.00080066
Iteration 4/25 | Loss: 0.00080066
Iteration 5/25 | Loss: 0.00080066
Iteration 6/25 | Loss: 0.00080066
Iteration 7/25 | Loss: 0.00080066
Iteration 8/25 | Loss: 0.00080066
Iteration 9/25 | Loss: 0.00080066
Iteration 10/25 | Loss: 0.00080066
Iteration 11/25 | Loss: 0.00080066
Iteration 12/25 | Loss: 0.00080066
Iteration 13/25 | Loss: 0.00080066
Iteration 14/25 | Loss: 0.00080066
Iteration 15/25 | Loss: 0.00080066
Iteration 16/25 | Loss: 0.00080066
Iteration 17/25 | Loss: 0.00080066
Iteration 18/25 | Loss: 0.00080066
Iteration 19/25 | Loss: 0.00080066
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008006570860743523, 0.0008006570860743523, 0.0008006570860743523, 0.0008006570860743523, 0.0008006570860743523]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008006570860743523

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080066
Iteration 2/1000 | Loss: 0.00003930
Iteration 3/1000 | Loss: 0.00002853
Iteration 4/1000 | Loss: 0.00002627
Iteration 5/1000 | Loss: 0.00002492
Iteration 6/1000 | Loss: 0.00002413
Iteration 7/1000 | Loss: 0.00002353
Iteration 8/1000 | Loss: 0.00002309
Iteration 9/1000 | Loss: 0.00002274
Iteration 10/1000 | Loss: 0.00002244
Iteration 11/1000 | Loss: 0.00002224
Iteration 12/1000 | Loss: 0.00002212
Iteration 13/1000 | Loss: 0.00002202
Iteration 14/1000 | Loss: 0.00002201
Iteration 15/1000 | Loss: 0.00002200
Iteration 16/1000 | Loss: 0.00002198
Iteration 17/1000 | Loss: 0.00002198
Iteration 18/1000 | Loss: 0.00002197
Iteration 19/1000 | Loss: 0.00002197
Iteration 20/1000 | Loss: 0.00002196
Iteration 21/1000 | Loss: 0.00002195
Iteration 22/1000 | Loss: 0.00002193
Iteration 23/1000 | Loss: 0.00002189
Iteration 24/1000 | Loss: 0.00002189
Iteration 25/1000 | Loss: 0.00002188
Iteration 26/1000 | Loss: 0.00002188
Iteration 27/1000 | Loss: 0.00002188
Iteration 28/1000 | Loss: 0.00002188
Iteration 29/1000 | Loss: 0.00002188
Iteration 30/1000 | Loss: 0.00002187
Iteration 31/1000 | Loss: 0.00002187
Iteration 32/1000 | Loss: 0.00002185
Iteration 33/1000 | Loss: 0.00002185
Iteration 34/1000 | Loss: 0.00002185
Iteration 35/1000 | Loss: 0.00002184
Iteration 36/1000 | Loss: 0.00002184
Iteration 37/1000 | Loss: 0.00002183
Iteration 38/1000 | Loss: 0.00002183
Iteration 39/1000 | Loss: 0.00002183
Iteration 40/1000 | Loss: 0.00002183
Iteration 41/1000 | Loss: 0.00002182
Iteration 42/1000 | Loss: 0.00002182
Iteration 43/1000 | Loss: 0.00002182
Iteration 44/1000 | Loss: 0.00002182
Iteration 45/1000 | Loss: 0.00002182
Iteration 46/1000 | Loss: 0.00002182
Iteration 47/1000 | Loss: 0.00002182
Iteration 48/1000 | Loss: 0.00002182
Iteration 49/1000 | Loss: 0.00002182
Iteration 50/1000 | Loss: 0.00002182
Iteration 51/1000 | Loss: 0.00002181
Iteration 52/1000 | Loss: 0.00002181
Iteration 53/1000 | Loss: 0.00002181
Iteration 54/1000 | Loss: 0.00002181
Iteration 55/1000 | Loss: 0.00002181
Iteration 56/1000 | Loss: 0.00002181
Iteration 57/1000 | Loss: 0.00002181
Iteration 58/1000 | Loss: 0.00002181
Iteration 59/1000 | Loss: 0.00002181
Iteration 60/1000 | Loss: 0.00002181
Iteration 61/1000 | Loss: 0.00002181
Iteration 62/1000 | Loss: 0.00002181
Iteration 63/1000 | Loss: 0.00002180
Iteration 64/1000 | Loss: 0.00002180
Iteration 65/1000 | Loss: 0.00002179
Iteration 66/1000 | Loss: 0.00002179
Iteration 67/1000 | Loss: 0.00002179
Iteration 68/1000 | Loss: 0.00002179
Iteration 69/1000 | Loss: 0.00002179
Iteration 70/1000 | Loss: 0.00002179
Iteration 71/1000 | Loss: 0.00002179
Iteration 72/1000 | Loss: 0.00002179
Iteration 73/1000 | Loss: 0.00002179
Iteration 74/1000 | Loss: 0.00002178
Iteration 75/1000 | Loss: 0.00002178
Iteration 76/1000 | Loss: 0.00002178
Iteration 77/1000 | Loss: 0.00002178
Iteration 78/1000 | Loss: 0.00002178
Iteration 79/1000 | Loss: 0.00002178
Iteration 80/1000 | Loss: 0.00002177
Iteration 81/1000 | Loss: 0.00002177
Iteration 82/1000 | Loss: 0.00002177
Iteration 83/1000 | Loss: 0.00002177
Iteration 84/1000 | Loss: 0.00002177
Iteration 85/1000 | Loss: 0.00002176
Iteration 86/1000 | Loss: 0.00002176
Iteration 87/1000 | Loss: 0.00002176
Iteration 88/1000 | Loss: 0.00002176
Iteration 89/1000 | Loss: 0.00002176
Iteration 90/1000 | Loss: 0.00002176
Iteration 91/1000 | Loss: 0.00002176
Iteration 92/1000 | Loss: 0.00002175
Iteration 93/1000 | Loss: 0.00002175
Iteration 94/1000 | Loss: 0.00002175
Iteration 95/1000 | Loss: 0.00002175
Iteration 96/1000 | Loss: 0.00002174
Iteration 97/1000 | Loss: 0.00002174
Iteration 98/1000 | Loss: 0.00002174
Iteration 99/1000 | Loss: 0.00002174
Iteration 100/1000 | Loss: 0.00002174
Iteration 101/1000 | Loss: 0.00002173
Iteration 102/1000 | Loss: 0.00002173
Iteration 103/1000 | Loss: 0.00002173
Iteration 104/1000 | Loss: 0.00002173
Iteration 105/1000 | Loss: 0.00002173
Iteration 106/1000 | Loss: 0.00002173
Iteration 107/1000 | Loss: 0.00002173
Iteration 108/1000 | Loss: 0.00002172
Iteration 109/1000 | Loss: 0.00002172
Iteration 110/1000 | Loss: 0.00002172
Iteration 111/1000 | Loss: 0.00002172
Iteration 112/1000 | Loss: 0.00002171
Iteration 113/1000 | Loss: 0.00002171
Iteration 114/1000 | Loss: 0.00002171
Iteration 115/1000 | Loss: 0.00002171
Iteration 116/1000 | Loss: 0.00002171
Iteration 117/1000 | Loss: 0.00002171
Iteration 118/1000 | Loss: 0.00002171
Iteration 119/1000 | Loss: 0.00002171
Iteration 120/1000 | Loss: 0.00002171
Iteration 121/1000 | Loss: 0.00002170
Iteration 122/1000 | Loss: 0.00002170
Iteration 123/1000 | Loss: 0.00002170
Iteration 124/1000 | Loss: 0.00002170
Iteration 125/1000 | Loss: 0.00002170
Iteration 126/1000 | Loss: 0.00002170
Iteration 127/1000 | Loss: 0.00002170
Iteration 128/1000 | Loss: 0.00002170
Iteration 129/1000 | Loss: 0.00002170
Iteration 130/1000 | Loss: 0.00002169
Iteration 131/1000 | Loss: 0.00002169
Iteration 132/1000 | Loss: 0.00002169
Iteration 133/1000 | Loss: 0.00002169
Iteration 134/1000 | Loss: 0.00002169
Iteration 135/1000 | Loss: 0.00002169
Iteration 136/1000 | Loss: 0.00002169
Iteration 137/1000 | Loss: 0.00002169
Iteration 138/1000 | Loss: 0.00002169
Iteration 139/1000 | Loss: 0.00002168
Iteration 140/1000 | Loss: 0.00002168
Iteration 141/1000 | Loss: 0.00002168
Iteration 142/1000 | Loss: 0.00002168
Iteration 143/1000 | Loss: 0.00002168
Iteration 144/1000 | Loss: 0.00002168
Iteration 145/1000 | Loss: 0.00002168
Iteration 146/1000 | Loss: 0.00002168
Iteration 147/1000 | Loss: 0.00002168
Iteration 148/1000 | Loss: 0.00002168
Iteration 149/1000 | Loss: 0.00002168
Iteration 150/1000 | Loss: 0.00002168
Iteration 151/1000 | Loss: 0.00002167
Iteration 152/1000 | Loss: 0.00002167
Iteration 153/1000 | Loss: 0.00002167
Iteration 154/1000 | Loss: 0.00002167
Iteration 155/1000 | Loss: 0.00002167
Iteration 156/1000 | Loss: 0.00002167
Iteration 157/1000 | Loss: 0.00002167
Iteration 158/1000 | Loss: 0.00002167
Iteration 159/1000 | Loss: 0.00002167
Iteration 160/1000 | Loss: 0.00002167
Iteration 161/1000 | Loss: 0.00002167
Iteration 162/1000 | Loss: 0.00002166
Iteration 163/1000 | Loss: 0.00002166
Iteration 164/1000 | Loss: 0.00002166
Iteration 165/1000 | Loss: 0.00002166
Iteration 166/1000 | Loss: 0.00002166
Iteration 167/1000 | Loss: 0.00002166
Iteration 168/1000 | Loss: 0.00002166
Iteration 169/1000 | Loss: 0.00002166
Iteration 170/1000 | Loss: 0.00002166
Iteration 171/1000 | Loss: 0.00002166
Iteration 172/1000 | Loss: 0.00002166
Iteration 173/1000 | Loss: 0.00002166
Iteration 174/1000 | Loss: 0.00002166
Iteration 175/1000 | Loss: 0.00002166
Iteration 176/1000 | Loss: 0.00002165
Iteration 177/1000 | Loss: 0.00002165
Iteration 178/1000 | Loss: 0.00002165
Iteration 179/1000 | Loss: 0.00002165
Iteration 180/1000 | Loss: 0.00002165
Iteration 181/1000 | Loss: 0.00002165
Iteration 182/1000 | Loss: 0.00002165
Iteration 183/1000 | Loss: 0.00002165
Iteration 184/1000 | Loss: 0.00002165
Iteration 185/1000 | Loss: 0.00002165
Iteration 186/1000 | Loss: 0.00002165
Iteration 187/1000 | Loss: 0.00002165
Iteration 188/1000 | Loss: 0.00002165
Iteration 189/1000 | Loss: 0.00002165
Iteration 190/1000 | Loss: 0.00002165
Iteration 191/1000 | Loss: 0.00002165
Iteration 192/1000 | Loss: 0.00002165
Iteration 193/1000 | Loss: 0.00002165
Iteration 194/1000 | Loss: 0.00002165
Iteration 195/1000 | Loss: 0.00002165
Iteration 196/1000 | Loss: 0.00002164
Iteration 197/1000 | Loss: 0.00002164
Iteration 198/1000 | Loss: 0.00002164
Iteration 199/1000 | Loss: 0.00002164
Iteration 200/1000 | Loss: 0.00002164
Iteration 201/1000 | Loss: 0.00002164
Iteration 202/1000 | Loss: 0.00002164
Iteration 203/1000 | Loss: 0.00002164
Iteration 204/1000 | Loss: 0.00002164
Iteration 205/1000 | Loss: 0.00002164
Iteration 206/1000 | Loss: 0.00002164
Iteration 207/1000 | Loss: 0.00002164
Iteration 208/1000 | Loss: 0.00002164
Iteration 209/1000 | Loss: 0.00002164
Iteration 210/1000 | Loss: 0.00002164
Iteration 211/1000 | Loss: 0.00002164
Iteration 212/1000 | Loss: 0.00002164
Iteration 213/1000 | Loss: 0.00002164
Iteration 214/1000 | Loss: 0.00002164
Iteration 215/1000 | Loss: 0.00002164
Iteration 216/1000 | Loss: 0.00002164
Iteration 217/1000 | Loss: 0.00002164
Iteration 218/1000 | Loss: 0.00002164
Iteration 219/1000 | Loss: 0.00002164
Iteration 220/1000 | Loss: 0.00002164
Iteration 221/1000 | Loss: 0.00002164
Iteration 222/1000 | Loss: 0.00002164
Iteration 223/1000 | Loss: 0.00002164
Iteration 224/1000 | Loss: 0.00002164
Iteration 225/1000 | Loss: 0.00002164
Iteration 226/1000 | Loss: 0.00002164
Iteration 227/1000 | Loss: 0.00002164
Iteration 228/1000 | Loss: 0.00002164
Iteration 229/1000 | Loss: 0.00002164
Iteration 230/1000 | Loss: 0.00002164
Iteration 231/1000 | Loss: 0.00002164
Iteration 232/1000 | Loss: 0.00002164
Iteration 233/1000 | Loss: 0.00002164
Iteration 234/1000 | Loss: 0.00002164
Iteration 235/1000 | Loss: 0.00002164
Iteration 236/1000 | Loss: 0.00002164
Iteration 237/1000 | Loss: 0.00002164
Iteration 238/1000 | Loss: 0.00002164
Iteration 239/1000 | Loss: 0.00002164
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 239. Stopping optimization.
Last 5 losses: [2.164401848858688e-05, 2.164401848858688e-05, 2.164401848858688e-05, 2.164401848858688e-05, 2.164401848858688e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.164401848858688e-05

Optimization complete. Final v2v error: 3.5363690853118896 mm

Highest mean error: 4.23136568069458 mm for frame 10

Lowest mean error: 2.53827166557312 mm for frame 63

Saving results

Total time: 42.80746841430664
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00822988
Iteration 2/25 | Loss: 0.00132607
Iteration 3/25 | Loss: 0.00107987
Iteration 4/25 | Loss: 0.00105914
Iteration 5/25 | Loss: 0.00105547
Iteration 6/25 | Loss: 0.00105514
Iteration 7/25 | Loss: 0.00105514
Iteration 8/25 | Loss: 0.00105514
Iteration 9/25 | Loss: 0.00105514
Iteration 10/25 | Loss: 0.00105514
Iteration 11/25 | Loss: 0.00105514
Iteration 12/25 | Loss: 0.00105514
Iteration 13/25 | Loss: 0.00105514
Iteration 14/25 | Loss: 0.00105514
Iteration 15/25 | Loss: 0.00105514
Iteration 16/25 | Loss: 0.00105514
Iteration 17/25 | Loss: 0.00105514
Iteration 18/25 | Loss: 0.00105514
Iteration 19/25 | Loss: 0.00105514
Iteration 20/25 | Loss: 0.00105514
Iteration 21/25 | Loss: 0.00105514
Iteration 22/25 | Loss: 0.00105514
Iteration 23/25 | Loss: 0.00105514
Iteration 24/25 | Loss: 0.00105514
Iteration 25/25 | Loss: 0.00105514

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.92178184
Iteration 2/25 | Loss: 0.00035484
Iteration 3/25 | Loss: 0.00035484
Iteration 4/25 | Loss: 0.00035483
Iteration 5/25 | Loss: 0.00035483
Iteration 6/25 | Loss: 0.00035483
Iteration 7/25 | Loss: 0.00035483
Iteration 8/25 | Loss: 0.00035483
Iteration 9/25 | Loss: 0.00035483
Iteration 10/25 | Loss: 0.00035483
Iteration 11/25 | Loss: 0.00035483
Iteration 12/25 | Loss: 0.00035483
Iteration 13/25 | Loss: 0.00035483
Iteration 14/25 | Loss: 0.00035483
Iteration 15/25 | Loss: 0.00035483
Iteration 16/25 | Loss: 0.00035483
Iteration 17/25 | Loss: 0.00035483
Iteration 18/25 | Loss: 0.00035483
Iteration 19/25 | Loss: 0.00035483
Iteration 20/25 | Loss: 0.00035483
Iteration 21/25 | Loss: 0.00035483
Iteration 22/25 | Loss: 0.00035483
Iteration 23/25 | Loss: 0.00035483
Iteration 24/25 | Loss: 0.00035483
Iteration 25/25 | Loss: 0.00035483

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00035483
Iteration 2/1000 | Loss: 0.00002916
Iteration 3/1000 | Loss: 0.00002265
Iteration 4/1000 | Loss: 0.00002083
Iteration 5/1000 | Loss: 0.00002006
Iteration 6/1000 | Loss: 0.00001962
Iteration 7/1000 | Loss: 0.00001915
Iteration 8/1000 | Loss: 0.00001891
Iteration 9/1000 | Loss: 0.00001872
Iteration 10/1000 | Loss: 0.00001858
Iteration 11/1000 | Loss: 0.00001858
Iteration 12/1000 | Loss: 0.00001857
Iteration 13/1000 | Loss: 0.00001856
Iteration 14/1000 | Loss: 0.00001856
Iteration 15/1000 | Loss: 0.00001856
Iteration 16/1000 | Loss: 0.00001855
Iteration 17/1000 | Loss: 0.00001855
Iteration 18/1000 | Loss: 0.00001855
Iteration 19/1000 | Loss: 0.00001855
Iteration 20/1000 | Loss: 0.00001855
Iteration 21/1000 | Loss: 0.00001855
Iteration 22/1000 | Loss: 0.00001855
Iteration 23/1000 | Loss: 0.00001855
Iteration 24/1000 | Loss: 0.00001855
Iteration 25/1000 | Loss: 0.00001854
Iteration 26/1000 | Loss: 0.00001854
Iteration 27/1000 | Loss: 0.00001854
Iteration 28/1000 | Loss: 0.00001854
Iteration 29/1000 | Loss: 0.00001854
Iteration 30/1000 | Loss: 0.00001854
Iteration 31/1000 | Loss: 0.00001854
Iteration 32/1000 | Loss: 0.00001853
Iteration 33/1000 | Loss: 0.00001853
Iteration 34/1000 | Loss: 0.00001853
Iteration 35/1000 | Loss: 0.00001853
Iteration 36/1000 | Loss: 0.00001853
Iteration 37/1000 | Loss: 0.00001852
Iteration 38/1000 | Loss: 0.00001852
Iteration 39/1000 | Loss: 0.00001852
Iteration 40/1000 | Loss: 0.00001852
Iteration 41/1000 | Loss: 0.00001852
Iteration 42/1000 | Loss: 0.00001852
Iteration 43/1000 | Loss: 0.00001852
Iteration 44/1000 | Loss: 0.00001851
Iteration 45/1000 | Loss: 0.00001851
Iteration 46/1000 | Loss: 0.00001849
Iteration 47/1000 | Loss: 0.00001847
Iteration 48/1000 | Loss: 0.00001847
Iteration 49/1000 | Loss: 0.00001847
Iteration 50/1000 | Loss: 0.00001846
Iteration 51/1000 | Loss: 0.00001846
Iteration 52/1000 | Loss: 0.00001846
Iteration 53/1000 | Loss: 0.00001846
Iteration 54/1000 | Loss: 0.00001846
Iteration 55/1000 | Loss: 0.00001845
Iteration 56/1000 | Loss: 0.00001845
Iteration 57/1000 | Loss: 0.00001845
Iteration 58/1000 | Loss: 0.00001845
Iteration 59/1000 | Loss: 0.00001844
Iteration 60/1000 | Loss: 0.00001844
Iteration 61/1000 | Loss: 0.00001844
Iteration 62/1000 | Loss: 0.00001843
Iteration 63/1000 | Loss: 0.00001842
Iteration 64/1000 | Loss: 0.00001842
Iteration 65/1000 | Loss: 0.00001841
Iteration 66/1000 | Loss: 0.00001841
Iteration 67/1000 | Loss: 0.00001841
Iteration 68/1000 | Loss: 0.00001841
Iteration 69/1000 | Loss: 0.00001841
Iteration 70/1000 | Loss: 0.00001841
Iteration 71/1000 | Loss: 0.00001841
Iteration 72/1000 | Loss: 0.00001841
Iteration 73/1000 | Loss: 0.00001840
Iteration 74/1000 | Loss: 0.00001840
Iteration 75/1000 | Loss: 0.00001840
Iteration 76/1000 | Loss: 0.00001840
Iteration 77/1000 | Loss: 0.00001840
Iteration 78/1000 | Loss: 0.00001840
Iteration 79/1000 | Loss: 0.00001839
Iteration 80/1000 | Loss: 0.00001839
Iteration 81/1000 | Loss: 0.00001839
Iteration 82/1000 | Loss: 0.00001839
Iteration 83/1000 | Loss: 0.00001839
Iteration 84/1000 | Loss: 0.00001839
Iteration 85/1000 | Loss: 0.00001839
Iteration 86/1000 | Loss: 0.00001839
Iteration 87/1000 | Loss: 0.00001839
Iteration 88/1000 | Loss: 0.00001838
Iteration 89/1000 | Loss: 0.00001838
Iteration 90/1000 | Loss: 0.00001838
Iteration 91/1000 | Loss: 0.00001838
Iteration 92/1000 | Loss: 0.00001838
Iteration 93/1000 | Loss: 0.00001838
Iteration 94/1000 | Loss: 0.00001838
Iteration 95/1000 | Loss: 0.00001837
Iteration 96/1000 | Loss: 0.00001837
Iteration 97/1000 | Loss: 0.00001837
Iteration 98/1000 | Loss: 0.00001837
Iteration 99/1000 | Loss: 0.00001837
Iteration 100/1000 | Loss: 0.00001837
Iteration 101/1000 | Loss: 0.00001837
Iteration 102/1000 | Loss: 0.00001837
Iteration 103/1000 | Loss: 0.00001837
Iteration 104/1000 | Loss: 0.00001837
Iteration 105/1000 | Loss: 0.00001837
Iteration 106/1000 | Loss: 0.00001836
Iteration 107/1000 | Loss: 0.00001836
Iteration 108/1000 | Loss: 0.00001836
Iteration 109/1000 | Loss: 0.00001836
Iteration 110/1000 | Loss: 0.00001836
Iteration 111/1000 | Loss: 0.00001836
Iteration 112/1000 | Loss: 0.00001835
Iteration 113/1000 | Loss: 0.00001835
Iteration 114/1000 | Loss: 0.00001835
Iteration 115/1000 | Loss: 0.00001835
Iteration 116/1000 | Loss: 0.00001835
Iteration 117/1000 | Loss: 0.00001835
Iteration 118/1000 | Loss: 0.00001835
Iteration 119/1000 | Loss: 0.00001835
Iteration 120/1000 | Loss: 0.00001835
Iteration 121/1000 | Loss: 0.00001835
Iteration 122/1000 | Loss: 0.00001835
Iteration 123/1000 | Loss: 0.00001835
Iteration 124/1000 | Loss: 0.00001835
Iteration 125/1000 | Loss: 0.00001835
Iteration 126/1000 | Loss: 0.00001835
Iteration 127/1000 | Loss: 0.00001835
Iteration 128/1000 | Loss: 0.00001835
Iteration 129/1000 | Loss: 0.00001835
Iteration 130/1000 | Loss: 0.00001835
Iteration 131/1000 | Loss: 0.00001835
Iteration 132/1000 | Loss: 0.00001835
Iteration 133/1000 | Loss: 0.00001835
Iteration 134/1000 | Loss: 0.00001835
Iteration 135/1000 | Loss: 0.00001835
Iteration 136/1000 | Loss: 0.00001835
Iteration 137/1000 | Loss: 0.00001835
Iteration 138/1000 | Loss: 0.00001835
Iteration 139/1000 | Loss: 0.00001835
Iteration 140/1000 | Loss: 0.00001835
Iteration 141/1000 | Loss: 0.00001835
Iteration 142/1000 | Loss: 0.00001835
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [1.834788781707175e-05, 1.834788781707175e-05, 1.834788781707175e-05, 1.834788781707175e-05, 1.834788781707175e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.834788781707175e-05

Optimization complete. Final v2v error: 3.5796945095062256 mm

Highest mean error: 3.680274724960327 mm for frame 67

Lowest mean error: 3.463127613067627 mm for frame 149

Saving results

Total time: 30.055009603500366
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01128978
Iteration 2/25 | Loss: 0.00208768
Iteration 3/25 | Loss: 0.00156509
Iteration 4/25 | Loss: 0.00144440
Iteration 5/25 | Loss: 0.00147143
Iteration 6/25 | Loss: 0.00144874
Iteration 7/25 | Loss: 0.00141174
Iteration 8/25 | Loss: 0.00135790
Iteration 9/25 | Loss: 0.00131590
Iteration 10/25 | Loss: 0.00130448
Iteration 11/25 | Loss: 0.00129828
Iteration 12/25 | Loss: 0.00129356
Iteration 13/25 | Loss: 0.00129117
Iteration 14/25 | Loss: 0.00128798
Iteration 15/25 | Loss: 0.00129046
Iteration 16/25 | Loss: 0.00129031
Iteration 17/25 | Loss: 0.00129010
Iteration 18/25 | Loss: 0.00129032
Iteration 19/25 | Loss: 0.00128987
Iteration 20/25 | Loss: 0.00128705
Iteration 21/25 | Loss: 0.00128485
Iteration 22/25 | Loss: 0.00128404
Iteration 23/25 | Loss: 0.00128497
Iteration 24/25 | Loss: 0.00128353
Iteration 25/25 | Loss: 0.00128684

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29745638
Iteration 2/25 | Loss: 0.00172735
Iteration 3/25 | Loss: 0.00172735
Iteration 4/25 | Loss: 0.00172735
Iteration 5/25 | Loss: 0.00172735
Iteration 6/25 | Loss: 0.00172735
Iteration 7/25 | Loss: 0.00172735
Iteration 8/25 | Loss: 0.00172735
Iteration 9/25 | Loss: 0.00172735
Iteration 10/25 | Loss: 0.00172735
Iteration 11/25 | Loss: 0.00172735
Iteration 12/25 | Loss: 0.00172735
Iteration 13/25 | Loss: 0.00172735
Iteration 14/25 | Loss: 0.00172735
Iteration 15/25 | Loss: 0.00172735
Iteration 16/25 | Loss: 0.00172735
Iteration 17/25 | Loss: 0.00172735
Iteration 18/25 | Loss: 0.00172735
Iteration 19/25 | Loss: 0.00172735
Iteration 20/25 | Loss: 0.00172735
Iteration 21/25 | Loss: 0.00172735
Iteration 22/25 | Loss: 0.00172735
Iteration 23/25 | Loss: 0.00172735
Iteration 24/25 | Loss: 0.00172735
Iteration 25/25 | Loss: 0.00172735

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00172735
Iteration 2/1000 | Loss: 0.00033896
Iteration 3/1000 | Loss: 0.00271235
Iteration 4/1000 | Loss: 0.00184345
Iteration 5/1000 | Loss: 0.00027917
Iteration 6/1000 | Loss: 0.00016505
Iteration 7/1000 | Loss: 0.00126176
Iteration 8/1000 | Loss: 0.00107414
Iteration 9/1000 | Loss: 0.00015420
Iteration 10/1000 | Loss: 0.00067441
Iteration 11/1000 | Loss: 0.00113390
Iteration 12/1000 | Loss: 0.00038118
Iteration 13/1000 | Loss: 0.00022820
Iteration 14/1000 | Loss: 0.00008823
Iteration 15/1000 | Loss: 0.00015444
Iteration 16/1000 | Loss: 0.00067017
Iteration 17/1000 | Loss: 0.00016180
Iteration 18/1000 | Loss: 0.00015204
Iteration 19/1000 | Loss: 0.00019998
Iteration 20/1000 | Loss: 0.00016171
Iteration 21/1000 | Loss: 0.00016418
Iteration 22/1000 | Loss: 0.00010796
Iteration 23/1000 | Loss: 0.00014605
Iteration 24/1000 | Loss: 0.00017441
Iteration 25/1000 | Loss: 0.00015034
Iteration 26/1000 | Loss: 0.00021174
Iteration 27/1000 | Loss: 0.00061918
Iteration 28/1000 | Loss: 0.00045417
Iteration 29/1000 | Loss: 0.00010377
Iteration 30/1000 | Loss: 0.00011757
Iteration 31/1000 | Loss: 0.00014328
Iteration 32/1000 | Loss: 0.00018107
Iteration 33/1000 | Loss: 0.00015158
Iteration 34/1000 | Loss: 0.00013645
Iteration 35/1000 | Loss: 0.00019589
Iteration 36/1000 | Loss: 0.00018918
Iteration 37/1000 | Loss: 0.00018159
Iteration 38/1000 | Loss: 0.00019385
Iteration 39/1000 | Loss: 0.00066270
Iteration 40/1000 | Loss: 0.00056945
Iteration 41/1000 | Loss: 0.00086620
Iteration 42/1000 | Loss: 0.00047661
Iteration 43/1000 | Loss: 0.00054266
Iteration 44/1000 | Loss: 0.00024144
Iteration 45/1000 | Loss: 0.00015898
Iteration 46/1000 | Loss: 0.00012929
Iteration 47/1000 | Loss: 0.00014764
Iteration 48/1000 | Loss: 0.00013791
Iteration 49/1000 | Loss: 0.00021061
Iteration 50/1000 | Loss: 0.00015987
Iteration 51/1000 | Loss: 0.00017486
Iteration 52/1000 | Loss: 0.00015567
Iteration 53/1000 | Loss: 0.00019768
Iteration 54/1000 | Loss: 0.00019009
Iteration 55/1000 | Loss: 0.00019909
Iteration 56/1000 | Loss: 0.00018449
Iteration 57/1000 | Loss: 0.00017268
Iteration 58/1000 | Loss: 0.00028699
Iteration 59/1000 | Loss: 0.00029281
Iteration 60/1000 | Loss: 0.00005749
Iteration 61/1000 | Loss: 0.00011679
Iteration 62/1000 | Loss: 0.00012784
Iteration 63/1000 | Loss: 0.00012778
Iteration 64/1000 | Loss: 0.00006541
Iteration 65/1000 | Loss: 0.00007497
Iteration 66/1000 | Loss: 0.00013779
Iteration 67/1000 | Loss: 0.00009785
Iteration 68/1000 | Loss: 0.00014680
Iteration 69/1000 | Loss: 0.00009485
Iteration 70/1000 | Loss: 0.00014704
Iteration 71/1000 | Loss: 0.00010756
Iteration 72/1000 | Loss: 0.00010570
Iteration 73/1000 | Loss: 0.00007326
Iteration 74/1000 | Loss: 0.00013630
Iteration 75/1000 | Loss: 0.00009152
Iteration 76/1000 | Loss: 0.00006326
Iteration 77/1000 | Loss: 0.00010809
Iteration 78/1000 | Loss: 0.00009678
Iteration 79/1000 | Loss: 0.00014161
Iteration 80/1000 | Loss: 0.00012565
Iteration 81/1000 | Loss: 0.00017525
Iteration 82/1000 | Loss: 0.00011241
Iteration 83/1000 | Loss: 0.00014378
Iteration 84/1000 | Loss: 0.00011197
Iteration 85/1000 | Loss: 0.00014040
Iteration 86/1000 | Loss: 0.00012054
Iteration 87/1000 | Loss: 0.00013072
Iteration 88/1000 | Loss: 0.00011607
Iteration 89/1000 | Loss: 0.00013168
Iteration 90/1000 | Loss: 0.00012693
Iteration 91/1000 | Loss: 0.00013683
Iteration 92/1000 | Loss: 0.00011423
Iteration 93/1000 | Loss: 0.00014810
Iteration 94/1000 | Loss: 0.00014967
Iteration 95/1000 | Loss: 0.00015240
Iteration 96/1000 | Loss: 0.00012567
Iteration 97/1000 | Loss: 0.00014471
Iteration 98/1000 | Loss: 0.00015640
Iteration 99/1000 | Loss: 0.00012515
Iteration 100/1000 | Loss: 0.00012624
Iteration 101/1000 | Loss: 0.00013609
Iteration 102/1000 | Loss: 0.00012092
Iteration 103/1000 | Loss: 0.00015335
Iteration 104/1000 | Loss: 0.00032519
Iteration 105/1000 | Loss: 0.00020186
Iteration 106/1000 | Loss: 0.00009657
Iteration 107/1000 | Loss: 0.00011262
Iteration 108/1000 | Loss: 0.00020594
Iteration 109/1000 | Loss: 0.00012034
Iteration 110/1000 | Loss: 0.00013572
Iteration 111/1000 | Loss: 0.00024383
Iteration 112/1000 | Loss: 0.00020259
Iteration 113/1000 | Loss: 0.00023707
Iteration 114/1000 | Loss: 0.00032529
Iteration 115/1000 | Loss: 0.00021630
Iteration 116/1000 | Loss: 0.00034396
Iteration 117/1000 | Loss: 0.00014677
Iteration 118/1000 | Loss: 0.00020722
Iteration 119/1000 | Loss: 0.00024041
Iteration 120/1000 | Loss: 0.00004701
Iteration 121/1000 | Loss: 0.00020921
Iteration 122/1000 | Loss: 0.00017332
Iteration 123/1000 | Loss: 0.00019798
Iteration 124/1000 | Loss: 0.00012631
Iteration 125/1000 | Loss: 0.00004630
Iteration 126/1000 | Loss: 0.00004573
Iteration 127/1000 | Loss: 0.00005142
Iteration 128/1000 | Loss: 0.00004592
Iteration 129/1000 | Loss: 0.00004980
Iteration 130/1000 | Loss: 0.00005133
Iteration 131/1000 | Loss: 0.00005085
Iteration 132/1000 | Loss: 0.00005268
Iteration 133/1000 | Loss: 0.00005092
Iteration 134/1000 | Loss: 0.00004944
Iteration 135/1000 | Loss: 0.00004878
Iteration 136/1000 | Loss: 0.00004950
Iteration 137/1000 | Loss: 0.00005019
Iteration 138/1000 | Loss: 0.00005056
Iteration 139/1000 | Loss: 0.00004986
Iteration 140/1000 | Loss: 0.00005062
Iteration 141/1000 | Loss: 0.00003980
Iteration 142/1000 | Loss: 0.00004810
Iteration 143/1000 | Loss: 0.00006149
Iteration 144/1000 | Loss: 0.00015701
Iteration 145/1000 | Loss: 0.00018046
Iteration 146/1000 | Loss: 0.00015519
Iteration 147/1000 | Loss: 0.00007217
Iteration 148/1000 | Loss: 0.00004133
Iteration 149/1000 | Loss: 0.00004030
Iteration 150/1000 | Loss: 0.00003943
Iteration 151/1000 | Loss: 0.00049066
Iteration 152/1000 | Loss: 0.00122808
Iteration 153/1000 | Loss: 0.00113682
Iteration 154/1000 | Loss: 0.00092433
Iteration 155/1000 | Loss: 0.00007953
Iteration 156/1000 | Loss: 0.00005000
Iteration 157/1000 | Loss: 0.00004431
Iteration 158/1000 | Loss: 0.00003909
Iteration 159/1000 | Loss: 0.00003639
Iteration 160/1000 | Loss: 0.00003478
Iteration 161/1000 | Loss: 0.00003347
Iteration 162/1000 | Loss: 0.00003288
Iteration 163/1000 | Loss: 0.00003241
Iteration 164/1000 | Loss: 0.00003205
Iteration 165/1000 | Loss: 0.00028063
Iteration 166/1000 | Loss: 0.00016307
Iteration 167/1000 | Loss: 0.00003187
Iteration 168/1000 | Loss: 0.00024781
Iteration 169/1000 | Loss: 0.00040832
Iteration 170/1000 | Loss: 0.00004980
Iteration 171/1000 | Loss: 0.00065939
Iteration 172/1000 | Loss: 0.00016259
Iteration 173/1000 | Loss: 0.00027759
Iteration 174/1000 | Loss: 0.00066213
Iteration 175/1000 | Loss: 0.00031167
Iteration 176/1000 | Loss: 0.00011367
Iteration 177/1000 | Loss: 0.00006448
Iteration 178/1000 | Loss: 0.00003759
Iteration 179/1000 | Loss: 0.00003552
Iteration 180/1000 | Loss: 0.00053976
Iteration 181/1000 | Loss: 0.00019932
Iteration 182/1000 | Loss: 0.00003562
Iteration 183/1000 | Loss: 0.00003435
Iteration 184/1000 | Loss: 0.00003298
Iteration 185/1000 | Loss: 0.00052032
Iteration 186/1000 | Loss: 0.00035681
Iteration 187/1000 | Loss: 0.00017448
Iteration 188/1000 | Loss: 0.00003273
Iteration 189/1000 | Loss: 0.00003161
Iteration 190/1000 | Loss: 0.00003125
Iteration 191/1000 | Loss: 0.00003097
Iteration 192/1000 | Loss: 0.00003077
Iteration 193/1000 | Loss: 0.00003076
Iteration 194/1000 | Loss: 0.00003075
Iteration 195/1000 | Loss: 0.00003063
Iteration 196/1000 | Loss: 0.00003043
Iteration 197/1000 | Loss: 0.00003029
Iteration 198/1000 | Loss: 0.00003022
Iteration 199/1000 | Loss: 0.00003021
Iteration 200/1000 | Loss: 0.00044340
Iteration 201/1000 | Loss: 0.00022574
Iteration 202/1000 | Loss: 0.00033785
Iteration 203/1000 | Loss: 0.00003159
Iteration 204/1000 | Loss: 0.00003028
Iteration 205/1000 | Loss: 0.00002946
Iteration 206/1000 | Loss: 0.00002894
Iteration 207/1000 | Loss: 0.00002850
Iteration 208/1000 | Loss: 0.00002818
Iteration 209/1000 | Loss: 0.00002808
Iteration 210/1000 | Loss: 0.00002806
Iteration 211/1000 | Loss: 0.00002805
Iteration 212/1000 | Loss: 0.00002802
Iteration 213/1000 | Loss: 0.00002801
Iteration 214/1000 | Loss: 0.00002801
Iteration 215/1000 | Loss: 0.00002800
Iteration 216/1000 | Loss: 0.00002800
Iteration 217/1000 | Loss: 0.00002799
Iteration 218/1000 | Loss: 0.00002799
Iteration 219/1000 | Loss: 0.00002798
Iteration 220/1000 | Loss: 0.00002796
Iteration 221/1000 | Loss: 0.00002795
Iteration 222/1000 | Loss: 0.00002795
Iteration 223/1000 | Loss: 0.00002795
Iteration 224/1000 | Loss: 0.00002794
Iteration 225/1000 | Loss: 0.00002794
Iteration 226/1000 | Loss: 0.00002793
Iteration 227/1000 | Loss: 0.00002793
Iteration 228/1000 | Loss: 0.00002793
Iteration 229/1000 | Loss: 0.00002792
Iteration 230/1000 | Loss: 0.00002792
Iteration 231/1000 | Loss: 0.00002792
Iteration 232/1000 | Loss: 0.00002792
Iteration 233/1000 | Loss: 0.00002792
Iteration 234/1000 | Loss: 0.00002792
Iteration 235/1000 | Loss: 0.00002792
Iteration 236/1000 | Loss: 0.00002791
Iteration 237/1000 | Loss: 0.00002791
Iteration 238/1000 | Loss: 0.00002791
Iteration 239/1000 | Loss: 0.00002790
Iteration 240/1000 | Loss: 0.00002790
Iteration 241/1000 | Loss: 0.00002790
Iteration 242/1000 | Loss: 0.00002789
Iteration 243/1000 | Loss: 0.00002789
Iteration 244/1000 | Loss: 0.00002789
Iteration 245/1000 | Loss: 0.00002788
Iteration 246/1000 | Loss: 0.00002788
Iteration 247/1000 | Loss: 0.00002788
Iteration 248/1000 | Loss: 0.00002788
Iteration 249/1000 | Loss: 0.00002788
Iteration 250/1000 | Loss: 0.00002788
Iteration 251/1000 | Loss: 0.00002788
Iteration 252/1000 | Loss: 0.00002788
Iteration 253/1000 | Loss: 0.00002788
Iteration 254/1000 | Loss: 0.00002788
Iteration 255/1000 | Loss: 0.00002788
Iteration 256/1000 | Loss: 0.00002788
Iteration 257/1000 | Loss: 0.00002788
Iteration 258/1000 | Loss: 0.00002787
Iteration 259/1000 | Loss: 0.00002787
Iteration 260/1000 | Loss: 0.00002787
Iteration 261/1000 | Loss: 0.00002786
Iteration 262/1000 | Loss: 0.00002786
Iteration 263/1000 | Loss: 0.00002786
Iteration 264/1000 | Loss: 0.00002786
Iteration 265/1000 | Loss: 0.00002786
Iteration 266/1000 | Loss: 0.00002786
Iteration 267/1000 | Loss: 0.00002786
Iteration 268/1000 | Loss: 0.00002786
Iteration 269/1000 | Loss: 0.00002786
Iteration 270/1000 | Loss: 0.00002786
Iteration 271/1000 | Loss: 0.00002786
Iteration 272/1000 | Loss: 0.00002786
Iteration 273/1000 | Loss: 0.00002785
Iteration 274/1000 | Loss: 0.00002785
Iteration 275/1000 | Loss: 0.00002785
Iteration 276/1000 | Loss: 0.00002785
Iteration 277/1000 | Loss: 0.00002785
Iteration 278/1000 | Loss: 0.00002785
Iteration 279/1000 | Loss: 0.00002785
Iteration 280/1000 | Loss: 0.00002785
Iteration 281/1000 | Loss: 0.00002785
Iteration 282/1000 | Loss: 0.00002785
Iteration 283/1000 | Loss: 0.00002785
Iteration 284/1000 | Loss: 0.00002785
Iteration 285/1000 | Loss: 0.00002785
Iteration 286/1000 | Loss: 0.00002785
Iteration 287/1000 | Loss: 0.00002785
Iteration 288/1000 | Loss: 0.00002785
Iteration 289/1000 | Loss: 0.00002785
Iteration 290/1000 | Loss: 0.00002785
Iteration 291/1000 | Loss: 0.00002784
Iteration 292/1000 | Loss: 0.00002784
Iteration 293/1000 | Loss: 0.00002784
Iteration 294/1000 | Loss: 0.00002784
Iteration 295/1000 | Loss: 0.00002784
Iteration 296/1000 | Loss: 0.00002784
Iteration 297/1000 | Loss: 0.00002784
Iteration 298/1000 | Loss: 0.00002784
Iteration 299/1000 | Loss: 0.00002784
Iteration 300/1000 | Loss: 0.00002784
Iteration 301/1000 | Loss: 0.00002784
Iteration 302/1000 | Loss: 0.00002784
Iteration 303/1000 | Loss: 0.00002784
Iteration 304/1000 | Loss: 0.00002784
Iteration 305/1000 | Loss: 0.00002784
Iteration 306/1000 | Loss: 0.00002784
Iteration 307/1000 | Loss: 0.00002784
Iteration 308/1000 | Loss: 0.00002784
Iteration 309/1000 | Loss: 0.00002783
Iteration 310/1000 | Loss: 0.00002783
Iteration 311/1000 | Loss: 0.00002783
Iteration 312/1000 | Loss: 0.00002783
Iteration 313/1000 | Loss: 0.00002783
Iteration 314/1000 | Loss: 0.00002783
Iteration 315/1000 | Loss: 0.00002783
Iteration 316/1000 | Loss: 0.00002783
Iteration 317/1000 | Loss: 0.00002783
Iteration 318/1000 | Loss: 0.00002783
Iteration 319/1000 | Loss: 0.00002783
Iteration 320/1000 | Loss: 0.00002783
Iteration 321/1000 | Loss: 0.00002783
Iteration 322/1000 | Loss: 0.00002783
Iteration 323/1000 | Loss: 0.00002783
Iteration 324/1000 | Loss: 0.00002783
Iteration 325/1000 | Loss: 0.00002783
Iteration 326/1000 | Loss: 0.00002783
Iteration 327/1000 | Loss: 0.00002783
Iteration 328/1000 | Loss: 0.00002783
Iteration 329/1000 | Loss: 0.00002783
Iteration 330/1000 | Loss: 0.00002782
Iteration 331/1000 | Loss: 0.00002782
Iteration 332/1000 | Loss: 0.00002782
Iteration 333/1000 | Loss: 0.00002782
Iteration 334/1000 | Loss: 0.00002782
Iteration 335/1000 | Loss: 0.00002782
Iteration 336/1000 | Loss: 0.00002782
Iteration 337/1000 | Loss: 0.00002782
Iteration 338/1000 | Loss: 0.00002782
Iteration 339/1000 | Loss: 0.00002782
Iteration 340/1000 | Loss: 0.00002782
Iteration 341/1000 | Loss: 0.00002782
Iteration 342/1000 | Loss: 0.00002782
Iteration 343/1000 | Loss: 0.00002782
Iteration 344/1000 | Loss: 0.00002782
Iteration 345/1000 | Loss: 0.00002782
Iteration 346/1000 | Loss: 0.00002782
Iteration 347/1000 | Loss: 0.00002782
Iteration 348/1000 | Loss: 0.00002782
Iteration 349/1000 | Loss: 0.00002782
Iteration 350/1000 | Loss: 0.00002782
Iteration 351/1000 | Loss: 0.00002782
Iteration 352/1000 | Loss: 0.00002782
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 352. Stopping optimization.
Last 5 losses: [2.781958937703166e-05, 2.781958937703166e-05, 2.781958937703166e-05, 2.781958937703166e-05, 2.781958937703166e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.781958937703166e-05

Optimization complete. Final v2v error: 4.127713203430176 mm

Highest mean error: 11.811944007873535 mm for frame 202

Lowest mean error: 3.2156145572662354 mm for frame 34

Saving results

Total time: 396.9299507141113
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01038194
Iteration 2/25 | Loss: 0.00270150
Iteration 3/25 | Loss: 0.00173097
Iteration 4/25 | Loss: 0.00150669
Iteration 5/25 | Loss: 0.00136175
Iteration 6/25 | Loss: 0.00150061
Iteration 7/25 | Loss: 0.00140218
Iteration 8/25 | Loss: 0.00123427
Iteration 9/25 | Loss: 0.00118491
Iteration 10/25 | Loss: 0.00118024
Iteration 11/25 | Loss: 0.00115021
Iteration 12/25 | Loss: 0.00112897
Iteration 13/25 | Loss: 0.00114272
Iteration 14/25 | Loss: 0.00111473
Iteration 15/25 | Loss: 0.00110057
Iteration 16/25 | Loss: 0.00110553
Iteration 17/25 | Loss: 0.00113249
Iteration 18/25 | Loss: 0.00110119
Iteration 19/25 | Loss: 0.00107463
Iteration 20/25 | Loss: 0.00106848
Iteration 21/25 | Loss: 0.00106440
Iteration 22/25 | Loss: 0.00106422
Iteration 23/25 | Loss: 0.00106361
Iteration 24/25 | Loss: 0.00106431
Iteration 25/25 | Loss: 0.00106239

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47317088
Iteration 2/25 | Loss: 0.00161469
Iteration 3/25 | Loss: 0.00125958
Iteration 4/25 | Loss: 0.00125958
Iteration 5/25 | Loss: 0.00125958
Iteration 6/25 | Loss: 0.00125958
Iteration 7/25 | Loss: 0.00125958
Iteration 8/25 | Loss: 0.00125957
Iteration 9/25 | Loss: 0.00125957
Iteration 10/25 | Loss: 0.00125957
Iteration 11/25 | Loss: 0.00125957
Iteration 12/25 | Loss: 0.00125957
Iteration 13/25 | Loss: 0.00125957
Iteration 14/25 | Loss: 0.00125957
Iteration 15/25 | Loss: 0.00125957
Iteration 16/25 | Loss: 0.00125957
Iteration 17/25 | Loss: 0.00125957
Iteration 18/25 | Loss: 0.00125957
Iteration 19/25 | Loss: 0.00125957
Iteration 20/25 | Loss: 0.00125957
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0012595742009580135, 0.0012595742009580135, 0.0012595742009580135, 0.0012595742009580135, 0.0012595742009580135]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012595742009580135

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125957
Iteration 2/1000 | Loss: 0.00090020
Iteration 3/1000 | Loss: 0.00196513
Iteration 4/1000 | Loss: 0.00035411
Iteration 5/1000 | Loss: 0.00008270
Iteration 6/1000 | Loss: 0.00047159
Iteration 7/1000 | Loss: 0.00059774
Iteration 8/1000 | Loss: 0.00038858
Iteration 9/1000 | Loss: 0.00033383
Iteration 10/1000 | Loss: 0.00025186
Iteration 11/1000 | Loss: 0.00020644
Iteration 12/1000 | Loss: 0.00006392
Iteration 13/1000 | Loss: 0.00271551
Iteration 14/1000 | Loss: 0.00054440
Iteration 15/1000 | Loss: 0.00141615
Iteration 16/1000 | Loss: 0.00029475
Iteration 17/1000 | Loss: 0.00016978
Iteration 18/1000 | Loss: 0.00028427
Iteration 19/1000 | Loss: 0.00055754
Iteration 20/1000 | Loss: 0.00047539
Iteration 21/1000 | Loss: 0.00051180
Iteration 22/1000 | Loss: 0.00051848
Iteration 23/1000 | Loss: 0.00103048
Iteration 24/1000 | Loss: 0.00048451
Iteration 25/1000 | Loss: 0.00046322
Iteration 26/1000 | Loss: 0.00046377
Iteration 27/1000 | Loss: 0.00020800
Iteration 28/1000 | Loss: 0.00011049
Iteration 29/1000 | Loss: 0.00027456
Iteration 30/1000 | Loss: 0.00021265
Iteration 31/1000 | Loss: 0.00020460
Iteration 32/1000 | Loss: 0.00006631
Iteration 33/1000 | Loss: 0.00042902
Iteration 34/1000 | Loss: 0.00034461
Iteration 35/1000 | Loss: 0.00006910
Iteration 36/1000 | Loss: 0.00006283
Iteration 37/1000 | Loss: 0.00020942
Iteration 38/1000 | Loss: 0.00017240
Iteration 39/1000 | Loss: 0.00033089
Iteration 40/1000 | Loss: 0.00037132
Iteration 41/1000 | Loss: 0.00024139
Iteration 42/1000 | Loss: 0.00009630
Iteration 43/1000 | Loss: 0.00006567
Iteration 44/1000 | Loss: 0.00008290
Iteration 45/1000 | Loss: 0.00006872
Iteration 46/1000 | Loss: 0.00026768
Iteration 47/1000 | Loss: 0.00091816
Iteration 48/1000 | Loss: 0.00050876
Iteration 49/1000 | Loss: 0.00088043
Iteration 50/1000 | Loss: 0.00060442
Iteration 51/1000 | Loss: 0.00066382
Iteration 52/1000 | Loss: 0.00044017
Iteration 53/1000 | Loss: 0.00096138
Iteration 54/1000 | Loss: 0.00027449
Iteration 55/1000 | Loss: 0.00038291
Iteration 56/1000 | Loss: 0.00014801
Iteration 57/1000 | Loss: 0.00033362
Iteration 58/1000 | Loss: 0.00014893
Iteration 59/1000 | Loss: 0.00019840
Iteration 60/1000 | Loss: 0.00012666
Iteration 61/1000 | Loss: 0.00088689
Iteration 62/1000 | Loss: 0.00087596
Iteration 63/1000 | Loss: 0.00068737
Iteration 64/1000 | Loss: 0.00037327
Iteration 65/1000 | Loss: 0.00014912
Iteration 66/1000 | Loss: 0.00090925
Iteration 67/1000 | Loss: 0.00007120
Iteration 68/1000 | Loss: 0.00004515
Iteration 69/1000 | Loss: 0.00004764
Iteration 70/1000 | Loss: 0.00005069
Iteration 71/1000 | Loss: 0.00079407
Iteration 72/1000 | Loss: 0.00006259
Iteration 73/1000 | Loss: 0.00009832
Iteration 74/1000 | Loss: 0.00067423
Iteration 75/1000 | Loss: 0.00047137
Iteration 76/1000 | Loss: 0.00007693
Iteration 77/1000 | Loss: 0.00004614
Iteration 78/1000 | Loss: 0.00003787
Iteration 79/1000 | Loss: 0.00082024
Iteration 80/1000 | Loss: 0.00014789
Iteration 81/1000 | Loss: 0.00012556
Iteration 82/1000 | Loss: 0.00014688
Iteration 83/1000 | Loss: 0.00003294
Iteration 84/1000 | Loss: 0.00002774
Iteration 85/1000 | Loss: 0.00003190
Iteration 86/1000 | Loss: 0.00002613
Iteration 87/1000 | Loss: 0.00004078
Iteration 88/1000 | Loss: 0.00014547
Iteration 89/1000 | Loss: 0.00016926
Iteration 90/1000 | Loss: 0.00014321
Iteration 91/1000 | Loss: 0.00003018
Iteration 92/1000 | Loss: 0.00003817
Iteration 93/1000 | Loss: 0.00003629
Iteration 94/1000 | Loss: 0.00003424
Iteration 95/1000 | Loss: 0.00003338
Iteration 96/1000 | Loss: 0.00003326
Iteration 97/1000 | Loss: 0.00026501
Iteration 98/1000 | Loss: 0.00013486
Iteration 99/1000 | Loss: 0.00003860
Iteration 100/1000 | Loss: 0.00003690
Iteration 101/1000 | Loss: 0.00003413
Iteration 102/1000 | Loss: 0.00003600
Iteration 103/1000 | Loss: 0.00005008
Iteration 104/1000 | Loss: 0.00004063
Iteration 105/1000 | Loss: 0.00026196
Iteration 106/1000 | Loss: 0.00026321
Iteration 107/1000 | Loss: 0.00005373
Iteration 108/1000 | Loss: 0.00002950
Iteration 109/1000 | Loss: 0.00002613
Iteration 110/1000 | Loss: 0.00002954
Iteration 111/1000 | Loss: 0.00004201
Iteration 112/1000 | Loss: 0.00002576
Iteration 113/1000 | Loss: 0.00004212
Iteration 114/1000 | Loss: 0.00002486
Iteration 115/1000 | Loss: 0.00003802
Iteration 116/1000 | Loss: 0.00002104
Iteration 117/1000 | Loss: 0.00002861
Iteration 118/1000 | Loss: 0.00002983
Iteration 119/1000 | Loss: 0.00001643
Iteration 120/1000 | Loss: 0.00001514
Iteration 121/1000 | Loss: 0.00001447
Iteration 122/1000 | Loss: 0.00001387
Iteration 123/1000 | Loss: 0.00001339
Iteration 124/1000 | Loss: 0.00001307
Iteration 125/1000 | Loss: 0.00001279
Iteration 126/1000 | Loss: 0.00001269
Iteration 127/1000 | Loss: 0.00012100
Iteration 128/1000 | Loss: 0.00012297
Iteration 129/1000 | Loss: 0.00001699
Iteration 130/1000 | Loss: 0.00001271
Iteration 131/1000 | Loss: 0.00001245
Iteration 132/1000 | Loss: 0.00001241
Iteration 133/1000 | Loss: 0.00001241
Iteration 134/1000 | Loss: 0.00001241
Iteration 135/1000 | Loss: 0.00001241
Iteration 136/1000 | Loss: 0.00001241
Iteration 137/1000 | Loss: 0.00001240
Iteration 138/1000 | Loss: 0.00001240
Iteration 139/1000 | Loss: 0.00001240
Iteration 140/1000 | Loss: 0.00001239
Iteration 141/1000 | Loss: 0.00001239
Iteration 142/1000 | Loss: 0.00001239
Iteration 143/1000 | Loss: 0.00001239
Iteration 144/1000 | Loss: 0.00001239
Iteration 145/1000 | Loss: 0.00001239
Iteration 146/1000 | Loss: 0.00001238
Iteration 147/1000 | Loss: 0.00001238
Iteration 148/1000 | Loss: 0.00001237
Iteration 149/1000 | Loss: 0.00001237
Iteration 150/1000 | Loss: 0.00001237
Iteration 151/1000 | Loss: 0.00001237
Iteration 152/1000 | Loss: 0.00001237
Iteration 153/1000 | Loss: 0.00001237
Iteration 154/1000 | Loss: 0.00001237
Iteration 155/1000 | Loss: 0.00001237
Iteration 156/1000 | Loss: 0.00001237
Iteration 157/1000 | Loss: 0.00001237
Iteration 158/1000 | Loss: 0.00001236
Iteration 159/1000 | Loss: 0.00001236
Iteration 160/1000 | Loss: 0.00001234
Iteration 161/1000 | Loss: 0.00001234
Iteration 162/1000 | Loss: 0.00001234
Iteration 163/1000 | Loss: 0.00001234
Iteration 164/1000 | Loss: 0.00001233
Iteration 165/1000 | Loss: 0.00001233
Iteration 166/1000 | Loss: 0.00001232
Iteration 167/1000 | Loss: 0.00001232
Iteration 168/1000 | Loss: 0.00012646
Iteration 169/1000 | Loss: 0.00008202
Iteration 170/1000 | Loss: 0.00001695
Iteration 171/1000 | Loss: 0.00012340
Iteration 172/1000 | Loss: 0.00008281
Iteration 173/1000 | Loss: 0.00012150
Iteration 174/1000 | Loss: 0.00001699
Iteration 175/1000 | Loss: 0.00001415
Iteration 176/1000 | Loss: 0.00001340
Iteration 177/1000 | Loss: 0.00023708
Iteration 178/1000 | Loss: 0.00012273
Iteration 179/1000 | Loss: 0.00014431
Iteration 180/1000 | Loss: 0.00016452
Iteration 181/1000 | Loss: 0.00014671
Iteration 182/1000 | Loss: 0.00001842
Iteration 183/1000 | Loss: 0.00020322
Iteration 184/1000 | Loss: 0.00014523
Iteration 185/1000 | Loss: 0.00021728
Iteration 186/1000 | Loss: 0.00031954
Iteration 187/1000 | Loss: 0.00017255
Iteration 188/1000 | Loss: 0.00006130
Iteration 189/1000 | Loss: 0.00018393
Iteration 190/1000 | Loss: 0.00017800
Iteration 191/1000 | Loss: 0.00004378
Iteration 192/1000 | Loss: 0.00002094
Iteration 193/1000 | Loss: 0.00018810
Iteration 194/1000 | Loss: 0.00005213
Iteration 195/1000 | Loss: 0.00009793
Iteration 196/1000 | Loss: 0.00006509
Iteration 197/1000 | Loss: 0.00007726
Iteration 198/1000 | Loss: 0.00002064
Iteration 199/1000 | Loss: 0.00001649
Iteration 200/1000 | Loss: 0.00001503
Iteration 201/1000 | Loss: 0.00001443
Iteration 202/1000 | Loss: 0.00001409
Iteration 203/1000 | Loss: 0.00001365
Iteration 204/1000 | Loss: 0.00002572
Iteration 205/1000 | Loss: 0.00001382
Iteration 206/1000 | Loss: 0.00001353
Iteration 207/1000 | Loss: 0.00001342
Iteration 208/1000 | Loss: 0.00001332
Iteration 209/1000 | Loss: 0.00001331
Iteration 210/1000 | Loss: 0.00001990
Iteration 211/1000 | Loss: 0.00001456
Iteration 212/1000 | Loss: 0.00027597
Iteration 213/1000 | Loss: 0.00002715
Iteration 214/1000 | Loss: 0.00001843
Iteration 215/1000 | Loss: 0.00001636
Iteration 216/1000 | Loss: 0.00001489
Iteration 217/1000 | Loss: 0.00001345
Iteration 218/1000 | Loss: 0.00001267
Iteration 219/1000 | Loss: 0.00001207
Iteration 220/1000 | Loss: 0.00001156
Iteration 221/1000 | Loss: 0.00001126
Iteration 222/1000 | Loss: 0.00001124
Iteration 223/1000 | Loss: 0.00001113
Iteration 224/1000 | Loss: 0.00001112
Iteration 225/1000 | Loss: 0.00001111
Iteration 226/1000 | Loss: 0.00001109
Iteration 227/1000 | Loss: 0.00001097
Iteration 228/1000 | Loss: 0.00001097
Iteration 229/1000 | Loss: 0.00001096
Iteration 230/1000 | Loss: 0.00001096
Iteration 231/1000 | Loss: 0.00001095
Iteration 232/1000 | Loss: 0.00001095
Iteration 233/1000 | Loss: 0.00001094
Iteration 234/1000 | Loss: 0.00001094
Iteration 235/1000 | Loss: 0.00001094
Iteration 236/1000 | Loss: 0.00001093
Iteration 237/1000 | Loss: 0.00001093
Iteration 238/1000 | Loss: 0.00001092
Iteration 239/1000 | Loss: 0.00001091
Iteration 240/1000 | Loss: 0.00001090
Iteration 241/1000 | Loss: 0.00001089
Iteration 242/1000 | Loss: 0.00001088
Iteration 243/1000 | Loss: 0.00001087
Iteration 244/1000 | Loss: 0.00001087
Iteration 245/1000 | Loss: 0.00001084
Iteration 246/1000 | Loss: 0.00001084
Iteration 247/1000 | Loss: 0.00001084
Iteration 248/1000 | Loss: 0.00001083
Iteration 249/1000 | Loss: 0.00001083
Iteration 250/1000 | Loss: 0.00001083
Iteration 251/1000 | Loss: 0.00001082
Iteration 252/1000 | Loss: 0.00001082
Iteration 253/1000 | Loss: 0.00001081
Iteration 254/1000 | Loss: 0.00001079
Iteration 255/1000 | Loss: 0.00001079
Iteration 256/1000 | Loss: 0.00001079
Iteration 257/1000 | Loss: 0.00001079
Iteration 258/1000 | Loss: 0.00001079
Iteration 259/1000 | Loss: 0.00001079
Iteration 260/1000 | Loss: 0.00001079
Iteration 261/1000 | Loss: 0.00001079
Iteration 262/1000 | Loss: 0.00001079
Iteration 263/1000 | Loss: 0.00001079
Iteration 264/1000 | Loss: 0.00001078
Iteration 265/1000 | Loss: 0.00001078
Iteration 266/1000 | Loss: 0.00001078
Iteration 267/1000 | Loss: 0.00001077
Iteration 268/1000 | Loss: 0.00001077
Iteration 269/1000 | Loss: 0.00001077
Iteration 270/1000 | Loss: 0.00001077
Iteration 271/1000 | Loss: 0.00001077
Iteration 272/1000 | Loss: 0.00001077
Iteration 273/1000 | Loss: 0.00001076
Iteration 274/1000 | Loss: 0.00001076
Iteration 275/1000 | Loss: 0.00001076
Iteration 276/1000 | Loss: 0.00001076
Iteration 277/1000 | Loss: 0.00001076
Iteration 278/1000 | Loss: 0.00001075
Iteration 279/1000 | Loss: 0.00001075
Iteration 280/1000 | Loss: 0.00001075
Iteration 281/1000 | Loss: 0.00001075
Iteration 282/1000 | Loss: 0.00001075
Iteration 283/1000 | Loss: 0.00001075
Iteration 284/1000 | Loss: 0.00001075
Iteration 285/1000 | Loss: 0.00001075
Iteration 286/1000 | Loss: 0.00001075
Iteration 287/1000 | Loss: 0.00001075
Iteration 288/1000 | Loss: 0.00001075
Iteration 289/1000 | Loss: 0.00001074
Iteration 290/1000 | Loss: 0.00001074
Iteration 291/1000 | Loss: 0.00001074
Iteration 292/1000 | Loss: 0.00001074
Iteration 293/1000 | Loss: 0.00001074
Iteration 294/1000 | Loss: 0.00001074
Iteration 295/1000 | Loss: 0.00001074
Iteration 296/1000 | Loss: 0.00001074
Iteration 297/1000 | Loss: 0.00001074
Iteration 298/1000 | Loss: 0.00001074
Iteration 299/1000 | Loss: 0.00001074
Iteration 300/1000 | Loss: 0.00001074
Iteration 301/1000 | Loss: 0.00001074
Iteration 302/1000 | Loss: 0.00001074
Iteration 303/1000 | Loss: 0.00001074
Iteration 304/1000 | Loss: 0.00001074
Iteration 305/1000 | Loss: 0.00001074
Iteration 306/1000 | Loss: 0.00001074
Iteration 307/1000 | Loss: 0.00001074
Iteration 308/1000 | Loss: 0.00001074
Iteration 309/1000 | Loss: 0.00001074
Iteration 310/1000 | Loss: 0.00001074
Iteration 311/1000 | Loss: 0.00001074
Iteration 312/1000 | Loss: 0.00001074
Iteration 313/1000 | Loss: 0.00001074
Iteration 314/1000 | Loss: 0.00001074
Iteration 315/1000 | Loss: 0.00001074
Iteration 316/1000 | Loss: 0.00001074
Iteration 317/1000 | Loss: 0.00001074
Iteration 318/1000 | Loss: 0.00001074
Iteration 319/1000 | Loss: 0.00001074
Iteration 320/1000 | Loss: 0.00001074
Iteration 321/1000 | Loss: 0.00001074
Iteration 322/1000 | Loss: 0.00001074
Iteration 323/1000 | Loss: 0.00001074
Iteration 324/1000 | Loss: 0.00001074
Iteration 325/1000 | Loss: 0.00001074
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 325. Stopping optimization.
Last 5 losses: [1.07355499494588e-05, 1.07355499494588e-05, 1.07355499494588e-05, 1.07355499494588e-05, 1.07355499494588e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.07355499494588e-05

Optimization complete. Final v2v error: 2.700897455215454 mm

Highest mean error: 8.101263999938965 mm for frame 1

Lowest mean error: 2.3948686122894287 mm for frame 32

Saving results

Total time: 316.7350583076477
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01051897
Iteration 2/25 | Loss: 0.00156719
Iteration 3/25 | Loss: 0.00156643
Iteration 4/25 | Loss: 0.00123567
Iteration 5/25 | Loss: 0.00110641
Iteration 6/25 | Loss: 0.00120050
Iteration 7/25 | Loss: 0.00105791
Iteration 8/25 | Loss: 0.00106729
Iteration 9/25 | Loss: 0.00104343
Iteration 10/25 | Loss: 0.00105282
Iteration 11/25 | Loss: 0.00104139
Iteration 12/25 | Loss: 0.00104168
Iteration 13/25 | Loss: 0.00102239
Iteration 14/25 | Loss: 0.00103657
Iteration 15/25 | Loss: 0.00102853
Iteration 16/25 | Loss: 0.00102584
Iteration 17/25 | Loss: 0.00102015
Iteration 18/25 | Loss: 0.00102640
Iteration 19/25 | Loss: 0.00102910
Iteration 20/25 | Loss: 0.00102763
Iteration 21/25 | Loss: 0.00102743
Iteration 22/25 | Loss: 0.00102105
Iteration 23/25 | Loss: 0.00102643
Iteration 24/25 | Loss: 0.00102194
Iteration 25/25 | Loss: 0.00101762

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39769328
Iteration 2/25 | Loss: 0.00113442
Iteration 3/25 | Loss: 0.00112276
Iteration 4/25 | Loss: 0.00112276
Iteration 5/25 | Loss: 0.00112275
Iteration 6/25 | Loss: 0.00112275
Iteration 7/25 | Loss: 0.00112275
Iteration 8/25 | Loss: 0.00112275
Iteration 9/25 | Loss: 0.00112275
Iteration 10/25 | Loss: 0.00112275
Iteration 11/25 | Loss: 0.00112275
Iteration 12/25 | Loss: 0.00112275
Iteration 13/25 | Loss: 0.00112275
Iteration 14/25 | Loss: 0.00112275
Iteration 15/25 | Loss: 0.00112275
Iteration 16/25 | Loss: 0.00112275
Iteration 17/25 | Loss: 0.00112275
Iteration 18/25 | Loss: 0.00112275
Iteration 19/25 | Loss: 0.00112275
Iteration 20/25 | Loss: 0.00112275
Iteration 21/25 | Loss: 0.00112275
Iteration 22/25 | Loss: 0.00112275
Iteration 23/25 | Loss: 0.00112275
Iteration 24/25 | Loss: 0.00112275
Iteration 25/25 | Loss: 0.00112275

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112275
Iteration 2/1000 | Loss: 0.00018803
Iteration 3/1000 | Loss: 0.00034447
Iteration 4/1000 | Loss: 0.00014794
Iteration 5/1000 | Loss: 0.00027701
Iteration 6/1000 | Loss: 0.00015827
Iteration 7/1000 | Loss: 0.00034573
Iteration 8/1000 | Loss: 0.00188349
Iteration 9/1000 | Loss: 0.00065134
Iteration 10/1000 | Loss: 0.00045869
Iteration 11/1000 | Loss: 0.00121609
Iteration 12/1000 | Loss: 0.00035760
Iteration 13/1000 | Loss: 0.00020047
Iteration 14/1000 | Loss: 0.00010867
Iteration 15/1000 | Loss: 0.00012670
Iteration 16/1000 | Loss: 0.00017031
Iteration 17/1000 | Loss: 0.00016936
Iteration 18/1000 | Loss: 0.00016721
Iteration 19/1000 | Loss: 0.00019211
Iteration 20/1000 | Loss: 0.00018780
Iteration 21/1000 | Loss: 0.00018026
Iteration 22/1000 | Loss: 0.00013189
Iteration 23/1000 | Loss: 0.00023080
Iteration 24/1000 | Loss: 0.00009675
Iteration 25/1000 | Loss: 0.00023515
Iteration 26/1000 | Loss: 0.00021198
Iteration 27/1000 | Loss: 0.00086672
Iteration 28/1000 | Loss: 0.00052362
Iteration 29/1000 | Loss: 0.00060668
Iteration 30/1000 | Loss: 0.00040609
Iteration 31/1000 | Loss: 0.00035540
Iteration 32/1000 | Loss: 0.00031760
Iteration 33/1000 | Loss: 0.00051668
Iteration 34/1000 | Loss: 0.00027744
Iteration 35/1000 | Loss: 0.00019836
Iteration 36/1000 | Loss: 0.00017047
Iteration 37/1000 | Loss: 0.00018122
Iteration 38/1000 | Loss: 0.00015071
Iteration 39/1000 | Loss: 0.00002201
Iteration 40/1000 | Loss: 0.00002002
Iteration 41/1000 | Loss: 0.00005697
Iteration 42/1000 | Loss: 0.00019474
Iteration 43/1000 | Loss: 0.00016030
Iteration 44/1000 | Loss: 0.00001836
Iteration 45/1000 | Loss: 0.00005576
Iteration 46/1000 | Loss: 0.00016309
Iteration 47/1000 | Loss: 0.00002551
Iteration 48/1000 | Loss: 0.00017679
Iteration 49/1000 | Loss: 0.00019443
Iteration 50/1000 | Loss: 0.00008909
Iteration 51/1000 | Loss: 0.00002082
Iteration 52/1000 | Loss: 0.00008250
Iteration 53/1000 | Loss: 0.00021593
Iteration 54/1000 | Loss: 0.00005931
Iteration 55/1000 | Loss: 0.00036241
Iteration 56/1000 | Loss: 0.00034964
Iteration 57/1000 | Loss: 0.00023112
Iteration 58/1000 | Loss: 0.00006783
Iteration 59/1000 | Loss: 0.00002482
Iteration 60/1000 | Loss: 0.00025252
Iteration 61/1000 | Loss: 0.00012686
Iteration 62/1000 | Loss: 0.00003168
Iteration 63/1000 | Loss: 0.00011645
Iteration 64/1000 | Loss: 0.00059436
Iteration 65/1000 | Loss: 0.00044944
Iteration 66/1000 | Loss: 0.00011439
Iteration 67/1000 | Loss: 0.00011976
Iteration 68/1000 | Loss: 0.00004158
Iteration 69/1000 | Loss: 0.00009638
Iteration 70/1000 | Loss: 0.00002612
Iteration 71/1000 | Loss: 0.00034020
Iteration 72/1000 | Loss: 0.00011420
Iteration 73/1000 | Loss: 0.00021355
Iteration 74/1000 | Loss: 0.00013071
Iteration 75/1000 | Loss: 0.00013664
Iteration 76/1000 | Loss: 0.00011357
Iteration 77/1000 | Loss: 0.00010839
Iteration 78/1000 | Loss: 0.00041939
Iteration 79/1000 | Loss: 0.00004383
Iteration 80/1000 | Loss: 0.00014728
Iteration 81/1000 | Loss: 0.00001885
Iteration 82/1000 | Loss: 0.00019134
Iteration 83/1000 | Loss: 0.00028255
Iteration 84/1000 | Loss: 0.00009274
Iteration 85/1000 | Loss: 0.00012886
Iteration 86/1000 | Loss: 0.00013105
Iteration 87/1000 | Loss: 0.00001581
Iteration 88/1000 | Loss: 0.00001450
Iteration 89/1000 | Loss: 0.00001399
Iteration 90/1000 | Loss: 0.00001364
Iteration 91/1000 | Loss: 0.00007875
Iteration 92/1000 | Loss: 0.00006337
Iteration 93/1000 | Loss: 0.00002346
Iteration 94/1000 | Loss: 0.00044046
Iteration 95/1000 | Loss: 0.00012192
Iteration 96/1000 | Loss: 0.00001369
Iteration 97/1000 | Loss: 0.00001335
Iteration 98/1000 | Loss: 0.00001325
Iteration 99/1000 | Loss: 0.00007968
Iteration 100/1000 | Loss: 0.00001465
Iteration 101/1000 | Loss: 0.00005110
Iteration 102/1000 | Loss: 0.00011008
Iteration 103/1000 | Loss: 0.00005208
Iteration 104/1000 | Loss: 0.00012613
Iteration 105/1000 | Loss: 0.00015945
Iteration 106/1000 | Loss: 0.00020065
Iteration 107/1000 | Loss: 0.00012191
Iteration 108/1000 | Loss: 0.00013262
Iteration 109/1000 | Loss: 0.00012417
Iteration 110/1000 | Loss: 0.00020032
Iteration 111/1000 | Loss: 0.00022357
Iteration 112/1000 | Loss: 0.00021238
Iteration 113/1000 | Loss: 0.00017628
Iteration 114/1000 | Loss: 0.00014816
Iteration 115/1000 | Loss: 0.00005559
Iteration 116/1000 | Loss: 0.00001531
Iteration 117/1000 | Loss: 0.00001421
Iteration 118/1000 | Loss: 0.00001359
Iteration 119/1000 | Loss: 0.00001342
Iteration 120/1000 | Loss: 0.00001338
Iteration 121/1000 | Loss: 0.00001337
Iteration 122/1000 | Loss: 0.00001323
Iteration 123/1000 | Loss: 0.00001322
Iteration 124/1000 | Loss: 0.00001321
Iteration 125/1000 | Loss: 0.00001317
Iteration 126/1000 | Loss: 0.00001315
Iteration 127/1000 | Loss: 0.00001315
Iteration 128/1000 | Loss: 0.00001311
Iteration 129/1000 | Loss: 0.00001309
Iteration 130/1000 | Loss: 0.00001308
Iteration 131/1000 | Loss: 0.00001308
Iteration 132/1000 | Loss: 0.00001306
Iteration 133/1000 | Loss: 0.00017824
Iteration 134/1000 | Loss: 0.00030244
Iteration 135/1000 | Loss: 0.00037263
Iteration 136/1000 | Loss: 0.00050238
Iteration 137/1000 | Loss: 0.00016154
Iteration 138/1000 | Loss: 0.00013416
Iteration 139/1000 | Loss: 0.00021374
Iteration 140/1000 | Loss: 0.00060153
Iteration 141/1000 | Loss: 0.00042224
Iteration 142/1000 | Loss: 0.00057484
Iteration 143/1000 | Loss: 0.00030412
Iteration 144/1000 | Loss: 0.00042550
Iteration 145/1000 | Loss: 0.00010946
Iteration 146/1000 | Loss: 0.00020652
Iteration 147/1000 | Loss: 0.00012635
Iteration 148/1000 | Loss: 0.00005729
Iteration 149/1000 | Loss: 0.00001988
Iteration 150/1000 | Loss: 0.00005574
Iteration 151/1000 | Loss: 0.00001910
Iteration 152/1000 | Loss: 0.00005218
Iteration 153/1000 | Loss: 0.00001717
Iteration 154/1000 | Loss: 0.00005415
Iteration 155/1000 | Loss: 0.00016534
Iteration 156/1000 | Loss: 0.00008314
Iteration 157/1000 | Loss: 0.00026304
Iteration 158/1000 | Loss: 0.00023353
Iteration 159/1000 | Loss: 0.00018522
Iteration 160/1000 | Loss: 0.00014921
Iteration 161/1000 | Loss: 0.00017832
Iteration 162/1000 | Loss: 0.00003548
Iteration 163/1000 | Loss: 0.00018499
Iteration 164/1000 | Loss: 0.00074143
Iteration 165/1000 | Loss: 0.00032182
Iteration 166/1000 | Loss: 0.00012543
Iteration 167/1000 | Loss: 0.00009638
Iteration 168/1000 | Loss: 0.00032012
Iteration 169/1000 | Loss: 0.00043211
Iteration 170/1000 | Loss: 0.00023126
Iteration 171/1000 | Loss: 0.00045406
Iteration 172/1000 | Loss: 0.00016813
Iteration 173/1000 | Loss: 0.00012193
Iteration 174/1000 | Loss: 0.00019681
Iteration 175/1000 | Loss: 0.00014117
Iteration 176/1000 | Loss: 0.00024721
Iteration 177/1000 | Loss: 0.00093942
Iteration 178/1000 | Loss: 0.00033457
Iteration 179/1000 | Loss: 0.00023532
Iteration 180/1000 | Loss: 0.00046503
Iteration 181/1000 | Loss: 0.00027245
Iteration 182/1000 | Loss: 0.00017823
Iteration 183/1000 | Loss: 0.00008586
Iteration 184/1000 | Loss: 0.00048377
Iteration 185/1000 | Loss: 0.00046946
Iteration 186/1000 | Loss: 0.00048020
Iteration 187/1000 | Loss: 0.00014190
Iteration 188/1000 | Loss: 0.00022207
Iteration 189/1000 | Loss: 0.00022172
Iteration 190/1000 | Loss: 0.00008463
Iteration 191/1000 | Loss: 0.00001839
Iteration 192/1000 | Loss: 0.00001644
Iteration 193/1000 | Loss: 0.00001527
Iteration 194/1000 | Loss: 0.00001464
Iteration 195/1000 | Loss: 0.00008564
Iteration 196/1000 | Loss: 0.00050225
Iteration 197/1000 | Loss: 0.00030519
Iteration 198/1000 | Loss: 0.00030178
Iteration 199/1000 | Loss: 0.00023962
Iteration 200/1000 | Loss: 0.00025077
Iteration 201/1000 | Loss: 0.00024149
Iteration 202/1000 | Loss: 0.00030697
Iteration 203/1000 | Loss: 0.00028764
Iteration 204/1000 | Loss: 0.00011400
Iteration 205/1000 | Loss: 0.00002818
Iteration 206/1000 | Loss: 0.00006765
Iteration 207/1000 | Loss: 0.00017916
Iteration 208/1000 | Loss: 0.00017402
Iteration 209/1000 | Loss: 0.00017613
Iteration 210/1000 | Loss: 0.00022216
Iteration 211/1000 | Loss: 0.00013782
Iteration 212/1000 | Loss: 0.00019376
Iteration 213/1000 | Loss: 0.00017204
Iteration 214/1000 | Loss: 0.00015283
Iteration 215/1000 | Loss: 0.00053319
Iteration 216/1000 | Loss: 0.00002431
Iteration 217/1000 | Loss: 0.00013871
Iteration 218/1000 | Loss: 0.00036310
Iteration 219/1000 | Loss: 0.00019398
Iteration 220/1000 | Loss: 0.00011522
Iteration 221/1000 | Loss: 0.00043447
Iteration 222/1000 | Loss: 0.00019059
Iteration 223/1000 | Loss: 0.00040511
Iteration 224/1000 | Loss: 0.00023166
Iteration 225/1000 | Loss: 0.00008611
Iteration 226/1000 | Loss: 0.00001974
Iteration 227/1000 | Loss: 0.00037094
Iteration 228/1000 | Loss: 0.00029256
Iteration 229/1000 | Loss: 0.00004219
Iteration 230/1000 | Loss: 0.00021992
Iteration 231/1000 | Loss: 0.00046670
Iteration 232/1000 | Loss: 0.00008545
Iteration 233/1000 | Loss: 0.00002595
Iteration 234/1000 | Loss: 0.00036662
Iteration 235/1000 | Loss: 0.00020254
Iteration 236/1000 | Loss: 0.00031965
Iteration 237/1000 | Loss: 0.00027096
Iteration 238/1000 | Loss: 0.00004580
Iteration 239/1000 | Loss: 0.00002334
Iteration 240/1000 | Loss: 0.00011058
Iteration 241/1000 | Loss: 0.00002884
Iteration 242/1000 | Loss: 0.00001940
Iteration 243/1000 | Loss: 0.00002970
Iteration 244/1000 | Loss: 0.00001607
Iteration 245/1000 | Loss: 0.00001569
Iteration 246/1000 | Loss: 0.00002250
Iteration 247/1000 | Loss: 0.00001493
Iteration 248/1000 | Loss: 0.00003448
Iteration 249/1000 | Loss: 0.00001461
Iteration 250/1000 | Loss: 0.00001318
Iteration 251/1000 | Loss: 0.00023952
Iteration 252/1000 | Loss: 0.00026807
Iteration 253/1000 | Loss: 0.00009654
Iteration 254/1000 | Loss: 0.00005778
Iteration 255/1000 | Loss: 0.00026162
Iteration 256/1000 | Loss: 0.00002906
Iteration 257/1000 | Loss: 0.00001423
Iteration 258/1000 | Loss: 0.00003579
Iteration 259/1000 | Loss: 0.00004336
Iteration 260/1000 | Loss: 0.00001139
Iteration 261/1000 | Loss: 0.00003535
Iteration 262/1000 | Loss: 0.00002944
Iteration 263/1000 | Loss: 0.00009610
Iteration 264/1000 | Loss: 0.00002604
Iteration 265/1000 | Loss: 0.00033018
Iteration 266/1000 | Loss: 0.00001091
Iteration 267/1000 | Loss: 0.00002507
Iteration 268/1000 | Loss: 0.00001463
Iteration 269/1000 | Loss: 0.00001090
Iteration 270/1000 | Loss: 0.00001088
Iteration 271/1000 | Loss: 0.00001086
Iteration 272/1000 | Loss: 0.00001082
Iteration 273/1000 | Loss: 0.00001081
Iteration 274/1000 | Loss: 0.00001081
Iteration 275/1000 | Loss: 0.00001079
Iteration 276/1000 | Loss: 0.00001636
Iteration 277/1000 | Loss: 0.00001288
Iteration 278/1000 | Loss: 0.00001240
Iteration 279/1000 | Loss: 0.00001072
Iteration 280/1000 | Loss: 0.00001072
Iteration 281/1000 | Loss: 0.00001072
Iteration 282/1000 | Loss: 0.00001072
Iteration 283/1000 | Loss: 0.00001072
Iteration 284/1000 | Loss: 0.00001072
Iteration 285/1000 | Loss: 0.00001071
Iteration 286/1000 | Loss: 0.00001071
Iteration 287/1000 | Loss: 0.00001071
Iteration 288/1000 | Loss: 0.00001071
Iteration 289/1000 | Loss: 0.00001365
Iteration 290/1000 | Loss: 0.00002500
Iteration 291/1000 | Loss: 0.00001135
Iteration 292/1000 | Loss: 0.00001063
Iteration 293/1000 | Loss: 0.00001062
Iteration 294/1000 | Loss: 0.00001061
Iteration 295/1000 | Loss: 0.00001061
Iteration 296/1000 | Loss: 0.00001061
Iteration 297/1000 | Loss: 0.00001061
Iteration 298/1000 | Loss: 0.00001061
Iteration 299/1000 | Loss: 0.00001061
Iteration 300/1000 | Loss: 0.00001061
Iteration 301/1000 | Loss: 0.00001061
Iteration 302/1000 | Loss: 0.00001061
Iteration 303/1000 | Loss: 0.00001061
Iteration 304/1000 | Loss: 0.00001061
Iteration 305/1000 | Loss: 0.00001061
Iteration 306/1000 | Loss: 0.00001061
Iteration 307/1000 | Loss: 0.00001061
Iteration 308/1000 | Loss: 0.00001061
Iteration 309/1000 | Loss: 0.00001061
Iteration 310/1000 | Loss: 0.00001061
Iteration 311/1000 | Loss: 0.00001061
Iteration 312/1000 | Loss: 0.00001061
Iteration 313/1000 | Loss: 0.00001061
Iteration 314/1000 | Loss: 0.00001061
Iteration 315/1000 | Loss: 0.00001061
Iteration 316/1000 | Loss: 0.00001061
Iteration 317/1000 | Loss: 0.00001061
Iteration 318/1000 | Loss: 0.00001061
Iteration 319/1000 | Loss: 0.00001061
Iteration 320/1000 | Loss: 0.00001061
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 320. Stopping optimization.
Last 5 losses: [1.0611029210849665e-05, 1.0611029210849665e-05, 1.0611029210849665e-05, 1.0611029210849665e-05, 1.0611029210849665e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0611029210849665e-05

Optimization complete. Final v2v error: 2.7585606575012207 mm

Highest mean error: 4.638436317443848 mm for frame 106

Lowest mean error: 2.4979782104492188 mm for frame 131

Saving results

Total time: 417.5943591594696
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00593565
Iteration 2/25 | Loss: 0.00160222
Iteration 3/25 | Loss: 0.00131443
Iteration 4/25 | Loss: 0.00129401
Iteration 5/25 | Loss: 0.00124843
Iteration 6/25 | Loss: 0.00118803
Iteration 7/25 | Loss: 0.00117289
Iteration 8/25 | Loss: 0.00114569
Iteration 9/25 | Loss: 0.00113155
Iteration 10/25 | Loss: 0.00113879
Iteration 11/25 | Loss: 0.00113676
Iteration 12/25 | Loss: 0.00112733
Iteration 13/25 | Loss: 0.00111663
Iteration 14/25 | Loss: 0.00111407
Iteration 15/25 | Loss: 0.00111264
Iteration 16/25 | Loss: 0.00111213
Iteration 17/25 | Loss: 0.00111173
Iteration 18/25 | Loss: 0.00111148
Iteration 19/25 | Loss: 0.00111098
Iteration 20/25 | Loss: 0.00111030
Iteration 21/25 | Loss: 0.00111008
Iteration 22/25 | Loss: 0.00111005
Iteration 23/25 | Loss: 0.00111005
Iteration 24/25 | Loss: 0.00111005
Iteration 25/25 | Loss: 0.00111003

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.86830211
Iteration 2/25 | Loss: 0.00091278
Iteration 3/25 | Loss: 0.00082413
Iteration 4/25 | Loss: 0.00082413
Iteration 5/25 | Loss: 0.00082413
Iteration 6/25 | Loss: 0.00082412
Iteration 7/25 | Loss: 0.00082412
Iteration 8/25 | Loss: 0.00082412
Iteration 9/25 | Loss: 0.00082412
Iteration 10/25 | Loss: 0.00082412
Iteration 11/25 | Loss: 0.00082412
Iteration 12/25 | Loss: 0.00082412
Iteration 13/25 | Loss: 0.00082412
Iteration 14/25 | Loss: 0.00082412
Iteration 15/25 | Loss: 0.00082412
Iteration 16/25 | Loss: 0.00082412
Iteration 17/25 | Loss: 0.00082412
Iteration 18/25 | Loss: 0.00082412
Iteration 19/25 | Loss: 0.00082412
Iteration 20/25 | Loss: 0.00082412
Iteration 21/25 | Loss: 0.00082412
Iteration 22/25 | Loss: 0.00082412
Iteration 23/25 | Loss: 0.00082412
Iteration 24/25 | Loss: 0.00082412
Iteration 25/25 | Loss: 0.00082412

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082412
Iteration 2/1000 | Loss: 0.00011694
Iteration 3/1000 | Loss: 0.00004994
Iteration 4/1000 | Loss: 0.00004322
Iteration 5/1000 | Loss: 0.00003968
Iteration 6/1000 | Loss: 0.00003751
Iteration 7/1000 | Loss: 0.00003609
Iteration 8/1000 | Loss: 0.00003497
Iteration 9/1000 | Loss: 0.00003424
Iteration 10/1000 | Loss: 0.00003354
Iteration 11/1000 | Loss: 0.00003305
Iteration 12/1000 | Loss: 0.00003263
Iteration 13/1000 | Loss: 0.00003235
Iteration 14/1000 | Loss: 0.00003209
Iteration 15/1000 | Loss: 0.00003190
Iteration 16/1000 | Loss: 0.00003183
Iteration 17/1000 | Loss: 0.00003170
Iteration 18/1000 | Loss: 0.00003169
Iteration 19/1000 | Loss: 0.00011185
Iteration 20/1000 | Loss: 0.00045457
Iteration 21/1000 | Loss: 0.00021058
Iteration 22/1000 | Loss: 0.00007078
Iteration 23/1000 | Loss: 0.00004047
Iteration 24/1000 | Loss: 0.00003251
Iteration 25/1000 | Loss: 0.00003157
Iteration 26/1000 | Loss: 0.00003064
Iteration 27/1000 | Loss: 0.00019992
Iteration 28/1000 | Loss: 0.00003028
Iteration 29/1000 | Loss: 0.00002993
Iteration 30/1000 | Loss: 0.00002991
Iteration 31/1000 | Loss: 0.00013814
Iteration 32/1000 | Loss: 0.00002967
Iteration 33/1000 | Loss: 0.00002966
Iteration 34/1000 | Loss: 0.00002965
Iteration 35/1000 | Loss: 0.00002965
Iteration 36/1000 | Loss: 0.00002964
Iteration 37/1000 | Loss: 0.00002964
Iteration 38/1000 | Loss: 0.00002961
Iteration 39/1000 | Loss: 0.00002957
Iteration 40/1000 | Loss: 0.00002956
Iteration 41/1000 | Loss: 0.00002954
Iteration 42/1000 | Loss: 0.00002953
Iteration 43/1000 | Loss: 0.00002952
Iteration 44/1000 | Loss: 0.00002950
Iteration 45/1000 | Loss: 0.00002950
Iteration 46/1000 | Loss: 0.00002948
Iteration 47/1000 | Loss: 0.00011918
Iteration 48/1000 | Loss: 0.00005640
Iteration 49/1000 | Loss: 0.00002959
Iteration 50/1000 | Loss: 0.00011050
Iteration 51/1000 | Loss: 0.00003921
Iteration 52/1000 | Loss: 0.00003259
Iteration 53/1000 | Loss: 0.00002959
Iteration 54/1000 | Loss: 0.00002936
Iteration 55/1000 | Loss: 0.00002931
Iteration 56/1000 | Loss: 0.00002930
Iteration 57/1000 | Loss: 0.00002930
Iteration 58/1000 | Loss: 0.00002928
Iteration 59/1000 | Loss: 0.00002928
Iteration 60/1000 | Loss: 0.00002927
Iteration 61/1000 | Loss: 0.00002927
Iteration 62/1000 | Loss: 0.00002927
Iteration 63/1000 | Loss: 0.00002926
Iteration 64/1000 | Loss: 0.00002925
Iteration 65/1000 | Loss: 0.00002925
Iteration 66/1000 | Loss: 0.00002924
Iteration 67/1000 | Loss: 0.00002924
Iteration 68/1000 | Loss: 0.00002923
Iteration 69/1000 | Loss: 0.00002922
Iteration 70/1000 | Loss: 0.00002922
Iteration 71/1000 | Loss: 0.00002921
Iteration 72/1000 | Loss: 0.00002920
Iteration 73/1000 | Loss: 0.00002920
Iteration 74/1000 | Loss: 0.00002920
Iteration 75/1000 | Loss: 0.00002920
Iteration 76/1000 | Loss: 0.00002919
Iteration 77/1000 | Loss: 0.00002919
Iteration 78/1000 | Loss: 0.00002919
Iteration 79/1000 | Loss: 0.00002919
Iteration 80/1000 | Loss: 0.00002918
Iteration 81/1000 | Loss: 0.00002918
Iteration 82/1000 | Loss: 0.00002914
Iteration 83/1000 | Loss: 0.00002914
Iteration 84/1000 | Loss: 0.00002913
Iteration 85/1000 | Loss: 0.00002912
Iteration 86/1000 | Loss: 0.00002912
Iteration 87/1000 | Loss: 0.00002911
Iteration 88/1000 | Loss: 0.00002911
Iteration 89/1000 | Loss: 0.00002911
Iteration 90/1000 | Loss: 0.00002911
Iteration 91/1000 | Loss: 0.00002911
Iteration 92/1000 | Loss: 0.00002911
Iteration 93/1000 | Loss: 0.00002911
Iteration 94/1000 | Loss: 0.00002911
Iteration 95/1000 | Loss: 0.00002909
Iteration 96/1000 | Loss: 0.00002909
Iteration 97/1000 | Loss: 0.00002909
Iteration 98/1000 | Loss: 0.00002908
Iteration 99/1000 | Loss: 0.00002908
Iteration 100/1000 | Loss: 0.00002907
Iteration 101/1000 | Loss: 0.00002907
Iteration 102/1000 | Loss: 0.00002906
Iteration 103/1000 | Loss: 0.00002904
Iteration 104/1000 | Loss: 0.00002903
Iteration 105/1000 | Loss: 0.00002903
Iteration 106/1000 | Loss: 0.00002903
Iteration 107/1000 | Loss: 0.00002902
Iteration 108/1000 | Loss: 0.00002902
Iteration 109/1000 | Loss: 0.00002902
Iteration 110/1000 | Loss: 0.00002901
Iteration 111/1000 | Loss: 0.00002901
Iteration 112/1000 | Loss: 0.00002901
Iteration 113/1000 | Loss: 0.00002901
Iteration 114/1000 | Loss: 0.00002901
Iteration 115/1000 | Loss: 0.00002901
Iteration 116/1000 | Loss: 0.00047743
Iteration 117/1000 | Loss: 0.00011393
Iteration 118/1000 | Loss: 0.00003275
Iteration 119/1000 | Loss: 0.00002932
Iteration 120/1000 | Loss: 0.00002906
Iteration 121/1000 | Loss: 0.00002904
Iteration 122/1000 | Loss: 0.00002903
Iteration 123/1000 | Loss: 0.00002903
Iteration 124/1000 | Loss: 0.00002902
Iteration 125/1000 | Loss: 0.00002902
Iteration 126/1000 | Loss: 0.00002901
Iteration 127/1000 | Loss: 0.00002901
Iteration 128/1000 | Loss: 0.00002901
Iteration 129/1000 | Loss: 0.00002901
Iteration 130/1000 | Loss: 0.00002901
Iteration 131/1000 | Loss: 0.00002901
Iteration 132/1000 | Loss: 0.00002901
Iteration 133/1000 | Loss: 0.00002900
Iteration 134/1000 | Loss: 0.00002898
Iteration 135/1000 | Loss: 0.00002898
Iteration 136/1000 | Loss: 0.00002897
Iteration 137/1000 | Loss: 0.00002896
Iteration 138/1000 | Loss: 0.00002896
Iteration 139/1000 | Loss: 0.00002896
Iteration 140/1000 | Loss: 0.00002896
Iteration 141/1000 | Loss: 0.00002896
Iteration 142/1000 | Loss: 0.00002896
Iteration 143/1000 | Loss: 0.00002895
Iteration 144/1000 | Loss: 0.00002895
Iteration 145/1000 | Loss: 0.00002895
Iteration 146/1000 | Loss: 0.00002895
Iteration 147/1000 | Loss: 0.00002895
Iteration 148/1000 | Loss: 0.00002895
Iteration 149/1000 | Loss: 0.00002895
Iteration 150/1000 | Loss: 0.00002895
Iteration 151/1000 | Loss: 0.00002895
Iteration 152/1000 | Loss: 0.00002895
Iteration 153/1000 | Loss: 0.00002894
Iteration 154/1000 | Loss: 0.00002894
Iteration 155/1000 | Loss: 0.00002894
Iteration 156/1000 | Loss: 0.00002894
Iteration 157/1000 | Loss: 0.00002894
Iteration 158/1000 | Loss: 0.00002894
Iteration 159/1000 | Loss: 0.00002894
Iteration 160/1000 | Loss: 0.00002894
Iteration 161/1000 | Loss: 0.00002894
Iteration 162/1000 | Loss: 0.00002894
Iteration 163/1000 | Loss: 0.00002894
Iteration 164/1000 | Loss: 0.00002893
Iteration 165/1000 | Loss: 0.00002893
Iteration 166/1000 | Loss: 0.00002893
Iteration 167/1000 | Loss: 0.00002893
Iteration 168/1000 | Loss: 0.00002893
Iteration 169/1000 | Loss: 0.00002893
Iteration 170/1000 | Loss: 0.00002893
Iteration 171/1000 | Loss: 0.00002893
Iteration 172/1000 | Loss: 0.00002893
Iteration 173/1000 | Loss: 0.00002893
Iteration 174/1000 | Loss: 0.00002893
Iteration 175/1000 | Loss: 0.00002893
Iteration 176/1000 | Loss: 0.00002893
Iteration 177/1000 | Loss: 0.00002893
Iteration 178/1000 | Loss: 0.00002892
Iteration 179/1000 | Loss: 0.00002892
Iteration 180/1000 | Loss: 0.00002892
Iteration 181/1000 | Loss: 0.00002892
Iteration 182/1000 | Loss: 0.00002892
Iteration 183/1000 | Loss: 0.00002892
Iteration 184/1000 | Loss: 0.00002892
Iteration 185/1000 | Loss: 0.00002892
Iteration 186/1000 | Loss: 0.00002892
Iteration 187/1000 | Loss: 0.00002892
Iteration 188/1000 | Loss: 0.00002891
Iteration 189/1000 | Loss: 0.00002891
Iteration 190/1000 | Loss: 0.00002891
Iteration 191/1000 | Loss: 0.00049889
Iteration 192/1000 | Loss: 0.00010269
Iteration 193/1000 | Loss: 0.00004009
Iteration 194/1000 | Loss: 0.00002982
Iteration 195/1000 | Loss: 0.00002940
Iteration 196/1000 | Loss: 0.00002917
Iteration 197/1000 | Loss: 0.00002906
Iteration 198/1000 | Loss: 0.00002904
Iteration 199/1000 | Loss: 0.00002904
Iteration 200/1000 | Loss: 0.00002904
Iteration 201/1000 | Loss: 0.00002903
Iteration 202/1000 | Loss: 0.00002903
Iteration 203/1000 | Loss: 0.00002903
Iteration 204/1000 | Loss: 0.00002903
Iteration 205/1000 | Loss: 0.00002902
Iteration 206/1000 | Loss: 0.00002902
Iteration 207/1000 | Loss: 0.00002902
Iteration 208/1000 | Loss: 0.00002902
Iteration 209/1000 | Loss: 0.00002902
Iteration 210/1000 | Loss: 0.00002901
Iteration 211/1000 | Loss: 0.00002901
Iteration 212/1000 | Loss: 0.00002901
Iteration 213/1000 | Loss: 0.00002901
Iteration 214/1000 | Loss: 0.00002901
Iteration 215/1000 | Loss: 0.00002901
Iteration 216/1000 | Loss: 0.00002901
Iteration 217/1000 | Loss: 0.00002900
Iteration 218/1000 | Loss: 0.00002900
Iteration 219/1000 | Loss: 0.00002900
Iteration 220/1000 | Loss: 0.00002900
Iteration 221/1000 | Loss: 0.00002900
Iteration 222/1000 | Loss: 0.00002900
Iteration 223/1000 | Loss: 0.00002899
Iteration 224/1000 | Loss: 0.00002899
Iteration 225/1000 | Loss: 0.00002898
Iteration 226/1000 | Loss: 0.00002898
Iteration 227/1000 | Loss: 0.00002898
Iteration 228/1000 | Loss: 0.00002897
Iteration 229/1000 | Loss: 0.00002897
Iteration 230/1000 | Loss: 0.00002897
Iteration 231/1000 | Loss: 0.00002896
Iteration 232/1000 | Loss: 0.00002896
Iteration 233/1000 | Loss: 0.00002895
Iteration 234/1000 | Loss: 0.00002895
Iteration 235/1000 | Loss: 0.00002894
Iteration 236/1000 | Loss: 0.00002894
Iteration 237/1000 | Loss: 0.00002893
Iteration 238/1000 | Loss: 0.00002893
Iteration 239/1000 | Loss: 0.00002893
Iteration 240/1000 | Loss: 0.00049485
Iteration 241/1000 | Loss: 0.00064367
Iteration 242/1000 | Loss: 0.00064932
Iteration 243/1000 | Loss: 0.00004985
Iteration 244/1000 | Loss: 0.00003308
Iteration 245/1000 | Loss: 0.00040962
Iteration 246/1000 | Loss: 0.00030037
Iteration 247/1000 | Loss: 0.00036289
Iteration 248/1000 | Loss: 0.00004016
Iteration 249/1000 | Loss: 0.00003195
Iteration 250/1000 | Loss: 0.00003009
Iteration 251/1000 | Loss: 0.00002948
Iteration 252/1000 | Loss: 0.00002923
Iteration 253/1000 | Loss: 0.00002898
Iteration 254/1000 | Loss: 0.00002894
Iteration 255/1000 | Loss: 0.00002894
Iteration 256/1000 | Loss: 0.00002893
Iteration 257/1000 | Loss: 0.00002893
Iteration 258/1000 | Loss: 0.00002892
Iteration 259/1000 | Loss: 0.00002892
Iteration 260/1000 | Loss: 0.00002888
Iteration 261/1000 | Loss: 0.00002888
Iteration 262/1000 | Loss: 0.00002888
Iteration 263/1000 | Loss: 0.00002888
Iteration 264/1000 | Loss: 0.00002888
Iteration 265/1000 | Loss: 0.00002887
Iteration 266/1000 | Loss: 0.00002887
Iteration 267/1000 | Loss: 0.00002887
Iteration 268/1000 | Loss: 0.00002887
Iteration 269/1000 | Loss: 0.00002887
Iteration 270/1000 | Loss: 0.00002886
Iteration 271/1000 | Loss: 0.00002886
Iteration 272/1000 | Loss: 0.00002886
Iteration 273/1000 | Loss: 0.00002886
Iteration 274/1000 | Loss: 0.00002886
Iteration 275/1000 | Loss: 0.00002886
Iteration 276/1000 | Loss: 0.00002886
Iteration 277/1000 | Loss: 0.00002886
Iteration 278/1000 | Loss: 0.00002886
Iteration 279/1000 | Loss: 0.00002886
Iteration 280/1000 | Loss: 0.00002886
Iteration 281/1000 | Loss: 0.00002886
Iteration 282/1000 | Loss: 0.00002885
Iteration 283/1000 | Loss: 0.00002885
Iteration 284/1000 | Loss: 0.00002885
Iteration 285/1000 | Loss: 0.00002885
Iteration 286/1000 | Loss: 0.00002885
Iteration 287/1000 | Loss: 0.00002885
Iteration 288/1000 | Loss: 0.00002884
Iteration 289/1000 | Loss: 0.00002884
Iteration 290/1000 | Loss: 0.00002884
Iteration 291/1000 | Loss: 0.00002884
Iteration 292/1000 | Loss: 0.00002884
Iteration 293/1000 | Loss: 0.00002884
Iteration 294/1000 | Loss: 0.00002883
Iteration 295/1000 | Loss: 0.00002883
Iteration 296/1000 | Loss: 0.00002883
Iteration 297/1000 | Loss: 0.00002883
Iteration 298/1000 | Loss: 0.00002883
Iteration 299/1000 | Loss: 0.00002883
Iteration 300/1000 | Loss: 0.00002883
Iteration 301/1000 | Loss: 0.00002883
Iteration 302/1000 | Loss: 0.00002883
Iteration 303/1000 | Loss: 0.00002883
Iteration 304/1000 | Loss: 0.00002883
Iteration 305/1000 | Loss: 0.00002883
Iteration 306/1000 | Loss: 0.00002883
Iteration 307/1000 | Loss: 0.00002883
Iteration 308/1000 | Loss: 0.00002883
Iteration 309/1000 | Loss: 0.00002883
Iteration 310/1000 | Loss: 0.00002883
Iteration 311/1000 | Loss: 0.00002883
Iteration 312/1000 | Loss: 0.00002882
Iteration 313/1000 | Loss: 0.00002882
Iteration 314/1000 | Loss: 0.00002882
Iteration 315/1000 | Loss: 0.00002882
Iteration 316/1000 | Loss: 0.00002882
Iteration 317/1000 | Loss: 0.00002882
Iteration 318/1000 | Loss: 0.00002882
Iteration 319/1000 | Loss: 0.00002882
Iteration 320/1000 | Loss: 0.00002882
Iteration 321/1000 | Loss: 0.00002882
Iteration 322/1000 | Loss: 0.00002882
Iteration 323/1000 | Loss: 0.00002882
Iteration 324/1000 | Loss: 0.00002882
Iteration 325/1000 | Loss: 0.00002882
Iteration 326/1000 | Loss: 0.00002882
Iteration 327/1000 | Loss: 0.00002882
Iteration 328/1000 | Loss: 0.00002881
Iteration 329/1000 | Loss: 0.00002881
Iteration 330/1000 | Loss: 0.00002881
Iteration 331/1000 | Loss: 0.00002881
Iteration 332/1000 | Loss: 0.00002881
Iteration 333/1000 | Loss: 0.00002881
Iteration 334/1000 | Loss: 0.00002881
Iteration 335/1000 | Loss: 0.00002881
Iteration 336/1000 | Loss: 0.00002881
Iteration 337/1000 | Loss: 0.00002881
Iteration 338/1000 | Loss: 0.00002881
Iteration 339/1000 | Loss: 0.00002881
Iteration 340/1000 | Loss: 0.00002881
Iteration 341/1000 | Loss: 0.00002881
Iteration 342/1000 | Loss: 0.00002880
Iteration 343/1000 | Loss: 0.00002880
Iteration 344/1000 | Loss: 0.00002880
Iteration 345/1000 | Loss: 0.00002880
Iteration 346/1000 | Loss: 0.00002880
Iteration 347/1000 | Loss: 0.00002880
Iteration 348/1000 | Loss: 0.00002880
Iteration 349/1000 | Loss: 0.00002880
Iteration 350/1000 | Loss: 0.00002880
Iteration 351/1000 | Loss: 0.00002880
Iteration 352/1000 | Loss: 0.00002879
Iteration 353/1000 | Loss: 0.00002879
Iteration 354/1000 | Loss: 0.00002879
Iteration 355/1000 | Loss: 0.00002879
Iteration 356/1000 | Loss: 0.00002879
Iteration 357/1000 | Loss: 0.00002879
Iteration 358/1000 | Loss: 0.00002879
Iteration 359/1000 | Loss: 0.00002879
Iteration 360/1000 | Loss: 0.00002879
Iteration 361/1000 | Loss: 0.00002879
Iteration 362/1000 | Loss: 0.00002879
Iteration 363/1000 | Loss: 0.00002879
Iteration 364/1000 | Loss: 0.00002879
Iteration 365/1000 | Loss: 0.00002879
Iteration 366/1000 | Loss: 0.00002879
Iteration 367/1000 | Loss: 0.00002879
Iteration 368/1000 | Loss: 0.00002879
Iteration 369/1000 | Loss: 0.00002878
Iteration 370/1000 | Loss: 0.00002878
Iteration 371/1000 | Loss: 0.00002878
Iteration 372/1000 | Loss: 0.00002878
Iteration 373/1000 | Loss: 0.00002878
Iteration 374/1000 | Loss: 0.00002878
Iteration 375/1000 | Loss: 0.00002878
Iteration 376/1000 | Loss: 0.00002878
Iteration 377/1000 | Loss: 0.00002878
Iteration 378/1000 | Loss: 0.00002878
Iteration 379/1000 | Loss: 0.00002878
Iteration 380/1000 | Loss: 0.00002878
Iteration 381/1000 | Loss: 0.00002878
Iteration 382/1000 | Loss: 0.00002878
Iteration 383/1000 | Loss: 0.00002878
Iteration 384/1000 | Loss: 0.00002878
Iteration 385/1000 | Loss: 0.00002878
Iteration 386/1000 | Loss: 0.00002878
Iteration 387/1000 | Loss: 0.00002878
Iteration 388/1000 | Loss: 0.00002878
Iteration 389/1000 | Loss: 0.00002878
Iteration 390/1000 | Loss: 0.00002878
Iteration 391/1000 | Loss: 0.00002878
Iteration 392/1000 | Loss: 0.00002878
Iteration 393/1000 | Loss: 0.00002878
Iteration 394/1000 | Loss: 0.00002878
Iteration 395/1000 | Loss: 0.00002878
Iteration 396/1000 | Loss: 0.00002878
Iteration 397/1000 | Loss: 0.00002878
Iteration 398/1000 | Loss: 0.00002878
Iteration 399/1000 | Loss: 0.00002878
Iteration 400/1000 | Loss: 0.00002878
Iteration 401/1000 | Loss: 0.00002878
Iteration 402/1000 | Loss: 0.00002878
Iteration 403/1000 | Loss: 0.00002878
Iteration 404/1000 | Loss: 0.00002878
Iteration 405/1000 | Loss: 0.00002878
Iteration 406/1000 | Loss: 0.00002878
Iteration 407/1000 | Loss: 0.00002878
Iteration 408/1000 | Loss: 0.00002878
Iteration 409/1000 | Loss: 0.00002878
Iteration 410/1000 | Loss: 0.00002878
Iteration 411/1000 | Loss: 0.00002878
Iteration 412/1000 | Loss: 0.00002878
Iteration 413/1000 | Loss: 0.00002878
Iteration 414/1000 | Loss: 0.00002878
Iteration 415/1000 | Loss: 0.00002878
Iteration 416/1000 | Loss: 0.00002878
Iteration 417/1000 | Loss: 0.00002878
Iteration 418/1000 | Loss: 0.00002878
Iteration 419/1000 | Loss: 0.00002878
Iteration 420/1000 | Loss: 0.00002878
Iteration 421/1000 | Loss: 0.00002878
Iteration 422/1000 | Loss: 0.00002878
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 422. Stopping optimization.
Last 5 losses: [2.878065424738452e-05, 2.878065424738452e-05, 2.878065424738452e-05, 2.878065424738452e-05, 2.878065424738452e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.878065424738452e-05

Optimization complete. Final v2v error: 3.635620594024658 mm

Highest mean error: 12.255395889282227 mm for frame 103

Lowest mean error: 2.6537094116210938 mm for frame 172

Saving results

Total time: 171.89098715782166
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00660688
Iteration 2/25 | Loss: 0.00130482
Iteration 3/25 | Loss: 0.00113048
Iteration 4/25 | Loss: 0.00111253
Iteration 5/25 | Loss: 0.00110977
Iteration 6/25 | Loss: 0.00110923
Iteration 7/25 | Loss: 0.00110923
Iteration 8/25 | Loss: 0.00110923
Iteration 9/25 | Loss: 0.00110923
Iteration 10/25 | Loss: 0.00110923
Iteration 11/25 | Loss: 0.00110923
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011092297499999404, 0.0011092297499999404, 0.0011092297499999404, 0.0011092297499999404, 0.0011092297499999404]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011092297499999404

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 10.97643185
Iteration 2/25 | Loss: 0.00062754
Iteration 3/25 | Loss: 0.00062740
Iteration 4/25 | Loss: 0.00062740
Iteration 5/25 | Loss: 0.00062740
Iteration 6/25 | Loss: 0.00062740
Iteration 7/25 | Loss: 0.00062740
Iteration 8/25 | Loss: 0.00062740
Iteration 9/25 | Loss: 0.00062740
Iteration 10/25 | Loss: 0.00062740
Iteration 11/25 | Loss: 0.00062740
Iteration 12/25 | Loss: 0.00062740
Iteration 13/25 | Loss: 0.00062740
Iteration 14/25 | Loss: 0.00062740
Iteration 15/25 | Loss: 0.00062740
Iteration 16/25 | Loss: 0.00062740
Iteration 17/25 | Loss: 0.00062740
Iteration 18/25 | Loss: 0.00062740
Iteration 19/25 | Loss: 0.00062740
Iteration 20/25 | Loss: 0.00062740
Iteration 21/25 | Loss: 0.00062740
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006273959879763424, 0.0006273959879763424, 0.0006273959879763424, 0.0006273959879763424, 0.0006273959879763424]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006273959879763424

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062740
Iteration 2/1000 | Loss: 0.00004674
Iteration 3/1000 | Loss: 0.00002639
Iteration 4/1000 | Loss: 0.00002204
Iteration 5/1000 | Loss: 0.00002023
Iteration 6/1000 | Loss: 0.00001939
Iteration 7/1000 | Loss: 0.00001887
Iteration 8/1000 | Loss: 0.00001850
Iteration 9/1000 | Loss: 0.00001819
Iteration 10/1000 | Loss: 0.00001796
Iteration 11/1000 | Loss: 0.00001776
Iteration 12/1000 | Loss: 0.00001766
Iteration 13/1000 | Loss: 0.00001760
Iteration 14/1000 | Loss: 0.00001756
Iteration 15/1000 | Loss: 0.00001749
Iteration 16/1000 | Loss: 0.00001742
Iteration 17/1000 | Loss: 0.00001742
Iteration 18/1000 | Loss: 0.00001742
Iteration 19/1000 | Loss: 0.00001740
Iteration 20/1000 | Loss: 0.00001739
Iteration 21/1000 | Loss: 0.00001739
Iteration 22/1000 | Loss: 0.00001739
Iteration 23/1000 | Loss: 0.00001738
Iteration 24/1000 | Loss: 0.00001738
Iteration 25/1000 | Loss: 0.00001738
Iteration 26/1000 | Loss: 0.00001737
Iteration 27/1000 | Loss: 0.00001737
Iteration 28/1000 | Loss: 0.00001737
Iteration 29/1000 | Loss: 0.00001736
Iteration 30/1000 | Loss: 0.00001736
Iteration 31/1000 | Loss: 0.00001736
Iteration 32/1000 | Loss: 0.00001735
Iteration 33/1000 | Loss: 0.00001735
Iteration 34/1000 | Loss: 0.00001734
Iteration 35/1000 | Loss: 0.00001734
Iteration 36/1000 | Loss: 0.00001733
Iteration 37/1000 | Loss: 0.00001733
Iteration 38/1000 | Loss: 0.00001733
Iteration 39/1000 | Loss: 0.00001732
Iteration 40/1000 | Loss: 0.00001732
Iteration 41/1000 | Loss: 0.00001732
Iteration 42/1000 | Loss: 0.00001731
Iteration 43/1000 | Loss: 0.00001731
Iteration 44/1000 | Loss: 0.00001731
Iteration 45/1000 | Loss: 0.00001731
Iteration 46/1000 | Loss: 0.00001731
Iteration 47/1000 | Loss: 0.00001731
Iteration 48/1000 | Loss: 0.00001730
Iteration 49/1000 | Loss: 0.00001730
Iteration 50/1000 | Loss: 0.00001730
Iteration 51/1000 | Loss: 0.00001730
Iteration 52/1000 | Loss: 0.00001729
Iteration 53/1000 | Loss: 0.00001729
Iteration 54/1000 | Loss: 0.00001728
Iteration 55/1000 | Loss: 0.00001728
Iteration 56/1000 | Loss: 0.00001728
Iteration 57/1000 | Loss: 0.00001727
Iteration 58/1000 | Loss: 0.00001727
Iteration 59/1000 | Loss: 0.00001726
Iteration 60/1000 | Loss: 0.00001726
Iteration 61/1000 | Loss: 0.00001726
Iteration 62/1000 | Loss: 0.00001725
Iteration 63/1000 | Loss: 0.00001725
Iteration 64/1000 | Loss: 0.00001725
Iteration 65/1000 | Loss: 0.00001724
Iteration 66/1000 | Loss: 0.00001724
Iteration 67/1000 | Loss: 0.00001722
Iteration 68/1000 | Loss: 0.00001721
Iteration 69/1000 | Loss: 0.00001721
Iteration 70/1000 | Loss: 0.00001720
Iteration 71/1000 | Loss: 0.00001720
Iteration 72/1000 | Loss: 0.00001720
Iteration 73/1000 | Loss: 0.00001719
Iteration 74/1000 | Loss: 0.00001719
Iteration 75/1000 | Loss: 0.00001718
Iteration 76/1000 | Loss: 0.00001718
Iteration 77/1000 | Loss: 0.00001718
Iteration 78/1000 | Loss: 0.00001717
Iteration 79/1000 | Loss: 0.00001717
Iteration 80/1000 | Loss: 0.00001717
Iteration 81/1000 | Loss: 0.00001717
Iteration 82/1000 | Loss: 0.00001717
Iteration 83/1000 | Loss: 0.00001716
Iteration 84/1000 | Loss: 0.00001716
Iteration 85/1000 | Loss: 0.00001716
Iteration 86/1000 | Loss: 0.00001716
Iteration 87/1000 | Loss: 0.00001716
Iteration 88/1000 | Loss: 0.00001715
Iteration 89/1000 | Loss: 0.00001715
Iteration 90/1000 | Loss: 0.00001715
Iteration 91/1000 | Loss: 0.00001715
Iteration 92/1000 | Loss: 0.00001714
Iteration 93/1000 | Loss: 0.00001714
Iteration 94/1000 | Loss: 0.00001714
Iteration 95/1000 | Loss: 0.00001714
Iteration 96/1000 | Loss: 0.00001714
Iteration 97/1000 | Loss: 0.00001714
Iteration 98/1000 | Loss: 0.00001713
Iteration 99/1000 | Loss: 0.00001713
Iteration 100/1000 | Loss: 0.00001713
Iteration 101/1000 | Loss: 0.00001713
Iteration 102/1000 | Loss: 0.00001712
Iteration 103/1000 | Loss: 0.00001712
Iteration 104/1000 | Loss: 0.00001712
Iteration 105/1000 | Loss: 0.00001712
Iteration 106/1000 | Loss: 0.00001711
Iteration 107/1000 | Loss: 0.00001711
Iteration 108/1000 | Loss: 0.00001711
Iteration 109/1000 | Loss: 0.00001711
Iteration 110/1000 | Loss: 0.00001710
Iteration 111/1000 | Loss: 0.00001710
Iteration 112/1000 | Loss: 0.00001710
Iteration 113/1000 | Loss: 0.00001710
Iteration 114/1000 | Loss: 0.00001710
Iteration 115/1000 | Loss: 0.00001709
Iteration 116/1000 | Loss: 0.00001709
Iteration 117/1000 | Loss: 0.00001709
Iteration 118/1000 | Loss: 0.00001709
Iteration 119/1000 | Loss: 0.00001709
Iteration 120/1000 | Loss: 0.00001709
Iteration 121/1000 | Loss: 0.00001708
Iteration 122/1000 | Loss: 0.00001708
Iteration 123/1000 | Loss: 0.00001708
Iteration 124/1000 | Loss: 0.00001708
Iteration 125/1000 | Loss: 0.00001707
Iteration 126/1000 | Loss: 0.00001707
Iteration 127/1000 | Loss: 0.00001707
Iteration 128/1000 | Loss: 0.00001707
Iteration 129/1000 | Loss: 0.00001707
Iteration 130/1000 | Loss: 0.00001707
Iteration 131/1000 | Loss: 0.00001706
Iteration 132/1000 | Loss: 0.00001706
Iteration 133/1000 | Loss: 0.00001706
Iteration 134/1000 | Loss: 0.00001706
Iteration 135/1000 | Loss: 0.00001706
Iteration 136/1000 | Loss: 0.00001705
Iteration 137/1000 | Loss: 0.00001705
Iteration 138/1000 | Loss: 0.00001705
Iteration 139/1000 | Loss: 0.00001705
Iteration 140/1000 | Loss: 0.00001705
Iteration 141/1000 | Loss: 0.00001705
Iteration 142/1000 | Loss: 0.00001705
Iteration 143/1000 | Loss: 0.00001705
Iteration 144/1000 | Loss: 0.00001705
Iteration 145/1000 | Loss: 0.00001705
Iteration 146/1000 | Loss: 0.00001705
Iteration 147/1000 | Loss: 0.00001705
Iteration 148/1000 | Loss: 0.00001705
Iteration 149/1000 | Loss: 0.00001704
Iteration 150/1000 | Loss: 0.00001704
Iteration 151/1000 | Loss: 0.00001704
Iteration 152/1000 | Loss: 0.00001704
Iteration 153/1000 | Loss: 0.00001704
Iteration 154/1000 | Loss: 0.00001704
Iteration 155/1000 | Loss: 0.00001704
Iteration 156/1000 | Loss: 0.00001704
Iteration 157/1000 | Loss: 0.00001703
Iteration 158/1000 | Loss: 0.00001703
Iteration 159/1000 | Loss: 0.00001703
Iteration 160/1000 | Loss: 0.00001703
Iteration 161/1000 | Loss: 0.00001703
Iteration 162/1000 | Loss: 0.00001703
Iteration 163/1000 | Loss: 0.00001703
Iteration 164/1000 | Loss: 0.00001702
Iteration 165/1000 | Loss: 0.00001702
Iteration 166/1000 | Loss: 0.00001702
Iteration 167/1000 | Loss: 0.00001702
Iteration 168/1000 | Loss: 0.00001702
Iteration 169/1000 | Loss: 0.00001702
Iteration 170/1000 | Loss: 0.00001702
Iteration 171/1000 | Loss: 0.00001702
Iteration 172/1000 | Loss: 0.00001702
Iteration 173/1000 | Loss: 0.00001702
Iteration 174/1000 | Loss: 0.00001702
Iteration 175/1000 | Loss: 0.00001701
Iteration 176/1000 | Loss: 0.00001701
Iteration 177/1000 | Loss: 0.00001701
Iteration 178/1000 | Loss: 0.00001701
Iteration 179/1000 | Loss: 0.00001701
Iteration 180/1000 | Loss: 0.00001701
Iteration 181/1000 | Loss: 0.00001701
Iteration 182/1000 | Loss: 0.00001701
Iteration 183/1000 | Loss: 0.00001701
Iteration 184/1000 | Loss: 0.00001701
Iteration 185/1000 | Loss: 0.00001701
Iteration 186/1000 | Loss: 0.00001701
Iteration 187/1000 | Loss: 0.00001701
Iteration 188/1000 | Loss: 0.00001701
Iteration 189/1000 | Loss: 0.00001701
Iteration 190/1000 | Loss: 0.00001700
Iteration 191/1000 | Loss: 0.00001700
Iteration 192/1000 | Loss: 0.00001700
Iteration 193/1000 | Loss: 0.00001700
Iteration 194/1000 | Loss: 0.00001700
Iteration 195/1000 | Loss: 0.00001700
Iteration 196/1000 | Loss: 0.00001700
Iteration 197/1000 | Loss: 0.00001700
Iteration 198/1000 | Loss: 0.00001700
Iteration 199/1000 | Loss: 0.00001700
Iteration 200/1000 | Loss: 0.00001700
Iteration 201/1000 | Loss: 0.00001700
Iteration 202/1000 | Loss: 0.00001699
Iteration 203/1000 | Loss: 0.00001699
Iteration 204/1000 | Loss: 0.00001699
Iteration 205/1000 | Loss: 0.00001699
Iteration 206/1000 | Loss: 0.00001699
Iteration 207/1000 | Loss: 0.00001699
Iteration 208/1000 | Loss: 0.00001699
Iteration 209/1000 | Loss: 0.00001699
Iteration 210/1000 | Loss: 0.00001699
Iteration 211/1000 | Loss: 0.00001699
Iteration 212/1000 | Loss: 0.00001699
Iteration 213/1000 | Loss: 0.00001699
Iteration 214/1000 | Loss: 0.00001699
Iteration 215/1000 | Loss: 0.00001699
Iteration 216/1000 | Loss: 0.00001699
Iteration 217/1000 | Loss: 0.00001699
Iteration 218/1000 | Loss: 0.00001699
Iteration 219/1000 | Loss: 0.00001699
Iteration 220/1000 | Loss: 0.00001699
Iteration 221/1000 | Loss: 0.00001698
Iteration 222/1000 | Loss: 0.00001698
Iteration 223/1000 | Loss: 0.00001698
Iteration 224/1000 | Loss: 0.00001698
Iteration 225/1000 | Loss: 0.00001698
Iteration 226/1000 | Loss: 0.00001698
Iteration 227/1000 | Loss: 0.00001698
Iteration 228/1000 | Loss: 0.00001698
Iteration 229/1000 | Loss: 0.00001698
Iteration 230/1000 | Loss: 0.00001698
Iteration 231/1000 | Loss: 0.00001698
Iteration 232/1000 | Loss: 0.00001698
Iteration 233/1000 | Loss: 0.00001698
Iteration 234/1000 | Loss: 0.00001698
Iteration 235/1000 | Loss: 0.00001698
Iteration 236/1000 | Loss: 0.00001698
Iteration 237/1000 | Loss: 0.00001698
Iteration 238/1000 | Loss: 0.00001698
Iteration 239/1000 | Loss: 0.00001698
Iteration 240/1000 | Loss: 0.00001698
Iteration 241/1000 | Loss: 0.00001697
Iteration 242/1000 | Loss: 0.00001697
Iteration 243/1000 | Loss: 0.00001697
Iteration 244/1000 | Loss: 0.00001697
Iteration 245/1000 | Loss: 0.00001697
Iteration 246/1000 | Loss: 0.00001697
Iteration 247/1000 | Loss: 0.00001697
Iteration 248/1000 | Loss: 0.00001697
Iteration 249/1000 | Loss: 0.00001697
Iteration 250/1000 | Loss: 0.00001697
Iteration 251/1000 | Loss: 0.00001697
Iteration 252/1000 | Loss: 0.00001697
Iteration 253/1000 | Loss: 0.00001697
Iteration 254/1000 | Loss: 0.00001697
Iteration 255/1000 | Loss: 0.00001697
Iteration 256/1000 | Loss: 0.00001697
Iteration 257/1000 | Loss: 0.00001697
Iteration 258/1000 | Loss: 0.00001696
Iteration 259/1000 | Loss: 0.00001696
Iteration 260/1000 | Loss: 0.00001696
Iteration 261/1000 | Loss: 0.00001696
Iteration 262/1000 | Loss: 0.00001696
Iteration 263/1000 | Loss: 0.00001696
Iteration 264/1000 | Loss: 0.00001696
Iteration 265/1000 | Loss: 0.00001696
Iteration 266/1000 | Loss: 0.00001696
Iteration 267/1000 | Loss: 0.00001696
Iteration 268/1000 | Loss: 0.00001696
Iteration 269/1000 | Loss: 0.00001696
Iteration 270/1000 | Loss: 0.00001696
Iteration 271/1000 | Loss: 0.00001696
Iteration 272/1000 | Loss: 0.00001696
Iteration 273/1000 | Loss: 0.00001696
Iteration 274/1000 | Loss: 0.00001696
Iteration 275/1000 | Loss: 0.00001696
Iteration 276/1000 | Loss: 0.00001696
Iteration 277/1000 | Loss: 0.00001696
Iteration 278/1000 | Loss: 0.00001696
Iteration 279/1000 | Loss: 0.00001696
Iteration 280/1000 | Loss: 0.00001696
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 280. Stopping optimization.
Last 5 losses: [1.695833998383023e-05, 1.695833998383023e-05, 1.695833998383023e-05, 1.695833998383023e-05, 1.695833998383023e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.695833998383023e-05

Optimization complete. Final v2v error: 3.3870365619659424 mm

Highest mean error: 4.114395618438721 mm for frame 124

Lowest mean error: 2.5410385131835938 mm for frame 20

Saving results

Total time: 44.381194829940796
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00647798
Iteration 2/25 | Loss: 0.00115590
Iteration 3/25 | Loss: 0.00105469
Iteration 4/25 | Loss: 0.00104622
Iteration 5/25 | Loss: 0.00104372
Iteration 6/25 | Loss: 0.00104363
Iteration 7/25 | Loss: 0.00104363
Iteration 8/25 | Loss: 0.00104363
Iteration 9/25 | Loss: 0.00104363
Iteration 10/25 | Loss: 0.00104363
Iteration 11/25 | Loss: 0.00104363
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001043632160872221, 0.001043632160872221, 0.001043632160872221, 0.001043632160872221, 0.001043632160872221]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001043632160872221

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.09351778
Iteration 2/25 | Loss: 0.00062185
Iteration 3/25 | Loss: 0.00062178
Iteration 4/25 | Loss: 0.00062178
Iteration 5/25 | Loss: 0.00062178
Iteration 6/25 | Loss: 0.00062178
Iteration 7/25 | Loss: 0.00062178
Iteration 8/25 | Loss: 0.00062178
Iteration 9/25 | Loss: 0.00062178
Iteration 10/25 | Loss: 0.00062178
Iteration 11/25 | Loss: 0.00062178
Iteration 12/25 | Loss: 0.00062178
Iteration 13/25 | Loss: 0.00062178
Iteration 14/25 | Loss: 0.00062178
Iteration 15/25 | Loss: 0.00062178
Iteration 16/25 | Loss: 0.00062178
Iteration 17/25 | Loss: 0.00062178
Iteration 18/25 | Loss: 0.00062178
Iteration 19/25 | Loss: 0.00062178
Iteration 20/25 | Loss: 0.00062178
Iteration 21/25 | Loss: 0.00062178
Iteration 22/25 | Loss: 0.00062178
Iteration 23/25 | Loss: 0.00062178
Iteration 24/25 | Loss: 0.00062178
Iteration 25/25 | Loss: 0.00062178

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062178
Iteration 2/1000 | Loss: 0.00002115
Iteration 3/1000 | Loss: 0.00001606
Iteration 4/1000 | Loss: 0.00001447
Iteration 5/1000 | Loss: 0.00001366
Iteration 6/1000 | Loss: 0.00001318
Iteration 7/1000 | Loss: 0.00001288
Iteration 8/1000 | Loss: 0.00001261
Iteration 9/1000 | Loss: 0.00001237
Iteration 10/1000 | Loss: 0.00001227
Iteration 11/1000 | Loss: 0.00001222
Iteration 12/1000 | Loss: 0.00001219
Iteration 13/1000 | Loss: 0.00001209
Iteration 14/1000 | Loss: 0.00001208
Iteration 15/1000 | Loss: 0.00001207
Iteration 16/1000 | Loss: 0.00001205
Iteration 17/1000 | Loss: 0.00001205
Iteration 18/1000 | Loss: 0.00001204
Iteration 19/1000 | Loss: 0.00001204
Iteration 20/1000 | Loss: 0.00001202
Iteration 21/1000 | Loss: 0.00001198
Iteration 22/1000 | Loss: 0.00001198
Iteration 23/1000 | Loss: 0.00001196
Iteration 24/1000 | Loss: 0.00001195
Iteration 25/1000 | Loss: 0.00001193
Iteration 26/1000 | Loss: 0.00001193
Iteration 27/1000 | Loss: 0.00001193
Iteration 28/1000 | Loss: 0.00001191
Iteration 29/1000 | Loss: 0.00001190
Iteration 30/1000 | Loss: 0.00001189
Iteration 31/1000 | Loss: 0.00001188
Iteration 32/1000 | Loss: 0.00001188
Iteration 33/1000 | Loss: 0.00001188
Iteration 34/1000 | Loss: 0.00001187
Iteration 35/1000 | Loss: 0.00001186
Iteration 36/1000 | Loss: 0.00001185
Iteration 37/1000 | Loss: 0.00001185
Iteration 38/1000 | Loss: 0.00001185
Iteration 39/1000 | Loss: 0.00001185
Iteration 40/1000 | Loss: 0.00001184
Iteration 41/1000 | Loss: 0.00001184
Iteration 42/1000 | Loss: 0.00001184
Iteration 43/1000 | Loss: 0.00001184
Iteration 44/1000 | Loss: 0.00001184
Iteration 45/1000 | Loss: 0.00001183
Iteration 46/1000 | Loss: 0.00001183
Iteration 47/1000 | Loss: 0.00001183
Iteration 48/1000 | Loss: 0.00001182
Iteration 49/1000 | Loss: 0.00001182
Iteration 50/1000 | Loss: 0.00001181
Iteration 51/1000 | Loss: 0.00001181
Iteration 52/1000 | Loss: 0.00001181
Iteration 53/1000 | Loss: 0.00001180
Iteration 54/1000 | Loss: 0.00001180
Iteration 55/1000 | Loss: 0.00001179
Iteration 56/1000 | Loss: 0.00001179
Iteration 57/1000 | Loss: 0.00001179
Iteration 58/1000 | Loss: 0.00001178
Iteration 59/1000 | Loss: 0.00001178
Iteration 60/1000 | Loss: 0.00001178
Iteration 61/1000 | Loss: 0.00001177
Iteration 62/1000 | Loss: 0.00001177
Iteration 63/1000 | Loss: 0.00001177
Iteration 64/1000 | Loss: 0.00001176
Iteration 65/1000 | Loss: 0.00001176
Iteration 66/1000 | Loss: 0.00001176
Iteration 67/1000 | Loss: 0.00001176
Iteration 68/1000 | Loss: 0.00001176
Iteration 69/1000 | Loss: 0.00001175
Iteration 70/1000 | Loss: 0.00001175
Iteration 71/1000 | Loss: 0.00001175
Iteration 72/1000 | Loss: 0.00001174
Iteration 73/1000 | Loss: 0.00001174
Iteration 74/1000 | Loss: 0.00001173
Iteration 75/1000 | Loss: 0.00001173
Iteration 76/1000 | Loss: 0.00001173
Iteration 77/1000 | Loss: 0.00001172
Iteration 78/1000 | Loss: 0.00001172
Iteration 79/1000 | Loss: 0.00001172
Iteration 80/1000 | Loss: 0.00001172
Iteration 81/1000 | Loss: 0.00001171
Iteration 82/1000 | Loss: 0.00001171
Iteration 83/1000 | Loss: 0.00001171
Iteration 84/1000 | Loss: 0.00001171
Iteration 85/1000 | Loss: 0.00001170
Iteration 86/1000 | Loss: 0.00001170
Iteration 87/1000 | Loss: 0.00001170
Iteration 88/1000 | Loss: 0.00001170
Iteration 89/1000 | Loss: 0.00001170
Iteration 90/1000 | Loss: 0.00001169
Iteration 91/1000 | Loss: 0.00001169
Iteration 92/1000 | Loss: 0.00001169
Iteration 93/1000 | Loss: 0.00001168
Iteration 94/1000 | Loss: 0.00001168
Iteration 95/1000 | Loss: 0.00001168
Iteration 96/1000 | Loss: 0.00001167
Iteration 97/1000 | Loss: 0.00001167
Iteration 98/1000 | Loss: 0.00001167
Iteration 99/1000 | Loss: 0.00001167
Iteration 100/1000 | Loss: 0.00001167
Iteration 101/1000 | Loss: 0.00001166
Iteration 102/1000 | Loss: 0.00001166
Iteration 103/1000 | Loss: 0.00001166
Iteration 104/1000 | Loss: 0.00001166
Iteration 105/1000 | Loss: 0.00001166
Iteration 106/1000 | Loss: 0.00001166
Iteration 107/1000 | Loss: 0.00001165
Iteration 108/1000 | Loss: 0.00001165
Iteration 109/1000 | Loss: 0.00001165
Iteration 110/1000 | Loss: 0.00001165
Iteration 111/1000 | Loss: 0.00001165
Iteration 112/1000 | Loss: 0.00001165
Iteration 113/1000 | Loss: 0.00001165
Iteration 114/1000 | Loss: 0.00001165
Iteration 115/1000 | Loss: 0.00001164
Iteration 116/1000 | Loss: 0.00001164
Iteration 117/1000 | Loss: 0.00001164
Iteration 118/1000 | Loss: 0.00001164
Iteration 119/1000 | Loss: 0.00001164
Iteration 120/1000 | Loss: 0.00001164
Iteration 121/1000 | Loss: 0.00001163
Iteration 122/1000 | Loss: 0.00001163
Iteration 123/1000 | Loss: 0.00001163
Iteration 124/1000 | Loss: 0.00001163
Iteration 125/1000 | Loss: 0.00001163
Iteration 126/1000 | Loss: 0.00001163
Iteration 127/1000 | Loss: 0.00001162
Iteration 128/1000 | Loss: 0.00001162
Iteration 129/1000 | Loss: 0.00001162
Iteration 130/1000 | Loss: 0.00001162
Iteration 131/1000 | Loss: 0.00001162
Iteration 132/1000 | Loss: 0.00001161
Iteration 133/1000 | Loss: 0.00001161
Iteration 134/1000 | Loss: 0.00001161
Iteration 135/1000 | Loss: 0.00001161
Iteration 136/1000 | Loss: 0.00001161
Iteration 137/1000 | Loss: 0.00001161
Iteration 138/1000 | Loss: 0.00001160
Iteration 139/1000 | Loss: 0.00001160
Iteration 140/1000 | Loss: 0.00001160
Iteration 141/1000 | Loss: 0.00001160
Iteration 142/1000 | Loss: 0.00001160
Iteration 143/1000 | Loss: 0.00001160
Iteration 144/1000 | Loss: 0.00001160
Iteration 145/1000 | Loss: 0.00001160
Iteration 146/1000 | Loss: 0.00001159
Iteration 147/1000 | Loss: 0.00001159
Iteration 148/1000 | Loss: 0.00001159
Iteration 149/1000 | Loss: 0.00001159
Iteration 150/1000 | Loss: 0.00001159
Iteration 151/1000 | Loss: 0.00001159
Iteration 152/1000 | Loss: 0.00001159
Iteration 153/1000 | Loss: 0.00001159
Iteration 154/1000 | Loss: 0.00001158
Iteration 155/1000 | Loss: 0.00001158
Iteration 156/1000 | Loss: 0.00001158
Iteration 157/1000 | Loss: 0.00001158
Iteration 158/1000 | Loss: 0.00001158
Iteration 159/1000 | Loss: 0.00001158
Iteration 160/1000 | Loss: 0.00001158
Iteration 161/1000 | Loss: 0.00001157
Iteration 162/1000 | Loss: 0.00001157
Iteration 163/1000 | Loss: 0.00001157
Iteration 164/1000 | Loss: 0.00001157
Iteration 165/1000 | Loss: 0.00001157
Iteration 166/1000 | Loss: 0.00001157
Iteration 167/1000 | Loss: 0.00001157
Iteration 168/1000 | Loss: 0.00001157
Iteration 169/1000 | Loss: 0.00001157
Iteration 170/1000 | Loss: 0.00001157
Iteration 171/1000 | Loss: 0.00001157
Iteration 172/1000 | Loss: 0.00001157
Iteration 173/1000 | Loss: 0.00001157
Iteration 174/1000 | Loss: 0.00001156
Iteration 175/1000 | Loss: 0.00001156
Iteration 176/1000 | Loss: 0.00001156
Iteration 177/1000 | Loss: 0.00001156
Iteration 178/1000 | Loss: 0.00001156
Iteration 179/1000 | Loss: 0.00001156
Iteration 180/1000 | Loss: 0.00001156
Iteration 181/1000 | Loss: 0.00001156
Iteration 182/1000 | Loss: 0.00001156
Iteration 183/1000 | Loss: 0.00001156
Iteration 184/1000 | Loss: 0.00001156
Iteration 185/1000 | Loss: 0.00001156
Iteration 186/1000 | Loss: 0.00001156
Iteration 187/1000 | Loss: 0.00001156
Iteration 188/1000 | Loss: 0.00001156
Iteration 189/1000 | Loss: 0.00001156
Iteration 190/1000 | Loss: 0.00001156
Iteration 191/1000 | Loss: 0.00001156
Iteration 192/1000 | Loss: 0.00001156
Iteration 193/1000 | Loss: 0.00001156
Iteration 194/1000 | Loss: 0.00001156
Iteration 195/1000 | Loss: 0.00001156
Iteration 196/1000 | Loss: 0.00001156
Iteration 197/1000 | Loss: 0.00001156
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [1.156055077444762e-05, 1.156055077444762e-05, 1.156055077444762e-05, 1.156055077444762e-05, 1.156055077444762e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.156055077444762e-05

Optimization complete. Final v2v error: 2.8559153079986572 mm

Highest mean error: 3.44616961479187 mm for frame 140

Lowest mean error: 2.61797833442688 mm for frame 9

Saving results

Total time: 37.77826285362244
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00783387
Iteration 2/25 | Loss: 0.00124018
Iteration 3/25 | Loss: 0.00108496
Iteration 4/25 | Loss: 0.00106303
Iteration 5/25 | Loss: 0.00105702
Iteration 6/25 | Loss: 0.00105468
Iteration 7/25 | Loss: 0.00105468
Iteration 8/25 | Loss: 0.00105468
Iteration 9/25 | Loss: 0.00105468
Iteration 10/25 | Loss: 0.00105468
Iteration 11/25 | Loss: 0.00105468
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010546828852966428, 0.0010546828852966428, 0.0010546828852966428, 0.0010546828852966428, 0.0010546828852966428]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010546828852966428

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38015103
Iteration 2/25 | Loss: 0.00070456
Iteration 3/25 | Loss: 0.00070455
Iteration 4/25 | Loss: 0.00070455
Iteration 5/25 | Loss: 0.00070455
Iteration 6/25 | Loss: 0.00070455
Iteration 7/25 | Loss: 0.00070455
Iteration 8/25 | Loss: 0.00070455
Iteration 9/25 | Loss: 0.00070455
Iteration 10/25 | Loss: 0.00070455
Iteration 11/25 | Loss: 0.00070455
Iteration 12/25 | Loss: 0.00070455
Iteration 13/25 | Loss: 0.00070455
Iteration 14/25 | Loss: 0.00070455
Iteration 15/25 | Loss: 0.00070455
Iteration 16/25 | Loss: 0.00070455
Iteration 17/25 | Loss: 0.00070455
Iteration 18/25 | Loss: 0.00070455
Iteration 19/25 | Loss: 0.00070455
Iteration 20/25 | Loss: 0.00070455
Iteration 21/25 | Loss: 0.00070455
Iteration 22/25 | Loss: 0.00070455
Iteration 23/25 | Loss: 0.00070455
Iteration 24/25 | Loss: 0.00070455
Iteration 25/25 | Loss: 0.00070455

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070455
Iteration 2/1000 | Loss: 0.00003822
Iteration 3/1000 | Loss: 0.00002623
Iteration 4/1000 | Loss: 0.00002223
Iteration 5/1000 | Loss: 0.00002050
Iteration 6/1000 | Loss: 0.00001912
Iteration 7/1000 | Loss: 0.00001818
Iteration 8/1000 | Loss: 0.00001755
Iteration 9/1000 | Loss: 0.00001709
Iteration 10/1000 | Loss: 0.00001675
Iteration 11/1000 | Loss: 0.00001655
Iteration 12/1000 | Loss: 0.00001651
Iteration 13/1000 | Loss: 0.00001632
Iteration 14/1000 | Loss: 0.00001626
Iteration 15/1000 | Loss: 0.00001622
Iteration 16/1000 | Loss: 0.00001616
Iteration 17/1000 | Loss: 0.00001615
Iteration 18/1000 | Loss: 0.00001615
Iteration 19/1000 | Loss: 0.00001614
Iteration 20/1000 | Loss: 0.00001614
Iteration 21/1000 | Loss: 0.00001611
Iteration 22/1000 | Loss: 0.00001610
Iteration 23/1000 | Loss: 0.00001609
Iteration 24/1000 | Loss: 0.00001608
Iteration 25/1000 | Loss: 0.00001608
Iteration 26/1000 | Loss: 0.00001608
Iteration 27/1000 | Loss: 0.00001607
Iteration 28/1000 | Loss: 0.00001607
Iteration 29/1000 | Loss: 0.00001606
Iteration 30/1000 | Loss: 0.00001606
Iteration 31/1000 | Loss: 0.00001606
Iteration 32/1000 | Loss: 0.00001606
Iteration 33/1000 | Loss: 0.00001605
Iteration 34/1000 | Loss: 0.00001605
Iteration 35/1000 | Loss: 0.00001605
Iteration 36/1000 | Loss: 0.00001605
Iteration 37/1000 | Loss: 0.00001605
Iteration 38/1000 | Loss: 0.00001605
Iteration 39/1000 | Loss: 0.00001605
Iteration 40/1000 | Loss: 0.00001605
Iteration 41/1000 | Loss: 0.00001605
Iteration 42/1000 | Loss: 0.00001605
Iteration 43/1000 | Loss: 0.00001604
Iteration 44/1000 | Loss: 0.00001604
Iteration 45/1000 | Loss: 0.00001604
Iteration 46/1000 | Loss: 0.00001604
Iteration 47/1000 | Loss: 0.00001604
Iteration 48/1000 | Loss: 0.00001603
Iteration 49/1000 | Loss: 0.00001602
Iteration 50/1000 | Loss: 0.00001602
Iteration 51/1000 | Loss: 0.00001602
Iteration 52/1000 | Loss: 0.00001602
Iteration 53/1000 | Loss: 0.00001601
Iteration 54/1000 | Loss: 0.00001601
Iteration 55/1000 | Loss: 0.00001600
Iteration 56/1000 | Loss: 0.00001600
Iteration 57/1000 | Loss: 0.00001599
Iteration 58/1000 | Loss: 0.00001599
Iteration 59/1000 | Loss: 0.00001596
Iteration 60/1000 | Loss: 0.00001596
Iteration 61/1000 | Loss: 0.00001595
Iteration 62/1000 | Loss: 0.00001594
Iteration 63/1000 | Loss: 0.00001594
Iteration 64/1000 | Loss: 0.00001594
Iteration 65/1000 | Loss: 0.00001594
Iteration 66/1000 | Loss: 0.00001594
Iteration 67/1000 | Loss: 0.00001593
Iteration 68/1000 | Loss: 0.00001593
Iteration 69/1000 | Loss: 0.00001593
Iteration 70/1000 | Loss: 0.00001592
Iteration 71/1000 | Loss: 0.00001592
Iteration 72/1000 | Loss: 0.00001592
Iteration 73/1000 | Loss: 0.00001591
Iteration 74/1000 | Loss: 0.00001591
Iteration 75/1000 | Loss: 0.00001591
Iteration 76/1000 | Loss: 0.00001591
Iteration 77/1000 | Loss: 0.00001590
Iteration 78/1000 | Loss: 0.00001589
Iteration 79/1000 | Loss: 0.00001589
Iteration 80/1000 | Loss: 0.00001589
Iteration 81/1000 | Loss: 0.00001589
Iteration 82/1000 | Loss: 0.00001589
Iteration 83/1000 | Loss: 0.00001589
Iteration 84/1000 | Loss: 0.00001589
Iteration 85/1000 | Loss: 0.00001589
Iteration 86/1000 | Loss: 0.00001589
Iteration 87/1000 | Loss: 0.00001589
Iteration 88/1000 | Loss: 0.00001588
Iteration 89/1000 | Loss: 0.00001588
Iteration 90/1000 | Loss: 0.00001588
Iteration 91/1000 | Loss: 0.00001588
Iteration 92/1000 | Loss: 0.00001588
Iteration 93/1000 | Loss: 0.00001587
Iteration 94/1000 | Loss: 0.00001587
Iteration 95/1000 | Loss: 0.00001587
Iteration 96/1000 | Loss: 0.00001587
Iteration 97/1000 | Loss: 0.00001586
Iteration 98/1000 | Loss: 0.00001586
Iteration 99/1000 | Loss: 0.00001586
Iteration 100/1000 | Loss: 0.00001585
Iteration 101/1000 | Loss: 0.00001585
Iteration 102/1000 | Loss: 0.00001585
Iteration 103/1000 | Loss: 0.00001584
Iteration 104/1000 | Loss: 0.00001584
Iteration 105/1000 | Loss: 0.00001584
Iteration 106/1000 | Loss: 0.00001584
Iteration 107/1000 | Loss: 0.00001584
Iteration 108/1000 | Loss: 0.00001584
Iteration 109/1000 | Loss: 0.00001584
Iteration 110/1000 | Loss: 0.00001584
Iteration 111/1000 | Loss: 0.00001584
Iteration 112/1000 | Loss: 0.00001584
Iteration 113/1000 | Loss: 0.00001584
Iteration 114/1000 | Loss: 0.00001584
Iteration 115/1000 | Loss: 0.00001584
Iteration 116/1000 | Loss: 0.00001584
Iteration 117/1000 | Loss: 0.00001584
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [1.5835275917197578e-05, 1.5835275917197578e-05, 1.5835275917197578e-05, 1.5835275917197578e-05, 1.5835275917197578e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5835275917197578e-05

Optimization complete. Final v2v error: 3.2620151042938232 mm

Highest mean error: 4.265984535217285 mm for frame 187

Lowest mean error: 2.550382375717163 mm for frame 211

Saving results

Total time: 40.717116355895996
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00938892
Iteration 2/25 | Loss: 0.00231706
Iteration 3/25 | Loss: 0.00143418
Iteration 4/25 | Loss: 0.00129090
Iteration 5/25 | Loss: 0.00125306
Iteration 6/25 | Loss: 0.00122015
Iteration 7/25 | Loss: 0.00120514
Iteration 8/25 | Loss: 0.00119487
Iteration 9/25 | Loss: 0.00119166
Iteration 10/25 | Loss: 0.00118908
Iteration 11/25 | Loss: 0.00118805
Iteration 12/25 | Loss: 0.00118724
Iteration 13/25 | Loss: 0.00118694
Iteration 14/25 | Loss: 0.00118693
Iteration 15/25 | Loss: 0.00118387
Iteration 16/25 | Loss: 0.00118320
Iteration 17/25 | Loss: 0.00118297
Iteration 18/25 | Loss: 0.00118287
Iteration 19/25 | Loss: 0.00118280
Iteration 20/25 | Loss: 0.00118272
Iteration 21/25 | Loss: 0.00118266
Iteration 22/25 | Loss: 0.00118266
Iteration 23/25 | Loss: 0.00118259
Iteration 24/25 | Loss: 0.00118252
Iteration 25/25 | Loss: 0.00118815

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.89439142
Iteration 2/25 | Loss: 0.00068635
Iteration 3/25 | Loss: 0.00066274
Iteration 4/25 | Loss: 0.00066274
Iteration 5/25 | Loss: 0.00066274
Iteration 6/25 | Loss: 0.00066274
Iteration 7/25 | Loss: 0.00066274
Iteration 8/25 | Loss: 0.00066273
Iteration 9/25 | Loss: 0.00066273
Iteration 10/25 | Loss: 0.00066273
Iteration 11/25 | Loss: 0.00066273
Iteration 12/25 | Loss: 0.00066273
Iteration 13/25 | Loss: 0.00066273
Iteration 14/25 | Loss: 0.00066273
Iteration 15/25 | Loss: 0.00066273
Iteration 16/25 | Loss: 0.00066273
Iteration 17/25 | Loss: 0.00066273
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006627341499552131, 0.0006627341499552131, 0.0006627341499552131, 0.0006627341499552131, 0.0006627341499552131]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006627341499552131

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066273
Iteration 2/1000 | Loss: 0.00006948
Iteration 3/1000 | Loss: 0.00020309
Iteration 4/1000 | Loss: 0.00002751
Iteration 5/1000 | Loss: 0.00002433
Iteration 6/1000 | Loss: 0.00002267
Iteration 7/1000 | Loss: 0.00002128
Iteration 8/1000 | Loss: 0.00002043
Iteration 9/1000 | Loss: 0.00001996
Iteration 10/1000 | Loss: 0.00014634
Iteration 11/1000 | Loss: 0.00003818
Iteration 12/1000 | Loss: 0.00002286
Iteration 13/1000 | Loss: 0.00001948
Iteration 14/1000 | Loss: 0.00001926
Iteration 15/1000 | Loss: 0.00001923
Iteration 16/1000 | Loss: 0.00001919
Iteration 17/1000 | Loss: 0.00001908
Iteration 18/1000 | Loss: 0.00001900
Iteration 19/1000 | Loss: 0.00001900
Iteration 20/1000 | Loss: 0.00001899
Iteration 21/1000 | Loss: 0.00001898
Iteration 22/1000 | Loss: 0.00001898
Iteration 23/1000 | Loss: 0.00001897
Iteration 24/1000 | Loss: 0.00001897
Iteration 25/1000 | Loss: 0.00001896
Iteration 26/1000 | Loss: 0.00001896
Iteration 27/1000 | Loss: 0.00001895
Iteration 28/1000 | Loss: 0.00001894
Iteration 29/1000 | Loss: 0.00001893
Iteration 30/1000 | Loss: 0.00001893
Iteration 31/1000 | Loss: 0.00001893
Iteration 32/1000 | Loss: 0.00001892
Iteration 33/1000 | Loss: 0.00001892
Iteration 34/1000 | Loss: 0.00001892
Iteration 35/1000 | Loss: 0.00001892
Iteration 36/1000 | Loss: 0.00001892
Iteration 37/1000 | Loss: 0.00001892
Iteration 38/1000 | Loss: 0.00001892
Iteration 39/1000 | Loss: 0.00001892
Iteration 40/1000 | Loss: 0.00001892
Iteration 41/1000 | Loss: 0.00001892
Iteration 42/1000 | Loss: 0.00001892
Iteration 43/1000 | Loss: 0.00001892
Iteration 44/1000 | Loss: 0.00001891
Iteration 45/1000 | Loss: 0.00001890
Iteration 46/1000 | Loss: 0.00001890
Iteration 47/1000 | Loss: 0.00001889
Iteration 48/1000 | Loss: 0.00001889
Iteration 49/1000 | Loss: 0.00001889
Iteration 50/1000 | Loss: 0.00001889
Iteration 51/1000 | Loss: 0.00001888
Iteration 52/1000 | Loss: 0.00001888
Iteration 53/1000 | Loss: 0.00001888
Iteration 54/1000 | Loss: 0.00001888
Iteration 55/1000 | Loss: 0.00001888
Iteration 56/1000 | Loss: 0.00001887
Iteration 57/1000 | Loss: 0.00001887
Iteration 58/1000 | Loss: 0.00001886
Iteration 59/1000 | Loss: 0.00001886
Iteration 60/1000 | Loss: 0.00001885
Iteration 61/1000 | Loss: 0.00001885
Iteration 62/1000 | Loss: 0.00001885
Iteration 63/1000 | Loss: 0.00001885
Iteration 64/1000 | Loss: 0.00001884
Iteration 65/1000 | Loss: 0.00001881
Iteration 66/1000 | Loss: 0.00001881
Iteration 67/1000 | Loss: 0.00001878
Iteration 68/1000 | Loss: 0.00001878
Iteration 69/1000 | Loss: 0.00001878
Iteration 70/1000 | Loss: 0.00001878
Iteration 71/1000 | Loss: 0.00001878
Iteration 72/1000 | Loss: 0.00001878
Iteration 73/1000 | Loss: 0.00001878
Iteration 74/1000 | Loss: 0.00001878
Iteration 75/1000 | Loss: 0.00001877
Iteration 76/1000 | Loss: 0.00001876
Iteration 77/1000 | Loss: 0.00001876
Iteration 78/1000 | Loss: 0.00001875
Iteration 79/1000 | Loss: 0.00001875
Iteration 80/1000 | Loss: 0.00001875
Iteration 81/1000 | Loss: 0.00001875
Iteration 82/1000 | Loss: 0.00001875
Iteration 83/1000 | Loss: 0.00001875
Iteration 84/1000 | Loss: 0.00001875
Iteration 85/1000 | Loss: 0.00001875
Iteration 86/1000 | Loss: 0.00001875
Iteration 87/1000 | Loss: 0.00001874
Iteration 88/1000 | Loss: 0.00001874
Iteration 89/1000 | Loss: 0.00001874
Iteration 90/1000 | Loss: 0.00001874
Iteration 91/1000 | Loss: 0.00001874
Iteration 92/1000 | Loss: 0.00001874
Iteration 93/1000 | Loss: 0.00001874
Iteration 94/1000 | Loss: 0.00001873
Iteration 95/1000 | Loss: 0.00001873
Iteration 96/1000 | Loss: 0.00001873
Iteration 97/1000 | Loss: 0.00001873
Iteration 98/1000 | Loss: 0.00001873
Iteration 99/1000 | Loss: 0.00001873
Iteration 100/1000 | Loss: 0.00001873
Iteration 101/1000 | Loss: 0.00001872
Iteration 102/1000 | Loss: 0.00001872
Iteration 103/1000 | Loss: 0.00001872
Iteration 104/1000 | Loss: 0.00001872
Iteration 105/1000 | Loss: 0.00001872
Iteration 106/1000 | Loss: 0.00001872
Iteration 107/1000 | Loss: 0.00001872
Iteration 108/1000 | Loss: 0.00001872
Iteration 109/1000 | Loss: 0.00001872
Iteration 110/1000 | Loss: 0.00001872
Iteration 111/1000 | Loss: 0.00001872
Iteration 112/1000 | Loss: 0.00001872
Iteration 113/1000 | Loss: 0.00001872
Iteration 114/1000 | Loss: 0.00001871
Iteration 115/1000 | Loss: 0.00001871
Iteration 116/1000 | Loss: 0.00001871
Iteration 117/1000 | Loss: 0.00001871
Iteration 118/1000 | Loss: 0.00001871
Iteration 119/1000 | Loss: 0.00001871
Iteration 120/1000 | Loss: 0.00001871
Iteration 121/1000 | Loss: 0.00001871
Iteration 122/1000 | Loss: 0.00001871
Iteration 123/1000 | Loss: 0.00001871
Iteration 124/1000 | Loss: 0.00001871
Iteration 125/1000 | Loss: 0.00001871
Iteration 126/1000 | Loss: 0.00001870
Iteration 127/1000 | Loss: 0.00001870
Iteration 128/1000 | Loss: 0.00001870
Iteration 129/1000 | Loss: 0.00001870
Iteration 130/1000 | Loss: 0.00001870
Iteration 131/1000 | Loss: 0.00001870
Iteration 132/1000 | Loss: 0.00001870
Iteration 133/1000 | Loss: 0.00001870
Iteration 134/1000 | Loss: 0.00001870
Iteration 135/1000 | Loss: 0.00001870
Iteration 136/1000 | Loss: 0.00001869
Iteration 137/1000 | Loss: 0.00001869
Iteration 138/1000 | Loss: 0.00001869
Iteration 139/1000 | Loss: 0.00001869
Iteration 140/1000 | Loss: 0.00001869
Iteration 141/1000 | Loss: 0.00001869
Iteration 142/1000 | Loss: 0.00001869
Iteration 143/1000 | Loss: 0.00001869
Iteration 144/1000 | Loss: 0.00001869
Iteration 145/1000 | Loss: 0.00001869
Iteration 146/1000 | Loss: 0.00001869
Iteration 147/1000 | Loss: 0.00001869
Iteration 148/1000 | Loss: 0.00001869
Iteration 149/1000 | Loss: 0.00001869
Iteration 150/1000 | Loss: 0.00001869
Iteration 151/1000 | Loss: 0.00001868
Iteration 152/1000 | Loss: 0.00001868
Iteration 153/1000 | Loss: 0.00001868
Iteration 154/1000 | Loss: 0.00001868
Iteration 155/1000 | Loss: 0.00001868
Iteration 156/1000 | Loss: 0.00001868
Iteration 157/1000 | Loss: 0.00001868
Iteration 158/1000 | Loss: 0.00001868
Iteration 159/1000 | Loss: 0.00001868
Iteration 160/1000 | Loss: 0.00001868
Iteration 161/1000 | Loss: 0.00001868
Iteration 162/1000 | Loss: 0.00001868
Iteration 163/1000 | Loss: 0.00001868
Iteration 164/1000 | Loss: 0.00001868
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.8682127119973302e-05, 1.8682127119973302e-05, 1.8682127119973302e-05, 1.8682127119973302e-05, 1.8682127119973302e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8682127119973302e-05

Optimization complete. Final v2v error: 3.452120780944824 mm

Highest mean error: 4.786414623260498 mm for frame 11

Lowest mean error: 3.1633143424987793 mm for frame 151

Saving results

Total time: 85.72681856155396
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00320647
Iteration 2/25 | Loss: 0.00111926
Iteration 3/25 | Loss: 0.00098240
Iteration 4/25 | Loss: 0.00096526
Iteration 5/25 | Loss: 0.00095940
Iteration 6/25 | Loss: 0.00095754
Iteration 7/25 | Loss: 0.00095754
Iteration 8/25 | Loss: 0.00095754
Iteration 9/25 | Loss: 0.00095754
Iteration 10/25 | Loss: 0.00095754
Iteration 11/25 | Loss: 0.00095754
Iteration 12/25 | Loss: 0.00095754
Iteration 13/25 | Loss: 0.00095754
Iteration 14/25 | Loss: 0.00095754
Iteration 15/25 | Loss: 0.00095754
Iteration 16/25 | Loss: 0.00095754
Iteration 17/25 | Loss: 0.00095754
Iteration 18/25 | Loss: 0.00095754
Iteration 19/25 | Loss: 0.00095754
Iteration 20/25 | Loss: 0.00095754
Iteration 21/25 | Loss: 0.00095754
Iteration 22/25 | Loss: 0.00095754
Iteration 23/25 | Loss: 0.00095754
Iteration 24/25 | Loss: 0.00095754
Iteration 25/25 | Loss: 0.00095754

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36040378
Iteration 2/25 | Loss: 0.00068799
Iteration 3/25 | Loss: 0.00068798
Iteration 4/25 | Loss: 0.00068798
Iteration 5/25 | Loss: 0.00068798
Iteration 6/25 | Loss: 0.00068798
Iteration 7/25 | Loss: 0.00068798
Iteration 8/25 | Loss: 0.00068798
Iteration 9/25 | Loss: 0.00068798
Iteration 10/25 | Loss: 0.00068798
Iteration 11/25 | Loss: 0.00068798
Iteration 12/25 | Loss: 0.00068798
Iteration 13/25 | Loss: 0.00068798
Iteration 14/25 | Loss: 0.00068798
Iteration 15/25 | Loss: 0.00068798
Iteration 16/25 | Loss: 0.00068798
Iteration 17/25 | Loss: 0.00068798
Iteration 18/25 | Loss: 0.00068798
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006879777647554874, 0.0006879777647554874, 0.0006879777647554874, 0.0006879777647554874, 0.0006879777647554874]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006879777647554874

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068798
Iteration 2/1000 | Loss: 0.00002713
Iteration 3/1000 | Loss: 0.00001598
Iteration 4/1000 | Loss: 0.00001374
Iteration 5/1000 | Loss: 0.00001281
Iteration 6/1000 | Loss: 0.00001177
Iteration 7/1000 | Loss: 0.00001128
Iteration 8/1000 | Loss: 0.00001088
Iteration 9/1000 | Loss: 0.00001062
Iteration 10/1000 | Loss: 0.00001037
Iteration 11/1000 | Loss: 0.00001021
Iteration 12/1000 | Loss: 0.00001020
Iteration 13/1000 | Loss: 0.00001019
Iteration 14/1000 | Loss: 0.00001017
Iteration 15/1000 | Loss: 0.00001014
Iteration 16/1000 | Loss: 0.00001007
Iteration 17/1000 | Loss: 0.00001006
Iteration 18/1000 | Loss: 0.00001006
Iteration 19/1000 | Loss: 0.00001005
Iteration 20/1000 | Loss: 0.00001005
Iteration 21/1000 | Loss: 0.00001004
Iteration 22/1000 | Loss: 0.00001004
Iteration 23/1000 | Loss: 0.00001003
Iteration 24/1000 | Loss: 0.00001003
Iteration 25/1000 | Loss: 0.00001002
Iteration 26/1000 | Loss: 0.00001002
Iteration 27/1000 | Loss: 0.00001001
Iteration 28/1000 | Loss: 0.00001001
Iteration 29/1000 | Loss: 0.00001001
Iteration 30/1000 | Loss: 0.00001001
Iteration 31/1000 | Loss: 0.00001000
Iteration 32/1000 | Loss: 0.00001000
Iteration 33/1000 | Loss: 0.00001000
Iteration 34/1000 | Loss: 0.00001000
Iteration 35/1000 | Loss: 0.00000999
Iteration 36/1000 | Loss: 0.00000999
Iteration 37/1000 | Loss: 0.00000999
Iteration 38/1000 | Loss: 0.00000999
Iteration 39/1000 | Loss: 0.00000998
Iteration 40/1000 | Loss: 0.00000998
Iteration 41/1000 | Loss: 0.00000998
Iteration 42/1000 | Loss: 0.00000998
Iteration 43/1000 | Loss: 0.00000997
Iteration 44/1000 | Loss: 0.00000997
Iteration 45/1000 | Loss: 0.00000997
Iteration 46/1000 | Loss: 0.00000996
Iteration 47/1000 | Loss: 0.00000996
Iteration 48/1000 | Loss: 0.00000995
Iteration 49/1000 | Loss: 0.00000995
Iteration 50/1000 | Loss: 0.00000995
Iteration 51/1000 | Loss: 0.00000994
Iteration 52/1000 | Loss: 0.00000994
Iteration 53/1000 | Loss: 0.00000994
Iteration 54/1000 | Loss: 0.00000994
Iteration 55/1000 | Loss: 0.00000994
Iteration 56/1000 | Loss: 0.00000994
Iteration 57/1000 | Loss: 0.00000994
Iteration 58/1000 | Loss: 0.00000994
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 58. Stopping optimization.
Last 5 losses: [9.94203128357185e-06, 9.94203128357185e-06, 9.94203128357185e-06, 9.94203128357185e-06, 9.94203128357185e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.94203128357185e-06

Optimization complete. Final v2v error: 2.654233694076538 mm

Highest mean error: 3.20462965965271 mm for frame 94

Lowest mean error: 2.270246982574463 mm for frame 266

Saving results

Total time: 34.58547878265381
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00442658
Iteration 2/25 | Loss: 0.00118128
Iteration 3/25 | Loss: 0.00105123
Iteration 4/25 | Loss: 0.00102784
Iteration 5/25 | Loss: 0.00102119
Iteration 6/25 | Loss: 0.00101994
Iteration 7/25 | Loss: 0.00101963
Iteration 8/25 | Loss: 0.00101963
Iteration 9/25 | Loss: 0.00101963
Iteration 10/25 | Loss: 0.00101963
Iteration 11/25 | Loss: 0.00101963
Iteration 12/25 | Loss: 0.00101963
Iteration 13/25 | Loss: 0.00101963
Iteration 14/25 | Loss: 0.00101963
Iteration 15/25 | Loss: 0.00101963
Iteration 16/25 | Loss: 0.00101963
Iteration 17/25 | Loss: 0.00101963
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001019628718495369, 0.001019628718495369, 0.001019628718495369, 0.001019628718495369, 0.001019628718495369]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001019628718495369

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37390792
Iteration 2/25 | Loss: 0.00066559
Iteration 3/25 | Loss: 0.00066559
Iteration 4/25 | Loss: 0.00066559
Iteration 5/25 | Loss: 0.00066559
Iteration 6/25 | Loss: 0.00066559
Iteration 7/25 | Loss: 0.00066559
Iteration 8/25 | Loss: 0.00066559
Iteration 9/25 | Loss: 0.00066559
Iteration 10/25 | Loss: 0.00066559
Iteration 11/25 | Loss: 0.00066559
Iteration 12/25 | Loss: 0.00066559
Iteration 13/25 | Loss: 0.00066559
Iteration 14/25 | Loss: 0.00066559
Iteration 15/25 | Loss: 0.00066559
Iteration 16/25 | Loss: 0.00066559
Iteration 17/25 | Loss: 0.00066559
Iteration 18/25 | Loss: 0.00066559
Iteration 19/25 | Loss: 0.00066559
Iteration 20/25 | Loss: 0.00066559
Iteration 21/25 | Loss: 0.00066559
Iteration 22/25 | Loss: 0.00066559
Iteration 23/25 | Loss: 0.00066559
Iteration 24/25 | Loss: 0.00066559
Iteration 25/25 | Loss: 0.00066559

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066559
Iteration 2/1000 | Loss: 0.00002983
Iteration 3/1000 | Loss: 0.00002212
Iteration 4/1000 | Loss: 0.00001984
Iteration 5/1000 | Loss: 0.00001876
Iteration 6/1000 | Loss: 0.00001809
Iteration 7/1000 | Loss: 0.00001756
Iteration 8/1000 | Loss: 0.00001711
Iteration 9/1000 | Loss: 0.00001681
Iteration 10/1000 | Loss: 0.00001661
Iteration 11/1000 | Loss: 0.00001639
Iteration 12/1000 | Loss: 0.00001636
Iteration 13/1000 | Loss: 0.00001629
Iteration 14/1000 | Loss: 0.00001617
Iteration 15/1000 | Loss: 0.00001613
Iteration 16/1000 | Loss: 0.00001613
Iteration 17/1000 | Loss: 0.00001607
Iteration 18/1000 | Loss: 0.00001607
Iteration 19/1000 | Loss: 0.00001607
Iteration 20/1000 | Loss: 0.00001603
Iteration 21/1000 | Loss: 0.00001599
Iteration 22/1000 | Loss: 0.00001598
Iteration 23/1000 | Loss: 0.00001598
Iteration 24/1000 | Loss: 0.00001598
Iteration 25/1000 | Loss: 0.00001597
Iteration 26/1000 | Loss: 0.00001597
Iteration 27/1000 | Loss: 0.00001593
Iteration 28/1000 | Loss: 0.00001588
Iteration 29/1000 | Loss: 0.00001588
Iteration 30/1000 | Loss: 0.00001586
Iteration 31/1000 | Loss: 0.00001586
Iteration 32/1000 | Loss: 0.00001585
Iteration 33/1000 | Loss: 0.00001583
Iteration 34/1000 | Loss: 0.00001583
Iteration 35/1000 | Loss: 0.00001582
Iteration 36/1000 | Loss: 0.00001582
Iteration 37/1000 | Loss: 0.00001582
Iteration 38/1000 | Loss: 0.00001581
Iteration 39/1000 | Loss: 0.00001581
Iteration 40/1000 | Loss: 0.00001580
Iteration 41/1000 | Loss: 0.00001580
Iteration 42/1000 | Loss: 0.00001580
Iteration 43/1000 | Loss: 0.00001580
Iteration 44/1000 | Loss: 0.00001580
Iteration 45/1000 | Loss: 0.00001579
Iteration 46/1000 | Loss: 0.00001579
Iteration 47/1000 | Loss: 0.00001579
Iteration 48/1000 | Loss: 0.00001578
Iteration 49/1000 | Loss: 0.00001578
Iteration 50/1000 | Loss: 0.00001578
Iteration 51/1000 | Loss: 0.00001577
Iteration 52/1000 | Loss: 0.00001577
Iteration 53/1000 | Loss: 0.00001577
Iteration 54/1000 | Loss: 0.00001577
Iteration 55/1000 | Loss: 0.00001576
Iteration 56/1000 | Loss: 0.00001576
Iteration 57/1000 | Loss: 0.00001576
Iteration 58/1000 | Loss: 0.00001576
Iteration 59/1000 | Loss: 0.00001576
Iteration 60/1000 | Loss: 0.00001576
Iteration 61/1000 | Loss: 0.00001575
Iteration 62/1000 | Loss: 0.00001575
Iteration 63/1000 | Loss: 0.00001574
Iteration 64/1000 | Loss: 0.00001574
Iteration 65/1000 | Loss: 0.00001574
Iteration 66/1000 | Loss: 0.00001574
Iteration 67/1000 | Loss: 0.00001574
Iteration 68/1000 | Loss: 0.00001574
Iteration 69/1000 | Loss: 0.00001574
Iteration 70/1000 | Loss: 0.00001574
Iteration 71/1000 | Loss: 0.00001573
Iteration 72/1000 | Loss: 0.00001573
Iteration 73/1000 | Loss: 0.00001573
Iteration 74/1000 | Loss: 0.00001572
Iteration 75/1000 | Loss: 0.00001572
Iteration 76/1000 | Loss: 0.00001572
Iteration 77/1000 | Loss: 0.00001572
Iteration 78/1000 | Loss: 0.00001572
Iteration 79/1000 | Loss: 0.00001572
Iteration 80/1000 | Loss: 0.00001572
Iteration 81/1000 | Loss: 0.00001571
Iteration 82/1000 | Loss: 0.00001571
Iteration 83/1000 | Loss: 0.00001571
Iteration 84/1000 | Loss: 0.00001571
Iteration 85/1000 | Loss: 0.00001571
Iteration 86/1000 | Loss: 0.00001570
Iteration 87/1000 | Loss: 0.00001570
Iteration 88/1000 | Loss: 0.00001570
Iteration 89/1000 | Loss: 0.00001570
Iteration 90/1000 | Loss: 0.00001570
Iteration 91/1000 | Loss: 0.00001570
Iteration 92/1000 | Loss: 0.00001570
Iteration 93/1000 | Loss: 0.00001570
Iteration 94/1000 | Loss: 0.00001569
Iteration 95/1000 | Loss: 0.00001569
Iteration 96/1000 | Loss: 0.00001569
Iteration 97/1000 | Loss: 0.00001569
Iteration 98/1000 | Loss: 0.00001569
Iteration 99/1000 | Loss: 0.00001569
Iteration 100/1000 | Loss: 0.00001569
Iteration 101/1000 | Loss: 0.00001569
Iteration 102/1000 | Loss: 0.00001569
Iteration 103/1000 | Loss: 0.00001569
Iteration 104/1000 | Loss: 0.00001569
Iteration 105/1000 | Loss: 0.00001568
Iteration 106/1000 | Loss: 0.00001568
Iteration 107/1000 | Loss: 0.00001568
Iteration 108/1000 | Loss: 0.00001568
Iteration 109/1000 | Loss: 0.00001568
Iteration 110/1000 | Loss: 0.00001568
Iteration 111/1000 | Loss: 0.00001568
Iteration 112/1000 | Loss: 0.00001568
Iteration 113/1000 | Loss: 0.00001567
Iteration 114/1000 | Loss: 0.00001567
Iteration 115/1000 | Loss: 0.00001567
Iteration 116/1000 | Loss: 0.00001567
Iteration 117/1000 | Loss: 0.00001567
Iteration 118/1000 | Loss: 0.00001566
Iteration 119/1000 | Loss: 0.00001566
Iteration 120/1000 | Loss: 0.00001566
Iteration 121/1000 | Loss: 0.00001566
Iteration 122/1000 | Loss: 0.00001566
Iteration 123/1000 | Loss: 0.00001566
Iteration 124/1000 | Loss: 0.00001566
Iteration 125/1000 | Loss: 0.00001566
Iteration 126/1000 | Loss: 0.00001566
Iteration 127/1000 | Loss: 0.00001566
Iteration 128/1000 | Loss: 0.00001566
Iteration 129/1000 | Loss: 0.00001566
Iteration 130/1000 | Loss: 0.00001566
Iteration 131/1000 | Loss: 0.00001565
Iteration 132/1000 | Loss: 0.00001565
Iteration 133/1000 | Loss: 0.00001565
Iteration 134/1000 | Loss: 0.00001565
Iteration 135/1000 | Loss: 0.00001565
Iteration 136/1000 | Loss: 0.00001565
Iteration 137/1000 | Loss: 0.00001565
Iteration 138/1000 | Loss: 0.00001565
Iteration 139/1000 | Loss: 0.00001565
Iteration 140/1000 | Loss: 0.00001565
Iteration 141/1000 | Loss: 0.00001565
Iteration 142/1000 | Loss: 0.00001564
Iteration 143/1000 | Loss: 0.00001564
Iteration 144/1000 | Loss: 0.00001564
Iteration 145/1000 | Loss: 0.00001564
Iteration 146/1000 | Loss: 0.00001564
Iteration 147/1000 | Loss: 0.00001564
Iteration 148/1000 | Loss: 0.00001564
Iteration 149/1000 | Loss: 0.00001564
Iteration 150/1000 | Loss: 0.00001564
Iteration 151/1000 | Loss: 0.00001564
Iteration 152/1000 | Loss: 0.00001564
Iteration 153/1000 | Loss: 0.00001564
Iteration 154/1000 | Loss: 0.00001564
Iteration 155/1000 | Loss: 0.00001564
Iteration 156/1000 | Loss: 0.00001564
Iteration 157/1000 | Loss: 0.00001564
Iteration 158/1000 | Loss: 0.00001564
Iteration 159/1000 | Loss: 0.00001564
Iteration 160/1000 | Loss: 0.00001564
Iteration 161/1000 | Loss: 0.00001564
Iteration 162/1000 | Loss: 0.00001564
Iteration 163/1000 | Loss: 0.00001564
Iteration 164/1000 | Loss: 0.00001564
Iteration 165/1000 | Loss: 0.00001564
Iteration 166/1000 | Loss: 0.00001564
Iteration 167/1000 | Loss: 0.00001564
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.5638785043847747e-05, 1.5638785043847747e-05, 1.5638785043847747e-05, 1.5638785043847747e-05, 1.5638785043847747e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5638785043847747e-05

Optimization complete. Final v2v error: 3.3281450271606445 mm

Highest mean error: 4.22867488861084 mm for frame 30

Lowest mean error: 3.157198190689087 mm for frame 3

Saving results

Total time: 40.80163502693176
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01052822
Iteration 2/25 | Loss: 0.00172758
Iteration 3/25 | Loss: 0.00134229
Iteration 4/25 | Loss: 0.00127383
Iteration 5/25 | Loss: 0.00125443
Iteration 6/25 | Loss: 0.00123576
Iteration 7/25 | Loss: 0.00122954
Iteration 8/25 | Loss: 0.00119979
Iteration 9/25 | Loss: 0.00119356
Iteration 10/25 | Loss: 0.00118751
Iteration 11/25 | Loss: 0.00118482
Iteration 12/25 | Loss: 0.00118333
Iteration 13/25 | Loss: 0.00118372
Iteration 14/25 | Loss: 0.00118191
Iteration 15/25 | Loss: 0.00118416
Iteration 16/25 | Loss: 0.00118699
Iteration 17/25 | Loss: 0.00118305
Iteration 18/25 | Loss: 0.00118442
Iteration 19/25 | Loss: 0.00118503
Iteration 20/25 | Loss: 0.00118450
Iteration 21/25 | Loss: 0.00118498
Iteration 22/25 | Loss: 0.00118195
Iteration 23/25 | Loss: 0.00118191
Iteration 24/25 | Loss: 0.00118015
Iteration 25/25 | Loss: 0.00118169

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38918531
Iteration 2/25 | Loss: 0.00097404
Iteration 3/25 | Loss: 0.00087346
Iteration 4/25 | Loss: 0.00087346
Iteration 5/25 | Loss: 0.00087346
Iteration 6/25 | Loss: 0.00087346
Iteration 7/25 | Loss: 0.00087346
Iteration 8/25 | Loss: 0.00087346
Iteration 9/25 | Loss: 0.00087346
Iteration 10/25 | Loss: 0.00087346
Iteration 11/25 | Loss: 0.00087346
Iteration 12/25 | Loss: 0.00087346
Iteration 13/25 | Loss: 0.00087346
Iteration 14/25 | Loss: 0.00087346
Iteration 15/25 | Loss: 0.00087346
Iteration 16/25 | Loss: 0.00087346
Iteration 17/25 | Loss: 0.00087346
Iteration 18/25 | Loss: 0.00087346
Iteration 19/25 | Loss: 0.00087346
Iteration 20/25 | Loss: 0.00087346
Iteration 21/25 | Loss: 0.00087346
Iteration 22/25 | Loss: 0.00087346
Iteration 23/25 | Loss: 0.00087346
Iteration 24/25 | Loss: 0.00087346
Iteration 25/25 | Loss: 0.00087346

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087346
Iteration 2/1000 | Loss: 0.00006656
Iteration 3/1000 | Loss: 0.00004272
Iteration 4/1000 | Loss: 0.00013589
Iteration 5/1000 | Loss: 0.00103124
Iteration 6/1000 | Loss: 0.00008774
Iteration 7/1000 | Loss: 0.00007769
Iteration 8/1000 | Loss: 0.00017479
Iteration 9/1000 | Loss: 0.00004882
Iteration 10/1000 | Loss: 0.00007083
Iteration 11/1000 | Loss: 0.00003600
Iteration 12/1000 | Loss: 0.00003491
Iteration 13/1000 | Loss: 0.00009904
Iteration 14/1000 | Loss: 0.00003437
Iteration 15/1000 | Loss: 0.00006165
Iteration 16/1000 | Loss: 0.00015167
Iteration 17/1000 | Loss: 0.00003372
Iteration 18/1000 | Loss: 0.00018276
Iteration 19/1000 | Loss: 0.00005987
Iteration 20/1000 | Loss: 0.00008457
Iteration 21/1000 | Loss: 0.00003654
Iteration 22/1000 | Loss: 0.00003328
Iteration 23/1000 | Loss: 0.00003313
Iteration 24/1000 | Loss: 0.00003310
Iteration 25/1000 | Loss: 0.00003309
Iteration 26/1000 | Loss: 0.00003308
Iteration 27/1000 | Loss: 0.00003308
Iteration 28/1000 | Loss: 0.00003307
Iteration 29/1000 | Loss: 0.00003303
Iteration 30/1000 | Loss: 0.00003302
Iteration 31/1000 | Loss: 0.00003300
Iteration 32/1000 | Loss: 0.00003299
Iteration 33/1000 | Loss: 0.00003298
Iteration 34/1000 | Loss: 0.00005388
Iteration 35/1000 | Loss: 0.00003289
Iteration 36/1000 | Loss: 0.00003287
Iteration 37/1000 | Loss: 0.00003285
Iteration 38/1000 | Loss: 0.00003283
Iteration 39/1000 | Loss: 0.00005763
Iteration 40/1000 | Loss: 0.00007697
Iteration 41/1000 | Loss: 0.00007195
Iteration 42/1000 | Loss: 0.00005263
Iteration 43/1000 | Loss: 0.00006203
Iteration 44/1000 | Loss: 0.00003417
Iteration 45/1000 | Loss: 0.00003261
Iteration 46/1000 | Loss: 0.00003258
Iteration 47/1000 | Loss: 0.00004547
Iteration 48/1000 | Loss: 0.00003426
Iteration 49/1000 | Loss: 0.00003256
Iteration 50/1000 | Loss: 0.00003256
Iteration 51/1000 | Loss: 0.00003256
Iteration 52/1000 | Loss: 0.00003256
Iteration 53/1000 | Loss: 0.00003256
Iteration 54/1000 | Loss: 0.00003256
Iteration 55/1000 | Loss: 0.00003256
Iteration 56/1000 | Loss: 0.00003256
Iteration 57/1000 | Loss: 0.00003256
Iteration 58/1000 | Loss: 0.00003256
Iteration 59/1000 | Loss: 0.00003256
Iteration 60/1000 | Loss: 0.00003256
Iteration 61/1000 | Loss: 0.00003255
Iteration 62/1000 | Loss: 0.00003255
Iteration 63/1000 | Loss: 0.00003255
Iteration 64/1000 | Loss: 0.00003254
Iteration 65/1000 | Loss: 0.00003254
Iteration 66/1000 | Loss: 0.00003254
Iteration 67/1000 | Loss: 0.00003254
Iteration 68/1000 | Loss: 0.00003254
Iteration 69/1000 | Loss: 0.00003254
Iteration 70/1000 | Loss: 0.00003254
Iteration 71/1000 | Loss: 0.00003254
Iteration 72/1000 | Loss: 0.00003254
Iteration 73/1000 | Loss: 0.00003254
Iteration 74/1000 | Loss: 0.00003254
Iteration 75/1000 | Loss: 0.00003253
Iteration 76/1000 | Loss: 0.00003253
Iteration 77/1000 | Loss: 0.00003253
Iteration 78/1000 | Loss: 0.00003253
Iteration 79/1000 | Loss: 0.00003253
Iteration 80/1000 | Loss: 0.00003253
Iteration 81/1000 | Loss: 0.00003252
Iteration 82/1000 | Loss: 0.00003859
Iteration 83/1000 | Loss: 0.00003252
Iteration 84/1000 | Loss: 0.00003252
Iteration 85/1000 | Loss: 0.00003252
Iteration 86/1000 | Loss: 0.00003252
Iteration 87/1000 | Loss: 0.00003252
Iteration 88/1000 | Loss: 0.00003252
Iteration 89/1000 | Loss: 0.00003252
Iteration 90/1000 | Loss: 0.00003252
Iteration 91/1000 | Loss: 0.00003251
Iteration 92/1000 | Loss: 0.00003250
Iteration 93/1000 | Loss: 0.00003250
Iteration 94/1000 | Loss: 0.00003250
Iteration 95/1000 | Loss: 0.00003250
Iteration 96/1000 | Loss: 0.00003249
Iteration 97/1000 | Loss: 0.00003249
Iteration 98/1000 | Loss: 0.00003248
Iteration 99/1000 | Loss: 0.00005499
Iteration 100/1000 | Loss: 0.00020984
Iteration 101/1000 | Loss: 0.00004394
Iteration 102/1000 | Loss: 0.00007167
Iteration 103/1000 | Loss: 0.00020981
Iteration 104/1000 | Loss: 0.00007867
Iteration 105/1000 | Loss: 0.00005364
Iteration 106/1000 | Loss: 0.00004256
Iteration 107/1000 | Loss: 0.00004124
Iteration 108/1000 | Loss: 0.00003246
Iteration 109/1000 | Loss: 0.00003245
Iteration 110/1000 | Loss: 0.00003245
Iteration 111/1000 | Loss: 0.00003245
Iteration 112/1000 | Loss: 0.00003245
Iteration 113/1000 | Loss: 0.00003244
Iteration 114/1000 | Loss: 0.00003244
Iteration 115/1000 | Loss: 0.00003242
Iteration 116/1000 | Loss: 0.00003242
Iteration 117/1000 | Loss: 0.00003242
Iteration 118/1000 | Loss: 0.00003242
Iteration 119/1000 | Loss: 0.00003242
Iteration 120/1000 | Loss: 0.00003242
Iteration 121/1000 | Loss: 0.00003241
Iteration 122/1000 | Loss: 0.00005145
Iteration 123/1000 | Loss: 0.00003385
Iteration 124/1000 | Loss: 0.00003242
Iteration 125/1000 | Loss: 0.00003241
Iteration 126/1000 | Loss: 0.00003241
Iteration 127/1000 | Loss: 0.00003241
Iteration 128/1000 | Loss: 0.00003241
Iteration 129/1000 | Loss: 0.00003241
Iteration 130/1000 | Loss: 0.00003241
Iteration 131/1000 | Loss: 0.00003241
Iteration 132/1000 | Loss: 0.00003241
Iteration 133/1000 | Loss: 0.00003241
Iteration 134/1000 | Loss: 0.00003240
Iteration 135/1000 | Loss: 0.00003239
Iteration 136/1000 | Loss: 0.00003239
Iteration 137/1000 | Loss: 0.00003679
Iteration 138/1000 | Loss: 0.00005010
Iteration 139/1000 | Loss: 0.00003240
Iteration 140/1000 | Loss: 0.00005929
Iteration 141/1000 | Loss: 0.00003237
Iteration 142/1000 | Loss: 0.00003237
Iteration 143/1000 | Loss: 0.00003237
Iteration 144/1000 | Loss: 0.00003237
Iteration 145/1000 | Loss: 0.00003237
Iteration 146/1000 | Loss: 0.00003236
Iteration 147/1000 | Loss: 0.00003236
Iteration 148/1000 | Loss: 0.00003236
Iteration 149/1000 | Loss: 0.00003236
Iteration 150/1000 | Loss: 0.00003870
Iteration 151/1000 | Loss: 0.00004022
Iteration 152/1000 | Loss: 0.00003235
Iteration 153/1000 | Loss: 0.00003235
Iteration 154/1000 | Loss: 0.00003235
Iteration 155/1000 | Loss: 0.00003235
Iteration 156/1000 | Loss: 0.00003235
Iteration 157/1000 | Loss: 0.00003235
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [3.234718678868376e-05, 3.234718678868376e-05, 3.234718678868376e-05, 3.234718678868376e-05, 3.234718678868376e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.234718678868376e-05

Optimization complete. Final v2v error: 4.247697353363037 mm

Highest mean error: 21.036895751953125 mm for frame 9

Lowest mean error: 3.7910521030426025 mm for frame 130

Saving results

Total time: 120.36560606956482
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00809309
Iteration 2/25 | Loss: 0.00143679
Iteration 3/25 | Loss: 0.00118693
Iteration 4/25 | Loss: 0.00113600
Iteration 5/25 | Loss: 0.00110966
Iteration 6/25 | Loss: 0.00110121
Iteration 7/25 | Loss: 0.00109854
Iteration 8/25 | Loss: 0.00109062
Iteration 9/25 | Loss: 0.00108800
Iteration 10/25 | Loss: 0.00108697
Iteration 11/25 | Loss: 0.00108640
Iteration 12/25 | Loss: 0.00108598
Iteration 13/25 | Loss: 0.00108570
Iteration 14/25 | Loss: 0.00108557
Iteration 15/25 | Loss: 0.00108547
Iteration 16/25 | Loss: 0.00108535
Iteration 17/25 | Loss: 0.00108525
Iteration 18/25 | Loss: 0.00108522
Iteration 19/25 | Loss: 0.00108521
Iteration 20/25 | Loss: 0.00108521
Iteration 21/25 | Loss: 0.00108521
Iteration 22/25 | Loss: 0.00108520
Iteration 23/25 | Loss: 0.00108520
Iteration 24/25 | Loss: 0.00108520
Iteration 25/25 | Loss: 0.00108520

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.81505096
Iteration 2/25 | Loss: 0.00097109
Iteration 3/25 | Loss: 0.00096302
Iteration 4/25 | Loss: 0.00096302
Iteration 5/25 | Loss: 0.00096302
Iteration 6/25 | Loss: 0.00096302
Iteration 7/25 | Loss: 0.00096302
Iteration 8/25 | Loss: 0.00096302
Iteration 9/25 | Loss: 0.00096302
Iteration 10/25 | Loss: 0.00096302
Iteration 11/25 | Loss: 0.00096302
Iteration 12/25 | Loss: 0.00096301
Iteration 13/25 | Loss: 0.00096301
Iteration 14/25 | Loss: 0.00096301
Iteration 15/25 | Loss: 0.00096301
Iteration 16/25 | Loss: 0.00096301
Iteration 17/25 | Loss: 0.00096301
Iteration 18/25 | Loss: 0.00096301
Iteration 19/25 | Loss: 0.00096301
Iteration 20/25 | Loss: 0.00096301
Iteration 21/25 | Loss: 0.00096301
Iteration 22/25 | Loss: 0.00096301
Iteration 23/25 | Loss: 0.00096301
Iteration 24/25 | Loss: 0.00096301
Iteration 25/25 | Loss: 0.00096301

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096301
Iteration 2/1000 | Loss: 0.00007572
Iteration 3/1000 | Loss: 0.00007068
Iteration 4/1000 | Loss: 0.00003181
Iteration 5/1000 | Loss: 0.00002797
Iteration 6/1000 | Loss: 0.00005164
Iteration 7/1000 | Loss: 0.00002481
Iteration 8/1000 | Loss: 0.00002383
Iteration 9/1000 | Loss: 0.00003062
Iteration 10/1000 | Loss: 0.00002463
Iteration 11/1000 | Loss: 0.00002271
Iteration 12/1000 | Loss: 0.00002238
Iteration 13/1000 | Loss: 0.00002215
Iteration 14/1000 | Loss: 0.00002195
Iteration 15/1000 | Loss: 0.00002185
Iteration 16/1000 | Loss: 0.00002184
Iteration 17/1000 | Loss: 0.00002179
Iteration 18/1000 | Loss: 0.00002178
Iteration 19/1000 | Loss: 0.00002177
Iteration 20/1000 | Loss: 0.00005659
Iteration 21/1000 | Loss: 0.00002525
Iteration 22/1000 | Loss: 0.00002248
Iteration 23/1000 | Loss: 0.00002179
Iteration 24/1000 | Loss: 0.00002147
Iteration 25/1000 | Loss: 0.00002146
Iteration 26/1000 | Loss: 0.00002145
Iteration 27/1000 | Loss: 0.00002144
Iteration 28/1000 | Loss: 0.00002143
Iteration 29/1000 | Loss: 0.00002142
Iteration 30/1000 | Loss: 0.00002141
Iteration 31/1000 | Loss: 0.00002141
Iteration 32/1000 | Loss: 0.00002137
Iteration 33/1000 | Loss: 0.00002135
Iteration 34/1000 | Loss: 0.00002134
Iteration 35/1000 | Loss: 0.00002133
Iteration 36/1000 | Loss: 0.00002129
Iteration 37/1000 | Loss: 0.00002129
Iteration 38/1000 | Loss: 0.00002128
Iteration 39/1000 | Loss: 0.00002127
Iteration 40/1000 | Loss: 0.00002126
Iteration 41/1000 | Loss: 0.00002125
Iteration 42/1000 | Loss: 0.00002125
Iteration 43/1000 | Loss: 0.00002124
Iteration 44/1000 | Loss: 0.00002123
Iteration 45/1000 | Loss: 0.00002122
Iteration 46/1000 | Loss: 0.00002122
Iteration 47/1000 | Loss: 0.00002121
Iteration 48/1000 | Loss: 0.00002121
Iteration 49/1000 | Loss: 0.00002120
Iteration 50/1000 | Loss: 0.00002120
Iteration 51/1000 | Loss: 0.00002120
Iteration 52/1000 | Loss: 0.00002119
Iteration 53/1000 | Loss: 0.00002119
Iteration 54/1000 | Loss: 0.00002119
Iteration 55/1000 | Loss: 0.00002119
Iteration 56/1000 | Loss: 0.00002118
Iteration 57/1000 | Loss: 0.00002117
Iteration 58/1000 | Loss: 0.00002114
Iteration 59/1000 | Loss: 0.00002109
Iteration 60/1000 | Loss: 0.00002108
Iteration 61/1000 | Loss: 0.00002107
Iteration 62/1000 | Loss: 0.00002107
Iteration 63/1000 | Loss: 0.00002106
Iteration 64/1000 | Loss: 0.00002106
Iteration 65/1000 | Loss: 0.00002106
Iteration 66/1000 | Loss: 0.00002100
Iteration 67/1000 | Loss: 0.00002100
Iteration 68/1000 | Loss: 0.00002093
Iteration 69/1000 | Loss: 0.00002092
Iteration 70/1000 | Loss: 0.00002090
Iteration 71/1000 | Loss: 0.00002087
Iteration 72/1000 | Loss: 0.00002087
Iteration 73/1000 | Loss: 0.00002087
Iteration 74/1000 | Loss: 0.00002087
Iteration 75/1000 | Loss: 0.00002087
Iteration 76/1000 | Loss: 0.00002087
Iteration 77/1000 | Loss: 0.00002087
Iteration 78/1000 | Loss: 0.00002087
Iteration 79/1000 | Loss: 0.00002087
Iteration 80/1000 | Loss: 0.00002086
Iteration 81/1000 | Loss: 0.00002086
Iteration 82/1000 | Loss: 0.00004906
Iteration 83/1000 | Loss: 0.00002087
Iteration 84/1000 | Loss: 0.00002084
Iteration 85/1000 | Loss: 0.00002083
Iteration 86/1000 | Loss: 0.00002082
Iteration 87/1000 | Loss: 0.00002081
Iteration 88/1000 | Loss: 0.00002080
Iteration 89/1000 | Loss: 0.00002080
Iteration 90/1000 | Loss: 0.00002080
Iteration 91/1000 | Loss: 0.00002080
Iteration 92/1000 | Loss: 0.00002080
Iteration 93/1000 | Loss: 0.00002080
Iteration 94/1000 | Loss: 0.00002080
Iteration 95/1000 | Loss: 0.00002079
Iteration 96/1000 | Loss: 0.00002079
Iteration 97/1000 | Loss: 0.00002079
Iteration 98/1000 | Loss: 0.00002079
Iteration 99/1000 | Loss: 0.00002079
Iteration 100/1000 | Loss: 0.00002079
Iteration 101/1000 | Loss: 0.00002079
Iteration 102/1000 | Loss: 0.00002079
Iteration 103/1000 | Loss: 0.00002079
Iteration 104/1000 | Loss: 0.00002078
Iteration 105/1000 | Loss: 0.00002078
Iteration 106/1000 | Loss: 0.00002078
Iteration 107/1000 | Loss: 0.00002078
Iteration 108/1000 | Loss: 0.00002077
Iteration 109/1000 | Loss: 0.00002077
Iteration 110/1000 | Loss: 0.00002076
Iteration 111/1000 | Loss: 0.00002076
Iteration 112/1000 | Loss: 0.00002075
Iteration 113/1000 | Loss: 0.00002075
Iteration 114/1000 | Loss: 0.00002075
Iteration 115/1000 | Loss: 0.00002075
Iteration 116/1000 | Loss: 0.00002074
Iteration 117/1000 | Loss: 0.00002073
Iteration 118/1000 | Loss: 0.00002073
Iteration 119/1000 | Loss: 0.00002073
Iteration 120/1000 | Loss: 0.00002073
Iteration 121/1000 | Loss: 0.00002072
Iteration 122/1000 | Loss: 0.00002072
Iteration 123/1000 | Loss: 0.00002072
Iteration 124/1000 | Loss: 0.00002072
Iteration 125/1000 | Loss: 0.00002071
Iteration 126/1000 | Loss: 0.00002071
Iteration 127/1000 | Loss: 0.00002071
Iteration 128/1000 | Loss: 0.00002071
Iteration 129/1000 | Loss: 0.00002070
Iteration 130/1000 | Loss: 0.00002070
Iteration 131/1000 | Loss: 0.00002070
Iteration 132/1000 | Loss: 0.00002070
Iteration 133/1000 | Loss: 0.00002069
Iteration 134/1000 | Loss: 0.00002069
Iteration 135/1000 | Loss: 0.00002069
Iteration 136/1000 | Loss: 0.00002069
Iteration 137/1000 | Loss: 0.00002069
Iteration 138/1000 | Loss: 0.00002068
Iteration 139/1000 | Loss: 0.00002068
Iteration 140/1000 | Loss: 0.00002068
Iteration 141/1000 | Loss: 0.00002068
Iteration 142/1000 | Loss: 0.00002068
Iteration 143/1000 | Loss: 0.00002068
Iteration 144/1000 | Loss: 0.00002068
Iteration 145/1000 | Loss: 0.00002068
Iteration 146/1000 | Loss: 0.00002068
Iteration 147/1000 | Loss: 0.00002068
Iteration 148/1000 | Loss: 0.00002068
Iteration 149/1000 | Loss: 0.00002068
Iteration 150/1000 | Loss: 0.00002068
Iteration 151/1000 | Loss: 0.00002067
Iteration 152/1000 | Loss: 0.00002067
Iteration 153/1000 | Loss: 0.00002067
Iteration 154/1000 | Loss: 0.00002067
Iteration 155/1000 | Loss: 0.00002067
Iteration 156/1000 | Loss: 0.00002066
Iteration 157/1000 | Loss: 0.00002066
Iteration 158/1000 | Loss: 0.00002066
Iteration 159/1000 | Loss: 0.00002065
Iteration 160/1000 | Loss: 0.00002065
Iteration 161/1000 | Loss: 0.00002065
Iteration 162/1000 | Loss: 0.00002064
Iteration 163/1000 | Loss: 0.00002064
Iteration 164/1000 | Loss: 0.00002064
Iteration 165/1000 | Loss: 0.00002064
Iteration 166/1000 | Loss: 0.00002064
Iteration 167/1000 | Loss: 0.00002064
Iteration 168/1000 | Loss: 0.00002063
Iteration 169/1000 | Loss: 0.00002063
Iteration 170/1000 | Loss: 0.00002063
Iteration 171/1000 | Loss: 0.00002063
Iteration 172/1000 | Loss: 0.00002063
Iteration 173/1000 | Loss: 0.00002062
Iteration 174/1000 | Loss: 0.00002062
Iteration 175/1000 | Loss: 0.00002062
Iteration 176/1000 | Loss: 0.00002061
Iteration 177/1000 | Loss: 0.00002061
Iteration 178/1000 | Loss: 0.00002060
Iteration 179/1000 | Loss: 0.00002060
Iteration 180/1000 | Loss: 0.00002060
Iteration 181/1000 | Loss: 0.00002060
Iteration 182/1000 | Loss: 0.00002060
Iteration 183/1000 | Loss: 0.00002059
Iteration 184/1000 | Loss: 0.00002059
Iteration 185/1000 | Loss: 0.00002059
Iteration 186/1000 | Loss: 0.00002059
Iteration 187/1000 | Loss: 0.00002058
Iteration 188/1000 | Loss: 0.00002058
Iteration 189/1000 | Loss: 0.00002058
Iteration 190/1000 | Loss: 0.00002058
Iteration 191/1000 | Loss: 0.00002057
Iteration 192/1000 | Loss: 0.00002057
Iteration 193/1000 | Loss: 0.00002057
Iteration 194/1000 | Loss: 0.00002057
Iteration 195/1000 | Loss: 0.00002057
Iteration 196/1000 | Loss: 0.00002057
Iteration 197/1000 | Loss: 0.00002057
Iteration 198/1000 | Loss: 0.00002057
Iteration 199/1000 | Loss: 0.00002057
Iteration 200/1000 | Loss: 0.00002057
Iteration 201/1000 | Loss: 0.00002057
Iteration 202/1000 | Loss: 0.00002057
Iteration 203/1000 | Loss: 0.00002057
Iteration 204/1000 | Loss: 0.00002057
Iteration 205/1000 | Loss: 0.00002057
Iteration 206/1000 | Loss: 0.00002057
Iteration 207/1000 | Loss: 0.00002057
Iteration 208/1000 | Loss: 0.00002057
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [2.0566099919960834e-05, 2.0566099919960834e-05, 2.0566099919960834e-05, 2.0566099919960834e-05, 2.0566099919960834e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0566099919960834e-05

Optimization complete. Final v2v error: 3.344357967376709 mm

Highest mean error: 12.383586883544922 mm for frame 48

Lowest mean error: 2.8069300651550293 mm for frame 130

Saving results

Total time: 78.82874727249146
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01029545
Iteration 2/25 | Loss: 0.00186503
Iteration 3/25 | Loss: 0.00155832
Iteration 4/25 | Loss: 0.00150193
Iteration 5/25 | Loss: 0.00129303
Iteration 6/25 | Loss: 0.00105889
Iteration 7/25 | Loss: 0.00105592
Iteration 8/25 | Loss: 0.00105433
Iteration 9/25 | Loss: 0.00105343
Iteration 10/25 | Loss: 0.00105869
Iteration 11/25 | Loss: 0.00105827
Iteration 12/25 | Loss: 0.00105216
Iteration 13/25 | Loss: 0.00105090
Iteration 14/25 | Loss: 0.00105032
Iteration 15/25 | Loss: 0.00105003
Iteration 16/25 | Loss: 0.00104996
Iteration 17/25 | Loss: 0.00104996
Iteration 18/25 | Loss: 0.00104996
Iteration 19/25 | Loss: 0.00104995
Iteration 20/25 | Loss: 0.00104993
Iteration 21/25 | Loss: 0.00104993
Iteration 22/25 | Loss: 0.00104993
Iteration 23/25 | Loss: 0.00104993
Iteration 24/25 | Loss: 0.00104993
Iteration 25/25 | Loss: 0.00104993

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34636056
Iteration 2/25 | Loss: 0.00063620
Iteration 3/25 | Loss: 0.00063620
Iteration 4/25 | Loss: 0.00063620
Iteration 5/25 | Loss: 0.00063620
Iteration 6/25 | Loss: 0.00063620
Iteration 7/25 | Loss: 0.00063620
Iteration 8/25 | Loss: 0.00063620
Iteration 9/25 | Loss: 0.00063620
Iteration 10/25 | Loss: 0.00063620
Iteration 11/25 | Loss: 0.00063620
Iteration 12/25 | Loss: 0.00063620
Iteration 13/25 | Loss: 0.00063620
Iteration 14/25 | Loss: 0.00063620
Iteration 15/25 | Loss: 0.00063620
Iteration 16/25 | Loss: 0.00063620
Iteration 17/25 | Loss: 0.00063620
Iteration 18/25 | Loss: 0.00063620
Iteration 19/25 | Loss: 0.00063620
Iteration 20/25 | Loss: 0.00063620
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006362008862197399, 0.0006362008862197399, 0.0006362008862197399, 0.0006362008862197399, 0.0006362008862197399]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006362008862197399

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063620
Iteration 2/1000 | Loss: 0.00003849
Iteration 3/1000 | Loss: 0.00012259
Iteration 4/1000 | Loss: 0.00001915
Iteration 5/1000 | Loss: 0.00007914
Iteration 6/1000 | Loss: 0.00010212
Iteration 7/1000 | Loss: 0.00001677
Iteration 8/1000 | Loss: 0.00001612
Iteration 9/1000 | Loss: 0.00001552
Iteration 10/1000 | Loss: 0.00007246
Iteration 11/1000 | Loss: 0.00003633
Iteration 12/1000 | Loss: 0.00003212
Iteration 13/1000 | Loss: 0.00003388
Iteration 14/1000 | Loss: 0.00002560
Iteration 15/1000 | Loss: 0.00047325
Iteration 16/1000 | Loss: 0.00011184
Iteration 17/1000 | Loss: 0.00001482
Iteration 18/1000 | Loss: 0.00003209
Iteration 19/1000 | Loss: 0.00027205
Iteration 20/1000 | Loss: 0.00013317
Iteration 21/1000 | Loss: 0.00012618
Iteration 22/1000 | Loss: 0.00011434
Iteration 23/1000 | Loss: 0.00007425
Iteration 24/1000 | Loss: 0.00002240
Iteration 25/1000 | Loss: 0.00008300
Iteration 26/1000 | Loss: 0.00001387
Iteration 27/1000 | Loss: 0.00001299
Iteration 28/1000 | Loss: 0.00001265
Iteration 29/1000 | Loss: 0.00001237
Iteration 30/1000 | Loss: 0.00001219
Iteration 31/1000 | Loss: 0.00001209
Iteration 32/1000 | Loss: 0.00001199
Iteration 33/1000 | Loss: 0.00001192
Iteration 34/1000 | Loss: 0.00001192
Iteration 35/1000 | Loss: 0.00001192
Iteration 36/1000 | Loss: 0.00001191
Iteration 37/1000 | Loss: 0.00001191
Iteration 38/1000 | Loss: 0.00001190
Iteration 39/1000 | Loss: 0.00001189
Iteration 40/1000 | Loss: 0.00001189
Iteration 41/1000 | Loss: 0.00001189
Iteration 42/1000 | Loss: 0.00001188
Iteration 43/1000 | Loss: 0.00001188
Iteration 44/1000 | Loss: 0.00011765
Iteration 45/1000 | Loss: 0.00001195
Iteration 46/1000 | Loss: 0.00001184
Iteration 47/1000 | Loss: 0.00001184
Iteration 48/1000 | Loss: 0.00001183
Iteration 49/1000 | Loss: 0.00001183
Iteration 50/1000 | Loss: 0.00001183
Iteration 51/1000 | Loss: 0.00001183
Iteration 52/1000 | Loss: 0.00001182
Iteration 53/1000 | Loss: 0.00001182
Iteration 54/1000 | Loss: 0.00001182
Iteration 55/1000 | Loss: 0.00001182
Iteration 56/1000 | Loss: 0.00001182
Iteration 57/1000 | Loss: 0.00001182
Iteration 58/1000 | Loss: 0.00001182
Iteration 59/1000 | Loss: 0.00001182
Iteration 60/1000 | Loss: 0.00001182
Iteration 61/1000 | Loss: 0.00001181
Iteration 62/1000 | Loss: 0.00001181
Iteration 63/1000 | Loss: 0.00001181
Iteration 64/1000 | Loss: 0.00001181
Iteration 65/1000 | Loss: 0.00001181
Iteration 66/1000 | Loss: 0.00001181
Iteration 67/1000 | Loss: 0.00001181
Iteration 68/1000 | Loss: 0.00001181
Iteration 69/1000 | Loss: 0.00001180
Iteration 70/1000 | Loss: 0.00001180
Iteration 71/1000 | Loss: 0.00001180
Iteration 72/1000 | Loss: 0.00001180
Iteration 73/1000 | Loss: 0.00001180
Iteration 74/1000 | Loss: 0.00001180
Iteration 75/1000 | Loss: 0.00001180
Iteration 76/1000 | Loss: 0.00001180
Iteration 77/1000 | Loss: 0.00001180
Iteration 78/1000 | Loss: 0.00001180
Iteration 79/1000 | Loss: 0.00001180
Iteration 80/1000 | Loss: 0.00001180
Iteration 81/1000 | Loss: 0.00001180
Iteration 82/1000 | Loss: 0.00001180
Iteration 83/1000 | Loss: 0.00001180
Iteration 84/1000 | Loss: 0.00001180
Iteration 85/1000 | Loss: 0.00001180
Iteration 86/1000 | Loss: 0.00001180
Iteration 87/1000 | Loss: 0.00001180
Iteration 88/1000 | Loss: 0.00001180
Iteration 89/1000 | Loss: 0.00001180
Iteration 90/1000 | Loss: 0.00001180
Iteration 91/1000 | Loss: 0.00001180
Iteration 92/1000 | Loss: 0.00001180
Iteration 93/1000 | Loss: 0.00001180
Iteration 94/1000 | Loss: 0.00001180
Iteration 95/1000 | Loss: 0.00001180
Iteration 96/1000 | Loss: 0.00001180
Iteration 97/1000 | Loss: 0.00001180
Iteration 98/1000 | Loss: 0.00001180
Iteration 99/1000 | Loss: 0.00001180
Iteration 100/1000 | Loss: 0.00001180
Iteration 101/1000 | Loss: 0.00001180
Iteration 102/1000 | Loss: 0.00001180
Iteration 103/1000 | Loss: 0.00001180
Iteration 104/1000 | Loss: 0.00001180
Iteration 105/1000 | Loss: 0.00001180
Iteration 106/1000 | Loss: 0.00001180
Iteration 107/1000 | Loss: 0.00001180
Iteration 108/1000 | Loss: 0.00001180
Iteration 109/1000 | Loss: 0.00001180
Iteration 110/1000 | Loss: 0.00001180
Iteration 111/1000 | Loss: 0.00001180
Iteration 112/1000 | Loss: 0.00001180
Iteration 113/1000 | Loss: 0.00001180
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [1.1796763828897383e-05, 1.1796763828897383e-05, 1.1796763828897383e-05, 1.1796763828897383e-05, 1.1796763828897383e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1796763828897383e-05

Optimization complete. Final v2v error: 2.8086235523223877 mm

Highest mean error: 11.124773025512695 mm for frame 134

Lowest mean error: 2.4852840900421143 mm for frame 1

Saving results

Total time: 76.60838556289673
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00416050
Iteration 2/25 | Loss: 0.00108012
Iteration 3/25 | Loss: 0.00099960
Iteration 4/25 | Loss: 0.00098330
Iteration 5/25 | Loss: 0.00097966
Iteration 6/25 | Loss: 0.00097877
Iteration 7/25 | Loss: 0.00097831
Iteration 8/25 | Loss: 0.00097831
Iteration 9/25 | Loss: 0.00097831
Iteration 10/25 | Loss: 0.00097831
Iteration 11/25 | Loss: 0.00097831
Iteration 12/25 | Loss: 0.00097831
Iteration 13/25 | Loss: 0.00097831
Iteration 14/25 | Loss: 0.00097831
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.000978314783424139, 0.000978314783424139, 0.000978314783424139, 0.000978314783424139, 0.000978314783424139]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000978314783424139

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55319321
Iteration 2/25 | Loss: 0.00066697
Iteration 3/25 | Loss: 0.00066696
Iteration 4/25 | Loss: 0.00066696
Iteration 5/25 | Loss: 0.00066696
Iteration 6/25 | Loss: 0.00066696
Iteration 7/25 | Loss: 0.00066696
Iteration 8/25 | Loss: 0.00066696
Iteration 9/25 | Loss: 0.00066696
Iteration 10/25 | Loss: 0.00066696
Iteration 11/25 | Loss: 0.00066696
Iteration 12/25 | Loss: 0.00066696
Iteration 13/25 | Loss: 0.00066696
Iteration 14/25 | Loss: 0.00066696
Iteration 15/25 | Loss: 0.00066696
Iteration 16/25 | Loss: 0.00066696
Iteration 17/25 | Loss: 0.00066696
Iteration 18/25 | Loss: 0.00066696
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006669603753834963, 0.0006669603753834963, 0.0006669603753834963, 0.0006669603753834963, 0.0006669603753834963]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006669603753834963

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066696
Iteration 2/1000 | Loss: 0.00001874
Iteration 3/1000 | Loss: 0.00001341
Iteration 4/1000 | Loss: 0.00001257
Iteration 5/1000 | Loss: 0.00001194
Iteration 6/1000 | Loss: 0.00001160
Iteration 7/1000 | Loss: 0.00001129
Iteration 8/1000 | Loss: 0.00001113
Iteration 9/1000 | Loss: 0.00001109
Iteration 10/1000 | Loss: 0.00001101
Iteration 11/1000 | Loss: 0.00001101
Iteration 12/1000 | Loss: 0.00001100
Iteration 13/1000 | Loss: 0.00001100
Iteration 14/1000 | Loss: 0.00001083
Iteration 15/1000 | Loss: 0.00001077
Iteration 16/1000 | Loss: 0.00001074
Iteration 17/1000 | Loss: 0.00001074
Iteration 18/1000 | Loss: 0.00001073
Iteration 19/1000 | Loss: 0.00001073
Iteration 20/1000 | Loss: 0.00001068
Iteration 21/1000 | Loss: 0.00001068
Iteration 22/1000 | Loss: 0.00001066
Iteration 23/1000 | Loss: 0.00001062
Iteration 24/1000 | Loss: 0.00001061
Iteration 25/1000 | Loss: 0.00001059
Iteration 26/1000 | Loss: 0.00001059
Iteration 27/1000 | Loss: 0.00001058
Iteration 28/1000 | Loss: 0.00001057
Iteration 29/1000 | Loss: 0.00001057
Iteration 30/1000 | Loss: 0.00001056
Iteration 31/1000 | Loss: 0.00001056
Iteration 32/1000 | Loss: 0.00001056
Iteration 33/1000 | Loss: 0.00001056
Iteration 34/1000 | Loss: 0.00001055
Iteration 35/1000 | Loss: 0.00001055
Iteration 36/1000 | Loss: 0.00001054
Iteration 37/1000 | Loss: 0.00001054
Iteration 38/1000 | Loss: 0.00001054
Iteration 39/1000 | Loss: 0.00001054
Iteration 40/1000 | Loss: 0.00001054
Iteration 41/1000 | Loss: 0.00001054
Iteration 42/1000 | Loss: 0.00001053
Iteration 43/1000 | Loss: 0.00001053
Iteration 44/1000 | Loss: 0.00001052
Iteration 45/1000 | Loss: 0.00001052
Iteration 46/1000 | Loss: 0.00001052
Iteration 47/1000 | Loss: 0.00001052
Iteration 48/1000 | Loss: 0.00001052
Iteration 49/1000 | Loss: 0.00001051
Iteration 50/1000 | Loss: 0.00001051
Iteration 51/1000 | Loss: 0.00001051
Iteration 52/1000 | Loss: 0.00001050
Iteration 53/1000 | Loss: 0.00001050
Iteration 54/1000 | Loss: 0.00001050
Iteration 55/1000 | Loss: 0.00001049
Iteration 56/1000 | Loss: 0.00001049
Iteration 57/1000 | Loss: 0.00001049
Iteration 58/1000 | Loss: 0.00001049
Iteration 59/1000 | Loss: 0.00001049
Iteration 60/1000 | Loss: 0.00001048
Iteration 61/1000 | Loss: 0.00001048
Iteration 62/1000 | Loss: 0.00001047
Iteration 63/1000 | Loss: 0.00001047
Iteration 64/1000 | Loss: 0.00001047
Iteration 65/1000 | Loss: 0.00001047
Iteration 66/1000 | Loss: 0.00001047
Iteration 67/1000 | Loss: 0.00001047
Iteration 68/1000 | Loss: 0.00001047
Iteration 69/1000 | Loss: 0.00001047
Iteration 70/1000 | Loss: 0.00001047
Iteration 71/1000 | Loss: 0.00001047
Iteration 72/1000 | Loss: 0.00001047
Iteration 73/1000 | Loss: 0.00001047
Iteration 74/1000 | Loss: 0.00001047
Iteration 75/1000 | Loss: 0.00001047
Iteration 76/1000 | Loss: 0.00001047
Iteration 77/1000 | Loss: 0.00001047
Iteration 78/1000 | Loss: 0.00001047
Iteration 79/1000 | Loss: 0.00001047
Iteration 80/1000 | Loss: 0.00001047
Iteration 81/1000 | Loss: 0.00001047
Iteration 82/1000 | Loss: 0.00001047
Iteration 83/1000 | Loss: 0.00001047
Iteration 84/1000 | Loss: 0.00001047
Iteration 85/1000 | Loss: 0.00001047
Iteration 86/1000 | Loss: 0.00001047
Iteration 87/1000 | Loss: 0.00001046
Iteration 88/1000 | Loss: 0.00001046
Iteration 89/1000 | Loss: 0.00001046
Iteration 90/1000 | Loss: 0.00001046
Iteration 91/1000 | Loss: 0.00001046
Iteration 92/1000 | Loss: 0.00001046
Iteration 93/1000 | Loss: 0.00001046
Iteration 94/1000 | Loss: 0.00001046
Iteration 95/1000 | Loss: 0.00001046
Iteration 96/1000 | Loss: 0.00001046
Iteration 97/1000 | Loss: 0.00001046
Iteration 98/1000 | Loss: 0.00001046
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [1.0463701983098872e-05, 1.0463701983098872e-05, 1.0463701983098872e-05, 1.0463701983098872e-05, 1.0463701983098872e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0463701983098872e-05

Optimization complete. Final v2v error: 2.777841329574585 mm

Highest mean error: 3.184210777282715 mm for frame 89

Lowest mean error: 2.636814594268799 mm for frame 124

Saving results

Total time: 32.22267150878906
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00478863
Iteration 2/25 | Loss: 0.00112018
Iteration 3/25 | Loss: 0.00102088
Iteration 4/25 | Loss: 0.00100928
Iteration 5/25 | Loss: 0.00100632
Iteration 6/25 | Loss: 0.00100632
Iteration 7/25 | Loss: 0.00100632
Iteration 8/25 | Loss: 0.00100632
Iteration 9/25 | Loss: 0.00100632
Iteration 10/25 | Loss: 0.00100632
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001006323262117803, 0.001006323262117803, 0.001006323262117803, 0.001006323262117803, 0.001006323262117803]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001006323262117803

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.43963861
Iteration 2/25 | Loss: 0.00061309
Iteration 3/25 | Loss: 0.00061309
Iteration 4/25 | Loss: 0.00061308
Iteration 5/25 | Loss: 0.00061308
Iteration 6/25 | Loss: 0.00061308
Iteration 7/25 | Loss: 0.00061308
Iteration 8/25 | Loss: 0.00061308
Iteration 9/25 | Loss: 0.00061308
Iteration 10/25 | Loss: 0.00061308
Iteration 11/25 | Loss: 0.00061308
Iteration 12/25 | Loss: 0.00061308
Iteration 13/25 | Loss: 0.00061308
Iteration 14/25 | Loss: 0.00061308
Iteration 15/25 | Loss: 0.00061308
Iteration 16/25 | Loss: 0.00061308
Iteration 17/25 | Loss: 0.00061308
Iteration 18/25 | Loss: 0.00061308
Iteration 19/25 | Loss: 0.00061308
Iteration 20/25 | Loss: 0.00061308
Iteration 21/25 | Loss: 0.00061308
Iteration 22/25 | Loss: 0.00061308
Iteration 23/25 | Loss: 0.00061308
Iteration 24/25 | Loss: 0.00061308
Iteration 25/25 | Loss: 0.00061308

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061308
Iteration 2/1000 | Loss: 0.00001858
Iteration 3/1000 | Loss: 0.00001429
Iteration 4/1000 | Loss: 0.00001310
Iteration 5/1000 | Loss: 0.00001257
Iteration 6/1000 | Loss: 0.00001217
Iteration 7/1000 | Loss: 0.00001189
Iteration 8/1000 | Loss: 0.00001184
Iteration 9/1000 | Loss: 0.00001162
Iteration 10/1000 | Loss: 0.00001160
Iteration 11/1000 | Loss: 0.00001159
Iteration 12/1000 | Loss: 0.00001153
Iteration 13/1000 | Loss: 0.00001152
Iteration 14/1000 | Loss: 0.00001151
Iteration 15/1000 | Loss: 0.00001139
Iteration 16/1000 | Loss: 0.00001137
Iteration 17/1000 | Loss: 0.00001136
Iteration 18/1000 | Loss: 0.00001133
Iteration 19/1000 | Loss: 0.00001133
Iteration 20/1000 | Loss: 0.00001132
Iteration 21/1000 | Loss: 0.00001131
Iteration 22/1000 | Loss: 0.00001130
Iteration 23/1000 | Loss: 0.00001130
Iteration 24/1000 | Loss: 0.00001129
Iteration 25/1000 | Loss: 0.00001128
Iteration 26/1000 | Loss: 0.00001127
Iteration 27/1000 | Loss: 0.00001127
Iteration 28/1000 | Loss: 0.00001126
Iteration 29/1000 | Loss: 0.00001126
Iteration 30/1000 | Loss: 0.00001126
Iteration 31/1000 | Loss: 0.00001125
Iteration 32/1000 | Loss: 0.00001124
Iteration 33/1000 | Loss: 0.00001123
Iteration 34/1000 | Loss: 0.00001123
Iteration 35/1000 | Loss: 0.00001122
Iteration 36/1000 | Loss: 0.00001122
Iteration 37/1000 | Loss: 0.00001120
Iteration 38/1000 | Loss: 0.00001120
Iteration 39/1000 | Loss: 0.00001120
Iteration 40/1000 | Loss: 0.00001119
Iteration 41/1000 | Loss: 0.00001118
Iteration 42/1000 | Loss: 0.00001117
Iteration 43/1000 | Loss: 0.00001116
Iteration 44/1000 | Loss: 0.00001116
Iteration 45/1000 | Loss: 0.00001115
Iteration 46/1000 | Loss: 0.00001114
Iteration 47/1000 | Loss: 0.00001114
Iteration 48/1000 | Loss: 0.00001113
Iteration 49/1000 | Loss: 0.00001112
Iteration 50/1000 | Loss: 0.00001112
Iteration 51/1000 | Loss: 0.00001112
Iteration 52/1000 | Loss: 0.00001111
Iteration 53/1000 | Loss: 0.00001111
Iteration 54/1000 | Loss: 0.00001110
Iteration 55/1000 | Loss: 0.00001110
Iteration 56/1000 | Loss: 0.00001109
Iteration 57/1000 | Loss: 0.00001109
Iteration 58/1000 | Loss: 0.00001108
Iteration 59/1000 | Loss: 0.00001108
Iteration 60/1000 | Loss: 0.00001107
Iteration 61/1000 | Loss: 0.00001107
Iteration 62/1000 | Loss: 0.00001106
Iteration 63/1000 | Loss: 0.00001106
Iteration 64/1000 | Loss: 0.00001105
Iteration 65/1000 | Loss: 0.00001104
Iteration 66/1000 | Loss: 0.00001103
Iteration 67/1000 | Loss: 0.00001103
Iteration 68/1000 | Loss: 0.00001102
Iteration 69/1000 | Loss: 0.00001102
Iteration 70/1000 | Loss: 0.00001102
Iteration 71/1000 | Loss: 0.00001101
Iteration 72/1000 | Loss: 0.00001101
Iteration 73/1000 | Loss: 0.00001101
Iteration 74/1000 | Loss: 0.00001101
Iteration 75/1000 | Loss: 0.00001100
Iteration 76/1000 | Loss: 0.00001100
Iteration 77/1000 | Loss: 0.00001100
Iteration 78/1000 | Loss: 0.00001099
Iteration 79/1000 | Loss: 0.00001099
Iteration 80/1000 | Loss: 0.00001099
Iteration 81/1000 | Loss: 0.00001099
Iteration 82/1000 | Loss: 0.00001099
Iteration 83/1000 | Loss: 0.00001099
Iteration 84/1000 | Loss: 0.00001098
Iteration 85/1000 | Loss: 0.00001098
Iteration 86/1000 | Loss: 0.00001098
Iteration 87/1000 | Loss: 0.00001098
Iteration 88/1000 | Loss: 0.00001098
Iteration 89/1000 | Loss: 0.00001097
Iteration 90/1000 | Loss: 0.00001097
Iteration 91/1000 | Loss: 0.00001097
Iteration 92/1000 | Loss: 0.00001096
Iteration 93/1000 | Loss: 0.00001096
Iteration 94/1000 | Loss: 0.00001096
Iteration 95/1000 | Loss: 0.00001096
Iteration 96/1000 | Loss: 0.00001095
Iteration 97/1000 | Loss: 0.00001095
Iteration 98/1000 | Loss: 0.00001095
Iteration 99/1000 | Loss: 0.00001095
Iteration 100/1000 | Loss: 0.00001094
Iteration 101/1000 | Loss: 0.00001094
Iteration 102/1000 | Loss: 0.00001093
Iteration 103/1000 | Loss: 0.00001093
Iteration 104/1000 | Loss: 0.00001093
Iteration 105/1000 | Loss: 0.00001093
Iteration 106/1000 | Loss: 0.00001092
Iteration 107/1000 | Loss: 0.00001092
Iteration 108/1000 | Loss: 0.00001092
Iteration 109/1000 | Loss: 0.00001092
Iteration 110/1000 | Loss: 0.00001091
Iteration 111/1000 | Loss: 0.00001091
Iteration 112/1000 | Loss: 0.00001091
Iteration 113/1000 | Loss: 0.00001090
Iteration 114/1000 | Loss: 0.00001090
Iteration 115/1000 | Loss: 0.00001089
Iteration 116/1000 | Loss: 0.00001089
Iteration 117/1000 | Loss: 0.00001089
Iteration 118/1000 | Loss: 0.00001089
Iteration 119/1000 | Loss: 0.00001089
Iteration 120/1000 | Loss: 0.00001089
Iteration 121/1000 | Loss: 0.00001088
Iteration 122/1000 | Loss: 0.00001088
Iteration 123/1000 | Loss: 0.00001088
Iteration 124/1000 | Loss: 0.00001088
Iteration 125/1000 | Loss: 0.00001088
Iteration 126/1000 | Loss: 0.00001088
Iteration 127/1000 | Loss: 0.00001088
Iteration 128/1000 | Loss: 0.00001088
Iteration 129/1000 | Loss: 0.00001088
Iteration 130/1000 | Loss: 0.00001088
Iteration 131/1000 | Loss: 0.00001087
Iteration 132/1000 | Loss: 0.00001087
Iteration 133/1000 | Loss: 0.00001087
Iteration 134/1000 | Loss: 0.00001086
Iteration 135/1000 | Loss: 0.00001086
Iteration 136/1000 | Loss: 0.00001086
Iteration 137/1000 | Loss: 0.00001086
Iteration 138/1000 | Loss: 0.00001086
Iteration 139/1000 | Loss: 0.00001086
Iteration 140/1000 | Loss: 0.00001085
Iteration 141/1000 | Loss: 0.00001085
Iteration 142/1000 | Loss: 0.00001085
Iteration 143/1000 | Loss: 0.00001084
Iteration 144/1000 | Loss: 0.00001084
Iteration 145/1000 | Loss: 0.00001084
Iteration 146/1000 | Loss: 0.00001084
Iteration 147/1000 | Loss: 0.00001083
Iteration 148/1000 | Loss: 0.00001083
Iteration 149/1000 | Loss: 0.00001083
Iteration 150/1000 | Loss: 0.00001083
Iteration 151/1000 | Loss: 0.00001083
Iteration 152/1000 | Loss: 0.00001083
Iteration 153/1000 | Loss: 0.00001083
Iteration 154/1000 | Loss: 0.00001083
Iteration 155/1000 | Loss: 0.00001083
Iteration 156/1000 | Loss: 0.00001083
Iteration 157/1000 | Loss: 0.00001082
Iteration 158/1000 | Loss: 0.00001082
Iteration 159/1000 | Loss: 0.00001082
Iteration 160/1000 | Loss: 0.00001082
Iteration 161/1000 | Loss: 0.00001081
Iteration 162/1000 | Loss: 0.00001081
Iteration 163/1000 | Loss: 0.00001081
Iteration 164/1000 | Loss: 0.00001081
Iteration 165/1000 | Loss: 0.00001081
Iteration 166/1000 | Loss: 0.00001081
Iteration 167/1000 | Loss: 0.00001081
Iteration 168/1000 | Loss: 0.00001081
Iteration 169/1000 | Loss: 0.00001080
Iteration 170/1000 | Loss: 0.00001080
Iteration 171/1000 | Loss: 0.00001080
Iteration 172/1000 | Loss: 0.00001080
Iteration 173/1000 | Loss: 0.00001079
Iteration 174/1000 | Loss: 0.00001079
Iteration 175/1000 | Loss: 0.00001079
Iteration 176/1000 | Loss: 0.00001079
Iteration 177/1000 | Loss: 0.00001079
Iteration 178/1000 | Loss: 0.00001079
Iteration 179/1000 | Loss: 0.00001079
Iteration 180/1000 | Loss: 0.00001079
Iteration 181/1000 | Loss: 0.00001079
Iteration 182/1000 | Loss: 0.00001079
Iteration 183/1000 | Loss: 0.00001079
Iteration 184/1000 | Loss: 0.00001079
Iteration 185/1000 | Loss: 0.00001079
Iteration 186/1000 | Loss: 0.00001079
Iteration 187/1000 | Loss: 0.00001079
Iteration 188/1000 | Loss: 0.00001079
Iteration 189/1000 | Loss: 0.00001079
Iteration 190/1000 | Loss: 0.00001079
Iteration 191/1000 | Loss: 0.00001079
Iteration 192/1000 | Loss: 0.00001079
Iteration 193/1000 | Loss: 0.00001079
Iteration 194/1000 | Loss: 0.00001079
Iteration 195/1000 | Loss: 0.00001079
Iteration 196/1000 | Loss: 0.00001079
Iteration 197/1000 | Loss: 0.00001079
Iteration 198/1000 | Loss: 0.00001079
Iteration 199/1000 | Loss: 0.00001079
Iteration 200/1000 | Loss: 0.00001079
Iteration 201/1000 | Loss: 0.00001079
Iteration 202/1000 | Loss: 0.00001079
Iteration 203/1000 | Loss: 0.00001079
Iteration 204/1000 | Loss: 0.00001079
Iteration 205/1000 | Loss: 0.00001079
Iteration 206/1000 | Loss: 0.00001079
Iteration 207/1000 | Loss: 0.00001079
Iteration 208/1000 | Loss: 0.00001079
Iteration 209/1000 | Loss: 0.00001079
Iteration 210/1000 | Loss: 0.00001079
Iteration 211/1000 | Loss: 0.00001079
Iteration 212/1000 | Loss: 0.00001079
Iteration 213/1000 | Loss: 0.00001079
Iteration 214/1000 | Loss: 0.00001079
Iteration 215/1000 | Loss: 0.00001079
Iteration 216/1000 | Loss: 0.00001079
Iteration 217/1000 | Loss: 0.00001079
Iteration 218/1000 | Loss: 0.00001079
Iteration 219/1000 | Loss: 0.00001079
Iteration 220/1000 | Loss: 0.00001079
Iteration 221/1000 | Loss: 0.00001079
Iteration 222/1000 | Loss: 0.00001079
Iteration 223/1000 | Loss: 0.00001079
Iteration 224/1000 | Loss: 0.00001079
Iteration 225/1000 | Loss: 0.00001079
Iteration 226/1000 | Loss: 0.00001079
Iteration 227/1000 | Loss: 0.00001079
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [1.0791115528263617e-05, 1.0791115528263617e-05, 1.0791115528263617e-05, 1.0791115528263617e-05, 1.0791115528263617e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0791115528263617e-05

Optimization complete. Final v2v error: 2.8008439540863037 mm

Highest mean error: 3.0738143920898438 mm for frame 196

Lowest mean error: 2.6193044185638428 mm for frame 0

Saving results

Total time: 41.57680368423462
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00809447
Iteration 2/25 | Loss: 0.00106994
Iteration 3/25 | Loss: 0.00096515
Iteration 4/25 | Loss: 0.00095504
Iteration 5/25 | Loss: 0.00095229
Iteration 6/25 | Loss: 0.00095194
Iteration 7/25 | Loss: 0.00095194
Iteration 8/25 | Loss: 0.00095194
Iteration 9/25 | Loss: 0.00095194
Iteration 10/25 | Loss: 0.00095194
Iteration 11/25 | Loss: 0.00095194
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009519359446130693, 0.0009519359446130693, 0.0009519359446130693, 0.0009519359446130693, 0.0009519359446130693]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009519359446130693

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38786983
Iteration 2/25 | Loss: 0.00063835
Iteration 3/25 | Loss: 0.00063835
Iteration 4/25 | Loss: 0.00063835
Iteration 5/25 | Loss: 0.00063835
Iteration 6/25 | Loss: 0.00063835
Iteration 7/25 | Loss: 0.00063835
Iteration 8/25 | Loss: 0.00063835
Iteration 9/25 | Loss: 0.00063834
Iteration 10/25 | Loss: 0.00063834
Iteration 11/25 | Loss: 0.00063834
Iteration 12/25 | Loss: 0.00063834
Iteration 13/25 | Loss: 0.00063834
Iteration 14/25 | Loss: 0.00063834
Iteration 15/25 | Loss: 0.00063834
Iteration 16/25 | Loss: 0.00063834
Iteration 17/25 | Loss: 0.00063834
Iteration 18/25 | Loss: 0.00063834
Iteration 19/25 | Loss: 0.00063834
Iteration 20/25 | Loss: 0.00063834
Iteration 21/25 | Loss: 0.00063834
Iteration 22/25 | Loss: 0.00063834
Iteration 23/25 | Loss: 0.00063834
Iteration 24/25 | Loss: 0.00063834
Iteration 25/25 | Loss: 0.00063834

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063834
Iteration 2/1000 | Loss: 0.00001962
Iteration 3/1000 | Loss: 0.00001185
Iteration 4/1000 | Loss: 0.00001031
Iteration 5/1000 | Loss: 0.00000937
Iteration 6/1000 | Loss: 0.00000885
Iteration 7/1000 | Loss: 0.00000855
Iteration 8/1000 | Loss: 0.00000829
Iteration 9/1000 | Loss: 0.00000815
Iteration 10/1000 | Loss: 0.00000813
Iteration 11/1000 | Loss: 0.00000813
Iteration 12/1000 | Loss: 0.00000813
Iteration 13/1000 | Loss: 0.00000813
Iteration 14/1000 | Loss: 0.00000813
Iteration 15/1000 | Loss: 0.00000813
Iteration 16/1000 | Loss: 0.00000805
Iteration 17/1000 | Loss: 0.00000799
Iteration 18/1000 | Loss: 0.00000798
Iteration 19/1000 | Loss: 0.00000794
Iteration 20/1000 | Loss: 0.00000793
Iteration 21/1000 | Loss: 0.00000793
Iteration 22/1000 | Loss: 0.00000791
Iteration 23/1000 | Loss: 0.00000791
Iteration 24/1000 | Loss: 0.00000791
Iteration 25/1000 | Loss: 0.00000790
Iteration 26/1000 | Loss: 0.00000789
Iteration 27/1000 | Loss: 0.00000789
Iteration 28/1000 | Loss: 0.00000789
Iteration 29/1000 | Loss: 0.00000788
Iteration 30/1000 | Loss: 0.00000788
Iteration 31/1000 | Loss: 0.00000787
Iteration 32/1000 | Loss: 0.00000787
Iteration 33/1000 | Loss: 0.00000787
Iteration 34/1000 | Loss: 0.00000786
Iteration 35/1000 | Loss: 0.00000785
Iteration 36/1000 | Loss: 0.00000784
Iteration 37/1000 | Loss: 0.00000784
Iteration 38/1000 | Loss: 0.00000784
Iteration 39/1000 | Loss: 0.00000783
Iteration 40/1000 | Loss: 0.00000782
Iteration 41/1000 | Loss: 0.00000781
Iteration 42/1000 | Loss: 0.00000781
Iteration 43/1000 | Loss: 0.00000781
Iteration 44/1000 | Loss: 0.00000780
Iteration 45/1000 | Loss: 0.00000780
Iteration 46/1000 | Loss: 0.00000780
Iteration 47/1000 | Loss: 0.00000780
Iteration 48/1000 | Loss: 0.00000780
Iteration 49/1000 | Loss: 0.00000779
Iteration 50/1000 | Loss: 0.00000779
Iteration 51/1000 | Loss: 0.00000779
Iteration 52/1000 | Loss: 0.00000779
Iteration 53/1000 | Loss: 0.00000777
Iteration 54/1000 | Loss: 0.00000776
Iteration 55/1000 | Loss: 0.00000776
Iteration 56/1000 | Loss: 0.00000775
Iteration 57/1000 | Loss: 0.00000774
Iteration 58/1000 | Loss: 0.00000774
Iteration 59/1000 | Loss: 0.00000774
Iteration 60/1000 | Loss: 0.00000773
Iteration 61/1000 | Loss: 0.00000773
Iteration 62/1000 | Loss: 0.00000772
Iteration 63/1000 | Loss: 0.00000772
Iteration 64/1000 | Loss: 0.00000771
Iteration 65/1000 | Loss: 0.00000771
Iteration 66/1000 | Loss: 0.00000771
Iteration 67/1000 | Loss: 0.00000771
Iteration 68/1000 | Loss: 0.00000771
Iteration 69/1000 | Loss: 0.00000771
Iteration 70/1000 | Loss: 0.00000771
Iteration 71/1000 | Loss: 0.00000771
Iteration 72/1000 | Loss: 0.00000771
Iteration 73/1000 | Loss: 0.00000770
Iteration 74/1000 | Loss: 0.00000770
Iteration 75/1000 | Loss: 0.00000770
Iteration 76/1000 | Loss: 0.00000770
Iteration 77/1000 | Loss: 0.00000770
Iteration 78/1000 | Loss: 0.00000770
Iteration 79/1000 | Loss: 0.00000770
Iteration 80/1000 | Loss: 0.00000770
Iteration 81/1000 | Loss: 0.00000769
Iteration 82/1000 | Loss: 0.00000769
Iteration 83/1000 | Loss: 0.00000769
Iteration 84/1000 | Loss: 0.00000769
Iteration 85/1000 | Loss: 0.00000769
Iteration 86/1000 | Loss: 0.00000769
Iteration 87/1000 | Loss: 0.00000769
Iteration 88/1000 | Loss: 0.00000768
Iteration 89/1000 | Loss: 0.00000768
Iteration 90/1000 | Loss: 0.00000768
Iteration 91/1000 | Loss: 0.00000768
Iteration 92/1000 | Loss: 0.00000768
Iteration 93/1000 | Loss: 0.00000768
Iteration 94/1000 | Loss: 0.00000767
Iteration 95/1000 | Loss: 0.00000767
Iteration 96/1000 | Loss: 0.00000767
Iteration 97/1000 | Loss: 0.00000767
Iteration 98/1000 | Loss: 0.00000766
Iteration 99/1000 | Loss: 0.00000766
Iteration 100/1000 | Loss: 0.00000766
Iteration 101/1000 | Loss: 0.00000765
Iteration 102/1000 | Loss: 0.00000765
Iteration 103/1000 | Loss: 0.00000765
Iteration 104/1000 | Loss: 0.00000765
Iteration 105/1000 | Loss: 0.00000765
Iteration 106/1000 | Loss: 0.00000764
Iteration 107/1000 | Loss: 0.00000764
Iteration 108/1000 | Loss: 0.00000764
Iteration 109/1000 | Loss: 0.00000764
Iteration 110/1000 | Loss: 0.00000764
Iteration 111/1000 | Loss: 0.00000764
Iteration 112/1000 | Loss: 0.00000764
Iteration 113/1000 | Loss: 0.00000764
Iteration 114/1000 | Loss: 0.00000764
Iteration 115/1000 | Loss: 0.00000764
Iteration 116/1000 | Loss: 0.00000764
Iteration 117/1000 | Loss: 0.00000764
Iteration 118/1000 | Loss: 0.00000764
Iteration 119/1000 | Loss: 0.00000763
Iteration 120/1000 | Loss: 0.00000763
Iteration 121/1000 | Loss: 0.00000763
Iteration 122/1000 | Loss: 0.00000763
Iteration 123/1000 | Loss: 0.00000762
Iteration 124/1000 | Loss: 0.00000762
Iteration 125/1000 | Loss: 0.00000762
Iteration 126/1000 | Loss: 0.00000761
Iteration 127/1000 | Loss: 0.00000761
Iteration 128/1000 | Loss: 0.00000761
Iteration 129/1000 | Loss: 0.00000761
Iteration 130/1000 | Loss: 0.00000761
Iteration 131/1000 | Loss: 0.00000761
Iteration 132/1000 | Loss: 0.00000761
Iteration 133/1000 | Loss: 0.00000760
Iteration 134/1000 | Loss: 0.00000760
Iteration 135/1000 | Loss: 0.00000760
Iteration 136/1000 | Loss: 0.00000760
Iteration 137/1000 | Loss: 0.00000760
Iteration 138/1000 | Loss: 0.00000760
Iteration 139/1000 | Loss: 0.00000760
Iteration 140/1000 | Loss: 0.00000760
Iteration 141/1000 | Loss: 0.00000760
Iteration 142/1000 | Loss: 0.00000759
Iteration 143/1000 | Loss: 0.00000759
Iteration 144/1000 | Loss: 0.00000759
Iteration 145/1000 | Loss: 0.00000759
Iteration 146/1000 | Loss: 0.00000759
Iteration 147/1000 | Loss: 0.00000759
Iteration 148/1000 | Loss: 0.00000759
Iteration 149/1000 | Loss: 0.00000759
Iteration 150/1000 | Loss: 0.00000759
Iteration 151/1000 | Loss: 0.00000759
Iteration 152/1000 | Loss: 0.00000759
Iteration 153/1000 | Loss: 0.00000759
Iteration 154/1000 | Loss: 0.00000759
Iteration 155/1000 | Loss: 0.00000758
Iteration 156/1000 | Loss: 0.00000758
Iteration 157/1000 | Loss: 0.00000758
Iteration 158/1000 | Loss: 0.00000758
Iteration 159/1000 | Loss: 0.00000757
Iteration 160/1000 | Loss: 0.00000757
Iteration 161/1000 | Loss: 0.00000757
Iteration 162/1000 | Loss: 0.00000757
Iteration 163/1000 | Loss: 0.00000757
Iteration 164/1000 | Loss: 0.00000757
Iteration 165/1000 | Loss: 0.00000757
Iteration 166/1000 | Loss: 0.00000757
Iteration 167/1000 | Loss: 0.00000757
Iteration 168/1000 | Loss: 0.00000757
Iteration 169/1000 | Loss: 0.00000757
Iteration 170/1000 | Loss: 0.00000756
Iteration 171/1000 | Loss: 0.00000756
Iteration 172/1000 | Loss: 0.00000756
Iteration 173/1000 | Loss: 0.00000756
Iteration 174/1000 | Loss: 0.00000756
Iteration 175/1000 | Loss: 0.00000755
Iteration 176/1000 | Loss: 0.00000755
Iteration 177/1000 | Loss: 0.00000755
Iteration 178/1000 | Loss: 0.00000755
Iteration 179/1000 | Loss: 0.00000755
Iteration 180/1000 | Loss: 0.00000755
Iteration 181/1000 | Loss: 0.00000755
Iteration 182/1000 | Loss: 0.00000755
Iteration 183/1000 | Loss: 0.00000755
Iteration 184/1000 | Loss: 0.00000755
Iteration 185/1000 | Loss: 0.00000755
Iteration 186/1000 | Loss: 0.00000754
Iteration 187/1000 | Loss: 0.00000754
Iteration 188/1000 | Loss: 0.00000754
Iteration 189/1000 | Loss: 0.00000753
Iteration 190/1000 | Loss: 0.00000753
Iteration 191/1000 | Loss: 0.00000753
Iteration 192/1000 | Loss: 0.00000753
Iteration 193/1000 | Loss: 0.00000753
Iteration 194/1000 | Loss: 0.00000753
Iteration 195/1000 | Loss: 0.00000753
Iteration 196/1000 | Loss: 0.00000753
Iteration 197/1000 | Loss: 0.00000753
Iteration 198/1000 | Loss: 0.00000753
Iteration 199/1000 | Loss: 0.00000753
Iteration 200/1000 | Loss: 0.00000752
Iteration 201/1000 | Loss: 0.00000752
Iteration 202/1000 | Loss: 0.00000752
Iteration 203/1000 | Loss: 0.00000752
Iteration 204/1000 | Loss: 0.00000751
Iteration 205/1000 | Loss: 0.00000751
Iteration 206/1000 | Loss: 0.00000751
Iteration 207/1000 | Loss: 0.00000751
Iteration 208/1000 | Loss: 0.00000751
Iteration 209/1000 | Loss: 0.00000751
Iteration 210/1000 | Loss: 0.00000751
Iteration 211/1000 | Loss: 0.00000751
Iteration 212/1000 | Loss: 0.00000751
Iteration 213/1000 | Loss: 0.00000751
Iteration 214/1000 | Loss: 0.00000751
Iteration 215/1000 | Loss: 0.00000751
Iteration 216/1000 | Loss: 0.00000751
Iteration 217/1000 | Loss: 0.00000751
Iteration 218/1000 | Loss: 0.00000750
Iteration 219/1000 | Loss: 0.00000750
Iteration 220/1000 | Loss: 0.00000750
Iteration 221/1000 | Loss: 0.00000750
Iteration 222/1000 | Loss: 0.00000750
Iteration 223/1000 | Loss: 0.00000750
Iteration 224/1000 | Loss: 0.00000750
Iteration 225/1000 | Loss: 0.00000750
Iteration 226/1000 | Loss: 0.00000750
Iteration 227/1000 | Loss: 0.00000750
Iteration 228/1000 | Loss: 0.00000750
Iteration 229/1000 | Loss: 0.00000750
Iteration 230/1000 | Loss: 0.00000750
Iteration 231/1000 | Loss: 0.00000750
Iteration 232/1000 | Loss: 0.00000750
Iteration 233/1000 | Loss: 0.00000750
Iteration 234/1000 | Loss: 0.00000750
Iteration 235/1000 | Loss: 0.00000750
Iteration 236/1000 | Loss: 0.00000750
Iteration 237/1000 | Loss: 0.00000750
Iteration 238/1000 | Loss: 0.00000750
Iteration 239/1000 | Loss: 0.00000750
Iteration 240/1000 | Loss: 0.00000750
Iteration 241/1000 | Loss: 0.00000750
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 241. Stopping optimization.
Last 5 losses: [7.504896075261058e-06, 7.504896075261058e-06, 7.504896075261058e-06, 7.504896075261058e-06, 7.504896075261058e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.504896075261058e-06

Optimization complete. Final v2v error: 2.3324127197265625 mm

Highest mean error: 2.506553888320923 mm for frame 59

Lowest mean error: 2.182539224624634 mm for frame 6

Saving results

Total time: 37.916346073150635
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01062048
Iteration 2/25 | Loss: 0.00156346
Iteration 3/25 | Loss: 0.00148525
Iteration 4/25 | Loss: 0.00105773
Iteration 5/25 | Loss: 0.00103069
Iteration 6/25 | Loss: 0.00101942
Iteration 7/25 | Loss: 0.00100727
Iteration 8/25 | Loss: 0.00099648
Iteration 9/25 | Loss: 0.00099598
Iteration 10/25 | Loss: 0.00099685
Iteration 11/25 | Loss: 0.00099400
Iteration 12/25 | Loss: 0.00099400
Iteration 13/25 | Loss: 0.00099653
Iteration 14/25 | Loss: 0.00099282
Iteration 15/25 | Loss: 0.00099273
Iteration 16/25 | Loss: 0.00099273
Iteration 17/25 | Loss: 0.00099272
Iteration 18/25 | Loss: 0.00099272
Iteration 19/25 | Loss: 0.00099272
Iteration 20/25 | Loss: 0.00099272
Iteration 21/25 | Loss: 0.00099272
Iteration 22/25 | Loss: 0.00099272
Iteration 23/25 | Loss: 0.00099272
Iteration 24/25 | Loss: 0.00099272
Iteration 25/25 | Loss: 0.00099272

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.93998814
Iteration 2/25 | Loss: 0.00101444
Iteration 3/25 | Loss: 0.00068312
Iteration 4/25 | Loss: 0.00068311
Iteration 5/25 | Loss: 0.00068311
Iteration 6/25 | Loss: 0.00068311
Iteration 7/25 | Loss: 0.00068310
Iteration 8/25 | Loss: 0.00068310
Iteration 9/25 | Loss: 0.00068310
Iteration 10/25 | Loss: 0.00068310
Iteration 11/25 | Loss: 0.00068310
Iteration 12/25 | Loss: 0.00068310
Iteration 13/25 | Loss: 0.00068310
Iteration 14/25 | Loss: 0.00068310
Iteration 15/25 | Loss: 0.00068310
Iteration 16/25 | Loss: 0.00068310
Iteration 17/25 | Loss: 0.00068310
Iteration 18/25 | Loss: 0.00068310
Iteration 19/25 | Loss: 0.00068310
Iteration 20/25 | Loss: 0.00068310
Iteration 21/25 | Loss: 0.00068310
Iteration 22/25 | Loss: 0.00068310
Iteration 23/25 | Loss: 0.00068310
Iteration 24/25 | Loss: 0.00068310
Iteration 25/25 | Loss: 0.00068310

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068310
Iteration 2/1000 | Loss: 0.00025102
Iteration 3/1000 | Loss: 0.00008822
Iteration 4/1000 | Loss: 0.00011576
Iteration 5/1000 | Loss: 0.00004326
Iteration 6/1000 | Loss: 0.00006377
Iteration 7/1000 | Loss: 0.00003509
Iteration 8/1000 | Loss: 0.00012316
Iteration 9/1000 | Loss: 0.00002158
Iteration 10/1000 | Loss: 0.00001427
Iteration 11/1000 | Loss: 0.00002732
Iteration 12/1000 | Loss: 0.00001382
Iteration 13/1000 | Loss: 0.00001352
Iteration 14/1000 | Loss: 0.00004857
Iteration 15/1000 | Loss: 0.00001366
Iteration 16/1000 | Loss: 0.00001326
Iteration 17/1000 | Loss: 0.00088157
Iteration 18/1000 | Loss: 0.00007743
Iteration 19/1000 | Loss: 0.00003566
Iteration 20/1000 | Loss: 0.00001295
Iteration 21/1000 | Loss: 0.00004048
Iteration 22/1000 | Loss: 0.00001162
Iteration 23/1000 | Loss: 0.00001104
Iteration 24/1000 | Loss: 0.00001069
Iteration 25/1000 | Loss: 0.00006490
Iteration 26/1000 | Loss: 0.00007939
Iteration 27/1000 | Loss: 0.00003053
Iteration 28/1000 | Loss: 0.00001042
Iteration 29/1000 | Loss: 0.00001038
Iteration 30/1000 | Loss: 0.00001038
Iteration 31/1000 | Loss: 0.00001037
Iteration 32/1000 | Loss: 0.00001036
Iteration 33/1000 | Loss: 0.00001036
Iteration 34/1000 | Loss: 0.00001035
Iteration 35/1000 | Loss: 0.00001035
Iteration 36/1000 | Loss: 0.00001035
Iteration 37/1000 | Loss: 0.00001035
Iteration 38/1000 | Loss: 0.00001034
Iteration 39/1000 | Loss: 0.00001027
Iteration 40/1000 | Loss: 0.00001023
Iteration 41/1000 | Loss: 0.00001022
Iteration 42/1000 | Loss: 0.00001017
Iteration 43/1000 | Loss: 0.00001015
Iteration 44/1000 | Loss: 0.00001015
Iteration 45/1000 | Loss: 0.00001015
Iteration 46/1000 | Loss: 0.00001015
Iteration 47/1000 | Loss: 0.00001015
Iteration 48/1000 | Loss: 0.00001014
Iteration 49/1000 | Loss: 0.00001014
Iteration 50/1000 | Loss: 0.00001014
Iteration 51/1000 | Loss: 0.00001014
Iteration 52/1000 | Loss: 0.00001014
Iteration 53/1000 | Loss: 0.00001014
Iteration 54/1000 | Loss: 0.00001014
Iteration 55/1000 | Loss: 0.00001014
Iteration 56/1000 | Loss: 0.00001014
Iteration 57/1000 | Loss: 0.00001011
Iteration 58/1000 | Loss: 0.00001010
Iteration 59/1000 | Loss: 0.00001010
Iteration 60/1000 | Loss: 0.00001010
Iteration 61/1000 | Loss: 0.00001010
Iteration 62/1000 | Loss: 0.00001010
Iteration 63/1000 | Loss: 0.00001010
Iteration 64/1000 | Loss: 0.00001009
Iteration 65/1000 | Loss: 0.00001009
Iteration 66/1000 | Loss: 0.00001009
Iteration 67/1000 | Loss: 0.00004934
Iteration 68/1000 | Loss: 0.00001010
Iteration 69/1000 | Loss: 0.00001009
Iteration 70/1000 | Loss: 0.00001005
Iteration 71/1000 | Loss: 0.00001005
Iteration 72/1000 | Loss: 0.00001004
Iteration 73/1000 | Loss: 0.00001004
Iteration 74/1000 | Loss: 0.00001001
Iteration 75/1000 | Loss: 0.00001001
Iteration 76/1000 | Loss: 0.00001001
Iteration 77/1000 | Loss: 0.00001000
Iteration 78/1000 | Loss: 0.00001000
Iteration 79/1000 | Loss: 0.00001000
Iteration 80/1000 | Loss: 0.00001000
Iteration 81/1000 | Loss: 0.00001000
Iteration 82/1000 | Loss: 0.00001000
Iteration 83/1000 | Loss: 0.00001000
Iteration 84/1000 | Loss: 0.00001000
Iteration 85/1000 | Loss: 0.00001000
Iteration 86/1000 | Loss: 0.00001000
Iteration 87/1000 | Loss: 0.00001000
Iteration 88/1000 | Loss: 0.00001000
Iteration 89/1000 | Loss: 0.00000999
Iteration 90/1000 | Loss: 0.00000999
Iteration 91/1000 | Loss: 0.00000999
Iteration 92/1000 | Loss: 0.00000999
Iteration 93/1000 | Loss: 0.00000999
Iteration 94/1000 | Loss: 0.00000999
Iteration 95/1000 | Loss: 0.00000998
Iteration 96/1000 | Loss: 0.00000998
Iteration 97/1000 | Loss: 0.00000998
Iteration 98/1000 | Loss: 0.00000998
Iteration 99/1000 | Loss: 0.00000997
Iteration 100/1000 | Loss: 0.00000997
Iteration 101/1000 | Loss: 0.00000997
Iteration 102/1000 | Loss: 0.00000997
Iteration 103/1000 | Loss: 0.00000996
Iteration 104/1000 | Loss: 0.00000996
Iteration 105/1000 | Loss: 0.00000996
Iteration 106/1000 | Loss: 0.00000996
Iteration 107/1000 | Loss: 0.00000996
Iteration 108/1000 | Loss: 0.00000996
Iteration 109/1000 | Loss: 0.00000996
Iteration 110/1000 | Loss: 0.00000996
Iteration 111/1000 | Loss: 0.00000996
Iteration 112/1000 | Loss: 0.00000996
Iteration 113/1000 | Loss: 0.00000996
Iteration 114/1000 | Loss: 0.00000996
Iteration 115/1000 | Loss: 0.00000996
Iteration 116/1000 | Loss: 0.00000996
Iteration 117/1000 | Loss: 0.00000996
Iteration 118/1000 | Loss: 0.00000996
Iteration 119/1000 | Loss: 0.00000996
Iteration 120/1000 | Loss: 0.00000996
Iteration 121/1000 | Loss: 0.00000996
Iteration 122/1000 | Loss: 0.00000996
Iteration 123/1000 | Loss: 0.00000996
Iteration 124/1000 | Loss: 0.00000996
Iteration 125/1000 | Loss: 0.00000996
Iteration 126/1000 | Loss: 0.00000996
Iteration 127/1000 | Loss: 0.00000996
Iteration 128/1000 | Loss: 0.00000996
Iteration 129/1000 | Loss: 0.00000996
Iteration 130/1000 | Loss: 0.00000996
Iteration 131/1000 | Loss: 0.00000996
Iteration 132/1000 | Loss: 0.00000996
Iteration 133/1000 | Loss: 0.00000996
Iteration 134/1000 | Loss: 0.00000996
Iteration 135/1000 | Loss: 0.00000996
Iteration 136/1000 | Loss: 0.00000996
Iteration 137/1000 | Loss: 0.00000996
Iteration 138/1000 | Loss: 0.00000996
Iteration 139/1000 | Loss: 0.00000996
Iteration 140/1000 | Loss: 0.00000996
Iteration 141/1000 | Loss: 0.00000996
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [9.955202585842926e-06, 9.955202585842926e-06, 9.955202585842926e-06, 9.955202585842926e-06, 9.955202585842926e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.955202585842926e-06

Optimization complete. Final v2v error: 2.6844165325164795 mm

Highest mean error: 3.4798691272735596 mm for frame 85

Lowest mean error: 2.4488866329193115 mm for frame 27

Saving results

Total time: 74.38278126716614
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01082376
Iteration 2/25 | Loss: 0.01082375
Iteration 3/25 | Loss: 0.00286826
Iteration 4/25 | Loss: 0.00215336
Iteration 5/25 | Loss: 0.00178467
Iteration 6/25 | Loss: 0.00178234
Iteration 7/25 | Loss: 0.00163301
Iteration 8/25 | Loss: 0.00153742
Iteration 9/25 | Loss: 0.00148272
Iteration 10/25 | Loss: 0.00141345
Iteration 11/25 | Loss: 0.00138293
Iteration 12/25 | Loss: 0.00135864
Iteration 13/25 | Loss: 0.00133959
Iteration 14/25 | Loss: 0.00134425
Iteration 15/25 | Loss: 0.00134773
Iteration 16/25 | Loss: 0.00135718
Iteration 17/25 | Loss: 0.00135282
Iteration 18/25 | Loss: 0.00133512
Iteration 19/25 | Loss: 0.00132217
Iteration 20/25 | Loss: 0.00131911
Iteration 21/25 | Loss: 0.00131551
Iteration 22/25 | Loss: 0.00131352
Iteration 23/25 | Loss: 0.00131270
Iteration 24/25 | Loss: 0.00131231
Iteration 25/25 | Loss: 0.00131222

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.08749628
Iteration 2/25 | Loss: 0.00101258
Iteration 3/25 | Loss: 0.00080617
Iteration 4/25 | Loss: 0.00080617
Iteration 5/25 | Loss: 0.00080617
Iteration 6/25 | Loss: 0.00080617
Iteration 7/25 | Loss: 0.00080617
Iteration 8/25 | Loss: 0.00080616
Iteration 9/25 | Loss: 0.00080616
Iteration 10/25 | Loss: 0.00080616
Iteration 11/25 | Loss: 0.00080616
Iteration 12/25 | Loss: 0.00080616
Iteration 13/25 | Loss: 0.00080616
Iteration 14/25 | Loss: 0.00080616
Iteration 15/25 | Loss: 0.00080616
Iteration 16/25 | Loss: 0.00080616
Iteration 17/25 | Loss: 0.00080616
Iteration 18/25 | Loss: 0.00080616
Iteration 19/25 | Loss: 0.00080616
Iteration 20/25 | Loss: 0.00080616
Iteration 21/25 | Loss: 0.00080616
Iteration 22/25 | Loss: 0.00080616
Iteration 23/25 | Loss: 0.00080616
Iteration 24/25 | Loss: 0.00080616
Iteration 25/25 | Loss: 0.00080616

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080616
Iteration 2/1000 | Loss: 0.00067964
Iteration 3/1000 | Loss: 0.00027013
Iteration 4/1000 | Loss: 0.00079139
Iteration 5/1000 | Loss: 0.00007883
Iteration 6/1000 | Loss: 0.00009585
Iteration 7/1000 | Loss: 0.00006119
Iteration 8/1000 | Loss: 0.00015855
Iteration 9/1000 | Loss: 0.00068938
Iteration 10/1000 | Loss: 0.00016682
Iteration 11/1000 | Loss: 0.00046882
Iteration 12/1000 | Loss: 0.00006480
Iteration 13/1000 | Loss: 0.00006186
Iteration 14/1000 | Loss: 0.00005999
Iteration 15/1000 | Loss: 0.00011620
Iteration 16/1000 | Loss: 0.00091918
Iteration 17/1000 | Loss: 0.00066114
Iteration 18/1000 | Loss: 0.00041210
Iteration 19/1000 | Loss: 0.00051690
Iteration 20/1000 | Loss: 0.00127314
Iteration 21/1000 | Loss: 0.00124501
Iteration 22/1000 | Loss: 0.00108346
Iteration 23/1000 | Loss: 0.00060074
Iteration 24/1000 | Loss: 0.00120971
Iteration 25/1000 | Loss: 0.00094061
Iteration 26/1000 | Loss: 0.00056051
Iteration 27/1000 | Loss: 0.00145749
Iteration 28/1000 | Loss: 0.00084000
Iteration 29/1000 | Loss: 0.00108830
Iteration 30/1000 | Loss: 0.00075298
Iteration 31/1000 | Loss: 0.00059883
Iteration 32/1000 | Loss: 0.00013310
Iteration 33/1000 | Loss: 0.00047569
Iteration 34/1000 | Loss: 0.00047395
Iteration 35/1000 | Loss: 0.00098016
Iteration 36/1000 | Loss: 0.00042678
Iteration 37/1000 | Loss: 0.00014621
Iteration 38/1000 | Loss: 0.00034591
Iteration 39/1000 | Loss: 0.00042426
Iteration 40/1000 | Loss: 0.00067400
Iteration 41/1000 | Loss: 0.00069579
Iteration 42/1000 | Loss: 0.00027146
Iteration 43/1000 | Loss: 0.00079331
Iteration 44/1000 | Loss: 0.00036619
Iteration 45/1000 | Loss: 0.00071980
Iteration 46/1000 | Loss: 0.00069338
Iteration 47/1000 | Loss: 0.00096063
Iteration 48/1000 | Loss: 0.00063864
Iteration 49/1000 | Loss: 0.00067835
Iteration 50/1000 | Loss: 0.00020824
Iteration 51/1000 | Loss: 0.00036059
Iteration 52/1000 | Loss: 0.00026507
Iteration 53/1000 | Loss: 0.00020473
Iteration 54/1000 | Loss: 0.00008810
Iteration 55/1000 | Loss: 0.00083664
Iteration 56/1000 | Loss: 0.00188196
Iteration 57/1000 | Loss: 0.00158615
Iteration 58/1000 | Loss: 0.00036730
Iteration 59/1000 | Loss: 0.00019677
Iteration 60/1000 | Loss: 0.00030996
Iteration 61/1000 | Loss: 0.00059839
Iteration 62/1000 | Loss: 0.00088162
Iteration 63/1000 | Loss: 0.00024851
Iteration 64/1000 | Loss: 0.00014374
Iteration 65/1000 | Loss: 0.00012238
Iteration 66/1000 | Loss: 0.00031142
Iteration 67/1000 | Loss: 0.00010965
Iteration 68/1000 | Loss: 0.00020040
Iteration 69/1000 | Loss: 0.00009826
Iteration 70/1000 | Loss: 0.00073136
Iteration 71/1000 | Loss: 0.00023822
Iteration 72/1000 | Loss: 0.00016661
Iteration 73/1000 | Loss: 0.00055383
Iteration 74/1000 | Loss: 0.00025328
Iteration 75/1000 | Loss: 0.00030250
Iteration 76/1000 | Loss: 0.00076969
Iteration 77/1000 | Loss: 0.00039277
Iteration 78/1000 | Loss: 0.00056424
Iteration 79/1000 | Loss: 0.00029337
Iteration 80/1000 | Loss: 0.00022953
Iteration 81/1000 | Loss: 0.00019528
Iteration 82/1000 | Loss: 0.00018227
Iteration 83/1000 | Loss: 0.00017282
Iteration 84/1000 | Loss: 0.00035050
Iteration 85/1000 | Loss: 0.00016794
Iteration 86/1000 | Loss: 0.00006587
Iteration 87/1000 | Loss: 0.00023035
Iteration 88/1000 | Loss: 0.00012185
Iteration 89/1000 | Loss: 0.00009396
Iteration 90/1000 | Loss: 0.00056096
Iteration 91/1000 | Loss: 0.00024932
Iteration 92/1000 | Loss: 0.00007966
Iteration 93/1000 | Loss: 0.00007498
Iteration 94/1000 | Loss: 0.00006041
Iteration 95/1000 | Loss: 0.00013703
Iteration 96/1000 | Loss: 0.00008591
Iteration 97/1000 | Loss: 0.00005596
Iteration 98/1000 | Loss: 0.00005298
Iteration 99/1000 | Loss: 0.00015919
Iteration 100/1000 | Loss: 0.00019689
Iteration 101/1000 | Loss: 0.00005185
Iteration 102/1000 | Loss: 0.00028805
Iteration 103/1000 | Loss: 0.00007273
Iteration 104/1000 | Loss: 0.00005534
Iteration 105/1000 | Loss: 0.00021817
Iteration 106/1000 | Loss: 0.00009777
Iteration 107/1000 | Loss: 0.00010348
Iteration 108/1000 | Loss: 0.00007421
Iteration 109/1000 | Loss: 0.00006055
Iteration 110/1000 | Loss: 0.00009907
Iteration 111/1000 | Loss: 0.00007117
Iteration 112/1000 | Loss: 0.00004964
Iteration 113/1000 | Loss: 0.00016260
Iteration 114/1000 | Loss: 0.00012751
Iteration 115/1000 | Loss: 0.00017212
Iteration 116/1000 | Loss: 0.00050467
Iteration 117/1000 | Loss: 0.00075011
Iteration 118/1000 | Loss: 0.00022273
Iteration 119/1000 | Loss: 0.00007693
Iteration 120/1000 | Loss: 0.00005279
Iteration 121/1000 | Loss: 0.00006531
Iteration 122/1000 | Loss: 0.00004699
Iteration 123/1000 | Loss: 0.00005424
Iteration 124/1000 | Loss: 0.00004627
Iteration 125/1000 | Loss: 0.00010977
Iteration 126/1000 | Loss: 0.00004452
Iteration 127/1000 | Loss: 0.00004360
Iteration 128/1000 | Loss: 0.00013827
Iteration 129/1000 | Loss: 0.00010557
Iteration 130/1000 | Loss: 0.00006758
Iteration 131/1000 | Loss: 0.00004964
Iteration 132/1000 | Loss: 0.00011529
Iteration 133/1000 | Loss: 0.00013907
Iteration 134/1000 | Loss: 0.00005243
Iteration 135/1000 | Loss: 0.00004730
Iteration 136/1000 | Loss: 0.00004420
Iteration 137/1000 | Loss: 0.00007355
Iteration 138/1000 | Loss: 0.00004301
Iteration 139/1000 | Loss: 0.00004244
Iteration 140/1000 | Loss: 0.00004715
Iteration 141/1000 | Loss: 0.00009181
Iteration 142/1000 | Loss: 0.00004465
Iteration 143/1000 | Loss: 0.00004275
Iteration 144/1000 | Loss: 0.00006158
Iteration 145/1000 | Loss: 0.00015647
Iteration 146/1000 | Loss: 0.00004241
Iteration 147/1000 | Loss: 0.00004126
Iteration 148/1000 | Loss: 0.00008895
Iteration 149/1000 | Loss: 0.00005174
Iteration 150/1000 | Loss: 0.00004071
Iteration 151/1000 | Loss: 0.00004048
Iteration 152/1000 | Loss: 0.00009136
Iteration 153/1000 | Loss: 0.00005930
Iteration 154/1000 | Loss: 0.00005315
Iteration 155/1000 | Loss: 0.00004139
Iteration 156/1000 | Loss: 0.00004046
Iteration 157/1000 | Loss: 0.00004024
Iteration 158/1000 | Loss: 0.00004022
Iteration 159/1000 | Loss: 0.00004022
Iteration 160/1000 | Loss: 0.00004022
Iteration 161/1000 | Loss: 0.00004022
Iteration 162/1000 | Loss: 0.00004021
Iteration 163/1000 | Loss: 0.00004021
Iteration 164/1000 | Loss: 0.00004020
Iteration 165/1000 | Loss: 0.00004020
Iteration 166/1000 | Loss: 0.00004020
Iteration 167/1000 | Loss: 0.00004020
Iteration 168/1000 | Loss: 0.00004020
Iteration 169/1000 | Loss: 0.00004020
Iteration 170/1000 | Loss: 0.00004020
Iteration 171/1000 | Loss: 0.00004020
Iteration 172/1000 | Loss: 0.00004020
Iteration 173/1000 | Loss: 0.00004020
Iteration 174/1000 | Loss: 0.00004020
Iteration 175/1000 | Loss: 0.00004020
Iteration 176/1000 | Loss: 0.00004020
Iteration 177/1000 | Loss: 0.00004020
Iteration 178/1000 | Loss: 0.00004020
Iteration 179/1000 | Loss: 0.00004020
Iteration 180/1000 | Loss: 0.00004020
Iteration 181/1000 | Loss: 0.00004020
Iteration 182/1000 | Loss: 0.00004020
Iteration 183/1000 | Loss: 0.00004020
Iteration 184/1000 | Loss: 0.00004020
Iteration 185/1000 | Loss: 0.00004020
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [4.019950210931711e-05, 4.019950210931711e-05, 4.019950210931711e-05, 4.019950210931711e-05, 4.019950210931711e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.019950210931711e-05

Optimization complete. Final v2v error: 4.71683406829834 mm

Highest mean error: 19.618526458740234 mm for frame 122

Lowest mean error: 3.5893537998199463 mm for frame 0

Saving results

Total time: 296.24204206466675
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00827732
Iteration 2/25 | Loss: 0.00148002
Iteration 3/25 | Loss: 0.00105615
Iteration 4/25 | Loss: 0.00102601
Iteration 5/25 | Loss: 0.00102337
Iteration 6/25 | Loss: 0.00102296
Iteration 7/25 | Loss: 0.00102296
Iteration 8/25 | Loss: 0.00102296
Iteration 9/25 | Loss: 0.00102296
Iteration 10/25 | Loss: 0.00102296
Iteration 11/25 | Loss: 0.00102296
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010229571489617229, 0.0010229571489617229, 0.0010229571489617229, 0.0010229571489617229, 0.0010229571489617229]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010229571489617229

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34559250
Iteration 2/25 | Loss: 0.00043936
Iteration 3/25 | Loss: 0.00043935
Iteration 4/25 | Loss: 0.00043935
Iteration 5/25 | Loss: 0.00043935
Iteration 6/25 | Loss: 0.00043935
Iteration 7/25 | Loss: 0.00043934
Iteration 8/25 | Loss: 0.00043934
Iteration 9/25 | Loss: 0.00043934
Iteration 10/25 | Loss: 0.00043934
Iteration 11/25 | Loss: 0.00043934
Iteration 12/25 | Loss: 0.00043934
Iteration 13/25 | Loss: 0.00043934
Iteration 14/25 | Loss: 0.00043934
Iteration 15/25 | Loss: 0.00043934
Iteration 16/25 | Loss: 0.00043934
Iteration 17/25 | Loss: 0.00043934
Iteration 18/25 | Loss: 0.00043934
Iteration 19/25 | Loss: 0.00043934
Iteration 20/25 | Loss: 0.00043934
Iteration 21/25 | Loss: 0.00043934
Iteration 22/25 | Loss: 0.00043934
Iteration 23/25 | Loss: 0.00043934
Iteration 24/25 | Loss: 0.00043934
Iteration 25/25 | Loss: 0.00043934

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043934
Iteration 2/1000 | Loss: 0.00002244
Iteration 3/1000 | Loss: 0.00001638
Iteration 4/1000 | Loss: 0.00001468
Iteration 5/1000 | Loss: 0.00001362
Iteration 6/1000 | Loss: 0.00001315
Iteration 7/1000 | Loss: 0.00001267
Iteration 8/1000 | Loss: 0.00001234
Iteration 9/1000 | Loss: 0.00001208
Iteration 10/1000 | Loss: 0.00001187
Iteration 11/1000 | Loss: 0.00001183
Iteration 12/1000 | Loss: 0.00001178
Iteration 13/1000 | Loss: 0.00001165
Iteration 14/1000 | Loss: 0.00001157
Iteration 15/1000 | Loss: 0.00001152
Iteration 16/1000 | Loss: 0.00001149
Iteration 17/1000 | Loss: 0.00001149
Iteration 18/1000 | Loss: 0.00001149
Iteration 19/1000 | Loss: 0.00001148
Iteration 20/1000 | Loss: 0.00001148
Iteration 21/1000 | Loss: 0.00001148
Iteration 22/1000 | Loss: 0.00001148
Iteration 23/1000 | Loss: 0.00001143
Iteration 24/1000 | Loss: 0.00001142
Iteration 25/1000 | Loss: 0.00001141
Iteration 26/1000 | Loss: 0.00001139
Iteration 27/1000 | Loss: 0.00001137
Iteration 28/1000 | Loss: 0.00001136
Iteration 29/1000 | Loss: 0.00001136
Iteration 30/1000 | Loss: 0.00001136
Iteration 31/1000 | Loss: 0.00001135
Iteration 32/1000 | Loss: 0.00001133
Iteration 33/1000 | Loss: 0.00001132
Iteration 34/1000 | Loss: 0.00001132
Iteration 35/1000 | Loss: 0.00001131
Iteration 36/1000 | Loss: 0.00001131
Iteration 37/1000 | Loss: 0.00001131
Iteration 38/1000 | Loss: 0.00001131
Iteration 39/1000 | Loss: 0.00001131
Iteration 40/1000 | Loss: 0.00001131
Iteration 41/1000 | Loss: 0.00001131
Iteration 42/1000 | Loss: 0.00001131
Iteration 43/1000 | Loss: 0.00001131
Iteration 44/1000 | Loss: 0.00001130
Iteration 45/1000 | Loss: 0.00001130
Iteration 46/1000 | Loss: 0.00001128
Iteration 47/1000 | Loss: 0.00001126
Iteration 48/1000 | Loss: 0.00001122
Iteration 49/1000 | Loss: 0.00001122
Iteration 50/1000 | Loss: 0.00001122
Iteration 51/1000 | Loss: 0.00001122
Iteration 52/1000 | Loss: 0.00001122
Iteration 53/1000 | Loss: 0.00001122
Iteration 54/1000 | Loss: 0.00001119
Iteration 55/1000 | Loss: 0.00001119
Iteration 56/1000 | Loss: 0.00001119
Iteration 57/1000 | Loss: 0.00001119
Iteration 58/1000 | Loss: 0.00001119
Iteration 59/1000 | Loss: 0.00001119
Iteration 60/1000 | Loss: 0.00001118
Iteration 61/1000 | Loss: 0.00001117
Iteration 62/1000 | Loss: 0.00001117
Iteration 63/1000 | Loss: 0.00001117
Iteration 64/1000 | Loss: 0.00001117
Iteration 65/1000 | Loss: 0.00001117
Iteration 66/1000 | Loss: 0.00001116
Iteration 67/1000 | Loss: 0.00001116
Iteration 68/1000 | Loss: 0.00001116
Iteration 69/1000 | Loss: 0.00001115
Iteration 70/1000 | Loss: 0.00001114
Iteration 71/1000 | Loss: 0.00001114
Iteration 72/1000 | Loss: 0.00001113
Iteration 73/1000 | Loss: 0.00001113
Iteration 74/1000 | Loss: 0.00001112
Iteration 75/1000 | Loss: 0.00001112
Iteration 76/1000 | Loss: 0.00001112
Iteration 77/1000 | Loss: 0.00001112
Iteration 78/1000 | Loss: 0.00001112
Iteration 79/1000 | Loss: 0.00001112
Iteration 80/1000 | Loss: 0.00001111
Iteration 81/1000 | Loss: 0.00001111
Iteration 82/1000 | Loss: 0.00001111
Iteration 83/1000 | Loss: 0.00001110
Iteration 84/1000 | Loss: 0.00001110
Iteration 85/1000 | Loss: 0.00001110
Iteration 86/1000 | Loss: 0.00001110
Iteration 87/1000 | Loss: 0.00001110
Iteration 88/1000 | Loss: 0.00001110
Iteration 89/1000 | Loss: 0.00001110
Iteration 90/1000 | Loss: 0.00001110
Iteration 91/1000 | Loss: 0.00001110
Iteration 92/1000 | Loss: 0.00001109
Iteration 93/1000 | Loss: 0.00001109
Iteration 94/1000 | Loss: 0.00001109
Iteration 95/1000 | Loss: 0.00001109
Iteration 96/1000 | Loss: 0.00001109
Iteration 97/1000 | Loss: 0.00001109
Iteration 98/1000 | Loss: 0.00001109
Iteration 99/1000 | Loss: 0.00001109
Iteration 100/1000 | Loss: 0.00001109
Iteration 101/1000 | Loss: 0.00001109
Iteration 102/1000 | Loss: 0.00001109
Iteration 103/1000 | Loss: 0.00001109
Iteration 104/1000 | Loss: 0.00001109
Iteration 105/1000 | Loss: 0.00001109
Iteration 106/1000 | Loss: 0.00001109
Iteration 107/1000 | Loss: 0.00001109
Iteration 108/1000 | Loss: 0.00001109
Iteration 109/1000 | Loss: 0.00001109
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [1.1089312465628609e-05, 1.1089312465628609e-05, 1.1089312465628609e-05, 1.1089312465628609e-05, 1.1089312465628609e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1089312465628609e-05

Optimization complete. Final v2v error: 2.8117876052856445 mm

Highest mean error: 2.8686623573303223 mm for frame 101

Lowest mean error: 2.7505507469177246 mm for frame 141

Saving results

Total time: 33.884682416915894
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01019226
Iteration 2/25 | Loss: 0.01019226
Iteration 3/25 | Loss: 0.01019226
Iteration 4/25 | Loss: 0.01019226
Iteration 5/25 | Loss: 0.01019226
Iteration 6/25 | Loss: 0.01019226
Iteration 7/25 | Loss: 0.01019226
Iteration 8/25 | Loss: 0.01019226
Iteration 9/25 | Loss: 0.01019225
Iteration 10/25 | Loss: 0.01019225
Iteration 11/25 | Loss: 0.01019225
Iteration 12/25 | Loss: 0.01019225
Iteration 13/25 | Loss: 0.01019225
Iteration 14/25 | Loss: 0.01019225
Iteration 15/25 | Loss: 0.01019225
Iteration 16/25 | Loss: 0.01019225
Iteration 17/25 | Loss: 0.01019225
Iteration 18/25 | Loss: 0.01019225
Iteration 19/25 | Loss: 0.01019225
Iteration 20/25 | Loss: 0.01019225
Iteration 21/25 | Loss: 0.01019225
Iteration 22/25 | Loss: 0.01019225
Iteration 23/25 | Loss: 0.01019225
Iteration 24/25 | Loss: 0.01019225
Iteration 25/25 | Loss: 0.01019225

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.03356934
Iteration 2/25 | Loss: 0.00424164
Iteration 3/25 | Loss: 0.00423988
Iteration 4/25 | Loss: 0.00423988
Iteration 5/25 | Loss: 0.00423988
Iteration 6/25 | Loss: 0.00423988
Iteration 7/25 | Loss: 0.00423988
Iteration 8/25 | Loss: 0.00423988
Iteration 9/25 | Loss: 0.00423988
Iteration 10/25 | Loss: 0.00423988
Iteration 11/25 | Loss: 0.00423988
Iteration 12/25 | Loss: 0.00423988
Iteration 13/25 | Loss: 0.00423988
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.004239875357598066, 0.004239875357598066, 0.004239875357598066, 0.004239875357598066, 0.004239875357598066]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004239875357598066

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00423988
Iteration 2/1000 | Loss: 0.00455575
Iteration 3/1000 | Loss: 0.00097345
Iteration 4/1000 | Loss: 0.00051505
Iteration 5/1000 | Loss: 0.00043912
Iteration 6/1000 | Loss: 0.00016869
Iteration 7/1000 | Loss: 0.00427108
Iteration 8/1000 | Loss: 0.00006158
Iteration 9/1000 | Loss: 0.00062470
Iteration 10/1000 | Loss: 0.00187970
Iteration 11/1000 | Loss: 0.00087637
Iteration 12/1000 | Loss: 0.00046087
Iteration 13/1000 | Loss: 0.00085859
Iteration 14/1000 | Loss: 0.00134200
Iteration 15/1000 | Loss: 0.00029032
Iteration 16/1000 | Loss: 0.00060051
Iteration 17/1000 | Loss: 0.00020688
Iteration 18/1000 | Loss: 0.00171939
Iteration 19/1000 | Loss: 0.00022965
Iteration 20/1000 | Loss: 0.00008305
Iteration 21/1000 | Loss: 0.00004647
Iteration 22/1000 | Loss: 0.00028853
Iteration 23/1000 | Loss: 0.00005578
Iteration 24/1000 | Loss: 0.00030857
Iteration 25/1000 | Loss: 0.00034804
Iteration 26/1000 | Loss: 0.00313560
Iteration 27/1000 | Loss: 0.00030385
Iteration 28/1000 | Loss: 0.00007267
Iteration 29/1000 | Loss: 0.00067564
Iteration 30/1000 | Loss: 0.00035204
Iteration 31/1000 | Loss: 0.00038987
Iteration 32/1000 | Loss: 0.00030515
Iteration 33/1000 | Loss: 0.00024702
Iteration 34/1000 | Loss: 0.00057518
Iteration 35/1000 | Loss: 0.00010587
Iteration 36/1000 | Loss: 0.00029870
Iteration 37/1000 | Loss: 0.00014690
Iteration 38/1000 | Loss: 0.00014932
Iteration 39/1000 | Loss: 0.00016325
Iteration 40/1000 | Loss: 0.00109035
Iteration 41/1000 | Loss: 0.00016577
Iteration 42/1000 | Loss: 0.00007283
Iteration 43/1000 | Loss: 0.00072394
Iteration 44/1000 | Loss: 0.00016375
Iteration 45/1000 | Loss: 0.00010084
Iteration 46/1000 | Loss: 0.00036448
Iteration 47/1000 | Loss: 0.00033163
Iteration 48/1000 | Loss: 0.00007654
Iteration 49/1000 | Loss: 0.00014904
Iteration 50/1000 | Loss: 0.00024956
Iteration 51/1000 | Loss: 0.00004554
Iteration 52/1000 | Loss: 0.00014900
Iteration 53/1000 | Loss: 0.00063955
Iteration 54/1000 | Loss: 0.00019676
Iteration 55/1000 | Loss: 0.00006282
Iteration 56/1000 | Loss: 0.00004915
Iteration 57/1000 | Loss: 0.00025921
Iteration 58/1000 | Loss: 0.00003809
Iteration 59/1000 | Loss: 0.00014818
Iteration 60/1000 | Loss: 0.00019956
Iteration 61/1000 | Loss: 0.00029390
Iteration 62/1000 | Loss: 0.00007535
Iteration 63/1000 | Loss: 0.00016492
Iteration 64/1000 | Loss: 0.00012162
Iteration 65/1000 | Loss: 0.00019922
Iteration 66/1000 | Loss: 0.00008573
Iteration 67/1000 | Loss: 0.00016394
Iteration 68/1000 | Loss: 0.00016553
Iteration 69/1000 | Loss: 0.00030443
Iteration 70/1000 | Loss: 0.00014481
Iteration 71/1000 | Loss: 0.00009990
Iteration 72/1000 | Loss: 0.00009220
Iteration 73/1000 | Loss: 0.00001582
Iteration 74/1000 | Loss: 0.00047591
Iteration 75/1000 | Loss: 0.00005144
Iteration 76/1000 | Loss: 0.00067225
Iteration 77/1000 | Loss: 0.00051743
Iteration 78/1000 | Loss: 0.00019096
Iteration 79/1000 | Loss: 0.00029158
Iteration 80/1000 | Loss: 0.00010919
Iteration 81/1000 | Loss: 0.00030523
Iteration 82/1000 | Loss: 0.00007508
Iteration 83/1000 | Loss: 0.00008993
Iteration 84/1000 | Loss: 0.00011734
Iteration 85/1000 | Loss: 0.00013428
Iteration 86/1000 | Loss: 0.00002307
Iteration 87/1000 | Loss: 0.00022348
Iteration 88/1000 | Loss: 0.00021502
Iteration 89/1000 | Loss: 0.00023291
Iteration 90/1000 | Loss: 0.00004612
Iteration 91/1000 | Loss: 0.00008041
Iteration 92/1000 | Loss: 0.00007795
Iteration 93/1000 | Loss: 0.00003287
Iteration 94/1000 | Loss: 0.00003109
Iteration 95/1000 | Loss: 0.00002420
Iteration 96/1000 | Loss: 0.00006963
Iteration 97/1000 | Loss: 0.00001859
Iteration 98/1000 | Loss: 0.00015467
Iteration 99/1000 | Loss: 0.00070206
Iteration 100/1000 | Loss: 0.00056707
Iteration 101/1000 | Loss: 0.00042139
Iteration 102/1000 | Loss: 0.00010097
Iteration 103/1000 | Loss: 0.00023860
Iteration 104/1000 | Loss: 0.00006615
Iteration 105/1000 | Loss: 0.00002215
Iteration 106/1000 | Loss: 0.00021254
Iteration 107/1000 | Loss: 0.00007308
Iteration 108/1000 | Loss: 0.00018864
Iteration 109/1000 | Loss: 0.00003066
Iteration 110/1000 | Loss: 0.00013639
Iteration 111/1000 | Loss: 0.00002593
Iteration 112/1000 | Loss: 0.00007854
Iteration 113/1000 | Loss: 0.00009200
Iteration 114/1000 | Loss: 0.00004186
Iteration 115/1000 | Loss: 0.00007484
Iteration 116/1000 | Loss: 0.00018170
Iteration 117/1000 | Loss: 0.00011564
Iteration 118/1000 | Loss: 0.00008488
Iteration 119/1000 | Loss: 0.00004057
Iteration 120/1000 | Loss: 0.00001829
Iteration 121/1000 | Loss: 0.00007367
Iteration 122/1000 | Loss: 0.00014445
Iteration 123/1000 | Loss: 0.00002401
Iteration 124/1000 | Loss: 0.00001469
Iteration 125/1000 | Loss: 0.00003297
Iteration 126/1000 | Loss: 0.00006966
Iteration 127/1000 | Loss: 0.00003763
Iteration 128/1000 | Loss: 0.00001382
Iteration 129/1000 | Loss: 0.00006775
Iteration 130/1000 | Loss: 0.00002779
Iteration 131/1000 | Loss: 0.00001374
Iteration 132/1000 | Loss: 0.00001373
Iteration 133/1000 | Loss: 0.00001373
Iteration 134/1000 | Loss: 0.00001373
Iteration 135/1000 | Loss: 0.00001373
Iteration 136/1000 | Loss: 0.00001373
Iteration 137/1000 | Loss: 0.00001373
Iteration 138/1000 | Loss: 0.00001373
Iteration 139/1000 | Loss: 0.00001373
Iteration 140/1000 | Loss: 0.00001373
Iteration 141/1000 | Loss: 0.00008561
Iteration 142/1000 | Loss: 0.00007334
Iteration 143/1000 | Loss: 0.00017840
Iteration 144/1000 | Loss: 0.00002959
Iteration 145/1000 | Loss: 0.00008394
Iteration 146/1000 | Loss: 0.00149567
Iteration 147/1000 | Loss: 0.00067253
Iteration 148/1000 | Loss: 0.00025120
Iteration 149/1000 | Loss: 0.00116775
Iteration 150/1000 | Loss: 0.00008614
Iteration 151/1000 | Loss: 0.00016811
Iteration 152/1000 | Loss: 0.00011613
Iteration 153/1000 | Loss: 0.00015686
Iteration 154/1000 | Loss: 0.00003353
Iteration 155/1000 | Loss: 0.00009109
Iteration 156/1000 | Loss: 0.00002336
Iteration 157/1000 | Loss: 0.00009152
Iteration 158/1000 | Loss: 0.00002214
Iteration 159/1000 | Loss: 0.00010574
Iteration 160/1000 | Loss: 0.00012423
Iteration 161/1000 | Loss: 0.00005700
Iteration 162/1000 | Loss: 0.00007263
Iteration 163/1000 | Loss: 0.00007571
Iteration 164/1000 | Loss: 0.00004587
Iteration 165/1000 | Loss: 0.00001809
Iteration 166/1000 | Loss: 0.00001364
Iteration 167/1000 | Loss: 0.00001364
Iteration 168/1000 | Loss: 0.00001363
Iteration 169/1000 | Loss: 0.00001363
Iteration 170/1000 | Loss: 0.00001363
Iteration 171/1000 | Loss: 0.00001363
Iteration 172/1000 | Loss: 0.00001363
Iteration 173/1000 | Loss: 0.00001363
Iteration 174/1000 | Loss: 0.00001363
Iteration 175/1000 | Loss: 0.00001363
Iteration 176/1000 | Loss: 0.00009539
Iteration 177/1000 | Loss: 0.00003347
Iteration 178/1000 | Loss: 0.00001678
Iteration 179/1000 | Loss: 0.00006284
Iteration 180/1000 | Loss: 0.00001701
Iteration 181/1000 | Loss: 0.00001360
Iteration 182/1000 | Loss: 0.00001359
Iteration 183/1000 | Loss: 0.00006311
Iteration 184/1000 | Loss: 0.00009273
Iteration 185/1000 | Loss: 0.00003672
Iteration 186/1000 | Loss: 0.00001354
Iteration 187/1000 | Loss: 0.00001354
Iteration 188/1000 | Loss: 0.00001354
Iteration 189/1000 | Loss: 0.00001354
Iteration 190/1000 | Loss: 0.00001354
Iteration 191/1000 | Loss: 0.00001354
Iteration 192/1000 | Loss: 0.00001354
Iteration 193/1000 | Loss: 0.00002276
Iteration 194/1000 | Loss: 0.00001353
Iteration 195/1000 | Loss: 0.00001352
Iteration 196/1000 | Loss: 0.00001351
Iteration 197/1000 | Loss: 0.00001351
Iteration 198/1000 | Loss: 0.00001351
Iteration 199/1000 | Loss: 0.00001351
Iteration 200/1000 | Loss: 0.00001351
Iteration 201/1000 | Loss: 0.00001351
Iteration 202/1000 | Loss: 0.00001351
Iteration 203/1000 | Loss: 0.00001351
Iteration 204/1000 | Loss: 0.00001351
Iteration 205/1000 | Loss: 0.00001350
Iteration 206/1000 | Loss: 0.00001350
Iteration 207/1000 | Loss: 0.00001350
Iteration 208/1000 | Loss: 0.00001350
Iteration 209/1000 | Loss: 0.00001350
Iteration 210/1000 | Loss: 0.00001350
Iteration 211/1000 | Loss: 0.00001350
Iteration 212/1000 | Loss: 0.00001350
Iteration 213/1000 | Loss: 0.00001350
Iteration 214/1000 | Loss: 0.00001350
Iteration 215/1000 | Loss: 0.00001350
Iteration 216/1000 | Loss: 0.00001350
Iteration 217/1000 | Loss: 0.00001349
Iteration 218/1000 | Loss: 0.00001349
Iteration 219/1000 | Loss: 0.00001349
Iteration 220/1000 | Loss: 0.00001349
Iteration 221/1000 | Loss: 0.00001349
Iteration 222/1000 | Loss: 0.00001349
Iteration 223/1000 | Loss: 0.00001349
Iteration 224/1000 | Loss: 0.00001349
Iteration 225/1000 | Loss: 0.00001349
Iteration 226/1000 | Loss: 0.00001349
Iteration 227/1000 | Loss: 0.00001349
Iteration 228/1000 | Loss: 0.00001349
Iteration 229/1000 | Loss: 0.00001349
Iteration 230/1000 | Loss: 0.00001349
Iteration 231/1000 | Loss: 0.00001348
Iteration 232/1000 | Loss: 0.00001348
Iteration 233/1000 | Loss: 0.00001348
Iteration 234/1000 | Loss: 0.00001348
Iteration 235/1000 | Loss: 0.00001348
Iteration 236/1000 | Loss: 0.00001348
Iteration 237/1000 | Loss: 0.00001348
Iteration 238/1000 | Loss: 0.00001348
Iteration 239/1000 | Loss: 0.00001348
Iteration 240/1000 | Loss: 0.00001348
Iteration 241/1000 | Loss: 0.00001348
Iteration 242/1000 | Loss: 0.00001348
Iteration 243/1000 | Loss: 0.00001348
Iteration 244/1000 | Loss: 0.00001348
Iteration 245/1000 | Loss: 0.00001348
Iteration 246/1000 | Loss: 0.00001347
Iteration 247/1000 | Loss: 0.00001347
Iteration 248/1000 | Loss: 0.00001347
Iteration 249/1000 | Loss: 0.00001347
Iteration 250/1000 | Loss: 0.00001347
Iteration 251/1000 | Loss: 0.00001347
Iteration 252/1000 | Loss: 0.00001347
Iteration 253/1000 | Loss: 0.00001347
Iteration 254/1000 | Loss: 0.00006119
Iteration 255/1000 | Loss: 0.00057770
Iteration 256/1000 | Loss: 0.00018918
Iteration 257/1000 | Loss: 0.00004179
Iteration 258/1000 | Loss: 0.00005904
Iteration 259/1000 | Loss: 0.00001559
Iteration 260/1000 | Loss: 0.00002488
Iteration 261/1000 | Loss: 0.00003264
Iteration 262/1000 | Loss: 0.00009176
Iteration 263/1000 | Loss: 0.00003727
Iteration 264/1000 | Loss: 0.00001582
Iteration 265/1000 | Loss: 0.00006320
Iteration 266/1000 | Loss: 0.00001554
Iteration 267/1000 | Loss: 0.00003542
Iteration 268/1000 | Loss: 0.00003351
Iteration 269/1000 | Loss: 0.00003032
Iteration 270/1000 | Loss: 0.00002256
Iteration 271/1000 | Loss: 0.00001353
Iteration 272/1000 | Loss: 0.00001353
Iteration 273/1000 | Loss: 0.00001352
Iteration 274/1000 | Loss: 0.00001352
Iteration 275/1000 | Loss: 0.00001352
Iteration 276/1000 | Loss: 0.00001352
Iteration 277/1000 | Loss: 0.00001352
Iteration 278/1000 | Loss: 0.00001352
Iteration 279/1000 | Loss: 0.00001352
Iteration 280/1000 | Loss: 0.00001352
Iteration 281/1000 | Loss: 0.00001351
Iteration 282/1000 | Loss: 0.00003959
Iteration 283/1000 | Loss: 0.00001818
Iteration 284/1000 | Loss: 0.00004694
Iteration 285/1000 | Loss: 0.00007711
Iteration 286/1000 | Loss: 0.00037948
Iteration 287/1000 | Loss: 0.00011339
Iteration 288/1000 | Loss: 0.00001934
Iteration 289/1000 | Loss: 0.00001358
Iteration 290/1000 | Loss: 0.00001358
Iteration 291/1000 | Loss: 0.00005139
Iteration 292/1000 | Loss: 0.00003038
Iteration 293/1000 | Loss: 0.00001654
Iteration 294/1000 | Loss: 0.00001352
Iteration 295/1000 | Loss: 0.00001352
Iteration 296/1000 | Loss: 0.00001352
Iteration 297/1000 | Loss: 0.00001352
Iteration 298/1000 | Loss: 0.00001351
Iteration 299/1000 | Loss: 0.00001351
Iteration 300/1000 | Loss: 0.00001351
Iteration 301/1000 | Loss: 0.00001351
Iteration 302/1000 | Loss: 0.00001351
Iteration 303/1000 | Loss: 0.00005254
Iteration 304/1000 | Loss: 0.00001470
Iteration 305/1000 | Loss: 0.00001355
Iteration 306/1000 | Loss: 0.00001353
Iteration 307/1000 | Loss: 0.00001352
Iteration 308/1000 | Loss: 0.00001352
Iteration 309/1000 | Loss: 0.00001350
Iteration 310/1000 | Loss: 0.00001350
Iteration 311/1000 | Loss: 0.00001349
Iteration 312/1000 | Loss: 0.00001349
Iteration 313/1000 | Loss: 0.00001925
Iteration 314/1000 | Loss: 0.00001347
Iteration 315/1000 | Loss: 0.00001347
Iteration 316/1000 | Loss: 0.00001347
Iteration 317/1000 | Loss: 0.00001347
Iteration 318/1000 | Loss: 0.00001347
Iteration 319/1000 | Loss: 0.00001347
Iteration 320/1000 | Loss: 0.00001347
Iteration 321/1000 | Loss: 0.00001347
Iteration 322/1000 | Loss: 0.00001347
Iteration 323/1000 | Loss: 0.00001347
Iteration 324/1000 | Loss: 0.00001347
Iteration 325/1000 | Loss: 0.00001347
Iteration 326/1000 | Loss: 0.00001347
Iteration 327/1000 | Loss: 0.00001347
Iteration 328/1000 | Loss: 0.00001347
Iteration 329/1000 | Loss: 0.00001347
Iteration 330/1000 | Loss: 0.00001347
Iteration 331/1000 | Loss: 0.00001347
Iteration 332/1000 | Loss: 0.00001347
Iteration 333/1000 | Loss: 0.00001347
Iteration 334/1000 | Loss: 0.00001347
Iteration 335/1000 | Loss: 0.00001347
Iteration 336/1000 | Loss: 0.00001347
Iteration 337/1000 | Loss: 0.00001347
Iteration 338/1000 | Loss: 0.00001347
Iteration 339/1000 | Loss: 0.00001347
Iteration 340/1000 | Loss: 0.00001347
Iteration 341/1000 | Loss: 0.00001347
Iteration 342/1000 | Loss: 0.00001347
Iteration 343/1000 | Loss: 0.00001347
Iteration 344/1000 | Loss: 0.00001347
Iteration 345/1000 | Loss: 0.00001347
Iteration 346/1000 | Loss: 0.00001347
Iteration 347/1000 | Loss: 0.00001347
Iteration 348/1000 | Loss: 0.00001347
Iteration 349/1000 | Loss: 0.00001347
Iteration 350/1000 | Loss: 0.00001347
Iteration 351/1000 | Loss: 0.00001347
Iteration 352/1000 | Loss: 0.00001347
Iteration 353/1000 | Loss: 0.00001347
Iteration 354/1000 | Loss: 0.00001347
Iteration 355/1000 | Loss: 0.00001347
Iteration 356/1000 | Loss: 0.00001347
Iteration 357/1000 | Loss: 0.00001347
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 357. Stopping optimization.
Last 5 losses: [1.3467954886436928e-05, 1.3467954886436928e-05, 1.3467954886436928e-05, 1.3467954886436928e-05, 1.3467954886436928e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3467954886436928e-05

Optimization complete. Final v2v error: 3.098463773727417 mm

Highest mean error: 3.443430185317993 mm for frame 2

Lowest mean error: 2.844264507293701 mm for frame 111

Saving results

Total time: 278.6492488384247
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00861118
Iteration 2/25 | Loss: 0.00113172
Iteration 3/25 | Loss: 0.00103761
Iteration 4/25 | Loss: 0.00102697
Iteration 5/25 | Loss: 0.00102337
Iteration 6/25 | Loss: 0.00102305
Iteration 7/25 | Loss: 0.00102305
Iteration 8/25 | Loss: 0.00102305
Iteration 9/25 | Loss: 0.00102305
Iteration 10/25 | Loss: 0.00102305
Iteration 11/25 | Loss: 0.00102305
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010230508632957935, 0.0010230508632957935, 0.0010230508632957935, 0.0010230508632957935, 0.0010230508632957935]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010230508632957935

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31932557
Iteration 2/25 | Loss: 0.00058866
Iteration 3/25 | Loss: 0.00058866
Iteration 4/25 | Loss: 0.00058866
Iteration 5/25 | Loss: 0.00058866
Iteration 6/25 | Loss: 0.00058866
Iteration 7/25 | Loss: 0.00058866
Iteration 8/25 | Loss: 0.00058866
Iteration 9/25 | Loss: 0.00058866
Iteration 10/25 | Loss: 0.00058866
Iteration 11/25 | Loss: 0.00058866
Iteration 12/25 | Loss: 0.00058866
Iteration 13/25 | Loss: 0.00058866
Iteration 14/25 | Loss: 0.00058866
Iteration 15/25 | Loss: 0.00058866
Iteration 16/25 | Loss: 0.00058866
Iteration 17/25 | Loss: 0.00058866
Iteration 18/25 | Loss: 0.00058866
Iteration 19/25 | Loss: 0.00058866
Iteration 20/25 | Loss: 0.00058866
Iteration 21/25 | Loss: 0.00058866
Iteration 22/25 | Loss: 0.00058866
Iteration 23/25 | Loss: 0.00058866
Iteration 24/25 | Loss: 0.00058866
Iteration 25/25 | Loss: 0.00058866

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058866
Iteration 2/1000 | Loss: 0.00002312
Iteration 3/1000 | Loss: 0.00001484
Iteration 4/1000 | Loss: 0.00001310
Iteration 5/1000 | Loss: 0.00001214
Iteration 6/1000 | Loss: 0.00001165
Iteration 7/1000 | Loss: 0.00001142
Iteration 8/1000 | Loss: 0.00001122
Iteration 9/1000 | Loss: 0.00001120
Iteration 10/1000 | Loss: 0.00001094
Iteration 11/1000 | Loss: 0.00001093
Iteration 12/1000 | Loss: 0.00001092
Iteration 13/1000 | Loss: 0.00001082
Iteration 14/1000 | Loss: 0.00001081
Iteration 15/1000 | Loss: 0.00001079
Iteration 16/1000 | Loss: 0.00001078
Iteration 17/1000 | Loss: 0.00001078
Iteration 18/1000 | Loss: 0.00001077
Iteration 19/1000 | Loss: 0.00001076
Iteration 20/1000 | Loss: 0.00001075
Iteration 21/1000 | Loss: 0.00001075
Iteration 22/1000 | Loss: 0.00001074
Iteration 23/1000 | Loss: 0.00001072
Iteration 24/1000 | Loss: 0.00001071
Iteration 25/1000 | Loss: 0.00001071
Iteration 26/1000 | Loss: 0.00001071
Iteration 27/1000 | Loss: 0.00001067
Iteration 28/1000 | Loss: 0.00001067
Iteration 29/1000 | Loss: 0.00001066
Iteration 30/1000 | Loss: 0.00001066
Iteration 31/1000 | Loss: 0.00001065
Iteration 32/1000 | Loss: 0.00001064
Iteration 33/1000 | Loss: 0.00001063
Iteration 34/1000 | Loss: 0.00001063
Iteration 35/1000 | Loss: 0.00001062
Iteration 36/1000 | Loss: 0.00001061
Iteration 37/1000 | Loss: 0.00001061
Iteration 38/1000 | Loss: 0.00001061
Iteration 39/1000 | Loss: 0.00001060
Iteration 40/1000 | Loss: 0.00001058
Iteration 41/1000 | Loss: 0.00001058
Iteration 42/1000 | Loss: 0.00001058
Iteration 43/1000 | Loss: 0.00001058
Iteration 44/1000 | Loss: 0.00001058
Iteration 45/1000 | Loss: 0.00001057
Iteration 46/1000 | Loss: 0.00001057
Iteration 47/1000 | Loss: 0.00001057
Iteration 48/1000 | Loss: 0.00001057
Iteration 49/1000 | Loss: 0.00001057
Iteration 50/1000 | Loss: 0.00001057
Iteration 51/1000 | Loss: 0.00001056
Iteration 52/1000 | Loss: 0.00001056
Iteration 53/1000 | Loss: 0.00001056
Iteration 54/1000 | Loss: 0.00001056
Iteration 55/1000 | Loss: 0.00001055
Iteration 56/1000 | Loss: 0.00001054
Iteration 57/1000 | Loss: 0.00001054
Iteration 58/1000 | Loss: 0.00001053
Iteration 59/1000 | Loss: 0.00001053
Iteration 60/1000 | Loss: 0.00001053
Iteration 61/1000 | Loss: 0.00001052
Iteration 62/1000 | Loss: 0.00001052
Iteration 63/1000 | Loss: 0.00001052
Iteration 64/1000 | Loss: 0.00001052
Iteration 65/1000 | Loss: 0.00001052
Iteration 66/1000 | Loss: 0.00001052
Iteration 67/1000 | Loss: 0.00001051
Iteration 68/1000 | Loss: 0.00001051
Iteration 69/1000 | Loss: 0.00001051
Iteration 70/1000 | Loss: 0.00001051
Iteration 71/1000 | Loss: 0.00001051
Iteration 72/1000 | Loss: 0.00001051
Iteration 73/1000 | Loss: 0.00001051
Iteration 74/1000 | Loss: 0.00001051
Iteration 75/1000 | Loss: 0.00001051
Iteration 76/1000 | Loss: 0.00001051
Iteration 77/1000 | Loss: 0.00001051
Iteration 78/1000 | Loss: 0.00001051
Iteration 79/1000 | Loss: 0.00001050
Iteration 80/1000 | Loss: 0.00001050
Iteration 81/1000 | Loss: 0.00001050
Iteration 82/1000 | Loss: 0.00001050
Iteration 83/1000 | Loss: 0.00001049
Iteration 84/1000 | Loss: 0.00001049
Iteration 85/1000 | Loss: 0.00001049
Iteration 86/1000 | Loss: 0.00001049
Iteration 87/1000 | Loss: 0.00001049
Iteration 88/1000 | Loss: 0.00001049
Iteration 89/1000 | Loss: 0.00001048
Iteration 90/1000 | Loss: 0.00001048
Iteration 91/1000 | Loss: 0.00001048
Iteration 92/1000 | Loss: 0.00001048
Iteration 93/1000 | Loss: 0.00001048
Iteration 94/1000 | Loss: 0.00001048
Iteration 95/1000 | Loss: 0.00001048
Iteration 96/1000 | Loss: 0.00001048
Iteration 97/1000 | Loss: 0.00001047
Iteration 98/1000 | Loss: 0.00001047
Iteration 99/1000 | Loss: 0.00001047
Iteration 100/1000 | Loss: 0.00001047
Iteration 101/1000 | Loss: 0.00001047
Iteration 102/1000 | Loss: 0.00001047
Iteration 103/1000 | Loss: 0.00001047
Iteration 104/1000 | Loss: 0.00001047
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [1.047465866577113e-05, 1.047465866577113e-05, 1.047465866577113e-05, 1.047465866577113e-05, 1.047465866577113e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.047465866577113e-05

Optimization complete. Final v2v error: 2.7571380138397217 mm

Highest mean error: 2.929978132247925 mm for frame 206

Lowest mean error: 2.6273529529571533 mm for frame 133

Saving results

Total time: 33.47819924354553
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00333199
Iteration 2/25 | Loss: 0.00113214
Iteration 3/25 | Loss: 0.00098933
Iteration 4/25 | Loss: 0.00096231
Iteration 5/25 | Loss: 0.00095505
Iteration 6/25 | Loss: 0.00095234
Iteration 7/25 | Loss: 0.00095203
Iteration 8/25 | Loss: 0.00095203
Iteration 9/25 | Loss: 0.00095203
Iteration 10/25 | Loss: 0.00095203
Iteration 11/25 | Loss: 0.00095203
Iteration 12/25 | Loss: 0.00095203
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009520300081931055, 0.0009520300081931055, 0.0009520300081931055, 0.0009520300081931055, 0.0009520300081931055]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009520300081931055

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35106575
Iteration 2/25 | Loss: 0.00069482
Iteration 3/25 | Loss: 0.00069482
Iteration 4/25 | Loss: 0.00069482
Iteration 5/25 | Loss: 0.00069482
Iteration 6/25 | Loss: 0.00069482
Iteration 7/25 | Loss: 0.00069482
Iteration 8/25 | Loss: 0.00069482
Iteration 9/25 | Loss: 0.00069482
Iteration 10/25 | Loss: 0.00069482
Iteration 11/25 | Loss: 0.00069482
Iteration 12/25 | Loss: 0.00069482
Iteration 13/25 | Loss: 0.00069482
Iteration 14/25 | Loss: 0.00069482
Iteration 15/25 | Loss: 0.00069482
Iteration 16/25 | Loss: 0.00069482
Iteration 17/25 | Loss: 0.00069482
Iteration 18/25 | Loss: 0.00069482
Iteration 19/25 | Loss: 0.00069482
Iteration 20/25 | Loss: 0.00069482
Iteration 21/25 | Loss: 0.00069482
Iteration 22/25 | Loss: 0.00069482
Iteration 23/25 | Loss: 0.00069482
Iteration 24/25 | Loss: 0.00069482
Iteration 25/25 | Loss: 0.00069482

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069482
Iteration 2/1000 | Loss: 0.00002735
Iteration 3/1000 | Loss: 0.00001774
Iteration 4/1000 | Loss: 0.00001422
Iteration 5/1000 | Loss: 0.00001256
Iteration 6/1000 | Loss: 0.00001176
Iteration 7/1000 | Loss: 0.00001109
Iteration 8/1000 | Loss: 0.00001074
Iteration 9/1000 | Loss: 0.00001049
Iteration 10/1000 | Loss: 0.00001047
Iteration 11/1000 | Loss: 0.00001035
Iteration 12/1000 | Loss: 0.00001034
Iteration 13/1000 | Loss: 0.00001030
Iteration 14/1000 | Loss: 0.00001014
Iteration 15/1000 | Loss: 0.00001009
Iteration 16/1000 | Loss: 0.00001005
Iteration 17/1000 | Loss: 0.00001000
Iteration 18/1000 | Loss: 0.00001000
Iteration 19/1000 | Loss: 0.00000999
Iteration 20/1000 | Loss: 0.00000998
Iteration 21/1000 | Loss: 0.00000998
Iteration 22/1000 | Loss: 0.00000998
Iteration 23/1000 | Loss: 0.00000997
Iteration 24/1000 | Loss: 0.00000997
Iteration 25/1000 | Loss: 0.00000997
Iteration 26/1000 | Loss: 0.00000995
Iteration 27/1000 | Loss: 0.00000994
Iteration 28/1000 | Loss: 0.00000994
Iteration 29/1000 | Loss: 0.00000992
Iteration 30/1000 | Loss: 0.00000992
Iteration 31/1000 | Loss: 0.00000992
Iteration 32/1000 | Loss: 0.00000992
Iteration 33/1000 | Loss: 0.00000992
Iteration 34/1000 | Loss: 0.00000992
Iteration 35/1000 | Loss: 0.00000991
Iteration 36/1000 | Loss: 0.00000991
Iteration 37/1000 | Loss: 0.00000990
Iteration 38/1000 | Loss: 0.00000990
Iteration 39/1000 | Loss: 0.00000989
Iteration 40/1000 | Loss: 0.00000989
Iteration 41/1000 | Loss: 0.00000988
Iteration 42/1000 | Loss: 0.00000988
Iteration 43/1000 | Loss: 0.00000988
Iteration 44/1000 | Loss: 0.00000988
Iteration 45/1000 | Loss: 0.00000988
Iteration 46/1000 | Loss: 0.00000987
Iteration 47/1000 | Loss: 0.00000987
Iteration 48/1000 | Loss: 0.00000986
Iteration 49/1000 | Loss: 0.00000986
Iteration 50/1000 | Loss: 0.00000985
Iteration 51/1000 | Loss: 0.00000984
Iteration 52/1000 | Loss: 0.00000984
Iteration 53/1000 | Loss: 0.00000984
Iteration 54/1000 | Loss: 0.00000983
Iteration 55/1000 | Loss: 0.00000983
Iteration 56/1000 | Loss: 0.00000982
Iteration 57/1000 | Loss: 0.00000982
Iteration 58/1000 | Loss: 0.00000982
Iteration 59/1000 | Loss: 0.00000981
Iteration 60/1000 | Loss: 0.00000981
Iteration 61/1000 | Loss: 0.00000981
Iteration 62/1000 | Loss: 0.00000981
Iteration 63/1000 | Loss: 0.00000980
Iteration 64/1000 | Loss: 0.00000980
Iteration 65/1000 | Loss: 0.00000980
Iteration 66/1000 | Loss: 0.00000980
Iteration 67/1000 | Loss: 0.00000980
Iteration 68/1000 | Loss: 0.00000980
Iteration 69/1000 | Loss: 0.00000979
Iteration 70/1000 | Loss: 0.00000979
Iteration 71/1000 | Loss: 0.00000979
Iteration 72/1000 | Loss: 0.00000979
Iteration 73/1000 | Loss: 0.00000979
Iteration 74/1000 | Loss: 0.00000978
Iteration 75/1000 | Loss: 0.00000978
Iteration 76/1000 | Loss: 0.00000978
Iteration 77/1000 | Loss: 0.00000977
Iteration 78/1000 | Loss: 0.00000977
Iteration 79/1000 | Loss: 0.00000977
Iteration 80/1000 | Loss: 0.00000977
Iteration 81/1000 | Loss: 0.00000976
Iteration 82/1000 | Loss: 0.00000976
Iteration 83/1000 | Loss: 0.00000976
Iteration 84/1000 | Loss: 0.00000976
Iteration 85/1000 | Loss: 0.00000976
Iteration 86/1000 | Loss: 0.00000975
Iteration 87/1000 | Loss: 0.00000975
Iteration 88/1000 | Loss: 0.00000975
Iteration 89/1000 | Loss: 0.00000975
Iteration 90/1000 | Loss: 0.00000975
Iteration 91/1000 | Loss: 0.00000975
Iteration 92/1000 | Loss: 0.00000975
Iteration 93/1000 | Loss: 0.00000975
Iteration 94/1000 | Loss: 0.00000975
Iteration 95/1000 | Loss: 0.00000974
Iteration 96/1000 | Loss: 0.00000974
Iteration 97/1000 | Loss: 0.00000974
Iteration 98/1000 | Loss: 0.00000974
Iteration 99/1000 | Loss: 0.00000974
Iteration 100/1000 | Loss: 0.00000974
Iteration 101/1000 | Loss: 0.00000974
Iteration 102/1000 | Loss: 0.00000974
Iteration 103/1000 | Loss: 0.00000974
Iteration 104/1000 | Loss: 0.00000973
Iteration 105/1000 | Loss: 0.00000973
Iteration 106/1000 | Loss: 0.00000973
Iteration 107/1000 | Loss: 0.00000973
Iteration 108/1000 | Loss: 0.00000973
Iteration 109/1000 | Loss: 0.00000973
Iteration 110/1000 | Loss: 0.00000973
Iteration 111/1000 | Loss: 0.00000973
Iteration 112/1000 | Loss: 0.00000972
Iteration 113/1000 | Loss: 0.00000972
Iteration 114/1000 | Loss: 0.00000972
Iteration 115/1000 | Loss: 0.00000972
Iteration 116/1000 | Loss: 0.00000972
Iteration 117/1000 | Loss: 0.00000971
Iteration 118/1000 | Loss: 0.00000971
Iteration 119/1000 | Loss: 0.00000971
Iteration 120/1000 | Loss: 0.00000971
Iteration 121/1000 | Loss: 0.00000971
Iteration 122/1000 | Loss: 0.00000971
Iteration 123/1000 | Loss: 0.00000971
Iteration 124/1000 | Loss: 0.00000970
Iteration 125/1000 | Loss: 0.00000970
Iteration 126/1000 | Loss: 0.00000970
Iteration 127/1000 | Loss: 0.00000970
Iteration 128/1000 | Loss: 0.00000970
Iteration 129/1000 | Loss: 0.00000970
Iteration 130/1000 | Loss: 0.00000969
Iteration 131/1000 | Loss: 0.00000969
Iteration 132/1000 | Loss: 0.00000969
Iteration 133/1000 | Loss: 0.00000968
Iteration 134/1000 | Loss: 0.00000968
Iteration 135/1000 | Loss: 0.00000968
Iteration 136/1000 | Loss: 0.00000967
Iteration 137/1000 | Loss: 0.00000967
Iteration 138/1000 | Loss: 0.00000967
Iteration 139/1000 | Loss: 0.00000966
Iteration 140/1000 | Loss: 0.00000966
Iteration 141/1000 | Loss: 0.00000966
Iteration 142/1000 | Loss: 0.00000966
Iteration 143/1000 | Loss: 0.00000966
Iteration 144/1000 | Loss: 0.00000965
Iteration 145/1000 | Loss: 0.00000965
Iteration 146/1000 | Loss: 0.00000965
Iteration 147/1000 | Loss: 0.00000965
Iteration 148/1000 | Loss: 0.00000965
Iteration 149/1000 | Loss: 0.00000964
Iteration 150/1000 | Loss: 0.00000964
Iteration 151/1000 | Loss: 0.00000964
Iteration 152/1000 | Loss: 0.00000964
Iteration 153/1000 | Loss: 0.00000964
Iteration 154/1000 | Loss: 0.00000964
Iteration 155/1000 | Loss: 0.00000963
Iteration 156/1000 | Loss: 0.00000963
Iteration 157/1000 | Loss: 0.00000963
Iteration 158/1000 | Loss: 0.00000963
Iteration 159/1000 | Loss: 0.00000963
Iteration 160/1000 | Loss: 0.00000963
Iteration 161/1000 | Loss: 0.00000963
Iteration 162/1000 | Loss: 0.00000962
Iteration 163/1000 | Loss: 0.00000962
Iteration 164/1000 | Loss: 0.00000962
Iteration 165/1000 | Loss: 0.00000962
Iteration 166/1000 | Loss: 0.00000962
Iteration 167/1000 | Loss: 0.00000962
Iteration 168/1000 | Loss: 0.00000962
Iteration 169/1000 | Loss: 0.00000962
Iteration 170/1000 | Loss: 0.00000962
Iteration 171/1000 | Loss: 0.00000962
Iteration 172/1000 | Loss: 0.00000962
Iteration 173/1000 | Loss: 0.00000962
Iteration 174/1000 | Loss: 0.00000962
Iteration 175/1000 | Loss: 0.00000962
Iteration 176/1000 | Loss: 0.00000962
Iteration 177/1000 | Loss: 0.00000962
Iteration 178/1000 | Loss: 0.00000962
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [9.619626325729769e-06, 9.619626325729769e-06, 9.619626325729769e-06, 9.619626325729769e-06, 9.619626325729769e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.619626325729769e-06

Optimization complete. Final v2v error: 2.64512038230896 mm

Highest mean error: 3.22713041305542 mm for frame 23

Lowest mean error: 2.3302645683288574 mm for frame 251

Saving results

Total time: 45.368765354156494
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00819258
Iteration 2/25 | Loss: 0.00122893
Iteration 3/25 | Loss: 0.00102285
Iteration 4/25 | Loss: 0.00100407
Iteration 5/25 | Loss: 0.00100026
Iteration 6/25 | Loss: 0.00099992
Iteration 7/25 | Loss: 0.00099992
Iteration 8/25 | Loss: 0.00099992
Iteration 9/25 | Loss: 0.00099992
Iteration 10/25 | Loss: 0.00099992
Iteration 11/25 | Loss: 0.00099992
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009999178582802415, 0.0009999178582802415, 0.0009999178582802415, 0.0009999178582802415, 0.0009999178582802415]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009999178582802415

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38231969
Iteration 2/25 | Loss: 0.00065164
Iteration 3/25 | Loss: 0.00065163
Iteration 4/25 | Loss: 0.00065163
Iteration 5/25 | Loss: 0.00065163
Iteration 6/25 | Loss: 0.00065163
Iteration 7/25 | Loss: 0.00065163
Iteration 8/25 | Loss: 0.00065163
Iteration 9/25 | Loss: 0.00065163
Iteration 10/25 | Loss: 0.00065163
Iteration 11/25 | Loss: 0.00065163
Iteration 12/25 | Loss: 0.00065163
Iteration 13/25 | Loss: 0.00065163
Iteration 14/25 | Loss: 0.00065163
Iteration 15/25 | Loss: 0.00065163
Iteration 16/25 | Loss: 0.00065163
Iteration 17/25 | Loss: 0.00065163
Iteration 18/25 | Loss: 0.00065163
Iteration 19/25 | Loss: 0.00065163
Iteration 20/25 | Loss: 0.00065163
Iteration 21/25 | Loss: 0.00065163
Iteration 22/25 | Loss: 0.00065163
Iteration 23/25 | Loss: 0.00065163
Iteration 24/25 | Loss: 0.00065163
Iteration 25/25 | Loss: 0.00065163

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065163
Iteration 2/1000 | Loss: 0.00001885
Iteration 3/1000 | Loss: 0.00001227
Iteration 4/1000 | Loss: 0.00001099
Iteration 5/1000 | Loss: 0.00001012
Iteration 6/1000 | Loss: 0.00000970
Iteration 7/1000 | Loss: 0.00000939
Iteration 8/1000 | Loss: 0.00000919
Iteration 9/1000 | Loss: 0.00000890
Iteration 10/1000 | Loss: 0.00000873
Iteration 11/1000 | Loss: 0.00000858
Iteration 12/1000 | Loss: 0.00000857
Iteration 13/1000 | Loss: 0.00000853
Iteration 14/1000 | Loss: 0.00000853
Iteration 15/1000 | Loss: 0.00000849
Iteration 16/1000 | Loss: 0.00000848
Iteration 17/1000 | Loss: 0.00000848
Iteration 18/1000 | Loss: 0.00000847
Iteration 19/1000 | Loss: 0.00000847
Iteration 20/1000 | Loss: 0.00000846
Iteration 21/1000 | Loss: 0.00000846
Iteration 22/1000 | Loss: 0.00000846
Iteration 23/1000 | Loss: 0.00000846
Iteration 24/1000 | Loss: 0.00000845
Iteration 25/1000 | Loss: 0.00000845
Iteration 26/1000 | Loss: 0.00000845
Iteration 27/1000 | Loss: 0.00000845
Iteration 28/1000 | Loss: 0.00000844
Iteration 29/1000 | Loss: 0.00000844
Iteration 30/1000 | Loss: 0.00000844
Iteration 31/1000 | Loss: 0.00000844
Iteration 32/1000 | Loss: 0.00000844
Iteration 33/1000 | Loss: 0.00000843
Iteration 34/1000 | Loss: 0.00000843
Iteration 35/1000 | Loss: 0.00000843
Iteration 36/1000 | Loss: 0.00000841
Iteration 37/1000 | Loss: 0.00000841
Iteration 38/1000 | Loss: 0.00000841
Iteration 39/1000 | Loss: 0.00000841
Iteration 40/1000 | Loss: 0.00000840
Iteration 41/1000 | Loss: 0.00000840
Iteration 42/1000 | Loss: 0.00000840
Iteration 43/1000 | Loss: 0.00000840
Iteration 44/1000 | Loss: 0.00000840
Iteration 45/1000 | Loss: 0.00000840
Iteration 46/1000 | Loss: 0.00000839
Iteration 47/1000 | Loss: 0.00000839
Iteration 48/1000 | Loss: 0.00000839
Iteration 49/1000 | Loss: 0.00000839
Iteration 50/1000 | Loss: 0.00000838
Iteration 51/1000 | Loss: 0.00000838
Iteration 52/1000 | Loss: 0.00000838
Iteration 53/1000 | Loss: 0.00000838
Iteration 54/1000 | Loss: 0.00000838
Iteration 55/1000 | Loss: 0.00000838
Iteration 56/1000 | Loss: 0.00000838
Iteration 57/1000 | Loss: 0.00000838
Iteration 58/1000 | Loss: 0.00000838
Iteration 59/1000 | Loss: 0.00000838
Iteration 60/1000 | Loss: 0.00000837
Iteration 61/1000 | Loss: 0.00000837
Iteration 62/1000 | Loss: 0.00000837
Iteration 63/1000 | Loss: 0.00000837
Iteration 64/1000 | Loss: 0.00000837
Iteration 65/1000 | Loss: 0.00000837
Iteration 66/1000 | Loss: 0.00000837
Iteration 67/1000 | Loss: 0.00000837
Iteration 68/1000 | Loss: 0.00000837
Iteration 69/1000 | Loss: 0.00000837
Iteration 70/1000 | Loss: 0.00000837
Iteration 71/1000 | Loss: 0.00000837
Iteration 72/1000 | Loss: 0.00000837
Iteration 73/1000 | Loss: 0.00000837
Iteration 74/1000 | Loss: 0.00000837
Iteration 75/1000 | Loss: 0.00000837
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 75. Stopping optimization.
Last 5 losses: [8.370571777049918e-06, 8.370571777049918e-06, 8.370571777049918e-06, 8.370571777049918e-06, 8.370571777049918e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.370571777049918e-06

Optimization complete. Final v2v error: 2.488997459411621 mm

Highest mean error: 2.746217966079712 mm for frame 114

Lowest mean error: 2.3072075843811035 mm for frame 62

Saving results

Total time: 32.803284645080566
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00810960
Iteration 2/25 | Loss: 0.00127161
Iteration 3/25 | Loss: 0.00107939
Iteration 4/25 | Loss: 0.00104230
Iteration 5/25 | Loss: 0.00103134
Iteration 6/25 | Loss: 0.00102831
Iteration 7/25 | Loss: 0.00102768
Iteration 8/25 | Loss: 0.00102768
Iteration 9/25 | Loss: 0.00102768
Iteration 10/25 | Loss: 0.00102768
Iteration 11/25 | Loss: 0.00102768
Iteration 12/25 | Loss: 0.00102768
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010276756947860122, 0.0010276756947860122, 0.0010276756947860122, 0.0010276756947860122, 0.0010276756947860122]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010276756947860122

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.49363685
Iteration 2/25 | Loss: 0.00083628
Iteration 3/25 | Loss: 0.00083626
Iteration 4/25 | Loss: 0.00083626
Iteration 5/25 | Loss: 0.00083625
Iteration 6/25 | Loss: 0.00083625
Iteration 7/25 | Loss: 0.00083625
Iteration 8/25 | Loss: 0.00083625
Iteration 9/25 | Loss: 0.00083625
Iteration 10/25 | Loss: 0.00083625
Iteration 11/25 | Loss: 0.00083625
Iteration 12/25 | Loss: 0.00083625
Iteration 13/25 | Loss: 0.00083625
Iteration 14/25 | Loss: 0.00083625
Iteration 15/25 | Loss: 0.00083625
Iteration 16/25 | Loss: 0.00083625
Iteration 17/25 | Loss: 0.00083625
Iteration 18/25 | Loss: 0.00083625
Iteration 19/25 | Loss: 0.00083625
Iteration 20/25 | Loss: 0.00083625
Iteration 21/25 | Loss: 0.00083625
Iteration 22/25 | Loss: 0.00083625
Iteration 23/25 | Loss: 0.00083625
Iteration 24/25 | Loss: 0.00083625
Iteration 25/25 | Loss: 0.00083625

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083625
Iteration 2/1000 | Loss: 0.00005779
Iteration 3/1000 | Loss: 0.00003081
Iteration 4/1000 | Loss: 0.00002086
Iteration 5/1000 | Loss: 0.00001839
Iteration 6/1000 | Loss: 0.00001719
Iteration 7/1000 | Loss: 0.00001628
Iteration 8/1000 | Loss: 0.00001588
Iteration 9/1000 | Loss: 0.00001552
Iteration 10/1000 | Loss: 0.00001521
Iteration 11/1000 | Loss: 0.00001499
Iteration 12/1000 | Loss: 0.00001499
Iteration 13/1000 | Loss: 0.00001493
Iteration 14/1000 | Loss: 0.00001493
Iteration 15/1000 | Loss: 0.00001492
Iteration 16/1000 | Loss: 0.00001487
Iteration 17/1000 | Loss: 0.00001483
Iteration 18/1000 | Loss: 0.00001482
Iteration 19/1000 | Loss: 0.00001482
Iteration 20/1000 | Loss: 0.00001478
Iteration 21/1000 | Loss: 0.00001474
Iteration 22/1000 | Loss: 0.00001470
Iteration 23/1000 | Loss: 0.00001468
Iteration 24/1000 | Loss: 0.00001467
Iteration 25/1000 | Loss: 0.00001466
Iteration 26/1000 | Loss: 0.00001466
Iteration 27/1000 | Loss: 0.00001465
Iteration 28/1000 | Loss: 0.00001465
Iteration 29/1000 | Loss: 0.00001464
Iteration 30/1000 | Loss: 0.00001464
Iteration 31/1000 | Loss: 0.00001463
Iteration 32/1000 | Loss: 0.00001463
Iteration 33/1000 | Loss: 0.00001461
Iteration 34/1000 | Loss: 0.00001461
Iteration 35/1000 | Loss: 0.00001460
Iteration 36/1000 | Loss: 0.00001460
Iteration 37/1000 | Loss: 0.00001459
Iteration 38/1000 | Loss: 0.00001459
Iteration 39/1000 | Loss: 0.00001456
Iteration 40/1000 | Loss: 0.00001456
Iteration 41/1000 | Loss: 0.00001451
Iteration 42/1000 | Loss: 0.00001451
Iteration 43/1000 | Loss: 0.00001450
Iteration 44/1000 | Loss: 0.00001449
Iteration 45/1000 | Loss: 0.00001449
Iteration 46/1000 | Loss: 0.00001449
Iteration 47/1000 | Loss: 0.00001448
Iteration 48/1000 | Loss: 0.00001448
Iteration 49/1000 | Loss: 0.00001448
Iteration 50/1000 | Loss: 0.00001448
Iteration 51/1000 | Loss: 0.00001447
Iteration 52/1000 | Loss: 0.00001447
Iteration 53/1000 | Loss: 0.00001447
Iteration 54/1000 | Loss: 0.00001447
Iteration 55/1000 | Loss: 0.00001447
Iteration 56/1000 | Loss: 0.00001446
Iteration 57/1000 | Loss: 0.00001446
Iteration 58/1000 | Loss: 0.00001446
Iteration 59/1000 | Loss: 0.00001445
Iteration 60/1000 | Loss: 0.00001444
Iteration 61/1000 | Loss: 0.00001444
Iteration 62/1000 | Loss: 0.00001444
Iteration 63/1000 | Loss: 0.00001444
Iteration 64/1000 | Loss: 0.00001443
Iteration 65/1000 | Loss: 0.00001443
Iteration 66/1000 | Loss: 0.00001443
Iteration 67/1000 | Loss: 0.00001443
Iteration 68/1000 | Loss: 0.00001442
Iteration 69/1000 | Loss: 0.00001442
Iteration 70/1000 | Loss: 0.00001441
Iteration 71/1000 | Loss: 0.00001441
Iteration 72/1000 | Loss: 0.00001441
Iteration 73/1000 | Loss: 0.00001440
Iteration 74/1000 | Loss: 0.00001440
Iteration 75/1000 | Loss: 0.00001440
Iteration 76/1000 | Loss: 0.00001439
Iteration 77/1000 | Loss: 0.00001439
Iteration 78/1000 | Loss: 0.00001439
Iteration 79/1000 | Loss: 0.00001438
Iteration 80/1000 | Loss: 0.00001438
Iteration 81/1000 | Loss: 0.00001438
Iteration 82/1000 | Loss: 0.00001438
Iteration 83/1000 | Loss: 0.00001437
Iteration 84/1000 | Loss: 0.00001437
Iteration 85/1000 | Loss: 0.00001437
Iteration 86/1000 | Loss: 0.00001437
Iteration 87/1000 | Loss: 0.00001437
Iteration 88/1000 | Loss: 0.00001437
Iteration 89/1000 | Loss: 0.00001437
Iteration 90/1000 | Loss: 0.00001437
Iteration 91/1000 | Loss: 0.00001437
Iteration 92/1000 | Loss: 0.00001437
Iteration 93/1000 | Loss: 0.00001437
Iteration 94/1000 | Loss: 0.00001437
Iteration 95/1000 | Loss: 0.00001437
Iteration 96/1000 | Loss: 0.00001436
Iteration 97/1000 | Loss: 0.00001436
Iteration 98/1000 | Loss: 0.00001436
Iteration 99/1000 | Loss: 0.00001436
Iteration 100/1000 | Loss: 0.00001436
Iteration 101/1000 | Loss: 0.00001436
Iteration 102/1000 | Loss: 0.00001436
Iteration 103/1000 | Loss: 0.00001435
Iteration 104/1000 | Loss: 0.00001435
Iteration 105/1000 | Loss: 0.00001435
Iteration 106/1000 | Loss: 0.00001435
Iteration 107/1000 | Loss: 0.00001435
Iteration 108/1000 | Loss: 0.00001435
Iteration 109/1000 | Loss: 0.00001435
Iteration 110/1000 | Loss: 0.00001435
Iteration 111/1000 | Loss: 0.00001435
Iteration 112/1000 | Loss: 0.00001435
Iteration 113/1000 | Loss: 0.00001435
Iteration 114/1000 | Loss: 0.00001434
Iteration 115/1000 | Loss: 0.00001434
Iteration 116/1000 | Loss: 0.00001434
Iteration 117/1000 | Loss: 0.00001434
Iteration 118/1000 | Loss: 0.00001434
Iteration 119/1000 | Loss: 0.00001434
Iteration 120/1000 | Loss: 0.00001434
Iteration 121/1000 | Loss: 0.00001434
Iteration 122/1000 | Loss: 0.00001434
Iteration 123/1000 | Loss: 0.00001434
Iteration 124/1000 | Loss: 0.00001434
Iteration 125/1000 | Loss: 0.00001434
Iteration 126/1000 | Loss: 0.00001434
Iteration 127/1000 | Loss: 0.00001433
Iteration 128/1000 | Loss: 0.00001433
Iteration 129/1000 | Loss: 0.00001433
Iteration 130/1000 | Loss: 0.00001433
Iteration 131/1000 | Loss: 0.00001433
Iteration 132/1000 | Loss: 0.00001433
Iteration 133/1000 | Loss: 0.00001433
Iteration 134/1000 | Loss: 0.00001433
Iteration 135/1000 | Loss: 0.00001433
Iteration 136/1000 | Loss: 0.00001433
Iteration 137/1000 | Loss: 0.00001433
Iteration 138/1000 | Loss: 0.00001433
Iteration 139/1000 | Loss: 0.00001433
Iteration 140/1000 | Loss: 0.00001433
Iteration 141/1000 | Loss: 0.00001433
Iteration 142/1000 | Loss: 0.00001433
Iteration 143/1000 | Loss: 0.00001433
Iteration 144/1000 | Loss: 0.00001432
Iteration 145/1000 | Loss: 0.00001432
Iteration 146/1000 | Loss: 0.00001432
Iteration 147/1000 | Loss: 0.00001432
Iteration 148/1000 | Loss: 0.00001432
Iteration 149/1000 | Loss: 0.00001432
Iteration 150/1000 | Loss: 0.00001432
Iteration 151/1000 | Loss: 0.00001432
Iteration 152/1000 | Loss: 0.00001432
Iteration 153/1000 | Loss: 0.00001432
Iteration 154/1000 | Loss: 0.00001432
Iteration 155/1000 | Loss: 0.00001432
Iteration 156/1000 | Loss: 0.00001432
Iteration 157/1000 | Loss: 0.00001431
Iteration 158/1000 | Loss: 0.00001431
Iteration 159/1000 | Loss: 0.00001431
Iteration 160/1000 | Loss: 0.00001431
Iteration 161/1000 | Loss: 0.00001431
Iteration 162/1000 | Loss: 0.00001431
Iteration 163/1000 | Loss: 0.00001431
Iteration 164/1000 | Loss: 0.00001431
Iteration 165/1000 | Loss: 0.00001431
Iteration 166/1000 | Loss: 0.00001430
Iteration 167/1000 | Loss: 0.00001430
Iteration 168/1000 | Loss: 0.00001430
Iteration 169/1000 | Loss: 0.00001430
Iteration 170/1000 | Loss: 0.00001430
Iteration 171/1000 | Loss: 0.00001430
Iteration 172/1000 | Loss: 0.00001430
Iteration 173/1000 | Loss: 0.00001430
Iteration 174/1000 | Loss: 0.00001430
Iteration 175/1000 | Loss: 0.00001430
Iteration 176/1000 | Loss: 0.00001430
Iteration 177/1000 | Loss: 0.00001429
Iteration 178/1000 | Loss: 0.00001429
Iteration 179/1000 | Loss: 0.00001429
Iteration 180/1000 | Loss: 0.00001429
Iteration 181/1000 | Loss: 0.00001429
Iteration 182/1000 | Loss: 0.00001429
Iteration 183/1000 | Loss: 0.00001429
Iteration 184/1000 | Loss: 0.00001429
Iteration 185/1000 | Loss: 0.00001429
Iteration 186/1000 | Loss: 0.00001429
Iteration 187/1000 | Loss: 0.00001429
Iteration 188/1000 | Loss: 0.00001429
Iteration 189/1000 | Loss: 0.00001429
Iteration 190/1000 | Loss: 0.00001428
Iteration 191/1000 | Loss: 0.00001428
Iteration 192/1000 | Loss: 0.00001428
Iteration 193/1000 | Loss: 0.00001428
Iteration 194/1000 | Loss: 0.00001428
Iteration 195/1000 | Loss: 0.00001428
Iteration 196/1000 | Loss: 0.00001428
Iteration 197/1000 | Loss: 0.00001428
Iteration 198/1000 | Loss: 0.00001428
Iteration 199/1000 | Loss: 0.00001428
Iteration 200/1000 | Loss: 0.00001428
Iteration 201/1000 | Loss: 0.00001428
Iteration 202/1000 | Loss: 0.00001428
Iteration 203/1000 | Loss: 0.00001428
Iteration 204/1000 | Loss: 0.00001427
Iteration 205/1000 | Loss: 0.00001427
Iteration 206/1000 | Loss: 0.00001427
Iteration 207/1000 | Loss: 0.00001427
Iteration 208/1000 | Loss: 0.00001427
Iteration 209/1000 | Loss: 0.00001427
Iteration 210/1000 | Loss: 0.00001427
Iteration 211/1000 | Loss: 0.00001427
Iteration 212/1000 | Loss: 0.00001427
Iteration 213/1000 | Loss: 0.00001427
Iteration 214/1000 | Loss: 0.00001427
Iteration 215/1000 | Loss: 0.00001427
Iteration 216/1000 | Loss: 0.00001427
Iteration 217/1000 | Loss: 0.00001427
Iteration 218/1000 | Loss: 0.00001426
Iteration 219/1000 | Loss: 0.00001426
Iteration 220/1000 | Loss: 0.00001426
Iteration 221/1000 | Loss: 0.00001426
Iteration 222/1000 | Loss: 0.00001426
Iteration 223/1000 | Loss: 0.00001426
Iteration 224/1000 | Loss: 0.00001426
Iteration 225/1000 | Loss: 0.00001426
Iteration 226/1000 | Loss: 0.00001426
Iteration 227/1000 | Loss: 0.00001426
Iteration 228/1000 | Loss: 0.00001426
Iteration 229/1000 | Loss: 0.00001426
Iteration 230/1000 | Loss: 0.00001426
Iteration 231/1000 | Loss: 0.00001426
Iteration 232/1000 | Loss: 0.00001426
Iteration 233/1000 | Loss: 0.00001426
Iteration 234/1000 | Loss: 0.00001426
Iteration 235/1000 | Loss: 0.00001426
Iteration 236/1000 | Loss: 0.00001426
Iteration 237/1000 | Loss: 0.00001426
Iteration 238/1000 | Loss: 0.00001426
Iteration 239/1000 | Loss: 0.00001425
Iteration 240/1000 | Loss: 0.00001425
Iteration 241/1000 | Loss: 0.00001425
Iteration 242/1000 | Loss: 0.00001425
Iteration 243/1000 | Loss: 0.00001425
Iteration 244/1000 | Loss: 0.00001425
Iteration 245/1000 | Loss: 0.00001425
Iteration 246/1000 | Loss: 0.00001425
Iteration 247/1000 | Loss: 0.00001425
Iteration 248/1000 | Loss: 0.00001425
Iteration 249/1000 | Loss: 0.00001425
Iteration 250/1000 | Loss: 0.00001425
Iteration 251/1000 | Loss: 0.00001425
Iteration 252/1000 | Loss: 0.00001425
Iteration 253/1000 | Loss: 0.00001425
Iteration 254/1000 | Loss: 0.00001425
Iteration 255/1000 | Loss: 0.00001425
Iteration 256/1000 | Loss: 0.00001425
Iteration 257/1000 | Loss: 0.00001425
Iteration 258/1000 | Loss: 0.00001425
Iteration 259/1000 | Loss: 0.00001425
Iteration 260/1000 | Loss: 0.00001425
Iteration 261/1000 | Loss: 0.00001425
Iteration 262/1000 | Loss: 0.00001425
Iteration 263/1000 | Loss: 0.00001425
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 263. Stopping optimization.
Last 5 losses: [1.4249124433263205e-05, 1.4249124433263205e-05, 1.4249124433263205e-05, 1.4249124433263205e-05, 1.4249124433263205e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4249124433263205e-05

Optimization complete. Final v2v error: 3.178084373474121 mm

Highest mean error: 3.864166259765625 mm for frame 10

Lowest mean error: 2.478227138519287 mm for frame 148

Saving results

Total time: 46.17974805831909
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_anna_posed_001/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_anna_posed_001/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00786476
Iteration 2/25 | Loss: 0.00136458
Iteration 3/25 | Loss: 0.00110630
Iteration 4/25 | Loss: 0.00106290
Iteration 5/25 | Loss: 0.00105675
Iteration 6/25 | Loss: 0.00105569
Iteration 7/25 | Loss: 0.00105569
Iteration 8/25 | Loss: 0.00105569
Iteration 9/25 | Loss: 0.00105569
Iteration 10/25 | Loss: 0.00105569
Iteration 11/25 | Loss: 0.00105569
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010556860361248255, 0.0010556860361248255, 0.0010556860361248255, 0.0010556860361248255, 0.0010556860361248255]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010556860361248255

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25470197
Iteration 2/25 | Loss: 0.00046731
Iteration 3/25 | Loss: 0.00046728
Iteration 4/25 | Loss: 0.00046728
Iteration 5/25 | Loss: 0.00046728
Iteration 6/25 | Loss: 0.00046727
Iteration 7/25 | Loss: 0.00046727
Iteration 8/25 | Loss: 0.00046727
Iteration 9/25 | Loss: 0.00046727
Iteration 10/25 | Loss: 0.00046727
Iteration 11/25 | Loss: 0.00046727
Iteration 12/25 | Loss: 0.00046727
Iteration 13/25 | Loss: 0.00046727
Iteration 14/25 | Loss: 0.00046727
Iteration 15/25 | Loss: 0.00046727
Iteration 16/25 | Loss: 0.00046727
Iteration 17/25 | Loss: 0.00046727
Iteration 18/25 | Loss: 0.00046727
Iteration 19/25 | Loss: 0.00046727
Iteration 20/25 | Loss: 0.00046727
Iteration 21/25 | Loss: 0.00046727
Iteration 22/25 | Loss: 0.00046727
Iteration 23/25 | Loss: 0.00046727
Iteration 24/25 | Loss: 0.00046727
Iteration 25/25 | Loss: 0.00046727

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046727
Iteration 2/1000 | Loss: 0.00004031
Iteration 3/1000 | Loss: 0.00002745
Iteration 4/1000 | Loss: 0.00002307
Iteration 5/1000 | Loss: 0.00002197
Iteration 6/1000 | Loss: 0.00002097
Iteration 7/1000 | Loss: 0.00002029
Iteration 8/1000 | Loss: 0.00001997
Iteration 9/1000 | Loss: 0.00001968
Iteration 10/1000 | Loss: 0.00001935
Iteration 11/1000 | Loss: 0.00001920
Iteration 12/1000 | Loss: 0.00001915
Iteration 13/1000 | Loss: 0.00001910
Iteration 14/1000 | Loss: 0.00001907
Iteration 15/1000 | Loss: 0.00001891
Iteration 16/1000 | Loss: 0.00001874
Iteration 17/1000 | Loss: 0.00001873
Iteration 18/1000 | Loss: 0.00001866
Iteration 19/1000 | Loss: 0.00001866
Iteration 20/1000 | Loss: 0.00001865
Iteration 21/1000 | Loss: 0.00001864
Iteration 22/1000 | Loss: 0.00001864
Iteration 23/1000 | Loss: 0.00001862
Iteration 24/1000 | Loss: 0.00001860
Iteration 25/1000 | Loss: 0.00001860
Iteration 26/1000 | Loss: 0.00001859
Iteration 27/1000 | Loss: 0.00001859
Iteration 28/1000 | Loss: 0.00001858
Iteration 29/1000 | Loss: 0.00001854
Iteration 30/1000 | Loss: 0.00001854
Iteration 31/1000 | Loss: 0.00001854
Iteration 32/1000 | Loss: 0.00001853
Iteration 33/1000 | Loss: 0.00001853
Iteration 34/1000 | Loss: 0.00001853
Iteration 35/1000 | Loss: 0.00001853
Iteration 36/1000 | Loss: 0.00001852
Iteration 37/1000 | Loss: 0.00001852
Iteration 38/1000 | Loss: 0.00001851
Iteration 39/1000 | Loss: 0.00001850
Iteration 40/1000 | Loss: 0.00001849
Iteration 41/1000 | Loss: 0.00001849
Iteration 42/1000 | Loss: 0.00001849
Iteration 43/1000 | Loss: 0.00001849
Iteration 44/1000 | Loss: 0.00001849
Iteration 45/1000 | Loss: 0.00001849
Iteration 46/1000 | Loss: 0.00001849
Iteration 47/1000 | Loss: 0.00001849
Iteration 48/1000 | Loss: 0.00001848
Iteration 49/1000 | Loss: 0.00001848
Iteration 50/1000 | Loss: 0.00001848
Iteration 51/1000 | Loss: 0.00001848
Iteration 52/1000 | Loss: 0.00001848
Iteration 53/1000 | Loss: 0.00001848
Iteration 54/1000 | Loss: 0.00001848
Iteration 55/1000 | Loss: 0.00001848
Iteration 56/1000 | Loss: 0.00001847
Iteration 57/1000 | Loss: 0.00001847
Iteration 58/1000 | Loss: 0.00001847
Iteration 59/1000 | Loss: 0.00001846
Iteration 60/1000 | Loss: 0.00001846
Iteration 61/1000 | Loss: 0.00001845
Iteration 62/1000 | Loss: 0.00001845
Iteration 63/1000 | Loss: 0.00001845
Iteration 64/1000 | Loss: 0.00001844
Iteration 65/1000 | Loss: 0.00001844
Iteration 66/1000 | Loss: 0.00001844
Iteration 67/1000 | Loss: 0.00001844
Iteration 68/1000 | Loss: 0.00001844
Iteration 69/1000 | Loss: 0.00001844
Iteration 70/1000 | Loss: 0.00001844
Iteration 71/1000 | Loss: 0.00001844
Iteration 72/1000 | Loss: 0.00001843
Iteration 73/1000 | Loss: 0.00001843
Iteration 74/1000 | Loss: 0.00001843
Iteration 75/1000 | Loss: 0.00001842
Iteration 76/1000 | Loss: 0.00001842
Iteration 77/1000 | Loss: 0.00001842
Iteration 78/1000 | Loss: 0.00001842
Iteration 79/1000 | Loss: 0.00001842
Iteration 80/1000 | Loss: 0.00001842
Iteration 81/1000 | Loss: 0.00001842
Iteration 82/1000 | Loss: 0.00001842
Iteration 83/1000 | Loss: 0.00001841
Iteration 84/1000 | Loss: 0.00001841
Iteration 85/1000 | Loss: 0.00001841
Iteration 86/1000 | Loss: 0.00001841
Iteration 87/1000 | Loss: 0.00001841
Iteration 88/1000 | Loss: 0.00001841
Iteration 89/1000 | Loss: 0.00001841
Iteration 90/1000 | Loss: 0.00001841
Iteration 91/1000 | Loss: 0.00001840
Iteration 92/1000 | Loss: 0.00001840
Iteration 93/1000 | Loss: 0.00001840
Iteration 94/1000 | Loss: 0.00001840
Iteration 95/1000 | Loss: 0.00001840
Iteration 96/1000 | Loss: 0.00001839
Iteration 97/1000 | Loss: 0.00001839
Iteration 98/1000 | Loss: 0.00001839
Iteration 99/1000 | Loss: 0.00001839
Iteration 100/1000 | Loss: 0.00001839
Iteration 101/1000 | Loss: 0.00001839
Iteration 102/1000 | Loss: 0.00001839
Iteration 103/1000 | Loss: 0.00001839
Iteration 104/1000 | Loss: 0.00001839
Iteration 105/1000 | Loss: 0.00001839
Iteration 106/1000 | Loss: 0.00001838
Iteration 107/1000 | Loss: 0.00001838
Iteration 108/1000 | Loss: 0.00001838
Iteration 109/1000 | Loss: 0.00001838
Iteration 110/1000 | Loss: 0.00001838
Iteration 111/1000 | Loss: 0.00001838
Iteration 112/1000 | Loss: 0.00001838
Iteration 113/1000 | Loss: 0.00001838
Iteration 114/1000 | Loss: 0.00001838
Iteration 115/1000 | Loss: 0.00001838
Iteration 116/1000 | Loss: 0.00001837
Iteration 117/1000 | Loss: 0.00001837
Iteration 118/1000 | Loss: 0.00001837
Iteration 119/1000 | Loss: 0.00001837
Iteration 120/1000 | Loss: 0.00001837
Iteration 121/1000 | Loss: 0.00001837
Iteration 122/1000 | Loss: 0.00001836
Iteration 123/1000 | Loss: 0.00001836
Iteration 124/1000 | Loss: 0.00001836
Iteration 125/1000 | Loss: 0.00001836
Iteration 126/1000 | Loss: 0.00001836
Iteration 127/1000 | Loss: 0.00001836
Iteration 128/1000 | Loss: 0.00001836
Iteration 129/1000 | Loss: 0.00001836
Iteration 130/1000 | Loss: 0.00001835
Iteration 131/1000 | Loss: 0.00001835
Iteration 132/1000 | Loss: 0.00001835
Iteration 133/1000 | Loss: 0.00001835
Iteration 134/1000 | Loss: 0.00001835
Iteration 135/1000 | Loss: 0.00001835
Iteration 136/1000 | Loss: 0.00001835
Iteration 137/1000 | Loss: 0.00001835
Iteration 138/1000 | Loss: 0.00001835
Iteration 139/1000 | Loss: 0.00001835
Iteration 140/1000 | Loss: 0.00001835
Iteration 141/1000 | Loss: 0.00001835
Iteration 142/1000 | Loss: 0.00001834
Iteration 143/1000 | Loss: 0.00001834
Iteration 144/1000 | Loss: 0.00001834
Iteration 145/1000 | Loss: 0.00001834
Iteration 146/1000 | Loss: 0.00001834
Iteration 147/1000 | Loss: 0.00001834
Iteration 148/1000 | Loss: 0.00001834
Iteration 149/1000 | Loss: 0.00001834
Iteration 150/1000 | Loss: 0.00001834
Iteration 151/1000 | Loss: 0.00001833
Iteration 152/1000 | Loss: 0.00001833
Iteration 153/1000 | Loss: 0.00001833
Iteration 154/1000 | Loss: 0.00001833
Iteration 155/1000 | Loss: 0.00001833
Iteration 156/1000 | Loss: 0.00001833
Iteration 157/1000 | Loss: 0.00001833
Iteration 158/1000 | Loss: 0.00001833
Iteration 159/1000 | Loss: 0.00001833
Iteration 160/1000 | Loss: 0.00001832
Iteration 161/1000 | Loss: 0.00001832
Iteration 162/1000 | Loss: 0.00001832
Iteration 163/1000 | Loss: 0.00001832
Iteration 164/1000 | Loss: 0.00001832
Iteration 165/1000 | Loss: 0.00001832
Iteration 166/1000 | Loss: 0.00001832
Iteration 167/1000 | Loss: 0.00001832
Iteration 168/1000 | Loss: 0.00001832
Iteration 169/1000 | Loss: 0.00001832
Iteration 170/1000 | Loss: 0.00001832
Iteration 171/1000 | Loss: 0.00001832
Iteration 172/1000 | Loss: 0.00001832
Iteration 173/1000 | Loss: 0.00001832
Iteration 174/1000 | Loss: 0.00001832
Iteration 175/1000 | Loss: 0.00001832
Iteration 176/1000 | Loss: 0.00001832
Iteration 177/1000 | Loss: 0.00001831
Iteration 178/1000 | Loss: 0.00001831
Iteration 179/1000 | Loss: 0.00001831
Iteration 180/1000 | Loss: 0.00001831
Iteration 181/1000 | Loss: 0.00001831
Iteration 182/1000 | Loss: 0.00001831
Iteration 183/1000 | Loss: 0.00001831
Iteration 184/1000 | Loss: 0.00001831
Iteration 185/1000 | Loss: 0.00001831
Iteration 186/1000 | Loss: 0.00001831
Iteration 187/1000 | Loss: 0.00001831
Iteration 188/1000 | Loss: 0.00001831
Iteration 189/1000 | Loss: 0.00001831
Iteration 190/1000 | Loss: 0.00001830
Iteration 191/1000 | Loss: 0.00001830
Iteration 192/1000 | Loss: 0.00001830
Iteration 193/1000 | Loss: 0.00001830
Iteration 194/1000 | Loss: 0.00001830
Iteration 195/1000 | Loss: 0.00001830
Iteration 196/1000 | Loss: 0.00001830
Iteration 197/1000 | Loss: 0.00001830
Iteration 198/1000 | Loss: 0.00001830
Iteration 199/1000 | Loss: 0.00001830
Iteration 200/1000 | Loss: 0.00001830
Iteration 201/1000 | Loss: 0.00001830
Iteration 202/1000 | Loss: 0.00001830
Iteration 203/1000 | Loss: 0.00001830
Iteration 204/1000 | Loss: 0.00001830
Iteration 205/1000 | Loss: 0.00001830
Iteration 206/1000 | Loss: 0.00001830
Iteration 207/1000 | Loss: 0.00001830
Iteration 208/1000 | Loss: 0.00001830
Iteration 209/1000 | Loss: 0.00001830
Iteration 210/1000 | Loss: 0.00001830
Iteration 211/1000 | Loss: 0.00001830
Iteration 212/1000 | Loss: 0.00001830
Iteration 213/1000 | Loss: 0.00001830
Iteration 214/1000 | Loss: 0.00001830
Iteration 215/1000 | Loss: 0.00001830
Iteration 216/1000 | Loss: 0.00001830
Iteration 217/1000 | Loss: 0.00001830
Iteration 218/1000 | Loss: 0.00001830
Iteration 219/1000 | Loss: 0.00001830
Iteration 220/1000 | Loss: 0.00001830
Iteration 221/1000 | Loss: 0.00001830
Iteration 222/1000 | Loss: 0.00001830
Iteration 223/1000 | Loss: 0.00001830
Iteration 224/1000 | Loss: 0.00001830
Iteration 225/1000 | Loss: 0.00001830
Iteration 226/1000 | Loss: 0.00001830
Iteration 227/1000 | Loss: 0.00001830
Iteration 228/1000 | Loss: 0.00001830
Iteration 229/1000 | Loss: 0.00001830
Iteration 230/1000 | Loss: 0.00001830
Iteration 231/1000 | Loss: 0.00001830
Iteration 232/1000 | Loss: 0.00001830
Iteration 233/1000 | Loss: 0.00001830
Iteration 234/1000 | Loss: 0.00001830
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 234. Stopping optimization.
Last 5 losses: [1.8297536371392198e-05, 1.8297536371392198e-05, 1.8297536371392198e-05, 1.8297536371392198e-05, 1.8297536371392198e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8297536371392198e-05

Optimization complete. Final v2v error: 3.5142815113067627 mm

Highest mean error: 4.491808891296387 mm for frame 137

Lowest mean error: 2.952558994293213 mm for frame 58

Saving results

Total time: 48.21736264228821
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_nl_6135/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_6135/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_6135/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00376098
Iteration 2/25 | Loss: 0.00110070
Iteration 3/25 | Loss: 0.00092748
Iteration 4/25 | Loss: 0.00091021
Iteration 5/25 | Loss: 0.00090807
Iteration 6/25 | Loss: 0.00090780
Iteration 7/25 | Loss: 0.00090780
Iteration 8/25 | Loss: 0.00090780
Iteration 9/25 | Loss: 0.00090780
Iteration 10/25 | Loss: 0.00090780
Iteration 11/25 | Loss: 0.00090780
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00090780173195526, 0.00090780173195526, 0.00090780173195526, 0.00090780173195526, 0.00090780173195526]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00090780173195526

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32670987
Iteration 2/25 | Loss: 0.00063140
Iteration 3/25 | Loss: 0.00063139
Iteration 4/25 | Loss: 0.00063139
Iteration 5/25 | Loss: 0.00063139
Iteration 6/25 | Loss: 0.00063139
Iteration 7/25 | Loss: 0.00063139
Iteration 8/25 | Loss: 0.00063139
Iteration 9/25 | Loss: 0.00063139
Iteration 10/25 | Loss: 0.00063139
Iteration 11/25 | Loss: 0.00063139
Iteration 12/25 | Loss: 0.00063139
Iteration 13/25 | Loss: 0.00063139
Iteration 14/25 | Loss: 0.00063139
Iteration 15/25 | Loss: 0.00063139
Iteration 16/25 | Loss: 0.00063139
Iteration 17/25 | Loss: 0.00063139
Iteration 18/25 | Loss: 0.00063139
Iteration 19/25 | Loss: 0.00063139
Iteration 20/25 | Loss: 0.00063139
Iteration 21/25 | Loss: 0.00063139
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006313897320069373, 0.0006313897320069373, 0.0006313897320069373, 0.0006313897320069373, 0.0006313897320069373]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006313897320069373

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063139
Iteration 2/1000 | Loss: 0.00003295
Iteration 3/1000 | Loss: 0.00001895
Iteration 4/1000 | Loss: 0.00001486
Iteration 5/1000 | Loss: 0.00001345
Iteration 6/1000 | Loss: 0.00001254
Iteration 7/1000 | Loss: 0.00001215
Iteration 8/1000 | Loss: 0.00001184
Iteration 9/1000 | Loss: 0.00001156
Iteration 10/1000 | Loss: 0.00001133
Iteration 11/1000 | Loss: 0.00001116
Iteration 12/1000 | Loss: 0.00001113
Iteration 13/1000 | Loss: 0.00001095
Iteration 14/1000 | Loss: 0.00001082
Iteration 15/1000 | Loss: 0.00001080
Iteration 16/1000 | Loss: 0.00001076
Iteration 17/1000 | Loss: 0.00001076
Iteration 18/1000 | Loss: 0.00001075
Iteration 19/1000 | Loss: 0.00001074
Iteration 20/1000 | Loss: 0.00001070
Iteration 21/1000 | Loss: 0.00001070
Iteration 22/1000 | Loss: 0.00001070
Iteration 23/1000 | Loss: 0.00001069
Iteration 24/1000 | Loss: 0.00001069
Iteration 25/1000 | Loss: 0.00001068
Iteration 26/1000 | Loss: 0.00001067
Iteration 27/1000 | Loss: 0.00001066
Iteration 28/1000 | Loss: 0.00001066
Iteration 29/1000 | Loss: 0.00001066
Iteration 30/1000 | Loss: 0.00001065
Iteration 31/1000 | Loss: 0.00001065
Iteration 32/1000 | Loss: 0.00001065
Iteration 33/1000 | Loss: 0.00001065
Iteration 34/1000 | Loss: 0.00001064
Iteration 35/1000 | Loss: 0.00001064
Iteration 36/1000 | Loss: 0.00001064
Iteration 37/1000 | Loss: 0.00001064
Iteration 38/1000 | Loss: 0.00001064
Iteration 39/1000 | Loss: 0.00001064
Iteration 40/1000 | Loss: 0.00001064
Iteration 41/1000 | Loss: 0.00001063
Iteration 42/1000 | Loss: 0.00001063
Iteration 43/1000 | Loss: 0.00001063
Iteration 44/1000 | Loss: 0.00001063
Iteration 45/1000 | Loss: 0.00001063
Iteration 46/1000 | Loss: 0.00001062
Iteration 47/1000 | Loss: 0.00001062
Iteration 48/1000 | Loss: 0.00001062
Iteration 49/1000 | Loss: 0.00001062
Iteration 50/1000 | Loss: 0.00001062
Iteration 51/1000 | Loss: 0.00001062
Iteration 52/1000 | Loss: 0.00001062
Iteration 53/1000 | Loss: 0.00001062
Iteration 54/1000 | Loss: 0.00001062
Iteration 55/1000 | Loss: 0.00001061
Iteration 56/1000 | Loss: 0.00001061
Iteration 57/1000 | Loss: 0.00001061
Iteration 58/1000 | Loss: 0.00001061
Iteration 59/1000 | Loss: 0.00001061
Iteration 60/1000 | Loss: 0.00001061
Iteration 61/1000 | Loss: 0.00001060
Iteration 62/1000 | Loss: 0.00001060
Iteration 63/1000 | Loss: 0.00001060
Iteration 64/1000 | Loss: 0.00001060
Iteration 65/1000 | Loss: 0.00001060
Iteration 66/1000 | Loss: 0.00001060
Iteration 67/1000 | Loss: 0.00001060
Iteration 68/1000 | Loss: 0.00001060
Iteration 69/1000 | Loss: 0.00001059
Iteration 70/1000 | Loss: 0.00001059
Iteration 71/1000 | Loss: 0.00001059
Iteration 72/1000 | Loss: 0.00001059
Iteration 73/1000 | Loss: 0.00001059
Iteration 74/1000 | Loss: 0.00001059
Iteration 75/1000 | Loss: 0.00001059
Iteration 76/1000 | Loss: 0.00001059
Iteration 77/1000 | Loss: 0.00001059
Iteration 78/1000 | Loss: 0.00001059
Iteration 79/1000 | Loss: 0.00001059
Iteration 80/1000 | Loss: 0.00001059
Iteration 81/1000 | Loss: 0.00001059
Iteration 82/1000 | Loss: 0.00001059
Iteration 83/1000 | Loss: 0.00001059
Iteration 84/1000 | Loss: 0.00001059
Iteration 85/1000 | Loss: 0.00001059
Iteration 86/1000 | Loss: 0.00001059
Iteration 87/1000 | Loss: 0.00001059
Iteration 88/1000 | Loss: 0.00001059
Iteration 89/1000 | Loss: 0.00001059
Iteration 90/1000 | Loss: 0.00001059
Iteration 91/1000 | Loss: 0.00001059
Iteration 92/1000 | Loss: 0.00001059
Iteration 93/1000 | Loss: 0.00001059
Iteration 94/1000 | Loss: 0.00001059
Iteration 95/1000 | Loss: 0.00001059
Iteration 96/1000 | Loss: 0.00001059
Iteration 97/1000 | Loss: 0.00001059
Iteration 98/1000 | Loss: 0.00001059
Iteration 99/1000 | Loss: 0.00001059
Iteration 100/1000 | Loss: 0.00001059
Iteration 101/1000 | Loss: 0.00001059
Iteration 102/1000 | Loss: 0.00001059
Iteration 103/1000 | Loss: 0.00001059
Iteration 104/1000 | Loss: 0.00001059
Iteration 105/1000 | Loss: 0.00001059
Iteration 106/1000 | Loss: 0.00001059
Iteration 107/1000 | Loss: 0.00001059
Iteration 108/1000 | Loss: 0.00001059
Iteration 109/1000 | Loss: 0.00001059
Iteration 110/1000 | Loss: 0.00001059
Iteration 111/1000 | Loss: 0.00001059
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [1.0592441867629532e-05, 1.0592441867629532e-05, 1.0592441867629532e-05, 1.0592441867629532e-05, 1.0592441867629532e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0592441867629532e-05

Optimization complete. Final v2v error: 2.755120038986206 mm

Highest mean error: 3.0072154998779297 mm for frame 16

Lowest mean error: 2.474504232406616 mm for frame 160

Saving results

Total time: 33.267648220062256
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_nl_6135/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_6135/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_6135/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01023428
Iteration 2/25 | Loss: 0.00381403
Iteration 3/25 | Loss: 0.00214557
Iteration 4/25 | Loss: 0.00180895
Iteration 5/25 | Loss: 0.00171071
Iteration 6/25 | Loss: 0.00175537
Iteration 7/25 | Loss: 0.00164475
Iteration 8/25 | Loss: 0.00152280
Iteration 9/25 | Loss: 0.00142531
Iteration 10/25 | Loss: 0.00137161
Iteration 11/25 | Loss: 0.00135733
Iteration 12/25 | Loss: 0.00134797
Iteration 13/25 | Loss: 0.00133965
Iteration 14/25 | Loss: 0.00130889
Iteration 15/25 | Loss: 0.00129756
Iteration 16/25 | Loss: 0.00128904
Iteration 17/25 | Loss: 0.00128410
Iteration 18/25 | Loss: 0.00128569
Iteration 19/25 | Loss: 0.00127980
Iteration 20/25 | Loss: 0.00127540
Iteration 21/25 | Loss: 0.00127533
Iteration 22/25 | Loss: 0.00127856
Iteration 23/25 | Loss: 0.00127673
Iteration 24/25 | Loss: 0.00127223
Iteration 25/25 | Loss: 0.00127184

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34770858
Iteration 2/25 | Loss: 0.00509283
Iteration 3/25 | Loss: 0.00377369
Iteration 4/25 | Loss: 0.00377369
Iteration 5/25 | Loss: 0.00377369
Iteration 6/25 | Loss: 0.00377369
Iteration 7/25 | Loss: 0.00377369
Iteration 8/25 | Loss: 0.00377369
Iteration 9/25 | Loss: 0.00377369
Iteration 10/25 | Loss: 0.00377369
Iteration 11/25 | Loss: 0.00377369
Iteration 12/25 | Loss: 0.00377369
Iteration 13/25 | Loss: 0.00377369
Iteration 14/25 | Loss: 0.00377369
Iteration 15/25 | Loss: 0.00377369
Iteration 16/25 | Loss: 0.00377369
Iteration 17/25 | Loss: 0.00377369
Iteration 18/25 | Loss: 0.00377369
Iteration 19/25 | Loss: 0.00377369
Iteration 20/25 | Loss: 0.00377369
Iteration 21/25 | Loss: 0.00377369
Iteration 22/25 | Loss: 0.00377369
Iteration 23/25 | Loss: 0.00377369
Iteration 24/25 | Loss: 0.00377369
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.003773687407374382, 0.003773687407374382, 0.003773687407374382, 0.003773687407374382, 0.003773687407374382]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003773687407374382

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00377369
Iteration 2/1000 | Loss: 0.00198071
Iteration 3/1000 | Loss: 0.00083445
Iteration 4/1000 | Loss: 0.00188824
Iteration 5/1000 | Loss: 0.00057907
Iteration 6/1000 | Loss: 0.00057630
Iteration 7/1000 | Loss: 0.00049598
Iteration 8/1000 | Loss: 0.00026479
Iteration 9/1000 | Loss: 0.00052615
Iteration 10/1000 | Loss: 0.00060945
Iteration 11/1000 | Loss: 0.00023386
Iteration 12/1000 | Loss: 0.00027483
Iteration 13/1000 | Loss: 0.00052981
Iteration 14/1000 | Loss: 0.00023833
Iteration 15/1000 | Loss: 0.00025163
Iteration 16/1000 | Loss: 0.00024189
Iteration 17/1000 | Loss: 0.00020194
Iteration 18/1000 | Loss: 0.00020517
Iteration 19/1000 | Loss: 0.00020308
Iteration 20/1000 | Loss: 0.00019419
Iteration 21/1000 | Loss: 0.00019790
Iteration 22/1000 | Loss: 0.00019054
Iteration 23/1000 | Loss: 0.00019706
Iteration 24/1000 | Loss: 0.00019875
Iteration 25/1000 | Loss: 0.00031417
Iteration 26/1000 | Loss: 0.00070952
Iteration 27/1000 | Loss: 0.00031674
Iteration 28/1000 | Loss: 0.00019990
Iteration 29/1000 | Loss: 0.00019998
Iteration 30/1000 | Loss: 0.00019202
Iteration 31/1000 | Loss: 0.00019047
Iteration 32/1000 | Loss: 0.00023779
Iteration 33/1000 | Loss: 0.00018737
Iteration 34/1000 | Loss: 0.00018928
Iteration 35/1000 | Loss: 0.00018489
Iteration 36/1000 | Loss: 0.00021776
Iteration 37/1000 | Loss: 0.00019094
Iteration 38/1000 | Loss: 0.00018374
Iteration 39/1000 | Loss: 0.00018871
Iteration 40/1000 | Loss: 0.00028951
Iteration 41/1000 | Loss: 0.00021826
Iteration 42/1000 | Loss: 0.00016324
Iteration 43/1000 | Loss: 0.00017068
Iteration 44/1000 | Loss: 0.00017793
Iteration 45/1000 | Loss: 0.00067315
Iteration 46/1000 | Loss: 0.00064346
Iteration 47/1000 | Loss: 0.00054935
Iteration 48/1000 | Loss: 0.00031474
Iteration 49/1000 | Loss: 0.00024107
Iteration 50/1000 | Loss: 0.00089855
Iteration 51/1000 | Loss: 0.00019724
Iteration 52/1000 | Loss: 0.00019981
Iteration 53/1000 | Loss: 0.00017646
Iteration 54/1000 | Loss: 0.00017463
Iteration 55/1000 | Loss: 0.00016485
Iteration 56/1000 | Loss: 0.00050206
Iteration 57/1000 | Loss: 0.00045047
Iteration 58/1000 | Loss: 0.00018808
Iteration 59/1000 | Loss: 0.00021087
Iteration 60/1000 | Loss: 0.00058493
Iteration 61/1000 | Loss: 0.00017705
Iteration 62/1000 | Loss: 0.00020763
Iteration 63/1000 | Loss: 0.00027780
Iteration 64/1000 | Loss: 0.00017416
Iteration 65/1000 | Loss: 0.00017629
Iteration 66/1000 | Loss: 0.00025316
Iteration 67/1000 | Loss: 0.00017648
Iteration 68/1000 | Loss: 0.00023979
Iteration 69/1000 | Loss: 0.00022756
Iteration 70/1000 | Loss: 0.00017119
Iteration 71/1000 | Loss: 0.00035247
Iteration 72/1000 | Loss: 0.00018111
Iteration 73/1000 | Loss: 0.00019583
Iteration 74/1000 | Loss: 0.00017134
Iteration 75/1000 | Loss: 0.00018488
Iteration 76/1000 | Loss: 0.00017020
Iteration 77/1000 | Loss: 0.00036485
Iteration 78/1000 | Loss: 0.00095695
Iteration 79/1000 | Loss: 0.00028736
Iteration 80/1000 | Loss: 0.00029520
Iteration 81/1000 | Loss: 0.00035401
Iteration 82/1000 | Loss: 0.00019612
Iteration 83/1000 | Loss: 0.00015858
Iteration 84/1000 | Loss: 0.00015447
Iteration 85/1000 | Loss: 0.00014192
Iteration 86/1000 | Loss: 0.00013389
Iteration 87/1000 | Loss: 0.00014112
Iteration 88/1000 | Loss: 0.00012889
Iteration 89/1000 | Loss: 0.00015774
Iteration 90/1000 | Loss: 0.00013723
Iteration 91/1000 | Loss: 0.00012678
Iteration 92/1000 | Loss: 0.00012614
Iteration 93/1000 | Loss: 0.00013038
Iteration 94/1000 | Loss: 0.00012895
Iteration 95/1000 | Loss: 0.00012522
Iteration 96/1000 | Loss: 0.00030520
Iteration 97/1000 | Loss: 0.00029743
Iteration 98/1000 | Loss: 0.00024322
Iteration 99/1000 | Loss: 0.00048540
Iteration 100/1000 | Loss: 0.00058870
Iteration 101/1000 | Loss: 0.00075783
Iteration 102/1000 | Loss: 0.00037152
Iteration 103/1000 | Loss: 0.00026210
Iteration 104/1000 | Loss: 0.00015594
Iteration 105/1000 | Loss: 0.00017353
Iteration 106/1000 | Loss: 0.00014572
Iteration 107/1000 | Loss: 0.00022552
Iteration 108/1000 | Loss: 0.00015753
Iteration 109/1000 | Loss: 0.00011735
Iteration 110/1000 | Loss: 0.00025869
Iteration 111/1000 | Loss: 0.00066031
Iteration 112/1000 | Loss: 0.00027964
Iteration 113/1000 | Loss: 0.00027248
Iteration 114/1000 | Loss: 0.00040916
Iteration 115/1000 | Loss: 0.00022773
Iteration 116/1000 | Loss: 0.00014652
Iteration 117/1000 | Loss: 0.00017597
Iteration 118/1000 | Loss: 0.00012738
Iteration 119/1000 | Loss: 0.00018693
Iteration 120/1000 | Loss: 0.00030756
Iteration 121/1000 | Loss: 0.00011475
Iteration 122/1000 | Loss: 0.00011088
Iteration 123/1000 | Loss: 0.00010775
Iteration 124/1000 | Loss: 0.00010644
Iteration 125/1000 | Loss: 0.00010525
Iteration 126/1000 | Loss: 0.00010381
Iteration 127/1000 | Loss: 0.00027071
Iteration 128/1000 | Loss: 0.00011265
Iteration 129/1000 | Loss: 0.00010630
Iteration 130/1000 | Loss: 0.00010456
Iteration 131/1000 | Loss: 0.00010285
Iteration 132/1000 | Loss: 0.00010192
Iteration 133/1000 | Loss: 0.00010104
Iteration 134/1000 | Loss: 0.00033034
Iteration 135/1000 | Loss: 0.00028165
Iteration 136/1000 | Loss: 0.00015775
Iteration 137/1000 | Loss: 0.00012251
Iteration 138/1000 | Loss: 0.00010926
Iteration 139/1000 | Loss: 0.00014293
Iteration 140/1000 | Loss: 0.00010187
Iteration 141/1000 | Loss: 0.00010021
Iteration 142/1000 | Loss: 0.00012364
Iteration 143/1000 | Loss: 0.00025724
Iteration 144/1000 | Loss: 0.00012835
Iteration 145/1000 | Loss: 0.00010323
Iteration 146/1000 | Loss: 0.00009732
Iteration 147/1000 | Loss: 0.00009580
Iteration 148/1000 | Loss: 0.00009446
Iteration 149/1000 | Loss: 0.00009358
Iteration 150/1000 | Loss: 0.00009301
Iteration 151/1000 | Loss: 0.00039357
Iteration 152/1000 | Loss: 0.00017972
Iteration 153/1000 | Loss: 0.00040180
Iteration 154/1000 | Loss: 0.00016598
Iteration 155/1000 | Loss: 0.00010411
Iteration 156/1000 | Loss: 0.00009757
Iteration 157/1000 | Loss: 0.00009511
Iteration 158/1000 | Loss: 0.00011719
Iteration 159/1000 | Loss: 0.00009915
Iteration 160/1000 | Loss: 0.00009112
Iteration 161/1000 | Loss: 0.00020319
Iteration 162/1000 | Loss: 0.00009341
Iteration 163/1000 | Loss: 0.00009077
Iteration 164/1000 | Loss: 0.00009008
Iteration 165/1000 | Loss: 0.00008821
Iteration 166/1000 | Loss: 0.00008736
Iteration 167/1000 | Loss: 0.00024289
Iteration 168/1000 | Loss: 0.00009666
Iteration 169/1000 | Loss: 0.00009031
Iteration 170/1000 | Loss: 0.00012068
Iteration 171/1000 | Loss: 0.00008772
Iteration 172/1000 | Loss: 0.00008631
Iteration 173/1000 | Loss: 0.00009940
Iteration 174/1000 | Loss: 0.00008526
Iteration 175/1000 | Loss: 0.00010435
Iteration 176/1000 | Loss: 0.00008462
Iteration 177/1000 | Loss: 0.00009692
Iteration 178/1000 | Loss: 0.00022756
Iteration 179/1000 | Loss: 0.00009453
Iteration 180/1000 | Loss: 0.00008794
Iteration 181/1000 | Loss: 0.00022835
Iteration 182/1000 | Loss: 0.00009564
Iteration 183/1000 | Loss: 0.00008824
Iteration 184/1000 | Loss: 0.00008508
Iteration 185/1000 | Loss: 0.00008379
Iteration 186/1000 | Loss: 0.00008242
Iteration 187/1000 | Loss: 0.00008179
Iteration 188/1000 | Loss: 0.00010586
Iteration 189/1000 | Loss: 0.00008125
Iteration 190/1000 | Loss: 0.00022230
Iteration 191/1000 | Loss: 0.00017662
Iteration 192/1000 | Loss: 0.00010712
Iteration 193/1000 | Loss: 0.00016710
Iteration 194/1000 | Loss: 0.00017133
Iteration 195/1000 | Loss: 0.00016065
Iteration 196/1000 | Loss: 0.00008296
Iteration 197/1000 | Loss: 0.00008081
Iteration 198/1000 | Loss: 0.00009005
Iteration 199/1000 | Loss: 0.00007830
Iteration 200/1000 | Loss: 0.00007773
Iteration 201/1000 | Loss: 0.00007732
Iteration 202/1000 | Loss: 0.00007716
Iteration 203/1000 | Loss: 0.00021708
Iteration 204/1000 | Loss: 0.00008690
Iteration 205/1000 | Loss: 0.00010174
Iteration 206/1000 | Loss: 0.00007865
Iteration 207/1000 | Loss: 0.00009447
Iteration 208/1000 | Loss: 0.00022231
Iteration 209/1000 | Loss: 0.00008514
Iteration 210/1000 | Loss: 0.00023754
Iteration 211/1000 | Loss: 0.00011005
Iteration 212/1000 | Loss: 0.00007994
Iteration 213/1000 | Loss: 0.00007573
Iteration 214/1000 | Loss: 0.00007418
Iteration 215/1000 | Loss: 0.00021075
Iteration 216/1000 | Loss: 0.00008320
Iteration 217/1000 | Loss: 0.00007634
Iteration 218/1000 | Loss: 0.00007370
Iteration 219/1000 | Loss: 0.00010104
Iteration 220/1000 | Loss: 0.00007156
Iteration 221/1000 | Loss: 0.00007120
Iteration 222/1000 | Loss: 0.00007016
Iteration 223/1000 | Loss: 0.00008811
Iteration 224/1000 | Loss: 0.00006984
Iteration 225/1000 | Loss: 0.00008582
Iteration 226/1000 | Loss: 0.00006974
Iteration 227/1000 | Loss: 0.00006962
Iteration 228/1000 | Loss: 0.00006961
Iteration 229/1000 | Loss: 0.00006957
Iteration 230/1000 | Loss: 0.00006957
Iteration 231/1000 | Loss: 0.00009145
Iteration 232/1000 | Loss: 0.00006935
Iteration 233/1000 | Loss: 0.00006926
Iteration 234/1000 | Loss: 0.00006926
Iteration 235/1000 | Loss: 0.00006926
Iteration 236/1000 | Loss: 0.00006926
Iteration 237/1000 | Loss: 0.00006926
Iteration 238/1000 | Loss: 0.00006926
Iteration 239/1000 | Loss: 0.00006926
Iteration 240/1000 | Loss: 0.00006926
Iteration 241/1000 | Loss: 0.00006926
Iteration 242/1000 | Loss: 0.00006926
Iteration 243/1000 | Loss: 0.00006926
Iteration 244/1000 | Loss: 0.00006925
Iteration 245/1000 | Loss: 0.00006925
Iteration 246/1000 | Loss: 0.00006925
Iteration 247/1000 | Loss: 0.00006925
Iteration 248/1000 | Loss: 0.00006925
Iteration 249/1000 | Loss: 0.00006925
Iteration 250/1000 | Loss: 0.00006925
Iteration 251/1000 | Loss: 0.00006924
Iteration 252/1000 | Loss: 0.00006921
Iteration 253/1000 | Loss: 0.00006920
Iteration 254/1000 | Loss: 0.00006919
Iteration 255/1000 | Loss: 0.00006919
Iteration 256/1000 | Loss: 0.00006919
Iteration 257/1000 | Loss: 0.00006919
Iteration 258/1000 | Loss: 0.00006919
Iteration 259/1000 | Loss: 0.00006933
Iteration 260/1000 | Loss: 0.00006933
Iteration 261/1000 | Loss: 0.00006928
Iteration 262/1000 | Loss: 0.00006913
Iteration 263/1000 | Loss: 0.00006908
Iteration 264/1000 | Loss: 0.00006908
Iteration 265/1000 | Loss: 0.00006908
Iteration 266/1000 | Loss: 0.00006908
Iteration 267/1000 | Loss: 0.00006908
Iteration 268/1000 | Loss: 0.00006908
Iteration 269/1000 | Loss: 0.00006908
Iteration 270/1000 | Loss: 0.00006908
Iteration 271/1000 | Loss: 0.00006908
Iteration 272/1000 | Loss: 0.00006907
Iteration 273/1000 | Loss: 0.00006907
Iteration 274/1000 | Loss: 0.00006907
Iteration 275/1000 | Loss: 0.00006906
Iteration 276/1000 | Loss: 0.00006905
Iteration 277/1000 | Loss: 0.00006905
Iteration 278/1000 | Loss: 0.00006905
Iteration 279/1000 | Loss: 0.00006904
Iteration 280/1000 | Loss: 0.00006904
Iteration 281/1000 | Loss: 0.00006904
Iteration 282/1000 | Loss: 0.00006903
Iteration 283/1000 | Loss: 0.00006903
Iteration 284/1000 | Loss: 0.00006903
Iteration 285/1000 | Loss: 0.00006903
Iteration 286/1000 | Loss: 0.00006902
Iteration 287/1000 | Loss: 0.00006901
Iteration 288/1000 | Loss: 0.00006942
Iteration 289/1000 | Loss: 0.00009880
Iteration 290/1000 | Loss: 0.00006949
Iteration 291/1000 | Loss: 0.00006913
Iteration 292/1000 | Loss: 0.00006893
Iteration 293/1000 | Loss: 0.00006893
Iteration 294/1000 | Loss: 0.00006893
Iteration 295/1000 | Loss: 0.00006893
Iteration 296/1000 | Loss: 0.00006893
Iteration 297/1000 | Loss: 0.00006893
Iteration 298/1000 | Loss: 0.00006893
Iteration 299/1000 | Loss: 0.00006893
Iteration 300/1000 | Loss: 0.00006892
Iteration 301/1000 | Loss: 0.00006892
Iteration 302/1000 | Loss: 0.00006892
Iteration 303/1000 | Loss: 0.00006892
Iteration 304/1000 | Loss: 0.00006892
Iteration 305/1000 | Loss: 0.00006892
Iteration 306/1000 | Loss: 0.00006891
Iteration 307/1000 | Loss: 0.00006891
Iteration 308/1000 | Loss: 0.00006891
Iteration 309/1000 | Loss: 0.00006891
Iteration 310/1000 | Loss: 0.00006891
Iteration 311/1000 | Loss: 0.00006891
Iteration 312/1000 | Loss: 0.00006891
Iteration 313/1000 | Loss: 0.00006891
Iteration 314/1000 | Loss: 0.00006891
Iteration 315/1000 | Loss: 0.00006891
Iteration 316/1000 | Loss: 0.00006890
Iteration 317/1000 | Loss: 0.00006890
Iteration 318/1000 | Loss: 0.00006890
Iteration 319/1000 | Loss: 0.00006890
Iteration 320/1000 | Loss: 0.00006890
Iteration 321/1000 | Loss: 0.00006890
Iteration 322/1000 | Loss: 0.00006890
Iteration 323/1000 | Loss: 0.00006890
Iteration 324/1000 | Loss: 0.00006890
Iteration 325/1000 | Loss: 0.00006890
Iteration 326/1000 | Loss: 0.00006890
Iteration 327/1000 | Loss: 0.00006890
Iteration 328/1000 | Loss: 0.00006890
Iteration 329/1000 | Loss: 0.00006890
Iteration 330/1000 | Loss: 0.00006890
Iteration 331/1000 | Loss: 0.00006890
Iteration 332/1000 | Loss: 0.00006890
Iteration 333/1000 | Loss: 0.00006890
Iteration 334/1000 | Loss: 0.00006890
Iteration 335/1000 | Loss: 0.00006890
Iteration 336/1000 | Loss: 0.00006890
Iteration 337/1000 | Loss: 0.00006890
Iteration 338/1000 | Loss: 0.00006890
Iteration 339/1000 | Loss: 0.00006890
Iteration 340/1000 | Loss: 0.00006890
Iteration 341/1000 | Loss: 0.00006890
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 341. Stopping optimization.
Last 5 losses: [6.890096847200766e-05, 6.890096847200766e-05, 6.890096847200766e-05, 6.890096847200766e-05, 6.890096847200766e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.890096847200766e-05

Optimization complete. Final v2v error: 3.974975109100342 mm

Highest mean error: 12.541690826416016 mm for frame 107

Lowest mean error: 2.5092990398406982 mm for frame 0

Saving results

Total time: 427.3327329158783
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_nl_6135/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_6135/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_6135/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00950849
Iteration 2/25 | Loss: 0.00254868
Iteration 3/25 | Loss: 0.00153794
Iteration 4/25 | Loss: 0.00131675
Iteration 5/25 | Loss: 0.00131140
Iteration 6/25 | Loss: 0.00130145
Iteration 7/25 | Loss: 0.00126891
Iteration 8/25 | Loss: 0.00124341
Iteration 9/25 | Loss: 0.00123221
Iteration 10/25 | Loss: 0.00123082
Iteration 11/25 | Loss: 0.00122555
Iteration 12/25 | Loss: 0.00122732
Iteration 13/25 | Loss: 0.00122082
Iteration 14/25 | Loss: 0.00120883
Iteration 15/25 | Loss: 0.00120347
Iteration 16/25 | Loss: 0.00119704
Iteration 17/25 | Loss: 0.00119594
Iteration 18/25 | Loss: 0.00119620
Iteration 19/25 | Loss: 0.00119575
Iteration 20/25 | Loss: 0.00118944
Iteration 21/25 | Loss: 0.00118809
Iteration 22/25 | Loss: 0.00118861
Iteration 23/25 | Loss: 0.00119461
Iteration 24/25 | Loss: 0.00119093
Iteration 25/25 | Loss: 0.00118805

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.69276404
Iteration 2/25 | Loss: 0.00310805
Iteration 3/25 | Loss: 0.00306817
Iteration 4/25 | Loss: 0.00306807
Iteration 5/25 | Loss: 0.00306807
Iteration 6/25 | Loss: 0.00306807
Iteration 7/25 | Loss: 0.00306807
Iteration 8/25 | Loss: 0.00306806
Iteration 9/25 | Loss: 0.00306807
Iteration 10/25 | Loss: 0.00306806
Iteration 11/25 | Loss: 0.00306806
Iteration 12/25 | Loss: 0.00306806
Iteration 13/25 | Loss: 0.00306806
Iteration 14/25 | Loss: 0.00306806
Iteration 15/25 | Loss: 0.00306806
Iteration 16/25 | Loss: 0.00306806
Iteration 17/25 | Loss: 0.00306806
Iteration 18/25 | Loss: 0.00306806
Iteration 19/25 | Loss: 0.00306806
Iteration 20/25 | Loss: 0.00306806
Iteration 21/25 | Loss: 0.00306806
Iteration 22/25 | Loss: 0.00306806
Iteration 23/25 | Loss: 0.00306806
Iteration 24/25 | Loss: 0.00306806
Iteration 25/25 | Loss: 0.00306806

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00306806
Iteration 2/1000 | Loss: 0.00046738
Iteration 3/1000 | Loss: 0.00033523
Iteration 4/1000 | Loss: 0.00025516
Iteration 5/1000 | Loss: 0.00042129
Iteration 6/1000 | Loss: 0.00043402
Iteration 7/1000 | Loss: 0.00033926
Iteration 8/1000 | Loss: 0.00021128
Iteration 9/1000 | Loss: 0.00019277
Iteration 10/1000 | Loss: 0.00020882
Iteration 11/1000 | Loss: 0.00018444
Iteration 12/1000 | Loss: 0.00020606
Iteration 13/1000 | Loss: 0.00017862
Iteration 14/1000 | Loss: 0.00018757
Iteration 15/1000 | Loss: 0.00019898
Iteration 16/1000 | Loss: 0.00019045
Iteration 17/1000 | Loss: 0.00018066
Iteration 18/1000 | Loss: 0.00016953
Iteration 19/1000 | Loss: 0.00020084
Iteration 20/1000 | Loss: 0.00017883
Iteration 21/1000 | Loss: 0.00018712
Iteration 22/1000 | Loss: 0.00038506
Iteration 23/1000 | Loss: 0.00032345
Iteration 24/1000 | Loss: 0.00019258
Iteration 25/1000 | Loss: 0.00032494
Iteration 26/1000 | Loss: 0.00029940
Iteration 27/1000 | Loss: 0.00017979
Iteration 28/1000 | Loss: 0.00016983
Iteration 29/1000 | Loss: 0.00017627
Iteration 30/1000 | Loss: 0.00017928
Iteration 31/1000 | Loss: 0.00018181
Iteration 32/1000 | Loss: 0.00018052
Iteration 33/1000 | Loss: 0.00072627
Iteration 34/1000 | Loss: 0.00118421
Iteration 35/1000 | Loss: 0.00047705
Iteration 36/1000 | Loss: 0.00074713
Iteration 37/1000 | Loss: 0.00061970
Iteration 38/1000 | Loss: 0.00018101
Iteration 39/1000 | Loss: 0.00016608
Iteration 40/1000 | Loss: 0.00015948
Iteration 41/1000 | Loss: 0.00061647
Iteration 42/1000 | Loss: 0.00028086
Iteration 43/1000 | Loss: 0.00056053
Iteration 44/1000 | Loss: 0.00030150
Iteration 45/1000 | Loss: 0.00029652
Iteration 46/1000 | Loss: 0.00046126
Iteration 47/1000 | Loss: 0.00019474
Iteration 48/1000 | Loss: 0.00017685
Iteration 49/1000 | Loss: 0.00019153
Iteration 50/1000 | Loss: 0.00018563
Iteration 51/1000 | Loss: 0.00018563
Iteration 52/1000 | Loss: 0.00017870
Iteration 53/1000 | Loss: 0.00018919
Iteration 54/1000 | Loss: 0.00039590
Iteration 55/1000 | Loss: 0.00031952
Iteration 56/1000 | Loss: 0.00038567
Iteration 57/1000 | Loss: 0.00021120
Iteration 58/1000 | Loss: 0.00020624
Iteration 59/1000 | Loss: 0.00017987
Iteration 60/1000 | Loss: 0.00018134
Iteration 61/1000 | Loss: 0.00017605
Iteration 62/1000 | Loss: 0.00019017
Iteration 63/1000 | Loss: 0.00017690
Iteration 64/1000 | Loss: 0.00018718
Iteration 65/1000 | Loss: 0.00018268
Iteration 66/1000 | Loss: 0.00080362
Iteration 67/1000 | Loss: 0.00113838
Iteration 68/1000 | Loss: 0.00135927
Iteration 69/1000 | Loss: 0.00035345
Iteration 70/1000 | Loss: 0.00017283
Iteration 71/1000 | Loss: 0.00018204
Iteration 72/1000 | Loss: 0.00016554
Iteration 73/1000 | Loss: 0.00015675
Iteration 74/1000 | Loss: 0.00103522
Iteration 75/1000 | Loss: 0.00229228
Iteration 76/1000 | Loss: 0.00142566
Iteration 77/1000 | Loss: 0.00060326
Iteration 78/1000 | Loss: 0.00093991
Iteration 79/1000 | Loss: 0.00139309
Iteration 80/1000 | Loss: 0.00086784
Iteration 81/1000 | Loss: 0.00120736
Iteration 82/1000 | Loss: 0.00246590
Iteration 83/1000 | Loss: 0.00191999
Iteration 84/1000 | Loss: 0.00280788
Iteration 85/1000 | Loss: 0.00232165
Iteration 86/1000 | Loss: 0.00236023
Iteration 87/1000 | Loss: 0.00023139
Iteration 88/1000 | Loss: 0.00219488
Iteration 89/1000 | Loss: 0.00062792
Iteration 90/1000 | Loss: 0.00017372
Iteration 91/1000 | Loss: 0.00075146
Iteration 92/1000 | Loss: 0.00053249
Iteration 93/1000 | Loss: 0.00027160
Iteration 94/1000 | Loss: 0.00106379
Iteration 95/1000 | Loss: 0.00093546
Iteration 96/1000 | Loss: 0.00024179
Iteration 97/1000 | Loss: 0.00086664
Iteration 98/1000 | Loss: 0.00069522
Iteration 99/1000 | Loss: 0.00045271
Iteration 100/1000 | Loss: 0.00098241
Iteration 101/1000 | Loss: 0.00036074
Iteration 102/1000 | Loss: 0.00014067
Iteration 103/1000 | Loss: 0.00085773
Iteration 104/1000 | Loss: 0.00030648
Iteration 105/1000 | Loss: 0.00027502
Iteration 106/1000 | Loss: 0.00019596
Iteration 107/1000 | Loss: 0.00013843
Iteration 108/1000 | Loss: 0.00016871
Iteration 109/1000 | Loss: 0.00018046
Iteration 110/1000 | Loss: 0.00015468
Iteration 111/1000 | Loss: 0.00120493
Iteration 112/1000 | Loss: 0.00058541
Iteration 113/1000 | Loss: 0.00016111
Iteration 114/1000 | Loss: 0.00015456
Iteration 115/1000 | Loss: 0.00014918
Iteration 116/1000 | Loss: 0.00022126
Iteration 117/1000 | Loss: 0.00015940
Iteration 118/1000 | Loss: 0.00012795
Iteration 119/1000 | Loss: 0.00095167
Iteration 120/1000 | Loss: 0.00063425
Iteration 121/1000 | Loss: 0.00014995
Iteration 122/1000 | Loss: 0.00099284
Iteration 123/1000 | Loss: 0.00162178
Iteration 124/1000 | Loss: 0.00031597
Iteration 125/1000 | Loss: 0.00026292
Iteration 126/1000 | Loss: 0.00111716
Iteration 127/1000 | Loss: 0.00103888
Iteration 128/1000 | Loss: 0.00095097
Iteration 129/1000 | Loss: 0.00086381
Iteration 130/1000 | Loss: 0.00075206
Iteration 131/1000 | Loss: 0.00040136
Iteration 132/1000 | Loss: 0.00133402
Iteration 133/1000 | Loss: 0.00096552
Iteration 134/1000 | Loss: 0.00076415
Iteration 135/1000 | Loss: 0.00131696
Iteration 136/1000 | Loss: 0.00098957
Iteration 137/1000 | Loss: 0.00032003
Iteration 138/1000 | Loss: 0.00058099
Iteration 139/1000 | Loss: 0.00081198
Iteration 140/1000 | Loss: 0.00023372
Iteration 141/1000 | Loss: 0.00066371
Iteration 142/1000 | Loss: 0.00048655
Iteration 143/1000 | Loss: 0.00096731
Iteration 144/1000 | Loss: 0.00023279
Iteration 145/1000 | Loss: 0.00029326
Iteration 146/1000 | Loss: 0.00021189
Iteration 147/1000 | Loss: 0.00023744
Iteration 148/1000 | Loss: 0.00021626
Iteration 149/1000 | Loss: 0.00021309
Iteration 150/1000 | Loss: 0.00019085
Iteration 151/1000 | Loss: 0.00016339
Iteration 152/1000 | Loss: 0.00027317
Iteration 153/1000 | Loss: 0.00022026
Iteration 154/1000 | Loss: 0.00027637
Iteration 155/1000 | Loss: 0.00019507
Iteration 156/1000 | Loss: 0.00012772
Iteration 157/1000 | Loss: 0.00013635
Iteration 158/1000 | Loss: 0.00012470
Iteration 159/1000 | Loss: 0.00014160
Iteration 160/1000 | Loss: 0.00015545
Iteration 161/1000 | Loss: 0.00014634
Iteration 162/1000 | Loss: 0.00012122
Iteration 163/1000 | Loss: 0.00012320
Iteration 164/1000 | Loss: 0.00012090
Iteration 165/1000 | Loss: 0.00013783
Iteration 166/1000 | Loss: 0.00013798
Iteration 167/1000 | Loss: 0.00013829
Iteration 168/1000 | Loss: 0.00011935
Iteration 169/1000 | Loss: 0.00012288
Iteration 170/1000 | Loss: 0.00011493
Iteration 171/1000 | Loss: 0.00010708
Iteration 172/1000 | Loss: 0.00011585
Iteration 173/1000 | Loss: 0.00013314
Iteration 174/1000 | Loss: 0.00012002
Iteration 175/1000 | Loss: 0.00012149
Iteration 176/1000 | Loss: 0.00012505
Iteration 177/1000 | Loss: 0.00012293
Iteration 178/1000 | Loss: 0.00012079
Iteration 179/1000 | Loss: 0.00011861
Iteration 180/1000 | Loss: 0.00010812
Iteration 181/1000 | Loss: 0.00012650
Iteration 182/1000 | Loss: 0.00011997
Iteration 183/1000 | Loss: 0.00011065
Iteration 184/1000 | Loss: 0.00011128
Iteration 185/1000 | Loss: 0.00012657
Iteration 186/1000 | Loss: 0.00011105
Iteration 187/1000 | Loss: 0.00011664
Iteration 188/1000 | Loss: 0.00011317
Iteration 189/1000 | Loss: 0.00012123
Iteration 190/1000 | Loss: 0.00010377
Iteration 191/1000 | Loss: 0.00011572
Iteration 192/1000 | Loss: 0.00011541
Iteration 193/1000 | Loss: 0.00012157
Iteration 194/1000 | Loss: 0.00011153
Iteration 195/1000 | Loss: 0.00011997
Iteration 196/1000 | Loss: 0.00010622
Iteration 197/1000 | Loss: 0.00010169
Iteration 198/1000 | Loss: 0.00010029
Iteration 199/1000 | Loss: 0.00009937
Iteration 200/1000 | Loss: 0.00052246
Iteration 201/1000 | Loss: 0.00022240
Iteration 202/1000 | Loss: 0.00009909
Iteration 203/1000 | Loss: 0.00009851
Iteration 204/1000 | Loss: 0.00051423
Iteration 205/1000 | Loss: 0.00067640
Iteration 206/1000 | Loss: 0.00025477
Iteration 207/1000 | Loss: 0.00011136
Iteration 208/1000 | Loss: 0.00009920
Iteration 209/1000 | Loss: 0.00009652
Iteration 210/1000 | Loss: 0.00009560
Iteration 211/1000 | Loss: 0.00009511
Iteration 212/1000 | Loss: 0.00009467
Iteration 213/1000 | Loss: 0.00009442
Iteration 214/1000 | Loss: 0.00009426
Iteration 215/1000 | Loss: 0.00009419
Iteration 216/1000 | Loss: 0.00009408
Iteration 217/1000 | Loss: 0.00009401
Iteration 218/1000 | Loss: 0.00009393
Iteration 219/1000 | Loss: 0.00009390
Iteration 220/1000 | Loss: 0.00009389
Iteration 221/1000 | Loss: 0.00009388
Iteration 222/1000 | Loss: 0.00009388
Iteration 223/1000 | Loss: 0.00009388
Iteration 224/1000 | Loss: 0.00009387
Iteration 225/1000 | Loss: 0.00009387
Iteration 226/1000 | Loss: 0.00009387
Iteration 227/1000 | Loss: 0.00009387
Iteration 228/1000 | Loss: 0.00009384
Iteration 229/1000 | Loss: 0.00009379
Iteration 230/1000 | Loss: 0.00009375
Iteration 231/1000 | Loss: 0.00009374
Iteration 232/1000 | Loss: 0.00009373
Iteration 233/1000 | Loss: 0.00009372
Iteration 234/1000 | Loss: 0.00009370
Iteration 235/1000 | Loss: 0.00009362
Iteration 236/1000 | Loss: 0.00009361
Iteration 237/1000 | Loss: 0.00009347
Iteration 238/1000 | Loss: 0.00009346
Iteration 239/1000 | Loss: 0.00009345
Iteration 240/1000 | Loss: 0.00009336
Iteration 241/1000 | Loss: 0.00009326
Iteration 242/1000 | Loss: 0.00009318
Iteration 243/1000 | Loss: 0.00009316
Iteration 244/1000 | Loss: 0.00009315
Iteration 245/1000 | Loss: 0.00009314
Iteration 246/1000 | Loss: 0.00009311
Iteration 247/1000 | Loss: 0.00009310
Iteration 248/1000 | Loss: 0.00009309
Iteration 249/1000 | Loss: 0.00009308
Iteration 250/1000 | Loss: 0.00009308
Iteration 251/1000 | Loss: 0.00009308
Iteration 252/1000 | Loss: 0.00009307
Iteration 253/1000 | Loss: 0.00009307
Iteration 254/1000 | Loss: 0.00009307
Iteration 255/1000 | Loss: 0.00009307
Iteration 256/1000 | Loss: 0.00009307
Iteration 257/1000 | Loss: 0.00009307
Iteration 258/1000 | Loss: 0.00009307
Iteration 259/1000 | Loss: 0.00009307
Iteration 260/1000 | Loss: 0.00009307
Iteration 261/1000 | Loss: 0.00009307
Iteration 262/1000 | Loss: 0.00009307
Iteration 263/1000 | Loss: 0.00009306
Iteration 264/1000 | Loss: 0.00009306
Iteration 265/1000 | Loss: 0.00009306
Iteration 266/1000 | Loss: 0.00009306
Iteration 267/1000 | Loss: 0.00009306
Iteration 268/1000 | Loss: 0.00009306
Iteration 269/1000 | Loss: 0.00009306
Iteration 270/1000 | Loss: 0.00009306
Iteration 271/1000 | Loss: 0.00009306
Iteration 272/1000 | Loss: 0.00009306
Iteration 273/1000 | Loss: 0.00009305
Iteration 274/1000 | Loss: 0.00009305
Iteration 275/1000 | Loss: 0.00009305
Iteration 276/1000 | Loss: 0.00009305
Iteration 277/1000 | Loss: 0.00009305
Iteration 278/1000 | Loss: 0.00009304
Iteration 279/1000 | Loss: 0.00009304
Iteration 280/1000 | Loss: 0.00009303
Iteration 281/1000 | Loss: 0.00009303
Iteration 282/1000 | Loss: 0.00009303
Iteration 283/1000 | Loss: 0.00009303
Iteration 284/1000 | Loss: 0.00009302
Iteration 285/1000 | Loss: 0.00009302
Iteration 286/1000 | Loss: 0.00009301
Iteration 287/1000 | Loss: 0.00009301
Iteration 288/1000 | Loss: 0.00009300
Iteration 289/1000 | Loss: 0.00009300
Iteration 290/1000 | Loss: 0.00009300
Iteration 291/1000 | Loss: 0.00009300
Iteration 292/1000 | Loss: 0.00009300
Iteration 293/1000 | Loss: 0.00009299
Iteration 294/1000 | Loss: 0.00009299
Iteration 295/1000 | Loss: 0.00009299
Iteration 296/1000 | Loss: 0.00009297
Iteration 297/1000 | Loss: 0.00009296
Iteration 298/1000 | Loss: 0.00009296
Iteration 299/1000 | Loss: 0.00009296
Iteration 300/1000 | Loss: 0.00009296
Iteration 301/1000 | Loss: 0.00009295
Iteration 302/1000 | Loss: 0.00009295
Iteration 303/1000 | Loss: 0.00009294
Iteration 304/1000 | Loss: 0.00009294
Iteration 305/1000 | Loss: 0.00009294
Iteration 306/1000 | Loss: 0.00009294
Iteration 307/1000 | Loss: 0.00009294
Iteration 308/1000 | Loss: 0.00009294
Iteration 309/1000 | Loss: 0.00009294
Iteration 310/1000 | Loss: 0.00009294
Iteration 311/1000 | Loss: 0.00009294
Iteration 312/1000 | Loss: 0.00009294
Iteration 313/1000 | Loss: 0.00009294
Iteration 314/1000 | Loss: 0.00009294
Iteration 315/1000 | Loss: 0.00009294
Iteration 316/1000 | Loss: 0.00009294
Iteration 317/1000 | Loss: 0.00009294
Iteration 318/1000 | Loss: 0.00009294
Iteration 319/1000 | Loss: 0.00009294
Iteration 320/1000 | Loss: 0.00009294
Iteration 321/1000 | Loss: 0.00009294
Iteration 322/1000 | Loss: 0.00009294
Iteration 323/1000 | Loss: 0.00009294
Iteration 324/1000 | Loss: 0.00009294
Iteration 325/1000 | Loss: 0.00009294
Iteration 326/1000 | Loss: 0.00009293
Iteration 327/1000 | Loss: 0.00009293
Iteration 328/1000 | Loss: 0.00009293
Iteration 329/1000 | Loss: 0.00009293
Iteration 330/1000 | Loss: 0.00009293
Iteration 331/1000 | Loss: 0.00009293
Iteration 332/1000 | Loss: 0.00009293
Iteration 333/1000 | Loss: 0.00009293
Iteration 334/1000 | Loss: 0.00009293
Iteration 335/1000 | Loss: 0.00009293
Iteration 336/1000 | Loss: 0.00009293
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 336. Stopping optimization.
Last 5 losses: [9.293474431615323e-05, 9.293474431615323e-05, 9.293474431615323e-05, 9.293474431615323e-05, 9.293474431615323e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.293474431615323e-05

Optimization complete. Final v2v error: 4.577797889709473 mm

Highest mean error: 13.222785949707031 mm for frame 59

Lowest mean error: 2.6590917110443115 mm for frame 15

Saving results

Total time: 375.3126666545868
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_nl_6135/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_6135/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_6135/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00418998
Iteration 2/25 | Loss: 0.00110412
Iteration 3/25 | Loss: 0.00098727
Iteration 4/25 | Loss: 0.00096411
Iteration 5/25 | Loss: 0.00095760
Iteration 6/25 | Loss: 0.00095635
Iteration 7/25 | Loss: 0.00095627
Iteration 8/25 | Loss: 0.00095627
Iteration 9/25 | Loss: 0.00095627
Iteration 10/25 | Loss: 0.00095627
Iteration 11/25 | Loss: 0.00095627
Iteration 12/25 | Loss: 0.00095627
Iteration 13/25 | Loss: 0.00095627
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0009562718914821744, 0.0009562718914821744, 0.0009562718914821744, 0.0009562718914821744, 0.0009562718914821744]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009562718914821744

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37344813
Iteration 2/25 | Loss: 0.00063881
Iteration 3/25 | Loss: 0.00063881
Iteration 4/25 | Loss: 0.00063881
Iteration 5/25 | Loss: 0.00063881
Iteration 6/25 | Loss: 0.00063881
Iteration 7/25 | Loss: 0.00063881
Iteration 8/25 | Loss: 0.00063881
Iteration 9/25 | Loss: 0.00063881
Iteration 10/25 | Loss: 0.00063881
Iteration 11/25 | Loss: 0.00063881
Iteration 12/25 | Loss: 0.00063881
Iteration 13/25 | Loss: 0.00063881
Iteration 14/25 | Loss: 0.00063881
Iteration 15/25 | Loss: 0.00063881
Iteration 16/25 | Loss: 0.00063881
Iteration 17/25 | Loss: 0.00063881
Iteration 18/25 | Loss: 0.00063881
Iteration 19/25 | Loss: 0.00063881
Iteration 20/25 | Loss: 0.00063881
Iteration 21/25 | Loss: 0.00063881
Iteration 22/25 | Loss: 0.00063881
Iteration 23/25 | Loss: 0.00063881
Iteration 24/25 | Loss: 0.00063881
Iteration 25/25 | Loss: 0.00063881
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006388100446201861, 0.0006388100446201861, 0.0006388100446201861, 0.0006388100446201861, 0.0006388100446201861]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006388100446201861

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063881
Iteration 2/1000 | Loss: 0.00003903
Iteration 3/1000 | Loss: 0.00002573
Iteration 4/1000 | Loss: 0.00002094
Iteration 5/1000 | Loss: 0.00001972
Iteration 6/1000 | Loss: 0.00001895
Iteration 7/1000 | Loss: 0.00001852
Iteration 8/1000 | Loss: 0.00001807
Iteration 9/1000 | Loss: 0.00001787
Iteration 10/1000 | Loss: 0.00001770
Iteration 11/1000 | Loss: 0.00001765
Iteration 12/1000 | Loss: 0.00001759
Iteration 13/1000 | Loss: 0.00001756
Iteration 14/1000 | Loss: 0.00001751
Iteration 15/1000 | Loss: 0.00001742
Iteration 16/1000 | Loss: 0.00001742
Iteration 17/1000 | Loss: 0.00001740
Iteration 18/1000 | Loss: 0.00001736
Iteration 19/1000 | Loss: 0.00001735
Iteration 20/1000 | Loss: 0.00001734
Iteration 21/1000 | Loss: 0.00001734
Iteration 22/1000 | Loss: 0.00001733
Iteration 23/1000 | Loss: 0.00001732
Iteration 24/1000 | Loss: 0.00001732
Iteration 25/1000 | Loss: 0.00001725
Iteration 26/1000 | Loss: 0.00001724
Iteration 27/1000 | Loss: 0.00001722
Iteration 28/1000 | Loss: 0.00001722
Iteration 29/1000 | Loss: 0.00001720
Iteration 30/1000 | Loss: 0.00001720
Iteration 31/1000 | Loss: 0.00001719
Iteration 32/1000 | Loss: 0.00001719
Iteration 33/1000 | Loss: 0.00001719
Iteration 34/1000 | Loss: 0.00001719
Iteration 35/1000 | Loss: 0.00001719
Iteration 36/1000 | Loss: 0.00001718
Iteration 37/1000 | Loss: 0.00001717
Iteration 38/1000 | Loss: 0.00001716
Iteration 39/1000 | Loss: 0.00001716
Iteration 40/1000 | Loss: 0.00001716
Iteration 41/1000 | Loss: 0.00001716
Iteration 42/1000 | Loss: 0.00001716
Iteration 43/1000 | Loss: 0.00001716
Iteration 44/1000 | Loss: 0.00001716
Iteration 45/1000 | Loss: 0.00001715
Iteration 46/1000 | Loss: 0.00001715
Iteration 47/1000 | Loss: 0.00001714
Iteration 48/1000 | Loss: 0.00001714
Iteration 49/1000 | Loss: 0.00001713
Iteration 50/1000 | Loss: 0.00001713
Iteration 51/1000 | Loss: 0.00001712
Iteration 52/1000 | Loss: 0.00001712
Iteration 53/1000 | Loss: 0.00001712
Iteration 54/1000 | Loss: 0.00001712
Iteration 55/1000 | Loss: 0.00001712
Iteration 56/1000 | Loss: 0.00001711
Iteration 57/1000 | Loss: 0.00001711
Iteration 58/1000 | Loss: 0.00001711
Iteration 59/1000 | Loss: 0.00001711
Iteration 60/1000 | Loss: 0.00001710
Iteration 61/1000 | Loss: 0.00001710
Iteration 62/1000 | Loss: 0.00001710
Iteration 63/1000 | Loss: 0.00001710
Iteration 64/1000 | Loss: 0.00001710
Iteration 65/1000 | Loss: 0.00001709
Iteration 66/1000 | Loss: 0.00001709
Iteration 67/1000 | Loss: 0.00001709
Iteration 68/1000 | Loss: 0.00001709
Iteration 69/1000 | Loss: 0.00001709
Iteration 70/1000 | Loss: 0.00001709
Iteration 71/1000 | Loss: 0.00001709
Iteration 72/1000 | Loss: 0.00001708
Iteration 73/1000 | Loss: 0.00001708
Iteration 74/1000 | Loss: 0.00001708
Iteration 75/1000 | Loss: 0.00001708
Iteration 76/1000 | Loss: 0.00001708
Iteration 77/1000 | Loss: 0.00001708
Iteration 78/1000 | Loss: 0.00001708
Iteration 79/1000 | Loss: 0.00001708
Iteration 80/1000 | Loss: 0.00001708
Iteration 81/1000 | Loss: 0.00001707
Iteration 82/1000 | Loss: 0.00001707
Iteration 83/1000 | Loss: 0.00001707
Iteration 84/1000 | Loss: 0.00001707
Iteration 85/1000 | Loss: 0.00001707
Iteration 86/1000 | Loss: 0.00001707
Iteration 87/1000 | Loss: 0.00001707
Iteration 88/1000 | Loss: 0.00001707
Iteration 89/1000 | Loss: 0.00001707
Iteration 90/1000 | Loss: 0.00001707
Iteration 91/1000 | Loss: 0.00001706
Iteration 92/1000 | Loss: 0.00001706
Iteration 93/1000 | Loss: 0.00001706
Iteration 94/1000 | Loss: 0.00001706
Iteration 95/1000 | Loss: 0.00001706
Iteration 96/1000 | Loss: 0.00001706
Iteration 97/1000 | Loss: 0.00001705
Iteration 98/1000 | Loss: 0.00001705
Iteration 99/1000 | Loss: 0.00001705
Iteration 100/1000 | Loss: 0.00001705
Iteration 101/1000 | Loss: 0.00001705
Iteration 102/1000 | Loss: 0.00001705
Iteration 103/1000 | Loss: 0.00001705
Iteration 104/1000 | Loss: 0.00001705
Iteration 105/1000 | Loss: 0.00001705
Iteration 106/1000 | Loss: 0.00001705
Iteration 107/1000 | Loss: 0.00001705
Iteration 108/1000 | Loss: 0.00001704
Iteration 109/1000 | Loss: 0.00001704
Iteration 110/1000 | Loss: 0.00001704
Iteration 111/1000 | Loss: 0.00001704
Iteration 112/1000 | Loss: 0.00001704
Iteration 113/1000 | Loss: 0.00001704
Iteration 114/1000 | Loss: 0.00001704
Iteration 115/1000 | Loss: 0.00001704
Iteration 116/1000 | Loss: 0.00001704
Iteration 117/1000 | Loss: 0.00001704
Iteration 118/1000 | Loss: 0.00001704
Iteration 119/1000 | Loss: 0.00001704
Iteration 120/1000 | Loss: 0.00001703
Iteration 121/1000 | Loss: 0.00001703
Iteration 122/1000 | Loss: 0.00001703
Iteration 123/1000 | Loss: 0.00001703
Iteration 124/1000 | Loss: 0.00001703
Iteration 125/1000 | Loss: 0.00001703
Iteration 126/1000 | Loss: 0.00001702
Iteration 127/1000 | Loss: 0.00001702
Iteration 128/1000 | Loss: 0.00001702
Iteration 129/1000 | Loss: 0.00001702
Iteration 130/1000 | Loss: 0.00001702
Iteration 131/1000 | Loss: 0.00001702
Iteration 132/1000 | Loss: 0.00001702
Iteration 133/1000 | Loss: 0.00001702
Iteration 134/1000 | Loss: 0.00001702
Iteration 135/1000 | Loss: 0.00001702
Iteration 136/1000 | Loss: 0.00001702
Iteration 137/1000 | Loss: 0.00001702
Iteration 138/1000 | Loss: 0.00001702
Iteration 139/1000 | Loss: 0.00001702
Iteration 140/1000 | Loss: 0.00001702
Iteration 141/1000 | Loss: 0.00001702
Iteration 142/1000 | Loss: 0.00001702
Iteration 143/1000 | Loss: 0.00001701
Iteration 144/1000 | Loss: 0.00001701
Iteration 145/1000 | Loss: 0.00001701
Iteration 146/1000 | Loss: 0.00001701
Iteration 147/1000 | Loss: 0.00001701
Iteration 148/1000 | Loss: 0.00001701
Iteration 149/1000 | Loss: 0.00001701
Iteration 150/1000 | Loss: 0.00001701
Iteration 151/1000 | Loss: 0.00001701
Iteration 152/1000 | Loss: 0.00001701
Iteration 153/1000 | Loss: 0.00001701
Iteration 154/1000 | Loss: 0.00001701
Iteration 155/1000 | Loss: 0.00001701
Iteration 156/1000 | Loss: 0.00001701
Iteration 157/1000 | Loss: 0.00001701
Iteration 158/1000 | Loss: 0.00001701
Iteration 159/1000 | Loss: 0.00001701
Iteration 160/1000 | Loss: 0.00001701
Iteration 161/1000 | Loss: 0.00001701
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.700802386039868e-05, 1.700802386039868e-05, 1.700802386039868e-05, 1.700802386039868e-05, 1.700802386039868e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.700802386039868e-05

Optimization complete. Final v2v error: 3.457505702972412 mm

Highest mean error: 3.905238389968872 mm for frame 90

Lowest mean error: 2.9378979206085205 mm for frame 34

Saving results

Total time: 36.69170784950256
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_nl_6135/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_6135/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_6135/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01043993
Iteration 2/25 | Loss: 0.00246602
Iteration 3/25 | Loss: 0.00194641
Iteration 4/25 | Loss: 0.00174315
Iteration 5/25 | Loss: 0.00165532
Iteration 6/25 | Loss: 0.00148056
Iteration 7/25 | Loss: 0.00119373
Iteration 8/25 | Loss: 0.00096757
Iteration 9/25 | Loss: 0.00091484
Iteration 10/25 | Loss: 0.00089203
Iteration 11/25 | Loss: 0.00087605
Iteration 12/25 | Loss: 0.00088469
Iteration 13/25 | Loss: 0.00087591
Iteration 14/25 | Loss: 0.00087401
Iteration 15/25 | Loss: 0.00086256
Iteration 16/25 | Loss: 0.00085689
Iteration 17/25 | Loss: 0.00085951
Iteration 18/25 | Loss: 0.00085859
Iteration 19/25 | Loss: 0.00085972
Iteration 20/25 | Loss: 0.00085774
Iteration 21/25 | Loss: 0.00085485
Iteration 22/25 | Loss: 0.00085443
Iteration 23/25 | Loss: 0.00085412
Iteration 24/25 | Loss: 0.00085405
Iteration 25/25 | Loss: 0.00085405

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36785746
Iteration 2/25 | Loss: 0.00057733
Iteration 3/25 | Loss: 0.00057733
Iteration 4/25 | Loss: 0.00057733
Iteration 5/25 | Loss: 0.00057733
Iteration 6/25 | Loss: 0.00057733
Iteration 7/25 | Loss: 0.00057733
Iteration 8/25 | Loss: 0.00057733
Iteration 9/25 | Loss: 0.00057733
Iteration 10/25 | Loss: 0.00057733
Iteration 11/25 | Loss: 0.00057733
Iteration 12/25 | Loss: 0.00057733
Iteration 13/25 | Loss: 0.00057733
Iteration 14/25 | Loss: 0.00057733
Iteration 15/25 | Loss: 0.00057733
Iteration 16/25 | Loss: 0.00057733
Iteration 17/25 | Loss: 0.00057733
Iteration 18/25 | Loss: 0.00057733
Iteration 19/25 | Loss: 0.00057733
Iteration 20/25 | Loss: 0.00057733
Iteration 21/25 | Loss: 0.00057733
Iteration 22/25 | Loss: 0.00057733
Iteration 23/25 | Loss: 0.00057733
Iteration 24/25 | Loss: 0.00057733
Iteration 25/25 | Loss: 0.00057733

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057733
Iteration 2/1000 | Loss: 0.00027706
Iteration 3/1000 | Loss: 0.00014676
Iteration 4/1000 | Loss: 0.00001619
Iteration 5/1000 | Loss: 0.00001422
Iteration 6/1000 | Loss: 0.00001323
Iteration 7/1000 | Loss: 0.00001281
Iteration 8/1000 | Loss: 0.00001244
Iteration 9/1000 | Loss: 0.00001212
Iteration 10/1000 | Loss: 0.00001190
Iteration 11/1000 | Loss: 0.00048978
Iteration 12/1000 | Loss: 0.00013198
Iteration 13/1000 | Loss: 0.00003560
Iteration 14/1000 | Loss: 0.00001154
Iteration 15/1000 | Loss: 0.00001049
Iteration 16/1000 | Loss: 0.00000986
Iteration 17/1000 | Loss: 0.00000955
Iteration 18/1000 | Loss: 0.00000944
Iteration 19/1000 | Loss: 0.00000933
Iteration 20/1000 | Loss: 0.00000932
Iteration 21/1000 | Loss: 0.00000932
Iteration 22/1000 | Loss: 0.00000930
Iteration 23/1000 | Loss: 0.00000928
Iteration 24/1000 | Loss: 0.00000925
Iteration 25/1000 | Loss: 0.00000925
Iteration 26/1000 | Loss: 0.00000919
Iteration 27/1000 | Loss: 0.00000919
Iteration 28/1000 | Loss: 0.00000918
Iteration 29/1000 | Loss: 0.00000917
Iteration 30/1000 | Loss: 0.00000917
Iteration 31/1000 | Loss: 0.00000915
Iteration 32/1000 | Loss: 0.00000915
Iteration 33/1000 | Loss: 0.00000914
Iteration 34/1000 | Loss: 0.00000914
Iteration 35/1000 | Loss: 0.00000908
Iteration 36/1000 | Loss: 0.00000908
Iteration 37/1000 | Loss: 0.00000908
Iteration 38/1000 | Loss: 0.00000908
Iteration 39/1000 | Loss: 0.00000908
Iteration 40/1000 | Loss: 0.00000907
Iteration 41/1000 | Loss: 0.00000907
Iteration 42/1000 | Loss: 0.00000907
Iteration 43/1000 | Loss: 0.00000907
Iteration 44/1000 | Loss: 0.00000906
Iteration 45/1000 | Loss: 0.00000905
Iteration 46/1000 | Loss: 0.00000904
Iteration 47/1000 | Loss: 0.00000904
Iteration 48/1000 | Loss: 0.00000904
Iteration 49/1000 | Loss: 0.00000904
Iteration 50/1000 | Loss: 0.00000903
Iteration 51/1000 | Loss: 0.00000903
Iteration 52/1000 | Loss: 0.00000903
Iteration 53/1000 | Loss: 0.00000903
Iteration 54/1000 | Loss: 0.00000903
Iteration 55/1000 | Loss: 0.00000902
Iteration 56/1000 | Loss: 0.00000902
Iteration 57/1000 | Loss: 0.00000902
Iteration 58/1000 | Loss: 0.00000901
Iteration 59/1000 | Loss: 0.00000901
Iteration 60/1000 | Loss: 0.00000901
Iteration 61/1000 | Loss: 0.00000901
Iteration 62/1000 | Loss: 0.00000901
Iteration 63/1000 | Loss: 0.00000901
Iteration 64/1000 | Loss: 0.00000901
Iteration 65/1000 | Loss: 0.00000901
Iteration 66/1000 | Loss: 0.00000901
Iteration 67/1000 | Loss: 0.00000900
Iteration 68/1000 | Loss: 0.00000900
Iteration 69/1000 | Loss: 0.00000900
Iteration 70/1000 | Loss: 0.00000900
Iteration 71/1000 | Loss: 0.00000899
Iteration 72/1000 | Loss: 0.00000899
Iteration 73/1000 | Loss: 0.00000899
Iteration 74/1000 | Loss: 0.00000899
Iteration 75/1000 | Loss: 0.00000898
Iteration 76/1000 | Loss: 0.00000898
Iteration 77/1000 | Loss: 0.00000898
Iteration 78/1000 | Loss: 0.00000898
Iteration 79/1000 | Loss: 0.00000898
Iteration 80/1000 | Loss: 0.00000897
Iteration 81/1000 | Loss: 0.00000897
Iteration 82/1000 | Loss: 0.00000897
Iteration 83/1000 | Loss: 0.00000896
Iteration 84/1000 | Loss: 0.00000896
Iteration 85/1000 | Loss: 0.00000896
Iteration 86/1000 | Loss: 0.00000896
Iteration 87/1000 | Loss: 0.00000895
Iteration 88/1000 | Loss: 0.00000895
Iteration 89/1000 | Loss: 0.00000895
Iteration 90/1000 | Loss: 0.00000894
Iteration 91/1000 | Loss: 0.00000894
Iteration 92/1000 | Loss: 0.00000894
Iteration 93/1000 | Loss: 0.00000894
Iteration 94/1000 | Loss: 0.00000894
Iteration 95/1000 | Loss: 0.00000894
Iteration 96/1000 | Loss: 0.00000894
Iteration 97/1000 | Loss: 0.00000894
Iteration 98/1000 | Loss: 0.00000894
Iteration 99/1000 | Loss: 0.00000894
Iteration 100/1000 | Loss: 0.00000894
Iteration 101/1000 | Loss: 0.00000893
Iteration 102/1000 | Loss: 0.00000893
Iteration 103/1000 | Loss: 0.00000893
Iteration 104/1000 | Loss: 0.00000893
Iteration 105/1000 | Loss: 0.00000893
Iteration 106/1000 | Loss: 0.00000893
Iteration 107/1000 | Loss: 0.00000893
Iteration 108/1000 | Loss: 0.00000893
Iteration 109/1000 | Loss: 0.00000893
Iteration 110/1000 | Loss: 0.00000893
Iteration 111/1000 | Loss: 0.00000892
Iteration 112/1000 | Loss: 0.00000892
Iteration 113/1000 | Loss: 0.00000892
Iteration 114/1000 | Loss: 0.00000892
Iteration 115/1000 | Loss: 0.00000892
Iteration 116/1000 | Loss: 0.00000892
Iteration 117/1000 | Loss: 0.00000892
Iteration 118/1000 | Loss: 0.00000892
Iteration 119/1000 | Loss: 0.00000891
Iteration 120/1000 | Loss: 0.00000891
Iteration 121/1000 | Loss: 0.00000891
Iteration 122/1000 | Loss: 0.00000891
Iteration 123/1000 | Loss: 0.00000891
Iteration 124/1000 | Loss: 0.00000891
Iteration 125/1000 | Loss: 0.00000891
Iteration 126/1000 | Loss: 0.00000891
Iteration 127/1000 | Loss: 0.00000891
Iteration 128/1000 | Loss: 0.00000891
Iteration 129/1000 | Loss: 0.00000891
Iteration 130/1000 | Loss: 0.00000891
Iteration 131/1000 | Loss: 0.00000891
Iteration 132/1000 | Loss: 0.00000890
Iteration 133/1000 | Loss: 0.00000890
Iteration 134/1000 | Loss: 0.00000890
Iteration 135/1000 | Loss: 0.00000890
Iteration 136/1000 | Loss: 0.00000890
Iteration 137/1000 | Loss: 0.00000890
Iteration 138/1000 | Loss: 0.00000890
Iteration 139/1000 | Loss: 0.00000890
Iteration 140/1000 | Loss: 0.00000890
Iteration 141/1000 | Loss: 0.00000890
Iteration 142/1000 | Loss: 0.00000890
Iteration 143/1000 | Loss: 0.00000890
Iteration 144/1000 | Loss: 0.00000890
Iteration 145/1000 | Loss: 0.00000890
Iteration 146/1000 | Loss: 0.00000890
Iteration 147/1000 | Loss: 0.00000889
Iteration 148/1000 | Loss: 0.00000889
Iteration 149/1000 | Loss: 0.00000889
Iteration 150/1000 | Loss: 0.00000889
Iteration 151/1000 | Loss: 0.00000889
Iteration 152/1000 | Loss: 0.00000889
Iteration 153/1000 | Loss: 0.00000889
Iteration 154/1000 | Loss: 0.00000889
Iteration 155/1000 | Loss: 0.00000889
Iteration 156/1000 | Loss: 0.00000889
Iteration 157/1000 | Loss: 0.00000889
Iteration 158/1000 | Loss: 0.00000889
Iteration 159/1000 | Loss: 0.00000889
Iteration 160/1000 | Loss: 0.00000889
Iteration 161/1000 | Loss: 0.00000889
Iteration 162/1000 | Loss: 0.00000889
Iteration 163/1000 | Loss: 0.00000889
Iteration 164/1000 | Loss: 0.00000889
Iteration 165/1000 | Loss: 0.00000889
Iteration 166/1000 | Loss: 0.00000889
Iteration 167/1000 | Loss: 0.00000889
Iteration 168/1000 | Loss: 0.00000889
Iteration 169/1000 | Loss: 0.00000889
Iteration 170/1000 | Loss: 0.00000889
Iteration 171/1000 | Loss: 0.00000889
Iteration 172/1000 | Loss: 0.00000889
Iteration 173/1000 | Loss: 0.00000889
Iteration 174/1000 | Loss: 0.00000889
Iteration 175/1000 | Loss: 0.00000889
Iteration 176/1000 | Loss: 0.00000889
Iteration 177/1000 | Loss: 0.00000889
Iteration 178/1000 | Loss: 0.00000889
Iteration 179/1000 | Loss: 0.00000889
Iteration 180/1000 | Loss: 0.00000889
Iteration 181/1000 | Loss: 0.00000889
Iteration 182/1000 | Loss: 0.00000889
Iteration 183/1000 | Loss: 0.00000889
Iteration 184/1000 | Loss: 0.00000889
Iteration 185/1000 | Loss: 0.00000889
Iteration 186/1000 | Loss: 0.00000889
Iteration 187/1000 | Loss: 0.00000889
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [8.888029697118327e-06, 8.888029697118327e-06, 8.888029697118327e-06, 8.888029697118327e-06, 8.888029697118327e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.888029697118327e-06

Optimization complete. Final v2v error: 2.424553394317627 mm

Highest mean error: 4.7081098556518555 mm for frame 49

Lowest mean error: 2.085728406906128 mm for frame 48

Saving results

Total time: 79.76290607452393
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_nl_6135/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_6135/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_6135/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00819670
Iteration 2/25 | Loss: 0.00126258
Iteration 3/25 | Loss: 0.00099027
Iteration 4/25 | Loss: 0.00095479
Iteration 5/25 | Loss: 0.00094873
Iteration 6/25 | Loss: 0.00094536
Iteration 7/25 | Loss: 0.00094480
Iteration 8/25 | Loss: 0.00094467
Iteration 9/25 | Loss: 0.00094465
Iteration 10/25 | Loss: 0.00094465
Iteration 11/25 | Loss: 0.00094465
Iteration 12/25 | Loss: 0.00094465
Iteration 13/25 | Loss: 0.00094465
Iteration 14/25 | Loss: 0.00094465
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0009446502663195133, 0.0009446502663195133, 0.0009446502663195133, 0.0009446502663195133, 0.0009446502663195133]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009446502663195133

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.11402702
Iteration 2/25 | Loss: 0.00078376
Iteration 3/25 | Loss: 0.00078373
Iteration 4/25 | Loss: 0.00078373
Iteration 5/25 | Loss: 0.00078373
Iteration 6/25 | Loss: 0.00078373
Iteration 7/25 | Loss: 0.00078373
Iteration 8/25 | Loss: 0.00078373
Iteration 9/25 | Loss: 0.00078373
Iteration 10/25 | Loss: 0.00078373
Iteration 11/25 | Loss: 0.00078373
Iteration 12/25 | Loss: 0.00078373
Iteration 13/25 | Loss: 0.00078373
Iteration 14/25 | Loss: 0.00078373
Iteration 15/25 | Loss: 0.00078373
Iteration 16/25 | Loss: 0.00078373
Iteration 17/25 | Loss: 0.00078373
Iteration 18/25 | Loss: 0.00078373
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007837305311113596, 0.0007837305311113596, 0.0007837305311113596, 0.0007837305311113596, 0.0007837305311113596]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007837305311113596

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078373
Iteration 2/1000 | Loss: 0.00002619
Iteration 3/1000 | Loss: 0.00002546
Iteration 4/1000 | Loss: 0.00001839
Iteration 5/1000 | Loss: 0.00002607
Iteration 6/1000 | Loss: 0.00004988
Iteration 7/1000 | Loss: 0.00003093
Iteration 8/1000 | Loss: 0.00001416
Iteration 9/1000 | Loss: 0.00001381
Iteration 10/1000 | Loss: 0.00001356
Iteration 11/1000 | Loss: 0.00001337
Iteration 12/1000 | Loss: 0.00001328
Iteration 13/1000 | Loss: 0.00001322
Iteration 14/1000 | Loss: 0.00001322
Iteration 15/1000 | Loss: 0.00001321
Iteration 16/1000 | Loss: 0.00001321
Iteration 17/1000 | Loss: 0.00001320
Iteration 18/1000 | Loss: 0.00001317
Iteration 19/1000 | Loss: 0.00001316
Iteration 20/1000 | Loss: 0.00002393
Iteration 21/1000 | Loss: 0.00001314
Iteration 22/1000 | Loss: 0.00001312
Iteration 23/1000 | Loss: 0.00001311
Iteration 24/1000 | Loss: 0.00001311
Iteration 25/1000 | Loss: 0.00001310
Iteration 26/1000 | Loss: 0.00001310
Iteration 27/1000 | Loss: 0.00001309
Iteration 28/1000 | Loss: 0.00001309
Iteration 29/1000 | Loss: 0.00002945
Iteration 30/1000 | Loss: 0.00001650
Iteration 31/1000 | Loss: 0.00001348
Iteration 32/1000 | Loss: 0.00001307
Iteration 33/1000 | Loss: 0.00001307
Iteration 34/1000 | Loss: 0.00001307
Iteration 35/1000 | Loss: 0.00001307
Iteration 36/1000 | Loss: 0.00001307
Iteration 37/1000 | Loss: 0.00001307
Iteration 38/1000 | Loss: 0.00001306
Iteration 39/1000 | Loss: 0.00001305
Iteration 40/1000 | Loss: 0.00001305
Iteration 41/1000 | Loss: 0.00001302
Iteration 42/1000 | Loss: 0.00001302
Iteration 43/1000 | Loss: 0.00001302
Iteration 44/1000 | Loss: 0.00001302
Iteration 45/1000 | Loss: 0.00001302
Iteration 46/1000 | Loss: 0.00001302
Iteration 47/1000 | Loss: 0.00001301
Iteration 48/1000 | Loss: 0.00001301
Iteration 49/1000 | Loss: 0.00001298
Iteration 50/1000 | Loss: 0.00001298
Iteration 51/1000 | Loss: 0.00001297
Iteration 52/1000 | Loss: 0.00001297
Iteration 53/1000 | Loss: 0.00001296
Iteration 54/1000 | Loss: 0.00001296
Iteration 55/1000 | Loss: 0.00001295
Iteration 56/1000 | Loss: 0.00001295
Iteration 57/1000 | Loss: 0.00001294
Iteration 58/1000 | Loss: 0.00001293
Iteration 59/1000 | Loss: 0.00001293
Iteration 60/1000 | Loss: 0.00001293
Iteration 61/1000 | Loss: 0.00001292
Iteration 62/1000 | Loss: 0.00001292
Iteration 63/1000 | Loss: 0.00001291
Iteration 64/1000 | Loss: 0.00001291
Iteration 65/1000 | Loss: 0.00001291
Iteration 66/1000 | Loss: 0.00001290
Iteration 67/1000 | Loss: 0.00001290
Iteration 68/1000 | Loss: 0.00002460
Iteration 69/1000 | Loss: 0.00001598
Iteration 70/1000 | Loss: 0.00001290
Iteration 71/1000 | Loss: 0.00001289
Iteration 72/1000 | Loss: 0.00001289
Iteration 73/1000 | Loss: 0.00001289
Iteration 74/1000 | Loss: 0.00001289
Iteration 75/1000 | Loss: 0.00001289
Iteration 76/1000 | Loss: 0.00001289
Iteration 77/1000 | Loss: 0.00001289
Iteration 78/1000 | Loss: 0.00001289
Iteration 79/1000 | Loss: 0.00001289
Iteration 80/1000 | Loss: 0.00001289
Iteration 81/1000 | Loss: 0.00001289
Iteration 82/1000 | Loss: 0.00001288
Iteration 83/1000 | Loss: 0.00001288
Iteration 84/1000 | Loss: 0.00001288
Iteration 85/1000 | Loss: 0.00001288
Iteration 86/1000 | Loss: 0.00001288
Iteration 87/1000 | Loss: 0.00001288
Iteration 88/1000 | Loss: 0.00001288
Iteration 89/1000 | Loss: 0.00001288
Iteration 90/1000 | Loss: 0.00001288
Iteration 91/1000 | Loss: 0.00001288
Iteration 92/1000 | Loss: 0.00001287
Iteration 93/1000 | Loss: 0.00001287
Iteration 94/1000 | Loss: 0.00001287
Iteration 95/1000 | Loss: 0.00001287
Iteration 96/1000 | Loss: 0.00001287
Iteration 97/1000 | Loss: 0.00001287
Iteration 98/1000 | Loss: 0.00001287
Iteration 99/1000 | Loss: 0.00001287
Iteration 100/1000 | Loss: 0.00001287
Iteration 101/1000 | Loss: 0.00001287
Iteration 102/1000 | Loss: 0.00001287
Iteration 103/1000 | Loss: 0.00001287
Iteration 104/1000 | Loss: 0.00001287
Iteration 105/1000 | Loss: 0.00001287
Iteration 106/1000 | Loss: 0.00001287
Iteration 107/1000 | Loss: 0.00001287
Iteration 108/1000 | Loss: 0.00001287
Iteration 109/1000 | Loss: 0.00001287
Iteration 110/1000 | Loss: 0.00001287
Iteration 111/1000 | Loss: 0.00001287
Iteration 112/1000 | Loss: 0.00001287
Iteration 113/1000 | Loss: 0.00001287
Iteration 114/1000 | Loss: 0.00001287
Iteration 115/1000 | Loss: 0.00001287
Iteration 116/1000 | Loss: 0.00001287
Iteration 117/1000 | Loss: 0.00001287
Iteration 118/1000 | Loss: 0.00001287
Iteration 119/1000 | Loss: 0.00001287
Iteration 120/1000 | Loss: 0.00001287
Iteration 121/1000 | Loss: 0.00001287
Iteration 122/1000 | Loss: 0.00001287
Iteration 123/1000 | Loss: 0.00001287
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [1.2865959433838725e-05, 1.2865959433838725e-05, 1.2865959433838725e-05, 1.2865959433838725e-05, 1.2865959433838725e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2865959433838725e-05

Optimization complete. Final v2v error: 3.0169761180877686 mm

Highest mean error: 3.4183568954467773 mm for frame 14

Lowest mean error: 2.662874460220337 mm for frame 191

Saving results

Total time: 51.46304416656494
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_nl_6135/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_6135/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_6135/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00387204
Iteration 2/25 | Loss: 0.00108669
Iteration 3/25 | Loss: 0.00092227
Iteration 4/25 | Loss: 0.00090340
Iteration 5/25 | Loss: 0.00089794
Iteration 6/25 | Loss: 0.00089584
Iteration 7/25 | Loss: 0.00089544
Iteration 8/25 | Loss: 0.00089538
Iteration 9/25 | Loss: 0.00089538
Iteration 10/25 | Loss: 0.00089538
Iteration 11/25 | Loss: 0.00089538
Iteration 12/25 | Loss: 0.00089538
Iteration 13/25 | Loss: 0.00089538
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008953846408985555, 0.0008953846408985555, 0.0008953846408985555, 0.0008953846408985555, 0.0008953846408985555]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008953846408985555

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32043099
Iteration 2/25 | Loss: 0.00076207
Iteration 3/25 | Loss: 0.00076205
Iteration 4/25 | Loss: 0.00076205
Iteration 5/25 | Loss: 0.00076205
Iteration 6/25 | Loss: 0.00076205
Iteration 7/25 | Loss: 0.00076205
Iteration 8/25 | Loss: 0.00076205
Iteration 9/25 | Loss: 0.00076205
Iteration 10/25 | Loss: 0.00076205
Iteration 11/25 | Loss: 0.00076205
Iteration 12/25 | Loss: 0.00076205
Iteration 13/25 | Loss: 0.00076205
Iteration 14/25 | Loss: 0.00076205
Iteration 15/25 | Loss: 0.00076205
Iteration 16/25 | Loss: 0.00076205
Iteration 17/25 | Loss: 0.00076205
Iteration 18/25 | Loss: 0.00076205
Iteration 19/25 | Loss: 0.00076205
Iteration 20/25 | Loss: 0.00076205
Iteration 21/25 | Loss: 0.00076205
Iteration 22/25 | Loss: 0.00076205
Iteration 23/25 | Loss: 0.00076205
Iteration 24/25 | Loss: 0.00076205
Iteration 25/25 | Loss: 0.00076205

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076205
Iteration 2/1000 | Loss: 0.00003669
Iteration 3/1000 | Loss: 0.00002260
Iteration 4/1000 | Loss: 0.00001806
Iteration 5/1000 | Loss: 0.00001680
Iteration 6/1000 | Loss: 0.00001601
Iteration 7/1000 | Loss: 0.00001537
Iteration 8/1000 | Loss: 0.00001503
Iteration 9/1000 | Loss: 0.00001461
Iteration 10/1000 | Loss: 0.00001436
Iteration 11/1000 | Loss: 0.00001434
Iteration 12/1000 | Loss: 0.00001434
Iteration 13/1000 | Loss: 0.00001433
Iteration 14/1000 | Loss: 0.00001431
Iteration 15/1000 | Loss: 0.00001431
Iteration 16/1000 | Loss: 0.00001430
Iteration 17/1000 | Loss: 0.00001426
Iteration 18/1000 | Loss: 0.00001417
Iteration 19/1000 | Loss: 0.00001415
Iteration 20/1000 | Loss: 0.00001415
Iteration 21/1000 | Loss: 0.00001415
Iteration 22/1000 | Loss: 0.00001415
Iteration 23/1000 | Loss: 0.00001415
Iteration 24/1000 | Loss: 0.00001415
Iteration 25/1000 | Loss: 0.00001414
Iteration 26/1000 | Loss: 0.00001410
Iteration 27/1000 | Loss: 0.00001409
Iteration 28/1000 | Loss: 0.00001408
Iteration 29/1000 | Loss: 0.00001408
Iteration 30/1000 | Loss: 0.00001408
Iteration 31/1000 | Loss: 0.00001408
Iteration 32/1000 | Loss: 0.00001407
Iteration 33/1000 | Loss: 0.00001407
Iteration 34/1000 | Loss: 0.00001407
Iteration 35/1000 | Loss: 0.00001407
Iteration 36/1000 | Loss: 0.00001407
Iteration 37/1000 | Loss: 0.00001405
Iteration 38/1000 | Loss: 0.00001405
Iteration 39/1000 | Loss: 0.00001405
Iteration 40/1000 | Loss: 0.00001404
Iteration 41/1000 | Loss: 0.00001403
Iteration 42/1000 | Loss: 0.00001403
Iteration 43/1000 | Loss: 0.00001402
Iteration 44/1000 | Loss: 0.00001402
Iteration 45/1000 | Loss: 0.00001402
Iteration 46/1000 | Loss: 0.00001402
Iteration 47/1000 | Loss: 0.00001402
Iteration 48/1000 | Loss: 0.00001402
Iteration 49/1000 | Loss: 0.00001402
Iteration 50/1000 | Loss: 0.00001402
Iteration 51/1000 | Loss: 0.00001402
Iteration 52/1000 | Loss: 0.00001402
Iteration 53/1000 | Loss: 0.00001402
Iteration 54/1000 | Loss: 0.00001402
Iteration 55/1000 | Loss: 0.00001401
Iteration 56/1000 | Loss: 0.00001401
Iteration 57/1000 | Loss: 0.00001401
Iteration 58/1000 | Loss: 0.00001401
Iteration 59/1000 | Loss: 0.00001401
Iteration 60/1000 | Loss: 0.00001401
Iteration 61/1000 | Loss: 0.00001400
Iteration 62/1000 | Loss: 0.00001400
Iteration 63/1000 | Loss: 0.00001399
Iteration 64/1000 | Loss: 0.00001399
Iteration 65/1000 | Loss: 0.00001398
Iteration 66/1000 | Loss: 0.00001398
Iteration 67/1000 | Loss: 0.00001398
Iteration 68/1000 | Loss: 0.00001397
Iteration 69/1000 | Loss: 0.00001397
Iteration 70/1000 | Loss: 0.00001396
Iteration 71/1000 | Loss: 0.00001396
Iteration 72/1000 | Loss: 0.00001396
Iteration 73/1000 | Loss: 0.00001395
Iteration 74/1000 | Loss: 0.00001395
Iteration 75/1000 | Loss: 0.00001394
Iteration 76/1000 | Loss: 0.00001394
Iteration 77/1000 | Loss: 0.00001394
Iteration 78/1000 | Loss: 0.00001394
Iteration 79/1000 | Loss: 0.00001393
Iteration 80/1000 | Loss: 0.00001393
Iteration 81/1000 | Loss: 0.00001393
Iteration 82/1000 | Loss: 0.00001393
Iteration 83/1000 | Loss: 0.00001392
Iteration 84/1000 | Loss: 0.00001392
Iteration 85/1000 | Loss: 0.00001392
Iteration 86/1000 | Loss: 0.00001392
Iteration 87/1000 | Loss: 0.00001392
Iteration 88/1000 | Loss: 0.00001391
Iteration 89/1000 | Loss: 0.00001391
Iteration 90/1000 | Loss: 0.00001391
Iteration 91/1000 | Loss: 0.00001391
Iteration 92/1000 | Loss: 0.00001391
Iteration 93/1000 | Loss: 0.00001390
Iteration 94/1000 | Loss: 0.00001390
Iteration 95/1000 | Loss: 0.00001390
Iteration 96/1000 | Loss: 0.00001390
Iteration 97/1000 | Loss: 0.00001389
Iteration 98/1000 | Loss: 0.00001388
Iteration 99/1000 | Loss: 0.00001388
Iteration 100/1000 | Loss: 0.00001387
Iteration 101/1000 | Loss: 0.00001387
Iteration 102/1000 | Loss: 0.00001387
Iteration 103/1000 | Loss: 0.00001387
Iteration 104/1000 | Loss: 0.00001387
Iteration 105/1000 | Loss: 0.00001386
Iteration 106/1000 | Loss: 0.00001386
Iteration 107/1000 | Loss: 0.00001386
Iteration 108/1000 | Loss: 0.00001385
Iteration 109/1000 | Loss: 0.00001385
Iteration 110/1000 | Loss: 0.00001385
Iteration 111/1000 | Loss: 0.00001384
Iteration 112/1000 | Loss: 0.00001384
Iteration 113/1000 | Loss: 0.00001384
Iteration 114/1000 | Loss: 0.00001383
Iteration 115/1000 | Loss: 0.00001383
Iteration 116/1000 | Loss: 0.00001383
Iteration 117/1000 | Loss: 0.00001383
Iteration 118/1000 | Loss: 0.00001383
Iteration 119/1000 | Loss: 0.00001382
Iteration 120/1000 | Loss: 0.00001382
Iteration 121/1000 | Loss: 0.00001382
Iteration 122/1000 | Loss: 0.00001382
Iteration 123/1000 | Loss: 0.00001382
Iteration 124/1000 | Loss: 0.00001381
Iteration 125/1000 | Loss: 0.00001381
Iteration 126/1000 | Loss: 0.00001381
Iteration 127/1000 | Loss: 0.00001381
Iteration 128/1000 | Loss: 0.00001381
Iteration 129/1000 | Loss: 0.00001381
Iteration 130/1000 | Loss: 0.00001380
Iteration 131/1000 | Loss: 0.00001380
Iteration 132/1000 | Loss: 0.00001380
Iteration 133/1000 | Loss: 0.00001380
Iteration 134/1000 | Loss: 0.00001380
Iteration 135/1000 | Loss: 0.00001380
Iteration 136/1000 | Loss: 0.00001379
Iteration 137/1000 | Loss: 0.00001379
Iteration 138/1000 | Loss: 0.00001379
Iteration 139/1000 | Loss: 0.00001379
Iteration 140/1000 | Loss: 0.00001379
Iteration 141/1000 | Loss: 0.00001379
Iteration 142/1000 | Loss: 0.00001379
Iteration 143/1000 | Loss: 0.00001379
Iteration 144/1000 | Loss: 0.00001379
Iteration 145/1000 | Loss: 0.00001379
Iteration 146/1000 | Loss: 0.00001378
Iteration 147/1000 | Loss: 0.00001378
Iteration 148/1000 | Loss: 0.00001378
Iteration 149/1000 | Loss: 0.00001378
Iteration 150/1000 | Loss: 0.00001378
Iteration 151/1000 | Loss: 0.00001378
Iteration 152/1000 | Loss: 0.00001378
Iteration 153/1000 | Loss: 0.00001378
Iteration 154/1000 | Loss: 0.00001378
Iteration 155/1000 | Loss: 0.00001377
Iteration 156/1000 | Loss: 0.00001377
Iteration 157/1000 | Loss: 0.00001377
Iteration 158/1000 | Loss: 0.00001377
Iteration 159/1000 | Loss: 0.00001377
Iteration 160/1000 | Loss: 0.00001377
Iteration 161/1000 | Loss: 0.00001377
Iteration 162/1000 | Loss: 0.00001377
Iteration 163/1000 | Loss: 0.00001377
Iteration 164/1000 | Loss: 0.00001377
Iteration 165/1000 | Loss: 0.00001376
Iteration 166/1000 | Loss: 0.00001376
Iteration 167/1000 | Loss: 0.00001376
Iteration 168/1000 | Loss: 0.00001376
Iteration 169/1000 | Loss: 0.00001376
Iteration 170/1000 | Loss: 0.00001376
Iteration 171/1000 | Loss: 0.00001376
Iteration 172/1000 | Loss: 0.00001376
Iteration 173/1000 | Loss: 0.00001376
Iteration 174/1000 | Loss: 0.00001376
Iteration 175/1000 | Loss: 0.00001376
Iteration 176/1000 | Loss: 0.00001376
Iteration 177/1000 | Loss: 0.00001376
Iteration 178/1000 | Loss: 0.00001376
Iteration 179/1000 | Loss: 0.00001376
Iteration 180/1000 | Loss: 0.00001376
Iteration 181/1000 | Loss: 0.00001376
Iteration 182/1000 | Loss: 0.00001376
Iteration 183/1000 | Loss: 0.00001376
Iteration 184/1000 | Loss: 0.00001376
Iteration 185/1000 | Loss: 0.00001375
Iteration 186/1000 | Loss: 0.00001375
Iteration 187/1000 | Loss: 0.00001375
Iteration 188/1000 | Loss: 0.00001375
Iteration 189/1000 | Loss: 0.00001375
Iteration 190/1000 | Loss: 0.00001375
Iteration 191/1000 | Loss: 0.00001375
Iteration 192/1000 | Loss: 0.00001375
Iteration 193/1000 | Loss: 0.00001375
Iteration 194/1000 | Loss: 0.00001375
Iteration 195/1000 | Loss: 0.00001375
Iteration 196/1000 | Loss: 0.00001375
Iteration 197/1000 | Loss: 0.00001375
Iteration 198/1000 | Loss: 0.00001375
Iteration 199/1000 | Loss: 0.00001375
Iteration 200/1000 | Loss: 0.00001374
Iteration 201/1000 | Loss: 0.00001374
Iteration 202/1000 | Loss: 0.00001374
Iteration 203/1000 | Loss: 0.00001374
Iteration 204/1000 | Loss: 0.00001374
Iteration 205/1000 | Loss: 0.00001374
Iteration 206/1000 | Loss: 0.00001374
Iteration 207/1000 | Loss: 0.00001374
Iteration 208/1000 | Loss: 0.00001374
Iteration 209/1000 | Loss: 0.00001374
Iteration 210/1000 | Loss: 0.00001374
Iteration 211/1000 | Loss: 0.00001374
Iteration 212/1000 | Loss: 0.00001374
Iteration 213/1000 | Loss: 0.00001374
Iteration 214/1000 | Loss: 0.00001374
Iteration 215/1000 | Loss: 0.00001374
Iteration 216/1000 | Loss: 0.00001374
Iteration 217/1000 | Loss: 0.00001374
Iteration 218/1000 | Loss: 0.00001374
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 218. Stopping optimization.
Last 5 losses: [1.3737957488046959e-05, 1.3737957488046959e-05, 1.3737957488046959e-05, 1.3737957488046959e-05, 1.3737957488046959e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3737957488046959e-05

Optimization complete. Final v2v error: 2.7695841789245605 mm

Highest mean error: 5.330516338348389 mm for frame 87

Lowest mean error: 1.7591043710708618 mm for frame 1

Saving results

Total time: 41.69202256202698
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_nl_6135/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_6135/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_6135/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00759384
Iteration 2/25 | Loss: 0.00102832
Iteration 3/25 | Loss: 0.00089915
Iteration 4/25 | Loss: 0.00087883
Iteration 5/25 | Loss: 0.00087592
Iteration 6/25 | Loss: 0.00087592
Iteration 7/25 | Loss: 0.00087592
Iteration 8/25 | Loss: 0.00087592
Iteration 9/25 | Loss: 0.00087592
Iteration 10/25 | Loss: 0.00087592
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0008759201155044138, 0.0008759201155044138, 0.0008759201155044138, 0.0008759201155044138, 0.0008759201155044138]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008759201155044138

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32338929
Iteration 2/25 | Loss: 0.00041415
Iteration 3/25 | Loss: 0.00041411
Iteration 4/25 | Loss: 0.00041411
Iteration 5/25 | Loss: 0.00041411
Iteration 6/25 | Loss: 0.00041411
Iteration 7/25 | Loss: 0.00041411
Iteration 8/25 | Loss: 0.00041411
Iteration 9/25 | Loss: 0.00041411
Iteration 10/25 | Loss: 0.00041411
Iteration 11/25 | Loss: 0.00041411
Iteration 12/25 | Loss: 0.00041411
Iteration 13/25 | Loss: 0.00041411
Iteration 14/25 | Loss: 0.00041411
Iteration 15/25 | Loss: 0.00041411
Iteration 16/25 | Loss: 0.00041411
Iteration 17/25 | Loss: 0.00041411
Iteration 18/25 | Loss: 0.00041411
Iteration 19/25 | Loss: 0.00041411
Iteration 20/25 | Loss: 0.00041411
Iteration 21/25 | Loss: 0.00041411
Iteration 22/25 | Loss: 0.00041411
Iteration 23/25 | Loss: 0.00041411
Iteration 24/25 | Loss: 0.00041411
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.00041411007987335324, 0.00041411007987335324, 0.00041411007987335324, 0.00041411007987335324, 0.00041411007987335324]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00041411007987335324

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041411
Iteration 2/1000 | Loss: 0.00002351
Iteration 3/1000 | Loss: 0.00001920
Iteration 4/1000 | Loss: 0.00001720
Iteration 5/1000 | Loss: 0.00001570
Iteration 6/1000 | Loss: 0.00001468
Iteration 7/1000 | Loss: 0.00001399
Iteration 8/1000 | Loss: 0.00001351
Iteration 9/1000 | Loss: 0.00001322
Iteration 10/1000 | Loss: 0.00001303
Iteration 11/1000 | Loss: 0.00001302
Iteration 12/1000 | Loss: 0.00001278
Iteration 13/1000 | Loss: 0.00001265
Iteration 14/1000 | Loss: 0.00001259
Iteration 15/1000 | Loss: 0.00001258
Iteration 16/1000 | Loss: 0.00001256
Iteration 17/1000 | Loss: 0.00001254
Iteration 18/1000 | Loss: 0.00001253
Iteration 19/1000 | Loss: 0.00001253
Iteration 20/1000 | Loss: 0.00001253
Iteration 21/1000 | Loss: 0.00001253
Iteration 22/1000 | Loss: 0.00001253
Iteration 23/1000 | Loss: 0.00001253
Iteration 24/1000 | Loss: 0.00001253
Iteration 25/1000 | Loss: 0.00001252
Iteration 26/1000 | Loss: 0.00001252
Iteration 27/1000 | Loss: 0.00001252
Iteration 28/1000 | Loss: 0.00001251
Iteration 29/1000 | Loss: 0.00001251
Iteration 30/1000 | Loss: 0.00001250
Iteration 31/1000 | Loss: 0.00001250
Iteration 32/1000 | Loss: 0.00001248
Iteration 33/1000 | Loss: 0.00001248
Iteration 34/1000 | Loss: 0.00001248
Iteration 35/1000 | Loss: 0.00001248
Iteration 36/1000 | Loss: 0.00001248
Iteration 37/1000 | Loss: 0.00001248
Iteration 38/1000 | Loss: 0.00001247
Iteration 39/1000 | Loss: 0.00001247
Iteration 40/1000 | Loss: 0.00001247
Iteration 41/1000 | Loss: 0.00001247
Iteration 42/1000 | Loss: 0.00001247
Iteration 43/1000 | Loss: 0.00001247
Iteration 44/1000 | Loss: 0.00001247
Iteration 45/1000 | Loss: 0.00001246
Iteration 46/1000 | Loss: 0.00001246
Iteration 47/1000 | Loss: 0.00001246
Iteration 48/1000 | Loss: 0.00001245
Iteration 49/1000 | Loss: 0.00001245
Iteration 50/1000 | Loss: 0.00001244
Iteration 51/1000 | Loss: 0.00001244
Iteration 52/1000 | Loss: 0.00001244
Iteration 53/1000 | Loss: 0.00001244
Iteration 54/1000 | Loss: 0.00001243
Iteration 55/1000 | Loss: 0.00001243
Iteration 56/1000 | Loss: 0.00001242
Iteration 57/1000 | Loss: 0.00001241
Iteration 58/1000 | Loss: 0.00001241
Iteration 59/1000 | Loss: 0.00001241
Iteration 60/1000 | Loss: 0.00001241
Iteration 61/1000 | Loss: 0.00001241
Iteration 62/1000 | Loss: 0.00001240
Iteration 63/1000 | Loss: 0.00001240
Iteration 64/1000 | Loss: 0.00001240
Iteration 65/1000 | Loss: 0.00001240
Iteration 66/1000 | Loss: 0.00001240
Iteration 67/1000 | Loss: 0.00001240
Iteration 68/1000 | Loss: 0.00001240
Iteration 69/1000 | Loss: 0.00001240
Iteration 70/1000 | Loss: 0.00001240
Iteration 71/1000 | Loss: 0.00001240
Iteration 72/1000 | Loss: 0.00001240
Iteration 73/1000 | Loss: 0.00001239
Iteration 74/1000 | Loss: 0.00001239
Iteration 75/1000 | Loss: 0.00001239
Iteration 76/1000 | Loss: 0.00001239
Iteration 77/1000 | Loss: 0.00001239
Iteration 78/1000 | Loss: 0.00001239
Iteration 79/1000 | Loss: 0.00001239
Iteration 80/1000 | Loss: 0.00001239
Iteration 81/1000 | Loss: 0.00001239
Iteration 82/1000 | Loss: 0.00001239
Iteration 83/1000 | Loss: 0.00001239
Iteration 84/1000 | Loss: 0.00001239
Iteration 85/1000 | Loss: 0.00001239
Iteration 86/1000 | Loss: 0.00001238
Iteration 87/1000 | Loss: 0.00001238
Iteration 88/1000 | Loss: 0.00001238
Iteration 89/1000 | Loss: 0.00001238
Iteration 90/1000 | Loss: 0.00001238
Iteration 91/1000 | Loss: 0.00001238
Iteration 92/1000 | Loss: 0.00001238
Iteration 93/1000 | Loss: 0.00001238
Iteration 94/1000 | Loss: 0.00001238
Iteration 95/1000 | Loss: 0.00001238
Iteration 96/1000 | Loss: 0.00001238
Iteration 97/1000 | Loss: 0.00001238
Iteration 98/1000 | Loss: 0.00001238
Iteration 99/1000 | Loss: 0.00001238
Iteration 100/1000 | Loss: 0.00001238
Iteration 101/1000 | Loss: 0.00001238
Iteration 102/1000 | Loss: 0.00001238
Iteration 103/1000 | Loss: 0.00001238
Iteration 104/1000 | Loss: 0.00001238
Iteration 105/1000 | Loss: 0.00001238
Iteration 106/1000 | Loss: 0.00001238
Iteration 107/1000 | Loss: 0.00001238
Iteration 108/1000 | Loss: 0.00001238
Iteration 109/1000 | Loss: 0.00001238
Iteration 110/1000 | Loss: 0.00001238
Iteration 111/1000 | Loss: 0.00001238
Iteration 112/1000 | Loss: 0.00001238
Iteration 113/1000 | Loss: 0.00001238
Iteration 114/1000 | Loss: 0.00001238
Iteration 115/1000 | Loss: 0.00001238
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [1.2383256944303866e-05, 1.2383256944303866e-05, 1.2383256944303866e-05, 1.2383256944303866e-05, 1.2383256944303866e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2383256944303866e-05

Optimization complete. Final v2v error: 3.004380702972412 mm

Highest mean error: 3.209641456604004 mm for frame 211

Lowest mean error: 2.8095786571502686 mm for frame 32

Saving results

Total time: 35.11744785308838
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_nl_6135/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_6135/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_6135/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00790341
Iteration 2/25 | Loss: 0.00177968
Iteration 3/25 | Loss: 0.00112722
Iteration 4/25 | Loss: 0.00102960
Iteration 5/25 | Loss: 0.00102041
Iteration 6/25 | Loss: 0.00101559
Iteration 7/25 | Loss: 0.00100562
Iteration 8/25 | Loss: 0.00099874
Iteration 9/25 | Loss: 0.00099589
Iteration 10/25 | Loss: 0.00099925
Iteration 11/25 | Loss: 0.00099666
Iteration 12/25 | Loss: 0.00099455
Iteration 13/25 | Loss: 0.00099374
Iteration 14/25 | Loss: 0.00099356
Iteration 15/25 | Loss: 0.00099733
Iteration 16/25 | Loss: 0.00099364
Iteration 17/25 | Loss: 0.00099231
Iteration 18/25 | Loss: 0.00099211
Iteration 19/25 | Loss: 0.00099208
Iteration 20/25 | Loss: 0.00099208
Iteration 21/25 | Loss: 0.00099208
Iteration 22/25 | Loss: 0.00099208
Iteration 23/25 | Loss: 0.00099208
Iteration 24/25 | Loss: 0.00099207
Iteration 25/25 | Loss: 0.00099207

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.48527908
Iteration 2/25 | Loss: 0.00043835
Iteration 3/25 | Loss: 0.00043833
Iteration 4/25 | Loss: 0.00043833
Iteration 5/25 | Loss: 0.00043833
Iteration 6/25 | Loss: 0.00043833
Iteration 7/25 | Loss: 0.00043833
Iteration 8/25 | Loss: 0.00043833
Iteration 9/25 | Loss: 0.00043833
Iteration 10/25 | Loss: 0.00043833
Iteration 11/25 | Loss: 0.00043833
Iteration 12/25 | Loss: 0.00043833
Iteration 13/25 | Loss: 0.00043833
Iteration 14/25 | Loss: 0.00043833
Iteration 15/25 | Loss: 0.00043833
Iteration 16/25 | Loss: 0.00043833
Iteration 17/25 | Loss: 0.00043833
Iteration 18/25 | Loss: 0.00043833
Iteration 19/25 | Loss: 0.00043833
Iteration 20/25 | Loss: 0.00043833
Iteration 21/25 | Loss: 0.00043833
Iteration 22/25 | Loss: 0.00043833
Iteration 23/25 | Loss: 0.00043833
Iteration 24/25 | Loss: 0.00043833
Iteration 25/25 | Loss: 0.00043833

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043833
Iteration 2/1000 | Loss: 0.00002574
Iteration 3/1000 | Loss: 0.00001845
Iteration 4/1000 | Loss: 0.00001629
Iteration 5/1000 | Loss: 0.00001568
Iteration 6/1000 | Loss: 0.00001526
Iteration 7/1000 | Loss: 0.00001478
Iteration 8/1000 | Loss: 0.00001448
Iteration 9/1000 | Loss: 0.00001429
Iteration 10/1000 | Loss: 0.00001427
Iteration 11/1000 | Loss: 0.00001423
Iteration 12/1000 | Loss: 0.00001422
Iteration 13/1000 | Loss: 0.00001422
Iteration 14/1000 | Loss: 0.00001422
Iteration 15/1000 | Loss: 0.00001420
Iteration 16/1000 | Loss: 0.00001420
Iteration 17/1000 | Loss: 0.00001420
Iteration 18/1000 | Loss: 0.00001419
Iteration 19/1000 | Loss: 0.00001416
Iteration 20/1000 | Loss: 0.00001415
Iteration 21/1000 | Loss: 0.00001413
Iteration 22/1000 | Loss: 0.00001413
Iteration 23/1000 | Loss: 0.00001412
Iteration 24/1000 | Loss: 0.00001410
Iteration 25/1000 | Loss: 0.00001409
Iteration 26/1000 | Loss: 0.00001407
Iteration 27/1000 | Loss: 0.00001407
Iteration 28/1000 | Loss: 0.00001407
Iteration 29/1000 | Loss: 0.00001406
Iteration 30/1000 | Loss: 0.00001406
Iteration 31/1000 | Loss: 0.00001405
Iteration 32/1000 | Loss: 0.00001405
Iteration 33/1000 | Loss: 0.00001404
Iteration 34/1000 | Loss: 0.00001403
Iteration 35/1000 | Loss: 0.00001403
Iteration 36/1000 | Loss: 0.00001403
Iteration 37/1000 | Loss: 0.00001403
Iteration 38/1000 | Loss: 0.00001402
Iteration 39/1000 | Loss: 0.00001402
Iteration 40/1000 | Loss: 0.00001401
Iteration 41/1000 | Loss: 0.00001401
Iteration 42/1000 | Loss: 0.00001401
Iteration 43/1000 | Loss: 0.00001401
Iteration 44/1000 | Loss: 0.00001401
Iteration 45/1000 | Loss: 0.00001401
Iteration 46/1000 | Loss: 0.00001400
Iteration 47/1000 | Loss: 0.00001400
Iteration 48/1000 | Loss: 0.00001400
Iteration 49/1000 | Loss: 0.00001400
Iteration 50/1000 | Loss: 0.00001400
Iteration 51/1000 | Loss: 0.00001400
Iteration 52/1000 | Loss: 0.00001400
Iteration 53/1000 | Loss: 0.00001400
Iteration 54/1000 | Loss: 0.00001400
Iteration 55/1000 | Loss: 0.00001399
Iteration 56/1000 | Loss: 0.00001399
Iteration 57/1000 | Loss: 0.00001399
Iteration 58/1000 | Loss: 0.00001399
Iteration 59/1000 | Loss: 0.00001399
Iteration 60/1000 | Loss: 0.00001399
Iteration 61/1000 | Loss: 0.00001399
Iteration 62/1000 | Loss: 0.00001399
Iteration 63/1000 | Loss: 0.00001399
Iteration 64/1000 | Loss: 0.00001399
Iteration 65/1000 | Loss: 0.00001399
Iteration 66/1000 | Loss: 0.00001399
Iteration 67/1000 | Loss: 0.00001399
Iteration 68/1000 | Loss: 0.00001399
Iteration 69/1000 | Loss: 0.00001399
Iteration 70/1000 | Loss: 0.00001399
Iteration 71/1000 | Loss: 0.00001399
Iteration 72/1000 | Loss: 0.00001399
Iteration 73/1000 | Loss: 0.00001399
Iteration 74/1000 | Loss: 0.00001399
Iteration 75/1000 | Loss: 0.00001399
Iteration 76/1000 | Loss: 0.00001399
Iteration 77/1000 | Loss: 0.00001399
Iteration 78/1000 | Loss: 0.00001399
Iteration 79/1000 | Loss: 0.00001399
Iteration 80/1000 | Loss: 0.00001399
Iteration 81/1000 | Loss: 0.00001399
Iteration 82/1000 | Loss: 0.00001399
Iteration 83/1000 | Loss: 0.00001399
Iteration 84/1000 | Loss: 0.00001399
Iteration 85/1000 | Loss: 0.00001399
Iteration 86/1000 | Loss: 0.00001399
Iteration 87/1000 | Loss: 0.00001399
Iteration 88/1000 | Loss: 0.00001399
Iteration 89/1000 | Loss: 0.00001399
Iteration 90/1000 | Loss: 0.00001399
Iteration 91/1000 | Loss: 0.00001399
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [1.3985951227368787e-05, 1.3985951227368787e-05, 1.3985951227368787e-05, 1.3985951227368787e-05, 1.3985951227368787e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3985951227368787e-05

Optimization complete. Final v2v error: 3.2179622650146484 mm

Highest mean error: 3.551854133605957 mm for frame 78

Lowest mean error: 2.8534257411956787 mm for frame 229

Saving results

Total time: 57.68182682991028
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_nl_6135/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_6135/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_6135/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00486105
Iteration 2/25 | Loss: 0.00151486
Iteration 3/25 | Loss: 0.00105797
Iteration 4/25 | Loss: 0.00098756
Iteration 5/25 | Loss: 0.00097873
Iteration 6/25 | Loss: 0.00097658
Iteration 7/25 | Loss: 0.00097643
Iteration 8/25 | Loss: 0.00097643
Iteration 9/25 | Loss: 0.00097643
Iteration 10/25 | Loss: 0.00097643
Iteration 11/25 | Loss: 0.00097643
Iteration 12/25 | Loss: 0.00097643
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009764283313415945, 0.0009764283313415945, 0.0009764283313415945, 0.0009764283313415945, 0.0009764283313415945]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009764283313415945

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39674926
Iteration 2/25 | Loss: 0.00058562
Iteration 3/25 | Loss: 0.00058562
Iteration 4/25 | Loss: 0.00058562
Iteration 5/25 | Loss: 0.00058562
Iteration 6/25 | Loss: 0.00058562
Iteration 7/25 | Loss: 0.00058562
Iteration 8/25 | Loss: 0.00058562
Iteration 9/25 | Loss: 0.00058562
Iteration 10/25 | Loss: 0.00058562
Iteration 11/25 | Loss: 0.00058562
Iteration 12/25 | Loss: 0.00058562
Iteration 13/25 | Loss: 0.00058562
Iteration 14/25 | Loss: 0.00058562
Iteration 15/25 | Loss: 0.00058562
Iteration 16/25 | Loss: 0.00058562
Iteration 17/25 | Loss: 0.00058562
Iteration 18/25 | Loss: 0.00058562
Iteration 19/25 | Loss: 0.00058562
Iteration 20/25 | Loss: 0.00058562
Iteration 21/25 | Loss: 0.00058562
Iteration 22/25 | Loss: 0.00058562
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0005856211064383388, 0.0005856211064383388, 0.0005856211064383388, 0.0005856211064383388, 0.0005856211064383388]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005856211064383388

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058562
Iteration 2/1000 | Loss: 0.00003448
Iteration 3/1000 | Loss: 0.00002243
Iteration 4/1000 | Loss: 0.00001698
Iteration 5/1000 | Loss: 0.00001551
Iteration 6/1000 | Loss: 0.00001464
Iteration 7/1000 | Loss: 0.00001407
Iteration 8/1000 | Loss: 0.00001375
Iteration 9/1000 | Loss: 0.00001350
Iteration 10/1000 | Loss: 0.00001327
Iteration 11/1000 | Loss: 0.00001316
Iteration 12/1000 | Loss: 0.00001311
Iteration 13/1000 | Loss: 0.00001310
Iteration 14/1000 | Loss: 0.00001307
Iteration 15/1000 | Loss: 0.00001298
Iteration 16/1000 | Loss: 0.00001298
Iteration 17/1000 | Loss: 0.00001297
Iteration 18/1000 | Loss: 0.00001296
Iteration 19/1000 | Loss: 0.00001292
Iteration 20/1000 | Loss: 0.00001283
Iteration 21/1000 | Loss: 0.00001278
Iteration 22/1000 | Loss: 0.00001277
Iteration 23/1000 | Loss: 0.00001273
Iteration 24/1000 | Loss: 0.00001272
Iteration 25/1000 | Loss: 0.00001267
Iteration 26/1000 | Loss: 0.00001267
Iteration 27/1000 | Loss: 0.00001267
Iteration 28/1000 | Loss: 0.00001266
Iteration 29/1000 | Loss: 0.00001265
Iteration 30/1000 | Loss: 0.00001265
Iteration 31/1000 | Loss: 0.00001263
Iteration 32/1000 | Loss: 0.00001263
Iteration 33/1000 | Loss: 0.00001263
Iteration 34/1000 | Loss: 0.00001263
Iteration 35/1000 | Loss: 0.00001263
Iteration 36/1000 | Loss: 0.00001262
Iteration 37/1000 | Loss: 0.00001262
Iteration 38/1000 | Loss: 0.00001262
Iteration 39/1000 | Loss: 0.00001261
Iteration 40/1000 | Loss: 0.00001261
Iteration 41/1000 | Loss: 0.00001261
Iteration 42/1000 | Loss: 0.00001261
Iteration 43/1000 | Loss: 0.00001261
Iteration 44/1000 | Loss: 0.00001261
Iteration 45/1000 | Loss: 0.00001260
Iteration 46/1000 | Loss: 0.00001260
Iteration 47/1000 | Loss: 0.00001260
Iteration 48/1000 | Loss: 0.00001260
Iteration 49/1000 | Loss: 0.00001260
Iteration 50/1000 | Loss: 0.00001259
Iteration 51/1000 | Loss: 0.00001259
Iteration 52/1000 | Loss: 0.00001259
Iteration 53/1000 | Loss: 0.00001259
Iteration 54/1000 | Loss: 0.00001259
Iteration 55/1000 | Loss: 0.00001259
Iteration 56/1000 | Loss: 0.00001259
Iteration 57/1000 | Loss: 0.00001259
Iteration 58/1000 | Loss: 0.00001259
Iteration 59/1000 | Loss: 0.00001259
Iteration 60/1000 | Loss: 0.00001259
Iteration 61/1000 | Loss: 0.00001259
Iteration 62/1000 | Loss: 0.00001259
Iteration 63/1000 | Loss: 0.00001259
Iteration 64/1000 | Loss: 0.00001259
Iteration 65/1000 | Loss: 0.00001258
Iteration 66/1000 | Loss: 0.00001258
Iteration 67/1000 | Loss: 0.00001258
Iteration 68/1000 | Loss: 0.00001258
Iteration 69/1000 | Loss: 0.00001258
Iteration 70/1000 | Loss: 0.00001258
Iteration 71/1000 | Loss: 0.00001258
Iteration 72/1000 | Loss: 0.00001258
Iteration 73/1000 | Loss: 0.00001258
Iteration 74/1000 | Loss: 0.00001258
Iteration 75/1000 | Loss: 0.00001258
Iteration 76/1000 | Loss: 0.00001258
Iteration 77/1000 | Loss: 0.00001258
Iteration 78/1000 | Loss: 0.00001258
Iteration 79/1000 | Loss: 0.00001257
Iteration 80/1000 | Loss: 0.00001257
Iteration 81/1000 | Loss: 0.00001257
Iteration 82/1000 | Loss: 0.00001257
Iteration 83/1000 | Loss: 0.00001257
Iteration 84/1000 | Loss: 0.00001257
Iteration 85/1000 | Loss: 0.00001257
Iteration 86/1000 | Loss: 0.00001256
Iteration 87/1000 | Loss: 0.00001256
Iteration 88/1000 | Loss: 0.00001256
Iteration 89/1000 | Loss: 0.00001256
Iteration 90/1000 | Loss: 0.00001256
Iteration 91/1000 | Loss: 0.00001256
Iteration 92/1000 | Loss: 0.00001256
Iteration 93/1000 | Loss: 0.00001256
Iteration 94/1000 | Loss: 0.00001256
Iteration 95/1000 | Loss: 0.00001256
Iteration 96/1000 | Loss: 0.00001255
Iteration 97/1000 | Loss: 0.00001255
Iteration 98/1000 | Loss: 0.00001255
Iteration 99/1000 | Loss: 0.00001254
Iteration 100/1000 | Loss: 0.00001254
Iteration 101/1000 | Loss: 0.00001254
Iteration 102/1000 | Loss: 0.00001254
Iteration 103/1000 | Loss: 0.00001254
Iteration 104/1000 | Loss: 0.00001254
Iteration 105/1000 | Loss: 0.00001253
Iteration 106/1000 | Loss: 0.00001253
Iteration 107/1000 | Loss: 0.00001253
Iteration 108/1000 | Loss: 0.00001253
Iteration 109/1000 | Loss: 0.00001253
Iteration 110/1000 | Loss: 0.00001253
Iteration 111/1000 | Loss: 0.00001253
Iteration 112/1000 | Loss: 0.00001253
Iteration 113/1000 | Loss: 0.00001253
Iteration 114/1000 | Loss: 0.00001253
Iteration 115/1000 | Loss: 0.00001253
Iteration 116/1000 | Loss: 0.00001253
Iteration 117/1000 | Loss: 0.00001253
Iteration 118/1000 | Loss: 0.00001253
Iteration 119/1000 | Loss: 0.00001253
Iteration 120/1000 | Loss: 0.00001252
Iteration 121/1000 | Loss: 0.00001252
Iteration 122/1000 | Loss: 0.00001252
Iteration 123/1000 | Loss: 0.00001252
Iteration 124/1000 | Loss: 0.00001252
Iteration 125/1000 | Loss: 0.00001252
Iteration 126/1000 | Loss: 0.00001251
Iteration 127/1000 | Loss: 0.00001251
Iteration 128/1000 | Loss: 0.00001251
Iteration 129/1000 | Loss: 0.00001251
Iteration 130/1000 | Loss: 0.00001251
Iteration 131/1000 | Loss: 0.00001251
Iteration 132/1000 | Loss: 0.00001251
Iteration 133/1000 | Loss: 0.00001251
Iteration 134/1000 | Loss: 0.00001251
Iteration 135/1000 | Loss: 0.00001251
Iteration 136/1000 | Loss: 0.00001251
Iteration 137/1000 | Loss: 0.00001251
Iteration 138/1000 | Loss: 0.00001250
Iteration 139/1000 | Loss: 0.00001250
Iteration 140/1000 | Loss: 0.00001250
Iteration 141/1000 | Loss: 0.00001250
Iteration 142/1000 | Loss: 0.00001250
Iteration 143/1000 | Loss: 0.00001250
Iteration 144/1000 | Loss: 0.00001250
Iteration 145/1000 | Loss: 0.00001249
Iteration 146/1000 | Loss: 0.00001249
Iteration 147/1000 | Loss: 0.00001249
Iteration 148/1000 | Loss: 0.00001249
Iteration 149/1000 | Loss: 0.00001249
Iteration 150/1000 | Loss: 0.00001249
Iteration 151/1000 | Loss: 0.00001249
Iteration 152/1000 | Loss: 0.00001249
Iteration 153/1000 | Loss: 0.00001249
Iteration 154/1000 | Loss: 0.00001249
Iteration 155/1000 | Loss: 0.00001249
Iteration 156/1000 | Loss: 0.00001249
Iteration 157/1000 | Loss: 0.00001249
Iteration 158/1000 | Loss: 0.00001249
Iteration 159/1000 | Loss: 0.00001249
Iteration 160/1000 | Loss: 0.00001249
Iteration 161/1000 | Loss: 0.00001249
Iteration 162/1000 | Loss: 0.00001249
Iteration 163/1000 | Loss: 0.00001249
Iteration 164/1000 | Loss: 0.00001249
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.2485214938351419e-05, 1.2485214938351419e-05, 1.2485214938351419e-05, 1.2485214938351419e-05, 1.2485214938351419e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2485214938351419e-05

Optimization complete. Final v2v error: 2.972107410430908 mm

Highest mean error: 3.7451744079589844 mm for frame 42

Lowest mean error: 2.491480588912964 mm for frame 161

Saving results

Total time: 41.25973653793335
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_nl_6135/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_6135/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_6135/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00411946
Iteration 2/25 | Loss: 0.00092750
Iteration 3/25 | Loss: 0.00085919
Iteration 4/25 | Loss: 0.00085248
Iteration 5/25 | Loss: 0.00085031
Iteration 6/25 | Loss: 0.00085008
Iteration 7/25 | Loss: 0.00085008
Iteration 8/25 | Loss: 0.00085008
Iteration 9/25 | Loss: 0.00085008
Iteration 10/25 | Loss: 0.00085008
Iteration 11/25 | Loss: 0.00085008
Iteration 12/25 | Loss: 0.00085008
Iteration 13/25 | Loss: 0.00085008
Iteration 14/25 | Loss: 0.00085008
Iteration 15/25 | Loss: 0.00085008
Iteration 16/25 | Loss: 0.00085008
Iteration 17/25 | Loss: 0.00085008
Iteration 18/25 | Loss: 0.00085008
Iteration 19/25 | Loss: 0.00085008
Iteration 20/25 | Loss: 0.00085008
Iteration 21/25 | Loss: 0.00085008
Iteration 22/25 | Loss: 0.00085008
Iteration 23/25 | Loss: 0.00085008
Iteration 24/25 | Loss: 0.00085008
Iteration 25/25 | Loss: 0.00085008

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.03096890
Iteration 2/25 | Loss: 0.00065883
Iteration 3/25 | Loss: 0.00065883
Iteration 4/25 | Loss: 0.00065883
Iteration 5/25 | Loss: 0.00065883
Iteration 6/25 | Loss: 0.00065883
Iteration 7/25 | Loss: 0.00065883
Iteration 8/25 | Loss: 0.00065883
Iteration 9/25 | Loss: 0.00065883
Iteration 10/25 | Loss: 0.00065883
Iteration 11/25 | Loss: 0.00065883
Iteration 12/25 | Loss: 0.00065883
Iteration 13/25 | Loss: 0.00065883
Iteration 14/25 | Loss: 0.00065883
Iteration 15/25 | Loss: 0.00065883
Iteration 16/25 | Loss: 0.00065883
Iteration 17/25 | Loss: 0.00065883
Iteration 18/25 | Loss: 0.00065883
Iteration 19/25 | Loss: 0.00065883
Iteration 20/25 | Loss: 0.00065883
Iteration 21/25 | Loss: 0.00065883
Iteration 22/25 | Loss: 0.00065883
Iteration 23/25 | Loss: 0.00065883
Iteration 24/25 | Loss: 0.00065883
Iteration 25/25 | Loss: 0.00065883

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065883
Iteration 2/1000 | Loss: 0.00001601
Iteration 3/1000 | Loss: 0.00001171
Iteration 4/1000 | Loss: 0.00001044
Iteration 5/1000 | Loss: 0.00000987
Iteration 6/1000 | Loss: 0.00000955
Iteration 7/1000 | Loss: 0.00000924
Iteration 8/1000 | Loss: 0.00000919
Iteration 9/1000 | Loss: 0.00000908
Iteration 10/1000 | Loss: 0.00000896
Iteration 11/1000 | Loss: 0.00000894
Iteration 12/1000 | Loss: 0.00000894
Iteration 13/1000 | Loss: 0.00000893
Iteration 14/1000 | Loss: 0.00000893
Iteration 15/1000 | Loss: 0.00000893
Iteration 16/1000 | Loss: 0.00000893
Iteration 17/1000 | Loss: 0.00000891
Iteration 18/1000 | Loss: 0.00000891
Iteration 19/1000 | Loss: 0.00000890
Iteration 20/1000 | Loss: 0.00000890
Iteration 21/1000 | Loss: 0.00000889
Iteration 22/1000 | Loss: 0.00000889
Iteration 23/1000 | Loss: 0.00000889
Iteration 24/1000 | Loss: 0.00000885
Iteration 25/1000 | Loss: 0.00000885
Iteration 26/1000 | Loss: 0.00000885
Iteration 27/1000 | Loss: 0.00000885
Iteration 28/1000 | Loss: 0.00000885
Iteration 29/1000 | Loss: 0.00000885
Iteration 30/1000 | Loss: 0.00000885
Iteration 31/1000 | Loss: 0.00000884
Iteration 32/1000 | Loss: 0.00000884
Iteration 33/1000 | Loss: 0.00000881
Iteration 34/1000 | Loss: 0.00000880
Iteration 35/1000 | Loss: 0.00000877
Iteration 36/1000 | Loss: 0.00000877
Iteration 37/1000 | Loss: 0.00000877
Iteration 38/1000 | Loss: 0.00000877
Iteration 39/1000 | Loss: 0.00000877
Iteration 40/1000 | Loss: 0.00000877
Iteration 41/1000 | Loss: 0.00000877
Iteration 42/1000 | Loss: 0.00000877
Iteration 43/1000 | Loss: 0.00000876
Iteration 44/1000 | Loss: 0.00000876
Iteration 45/1000 | Loss: 0.00000876
Iteration 46/1000 | Loss: 0.00000876
Iteration 47/1000 | Loss: 0.00000875
Iteration 48/1000 | Loss: 0.00000874
Iteration 49/1000 | Loss: 0.00000873
Iteration 50/1000 | Loss: 0.00000873
Iteration 51/1000 | Loss: 0.00000872
Iteration 52/1000 | Loss: 0.00000872
Iteration 53/1000 | Loss: 0.00000872
Iteration 54/1000 | Loss: 0.00000872
Iteration 55/1000 | Loss: 0.00000872
Iteration 56/1000 | Loss: 0.00000872
Iteration 57/1000 | Loss: 0.00000872
Iteration 58/1000 | Loss: 0.00000872
Iteration 59/1000 | Loss: 0.00000871
Iteration 60/1000 | Loss: 0.00000871
Iteration 61/1000 | Loss: 0.00000871
Iteration 62/1000 | Loss: 0.00000870
Iteration 63/1000 | Loss: 0.00000869
Iteration 64/1000 | Loss: 0.00000868
Iteration 65/1000 | Loss: 0.00000868
Iteration 66/1000 | Loss: 0.00000868
Iteration 67/1000 | Loss: 0.00000868
Iteration 68/1000 | Loss: 0.00000868
Iteration 69/1000 | Loss: 0.00000868
Iteration 70/1000 | Loss: 0.00000868
Iteration 71/1000 | Loss: 0.00000868
Iteration 72/1000 | Loss: 0.00000868
Iteration 73/1000 | Loss: 0.00000868
Iteration 74/1000 | Loss: 0.00000868
Iteration 75/1000 | Loss: 0.00000867
Iteration 76/1000 | Loss: 0.00000867
Iteration 77/1000 | Loss: 0.00000866
Iteration 78/1000 | Loss: 0.00000866
Iteration 79/1000 | Loss: 0.00000866
Iteration 80/1000 | Loss: 0.00000866
Iteration 81/1000 | Loss: 0.00000865
Iteration 82/1000 | Loss: 0.00000865
Iteration 83/1000 | Loss: 0.00000865
Iteration 84/1000 | Loss: 0.00000865
Iteration 85/1000 | Loss: 0.00000865
Iteration 86/1000 | Loss: 0.00000865
Iteration 87/1000 | Loss: 0.00000865
Iteration 88/1000 | Loss: 0.00000865
Iteration 89/1000 | Loss: 0.00000865
Iteration 90/1000 | Loss: 0.00000864
Iteration 91/1000 | Loss: 0.00000864
Iteration 92/1000 | Loss: 0.00000864
Iteration 93/1000 | Loss: 0.00000863
Iteration 94/1000 | Loss: 0.00000862
Iteration 95/1000 | Loss: 0.00000862
Iteration 96/1000 | Loss: 0.00000862
Iteration 97/1000 | Loss: 0.00000862
Iteration 98/1000 | Loss: 0.00000861
Iteration 99/1000 | Loss: 0.00000861
Iteration 100/1000 | Loss: 0.00000861
Iteration 101/1000 | Loss: 0.00000861
Iteration 102/1000 | Loss: 0.00000861
Iteration 103/1000 | Loss: 0.00000861
Iteration 104/1000 | Loss: 0.00000861
Iteration 105/1000 | Loss: 0.00000861
Iteration 106/1000 | Loss: 0.00000861
Iteration 107/1000 | Loss: 0.00000861
Iteration 108/1000 | Loss: 0.00000861
Iteration 109/1000 | Loss: 0.00000860
Iteration 110/1000 | Loss: 0.00000860
Iteration 111/1000 | Loss: 0.00000860
Iteration 112/1000 | Loss: 0.00000859
Iteration 113/1000 | Loss: 0.00000859
Iteration 114/1000 | Loss: 0.00000859
Iteration 115/1000 | Loss: 0.00000859
Iteration 116/1000 | Loss: 0.00000859
Iteration 117/1000 | Loss: 0.00000859
Iteration 118/1000 | Loss: 0.00000859
Iteration 119/1000 | Loss: 0.00000859
Iteration 120/1000 | Loss: 0.00000859
Iteration 121/1000 | Loss: 0.00000859
Iteration 122/1000 | Loss: 0.00000858
Iteration 123/1000 | Loss: 0.00000858
Iteration 124/1000 | Loss: 0.00000858
Iteration 125/1000 | Loss: 0.00000858
Iteration 126/1000 | Loss: 0.00000857
Iteration 127/1000 | Loss: 0.00000857
Iteration 128/1000 | Loss: 0.00000857
Iteration 129/1000 | Loss: 0.00000857
Iteration 130/1000 | Loss: 0.00000857
Iteration 131/1000 | Loss: 0.00000857
Iteration 132/1000 | Loss: 0.00000856
Iteration 133/1000 | Loss: 0.00000856
Iteration 134/1000 | Loss: 0.00000856
Iteration 135/1000 | Loss: 0.00000856
Iteration 136/1000 | Loss: 0.00000856
Iteration 137/1000 | Loss: 0.00000856
Iteration 138/1000 | Loss: 0.00000856
Iteration 139/1000 | Loss: 0.00000856
Iteration 140/1000 | Loss: 0.00000856
Iteration 141/1000 | Loss: 0.00000855
Iteration 142/1000 | Loss: 0.00000855
Iteration 143/1000 | Loss: 0.00000855
Iteration 144/1000 | Loss: 0.00000855
Iteration 145/1000 | Loss: 0.00000855
Iteration 146/1000 | Loss: 0.00000855
Iteration 147/1000 | Loss: 0.00000855
Iteration 148/1000 | Loss: 0.00000855
Iteration 149/1000 | Loss: 0.00000855
Iteration 150/1000 | Loss: 0.00000855
Iteration 151/1000 | Loss: 0.00000855
Iteration 152/1000 | Loss: 0.00000855
Iteration 153/1000 | Loss: 0.00000854
Iteration 154/1000 | Loss: 0.00000854
Iteration 155/1000 | Loss: 0.00000854
Iteration 156/1000 | Loss: 0.00000854
Iteration 157/1000 | Loss: 0.00000854
Iteration 158/1000 | Loss: 0.00000854
Iteration 159/1000 | Loss: 0.00000854
Iteration 160/1000 | Loss: 0.00000854
Iteration 161/1000 | Loss: 0.00000854
Iteration 162/1000 | Loss: 0.00000853
Iteration 163/1000 | Loss: 0.00000853
Iteration 164/1000 | Loss: 0.00000853
Iteration 165/1000 | Loss: 0.00000853
Iteration 166/1000 | Loss: 0.00000853
Iteration 167/1000 | Loss: 0.00000853
Iteration 168/1000 | Loss: 0.00000852
Iteration 169/1000 | Loss: 0.00000852
Iteration 170/1000 | Loss: 0.00000852
Iteration 171/1000 | Loss: 0.00000852
Iteration 172/1000 | Loss: 0.00000852
Iteration 173/1000 | Loss: 0.00000852
Iteration 174/1000 | Loss: 0.00000851
Iteration 175/1000 | Loss: 0.00000851
Iteration 176/1000 | Loss: 0.00000851
Iteration 177/1000 | Loss: 0.00000851
Iteration 178/1000 | Loss: 0.00000851
Iteration 179/1000 | Loss: 0.00000851
Iteration 180/1000 | Loss: 0.00000851
Iteration 181/1000 | Loss: 0.00000851
Iteration 182/1000 | Loss: 0.00000851
Iteration 183/1000 | Loss: 0.00000851
Iteration 184/1000 | Loss: 0.00000851
Iteration 185/1000 | Loss: 0.00000851
Iteration 186/1000 | Loss: 0.00000851
Iteration 187/1000 | Loss: 0.00000851
Iteration 188/1000 | Loss: 0.00000851
Iteration 189/1000 | Loss: 0.00000851
Iteration 190/1000 | Loss: 0.00000851
Iteration 191/1000 | Loss: 0.00000851
Iteration 192/1000 | Loss: 0.00000851
Iteration 193/1000 | Loss: 0.00000851
Iteration 194/1000 | Loss: 0.00000851
Iteration 195/1000 | Loss: 0.00000850
Iteration 196/1000 | Loss: 0.00000850
Iteration 197/1000 | Loss: 0.00000850
Iteration 198/1000 | Loss: 0.00000850
Iteration 199/1000 | Loss: 0.00000850
Iteration 200/1000 | Loss: 0.00000850
Iteration 201/1000 | Loss: 0.00000850
Iteration 202/1000 | Loss: 0.00000850
Iteration 203/1000 | Loss: 0.00000850
Iteration 204/1000 | Loss: 0.00000850
Iteration 205/1000 | Loss: 0.00000850
Iteration 206/1000 | Loss: 0.00000849
Iteration 207/1000 | Loss: 0.00000849
Iteration 208/1000 | Loss: 0.00000849
Iteration 209/1000 | Loss: 0.00000849
Iteration 210/1000 | Loss: 0.00000849
Iteration 211/1000 | Loss: 0.00000849
Iteration 212/1000 | Loss: 0.00000849
Iteration 213/1000 | Loss: 0.00000849
Iteration 214/1000 | Loss: 0.00000849
Iteration 215/1000 | Loss: 0.00000849
Iteration 216/1000 | Loss: 0.00000849
Iteration 217/1000 | Loss: 0.00000849
Iteration 218/1000 | Loss: 0.00000849
Iteration 219/1000 | Loss: 0.00000849
Iteration 220/1000 | Loss: 0.00000849
Iteration 221/1000 | Loss: 0.00000849
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 221. Stopping optimization.
Last 5 losses: [8.490876098221634e-06, 8.490876098221634e-06, 8.490876098221634e-06, 8.490876098221634e-06, 8.490876098221634e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.490876098221634e-06

Optimization complete. Final v2v error: 2.500755548477173 mm

Highest mean error: 2.8678812980651855 mm for frame 89

Lowest mean error: 1.8550052642822266 mm for frame 176

Saving results

Total time: 37.296085596084595
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_27_nl_6135/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_6135/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_27_nl_6135/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00551503
Iteration 2/25 | Loss: 0.00091040
Iteration 3/25 | Loss: 0.00084498
Iteration 4/25 | Loss: 0.00083555
Iteration 5/25 | Loss: 0.00083258
Iteration 6/25 | Loss: 0.00083208
Iteration 7/25 | Loss: 0.00083208
Iteration 8/25 | Loss: 0.00083208
Iteration 9/25 | Loss: 0.00083208
Iteration 10/25 | Loss: 0.00083208
Iteration 11/25 | Loss: 0.00083208
Iteration 12/25 | Loss: 0.00083208
Iteration 13/25 | Loss: 0.00083208
Iteration 14/25 | Loss: 0.00083208
Iteration 15/25 | Loss: 0.00083208
Iteration 16/25 | Loss: 0.00083208
Iteration 17/25 | Loss: 0.00083208
Iteration 18/25 | Loss: 0.00083208
Iteration 19/25 | Loss: 0.00083208
Iteration 20/25 | Loss: 0.00083208
Iteration 21/25 | Loss: 0.00083208
Iteration 22/25 | Loss: 0.00083208
Iteration 23/25 | Loss: 0.00083208
Iteration 24/25 | Loss: 0.00083208
Iteration 25/25 | Loss: 0.00083208

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.17460632
Iteration 2/25 | Loss: 0.00060145
Iteration 3/25 | Loss: 0.00060145
Iteration 4/25 | Loss: 0.00060145
Iteration 5/25 | Loss: 0.00060145
Iteration 6/25 | Loss: 0.00060145
Iteration 7/25 | Loss: 0.00060145
Iteration 8/25 | Loss: 0.00060145
Iteration 9/25 | Loss: 0.00060145
Iteration 10/25 | Loss: 0.00060145
Iteration 11/25 | Loss: 0.00060145
Iteration 12/25 | Loss: 0.00060145
Iteration 13/25 | Loss: 0.00060145
Iteration 14/25 | Loss: 0.00060145
Iteration 15/25 | Loss: 0.00060145
Iteration 16/25 | Loss: 0.00060145
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006014462560415268, 0.0006014462560415268, 0.0006014462560415268, 0.0006014462560415268, 0.0006014462560415268]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006014462560415268

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060145
Iteration 2/1000 | Loss: 0.00002084
Iteration 3/1000 | Loss: 0.00001299
Iteration 4/1000 | Loss: 0.00001058
Iteration 5/1000 | Loss: 0.00000968
Iteration 6/1000 | Loss: 0.00000923
Iteration 7/1000 | Loss: 0.00000885
Iteration 8/1000 | Loss: 0.00000878
Iteration 9/1000 | Loss: 0.00000855
Iteration 10/1000 | Loss: 0.00000851
Iteration 11/1000 | Loss: 0.00000845
Iteration 12/1000 | Loss: 0.00000843
Iteration 13/1000 | Loss: 0.00000841
Iteration 14/1000 | Loss: 0.00000839
Iteration 15/1000 | Loss: 0.00000838
Iteration 16/1000 | Loss: 0.00000837
Iteration 17/1000 | Loss: 0.00000837
Iteration 18/1000 | Loss: 0.00000836
Iteration 19/1000 | Loss: 0.00000836
Iteration 20/1000 | Loss: 0.00000833
Iteration 21/1000 | Loss: 0.00000830
Iteration 22/1000 | Loss: 0.00000829
Iteration 23/1000 | Loss: 0.00000828
Iteration 24/1000 | Loss: 0.00000828
Iteration 25/1000 | Loss: 0.00000827
Iteration 26/1000 | Loss: 0.00000826
Iteration 27/1000 | Loss: 0.00000825
Iteration 28/1000 | Loss: 0.00000825
Iteration 29/1000 | Loss: 0.00000824
Iteration 30/1000 | Loss: 0.00000824
Iteration 31/1000 | Loss: 0.00000824
Iteration 32/1000 | Loss: 0.00000824
Iteration 33/1000 | Loss: 0.00000823
Iteration 34/1000 | Loss: 0.00000822
Iteration 35/1000 | Loss: 0.00000821
Iteration 36/1000 | Loss: 0.00000821
Iteration 37/1000 | Loss: 0.00000821
Iteration 38/1000 | Loss: 0.00000820
Iteration 39/1000 | Loss: 0.00000819
Iteration 40/1000 | Loss: 0.00000819
Iteration 41/1000 | Loss: 0.00000818
Iteration 42/1000 | Loss: 0.00000817
Iteration 43/1000 | Loss: 0.00000816
Iteration 44/1000 | Loss: 0.00000816
Iteration 45/1000 | Loss: 0.00000816
Iteration 46/1000 | Loss: 0.00000815
Iteration 47/1000 | Loss: 0.00000815
Iteration 48/1000 | Loss: 0.00000814
Iteration 49/1000 | Loss: 0.00000813
Iteration 50/1000 | Loss: 0.00000812
Iteration 51/1000 | Loss: 0.00000811
Iteration 52/1000 | Loss: 0.00000811
Iteration 53/1000 | Loss: 0.00000811
Iteration 54/1000 | Loss: 0.00000810
Iteration 55/1000 | Loss: 0.00000810
Iteration 56/1000 | Loss: 0.00000809
Iteration 57/1000 | Loss: 0.00000809
Iteration 58/1000 | Loss: 0.00000809
Iteration 59/1000 | Loss: 0.00000809
Iteration 60/1000 | Loss: 0.00000809
Iteration 61/1000 | Loss: 0.00000809
Iteration 62/1000 | Loss: 0.00000809
Iteration 63/1000 | Loss: 0.00000808
Iteration 64/1000 | Loss: 0.00000808
Iteration 65/1000 | Loss: 0.00000808
Iteration 66/1000 | Loss: 0.00000807
Iteration 67/1000 | Loss: 0.00000807
Iteration 68/1000 | Loss: 0.00000807
Iteration 69/1000 | Loss: 0.00000806
Iteration 70/1000 | Loss: 0.00000806
Iteration 71/1000 | Loss: 0.00000806
Iteration 72/1000 | Loss: 0.00000806
Iteration 73/1000 | Loss: 0.00000806
Iteration 74/1000 | Loss: 0.00000806
Iteration 75/1000 | Loss: 0.00000806
Iteration 76/1000 | Loss: 0.00000806
Iteration 77/1000 | Loss: 0.00000806
Iteration 78/1000 | Loss: 0.00000805
Iteration 79/1000 | Loss: 0.00000805
Iteration 80/1000 | Loss: 0.00000805
Iteration 81/1000 | Loss: 0.00000805
Iteration 82/1000 | Loss: 0.00000804
Iteration 83/1000 | Loss: 0.00000804
Iteration 84/1000 | Loss: 0.00000804
Iteration 85/1000 | Loss: 0.00000804
Iteration 86/1000 | Loss: 0.00000804
Iteration 87/1000 | Loss: 0.00000804
Iteration 88/1000 | Loss: 0.00000803
Iteration 89/1000 | Loss: 0.00000803
Iteration 90/1000 | Loss: 0.00000803
Iteration 91/1000 | Loss: 0.00000802
Iteration 92/1000 | Loss: 0.00000802
Iteration 93/1000 | Loss: 0.00000802
Iteration 94/1000 | Loss: 0.00000801
Iteration 95/1000 | Loss: 0.00000801
Iteration 96/1000 | Loss: 0.00000801
Iteration 97/1000 | Loss: 0.00000801
Iteration 98/1000 | Loss: 0.00000801
Iteration 99/1000 | Loss: 0.00000801
Iteration 100/1000 | Loss: 0.00000800
Iteration 101/1000 | Loss: 0.00000800
Iteration 102/1000 | Loss: 0.00000800
Iteration 103/1000 | Loss: 0.00000800
Iteration 104/1000 | Loss: 0.00000800
Iteration 105/1000 | Loss: 0.00000800
Iteration 106/1000 | Loss: 0.00000800
Iteration 107/1000 | Loss: 0.00000800
Iteration 108/1000 | Loss: 0.00000800
Iteration 109/1000 | Loss: 0.00000799
Iteration 110/1000 | Loss: 0.00000799
Iteration 111/1000 | Loss: 0.00000799
Iteration 112/1000 | Loss: 0.00000799
Iteration 113/1000 | Loss: 0.00000799
Iteration 114/1000 | Loss: 0.00000799
Iteration 115/1000 | Loss: 0.00000799
Iteration 116/1000 | Loss: 0.00000799
Iteration 117/1000 | Loss: 0.00000799
Iteration 118/1000 | Loss: 0.00000799
Iteration 119/1000 | Loss: 0.00000799
Iteration 120/1000 | Loss: 0.00000799
Iteration 121/1000 | Loss: 0.00000799
Iteration 122/1000 | Loss: 0.00000799
Iteration 123/1000 | Loss: 0.00000799
Iteration 124/1000 | Loss: 0.00000799
Iteration 125/1000 | Loss: 0.00000799
Iteration 126/1000 | Loss: 0.00000799
Iteration 127/1000 | Loss: 0.00000799
Iteration 128/1000 | Loss: 0.00000799
Iteration 129/1000 | Loss: 0.00000799
Iteration 130/1000 | Loss: 0.00000799
Iteration 131/1000 | Loss: 0.00000799
Iteration 132/1000 | Loss: 0.00000799
Iteration 133/1000 | Loss: 0.00000799
Iteration 134/1000 | Loss: 0.00000799
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [7.985329830262344e-06, 7.985329830262344e-06, 7.985329830262344e-06, 7.985329830262344e-06, 7.985329830262344e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.985329830262344e-06

Optimization complete. Final v2v error: 2.3929014205932617 mm

Highest mean error: 2.811065435409546 mm for frame 144

Lowest mean error: 1.8796288967132568 mm for frame 200

Saving results

Total time: 33.10724425315857
