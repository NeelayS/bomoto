Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=181, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 10136-10191
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00671530
Iteration 2/25 | Loss: 0.00137465
Iteration 3/25 | Loss: 0.00121053
Iteration 4/25 | Loss: 0.00119085
Iteration 5/25 | Loss: 0.00118659
Iteration 6/25 | Loss: 0.00118622
Iteration 7/25 | Loss: 0.00118622
Iteration 8/25 | Loss: 0.00118622
Iteration 9/25 | Loss: 0.00118622
Iteration 10/25 | Loss: 0.00118622
Iteration 11/25 | Loss: 0.00118622
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011862237006425858, 0.0011862237006425858, 0.0011862237006425858, 0.0011862237006425858, 0.0011862237006425858]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011862237006425858

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.85463810
Iteration 2/25 | Loss: 0.00071567
Iteration 3/25 | Loss: 0.00071566
Iteration 4/25 | Loss: 0.00071565
Iteration 5/25 | Loss: 0.00071565
Iteration 6/25 | Loss: 0.00071565
Iteration 7/25 | Loss: 0.00071565
Iteration 8/25 | Loss: 0.00071565
Iteration 9/25 | Loss: 0.00071565
Iteration 10/25 | Loss: 0.00071565
Iteration 11/25 | Loss: 0.00071565
Iteration 12/25 | Loss: 0.00071565
Iteration 13/25 | Loss: 0.00071565
Iteration 14/25 | Loss: 0.00071565
Iteration 15/25 | Loss: 0.00071565
Iteration 16/25 | Loss: 0.00071565
Iteration 17/25 | Loss: 0.00071565
Iteration 18/25 | Loss: 0.00071565
Iteration 19/25 | Loss: 0.00071565
Iteration 20/25 | Loss: 0.00071565
Iteration 21/25 | Loss: 0.00071565
Iteration 22/25 | Loss: 0.00071565
Iteration 23/25 | Loss: 0.00071565
Iteration 24/25 | Loss: 0.00071565
Iteration 25/25 | Loss: 0.00071565

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071565
Iteration 2/1000 | Loss: 0.00003453
Iteration 3/1000 | Loss: 0.00002372
Iteration 4/1000 | Loss: 0.00002168
Iteration 5/1000 | Loss: 0.00002088
Iteration 6/1000 | Loss: 0.00002038
Iteration 7/1000 | Loss: 0.00001992
Iteration 8/1000 | Loss: 0.00001964
Iteration 9/1000 | Loss: 0.00001944
Iteration 10/1000 | Loss: 0.00001930
Iteration 11/1000 | Loss: 0.00001923
Iteration 12/1000 | Loss: 0.00001918
Iteration 13/1000 | Loss: 0.00001915
Iteration 14/1000 | Loss: 0.00001912
Iteration 15/1000 | Loss: 0.00001912
Iteration 16/1000 | Loss: 0.00001912
Iteration 17/1000 | Loss: 0.00001911
Iteration 18/1000 | Loss: 0.00001910
Iteration 19/1000 | Loss: 0.00001910
Iteration 20/1000 | Loss: 0.00001908
Iteration 21/1000 | Loss: 0.00001907
Iteration 22/1000 | Loss: 0.00001906
Iteration 23/1000 | Loss: 0.00001905
Iteration 24/1000 | Loss: 0.00001905
Iteration 25/1000 | Loss: 0.00001902
Iteration 26/1000 | Loss: 0.00001902
Iteration 27/1000 | Loss: 0.00001901
Iteration 28/1000 | Loss: 0.00001901
Iteration 29/1000 | Loss: 0.00001898
Iteration 30/1000 | Loss: 0.00001898
Iteration 31/1000 | Loss: 0.00001898
Iteration 32/1000 | Loss: 0.00001897
Iteration 33/1000 | Loss: 0.00001896
Iteration 34/1000 | Loss: 0.00001895
Iteration 35/1000 | Loss: 0.00001894
Iteration 36/1000 | Loss: 0.00001894
Iteration 37/1000 | Loss: 0.00001893
Iteration 38/1000 | Loss: 0.00001893
Iteration 39/1000 | Loss: 0.00001893
Iteration 40/1000 | Loss: 0.00001893
Iteration 41/1000 | Loss: 0.00001892
Iteration 42/1000 | Loss: 0.00001892
Iteration 43/1000 | Loss: 0.00001892
Iteration 44/1000 | Loss: 0.00001892
Iteration 45/1000 | Loss: 0.00001892
Iteration 46/1000 | Loss: 0.00001892
Iteration 47/1000 | Loss: 0.00001891
Iteration 48/1000 | Loss: 0.00001891
Iteration 49/1000 | Loss: 0.00001891
Iteration 50/1000 | Loss: 0.00001891
Iteration 51/1000 | Loss: 0.00001890
Iteration 52/1000 | Loss: 0.00001890
Iteration 53/1000 | Loss: 0.00001890
Iteration 54/1000 | Loss: 0.00001890
Iteration 55/1000 | Loss: 0.00001890
Iteration 56/1000 | Loss: 0.00001889
Iteration 57/1000 | Loss: 0.00001889
Iteration 58/1000 | Loss: 0.00001889
Iteration 59/1000 | Loss: 0.00001889
Iteration 60/1000 | Loss: 0.00001889
Iteration 61/1000 | Loss: 0.00001889
Iteration 62/1000 | Loss: 0.00001888
Iteration 63/1000 | Loss: 0.00001888
Iteration 64/1000 | Loss: 0.00001888
Iteration 65/1000 | Loss: 0.00001888
Iteration 66/1000 | Loss: 0.00001888
Iteration 67/1000 | Loss: 0.00001887
Iteration 68/1000 | Loss: 0.00001887
Iteration 69/1000 | Loss: 0.00001887
Iteration 70/1000 | Loss: 0.00001887
Iteration 71/1000 | Loss: 0.00001886
Iteration 72/1000 | Loss: 0.00001886
Iteration 73/1000 | Loss: 0.00001886
Iteration 74/1000 | Loss: 0.00001886
Iteration 75/1000 | Loss: 0.00001886
Iteration 76/1000 | Loss: 0.00001886
Iteration 77/1000 | Loss: 0.00001886
Iteration 78/1000 | Loss: 0.00001885
Iteration 79/1000 | Loss: 0.00001885
Iteration 80/1000 | Loss: 0.00001885
Iteration 81/1000 | Loss: 0.00001885
Iteration 82/1000 | Loss: 0.00001885
Iteration 83/1000 | Loss: 0.00001885
Iteration 84/1000 | Loss: 0.00001885
Iteration 85/1000 | Loss: 0.00001885
Iteration 86/1000 | Loss: 0.00001885
Iteration 87/1000 | Loss: 0.00001885
Iteration 88/1000 | Loss: 0.00001885
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [1.8851296772481874e-05, 1.8851296772481874e-05, 1.8851296772481874e-05, 1.8851296772481874e-05, 1.8851296772481874e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8851296772481874e-05

Optimization complete. Final v2v error: 3.5917153358459473 mm

Highest mean error: 4.698483467102051 mm for frame 98

Lowest mean error: 2.95902681350708 mm for frame 10

Saving results

Total time: 36.80209183692932
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00406499
Iteration 2/25 | Loss: 0.00122240
Iteration 3/25 | Loss: 0.00113506
Iteration 4/25 | Loss: 0.00112271
Iteration 5/25 | Loss: 0.00111883
Iteration 6/25 | Loss: 0.00111814
Iteration 7/25 | Loss: 0.00111813
Iteration 8/25 | Loss: 0.00111813
Iteration 9/25 | Loss: 0.00111813
Iteration 10/25 | Loss: 0.00111813
Iteration 11/25 | Loss: 0.00111813
Iteration 12/25 | Loss: 0.00111813
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011181286536157131, 0.0011181286536157131, 0.0011181286536157131, 0.0011181286536157131, 0.0011181286536157131]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011181286536157131

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.99318588
Iteration 2/25 | Loss: 0.00077818
Iteration 3/25 | Loss: 0.00077817
Iteration 4/25 | Loss: 0.00077817
Iteration 5/25 | Loss: 0.00077817
Iteration 6/25 | Loss: 0.00077817
Iteration 7/25 | Loss: 0.00077817
Iteration 8/25 | Loss: 0.00077817
Iteration 9/25 | Loss: 0.00077817
Iteration 10/25 | Loss: 0.00077817
Iteration 11/25 | Loss: 0.00077817
Iteration 12/25 | Loss: 0.00077817
Iteration 13/25 | Loss: 0.00077817
Iteration 14/25 | Loss: 0.00077817
Iteration 15/25 | Loss: 0.00077817
Iteration 16/25 | Loss: 0.00077817
Iteration 17/25 | Loss: 0.00077817
Iteration 18/25 | Loss: 0.00077817
Iteration 19/25 | Loss: 0.00077817
Iteration 20/25 | Loss: 0.00077817
Iteration 21/25 | Loss: 0.00077817
Iteration 22/25 | Loss: 0.00077817
Iteration 23/25 | Loss: 0.00077817
Iteration 24/25 | Loss: 0.00077817
Iteration 25/25 | Loss: 0.00077817

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077817
Iteration 2/1000 | Loss: 0.00002666
Iteration 3/1000 | Loss: 0.00002080
Iteration 4/1000 | Loss: 0.00001958
Iteration 5/1000 | Loss: 0.00001872
Iteration 6/1000 | Loss: 0.00001801
Iteration 7/1000 | Loss: 0.00001752
Iteration 8/1000 | Loss: 0.00001718
Iteration 9/1000 | Loss: 0.00001683
Iteration 10/1000 | Loss: 0.00001660
Iteration 11/1000 | Loss: 0.00001650
Iteration 12/1000 | Loss: 0.00001643
Iteration 13/1000 | Loss: 0.00001642
Iteration 14/1000 | Loss: 0.00001635
Iteration 15/1000 | Loss: 0.00001634
Iteration 16/1000 | Loss: 0.00001633
Iteration 17/1000 | Loss: 0.00001632
Iteration 18/1000 | Loss: 0.00001627
Iteration 19/1000 | Loss: 0.00001627
Iteration 20/1000 | Loss: 0.00001627
Iteration 21/1000 | Loss: 0.00001625
Iteration 22/1000 | Loss: 0.00001624
Iteration 23/1000 | Loss: 0.00001623
Iteration 24/1000 | Loss: 0.00001622
Iteration 25/1000 | Loss: 0.00001622
Iteration 26/1000 | Loss: 0.00001621
Iteration 27/1000 | Loss: 0.00001621
Iteration 28/1000 | Loss: 0.00001620
Iteration 29/1000 | Loss: 0.00001619
Iteration 30/1000 | Loss: 0.00001619
Iteration 31/1000 | Loss: 0.00001615
Iteration 32/1000 | Loss: 0.00001609
Iteration 33/1000 | Loss: 0.00001609
Iteration 34/1000 | Loss: 0.00001607
Iteration 35/1000 | Loss: 0.00001603
Iteration 36/1000 | Loss: 0.00001603
Iteration 37/1000 | Loss: 0.00001603
Iteration 38/1000 | Loss: 0.00001603
Iteration 39/1000 | Loss: 0.00001602
Iteration 40/1000 | Loss: 0.00001602
Iteration 41/1000 | Loss: 0.00001600
Iteration 42/1000 | Loss: 0.00001600
Iteration 43/1000 | Loss: 0.00001599
Iteration 44/1000 | Loss: 0.00001599
Iteration 45/1000 | Loss: 0.00001599
Iteration 46/1000 | Loss: 0.00001598
Iteration 47/1000 | Loss: 0.00001598
Iteration 48/1000 | Loss: 0.00001597
Iteration 49/1000 | Loss: 0.00001597
Iteration 50/1000 | Loss: 0.00001596
Iteration 51/1000 | Loss: 0.00001596
Iteration 52/1000 | Loss: 0.00001596
Iteration 53/1000 | Loss: 0.00001593
Iteration 54/1000 | Loss: 0.00001593
Iteration 55/1000 | Loss: 0.00001593
Iteration 56/1000 | Loss: 0.00001592
Iteration 57/1000 | Loss: 0.00001592
Iteration 58/1000 | Loss: 0.00001591
Iteration 59/1000 | Loss: 0.00001591
Iteration 60/1000 | Loss: 0.00001591
Iteration 61/1000 | Loss: 0.00001590
Iteration 62/1000 | Loss: 0.00001590
Iteration 63/1000 | Loss: 0.00001590
Iteration 64/1000 | Loss: 0.00001590
Iteration 65/1000 | Loss: 0.00001590
Iteration 66/1000 | Loss: 0.00001590
Iteration 67/1000 | Loss: 0.00001590
Iteration 68/1000 | Loss: 0.00001590
Iteration 69/1000 | Loss: 0.00001590
Iteration 70/1000 | Loss: 0.00001590
Iteration 71/1000 | Loss: 0.00001589
Iteration 72/1000 | Loss: 0.00001589
Iteration 73/1000 | Loss: 0.00001589
Iteration 74/1000 | Loss: 0.00001589
Iteration 75/1000 | Loss: 0.00001589
Iteration 76/1000 | Loss: 0.00001589
Iteration 77/1000 | Loss: 0.00001589
Iteration 78/1000 | Loss: 0.00001589
Iteration 79/1000 | Loss: 0.00001588
Iteration 80/1000 | Loss: 0.00001588
Iteration 81/1000 | Loss: 0.00001588
Iteration 82/1000 | Loss: 0.00001588
Iteration 83/1000 | Loss: 0.00001588
Iteration 84/1000 | Loss: 0.00001588
Iteration 85/1000 | Loss: 0.00001588
Iteration 86/1000 | Loss: 0.00001587
Iteration 87/1000 | Loss: 0.00001587
Iteration 88/1000 | Loss: 0.00001587
Iteration 89/1000 | Loss: 0.00001587
Iteration 90/1000 | Loss: 0.00001587
Iteration 91/1000 | Loss: 0.00001587
Iteration 92/1000 | Loss: 0.00001587
Iteration 93/1000 | Loss: 0.00001586
Iteration 94/1000 | Loss: 0.00001586
Iteration 95/1000 | Loss: 0.00001586
Iteration 96/1000 | Loss: 0.00001586
Iteration 97/1000 | Loss: 0.00001586
Iteration 98/1000 | Loss: 0.00001586
Iteration 99/1000 | Loss: 0.00001585
Iteration 100/1000 | Loss: 0.00001585
Iteration 101/1000 | Loss: 0.00001585
Iteration 102/1000 | Loss: 0.00001585
Iteration 103/1000 | Loss: 0.00001585
Iteration 104/1000 | Loss: 0.00001584
Iteration 105/1000 | Loss: 0.00001584
Iteration 106/1000 | Loss: 0.00001584
Iteration 107/1000 | Loss: 0.00001584
Iteration 108/1000 | Loss: 0.00001584
Iteration 109/1000 | Loss: 0.00001584
Iteration 110/1000 | Loss: 0.00001584
Iteration 111/1000 | Loss: 0.00001584
Iteration 112/1000 | Loss: 0.00001583
Iteration 113/1000 | Loss: 0.00001583
Iteration 114/1000 | Loss: 0.00001583
Iteration 115/1000 | Loss: 0.00001583
Iteration 116/1000 | Loss: 0.00001583
Iteration 117/1000 | Loss: 0.00001582
Iteration 118/1000 | Loss: 0.00001582
Iteration 119/1000 | Loss: 0.00001582
Iteration 120/1000 | Loss: 0.00001582
Iteration 121/1000 | Loss: 0.00001582
Iteration 122/1000 | Loss: 0.00001582
Iteration 123/1000 | Loss: 0.00001582
Iteration 124/1000 | Loss: 0.00001582
Iteration 125/1000 | Loss: 0.00001582
Iteration 126/1000 | Loss: 0.00001582
Iteration 127/1000 | Loss: 0.00001582
Iteration 128/1000 | Loss: 0.00001582
Iteration 129/1000 | Loss: 0.00001582
Iteration 130/1000 | Loss: 0.00001581
Iteration 131/1000 | Loss: 0.00001581
Iteration 132/1000 | Loss: 0.00001581
Iteration 133/1000 | Loss: 0.00001580
Iteration 134/1000 | Loss: 0.00001580
Iteration 135/1000 | Loss: 0.00001580
Iteration 136/1000 | Loss: 0.00001580
Iteration 137/1000 | Loss: 0.00001580
Iteration 138/1000 | Loss: 0.00001580
Iteration 139/1000 | Loss: 0.00001580
Iteration 140/1000 | Loss: 0.00001580
Iteration 141/1000 | Loss: 0.00001580
Iteration 142/1000 | Loss: 0.00001580
Iteration 143/1000 | Loss: 0.00001580
Iteration 144/1000 | Loss: 0.00001579
Iteration 145/1000 | Loss: 0.00001579
Iteration 146/1000 | Loss: 0.00001579
Iteration 147/1000 | Loss: 0.00001579
Iteration 148/1000 | Loss: 0.00001579
Iteration 149/1000 | Loss: 0.00001578
Iteration 150/1000 | Loss: 0.00001578
Iteration 151/1000 | Loss: 0.00001578
Iteration 152/1000 | Loss: 0.00001578
Iteration 153/1000 | Loss: 0.00001578
Iteration 154/1000 | Loss: 0.00001578
Iteration 155/1000 | Loss: 0.00001578
Iteration 156/1000 | Loss: 0.00001577
Iteration 157/1000 | Loss: 0.00001577
Iteration 158/1000 | Loss: 0.00001577
Iteration 159/1000 | Loss: 0.00001577
Iteration 160/1000 | Loss: 0.00001577
Iteration 161/1000 | Loss: 0.00001577
Iteration 162/1000 | Loss: 0.00001577
Iteration 163/1000 | Loss: 0.00001577
Iteration 164/1000 | Loss: 0.00001577
Iteration 165/1000 | Loss: 0.00001577
Iteration 166/1000 | Loss: 0.00001577
Iteration 167/1000 | Loss: 0.00001577
Iteration 168/1000 | Loss: 0.00001577
Iteration 169/1000 | Loss: 0.00001577
Iteration 170/1000 | Loss: 0.00001577
Iteration 171/1000 | Loss: 0.00001577
Iteration 172/1000 | Loss: 0.00001577
Iteration 173/1000 | Loss: 0.00001577
Iteration 174/1000 | Loss: 0.00001577
Iteration 175/1000 | Loss: 0.00001577
Iteration 176/1000 | Loss: 0.00001577
Iteration 177/1000 | Loss: 0.00001577
Iteration 178/1000 | Loss: 0.00001577
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [1.5767534932820126e-05, 1.5767534932820126e-05, 1.5767534932820126e-05, 1.5767534932820126e-05, 1.5767534932820126e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5767534932820126e-05

Optimization complete. Final v2v error: 3.3540091514587402 mm

Highest mean error: 3.9296863079071045 mm for frame 29

Lowest mean error: 3.0744290351867676 mm for frame 60

Saving results

Total time: 41.33429503440857
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01052622
Iteration 2/25 | Loss: 0.01052622
Iteration 3/25 | Loss: 0.01052622
Iteration 4/25 | Loss: 0.01052622
Iteration 5/25 | Loss: 0.01052622
Iteration 6/25 | Loss: 0.01052622
Iteration 7/25 | Loss: 0.01052622
Iteration 8/25 | Loss: 0.01052622
Iteration 9/25 | Loss: 0.01052622
Iteration 10/25 | Loss: 0.01052622
Iteration 11/25 | Loss: 0.01052621
Iteration 12/25 | Loss: 0.01052621
Iteration 13/25 | Loss: 0.01052621
Iteration 14/25 | Loss: 0.01052621
Iteration 15/25 | Loss: 0.01052621
Iteration 16/25 | Loss: 0.01052621
Iteration 17/25 | Loss: 0.01052621
Iteration 18/25 | Loss: 0.01052621
Iteration 19/25 | Loss: 0.01052621
Iteration 20/25 | Loss: 0.01052621
Iteration 21/25 | Loss: 0.01052620
Iteration 22/25 | Loss: 0.01052620
Iteration 23/25 | Loss: 0.01052620
Iteration 24/25 | Loss: 0.01052620
Iteration 25/25 | Loss: 0.01052620

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64224565
Iteration 2/25 | Loss: 0.09533113
Iteration 3/25 | Loss: 0.09511809
Iteration 4/25 | Loss: 0.09510037
Iteration 5/25 | Loss: 0.09510035
Iteration 6/25 | Loss: 0.09510035
Iteration 7/25 | Loss: 0.09510034
Iteration 8/25 | Loss: 0.09510033
Iteration 9/25 | Loss: 0.09510033
Iteration 10/25 | Loss: 0.09510033
Iteration 11/25 | Loss: 0.09510032
Iteration 12/25 | Loss: 0.09510032
Iteration 13/25 | Loss: 0.09510032
Iteration 14/25 | Loss: 0.09510032
Iteration 15/25 | Loss: 0.09510032
Iteration 16/25 | Loss: 0.09510032
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.09510032087564468, 0.09510032087564468, 0.09510032087564468, 0.09510032087564468, 0.09510032087564468]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.09510032087564468

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.09510032
Iteration 2/1000 | Loss: 0.00366533
Iteration 3/1000 | Loss: 0.00184444
Iteration 4/1000 | Loss: 0.00156187
Iteration 5/1000 | Loss: 0.00059723
Iteration 6/1000 | Loss: 0.00022552
Iteration 7/1000 | Loss: 0.00017955
Iteration 8/1000 | Loss: 0.00022051
Iteration 9/1000 | Loss: 0.00028953
Iteration 10/1000 | Loss: 0.00042977
Iteration 11/1000 | Loss: 0.00003794
Iteration 12/1000 | Loss: 0.00014224
Iteration 13/1000 | Loss: 0.00002727
Iteration 14/1000 | Loss: 0.00008246
Iteration 15/1000 | Loss: 0.00002762
Iteration 16/1000 | Loss: 0.00002182
Iteration 17/1000 | Loss: 0.00005732
Iteration 18/1000 | Loss: 0.00002170
Iteration 19/1000 | Loss: 0.00001704
Iteration 20/1000 | Loss: 0.00001594
Iteration 21/1000 | Loss: 0.00001509
Iteration 22/1000 | Loss: 0.00002298
Iteration 23/1000 | Loss: 0.00017430
Iteration 24/1000 | Loss: 0.00001487
Iteration 25/1000 | Loss: 0.00001358
Iteration 26/1000 | Loss: 0.00001993
Iteration 27/1000 | Loss: 0.00001289
Iteration 28/1000 | Loss: 0.00009388
Iteration 29/1000 | Loss: 0.00001285
Iteration 30/1000 | Loss: 0.00003008
Iteration 31/1000 | Loss: 0.00001220
Iteration 32/1000 | Loss: 0.00001286
Iteration 33/1000 | Loss: 0.00001187
Iteration 34/1000 | Loss: 0.00001186
Iteration 35/1000 | Loss: 0.00001185
Iteration 36/1000 | Loss: 0.00001184
Iteration 37/1000 | Loss: 0.00001183
Iteration 38/1000 | Loss: 0.00002139
Iteration 39/1000 | Loss: 0.00001178
Iteration 40/1000 | Loss: 0.00005231
Iteration 41/1000 | Loss: 0.00001157
Iteration 42/1000 | Loss: 0.00001140
Iteration 43/1000 | Loss: 0.00001126
Iteration 44/1000 | Loss: 0.00001122
Iteration 45/1000 | Loss: 0.00001258
Iteration 46/1000 | Loss: 0.00001258
Iteration 47/1000 | Loss: 0.00002020
Iteration 48/1000 | Loss: 0.00004404
Iteration 49/1000 | Loss: 0.00001997
Iteration 50/1000 | Loss: 0.00001135
Iteration 51/1000 | Loss: 0.00001104
Iteration 52/1000 | Loss: 0.00001104
Iteration 53/1000 | Loss: 0.00001104
Iteration 54/1000 | Loss: 0.00001104
Iteration 55/1000 | Loss: 0.00001103
Iteration 56/1000 | Loss: 0.00001103
Iteration 57/1000 | Loss: 0.00001102
Iteration 58/1000 | Loss: 0.00001101
Iteration 59/1000 | Loss: 0.00001124
Iteration 60/1000 | Loss: 0.00001097
Iteration 61/1000 | Loss: 0.00001097
Iteration 62/1000 | Loss: 0.00001097
Iteration 63/1000 | Loss: 0.00001097
Iteration 64/1000 | Loss: 0.00001097
Iteration 65/1000 | Loss: 0.00001097
Iteration 66/1000 | Loss: 0.00001097
Iteration 67/1000 | Loss: 0.00001097
Iteration 68/1000 | Loss: 0.00001096
Iteration 69/1000 | Loss: 0.00001096
Iteration 70/1000 | Loss: 0.00001096
Iteration 71/1000 | Loss: 0.00001096
Iteration 72/1000 | Loss: 0.00001096
Iteration 73/1000 | Loss: 0.00001096
Iteration 74/1000 | Loss: 0.00001095
Iteration 75/1000 | Loss: 0.00001095
Iteration 76/1000 | Loss: 0.00001147
Iteration 77/1000 | Loss: 0.00001093
Iteration 78/1000 | Loss: 0.00001093
Iteration 79/1000 | Loss: 0.00001092
Iteration 80/1000 | Loss: 0.00001092
Iteration 81/1000 | Loss: 0.00001092
Iteration 82/1000 | Loss: 0.00001092
Iteration 83/1000 | Loss: 0.00001092
Iteration 84/1000 | Loss: 0.00001092
Iteration 85/1000 | Loss: 0.00001092
Iteration 86/1000 | Loss: 0.00001092
Iteration 87/1000 | Loss: 0.00001145
Iteration 88/1000 | Loss: 0.00001091
Iteration 89/1000 | Loss: 0.00001091
Iteration 90/1000 | Loss: 0.00001091
Iteration 91/1000 | Loss: 0.00001091
Iteration 92/1000 | Loss: 0.00001091
Iteration 93/1000 | Loss: 0.00001090
Iteration 94/1000 | Loss: 0.00001090
Iteration 95/1000 | Loss: 0.00001090
Iteration 96/1000 | Loss: 0.00001090
Iteration 97/1000 | Loss: 0.00001090
Iteration 98/1000 | Loss: 0.00001090
Iteration 99/1000 | Loss: 0.00001090
Iteration 100/1000 | Loss: 0.00001090
Iteration 101/1000 | Loss: 0.00001090
Iteration 102/1000 | Loss: 0.00001090
Iteration 103/1000 | Loss: 0.00001089
Iteration 104/1000 | Loss: 0.00001089
Iteration 105/1000 | Loss: 0.00001089
Iteration 106/1000 | Loss: 0.00001089
Iteration 107/1000 | Loss: 0.00001089
Iteration 108/1000 | Loss: 0.00001089
Iteration 109/1000 | Loss: 0.00001089
Iteration 110/1000 | Loss: 0.00001089
Iteration 111/1000 | Loss: 0.00001089
Iteration 112/1000 | Loss: 0.00001089
Iteration 113/1000 | Loss: 0.00001089
Iteration 114/1000 | Loss: 0.00001089
Iteration 115/1000 | Loss: 0.00001089
Iteration 116/1000 | Loss: 0.00001089
Iteration 117/1000 | Loss: 0.00001089
Iteration 118/1000 | Loss: 0.00001089
Iteration 119/1000 | Loss: 0.00001089
Iteration 120/1000 | Loss: 0.00001089
Iteration 121/1000 | Loss: 0.00001089
Iteration 122/1000 | Loss: 0.00001089
Iteration 123/1000 | Loss: 0.00001089
Iteration 124/1000 | Loss: 0.00001089
Iteration 125/1000 | Loss: 0.00001089
Iteration 126/1000 | Loss: 0.00001089
Iteration 127/1000 | Loss: 0.00001089
Iteration 128/1000 | Loss: 0.00001089
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.0893897524510976e-05, 1.0893897524510976e-05, 1.0893897524510976e-05, 1.0893897524510976e-05, 1.0893897524510976e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0893897524510976e-05

Optimization complete. Final v2v error: 2.819113254547119 mm

Highest mean error: 3.5886588096618652 mm for frame 75

Lowest mean error: 2.384617805480957 mm for frame 97

Saving results

Total time: 82.87154936790466
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00388816
Iteration 2/25 | Loss: 0.00120339
Iteration 3/25 | Loss: 0.00112404
Iteration 4/25 | Loss: 0.00110417
Iteration 5/25 | Loss: 0.00109724
Iteration 6/25 | Loss: 0.00109592
Iteration 7/25 | Loss: 0.00109592
Iteration 8/25 | Loss: 0.00109592
Iteration 9/25 | Loss: 0.00109592
Iteration 10/25 | Loss: 0.00109592
Iteration 11/25 | Loss: 0.00109592
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001095923245884478, 0.001095923245884478, 0.001095923245884478, 0.001095923245884478, 0.001095923245884478]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001095923245884478

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.25781012
Iteration 2/25 | Loss: 0.00085889
Iteration 3/25 | Loss: 0.00085889
Iteration 4/25 | Loss: 0.00085889
Iteration 5/25 | Loss: 0.00085889
Iteration 6/25 | Loss: 0.00085889
Iteration 7/25 | Loss: 0.00085889
Iteration 8/25 | Loss: 0.00085889
Iteration 9/25 | Loss: 0.00085889
Iteration 10/25 | Loss: 0.00085889
Iteration 11/25 | Loss: 0.00085889
Iteration 12/25 | Loss: 0.00085889
Iteration 13/25 | Loss: 0.00085889
Iteration 14/25 | Loss: 0.00085889
Iteration 15/25 | Loss: 0.00085889
Iteration 16/25 | Loss: 0.00085889
Iteration 17/25 | Loss: 0.00085889
Iteration 18/25 | Loss: 0.00085889
Iteration 19/25 | Loss: 0.00085889
Iteration 20/25 | Loss: 0.00085889
Iteration 21/25 | Loss: 0.00085889
Iteration 22/25 | Loss: 0.00085889
Iteration 23/25 | Loss: 0.00085889
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008588864584453404, 0.0008588864584453404, 0.0008588864584453404, 0.0008588864584453404, 0.0008588864584453404]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008588864584453404

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085889
Iteration 2/1000 | Loss: 0.00002598
Iteration 3/1000 | Loss: 0.00001890
Iteration 4/1000 | Loss: 0.00001777
Iteration 5/1000 | Loss: 0.00001728
Iteration 6/1000 | Loss: 0.00001688
Iteration 7/1000 | Loss: 0.00001647
Iteration 8/1000 | Loss: 0.00001625
Iteration 9/1000 | Loss: 0.00001598
Iteration 10/1000 | Loss: 0.00001575
Iteration 11/1000 | Loss: 0.00001569
Iteration 12/1000 | Loss: 0.00001568
Iteration 13/1000 | Loss: 0.00001553
Iteration 14/1000 | Loss: 0.00001543
Iteration 15/1000 | Loss: 0.00001542
Iteration 16/1000 | Loss: 0.00001536
Iteration 17/1000 | Loss: 0.00001533
Iteration 18/1000 | Loss: 0.00001532
Iteration 19/1000 | Loss: 0.00001532
Iteration 20/1000 | Loss: 0.00001525
Iteration 21/1000 | Loss: 0.00001524
Iteration 22/1000 | Loss: 0.00001523
Iteration 23/1000 | Loss: 0.00001523
Iteration 24/1000 | Loss: 0.00001522
Iteration 25/1000 | Loss: 0.00001522
Iteration 26/1000 | Loss: 0.00001522
Iteration 27/1000 | Loss: 0.00001518
Iteration 28/1000 | Loss: 0.00001518
Iteration 29/1000 | Loss: 0.00001518
Iteration 30/1000 | Loss: 0.00001518
Iteration 31/1000 | Loss: 0.00001517
Iteration 32/1000 | Loss: 0.00001517
Iteration 33/1000 | Loss: 0.00001517
Iteration 34/1000 | Loss: 0.00001513
Iteration 35/1000 | Loss: 0.00001512
Iteration 36/1000 | Loss: 0.00001512
Iteration 37/1000 | Loss: 0.00001509
Iteration 38/1000 | Loss: 0.00001509
Iteration 39/1000 | Loss: 0.00001509
Iteration 40/1000 | Loss: 0.00001509
Iteration 41/1000 | Loss: 0.00001509
Iteration 42/1000 | Loss: 0.00001509
Iteration 43/1000 | Loss: 0.00001508
Iteration 44/1000 | Loss: 0.00001508
Iteration 45/1000 | Loss: 0.00001505
Iteration 46/1000 | Loss: 0.00001504
Iteration 47/1000 | Loss: 0.00001504
Iteration 48/1000 | Loss: 0.00001504
Iteration 49/1000 | Loss: 0.00001504
Iteration 50/1000 | Loss: 0.00001503
Iteration 51/1000 | Loss: 0.00001503
Iteration 52/1000 | Loss: 0.00001502
Iteration 53/1000 | Loss: 0.00001500
Iteration 54/1000 | Loss: 0.00001499
Iteration 55/1000 | Loss: 0.00001499
Iteration 56/1000 | Loss: 0.00001498
Iteration 57/1000 | Loss: 0.00001498
Iteration 58/1000 | Loss: 0.00001497
Iteration 59/1000 | Loss: 0.00001497
Iteration 60/1000 | Loss: 0.00001497
Iteration 61/1000 | Loss: 0.00001497
Iteration 62/1000 | Loss: 0.00001496
Iteration 63/1000 | Loss: 0.00001496
Iteration 64/1000 | Loss: 0.00001496
Iteration 65/1000 | Loss: 0.00001496
Iteration 66/1000 | Loss: 0.00001496
Iteration 67/1000 | Loss: 0.00001495
Iteration 68/1000 | Loss: 0.00001495
Iteration 69/1000 | Loss: 0.00001495
Iteration 70/1000 | Loss: 0.00001495
Iteration 71/1000 | Loss: 0.00001495
Iteration 72/1000 | Loss: 0.00001495
Iteration 73/1000 | Loss: 0.00001495
Iteration 74/1000 | Loss: 0.00001495
Iteration 75/1000 | Loss: 0.00001495
Iteration 76/1000 | Loss: 0.00001495
Iteration 77/1000 | Loss: 0.00001495
Iteration 78/1000 | Loss: 0.00001495
Iteration 79/1000 | Loss: 0.00001495
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 79. Stopping optimization.
Last 5 losses: [1.4952220226405188e-05, 1.4952220226405188e-05, 1.4952220226405188e-05, 1.4952220226405188e-05, 1.4952220226405188e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4952220226405188e-05

Optimization complete. Final v2v error: 3.294381856918335 mm

Highest mean error: 3.588548183441162 mm for frame 109

Lowest mean error: 3.1581571102142334 mm for frame 0

Saving results

Total time: 36.71532940864563
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00930438
Iteration 2/25 | Loss: 0.00132690
Iteration 3/25 | Loss: 0.00121522
Iteration 4/25 | Loss: 0.00119356
Iteration 5/25 | Loss: 0.00118624
Iteration 6/25 | Loss: 0.00118432
Iteration 7/25 | Loss: 0.00118369
Iteration 8/25 | Loss: 0.00118369
Iteration 9/25 | Loss: 0.00118369
Iteration 10/25 | Loss: 0.00118369
Iteration 11/25 | Loss: 0.00118369
Iteration 12/25 | Loss: 0.00118369
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001183686195872724, 0.001183686195872724, 0.001183686195872724, 0.001183686195872724, 0.001183686195872724]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001183686195872724

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.16334495
Iteration 2/25 | Loss: 0.00085215
Iteration 3/25 | Loss: 0.00085215
Iteration 4/25 | Loss: 0.00085215
Iteration 5/25 | Loss: 0.00085215
Iteration 6/25 | Loss: 0.00085215
Iteration 7/25 | Loss: 0.00085215
Iteration 8/25 | Loss: 0.00085215
Iteration 9/25 | Loss: 0.00085215
Iteration 10/25 | Loss: 0.00085215
Iteration 11/25 | Loss: 0.00085215
Iteration 12/25 | Loss: 0.00085215
Iteration 13/25 | Loss: 0.00085215
Iteration 14/25 | Loss: 0.00085215
Iteration 15/25 | Loss: 0.00085215
Iteration 16/25 | Loss: 0.00085215
Iteration 17/25 | Loss: 0.00085215
Iteration 18/25 | Loss: 0.00085215
Iteration 19/25 | Loss: 0.00085215
Iteration 20/25 | Loss: 0.00085215
Iteration 21/25 | Loss: 0.00085215
Iteration 22/25 | Loss: 0.00085215
Iteration 23/25 | Loss: 0.00085215
Iteration 24/25 | Loss: 0.00085215
Iteration 25/25 | Loss: 0.00085215

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085215
Iteration 2/1000 | Loss: 0.00005972
Iteration 3/1000 | Loss: 0.00004068
Iteration 4/1000 | Loss: 0.00003483
Iteration 5/1000 | Loss: 0.00003101
Iteration 6/1000 | Loss: 0.00002940
Iteration 7/1000 | Loss: 0.00002827
Iteration 8/1000 | Loss: 0.00002781
Iteration 9/1000 | Loss: 0.00002732
Iteration 10/1000 | Loss: 0.00002685
Iteration 11/1000 | Loss: 0.00002651
Iteration 12/1000 | Loss: 0.00002636
Iteration 13/1000 | Loss: 0.00002636
Iteration 14/1000 | Loss: 0.00002614
Iteration 15/1000 | Loss: 0.00002597
Iteration 16/1000 | Loss: 0.00002589
Iteration 17/1000 | Loss: 0.00002576
Iteration 18/1000 | Loss: 0.00002557
Iteration 19/1000 | Loss: 0.00002556
Iteration 20/1000 | Loss: 0.00002546
Iteration 21/1000 | Loss: 0.00002537
Iteration 22/1000 | Loss: 0.00002534
Iteration 23/1000 | Loss: 0.00002531
Iteration 24/1000 | Loss: 0.00002531
Iteration 25/1000 | Loss: 0.00002531
Iteration 26/1000 | Loss: 0.00002531
Iteration 27/1000 | Loss: 0.00002531
Iteration 28/1000 | Loss: 0.00002531
Iteration 29/1000 | Loss: 0.00002531
Iteration 30/1000 | Loss: 0.00002531
Iteration 31/1000 | Loss: 0.00002531
Iteration 32/1000 | Loss: 0.00002530
Iteration 33/1000 | Loss: 0.00002530
Iteration 34/1000 | Loss: 0.00002530
Iteration 35/1000 | Loss: 0.00002530
Iteration 36/1000 | Loss: 0.00002530
Iteration 37/1000 | Loss: 0.00002530
Iteration 38/1000 | Loss: 0.00002530
Iteration 39/1000 | Loss: 0.00002530
Iteration 40/1000 | Loss: 0.00002528
Iteration 41/1000 | Loss: 0.00002528
Iteration 42/1000 | Loss: 0.00002523
Iteration 43/1000 | Loss: 0.00002523
Iteration 44/1000 | Loss: 0.00002523
Iteration 45/1000 | Loss: 0.00002523
Iteration 46/1000 | Loss: 0.00002523
Iteration 47/1000 | Loss: 0.00002523
Iteration 48/1000 | Loss: 0.00002523
Iteration 49/1000 | Loss: 0.00002523
Iteration 50/1000 | Loss: 0.00002522
Iteration 51/1000 | Loss: 0.00002520
Iteration 52/1000 | Loss: 0.00002519
Iteration 53/1000 | Loss: 0.00002518
Iteration 54/1000 | Loss: 0.00002518
Iteration 55/1000 | Loss: 0.00002518
Iteration 56/1000 | Loss: 0.00002518
Iteration 57/1000 | Loss: 0.00002518
Iteration 58/1000 | Loss: 0.00002518
Iteration 59/1000 | Loss: 0.00002518
Iteration 60/1000 | Loss: 0.00002517
Iteration 61/1000 | Loss: 0.00002516
Iteration 62/1000 | Loss: 0.00002516
Iteration 63/1000 | Loss: 0.00002516
Iteration 64/1000 | Loss: 0.00002516
Iteration 65/1000 | Loss: 0.00002515
Iteration 66/1000 | Loss: 0.00002515
Iteration 67/1000 | Loss: 0.00002515
Iteration 68/1000 | Loss: 0.00002515
Iteration 69/1000 | Loss: 0.00002515
Iteration 70/1000 | Loss: 0.00002515
Iteration 71/1000 | Loss: 0.00002515
Iteration 72/1000 | Loss: 0.00002515
Iteration 73/1000 | Loss: 0.00002515
Iteration 74/1000 | Loss: 0.00002515
Iteration 75/1000 | Loss: 0.00002515
Iteration 76/1000 | Loss: 0.00002514
Iteration 77/1000 | Loss: 0.00002514
Iteration 78/1000 | Loss: 0.00002514
Iteration 79/1000 | Loss: 0.00002514
Iteration 80/1000 | Loss: 0.00002514
Iteration 81/1000 | Loss: 0.00002514
Iteration 82/1000 | Loss: 0.00002514
Iteration 83/1000 | Loss: 0.00002513
Iteration 84/1000 | Loss: 0.00002513
Iteration 85/1000 | Loss: 0.00002513
Iteration 86/1000 | Loss: 0.00002513
Iteration 87/1000 | Loss: 0.00002513
Iteration 88/1000 | Loss: 0.00002512
Iteration 89/1000 | Loss: 0.00002512
Iteration 90/1000 | Loss: 0.00002512
Iteration 91/1000 | Loss: 0.00002512
Iteration 92/1000 | Loss: 0.00002512
Iteration 93/1000 | Loss: 0.00002511
Iteration 94/1000 | Loss: 0.00002511
Iteration 95/1000 | Loss: 0.00002511
Iteration 96/1000 | Loss: 0.00002511
Iteration 97/1000 | Loss: 0.00002511
Iteration 98/1000 | Loss: 0.00002511
Iteration 99/1000 | Loss: 0.00002511
Iteration 100/1000 | Loss: 0.00002511
Iteration 101/1000 | Loss: 0.00002511
Iteration 102/1000 | Loss: 0.00002510
Iteration 103/1000 | Loss: 0.00002510
Iteration 104/1000 | Loss: 0.00002510
Iteration 105/1000 | Loss: 0.00002510
Iteration 106/1000 | Loss: 0.00002510
Iteration 107/1000 | Loss: 0.00002510
Iteration 108/1000 | Loss: 0.00002510
Iteration 109/1000 | Loss: 0.00002510
Iteration 110/1000 | Loss: 0.00002510
Iteration 111/1000 | Loss: 0.00002510
Iteration 112/1000 | Loss: 0.00002510
Iteration 113/1000 | Loss: 0.00002510
Iteration 114/1000 | Loss: 0.00002510
Iteration 115/1000 | Loss: 0.00002510
Iteration 116/1000 | Loss: 0.00002510
Iteration 117/1000 | Loss: 0.00002510
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [2.510047488613054e-05, 2.510047488613054e-05, 2.510047488613054e-05, 2.510047488613054e-05, 2.510047488613054e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.510047488613054e-05

Optimization complete. Final v2v error: 4.134006500244141 mm

Highest mean error: 4.428312301635742 mm for frame 48

Lowest mean error: 3.727928400039673 mm for frame 9

Saving results

Total time: 41.044772148132324
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00964972
Iteration 2/25 | Loss: 0.00253966
Iteration 3/25 | Loss: 0.00195331
Iteration 4/25 | Loss: 0.00177492
Iteration 5/25 | Loss: 0.00132994
Iteration 6/25 | Loss: 0.00123309
Iteration 7/25 | Loss: 0.00121683
Iteration 8/25 | Loss: 0.00121006
Iteration 9/25 | Loss: 0.00120706
Iteration 10/25 | Loss: 0.00119631
Iteration 11/25 | Loss: 0.00119942
Iteration 12/25 | Loss: 0.00118991
Iteration 13/25 | Loss: 0.00119862
Iteration 14/25 | Loss: 0.00118448
Iteration 15/25 | Loss: 0.00118299
Iteration 16/25 | Loss: 0.00118267
Iteration 17/25 | Loss: 0.00118260
Iteration 18/25 | Loss: 0.00118257
Iteration 19/25 | Loss: 0.00118257
Iteration 20/25 | Loss: 0.00118257
Iteration 21/25 | Loss: 0.00118257
Iteration 22/25 | Loss: 0.00118257
Iteration 23/25 | Loss: 0.00118257
Iteration 24/25 | Loss: 0.00118257
Iteration 25/25 | Loss: 0.00118256

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35939133
Iteration 2/25 | Loss: 0.00053692
Iteration 3/25 | Loss: 0.00053691
Iteration 4/25 | Loss: 0.00053691
Iteration 5/25 | Loss: 0.00053691
Iteration 6/25 | Loss: 0.00053691
Iteration 7/25 | Loss: 0.00053691
Iteration 8/25 | Loss: 0.00053691
Iteration 9/25 | Loss: 0.00053691
Iteration 10/25 | Loss: 0.00053691
Iteration 11/25 | Loss: 0.00053691
Iteration 12/25 | Loss: 0.00053691
Iteration 13/25 | Loss: 0.00053691
Iteration 14/25 | Loss: 0.00053691
Iteration 15/25 | Loss: 0.00053691
Iteration 16/25 | Loss: 0.00053691
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005369106656871736, 0.0005369106656871736, 0.0005369106656871736, 0.0005369106656871736, 0.0005369106656871736]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005369106656871736

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053691
Iteration 2/1000 | Loss: 0.00003531
Iteration 3/1000 | Loss: 0.00002347
Iteration 4/1000 | Loss: 0.00002192
Iteration 5/1000 | Loss: 0.00002073
Iteration 6/1000 | Loss: 0.00002016
Iteration 7/1000 | Loss: 0.00001980
Iteration 8/1000 | Loss: 0.00001951
Iteration 9/1000 | Loss: 0.00001928
Iteration 10/1000 | Loss: 0.00001909
Iteration 11/1000 | Loss: 0.00001889
Iteration 12/1000 | Loss: 0.00001876
Iteration 13/1000 | Loss: 0.00001875
Iteration 14/1000 | Loss: 0.00001870
Iteration 15/1000 | Loss: 0.00001869
Iteration 16/1000 | Loss: 0.00001869
Iteration 17/1000 | Loss: 0.00001868
Iteration 18/1000 | Loss: 0.00001867
Iteration 19/1000 | Loss: 0.00001867
Iteration 20/1000 | Loss: 0.00001867
Iteration 21/1000 | Loss: 0.00001866
Iteration 22/1000 | Loss: 0.00001866
Iteration 23/1000 | Loss: 0.00001865
Iteration 24/1000 | Loss: 0.00001853
Iteration 25/1000 | Loss: 0.00001849
Iteration 26/1000 | Loss: 0.00001847
Iteration 27/1000 | Loss: 0.00001847
Iteration 28/1000 | Loss: 0.00001847
Iteration 29/1000 | Loss: 0.00001847
Iteration 30/1000 | Loss: 0.00001845
Iteration 31/1000 | Loss: 0.00001844
Iteration 32/1000 | Loss: 0.00001843
Iteration 33/1000 | Loss: 0.00001843
Iteration 34/1000 | Loss: 0.00001842
Iteration 35/1000 | Loss: 0.00001842
Iteration 36/1000 | Loss: 0.00001842
Iteration 37/1000 | Loss: 0.00001841
Iteration 38/1000 | Loss: 0.00001840
Iteration 39/1000 | Loss: 0.00001839
Iteration 40/1000 | Loss: 0.00001839
Iteration 41/1000 | Loss: 0.00001839
Iteration 42/1000 | Loss: 0.00001839
Iteration 43/1000 | Loss: 0.00001839
Iteration 44/1000 | Loss: 0.00001839
Iteration 45/1000 | Loss: 0.00001839
Iteration 46/1000 | Loss: 0.00001839
Iteration 47/1000 | Loss: 0.00001839
Iteration 48/1000 | Loss: 0.00001839
Iteration 49/1000 | Loss: 0.00001839
Iteration 50/1000 | Loss: 0.00001838
Iteration 51/1000 | Loss: 0.00001838
Iteration 52/1000 | Loss: 0.00001838
Iteration 53/1000 | Loss: 0.00001837
Iteration 54/1000 | Loss: 0.00001837
Iteration 55/1000 | Loss: 0.00001837
Iteration 56/1000 | Loss: 0.00001837
Iteration 57/1000 | Loss: 0.00001837
Iteration 58/1000 | Loss: 0.00001837
Iteration 59/1000 | Loss: 0.00001837
Iteration 60/1000 | Loss: 0.00001837
Iteration 61/1000 | Loss: 0.00001837
Iteration 62/1000 | Loss: 0.00001837
Iteration 63/1000 | Loss: 0.00001837
Iteration 64/1000 | Loss: 0.00001837
Iteration 65/1000 | Loss: 0.00001837
Iteration 66/1000 | Loss: 0.00001837
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 66. Stopping optimization.
Last 5 losses: [1.8372378690401092e-05, 1.8372378690401092e-05, 1.8372378690401092e-05, 1.8372378690401092e-05, 1.8372378690401092e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8372378690401092e-05

Optimization complete. Final v2v error: 3.6464662551879883 mm

Highest mean error: 3.7872672080993652 mm for frame 0

Lowest mean error: 3.41457200050354 mm for frame 17

Saving results

Total time: 51.553797483444214
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01048158
Iteration 2/25 | Loss: 0.01048158
Iteration 3/25 | Loss: 0.00176045
Iteration 4/25 | Loss: 0.00135699
Iteration 5/25 | Loss: 0.00127660
Iteration 6/25 | Loss: 0.00132367
Iteration 7/25 | Loss: 0.00122334
Iteration 8/25 | Loss: 0.00116004
Iteration 9/25 | Loss: 0.00114562
Iteration 10/25 | Loss: 0.00114315
Iteration 11/25 | Loss: 0.00114269
Iteration 12/25 | Loss: 0.00114269
Iteration 13/25 | Loss: 0.00114269
Iteration 14/25 | Loss: 0.00114269
Iteration 15/25 | Loss: 0.00114269
Iteration 16/25 | Loss: 0.00114269
Iteration 17/25 | Loss: 0.00114269
Iteration 18/25 | Loss: 0.00114269
Iteration 19/25 | Loss: 0.00114269
Iteration 20/25 | Loss: 0.00114269
Iteration 21/25 | Loss: 0.00114269
Iteration 22/25 | Loss: 0.00114269
Iteration 23/25 | Loss: 0.00114269
Iteration 24/25 | Loss: 0.00114269
Iteration 25/25 | Loss: 0.00114269
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0011426920536905527, 0.0011426920536905527, 0.0011426920536905527, 0.0011426920536905527, 0.0011426920536905527]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011426920536905527

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40280116
Iteration 2/25 | Loss: 0.00074451
Iteration 3/25 | Loss: 0.00074451
Iteration 4/25 | Loss: 0.00074451
Iteration 5/25 | Loss: 0.00074451
Iteration 6/25 | Loss: 0.00074451
Iteration 7/25 | Loss: 0.00074451
Iteration 8/25 | Loss: 0.00074451
Iteration 9/25 | Loss: 0.00074451
Iteration 10/25 | Loss: 0.00074451
Iteration 11/25 | Loss: 0.00074451
Iteration 12/25 | Loss: 0.00074451
Iteration 13/25 | Loss: 0.00074451
Iteration 14/25 | Loss: 0.00074451
Iteration 15/25 | Loss: 0.00074451
Iteration 16/25 | Loss: 0.00074451
Iteration 17/25 | Loss: 0.00074451
Iteration 18/25 | Loss: 0.00074451
Iteration 19/25 | Loss: 0.00074451
Iteration 20/25 | Loss: 0.00074451
Iteration 21/25 | Loss: 0.00074451
Iteration 22/25 | Loss: 0.00074451
Iteration 23/25 | Loss: 0.00074451
Iteration 24/25 | Loss: 0.00074451
Iteration 25/25 | Loss: 0.00074451

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074451
Iteration 2/1000 | Loss: 0.00002374
Iteration 3/1000 | Loss: 0.00002016
Iteration 4/1000 | Loss: 0.00001898
Iteration 5/1000 | Loss: 0.00001831
Iteration 6/1000 | Loss: 0.00001796
Iteration 7/1000 | Loss: 0.00001764
Iteration 8/1000 | Loss: 0.00001748
Iteration 9/1000 | Loss: 0.00001744
Iteration 10/1000 | Loss: 0.00001724
Iteration 11/1000 | Loss: 0.00001705
Iteration 12/1000 | Loss: 0.00001694
Iteration 13/1000 | Loss: 0.00001693
Iteration 14/1000 | Loss: 0.00001681
Iteration 15/1000 | Loss: 0.00001679
Iteration 16/1000 | Loss: 0.00001676
Iteration 17/1000 | Loss: 0.00001672
Iteration 18/1000 | Loss: 0.00001671
Iteration 19/1000 | Loss: 0.00001670
Iteration 20/1000 | Loss: 0.00001670
Iteration 21/1000 | Loss: 0.00001669
Iteration 22/1000 | Loss: 0.00001669
Iteration 23/1000 | Loss: 0.00001668
Iteration 24/1000 | Loss: 0.00001668
Iteration 25/1000 | Loss: 0.00001667
Iteration 26/1000 | Loss: 0.00001666
Iteration 27/1000 | Loss: 0.00001665
Iteration 28/1000 | Loss: 0.00001664
Iteration 29/1000 | Loss: 0.00001663
Iteration 30/1000 | Loss: 0.00001663
Iteration 31/1000 | Loss: 0.00001662
Iteration 32/1000 | Loss: 0.00001662
Iteration 33/1000 | Loss: 0.00001661
Iteration 34/1000 | Loss: 0.00001656
Iteration 35/1000 | Loss: 0.00001653
Iteration 36/1000 | Loss: 0.00001652
Iteration 37/1000 | Loss: 0.00001652
Iteration 38/1000 | Loss: 0.00001648
Iteration 39/1000 | Loss: 0.00001648
Iteration 40/1000 | Loss: 0.00001647
Iteration 41/1000 | Loss: 0.00001645
Iteration 42/1000 | Loss: 0.00001645
Iteration 43/1000 | Loss: 0.00001645
Iteration 44/1000 | Loss: 0.00001645
Iteration 45/1000 | Loss: 0.00001644
Iteration 46/1000 | Loss: 0.00001644
Iteration 47/1000 | Loss: 0.00001644
Iteration 48/1000 | Loss: 0.00001644
Iteration 49/1000 | Loss: 0.00001644
Iteration 50/1000 | Loss: 0.00001643
Iteration 51/1000 | Loss: 0.00001643
Iteration 52/1000 | Loss: 0.00001643
Iteration 53/1000 | Loss: 0.00001642
Iteration 54/1000 | Loss: 0.00001642
Iteration 55/1000 | Loss: 0.00001642
Iteration 56/1000 | Loss: 0.00001642
Iteration 57/1000 | Loss: 0.00001642
Iteration 58/1000 | Loss: 0.00001641
Iteration 59/1000 | Loss: 0.00001641
Iteration 60/1000 | Loss: 0.00001641
Iteration 61/1000 | Loss: 0.00001641
Iteration 62/1000 | Loss: 0.00001641
Iteration 63/1000 | Loss: 0.00001640
Iteration 64/1000 | Loss: 0.00001640
Iteration 65/1000 | Loss: 0.00001640
Iteration 66/1000 | Loss: 0.00001640
Iteration 67/1000 | Loss: 0.00001640
Iteration 68/1000 | Loss: 0.00001640
Iteration 69/1000 | Loss: 0.00001640
Iteration 70/1000 | Loss: 0.00001640
Iteration 71/1000 | Loss: 0.00001640
Iteration 72/1000 | Loss: 0.00001640
Iteration 73/1000 | Loss: 0.00001640
Iteration 74/1000 | Loss: 0.00001640
Iteration 75/1000 | Loss: 0.00001640
Iteration 76/1000 | Loss: 0.00001640
Iteration 77/1000 | Loss: 0.00001639
Iteration 78/1000 | Loss: 0.00001639
Iteration 79/1000 | Loss: 0.00001639
Iteration 80/1000 | Loss: 0.00001639
Iteration 81/1000 | Loss: 0.00001639
Iteration 82/1000 | Loss: 0.00001639
Iteration 83/1000 | Loss: 0.00001639
Iteration 84/1000 | Loss: 0.00001639
Iteration 85/1000 | Loss: 0.00001639
Iteration 86/1000 | Loss: 0.00001639
Iteration 87/1000 | Loss: 0.00001639
Iteration 88/1000 | Loss: 0.00001639
Iteration 89/1000 | Loss: 0.00001639
Iteration 90/1000 | Loss: 0.00001639
Iteration 91/1000 | Loss: 0.00001639
Iteration 92/1000 | Loss: 0.00001639
Iteration 93/1000 | Loss: 0.00001638
Iteration 94/1000 | Loss: 0.00001638
Iteration 95/1000 | Loss: 0.00001638
Iteration 96/1000 | Loss: 0.00001638
Iteration 97/1000 | Loss: 0.00001638
Iteration 98/1000 | Loss: 0.00001638
Iteration 99/1000 | Loss: 0.00001637
Iteration 100/1000 | Loss: 0.00001637
Iteration 101/1000 | Loss: 0.00001637
Iteration 102/1000 | Loss: 0.00001637
Iteration 103/1000 | Loss: 0.00001637
Iteration 104/1000 | Loss: 0.00001637
Iteration 105/1000 | Loss: 0.00001637
Iteration 106/1000 | Loss: 0.00001637
Iteration 107/1000 | Loss: 0.00001637
Iteration 108/1000 | Loss: 0.00001636
Iteration 109/1000 | Loss: 0.00001636
Iteration 110/1000 | Loss: 0.00001636
Iteration 111/1000 | Loss: 0.00001636
Iteration 112/1000 | Loss: 0.00001636
Iteration 113/1000 | Loss: 0.00001635
Iteration 114/1000 | Loss: 0.00001635
Iteration 115/1000 | Loss: 0.00001635
Iteration 116/1000 | Loss: 0.00001635
Iteration 117/1000 | Loss: 0.00001635
Iteration 118/1000 | Loss: 0.00001634
Iteration 119/1000 | Loss: 0.00001634
Iteration 120/1000 | Loss: 0.00001634
Iteration 121/1000 | Loss: 0.00001634
Iteration 122/1000 | Loss: 0.00001634
Iteration 123/1000 | Loss: 0.00001634
Iteration 124/1000 | Loss: 0.00001633
Iteration 125/1000 | Loss: 0.00001633
Iteration 126/1000 | Loss: 0.00001633
Iteration 127/1000 | Loss: 0.00001633
Iteration 128/1000 | Loss: 0.00001633
Iteration 129/1000 | Loss: 0.00001633
Iteration 130/1000 | Loss: 0.00001633
Iteration 131/1000 | Loss: 0.00001633
Iteration 132/1000 | Loss: 0.00001632
Iteration 133/1000 | Loss: 0.00001632
Iteration 134/1000 | Loss: 0.00001632
Iteration 135/1000 | Loss: 0.00001632
Iteration 136/1000 | Loss: 0.00001632
Iteration 137/1000 | Loss: 0.00001632
Iteration 138/1000 | Loss: 0.00001632
Iteration 139/1000 | Loss: 0.00001632
Iteration 140/1000 | Loss: 0.00001632
Iteration 141/1000 | Loss: 0.00001632
Iteration 142/1000 | Loss: 0.00001632
Iteration 143/1000 | Loss: 0.00001632
Iteration 144/1000 | Loss: 0.00001632
Iteration 145/1000 | Loss: 0.00001632
Iteration 146/1000 | Loss: 0.00001632
Iteration 147/1000 | Loss: 0.00001632
Iteration 148/1000 | Loss: 0.00001632
Iteration 149/1000 | Loss: 0.00001632
Iteration 150/1000 | Loss: 0.00001632
Iteration 151/1000 | Loss: 0.00001632
Iteration 152/1000 | Loss: 0.00001632
Iteration 153/1000 | Loss: 0.00001632
Iteration 154/1000 | Loss: 0.00001631
Iteration 155/1000 | Loss: 0.00001631
Iteration 156/1000 | Loss: 0.00001631
Iteration 157/1000 | Loss: 0.00001631
Iteration 158/1000 | Loss: 0.00001631
Iteration 159/1000 | Loss: 0.00001631
Iteration 160/1000 | Loss: 0.00001631
Iteration 161/1000 | Loss: 0.00001631
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.6314996173605323e-05, 1.6314996173605323e-05, 1.6314996173605323e-05, 1.6314996173605323e-05, 1.6314996173605323e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6314996173605323e-05

Optimization complete. Final v2v error: 3.400566816329956 mm

Highest mean error: 3.9446921348571777 mm for frame 47

Lowest mean error: 3.044095039367676 mm for frame 151

Saving results

Total time: 50.715572357177734
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01057398
Iteration 2/25 | Loss: 0.00197962
Iteration 3/25 | Loss: 0.00136233
Iteration 4/25 | Loss: 0.00131962
Iteration 5/25 | Loss: 0.00131515
Iteration 6/25 | Loss: 0.00131501
Iteration 7/25 | Loss: 0.00131501
Iteration 8/25 | Loss: 0.00131501
Iteration 9/25 | Loss: 0.00131501
Iteration 10/25 | Loss: 0.00131501
Iteration 11/25 | Loss: 0.00131501
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001315009780228138, 0.001315009780228138, 0.001315009780228138, 0.001315009780228138, 0.001315009780228138]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001315009780228138

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.85709292
Iteration 2/25 | Loss: 0.00057923
Iteration 3/25 | Loss: 0.00057922
Iteration 4/25 | Loss: 0.00057922
Iteration 5/25 | Loss: 0.00057922
Iteration 6/25 | Loss: 0.00057922
Iteration 7/25 | Loss: 0.00057922
Iteration 8/25 | Loss: 0.00057922
Iteration 9/25 | Loss: 0.00057922
Iteration 10/25 | Loss: 0.00057922
Iteration 11/25 | Loss: 0.00057922
Iteration 12/25 | Loss: 0.00057922
Iteration 13/25 | Loss: 0.00057922
Iteration 14/25 | Loss: 0.00057922
Iteration 15/25 | Loss: 0.00057922
Iteration 16/25 | Loss: 0.00057922
Iteration 17/25 | Loss: 0.00057922
Iteration 18/25 | Loss: 0.00057922
Iteration 19/25 | Loss: 0.00057922
Iteration 20/25 | Loss: 0.00057922
Iteration 21/25 | Loss: 0.00057922
Iteration 22/25 | Loss: 0.00057922
Iteration 23/25 | Loss: 0.00057922
Iteration 24/25 | Loss: 0.00057922
Iteration 25/25 | Loss: 0.00057922

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057922
Iteration 2/1000 | Loss: 0.00006325
Iteration 3/1000 | Loss: 0.00004327
Iteration 4/1000 | Loss: 0.00003848
Iteration 5/1000 | Loss: 0.00003729
Iteration 6/1000 | Loss: 0.00003612
Iteration 7/1000 | Loss: 0.00003526
Iteration 8/1000 | Loss: 0.00003475
Iteration 9/1000 | Loss: 0.00003428
Iteration 10/1000 | Loss: 0.00003378
Iteration 11/1000 | Loss: 0.00003345
Iteration 12/1000 | Loss: 0.00003313
Iteration 13/1000 | Loss: 0.00003282
Iteration 14/1000 | Loss: 0.00003264
Iteration 15/1000 | Loss: 0.00003245
Iteration 16/1000 | Loss: 0.00003235
Iteration 17/1000 | Loss: 0.00003224
Iteration 18/1000 | Loss: 0.00003219
Iteration 19/1000 | Loss: 0.00003219
Iteration 20/1000 | Loss: 0.00003218
Iteration 21/1000 | Loss: 0.00003218
Iteration 22/1000 | Loss: 0.00003214
Iteration 23/1000 | Loss: 0.00003214
Iteration 24/1000 | Loss: 0.00003214
Iteration 25/1000 | Loss: 0.00003213
Iteration 26/1000 | Loss: 0.00003213
Iteration 27/1000 | Loss: 0.00003213
Iteration 28/1000 | Loss: 0.00003212
Iteration 29/1000 | Loss: 0.00003210
Iteration 30/1000 | Loss: 0.00003209
Iteration 31/1000 | Loss: 0.00003209
Iteration 32/1000 | Loss: 0.00003208
Iteration 33/1000 | Loss: 0.00003207
Iteration 34/1000 | Loss: 0.00003203
Iteration 35/1000 | Loss: 0.00003203
Iteration 36/1000 | Loss: 0.00003203
Iteration 37/1000 | Loss: 0.00003203
Iteration 38/1000 | Loss: 0.00003203
Iteration 39/1000 | Loss: 0.00003203
Iteration 40/1000 | Loss: 0.00003203
Iteration 41/1000 | Loss: 0.00003202
Iteration 42/1000 | Loss: 0.00003201
Iteration 43/1000 | Loss: 0.00003199
Iteration 44/1000 | Loss: 0.00003199
Iteration 45/1000 | Loss: 0.00003198
Iteration 46/1000 | Loss: 0.00003198
Iteration 47/1000 | Loss: 0.00003198
Iteration 48/1000 | Loss: 0.00003198
Iteration 49/1000 | Loss: 0.00003198
Iteration 50/1000 | Loss: 0.00003197
Iteration 51/1000 | Loss: 0.00003197
Iteration 52/1000 | Loss: 0.00003197
Iteration 53/1000 | Loss: 0.00003197
Iteration 54/1000 | Loss: 0.00003197
Iteration 55/1000 | Loss: 0.00003197
Iteration 56/1000 | Loss: 0.00003197
Iteration 57/1000 | Loss: 0.00003197
Iteration 58/1000 | Loss: 0.00003197
Iteration 59/1000 | Loss: 0.00003196
Iteration 60/1000 | Loss: 0.00003196
Iteration 61/1000 | Loss: 0.00003196
Iteration 62/1000 | Loss: 0.00003195
Iteration 63/1000 | Loss: 0.00003195
Iteration 64/1000 | Loss: 0.00003195
Iteration 65/1000 | Loss: 0.00003195
Iteration 66/1000 | Loss: 0.00003195
Iteration 67/1000 | Loss: 0.00003195
Iteration 68/1000 | Loss: 0.00003195
Iteration 69/1000 | Loss: 0.00003194
Iteration 70/1000 | Loss: 0.00003194
Iteration 71/1000 | Loss: 0.00003194
Iteration 72/1000 | Loss: 0.00003194
Iteration 73/1000 | Loss: 0.00003193
Iteration 74/1000 | Loss: 0.00003193
Iteration 75/1000 | Loss: 0.00003192
Iteration 76/1000 | Loss: 0.00003192
Iteration 77/1000 | Loss: 0.00003192
Iteration 78/1000 | Loss: 0.00003192
Iteration 79/1000 | Loss: 0.00003191
Iteration 80/1000 | Loss: 0.00003191
Iteration 81/1000 | Loss: 0.00003191
Iteration 82/1000 | Loss: 0.00003190
Iteration 83/1000 | Loss: 0.00003190
Iteration 84/1000 | Loss: 0.00003189
Iteration 85/1000 | Loss: 0.00003188
Iteration 86/1000 | Loss: 0.00003188
Iteration 87/1000 | Loss: 0.00003188
Iteration 88/1000 | Loss: 0.00003188
Iteration 89/1000 | Loss: 0.00003187
Iteration 90/1000 | Loss: 0.00003187
Iteration 91/1000 | Loss: 0.00003187
Iteration 92/1000 | Loss: 0.00003187
Iteration 93/1000 | Loss: 0.00003186
Iteration 94/1000 | Loss: 0.00003185
Iteration 95/1000 | Loss: 0.00003185
Iteration 96/1000 | Loss: 0.00003185
Iteration 97/1000 | Loss: 0.00003184
Iteration 98/1000 | Loss: 0.00003184
Iteration 99/1000 | Loss: 0.00003183
Iteration 100/1000 | Loss: 0.00003183
Iteration 101/1000 | Loss: 0.00003182
Iteration 102/1000 | Loss: 0.00003182
Iteration 103/1000 | Loss: 0.00003182
Iteration 104/1000 | Loss: 0.00003182
Iteration 105/1000 | Loss: 0.00003182
Iteration 106/1000 | Loss: 0.00003182
Iteration 107/1000 | Loss: 0.00003182
Iteration 108/1000 | Loss: 0.00003182
Iteration 109/1000 | Loss: 0.00003182
Iteration 110/1000 | Loss: 0.00003182
Iteration 111/1000 | Loss: 0.00003182
Iteration 112/1000 | Loss: 0.00003182
Iteration 113/1000 | Loss: 0.00003182
Iteration 114/1000 | Loss: 0.00003182
Iteration 115/1000 | Loss: 0.00003182
Iteration 116/1000 | Loss: 0.00003182
Iteration 117/1000 | Loss: 0.00003182
Iteration 118/1000 | Loss: 0.00003182
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [3.181922511430457e-05, 3.181922511430457e-05, 3.181922511430457e-05, 3.181922511430457e-05, 3.181922511430457e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.181922511430457e-05

Optimization complete. Final v2v error: 4.603934288024902 mm

Highest mean error: 5.0948381423950195 mm for frame 100

Lowest mean error: 4.113110065460205 mm for frame 221

Saving results

Total time: 47.79597592353821
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00525756
Iteration 2/25 | Loss: 0.00135119
Iteration 3/25 | Loss: 0.00118315
Iteration 4/25 | Loss: 0.00114273
Iteration 5/25 | Loss: 0.00111900
Iteration 6/25 | Loss: 0.00110551
Iteration 7/25 | Loss: 0.00111002
Iteration 8/25 | Loss: 0.00111053
Iteration 9/25 | Loss: 0.00109469
Iteration 10/25 | Loss: 0.00109267
Iteration 11/25 | Loss: 0.00109007
Iteration 12/25 | Loss: 0.00108899
Iteration 13/25 | Loss: 0.00108863
Iteration 14/25 | Loss: 0.00108897
Iteration 15/25 | Loss: 0.00108855
Iteration 16/25 | Loss: 0.00108744
Iteration 17/25 | Loss: 0.00108788
Iteration 18/25 | Loss: 0.00108631
Iteration 19/25 | Loss: 0.00108303
Iteration 20/25 | Loss: 0.00108059
Iteration 21/25 | Loss: 0.00107980
Iteration 22/25 | Loss: 0.00107960
Iteration 23/25 | Loss: 0.00107960
Iteration 24/25 | Loss: 0.00107960
Iteration 25/25 | Loss: 0.00107960

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40996480
Iteration 2/25 | Loss: 0.00091353
Iteration 3/25 | Loss: 0.00091352
Iteration 4/25 | Loss: 0.00091352
Iteration 5/25 | Loss: 0.00091352
Iteration 6/25 | Loss: 0.00091352
Iteration 7/25 | Loss: 0.00091352
Iteration 8/25 | Loss: 0.00091352
Iteration 9/25 | Loss: 0.00091352
Iteration 10/25 | Loss: 0.00091352
Iteration 11/25 | Loss: 0.00091352
Iteration 12/25 | Loss: 0.00091352
Iteration 13/25 | Loss: 0.00091352
Iteration 14/25 | Loss: 0.00091352
Iteration 15/25 | Loss: 0.00091352
Iteration 16/25 | Loss: 0.00091352
Iteration 17/25 | Loss: 0.00091352
Iteration 18/25 | Loss: 0.00091352
Iteration 19/25 | Loss: 0.00091352
Iteration 20/25 | Loss: 0.00091352
Iteration 21/25 | Loss: 0.00091352
Iteration 22/25 | Loss: 0.00091352
Iteration 23/25 | Loss: 0.00091352
Iteration 24/25 | Loss: 0.00091352
Iteration 25/25 | Loss: 0.00091352

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091352
Iteration 2/1000 | Loss: 0.00002016
Iteration 3/1000 | Loss: 0.00001363
Iteration 4/1000 | Loss: 0.00001226
Iteration 5/1000 | Loss: 0.00001162
Iteration 6/1000 | Loss: 0.00001124
Iteration 7/1000 | Loss: 0.00001083
Iteration 8/1000 | Loss: 0.00001060
Iteration 9/1000 | Loss: 0.00001059
Iteration 10/1000 | Loss: 0.00001056
Iteration 11/1000 | Loss: 0.00001047
Iteration 12/1000 | Loss: 0.00001027
Iteration 13/1000 | Loss: 0.00001018
Iteration 14/1000 | Loss: 0.00001016
Iteration 15/1000 | Loss: 0.00001015
Iteration 16/1000 | Loss: 0.00001011
Iteration 17/1000 | Loss: 0.00001008
Iteration 18/1000 | Loss: 0.00001004
Iteration 19/1000 | Loss: 0.00001003
Iteration 20/1000 | Loss: 0.00000994
Iteration 21/1000 | Loss: 0.00000994
Iteration 22/1000 | Loss: 0.00000986
Iteration 23/1000 | Loss: 0.00000986
Iteration 24/1000 | Loss: 0.00000985
Iteration 25/1000 | Loss: 0.00000984
Iteration 26/1000 | Loss: 0.00000983
Iteration 27/1000 | Loss: 0.00000983
Iteration 28/1000 | Loss: 0.00000982
Iteration 29/1000 | Loss: 0.00000981
Iteration 30/1000 | Loss: 0.00000981
Iteration 31/1000 | Loss: 0.00000981
Iteration 32/1000 | Loss: 0.00000981
Iteration 33/1000 | Loss: 0.00000981
Iteration 34/1000 | Loss: 0.00000980
Iteration 35/1000 | Loss: 0.00000980
Iteration 36/1000 | Loss: 0.00000979
Iteration 37/1000 | Loss: 0.00000978
Iteration 38/1000 | Loss: 0.00000978
Iteration 39/1000 | Loss: 0.00000977
Iteration 40/1000 | Loss: 0.00000977
Iteration 41/1000 | Loss: 0.00000977
Iteration 42/1000 | Loss: 0.00000977
Iteration 43/1000 | Loss: 0.00000977
Iteration 44/1000 | Loss: 0.00000977
Iteration 45/1000 | Loss: 0.00000977
Iteration 46/1000 | Loss: 0.00000977
Iteration 47/1000 | Loss: 0.00000976
Iteration 48/1000 | Loss: 0.00000975
Iteration 49/1000 | Loss: 0.00000974
Iteration 50/1000 | Loss: 0.00000974
Iteration 51/1000 | Loss: 0.00000974
Iteration 52/1000 | Loss: 0.00000974
Iteration 53/1000 | Loss: 0.00000973
Iteration 54/1000 | Loss: 0.00000973
Iteration 55/1000 | Loss: 0.00000973
Iteration 56/1000 | Loss: 0.00000973
Iteration 57/1000 | Loss: 0.00000973
Iteration 58/1000 | Loss: 0.00000973
Iteration 59/1000 | Loss: 0.00000973
Iteration 60/1000 | Loss: 0.00000973
Iteration 61/1000 | Loss: 0.00000973
Iteration 62/1000 | Loss: 0.00000972
Iteration 63/1000 | Loss: 0.00000972
Iteration 64/1000 | Loss: 0.00000971
Iteration 65/1000 | Loss: 0.00000971
Iteration 66/1000 | Loss: 0.00000971
Iteration 67/1000 | Loss: 0.00000971
Iteration 68/1000 | Loss: 0.00000971
Iteration 69/1000 | Loss: 0.00000969
Iteration 70/1000 | Loss: 0.00000969
Iteration 71/1000 | Loss: 0.00000968
Iteration 72/1000 | Loss: 0.00000968
Iteration 73/1000 | Loss: 0.00000968
Iteration 74/1000 | Loss: 0.00000968
Iteration 75/1000 | Loss: 0.00000967
Iteration 76/1000 | Loss: 0.00000967
Iteration 77/1000 | Loss: 0.00000966
Iteration 78/1000 | Loss: 0.00000966
Iteration 79/1000 | Loss: 0.00000965
Iteration 80/1000 | Loss: 0.00000965
Iteration 81/1000 | Loss: 0.00000965
Iteration 82/1000 | Loss: 0.00000965
Iteration 83/1000 | Loss: 0.00000964
Iteration 84/1000 | Loss: 0.00000964
Iteration 85/1000 | Loss: 0.00000964
Iteration 86/1000 | Loss: 0.00000964
Iteration 87/1000 | Loss: 0.00000964
Iteration 88/1000 | Loss: 0.00000964
Iteration 89/1000 | Loss: 0.00000963
Iteration 90/1000 | Loss: 0.00000963
Iteration 91/1000 | Loss: 0.00000963
Iteration 92/1000 | Loss: 0.00000963
Iteration 93/1000 | Loss: 0.00000963
Iteration 94/1000 | Loss: 0.00000962
Iteration 95/1000 | Loss: 0.00000962
Iteration 96/1000 | Loss: 0.00000962
Iteration 97/1000 | Loss: 0.00000962
Iteration 98/1000 | Loss: 0.00000961
Iteration 99/1000 | Loss: 0.00000961
Iteration 100/1000 | Loss: 0.00000961
Iteration 101/1000 | Loss: 0.00000961
Iteration 102/1000 | Loss: 0.00000961
Iteration 103/1000 | Loss: 0.00000961
Iteration 104/1000 | Loss: 0.00000961
Iteration 105/1000 | Loss: 0.00000961
Iteration 106/1000 | Loss: 0.00000961
Iteration 107/1000 | Loss: 0.00000960
Iteration 108/1000 | Loss: 0.00000960
Iteration 109/1000 | Loss: 0.00000960
Iteration 110/1000 | Loss: 0.00000960
Iteration 111/1000 | Loss: 0.00000960
Iteration 112/1000 | Loss: 0.00000960
Iteration 113/1000 | Loss: 0.00000960
Iteration 114/1000 | Loss: 0.00000959
Iteration 115/1000 | Loss: 0.00000959
Iteration 116/1000 | Loss: 0.00000959
Iteration 117/1000 | Loss: 0.00000959
Iteration 118/1000 | Loss: 0.00000959
Iteration 119/1000 | Loss: 0.00000959
Iteration 120/1000 | Loss: 0.00000959
Iteration 121/1000 | Loss: 0.00000958
Iteration 122/1000 | Loss: 0.00000958
Iteration 123/1000 | Loss: 0.00000958
Iteration 124/1000 | Loss: 0.00000958
Iteration 125/1000 | Loss: 0.00000958
Iteration 126/1000 | Loss: 0.00000958
Iteration 127/1000 | Loss: 0.00000958
Iteration 128/1000 | Loss: 0.00000958
Iteration 129/1000 | Loss: 0.00000958
Iteration 130/1000 | Loss: 0.00000958
Iteration 131/1000 | Loss: 0.00000958
Iteration 132/1000 | Loss: 0.00000957
Iteration 133/1000 | Loss: 0.00000957
Iteration 134/1000 | Loss: 0.00000957
Iteration 135/1000 | Loss: 0.00000956
Iteration 136/1000 | Loss: 0.00000956
Iteration 137/1000 | Loss: 0.00000956
Iteration 138/1000 | Loss: 0.00000956
Iteration 139/1000 | Loss: 0.00000956
Iteration 140/1000 | Loss: 0.00000956
Iteration 141/1000 | Loss: 0.00000956
Iteration 142/1000 | Loss: 0.00000956
Iteration 143/1000 | Loss: 0.00000956
Iteration 144/1000 | Loss: 0.00000956
Iteration 145/1000 | Loss: 0.00000956
Iteration 146/1000 | Loss: 0.00000955
Iteration 147/1000 | Loss: 0.00000955
Iteration 148/1000 | Loss: 0.00000955
Iteration 149/1000 | Loss: 0.00000955
Iteration 150/1000 | Loss: 0.00000955
Iteration 151/1000 | Loss: 0.00000955
Iteration 152/1000 | Loss: 0.00000955
Iteration 153/1000 | Loss: 0.00000955
Iteration 154/1000 | Loss: 0.00000954
Iteration 155/1000 | Loss: 0.00000954
Iteration 156/1000 | Loss: 0.00000954
Iteration 157/1000 | Loss: 0.00000954
Iteration 158/1000 | Loss: 0.00000954
Iteration 159/1000 | Loss: 0.00000954
Iteration 160/1000 | Loss: 0.00000954
Iteration 161/1000 | Loss: 0.00000954
Iteration 162/1000 | Loss: 0.00000953
Iteration 163/1000 | Loss: 0.00000953
Iteration 164/1000 | Loss: 0.00000953
Iteration 165/1000 | Loss: 0.00000953
Iteration 166/1000 | Loss: 0.00000953
Iteration 167/1000 | Loss: 0.00000953
Iteration 168/1000 | Loss: 0.00000953
Iteration 169/1000 | Loss: 0.00000953
Iteration 170/1000 | Loss: 0.00000953
Iteration 171/1000 | Loss: 0.00000953
Iteration 172/1000 | Loss: 0.00000953
Iteration 173/1000 | Loss: 0.00000953
Iteration 174/1000 | Loss: 0.00000952
Iteration 175/1000 | Loss: 0.00000952
Iteration 176/1000 | Loss: 0.00000952
Iteration 177/1000 | Loss: 0.00000952
Iteration 178/1000 | Loss: 0.00000951
Iteration 179/1000 | Loss: 0.00000951
Iteration 180/1000 | Loss: 0.00000951
Iteration 181/1000 | Loss: 0.00000951
Iteration 182/1000 | Loss: 0.00000951
Iteration 183/1000 | Loss: 0.00000951
Iteration 184/1000 | Loss: 0.00000951
Iteration 185/1000 | Loss: 0.00000951
Iteration 186/1000 | Loss: 0.00000951
Iteration 187/1000 | Loss: 0.00000951
Iteration 188/1000 | Loss: 0.00000951
Iteration 189/1000 | Loss: 0.00000951
Iteration 190/1000 | Loss: 0.00000951
Iteration 191/1000 | Loss: 0.00000951
Iteration 192/1000 | Loss: 0.00000951
Iteration 193/1000 | Loss: 0.00000951
Iteration 194/1000 | Loss: 0.00000951
Iteration 195/1000 | Loss: 0.00000951
Iteration 196/1000 | Loss: 0.00000950
Iteration 197/1000 | Loss: 0.00000950
Iteration 198/1000 | Loss: 0.00000950
Iteration 199/1000 | Loss: 0.00000950
Iteration 200/1000 | Loss: 0.00000950
Iteration 201/1000 | Loss: 0.00000950
Iteration 202/1000 | Loss: 0.00000949
Iteration 203/1000 | Loss: 0.00000949
Iteration 204/1000 | Loss: 0.00000949
Iteration 205/1000 | Loss: 0.00000949
Iteration 206/1000 | Loss: 0.00000949
Iteration 207/1000 | Loss: 0.00000949
Iteration 208/1000 | Loss: 0.00000949
Iteration 209/1000 | Loss: 0.00000949
Iteration 210/1000 | Loss: 0.00000949
Iteration 211/1000 | Loss: 0.00000949
Iteration 212/1000 | Loss: 0.00000949
Iteration 213/1000 | Loss: 0.00000949
Iteration 214/1000 | Loss: 0.00000949
Iteration 215/1000 | Loss: 0.00000949
Iteration 216/1000 | Loss: 0.00000949
Iteration 217/1000 | Loss: 0.00000949
Iteration 218/1000 | Loss: 0.00000948
Iteration 219/1000 | Loss: 0.00000948
Iteration 220/1000 | Loss: 0.00000948
Iteration 221/1000 | Loss: 0.00000948
Iteration 222/1000 | Loss: 0.00000948
Iteration 223/1000 | Loss: 0.00000948
Iteration 224/1000 | Loss: 0.00000948
Iteration 225/1000 | Loss: 0.00000947
Iteration 226/1000 | Loss: 0.00000947
Iteration 227/1000 | Loss: 0.00000947
Iteration 228/1000 | Loss: 0.00000947
Iteration 229/1000 | Loss: 0.00000947
Iteration 230/1000 | Loss: 0.00000947
Iteration 231/1000 | Loss: 0.00000947
Iteration 232/1000 | Loss: 0.00000947
Iteration 233/1000 | Loss: 0.00000947
Iteration 234/1000 | Loss: 0.00000947
Iteration 235/1000 | Loss: 0.00000947
Iteration 236/1000 | Loss: 0.00000947
Iteration 237/1000 | Loss: 0.00000947
Iteration 238/1000 | Loss: 0.00000947
Iteration 239/1000 | Loss: 0.00000947
Iteration 240/1000 | Loss: 0.00000947
Iteration 241/1000 | Loss: 0.00000947
Iteration 242/1000 | Loss: 0.00000947
Iteration 243/1000 | Loss: 0.00000947
Iteration 244/1000 | Loss: 0.00000947
Iteration 245/1000 | Loss: 0.00000947
Iteration 246/1000 | Loss: 0.00000947
Iteration 247/1000 | Loss: 0.00000947
Iteration 248/1000 | Loss: 0.00000947
Iteration 249/1000 | Loss: 0.00000947
Iteration 250/1000 | Loss: 0.00000947
Iteration 251/1000 | Loss: 0.00000947
Iteration 252/1000 | Loss: 0.00000947
Iteration 253/1000 | Loss: 0.00000947
Iteration 254/1000 | Loss: 0.00000947
Iteration 255/1000 | Loss: 0.00000947
Iteration 256/1000 | Loss: 0.00000947
Iteration 257/1000 | Loss: 0.00000947
Iteration 258/1000 | Loss: 0.00000947
Iteration 259/1000 | Loss: 0.00000947
Iteration 260/1000 | Loss: 0.00000947
Iteration 261/1000 | Loss: 0.00000947
Iteration 262/1000 | Loss: 0.00000947
Iteration 263/1000 | Loss: 0.00000947
Iteration 264/1000 | Loss: 0.00000947
Iteration 265/1000 | Loss: 0.00000947
Iteration 266/1000 | Loss: 0.00000947
Iteration 267/1000 | Loss: 0.00000947
Iteration 268/1000 | Loss: 0.00000947
Iteration 269/1000 | Loss: 0.00000947
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 269. Stopping optimization.
Last 5 losses: [9.47142416407587e-06, 9.47142416407587e-06, 9.47142416407587e-06, 9.47142416407587e-06, 9.47142416407587e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.47142416407587e-06

Optimization complete. Final v2v error: 2.636059522628784 mm

Highest mean error: 3.2200751304626465 mm for frame 40

Lowest mean error: 2.343963861465454 mm for frame 119

Saving results

Total time: 73.59535765647888
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00481338
Iteration 2/25 | Loss: 0.00147867
Iteration 3/25 | Loss: 0.00119312
Iteration 4/25 | Loss: 0.00115369
Iteration 5/25 | Loss: 0.00114880
Iteration 6/25 | Loss: 0.00114792
Iteration 7/25 | Loss: 0.00114792
Iteration 8/25 | Loss: 0.00114792
Iteration 9/25 | Loss: 0.00114792
Iteration 10/25 | Loss: 0.00114792
Iteration 11/25 | Loss: 0.00114792
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011479188688099384, 0.0011479188688099384, 0.0011479188688099384, 0.0011479188688099384, 0.0011479188688099384]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011479188688099384

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40259778
Iteration 2/25 | Loss: 0.00065685
Iteration 3/25 | Loss: 0.00065685
Iteration 4/25 | Loss: 0.00065685
Iteration 5/25 | Loss: 0.00065685
Iteration 6/25 | Loss: 0.00065685
Iteration 7/25 | Loss: 0.00065685
Iteration 8/25 | Loss: 0.00065685
Iteration 9/25 | Loss: 0.00065685
Iteration 10/25 | Loss: 0.00065684
Iteration 11/25 | Loss: 0.00065684
Iteration 12/25 | Loss: 0.00065684
Iteration 13/25 | Loss: 0.00065684
Iteration 14/25 | Loss: 0.00065684
Iteration 15/25 | Loss: 0.00065684
Iteration 16/25 | Loss: 0.00065684
Iteration 17/25 | Loss: 0.00065684
Iteration 18/25 | Loss: 0.00065684
Iteration 19/25 | Loss: 0.00065684
Iteration 20/25 | Loss: 0.00065684
Iteration 21/25 | Loss: 0.00065684
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006568444077856839, 0.0006568444077856839, 0.0006568444077856839, 0.0006568444077856839, 0.0006568444077856839]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006568444077856839

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065684
Iteration 2/1000 | Loss: 0.00003322
Iteration 3/1000 | Loss: 0.00002252
Iteration 4/1000 | Loss: 0.00001918
Iteration 5/1000 | Loss: 0.00001797
Iteration 6/1000 | Loss: 0.00001708
Iteration 7/1000 | Loss: 0.00001651
Iteration 8/1000 | Loss: 0.00001606
Iteration 9/1000 | Loss: 0.00001568
Iteration 10/1000 | Loss: 0.00001535
Iteration 11/1000 | Loss: 0.00001530
Iteration 12/1000 | Loss: 0.00001511
Iteration 13/1000 | Loss: 0.00001493
Iteration 14/1000 | Loss: 0.00001492
Iteration 15/1000 | Loss: 0.00001488
Iteration 16/1000 | Loss: 0.00001484
Iteration 17/1000 | Loss: 0.00001468
Iteration 18/1000 | Loss: 0.00001468
Iteration 19/1000 | Loss: 0.00001460
Iteration 20/1000 | Loss: 0.00001458
Iteration 21/1000 | Loss: 0.00001458
Iteration 22/1000 | Loss: 0.00001456
Iteration 23/1000 | Loss: 0.00001455
Iteration 24/1000 | Loss: 0.00001454
Iteration 25/1000 | Loss: 0.00001454
Iteration 26/1000 | Loss: 0.00001453
Iteration 27/1000 | Loss: 0.00001453
Iteration 28/1000 | Loss: 0.00001452
Iteration 29/1000 | Loss: 0.00001450
Iteration 30/1000 | Loss: 0.00001450
Iteration 31/1000 | Loss: 0.00001449
Iteration 32/1000 | Loss: 0.00001448
Iteration 33/1000 | Loss: 0.00001447
Iteration 34/1000 | Loss: 0.00001446
Iteration 35/1000 | Loss: 0.00001446
Iteration 36/1000 | Loss: 0.00001446
Iteration 37/1000 | Loss: 0.00001446
Iteration 38/1000 | Loss: 0.00001446
Iteration 39/1000 | Loss: 0.00001446
Iteration 40/1000 | Loss: 0.00001446
Iteration 41/1000 | Loss: 0.00001445
Iteration 42/1000 | Loss: 0.00001445
Iteration 43/1000 | Loss: 0.00001445
Iteration 44/1000 | Loss: 0.00001445
Iteration 45/1000 | Loss: 0.00001445
Iteration 46/1000 | Loss: 0.00001445
Iteration 47/1000 | Loss: 0.00001445
Iteration 48/1000 | Loss: 0.00001445
Iteration 49/1000 | Loss: 0.00001445
Iteration 50/1000 | Loss: 0.00001445
Iteration 51/1000 | Loss: 0.00001444
Iteration 52/1000 | Loss: 0.00001444
Iteration 53/1000 | Loss: 0.00001443
Iteration 54/1000 | Loss: 0.00001443
Iteration 55/1000 | Loss: 0.00001443
Iteration 56/1000 | Loss: 0.00001443
Iteration 57/1000 | Loss: 0.00001442
Iteration 58/1000 | Loss: 0.00001442
Iteration 59/1000 | Loss: 0.00001440
Iteration 60/1000 | Loss: 0.00001440
Iteration 61/1000 | Loss: 0.00001440
Iteration 62/1000 | Loss: 0.00001439
Iteration 63/1000 | Loss: 0.00001439
Iteration 64/1000 | Loss: 0.00001438
Iteration 65/1000 | Loss: 0.00001438
Iteration 66/1000 | Loss: 0.00001438
Iteration 67/1000 | Loss: 0.00001438
Iteration 68/1000 | Loss: 0.00001438
Iteration 69/1000 | Loss: 0.00001438
Iteration 70/1000 | Loss: 0.00001438
Iteration 71/1000 | Loss: 0.00001438
Iteration 72/1000 | Loss: 0.00001438
Iteration 73/1000 | Loss: 0.00001438
Iteration 74/1000 | Loss: 0.00001437
Iteration 75/1000 | Loss: 0.00001437
Iteration 76/1000 | Loss: 0.00001436
Iteration 77/1000 | Loss: 0.00001436
Iteration 78/1000 | Loss: 0.00001435
Iteration 79/1000 | Loss: 0.00001435
Iteration 80/1000 | Loss: 0.00001435
Iteration 81/1000 | Loss: 0.00001435
Iteration 82/1000 | Loss: 0.00001435
Iteration 83/1000 | Loss: 0.00001435
Iteration 84/1000 | Loss: 0.00001434
Iteration 85/1000 | Loss: 0.00001434
Iteration 86/1000 | Loss: 0.00001434
Iteration 87/1000 | Loss: 0.00001434
Iteration 88/1000 | Loss: 0.00001433
Iteration 89/1000 | Loss: 0.00001433
Iteration 90/1000 | Loss: 0.00001433
Iteration 91/1000 | Loss: 0.00001433
Iteration 92/1000 | Loss: 0.00001433
Iteration 93/1000 | Loss: 0.00001432
Iteration 94/1000 | Loss: 0.00001432
Iteration 95/1000 | Loss: 0.00001432
Iteration 96/1000 | Loss: 0.00001432
Iteration 97/1000 | Loss: 0.00001432
Iteration 98/1000 | Loss: 0.00001432
Iteration 99/1000 | Loss: 0.00001431
Iteration 100/1000 | Loss: 0.00001431
Iteration 101/1000 | Loss: 0.00001431
Iteration 102/1000 | Loss: 0.00001431
Iteration 103/1000 | Loss: 0.00001431
Iteration 104/1000 | Loss: 0.00001431
Iteration 105/1000 | Loss: 0.00001431
Iteration 106/1000 | Loss: 0.00001431
Iteration 107/1000 | Loss: 0.00001430
Iteration 108/1000 | Loss: 0.00001430
Iteration 109/1000 | Loss: 0.00001430
Iteration 110/1000 | Loss: 0.00001429
Iteration 111/1000 | Loss: 0.00001429
Iteration 112/1000 | Loss: 0.00001429
Iteration 113/1000 | Loss: 0.00001429
Iteration 114/1000 | Loss: 0.00001429
Iteration 115/1000 | Loss: 0.00001429
Iteration 116/1000 | Loss: 0.00001428
Iteration 117/1000 | Loss: 0.00001428
Iteration 118/1000 | Loss: 0.00001428
Iteration 119/1000 | Loss: 0.00001428
Iteration 120/1000 | Loss: 0.00001428
Iteration 121/1000 | Loss: 0.00001427
Iteration 122/1000 | Loss: 0.00001427
Iteration 123/1000 | Loss: 0.00001427
Iteration 124/1000 | Loss: 0.00001426
Iteration 125/1000 | Loss: 0.00001426
Iteration 126/1000 | Loss: 0.00001426
Iteration 127/1000 | Loss: 0.00001426
Iteration 128/1000 | Loss: 0.00001425
Iteration 129/1000 | Loss: 0.00001425
Iteration 130/1000 | Loss: 0.00001425
Iteration 131/1000 | Loss: 0.00001425
Iteration 132/1000 | Loss: 0.00001425
Iteration 133/1000 | Loss: 0.00001425
Iteration 134/1000 | Loss: 0.00001424
Iteration 135/1000 | Loss: 0.00001424
Iteration 136/1000 | Loss: 0.00001424
Iteration 137/1000 | Loss: 0.00001424
Iteration 138/1000 | Loss: 0.00001423
Iteration 139/1000 | Loss: 0.00001423
Iteration 140/1000 | Loss: 0.00001423
Iteration 141/1000 | Loss: 0.00001423
Iteration 142/1000 | Loss: 0.00001423
Iteration 143/1000 | Loss: 0.00001423
Iteration 144/1000 | Loss: 0.00001423
Iteration 145/1000 | Loss: 0.00001423
Iteration 146/1000 | Loss: 0.00001423
Iteration 147/1000 | Loss: 0.00001423
Iteration 148/1000 | Loss: 0.00001423
Iteration 149/1000 | Loss: 0.00001423
Iteration 150/1000 | Loss: 0.00001423
Iteration 151/1000 | Loss: 0.00001423
Iteration 152/1000 | Loss: 0.00001423
Iteration 153/1000 | Loss: 0.00001423
Iteration 154/1000 | Loss: 0.00001423
Iteration 155/1000 | Loss: 0.00001423
Iteration 156/1000 | Loss: 0.00001423
Iteration 157/1000 | Loss: 0.00001423
Iteration 158/1000 | Loss: 0.00001423
Iteration 159/1000 | Loss: 0.00001423
Iteration 160/1000 | Loss: 0.00001423
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.4231172826839611e-05, 1.4231172826839611e-05, 1.4231172826839611e-05, 1.4231172826839611e-05, 1.4231172826839611e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4231172826839611e-05

Optimization complete. Final v2v error: 3.184049129486084 mm

Highest mean error: 3.6467630863189697 mm for frame 45

Lowest mean error: 2.7213823795318604 mm for frame 156

Saving results

Total time: 40.74719977378845
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00950976
Iteration 2/25 | Loss: 0.00366288
Iteration 3/25 | Loss: 0.00210710
Iteration 4/25 | Loss: 0.00187581
Iteration 5/25 | Loss: 0.00177929
Iteration 6/25 | Loss: 0.00171676
Iteration 7/25 | Loss: 0.00194419
Iteration 8/25 | Loss: 0.00170528
Iteration 9/25 | Loss: 0.00154419
Iteration 10/25 | Loss: 0.00144661
Iteration 11/25 | Loss: 0.00143558
Iteration 12/25 | Loss: 0.00143713
Iteration 13/25 | Loss: 0.00142665
Iteration 14/25 | Loss: 0.00140995
Iteration 15/25 | Loss: 0.00139730
Iteration 16/25 | Loss: 0.00141123
Iteration 17/25 | Loss: 0.00140729
Iteration 18/25 | Loss: 0.00138976
Iteration 19/25 | Loss: 0.00139383
Iteration 20/25 | Loss: 0.00139153
Iteration 21/25 | Loss: 0.00138768
Iteration 22/25 | Loss: 0.00137137
Iteration 23/25 | Loss: 0.00136146
Iteration 24/25 | Loss: 0.00136134
Iteration 25/25 | Loss: 0.00135659

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33639908
Iteration 2/25 | Loss: 0.00371275
Iteration 3/25 | Loss: 0.00232138
Iteration 4/25 | Loss: 0.00232137
Iteration 5/25 | Loss: 0.00232137
Iteration 6/25 | Loss: 0.00232137
Iteration 7/25 | Loss: 0.00232137
Iteration 8/25 | Loss: 0.00232137
Iteration 9/25 | Loss: 0.00232137
Iteration 10/25 | Loss: 0.00232137
Iteration 11/25 | Loss: 0.00232137
Iteration 12/25 | Loss: 0.00232137
Iteration 13/25 | Loss: 0.00232137
Iteration 14/25 | Loss: 0.00232137
Iteration 15/25 | Loss: 0.00232137
Iteration 16/25 | Loss: 0.00232137
Iteration 17/25 | Loss: 0.00232137
Iteration 18/25 | Loss: 0.00232137
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.002321370877325535, 0.002321370877325535, 0.002321370877325535, 0.002321370877325535, 0.002321370877325535]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002321370877325535

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00232137
Iteration 2/1000 | Loss: 0.00455496
Iteration 3/1000 | Loss: 0.00169756
Iteration 4/1000 | Loss: 0.00115723
Iteration 5/1000 | Loss: 0.00063848
Iteration 6/1000 | Loss: 0.00065766
Iteration 7/1000 | Loss: 0.00138075
Iteration 8/1000 | Loss: 0.00052878
Iteration 9/1000 | Loss: 0.00059929
Iteration 10/1000 | Loss: 0.00065410
Iteration 11/1000 | Loss: 0.00060992
Iteration 12/1000 | Loss: 0.00103801
Iteration 13/1000 | Loss: 0.00155123
Iteration 14/1000 | Loss: 0.00274166
Iteration 15/1000 | Loss: 0.00066123
Iteration 16/1000 | Loss: 0.00089432
Iteration 17/1000 | Loss: 0.00092437
Iteration 18/1000 | Loss: 0.00148098
Iteration 19/1000 | Loss: 0.00107145
Iteration 20/1000 | Loss: 0.00094742
Iteration 21/1000 | Loss: 0.00189380
Iteration 22/1000 | Loss: 0.00120174
Iteration 23/1000 | Loss: 0.00104763
Iteration 24/1000 | Loss: 0.00124428
Iteration 25/1000 | Loss: 0.00067858
Iteration 26/1000 | Loss: 0.00043260
Iteration 27/1000 | Loss: 0.00040441
Iteration 28/1000 | Loss: 0.00050226
Iteration 29/1000 | Loss: 0.00014575
Iteration 30/1000 | Loss: 0.00017190
Iteration 31/1000 | Loss: 0.00013953
Iteration 32/1000 | Loss: 0.00109903
Iteration 33/1000 | Loss: 0.00074170
Iteration 34/1000 | Loss: 0.00053268
Iteration 35/1000 | Loss: 0.00078316
Iteration 36/1000 | Loss: 0.00030268
Iteration 37/1000 | Loss: 0.00045687
Iteration 38/1000 | Loss: 0.00040930
Iteration 39/1000 | Loss: 0.00041063
Iteration 40/1000 | Loss: 0.00060698
Iteration 41/1000 | Loss: 0.00039185
Iteration 42/1000 | Loss: 0.00041256
Iteration 43/1000 | Loss: 0.00026732
Iteration 44/1000 | Loss: 0.00015281
Iteration 45/1000 | Loss: 0.00024948
Iteration 46/1000 | Loss: 0.00030355
Iteration 47/1000 | Loss: 0.00025808
Iteration 48/1000 | Loss: 0.00029162
Iteration 49/1000 | Loss: 0.00047458
Iteration 50/1000 | Loss: 0.00017037
Iteration 51/1000 | Loss: 0.00051202
Iteration 52/1000 | Loss: 0.00036459
Iteration 53/1000 | Loss: 0.00037838
Iteration 54/1000 | Loss: 0.00029369
Iteration 55/1000 | Loss: 0.00007436
Iteration 56/1000 | Loss: 0.00017514
Iteration 57/1000 | Loss: 0.00007770
Iteration 58/1000 | Loss: 0.00010935
Iteration 59/1000 | Loss: 0.00007214
Iteration 60/1000 | Loss: 0.00030398
Iteration 61/1000 | Loss: 0.00025378
Iteration 62/1000 | Loss: 0.00014716
Iteration 63/1000 | Loss: 0.00012642
Iteration 64/1000 | Loss: 0.00014881
Iteration 65/1000 | Loss: 0.00010694
Iteration 66/1000 | Loss: 0.00011142
Iteration 67/1000 | Loss: 0.00065485
Iteration 68/1000 | Loss: 0.00040343
Iteration 69/1000 | Loss: 0.00075979
Iteration 70/1000 | Loss: 0.00051683
Iteration 71/1000 | Loss: 0.00029687
Iteration 72/1000 | Loss: 0.00018647
Iteration 73/1000 | Loss: 0.00029185
Iteration 74/1000 | Loss: 0.00017537
Iteration 75/1000 | Loss: 0.00023615
Iteration 76/1000 | Loss: 0.00019911
Iteration 77/1000 | Loss: 0.00017471
Iteration 78/1000 | Loss: 0.00016067
Iteration 79/1000 | Loss: 0.00022758
Iteration 80/1000 | Loss: 0.00016318
Iteration 81/1000 | Loss: 0.00008030
Iteration 82/1000 | Loss: 0.00012816
Iteration 83/1000 | Loss: 0.00026174
Iteration 84/1000 | Loss: 0.00094585
Iteration 85/1000 | Loss: 0.00076992
Iteration 86/1000 | Loss: 0.00060867
Iteration 87/1000 | Loss: 0.00014877
Iteration 88/1000 | Loss: 0.00013538
Iteration 89/1000 | Loss: 0.00005737
Iteration 90/1000 | Loss: 0.00009018
Iteration 91/1000 | Loss: 0.00009539
Iteration 92/1000 | Loss: 0.00005582
Iteration 93/1000 | Loss: 0.00007127
Iteration 94/1000 | Loss: 0.00014276
Iteration 95/1000 | Loss: 0.00007888
Iteration 96/1000 | Loss: 0.00008573
Iteration 97/1000 | Loss: 0.00026055
Iteration 98/1000 | Loss: 0.00017164
Iteration 99/1000 | Loss: 0.00009895
Iteration 100/1000 | Loss: 0.00007364
Iteration 101/1000 | Loss: 0.00009276
Iteration 102/1000 | Loss: 0.00008293
Iteration 103/1000 | Loss: 0.00008656
Iteration 104/1000 | Loss: 0.00008216
Iteration 105/1000 | Loss: 0.00008154
Iteration 106/1000 | Loss: 0.00031419
Iteration 107/1000 | Loss: 0.00010193
Iteration 108/1000 | Loss: 0.00006700
Iteration 109/1000 | Loss: 0.00012011
Iteration 110/1000 | Loss: 0.00007834
Iteration 111/1000 | Loss: 0.00009773
Iteration 112/1000 | Loss: 0.00011912
Iteration 113/1000 | Loss: 0.00010262
Iteration 114/1000 | Loss: 0.00029846
Iteration 115/1000 | Loss: 0.00010199
Iteration 116/1000 | Loss: 0.00006975
Iteration 117/1000 | Loss: 0.00007329
Iteration 118/1000 | Loss: 0.00008137
Iteration 119/1000 | Loss: 0.00006202
Iteration 120/1000 | Loss: 0.00006421
Iteration 121/1000 | Loss: 0.00007415
Iteration 122/1000 | Loss: 0.00007430
Iteration 123/1000 | Loss: 0.00008314
Iteration 124/1000 | Loss: 0.00007939
Iteration 125/1000 | Loss: 0.00005765
Iteration 126/1000 | Loss: 0.00005837
Iteration 127/1000 | Loss: 0.00006837
Iteration 128/1000 | Loss: 0.00006360
Iteration 129/1000 | Loss: 0.00008481
Iteration 130/1000 | Loss: 0.00008597
Iteration 131/1000 | Loss: 0.00006059
Iteration 132/1000 | Loss: 0.00008228
Iteration 133/1000 | Loss: 0.00004693
Iteration 134/1000 | Loss: 0.00006509
Iteration 135/1000 | Loss: 0.00009646
Iteration 136/1000 | Loss: 0.00008076
Iteration 137/1000 | Loss: 0.00005138
Iteration 138/1000 | Loss: 0.00005820
Iteration 139/1000 | Loss: 0.00005361
Iteration 140/1000 | Loss: 0.00011125
Iteration 141/1000 | Loss: 0.00005302
Iteration 142/1000 | Loss: 0.00004703
Iteration 143/1000 | Loss: 0.00004842
Iteration 144/1000 | Loss: 0.00004523
Iteration 145/1000 | Loss: 0.00003903
Iteration 146/1000 | Loss: 0.00004950
Iteration 147/1000 | Loss: 0.00004556
Iteration 148/1000 | Loss: 0.00031565
Iteration 149/1000 | Loss: 0.00005703
Iteration 150/1000 | Loss: 0.00050940
Iteration 151/1000 | Loss: 0.00009894
Iteration 152/1000 | Loss: 0.00005486
Iteration 153/1000 | Loss: 0.00004809
Iteration 154/1000 | Loss: 0.00006589
Iteration 155/1000 | Loss: 0.00004765
Iteration 156/1000 | Loss: 0.00005590
Iteration 157/1000 | Loss: 0.00005245
Iteration 158/1000 | Loss: 0.00004269
Iteration 159/1000 | Loss: 0.00004466
Iteration 160/1000 | Loss: 0.00004440
Iteration 161/1000 | Loss: 0.00004365
Iteration 162/1000 | Loss: 0.00004693
Iteration 163/1000 | Loss: 0.00007554
Iteration 164/1000 | Loss: 0.00013285
Iteration 165/1000 | Loss: 0.00014552
Iteration 166/1000 | Loss: 0.00011513
Iteration 167/1000 | Loss: 0.00006590
Iteration 168/1000 | Loss: 0.00008833
Iteration 169/1000 | Loss: 0.00010342
Iteration 170/1000 | Loss: 0.00004814
Iteration 171/1000 | Loss: 0.00004869
Iteration 172/1000 | Loss: 0.00004155
Iteration 173/1000 | Loss: 0.00004260
Iteration 174/1000 | Loss: 0.00003844
Iteration 175/1000 | Loss: 0.00004370
Iteration 176/1000 | Loss: 0.00010538
Iteration 177/1000 | Loss: 0.00008747
Iteration 178/1000 | Loss: 0.00004998
Iteration 179/1000 | Loss: 0.00019961
Iteration 180/1000 | Loss: 0.00015416
Iteration 181/1000 | Loss: 0.00077371
Iteration 182/1000 | Loss: 0.00020235
Iteration 183/1000 | Loss: 0.00042494
Iteration 184/1000 | Loss: 0.00031194
Iteration 185/1000 | Loss: 0.00037979
Iteration 186/1000 | Loss: 0.00004420
Iteration 187/1000 | Loss: 0.00003999
Iteration 188/1000 | Loss: 0.00003814
Iteration 189/1000 | Loss: 0.00009305
Iteration 190/1000 | Loss: 0.00003899
Iteration 191/1000 | Loss: 0.00004038
Iteration 192/1000 | Loss: 0.00004005
Iteration 193/1000 | Loss: 0.00004070
Iteration 194/1000 | Loss: 0.00013604
Iteration 195/1000 | Loss: 0.00004558
Iteration 196/1000 | Loss: 0.00004719
Iteration 197/1000 | Loss: 0.00004205
Iteration 198/1000 | Loss: 0.00004012
Iteration 199/1000 | Loss: 0.00005546
Iteration 200/1000 | Loss: 0.00021356
Iteration 201/1000 | Loss: 0.00003700
Iteration 202/1000 | Loss: 0.00004375
Iteration 203/1000 | Loss: 0.00003108
Iteration 204/1000 | Loss: 0.00003102
Iteration 205/1000 | Loss: 0.00002843
Iteration 206/1000 | Loss: 0.00006251
Iteration 207/1000 | Loss: 0.00004226
Iteration 208/1000 | Loss: 0.00014951
Iteration 209/1000 | Loss: 0.00020514
Iteration 210/1000 | Loss: 0.00003920
Iteration 211/1000 | Loss: 0.00002754
Iteration 212/1000 | Loss: 0.00002749
Iteration 213/1000 | Loss: 0.00014581
Iteration 214/1000 | Loss: 0.00005750
Iteration 215/1000 | Loss: 0.00002805
Iteration 216/1000 | Loss: 0.00006555
Iteration 217/1000 | Loss: 0.00006344
Iteration 218/1000 | Loss: 0.00003141
Iteration 219/1000 | Loss: 0.00004008
Iteration 220/1000 | Loss: 0.00086530
Iteration 221/1000 | Loss: 0.00003324
Iteration 222/1000 | Loss: 0.00002692
Iteration 223/1000 | Loss: 0.00002507
Iteration 224/1000 | Loss: 0.00002374
Iteration 225/1000 | Loss: 0.00005220
Iteration 226/1000 | Loss: 0.00002225
Iteration 227/1000 | Loss: 0.00006444
Iteration 228/1000 | Loss: 0.00002146
Iteration 229/1000 | Loss: 0.00002123
Iteration 230/1000 | Loss: 0.00002111
Iteration 231/1000 | Loss: 0.00002093
Iteration 232/1000 | Loss: 0.00002088
Iteration 233/1000 | Loss: 0.00003613
Iteration 234/1000 | Loss: 0.00002083
Iteration 235/1000 | Loss: 0.00002079
Iteration 236/1000 | Loss: 0.00002078
Iteration 237/1000 | Loss: 0.00003220
Iteration 238/1000 | Loss: 0.00002068
Iteration 239/1000 | Loss: 0.00002062
Iteration 240/1000 | Loss: 0.00002062
Iteration 241/1000 | Loss: 0.00002062
Iteration 242/1000 | Loss: 0.00002061
Iteration 243/1000 | Loss: 0.00002061
Iteration 244/1000 | Loss: 0.00002061
Iteration 245/1000 | Loss: 0.00002406
Iteration 246/1000 | Loss: 0.00002061
Iteration 247/1000 | Loss: 0.00002061
Iteration 248/1000 | Loss: 0.00002061
Iteration 249/1000 | Loss: 0.00002061
Iteration 250/1000 | Loss: 0.00002061
Iteration 251/1000 | Loss: 0.00002061
Iteration 252/1000 | Loss: 0.00002061
Iteration 253/1000 | Loss: 0.00002061
Iteration 254/1000 | Loss: 0.00002060
Iteration 255/1000 | Loss: 0.00002060
Iteration 256/1000 | Loss: 0.00002060
Iteration 257/1000 | Loss: 0.00002059
Iteration 258/1000 | Loss: 0.00002059
Iteration 259/1000 | Loss: 0.00002058
Iteration 260/1000 | Loss: 0.00002358
Iteration 261/1000 | Loss: 0.00002056
Iteration 262/1000 | Loss: 0.00002051
Iteration 263/1000 | Loss: 0.00002051
Iteration 264/1000 | Loss: 0.00002051
Iteration 265/1000 | Loss: 0.00002051
Iteration 266/1000 | Loss: 0.00002051
Iteration 267/1000 | Loss: 0.00002051
Iteration 268/1000 | Loss: 0.00002051
Iteration 269/1000 | Loss: 0.00002051
Iteration 270/1000 | Loss: 0.00002050
Iteration 271/1000 | Loss: 0.00002050
Iteration 272/1000 | Loss: 0.00002050
Iteration 273/1000 | Loss: 0.00002050
Iteration 274/1000 | Loss: 0.00002050
Iteration 275/1000 | Loss: 0.00002050
Iteration 276/1000 | Loss: 0.00002050
Iteration 277/1000 | Loss: 0.00002050
Iteration 278/1000 | Loss: 0.00002050
Iteration 279/1000 | Loss: 0.00002050
Iteration 280/1000 | Loss: 0.00002050
Iteration 281/1000 | Loss: 0.00002050
Iteration 282/1000 | Loss: 0.00002049
Iteration 283/1000 | Loss: 0.00002049
Iteration 284/1000 | Loss: 0.00002049
Iteration 285/1000 | Loss: 0.00002049
Iteration 286/1000 | Loss: 0.00002049
Iteration 287/1000 | Loss: 0.00002049
Iteration 288/1000 | Loss: 0.00002049
Iteration 289/1000 | Loss: 0.00002049
Iteration 290/1000 | Loss: 0.00002961
Iteration 291/1000 | Loss: 0.00002052
Iteration 292/1000 | Loss: 0.00002052
Iteration 293/1000 | Loss: 0.00002052
Iteration 294/1000 | Loss: 0.00002052
Iteration 295/1000 | Loss: 0.00002052
Iteration 296/1000 | Loss: 0.00002052
Iteration 297/1000 | Loss: 0.00002052
Iteration 298/1000 | Loss: 0.00002052
Iteration 299/1000 | Loss: 0.00002052
Iteration 300/1000 | Loss: 0.00002049
Iteration 301/1000 | Loss: 0.00002048
Iteration 302/1000 | Loss: 0.00002048
Iteration 303/1000 | Loss: 0.00002048
Iteration 304/1000 | Loss: 0.00002048
Iteration 305/1000 | Loss: 0.00002048
Iteration 306/1000 | Loss: 0.00002047
Iteration 307/1000 | Loss: 0.00002047
Iteration 308/1000 | Loss: 0.00002047
Iteration 309/1000 | Loss: 0.00002047
Iteration 310/1000 | Loss: 0.00002047
Iteration 311/1000 | Loss: 0.00002047
Iteration 312/1000 | Loss: 0.00002047
Iteration 313/1000 | Loss: 0.00002047
Iteration 314/1000 | Loss: 0.00002047
Iteration 315/1000 | Loss: 0.00002047
Iteration 316/1000 | Loss: 0.00002047
Iteration 317/1000 | Loss: 0.00002047
Iteration 318/1000 | Loss: 0.00002047
Iteration 319/1000 | Loss: 0.00002047
Iteration 320/1000 | Loss: 0.00002046
Iteration 321/1000 | Loss: 0.00002046
Iteration 322/1000 | Loss: 0.00002046
Iteration 323/1000 | Loss: 0.00002046
Iteration 324/1000 | Loss: 0.00002046
Iteration 325/1000 | Loss: 0.00002046
Iteration 326/1000 | Loss: 0.00002046
Iteration 327/1000 | Loss: 0.00002046
Iteration 328/1000 | Loss: 0.00002045
Iteration 329/1000 | Loss: 0.00002045
Iteration 330/1000 | Loss: 0.00002045
Iteration 331/1000 | Loss: 0.00002045
Iteration 332/1000 | Loss: 0.00002045
Iteration 333/1000 | Loss: 0.00002045
Iteration 334/1000 | Loss: 0.00002044
Iteration 335/1000 | Loss: 0.00002044
Iteration 336/1000 | Loss: 0.00002044
Iteration 337/1000 | Loss: 0.00002044
Iteration 338/1000 | Loss: 0.00002044
Iteration 339/1000 | Loss: 0.00002044
Iteration 340/1000 | Loss: 0.00002044
Iteration 341/1000 | Loss: 0.00002044
Iteration 342/1000 | Loss: 0.00002044
Iteration 343/1000 | Loss: 0.00002043
Iteration 344/1000 | Loss: 0.00002043
Iteration 345/1000 | Loss: 0.00002043
Iteration 346/1000 | Loss: 0.00002043
Iteration 347/1000 | Loss: 0.00002043
Iteration 348/1000 | Loss: 0.00002043
Iteration 349/1000 | Loss: 0.00002043
Iteration 350/1000 | Loss: 0.00002043
Iteration 351/1000 | Loss: 0.00002043
Iteration 352/1000 | Loss: 0.00002043
Iteration 353/1000 | Loss: 0.00002043
Iteration 354/1000 | Loss: 0.00002043
Iteration 355/1000 | Loss: 0.00002043
Iteration 356/1000 | Loss: 0.00002043
Iteration 357/1000 | Loss: 0.00002043
Iteration 358/1000 | Loss: 0.00002042
Iteration 359/1000 | Loss: 0.00002042
Iteration 360/1000 | Loss: 0.00002042
Iteration 361/1000 | Loss: 0.00002042
Iteration 362/1000 | Loss: 0.00002042
Iteration 363/1000 | Loss: 0.00002042
Iteration 364/1000 | Loss: 0.00002042
Iteration 365/1000 | Loss: 0.00002042
Iteration 366/1000 | Loss: 0.00002042
Iteration 367/1000 | Loss: 0.00002042
Iteration 368/1000 | Loss: 0.00002042
Iteration 369/1000 | Loss: 0.00002042
Iteration 370/1000 | Loss: 0.00002041
Iteration 371/1000 | Loss: 0.00002041
Iteration 372/1000 | Loss: 0.00002041
Iteration 373/1000 | Loss: 0.00002041
Iteration 374/1000 | Loss: 0.00002041
Iteration 375/1000 | Loss: 0.00002041
Iteration 376/1000 | Loss: 0.00002041
Iteration 377/1000 | Loss: 0.00002041
Iteration 378/1000 | Loss: 0.00002041
Iteration 379/1000 | Loss: 0.00002041
Iteration 380/1000 | Loss: 0.00002041
Iteration 381/1000 | Loss: 0.00002041
Iteration 382/1000 | Loss: 0.00002041
Iteration 383/1000 | Loss: 0.00002041
Iteration 384/1000 | Loss: 0.00002041
Iteration 385/1000 | Loss: 0.00002041
Iteration 386/1000 | Loss: 0.00002040
Iteration 387/1000 | Loss: 0.00002040
Iteration 388/1000 | Loss: 0.00002040
Iteration 389/1000 | Loss: 0.00002040
Iteration 390/1000 | Loss: 0.00002040
Iteration 391/1000 | Loss: 0.00002040
Iteration 392/1000 | Loss: 0.00002040
Iteration 393/1000 | Loss: 0.00002040
Iteration 394/1000 | Loss: 0.00002040
Iteration 395/1000 | Loss: 0.00002040
Iteration 396/1000 | Loss: 0.00002040
Iteration 397/1000 | Loss: 0.00002040
Iteration 398/1000 | Loss: 0.00002040
Iteration 399/1000 | Loss: 0.00002040
Iteration 400/1000 | Loss: 0.00002040
Iteration 401/1000 | Loss: 0.00002040
Iteration 402/1000 | Loss: 0.00002040
Iteration 403/1000 | Loss: 0.00002040
Iteration 404/1000 | Loss: 0.00002040
Iteration 405/1000 | Loss: 0.00002040
Iteration 406/1000 | Loss: 0.00002040
Iteration 407/1000 | Loss: 0.00002040
Iteration 408/1000 | Loss: 0.00002040
Iteration 409/1000 | Loss: 0.00002040
Iteration 410/1000 | Loss: 0.00002040
Iteration 411/1000 | Loss: 0.00002040
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 411. Stopping optimization.
Last 5 losses: [2.0396348190843128e-05, 2.0396348190843128e-05, 2.0396348190843128e-05, 2.0396348190843128e-05, 2.0396348190843128e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0396348190843128e-05

Optimization complete. Final v2v error: 3.758971929550171 mm

Highest mean error: 5.712292671203613 mm for frame 117

Lowest mean error: 2.7789809703826904 mm for frame 43

Saving results

Total time: 441.95366287231445
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00904813
Iteration 2/25 | Loss: 0.00160819
Iteration 3/25 | Loss: 0.00129610
Iteration 4/25 | Loss: 0.00123044
Iteration 5/25 | Loss: 0.00121180
Iteration 6/25 | Loss: 0.00119057
Iteration 7/25 | Loss: 0.00118693
Iteration 8/25 | Loss: 0.00117980
Iteration 9/25 | Loss: 0.00117781
Iteration 10/25 | Loss: 0.00118517
Iteration 11/25 | Loss: 0.00117574
Iteration 12/25 | Loss: 0.00117493
Iteration 13/25 | Loss: 0.00117473
Iteration 14/25 | Loss: 0.00117465
Iteration 15/25 | Loss: 0.00117464
Iteration 16/25 | Loss: 0.00117464
Iteration 17/25 | Loss: 0.00117464
Iteration 18/25 | Loss: 0.00117464
Iteration 19/25 | Loss: 0.00117464
Iteration 20/25 | Loss: 0.00117464
Iteration 21/25 | Loss: 0.00117464
Iteration 22/25 | Loss: 0.00117464
Iteration 23/25 | Loss: 0.00117463
Iteration 24/25 | Loss: 0.00117463
Iteration 25/25 | Loss: 0.00117463

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.11740541
Iteration 2/25 | Loss: 0.00084340
Iteration 3/25 | Loss: 0.00084336
Iteration 4/25 | Loss: 0.00084336
Iteration 5/25 | Loss: 0.00084336
Iteration 6/25 | Loss: 0.00084336
Iteration 7/25 | Loss: 0.00084336
Iteration 8/25 | Loss: 0.00084336
Iteration 9/25 | Loss: 0.00084336
Iteration 10/25 | Loss: 0.00084336
Iteration 11/25 | Loss: 0.00084336
Iteration 12/25 | Loss: 0.00084336
Iteration 13/25 | Loss: 0.00084336
Iteration 14/25 | Loss: 0.00084336
Iteration 15/25 | Loss: 0.00084336
Iteration 16/25 | Loss: 0.00084336
Iteration 17/25 | Loss: 0.00084336
Iteration 18/25 | Loss: 0.00084336
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008433570037595928, 0.0008433570037595928, 0.0008433570037595928, 0.0008433570037595928, 0.0008433570037595928]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008433570037595928

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084336
Iteration 2/1000 | Loss: 0.00004143
Iteration 3/1000 | Loss: 0.00002572
Iteration 4/1000 | Loss: 0.00002322
Iteration 5/1000 | Loss: 0.00002216
Iteration 6/1000 | Loss: 0.00002126
Iteration 7/1000 | Loss: 0.00002064
Iteration 8/1000 | Loss: 0.00002017
Iteration 9/1000 | Loss: 0.00001989
Iteration 10/1000 | Loss: 0.00001962
Iteration 11/1000 | Loss: 0.00001939
Iteration 12/1000 | Loss: 0.00001926
Iteration 13/1000 | Loss: 0.00001922
Iteration 14/1000 | Loss: 0.00001918
Iteration 15/1000 | Loss: 0.00001916
Iteration 16/1000 | Loss: 0.00001908
Iteration 17/1000 | Loss: 0.00001902
Iteration 18/1000 | Loss: 0.00001896
Iteration 19/1000 | Loss: 0.00001896
Iteration 20/1000 | Loss: 0.00001894
Iteration 21/1000 | Loss: 0.00001893
Iteration 22/1000 | Loss: 0.00001893
Iteration 23/1000 | Loss: 0.00001893
Iteration 24/1000 | Loss: 0.00001892
Iteration 25/1000 | Loss: 0.00001892
Iteration 26/1000 | Loss: 0.00001892
Iteration 27/1000 | Loss: 0.00001891
Iteration 28/1000 | Loss: 0.00001891
Iteration 29/1000 | Loss: 0.00001891
Iteration 30/1000 | Loss: 0.00001890
Iteration 31/1000 | Loss: 0.00001890
Iteration 32/1000 | Loss: 0.00001890
Iteration 33/1000 | Loss: 0.00001890
Iteration 34/1000 | Loss: 0.00001889
Iteration 35/1000 | Loss: 0.00001889
Iteration 36/1000 | Loss: 0.00001889
Iteration 37/1000 | Loss: 0.00001888
Iteration 38/1000 | Loss: 0.00001888
Iteration 39/1000 | Loss: 0.00001888
Iteration 40/1000 | Loss: 0.00001887
Iteration 41/1000 | Loss: 0.00001887
Iteration 42/1000 | Loss: 0.00001887
Iteration 43/1000 | Loss: 0.00001886
Iteration 44/1000 | Loss: 0.00001886
Iteration 45/1000 | Loss: 0.00001886
Iteration 46/1000 | Loss: 0.00001886
Iteration 47/1000 | Loss: 0.00001885
Iteration 48/1000 | Loss: 0.00001885
Iteration 49/1000 | Loss: 0.00001885
Iteration 50/1000 | Loss: 0.00001885
Iteration 51/1000 | Loss: 0.00001885
Iteration 52/1000 | Loss: 0.00001885
Iteration 53/1000 | Loss: 0.00001885
Iteration 54/1000 | Loss: 0.00001884
Iteration 55/1000 | Loss: 0.00001884
Iteration 56/1000 | Loss: 0.00001883
Iteration 57/1000 | Loss: 0.00001883
Iteration 58/1000 | Loss: 0.00001883
Iteration 59/1000 | Loss: 0.00001883
Iteration 60/1000 | Loss: 0.00001882
Iteration 61/1000 | Loss: 0.00001882
Iteration 62/1000 | Loss: 0.00001882
Iteration 63/1000 | Loss: 0.00001881
Iteration 64/1000 | Loss: 0.00001881
Iteration 65/1000 | Loss: 0.00001881
Iteration 66/1000 | Loss: 0.00001881
Iteration 67/1000 | Loss: 0.00001881
Iteration 68/1000 | Loss: 0.00001881
Iteration 69/1000 | Loss: 0.00001880
Iteration 70/1000 | Loss: 0.00001880
Iteration 71/1000 | Loss: 0.00001880
Iteration 72/1000 | Loss: 0.00001879
Iteration 73/1000 | Loss: 0.00001879
Iteration 74/1000 | Loss: 0.00001879
Iteration 75/1000 | Loss: 0.00001879
Iteration 76/1000 | Loss: 0.00001879
Iteration 77/1000 | Loss: 0.00001879
Iteration 78/1000 | Loss: 0.00001878
Iteration 79/1000 | Loss: 0.00001878
Iteration 80/1000 | Loss: 0.00001878
Iteration 81/1000 | Loss: 0.00001878
Iteration 82/1000 | Loss: 0.00001878
Iteration 83/1000 | Loss: 0.00001878
Iteration 84/1000 | Loss: 0.00001878
Iteration 85/1000 | Loss: 0.00001878
Iteration 86/1000 | Loss: 0.00001878
Iteration 87/1000 | Loss: 0.00001878
Iteration 88/1000 | Loss: 0.00001878
Iteration 89/1000 | Loss: 0.00001877
Iteration 90/1000 | Loss: 0.00001877
Iteration 91/1000 | Loss: 0.00001877
Iteration 92/1000 | Loss: 0.00001877
Iteration 93/1000 | Loss: 0.00001876
Iteration 94/1000 | Loss: 0.00001876
Iteration 95/1000 | Loss: 0.00001876
Iteration 96/1000 | Loss: 0.00001876
Iteration 97/1000 | Loss: 0.00001876
Iteration 98/1000 | Loss: 0.00001876
Iteration 99/1000 | Loss: 0.00001876
Iteration 100/1000 | Loss: 0.00001876
Iteration 101/1000 | Loss: 0.00001876
Iteration 102/1000 | Loss: 0.00001876
Iteration 103/1000 | Loss: 0.00001876
Iteration 104/1000 | Loss: 0.00001875
Iteration 105/1000 | Loss: 0.00001875
Iteration 106/1000 | Loss: 0.00001875
Iteration 107/1000 | Loss: 0.00001875
Iteration 108/1000 | Loss: 0.00001875
Iteration 109/1000 | Loss: 0.00001875
Iteration 110/1000 | Loss: 0.00001875
Iteration 111/1000 | Loss: 0.00001875
Iteration 112/1000 | Loss: 0.00001875
Iteration 113/1000 | Loss: 0.00001875
Iteration 114/1000 | Loss: 0.00001875
Iteration 115/1000 | Loss: 0.00001875
Iteration 116/1000 | Loss: 0.00001875
Iteration 117/1000 | Loss: 0.00001875
Iteration 118/1000 | Loss: 0.00001875
Iteration 119/1000 | Loss: 0.00001875
Iteration 120/1000 | Loss: 0.00001875
Iteration 121/1000 | Loss: 0.00001875
Iteration 122/1000 | Loss: 0.00001875
Iteration 123/1000 | Loss: 0.00001875
Iteration 124/1000 | Loss: 0.00001875
Iteration 125/1000 | Loss: 0.00001875
Iteration 126/1000 | Loss: 0.00001875
Iteration 127/1000 | Loss: 0.00001875
Iteration 128/1000 | Loss: 0.00001875
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.8750053641269915e-05, 1.8750053641269915e-05, 1.8750053641269915e-05, 1.8750053641269915e-05, 1.8750053641269915e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8750053641269915e-05

Optimization complete. Final v2v error: 3.59637451171875 mm

Highest mean error: 4.281619071960449 mm for frame 63

Lowest mean error: 3.1611123085021973 mm for frame 138

Saving results

Total time: 52.41021275520325
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00693952
Iteration 2/25 | Loss: 0.00141772
Iteration 3/25 | Loss: 0.00122545
Iteration 4/25 | Loss: 0.00119192
Iteration 5/25 | Loss: 0.00118034
Iteration 6/25 | Loss: 0.00117556
Iteration 7/25 | Loss: 0.00118129
Iteration 8/25 | Loss: 0.00117550
Iteration 9/25 | Loss: 0.00117647
Iteration 10/25 | Loss: 0.00117419
Iteration 11/25 | Loss: 0.00116941
Iteration 12/25 | Loss: 0.00116568
Iteration 13/25 | Loss: 0.00116222
Iteration 14/25 | Loss: 0.00116108
Iteration 15/25 | Loss: 0.00116074
Iteration 16/25 | Loss: 0.00116064
Iteration 17/25 | Loss: 0.00116062
Iteration 18/25 | Loss: 0.00116062
Iteration 19/25 | Loss: 0.00116061
Iteration 20/25 | Loss: 0.00116061
Iteration 21/25 | Loss: 0.00116061
Iteration 22/25 | Loss: 0.00116061
Iteration 23/25 | Loss: 0.00116061
Iteration 24/25 | Loss: 0.00116061
Iteration 25/25 | Loss: 0.00116061

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52419460
Iteration 2/25 | Loss: 0.00079625
Iteration 3/25 | Loss: 0.00079624
Iteration 4/25 | Loss: 0.00079624
Iteration 5/25 | Loss: 0.00079624
Iteration 6/25 | Loss: 0.00079624
Iteration 7/25 | Loss: 0.00079624
Iteration 8/25 | Loss: 0.00079624
Iteration 9/25 | Loss: 0.00079624
Iteration 10/25 | Loss: 0.00079624
Iteration 11/25 | Loss: 0.00079624
Iteration 12/25 | Loss: 0.00079624
Iteration 13/25 | Loss: 0.00079624
Iteration 14/25 | Loss: 0.00079624
Iteration 15/25 | Loss: 0.00079624
Iteration 16/25 | Loss: 0.00079624
Iteration 17/25 | Loss: 0.00079624
Iteration 18/25 | Loss: 0.00079624
Iteration 19/25 | Loss: 0.00079624
Iteration 20/25 | Loss: 0.00079624
Iteration 21/25 | Loss: 0.00079624
Iteration 22/25 | Loss: 0.00079624
Iteration 23/25 | Loss: 0.00079624
Iteration 24/25 | Loss: 0.00079624
Iteration 25/25 | Loss: 0.00079624

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079624
Iteration 2/1000 | Loss: 0.00006443
Iteration 3/1000 | Loss: 0.00003721
Iteration 4/1000 | Loss: 0.00002824
Iteration 5/1000 | Loss: 0.00002567
Iteration 6/1000 | Loss: 0.00002415
Iteration 7/1000 | Loss: 0.00002323
Iteration 8/1000 | Loss: 0.00002266
Iteration 9/1000 | Loss: 0.00002216
Iteration 10/1000 | Loss: 0.00002187
Iteration 11/1000 | Loss: 0.00002161
Iteration 12/1000 | Loss: 0.00002148
Iteration 13/1000 | Loss: 0.00002127
Iteration 14/1000 | Loss: 0.00002108
Iteration 15/1000 | Loss: 0.00002106
Iteration 16/1000 | Loss: 0.00002104
Iteration 17/1000 | Loss: 0.00002099
Iteration 18/1000 | Loss: 0.00002097
Iteration 19/1000 | Loss: 0.00002096
Iteration 20/1000 | Loss: 0.00002095
Iteration 21/1000 | Loss: 0.00002094
Iteration 22/1000 | Loss: 0.00002092
Iteration 23/1000 | Loss: 0.00002088
Iteration 24/1000 | Loss: 0.00002084
Iteration 25/1000 | Loss: 0.00002081
Iteration 26/1000 | Loss: 0.00002080
Iteration 27/1000 | Loss: 0.00002080
Iteration 28/1000 | Loss: 0.00002078
Iteration 29/1000 | Loss: 0.00002077
Iteration 30/1000 | Loss: 0.00002076
Iteration 31/1000 | Loss: 0.00002069
Iteration 32/1000 | Loss: 0.00002069
Iteration 33/1000 | Loss: 0.00002068
Iteration 34/1000 | Loss: 0.00002068
Iteration 35/1000 | Loss: 0.00002066
Iteration 36/1000 | Loss: 0.00002065
Iteration 37/1000 | Loss: 0.00002064
Iteration 38/1000 | Loss: 0.00002064
Iteration 39/1000 | Loss: 0.00002063
Iteration 40/1000 | Loss: 0.00002061
Iteration 41/1000 | Loss: 0.00002061
Iteration 42/1000 | Loss: 0.00002061
Iteration 43/1000 | Loss: 0.00002060
Iteration 44/1000 | Loss: 0.00002060
Iteration 45/1000 | Loss: 0.00002057
Iteration 46/1000 | Loss: 0.00002057
Iteration 47/1000 | Loss: 0.00002057
Iteration 48/1000 | Loss: 0.00002057
Iteration 49/1000 | Loss: 0.00002057
Iteration 50/1000 | Loss: 0.00002057
Iteration 51/1000 | Loss: 0.00002057
Iteration 52/1000 | Loss: 0.00002056
Iteration 53/1000 | Loss: 0.00002055
Iteration 54/1000 | Loss: 0.00002055
Iteration 55/1000 | Loss: 0.00002054
Iteration 56/1000 | Loss: 0.00002054
Iteration 57/1000 | Loss: 0.00002054
Iteration 58/1000 | Loss: 0.00002053
Iteration 59/1000 | Loss: 0.00002053
Iteration 60/1000 | Loss: 0.00002053
Iteration 61/1000 | Loss: 0.00002053
Iteration 62/1000 | Loss: 0.00002052
Iteration 63/1000 | Loss: 0.00002052
Iteration 64/1000 | Loss: 0.00002052
Iteration 65/1000 | Loss: 0.00002052
Iteration 66/1000 | Loss: 0.00002051
Iteration 67/1000 | Loss: 0.00002051
Iteration 68/1000 | Loss: 0.00002051
Iteration 69/1000 | Loss: 0.00002050
Iteration 70/1000 | Loss: 0.00002050
Iteration 71/1000 | Loss: 0.00002050
Iteration 72/1000 | Loss: 0.00002050
Iteration 73/1000 | Loss: 0.00002050
Iteration 74/1000 | Loss: 0.00002049
Iteration 75/1000 | Loss: 0.00002049
Iteration 76/1000 | Loss: 0.00002049
Iteration 77/1000 | Loss: 0.00002049
Iteration 78/1000 | Loss: 0.00002049
Iteration 79/1000 | Loss: 0.00002048
Iteration 80/1000 | Loss: 0.00002048
Iteration 81/1000 | Loss: 0.00002048
Iteration 82/1000 | Loss: 0.00002048
Iteration 83/1000 | Loss: 0.00002048
Iteration 84/1000 | Loss: 0.00002048
Iteration 85/1000 | Loss: 0.00002048
Iteration 86/1000 | Loss: 0.00002048
Iteration 87/1000 | Loss: 0.00002048
Iteration 88/1000 | Loss: 0.00002047
Iteration 89/1000 | Loss: 0.00002047
Iteration 90/1000 | Loss: 0.00002047
Iteration 91/1000 | Loss: 0.00002047
Iteration 92/1000 | Loss: 0.00002047
Iteration 93/1000 | Loss: 0.00002047
Iteration 94/1000 | Loss: 0.00002047
Iteration 95/1000 | Loss: 0.00002047
Iteration 96/1000 | Loss: 0.00002047
Iteration 97/1000 | Loss: 0.00002047
Iteration 98/1000 | Loss: 0.00002047
Iteration 99/1000 | Loss: 0.00002046
Iteration 100/1000 | Loss: 0.00002046
Iteration 101/1000 | Loss: 0.00002046
Iteration 102/1000 | Loss: 0.00002046
Iteration 103/1000 | Loss: 0.00002046
Iteration 104/1000 | Loss: 0.00002046
Iteration 105/1000 | Loss: 0.00002046
Iteration 106/1000 | Loss: 0.00002046
Iteration 107/1000 | Loss: 0.00002045
Iteration 108/1000 | Loss: 0.00002045
Iteration 109/1000 | Loss: 0.00002045
Iteration 110/1000 | Loss: 0.00002045
Iteration 111/1000 | Loss: 0.00002044
Iteration 112/1000 | Loss: 0.00002044
Iteration 113/1000 | Loss: 0.00002044
Iteration 114/1000 | Loss: 0.00002044
Iteration 115/1000 | Loss: 0.00002043
Iteration 116/1000 | Loss: 0.00002043
Iteration 117/1000 | Loss: 0.00002043
Iteration 118/1000 | Loss: 0.00002043
Iteration 119/1000 | Loss: 0.00002042
Iteration 120/1000 | Loss: 0.00002042
Iteration 121/1000 | Loss: 0.00002042
Iteration 122/1000 | Loss: 0.00002042
Iteration 123/1000 | Loss: 0.00002042
Iteration 124/1000 | Loss: 0.00002042
Iteration 125/1000 | Loss: 0.00002042
Iteration 126/1000 | Loss: 0.00002042
Iteration 127/1000 | Loss: 0.00002042
Iteration 128/1000 | Loss: 0.00002042
Iteration 129/1000 | Loss: 0.00002042
Iteration 130/1000 | Loss: 0.00002042
Iteration 131/1000 | Loss: 0.00002042
Iteration 132/1000 | Loss: 0.00002042
Iteration 133/1000 | Loss: 0.00002041
Iteration 134/1000 | Loss: 0.00002041
Iteration 135/1000 | Loss: 0.00002041
Iteration 136/1000 | Loss: 0.00002041
Iteration 137/1000 | Loss: 0.00002041
Iteration 138/1000 | Loss: 0.00002041
Iteration 139/1000 | Loss: 0.00002040
Iteration 140/1000 | Loss: 0.00002040
Iteration 141/1000 | Loss: 0.00002040
Iteration 142/1000 | Loss: 0.00002040
Iteration 143/1000 | Loss: 0.00002040
Iteration 144/1000 | Loss: 0.00002040
Iteration 145/1000 | Loss: 0.00002040
Iteration 146/1000 | Loss: 0.00002039
Iteration 147/1000 | Loss: 0.00002039
Iteration 148/1000 | Loss: 0.00002039
Iteration 149/1000 | Loss: 0.00002039
Iteration 150/1000 | Loss: 0.00002039
Iteration 151/1000 | Loss: 0.00002039
Iteration 152/1000 | Loss: 0.00002039
Iteration 153/1000 | Loss: 0.00002039
Iteration 154/1000 | Loss: 0.00002039
Iteration 155/1000 | Loss: 0.00002038
Iteration 156/1000 | Loss: 0.00002038
Iteration 157/1000 | Loss: 0.00002038
Iteration 158/1000 | Loss: 0.00002038
Iteration 159/1000 | Loss: 0.00002038
Iteration 160/1000 | Loss: 0.00002038
Iteration 161/1000 | Loss: 0.00002038
Iteration 162/1000 | Loss: 0.00002038
Iteration 163/1000 | Loss: 0.00002038
Iteration 164/1000 | Loss: 0.00002038
Iteration 165/1000 | Loss: 0.00002038
Iteration 166/1000 | Loss: 0.00002038
Iteration 167/1000 | Loss: 0.00002038
Iteration 168/1000 | Loss: 0.00002037
Iteration 169/1000 | Loss: 0.00002037
Iteration 170/1000 | Loss: 0.00002037
Iteration 171/1000 | Loss: 0.00002036
Iteration 172/1000 | Loss: 0.00002036
Iteration 173/1000 | Loss: 0.00002036
Iteration 174/1000 | Loss: 0.00002036
Iteration 175/1000 | Loss: 0.00002036
Iteration 176/1000 | Loss: 0.00002036
Iteration 177/1000 | Loss: 0.00002036
Iteration 178/1000 | Loss: 0.00002036
Iteration 179/1000 | Loss: 0.00002036
Iteration 180/1000 | Loss: 0.00002036
Iteration 181/1000 | Loss: 0.00002036
Iteration 182/1000 | Loss: 0.00002035
Iteration 183/1000 | Loss: 0.00002035
Iteration 184/1000 | Loss: 0.00002035
Iteration 185/1000 | Loss: 0.00002035
Iteration 186/1000 | Loss: 0.00002035
Iteration 187/1000 | Loss: 0.00002035
Iteration 188/1000 | Loss: 0.00002035
Iteration 189/1000 | Loss: 0.00002035
Iteration 190/1000 | Loss: 0.00002035
Iteration 191/1000 | Loss: 0.00002035
Iteration 192/1000 | Loss: 0.00002035
Iteration 193/1000 | Loss: 0.00002035
Iteration 194/1000 | Loss: 0.00002035
Iteration 195/1000 | Loss: 0.00002035
Iteration 196/1000 | Loss: 0.00002035
Iteration 197/1000 | Loss: 0.00002035
Iteration 198/1000 | Loss: 0.00002035
Iteration 199/1000 | Loss: 0.00002034
Iteration 200/1000 | Loss: 0.00002034
Iteration 201/1000 | Loss: 0.00002034
Iteration 202/1000 | Loss: 0.00002034
Iteration 203/1000 | Loss: 0.00002034
Iteration 204/1000 | Loss: 0.00002034
Iteration 205/1000 | Loss: 0.00002034
Iteration 206/1000 | Loss: 0.00002034
Iteration 207/1000 | Loss: 0.00002034
Iteration 208/1000 | Loss: 0.00002034
Iteration 209/1000 | Loss: 0.00002034
Iteration 210/1000 | Loss: 0.00002034
Iteration 211/1000 | Loss: 0.00002034
Iteration 212/1000 | Loss: 0.00002033
Iteration 213/1000 | Loss: 0.00002033
Iteration 214/1000 | Loss: 0.00002033
Iteration 215/1000 | Loss: 0.00002033
Iteration 216/1000 | Loss: 0.00002033
Iteration 217/1000 | Loss: 0.00002033
Iteration 218/1000 | Loss: 0.00002033
Iteration 219/1000 | Loss: 0.00002033
Iteration 220/1000 | Loss: 0.00002033
Iteration 221/1000 | Loss: 0.00002033
Iteration 222/1000 | Loss: 0.00002033
Iteration 223/1000 | Loss: 0.00002033
Iteration 224/1000 | Loss: 0.00002033
Iteration 225/1000 | Loss: 0.00002033
Iteration 226/1000 | Loss: 0.00002033
Iteration 227/1000 | Loss: 0.00002033
Iteration 228/1000 | Loss: 0.00002033
Iteration 229/1000 | Loss: 0.00002033
Iteration 230/1000 | Loss: 0.00002032
Iteration 231/1000 | Loss: 0.00002032
Iteration 232/1000 | Loss: 0.00002032
Iteration 233/1000 | Loss: 0.00002032
Iteration 234/1000 | Loss: 0.00002032
Iteration 235/1000 | Loss: 0.00002032
Iteration 236/1000 | Loss: 0.00002031
Iteration 237/1000 | Loss: 0.00002031
Iteration 238/1000 | Loss: 0.00002031
Iteration 239/1000 | Loss: 0.00002031
Iteration 240/1000 | Loss: 0.00002031
Iteration 241/1000 | Loss: 0.00002031
Iteration 242/1000 | Loss: 0.00002031
Iteration 243/1000 | Loss: 0.00002031
Iteration 244/1000 | Loss: 0.00002031
Iteration 245/1000 | Loss: 0.00002031
Iteration 246/1000 | Loss: 0.00002031
Iteration 247/1000 | Loss: 0.00002031
Iteration 248/1000 | Loss: 0.00002031
Iteration 249/1000 | Loss: 0.00002031
Iteration 250/1000 | Loss: 0.00002031
Iteration 251/1000 | Loss: 0.00002031
Iteration 252/1000 | Loss: 0.00002031
Iteration 253/1000 | Loss: 0.00002031
Iteration 254/1000 | Loss: 0.00002031
Iteration 255/1000 | Loss: 0.00002030
Iteration 256/1000 | Loss: 0.00002030
Iteration 257/1000 | Loss: 0.00002030
Iteration 258/1000 | Loss: 0.00002030
Iteration 259/1000 | Loss: 0.00002030
Iteration 260/1000 | Loss: 0.00002030
Iteration 261/1000 | Loss: 0.00002030
Iteration 262/1000 | Loss: 0.00002030
Iteration 263/1000 | Loss: 0.00002030
Iteration 264/1000 | Loss: 0.00002030
Iteration 265/1000 | Loss: 0.00002030
Iteration 266/1000 | Loss: 0.00002030
Iteration 267/1000 | Loss: 0.00002030
Iteration 268/1000 | Loss: 0.00002030
Iteration 269/1000 | Loss: 0.00002030
Iteration 270/1000 | Loss: 0.00002030
Iteration 271/1000 | Loss: 0.00002030
Iteration 272/1000 | Loss: 0.00002030
Iteration 273/1000 | Loss: 0.00002030
Iteration 274/1000 | Loss: 0.00002030
Iteration 275/1000 | Loss: 0.00002030
Iteration 276/1000 | Loss: 0.00002030
Iteration 277/1000 | Loss: 0.00002030
Iteration 278/1000 | Loss: 0.00002030
Iteration 279/1000 | Loss: 0.00002030
Iteration 280/1000 | Loss: 0.00002030
Iteration 281/1000 | Loss: 0.00002030
Iteration 282/1000 | Loss: 0.00002030
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 282. Stopping optimization.
Last 5 losses: [2.0298617528169416e-05, 2.0298617528169416e-05, 2.0298617528169416e-05, 2.0298617528169416e-05, 2.0298617528169416e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0298617528169416e-05

Optimization complete. Final v2v error: 3.740110397338867 mm

Highest mean error: 5.495640754699707 mm for frame 112

Lowest mean error: 2.912618637084961 mm for frame 20

Saving results

Total time: 65.6628577709198
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00827758
Iteration 2/25 | Loss: 0.00168001
Iteration 3/25 | Loss: 0.00144987
Iteration 4/25 | Loss: 0.00139216
Iteration 5/25 | Loss: 0.00139521
Iteration 6/25 | Loss: 0.00137008
Iteration 7/25 | Loss: 0.00142093
Iteration 8/25 | Loss: 0.00137064
Iteration 9/25 | Loss: 0.00131182
Iteration 10/25 | Loss: 0.00132332
Iteration 11/25 | Loss: 0.00131341
Iteration 12/25 | Loss: 0.00131638
Iteration 13/25 | Loss: 0.00129041
Iteration 14/25 | Loss: 0.00124684
Iteration 15/25 | Loss: 0.00123181
Iteration 16/25 | Loss: 0.00122706
Iteration 17/25 | Loss: 0.00122226
Iteration 18/25 | Loss: 0.00121925
Iteration 19/25 | Loss: 0.00121798
Iteration 20/25 | Loss: 0.00122047
Iteration 21/25 | Loss: 0.00121778
Iteration 22/25 | Loss: 0.00121334
Iteration 23/25 | Loss: 0.00121481
Iteration 24/25 | Loss: 0.00121125
Iteration 25/25 | Loss: 0.00121309

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31291080
Iteration 2/25 | Loss: 0.00105675
Iteration 3/25 | Loss: 0.00105672
Iteration 4/25 | Loss: 0.00105672
Iteration 5/25 | Loss: 0.00105672
Iteration 6/25 | Loss: 0.00105672
Iteration 7/25 | Loss: 0.00105672
Iteration 8/25 | Loss: 0.00105672
Iteration 9/25 | Loss: 0.00105672
Iteration 10/25 | Loss: 0.00105672
Iteration 11/25 | Loss: 0.00105672
Iteration 12/25 | Loss: 0.00105672
Iteration 13/25 | Loss: 0.00105672
Iteration 14/25 | Loss: 0.00105672
Iteration 15/25 | Loss: 0.00105672
Iteration 16/25 | Loss: 0.00105672
Iteration 17/25 | Loss: 0.00105672
Iteration 18/25 | Loss: 0.00105672
Iteration 19/25 | Loss: 0.00105672
Iteration 20/25 | Loss: 0.00105672
Iteration 21/25 | Loss: 0.00105672
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0010567217832431197, 0.0010567217832431197, 0.0010567217832431197, 0.0010567217832431197, 0.0010567217832431197]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010567217832431197

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105672
Iteration 2/1000 | Loss: 0.00017903
Iteration 3/1000 | Loss: 0.00012386
Iteration 4/1000 | Loss: 0.00012606
Iteration 5/1000 | Loss: 0.00010906
Iteration 6/1000 | Loss: 0.00017796
Iteration 7/1000 | Loss: 0.00026026
Iteration 8/1000 | Loss: 0.00021182
Iteration 9/1000 | Loss: 0.00014624
Iteration 10/1000 | Loss: 0.00015159
Iteration 11/1000 | Loss: 0.00013123
Iteration 12/1000 | Loss: 0.00008967
Iteration 13/1000 | Loss: 0.00011272
Iteration 14/1000 | Loss: 0.00065525
Iteration 15/1000 | Loss: 0.00040880
Iteration 16/1000 | Loss: 0.00004537
Iteration 17/1000 | Loss: 0.00039231
Iteration 18/1000 | Loss: 0.00016772
Iteration 19/1000 | Loss: 0.00082686
Iteration 20/1000 | Loss: 0.00056765
Iteration 21/1000 | Loss: 0.00025881
Iteration 22/1000 | Loss: 0.00035512
Iteration 23/1000 | Loss: 0.00107918
Iteration 24/1000 | Loss: 0.00007902
Iteration 25/1000 | Loss: 0.00010534
Iteration 26/1000 | Loss: 0.00011224
Iteration 27/1000 | Loss: 0.00009637
Iteration 28/1000 | Loss: 0.00012384
Iteration 29/1000 | Loss: 0.00011028
Iteration 30/1000 | Loss: 0.00012477
Iteration 31/1000 | Loss: 0.00010125
Iteration 32/1000 | Loss: 0.00014759
Iteration 33/1000 | Loss: 0.00013306
Iteration 34/1000 | Loss: 0.00012730
Iteration 35/1000 | Loss: 0.00011171
Iteration 36/1000 | Loss: 0.00012089
Iteration 37/1000 | Loss: 0.00013118
Iteration 38/1000 | Loss: 0.00004859
Iteration 39/1000 | Loss: 0.00017836
Iteration 40/1000 | Loss: 0.00004519
Iteration 41/1000 | Loss: 0.00004084
Iteration 42/1000 | Loss: 0.00027206
Iteration 43/1000 | Loss: 0.00004043
Iteration 44/1000 | Loss: 0.00003690
Iteration 45/1000 | Loss: 0.00003514
Iteration 46/1000 | Loss: 0.00017370
Iteration 47/1000 | Loss: 0.00023927
Iteration 48/1000 | Loss: 0.00029916
Iteration 49/1000 | Loss: 0.00027266
Iteration 50/1000 | Loss: 0.00025203
Iteration 51/1000 | Loss: 0.00024972
Iteration 52/1000 | Loss: 0.00017058
Iteration 53/1000 | Loss: 0.00011461
Iteration 54/1000 | Loss: 0.00015994
Iteration 55/1000 | Loss: 0.00025937
Iteration 56/1000 | Loss: 0.00023570
Iteration 57/1000 | Loss: 0.00163457
Iteration 58/1000 | Loss: 0.00004950
Iteration 59/1000 | Loss: 0.00014869
Iteration 60/1000 | Loss: 0.00016505
Iteration 61/1000 | Loss: 0.00003701
Iteration 62/1000 | Loss: 0.00015132
Iteration 63/1000 | Loss: 0.00003325
Iteration 64/1000 | Loss: 0.00022146
Iteration 65/1000 | Loss: 0.00055118
Iteration 66/1000 | Loss: 0.00030772
Iteration 67/1000 | Loss: 0.00024111
Iteration 68/1000 | Loss: 0.00003439
Iteration 69/1000 | Loss: 0.00010295
Iteration 70/1000 | Loss: 0.00003049
Iteration 71/1000 | Loss: 0.00002731
Iteration 72/1000 | Loss: 0.00002550
Iteration 73/1000 | Loss: 0.00002465
Iteration 74/1000 | Loss: 0.00002391
Iteration 75/1000 | Loss: 0.00002342
Iteration 76/1000 | Loss: 0.00002308
Iteration 77/1000 | Loss: 0.00002280
Iteration 78/1000 | Loss: 0.00002247
Iteration 79/1000 | Loss: 0.00002223
Iteration 80/1000 | Loss: 0.00002203
Iteration 81/1000 | Loss: 0.00002190
Iteration 82/1000 | Loss: 0.00002182
Iteration 83/1000 | Loss: 0.00002175
Iteration 84/1000 | Loss: 0.00002174
Iteration 85/1000 | Loss: 0.00002170
Iteration 86/1000 | Loss: 0.00002169
Iteration 87/1000 | Loss: 0.00002168
Iteration 88/1000 | Loss: 0.00002167
Iteration 89/1000 | Loss: 0.00002154
Iteration 90/1000 | Loss: 0.00002148
Iteration 91/1000 | Loss: 0.00002147
Iteration 92/1000 | Loss: 0.00002144
Iteration 93/1000 | Loss: 0.00002143
Iteration 94/1000 | Loss: 0.00002142
Iteration 95/1000 | Loss: 0.00002141
Iteration 96/1000 | Loss: 0.00002140
Iteration 97/1000 | Loss: 0.00002904
Iteration 98/1000 | Loss: 0.00002903
Iteration 99/1000 | Loss: 0.00002344
Iteration 100/1000 | Loss: 0.00002237
Iteration 101/1000 | Loss: 0.00002188
Iteration 102/1000 | Loss: 0.00002148
Iteration 103/1000 | Loss: 0.00002139
Iteration 104/1000 | Loss: 0.00002133
Iteration 105/1000 | Loss: 0.00002132
Iteration 106/1000 | Loss: 0.00002130
Iteration 107/1000 | Loss: 0.00002129
Iteration 108/1000 | Loss: 0.00002129
Iteration 109/1000 | Loss: 0.00002128
Iteration 110/1000 | Loss: 0.00002127
Iteration 111/1000 | Loss: 0.00002127
Iteration 112/1000 | Loss: 0.00002125
Iteration 113/1000 | Loss: 0.00002124
Iteration 114/1000 | Loss: 0.00002123
Iteration 115/1000 | Loss: 0.00002122
Iteration 116/1000 | Loss: 0.00002121
Iteration 117/1000 | Loss: 0.00002119
Iteration 118/1000 | Loss: 0.00002119
Iteration 119/1000 | Loss: 0.00002106
Iteration 120/1000 | Loss: 0.00002098
Iteration 121/1000 | Loss: 0.00002098
Iteration 122/1000 | Loss: 0.00002096
Iteration 123/1000 | Loss: 0.00002095
Iteration 124/1000 | Loss: 0.00002095
Iteration 125/1000 | Loss: 0.00002095
Iteration 126/1000 | Loss: 0.00002095
Iteration 127/1000 | Loss: 0.00002094
Iteration 128/1000 | Loss: 0.00002094
Iteration 129/1000 | Loss: 0.00002094
Iteration 130/1000 | Loss: 0.00002094
Iteration 131/1000 | Loss: 0.00002093
Iteration 132/1000 | Loss: 0.00002093
Iteration 133/1000 | Loss: 0.00002093
Iteration 134/1000 | Loss: 0.00002093
Iteration 135/1000 | Loss: 0.00002093
Iteration 136/1000 | Loss: 0.00002093
Iteration 137/1000 | Loss: 0.00002093
Iteration 138/1000 | Loss: 0.00002093
Iteration 139/1000 | Loss: 0.00002092
Iteration 140/1000 | Loss: 0.00002092
Iteration 141/1000 | Loss: 0.00002091
Iteration 142/1000 | Loss: 0.00002091
Iteration 143/1000 | Loss: 0.00002090
Iteration 144/1000 | Loss: 0.00002090
Iteration 145/1000 | Loss: 0.00002089
Iteration 146/1000 | Loss: 0.00002089
Iteration 147/1000 | Loss: 0.00002088
Iteration 148/1000 | Loss: 0.00002088
Iteration 149/1000 | Loss: 0.00002087
Iteration 150/1000 | Loss: 0.00002087
Iteration 151/1000 | Loss: 0.00002086
Iteration 152/1000 | Loss: 0.00002086
Iteration 153/1000 | Loss: 0.00002086
Iteration 154/1000 | Loss: 0.00002086
Iteration 155/1000 | Loss: 0.00002086
Iteration 156/1000 | Loss: 0.00002085
Iteration 157/1000 | Loss: 0.00002084
Iteration 158/1000 | Loss: 0.00002084
Iteration 159/1000 | Loss: 0.00002084
Iteration 160/1000 | Loss: 0.00002083
Iteration 161/1000 | Loss: 0.00002083
Iteration 162/1000 | Loss: 0.00002083
Iteration 163/1000 | Loss: 0.00002083
Iteration 164/1000 | Loss: 0.00002083
Iteration 165/1000 | Loss: 0.00002083
Iteration 166/1000 | Loss: 0.00002082
Iteration 167/1000 | Loss: 0.00002082
Iteration 168/1000 | Loss: 0.00002082
Iteration 169/1000 | Loss: 0.00002082
Iteration 170/1000 | Loss: 0.00002081
Iteration 171/1000 | Loss: 0.00002081
Iteration 172/1000 | Loss: 0.00002081
Iteration 173/1000 | Loss: 0.00002081
Iteration 174/1000 | Loss: 0.00002081
Iteration 175/1000 | Loss: 0.00002081
Iteration 176/1000 | Loss: 0.00002081
Iteration 177/1000 | Loss: 0.00002081
Iteration 178/1000 | Loss: 0.00002081
Iteration 179/1000 | Loss: 0.00002081
Iteration 180/1000 | Loss: 0.00002081
Iteration 181/1000 | Loss: 0.00002081
Iteration 182/1000 | Loss: 0.00002081
Iteration 183/1000 | Loss: 0.00002080
Iteration 184/1000 | Loss: 0.00002080
Iteration 185/1000 | Loss: 0.00002080
Iteration 186/1000 | Loss: 0.00002080
Iteration 187/1000 | Loss: 0.00002080
Iteration 188/1000 | Loss: 0.00002080
Iteration 189/1000 | Loss: 0.00002079
Iteration 190/1000 | Loss: 0.00002079
Iteration 191/1000 | Loss: 0.00002079
Iteration 192/1000 | Loss: 0.00002079
Iteration 193/1000 | Loss: 0.00002079
Iteration 194/1000 | Loss: 0.00002079
Iteration 195/1000 | Loss: 0.00002079
Iteration 196/1000 | Loss: 0.00002079
Iteration 197/1000 | Loss: 0.00002079
Iteration 198/1000 | Loss: 0.00002078
Iteration 199/1000 | Loss: 0.00002078
Iteration 200/1000 | Loss: 0.00002078
Iteration 201/1000 | Loss: 0.00002078
Iteration 202/1000 | Loss: 0.00002078
Iteration 203/1000 | Loss: 0.00002078
Iteration 204/1000 | Loss: 0.00002078
Iteration 205/1000 | Loss: 0.00002078
Iteration 206/1000 | Loss: 0.00002078
Iteration 207/1000 | Loss: 0.00002077
Iteration 208/1000 | Loss: 0.00002077
Iteration 209/1000 | Loss: 0.00002077
Iteration 210/1000 | Loss: 0.00002077
Iteration 211/1000 | Loss: 0.00002076
Iteration 212/1000 | Loss: 0.00002076
Iteration 213/1000 | Loss: 0.00002076
Iteration 214/1000 | Loss: 0.00002076
Iteration 215/1000 | Loss: 0.00002075
Iteration 216/1000 | Loss: 0.00002075
Iteration 217/1000 | Loss: 0.00002074
Iteration 218/1000 | Loss: 0.00002074
Iteration 219/1000 | Loss: 0.00002074
Iteration 220/1000 | Loss: 0.00002074
Iteration 221/1000 | Loss: 0.00002074
Iteration 222/1000 | Loss: 0.00002074
Iteration 223/1000 | Loss: 0.00002073
Iteration 224/1000 | Loss: 0.00002073
Iteration 225/1000 | Loss: 0.00002073
Iteration 226/1000 | Loss: 0.00002072
Iteration 227/1000 | Loss: 0.00002071
Iteration 228/1000 | Loss: 0.00002071
Iteration 229/1000 | Loss: 0.00002071
Iteration 230/1000 | Loss: 0.00002071
Iteration 231/1000 | Loss: 0.00002071
Iteration 232/1000 | Loss: 0.00002070
Iteration 233/1000 | Loss: 0.00002070
Iteration 234/1000 | Loss: 0.00002070
Iteration 235/1000 | Loss: 0.00002070
Iteration 236/1000 | Loss: 0.00002070
Iteration 237/1000 | Loss: 0.00002070
Iteration 238/1000 | Loss: 0.00002070
Iteration 239/1000 | Loss: 0.00002070
Iteration 240/1000 | Loss: 0.00002070
Iteration 241/1000 | Loss: 0.00002069
Iteration 242/1000 | Loss: 0.00002069
Iteration 243/1000 | Loss: 0.00002068
Iteration 244/1000 | Loss: 0.00002068
Iteration 245/1000 | Loss: 0.00002068
Iteration 246/1000 | Loss: 0.00002068
Iteration 247/1000 | Loss: 0.00002068
Iteration 248/1000 | Loss: 0.00002068
Iteration 249/1000 | Loss: 0.00002068
Iteration 250/1000 | Loss: 0.00002068
Iteration 251/1000 | Loss: 0.00002067
Iteration 252/1000 | Loss: 0.00002067
Iteration 253/1000 | Loss: 0.00002067
Iteration 254/1000 | Loss: 0.00002066
Iteration 255/1000 | Loss: 0.00002066
Iteration 256/1000 | Loss: 0.00002066
Iteration 257/1000 | Loss: 0.00002066
Iteration 258/1000 | Loss: 0.00002066
Iteration 259/1000 | Loss: 0.00002066
Iteration 260/1000 | Loss: 0.00002066
Iteration 261/1000 | Loss: 0.00002065
Iteration 262/1000 | Loss: 0.00002065
Iteration 263/1000 | Loss: 0.00002065
Iteration 264/1000 | Loss: 0.00002065
Iteration 265/1000 | Loss: 0.00002065
Iteration 266/1000 | Loss: 0.00002065
Iteration 267/1000 | Loss: 0.00002065
Iteration 268/1000 | Loss: 0.00002065
Iteration 269/1000 | Loss: 0.00002065
Iteration 270/1000 | Loss: 0.00002065
Iteration 271/1000 | Loss: 0.00002064
Iteration 272/1000 | Loss: 0.00002064
Iteration 273/1000 | Loss: 0.00002064
Iteration 274/1000 | Loss: 0.00002064
Iteration 275/1000 | Loss: 0.00002064
Iteration 276/1000 | Loss: 0.00002064
Iteration 277/1000 | Loss: 0.00002064
Iteration 278/1000 | Loss: 0.00002064
Iteration 279/1000 | Loss: 0.00002064
Iteration 280/1000 | Loss: 0.00002064
Iteration 281/1000 | Loss: 0.00002064
Iteration 282/1000 | Loss: 0.00002064
Iteration 283/1000 | Loss: 0.00002063
Iteration 284/1000 | Loss: 0.00002063
Iteration 285/1000 | Loss: 0.00002063
Iteration 286/1000 | Loss: 0.00002063
Iteration 287/1000 | Loss: 0.00002063
Iteration 288/1000 | Loss: 0.00002063
Iteration 289/1000 | Loss: 0.00002063
Iteration 290/1000 | Loss: 0.00002063
Iteration 291/1000 | Loss: 0.00002063
Iteration 292/1000 | Loss: 0.00002063
Iteration 293/1000 | Loss: 0.00002063
Iteration 294/1000 | Loss: 0.00002063
Iteration 295/1000 | Loss: 0.00002063
Iteration 296/1000 | Loss: 0.00002063
Iteration 297/1000 | Loss: 0.00002063
Iteration 298/1000 | Loss: 0.00002063
Iteration 299/1000 | Loss: 0.00002063
Iteration 300/1000 | Loss: 0.00002063
Iteration 301/1000 | Loss: 0.00002063
Iteration 302/1000 | Loss: 0.00002063
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 302. Stopping optimization.
Last 5 losses: [2.063391002593562e-05, 2.063391002593562e-05, 2.063391002593562e-05, 2.063391002593562e-05, 2.063391002593562e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.063391002593562e-05

Optimization complete. Final v2v error: 3.723428964614868 mm

Highest mean error: 6.365044116973877 mm for frame 85

Lowest mean error: 3.3885271549224854 mm for frame 31

Saving results

Total time: 216.2187840938568
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00405598
Iteration 2/25 | Loss: 0.00133468
Iteration 3/25 | Loss: 0.00113044
Iteration 4/25 | Loss: 0.00110129
Iteration 5/25 | Loss: 0.00109753
Iteration 6/25 | Loss: 0.00109646
Iteration 7/25 | Loss: 0.00109646
Iteration 8/25 | Loss: 0.00109646
Iteration 9/25 | Loss: 0.00109646
Iteration 10/25 | Loss: 0.00109646
Iteration 11/25 | Loss: 0.00109646
Iteration 12/25 | Loss: 0.00109646
Iteration 13/25 | Loss: 0.00109646
Iteration 14/25 | Loss: 0.00109646
Iteration 15/25 | Loss: 0.00109646
Iteration 16/25 | Loss: 0.00109646
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010964588727802038, 0.0010964588727802038, 0.0010964588727802038, 0.0010964588727802038, 0.0010964588727802038]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010964588727802038

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35254014
Iteration 2/25 | Loss: 0.00085022
Iteration 3/25 | Loss: 0.00085021
Iteration 4/25 | Loss: 0.00085021
Iteration 5/25 | Loss: 0.00085021
Iteration 6/25 | Loss: 0.00085021
Iteration 7/25 | Loss: 0.00085021
Iteration 8/25 | Loss: 0.00085021
Iteration 9/25 | Loss: 0.00085021
Iteration 10/25 | Loss: 0.00085021
Iteration 11/25 | Loss: 0.00085021
Iteration 12/25 | Loss: 0.00085021
Iteration 13/25 | Loss: 0.00085021
Iteration 14/25 | Loss: 0.00085021
Iteration 15/25 | Loss: 0.00085021
Iteration 16/25 | Loss: 0.00085021
Iteration 17/25 | Loss: 0.00085021
Iteration 18/25 | Loss: 0.00085021
Iteration 19/25 | Loss: 0.00085021
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.000850209966301918, 0.000850209966301918, 0.000850209966301918, 0.000850209966301918, 0.000850209966301918]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000850209966301918

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085021
Iteration 2/1000 | Loss: 0.00002965
Iteration 3/1000 | Loss: 0.00001707
Iteration 4/1000 | Loss: 0.00001416
Iteration 5/1000 | Loss: 0.00001302
Iteration 6/1000 | Loss: 0.00001226
Iteration 7/1000 | Loss: 0.00001165
Iteration 8/1000 | Loss: 0.00001126
Iteration 9/1000 | Loss: 0.00001103
Iteration 10/1000 | Loss: 0.00001076
Iteration 11/1000 | Loss: 0.00001068
Iteration 12/1000 | Loss: 0.00001050
Iteration 13/1000 | Loss: 0.00001048
Iteration 14/1000 | Loss: 0.00001047
Iteration 15/1000 | Loss: 0.00001047
Iteration 16/1000 | Loss: 0.00001046
Iteration 17/1000 | Loss: 0.00001046
Iteration 18/1000 | Loss: 0.00001045
Iteration 19/1000 | Loss: 0.00001044
Iteration 20/1000 | Loss: 0.00001042
Iteration 21/1000 | Loss: 0.00001041
Iteration 22/1000 | Loss: 0.00001038
Iteration 23/1000 | Loss: 0.00001036
Iteration 24/1000 | Loss: 0.00001036
Iteration 25/1000 | Loss: 0.00001035
Iteration 26/1000 | Loss: 0.00001034
Iteration 27/1000 | Loss: 0.00001033
Iteration 28/1000 | Loss: 0.00001033
Iteration 29/1000 | Loss: 0.00001033
Iteration 30/1000 | Loss: 0.00001032
Iteration 31/1000 | Loss: 0.00001032
Iteration 32/1000 | Loss: 0.00001031
Iteration 33/1000 | Loss: 0.00001030
Iteration 34/1000 | Loss: 0.00001030
Iteration 35/1000 | Loss: 0.00001029
Iteration 36/1000 | Loss: 0.00001029
Iteration 37/1000 | Loss: 0.00001028
Iteration 38/1000 | Loss: 0.00001028
Iteration 39/1000 | Loss: 0.00001027
Iteration 40/1000 | Loss: 0.00001026
Iteration 41/1000 | Loss: 0.00001026
Iteration 42/1000 | Loss: 0.00001025
Iteration 43/1000 | Loss: 0.00001024
Iteration 44/1000 | Loss: 0.00001024
Iteration 45/1000 | Loss: 0.00001024
Iteration 46/1000 | Loss: 0.00001023
Iteration 47/1000 | Loss: 0.00001023
Iteration 48/1000 | Loss: 0.00001022
Iteration 49/1000 | Loss: 0.00001021
Iteration 50/1000 | Loss: 0.00001021
Iteration 51/1000 | Loss: 0.00001020
Iteration 52/1000 | Loss: 0.00001019
Iteration 53/1000 | Loss: 0.00001019
Iteration 54/1000 | Loss: 0.00001018
Iteration 55/1000 | Loss: 0.00001018
Iteration 56/1000 | Loss: 0.00001018
Iteration 57/1000 | Loss: 0.00001018
Iteration 58/1000 | Loss: 0.00001018
Iteration 59/1000 | Loss: 0.00001018
Iteration 60/1000 | Loss: 0.00001017
Iteration 61/1000 | Loss: 0.00001017
Iteration 62/1000 | Loss: 0.00001017
Iteration 63/1000 | Loss: 0.00001017
Iteration 64/1000 | Loss: 0.00001017
Iteration 65/1000 | Loss: 0.00001016
Iteration 66/1000 | Loss: 0.00001016
Iteration 67/1000 | Loss: 0.00001015
Iteration 68/1000 | Loss: 0.00001015
Iteration 69/1000 | Loss: 0.00001015
Iteration 70/1000 | Loss: 0.00001015
Iteration 71/1000 | Loss: 0.00001014
Iteration 72/1000 | Loss: 0.00001014
Iteration 73/1000 | Loss: 0.00001014
Iteration 74/1000 | Loss: 0.00001014
Iteration 75/1000 | Loss: 0.00001013
Iteration 76/1000 | Loss: 0.00001013
Iteration 77/1000 | Loss: 0.00001012
Iteration 78/1000 | Loss: 0.00001012
Iteration 79/1000 | Loss: 0.00001012
Iteration 80/1000 | Loss: 0.00001012
Iteration 81/1000 | Loss: 0.00001012
Iteration 82/1000 | Loss: 0.00001012
Iteration 83/1000 | Loss: 0.00001012
Iteration 84/1000 | Loss: 0.00001011
Iteration 85/1000 | Loss: 0.00001011
Iteration 86/1000 | Loss: 0.00001011
Iteration 87/1000 | Loss: 0.00001011
Iteration 88/1000 | Loss: 0.00001011
Iteration 89/1000 | Loss: 0.00001011
Iteration 90/1000 | Loss: 0.00001011
Iteration 91/1000 | Loss: 0.00001011
Iteration 92/1000 | Loss: 0.00001011
Iteration 93/1000 | Loss: 0.00001011
Iteration 94/1000 | Loss: 0.00001011
Iteration 95/1000 | Loss: 0.00001010
Iteration 96/1000 | Loss: 0.00001010
Iteration 97/1000 | Loss: 0.00001010
Iteration 98/1000 | Loss: 0.00001010
Iteration 99/1000 | Loss: 0.00001010
Iteration 100/1000 | Loss: 0.00001010
Iteration 101/1000 | Loss: 0.00001010
Iteration 102/1000 | Loss: 0.00001010
Iteration 103/1000 | Loss: 0.00001010
Iteration 104/1000 | Loss: 0.00001009
Iteration 105/1000 | Loss: 0.00001009
Iteration 106/1000 | Loss: 0.00001009
Iteration 107/1000 | Loss: 0.00001009
Iteration 108/1000 | Loss: 0.00001008
Iteration 109/1000 | Loss: 0.00001008
Iteration 110/1000 | Loss: 0.00001008
Iteration 111/1000 | Loss: 0.00001007
Iteration 112/1000 | Loss: 0.00001007
Iteration 113/1000 | Loss: 0.00001006
Iteration 114/1000 | Loss: 0.00001006
Iteration 115/1000 | Loss: 0.00001006
Iteration 116/1000 | Loss: 0.00001006
Iteration 117/1000 | Loss: 0.00001005
Iteration 118/1000 | Loss: 0.00001005
Iteration 119/1000 | Loss: 0.00001005
Iteration 120/1000 | Loss: 0.00001005
Iteration 121/1000 | Loss: 0.00001004
Iteration 122/1000 | Loss: 0.00001004
Iteration 123/1000 | Loss: 0.00001004
Iteration 124/1000 | Loss: 0.00001004
Iteration 125/1000 | Loss: 0.00001004
Iteration 126/1000 | Loss: 0.00001003
Iteration 127/1000 | Loss: 0.00001003
Iteration 128/1000 | Loss: 0.00001003
Iteration 129/1000 | Loss: 0.00001003
Iteration 130/1000 | Loss: 0.00001003
Iteration 131/1000 | Loss: 0.00001003
Iteration 132/1000 | Loss: 0.00001003
Iteration 133/1000 | Loss: 0.00001003
Iteration 134/1000 | Loss: 0.00001003
Iteration 135/1000 | Loss: 0.00001003
Iteration 136/1000 | Loss: 0.00001003
Iteration 137/1000 | Loss: 0.00001003
Iteration 138/1000 | Loss: 0.00001002
Iteration 139/1000 | Loss: 0.00001002
Iteration 140/1000 | Loss: 0.00001002
Iteration 141/1000 | Loss: 0.00001002
Iteration 142/1000 | Loss: 0.00001002
Iteration 143/1000 | Loss: 0.00001002
Iteration 144/1000 | Loss: 0.00001002
Iteration 145/1000 | Loss: 0.00001002
Iteration 146/1000 | Loss: 0.00001002
Iteration 147/1000 | Loss: 0.00001001
Iteration 148/1000 | Loss: 0.00001001
Iteration 149/1000 | Loss: 0.00001001
Iteration 150/1000 | Loss: 0.00001001
Iteration 151/1000 | Loss: 0.00001000
Iteration 152/1000 | Loss: 0.00001000
Iteration 153/1000 | Loss: 0.00001000
Iteration 154/1000 | Loss: 0.00001000
Iteration 155/1000 | Loss: 0.00001000
Iteration 156/1000 | Loss: 0.00001000
Iteration 157/1000 | Loss: 0.00001000
Iteration 158/1000 | Loss: 0.00001000
Iteration 159/1000 | Loss: 0.00001000
Iteration 160/1000 | Loss: 0.00001000
Iteration 161/1000 | Loss: 0.00000999
Iteration 162/1000 | Loss: 0.00000999
Iteration 163/1000 | Loss: 0.00000999
Iteration 164/1000 | Loss: 0.00000999
Iteration 165/1000 | Loss: 0.00000999
Iteration 166/1000 | Loss: 0.00000998
Iteration 167/1000 | Loss: 0.00000998
Iteration 168/1000 | Loss: 0.00000998
Iteration 169/1000 | Loss: 0.00000998
Iteration 170/1000 | Loss: 0.00000998
Iteration 171/1000 | Loss: 0.00000998
Iteration 172/1000 | Loss: 0.00000998
Iteration 173/1000 | Loss: 0.00000998
Iteration 174/1000 | Loss: 0.00000998
Iteration 175/1000 | Loss: 0.00000998
Iteration 176/1000 | Loss: 0.00000998
Iteration 177/1000 | Loss: 0.00000998
Iteration 178/1000 | Loss: 0.00000997
Iteration 179/1000 | Loss: 0.00000997
Iteration 180/1000 | Loss: 0.00000997
Iteration 181/1000 | Loss: 0.00000997
Iteration 182/1000 | Loss: 0.00000997
Iteration 183/1000 | Loss: 0.00000997
Iteration 184/1000 | Loss: 0.00000997
Iteration 185/1000 | Loss: 0.00000997
Iteration 186/1000 | Loss: 0.00000997
Iteration 187/1000 | Loss: 0.00000997
Iteration 188/1000 | Loss: 0.00000997
Iteration 189/1000 | Loss: 0.00000997
Iteration 190/1000 | Loss: 0.00000997
Iteration 191/1000 | Loss: 0.00000997
Iteration 192/1000 | Loss: 0.00000997
Iteration 193/1000 | Loss: 0.00000997
Iteration 194/1000 | Loss: 0.00000997
Iteration 195/1000 | Loss: 0.00000997
Iteration 196/1000 | Loss: 0.00000997
Iteration 197/1000 | Loss: 0.00000997
Iteration 198/1000 | Loss: 0.00000997
Iteration 199/1000 | Loss: 0.00000997
Iteration 200/1000 | Loss: 0.00000997
Iteration 201/1000 | Loss: 0.00000997
Iteration 202/1000 | Loss: 0.00000997
Iteration 203/1000 | Loss: 0.00000997
Iteration 204/1000 | Loss: 0.00000997
Iteration 205/1000 | Loss: 0.00000997
Iteration 206/1000 | Loss: 0.00000997
Iteration 207/1000 | Loss: 0.00000997
Iteration 208/1000 | Loss: 0.00000997
Iteration 209/1000 | Loss: 0.00000997
Iteration 210/1000 | Loss: 0.00000997
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [9.972032785299234e-06, 9.972032785299234e-06, 9.972032785299234e-06, 9.972032785299234e-06, 9.972032785299234e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.972032785299234e-06

Optimization complete. Final v2v error: 2.71551251411438 mm

Highest mean error: 3.325044631958008 mm for frame 97

Lowest mean error: 2.4880940914154053 mm for frame 178

Saving results

Total time: 40.5660445690155
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00895343
Iteration 2/25 | Loss: 0.00295879
Iteration 3/25 | Loss: 0.00202381
Iteration 4/25 | Loss: 0.00186276
Iteration 5/25 | Loss: 0.00163775
Iteration 6/25 | Loss: 0.00154923
Iteration 7/25 | Loss: 0.00144113
Iteration 8/25 | Loss: 0.00144924
Iteration 9/25 | Loss: 0.00141702
Iteration 10/25 | Loss: 0.00139301
Iteration 11/25 | Loss: 0.00137362
Iteration 12/25 | Loss: 0.00135916
Iteration 13/25 | Loss: 0.00133305
Iteration 14/25 | Loss: 0.00135475
Iteration 15/25 | Loss: 0.00134275
Iteration 16/25 | Loss: 0.00132595
Iteration 17/25 | Loss: 0.00131808
Iteration 18/25 | Loss: 0.00131669
Iteration 19/25 | Loss: 0.00131640
Iteration 20/25 | Loss: 0.00131755
Iteration 21/25 | Loss: 0.00131580
Iteration 22/25 | Loss: 0.00131726
Iteration 23/25 | Loss: 0.00131634
Iteration 24/25 | Loss: 0.00131393
Iteration 25/25 | Loss: 0.00131277

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.53240633
Iteration 2/25 | Loss: 0.00518067
Iteration 3/25 | Loss: 0.00266995
Iteration 4/25 | Loss: 0.00266994
Iteration 5/25 | Loss: 0.00266994
Iteration 6/25 | Loss: 0.00266994
Iteration 7/25 | Loss: 0.00266994
Iteration 8/25 | Loss: 0.00266994
Iteration 9/25 | Loss: 0.00266994
Iteration 10/25 | Loss: 0.00266994
Iteration 11/25 | Loss: 0.00266994
Iteration 12/25 | Loss: 0.00266994
Iteration 13/25 | Loss: 0.00266994
Iteration 14/25 | Loss: 0.00266994
Iteration 15/25 | Loss: 0.00266994
Iteration 16/25 | Loss: 0.00266994
Iteration 17/25 | Loss: 0.00266994
Iteration 18/25 | Loss: 0.00266994
Iteration 19/25 | Loss: 0.00266994
Iteration 20/25 | Loss: 0.00266994
Iteration 21/25 | Loss: 0.00266994
Iteration 22/25 | Loss: 0.00266994
Iteration 23/25 | Loss: 0.00266994
Iteration 24/25 | Loss: 0.00266994
Iteration 25/25 | Loss: 0.00266994

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00266994
Iteration 2/1000 | Loss: 0.00141340
Iteration 3/1000 | Loss: 0.00106537
Iteration 4/1000 | Loss: 0.00130712
Iteration 5/1000 | Loss: 0.00271721
Iteration 6/1000 | Loss: 0.00107765
Iteration 7/1000 | Loss: 0.00187516
Iteration 8/1000 | Loss: 0.00104428
Iteration 9/1000 | Loss: 0.00111484
Iteration 10/1000 | Loss: 0.00072675
Iteration 11/1000 | Loss: 0.00037262
Iteration 12/1000 | Loss: 0.00017334
Iteration 13/1000 | Loss: 0.00088389
Iteration 14/1000 | Loss: 0.00025969
Iteration 15/1000 | Loss: 0.00021313
Iteration 16/1000 | Loss: 0.00042050
Iteration 17/1000 | Loss: 0.00013319
Iteration 18/1000 | Loss: 0.00037731
Iteration 19/1000 | Loss: 0.00013417
Iteration 20/1000 | Loss: 0.00017142
Iteration 21/1000 | Loss: 0.00007122
Iteration 22/1000 | Loss: 0.00067827
Iteration 23/1000 | Loss: 0.00022482
Iteration 24/1000 | Loss: 0.00057583
Iteration 25/1000 | Loss: 0.00050018
Iteration 26/1000 | Loss: 0.00039479
Iteration 27/1000 | Loss: 0.00052172
Iteration 28/1000 | Loss: 0.00113892
Iteration 29/1000 | Loss: 0.00195791
Iteration 30/1000 | Loss: 0.00059794
Iteration 31/1000 | Loss: 0.00134472
Iteration 32/1000 | Loss: 0.00161412
Iteration 33/1000 | Loss: 0.00078464
Iteration 34/1000 | Loss: 0.00027460
Iteration 35/1000 | Loss: 0.00030264
Iteration 36/1000 | Loss: 0.00036050
Iteration 37/1000 | Loss: 0.00282970
Iteration 38/1000 | Loss: 0.00053513
Iteration 39/1000 | Loss: 0.00232982
Iteration 40/1000 | Loss: 0.00223383
Iteration 41/1000 | Loss: 0.00189299
Iteration 42/1000 | Loss: 0.00143191
Iteration 43/1000 | Loss: 0.00187047
Iteration 44/1000 | Loss: 0.00272872
Iteration 45/1000 | Loss: 0.00097036
Iteration 46/1000 | Loss: 0.00182483
Iteration 47/1000 | Loss: 0.00113724
Iteration 48/1000 | Loss: 0.00101067
Iteration 49/1000 | Loss: 0.00080341
Iteration 50/1000 | Loss: 0.00089189
Iteration 51/1000 | Loss: 0.00067457
Iteration 52/1000 | Loss: 0.00026959
Iteration 53/1000 | Loss: 0.00028443
Iteration 54/1000 | Loss: 0.00013612
Iteration 55/1000 | Loss: 0.00028960
Iteration 56/1000 | Loss: 0.00038552
Iteration 57/1000 | Loss: 0.00034626
Iteration 58/1000 | Loss: 0.00027333
Iteration 59/1000 | Loss: 0.00058487
Iteration 60/1000 | Loss: 0.00010539
Iteration 61/1000 | Loss: 0.00047537
Iteration 62/1000 | Loss: 0.00027729
Iteration 63/1000 | Loss: 0.00043119
Iteration 64/1000 | Loss: 0.00045433
Iteration 65/1000 | Loss: 0.00093005
Iteration 66/1000 | Loss: 0.00070110
Iteration 67/1000 | Loss: 0.00183831
Iteration 68/1000 | Loss: 0.00032291
Iteration 69/1000 | Loss: 0.00005296
Iteration 70/1000 | Loss: 0.00029718
Iteration 71/1000 | Loss: 0.00004168
Iteration 72/1000 | Loss: 0.00003629
Iteration 73/1000 | Loss: 0.00104799
Iteration 74/1000 | Loss: 0.00004166
Iteration 75/1000 | Loss: 0.00003082
Iteration 76/1000 | Loss: 0.00002838
Iteration 77/1000 | Loss: 0.00002617
Iteration 78/1000 | Loss: 0.00002440
Iteration 79/1000 | Loss: 0.00025798
Iteration 80/1000 | Loss: 0.00002956
Iteration 81/1000 | Loss: 0.00002501
Iteration 82/1000 | Loss: 0.00002159
Iteration 83/1000 | Loss: 0.00001979
Iteration 84/1000 | Loss: 0.00033124
Iteration 85/1000 | Loss: 0.00006135
Iteration 86/1000 | Loss: 0.00028684
Iteration 87/1000 | Loss: 0.00002804
Iteration 88/1000 | Loss: 0.00002217
Iteration 89/1000 | Loss: 0.00001951
Iteration 90/1000 | Loss: 0.00001868
Iteration 91/1000 | Loss: 0.00001800
Iteration 92/1000 | Loss: 0.00025344
Iteration 93/1000 | Loss: 0.00001760
Iteration 94/1000 | Loss: 0.00014372
Iteration 95/1000 | Loss: 0.00001613
Iteration 96/1000 | Loss: 0.00018507
Iteration 97/1000 | Loss: 0.00001586
Iteration 98/1000 | Loss: 0.00001502
Iteration 99/1000 | Loss: 0.00001476
Iteration 100/1000 | Loss: 0.00001463
Iteration 101/1000 | Loss: 0.00001463
Iteration 102/1000 | Loss: 0.00001460
Iteration 103/1000 | Loss: 0.00001454
Iteration 104/1000 | Loss: 0.00001451
Iteration 105/1000 | Loss: 0.00001446
Iteration 106/1000 | Loss: 0.00001438
Iteration 107/1000 | Loss: 0.00001434
Iteration 108/1000 | Loss: 0.00001433
Iteration 109/1000 | Loss: 0.00001433
Iteration 110/1000 | Loss: 0.00001432
Iteration 111/1000 | Loss: 0.00001431
Iteration 112/1000 | Loss: 0.00001431
Iteration 113/1000 | Loss: 0.00001431
Iteration 114/1000 | Loss: 0.00001430
Iteration 115/1000 | Loss: 0.00001429
Iteration 116/1000 | Loss: 0.00001428
Iteration 117/1000 | Loss: 0.00001427
Iteration 118/1000 | Loss: 0.00001426
Iteration 119/1000 | Loss: 0.00001426
Iteration 120/1000 | Loss: 0.00001425
Iteration 121/1000 | Loss: 0.00001422
Iteration 122/1000 | Loss: 0.00001422
Iteration 123/1000 | Loss: 0.00001422
Iteration 124/1000 | Loss: 0.00001420
Iteration 125/1000 | Loss: 0.00001420
Iteration 126/1000 | Loss: 0.00001419
Iteration 127/1000 | Loss: 0.00001419
Iteration 128/1000 | Loss: 0.00001419
Iteration 129/1000 | Loss: 0.00001418
Iteration 130/1000 | Loss: 0.00001418
Iteration 131/1000 | Loss: 0.00001417
Iteration 132/1000 | Loss: 0.00001417
Iteration 133/1000 | Loss: 0.00001416
Iteration 134/1000 | Loss: 0.00001416
Iteration 135/1000 | Loss: 0.00001416
Iteration 136/1000 | Loss: 0.00001416
Iteration 137/1000 | Loss: 0.00001415
Iteration 138/1000 | Loss: 0.00001415
Iteration 139/1000 | Loss: 0.00001415
Iteration 140/1000 | Loss: 0.00001415
Iteration 141/1000 | Loss: 0.00001414
Iteration 142/1000 | Loss: 0.00001414
Iteration 143/1000 | Loss: 0.00001414
Iteration 144/1000 | Loss: 0.00001414
Iteration 145/1000 | Loss: 0.00001413
Iteration 146/1000 | Loss: 0.00001413
Iteration 147/1000 | Loss: 0.00001411
Iteration 148/1000 | Loss: 0.00001409
Iteration 149/1000 | Loss: 0.00001409
Iteration 150/1000 | Loss: 0.00001408
Iteration 151/1000 | Loss: 0.00001408
Iteration 152/1000 | Loss: 0.00001407
Iteration 153/1000 | Loss: 0.00001407
Iteration 154/1000 | Loss: 0.00001406
Iteration 155/1000 | Loss: 0.00001406
Iteration 156/1000 | Loss: 0.00001406
Iteration 157/1000 | Loss: 0.00001405
Iteration 158/1000 | Loss: 0.00001405
Iteration 159/1000 | Loss: 0.00001405
Iteration 160/1000 | Loss: 0.00001404
Iteration 161/1000 | Loss: 0.00001404
Iteration 162/1000 | Loss: 0.00001404
Iteration 163/1000 | Loss: 0.00001404
Iteration 164/1000 | Loss: 0.00001404
Iteration 165/1000 | Loss: 0.00001403
Iteration 166/1000 | Loss: 0.00001403
Iteration 167/1000 | Loss: 0.00001402
Iteration 168/1000 | Loss: 0.00001401
Iteration 169/1000 | Loss: 0.00001401
Iteration 170/1000 | Loss: 0.00001401
Iteration 171/1000 | Loss: 0.00001400
Iteration 172/1000 | Loss: 0.00001400
Iteration 173/1000 | Loss: 0.00001400
Iteration 174/1000 | Loss: 0.00001399
Iteration 175/1000 | Loss: 0.00001399
Iteration 176/1000 | Loss: 0.00001399
Iteration 177/1000 | Loss: 0.00001399
Iteration 178/1000 | Loss: 0.00001399
Iteration 179/1000 | Loss: 0.00001399
Iteration 180/1000 | Loss: 0.00001399
Iteration 181/1000 | Loss: 0.00001399
Iteration 182/1000 | Loss: 0.00001399
Iteration 183/1000 | Loss: 0.00001398
Iteration 184/1000 | Loss: 0.00001398
Iteration 185/1000 | Loss: 0.00001398
Iteration 186/1000 | Loss: 0.00001398
Iteration 187/1000 | Loss: 0.00001398
Iteration 188/1000 | Loss: 0.00001397
Iteration 189/1000 | Loss: 0.00001397
Iteration 190/1000 | Loss: 0.00001397
Iteration 191/1000 | Loss: 0.00001397
Iteration 192/1000 | Loss: 0.00001396
Iteration 193/1000 | Loss: 0.00001396
Iteration 194/1000 | Loss: 0.00001396
Iteration 195/1000 | Loss: 0.00001396
Iteration 196/1000 | Loss: 0.00001395
Iteration 197/1000 | Loss: 0.00001395
Iteration 198/1000 | Loss: 0.00001395
Iteration 199/1000 | Loss: 0.00001395
Iteration 200/1000 | Loss: 0.00001395
Iteration 201/1000 | Loss: 0.00001395
Iteration 202/1000 | Loss: 0.00001395
Iteration 203/1000 | Loss: 0.00001395
Iteration 204/1000 | Loss: 0.00001395
Iteration 205/1000 | Loss: 0.00001395
Iteration 206/1000 | Loss: 0.00001395
Iteration 207/1000 | Loss: 0.00001395
Iteration 208/1000 | Loss: 0.00001395
Iteration 209/1000 | Loss: 0.00001394
Iteration 210/1000 | Loss: 0.00001394
Iteration 211/1000 | Loss: 0.00001394
Iteration 212/1000 | Loss: 0.00001394
Iteration 213/1000 | Loss: 0.00001394
Iteration 214/1000 | Loss: 0.00001394
Iteration 215/1000 | Loss: 0.00001393
Iteration 216/1000 | Loss: 0.00001393
Iteration 217/1000 | Loss: 0.00001393
Iteration 218/1000 | Loss: 0.00001393
Iteration 219/1000 | Loss: 0.00001393
Iteration 220/1000 | Loss: 0.00001393
Iteration 221/1000 | Loss: 0.00001393
Iteration 222/1000 | Loss: 0.00001393
Iteration 223/1000 | Loss: 0.00001392
Iteration 224/1000 | Loss: 0.00001392
Iteration 225/1000 | Loss: 0.00001392
Iteration 226/1000 | Loss: 0.00001392
Iteration 227/1000 | Loss: 0.00001392
Iteration 228/1000 | Loss: 0.00001392
Iteration 229/1000 | Loss: 0.00001392
Iteration 230/1000 | Loss: 0.00001392
Iteration 231/1000 | Loss: 0.00001392
Iteration 232/1000 | Loss: 0.00001392
Iteration 233/1000 | Loss: 0.00001392
Iteration 234/1000 | Loss: 0.00001392
Iteration 235/1000 | Loss: 0.00001392
Iteration 236/1000 | Loss: 0.00001392
Iteration 237/1000 | Loss: 0.00001392
Iteration 238/1000 | Loss: 0.00001392
Iteration 239/1000 | Loss: 0.00001392
Iteration 240/1000 | Loss: 0.00001392
Iteration 241/1000 | Loss: 0.00001392
Iteration 242/1000 | Loss: 0.00001392
Iteration 243/1000 | Loss: 0.00001392
Iteration 244/1000 | Loss: 0.00001392
Iteration 245/1000 | Loss: 0.00001392
Iteration 246/1000 | Loss: 0.00001392
Iteration 247/1000 | Loss: 0.00001392
Iteration 248/1000 | Loss: 0.00001392
Iteration 249/1000 | Loss: 0.00001392
Iteration 250/1000 | Loss: 0.00001392
Iteration 251/1000 | Loss: 0.00001392
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 251. Stopping optimization.
Last 5 losses: [1.3921180652687326e-05, 1.3921180652687326e-05, 1.3921180652687326e-05, 1.3921180652687326e-05, 1.3921180652687326e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3921180652687326e-05

Optimization complete. Final v2v error: 3.1157658100128174 mm

Highest mean error: 4.763118743896484 mm for frame 78

Lowest mean error: 2.6241557598114014 mm for frame 136

Saving results

Total time: 214.00644087791443
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00852849
Iteration 2/25 | Loss: 0.00151587
Iteration 3/25 | Loss: 0.00125420
Iteration 4/25 | Loss: 0.00118268
Iteration 5/25 | Loss: 0.00115196
Iteration 6/25 | Loss: 0.00114499
Iteration 7/25 | Loss: 0.00111691
Iteration 8/25 | Loss: 0.00110848
Iteration 9/25 | Loss: 0.00110407
Iteration 10/25 | Loss: 0.00110851
Iteration 11/25 | Loss: 0.00110591
Iteration 12/25 | Loss: 0.00111166
Iteration 13/25 | Loss: 0.00110518
Iteration 14/25 | Loss: 0.00109977
Iteration 15/25 | Loss: 0.00110184
Iteration 16/25 | Loss: 0.00109695
Iteration 17/25 | Loss: 0.00109642
Iteration 18/25 | Loss: 0.00109596
Iteration 19/25 | Loss: 0.00109725
Iteration 20/25 | Loss: 0.00109525
Iteration 21/25 | Loss: 0.00109466
Iteration 22/25 | Loss: 0.00109450
Iteration 23/25 | Loss: 0.00109450
Iteration 24/25 | Loss: 0.00109449
Iteration 25/25 | Loss: 0.00109449

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 12.32174015
Iteration 2/25 | Loss: 0.00112155
Iteration 3/25 | Loss: 0.00112140
Iteration 4/25 | Loss: 0.00112140
Iteration 5/25 | Loss: 0.00112140
Iteration 6/25 | Loss: 0.00112140
Iteration 7/25 | Loss: 0.00112140
Iteration 8/25 | Loss: 0.00112140
Iteration 9/25 | Loss: 0.00103071
Iteration 10/25 | Loss: 0.00103063
Iteration 11/25 | Loss: 0.00103063
Iteration 12/25 | Loss: 0.00103063
Iteration 13/25 | Loss: 0.00103063
Iteration 14/25 | Loss: 0.00103063
Iteration 15/25 | Loss: 0.00103063
Iteration 16/25 | Loss: 0.00103062
Iteration 17/25 | Loss: 0.00103062
Iteration 18/25 | Loss: 0.00103062
Iteration 19/25 | Loss: 0.00103062
Iteration 20/25 | Loss: 0.00103062
Iteration 21/25 | Loss: 0.00103062
Iteration 22/25 | Loss: 0.00103062
Iteration 23/25 | Loss: 0.00103062
Iteration 24/25 | Loss: 0.00103062
Iteration 25/25 | Loss: 0.00103062

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103062
Iteration 2/1000 | Loss: 0.00003616
Iteration 3/1000 | Loss: 0.00017793
Iteration 4/1000 | Loss: 0.00008122
Iteration 5/1000 | Loss: 0.00003141
Iteration 6/1000 | Loss: 0.00008917
Iteration 7/1000 | Loss: 0.00054978
Iteration 8/1000 | Loss: 0.00001704
Iteration 9/1000 | Loss: 0.00001628
Iteration 10/1000 | Loss: 0.00006394
Iteration 11/1000 | Loss: 0.00001562
Iteration 12/1000 | Loss: 0.00001521
Iteration 13/1000 | Loss: 0.00001510
Iteration 14/1000 | Loss: 0.00001506
Iteration 15/1000 | Loss: 0.00001486
Iteration 16/1000 | Loss: 0.00001468
Iteration 17/1000 | Loss: 0.00001466
Iteration 18/1000 | Loss: 0.00001462
Iteration 19/1000 | Loss: 0.00001457
Iteration 20/1000 | Loss: 0.00006569
Iteration 21/1000 | Loss: 0.00001945
Iteration 22/1000 | Loss: 0.00002295
Iteration 23/1000 | Loss: 0.00001443
Iteration 24/1000 | Loss: 0.00001440
Iteration 25/1000 | Loss: 0.00001439
Iteration 26/1000 | Loss: 0.00001437
Iteration 27/1000 | Loss: 0.00001437
Iteration 28/1000 | Loss: 0.00001437
Iteration 29/1000 | Loss: 0.00001437
Iteration 30/1000 | Loss: 0.00001437
Iteration 31/1000 | Loss: 0.00001437
Iteration 32/1000 | Loss: 0.00001437
Iteration 33/1000 | Loss: 0.00001436
Iteration 34/1000 | Loss: 0.00001436
Iteration 35/1000 | Loss: 0.00001435
Iteration 36/1000 | Loss: 0.00001435
Iteration 37/1000 | Loss: 0.00001435
Iteration 38/1000 | Loss: 0.00001435
Iteration 39/1000 | Loss: 0.00001434
Iteration 40/1000 | Loss: 0.00001434
Iteration 41/1000 | Loss: 0.00001433
Iteration 42/1000 | Loss: 0.00001431
Iteration 43/1000 | Loss: 0.00001430
Iteration 44/1000 | Loss: 0.00001430
Iteration 45/1000 | Loss: 0.00001429
Iteration 46/1000 | Loss: 0.00001429
Iteration 47/1000 | Loss: 0.00001428
Iteration 48/1000 | Loss: 0.00001428
Iteration 49/1000 | Loss: 0.00001428
Iteration 50/1000 | Loss: 0.00001427
Iteration 51/1000 | Loss: 0.00001427
Iteration 52/1000 | Loss: 0.00001426
Iteration 53/1000 | Loss: 0.00001426
Iteration 54/1000 | Loss: 0.00001426
Iteration 55/1000 | Loss: 0.00001425
Iteration 56/1000 | Loss: 0.00001425
Iteration 57/1000 | Loss: 0.00001424
Iteration 58/1000 | Loss: 0.00001424
Iteration 59/1000 | Loss: 0.00001423
Iteration 60/1000 | Loss: 0.00001422
Iteration 61/1000 | Loss: 0.00001422
Iteration 62/1000 | Loss: 0.00001422
Iteration 63/1000 | Loss: 0.00001422
Iteration 64/1000 | Loss: 0.00001422
Iteration 65/1000 | Loss: 0.00001422
Iteration 66/1000 | Loss: 0.00001422
Iteration 67/1000 | Loss: 0.00001422
Iteration 68/1000 | Loss: 0.00001421
Iteration 69/1000 | Loss: 0.00001421
Iteration 70/1000 | Loss: 0.00001421
Iteration 71/1000 | Loss: 0.00001420
Iteration 72/1000 | Loss: 0.00001420
Iteration 73/1000 | Loss: 0.00001420
Iteration 74/1000 | Loss: 0.00001420
Iteration 75/1000 | Loss: 0.00001418
Iteration 76/1000 | Loss: 0.00001418
Iteration 77/1000 | Loss: 0.00001417
Iteration 78/1000 | Loss: 0.00001417
Iteration 79/1000 | Loss: 0.00001417
Iteration 80/1000 | Loss: 0.00001417
Iteration 81/1000 | Loss: 0.00001417
Iteration 82/1000 | Loss: 0.00001416
Iteration 83/1000 | Loss: 0.00001416
Iteration 84/1000 | Loss: 0.00001416
Iteration 85/1000 | Loss: 0.00001416
Iteration 86/1000 | Loss: 0.00001415
Iteration 87/1000 | Loss: 0.00001415
Iteration 88/1000 | Loss: 0.00001415
Iteration 89/1000 | Loss: 0.00001415
Iteration 90/1000 | Loss: 0.00001415
Iteration 91/1000 | Loss: 0.00001415
Iteration 92/1000 | Loss: 0.00001415
Iteration 93/1000 | Loss: 0.00001415
Iteration 94/1000 | Loss: 0.00001415
Iteration 95/1000 | Loss: 0.00001415
Iteration 96/1000 | Loss: 0.00001415
Iteration 97/1000 | Loss: 0.00001415
Iteration 98/1000 | Loss: 0.00001414
Iteration 99/1000 | Loss: 0.00001414
Iteration 100/1000 | Loss: 0.00001414
Iteration 101/1000 | Loss: 0.00001414
Iteration 102/1000 | Loss: 0.00001414
Iteration 103/1000 | Loss: 0.00001414
Iteration 104/1000 | Loss: 0.00001414
Iteration 105/1000 | Loss: 0.00001414
Iteration 106/1000 | Loss: 0.00001414
Iteration 107/1000 | Loss: 0.00001413
Iteration 108/1000 | Loss: 0.00001413
Iteration 109/1000 | Loss: 0.00001413
Iteration 110/1000 | Loss: 0.00001413
Iteration 111/1000 | Loss: 0.00001413
Iteration 112/1000 | Loss: 0.00001413
Iteration 113/1000 | Loss: 0.00001413
Iteration 114/1000 | Loss: 0.00001413
Iteration 115/1000 | Loss: 0.00001413
Iteration 116/1000 | Loss: 0.00001413
Iteration 117/1000 | Loss: 0.00001413
Iteration 118/1000 | Loss: 0.00001413
Iteration 119/1000 | Loss: 0.00001413
Iteration 120/1000 | Loss: 0.00001412
Iteration 121/1000 | Loss: 0.00001412
Iteration 122/1000 | Loss: 0.00001412
Iteration 123/1000 | Loss: 0.00001412
Iteration 124/1000 | Loss: 0.00001412
Iteration 125/1000 | Loss: 0.00001412
Iteration 126/1000 | Loss: 0.00001412
Iteration 127/1000 | Loss: 0.00001412
Iteration 128/1000 | Loss: 0.00001411
Iteration 129/1000 | Loss: 0.00001411
Iteration 130/1000 | Loss: 0.00001411
Iteration 131/1000 | Loss: 0.00001411
Iteration 132/1000 | Loss: 0.00001411
Iteration 133/1000 | Loss: 0.00001411
Iteration 134/1000 | Loss: 0.00001410
Iteration 135/1000 | Loss: 0.00001410
Iteration 136/1000 | Loss: 0.00001410
Iteration 137/1000 | Loss: 0.00001410
Iteration 138/1000 | Loss: 0.00001410
Iteration 139/1000 | Loss: 0.00001410
Iteration 140/1000 | Loss: 0.00001410
Iteration 141/1000 | Loss: 0.00001409
Iteration 142/1000 | Loss: 0.00001409
Iteration 143/1000 | Loss: 0.00001409
Iteration 144/1000 | Loss: 0.00001409
Iteration 145/1000 | Loss: 0.00001409
Iteration 146/1000 | Loss: 0.00001409
Iteration 147/1000 | Loss: 0.00001409
Iteration 148/1000 | Loss: 0.00001409
Iteration 149/1000 | Loss: 0.00001409
Iteration 150/1000 | Loss: 0.00001409
Iteration 151/1000 | Loss: 0.00001408
Iteration 152/1000 | Loss: 0.00001408
Iteration 153/1000 | Loss: 0.00001408
Iteration 154/1000 | Loss: 0.00001408
Iteration 155/1000 | Loss: 0.00001408
Iteration 156/1000 | Loss: 0.00001408
Iteration 157/1000 | Loss: 0.00001408
Iteration 158/1000 | Loss: 0.00001407
Iteration 159/1000 | Loss: 0.00001407
Iteration 160/1000 | Loss: 0.00001407
Iteration 161/1000 | Loss: 0.00001407
Iteration 162/1000 | Loss: 0.00001407
Iteration 163/1000 | Loss: 0.00001407
Iteration 164/1000 | Loss: 0.00001407
Iteration 165/1000 | Loss: 0.00001407
Iteration 166/1000 | Loss: 0.00001407
Iteration 167/1000 | Loss: 0.00001407
Iteration 168/1000 | Loss: 0.00001407
Iteration 169/1000 | Loss: 0.00001407
Iteration 170/1000 | Loss: 0.00001407
Iteration 171/1000 | Loss: 0.00001407
Iteration 172/1000 | Loss: 0.00001407
Iteration 173/1000 | Loss: 0.00001407
Iteration 174/1000 | Loss: 0.00001406
Iteration 175/1000 | Loss: 0.00001406
Iteration 176/1000 | Loss: 0.00001406
Iteration 177/1000 | Loss: 0.00001406
Iteration 178/1000 | Loss: 0.00001406
Iteration 179/1000 | Loss: 0.00001406
Iteration 180/1000 | Loss: 0.00001406
Iteration 181/1000 | Loss: 0.00001406
Iteration 182/1000 | Loss: 0.00001406
Iteration 183/1000 | Loss: 0.00001406
Iteration 184/1000 | Loss: 0.00001406
Iteration 185/1000 | Loss: 0.00001406
Iteration 186/1000 | Loss: 0.00001406
Iteration 187/1000 | Loss: 0.00001406
Iteration 188/1000 | Loss: 0.00001406
Iteration 189/1000 | Loss: 0.00001406
Iteration 190/1000 | Loss: 0.00001406
Iteration 191/1000 | Loss: 0.00001406
Iteration 192/1000 | Loss: 0.00001405
Iteration 193/1000 | Loss: 0.00001405
Iteration 194/1000 | Loss: 0.00001405
Iteration 195/1000 | Loss: 0.00001405
Iteration 196/1000 | Loss: 0.00001405
Iteration 197/1000 | Loss: 0.00001404
Iteration 198/1000 | Loss: 0.00001404
Iteration 199/1000 | Loss: 0.00001404
Iteration 200/1000 | Loss: 0.00001404
Iteration 201/1000 | Loss: 0.00001404
Iteration 202/1000 | Loss: 0.00001404
Iteration 203/1000 | Loss: 0.00001404
Iteration 204/1000 | Loss: 0.00001403
Iteration 205/1000 | Loss: 0.00001403
Iteration 206/1000 | Loss: 0.00001403
Iteration 207/1000 | Loss: 0.00001403
Iteration 208/1000 | Loss: 0.00001403
Iteration 209/1000 | Loss: 0.00001403
Iteration 210/1000 | Loss: 0.00001403
Iteration 211/1000 | Loss: 0.00001403
Iteration 212/1000 | Loss: 0.00001403
Iteration 213/1000 | Loss: 0.00001403
Iteration 214/1000 | Loss: 0.00001403
Iteration 215/1000 | Loss: 0.00001403
Iteration 216/1000 | Loss: 0.00001402
Iteration 217/1000 | Loss: 0.00001402
Iteration 218/1000 | Loss: 0.00001402
Iteration 219/1000 | Loss: 0.00001402
Iteration 220/1000 | Loss: 0.00001402
Iteration 221/1000 | Loss: 0.00001402
Iteration 222/1000 | Loss: 0.00001402
Iteration 223/1000 | Loss: 0.00001402
Iteration 224/1000 | Loss: 0.00001402
Iteration 225/1000 | Loss: 0.00001402
Iteration 226/1000 | Loss: 0.00001402
Iteration 227/1000 | Loss: 0.00001402
Iteration 228/1000 | Loss: 0.00001402
Iteration 229/1000 | Loss: 0.00001402
Iteration 230/1000 | Loss: 0.00001402
Iteration 231/1000 | Loss: 0.00001401
Iteration 232/1000 | Loss: 0.00001401
Iteration 233/1000 | Loss: 0.00001401
Iteration 234/1000 | Loss: 0.00001401
Iteration 235/1000 | Loss: 0.00001401
Iteration 236/1000 | Loss: 0.00001401
Iteration 237/1000 | Loss: 0.00001401
Iteration 238/1000 | Loss: 0.00001401
Iteration 239/1000 | Loss: 0.00001401
Iteration 240/1000 | Loss: 0.00001401
Iteration 241/1000 | Loss: 0.00001401
Iteration 242/1000 | Loss: 0.00001400
Iteration 243/1000 | Loss: 0.00001400
Iteration 244/1000 | Loss: 0.00001400
Iteration 245/1000 | Loss: 0.00001400
Iteration 246/1000 | Loss: 0.00001400
Iteration 247/1000 | Loss: 0.00001400
Iteration 248/1000 | Loss: 0.00001400
Iteration 249/1000 | Loss: 0.00001400
Iteration 250/1000 | Loss: 0.00001400
Iteration 251/1000 | Loss: 0.00001400
Iteration 252/1000 | Loss: 0.00001400
Iteration 253/1000 | Loss: 0.00001400
Iteration 254/1000 | Loss: 0.00001400
Iteration 255/1000 | Loss: 0.00001399
Iteration 256/1000 | Loss: 0.00001399
Iteration 257/1000 | Loss: 0.00001399
Iteration 258/1000 | Loss: 0.00001399
Iteration 259/1000 | Loss: 0.00001399
Iteration 260/1000 | Loss: 0.00001399
Iteration 261/1000 | Loss: 0.00001399
Iteration 262/1000 | Loss: 0.00001399
Iteration 263/1000 | Loss: 0.00001399
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 263. Stopping optimization.
Last 5 losses: [1.3993922948429827e-05, 1.3993922948429827e-05, 1.3993922948429827e-05, 1.3993922948429827e-05, 1.3993922948429827e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3993922948429827e-05

Optimization complete. Final v2v error: 3.149254322052002 mm

Highest mean error: 3.823356866836548 mm for frame 77

Lowest mean error: 2.50571346282959 mm for frame 2

Saving results

Total time: 86.80141091346741
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01006541
Iteration 2/25 | Loss: 0.00229171
Iteration 3/25 | Loss: 0.00177522
Iteration 4/25 | Loss: 0.00159869
Iteration 5/25 | Loss: 0.00149484
Iteration 6/25 | Loss: 0.00124386
Iteration 7/25 | Loss: 0.00121134
Iteration 8/25 | Loss: 0.00120786
Iteration 9/25 | Loss: 0.00120786
Iteration 10/25 | Loss: 0.00120786
Iteration 11/25 | Loss: 0.00120786
Iteration 12/25 | Loss: 0.00120786
Iteration 13/25 | Loss: 0.00120786
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012078583240509033, 0.0012078583240509033, 0.0012078583240509033, 0.0012078583240509033, 0.0012078583240509033]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012078583240509033

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31426299
Iteration 2/25 | Loss: 0.00080113
Iteration 3/25 | Loss: 0.00080113
Iteration 4/25 | Loss: 0.00080113
Iteration 5/25 | Loss: 0.00080112
Iteration 6/25 | Loss: 0.00080112
Iteration 7/25 | Loss: 0.00080112
Iteration 8/25 | Loss: 0.00080112
Iteration 9/25 | Loss: 0.00080112
Iteration 10/25 | Loss: 0.00080112
Iteration 11/25 | Loss: 0.00080112
Iteration 12/25 | Loss: 0.00080112
Iteration 13/25 | Loss: 0.00080112
Iteration 14/25 | Loss: 0.00080112
Iteration 15/25 | Loss: 0.00080112
Iteration 16/25 | Loss: 0.00080112
Iteration 17/25 | Loss: 0.00080112
Iteration 18/25 | Loss: 0.00080112
Iteration 19/25 | Loss: 0.00080112
Iteration 20/25 | Loss: 0.00080112
Iteration 21/25 | Loss: 0.00080112
Iteration 22/25 | Loss: 0.00080112
Iteration 23/25 | Loss: 0.00080112
Iteration 24/25 | Loss: 0.00080112
Iteration 25/25 | Loss: 0.00080112

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080112
Iteration 2/1000 | Loss: 0.00003259
Iteration 3/1000 | Loss: 0.00002453
Iteration 4/1000 | Loss: 0.00002338
Iteration 5/1000 | Loss: 0.00002269
Iteration 6/1000 | Loss: 0.00002216
Iteration 7/1000 | Loss: 0.00002191
Iteration 8/1000 | Loss: 0.00002163
Iteration 9/1000 | Loss: 0.00002147
Iteration 10/1000 | Loss: 0.00002127
Iteration 11/1000 | Loss: 0.00002108
Iteration 12/1000 | Loss: 0.00002094
Iteration 13/1000 | Loss: 0.00002090
Iteration 14/1000 | Loss: 0.00002090
Iteration 15/1000 | Loss: 0.00002089
Iteration 16/1000 | Loss: 0.00002088
Iteration 17/1000 | Loss: 0.00002087
Iteration 18/1000 | Loss: 0.00002087
Iteration 19/1000 | Loss: 0.00002084
Iteration 20/1000 | Loss: 0.00002084
Iteration 21/1000 | Loss: 0.00002084
Iteration 22/1000 | Loss: 0.00002084
Iteration 23/1000 | Loss: 0.00002084
Iteration 24/1000 | Loss: 0.00002084
Iteration 25/1000 | Loss: 0.00002083
Iteration 26/1000 | Loss: 0.00002083
Iteration 27/1000 | Loss: 0.00002083
Iteration 28/1000 | Loss: 0.00002083
Iteration 29/1000 | Loss: 0.00002083
Iteration 30/1000 | Loss: 0.00002079
Iteration 31/1000 | Loss: 0.00002079
Iteration 32/1000 | Loss: 0.00002078
Iteration 33/1000 | Loss: 0.00002077
Iteration 34/1000 | Loss: 0.00002076
Iteration 35/1000 | Loss: 0.00002076
Iteration 36/1000 | Loss: 0.00002076
Iteration 37/1000 | Loss: 0.00002076
Iteration 38/1000 | Loss: 0.00002076
Iteration 39/1000 | Loss: 0.00002076
Iteration 40/1000 | Loss: 0.00002075
Iteration 41/1000 | Loss: 0.00002075
Iteration 42/1000 | Loss: 0.00002075
Iteration 43/1000 | Loss: 0.00002075
Iteration 44/1000 | Loss: 0.00002074
Iteration 45/1000 | Loss: 0.00002074
Iteration 46/1000 | Loss: 0.00002073
Iteration 47/1000 | Loss: 0.00002073
Iteration 48/1000 | Loss: 0.00002073
Iteration 49/1000 | Loss: 0.00002072
Iteration 50/1000 | Loss: 0.00002072
Iteration 51/1000 | Loss: 0.00002072
Iteration 52/1000 | Loss: 0.00002072
Iteration 53/1000 | Loss: 0.00002072
Iteration 54/1000 | Loss: 0.00002071
Iteration 55/1000 | Loss: 0.00002071
Iteration 56/1000 | Loss: 0.00002071
Iteration 57/1000 | Loss: 0.00002070
Iteration 58/1000 | Loss: 0.00002070
Iteration 59/1000 | Loss: 0.00002070
Iteration 60/1000 | Loss: 0.00002070
Iteration 61/1000 | Loss: 0.00002070
Iteration 62/1000 | Loss: 0.00002070
Iteration 63/1000 | Loss: 0.00002070
Iteration 64/1000 | Loss: 0.00002070
Iteration 65/1000 | Loss: 0.00002069
Iteration 66/1000 | Loss: 0.00002069
Iteration 67/1000 | Loss: 0.00002069
Iteration 68/1000 | Loss: 0.00002069
Iteration 69/1000 | Loss: 0.00002069
Iteration 70/1000 | Loss: 0.00002069
Iteration 71/1000 | Loss: 0.00002069
Iteration 72/1000 | Loss: 0.00002069
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 72. Stopping optimization.
Last 5 losses: [2.069388392555993e-05, 2.069388392555993e-05, 2.069388392555993e-05, 2.069388392555993e-05, 2.069388392555993e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.069388392555993e-05

Optimization complete. Final v2v error: 3.899040460586548 mm

Highest mean error: 3.9877676963806152 mm for frame 21

Lowest mean error: 3.8060126304626465 mm for frame 115

Saving results

Total time: 39.480841875076294
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00401458
Iteration 2/25 | Loss: 0.00120643
Iteration 3/25 | Loss: 0.00111037
Iteration 4/25 | Loss: 0.00109618
Iteration 5/25 | Loss: 0.00109194
Iteration 6/25 | Loss: 0.00109049
Iteration 7/25 | Loss: 0.00109045
Iteration 8/25 | Loss: 0.00109045
Iteration 9/25 | Loss: 0.00109045
Iteration 10/25 | Loss: 0.00109045
Iteration 11/25 | Loss: 0.00109045
Iteration 12/25 | Loss: 0.00109045
Iteration 13/25 | Loss: 0.00109045
Iteration 14/25 | Loss: 0.00109045
Iteration 15/25 | Loss: 0.00109045
Iteration 16/25 | Loss: 0.00109045
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001090448466129601, 0.001090448466129601, 0.001090448466129601, 0.001090448466129601, 0.001090448466129601]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001090448466129601

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34290612
Iteration 2/25 | Loss: 0.00096080
Iteration 3/25 | Loss: 0.00096078
Iteration 4/25 | Loss: 0.00096078
Iteration 5/25 | Loss: 0.00096078
Iteration 6/25 | Loss: 0.00096077
Iteration 7/25 | Loss: 0.00096077
Iteration 8/25 | Loss: 0.00096077
Iteration 9/25 | Loss: 0.00096077
Iteration 10/25 | Loss: 0.00096077
Iteration 11/25 | Loss: 0.00096077
Iteration 12/25 | Loss: 0.00096077
Iteration 13/25 | Loss: 0.00096077
Iteration 14/25 | Loss: 0.00096077
Iteration 15/25 | Loss: 0.00096077
Iteration 16/25 | Loss: 0.00096077
Iteration 17/25 | Loss: 0.00096077
Iteration 18/25 | Loss: 0.00096077
Iteration 19/25 | Loss: 0.00096077
Iteration 20/25 | Loss: 0.00096077
Iteration 21/25 | Loss: 0.00096077
Iteration 22/25 | Loss: 0.00096077
Iteration 23/25 | Loss: 0.00096077
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0009607735555619001, 0.0009607735555619001, 0.0009607735555619001, 0.0009607735555619001, 0.0009607735555619001]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009607735555619001

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096077
Iteration 2/1000 | Loss: 0.00004790
Iteration 3/1000 | Loss: 0.00002999
Iteration 4/1000 | Loss: 0.00002303
Iteration 5/1000 | Loss: 0.00001889
Iteration 6/1000 | Loss: 0.00001745
Iteration 7/1000 | Loss: 0.00001629
Iteration 8/1000 | Loss: 0.00001550
Iteration 9/1000 | Loss: 0.00001510
Iteration 10/1000 | Loss: 0.00001460
Iteration 11/1000 | Loss: 0.00001436
Iteration 12/1000 | Loss: 0.00001424
Iteration 13/1000 | Loss: 0.00001420
Iteration 14/1000 | Loss: 0.00001400
Iteration 15/1000 | Loss: 0.00001396
Iteration 16/1000 | Loss: 0.00001392
Iteration 17/1000 | Loss: 0.00001390
Iteration 18/1000 | Loss: 0.00001385
Iteration 19/1000 | Loss: 0.00001383
Iteration 20/1000 | Loss: 0.00001381
Iteration 21/1000 | Loss: 0.00001380
Iteration 22/1000 | Loss: 0.00001377
Iteration 23/1000 | Loss: 0.00001376
Iteration 24/1000 | Loss: 0.00001376
Iteration 25/1000 | Loss: 0.00001376
Iteration 26/1000 | Loss: 0.00001376
Iteration 27/1000 | Loss: 0.00001376
Iteration 28/1000 | Loss: 0.00001375
Iteration 29/1000 | Loss: 0.00001375
Iteration 30/1000 | Loss: 0.00001375
Iteration 31/1000 | Loss: 0.00001375
Iteration 32/1000 | Loss: 0.00001375
Iteration 33/1000 | Loss: 0.00001375
Iteration 34/1000 | Loss: 0.00001375
Iteration 35/1000 | Loss: 0.00001375
Iteration 36/1000 | Loss: 0.00001375
Iteration 37/1000 | Loss: 0.00001375
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 37. Stopping optimization.
Last 5 losses: [1.3750716789218131e-05, 1.3750716789218131e-05, 1.3750716789218131e-05, 1.3750716789218131e-05, 1.3750716789218131e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3750716789218131e-05

Optimization complete. Final v2v error: 2.982327461242676 mm

Highest mean error: 5.088383674621582 mm for frame 74

Lowest mean error: 2.3928117752075195 mm for frame 115

Saving results

Total time: 31.114238262176514
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00354361
Iteration 2/25 | Loss: 0.00125436
Iteration 3/25 | Loss: 0.00112613
Iteration 4/25 | Loss: 0.00109656
Iteration 5/25 | Loss: 0.00108661
Iteration 6/25 | Loss: 0.00108395
Iteration 7/25 | Loss: 0.00108354
Iteration 8/25 | Loss: 0.00108354
Iteration 9/25 | Loss: 0.00108354
Iteration 10/25 | Loss: 0.00108354
Iteration 11/25 | Loss: 0.00108354
Iteration 12/25 | Loss: 0.00108354
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010835424764081836, 0.0010835424764081836, 0.0010835424764081836, 0.0010835424764081836, 0.0010835424764081836]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010835424764081836

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33971155
Iteration 2/25 | Loss: 0.00110964
Iteration 3/25 | Loss: 0.00110964
Iteration 4/25 | Loss: 0.00110964
Iteration 5/25 | Loss: 0.00110964
Iteration 6/25 | Loss: 0.00110963
Iteration 7/25 | Loss: 0.00110963
Iteration 8/25 | Loss: 0.00110963
Iteration 9/25 | Loss: 0.00110963
Iteration 10/25 | Loss: 0.00110963
Iteration 11/25 | Loss: 0.00110963
Iteration 12/25 | Loss: 0.00110963
Iteration 13/25 | Loss: 0.00110963
Iteration 14/25 | Loss: 0.00110963
Iteration 15/25 | Loss: 0.00110963
Iteration 16/25 | Loss: 0.00110963
Iteration 17/25 | Loss: 0.00110963
Iteration 18/25 | Loss: 0.00110963
Iteration 19/25 | Loss: 0.00110963
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011096332455053926, 0.0011096332455053926, 0.0011096332455053926, 0.0011096332455053926, 0.0011096332455053926]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011096332455053926

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110963
Iteration 2/1000 | Loss: 0.00005065
Iteration 3/1000 | Loss: 0.00003328
Iteration 4/1000 | Loss: 0.00002454
Iteration 5/1000 | Loss: 0.00002261
Iteration 6/1000 | Loss: 0.00002143
Iteration 7/1000 | Loss: 0.00002059
Iteration 8/1000 | Loss: 0.00002004
Iteration 9/1000 | Loss: 0.00001962
Iteration 10/1000 | Loss: 0.00001934
Iteration 11/1000 | Loss: 0.00001909
Iteration 12/1000 | Loss: 0.00001887
Iteration 13/1000 | Loss: 0.00001884
Iteration 14/1000 | Loss: 0.00001875
Iteration 15/1000 | Loss: 0.00001872
Iteration 16/1000 | Loss: 0.00001871
Iteration 17/1000 | Loss: 0.00001869
Iteration 18/1000 | Loss: 0.00001867
Iteration 19/1000 | Loss: 0.00001867
Iteration 20/1000 | Loss: 0.00001864
Iteration 21/1000 | Loss: 0.00001860
Iteration 22/1000 | Loss: 0.00001855
Iteration 23/1000 | Loss: 0.00001854
Iteration 24/1000 | Loss: 0.00001853
Iteration 25/1000 | Loss: 0.00001853
Iteration 26/1000 | Loss: 0.00001852
Iteration 27/1000 | Loss: 0.00001852
Iteration 28/1000 | Loss: 0.00001852
Iteration 29/1000 | Loss: 0.00001850
Iteration 30/1000 | Loss: 0.00001849
Iteration 31/1000 | Loss: 0.00001847
Iteration 32/1000 | Loss: 0.00001846
Iteration 33/1000 | Loss: 0.00001846
Iteration 34/1000 | Loss: 0.00001845
Iteration 35/1000 | Loss: 0.00001844
Iteration 36/1000 | Loss: 0.00001843
Iteration 37/1000 | Loss: 0.00001842
Iteration 38/1000 | Loss: 0.00001841
Iteration 39/1000 | Loss: 0.00001836
Iteration 40/1000 | Loss: 0.00001835
Iteration 41/1000 | Loss: 0.00001835
Iteration 42/1000 | Loss: 0.00001834
Iteration 43/1000 | Loss: 0.00001833
Iteration 44/1000 | Loss: 0.00001833
Iteration 45/1000 | Loss: 0.00001832
Iteration 46/1000 | Loss: 0.00001832
Iteration 47/1000 | Loss: 0.00001831
Iteration 48/1000 | Loss: 0.00001831
Iteration 49/1000 | Loss: 0.00001830
Iteration 50/1000 | Loss: 0.00001829
Iteration 51/1000 | Loss: 0.00001829
Iteration 52/1000 | Loss: 0.00001829
Iteration 53/1000 | Loss: 0.00001828
Iteration 54/1000 | Loss: 0.00001828
Iteration 55/1000 | Loss: 0.00001828
Iteration 56/1000 | Loss: 0.00001827
Iteration 57/1000 | Loss: 0.00001827
Iteration 58/1000 | Loss: 0.00001827
Iteration 59/1000 | Loss: 0.00001827
Iteration 60/1000 | Loss: 0.00001826
Iteration 61/1000 | Loss: 0.00001826
Iteration 62/1000 | Loss: 0.00001826
Iteration 63/1000 | Loss: 0.00001826
Iteration 64/1000 | Loss: 0.00001825
Iteration 65/1000 | Loss: 0.00001824
Iteration 66/1000 | Loss: 0.00001824
Iteration 67/1000 | Loss: 0.00001823
Iteration 68/1000 | Loss: 0.00001823
Iteration 69/1000 | Loss: 0.00001822
Iteration 70/1000 | Loss: 0.00001821
Iteration 71/1000 | Loss: 0.00001821
Iteration 72/1000 | Loss: 0.00001821
Iteration 73/1000 | Loss: 0.00001820
Iteration 74/1000 | Loss: 0.00001820
Iteration 75/1000 | Loss: 0.00001820
Iteration 76/1000 | Loss: 0.00001819
Iteration 77/1000 | Loss: 0.00001819
Iteration 78/1000 | Loss: 0.00001818
Iteration 79/1000 | Loss: 0.00001818
Iteration 80/1000 | Loss: 0.00001818
Iteration 81/1000 | Loss: 0.00001817
Iteration 82/1000 | Loss: 0.00001817
Iteration 83/1000 | Loss: 0.00001817
Iteration 84/1000 | Loss: 0.00001816
Iteration 85/1000 | Loss: 0.00001816
Iteration 86/1000 | Loss: 0.00001815
Iteration 87/1000 | Loss: 0.00001815
Iteration 88/1000 | Loss: 0.00001814
Iteration 89/1000 | Loss: 0.00001814
Iteration 90/1000 | Loss: 0.00001814
Iteration 91/1000 | Loss: 0.00001809
Iteration 92/1000 | Loss: 0.00001809
Iteration 93/1000 | Loss: 0.00001808
Iteration 94/1000 | Loss: 0.00001808
Iteration 95/1000 | Loss: 0.00001808
Iteration 96/1000 | Loss: 0.00001808
Iteration 97/1000 | Loss: 0.00001808
Iteration 98/1000 | Loss: 0.00001808
Iteration 99/1000 | Loss: 0.00001808
Iteration 100/1000 | Loss: 0.00001807
Iteration 101/1000 | Loss: 0.00001807
Iteration 102/1000 | Loss: 0.00001807
Iteration 103/1000 | Loss: 0.00001807
Iteration 104/1000 | Loss: 0.00001806
Iteration 105/1000 | Loss: 0.00001806
Iteration 106/1000 | Loss: 0.00001806
Iteration 107/1000 | Loss: 0.00001805
Iteration 108/1000 | Loss: 0.00001805
Iteration 109/1000 | Loss: 0.00001804
Iteration 110/1000 | Loss: 0.00001804
Iteration 111/1000 | Loss: 0.00001804
Iteration 112/1000 | Loss: 0.00001803
Iteration 113/1000 | Loss: 0.00001803
Iteration 114/1000 | Loss: 0.00001803
Iteration 115/1000 | Loss: 0.00001803
Iteration 116/1000 | Loss: 0.00001803
Iteration 117/1000 | Loss: 0.00001802
Iteration 118/1000 | Loss: 0.00001802
Iteration 119/1000 | Loss: 0.00001802
Iteration 120/1000 | Loss: 0.00001802
Iteration 121/1000 | Loss: 0.00001802
Iteration 122/1000 | Loss: 0.00001802
Iteration 123/1000 | Loss: 0.00001802
Iteration 124/1000 | Loss: 0.00001801
Iteration 125/1000 | Loss: 0.00001801
Iteration 126/1000 | Loss: 0.00001801
Iteration 127/1000 | Loss: 0.00001801
Iteration 128/1000 | Loss: 0.00001801
Iteration 129/1000 | Loss: 0.00001801
Iteration 130/1000 | Loss: 0.00001801
Iteration 131/1000 | Loss: 0.00001800
Iteration 132/1000 | Loss: 0.00001800
Iteration 133/1000 | Loss: 0.00001800
Iteration 134/1000 | Loss: 0.00001800
Iteration 135/1000 | Loss: 0.00001799
Iteration 136/1000 | Loss: 0.00001799
Iteration 137/1000 | Loss: 0.00001799
Iteration 138/1000 | Loss: 0.00001799
Iteration 139/1000 | Loss: 0.00001799
Iteration 140/1000 | Loss: 0.00001799
Iteration 141/1000 | Loss: 0.00001799
Iteration 142/1000 | Loss: 0.00001798
Iteration 143/1000 | Loss: 0.00001798
Iteration 144/1000 | Loss: 0.00001798
Iteration 145/1000 | Loss: 0.00001798
Iteration 146/1000 | Loss: 0.00001798
Iteration 147/1000 | Loss: 0.00001798
Iteration 148/1000 | Loss: 0.00001798
Iteration 149/1000 | Loss: 0.00001798
Iteration 150/1000 | Loss: 0.00001798
Iteration 151/1000 | Loss: 0.00001798
Iteration 152/1000 | Loss: 0.00001797
Iteration 153/1000 | Loss: 0.00001797
Iteration 154/1000 | Loss: 0.00001797
Iteration 155/1000 | Loss: 0.00001797
Iteration 156/1000 | Loss: 0.00001797
Iteration 157/1000 | Loss: 0.00001797
Iteration 158/1000 | Loss: 0.00001797
Iteration 159/1000 | Loss: 0.00001797
Iteration 160/1000 | Loss: 0.00001797
Iteration 161/1000 | Loss: 0.00001797
Iteration 162/1000 | Loss: 0.00001797
Iteration 163/1000 | Loss: 0.00001797
Iteration 164/1000 | Loss: 0.00001797
Iteration 165/1000 | Loss: 0.00001797
Iteration 166/1000 | Loss: 0.00001796
Iteration 167/1000 | Loss: 0.00001796
Iteration 168/1000 | Loss: 0.00001796
Iteration 169/1000 | Loss: 0.00001796
Iteration 170/1000 | Loss: 0.00001796
Iteration 171/1000 | Loss: 0.00001796
Iteration 172/1000 | Loss: 0.00001796
Iteration 173/1000 | Loss: 0.00001796
Iteration 174/1000 | Loss: 0.00001796
Iteration 175/1000 | Loss: 0.00001796
Iteration 176/1000 | Loss: 0.00001796
Iteration 177/1000 | Loss: 0.00001795
Iteration 178/1000 | Loss: 0.00001795
Iteration 179/1000 | Loss: 0.00001795
Iteration 180/1000 | Loss: 0.00001795
Iteration 181/1000 | Loss: 0.00001795
Iteration 182/1000 | Loss: 0.00001795
Iteration 183/1000 | Loss: 0.00001795
Iteration 184/1000 | Loss: 0.00001795
Iteration 185/1000 | Loss: 0.00001795
Iteration 186/1000 | Loss: 0.00001795
Iteration 187/1000 | Loss: 0.00001795
Iteration 188/1000 | Loss: 0.00001795
Iteration 189/1000 | Loss: 0.00001795
Iteration 190/1000 | Loss: 0.00001795
Iteration 191/1000 | Loss: 0.00001795
Iteration 192/1000 | Loss: 0.00001795
Iteration 193/1000 | Loss: 0.00001795
Iteration 194/1000 | Loss: 0.00001795
Iteration 195/1000 | Loss: 0.00001794
Iteration 196/1000 | Loss: 0.00001794
Iteration 197/1000 | Loss: 0.00001794
Iteration 198/1000 | Loss: 0.00001794
Iteration 199/1000 | Loss: 0.00001794
Iteration 200/1000 | Loss: 0.00001794
Iteration 201/1000 | Loss: 0.00001794
Iteration 202/1000 | Loss: 0.00001794
Iteration 203/1000 | Loss: 0.00001794
Iteration 204/1000 | Loss: 0.00001794
Iteration 205/1000 | Loss: 0.00001794
Iteration 206/1000 | Loss: 0.00001794
Iteration 207/1000 | Loss: 0.00001794
Iteration 208/1000 | Loss: 0.00001794
Iteration 209/1000 | Loss: 0.00001794
Iteration 210/1000 | Loss: 0.00001794
Iteration 211/1000 | Loss: 0.00001794
Iteration 212/1000 | Loss: 0.00001794
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [1.7940677935257554e-05, 1.7940677935257554e-05, 1.7940677935257554e-05, 1.7940677935257554e-05, 1.7940677935257554e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7940677935257554e-05

Optimization complete. Final v2v error: 3.478548765182495 mm

Highest mean error: 4.551466464996338 mm for frame 130

Lowest mean error: 2.4337151050567627 mm for frame 223

Saving results

Total time: 52.33786606788635
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00594619
Iteration 2/25 | Loss: 0.00148780
Iteration 3/25 | Loss: 0.00124957
Iteration 4/25 | Loss: 0.00122297
Iteration 5/25 | Loss: 0.00121666
Iteration 6/25 | Loss: 0.00121551
Iteration 7/25 | Loss: 0.00121481
Iteration 8/25 | Loss: 0.00121535
Iteration 9/25 | Loss: 0.00121536
Iteration 10/25 | Loss: 0.00121509
Iteration 11/25 | Loss: 0.00121507
Iteration 12/25 | Loss: 0.00121507
Iteration 13/25 | Loss: 0.00121506
Iteration 14/25 | Loss: 0.00121506
Iteration 15/25 | Loss: 0.00121505
Iteration 16/25 | Loss: 0.00121504
Iteration 17/25 | Loss: 0.00121455
Iteration 18/25 | Loss: 0.00121415
Iteration 19/25 | Loss: 0.00121496
Iteration 20/25 | Loss: 0.00121530
Iteration 21/25 | Loss: 0.00121476
Iteration 22/25 | Loss: 0.00121485
Iteration 23/25 | Loss: 0.00121537
Iteration 24/25 | Loss: 0.00121477
Iteration 25/25 | Loss: 0.00121477

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33572638
Iteration 2/25 | Loss: 0.00097471
Iteration 3/25 | Loss: 0.00097469
Iteration 4/25 | Loss: 0.00097469
Iteration 5/25 | Loss: 0.00097469
Iteration 6/25 | Loss: 0.00097469
Iteration 7/25 | Loss: 0.00097469
Iteration 8/25 | Loss: 0.00097469
Iteration 9/25 | Loss: 0.00097469
Iteration 10/25 | Loss: 0.00097469
Iteration 11/25 | Loss: 0.00097469
Iteration 12/25 | Loss: 0.00097469
Iteration 13/25 | Loss: 0.00097469
Iteration 14/25 | Loss: 0.00097469
Iteration 15/25 | Loss: 0.00097469
Iteration 16/25 | Loss: 0.00097469
Iteration 17/25 | Loss: 0.00097469
Iteration 18/25 | Loss: 0.00097469
Iteration 19/25 | Loss: 0.00097469
Iteration 20/25 | Loss: 0.00097469
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009746869909577072, 0.0009746869909577072, 0.0009746869909577072, 0.0009746869909577072, 0.0009746869909577072]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009746869909577072

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097469
Iteration 2/1000 | Loss: 0.00003805
Iteration 3/1000 | Loss: 0.00002430
Iteration 4/1000 | Loss: 0.00002239
Iteration 5/1000 | Loss: 0.00002150
Iteration 6/1000 | Loss: 0.00002073
Iteration 7/1000 | Loss: 0.00002025
Iteration 8/1000 | Loss: 0.00001994
Iteration 9/1000 | Loss: 0.00001965
Iteration 10/1000 | Loss: 0.00001945
Iteration 11/1000 | Loss: 0.00001926
Iteration 12/1000 | Loss: 0.00001909
Iteration 13/1000 | Loss: 0.00001901
Iteration 14/1000 | Loss: 0.00001897
Iteration 15/1000 | Loss: 0.00001896
Iteration 16/1000 | Loss: 0.00001894
Iteration 17/1000 | Loss: 0.00001893
Iteration 18/1000 | Loss: 0.00001893
Iteration 19/1000 | Loss: 0.00001892
Iteration 20/1000 | Loss: 0.00001892
Iteration 21/1000 | Loss: 0.00001889
Iteration 22/1000 | Loss: 0.00001888
Iteration 23/1000 | Loss: 0.00001888
Iteration 24/1000 | Loss: 0.00001880
Iteration 25/1000 | Loss: 0.00001876
Iteration 26/1000 | Loss: 0.00001876
Iteration 27/1000 | Loss: 0.00001875
Iteration 28/1000 | Loss: 0.00001874
Iteration 29/1000 | Loss: 0.00001873
Iteration 30/1000 | Loss: 0.00001873
Iteration 31/1000 | Loss: 0.00001872
Iteration 32/1000 | Loss: 0.00001871
Iteration 33/1000 | Loss: 0.00001871
Iteration 34/1000 | Loss: 0.00001870
Iteration 35/1000 | Loss: 0.00001870
Iteration 36/1000 | Loss: 0.00001866
Iteration 37/1000 | Loss: 0.00001865
Iteration 38/1000 | Loss: 0.00001865
Iteration 39/1000 | Loss: 0.00001858
Iteration 40/1000 | Loss: 0.00001858
Iteration 41/1000 | Loss: 0.00001857
Iteration 42/1000 | Loss: 0.00001856
Iteration 43/1000 | Loss: 0.00001856
Iteration 44/1000 | Loss: 0.00001856
Iteration 45/1000 | Loss: 0.00001855
Iteration 46/1000 | Loss: 0.00001855
Iteration 47/1000 | Loss: 0.00001854
Iteration 48/1000 | Loss: 0.00001854
Iteration 49/1000 | Loss: 0.00001854
Iteration 50/1000 | Loss: 0.00001854
Iteration 51/1000 | Loss: 0.00001854
Iteration 52/1000 | Loss: 0.00001854
Iteration 53/1000 | Loss: 0.00001853
Iteration 54/1000 | Loss: 0.00001853
Iteration 55/1000 | Loss: 0.00001852
Iteration 56/1000 | Loss: 0.00001852
Iteration 57/1000 | Loss: 0.00001851
Iteration 58/1000 | Loss: 0.00001851
Iteration 59/1000 | Loss: 0.00001851
Iteration 60/1000 | Loss: 0.00001850
Iteration 61/1000 | Loss: 0.00001850
Iteration 62/1000 | Loss: 0.00001850
Iteration 63/1000 | Loss: 0.00001849
Iteration 64/1000 | Loss: 0.00001849
Iteration 65/1000 | Loss: 0.00001849
Iteration 66/1000 | Loss: 0.00001848
Iteration 67/1000 | Loss: 0.00001848
Iteration 68/1000 | Loss: 0.00001847
Iteration 69/1000 | Loss: 0.00001847
Iteration 70/1000 | Loss: 0.00001846
Iteration 71/1000 | Loss: 0.00001846
Iteration 72/1000 | Loss: 0.00001846
Iteration 73/1000 | Loss: 0.00001845
Iteration 74/1000 | Loss: 0.00001845
Iteration 75/1000 | Loss: 0.00001845
Iteration 76/1000 | Loss: 0.00001844
Iteration 77/1000 | Loss: 0.00001844
Iteration 78/1000 | Loss: 0.00001844
Iteration 79/1000 | Loss: 0.00001844
Iteration 80/1000 | Loss: 0.00001844
Iteration 81/1000 | Loss: 0.00001843
Iteration 82/1000 | Loss: 0.00001843
Iteration 83/1000 | Loss: 0.00001843
Iteration 84/1000 | Loss: 0.00001843
Iteration 85/1000 | Loss: 0.00001842
Iteration 86/1000 | Loss: 0.00001842
Iteration 87/1000 | Loss: 0.00001842
Iteration 88/1000 | Loss: 0.00001842
Iteration 89/1000 | Loss: 0.00001841
Iteration 90/1000 | Loss: 0.00001841
Iteration 91/1000 | Loss: 0.00001841
Iteration 92/1000 | Loss: 0.00001841
Iteration 93/1000 | Loss: 0.00001841
Iteration 94/1000 | Loss: 0.00001840
Iteration 95/1000 | Loss: 0.00001840
Iteration 96/1000 | Loss: 0.00001840
Iteration 97/1000 | Loss: 0.00001840
Iteration 98/1000 | Loss: 0.00001839
Iteration 99/1000 | Loss: 0.00001839
Iteration 100/1000 | Loss: 0.00001839
Iteration 101/1000 | Loss: 0.00001839
Iteration 102/1000 | Loss: 0.00001838
Iteration 103/1000 | Loss: 0.00001838
Iteration 104/1000 | Loss: 0.00001838
Iteration 105/1000 | Loss: 0.00001838
Iteration 106/1000 | Loss: 0.00001838
Iteration 107/1000 | Loss: 0.00001838
Iteration 108/1000 | Loss: 0.00001838
Iteration 109/1000 | Loss: 0.00001838
Iteration 110/1000 | Loss: 0.00001837
Iteration 111/1000 | Loss: 0.00001837
Iteration 112/1000 | Loss: 0.00001837
Iteration 113/1000 | Loss: 0.00001836
Iteration 114/1000 | Loss: 0.00001836
Iteration 115/1000 | Loss: 0.00001836
Iteration 116/1000 | Loss: 0.00001836
Iteration 117/1000 | Loss: 0.00001836
Iteration 118/1000 | Loss: 0.00001836
Iteration 119/1000 | Loss: 0.00001836
Iteration 120/1000 | Loss: 0.00001835
Iteration 121/1000 | Loss: 0.00001835
Iteration 122/1000 | Loss: 0.00001835
Iteration 123/1000 | Loss: 0.00001835
Iteration 124/1000 | Loss: 0.00001835
Iteration 125/1000 | Loss: 0.00001835
Iteration 126/1000 | Loss: 0.00001835
Iteration 127/1000 | Loss: 0.00001835
Iteration 128/1000 | Loss: 0.00001835
Iteration 129/1000 | Loss: 0.00001834
Iteration 130/1000 | Loss: 0.00001834
Iteration 131/1000 | Loss: 0.00001834
Iteration 132/1000 | Loss: 0.00001834
Iteration 133/1000 | Loss: 0.00001834
Iteration 134/1000 | Loss: 0.00001834
Iteration 135/1000 | Loss: 0.00001834
Iteration 136/1000 | Loss: 0.00001834
Iteration 137/1000 | Loss: 0.00001834
Iteration 138/1000 | Loss: 0.00001834
Iteration 139/1000 | Loss: 0.00001834
Iteration 140/1000 | Loss: 0.00001834
Iteration 141/1000 | Loss: 0.00001834
Iteration 142/1000 | Loss: 0.00001834
Iteration 143/1000 | Loss: 0.00001834
Iteration 144/1000 | Loss: 0.00001834
Iteration 145/1000 | Loss: 0.00001834
Iteration 146/1000 | Loss: 0.00001834
Iteration 147/1000 | Loss: 0.00001834
Iteration 148/1000 | Loss: 0.00001834
Iteration 149/1000 | Loss: 0.00001834
Iteration 150/1000 | Loss: 0.00001834
Iteration 151/1000 | Loss: 0.00001834
Iteration 152/1000 | Loss: 0.00001834
Iteration 153/1000 | Loss: 0.00001834
Iteration 154/1000 | Loss: 0.00001834
Iteration 155/1000 | Loss: 0.00001834
Iteration 156/1000 | Loss: 0.00001834
Iteration 157/1000 | Loss: 0.00001834
Iteration 158/1000 | Loss: 0.00001834
Iteration 159/1000 | Loss: 0.00001834
Iteration 160/1000 | Loss: 0.00001834
Iteration 161/1000 | Loss: 0.00001834
Iteration 162/1000 | Loss: 0.00001834
Iteration 163/1000 | Loss: 0.00001834
Iteration 164/1000 | Loss: 0.00001834
Iteration 165/1000 | Loss: 0.00001834
Iteration 166/1000 | Loss: 0.00001834
Iteration 167/1000 | Loss: 0.00001834
Iteration 168/1000 | Loss: 0.00001834
Iteration 169/1000 | Loss: 0.00001834
Iteration 170/1000 | Loss: 0.00001834
Iteration 171/1000 | Loss: 0.00001834
Iteration 172/1000 | Loss: 0.00001834
Iteration 173/1000 | Loss: 0.00001834
Iteration 174/1000 | Loss: 0.00001834
Iteration 175/1000 | Loss: 0.00001834
Iteration 176/1000 | Loss: 0.00001834
Iteration 177/1000 | Loss: 0.00001834
Iteration 178/1000 | Loss: 0.00001834
Iteration 179/1000 | Loss: 0.00001834
Iteration 180/1000 | Loss: 0.00001834
Iteration 181/1000 | Loss: 0.00001834
Iteration 182/1000 | Loss: 0.00001834
Iteration 183/1000 | Loss: 0.00001834
Iteration 184/1000 | Loss: 0.00001834
Iteration 185/1000 | Loss: 0.00001834
Iteration 186/1000 | Loss: 0.00001834
Iteration 187/1000 | Loss: 0.00001834
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.8338641893933527e-05, 1.8338641893933527e-05, 1.8338641893933527e-05, 1.8338641893933527e-05, 1.8338641893933527e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8338641893933527e-05

Optimization complete. Final v2v error: 3.6236889362335205 mm

Highest mean error: 4.790129661560059 mm for frame 60

Lowest mean error: 3.328174352645874 mm for frame 111

Saving results

Total time: 77.59393048286438
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01038753
Iteration 2/25 | Loss: 0.01038753
Iteration 3/25 | Loss: 0.01038753
Iteration 4/25 | Loss: 0.01038753
Iteration 5/25 | Loss: 0.01038753
Iteration 6/25 | Loss: 0.01038753
Iteration 7/25 | Loss: 0.01038753
Iteration 8/25 | Loss: 0.01038753
Iteration 9/25 | Loss: 0.01038753
Iteration 10/25 | Loss: 0.01038752
Iteration 11/25 | Loss: 0.01038752
Iteration 12/25 | Loss: 0.01038752
Iteration 13/25 | Loss: 0.01038752
Iteration 14/25 | Loss: 0.01038752
Iteration 15/25 | Loss: 0.01038752
Iteration 16/25 | Loss: 0.01038752
Iteration 17/25 | Loss: 0.01038751
Iteration 18/25 | Loss: 0.01038751
Iteration 19/25 | Loss: 0.01038751
Iteration 20/25 | Loss: 0.01038751
Iteration 21/25 | Loss: 0.01038751
Iteration 22/25 | Loss: 0.01038751
Iteration 23/25 | Loss: 0.01038751
Iteration 24/25 | Loss: 0.01038751
Iteration 25/25 | Loss: 0.01038751

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.81673229
Iteration 2/25 | Loss: 0.06255974
Iteration 3/25 | Loss: 0.06254236
Iteration 4/25 | Loss: 0.06254236
Iteration 5/25 | Loss: 0.06254236
Iteration 6/25 | Loss: 0.06254236
Iteration 7/25 | Loss: 0.06254236
Iteration 8/25 | Loss: 0.06254236
Iteration 9/25 | Loss: 0.06254236
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.06254235655069351, 0.06254235655069351, 0.06254235655069351, 0.06254235655069351, 0.06254235655069351]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.06254235655069351

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.06254236
Iteration 2/1000 | Loss: 0.00386144
Iteration 3/1000 | Loss: 0.00083521
Iteration 4/1000 | Loss: 0.00033215
Iteration 5/1000 | Loss: 0.00016371
Iteration 6/1000 | Loss: 0.00011427
Iteration 7/1000 | Loss: 0.00008501
Iteration 8/1000 | Loss: 0.00006626
Iteration 9/1000 | Loss: 0.00005592
Iteration 10/1000 | Loss: 0.00004800
Iteration 11/1000 | Loss: 0.00004191
Iteration 12/1000 | Loss: 0.00003725
Iteration 13/1000 | Loss: 0.00003398
Iteration 14/1000 | Loss: 0.00003109
Iteration 15/1000 | Loss: 0.00002887
Iteration 16/1000 | Loss: 0.00002727
Iteration 17/1000 | Loss: 0.00002538
Iteration 18/1000 | Loss: 0.00002401
Iteration 19/1000 | Loss: 0.00002297
Iteration 20/1000 | Loss: 0.00002211
Iteration 21/1000 | Loss: 0.00002127
Iteration 22/1000 | Loss: 0.00002045
Iteration 23/1000 | Loss: 0.00001994
Iteration 24/1000 | Loss: 0.00001954
Iteration 25/1000 | Loss: 0.00001919
Iteration 26/1000 | Loss: 0.00001886
Iteration 27/1000 | Loss: 0.00001849
Iteration 28/1000 | Loss: 0.00001824
Iteration 29/1000 | Loss: 0.00001806
Iteration 30/1000 | Loss: 0.00001796
Iteration 31/1000 | Loss: 0.00001779
Iteration 32/1000 | Loss: 0.00001779
Iteration 33/1000 | Loss: 0.00001775
Iteration 34/1000 | Loss: 0.00001774
Iteration 35/1000 | Loss: 0.00001773
Iteration 36/1000 | Loss: 0.00001772
Iteration 37/1000 | Loss: 0.00001772
Iteration 38/1000 | Loss: 0.00001771
Iteration 39/1000 | Loss: 0.00001765
Iteration 40/1000 | Loss: 0.00001758
Iteration 41/1000 | Loss: 0.00001755
Iteration 42/1000 | Loss: 0.00001745
Iteration 43/1000 | Loss: 0.00001744
Iteration 44/1000 | Loss: 0.00001741
Iteration 45/1000 | Loss: 0.00001740
Iteration 46/1000 | Loss: 0.00001738
Iteration 47/1000 | Loss: 0.00001738
Iteration 48/1000 | Loss: 0.00001737
Iteration 49/1000 | Loss: 0.00001737
Iteration 50/1000 | Loss: 0.00001737
Iteration 51/1000 | Loss: 0.00001737
Iteration 52/1000 | Loss: 0.00001736
Iteration 53/1000 | Loss: 0.00001736
Iteration 54/1000 | Loss: 0.00001736
Iteration 55/1000 | Loss: 0.00001736
Iteration 56/1000 | Loss: 0.00001735
Iteration 57/1000 | Loss: 0.00001735
Iteration 58/1000 | Loss: 0.00001735
Iteration 59/1000 | Loss: 0.00001734
Iteration 60/1000 | Loss: 0.00001734
Iteration 61/1000 | Loss: 0.00001733
Iteration 62/1000 | Loss: 0.00001733
Iteration 63/1000 | Loss: 0.00001733
Iteration 64/1000 | Loss: 0.00001732
Iteration 65/1000 | Loss: 0.00001732
Iteration 66/1000 | Loss: 0.00001732
Iteration 67/1000 | Loss: 0.00001731
Iteration 68/1000 | Loss: 0.00001731
Iteration 69/1000 | Loss: 0.00001730
Iteration 70/1000 | Loss: 0.00001730
Iteration 71/1000 | Loss: 0.00001730
Iteration 72/1000 | Loss: 0.00001729
Iteration 73/1000 | Loss: 0.00001729
Iteration 74/1000 | Loss: 0.00001729
Iteration 75/1000 | Loss: 0.00001728
Iteration 76/1000 | Loss: 0.00001728
Iteration 77/1000 | Loss: 0.00001727
Iteration 78/1000 | Loss: 0.00001727
Iteration 79/1000 | Loss: 0.00001727
Iteration 80/1000 | Loss: 0.00001726
Iteration 81/1000 | Loss: 0.00001726
Iteration 82/1000 | Loss: 0.00001726
Iteration 83/1000 | Loss: 0.00001726
Iteration 84/1000 | Loss: 0.00001725
Iteration 85/1000 | Loss: 0.00001725
Iteration 86/1000 | Loss: 0.00001724
Iteration 87/1000 | Loss: 0.00001724
Iteration 88/1000 | Loss: 0.00001724
Iteration 89/1000 | Loss: 0.00001724
Iteration 90/1000 | Loss: 0.00001723
Iteration 91/1000 | Loss: 0.00001723
Iteration 92/1000 | Loss: 0.00001723
Iteration 93/1000 | Loss: 0.00001722
Iteration 94/1000 | Loss: 0.00001722
Iteration 95/1000 | Loss: 0.00001722
Iteration 96/1000 | Loss: 0.00001722
Iteration 97/1000 | Loss: 0.00001721
Iteration 98/1000 | Loss: 0.00001721
Iteration 99/1000 | Loss: 0.00001721
Iteration 100/1000 | Loss: 0.00001721
Iteration 101/1000 | Loss: 0.00001721
Iteration 102/1000 | Loss: 0.00001720
Iteration 103/1000 | Loss: 0.00001720
Iteration 104/1000 | Loss: 0.00001720
Iteration 105/1000 | Loss: 0.00001720
Iteration 106/1000 | Loss: 0.00001720
Iteration 107/1000 | Loss: 0.00001720
Iteration 108/1000 | Loss: 0.00001719
Iteration 109/1000 | Loss: 0.00001719
Iteration 110/1000 | Loss: 0.00001719
Iteration 111/1000 | Loss: 0.00001719
Iteration 112/1000 | Loss: 0.00001719
Iteration 113/1000 | Loss: 0.00001719
Iteration 114/1000 | Loss: 0.00001719
Iteration 115/1000 | Loss: 0.00001719
Iteration 116/1000 | Loss: 0.00001719
Iteration 117/1000 | Loss: 0.00001719
Iteration 118/1000 | Loss: 0.00001719
Iteration 119/1000 | Loss: 0.00001719
Iteration 120/1000 | Loss: 0.00001718
Iteration 121/1000 | Loss: 0.00001718
Iteration 122/1000 | Loss: 0.00001718
Iteration 123/1000 | Loss: 0.00001718
Iteration 124/1000 | Loss: 0.00001718
Iteration 125/1000 | Loss: 0.00001718
Iteration 126/1000 | Loss: 0.00001718
Iteration 127/1000 | Loss: 0.00001718
Iteration 128/1000 | Loss: 0.00001718
Iteration 129/1000 | Loss: 0.00001717
Iteration 130/1000 | Loss: 0.00001717
Iteration 131/1000 | Loss: 0.00001717
Iteration 132/1000 | Loss: 0.00001717
Iteration 133/1000 | Loss: 0.00001717
Iteration 134/1000 | Loss: 0.00001717
Iteration 135/1000 | Loss: 0.00001717
Iteration 136/1000 | Loss: 0.00001716
Iteration 137/1000 | Loss: 0.00001716
Iteration 138/1000 | Loss: 0.00001716
Iteration 139/1000 | Loss: 0.00001716
Iteration 140/1000 | Loss: 0.00001716
Iteration 141/1000 | Loss: 0.00001716
Iteration 142/1000 | Loss: 0.00001716
Iteration 143/1000 | Loss: 0.00001716
Iteration 144/1000 | Loss: 0.00001716
Iteration 145/1000 | Loss: 0.00001716
Iteration 146/1000 | Loss: 0.00001716
Iteration 147/1000 | Loss: 0.00001716
Iteration 148/1000 | Loss: 0.00001716
Iteration 149/1000 | Loss: 0.00001716
Iteration 150/1000 | Loss: 0.00001716
Iteration 151/1000 | Loss: 0.00001715
Iteration 152/1000 | Loss: 0.00001715
Iteration 153/1000 | Loss: 0.00001715
Iteration 154/1000 | Loss: 0.00001715
Iteration 155/1000 | Loss: 0.00001715
Iteration 156/1000 | Loss: 0.00001715
Iteration 157/1000 | Loss: 0.00001715
Iteration 158/1000 | Loss: 0.00001715
Iteration 159/1000 | Loss: 0.00001715
Iteration 160/1000 | Loss: 0.00001715
Iteration 161/1000 | Loss: 0.00001715
Iteration 162/1000 | Loss: 0.00001715
Iteration 163/1000 | Loss: 0.00001715
Iteration 164/1000 | Loss: 0.00001715
Iteration 165/1000 | Loss: 0.00001715
Iteration 166/1000 | Loss: 0.00001715
Iteration 167/1000 | Loss: 0.00001715
Iteration 168/1000 | Loss: 0.00001715
Iteration 169/1000 | Loss: 0.00001715
Iteration 170/1000 | Loss: 0.00001715
Iteration 171/1000 | Loss: 0.00001715
Iteration 172/1000 | Loss: 0.00001715
Iteration 173/1000 | Loss: 0.00001715
Iteration 174/1000 | Loss: 0.00001715
Iteration 175/1000 | Loss: 0.00001715
Iteration 176/1000 | Loss: 0.00001715
Iteration 177/1000 | Loss: 0.00001715
Iteration 178/1000 | Loss: 0.00001715
Iteration 179/1000 | Loss: 0.00001715
Iteration 180/1000 | Loss: 0.00001715
Iteration 181/1000 | Loss: 0.00001715
Iteration 182/1000 | Loss: 0.00001715
Iteration 183/1000 | Loss: 0.00001715
Iteration 184/1000 | Loss: 0.00001715
Iteration 185/1000 | Loss: 0.00001715
Iteration 186/1000 | Loss: 0.00001715
Iteration 187/1000 | Loss: 0.00001715
Iteration 188/1000 | Loss: 0.00001715
Iteration 189/1000 | Loss: 0.00001715
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.714855716272723e-05, 1.714855716272723e-05, 1.714855716272723e-05, 1.714855716272723e-05, 1.714855716272723e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.714855716272723e-05

Optimization complete. Final v2v error: 3.3156180381774902 mm

Highest mean error: 5.273240089416504 mm for frame 44

Lowest mean error: 2.6675913333892822 mm for frame 192

Saving results

Total time: 71.27971601486206
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00405332
Iteration 2/25 | Loss: 0.00119701
Iteration 3/25 | Loss: 0.00109977
Iteration 4/25 | Loss: 0.00109248
Iteration 5/25 | Loss: 0.00108972
Iteration 6/25 | Loss: 0.00108972
Iteration 7/25 | Loss: 0.00108972
Iteration 8/25 | Loss: 0.00108972
Iteration 9/25 | Loss: 0.00108972
Iteration 10/25 | Loss: 0.00108972
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010897184256464243, 0.0010897184256464243, 0.0010897184256464243, 0.0010897184256464243, 0.0010897184256464243]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010897184256464243

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63044441
Iteration 2/25 | Loss: 0.00068443
Iteration 3/25 | Loss: 0.00068443
Iteration 4/25 | Loss: 0.00068443
Iteration 5/25 | Loss: 0.00068443
Iteration 6/25 | Loss: 0.00068443
Iteration 7/25 | Loss: 0.00068443
Iteration 8/25 | Loss: 0.00068443
Iteration 9/25 | Loss: 0.00068443
Iteration 10/25 | Loss: 0.00068443
Iteration 11/25 | Loss: 0.00068443
Iteration 12/25 | Loss: 0.00068443
Iteration 13/25 | Loss: 0.00068443
Iteration 14/25 | Loss: 0.00068443
Iteration 15/25 | Loss: 0.00068443
Iteration 16/25 | Loss: 0.00068443
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000684428378008306, 0.000684428378008306, 0.000684428378008306, 0.000684428378008306, 0.000684428378008306]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000684428378008306

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068443
Iteration 2/1000 | Loss: 0.00002305
Iteration 3/1000 | Loss: 0.00001480
Iteration 4/1000 | Loss: 0.00001232
Iteration 5/1000 | Loss: 0.00001133
Iteration 6/1000 | Loss: 0.00001073
Iteration 7/1000 | Loss: 0.00001035
Iteration 8/1000 | Loss: 0.00001017
Iteration 9/1000 | Loss: 0.00000987
Iteration 10/1000 | Loss: 0.00000959
Iteration 11/1000 | Loss: 0.00000954
Iteration 12/1000 | Loss: 0.00000954
Iteration 13/1000 | Loss: 0.00000952
Iteration 14/1000 | Loss: 0.00000949
Iteration 15/1000 | Loss: 0.00000947
Iteration 16/1000 | Loss: 0.00000947
Iteration 17/1000 | Loss: 0.00000946
Iteration 18/1000 | Loss: 0.00000946
Iteration 19/1000 | Loss: 0.00000945
Iteration 20/1000 | Loss: 0.00000945
Iteration 21/1000 | Loss: 0.00000937
Iteration 22/1000 | Loss: 0.00000936
Iteration 23/1000 | Loss: 0.00000932
Iteration 24/1000 | Loss: 0.00000919
Iteration 25/1000 | Loss: 0.00000915
Iteration 26/1000 | Loss: 0.00000911
Iteration 27/1000 | Loss: 0.00000910
Iteration 28/1000 | Loss: 0.00000910
Iteration 29/1000 | Loss: 0.00000909
Iteration 30/1000 | Loss: 0.00000909
Iteration 31/1000 | Loss: 0.00000909
Iteration 32/1000 | Loss: 0.00000909
Iteration 33/1000 | Loss: 0.00000908
Iteration 34/1000 | Loss: 0.00000907
Iteration 35/1000 | Loss: 0.00000907
Iteration 36/1000 | Loss: 0.00000907
Iteration 37/1000 | Loss: 0.00000907
Iteration 38/1000 | Loss: 0.00000907
Iteration 39/1000 | Loss: 0.00000907
Iteration 40/1000 | Loss: 0.00000907
Iteration 41/1000 | Loss: 0.00000907
Iteration 42/1000 | Loss: 0.00000906
Iteration 43/1000 | Loss: 0.00000906
Iteration 44/1000 | Loss: 0.00000906
Iteration 45/1000 | Loss: 0.00000905
Iteration 46/1000 | Loss: 0.00000905
Iteration 47/1000 | Loss: 0.00000904
Iteration 48/1000 | Loss: 0.00000904
Iteration 49/1000 | Loss: 0.00000904
Iteration 50/1000 | Loss: 0.00000904
Iteration 51/1000 | Loss: 0.00000903
Iteration 52/1000 | Loss: 0.00000903
Iteration 53/1000 | Loss: 0.00000902
Iteration 54/1000 | Loss: 0.00000902
Iteration 55/1000 | Loss: 0.00000901
Iteration 56/1000 | Loss: 0.00000901
Iteration 57/1000 | Loss: 0.00000900
Iteration 58/1000 | Loss: 0.00000900
Iteration 59/1000 | Loss: 0.00000900
Iteration 60/1000 | Loss: 0.00000899
Iteration 61/1000 | Loss: 0.00000899
Iteration 62/1000 | Loss: 0.00000899
Iteration 63/1000 | Loss: 0.00000898
Iteration 64/1000 | Loss: 0.00000898
Iteration 65/1000 | Loss: 0.00000898
Iteration 66/1000 | Loss: 0.00000898
Iteration 67/1000 | Loss: 0.00000898
Iteration 68/1000 | Loss: 0.00000898
Iteration 69/1000 | Loss: 0.00000898
Iteration 70/1000 | Loss: 0.00000898
Iteration 71/1000 | Loss: 0.00000898
Iteration 72/1000 | Loss: 0.00000898
Iteration 73/1000 | Loss: 0.00000897
Iteration 74/1000 | Loss: 0.00000897
Iteration 75/1000 | Loss: 0.00000897
Iteration 76/1000 | Loss: 0.00000897
Iteration 77/1000 | Loss: 0.00000896
Iteration 78/1000 | Loss: 0.00000896
Iteration 79/1000 | Loss: 0.00000896
Iteration 80/1000 | Loss: 0.00000896
Iteration 81/1000 | Loss: 0.00000896
Iteration 82/1000 | Loss: 0.00000896
Iteration 83/1000 | Loss: 0.00000896
Iteration 84/1000 | Loss: 0.00000896
Iteration 85/1000 | Loss: 0.00000896
Iteration 86/1000 | Loss: 0.00000896
Iteration 87/1000 | Loss: 0.00000896
Iteration 88/1000 | Loss: 0.00000896
Iteration 89/1000 | Loss: 0.00000896
Iteration 90/1000 | Loss: 0.00000896
Iteration 91/1000 | Loss: 0.00000895
Iteration 92/1000 | Loss: 0.00000895
Iteration 93/1000 | Loss: 0.00000895
Iteration 94/1000 | Loss: 0.00000894
Iteration 95/1000 | Loss: 0.00000894
Iteration 96/1000 | Loss: 0.00000894
Iteration 97/1000 | Loss: 0.00000894
Iteration 98/1000 | Loss: 0.00000894
Iteration 99/1000 | Loss: 0.00000894
Iteration 100/1000 | Loss: 0.00000893
Iteration 101/1000 | Loss: 0.00000893
Iteration 102/1000 | Loss: 0.00000893
Iteration 103/1000 | Loss: 0.00000893
Iteration 104/1000 | Loss: 0.00000893
Iteration 105/1000 | Loss: 0.00000893
Iteration 106/1000 | Loss: 0.00000893
Iteration 107/1000 | Loss: 0.00000892
Iteration 108/1000 | Loss: 0.00000892
Iteration 109/1000 | Loss: 0.00000891
Iteration 110/1000 | Loss: 0.00000891
Iteration 111/1000 | Loss: 0.00000891
Iteration 112/1000 | Loss: 0.00000891
Iteration 113/1000 | Loss: 0.00000891
Iteration 114/1000 | Loss: 0.00000891
Iteration 115/1000 | Loss: 0.00000891
Iteration 116/1000 | Loss: 0.00000891
Iteration 117/1000 | Loss: 0.00000891
Iteration 118/1000 | Loss: 0.00000891
Iteration 119/1000 | Loss: 0.00000891
Iteration 120/1000 | Loss: 0.00000891
Iteration 121/1000 | Loss: 0.00000891
Iteration 122/1000 | Loss: 0.00000891
Iteration 123/1000 | Loss: 0.00000891
Iteration 124/1000 | Loss: 0.00000891
Iteration 125/1000 | Loss: 0.00000890
Iteration 126/1000 | Loss: 0.00000890
Iteration 127/1000 | Loss: 0.00000890
Iteration 128/1000 | Loss: 0.00000890
Iteration 129/1000 | Loss: 0.00000890
Iteration 130/1000 | Loss: 0.00000890
Iteration 131/1000 | Loss: 0.00000890
Iteration 132/1000 | Loss: 0.00000889
Iteration 133/1000 | Loss: 0.00000889
Iteration 134/1000 | Loss: 0.00000889
Iteration 135/1000 | Loss: 0.00000889
Iteration 136/1000 | Loss: 0.00000889
Iteration 137/1000 | Loss: 0.00000889
Iteration 138/1000 | Loss: 0.00000889
Iteration 139/1000 | Loss: 0.00000889
Iteration 140/1000 | Loss: 0.00000889
Iteration 141/1000 | Loss: 0.00000888
Iteration 142/1000 | Loss: 0.00000888
Iteration 143/1000 | Loss: 0.00000888
Iteration 144/1000 | Loss: 0.00000888
Iteration 145/1000 | Loss: 0.00000888
Iteration 146/1000 | Loss: 0.00000888
Iteration 147/1000 | Loss: 0.00000888
Iteration 148/1000 | Loss: 0.00000888
Iteration 149/1000 | Loss: 0.00000888
Iteration 150/1000 | Loss: 0.00000888
Iteration 151/1000 | Loss: 0.00000887
Iteration 152/1000 | Loss: 0.00000887
Iteration 153/1000 | Loss: 0.00000887
Iteration 154/1000 | Loss: 0.00000887
Iteration 155/1000 | Loss: 0.00000887
Iteration 156/1000 | Loss: 0.00000887
Iteration 157/1000 | Loss: 0.00000887
Iteration 158/1000 | Loss: 0.00000887
Iteration 159/1000 | Loss: 0.00000887
Iteration 160/1000 | Loss: 0.00000887
Iteration 161/1000 | Loss: 0.00000887
Iteration 162/1000 | Loss: 0.00000887
Iteration 163/1000 | Loss: 0.00000887
Iteration 164/1000 | Loss: 0.00000887
Iteration 165/1000 | Loss: 0.00000887
Iteration 166/1000 | Loss: 0.00000887
Iteration 167/1000 | Loss: 0.00000887
Iteration 168/1000 | Loss: 0.00000887
Iteration 169/1000 | Loss: 0.00000886
Iteration 170/1000 | Loss: 0.00000886
Iteration 171/1000 | Loss: 0.00000886
Iteration 172/1000 | Loss: 0.00000886
Iteration 173/1000 | Loss: 0.00000886
Iteration 174/1000 | Loss: 0.00000886
Iteration 175/1000 | Loss: 0.00000886
Iteration 176/1000 | Loss: 0.00000886
Iteration 177/1000 | Loss: 0.00000886
Iteration 178/1000 | Loss: 0.00000886
Iteration 179/1000 | Loss: 0.00000886
Iteration 180/1000 | Loss: 0.00000886
Iteration 181/1000 | Loss: 0.00000886
Iteration 182/1000 | Loss: 0.00000886
Iteration 183/1000 | Loss: 0.00000886
Iteration 184/1000 | Loss: 0.00000885
Iteration 185/1000 | Loss: 0.00000885
Iteration 186/1000 | Loss: 0.00000885
Iteration 187/1000 | Loss: 0.00000885
Iteration 188/1000 | Loss: 0.00000885
Iteration 189/1000 | Loss: 0.00000884
Iteration 190/1000 | Loss: 0.00000884
Iteration 191/1000 | Loss: 0.00000884
Iteration 192/1000 | Loss: 0.00000884
Iteration 193/1000 | Loss: 0.00000884
Iteration 194/1000 | Loss: 0.00000884
Iteration 195/1000 | Loss: 0.00000884
Iteration 196/1000 | Loss: 0.00000884
Iteration 197/1000 | Loss: 0.00000884
Iteration 198/1000 | Loss: 0.00000883
Iteration 199/1000 | Loss: 0.00000883
Iteration 200/1000 | Loss: 0.00000883
Iteration 201/1000 | Loss: 0.00000883
Iteration 202/1000 | Loss: 0.00000883
Iteration 203/1000 | Loss: 0.00000882
Iteration 204/1000 | Loss: 0.00000882
Iteration 205/1000 | Loss: 0.00000882
Iteration 206/1000 | Loss: 0.00000882
Iteration 207/1000 | Loss: 0.00000882
Iteration 208/1000 | Loss: 0.00000882
Iteration 209/1000 | Loss: 0.00000882
Iteration 210/1000 | Loss: 0.00000882
Iteration 211/1000 | Loss: 0.00000882
Iteration 212/1000 | Loss: 0.00000882
Iteration 213/1000 | Loss: 0.00000882
Iteration 214/1000 | Loss: 0.00000881
Iteration 215/1000 | Loss: 0.00000881
Iteration 216/1000 | Loss: 0.00000881
Iteration 217/1000 | Loss: 0.00000881
Iteration 218/1000 | Loss: 0.00000881
Iteration 219/1000 | Loss: 0.00000881
Iteration 220/1000 | Loss: 0.00000881
Iteration 221/1000 | Loss: 0.00000881
Iteration 222/1000 | Loss: 0.00000881
Iteration 223/1000 | Loss: 0.00000881
Iteration 224/1000 | Loss: 0.00000881
Iteration 225/1000 | Loss: 0.00000881
Iteration 226/1000 | Loss: 0.00000881
Iteration 227/1000 | Loss: 0.00000881
Iteration 228/1000 | Loss: 0.00000881
Iteration 229/1000 | Loss: 0.00000880
Iteration 230/1000 | Loss: 0.00000880
Iteration 231/1000 | Loss: 0.00000880
Iteration 232/1000 | Loss: 0.00000880
Iteration 233/1000 | Loss: 0.00000880
Iteration 234/1000 | Loss: 0.00000880
Iteration 235/1000 | Loss: 0.00000880
Iteration 236/1000 | Loss: 0.00000880
Iteration 237/1000 | Loss: 0.00000880
Iteration 238/1000 | Loss: 0.00000880
Iteration 239/1000 | Loss: 0.00000880
Iteration 240/1000 | Loss: 0.00000880
Iteration 241/1000 | Loss: 0.00000880
Iteration 242/1000 | Loss: 0.00000880
Iteration 243/1000 | Loss: 0.00000880
Iteration 244/1000 | Loss: 0.00000880
Iteration 245/1000 | Loss: 0.00000880
Iteration 246/1000 | Loss: 0.00000880
Iteration 247/1000 | Loss: 0.00000880
Iteration 248/1000 | Loss: 0.00000880
Iteration 249/1000 | Loss: 0.00000880
Iteration 250/1000 | Loss: 0.00000880
Iteration 251/1000 | Loss: 0.00000880
Iteration 252/1000 | Loss: 0.00000880
Iteration 253/1000 | Loss: 0.00000880
Iteration 254/1000 | Loss: 0.00000880
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 254. Stopping optimization.
Last 5 losses: [8.804247045191005e-06, 8.804247045191005e-06, 8.804247045191005e-06, 8.804247045191005e-06, 8.804247045191005e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.804247045191005e-06

Optimization complete. Final v2v error: 2.569737195968628 mm

Highest mean error: 2.771182060241699 mm for frame 89

Lowest mean error: 2.431318521499634 mm for frame 85

Saving results

Total time: 47.220699071884155
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00802122
Iteration 2/25 | Loss: 0.00130981
Iteration 3/25 | Loss: 0.00115880
Iteration 4/25 | Loss: 0.00113964
Iteration 5/25 | Loss: 0.00113543
Iteration 6/25 | Loss: 0.00113509
Iteration 7/25 | Loss: 0.00113509
Iteration 8/25 | Loss: 0.00113509
Iteration 9/25 | Loss: 0.00113509
Iteration 10/25 | Loss: 0.00113509
Iteration 11/25 | Loss: 0.00113509
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011350902495905757, 0.0011350902495905757, 0.0011350902495905757, 0.0011350902495905757, 0.0011350902495905757]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011350902495905757

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31703317
Iteration 2/25 | Loss: 0.00072840
Iteration 3/25 | Loss: 0.00072835
Iteration 4/25 | Loss: 0.00072835
Iteration 5/25 | Loss: 0.00072835
Iteration 6/25 | Loss: 0.00072835
Iteration 7/25 | Loss: 0.00072835
Iteration 8/25 | Loss: 0.00072835
Iteration 9/25 | Loss: 0.00072835
Iteration 10/25 | Loss: 0.00072835
Iteration 11/25 | Loss: 0.00072835
Iteration 12/25 | Loss: 0.00072835
Iteration 13/25 | Loss: 0.00072835
Iteration 14/25 | Loss: 0.00072835
Iteration 15/25 | Loss: 0.00072835
Iteration 16/25 | Loss: 0.00072835
Iteration 17/25 | Loss: 0.00072835
Iteration 18/25 | Loss: 0.00072835
Iteration 19/25 | Loss: 0.00072835
Iteration 20/25 | Loss: 0.00072835
Iteration 21/25 | Loss: 0.00072835
Iteration 22/25 | Loss: 0.00072835
Iteration 23/25 | Loss: 0.00072835
Iteration 24/25 | Loss: 0.00072835
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007283513550646603, 0.0007283513550646603, 0.0007283513550646603, 0.0007283513550646603, 0.0007283513550646603]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007283513550646603

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072835
Iteration 2/1000 | Loss: 0.00003564
Iteration 3/1000 | Loss: 0.00002469
Iteration 4/1000 | Loss: 0.00001910
Iteration 5/1000 | Loss: 0.00001782
Iteration 6/1000 | Loss: 0.00001688
Iteration 7/1000 | Loss: 0.00001625
Iteration 8/1000 | Loss: 0.00001569
Iteration 9/1000 | Loss: 0.00001539
Iteration 10/1000 | Loss: 0.00001512
Iteration 11/1000 | Loss: 0.00001478
Iteration 12/1000 | Loss: 0.00001461
Iteration 13/1000 | Loss: 0.00001459
Iteration 14/1000 | Loss: 0.00001458
Iteration 15/1000 | Loss: 0.00001446
Iteration 16/1000 | Loss: 0.00001446
Iteration 17/1000 | Loss: 0.00001441
Iteration 18/1000 | Loss: 0.00001437
Iteration 19/1000 | Loss: 0.00001437
Iteration 20/1000 | Loss: 0.00001437
Iteration 21/1000 | Loss: 0.00001436
Iteration 22/1000 | Loss: 0.00001435
Iteration 23/1000 | Loss: 0.00001434
Iteration 24/1000 | Loss: 0.00001433
Iteration 25/1000 | Loss: 0.00001432
Iteration 26/1000 | Loss: 0.00001431
Iteration 27/1000 | Loss: 0.00001425
Iteration 28/1000 | Loss: 0.00001424
Iteration 29/1000 | Loss: 0.00001424
Iteration 30/1000 | Loss: 0.00001421
Iteration 31/1000 | Loss: 0.00001420
Iteration 32/1000 | Loss: 0.00001420
Iteration 33/1000 | Loss: 0.00001419
Iteration 34/1000 | Loss: 0.00001418
Iteration 35/1000 | Loss: 0.00001418
Iteration 36/1000 | Loss: 0.00001418
Iteration 37/1000 | Loss: 0.00001418
Iteration 38/1000 | Loss: 0.00001418
Iteration 39/1000 | Loss: 0.00001418
Iteration 40/1000 | Loss: 0.00001418
Iteration 41/1000 | Loss: 0.00001418
Iteration 42/1000 | Loss: 0.00001418
Iteration 43/1000 | Loss: 0.00001418
Iteration 44/1000 | Loss: 0.00001417
Iteration 45/1000 | Loss: 0.00001417
Iteration 46/1000 | Loss: 0.00001417
Iteration 47/1000 | Loss: 0.00001417
Iteration 48/1000 | Loss: 0.00001417
Iteration 49/1000 | Loss: 0.00001416
Iteration 50/1000 | Loss: 0.00001416
Iteration 51/1000 | Loss: 0.00001416
Iteration 52/1000 | Loss: 0.00001414
Iteration 53/1000 | Loss: 0.00001414
Iteration 54/1000 | Loss: 0.00001413
Iteration 55/1000 | Loss: 0.00001413
Iteration 56/1000 | Loss: 0.00001413
Iteration 57/1000 | Loss: 0.00001412
Iteration 58/1000 | Loss: 0.00001412
Iteration 59/1000 | Loss: 0.00001411
Iteration 60/1000 | Loss: 0.00001411
Iteration 61/1000 | Loss: 0.00001410
Iteration 62/1000 | Loss: 0.00001410
Iteration 63/1000 | Loss: 0.00001410
Iteration 64/1000 | Loss: 0.00001409
Iteration 65/1000 | Loss: 0.00001409
Iteration 66/1000 | Loss: 0.00001408
Iteration 67/1000 | Loss: 0.00001408
Iteration 68/1000 | Loss: 0.00001408
Iteration 69/1000 | Loss: 0.00001408
Iteration 70/1000 | Loss: 0.00001408
Iteration 71/1000 | Loss: 0.00001407
Iteration 72/1000 | Loss: 0.00001407
Iteration 73/1000 | Loss: 0.00001407
Iteration 74/1000 | Loss: 0.00001406
Iteration 75/1000 | Loss: 0.00001406
Iteration 76/1000 | Loss: 0.00001406
Iteration 77/1000 | Loss: 0.00001406
Iteration 78/1000 | Loss: 0.00001406
Iteration 79/1000 | Loss: 0.00001405
Iteration 80/1000 | Loss: 0.00001405
Iteration 81/1000 | Loss: 0.00001405
Iteration 82/1000 | Loss: 0.00001405
Iteration 83/1000 | Loss: 0.00001405
Iteration 84/1000 | Loss: 0.00001405
Iteration 85/1000 | Loss: 0.00001405
Iteration 86/1000 | Loss: 0.00001405
Iteration 87/1000 | Loss: 0.00001405
Iteration 88/1000 | Loss: 0.00001404
Iteration 89/1000 | Loss: 0.00001404
Iteration 90/1000 | Loss: 0.00001404
Iteration 91/1000 | Loss: 0.00001403
Iteration 92/1000 | Loss: 0.00001403
Iteration 93/1000 | Loss: 0.00001403
Iteration 94/1000 | Loss: 0.00001403
Iteration 95/1000 | Loss: 0.00001403
Iteration 96/1000 | Loss: 0.00001403
Iteration 97/1000 | Loss: 0.00001403
Iteration 98/1000 | Loss: 0.00001403
Iteration 99/1000 | Loss: 0.00001402
Iteration 100/1000 | Loss: 0.00001402
Iteration 101/1000 | Loss: 0.00001402
Iteration 102/1000 | Loss: 0.00001402
Iteration 103/1000 | Loss: 0.00001402
Iteration 104/1000 | Loss: 0.00001402
Iteration 105/1000 | Loss: 0.00001402
Iteration 106/1000 | Loss: 0.00001401
Iteration 107/1000 | Loss: 0.00001401
Iteration 108/1000 | Loss: 0.00001401
Iteration 109/1000 | Loss: 0.00001401
Iteration 110/1000 | Loss: 0.00001401
Iteration 111/1000 | Loss: 0.00001401
Iteration 112/1000 | Loss: 0.00001401
Iteration 113/1000 | Loss: 0.00001401
Iteration 114/1000 | Loss: 0.00001400
Iteration 115/1000 | Loss: 0.00001400
Iteration 116/1000 | Loss: 0.00001400
Iteration 117/1000 | Loss: 0.00001400
Iteration 118/1000 | Loss: 0.00001400
Iteration 119/1000 | Loss: 0.00001399
Iteration 120/1000 | Loss: 0.00001399
Iteration 121/1000 | Loss: 0.00001399
Iteration 122/1000 | Loss: 0.00001399
Iteration 123/1000 | Loss: 0.00001399
Iteration 124/1000 | Loss: 0.00001398
Iteration 125/1000 | Loss: 0.00001398
Iteration 126/1000 | Loss: 0.00001398
Iteration 127/1000 | Loss: 0.00001398
Iteration 128/1000 | Loss: 0.00001397
Iteration 129/1000 | Loss: 0.00001397
Iteration 130/1000 | Loss: 0.00001397
Iteration 131/1000 | Loss: 0.00001397
Iteration 132/1000 | Loss: 0.00001397
Iteration 133/1000 | Loss: 0.00001396
Iteration 134/1000 | Loss: 0.00001396
Iteration 135/1000 | Loss: 0.00001396
Iteration 136/1000 | Loss: 0.00001396
Iteration 137/1000 | Loss: 0.00001396
Iteration 138/1000 | Loss: 0.00001396
Iteration 139/1000 | Loss: 0.00001396
Iteration 140/1000 | Loss: 0.00001396
Iteration 141/1000 | Loss: 0.00001395
Iteration 142/1000 | Loss: 0.00001395
Iteration 143/1000 | Loss: 0.00001395
Iteration 144/1000 | Loss: 0.00001395
Iteration 145/1000 | Loss: 0.00001395
Iteration 146/1000 | Loss: 0.00001395
Iteration 147/1000 | Loss: 0.00001395
Iteration 148/1000 | Loss: 0.00001395
Iteration 149/1000 | Loss: 0.00001395
Iteration 150/1000 | Loss: 0.00001394
Iteration 151/1000 | Loss: 0.00001394
Iteration 152/1000 | Loss: 0.00001394
Iteration 153/1000 | Loss: 0.00001394
Iteration 154/1000 | Loss: 0.00001394
Iteration 155/1000 | Loss: 0.00001394
Iteration 156/1000 | Loss: 0.00001394
Iteration 157/1000 | Loss: 0.00001394
Iteration 158/1000 | Loss: 0.00001393
Iteration 159/1000 | Loss: 0.00001393
Iteration 160/1000 | Loss: 0.00001393
Iteration 161/1000 | Loss: 0.00001393
Iteration 162/1000 | Loss: 0.00001393
Iteration 163/1000 | Loss: 0.00001393
Iteration 164/1000 | Loss: 0.00001393
Iteration 165/1000 | Loss: 0.00001393
Iteration 166/1000 | Loss: 0.00001393
Iteration 167/1000 | Loss: 0.00001393
Iteration 168/1000 | Loss: 0.00001393
Iteration 169/1000 | Loss: 0.00001392
Iteration 170/1000 | Loss: 0.00001392
Iteration 171/1000 | Loss: 0.00001392
Iteration 172/1000 | Loss: 0.00001391
Iteration 173/1000 | Loss: 0.00001391
Iteration 174/1000 | Loss: 0.00001391
Iteration 175/1000 | Loss: 0.00001391
Iteration 176/1000 | Loss: 0.00001391
Iteration 177/1000 | Loss: 0.00001390
Iteration 178/1000 | Loss: 0.00001390
Iteration 179/1000 | Loss: 0.00001390
Iteration 180/1000 | Loss: 0.00001390
Iteration 181/1000 | Loss: 0.00001389
Iteration 182/1000 | Loss: 0.00001389
Iteration 183/1000 | Loss: 0.00001389
Iteration 184/1000 | Loss: 0.00001389
Iteration 185/1000 | Loss: 0.00001388
Iteration 186/1000 | Loss: 0.00001388
Iteration 187/1000 | Loss: 0.00001388
Iteration 188/1000 | Loss: 0.00001388
Iteration 189/1000 | Loss: 0.00001388
Iteration 190/1000 | Loss: 0.00001388
Iteration 191/1000 | Loss: 0.00001388
Iteration 192/1000 | Loss: 0.00001387
Iteration 193/1000 | Loss: 0.00001387
Iteration 194/1000 | Loss: 0.00001387
Iteration 195/1000 | Loss: 0.00001387
Iteration 196/1000 | Loss: 0.00001387
Iteration 197/1000 | Loss: 0.00001387
Iteration 198/1000 | Loss: 0.00001387
Iteration 199/1000 | Loss: 0.00001387
Iteration 200/1000 | Loss: 0.00001386
Iteration 201/1000 | Loss: 0.00001386
Iteration 202/1000 | Loss: 0.00001386
Iteration 203/1000 | Loss: 0.00001386
Iteration 204/1000 | Loss: 0.00001386
Iteration 205/1000 | Loss: 0.00001386
Iteration 206/1000 | Loss: 0.00001386
Iteration 207/1000 | Loss: 0.00001385
Iteration 208/1000 | Loss: 0.00001385
Iteration 209/1000 | Loss: 0.00001385
Iteration 210/1000 | Loss: 0.00001385
Iteration 211/1000 | Loss: 0.00001385
Iteration 212/1000 | Loss: 0.00001385
Iteration 213/1000 | Loss: 0.00001385
Iteration 214/1000 | Loss: 0.00001385
Iteration 215/1000 | Loss: 0.00001385
Iteration 216/1000 | Loss: 0.00001385
Iteration 217/1000 | Loss: 0.00001385
Iteration 218/1000 | Loss: 0.00001385
Iteration 219/1000 | Loss: 0.00001385
Iteration 220/1000 | Loss: 0.00001385
Iteration 221/1000 | Loss: 0.00001385
Iteration 222/1000 | Loss: 0.00001385
Iteration 223/1000 | Loss: 0.00001385
Iteration 224/1000 | Loss: 0.00001385
Iteration 225/1000 | Loss: 0.00001385
Iteration 226/1000 | Loss: 0.00001385
Iteration 227/1000 | Loss: 0.00001385
Iteration 228/1000 | Loss: 0.00001385
Iteration 229/1000 | Loss: 0.00001385
Iteration 230/1000 | Loss: 0.00001385
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 230. Stopping optimization.
Last 5 losses: [1.3847180525772274e-05, 1.3847180525772274e-05, 1.3847180525772274e-05, 1.3847180525772274e-05, 1.3847180525772274e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3847180525772274e-05

Optimization complete. Final v2v error: 3.0841689109802246 mm

Highest mean error: 4.449814796447754 mm for frame 6

Lowest mean error: 2.5744385719299316 mm for frame 188

Saving results

Total time: 50.37313389778137
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00917832
Iteration 2/25 | Loss: 0.00139832
Iteration 3/25 | Loss: 0.00134215
Iteration 4/25 | Loss: 0.00114476
Iteration 5/25 | Loss: 0.00113967
Iteration 6/25 | Loss: 0.00113749
Iteration 7/25 | Loss: 0.00113647
Iteration 8/25 | Loss: 0.00114001
Iteration 9/25 | Loss: 0.00114077
Iteration 10/25 | Loss: 0.00113527
Iteration 11/25 | Loss: 0.00113322
Iteration 12/25 | Loss: 0.00113286
Iteration 13/25 | Loss: 0.00113278
Iteration 14/25 | Loss: 0.00113277
Iteration 15/25 | Loss: 0.00113277
Iteration 16/25 | Loss: 0.00113277
Iteration 17/25 | Loss: 0.00113277
Iteration 18/25 | Loss: 0.00113277
Iteration 19/25 | Loss: 0.00113277
Iteration 20/25 | Loss: 0.00113277
Iteration 21/25 | Loss: 0.00113277
Iteration 22/25 | Loss: 0.00113277
Iteration 23/25 | Loss: 0.00113277
Iteration 24/25 | Loss: 0.00113276
Iteration 25/25 | Loss: 0.00113276

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47883976
Iteration 2/25 | Loss: 0.00086435
Iteration 3/25 | Loss: 0.00079752
Iteration 4/25 | Loss: 0.00079752
Iteration 5/25 | Loss: 0.00079752
Iteration 6/25 | Loss: 0.00079752
Iteration 7/25 | Loss: 0.00079752
Iteration 8/25 | Loss: 0.00079752
Iteration 9/25 | Loss: 0.00079751
Iteration 10/25 | Loss: 0.00079751
Iteration 11/25 | Loss: 0.00079751
Iteration 12/25 | Loss: 0.00079751
Iteration 13/25 | Loss: 0.00079751
Iteration 14/25 | Loss: 0.00079751
Iteration 15/25 | Loss: 0.00079751
Iteration 16/25 | Loss: 0.00079751
Iteration 17/25 | Loss: 0.00079751
Iteration 18/25 | Loss: 0.00079751
Iteration 19/25 | Loss: 0.00079751
Iteration 20/25 | Loss: 0.00079751
Iteration 21/25 | Loss: 0.00079751
Iteration 22/25 | Loss: 0.00079751
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007975142216309905, 0.0007975142216309905, 0.0007975142216309905, 0.0007975142216309905, 0.0007975142216309905]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007975142216309905

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079751
Iteration 2/1000 | Loss: 0.00003314
Iteration 3/1000 | Loss: 0.00017726
Iteration 4/1000 | Loss: 0.00001951
Iteration 5/1000 | Loss: 0.00001832
Iteration 6/1000 | Loss: 0.00001725
Iteration 7/1000 | Loss: 0.00001660
Iteration 8/1000 | Loss: 0.00001616
Iteration 9/1000 | Loss: 0.00001583
Iteration 10/1000 | Loss: 0.00001560
Iteration 11/1000 | Loss: 0.00001551
Iteration 12/1000 | Loss: 0.00001529
Iteration 13/1000 | Loss: 0.00001504
Iteration 14/1000 | Loss: 0.00001498
Iteration 15/1000 | Loss: 0.00001493
Iteration 16/1000 | Loss: 0.00001490
Iteration 17/1000 | Loss: 0.00001490
Iteration 18/1000 | Loss: 0.00001485
Iteration 19/1000 | Loss: 0.00001482
Iteration 20/1000 | Loss: 0.00001482
Iteration 21/1000 | Loss: 0.00001481
Iteration 22/1000 | Loss: 0.00001481
Iteration 23/1000 | Loss: 0.00001479
Iteration 24/1000 | Loss: 0.00001478
Iteration 25/1000 | Loss: 0.00001476
Iteration 26/1000 | Loss: 0.00001475
Iteration 27/1000 | Loss: 0.00001475
Iteration 28/1000 | Loss: 0.00001474
Iteration 29/1000 | Loss: 0.00001474
Iteration 30/1000 | Loss: 0.00001471
Iteration 31/1000 | Loss: 0.00001471
Iteration 32/1000 | Loss: 0.00001470
Iteration 33/1000 | Loss: 0.00001469
Iteration 34/1000 | Loss: 0.00001469
Iteration 35/1000 | Loss: 0.00001469
Iteration 36/1000 | Loss: 0.00001469
Iteration 37/1000 | Loss: 0.00001468
Iteration 38/1000 | Loss: 0.00001468
Iteration 39/1000 | Loss: 0.00001468
Iteration 40/1000 | Loss: 0.00001467
Iteration 41/1000 | Loss: 0.00001467
Iteration 42/1000 | Loss: 0.00001466
Iteration 43/1000 | Loss: 0.00001466
Iteration 44/1000 | Loss: 0.00001466
Iteration 45/1000 | Loss: 0.00001466
Iteration 46/1000 | Loss: 0.00001466
Iteration 47/1000 | Loss: 0.00001466
Iteration 48/1000 | Loss: 0.00001465
Iteration 49/1000 | Loss: 0.00001465
Iteration 50/1000 | Loss: 0.00001465
Iteration 51/1000 | Loss: 0.00001464
Iteration 52/1000 | Loss: 0.00001464
Iteration 53/1000 | Loss: 0.00001463
Iteration 54/1000 | Loss: 0.00001463
Iteration 55/1000 | Loss: 0.00001462
Iteration 56/1000 | Loss: 0.00001462
Iteration 57/1000 | Loss: 0.00001462
Iteration 58/1000 | Loss: 0.00001462
Iteration 59/1000 | Loss: 0.00001461
Iteration 60/1000 | Loss: 0.00001461
Iteration 61/1000 | Loss: 0.00001461
Iteration 62/1000 | Loss: 0.00001461
Iteration 63/1000 | Loss: 0.00001461
Iteration 64/1000 | Loss: 0.00001461
Iteration 65/1000 | Loss: 0.00001461
Iteration 66/1000 | Loss: 0.00001461
Iteration 67/1000 | Loss: 0.00001461
Iteration 68/1000 | Loss: 0.00001461
Iteration 69/1000 | Loss: 0.00001460
Iteration 70/1000 | Loss: 0.00001460
Iteration 71/1000 | Loss: 0.00001460
Iteration 72/1000 | Loss: 0.00001459
Iteration 73/1000 | Loss: 0.00001459
Iteration 74/1000 | Loss: 0.00001459
Iteration 75/1000 | Loss: 0.00001459
Iteration 76/1000 | Loss: 0.00001459
Iteration 77/1000 | Loss: 0.00001459
Iteration 78/1000 | Loss: 0.00001459
Iteration 79/1000 | Loss: 0.00001459
Iteration 80/1000 | Loss: 0.00001459
Iteration 81/1000 | Loss: 0.00001459
Iteration 82/1000 | Loss: 0.00001459
Iteration 83/1000 | Loss: 0.00001458
Iteration 84/1000 | Loss: 0.00001458
Iteration 85/1000 | Loss: 0.00001458
Iteration 86/1000 | Loss: 0.00001458
Iteration 87/1000 | Loss: 0.00001458
Iteration 88/1000 | Loss: 0.00001458
Iteration 89/1000 | Loss: 0.00001458
Iteration 90/1000 | Loss: 0.00001458
Iteration 91/1000 | Loss: 0.00001457
Iteration 92/1000 | Loss: 0.00001457
Iteration 93/1000 | Loss: 0.00001457
Iteration 94/1000 | Loss: 0.00001457
Iteration 95/1000 | Loss: 0.00001457
Iteration 96/1000 | Loss: 0.00001457
Iteration 97/1000 | Loss: 0.00001456
Iteration 98/1000 | Loss: 0.00001456
Iteration 99/1000 | Loss: 0.00001456
Iteration 100/1000 | Loss: 0.00001456
Iteration 101/1000 | Loss: 0.00001455
Iteration 102/1000 | Loss: 0.00001455
Iteration 103/1000 | Loss: 0.00001455
Iteration 104/1000 | Loss: 0.00001455
Iteration 105/1000 | Loss: 0.00001455
Iteration 106/1000 | Loss: 0.00001454
Iteration 107/1000 | Loss: 0.00001454
Iteration 108/1000 | Loss: 0.00001454
Iteration 109/1000 | Loss: 0.00001454
Iteration 110/1000 | Loss: 0.00001454
Iteration 111/1000 | Loss: 0.00001454
Iteration 112/1000 | Loss: 0.00001453
Iteration 113/1000 | Loss: 0.00001453
Iteration 114/1000 | Loss: 0.00001453
Iteration 115/1000 | Loss: 0.00001453
Iteration 116/1000 | Loss: 0.00001453
Iteration 117/1000 | Loss: 0.00001453
Iteration 118/1000 | Loss: 0.00001453
Iteration 119/1000 | Loss: 0.00001453
Iteration 120/1000 | Loss: 0.00001453
Iteration 121/1000 | Loss: 0.00001453
Iteration 122/1000 | Loss: 0.00001453
Iteration 123/1000 | Loss: 0.00001453
Iteration 124/1000 | Loss: 0.00001452
Iteration 125/1000 | Loss: 0.00001452
Iteration 126/1000 | Loss: 0.00001452
Iteration 127/1000 | Loss: 0.00001452
Iteration 128/1000 | Loss: 0.00001452
Iteration 129/1000 | Loss: 0.00001452
Iteration 130/1000 | Loss: 0.00001452
Iteration 131/1000 | Loss: 0.00001452
Iteration 132/1000 | Loss: 0.00001452
Iteration 133/1000 | Loss: 0.00001451
Iteration 134/1000 | Loss: 0.00001451
Iteration 135/1000 | Loss: 0.00001451
Iteration 136/1000 | Loss: 0.00001451
Iteration 137/1000 | Loss: 0.00001451
Iteration 138/1000 | Loss: 0.00001451
Iteration 139/1000 | Loss: 0.00001451
Iteration 140/1000 | Loss: 0.00001451
Iteration 141/1000 | Loss: 0.00001451
Iteration 142/1000 | Loss: 0.00001451
Iteration 143/1000 | Loss: 0.00001451
Iteration 144/1000 | Loss: 0.00001451
Iteration 145/1000 | Loss: 0.00001451
Iteration 146/1000 | Loss: 0.00001451
Iteration 147/1000 | Loss: 0.00001450
Iteration 148/1000 | Loss: 0.00001450
Iteration 149/1000 | Loss: 0.00001450
Iteration 150/1000 | Loss: 0.00001450
Iteration 151/1000 | Loss: 0.00001450
Iteration 152/1000 | Loss: 0.00001450
Iteration 153/1000 | Loss: 0.00001450
Iteration 154/1000 | Loss: 0.00001450
Iteration 155/1000 | Loss: 0.00001450
Iteration 156/1000 | Loss: 0.00001450
Iteration 157/1000 | Loss: 0.00001450
Iteration 158/1000 | Loss: 0.00001450
Iteration 159/1000 | Loss: 0.00001450
Iteration 160/1000 | Loss: 0.00001450
Iteration 161/1000 | Loss: 0.00001450
Iteration 162/1000 | Loss: 0.00001450
Iteration 163/1000 | Loss: 0.00001450
Iteration 164/1000 | Loss: 0.00001450
Iteration 165/1000 | Loss: 0.00001450
Iteration 166/1000 | Loss: 0.00001450
Iteration 167/1000 | Loss: 0.00001450
Iteration 168/1000 | Loss: 0.00001450
Iteration 169/1000 | Loss: 0.00001450
Iteration 170/1000 | Loss: 0.00001450
Iteration 171/1000 | Loss: 0.00001450
Iteration 172/1000 | Loss: 0.00001450
Iteration 173/1000 | Loss: 0.00001450
Iteration 174/1000 | Loss: 0.00001450
Iteration 175/1000 | Loss: 0.00001450
Iteration 176/1000 | Loss: 0.00001450
Iteration 177/1000 | Loss: 0.00001450
Iteration 178/1000 | Loss: 0.00001450
Iteration 179/1000 | Loss: 0.00001450
Iteration 180/1000 | Loss: 0.00001450
Iteration 181/1000 | Loss: 0.00001450
Iteration 182/1000 | Loss: 0.00001450
Iteration 183/1000 | Loss: 0.00001450
Iteration 184/1000 | Loss: 0.00001450
Iteration 185/1000 | Loss: 0.00001450
Iteration 186/1000 | Loss: 0.00001450
Iteration 187/1000 | Loss: 0.00001450
Iteration 188/1000 | Loss: 0.00001450
Iteration 189/1000 | Loss: 0.00001450
Iteration 190/1000 | Loss: 0.00001450
Iteration 191/1000 | Loss: 0.00001450
Iteration 192/1000 | Loss: 0.00001450
Iteration 193/1000 | Loss: 0.00001450
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [1.4499429198622238e-05, 1.4499429198622238e-05, 1.4499429198622238e-05, 1.4499429198622238e-05, 1.4499429198622238e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4499429198622238e-05

Optimization complete. Final v2v error: 3.22354793548584 mm

Highest mean error: 3.9127614498138428 mm for frame 38

Lowest mean error: 2.736269474029541 mm for frame 121

Saving results

Total time: 53.93453645706177
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01028157
Iteration 2/25 | Loss: 0.00244326
Iteration 3/25 | Loss: 0.00178941
Iteration 4/25 | Loss: 0.00181400
Iteration 5/25 | Loss: 0.00169487
Iteration 6/25 | Loss: 0.00154190
Iteration 7/25 | Loss: 0.00140537
Iteration 8/25 | Loss: 0.00139887
Iteration 9/25 | Loss: 0.00132392
Iteration 10/25 | Loss: 0.00126127
Iteration 11/25 | Loss: 0.00127663
Iteration 12/25 | Loss: 0.00122140
Iteration 13/25 | Loss: 0.00120204
Iteration 14/25 | Loss: 0.00118987
Iteration 15/25 | Loss: 0.00118180
Iteration 16/25 | Loss: 0.00116916
Iteration 17/25 | Loss: 0.00116885
Iteration 18/25 | Loss: 0.00116567
Iteration 19/25 | Loss: 0.00115893
Iteration 20/25 | Loss: 0.00115472
Iteration 21/25 | Loss: 0.00115777
Iteration 22/25 | Loss: 0.00115932
Iteration 23/25 | Loss: 0.00115490
Iteration 24/25 | Loss: 0.00115190
Iteration 25/25 | Loss: 0.00116274

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38872850
Iteration 2/25 | Loss: 0.00154813
Iteration 3/25 | Loss: 0.00154662
Iteration 4/25 | Loss: 0.00154662
Iteration 5/25 | Loss: 0.00154662
Iteration 6/25 | Loss: 0.00154662
Iteration 7/25 | Loss: 0.00154662
Iteration 8/25 | Loss: 0.00154661
Iteration 9/25 | Loss: 0.00154661
Iteration 10/25 | Loss: 0.00154661
Iteration 11/25 | Loss: 0.00154661
Iteration 12/25 | Loss: 0.00154661
Iteration 13/25 | Loss: 0.00154661
Iteration 14/25 | Loss: 0.00154661
Iteration 15/25 | Loss: 0.00154661
Iteration 16/25 | Loss: 0.00154661
Iteration 17/25 | Loss: 0.00154661
Iteration 18/25 | Loss: 0.00154661
Iteration 19/25 | Loss: 0.00154661
Iteration 20/25 | Loss: 0.00154661
Iteration 21/25 | Loss: 0.00154661
Iteration 22/25 | Loss: 0.00154661
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0015466139884665608, 0.0015466139884665608, 0.0015466139884665608, 0.0015466139884665608, 0.0015466139884665608]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015466139884665608

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00154661
Iteration 2/1000 | Loss: 0.00017215
Iteration 3/1000 | Loss: 0.00052936
Iteration 4/1000 | Loss: 0.00063144
Iteration 5/1000 | Loss: 0.00010342
Iteration 6/1000 | Loss: 0.00022734
Iteration 7/1000 | Loss: 0.00028660
Iteration 8/1000 | Loss: 0.00037522
Iteration 9/1000 | Loss: 0.00026286
Iteration 10/1000 | Loss: 0.00040824
Iteration 11/1000 | Loss: 0.00033534
Iteration 12/1000 | Loss: 0.00019572
Iteration 13/1000 | Loss: 0.00043656
Iteration 14/1000 | Loss: 0.00031157
Iteration 15/1000 | Loss: 0.00023536
Iteration 16/1000 | Loss: 0.00062478
Iteration 17/1000 | Loss: 0.00035624
Iteration 18/1000 | Loss: 0.00019866
Iteration 19/1000 | Loss: 0.00029940
Iteration 20/1000 | Loss: 0.00055535
Iteration 21/1000 | Loss: 0.00056962
Iteration 22/1000 | Loss: 0.00142062
Iteration 23/1000 | Loss: 0.00060082
Iteration 24/1000 | Loss: 0.00011957
Iteration 25/1000 | Loss: 0.00013528
Iteration 26/1000 | Loss: 0.00007809
Iteration 27/1000 | Loss: 0.00052199
Iteration 28/1000 | Loss: 0.00019272
Iteration 29/1000 | Loss: 0.00044088
Iteration 30/1000 | Loss: 0.00047220
Iteration 31/1000 | Loss: 0.00014105
Iteration 32/1000 | Loss: 0.00007514
Iteration 33/1000 | Loss: 0.00004433
Iteration 34/1000 | Loss: 0.00013256
Iteration 35/1000 | Loss: 0.00013742
Iteration 36/1000 | Loss: 0.00011362
Iteration 37/1000 | Loss: 0.00004080
Iteration 38/1000 | Loss: 0.00017728
Iteration 39/1000 | Loss: 0.00003558
Iteration 40/1000 | Loss: 0.00009852
Iteration 41/1000 | Loss: 0.00003220
Iteration 42/1000 | Loss: 0.00061050
Iteration 43/1000 | Loss: 0.00020230
Iteration 44/1000 | Loss: 0.00140547
Iteration 45/1000 | Loss: 0.00093238
Iteration 46/1000 | Loss: 0.00088956
Iteration 47/1000 | Loss: 0.00035023
Iteration 48/1000 | Loss: 0.00004384
Iteration 49/1000 | Loss: 0.00057093
Iteration 50/1000 | Loss: 0.00003067
Iteration 51/1000 | Loss: 0.00011607
Iteration 52/1000 | Loss: 0.00002303
Iteration 53/1000 | Loss: 0.00005261
Iteration 54/1000 | Loss: 0.00001949
Iteration 55/1000 | Loss: 0.00001799
Iteration 56/1000 | Loss: 0.00001688
Iteration 57/1000 | Loss: 0.00022948
Iteration 58/1000 | Loss: 0.00015210
Iteration 59/1000 | Loss: 0.00001849
Iteration 60/1000 | Loss: 0.00023204
Iteration 61/1000 | Loss: 0.00002374
Iteration 62/1000 | Loss: 0.00023797
Iteration 63/1000 | Loss: 0.00003054
Iteration 64/1000 | Loss: 0.00002088
Iteration 65/1000 | Loss: 0.00001952
Iteration 66/1000 | Loss: 0.00001836
Iteration 67/1000 | Loss: 0.00001747
Iteration 68/1000 | Loss: 0.00001620
Iteration 69/1000 | Loss: 0.00001497
Iteration 70/1000 | Loss: 0.00001419
Iteration 71/1000 | Loss: 0.00001364
Iteration 72/1000 | Loss: 0.00001331
Iteration 73/1000 | Loss: 0.00001312
Iteration 74/1000 | Loss: 0.00001311
Iteration 75/1000 | Loss: 0.00001308
Iteration 76/1000 | Loss: 0.00001307
Iteration 77/1000 | Loss: 0.00001307
Iteration 78/1000 | Loss: 0.00001307
Iteration 79/1000 | Loss: 0.00001306
Iteration 80/1000 | Loss: 0.00001304
Iteration 81/1000 | Loss: 0.00001303
Iteration 82/1000 | Loss: 0.00001302
Iteration 83/1000 | Loss: 0.00001300
Iteration 84/1000 | Loss: 0.00001299
Iteration 85/1000 | Loss: 0.00001299
Iteration 86/1000 | Loss: 0.00001299
Iteration 87/1000 | Loss: 0.00001299
Iteration 88/1000 | Loss: 0.00001299
Iteration 89/1000 | Loss: 0.00001299
Iteration 90/1000 | Loss: 0.00001299
Iteration 91/1000 | Loss: 0.00001299
Iteration 92/1000 | Loss: 0.00001299
Iteration 93/1000 | Loss: 0.00001299
Iteration 94/1000 | Loss: 0.00001297
Iteration 95/1000 | Loss: 0.00001296
Iteration 96/1000 | Loss: 0.00001295
Iteration 97/1000 | Loss: 0.00001294
Iteration 98/1000 | Loss: 0.00001294
Iteration 99/1000 | Loss: 0.00001294
Iteration 100/1000 | Loss: 0.00001294
Iteration 101/1000 | Loss: 0.00001294
Iteration 102/1000 | Loss: 0.00001294
Iteration 103/1000 | Loss: 0.00001293
Iteration 104/1000 | Loss: 0.00001293
Iteration 105/1000 | Loss: 0.00001293
Iteration 106/1000 | Loss: 0.00001293
Iteration 107/1000 | Loss: 0.00001292
Iteration 108/1000 | Loss: 0.00001291
Iteration 109/1000 | Loss: 0.00001291
Iteration 110/1000 | Loss: 0.00001291
Iteration 111/1000 | Loss: 0.00001290
Iteration 112/1000 | Loss: 0.00001290
Iteration 113/1000 | Loss: 0.00001290
Iteration 114/1000 | Loss: 0.00001290
Iteration 115/1000 | Loss: 0.00001290
Iteration 116/1000 | Loss: 0.00001290
Iteration 117/1000 | Loss: 0.00001289
Iteration 118/1000 | Loss: 0.00001289
Iteration 119/1000 | Loss: 0.00001289
Iteration 120/1000 | Loss: 0.00001288
Iteration 121/1000 | Loss: 0.00001288
Iteration 122/1000 | Loss: 0.00001288
Iteration 123/1000 | Loss: 0.00001287
Iteration 124/1000 | Loss: 0.00001287
Iteration 125/1000 | Loss: 0.00001286
Iteration 126/1000 | Loss: 0.00001286
Iteration 127/1000 | Loss: 0.00001285
Iteration 128/1000 | Loss: 0.00001285
Iteration 129/1000 | Loss: 0.00001284
Iteration 130/1000 | Loss: 0.00001284
Iteration 131/1000 | Loss: 0.00001283
Iteration 132/1000 | Loss: 0.00001283
Iteration 133/1000 | Loss: 0.00001283
Iteration 134/1000 | Loss: 0.00001283
Iteration 135/1000 | Loss: 0.00001282
Iteration 136/1000 | Loss: 0.00001282
Iteration 137/1000 | Loss: 0.00001281
Iteration 138/1000 | Loss: 0.00001281
Iteration 139/1000 | Loss: 0.00001281
Iteration 140/1000 | Loss: 0.00001280
Iteration 141/1000 | Loss: 0.00001280
Iteration 142/1000 | Loss: 0.00001280
Iteration 143/1000 | Loss: 0.00001280
Iteration 144/1000 | Loss: 0.00001280
Iteration 145/1000 | Loss: 0.00001280
Iteration 146/1000 | Loss: 0.00001279
Iteration 147/1000 | Loss: 0.00001279
Iteration 148/1000 | Loss: 0.00001278
Iteration 149/1000 | Loss: 0.00001278
Iteration 150/1000 | Loss: 0.00001278
Iteration 151/1000 | Loss: 0.00001278
Iteration 152/1000 | Loss: 0.00001278
Iteration 153/1000 | Loss: 0.00001278
Iteration 154/1000 | Loss: 0.00001278
Iteration 155/1000 | Loss: 0.00001277
Iteration 156/1000 | Loss: 0.00001277
Iteration 157/1000 | Loss: 0.00001277
Iteration 158/1000 | Loss: 0.00001277
Iteration 159/1000 | Loss: 0.00001277
Iteration 160/1000 | Loss: 0.00001277
Iteration 161/1000 | Loss: 0.00001276
Iteration 162/1000 | Loss: 0.00001276
Iteration 163/1000 | Loss: 0.00001276
Iteration 164/1000 | Loss: 0.00001276
Iteration 165/1000 | Loss: 0.00001276
Iteration 166/1000 | Loss: 0.00001276
Iteration 167/1000 | Loss: 0.00001276
Iteration 168/1000 | Loss: 0.00001276
Iteration 169/1000 | Loss: 0.00001276
Iteration 170/1000 | Loss: 0.00001276
Iteration 171/1000 | Loss: 0.00001276
Iteration 172/1000 | Loss: 0.00001276
Iteration 173/1000 | Loss: 0.00001276
Iteration 174/1000 | Loss: 0.00001276
Iteration 175/1000 | Loss: 0.00001276
Iteration 176/1000 | Loss: 0.00001276
Iteration 177/1000 | Loss: 0.00001276
Iteration 178/1000 | Loss: 0.00001276
Iteration 179/1000 | Loss: 0.00001276
Iteration 180/1000 | Loss: 0.00001275
Iteration 181/1000 | Loss: 0.00001275
Iteration 182/1000 | Loss: 0.00001275
Iteration 183/1000 | Loss: 0.00001275
Iteration 184/1000 | Loss: 0.00001275
Iteration 185/1000 | Loss: 0.00001275
Iteration 186/1000 | Loss: 0.00001275
Iteration 187/1000 | Loss: 0.00001275
Iteration 188/1000 | Loss: 0.00001275
Iteration 189/1000 | Loss: 0.00001275
Iteration 190/1000 | Loss: 0.00001275
Iteration 191/1000 | Loss: 0.00001275
Iteration 192/1000 | Loss: 0.00001274
Iteration 193/1000 | Loss: 0.00001274
Iteration 194/1000 | Loss: 0.00001274
Iteration 195/1000 | Loss: 0.00001274
Iteration 196/1000 | Loss: 0.00001274
Iteration 197/1000 | Loss: 0.00001274
Iteration 198/1000 | Loss: 0.00001274
Iteration 199/1000 | Loss: 0.00001274
Iteration 200/1000 | Loss: 0.00001274
Iteration 201/1000 | Loss: 0.00001274
Iteration 202/1000 | Loss: 0.00001274
Iteration 203/1000 | Loss: 0.00001274
Iteration 204/1000 | Loss: 0.00001274
Iteration 205/1000 | Loss: 0.00001274
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [1.2743234947265591e-05, 1.2743234947265591e-05, 1.2743234947265591e-05, 1.2743234947265591e-05, 1.2743234947265591e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2743234947265591e-05

Optimization complete. Final v2v error: 3.0136704444885254 mm

Highest mean error: 4.643539905548096 mm for frame 77

Lowest mean error: 2.5339062213897705 mm for frame 112

Saving results

Total time: 156.8801567554474
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00568635
Iteration 2/25 | Loss: 0.00114949
Iteration 3/25 | Loss: 0.00107646
Iteration 4/25 | Loss: 0.00106659
Iteration 5/25 | Loss: 0.00106301
Iteration 6/25 | Loss: 0.00106223
Iteration 7/25 | Loss: 0.00106223
Iteration 8/25 | Loss: 0.00106223
Iteration 9/25 | Loss: 0.00106223
Iteration 10/25 | Loss: 0.00106223
Iteration 11/25 | Loss: 0.00106223
Iteration 12/25 | Loss: 0.00106223
Iteration 13/25 | Loss: 0.00106223
Iteration 14/25 | Loss: 0.00106223
Iteration 15/25 | Loss: 0.00106223
Iteration 16/25 | Loss: 0.00106223
Iteration 17/25 | Loss: 0.00106223
Iteration 18/25 | Loss: 0.00106223
Iteration 19/25 | Loss: 0.00106223
Iteration 20/25 | Loss: 0.00106223
Iteration 21/25 | Loss: 0.00106223
Iteration 22/25 | Loss: 0.00106223
Iteration 23/25 | Loss: 0.00106223
Iteration 24/25 | Loss: 0.00106223
Iteration 25/25 | Loss: 0.00106223

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.76581860
Iteration 2/25 | Loss: 0.00082405
Iteration 3/25 | Loss: 0.00082405
Iteration 4/25 | Loss: 0.00082405
Iteration 5/25 | Loss: 0.00082405
Iteration 6/25 | Loss: 0.00082405
Iteration 7/25 | Loss: 0.00082405
Iteration 8/25 | Loss: 0.00082405
Iteration 9/25 | Loss: 0.00082405
Iteration 10/25 | Loss: 0.00082405
Iteration 11/25 | Loss: 0.00082405
Iteration 12/25 | Loss: 0.00082405
Iteration 13/25 | Loss: 0.00082405
Iteration 14/25 | Loss: 0.00082405
Iteration 15/25 | Loss: 0.00082405
Iteration 16/25 | Loss: 0.00082405
Iteration 17/25 | Loss: 0.00082405
Iteration 18/25 | Loss: 0.00082405
Iteration 19/25 | Loss: 0.00082405
Iteration 20/25 | Loss: 0.00082405
Iteration 21/25 | Loss: 0.00082405
Iteration 22/25 | Loss: 0.00082405
Iteration 23/25 | Loss: 0.00082405
Iteration 24/25 | Loss: 0.00082405
Iteration 25/25 | Loss: 0.00082405

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082405
Iteration 2/1000 | Loss: 0.00001888
Iteration 3/1000 | Loss: 0.00001320
Iteration 4/1000 | Loss: 0.00001139
Iteration 5/1000 | Loss: 0.00001062
Iteration 6/1000 | Loss: 0.00001056
Iteration 7/1000 | Loss: 0.00001013
Iteration 8/1000 | Loss: 0.00000977
Iteration 9/1000 | Loss: 0.00000962
Iteration 10/1000 | Loss: 0.00000940
Iteration 11/1000 | Loss: 0.00000935
Iteration 12/1000 | Loss: 0.00000932
Iteration 13/1000 | Loss: 0.00000931
Iteration 14/1000 | Loss: 0.00000920
Iteration 15/1000 | Loss: 0.00000915
Iteration 16/1000 | Loss: 0.00000914
Iteration 17/1000 | Loss: 0.00000914
Iteration 18/1000 | Loss: 0.00000913
Iteration 19/1000 | Loss: 0.00000910
Iteration 20/1000 | Loss: 0.00000907
Iteration 21/1000 | Loss: 0.00000904
Iteration 22/1000 | Loss: 0.00000898
Iteration 23/1000 | Loss: 0.00000897
Iteration 24/1000 | Loss: 0.00000895
Iteration 25/1000 | Loss: 0.00000895
Iteration 26/1000 | Loss: 0.00000894
Iteration 27/1000 | Loss: 0.00000894
Iteration 28/1000 | Loss: 0.00000891
Iteration 29/1000 | Loss: 0.00000889
Iteration 30/1000 | Loss: 0.00000888
Iteration 31/1000 | Loss: 0.00000887
Iteration 32/1000 | Loss: 0.00000886
Iteration 33/1000 | Loss: 0.00000886
Iteration 34/1000 | Loss: 0.00000886
Iteration 35/1000 | Loss: 0.00000886
Iteration 36/1000 | Loss: 0.00000885
Iteration 37/1000 | Loss: 0.00000884
Iteration 38/1000 | Loss: 0.00000884
Iteration 39/1000 | Loss: 0.00000884
Iteration 40/1000 | Loss: 0.00000884
Iteration 41/1000 | Loss: 0.00000884
Iteration 42/1000 | Loss: 0.00000884
Iteration 43/1000 | Loss: 0.00000884
Iteration 44/1000 | Loss: 0.00000884
Iteration 45/1000 | Loss: 0.00000884
Iteration 46/1000 | Loss: 0.00000884
Iteration 47/1000 | Loss: 0.00000884
Iteration 48/1000 | Loss: 0.00000883
Iteration 49/1000 | Loss: 0.00000883
Iteration 50/1000 | Loss: 0.00000882
Iteration 51/1000 | Loss: 0.00000881
Iteration 52/1000 | Loss: 0.00000881
Iteration 53/1000 | Loss: 0.00000880
Iteration 54/1000 | Loss: 0.00000880
Iteration 55/1000 | Loss: 0.00000879
Iteration 56/1000 | Loss: 0.00000879
Iteration 57/1000 | Loss: 0.00000879
Iteration 58/1000 | Loss: 0.00000878
Iteration 59/1000 | Loss: 0.00000878
Iteration 60/1000 | Loss: 0.00000878
Iteration 61/1000 | Loss: 0.00000878
Iteration 62/1000 | Loss: 0.00000878
Iteration 63/1000 | Loss: 0.00000878
Iteration 64/1000 | Loss: 0.00000877
Iteration 65/1000 | Loss: 0.00000877
Iteration 66/1000 | Loss: 0.00000876
Iteration 67/1000 | Loss: 0.00000876
Iteration 68/1000 | Loss: 0.00000875
Iteration 69/1000 | Loss: 0.00000875
Iteration 70/1000 | Loss: 0.00000874
Iteration 71/1000 | Loss: 0.00000874
Iteration 72/1000 | Loss: 0.00000874
Iteration 73/1000 | Loss: 0.00000873
Iteration 74/1000 | Loss: 0.00000873
Iteration 75/1000 | Loss: 0.00000873
Iteration 76/1000 | Loss: 0.00000872
Iteration 77/1000 | Loss: 0.00000872
Iteration 78/1000 | Loss: 0.00000871
Iteration 79/1000 | Loss: 0.00000871
Iteration 80/1000 | Loss: 0.00000871
Iteration 81/1000 | Loss: 0.00000871
Iteration 82/1000 | Loss: 0.00000871
Iteration 83/1000 | Loss: 0.00000871
Iteration 84/1000 | Loss: 0.00000870
Iteration 85/1000 | Loss: 0.00000870
Iteration 86/1000 | Loss: 0.00000870
Iteration 87/1000 | Loss: 0.00000869
Iteration 88/1000 | Loss: 0.00000869
Iteration 89/1000 | Loss: 0.00000869
Iteration 90/1000 | Loss: 0.00000869
Iteration 91/1000 | Loss: 0.00000869
Iteration 92/1000 | Loss: 0.00000869
Iteration 93/1000 | Loss: 0.00000869
Iteration 94/1000 | Loss: 0.00000869
Iteration 95/1000 | Loss: 0.00000869
Iteration 96/1000 | Loss: 0.00000869
Iteration 97/1000 | Loss: 0.00000868
Iteration 98/1000 | Loss: 0.00000868
Iteration 99/1000 | Loss: 0.00000868
Iteration 100/1000 | Loss: 0.00000868
Iteration 101/1000 | Loss: 0.00000867
Iteration 102/1000 | Loss: 0.00000867
Iteration 103/1000 | Loss: 0.00000867
Iteration 104/1000 | Loss: 0.00000867
Iteration 105/1000 | Loss: 0.00000867
Iteration 106/1000 | Loss: 0.00000867
Iteration 107/1000 | Loss: 0.00000867
Iteration 108/1000 | Loss: 0.00000867
Iteration 109/1000 | Loss: 0.00000866
Iteration 110/1000 | Loss: 0.00000866
Iteration 111/1000 | Loss: 0.00000866
Iteration 112/1000 | Loss: 0.00000866
Iteration 113/1000 | Loss: 0.00000866
Iteration 114/1000 | Loss: 0.00000865
Iteration 115/1000 | Loss: 0.00000865
Iteration 116/1000 | Loss: 0.00000865
Iteration 117/1000 | Loss: 0.00000865
Iteration 118/1000 | Loss: 0.00000865
Iteration 119/1000 | Loss: 0.00000865
Iteration 120/1000 | Loss: 0.00000865
Iteration 121/1000 | Loss: 0.00000865
Iteration 122/1000 | Loss: 0.00000865
Iteration 123/1000 | Loss: 0.00000865
Iteration 124/1000 | Loss: 0.00000865
Iteration 125/1000 | Loss: 0.00000865
Iteration 126/1000 | Loss: 0.00000865
Iteration 127/1000 | Loss: 0.00000864
Iteration 128/1000 | Loss: 0.00000864
Iteration 129/1000 | Loss: 0.00000864
Iteration 130/1000 | Loss: 0.00000864
Iteration 131/1000 | Loss: 0.00000864
Iteration 132/1000 | Loss: 0.00000864
Iteration 133/1000 | Loss: 0.00000864
Iteration 134/1000 | Loss: 0.00000864
Iteration 135/1000 | Loss: 0.00000864
Iteration 136/1000 | Loss: 0.00000864
Iteration 137/1000 | Loss: 0.00000864
Iteration 138/1000 | Loss: 0.00000864
Iteration 139/1000 | Loss: 0.00000864
Iteration 140/1000 | Loss: 0.00000864
Iteration 141/1000 | Loss: 0.00000864
Iteration 142/1000 | Loss: 0.00000863
Iteration 143/1000 | Loss: 0.00000863
Iteration 144/1000 | Loss: 0.00000863
Iteration 145/1000 | Loss: 0.00000862
Iteration 146/1000 | Loss: 0.00000862
Iteration 147/1000 | Loss: 0.00000862
Iteration 148/1000 | Loss: 0.00000862
Iteration 149/1000 | Loss: 0.00000861
Iteration 150/1000 | Loss: 0.00000861
Iteration 151/1000 | Loss: 0.00000861
Iteration 152/1000 | Loss: 0.00000861
Iteration 153/1000 | Loss: 0.00000861
Iteration 154/1000 | Loss: 0.00000861
Iteration 155/1000 | Loss: 0.00000861
Iteration 156/1000 | Loss: 0.00000861
Iteration 157/1000 | Loss: 0.00000861
Iteration 158/1000 | Loss: 0.00000861
Iteration 159/1000 | Loss: 0.00000861
Iteration 160/1000 | Loss: 0.00000861
Iteration 161/1000 | Loss: 0.00000861
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [8.61211447045207e-06, 8.61211447045207e-06, 8.61211447045207e-06, 8.61211447045207e-06, 8.61211447045207e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.61211447045207e-06

Optimization complete. Final v2v error: 2.53857684135437 mm

Highest mean error: 2.9038937091827393 mm for frame 112

Lowest mean error: 2.3808305263519287 mm for frame 39

Saving results

Total time: 37.15385389328003
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00792423
Iteration 2/25 | Loss: 0.00131941
Iteration 3/25 | Loss: 0.00110111
Iteration 4/25 | Loss: 0.00108562
Iteration 5/25 | Loss: 0.00108023
Iteration 6/25 | Loss: 0.00107844
Iteration 7/25 | Loss: 0.00107832
Iteration 8/25 | Loss: 0.00107832
Iteration 9/25 | Loss: 0.00107832
Iteration 10/25 | Loss: 0.00107832
Iteration 11/25 | Loss: 0.00107832
Iteration 12/25 | Loss: 0.00107832
Iteration 13/25 | Loss: 0.00107832
Iteration 14/25 | Loss: 0.00107832
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0010783206671476364, 0.0010783206671476364, 0.0010783206671476364, 0.0010783206671476364, 0.0010783206671476364]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010783206671476364

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16088009
Iteration 2/25 | Loss: 0.00090433
Iteration 3/25 | Loss: 0.00090433
Iteration 4/25 | Loss: 0.00090433
Iteration 5/25 | Loss: 0.00090433
Iteration 6/25 | Loss: 0.00090433
Iteration 7/25 | Loss: 0.00090433
Iteration 8/25 | Loss: 0.00090433
Iteration 9/25 | Loss: 0.00090433
Iteration 10/25 | Loss: 0.00090433
Iteration 11/25 | Loss: 0.00090433
Iteration 12/25 | Loss: 0.00090433
Iteration 13/25 | Loss: 0.00090433
Iteration 14/25 | Loss: 0.00090433
Iteration 15/25 | Loss: 0.00090433
Iteration 16/25 | Loss: 0.00090433
Iteration 17/25 | Loss: 0.00090433
Iteration 18/25 | Loss: 0.00090433
Iteration 19/25 | Loss: 0.00090433
Iteration 20/25 | Loss: 0.00090433
Iteration 21/25 | Loss: 0.00090433
Iteration 22/25 | Loss: 0.00090433
Iteration 23/25 | Loss: 0.00090433
Iteration 24/25 | Loss: 0.00090433
Iteration 25/25 | Loss: 0.00090433

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090433
Iteration 2/1000 | Loss: 0.00004510
Iteration 3/1000 | Loss: 0.00002720
Iteration 4/1000 | Loss: 0.00002170
Iteration 5/1000 | Loss: 0.00001786
Iteration 6/1000 | Loss: 0.00001647
Iteration 7/1000 | Loss: 0.00001535
Iteration 8/1000 | Loss: 0.00001470
Iteration 9/1000 | Loss: 0.00001415
Iteration 10/1000 | Loss: 0.00001373
Iteration 11/1000 | Loss: 0.00001346
Iteration 12/1000 | Loss: 0.00001323
Iteration 13/1000 | Loss: 0.00001306
Iteration 14/1000 | Loss: 0.00001298
Iteration 15/1000 | Loss: 0.00001293
Iteration 16/1000 | Loss: 0.00001289
Iteration 17/1000 | Loss: 0.00001288
Iteration 18/1000 | Loss: 0.00001287
Iteration 19/1000 | Loss: 0.00001286
Iteration 20/1000 | Loss: 0.00001285
Iteration 21/1000 | Loss: 0.00001278
Iteration 22/1000 | Loss: 0.00001265
Iteration 23/1000 | Loss: 0.00001262
Iteration 24/1000 | Loss: 0.00001260
Iteration 25/1000 | Loss: 0.00001259
Iteration 26/1000 | Loss: 0.00001259
Iteration 27/1000 | Loss: 0.00001259
Iteration 28/1000 | Loss: 0.00001258
Iteration 29/1000 | Loss: 0.00001258
Iteration 30/1000 | Loss: 0.00001258
Iteration 31/1000 | Loss: 0.00001257
Iteration 32/1000 | Loss: 0.00001257
Iteration 33/1000 | Loss: 0.00001256
Iteration 34/1000 | Loss: 0.00001255
Iteration 35/1000 | Loss: 0.00001255
Iteration 36/1000 | Loss: 0.00001254
Iteration 37/1000 | Loss: 0.00001254
Iteration 38/1000 | Loss: 0.00001254
Iteration 39/1000 | Loss: 0.00001253
Iteration 40/1000 | Loss: 0.00001252
Iteration 41/1000 | Loss: 0.00001251
Iteration 42/1000 | Loss: 0.00001250
Iteration 43/1000 | Loss: 0.00001249
Iteration 44/1000 | Loss: 0.00001248
Iteration 45/1000 | Loss: 0.00001247
Iteration 46/1000 | Loss: 0.00001247
Iteration 47/1000 | Loss: 0.00001247
Iteration 48/1000 | Loss: 0.00001246
Iteration 49/1000 | Loss: 0.00001246
Iteration 50/1000 | Loss: 0.00001245
Iteration 51/1000 | Loss: 0.00001245
Iteration 52/1000 | Loss: 0.00001245
Iteration 53/1000 | Loss: 0.00001244
Iteration 54/1000 | Loss: 0.00001244
Iteration 55/1000 | Loss: 0.00001244
Iteration 56/1000 | Loss: 0.00001243
Iteration 57/1000 | Loss: 0.00001243
Iteration 58/1000 | Loss: 0.00001243
Iteration 59/1000 | Loss: 0.00001243
Iteration 60/1000 | Loss: 0.00001242
Iteration 61/1000 | Loss: 0.00001242
Iteration 62/1000 | Loss: 0.00001241
Iteration 63/1000 | Loss: 0.00001241
Iteration 64/1000 | Loss: 0.00001241
Iteration 65/1000 | Loss: 0.00001240
Iteration 66/1000 | Loss: 0.00001240
Iteration 67/1000 | Loss: 0.00001239
Iteration 68/1000 | Loss: 0.00001239
Iteration 69/1000 | Loss: 0.00001239
Iteration 70/1000 | Loss: 0.00001239
Iteration 71/1000 | Loss: 0.00001238
Iteration 72/1000 | Loss: 0.00001238
Iteration 73/1000 | Loss: 0.00001238
Iteration 74/1000 | Loss: 0.00001238
Iteration 75/1000 | Loss: 0.00001238
Iteration 76/1000 | Loss: 0.00001238
Iteration 77/1000 | Loss: 0.00001238
Iteration 78/1000 | Loss: 0.00001238
Iteration 79/1000 | Loss: 0.00001238
Iteration 80/1000 | Loss: 0.00001237
Iteration 81/1000 | Loss: 0.00001237
Iteration 82/1000 | Loss: 0.00001237
Iteration 83/1000 | Loss: 0.00001237
Iteration 84/1000 | Loss: 0.00001236
Iteration 85/1000 | Loss: 0.00001236
Iteration 86/1000 | Loss: 0.00001236
Iteration 87/1000 | Loss: 0.00001236
Iteration 88/1000 | Loss: 0.00001235
Iteration 89/1000 | Loss: 0.00001235
Iteration 90/1000 | Loss: 0.00001235
Iteration 91/1000 | Loss: 0.00001235
Iteration 92/1000 | Loss: 0.00001234
Iteration 93/1000 | Loss: 0.00001234
Iteration 94/1000 | Loss: 0.00001234
Iteration 95/1000 | Loss: 0.00001234
Iteration 96/1000 | Loss: 0.00001234
Iteration 97/1000 | Loss: 0.00001233
Iteration 98/1000 | Loss: 0.00001233
Iteration 99/1000 | Loss: 0.00001233
Iteration 100/1000 | Loss: 0.00001233
Iteration 101/1000 | Loss: 0.00001233
Iteration 102/1000 | Loss: 0.00001233
Iteration 103/1000 | Loss: 0.00001233
Iteration 104/1000 | Loss: 0.00001233
Iteration 105/1000 | Loss: 0.00001233
Iteration 106/1000 | Loss: 0.00001233
Iteration 107/1000 | Loss: 0.00001233
Iteration 108/1000 | Loss: 0.00001233
Iteration 109/1000 | Loss: 0.00001233
Iteration 110/1000 | Loss: 0.00001233
Iteration 111/1000 | Loss: 0.00001233
Iteration 112/1000 | Loss: 0.00001233
Iteration 113/1000 | Loss: 0.00001232
Iteration 114/1000 | Loss: 0.00001232
Iteration 115/1000 | Loss: 0.00001232
Iteration 116/1000 | Loss: 0.00001232
Iteration 117/1000 | Loss: 0.00001232
Iteration 118/1000 | Loss: 0.00001232
Iteration 119/1000 | Loss: 0.00001232
Iteration 120/1000 | Loss: 0.00001232
Iteration 121/1000 | Loss: 0.00001232
Iteration 122/1000 | Loss: 0.00001232
Iteration 123/1000 | Loss: 0.00001232
Iteration 124/1000 | Loss: 0.00001232
Iteration 125/1000 | Loss: 0.00001232
Iteration 126/1000 | Loss: 0.00001232
Iteration 127/1000 | Loss: 0.00001231
Iteration 128/1000 | Loss: 0.00001231
Iteration 129/1000 | Loss: 0.00001231
Iteration 130/1000 | Loss: 0.00001231
Iteration 131/1000 | Loss: 0.00001231
Iteration 132/1000 | Loss: 0.00001231
Iteration 133/1000 | Loss: 0.00001231
Iteration 134/1000 | Loss: 0.00001231
Iteration 135/1000 | Loss: 0.00001231
Iteration 136/1000 | Loss: 0.00001231
Iteration 137/1000 | Loss: 0.00001231
Iteration 138/1000 | Loss: 0.00001230
Iteration 139/1000 | Loss: 0.00001230
Iteration 140/1000 | Loss: 0.00001230
Iteration 141/1000 | Loss: 0.00001230
Iteration 142/1000 | Loss: 0.00001230
Iteration 143/1000 | Loss: 0.00001230
Iteration 144/1000 | Loss: 0.00001230
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.2304984920774586e-05, 1.2304984920774586e-05, 1.2304984920774586e-05, 1.2304984920774586e-05, 1.2304984920774586e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2304984920774586e-05

Optimization complete. Final v2v error: 2.8766117095947266 mm

Highest mean error: 4.263375282287598 mm for frame 67

Lowest mean error: 2.3287646770477295 mm for frame 102

Saving results

Total time: 40.19978046417236
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00383577
Iteration 2/25 | Loss: 0.00119592
Iteration 3/25 | Loss: 0.00108307
Iteration 4/25 | Loss: 0.00107299
Iteration 5/25 | Loss: 0.00106999
Iteration 6/25 | Loss: 0.00106896
Iteration 7/25 | Loss: 0.00106864
Iteration 8/25 | Loss: 0.00106864
Iteration 9/25 | Loss: 0.00106864
Iteration 10/25 | Loss: 0.00106864
Iteration 11/25 | Loss: 0.00106864
Iteration 12/25 | Loss: 0.00106864
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010686382884159684, 0.0010686382884159684, 0.0010686382884159684, 0.0010686382884159684, 0.0010686382884159684]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010686382884159684

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29166448
Iteration 2/25 | Loss: 0.00091546
Iteration 3/25 | Loss: 0.00091546
Iteration 4/25 | Loss: 0.00091546
Iteration 5/25 | Loss: 0.00091546
Iteration 6/25 | Loss: 0.00091546
Iteration 7/25 | Loss: 0.00091546
Iteration 8/25 | Loss: 0.00091546
Iteration 9/25 | Loss: 0.00091546
Iteration 10/25 | Loss: 0.00091546
Iteration 11/25 | Loss: 0.00091546
Iteration 12/25 | Loss: 0.00091546
Iteration 13/25 | Loss: 0.00091546
Iteration 14/25 | Loss: 0.00091546
Iteration 15/25 | Loss: 0.00091546
Iteration 16/25 | Loss: 0.00091546
Iteration 17/25 | Loss: 0.00091546
Iteration 18/25 | Loss: 0.00091546
Iteration 19/25 | Loss: 0.00091546
Iteration 20/25 | Loss: 0.00091546
Iteration 21/25 | Loss: 0.00091546
Iteration 22/25 | Loss: 0.00091546
Iteration 23/25 | Loss: 0.00091546
Iteration 24/25 | Loss: 0.00091546
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0009154552826657891, 0.0009154552826657891, 0.0009154552826657891, 0.0009154552826657891, 0.0009154552826657891]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009154552826657891

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091546
Iteration 2/1000 | Loss: 0.00003522
Iteration 3/1000 | Loss: 0.00002127
Iteration 4/1000 | Loss: 0.00001404
Iteration 5/1000 | Loss: 0.00001235
Iteration 6/1000 | Loss: 0.00001150
Iteration 7/1000 | Loss: 0.00001085
Iteration 8/1000 | Loss: 0.00001041
Iteration 9/1000 | Loss: 0.00001002
Iteration 10/1000 | Loss: 0.00000980
Iteration 11/1000 | Loss: 0.00000961
Iteration 12/1000 | Loss: 0.00000954
Iteration 13/1000 | Loss: 0.00000939
Iteration 14/1000 | Loss: 0.00000934
Iteration 15/1000 | Loss: 0.00000933
Iteration 16/1000 | Loss: 0.00000931
Iteration 17/1000 | Loss: 0.00000926
Iteration 18/1000 | Loss: 0.00000923
Iteration 19/1000 | Loss: 0.00000919
Iteration 20/1000 | Loss: 0.00000917
Iteration 21/1000 | Loss: 0.00000916
Iteration 22/1000 | Loss: 0.00000912
Iteration 23/1000 | Loss: 0.00000911
Iteration 24/1000 | Loss: 0.00000911
Iteration 25/1000 | Loss: 0.00000910
Iteration 26/1000 | Loss: 0.00000910
Iteration 27/1000 | Loss: 0.00000909
Iteration 28/1000 | Loss: 0.00000909
Iteration 29/1000 | Loss: 0.00000908
Iteration 30/1000 | Loss: 0.00000904
Iteration 31/1000 | Loss: 0.00000901
Iteration 32/1000 | Loss: 0.00000901
Iteration 33/1000 | Loss: 0.00000901
Iteration 34/1000 | Loss: 0.00000901
Iteration 35/1000 | Loss: 0.00000901
Iteration 36/1000 | Loss: 0.00000900
Iteration 37/1000 | Loss: 0.00000900
Iteration 38/1000 | Loss: 0.00000899
Iteration 39/1000 | Loss: 0.00000899
Iteration 40/1000 | Loss: 0.00000898
Iteration 41/1000 | Loss: 0.00000898
Iteration 42/1000 | Loss: 0.00000897
Iteration 43/1000 | Loss: 0.00000897
Iteration 44/1000 | Loss: 0.00000896
Iteration 45/1000 | Loss: 0.00000896
Iteration 46/1000 | Loss: 0.00000895
Iteration 47/1000 | Loss: 0.00000895
Iteration 48/1000 | Loss: 0.00000894
Iteration 49/1000 | Loss: 0.00000894
Iteration 50/1000 | Loss: 0.00000894
Iteration 51/1000 | Loss: 0.00000894
Iteration 52/1000 | Loss: 0.00000894
Iteration 53/1000 | Loss: 0.00000894
Iteration 54/1000 | Loss: 0.00000894
Iteration 55/1000 | Loss: 0.00000893
Iteration 56/1000 | Loss: 0.00000893
Iteration 57/1000 | Loss: 0.00000893
Iteration 58/1000 | Loss: 0.00000893
Iteration 59/1000 | Loss: 0.00000893
Iteration 60/1000 | Loss: 0.00000893
Iteration 61/1000 | Loss: 0.00000892
Iteration 62/1000 | Loss: 0.00000892
Iteration 63/1000 | Loss: 0.00000891
Iteration 64/1000 | Loss: 0.00000891
Iteration 65/1000 | Loss: 0.00000891
Iteration 66/1000 | Loss: 0.00000890
Iteration 67/1000 | Loss: 0.00000890
Iteration 68/1000 | Loss: 0.00000890
Iteration 69/1000 | Loss: 0.00000890
Iteration 70/1000 | Loss: 0.00000890
Iteration 71/1000 | Loss: 0.00000890
Iteration 72/1000 | Loss: 0.00000889
Iteration 73/1000 | Loss: 0.00000889
Iteration 74/1000 | Loss: 0.00000889
Iteration 75/1000 | Loss: 0.00000889
Iteration 76/1000 | Loss: 0.00000889
Iteration 77/1000 | Loss: 0.00000889
Iteration 78/1000 | Loss: 0.00000889
Iteration 79/1000 | Loss: 0.00000889
Iteration 80/1000 | Loss: 0.00000888
Iteration 81/1000 | Loss: 0.00000888
Iteration 82/1000 | Loss: 0.00000888
Iteration 83/1000 | Loss: 0.00000888
Iteration 84/1000 | Loss: 0.00000888
Iteration 85/1000 | Loss: 0.00000888
Iteration 86/1000 | Loss: 0.00000888
Iteration 87/1000 | Loss: 0.00000888
Iteration 88/1000 | Loss: 0.00000888
Iteration 89/1000 | Loss: 0.00000888
Iteration 90/1000 | Loss: 0.00000888
Iteration 91/1000 | Loss: 0.00000888
Iteration 92/1000 | Loss: 0.00000887
Iteration 93/1000 | Loss: 0.00000887
Iteration 94/1000 | Loss: 0.00000887
Iteration 95/1000 | Loss: 0.00000887
Iteration 96/1000 | Loss: 0.00000887
Iteration 97/1000 | Loss: 0.00000887
Iteration 98/1000 | Loss: 0.00000887
Iteration 99/1000 | Loss: 0.00000887
Iteration 100/1000 | Loss: 0.00000887
Iteration 101/1000 | Loss: 0.00000887
Iteration 102/1000 | Loss: 0.00000887
Iteration 103/1000 | Loss: 0.00000887
Iteration 104/1000 | Loss: 0.00000887
Iteration 105/1000 | Loss: 0.00000887
Iteration 106/1000 | Loss: 0.00000887
Iteration 107/1000 | Loss: 0.00000886
Iteration 108/1000 | Loss: 0.00000886
Iteration 109/1000 | Loss: 0.00000886
Iteration 110/1000 | Loss: 0.00000886
Iteration 111/1000 | Loss: 0.00000886
Iteration 112/1000 | Loss: 0.00000886
Iteration 113/1000 | Loss: 0.00000886
Iteration 114/1000 | Loss: 0.00000886
Iteration 115/1000 | Loss: 0.00000886
Iteration 116/1000 | Loss: 0.00000886
Iteration 117/1000 | Loss: 0.00000886
Iteration 118/1000 | Loss: 0.00000886
Iteration 119/1000 | Loss: 0.00000886
Iteration 120/1000 | Loss: 0.00000886
Iteration 121/1000 | Loss: 0.00000885
Iteration 122/1000 | Loss: 0.00000885
Iteration 123/1000 | Loss: 0.00000885
Iteration 124/1000 | Loss: 0.00000885
Iteration 125/1000 | Loss: 0.00000885
Iteration 126/1000 | Loss: 0.00000885
Iteration 127/1000 | Loss: 0.00000885
Iteration 128/1000 | Loss: 0.00000885
Iteration 129/1000 | Loss: 0.00000885
Iteration 130/1000 | Loss: 0.00000885
Iteration 131/1000 | Loss: 0.00000885
Iteration 132/1000 | Loss: 0.00000885
Iteration 133/1000 | Loss: 0.00000885
Iteration 134/1000 | Loss: 0.00000885
Iteration 135/1000 | Loss: 0.00000885
Iteration 136/1000 | Loss: 0.00000885
Iteration 137/1000 | Loss: 0.00000885
Iteration 138/1000 | Loss: 0.00000885
Iteration 139/1000 | Loss: 0.00000885
Iteration 140/1000 | Loss: 0.00000884
Iteration 141/1000 | Loss: 0.00000884
Iteration 142/1000 | Loss: 0.00000884
Iteration 143/1000 | Loss: 0.00000884
Iteration 144/1000 | Loss: 0.00000884
Iteration 145/1000 | Loss: 0.00000884
Iteration 146/1000 | Loss: 0.00000884
Iteration 147/1000 | Loss: 0.00000884
Iteration 148/1000 | Loss: 0.00000884
Iteration 149/1000 | Loss: 0.00000884
Iteration 150/1000 | Loss: 0.00000884
Iteration 151/1000 | Loss: 0.00000884
Iteration 152/1000 | Loss: 0.00000884
Iteration 153/1000 | Loss: 0.00000884
Iteration 154/1000 | Loss: 0.00000884
Iteration 155/1000 | Loss: 0.00000884
Iteration 156/1000 | Loss: 0.00000884
Iteration 157/1000 | Loss: 0.00000884
Iteration 158/1000 | Loss: 0.00000883
Iteration 159/1000 | Loss: 0.00000883
Iteration 160/1000 | Loss: 0.00000883
Iteration 161/1000 | Loss: 0.00000883
Iteration 162/1000 | Loss: 0.00000883
Iteration 163/1000 | Loss: 0.00000883
Iteration 164/1000 | Loss: 0.00000883
Iteration 165/1000 | Loss: 0.00000883
Iteration 166/1000 | Loss: 0.00000883
Iteration 167/1000 | Loss: 0.00000883
Iteration 168/1000 | Loss: 0.00000883
Iteration 169/1000 | Loss: 0.00000883
Iteration 170/1000 | Loss: 0.00000883
Iteration 171/1000 | Loss: 0.00000883
Iteration 172/1000 | Loss: 0.00000883
Iteration 173/1000 | Loss: 0.00000883
Iteration 174/1000 | Loss: 0.00000883
Iteration 175/1000 | Loss: 0.00000883
Iteration 176/1000 | Loss: 0.00000883
Iteration 177/1000 | Loss: 0.00000882
Iteration 178/1000 | Loss: 0.00000882
Iteration 179/1000 | Loss: 0.00000882
Iteration 180/1000 | Loss: 0.00000882
Iteration 181/1000 | Loss: 0.00000882
Iteration 182/1000 | Loss: 0.00000882
Iteration 183/1000 | Loss: 0.00000882
Iteration 184/1000 | Loss: 0.00000882
Iteration 185/1000 | Loss: 0.00000882
Iteration 186/1000 | Loss: 0.00000882
Iteration 187/1000 | Loss: 0.00000882
Iteration 188/1000 | Loss: 0.00000882
Iteration 189/1000 | Loss: 0.00000882
Iteration 190/1000 | Loss: 0.00000882
Iteration 191/1000 | Loss: 0.00000882
Iteration 192/1000 | Loss: 0.00000882
Iteration 193/1000 | Loss: 0.00000882
Iteration 194/1000 | Loss: 0.00000882
Iteration 195/1000 | Loss: 0.00000882
Iteration 196/1000 | Loss: 0.00000882
Iteration 197/1000 | Loss: 0.00000882
Iteration 198/1000 | Loss: 0.00000882
Iteration 199/1000 | Loss: 0.00000882
Iteration 200/1000 | Loss: 0.00000882
Iteration 201/1000 | Loss: 0.00000881
Iteration 202/1000 | Loss: 0.00000881
Iteration 203/1000 | Loss: 0.00000881
Iteration 204/1000 | Loss: 0.00000881
Iteration 205/1000 | Loss: 0.00000881
Iteration 206/1000 | Loss: 0.00000881
Iteration 207/1000 | Loss: 0.00000881
Iteration 208/1000 | Loss: 0.00000881
Iteration 209/1000 | Loss: 0.00000881
Iteration 210/1000 | Loss: 0.00000881
Iteration 211/1000 | Loss: 0.00000881
Iteration 212/1000 | Loss: 0.00000881
Iteration 213/1000 | Loss: 0.00000881
Iteration 214/1000 | Loss: 0.00000881
Iteration 215/1000 | Loss: 0.00000881
Iteration 216/1000 | Loss: 0.00000881
Iteration 217/1000 | Loss: 0.00000881
Iteration 218/1000 | Loss: 0.00000881
Iteration 219/1000 | Loss: 0.00000881
Iteration 220/1000 | Loss: 0.00000881
Iteration 221/1000 | Loss: 0.00000880
Iteration 222/1000 | Loss: 0.00000880
Iteration 223/1000 | Loss: 0.00000880
Iteration 224/1000 | Loss: 0.00000880
Iteration 225/1000 | Loss: 0.00000880
Iteration 226/1000 | Loss: 0.00000880
Iteration 227/1000 | Loss: 0.00000880
Iteration 228/1000 | Loss: 0.00000880
Iteration 229/1000 | Loss: 0.00000880
Iteration 230/1000 | Loss: 0.00000879
Iteration 231/1000 | Loss: 0.00000879
Iteration 232/1000 | Loss: 0.00000879
Iteration 233/1000 | Loss: 0.00000879
Iteration 234/1000 | Loss: 0.00000879
Iteration 235/1000 | Loss: 0.00000879
Iteration 236/1000 | Loss: 0.00000879
Iteration 237/1000 | Loss: 0.00000879
Iteration 238/1000 | Loss: 0.00000879
Iteration 239/1000 | Loss: 0.00000879
Iteration 240/1000 | Loss: 0.00000879
Iteration 241/1000 | Loss: 0.00000879
Iteration 242/1000 | Loss: 0.00000879
Iteration 243/1000 | Loss: 0.00000879
Iteration 244/1000 | Loss: 0.00000879
Iteration 245/1000 | Loss: 0.00000879
Iteration 246/1000 | Loss: 0.00000879
Iteration 247/1000 | Loss: 0.00000879
Iteration 248/1000 | Loss: 0.00000879
Iteration 249/1000 | Loss: 0.00000879
Iteration 250/1000 | Loss: 0.00000879
Iteration 251/1000 | Loss: 0.00000878
Iteration 252/1000 | Loss: 0.00000878
Iteration 253/1000 | Loss: 0.00000878
Iteration 254/1000 | Loss: 0.00000878
Iteration 255/1000 | Loss: 0.00000878
Iteration 256/1000 | Loss: 0.00000878
Iteration 257/1000 | Loss: 0.00000878
Iteration 258/1000 | Loss: 0.00000878
Iteration 259/1000 | Loss: 0.00000878
Iteration 260/1000 | Loss: 0.00000878
Iteration 261/1000 | Loss: 0.00000877
Iteration 262/1000 | Loss: 0.00000877
Iteration 263/1000 | Loss: 0.00000877
Iteration 264/1000 | Loss: 0.00000877
Iteration 265/1000 | Loss: 0.00000877
Iteration 266/1000 | Loss: 0.00000877
Iteration 267/1000 | Loss: 0.00000877
Iteration 268/1000 | Loss: 0.00000877
Iteration 269/1000 | Loss: 0.00000877
Iteration 270/1000 | Loss: 0.00000877
Iteration 271/1000 | Loss: 0.00000877
Iteration 272/1000 | Loss: 0.00000877
Iteration 273/1000 | Loss: 0.00000877
Iteration 274/1000 | Loss: 0.00000877
Iteration 275/1000 | Loss: 0.00000877
Iteration 276/1000 | Loss: 0.00000877
Iteration 277/1000 | Loss: 0.00000876
Iteration 278/1000 | Loss: 0.00000876
Iteration 279/1000 | Loss: 0.00000876
Iteration 280/1000 | Loss: 0.00000876
Iteration 281/1000 | Loss: 0.00000876
Iteration 282/1000 | Loss: 0.00000876
Iteration 283/1000 | Loss: 0.00000876
Iteration 284/1000 | Loss: 0.00000876
Iteration 285/1000 | Loss: 0.00000876
Iteration 286/1000 | Loss: 0.00000876
Iteration 287/1000 | Loss: 0.00000876
Iteration 288/1000 | Loss: 0.00000876
Iteration 289/1000 | Loss: 0.00000876
Iteration 290/1000 | Loss: 0.00000876
Iteration 291/1000 | Loss: 0.00000876
Iteration 292/1000 | Loss: 0.00000876
Iteration 293/1000 | Loss: 0.00000876
Iteration 294/1000 | Loss: 0.00000876
Iteration 295/1000 | Loss: 0.00000876
Iteration 296/1000 | Loss: 0.00000876
Iteration 297/1000 | Loss: 0.00000876
Iteration 298/1000 | Loss: 0.00000876
Iteration 299/1000 | Loss: 0.00000876
Iteration 300/1000 | Loss: 0.00000876
Iteration 301/1000 | Loss: 0.00000876
Iteration 302/1000 | Loss: 0.00000875
Iteration 303/1000 | Loss: 0.00000875
Iteration 304/1000 | Loss: 0.00000875
Iteration 305/1000 | Loss: 0.00000875
Iteration 306/1000 | Loss: 0.00000875
Iteration 307/1000 | Loss: 0.00000875
Iteration 308/1000 | Loss: 0.00000875
Iteration 309/1000 | Loss: 0.00000875
Iteration 310/1000 | Loss: 0.00000875
Iteration 311/1000 | Loss: 0.00000875
Iteration 312/1000 | Loss: 0.00000875
Iteration 313/1000 | Loss: 0.00000875
Iteration 314/1000 | Loss: 0.00000875
Iteration 315/1000 | Loss: 0.00000875
Iteration 316/1000 | Loss: 0.00000875
Iteration 317/1000 | Loss: 0.00000875
Iteration 318/1000 | Loss: 0.00000875
Iteration 319/1000 | Loss: 0.00000875
Iteration 320/1000 | Loss: 0.00000875
Iteration 321/1000 | Loss: 0.00000875
Iteration 322/1000 | Loss: 0.00000875
Iteration 323/1000 | Loss: 0.00000875
Iteration 324/1000 | Loss: 0.00000875
Iteration 325/1000 | Loss: 0.00000875
Iteration 326/1000 | Loss: 0.00000875
Iteration 327/1000 | Loss: 0.00000875
Iteration 328/1000 | Loss: 0.00000875
Iteration 329/1000 | Loss: 0.00000875
Iteration 330/1000 | Loss: 0.00000875
Iteration 331/1000 | Loss: 0.00000875
Iteration 332/1000 | Loss: 0.00000875
Iteration 333/1000 | Loss: 0.00000875
Iteration 334/1000 | Loss: 0.00000875
Iteration 335/1000 | Loss: 0.00000875
Iteration 336/1000 | Loss: 0.00000875
Iteration 337/1000 | Loss: 0.00000875
Iteration 338/1000 | Loss: 0.00000875
Iteration 339/1000 | Loss: 0.00000875
Iteration 340/1000 | Loss: 0.00000875
Iteration 341/1000 | Loss: 0.00000875
Iteration 342/1000 | Loss: 0.00000875
Iteration 343/1000 | Loss: 0.00000875
Iteration 344/1000 | Loss: 0.00000875
Iteration 345/1000 | Loss: 0.00000875
Iteration 346/1000 | Loss: 0.00000875
Iteration 347/1000 | Loss: 0.00000875
Iteration 348/1000 | Loss: 0.00000875
Iteration 349/1000 | Loss: 0.00000875
Iteration 350/1000 | Loss: 0.00000875
Iteration 351/1000 | Loss: 0.00000875
Iteration 352/1000 | Loss: 0.00000875
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 352. Stopping optimization.
Last 5 losses: [8.745300874579698e-06, 8.745300874579698e-06, 8.745300874579698e-06, 8.745300874579698e-06, 8.745300874579698e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.745300874579698e-06

Optimization complete. Final v2v error: 2.5284273624420166 mm

Highest mean error: 3.2035024166107178 mm for frame 104

Lowest mean error: 2.2146754264831543 mm for frame 13

Saving results

Total time: 47.440099477767944
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00398934
Iteration 2/25 | Loss: 0.00112998
Iteration 3/25 | Loss: 0.00107451
Iteration 4/25 | Loss: 0.00106581
Iteration 5/25 | Loss: 0.00106273
Iteration 6/25 | Loss: 0.00106209
Iteration 7/25 | Loss: 0.00106209
Iteration 8/25 | Loss: 0.00106209
Iteration 9/25 | Loss: 0.00106209
Iteration 10/25 | Loss: 0.00106209
Iteration 11/25 | Loss: 0.00106209
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010620865505188704, 0.0010620865505188704, 0.0010620865505188704, 0.0010620865505188704, 0.0010620865505188704]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010620865505188704

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.83125162
Iteration 2/25 | Loss: 0.00084347
Iteration 3/25 | Loss: 0.00084346
Iteration 4/25 | Loss: 0.00084346
Iteration 5/25 | Loss: 0.00084346
Iteration 6/25 | Loss: 0.00084346
Iteration 7/25 | Loss: 0.00084346
Iteration 8/25 | Loss: 0.00084346
Iteration 9/25 | Loss: 0.00084346
Iteration 10/25 | Loss: 0.00084346
Iteration 11/25 | Loss: 0.00084346
Iteration 12/25 | Loss: 0.00084346
Iteration 13/25 | Loss: 0.00084346
Iteration 14/25 | Loss: 0.00084346
Iteration 15/25 | Loss: 0.00084346
Iteration 16/25 | Loss: 0.00084346
Iteration 17/25 | Loss: 0.00084346
Iteration 18/25 | Loss: 0.00084346
Iteration 19/25 | Loss: 0.00084346
Iteration 20/25 | Loss: 0.00084346
Iteration 21/25 | Loss: 0.00084346
Iteration 22/25 | Loss: 0.00084346
Iteration 23/25 | Loss: 0.00084346
Iteration 24/25 | Loss: 0.00084346
Iteration 25/25 | Loss: 0.00084346

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084346
Iteration 2/1000 | Loss: 0.00002193
Iteration 3/1000 | Loss: 0.00001521
Iteration 4/1000 | Loss: 0.00001280
Iteration 5/1000 | Loss: 0.00001188
Iteration 6/1000 | Loss: 0.00001137
Iteration 7/1000 | Loss: 0.00001099
Iteration 8/1000 | Loss: 0.00001066
Iteration 9/1000 | Loss: 0.00001053
Iteration 10/1000 | Loss: 0.00001045
Iteration 11/1000 | Loss: 0.00001027
Iteration 12/1000 | Loss: 0.00001027
Iteration 13/1000 | Loss: 0.00001012
Iteration 14/1000 | Loss: 0.00001011
Iteration 15/1000 | Loss: 0.00001011
Iteration 16/1000 | Loss: 0.00001009
Iteration 17/1000 | Loss: 0.00001009
Iteration 18/1000 | Loss: 0.00001006
Iteration 19/1000 | Loss: 0.00001006
Iteration 20/1000 | Loss: 0.00001005
Iteration 21/1000 | Loss: 0.00001004
Iteration 22/1000 | Loss: 0.00001004
Iteration 23/1000 | Loss: 0.00001003
Iteration 24/1000 | Loss: 0.00001003
Iteration 25/1000 | Loss: 0.00001000
Iteration 26/1000 | Loss: 0.00000999
Iteration 27/1000 | Loss: 0.00000998
Iteration 28/1000 | Loss: 0.00000997
Iteration 29/1000 | Loss: 0.00000996
Iteration 30/1000 | Loss: 0.00000992
Iteration 31/1000 | Loss: 0.00000992
Iteration 32/1000 | Loss: 0.00000992
Iteration 33/1000 | Loss: 0.00000992
Iteration 34/1000 | Loss: 0.00000992
Iteration 35/1000 | Loss: 0.00000990
Iteration 36/1000 | Loss: 0.00000990
Iteration 37/1000 | Loss: 0.00000989
Iteration 38/1000 | Loss: 0.00000988
Iteration 39/1000 | Loss: 0.00000988
Iteration 40/1000 | Loss: 0.00000988
Iteration 41/1000 | Loss: 0.00000988
Iteration 42/1000 | Loss: 0.00000987
Iteration 43/1000 | Loss: 0.00000987
Iteration 44/1000 | Loss: 0.00000987
Iteration 45/1000 | Loss: 0.00000987
Iteration 46/1000 | Loss: 0.00000987
Iteration 47/1000 | Loss: 0.00000987
Iteration 48/1000 | Loss: 0.00000986
Iteration 49/1000 | Loss: 0.00000985
Iteration 50/1000 | Loss: 0.00000985
Iteration 51/1000 | Loss: 0.00000984
Iteration 52/1000 | Loss: 0.00000984
Iteration 53/1000 | Loss: 0.00000984
Iteration 54/1000 | Loss: 0.00000983
Iteration 55/1000 | Loss: 0.00000983
Iteration 56/1000 | Loss: 0.00000983
Iteration 57/1000 | Loss: 0.00000983
Iteration 58/1000 | Loss: 0.00000982
Iteration 59/1000 | Loss: 0.00000982
Iteration 60/1000 | Loss: 0.00000981
Iteration 61/1000 | Loss: 0.00000981
Iteration 62/1000 | Loss: 0.00000981
Iteration 63/1000 | Loss: 0.00000981
Iteration 64/1000 | Loss: 0.00000980
Iteration 65/1000 | Loss: 0.00000980
Iteration 66/1000 | Loss: 0.00000980
Iteration 67/1000 | Loss: 0.00000979
Iteration 68/1000 | Loss: 0.00000979
Iteration 69/1000 | Loss: 0.00000979
Iteration 70/1000 | Loss: 0.00000978
Iteration 71/1000 | Loss: 0.00000978
Iteration 72/1000 | Loss: 0.00000978
Iteration 73/1000 | Loss: 0.00000977
Iteration 74/1000 | Loss: 0.00000977
Iteration 75/1000 | Loss: 0.00000977
Iteration 76/1000 | Loss: 0.00000976
Iteration 77/1000 | Loss: 0.00000976
Iteration 78/1000 | Loss: 0.00000976
Iteration 79/1000 | Loss: 0.00000975
Iteration 80/1000 | Loss: 0.00000975
Iteration 81/1000 | Loss: 0.00000975
Iteration 82/1000 | Loss: 0.00000974
Iteration 83/1000 | Loss: 0.00000974
Iteration 84/1000 | Loss: 0.00000974
Iteration 85/1000 | Loss: 0.00000973
Iteration 86/1000 | Loss: 0.00000973
Iteration 87/1000 | Loss: 0.00000973
Iteration 88/1000 | Loss: 0.00000973
Iteration 89/1000 | Loss: 0.00000972
Iteration 90/1000 | Loss: 0.00000972
Iteration 91/1000 | Loss: 0.00000971
Iteration 92/1000 | Loss: 0.00000971
Iteration 93/1000 | Loss: 0.00000971
Iteration 94/1000 | Loss: 0.00000971
Iteration 95/1000 | Loss: 0.00000971
Iteration 96/1000 | Loss: 0.00000970
Iteration 97/1000 | Loss: 0.00000970
Iteration 98/1000 | Loss: 0.00000970
Iteration 99/1000 | Loss: 0.00000970
Iteration 100/1000 | Loss: 0.00000970
Iteration 101/1000 | Loss: 0.00000970
Iteration 102/1000 | Loss: 0.00000970
Iteration 103/1000 | Loss: 0.00000970
Iteration 104/1000 | Loss: 0.00000969
Iteration 105/1000 | Loss: 0.00000969
Iteration 106/1000 | Loss: 0.00000969
Iteration 107/1000 | Loss: 0.00000968
Iteration 108/1000 | Loss: 0.00000968
Iteration 109/1000 | Loss: 0.00000968
Iteration 110/1000 | Loss: 0.00000967
Iteration 111/1000 | Loss: 0.00000967
Iteration 112/1000 | Loss: 0.00000966
Iteration 113/1000 | Loss: 0.00000966
Iteration 114/1000 | Loss: 0.00000966
Iteration 115/1000 | Loss: 0.00000966
Iteration 116/1000 | Loss: 0.00000966
Iteration 117/1000 | Loss: 0.00000966
Iteration 118/1000 | Loss: 0.00000966
Iteration 119/1000 | Loss: 0.00000965
Iteration 120/1000 | Loss: 0.00000965
Iteration 121/1000 | Loss: 0.00000965
Iteration 122/1000 | Loss: 0.00000965
Iteration 123/1000 | Loss: 0.00000965
Iteration 124/1000 | Loss: 0.00000964
Iteration 125/1000 | Loss: 0.00000964
Iteration 126/1000 | Loss: 0.00000963
Iteration 127/1000 | Loss: 0.00000963
Iteration 128/1000 | Loss: 0.00000963
Iteration 129/1000 | Loss: 0.00000963
Iteration 130/1000 | Loss: 0.00000963
Iteration 131/1000 | Loss: 0.00000962
Iteration 132/1000 | Loss: 0.00000962
Iteration 133/1000 | Loss: 0.00000962
Iteration 134/1000 | Loss: 0.00000962
Iteration 135/1000 | Loss: 0.00000961
Iteration 136/1000 | Loss: 0.00000961
Iteration 137/1000 | Loss: 0.00000961
Iteration 138/1000 | Loss: 0.00000961
Iteration 139/1000 | Loss: 0.00000961
Iteration 140/1000 | Loss: 0.00000961
Iteration 141/1000 | Loss: 0.00000961
Iteration 142/1000 | Loss: 0.00000961
Iteration 143/1000 | Loss: 0.00000960
Iteration 144/1000 | Loss: 0.00000960
Iteration 145/1000 | Loss: 0.00000960
Iteration 146/1000 | Loss: 0.00000960
Iteration 147/1000 | Loss: 0.00000960
Iteration 148/1000 | Loss: 0.00000960
Iteration 149/1000 | Loss: 0.00000959
Iteration 150/1000 | Loss: 0.00000959
Iteration 151/1000 | Loss: 0.00000959
Iteration 152/1000 | Loss: 0.00000959
Iteration 153/1000 | Loss: 0.00000959
Iteration 154/1000 | Loss: 0.00000959
Iteration 155/1000 | Loss: 0.00000959
Iteration 156/1000 | Loss: 0.00000959
Iteration 157/1000 | Loss: 0.00000958
Iteration 158/1000 | Loss: 0.00000958
Iteration 159/1000 | Loss: 0.00000958
Iteration 160/1000 | Loss: 0.00000958
Iteration 161/1000 | Loss: 0.00000957
Iteration 162/1000 | Loss: 0.00000957
Iteration 163/1000 | Loss: 0.00000957
Iteration 164/1000 | Loss: 0.00000957
Iteration 165/1000 | Loss: 0.00000957
Iteration 166/1000 | Loss: 0.00000957
Iteration 167/1000 | Loss: 0.00000957
Iteration 168/1000 | Loss: 0.00000956
Iteration 169/1000 | Loss: 0.00000956
Iteration 170/1000 | Loss: 0.00000956
Iteration 171/1000 | Loss: 0.00000956
Iteration 172/1000 | Loss: 0.00000956
Iteration 173/1000 | Loss: 0.00000956
Iteration 174/1000 | Loss: 0.00000956
Iteration 175/1000 | Loss: 0.00000956
Iteration 176/1000 | Loss: 0.00000956
Iteration 177/1000 | Loss: 0.00000956
Iteration 178/1000 | Loss: 0.00000956
Iteration 179/1000 | Loss: 0.00000956
Iteration 180/1000 | Loss: 0.00000956
Iteration 181/1000 | Loss: 0.00000956
Iteration 182/1000 | Loss: 0.00000956
Iteration 183/1000 | Loss: 0.00000956
Iteration 184/1000 | Loss: 0.00000956
Iteration 185/1000 | Loss: 0.00000956
Iteration 186/1000 | Loss: 0.00000956
Iteration 187/1000 | Loss: 0.00000956
Iteration 188/1000 | Loss: 0.00000956
Iteration 189/1000 | Loss: 0.00000956
Iteration 190/1000 | Loss: 0.00000956
Iteration 191/1000 | Loss: 0.00000956
Iteration 192/1000 | Loss: 0.00000955
Iteration 193/1000 | Loss: 0.00000955
Iteration 194/1000 | Loss: 0.00000955
Iteration 195/1000 | Loss: 0.00000955
Iteration 196/1000 | Loss: 0.00000955
Iteration 197/1000 | Loss: 0.00000955
Iteration 198/1000 | Loss: 0.00000955
Iteration 199/1000 | Loss: 0.00000955
Iteration 200/1000 | Loss: 0.00000955
Iteration 201/1000 | Loss: 0.00000955
Iteration 202/1000 | Loss: 0.00000955
Iteration 203/1000 | Loss: 0.00000955
Iteration 204/1000 | Loss: 0.00000955
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [9.554524694976863e-06, 9.554524694976863e-06, 9.554524694976863e-06, 9.554524694976863e-06, 9.554524694976863e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.554524694976863e-06

Optimization complete. Final v2v error: 2.660142183303833 mm

Highest mean error: 2.990072250366211 mm for frame 81

Lowest mean error: 2.4333274364471436 mm for frame 27

Saving results

Total time: 38.722445249557495
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01008012
Iteration 2/25 | Loss: 0.00258740
Iteration 3/25 | Loss: 0.00208289
Iteration 4/25 | Loss: 0.00182799
Iteration 5/25 | Loss: 0.00183628
Iteration 6/25 | Loss: 0.00174491
Iteration 7/25 | Loss: 0.00161394
Iteration 8/25 | Loss: 0.00153230
Iteration 9/25 | Loss: 0.00146575
Iteration 10/25 | Loss: 0.00142317
Iteration 11/25 | Loss: 0.00139378
Iteration 12/25 | Loss: 0.00138134
Iteration 13/25 | Loss: 0.00137000
Iteration 14/25 | Loss: 0.00136903
Iteration 15/25 | Loss: 0.00136644
Iteration 16/25 | Loss: 0.00135810
Iteration 17/25 | Loss: 0.00135110
Iteration 18/25 | Loss: 0.00134995
Iteration 19/25 | Loss: 0.00135148
Iteration 20/25 | Loss: 0.00135131
Iteration 21/25 | Loss: 0.00135240
Iteration 22/25 | Loss: 0.00135005
Iteration 23/25 | Loss: 0.00134854
Iteration 24/25 | Loss: 0.00134606
Iteration 25/25 | Loss: 0.00134512

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38805389
Iteration 2/25 | Loss: 0.00234336
Iteration 3/25 | Loss: 0.00234336
Iteration 4/25 | Loss: 0.00234336
Iteration 5/25 | Loss: 0.00234336
Iteration 6/25 | Loss: 0.00234336
Iteration 7/25 | Loss: 0.00234336
Iteration 8/25 | Loss: 0.00234336
Iteration 9/25 | Loss: 0.00234336
Iteration 10/25 | Loss: 0.00234336
Iteration 11/25 | Loss: 0.00234336
Iteration 12/25 | Loss: 0.00234336
Iteration 13/25 | Loss: 0.00234336
Iteration 14/25 | Loss: 0.00234336
Iteration 15/25 | Loss: 0.00234336
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.002343356842175126, 0.002343356842175126, 0.002343356842175126, 0.002343356842175126, 0.002343356842175126]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002343356842175126

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00234336
Iteration 2/1000 | Loss: 0.00031676
Iteration 3/1000 | Loss: 0.00033954
Iteration 4/1000 | Loss: 0.00021682
Iteration 5/1000 | Loss: 0.00016354
Iteration 6/1000 | Loss: 0.00017081
Iteration 7/1000 | Loss: 0.00028861
Iteration 8/1000 | Loss: 0.00020673
Iteration 9/1000 | Loss: 0.00013995
Iteration 10/1000 | Loss: 0.00027675
Iteration 11/1000 | Loss: 0.00034136
Iteration 12/1000 | Loss: 0.00133649
Iteration 13/1000 | Loss: 0.00034523
Iteration 14/1000 | Loss: 0.00049378
Iteration 15/1000 | Loss: 0.00013985
Iteration 16/1000 | Loss: 0.00027934
Iteration 17/1000 | Loss: 0.00020585
Iteration 18/1000 | Loss: 0.00012603
Iteration 19/1000 | Loss: 0.00035945
Iteration 20/1000 | Loss: 0.00034506
Iteration 21/1000 | Loss: 0.00043236
Iteration 22/1000 | Loss: 0.00045196
Iteration 23/1000 | Loss: 0.00051076
Iteration 24/1000 | Loss: 0.00055144
Iteration 25/1000 | Loss: 0.00022199
Iteration 26/1000 | Loss: 0.00037755
Iteration 27/1000 | Loss: 0.00022983
Iteration 28/1000 | Loss: 0.00038374
Iteration 29/1000 | Loss: 0.00028406
Iteration 30/1000 | Loss: 0.00021979
Iteration 31/1000 | Loss: 0.00036664
Iteration 32/1000 | Loss: 0.00035850
Iteration 33/1000 | Loss: 0.00029174
Iteration 34/1000 | Loss: 0.00038464
Iteration 35/1000 | Loss: 0.00035824
Iteration 36/1000 | Loss: 0.00031426
Iteration 37/1000 | Loss: 0.00016485
Iteration 38/1000 | Loss: 0.00027734
Iteration 39/1000 | Loss: 0.00016515
Iteration 40/1000 | Loss: 0.00022697
Iteration 41/1000 | Loss: 0.00024750
Iteration 42/1000 | Loss: 0.00024386
Iteration 43/1000 | Loss: 0.00013162
Iteration 44/1000 | Loss: 0.00021299
Iteration 45/1000 | Loss: 0.00012819
Iteration 46/1000 | Loss: 0.00012844
Iteration 47/1000 | Loss: 0.00024063
Iteration 48/1000 | Loss: 0.00025793
Iteration 49/1000 | Loss: 0.00087332
Iteration 50/1000 | Loss: 0.00046396
Iteration 51/1000 | Loss: 0.00061994
Iteration 52/1000 | Loss: 0.00096032
Iteration 53/1000 | Loss: 0.00036335
Iteration 54/1000 | Loss: 0.00024194
Iteration 55/1000 | Loss: 0.00023225
Iteration 56/1000 | Loss: 0.00022396
Iteration 57/1000 | Loss: 0.00030332
Iteration 58/1000 | Loss: 0.00020569
Iteration 59/1000 | Loss: 0.00025649
Iteration 60/1000 | Loss: 0.00020237
Iteration 61/1000 | Loss: 0.00016872
Iteration 62/1000 | Loss: 0.00024701
Iteration 63/1000 | Loss: 0.00024651
Iteration 64/1000 | Loss: 0.00014983
Iteration 65/1000 | Loss: 0.00015434
Iteration 66/1000 | Loss: 0.00016005
Iteration 67/1000 | Loss: 0.00011588
Iteration 68/1000 | Loss: 0.00011202
Iteration 69/1000 | Loss: 0.00011104
Iteration 70/1000 | Loss: 0.00009979
Iteration 71/1000 | Loss: 0.00010376
Iteration 72/1000 | Loss: 0.00102935
Iteration 73/1000 | Loss: 0.00067608
Iteration 74/1000 | Loss: 0.00019533
Iteration 75/1000 | Loss: 0.00012185
Iteration 76/1000 | Loss: 0.00012318
Iteration 77/1000 | Loss: 0.00011180
Iteration 78/1000 | Loss: 0.00013961
Iteration 79/1000 | Loss: 0.00021116
Iteration 80/1000 | Loss: 0.00021732
Iteration 81/1000 | Loss: 0.00049691
Iteration 82/1000 | Loss: 0.00010750
Iteration 83/1000 | Loss: 0.00010507
Iteration 84/1000 | Loss: 0.00023618
Iteration 85/1000 | Loss: 0.00051454
Iteration 86/1000 | Loss: 0.00035016
Iteration 87/1000 | Loss: 0.00064114
Iteration 88/1000 | Loss: 0.00012787
Iteration 89/1000 | Loss: 0.00027175
Iteration 90/1000 | Loss: 0.00023550
Iteration 91/1000 | Loss: 0.00009973
Iteration 92/1000 | Loss: 0.00010092
Iteration 93/1000 | Loss: 0.00009467
Iteration 94/1000 | Loss: 0.00009585
Iteration 95/1000 | Loss: 0.00036556
Iteration 96/1000 | Loss: 0.00009459
Iteration 97/1000 | Loss: 0.00016394
Iteration 98/1000 | Loss: 0.00052988
Iteration 99/1000 | Loss: 0.00077351
Iteration 100/1000 | Loss: 0.00024756
Iteration 101/1000 | Loss: 0.00021289
Iteration 102/1000 | Loss: 0.00012347
Iteration 103/1000 | Loss: 0.00008906
Iteration 104/1000 | Loss: 0.00016768
Iteration 105/1000 | Loss: 0.00007408
Iteration 106/1000 | Loss: 0.00008045
Iteration 107/1000 | Loss: 0.00019596
Iteration 108/1000 | Loss: 0.00020582
Iteration 109/1000 | Loss: 0.00018692
Iteration 110/1000 | Loss: 0.00014288
Iteration 111/1000 | Loss: 0.00071350
Iteration 112/1000 | Loss: 0.00023109
Iteration 113/1000 | Loss: 0.00018896
Iteration 114/1000 | Loss: 0.00006903
Iteration 115/1000 | Loss: 0.00006819
Iteration 116/1000 | Loss: 0.00021371
Iteration 117/1000 | Loss: 0.00022264
Iteration 118/1000 | Loss: 0.00008616
Iteration 119/1000 | Loss: 0.00007070
Iteration 120/1000 | Loss: 0.00005873
Iteration 121/1000 | Loss: 0.00006025
Iteration 122/1000 | Loss: 0.00006306
Iteration 123/1000 | Loss: 0.00006234
Iteration 124/1000 | Loss: 0.00006284
Iteration 125/1000 | Loss: 0.00006135
Iteration 126/1000 | Loss: 0.00004945
Iteration 127/1000 | Loss: 0.00005362
Iteration 128/1000 | Loss: 0.00005437
Iteration 129/1000 | Loss: 0.00042061
Iteration 130/1000 | Loss: 0.00045636
Iteration 131/1000 | Loss: 0.00008413
Iteration 132/1000 | Loss: 0.00006619
Iteration 133/1000 | Loss: 0.00006045
Iteration 134/1000 | Loss: 0.00005712
Iteration 135/1000 | Loss: 0.00033671
Iteration 136/1000 | Loss: 0.00023395
Iteration 137/1000 | Loss: 0.00007137
Iteration 138/1000 | Loss: 0.00005624
Iteration 139/1000 | Loss: 0.00006480
Iteration 140/1000 | Loss: 0.00019322
Iteration 141/1000 | Loss: 0.00006199
Iteration 142/1000 | Loss: 0.00004422
Iteration 143/1000 | Loss: 0.00004807
Iteration 144/1000 | Loss: 0.00005775
Iteration 145/1000 | Loss: 0.00005784
Iteration 146/1000 | Loss: 0.00006272
Iteration 147/1000 | Loss: 0.00006350
Iteration 148/1000 | Loss: 0.00006550
Iteration 149/1000 | Loss: 0.00005828
Iteration 150/1000 | Loss: 0.00005676
Iteration 151/1000 | Loss: 0.00006337
Iteration 152/1000 | Loss: 0.00006350
Iteration 153/1000 | Loss: 0.00004372
Iteration 154/1000 | Loss: 0.00006608
Iteration 155/1000 | Loss: 0.00005594
Iteration 156/1000 | Loss: 0.00006424
Iteration 157/1000 | Loss: 0.00005604
Iteration 158/1000 | Loss: 0.00006397
Iteration 159/1000 | Loss: 0.00005180
Iteration 160/1000 | Loss: 0.00005497
Iteration 161/1000 | Loss: 0.00005215
Iteration 162/1000 | Loss: 0.00005468
Iteration 163/1000 | Loss: 0.00005962
Iteration 164/1000 | Loss: 0.00006546
Iteration 165/1000 | Loss: 0.00004502
Iteration 166/1000 | Loss: 0.00004136
Iteration 167/1000 | Loss: 0.00005197
Iteration 168/1000 | Loss: 0.00004918
Iteration 169/1000 | Loss: 0.00004485
Iteration 170/1000 | Loss: 0.00005315
Iteration 171/1000 | Loss: 0.00005008
Iteration 172/1000 | Loss: 0.00005153
Iteration 173/1000 | Loss: 0.00005512
Iteration 174/1000 | Loss: 0.00004590
Iteration 175/1000 | Loss: 0.00004252
Iteration 176/1000 | Loss: 0.00005116
Iteration 177/1000 | Loss: 0.00004905
Iteration 178/1000 | Loss: 0.00005729
Iteration 179/1000 | Loss: 0.00005379
Iteration 180/1000 | Loss: 0.00005388
Iteration 181/1000 | Loss: 0.00005794
Iteration 182/1000 | Loss: 0.00005169
Iteration 183/1000 | Loss: 0.00006504
Iteration 184/1000 | Loss: 0.00004998
Iteration 185/1000 | Loss: 0.00006671
Iteration 186/1000 | Loss: 0.00006047
Iteration 187/1000 | Loss: 0.00006101
Iteration 188/1000 | Loss: 0.00005900
Iteration 189/1000 | Loss: 0.00004329
Iteration 190/1000 | Loss: 0.00005890
Iteration 191/1000 | Loss: 0.00006062
Iteration 192/1000 | Loss: 0.00004760
Iteration 193/1000 | Loss: 0.00005710
Iteration 194/1000 | Loss: 0.00005509
Iteration 195/1000 | Loss: 0.00005773
Iteration 196/1000 | Loss: 0.00005757
Iteration 197/1000 | Loss: 0.00005678
Iteration 198/1000 | Loss: 0.00004952
Iteration 199/1000 | Loss: 0.00005704
Iteration 200/1000 | Loss: 0.00006049
Iteration 201/1000 | Loss: 0.00005541
Iteration 202/1000 | Loss: 0.00004807
Iteration 203/1000 | Loss: 0.00004465
Iteration 204/1000 | Loss: 0.00005694
Iteration 205/1000 | Loss: 0.00005831
Iteration 206/1000 | Loss: 0.00005926
Iteration 207/1000 | Loss: 0.00005377
Iteration 208/1000 | Loss: 0.00006008
Iteration 209/1000 | Loss: 0.00005866
Iteration 210/1000 | Loss: 0.00005905
Iteration 211/1000 | Loss: 0.00004768
Iteration 212/1000 | Loss: 0.00008193
Iteration 213/1000 | Loss: 0.00005130
Iteration 214/1000 | Loss: 0.00004939
Iteration 215/1000 | Loss: 0.00005927
Iteration 216/1000 | Loss: 0.00005151
Iteration 217/1000 | Loss: 0.00004580
Iteration 218/1000 | Loss: 0.00004276
Iteration 219/1000 | Loss: 0.00003562
Iteration 220/1000 | Loss: 0.00004459
Iteration 221/1000 | Loss: 0.00003761
Iteration 222/1000 | Loss: 0.00004290
Iteration 223/1000 | Loss: 0.00004005
Iteration 224/1000 | Loss: 0.00004591
Iteration 225/1000 | Loss: 0.00004324
Iteration 226/1000 | Loss: 0.00004937
Iteration 227/1000 | Loss: 0.00003862
Iteration 228/1000 | Loss: 0.00005108
Iteration 229/1000 | Loss: 0.00005338
Iteration 230/1000 | Loss: 0.00003851
Iteration 231/1000 | Loss: 0.00003608
Iteration 232/1000 | Loss: 0.00003524
Iteration 233/1000 | Loss: 0.00004164
Iteration 234/1000 | Loss: 0.00004974
Iteration 235/1000 | Loss: 0.00004360
Iteration 236/1000 | Loss: 0.00005519
Iteration 237/1000 | Loss: 0.00004122
Iteration 238/1000 | Loss: 0.00003942
Iteration 239/1000 | Loss: 0.00004149
Iteration 240/1000 | Loss: 0.00004119
Iteration 241/1000 | Loss: 0.00004092
Iteration 242/1000 | Loss: 0.00004125
Iteration 243/1000 | Loss: 0.00004526
Iteration 244/1000 | Loss: 0.00004140
Iteration 245/1000 | Loss: 0.00004323
Iteration 246/1000 | Loss: 0.00003842
Iteration 247/1000 | Loss: 0.00004206
Iteration 248/1000 | Loss: 0.00003817
Iteration 249/1000 | Loss: 0.00003680
Iteration 250/1000 | Loss: 0.00003390
Iteration 251/1000 | Loss: 0.00003303
Iteration 252/1000 | Loss: 0.00003282
Iteration 253/1000 | Loss: 0.00003281
Iteration 254/1000 | Loss: 0.00003278
Iteration 255/1000 | Loss: 0.00003274
Iteration 256/1000 | Loss: 0.00003274
Iteration 257/1000 | Loss: 0.00003274
Iteration 258/1000 | Loss: 0.00003274
Iteration 259/1000 | Loss: 0.00003274
Iteration 260/1000 | Loss: 0.00003274
Iteration 261/1000 | Loss: 0.00003274
Iteration 262/1000 | Loss: 0.00003274
Iteration 263/1000 | Loss: 0.00003274
Iteration 264/1000 | Loss: 0.00003274
Iteration 265/1000 | Loss: 0.00003273
Iteration 266/1000 | Loss: 0.00003273
Iteration 267/1000 | Loss: 0.00003272
Iteration 268/1000 | Loss: 0.00003272
Iteration 269/1000 | Loss: 0.00003271
Iteration 270/1000 | Loss: 0.00003271
Iteration 271/1000 | Loss: 0.00003271
Iteration 272/1000 | Loss: 0.00003271
Iteration 273/1000 | Loss: 0.00003271
Iteration 274/1000 | Loss: 0.00003271
Iteration 275/1000 | Loss: 0.00003271
Iteration 276/1000 | Loss: 0.00003271
Iteration 277/1000 | Loss: 0.00003271
Iteration 278/1000 | Loss: 0.00003271
Iteration 279/1000 | Loss: 0.00003270
Iteration 280/1000 | Loss: 0.00003270
Iteration 281/1000 | Loss: 0.00003269
Iteration 282/1000 | Loss: 0.00003269
Iteration 283/1000 | Loss: 0.00003268
Iteration 284/1000 | Loss: 0.00003268
Iteration 285/1000 | Loss: 0.00003268
Iteration 286/1000 | Loss: 0.00003267
Iteration 287/1000 | Loss: 0.00003267
Iteration 288/1000 | Loss: 0.00003267
Iteration 289/1000 | Loss: 0.00003267
Iteration 290/1000 | Loss: 0.00003267
Iteration 291/1000 | Loss: 0.00003266
Iteration 292/1000 | Loss: 0.00003266
Iteration 293/1000 | Loss: 0.00003266
Iteration 294/1000 | Loss: 0.00003266
Iteration 295/1000 | Loss: 0.00003266
Iteration 296/1000 | Loss: 0.00003266
Iteration 297/1000 | Loss: 0.00003266
Iteration 298/1000 | Loss: 0.00003265
Iteration 299/1000 | Loss: 0.00003265
Iteration 300/1000 | Loss: 0.00003265
Iteration 301/1000 | Loss: 0.00003265
Iteration 302/1000 | Loss: 0.00003265
Iteration 303/1000 | Loss: 0.00003264
Iteration 304/1000 | Loss: 0.00003264
Iteration 305/1000 | Loss: 0.00003264
Iteration 306/1000 | Loss: 0.00003264
Iteration 307/1000 | Loss: 0.00003264
Iteration 308/1000 | Loss: 0.00003264
Iteration 309/1000 | Loss: 0.00003264
Iteration 310/1000 | Loss: 0.00003264
Iteration 311/1000 | Loss: 0.00003264
Iteration 312/1000 | Loss: 0.00003264
Iteration 313/1000 | Loss: 0.00003263
Iteration 314/1000 | Loss: 0.00003263
Iteration 315/1000 | Loss: 0.00003263
Iteration 316/1000 | Loss: 0.00003263
Iteration 317/1000 | Loss: 0.00003263
Iteration 318/1000 | Loss: 0.00003263
Iteration 319/1000 | Loss: 0.00003263
Iteration 320/1000 | Loss: 0.00003262
Iteration 321/1000 | Loss: 0.00003262
Iteration 322/1000 | Loss: 0.00003262
Iteration 323/1000 | Loss: 0.00003261
Iteration 324/1000 | Loss: 0.00003261
Iteration 325/1000 | Loss: 0.00003260
Iteration 326/1000 | Loss: 0.00003260
Iteration 327/1000 | Loss: 0.00003260
Iteration 328/1000 | Loss: 0.00003260
Iteration 329/1000 | Loss: 0.00003260
Iteration 330/1000 | Loss: 0.00003260
Iteration 331/1000 | Loss: 0.00003259
Iteration 332/1000 | Loss: 0.00003259
Iteration 333/1000 | Loss: 0.00003259
Iteration 334/1000 | Loss: 0.00003259
Iteration 335/1000 | Loss: 0.00003258
Iteration 336/1000 | Loss: 0.00003258
Iteration 337/1000 | Loss: 0.00003258
Iteration 338/1000 | Loss: 0.00003258
Iteration 339/1000 | Loss: 0.00003257
Iteration 340/1000 | Loss: 0.00003257
Iteration 341/1000 | Loss: 0.00003257
Iteration 342/1000 | Loss: 0.00004480
Iteration 343/1000 | Loss: 0.00003687
Iteration 344/1000 | Loss: 0.00003428
Iteration 345/1000 | Loss: 0.00003360
Iteration 346/1000 | Loss: 0.00003336
Iteration 347/1000 | Loss: 0.00003291
Iteration 348/1000 | Loss: 0.00003267
Iteration 349/1000 | Loss: 0.00003260
Iteration 350/1000 | Loss: 0.00003254
Iteration 351/1000 | Loss: 0.00003249
Iteration 352/1000 | Loss: 0.00003244
Iteration 353/1000 | Loss: 0.00003240
Iteration 354/1000 | Loss: 0.00003226
Iteration 355/1000 | Loss: 0.00003225
Iteration 356/1000 | Loss: 0.00003224
Iteration 357/1000 | Loss: 0.00003224
Iteration 358/1000 | Loss: 0.00003223
Iteration 359/1000 | Loss: 0.00003219
Iteration 360/1000 | Loss: 0.00003219
Iteration 361/1000 | Loss: 0.00003218
Iteration 362/1000 | Loss: 0.00003207
Iteration 363/1000 | Loss: 0.00003207
Iteration 364/1000 | Loss: 0.00003207
Iteration 365/1000 | Loss: 0.00003207
Iteration 366/1000 | Loss: 0.00003206
Iteration 367/1000 | Loss: 0.00003206
Iteration 368/1000 | Loss: 0.00003206
Iteration 369/1000 | Loss: 0.00003205
Iteration 370/1000 | Loss: 0.00003202
Iteration 371/1000 | Loss: 0.00003202
Iteration 372/1000 | Loss: 0.00003201
Iteration 373/1000 | Loss: 0.00003201
Iteration 374/1000 | Loss: 0.00003201
Iteration 375/1000 | Loss: 0.00003200
Iteration 376/1000 | Loss: 0.00003200
Iteration 377/1000 | Loss: 0.00003200
Iteration 378/1000 | Loss: 0.00003200
Iteration 379/1000 | Loss: 0.00003199
Iteration 380/1000 | Loss: 0.00003199
Iteration 381/1000 | Loss: 0.00003199
Iteration 382/1000 | Loss: 0.00003199
Iteration 383/1000 | Loss: 0.00003199
Iteration 384/1000 | Loss: 0.00003198
Iteration 385/1000 | Loss: 0.00003198
Iteration 386/1000 | Loss: 0.00003198
Iteration 387/1000 | Loss: 0.00003198
Iteration 388/1000 | Loss: 0.00003198
Iteration 389/1000 | Loss: 0.00003198
Iteration 390/1000 | Loss: 0.00003198
Iteration 391/1000 | Loss: 0.00003198
Iteration 392/1000 | Loss: 0.00003198
Iteration 393/1000 | Loss: 0.00003198
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 393. Stopping optimization.
Last 5 losses: [3.198377089574933e-05, 3.198377089574933e-05, 3.198377089574933e-05, 3.198377089574933e-05, 3.198377089574933e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.198377089574933e-05

Optimization complete. Final v2v error: 3.87019419670105 mm

Highest mean error: 11.996880531311035 mm for frame 213

Lowest mean error: 2.9315671920776367 mm for frame 6

Saving results

Total time: 479.2554919719696
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00411167
Iteration 2/25 | Loss: 0.00127807
Iteration 3/25 | Loss: 0.00114028
Iteration 4/25 | Loss: 0.00111851
Iteration 5/25 | Loss: 0.00111285
Iteration 6/25 | Loss: 0.00111131
Iteration 7/25 | Loss: 0.00111087
Iteration 8/25 | Loss: 0.00111081
Iteration 9/25 | Loss: 0.00111081
Iteration 10/25 | Loss: 0.00111081
Iteration 11/25 | Loss: 0.00111081
Iteration 12/25 | Loss: 0.00111081
Iteration 13/25 | Loss: 0.00111081
Iteration 14/25 | Loss: 0.00111081
Iteration 15/25 | Loss: 0.00111081
Iteration 16/25 | Loss: 0.00111081
Iteration 17/25 | Loss: 0.00111081
Iteration 18/25 | Loss: 0.00111081
Iteration 19/25 | Loss: 0.00111081
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011108110193163157, 0.0011108110193163157, 0.0011108110193163157, 0.0011108110193163157, 0.0011108110193163157]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011108110193163157

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39117587
Iteration 2/25 | Loss: 0.00078538
Iteration 3/25 | Loss: 0.00078537
Iteration 4/25 | Loss: 0.00078537
Iteration 5/25 | Loss: 0.00078537
Iteration 6/25 | Loss: 0.00078537
Iteration 7/25 | Loss: 0.00078537
Iteration 8/25 | Loss: 0.00078537
Iteration 9/25 | Loss: 0.00078537
Iteration 10/25 | Loss: 0.00078537
Iteration 11/25 | Loss: 0.00078537
Iteration 12/25 | Loss: 0.00078537
Iteration 13/25 | Loss: 0.00078537
Iteration 14/25 | Loss: 0.00078537
Iteration 15/25 | Loss: 0.00078537
Iteration 16/25 | Loss: 0.00078537
Iteration 17/25 | Loss: 0.00078537
Iteration 18/25 | Loss: 0.00078537
Iteration 19/25 | Loss: 0.00078537
Iteration 20/25 | Loss: 0.00078537
Iteration 21/25 | Loss: 0.00078537
Iteration 22/25 | Loss: 0.00078537
Iteration 23/25 | Loss: 0.00078537
Iteration 24/25 | Loss: 0.00078537
Iteration 25/25 | Loss: 0.00078537

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078537
Iteration 2/1000 | Loss: 0.00003770
Iteration 3/1000 | Loss: 0.00002217
Iteration 4/1000 | Loss: 0.00001736
Iteration 5/1000 | Loss: 0.00001625
Iteration 6/1000 | Loss: 0.00001532
Iteration 7/1000 | Loss: 0.00001483
Iteration 8/1000 | Loss: 0.00001443
Iteration 9/1000 | Loss: 0.00001406
Iteration 10/1000 | Loss: 0.00001384
Iteration 11/1000 | Loss: 0.00001369
Iteration 12/1000 | Loss: 0.00001356
Iteration 13/1000 | Loss: 0.00001347
Iteration 14/1000 | Loss: 0.00001343
Iteration 15/1000 | Loss: 0.00001339
Iteration 16/1000 | Loss: 0.00001338
Iteration 17/1000 | Loss: 0.00001338
Iteration 18/1000 | Loss: 0.00001337
Iteration 19/1000 | Loss: 0.00001337
Iteration 20/1000 | Loss: 0.00001337
Iteration 21/1000 | Loss: 0.00001336
Iteration 22/1000 | Loss: 0.00001336
Iteration 23/1000 | Loss: 0.00001335
Iteration 24/1000 | Loss: 0.00001334
Iteration 25/1000 | Loss: 0.00001334
Iteration 26/1000 | Loss: 0.00001333
Iteration 27/1000 | Loss: 0.00001332
Iteration 28/1000 | Loss: 0.00001332
Iteration 29/1000 | Loss: 0.00001330
Iteration 30/1000 | Loss: 0.00001328
Iteration 31/1000 | Loss: 0.00001327
Iteration 32/1000 | Loss: 0.00001327
Iteration 33/1000 | Loss: 0.00001326
Iteration 34/1000 | Loss: 0.00001326
Iteration 35/1000 | Loss: 0.00001326
Iteration 36/1000 | Loss: 0.00001326
Iteration 37/1000 | Loss: 0.00001325
Iteration 38/1000 | Loss: 0.00001325
Iteration 39/1000 | Loss: 0.00001324
Iteration 40/1000 | Loss: 0.00001324
Iteration 41/1000 | Loss: 0.00001322
Iteration 42/1000 | Loss: 0.00001322
Iteration 43/1000 | Loss: 0.00001321
Iteration 44/1000 | Loss: 0.00001321
Iteration 45/1000 | Loss: 0.00001321
Iteration 46/1000 | Loss: 0.00001320
Iteration 47/1000 | Loss: 0.00001320
Iteration 48/1000 | Loss: 0.00001318
Iteration 49/1000 | Loss: 0.00001318
Iteration 50/1000 | Loss: 0.00001318
Iteration 51/1000 | Loss: 0.00001318
Iteration 52/1000 | Loss: 0.00001317
Iteration 53/1000 | Loss: 0.00001317
Iteration 54/1000 | Loss: 0.00001316
Iteration 55/1000 | Loss: 0.00001316
Iteration 56/1000 | Loss: 0.00001316
Iteration 57/1000 | Loss: 0.00001315
Iteration 58/1000 | Loss: 0.00001315
Iteration 59/1000 | Loss: 0.00001314
Iteration 60/1000 | Loss: 0.00001314
Iteration 61/1000 | Loss: 0.00001314
Iteration 62/1000 | Loss: 0.00001314
Iteration 63/1000 | Loss: 0.00001314
Iteration 64/1000 | Loss: 0.00001313
Iteration 65/1000 | Loss: 0.00001313
Iteration 66/1000 | Loss: 0.00001313
Iteration 67/1000 | Loss: 0.00001313
Iteration 68/1000 | Loss: 0.00001313
Iteration 69/1000 | Loss: 0.00001313
Iteration 70/1000 | Loss: 0.00001313
Iteration 71/1000 | Loss: 0.00001312
Iteration 72/1000 | Loss: 0.00001312
Iteration 73/1000 | Loss: 0.00001312
Iteration 74/1000 | Loss: 0.00001312
Iteration 75/1000 | Loss: 0.00001311
Iteration 76/1000 | Loss: 0.00001311
Iteration 77/1000 | Loss: 0.00001311
Iteration 78/1000 | Loss: 0.00001311
Iteration 79/1000 | Loss: 0.00001311
Iteration 80/1000 | Loss: 0.00001311
Iteration 81/1000 | Loss: 0.00001310
Iteration 82/1000 | Loss: 0.00001310
Iteration 83/1000 | Loss: 0.00001310
Iteration 84/1000 | Loss: 0.00001310
Iteration 85/1000 | Loss: 0.00001310
Iteration 86/1000 | Loss: 0.00001310
Iteration 87/1000 | Loss: 0.00001310
Iteration 88/1000 | Loss: 0.00001310
Iteration 89/1000 | Loss: 0.00001310
Iteration 90/1000 | Loss: 0.00001310
Iteration 91/1000 | Loss: 0.00001310
Iteration 92/1000 | Loss: 0.00001310
Iteration 93/1000 | Loss: 0.00001310
Iteration 94/1000 | Loss: 0.00001310
Iteration 95/1000 | Loss: 0.00001309
Iteration 96/1000 | Loss: 0.00001309
Iteration 97/1000 | Loss: 0.00001309
Iteration 98/1000 | Loss: 0.00001309
Iteration 99/1000 | Loss: 0.00001309
Iteration 100/1000 | Loss: 0.00001309
Iteration 101/1000 | Loss: 0.00001308
Iteration 102/1000 | Loss: 0.00001308
Iteration 103/1000 | Loss: 0.00001308
Iteration 104/1000 | Loss: 0.00001308
Iteration 105/1000 | Loss: 0.00001308
Iteration 106/1000 | Loss: 0.00001308
Iteration 107/1000 | Loss: 0.00001308
Iteration 108/1000 | Loss: 0.00001308
Iteration 109/1000 | Loss: 0.00001308
Iteration 110/1000 | Loss: 0.00001308
Iteration 111/1000 | Loss: 0.00001308
Iteration 112/1000 | Loss: 0.00001308
Iteration 113/1000 | Loss: 0.00001307
Iteration 114/1000 | Loss: 0.00001307
Iteration 115/1000 | Loss: 0.00001307
Iteration 116/1000 | Loss: 0.00001307
Iteration 117/1000 | Loss: 0.00001307
Iteration 118/1000 | Loss: 0.00001306
Iteration 119/1000 | Loss: 0.00001306
Iteration 120/1000 | Loss: 0.00001306
Iteration 121/1000 | Loss: 0.00001306
Iteration 122/1000 | Loss: 0.00001305
Iteration 123/1000 | Loss: 0.00001305
Iteration 124/1000 | Loss: 0.00001305
Iteration 125/1000 | Loss: 0.00001305
Iteration 126/1000 | Loss: 0.00001305
Iteration 127/1000 | Loss: 0.00001305
Iteration 128/1000 | Loss: 0.00001304
Iteration 129/1000 | Loss: 0.00001304
Iteration 130/1000 | Loss: 0.00001304
Iteration 131/1000 | Loss: 0.00001304
Iteration 132/1000 | Loss: 0.00001304
Iteration 133/1000 | Loss: 0.00001304
Iteration 134/1000 | Loss: 0.00001304
Iteration 135/1000 | Loss: 0.00001304
Iteration 136/1000 | Loss: 0.00001303
Iteration 137/1000 | Loss: 0.00001303
Iteration 138/1000 | Loss: 0.00001303
Iteration 139/1000 | Loss: 0.00001303
Iteration 140/1000 | Loss: 0.00001303
Iteration 141/1000 | Loss: 0.00001303
Iteration 142/1000 | Loss: 0.00001303
Iteration 143/1000 | Loss: 0.00001303
Iteration 144/1000 | Loss: 0.00001302
Iteration 145/1000 | Loss: 0.00001302
Iteration 146/1000 | Loss: 0.00001302
Iteration 147/1000 | Loss: 0.00001302
Iteration 148/1000 | Loss: 0.00001302
Iteration 149/1000 | Loss: 0.00001302
Iteration 150/1000 | Loss: 0.00001302
Iteration 151/1000 | Loss: 0.00001302
Iteration 152/1000 | Loss: 0.00001301
Iteration 153/1000 | Loss: 0.00001301
Iteration 154/1000 | Loss: 0.00001301
Iteration 155/1000 | Loss: 0.00001301
Iteration 156/1000 | Loss: 0.00001301
Iteration 157/1000 | Loss: 0.00001301
Iteration 158/1000 | Loss: 0.00001301
Iteration 159/1000 | Loss: 0.00001301
Iteration 160/1000 | Loss: 0.00001301
Iteration 161/1000 | Loss: 0.00001301
Iteration 162/1000 | Loss: 0.00001301
Iteration 163/1000 | Loss: 0.00001301
Iteration 164/1000 | Loss: 0.00001301
Iteration 165/1000 | Loss: 0.00001301
Iteration 166/1000 | Loss: 0.00001301
Iteration 167/1000 | Loss: 0.00001301
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.3012200724915601e-05, 1.3012200724915601e-05, 1.3012200724915601e-05, 1.3012200724915601e-05, 1.3012200724915601e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3012200724915601e-05

Optimization complete. Final v2v error: 3.005068302154541 mm

Highest mean error: 4.347175598144531 mm for frame 54

Lowest mean error: 2.4680447578430176 mm for frame 117

Saving results

Total time: 40.68031287193298
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01029789
Iteration 2/25 | Loss: 0.00192540
Iteration 3/25 | Loss: 0.00148167
Iteration 4/25 | Loss: 0.00155297
Iteration 5/25 | Loss: 0.00139641
Iteration 6/25 | Loss: 0.00143869
Iteration 7/25 | Loss: 0.00133322
Iteration 8/25 | Loss: 0.00131199
Iteration 9/25 | Loss: 0.00130274
Iteration 10/25 | Loss: 0.00125847
Iteration 11/25 | Loss: 0.00122051
Iteration 12/25 | Loss: 0.00121508
Iteration 13/25 | Loss: 0.00121221
Iteration 14/25 | Loss: 0.00120475
Iteration 15/25 | Loss: 0.00118679
Iteration 16/25 | Loss: 0.00117980
Iteration 17/25 | Loss: 0.00120412
Iteration 18/25 | Loss: 0.00116191
Iteration 19/25 | Loss: 0.00115432
Iteration 20/25 | Loss: 0.00115350
Iteration 21/25 | Loss: 0.00115322
Iteration 22/25 | Loss: 0.00115304
Iteration 23/25 | Loss: 0.00115297
Iteration 24/25 | Loss: 0.00115296
Iteration 25/25 | Loss: 0.00115296

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63667643
Iteration 2/25 | Loss: 0.00117583
Iteration 3/25 | Loss: 0.00117583
Iteration 4/25 | Loss: 0.00117583
Iteration 5/25 | Loss: 0.00117583
Iteration 6/25 | Loss: 0.00117583
Iteration 7/25 | Loss: 0.00117583
Iteration 8/25 | Loss: 0.00117583
Iteration 9/25 | Loss: 0.00117583
Iteration 10/25 | Loss: 0.00117583
Iteration 11/25 | Loss: 0.00117583
Iteration 12/25 | Loss: 0.00117583
Iteration 13/25 | Loss: 0.00117583
Iteration 14/25 | Loss: 0.00117583
Iteration 15/25 | Loss: 0.00117583
Iteration 16/25 | Loss: 0.00117583
Iteration 17/25 | Loss: 0.00117583
Iteration 18/25 | Loss: 0.00117583
Iteration 19/25 | Loss: 0.00117583
Iteration 20/25 | Loss: 0.00117583
Iteration 21/25 | Loss: 0.00117583
Iteration 22/25 | Loss: 0.00117583
Iteration 23/25 | Loss: 0.00117583
Iteration 24/25 | Loss: 0.00117583
Iteration 25/25 | Loss: 0.00117583

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117583
Iteration 2/1000 | Loss: 0.00010492
Iteration 3/1000 | Loss: 0.00006598
Iteration 4/1000 | Loss: 0.00005533
Iteration 5/1000 | Loss: 0.00005130
Iteration 6/1000 | Loss: 0.00017842
Iteration 7/1000 | Loss: 0.00065900
Iteration 8/1000 | Loss: 0.00073436
Iteration 9/1000 | Loss: 0.00069431
Iteration 10/1000 | Loss: 0.00045211
Iteration 11/1000 | Loss: 0.00066445
Iteration 12/1000 | Loss: 0.00087389
Iteration 13/1000 | Loss: 0.00036864
Iteration 14/1000 | Loss: 0.00006098
Iteration 15/1000 | Loss: 0.00005214
Iteration 16/1000 | Loss: 0.00004820
Iteration 17/1000 | Loss: 0.00065403
Iteration 18/1000 | Loss: 0.00022741
Iteration 19/1000 | Loss: 0.00023709
Iteration 20/1000 | Loss: 0.00004535
Iteration 21/1000 | Loss: 0.00004265
Iteration 22/1000 | Loss: 0.00029458
Iteration 23/1000 | Loss: 0.00045462
Iteration 24/1000 | Loss: 0.00016863
Iteration 25/1000 | Loss: 0.00004322
Iteration 26/1000 | Loss: 0.00003875
Iteration 27/1000 | Loss: 0.00024867
Iteration 28/1000 | Loss: 0.00003711
Iteration 29/1000 | Loss: 0.00003556
Iteration 30/1000 | Loss: 0.00003465
Iteration 31/1000 | Loss: 0.00071067
Iteration 32/1000 | Loss: 0.00076459
Iteration 33/1000 | Loss: 0.00064698
Iteration 34/1000 | Loss: 0.00033863
Iteration 35/1000 | Loss: 0.00059708
Iteration 36/1000 | Loss: 0.00019105
Iteration 37/1000 | Loss: 0.00015642
Iteration 38/1000 | Loss: 0.00041828
Iteration 39/1000 | Loss: 0.00018813
Iteration 40/1000 | Loss: 0.00021030
Iteration 41/1000 | Loss: 0.00002513
Iteration 42/1000 | Loss: 0.00041717
Iteration 43/1000 | Loss: 0.00002205
Iteration 44/1000 | Loss: 0.00001884
Iteration 45/1000 | Loss: 0.00001751
Iteration 46/1000 | Loss: 0.00001626
Iteration 47/1000 | Loss: 0.00040199
Iteration 48/1000 | Loss: 0.00001624
Iteration 49/1000 | Loss: 0.00001467
Iteration 50/1000 | Loss: 0.00001388
Iteration 51/1000 | Loss: 0.00001299
Iteration 52/1000 | Loss: 0.00001250
Iteration 53/1000 | Loss: 0.00001226
Iteration 54/1000 | Loss: 0.00001208
Iteration 55/1000 | Loss: 0.00001197
Iteration 56/1000 | Loss: 0.00001196
Iteration 57/1000 | Loss: 0.00001196
Iteration 58/1000 | Loss: 0.00001195
Iteration 59/1000 | Loss: 0.00001195
Iteration 60/1000 | Loss: 0.00001192
Iteration 61/1000 | Loss: 0.00001192
Iteration 62/1000 | Loss: 0.00001191
Iteration 63/1000 | Loss: 0.00001191
Iteration 64/1000 | Loss: 0.00001190
Iteration 65/1000 | Loss: 0.00001190
Iteration 66/1000 | Loss: 0.00001189
Iteration 67/1000 | Loss: 0.00001189
Iteration 68/1000 | Loss: 0.00001188
Iteration 69/1000 | Loss: 0.00001188
Iteration 70/1000 | Loss: 0.00001186
Iteration 71/1000 | Loss: 0.00001185
Iteration 72/1000 | Loss: 0.00001184
Iteration 73/1000 | Loss: 0.00001183
Iteration 74/1000 | Loss: 0.00001182
Iteration 75/1000 | Loss: 0.00001182
Iteration 76/1000 | Loss: 0.00001181
Iteration 77/1000 | Loss: 0.00001181
Iteration 78/1000 | Loss: 0.00001180
Iteration 79/1000 | Loss: 0.00001180
Iteration 80/1000 | Loss: 0.00001180
Iteration 81/1000 | Loss: 0.00001179
Iteration 82/1000 | Loss: 0.00001179
Iteration 83/1000 | Loss: 0.00001178
Iteration 84/1000 | Loss: 0.00001178
Iteration 85/1000 | Loss: 0.00001175
Iteration 86/1000 | Loss: 0.00001175
Iteration 87/1000 | Loss: 0.00001173
Iteration 88/1000 | Loss: 0.00001173
Iteration 89/1000 | Loss: 0.00001173
Iteration 90/1000 | Loss: 0.00001172
Iteration 91/1000 | Loss: 0.00001172
Iteration 92/1000 | Loss: 0.00001172
Iteration 93/1000 | Loss: 0.00001172
Iteration 94/1000 | Loss: 0.00001171
Iteration 95/1000 | Loss: 0.00001171
Iteration 96/1000 | Loss: 0.00001171
Iteration 97/1000 | Loss: 0.00001171
Iteration 98/1000 | Loss: 0.00001171
Iteration 99/1000 | Loss: 0.00001170
Iteration 100/1000 | Loss: 0.00001170
Iteration 101/1000 | Loss: 0.00001170
Iteration 102/1000 | Loss: 0.00001170
Iteration 103/1000 | Loss: 0.00001170
Iteration 104/1000 | Loss: 0.00001170
Iteration 105/1000 | Loss: 0.00001169
Iteration 106/1000 | Loss: 0.00001169
Iteration 107/1000 | Loss: 0.00001169
Iteration 108/1000 | Loss: 0.00001169
Iteration 109/1000 | Loss: 0.00001168
Iteration 110/1000 | Loss: 0.00001168
Iteration 111/1000 | Loss: 0.00001168
Iteration 112/1000 | Loss: 0.00001167
Iteration 113/1000 | Loss: 0.00001167
Iteration 114/1000 | Loss: 0.00001167
Iteration 115/1000 | Loss: 0.00001167
Iteration 116/1000 | Loss: 0.00001166
Iteration 117/1000 | Loss: 0.00001166
Iteration 118/1000 | Loss: 0.00001166
Iteration 119/1000 | Loss: 0.00001166
Iteration 120/1000 | Loss: 0.00001166
Iteration 121/1000 | Loss: 0.00001166
Iteration 122/1000 | Loss: 0.00001166
Iteration 123/1000 | Loss: 0.00001165
Iteration 124/1000 | Loss: 0.00001165
Iteration 125/1000 | Loss: 0.00001165
Iteration 126/1000 | Loss: 0.00001165
Iteration 127/1000 | Loss: 0.00001165
Iteration 128/1000 | Loss: 0.00001165
Iteration 129/1000 | Loss: 0.00001165
Iteration 130/1000 | Loss: 0.00001165
Iteration 131/1000 | Loss: 0.00001164
Iteration 132/1000 | Loss: 0.00001164
Iteration 133/1000 | Loss: 0.00001164
Iteration 134/1000 | Loss: 0.00001164
Iteration 135/1000 | Loss: 0.00001164
Iteration 136/1000 | Loss: 0.00001164
Iteration 137/1000 | Loss: 0.00001164
Iteration 138/1000 | Loss: 0.00001164
Iteration 139/1000 | Loss: 0.00001164
Iteration 140/1000 | Loss: 0.00001164
Iteration 141/1000 | Loss: 0.00001164
Iteration 142/1000 | Loss: 0.00001164
Iteration 143/1000 | Loss: 0.00001163
Iteration 144/1000 | Loss: 0.00001163
Iteration 145/1000 | Loss: 0.00001163
Iteration 146/1000 | Loss: 0.00001163
Iteration 147/1000 | Loss: 0.00001163
Iteration 148/1000 | Loss: 0.00001162
Iteration 149/1000 | Loss: 0.00001162
Iteration 150/1000 | Loss: 0.00001162
Iteration 151/1000 | Loss: 0.00001162
Iteration 152/1000 | Loss: 0.00001162
Iteration 153/1000 | Loss: 0.00001162
Iteration 154/1000 | Loss: 0.00001162
Iteration 155/1000 | Loss: 0.00001161
Iteration 156/1000 | Loss: 0.00001161
Iteration 157/1000 | Loss: 0.00001161
Iteration 158/1000 | Loss: 0.00001160
Iteration 159/1000 | Loss: 0.00001160
Iteration 160/1000 | Loss: 0.00001160
Iteration 161/1000 | Loss: 0.00001160
Iteration 162/1000 | Loss: 0.00001160
Iteration 163/1000 | Loss: 0.00001160
Iteration 164/1000 | Loss: 0.00001160
Iteration 165/1000 | Loss: 0.00001159
Iteration 166/1000 | Loss: 0.00001159
Iteration 167/1000 | Loss: 0.00001159
Iteration 168/1000 | Loss: 0.00001159
Iteration 169/1000 | Loss: 0.00001159
Iteration 170/1000 | Loss: 0.00001159
Iteration 171/1000 | Loss: 0.00001158
Iteration 172/1000 | Loss: 0.00001158
Iteration 173/1000 | Loss: 0.00001158
Iteration 174/1000 | Loss: 0.00001158
Iteration 175/1000 | Loss: 0.00001158
Iteration 176/1000 | Loss: 0.00001157
Iteration 177/1000 | Loss: 0.00001157
Iteration 178/1000 | Loss: 0.00001157
Iteration 179/1000 | Loss: 0.00001157
Iteration 180/1000 | Loss: 0.00001157
Iteration 181/1000 | Loss: 0.00001157
Iteration 182/1000 | Loss: 0.00001156
Iteration 183/1000 | Loss: 0.00001156
Iteration 184/1000 | Loss: 0.00001156
Iteration 185/1000 | Loss: 0.00001156
Iteration 186/1000 | Loss: 0.00001156
Iteration 187/1000 | Loss: 0.00001156
Iteration 188/1000 | Loss: 0.00001156
Iteration 189/1000 | Loss: 0.00001156
Iteration 190/1000 | Loss: 0.00001156
Iteration 191/1000 | Loss: 0.00001156
Iteration 192/1000 | Loss: 0.00001156
Iteration 193/1000 | Loss: 0.00001156
Iteration 194/1000 | Loss: 0.00001156
Iteration 195/1000 | Loss: 0.00001155
Iteration 196/1000 | Loss: 0.00001155
Iteration 197/1000 | Loss: 0.00001155
Iteration 198/1000 | Loss: 0.00001155
Iteration 199/1000 | Loss: 0.00001155
Iteration 200/1000 | Loss: 0.00001155
Iteration 201/1000 | Loss: 0.00001155
Iteration 202/1000 | Loss: 0.00001155
Iteration 203/1000 | Loss: 0.00001155
Iteration 204/1000 | Loss: 0.00001155
Iteration 205/1000 | Loss: 0.00001155
Iteration 206/1000 | Loss: 0.00001155
Iteration 207/1000 | Loss: 0.00001155
Iteration 208/1000 | Loss: 0.00001155
Iteration 209/1000 | Loss: 0.00001155
Iteration 210/1000 | Loss: 0.00001155
Iteration 211/1000 | Loss: 0.00001155
Iteration 212/1000 | Loss: 0.00001155
Iteration 213/1000 | Loss: 0.00001154
Iteration 214/1000 | Loss: 0.00001154
Iteration 215/1000 | Loss: 0.00001154
Iteration 216/1000 | Loss: 0.00001154
Iteration 217/1000 | Loss: 0.00001154
Iteration 218/1000 | Loss: 0.00001154
Iteration 219/1000 | Loss: 0.00001154
Iteration 220/1000 | Loss: 0.00001154
Iteration 221/1000 | Loss: 0.00001154
Iteration 222/1000 | Loss: 0.00001154
Iteration 223/1000 | Loss: 0.00001154
Iteration 224/1000 | Loss: 0.00001154
Iteration 225/1000 | Loss: 0.00001154
Iteration 226/1000 | Loss: 0.00001154
Iteration 227/1000 | Loss: 0.00001154
Iteration 228/1000 | Loss: 0.00001154
Iteration 229/1000 | Loss: 0.00001154
Iteration 230/1000 | Loss: 0.00001154
Iteration 231/1000 | Loss: 0.00001154
Iteration 232/1000 | Loss: 0.00001154
Iteration 233/1000 | Loss: 0.00001154
Iteration 234/1000 | Loss: 0.00001154
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 234. Stopping optimization.
Last 5 losses: [1.1535633348103147e-05, 1.1535633348103147e-05, 1.1535633348103147e-05, 1.1535633348103147e-05, 1.1535633348103147e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1535633348103147e-05

Optimization complete. Final v2v error: 2.8511502742767334 mm

Highest mean error: 5.295500755310059 mm for frame 93

Lowest mean error: 2.4831385612487793 mm for frame 20

Saving results

Total time: 132.0186243057251
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00844976
Iteration 2/25 | Loss: 0.00126216
Iteration 3/25 | Loss: 0.00111378
Iteration 4/25 | Loss: 0.00110054
Iteration 5/25 | Loss: 0.00109703
Iteration 6/25 | Loss: 0.00109681
Iteration 7/25 | Loss: 0.00109681
Iteration 8/25 | Loss: 0.00109681
Iteration 9/25 | Loss: 0.00109681
Iteration 10/25 | Loss: 0.00109681
Iteration 11/25 | Loss: 0.00109681
Iteration 12/25 | Loss: 0.00109681
Iteration 13/25 | Loss: 0.00109681
Iteration 14/25 | Loss: 0.00109681
Iteration 15/25 | Loss: 0.00109681
Iteration 16/25 | Loss: 0.00109681
Iteration 17/25 | Loss: 0.00109681
Iteration 18/25 | Loss: 0.00109681
Iteration 19/25 | Loss: 0.00109681
Iteration 20/25 | Loss: 0.00109681
Iteration 21/25 | Loss: 0.00109681
Iteration 22/25 | Loss: 0.00109681
Iteration 23/25 | Loss: 0.00109681
Iteration 24/25 | Loss: 0.00109681
Iteration 25/25 | Loss: 0.00109681

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30101597
Iteration 2/25 | Loss: 0.00090440
Iteration 3/25 | Loss: 0.00090440
Iteration 4/25 | Loss: 0.00090440
Iteration 5/25 | Loss: 0.00090440
Iteration 6/25 | Loss: 0.00090440
Iteration 7/25 | Loss: 0.00090440
Iteration 8/25 | Loss: 0.00090440
Iteration 9/25 | Loss: 0.00090440
Iteration 10/25 | Loss: 0.00090440
Iteration 11/25 | Loss: 0.00090440
Iteration 12/25 | Loss: 0.00090440
Iteration 13/25 | Loss: 0.00090440
Iteration 14/25 | Loss: 0.00090440
Iteration 15/25 | Loss: 0.00090440
Iteration 16/25 | Loss: 0.00090440
Iteration 17/25 | Loss: 0.00090440
Iteration 18/25 | Loss: 0.00090440
Iteration 19/25 | Loss: 0.00090440
Iteration 20/25 | Loss: 0.00090440
Iteration 21/25 | Loss: 0.00090440
Iteration 22/25 | Loss: 0.00090440
Iteration 23/25 | Loss: 0.00090440
Iteration 24/25 | Loss: 0.00090440
Iteration 25/25 | Loss: 0.00090440

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090440
Iteration 2/1000 | Loss: 0.00002703
Iteration 3/1000 | Loss: 0.00001592
Iteration 4/1000 | Loss: 0.00001406
Iteration 5/1000 | Loss: 0.00001347
Iteration 6/1000 | Loss: 0.00001296
Iteration 7/1000 | Loss: 0.00001269
Iteration 8/1000 | Loss: 0.00001241
Iteration 9/1000 | Loss: 0.00001221
Iteration 10/1000 | Loss: 0.00001207
Iteration 11/1000 | Loss: 0.00001195
Iteration 12/1000 | Loss: 0.00001194
Iteration 13/1000 | Loss: 0.00001192
Iteration 14/1000 | Loss: 0.00001191
Iteration 15/1000 | Loss: 0.00001191
Iteration 16/1000 | Loss: 0.00001191
Iteration 17/1000 | Loss: 0.00001190
Iteration 18/1000 | Loss: 0.00001190
Iteration 19/1000 | Loss: 0.00001190
Iteration 20/1000 | Loss: 0.00001189
Iteration 21/1000 | Loss: 0.00001188
Iteration 22/1000 | Loss: 0.00001188
Iteration 23/1000 | Loss: 0.00001187
Iteration 24/1000 | Loss: 0.00001187
Iteration 25/1000 | Loss: 0.00001186
Iteration 26/1000 | Loss: 0.00001185
Iteration 27/1000 | Loss: 0.00001185
Iteration 28/1000 | Loss: 0.00001184
Iteration 29/1000 | Loss: 0.00001182
Iteration 30/1000 | Loss: 0.00001179
Iteration 31/1000 | Loss: 0.00001179
Iteration 32/1000 | Loss: 0.00001179
Iteration 33/1000 | Loss: 0.00001178
Iteration 34/1000 | Loss: 0.00001178
Iteration 35/1000 | Loss: 0.00001178
Iteration 36/1000 | Loss: 0.00001178
Iteration 37/1000 | Loss: 0.00001176
Iteration 38/1000 | Loss: 0.00001175
Iteration 39/1000 | Loss: 0.00001175
Iteration 40/1000 | Loss: 0.00001174
Iteration 41/1000 | Loss: 0.00001174
Iteration 42/1000 | Loss: 0.00001174
Iteration 43/1000 | Loss: 0.00001173
Iteration 44/1000 | Loss: 0.00001173
Iteration 45/1000 | Loss: 0.00001173
Iteration 46/1000 | Loss: 0.00001172
Iteration 47/1000 | Loss: 0.00001172
Iteration 48/1000 | Loss: 0.00001172
Iteration 49/1000 | Loss: 0.00001172
Iteration 50/1000 | Loss: 0.00001172
Iteration 51/1000 | Loss: 0.00001172
Iteration 52/1000 | Loss: 0.00001171
Iteration 53/1000 | Loss: 0.00001171
Iteration 54/1000 | Loss: 0.00001170
Iteration 55/1000 | Loss: 0.00001170
Iteration 56/1000 | Loss: 0.00001170
Iteration 57/1000 | Loss: 0.00001169
Iteration 58/1000 | Loss: 0.00001169
Iteration 59/1000 | Loss: 0.00001169
Iteration 60/1000 | Loss: 0.00001169
Iteration 61/1000 | Loss: 0.00001169
Iteration 62/1000 | Loss: 0.00001168
Iteration 63/1000 | Loss: 0.00001168
Iteration 64/1000 | Loss: 0.00001167
Iteration 65/1000 | Loss: 0.00001166
Iteration 66/1000 | Loss: 0.00001166
Iteration 67/1000 | Loss: 0.00001166
Iteration 68/1000 | Loss: 0.00001166
Iteration 69/1000 | Loss: 0.00001166
Iteration 70/1000 | Loss: 0.00001165
Iteration 71/1000 | Loss: 0.00001165
Iteration 72/1000 | Loss: 0.00001164
Iteration 73/1000 | Loss: 0.00001163
Iteration 74/1000 | Loss: 0.00001162
Iteration 75/1000 | Loss: 0.00001162
Iteration 76/1000 | Loss: 0.00001162
Iteration 77/1000 | Loss: 0.00001162
Iteration 78/1000 | Loss: 0.00001161
Iteration 79/1000 | Loss: 0.00001161
Iteration 80/1000 | Loss: 0.00001161
Iteration 81/1000 | Loss: 0.00001160
Iteration 82/1000 | Loss: 0.00001160
Iteration 83/1000 | Loss: 0.00001159
Iteration 84/1000 | Loss: 0.00001159
Iteration 85/1000 | Loss: 0.00001159
Iteration 86/1000 | Loss: 0.00001159
Iteration 87/1000 | Loss: 0.00001158
Iteration 88/1000 | Loss: 0.00001158
Iteration 89/1000 | Loss: 0.00001158
Iteration 90/1000 | Loss: 0.00001158
Iteration 91/1000 | Loss: 0.00001158
Iteration 92/1000 | Loss: 0.00001158
Iteration 93/1000 | Loss: 0.00001158
Iteration 94/1000 | Loss: 0.00001158
Iteration 95/1000 | Loss: 0.00001158
Iteration 96/1000 | Loss: 0.00001158
Iteration 97/1000 | Loss: 0.00001157
Iteration 98/1000 | Loss: 0.00001157
Iteration 99/1000 | Loss: 0.00001157
Iteration 100/1000 | Loss: 0.00001157
Iteration 101/1000 | Loss: 0.00001157
Iteration 102/1000 | Loss: 0.00001157
Iteration 103/1000 | Loss: 0.00001157
Iteration 104/1000 | Loss: 0.00001157
Iteration 105/1000 | Loss: 0.00001156
Iteration 106/1000 | Loss: 0.00001156
Iteration 107/1000 | Loss: 0.00001156
Iteration 108/1000 | Loss: 0.00001156
Iteration 109/1000 | Loss: 0.00001155
Iteration 110/1000 | Loss: 0.00001155
Iteration 111/1000 | Loss: 0.00001155
Iteration 112/1000 | Loss: 0.00001155
Iteration 113/1000 | Loss: 0.00001155
Iteration 114/1000 | Loss: 0.00001154
Iteration 115/1000 | Loss: 0.00001154
Iteration 116/1000 | Loss: 0.00001154
Iteration 117/1000 | Loss: 0.00001154
Iteration 118/1000 | Loss: 0.00001154
Iteration 119/1000 | Loss: 0.00001154
Iteration 120/1000 | Loss: 0.00001154
Iteration 121/1000 | Loss: 0.00001154
Iteration 122/1000 | Loss: 0.00001154
Iteration 123/1000 | Loss: 0.00001154
Iteration 124/1000 | Loss: 0.00001154
Iteration 125/1000 | Loss: 0.00001154
Iteration 126/1000 | Loss: 0.00001153
Iteration 127/1000 | Loss: 0.00001153
Iteration 128/1000 | Loss: 0.00001153
Iteration 129/1000 | Loss: 0.00001153
Iteration 130/1000 | Loss: 0.00001153
Iteration 131/1000 | Loss: 0.00001153
Iteration 132/1000 | Loss: 0.00001153
Iteration 133/1000 | Loss: 0.00001153
Iteration 134/1000 | Loss: 0.00001153
Iteration 135/1000 | Loss: 0.00001153
Iteration 136/1000 | Loss: 0.00001153
Iteration 137/1000 | Loss: 0.00001152
Iteration 138/1000 | Loss: 0.00001152
Iteration 139/1000 | Loss: 0.00001152
Iteration 140/1000 | Loss: 0.00001152
Iteration 141/1000 | Loss: 0.00001152
Iteration 142/1000 | Loss: 0.00001152
Iteration 143/1000 | Loss: 0.00001152
Iteration 144/1000 | Loss: 0.00001152
Iteration 145/1000 | Loss: 0.00001152
Iteration 146/1000 | Loss: 0.00001152
Iteration 147/1000 | Loss: 0.00001152
Iteration 148/1000 | Loss: 0.00001152
Iteration 149/1000 | Loss: 0.00001152
Iteration 150/1000 | Loss: 0.00001152
Iteration 151/1000 | Loss: 0.00001152
Iteration 152/1000 | Loss: 0.00001152
Iteration 153/1000 | Loss: 0.00001152
Iteration 154/1000 | Loss: 0.00001152
Iteration 155/1000 | Loss: 0.00001152
Iteration 156/1000 | Loss: 0.00001152
Iteration 157/1000 | Loss: 0.00001152
Iteration 158/1000 | Loss: 0.00001152
Iteration 159/1000 | Loss: 0.00001152
Iteration 160/1000 | Loss: 0.00001152
Iteration 161/1000 | Loss: 0.00001152
Iteration 162/1000 | Loss: 0.00001152
Iteration 163/1000 | Loss: 0.00001152
Iteration 164/1000 | Loss: 0.00001152
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.1515136975503992e-05, 1.1515136975503992e-05, 1.1515136975503992e-05, 1.1515136975503992e-05, 1.1515136975503992e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1515136975503992e-05

Optimization complete. Final v2v error: 2.7301993370056152 mm

Highest mean error: 3.659062623977661 mm for frame 218

Lowest mean error: 2.317551851272583 mm for frame 151

Saving results

Total time: 39.12461447715759
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00527134
Iteration 2/25 | Loss: 0.00129862
Iteration 3/25 | Loss: 0.00119515
Iteration 4/25 | Loss: 0.00117556
Iteration 5/25 | Loss: 0.00116865
Iteration 6/25 | Loss: 0.00116726
Iteration 7/25 | Loss: 0.00116710
Iteration 8/25 | Loss: 0.00116710
Iteration 9/25 | Loss: 0.00116710
Iteration 10/25 | Loss: 0.00116710
Iteration 11/25 | Loss: 0.00116710
Iteration 12/25 | Loss: 0.00116710
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011671013198792934, 0.0011671013198792934, 0.0011671013198792934, 0.0011671013198792934, 0.0011671013198792934]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011671013198792934

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35424256
Iteration 2/25 | Loss: 0.00110331
Iteration 3/25 | Loss: 0.00110330
Iteration 4/25 | Loss: 0.00110330
Iteration 5/25 | Loss: 0.00110330
Iteration 6/25 | Loss: 0.00110330
Iteration 7/25 | Loss: 0.00110330
Iteration 8/25 | Loss: 0.00110330
Iteration 9/25 | Loss: 0.00110330
Iteration 10/25 | Loss: 0.00110330
Iteration 11/25 | Loss: 0.00110330
Iteration 12/25 | Loss: 0.00110330
Iteration 13/25 | Loss: 0.00110330
Iteration 14/25 | Loss: 0.00110330
Iteration 15/25 | Loss: 0.00110330
Iteration 16/25 | Loss: 0.00110330
Iteration 17/25 | Loss: 0.00110330
Iteration 18/25 | Loss: 0.00110330
Iteration 19/25 | Loss: 0.00110330
Iteration 20/25 | Loss: 0.00110330
Iteration 21/25 | Loss: 0.00110330
Iteration 22/25 | Loss: 0.00110330
Iteration 23/25 | Loss: 0.00110330
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0011033009504899383, 0.0011033009504899383, 0.0011033009504899383, 0.0011033009504899383, 0.0011033009504899383]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011033009504899383

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110330
Iteration 2/1000 | Loss: 0.00006477
Iteration 3/1000 | Loss: 0.00003924
Iteration 4/1000 | Loss: 0.00003013
Iteration 5/1000 | Loss: 0.00002723
Iteration 6/1000 | Loss: 0.00002588
Iteration 7/1000 | Loss: 0.00002483
Iteration 8/1000 | Loss: 0.00002427
Iteration 9/1000 | Loss: 0.00002388
Iteration 10/1000 | Loss: 0.00002344
Iteration 11/1000 | Loss: 0.00002319
Iteration 12/1000 | Loss: 0.00002296
Iteration 13/1000 | Loss: 0.00002277
Iteration 14/1000 | Loss: 0.00002273
Iteration 15/1000 | Loss: 0.00002272
Iteration 16/1000 | Loss: 0.00002270
Iteration 17/1000 | Loss: 0.00002262
Iteration 18/1000 | Loss: 0.00002257
Iteration 19/1000 | Loss: 0.00002251
Iteration 20/1000 | Loss: 0.00002250
Iteration 21/1000 | Loss: 0.00002249
Iteration 22/1000 | Loss: 0.00002246
Iteration 23/1000 | Loss: 0.00002242
Iteration 24/1000 | Loss: 0.00002242
Iteration 25/1000 | Loss: 0.00002241
Iteration 26/1000 | Loss: 0.00002240
Iteration 27/1000 | Loss: 0.00002239
Iteration 28/1000 | Loss: 0.00002238
Iteration 29/1000 | Loss: 0.00002238
Iteration 30/1000 | Loss: 0.00002237
Iteration 31/1000 | Loss: 0.00002236
Iteration 32/1000 | Loss: 0.00002236
Iteration 33/1000 | Loss: 0.00002235
Iteration 34/1000 | Loss: 0.00002235
Iteration 35/1000 | Loss: 0.00002234
Iteration 36/1000 | Loss: 0.00002233
Iteration 37/1000 | Loss: 0.00002233
Iteration 38/1000 | Loss: 0.00002233
Iteration 39/1000 | Loss: 0.00002232
Iteration 40/1000 | Loss: 0.00002232
Iteration 41/1000 | Loss: 0.00002231
Iteration 42/1000 | Loss: 0.00002231
Iteration 43/1000 | Loss: 0.00002227
Iteration 44/1000 | Loss: 0.00002226
Iteration 45/1000 | Loss: 0.00002226
Iteration 46/1000 | Loss: 0.00002223
Iteration 47/1000 | Loss: 0.00002222
Iteration 48/1000 | Loss: 0.00002222
Iteration 49/1000 | Loss: 0.00002221
Iteration 50/1000 | Loss: 0.00002220
Iteration 51/1000 | Loss: 0.00002220
Iteration 52/1000 | Loss: 0.00002219
Iteration 53/1000 | Loss: 0.00002219
Iteration 54/1000 | Loss: 0.00002219
Iteration 55/1000 | Loss: 0.00002218
Iteration 56/1000 | Loss: 0.00002218
Iteration 57/1000 | Loss: 0.00002217
Iteration 58/1000 | Loss: 0.00002216
Iteration 59/1000 | Loss: 0.00002215
Iteration 60/1000 | Loss: 0.00002215
Iteration 61/1000 | Loss: 0.00002214
Iteration 62/1000 | Loss: 0.00002214
Iteration 63/1000 | Loss: 0.00002213
Iteration 64/1000 | Loss: 0.00002213
Iteration 65/1000 | Loss: 0.00002213
Iteration 66/1000 | Loss: 0.00002212
Iteration 67/1000 | Loss: 0.00002212
Iteration 68/1000 | Loss: 0.00002212
Iteration 69/1000 | Loss: 0.00002211
Iteration 70/1000 | Loss: 0.00002211
Iteration 71/1000 | Loss: 0.00002211
Iteration 72/1000 | Loss: 0.00002210
Iteration 73/1000 | Loss: 0.00002210
Iteration 74/1000 | Loss: 0.00002210
Iteration 75/1000 | Loss: 0.00002210
Iteration 76/1000 | Loss: 0.00002210
Iteration 77/1000 | Loss: 0.00002210
Iteration 78/1000 | Loss: 0.00002209
Iteration 79/1000 | Loss: 0.00002209
Iteration 80/1000 | Loss: 0.00002209
Iteration 81/1000 | Loss: 0.00002208
Iteration 82/1000 | Loss: 0.00002208
Iteration 83/1000 | Loss: 0.00002208
Iteration 84/1000 | Loss: 0.00002207
Iteration 85/1000 | Loss: 0.00002207
Iteration 86/1000 | Loss: 0.00002207
Iteration 87/1000 | Loss: 0.00002206
Iteration 88/1000 | Loss: 0.00002206
Iteration 89/1000 | Loss: 0.00002206
Iteration 90/1000 | Loss: 0.00002205
Iteration 91/1000 | Loss: 0.00002205
Iteration 92/1000 | Loss: 0.00002205
Iteration 93/1000 | Loss: 0.00002205
Iteration 94/1000 | Loss: 0.00002204
Iteration 95/1000 | Loss: 0.00002204
Iteration 96/1000 | Loss: 0.00002204
Iteration 97/1000 | Loss: 0.00002204
Iteration 98/1000 | Loss: 0.00002203
Iteration 99/1000 | Loss: 0.00002203
Iteration 100/1000 | Loss: 0.00002203
Iteration 101/1000 | Loss: 0.00002203
Iteration 102/1000 | Loss: 0.00002203
Iteration 103/1000 | Loss: 0.00002202
Iteration 104/1000 | Loss: 0.00002202
Iteration 105/1000 | Loss: 0.00002202
Iteration 106/1000 | Loss: 0.00002202
Iteration 107/1000 | Loss: 0.00002201
Iteration 108/1000 | Loss: 0.00002201
Iteration 109/1000 | Loss: 0.00002201
Iteration 110/1000 | Loss: 0.00002201
Iteration 111/1000 | Loss: 0.00002200
Iteration 112/1000 | Loss: 0.00002200
Iteration 113/1000 | Loss: 0.00002200
Iteration 114/1000 | Loss: 0.00002200
Iteration 115/1000 | Loss: 0.00002200
Iteration 116/1000 | Loss: 0.00002200
Iteration 117/1000 | Loss: 0.00002200
Iteration 118/1000 | Loss: 0.00002200
Iteration 119/1000 | Loss: 0.00002200
Iteration 120/1000 | Loss: 0.00002199
Iteration 121/1000 | Loss: 0.00002199
Iteration 122/1000 | Loss: 0.00002199
Iteration 123/1000 | Loss: 0.00002199
Iteration 124/1000 | Loss: 0.00002198
Iteration 125/1000 | Loss: 0.00002198
Iteration 126/1000 | Loss: 0.00002198
Iteration 127/1000 | Loss: 0.00002198
Iteration 128/1000 | Loss: 0.00002198
Iteration 129/1000 | Loss: 0.00002197
Iteration 130/1000 | Loss: 0.00002197
Iteration 131/1000 | Loss: 0.00002197
Iteration 132/1000 | Loss: 0.00002197
Iteration 133/1000 | Loss: 0.00002197
Iteration 134/1000 | Loss: 0.00002197
Iteration 135/1000 | Loss: 0.00002197
Iteration 136/1000 | Loss: 0.00002197
Iteration 137/1000 | Loss: 0.00002197
Iteration 138/1000 | Loss: 0.00002197
Iteration 139/1000 | Loss: 0.00002197
Iteration 140/1000 | Loss: 0.00002197
Iteration 141/1000 | Loss: 0.00002196
Iteration 142/1000 | Loss: 0.00002196
Iteration 143/1000 | Loss: 0.00002196
Iteration 144/1000 | Loss: 0.00002196
Iteration 145/1000 | Loss: 0.00002196
Iteration 146/1000 | Loss: 0.00002196
Iteration 147/1000 | Loss: 0.00002196
Iteration 148/1000 | Loss: 0.00002196
Iteration 149/1000 | Loss: 0.00002196
Iteration 150/1000 | Loss: 0.00002196
Iteration 151/1000 | Loss: 0.00002196
Iteration 152/1000 | Loss: 0.00002195
Iteration 153/1000 | Loss: 0.00002195
Iteration 154/1000 | Loss: 0.00002195
Iteration 155/1000 | Loss: 0.00002195
Iteration 156/1000 | Loss: 0.00002195
Iteration 157/1000 | Loss: 0.00002195
Iteration 158/1000 | Loss: 0.00002195
Iteration 159/1000 | Loss: 0.00002195
Iteration 160/1000 | Loss: 0.00002195
Iteration 161/1000 | Loss: 0.00002194
Iteration 162/1000 | Loss: 0.00002194
Iteration 163/1000 | Loss: 0.00002194
Iteration 164/1000 | Loss: 0.00002194
Iteration 165/1000 | Loss: 0.00002194
Iteration 166/1000 | Loss: 0.00002194
Iteration 167/1000 | Loss: 0.00002194
Iteration 168/1000 | Loss: 0.00002194
Iteration 169/1000 | Loss: 0.00002194
Iteration 170/1000 | Loss: 0.00002194
Iteration 171/1000 | Loss: 0.00002194
Iteration 172/1000 | Loss: 0.00002193
Iteration 173/1000 | Loss: 0.00002193
Iteration 174/1000 | Loss: 0.00002193
Iteration 175/1000 | Loss: 0.00002193
Iteration 176/1000 | Loss: 0.00002193
Iteration 177/1000 | Loss: 0.00002193
Iteration 178/1000 | Loss: 0.00002193
Iteration 179/1000 | Loss: 0.00002192
Iteration 180/1000 | Loss: 0.00002192
Iteration 181/1000 | Loss: 0.00002192
Iteration 182/1000 | Loss: 0.00002192
Iteration 183/1000 | Loss: 0.00002192
Iteration 184/1000 | Loss: 0.00002192
Iteration 185/1000 | Loss: 0.00002192
Iteration 186/1000 | Loss: 0.00002192
Iteration 187/1000 | Loss: 0.00002191
Iteration 188/1000 | Loss: 0.00002191
Iteration 189/1000 | Loss: 0.00002191
Iteration 190/1000 | Loss: 0.00002191
Iteration 191/1000 | Loss: 0.00002191
Iteration 192/1000 | Loss: 0.00002191
Iteration 193/1000 | Loss: 0.00002191
Iteration 194/1000 | Loss: 0.00002191
Iteration 195/1000 | Loss: 0.00002190
Iteration 196/1000 | Loss: 0.00002190
Iteration 197/1000 | Loss: 0.00002190
Iteration 198/1000 | Loss: 0.00002190
Iteration 199/1000 | Loss: 0.00002190
Iteration 200/1000 | Loss: 0.00002190
Iteration 201/1000 | Loss: 0.00002190
Iteration 202/1000 | Loss: 0.00002190
Iteration 203/1000 | Loss: 0.00002189
Iteration 204/1000 | Loss: 0.00002189
Iteration 205/1000 | Loss: 0.00002189
Iteration 206/1000 | Loss: 0.00002189
Iteration 207/1000 | Loss: 0.00002189
Iteration 208/1000 | Loss: 0.00002189
Iteration 209/1000 | Loss: 0.00002189
Iteration 210/1000 | Loss: 0.00002189
Iteration 211/1000 | Loss: 0.00002189
Iteration 212/1000 | Loss: 0.00002189
Iteration 213/1000 | Loss: 0.00002189
Iteration 214/1000 | Loss: 0.00002188
Iteration 215/1000 | Loss: 0.00002188
Iteration 216/1000 | Loss: 0.00002188
Iteration 217/1000 | Loss: 0.00002188
Iteration 218/1000 | Loss: 0.00002188
Iteration 219/1000 | Loss: 0.00002188
Iteration 220/1000 | Loss: 0.00002188
Iteration 221/1000 | Loss: 0.00002188
Iteration 222/1000 | Loss: 0.00002188
Iteration 223/1000 | Loss: 0.00002188
Iteration 224/1000 | Loss: 0.00002187
Iteration 225/1000 | Loss: 0.00002187
Iteration 226/1000 | Loss: 0.00002187
Iteration 227/1000 | Loss: 0.00002187
Iteration 228/1000 | Loss: 0.00002187
Iteration 229/1000 | Loss: 0.00002187
Iteration 230/1000 | Loss: 0.00002187
Iteration 231/1000 | Loss: 0.00002186
Iteration 232/1000 | Loss: 0.00002186
Iteration 233/1000 | Loss: 0.00002186
Iteration 234/1000 | Loss: 0.00002186
Iteration 235/1000 | Loss: 0.00002186
Iteration 236/1000 | Loss: 0.00002186
Iteration 237/1000 | Loss: 0.00002186
Iteration 238/1000 | Loss: 0.00002186
Iteration 239/1000 | Loss: 0.00002185
Iteration 240/1000 | Loss: 0.00002185
Iteration 241/1000 | Loss: 0.00002185
Iteration 242/1000 | Loss: 0.00002185
Iteration 243/1000 | Loss: 0.00002185
Iteration 244/1000 | Loss: 0.00002185
Iteration 245/1000 | Loss: 0.00002185
Iteration 246/1000 | Loss: 0.00002185
Iteration 247/1000 | Loss: 0.00002185
Iteration 248/1000 | Loss: 0.00002185
Iteration 249/1000 | Loss: 0.00002185
Iteration 250/1000 | Loss: 0.00002185
Iteration 251/1000 | Loss: 0.00002185
Iteration 252/1000 | Loss: 0.00002185
Iteration 253/1000 | Loss: 0.00002185
Iteration 254/1000 | Loss: 0.00002184
Iteration 255/1000 | Loss: 0.00002184
Iteration 256/1000 | Loss: 0.00002184
Iteration 257/1000 | Loss: 0.00002184
Iteration 258/1000 | Loss: 0.00002184
Iteration 259/1000 | Loss: 0.00002184
Iteration 260/1000 | Loss: 0.00002184
Iteration 261/1000 | Loss: 0.00002184
Iteration 262/1000 | Loss: 0.00002184
Iteration 263/1000 | Loss: 0.00002184
Iteration 264/1000 | Loss: 0.00002184
Iteration 265/1000 | Loss: 0.00002184
Iteration 266/1000 | Loss: 0.00002184
Iteration 267/1000 | Loss: 0.00002184
Iteration 268/1000 | Loss: 0.00002184
Iteration 269/1000 | Loss: 0.00002184
Iteration 270/1000 | Loss: 0.00002184
Iteration 271/1000 | Loss: 0.00002184
Iteration 272/1000 | Loss: 0.00002184
Iteration 273/1000 | Loss: 0.00002184
Iteration 274/1000 | Loss: 0.00002184
Iteration 275/1000 | Loss: 0.00002184
Iteration 276/1000 | Loss: 0.00002184
Iteration 277/1000 | Loss: 0.00002184
Iteration 278/1000 | Loss: 0.00002184
Iteration 279/1000 | Loss: 0.00002184
Iteration 280/1000 | Loss: 0.00002184
Iteration 281/1000 | Loss: 0.00002184
Iteration 282/1000 | Loss: 0.00002184
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 282. Stopping optimization.
Last 5 losses: [2.1835247025592253e-05, 2.1835247025592253e-05, 2.1835247025592253e-05, 2.1835247025592253e-05, 2.1835247025592253e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1835247025592253e-05

Optimization complete. Final v2v error: 3.703866720199585 mm

Highest mean error: 4.427961349487305 mm for frame 110

Lowest mean error: 2.73479962348938 mm for frame 8

Saving results

Total time: 49.49359583854675
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00273883
Iteration 2/25 | Loss: 0.00130385
Iteration 3/25 | Loss: 0.00114394
Iteration 4/25 | Loss: 0.00110632
Iteration 5/25 | Loss: 0.00109895
Iteration 6/25 | Loss: 0.00109738
Iteration 7/25 | Loss: 0.00109680
Iteration 8/25 | Loss: 0.00109680
Iteration 9/25 | Loss: 0.00109680
Iteration 10/25 | Loss: 0.00109680
Iteration 11/25 | Loss: 0.00109680
Iteration 12/25 | Loss: 0.00109680
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001096796477213502, 0.001096796477213502, 0.001096796477213502, 0.001096796477213502, 0.001096796477213502]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001096796477213502

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33727849
Iteration 2/25 | Loss: 0.00106905
Iteration 3/25 | Loss: 0.00106905
Iteration 4/25 | Loss: 0.00106905
Iteration 5/25 | Loss: 0.00106905
Iteration 6/25 | Loss: 0.00106905
Iteration 7/25 | Loss: 0.00106905
Iteration 8/25 | Loss: 0.00106905
Iteration 9/25 | Loss: 0.00106905
Iteration 10/25 | Loss: 0.00106905
Iteration 11/25 | Loss: 0.00106905
Iteration 12/25 | Loss: 0.00106905
Iteration 13/25 | Loss: 0.00106905
Iteration 14/25 | Loss: 0.00106905
Iteration 15/25 | Loss: 0.00106905
Iteration 16/25 | Loss: 0.00106905
Iteration 17/25 | Loss: 0.00106905
Iteration 18/25 | Loss: 0.00106905
Iteration 19/25 | Loss: 0.00106905
Iteration 20/25 | Loss: 0.00106905
Iteration 21/25 | Loss: 0.00106905
Iteration 22/25 | Loss: 0.00106905
Iteration 23/25 | Loss: 0.00106905
Iteration 24/25 | Loss: 0.00106905
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0010690450435504317, 0.0010690450435504317, 0.0010690450435504317, 0.0010690450435504317, 0.0010690450435504317]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010690450435504317

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106904
Iteration 2/1000 | Loss: 0.00003814
Iteration 3/1000 | Loss: 0.00002205
Iteration 4/1000 | Loss: 0.00001876
Iteration 5/1000 | Loss: 0.00001762
Iteration 6/1000 | Loss: 0.00001705
Iteration 7/1000 | Loss: 0.00001660
Iteration 8/1000 | Loss: 0.00001621
Iteration 9/1000 | Loss: 0.00001581
Iteration 10/1000 | Loss: 0.00001549
Iteration 11/1000 | Loss: 0.00001520
Iteration 12/1000 | Loss: 0.00001502
Iteration 13/1000 | Loss: 0.00001484
Iteration 14/1000 | Loss: 0.00001474
Iteration 15/1000 | Loss: 0.00001472
Iteration 16/1000 | Loss: 0.00001472
Iteration 17/1000 | Loss: 0.00001470
Iteration 18/1000 | Loss: 0.00001469
Iteration 19/1000 | Loss: 0.00001465
Iteration 20/1000 | Loss: 0.00001463
Iteration 21/1000 | Loss: 0.00001463
Iteration 22/1000 | Loss: 0.00001462
Iteration 23/1000 | Loss: 0.00001462
Iteration 24/1000 | Loss: 0.00001462
Iteration 25/1000 | Loss: 0.00001462
Iteration 26/1000 | Loss: 0.00001462
Iteration 27/1000 | Loss: 0.00001462
Iteration 28/1000 | Loss: 0.00001461
Iteration 29/1000 | Loss: 0.00001461
Iteration 30/1000 | Loss: 0.00001461
Iteration 31/1000 | Loss: 0.00001461
Iteration 32/1000 | Loss: 0.00001460
Iteration 33/1000 | Loss: 0.00001459
Iteration 34/1000 | Loss: 0.00001459
Iteration 35/1000 | Loss: 0.00001459
Iteration 36/1000 | Loss: 0.00001459
Iteration 37/1000 | Loss: 0.00001459
Iteration 38/1000 | Loss: 0.00001459
Iteration 39/1000 | Loss: 0.00001458
Iteration 40/1000 | Loss: 0.00001458
Iteration 41/1000 | Loss: 0.00001458
Iteration 42/1000 | Loss: 0.00001457
Iteration 43/1000 | Loss: 0.00001457
Iteration 44/1000 | Loss: 0.00001457
Iteration 45/1000 | Loss: 0.00001457
Iteration 46/1000 | Loss: 0.00001457
Iteration 47/1000 | Loss: 0.00001456
Iteration 48/1000 | Loss: 0.00001456
Iteration 49/1000 | Loss: 0.00001456
Iteration 50/1000 | Loss: 0.00001455
Iteration 51/1000 | Loss: 0.00001455
Iteration 52/1000 | Loss: 0.00001455
Iteration 53/1000 | Loss: 0.00001455
Iteration 54/1000 | Loss: 0.00001455
Iteration 55/1000 | Loss: 0.00001454
Iteration 56/1000 | Loss: 0.00001454
Iteration 57/1000 | Loss: 0.00001454
Iteration 58/1000 | Loss: 0.00001454
Iteration 59/1000 | Loss: 0.00001454
Iteration 60/1000 | Loss: 0.00001454
Iteration 61/1000 | Loss: 0.00001454
Iteration 62/1000 | Loss: 0.00001454
Iteration 63/1000 | Loss: 0.00001454
Iteration 64/1000 | Loss: 0.00001454
Iteration 65/1000 | Loss: 0.00001454
Iteration 66/1000 | Loss: 0.00001454
Iteration 67/1000 | Loss: 0.00001454
Iteration 68/1000 | Loss: 0.00001454
Iteration 69/1000 | Loss: 0.00001454
Iteration 70/1000 | Loss: 0.00001454
Iteration 71/1000 | Loss: 0.00001454
Iteration 72/1000 | Loss: 0.00001454
Iteration 73/1000 | Loss: 0.00001454
Iteration 74/1000 | Loss: 0.00001454
Iteration 75/1000 | Loss: 0.00001454
Iteration 76/1000 | Loss: 0.00001454
Iteration 77/1000 | Loss: 0.00001454
Iteration 78/1000 | Loss: 0.00001454
Iteration 79/1000 | Loss: 0.00001454
Iteration 80/1000 | Loss: 0.00001454
Iteration 81/1000 | Loss: 0.00001454
Iteration 82/1000 | Loss: 0.00001454
Iteration 83/1000 | Loss: 0.00001454
Iteration 84/1000 | Loss: 0.00001454
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 84. Stopping optimization.
Last 5 losses: [1.4535545233229641e-05, 1.4535545233229641e-05, 1.4535545233229641e-05, 1.4535545233229641e-05, 1.4535545233229641e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4535545233229641e-05

Optimization complete. Final v2v error: 3.1939656734466553 mm

Highest mean error: 3.6500420570373535 mm for frame 88

Lowest mean error: 2.9157607555389404 mm for frame 0

Saving results

Total time: 35.04395842552185
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01021744
Iteration 2/25 | Loss: 0.00247922
Iteration 3/25 | Loss: 0.00212792
Iteration 4/25 | Loss: 0.00188606
Iteration 5/25 | Loss: 0.00190737
Iteration 6/25 | Loss: 0.00152615
Iteration 7/25 | Loss: 0.00132723
Iteration 8/25 | Loss: 0.00126427
Iteration 9/25 | Loss: 0.00125609
Iteration 10/25 | Loss: 0.00125298
Iteration 11/25 | Loss: 0.00124564
Iteration 12/25 | Loss: 0.00124748
Iteration 13/25 | Loss: 0.00124171
Iteration 14/25 | Loss: 0.00124087
Iteration 15/25 | Loss: 0.00123458
Iteration 16/25 | Loss: 0.00123376
Iteration 17/25 | Loss: 0.00123242
Iteration 18/25 | Loss: 0.00123226
Iteration 19/25 | Loss: 0.00123123
Iteration 20/25 | Loss: 0.00123103
Iteration 21/25 | Loss: 0.00123033
Iteration 22/25 | Loss: 0.00123003
Iteration 23/25 | Loss: 0.00122969
Iteration 24/25 | Loss: 0.00122846
Iteration 25/25 | Loss: 0.00123005

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39010346
Iteration 2/25 | Loss: 0.00069551
Iteration 3/25 | Loss: 0.00069551
Iteration 4/25 | Loss: 0.00069550
Iteration 5/25 | Loss: 0.00069550
Iteration 6/25 | Loss: 0.00069550
Iteration 7/25 | Loss: 0.00069550
Iteration 8/25 | Loss: 0.00069550
Iteration 9/25 | Loss: 0.00069550
Iteration 10/25 | Loss: 0.00069550
Iteration 11/25 | Loss: 0.00069550
Iteration 12/25 | Loss: 0.00069550
Iteration 13/25 | Loss: 0.00069550
Iteration 14/25 | Loss: 0.00069550
Iteration 15/25 | Loss: 0.00069550
Iteration 16/25 | Loss: 0.00069550
Iteration 17/25 | Loss: 0.00069550
Iteration 18/25 | Loss: 0.00069550
Iteration 19/25 | Loss: 0.00069550
Iteration 20/25 | Loss: 0.00069550
Iteration 21/25 | Loss: 0.00069550
Iteration 22/25 | Loss: 0.00069550
Iteration 23/25 | Loss: 0.00069550
Iteration 24/25 | Loss: 0.00069550
Iteration 25/25 | Loss: 0.00069550

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069550
Iteration 2/1000 | Loss: 0.00004656
Iteration 3/1000 | Loss: 0.00004424
Iteration 4/1000 | Loss: 0.00002838
Iteration 5/1000 | Loss: 0.00003767
Iteration 6/1000 | Loss: 0.00002977
Iteration 7/1000 | Loss: 0.00004403
Iteration 8/1000 | Loss: 0.00003876
Iteration 9/1000 | Loss: 0.00004483
Iteration 10/1000 | Loss: 0.00003141
Iteration 11/1000 | Loss: 0.00003400
Iteration 12/1000 | Loss: 0.00004420
Iteration 13/1000 | Loss: 0.00004822
Iteration 14/1000 | Loss: 0.00003232
Iteration 15/1000 | Loss: 0.00002700
Iteration 16/1000 | Loss: 0.00002569
Iteration 17/1000 | Loss: 0.00002462
Iteration 18/1000 | Loss: 0.00002415
Iteration 19/1000 | Loss: 0.00002390
Iteration 20/1000 | Loss: 0.00002377
Iteration 21/1000 | Loss: 0.00002376
Iteration 22/1000 | Loss: 0.00002362
Iteration 23/1000 | Loss: 0.00002354
Iteration 24/1000 | Loss: 0.00002346
Iteration 25/1000 | Loss: 0.00002342
Iteration 26/1000 | Loss: 0.00002333
Iteration 27/1000 | Loss: 0.00002333
Iteration 28/1000 | Loss: 0.00002330
Iteration 29/1000 | Loss: 0.00002322
Iteration 30/1000 | Loss: 0.00002320
Iteration 31/1000 | Loss: 0.00002320
Iteration 32/1000 | Loss: 0.00002319
Iteration 33/1000 | Loss: 0.00002319
Iteration 34/1000 | Loss: 0.00002319
Iteration 35/1000 | Loss: 0.00002319
Iteration 36/1000 | Loss: 0.00002319
Iteration 37/1000 | Loss: 0.00002319
Iteration 38/1000 | Loss: 0.00002319
Iteration 39/1000 | Loss: 0.00002319
Iteration 40/1000 | Loss: 0.00002318
Iteration 41/1000 | Loss: 0.00002318
Iteration 42/1000 | Loss: 0.00002317
Iteration 43/1000 | Loss: 0.00002315
Iteration 44/1000 | Loss: 0.00002315
Iteration 45/1000 | Loss: 0.00002315
Iteration 46/1000 | Loss: 0.00002315
Iteration 47/1000 | Loss: 0.00002315
Iteration 48/1000 | Loss: 0.00002315
Iteration 49/1000 | Loss: 0.00002314
Iteration 50/1000 | Loss: 0.00002314
Iteration 51/1000 | Loss: 0.00002314
Iteration 52/1000 | Loss: 0.00002312
Iteration 53/1000 | Loss: 0.00002312
Iteration 54/1000 | Loss: 0.00002311
Iteration 55/1000 | Loss: 0.00002311
Iteration 56/1000 | Loss: 0.00002311
Iteration 57/1000 | Loss: 0.00002311
Iteration 58/1000 | Loss: 0.00002310
Iteration 59/1000 | Loss: 0.00002310
Iteration 60/1000 | Loss: 0.00002310
Iteration 61/1000 | Loss: 0.00002310
Iteration 62/1000 | Loss: 0.00002310
Iteration 63/1000 | Loss: 0.00002310
Iteration 64/1000 | Loss: 0.00002310
Iteration 65/1000 | Loss: 0.00002310
Iteration 66/1000 | Loss: 0.00002310
Iteration 67/1000 | Loss: 0.00002309
Iteration 68/1000 | Loss: 0.00002309
Iteration 69/1000 | Loss: 0.00002309
Iteration 70/1000 | Loss: 0.00002309
Iteration 71/1000 | Loss: 0.00002309
Iteration 72/1000 | Loss: 0.00002309
Iteration 73/1000 | Loss: 0.00002309
Iteration 74/1000 | Loss: 0.00002309
Iteration 75/1000 | Loss: 0.00002309
Iteration 76/1000 | Loss: 0.00002309
Iteration 77/1000 | Loss: 0.00002308
Iteration 78/1000 | Loss: 0.00002308
Iteration 79/1000 | Loss: 0.00002308
Iteration 80/1000 | Loss: 0.00002308
Iteration 81/1000 | Loss: 0.00002308
Iteration 82/1000 | Loss: 0.00002307
Iteration 83/1000 | Loss: 0.00002307
Iteration 84/1000 | Loss: 0.00002307
Iteration 85/1000 | Loss: 0.00002307
Iteration 86/1000 | Loss: 0.00002307
Iteration 87/1000 | Loss: 0.00002306
Iteration 88/1000 | Loss: 0.00002306
Iteration 89/1000 | Loss: 0.00002306
Iteration 90/1000 | Loss: 0.00002306
Iteration 91/1000 | Loss: 0.00002306
Iteration 92/1000 | Loss: 0.00002306
Iteration 93/1000 | Loss: 0.00002306
Iteration 94/1000 | Loss: 0.00002305
Iteration 95/1000 | Loss: 0.00002305
Iteration 96/1000 | Loss: 0.00002305
Iteration 97/1000 | Loss: 0.00002305
Iteration 98/1000 | Loss: 0.00002305
Iteration 99/1000 | Loss: 0.00002305
Iteration 100/1000 | Loss: 0.00002305
Iteration 101/1000 | Loss: 0.00002304
Iteration 102/1000 | Loss: 0.00002304
Iteration 103/1000 | Loss: 0.00002304
Iteration 104/1000 | Loss: 0.00002304
Iteration 105/1000 | Loss: 0.00002304
Iteration 106/1000 | Loss: 0.00002304
Iteration 107/1000 | Loss: 0.00002303
Iteration 108/1000 | Loss: 0.00002303
Iteration 109/1000 | Loss: 0.00002303
Iteration 110/1000 | Loss: 0.00002303
Iteration 111/1000 | Loss: 0.00002303
Iteration 112/1000 | Loss: 0.00002302
Iteration 113/1000 | Loss: 0.00002302
Iteration 114/1000 | Loss: 0.00002302
Iteration 115/1000 | Loss: 0.00002302
Iteration 116/1000 | Loss: 0.00002302
Iteration 117/1000 | Loss: 0.00002302
Iteration 118/1000 | Loss: 0.00002302
Iteration 119/1000 | Loss: 0.00002302
Iteration 120/1000 | Loss: 0.00002302
Iteration 121/1000 | Loss: 0.00002302
Iteration 122/1000 | Loss: 0.00002301
Iteration 123/1000 | Loss: 0.00002301
Iteration 124/1000 | Loss: 0.00002301
Iteration 125/1000 | Loss: 0.00002301
Iteration 126/1000 | Loss: 0.00002301
Iteration 127/1000 | Loss: 0.00002301
Iteration 128/1000 | Loss: 0.00002301
Iteration 129/1000 | Loss: 0.00002301
Iteration 130/1000 | Loss: 0.00002301
Iteration 131/1000 | Loss: 0.00002301
Iteration 132/1000 | Loss: 0.00002301
Iteration 133/1000 | Loss: 0.00002301
Iteration 134/1000 | Loss: 0.00002301
Iteration 135/1000 | Loss: 0.00002301
Iteration 136/1000 | Loss: 0.00002301
Iteration 137/1000 | Loss: 0.00002301
Iteration 138/1000 | Loss: 0.00002301
Iteration 139/1000 | Loss: 0.00002300
Iteration 140/1000 | Loss: 0.00002300
Iteration 141/1000 | Loss: 0.00002300
Iteration 142/1000 | Loss: 0.00002300
Iteration 143/1000 | Loss: 0.00002300
Iteration 144/1000 | Loss: 0.00002300
Iteration 145/1000 | Loss: 0.00002300
Iteration 146/1000 | Loss: 0.00002300
Iteration 147/1000 | Loss: 0.00002300
Iteration 148/1000 | Loss: 0.00002300
Iteration 149/1000 | Loss: 0.00002300
Iteration 150/1000 | Loss: 0.00002300
Iteration 151/1000 | Loss: 0.00002300
Iteration 152/1000 | Loss: 0.00002300
Iteration 153/1000 | Loss: 0.00002300
Iteration 154/1000 | Loss: 0.00002300
Iteration 155/1000 | Loss: 0.00002299
Iteration 156/1000 | Loss: 0.00002299
Iteration 157/1000 | Loss: 0.00002299
Iteration 158/1000 | Loss: 0.00002299
Iteration 159/1000 | Loss: 0.00002299
Iteration 160/1000 | Loss: 0.00002299
Iteration 161/1000 | Loss: 0.00002299
Iteration 162/1000 | Loss: 0.00002299
Iteration 163/1000 | Loss: 0.00002299
Iteration 164/1000 | Loss: 0.00002299
Iteration 165/1000 | Loss: 0.00002299
Iteration 166/1000 | Loss: 0.00002299
Iteration 167/1000 | Loss: 0.00002299
Iteration 168/1000 | Loss: 0.00002299
Iteration 169/1000 | Loss: 0.00002299
Iteration 170/1000 | Loss: 0.00002298
Iteration 171/1000 | Loss: 0.00002298
Iteration 172/1000 | Loss: 0.00002298
Iteration 173/1000 | Loss: 0.00002298
Iteration 174/1000 | Loss: 0.00002298
Iteration 175/1000 | Loss: 0.00002298
Iteration 176/1000 | Loss: 0.00002298
Iteration 177/1000 | Loss: 0.00002298
Iteration 178/1000 | Loss: 0.00002298
Iteration 179/1000 | Loss: 0.00002298
Iteration 180/1000 | Loss: 0.00002298
Iteration 181/1000 | Loss: 0.00002297
Iteration 182/1000 | Loss: 0.00002297
Iteration 183/1000 | Loss: 0.00002297
Iteration 184/1000 | Loss: 0.00002297
Iteration 185/1000 | Loss: 0.00002297
Iteration 186/1000 | Loss: 0.00002297
Iteration 187/1000 | Loss: 0.00002297
Iteration 188/1000 | Loss: 0.00002297
Iteration 189/1000 | Loss: 0.00002297
Iteration 190/1000 | Loss: 0.00002297
Iteration 191/1000 | Loss: 0.00002297
Iteration 192/1000 | Loss: 0.00002297
Iteration 193/1000 | Loss: 0.00002297
Iteration 194/1000 | Loss: 0.00002297
Iteration 195/1000 | Loss: 0.00002297
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [2.29709294217173e-05, 2.29709294217173e-05, 2.29709294217173e-05, 2.29709294217173e-05, 2.29709294217173e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.29709294217173e-05

Optimization complete. Final v2v error: 4.1225905418396 mm

Highest mean error: 4.395561695098877 mm for frame 146

Lowest mean error: 3.6210620403289795 mm for frame 6

Saving results

Total time: 91.57969284057617
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00870395
Iteration 2/25 | Loss: 0.00155068
Iteration 3/25 | Loss: 0.00128489
Iteration 4/25 | Loss: 0.00124630
Iteration 5/25 | Loss: 0.00124144
Iteration 6/25 | Loss: 0.00124300
Iteration 7/25 | Loss: 0.00123119
Iteration 8/25 | Loss: 0.00122728
Iteration 9/25 | Loss: 0.00120971
Iteration 10/25 | Loss: 0.00118785
Iteration 11/25 | Loss: 0.00118267
Iteration 12/25 | Loss: 0.00118147
Iteration 13/25 | Loss: 0.00118122
Iteration 14/25 | Loss: 0.00118119
Iteration 15/25 | Loss: 0.00118119
Iteration 16/25 | Loss: 0.00118119
Iteration 17/25 | Loss: 0.00118119
Iteration 18/25 | Loss: 0.00118119
Iteration 19/25 | Loss: 0.00118119
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001181192696094513, 0.001181192696094513, 0.001181192696094513, 0.001181192696094513, 0.001181192696094513]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001181192696094513

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.90775847
Iteration 2/25 | Loss: 0.00044182
Iteration 3/25 | Loss: 0.00044181
Iteration 4/25 | Loss: 0.00044181
Iteration 5/25 | Loss: 0.00044181
Iteration 6/25 | Loss: 0.00044181
Iteration 7/25 | Loss: 0.00044181
Iteration 8/25 | Loss: 0.00044181
Iteration 9/25 | Loss: 0.00044181
Iteration 10/25 | Loss: 0.00044181
Iteration 11/25 | Loss: 0.00044181
Iteration 12/25 | Loss: 0.00044181
Iteration 13/25 | Loss: 0.00044181
Iteration 14/25 | Loss: 0.00044181
Iteration 15/25 | Loss: 0.00044181
Iteration 16/25 | Loss: 0.00044181
Iteration 17/25 | Loss: 0.00044181
Iteration 18/25 | Loss: 0.00044181
Iteration 19/25 | Loss: 0.00044181
Iteration 20/25 | Loss: 0.00044181
Iteration 21/25 | Loss: 0.00044181
Iteration 22/25 | Loss: 0.00044181
Iteration 23/25 | Loss: 0.00044181
Iteration 24/25 | Loss: 0.00044181
Iteration 25/25 | Loss: 0.00044181

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00044181
Iteration 2/1000 | Loss: 0.00004147
Iteration 3/1000 | Loss: 0.00003288
Iteration 4/1000 | Loss: 0.00003037
Iteration 5/1000 | Loss: 0.00002910
Iteration 6/1000 | Loss: 0.00002826
Iteration 7/1000 | Loss: 0.00002750
Iteration 8/1000 | Loss: 0.00002708
Iteration 9/1000 | Loss: 0.00002670
Iteration 10/1000 | Loss: 0.00002664
Iteration 11/1000 | Loss: 0.00002642
Iteration 12/1000 | Loss: 0.00002637
Iteration 13/1000 | Loss: 0.00002634
Iteration 14/1000 | Loss: 0.00002622
Iteration 15/1000 | Loss: 0.00002620
Iteration 16/1000 | Loss: 0.00002620
Iteration 17/1000 | Loss: 0.00002619
Iteration 18/1000 | Loss: 0.00002613
Iteration 19/1000 | Loss: 0.00002612
Iteration 20/1000 | Loss: 0.00002610
Iteration 21/1000 | Loss: 0.00002610
Iteration 22/1000 | Loss: 0.00002608
Iteration 23/1000 | Loss: 0.00002608
Iteration 24/1000 | Loss: 0.00002608
Iteration 25/1000 | Loss: 0.00002608
Iteration 26/1000 | Loss: 0.00002608
Iteration 27/1000 | Loss: 0.00002608
Iteration 28/1000 | Loss: 0.00002608
Iteration 29/1000 | Loss: 0.00002607
Iteration 30/1000 | Loss: 0.00002607
Iteration 31/1000 | Loss: 0.00002605
Iteration 32/1000 | Loss: 0.00002604
Iteration 33/1000 | Loss: 0.00002604
Iteration 34/1000 | Loss: 0.00002604
Iteration 35/1000 | Loss: 0.00002603
Iteration 36/1000 | Loss: 0.00002603
Iteration 37/1000 | Loss: 0.00002603
Iteration 38/1000 | Loss: 0.00002603
Iteration 39/1000 | Loss: 0.00002603
Iteration 40/1000 | Loss: 0.00002603
Iteration 41/1000 | Loss: 0.00002603
Iteration 42/1000 | Loss: 0.00002603
Iteration 43/1000 | Loss: 0.00002602
Iteration 44/1000 | Loss: 0.00002602
Iteration 45/1000 | Loss: 0.00002602
Iteration 46/1000 | Loss: 0.00002602
Iteration 47/1000 | Loss: 0.00002602
Iteration 48/1000 | Loss: 0.00002602
Iteration 49/1000 | Loss: 0.00002601
Iteration 50/1000 | Loss: 0.00002601
Iteration 51/1000 | Loss: 0.00002601
Iteration 52/1000 | Loss: 0.00002601
Iteration 53/1000 | Loss: 0.00002601
Iteration 54/1000 | Loss: 0.00002601
Iteration 55/1000 | Loss: 0.00002600
Iteration 56/1000 | Loss: 0.00002600
Iteration 57/1000 | Loss: 0.00002600
Iteration 58/1000 | Loss: 0.00002600
Iteration 59/1000 | Loss: 0.00002599
Iteration 60/1000 | Loss: 0.00002599
Iteration 61/1000 | Loss: 0.00002599
Iteration 62/1000 | Loss: 0.00002599
Iteration 63/1000 | Loss: 0.00002599
Iteration 64/1000 | Loss: 0.00002599
Iteration 65/1000 | Loss: 0.00002599
Iteration 66/1000 | Loss: 0.00002599
Iteration 67/1000 | Loss: 0.00002599
Iteration 68/1000 | Loss: 0.00002599
Iteration 69/1000 | Loss: 0.00002599
Iteration 70/1000 | Loss: 0.00002599
Iteration 71/1000 | Loss: 0.00002599
Iteration 72/1000 | Loss: 0.00002599
Iteration 73/1000 | Loss: 0.00002598
Iteration 74/1000 | Loss: 0.00002598
Iteration 75/1000 | Loss: 0.00002598
Iteration 76/1000 | Loss: 0.00002598
Iteration 77/1000 | Loss: 0.00002598
Iteration 78/1000 | Loss: 0.00002598
Iteration 79/1000 | Loss: 0.00002598
Iteration 80/1000 | Loss: 0.00002598
Iteration 81/1000 | Loss: 0.00002598
Iteration 82/1000 | Loss: 0.00002598
Iteration 83/1000 | Loss: 0.00002598
Iteration 84/1000 | Loss: 0.00002598
Iteration 85/1000 | Loss: 0.00002598
Iteration 86/1000 | Loss: 0.00002598
Iteration 87/1000 | Loss: 0.00002598
Iteration 88/1000 | Loss: 0.00002598
Iteration 89/1000 | Loss: 0.00002598
Iteration 90/1000 | Loss: 0.00002597
Iteration 91/1000 | Loss: 0.00002597
Iteration 92/1000 | Loss: 0.00002597
Iteration 93/1000 | Loss: 0.00002597
Iteration 94/1000 | Loss: 0.00002597
Iteration 95/1000 | Loss: 0.00002597
Iteration 96/1000 | Loss: 0.00002597
Iteration 97/1000 | Loss: 0.00002597
Iteration 98/1000 | Loss: 0.00002596
Iteration 99/1000 | Loss: 0.00002596
Iteration 100/1000 | Loss: 0.00002596
Iteration 101/1000 | Loss: 0.00002596
Iteration 102/1000 | Loss: 0.00002596
Iteration 103/1000 | Loss: 0.00002596
Iteration 104/1000 | Loss: 0.00002596
Iteration 105/1000 | Loss: 0.00002596
Iteration 106/1000 | Loss: 0.00002596
Iteration 107/1000 | Loss: 0.00002596
Iteration 108/1000 | Loss: 0.00002596
Iteration 109/1000 | Loss: 0.00002596
Iteration 110/1000 | Loss: 0.00002596
Iteration 111/1000 | Loss: 0.00002596
Iteration 112/1000 | Loss: 0.00002596
Iteration 113/1000 | Loss: 0.00002596
Iteration 114/1000 | Loss: 0.00002596
Iteration 115/1000 | Loss: 0.00002596
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [2.5959574486478232e-05, 2.5959574486478232e-05, 2.5959574486478232e-05, 2.5959574486478232e-05, 2.5959574486478232e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5959574486478232e-05

Optimization complete. Final v2v error: 4.286525249481201 mm

Highest mean error: 4.5242815017700195 mm for frame 134

Lowest mean error: 4.1698079109191895 mm for frame 40

Saving results

Total time: 46.46088147163391
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00483969
Iteration 2/25 | Loss: 0.00126988
Iteration 3/25 | Loss: 0.00117280
Iteration 4/25 | Loss: 0.00114960
Iteration 5/25 | Loss: 0.00114202
Iteration 6/25 | Loss: 0.00113991
Iteration 7/25 | Loss: 0.00113964
Iteration 8/25 | Loss: 0.00113964
Iteration 9/25 | Loss: 0.00113964
Iteration 10/25 | Loss: 0.00113964
Iteration 11/25 | Loss: 0.00113964
Iteration 12/25 | Loss: 0.00113964
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011396431364119053, 0.0011396431364119053, 0.0011396431364119053, 0.0011396431364119053, 0.0011396431364119053]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011396431364119053

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51101017
Iteration 2/25 | Loss: 0.00116452
Iteration 3/25 | Loss: 0.00116451
Iteration 4/25 | Loss: 0.00116451
Iteration 5/25 | Loss: 0.00116451
Iteration 6/25 | Loss: 0.00116451
Iteration 7/25 | Loss: 0.00116451
Iteration 8/25 | Loss: 0.00116451
Iteration 9/25 | Loss: 0.00116451
Iteration 10/25 | Loss: 0.00116451
Iteration 11/25 | Loss: 0.00116451
Iteration 12/25 | Loss: 0.00116451
Iteration 13/25 | Loss: 0.00116451
Iteration 14/25 | Loss: 0.00116451
Iteration 15/25 | Loss: 0.00116451
Iteration 16/25 | Loss: 0.00116451
Iteration 17/25 | Loss: 0.00116451
Iteration 18/25 | Loss: 0.00116451
Iteration 19/25 | Loss: 0.00116451
Iteration 20/25 | Loss: 0.00116451
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011645067716017365, 0.0011645067716017365, 0.0011645067716017365, 0.0011645067716017365, 0.0011645067716017365]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011645067716017365

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116451
Iteration 2/1000 | Loss: 0.00005332
Iteration 3/1000 | Loss: 0.00003740
Iteration 4/1000 | Loss: 0.00003117
Iteration 5/1000 | Loss: 0.00002831
Iteration 6/1000 | Loss: 0.00002695
Iteration 7/1000 | Loss: 0.00002561
Iteration 8/1000 | Loss: 0.00002487
Iteration 9/1000 | Loss: 0.00002429
Iteration 10/1000 | Loss: 0.00002391
Iteration 11/1000 | Loss: 0.00002356
Iteration 12/1000 | Loss: 0.00002347
Iteration 13/1000 | Loss: 0.00002325
Iteration 14/1000 | Loss: 0.00002307
Iteration 15/1000 | Loss: 0.00002296
Iteration 16/1000 | Loss: 0.00002296
Iteration 17/1000 | Loss: 0.00002287
Iteration 18/1000 | Loss: 0.00002281
Iteration 19/1000 | Loss: 0.00002280
Iteration 20/1000 | Loss: 0.00002277
Iteration 21/1000 | Loss: 0.00002276
Iteration 22/1000 | Loss: 0.00002275
Iteration 23/1000 | Loss: 0.00002274
Iteration 24/1000 | Loss: 0.00002273
Iteration 25/1000 | Loss: 0.00002273
Iteration 26/1000 | Loss: 0.00002272
Iteration 27/1000 | Loss: 0.00002271
Iteration 28/1000 | Loss: 0.00002267
Iteration 29/1000 | Loss: 0.00002263
Iteration 30/1000 | Loss: 0.00002262
Iteration 31/1000 | Loss: 0.00002261
Iteration 32/1000 | Loss: 0.00002261
Iteration 33/1000 | Loss: 0.00002260
Iteration 34/1000 | Loss: 0.00002259
Iteration 35/1000 | Loss: 0.00002258
Iteration 36/1000 | Loss: 0.00002257
Iteration 37/1000 | Loss: 0.00002257
Iteration 38/1000 | Loss: 0.00002257
Iteration 39/1000 | Loss: 0.00002256
Iteration 40/1000 | Loss: 0.00002255
Iteration 41/1000 | Loss: 0.00002254
Iteration 42/1000 | Loss: 0.00002252
Iteration 43/1000 | Loss: 0.00002250
Iteration 44/1000 | Loss: 0.00002249
Iteration 45/1000 | Loss: 0.00002249
Iteration 46/1000 | Loss: 0.00002244
Iteration 47/1000 | Loss: 0.00002244
Iteration 48/1000 | Loss: 0.00002244
Iteration 49/1000 | Loss: 0.00002244
Iteration 50/1000 | Loss: 0.00002244
Iteration 51/1000 | Loss: 0.00002244
Iteration 52/1000 | Loss: 0.00002243
Iteration 53/1000 | Loss: 0.00002243
Iteration 54/1000 | Loss: 0.00002243
Iteration 55/1000 | Loss: 0.00002243
Iteration 56/1000 | Loss: 0.00002243
Iteration 57/1000 | Loss: 0.00002243
Iteration 58/1000 | Loss: 0.00002242
Iteration 59/1000 | Loss: 0.00002242
Iteration 60/1000 | Loss: 0.00002242
Iteration 61/1000 | Loss: 0.00002241
Iteration 62/1000 | Loss: 0.00002241
Iteration 63/1000 | Loss: 0.00002241
Iteration 64/1000 | Loss: 0.00002241
Iteration 65/1000 | Loss: 0.00002240
Iteration 66/1000 | Loss: 0.00002240
Iteration 67/1000 | Loss: 0.00002240
Iteration 68/1000 | Loss: 0.00002239
Iteration 69/1000 | Loss: 0.00002239
Iteration 70/1000 | Loss: 0.00002239
Iteration 71/1000 | Loss: 0.00002238
Iteration 72/1000 | Loss: 0.00002238
Iteration 73/1000 | Loss: 0.00002238
Iteration 74/1000 | Loss: 0.00002237
Iteration 75/1000 | Loss: 0.00002237
Iteration 76/1000 | Loss: 0.00002237
Iteration 77/1000 | Loss: 0.00002236
Iteration 78/1000 | Loss: 0.00002236
Iteration 79/1000 | Loss: 0.00002236
Iteration 80/1000 | Loss: 0.00002236
Iteration 81/1000 | Loss: 0.00002236
Iteration 82/1000 | Loss: 0.00002235
Iteration 83/1000 | Loss: 0.00002235
Iteration 84/1000 | Loss: 0.00002235
Iteration 85/1000 | Loss: 0.00002235
Iteration 86/1000 | Loss: 0.00002235
Iteration 87/1000 | Loss: 0.00002235
Iteration 88/1000 | Loss: 0.00002235
Iteration 89/1000 | Loss: 0.00002235
Iteration 90/1000 | Loss: 0.00002234
Iteration 91/1000 | Loss: 0.00002234
Iteration 92/1000 | Loss: 0.00002234
Iteration 93/1000 | Loss: 0.00002234
Iteration 94/1000 | Loss: 0.00002233
Iteration 95/1000 | Loss: 0.00002233
Iteration 96/1000 | Loss: 0.00002233
Iteration 97/1000 | Loss: 0.00002233
Iteration 98/1000 | Loss: 0.00002233
Iteration 99/1000 | Loss: 0.00002233
Iteration 100/1000 | Loss: 0.00002233
Iteration 101/1000 | Loss: 0.00002232
Iteration 102/1000 | Loss: 0.00002232
Iteration 103/1000 | Loss: 0.00002232
Iteration 104/1000 | Loss: 0.00002232
Iteration 105/1000 | Loss: 0.00002232
Iteration 106/1000 | Loss: 0.00002232
Iteration 107/1000 | Loss: 0.00002232
Iteration 108/1000 | Loss: 0.00002232
Iteration 109/1000 | Loss: 0.00002232
Iteration 110/1000 | Loss: 0.00002232
Iteration 111/1000 | Loss: 0.00002232
Iteration 112/1000 | Loss: 0.00002232
Iteration 113/1000 | Loss: 0.00002231
Iteration 114/1000 | Loss: 0.00002231
Iteration 115/1000 | Loss: 0.00002231
Iteration 116/1000 | Loss: 0.00002231
Iteration 117/1000 | Loss: 0.00002231
Iteration 118/1000 | Loss: 0.00002231
Iteration 119/1000 | Loss: 0.00002231
Iteration 120/1000 | Loss: 0.00002230
Iteration 121/1000 | Loss: 0.00002230
Iteration 122/1000 | Loss: 0.00002230
Iteration 123/1000 | Loss: 0.00002230
Iteration 124/1000 | Loss: 0.00002229
Iteration 125/1000 | Loss: 0.00002229
Iteration 126/1000 | Loss: 0.00002229
Iteration 127/1000 | Loss: 0.00002229
Iteration 128/1000 | Loss: 0.00002229
Iteration 129/1000 | Loss: 0.00002228
Iteration 130/1000 | Loss: 0.00002228
Iteration 131/1000 | Loss: 0.00002228
Iteration 132/1000 | Loss: 0.00002228
Iteration 133/1000 | Loss: 0.00002228
Iteration 134/1000 | Loss: 0.00002228
Iteration 135/1000 | Loss: 0.00002228
Iteration 136/1000 | Loss: 0.00002228
Iteration 137/1000 | Loss: 0.00002228
Iteration 138/1000 | Loss: 0.00002228
Iteration 139/1000 | Loss: 0.00002227
Iteration 140/1000 | Loss: 0.00002227
Iteration 141/1000 | Loss: 0.00002227
Iteration 142/1000 | Loss: 0.00002227
Iteration 143/1000 | Loss: 0.00002227
Iteration 144/1000 | Loss: 0.00002227
Iteration 145/1000 | Loss: 0.00002227
Iteration 146/1000 | Loss: 0.00002227
Iteration 147/1000 | Loss: 0.00002227
Iteration 148/1000 | Loss: 0.00002227
Iteration 149/1000 | Loss: 0.00002226
Iteration 150/1000 | Loss: 0.00002226
Iteration 151/1000 | Loss: 0.00002226
Iteration 152/1000 | Loss: 0.00002226
Iteration 153/1000 | Loss: 0.00002226
Iteration 154/1000 | Loss: 0.00002226
Iteration 155/1000 | Loss: 0.00002226
Iteration 156/1000 | Loss: 0.00002226
Iteration 157/1000 | Loss: 0.00002226
Iteration 158/1000 | Loss: 0.00002226
Iteration 159/1000 | Loss: 0.00002226
Iteration 160/1000 | Loss: 0.00002226
Iteration 161/1000 | Loss: 0.00002226
Iteration 162/1000 | Loss: 0.00002225
Iteration 163/1000 | Loss: 0.00002225
Iteration 164/1000 | Loss: 0.00002225
Iteration 165/1000 | Loss: 0.00002225
Iteration 166/1000 | Loss: 0.00002225
Iteration 167/1000 | Loss: 0.00002225
Iteration 168/1000 | Loss: 0.00002225
Iteration 169/1000 | Loss: 0.00002225
Iteration 170/1000 | Loss: 0.00002225
Iteration 171/1000 | Loss: 0.00002225
Iteration 172/1000 | Loss: 0.00002225
Iteration 173/1000 | Loss: 0.00002225
Iteration 174/1000 | Loss: 0.00002224
Iteration 175/1000 | Loss: 0.00002224
Iteration 176/1000 | Loss: 0.00002224
Iteration 177/1000 | Loss: 0.00002224
Iteration 178/1000 | Loss: 0.00002224
Iteration 179/1000 | Loss: 0.00002224
Iteration 180/1000 | Loss: 0.00002224
Iteration 181/1000 | Loss: 0.00002224
Iteration 182/1000 | Loss: 0.00002224
Iteration 183/1000 | Loss: 0.00002224
Iteration 184/1000 | Loss: 0.00002224
Iteration 185/1000 | Loss: 0.00002223
Iteration 186/1000 | Loss: 0.00002223
Iteration 187/1000 | Loss: 0.00002223
Iteration 188/1000 | Loss: 0.00002223
Iteration 189/1000 | Loss: 0.00002223
Iteration 190/1000 | Loss: 0.00002223
Iteration 191/1000 | Loss: 0.00002223
Iteration 192/1000 | Loss: 0.00002223
Iteration 193/1000 | Loss: 0.00002223
Iteration 194/1000 | Loss: 0.00002222
Iteration 195/1000 | Loss: 0.00002222
Iteration 196/1000 | Loss: 0.00002222
Iteration 197/1000 | Loss: 0.00002222
Iteration 198/1000 | Loss: 0.00002222
Iteration 199/1000 | Loss: 0.00002222
Iteration 200/1000 | Loss: 0.00002222
Iteration 201/1000 | Loss: 0.00002222
Iteration 202/1000 | Loss: 0.00002222
Iteration 203/1000 | Loss: 0.00002222
Iteration 204/1000 | Loss: 0.00002221
Iteration 205/1000 | Loss: 0.00002221
Iteration 206/1000 | Loss: 0.00002221
Iteration 207/1000 | Loss: 0.00002221
Iteration 208/1000 | Loss: 0.00002221
Iteration 209/1000 | Loss: 0.00002221
Iteration 210/1000 | Loss: 0.00002221
Iteration 211/1000 | Loss: 0.00002221
Iteration 212/1000 | Loss: 0.00002220
Iteration 213/1000 | Loss: 0.00002220
Iteration 214/1000 | Loss: 0.00002220
Iteration 215/1000 | Loss: 0.00002220
Iteration 216/1000 | Loss: 0.00002219
Iteration 217/1000 | Loss: 0.00002219
Iteration 218/1000 | Loss: 0.00002219
Iteration 219/1000 | Loss: 0.00002219
Iteration 220/1000 | Loss: 0.00002219
Iteration 221/1000 | Loss: 0.00002218
Iteration 222/1000 | Loss: 0.00002218
Iteration 223/1000 | Loss: 0.00002218
Iteration 224/1000 | Loss: 0.00002218
Iteration 225/1000 | Loss: 0.00002218
Iteration 226/1000 | Loss: 0.00002218
Iteration 227/1000 | Loss: 0.00002218
Iteration 228/1000 | Loss: 0.00002218
Iteration 229/1000 | Loss: 0.00002218
Iteration 230/1000 | Loss: 0.00002218
Iteration 231/1000 | Loss: 0.00002217
Iteration 232/1000 | Loss: 0.00002217
Iteration 233/1000 | Loss: 0.00002217
Iteration 234/1000 | Loss: 0.00002217
Iteration 235/1000 | Loss: 0.00002217
Iteration 236/1000 | Loss: 0.00002217
Iteration 237/1000 | Loss: 0.00002217
Iteration 238/1000 | Loss: 0.00002217
Iteration 239/1000 | Loss: 0.00002217
Iteration 240/1000 | Loss: 0.00002217
Iteration 241/1000 | Loss: 0.00002217
Iteration 242/1000 | Loss: 0.00002217
Iteration 243/1000 | Loss: 0.00002217
Iteration 244/1000 | Loss: 0.00002217
Iteration 245/1000 | Loss: 0.00002216
Iteration 246/1000 | Loss: 0.00002216
Iteration 247/1000 | Loss: 0.00002216
Iteration 248/1000 | Loss: 0.00002216
Iteration 249/1000 | Loss: 0.00002216
Iteration 250/1000 | Loss: 0.00002216
Iteration 251/1000 | Loss: 0.00002216
Iteration 252/1000 | Loss: 0.00002216
Iteration 253/1000 | Loss: 0.00002216
Iteration 254/1000 | Loss: 0.00002216
Iteration 255/1000 | Loss: 0.00002216
Iteration 256/1000 | Loss: 0.00002216
Iteration 257/1000 | Loss: 0.00002216
Iteration 258/1000 | Loss: 0.00002216
Iteration 259/1000 | Loss: 0.00002216
Iteration 260/1000 | Loss: 0.00002216
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 260. Stopping optimization.
Last 5 losses: [2.2159556465339847e-05, 2.2159556465339847e-05, 2.2159556465339847e-05, 2.2159556465339847e-05, 2.2159556465339847e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2159556465339847e-05

Optimization complete. Final v2v error: 3.8837294578552246 mm

Highest mean error: 4.820779323577881 mm for frame 23

Lowest mean error: 2.8063151836395264 mm for frame 238

Saving results

Total time: 56.612229347229004
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01081593
Iteration 2/25 | Loss: 0.01081593
Iteration 3/25 | Loss: 0.00347364
Iteration 4/25 | Loss: 0.00230655
Iteration 5/25 | Loss: 0.00252695
Iteration 6/25 | Loss: 0.00271602
Iteration 7/25 | Loss: 0.00220505
Iteration 8/25 | Loss: 0.00178211
Iteration 9/25 | Loss: 0.00161833
Iteration 10/25 | Loss: 0.00155804
Iteration 11/25 | Loss: 0.00153637
Iteration 12/25 | Loss: 0.00151700
Iteration 13/25 | Loss: 0.00150127
Iteration 14/25 | Loss: 0.00148654
Iteration 15/25 | Loss: 0.00148687
Iteration 16/25 | Loss: 0.00147994
Iteration 17/25 | Loss: 0.00147452
Iteration 18/25 | Loss: 0.00147082
Iteration 19/25 | Loss: 0.00146350
Iteration 20/25 | Loss: 0.00146022
Iteration 21/25 | Loss: 0.00145506
Iteration 22/25 | Loss: 0.00145598
Iteration 23/25 | Loss: 0.00145688
Iteration 24/25 | Loss: 0.00145627
Iteration 25/25 | Loss: 0.00145636

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.68912143
Iteration 2/25 | Loss: 0.00128840
Iteration 3/25 | Loss: 0.00128840
Iteration 4/25 | Loss: 0.00128840
Iteration 5/25 | Loss: 0.00128840
Iteration 6/25 | Loss: 0.00128840
Iteration 7/25 | Loss: 0.00128840
Iteration 8/25 | Loss: 0.00128840
Iteration 9/25 | Loss: 0.00128839
Iteration 10/25 | Loss: 0.00128839
Iteration 11/25 | Loss: 0.00128839
Iteration 12/25 | Loss: 0.00128839
Iteration 13/25 | Loss: 0.00128839
Iteration 14/25 | Loss: 0.00128839
Iteration 15/25 | Loss: 0.00128839
Iteration 16/25 | Loss: 0.00128839
Iteration 17/25 | Loss: 0.00128839
Iteration 18/25 | Loss: 0.00128839
Iteration 19/25 | Loss: 0.00128839
Iteration 20/25 | Loss: 0.00128839
Iteration 21/25 | Loss: 0.00128839
Iteration 22/25 | Loss: 0.00128839
Iteration 23/25 | Loss: 0.00128839
Iteration 24/25 | Loss: 0.00128839
Iteration 25/25 | Loss: 0.00128839

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128839
Iteration 2/1000 | Loss: 0.00014789
Iteration 3/1000 | Loss: 0.00010532
Iteration 4/1000 | Loss: 0.00012975
Iteration 5/1000 | Loss: 0.00009338
Iteration 6/1000 | Loss: 0.00026552
Iteration 7/1000 | Loss: 0.00020929
Iteration 8/1000 | Loss: 0.00009194
Iteration 9/1000 | Loss: 0.00011005
Iteration 10/1000 | Loss: 0.00008198
Iteration 11/1000 | Loss: 0.00012835
Iteration 12/1000 | Loss: 0.00024092
Iteration 13/1000 | Loss: 0.00010694
Iteration 14/1000 | Loss: 0.00013881
Iteration 15/1000 | Loss: 0.00016475
Iteration 16/1000 | Loss: 0.00013965
Iteration 17/1000 | Loss: 0.00018387
Iteration 18/1000 | Loss: 0.00031046
Iteration 19/1000 | Loss: 0.00014268
Iteration 20/1000 | Loss: 0.00017167
Iteration 21/1000 | Loss: 0.00010738
Iteration 22/1000 | Loss: 0.00012626
Iteration 23/1000 | Loss: 0.00011235
Iteration 24/1000 | Loss: 0.00016202
Iteration 25/1000 | Loss: 0.00009855
Iteration 26/1000 | Loss: 0.00009782
Iteration 27/1000 | Loss: 0.00010544
Iteration 28/1000 | Loss: 0.00008022
Iteration 29/1000 | Loss: 0.00009802
Iteration 30/1000 | Loss: 0.00010680
Iteration 31/1000 | Loss: 0.00011212
Iteration 32/1000 | Loss: 0.00010975
Iteration 33/1000 | Loss: 0.00009446
Iteration 34/1000 | Loss: 0.00017418
Iteration 35/1000 | Loss: 0.00010946
Iteration 36/1000 | Loss: 0.00010456
Iteration 37/1000 | Loss: 0.00016554
Iteration 38/1000 | Loss: 0.00020016
Iteration 39/1000 | Loss: 0.00017862
Iteration 40/1000 | Loss: 0.00021019
Iteration 41/1000 | Loss: 0.00011042
Iteration 42/1000 | Loss: 0.00014154
Iteration 43/1000 | Loss: 0.00019532
Iteration 44/1000 | Loss: 0.00011493
Iteration 45/1000 | Loss: 0.00012247
Iteration 46/1000 | Loss: 0.00010746
Iteration 47/1000 | Loss: 0.00009585
Iteration 48/1000 | Loss: 0.00018568
Iteration 49/1000 | Loss: 0.00014920
Iteration 50/1000 | Loss: 0.00016715
Iteration 51/1000 | Loss: 0.00016038
Iteration 52/1000 | Loss: 0.00015514
Iteration 53/1000 | Loss: 0.00016690
Iteration 54/1000 | Loss: 0.00017778
Iteration 55/1000 | Loss: 0.00018642
Iteration 56/1000 | Loss: 0.00017954
Iteration 57/1000 | Loss: 0.00011904
Iteration 58/1000 | Loss: 0.00009418
Iteration 59/1000 | Loss: 0.00015268
Iteration 60/1000 | Loss: 0.00013394
Iteration 61/1000 | Loss: 0.00013039
Iteration 62/1000 | Loss: 0.00017275
Iteration 63/1000 | Loss: 0.00017947
Iteration 64/1000 | Loss: 0.00019223
Iteration 65/1000 | Loss: 0.00014915
Iteration 66/1000 | Loss: 0.00018275
Iteration 67/1000 | Loss: 0.00014194
Iteration 68/1000 | Loss: 0.00016027
Iteration 69/1000 | Loss: 0.00017270
Iteration 70/1000 | Loss: 0.00017029
Iteration 71/1000 | Loss: 0.00013626
Iteration 72/1000 | Loss: 0.00018948
Iteration 73/1000 | Loss: 0.00014769
Iteration 74/1000 | Loss: 0.00011148
Iteration 75/1000 | Loss: 0.00019372
Iteration 76/1000 | Loss: 0.00015395
Iteration 77/1000 | Loss: 0.00023761
Iteration 78/1000 | Loss: 0.00007548
Iteration 79/1000 | Loss: 0.00006301
Iteration 80/1000 | Loss: 0.00008079
Iteration 81/1000 | Loss: 0.00009916
Iteration 82/1000 | Loss: 0.00009732
Iteration 83/1000 | Loss: 0.00010231
Iteration 84/1000 | Loss: 0.00009452
Iteration 85/1000 | Loss: 0.00010188
Iteration 86/1000 | Loss: 0.00008780
Iteration 87/1000 | Loss: 0.00010099
Iteration 88/1000 | Loss: 0.00008616
Iteration 89/1000 | Loss: 0.00011176
Iteration 90/1000 | Loss: 0.00008613
Iteration 91/1000 | Loss: 0.00008527
Iteration 92/1000 | Loss: 0.00008511
Iteration 93/1000 | Loss: 0.00009230
Iteration 94/1000 | Loss: 0.00009074
Iteration 95/1000 | Loss: 0.00010063
Iteration 96/1000 | Loss: 0.00009816
Iteration 97/1000 | Loss: 0.00007761
Iteration 98/1000 | Loss: 0.00008728
Iteration 99/1000 | Loss: 0.00009595
Iteration 100/1000 | Loss: 0.00008959
Iteration 101/1000 | Loss: 0.00009862
Iteration 102/1000 | Loss: 0.00009018
Iteration 103/1000 | Loss: 0.00008892
Iteration 104/1000 | Loss: 0.00009209
Iteration 105/1000 | Loss: 0.00008858
Iteration 106/1000 | Loss: 0.00008931
Iteration 107/1000 | Loss: 0.00009178
Iteration 108/1000 | Loss: 0.00008827
Iteration 109/1000 | Loss: 0.00008987
Iteration 110/1000 | Loss: 0.00008735
Iteration 111/1000 | Loss: 0.00009118
Iteration 112/1000 | Loss: 0.00009061
Iteration 113/1000 | Loss: 0.00009041
Iteration 114/1000 | Loss: 0.00009061
Iteration 115/1000 | Loss: 0.00009160
Iteration 116/1000 | Loss: 0.00009134
Iteration 117/1000 | Loss: 0.00009557
Iteration 118/1000 | Loss: 0.00009628
Iteration 119/1000 | Loss: 0.00009378
Iteration 120/1000 | Loss: 0.00008673
Iteration 121/1000 | Loss: 0.00008972
Iteration 122/1000 | Loss: 0.00008156
Iteration 123/1000 | Loss: 0.00009064
Iteration 124/1000 | Loss: 0.00008780
Iteration 125/1000 | Loss: 0.00009626
Iteration 126/1000 | Loss: 0.00009156
Iteration 127/1000 | Loss: 0.00008312
Iteration 128/1000 | Loss: 0.00008939
Iteration 129/1000 | Loss: 0.00009232
Iteration 130/1000 | Loss: 0.00009083
Iteration 131/1000 | Loss: 0.00009456
Iteration 132/1000 | Loss: 0.00009062
Iteration 133/1000 | Loss: 0.00009059
Iteration 134/1000 | Loss: 0.00009013
Iteration 135/1000 | Loss: 0.00008956
Iteration 136/1000 | Loss: 0.00009074
Iteration 137/1000 | Loss: 0.00009137
Iteration 138/1000 | Loss: 0.00009401
Iteration 139/1000 | Loss: 0.00009237
Iteration 140/1000 | Loss: 0.00008694
Iteration 141/1000 | Loss: 0.00009102
Iteration 142/1000 | Loss: 0.00008589
Iteration 143/1000 | Loss: 0.00009367
Iteration 144/1000 | Loss: 0.00008714
Iteration 145/1000 | Loss: 0.00008844
Iteration 146/1000 | Loss: 0.00009290
Iteration 147/1000 | Loss: 0.00008881
Iteration 148/1000 | Loss: 0.00008794
Iteration 149/1000 | Loss: 0.00008986
Iteration 150/1000 | Loss: 0.00007787
Iteration 151/1000 | Loss: 0.00008715
Iteration 152/1000 | Loss: 0.00007919
Iteration 153/1000 | Loss: 0.00008622
Iteration 154/1000 | Loss: 0.00008754
Iteration 155/1000 | Loss: 0.00008610
Iteration 156/1000 | Loss: 0.00009061
Iteration 157/1000 | Loss: 0.00008463
Iteration 158/1000 | Loss: 0.00009375
Iteration 159/1000 | Loss: 0.00008704
Iteration 160/1000 | Loss: 0.00010363
Iteration 161/1000 | Loss: 0.00008267
Iteration 162/1000 | Loss: 0.00006714
Iteration 163/1000 | Loss: 0.00006190
Iteration 164/1000 | Loss: 0.00007906
Iteration 165/1000 | Loss: 0.00007161
Iteration 166/1000 | Loss: 0.00008234
Iteration 167/1000 | Loss: 0.00006754
Iteration 168/1000 | Loss: 0.00007689
Iteration 169/1000 | Loss: 0.00007247
Iteration 170/1000 | Loss: 0.00007831
Iteration 171/1000 | Loss: 0.00007762
Iteration 172/1000 | Loss: 0.00007615
Iteration 173/1000 | Loss: 0.00007913
Iteration 174/1000 | Loss: 0.00008198
Iteration 175/1000 | Loss: 0.00007951
Iteration 176/1000 | Loss: 0.00007914
Iteration 177/1000 | Loss: 0.00008141
Iteration 178/1000 | Loss: 0.00007671
Iteration 179/1000 | Loss: 0.00008039
Iteration 180/1000 | Loss: 0.00007100
Iteration 181/1000 | Loss: 0.00008309
Iteration 182/1000 | Loss: 0.00007740
Iteration 183/1000 | Loss: 0.00008271
Iteration 184/1000 | Loss: 0.00007896
Iteration 185/1000 | Loss: 0.00007910
Iteration 186/1000 | Loss: 0.00008003
Iteration 187/1000 | Loss: 0.00008072
Iteration 188/1000 | Loss: 0.00007907
Iteration 189/1000 | Loss: 0.00007364
Iteration 190/1000 | Loss: 0.00007513
Iteration 191/1000 | Loss: 0.00007812
Iteration 192/1000 | Loss: 0.00007543
Iteration 193/1000 | Loss: 0.00007596
Iteration 194/1000 | Loss: 0.00007417
Iteration 195/1000 | Loss: 0.00007660
Iteration 196/1000 | Loss: 0.00007407
Iteration 197/1000 | Loss: 0.00007792
Iteration 198/1000 | Loss: 0.00007364
Iteration 199/1000 | Loss: 0.00007737
Iteration 200/1000 | Loss: 0.00007598
Iteration 201/1000 | Loss: 0.00007601
Iteration 202/1000 | Loss: 0.00007638
Iteration 203/1000 | Loss: 0.00005837
Iteration 204/1000 | Loss: 0.00006852
Iteration 205/1000 | Loss: 0.00008245
Iteration 206/1000 | Loss: 0.00006543
Iteration 207/1000 | Loss: 0.00006858
Iteration 208/1000 | Loss: 0.00007624
Iteration 209/1000 | Loss: 0.00007247
Iteration 210/1000 | Loss: 0.00007240
Iteration 211/1000 | Loss: 0.00006284
Iteration 212/1000 | Loss: 0.00006730
Iteration 213/1000 | Loss: 0.00007737
Iteration 214/1000 | Loss: 0.00007728
Iteration 215/1000 | Loss: 0.00007078
Iteration 216/1000 | Loss: 0.00007522
Iteration 217/1000 | Loss: 0.00006670
Iteration 218/1000 | Loss: 0.00008950
Iteration 219/1000 | Loss: 0.00007542
Iteration 220/1000 | Loss: 0.00007790
Iteration 221/1000 | Loss: 0.00008035
Iteration 222/1000 | Loss: 0.00009448
Iteration 223/1000 | Loss: 0.00006377
Iteration 224/1000 | Loss: 0.00005500
Iteration 225/1000 | Loss: 0.00005057
Iteration 226/1000 | Loss: 0.00004753
Iteration 227/1000 | Loss: 0.00004640
Iteration 228/1000 | Loss: 0.00004588
Iteration 229/1000 | Loss: 0.00004547
Iteration 230/1000 | Loss: 0.00004516
Iteration 231/1000 | Loss: 0.00004499
Iteration 232/1000 | Loss: 0.00004478
Iteration 233/1000 | Loss: 0.00004458
Iteration 234/1000 | Loss: 0.00004440
Iteration 235/1000 | Loss: 0.00004422
Iteration 236/1000 | Loss: 0.00004410
Iteration 237/1000 | Loss: 0.00004407
Iteration 238/1000 | Loss: 0.00004405
Iteration 239/1000 | Loss: 0.00004398
Iteration 240/1000 | Loss: 0.00004392
Iteration 241/1000 | Loss: 0.00004392
Iteration 242/1000 | Loss: 0.00004390
Iteration 243/1000 | Loss: 0.00004390
Iteration 244/1000 | Loss: 0.00004390
Iteration 245/1000 | Loss: 0.00004390
Iteration 246/1000 | Loss: 0.00004390
Iteration 247/1000 | Loss: 0.00004390
Iteration 248/1000 | Loss: 0.00004390
Iteration 249/1000 | Loss: 0.00004390
Iteration 250/1000 | Loss: 0.00004390
Iteration 251/1000 | Loss: 0.00004390
Iteration 252/1000 | Loss: 0.00004390
Iteration 253/1000 | Loss: 0.00004390
Iteration 254/1000 | Loss: 0.00004390
Iteration 255/1000 | Loss: 0.00004389
Iteration 256/1000 | Loss: 0.00004389
Iteration 257/1000 | Loss: 0.00004389
Iteration 258/1000 | Loss: 0.00004389
Iteration 259/1000 | Loss: 0.00004389
Iteration 260/1000 | Loss: 0.00004389
Iteration 261/1000 | Loss: 0.00004389
Iteration 262/1000 | Loss: 0.00004389
Iteration 263/1000 | Loss: 0.00004389
Iteration 264/1000 | Loss: 0.00004389
Iteration 265/1000 | Loss: 0.00004389
Iteration 266/1000 | Loss: 0.00004389
Iteration 267/1000 | Loss: 0.00004389
Iteration 268/1000 | Loss: 0.00004389
Iteration 269/1000 | Loss: 0.00004389
Iteration 270/1000 | Loss: 0.00004389
Iteration 271/1000 | Loss: 0.00004389
Iteration 272/1000 | Loss: 0.00004389
Iteration 273/1000 | Loss: 0.00004388
Iteration 274/1000 | Loss: 0.00004388
Iteration 275/1000 | Loss: 0.00004388
Iteration 276/1000 | Loss: 0.00004387
Iteration 277/1000 | Loss: 0.00004387
Iteration 278/1000 | Loss: 0.00004387
Iteration 279/1000 | Loss: 0.00004387
Iteration 280/1000 | Loss: 0.00004387
Iteration 281/1000 | Loss: 0.00004387
Iteration 282/1000 | Loss: 0.00004387
Iteration 283/1000 | Loss: 0.00004386
Iteration 284/1000 | Loss: 0.00004385
Iteration 285/1000 | Loss: 0.00004385
Iteration 286/1000 | Loss: 0.00004385
Iteration 287/1000 | Loss: 0.00004385
Iteration 288/1000 | Loss: 0.00004385
Iteration 289/1000 | Loss: 0.00004385
Iteration 290/1000 | Loss: 0.00004385
Iteration 291/1000 | Loss: 0.00004385
Iteration 292/1000 | Loss: 0.00004385
Iteration 293/1000 | Loss: 0.00004385
Iteration 294/1000 | Loss: 0.00004384
Iteration 295/1000 | Loss: 0.00004384
Iteration 296/1000 | Loss: 0.00004384
Iteration 297/1000 | Loss: 0.00004384
Iteration 298/1000 | Loss: 0.00004384
Iteration 299/1000 | Loss: 0.00004383
Iteration 300/1000 | Loss: 0.00004383
Iteration 301/1000 | Loss: 0.00004383
Iteration 302/1000 | Loss: 0.00004383
Iteration 303/1000 | Loss: 0.00004383
Iteration 304/1000 | Loss: 0.00004382
Iteration 305/1000 | Loss: 0.00004382
Iteration 306/1000 | Loss: 0.00004382
Iteration 307/1000 | Loss: 0.00004382
Iteration 308/1000 | Loss: 0.00004382
Iteration 309/1000 | Loss: 0.00004382
Iteration 310/1000 | Loss: 0.00004382
Iteration 311/1000 | Loss: 0.00004382
Iteration 312/1000 | Loss: 0.00004381
Iteration 313/1000 | Loss: 0.00004381
Iteration 314/1000 | Loss: 0.00004381
Iteration 315/1000 | Loss: 0.00004381
Iteration 316/1000 | Loss: 0.00004381
Iteration 317/1000 | Loss: 0.00004381
Iteration 318/1000 | Loss: 0.00004381
Iteration 319/1000 | Loss: 0.00004381
Iteration 320/1000 | Loss: 0.00004380
Iteration 321/1000 | Loss: 0.00004380
Iteration 322/1000 | Loss: 0.00004380
Iteration 323/1000 | Loss: 0.00004380
Iteration 324/1000 | Loss: 0.00004380
Iteration 325/1000 | Loss: 0.00004380
Iteration 326/1000 | Loss: 0.00004380
Iteration 327/1000 | Loss: 0.00004380
Iteration 328/1000 | Loss: 0.00004380
Iteration 329/1000 | Loss: 0.00004380
Iteration 330/1000 | Loss: 0.00004380
Iteration 331/1000 | Loss: 0.00004380
Iteration 332/1000 | Loss: 0.00004380
Iteration 333/1000 | Loss: 0.00004380
Iteration 334/1000 | Loss: 0.00004380
Iteration 335/1000 | Loss: 0.00004380
Iteration 336/1000 | Loss: 0.00004380
Iteration 337/1000 | Loss: 0.00004380
Iteration 338/1000 | Loss: 0.00004380
Iteration 339/1000 | Loss: 0.00004380
Iteration 340/1000 | Loss: 0.00004380
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 340. Stopping optimization.
Last 5 losses: [4.379704114398919e-05, 4.379704114398919e-05, 4.379704114398919e-05, 4.379704114398919e-05, 4.379704114398919e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.379704114398919e-05

Optimization complete. Final v2v error: 5.288444519042969 mm

Highest mean error: 5.99117374420166 mm for frame 218

Lowest mean error: 4.875298976898193 mm for frame 249

Saving results

Total time: 443.06415247917175
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00889286
Iteration 2/25 | Loss: 0.00124461
Iteration 3/25 | Loss: 0.00114021
Iteration 4/25 | Loss: 0.00112079
Iteration 5/25 | Loss: 0.00111418
Iteration 6/25 | Loss: 0.00111261
Iteration 7/25 | Loss: 0.00111261
Iteration 8/25 | Loss: 0.00111261
Iteration 9/25 | Loss: 0.00111261
Iteration 10/25 | Loss: 0.00111261
Iteration 11/25 | Loss: 0.00111261
Iteration 12/25 | Loss: 0.00111261
Iteration 13/25 | Loss: 0.00111261
Iteration 14/25 | Loss: 0.00111261
Iteration 15/25 | Loss: 0.00111261
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001112605445086956, 0.001112605445086956, 0.001112605445086956, 0.001112605445086956, 0.001112605445086956]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001112605445086956

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36627352
Iteration 2/25 | Loss: 0.00080823
Iteration 3/25 | Loss: 0.00080821
Iteration 4/25 | Loss: 0.00080821
Iteration 5/25 | Loss: 0.00080821
Iteration 6/25 | Loss: 0.00080821
Iteration 7/25 | Loss: 0.00080821
Iteration 8/25 | Loss: 0.00080821
Iteration 9/25 | Loss: 0.00080821
Iteration 10/25 | Loss: 0.00080821
Iteration 11/25 | Loss: 0.00080821
Iteration 12/25 | Loss: 0.00080821
Iteration 13/25 | Loss: 0.00080821
Iteration 14/25 | Loss: 0.00080821
Iteration 15/25 | Loss: 0.00080821
Iteration 16/25 | Loss: 0.00080821
Iteration 17/25 | Loss: 0.00080821
Iteration 18/25 | Loss: 0.00080821
Iteration 19/25 | Loss: 0.00080821
Iteration 20/25 | Loss: 0.00080821
Iteration 21/25 | Loss: 0.00080821
Iteration 22/25 | Loss: 0.00080821
Iteration 23/25 | Loss: 0.00080821
Iteration 24/25 | Loss: 0.00080821
Iteration 25/25 | Loss: 0.00080821

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080821
Iteration 2/1000 | Loss: 0.00003934
Iteration 3/1000 | Loss: 0.00002653
Iteration 4/1000 | Loss: 0.00002091
Iteration 5/1000 | Loss: 0.00001919
Iteration 6/1000 | Loss: 0.00001827
Iteration 7/1000 | Loss: 0.00001782
Iteration 8/1000 | Loss: 0.00001737
Iteration 9/1000 | Loss: 0.00001687
Iteration 10/1000 | Loss: 0.00001662
Iteration 11/1000 | Loss: 0.00001639
Iteration 12/1000 | Loss: 0.00001635
Iteration 13/1000 | Loss: 0.00001626
Iteration 14/1000 | Loss: 0.00001620
Iteration 15/1000 | Loss: 0.00001618
Iteration 16/1000 | Loss: 0.00001617
Iteration 17/1000 | Loss: 0.00001617
Iteration 18/1000 | Loss: 0.00001614
Iteration 19/1000 | Loss: 0.00001612
Iteration 20/1000 | Loss: 0.00001611
Iteration 21/1000 | Loss: 0.00001611
Iteration 22/1000 | Loss: 0.00001611
Iteration 23/1000 | Loss: 0.00001609
Iteration 24/1000 | Loss: 0.00001608
Iteration 25/1000 | Loss: 0.00001604
Iteration 26/1000 | Loss: 0.00001603
Iteration 27/1000 | Loss: 0.00001601
Iteration 28/1000 | Loss: 0.00001598
Iteration 29/1000 | Loss: 0.00001597
Iteration 30/1000 | Loss: 0.00001590
Iteration 31/1000 | Loss: 0.00001586
Iteration 32/1000 | Loss: 0.00001585
Iteration 33/1000 | Loss: 0.00001585
Iteration 34/1000 | Loss: 0.00001585
Iteration 35/1000 | Loss: 0.00001584
Iteration 36/1000 | Loss: 0.00001584
Iteration 37/1000 | Loss: 0.00001584
Iteration 38/1000 | Loss: 0.00001584
Iteration 39/1000 | Loss: 0.00001584
Iteration 40/1000 | Loss: 0.00001584
Iteration 41/1000 | Loss: 0.00001584
Iteration 42/1000 | Loss: 0.00001583
Iteration 43/1000 | Loss: 0.00001583
Iteration 44/1000 | Loss: 0.00001582
Iteration 45/1000 | Loss: 0.00001581
Iteration 46/1000 | Loss: 0.00001580
Iteration 47/1000 | Loss: 0.00001580
Iteration 48/1000 | Loss: 0.00001579
Iteration 49/1000 | Loss: 0.00001579
Iteration 50/1000 | Loss: 0.00001578
Iteration 51/1000 | Loss: 0.00001578
Iteration 52/1000 | Loss: 0.00001578
Iteration 53/1000 | Loss: 0.00001577
Iteration 54/1000 | Loss: 0.00001577
Iteration 55/1000 | Loss: 0.00001577
Iteration 56/1000 | Loss: 0.00001577
Iteration 57/1000 | Loss: 0.00001577
Iteration 58/1000 | Loss: 0.00001576
Iteration 59/1000 | Loss: 0.00001576
Iteration 60/1000 | Loss: 0.00001576
Iteration 61/1000 | Loss: 0.00001576
Iteration 62/1000 | Loss: 0.00001575
Iteration 63/1000 | Loss: 0.00001575
Iteration 64/1000 | Loss: 0.00001575
Iteration 65/1000 | Loss: 0.00001575
Iteration 66/1000 | Loss: 0.00001575
Iteration 67/1000 | Loss: 0.00001575
Iteration 68/1000 | Loss: 0.00001575
Iteration 69/1000 | Loss: 0.00001574
Iteration 70/1000 | Loss: 0.00001573
Iteration 71/1000 | Loss: 0.00001573
Iteration 72/1000 | Loss: 0.00001573
Iteration 73/1000 | Loss: 0.00001572
Iteration 74/1000 | Loss: 0.00001572
Iteration 75/1000 | Loss: 0.00001572
Iteration 76/1000 | Loss: 0.00001572
Iteration 77/1000 | Loss: 0.00001572
Iteration 78/1000 | Loss: 0.00001572
Iteration 79/1000 | Loss: 0.00001571
Iteration 80/1000 | Loss: 0.00001571
Iteration 81/1000 | Loss: 0.00001571
Iteration 82/1000 | Loss: 0.00001570
Iteration 83/1000 | Loss: 0.00001570
Iteration 84/1000 | Loss: 0.00001570
Iteration 85/1000 | Loss: 0.00001569
Iteration 86/1000 | Loss: 0.00001569
Iteration 87/1000 | Loss: 0.00001568
Iteration 88/1000 | Loss: 0.00001568
Iteration 89/1000 | Loss: 0.00001568
Iteration 90/1000 | Loss: 0.00001567
Iteration 91/1000 | Loss: 0.00001567
Iteration 92/1000 | Loss: 0.00001566
Iteration 93/1000 | Loss: 0.00001566
Iteration 94/1000 | Loss: 0.00001566
Iteration 95/1000 | Loss: 0.00001566
Iteration 96/1000 | Loss: 0.00001566
Iteration 97/1000 | Loss: 0.00001566
Iteration 98/1000 | Loss: 0.00001565
Iteration 99/1000 | Loss: 0.00001565
Iteration 100/1000 | Loss: 0.00001564
Iteration 101/1000 | Loss: 0.00001564
Iteration 102/1000 | Loss: 0.00001564
Iteration 103/1000 | Loss: 0.00001563
Iteration 104/1000 | Loss: 0.00001563
Iteration 105/1000 | Loss: 0.00001563
Iteration 106/1000 | Loss: 0.00001563
Iteration 107/1000 | Loss: 0.00001562
Iteration 108/1000 | Loss: 0.00001562
Iteration 109/1000 | Loss: 0.00001561
Iteration 110/1000 | Loss: 0.00001561
Iteration 111/1000 | Loss: 0.00001561
Iteration 112/1000 | Loss: 0.00001561
Iteration 113/1000 | Loss: 0.00001560
Iteration 114/1000 | Loss: 0.00001560
Iteration 115/1000 | Loss: 0.00001560
Iteration 116/1000 | Loss: 0.00001560
Iteration 117/1000 | Loss: 0.00001560
Iteration 118/1000 | Loss: 0.00001560
Iteration 119/1000 | Loss: 0.00001559
Iteration 120/1000 | Loss: 0.00001559
Iteration 121/1000 | Loss: 0.00001559
Iteration 122/1000 | Loss: 0.00001559
Iteration 123/1000 | Loss: 0.00001559
Iteration 124/1000 | Loss: 0.00001559
Iteration 125/1000 | Loss: 0.00001559
Iteration 126/1000 | Loss: 0.00001559
Iteration 127/1000 | Loss: 0.00001558
Iteration 128/1000 | Loss: 0.00001558
Iteration 129/1000 | Loss: 0.00001558
Iteration 130/1000 | Loss: 0.00001558
Iteration 131/1000 | Loss: 0.00001558
Iteration 132/1000 | Loss: 0.00001558
Iteration 133/1000 | Loss: 0.00001558
Iteration 134/1000 | Loss: 0.00001557
Iteration 135/1000 | Loss: 0.00001557
Iteration 136/1000 | Loss: 0.00001557
Iteration 137/1000 | Loss: 0.00001557
Iteration 138/1000 | Loss: 0.00001557
Iteration 139/1000 | Loss: 0.00001557
Iteration 140/1000 | Loss: 0.00001557
Iteration 141/1000 | Loss: 0.00001557
Iteration 142/1000 | Loss: 0.00001557
Iteration 143/1000 | Loss: 0.00001556
Iteration 144/1000 | Loss: 0.00001556
Iteration 145/1000 | Loss: 0.00001556
Iteration 146/1000 | Loss: 0.00001556
Iteration 147/1000 | Loss: 0.00001556
Iteration 148/1000 | Loss: 0.00001556
Iteration 149/1000 | Loss: 0.00001556
Iteration 150/1000 | Loss: 0.00001555
Iteration 151/1000 | Loss: 0.00001555
Iteration 152/1000 | Loss: 0.00001555
Iteration 153/1000 | Loss: 0.00001555
Iteration 154/1000 | Loss: 0.00001555
Iteration 155/1000 | Loss: 0.00001554
Iteration 156/1000 | Loss: 0.00001554
Iteration 157/1000 | Loss: 0.00001554
Iteration 158/1000 | Loss: 0.00001554
Iteration 159/1000 | Loss: 0.00001554
Iteration 160/1000 | Loss: 0.00001554
Iteration 161/1000 | Loss: 0.00001553
Iteration 162/1000 | Loss: 0.00001553
Iteration 163/1000 | Loss: 0.00001553
Iteration 164/1000 | Loss: 0.00001553
Iteration 165/1000 | Loss: 0.00001552
Iteration 166/1000 | Loss: 0.00001552
Iteration 167/1000 | Loss: 0.00001552
Iteration 168/1000 | Loss: 0.00001551
Iteration 169/1000 | Loss: 0.00001551
Iteration 170/1000 | Loss: 0.00001551
Iteration 171/1000 | Loss: 0.00001551
Iteration 172/1000 | Loss: 0.00001551
Iteration 173/1000 | Loss: 0.00001551
Iteration 174/1000 | Loss: 0.00001550
Iteration 175/1000 | Loss: 0.00001550
Iteration 176/1000 | Loss: 0.00001550
Iteration 177/1000 | Loss: 0.00001550
Iteration 178/1000 | Loss: 0.00001550
Iteration 179/1000 | Loss: 0.00001550
Iteration 180/1000 | Loss: 0.00001550
Iteration 181/1000 | Loss: 0.00001549
Iteration 182/1000 | Loss: 0.00001549
Iteration 183/1000 | Loss: 0.00001549
Iteration 184/1000 | Loss: 0.00001549
Iteration 185/1000 | Loss: 0.00001549
Iteration 186/1000 | Loss: 0.00001549
Iteration 187/1000 | Loss: 0.00001548
Iteration 188/1000 | Loss: 0.00001548
Iteration 189/1000 | Loss: 0.00001548
Iteration 190/1000 | Loss: 0.00001548
Iteration 191/1000 | Loss: 0.00001548
Iteration 192/1000 | Loss: 0.00001548
Iteration 193/1000 | Loss: 0.00001548
Iteration 194/1000 | Loss: 0.00001548
Iteration 195/1000 | Loss: 0.00001548
Iteration 196/1000 | Loss: 0.00001548
Iteration 197/1000 | Loss: 0.00001548
Iteration 198/1000 | Loss: 0.00001548
Iteration 199/1000 | Loss: 0.00001547
Iteration 200/1000 | Loss: 0.00001547
Iteration 201/1000 | Loss: 0.00001547
Iteration 202/1000 | Loss: 0.00001547
Iteration 203/1000 | Loss: 0.00001547
Iteration 204/1000 | Loss: 0.00001547
Iteration 205/1000 | Loss: 0.00001546
Iteration 206/1000 | Loss: 0.00001546
Iteration 207/1000 | Loss: 0.00001546
Iteration 208/1000 | Loss: 0.00001546
Iteration 209/1000 | Loss: 0.00001546
Iteration 210/1000 | Loss: 0.00001546
Iteration 211/1000 | Loss: 0.00001546
Iteration 212/1000 | Loss: 0.00001546
Iteration 213/1000 | Loss: 0.00001546
Iteration 214/1000 | Loss: 0.00001546
Iteration 215/1000 | Loss: 0.00001546
Iteration 216/1000 | Loss: 0.00001546
Iteration 217/1000 | Loss: 0.00001546
Iteration 218/1000 | Loss: 0.00001546
Iteration 219/1000 | Loss: 0.00001546
Iteration 220/1000 | Loss: 0.00001546
Iteration 221/1000 | Loss: 0.00001546
Iteration 222/1000 | Loss: 0.00001546
Iteration 223/1000 | Loss: 0.00001546
Iteration 224/1000 | Loss: 0.00001546
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 224. Stopping optimization.
Last 5 losses: [1.5458252164535224e-05, 1.5458252164535224e-05, 1.5458252164535224e-05, 1.5458252164535224e-05, 1.5458252164535224e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5458252164535224e-05

Optimization complete. Final v2v error: 3.3157145977020264 mm

Highest mean error: 4.966525554656982 mm for frame 69

Lowest mean error: 2.871220588684082 mm for frame 50

Saving results

Total time: 45.905709981918335
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00381350
Iteration 2/25 | Loss: 0.00120663
Iteration 3/25 | Loss: 0.00109078
Iteration 4/25 | Loss: 0.00108270
Iteration 5/25 | Loss: 0.00108041
Iteration 6/25 | Loss: 0.00108041
Iteration 7/25 | Loss: 0.00108041
Iteration 8/25 | Loss: 0.00108041
Iteration 9/25 | Loss: 0.00108041
Iteration 10/25 | Loss: 0.00108041
Iteration 11/25 | Loss: 0.00108041
Iteration 12/25 | Loss: 0.00108041
Iteration 13/25 | Loss: 0.00108041
Iteration 14/25 | Loss: 0.00108041
Iteration 15/25 | Loss: 0.00108041
Iteration 16/25 | Loss: 0.00108041
Iteration 17/25 | Loss: 0.00108041
Iteration 18/25 | Loss: 0.00108041
Iteration 19/25 | Loss: 0.00108041
Iteration 20/25 | Loss: 0.00108041
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010804114863276482, 0.0010804114863276482, 0.0010804114863276482, 0.0010804114863276482, 0.0010804114863276482]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010804114863276482

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51277757
Iteration 2/25 | Loss: 0.00072164
Iteration 3/25 | Loss: 0.00072164
Iteration 4/25 | Loss: 0.00072164
Iteration 5/25 | Loss: 0.00072164
Iteration 6/25 | Loss: 0.00072164
Iteration 7/25 | Loss: 0.00072164
Iteration 8/25 | Loss: 0.00072164
Iteration 9/25 | Loss: 0.00072164
Iteration 10/25 | Loss: 0.00072164
Iteration 11/25 | Loss: 0.00072164
Iteration 12/25 | Loss: 0.00072164
Iteration 13/25 | Loss: 0.00072164
Iteration 14/25 | Loss: 0.00072164
Iteration 15/25 | Loss: 0.00072164
Iteration 16/25 | Loss: 0.00072164
Iteration 17/25 | Loss: 0.00072164
Iteration 18/25 | Loss: 0.00072164
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007216419908218086, 0.0007216419908218086, 0.0007216419908218086, 0.0007216419908218086, 0.0007216419908218086]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007216419908218086

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072164
Iteration 2/1000 | Loss: 0.00002119
Iteration 3/1000 | Loss: 0.00001312
Iteration 4/1000 | Loss: 0.00001184
Iteration 5/1000 | Loss: 0.00001128
Iteration 6/1000 | Loss: 0.00001070
Iteration 7/1000 | Loss: 0.00001034
Iteration 8/1000 | Loss: 0.00001007
Iteration 9/1000 | Loss: 0.00000975
Iteration 10/1000 | Loss: 0.00000954
Iteration 11/1000 | Loss: 0.00000942
Iteration 12/1000 | Loss: 0.00000939
Iteration 13/1000 | Loss: 0.00000933
Iteration 14/1000 | Loss: 0.00000928
Iteration 15/1000 | Loss: 0.00000928
Iteration 16/1000 | Loss: 0.00000927
Iteration 17/1000 | Loss: 0.00000927
Iteration 18/1000 | Loss: 0.00000927
Iteration 19/1000 | Loss: 0.00000926
Iteration 20/1000 | Loss: 0.00000926
Iteration 21/1000 | Loss: 0.00000925
Iteration 22/1000 | Loss: 0.00000924
Iteration 23/1000 | Loss: 0.00000924
Iteration 24/1000 | Loss: 0.00000924
Iteration 25/1000 | Loss: 0.00000923
Iteration 26/1000 | Loss: 0.00000923
Iteration 27/1000 | Loss: 0.00000923
Iteration 28/1000 | Loss: 0.00000922
Iteration 29/1000 | Loss: 0.00000922
Iteration 30/1000 | Loss: 0.00000921
Iteration 31/1000 | Loss: 0.00000921
Iteration 32/1000 | Loss: 0.00000920
Iteration 33/1000 | Loss: 0.00000920
Iteration 34/1000 | Loss: 0.00000920
Iteration 35/1000 | Loss: 0.00000919
Iteration 36/1000 | Loss: 0.00000919
Iteration 37/1000 | Loss: 0.00000919
Iteration 38/1000 | Loss: 0.00000919
Iteration 39/1000 | Loss: 0.00000918
Iteration 40/1000 | Loss: 0.00000918
Iteration 41/1000 | Loss: 0.00000917
Iteration 42/1000 | Loss: 0.00000917
Iteration 43/1000 | Loss: 0.00000917
Iteration 44/1000 | Loss: 0.00000916
Iteration 45/1000 | Loss: 0.00000916
Iteration 46/1000 | Loss: 0.00000916
Iteration 47/1000 | Loss: 0.00000916
Iteration 48/1000 | Loss: 0.00000916
Iteration 49/1000 | Loss: 0.00000916
Iteration 50/1000 | Loss: 0.00000916
Iteration 51/1000 | Loss: 0.00000916
Iteration 52/1000 | Loss: 0.00000915
Iteration 53/1000 | Loss: 0.00000915
Iteration 54/1000 | Loss: 0.00000914
Iteration 55/1000 | Loss: 0.00000914
Iteration 56/1000 | Loss: 0.00000912
Iteration 57/1000 | Loss: 0.00000912
Iteration 58/1000 | Loss: 0.00000912
Iteration 59/1000 | Loss: 0.00000911
Iteration 60/1000 | Loss: 0.00000911
Iteration 61/1000 | Loss: 0.00000911
Iteration 62/1000 | Loss: 0.00000911
Iteration 63/1000 | Loss: 0.00000911
Iteration 64/1000 | Loss: 0.00000911
Iteration 65/1000 | Loss: 0.00000911
Iteration 66/1000 | Loss: 0.00000911
Iteration 67/1000 | Loss: 0.00000910
Iteration 68/1000 | Loss: 0.00000910
Iteration 69/1000 | Loss: 0.00000910
Iteration 70/1000 | Loss: 0.00000909
Iteration 71/1000 | Loss: 0.00000909
Iteration 72/1000 | Loss: 0.00000908
Iteration 73/1000 | Loss: 0.00000908
Iteration 74/1000 | Loss: 0.00000908
Iteration 75/1000 | Loss: 0.00000908
Iteration 76/1000 | Loss: 0.00000908
Iteration 77/1000 | Loss: 0.00000908
Iteration 78/1000 | Loss: 0.00000907
Iteration 79/1000 | Loss: 0.00000907
Iteration 80/1000 | Loss: 0.00000907
Iteration 81/1000 | Loss: 0.00000907
Iteration 82/1000 | Loss: 0.00000907
Iteration 83/1000 | Loss: 0.00000906
Iteration 84/1000 | Loss: 0.00000906
Iteration 85/1000 | Loss: 0.00000906
Iteration 86/1000 | Loss: 0.00000905
Iteration 87/1000 | Loss: 0.00000905
Iteration 88/1000 | Loss: 0.00000904
Iteration 89/1000 | Loss: 0.00000904
Iteration 90/1000 | Loss: 0.00000904
Iteration 91/1000 | Loss: 0.00000904
Iteration 92/1000 | Loss: 0.00000904
Iteration 93/1000 | Loss: 0.00000903
Iteration 94/1000 | Loss: 0.00000903
Iteration 95/1000 | Loss: 0.00000902
Iteration 96/1000 | Loss: 0.00000902
Iteration 97/1000 | Loss: 0.00000902
Iteration 98/1000 | Loss: 0.00000902
Iteration 99/1000 | Loss: 0.00000901
Iteration 100/1000 | Loss: 0.00000901
Iteration 101/1000 | Loss: 0.00000901
Iteration 102/1000 | Loss: 0.00000901
Iteration 103/1000 | Loss: 0.00000901
Iteration 104/1000 | Loss: 0.00000901
Iteration 105/1000 | Loss: 0.00000901
Iteration 106/1000 | Loss: 0.00000900
Iteration 107/1000 | Loss: 0.00000900
Iteration 108/1000 | Loss: 0.00000900
Iteration 109/1000 | Loss: 0.00000900
Iteration 110/1000 | Loss: 0.00000899
Iteration 111/1000 | Loss: 0.00000899
Iteration 112/1000 | Loss: 0.00000899
Iteration 113/1000 | Loss: 0.00000899
Iteration 114/1000 | Loss: 0.00000899
Iteration 115/1000 | Loss: 0.00000899
Iteration 116/1000 | Loss: 0.00000899
Iteration 117/1000 | Loss: 0.00000899
Iteration 118/1000 | Loss: 0.00000899
Iteration 119/1000 | Loss: 0.00000899
Iteration 120/1000 | Loss: 0.00000898
Iteration 121/1000 | Loss: 0.00000898
Iteration 122/1000 | Loss: 0.00000898
Iteration 123/1000 | Loss: 0.00000898
Iteration 124/1000 | Loss: 0.00000898
Iteration 125/1000 | Loss: 0.00000898
Iteration 126/1000 | Loss: 0.00000898
Iteration 127/1000 | Loss: 0.00000898
Iteration 128/1000 | Loss: 0.00000898
Iteration 129/1000 | Loss: 0.00000898
Iteration 130/1000 | Loss: 0.00000898
Iteration 131/1000 | Loss: 0.00000898
Iteration 132/1000 | Loss: 0.00000898
Iteration 133/1000 | Loss: 0.00000898
Iteration 134/1000 | Loss: 0.00000898
Iteration 135/1000 | Loss: 0.00000898
Iteration 136/1000 | Loss: 0.00000898
Iteration 137/1000 | Loss: 0.00000898
Iteration 138/1000 | Loss: 0.00000898
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [8.976497156254482e-06, 8.976497156254482e-06, 8.976497156254482e-06, 8.976497156254482e-06, 8.976497156254482e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.976497156254482e-06

Optimization complete. Final v2v error: 2.5720324516296387 mm

Highest mean error: 2.717979907989502 mm for frame 237

Lowest mean error: 2.4149248600006104 mm for frame 133

Saving results

Total time: 39.06862998008728
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00784108
Iteration 2/25 | Loss: 0.00137568
Iteration 3/25 | Loss: 0.00119237
Iteration 4/25 | Loss: 0.00118082
Iteration 5/25 | Loss: 0.00118015
Iteration 6/25 | Loss: 0.00118015
Iteration 7/25 | Loss: 0.00118015
Iteration 8/25 | Loss: 0.00118015
Iteration 9/25 | Loss: 0.00118015
Iteration 10/25 | Loss: 0.00118015
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011801532236859202, 0.0011801532236859202, 0.0011801532236859202, 0.0011801532236859202, 0.0011801532236859202]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011801532236859202

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32763875
Iteration 2/25 | Loss: 0.00056553
Iteration 3/25 | Loss: 0.00056553
Iteration 4/25 | Loss: 0.00056553
Iteration 5/25 | Loss: 0.00056553
Iteration 6/25 | Loss: 0.00056553
Iteration 7/25 | Loss: 0.00056552
Iteration 8/25 | Loss: 0.00056552
Iteration 9/25 | Loss: 0.00056552
Iteration 10/25 | Loss: 0.00056552
Iteration 11/25 | Loss: 0.00056552
Iteration 12/25 | Loss: 0.00056552
Iteration 13/25 | Loss: 0.00056552
Iteration 14/25 | Loss: 0.00056552
Iteration 15/25 | Loss: 0.00056552
Iteration 16/25 | Loss: 0.00056552
Iteration 17/25 | Loss: 0.00056552
Iteration 18/25 | Loss: 0.00056552
Iteration 19/25 | Loss: 0.00056552
Iteration 20/25 | Loss: 0.00056552
Iteration 21/25 | Loss: 0.00056552
Iteration 22/25 | Loss: 0.00056552
Iteration 23/25 | Loss: 0.00056552
Iteration 24/25 | Loss: 0.00056552
Iteration 25/25 | Loss: 0.00056552

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056552
Iteration 2/1000 | Loss: 0.00002848
Iteration 3/1000 | Loss: 0.00002258
Iteration 4/1000 | Loss: 0.00002103
Iteration 5/1000 | Loss: 0.00002013
Iteration 6/1000 | Loss: 0.00001944
Iteration 7/1000 | Loss: 0.00001885
Iteration 8/1000 | Loss: 0.00001845
Iteration 9/1000 | Loss: 0.00001822
Iteration 10/1000 | Loss: 0.00001818
Iteration 11/1000 | Loss: 0.00001814
Iteration 12/1000 | Loss: 0.00001807
Iteration 13/1000 | Loss: 0.00001803
Iteration 14/1000 | Loss: 0.00001798
Iteration 15/1000 | Loss: 0.00001797
Iteration 16/1000 | Loss: 0.00001796
Iteration 17/1000 | Loss: 0.00001796
Iteration 18/1000 | Loss: 0.00001795
Iteration 19/1000 | Loss: 0.00001795
Iteration 20/1000 | Loss: 0.00001795
Iteration 21/1000 | Loss: 0.00001793
Iteration 22/1000 | Loss: 0.00001793
Iteration 23/1000 | Loss: 0.00001792
Iteration 24/1000 | Loss: 0.00001792
Iteration 25/1000 | Loss: 0.00001791
Iteration 26/1000 | Loss: 0.00001791
Iteration 27/1000 | Loss: 0.00001790
Iteration 28/1000 | Loss: 0.00001788
Iteration 29/1000 | Loss: 0.00001787
Iteration 30/1000 | Loss: 0.00001787
Iteration 31/1000 | Loss: 0.00001786
Iteration 32/1000 | Loss: 0.00001785
Iteration 33/1000 | Loss: 0.00001785
Iteration 34/1000 | Loss: 0.00001784
Iteration 35/1000 | Loss: 0.00001784
Iteration 36/1000 | Loss: 0.00001784
Iteration 37/1000 | Loss: 0.00001784
Iteration 38/1000 | Loss: 0.00001784
Iteration 39/1000 | Loss: 0.00001784
Iteration 40/1000 | Loss: 0.00001784
Iteration 41/1000 | Loss: 0.00001784
Iteration 42/1000 | Loss: 0.00001783
Iteration 43/1000 | Loss: 0.00001783
Iteration 44/1000 | Loss: 0.00001783
Iteration 45/1000 | Loss: 0.00001783
Iteration 46/1000 | Loss: 0.00001783
Iteration 47/1000 | Loss: 0.00001782
Iteration 48/1000 | Loss: 0.00001782
Iteration 49/1000 | Loss: 0.00001781
Iteration 50/1000 | Loss: 0.00001781
Iteration 51/1000 | Loss: 0.00001781
Iteration 52/1000 | Loss: 0.00001781
Iteration 53/1000 | Loss: 0.00001780
Iteration 54/1000 | Loss: 0.00001780
Iteration 55/1000 | Loss: 0.00001780
Iteration 56/1000 | Loss: 0.00001779
Iteration 57/1000 | Loss: 0.00001779
Iteration 58/1000 | Loss: 0.00001779
Iteration 59/1000 | Loss: 0.00001779
Iteration 60/1000 | Loss: 0.00001779
Iteration 61/1000 | Loss: 0.00001778
Iteration 62/1000 | Loss: 0.00001778
Iteration 63/1000 | Loss: 0.00001778
Iteration 64/1000 | Loss: 0.00001778
Iteration 65/1000 | Loss: 0.00001778
Iteration 66/1000 | Loss: 0.00001778
Iteration 67/1000 | Loss: 0.00001778
Iteration 68/1000 | Loss: 0.00001778
Iteration 69/1000 | Loss: 0.00001778
Iteration 70/1000 | Loss: 0.00001778
Iteration 71/1000 | Loss: 0.00001778
Iteration 72/1000 | Loss: 0.00001778
Iteration 73/1000 | Loss: 0.00001778
Iteration 74/1000 | Loss: 0.00001778
Iteration 75/1000 | Loss: 0.00001778
Iteration 76/1000 | Loss: 0.00001778
Iteration 77/1000 | Loss: 0.00001778
Iteration 78/1000 | Loss: 0.00001778
Iteration 79/1000 | Loss: 0.00001778
Iteration 80/1000 | Loss: 0.00001778
Iteration 81/1000 | Loss: 0.00001778
Iteration 82/1000 | Loss: 0.00001778
Iteration 83/1000 | Loss: 0.00001778
Iteration 84/1000 | Loss: 0.00001778
Iteration 85/1000 | Loss: 0.00001778
Iteration 86/1000 | Loss: 0.00001778
Iteration 87/1000 | Loss: 0.00001778
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 87. Stopping optimization.
Last 5 losses: [1.7779706467990763e-05, 1.7779706467990763e-05, 1.7779706467990763e-05, 1.7779706467990763e-05, 1.7779706467990763e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7779706467990763e-05

Optimization complete. Final v2v error: 3.519204616546631 mm

Highest mean error: 3.7565200328826904 mm for frame 23

Lowest mean error: 3.3026742935180664 mm for frame 88

Saving results

Total time: 28.796128034591675
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00398198
Iteration 2/25 | Loss: 0.00112691
Iteration 3/25 | Loss: 0.00105863
Iteration 4/25 | Loss: 0.00104905
Iteration 5/25 | Loss: 0.00104664
Iteration 6/25 | Loss: 0.00104664
Iteration 7/25 | Loss: 0.00104664
Iteration 8/25 | Loss: 0.00104664
Iteration 9/25 | Loss: 0.00104664
Iteration 10/25 | Loss: 0.00104664
Iteration 11/25 | Loss: 0.00104664
Iteration 12/25 | Loss: 0.00104664
Iteration 13/25 | Loss: 0.00104664
Iteration 14/25 | Loss: 0.00104664
Iteration 15/25 | Loss: 0.00104664
Iteration 16/25 | Loss: 0.00104664
Iteration 17/25 | Loss: 0.00104664
Iteration 18/25 | Loss: 0.00104664
Iteration 19/25 | Loss: 0.00104664
Iteration 20/25 | Loss: 0.00104664
Iteration 21/25 | Loss: 0.00104664
Iteration 22/25 | Loss: 0.00104664
Iteration 23/25 | Loss: 0.00104664
Iteration 24/25 | Loss: 0.00104664
Iteration 25/25 | Loss: 0.00104664

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.96110535
Iteration 2/25 | Loss: 0.00074247
Iteration 3/25 | Loss: 0.00074247
Iteration 4/25 | Loss: 0.00074247
Iteration 5/25 | Loss: 0.00074247
Iteration 6/25 | Loss: 0.00074247
Iteration 7/25 | Loss: 0.00074247
Iteration 8/25 | Loss: 0.00074247
Iteration 9/25 | Loss: 0.00074247
Iteration 10/25 | Loss: 0.00074247
Iteration 11/25 | Loss: 0.00074247
Iteration 12/25 | Loss: 0.00074247
Iteration 13/25 | Loss: 0.00074247
Iteration 14/25 | Loss: 0.00074247
Iteration 15/25 | Loss: 0.00074247
Iteration 16/25 | Loss: 0.00074247
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007424678187817335, 0.0007424678187817335, 0.0007424678187817335, 0.0007424678187817335, 0.0007424678187817335]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007424678187817335

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074247
Iteration 2/1000 | Loss: 0.00001609
Iteration 3/1000 | Loss: 0.00001285
Iteration 4/1000 | Loss: 0.00001179
Iteration 5/1000 | Loss: 0.00001124
Iteration 6/1000 | Loss: 0.00001080
Iteration 7/1000 | Loss: 0.00001057
Iteration 8/1000 | Loss: 0.00001034
Iteration 9/1000 | Loss: 0.00001013
Iteration 10/1000 | Loss: 0.00001001
Iteration 11/1000 | Loss: 0.00001000
Iteration 12/1000 | Loss: 0.00000995
Iteration 13/1000 | Loss: 0.00000994
Iteration 14/1000 | Loss: 0.00000993
Iteration 15/1000 | Loss: 0.00000992
Iteration 16/1000 | Loss: 0.00000991
Iteration 17/1000 | Loss: 0.00000983
Iteration 18/1000 | Loss: 0.00000971
Iteration 19/1000 | Loss: 0.00000960
Iteration 20/1000 | Loss: 0.00000956
Iteration 21/1000 | Loss: 0.00000955
Iteration 22/1000 | Loss: 0.00000955
Iteration 23/1000 | Loss: 0.00000954
Iteration 24/1000 | Loss: 0.00000954
Iteration 25/1000 | Loss: 0.00000953
Iteration 26/1000 | Loss: 0.00000953
Iteration 27/1000 | Loss: 0.00000952
Iteration 28/1000 | Loss: 0.00000952
Iteration 29/1000 | Loss: 0.00000952
Iteration 30/1000 | Loss: 0.00000950
Iteration 31/1000 | Loss: 0.00000949
Iteration 32/1000 | Loss: 0.00000948
Iteration 33/1000 | Loss: 0.00000946
Iteration 34/1000 | Loss: 0.00000945
Iteration 35/1000 | Loss: 0.00000945
Iteration 36/1000 | Loss: 0.00000945
Iteration 37/1000 | Loss: 0.00000945
Iteration 38/1000 | Loss: 0.00000944
Iteration 39/1000 | Loss: 0.00000938
Iteration 40/1000 | Loss: 0.00000938
Iteration 41/1000 | Loss: 0.00000936
Iteration 42/1000 | Loss: 0.00000936
Iteration 43/1000 | Loss: 0.00000929
Iteration 44/1000 | Loss: 0.00000929
Iteration 45/1000 | Loss: 0.00000929
Iteration 46/1000 | Loss: 0.00000929
Iteration 47/1000 | Loss: 0.00000928
Iteration 48/1000 | Loss: 0.00000928
Iteration 49/1000 | Loss: 0.00000928
Iteration 50/1000 | Loss: 0.00000927
Iteration 51/1000 | Loss: 0.00000926
Iteration 52/1000 | Loss: 0.00000926
Iteration 53/1000 | Loss: 0.00000926
Iteration 54/1000 | Loss: 0.00000926
Iteration 55/1000 | Loss: 0.00000925
Iteration 56/1000 | Loss: 0.00000925
Iteration 57/1000 | Loss: 0.00000925
Iteration 58/1000 | Loss: 0.00000925
Iteration 59/1000 | Loss: 0.00000925
Iteration 60/1000 | Loss: 0.00000925
Iteration 61/1000 | Loss: 0.00000925
Iteration 62/1000 | Loss: 0.00000925
Iteration 63/1000 | Loss: 0.00000924
Iteration 64/1000 | Loss: 0.00000924
Iteration 65/1000 | Loss: 0.00000924
Iteration 66/1000 | Loss: 0.00000924
Iteration 67/1000 | Loss: 0.00000924
Iteration 68/1000 | Loss: 0.00000924
Iteration 69/1000 | Loss: 0.00000924
Iteration 70/1000 | Loss: 0.00000923
Iteration 71/1000 | Loss: 0.00000923
Iteration 72/1000 | Loss: 0.00000923
Iteration 73/1000 | Loss: 0.00000923
Iteration 74/1000 | Loss: 0.00000923
Iteration 75/1000 | Loss: 0.00000922
Iteration 76/1000 | Loss: 0.00000922
Iteration 77/1000 | Loss: 0.00000922
Iteration 78/1000 | Loss: 0.00000922
Iteration 79/1000 | Loss: 0.00000922
Iteration 80/1000 | Loss: 0.00000922
Iteration 81/1000 | Loss: 0.00000921
Iteration 82/1000 | Loss: 0.00000921
Iteration 83/1000 | Loss: 0.00000921
Iteration 84/1000 | Loss: 0.00000920
Iteration 85/1000 | Loss: 0.00000920
Iteration 86/1000 | Loss: 0.00000920
Iteration 87/1000 | Loss: 0.00000919
Iteration 88/1000 | Loss: 0.00000919
Iteration 89/1000 | Loss: 0.00000919
Iteration 90/1000 | Loss: 0.00000918
Iteration 91/1000 | Loss: 0.00000917
Iteration 92/1000 | Loss: 0.00000917
Iteration 93/1000 | Loss: 0.00000916
Iteration 94/1000 | Loss: 0.00000916
Iteration 95/1000 | Loss: 0.00000915
Iteration 96/1000 | Loss: 0.00000915
Iteration 97/1000 | Loss: 0.00000915
Iteration 98/1000 | Loss: 0.00000915
Iteration 99/1000 | Loss: 0.00000914
Iteration 100/1000 | Loss: 0.00000914
Iteration 101/1000 | Loss: 0.00000913
Iteration 102/1000 | Loss: 0.00000913
Iteration 103/1000 | Loss: 0.00000913
Iteration 104/1000 | Loss: 0.00000913
Iteration 105/1000 | Loss: 0.00000912
Iteration 106/1000 | Loss: 0.00000911
Iteration 107/1000 | Loss: 0.00000911
Iteration 108/1000 | Loss: 0.00000911
Iteration 109/1000 | Loss: 0.00000911
Iteration 110/1000 | Loss: 0.00000910
Iteration 111/1000 | Loss: 0.00000910
Iteration 112/1000 | Loss: 0.00000910
Iteration 113/1000 | Loss: 0.00000910
Iteration 114/1000 | Loss: 0.00000910
Iteration 115/1000 | Loss: 0.00000910
Iteration 116/1000 | Loss: 0.00000910
Iteration 117/1000 | Loss: 0.00000910
Iteration 118/1000 | Loss: 0.00000909
Iteration 119/1000 | Loss: 0.00000909
Iteration 120/1000 | Loss: 0.00000909
Iteration 121/1000 | Loss: 0.00000909
Iteration 122/1000 | Loss: 0.00000909
Iteration 123/1000 | Loss: 0.00000908
Iteration 124/1000 | Loss: 0.00000908
Iteration 125/1000 | Loss: 0.00000908
Iteration 126/1000 | Loss: 0.00000908
Iteration 127/1000 | Loss: 0.00000908
Iteration 128/1000 | Loss: 0.00000908
Iteration 129/1000 | Loss: 0.00000908
Iteration 130/1000 | Loss: 0.00000907
Iteration 131/1000 | Loss: 0.00000907
Iteration 132/1000 | Loss: 0.00000907
Iteration 133/1000 | Loss: 0.00000907
Iteration 134/1000 | Loss: 0.00000907
Iteration 135/1000 | Loss: 0.00000907
Iteration 136/1000 | Loss: 0.00000907
Iteration 137/1000 | Loss: 0.00000907
Iteration 138/1000 | Loss: 0.00000907
Iteration 139/1000 | Loss: 0.00000907
Iteration 140/1000 | Loss: 0.00000907
Iteration 141/1000 | Loss: 0.00000907
Iteration 142/1000 | Loss: 0.00000907
Iteration 143/1000 | Loss: 0.00000907
Iteration 144/1000 | Loss: 0.00000907
Iteration 145/1000 | Loss: 0.00000907
Iteration 146/1000 | Loss: 0.00000907
Iteration 147/1000 | Loss: 0.00000907
Iteration 148/1000 | Loss: 0.00000907
Iteration 149/1000 | Loss: 0.00000907
Iteration 150/1000 | Loss: 0.00000907
Iteration 151/1000 | Loss: 0.00000907
Iteration 152/1000 | Loss: 0.00000907
Iteration 153/1000 | Loss: 0.00000907
Iteration 154/1000 | Loss: 0.00000907
Iteration 155/1000 | Loss: 0.00000907
Iteration 156/1000 | Loss: 0.00000907
Iteration 157/1000 | Loss: 0.00000907
Iteration 158/1000 | Loss: 0.00000907
Iteration 159/1000 | Loss: 0.00000907
Iteration 160/1000 | Loss: 0.00000907
Iteration 161/1000 | Loss: 0.00000907
Iteration 162/1000 | Loss: 0.00000907
Iteration 163/1000 | Loss: 0.00000907
Iteration 164/1000 | Loss: 0.00000907
Iteration 165/1000 | Loss: 0.00000907
Iteration 166/1000 | Loss: 0.00000907
Iteration 167/1000 | Loss: 0.00000907
Iteration 168/1000 | Loss: 0.00000907
Iteration 169/1000 | Loss: 0.00000907
Iteration 170/1000 | Loss: 0.00000907
Iteration 171/1000 | Loss: 0.00000907
Iteration 172/1000 | Loss: 0.00000907
Iteration 173/1000 | Loss: 0.00000907
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [9.073063665709924e-06, 9.073063665709924e-06, 9.073063665709924e-06, 9.073063665709924e-06, 9.073063665709924e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.073063665709924e-06

Optimization complete. Final v2v error: 2.6162514686584473 mm

Highest mean error: 2.866495132446289 mm for frame 139

Lowest mean error: 2.4997429847717285 mm for frame 201

Saving results

Total time: 43.183886766433716
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00876303
Iteration 2/25 | Loss: 0.00118716
Iteration 3/25 | Loss: 0.00110815
Iteration 4/25 | Loss: 0.00109158
Iteration 5/25 | Loss: 0.00108628
Iteration 6/25 | Loss: 0.00108530
Iteration 7/25 | Loss: 0.00108530
Iteration 8/25 | Loss: 0.00108530
Iteration 9/25 | Loss: 0.00108530
Iteration 10/25 | Loss: 0.00108530
Iteration 11/25 | Loss: 0.00108530
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010853026760742068, 0.0010853026760742068, 0.0010853026760742068, 0.0010853026760742068, 0.0010853026760742068]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010853026760742068

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41968036
Iteration 2/25 | Loss: 0.00078714
Iteration 3/25 | Loss: 0.00078713
Iteration 4/25 | Loss: 0.00078713
Iteration 5/25 | Loss: 0.00078713
Iteration 6/25 | Loss: 0.00078713
Iteration 7/25 | Loss: 0.00078713
Iteration 8/25 | Loss: 0.00078713
Iteration 9/25 | Loss: 0.00078713
Iteration 10/25 | Loss: 0.00078713
Iteration 11/25 | Loss: 0.00078713
Iteration 12/25 | Loss: 0.00078713
Iteration 13/25 | Loss: 0.00078713
Iteration 14/25 | Loss: 0.00078713
Iteration 15/25 | Loss: 0.00078713
Iteration 16/25 | Loss: 0.00078713
Iteration 17/25 | Loss: 0.00078713
Iteration 18/25 | Loss: 0.00078713
Iteration 19/25 | Loss: 0.00078713
Iteration 20/25 | Loss: 0.00078713
Iteration 21/25 | Loss: 0.00078713
Iteration 22/25 | Loss: 0.00078713
Iteration 23/25 | Loss: 0.00078713
Iteration 24/25 | Loss: 0.00078713
Iteration 25/25 | Loss: 0.00078713

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078713
Iteration 2/1000 | Loss: 0.00002666
Iteration 3/1000 | Loss: 0.00001830
Iteration 4/1000 | Loss: 0.00001543
Iteration 5/1000 | Loss: 0.00001452
Iteration 6/1000 | Loss: 0.00001377
Iteration 7/1000 | Loss: 0.00001341
Iteration 8/1000 | Loss: 0.00001313
Iteration 9/1000 | Loss: 0.00001291
Iteration 10/1000 | Loss: 0.00001280
Iteration 11/1000 | Loss: 0.00001259
Iteration 12/1000 | Loss: 0.00001246
Iteration 13/1000 | Loss: 0.00001244
Iteration 14/1000 | Loss: 0.00001240
Iteration 15/1000 | Loss: 0.00001239
Iteration 16/1000 | Loss: 0.00001234
Iteration 17/1000 | Loss: 0.00001231
Iteration 18/1000 | Loss: 0.00001228
Iteration 19/1000 | Loss: 0.00001223
Iteration 20/1000 | Loss: 0.00001223
Iteration 21/1000 | Loss: 0.00001222
Iteration 22/1000 | Loss: 0.00001222
Iteration 23/1000 | Loss: 0.00001219
Iteration 24/1000 | Loss: 0.00001215
Iteration 25/1000 | Loss: 0.00001214
Iteration 26/1000 | Loss: 0.00001214
Iteration 27/1000 | Loss: 0.00001213
Iteration 28/1000 | Loss: 0.00001213
Iteration 29/1000 | Loss: 0.00001213
Iteration 30/1000 | Loss: 0.00001213
Iteration 31/1000 | Loss: 0.00001213
Iteration 32/1000 | Loss: 0.00001212
Iteration 33/1000 | Loss: 0.00001212
Iteration 34/1000 | Loss: 0.00001212
Iteration 35/1000 | Loss: 0.00001212
Iteration 36/1000 | Loss: 0.00001212
Iteration 37/1000 | Loss: 0.00001212
Iteration 38/1000 | Loss: 0.00001212
Iteration 39/1000 | Loss: 0.00001206
Iteration 40/1000 | Loss: 0.00001202
Iteration 41/1000 | Loss: 0.00001201
Iteration 42/1000 | Loss: 0.00001201
Iteration 43/1000 | Loss: 0.00001201
Iteration 44/1000 | Loss: 0.00001201
Iteration 45/1000 | Loss: 0.00001201
Iteration 46/1000 | Loss: 0.00001201
Iteration 47/1000 | Loss: 0.00001201
Iteration 48/1000 | Loss: 0.00001201
Iteration 49/1000 | Loss: 0.00001201
Iteration 50/1000 | Loss: 0.00001201
Iteration 51/1000 | Loss: 0.00001200
Iteration 52/1000 | Loss: 0.00001199
Iteration 53/1000 | Loss: 0.00001198
Iteration 54/1000 | Loss: 0.00001198
Iteration 55/1000 | Loss: 0.00001198
Iteration 56/1000 | Loss: 0.00001198
Iteration 57/1000 | Loss: 0.00001198
Iteration 58/1000 | Loss: 0.00001198
Iteration 59/1000 | Loss: 0.00001198
Iteration 60/1000 | Loss: 0.00001197
Iteration 61/1000 | Loss: 0.00001197
Iteration 62/1000 | Loss: 0.00001197
Iteration 63/1000 | Loss: 0.00001197
Iteration 64/1000 | Loss: 0.00001197
Iteration 65/1000 | Loss: 0.00001197
Iteration 66/1000 | Loss: 0.00001197
Iteration 67/1000 | Loss: 0.00001197
Iteration 68/1000 | Loss: 0.00001197
Iteration 69/1000 | Loss: 0.00001196
Iteration 70/1000 | Loss: 0.00001195
Iteration 71/1000 | Loss: 0.00001195
Iteration 72/1000 | Loss: 0.00001194
Iteration 73/1000 | Loss: 0.00001194
Iteration 74/1000 | Loss: 0.00001193
Iteration 75/1000 | Loss: 0.00001193
Iteration 76/1000 | Loss: 0.00001193
Iteration 77/1000 | Loss: 0.00001193
Iteration 78/1000 | Loss: 0.00001192
Iteration 79/1000 | Loss: 0.00001192
Iteration 80/1000 | Loss: 0.00001192
Iteration 81/1000 | Loss: 0.00001191
Iteration 82/1000 | Loss: 0.00001191
Iteration 83/1000 | Loss: 0.00001190
Iteration 84/1000 | Loss: 0.00001190
Iteration 85/1000 | Loss: 0.00001189
Iteration 86/1000 | Loss: 0.00001189
Iteration 87/1000 | Loss: 0.00001189
Iteration 88/1000 | Loss: 0.00001189
Iteration 89/1000 | Loss: 0.00001189
Iteration 90/1000 | Loss: 0.00001189
Iteration 91/1000 | Loss: 0.00001189
Iteration 92/1000 | Loss: 0.00001189
Iteration 93/1000 | Loss: 0.00001189
Iteration 94/1000 | Loss: 0.00001186
Iteration 95/1000 | Loss: 0.00001186
Iteration 96/1000 | Loss: 0.00001186
Iteration 97/1000 | Loss: 0.00001186
Iteration 98/1000 | Loss: 0.00001185
Iteration 99/1000 | Loss: 0.00001185
Iteration 100/1000 | Loss: 0.00001185
Iteration 101/1000 | Loss: 0.00001185
Iteration 102/1000 | Loss: 0.00001184
Iteration 103/1000 | Loss: 0.00001184
Iteration 104/1000 | Loss: 0.00001184
Iteration 105/1000 | Loss: 0.00001183
Iteration 106/1000 | Loss: 0.00001183
Iteration 107/1000 | Loss: 0.00001182
Iteration 108/1000 | Loss: 0.00001182
Iteration 109/1000 | Loss: 0.00001182
Iteration 110/1000 | Loss: 0.00001181
Iteration 111/1000 | Loss: 0.00001181
Iteration 112/1000 | Loss: 0.00001181
Iteration 113/1000 | Loss: 0.00001181
Iteration 114/1000 | Loss: 0.00001181
Iteration 115/1000 | Loss: 0.00001181
Iteration 116/1000 | Loss: 0.00001180
Iteration 117/1000 | Loss: 0.00001180
Iteration 118/1000 | Loss: 0.00001180
Iteration 119/1000 | Loss: 0.00001180
Iteration 120/1000 | Loss: 0.00001180
Iteration 121/1000 | Loss: 0.00001179
Iteration 122/1000 | Loss: 0.00001179
Iteration 123/1000 | Loss: 0.00001179
Iteration 124/1000 | Loss: 0.00001179
Iteration 125/1000 | Loss: 0.00001179
Iteration 126/1000 | Loss: 0.00001178
Iteration 127/1000 | Loss: 0.00001178
Iteration 128/1000 | Loss: 0.00001178
Iteration 129/1000 | Loss: 0.00001178
Iteration 130/1000 | Loss: 0.00001178
Iteration 131/1000 | Loss: 0.00001178
Iteration 132/1000 | Loss: 0.00001178
Iteration 133/1000 | Loss: 0.00001178
Iteration 134/1000 | Loss: 0.00001178
Iteration 135/1000 | Loss: 0.00001177
Iteration 136/1000 | Loss: 0.00001177
Iteration 137/1000 | Loss: 0.00001177
Iteration 138/1000 | Loss: 0.00001177
Iteration 139/1000 | Loss: 0.00001177
Iteration 140/1000 | Loss: 0.00001177
Iteration 141/1000 | Loss: 0.00001177
Iteration 142/1000 | Loss: 0.00001177
Iteration 143/1000 | Loss: 0.00001177
Iteration 144/1000 | Loss: 0.00001177
Iteration 145/1000 | Loss: 0.00001176
Iteration 146/1000 | Loss: 0.00001176
Iteration 147/1000 | Loss: 0.00001176
Iteration 148/1000 | Loss: 0.00001176
Iteration 149/1000 | Loss: 0.00001176
Iteration 150/1000 | Loss: 0.00001176
Iteration 151/1000 | Loss: 0.00001176
Iteration 152/1000 | Loss: 0.00001176
Iteration 153/1000 | Loss: 0.00001175
Iteration 154/1000 | Loss: 0.00001175
Iteration 155/1000 | Loss: 0.00001175
Iteration 156/1000 | Loss: 0.00001175
Iteration 157/1000 | Loss: 0.00001175
Iteration 158/1000 | Loss: 0.00001175
Iteration 159/1000 | Loss: 0.00001175
Iteration 160/1000 | Loss: 0.00001175
Iteration 161/1000 | Loss: 0.00001175
Iteration 162/1000 | Loss: 0.00001175
Iteration 163/1000 | Loss: 0.00001175
Iteration 164/1000 | Loss: 0.00001175
Iteration 165/1000 | Loss: 0.00001174
Iteration 166/1000 | Loss: 0.00001174
Iteration 167/1000 | Loss: 0.00001174
Iteration 168/1000 | Loss: 0.00001174
Iteration 169/1000 | Loss: 0.00001174
Iteration 170/1000 | Loss: 0.00001174
Iteration 171/1000 | Loss: 0.00001174
Iteration 172/1000 | Loss: 0.00001174
Iteration 173/1000 | Loss: 0.00001174
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [1.1744103176170029e-05, 1.1744103176170029e-05, 1.1744103176170029e-05, 1.1744103176170029e-05, 1.1744103176170029e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1744103176170029e-05

Optimization complete. Final v2v error: 2.929868459701538 mm

Highest mean error: 3.1876766681671143 mm for frame 85

Lowest mean error: 2.675429105758667 mm for frame 60

Saving results

Total time: 39.85314440727234
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00864570
Iteration 2/25 | Loss: 0.00116555
Iteration 3/25 | Loss: 0.00107764
Iteration 4/25 | Loss: 0.00105965
Iteration 5/25 | Loss: 0.00105261
Iteration 6/25 | Loss: 0.00105050
Iteration 7/25 | Loss: 0.00105020
Iteration 8/25 | Loss: 0.00105020
Iteration 9/25 | Loss: 0.00105020
Iteration 10/25 | Loss: 0.00105020
Iteration 11/25 | Loss: 0.00105020
Iteration 12/25 | Loss: 0.00105020
Iteration 13/25 | Loss: 0.00105020
Iteration 14/25 | Loss: 0.00105020
Iteration 15/25 | Loss: 0.00105020
Iteration 16/25 | Loss: 0.00105020
Iteration 17/25 | Loss: 0.00105020
Iteration 18/25 | Loss: 0.00105020
Iteration 19/25 | Loss: 0.00105020
Iteration 20/25 | Loss: 0.00105020
Iteration 21/25 | Loss: 0.00105020
Iteration 22/25 | Loss: 0.00105020
Iteration 23/25 | Loss: 0.00105020
Iteration 24/25 | Loss: 0.00105020
Iteration 25/25 | Loss: 0.00105020

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31124902
Iteration 2/25 | Loss: 0.00107661
Iteration 3/25 | Loss: 0.00107660
Iteration 4/25 | Loss: 0.00107660
Iteration 5/25 | Loss: 0.00107660
Iteration 6/25 | Loss: 0.00107660
Iteration 7/25 | Loss: 0.00107660
Iteration 8/25 | Loss: 0.00107660
Iteration 9/25 | Loss: 0.00107660
Iteration 10/25 | Loss: 0.00107660
Iteration 11/25 | Loss: 0.00107660
Iteration 12/25 | Loss: 0.00107660
Iteration 13/25 | Loss: 0.00107660
Iteration 14/25 | Loss: 0.00107660
Iteration 15/25 | Loss: 0.00107660
Iteration 16/25 | Loss: 0.00107660
Iteration 17/25 | Loss: 0.00107660
Iteration 18/25 | Loss: 0.00107660
Iteration 19/25 | Loss: 0.00107660
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010765991173684597, 0.0010765991173684597, 0.0010765991173684597, 0.0010765991173684597, 0.0010765991173684597]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010765991173684597

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107660
Iteration 2/1000 | Loss: 0.00003723
Iteration 3/1000 | Loss: 0.00002801
Iteration 4/1000 | Loss: 0.00002245
Iteration 5/1000 | Loss: 0.00002066
Iteration 6/1000 | Loss: 0.00001944
Iteration 7/1000 | Loss: 0.00001854
Iteration 8/1000 | Loss: 0.00001775
Iteration 9/1000 | Loss: 0.00001725
Iteration 10/1000 | Loss: 0.00001677
Iteration 11/1000 | Loss: 0.00001649
Iteration 12/1000 | Loss: 0.00001628
Iteration 13/1000 | Loss: 0.00001625
Iteration 14/1000 | Loss: 0.00001623
Iteration 15/1000 | Loss: 0.00001622
Iteration 16/1000 | Loss: 0.00001619
Iteration 17/1000 | Loss: 0.00001618
Iteration 18/1000 | Loss: 0.00001617
Iteration 19/1000 | Loss: 0.00001602
Iteration 20/1000 | Loss: 0.00001599
Iteration 21/1000 | Loss: 0.00001589
Iteration 22/1000 | Loss: 0.00001587
Iteration 23/1000 | Loss: 0.00001586
Iteration 24/1000 | Loss: 0.00001582
Iteration 25/1000 | Loss: 0.00001581
Iteration 26/1000 | Loss: 0.00001576
Iteration 27/1000 | Loss: 0.00001573
Iteration 28/1000 | Loss: 0.00001568
Iteration 29/1000 | Loss: 0.00001568
Iteration 30/1000 | Loss: 0.00001565
Iteration 31/1000 | Loss: 0.00001565
Iteration 32/1000 | Loss: 0.00001565
Iteration 33/1000 | Loss: 0.00001565
Iteration 34/1000 | Loss: 0.00001564
Iteration 35/1000 | Loss: 0.00001563
Iteration 36/1000 | Loss: 0.00001562
Iteration 37/1000 | Loss: 0.00001562
Iteration 38/1000 | Loss: 0.00001561
Iteration 39/1000 | Loss: 0.00001561
Iteration 40/1000 | Loss: 0.00001561
Iteration 41/1000 | Loss: 0.00001561
Iteration 42/1000 | Loss: 0.00001560
Iteration 43/1000 | Loss: 0.00001560
Iteration 44/1000 | Loss: 0.00001560
Iteration 45/1000 | Loss: 0.00001559
Iteration 46/1000 | Loss: 0.00001559
Iteration 47/1000 | Loss: 0.00001559
Iteration 48/1000 | Loss: 0.00001558
Iteration 49/1000 | Loss: 0.00001558
Iteration 50/1000 | Loss: 0.00001558
Iteration 51/1000 | Loss: 0.00001558
Iteration 52/1000 | Loss: 0.00001557
Iteration 53/1000 | Loss: 0.00001556
Iteration 54/1000 | Loss: 0.00001556
Iteration 55/1000 | Loss: 0.00001556
Iteration 56/1000 | Loss: 0.00001556
Iteration 57/1000 | Loss: 0.00001556
Iteration 58/1000 | Loss: 0.00001556
Iteration 59/1000 | Loss: 0.00001555
Iteration 60/1000 | Loss: 0.00001555
Iteration 61/1000 | Loss: 0.00001554
Iteration 62/1000 | Loss: 0.00001554
Iteration 63/1000 | Loss: 0.00001553
Iteration 64/1000 | Loss: 0.00001553
Iteration 65/1000 | Loss: 0.00001553
Iteration 66/1000 | Loss: 0.00001552
Iteration 67/1000 | Loss: 0.00001552
Iteration 68/1000 | Loss: 0.00001552
Iteration 69/1000 | Loss: 0.00001551
Iteration 70/1000 | Loss: 0.00001551
Iteration 71/1000 | Loss: 0.00001551
Iteration 72/1000 | Loss: 0.00001551
Iteration 73/1000 | Loss: 0.00001551
Iteration 74/1000 | Loss: 0.00001551
Iteration 75/1000 | Loss: 0.00001550
Iteration 76/1000 | Loss: 0.00001550
Iteration 77/1000 | Loss: 0.00001550
Iteration 78/1000 | Loss: 0.00001550
Iteration 79/1000 | Loss: 0.00001550
Iteration 80/1000 | Loss: 0.00001549
Iteration 81/1000 | Loss: 0.00001549
Iteration 82/1000 | Loss: 0.00001549
Iteration 83/1000 | Loss: 0.00001549
Iteration 84/1000 | Loss: 0.00001548
Iteration 85/1000 | Loss: 0.00001548
Iteration 86/1000 | Loss: 0.00001548
Iteration 87/1000 | Loss: 0.00001548
Iteration 88/1000 | Loss: 0.00001548
Iteration 89/1000 | Loss: 0.00001548
Iteration 90/1000 | Loss: 0.00001548
Iteration 91/1000 | Loss: 0.00001548
Iteration 92/1000 | Loss: 0.00001548
Iteration 93/1000 | Loss: 0.00001548
Iteration 94/1000 | Loss: 0.00001548
Iteration 95/1000 | Loss: 0.00001547
Iteration 96/1000 | Loss: 0.00001547
Iteration 97/1000 | Loss: 0.00001547
Iteration 98/1000 | Loss: 0.00001547
Iteration 99/1000 | Loss: 0.00001546
Iteration 100/1000 | Loss: 0.00001546
Iteration 101/1000 | Loss: 0.00001546
Iteration 102/1000 | Loss: 0.00001546
Iteration 103/1000 | Loss: 0.00001545
Iteration 104/1000 | Loss: 0.00001545
Iteration 105/1000 | Loss: 0.00001545
Iteration 106/1000 | Loss: 0.00001544
Iteration 107/1000 | Loss: 0.00001544
Iteration 108/1000 | Loss: 0.00001544
Iteration 109/1000 | Loss: 0.00001544
Iteration 110/1000 | Loss: 0.00001544
Iteration 111/1000 | Loss: 0.00001544
Iteration 112/1000 | Loss: 0.00001544
Iteration 113/1000 | Loss: 0.00001544
Iteration 114/1000 | Loss: 0.00001544
Iteration 115/1000 | Loss: 0.00001544
Iteration 116/1000 | Loss: 0.00001544
Iteration 117/1000 | Loss: 0.00001544
Iteration 118/1000 | Loss: 0.00001544
Iteration 119/1000 | Loss: 0.00001543
Iteration 120/1000 | Loss: 0.00001543
Iteration 121/1000 | Loss: 0.00001543
Iteration 122/1000 | Loss: 0.00001543
Iteration 123/1000 | Loss: 0.00001543
Iteration 124/1000 | Loss: 0.00001543
Iteration 125/1000 | Loss: 0.00001543
Iteration 126/1000 | Loss: 0.00001543
Iteration 127/1000 | Loss: 0.00001543
Iteration 128/1000 | Loss: 0.00001543
Iteration 129/1000 | Loss: 0.00001543
Iteration 130/1000 | Loss: 0.00001543
Iteration 131/1000 | Loss: 0.00001542
Iteration 132/1000 | Loss: 0.00001542
Iteration 133/1000 | Loss: 0.00001542
Iteration 134/1000 | Loss: 0.00001542
Iteration 135/1000 | Loss: 0.00001542
Iteration 136/1000 | Loss: 0.00001542
Iteration 137/1000 | Loss: 0.00001542
Iteration 138/1000 | Loss: 0.00001542
Iteration 139/1000 | Loss: 0.00001542
Iteration 140/1000 | Loss: 0.00001542
Iteration 141/1000 | Loss: 0.00001542
Iteration 142/1000 | Loss: 0.00001542
Iteration 143/1000 | Loss: 0.00001542
Iteration 144/1000 | Loss: 0.00001542
Iteration 145/1000 | Loss: 0.00001542
Iteration 146/1000 | Loss: 0.00001542
Iteration 147/1000 | Loss: 0.00001542
Iteration 148/1000 | Loss: 0.00001542
Iteration 149/1000 | Loss: 0.00001541
Iteration 150/1000 | Loss: 0.00001541
Iteration 151/1000 | Loss: 0.00001541
Iteration 152/1000 | Loss: 0.00001541
Iteration 153/1000 | Loss: 0.00001541
Iteration 154/1000 | Loss: 0.00001541
Iteration 155/1000 | Loss: 0.00001541
Iteration 156/1000 | Loss: 0.00001541
Iteration 157/1000 | Loss: 0.00001541
Iteration 158/1000 | Loss: 0.00001541
Iteration 159/1000 | Loss: 0.00001541
Iteration 160/1000 | Loss: 0.00001541
Iteration 161/1000 | Loss: 0.00001540
Iteration 162/1000 | Loss: 0.00001540
Iteration 163/1000 | Loss: 0.00001540
Iteration 164/1000 | Loss: 0.00001540
Iteration 165/1000 | Loss: 0.00001540
Iteration 166/1000 | Loss: 0.00001540
Iteration 167/1000 | Loss: 0.00001540
Iteration 168/1000 | Loss: 0.00001540
Iteration 169/1000 | Loss: 0.00001540
Iteration 170/1000 | Loss: 0.00001540
Iteration 171/1000 | Loss: 0.00001540
Iteration 172/1000 | Loss: 0.00001539
Iteration 173/1000 | Loss: 0.00001539
Iteration 174/1000 | Loss: 0.00001539
Iteration 175/1000 | Loss: 0.00001539
Iteration 176/1000 | Loss: 0.00001539
Iteration 177/1000 | Loss: 0.00001539
Iteration 178/1000 | Loss: 0.00001539
Iteration 179/1000 | Loss: 0.00001539
Iteration 180/1000 | Loss: 0.00001539
Iteration 181/1000 | Loss: 0.00001539
Iteration 182/1000 | Loss: 0.00001539
Iteration 183/1000 | Loss: 0.00001538
Iteration 184/1000 | Loss: 0.00001538
Iteration 185/1000 | Loss: 0.00001538
Iteration 186/1000 | Loss: 0.00001538
Iteration 187/1000 | Loss: 0.00001538
Iteration 188/1000 | Loss: 0.00001538
Iteration 189/1000 | Loss: 0.00001538
Iteration 190/1000 | Loss: 0.00001538
Iteration 191/1000 | Loss: 0.00001538
Iteration 192/1000 | Loss: 0.00001537
Iteration 193/1000 | Loss: 0.00001537
Iteration 194/1000 | Loss: 0.00001537
Iteration 195/1000 | Loss: 0.00001537
Iteration 196/1000 | Loss: 0.00001537
Iteration 197/1000 | Loss: 0.00001537
Iteration 198/1000 | Loss: 0.00001537
Iteration 199/1000 | Loss: 0.00001537
Iteration 200/1000 | Loss: 0.00001537
Iteration 201/1000 | Loss: 0.00001537
Iteration 202/1000 | Loss: 0.00001536
Iteration 203/1000 | Loss: 0.00001536
Iteration 204/1000 | Loss: 0.00001536
Iteration 205/1000 | Loss: 0.00001536
Iteration 206/1000 | Loss: 0.00001536
Iteration 207/1000 | Loss: 0.00001536
Iteration 208/1000 | Loss: 0.00001536
Iteration 209/1000 | Loss: 0.00001535
Iteration 210/1000 | Loss: 0.00001535
Iteration 211/1000 | Loss: 0.00001535
Iteration 212/1000 | Loss: 0.00001535
Iteration 213/1000 | Loss: 0.00001535
Iteration 214/1000 | Loss: 0.00001535
Iteration 215/1000 | Loss: 0.00001535
Iteration 216/1000 | Loss: 0.00001535
Iteration 217/1000 | Loss: 0.00001535
Iteration 218/1000 | Loss: 0.00001535
Iteration 219/1000 | Loss: 0.00001534
Iteration 220/1000 | Loss: 0.00001534
Iteration 221/1000 | Loss: 0.00001534
Iteration 222/1000 | Loss: 0.00001534
Iteration 223/1000 | Loss: 0.00001534
Iteration 224/1000 | Loss: 0.00001534
Iteration 225/1000 | Loss: 0.00001534
Iteration 226/1000 | Loss: 0.00001534
Iteration 227/1000 | Loss: 0.00001534
Iteration 228/1000 | Loss: 0.00001534
Iteration 229/1000 | Loss: 0.00001534
Iteration 230/1000 | Loss: 0.00001534
Iteration 231/1000 | Loss: 0.00001534
Iteration 232/1000 | Loss: 0.00001534
Iteration 233/1000 | Loss: 0.00001534
Iteration 234/1000 | Loss: 0.00001533
Iteration 235/1000 | Loss: 0.00001533
Iteration 236/1000 | Loss: 0.00001533
Iteration 237/1000 | Loss: 0.00001533
Iteration 238/1000 | Loss: 0.00001533
Iteration 239/1000 | Loss: 0.00001533
Iteration 240/1000 | Loss: 0.00001533
Iteration 241/1000 | Loss: 0.00001533
Iteration 242/1000 | Loss: 0.00001533
Iteration 243/1000 | Loss: 0.00001533
Iteration 244/1000 | Loss: 0.00001533
Iteration 245/1000 | Loss: 0.00001532
Iteration 246/1000 | Loss: 0.00001532
Iteration 247/1000 | Loss: 0.00001532
Iteration 248/1000 | Loss: 0.00001532
Iteration 249/1000 | Loss: 0.00001532
Iteration 250/1000 | Loss: 0.00001532
Iteration 251/1000 | Loss: 0.00001532
Iteration 252/1000 | Loss: 0.00001532
Iteration 253/1000 | Loss: 0.00001532
Iteration 254/1000 | Loss: 0.00001532
Iteration 255/1000 | Loss: 0.00001532
Iteration 256/1000 | Loss: 0.00001532
Iteration 257/1000 | Loss: 0.00001532
Iteration 258/1000 | Loss: 0.00001531
Iteration 259/1000 | Loss: 0.00001531
Iteration 260/1000 | Loss: 0.00001531
Iteration 261/1000 | Loss: 0.00001531
Iteration 262/1000 | Loss: 0.00001531
Iteration 263/1000 | Loss: 0.00001531
Iteration 264/1000 | Loss: 0.00001531
Iteration 265/1000 | Loss: 0.00001531
Iteration 266/1000 | Loss: 0.00001531
Iteration 267/1000 | Loss: 0.00001531
Iteration 268/1000 | Loss: 0.00001531
Iteration 269/1000 | Loss: 0.00001531
Iteration 270/1000 | Loss: 0.00001531
Iteration 271/1000 | Loss: 0.00001531
Iteration 272/1000 | Loss: 0.00001530
Iteration 273/1000 | Loss: 0.00001530
Iteration 274/1000 | Loss: 0.00001530
Iteration 275/1000 | Loss: 0.00001530
Iteration 276/1000 | Loss: 0.00001530
Iteration 277/1000 | Loss: 0.00001530
Iteration 278/1000 | Loss: 0.00001530
Iteration 279/1000 | Loss: 0.00001530
Iteration 280/1000 | Loss: 0.00001530
Iteration 281/1000 | Loss: 0.00001530
Iteration 282/1000 | Loss: 0.00001530
Iteration 283/1000 | Loss: 0.00001530
Iteration 284/1000 | Loss: 0.00001530
Iteration 285/1000 | Loss: 0.00001530
Iteration 286/1000 | Loss: 0.00001530
Iteration 287/1000 | Loss: 0.00001530
Iteration 288/1000 | Loss: 0.00001530
Iteration 289/1000 | Loss: 0.00001530
Iteration 290/1000 | Loss: 0.00001530
Iteration 291/1000 | Loss: 0.00001530
Iteration 292/1000 | Loss: 0.00001530
Iteration 293/1000 | Loss: 0.00001530
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 293. Stopping optimization.
Last 5 losses: [1.529520704934839e-05, 1.529520704934839e-05, 1.529520704934839e-05, 1.529520704934839e-05, 1.529520704934839e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.529520704934839e-05

Optimization complete. Final v2v error: 3.279844045639038 mm

Highest mean error: 3.8350977897644043 mm for frame 122

Lowest mean error: 2.607236385345459 mm for frame 69

Saving results

Total time: 49.40762734413147
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00579781
Iteration 2/25 | Loss: 0.00152331
Iteration 3/25 | Loss: 0.00118252
Iteration 4/25 | Loss: 0.00114176
Iteration 5/25 | Loss: 0.00113923
Iteration 6/25 | Loss: 0.00113884
Iteration 7/25 | Loss: 0.00113884
Iteration 8/25 | Loss: 0.00113884
Iteration 9/25 | Loss: 0.00113884
Iteration 10/25 | Loss: 0.00113884
Iteration 11/25 | Loss: 0.00113884
Iteration 12/25 | Loss: 0.00113884
Iteration 13/25 | Loss: 0.00113884
Iteration 14/25 | Loss: 0.00113884
Iteration 15/25 | Loss: 0.00113884
Iteration 16/25 | Loss: 0.00113884
Iteration 17/25 | Loss: 0.00113884
Iteration 18/25 | Loss: 0.00113884
Iteration 19/25 | Loss: 0.00113884
Iteration 20/25 | Loss: 0.00113884
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001138842897489667, 0.001138842897489667, 0.001138842897489667, 0.001138842897489667, 0.001138842897489667]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001138842897489667

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30876279
Iteration 2/25 | Loss: 0.00063678
Iteration 3/25 | Loss: 0.00063674
Iteration 4/25 | Loss: 0.00063674
Iteration 5/25 | Loss: 0.00063674
Iteration 6/25 | Loss: 0.00063674
Iteration 7/25 | Loss: 0.00063674
Iteration 8/25 | Loss: 0.00063674
Iteration 9/25 | Loss: 0.00063674
Iteration 10/25 | Loss: 0.00063674
Iteration 11/25 | Loss: 0.00063674
Iteration 12/25 | Loss: 0.00063674
Iteration 13/25 | Loss: 0.00063674
Iteration 14/25 | Loss: 0.00063674
Iteration 15/25 | Loss: 0.00063674
Iteration 16/25 | Loss: 0.00063674
Iteration 17/25 | Loss: 0.00063674
Iteration 18/25 | Loss: 0.00063674
Iteration 19/25 | Loss: 0.00063674
Iteration 20/25 | Loss: 0.00063674
Iteration 21/25 | Loss: 0.00063674
Iteration 22/25 | Loss: 0.00063674
Iteration 23/25 | Loss: 0.00063674
Iteration 24/25 | Loss: 0.00063674
Iteration 25/25 | Loss: 0.00063674

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063674
Iteration 2/1000 | Loss: 0.00004250
Iteration 3/1000 | Loss: 0.00002896
Iteration 4/1000 | Loss: 0.00002185
Iteration 5/1000 | Loss: 0.00001941
Iteration 6/1000 | Loss: 0.00001824
Iteration 7/1000 | Loss: 0.00001740
Iteration 8/1000 | Loss: 0.00001676
Iteration 9/1000 | Loss: 0.00001637
Iteration 10/1000 | Loss: 0.00001612
Iteration 11/1000 | Loss: 0.00001589
Iteration 12/1000 | Loss: 0.00001574
Iteration 13/1000 | Loss: 0.00001553
Iteration 14/1000 | Loss: 0.00001543
Iteration 15/1000 | Loss: 0.00001527
Iteration 16/1000 | Loss: 0.00001511
Iteration 17/1000 | Loss: 0.00001508
Iteration 18/1000 | Loss: 0.00001508
Iteration 19/1000 | Loss: 0.00001504
Iteration 20/1000 | Loss: 0.00001503
Iteration 21/1000 | Loss: 0.00001497
Iteration 22/1000 | Loss: 0.00001495
Iteration 23/1000 | Loss: 0.00001492
Iteration 24/1000 | Loss: 0.00001490
Iteration 25/1000 | Loss: 0.00001490
Iteration 26/1000 | Loss: 0.00001489
Iteration 27/1000 | Loss: 0.00001486
Iteration 28/1000 | Loss: 0.00001486
Iteration 29/1000 | Loss: 0.00001484
Iteration 30/1000 | Loss: 0.00001484
Iteration 31/1000 | Loss: 0.00001482
Iteration 32/1000 | Loss: 0.00001481
Iteration 33/1000 | Loss: 0.00001480
Iteration 34/1000 | Loss: 0.00001480
Iteration 35/1000 | Loss: 0.00001479
Iteration 36/1000 | Loss: 0.00001478
Iteration 37/1000 | Loss: 0.00001475
Iteration 38/1000 | Loss: 0.00001475
Iteration 39/1000 | Loss: 0.00001474
Iteration 40/1000 | Loss: 0.00001474
Iteration 41/1000 | Loss: 0.00001474
Iteration 42/1000 | Loss: 0.00001473
Iteration 43/1000 | Loss: 0.00001472
Iteration 44/1000 | Loss: 0.00001471
Iteration 45/1000 | Loss: 0.00001471
Iteration 46/1000 | Loss: 0.00001470
Iteration 47/1000 | Loss: 0.00001470
Iteration 48/1000 | Loss: 0.00001470
Iteration 49/1000 | Loss: 0.00001469
Iteration 50/1000 | Loss: 0.00001469
Iteration 51/1000 | Loss: 0.00001468
Iteration 52/1000 | Loss: 0.00001468
Iteration 53/1000 | Loss: 0.00001467
Iteration 54/1000 | Loss: 0.00001467
Iteration 55/1000 | Loss: 0.00001467
Iteration 56/1000 | Loss: 0.00001466
Iteration 57/1000 | Loss: 0.00001466
Iteration 58/1000 | Loss: 0.00001465
Iteration 59/1000 | Loss: 0.00001465
Iteration 60/1000 | Loss: 0.00001465
Iteration 61/1000 | Loss: 0.00001464
Iteration 62/1000 | Loss: 0.00001464
Iteration 63/1000 | Loss: 0.00001464
Iteration 64/1000 | Loss: 0.00001463
Iteration 65/1000 | Loss: 0.00001463
Iteration 66/1000 | Loss: 0.00001463
Iteration 67/1000 | Loss: 0.00001462
Iteration 68/1000 | Loss: 0.00001462
Iteration 69/1000 | Loss: 0.00001462
Iteration 70/1000 | Loss: 0.00001461
Iteration 71/1000 | Loss: 0.00001461
Iteration 72/1000 | Loss: 0.00001461
Iteration 73/1000 | Loss: 0.00001461
Iteration 74/1000 | Loss: 0.00001461
Iteration 75/1000 | Loss: 0.00001461
Iteration 76/1000 | Loss: 0.00001461
Iteration 77/1000 | Loss: 0.00001461
Iteration 78/1000 | Loss: 0.00001460
Iteration 79/1000 | Loss: 0.00001460
Iteration 80/1000 | Loss: 0.00001460
Iteration 81/1000 | Loss: 0.00001460
Iteration 82/1000 | Loss: 0.00001460
Iteration 83/1000 | Loss: 0.00001460
Iteration 84/1000 | Loss: 0.00001459
Iteration 85/1000 | Loss: 0.00001459
Iteration 86/1000 | Loss: 0.00001459
Iteration 87/1000 | Loss: 0.00001459
Iteration 88/1000 | Loss: 0.00001459
Iteration 89/1000 | Loss: 0.00001459
Iteration 90/1000 | Loss: 0.00001459
Iteration 91/1000 | Loss: 0.00001459
Iteration 92/1000 | Loss: 0.00001459
Iteration 93/1000 | Loss: 0.00001459
Iteration 94/1000 | Loss: 0.00001458
Iteration 95/1000 | Loss: 0.00001458
Iteration 96/1000 | Loss: 0.00001458
Iteration 97/1000 | Loss: 0.00001458
Iteration 98/1000 | Loss: 0.00001458
Iteration 99/1000 | Loss: 0.00001458
Iteration 100/1000 | Loss: 0.00001458
Iteration 101/1000 | Loss: 0.00001458
Iteration 102/1000 | Loss: 0.00001458
Iteration 103/1000 | Loss: 0.00001458
Iteration 104/1000 | Loss: 0.00001458
Iteration 105/1000 | Loss: 0.00001457
Iteration 106/1000 | Loss: 0.00001457
Iteration 107/1000 | Loss: 0.00001457
Iteration 108/1000 | Loss: 0.00001457
Iteration 109/1000 | Loss: 0.00001457
Iteration 110/1000 | Loss: 0.00001457
Iteration 111/1000 | Loss: 0.00001457
Iteration 112/1000 | Loss: 0.00001456
Iteration 113/1000 | Loss: 0.00001456
Iteration 114/1000 | Loss: 0.00001456
Iteration 115/1000 | Loss: 0.00001456
Iteration 116/1000 | Loss: 0.00001456
Iteration 117/1000 | Loss: 0.00001456
Iteration 118/1000 | Loss: 0.00001456
Iteration 119/1000 | Loss: 0.00001456
Iteration 120/1000 | Loss: 0.00001456
Iteration 121/1000 | Loss: 0.00001456
Iteration 122/1000 | Loss: 0.00001456
Iteration 123/1000 | Loss: 0.00001456
Iteration 124/1000 | Loss: 0.00001455
Iteration 125/1000 | Loss: 0.00001455
Iteration 126/1000 | Loss: 0.00001455
Iteration 127/1000 | Loss: 0.00001455
Iteration 128/1000 | Loss: 0.00001455
Iteration 129/1000 | Loss: 0.00001455
Iteration 130/1000 | Loss: 0.00001455
Iteration 131/1000 | Loss: 0.00001455
Iteration 132/1000 | Loss: 0.00001455
Iteration 133/1000 | Loss: 0.00001455
Iteration 134/1000 | Loss: 0.00001455
Iteration 135/1000 | Loss: 0.00001454
Iteration 136/1000 | Loss: 0.00001454
Iteration 137/1000 | Loss: 0.00001454
Iteration 138/1000 | Loss: 0.00001454
Iteration 139/1000 | Loss: 0.00001454
Iteration 140/1000 | Loss: 0.00001454
Iteration 141/1000 | Loss: 0.00001454
Iteration 142/1000 | Loss: 0.00001454
Iteration 143/1000 | Loss: 0.00001454
Iteration 144/1000 | Loss: 0.00001454
Iteration 145/1000 | Loss: 0.00001454
Iteration 146/1000 | Loss: 0.00001454
Iteration 147/1000 | Loss: 0.00001454
Iteration 148/1000 | Loss: 0.00001454
Iteration 149/1000 | Loss: 0.00001454
Iteration 150/1000 | Loss: 0.00001454
Iteration 151/1000 | Loss: 0.00001453
Iteration 152/1000 | Loss: 0.00001453
Iteration 153/1000 | Loss: 0.00001453
Iteration 154/1000 | Loss: 0.00001453
Iteration 155/1000 | Loss: 0.00001453
Iteration 156/1000 | Loss: 0.00001453
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.4534914953401312e-05, 1.4534914953401312e-05, 1.4534914953401312e-05, 1.4534914953401312e-05, 1.4534914953401312e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4534914953401312e-05

Optimization complete. Final v2v error: 3.21633243560791 mm

Highest mean error: 3.5146565437316895 mm for frame 69

Lowest mean error: 2.8456192016601562 mm for frame 16

Saving results

Total time: 42.19609761238098
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00365462
Iteration 2/25 | Loss: 0.00125502
Iteration 3/25 | Loss: 0.00113786
Iteration 4/25 | Loss: 0.00111955
Iteration 5/25 | Loss: 0.00111211
Iteration 6/25 | Loss: 0.00111004
Iteration 7/25 | Loss: 0.00110999
Iteration 8/25 | Loss: 0.00110999
Iteration 9/25 | Loss: 0.00110999
Iteration 10/25 | Loss: 0.00110999
Iteration 11/25 | Loss: 0.00110999
Iteration 12/25 | Loss: 0.00110999
Iteration 13/25 | Loss: 0.00110999
Iteration 14/25 | Loss: 0.00110999
Iteration 15/25 | Loss: 0.00110999
Iteration 16/25 | Loss: 0.00110999
Iteration 17/25 | Loss: 0.00110999
Iteration 18/25 | Loss: 0.00110999
Iteration 19/25 | Loss: 0.00110999
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011099905241280794, 0.0011099905241280794, 0.0011099905241280794, 0.0011099905241280794, 0.0011099905241280794]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011099905241280794

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.84723860
Iteration 2/25 | Loss: 0.00106717
Iteration 3/25 | Loss: 0.00106717
Iteration 4/25 | Loss: 0.00106717
Iteration 5/25 | Loss: 0.00106717
Iteration 6/25 | Loss: 0.00106717
Iteration 7/25 | Loss: 0.00106717
Iteration 8/25 | Loss: 0.00106717
Iteration 9/25 | Loss: 0.00106717
Iteration 10/25 | Loss: 0.00106717
Iteration 11/25 | Loss: 0.00106717
Iteration 12/25 | Loss: 0.00106717
Iteration 13/25 | Loss: 0.00106717
Iteration 14/25 | Loss: 0.00106717
Iteration 15/25 | Loss: 0.00106717
Iteration 16/25 | Loss: 0.00106717
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001067167380824685, 0.001067167380824685, 0.001067167380824685, 0.001067167380824685, 0.001067167380824685]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001067167380824685

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106717
Iteration 2/1000 | Loss: 0.00003718
Iteration 3/1000 | Loss: 0.00002131
Iteration 4/1000 | Loss: 0.00001878
Iteration 5/1000 | Loss: 0.00001767
Iteration 6/1000 | Loss: 0.00001664
Iteration 7/1000 | Loss: 0.00001612
Iteration 8/1000 | Loss: 0.00001574
Iteration 9/1000 | Loss: 0.00001549
Iteration 10/1000 | Loss: 0.00001529
Iteration 11/1000 | Loss: 0.00001506
Iteration 12/1000 | Loss: 0.00001493
Iteration 13/1000 | Loss: 0.00001471
Iteration 14/1000 | Loss: 0.00001464
Iteration 15/1000 | Loss: 0.00001463
Iteration 16/1000 | Loss: 0.00001463
Iteration 17/1000 | Loss: 0.00001460
Iteration 18/1000 | Loss: 0.00001460
Iteration 19/1000 | Loss: 0.00001459
Iteration 20/1000 | Loss: 0.00001458
Iteration 21/1000 | Loss: 0.00001458
Iteration 22/1000 | Loss: 0.00001457
Iteration 23/1000 | Loss: 0.00001457
Iteration 24/1000 | Loss: 0.00001456
Iteration 25/1000 | Loss: 0.00001456
Iteration 26/1000 | Loss: 0.00001451
Iteration 27/1000 | Loss: 0.00001447
Iteration 28/1000 | Loss: 0.00001446
Iteration 29/1000 | Loss: 0.00001446
Iteration 30/1000 | Loss: 0.00001446
Iteration 31/1000 | Loss: 0.00001446
Iteration 32/1000 | Loss: 0.00001445
Iteration 33/1000 | Loss: 0.00001445
Iteration 34/1000 | Loss: 0.00001444
Iteration 35/1000 | Loss: 0.00001443
Iteration 36/1000 | Loss: 0.00001441
Iteration 37/1000 | Loss: 0.00001441
Iteration 38/1000 | Loss: 0.00001441
Iteration 39/1000 | Loss: 0.00001441
Iteration 40/1000 | Loss: 0.00001440
Iteration 41/1000 | Loss: 0.00001440
Iteration 42/1000 | Loss: 0.00001440
Iteration 43/1000 | Loss: 0.00001439
Iteration 44/1000 | Loss: 0.00001438
Iteration 45/1000 | Loss: 0.00001437
Iteration 46/1000 | Loss: 0.00001437
Iteration 47/1000 | Loss: 0.00001436
Iteration 48/1000 | Loss: 0.00001435
Iteration 49/1000 | Loss: 0.00001434
Iteration 50/1000 | Loss: 0.00001434
Iteration 51/1000 | Loss: 0.00001433
Iteration 52/1000 | Loss: 0.00001432
Iteration 53/1000 | Loss: 0.00001428
Iteration 54/1000 | Loss: 0.00001427
Iteration 55/1000 | Loss: 0.00001424
Iteration 56/1000 | Loss: 0.00001424
Iteration 57/1000 | Loss: 0.00001421
Iteration 58/1000 | Loss: 0.00001419
Iteration 59/1000 | Loss: 0.00001417
Iteration 60/1000 | Loss: 0.00001417
Iteration 61/1000 | Loss: 0.00001416
Iteration 62/1000 | Loss: 0.00001414
Iteration 63/1000 | Loss: 0.00001413
Iteration 64/1000 | Loss: 0.00001413
Iteration 65/1000 | Loss: 0.00001413
Iteration 66/1000 | Loss: 0.00001412
Iteration 67/1000 | Loss: 0.00001412
Iteration 68/1000 | Loss: 0.00001411
Iteration 69/1000 | Loss: 0.00001410
Iteration 70/1000 | Loss: 0.00001410
Iteration 71/1000 | Loss: 0.00001409
Iteration 72/1000 | Loss: 0.00001409
Iteration 73/1000 | Loss: 0.00001408
Iteration 74/1000 | Loss: 0.00001408
Iteration 75/1000 | Loss: 0.00001407
Iteration 76/1000 | Loss: 0.00001407
Iteration 77/1000 | Loss: 0.00001407
Iteration 78/1000 | Loss: 0.00001406
Iteration 79/1000 | Loss: 0.00001406
Iteration 80/1000 | Loss: 0.00001405
Iteration 81/1000 | Loss: 0.00001405
Iteration 82/1000 | Loss: 0.00001405
Iteration 83/1000 | Loss: 0.00001403
Iteration 84/1000 | Loss: 0.00001403
Iteration 85/1000 | Loss: 0.00001403
Iteration 86/1000 | Loss: 0.00001402
Iteration 87/1000 | Loss: 0.00001402
Iteration 88/1000 | Loss: 0.00001402
Iteration 89/1000 | Loss: 0.00001399
Iteration 90/1000 | Loss: 0.00001399
Iteration 91/1000 | Loss: 0.00001398
Iteration 92/1000 | Loss: 0.00001398
Iteration 93/1000 | Loss: 0.00001397
Iteration 94/1000 | Loss: 0.00001397
Iteration 95/1000 | Loss: 0.00001396
Iteration 96/1000 | Loss: 0.00001396
Iteration 97/1000 | Loss: 0.00001395
Iteration 98/1000 | Loss: 0.00001395
Iteration 99/1000 | Loss: 0.00001395
Iteration 100/1000 | Loss: 0.00001395
Iteration 101/1000 | Loss: 0.00001395
Iteration 102/1000 | Loss: 0.00001395
Iteration 103/1000 | Loss: 0.00001395
Iteration 104/1000 | Loss: 0.00001394
Iteration 105/1000 | Loss: 0.00001394
Iteration 106/1000 | Loss: 0.00001394
Iteration 107/1000 | Loss: 0.00001393
Iteration 108/1000 | Loss: 0.00001392
Iteration 109/1000 | Loss: 0.00001392
Iteration 110/1000 | Loss: 0.00001392
Iteration 111/1000 | Loss: 0.00001392
Iteration 112/1000 | Loss: 0.00001392
Iteration 113/1000 | Loss: 0.00001391
Iteration 114/1000 | Loss: 0.00001391
Iteration 115/1000 | Loss: 0.00001391
Iteration 116/1000 | Loss: 0.00001390
Iteration 117/1000 | Loss: 0.00001390
Iteration 118/1000 | Loss: 0.00001390
Iteration 119/1000 | Loss: 0.00001390
Iteration 120/1000 | Loss: 0.00001390
Iteration 121/1000 | Loss: 0.00001390
Iteration 122/1000 | Loss: 0.00001390
Iteration 123/1000 | Loss: 0.00001390
Iteration 124/1000 | Loss: 0.00001390
Iteration 125/1000 | Loss: 0.00001390
Iteration 126/1000 | Loss: 0.00001390
Iteration 127/1000 | Loss: 0.00001390
Iteration 128/1000 | Loss: 0.00001390
Iteration 129/1000 | Loss: 0.00001390
Iteration 130/1000 | Loss: 0.00001389
Iteration 131/1000 | Loss: 0.00001389
Iteration 132/1000 | Loss: 0.00001389
Iteration 133/1000 | Loss: 0.00001389
Iteration 134/1000 | Loss: 0.00001389
Iteration 135/1000 | Loss: 0.00001389
Iteration 136/1000 | Loss: 0.00001389
Iteration 137/1000 | Loss: 0.00001389
Iteration 138/1000 | Loss: 0.00001388
Iteration 139/1000 | Loss: 0.00001388
Iteration 140/1000 | Loss: 0.00001388
Iteration 141/1000 | Loss: 0.00001388
Iteration 142/1000 | Loss: 0.00001388
Iteration 143/1000 | Loss: 0.00001388
Iteration 144/1000 | Loss: 0.00001388
Iteration 145/1000 | Loss: 0.00001388
Iteration 146/1000 | Loss: 0.00001387
Iteration 147/1000 | Loss: 0.00001387
Iteration 148/1000 | Loss: 0.00001387
Iteration 149/1000 | Loss: 0.00001387
Iteration 150/1000 | Loss: 0.00001387
Iteration 151/1000 | Loss: 0.00001387
Iteration 152/1000 | Loss: 0.00001387
Iteration 153/1000 | Loss: 0.00001387
Iteration 154/1000 | Loss: 0.00001387
Iteration 155/1000 | Loss: 0.00001387
Iteration 156/1000 | Loss: 0.00001387
Iteration 157/1000 | Loss: 0.00001386
Iteration 158/1000 | Loss: 0.00001386
Iteration 159/1000 | Loss: 0.00001386
Iteration 160/1000 | Loss: 0.00001386
Iteration 161/1000 | Loss: 0.00001386
Iteration 162/1000 | Loss: 0.00001386
Iteration 163/1000 | Loss: 0.00001386
Iteration 164/1000 | Loss: 0.00001386
Iteration 165/1000 | Loss: 0.00001386
Iteration 166/1000 | Loss: 0.00001386
Iteration 167/1000 | Loss: 0.00001385
Iteration 168/1000 | Loss: 0.00001385
Iteration 169/1000 | Loss: 0.00001385
Iteration 170/1000 | Loss: 0.00001385
Iteration 171/1000 | Loss: 0.00001384
Iteration 172/1000 | Loss: 0.00001384
Iteration 173/1000 | Loss: 0.00001384
Iteration 174/1000 | Loss: 0.00001384
Iteration 175/1000 | Loss: 0.00001384
Iteration 176/1000 | Loss: 0.00001384
Iteration 177/1000 | Loss: 0.00001384
Iteration 178/1000 | Loss: 0.00001384
Iteration 179/1000 | Loss: 0.00001384
Iteration 180/1000 | Loss: 0.00001384
Iteration 181/1000 | Loss: 0.00001384
Iteration 182/1000 | Loss: 0.00001384
Iteration 183/1000 | Loss: 0.00001384
Iteration 184/1000 | Loss: 0.00001384
Iteration 185/1000 | Loss: 0.00001384
Iteration 186/1000 | Loss: 0.00001383
Iteration 187/1000 | Loss: 0.00001383
Iteration 188/1000 | Loss: 0.00001383
Iteration 189/1000 | Loss: 0.00001383
Iteration 190/1000 | Loss: 0.00001383
Iteration 191/1000 | Loss: 0.00001383
Iteration 192/1000 | Loss: 0.00001383
Iteration 193/1000 | Loss: 0.00001383
Iteration 194/1000 | Loss: 0.00001383
Iteration 195/1000 | Loss: 0.00001383
Iteration 196/1000 | Loss: 0.00001383
Iteration 197/1000 | Loss: 0.00001382
Iteration 198/1000 | Loss: 0.00001382
Iteration 199/1000 | Loss: 0.00001382
Iteration 200/1000 | Loss: 0.00001382
Iteration 201/1000 | Loss: 0.00001382
Iteration 202/1000 | Loss: 0.00001382
Iteration 203/1000 | Loss: 0.00001382
Iteration 204/1000 | Loss: 0.00001382
Iteration 205/1000 | Loss: 0.00001382
Iteration 206/1000 | Loss: 0.00001381
Iteration 207/1000 | Loss: 0.00001381
Iteration 208/1000 | Loss: 0.00001381
Iteration 209/1000 | Loss: 0.00001381
Iteration 210/1000 | Loss: 0.00001381
Iteration 211/1000 | Loss: 0.00001381
Iteration 212/1000 | Loss: 0.00001380
Iteration 213/1000 | Loss: 0.00001380
Iteration 214/1000 | Loss: 0.00001380
Iteration 215/1000 | Loss: 0.00001380
Iteration 216/1000 | Loss: 0.00001380
Iteration 217/1000 | Loss: 0.00001379
Iteration 218/1000 | Loss: 0.00001379
Iteration 219/1000 | Loss: 0.00001379
Iteration 220/1000 | Loss: 0.00001379
Iteration 221/1000 | Loss: 0.00001379
Iteration 222/1000 | Loss: 0.00001379
Iteration 223/1000 | Loss: 0.00001378
Iteration 224/1000 | Loss: 0.00001378
Iteration 225/1000 | Loss: 0.00001378
Iteration 226/1000 | Loss: 0.00001378
Iteration 227/1000 | Loss: 0.00001378
Iteration 228/1000 | Loss: 0.00001378
Iteration 229/1000 | Loss: 0.00001378
Iteration 230/1000 | Loss: 0.00001378
Iteration 231/1000 | Loss: 0.00001378
Iteration 232/1000 | Loss: 0.00001378
Iteration 233/1000 | Loss: 0.00001377
Iteration 234/1000 | Loss: 0.00001377
Iteration 235/1000 | Loss: 0.00001377
Iteration 236/1000 | Loss: 0.00001377
Iteration 237/1000 | Loss: 0.00001377
Iteration 238/1000 | Loss: 0.00001377
Iteration 239/1000 | Loss: 0.00001377
Iteration 240/1000 | Loss: 0.00001377
Iteration 241/1000 | Loss: 0.00001377
Iteration 242/1000 | Loss: 0.00001376
Iteration 243/1000 | Loss: 0.00001376
Iteration 244/1000 | Loss: 0.00001376
Iteration 245/1000 | Loss: 0.00001376
Iteration 246/1000 | Loss: 0.00001376
Iteration 247/1000 | Loss: 0.00001376
Iteration 248/1000 | Loss: 0.00001376
Iteration 249/1000 | Loss: 0.00001375
Iteration 250/1000 | Loss: 0.00001375
Iteration 251/1000 | Loss: 0.00001375
Iteration 252/1000 | Loss: 0.00001375
Iteration 253/1000 | Loss: 0.00001374
Iteration 254/1000 | Loss: 0.00001374
Iteration 255/1000 | Loss: 0.00001374
Iteration 256/1000 | Loss: 0.00001374
Iteration 257/1000 | Loss: 0.00001374
Iteration 258/1000 | Loss: 0.00001374
Iteration 259/1000 | Loss: 0.00001374
Iteration 260/1000 | Loss: 0.00001374
Iteration 261/1000 | Loss: 0.00001374
Iteration 262/1000 | Loss: 0.00001374
Iteration 263/1000 | Loss: 0.00001374
Iteration 264/1000 | Loss: 0.00001374
Iteration 265/1000 | Loss: 0.00001374
Iteration 266/1000 | Loss: 0.00001374
Iteration 267/1000 | Loss: 0.00001373
Iteration 268/1000 | Loss: 0.00001373
Iteration 269/1000 | Loss: 0.00001373
Iteration 270/1000 | Loss: 0.00001373
Iteration 271/1000 | Loss: 0.00001373
Iteration 272/1000 | Loss: 0.00001373
Iteration 273/1000 | Loss: 0.00001373
Iteration 274/1000 | Loss: 0.00001373
Iteration 275/1000 | Loss: 0.00001373
Iteration 276/1000 | Loss: 0.00001373
Iteration 277/1000 | Loss: 0.00001372
Iteration 278/1000 | Loss: 0.00001372
Iteration 279/1000 | Loss: 0.00001372
Iteration 280/1000 | Loss: 0.00001372
Iteration 281/1000 | Loss: 0.00001372
Iteration 282/1000 | Loss: 0.00001372
Iteration 283/1000 | Loss: 0.00001372
Iteration 284/1000 | Loss: 0.00001372
Iteration 285/1000 | Loss: 0.00001372
Iteration 286/1000 | Loss: 0.00001372
Iteration 287/1000 | Loss: 0.00001372
Iteration 288/1000 | Loss: 0.00001372
Iteration 289/1000 | Loss: 0.00001372
Iteration 290/1000 | Loss: 0.00001371
Iteration 291/1000 | Loss: 0.00001371
Iteration 292/1000 | Loss: 0.00001371
Iteration 293/1000 | Loss: 0.00001371
Iteration 294/1000 | Loss: 0.00001371
Iteration 295/1000 | Loss: 0.00001371
Iteration 296/1000 | Loss: 0.00001371
Iteration 297/1000 | Loss: 0.00001371
Iteration 298/1000 | Loss: 0.00001371
Iteration 299/1000 | Loss: 0.00001371
Iteration 300/1000 | Loss: 0.00001371
Iteration 301/1000 | Loss: 0.00001371
Iteration 302/1000 | Loss: 0.00001371
Iteration 303/1000 | Loss: 0.00001371
Iteration 304/1000 | Loss: 0.00001371
Iteration 305/1000 | Loss: 0.00001371
Iteration 306/1000 | Loss: 0.00001371
Iteration 307/1000 | Loss: 0.00001371
Iteration 308/1000 | Loss: 0.00001371
Iteration 309/1000 | Loss: 0.00001371
Iteration 310/1000 | Loss: 0.00001371
Iteration 311/1000 | Loss: 0.00001371
Iteration 312/1000 | Loss: 0.00001371
Iteration 313/1000 | Loss: 0.00001371
Iteration 314/1000 | Loss: 0.00001371
Iteration 315/1000 | Loss: 0.00001371
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 315. Stopping optimization.
Last 5 losses: [1.3709729500988033e-05, 1.3709729500988033e-05, 1.3709729500988033e-05, 1.3709729500988033e-05, 1.3709729500988033e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3709729500988033e-05

Optimization complete. Final v2v error: 3.1099467277526855 mm

Highest mean error: 3.215874195098877 mm for frame 248

Lowest mean error: 3.0416524410247803 mm for frame 189

Saving results

Total time: 59.75485301017761
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00808113
Iteration 2/25 | Loss: 0.00165650
Iteration 3/25 | Loss: 0.00127608
Iteration 4/25 | Loss: 0.00123053
Iteration 5/25 | Loss: 0.00122448
Iteration 6/25 | Loss: 0.00122294
Iteration 7/25 | Loss: 0.00122260
Iteration 8/25 | Loss: 0.00122260
Iteration 9/25 | Loss: 0.00122260
Iteration 10/25 | Loss: 0.00122260
Iteration 11/25 | Loss: 0.00122260
Iteration 12/25 | Loss: 0.00122260
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012225997634232044, 0.0012225997634232044, 0.0012225997634232044, 0.0012225997634232044, 0.0012225997634232044]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012225997634232044

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39901149
Iteration 2/25 | Loss: 0.00065828
Iteration 3/25 | Loss: 0.00065828
Iteration 4/25 | Loss: 0.00065827
Iteration 5/25 | Loss: 0.00065827
Iteration 6/25 | Loss: 0.00065827
Iteration 7/25 | Loss: 0.00065827
Iteration 8/25 | Loss: 0.00065827
Iteration 9/25 | Loss: 0.00065827
Iteration 10/25 | Loss: 0.00065827
Iteration 11/25 | Loss: 0.00065827
Iteration 12/25 | Loss: 0.00065827
Iteration 13/25 | Loss: 0.00065827
Iteration 14/25 | Loss: 0.00065827
Iteration 15/25 | Loss: 0.00065827
Iteration 16/25 | Loss: 0.00065827
Iteration 17/25 | Loss: 0.00065827
Iteration 18/25 | Loss: 0.00065827
Iteration 19/25 | Loss: 0.00065827
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0006582728819921613, 0.0006582728819921613, 0.0006582728819921613, 0.0006582728819921613, 0.0006582728819921613]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006582728819921613

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065827
Iteration 2/1000 | Loss: 0.00005772
Iteration 3/1000 | Loss: 0.00003851
Iteration 4/1000 | Loss: 0.00003130
Iteration 5/1000 | Loss: 0.00002956
Iteration 6/1000 | Loss: 0.00002850
Iteration 7/1000 | Loss: 0.00002763
Iteration 8/1000 | Loss: 0.00002699
Iteration 9/1000 | Loss: 0.00002650
Iteration 10/1000 | Loss: 0.00002620
Iteration 11/1000 | Loss: 0.00002597
Iteration 12/1000 | Loss: 0.00002579
Iteration 13/1000 | Loss: 0.00002563
Iteration 14/1000 | Loss: 0.00002559
Iteration 15/1000 | Loss: 0.00002558
Iteration 16/1000 | Loss: 0.00002558
Iteration 17/1000 | Loss: 0.00002555
Iteration 18/1000 | Loss: 0.00002554
Iteration 19/1000 | Loss: 0.00002554
Iteration 20/1000 | Loss: 0.00002549
Iteration 21/1000 | Loss: 0.00002548
Iteration 22/1000 | Loss: 0.00002547
Iteration 23/1000 | Loss: 0.00002539
Iteration 24/1000 | Loss: 0.00002537
Iteration 25/1000 | Loss: 0.00002537
Iteration 26/1000 | Loss: 0.00002537
Iteration 27/1000 | Loss: 0.00002536
Iteration 28/1000 | Loss: 0.00002534
Iteration 29/1000 | Loss: 0.00002532
Iteration 30/1000 | Loss: 0.00002532
Iteration 31/1000 | Loss: 0.00002532
Iteration 32/1000 | Loss: 0.00002532
Iteration 33/1000 | Loss: 0.00002531
Iteration 34/1000 | Loss: 0.00002531
Iteration 35/1000 | Loss: 0.00002531
Iteration 36/1000 | Loss: 0.00002531
Iteration 37/1000 | Loss: 0.00002531
Iteration 38/1000 | Loss: 0.00002530
Iteration 39/1000 | Loss: 0.00002530
Iteration 40/1000 | Loss: 0.00002529
Iteration 41/1000 | Loss: 0.00002528
Iteration 42/1000 | Loss: 0.00002528
Iteration 43/1000 | Loss: 0.00002527
Iteration 44/1000 | Loss: 0.00002527
Iteration 45/1000 | Loss: 0.00002526
Iteration 46/1000 | Loss: 0.00002526
Iteration 47/1000 | Loss: 0.00002526
Iteration 48/1000 | Loss: 0.00002526
Iteration 49/1000 | Loss: 0.00002526
Iteration 50/1000 | Loss: 0.00002526
Iteration 51/1000 | Loss: 0.00002526
Iteration 52/1000 | Loss: 0.00002526
Iteration 53/1000 | Loss: 0.00002526
Iteration 54/1000 | Loss: 0.00002526
Iteration 55/1000 | Loss: 0.00002525
Iteration 56/1000 | Loss: 0.00002525
Iteration 57/1000 | Loss: 0.00002525
Iteration 58/1000 | Loss: 0.00002525
Iteration 59/1000 | Loss: 0.00002524
Iteration 60/1000 | Loss: 0.00002523
Iteration 61/1000 | Loss: 0.00002523
Iteration 62/1000 | Loss: 0.00002523
Iteration 63/1000 | Loss: 0.00002523
Iteration 64/1000 | Loss: 0.00002523
Iteration 65/1000 | Loss: 0.00002523
Iteration 66/1000 | Loss: 0.00002523
Iteration 67/1000 | Loss: 0.00002523
Iteration 68/1000 | Loss: 0.00002522
Iteration 69/1000 | Loss: 0.00002522
Iteration 70/1000 | Loss: 0.00002522
Iteration 71/1000 | Loss: 0.00002522
Iteration 72/1000 | Loss: 0.00002521
Iteration 73/1000 | Loss: 0.00002521
Iteration 74/1000 | Loss: 0.00002521
Iteration 75/1000 | Loss: 0.00002521
Iteration 76/1000 | Loss: 0.00002521
Iteration 77/1000 | Loss: 0.00002521
Iteration 78/1000 | Loss: 0.00002521
Iteration 79/1000 | Loss: 0.00002520
Iteration 80/1000 | Loss: 0.00002520
Iteration 81/1000 | Loss: 0.00002520
Iteration 82/1000 | Loss: 0.00002520
Iteration 83/1000 | Loss: 0.00002520
Iteration 84/1000 | Loss: 0.00002520
Iteration 85/1000 | Loss: 0.00002520
Iteration 86/1000 | Loss: 0.00002520
Iteration 87/1000 | Loss: 0.00002520
Iteration 88/1000 | Loss: 0.00002520
Iteration 89/1000 | Loss: 0.00002520
Iteration 90/1000 | Loss: 0.00002519
Iteration 91/1000 | Loss: 0.00002519
Iteration 92/1000 | Loss: 0.00002518
Iteration 93/1000 | Loss: 0.00002518
Iteration 94/1000 | Loss: 0.00002518
Iteration 95/1000 | Loss: 0.00002518
Iteration 96/1000 | Loss: 0.00002517
Iteration 97/1000 | Loss: 0.00002517
Iteration 98/1000 | Loss: 0.00002517
Iteration 99/1000 | Loss: 0.00002517
Iteration 100/1000 | Loss: 0.00002516
Iteration 101/1000 | Loss: 0.00002516
Iteration 102/1000 | Loss: 0.00002516
Iteration 103/1000 | Loss: 0.00002516
Iteration 104/1000 | Loss: 0.00002515
Iteration 105/1000 | Loss: 0.00002515
Iteration 106/1000 | Loss: 0.00002515
Iteration 107/1000 | Loss: 0.00002515
Iteration 108/1000 | Loss: 0.00002515
Iteration 109/1000 | Loss: 0.00002515
Iteration 110/1000 | Loss: 0.00002515
Iteration 111/1000 | Loss: 0.00002515
Iteration 112/1000 | Loss: 0.00002515
Iteration 113/1000 | Loss: 0.00002515
Iteration 114/1000 | Loss: 0.00002515
Iteration 115/1000 | Loss: 0.00002515
Iteration 116/1000 | Loss: 0.00002514
Iteration 117/1000 | Loss: 0.00002514
Iteration 118/1000 | Loss: 0.00002514
Iteration 119/1000 | Loss: 0.00002514
Iteration 120/1000 | Loss: 0.00002514
Iteration 121/1000 | Loss: 0.00002514
Iteration 122/1000 | Loss: 0.00002514
Iteration 123/1000 | Loss: 0.00002514
Iteration 124/1000 | Loss: 0.00002514
Iteration 125/1000 | Loss: 0.00002514
Iteration 126/1000 | Loss: 0.00002514
Iteration 127/1000 | Loss: 0.00002513
Iteration 128/1000 | Loss: 0.00002513
Iteration 129/1000 | Loss: 0.00002513
Iteration 130/1000 | Loss: 0.00002513
Iteration 131/1000 | Loss: 0.00002513
Iteration 132/1000 | Loss: 0.00002512
Iteration 133/1000 | Loss: 0.00002512
Iteration 134/1000 | Loss: 0.00002512
Iteration 135/1000 | Loss: 0.00002512
Iteration 136/1000 | Loss: 0.00002512
Iteration 137/1000 | Loss: 0.00002512
Iteration 138/1000 | Loss: 0.00002512
Iteration 139/1000 | Loss: 0.00002512
Iteration 140/1000 | Loss: 0.00002512
Iteration 141/1000 | Loss: 0.00002512
Iteration 142/1000 | Loss: 0.00002512
Iteration 143/1000 | Loss: 0.00002512
Iteration 144/1000 | Loss: 0.00002512
Iteration 145/1000 | Loss: 0.00002512
Iteration 146/1000 | Loss: 0.00002512
Iteration 147/1000 | Loss: 0.00002512
Iteration 148/1000 | Loss: 0.00002511
Iteration 149/1000 | Loss: 0.00002511
Iteration 150/1000 | Loss: 0.00002511
Iteration 151/1000 | Loss: 0.00002511
Iteration 152/1000 | Loss: 0.00002510
Iteration 153/1000 | Loss: 0.00002510
Iteration 154/1000 | Loss: 0.00002510
Iteration 155/1000 | Loss: 0.00002510
Iteration 156/1000 | Loss: 0.00002510
Iteration 157/1000 | Loss: 0.00002510
Iteration 158/1000 | Loss: 0.00002510
Iteration 159/1000 | Loss: 0.00002510
Iteration 160/1000 | Loss: 0.00002510
Iteration 161/1000 | Loss: 0.00002509
Iteration 162/1000 | Loss: 0.00002509
Iteration 163/1000 | Loss: 0.00002509
Iteration 164/1000 | Loss: 0.00002508
Iteration 165/1000 | Loss: 0.00002508
Iteration 166/1000 | Loss: 0.00002508
Iteration 167/1000 | Loss: 0.00002508
Iteration 168/1000 | Loss: 0.00002508
Iteration 169/1000 | Loss: 0.00002507
Iteration 170/1000 | Loss: 0.00002507
Iteration 171/1000 | Loss: 0.00002507
Iteration 172/1000 | Loss: 0.00002507
Iteration 173/1000 | Loss: 0.00002507
Iteration 174/1000 | Loss: 0.00002507
Iteration 175/1000 | Loss: 0.00002507
Iteration 176/1000 | Loss: 0.00002507
Iteration 177/1000 | Loss: 0.00002507
Iteration 178/1000 | Loss: 0.00002507
Iteration 179/1000 | Loss: 0.00002507
Iteration 180/1000 | Loss: 0.00002507
Iteration 181/1000 | Loss: 0.00002507
Iteration 182/1000 | Loss: 0.00002507
Iteration 183/1000 | Loss: 0.00002507
Iteration 184/1000 | Loss: 0.00002507
Iteration 185/1000 | Loss: 0.00002507
Iteration 186/1000 | Loss: 0.00002506
Iteration 187/1000 | Loss: 0.00002506
Iteration 188/1000 | Loss: 0.00002506
Iteration 189/1000 | Loss: 0.00002506
Iteration 190/1000 | Loss: 0.00002506
Iteration 191/1000 | Loss: 0.00002506
Iteration 192/1000 | Loss: 0.00002506
Iteration 193/1000 | Loss: 0.00002506
Iteration 194/1000 | Loss: 0.00002506
Iteration 195/1000 | Loss: 0.00002506
Iteration 196/1000 | Loss: 0.00002506
Iteration 197/1000 | Loss: 0.00002506
Iteration 198/1000 | Loss: 0.00002506
Iteration 199/1000 | Loss: 0.00002506
Iteration 200/1000 | Loss: 0.00002506
Iteration 201/1000 | Loss: 0.00002506
Iteration 202/1000 | Loss: 0.00002505
Iteration 203/1000 | Loss: 0.00002505
Iteration 204/1000 | Loss: 0.00002505
Iteration 205/1000 | Loss: 0.00002505
Iteration 206/1000 | Loss: 0.00002505
Iteration 207/1000 | Loss: 0.00002505
Iteration 208/1000 | Loss: 0.00002505
Iteration 209/1000 | Loss: 0.00002505
Iteration 210/1000 | Loss: 0.00002505
Iteration 211/1000 | Loss: 0.00002505
Iteration 212/1000 | Loss: 0.00002505
Iteration 213/1000 | Loss: 0.00002505
Iteration 214/1000 | Loss: 0.00002505
Iteration 215/1000 | Loss: 0.00002505
Iteration 216/1000 | Loss: 0.00002505
Iteration 217/1000 | Loss: 0.00002505
Iteration 218/1000 | Loss: 0.00002505
Iteration 219/1000 | Loss: 0.00002505
Iteration 220/1000 | Loss: 0.00002504
Iteration 221/1000 | Loss: 0.00002504
Iteration 222/1000 | Loss: 0.00002504
Iteration 223/1000 | Loss: 0.00002504
Iteration 224/1000 | Loss: 0.00002504
Iteration 225/1000 | Loss: 0.00002504
Iteration 226/1000 | Loss: 0.00002504
Iteration 227/1000 | Loss: 0.00002504
Iteration 228/1000 | Loss: 0.00002504
Iteration 229/1000 | Loss: 0.00002504
Iteration 230/1000 | Loss: 0.00002504
Iteration 231/1000 | Loss: 0.00002504
Iteration 232/1000 | Loss: 0.00002504
Iteration 233/1000 | Loss: 0.00002504
Iteration 234/1000 | Loss: 0.00002504
Iteration 235/1000 | Loss: 0.00002504
Iteration 236/1000 | Loss: 0.00002504
Iteration 237/1000 | Loss: 0.00002504
Iteration 238/1000 | Loss: 0.00002504
Iteration 239/1000 | Loss: 0.00002504
Iteration 240/1000 | Loss: 0.00002504
Iteration 241/1000 | Loss: 0.00002504
Iteration 242/1000 | Loss: 0.00002504
Iteration 243/1000 | Loss: 0.00002504
Iteration 244/1000 | Loss: 0.00002504
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 244. Stopping optimization.
Last 5 losses: [2.5039353204192594e-05, 2.5039353204192594e-05, 2.5039353204192594e-05, 2.5039353204192594e-05, 2.5039353204192594e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5039353204192594e-05

Optimization complete. Final v2v error: 4.170197010040283 mm

Highest mean error: 5.005787372589111 mm for frame 36

Lowest mean error: 3.655163526535034 mm for frame 14

Saving results

Total time: 43.71188950538635
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01101852
Iteration 2/25 | Loss: 0.00352678
Iteration 3/25 | Loss: 0.00291134
Iteration 4/25 | Loss: 0.00219490
Iteration 5/25 | Loss: 0.00194593
Iteration 6/25 | Loss: 0.00187019
Iteration 7/25 | Loss: 0.00178628
Iteration 8/25 | Loss: 0.00171883
Iteration 9/25 | Loss: 0.00166500
Iteration 10/25 | Loss: 0.00173343
Iteration 11/25 | Loss: 0.00164779
Iteration 12/25 | Loss: 0.00163833
Iteration 13/25 | Loss: 0.00163533
Iteration 14/25 | Loss: 0.00163327
Iteration 15/25 | Loss: 0.00163272
Iteration 16/25 | Loss: 0.00163236
Iteration 17/25 | Loss: 0.00163221
Iteration 18/25 | Loss: 0.00163208
Iteration 19/25 | Loss: 0.00163682
Iteration 20/25 | Loss: 0.00164054
Iteration 21/25 | Loss: 0.00163298
Iteration 22/25 | Loss: 0.00162385
Iteration 23/25 | Loss: 0.00162210
Iteration 24/25 | Loss: 0.00162177
Iteration 25/25 | Loss: 0.00162164

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.42308366
Iteration 2/25 | Loss: 0.00982232
Iteration 3/25 | Loss: 0.00144361
Iteration 4/25 | Loss: 0.00144360
Iteration 5/25 | Loss: 0.00144360
Iteration 6/25 | Loss: 0.00144360
Iteration 7/25 | Loss: 0.00144360
Iteration 8/25 | Loss: 0.00144360
Iteration 9/25 | Loss: 0.00144360
Iteration 10/25 | Loss: 0.00144360
Iteration 11/25 | Loss: 0.00144360
Iteration 12/25 | Loss: 0.00144360
Iteration 13/25 | Loss: 0.00144360
Iteration 14/25 | Loss: 0.00144360
Iteration 15/25 | Loss: 0.00144360
Iteration 16/25 | Loss: 0.00144360
Iteration 17/25 | Loss: 0.00144360
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0014436019118875265, 0.0014436019118875265, 0.0014436019118875265, 0.0014436019118875265, 0.0014436019118875265]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014436019118875265

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00144360
Iteration 2/1000 | Loss: 0.00220989
Iteration 3/1000 | Loss: 0.00339474
Iteration 4/1000 | Loss: 0.00302567
Iteration 5/1000 | Loss: 0.00291320
Iteration 6/1000 | Loss: 0.00314664
Iteration 7/1000 | Loss: 0.00229360
Iteration 8/1000 | Loss: 0.00241006
Iteration 9/1000 | Loss: 0.00251050
Iteration 10/1000 | Loss: 0.00242310
Iteration 11/1000 | Loss: 0.00098812
Iteration 12/1000 | Loss: 0.00088368
Iteration 13/1000 | Loss: 0.00087991
Iteration 14/1000 | Loss: 0.00064610
Iteration 15/1000 | Loss: 0.00025531
Iteration 16/1000 | Loss: 0.00020705
Iteration 17/1000 | Loss: 0.00039657
Iteration 18/1000 | Loss: 0.00049308
Iteration 19/1000 | Loss: 0.00049618
Iteration 20/1000 | Loss: 0.00021209
Iteration 21/1000 | Loss: 0.00018172
Iteration 22/1000 | Loss: 0.00036586
Iteration 23/1000 | Loss: 0.00047818
Iteration 24/1000 | Loss: 0.00015500
Iteration 25/1000 | Loss: 0.00023643
Iteration 26/1000 | Loss: 0.00021001
Iteration 27/1000 | Loss: 0.00026273
Iteration 28/1000 | Loss: 0.00022161
Iteration 29/1000 | Loss: 0.00009595
Iteration 30/1000 | Loss: 0.00008454
Iteration 31/1000 | Loss: 0.00020094
Iteration 32/1000 | Loss: 0.00007714
Iteration 33/1000 | Loss: 0.00007220
Iteration 34/1000 | Loss: 0.00006771
Iteration 35/1000 | Loss: 0.00006481
Iteration 36/1000 | Loss: 0.00006230
Iteration 37/1000 | Loss: 0.00006011
Iteration 38/1000 | Loss: 0.00005882
Iteration 39/1000 | Loss: 0.00005768
Iteration 40/1000 | Loss: 0.00005663
Iteration 41/1000 | Loss: 0.00005584
Iteration 42/1000 | Loss: 0.00005508
Iteration 43/1000 | Loss: 0.00005446
Iteration 44/1000 | Loss: 0.00005392
Iteration 45/1000 | Loss: 0.00005353
Iteration 46/1000 | Loss: 0.00005324
Iteration 47/1000 | Loss: 0.00005309
Iteration 48/1000 | Loss: 0.00005299
Iteration 49/1000 | Loss: 0.00005286
Iteration 50/1000 | Loss: 0.00005280
Iteration 51/1000 | Loss: 0.00005274
Iteration 52/1000 | Loss: 0.00005263
Iteration 53/1000 | Loss: 0.00005251
Iteration 54/1000 | Loss: 0.00005249
Iteration 55/1000 | Loss: 0.00005247
Iteration 56/1000 | Loss: 0.00005242
Iteration 57/1000 | Loss: 0.00005240
Iteration 58/1000 | Loss: 0.00005240
Iteration 59/1000 | Loss: 0.00005239
Iteration 60/1000 | Loss: 0.00005236
Iteration 61/1000 | Loss: 0.00005236
Iteration 62/1000 | Loss: 0.00005236
Iteration 63/1000 | Loss: 0.00005236
Iteration 64/1000 | Loss: 0.00005236
Iteration 65/1000 | Loss: 0.00005236
Iteration 66/1000 | Loss: 0.00005236
Iteration 67/1000 | Loss: 0.00005236
Iteration 68/1000 | Loss: 0.00005235
Iteration 69/1000 | Loss: 0.00005235
Iteration 70/1000 | Loss: 0.00005234
Iteration 71/1000 | Loss: 0.00005233
Iteration 72/1000 | Loss: 0.00005233
Iteration 73/1000 | Loss: 0.00005233
Iteration 74/1000 | Loss: 0.00005233
Iteration 75/1000 | Loss: 0.00005233
Iteration 76/1000 | Loss: 0.00005233
Iteration 77/1000 | Loss: 0.00005232
Iteration 78/1000 | Loss: 0.00005232
Iteration 79/1000 | Loss: 0.00005232
Iteration 80/1000 | Loss: 0.00005232
Iteration 81/1000 | Loss: 0.00005232
Iteration 82/1000 | Loss: 0.00005232
Iteration 83/1000 | Loss: 0.00005232
Iteration 84/1000 | Loss: 0.00005232
Iteration 85/1000 | Loss: 0.00005232
Iteration 86/1000 | Loss: 0.00005232
Iteration 87/1000 | Loss: 0.00005232
Iteration 88/1000 | Loss: 0.00005232
Iteration 89/1000 | Loss: 0.00005232
Iteration 90/1000 | Loss: 0.00005232
Iteration 91/1000 | Loss: 0.00005231
Iteration 92/1000 | Loss: 0.00005231
Iteration 93/1000 | Loss: 0.00005231
Iteration 94/1000 | Loss: 0.00005231
Iteration 95/1000 | Loss: 0.00005231
Iteration 96/1000 | Loss: 0.00005231
Iteration 97/1000 | Loss: 0.00005231
Iteration 98/1000 | Loss: 0.00005231
Iteration 99/1000 | Loss: 0.00005231
Iteration 100/1000 | Loss: 0.00005231
Iteration 101/1000 | Loss: 0.00005231
Iteration 102/1000 | Loss: 0.00005231
Iteration 103/1000 | Loss: 0.00005230
Iteration 104/1000 | Loss: 0.00005230
Iteration 105/1000 | Loss: 0.00005230
Iteration 106/1000 | Loss: 0.00005230
Iteration 107/1000 | Loss: 0.00005230
Iteration 108/1000 | Loss: 0.00005230
Iteration 109/1000 | Loss: 0.00005230
Iteration 110/1000 | Loss: 0.00005230
Iteration 111/1000 | Loss: 0.00005230
Iteration 112/1000 | Loss: 0.00005230
Iteration 113/1000 | Loss: 0.00005229
Iteration 114/1000 | Loss: 0.00005229
Iteration 115/1000 | Loss: 0.00005229
Iteration 116/1000 | Loss: 0.00005229
Iteration 117/1000 | Loss: 0.00005229
Iteration 118/1000 | Loss: 0.00005229
Iteration 119/1000 | Loss: 0.00005228
Iteration 120/1000 | Loss: 0.00005228
Iteration 121/1000 | Loss: 0.00005228
Iteration 122/1000 | Loss: 0.00005227
Iteration 123/1000 | Loss: 0.00005227
Iteration 124/1000 | Loss: 0.00005227
Iteration 125/1000 | Loss: 0.00005227
Iteration 126/1000 | Loss: 0.00005227
Iteration 127/1000 | Loss: 0.00005227
Iteration 128/1000 | Loss: 0.00005227
Iteration 129/1000 | Loss: 0.00005227
Iteration 130/1000 | Loss: 0.00005227
Iteration 131/1000 | Loss: 0.00005227
Iteration 132/1000 | Loss: 0.00005226
Iteration 133/1000 | Loss: 0.00005226
Iteration 134/1000 | Loss: 0.00005225
Iteration 135/1000 | Loss: 0.00005225
Iteration 136/1000 | Loss: 0.00005225
Iteration 137/1000 | Loss: 0.00005225
Iteration 138/1000 | Loss: 0.00005224
Iteration 139/1000 | Loss: 0.00005224
Iteration 140/1000 | Loss: 0.00005224
Iteration 141/1000 | Loss: 0.00005224
Iteration 142/1000 | Loss: 0.00005224
Iteration 143/1000 | Loss: 0.00005224
Iteration 144/1000 | Loss: 0.00005224
Iteration 145/1000 | Loss: 0.00005224
Iteration 146/1000 | Loss: 0.00005224
Iteration 147/1000 | Loss: 0.00005224
Iteration 148/1000 | Loss: 0.00005224
Iteration 149/1000 | Loss: 0.00005223
Iteration 150/1000 | Loss: 0.00005223
Iteration 151/1000 | Loss: 0.00005223
Iteration 152/1000 | Loss: 0.00005223
Iteration 153/1000 | Loss: 0.00005222
Iteration 154/1000 | Loss: 0.00005222
Iteration 155/1000 | Loss: 0.00005222
Iteration 156/1000 | Loss: 0.00005222
Iteration 157/1000 | Loss: 0.00005222
Iteration 158/1000 | Loss: 0.00005222
Iteration 159/1000 | Loss: 0.00005222
Iteration 160/1000 | Loss: 0.00005222
Iteration 161/1000 | Loss: 0.00005222
Iteration 162/1000 | Loss: 0.00005222
Iteration 163/1000 | Loss: 0.00005222
Iteration 164/1000 | Loss: 0.00005222
Iteration 165/1000 | Loss: 0.00005222
Iteration 166/1000 | Loss: 0.00005221
Iteration 167/1000 | Loss: 0.00005221
Iteration 168/1000 | Loss: 0.00005221
Iteration 169/1000 | Loss: 0.00005221
Iteration 170/1000 | Loss: 0.00005221
Iteration 171/1000 | Loss: 0.00005221
Iteration 172/1000 | Loss: 0.00005221
Iteration 173/1000 | Loss: 0.00005221
Iteration 174/1000 | Loss: 0.00005221
Iteration 175/1000 | Loss: 0.00005221
Iteration 176/1000 | Loss: 0.00005221
Iteration 177/1000 | Loss: 0.00005221
Iteration 178/1000 | Loss: 0.00005221
Iteration 179/1000 | Loss: 0.00005220
Iteration 180/1000 | Loss: 0.00005220
Iteration 181/1000 | Loss: 0.00005220
Iteration 182/1000 | Loss: 0.00005220
Iteration 183/1000 | Loss: 0.00005220
Iteration 184/1000 | Loss: 0.00005220
Iteration 185/1000 | Loss: 0.00005220
Iteration 186/1000 | Loss: 0.00005220
Iteration 187/1000 | Loss: 0.00005220
Iteration 188/1000 | Loss: 0.00005220
Iteration 189/1000 | Loss: 0.00005219
Iteration 190/1000 | Loss: 0.00005219
Iteration 191/1000 | Loss: 0.00005219
Iteration 192/1000 | Loss: 0.00005219
Iteration 193/1000 | Loss: 0.00005219
Iteration 194/1000 | Loss: 0.00005219
Iteration 195/1000 | Loss: 0.00005219
Iteration 196/1000 | Loss: 0.00005219
Iteration 197/1000 | Loss: 0.00005219
Iteration 198/1000 | Loss: 0.00005219
Iteration 199/1000 | Loss: 0.00005218
Iteration 200/1000 | Loss: 0.00005218
Iteration 201/1000 | Loss: 0.00005218
Iteration 202/1000 | Loss: 0.00005218
Iteration 203/1000 | Loss: 0.00005218
Iteration 204/1000 | Loss: 0.00005218
Iteration 205/1000 | Loss: 0.00005218
Iteration 206/1000 | Loss: 0.00005218
Iteration 207/1000 | Loss: 0.00005218
Iteration 208/1000 | Loss: 0.00005218
Iteration 209/1000 | Loss: 0.00005218
Iteration 210/1000 | Loss: 0.00005218
Iteration 211/1000 | Loss: 0.00005217
Iteration 212/1000 | Loss: 0.00005217
Iteration 213/1000 | Loss: 0.00005217
Iteration 214/1000 | Loss: 0.00005217
Iteration 215/1000 | Loss: 0.00005217
Iteration 216/1000 | Loss: 0.00005217
Iteration 217/1000 | Loss: 0.00005217
Iteration 218/1000 | Loss: 0.00005217
Iteration 219/1000 | Loss: 0.00005217
Iteration 220/1000 | Loss: 0.00005217
Iteration 221/1000 | Loss: 0.00005217
Iteration 222/1000 | Loss: 0.00005217
Iteration 223/1000 | Loss: 0.00005217
Iteration 224/1000 | Loss: 0.00005217
Iteration 225/1000 | Loss: 0.00005217
Iteration 226/1000 | Loss: 0.00005217
Iteration 227/1000 | Loss: 0.00005217
Iteration 228/1000 | Loss: 0.00005217
Iteration 229/1000 | Loss: 0.00005217
Iteration 230/1000 | Loss: 0.00005217
Iteration 231/1000 | Loss: 0.00005217
Iteration 232/1000 | Loss: 0.00005217
Iteration 233/1000 | Loss: 0.00005217
Iteration 234/1000 | Loss: 0.00005217
Iteration 235/1000 | Loss: 0.00005217
Iteration 236/1000 | Loss: 0.00005217
Iteration 237/1000 | Loss: 0.00005217
Iteration 238/1000 | Loss: 0.00005217
Iteration 239/1000 | Loss: 0.00005217
Iteration 240/1000 | Loss: 0.00005217
Iteration 241/1000 | Loss: 0.00005217
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 241. Stopping optimization.
Last 5 losses: [5.217295984039083e-05, 5.217295984039083e-05, 5.217295984039083e-05, 5.217295984039083e-05, 5.217295984039083e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.217295984039083e-05

Optimization complete. Final v2v error: 5.746887683868408 mm

Highest mean error: 7.227510929107666 mm for frame 73

Lowest mean error: 4.676689624786377 mm for frame 121

Saving results

Total time: 127.07008814811707
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00794200
Iteration 2/25 | Loss: 0.00116534
Iteration 3/25 | Loss: 0.00108478
Iteration 4/25 | Loss: 0.00107201
Iteration 5/25 | Loss: 0.00106937
Iteration 6/25 | Loss: 0.00106914
Iteration 7/25 | Loss: 0.00106914
Iteration 8/25 | Loss: 0.00106914
Iteration 9/25 | Loss: 0.00106914
Iteration 10/25 | Loss: 0.00106914
Iteration 11/25 | Loss: 0.00106914
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010691401548683643, 0.0010691401548683643, 0.0010691401548683643, 0.0010691401548683643, 0.0010691401548683643]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010691401548683643

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38915646
Iteration 2/25 | Loss: 0.00079004
Iteration 3/25 | Loss: 0.00079004
Iteration 4/25 | Loss: 0.00079004
Iteration 5/25 | Loss: 0.00079004
Iteration 6/25 | Loss: 0.00079004
Iteration 7/25 | Loss: 0.00079004
Iteration 8/25 | Loss: 0.00079004
Iteration 9/25 | Loss: 0.00079004
Iteration 10/25 | Loss: 0.00079004
Iteration 11/25 | Loss: 0.00079004
Iteration 12/25 | Loss: 0.00079004
Iteration 13/25 | Loss: 0.00079004
Iteration 14/25 | Loss: 0.00079004
Iteration 15/25 | Loss: 0.00079004
Iteration 16/25 | Loss: 0.00079004
Iteration 17/25 | Loss: 0.00079004
Iteration 18/25 | Loss: 0.00079004
Iteration 19/25 | Loss: 0.00079004
Iteration 20/25 | Loss: 0.00079004
Iteration 21/25 | Loss: 0.00079004
Iteration 22/25 | Loss: 0.00079004
Iteration 23/25 | Loss: 0.00079004
Iteration 24/25 | Loss: 0.00079004
Iteration 25/25 | Loss: 0.00079004

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079004
Iteration 2/1000 | Loss: 0.00002240
Iteration 3/1000 | Loss: 0.00001522
Iteration 4/1000 | Loss: 0.00001366
Iteration 5/1000 | Loss: 0.00001297
Iteration 6/1000 | Loss: 0.00001245
Iteration 7/1000 | Loss: 0.00001211
Iteration 8/1000 | Loss: 0.00001175
Iteration 9/1000 | Loss: 0.00001173
Iteration 10/1000 | Loss: 0.00001163
Iteration 11/1000 | Loss: 0.00001135
Iteration 12/1000 | Loss: 0.00001126
Iteration 13/1000 | Loss: 0.00001122
Iteration 14/1000 | Loss: 0.00001115
Iteration 15/1000 | Loss: 0.00001114
Iteration 16/1000 | Loss: 0.00001113
Iteration 17/1000 | Loss: 0.00001101
Iteration 18/1000 | Loss: 0.00001096
Iteration 19/1000 | Loss: 0.00001095
Iteration 20/1000 | Loss: 0.00001083
Iteration 21/1000 | Loss: 0.00001080
Iteration 22/1000 | Loss: 0.00001077
Iteration 23/1000 | Loss: 0.00001077
Iteration 24/1000 | Loss: 0.00001077
Iteration 25/1000 | Loss: 0.00001076
Iteration 26/1000 | Loss: 0.00001072
Iteration 27/1000 | Loss: 0.00001071
Iteration 28/1000 | Loss: 0.00001071
Iteration 29/1000 | Loss: 0.00001071
Iteration 30/1000 | Loss: 0.00001070
Iteration 31/1000 | Loss: 0.00001070
Iteration 32/1000 | Loss: 0.00001069
Iteration 33/1000 | Loss: 0.00001069
Iteration 34/1000 | Loss: 0.00001069
Iteration 35/1000 | Loss: 0.00001068
Iteration 36/1000 | Loss: 0.00001067
Iteration 37/1000 | Loss: 0.00001067
Iteration 38/1000 | Loss: 0.00001066
Iteration 39/1000 | Loss: 0.00001066
Iteration 40/1000 | Loss: 0.00001066
Iteration 41/1000 | Loss: 0.00001066
Iteration 42/1000 | Loss: 0.00001066
Iteration 43/1000 | Loss: 0.00001066
Iteration 44/1000 | Loss: 0.00001066
Iteration 45/1000 | Loss: 0.00001066
Iteration 46/1000 | Loss: 0.00001066
Iteration 47/1000 | Loss: 0.00001066
Iteration 48/1000 | Loss: 0.00001065
Iteration 49/1000 | Loss: 0.00001065
Iteration 50/1000 | Loss: 0.00001065
Iteration 51/1000 | Loss: 0.00001065
Iteration 52/1000 | Loss: 0.00001065
Iteration 53/1000 | Loss: 0.00001065
Iteration 54/1000 | Loss: 0.00001065
Iteration 55/1000 | Loss: 0.00001064
Iteration 56/1000 | Loss: 0.00001062
Iteration 57/1000 | Loss: 0.00001062
Iteration 58/1000 | Loss: 0.00001062
Iteration 59/1000 | Loss: 0.00001061
Iteration 60/1000 | Loss: 0.00001061
Iteration 61/1000 | Loss: 0.00001061
Iteration 62/1000 | Loss: 0.00001060
Iteration 63/1000 | Loss: 0.00001060
Iteration 64/1000 | Loss: 0.00001059
Iteration 65/1000 | Loss: 0.00001057
Iteration 66/1000 | Loss: 0.00001056
Iteration 67/1000 | Loss: 0.00001056
Iteration 68/1000 | Loss: 0.00001056
Iteration 69/1000 | Loss: 0.00001056
Iteration 70/1000 | Loss: 0.00001056
Iteration 71/1000 | Loss: 0.00001056
Iteration 72/1000 | Loss: 0.00001054
Iteration 73/1000 | Loss: 0.00001054
Iteration 74/1000 | Loss: 0.00001053
Iteration 75/1000 | Loss: 0.00001053
Iteration 76/1000 | Loss: 0.00001053
Iteration 77/1000 | Loss: 0.00001053
Iteration 78/1000 | Loss: 0.00001053
Iteration 79/1000 | Loss: 0.00001053
Iteration 80/1000 | Loss: 0.00001053
Iteration 81/1000 | Loss: 0.00001053
Iteration 82/1000 | Loss: 0.00001053
Iteration 83/1000 | Loss: 0.00001052
Iteration 84/1000 | Loss: 0.00001052
Iteration 85/1000 | Loss: 0.00001051
Iteration 86/1000 | Loss: 0.00001051
Iteration 87/1000 | Loss: 0.00001051
Iteration 88/1000 | Loss: 0.00001051
Iteration 89/1000 | Loss: 0.00001051
Iteration 90/1000 | Loss: 0.00001050
Iteration 91/1000 | Loss: 0.00001050
Iteration 92/1000 | Loss: 0.00001050
Iteration 93/1000 | Loss: 0.00001050
Iteration 94/1000 | Loss: 0.00001049
Iteration 95/1000 | Loss: 0.00001049
Iteration 96/1000 | Loss: 0.00001049
Iteration 97/1000 | Loss: 0.00001049
Iteration 98/1000 | Loss: 0.00001049
Iteration 99/1000 | Loss: 0.00001049
Iteration 100/1000 | Loss: 0.00001049
Iteration 101/1000 | Loss: 0.00001049
Iteration 102/1000 | Loss: 0.00001049
Iteration 103/1000 | Loss: 0.00001049
Iteration 104/1000 | Loss: 0.00001049
Iteration 105/1000 | Loss: 0.00001048
Iteration 106/1000 | Loss: 0.00001048
Iteration 107/1000 | Loss: 0.00001048
Iteration 108/1000 | Loss: 0.00001048
Iteration 109/1000 | Loss: 0.00001048
Iteration 110/1000 | Loss: 0.00001048
Iteration 111/1000 | Loss: 0.00001047
Iteration 112/1000 | Loss: 0.00001047
Iteration 113/1000 | Loss: 0.00001047
Iteration 114/1000 | Loss: 0.00001047
Iteration 115/1000 | Loss: 0.00001047
Iteration 116/1000 | Loss: 0.00001047
Iteration 117/1000 | Loss: 0.00001047
Iteration 118/1000 | Loss: 0.00001047
Iteration 119/1000 | Loss: 0.00001047
Iteration 120/1000 | Loss: 0.00001047
Iteration 121/1000 | Loss: 0.00001047
Iteration 122/1000 | Loss: 0.00001047
Iteration 123/1000 | Loss: 0.00001047
Iteration 124/1000 | Loss: 0.00001047
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.0473358088347595e-05, 1.0473358088347595e-05, 1.0473358088347595e-05, 1.0473358088347595e-05, 1.0473358088347595e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0473358088347595e-05

Optimization complete. Final v2v error: 2.776895761489868 mm

Highest mean error: 3.018181562423706 mm for frame 64

Lowest mean error: 2.6546294689178467 mm for frame 146

Saving results

Total time: 36.44274377822876
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01033661
Iteration 2/25 | Loss: 0.00207837
Iteration 3/25 | Loss: 0.00188951
Iteration 4/25 | Loss: 0.00133853
Iteration 5/25 | Loss: 0.00137377
Iteration 6/25 | Loss: 0.00132080
Iteration 7/25 | Loss: 0.00121485
Iteration 8/25 | Loss: 0.00118862
Iteration 9/25 | Loss: 0.00114687
Iteration 10/25 | Loss: 0.00112901
Iteration 11/25 | Loss: 0.00112411
Iteration 12/25 | Loss: 0.00112162
Iteration 13/25 | Loss: 0.00111345
Iteration 14/25 | Loss: 0.00111012
Iteration 15/25 | Loss: 0.00110804
Iteration 16/25 | Loss: 0.00110854
Iteration 17/25 | Loss: 0.00110793
Iteration 18/25 | Loss: 0.00110714
Iteration 19/25 | Loss: 0.00110773
Iteration 20/25 | Loss: 0.00110743
Iteration 21/25 | Loss: 0.00110582
Iteration 22/25 | Loss: 0.00110769
Iteration 23/25 | Loss: 0.00110862
Iteration 24/25 | Loss: 0.00110944
Iteration 25/25 | Loss: 0.00110816

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39205790
Iteration 2/25 | Loss: 0.00134894
Iteration 3/25 | Loss: 0.00100397
Iteration 4/25 | Loss: 0.00100397
Iteration 5/25 | Loss: 0.00100396
Iteration 6/25 | Loss: 0.00100396
Iteration 7/25 | Loss: 0.00100396
Iteration 8/25 | Loss: 0.00100396
Iteration 9/25 | Loss: 0.00100396
Iteration 10/25 | Loss: 0.00100396
Iteration 11/25 | Loss: 0.00100396
Iteration 12/25 | Loss: 0.00100396
Iteration 13/25 | Loss: 0.00100396
Iteration 14/25 | Loss: 0.00100396
Iteration 15/25 | Loss: 0.00100396
Iteration 16/25 | Loss: 0.00100396
Iteration 17/25 | Loss: 0.00100396
Iteration 18/25 | Loss: 0.00100396
Iteration 19/25 | Loss: 0.00100396
Iteration 20/25 | Loss: 0.00100396
Iteration 21/25 | Loss: 0.00100396
Iteration 22/25 | Loss: 0.00100396
Iteration 23/25 | Loss: 0.00100396
Iteration 24/25 | Loss: 0.00100396
Iteration 25/25 | Loss: 0.00100396

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100396
Iteration 2/1000 | Loss: 0.00038298
Iteration 3/1000 | Loss: 0.00009060
Iteration 4/1000 | Loss: 0.00017138
Iteration 5/1000 | Loss: 0.00007124
Iteration 6/1000 | Loss: 0.00002613
Iteration 7/1000 | Loss: 0.00005022
Iteration 8/1000 | Loss: 0.00088878
Iteration 9/1000 | Loss: 0.00025885
Iteration 10/1000 | Loss: 0.00008011
Iteration 11/1000 | Loss: 0.00002232
Iteration 12/1000 | Loss: 0.00003470
Iteration 13/1000 | Loss: 0.00002042
Iteration 14/1000 | Loss: 0.00101335
Iteration 15/1000 | Loss: 0.00022770
Iteration 16/1000 | Loss: 0.00029247
Iteration 17/1000 | Loss: 0.00016742
Iteration 18/1000 | Loss: 0.00044525
Iteration 19/1000 | Loss: 0.00002132
Iteration 20/1000 | Loss: 0.00001848
Iteration 21/1000 | Loss: 0.00004269
Iteration 22/1000 | Loss: 0.00035270
Iteration 23/1000 | Loss: 0.00015392
Iteration 24/1000 | Loss: 0.00002410
Iteration 25/1000 | Loss: 0.00007123
Iteration 26/1000 | Loss: 0.00001613
Iteration 27/1000 | Loss: 0.00001552
Iteration 28/1000 | Loss: 0.00031458
Iteration 29/1000 | Loss: 0.00029623
Iteration 30/1000 | Loss: 0.00026848
Iteration 31/1000 | Loss: 0.00025819
Iteration 32/1000 | Loss: 0.00026306
Iteration 33/1000 | Loss: 0.00017752
Iteration 34/1000 | Loss: 0.00021939
Iteration 35/1000 | Loss: 0.00021512
Iteration 36/1000 | Loss: 0.00022264
Iteration 37/1000 | Loss: 0.00020474
Iteration 38/1000 | Loss: 0.00022358
Iteration 39/1000 | Loss: 0.00020283
Iteration 40/1000 | Loss: 0.00003112
Iteration 41/1000 | Loss: 0.00001863
Iteration 42/1000 | Loss: 0.00001598
Iteration 43/1000 | Loss: 0.00001434
Iteration 44/1000 | Loss: 0.00003769
Iteration 45/1000 | Loss: 0.00001342
Iteration 46/1000 | Loss: 0.00002726
Iteration 47/1000 | Loss: 0.00001273
Iteration 48/1000 | Loss: 0.00001251
Iteration 49/1000 | Loss: 0.00001248
Iteration 50/1000 | Loss: 0.00001240
Iteration 51/1000 | Loss: 0.00005698
Iteration 52/1000 | Loss: 0.00001240
Iteration 53/1000 | Loss: 0.00002546
Iteration 54/1000 | Loss: 0.00001229
Iteration 55/1000 | Loss: 0.00001227
Iteration 56/1000 | Loss: 0.00001227
Iteration 57/1000 | Loss: 0.00001226
Iteration 58/1000 | Loss: 0.00001226
Iteration 59/1000 | Loss: 0.00001226
Iteration 60/1000 | Loss: 0.00001226
Iteration 61/1000 | Loss: 0.00001226
Iteration 62/1000 | Loss: 0.00001225
Iteration 63/1000 | Loss: 0.00001225
Iteration 64/1000 | Loss: 0.00001224
Iteration 65/1000 | Loss: 0.00001224
Iteration 66/1000 | Loss: 0.00001223
Iteration 67/1000 | Loss: 0.00001223
Iteration 68/1000 | Loss: 0.00001223
Iteration 69/1000 | Loss: 0.00001221
Iteration 70/1000 | Loss: 0.00001221
Iteration 71/1000 | Loss: 0.00001221
Iteration 72/1000 | Loss: 0.00001220
Iteration 73/1000 | Loss: 0.00001219
Iteration 74/1000 | Loss: 0.00001215
Iteration 75/1000 | Loss: 0.00001213
Iteration 76/1000 | Loss: 0.00001213
Iteration 77/1000 | Loss: 0.00001211
Iteration 78/1000 | Loss: 0.00001210
Iteration 79/1000 | Loss: 0.00001210
Iteration 80/1000 | Loss: 0.00001210
Iteration 81/1000 | Loss: 0.00001209
Iteration 82/1000 | Loss: 0.00006532
Iteration 83/1000 | Loss: 0.00001406
Iteration 84/1000 | Loss: 0.00001207
Iteration 85/1000 | Loss: 0.00001197
Iteration 86/1000 | Loss: 0.00001193
Iteration 87/1000 | Loss: 0.00001192
Iteration 88/1000 | Loss: 0.00001191
Iteration 89/1000 | Loss: 0.00001191
Iteration 90/1000 | Loss: 0.00001191
Iteration 91/1000 | Loss: 0.00001190
Iteration 92/1000 | Loss: 0.00001190
Iteration 93/1000 | Loss: 0.00001190
Iteration 94/1000 | Loss: 0.00003620
Iteration 95/1000 | Loss: 0.00001194
Iteration 96/1000 | Loss: 0.00001192
Iteration 97/1000 | Loss: 0.00001192
Iteration 98/1000 | Loss: 0.00001192
Iteration 99/1000 | Loss: 0.00001192
Iteration 100/1000 | Loss: 0.00001192
Iteration 101/1000 | Loss: 0.00001192
Iteration 102/1000 | Loss: 0.00001192
Iteration 103/1000 | Loss: 0.00001192
Iteration 104/1000 | Loss: 0.00001192
Iteration 105/1000 | Loss: 0.00001192
Iteration 106/1000 | Loss: 0.00001191
Iteration 107/1000 | Loss: 0.00001191
Iteration 108/1000 | Loss: 0.00001191
Iteration 109/1000 | Loss: 0.00001190
Iteration 110/1000 | Loss: 0.00001190
Iteration 111/1000 | Loss: 0.00001190
Iteration 112/1000 | Loss: 0.00001190
Iteration 113/1000 | Loss: 0.00001190
Iteration 114/1000 | Loss: 0.00001190
Iteration 115/1000 | Loss: 0.00001190
Iteration 116/1000 | Loss: 0.00001190
Iteration 117/1000 | Loss: 0.00001190
Iteration 118/1000 | Loss: 0.00001190
Iteration 119/1000 | Loss: 0.00001190
Iteration 120/1000 | Loss: 0.00001190
Iteration 121/1000 | Loss: 0.00001190
Iteration 122/1000 | Loss: 0.00001190
Iteration 123/1000 | Loss: 0.00001190
Iteration 124/1000 | Loss: 0.00001190
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.1897895092261024e-05, 1.1897895092261024e-05, 1.1897895092261024e-05, 1.1897895092261024e-05, 1.1897895092261024e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1897895092261024e-05

Optimization complete. Final v2v error: 2.945763111114502 mm

Highest mean error: 3.6429531574249268 mm for frame 57

Lowest mean error: 2.499441146850586 mm for frame 163

Saving results

Total time: 130.35825490951538
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00801485
Iteration 2/25 | Loss: 0.00116107
Iteration 3/25 | Loss: 0.00106526
Iteration 4/25 | Loss: 0.00105648
Iteration 5/25 | Loss: 0.00105404
Iteration 6/25 | Loss: 0.00105387
Iteration 7/25 | Loss: 0.00105387
Iteration 8/25 | Loss: 0.00105387
Iteration 9/25 | Loss: 0.00105387
Iteration 10/25 | Loss: 0.00105387
Iteration 11/25 | Loss: 0.00105387
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010538715869188309, 0.0010538715869188309, 0.0010538715869188309, 0.0010538715869188309, 0.0010538715869188309]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010538715869188309

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35782480
Iteration 2/25 | Loss: 0.00081399
Iteration 3/25 | Loss: 0.00081398
Iteration 4/25 | Loss: 0.00081398
Iteration 5/25 | Loss: 0.00081398
Iteration 6/25 | Loss: 0.00081398
Iteration 7/25 | Loss: 0.00081398
Iteration 8/25 | Loss: 0.00081398
Iteration 9/25 | Loss: 0.00081398
Iteration 10/25 | Loss: 0.00081398
Iteration 11/25 | Loss: 0.00081398
Iteration 12/25 | Loss: 0.00081398
Iteration 13/25 | Loss: 0.00081398
Iteration 14/25 | Loss: 0.00081398
Iteration 15/25 | Loss: 0.00081398
Iteration 16/25 | Loss: 0.00081398
Iteration 17/25 | Loss: 0.00081398
Iteration 18/25 | Loss: 0.00081398
Iteration 19/25 | Loss: 0.00081398
Iteration 20/25 | Loss: 0.00081398
Iteration 21/25 | Loss: 0.00081398
Iteration 22/25 | Loss: 0.00081398
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008139803539961576, 0.0008139803539961576, 0.0008139803539961576, 0.0008139803539961576, 0.0008139803539961576]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008139803539961576

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081398
Iteration 2/1000 | Loss: 0.00002066
Iteration 3/1000 | Loss: 0.00001302
Iteration 4/1000 | Loss: 0.00001140
Iteration 5/1000 | Loss: 0.00001065
Iteration 6/1000 | Loss: 0.00001016
Iteration 7/1000 | Loss: 0.00000972
Iteration 8/1000 | Loss: 0.00000952
Iteration 9/1000 | Loss: 0.00000951
Iteration 10/1000 | Loss: 0.00000933
Iteration 11/1000 | Loss: 0.00000918
Iteration 12/1000 | Loss: 0.00000915
Iteration 13/1000 | Loss: 0.00000914
Iteration 14/1000 | Loss: 0.00000911
Iteration 15/1000 | Loss: 0.00000911
Iteration 16/1000 | Loss: 0.00000910
Iteration 17/1000 | Loss: 0.00000909
Iteration 18/1000 | Loss: 0.00000906
Iteration 19/1000 | Loss: 0.00000900
Iteration 20/1000 | Loss: 0.00000900
Iteration 21/1000 | Loss: 0.00000897
Iteration 22/1000 | Loss: 0.00000897
Iteration 23/1000 | Loss: 0.00000896
Iteration 24/1000 | Loss: 0.00000894
Iteration 25/1000 | Loss: 0.00000893
Iteration 26/1000 | Loss: 0.00000893
Iteration 27/1000 | Loss: 0.00000892
Iteration 28/1000 | Loss: 0.00000891
Iteration 29/1000 | Loss: 0.00000891
Iteration 30/1000 | Loss: 0.00000891
Iteration 31/1000 | Loss: 0.00000890
Iteration 32/1000 | Loss: 0.00000889
Iteration 33/1000 | Loss: 0.00000888
Iteration 34/1000 | Loss: 0.00000888
Iteration 35/1000 | Loss: 0.00000886
Iteration 36/1000 | Loss: 0.00000885
Iteration 37/1000 | Loss: 0.00000885
Iteration 38/1000 | Loss: 0.00000885
Iteration 39/1000 | Loss: 0.00000885
Iteration 40/1000 | Loss: 0.00000885
Iteration 41/1000 | Loss: 0.00000884
Iteration 42/1000 | Loss: 0.00000884
Iteration 43/1000 | Loss: 0.00000883
Iteration 44/1000 | Loss: 0.00000883
Iteration 45/1000 | Loss: 0.00000882
Iteration 46/1000 | Loss: 0.00000881
Iteration 47/1000 | Loss: 0.00000881
Iteration 48/1000 | Loss: 0.00000880
Iteration 49/1000 | Loss: 0.00000880
Iteration 50/1000 | Loss: 0.00000880
Iteration 51/1000 | Loss: 0.00000879
Iteration 52/1000 | Loss: 0.00000879
Iteration 53/1000 | Loss: 0.00000878
Iteration 54/1000 | Loss: 0.00000878
Iteration 55/1000 | Loss: 0.00000878
Iteration 56/1000 | Loss: 0.00000878
Iteration 57/1000 | Loss: 0.00000877
Iteration 58/1000 | Loss: 0.00000877
Iteration 59/1000 | Loss: 0.00000877
Iteration 60/1000 | Loss: 0.00000877
Iteration 61/1000 | Loss: 0.00000877
Iteration 62/1000 | Loss: 0.00000876
Iteration 63/1000 | Loss: 0.00000876
Iteration 64/1000 | Loss: 0.00000876
Iteration 65/1000 | Loss: 0.00000876
Iteration 66/1000 | Loss: 0.00000876
Iteration 67/1000 | Loss: 0.00000876
Iteration 68/1000 | Loss: 0.00000876
Iteration 69/1000 | Loss: 0.00000875
Iteration 70/1000 | Loss: 0.00000874
Iteration 71/1000 | Loss: 0.00000874
Iteration 72/1000 | Loss: 0.00000874
Iteration 73/1000 | Loss: 0.00000874
Iteration 74/1000 | Loss: 0.00000874
Iteration 75/1000 | Loss: 0.00000874
Iteration 76/1000 | Loss: 0.00000873
Iteration 77/1000 | Loss: 0.00000873
Iteration 78/1000 | Loss: 0.00000873
Iteration 79/1000 | Loss: 0.00000873
Iteration 80/1000 | Loss: 0.00000872
Iteration 81/1000 | Loss: 0.00000871
Iteration 82/1000 | Loss: 0.00000870
Iteration 83/1000 | Loss: 0.00000870
Iteration 84/1000 | Loss: 0.00000869
Iteration 85/1000 | Loss: 0.00000869
Iteration 86/1000 | Loss: 0.00000869
Iteration 87/1000 | Loss: 0.00000869
Iteration 88/1000 | Loss: 0.00000869
Iteration 89/1000 | Loss: 0.00000869
Iteration 90/1000 | Loss: 0.00000869
Iteration 91/1000 | Loss: 0.00000869
Iteration 92/1000 | Loss: 0.00000869
Iteration 93/1000 | Loss: 0.00000868
Iteration 94/1000 | Loss: 0.00000868
Iteration 95/1000 | Loss: 0.00000868
Iteration 96/1000 | Loss: 0.00000868
Iteration 97/1000 | Loss: 0.00000867
Iteration 98/1000 | Loss: 0.00000867
Iteration 99/1000 | Loss: 0.00000867
Iteration 100/1000 | Loss: 0.00000867
Iteration 101/1000 | Loss: 0.00000867
Iteration 102/1000 | Loss: 0.00000867
Iteration 103/1000 | Loss: 0.00000867
Iteration 104/1000 | Loss: 0.00000867
Iteration 105/1000 | Loss: 0.00000866
Iteration 106/1000 | Loss: 0.00000866
Iteration 107/1000 | Loss: 0.00000866
Iteration 108/1000 | Loss: 0.00000866
Iteration 109/1000 | Loss: 0.00000865
Iteration 110/1000 | Loss: 0.00000865
Iteration 111/1000 | Loss: 0.00000865
Iteration 112/1000 | Loss: 0.00000865
Iteration 113/1000 | Loss: 0.00000865
Iteration 114/1000 | Loss: 0.00000865
Iteration 115/1000 | Loss: 0.00000864
Iteration 116/1000 | Loss: 0.00000864
Iteration 117/1000 | Loss: 0.00000864
Iteration 118/1000 | Loss: 0.00000864
Iteration 119/1000 | Loss: 0.00000863
Iteration 120/1000 | Loss: 0.00000863
Iteration 121/1000 | Loss: 0.00000863
Iteration 122/1000 | Loss: 0.00000863
Iteration 123/1000 | Loss: 0.00000862
Iteration 124/1000 | Loss: 0.00000862
Iteration 125/1000 | Loss: 0.00000861
Iteration 126/1000 | Loss: 0.00000861
Iteration 127/1000 | Loss: 0.00000861
Iteration 128/1000 | Loss: 0.00000860
Iteration 129/1000 | Loss: 0.00000860
Iteration 130/1000 | Loss: 0.00000859
Iteration 131/1000 | Loss: 0.00000859
Iteration 132/1000 | Loss: 0.00000859
Iteration 133/1000 | Loss: 0.00000859
Iteration 134/1000 | Loss: 0.00000858
Iteration 135/1000 | Loss: 0.00000858
Iteration 136/1000 | Loss: 0.00000857
Iteration 137/1000 | Loss: 0.00000857
Iteration 138/1000 | Loss: 0.00000857
Iteration 139/1000 | Loss: 0.00000857
Iteration 140/1000 | Loss: 0.00000856
Iteration 141/1000 | Loss: 0.00000856
Iteration 142/1000 | Loss: 0.00000856
Iteration 143/1000 | Loss: 0.00000855
Iteration 144/1000 | Loss: 0.00000855
Iteration 145/1000 | Loss: 0.00000854
Iteration 146/1000 | Loss: 0.00000854
Iteration 147/1000 | Loss: 0.00000854
Iteration 148/1000 | Loss: 0.00000854
Iteration 149/1000 | Loss: 0.00000854
Iteration 150/1000 | Loss: 0.00000854
Iteration 151/1000 | Loss: 0.00000853
Iteration 152/1000 | Loss: 0.00000853
Iteration 153/1000 | Loss: 0.00000853
Iteration 154/1000 | Loss: 0.00000853
Iteration 155/1000 | Loss: 0.00000853
Iteration 156/1000 | Loss: 0.00000853
Iteration 157/1000 | Loss: 0.00000853
Iteration 158/1000 | Loss: 0.00000853
Iteration 159/1000 | Loss: 0.00000853
Iteration 160/1000 | Loss: 0.00000852
Iteration 161/1000 | Loss: 0.00000852
Iteration 162/1000 | Loss: 0.00000852
Iteration 163/1000 | Loss: 0.00000852
Iteration 164/1000 | Loss: 0.00000852
Iteration 165/1000 | Loss: 0.00000852
Iteration 166/1000 | Loss: 0.00000852
Iteration 167/1000 | Loss: 0.00000851
Iteration 168/1000 | Loss: 0.00000851
Iteration 169/1000 | Loss: 0.00000851
Iteration 170/1000 | Loss: 0.00000851
Iteration 171/1000 | Loss: 0.00000851
Iteration 172/1000 | Loss: 0.00000850
Iteration 173/1000 | Loss: 0.00000850
Iteration 174/1000 | Loss: 0.00000850
Iteration 175/1000 | Loss: 0.00000850
Iteration 176/1000 | Loss: 0.00000850
Iteration 177/1000 | Loss: 0.00000850
Iteration 178/1000 | Loss: 0.00000850
Iteration 179/1000 | Loss: 0.00000850
Iteration 180/1000 | Loss: 0.00000850
Iteration 181/1000 | Loss: 0.00000850
Iteration 182/1000 | Loss: 0.00000850
Iteration 183/1000 | Loss: 0.00000850
Iteration 184/1000 | Loss: 0.00000850
Iteration 185/1000 | Loss: 0.00000850
Iteration 186/1000 | Loss: 0.00000850
Iteration 187/1000 | Loss: 0.00000850
Iteration 188/1000 | Loss: 0.00000850
Iteration 189/1000 | Loss: 0.00000850
Iteration 190/1000 | Loss: 0.00000850
Iteration 191/1000 | Loss: 0.00000850
Iteration 192/1000 | Loss: 0.00000850
Iteration 193/1000 | Loss: 0.00000850
Iteration 194/1000 | Loss: 0.00000849
Iteration 195/1000 | Loss: 0.00000849
Iteration 196/1000 | Loss: 0.00000849
Iteration 197/1000 | Loss: 0.00000849
Iteration 198/1000 | Loss: 0.00000848
Iteration 199/1000 | Loss: 0.00000848
Iteration 200/1000 | Loss: 0.00000848
Iteration 201/1000 | Loss: 0.00000848
Iteration 202/1000 | Loss: 0.00000848
Iteration 203/1000 | Loss: 0.00000848
Iteration 204/1000 | Loss: 0.00000848
Iteration 205/1000 | Loss: 0.00000848
Iteration 206/1000 | Loss: 0.00000848
Iteration 207/1000 | Loss: 0.00000848
Iteration 208/1000 | Loss: 0.00000848
Iteration 209/1000 | Loss: 0.00000848
Iteration 210/1000 | Loss: 0.00000848
Iteration 211/1000 | Loss: 0.00000848
Iteration 212/1000 | Loss: 0.00000848
Iteration 213/1000 | Loss: 0.00000848
Iteration 214/1000 | Loss: 0.00000848
Iteration 215/1000 | Loss: 0.00000848
Iteration 216/1000 | Loss: 0.00000847
Iteration 217/1000 | Loss: 0.00000847
Iteration 218/1000 | Loss: 0.00000847
Iteration 219/1000 | Loss: 0.00000847
Iteration 220/1000 | Loss: 0.00000847
Iteration 221/1000 | Loss: 0.00000847
Iteration 222/1000 | Loss: 0.00000847
Iteration 223/1000 | Loss: 0.00000847
Iteration 224/1000 | Loss: 0.00000847
Iteration 225/1000 | Loss: 0.00000847
Iteration 226/1000 | Loss: 0.00000847
Iteration 227/1000 | Loss: 0.00000847
Iteration 228/1000 | Loss: 0.00000847
Iteration 229/1000 | Loss: 0.00000847
Iteration 230/1000 | Loss: 0.00000847
Iteration 231/1000 | Loss: 0.00000847
Iteration 232/1000 | Loss: 0.00000847
Iteration 233/1000 | Loss: 0.00000846
Iteration 234/1000 | Loss: 0.00000846
Iteration 235/1000 | Loss: 0.00000846
Iteration 236/1000 | Loss: 0.00000846
Iteration 237/1000 | Loss: 0.00000846
Iteration 238/1000 | Loss: 0.00000846
Iteration 239/1000 | Loss: 0.00000846
Iteration 240/1000 | Loss: 0.00000846
Iteration 241/1000 | Loss: 0.00000846
Iteration 242/1000 | Loss: 0.00000846
Iteration 243/1000 | Loss: 0.00000846
Iteration 244/1000 | Loss: 0.00000846
Iteration 245/1000 | Loss: 0.00000846
Iteration 246/1000 | Loss: 0.00000846
Iteration 247/1000 | Loss: 0.00000846
Iteration 248/1000 | Loss: 0.00000846
Iteration 249/1000 | Loss: 0.00000846
Iteration 250/1000 | Loss: 0.00000846
Iteration 251/1000 | Loss: 0.00000846
Iteration 252/1000 | Loss: 0.00000846
Iteration 253/1000 | Loss: 0.00000846
Iteration 254/1000 | Loss: 0.00000846
Iteration 255/1000 | Loss: 0.00000846
Iteration 256/1000 | Loss: 0.00000846
Iteration 257/1000 | Loss: 0.00000846
Iteration 258/1000 | Loss: 0.00000846
Iteration 259/1000 | Loss: 0.00000846
Iteration 260/1000 | Loss: 0.00000846
Iteration 261/1000 | Loss: 0.00000846
Iteration 262/1000 | Loss: 0.00000846
Iteration 263/1000 | Loss: 0.00000846
Iteration 264/1000 | Loss: 0.00000846
Iteration 265/1000 | Loss: 0.00000846
Iteration 266/1000 | Loss: 0.00000846
Iteration 267/1000 | Loss: 0.00000846
Iteration 268/1000 | Loss: 0.00000846
Iteration 269/1000 | Loss: 0.00000846
Iteration 270/1000 | Loss: 0.00000846
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 270. Stopping optimization.
Last 5 losses: [8.461539437121246e-06, 8.461539437121246e-06, 8.461539437121246e-06, 8.461539437121246e-06, 8.461539437121246e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.461539437121246e-06

Optimization complete. Final v2v error: 2.4876344203948975 mm

Highest mean error: 2.6504929065704346 mm for frame 57

Lowest mean error: 2.3486618995666504 mm for frame 105

Saving results

Total time: 40.60458946228027
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00419497
Iteration 2/25 | Loss: 0.00122845
Iteration 3/25 | Loss: 0.00115556
Iteration 4/25 | Loss: 0.00114924
Iteration 5/25 | Loss: 0.00114897
Iteration 6/25 | Loss: 0.00114897
Iteration 7/25 | Loss: 0.00114897
Iteration 8/25 | Loss: 0.00114897
Iteration 9/25 | Loss: 0.00114897
Iteration 10/25 | Loss: 0.00114897
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001148967887274921, 0.001148967887274921, 0.001148967887274921, 0.001148967887274921, 0.001148967887274921]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001148967887274921

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35361874
Iteration 2/25 | Loss: 0.00063429
Iteration 3/25 | Loss: 0.00063429
Iteration 4/25 | Loss: 0.00063429
Iteration 5/25 | Loss: 0.00063429
Iteration 6/25 | Loss: 0.00063429
Iteration 7/25 | Loss: 0.00063429
Iteration 8/25 | Loss: 0.00063429
Iteration 9/25 | Loss: 0.00063429
Iteration 10/25 | Loss: 0.00063429
Iteration 11/25 | Loss: 0.00063429
Iteration 12/25 | Loss: 0.00063429
Iteration 13/25 | Loss: 0.00063429
Iteration 14/25 | Loss: 0.00063429
Iteration 15/25 | Loss: 0.00063429
Iteration 16/25 | Loss: 0.00063429
Iteration 17/25 | Loss: 0.00063429
Iteration 18/25 | Loss: 0.00063429
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000634286436252296, 0.000634286436252296, 0.000634286436252296, 0.000634286436252296, 0.000634286436252296]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000634286436252296

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063429
Iteration 2/1000 | Loss: 0.00002059
Iteration 3/1000 | Loss: 0.00001832
Iteration 4/1000 | Loss: 0.00001737
Iteration 5/1000 | Loss: 0.00001650
Iteration 6/1000 | Loss: 0.00001594
Iteration 7/1000 | Loss: 0.00001592
Iteration 8/1000 | Loss: 0.00001537
Iteration 9/1000 | Loss: 0.00001513
Iteration 10/1000 | Loss: 0.00001497
Iteration 11/1000 | Loss: 0.00001497
Iteration 12/1000 | Loss: 0.00001486
Iteration 13/1000 | Loss: 0.00001481
Iteration 14/1000 | Loss: 0.00001481
Iteration 15/1000 | Loss: 0.00001480
Iteration 16/1000 | Loss: 0.00001478
Iteration 17/1000 | Loss: 0.00001477
Iteration 18/1000 | Loss: 0.00001474
Iteration 19/1000 | Loss: 0.00001474
Iteration 20/1000 | Loss: 0.00001474
Iteration 21/1000 | Loss: 0.00001474
Iteration 22/1000 | Loss: 0.00001473
Iteration 23/1000 | Loss: 0.00001473
Iteration 24/1000 | Loss: 0.00001472
Iteration 25/1000 | Loss: 0.00001471
Iteration 26/1000 | Loss: 0.00001471
Iteration 27/1000 | Loss: 0.00001463
Iteration 28/1000 | Loss: 0.00001460
Iteration 29/1000 | Loss: 0.00001460
Iteration 30/1000 | Loss: 0.00001460
Iteration 31/1000 | Loss: 0.00001460
Iteration 32/1000 | Loss: 0.00001460
Iteration 33/1000 | Loss: 0.00001460
Iteration 34/1000 | Loss: 0.00001459
Iteration 35/1000 | Loss: 0.00001459
Iteration 36/1000 | Loss: 0.00001459
Iteration 37/1000 | Loss: 0.00001459
Iteration 38/1000 | Loss: 0.00001459
Iteration 39/1000 | Loss: 0.00001459
Iteration 40/1000 | Loss: 0.00001459
Iteration 41/1000 | Loss: 0.00001459
Iteration 42/1000 | Loss: 0.00001458
Iteration 43/1000 | Loss: 0.00001458
Iteration 44/1000 | Loss: 0.00001457
Iteration 45/1000 | Loss: 0.00001457
Iteration 46/1000 | Loss: 0.00001456
Iteration 47/1000 | Loss: 0.00001456
Iteration 48/1000 | Loss: 0.00001455
Iteration 49/1000 | Loss: 0.00001454
Iteration 50/1000 | Loss: 0.00001453
Iteration 51/1000 | Loss: 0.00001453
Iteration 52/1000 | Loss: 0.00001453
Iteration 53/1000 | Loss: 0.00001453
Iteration 54/1000 | Loss: 0.00001452
Iteration 55/1000 | Loss: 0.00001452
Iteration 56/1000 | Loss: 0.00001452
Iteration 57/1000 | Loss: 0.00001452
Iteration 58/1000 | Loss: 0.00001451
Iteration 59/1000 | Loss: 0.00001451
Iteration 60/1000 | Loss: 0.00001450
Iteration 61/1000 | Loss: 0.00001450
Iteration 62/1000 | Loss: 0.00001450
Iteration 63/1000 | Loss: 0.00001450
Iteration 64/1000 | Loss: 0.00001450
Iteration 65/1000 | Loss: 0.00001449
Iteration 66/1000 | Loss: 0.00001449
Iteration 67/1000 | Loss: 0.00001448
Iteration 68/1000 | Loss: 0.00001447
Iteration 69/1000 | Loss: 0.00001447
Iteration 70/1000 | Loss: 0.00001445
Iteration 71/1000 | Loss: 0.00001445
Iteration 72/1000 | Loss: 0.00001445
Iteration 73/1000 | Loss: 0.00001444
Iteration 74/1000 | Loss: 0.00001444
Iteration 75/1000 | Loss: 0.00001444
Iteration 76/1000 | Loss: 0.00001444
Iteration 77/1000 | Loss: 0.00001444
Iteration 78/1000 | Loss: 0.00001444
Iteration 79/1000 | Loss: 0.00001443
Iteration 80/1000 | Loss: 0.00001442
Iteration 81/1000 | Loss: 0.00001442
Iteration 82/1000 | Loss: 0.00001442
Iteration 83/1000 | Loss: 0.00001441
Iteration 84/1000 | Loss: 0.00001441
Iteration 85/1000 | Loss: 0.00001441
Iteration 86/1000 | Loss: 0.00001441
Iteration 87/1000 | Loss: 0.00001440
Iteration 88/1000 | Loss: 0.00001440
Iteration 89/1000 | Loss: 0.00001439
Iteration 90/1000 | Loss: 0.00001439
Iteration 91/1000 | Loss: 0.00001439
Iteration 92/1000 | Loss: 0.00001438
Iteration 93/1000 | Loss: 0.00001438
Iteration 94/1000 | Loss: 0.00001438
Iteration 95/1000 | Loss: 0.00001438
Iteration 96/1000 | Loss: 0.00001438
Iteration 97/1000 | Loss: 0.00001438
Iteration 98/1000 | Loss: 0.00001438
Iteration 99/1000 | Loss: 0.00001438
Iteration 100/1000 | Loss: 0.00001437
Iteration 101/1000 | Loss: 0.00001437
Iteration 102/1000 | Loss: 0.00001437
Iteration 103/1000 | Loss: 0.00001437
Iteration 104/1000 | Loss: 0.00001437
Iteration 105/1000 | Loss: 0.00001437
Iteration 106/1000 | Loss: 0.00001437
Iteration 107/1000 | Loss: 0.00001437
Iteration 108/1000 | Loss: 0.00001437
Iteration 109/1000 | Loss: 0.00001437
Iteration 110/1000 | Loss: 0.00001437
Iteration 111/1000 | Loss: 0.00001437
Iteration 112/1000 | Loss: 0.00001437
Iteration 113/1000 | Loss: 0.00001437
Iteration 114/1000 | Loss: 0.00001437
Iteration 115/1000 | Loss: 0.00001437
Iteration 116/1000 | Loss: 0.00001437
Iteration 117/1000 | Loss: 0.00001437
Iteration 118/1000 | Loss: 0.00001437
Iteration 119/1000 | Loss: 0.00001437
Iteration 120/1000 | Loss: 0.00001437
Iteration 121/1000 | Loss: 0.00001437
Iteration 122/1000 | Loss: 0.00001437
Iteration 123/1000 | Loss: 0.00001437
Iteration 124/1000 | Loss: 0.00001437
Iteration 125/1000 | Loss: 0.00001437
Iteration 126/1000 | Loss: 0.00001437
Iteration 127/1000 | Loss: 0.00001437
Iteration 128/1000 | Loss: 0.00001437
Iteration 129/1000 | Loss: 0.00001437
Iteration 130/1000 | Loss: 0.00001437
Iteration 131/1000 | Loss: 0.00001437
Iteration 132/1000 | Loss: 0.00001437
Iteration 133/1000 | Loss: 0.00001437
Iteration 134/1000 | Loss: 0.00001437
Iteration 135/1000 | Loss: 0.00001437
Iteration 136/1000 | Loss: 0.00001437
Iteration 137/1000 | Loss: 0.00001436
Iteration 138/1000 | Loss: 0.00001436
Iteration 139/1000 | Loss: 0.00001436
Iteration 140/1000 | Loss: 0.00001436
Iteration 141/1000 | Loss: 0.00001436
Iteration 142/1000 | Loss: 0.00001436
Iteration 143/1000 | Loss: 0.00001436
Iteration 144/1000 | Loss: 0.00001436
Iteration 145/1000 | Loss: 0.00001436
Iteration 146/1000 | Loss: 0.00001436
Iteration 147/1000 | Loss: 0.00001436
Iteration 148/1000 | Loss: 0.00001436
Iteration 149/1000 | Loss: 0.00001436
Iteration 150/1000 | Loss: 0.00001436
Iteration 151/1000 | Loss: 0.00001436
Iteration 152/1000 | Loss: 0.00001436
Iteration 153/1000 | Loss: 0.00001436
Iteration 154/1000 | Loss: 0.00001436
Iteration 155/1000 | Loss: 0.00001436
Iteration 156/1000 | Loss: 0.00001436
Iteration 157/1000 | Loss: 0.00001436
Iteration 158/1000 | Loss: 0.00001436
Iteration 159/1000 | Loss: 0.00001436
Iteration 160/1000 | Loss: 0.00001436
Iteration 161/1000 | Loss: 0.00001436
Iteration 162/1000 | Loss: 0.00001436
Iteration 163/1000 | Loss: 0.00001436
Iteration 164/1000 | Loss: 0.00001436
Iteration 165/1000 | Loss: 0.00001436
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.4364039998326916e-05, 1.4364039998326916e-05, 1.4364039998326916e-05, 1.4364039998326916e-05, 1.4364039998326916e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4364039998326916e-05

Optimization complete. Final v2v error: 3.2002837657928467 mm

Highest mean error: 3.2299821376800537 mm for frame 1

Lowest mean error: 3.17873215675354 mm for frame 205

Saving results

Total time: 36.41510319709778
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00408495
Iteration 2/25 | Loss: 0.00114128
Iteration 3/25 | Loss: 0.00107651
Iteration 4/25 | Loss: 0.00106764
Iteration 5/25 | Loss: 0.00106520
Iteration 6/25 | Loss: 0.00106479
Iteration 7/25 | Loss: 0.00106479
Iteration 8/25 | Loss: 0.00106479
Iteration 9/25 | Loss: 0.00106479
Iteration 10/25 | Loss: 0.00106479
Iteration 11/25 | Loss: 0.00106479
Iteration 12/25 | Loss: 0.00106479
Iteration 13/25 | Loss: 0.00106479
Iteration 14/25 | Loss: 0.00106479
Iteration 15/25 | Loss: 0.00106479
Iteration 16/25 | Loss: 0.00106479
Iteration 17/25 | Loss: 0.00106479
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010647944873198867, 0.0010647944873198867, 0.0010647944873198867, 0.0010647944873198867, 0.0010647944873198867]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010647944873198867

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.44627404
Iteration 2/25 | Loss: 0.00078440
Iteration 3/25 | Loss: 0.00078440
Iteration 4/25 | Loss: 0.00078440
Iteration 5/25 | Loss: 0.00078440
Iteration 6/25 | Loss: 0.00078440
Iteration 7/25 | Loss: 0.00078439
Iteration 8/25 | Loss: 0.00078439
Iteration 9/25 | Loss: 0.00078439
Iteration 10/25 | Loss: 0.00078439
Iteration 11/25 | Loss: 0.00078439
Iteration 12/25 | Loss: 0.00078439
Iteration 13/25 | Loss: 0.00078439
Iteration 14/25 | Loss: 0.00078439
Iteration 15/25 | Loss: 0.00078439
Iteration 16/25 | Loss: 0.00078439
Iteration 17/25 | Loss: 0.00078439
Iteration 18/25 | Loss: 0.00078439
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007843936327844858, 0.0007843936327844858, 0.0007843936327844858, 0.0007843936327844858, 0.0007843936327844858]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007843936327844858

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078439
Iteration 2/1000 | Loss: 0.00001955
Iteration 3/1000 | Loss: 0.00001293
Iteration 4/1000 | Loss: 0.00001189
Iteration 5/1000 | Loss: 0.00001138
Iteration 6/1000 | Loss: 0.00001105
Iteration 7/1000 | Loss: 0.00001068
Iteration 8/1000 | Loss: 0.00001047
Iteration 9/1000 | Loss: 0.00001038
Iteration 10/1000 | Loss: 0.00001020
Iteration 11/1000 | Loss: 0.00001016
Iteration 12/1000 | Loss: 0.00001015
Iteration 13/1000 | Loss: 0.00001014
Iteration 14/1000 | Loss: 0.00001010
Iteration 15/1000 | Loss: 0.00001009
Iteration 16/1000 | Loss: 0.00001008
Iteration 17/1000 | Loss: 0.00001005
Iteration 18/1000 | Loss: 0.00000991
Iteration 19/1000 | Loss: 0.00000983
Iteration 20/1000 | Loss: 0.00000983
Iteration 21/1000 | Loss: 0.00000982
Iteration 22/1000 | Loss: 0.00000981
Iteration 23/1000 | Loss: 0.00000978
Iteration 24/1000 | Loss: 0.00000978
Iteration 25/1000 | Loss: 0.00000976
Iteration 26/1000 | Loss: 0.00000972
Iteration 27/1000 | Loss: 0.00000972
Iteration 28/1000 | Loss: 0.00000969
Iteration 29/1000 | Loss: 0.00000969
Iteration 30/1000 | Loss: 0.00000968
Iteration 31/1000 | Loss: 0.00000968
Iteration 32/1000 | Loss: 0.00000968
Iteration 33/1000 | Loss: 0.00000967
Iteration 34/1000 | Loss: 0.00000966
Iteration 35/1000 | Loss: 0.00000966
Iteration 36/1000 | Loss: 0.00000966
Iteration 37/1000 | Loss: 0.00000965
Iteration 38/1000 | Loss: 0.00000965
Iteration 39/1000 | Loss: 0.00000965
Iteration 40/1000 | Loss: 0.00000964
Iteration 41/1000 | Loss: 0.00000964
Iteration 42/1000 | Loss: 0.00000964
Iteration 43/1000 | Loss: 0.00000963
Iteration 44/1000 | Loss: 0.00000963
Iteration 45/1000 | Loss: 0.00000963
Iteration 46/1000 | Loss: 0.00000962
Iteration 47/1000 | Loss: 0.00000962
Iteration 48/1000 | Loss: 0.00000962
Iteration 49/1000 | Loss: 0.00000961
Iteration 50/1000 | Loss: 0.00000961
Iteration 51/1000 | Loss: 0.00000961
Iteration 52/1000 | Loss: 0.00000961
Iteration 53/1000 | Loss: 0.00000961
Iteration 54/1000 | Loss: 0.00000960
Iteration 55/1000 | Loss: 0.00000960
Iteration 56/1000 | Loss: 0.00000960
Iteration 57/1000 | Loss: 0.00000959
Iteration 58/1000 | Loss: 0.00000959
Iteration 59/1000 | Loss: 0.00000958
Iteration 60/1000 | Loss: 0.00000958
Iteration 61/1000 | Loss: 0.00000958
Iteration 62/1000 | Loss: 0.00000958
Iteration 63/1000 | Loss: 0.00000958
Iteration 64/1000 | Loss: 0.00000958
Iteration 65/1000 | Loss: 0.00000957
Iteration 66/1000 | Loss: 0.00000957
Iteration 67/1000 | Loss: 0.00000957
Iteration 68/1000 | Loss: 0.00000956
Iteration 69/1000 | Loss: 0.00000956
Iteration 70/1000 | Loss: 0.00000955
Iteration 71/1000 | Loss: 0.00000955
Iteration 72/1000 | Loss: 0.00000955
Iteration 73/1000 | Loss: 0.00000954
Iteration 74/1000 | Loss: 0.00000954
Iteration 75/1000 | Loss: 0.00000954
Iteration 76/1000 | Loss: 0.00000954
Iteration 77/1000 | Loss: 0.00000954
Iteration 78/1000 | Loss: 0.00000954
Iteration 79/1000 | Loss: 0.00000954
Iteration 80/1000 | Loss: 0.00000953
Iteration 81/1000 | Loss: 0.00000953
Iteration 82/1000 | Loss: 0.00000953
Iteration 83/1000 | Loss: 0.00000953
Iteration 84/1000 | Loss: 0.00000952
Iteration 85/1000 | Loss: 0.00000952
Iteration 86/1000 | Loss: 0.00000952
Iteration 87/1000 | Loss: 0.00000952
Iteration 88/1000 | Loss: 0.00000952
Iteration 89/1000 | Loss: 0.00000951
Iteration 90/1000 | Loss: 0.00000951
Iteration 91/1000 | Loss: 0.00000951
Iteration 92/1000 | Loss: 0.00000951
Iteration 93/1000 | Loss: 0.00000951
Iteration 94/1000 | Loss: 0.00000950
Iteration 95/1000 | Loss: 0.00000950
Iteration 96/1000 | Loss: 0.00000950
Iteration 97/1000 | Loss: 0.00000950
Iteration 98/1000 | Loss: 0.00000950
Iteration 99/1000 | Loss: 0.00000949
Iteration 100/1000 | Loss: 0.00000949
Iteration 101/1000 | Loss: 0.00000949
Iteration 102/1000 | Loss: 0.00000949
Iteration 103/1000 | Loss: 0.00000949
Iteration 104/1000 | Loss: 0.00000949
Iteration 105/1000 | Loss: 0.00000949
Iteration 106/1000 | Loss: 0.00000949
Iteration 107/1000 | Loss: 0.00000948
Iteration 108/1000 | Loss: 0.00000948
Iteration 109/1000 | Loss: 0.00000948
Iteration 110/1000 | Loss: 0.00000948
Iteration 111/1000 | Loss: 0.00000948
Iteration 112/1000 | Loss: 0.00000947
Iteration 113/1000 | Loss: 0.00000947
Iteration 114/1000 | Loss: 0.00000947
Iteration 115/1000 | Loss: 0.00000947
Iteration 116/1000 | Loss: 0.00000947
Iteration 117/1000 | Loss: 0.00000947
Iteration 118/1000 | Loss: 0.00000947
Iteration 119/1000 | Loss: 0.00000947
Iteration 120/1000 | Loss: 0.00000947
Iteration 121/1000 | Loss: 0.00000947
Iteration 122/1000 | Loss: 0.00000946
Iteration 123/1000 | Loss: 0.00000946
Iteration 124/1000 | Loss: 0.00000946
Iteration 125/1000 | Loss: 0.00000945
Iteration 126/1000 | Loss: 0.00000945
Iteration 127/1000 | Loss: 0.00000945
Iteration 128/1000 | Loss: 0.00000945
Iteration 129/1000 | Loss: 0.00000944
Iteration 130/1000 | Loss: 0.00000944
Iteration 131/1000 | Loss: 0.00000944
Iteration 132/1000 | Loss: 0.00000944
Iteration 133/1000 | Loss: 0.00000944
Iteration 134/1000 | Loss: 0.00000944
Iteration 135/1000 | Loss: 0.00000944
Iteration 136/1000 | Loss: 0.00000944
Iteration 137/1000 | Loss: 0.00000943
Iteration 138/1000 | Loss: 0.00000943
Iteration 139/1000 | Loss: 0.00000943
Iteration 140/1000 | Loss: 0.00000943
Iteration 141/1000 | Loss: 0.00000943
Iteration 142/1000 | Loss: 0.00000943
Iteration 143/1000 | Loss: 0.00000943
Iteration 144/1000 | Loss: 0.00000943
Iteration 145/1000 | Loss: 0.00000943
Iteration 146/1000 | Loss: 0.00000942
Iteration 147/1000 | Loss: 0.00000942
Iteration 148/1000 | Loss: 0.00000942
Iteration 149/1000 | Loss: 0.00000942
Iteration 150/1000 | Loss: 0.00000942
Iteration 151/1000 | Loss: 0.00000942
Iteration 152/1000 | Loss: 0.00000942
Iteration 153/1000 | Loss: 0.00000942
Iteration 154/1000 | Loss: 0.00000942
Iteration 155/1000 | Loss: 0.00000942
Iteration 156/1000 | Loss: 0.00000942
Iteration 157/1000 | Loss: 0.00000942
Iteration 158/1000 | Loss: 0.00000942
Iteration 159/1000 | Loss: 0.00000942
Iteration 160/1000 | Loss: 0.00000942
Iteration 161/1000 | Loss: 0.00000941
Iteration 162/1000 | Loss: 0.00000941
Iteration 163/1000 | Loss: 0.00000941
Iteration 164/1000 | Loss: 0.00000941
Iteration 165/1000 | Loss: 0.00000941
Iteration 166/1000 | Loss: 0.00000941
Iteration 167/1000 | Loss: 0.00000941
Iteration 168/1000 | Loss: 0.00000941
Iteration 169/1000 | Loss: 0.00000941
Iteration 170/1000 | Loss: 0.00000941
Iteration 171/1000 | Loss: 0.00000941
Iteration 172/1000 | Loss: 0.00000941
Iteration 173/1000 | Loss: 0.00000941
Iteration 174/1000 | Loss: 0.00000941
Iteration 175/1000 | Loss: 0.00000941
Iteration 176/1000 | Loss: 0.00000940
Iteration 177/1000 | Loss: 0.00000940
Iteration 178/1000 | Loss: 0.00000940
Iteration 179/1000 | Loss: 0.00000940
Iteration 180/1000 | Loss: 0.00000940
Iteration 181/1000 | Loss: 0.00000940
Iteration 182/1000 | Loss: 0.00000940
Iteration 183/1000 | Loss: 0.00000940
Iteration 184/1000 | Loss: 0.00000940
Iteration 185/1000 | Loss: 0.00000940
Iteration 186/1000 | Loss: 0.00000940
Iteration 187/1000 | Loss: 0.00000940
Iteration 188/1000 | Loss: 0.00000940
Iteration 189/1000 | Loss: 0.00000940
Iteration 190/1000 | Loss: 0.00000940
Iteration 191/1000 | Loss: 0.00000940
Iteration 192/1000 | Loss: 0.00000940
Iteration 193/1000 | Loss: 0.00000940
Iteration 194/1000 | Loss: 0.00000940
Iteration 195/1000 | Loss: 0.00000940
Iteration 196/1000 | Loss: 0.00000940
Iteration 197/1000 | Loss: 0.00000940
Iteration 198/1000 | Loss: 0.00000939
Iteration 199/1000 | Loss: 0.00000939
Iteration 200/1000 | Loss: 0.00000939
Iteration 201/1000 | Loss: 0.00000939
Iteration 202/1000 | Loss: 0.00000939
Iteration 203/1000 | Loss: 0.00000939
Iteration 204/1000 | Loss: 0.00000939
Iteration 205/1000 | Loss: 0.00000939
Iteration 206/1000 | Loss: 0.00000939
Iteration 207/1000 | Loss: 0.00000939
Iteration 208/1000 | Loss: 0.00000939
Iteration 209/1000 | Loss: 0.00000939
Iteration 210/1000 | Loss: 0.00000939
Iteration 211/1000 | Loss: 0.00000939
Iteration 212/1000 | Loss: 0.00000939
Iteration 213/1000 | Loss: 0.00000939
Iteration 214/1000 | Loss: 0.00000939
Iteration 215/1000 | Loss: 0.00000939
Iteration 216/1000 | Loss: 0.00000939
Iteration 217/1000 | Loss: 0.00000939
Iteration 218/1000 | Loss: 0.00000939
Iteration 219/1000 | Loss: 0.00000939
Iteration 220/1000 | Loss: 0.00000939
Iteration 221/1000 | Loss: 0.00000939
Iteration 222/1000 | Loss: 0.00000939
Iteration 223/1000 | Loss: 0.00000939
Iteration 224/1000 | Loss: 0.00000939
Iteration 225/1000 | Loss: 0.00000939
Iteration 226/1000 | Loss: 0.00000939
Iteration 227/1000 | Loss: 0.00000939
Iteration 228/1000 | Loss: 0.00000939
Iteration 229/1000 | Loss: 0.00000939
Iteration 230/1000 | Loss: 0.00000939
Iteration 231/1000 | Loss: 0.00000939
Iteration 232/1000 | Loss: 0.00000939
Iteration 233/1000 | Loss: 0.00000939
Iteration 234/1000 | Loss: 0.00000939
Iteration 235/1000 | Loss: 0.00000939
Iteration 236/1000 | Loss: 0.00000939
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 236. Stopping optimization.
Last 5 losses: [9.389229489897843e-06, 9.389229489897843e-06, 9.389229489897843e-06, 9.389229489897843e-06, 9.389229489897843e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.389229489897843e-06

Optimization complete. Final v2v error: 2.6343629360198975 mm

Highest mean error: 3.05037522315979 mm for frame 58

Lowest mean error: 2.3789756298065186 mm for frame 42

Saving results

Total time: 39.79688501358032
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_025/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_025/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00830808
Iteration 2/25 | Loss: 0.00117020
Iteration 3/25 | Loss: 0.00109125
Iteration 4/25 | Loss: 0.00107841
Iteration 5/25 | Loss: 0.00107493
Iteration 6/25 | Loss: 0.00107493
Iteration 7/25 | Loss: 0.00107493
Iteration 8/25 | Loss: 0.00107493
Iteration 9/25 | Loss: 0.00107493
Iteration 10/25 | Loss: 0.00107493
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010749251814559102, 0.0010749251814559102, 0.0010749251814559102, 0.0010749251814559102, 0.0010749251814559102]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010749251814559102

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36723697
Iteration 2/25 | Loss: 0.00078100
Iteration 3/25 | Loss: 0.00078099
Iteration 4/25 | Loss: 0.00078099
Iteration 5/25 | Loss: 0.00078099
Iteration 6/25 | Loss: 0.00078099
Iteration 7/25 | Loss: 0.00078099
Iteration 8/25 | Loss: 0.00078099
Iteration 9/25 | Loss: 0.00078099
Iteration 10/25 | Loss: 0.00078099
Iteration 11/25 | Loss: 0.00078099
Iteration 12/25 | Loss: 0.00078099
Iteration 13/25 | Loss: 0.00078099
Iteration 14/25 | Loss: 0.00078099
Iteration 15/25 | Loss: 0.00078099
Iteration 16/25 | Loss: 0.00078099
Iteration 17/25 | Loss: 0.00078099
Iteration 18/25 | Loss: 0.00078099
Iteration 19/25 | Loss: 0.00078099
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007809908711351454, 0.0007809908711351454, 0.0007809908711351454, 0.0007809908711351454, 0.0007809908711351454]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007809908711351454

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078099
Iteration 2/1000 | Loss: 0.00002190
Iteration 3/1000 | Loss: 0.00001520
Iteration 4/1000 | Loss: 0.00001389
Iteration 5/1000 | Loss: 0.00001321
Iteration 6/1000 | Loss: 0.00001271
Iteration 7/1000 | Loss: 0.00001235
Iteration 8/1000 | Loss: 0.00001218
Iteration 9/1000 | Loss: 0.00001192
Iteration 10/1000 | Loss: 0.00001165
Iteration 11/1000 | Loss: 0.00001162
Iteration 12/1000 | Loss: 0.00001155
Iteration 13/1000 | Loss: 0.00001155
Iteration 14/1000 | Loss: 0.00001154
Iteration 15/1000 | Loss: 0.00001153
Iteration 16/1000 | Loss: 0.00001152
Iteration 17/1000 | Loss: 0.00001150
Iteration 18/1000 | Loss: 0.00001147
Iteration 19/1000 | Loss: 0.00001146
Iteration 20/1000 | Loss: 0.00001143
Iteration 21/1000 | Loss: 0.00001142
Iteration 22/1000 | Loss: 0.00001139
Iteration 23/1000 | Loss: 0.00001139
Iteration 24/1000 | Loss: 0.00001136
Iteration 25/1000 | Loss: 0.00001133
Iteration 26/1000 | Loss: 0.00001133
Iteration 27/1000 | Loss: 0.00001131
Iteration 28/1000 | Loss: 0.00001125
Iteration 29/1000 | Loss: 0.00001124
Iteration 30/1000 | Loss: 0.00001121
Iteration 31/1000 | Loss: 0.00001120
Iteration 32/1000 | Loss: 0.00001120
Iteration 33/1000 | Loss: 0.00001120
Iteration 34/1000 | Loss: 0.00001120
Iteration 35/1000 | Loss: 0.00001120
Iteration 36/1000 | Loss: 0.00001120
Iteration 37/1000 | Loss: 0.00001119
Iteration 38/1000 | Loss: 0.00001119
Iteration 39/1000 | Loss: 0.00001118
Iteration 40/1000 | Loss: 0.00001117
Iteration 41/1000 | Loss: 0.00001116
Iteration 42/1000 | Loss: 0.00001116
Iteration 43/1000 | Loss: 0.00001115
Iteration 44/1000 | Loss: 0.00001115
Iteration 45/1000 | Loss: 0.00001115
Iteration 46/1000 | Loss: 0.00001115
Iteration 47/1000 | Loss: 0.00001114
Iteration 48/1000 | Loss: 0.00001114
Iteration 49/1000 | Loss: 0.00001114
Iteration 50/1000 | Loss: 0.00001113
Iteration 51/1000 | Loss: 0.00001110
Iteration 52/1000 | Loss: 0.00001110
Iteration 53/1000 | Loss: 0.00001110
Iteration 54/1000 | Loss: 0.00001110
Iteration 55/1000 | Loss: 0.00001110
Iteration 56/1000 | Loss: 0.00001109
Iteration 57/1000 | Loss: 0.00001109
Iteration 58/1000 | Loss: 0.00001107
Iteration 59/1000 | Loss: 0.00001107
Iteration 60/1000 | Loss: 0.00001105
Iteration 61/1000 | Loss: 0.00001105
Iteration 62/1000 | Loss: 0.00001104
Iteration 63/1000 | Loss: 0.00001102
Iteration 64/1000 | Loss: 0.00001101
Iteration 65/1000 | Loss: 0.00001101
Iteration 66/1000 | Loss: 0.00001101
Iteration 67/1000 | Loss: 0.00001100
Iteration 68/1000 | Loss: 0.00001100
Iteration 69/1000 | Loss: 0.00001100
Iteration 70/1000 | Loss: 0.00001100
Iteration 71/1000 | Loss: 0.00001100
Iteration 72/1000 | Loss: 0.00001100
Iteration 73/1000 | Loss: 0.00001099
Iteration 74/1000 | Loss: 0.00001099
Iteration 75/1000 | Loss: 0.00001098
Iteration 76/1000 | Loss: 0.00001098
Iteration 77/1000 | Loss: 0.00001097
Iteration 78/1000 | Loss: 0.00001097
Iteration 79/1000 | Loss: 0.00001097
Iteration 80/1000 | Loss: 0.00001097
Iteration 81/1000 | Loss: 0.00001097
Iteration 82/1000 | Loss: 0.00001097
Iteration 83/1000 | Loss: 0.00001097
Iteration 84/1000 | Loss: 0.00001097
Iteration 85/1000 | Loss: 0.00001097
Iteration 86/1000 | Loss: 0.00001097
Iteration 87/1000 | Loss: 0.00001096
Iteration 88/1000 | Loss: 0.00001096
Iteration 89/1000 | Loss: 0.00001096
Iteration 90/1000 | Loss: 0.00001095
Iteration 91/1000 | Loss: 0.00001095
Iteration 92/1000 | Loss: 0.00001094
Iteration 93/1000 | Loss: 0.00001094
Iteration 94/1000 | Loss: 0.00001094
Iteration 95/1000 | Loss: 0.00001094
Iteration 96/1000 | Loss: 0.00001094
Iteration 97/1000 | Loss: 0.00001094
Iteration 98/1000 | Loss: 0.00001094
Iteration 99/1000 | Loss: 0.00001093
Iteration 100/1000 | Loss: 0.00001093
Iteration 101/1000 | Loss: 0.00001093
Iteration 102/1000 | Loss: 0.00001093
Iteration 103/1000 | Loss: 0.00001092
Iteration 104/1000 | Loss: 0.00001091
Iteration 105/1000 | Loss: 0.00001091
Iteration 106/1000 | Loss: 0.00001091
Iteration 107/1000 | Loss: 0.00001091
Iteration 108/1000 | Loss: 0.00001090
Iteration 109/1000 | Loss: 0.00001090
Iteration 110/1000 | Loss: 0.00001090
Iteration 111/1000 | Loss: 0.00001090
Iteration 112/1000 | Loss: 0.00001090
Iteration 113/1000 | Loss: 0.00001090
Iteration 114/1000 | Loss: 0.00001090
Iteration 115/1000 | Loss: 0.00001090
Iteration 116/1000 | Loss: 0.00001090
Iteration 117/1000 | Loss: 0.00001090
Iteration 118/1000 | Loss: 0.00001090
Iteration 119/1000 | Loss: 0.00001090
Iteration 120/1000 | Loss: 0.00001089
Iteration 121/1000 | Loss: 0.00001087
Iteration 122/1000 | Loss: 0.00001087
Iteration 123/1000 | Loss: 0.00001087
Iteration 124/1000 | Loss: 0.00001087
Iteration 125/1000 | Loss: 0.00001087
Iteration 126/1000 | Loss: 0.00001087
Iteration 127/1000 | Loss: 0.00001087
Iteration 128/1000 | Loss: 0.00001086
Iteration 129/1000 | Loss: 0.00001086
Iteration 130/1000 | Loss: 0.00001086
Iteration 131/1000 | Loss: 0.00001086
Iteration 132/1000 | Loss: 0.00001086
Iteration 133/1000 | Loss: 0.00001086
Iteration 134/1000 | Loss: 0.00001086
Iteration 135/1000 | Loss: 0.00001086
Iteration 136/1000 | Loss: 0.00001086
Iteration 137/1000 | Loss: 0.00001086
Iteration 138/1000 | Loss: 0.00001086
Iteration 139/1000 | Loss: 0.00001086
Iteration 140/1000 | Loss: 0.00001086
Iteration 141/1000 | Loss: 0.00001086
Iteration 142/1000 | Loss: 0.00001086
Iteration 143/1000 | Loss: 0.00001085
Iteration 144/1000 | Loss: 0.00001085
Iteration 145/1000 | Loss: 0.00001085
Iteration 146/1000 | Loss: 0.00001085
Iteration 147/1000 | Loss: 0.00001085
Iteration 148/1000 | Loss: 0.00001084
Iteration 149/1000 | Loss: 0.00001084
Iteration 150/1000 | Loss: 0.00001084
Iteration 151/1000 | Loss: 0.00001084
Iteration 152/1000 | Loss: 0.00001084
Iteration 153/1000 | Loss: 0.00001084
Iteration 154/1000 | Loss: 0.00001084
Iteration 155/1000 | Loss: 0.00001084
Iteration 156/1000 | Loss: 0.00001084
Iteration 157/1000 | Loss: 0.00001084
Iteration 158/1000 | Loss: 0.00001084
Iteration 159/1000 | Loss: 0.00001083
Iteration 160/1000 | Loss: 0.00001083
Iteration 161/1000 | Loss: 0.00001083
Iteration 162/1000 | Loss: 0.00001083
Iteration 163/1000 | Loss: 0.00001083
Iteration 164/1000 | Loss: 0.00001083
Iteration 165/1000 | Loss: 0.00001083
Iteration 166/1000 | Loss: 0.00001083
Iteration 167/1000 | Loss: 0.00001083
Iteration 168/1000 | Loss: 0.00001083
Iteration 169/1000 | Loss: 0.00001082
Iteration 170/1000 | Loss: 0.00001082
Iteration 171/1000 | Loss: 0.00001082
Iteration 172/1000 | Loss: 0.00001082
Iteration 173/1000 | Loss: 0.00001082
Iteration 174/1000 | Loss: 0.00001082
Iteration 175/1000 | Loss: 0.00001082
Iteration 176/1000 | Loss: 0.00001082
Iteration 177/1000 | Loss: 0.00001082
Iteration 178/1000 | Loss: 0.00001082
Iteration 179/1000 | Loss: 0.00001081
Iteration 180/1000 | Loss: 0.00001081
Iteration 181/1000 | Loss: 0.00001081
Iteration 182/1000 | Loss: 0.00001081
Iteration 183/1000 | Loss: 0.00001081
Iteration 184/1000 | Loss: 0.00001081
Iteration 185/1000 | Loss: 0.00001081
Iteration 186/1000 | Loss: 0.00001081
Iteration 187/1000 | Loss: 0.00001081
Iteration 188/1000 | Loss: 0.00001081
Iteration 189/1000 | Loss: 0.00001081
Iteration 190/1000 | Loss: 0.00001081
Iteration 191/1000 | Loss: 0.00001081
Iteration 192/1000 | Loss: 0.00001081
Iteration 193/1000 | Loss: 0.00001081
Iteration 194/1000 | Loss: 0.00001081
Iteration 195/1000 | Loss: 0.00001081
Iteration 196/1000 | Loss: 0.00001081
Iteration 197/1000 | Loss: 0.00001081
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [1.0806931641127449e-05, 1.0806931641127449e-05, 1.0806931641127449e-05, 1.0806931641127449e-05, 1.0806931641127449e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0806931641127449e-05

Optimization complete. Final v2v error: 2.7952985763549805 mm

Highest mean error: 3.213695764541626 mm for frame 97

Lowest mean error: 2.655871868133545 mm for frame 144

Saving results

Total time: 40.93228816986084
